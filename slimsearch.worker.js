/**
* @vue/shared v3.5.17
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**//*! #__NO_SIDE_EFFECTS__ */function xt(e){const t=Object.create(null);for(const n of e.split(","))t[n]=1;return n=>n in t}const Ot={},St=()=>{},ze=Object.assign,It=Object.prototype.hasOwnProperty,se=(e,t)=>It.call(e,t),z=Array.isArray,Y=e=>Ve(e)==="[object Map]",oe=e=>typeof e=="function",Ce=e=>typeof e=="string",L=e=>typeof e=="symbol",B=e=>e!==null&&typeof e=="object",Nt=Object.prototype.toString,Ve=e=>Nt.call(e),ke=e=>Ve(e).slice(8,-1),ie=e=>Ce(e)&&e!=="NaN"&&e[0]!=="-"&&""+parseInt(e,10)===e,Mt=e=>{const t=Object.create(null);return n=>t[n]||(t[n]=e(n))},Ln=Mt(e=>e.charAt(0).toUpperCase()+e.slice(1)),W=(e,t)=>!Object.is(e,t);let $e;const ce=()=>$e||($e=typeof globalThis<"u"?globalThis:typeof self<"u"?self:typeof window<"u"?window:typeof global<"u"?global:{});/**
* @vue/reactivity v3.5.17
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/function Rt(e,...t){console.warn(`[Vue warn] ${e}`,...t)}let Tt,je=0,ae;function ue(){je++}function le(){if(--je>0)return;let e;for(;ae;){let t=ae;for(ae=void 0;t;){const n=t.next;if(t.next=void 0,t.flags&=-9,t.flags&1)try{t.trigger()}catch(s){e||(e=s)}t=n}}if(e)throw e}let G=!0;const De=[];function fe(){De.push(G),G=!1}function de(){const e=De.pop();G=e===void 0?!0:e}class Fe{constructor(t){this.computed=t,this.version=0,this.activeLink=void 0,this.subs=void 0,this.map=void 0,this.key=void 0,this.sc=0,this.__v_skip=!0}track(t){}trigger(t){this.version++,this.notify(t)}notify(t){ue();try{for(let n=this.subs;n;n=n.prevSub)n.sub.notify()&&n.sub.dep.notify()}finally{le()}}}const he=new WeakMap,C=Symbol(""),pe=Symbol(""),H=Symbol("");function E(e,t,n){if(G&&Tt){let s=he.get(e);s||he.set(e,s=new Map);let r=s.get(n);r||(s.set(n,r=new Fe),r.map=s,r.key=n),r.track()}}function I(e,t,n,s,r,o){const i=he.get(e);if(!i)return;const c=a=>{a&&a.trigger()};if(ue(),t==="clear")i.forEach(c);else{const a=z(e),u=a&&ie(n);if(a&&n==="length"){const l=Number(s);i.forEach((f,d)=>{(d==="length"||d===H||!L(d)&&d>=l)&&c(f)})}else switch((n!==void 0||i.has(void 0))&&c(i.get(n)),u&&c(i.get(H)),t){case"add":a?u&&c(i.get("length")):(c(i.get(C)),Y(e)&&c(i.get(pe)));break;case"delete":a||(c(i.get(C)),Y(e)&&c(i.get(pe)));break;case"set":Y(e)&&c(i.get(C));break}}le()}function $(e){const t=p(e);return t===e?t:(E(t,"iterate",H),N(e)?t:t.map(v))}function ge(e){return E(e=p(e),"iterate",H),e}const zt={__proto__:null,[Symbol.iterator](){return _e(this,Symbol.iterator,v)},concat(...e){return $(this).concat(...e.map(t=>z(t)?$(t):t))},entries(){return _e(this,"entries",e=>(e[1]=v(e[1]),e))},every(e,t){return O(this,"every",e,t,void 0,arguments)},filter(e,t){return O(this,"filter",e,t,n=>n.map(v),arguments)},find(e,t){return O(this,"find",e,t,v,arguments)},findIndex(e,t){return O(this,"findIndex",e,t,void 0,arguments)},findLast(e,t){return O(this,"findLast",e,t,v,arguments)},findLastIndex(e,t){return O(this,"findLastIndex",e,t,void 0,arguments)},forEach(e,t){return O(this,"forEach",e,t,void 0,arguments)},includes(...e){return me(this,"includes",e)},indexOf(...e){return me(this,"indexOf",e)},join(e){return $(this).join(e)},lastIndexOf(...e){return me(this,"lastIndexOf",e)},map(e,t){return O(this,"map",e,t,void 0,arguments)},pop(){return K(this,"pop")},push(...e){return K(this,"push",e)},reduce(e,...t){return Pe(this,"reduce",e,t)},reduceRight(e,...t){return Pe(this,"reduceRight",e,t)},shift(){return K(this,"shift")},some(e,t){return O(this,"some",e,t,void 0,arguments)},splice(...e){return K(this,"splice",e)},toReversed(){return $(this).toReversed()},toSorted(e){return $(this).toSorted(e)},toSpliced(...e){return $(this).toSpliced(...e)},unshift(...e){return K(this,"unshift",e)},values(){return _e(this,"values",v)}};function _e(e,t,n){const s=ge(e),r=s[t]();return s!==e&&!N(e)&&(r._next=r.next,r.next=()=>{const o=r._next();return o.value&&(o.value=n(o.value)),o}),r}const Ct=Array.prototype;function O(e,t,n,s,r,o){const i=ge(e),c=i!==e&&!N(e),a=i[t];if(a!==Ct[t]){const f=a.apply(e,o);return c?v(f):f}let u=n;i!==e&&(c?u=function(f,d){return n.call(this,v(f),d,e)}:n.length>2&&(u=function(f,d){return n.call(this,f,d,e)}));const l=a.call(i,u,s);return c&&r?r(l):l}function Pe(e,t,n,s){const r=ge(e);let o=n;return r!==e&&(N(e)?n.length>3&&(o=function(i,c,a){return n.call(this,i,c,a,e)}):o=function(i,c,a){return n.call(this,i,v(c),a,e)}),r[t](o,...s)}function me(e,t,n){const s=p(e);E(s,"iterate",H);const r=s[t](...n);return(r===-1||r===!1)&&Jt(n[0])?(n[0]=p(n[0]),s[t](...n)):r}function K(e,t,n=[]){fe(),ue();const s=p(e)[t].apply(e,n);return le(),de(),s}const Vt=xt("__proto__,__v_isRef,__isVue"),Ae=new Set(Object.getOwnPropertyNames(Symbol).filter(e=>e!=="arguments"&&e!=="caller").map(e=>Symbol[e]).filter(L));function kt(e){L(e)||(e=String(e));const t=p(this);return E(t,"has",e),t.hasOwnProperty(e)}class Le{constructor(t=!1,n=!1){this._isReadonly=t,this._isShallow=n}get(t,n,s){if(n==="__v_skip")return t.__v_skip;const r=this._isReadonly,o=this._isShallow;if(n==="__v_isReactive")return!r;if(n==="__v_isReadonly")return r;if(n==="__v_isShallow")return o;if(n==="__v_raw")return s===(r?o?Kt:Ke:o?Ht:He).get(t)||Object.getPrototypeOf(t)===Object.getPrototypeOf(s)?t:void 0;const i=z(t);if(!r){let a;if(i&&(a=zt[n]))return a;if(n==="hasOwnProperty")return kt}const c=Reflect.get(t,n,V(t)?t:s);return(L(n)?Ae.has(n):Vt(n))||(r||E(t,"get",n),o)?c:V(c)?i&&ie(n)?c:c.value:B(c)?r?Ue(c):qe(c):c}}class $t extends Le{constructor(t=!1){super(!1,t)}set(t,n,s,r){let o=t[n];if(!this._isShallow){const a=j(o);if(!N(s)&&!j(s)&&(o=p(o),s=p(s)),!z(t)&&V(o)&&!V(s))return a?!1:(o.value=s,!0)}const i=z(t)&&ie(n)?Number(n)<t.length:se(t,n),c=Reflect.set(t,n,s,V(t)?t:r);return t===p(r)&&(i?W(s,o)&&I(t,"set",n,s,o):I(t,"add",n,s)),c}deleteProperty(t,n){const s=se(t,n),r=t[n],o=Reflect.deleteProperty(t,n);return o&&s&&I(t,"delete",n,void 0,r),o}has(t,n){const s=Reflect.has(t,n);return(!L(n)||!Ae.has(n))&&E(t,"has",n),s}ownKeys(t){return E(t,"iterate",z(t)?"length":C),Reflect.ownKeys(t)}}class jt extends Le{constructor(t=!1){super(!0,t)}set(t,n){return!0}deleteProperty(t,n){return!0}}const Dt=new $t,Ft=new jt,ye=e=>e,Q=e=>Reflect.getPrototypeOf(e);function Pt(e,t,n){return function(...s){const r=this.__v_raw,o=p(r),i=Y(o),c=e==="entries"||e===Symbol.iterator&&i,a=e==="keys"&&i,u=r[e](...s),l=n?ye:t?we:v;return!t&&E(o,"iterate",a?pe:C),{next(){const{value:f,done:d}=u.next();return d?{value:f,done:d}:{value:c?[l(f[0]),l(f[1])]:l(f),done:d}},[Symbol.iterator](){return this}}}}function X(e){return function(...t){return e==="delete"?!1:e==="clear"?void 0:this}}function At(e,t){const n={get(r){const o=this.__v_raw,i=p(o),c=p(r);e||(W(r,c)&&E(i,"get",r),E(i,"get",c));const{has:a}=Q(i),u=t?ye:e?we:v;if(a.call(i,r))return u(o.get(r));if(a.call(i,c))return u(o.get(c));o!==i&&o.get(r)},get size(){const r=this.__v_raw;return!e&&E(p(r),"iterate",C),Reflect.get(r,"size",r)},has(r){const o=this.__v_raw,i=p(o),c=p(r);return e||(W(r,c)&&E(i,"has",r),E(i,"has",c)),r===c?o.has(r):o.has(r)||o.has(c)},forEach(r,o){const i=this,c=i.__v_raw,a=p(c),u=t?ye:e?we:v;return!e&&E(a,"iterate",C),c.forEach((l,f)=>r.call(o,u(l),u(f),i))}};return ze(n,e?{add:X("add"),set:X("set"),delete:X("delete"),clear:X("clear")}:{add(r){!t&&!N(r)&&!j(r)&&(r=p(r));const o=p(this);return Q(o).has.call(o,r)||(o.add(r),I(o,"add",r,r)),this},set(r,o){!t&&!N(o)&&!j(o)&&(o=p(o));const i=p(this),{has:c,get:a}=Q(i);let u=c.call(i,r);u||(r=p(r),u=c.call(i,r));const l=a.call(i,r);return i.set(r,o),u?W(o,l)&&I(i,"set",r,o,l):I(i,"add",r,o),this},delete(r){const o=p(this),{has:i,get:c}=Q(o);let a=i.call(o,r);a||(r=p(r),a=i.call(o,r));const u=c?c.call(o,r):void 0,l=o.delete(r);return a&&I(o,"delete",r,void 0,u),l},clear(){const r=p(this),o=r.size!==0,i=void 0,c=r.clear();return o&&I(r,"clear",void 0,void 0,i),c}}),["keys","values","entries",Symbol.iterator].forEach(r=>{n[r]=Pt(r,e,t)}),n}function We(e,t){const n=At(e,t);return(s,r,o)=>r==="__v_isReactive"?!e:r==="__v_isReadonly"?e:r==="__v_raw"?s:Reflect.get(se(n,r)&&r in s?n:s,r,o)}const Lt={get:We(!1,!1)},Wt={get:We(!0,!1)};function Wn(e,t,n){const s=p(n);if(s!==n&&t.call(e,s)){const r=ke(e);Rt(`Reactive ${r} contains both the raw and reactive versions of the same object${r==="Map"?" as keys":""}, which can lead to inconsistencies. Avoid differentiating between the raw and reactive versions of an object and only use the reactive version if possible.`)}}const He=new WeakMap,Ht=new WeakMap,Ke=new WeakMap,Kt=new WeakMap;function qt(e){switch(e){case"Object":case"Array":return 1;case"Map":case"Set":case"WeakMap":case"WeakSet":return 2;default:return 0}}function Ut(e){return e.__v_skip||!Object.isExtensible(e)?0:qt(ke(e))}function qe(e){return j(e)?e:Je(e,!1,Dt,Lt,He)}function Ue(e){return Je(e,!0,Ft,Wt,Ke)}function Je(e,t,n,s,r){if(!B(e)||e.__v_raw&&!(t&&e.__v_isReactive))return e;const o=Ut(e);if(o===0)return e;const i=r.get(e);if(i)return i;const c=new Proxy(e,o===2?s:n);return r.set(e,c),c}function j(e){return!!(e&&e.__v_isReadonly)}function N(e){return!!(e&&e.__v_isShallow)}function Jt(e){return e?!!e.__v_raw:!1}function p(e){const t=e&&e.__v_raw;return t?p(t):e}const v=e=>B(e)?qe(e):e,we=e=>B(e)?Ue(e):e;function V(e){return e?e.__v_isRef===!0:!1}function Yt(e){return Bt(e,!1)}function Bt(e,t){return V(e)?e:new Gt(e,t)}class Gt{constructor(t,n){this.dep=new Fe,this.__v_isRef=!0,this.__v_isShallow=!1,this._rawValue=n?t:p(t),this._value=n?t:v(t),this.__v_isShallow=n}get value(){return this.dep.track(),this._value}set value(t){const n=this._rawValue,s=this.__v_isShallow||N(t)||j(t);t=s?t:p(t),W(t,n)&&(this._rawValue=t,this._value=s?t:v(t),this.dep.trigger())}}/**
* @vue/runtime-core v3.5.17
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/const k=[];function Hn(e){k.push(e)}function Kn(){k.pop()}let be=!1;function qn(e,...t){if(be)return;be=!0,fe();const n=k.length?k[k.length-1].component:null,s=n&&n.appContext.config.warnHandler,r=Qt();if(s)ve(s,n,11,[e+t.map(o=>{var i,c;return(c=(i=o.toString)==null?void 0:i.call(o))!=null?c:JSON.stringify(o)}).join(""),n&&n.proxy,r.map(({vnode:o})=>`at <${et(n,o.type)}>`).join(`
`),r]);else{const o=[`[Vue warn]: ${e}`,...t];r.length&&o.push(`
`,...Xt(r)),console.warn(...o)}de(),be=!1}function Qt(){let e=k[k.length-1];if(!e)return[];const t=[];for(;e;){const n=t[0];n&&n.vnode===e?n.recurseCount++:t.push({vnode:e,recurseCount:0});const s=e.component&&e.component.parent;e=s&&s.vnode}return t}function Xt(e){const t=[];return e.forEach((n,s)=>{t.push(...s===0?[]:[`
`],...Zt(n))}),t}function Zt({vnode:e,recurseCount:t}){const n=t>0?`... (${t} recursive calls)`:"",s=e.component?e.component.parent==null:!1,r=` at <${et(e.component,e.type,s)}`,o=">"+n;return e.props?[r,...en(e.props),o]:[r+o]}function en(e){const t=[],n=Object.keys(e);return n.slice(0,3).forEach(s=>{t.push(...Ye(s,e[s]))}),n.length>3&&t.push(" ..."),t}function Ye(e,t,n){return Ce(t)?(t=JSON.stringify(t),n?t:[`${e}=${t}`]):typeof t=="number"||typeof t=="boolean"||t==null?n?t:[`${e}=${t}`]:V(t)?(t=Ye(e,p(t.value),!0),n?t:[`${e}=Ref<`,t,">"]):oe(t)?[`${e}=fn${t.name?`<${t.name}>`:""}`]:(t=p(t),n?t:[`${e}=`,t])}const Un={sp:"serverPrefetch hook",bc:"beforeCreate hook",c:"created hook",bm:"beforeMount hook",m:"mounted hook",bu:"beforeUpdate hook",u:"updated",bum:"beforeUnmount hook",um:"unmounted hook",a:"activated hook",da:"deactivated hook",ec:"errorCaptured hook",rtc:"renderTracked hook",rtg:"renderTriggered hook",0:"setup function",1:"render function",2:"watcher getter",3:"watcher callback",4:"watcher cleanup function",5:"native event handler",6:"component event handler",7:"vnode hook",8:"directive hook",9:"transition hook",10:"app errorHandler",11:"app warnHandler",12:"ref function",13:"async component loader",14:"scheduler flush",15:"component update",16:"app unmount cleanup function"};function ve(e,t,n,s){try{return s?e(...s):e()}catch(r){Be(r,t,n)}}function Be(e,t,n,s=!0){const r=t?t.vnode:null,{errorHandler:o,throwUnhandledErrorInProduction:i}=t&&t.appContext.config||Ot;if(t){let c=t.parent;const a=t.proxy,u=`https://vuejs.org/error-reference/#runtime-${n}`;for(;c;){const l=c.ec;if(l){for(let f=0;f<l.length;f++)if(l[f](e,a,u)===!1)return}c=c.parent}if(o){fe(),ve(o,null,10,[e,a,u]),de();return}}tn(e,n,r,s,i)}function tn(e,t,n,s=!0,r=!1){if(r)throw e;console.error(e)}const x=[];let S=-1;const D=[];let M=null,F=0;const nn=Promise.resolve();let Ee=null;const rn=100;function sn(e){let t=S+1,n=x.length;for(;t<n;){const s=t+n>>>1,r=x[s],o=q(r);o<e||o===e&&r.flags&2?t=s+1:n=s}return t}function on(e){if(!(e.flags&1)){const t=q(e),n=x[x.length-1];!n||!(e.flags&2)&&t>=q(n)?x.push(e):x.splice(sn(t),0,e),e.flags|=1,Ge()}}function Ge(){Ee||(Ee=nn.then(Qe))}function cn(e){z(e)?D.push(...e):M&&e.id===-1?M.splice(F+1,0,e):e.flags&1||(D.push(e),e.flags|=1),Ge()}function an(e){if(D.length){const t=[...new Set(D)].sort((n,s)=>q(n)-q(s));if(D.length=0,M){M.push(...t);return}for(M=t,F=0;F<M.length;F++){const n=M[F];n.flags&4&&(n.flags&=-2),n.flags&8||n(),n.flags&=-2}M=null,F=0}}const q=e=>e.id==null?e.flags&2?-1:1/0:e.id;function Qe(e){const t=St;try{for(S=0;S<x.length;S++){const n=x[S];n&&!(n.flags&8)&&(n.flags&4&&(n.flags&=-2),ve(n,n.i,n.i?15:14),n.flags&4||(n.flags&=-2))}}finally{for(;S<x.length;S++){const n=x[S];n&&(n.flags&=-2)}S=-1,x.length=0,an(e),Ee=null,(x.length||D.length)&&Qe(e)}}function Jn(e,t){const n=e.get(t)||0;if(n>rn){const s=t.i,r=s&&Ze(s.type);return Be(`Maximum recursive updates exceeded${r?` in component <${r}>`:""}. This means you have a reactive effect that is mutating its own dependencies and thus recursively triggering itself. Possible sources include component template, render function, updated hook or watcher source function.`,null,10),!0}return e.set(t,n+1),!1}const xe=new Map,Z=new Map;function Yn(e,t){return Z.has(e)?!1:(Z.set(e,{initialDef:ee(t),instances:new Set}),!0)}function ee(e){return fn(e)?e.__vccOpts:e}function Bn(e,t){const n=Z.get(e);n&&(n.initialDef.render=t,[...n.instances].forEach(s=>{t&&(s.render=t,ee(s.type).render=t),s.renderCache=[],s.update()}))}function Gn(e,t){const n=Z.get(e);if(!n)return;t=ee(t),Xe(n.initialDef,t);const s=[...n.instances];for(let r=0;r<s.length;r++){const o=s[r],i=ee(o.type);let c=xe.get(i);c||(i!==n.initialDef&&Xe(i,t),xe.set(i,c=new Set)),c.add(o),o.appContext.propsCache.delete(o.type),o.appContext.emitsCache.delete(o.type),o.appContext.optionsCache.delete(o.type),o.ceReload?(c.add(o),o.ceReload(t.styles),c.delete(o)):o.parent?on(()=>{o.parent.update(),c.delete(o)}):o.appContext.reload?o.appContext.reload():typeof window<"u"?window.location.reload():console.warn("[HMR] Root or manually mounted instance modified. Full reload required."),o.root.ce&&o!==o.root&&o.root.ce._removeChildStyle(i)}cn(()=>{xe.clear()})}function Xe(e,t){ze(e,t);for(const n in e)n!=="__file"&&!(n in t)&&delete e[n]}function Qn(e){return(t,n)=>{try{return e(t,n)}catch(s){console.error(s),console.warn("[HMR] Something went wrong during Vue component hot-reload. Full reload required.")}}}ce().requestIdleCallback,ce().cancelIdleCallback;const Xn={};{const e=ce(),t=(n,s)=>{let r;return(r=e[n])||(r=e[n]=[]),r.push(s),o=>{r.length>1?r.forEach(i=>i(o)):r[0](o)}};t("__VUE_INSTANCE_SETTERS__",n=>n),t("__VUE_SSR_SETTERS__",n=>n)}const un=/(?:^|[-_])(\w)/g,ln=e=>e.replace(un,t=>t.toUpperCase()).replace(/[-_]/g,"");function Ze(e,t=!0){return oe(e)?e.displayName||e.name:e.name||t&&e.__name}function et(e,t,n=!1){let s=Ze(t);if(!s&&t.__file){const r=t.__file.match(/([^/\\]+)\.\w+$/);r&&(s=r[1])}if(!s&&e&&e.parent){const r=o=>{for(const i in o)if(o[i]===t)return i};s=r(e.components||e.parent.type.components)||r(e.appContext.components)}return s?ln(s):n?"App":"Anonymous"}function fn(e){return oe(e)&&"__vccOpts"in e}const tt=()=>document.documentElement.getAttribute("data-theme")==="dark";[...new Array(6)].map((e,t)=>`[vp-content] h${t+1}`).join(",");const{entries:dn}=Object,{fromEntries:hn}=Object,nt=Yt(!1);typeof document<"u"&&(nt.value=tt(),new MutationObserver(()=>{nt.value=tt()}).observe(document.documentElement,{attributeFilter:["data-theme"],attributes:!0}));const pn="ENTRIES",rt="KEYS",st="VALUES",w="";class Oe{set;_type;_path;constructor(t,n){const s=t._tree,r=Array.from(s.keys());this.set=t,this._type=n,this._path=r.length>0?[{node:s,keys:r}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:n}=P(this._path);if(P(n)===w)return{done:!1,value:this.result()};const s=t.get(P(n));return this._path.push({node:s,keys:Array.from(s.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=P(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>P(t)).filter(t=>t!==w).join("")}value(){return P(this._path).node.get(w)}result(){switch(this._type){case st:return this.value();case rt:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const P=e=>e[e.length-1],gn=(e,t,n)=>{const s=new Map;if(typeof t!="string")return s;const r=t.length+1,o=r+n,i=new Uint8Array(o*r).fill(n+1);for(let c=0;c<r;++c)i[c]=c;for(let c=1;c<o;++c)i[c*r]=c;return ot(e,t,n,s,i,1,r,""),s},ot=(e,t,n,s,r,o,i,c)=>{const a=o*i;e:for(const u of e.keys())if(u===w){const l=r[a-1];l<=n&&s.set(c,[e.get(u),l])}else{let l=o;for(let f=0;f<u.length;++f,++l){const d=u[f],m=i*l,b=m-i;let g=r[m];const h=Math.max(0,l-n-1),_=Math.min(i-1,l+n);for(let y=h;y<_;++y){const R=d!==t[y],re=r[b+y]+ +R,J=r[b+y+1]+1,T=r[m+y]+1,A=r[m+y+1]=Math.min(re,J,T);A<g&&(g=A)}if(g>n)continue e}ot(e.get(u),t,n,s,r,l,i,c+u)}};let it=class U{_tree;_prefix;_size=void 0;constructor(t=new Map,n=""){this._tree=t,this._prefix=n}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[n,s]=te(this._tree,t.slice(this._prefix.length));if(n===void 0){const[r,o]=Ne(s);for(const i of r.keys())if(i!==w&&i.startsWith(o)){const c=new Map;return c.set(i.slice(o.length),r.get(i)),new U(c,t)}}return new U(n,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,_n(this._tree,t)}entries(){return new Oe(this,pn)}forEach(t){for(const[n,s]of this)t(n,s,this)}fuzzyGet(t,n){return gn(this._tree,t,n)}get(t){const n=Se(this._tree,t);return n!==void 0?n.get(w):void 0}has(t){return Se(this._tree,t)?.has(w)??!1}keys(){return new Oe(this,rt)}set(t,n){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,Ie(this._tree,t).set(w,n),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,n){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const s=Ie(this._tree,t);return s.set(w,n(s.get(w))),this}fetch(t,n){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const s=Ie(this._tree,t);let r=s.get(w);return r===void 0&&s.set(w,r=n()),r}values(){return new Oe(this,st)}[Symbol.iterator](){return this.entries()}static from(t){const n=new U;for(const[s,r]of t)n.set(s,r);return n}static fromObject(t){return U.from(Object.entries(t))}};const te=(e,t,n=[])=>{if(t.length===0||e==null)return[e,n];for(const s of e.keys())if(s!==w&&t.startsWith(s))return n.push([e,s]),te(e.get(s),t.slice(s.length),n);return n.push([e,t]),te(void 0,"",n)},Se=(e,t)=>{if(t.length===0||!e)return e;for(const n of e.keys())if(n!==w&&t.startsWith(n))return Se(e.get(n),t.slice(n.length))},Ie=(e,t)=>{const n=t.length;e:for(let s=0;e&&s<n;){for(const o of e.keys())if(o!==w&&t[s]===o[0]){const i=Math.min(n-s,o.length);let c=1;for(;c<i&&t[s+c]===o[c];)++c;const a=e.get(o);if(c===o.length)e=a;else{const u=new Map;u.set(o.slice(c),a),e.set(t.slice(s,s+c),u),e.delete(o),e=u}s+=c;continue e}const r=new Map;return e.set(t.slice(s),r),r}return e},_n=(e,t)=>{const[n,s]=te(e,t);if(n!==void 0){if(n.delete(w),n.size===0)ct(s);else if(n.size===1){const[r,o]=n.entries().next().value;at(s,r,o)}}},ct=e=>{if(e.length===0)return;const[t,n]=Ne(e);if(t.delete(n),t.size===0)ct(e.slice(0,-1));else if(t.size===1){const[s,r]=t.entries().next().value;s!==w&&at(e.slice(0,-1),s,r)}},at=(e,t,n)=>{if(e.length===0)return;const[s,r]=Ne(e);s.set(r+t,n),s.delete(r)},Ne=e=>e[e.length-1],mn=(e,t)=>{const n=e._idToShortId.get(t);if(n!=null)return e._storedFields.get(n)},yn=/[\n\r\p{Z}\p{P}]+/u,Me="or",ut="and",wn="and_not",bn=(e,t)=>{e.includes(t)||e.push(t)},lt=(e,t)=>{for(const n of t)e.includes(n)||e.push(n)},ft=({score:e},{score:t})=>t-e,vn=()=>new Map,ne=e=>{const t=new Map;for(const n of Object.keys(e))t.set(parseInt(n,10),e[n]);return t},dt=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,ht={[Me]:(e,t)=>{for(const n of t.keys()){const s=e.get(n);if(s==null)e.set(n,t.get(n));else{const{score:r,terms:o,match:i}=t.get(n);s.score=s.score+r,s.match=Object.assign(s.match,i),lt(s.terms,o)}}return e},[ut]:(e,t)=>{const n=new Map;for(const s of t.keys()){const r=e.get(s);if(r==null)continue;const{score:o,terms:i,match:c}=t.get(s);lt(r.terms,i),n.set(s,{score:r.score+o,terms:r.terms,match:Object.assign(r.match,c)})}return n},[wn]:(e,t)=>{for(const n of t.keys())e.delete(n);return e}},En=(e,t,n,s,r,o)=>{const{k:i,b:c,d:a}=o;return Math.log(1+(n-t+.5)/(t+.5))*(a+e*(i+1)/(e+i*(1-c+c*s/r)))},xn=e=>(t,n,s)=>({term:t,fuzzy:typeof e.fuzzy=="function"?e.fuzzy(t,n,s):e.fuzzy??!1,prefix:typeof e.prefix=="function"?e.prefix(t,n,s):e.prefix===!0,termBoost:typeof e.boostTerm=="function"?e.boostTerm(t,n,s):1}),pt=(e,t,n,s)=>{for(const r of Object.keys(e._fieldIds))if(e._fieldIds[r]===n){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${s}" was not present in field "${r}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},On=(e,t,n,s)=>{if(!e._index.has(s)){pt(e,n,t,s);return}const r=e._index.fetch(s,vn),o=r.get(t),i=o?.get(n);!o||typeof i>"u"?pt(e,n,t,s):i<=1?o.size<=1?r.delete(t):o.delete(n):o.set(n,i-1),e._index.get(s).size===0&&e._index.delete(s)},Sn={k:1.2,b:.7,d:.5},In={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(yn),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{console?.[e]?.(t)},autoVacuum:!0},gt={combineWith:Me,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:Sn},Nn={combineWith:ut,prefix:(e,t,n)=>t===n.length-1},Mn={batchSize:1e3,batchWait:10},_t={minDirtFactor:.1,minDirtCount:20},Rn={...Mn,..._t},mt=Symbol("*"),Tn=(e,t)=>{const n=new Map,s={...e._options.searchOptions,...t};for(const[r,o]of e._documentIds){const i=s.boostDocument?s.boostDocument(o,"",e._storedFields.get(r)):1;n.set(r,{score:i,terms:[],match:{}})}return n},yt=(e,t=Me)=>{if(e.length===0)return new Map;const n=t.toLowerCase();if(!(n in ht))throw new Error(`Invalid combination operator: ${t}`);return e.reduce(ht[n])},Re=(e,t,n,s,r,o,i,c,a,u=new Map)=>{if(o==null)return u;for(const l of Object.keys(i)){const f=i[l],d=e._fieldIds[l],m=o.get(d);if(m==null)continue;let b=m.size;const g=e._avgFieldLength[d];for(const h of m.keys()){if(!e._documentIds.has(h)){On(e,d,h,n),b-=1;continue}const _=c?c(e._documentIds.get(h),n,e._storedFields.get(h)):1;if(!_)continue;const y=m.get(h),R=e._fieldLength.get(h)[d],re=En(y,b,e._documentCount,R,g,a),J=s*r*f*_*re,T=u.get(h);if(T){T.score+=J,bn(T.terms,t);const A=dt(T.match,n);A?A.push(l):T.match[n]=[l]}else u.set(h,{score:J,terms:[t],match:{[n]:[l]}})}}return u},zn=(e,t,n)=>{const s={...e._options.searchOptions,...n},r=(s.fields??e._options.fields).reduce((g,h)=>({...g,[h]:dt(s.boost,h)||1}),{}),{boostDocument:o,weights:i,maxFuzzy:c,bm25:a}=s,{fuzzy:u,prefix:l}={...gt.weights,...i},f=e._index.get(t.term),d=Re(e,t.term,t.term,1,t.termBoost,f,r,o,a);let m,b;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const g=t.fuzzy===!0?.2:t.fuzzy,h=g<1?Math.min(c,Math.round(t.term.length*g)):g;h&&(b=e._index.fuzzyGet(t.term,h))}if(m)for(const[g,h]of m){const _=g.length-t.term.length;if(!_)continue;b?.delete(g);const y=l*g.length/(g.length+.3*_);Re(e,t.term,g,y,t.termBoost,h,r,o,a,d)}if(b)for(const g of b.keys()){const[h,_]=b.get(g);if(!_)continue;const y=u*g.length/(g.length+_);Re(e,t.term,g,y,t.termBoost,h,r,o,a,d)}return d},wt=(e,t,n={})=>{if(t===mt)return Tn(e,n);if(typeof t!="string"){const l={...n,...t,queries:void 0},f=t.queries.map(d=>wt(e,d,l));return yt(f,l.combineWith)}const{tokenize:s,processTerm:r,searchOptions:o}=e._options,i={tokenize:s,processTerm:r,...o,...n},{tokenize:c,processTerm:a}=i,u=c(t).flatMap(l=>a(l)).filter(l=>!!l).map(xn(i)).map(l=>zn(e,l,i));return yt(u,i.combineWith)},bt=(e,t,n={})=>{const{searchOptions:s}=e._options,r={...s,...n},o=wt(e,t,n),i=[];for(const[c,{score:a,terms:u,match:l}]of o){const f=u.length||1,d={id:e._documentIds.get(c),score:a*f,terms:Object.keys(l),queryTerms:u,match:l};Object.assign(d,e._storedFields.get(c)),(r.filter==null||r.filter(d))&&i.push(d)}return t===mt&&r.boostDocument==null||i.sort(ft),i},Cn=(e,t,n={})=>{n={...e._options.autoSuggestOptions,...n};const s=new Map;for(const{score:o,terms:i}of bt(e,t,n)){const c=i.join(" "),a=s.get(c);a!=null?(a.score+=o,a.count+=1):s.set(c,{score:o,terms:i,count:1})}const r=[];for(const[o,{score:i,terms:c,count:a}]of s)r.push({suggestion:o,terms:c,score:i/a});return r.sort(ft),r};class Vn{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(!t?.fields)throw new Error('SlimSearch: option "fields" must be provided');const n=t.autoVacuum==null||t.autoVacuum===!0?Rn:t.autoVacuum;this._options={...In,...t,autoVacuum:n,searchOptions:{...gt,...t.searchOptions},autoSuggestOptions:{...Nn,...t.autoSuggestOptions}},this._index=new it,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=_t,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[n,s]of this._index){const r={};for(const[o,i]of s)r[o]=Object.fromEntries(i);t.push([n,r])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,version:2}}addFields(t){for(let n=0;n<t.length;n++)this._fieldIds[t[n]]=n}}const kn=e=>new Vn(e),$n=({documentCount:e,nextId:t,fieldIds:n,averageFieldLength:s,dirtCount:r,version:o},i)=>{if(o!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const c=kn(i);return c._documentCount=e,c._nextId=t,c._idToShortId=new Map,c._fieldIds=n,c._avgFieldLength=s,c._dirtCount=r??0,c._index=new it,c},jn=(e,t)=>{const{index:n,documentIds:s,fieldLength:r,storedFields:o}=e,i=$n(e,t);i._documentIds=ne(s),i._fieldLength=ne(r),i._storedFields=ne(o);for(const[c,a]of i._documentIds)i._idToShortId.set(a,c);for(const[c,a]of n){const u=new Map;for(const l of Object.keys(a))u.set(parseInt(l,10),ne(a[l]));i._index.set(c,u)}return i},Te=(e,t)=>{const n=e.toLowerCase(),s=t.toLowerCase(),r=[];let o=0,i=0;const c=(u,l=!1)=>{let f;i===0?f=u.length>20?`… ${u.slice(-20)}`:u:l?f=u.length+i>100?`${u.slice(0,100-i)}… `:u:f=u.length>20?`${u.slice(0,20)} … ${u.slice(-20)}`:u,f&&r.push(f),i+=f.length,l||(r.push(["mark",t]),i+=t.length,i>=100&&r.push(" …"))};let a=n.indexOf(s,o);if(a===-1)return null;for(;a>=0;){const u=a+s.length;if(c(e.slice(o,a)),o=u,i>100)break;a=n.indexOf(s,o)}return i<100&&c(e.slice(o),!0),r},{entries:Dn}=Object,Fn=(e,t)=>t.contents.reduce((n,[,s])=>n+s,0)-e.contents.reduce((n,[,s])=>n+s,0),Pn=(e,t)=>Math.max(...t.contents.map(([,n])=>n))-Math.max(...e.contents.map(([,n])=>n)),vt=(e,t,n={},s="max")=>{const r={};return bt(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...n}).forEach(o=>{const{id:i,terms:c,score:a}=o,u=i.includes("@"),l=i.includes("#"),[f,d]=i.split(/[#@]/),m=Number(f),b=c.sort((h,_)=>h.length-_.length).filter((h,_)=>c.slice(_+1).every(y=>!y.includes(h))),{contents:g}=r[m]??={title:"",contents:[]};if(u)g.push([{type:"customField",id:m,index:d,display:b.map(h=>o.c.map(_=>Te(_,h))).flat().filter(h=>h!==null)},a]);else{const h=b.map(_=>Te(o.h,_)).filter(_=>_!==null);if(h.length&&g.push([{type:l?"heading":"title",id:m,...l&&{anchor:d},display:h},a]),"t"in o&&o.t)for(const _ of o.t){const y=b.map(R=>Te(_,R)).filter(R=>R!==null);y.length&&g.push([{type:"text",id:m,...l&&{anchor:d},display:y},a])}}}),Dn(r).sort(([,o],[,i])=>(s?Fn:Pn)(o,i)).map(([o,{title:i,contents:c}])=>{if(!i){const a=mn(t,o);a&&(i=a.h)}return{title:i,contents:c.map(([a])=>a)}})},Et=(e,t,n={})=>{const s=Cn(t,e,{fuzzy:.2,maxFuzzy:3,...n}).map(({suggestion:r})=>r);return e.includes(" ")?s:s.filter(r=>!r.includes(" "))},An=hn(dn(JSON.parse("{\"/\":{\"documentCount\":968,\"nextId\":968,\"documentIds\":{\"0\":\"0\",\"1\":\"1\",\"2\":\"1#binary-oracle\",\"3\":\"1#elowen\",\"4\":\"2\",\"5\":\"2#点云-文本\",\"6\":\"2#affogato-arxiv-2025-06\",\"7\":\"2#seqafford-cvpr-2025\",\"8\":\"2#点云-图像\",\"9\":\"2#iagnet-iccv-2023\",\"10\":\"2#点云-文本-图像\",\"11\":\"2#great-cvpr-2025\",\"12\":\"2#lmaffordance3d-cvpr-2025\",\"13\":\"2#_3d-gaussian-splatting-3dgs\",\"14\":\"2#geal-cvpr-2025\",\"15\":\"2#_3daffordsplat-arxiv-2025-04\",\"16\":\"2#iaao-cvpr-2025\",\"17\":\"2#idea\",\"18\":\"3\",\"19\":\"3#引言\",\"20\":\"3#相关工作\",\"21\":\"3#方法\",\"22\":\"3#_1-3d-2d-映射-如图2左侧\",\"23\":\"3#_2-跨模态一致性对齐-如图2右侧、图3\",\"24\":\"3#_3-可供性解码与训练\",\"25\":\"3#_4-数据损坏基准\",\"26\":\"3#_5-个人理解\",\"27\":\"3#局限性\",\"28\":\"4\",\"29\":\"4#摘要\",\"30\":\"4#简介\",\"31\":\"4#相关工作\",\"32\":\"4#方法\",\"33\":\"4#_3-2-multi-head-affordance-chain-of-thought\",\"34\":\"4#fine-tuning-mllm\",\"35\":\"4#object-head-reasoning-几何推理\",\"36\":\"4#affordance-head-reasoning-类比推理\",\"37\":\"4#knowledge-encoding-and-integration\",\"38\":\"4#_3-3-cross-modal-adaptive-fusion-module-cmafm\",\"39\":\"4#_3-4-decoder-and-loss-functions\",\"40\":\"4#数据集\",\"41\":\"4#数据收集-collection\",\"42\":\"4#标注策略-annotation\",\"43\":\"4#统计分析-statistical-analysis\",\"44\":\"4#数据划分-data-partitions\",\"45\":\"4#实验\",\"46\":\"4#_5-1-benchmark-setting\",\"47\":\"4#_5-2-comparison-results\",\"48\":\"4#_5-3-ablation-study\",\"49\":\"4#_5-4-performance-analysis\",\"50\":\"4#结论\",\"51\":\"4#代码\",\"52\":\"4#multi-head-affordance-chain-of-thought\",\"53\":\"4#数据集-1\",\"54\":\"4#模型\",\"55\":\"4#文本编码\",\"56\":\"4#改良的交叉注意力\",\"57\":\"4#几何结构信息与交互信息的融合\",\"58\":\"4#交互信息与图像特征的融合\",\"59\":\"4#解码阶段\",\"60\":\"4#点云特征与几何结构特征的融合\",\"61\":\"5\",\"62\":\"5#环境配置-待完善\",\"63\":\"5#模型结构\",\"64\":\"5#lmaffordance3d\",\"65\":\"5#step-2-融合多模态空间特征\",\"66\":\"5#step-3-多模态特征投影到语言语义空间\",\"67\":\"5#step-6-拼接多模态嵌入与语言嵌入\",\"68\":\"5#step-8-降维适配器\",\"69\":\"5#step-9-解码器融合所有特征以预测可操作性特征\",\"70\":\"5#step-10-使用分割头预测最终的-3d-可操作性热图\",\"71\":\"6\",\"72\":\"6#摘要\",\"73\":\"6#简介\",\"74\":\"6#相关工作\",\"75\":\"6#_1-功能学习-affordance-learning\",\"76\":\"6#_2-图像-点云跨模态学习\",\"77\":\"6#方法\",\"78\":\"6#_1-整体框架-iag网络\",\"79\":\"6#_2-损失函数\",\"80\":\"6#_3-关键创新\",\"81\":\"6#代码\",\"82\":\"6#数据集\",\"83\":\"6#模型\",\"84\":\"7\",\"85\":\"7#数据集\",\"86\":\"7#_1-基础数据来源\",\"87\":\"7#_2-构建问题-question-crafting\",\"88\":\"7#_3-标注-gt-mask-ground-truth-mask\",\"89\":\"7#_4-数据集组织方式\",\"90\":\"7#_5-数据增强与配对策略\",\"91\":\"7#_6-数据集统计信息-来自论文图3\",\"92\":\"7#_7-代码实现\",\"93\":\"7#_8-总结\",\"94\":\"7#模型实现\",\"95\":\"7#afm-自适应融合模块\",\"96\":\"7#_1️⃣-grouping-文本引导的点特征分组\",\"97\":\"7#_2️⃣-mixing-mlp-mixer-进行组内和通道间的信息混合\",\"98\":\"7#_3️⃣-ungrouping-将融合特征映射回点空间\",\"99\":\"7#_4️⃣-afm-自适应融合模块\",\"100\":\"7#rpo-参考点解码器\",\"101\":\"7#损失函数\",\"102\":\"7#hm-loss-hybrid-mask-loss\",\"103\":\"7#训练\",\"104\":\"7#准备\",\"105\":\"7#训练-1\",\"106\":\"7#评估\",\"107\":\"7#复现\",\"108\":\"8\",\"109\":\"8#引言\",\"110\":\"8#相关工作\",\"111\":\"8#方法\",\"112\":\"8#背景\",\"113\":\"8#point-transformer-层\",\"114\":\"8#位置编码\",\"115\":\"8#point-transformer-模块\",\"116\":\"8#网络架构\",\"117\":\"8#消融实验\",\"118\":\"8#代码实现\",\"119\":\"8#向量自注意力层\",\"120\":\"8#point-transformer-残差块\",\"121\":\"8#下采样层\",\"122\":\"8#上采样层\",\"123\":\"8#point-transformer-主模型\",\"124\":\"9\",\"125\":\"9#引言\",\"126\":\"9#方法\",\"127\":\"10\",\"128\":\"11\",\"129\":\"12\",\"130\":\"13\",\"131\":\"13#背景\",\"132\":\"13#模型结构\",\"133\":\"13#层次化点集特征学习\",\"134\":\"13#sampling-layer\",\"135\":\"13#grouping-layer\",\"136\":\"13#pointnet-layer\",\"137\":\"13#代码实现\",\"138\":\"13#单尺度分组分类模型\",\"139\":\"13#非均匀密度下稳定的特征学习\",\"140\":\"13#多尺度分组-multi-scale-grouping\",\"141\":\"13#多尺度分组分类模型\",\"142\":\"13#多分辨率分组-multi-resolution-grouping\",\"143\":\"13#点云语义分割\",\"144\":\"13#代码实现-1\",\"145\":\"13#特征传播层\",\"146\":\"13#点云语义分割模型\",\"147\":\"14\",\"148\":\"14#核心\",\"149\":\"14#难点\",\"150\":\"14#解决方案\",\"151\":\"14#代码-pytorch版本\",\"152\":\"14#输入标准化\",\"153\":\"14#正则化损失\",\"154\":\"14#特征提取\",\"155\":\"14#分类任务\",\"156\":\"14#分割任务\",\"157\":\"14#缺陷\",\"158\":\"14#背景知识扫盲-可选\",\"159\":\"14#点云\",\"160\":\"14#对称函数\",\"161\":\"14#刚性运动\",\"162\":\"14#正交变换\",\"163\":\"15\",\"164\":\"16\",\"165\":\"16#introduction\",\"166\":\"16#related-work\",\"167\":\"16#视觉-语言预训练-vlp\",\"168\":\"16#知识蒸馏-knowledge-distillation\",\"169\":\"16#数据增强-data-augmentation\",\"170\":\"16#method\",\"171\":\"16#模型架构-med-multimodal-mixture-of-encoder-decoder\",\"172\":\"16#预训练目标-itc、itm、lm\",\"173\":\"16#capfilt-图文数据的自举式清洗机制\",\"174\":\"16#小结\",\"175\":\"16#experiments-and-discussions\",\"176\":\"16#预训练细节\",\"177\":\"16#capfilt-效果验证\",\"178\":\"16#合成文本的多样性对性能的影响\",\"179\":\"16#编码器-解码器参数共享与解耦\",\"180\":\"16#ablation-study\",\"181\":\"16#capfilt-的性能提升并非源于更长的训练时间\",\"182\":\"16#应使用-bootstrapped-数据集重新训练模型\",\"183\":\"16#conclusion\",\"184\":\"16#code-implementation\",\"185\":\"16#capfilt-模块实现\",\"186\":\"16#captioner-模块\",\"187\":\"16#微调阶段\",\"188\":\"16#生成阶段\",\"189\":\"16#filter-模块\",\"190\":\"16#微调阶段-1\",\"191\":\"16#过滤阶段\",\"192\":\"16#blip-预训练\",\"193\":\"17\",\"194\":\"17#introduction\",\"195\":\"17#related-work\",\"196\":\"17#albef\",\"197\":\"17#model-structure\",\"198\":\"17#pre-training-objectives\",\"199\":\"17#image-text-contrastive-learning\",\"200\":\"17#masked-language-modeling-mlm\",\"201\":\"17#image-text-matching-itm\",\"202\":\"17#momentum-distillation\",\"203\":\"17#code-implementation\",\"204\":\"17#train\",\"205\":\"17#model-init\",\"206\":\"17#itc\",\"207\":\"17#itm\",\"208\":\"17#mlm\",\"209\":\"18\",\"210\":\"18#引言\",\"211\":\"18#方法\",\"212\":\"18#预训练阶段一-向量量化知识蒸馏算法用于d-vae预训练\",\"213\":\"18#代码实现\",\"214\":\"18#预训练阶段二-掩码图像建模学习目标用于beit预训练\",\"215\":\"18#实验效果\",\"216\":\"18#相关工作\",\"217\":\"18#总结\",\"218\":\"18#补充\",\"219\":\"19\",\"220\":\"19#引言\",\"221\":\"19#方法\",\"222\":\"19#骨干网络-multiway-transformers\",\"223\":\"19#预训练任务-掩码数据建模\",\"224\":\"19#模型与预训练规模化\",\"225\":\"19#总结\",\"226\":\"20\",\"227\":\"20#摘要\",\"228\":\"20#简介\",\"229\":\"20#方法\",\"230\":\"20#图像表示\",\"231\":\"20#图像-patch\",\"232\":\"20#视觉-token\",\"233\":\"20#主干网络-图像-transformer\",\"234\":\"20#beit-的预训练-掩码图像建模-masked-image-modeling\",\"235\":\"20#从变分自编码器的视角\",\"236\":\"20#预训练设置-pre-training-setup\",\"237\":\"20#在下游视觉任务上微调-beit\",\"238\":\"20#图像分类-image-classification\",\"239\":\"20#语义分割-semantic-segmentation\",\"240\":\"20#中间微调-intermediate-fine-tuning\",\"241\":\"20#实验\",\"242\":\"20#消融实验-ablation-studies\",\"243\":\"20#自注意力图的分析\",\"244\":\"20#相关工作\",\"245\":\"20#自监督视觉预训练\",\"246\":\"20#_1-对比学习-contrastive-learning\",\"247\":\"20#_2-生成式预训练-generative-pretraining\",\"248\":\"20#_3-目标预训练-pretext-task\",\"249\":\"20#离散表示学习-discrete-representation-learning\",\"250\":\"20#bert-式的预训练方法-bert-style-pretraining\",\"251\":\"20#多模态预训练-multimodal-pretraining\",\"252\":\"20#结论\",\"253\":\"21\",\"254\":\"21#dvae-预训练\",\"255\":\"21#discretevae-初始化\",\"256\":\"21#discretevae-前向传播\",\"257\":\"21#gumbel-softmax\",\"258\":\"21#hard-true时-如何实现的\",\"259\":\"21#smooth-l1-loss\",\"260\":\"21#kl散度计算\",\"261\":\"21#log-target-参数\",\"262\":\"21#为什么先验分布设置为均匀分布\",\"263\":\"21#块状遮挡-blockwise-masking-策略\",\"264\":\"21#数据集加载\",\"265\":\"21#beit主模型预训练\",\"266\":\"21#主模型代码实现\",\"267\":\"22\",\"268\":\"22#引言\",\"269\":\"22#相关工作\",\"270\":\"22#方法\",\"271\":\"22#自然语言监督\",\"272\":\"22#对比描述预训练-contrastive-captioners-pretraining\",\"273\":\"22#coca-在下游任务中的应用\",\"274\":\"22#代码实现\",\"275\":\"22#消融实验\",\"276\":\"22#训练目标和损失\",\"277\":\"22#解码器设计和文本嵌入\",\"278\":\"22#注意力池化器设计\",\"279\":\"23\",\"280\":\"23#引言\",\"281\":\"23#相关工作\",\"282\":\"23#自监督学习的发展路径\",\"283\":\"23#自训练与知识蒸馏的联系\",\"284\":\"23#方法\",\"285\":\"23#基于知识蒸馏的自监督学习\",\"286\":\"23#实现细节\",\"287\":\"23#消融实验\",\"288\":\"23#patch大小的重要性\",\"289\":\"23#教师网络的选择\",\"290\":\"23#避免崩溃\",\"291\":\"23#计算需求\",\"292\":\"23#小批量训练\",\"293\":\"23#代码解析\",\"294\":\"24\",\"295\":\"24#摘要\",\"296\":\"24#简介\",\"297\":\"24#相关工作\",\"298\":\"24#_1-视觉基础模型-vision-foundation-models\",\"299\":\"24#_2-大语言模型-large-language-models-llms\",\"300\":\"24#_3-视觉大语言模型-vision-large-language-models-vllms\",\"301\":\"24#核心问题与本文定位\",\"302\":\"24#方法\",\"303\":\"24#_1-整体架构设计\",\"304\":\"24#_2-模型设计\",\"305\":\"24#_3-对齐策略\",\"306\":\"24#实现细节\",\"307\":\"24#实验\",\"308\":\"24#视觉感知能力验证-visual-perception-benchmarks\",\"309\":\"24#视觉-语言任务能力-vision-language-benchmarks\",\"310\":\"24#多模态对话任务-multi-modal-dialogue-benchmarks\",\"311\":\"24#消融实验-ablation-study\",\"312\":\"24#总结\",\"313\":\"24#结论\",\"314\":\"24#详细训练设置-附录内容\",\"315\":\"24#第一阶段设置-stage-1\",\"316\":\"24#第二阶段设置-stage-2\",\"317\":\"24#第三阶段设置-stage-3\",\"318\":\"24#检索任务微调设置\",\"319\":\"24#imagenet-线性探测设置\",\"320\":\"24#ade20k-语义分割设置\",\"321\":\"25\",\"322\":\"25#摘要\",\"323\":\"25#简介\",\"324\":\"25#相关工作\",\"325\":\"25#_1-商业专有多模态大模型-proprietary-commercial-mllms\",\"326\":\"25#_2-开源多模态大模型-open-source-mllms\",\"327\":\"25#_3-视觉基础模型-vision-foundation-models-vfms\",\"328\":\"25#方法\",\"329\":\"25#_1-整体架构\",\"330\":\"25#_2-强视觉编码器\",\"331\":\"25#_3-动态高分辨率策略\",\"332\":\"25#_4-高质量双语数据集\",\"333\":\"25#实验\",\"334\":\"25#_1-实现细节\",\"335\":\"25#_2-基准测试结果\",\"336\":\"25#_3-关键消融研究\",\"337\":\"25#结论\",\"338\":\"26\",\"339\":\"26#背景\",\"340\":\"26#方法\",\"341\":\"26#预训练\",\"342\":\"26#微调\",\"343\":\"26#联合-gpt-4-的推理机制-ensemble-with-gpt-4\",\"344\":\"26#ablation-study-消融实验\",\"345\":\"26#补充\",\"346\":\"26#辨析-instruction-tuning-和-prompt-tuning\",\"347\":\"27\",\"348\":\"27#introduction\",\"349\":\"27#what-is-contrast-learning\",\"350\":\"27#instance-discrimination-task\",\"351\":\"27#momentum-contrast\",\"352\":\"27#abstract\",\"353\":\"27#introduction-1\",\"354\":\"27#conclusion\",\"355\":\"27#related-work\",\"356\":\"27#detail\",\"357\":\"27#preview-work\",\"358\":\"27#code-implementation\",\"359\":\"27#train-code\",\"360\":\"27#model-implementation\",\"361\":\"27#model-init\",\"362\":\"27#model-forward\",\"363\":\"27#momentum-update\",\"364\":\"27#dequeue-and-enqueue\",\"365\":\"28\",\"366\":\"29\",\"367\":\"30\",\"368\":\"30#introduction\",\"369\":\"30#related-work\",\"370\":\"30#method\",\"371\":\"30#输入表示\",\"372\":\"30#多模态专家混合-transformer-mome-transformer\",\"373\":\"30#预训练任务\",\"374\":\"30#分阶段预训练-stagewise-pre-training\",\"375\":\"30#下游任务微调\",\"376\":\"30#ablation-studies\",\"377\":\"30#conclusion\",\"378\":\"31\",\"379\":\"31#前置知识\",\"380\":\"31#mome-mixture-of-multimodal-experts-transformer\",\"381\":\"31#vlmo\",\"382\":\"31#数据模块\",\"383\":\"31#模型实现\",\"384\":\"31#masked-language-modeling\",\"385\":\"31#contrastive-loss-for-pretraining\",\"386\":\"31#image-text-matching\",\"387\":\"32\",\"388\":\"32#introduction\",\"389\":\"32#motivation\",\"390\":\"32#method\",\"391\":\"32#modality-interaction-schema\",\"392\":\"32#model-structure\",\"393\":\"32#pretraining-objectives\",\"394\":\"32#conclusion\",\"395\":\"33\",\"396\":\"33#多模态-bert-前向传播流程\",\"397\":\"33#_1-整体流程总览-bertmodel\",\"398\":\"33#_2-编码器-bertencoder\",\"399\":\"33#_3-transformer-层-bertlayer\",\"400\":\"33#_4-attention-模块-bertattention\",\"401\":\"33#_5-核心计算-bertselfattention\",\"402\":\"33#_6-小结\",\"403\":\"33#自回归语言建模\",\"404\":\"34\",\"405\":\"34#引言\",\"406\":\"34#介绍\",\"407\":\"34#训练\",\"408\":\"34#推理\",\"409\":\"34#文本描述生成\",\"410\":\"34#花卉图片分类\",\"411\":\"34#文字搜索图像\",\"412\":\"34#完整代码\",\"413\":\"34#小结\",\"414\":\"35\",\"415\":\"35#背景\",\"416\":\"35#模型结构\",\"417\":\"35#stage-1-representation-learning-表征学习\",\"418\":\"35#_1、image-text-contrastive-learning-itc-loss-clip-like\",\"419\":\"35#_2、image-text-matching-itm-loss-二分类task\",\"420\":\"35#_3、image-grounded-text-generation-itg-loss-gpt-like\",\"421\":\"35#stage-2-generative-learning-生成学习\",\"422\":\"36\",\"423\":\"36#原理\",\"424\":\"36#_0-数据下载\",\"425\":\"36#_1-图片预处理\",\"426\":\"36#_2-图片切割\",\"427\":\"36#_3-添加-class-token\",\"428\":\"36#_4-添加位置编码\",\"429\":\"36#_5-encoder\",\"430\":\"36#_6-多头自注意力\",\"431\":\"36#_7-mlp-head\",\"432\":\"36#效果对比\",\"433\":\"36#注意力可视化\",\"434\":\"36#混合模型探索\",\"435\":\"36#加载预训练模型\",\"436\":\"36#总结\",\"437\":\"37\",\"438\":\"38\",\"439\":\"38#numpy\",\"440\":\"38#np-linspace\",\"441\":\"38#np-concatenate\",\"442\":\"39\",\"443\":\"39#python\",\"444\":\"39#作用域\",\"445\":\"39#位置参数与关键字参数\",\"446\":\"39#闭包与高阶导数\",\"447\":\"39#什么是高阶函数\",\"448\":\"39#什么是闭包\",\"449\":\"39#装饰器的实现用到了什么\",\"450\":\"39#装饰器\",\"451\":\"39#最基本的函数装饰器\",\"452\":\"39#带参数的函数装饰器\",\"453\":\"39#带参数的装饰器-装饰器工厂\",\"454\":\"39#使用-functools-wraps-保留原函数元信息\",\"455\":\"39#装饰类方法-普通方法-类方法-静态方法\",\"456\":\"39#装饰整个类\",\"457\":\"39#装饰器的底层原理与执行过程\",\"458\":\"39#多个装饰器叠加时的执行顺序-从内到外\",\"459\":\"39#类装饰器\",\"460\":\"39#总结\",\"461\":\"39#典型应用场景举例\",\"462\":\"39#地板除\",\"463\":\"39#ellipsis\",\"464\":\"40\",\"465\":\"40#pytorch\",\"466\":\"40#stack\",\"467\":\"40#transpose\",\"468\":\"40#permute\",\"469\":\"40#view\",\"470\":\"40#reshape\",\"471\":\"40#repeat\",\"472\":\"40#expand\",\"473\":\"40#torch-no-grad\",\"474\":\"40#register-buffer\",\"475\":\"40#einsum\",\"476\":\"40#where\",\"477\":\"40#torch-nn-functional-pad\",\"478\":\"40#rearrange\",\"479\":\"40#tensor-uniform\",\"480\":\"40#torch-unique-consecutive\",\"481\":\"40#torch-cumsum\",\"482\":\"40#torch-tensor-的-chunk-方法\",\"483\":\"40#torch-randperm\",\"484\":\"40#torch-randint\",\"485\":\"40#torch-bincount\",\"486\":\"40#tensor-new-zeros\",\"487\":\"40#tensor-scatter-add\",\"488\":\"40#torch-topk\",\"489\":\"40#连续性\",\"490\":\"40#tensor-is-contiguous\",\"491\":\"40#tensor-contiguous\",\"492\":\"40#为什么需要-contiguous\",\"493\":\"40#归一化层对连续性的要求\",\"494\":\"40#总结\",\"495\":\"41\",\"496\":\"41#模型\",\"497\":\"41#resnet18\",\"498\":\"41#bert\",\"499\":\"41#公式-定理\",\"500\":\"41#通用近似定理\",\"501\":\"41#roi-pooling\",\"502\":\"41#roi-align\",\"503\":\"41#上采样\",\"504\":\"41#最近邻-nearest-插值\",\"505\":\"41#双线性-bilinear-插值\",\"506\":\"41#余弦相似度\",\"507\":\"41#l1-归一化-l1-normalization\",\"508\":\"41#l2-归一化-l2-normalization\",\"509\":\"42\",\"510\":\"42#timm-库\",\"511\":\"42#create-model-与-register-model-装饰器\",\"512\":\"42#scikit-learn-库\",\"513\":\"42#train-test-split\",\"514\":\"42#compute-class-weight\",\"515\":\"42#python-内置-collections-库\",\"516\":\"42#counter\",\"517\":\"42#pytorch-内置-采样库\",\"518\":\"42#weightedrandomsampler\",\"519\":\"43\",\"520\":\"43#os-environ-cuda-visible-devices\",\"521\":\"43#随机数种子与确定性\",\"522\":\"43#偏置与归一化层的关系\",\"523\":\"44\",\"524\":\"44#一、注意力机制的基本流程\",\"525\":\"44#二、q、k、v-的初始维度对结果的影响\",\"526\":\"44#_1-q-×-k-t-的维度\",\"527\":\"44#_2-softmax-操作\",\"528\":\"44#_3-与-v-相乘\",\"529\":\"44#三、总结-输入维度-→-输出维度\",\"530\":\"44#四、如何理解这个过程\",\"531\":\"44#✅-1-信息融合机制\",\"532\":\"44#✅-2-维度设计的灵活性\",\"533\":\"44#✅-3-可类比为-软检索-系统\",\"534\":\"44#五、例子说明-以-transformer-为例\",\"535\":\"44#六、常见疑问解答\",\"536\":\"44#❓q-为什么和可以不同\",\"537\":\"44#❓q-为什么要除以\",\"538\":\"44#七、可视化示意\",\"539\":\"45\",\"540\":\"45#引言\",\"541\":\"45#连续性\",\"542\":\"45#步长\",\"543\":\"45#张量变换操作\",\"544\":\"45#切片-slice\",\"545\":\"45#转置-transpose\",\"546\":\"45#广播-broadcast\",\"547\":\"45#维度问题\",\"548\":\"46\",\"549\":\"47\",\"550\":\"47#一、创建新环境\",\"551\":\"47#二、激活-切换-环境\",\"552\":\"47#三、退出当前环境\",\"553\":\"47#四、查看所有已创建的环境\",\"554\":\"47#五、删除已创建的环境\",\"555\":\"47#六、查看当前激活的环境\",\"556\":\"47#七、查看当前环境已安装的包\",\"557\":\"47#八、在当前环境下安装包\",\"558\":\"47#九、常见错误\",\"559\":\"48\",\"560\":\"48#二元分类场景\",\"561\":\"48#混淆矩阵-confusion-matrix\",\"562\":\"48#准确率-accuracy\",\"563\":\"48#召回率-recall-真正例率\",\"564\":\"48#误报概率-假正例率\",\"565\":\"48#精确率\",\"566\":\"48#指标的选择和权衡\",\"567\":\"48#f1-得分\",\"568\":\"48#roc-曲线和-auc\",\"569\":\"48#roc-receiver-operating-characteristic\",\"570\":\"48#auc-曲线下面积\",\"571\":\"48#精确率与召回率曲线\",\"572\":\"48#用于选择模型和阈值的-auc-和-roc\",\"573\":\"49\",\"574\":\"49#协方差矩阵\",\"575\":\"49#马氏距离\",\"576\":\"49#欧几里得距离-euclidean-distance\",\"577\":\"49#马氏距离-mahalanobis-distance\",\"578\":\"49#尺度差异性\",\"579\":\"49#总结\",\"580\":\"50\",\"581\":\"50#注意力图可视化\",\"582\":\"50#vit-模型\",\"583\":\"51\",\"584\":\"51#语义分割\",\"585\":\"51#损失函数\",\"586\":\"51#dice-loss\",\"587\":\"51#bce-dice-loss\",\"588\":\"51#jaccard-intersection-over-union-iou-loss\",\"589\":\"51#focal-loss\",\"590\":\"51#tversky-loss\",\"591\":\"51#lovasz-hinge-loss\",\"592\":\"51#combo-loss\",\"593\":\"51#如何选择\",\"594\":\"52\",\"595\":\"52#预训练过程\",\"596\":\"52#分词过程\",\"597\":\"52#附录\",\"598\":\"53\",\"599\":\"54\",\"600\":\"54#什么是大模型\",\"601\":\"54#为什么要对大模型进行微调\",\"602\":\"54#如何对大模型进行微调\",\"603\":\"54#常用的peft方案\",\"604\":\"54#prompt-tuning\",\"605\":\"54#prefix-tuning\",\"606\":\"54#lora\",\"607\":\"54#qlora\",\"608\":\"55\",\"609\":\"55#符合认知的大模型微调流程\",\"610\":\"55#大模型微调大致发展历史\",\"611\":\"55#lora-微调\",\"612\":\"55#矩阵a和b为什么不能同时为零\",\"613\":\"55#秩的选择\",\"614\":\"55#注意\",\"615\":\"56\",\"616\":\"56#什么是prompt-engineering\",\"617\":\"56#如何写好prompt\",\"618\":\"56#要明确-要具体\",\"619\":\"56#给llm更多的时间去思考\",\"620\":\"56#思维链技术-chain-of-thought\",\"621\":\"56#自一致性技术-self-consistency\",\"622\":\"56#从易至难技术-least-to-most\",\"623\":\"57\",\"624\":\"58\",\"625\":\"58#摘要\",\"626\":\"58#简介\",\"627\":\"58#相关工作\",\"628\":\"58#框架\",\"629\":\"58#无监督预训练\",\"630\":\"58#有监督微调\",\"631\":\"58#特定任务输入转换\",\"632\":\"58#实验\",\"633\":\"58#设置\",\"634\":\"58#监督微调\",\"635\":\"58#分析\",\"636\":\"58#结论\",\"637\":\"59\",\"638\":\"59#摘要\",\"639\":\"59#简介\",\"640\":\"59#方法\",\"641\":\"59#实验\",\"642\":\"59#讨论\",\"643\":\"59#总结\",\"644\":\"60\",\"645\":\"60#摘要\",\"646\":\"60#简介\",\"647\":\"60#方法\",\"648\":\"60#结果\",\"649\":\"60#局限性\",\"650\":\"60#相关工作\",\"651\":\"60#结论\",\"652\":\"61\",\"653\":\"61#摘要\",\"654\":\"61#简介\",\"655\":\"61#相关工作\",\"656\":\"61#方法\",\"657\":\"61#结果\",\"658\":\"61#讨论\",\"659\":\"62\",\"660\":\"62#why-we-need-kv-cache\",\"661\":\"62#self-attention-without-cache\",\"662\":\"62#self-attention-with-cache\",\"663\":\"62#huggingface-官方代码实现\",\"664\":\"63\",\"665\":\"63#摘要\",\"666\":\"63#简介\",\"667\":\"63#方法\",\"668\":\"63#结果\",\"669\":\"63#指令微调\",\"670\":\"63#bias-toxicity-and-misinformation\",\"671\":\"63#相关工作\",\"672\":\"63#总结\",\"673\":\"64\",\"674\":\"64#摘要\",\"675\":\"65\",\"676\":\"66\",\"677\":\"66#摘要\",\"678\":\"66#引言\",\"679\":\"66#背景\",\"680\":\"66#实验步骤\",\"681\":\"66#训练步骤分析\",\"682\":\"66#roberta核心改进总结\",\"683\":\"66#_1-训练策略优化\",\"684\":\"66#_2-数据规模与训练时长\",\"685\":\"66#_3-性能表现-关键结果\",\"686\":\"66#_4-结论与启示\",\"687\":\"66#相关工作\",\"688\":\"66#总结\",\"689\":\"67\",\"690\":\"67#bert-是什么\",\"691\":\"67#masked-language-model\",\"692\":\"67#next-sentence-prediction\",\"693\":\"67#multi-task-learning\",\"694\":\"67#fine-tuning\",\"695\":\"67#从-零-开始的预训练\",\"696\":\"67#数据清洗\",\"697\":\"67#分词器实现\",\"698\":\"67#batch数据准备\",\"699\":\"67#模型\",\"700\":\"67#训练\",\"701\":\"67#效果\",\"702\":\"67#details\",\"703\":\"67#padding-mask-如何生成并起作用的\",\"704\":\"68\",\"705\":\"68#绝对位置编码-absolute-positional-encoding-ape\",\"706\":\"68#正弦-余弦位置编码-sinusoidal-positional-encoding\",\"707\":\"68#基于可学习的嵌入\",\"708\":\"68#相对位置编码-relative-position-encoding-rpe\",\"709\":\"68#relative-position-representations-shaw-et-al-2018\",\"710\":\"68#t5-相对位置偏置-relative-position-bias-rpb\",\"711\":\"69\",\"712\":\"69#环境搭建\",\"713\":\"69#数据预处理\",\"714\":\"69#模型架构\",\"715\":\"69#dataloader\",\"716\":\"69#bertembeddings\",\"717\":\"69#bertencoder\",\"718\":\"69#bertlayer\",\"719\":\"69#bertencoder-1\",\"720\":\"69#bertpooler\",\"721\":\"69#bertmodel\",\"722\":\"69#bertforsequenceclassification\",\"723\":\"69#bertattention\",\"724\":\"69#bertselfattention\",\"725\":\"69#bertselfoutput\",\"726\":\"69#bertattention-1\",\"727\":\"69#预训练\",\"728\":\"69#bertpredictionheadtransform\",\"729\":\"69#bertlmpredictionhead\",\"730\":\"69#bertpretrainingheads\",\"731\":\"69#bertforpretraining\",\"732\":\"69#其他下游任务\",\"733\":\"69#问答任务\",\"734\":\"69#代码实现\",\"735\":\"69#易混淆\",\"736\":\"69#token分类任务\",\"737\":\"69#多项选择任务\",\"738\":\"70\",\"739\":\"70#环境\",\"740\":\"70#背景\",\"741\":\"70#模型架构\",\"742\":\"70#encoder-decoder-结构\",\"743\":\"70#generator\",\"744\":\"70#encoder-结构\",\"745\":\"70#sublayerconnection\",\"746\":\"70#encoderlayer\",\"747\":\"70#encoder\",\"748\":\"70#decoder-结构\",\"749\":\"70#decoderlayer\",\"750\":\"70#decoder\",\"751\":\"70#多头自注意力\",\"752\":\"71\",\"753\":\"71#引言-揭开深度学习框架的神秘面纱\",\"754\":\"71#步骤1-作为-箱子-的变量\",\"755\":\"71#变量的基本概念\",\"756\":\"71#代码实现\",\"757\":\"71#使用示例\",\"758\":\"71#关键要点\",\"759\":\"71#步骤2-创建变量的函数\",\"760\":\"71#函数与计算图\",\"761\":\"71#函数类的设计\",\"762\":\"71#代码实现-1\",\"763\":\"71#辅助函数\",\"764\":\"71#步骤3-函数的连续调用\",\"765\":\"71#复合函数的计算\",\"766\":\"71#代码示例\",\"767\":\"71#计算图的意义\",\"768\":\"71#步骤4-数值微分\",\"769\":\"71#导数的定义\",\"770\":\"71#数值微分的实现\",\"771\":\"71#代码实现-2\",\"772\":\"71#数值微分的问题\",\"773\":\"71#步骤5-反向传播的理论知识\",\"774\":\"71#链式法则\",\"775\":\"71#反向传播的方向\",\"776\":\"71#计算图的反向传播\",\"777\":\"71#步骤6-手动进行反向传播\",\"778\":\"71#扩展variable类\",\"779\":\"71#扩展function类\",\"780\":\"71#具体函数的反向传播\",\"781\":\"71#反向传播的执行\",\"782\":\"71#步骤7-反向传播的自动化\",\"783\":\"71#建立变量与函数的连接\",\"784\":\"71#自动反向传播的实现\",\"785\":\"71#步骤8-从递归到循环\",\"786\":\"71#递归实现的问题\",\"787\":\"71#循环实现反向传播\",\"788\":\"71#循环实现的优势\",\"789\":\"71#步骤9-让函数更易用\",\"790\":\"71#函数的python化\",\"791\":\"71#自动设置梯度\",\"792\":\"71#数据类型检查\",\"793\":\"71#步骤10-测试\",\"794\":\"71#单元测试\",\"795\":\"71#梯度检验\",\"796\":\"71#测试的重要性\",\"797\":\"71#第一阶段总结\",\"798\":\"72\",\"799\":\"72#引言-从自动微分迈向通用框架\",\"800\":\"72#步骤11-多输入与多输出\",\"801\":\"72#步骤12-backward-的多输入实现\",\"802\":\"72#步骤13-重置导数\",\"803\":\"72#步骤14-共享变量与梯度累加\",\"804\":\"72#步骤15-梯度重复累加的问题\",\"805\":\"72#步骤16-辈分-机制\",\"806\":\"72#步骤17-循环引用与内存释放\",\"807\":\"72#步骤18-优化内存消耗\",\"808\":\"72#步骤19-variable-功能增强\",\"809\":\"72#步骤20–22-运算符重载\",\"810\":\"72#步骤23-项目模块化结构\",\"811\":\"72#步骤24-复杂函数的求导\",\"812\":\"72#第二阶段总结\",\"813\":\"73\",\"814\":\"73#引言-从自动微分走向-可视化-高阶导数-灵活控制\",\"815\":\"73#步骤25-可视化计算图\",\"816\":\"73#步骤26-寻找函数最优解\",\"817\":\"73#步骤27-高阶导数\",\"818\":\"74\",\"819\":\"74#引言-从自动微分迈向可训练的神经网络模型\",\"820\":\"75\",\"821\":\"76\",\"822\":\"76#大语言模型\",\"823\":\"76#常见的llm\",\"824\":\"76#llm-的特点与能力\",\"825\":\"76#涌现能力-emergent-abilities\",\"826\":\"76#作为基座模型支持多元应用的能力\",\"827\":\"76#支持对话作为统一入口的能力\",\"828\":\"76#检索增强生成-rag-retrieval-augmented-generation\",\"829\":\"76#工作流程\",\"830\":\"76#rag-vs-finetune\",\"831\":\"76#langchain\",\"832\":\"76#核心组件\",\"833\":\"76#版本迭代\",\"834\":\"76#生态圈\",\"835\":\"76#大模型开发\",\"836\":\"76#基本流程\",\"837\":\"76#参考\",\"838\":\"77\",\"839\":\"78\",\"840\":\"79\",\"841\":\"80\",\"842\":\"81\",\"843\":\"82\",\"844\":\"83\",\"845\":\"83#概率空间\",\"846\":\"83#离散随机变量\",\"847\":\"83#连续随机变量\",\"848\":\"83#概率公理\",\"849\":\"83#条件概率\",\"850\":\"83#全概率公式-law-of-total-probability\",\"851\":\"83#贝叶斯法则\",\"852\":\"83#离散随机变量形式\",\"853\":\"83#连续随机变量形式\",\"854\":\"83#一些常见的概率分布\",\"855\":\"83#离散分布\",\"856\":\"83#伯努利分布与二项分布-bernoulli-and-binomial-distributions\",\"857\":\"83#分类分布与多项分布-categorical-and-multinomial-distributions\",\"858\":\"83#泊松分布-poisson-distribution\",\"859\":\"83#负二项分布-negative-binomial-distribution\",\"860\":\"83#转换视角-定义失败为红球-成功为蓝球\",\"861\":\"83#特殊情况说明\",\"862\":\"83#数学期望与方差\",\"863\":\"83#负二项分布的意义与优势\",\"864\":\"83#定义在实数上的连续分布\",\"865\":\"83#高斯分布-正态分布\",\"866\":\"83#半正态分布-half-normal\",\"867\":\"83#学生-t-分布-student-t-distribution\",\"868\":\"83#柯西分布-cauchy-distribution\",\"869\":\"83#高斯联合分布-gaussian-joint-distributions\",\"870\":\"83#多元正态分布-the-multivariate-normal\",\"871\":\"83#定义-definition\",\"872\":\"83#高斯壳-gaussian-shells\",\"873\":\"83#直观解释如下\",\"874\":\"83#数学解释-为什么高斯样本集中在壳层上\",\"875\":\"83#图像空间的含义-例如灰度图像\",\"876\":\"84\",\"877\":\"84#bayes-rule\",\"878\":\"84#inverse-problems\",\"879\":\"85\",\"880\":\"85#计数法则\",\"881\":\"85#排列-考虑元素之间的顺序\",\"882\":\"85#组合-不考虑元素之间的顺序\",\"883\":\"86\",\"884\":\"86#引言\",\"885\":\"86#方法\",\"886\":\"86#阶段一-学习视觉码本-visual-codebook\",\"887\":\"86#阶段二-学习先验\",\"888\":\"86#数据收集\",\"889\":\"86#采样生成\",\"890\":\"87\",\"891\":\"87#代码实现\",\"892\":\"87#模型初始化\",\"893\":\"87#前向传播流程\",\"894\":\"87#classifier-free-guidance-无条件引导技术\",\"895\":\"87#推理过程-图文联合生成图像\",\"896\":\"87#top-k-采样\",\"897\":\"87#gumbel-sampling\",\"898\":\"87#语言建模能力-的回溯性验证\",\"899\":\"87#discretevae-离散化变分自编码器\",\"900\":\"87#生成质量判别器-clip\",\"901\":\"88\",\"902\":\"88#前置知识\",\"903\":\"88#最大似然估计-maximum-likelihood-estimation-mle\",\"904\":\"88#数学推导\",\"905\":\"88#信息论-信息量-熵-交叉熵-kl散度\",\"906\":\"88#_1-信息量-self-information\",\"907\":\"88#_2-熵-entropy\",\"908\":\"88#_3-交叉熵-cross-entropy\",\"909\":\"88#_4-kl-散度-信息增益\",\"910\":\"88#交叉熵损失-cross-entropy-loss\",\"911\":\"88#js散度-jensen-shannon-divergence\",\"912\":\"88#_1-js散度是什么-浅层直观\",\"913\":\"88#_2-为什么要用js散度而不是kl散度\",\"914\":\"88#_3-js散度的数学定义\",\"915\":\"88#_4-直观理解js散度\",\"916\":\"88#_5-举个简单例子\",\"917\":\"88#_1-lipschitz-函数\",\"918\":\"88#原始-gan\",\"919\":\"88#推荐资料\",\"920\":\"89\",\"921\":\"89#引言\",\"922\":\"89#掩码卷积\",\"923\":\"89#空间掩码\",\"924\":\"89#通道掩码\",\"925\":\"89#思考\",\"926\":\"89#minist-数据集上的实战测试\",\"927\":\"89#摘录\",\"928\":\"90\",\"929\":\"90#实现vae\",\"930\":\"90#_1-安装和导入依赖\",\"931\":\"90#_2-定义-vae-模型\",\"932\":\"90#_3-定义损失函数-重构损失-kl散度\",\"933\":\"90#_4-数据加载\",\"934\":\"90#_5-训练模型\",\"935\":\"90#_6-模型评估\",\"936\":\"90#cvae-实现\",\"937\":\"90#_2-定义-cvae-模型\",\"938\":\"90#_5-训练过程\",\"939\":\"90#_6-条件生成图像-指定标签\",\"940\":\"91\",\"941\":\"92\",\"942\":\"92#引言\",\"943\":\"92#预备知识-潜变量模型\",\"944\":\"92#变分自编码器\",\"945\":\"92#构建目标函数\",\"946\":\"92#优化目标函数\",\"947\":\"92#测试已学习的模型\",\"948\":\"92#解读目标函数\",\"949\":\"92#项引入的误差分析\",\"950\":\"92#信息论视角\",\"951\":\"92#vae-与正则化参数\",\"952\":\"92#条件变分自编码器\",\"953\":\"92#推荐阅读\",\"954\":\"93\",\"955\":\"93#引言\",\"956\":\"93#从-ae-到-vq-vae\",\"957\":\"93#vq-vae-实现细节\",\"958\":\"93#输出离散编码\",\"959\":\"93#优化编码器和解码器\",\"960\":\"93#优化嵌入空间\",\"961\":\"93#总结\",\"962\":\"93#代码实现\",\"963\":\"93#训练阶段\",\"964\":\"93#生成阶段\",\"965\":\"93#转载\",\"966\":\"94\",\"967\":\"95\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1,7],\"1\":[1],\"2\":[2,16],\"3\":[1,5],\"4\":[4,4],\"5\":[3],\"6\":[1,35],\"7\":[1,59],\"8\":[3],\"9\":[1,4],\"10\":[4],\"11\":[1,82],\"12\":[1,63],\"13\":[5],\"14\":[1,61],\"15\":[1],\"16\":[1],\"17\":[1,9],\"18\":[2,11],\"19\":[1,113],\"20\":[1,66],\"21\":[1,12],\"22\":[6,42],\"23\":[5,40],\"24\":[2,30],\"25\":[2,29],\"26\":[2,52],\"27\":[1,16],\"28\":[2,35],\"29\":[1,25],\"30\":[1,83],\"31\":[1,71],\"32\":[1,48],\"33\":[8],\"34\":[3,22],\"35\":[5,16],\"36\":[5,19],\"37\":[4,15],\"38\":[8,27],\"39\":[6,24],\"40\":[1,16],\"41\":[3,40],\"42\":[3,12],\"43\":[4,18],\"44\":[4,15],\"45\":[1,8],\"46\":[4,59],\"47\":[4,45],\"48\":[4,48],\"49\":[4,22],\"50\":[1,49],\"51\":[1],\"52\":[6,202],\"53\":[1,242],\"54\":[1,73],\"55\":[1,34],\"56\":[1,69],\"57\":[1,40],\"58\":[1,54],\"59\":[1,100],\"60\":[1,75],\"61\":[2,29],\"62\":[3,4],\"63\":[1,1],\"64\":[1,223],\"65\":[3,92],\"66\":[3,25],\"67\":[3,114],\"68\":[3,20],\"69\":[3,142],\"70\":[5,153],\"71\":[2,27],\"72\":[1,29],\"73\":[1,73],\"74\":[1],\"75\":[1,31],\"76\":[1,19],\"77\":[1],\"78\":[1,35],\"79\":[1,15],\"80\":[1,14],\"81\":[1],\"82\":[1,152],\"83\":[1,403],\"84\":[2,31],\"85\":[1],\"86\":[2,25],\"87\":[5,72],\"88\":[7,26],\"89\":[2,54],\"90\":[2,14],\"91\":[4,39],\"92\":[2,211],\"93\":[2,20],\"94\":[1,154],\"95\":[2,40],\"96\":[3,78],\"97\":[5,118],\"98\":[3,61],\"99\":[3,96],\"100\":[2,224],\"101\":[1],\"102\":[5,174],\"103\":[1,6],\"104\":[1,105],\"105\":[1,61],\"106\":[1,338],\"107\":[1,310],\"108\":[3,11],\"109\":[1,51],\"110\":[1,126],\"111\":[1],\"112\":[1,34],\"113\":[3,18],\"114\":[1,23],\"115\":[3,18],\"116\":[1,48],\"117\":[1,86],\"118\":[1],\"119\":[1,363],\"120\":[3,118],\"121\":[1,279],\"122\":[1,289],\"123\":[3,179],\"124\":[4,17],\"125\":[1,65],\"126\":[1],\"127\":[3],\"128\":[2,17],\"129\":[2,17],\"130\":[1,19],\"131\":[1,73],\"132\":[1,15],\"133\":[1,24],\"134\":[2,23],\"135\":[2,51],\"136\":[2,15],\"137\":[1,316],\"138\":[1,122],\"139\":[1,17],\"140\":[5,35],\"141\":[1,196],\"142\":[5,40],\"143\":[1,151],\"144\":[1,42],\"145\":[1,222],\"146\":[1,140],\"147\":[1,17],\"148\":[1,33],\"149\":[1,28],\"150\":[1,121],\"151\":[3,1],\"152\":[1,192],\"153\":[1,73],\"154\":[1,112],\"155\":[1,54],\"156\":[1,94],\"157\":[1,290],\"158\":[3],\"159\":[1,65],\"160\":[1,83],\"161\":[1,24],\"162\":[1,13],\"163\":[1],\"164\":[2,19],\"165\":[1,65],\"166\":[2],\"167\":[4,26],\"168\":[4,19],\"169\":[4,11],\"170\":[1,11],\"171\":[8,87],\"172\":[5,55],\"173\":[2,43],\"174\":[1,22],\"175\":[3],\"176\":[1,69],\"177\":[2,32],\"178\":[1,25],\"179\":[2,61],\"180\":[2],\"181\":[2,23],\"182\":[3,17],\"183\":[1,42],\"184\":[2],\"185\":[2,43],\"186\":[2],\"187\":[1,248],\"188\":[1,149],\"189\":[2],\"190\":[1,353],\"191\":[1,92],\"192\":[2,363],\"193\":[2,19],\"194\":[1,46],\"195\":[2,36],\"196\":[1],\"197\":[2,27],\"198\":[3,12],\"199\":[4,35],\"200\":[5,27],\"201\":[5,39],\"202\":[2,109],\"203\":[2],\"204\":[1,110],\"205\":[2,147],\"206\":[1,173],\"207\":[1,191],\"208\":[1,307],\"209\":[2,21],\"210\":[1,44],\"211\":[1],\"212\":[3,105],\"213\":[1,660],\"214\":[2,104],\"215\":[1,113],\"216\":[1,78],\"217\":[1,33],\"218\":[1,4],\"219\":[2,23],\"220\":[1,141],\"221\":[1,13],\"222\":[3,54],\"223\":[2,70],\"224\":[1,140],\"225\":[1,43],\"226\":[2,17],\"227\":[1,27],\"228\":[1,69],\"229\":[1,22],\"230\":[1,9],\"231\":[2,32],\"232\":[2,49],\"233\":[3,36],\"234\":[7,114],\"235\":[1,102],\"236\":[5,115],\"237\":[2,21],\"238\":[4,26],\"239\":[4,20],\"240\":[5,16],\"241\":[1],\"242\":[4,52],\"243\":[1,30],\"244\":[1],\"245\":[1,2],\"246\":[5,23],\"247\":[5,15],\"248\":[5,18],\"249\":[5,25],\"250\":[5,31],\"251\":[4,26],\"252\":[1,51],\"253\":[2,3],\"254\":[2,27],\"255\":[2,148],\"256\":[2,123],\"257\":[2,105],\"258\":[3,79],\"259\":[3,43],\"260\":[1,77],\"261\":[3,31],\"262\":[2,46],\"263\":[4,200],\"264\":[1,220],\"265\":[1,242],\"266\":[1,203],\"267\":[2,16],\"268\":[1,186],\"269\":[1,109],\"270\":[1,13],\"271\":[1,82],\"272\":[5,164],\"273\":[2,103],\"274\":[1,257],\"275\":[1],\"276\":[1,22],\"277\":[1,20],\"278\":[1,22],\"279\":[2,15],\"280\":[1,136],\"281\":[1],\"282\":[1,69],\"283\":[1,60],\"284\":[1],\"285\":[1,127],\"286\":[1,125],\"287\":[1],\"288\":[1,29],\"289\":[1,31],\"290\":[1,33],\"291\":[1,39],\"292\":[1,23],\"293\":[1,602],\"294\":[6,27],\"295\":[1,21],\"296\":[1,62],\"297\":[1],\"298\":[1,27],\"299\":[1,15],\"300\":[1,21],\"301\":[1,6],\"302\":[1],\"303\":[1,80],\"304\":[1,72],\"305\":[1,131],\"306\":[1,75],\"307\":[1],\"308\":[1,53],\"309\":[1,69],\"310\":[1,41],\"311\":[1,45],\"312\":[1,21],\"313\":[1,25],\"314\":[3],\"315\":[1,60],\"316\":[1,37],\"317\":[1,33],\"318\":[1,23],\"319\":[1,23],\"320\":[1,11],\"321\":[6,19],\"322\":[1,52],\"323\":[1,83],\"324\":[1],\"325\":[1,39],\"326\":[1,27],\"327\":[1,41],\"328\":[1],\"329\":[1,41],\"330\":[1,58],\"331\":[1,25],\"332\":[1,51],\"333\":[1],\"334\":[1,34],\"335\":[1,37],\"336\":[1,22],\"337\":[1,20],\"338\":[9,21],\"339\":[1,130],\"340\":[1,21],\"341\":[1,126],\"342\":[1,127],\"343\":[7,40],\"344\":[4,28],\"345\":[1],\"346\":[5,107],\"347\":[2,15],\"348\":[1,13],\"349\":[5,40],\"350\":[3,50],\"351\":[2,24],\"352\":[1,45],\"353\":[1,153],\"354\":[1,45],\"355\":[2,179],\"356\":[1,62],\"357\":[2,74],\"358\":[2],\"359\":[2,151],\"360\":[2],\"361\":[2,98],\"362\":[2,109],\"363\":[2,30],\"364\":[3,54],\"365\":[1],\"366\":[3,20],\"367\":[2,22],\"368\":[1,118],\"369\":[2,70],\"370\":[1,38],\"371\":[1,49],\"372\":[4,42],\"373\":[1,64],\"374\":[5,26],\"375\":[1,22],\"376\":[2,123],\"377\":[1,60],\"378\":[2,22],\"379\":[1,17],\"380\":[6,528],\"381\":[1,102],\"382\":[1,356],\"383\":[1,94],\"384\":[3,240],\"385\":[4,387],\"386\":[3,160],\"387\":[2,18],\"388\":[1,56],\"389\":[1,7],\"390\":[1,47],\"391\":[3,35],\"392\":[2,31],\"393\":[2,50],\"394\":[1,21],\"395\":[1,7],\"396\":[3,5],\"397\":[4,85],\"398\":[3,43],\"399\":[4,42],\"400\":[4,39],\"401\":[3,71],\"402\":[2,46],\"403\":[1,143],\"404\":[1,3],\"405\":[1,31],\"406\":[1,20],\"407\":[1,146],\"408\":[1,156],\"409\":[1,57],\"410\":[1,245],\"411\":[1,92],\"412\":[1,210],\"413\":[1,132],\"414\":[1,17],\"415\":[1,66],\"416\":[1,20],\"417\":[6,144],\"418\":[10,95],\"419\":[8,183],\"420\":[10,349],\"421\":[6,194],\"422\":[1,53],\"423\":[1,4],\"424\":[2,234],\"425\":[2,130],\"426\":[2,181],\"427\":[4,160],\"428\":[2,141],\"429\":[2,153],\"430\":[2,138],\"431\":[3,220],\"432\":[1,61],\"433\":[1,13],\"434\":[1,51],\"435\":[1,158],\"436\":[1,15],\"437\":[1],\"438\":[1,1],\"439\":[1],\"440\":[2,40],\"441\":[2,33],\"442\":[1,1],\"443\":[1],\"444\":[1,86],\"445\":[1,19],\"446\":[1],\"447\":[2,25],\"448\":[2,37],\"449\":[2,40],\"450\":[1,22],\"451\":[1,23],\"452\":[1,24],\"453\":[3,27],\"454\":[2,64],\"455\":[5,20],\"456\":[1,12],\"457\":[1,17],\"458\":[3,12],\"459\":[1,23],\"460\":[1,16],\"461\":[1,35],\"462\":[2,16],\"463\":[2,41],\"464\":[1,1],\"465\":[1],\"466\":[1,31],\"467\":[1,16],\"468\":[1,26],\"469\":[1,37],\"470\":[1,42],\"471\":[1,26],\"472\":[1,77],\"473\":[4,8],\"474\":[2,64],\"475\":[1,45],\"476\":[1,16],\"477\":[4,44],\"478\":[1,45],\"479\":[3,20],\"480\":[3,64],\"481\":[2,49],\"482\":[5,56],\"483\":[2,22],\"484\":[2,23],\"485\":[2,47],\"486\":[3,42],\"487\":[4,24],\"488\":[2,48],\"489\":[1,47],\"490\":[4,50],\"491\":[3,48],\"492\":[2,57],\"493\":[1,36],\"494\":[1,24],\"495\":[1,1],\"496\":[1],\"497\":[1,16],\"498\":[1,4],\"499\":[2],\"500\":[1,266],\"501\":[2,94],\"502\":[2,128],\"503\":[1,46],\"504\":[3,35],\"505\":[3,54],\"506\":[1,27],\"507\":[4,17],\"508\":[4,22],\"509\":[1,1],\"510\":[2,99],\"511\":[5,22],\"512\":[3],\"513\":[3,91],\"514\":[3,72],\"515\":[4],\"516\":[1,50],\"517\":[3],\"518\":[1,68],\"519\":[1,1],\"520\":[6,43],\"521\":[1,72],\"522\":[1,97],\"523\":[1,16],\"524\":[2,19],\"525\":[5],\"526\":[2,11],\"527\":[2,6],\"528\":[2,13],\"529\":[5,24],\"530\":[3,2],\"531\":[3,12],\"532\":[3,9],\"533\":[3,10],\"534\":[6,18],\"535\":[2],\"536\":[5,5],\"537\":[3,8],\"538\":[2,14],\"539\":[1,1],\"540\":[1,52],\"541\":[1,30],\"542\":[1,147],\"543\":[1],\"544\":[3,114],\"545\":[3,120],\"546\":[3,83],\"547\":[1,28],\"548\":[1],\"549\":[1,1],\"550\":[2,25],\"551\":[4,13],\"552\":[2,6],\"553\":[2,20],\"554\":[2,12],\"555\":[2,3],\"556\":[2,3],\"557\":[2,63],\"558\":[2,13],\"559\":[1,1],\"560\":[1],\"561\":[4,37],\"562\":[3,35],\"563\":[3,25],\"564\":[2,23],\"565\":[1,29],\"566\":[1,30],\"567\":[2,20],\"568\":[3,6],\"569\":[5,29],\"570\":[3,34],\"571\":[1,20],\"572\":[4,49],\"573\":[1,1],\"574\":[1,121],\"575\":[1],\"576\":[4,13],\"577\":[4,12],\"578\":[1,70],\"579\":[1,7],\"580\":[1,1],\"581\":[1],\"582\":[2,199],\"583\":[1,1],\"584\":[1,22],\"585\":[1],\"586\":[2,144],\"587\":[3,150],\"588\":[6,159],\"589\":[2,321],\"590\":[2,158],\"591\":[3,14],\"592\":[2,190],\"593\":[2,22],\"594\":[1,29],\"595\":[1,200],\"596\":[1,98],\"597\":[1,214],\"598\":[1],\"599\":[4,4],\"600\":[2,40],\"601\":[2,47],\"602\":[2,67],\"603\":[1,4],\"604\":[2,31],\"605\":[2,31],\"606\":[1,54],\"607\":[1,66],\"608\":[1,10],\"609\":[1,59],\"610\":[1,63],\"611\":[2,92],\"612\":[2,34],\"613\":[1,15],\"614\":[1,62],\"615\":[3,3],\"616\":[3,33],\"617\":[2],\"618\":[2,21],\"619\":[1,44],\"620\":[4,51],\"621\":[3,43],\"622\":[4,46],\"623\":[1],\"624\":[3,10],\"625\":[1,35],\"626\":[1,83],\"627\":[1,16],\"628\":[1,4],\"629\":[1,24],\"630\":[1,30],\"631\":[1,52],\"632\":[1],\"633\":[1,113],\"634\":[1,100],\"635\":[1,66],\"636\":[1,14],\"637\":[3,10],\"638\":[1,29],\"639\":[1,64],\"640\":[1,213],\"641\":[1,235],\"642\":[1,35],\"643\":[1,26],\"644\":[3,10],\"645\":[1,18],\"646\":[1,100],\"647\":[1,128],\"648\":[1,150],\"649\":[1,104],\"650\":[1,135],\"651\":[1,18],\"652\":[2,12],\"653\":[1,26],\"654\":[1,76],\"655\":[1,166],\"656\":[1,287],\"657\":[1,202],\"658\":[1,230],\"659\":[3,2],\"660\":[6,136],\"661\":[4,16],\"662\":[4,5],\"663\":[2,333],\"664\":[2,10],\"665\":[1,16],\"666\":[1,67],\"667\":[1,114],\"668\":[1,166],\"669\":[1,68],\"670\":[4,69],\"671\":[1,47],\"672\":[1,5],\"673\":[2,12],\"674\":[1],\"675\":[1],\"676\":[2,9],\"677\":[1,20],\"678\":[1,62],\"679\":[1,85],\"680\":[1,101],\"681\":[1,148],\"682\":[1,11],\"683\":[1,62],\"684\":[1,31],\"685\":[1,37],\"686\":[1,18],\"687\":[1,19],\"688\":[1,15],\"689\":[4,9],\"690\":[3,50],\"691\":[3,64],\"692\":[3,42],\"693\":[3,7],\"694\":[2,129],\"695\":[3,6],\"696\":[1,150],\"697\":[1,181],\"698\":[1,116],\"699\":[1,176],\"700\":[1,177],\"701\":[1,23],\"702\":[1,2],\"703\":[4,77],\"704\":[1,1],\"705\":[6],\"706\":[6,130],\"707\":[1,43],\"708\":[6,17],\"709\":[8,230],\"710\":[7,400],\"711\":[2,2],\"712\":[1,138],\"713\":[1,165],\"714\":[1],\"715\":[1,48],\"716\":[1,66],\"717\":[1],\"718\":[1,58],\"719\":[1,30],\"720\":[1,40],\"721\":[1,49],\"722\":[1,73],\"723\":[1],\"724\":[1,104],\"725\":[1,32],\"726\":[1,23],\"727\":[1,1],\"728\":[1,41],\"729\":[1,44],\"730\":[1,28],\"731\":[1,75],\"732\":[1,1],\"733\":[1,120],\"734\":[1,95],\"735\":[1,173],\"736\":[1,75],\"737\":[1,119],\"738\":[1,2],\"739\":[1,41],\"740\":[1,23],\"741\":[1,64],\"742\":[3,30],\"743\":[1,28],\"744\":[2],\"745\":[1,33],\"746\":[1,37],\"747\":[1,37],\"748\":[2],\"749\":[1,49],\"750\":[1,39],\"751\":[1,116],\"752\":[4,11],\"753\":[2,11],\"754\":[4],\"755\":[1,8],\"756\":[1,7],\"757\":[1,14],\"758\":[1,4],\"759\":[2],\"760\":[1,6],\"761\":[1,5],\"762\":[1,19],\"763\":[1,8],\"764\":[2],\"765\":[1,6],\"766\":[1,27],\"767\":[1,4],\"768\":[2],\"769\":[1,3],\"770\":[1,5],\"771\":[1,19],\"772\":[1,6],\"773\":[2],\"774\":[1,6],\"775\":[1,33],\"776\":[1,5],\"777\":[2],\"778\":[1,11],\"779\":[1,41],\"780\":[1,15],\"781\":[1,17],\"782\":[2],\"783\":[1,19],\"784\":[1,25],\"785\":[2],\"786\":[1,3],\"787\":[1,27],\"788\":[1,3],\"789\":[2],\"790\":[1,10],\"791\":[1,17],\"792\":[1,23],\"793\":[2],\"794\":[1,28],\"795\":[1,32],\"796\":[1,3],\"797\":[1,17],\"798\":[4,11],\"799\":[2,34],\"800\":[2,64],\"801\":[3,50],\"802\":[2,15],\"803\":[2,65],\"804\":[2,38],\"805\":[3,105],\"806\":[2,137],\"807\":[2,159],\"808\":[3,92],\"809\":[3,278],\"810\":[2,67],\"811\":[2,113],\"812\":[1,25],\"813\":[4,11],\"814\":[7,50],\"815\":[2,236],\"816\":[2,190],\"817\":[2],\"818\":[4,11],\"819\":[2,44],\"820\":[1],\"821\":[2,3],\"822\":[1,133],\"823\":[1,594],\"824\":[2,59],\"825\":[1,55],\"826\":[1,32],\"827\":[1,66],\"828\":[6,72],\"829\":[1,19],\"830\":[3,51],\"831\":[1,50],\"832\":[1,45],\"833\":[1,100],\"834\":[1,52],\"835\":[1,78],\"836\":[1,114],\"837\":[1,28],\"838\":[2,4],\"839\":[1],\"840\":[2],\"841\":[1],\"842\":[1],\"843\":[1],\"844\":[1,5],\"845\":[1,13],\"846\":[1,117],\"847\":[1,166],\"848\":[1,50],\"849\":[1,37],\"850\":[6,51],\"851\":[1,12],\"852\":[1,16],\"853\":[1,9],\"854\":[1,12],\"855\":[1,4],\"856\":[6,20],\"857\":[6,37],\"858\":[4,13],\"859\":[5,18],\"860\":[3,40],\"861\":[1,6],\"862\":[1,4],\"863\":[1,16],\"864\":[1,6],\"865\":[3,33],\"866\":[4,17],\"867\":[6,32],\"868\":[4,33],\"869\":[5,18],\"870\":[5,5],\"871\":[3,60],\"872\":[4,23],\"873\":[2,27],\"874\":[3,18],\"875\":[3,23],\"876\":[1,5],\"877\":[2,121],\"878\":[2,46],\"879\":[1,1],\"880\":[1,18],\"881\":[3,41],\"882\":[3,9],\"883\":[3,11],\"884\":[1,70],\"885\":[1,181],\"886\":[5,73],\"887\":[2,100],\"888\":[1,45],\"889\":[1,36],\"890\":[3,11],\"891\":[1,14],\"892\":[1,282],\"893\":[1,291],\"894\":[5,103],\"895\":[2,221],\"896\":[3,58],\"897\":[2,56],\"898\":[3,181],\"899\":[2,396],\"900\":[2,184],\"901\":[3,3],\"902\":[1],\"903\":[6,35],\"904\":[1,56],\"905\":[5],\"906\":[2,24],\"907\":[2,23],\"908\":[2,16],\"909\":[2,38],\"910\":[5,27],\"911\":[5],\"912\":[4,11],\"913\":[3,11],\"914\":[2,7],\"915\":[2,30],\"916\":[2,8],\"917\":[3,40],\"918\":[2,283],\"919\":[1,11],\"920\":[2,2],\"921\":[1,117],\"922\":[1],\"923\":[1,64],\"924\":[1,107],\"925\":[1,110],\"926\":[2,272],\"927\":[1,9],\"928\":[5,10],\"929\":[1],\"930\":[2,17],\"931\":[4,105],\"932\":[6,96],\"933\":[2,19],\"934\":[2,54],\"935\":[2,96],\"936\":[2,26],\"937\":[4,59],\"938\":[2,58],\"939\":[4,50],\"940\":[1],\"941\":[5,6],\"942\":[1,59],\"943\":[2,53],\"944\":[1,135],\"945\":[1,116],\"946\":[1,133],\"947\":[1,108],\"948\":[1,39],\"949\":[1,91],\"950\":[1,84],\"951\":[2,125],\"952\":[1,56],\"953\":[1,4],\"954\":[3,3],\"955\":[1,27],\"956\":[5,187],\"957\":[3,12],\"958\":[1,61],\"959\":[1,100],\"960\":[1,51],\"961\":[1,53],\"962\":[1],\"963\":[1,300],\"964\":[1,232],\"965\":[1,7],\"966\":[2,2],\"967\":[1,3]},\"averageFieldLength\":[1.977272727272729,66.57152538347299],\"storedFields\":{\"0\":{\"h\":\"主页\",\"t\":[\"知识星球: MetaMind , 小红书: BinaryOracle , CSDN: Binary Oracle\"]},\"1\":{\"h\":\"关于我们\"},\"2\":{\"h\":\"Binary Oracle\",\"t\":[\"一名普通但十分热爱探索技术的Coder\",\"开源框架 Spring committer\",\"Golang 开源网络库 netpoll committer\",\"Javaer 转型 3D - VL 方向研究\",\"现就读于四川大学\",\"有问题需要咨询的小伙伴，可以加微信备注来意:\"]},\"3\":{\"h\":\"Elowen\",\"t\":[\"CV 转 LLM 领域\",\"现就读于电子科技大学\"]},\"4\":{\"h\":\"3D Affordance Grounding 方向复盘\",\"t\":[\"3D Affordance Grounding 方向复盘\"]},\"5\":{\"h\":\"点云 + 文本\"},\"6\":{\"h\":\"\",\"t\":[\"特点:\",\"AFFOrdance Grounding All aT Once\",\"a large-scale dataset for 3D and 2D affordance grounding\",\"minimalistic architecture\",\"损失函数:\",\"Focal Loss to handle class imbalance\",\"Dice Loss to improve region-level alignment.\",\"现状:\",\"wait for code release\",\"dataset available\"]},\"7\":{\"h\":\"\",\"t\":[\"特点:\",\"Propose a 3D multimodal large language model (referring to the LLaVA model architecture)\",\"Feed the <SEG> segmentation tokens output by the 3D MMLLM into the multi-granularity language-point cloud combination module to complete 3D dense prediction\",\"Support sequential instruction execution\",\"Large-scale instruction-point cloud pair dataset: A dataset with 180,000 instruction-point cloud pairs, covering single and sequential operability reasoning tasks\",\"损失函数:\",\"Autoregressive Cross-Entropy Loss\",\"Dice Loss\",\"Binary Cross-Entropy Loss\",\"现状:\",\"code available\",\"dataset available\"]},\"8\":{\"h\":\"点云 + 图像\"},\"9\":{\"h\":\"\",\"t\":[\"特点:\",\"损失函数:\",\"现状:\"]},\"10\":{\"h\":\"点云 + 文本 + 图像\"},\"11\":{\"h\":\"\",\"t\":[\"特点:\",\"grounding 3D object affordance in an Open-Vocabulary fashion\",\"Multi-Head Affordance Chain-of-Thought\",\"Data preparation stage:\",\"Use prompts to generate descriptions of the object interaction area, the morphology(形态学) of the interaction area, the interaction behavior, and other common interaction behaviors of the object.\",\"Geometric structure knowledge = Answers to Prompt 1 + Prompt 2 = Interaction parts + Inference of geometric properties of these parts\",\"Interaction knowledge = Answers to Prompt 3 + Prompt 4 = Current interaction + Analogous(类似的)/supplementary(补充) interaction methods\",\"PIADv2 dataset\",\"24 affordance , 43 object categories, 15K interaction images , 38K 3D objects with annotations.\",\"损失函数:\",\"Focal Loss to handle class imbalance\",\"Dice Loss to improve region-level alignment.\",\"现状:\",\"code available\",\"dataset available\"]},\"12\":{\"h\":\"\",\"t\":[\"特点:\",\"Combine language instructions, visual observations, and interaction information to locate the affordance of manipulable objects in 3D space.\",\"AGPIL（Affordance Grounding dataset with Points, Images and Language instructions）\",\"This dataset includes estimations of object affordances observed from full-view, partial-view, and rotated perspectives, taking into account factors such as real-world observation angles, object rotation, and spatial occlusion (遮挡).\",\"损失函数:\",\"focal loss\",\"dice loss\",\"现状:\",\"wait for code release\",\"dataset available\"]},\"13\":{\"h\":\"3D Gaussian Splatting (3DGS)\"},\"14\":{\"h\":\"\",\"t\":[\"特点:\",\"\\\"Knowledge Distillation\\\" from 2D to 3D: Transfer the semantic capabilities of pre-trained 2D models to the 3D affordance prediction model through Gaussian splat mapping, cross-modal consistency alignment, and multi-scale fusion.\",\"Noisy Dataset: Construct a new benchmark with multiple types of noise/damage to evaluate the generalization and robustness of the model under real/harsh conditions.\",\"损失函数:\",\"BCE\",\"Dice Loss\",\"Consistency Loss（MSE 损失）\",\"现状:\",\"wait for code release\",\"wait for dataset release\"]},\"15\":{\"h\":\"\"},\"16\":{\"h\":\"\"},\"17\":{\"h\":\"idea\",\"t\":[\"Momentum Encoder 生成伪标签应对噪声问题，实现更加稳健的学习 ？(参考: MoCo , ALBEF , DINO)\"]},\"18\":{\"h\":\"GEAL 论文\",\"t\":[\"GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency 论文\",\"论文链接: GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency\"]},\"19\":{\"h\":\"引言\",\"t\":[\"研究背景与动机：\",\"3D 可供性学习的目标是根据语义线索（例如图像、文本指令），在三维物体上找出可以交互的区域。比如机器人需要知道 哪里能抓住把手、哪里能按下按钮。这种能力对机器人学和人机交互非常重要，能够支持 动作预测、物体操作以及自主决策 等任务。\",\"然而，现有 3D 可供性学习方法存在几个主要问题：\",\"数据稀缺：与 2D 任务相比，3D 数据的标注非常有限，因此泛化性不足。\",\"主干网络受限：当前 3D 模型大多依赖几何与位置编码，无法很好地捕捉全局语义，导致在 复杂场景、噪声干扰、传感器误差或数据损坏 下表现不佳。\",\"这些问题导致现有方法在鲁棒性和适应性上均受限制。\",\"本文提出的解决方案 GEAL：\",\"GEAL（Generalizable 3D Affordance Learning）旨在同时提升 泛化性 和 鲁棒性。它的设计核心包括：\",\"双分支架构：一个分支处理 3D 点云，另一个分支通过 3D Gaussian Splatting (3DGS) 将稀疏点云渲染成逼真的 2D 图像，从而建立一致的 2D-3D 映射。这样可以利用 大规模预训练 2D 模型 的语义知识与泛化能力来增强 3D 分支。\",\"粒度自适应融合模块：动态融合多层次的视觉与文本特征，使模型能在不同尺度、不同粒度下准确回答可供性相关问题。\",\"2D-3D 一致性对齐模块：在特征层面建立 2D 与 3D 模态的可靠对应关系（通过嵌入到 3DGS 的高斯基元中实现），确保知识有效迁移，并提升 3D 分支的泛化与鲁棒性。\",\"新的鲁棒性基准：\",\"为了弥补现有研究缺乏鲁棒性评测的不足，作者构建了两个新的数据集：\",\"PIAD-Corrupt\",\"LASO-Corrupt\",\"这两个基准数据集基于常用的 3D 可供性数据集构建，并通过引入 缩放、裁剪等真实场景中的损坏方式 来模拟噪声和破坏，从而提供一个标准化的评测平台。\",\"实验结果与贡献总结：\",\"大量实验表明，GEAL 在 已见类别、未见类别以及带有噪声/损坏的数据 上，均优于现有方法，显示出强大的适应性和鲁棒性。\",\"本文的主要贡献可以总结如下：\",\"提出 GEAL，一种用于通用化 3D 可供性学习的新方法；通过 3DGS 构建 2D 分支，并利用预训练 2D 模型的语义知识提升 3D 预测能力。\",\"设计 粒度自适应融合模块 与 2D-3D 一致性对齐模块，在双分支架构下实现跨模态知识整合与传播。\",\"构建两个基于损坏的评测基准：PIAD-C 和 LASO-C，为社区提供了一个衡量 3D 可供性方法鲁棒性的标准。\",\"在主流与损坏基准上进行大量实验，验证了 GEAL 在多种条件下均能保持优秀性能，具备较强的泛化能力和鲁棒性。\"]},\"20\":{\"h\":\"相关工作\",\"t\":[\"2D 可供性学习：\",\"可供性（affordances）是指物体或环境的属性决定了观察者可以执行的潜在动作。早期方法主要在图像或视频中识别交互区域，但缺乏对物体可供性相关部分的精确定位。\",\"后来研究通过示例性的 2D 数据，改进了可供性定位精度。同时，大规模预训练模型可以将视觉特征与可供性相关的文本描述对齐，从而减少对人工标注的依赖，并在新场景下提升可供性预测能力。\",\"近期一些研究进一步利用 基础模型（foundation models），将可供性检测推广到新颖物体和不同视角，实现更好的泛化。\",\"3D 可供性学习：\",\"将可供性检测扩展到 3D 空间更具挑战，因为需要精确的空间和深度信息。\",\"一些方法尝试使用 2D 数据来预测 3D 可供性区域，但难以精确定位交互点位。\",\"随着大规模 3D 物体数据集的出现，研究者开始直接将可供性映射到 3D 结构上，以捕捉复杂空间关系。\",\"最近的方法利用 2D 视觉和语言模型进行开放词汇（open-vocabulary）可供性检测，在无需固定标签集的情况下增强泛化能力。\",\"尽管如此，3D 模型仍然缺乏 2D 基础模型的泛化能力，因此仍然很难实现稳健泛化。本文的方法正是通过引入 大规模 2D 基础模型 来提升 3D 可供性学习的泛化性。\",\"3D 可供性学习的鲁棒性：\",\"在真实世界中，3D 可供性学习容易受到点云损坏影响，这些损坏可能源自：\",\"场景复杂性\",\"传感器误差\",\"数据处理错误\",\"现有研究尝试提升 3D 感知在噪声和损坏条件下的鲁棒性，但可供性学习要求在 数据退化的情况下依然精确识别交互区域。\",\"据作者所述，本文是 首个专门针对 3D 可供性学习鲁棒性 的研究，提出了一种针对性的解决方案，旨在提升模型在各种复杂环境下的可靠性。\"]},\"21\":{\"h\":\"方法\",\"t\":[\"整体流程：\",\"GEAL 框架旨在从 3D 点云和指令文本中预测物体各点的可供性分数 。整体包括三大模块：\",\"3D-2D 映射（Gaussian Splatting）\",\"跨模态一致性对齐\",\"可供性解码与预测\"]},\"22\":{\"h\":\"1. 3D-2D 映射（如图2左侧）\",\"t\":[\"动机：\",\"现有 3D 可供性学习泛化性差、鲁棒性不足\",\"2D 预训练模型具有强泛化能力和丰富语义知识\",\"因此通过 3D → 2D 映射，可以利用 2D 模型优势\",\"方法：\",\"使用 3D Gaussian Splatting 将点云表示为高斯基元\",\"高斯基元参数包括位置 、协方差 、颜色 和不透明度 \",\"渲染公式：\",\"使用深度图和颜色图生成 个视角的 2D 图像 \",\"将可供性分数 转换为灰度，赋给高斯颜色 ，生成 2D 可供性掩码 \",\"建立 3D 点云和 2D 表示的一致映射\",\"编码器：\",\"3D 分支使用 PointNet++ 提取多尺度点云特征 \",\"2D 分支使用 DINOV2 提取图像特征 \",\"文本指令 通过轻量语言模型提取文本特征 ，2D 分支加入视角信息增强语义理解\"]},\"23\":{\"h\":\"2. 跨模态一致性对齐（如图2右侧、图3）\",\"t\":[\"核心目标：将点云、图像和文本特征统一到共享嵌入空间，实现 2D → 3D 知识迁移，提高泛化性与鲁棒性。\",\"粒度自适应视觉-文本融合（GAFM）\",\"融合多尺度视觉特征和文本特征\",\"Flexible Granularity Feature Aggregation：通过自适应权重融合不同尺度特征，支持多粒度可供性推理\",\"Text-Conditioned Visual Alignment：文本信息增强视觉特征，使视觉特征保留空间结构的同时关注指令相关区域\",\"3D 分支类似处理，但先在每个尺度进行对齐，再上采样统一分辨率，然后融合多尺度特征\",\"2D-3D 一致性对齐（CAM）\",\"2D 特征保留语义，3D 特征保留几何信息\",\"将 3D 特征通过高斯映射投影到 2D\",\"对齐特征并使用 L2 损失约束：\",\"实现 2D → 3D 知识迁移，增强 3D 分支对未见物体和噪声数据的理解能力\"]},\"24\":{\"h\":\"3. 可供性解码与训练\",\"t\":[\"解码器：\",\"使用 3 层 Transformer 解码器\",\"文本特征作为 query，视觉特征作为 key/value\",\"输出增强文本特征 ，作为动态卷积核预测可供性分数\",\"损失函数：\",\"BCE + Dice Loss，用于解决类别不平衡并提升分割精度\",\"训练分两阶段：\",\"先训练 2D 分支\",\"冻结 2D 分支（CAM 除外），训练 3D 分支并加入一致性损失 \",\"推理阶段仅使用 3D 分支，实现高效预测\"]},\"25\":{\"h\":\"4. 数据损坏基准\",\"t\":[\"PIAD-C 和 LASO-C\",\"七类损坏类型：Add Global、Add Local、Drop Global、Drop Local、Rotate、Scale、Jitter\",\"每类 5 个严重度，总计 4,890 对对象-可供性\",\"覆盖 17 个可供性类别、23 个对象类别、2,047 个不同形状\",\"用于评测模型在真实场景中的鲁棒性\"]},\"26\":{\"h\":\"5. 个人理解\",\"t\":[\"2D 领域情况\",\"有丰富的大规模数据和成熟的预训练基础模型（如 CLIP、DINOV2 等），所以 2D affordance grounding 已经相对成熟。\",\"这些模型具备很强的语义理解与泛化能力。\",\"3D 领域情况\",\"点云本身稀疏、缺乏纹理，语义信息表达能力弱。\",\"标注数据有限，导致模型在 泛化性和鲁棒性 上表现不佳。\",\"论文的策略\",\"利用 高斯散点渲染 (Gaussian Splatting) 将 3D 点云转化为更具语义信息的 2D 视图。\",\"引入 2D 分支（教师），保持其预训练能力不变，作为稳定的语义参考。\",\"在训练中通过 2D–3D 一致性约束（Consistency Alignment Module, CAM），强制 3D 分支的表示与 2D 分支对齐。\",\"这样，2D 分支的知识就能迁移到 3D 分支，使其在泛化性和鲁棒性上大幅提升。\",\"简化成一句话：\",\"2D 分支是老师，提供强大的语义泛化能力；3D 分支是学生，通过一致性约束继承老师的能力，从而弥补自身在稀疏点云上的不足。\"]},\"27\":{\"h\":\"局限性\",\"t\":[\"内部可供性不足：模型主要依赖点云的外部表面信息，难以识别和泛化与内部结构相关的可供性（如瓶子的“容纳”功能），同时缺乏高质量内部可供性数据进一步限制了表现。\",\"伦理风险：在监控或自主决策等场景中，若技术被滥用可能侵犯隐私或导致缺乏问责，需要建立完善的伦理规范。\",\"资源消耗高：训练和部署需要大量计算资源，中小型组织或技术基础设施不足的地区难以承担，限制了框架的普及性。\"]},\"28\":{\"h\":\"GREAT 论文解读\",\"t\":[\"GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding 论文解读\",\"论文: https://arxiv.org/abs/2411.19626 代码: https://github.com/yawen-shao/GREAT_code 数据集: https://drive.google.com/drive/folders/1n_L_mSmVpAM-1ASoW2T2MltYkaiA_X9X\"]},\"29\":{\"h\":\"摘要\",\"t\":[\"GREAT（Geometry-Intention Collaborative Inference）是一种新颖的框架，旨在通过挖掘物体的不变几何属性和潜在交互意图，以开放词汇的方式定位3D物体的功能区域（affordance）。该框架结合了多模态大语言模型（MLLMs）的推理能力，设计了多头部功能链式思维（MHACoT）策略，逐步分析交互图像中的几何属性和交互意图，并通过跨模态自适应融合模块（CMAFM）将这些知识与点云和图像特征结合，实现精准的3D功能定位。此外，研究还提出了目前最大的3D功能数据集PIADv2，包含15K交互图像和38K标注的3D物体实例。实验证明了GREAT在开放词汇场景下的有效性和优越性。\"]},\"30\":{\"h\":\"简介\",\"t\":[\"Open-Vocabulary 3D对象功能定位（OVAG）旨在通过任意指令定位物体上支持特定交互的“动作可能性”区域，对机器人感知与操作至关重要。现有方法（如IAGNet、LASO）通过结合描述交互的图像或语言与3D几何结构引入外部先验，但存在以下局限性（如图1(b)所示）：\",\"语义空间受限：依赖预定义类别，难以泛化到未见过的功能（如将“pour”错误分类为“grasp”）。\",\"几何与意图利用不足：未充分挖掘物体间共享的几何不变性（如手柄的抓握属性）和同一物体的多交互意图关联。\",\"人类认知启发:\",\"研究表明（Gick & Holyoak, 1980），人类通过多步推理和类比思维解决复杂任务。例如，观察倒水场景时（图1(c)），人类会：\",\"识别交互部件（壶嘴）\",\"提取几何属性（倾斜曲面）\",\"推理潜在意图（倒水/注水）\",\"方法创新:\",\"GREAT框架通过以下设计模拟这一过程（图1(d)）：\",\"MHACoT推理链：基于微调的MLLM（如InternVL）分步推理：\",\"Object-Head：定位交互部件并分析几何结构（如“为什么壶嘴适合倒水”）\",\"Affordance-Head：描述实际交互（如“握柄倒水”）并联想潜在意图（如“注水/清洗”）\",\"跨模态融合：通过CMAFM模块将几何属性（）与交互意图（）注入点云（）和图像特征（），最终解码为3D功能热图 。\",\"数据集贡献:\",\"扩展构建了PIADv2（对比见表1）：\",\"规模：15K交互图像（×3）和38K 3D实例（×5）\",\"多样性：43类物体、24类功能，覆盖多对多关联（图3(c)）\"]},\"31\":{\"h\":\"相关工作\",\"t\":[\"1. Affordance Grounding\",\"现有研究主要从2D数据（如图像、视频）和自然语言理解出发，定位“动作可能性”区域。例如，部分工作通过语言理解在2D数据中定位功能区域（3, 21），但机器人操作需要3D信息，2D方法难以直接迁移。随着3D数据集（如5, 6）的出现，部分研究开始映射语义功能到3D结构，但受限于预定义类别，无法处理开放词汇场景。\",\"2. Open-Vocabulary 3D Affordance Grounding (OVAG)\",\"OVAG旨在通过额外指令（如文本或图像）引入交互先验，提升泛化能力。例如：\",\"IAGNet 利用2D交互语义指导3D功能定位；\",\"LASO 通过文本条件查询分割功能区域；\",\"OpenAD 和 OpenKD 利用CLIP编码器实现文本-点云关联。\",\"这些方法仍受限于训练语义空间，而GREAT通过几何-意图协同推理（CoT）解决此问题（如表2所示）。\",\"3. Chain-of-Thought (CoT) 与多模态大模型 (MLLMs)\",\"CoT及其变体通过多步推理增强MLLMs能力。例如：\",\"视觉任务中，MLLMs（如InternVL）结合CoT在目标检测、机器人操作等任务中表现优异；\",\"但动态功能特性使得MLLMs难以直接从交互图像推理3D功能，GREAT通过微调MLLMs并设计MHACoT策略解决这一问题。\",\"关键问题（如图1所示）：\",\"现有方法依赖数据对齐，泛化性不足（如将“pour”误分类为“grasp”）；\",\"GREAT通过模拟人类多步推理（几何属性提取+意图类比）实现开放词汇功能定位。\"]},\"32\":{\"h\":\"方法\",\"t\":[\"GREAT 的输入为 ，其中 是点云，包含物体的坐标 和其对应的 3D 可供性标注 ， 为图像。目标是优化模型 ，输出 3D 物体可供性 ，即：\",\"如图2所示，首先使用 ResNet [9] 和 PointNet++ [43] 提取特征，分别得到 和 ，随后将 reshape 为 （其中 ）。接着通过多头可供性链式思维（MHACoT）策略对交互图像进行推理，挖掘不变几何属性与潜在交互意图。\",\"然后，使用 Roberta [28] 编码推理结果，通过交叉注意力机制计算对象几何特征 和可供性意图特征 （见 Sec. 3.2）。GREAT 利用跨模态自适应融合模块（CMAFM）将这些知识注入点云特征并与图像特征融合，得到融合特征 （见 Sec. 3.3）。最后将这两个特征送入解码器以获得可供性输出 ，并通过复合损失优化整个流程（见 Sec. 3.4）。\"]},\"33\":{\"h\":\"3.2 Multi-Head Affordance Chain-of-Thought\"},\"34\":{\"h\":\"Fine-Tuning MLLM\",\"t\":[\"为了获得对物体可供性更深入的理解，我们对 InternVL [4] 使用可学习的 Adapter [10] 进行微调，仅更新 Adapter 模块（10 个 epoch，学习率 4e-5，LoRA rank 为 16），其余参数保持冻结，以保持原始模型识别能力的同时增强其推理能力。\"]},\"35\":{\"h\":\"Object-Head Reasoning（几何推理）\",\"t\":[\"该部分包含：\",\"物体交互感知（Object Interaction Perception）：识别图像中物体与人发生交互的部分。Prompt 示例为：“指出图像中物体与人交互的部分。”\",\"几何结构推理（Geometric Structure Reasoning）：进一步从几何结构角度推理为什么该部位适合交互。Prompt 示例为：“从几何结构解释该部位可以交互的原因。”\"]},\"36\":{\"h\":\"Affordance-Head Reasoning（类比推理）\",\"t\":[\"该部分包含：\",\"交互细节描述（Interaction Detailed Description）：描述图像中人与物体之间的完整交互过程，生成细粒度表示。Prompt 示例为：“描述图像中人与物体的交互方式。”\",\"交互类比推理（Interactive Analogical Reasoning）：模拟人类对交互方式的联想，挖掘其他可能交互意图，增强类比能力。Prompt 示例为：“列举两个该物体常见的其他交互方式。”\"]},\"37\":{\"h\":\"Knowledge Encoding and Integration\",\"t\":[\"从 Object-Head 得到的几何属性描述与 Affordance-Head 推理的交互描述被 Roberta 编码为两个特征：\",\"：物体几何知识特征\",\"：可供性意图知识特征\",\"通过交叉注意力层 与自注意力层 对齐二者，公式如下：\"]},\"38\":{\"h\":\"3.3 Cross-Modal Adaptive Fusion Module (CMAFM)\",\"t\":[\"为了将几何属性与点云特征更好地对齐融合，CMAFM 将 融合至 PointNet++ 最深层特征，并与图像特征联合用于预测。\",\"具体地，对点云特征 和知识特征 进行线性映射形成 Query、Key、Value：\",\"跨注意力融合公式为：\",\"最终点云融合特征表示为：\",\"其中 为全连接层， 表示池化后扩展为 ， 为 卷积，输出 上采样至原始点数后记为：\",\"图像特征 与意图特征 融合表示为：\"]},\"39\":{\"h\":\"3.4 Decoder and Loss Functions\",\"t\":[\"最终将融合后的图像特征 和点云特征 拼接后送入解码器输出可供性预测：\",\"其中 为 sigmoid 激活， 为输出头，， 是最终的 3D 可供性预测。\",\"损失函数由 focal loss [26] 与 dice loss [37] 组成：\",\"这种设计无需依赖具体的可供性分类标签，而是通过监督点级热图，将 3D 可供性与交互图像直接联系起来。\"]},\"40\":{\"h\":\"数据集\",\"t\":[\"为支撑开放词汇 3D 物体可供性定位任务，本文构建了 PIADv2（Point Image Affordance Dataset v2），由成对的 2D 交互图像与 3D 点云对象组成，是当前规模最大的同类数据集。\"]},\"41\":{\"h\":\"数据收集（Collection）\",\"t\":[\"点云部分主要来自以下开源数据源：\",\"3DIR [57]\",\"3D-AffordanceNet [6]\",\"Objaverse [5]\",\"图像部分主要来源于：\",\"AGD20k [32]\",\"OpenImage [18]\",\"其他开源许可网站\",\"总体数据统计：\",\"图像数：15,213\",\"点云数：38,889\",\"覆盖类别：\",\"物体类别：43类\",\"可供性类别：24类\",\"该数据集大大超越了前作 PIAD [56]，其图像数量是前者的三倍，点云数量是前者的五倍。\",\"如图3(a) 所示，红色区域为点云的可供性标注。图3(b) 展示了各类别的分布情况，显示出数据集对交互多样性和类别多样性的全面覆盖。\"]},\"42\":{\"h\":\"标注策略（Annotation）\",\"t\":[\"对于点云实例：\",\"每个点云实例按可供性类别标注\",\"每个样本为一个 的矩阵，含：\",\"2048 个点\",\"每个点包括 坐标与热力图形式的可供性值\",\"对于图像：\",\"图像按可供性类别进行分类，以支持训练阶段的匹配与推理\"]},\"43\":{\"h\":\"统计分析（Statistical Analysis）\",\"t\":[\"图像与点云之间不需要一一对应，二者分别从不同实例中采样，以增强泛化能力\",\"多对多关系分析：如图3(c) 所示，affordance 与 object 类别间存在明显的多对多关系，挑战模型对可供性的泛化能力\",\"类平衡分析：图3(d) 展示了各 object 类别下图像与点云的数量比例，体现出数据集在样本分布上的全面性和均衡性\"]},\"44\":{\"h\":\"数据划分（Data Partitions）\",\"t\":[\"PIADv2 提供三种标准划分方式（前两种与 PIAD [56] 保持一致）：\",\"Seen：\",\"训练集与测试集中的物体与可供性类别相同\",\"Unseen Object：\",\"测试集中包含训练集中未出现的物体类别，但可供性类别相同\",\"Unseen Affordance：\",\"测试集中的可供性类别未在训练集中出现，同时包含部分新物体类别\"]},\"45\":{\"h\":\"实验\",\"t\":[\"为验证所提方法 GREAT 的有效性与泛化能力，作者在提出的 PIADv2 数据集上开展了系统性的实验评估，包括与多个先进方法的对比以及消融实验和可视化分析。\"]},\"46\":{\"h\":\"5.1 Benchmark Setting\",\"t\":[\"评估指标：\",\"实验采用以下评估指标评估 3D 可供性预测质量（参考 [25, 56]）：\",\"AUC（Area Under Curve）[29]\",\"aIOU（average Intersection over Union）[45]\",\"SIM（Similarity）[47]\",\"MAE（Mean Absolute Error）[52]\",\"对比方法：\",\"IAG (2023)：2D-引导的3D可供性方法\",\"LASO (2024)：基于语言引导的3D可供性分割\",\"FRCNN [54]：LiDAR-图像融合两阶段3D检测框架\",\"XMF [1]：图像-点云的跨模态点云形状补全方法\",\"Baseline：直接拼接图像与点云特征作为输入\",\"实现细节：\",\"3D backbone：PointNet++ [43]\",\"2D backbone：ResNet18 [9]\",\"优化器：Adam\",\"学习率：1e-4\",\"批大小：16\",\"总训练轮次：65\"]},\"47\":{\"h\":\"5.2 Comparison Results\",\"t\":[\"如表2所示，GREAT 在所有划分（Seen、Unseen Object、Unseen Affordance）下均显著优于现有方法，达成最新最优性能。\",\"量化分析：\",\"在 Unseen Affordance 这一最具挑战性的设置下，GREAT 依旧表现出色：\",\"AUC：69.81（高出 LASO 约 9%）\",\"aIOU：12.05（高出 IAG 约 34%）\",\"SIM：0.290（大幅超越所有基线）\",\"MAE：0.127（最小）\",\"可视化分析（如图4所示）：\",\"Seen setting：各方法差别不大\",\"Unseen setting：\",\"其他方法倾向于错误地预测为训练集中频繁出现的 affordance（如 grasp）\",\"GREAT 能正确捕捉如 \\\"pour\\\" 这类 unseen affordance，定位精度显著更高\"]},\"48\":{\"h\":\"5.3 Ablation Study\",\"t\":[\"表3 展示了对关键模块的消融实验结果：\",\"消融项分析：\",\"✗ AffCoT（无意图推理）：\",\"unseen affordance 的 aIOU 下降了 1.12，表明交互意图推理对泛化至新 affordance 极为重要\",\"✗ ObjCoT（无几何推理）：\",\"模型对物体关键交互区域的识别能力下降\",\"✗ CMAFM（无跨模态融合）：\",\"几何信息无法有效注入点云，导致各项指标大幅下降（aIOU 从 38.03 降到 29.48）\",\"✗ FT（无 MLLM 微调）：\",\"推理能力受限，泛化性下降明显\",\"可视化支持（见图5）：\",\"(a)：若缺失 AffCoT，模型无法进行类比推理，预测倾向训练集中已有的 affordance\",\"(b)：缺失 ObjCoT 时，模型无法精确聚焦于关键交互部位（如 kettle 的 spout）\"]},\"49\":{\"h\":\"5.4 Performance Analysis\",\"t\":[\"为进一步评估模型的理解与泛化能力，作者设计了多个分析实验。\",\"多个物体场景（Multiple Objects）：\",\"在同一张交互图像中存在多个物体时，模型能准确对每个对象生成独立的 affordance 区域（见图6）\",\"多种可供性（Multiple Affordances）：\",\"同一物体在不同交互图像中被推理出不同的 3D affordance 区域，体现出模型对语义的灵活解析能力（见图7）\",\"多实例鲁棒性（Multiple Instances）：\",\"在几何形状变化显著的同类物体中，模型依然能稳定预测合理的交互区域（见图8），说明其具备良好的泛化能力与鲁棒性\"]},\"50\":{\"h\":\"结论\",\"t\":[\"我们提出了一种开放词汇形式的 3D 物体可供性定位方法，该方法从交互图像中进行推理，能够突破预定义样本空间的限制，并推广至未见场景。为实现这一目标，我们设计了一个新颖的框架 —— 通过多头可供性链式思维（Multi-Head Affordance Chain-of-Thought）推理，挖掘物体的不变几何属性，并对潜在交互方式进行类比推理，同时结合跨模态特征对齐，实现对 3D 可供性区域的精准定位。\",\"此外，我们引入了目前最大规模的 3D 可供性数据集 PIADv2，涵盖 1.5 万张交互图像与超过 3.8 万个标注完整的 3D 物体。大量实验验证了我们提出的 GREAT 框架在多项评估指标上具有显著优势，能够在开放场景下支持可供性理解，有望提升机器人在未知环境中的自主交互能力。我们相信该研究将为视觉可供性理解领域带来新的启发并推动其发展。\",\"局限性与未来工作： GREAT 的主要局限在于其多步推理机制带来了较高的计算复杂度，在大规模或实时应用中可能成为瓶颈。未来，我们计划构建专用于推理的数据集，并利用这些数据集对多模态模型进行知识蒸馏，使其专注于特定领域，从而在实际应用中实现更快、更高效的性能。\"]},\"51\":{\"h\":\"代码\"},\"52\":{\"h\":\"Multi-Head Affordance Chain-of-Thought\",\"t\":[\"MHACoT是一种类人推理方式，分多个步骤，模拟人观察交互图像时的思维链条：\",\"识别交互部位（Object Interaction Perception）\",\"解析几何属性（Geometric Structure Reasoning）\",\"详细描述交互（Interaction Detailed Description）\",\"类比额外交互（Interactive Analogical Reasoning）\",\"每个子步骤都由一个 prompt 引导 MLLM（如 InternVL）做回答，从而获得：\",\"对象的交互区域\",\"Object Interaction Perception Prompt 1: Point out which part of the object in the image interacts with the person.\",\"🔹目标：定位交互发生的对象区域（如“水壶的壶嘴”）\",\"对应的几何属性\",\"Geometric Structure Reasoning Prompt 2: Explain why this part can interact from the geometric structure of the object.\",\"🔹目标：推理几何形态支持该交互（如“壶嘴上开口狭窄、带曲线”）\",\"当前交互行为\",\"Interaction Detailed Description Prompt 3: Describe the interaction between object and the person.\",\"🔹目标：细致地识别交互动作及其参与部位（如“用手握住壶把倒水”）\",\"潜在交互意图\",\"Interactive Analogical Reasoning Prompt 4: List two interactions that describe additional common interactions that the object can interact with people.\",\"🔹目标：推理除了当前交互以外，该物体常见的其他交互（如“开壶盖、抓握中部”）\",\"核心代码实现如下:\",\"# 1. 加载预训练多模态大模型 model = AutoModel.from_pretrained( path, torch_dtype=torch.bfloat16, #load_in_8bit=True, low_cpu_mem_usage=True, trust_remote_code=True, device_map=device_map).eval() tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False) # 2. 加载图像数据 image_path = 'PATH/Data/Kettle/Internet/pour/kettle_pour_1.jpg' pixel_values = load_image(image_path, max_num=12).to(torch.bfloat16).cuda() object = image_path.split('/')[-4] # 图像所属的物体名 # 3. 定位交互部位 question1 = f'Point out which part of the {object} in the image interacts with the person. If this part is different from the part of the {object} shown in the image that performs the main function, point out the part of the {object} that performs the main function shown in the image.' response1, history = model.chat(tokenizer, pixel_values, question1, generation_config, history=None, return_history=True) print(f'{response1}') # 4. 推理几何结构 question2 = f'Explain why this part can interact from the geometric structure of the {object}. Just give the final result in one sentence.' response2, history = model.chat(tokenizer, pixel_values, question2, generation_config, history=history, return_history=True) print(f'{response2}') # 5. 详细交互行为 question3 = f'Describe the interaction between {object} and the person in the image, including the interaction type, the interaction part of the {object}, and the interaction part of the person.' response3, history= model.chat(tokenizer, pixel_values, question3, generation_config, history=history, return_history=True) print(f'{response3}') # 6. 推测其他交互 question4 = f'List two interactions that describe additional common interactions that the {object} can interact with people, including the interaction type, the interaction part of the {object}, and the interaction part of the person.' response4, history= model.chat(tokenizer, pixel_values, question4, generation_config, history=history, return_history=True) print(f'{response4}') ''' Sample output 1. the spout of kettle. 2. a narrow opening, a slight curve and the spout's position at the top of the kettle. 3. pour the liquid from the spout of the kettle using people’s hand 4. grasp the kettle using person's hand around middle body, open the kettle using people's fingers on the lid object knowledge: the spout of kettle: a narrow opening, a slight curve and the spout's position at the top of the kettle. affordance/human knowledge: pour the liquid from the spout of the kettle using people’s hand, grasp the kettle using person's hand around handle, open the kettle using people's fingers on the lid ''\",\"其中:\",\"几何结构知识 = Prompt 1 + Prompt 2 的回答 = 交互部位 + 该部位的几何属性推理\",\"交互知识 = Prompt 3 + Prompt 4 的回答 = 当前交互 + 类比/补充的交互方式\",\"MHACoT 这个过程发生在数据集准备阶段。\"]},\"53\":{\"h\":\"数据集\",\"t\":[\"先了解一下GREAT项目对应的数据集目录结构:\",\"数据集的初始化:\",\"class PIAD(Dataset): def __init__(self, run_type, setting_type, point_path, img_path, text_hk_path, text_ok_path, pair=2, img_size=(224, 224)): super().__init__() self.run_type = run_type # 当前是训练/测试/验证环境 self.p_path = point_path # 点云索引文件路径 self.i_path = img_path # 图片索引文件路径 self.text_hk_path = text_hk_path # 物体几何结构文本数据文件路径 self.text_ok_path = text_ok_path # 人类交互文本数据文件路径 self.pair_num = pair # 控制每个 图像样本 对应多少个 3D点云样本 self.affordance_label_list = ['grasp', 'contain', 'lift', 'open', 'lay', 'sit', 'support', 'wrapgrasp', 'pour', 'move', 'display', 'push', 'listen', 'wear', 'press', 'cut', 'stab', 'carry', 'ride', 'clean', 'play', 'beat', 'speak', 'pull'] # 24 ... ''' Seen ''' # 43 if setting_type == 'Seen': number_dict = {'Bag': 0, 'Microphone': 0, 'Toothbrush': 0, 'TrashCan': 0, 'Bicycle': 0, 'Guitar': 0, 'Glasses': 0, 'Hat': 0, 'Microwave':0, 'Backpack': 0, 'Door':0, 'Scissors': 0, 'Bowl': 0, 'Baseballbat': 0, 'Mop': 0, 'Dishwasher': 0, 'Bed': 0, 'Keyboard': 0, 'Clock': 0, 'Vase': 0, 'Knife': 0, 'Suitcase': 0, 'Hammer': 0, 'Refrigerator': 0, 'Chair': 0, 'Umbrella': 0, 'Bucket': 0, 'Display': 0, 'Earphone': 0, 'Motorcycle': 0, 'StorageFurniture': 0, 'Fork': 0, 'Broom': 0, 'Skateboard': 0, 'Tennisracket': 0, 'Laptop': 0, 'Table':0, 'Bottle': 0, 'Faucet': 0, 'Kettle': 0, 'Surfboard': 0, 'Mug': 0, 'Spoon': 0 } # 读取所有图片路径，所有人类交互文本数据，所有物体几何结构文本数据 self.img_files = self.read_file(self.i_path) self.text_human_files = self.read_file(self.text_hk_path) self.text_object_files = self.read_file(self.text_ok_path) self.img_size = img_size if self.run_type == 'train': # 读取所有点云路径，同时记录每类物体对应的样本总量，比如: 椅子对应的点云一共1000个 self.point_files, self.number_dict = self.read_file(self.p_path, number_dict) self.object_list = list(number_dict.keys()) # 注意: Dict 按照key的插入顺序返回的 self.object_train_split = {} start_index = 0 # 记录每个物体对应的点云索引下标区间 for obj_ in self.object_list: temp_split = [start_index, start_index + self.number_dict[obj_]] self.object_train_split[obj_] = temp_split start_index += self.number_dict[obj_] else: self.point_files = self.read_file(self.p_path)\",\"为什么我们需要pair_num参数?\",\"问题背景：GREAT 需要将 2D 交互图像（Image）与 3D 点云（Point Cloud）的特征进行对齐，但同一物体的不同实例可能有几何差异（例如不同形状的椅子）。\",\"解决方案：通过为每张图像配对多个点云（pair_num > 1），模型能够学习从 多样化的几何变体 中提取共性的几何属性（如“可抓握”的共享结构特征），而不仅仅依赖单一实例。\",\"代码体现：在 getitem 中，训练时会对每个图像随机采样 pair_num 个同类别点云（见 point_sample_idx 的生成逻辑）\",\"GREAT 项目的数据组织中，将每个样本属于的物体类型，待预测功能区域类型全部隐含在了样本对应的文件路径中:\",\"获取数据:\",\" def __getitem__(self, index): # 1. 获取图片，人类交互文本，物体几何结构文本 img_path = self.img_files[index] text_hd = self.text_human_files[index] text_od = self.text_object_files[index] # 2.1 评估时需要标准的单一样本对比 if (self.run_type=='val'): point_path = self.point_files[index] else: # 2.2 从图片路径中截取得到物体名，交互行为名，点云索引下标区间 object_name = img_path.split('/')[-4] affordance_name = img_path.split('/')[-2] range_ = self.object_train_split[object_name] # 从索引区间中随机采样pair_num个点云样本 point_sample_idx = random.sample(range(range_[0],range_[1]), self.pair_num) # 3. 加载点云样本，同时判断是否与当前图片交互行为一致，不一致则重新随机选 for i ,idx in enumerate(point_sample_idx): while True: point_path = self.point_files[idx] sele_affordance = point_path.split('/')[-2] if sele_affordance == affordance_name: point_sample_idx[i] = idx break else: idx = random.randint(range_[0],range_[1]-1) # re-select idx Img = Image.open(img_path).convert('RGB') if(self.run_type == 'train'): Img = Img.resize(self.img_size) Img = img_normalize_train(Img) # 4. 加载列表中所有点云样本 Points_List = [] affordance_label_List = [] affordance_index_List = [] for id_x in point_sample_idx: point_path = self.point_files[id_x] # 加载点云数据和功能区域掩码(功能区域热力图) Points, affordance_label = self.extract_point_file(point_path) # （2048，3） Points,_,_ = pc_normalize(Points) Points = Points.transpose() # (3,2048) affordance_index = self.get_affordance_label(img_path) # 当前点云待预测的交互行为/功能区域类型 Points_List.append(Points) # 点云 affordance_label_List.append(affordance_label) # 功能区域热力图 affordance_index_List.append(affordance_index) # 待预测功能区域类型 else: Img = Img.resize(self.img_size) Img = img_normalize_train(Img) Point, affordance_label = self.extract_point_file(point_path) Point,_,_ = pc_normalize(Point) Point = Point.transpose() if(self.run_type == 'train'): # 图片 ， 交互信息文本，物体几何结构文本，点云样本列表，功能区域热力图列表，待预测功能区域类型列表 return Img, text_hd, text_od, Points_List, affordance_label_List, affordance_index_List else: return Img, text_hd, text_od, Point, affordance_label, img_path, point_path\"]},\"54\":{\"h\":\"模型\",\"t\":[\"class GREAT(nn.Module): ... def forward(self, img, xyz, text_human, text_object): ''' img: [B, 3, H, W] xyz: [B, 3, 2048] ''' B, C, N = xyz.size() # 1. 用Resnet18对图像进行编码，返回的高维隐向量维度为 (batch,512,7,7) -- （batch,channel,h,w) F_I = self.img_encoder(img) # 维度展平(batch,channel,h*w) F_i = F_I.view(B, self.emb_dim, -1) # 2， PointNet++ 对点云进行编码 F_p_wise = self.point_encoder(xyz) # 3. Roberta 对交互文本和几何结构文本进行编码 T_h= self.text_encoder(text_human) # (batch,3,512) T_o = self.text_encoder2(text_object) # (batch,1,512) # 4. 交互文本和几何结构文本的信息通过改良的交叉注意力机制进行交互融合 T_h_, T_o_ =self.affordance_dictionary_fusion(T_h, T_o) # 维度同上，均保持不变 # 5. 交互文本信息与图像信息进行融合 I_h = self.img_text_fusion(F_i,T_h_) # (batch,512,49) # 6. 几何结构文本信息与点云信息进行融合，然后进入pointnet++的特征传播阶段(插值阶段)，最后再与I_h进行交互融合 _3daffordance = self.decoder(T_o_, I_h.permute(0,2,1), F_p_wise) # T_o_(batch,1,512)，I_h.permute(batch,49,512)，点云特征 return _3daffordance\"]},\"55\":{\"h\":\"文本编码\",\"t\":[\"使用 RoBerta 对交互文本和几何结构文本进行编码这块，需要注意在对交互文本进行编码时，会按照 \\\",\\\" 将文本切分为多个句子，对每个句子独立进行编码:\",\"原始交互文本: pour the liquid from the spout of the kettle using people’s hand, grasp the kettle using person's hand around handle, open the kettle using people's fingers on the lid 切分后: pour the liquid from the spout of the kettle using people’s hand grasp the kettle using person's hand around handle open the kettle using people's fingers on the lid\",\"这样做的原因是因为交互文本由当前图片反映的交互行为和模型额外补充的当前物体存在的其他交互行为构成，他们之间的关系是独立的。而几何结构文本则是单一连贯的几何描述，无需切分，直接对整句进行编码。\"]},\"56\":{\"h\":\"改良的交叉注意力\",\"t\":[\"人类通过同时分析物体的 功能意图（如\\\"倒水\\\"）和 几何属性（如\\\"壶嘴的形状\\\"）来推断交互可能性。交叉注意力模拟了这种双向推理过程，通过建立意图与几何的显式关联，实现类似人类的类比推理能力。\",\"class Cross_Attention(nn.Module): ... def forward(self, hk, ok): ''' hk : human knowledge [B,N_hk,C] ok : object knowledge [B,N_ok,C] ''' # 用意图文本（如\\\"pour\\\"）筛选相关的几何特征（强化\\\"壶嘴\\\"结构，弱化\\\"把手\\\"） hk_q = self.proj_hq(hk) ok_key = self.proj_ok(ok) ok_value = self.proj_ov(ok) ok_key_ = torch.cat((hk_q,ok_key),dim=1) # 强化人类意图在物体语义推理中的引导作用 ok_value_ = torch.cat((hk_q,ok_value),dim=1) atten_I1 = torch.bmm(hk_q, ok_key_.permute(0, 2, 1))*self.scale atten_I1 = atten_I1.softmax(dim=-1) I_1 = torch.bmm(atten_I1, ok_value_) I_1 = self.layernorm(hk + I_1) # 用几何结构（如\\\"cylindrical handle\\\"）修正意图理解（排除与几何矛盾的意图） ok_q = self.proj_oq(ok) hk_key = self.proj_hk(hk) hk_value = self.proj_hv(hk) hk_key_ = torch.cat((ok_q,hk_key),dim=1) # 利用物体结构辅助推断更多人类交互意图 hk_value_ = torch.cat((ok_q,hk_value),dim=1) atten_I2 = torch.bmm(ok_q, hk_key_.permute(0, 2, 1))*self.scale atten_I2 = atten_I2.softmax(dim=-1) I_2 = torch.bmm(atten_I2, hk_value_) I_2 = self.layernorm(ok + I_2) return I_1, I_2\"]},\"57\":{\"h\":\"几何结构信息与交互信息的融合\",\"t\":[\"class affordance_dictionary_fusion(nn.Module): ... def forward(self,f_hk,f_ok): # 第一阶段：语义对齐（cross attention）➜ 把 Human 与 Object 信息“连接”起来 H, O = self.cross_atten(f_hk, f_ok) # 第二阶段：结构融合（self attention）➜ 在 Human 内部或 Object 内部 “整理、总结、泛化” H_= self.h_atten(H) O_= self.o_atten(O) return H_, O_\"]},\"58\":{\"h\":\"交互信息与图像特征的融合\",\"t\":[\"class img_text_fusion(nn.Module): def __init__(self, emb_dim = 512, proj_dim = 512): class SwapAxes(nn.Module): def __init__(self): super().__init__() def forward(self, x): return x.transpose(1, 2) super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim self.fusion = nn.Sequential( nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) self.reshape = nn.Sequential( nn.Linear(3, 3 * 8), # (batch,512,24) SwapAxes(), # (batch,24,512) nn.BatchNorm1d(3 * 8), nn.ReLU(), SwapAxes(), # (batch,512,24) nn.Linear(3 * 8, 49), # （batch,512,49) ) # F_i (batch,512,49) --> (batch,channel,H*W) def forward(self,F_i,T_h_): # T_h_(batch,3,512) ---> 转置后 (batch,512,3) --> reshape后 (batch,512,49) T_h_ = self.reshape(T_h_.permute(0,2,1)) # 拼接后: (batch,1024,49) I_ = torch.cat((F_i, T_h_),dim=1) # 通道维度上进行特征融合，同时降维: (batch.512,49) I_ = self.fusion(I_) return I_\"]},\"59\":{\"h\":\"解码阶段\",\"t\":[\"class Decoder(nn.Module): def __init__(self, additional_channel, emb_dim, proj_dim): class SwapAxes(nn.Module): def __init__(self): super().__init__() def forward(self, x): return x.transpose(1, 2) super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim #upsample self.fp3 = PointNetFeaturePropagation(in_channel=512+self.emb_dim, mlp=[768, 512]) self.fp2 = PointNetFeaturePropagation(in_channel=832, mlp=[768, 512]) self.fp1 = PointNetFeaturePropagation(in_channel=518+additional_channel, mlp=[512, 512]) self.cmff = Cross_Modal_Feature_Fusion(emb_dim, proj_dim) self.out_head = nn.Sequential( nn.Linear(self.emb_dim, self.emb_dim // 8), SwapAxes(), nn.BatchNorm1d(self.emb_dim // 8), nn.ReLU(), SwapAxes(), nn.Linear(self.emb_dim // 8, 1), ) self.reshape = nn.Sequential( nn.Linear(49, 49 * 8), SwapAxes(), nn.BatchNorm1d(49 * 8), nn.ReLU(), SwapAxes(), nn.Linear(49 * 8, 2048), ) self.sigmoid = nn.Sigmoid() self.fusion = nn.Sequential( nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) def forward(self, T_o, I_h, encoder_p): ''' T_o --->object knowledge embedding (batch,1,512) I_h ---> [B, N_i, C] (batch,49,512) encoder_p ---> [Hierarchy feature] ''' B, _, _ = I_h.shape # p_i[1]: (1,3,2048) , （1，320，512) , (1,512,128) , (1,512,64) --> (batch,features,points) # p_i[0] 为坐标 p_0, p_1, p_2, p_3 = encoder_p # 逐层点云特征列表 # 1. 传入数据维度: (1,1,512) , (1,64,512) , 点云特征和几何结构特征做特征融合 p_3[1] = self.cmff(T_o, p_3[1].transpose(-2, -1)) # (1,512,64) # 2. 进入PointNet++经典的特征传播阶段 up_sample = self.fp3(p_2[0], p_3[0], p_2[1], p_3[1]) # (1,512,128) up_sample = self.fp2(p_1[0], p_2[0], p_1[1], up_sample) # (1,512,512) up_sample = self.fp1(p_0[0], p_1[0], torch.cat([p_0[0], p_0[1]],1), up_sample) # (1,512,2048) # 3. I_h reshape后 (1,512,2048) F_I = self.reshape(I_h.permute(0,2,1)) # 4. 图像交互信息与点云特征做融合: 拼接后，通道维度上进行特征融合，同时降维: (1,512,2048) F_j = torch.cat((F_I, up_sample),dim=1) F_j_fusion = self.fusion(F_j) # 5. F_j_fusion.permute后(1,2048,512) --> (1,2048,1) _3daffordance = self.out_head(F_j_fusion.permute(0, 2, 1)) _3daffordance = self.sigmoid(_3daffordance) # 生成功能区域掩码 return _3daffordance\"]},\"60\":{\"h\":\"点云特征与几何结构特征的融合\",\"t\":[\"class Cross_Modal_Feature_Fusion(nn.Module): def __init__(self, emb_dim, proj_dim): class SwapAxes(nn.Module): def __init__(self): super().__init__() def forward(self, x): return x.transpose(1, 2) super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim self.cross_atten1 = Cross_Attention(emb_dim = self.emb_dim, proj_dim = self.proj_dim) # 假设输入数据维度为 (1,64,512) : 先降维，进行信息压缩 self.fc = nn.Sequential( nn.Linear(self.emb_dim, self.emb_dim//2), # (1,64,256) SwapAxes(), # (1,256,64) nn.BatchNorm1d(self.emb_dim // 2), nn.ReLU(), SwapAxes(), # (1,64,256) nn.Linear(self.emb_dim//2, self.emb_dim), # (1,64,512) SwapAxes(), # (1,512,64) nn.BatchNorm1d(self.emb_dim), SwapAxes(), # (1,64,512) ) self.norm1 = nn.LayerNorm(self.emb_dim) self.norm2 = nn.LayerNorm(self.emb_dim) self.pool = nn.AdaptiveAvgPool1d(1) self.fusion = nn.Sequential( nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) # (1,1,512) , (1,64,512) def forward(self,f_t,f_p): _, N_P, _ = f_p.size() # 1. 应用改良的交叉注意力机制 f_to, f_po = self.cross_atten1(f_t, f_p) # 2. 注意力后，加上经典的: x + FNN f_to = f_to + self.fc(f_to) f_po = f_po + self.fc(f_po) # 3. f_to.permute维度(1,512,1) --> pool后(1,512,1) f_t_p = self.pool(f_to.permute(0,2,1)) # 4. 维度扩展到64 --> (1,512,64) f_t_r = f_t_p.repeat(1, 1, N_P) # 5. f_po.permute维度(1,512,64) --> 拼接后(1,1024,64) joint = torch.cat((f_po.permute(0,2,1), f_t_r), dim = 1) # 6. 通道维度作信息融合(1,512,64) output = self.fusion(joint) return output\"]},\"61\":{\"h\":\"LMAffordance3D 模型代码解读与复现\",\"t\":[\"Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions 论文代码解读与复现\",\"论文: https://arxiv.org/abs/2504.04744 代码: https://github.com/cn-hezhu/LMAffordance3D\",\"由于论文数据集还未开源，加之原本在Github上开源的代码后续被下架，导致本论文复现流程暂时终止。\"]},\"62\":{\"h\":\"环境配置 (待完善)\",\"t\":[\"建议用Linux或者Windows系统进行测试，MacOS系统某些包的加载和依赖关系上存在问题，不方便进行处理。\"]},\"63\":{\"h\":\"模型结构\",\"t\":[\"模型结构图\"]},\"64\":{\"h\":\"LMAffordance3D\",\"t\":[\"class LMAffordance3D(Blip2Base): ... def forward(self, img, point, description, label, inference_mode=False): ''' img: [B, 3, H, W] -> 输入图像 (batch_size, channels, height, width) point: [B, 3, 2048] -> 点云数据 (batch_size, dimensions, num_points) description: 自然语言指令 (e.g., \\\"Grasp the bottle\\\") label: 真实标签，即每个点对应的 affordance 概率分布 (B, 2048, 1) inference_mode: 是否为推理模式（True/False） ''' # 获取输入维度信息 B, C, H, W = img.size() B, D, N = point.size() device = img.device # 获取设备信息（CPU/GPU） # Step 1: 提取图像和点云的特征 # -------------------------------------------------- # 图像编码器：ResNet18 提取 2D 特征 F2D ∈ RB×CI×H×W img_feature = self.img_encoder(img) # shape: [B, CI, H', W'] # 点云编码器：PointNet++ 提取 3D 特征 F3D ∈ RB×CP×NP point_feature = self.point_encoder(point) # shape: [B, CP, NP] # Step 2: 融合多模态空间特征 # -------------------------------------------------- # 使用 MLP 和自注意力机制融合图像与点云特征 spatial_feature = self.fusion(img_feature, point_feature) # shape: [B, NS, CS] # Step 3: 多模态特征投影到语言语义空间 # -------------------------------------------------- # 将融合后的空间特征通过适配器上采样到与语言模型匹配的维度 if self.has_qformer: ... # 如果使用 Q-Former，则进行额外处理 else: multi_embeds = self.adapter_up(spatial_feature) # shape: [B, NS, CL] image_atts = None # 默认图像注意力掩码为空 # Step 4: 对自然语言指令进行 Tokenization # -------------------------------------------------- # 设置 tokenizer 的 padding 和 truncation 方向 self.llm_tokenizer.padding_side = \\\"right\\\" self.llm_tokenizer.truncation_side = 'left' # 对语言指令进行分词，转换为 token ID 并生成 attention mask text_input_tokens = self.llm_tokenizer( description, return_tensors=\\\"pt\\\", padding=\\\"longest\\\", # 填充至最长序列长度 truncation=True, # 截断过长文本 max_length=self.max_txt_len, # 最大文本长度 ).to(device) # Step 5: 获取语言嵌入 # -------------------------------------------------- # 使用 LLM 的 embedding 层将 token ID 转换为嵌入向量 inputs_embeds = self.llm_model.get_input_embeddings()(text_input_tokens.input_ids) # shape: [B, NL, CL] （NL=token数，CL=语言嵌入维度） # Step 6: 拼接多模态嵌入与语言嵌入 # -------------------------------------------------- # 调用 concat_input 函数，将图像+点云特征插入语言嵌入中 llm_inputs, llm_attention_mask = self.concat_input( inputs_embeds, text_input_tokens.attention_mask, multi_embeds, image_atts ) # llm_inputs: [B, NL + NS, CL] # llm_attention_mask: [B, NL + NS] # Step 7: 使用 Vision-Language Model 进行联合推理 # -------------------------------------------------- # 在混合精度下运行 LLM，融合语言与视觉特征 with self.maybe_autocast(): hidden_states = self.llm_model( inputs_embeds=llm_inputs, attention_mask=llm_attention_mask, return_dict=False, # 返回 tuple 格式输出 ) # Step 8: 降维适配器 # -------------------------------------------------- # 通过适配器层将 LLM 输出映射回合适维度 hidden_states = self.adapter_down(hidden_states) # shape: [B, NS + NL, CS] # 分割出 semantic feature 和 instructional feature # 视觉语义特征 和 语言指令理解特征 semantic_feature, instructional_feature = torch.split( hidden_states, split_size_or_sections=spatial_feature.size(1), dim=1 ) # Step 9: 解码器融合所有特征以预测可操作性特征 # -------------------------------------------------- # 使用 cross-attention 融合 instruction, semantic, spatial features affordance_feature = self.affordance_decoder( spatial_feature, instructional_feature, semantic_feature ) # shape: [B, NA, CA] # Step 10: 使用分割头预测最终的 3D 可操作性热图 # -------------------------------------------------- out = self.head(spatial_feature, affordance_feature, point_feature) # 输出 shape: [B, 2048, 1]，表示每个点是否具有特定可操作性的概率 # Step 11: 推理或训练分支 # -------------------------------------------------- if inference_mode == True: return out # 仅返回预测结果 else: loss_hm = self.loss_hm(out, label) # 计算 heatmap 的损失（focal + dice） loss = loss_hm * self.w_hm # 加权总损失 return { \\\"out\\\": out, \\\"loss\\\": loss, \\\"loss_hm\\\": loss_hm }\"]},\"65\":{\"h\":\"Step 2: 融合多模态空间特征\",\"t\":[\"class Fusion(nn.Module): def __init__(self, emb_dim = 512, num_heads = 4): super().__init__() self.emb_dim = emb_dim # 对点积结果进行缩放，防止 softmax 梯度消失或爆炸。 self.div_scale = self.emb_dim ** (-0.5) self.num_heads = num_heads # 对图像和点云特征进行 非线性增强和空间对齐 ，使得它们能够在统一的语义空间中进行有效的跨模态交互。 self.mlp = nn.Sequential( nn.Conv1d(self.emb_dim, 2*self.emb_dim, 1, 1), nn.BatchNorm1d(2*self.emb_dim), nn.ReLU(), nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) self.img_attention = Self_Attention(self.emb_dim, self.num_heads) self.point_attention = Self_Attention(self.emb_dim, self.num_heads) self.joint_attention = Self_Attention(self.emb_dim, self.num_heads) def forward(self, img_feature, point_feature): ''' i_feature: [B, C, H, W] p_feature: [B, C, N_p] HW = N_i ''' B, C, H, W = img_feature.size() img_feature = img_feature.view(B, self.emb_dim, -1) #[B, C, N_i] point_feature = point_feature[-1][1] # 对图像和点云特征进行 非线性增强和空间对齐 ，使得它们能够在统一的语义空间中进行有效的跨模态交互。 p_feature = self.mlp(point_feature) i_feature = self.mlp(img_feature) # 跨模态注意力矩阵: 每个点云点与图像中每个位置之间的相似度得分 phi = torch.bmm(p_feature.permute(0, 2, 1), i_feature)*self.div_scale #[B, N_p, N_i] # 每列是一个 softmax 分布（每个图像位置对应的所有点云点）, 表示：“对于图像中的每一个位置，应该关注哪些点云点？” phi_p = F.softmax(phi,dim=1) # 每行是一个 softmax 分布（每个点云点对应的所有图像位置）, 表示：“对于点云中的每一个点，应该关注图像中的哪些位置？” phi_i = F.softmax(phi,dim=-1) # I_enhance 是图像 patch 引导下提取的点云信息增强后的图像特征 # 它不是直接包含原始图像 patch 的语义 # 而是通过“点云中相关点”的方式重构图像 patch 的语义 I_enhance = torch.bmm(p_feature, phi_p) #[B, C, N_i] # P_enhance 是每个点云局部区域关键点引导下提取的图像信息增强后的点云关键点局部区域特征 P_enhance = torch.bmm(i_feature, phi_i.permute(0,2,1)) #[B, C, N_p] # 在跨模态融合后，进一步提取各自模态内部的语义一致性与结构关系，形成更稳定的联合表示。 I = self.img_attention(I_enhance.mT) #[B, N_i, C] P = self.point_attention(P_enhance.mT) #[B, N_p, C] # 将图像patch和点云点拼接成一个统一的token序列 # 使用自注意力机制提炼两个模态之间的语义一致性 joint_patch = torch.cat((P, I), dim=1) multi_feature = self.joint_attention(joint_patch) #[B, N_p+N_i, C] return multi_feature\"]},\"66\":{\"h\":\"Step 3: 多模态特征投影到语言语义空间\",\"t\":[\" # 将融合后的 3D 和 2D 特征从原始嵌入维度 (self.emb_dim) 映射到 LLM（语言模型）所使用的隐藏状态空间维度 （self.llm_model.config.hidden_size）。 self.adapter_up = nn.Sequential( nn.Linear(self.emb_dim, self.emb_dim), nn.ReLU(), nn.Linear(self.emb_dim, self.llm_model.config.hidden_size) )\"]},\"67\":{\"h\":\"Step 6: 拼接多模态嵌入与语言嵌入\",\"t\":[\"def concat_input(self, input_embeds, input_atts, multi_embeds, image_atts=None): ''' 将语言嵌入（text embeddings）与多模态嵌入（如图像、点云等）拼接在一起， 构建 Vision-Language Model (VLM) 所需的输入格式。 Args: input_embeds: (batch_size, sequence_length, hidden_size) - 语言 token 经过 embedding 层后的结果。 input_atts: (batch_size, sequence_length) - 语言部分的 attention mask（1 表示有效，0 表示填充）。 multi_embeds: (batch_size, n, hidden_size) - 多模态嵌入（如图像或点云特征），形状为 [B, n, H]。 image_atts: (batch_size, n), optional - 多模态数据的 attention mask，默认为全 1（即所有 token 都有效）。 Returns: llm_inputs: (batch_size, total_length, hidden_size) - 拼接后的输入嵌入，供 LLM 使用。 llm_attention_mask: (batch_size, total_length) - 对应的注意力掩码。 ''' # 初始化用于存储每个样本拼接后输入和 attention mask 的列表 llm_inputs = [] llm_attention_mask = [] # 获取 batch size bs = multi_embeds.size()[0] # 对每个样本单独处理（逐个拼接） for i in range(bs): # 获取当前样本中多模态嵌入的维度信息：(n, dim) _, n, dim = multi_embeds.size() # 计算当前语言输入中有多少个有效 token（非 padding） this_input_ones = input_atts[i].sum() # 拼接嵌入向量： # 语言前半段（有效的部分）+ 多模态嵌入 + 语言后半段（padding 部分） llm_inputs.append( torch.cat([ input_embeds[i][:this_input_ones], # 有效语言部分 multi_embeds[i], # 插入的多模态嵌入 input_embeds[i][this_input_ones:] # 剩余的语言 padding 部分 ]) ) # 构建 attention mask： if image_atts is None: # 如果没有提供 image_atts，则默认多模态 token 都是有效的（mask 全为 1） llm_attention_mask.append( torch.cat([ input_atts[i][:this_input_ones], torch.ones((n), device=multi_embeds.device, dtype=torch.long), input_atts[i][this_input_ones:] ]) ) else: # 否则使用给定的 image_atts 来标记哪些多模态 token 是有效的 llm_attention_mask.append( torch.cat([ input_atts[i][:this_input_ones], image_atts[i], input_atts[i][this_input_ones:] ]) ) # 将 list 转换为 batched tensor llm_inputs = torch.stack(llm_inputs, 0) llm_attention_mask = torch.stack(llm_attention_mask, 0) # 返回拼接好的输入和 attention mask return llm_inputs, llm_attention_mask\"]},\"68\":{\"h\":\"Step 8: 降维适配器\",\"t\":[\" # 降维适配器：将 LLM 输出的隐藏状态映射回原始嵌入维度（self.emb_dim） self.adapter_down = nn.Sequential( nn.Linear(self.llm_model.config.hidden_size, self.llm_model.config.hidden_size), nn.ReLU(), nn.Linear(self.llm_model.config.hidden_size, self.emb_dim) )\"]},\"69\":{\"h\":\"Step 9: 解码器融合所有特征以预测可操作性特征\",\"t\":[\"class Affordance_Decoder(nn.Module): def __init__(self, emb_dim, proj_dim): super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim self.cross_atten = Cross_Attention(emb_dim = self.emb_dim, proj_dim = self.proj_dim) self.fusion = nn.Sequential( nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) def forward(self, query, key, value): ''' query: [B, N_p + N_i, C] -> spatial_feature (query) key: [B, N_l, C] -> instructional_feature (key) value: [B, N_l, C] -> semantic_feature (value) ''' B, _, C = query.size() # 调整 key 和 value 的形状为 [B, C, N_l] key = key.view(B, C, -1) # [B, C, N_l] value = value.view(B, C, -1) # [B, C, N_l] # 使用 cross attention 获取两个注意力加权结果 Theta_1, Theta_2 = self.cross_atten(query, key.mT, value.mT) # 将两个注意力输出拼接在一起 joint_context = torch.cat((Theta_1.mT, Theta_2.mT), dim=1) # [B, 2C, N_p + N_i] # 使用 Conv1D 融合通道信息 affordance = self.fusion(joint_context) # [B, C, N_p + N_i] # 调整输出格式为 [B, N_p + N_i, C] affordance = affordance.permute(0, 2, 1) # [B, N_p + N_i, C] return affordance\",\"class Cross_Attention(nn.Module): def __init__(self, emb_dim, proj_dim): \\\"\\\"\\\" 多模态交叉注意力模块（Cross-Attention Module）， 用于融合来自语言模型的不同语义信息，增强空间特征表达。 Args: emb_dim: 输入特征维度（embedding dimension），例如 LLM 的 hidden size（如 4096） proj_dim: 投影维度，用于降低计算复杂度，在 attention 中使用 \\\"\\\"\\\" super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim # 定义投影层，将输入映射到低维空间以进行 attention 计算 self.proj_q = nn.Linear(self.emb_dim, proj_dim) # query 投影 self.proj_sk = nn.Linear(self.emb_dim, proj_dim) # sub key 投影 self.proj_sv = nn.Linear(self.emb_dim, proj_dim) # sub value 投影 self.proj_ek = nn.Linear(self.emb_dim, proj_dim) # scene key 投影 self.proj_ev = nn.Linear(self.emb_dim, proj_dim) # scene value 投影 # 缩放因子，用于 attention 分数归一化 self.scale = self.proj_dim ** (-0.5) # 层归一化（LayerNorm），用于稳定训练过程 self.layernorm = nn.LayerNorm(self.emb_dim) def forward(self, obj, sub, scene): \\\"\\\"\\\" 执行交叉注意力机制，融合不同来源的信息： - obj: 空间特征（spatial feature），作为 query； - sub: 指令理解特征（instructional feature），作为第一个 attention 的 key 和 value； - scene: 视觉语义特征（semantic feature），作为第二个 attention 的 key 和 value； Args: obj: [B, N_p + HW, C] → spatial_feature（query 来源） sub: [B, HW, C] → instructional_feature（key/value 来源之一） scene: [B, HW, C] → semantic_feature（key/value 来源之二） Returns: I_1: 经过 attention 加权后的输出（第一分支） I_2: 经过 attention 加权后的输出（第二分支） \\\"\\\"\\\" B, seq_length, C = obj.size() # 获取 batch size 和通道维度 # 将输入分别投影到低维空间，便于后续 attention 计算 query = self.proj_q(obj) # [B, N_q, proj_dim] s_key = self.proj_sk(sub) # [B, N_i, proj_dim] s_value = self.proj_sv(sub) # [B, N_i, proj_dim] e_key = self.proj_ek(scene) # [B, N_e, proj_dim] e_value = self.proj_ev(scene) # [B, N_e, proj_dim] # 第一个 cross attention：使用 sub 的 key 和 value 增强 query atten_I1 = torch.bmm(query, s_key.mT) * self.scale # [B, N_q, N_i] atten_I1 = atten_I1.softmax(dim=-1) # softmax 归一化 I_1 = torch.bmm(atten_I1, s_value) # [B, N_q, proj_dim] # 第二个 cross attention：使用 scene 的 key 和 value 增强 query atten_I2 = torch.bmm(query, e_key.mT) * self.scale # [B, N_q, N_e] atten_I2 = atten_I2.softmax(dim=-1) I_2 = torch.bmm(atten_I2, e_value) # [B, N_q, proj_dim] # 使用残差连接 + LayerNorm 增强稳定性 I_1 = self.layernorm(obj + I_1) # [B, N_q, emb_dim] I_2 = self.layernorm(obj + I_2) # [B, N_q, emb_dim] return I_1, I_2\"]},\"70\":{\"h\":\"Step 10: 使用分割头预测最终的 3D 可操作性热图\",\"t\":[\"class Head(nn.Module): def __init__(self, additional_channel, emb_dim, N_p, N_raw): \\\"\\\"\\\" Head 模块用于最终的 3D 可操作性（affordance）预测。 它接收来自编码器和解码器的特征，并通过多尺度上采样与融合， 输出每个点云点的 affordance 热图（heatmap），表示该点是否具有可操作性。 Args: additional_channel: 额外通道数，例如法向量、颜色等信息 emb_dim: 特征维度（embedding dimension） N_p: point cloud token 数量（如 64） N_raw: 原始点云数量（如 2048） Notes: - 使用 PointNetFeaturePropagation 进行逐级上采样； - 结合全局池化增强语义表达； - 最终使用 MLP + Sigmoid 输出每个点的 affordance score； \\\"\\\"\\\" super().__init__() self.emb_dim = emb_dim self.N_p = N_p # point cloud token 数量 self.N_raw = N_raw # 原始点云数量（如 2048） # 多尺度上采样模块：PointNetFeaturePropagation # fp3: 输入为 [512 + emb_dim]，输出为 512 维度 self.fp3 = PointNetFeaturePropagation(in_channel=512+self.emb_dim, mlp=[768, 512]) self.fp2 = PointNetFeaturePropagation(in_channel=832, mlp=[768, 512]) self.fp1 = PointNetFeaturePropagation(in_channel=518+additional_channel, mlp=[512, 512]) # 全局平均池化层，压缩时间/空间维度 self.pool = nn.AdaptiveAvgPool1d(1) # 最终输出头：MLP + BatchNorm + ReLU + Sigmoid self.out_head = nn.Sequential( nn.Linear(self.emb_dim, self.emb_dim // 8), nn.BatchNorm1d(self.N_raw), # 对点数维度做 BN nn.ReLU(), nn.Linear(self.emb_dim // 8, 1), # 输出每个点的 affordance score nn.Sigmoid() # 输出范围 [0,1]，表示概率 ) def forward(self, multi_feature, affordance_feature, encoder_p): \\\"\\\"\\\" 执行 Head 模块的前向传播，生成最终的 3D affordance heatmap。 Args: multi_feature: [B, N_p + N_i, C] → 来自 Vision-Language Model 的拼接特征 affordance_feature: [B, N_p + N_i, C] → 来自 decoder 的可操作性特征 encoder_p: [p0, p1, p2, p3] → 编码器不同层级的点云特征 Returns: out: [B, N_raw, 1] → 每个点的 affordance score（概率值） \\\"\\\"\\\" B, N, C = multi_feature.size() # 解包编码器输出的不同层级特征 p_0, p_1, p_2, p_3 = encoder_p # 从 multi_feature 和 affordance_feature 中提取 point cloud token 部分 P_align, _ = torch.split(multi_feature, split_size_or_sections=self.N_p, dim=1) F_pa, _ = torch.split(affordance_feature, split_size_or_sections=self.N_p, dim=1) # 上采样过程：fp3 -> fp2 -> fp1 up_sample = self.fp3(p_2[0], p_3[0], p_2[1], P_align.mT) # [B, emb_dim, npoint_sa2] up_sample = self.fp2(p_1[0], p_2[0], p_1[1], up_sample) # [B, emb_dim, npoint_sa1] up_sample = self.fp1(p_0[0], p_1[0], torch.cat([p_0[0], p_0[1]], 1), up_sample) # [B, emb_dim, N_raw] # 对 F_pa 做全局池化，得到一个全局语义向量 F_pa_pool = self.pool(F_pa.mT) # [B, emb_dim, 1] # 将全局语义向量扩展回原始点云数量，实现 feature-wise attention affordance = up_sample * F_pa_pool.expand(-1, -1, self.N_raw) # [B, emb_dim, N_raw] # 输出 head：将特征映射到 0~1 的概率值，表示每个点是否具有可操作性 out = self.out_head(affordance.mT) # [B, N_raw, 1] return out\"]},\"71\":{\"h\":\"IAGNet 论文解读\",\"t\":[\"Grounding 3D Object Affordance from 2D Interactions in Images 论文解读\",\"论文: https://arxiv.org/abs/2303.10437 代码: https://github.com/yyvhang/IAGNet 数据集: https://drive.google.com/drive/folders/1F242TsdXjRZkKQotiBsiN2u6rJAGRZ2W\"]},\"72\":{\"h\":\"摘要\",\"t\":[\"这篇论文提出了一种新颖的任务设定：通过2D图像中的交互信息来预测3D物体的功能区域（affordance），旨在为具身智能体建立感知与操作之间的联系。作者设计了一个名为IAG（Interaction-driven 3D Affordance Grounding Network）的框架，通过联合区域对齐模块（JRA）解决不同来源物体区域的对齐问题，并通过功能揭示模块（ARM）建模交互上下文以明确功能区域。此外，作者还构建了一个包含图像-点云配对数据的数据集PIAD，用于支持该任务。实验结果表明，该方法在PIAD数据集上表现优异，验证了任务设定的可行性和方法的有效性。这一研究为功能学习领域提供了新的视角，并有望应用于机器人操作、增强现实等领域。\"]},\"73\":{\"h\":\"简介\",\"t\":[\"Gibson（2014）提出的“功能可供性”（affordance）概念，即物体支持的交互可能性，是连接具身智能体感知与操作的关键。现有研究主要分为两类：\",\"几何结构映射方法（如11、22）通过标注物体交互区域建立几何结构与功能的固定关联，但泛化性受限，且对多功能的相似结构易产生混淆（如图2(b)中椅子的“坐”与“移动”功能）。\",\"强化学习方法（如54）通过智能体在虚拟环境中主动交互学习功能，但搜索空间大、耗时严重。\",\"本文创新点：\",\"任务设定：首次提出通过2D交互图像预测3D物体功能区域（如图1），模拟人类通过观察学习物体功能的能力。\",\"核心挑战： \",\"对齐模糊性：2D演示与3D物体来自不同实例，需跨源对齐区域（图2(a)展示同类物体的结构相似性可辅助对齐）。\",\"功能模糊性：同一物体区域可能支持多功能（如“杯子”既可“握持”也可“盛放”），需通过交互上下文建模解决（图2(b)）。\",\"解决方案：\",\"IAG框架：包含JRA模块（通过密集跨模态相似性 对齐区域）和ARM模块（通过交叉注意力建模物体-主体/场景交互以揭示功能）。\",\"PIAD数据集：包含7,012个点云和5,162张图像，覆盖23类物体和17种功能，支持“可见”与“未见”场景的评估（图4）。\",\"意义：该方法摆脱了对几何标注或固定场景的依赖，为机器人操作、AR/VR等应用提供了更通用的功能理解范式。\"]},\"74\":{\"h\":\"相关工作\"},\"75\":{\"h\":\"\",\"t\":[\"现有研究可分为三类（如表1所示）：\",\"2D功能检测：\",\"早期工作（如12、69）从图像/视频中分割功能区域，但无法定位具体交互部位。\",\"语言辅助方法（如36）结合文本描述提升语义理解。\",\"3D功能定位：\",\"基于几何映射的方法（如11）直接关联结构与功能，泛化性差。\",\"强化学习方法（如54）通过智能体主动交互学习，但效率低。\",\"机器人操作应用：\",\"针对铰接物体（如48）设计功能热图，指导抓取和运动规划。\",\"本文区别：首次通过非配对的2D-3D数据学习功能，摆脱几何标注和固定场景限制。\"]},\"76\":{\"h\":\"\",\"t\":[\"现有方法依赖两类对齐策略：\",\"空间先验对齐：\",\"基于相机参数（如68、90）将点云投影到图像平面，需严格的空间对应。\",\"特征空间对齐：\",\"无相机参数方法（如1、6）直接建模跨模态特征相似性。\",\"本文创新：利用功能-结构的隐式关联（如图2(a)），在无空间先验下实现跨源特征对齐。\"]},\"77\":{\"h\":\"方法\"},\"78\":{\"h\":\"\",\"t\":[\"如图3所示，IAG网络输入为四元组 ，其中：\",\" 为点云坐标\",\" 为RGB图像\",\" 为图像中主体和物体的边界框\",\" 为功能类别标签\",\"处理流程：\",\"特征提取：\",\"图像分支：ResNet提取特征 \",\"点云分支：PointNet++提取特征 \",\"区域定位：\",\"通过ROI-Align获取物体/主体/场景特征 （）\",\"联合区域对齐（JRA模块）：\",\"计算密集跨模态相似性矩阵：\",\"通过自注意力建模模态内结构关系：\",\"联合注意力生成对齐特征 \",\"功能揭示（ARM模块）：\",\"交叉注意力建模交互上下文：\",\"融合生成功能表征 \",\"解码输出：\",\"功能类别预测 ：对 和 池化后拼接\",\"3D功能热图 ：通过特征传播层上采样：\"]},\"79\":{\"h\":\"\",\"t\":[\"总损失包含三项：\",\"功能分类损失：交叉熵损失监督 \",\"特征分布对齐损失：KL散度约束 与 分布：\",\"热图回归损失：Focal Loss + Dice Loss监督 \",\"最终损失为加权和：\"]},\"80\":{\"h\":\"\",\"t\":[\"JRA模块：通过跨模态相似性（）和联合注意力（）实现无先验对齐\",\"ARM模块：通过双路交叉注意力分别建模物体-主体（）和物体-场景（）交互\",\"互优化机制： 使功能表征与对齐特征相互增强（如图15所示）\"]},\"81\":{\"h\":\"代码\"},\"82\":{\"h\":\"数据集\",\"t\":[\"数据集目录下的组织方式:\",\"数据集初始化\",\"class PIAD(Dataset): def __init__(self, run_type, setting_type, point_path, img_path, box_path, pair=2, img_size=(224, 224)): super().__init__() self.run_type = run_type # train/val/test self.p_path = point_path self.i_path = img_path self.b_path = box_path # 记录物体边界框 self.pair_num = pair self.affordance_label_list = ['grasp', 'contain', 'lift', 'open', 'lay', 'sit', 'support', 'wrapgrasp', 'pour', 'move', 'display', 'push', 'listen', 'wear', 'press', 'cut', 'stab'] ... ''' Seen ''' if setting_type == 'Seen': number_dict = {'Earphone': 0, 'Bag': 0, 'Chair': 0, 'Refrigerator': 0, 'Knife': 0, 'Dishwasher': 0, 'Keyboard': 0, 'Scissors': 0, 'Table': 0, 'StorageFurniture': 0, 'Bottle': 0, 'Bowl': 0, 'Microwave': 0, 'Display': 0, 'TrashCan': 0, 'Hat': 0, 'Clock': 0, 'Door': 0, 'Mug': 0, 'Faucet': 0, 'Vase': 0, 'Laptop': 0, 'Bed': 0} # 读取出所有图片路径，存储了物体边界框文件路径 self.img_files = self.read_file(self.i_path) self.box_files = self.read_file(self.b_path) self.img_size = img_size if self.run_type == 'train': # 读取出所有点云文件路径,同时记录每类物体共对应多少不同的点云 self.point_files, self.number_dict = self.read_file(self.p_path, number_dict) self.object_list = list(number_dict.keys()) self.object_train_split = {} start_index = 0 # 记录每类物体对应的点云文件下标索引区间 for obj_ in self.object_list: temp_split = [start_index, start_index + self.number_dict[obj_]] self.object_train_split[obj_] = temp_split start_index += self.number_dict[obj_] else: self.point_files = self.read_file(self.p_path)\",\"获取数据\",\" def __getitem__(self, index): # 1. 获取图片，Box框文件路径 img_path = self.img_files[index] box_path = self.box_files[index] if (self.run_type=='val'): point_path = self.point_files[index] else: # 2. 从文件路径中提取物体名 object_name = img_path.split('_')[-3] # 3. 一张图片对应多张同物体但形状不同的点云图片 range_ = self.object_train_split[object_name] point_sample_idx = random.sample(range(range_[0],range_[1]), self.pair_num) Img = Image.open(img_path).convert('RGB') if(self.run_type == 'train'): # 4. 随机裁剪图片，同时获取裁剪后的物体框(交互主体框，目标物体框) Img, subject, object = self.get_crop(box_path, Img, self.run_type) # 5. 对图片进行缩放，同时等比例对物体框做同样的缩放 sub_box, obj_box = self.get_resize_box(Img, self.img_size, subject, object) sub_box, obj_box = torch.tensor(sub_box).float(), torch.tensor(obj_box).float() Img = Img.resize(self.img_size) Img = img_normalize_train(Img) Points_List = [] affordance_label_List = [] affordance_index_List = [] # 6. 加载点云 for id_x in point_sample_idx: point_path = self.point_files[id_x] Points, affordance_label = self.extract_point_file(point_path) Points,_,_ = pc_normalize(Points) Points = Points.transpose() affordance_label, affordance_index = self.get_affordance_label(img_path, affordance_label) Points_List.append(Points) affordance_label_List.append(affordance_label) affordance_index_List.append(affordance_index) else: ... if(self.run_type == 'train'): # 7. 图片，点云列表，点云功能区域掩码列表，点云功能区域索引列表，交互主体框，目标物体框 return Img, Points_List, affordance_label_List, affordance_index_List, sub_box, obj_box else: return Img, Point, affordance_label, img_path, point_path, sub_box, obj_box\"]},\"83\":{\"h\":\"模型\",\"t\":[\"class IAG(nn.Module): ... def forward(self, img, xyz, sub_box, obj_box): ''' img: [B, 3, H, W] xyz: [B, 3, 2048] sub_box: bounding box of the interactive subject obj_box: bounding box of the interactive object ''' B, C, N = xyz.size() ... # 1. ResNet18 编码图像 (batch,512,7,7) F_I = self.img_encoder(img) # 2. 利用ROI Align技术，得到目标物体区域特征，交互主体区域特征，背景区域特征 ROI_box = self.get_roi_box(B).to(device) F_i, F_s, F_e = self.get_mask_feature(img, F_I, sub_box, obj_box, device) # 背景区域特征图经过ROI Align映射为4*4大小的特征图 # ROI_box 大小为 7*7 , 正好为resnet18最后生成的特征图的分辨率, 因为背景区域大小等于特征图大小 F_e = roi_align(F_e, ROI_box, output_size=(4,4)) # F_i (batch,512,4,4) , F_s (batch,512,4,4) , F_e (batch,512,4,4) # 3. PointNet编码点云 # (B,3,2048) , (B,320,512) , (B,512,128) ， (B,512,64) F_p_wise = self.point_encoder(xyz) # 4. 建立目标物体局部交互区域到点云局部区域的特征对应映射关系 # (B,80,512) F_j = self.JRA(F_i, F_p_wise[-1][1]) # 5. # (B,80,512) affordance = self.ARM(F_j, F_s, F_e) # 6. 解码 _3daffordance, logits, to_KL = self.decoder(F_j, affordance, F_p_wise) return _3daffordance, logits, to_KL\",\"关于利用ROI Align技术，得到目标物体区域特征，交互主体区域特征，背景区域特征过程的实现细节如下:\",\" def get_mask_feature(self, raw_img, img_feature, sub_box, obj_box, device): raw_size = raw_img.size(2) current_size = img_feature.size(2) B = img_feature.size(0) # 1. 计算经过下采样得到的特征图相比于原始图片的缩小比例 scale_factor = current_size / raw_size # 2. 将交互主体框和目标物体框等比例缩小 sub_box[:, :] = sub_box[:, :] * scale_factor obj_box[:, :] = obj_box[:, :] * scale_factor # 3. 根据目标物体框，将掩码图像中目标物体所在区域激活，得到目标物体区域掩码 obj_mask = torch.zeros_like(img_feature) obj_roi_box = [] for i in range(B): obj_mask[i,:, int(obj_box[i][1]+0.5):int(obj_box[i][3]+0.5), int(obj_box[i][0]+0.5):int(obj_box[i][2]+0.5)] = 1 roi_obj = [obj_box[i][0], obj_box[i][1], obj_box[i][2]+0.5, obj_box[i][3]] # 对交互主体框位置进行精细调整(just a trick) roi_obj.insert(0, i) # 插入批次索引 -- ROI Align对齐方法需要 obj_roi_box.append(roi_obj) obj_roi_box = torch.tensor(obj_roi_box).float().to(device) sub_roi_box = [] # 4. 根据交互主体框，在目标物体区域掩码之上，激活交互主体所在区域 Scene_mask = obj_mask.clone() for i in range(B): Scene_mask[i,:, int(sub_box[i][1]+0.5):int(sub_box[i][3]+0.5), int(sub_box[i][0]+0.5):int(sub_box[i][2]+0.5)] = 1 roi_sub = [sub_box[i][0], sub_box[i][1], sub_box[i][2], sub_box[i][3]] roi_sub.insert(0,i) sub_roi_box.append(roi_sub) # 5. 借助取反激活图片背景区域 Scene_mask = torch.abs(Scene_mask - 1) # 6. 拿到图片背景区域特征图 Scene_mask_feature = img_feature * Scene_mask sub_roi_box = torch.tensor(sub_roi_box).float().to(device) # 7. 利用ROI Align技术，将目标物体区域框在特征图中框出的区域，映射为4*4大小的特征图 obj_feature = roi_align(img_feature, obj_roi_box, output_size=(4,4), sampling_ratio=4) # 8. 利用ROI Align技术，将交互主体区域框在特征图中框出的区域，映射为4*4大小的特征图 sub_feature = roi_align(img_feature, sub_roi_box, output_size=(4,4), sampling_ratio=4) # 9. 返回目标物体区域特征图，交互主体区域特征图，背景区域特征图(未经ROI Align进行映射) return obj_feature, sub_feature, Scene_mask_feature\",\"JRA 模块就是在图像和点云之间建立“局部区域级别”的对应关系，让 2D 交互信号能准确落到 3D 对象上；具体来说:\",\"图像分支提取到的是 2D 目标区域的特征，包含了交互提示（比如“人手接触杯子的边缘”）；\",\"点云分支提取到的是 3D 物体点云的区域特征，包含了几何结构（比如“杯子的边缘曲面”）；\",\"JRA 模块通过 \\\" 投影统一 → 跨模态相似性匹配 → 局部自注意力 → 全局自注意力 \\\" ，在共享空间里把图像的局部区域和点云的局部区域对应起来；\",\"这样，模型就能理解“图像里交互的这部分 → 点云里对应的这部分结构”，为后续 3D affordance grounding 提供支撑。\",\"class Joint_Region_Alignment(nn.Module): def __init__(self, emb_dim = 512, num_heads = 4): super().__init__() class SwapAxes(nn.Module): def __init__(self): super().__init__() def forward(self, x): return x.transpose(1, 2) self.emb_dim = emb_dim self.div_scale = self.emb_dim ** (-0.5) self.num_heads = num_heads self.to_common = nn.Sequential( nn.Conv1d(self.emb_dim, 2*self.emb_dim, 1, 1), nn.BatchNorm1d(2*self.emb_dim), nn.ReLU(), nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) self.i_atten = Inherent_relation(self.emb_dim, self.num_heads) self.p_atten = Inherent_relation(self.emb_dim, self.num_heads) self.joint_atten = Inherent_relation(self.emb_dim, self.num_heads) def forward(self, F_i, F_p): ''' i_feature: [B, C, H, W] p_feature: [B, C, N_p] HW = N_i ''' B,_,N_p = F_p.size() # (B,512,64) # 1. 物体区域特征图展平: (B,512,4,4) --> (B,512,4*4) F_i = F_i.view(B, self.emb_dim, -1) #[B, C, N_i] # 2. 通过共享MLP迫使图像和点云特征在相同空间分布，消除模态差异 I = self.to_common(F_i) # (B,512,16) P = self.to_common(F_p) # (B,512,64) # 3. 计算相似度矩阵: (B,64,512) * (B,512,16) = (B,64,16) phi = torch.bmm(P.permute(0, 2, 1), I)*self.div_scale #[B, N_p, N_i] phi_p = F.softmax(phi,dim=1) # 计算特征图中每个点和点云每个点特征的相似度 phi_i = F.softmax(phi,dim=-1) # 计算点云中每个点和特征图中每个点特征的相似度 # 4. 特征增强(按照相似度完成信息融合 + 自注意力完成内部信息建模) I_enhance = torch.bmm(P, phi_p) # (B,512,64) * (B,64,16) = （B,512,16） [B, C, N_i] P_enhance = torch.bmm(I, phi_i.permute(0,2,1)) # (B,512,16) * (B,16,64) = （B,512,64） [B, C, N_p] I_ = self.i_atten(I_enhance.mT) #[B, N_i, C] P_ = self.p_atten(P_enhance.mT) #[B, N_p, C] # I_ (B,16,512) , P_ (B,64,512) # 5. 联合建模: 拼接 (B,80,512) + 自注意力 joint_patch = torch.cat((P_, I_), dim=1) F_j = self.joint_atten(joint_patch) #[B, N_p+N_i, C] return F_j\",\"ARM 的任务是：在对齐后的 joint feature 基础上，融合交互主体和环境的语义线索，显式地“揭示”出物体上可能的 affordance 区域。\",\"换句话说，它要回答：\",\"“在这个交互场景里，物体的哪一部分因为主体和环境的作用而具备可交互潜能？”\",\"上下文注入 (Context injection)\",\"F_s 提供“谁在和物体交互”（比如人手/手臂）\",\"F_e 提供“交互发生的场景背景”\",\"将这些信息和 F_j 融合，可以避免仅凭物体几何去猜 affordance。\",\"显式区域挖掘 (Explicit affordance mining)\",\"对齐特征 F_j 已经把“图像交互提示区域 ↔ 点云几何局部”对应起来，但还没有明确说“这里就是 affordance 区域”。\",\"ARM 进一步处理后，输出一个更抽象的 affordance 语义表示，告诉 decoder 哪些区域应该被激活。\",\"输出 → 送入 Decoder\",\"affordance 被送进 self.decoder(F_j, affordance, F_p_wise)\",\"Decoder 再结合原始点云逐点特征，把这些抽象语义转化为 点级别的 affordance mask。\",\"可以把 JRA + ARM 的关系想成：\",\"JRA：帮你在“交互图片里的区域” 和 “点云里的几何部分”之间拉了一根线（对齐）。\",\"ARM：在这根线的两端加上“语义电流”（主体 & 背景），让网络明确知道哪些区域真正具备 affordance。\",\"ARM 模块的作用是将 JRA 对齐得到的图像–点云联合特征，与交互主体和环境语义结合，挖掘并生成显式的 affordance 表示，为 Decoder 输出逐点 affordance mask 提供语义指导。\",\"class Affordance_Revealed_Module(nn.Module): def __init__(self, emb_dim, proj_dim): class SwapAxes(nn.Module): def __init__(self): super().__init__() def forward(self, x): return x.transpose(1, 2) super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim self.cross_atten = Cross_Attention(emb_dim = self.emb_dim, proj_dim = self.proj_dim) self.fusion = nn.Sequential( nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) def forward(self, F_j, F_s, F_e): ''' F_j: [B, N_p + N_i, C] (B,80,512) 物体区域特征和点云特征的联合建模 F_s: [B, H, W, C] (B,512,4,4) 交互区域特征 F_e: [B, H, W, C] (B,512,4,4) 背景特征 ''' B,_,C = F_j.size() # 拉平: (B,512,4,4) --> (B,512,4*4) F_s = F_s.view(B, C, -1) #[B, N_i, C] F_e = F_e.view(B, C, -1) #[B, N_i, C] # 利用联合建模特征作为query，从目标主体区域特征和背景特征中提取相关信息分别单独加到自己身上 Theta_1, Theta_2 = self.cross_atten(F_j, F_s.mT, F_e.mT) #[B, C, N_p + N_i] # 通道维度完成拼接后，利用1x1卷积完成通道维度上的信息融合 joint_context = torch.cat((Theta_1.mT, Theta_2.mT), dim=1) #[B, 2C, N_p + N_i] affordance = self.fusion(joint_context) #[B, C, N_p + N_i] affordance = affordance.permute(0, 2, 1) #[B, N_p + N_i, C] return affordance # （B,80,512)\",\"class Decoder(nn.Module): def __init__(self, additional_channel, emb_dim, N_p, N_raw, num_affordance): \\\"\\\"\\\" Decoder 模块 参数: additional_channel: 附加输入通道数 emb_dim: 特征嵌入维度 N_p: 点云子集数量 (point number for part/point-level alignment) N_raw: 原始点云点数 num_affordance: affordance 分类数量 \\\"\\\"\\\" class SwapAxes(nn.Module): \\\"\\\"\\\"交换张量的第1维和第2维, 用于Linear/BN的维度匹配\\\"\\\"\\\" def __init__(self): super().__init__() def forward(self, x): # x: [B, N, C] -> [B, C, N] return x.transpose(1, 2) super().__init__() self.emb_dim = emb_dim self.N_p = N_p self.N = N_raw self.num_affordance = num_affordance # ---------- 特征传播层 (PointNet++ Feature Propagation) ---------- # 逐层上采样，将 encoder 输出的层次化点特征恢复到原始点数 N self.fp3 = PointNetFeaturePropagation(in_channel=512+self.emb_dim, mlp=[768, 512]) self.fp2 = PointNetFeaturePropagation(in_channel=832, mlp=[768, 512]) self.fp1 = PointNetFeaturePropagation(in_channel=518+additional_channel, mlp=[512, 512]) # 全局池化 (用于 part-level 和 image-level 特征压缩) self.pool = nn.AdaptiveAvgPool1d(1) # 输入 [B, C, N] -> 输出 [B, C, 1] # ---------- 输出头 (3D affordance 预测) ---------- self.out_head = nn.Sequential( nn.Linear(self.emb_dim, self.emb_dim // 8), # [B, N, C] -> [B, N, C/8] SwapAxes(), # [B, N, C/8] -> [B, C/8, N] 方便 BatchNorm1d nn.BatchNorm1d(self.emb_dim // 8), nn.ReLU(), SwapAxes(), # [B, C/8, N] -> [B, N, C/8] nn.Linear(self.emb_dim // 8, 1), # [B, N, C/8] -> [B, N, 1] ) # ---------- 分类头 (affordance 分类) ---------- self.cls_head = nn.Sequential( nn.Linear(2*self.emb_dim, self.emb_dim // 2), # [B, 2C] -> [B, C/2] nn.BatchNorm1d(self.emb_dim // 2), nn.ReLU(), nn.Linear(self.emb_dim // 2, self.num_affordance), # [B, C/2] -> [B, num_affordance] nn.BatchNorm1d(self.num_affordance) ) self.sigmoid = nn.Sigmoid() def forward(self, F_j, affordance, encoder_p): \\\"\\\"\\\" 前向传播 输入: F_j: [B, N_p + N_i, C] (joint features, part/image 对齐后的特征) affordance: [B, N_p + N_i, C] (affordance 特征) encoder_p: [p0, p1, p2, p3] (encoder 分层特征, PointNet++ 输出) 输出: _3daffordance: [B, N, 1] (点云每个点的 affordance 激活概率) logits: [B, num_affordance] (全局 affordance 分类结果) [F_ia^T, I_align^T]: [B, C, N_i], [B, C, N_i] (image-aligned features) \\\"\\\"\\\" B, _, _ = F_j.size() p_0, p_1, p_2, p_3 = encoder_p # --- 将 joint feature 拆成 part-aligned (P_align) 和 image-aligned (I_align) --- P_align, I_align = torch.split(F_j, split_size_or_sections=self.N_p, dim=1) # P_align: [B, N_p, C] # I_align: [B, N_i, C] # --- 将 affordance 特征拆分为 part-level 和 image-level --- F_pa, F_ia = torch.split(affordance, split_size_or_sections=self.N_p, dim=1) # F_pa: [B, N_p, C] # F_ia: [B, N_i, C] # --- 上采样特征 (逐级恢复点云分辨率) --- # p_k: [点坐标, 点特征] up_sample = self.fp3(p_2[0], p_3[0], p_2[1], P_align.mT) # P_align.mT: [B, C, N_p] -> 融合到 SA2 层点数 (npoint_sa2) # 输出: [B, C, npoint_sa2] up_sample = self.fp2(p_1[0], p_2[0], p_1[1], up_sample) # 输出: [B, C, npoint_sa1] up_sample = self.fp1(p_0[0], p_1[0], torch.cat([p_0[0], p_0[1]],1), up_sample) # 输出: [B, C, N] (恢复到原始点数 N) # --- 全局池化 (part/image 特征池化) --- F_pa_pool = self.pool(F_pa.mT) # [B, C, N_p] -> [B, C, 1] F_ia_pool = self.pool(F_ia.mT) # [B, C, N_i] -> [B, C, 1] # --- 分类预测 (全局 affordance 类别预测) --- logits = torch.cat((F_pa_pool, F_ia_pool), dim=1) # [B, 2C, 1] logits = self.cls_head(logits.view(B,-1)) # [B, num_affordance] # --- 3D affordance 区域预测 (点级别) --- _3daffordance = up_sample * F_pa_pool.expand(-1, -1, self.N) # up_sample: [B, C, N] # F_pa_pool: [B, C, 1] -> expand: [B, C, N] # 相乘: [B, C, N] _3daffordance = self.out_head(_3daffordance.mT) # _3daffordance.mT: [B, N, C] # out_head: [B, N, 1] _3daffordance = self.sigmoid(_3daffordance) # [B, N, 1], 每个点的 affordance 概率 (0~1) # 返回: # - 点级别 3D affordance mask # - 全局 affordance 分类结果 # - image-aligned 的特征 return _3daffordance, logits, [F_ia.mT.contiguous(), I_align.mT.contiguous()]\"]},\"84\":{\"h\":\"LASO 模型代码解读与复现\",\"t\":[\"LASO: Language-guided Affordance Segmentation on 3D Object 论文代码解读与复现\",\"论文: https://openaccess.thecvf.com/content/CVPR2024/papers/Li_LASO_Language-guided_Affordance_Segmentation_on_3D_Object_CVPR_2024_paper.pdf 代码: https://github.com/yl3800/LASO\",\"这篇论文提出了一项新的任务和一个配套的数据集，旨在推动 语言引导下的 3D对象功能区域分割（Language-guided Affordance Segmentation on 3D Object, 简称 LASO）。\"]},\"85\":{\"h\":\"数据集\"},\"86\":{\"h\":\"1. 基础数据来源\",\"t\":[\"数据集基于 3D-AffordanceNet 提供的点云和功能区域标注构建：\",\"每个物体都以点云形式表示；\",\"点云中的每个点被标注为支持一个或多个功能类型（multi-class affordance labels），例如 grasp、open、lift、move 等；\",\"这些功能标注是人工标注的，具有语义意义；\",\"为什么使用 3D-AffordanceNet？\",\"因为它提供了高质量的点云和功能标注，能够很好地支持 LASO 的目标：根据自然语言问题找出与之相关的功能区域。\"]},\"87\":{\"h\":\"2. 构建问题（Question Crafting）\",\"t\":[\"选取物体-功能组合： \",\"从 3D-AffordanceNet 中选取了 58 种物体-功能组合（如 mug-grasp、door-open 等）；\",\"手工设计问题： \",\"对每种组合手工编写 5 个代表性问题；\",\"使用 GPT-4 扩展生成更多问题： \",\"使用 GPT-4 为每个组合额外生成 10 个问题；\",\"总共得到 870 个专家设计的问题（58 × 15 = 870）；\",\"Affordance-Question数据可视化\",\"在扩展过程中，GPT-4 生成的问题遵循以下三个关键原则，以确保问题多样性和语义丰富性：\",\"原则\",\"描述\",\"Contextual Enrichment（上下文丰富化）\",\"添加更多上下文细节，使问题更具体地连接目标对象的功能；例：将 “Grasping scissors: top choice?” 改为 “Identify the key points on the scissors that ensure successful grasping.”\",\"Concise Phrasing（简洁表达）\",\"提炼问题本质，使其简短但仍有意义；\",\"Structural Diversity（结构多样性）\",\"使用不同句式结构（疑问句、陈述句等），防止模型偏向特定句式或长度；\"]},\"88\":{\"h\":\"3. 标注 GT Mask（Ground Truth Mask）\",\"t\":[\"对于每个问题，结合其对应的功能类型和原始点云标注信息，构造出对应的二值掩码 gt_mask：\",\"每个点是否属于当前问题描述的功能区域；\",\"gt_mask 是 (N,) 形状的一维数组，其中 N 是点数；\",\"数值可以是 0/1（binary mask），也可以是软标签（soft label），表示点属于该功能区域的概率；\",\"软标签通常用于边界模糊区域，反映点与功能核心区域的距离远近；\",\"💡 注意：这些功能标签仅用于构造问题和定位正确功能区域，在训练和测试中不作为显式监督信号。\"]},\"89\":{\"h\":\"4. 数据集组织方式\",\"t\":[\"数据总量：\",\"总样本数：19,751 个点云-问题配对；\",\"物体类别数：23 类；\",\"功能类型数：17 类；\",\"问题总数：870 个专家设计的问题；\",\"每个物体类别可有多个形状实例；\",\"一个问题可以作用于多个物体类别（泛化能力）；\",\"数据集设置（两种模式）：\",\"🔹 Seen（见过）\",\"训练和测试阶段共享相似的物体类别和功能类型的分布；\",\"目的是评估模型在熟悉场景下的表现；\",\"🔹 Unseen（未见）\",\"某些功能类型在特定物体类别下会从训练集中省略，但在测试集中保留；\",\"目的是测试模型对新组合的泛化能力；\",\"例如：模型在训练期间学会了抓取包和杯子，但测试时要求“抓取耳机”——这是训练中未曾遇到过的功能-物体组合；\",\"数据划分方式：\",\"分区\",\"物体类别数\",\"问题数\",\"样本数\",\"Train\",\"6883\",\"638\",\"16,120\",\"Val\",\"516\",\"58\",\"1,215\",\"Test\",\"1035\",\"174\",\"2,416\"]},\"90\":{\"h\":\"5. 数据增强与配对策略\",\"t\":[\"训练阶段：\",\"每次迭代中，每个形状实例随机匹配一个与其功能类型一致的问题；\",\"随机配对使模型暴露于各种语义上下文中，提升泛化能力；\",\"推理阶段（验证 & 测试）：\",\"问题配对是固定的；\",\"所有问题专属于评估阶段，不在训练中透露；\",\"确保推理一致性，保持评估完整性；\"]},\"91\":{\"h\":\"6. 数据集统计信息（来自论文图3）\",\"t\":[\"维度\",\"内容\",\"功能类型\",\"17 类，如 grasp、open、lift、move 等\",\"物体类别\",\"23 类，如 mug、microwave、chair、door 等\",\"物体-功能组合\",\"58 种唯一组合（object-affordance pairs）\",\"问题总数\",\"870 个定制化问题\",\"点云-问题配对\",\"19,751 对\",\"点云来源\",\"来自 3D-AffordanceNet，每个点云约 2048 个点\"]},\"92\":{\"h\":\"7. 代码实现\",\"t\":[\"数据集初始化的核心代码实现如下:\",\"class AffordQ(Dataset): def __init__(self, split='train', **kwargs ): # 数据集存放目录 data_root='LASO_dataset' # 数据集类型: 训练集，评估集，测试集 self.split = split # 所支持的23种物体类型和17种功能类型 classes = [\\\"Bag\\\", \\\"Bed\\\", \\\"Bowl\\\",\\\"Clock\\\", \\\"Dishwasher\\\", \\\"Display\\\", \\\"Door\\\", \\\"Earphone\\\", \\\"Faucet\\\", \\\"Hat\\\", \\\"StorageFurniture\\\", \\\"Keyboard\\\", \\\"Knife\\\", \\\"Laptop\\\", \\\"Microwave\\\", \\\"Mug\\\", \\\"Refrigerator\\\", \\\"Chair\\\", \\\"Scissors\\\", \\\"Table\\\", \\\"TrashCan\\\", \\\"Vase\\\", \\\"Bottle\\\"] afford_cl = ['lay','sit','support','grasp','lift','contain','open','wrap_grasp','pour', 'move','display','push','pull','listen','wear','press','cut','stab'] # 建立物体类型和功能类型的索引映射关系，神经网络模型只认识数字 self.cls2idx = {cls.lower():np.array(i).astype(np.int64) for i, cls in enumerate(classes)} self.aff2idx = {cls:np.array(i).astype(np.int64) for i, cls in enumerate(afford_cl)} # 加载标注数据 with open(os.path.join(data_root, f'anno_{split}.pkl'), 'rb') as f: self.anno = pickle.load(f) # 加载点云数据 with open(os.path.join(data_root, f'objects_{split}.pkl'), 'rb') as f: self.objects = pickle.load(f) # 加载58种物体-功能组合的标注数据 (数据组织形式，参考上文的 Affordance-Question数据可视化图) self.question_df = pd.read_csv(os.path.join(data_root, 'Affordance-Question.csv')) # sort anno by object class and affordance type -- 遍历标注数据列表 self.sort_anno ={} for item in sorted(self.anno, key=lambda x: x['class']): # 获取当前样本的物体类别和物体信息值: 点云ID, 功能区域掩码, 功能类别 key = item['class'] value = {'shape_id': item['shape_id'], 'mask': item['mask'], 'affordance': item['affordance']} # 每种物体可以对应多种形状实例和功能类别 if key not in self.sort_anno: # 如果当前物体类别不在排序后的字典中，直接添加 self.sort_anno[key] = [value] else: # 如果当前物体类别在排序后的字典中，将当前样本的物体信息值追加到对应列表中 self.sort_anno[key].append(value)\",\"加载的标注数据中每个样本的组织形式如下:\",\"shape_id ：点云ID\",\"class ：物体类别（如bed）\",\"affordance ：功能类别（如lay）\",\"mask ：功能区域掩码（点级别标注）\",\"标注数据组织形式\",\"点云数据组织形式\",\"每种物体可以对应多种形状实例和功能类别\",\"获取样本的代码实现:\",\" def __getitem__(self, index): # 根据样本索引取出样本数据 data = self.anno[index] # 获取当前样本对应的点云ID shape_id = data['shape_id'] # 获取当前样本对应的物体类别 cls = data['class'] # 获取当前样本对应的功能类型 affordance = data['affordance'] # 获取当前样本对应的功能区域掩码 gt_mask = data['mask'] # 取出当前样本对应的点云数据 ，（2048,3) point_set = self.objects[str(shape_id)] # 对点云数据进行归一化处理，消除尺度差异 point_set,_,_ = pc_normalize(point_set) # 对点云数据进行转置操作 ，（3,2048) point_set = point_set.transpose() # 获取当前样本对应的问题文本(训练: 随机选； 验证&测试: 固定返回问题0) question = self.find_rephrase(self.question_df, cls, affordance) # 获取当前功能类型对应的索引值 affordance = self.aff2idx[affordance] # 返回: 点云数据， 物体类别索引， 功能区域掩码， 问题文本， 功能类型索引 return point_set, self.cls2idx[cls], gt_mask, question, affordance def find_rephrase(self, df, object_name, affordance): # 如果当前是训练模式，则从问题1～15中随机选择一个问题，否则固定返回问题0 qid = str(np.random.randint(1, 15)) if self.split == 'train' else '0' qid = 'Question'+qid # 从 DataFrame df 中筛选出同时满足 物体名称匹配 和 功能属性匹配 的行，并仅保留 qid 指定的列，也就是取出上面随机选择的问题文本 result = df.loc[(df['Object'] == object_name) & (df['Affordance'] == affordance), [qid]] # 问题文本不为空，则返回该问题文本 if not result.empty: # return result.index[0], result.iloc[0]['Rephrase'] return result.iloc[0][qid] else: raise NotImplementedError\"]},\"93\":{\"h\":\"8. 总结\",\"t\":[\"LASO 数据集基于 3D-AffordanceNet 的点云和功能标注，结合人工+GPT-4 生成的多样化问题，构造出 19,751 个点云-问题配对，旨在实现语言引导下的 3D 功能区域分割，推动 3D 视觉与大语言模型（LLM）的深度融合。\"]},\"94\":{\"h\":\"模型实现\",\"t\":[\"论文提出了一个全新的模型：PointRefer，用于解决一个新颖的任务 —— 语言引导的 3D 对象功能区域分割（LASO）。\",\"模型目标： 给定一个 3D 点云对象和一个自然语言问题（例如：“Where would you grasp this mug?”），PointRefer 的目标是预测出与该问题相关的点云区域，即生成一个二值掩码，表示哪些点属于目标功能区域。\",\"PointRefer 包括以下核心模块：\",\"3D 骨干网络（3D Backbone）\",\"使用 PointNet++ 编码点云特征；\",\"多阶段编码-解码结构提取多尺度点特征；\",\"自适应融合模块（Adaptive Fusion Module, AFM）\",\"在不同解码层注入语言信息；\",\"实现语言引导下的跨模态融合；\",\"增强点特征的语义判别能力；\",\"参考点解码器（Referred Point Decoder, RPD）\",\"引入一组可学习的“问题条件化查询”（affordance queries）；\",\"利用 Transformer 解码器将这些查询与点云特征进行交互；\",\"生成动态卷积核（dynamic kernels）；\",\"最终通过卷积操作生成分割掩码；\",\"PointRefer模型结构图\",\"PointRefer 前向传播过程如下:\",\"class PointRefer(nn.Module): # 传入question文本 和 point点云数据 def forward(self, text, xyz): ''' text: [B, L, 768] xyz: [B, 3, 2048] -- (b,c,n) ''' B, C, N = xyz.size() # 1. Encoding 过程 # 1.1 Language Encoding 使用RoBert编码文本 t_feat, t_mask = self.forward_text(list(text), xyz.device) # [batch, q_len, d_model] # 1.2 BackBone Encoding 使用PointNet++编码点云 F_p_wise = self.point_encoder(xyz) \\\"\\\"\\\" Decoding \\\"\\\"\\\" # 1.3 PointNet++ 逐级做点集抽象得到的每层的点集坐标和点集特征集合 p_0, p_1, p_2, p_3 = F_p_wise # 2.Backbone Decoding过程 # 2.1 点集集合中每个点的特征和文本特征信息进行融合,传入的点集特征集合经过转置处理后的维度为: (b, n, c) p_3[1] = self.gpb(t_feat, p_3[1].transpose(-2, -1)).transpose(-2, -1) # 2.2 PointNet++ 特征传播阶段: 上采样过程中，上一层点集中的点特征重建过程中，充分吸收了高级区域抽象特征和文本特征 up_sample = self.fp3(p_2[0], p_3[0], p_2[1], p_3[1]) #[B, emb_dim, npoint_sa2] up_sample = self.gpb(t_feat, up_sample.transpose(-2, -1)).transpose(-2, -1) up_sample = self.fp2(p_1[0], p_2[0], p_1[1], up_sample) #[B, emb_dim, npoint_sa1] up_sample = self.gpb(t_feat, up_sample.transpose(-2, -1)).transpose(-2, -1) # 2.3 特征传播阶段结束: 一步步重建回原始点数量 128->256->512->1024->2048 up_sample = self.fp1(p_0[0], p_1[0], torch.cat([p_0[0], p_0[1]],1), up_sample) #[B, emb_dim, N] # 3. Referred Point Decoding过程 t_feat = self.decoder(t_feat, up_sample.transpose(-2, -1), tgt_key_padding_mask=t_mask, query_pos=self.pos1d) # b,l,c t_feat *= t_mask.unsqueeze(-1).float() _3daffordance = torch.einsum('blc,bcn->bln', t_feat, up_sample) _3daffordance = _3daffordance.sum(1)/(t_mask.float().sum(1).unsqueeze(-1)) _3daffordance = torch.sigmoid(_3daffordance) return _3daffordance.squeeze(-1)\",\"论文中所给的模型架构图中的Encoder layer指的是PointNet++中提供的PointNetSetAbstractionMsg多尺度分组点集特征抽取类\",\"论文中所给的模型架构图中的Decoder layer指的是PointNet++中提供的PointNetFeaturePropagation特征传播类\"]},\"95\":{\"h\":\"AFM 自适应融合模块\",\"t\":[\"在 LASO 任务中，模型需要根据自然语言问题（如 “Where to grasp?”）识别点云中的功能区域。由于目标功能区域的尺度、形状多样，传统方法难以适应不同情况。为此，作者设计了 AFM 模块，以增强 PointNet++ 解码过程中点特征的语言引导能力。\",\"AFM 的目标是：在不同解码阶段注入语言线索（text clues），将文本语义信息与点云特征进行跨模态融合，逐步以自上而下的方式细化点特征图，从而提升模型对多尺度、多形状的功能区域的感知能力。\",\"AFM 遵循一个 瓶颈式架构（bottleneck architecture），包含三个关键步骤：\",\"Grouping（分组）\",\"Mixing（混合）\",\"Ungrouping（解组）\",\"这三个步骤构成了一个完整的跨模态融合流程。\"]},\"96\":{\"h\":\"1️⃣ Grouping：文本引导的点特征分组\",\"t\":[\"输入：\",\"X ∈ R^{L×d}：问题编码后的文本特征（由 RoBERTa 编码得到）\",\"P ∈ R^{T×d}：某一层解码器输出的点特征，其中 T 表示该层点数\",\"处理过程：\",\"使用一个轻量级的交叉注意力模块，将文本特征作为查询（query），点特征作为键（key）和值（value），输出分组标记 G：\",\"其中：\",\" 是一个线性变换；\",\"注意力机制使得每个文本 token 对应一组相关的点特征；\",\"分组操作实现了“语言引导的点特征筛选”。\",\"重点是如何理解这里的分组: 每个文本Token询问所有点Key后，知道了哪些点跟自身的相关度更大，因此加权融合的时候，侧重于给这些点的特征分配更大的融合权重。\",\"这部分代码实现如下:\",\"# group_layer 的实现 class LightGroupAttnBlock(nn.Module): # query 是RoBerta编码后的文本特征 , (b,l,c) # key和value都是点云特征 , (b,n,c) def forward(self, query, key, value, q_mask=None): def _inner_forward(query, key, value): q = self.norm_query(query) k = q if self.key_is_query else self.norm_key(key) v = k if self.value_is_key else self.norm_value(value) # 让每个语言 token 去关注点云中最相关的区域，并在此基础上强化自身的语义表达。 # 加上原始 X 是一种残差连接（Residual Connection），可以确保语言语义不会丢失。 x = self.attn(q, k, v, q_mask) + self.drop(q) return x return _inner_forward(query, key, value)\"]},\"97\":{\"h\":\"2️⃣ Mixing：MLP-Mixer 进行组内和通道间的信息混合\",\"t\":[\"MLP-Mixer 是一种 基于 MLP（多层感知机）的视觉模型架构 ，由 Google Research 在 2021 年提出。它不使用任何注意力机制，而是通过 空间混合（mixing）和通道混合（mixing）操作 来实现全局信息建模。\",\"MLP-Mixer: An all-MLP Architecture for Vision\",\"MLP-Mixer 的核心思想是：用 MLP 替代 Transformer 中的自注意力机制 ，从而减少计算复杂度并保持性能。\",\"Token-mixing MLP\",\"对所有点/patch 的相同通道进行混合；\",\"相当于跨空间位置的信息交换；\",\"类似于 CNN 中的空间卷积；\",\"Channel-mixing MLP\",\"对每个 token 的所有通道进行处理；\",\"提取更高级的特征表示；\",\"类似于传统的全连接层或 1x1 卷积；\",\"这两个操作交替进行，形成一个类似于 Transformer 的堆叠结构，但完全不使用注意力机制。\",\"输入：\",\"G ∈ R^{L×d}：分组后的文本引导特征\",\"处理过程：\",\"使用 MLP-Mixer 来更新分组特征，生成融合特征 F：\",\"其中：\",\"MLP₁ 负责组内信息混合（token 内部）；\",\"MLP₂ 负责通道间信息混合（feature channel）；\",\"两个 MLP 交替作用，实现跨模态信息的充分交互；\",\"最终输出融合特征 F；\",\"这部分代码实现如下:\",\"# mixer 的实现 class MLPMixerLayer(nn.Module): def __init__(self, num_patches, embed_dims, patch_expansion, channel_expansion, drop_out, **kwargs): super().__init__() patch_mix_dims = int(patch_expansion * embed_dims) # 16 channel_mix_dims = int(channel_expansion * embed_dims) # 128 self.patch_mixer = nn.Sequential( nn.Linear(num_patches, patch_mix_dims, bias=False), # try here nn.GELU(), nn.Dropout(drop_out), nn.Linear(patch_mix_dims, num_patches, bias=False), nn.Dropout(drop_out) ) self.channel_mixer = nn.Sequential( nn.Linear(embed_dims, channel_mix_dims), nn.GELU(), nn.Dropout(drop_out), nn.Linear(channel_mix_dims, embed_dims), nn.Dropout(drop_out) ) self.norm1 = nn.LayerNorm(embed_dims) self.norm2 = nn.LayerNorm(embed_dims) # x 分组后的文本引导特征 : (b,l,c) def forward(self, x): # x 转置后: (b,c,l) , patch_mixer 负责组内信息混合（token 内部） x = x + self.patch_mixer(self.norm1(x).transpose(1,2)).transpose(1,2) # channel_mixer 负责通道间信息混合（feature channel) x = x + self.channel_mixer(self.norm2(x)) return x\"]},\"98\":{\"h\":\"3️⃣ Ungrouping：将融合特征映射回点空间\",\"t\":[\"输入：\",\"原始点特征 P；\",\"融合后的文本特征 F；\",\"处理过程：\",\"使用另一个注意力模块，将融合特征重新分配给每个点：\",\"其中：\",\"W₂ 是线性变换；\",\"注意力机制让每个点从融合特征中提取相关信息；\",\"输出 P_m 是语言增强后的点特征；\",\"最后加上残差连接形成最终输出 P_o：\",\"这个 P_o 就是经过 AFM 增强的点特征图，用于后续分割掩码预测。\",\"class FullAttnCatBlock(nn.Module): # query 为点云: (b,n,c) , key和value为融合后的文本特征: (b,l,c) def forward(self, query, key, value, key_padding_mask=None): def _inner_forward(query, key, value, key_padding_mask): q = self.norm_query(query) k = q if self.key_is_query else self.norm_key(key) v = k if self.value_is_key else self.norm_value(value) # 使用另一个注意力模块，将融合特征重新分配给每个点 x = self.attn(q, k, v, key_padding_mask) + self.drop(query) # MLP映射 + Residual Connection x = self.ffn(self.norm2(x)) + x return x return _inner_forward(query, key, value, key_padding_mask)\"]},\"99\":{\"h\":\"4️⃣ AFM 自适应融合模块\",\"t\":[\"有了以上 Grouping - Mixing - Ungrouping 三个关键步骤的实现，下面只需要把以上的三个步骤按流程组织起来即可得到AFM模块的完整实现了:\",\"class GPBlock(nn.Module): # q: 文本特征 (b, l, c) ， x: 点集特征集合 (b, n, c) def forward(self, q, x, q_mask=None): # Grouping阶段 gt = self.group_layer(query=q, key=x, value=x) if q_mask is not None: gt *= q_mask.unsqueeze(-1) # Mixing阶段 gt = self.mixer(gt) + self.drop(gt) # Ungrouping阶段 ungroup_tokens = self.un_group_layer(query=x, key=gt, value=gt, key_padding_mask=q_mask) return ungroup_tokens\",\"AFM 的网络结构可视化理解\",\"文本特征 X ──┐ ↓ Grouping (Cross-Attention) ↓ Mixing (MLP-Mixer) ↓ Ungrouping (Attention) ↓ 输出增强后的点特征 P_o\",\"Grouping：用语言引导点特征分组；\",\"Mixing：在分组内进行信息交换；\",\"Ungrouping：再将融合信息返回点空间；\",\"这种设计使得语言信息能有效地指导点特征的学习过程，论文中也进行了大量消融实验来验证 AFM 的有效性：\",\"模型变体\",\"mIoU\",\"AUC\",\"SIM\",\"MAE\",\"基线（不加 AFM）\",\"17.7\",\"82.1\",\"0.558\",\"0.110\",\"加入 AFM 后\",\"20.8\",\"87.3\",\"0.629\",\"0.093\",\"结果表明：加入 AFM 显著提升了所有指标，说明其确实有效增强了语言-视觉的跨模态交互能力。\"]},\"100\":{\"h\":\"RPO 参考点解码器\",\"t\":[\"Referred Point Decoder（RPD）是 LASO 任务中用于生成功能区域掩码的核心模块。\",\"它的主要目标是：\",\"利用一组问题条件化的 affordance queries 通过 Transformer 解码器与点云特征交互 ，生成一组动态卷积核（dynamic kernels），最终通过这些 kernel 对 AFM 增强后的点特征进行卷积，得到分割掩码。\",\"class TransformerDecoderLayer(nn.Module): # tgt: text feature (b,l,c), memory: up_sample (b,n,c) def forward(self, tgt, memory, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None, pos: Optional[Tensor] = None, query_pos: Optional[Tensor] = None): # 1. Affordance Query = 问题嵌入（Question Embedding）X + 可学习的位置编码（Learnable Position Embeddings） # 这里tgt就是Roberta编码得到的文本特征嵌入向量 # 使用 X 作为初始输入，确保每个 query 都带有原始语言上下文； # 如果只用 learnable embeddings，模型将完全依赖随机初始化的参数去“猜”语言含义，效率极低； q = k = self.with_pos_embed(tgt, query_pos) # 2. 自注意力机制: 让每个 query 不仅理解自己的语义，还能感知其他 query 的信息，从而形成更完整的语言上下文理解。 tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask) tgt = tgt + self.dropout1(tgt2) tgt = self.norm1(tgt) # (b,l,c) # 3. 跨模态注意力机制: 每个 affordance query 都会基于其语言语义，从点云中找出最相关的功能区域，从而为后续的动态卷积和掩码预测提供基础。 tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos), key=self.with_pos_embed(memory, pos), value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask, output_attentions = True) tgt = tgt + self.dropout2(tgt2) tgt = self.norm2(tgt) # (b,l,c) # 4. MLP: 每个query通道维度做特征融合 tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt)))) tgt = tgt + self.dropout3(tgt2) tgt = self.norm3(tgt) # (b,l,c) return tgt class PointRefer(nn.Module): def forward(self, text, xyz): ... # 3. Referred Point Decoding过程 # 3.1 利用一组问题条件化的 affordance queries 通过 Transformer 解码器与点云特征交互 ，生成一组动态卷积核（dynamic kernels）(b,l,c) t_feat = self.decoder(t_feat, up_sample.transpose(-2, -1), tgt_key_padding_mask=t_mask, query_pos=self.pos1d) # 3.2 对无效 token（padding）做掩码操作，防止其影响后续计算。 (b,l,c) t_feat *= t_mask.unsqueeze(-1).float() # 3.3 执行 动态卷积（Dynamic Convolution） 操作，用增强后的语言查询去“扫描”点云特征图 （b,l,n) _3daffordance = torch.einsum('blc,bcn->bln', t_feat, up_sample) # 3.4 对affordance query的响应图进行平均池化，融合所有 affordance query 的得分结果。 (b,n) _3daffordance = _3daffordance.sum(1)/(t_mask.float().sum(1).unsqueeze(-1)) # 3.5 将响应值映射到 [0, 1] 区间，表示每个点属于目标功能区域的概率。 (b,n) _3daffordance = torch.sigmoid(_3daffordance) return _3daffordance # (b,n)\",\"PyTorch 的 einsum 函数，它是一个非常强大且灵活的张量操作函数，支持通过爱因斯坦求和约定（Einstein Summation Convention） 来表达各种线性代数运算。\",\"下面详细介绍一下动态卷机核卷积的过程:\",\"t_feat: 语言查询特征 , 形状：(B, L, C) , 这是 经过 Referred Point Decoder (RPD) 处理后的 affordance queries，表示每个 token 对应的“动态卷积核”。\",\"up_sample: 上采样后的点云特征 , 形状：(B, C, N)。\",\"而下面这行代码实现的是一个 动态卷积（Dynamic Convolution） 操作：\",\"_3daffordance = torch.einsum('blc,bcn->bln', t_feat, up_sample)\",\"它的本质是： 用一组由语言引导的动态卷积核 t_feat 去卷积点云特征 up_sample，得到每个 token 对每个点的关注响应。\",\"详细解释 einsum 表达式:\",\"torch.einsum('blc,bcn->bln', t_feat, up_sample)\",\"维度\",\"含义\",\"b\",\"batch 维度，保持不变\",\"l\",\"token 维度，保留下来\",\"c\",\"特征通道维度，进行内积操作（求和）\",\"n\",\"点云维度，保留下来\",\"所以这个表达式的含义是：\",\"也就是说，对于每一个 batch 中的数据：\",\"每个 token（l）都与所有点（n）交互；\",\"每个 token 实际上是一个动态生成的卷积核（C × 1 × 1），作用于点云特征图（C × N）；\",\"最终输出形状为 (B, L, N)，表示： \",\"每个 token 对每个点的关注程度；\",\"输出张量\",\"形状\",\"含义\",\"_3daffordance\",\"(B, L, N)\",\"每个 token 对每个点的响应值（得分）\",\"然后在后续会进行如下处理：\",\"_3daffordance = _3daffordance.sum(1) / (t_mask.float().sum(1).unsqueeze(-1)) _3daffordance = torch.sigmoid(_3daffordance)\",\"即：\",\"在 token 维度求和（或平均池化），融合多个 token 的关注信息；\",\"使用 sigmoid 得到最终的掩码，形状 (B, N)；\",\"每个点的值 ∈ [0, 1]，表示其属于目标功能区域的概率；\"]},\"101\":{\"h\":\"损失函数\"},\"102\":{\"h\":\"HM_Loss（Hybrid Mask Loss）\",\"t\":[\"在 LASO 数据集中，模型需要根据自然语言问题识别点云中最相关的功能区域（如 grasping area, opening area 等），而 HM_Loss 是 PointRefer 模型的监督信号，它结合了：\",\"Focal Loss ：用于缓解类别不平衡问题；\",\"Dice Loss ：用于衡量预测掩码与真实标签之间的空间重合度；\",\"最终 loss = CELoss + DiceLoss，让模型同时关注逐点分类精度和整体区域匹配。\",\"import torch import torch.nn as nn import torch.nn.functional as F class HM_Loss(nn.Module): def __init__(self): \\\"\\\"\\\" Hybrid Mask Loss 实现： - BCE-Focal Loss（加权交叉熵） - Dice Loss（衡量预测掩码与 GT 的重合度） 公式来自论文 Section 4.2，用于语言引导下的功能区域分割。 \\\"\\\"\\\" super(HM_Loss, self).__init__() # 设置 Focal Loss 参数 self.gamma = 2 # 聚焦参数，放大难分类样本影响 self.alpha = 0.25 # 平衡因子，强调正类（前景点）loss def forward(self, pred, target): \\\"\\\"\\\" 输入： pred: 模型输出的原始 logit 或经过 sigmoid 的概率值； 形状为 [B, N] target: ground truth 掩码（soft mask），形状也为 [B, N] 返回： total_loss: CELoss + DiceLoss 的加权和 \\\"\\\"\\\" # Step 1: 构建 Focal Loss 权重项 # temp1：负类 loss（背景点） # temp2：正类 loss（目标功能区域） # 1e-6 的加入是为了让 log 计算保持稳定，尤其是在预测值接近极端值（0 或 1）时 temp1 = -(1 - self.alpha) * torch.mul( pred ** self.gamma, torch.mul(1 - target, torch.log(1 - pred + 1e-6)) ) temp2 = -self.alpha * torch.mul( (1 - pred) ** self.gamma, torch.mul(target, torch.log(pred + 1e-6)) ) # 将两个方向的 loss 合并，并取 batch 和点维度的平均 temp = temp1 + temp2 CELoss = torch.sum(torch.mean(temp, dim=(0, 1))) # Step 2: 计算正类 Dice Loss（预测与 Ground Truth 的交集 / 并集） intersection_positive = torch.sum(pred * target, dim=1) cardinality_positive = torch.sum(torch.abs(pred) + torch.abs(target), dim=1) dice_positive = (intersection_positive + 1e-6) / (cardinality_positive + 1e-6) # Step 3: 计算负类 Dice Loss（非目标区域匹配度） intersection_negative = torch.sum((1 - pred) * (1 - target), dim=1) cardinality_negative = torch.sum(2 - torch.abs(pred) - torch.abs(target), dim=1) dice_negative = (intersection_negative + 1e-6) / (cardinality_negative + 1e-6) # Step 4: 构建 Dice Loss，形式为 1 - Dice Score # 使用了一个偏置项 1.5（可能是经验设定） temp3 = torch.mean(1.5 - dice_positive - dice_negative, dim=0) DICELoss = torch.sum(temp3) # Step 5: 总损失 = 分类误差 + 区域匹配误差 return CELoss + 1.0 * DICELoss\",\"在论文 Section 4.2 中提到：\",\"“We solely employ Dice loss and Binary Cross-Entropy (BCE) loss to guide the segmentation mask prediction.”\",\"虽然这里用的是 Focal Loss + Dice Loss 的组合形式，但它本质上是 BCE + Dice 的改进版，具有以下优势：\",\"Focal Loss: 抑制 easy examples，放大 hard examples，防止忽略小区域\",\"Dice Loss: 关注整体掩码匹配度，提升边界识别能力\",\"两者结合可以：\",\"缓解类别极度不平衡问题；\",\"提高模型对语言指令下功能区域的理解能力；\",\"更好地应对 LASO 中的语言引导 + soft mask 场景；\"]},\"103\":{\"h\":\"训练\",\"t\":[\"模型的训练过程大体分为了 准备，训练，评估 三个流程；\"]},\"104\":{\"h\":\"准备\",\"t\":[\"准备阶段主要完成数据集加载，模型初始化，损失函数定义，优化器设置，学习率调度器初始化等；\",\"def main(opt, dict): # 1. 加载训练集，验证集，测试集 train_dataset = AffordQ('train') train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8 ,shuffle=True, drop_last=True) val_dataset = AffordQ('val') val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=8, shuffle=False) test_dataset = AffordQ('test') test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=8, shuffle=False) # 2. 初始化模型 model = get_PointRefer(emb_dim=dict['emb_dim'], proj_dim=dict['proj_dim'], num_heads=dict['num_heads'], N_raw=dict['N_raw'], num_affordance = dict['num_affordance'], n_groups=opt.n_groups) # 3. 初始化损失函数，优化器，学习率调度器 criterion_hm = HM_Loss() criterion_ce = nn.CrossEntropyLoss() param_dicts = [ {\\\"params\\\": [p for n, p in model.named_parameters() if \\\"text_encoder\\\" not in n and p.requires_grad]}, {\\\"params\\\": [p for n, p in model.named_parameters() if \\\"text_encoder\\\" in n and p.requires_grad], \\\"lr\\\": opt.tlr}] optimizer = torch.optim.Adam(params = param_dicts, lr=dict['lr'], betas=(0.9, 0.999), eps=1e-8, weight_decay=opt.decay_rate) # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=dict['Epoch'], eta_min=1e-6) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\"]},\"105\":{\"h\":\"训练\",\"t\":[\"训练阶段则是模型的核心迭代过程，包括前向传播，损失计算，反向传播，参数更新等:\",\" ''' Training ''' for epoch in range(start_epoch+1, dict['Epoch']): num_batches = len(train_loader) loss_sum = 0 total_point = 0 model = model.train() for i,(point, cls, gt_mask, question, aff_label) in enumerate(train_loader): optimizer.zero_grad() # 4. 前向传播过程 _3d = model(question, point) # 5. 计算损失 loss_hm = criterion_hm(_3d, gt_mask) # loss_ce = criterion_ce(logits, cls) # 6. 反向传播 temp_loss = loss_hm # + opt.loss_cls*loss_ce temp_loss.backward() optimizer.step() results = torch.zeros((len(val_dataset), 2048, 1)) targets = torch.zeros((len(val_dataset), 2048, 1))\"]},\"106\":{\"h\":\"评估\",\"t\":[\"评估阶段则是在验证集或测试集上评估模型的性能，计算指标包括 MAE，SIM，AUC，mIoU。\",\"在 LASO（Language-guided Affordance Segmentation on 3D Object）任务 中，作者使用了四个核心评估指标来衡量模型对语言引导下功能区域的识别能力：\",\"指标\",\"名称\",\"英文全称\",\"MAE\",\"平均绝对误差\",\"Mean Absolute Error\",\"SIM\",\"相似性得分\",\"Similarity Score\",\"AUC\",\"曲线下面积\",\"Area Under the Curve\",\"mIoU\",\"平均交并比\",\"mean Intersection over Union\",\"MAE（Mean Absolute Error）是预测值与真实值之间的平均绝对误差，用于衡量模型输出的 soft mask 与 ground truth 掩码之间的逐点偏差。\",\"其中：\",\"：点云中点的数量；\",\"：模型预测该点属于功能区域的概率；\",\"：ground truth 标签（可以是 soft mask 或 binary mask）；\",\"特点与作用：\",\"特性\",\"描述\",\"✔️ 支持 soft mask 输入\",\"不依赖 thresholding，适用于连续响应值\",\"✔️ 衡量整体分布一致性\",\"反映模型是否准确学习语言引导下的响应强度\",\"⚠️ 对边界模糊区域不敏感\",\"IoU 等指标更关注重合度\",\"SIM（Similarity）是一种基于直方图交集的相似性指标，用于衡量两个概率分布之间的匹配程度。它常用于图像检索、图像分割等任务。\",\"即：对每个点取预测值和真实值中的较小者，然后求和。也可以归一化为：\",\"特点与作用：\",\"特性\",\"描述\",\"✔️ 不需要 thresholding\",\"支持 soft mask 输入\",\"✔️ 强调分布匹配\",\"不仅看交集，还看响应强度分布\",\"✔️ 对边界模糊区域友好\",\"不像 IoU 那样依赖 hard threshold\",\"⚠️ 不直接优化最终目标\",\"不能作为 loss 使用，更适合评估\",\"AUC（Area Under ROC Curve）是 Receiver Operating Characteristic (ROC) 曲线下的面积，衡量模型对二分类问题的判别能力。\",\"AUC 的计算流程如下：\",\"对不同阈值计算 TPR 和 FPR；\",\"绘制 ROC 曲线；\",\"计算曲线下面积（AUC）；\",\"特点与作用：\",\"特性\",\"描述\",\"✔️ 不依赖特定阈值\",\"考察所有可能的 threshold 下的表现\",\"✔️ 关注排序能力\",\"判断模型是否能正确区分前景和背景\",\"✔️ 适用于 binary 分类\",\"需要先将 soft mask 转换为 binary\",\"⚠️ 对 small region 敏感度有限\",\"需结合 mIoU 使用\",\"mIoU（mean Intersection over Union）是图像/点云分割中最常用的指标之一，衡量预测区域与真实标签之间的空间重合度。\",\"公式如下：\",\"其中：\",\"：预测的 binary mask；\",\"：真实的 binary mask；\",\"通常我们会使用多个 threshold（如 np.linspace(0, 1, 20)），然后取平均得到 aiou（average IoU）。\",\"特点与作用：\",\"特性\",\"描述\",\"✔️ 直接评价分割精度\",\"最贴近实际应用需求\",\"✔️ 对边界敏感\",\"能反映边缘响应质量\",\"✔️ 易受 threshold 影响\",\"多阈值评估更稳定\",\"⚠️ 不支持 soft mask 直接输入\",\"需先 threshold 成 binary mask\",\"四个指标对比总结:\",\"指标\",\"是否支持 soft mask\",\"是否依赖 threshold\",\"是否关注分布相似性\",\"是否关注空间重合度\",\"输出范围\",\"MAE\",\"✅ 是\",\"❌ 否\",\"✅ 是\",\"❌ 否\",\"[0, ∞)\",\"SIM\",\"✅ 是\",\"❌ 否\",\"✅ 是\",\"❌ 否\",\"[0, 1]\",\"AUC\",\"✅ 是（排序）\",\"✅ 是（binary）\",\"❌ 否\",\"❌ 否\",\"[0, 1]\",\"mIoU\",\"❌ 否（需先 threshold）\",\"✅ 是\",\"❌ 否\",\"✅ 是\",\"[0, 1]\",\"结合论文理解这些指标的意义，来自论文 Table 3 的结果：\",\"方法\",\"mIoU\",\"AUC\",\"SIM\",\"MAE\",\"PointRefer（完整方法）\",\"20.8%\",\"87.3%\",\"0.629\",\"0.093\",\"这些指标共同构成了 LASO 任务的评估体系，分别从以下角度衡量模型表现：\",\"角度\",\"对应指标\",\"1. 分布一致性\",\"SIM\",\"2. 分类判别能力\",\"AUC\",\"3. 逐点误差\",\"MAE\",\"4. 区域重合度\",\"mIoU\",\"这意味着：\",\"PointRefer 不仅理解语言指令；\",\"还能生成与 GT 掩码高度匹配的功能区域；\",\"并且在 unseen object 上也具有泛化能力；\",\"在 LASO 这种类别不平衡、soft mask、语言引导的 3D 功能区域识别任务中，四个指标协同工作：\",\"MAE 衡量逐点误差；\",\"SIM 衡量分布相似性；\",\"AUC 衡量分类器排序能力；\",\"mIoU 衡量空间重合度；\",\"它们共同帮助我们判断模型是否真正理解语言引导下的功能区域语义。\",\"验证集上进行评估的核心代码实现如下:\",\" num = 0 for i, (point, _, label, question, aff_label) in enumerate(val_loader): # 1. 前向传播，得到预测的 soft mask `_3d` ∈ [B, N] _3d = model(question, point) # 2. 计算 MAE（Mean Absolute Error），衡量逐点误差 mae, point_nums = evaluating(_3d, label) total_point += point_nums total_MAE += mae.item() pred_num = _3d.shape[0] # 当前 batch 的样本数 # 3. 收集所有样本的预测结果，便于后续统一评估 results[num : num + pred_num, :, :] = _3d.unsqueeze(-1) # shape: [B, N, 1] targets[num : num + pred_num, :, :] = label.unsqueeze(-1) # shape: [B, N, 1] num += pred_num # 更新索引 # 4. 计算平均 MAE（Mean Absolute Error） mean_mae = total_MAE / total_point # 5. 计算 SIM（Similarity Metric）—— 直方图交集，衡量分布相似性 SIM_matrix = np.zeros(targets.shape[0]) for i in range(targets.shape[0]): SIM_matrix[i] = SIM(results[i], targets[i]) # SIM 函数定义见 utils.eval sim = np.mean(SIM_matrix) # 6. 初始化 AUC 和 IOU 存储数组 AUC = np.zeros((targets.shape[0], targets.shape[2])) # shape: [num_samples, 1] IOU = np.zeros((targets.shape[0], targets.shape[2])) IOU_thres = np.linspace(0, 1, 20) # 多阈值下的 IoU 计算 # 7. 将 GT 标签二值化（soft mask → binary mask） targets_binary = (targets >= 0.5).astype(int) for i in range(AUC.shape[0]): t_true = targets_binary[i].flatten() # 真实标签 p_score = results[i].flatten() # 模型输出的概率值 if np.sum(t_true) == 0: # 8. 如果当前样本没有正类（即无功能区域），标记为 nan AUC[i] = np.nan IOU[i] = np.nan else: # 9. 计算 AUC（Area Under the Curve），衡量分类器整体判别能力 auc = roc_auc_score(t_true, p_score) AUC[i] = auc # 10. 使用多个阈值计算 mIoU（mean Intersection over Union） temp_iou = [] for thre in IOU_thres: p_mask = (p_score >= thre).astype(int) # 用不同 threshold 生成 binary mask intersect = np.sum(p_mask & t_true) # 交集 union = np.sum(p_mask | t_true) # 并集 temp_iou.append(intersect / union) # IoU = intersect / union temp_iou = np.array(temp_iou) aiou = np.mean(temp_iou) # 对所有 threshold 下的 IoU 取均值 IOU[i] = aiou # 10. 最终取所有样本的 AUC 和 mIoU 均值作为最终评估指标 AUC = np.nanmean(AUC) IOU = np.nanmean(IOU) # 11. 打印当前性能指标 logger.debug(f'AUC:{AUC} | IOU:{IOU} | SIM:{sim} | MAE:{mean_mae}') current_IOU = IOU # 12. 如果当前 mIoU 超过历史最佳，则保存 best model if current_IOU > best_IOU: best_IOU = current_IOU best_model_path = save_path + '/best_model-{}.pt'.format(sign) checkpoint = { 'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'Epoch': epoch } torch.save(checkpoint, best_model_path)\",\"测试集最终评估:\",\"category_metrics, affordance_metrics, overall_metrics = evaluate(model, test_loader, device, 3) print_metrics_in_table(category_metrics, affordance_metrics, overall_metrics, logger)\"]},\"107\":{\"h\":\"复现\",\"t\":[\"设置后台运行，同时将运行时输出写入日志:\",\"nohup python -u train.py > train.log 2>&1 &\",\"在 Python 命令中， -u 是 unbuffered 的缩写。Python 存在缓存机制， sys.stdout （标准输出）是有缓存的，只有遇到换行符或者输出内容积累到一定大小时，才会将内容显示到屏幕上；而 sys.stderr （标准错误）是无缓存的，程序向 sys.stderr 输出一个字符，就会立即在屏幕上显示一个字符 1 2 3 。\",\"当在 Python 命令后加上 -u 参数（例如 python -u xx.py ），会强制标准输出也像标准错误一样，不通过缓存直接将内容打印到屏幕上 1 2 3 。这在使用 nohup 后台运行 Python 脚本时非常有用，可以避免因为输出缓存导致日志卡在某一行不输出的问题。\",\"持续追踪日志最新输出:\",\"tail -f train.log\",\"杀死训练进程:\",\"pkill -f \\\"python train.py\\\"\",\"用训练好的模型权重，进行推理 (以下代码是我自己写的一个测试代码):\",\"import os import pickle import torch import numpy as np import open3d as o3d from utils.util import read_yaml from model.PointRefer import get_PointRefer def pc_normalize(pc): \\\"\\\"\\\" 点云数据归一化处理 Args: pc: 输入点云数据，形状为 [N, 3] 的 numpy 数组 Returns: 归一化后的点云数据 \\\"\\\"\\\" centroid = np.mean(pc, axis=0) # 计算点云质心 pc = pc - centroid # 中心化 m = np.max(np.sqrt(np.sum(pc**2, axis=1))) # 计算最大半径 pc = pc / m # 归一化到单位球内 return pc def predict_affordance_mask(points, text, model_path): \\\"\\\"\\\" 预测点云的功能区域掩码 Args: points: 输入点云数据 [N, 3] text: 描述功能的文本提示 model_path: 预训练模型路径 Returns: 功能区域预测结果 [N] \\\"\\\"\\\" # 加载模型配置 dict = read_yaml(\\\"config/default.yaml\\\") dict['bs'] = 16 # 设置batch size dict['lr'] = 1e-4 # 设置学习率 dict['Epoch'] = 50 # 设置训练轮数 # 初始化PointRefer模型 model = get_PointRefer( emb_dim=dict['emb_dim'], proj_dim=dict['proj_dim'], num_heads=dict['num_heads'], N_raw=dict['N_raw'], num_affordance=dict['num_affordance'], n_groups=40 ) # 加载预训练权重 checkpoint = torch.load(model_path, map_location='cpu') model.load_state_dict(checkpoint['model']) model.eval() # 设置为评估模式 # 点云预处理 Point = pc_normalize(points) # 归一化 Point = Point.transpose() # 转置为 [3, N] Points = torch.from_numpy(Point).unsqueeze(0).float() # 增加batch维度 [1, 3, N] text_list = [text] # 文本输入格式处理 # 模型推理 pred = model(text_list, Points) pred = torch.squeeze(pred) # 去除batch维度 [N] affordance_pred = pred.cpu().detach().numpy() # 转为numpy数组 return affordance_pred def visualize_affordance(points_coordinates, affordance_pred): \\\"\\\"\\\" 可视化功能区域预测结果（渐变颜色） Args: points_coordinates: 点云坐标 [N, 3] affordance_pred: 预测结果 [N] \\\"\\\"\\\" pred_point = o3d.geometry.PointCloud() pred_point.points = o3d.utility.Vector3dVector(points_coordinates) # 颜色映射：根据预测值从灰色(背景)到红色(功能区域)渐变 color = np.zeros((2048, 3)) reference_color = np.array([255, 0, 0]) # 红色 back_color = np.array([190, 190, 190]) # 灰色 for i, aff_pred in enumerate(affordance_pred): scale_i = aff_pred color[i] = (reference_color - back_color) * scale_i + back_color pred_point.colors = o3d.utility.Vector3dVector(color.astype(np.float64) / 255.0) o3d.visualization.draw_geometries([pred_point], window_name='Predicted Affordance', width=600, height=600) def visualize_point_cloud(points, pred_mask=None): \\\"\\\"\\\" 基础点云可视化（二值化显示） Args: points: 点云数据 [N, 3] 或 [N, 4] pred_mask: 可选，预测掩码 [N] \\\"\\\"\\\" pcd = o3d.geometry.PointCloud() # 数据预处理 if points.shape[1] == 4: # 如果包含强度值 coordinates = points[:, :3].astype(np.float64) if pred_mask is None: pred_mask = points[:, 3] > 0.5 # 使用第4列作为默认掩码 else: coordinates = points.astype(np.float64) if pred_mask is None: pred_mask = np.zeros(len(points), dtype=bool) # 设置颜色：红色=功能区域，蓝色=背景 colors = np.zeros((len(points), 3)) colors[pred_mask] = [1, 0, 0] # 红色 colors[~pred_mask] = [0, 0, 1] # 蓝色 pcd.points = o3d.utility.Vector3dVector(coordinates) pcd.colors = o3d.utility.Vector3dVector(colors.astype(np.float64)) # 创建可视化窗口 vis = o3d.visualization.Visualizer() vis.create_window(window_name='LASO Prediction Visualization') vis.add_geometry(pcd) # 设置渲染参数 opt = vis.get_render_option() opt.point_size = 2.5 # 点大小 opt.background_color = np.array([1, 1, 1]) # 白色背景 vis.run() # 运行可视化 vis.destroy_window() if __name__ == '__main__': # 数据路径设置 data_root = 'LASO_dataset' # 加载标注数据 with open(os.path.join(data_root, f'anno_val.pkl'), 'rb') as f: anno = pickle.load(f) # 加载点云数据 with open(os.path.join(data_root, f'objects_val.pkl'), 'rb') as f: objects = pickle.load(f) # 示例数据（第500个样本） print(\\\"当前物体类型: \\\", anno[500][\\\"class\\\"]) print(\\\"当前物体待预测的功能区域: \\\", anno[500][\\\"affordance\\\"]) point_cloud_data = objects[anno[500][\\\"shape_id\\\"]] # visualize_point_cloud(point_cloud_data) -- 可视化当前物体点云，再决定要使用什么文本查询 # 示例文本查询 text = \\\"If I want to move this chair, which part of the chair should I hold with my hands?\\\" # 执行预测和可视化 affordance_pred = predict_affordance_mask( point_cloud_data, text, \\\"runs/train/PointRefer/best_model-try_at_6.15_23.53.29.pt\\\" ) visualize_affordance(point_cloud_data, affordance_pred)\",\"预测结果可视化:\",\"If I want to move this chair, which part of the chair should I hold with my hands?\",\"If I want to sit on this chair, which part of the chair should I sit on?\",\"本文使用的是训练了9个epoch后的模型权重进行的推理演示，后续训练完50个epoch后，会进行推理能力结果更新。\"]},\"108\":{\"h\":\"Point Transformer 论文\",\"t\":[\"Point Transformer 论文\",\"论文: Point Transformer 代码: https://github.com/POSTECH-CVLab/point-transformer\"]},\"109\":{\"h\":\"引言\",\"t\":[\"背景与动机（如图1所示）：\",\"3D 点云广泛存在于自动驾驶、增强现实和机器人等应用中，但它们与图像不同，是嵌入在连续空间的集合，因此传统基于卷积的视觉网络难以直接应用。\",\"现有方法主要有三类：\",\"将点云体素化后应用 3D 离散卷积，但计算和内存开销大，并未充分利用点云稀疏性。\",\"使用稀疏卷积、池化或连续卷积直接在点上操作，部分缓解了计算负担。\",\"将点集构建成图结构，通过消息传递进行信息传播。\",\"Transformer 的自注意力机制天然适合点云，因为它对输入的排列和数量不敏感，而点云本质上就是集合结构。\",\"方法设计（如图2所示）：\",\"我们提出了 Point Transformer 层，能够处理点云的排列和数量不变性，通过局部邻域的自注意力传播信息。\",\"在网络设计上，完全由自注意力和逐点操作组成，不依赖卷积操作，也不需要事先体素化。\",\"通过对自注意力算子形式、局部邻域应用方式及位置编码方法的设计，构建了高表达能力的网络骨干，可用于多种 3D 理解任务。\",\"主要贡献：\",\"设计了高度表达能力的 Point Transformer 层，本质上适合点云处理，对排列和数量不敏感。\",\"构建了基于 Point Transformer 层的高性能网络，可作为 3D 场景理解的通用骨干，适用于分类和密集预测任务。\",\"在多个领域和数据集上进行了广泛实验，创下多个最先进水平，超越大量先前方法。\"]},\"110\":{\"h\":\"相关工作\",\"t\":[\"3D 点云与二维图像不同，是无序分散在三维空间的点集合，因此传统卷积网络难以直接应用。基于学习的方法主要分为三类：\",\"投影型网络：\",\"将点云投影到二维平面，生成规则图像，然后使用 2D CNN 提取特征，再进行多视角融合。\",\"TangentConv 将局部表面几何投影到切平面，生成切平面图像，用二维卷积处理，但依赖切平面估计。\",\"缺点：投影会压缩几何信息，可能未充分利用点云稀疏性，平面选择和三维遮挡可能影响识别性能。\",\"体素型网络：\",\"将点云体素化，再在三维网格上进行卷积。\",\"优点：将不规则点云转化为规则表示，便于卷积操作。\",\"缺点：分辨率增加时计算和内存开销大。\",\"解决方案：利用稀疏性，如 OctNet 使用不平衡八叉树，稀疏卷积只计算非空体素【9,3】。\",\"注意：体素化量化仍可能导致几何细节丢失。\",\"点型网络：\",\"直接处理嵌入连续空间的点云集合，无需量化或投影。\",\"PointNet 使用排列不变操作（逐点 MLP + 池化）聚合集合特征。\",\"PointNet++ 在层级空间结构中增加对局部几何布局的敏感性。\",\"可结合高效采样策略，提高计算效率【27,7,46,50,11】。\",\"基于图的方法：\",\"将点集构建成图，进行消息传递或图卷积：\",\"DGCNN 在 kNN 图上进行图卷积\",\"PointWeb 密集连接局部邻域\",\"ECC 使用动态边条件卷积\",\"SPG 使用超级点图表示上下文关系\",\"KCNet 使用核相关 + 图池化\",\"Wang 等研究局部谱图卷积\",\"GACNet 使用图注意力卷积\",\"HPEIN 构建层级点-边交互架构\",\"DeepGCNs 探索图卷积深度在 3D 场景理解中的优势\",\"基于连续卷积的方法：\",\"PCCN 将卷积核表示为 MLP\",\"SpiderCNN 使用多项式函数族定义卷积核权重\",\"Spherical CNN 解决 3D 旋转等变性\",\"PointConv 和 KPConv 根据坐标构建卷积核\",\"InterpCNN 使用坐标插值生成卷积权重\",\"PointCNN 对无序点云重新排序\",\"Ummenhofer 等将连续卷积应用于粒子流体动力学\",\"Transformer 在机器翻译和 NLP 上取得巨大成功，也启发了 2D 图像识别。 自注意力本质上是集合操作：位置信息作为元素属性处理，而点云本质上就是带位置属性的点集合。\",\"现有点云注意力方法多为全局注意力：\",\"计算开销大\",\"不适合大规模场景\",\"使用标量点积，所有通道共享聚合权重\",\"本文方法创新点：\",\"在局部应用自注意力，使网络可扩展到百万点大场景\",\"使用向量注意力，提高精度\",\"强调位置编码的重要性，而先前方法通常忽略\",\"总体效果：设计合理的自注意力网络可扩展到复杂大规模场景，并显著提升点云理解性能\"]},\"111\":{\"h\":\"方法\"},\"112\":{\"h\":\"背景\",\"t\":[\"Transformer 和自注意力在 NLP 和图像分析中已经非常成功。自注意力分为两种：标量注意力和向量注意力。\",\"标量注意力\",\"输入是一组特征向量 ，输出 的计算公式为：\",\"其中 是点级特征变换（线性投影或 MLP）， 是位置编码， 是归一化（如 softmax）。注意力权重是标量。\",\"向量注意力\",\"注意力权重是向量，可以对不同通道单独调制，计算方式为：\",\"其中 是关系函数（如减法）， 是生成向量权重的 MLP。\",\"这两类自注意力算子本质上都是集合算子，既可以在整个集合上作用（如句子、整幅图像），也可以只在局部子集上作用（如图像 patch）。\"]},\"113\":{\"h\":\"Point Transformer 层\",\"t\":[\"点云天然就是不规则的集合，因此自注意力特别适合点云。Point Transformer 层基于向量自注意力，采用减法关系并在注意力分支和特征分支都加入位置编码：\",\"其中 是 的 近邻集合（局部邻域）。映射函数 是一个两层线性层 + ReLU 的 MLP。结构如图2所示。\"]},\"114\":{\"h\":\"位置编码\",\"t\":[\"位置编码让注意力能够感知局部空间结构。\",\"在 NLP/图像中，常用正弦余弦或归一化坐标范围手工设计。\",\"在 3D 点云中，点的坐标天然可用作位置编码。\",\"本文提出 可训练的参数化位置编码：\",\"其中 是一个两层线性层 + ReLU 的 MLP。\",\"实验发现，位置编码对注意力生成分支和特征变换分支都很重要，因此在公式 (3) 的两个分支中都加了 。 与整个网络一起端到端训练。\"]},\"115\":{\"h\":\"Point Transformer 模块\",\"t\":[\"Point Transformer 模块是一个残差结构，如图4(a) 所示。它包含：\",\"Point Transformer 层（核心）\",\"降维的线性投影（加速计算）\",\"残差连接\",\"输入是一组点的特征向量及其 3D 坐标，输出是更新后的点特征。该模块在特征内容和三维空间布局上都能自适应进行信息聚合。\"]},\"116\":{\"h\":\"网络架构\",\"t\":[\"整个网络完全由 Point Transformer 模块、点级变换和池化组成，不依赖卷积。结构如图3所示。\",\"骨干网络结构：\",\"语义分割/分类的特征编码器有 5 个阶段，下采样率为 ，输出点数依次为 。相邻阶段通过转换模块连接：\",\"Transition down（图4b）：\",\"从 采样子集 （最远点采样），并将 的特征汇聚到 。流程：\",\"线性层 → BN → ReLU\",\"kNN 聚合（）\",\"max pooling\",\"Transition up（图4c）：\",\"在解码阶段，将 的特征映射回更高分辨率点集 ：\",\"线性层 → BN → ReLU\",\"三线性插值\",\"融合来自编码器的跳跃连接特征\",\"输出头：\",\"语义分割：为每个点生成特征 → MLP → 每点分类 logits\",\"分类任务：对点特征全局平均池化 → 全局特征向量 → MLP → 分类 logits\"]},\"117\":{\"h\":\"消融实验\",\"t\":[\"为了验证 Point Transformer 设计中的关键选择，作者进行了多组消融实验，结果如下：\",\"邻居数量 k 的影响：\",\"当 时性能最佳。\",\"当 或 时，邻域过小，缺乏足够上下文，导致预测不准确。\",\"当 或 时，邻域过大，引入过多不相关点，噪声增加，反而降低性能。\",\"Softmax 正则化的作用：\",\"不使用 Softmax 时，mIoU/mAcc/OA = 66.5% / 72.8% / 89.3%。\",\"使用 Softmax 时，性能显著提升至 70.4% / 76.5% / 90.8%。\",\"结论：Softmax 归一化是必要的，否则模型性能显著下降。\",\"位置编码 δ 的选择：\",\"无位置编码 → 性能明显下降。\",\"使用绝对位置编码 → 性能有所提升。\",\"使用相对位置编码 → 性能最佳。\",\"仅在注意力生成分支或特征变换分支单独加入相对位置编码 → 性能下降。\",\"结论：必须在两条分支同时加入相对位置编码，效果最好。\",\"注意力类型的比较：\",\"MLP：逐点 MLP，无邻域交互，性能最差。\",\"MLP+pooling：逐点 MLP + 邻域池化，有信息交互但无注意力机制，性能优于纯 MLP。\",\"标量注意力：基于公式 (1)，比前两者更强。\",\"向量注意力：基于公式 (3)，性能最优。\",\"实验结果显示，标量注意力的 mIoU 为 64.6%，而向量注意力达到 70.4%，提升 5.8 个百分点。\",\"结论：向量注意力由于支持逐通道自适应调制，表达能力更强，对 3D 数据处理尤为有利。\"]},\"118\":{\"h\":\"代码实现\"},\"119\":{\"h\":\"向量自注意力层\",\"t\":[\"PointTransformerLayer 类实现了Point Transformer论文中提出的向量自注意力机制，该机制是点云处理领域的重要创新。与传统的标量注意力不同，向量注意力能够更好地捕获3D空间中的几何关系。\",\"核心设计理念：\",\"向量化注意力：注意力权重不再是标量，而是向量形式，能够编码更丰富的空间信息\",\"位置感知：通过位置编码函数θ将3D相对坐标信息融入注意力计算\",\"局部邻域处理：基于KNN构建的局部邻域，实现高效的点云特征聚合\",\"算法流程概述：\",\"特征变换：将输入特征通过线性层生成Q、K、V三元组\",\"邻域构建：利用KNN算法为每个点构建局部邻域\",\"位置编码：将相对坐标通过MLP网络映射到高维特征空间\",\"注意力计算：结合特征差值和位置编码生成向量化注意力权重\",\"特征聚合：基于注意力权重对邻域特征进行加权融合\",\"完整的向量自注意力计算代码实现如下：\",\"class PointTransformerLayer(nn.Module): def __init__(self, in_planes, out_planes, share_planes=8, nsample=16): super().__init__() # 中间通道数，简化处理（这里直接等于 out_planes） self.mid_planes = mid_planes = out_planes // 1 self.out_planes = out_planes self.share_planes = share_planes self.nsample = nsample # Q, K, V 的线性变换 self.linear_q = nn.Linear(in_planes, mid_planes) # 查询向量 (query) self.linear_k = nn.Linear(in_planes, mid_planes) # 键向量 (key) self.linear_v = nn.Linear(in_planes, out_planes) # 值向量 (value) # 位置编码 δ (论文 Eq.(4): δ = θ(pi − pj)) # 输入是相对坐标 (3D)，输出是与 out_planes 对齐的特征 self.linear_p = nn.Sequential( nn.Linear(3, 3), nn.BatchNorm1d(3), nn.ReLU(inplace=True), nn.Linear(3, out_planes) ) # 权重生成函数 γ (MLP)，作用在 (q - k + δ) 上 # 注意这里做了“通道分组”（share_planes），减少计算量 self.linear_w = nn.Sequential( nn.BatchNorm1d(mid_planes), nn.ReLU(inplace=True), nn.Linear(mid_planes, mid_planes // share_planes), nn.BatchNorm1d(mid_planes // share_planes), nn.ReLU(inplace=True), nn.Linear(mid_planes // share_planes, out_planes // share_planes) ) # softmax 用来对注意力权重归一化 self.softmax = nn.Softmax(dim=1) def forward(self, pxo) -> torch.Tensor: # 输入: # p: 点的坐标 (n, 3) # x: 点的特征 (n, c) # o: batch 索引 (b) p, x, o = pxo # 得到 Q, K, V x_q, x_k, x_v = self.linear_q(x), self.linear_k(x), self.linear_v(x) # (n, c) # 构建邻域 (kNN)，并返回局部邻域的特征 # x_k: (n, nsample, 3+c)，包含相对坐标和 K 特征 # x_v: (n, nsample, c)，邻域内的 V 特征 x_k = pointops.queryandgroup(self.nsample, p, p, x_k, None, o, o, use_xyz=True) x_v = pointops.queryandgroup(self.nsample, p, p, x_v, None, o, o, use_xyz=False) # 分离相对坐标 p_r 和邻域内的 K 特征 p_r, x_k = x_k[:, :, 0:3], x_k[:, :, 3:] # 将相对坐标 p_r 输入位置编码 MLP θ # 这里因为 BatchNorm 的维度问题，需要转置 (n, nsample, 3) ↔ (n, 3, nsample) for i, layer in enumerate(self.linear_p): p_r = layer(p_r.transpose(1, 2).contiguous()).transpose(1, 2).contiguous() if i == 1 else layer(p_r) # 经过 MLP 后: (n, nsample, out_planes) # 根据 Eq.(3): w = γ(φ(xi) − ψ(xj) + δ) # x_q.unsqueeze(1): (n, 1, c)，与邻域对齐 # p_r reshape 后与 x_k 对齐做相加 w = x_k - x_q.unsqueeze(1) + p_r.view( p_r.shape[0], p_r.shape[1], self.out_planes // self.mid_planes, self.mid_planes ).sum(2) # (n, nsample, c) # 将 w 输入 γ MLP (linear_w)，得到注意力权重 for i, layer in enumerate(self.linear_w): w = layer(w.transpose(1, 2).contiguous()).transpose(1, 2).contiguous() if i % 3 == 0 else layer(w) # softmax 归一化注意力权重 w = self.softmax(w) # (n, nsample, c) # 最终聚合 (Eq.(3) 中 ρ(...)*α(xj+δ)) n, nsample, c = x_v.shape s = self.share_planes x = ((x_v + p_r).view(n, nsample, s, c // s) * w.unsqueeze(2)).sum(1).view(n, c) return x\",\"下面针对上面部分代码进行进一步说明:\",\"计算注意力权重: 领域内最近邻键特征 - 领域所在中心点查询特征 + 相对位置编码(巧妙的view方式，个人理解是为了确保维度对齐)\",\" w = x_k - x_q.unsqueeze(1) + p_r.view( p_r.shape[0], p_r.shape[1], self.out_planes // self.mid_planes, self.mid_planes ).sum(2) # (n, nsample, c)\",\"聚合: 对每个中心点的所有邻居点的特征在特征维度上进行分组，做通道分组(类似多头注意力，但是作用不完全相同) + 利用广播后做逐元素相乘，完成对同一个邻居点的所有通道分组应用相同权重分配的过程 + 所有邻居点特征进行求和，完成领域值信息聚合过程 + 多头重组回原貌\",\" # （200，8，8，4） * （200，8，1，4） -> (200, 8, 8, 4) -> (200,8,4) -> (200,32) x = ((x_v + p_r).view(n, nsample, s, c // s) * w.unsqueeze(2)).sum(1).view(n, c)\",\"分组计算过程可参考如下这个简化版例子:\",\"# 分组后的特征 (1个点，1个邻居，2组，每组2个通道) grouped_features = torch.tensor([[ [[1.0, 2.0], # 组0: 通道0,1 [3.0, 4.0]] # 组1: 通道2,3 ]]) # shape: (1, 1, 2, 2) # 注意力权重 (16维权重，这里简化为2维) attention_weights = torch.tensor([[ [[0.5, 1.0]] # 权重向量 ]]) # shape: (1, 1, 1, 2) # 逐元素相乘 result = grouped_features * attention_weights # [[[[1.0*0.5, 2.0*1.0], # 组0: [0.5, 2.0] # [3.0*0.5, 4.0*1.0]]]] # 组1: [1.5, 4.0]\",\"这实际上是一种分组通道注意力（Grouped Channel Attention）：\",\"不是在序列维度上做注意力（token-token）\",\"而是在通道维度上做注意力（channel-channel）\",\"通过分组实现参数共享\",\"queryandgroup 方法实现了点云的邻域查询和特征分组功能。具体流程如下：\",\"邻域查询：对于查询点集合中的每个点，利用 KNN 算法在所有点集合中寻找最近的 nsample 个邻居点，并返回这些邻居点的索引；\",\"相对坐标计算：将每个查询点的邻居点坐标减去查询点自身坐标，得到以查询点为原点的局部相对坐标系；\",\"特征分组：根据邻居点索引，提取对应的特征向量，形成每个查询点的邻域特征集合。\",\"该方法的核心作用是将无序的点云数据转换为有序的局部邻域结构，为后续的注意力计算提供空间上下文信息。完整代码实现如下所示:\",\"def queryandgroup(nsample, xyz, new_xyz, feat, idx, offset, new_offset, use_xyz=True): \\\"\\\"\\\" 查询并分组函数：为每个查询点找到最近邻并分组其特征 input: nsample: 最近邻数量 xyz: 所有点的坐标 (n, 3) new_xyz: 查询点的坐标 (m, 3) feat: 所有点的特征 (n, c) idx: 预计算的最近邻索引，如果为None则重新计算 offset: 每个batch的点的结束索引 (b) new_offset: 每个batch的查询点的结束索引 (b) use_xyz: 是否在输出中包含相对坐标信息 output: new_feat: 分组后的特征，如果use_xyz=True则为(m, nsample, 3+c)，否则为(m, nsample, c) grouped_idx: 分组后的索引 (m, nsample) \\\"\\\"\\\" assert xyz.is_contiguous() and new_xyz.is_contiguous() and feat.is_contiguous() # 如果没有指定查询点，则使用所有点作为查询点 if new_xyz is None: new_xyz = xyz # 如果没有提供预计算的索引，则调用KNN查询函数计算 if idx is None: idx, _ = knnquery(nsample, xyz, new_xyz, offset, new_offset) # (m, nsample) n, m, c = xyz.shape[0], new_xyz.shape[0], feat.shape[1] # 根据索引分组坐标：获取每个查询点的邻居坐标 grouped_xyz = xyz[idx.view(-1).long(), :].view(m, nsample, 3) # (m, nsample, 3) # 计算相对坐标：邻居坐标减去查询点坐标（局部坐标系）： （200，8，3） - （200，1，3）= （ 200,8,3 ） grouped_xyz -= new_xyz.unsqueeze(1) # (m, nsample, 3) # 根据索引分组特征：获取每个查询点的邻居特征 grouped_feat = feat[idx.view(-1).long(), :].view(m, nsample, c) # (m, nsample, c) # 根据use_xyz标志决定输出格式 if use_xyz: # 拼接相对坐标和特征：输出形状为(m, nsample, 3+c) return torch.cat((grouped_xyz, grouped_feat), -1) else: # 只返回特征：输出形状为(m, nsample, c) return grouped_feat\",\"KNNQuery 类实现了K近邻查询算法，其主要功能是为每个查询点寻找最近的邻居点。具体实现包含以下几个关键步骤：\",\"1. 问题背景：在批处理点云数据时，不同样本的点云可能包含不同数量的点（如第一个点云1024个点，第二个点云2048个点），因此需要使用 offset 和 new_offset 参数来标记每个batch中点云的边界范围。\",\"2. 算法流程：\",\"对于每个查询点，计算其与当前batch内所有候选点的欧几里得距离\",\"使用 torch.topk 函数选出距离最小的 nsample 个点\",\"返回最近邻点的索引和对应的距离值\",\"3. 实现特点：采用批处理方式提高计算效率，同时处理点云数量不一致的情况。完整代码实现如下:\",\"class KNNQuery(Function): @staticmethod def forward(ctx, nsample, xyz, new_xyz, offset, new_offset): \\\"\\\"\\\" KNN查询的前向传播函数 input: nsample: 需要查询的最近邻数量 xyz: 所有点的坐标 (n, 3) new_xyz: 查询点的坐标 (m, 3)，如果为None则使用xyz offset: 每个batch的点的结束索引 (b) new_offset: 每个batch的查询点的结束索引 (b) output: idx: 每个查询点的最近邻点索引 (m, nsample) dist2: 每个查询点到最近邻点的平方距离 (m, nsample) \\\"\\\"\\\" if new_xyz is None: new_xyz = xyz # 如果没有指定查询点，则对所有点进行自查询 assert xyz.is_contiguous() and new_xyz.is_contiguous() m = new_xyz.shape[0] # 查询点的数量 # 初始化输出张量：索引矩阵和距离矩阵 idx = torch.zeros((m, nsample), dtype=torch.long) dist2 = torch.zeros((m, nsample)) # 按batch处理数据 start_idx, new_start_idx = 0, 0 # 当前batch的起始索引 for i in range(len(offset)): # 计算当前batch的结束索引 end_idx = offset[i] if i < len(offset) else xyz.shape[0] new_end_idx = new_offset[i] if i < len(new_offset) else m # 确保当前batch有数据需要处理 if end_idx > start_idx and new_end_idx > new_start_idx: # 提取当前batch的点坐标和查询点坐标 batch_xyz = xyz[start_idx:end_idx] batch_new_xyz = new_xyz[new_start_idx:new_end_idx] # 计算查询点与所有点之间的欧几里得距离平方 # 使用广播机制计算坐标差: (1,n,3) - (m,1,3) = (m,n,3) - (m,n,3) = (m_batch, n_batch, 3) diff = batch_xyz.unsqueeze(0) - batch_new_xyz.unsqueeze(1) # (m_batch, n_batch) - 平方距离矩阵 distances = torch.sum(diff ** 2, dim=-1) # 获取k个最近邻的索引和距离 actual_nsample = min(nsample, distances.shape[1]) # 实际可用的最近邻数量 # torch.topk返回最小的k个值及其索引: (m_batch,actual_nsample) knn_dist, knn_idx = torch.topk(distances, actual_nsample, dim=1, largest=False) # 如果实际邻居数量小于要求的nsample，进行填充 if actual_nsample < nsample: # 使用0填充索引和距离矩阵 padding = torch.zeros((knn_idx.shape[0], nsample - actual_nsample), dtype=knn_idx.dtype) knn_idx = torch.cat([knn_idx, padding], dim=1) knn_dist = torch.cat( [knn_dist, torch.zeros((knn_dist.shape[0], nsample - actual_nsample), dtype=knn_dist.dtype)], dim=1) # 将当前batch的结果存入总输出中，注意加上全局偏移量 idx[new_start_idx:new_end_idx] = knn_idx + start_idx dist2[new_start_idx:new_end_idx] = knn_dist # 更新下一个batch的起始索引 start_idx, new_start_idx = end_idx, new_end_idx # 返回最近邻索引和实际距离（加上小常数避免数值不稳定） return idx, torch.sqrt(dist2 + 1e-8)\"]},\"120\":{\"h\":\"Point Transformer 残差块\",\"t\":[\"PointTransformerBlock 类是 Point Transformer 残差块，是 Point Transformer 架构中的基本构建模块 , 其代码实现如下所示:\",\"class PointTransformerBlock(nn.Module): \\\"\\\"\\\" Point Transformer 残差块 实现预激活（Pre-Activation）的残差连接结构 \\\"\\\"\\\" expansion = 1 # 维度扩展系数，1表示输出维度与输入维度相同 def __init__(self, in_planes, planes, share_planes=8, nsample=16): \\\"\\\"\\\" 初始化函数 Args: in_planes: 输入特征维度 planes: 中间特征维度（也是输出维度，因为expansion=1） share_planes: 通道分组数，用于减少计算量 nsample: 每个点的邻居数量，用于kNN搜索 \\\"\\\"\\\" super(PointTransformerBlock, self).__init__() # 第一层：线性变换 + 批归一化（升维或保持维度） self.linear1 = nn.Linear(in_planes, planes, bias=False) # 无偏置，因为后面有BN self.bn1 = nn.BatchNorm1d(planes) # 批归一化，加速训练 # 核心：Point Transformer 自注意力层 self.transformer2 = PointTransformerLayer(planes, planes, share_planes, nsample) self.bn2 = nn.BatchNorm1d(planes) # Transformer后的批归一化 # 第三层：线性变换 + 批归一化（调整到最终输出维度） self.linear3 = nn.Linear(planes, planes * self.expansion, bias=False) self.bn3 = nn.BatchNorm1d(planes * self.expansion) # 最终批归一化 # 激活函数（原地操作节省内存） self.relu = nn.ReLU(inplace=True) # 注意：这里应该有残差连接的shortcut处理 # 如果 in_planes != planes * expansion，需要投影层 if in_planes != planes * self.expansion: self.shortcut = nn.Sequential( nn.Linear(in_planes, planes * self.expansion, bias=False), nn.BatchNorm1d(planes * self.expansion) ) else: self.shortcut = nn.Identity() # 恒等映射 def forward(self, pxo): \\\"\\\"\\\" 前向传播 Args: pxo: 元组 (p, x, o) p: 点坐标，形状 (n, 3) x: 点特征，形状 (n, in_planes) o: 批次索引，形状 (b) Returns: 元组 (p, x, o): 变换后的点坐标、特征和批次索引 \\\"\\\"\\\" p, x, o = pxo # 解包：点坐标, 点特征, batch索引 # 保存原始输入用于残差连接（需要处理维度匹配） identity = x # 第一层：线性变换 → BN → ReLU x = self.linear1(x) # (n, in_planes) → (n, planes) x = self.bn1(x) # 批归一化 x = self.relu(x) # ReLU激活 # 第二层：Point Transformer 自注意力 → BN → ReLU x = self.transformer2([p, x, o]) # 应用自注意力，形状 (n, planes) x = self.bn2(x) # 批归一化 x = self.relu(x) # ReLU激活 # 第三层：线性变换 → BN x = self.linear3(x) # (n, planes) → (n, planes * expansion) x = self.bn3(x) # 最终批归一化 # 残差连接：处理维度匹配问题 identity = self.shortcut(identity) # 如果需要，投影到相同维度 # 残差连接 + 激活 x += identity # 添加残差连接 x = self.relu(x) # 最终ReLU激活 # 返回相同格式的数据 return [p, x, o]\"]},\"121\":{\"h\":\"下采样层\",\"t\":[\"TransitionDown层的主要作用是在点云处理中进行层次化的特征学习和分辨率降低，类似于CNN中的下采样层（如池化层），但专门为点云数据设计。\",\"降低点云分辨率（当stride≠1时）\",\"# 输入: 1000个点 → 输出: 500个点（stride=2时） # 通过最远点采样选择最具代表性的点 subset\",\"增加特征维度\",\"# 输入: 64维特征 → 输出: 128维特征 # 扩展每个点的特征表达能力\",\"聚合局部信息\",\"# 为每个采样点聚合其邻域内的特征信息 # 使用最大池化提取最显著的特征\",\"保持几何结构\",\"# 下采样后的点云仍然保持原始点云的几何形状 # 最远点采样确保点分布均匀\",\"相当于CNN中的：\",\"MaxPooling（降低分辨率）\",\"Conv1x1（增加通道数）\",\"局部感受野（聚合邻域信息）\",\"的三合一操作，但专门为无序、不规则的点云数据设计。 通常用在层次化架构中：\",\"高分辨率 → TransitionDown → 中分辨率 → TransitionDown → 低分辨率 很多点 ↓ 中等点 ↓ 少量点 浅层特征 ↓ 中层特征 ↓ 深层特征\",\"完整代码实现如下:\",\"class TransitionDown(nn.Module): \\\"\\\"\\\" 点云下采样过渡层 功能：降低点云分辨率同时增加特征维度，保持批处理信息 \\\"\\\"\\\" def __init__(self, in_planes, out_planes, stride=1, nsample=16): \\\"\\\"\\\" 初始化下采样层 Args: in_planes: 输入特征维度 out_planes: 输出特征维度 stride: 下采样步长（stride=1表示无下采样，只做特征变换） nsample: 邻域采样点数，用于局部特征聚合 \\\"\\\"\\\" super().__init__() self.stride = stride # 下采样率 self.nsample = nsample # 邻域采样数 if stride != 1: # 下采样模式：需要处理坐标和特征，输出维度为3+in_planes self.linear = nn.Linear(3 + in_planes, out_planes, bias=False) # 无偏置，因为后面有BN self.pool = nn.MaxPool1d(nsample) # 最大池化，聚合邻域特征 else: # 无下采样模式：只做特征变换 self.linear = nn.Linear(in_planes, out_planes, bias=False) # 共享的批归一化和激活函数 self.bn = nn.BatchNorm1d(out_planes) # 批归一化 self.relu = nn.ReLU(inplace=True) # ReLU激活函数（原地操作节省内存） def forward(self, pxo): \\\"\\\"\\\" 前向传播 Args: pxo: 元组 (p, x, o) p: 点坐标，形状 (n, 3) x: 点特征，形状 (n, in_planes) o: 批次索引，形状 (b) - 每个元素表示该批次点的结束索引 Returns: 元组 (p, x, o): 下采样后的点坐标、特征和批次索引 \\\"\\\"\\\" p, x, o = pxo # 解包：点坐标, 点特征, 批次索引 if self.stride != 1: # ==================== 下采样模式 ==================== # 计算下采样后的批次索引 n_o n_o, count = [o[0].item() // self.stride], o[0].item() // self.stride for i in range(1, o.shape[0]): # 计算每个批次下采样后的点数 count += (o[i].item() - o[i-1].item()) // self.stride n_o.append(count) n_o = torch.IntTensor(n_o).to(o.device) # 转换为张量并保持设备一致 # 1. 最远点采样：从原始点云中选择代表性点 idx = pointops.furthestsampling(p, o, n_o) # (m) - 采样点索引，m为下采样后的点数 n_p = p[idx.long(), :] # (m, 3) - 下采样后的点坐标 # 2. 查询和分组：为每个采样点找到邻域并聚合特征 # 输出形状: (m, 3 + in_planes, nsample) # 包含：相对坐标(3) + 原始特征(in_planes) x = pointops.queryandgroup(self.nsample, p, n_p, x, None, o, n_o, use_xyz=True) # 3. 线性变换 + BN + ReLU # 先将特征维度转到最后： (m, 3+c, nsample) → (m, nsample, 3+c) x = self.linear(x.transpose(1, 2).contiguous()) # (m, nsample, out_planes) x = self.bn(x.transpose(1, 2).contiguous()) # (m, out_planes, nsample) - BN要求通道维度在前 x = self.relu(x) # ReLU激活 # 4. 最大池化：在邻域维度上池化，得到每个点的最终特征 x = self.pool(x) # (m, out_planes, 1) - 沿nsample维度池化 x = x.squeeze(-1) # (m, out_planes) - 移除最后一个维度 # 更新点和批次信息 p, o = n_p, n_o # 使用下采样后的点坐标和批次索引 else: # ==================== 无下采样模式 ==================== # 只进行特征变换：Linear → BN → ReLU x = self.linear(x) # (n, in_planes) → (n, out_planes) x = self.bn(x) # 批归一化 x = self.relu(x) # ReLU激活 # 返回相同格式的数据 return [p, x, o]\",\"最远点采样（FPS）算法：\",\"初始化：随机选择一个起始点\",\"迭代选择：\",\"计算所有点到已选点集的最小距离\",\"选择距离最大的点（即最远的点）\",\"重复直到选择足够多的点\",\"class FurthestSampling(Function): \\\"\\\"\\\" 最远点采样（Farthest Point Sampling）的自定义PyTorch函数 用于从点云中选择分布最均匀的点的子集 \\\"\\\"\\\" @staticmethod def forward(ctx, xyz, offset, new_offset): \\\"\\\"\\\" 前向传播：执行最远点采样算法 Args: ctx: 上下文对象，用于存储反向传播需要的信息 xyz: 输入点云坐标，形状为 (n, 3)，需要是内存连续的 offset: 原始批次索引，形状为 (b)，每个元素表示该批次点的结束位置 new_offset: 目标批次索引，形状为 (b)，每个元素表示下采样后该批次点的结束位置 Returns: idx: 采样点的索引，形状为 (m)，其中 m = new_offset[-1] \\\"\\\"\\\" # 确保输入张量是内存连续的 assert xyz.is_contiguous() n, b = xyz.shape[0], offset.shape[0] # n: 总点数, b: 批次数 # 创建输出索引张量，大小为下采样后的总点数 idx = torch.zeros(new_offset[b-1].item(), dtype=torch.long, device=xyz.device) # CPU实现的最远点采样算法 start_idx = 0 # 当前批次的起始索引 result_idx = 0 # 结果中的当前写入位置 # 遍历每个批次 for i in range(b): # 计算当前批次的结束索引 end_idx = offset[i] if i < b else n batch_size = end_idx - start_idx # 当前批次的点数 if batch_size > 0: # 创建选择标记数组，初始全为False selected = torch.zeros(batch_size, dtype=torch.bool, device=xyz.device) # 随机选择第一个点（这里固定选择第一个点） selected[0] = True # 初始化距离数组，存储每个点到已选点集的最小距离 dist = torch.full((batch_size,), float('inf'), device=xyz.device) # 计算当前批次需要采样的点数 if i == 0: num_to_sample = new_offset[i].item() # 第一个批次 else: num_to_sample = new_offset[i].item() - new_offset[i-1].item() # 后续批次 # 执行最远点采样迭代 for j in range(1, min(batch_size, num_to_sample)): # 更新距离：计算所有点到最新选中点的距离，并取最小值 last_selected = torch.where(selected)[0][-1] # 最后一个选中的点 new_dist = torch.sum((batch_xyz - batch_xyz[last_selected]) ** 2, dim=1) # 计算欧氏距离平方 dist = torch.minimum(dist, new_dist) # 保持每个点的最小距离 # 选择距离最大的点（离已选点集最远的点） next_point = torch.argmax(dist) selected[next_point] = True # 标记为已选 dist[next_point] = 0 # 将该点的距离设为0，避免重复选择 # 存储选中的索引（需要加上批次的起始偏移） selected_indices = torch.where(selected)[0] + start_idx idx[result_idx:result_idx + len(selected_indices)] = selected_indices result_idx += len(selected_indices) # 移动到下一个批次 start_idx = end_idx return idx\"]},\"122\":{\"h\":\"上采样层\",\"t\":[\"TransitionUp层的主要作用是在点云处理中恢复分辨率并融合多尺度特征，类似于CNN中的上采样层（如转置卷积），但专门为点云数据设计。\",\"恢复点云分辨率\",\"# 将低分辨率特征上采样到高分辨率 # 输入: 500个点 → 输出: 1000个点（分辨率恢复）\",\"多尺度特征融合\",\"# 融合编码器（深层抽象特征）和解码器（浅层细节特征） # 结合高层语义信息和底层几何细节\",\"特征增强\",\"# 通过全局上下文信息增强局部特征 # 每个点都能感知整个点云的全局信息\",\"构建解码器路径\",\"# 在U-Net类架构中逐步恢复空间分辨率 # 同时保持丰富的特征表示\",\"两种工作模式：\",\"模式1：全局特征增强（无跳跃连接）\",\"当前层特征 → 计算全局平均特征 → 与每个点特征拼接 → 增强表示\",\"模式2：跳跃连接融合（有跳跃连接）\",\"当前层特征 + 上采样的编码器特征 → 融合 → 输出\",\"通常用在解码器中，与编码器的TransitionDown对应：\",\"编码器: 高分辨率 → TransitionDown → 中分辨率 → TransitionDown → 低分辨率 解码器: 低分辨率 → TransitionUp → 中分辨率 → TransitionUp → 高分辨率\",\"相当于CNN中的：\",\"转置卷积/上采样（恢复分辨率）\",\"跳跃连接（融合多尺度特征）\",\"注意力机制（引入全局上下文）\",\"的组合操作，但专门为点云数据设计; 完整代码实现如下所示:\",\"class TransitionUp(nn.Module): \\\"\\\"\\\" 点云上采样过渡层 功能：恢复点云分辨率并融合不同层级的特征，实现特征上采样 类似于CNN中的上采样/转置卷积层，但专为点云设计 \\\"\\\"\\\" def __init__(self, in_planes, out_planes=None): \\\"\\\"\\\" 初始化上采样层 Args: in_planes: 输入特征维度 out_planes: 输出特征维度（如果为None，则输出维度与输入相同） \\\"\\\"\\\" super().__init__() if out_planes is None: # 模式1：输出维度与输入相同（通常用于解码器中间层） self.linear1 = nn.Sequential( nn.Linear(2 * in_planes, in_planes), # 将拼接后的特征映射回原维度 nn.BatchNorm1d(in_planes), # 批归一化 nn.ReLU(inplace=True) # ReLU激活 ) self.linear2 = nn.Sequential( nn.Linear(in_planes, in_planes), # 全局特征变换 nn.ReLU(inplace=True) # ReLU激活 ) else: # 模式2：改变输出维度（通常用于连接编码器和解码器） self.linear1 = nn.Sequential( nn.Linear(out_planes, out_planes), # 恒等映射变换 nn.BatchNorm1d(out_planes), # 批归一化 nn.ReLU(inplace=True) # ReLU激活 ) self.linear2 = nn.Sequential( nn.Linear(in_planes, out_planes), # 维度变换 nn.BatchNorm1d(out_planes), # 批归一化 nn.ReLU(inplace=True) # ReLU激活 ) def forward(self, pxo1, pxo2=None): \\\"\\\"\\\" 前向传播：两种模式 Mode 1 (pxo2 is None): 仅使用全局特征增强当前层特征 Mode 2 (pxo2 provided): 跳跃连接 - 融合深层特征和浅层特征 Args: pxo1: 元组 (p, x, o) - 当前层的点坐标、特征、批次索引 pxo2: 元组 (p, x, o) - 跳跃连接来自编码器的点坐标、特征、批次索引（可选） Returns: x: 上采样后的特征，形状与pxo1中的特征相同或变换后的维度 \\\"\\\"\\\" if pxo2 is None: # ==================== 模式1：全局特征增强 ==================== # 仅使用当前层特征进行自增强（无跳跃连接） _, x, o = pxo1 # 解包：忽略坐标，只取特征和批次索引 x_tmp = [] # 存储处理后的每个批次特征 # 按批次处理 for i in range(o.shape[0]): # 计算当前批次的起始、结束索引和点数 if i == 0: s_i, e_i, cnt = 0, o[0].item(), o[0].item() # 第一个批次 else: s_i, e_i = o[i-1].item(), o[i].item() # 后续批次 cnt = e_i - s_i # 当前批次点数 # 提取当前批次的特征 x_b = x[s_i:e_i, :] # (cnt, in_planes) # 计算全局平均特征并变换 global_feat = x_b.sum(0, keepdim=True) / cnt # (1, in_planes) - 批次平均特征 transformed_global = self.linear2(global_feat) # (1, in_planes) - 变换后的全局特征 # 将全局特征复制到每个点，并与原始特征拼接 x_b = torch.cat((x_b, transformed_global.repeat(cnt, 1)), dim=1) # (cnt, 2*in_planes) x_tmp.append(x_b) # 合并所有批次 x = torch.cat(x_tmp, 0) # (n, 2*in_planes) # 最终变换：降维 + 激活 x = self.linear1(x) # (n, in_planes) else: # ==================== 模式2：跳跃连接特征融合 ==================== # 融合编码器（深层）和解码器（浅层）的特征 p1, x1, o1 = pxo1 # 当前层（解码器）：通常分辨率更高 p2, x2, o2 = pxo2 # 跳跃连接层（编码器）：通常特征更抽象 # 处理当前层特征 x1_transformed = self.linear1(x1) # (n1, out_planes) # 处理跳跃连接特征并进行上采样（插值） x2_transformed = self.linear2(x2) # (n2, out_planes) # 将深层特征上采样到浅层分辨率：通过点云插值 # 将p2位置的特征插值到p1位置 x2_upsampled = pointops.interpolation(p2, p1, x2_transformed, o2, o1) # 特征融合：当前层特征 + 上采样的编码器特征 x = x1_transformed + x2_upsampled # 逐元素相加 return x\",\"该插值流程就是：对每个目标点，找到源点云的 k 个最近邻 → 根据反距离加权分配权重 → 用邻居特征加权求和 → 得到目标点特征。\",\"def interpolation(xyz, new_xyz, feat, offset, new_offset, k=3): \\\"\\\"\\\" 点云特征插值函数（基于 KNN + 反距离加权） Args: xyz: (m, 3) 源点云坐标（低分辨率点云，比如 encoder 输出） new_xyz: (n, 3) 目标点云坐标（高分辨率点云，比如 decoder 对应层） feat: (m, c) 源点云的特征 offset: (b) 每个 batch 的点数累积和（源点云） new_offset: (b) 每个 batch 的点数累积和（目标点云） k: int，插值时选取的近邻点个数（默认3） Returns: new_feat: (n, c)，插值到目标点上的特征 \\\"\\\"\\\" # 确保输入 tensor 在内存中是连续存放的，提高计算效率 assert xyz.is_contiguous() and new_xyz.is_contiguous() and feat.is_contiguous() # 在源点云 xyz 中，查找目标点云 new_xyz 的 k 个最近邻 # idx: (n, k) 最近邻点索引 # dist: (n, k) 最近邻点对应的欧氏距离 idx, dist = knnquery(k, xyz, new_xyz, offset, new_offset) # (n, 3), (n, 3) # 计算距离的倒数，避免除零加一个小量 dist_recip = 1.0 / (dist + 1e-8) # (n, k) # 对权重进行归一化，使每个点的权重和为 1 norm = torch.sum(dist_recip, dim=1, keepdim=True) # (n, 1) weight = dist_recip / norm # (n, k) # 初始化插值后的特征 (n, c)，全零 new_feat = torch.zeros((new_xyz.shape[0], feat.shape[1]), dtype=feat.dtype) # 遍历每个近邻点（这里默认 k=3） for i in range(k): indices = idx[:, i].long() # 第 i 个邻居的索引 # 有效性检查：确保索引在合法范围内 valid_mask = (indices >= 0) & (indices < feat.shape[0]) if valid_mask.any(): # 对有效邻居点：加权累加特征 # feat[indices] : (n, c) 邻居点特征 # weight[:, i].unsqueeze(-1) : (n, 1) 权重 # → 逐点乘法，最后累加到 new_feat new_feat[valid_mask] += feat[indices[valid_mask], :] * weight[valid_mask, i].unsqueeze(-1) return new_feat\"]},\"123\":{\"h\":\"Point Transformer 主模型\",\"t\":[\"class PointTransformerSeg(nn.Module): \\\"\\\"\\\" Point Transformer 用于点云语义分割的网络 采用编码器-解码器结构（类似 U-Net）， 编码器用于下采样和提取抽象特征， 解码器用于上采样和特征融合，最终输出每个点的类别概率。 \\\"\\\"\\\" def __init__(self, block, blocks, c=6, k=13): \\\"\\\"\\\" Args: block: 点变换模块类型（Point Transformer Block） blocks: 每一层包含 block 数量列表 c: 输入点特征维度（通常是 xyz + 额外特征） k: 分类类别数量 \\\"\\\"\\\" super().__init__() self.c = c self.in_planes, planes = c, [32, 64, 128, 256, 512] # 编码器各层输出通道 fpn_planes, fpnhead_planes, share_planes = 128, 64, 8 stride, nsample = [1, 4, 4, 4, 4], [8, 16, 16, 16, 16] # 下采样比例与邻居点数 # ========== 编码器 ========== # enc1: 分辨率 N/1 self.enc1 = self._make_enc(block, planes[0], blocks[0], share_planes, stride=stride[0], nsample=nsample[0]) # enc2: 分辨率 N/4 self.enc2 = self._make_enc(block, planes[1], blocks[1], share_planes, stride=stride[1], nsample=nsample[1]) # enc3: 分辨率 N/16 self.enc3 = self._make_enc(block, planes[2], blocks[2], share_planes, stride=stride[2], nsample=nsample[2]) # enc4: 分辨率 N/64 self.enc4 = self._make_enc(block, planes[3], blocks[3], share_planes, stride=stride[3], nsample=nsample[3]) # enc5: 分辨率 N/256 self.enc5 = self._make_enc(block, planes[4], blocks[4], share_planes, stride=stride[4], nsample=nsample[4]) # ========== 解码器 ========== # dec5: 解码器最深层，转换 p5 特征（is_head=True 表示输出头，不进行 skip 融合） self.dec5 = self._make_dec(block, planes[4], 2, share_planes, nsample=nsample[4], is_head=True) # dec4: 融合 p5 与 p4 self.dec4 = self._make_dec(block, planes[3], 2, share_planes, nsample=nsample[3]) # dec3: 融合 p4 与 p3 self.dec3 = self._make_dec(block, planes[2], 2, share_planes, nsample=nsample[2]) # dec2: 融合 p3 与 p2 self.dec2 = self._make_dec(block, planes[1], 2, share_planes, nsample=nsample[1]) # dec1: 融合 p2 与 p1 self.dec1 = self._make_dec(block, planes[0], 2, share_planes, nsample=nsample[0]) # 分类头：每个点输出 k 个类别得分 self.cls = nn.Sequential( nn.Linear(planes[0], planes[0]), nn.BatchNorm1d(planes[0]), nn.ReLU(inplace=True), nn.Linear(planes[0], k) ) # ========== 构建编码器层 ========== def _make_enc(self, block, planes, blocks, share_planes=8, stride=1, nsample=16): layers = [] # TransitionDown: 点云下采样 + 特征升维 layers.append(TransitionDown(self.in_planes, planes * block.expansion, stride, nsample)) self.in_planes = planes * block.expansion # 后续 block 叠加处理下采样后的特征 for _ in range(1, blocks): layers.append(block(self.in_planes, self.in_planes, share_planes, nsample=nsample)) return nn.Sequential(*layers) # ========== 构建解码器层 ========== def _make_dec(self, block, planes, blocks, share_planes=8, nsample=16, is_head=False): layers = [] # TransitionUp: 点云上采样 + 特征融合 # is_head=True 时表示输出层，不进行 skip 融合 layers.append(TransitionUp(self.in_planes, None if is_head else planes * block.expansion)) self.in_planes = planes * block.expansion # 后续 block 叠加处理上采样后的特征 for _ in range(1, blocks): layers.append(block(self.in_planes, self.in_planes, share_planes, nsample=nsample)) return nn.Sequential(*layers) # ========== 前向传播 ========== def forward(self, pxo): \\\"\\\"\\\" Args: pxo: tuple (p0, x0, o0) p0: (n,3) 点坐标 x0: (n,c) 点特征 o0: (b) 每个 batch 的点累积偏移 Returns: x: (n,k) 每个点的类别预测 \\\"\\\"\\\" p0, x0, o0 = pxo # 如果输入特征只有 xyz，直接使用 p0，否则拼接额外特征 x0 = p0 if self.c == 3 else torch.cat((p0, x0), 1) # ================= 编码器 ================= p1, x1, o1 = self.enc1([p0, x0, o0]) p2, x2, o2 = self.enc2([p1, x1, o1]) p3, x3, o3 = self.enc3([p2, x2, o2]) p4, x4, o4 = self.enc4([p3, x3, o3]) p5, x5, o5 = self.enc5([p4, x4, o4]) # ================= 解码器 ================= # 注意 decX[0] 是 TransitionUp，上采样层 # decX[1:] 是 Point Transformer Block，处理上采样后的特征 x5 = self.dec5[1:]([p5, self.dec5[0]([p5, x5, o5]), o5])[1] x4 = self.dec4[1:]([p4, self.dec4[0]([p4, x4, o4], [p5, x5, o5]), o4])[1] x3 = self.dec3[1:]([p3, self.dec3[0]([p3, x3, o3], [p4, x4, o4]), o3])[1] x2 = self.dec2[1:]([p2, self.dec2[0]([p2, x2, o2], [p3, x3, o3]), o2])[1] x1 = self.dec1[1:]([p1, self.dec1[0]([p1, x1, o1], [p2, x2, o2]), o1])[1] # ================= 分类头 ================= x = self.cls(x1) # 输出每个点的 k 类得分 return x\",\"切片 self.dec5[1:] : 在 Python 中，nn.Sequential 支持切片操作，返回的是 新的 nn.Sequential 对象，而不是元组。\"]},\"124\":{\"h\":\"Point Transformer V2 论文\",\"t\":[\"Point Transformer V2 论文\",\"论文: Point Transformer V2: Grouped Vector Attention and Partition-based Pooling 代码: https://github.com/Pointcept/PointTransformerV2\"]},\"125\":{\"h\":\"引言\",\"t\":[\"Point Transformer (PTv1) 首次将自注意力网络引入三维点云理解，并结合 向量注意力 与 U-Net 风格的编码器-解码器框架，在分类、分割等任务上取得了显著成绩。但其仍存在一些不足：\",\"向量注意力的权重编码依赖 MLP，当模型加深、通道数增加时，参数量急剧膨胀，容易导致严重过拟合并限制模型深度。\",\"三维点云的位置信息比二维像素更关键，但已有方法大多借鉴二维的编码方式，未能充分利用三维坐标中的几何特性。\",\"点云的不规则分布给池化带来挑战，以往方法依赖采样（如最远点采样、网格采样）与邻域查询（如 kNN、半径查询）的结合，既耗时又缺乏良好的空间对齐。\",\"提出的方法\",\"作者提出了新的 Point Transformer V2 (PTv2)，在多个方面改进了 PTv1：\",\"分组向量注意力（Grouped Vector Attention, GVA）\",\"将向量注意力划分为多个组，每组共享注意力权重，从而减少参数量，提升效率。 GVA 同时包含了 多头注意力 与 向量注意力 的优势，并且二者都可以看作是 GVA 的特例。\",\"改进的位置编码机制\",\"在关系向量中额外引入 位置编码乘子，强化三维点的空间关系，使模型更好地利用点云的几何信息。\",\"基于分区的池化策略\",\"将点云划分为 互不重叠的分区，并直接在同一区域内融合点信息，避免了传统方法对采样和邻域查询的依赖，实现了更高效、更精准的空间对齐。\"]},\"126\":{\"h\":\"方法\"},\"127\":{\"h\":\"3D-Vision Language\"},\"128\":{\"h\":\"SeqAfford 论文\",\"t\":[\"SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model 论文\",\"论文: SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model 代码: https://github.com/hq-King/SeqAfford\"]},\"129\":{\"h\":\"SpatialLM 论文\",\"t\":[\"SpatialLM: Training Large Language Models for Structured Indoor Modeling 论文\",\"论文: SpatialLM: Training Large Language Models for Structured Indoor Modeling 代码: https://manycore-research.github.io/SpatialLM/\"]},\"130\":{\"h\":\"简析PointNet++\",\"t\":[\"简析PointNet++\",\"论文: https://arxiv.org/abs/1706.02413 TensorFlow 版本代码: https://github.com/charlesq34/pointnet2 Pytorch 版本代码: https://github.com/yanx27/Pointnet_Pointnet2_pytorch\"]},\"131\":{\"h\":\"背景\",\"t\":[\"在PointNet中，网络对每一个点做低维到高维的映射，进行特征学习，然后把所有点映射到高维的特征通过最大池化最终表示全局特征。从本质上来说，要么对一个点做操作，要么对所有点做操作，实际上没有局部的概念(loal context) 。同时缺少 local context 在平移不变性上也有局限性（世界坐标系和局部坐标系）。对点云数据做平移操作后，所有的数据都将发生变化，导致所有的特征，全局特征都不一样了。对于单个的物体还好，可以将其平移到坐标系的中心，把他的大小归一化到一个球中，但是在一个场景中有多个物体时则不好办，需要对哪个物体做归一化呢？\",\"PointNet++ 解决了两个问题：如何生成点集的划分（Partitioning），以及如何通过局部特征学习器（Local Feature Learner）抽象点集或局部特征。\",\"生成点集的划分（Partitioning）：\",\"点集划分是指如何将一个大的点云分割成更小的、更易于管理的子集。这个过程类似于在传统的卷积神经网络中如何处理图像的小区域（或“patches”），以便可以在这些区域上应用局部操作。PointNet++需要一种方法来有效地将点云分割成多个部分，这样可以在每个部分上独立地学习特征。\",\"通过局部特征学习器（Local Feature Learner）抽象点集或局部特征：\",\"一旦点云被划分成小的子集，PointNet++的下一个任务是学习这些子集（或局部区域）的特征。这需要一个“局部特征学习器”，它能够从每个子集中提取有用的信息或特征。这与在传统CNN中学习图像局部区域特征的过程相似。\",\"两个问题是相关联的，因为：\",\"点集的划分必须产生跨分区的共同结构：为了能够在不同的局部子集上共享权重（类似于在CNN中权重共享的概念），PointNet++在进行点集划分时，需要确保这些划分具有一定的一致性或共同结构。这意味着即使是不同的局部子集，也应该以一种方式被处理，使得在它们之间可以共享学习到的特征表示的权重。这样做的目的是提高模型的效率和泛化能力，因为学习到的特征和权重可以在多个局部区域中复用。\",\"上述即为PointNet++设计中的两个核心挑战：\",\"如何有效地对点云进行分区，以便可以在这些分区上独立地学习特征。\",\"如何设计一个能够从这些局部分区中学习有用特征的机制，同时确保这些分区的处理方式允许在它们之间共享模型权重。 \",\"为了模仿传统卷积网络中的权重共享机制以提高学习效率和模型的泛化能力。\",\"PointNet++选择PointNet作为局部特征学习器（它是无序点云数据特征提取的高效算法）。\",\"可以理解为：PointNet++应用PointNet递归地对输入集进行嵌套分区。\"]},\"132\":{\"h\":\"模型结构\",\"t\":[\"以二维欧几里得空间为例，网络的分割和分类模型\",\"网络的每一组set abstraction layers主要包括3个部分：\",\"Sample layer : 对输入点进行采样，在这些点中选出若干个中心点。\",\"Grouping layer : 利用上一步得到的中心点将点集划分成若干个区域。\",\"PointNet layer : 对上述得到的每个区域进行编码，变成特征向量。\"]},\"133\":{\"h\":\"层次化点集特征学习\",\"t\":[\"层次化结构由多个set abstraction layers组成，在每个层上，一组点云被处理和抽象，以产生一个更少元素的新集合。set abstraction layers 由 Sampling layer、Grouping layer 和 PointNet layer 三部分组成。\",\"Sampling layer ：采样层 从输入点中选取一组点，定义局部区域的形心。\",\"Grouping layer ：通过查找形心点周围的“邻近点”来构建局部区域点集。\",\"PointNet layer ：使用mini-PointNet将局部区域编码为特征向量。\"]},\"134\":{\"h\":\"Sampling layer\",\"t\":[\"使用farthest point sampling（FPS）选择𝑁个点（相比于随机采样，该方法能更好的覆盖整个点集，具体选择多少个中心点以及邻域内的数量由超参数确定）\",\"FPS是一种在点云、图像处理或其他数据集中用于抽样的算法。目的是从一个大的数据集中选出一组代表性强的点，这些点彼此之间的最小距离尽可能大。\",\"作者通过FPS来抽样点集中较为重要的点。（即任务是找到点云集中的局部区域的中心点）\",\"可能存在的问题：计算成本、样本分布偏差（可能导致样本在高密度区域内过度集中，低密度区域则过于稀缺）、参数依赖（依赖初始点和距离度量方式的选择）、可能无法捕捉重要的几何细节。\"]},\"135\":{\"h\":\"Grouping layer\",\"t\":[\"文中作者通过Ball query来查询形心的邻居点。\",\"具体做法：给定两个超参数（每个区域中点的数量𝐾和query的半径𝑟），对于某个形心，Ball query找到该查询点在半径为𝑟范围内点，该范围确保局部区域的尺度是固定的。\",\"与K最近邻（kNN）查询相比，Ball query通过固定区域尺度而不是固定邻居数量来定义邻域。kNN查询寻找最近的K个邻居，但这可能导致所选邻域的实际尺寸随点的密度变化而变化，这在处理非均匀采样的数据时可能不是最优的选择。相反，Ball query通过确保每个局部区域都有一个固定的尺度，提高了模型在空间上的泛化能力。在实现时，通常会设置一个上限K，以限制每个局部区域中考虑的点的数量，以保持计算的可管理性。\",\"可改进的地方：对点云密度变换较为敏感、对参数选择依赖性高（半径太小可能无法有效捕获足够的局部详细，太大则可能导致不相关的点增多，使局部特征的表示不够精确）、计算效率问题、均匀性假设（Ball query是基于欧氏距离的均匀性假设）\",\"欧式距离的均匀性假设：即在欧氏空间中，两点的距离反映了这两点的实际相似度或关联度。\",\"基于以下前提： \",\"空间均匀性：空间是均匀和各向同性的，即任何方向上的度量都是等价的，距离的度量不受空间中位置的影响。\",\"距离直观性：在屋里空间或某些特定的抽象空间中，两个点之间的直线距离被认为是相似度或连接强度的直观表示。\",\"规模一致性：假设空间中所有区域的尺度或特征分布具有一定的一致性，即空间中的任何距离值具有相似的含义。\",\"总结: Grouping layer的任务是通过中心点找到邻居点，并将它们组织称为局部区域集。\"]},\"136\":{\"h\":\"PointNet layer\",\"t\":[\"局部坐标系转换：局部区域中的点转换成相对于形心的局部坐标系。\",\"局部区域中的每个点将相对于形心所在位置进行调整，以反映其相对位置。\",\"实现方法：通过将局部区域中的每个点-形心点的坐标来实现。\",\"特征编码：将转换后的坐标以及点的附加特征（文中的𝐶所表示的其他信息）一起送入mini-PointNet来提取局部区域中的特征。\",\"输出：利用相对坐标与点特征相结合的方式可以捕获局部区域中点与点之间的关系。\"]},\"137\":{\"h\":\"代码实现\",\"t\":[\"sample_and_group 这个函数的作用是从输入点云中：\",\"采样一些关键点\",\"为每个关键点构建局部邻域（局部区域）\",\"提取这些局部区域中的点及其特征\",\"def sample_and_group(npoint, radius, nsample, xyz, points, returnfps=False): \\\"\\\"\\\" Input: npoint: 采样的关键点数量 radius: 构建局部邻域的半径 nsample: 每个邻域内采样的关键点数量 xyz: 点云坐标数据 , [B, N, 3] points: 点的特征数据（可选）, [B, N, D] Return: new_xyz: 采样得到的关键点坐标, [B, npoint, nsample, 3] new_points: 每个关键点对应的局部区域点和特征, [B, npoint, nsample, 3+D] \\\"\\\"\\\" B, N, C = xyz.shape S = npoint # 使用 最远点采样（FPS） 从原始点云中选出 npoint 个具有代表性的点。 fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint] new_xyz = index_points(xyz, fps_idx) # [B, npoint, 3] # 对于每个选中的关键点，使用 球查询（Ball Query） 找出它周围距离小于 radius 的所有邻近点。 # 最多保留 nsample 个点，如果不够就重复最近的点来填充。 idx = query_ball_point(radius, nsample, xyz, new_xyz) # 把刚才找到的邻近点的坐标提取出来。 grouped_xyz = index_points(xyz, idx) # [B, npoint, nsample, 3] # 把它们相对于关键点的位置进行归一化（平移中心到以关键点为原点的局部坐标系上）。 grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C) # [B, npoint, nsample, 3] # 如果有额外的点特征（比如颜色、法线等），也一并提取。 if points is not None: grouped_points = index_points(points, idx) # 把邻近点的坐标和特征拼接在一起，形成最终的局部区域表示。 new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D] else: new_points = grouped_xyz_norm if returnfps: return new_xyz, new_points, grouped_xyz, fps_idx else: return new_xyz, new_points\",\"farthest_point_sample 这个函数实现的是最远点采样（Farthest Point Sampling, FPS）, 这是 PointNet++ 中用于从点云中选择具有代表性的采样点的一种策略。它的核心思想是：在点云中逐步选择离已选点尽可能远的点，使得采样点在整个点云空间中分布尽可能均匀 。\",\"def farthest_point_sample(xyz, npoint): \\\"\\\"\\\" Input: xyz: pointcloud data, [B, N, 3] npoint: number of samples Return: centroids: sampled pointcloud index, [B, npoint] \\\"\\\"\\\" device = xyz.device B, N, C = xyz.shape centroids = torch.zeros(B, npoint, dtype=torch.long).to(device) # 存储每次选出的“最远点”的索引。 distance = torch.ones(B, N).to(device) * 1e10 # 每个点到当前所有已选中心点的最小距离，初始设为一个极大值（1e10）。 farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device) # 初始时随机选择一个点作为第一个中心点。 batch_indices = torch.arange(B, dtype=torch.long).to(device) # 批次索引，用于快速访问每个 batch 的点。 # 重复 npoint 次，最终得到 npoint 个分布尽可能均匀的采样点索引。 for i in range(npoint): # 将当前选中的“最远点”索引保存下来； centroids[:, i] = farthest # （batch,npoint) # 取出当前最远点的坐标，用于后续计算其他点到该点的距离; centroid = xyz[batch_indices, farthest, :].view(B, 1, 3) # # （batch, 1 , 3) # 计算当前中心点与所有点之间的欧氏距离平方。 dist = torch.sum((xyz - centroid) ** 2, -1) # （batch,npoint) # 如果某个点到新中心点的距离比之前记录的“最小距离”还小，就更新它。 mask = dist < distance # 在 distance 中找到最大的那个距离对应的点，这就是下一个“最远点”。 distance[mask] = dist[mask] # （batch,npoint) # 在 distance 中找到最大的那个距离对应的点，这就是下一个“最远点”。 # 返回：一个元组：(values, indices)，分别是最大值和它们的位置索引。 farthest = torch.max(distance, -1)[1] # 返回位置索引 return centroids\",\"index_points 这个函数实现的是根据给定的索引 idx，从输入点云 points 中提取对应的点，形成一个新的子集。\",\"def index_points(points, idx): \\\"\\\"\\\" Input: points: input points data, [B, N, C] idx: sample index data, [B, S] Return: new_points:, indexed points data, [B, S, C] \\\"\\\"\\\" device = points.device B = points.shape[0] view_shape = list(idx.shape) view_shape[1:] = [1] * (len(view_shape) - 1) # 将view_shape的形状从[B, S]变成[B, 1]，便于广播 repeat_shape = list(idx.shape) repeat_shape[0] = 1 # 从[B, S]变成[1, S] # 从点云中根据索引提取特定点 (看不懂下面两行代码的话，可以先去了解一下python中的高级索引机制)。 batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape) new_points = points[batch_indices, idx, :] # （batch,npoint,3) return new_points\",\"query_ball_point 这个函数的作用是从点云中找出每个查询点周围一定半径范围内的邻近点索引。这个操作被称为 球查询（Ball Query）。\",\"def query_ball_point(radius, nsample, xyz, new_xyz): \\\"\\\"\\\" Input: radius: local region radius nsample: max sample number in local region xyz: all points, [B, N, 3] new_xyz: query points, [B, S, 3] Return: group_idx: grouped points index, [B, S, nsample] \\\"\\\"\\\" device = xyz.device B, N, C = xyz.shape _, S, _ = new_xyz.shape # 查询点数量（比如通过 FPS 得到的质心） # 构造一个从 0 到 N-1 的索引数组，代表原始点云中每个点的“身份证号” # 然后复制这个索引数组到每个 batch 和每个查询点上，形成 [B, S, N] 的结构 group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1]) # 计算每个查询点（new_xyz）与原始点（xyz）之间的平方欧氏距离 # 输出形状为 [B, S, N]：每个查询点对所有原始点的距离 sqrdists = square_distance(new_xyz, xyz) # 把距离超过 radius^2 的点全部替换为 N（一个非法索引），表示“这些人离我太远了，我不感兴趣。” group_idx[sqrdists > radius ** 2] = N # 对每个查询点的邻近点按索引排序（因为前面有 N，所以小的才是有效点） # 然后只保留前 nsample 个点 group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample] # 如果某个查询点附近的点太少，有些位置被标记为 N（无效）。 # 我们就用该查询点最近的那个点（第一个点）去填充这些空缺。 group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample]) mask = group_idx == N group_idx[mask] = group_first[mask] return group_idx # （batch,npoint,nsample)\",\"sample_and_group流程图\",\"sample_and_group_all 函数的作用是将整个点云视为一个“大局部区域”，不进行采样，直接对所有点进行特征提取，用于 PointNet++ 中的全局特征学习。\",\"def sample_and_group_all(xyz, points): \\\"\\\"\\\" Input: xyz: input points position data, [B, N, 3], 点云坐标数据 points: input points data, [B, N, D], 点云的额外特征（如法线、颜色等） Return: new_xyz: sampled points position data, [B, 1, 3] new_points: sampled points data, [B, 1, N, 3+D] \\\"\\\"\\\" device = xyz.device B, N, C = xyz.shape # 创建一个全零点作为“质心” # 虽然这个点没有实际意义，但它是为了统一接口设计的一个占位符 new_xyz = torch.zeros(B, 1, C).to(device) # 把原始点云 reshape 成一个大的局部区域 grouped_xyz = xyz.view(B, 1, N, C) # 如果有额外特征（比如法线、颜色），也一并加入 if points is not None: # 终输出的 new_points 是 [B, 1, N, 3+D]，代表每个 batch 中只有一组“大区域”的点及其特征 new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1) else: new_points = grouped_xyz return new_xyz, new_points # 全局质心点（0 位置）, 所有点组成的局部区域\",\"sample_and_group_all流程图\",\"PointNetSetAbstraction（点集抽象层） 是 PointNet++ 中的核心模块 ， 它的作用是负责从输入的点云数据中采样关键点，构建它们的局部邻域区域，并通过一个小型 PointNet 提取这些区域的高维特征，从而实现点云的分层特征学习。\",\"class PointNetSetAbstraction(nn.Module): def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all): super(PointNetSetAbstraction, self).__init__() self.npoint = npoint # 采样的关键点数量 self.radius = radius # 构建局部邻域的半径 self.nsample = nsample # 每个邻域内采样的关键点数量 self.mlp_convs = nn.ModuleList() self.mlp_bns = nn.ModuleList() last_channel = in_channel # 输入点的特征维度 for out_channel in mlp: self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1)) self.mlp_bns.append(nn.BatchNorm2d(out_channel)) last_channel = out_channel self.group_all = group_all def forward(self, xyz, points): \\\"\\\"\\\" Input: xyz: input points position data, [B, C, N] points: input points data, [B, D, N] Return: new_xyz: sampled points position data, [B, C, S] new_points_concat: sample points feature data, [B, D', S] \\\"\\\"\\\" xyz = xyz.permute(0, 2, 1) # [B, N, C] if points is not None: points = points.permute(0, 2, 1) # 如果 group_all=True，则对整个点云做全局特征提取。 if self.group_all: new_xyz, new_points = sample_and_group_all(xyz, points) else: # 否则使用 FPS（最远点采样）选关键点，再用 Ball Query 找出每个点的局部邻近点。 # 参数: 质点数量，采样半径，采样点数量，点坐标，点额外特征 new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points) # 局部特征编码（Mini-PointNet） # new_xyz: sampled points position data, [B, npoint, C] # new_points: sampled points data, [B, npoint, nsample, C+D] # 把邻域点的数据整理成适合卷积的格式 [B, C+D, nsample, npoint] new_points = new_points.permute(0, 3, 2, 1) # 使用多个 Conv2d + BatchNorm + ReLU 层提取特征 for i, conv in enumerate(self.mlp_convs): bn = self.mlp_bns[i] new_points = F.relu(bn(conv(new_points))) # [B, out_channel , nsample, npoint] # 对每个局部区域内所有点的最大响应值进行池化，得到该区域的固定长度特征表示。 # 在 new_points 的第 2 个维度（即每个局部邻域内的点数量维度）上做最大池化（max pooling）。 # 输出形状为 [B, out_channel, npoint]，即每个查询点有一个特征向量。 new_points = torch.max(new_points, 2)[0] # [B, out_channel, npoint] new_xyz = new_xyz.permute(0, 2, 1) # [B, C, npoint] return new_xyz, new_points # 查询点的位置(质心) ， 每个查询点点局部特征。\",\"最终每个采样得到的关键点所在的局部领域，都会被压缩为一个固定长度的特征向量。这个特征向量代表了这个局部区域的高维特征，它包含了这个区域内所有点的信息。\"]},\"138\":{\"h\":\"单尺度分组分类模型\",\"t\":[\"PointNet++ 的 单尺度分组（SSG）架构 ，通过多层 Set Abstraction 提取点云的层次化特征，并最终输出分类结果。\",\"Single-Scale Grouping (SSG)\",\"代码实现如下:\",\"# pointnet2_cls_ssg.py class get_model(nn.Module): # num_class: 输出类别数 # normal_channel: 是否包含法线信息（默认有 (x,y,z,nx,ny,nz)，否则只有 (x,y,z)） def __init__(self,num_class,normal_channel=True): super(get_model, self).__init__() in_channel = 6 if normal_channel else 3 self.normal_channel = normal_channel # PointNet++ 的核心就是逐层提取局部特征。这里的三个 SA 层构成了一个 三层分层特征学习结构 ： self.sa1 = PointNetSetAbstraction(npoint=512, radius=0.2, nsample=32, in_channel=in_channel, mlp=[64, 64, 128], group_all=False) self.sa2 = PointNetSetAbstraction(npoint=128, radius=0.4, nsample=64, in_channel=128 + 3, mlp=[128, 128, 256], group_all=False) self.sa3 = PointNetSetAbstraction(npoint=None, radius=None, nsample=None, in_channel=256 + 3, mlp=[256, 512, 1024], group_all=True) self.fc1 = nn.Linear(1024, 512) self.bn1 = nn.BatchNorm1d(512) self.drop1 = nn.Dropout(0.4) self.fc2 = nn.Linear(512, 256) self.bn2 = nn.BatchNorm1d(256) self.drop2 = nn.Dropout(0.4) self.fc3 = nn.Linear(256, num_class) def forward(self, xyz): B, _, _ = xyz.shape if self.normal_channel: norm = xyz[:, 3:, :] xyz = xyz[:, :3, :] else: norm = None l1_xyz, l1_points = self.sa1(xyz, norm) l2_xyz, l2_points = self.sa2(l1_xyz, l1_points) l3_xyz, l3_points = self.sa3(l2_xyz, l2_points) x = l3_points.view(B, 1024) x = self.drop1(F.relu(self.bn1(self.fc1(x)))) x = self.drop2(F.relu(self.bn2(self.fc2(x)))) x = self.fc3(x) x = F.log_softmax(x, -1) return x, l3_points\",\"完整的单尺度分组分类流程为:\",\"原始点云数据，首次sample，grouping，mini-PointNet后，得到:\",\"512 个关键点的坐标\",\"512 个关键点对应的局部区域特征向量\",\"二次sample，grouping，mini-PointNet后，得到:\",\"128 个关键点的坐标\",\"128 个关键点对应的局部区域特征向量\",\"三次sample，grouping，mini-PointNet后，得到:\",\"1 个关键点的坐标\",\"1 个关键点对应的全局区域特征向量\",\"获取全局区域特征向量后，通过全连接层进行分类。\"]},\"139\":{\"h\":\"非均匀密度下稳定的特征学习\",\"t\":[\"由于点集在不同区域可能会有不同的采样密度，这种非均匀性为点集特征学习带来了显著挑战。在密集采样的区域中学到的特征可能无法很好地泛化到稀疏采样的区域，反之亦然。因此，为了解决这一问题，PointNet++提出了密度自适应PointNet层，包含两种适应性特征学习层：多尺度分组（Multi-Scale Grouping, MSG）和多分辨率分组（Multi-Resolution Grouping, MRG）。\"]},\"140\":{\"h\":\"多尺度分组 （Multi-Scale Grouping）\",\"t\":[\"MSG通过应用不同尺度的分组层（按照不同的搜索半径或领域大小对点集进行分组），然后通过对应的PointNets提取每个尺度上的特征来捕获多尺度模式。不同尺度的特征被串联形成多尺度特征向量。这种方法使网络能够通过在训练期间随机丢弃输入点（称为随机输入丢弃 - random input dropout）来学习优化的策略，以结合来自不同尺度的特征。这样，网络在训练时被呈现了不同稀疏度的点集，从而学会根据输入数据的变化自适应地加权不同尺度上检测到的模式。\",\"多尺度分组\",\"具体来说，在MSG中，网络对于每个选定的形心点，按照几个预定义的半径值来搜索周围的邻近点。每个半径定义了一个局部邻域的大小，因此每个质心将根据这些不同的半径值与其周围点形成多个点集群。这样，对于每个质心点，网络不是只捕获一个尺度上的局部特征，而是能够捕获多个尺度上的局部特征。\",\"每个尺度（或每组邻域大小）的点集群都将独立地送入对应的PointNet网络进行特征提取，之后这些不同尺度上提取的特征被串联起来，形成一个综合的多尺度特征表示。这种方法使得网络能够在细节丰富的区域（通过较小的邻域尺度捕获细节）和稀疏采样的区域（通过较大的邻域尺度避免过度稀疏的问题）中均能有效提取特征。\"]},\"141\":{\"h\":\"多尺度分组分类模型\",\"t\":[\"PointNetSetAbstractionMsg 这个模块实现了 PointNet++ 中的 多尺度特征提取机制 ：对于每个局部区域，使用多个不同大小的邻域球（multi-scale ball query），分别提取特征，然后将这些不同尺度的特征拼接在一起，以获得更强的局部几何感知能力。\",\"class PointNetSetAbstractionMsg(nn.Module): def __init__(self, npoint, radius_list, nsample_list, in_channel, mlp_list): super(PointNetSetAbstractionMsg, self).__init__() self.npoint = npoint # 要采样的质心点数量 self.radius_list = radius_list # 不同尺度的查询半径列表 self.nsample_list = nsample_list # 对应半径下最多取多少邻近点 self.conv_blocks = nn.ModuleList() self.bn_blocks = nn.ModuleList() # 为每个尺度构建一个独立的小型 PointNet（Conv2d + BN + ReLU） # 每个尺度可以有不同的网络深度和宽度 # 所有尺度的网络并行运行，最后拼接结果 for i in range(len(mlp_list)): convs = nn.ModuleList() bns = nn.ModuleList() last_channel = in_channel + 3 for out_channel in mlp_list[i]: convs.append(nn.Conv2d(last_channel, out_channel, 1)) bns.append(nn.BatchNorm2d(out_channel)) last_channel = out_channel self.conv_blocks.append(convs) self.bn_blocks.append(bns) def forward(self, xyz, points): \\\"\\\"\\\" Input: xyz: input points position data, [B, C, N] points: input points data, [B, D, N] Return: new_xyz: sampled points position data, [B, C, S] new_points_concat: sample points feature data, [B, D', S] \\\"\\\"\\\" xyz = xyz.permute(0, 2, 1) # [B, N, C] if points is not None: points = points.permute(0, 2, 1) B, N, C = xyz.shape S = self.npoint # 使用 FPS（最远点采样）选出 S 个关键点作为局部区域中心 new_xyz = index_points(xyz, farthest_point_sample(xyz, S)) new_points_list = [] for i, radius in enumerate(self.radius_list): K = self.nsample_list[i] # 对每个半径 radius，找出该尺度下每个质心点周围的邻近点 group_idx = query_ball_point(radius, K, xyz, new_xyz) grouped_xyz = index_points(xyz, group_idx) # 把这些点的坐标归一化到以质心为中心的局部坐标系下 grouped_xyz -= new_xyz.view(B, S, 1, C) # 如果有额外特征，也一并加入 if points is not None: grouped_points = index_points(points, group_idx) grouped_points = torch.cat([grouped_points, grouped_xyz], dim=-1) else: grouped_points = grouped_xyz # 对每个尺度的局部点集应用对应的 Conv2d + BN + ReLU grouped_points = grouped_points.permute(0, 3, 2, 1) # [B, D, K, S] for j in range(len(self.conv_blocks[i])): conv = self.conv_blocks[i][j] bn = self.bn_blocks[i][j] grouped_points = F.relu(bn(conv(grouped_points))) # 使用最大池化聚合局部信息，生成固定长度的特征向量 new_points = torch.max(grouped_points, 2)[0] # [B, D', S] # 所有尺度的特征保存到 new_points_list new_points_list.append(new_points) new_xyz = new_xyz.permute(0, 2, 1) # 把不同尺度学到的特征拼接在一起，形成最终的局部特征表示 new_points_concat = torch.cat(new_points_list, dim=1) # 最终输出就是： 一组新的关键点位置； 每个关键点的多尺度特征表示 return new_xyz, new_points_concat\",\"pointnet2_cls_msg 这个模型使用了 PointNet++ 的 多尺度分组（MSG）策略 ，通过多个局部区域球查询提取不同尺度的局部特征，逐层抽象后融合成全局特征，最后通过全连接层完成分类任务。\",\"# pointnet2_cls_msg.py class get_model(nn.Module): def __init__(self,num_class,normal_channel=True): super(get_model, self).__init__() in_channel = 3 if normal_channel else 0 self.normal_channel = normal_channel self.sa1 = PointNetSetAbstractionMsg(512, [0.1, 0.2, 0.4], [16, 32, 128], in_channel,[[32, 32, 64], [64, 64, 128], [64, 96, 128]]) self.sa2 = PointNetSetAbstractionMsg(128, [0.2, 0.4, 0.8], [32, 64, 128], 320,[[64, 64, 128], [128, 128, 256], [128, 128, 256]]) self.sa3 = PointNetSetAbstraction(None, None, None, 640 + 3, [256, 512, 1024], True) self.fc1 = nn.Linear(1024, 512) self.bn1 = nn.BatchNorm1d(512) self.drop1 = nn.Dropout(0.4) self.fc2 = nn.Linear(512, 256) self.bn2 = nn.BatchNorm1d(256) self.drop2 = nn.Dropout(0.5) self.fc3 = nn.Linear(256, num_class) def forward(self, xyz): B, _, _ = xyz.shape if self.normal_channel: norm = xyz[:, 3:, :] xyz = xyz[:, :3, :] else: norm = None l1_xyz, l1_points = self.sa1(xyz, norm) l2_xyz, l2_points = self.sa2(l1_xyz, l1_points) l3_xyz, l3_points = self.sa3(l2_xyz, l2_points) x = l3_points.view(B, 1024) x = self.drop1(F.relu(self.bn1(self.fc1(x)))) x = self.drop2(F.relu(self.bn2(self.fc2(x)))) x = self.fc3(x) x = F.log_softmax(x, -1) return x,l3_points\",\"MSG的关键优点在于它通过在训练期间的随机输入丢弃（即随机移除一部分输入点）来模拟不同的采样密度，从而训练网络在面对实际应用中可能遇到的各种采样密度时，能够自适应地选择最适合的特征尺度进行组合，以实现最佳的性能。这种方法大大增强了网络处理非均匀采样数据的能力，提高了模型的泛化性和稳健性。\",\"在训练时引入不同密度的点集情况，使网络能学习不同采样密度下局部点云特征的提取，捕获密集到稀疏采样区域内的多尺度信息 -- 通过随机丢弃来模拟不同密度的采样，使网络能够应对实际中各种密度变换的情况-提高模型的泛化性能。\",\"MSG相当于并联了多个hierarchical structure，每个结构中心点不变，但是尺度不同。通过PointNet获取每个形心多尺度信息，之后concat形成该区域提取的总特征。在训练时引入随机丢弃形心来模拟不同密度情况，提高算法鲁棒性。\"]},\"142\":{\"h\":\"多分辨率分组（Multi-Resolution Grouping）\",\"t\":[\"MSG方法虽然有效，但在计算上可能非常昂贵，尤其是在低层次上对每个质心点运行局部PointNet时。为此，MRG为一种低成本的替代方案。\",\"MRG通过结合来自不同分辨率的特征来实现效率和适应性的平衡。具体而言，MRG策略在处理每个局部区域时，不仅考虑从当前分辨率下抽象得到的特征，还考虑了从更低分辨率（即上一层级）直接提取的特征。这两种特征被concat为一个复合特征向量，为后续的处理步骤提供信息。\",\"多分辨率分组\",\"在MRG中，某一层次𝐿𝑖的区域特征是通过将来自下一级𝐿𝑖−1的子区域特征总结后的向量与直接处理该局部区域所有原始点的单个PointNet得到的特征向量进行concat得到的。当局部区域的密度较低时，由于子区域在计算第一个向量时包含的点更稀疏，因此可能比第二个向量更不可靠。在这种情况下，应该更高地加权第二个向量。相反，当局部区域的密度较高时，第一个向量提供了更细致的信息，因为它能够在更低层次上递归地检视更高分辨率。\",\"来自下一级的特征：首先，将来自下一级（更高分辨率）的特征进行汇总，形成一个特征向量。这一过程通过对每个子区域应用集合抽象层（set abstraction level）完成。\",\"直接处理的原始点特征：另一部分特征是通过在当前分辨率直接对所有原始点应用单个PointNet得到的。\"]},\"143\":{\"h\":\"点云语义分割\",\"t\":[\"PointNet++ 完成点云分割任务的过程是一个典型的“编码-解码”结构，结合了层级特征提取和多尺度融合机制。\",\"目标: 给定一个点云，模型需要为每个点预测一个类别标签（如桌子、椅子、墙壁等）。\",\"输入：xyz: [B, N, 3]\",\"输出：labels: [B, N, C]，其中 C 是类别数\",\"PointNet++ 分割的整体结构 :\",\"Input Points (xyz): [ B, N, 3 ] ↓ Set Abstraction Layers（编码器） ↓ Feature Vectors at Multiple Scales ↓ Feature Propagation Layers（解码器） ↓ Recovered Features at Original Resolution ↓ MLP + Softmax → Per-point Semantic Labels\",\"第一步：Set Abstraction（集合抽象）—— 编码器: 对点云进行下采样，并在每个局部区域提取特征。\",\"核心操作包括：\",\"FPS（Farthest Point Sampling）：从点云中选出有代表性的点作为中心点。\",\"Ball Query：为每个中心点找到其邻域内的点。\",\"Grouping：将邻域点组合成局部点云组。\",\"PointNet 操作：使用 T-Net 对局部点云进行变换，然后通过 MLP 提取特征。\",\"Pooling：对局部点云组做最大池化或平均池化，得到该区域的特征。\",\"多个 Set Abstraction 层堆叠，逐步减少点的数量，增加特征维度，形成多尺度特征表示。\",\"第二步：Feature Propagation（特征传播）—— 解码器: 从最稀疏的点开始，逐层将特征插值回原始点数量。\",\"特征插值方式：\",\"使用 反距离加权插值（IDW），即根据最近的几个邻近点的距离进行加权平均。\",\"可选地拼接 skip connection 中的原始特征（来自 Set Abstraction 前的某一层）。\",\"输入输出示例：\",\"def forward(xyz1, xyz2, points1, points2): # xyz1: 原始点坐标（多） # xyz2: 下采样点坐标（少） # points1: 原始点特征（可为空） # points2: 下采样点特征 return interpolated_points # 插值得到的密集特征，形状与 xyz1 一致\",\"多个 Feature Propagation 层堆叠，逐渐恢复点数，最终回到原始点数量。\",\"第三步：Head 预测头 —— 分类每个点: 对每个点的特征做一个简单的分类器，输出类别概率。\",\"实现方式：\",\"将最后一层 Feature Propagation 输出的特征送入一个小型 MLP。\",\"最后一层使用 Softmax（对于多分类）或 Sigmoid（对于多标签）激活函数。\",\"例如：\",\"mlp = nn.Sequential( nn.Conv1d(128, 128, 1), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.5), nn.Conv1d(128, num_classes, 1) ) logits = mlp(final_features) # shape: [B, C, N]\"]},\"144\":{\"h\":\"代码实现\",\"t\":[\"PointNet++ 的整体结构是一个典型的 编码器-解码器（Encoder-Decoder）架构 ：\",\"Set Abstraction 层 ：不断对点云进行下采样 + 提取局部特征（编码过程）\",\"Feature Propagation 层 ：从最稀疏的点开始，逐层恢复到原始点数（解码过程）\",\"[Input Points] ↓ SA Layer 1 → [Points: 1024 → 512] ↓ SA Layer 2 → [Points: 512 → 128] ↓ SA Layer 3 → [Points: 128 → 32] ↓ FP Layer 3 ← [Points: 32 → 128] ↓ FP Layer 2 ← [Points: 128 → 512] ↓ FP Layer 1 ← [Points: 512 → 1024] ↓ [Per-point Classification Head] ↓ [Output: per-point labels]\"]},\"145\":{\"h\":\"特征传播层\",\"t\":[\"PointNetFeaturePropagation 是 PointNet++ 中用于点云“特征传播”（Feature Propagation）的核心模块，主要作用是：\",\"将稀疏点集的特征插值回原始点集的位置上。\",\"换句话说：\",\"输入：少量点的坐标 + 特征（如经过下采样后的点）\",\"输出：在原始点数量下的每个点都拥有一个合理的特征向量\",\"这一步相当于图像任务中的 上采样（upsample）或转置卷积（transpose convolution） ，但在点云这种非结构化数据中，不能直接使用这些操作。\",\"class PointNetFeaturePropagation(nn.Module): def __init__(self, in_channel, mlp): \\\"\\\"\\\" 初始化函数，构建用于特征传播（上采样）的MLP层 参数： in_channel: 输入特征的通道数（维度） mlp: 一个列表，表示每一层MLP的输出通道数，例如 [64, 128] \\\"\\\"\\\" super(PointNetFeaturePropagation, self).__init__() # 用于保存卷积层和批归一化层 self.mlp_convs = nn.ModuleList() self.mlp_bns = nn.ModuleList() last_channel = in_channel # 当前输入通道数初始化为in_channel # 构建MLP层：每个层是一个Conv1d + BatchNorm1d + ReLU for out_channel in mlp: self.mlp_convs.append(nn.Conv1d(last_channel, out_channel, 1)) self.mlp_bns.append(nn.BatchNorm1d(out_channel)) last_channel = out_channel # 更新下一层的输入通道数 def forward(self, xyz1, xyz2, points1, points2): \\\"\\\"\\\" 前向传播函数：将稀疏点集points2插值到密集点集xyz1的位置上 参数： xyz1: 原始点坐标数据，形状 [B, C, N] （如 1024 个点） xyz2: 下采样后的点坐标数据，形状 [B, C, S] （如 256 个点） points1: 原始点对应的特征数据，形状 [B, D, N] （可为 None） points2: 下采样点对应的特征数据，形状 [B, D, S] 返回： new_points: 插值并融合后的特征，形状 [B, D', N] \\\"\\\"\\\" # 将坐标和特征从 [B, C, N] 转换为 [B, N, C] 格式，便于后续计算 xyz1 = xyz1.permute(0, 2, 1) # [B, N, C] xyz2 = xyz2.permute(0, 2, 1) # [B, S, C] points2 = points2.permute(0, 2, 1) # [B, S, D] B, N, C = xyz1.shape # 原始点数量 N _, S, _ = xyz2.shape # 下采样点数量 S # 如果只有1个下采样点，直接复制其特征到所有原始点 if S == 1: interpolated_points = points2.repeat(1, N, 1) # [B, N, D] else: # 计算原始点与下采样点之间的距离矩阵（欧氏距离平方） dists = square_distance(xyz1, xyz2) # [B, N, S] # 对每个原始点，找到最近的3个邻近点 dists, idx = dists.sort(dim=-1) dists = dists[:, :, :3] # 取最小的三个距离 [B, N, 3] idx = idx[:, :, :3] # 取对应的索引 [B, N, 3] # 使用反距离加权（IDW）计算权重: # 1.将距离转换为“权重”，距离越近，权重越大 dist_recip = 1.0 / (dists + 1e-8) # 避免除以零 # 2.对每个点的3个权重求和，得到归一化因子 norm = torch.sum(dist_recip, dim=2, keepdim=True) # 归一化因子 # 3.归一化权重，使得每个点的权重之和为1 weight = dist_recip / norm # 加权平均系数 [B, N, 3] # 为每个原始点，找到它最近的 3 个邻近点，根据距离分配权重，然后对它们的特征做加权平均，从而插值得到该点的特征。 # index_points: [B, S, D] -> [B, N, 3, D] # weight.view(B, N, 3, 1): 扩展维度后相乘 interpolated_points = torch.sum( # 1. 从下采样点中取出每个原始点对应的最近邻点的特征。 # points2: [B, S, D] —— 下采样点的特征（S 个点，每个点有 D 维特征） # idx: [B, N, 3] —— 每个原始点对应的 3 个最近邻点索引 # [B, N, 3, D] —— 每个原始点都有了它最近的 3 个邻近点的特征 index_points(points2, idx) # 将之前计算好的权重扩展维度，以便和特征相乘。 # weight: [B, N, 3] —— 每个点的三个邻近点的权重 # [B, N, 3, 1] —— 扩展后便于广播乘法 * weight.view(B, N, 3, 1), dim=2 ) # [B, N, D] # 如果原始点有特征，则拼接起来（skip connection） if points1 is not None: points1 = points1.permute(0, 2, 1) # [B, N, D] new_points = torch.cat([points1, interpolated_points], dim=-1) # [B, N, D1+D2] else: new_points = interpolated_points # [B, N, D] # 恢复张量格式为 [B, D, N]，以适配后面的卷积操作 new_points = new_points.permute(0, 2, 1) # [B, D', N] # 经过MLP进一步提取和融合特征 for i, conv in enumerate(self.mlp_convs): bn = self.mlp_bns[i] new_points = F.relu(bn(conv(new_points))) # Conv1d + BN + ReLU return new_points # 最终输出特征 [B, D', N]\",\"流程四步走：\",\"1️⃣ 找到邻居 “我这个点最近的3个熟人是谁？”\",\"计算每个原始点和下采样点之间的距离；\",\"找出最近的3个邻近点。\",\"2️⃣ 分配权重 “谁离我越近，说话越有分量。”\",\"根据距离反比加权（IDW），给这3个邻近点分配权重；\",\"权重归一化，确保它们加起来是1。\",\"3️⃣ 加权平均插值 “综合最近几个熟人的意见，猜出我的特征。”\",\"提取邻近点的特征；\",\"按照权重做加权平均；\",\"得到每个原始点的插值特征。\",\"4️⃣ 融合+增强 “如果我本来就有特征，那就一起用；再用MLP提提神。”\",\"如果原始点有自己的特征（points1），就拼接起来；\",\"经过几层 Conv1d + BN + ReLU，进一步提取和融合特征；\",\"输出最终的插值后特征。\",\"📦 输出结果\",\"new_points: 每个原始点都有了一个新的特征向量 [B, D', N]\"]},\"146\":{\"h\":\"点云语义分割模型\",\"t\":[\"下面给出的是一个基于PointNet++的点云语义分割模型定义 ，其主要功能是：\",\"对输入点云中的每个点进行分类（如桌子、椅子、地板等），输出每个点的类别概率。\",\"网络结构特点：\",\"使用 Set Abstraction（SA）层 进行多尺度特征提取和下采样；\",\"使用 Feature Propagation（FP）层 进行特征插值和上采样；\",\"最后通过两个卷积层输出每个点的分类结果；\",\"输出为 [B, N, num_classes]，即每个点都有一个类别预测。\",\"class get_model(nn.Module): def __init__(self, num_classes): \\\"\\\"\\\" 初始化 PointNet++ 分割网络 参数： num_classes: 分类类别数 \\\"\\\"\\\" super(get_model, self).__init__() # Set Abstraction 层（编码器部分） # 每层逐步下采样，并提取更高级别的局部特征 self.sa1 = PointNetSetAbstraction(npoint=1024, radius=0.1, nsample=32, in_channel=9+3, mlp=[32, 32, 64], group_all=False) self.sa2 = PointNetSetAbstraction(npoint=256, radius=0.2, nsample=32, in_channel=64+3, mlp=[64, 64, 128], group_all=False) self.sa3 = PointNetSetAbstraction(npoint=64, radius=0.4, nsample=32, in_channel=128+3, mlp=[128, 128, 256], group_all=False) self.sa4 = PointNetSetAbstraction(npoint=16, radius=0.8, nsample=32, in_channel=256+3, mlp=[256, 256, 512], group_all=False) # Feature Propagation 层（解码器部分） # 从稀疏点恢复到原始点密度，逐层融合上下文信息 self.fp4 = PointNetFeaturePropagation(in_channel=768, mlp=[256, 256]) self.fp3 = PointNetFeaturePropagation(in_channel=384, mlp=[256, 256]) self.fp2 = PointNetFeaturePropagation(in_channel=320, mlp=[256, 128]) self.fp1 = PointNetFeaturePropagation(in_channel=128, mlp=[128, 128, 128]) # 最终分类头 self.conv1 = nn.Conv1d(128, 128, 1) self.bn1 = nn.BatchNorm1d(128) self.drop1 = nn.Dropout(0.5) self.conv2 = nn.Conv1d(128, num_classes, 1) def forward(self, xyz): \\\"\\\"\\\" 前向传播函数 输入： xyz: 点云数据，形状 [B, C, N] 返回： x: 每个点的分类结果，形状 [B, N, num_classes] l4_points: 最后一层抽象特征，用于其他任务 \\\"\\\"\\\" # l0 表示最原始的点云 l0_points = xyz l0_xyz = xyz[:, :3, :] # 只取 xyz 坐标，不带法向量或其他属性 # 编码器：层层下采样并提取特征 l1_xyz, l1_points = self.sa1(l0_xyz, l0_points) # 1024 points l2_xyz, l2_points = self.sa2(l1_xyz, l1_points) # 256 points l3_xyz, l3_points = self.sa3(l2_xyz, l2_points) # 64 points l4_xyz, l4_points = self.sa4(l3_xyz, l3_points) # 16 points # 解码器：层层插值并融合特征 l3_points = self.fp4(l3_xyz, l4_xyz, l3_points, l4_points) # 64 → 64 l2_points = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points) # 256 → 256 l1_points = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points) # 1024 → 1024 l0_points = self.fp1(l0_xyz, l1_xyz, None, l1_points) # 4096 → 4096 # MLP 头部处理：进一步增强特征 x = self.drop1(F.relu(self.bn1(self.conv1(l0_points)), inplace=True)) # [B, 128, N] x = self.conv2(x) # [B, num_classes, N] # Softmax 分类 x = F.log_softmax(x, dim=1) # [B, num_classes, N] # 调整维度，返回 [B, N, num_classes] x = x.permute(0, 2, 1) return x, l4_points # 返回每个点的分类结果和抽象特征\"]},\"147\":{\"h\":\"简析PointNet\",\"t\":[\"简析PointNet网络模型及其背后原理\",\"论文: https://arxiv.org/abs/1612.00593 TensorFlow 版本代码: https://github.com/charlesq34/pointnet Pytorch 版本代码: https://github.com/fxia22/pointnet.pytorch\"]},\"148\":{\"h\":\"核心\",\"t\":[\"问题背景: 点云是三维几何数据的一种重要表示形式，但由于其无序性和非规则性，传统卷积神经网络难以直接处理。\",\"❌ 传统方法的缺陷 ：\",\"将点云转换为体素网格（voxel grid）或图像视图（multi-view rendering）， 这些方法会导致信息损失、计算量大、不灵活等问题。\",\"🌟 PointNet 的创新点 ：\",\"直接以点集作为输入，避免了复杂的预处理；\",\"设计了一个统一架构，适用于分类、物体分割和场景语义解析；\",\"利用对称函数（如最大池化）实现点集顺序不变性；\",\"引入 T-Net（空间变换网络）标准化输入点云和特征空间。\"]},\"149\":{\"h\":\"难点\",\"t\":[\"点云的无序性（Unordered）: 点云是点的集合，没有固定顺序；模型必须对输入点的排列顺序不敏感（permutation invariant）。\",\"点之间存在相互作用（Interaction among points）: 点与点之间有空间关系，需要捕捉局部结构。\",\"对几何变换的不变性（Invariance under transformations）: 模型输出应不受刚性变换影响（如旋转、平移）。\",\"输入点云可能缺失或包含噪声（Missing or noisy points）: 实际采集的点云常有遮挡、稀疏、异常值等问题。\"]},\"150\":{\"h\":\"解决方案\",\"t\":[\"✅ 难点 1：点云的无序性 → 使用对称函数（Symmetric Function）\",\"使用 max pooling 作为对称函数，聚合所有点的信息；\",\"所有点经过共享参数的 MLP 提取特征；\",\"最终输出与点的顺序无关；\",\"原理说明：\",\"f({x1, ..., xn}) ≈ g(h(x1), ..., h(xn)) = γ(MAX(h(x1), ..., h(xn)))\",\"其中：\",\"h(xi) 是每个点的高维特征；\",\"MAX 是 max pooling 函数；\",\"γ 是后续的全连接网络；\",\"整个函数 f 是对称的，即对点顺序不敏感。\",\"效果：\",\"实验证明 max pooling 比排序、RNN、average pooling 更有效；s\",\"PointNet 可以处理任意顺序的点集；\",\"✅ 难点 2：点之间的相互作用 → 设计局部 + 全局信息融合机制\",\"在分割任务中，将全局特征与每个点的局部特征拼接起来；\",\"这样每个点在预测标签时都能看到整个物体的上下文；\",\"效果：\",\"显著提升了分割性能；\",\"让模型既关注局部细节，又理解整体结构；\",\"✅ 难点 3：对几何变换的不变性 → 引入 T-Net（空间变换网络）\",\"引入两个空间变换网络： \",\"STN3d：对输入点云做刚性变换（3×3 矩阵）；\",\"STNkd：对特征空间做变换（64×64 矩阵）；\",\"加入正则项约束变换矩阵接近正交：\",\"L_reg = ||I - A @ A^T||_F^2\",\"效果：\",\"PointNet 对点云的旋转、平移等变换具有鲁棒性；\",\"提升了模型的泛化能力和稳定性；\",\"✅ 难点 4：输入点云可能缺失或含有异常点 → 理论分析保证模型鲁棒性\",\"理论上证明 PointNet 学到的是一个“关键点集”（critical point set），即只依赖一小部分关键点就能判断整体形状；\",\"即使丢失一些点或加入异常点，只要关键点还在，结果就不会变；\",\"定理表明：\",\"小扰动不会改变函数输出；\",\"网络输出由一个有限子集 CS 决定（大小不超过 bottleneck 维度 K）；\",\"CS 是关键点集合，NS 是最大可容忍的点云范围；\",\"实验验证：\",\"即使 50% 的点缺失，分类准确率仅下降约 3.7%；\",\"对异常点也有一定容忍能力；\",\"✅ 总结: PointNet 通过 max pooling 实现对称性，结合 T-Net 实现变换不变性，并通过局部+全局特征融合机制实现强大的点云建模能力，解决了点云处理中的四大技术难点，为后续三维深度学习奠定了基础。\"]},\"151\":{\"h\":\"代码(Pytorch版本)\",\"t\":[\"PointNet网络模型结构图\"]},\"152\":{\"h\":\"输入标准化\",\"t\":[\"在 PointNet 架构中，第一层是一个叫做 STN3d（Spatial Transformer Network for 3D points） 的模块，它的目标是：\",\"✅ 对输入的点云做刚性变换（如旋转 + 平移），使其姿态统一，提升模型鲁棒性。\",\"这是因为在实际采集过程中，点云的姿态可能各不相同（比如椅子朝向不同、扫描角度不同等），如果不加处理，会影响特征提取的一致性。\",\"STN3d 是一个小型神经网络，专门用于预测一个 3×3 的变换矩阵 ，这个矩阵表示对点云所做的变换（通常是旋转或反射）。\",\"它具有以下特点：\",\"输入是原始点云（shape: (B, 3, N)）；\",\"输出是一个变换矩阵（shape: (B, 3, 3)）；\",\"这个变换矩阵是近似正交的，保证变换是刚性的；\",\"变换矩阵会通过 torch.bmm() 应用到原始点云上（这一步不在 STN3d 类中）；\",\"目的是让点云“摆正”，便于后续处理。\",\"代码实现:\",\"class STN3d(nn.Module): def __init__(self): super(STN3d, self).__init__() # 使用 1D 卷积 处理点云数据（每个点有 3 个坐标值） # kernel_size=1 表示只在通道维度操作，不考虑空间邻域关系 # 提取每一点的特征向量（从 3 → 64 → 128 → 1024） self.conv1 = torch.nn.Conv1d(3, 64, 1) self.conv2 = torch.nn.Conv1d(64, 128, 1) self.conv3 = torch.nn.Conv1d(128, 1024, 1) # 经过全局池化后得到一个全局特征向量（1024维） # 用全连接层逐步压缩到 9 个输出 → 对应一个 3x3 的变换矩阵 self.fc1 = nn.Linear(1024, 512) self.fc2 = nn.Linear(512, 256) self.fc3 = nn.Linear(256, 9) # 所有卷积和 FC 层后面都加了 BN 和 ReLU，帮助训练稳定收敛 self.relu = nn.ReLU() self.bn1 = nn.BatchNorm1d(64) self.bn2 = nn.BatchNorm1d(128) self.bn3 = nn.BatchNorm1d(1024) self.bn4 = nn.BatchNorm1d(512) self.bn5 = nn.BatchNorm1d(256) # x: (batch,3,point_size) def forward(self, x): # 获取当前 batch 的大小（有多少组点云） batchsize = x.size()[0] # CNN 逐点，通道维度特征提取阶段 x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) # x: (batch,1024,point_size) # 全局最大池化（Global Max Pooling） # 从所有点中选出每个通道的最大响应值，作为整个点云的“抽象”表示，shape: (B, 1024) # 返回：一个元组：(values, indices)，分别是最大值和它们的位置索引。 x = torch.max(x, 2, keepdim=True)[0] # x: (batch,1024,1) x = x.view(-1, 1024) # x: (batch,1024) # 全连接层预测变换矩阵 x = F.relu(self.bn4(self.fc1(x))) x = F.relu(self.bn5(self.fc2(x))) x = self.fc3(x) # x: (batch,9) # 加上单位矩阵作为初始偏置 # 初始假设变换为恒等变换（不做任何变化） iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1) if x.is_cuda: iden = iden.cuda() # 让网络从一个小扰动开始学习，更容易训练 x = x + iden # 最终 reshape 成 3x3 矩阵返回 x = x.view(-1, 3, 3) return x\",\"标准化的意义:\",\"✅ 1. 解决点云姿态不一致问题\",\"输入点云可能来自不同角度、不同位置；\",\"T-Net 把它们“对齐”到一个标准姿态；\",\"这样 PointNet 后续的特征提取更稳定。\",\"✅ 2. 提升模型鲁棒性\",\"如果没有 T-Net，PointNet 必须自己学会对各种姿态都识别准确；\",\"加入 T-Net 后，相当于加了一个“预处理层”，让模型更容易训练和泛化。\",\"神经网络的输出在训练初期往往接近于零，如果直接作为变换矩阵，会导致非正交、不稳定。PointNet 通过“加单位矩阵”的方式，让变换矩阵从一个恒等变换开始学习，并结合正则化损失，逐步向正交矩阵靠拢，从而保证变换是刚性的、稳定的。\"]},\"153\":{\"h\":\"正则化损失\",\"t\":[\"feature_transform_regularizer 是 PointNet 中用于约束变换矩阵接近正交性的正则化损失函数。\",\"🧠 为什么需要这个正则化项？\",\"在 PointNet 中，为了提升模型对点云姿态变化的鲁棒性，引入了两个变换网络：\",\"STN3d: 对原始点云做刚性变换（如旋转、反射），使其标准化。\",\"STNkd: 对特征空间做变换，使特征分布更稳定。\",\"这两个网络输出的是变换矩阵（分别是 3×3 和 k×k 的矩阵）。但由于它们是神经网络直接预测出来的，并不能保证这些矩阵是正交矩阵（orthogonal matrix） 。\",\"❗而只有正交矩阵才能表示刚性变换（rigid transformation），即只改变物体的方向而不改变形状和大小。\",\"所以我们需要加一个正则化项来鼓励变换矩阵接近正交矩阵 , 这就是 feature_transform_regularizer 的作用！\",\"def feature_transform_regularizer(trans): d = trans.size()[1] batchsize = trans.size()[0] # 构造一个单位矩阵 I，用于后续比较； # 添加 None 是为了扩展成 (1, d, d)，便于广播到整个 batch； I = torch.eye(d)[None, :, :] if trans.is_cuda: I = I.cuda() # 计算变换矩阵与其转置相乘后与单位矩阵之间的距离（Frobenius 范数），然后取 batch 平均值作为损失项，鼓励变换矩阵接近正交矩阵。 # Frobenius 范数（矩阵所有元素平方和开方） loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2))) return loss\"]},\"154\":{\"h\":\"特征提取\",\"t\":[\"PointNet 的核心特征提取模块 PointNetfeat ，它负责从输入点云中提取出可用于分类或分割的特征。\",\"class PointNetfeat(nn.Module): def __init__(self, global_feat = True, feature_transform = False): super(PointNetfeat, self).__init__() # 输入点云变换网络（3D） self.stn = STN3d() # 使用 Conv1D 对每个点进行特征提取； # 每个卷积层后跟一个 BatchNorm 层； # 最终输出高维特征（1024维）； self.conv1 = torch.nn.Conv1d(3, 64, 1) self.conv2 = torch.nn.Conv1d(64, 128, 1) self.conv3 = torch.nn.Conv1d(128, 1024, 1) self.bn1 = nn.BatchNorm1d(64) self.bn2 = nn.BatchNorm1d(128) self.bn3 = nn.BatchNorm1d(1024) # 全局特征开关：控制是否输出全局特征 self.global_feat = global_feat # 特征变换开关：控制是否使用 STN 对特征空间进行变换 self.feature_transform = feature_transform if self.feature_transform: self.fstn = STNkd(k=64) def forward(self, x): n_pts = x.size()[2] # 使用 STN3d 预测出一个变换矩阵； trans = self.stn(x) x = x.transpose(2, 1) # 将原始点云“摆正”； x = torch.bmm(x, trans) x = x.transpose(2, 1) # 再通过第一个卷积层提取初始特征； x = F.relu(self.bn1(self.conv1(x))) if self.feature_transform: trans_feat = self.fstn(x) x = x.transpose(2,1) x = torch.bmm(x, trans_feat) x = x.transpose(2,1) else: trans_feat = None # 提取更高维的特征； # 最后一层输出 shape: (B, 1024, N) pointfeat = x x = F.relu(self.bn2(self.conv2(x))) x = self.bn3(self.conv3(x)) # 从所有点中选出每个通道的最大响应值，作为整个点云的“抽象”表示，shape: (B, 1024) # 返回：一个元组：(values, indices)，分别是最大值和它们的位置索引。 x = torch.max(x, 2, keepdim=True)[0] x = x.view(-1, 1024) # 如果是分类任务 (global_feat=True)： if self.global_feat: return x, trans, trans_feat else: # 如果是分割任务 (global_feat=False)： x = x.view(-1, 1024, 1).repeat(1, 1, n_pts) return torch.cat([x, pointfeat], 1), trans, trans_feat\",\"✅ 如果是分类任务 (global_feat=True)，则返回：\",\"x: 全局特征 (B, 1024)\",\"trans: 输入点云变换矩阵\",\"trans_feat: 特征空间变换矩阵（可选）\",\"✅ 如果是分割任务 (global_feat=False)， 则返回：\",\"把全局特征复制 N 次并与每个点的局部特征，在通道维度进行拼接\",\"将每个点的局部特征与整个点云的全局特征拼接起来，让每个点都能看到上下文信息\",\"输出 shape: (B, 1088, N) ，即 1088 = 1024+64\"]},\"155\":{\"h\":\"分类任务\",\"t\":[\"PointNet 的分类模块 PointNetCls ，它基于前面的特征提取模块 PointNetfeat 来完成点云分类任务。\",\"class PointNetCls(nn.Module): def __init__(self, k=2, feature_transform=False): super(PointNetCls, self).__init__() self.feature_transform = feature_transform # 它使用 PointNetfeat 提取全局特征（1024维）； self.feat = PointNetfeat(global_feat=True, feature_transform=feature_transform) # 然后通过全连接层（MLP）将这些特征映射到类别空间； self.fc1 = nn.Linear(1024, 512) self.fc2 = nn.Linear(512, 256) self.fc3 = nn.Linear(256, k) self.dropout = nn.Dropout(p=0.3) self.bn1 = nn.BatchNorm1d(512) self.bn2 = nn.BatchNorm1d(256) self.relu = nn.ReLU() def forward(self, x): # 它使用 PointNetfeat 提取全局特征（1024维）； x, trans, trans_feat = self.feat(x) # 然后通过全连接层（MLP）将这些特征映射到类别空间； x = F.relu(self.bn1(self.fc1(x))) x = F.relu(self.bn2(self.dropout(self.fc2(x)))) x = self.fc3(x) # 最终输出每个类别的概率分布（log_softmax）； return F.log_softmax(x, dim=1), trans, trans_feat\"]},\"156\":{\"h\":\"分割任务\",\"t\":[\"PointNet 的分割模块 PointNetDenseCls ，它基于前面的特征提取模块 PointNetfeat 来完成点云物体分割任务。\",\"class PointNetDenseCls(nn.Module): def __init__(self, k = 2, feature_transform=False): super(PointNetDenseCls, self).__init__() self.k = k self.feature_transform=feature_transform self.feat = PointNetfeat(global_feat=False, feature_transform=feature_transform) self.conv1 = torch.nn.Conv1d(1088, 512, 1) self.conv2 = torch.nn.Conv1d(512, 256, 1) self.conv3 = torch.nn.Conv1d(256, 128, 1) self.conv4 = torch.nn.Conv1d(128, self.k, 1) self.bn1 = nn.BatchNorm1d(512) self.bn2 = nn.BatchNorm1d(256) self.bn3 = nn.BatchNorm1d(128) def forward(self, x): batchsize = x.size()[0] n_pts = x.size()[2] # 点的数量 # 调用 PointNetfeat 提取特征 # 最后将每个点的局部特征与整个点云的全局特征拼接起来，让每个点都能看到上下文信息 x, trans, trans_feat = self.feat(x) # 使用多层 Conv1D 层进一步融合局部 + 全局信息 # 最终输出 shape: (B, k, N) x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) x = self.conv4(x) # shape: (B, k, N) -> (B, N, k) , 即每个点的各个类别得分 x = x.transpose(2,1).contiguous() # 使用 log_softmax 得到 log 概率分布； x = F.log_softmax(x.view(-1,self.k), dim=-1) # shape: (B*N, k) x = x.view(batchsize, n_pts, self.k) # shape: (B, N, k) return x, trans, trans_feat\",\"✅ 1. 每个点都需要全局上下文\",\"仅靠局部特征很难判断某个点属于哪个部件（比如椅子的腿 vs 座位）；\",\"加上全局特征后，相当于告诉模型：“你知道吗，这是一个椅子”；\",\"这样模型就能根据上下文更准确地做出判断；\",\"✅ 2. 全局特征不能直接用于分割\",\"全局特征只有一份（(B, 1024)），无法直接用于每个点；\",\"所以要把它复制 N 次，变成 (B, 1024, N)；\",\"再与每个点的局部特征拼接；\"]},\"157\":{\"h\":\"缺陷\",\"t\":[\"🧠 一、核心问题：忽略局部结构信息\",\"PointNet 只通过 max pooling 聚合所有点的信息，忽略了局部邻域之间的结构关系。\",\"🔍 原因分析：\",\"PointNet 对每个点独立处理（参数共享），然后使用全局最大池化（Global Max Pooling）提取特征；\",\"这种设计使得网络只关注“最显著的点”，而没有建模点与点之间的局部几何关系；\",\"导致模型无法捕捉到更细粒度的几何细节，比如边缘、曲率、表面纹理等；\",\"💡 论文中的验证：\",\"在部件分割任务中，虽然 PointNet 表现不错，但在一些复杂区域（如椅子腿和桌面连接处）容易出错；\",\"分类任务中对缺失点具有一定鲁棒性，但遇到遮挡严重或点分布不均匀时性能下降明显；\",\"📉 二、分割任务依赖拼接机制，不够精细\",\"PointNet 的分割模块通过拼接全局特征 + 局部特征实现上下文感知，但这种方式表达能力有限。\",\"🔍 原理回顾：\",\"PointNet 的分割网络将全局特征复制 N 次并与每个点的局部特征拼接；\",\"然后使用 Conv1D 进行分类；\",\"实际上是用一个固定大小的全局特征去“广播”给每个点；\",\"⚠️ 问题所在：\",\"全局特征不能很好地反映每个点的上下文；\",\"拼接方式缺乏动态调整机制；\",\"难以区分语义相近但位置不同的区域（如桌子边缘 vs 中心）；\",\"🧱 三、对局部形状变化敏感\",\"PointNet 提取的关键点集合（critical point set）可能不足以代表复杂的局部结构。\",\"🔍 实验观察：\",\"在论文中提到，PointNet 学到的是一个关键点集合，这些点大致构成物体的骨架；\",\"如果这些关键点缺失或被遮挡，即使其他点都在，也可能导致错误分类；\",\"对于非刚性变形（如人体姿态变化），PointNet 的表现不如基于图结构的模型；\",\"📈 四、分类性能略逊于多视角方法\",\"在某些标准数据集（如 ModelNet40）上，PointNet 的分类准确率略低于 MVCNN 等基于图像的方法。\",\"方法\",\"分类准确率\",\"MVCNN（多视角 CNN）\",\"90.1%\",\"VoxNet（体素 CNN）\",\"85.9%\",\"PointNet\",\"89.2%\",\"虽然 PointNet 在速度和效率上占优，但在精度上仍略逊一筹。\",\"🧩 五、难以捕捉非刚性变换下的不变性\",\"PointNet 使用 T-Net 强制学习正交变换矩阵，只能处理刚性变换（旋转、反射），无法处理非刚性形变（如弯曲、拉伸）。\",\"🔍 举例说明：\",\"如果你有一张人脸的点云，由于表情不同，面部发生形变；\",\"PointNet 很难在这种情况下保持分类的一致性；\",\"相比之下，基于图卷积或注意力机制的模型更能捕捉这种非刚性变化；\",\"🧱 六、缺乏层次化特征提取机制\",\"PointNet 是一种单尺度网络，无法像 CNN 那样逐层提取多层次的抽象特征。\",\"✅ 后续改进：\",\"PointNet++ 正是对这一缺陷的改进；\",\"它引入了局部区域搜索 + 多尺度聚合机制；\",\"从而能够更好地捕捉点云的局部结构和层次信息；\",\"📊 七、对稀疏点云敏感\",\"当输入点云非常稀疏时（如只有几十个点），PointNet 的性能会显著下降。\",\"🔍 原因分析：\",\"PointNet 的全局特征来自于 max pooling；\",\"如果点太少，max pooling 得到的特征可能无法覆盖整个物体；\",\"特别是在遮挡严重的情况下，关键点可能丢失；\",\"📐 八、结构简单，不利于高维空间建模\",\"PointNet 的结构过于简单，难以建模更高维度的空间关系。\",\"✅ 后续发展：\",\"后续的 3D 深度学习模型（如 DGCNN、SpiderCNN、PointCNN、Transformer-based 点云模型）都尝试引入更复杂的结构来提升建模能力；\",\"如：构建点之间的邻接图、使用 attention、引入多尺度采样等；\",\"🧪 九、理论上的限制：受限于瓶颈维度 K\",\"PointNet 的表达能力受 max pooling 层维度 K 的限制，即 bottleneck dimension。\",\"📌 来自论文的理论分析：\",\"Theorem 2 表明，PointNet 的输出仅由一个不超过 K 个点的子集决定（critical point set），这意味着：\",\"如果 K 不够大，PointNet 可能遗漏重要细节；\",\"如果 K 太大，又会导致计算资源浪费；\",\"🧱 十、对噪声点敏感（尤其未训练时）\",\"虽然 PointNet 对少量异常点有一定鲁棒性，但如果训练时没有加入扰动，面对大量噪声点时效果较差。\",\"🔍 实验验证：\",\"论文中做了“插入异常点”的实验；\",\"结果显示，如果训练过程中加入了噪声，模型表现良好；\",\"否则，异常点会影响分类和分割性能；\",\"📉 十一、在大规模场景理解任务中表现一般\",\"PointNet 的时间复杂度虽然是 O(N)，但在处理超大规模点云时，仍然不如分块处理或多层级聚合模型高效。\",\"✅ 后续改进方向：\",\"使用分块策略（chunking）\",\"构建点云的层次化表示\",\"引入 attention 或图结构增强局部建模能力\",\"🧩 总结表格：PointNet 的主要缺陷\",\"缺陷类型\",\"描述\",\"是否被后续模型改进\",\"忽略局部结构\",\"仅靠 max pooling 提取特征，无局部聚合机制\",\"✅ PointNet++ 改进\",\"分割精度不高\",\"拼接机制不够精细，缺乏动态上下文感知\",\"✅ Transformer-based 改进\",\"无法处理非刚性变形\",\"T-Net 只学正交变换，无法应对弯曲、拉伸等形变\",\"✅ 图卷积、attention 改进\",\"分类精度略低\",\"在 ModelNet40 上略低于 MVCNN\",\"✅ 多视角 + PointNet 混合模型改进\",\"稀疏点云下性能差\",\"少量点无法覆盖关键结构\",\"✅ PointNet++ 改进\",\"局部建模能力弱\",\"无法捕捉边缘、曲率等细节\",\"✅ DGCNN、SpiderCNN 改进\",\"对噪声点敏感\",\"未经扰动训练时，对异常点鲁棒性差\",\"✅ 加入数据增强后缓解\",\"结构单一\",\"缺乏层次化、多尺度建模能力\",\"✅ PointNet++ / Transformer 改进\",\"📈 PointNet 的优势 vs 缺陷对比\",\"维度\",\"优势\",\"缺陷\",\"输入形式\",\"支持原始点云，无需预处理\",\"无法有效利用局部结构\",\"排列不变性\",\"完全支持\",\"无法区分顺序信息（如时间序列点云）\",\"变换不变性\",\"支持刚性变换标准化\",\"无法处理非刚性形变\",\"分类性能\",\"接近 SOTA\",\"略逊于多视角 CNN\",\"分割性能\",\"表现良好\",\"缺乏精细建模\",\"效率\",\"极其高效（O(N)）\",\"无法充分利用 GPU 并行优化\",\"扩展性\",\"易于扩展为检测、检索等任务\",\"表达能力受限于 max pooling 维度\",\"✅ 一句话总结：\",\"PointNet 的最大缺陷在于它“看不清细节”，只关注全局结构，忽视局部邻域关系，这使得它在细粒度识别、非刚性变形、稀疏点云等任务中表现受限，但它也为后续模型奠定了基础。\"]},\"158\":{\"h\":\"背景知识扫盲(可选)\"},\"159\":{\"h\":\"点云\",\"t\":[\"点云: 是一种表示三维空间中物体或场景的方式，它由大量带有位置信息的点组成。\",\"每个点通常包含：\",\"坐标信息 ：x, y, z（3D 空间中的位置）。\",\"可选属性：颜色（RGB）、法向量（Normal）、强度（Intensity）、时间戳等。\",\"表示形式:\",\"点云（Point Cloud）: 原始点集合：每个点有(x, y, z)坐标; 可选颜色、法向量等属性, 简洁、轻便; 保留原始几何信息,无序性、非结构化、难以用 CNN 处理。\",\"体素网格 (voxel grids) : 将空间划分成立方体格子，每个格子表示是否有物体; 结构规整，适合 3D CNN; 计算复杂度高、稀疏性强、精度受限。\",\"多视角图像（Multi-View Images）: 从多个角度渲染点云或 3D 模型为 2D 图像; 可使用成熟的 2D CNN 方法; 丢失部分几何信息，依赖视角选择。\",\"网格（Mesh）： 由三角形面片组成的 3D 模型； 包含表面细节，适合渲染； 难以自动构建，拓扑复杂。\"]},\"160\":{\"h\":\"对称函数\",\"t\":[\"对称函数（Symmetric Function）是一种对输入顺序不敏感的函数；换句话说，无论你如何打乱输入元素的顺序，输出结果都保持不变。\",\"🧠 数学定义:\",\"设是一个函数，如果对于任意排列（permutation），都有：\",\"那么就是一个 对称函数。\",\"PointNet 处理的是点云数据，而点云是无序集合（unordered set） ，即：\",\"点云中点的顺序不影响整体形状。\",\"所以模型必须具有对点顺序的不变性（permutation invariance）。\",\"这就要求网络中的某些关键操作必须是对称函数 ，才能保证整个网络输出与输入点的顺序无关。\",\"📦 常见的对称函数:\",\"函数\",\"描述\",\"是否可微\",\"应用场景\",\"最大池化（Max Pooling）\",\"取所有点的最大值：\",\"✅ 是\",\"PointNet 中的核心操作\",\"平均池化（Average Pooling）\",\"取所有点的平均值：\",\"✅ 是\",\"特征融合、平滑处理\",\"求和（Summation）\",\"所有点相加：\",\"✅ 是\",\"构建全局特征向量\",\"乘积（Product）\",\"所有点相乘：\",\"⚠️ 对数值变化敏感\",\"不常用，但可用于特定任务\",\"最小池化（Min Pooling）\",\"取最小值：\",\"✅ 是\",\"异常检测等特殊场景\",\"Softmax + 加权和（Attention-based Sum）\",\"根据注意力机制加权求和，权重由 softmax 得出\",\"✅ 是\",\"DGCNN、Transformer 中使用\",\"统计量（如方差、标准差）\",\"计算点集的分布特性\",\"✅ 是\",\"特征增强、异常检测\",\"集合函数近似器（如 Deep Sets）\",\"使用神经网络直接学习对称函数\",\"✅ 是\",\"更复杂的对称函数建模\"]},\"161\":{\"h\":\"刚性运动\",\"t\":[\"刚性运动(rigid motions) 是指：物体在空间中移动时，其形状和大小保持不变的运动方式 。\",\"刚性运动\",\"❌ 不改变\",\"移动、旋转\",\"非刚性运动\",\"✅ 改变\",\"弯曲、拉伸、缩放（非均匀）、变形\",\"刚性运动 = 平移 + 旋转，不改变物体形状和内部结构，只改变位置和朝向。\"]},\"162\":{\"h\":\"正交变换\",\"t\":[\"正交变换的本质是：只改变物体的方向（旋转），不改变形状和大小\",\"所以：\",\"正交变换包括：旋转 + 反射。\",\"不包括：缩放、剪切、拉伸等会导致形变的操作。\"]},\"163\":{\"h\":\"大语言模型\"},\"164\":{\"h\":\"BLIP 论文\",\"t\":[\"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 论文解读\",\"论文链接: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 代码链接: https://github.com/salesforce/BLIP\"]},\"165\":{\"h\":\"Introduction\",\"t\":[\"当前视觉-语言预训练（VLP）方法虽然在多模态任务上取得进展，但普遍存在两个问题：\",\"模型限制：编码器模型不适合文本生成任务；编码器-解码器模型难以用于图文检索。\",\"数据质量差：大多使用从网络收集的嘈杂图文对作为训练数据，监督信号不理想。\",\"BLIP（Bootstrapping Language-Image Pre-training）是一个新颖的 VLP 框架，兼顾理解与生成能力。其两大创新点：\",\"MED 模型结构（Multimodal Mixture of Encoder-Decoder）：\",\"同时支持编码器、图像条件编码器、图像条件解码器三种模式。\",\"联合训练三种任务：图文对比学习、图文匹配、图像条件语言建模。\",\"实现多任务预训练与灵活迁移。\",\"CapFilt 数据自举方法（Captioning and Filtering）：\",\"使用训练好的 MED 模型构建两个模块：\",\"描述器（captioner）生成图像的合成描述；\",\"过滤器（filter）剔除原始和生成的低质量描述。\",\"在保留信息的同时提升训练数据质量。\",\"实验结果与表现:\",\"BLIP 在多个任务（图文检索、图像描述、VQA 等）上取得最先进性能。\",\"同时，在两个视频-语言任务上以零样本方式迁移也表现优异。\",\"实验证明：描述器与过滤器的组合能显著提升性能，多样化描述更有利于学习。\"]},\"166\":{\"h\":\"Related Work\"},\"167\":{\"h\":\"视觉-语言预训练（VLP）\",\"t\":[\"现状问题：\",\"主流 VLP 方法依赖从网络抓取的图文对数据，虽然规模大，但包含大量噪声文本。\",\"尽管使用简单的过滤规则，噪声仍广泛存在。\",\"编码器模型适合理解类任务但难以生成文本；编码器-解码器适合生成任务但不适用于检索。\",\"BLIP 的改进：\",\"提出 CapFilt：通过“生成 + 过滤”的方式优化数据质量。\",\"提出 MED 模型结构：在保持预训练高效的前提下，同时兼顾理解与生成任务，提升泛化能力。\"]},\"168\":{\"h\":\"知识蒸馏（Knowledge Distillation）\",\"t\":[\"现有做法：\",\"知识蒸馏让小模型（学生）学习大模型（教师）的预测结果。\",\"自蒸馏也取得了不错效果，尤其在图像分类与部分 VLP 方法中已开始尝试。\",\"BLIP 的新视角：\",\"CapFilt 可视为一种结构化的知识蒸馏方式：\",\"Captioner 模块用生成的语义丰富描述进行蒸馏；\",\"Filter 模块通过剔除噪声文本完成隐式知识过滤。\"]},\"169\":{\"h\":\"数据增强（Data Augmentation）\",\"t\":[\"现有做法：\",\"图像任务中数据增强广泛应用，但语言任务的数据增强较困难。\",\"近年来生成模型被用于文本任务的样本合成，但多用于低资源语言场景。\",\"BLIP 的贡献：\",\"展示了在大规模视觉-语言预训练中使用合成图像描述的独特优势，提升了多模态学习效果。\"]},\"170\":{\"h\":\"Method\",\"t\":[\"在本文中，作者提出了一个统一的视觉语言预训练框架 BLIP，该方法能够有效从噪声图文对中学习。方法部分主要分为三个内容：模型架构 MED、预训练目标，以及数据集自举策略 CapFilt。\"]},\"171\":{\"h\":\"模型架构：MED（Multimodal Mixture of Encoder-Decoder）\",\"t\":[\"作者在模型中采用了 视觉 Transformer（ViT）（Dosovitskiy et al., 2021）作为图像编码器，它会将图像切分为 patch，并编码为一系列嵌入表示，其中额外添加的 [CLS] token 被用作图像的全局特征。相比使用预训练目标检测器（如 Chen et al., 2020），ViT 的使用更具计算效率，也被近年来的工作所采纳（Li et al., 2021a；Kim et al., 2021）。\",\"为了训练一个既具理解能力又具生成能力的统一模型，作者提出了一个多任务模型架构：多模态混合的编码器-解码器（MED）。该架构支持以下三种功能模式：\",\"Unimodal Encoder（单模态编码器）\",\"图像和文本分别编码。\",\"文本编码器使用 BERT（Devlin et al., 2019），在文本前添加 [CLS] token 来表示整句话的语义。\",\"Image-grounded Text Encoder（图像引导的文本编码器）\",\"在 BERT 的每个 block 中添加一层 cross-attention（CA），位于 self-attention（SA）和 FFN 之间。\",\"文本末尾添加一个 [Encode] token，输出的该 token 表示图文对的多模态表示。\",\"Image-grounded Text Decoder（图像引导的文本解码器）\",\"在 encoder 的基础上将双向 SA 替换为因果 self-attention，以实现生成任务。\",\"使用 [Decode] token 标记序列开始，使用 <eos> 表示结束。\",\"为了实现多任务高效预训练，模型设计上 encoder 和 decoder 共享除了 SA 层以外的所有参数。作者认为编码和解码之间的主要差异体现在 SA 层（前者为双向，后者为因果），而嵌入层、CA 和 FFN 层则可以共享。这样的共享设计能够提升训练效率，并有利于多任务学习。\"]},\"172\":{\"h\":\"预训练目标（ITC、ITM、LM）\",\"t\":[\"BLIP 同时优化三个预训练目标，其中包括两个理解任务和一个生成任务。每对图文样本只需通过一次视觉 Transformer，但文本 Transformer 会根据不同功能分支前向三次，以计算以下三种损失：\",\"Image-Text Contrastive Loss（ITC）\",\"启用 Unimodal Encoder，用于对齐图像和文本的表示空间。\",\"目标是使正样本的图文对在特征空间中接近，同时区分负样本。\",\"方法参考 Li et al. (2021a)，采用动量编码器来生成表示，并用其产生的软标签作为训练目标，从而考虑负样本中的潜在正样本。\",\"Image-Text Matching Loss（ITM）\",\"启用 Image-grounded Text Encoder，用于学习图文之间的细粒度对齐关系。\",\"本质是一个二分类任务，判断图文是否匹配，使用一个线性分类头进行预测。\",\"采用 Li et al. (2021a) 提出的 困难负样本挖掘策略，即在 batch 中优先选择对比相似度高的负样本来增强训练信号。\",\"Language Modeling Loss（LM）\",\"启用 Image-grounded Text Decoder，训练模型根据图像生成文本描述。\",\"使用交叉熵损失，训练模型以自回归方式生成文本；计算时引入 0.1 的标签平滑。\",\"相比于传统的 MLM，LM 更能增强模型将视觉信息转化为自然语言的能力。\"]},\"173\":{\"h\":\"CapFilt：图文数据的自举式清洗机制\",\"t\":[\"由于高质量人工标注图文对（如 COCO）数量有限，而自动爬取的网页图文对（）噪声严重，作者提出了一个新的数据处理流程：CapFilt（Captioning and Filtering），用于提升图文语料的质量。\",\"CapFilt 包含两个模块，分别用于生成和过滤文本：\",\"Captioner\",\"是一个图像引导的文本解码器，使用 LM 目标在 COCO 上轻量微调。\",\"输入网页图像 ，输出合成描述 （一张图对应一条生成的 caption）。\",\"Filter\",\"是一个图像引导的文本编码器，使用 ITC 和 ITM 目标进行微调。\",\"输入图像及文本，判断是否匹配；若 ITM 头预测为不匹配，则认为是噪声文本。\",\"过滤对象既包括网页原始文本 ，也包括合成描述 。\",\"最后，作者将通过 Filter 筛选出的图文对与人工标注数据结合，组成新的训练集，用于训练下一个更强的模型。\"]},\"174\":{\"h\":\"小结\",\"t\":[\"BLIP 在模型设计上提出了一个灵活、统一的多任务架构（MED），同时结合图文对比、匹配和生成等任务目标进行联合训练。在数据方面，通过 CapFilt 机制有效从网页图文对中挖掘高质量样本，显著扩展了训练数据的规模和质量。\",\"整体来看，BLIP 是一个兼顾理解与生成、统一建模与数据增强的多模态预训练方法，具有较强的实用性与拓展性。\",\"以下是对“Experiments and Discussions”部分翻译内容的总结，遵循您的格式规范：\"]},\"175\":{\"h\":\"Experiments and Discussions\"},\"176\":{\"h\":\"预训练细节\",\"t\":[\"BLIP 模型使用 PyTorch 实现，预训练环境为两个16-GPU节点。图像编码器基于在 ImageNet 上预训练的 ViT（参考 Touvron et al., 2020；Dosovitskiy et al., 2021），文本编码器则基于 BERTbase（Devlin et al., 2019）。模型变体包括 ViT-B/16 和 ViT-L/16，其中默认使用 ViT-B。\",\"训练配置如下：\",\"训练轮数为 20，批量大小为 2880（ViT-B）/ 2400（ViT-L）；\",\"优化器使用 AdamW（Loshchilov & Hutter, 2017），权重衰减为 0.05，学习率预热后分别达到 （ViT-B）和 （ViT-L），随后线性衰减；\",\"图像预训练分辨率为 ，微调时提升为 。\",\"所使用的训练数据总共包含约 1400 万张图像，覆盖以下数据集：\",\"COCO 和 Visual Genome（人工标注）；\",\"Conceptual Captions、Conceptual 12M 和 SBU Captions（网页收集）；\",\"补充实验中还使用了 LAION（Schuhmann et al., 2021）的大规模网页数据集（1.15 亿图像），每轮只使用 1/5 数据。\"]},\"177\":{\"h\":\"CapFilt 效果验证\",\"t\":[\"CapFilt 模块通过 Captioner 生成合成文本，再由 Filter 去除噪声文本，显著提升模型性能（表1）。三种设置下的对比表明：\",\"单独使用 Captioner 或 Filter 均有性能提升；\",\"两者联合使用时效果最佳，且具备数据量和模型规模的可扩展性；\",\"更强的视觉主干（如 ViT-L）可进一步增强性能。\",\"图4 展示了网页原始文本（）与合成文本（）的对比，绿色为 Filter 接收的文本，红色为剔除的文本，验证了 Captioner 提供新描述、Filter 移除无效数据的有效 性。\"]},\"178\":{\"h\":\"合成文本的多样性对性能的影响\",\"t\":[\"表2 比较了两种文本生成方式：\",\"Beam search：确定性搜索，噪声比例较低（19%）；\",\"Nucleus sampling：随机采样，噪声比例稍高（25%），但性能全面超越 Beam search。\",\"作者推测，nucleus sampling 生成的文本更具有 新颖性与多样性，提供更多额外信息；而 beam search 更倾向于生成数据集中常见的“安全”文本，难以提升模型泛化能力。\"]},\"179\":{\"h\":\"编码器-解码器参数共享与解耦\",\"t\":[\"表3 分析了不同的参数共享策略对模型性能的影响。结论如下：\",\"最佳方案是 仅在 SA 层不共享参数，其余部分共享；\",\"如果完全不共享参数，则模型体积大、性能次优；\",\"如果共享 SA 层，性能反而下降，因其在编码（双向注意力）与解码（因果注意力）间存在功能冲突。\",\"在 CapFilt 阶段，Captioner 与 Filter 分别进行微调。表4 显示，如果二者参数共享，则会因“确认偏差”导致性能下降 —— 生成的错误文本更难被 Filter 剔除（噪声比例仅 8%，远低于解耦时的 25%）。\",\"以下是论文 BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 中第 6章 Additional Ablation Study 和第 7章 Conclusion 部分的翻译：\"]},\"180\":{\"h\":\"Ablation Study\"},\"181\":{\"h\":\"CapFilt 的性能提升并非源于更长的训练时间\",\"t\":[\"由于经过 CapFilt 处理后的数据集包含比原始数据集更多的文本，因此在相同的 epoch 数下，训练时间会更长。为了验证 CapFilt 的有效性是否真正来自其机制本身，而非训练时间变长，我们将原始数据集中的网页文本复制，使得每个 epoch 的样本数量与 bootstrapped 数据集一致。\",\"如表 12 所示，仅通过扩充原始数据进行更长时间训练 并未带来性能提升，验证了 CapFilt 的真正价值。\"]},\"182\":{\"h\":\"应使用 Bootstrapped 数据集重新训练模型\",\"t\":[\"Bootstrapped 数据集应当用于重新训练一个新模型。我们对比了两种方式：\",\"在预训练模型基础上继续使用 CapFilt 数据训练；\",\"用 CapFilt 数据从头训练一个新模型。\",\"表 13 显示，继续训练的效果不如重新训练，这与知识蒸馏领域的常见做法一致：学生模型不能由教师模型直接初始化。这也间接说明 CapFilt 机制与重新初始化更契合。\"]},\"183\":{\"h\":\"Conclusion\",\"t\":[\"BLIP 是一个新的视觉-语言预训练框架，在众多下游任务中都实现了最先进（SOTA）性能，包括理解类任务和生成类任务。\",\"BLIP 使用多模态混合的编码器-解码器模型（Multimodal Mixture of Encoder-Decoder, MED），并通过对大规模嘈杂图文数据进行 bootstrapping 构建预训练语料：注入多样的合成描述，并剔除低质量描述。\",\"发布了 bootstrapped 数据集，以促进视觉-语言研究的发展。\",\"未来可进一步探索以下方向以提升 BLIP 表现：\",\"多轮数据集 Bootstrapping；\",\"为每张图像生成 多个合成描述，进一步扩大语料规模；\",\"使用多个不同的 Captioner 和 Filter 训练多个模型，再进行集成，增强 CapFilt 的效果。\"]},\"184\":{\"h\":\"Code Implementation\"},\"185\":{\"h\":\"CapFilt 模块实现\",\"t\":[\"BLIP 使用 CapFilt 对多个大规模噪声网页图文数据集（包括 CC12M、CC3M 和 SBU Captions）进行增强，首先通过 captioner 为图像生成合成文本，再通过 filter 过滤掉与图像不匹配的原始和合成文本，最终构建出高质量的自举数据集（bootstrapped dataset），用于预训练新模型。\",\"在 CapFilt 模块微调阶段，BLIP 则基于高质量人工标注的数据集如 COCO Captions、Visual Genome 和 Flickr30K 进行训练和评估。\",\"经过 CapFilt 处理后，输出的数据集是经过图文对齐质量优化的图文对集合，有效提升了下游任务中的表现。\",\"官方代码库并没有非常清晰指明CapFilt模块的实现代码位置，但是官方仓库的ISSUE给出了明确答复: https://github.com/salesforce/BLIP/issues/86?utm_source=chatgpt.com\"]},\"186\":{\"h\":\"Captioner 模块\"},\"187\":{\"h\":\"微调阶段\",\"t\":[\"Captioner 基于 Coco 数据集进行微调:\",\"# 训练函数：执行一个 epoch 的训练流程 def train(model, data_loader, optimizer, device): for i, (image, caption, _) in data_loader: loss = model(image, caption) # 前向传播，计算语言建模损失 optimizer.zero_grad() # 清除旧梯度 loss.backward() # 反向传播 optimizer.step() # 更新模型参数 # 主流程：加载数据、初始化模型和优化器、执行多轮训练 def main(args, config): #### Dataset #### # 加载 COCO Caption 数据集的训练/验证/测试划分 train_dataset, val_dataset, test_dataset = create_dataset('caption_coco', config) # 构造对应的数据加载器 train_loader, val_loader, test_loader = create_loader( [train_dataset, val_dataset, test_dataset], samplers, batch_size=[config['batch_size']]*3, num_workers=[4, 4, 4], is_trains=[True, False, False], collate_fns=[None, None, None] ) #### Model #### # 初始化 BLIP 解码器模型（用于图像字幕生成），加载预训练视觉编码器与文本解码器 model = blip_decoder( pretrained=config['pretrained'], image_size=config['image_size'], vit=config['vit'], vit_grad_ckpt=config['vit_grad_ckpt'], vit_ckpt_layer=config['vit_ckpt_layer'], prompt=config['prompt'] ) # 初始化优化器（AdamW） optimizer = torch.optim.AdamW( params=model.parameters(), lr=config['init_lr'], weight_decay=config['weight_decay'] ) #### Train #### for epoch in range(0, config['max_epoch']): # 每个 epoch 执行一次训练 train_stats = train(model, train_loader, optimizer, epoch, device)\",\"训练阶段唯一需要注意的一点就是数据集的构造过程中，会在Coco数据集每个样本原有Caption的基础上添加一个Prompt:\",\"class coco_karpathy_train(Dataset): def __getitem__(self, index): ann = self.annotation[index] image_path = os.path.join(self.image_root,ann['image']) image = Image.open(image_path).convert('RGB') image = self.transform(image) # 在caption前添加prompt , prompt 默认为 'a picture of ' caption = self.prompt+pre_caption(ann['caption'], self.max_words) # self.img_ids[ann['image_id']]: 取出图像 ID 对应的 索引编号 return image, caption, self.img_ids[ann['image_id']]\",\"下面将给出Captioner模块基于Coco数据集，采用 next token predict 方法进行训练的代码实现：\",\"class BLIP_Decoder(nn.Module): def __init__(self, med_config = 'configs/med_config.json', image_size = 384, vit = 'base', vit_grad_ckpt = False, vit_ckpt_layer = 0, prompt = 'a picture of ', ): \\\"\\\"\\\" BLIP Captioner模块初始化，实现论文中提出的图像-文本跨模态编码器-解码器架构 Args: med_config (str): 混合编码器-解码器模型配置文件路径，对应论文3.1节中提到的多模态融合模块配置 image_size (int): 输入图像尺寸，论文4.1节实验设置中使用384x384 vit (str): 视觉Transformer模型大小，论文中采用ViT-Base作为默认视觉编码器 vit_grad_ckpt (bool): 是否使用梯度检查点优化ViT显存占用，论文附录A中提到的训练优化策略 vit_ckpt_layer (int): ViT梯度检查点层数，用于平衡训练效率与显存使用 prompt (str): 图像描述生成的引导提示词，对应论文3.2节中使用的prompt engineering技术 \\\"\\\"\\\" super().__init__() self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer) # 初始化视觉编码器，对应论文图1中的视觉Transformer self.tokenizer = init_tokenizer() # 初始化文本分词器，采用BERT分词器实现论文中的文本预处理 med_config = BertConfig.from_json_file(med_config) med_config.encoder_width = vision_width self.text_decoder = BertLMHeadModel(config=med_config) # 初始化文本解码器，实现论文3.1节中的跨模态解码器 self.prompt = prompt # 存储图像描述引导提示词，用于论文3.3节中的条件生成任务 self.prompt_length = len(self.tokenizer(self.prompt).input_ids)-1 # 计算提示词token长度，用于后续解码时区分提示与生成文本 def forward(self, image, caption): # 提取图像特征表示 image_embeds = self.visual_encoder(image) image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device) # 编码文本输入，并替换开头 token 为 [BOS] text = self.tokenizer(caption, padding='longest', truncation=True, max_length=40, return_tensors=\\\"pt\\\").to(image.device) text.input_ids[:, 0] = self.tokenizer.bos_token_id # 构建语言建模标签：屏蔽掉 padding 和 prompt 部分 decoder_targets = text.input_ids.masked_fill(text.input_ids == self.tokenizer.pad_token_id, -100) decoder_targets[:, :self.prompt_length] = -100 # 调用跨模态解码器，执行语言建模训练 decoder_output = self.text_decoder( text.input_ids, attention_mask=text.attention_mask, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, labels=decoder_targets, return_dict=True ) loss_lm = decoder_output.loss # 提取语言建模损失 return loss_lm\",\"BertLMHeadModel自回归语言建模实现\"]},\"188\":{\"h\":\"生成阶段\",\"t\":[\"当 Captioner 模块在 Coco 数据集上进行训练后，即可用于生成图像描述。\",\"def generate(self, image, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0): # 通过视觉编码器提取图像的视觉特征（image embeddings） image_embeds = self.visual_encoder(image) if not sample: # 如果使用 beam search 生成，则需要将图像特征复制 num_beams 份 # 这是为了每个 beam 都能接收相同的图像信息 image_embeds = image_embeds.repeat_interleave(num_beams, dim=0) # 构造图像 attention mask，全为 1，表示图像特征没有被 mask image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device) # 构造 encoder-decoder 所需的关键词参数（即图像特征作为 cross-attention 的条件输入） model_kwargs = { \\\"encoder_hidden_states\\\": image_embeds, \\\"encoder_attention_mask\\\": image_atts } # 构造输入的 prompt，格式为 [\\\"a picture of \\\", \\\"a picture of \\\", ...] prompt = [self.prompt] * image.size(0) # 对 prompt 进行分词并转为 tensor，作为 decoder 的输入起点（input_ids） input_ids = self.tokenizer(prompt, return_tensors=\\\"pt\\\").input_ids.to(image.device) # 强制将每个样本开头的 token 设为 [BOS]（beginning-of-sentence） input_ids[:, 0] = self.tokenizer.bos_token_id # 移除最后一个 token（保持 prompt 是 decoder 的 prefix） input_ids = input_ids[:, :-1] if sample: # ---------- 使用 nucleus sampling（核采样）生成 ---------- outputs = self.text_decoder.generate( input_ids=input_ids, max_length=max_length, min_length=min_length, do_sample=True, # 启用采样 top_p=top_p, # nucleus 采样的阈值 num_return_sequences=1, # 每张图像生成一个序列 eos_token_id=self.tokenizer.sep_token_id, # 使用 [SEP] 作为结束标记 pad_token_id=self.tokenizer.pad_token_id, # 使用 [PAD] 作为 padding repetition_penalty=1.1, # 防止重复生成 **model_kwargs # 传入图像编码信息 ) else: # ---------- 使用 beam search（束搜索）生成 ---------- outputs = self.text_decoder.generate( input_ids=input_ids, max_length=max_length, min_length=min_length, num_beams=num_beams, # beam 数量 eos_token_id=self.tokenizer.sep_token_id, pad_token_id=self.tokenizer.pad_token_id, repetition_penalty=repetition_penalty, # 重复惩罚项 **model_kwargs ) # ---------- 解码生成的 token 序列为文本 ---------- captions = [] for output in outputs: caption = self.tokenizer.decode(output, skip_special_tokens=True) # 去掉 prompt 的前缀，只保留生成部分 captions.append(caption[len(self.prompt):]) return captions\"]},\"189\":{\"h\":\"Filter 模块\"},\"190\":{\"h\":\"微调阶段\",\"t\":[\"Filter 模块同样也是基于 Coco 数据集进行微调，但采用图文检索和图文匹配作为训练目标:\",\"def train(model, data_loader, optimizer, epoch, device, config): for i,(image, caption, idx) in data_loader: # 采用Moco的动量慢更新策略进行学习 if epoch>0: alpha = config['alpha'] else: alpha = config['alpha']*min(1,i/len(data_loader)) # idx 是每个图像对应的索引编号 loss_ita, loss_itm = model(image, caption, alpha=alpha, idx=idx) loss = loss_ita + loss_itm optimizer.zero_grad() loss.backward() optimizer.step() def main(args, config): #### Dataset #### # 数据集处理方面，同样会为Coco数据集中每个样本的Caption前添加固定长度的Prompt: 'a picture of' train_dataset, val_dataset, test_dataset = create_dataset('retrieval_%s'%config['dataset'], config) train_loader, val_loader, test_loader = create_loader([train_dataset, val_dataset, test_dataset],samplers, batch_size=[config['batch_size_train']]+[config['batch_size_test']]*2, num_workers=[4,4,4], is_trains=[True, False, False], collate_fns=[None,None,None]) #### Model #### model = blip_retrieval(pretrained=config['pretrained'], image_size=config['image_size'], vit=config['vit'], vit_grad_ckpt=config['vit_grad_ckpt'], vit_ckpt_layer=config['vit_ckpt_layer'], queue_size=config['queue_size'], negative_all_rank=config['negative_all_rank']) optimizer = torch.optim.AdamW(params=model.parameters(), lr=config['init_lr'], weight_decay=config['weight_decay']) for epoch in range(0, config['max_epoch']): train(model, train_loader, optimizer, epoch, device, config)\",\"训练过程代码实现基本遵循Moco论文中所提出的动量慢更新对比学习代码实现，下面先给出 BLIP_Retrieval 模型 init 方法实现:\",\"class BLIP_Retrieval(nn.Module): def __init__(self, med_config='configs/med_config.json', image_size=384, vit='base', vit_grad_ckpt=False, vit_ckpt_layer=0, embed_dim=256, queue_size=57600, momentum=0.995, negative_all_rank=False): # 初始化视觉编码器 self.visual_encoder, vision_width = create_vit(vit, image_size, vit_grad_ckpt, vit_ckpt_layer) # 初始化文本编码器和分词器 self.tokenizer = init_tokenizer() self.text_encoder = BertModel(config=med_config, add_pooling_layer=False) # 特征投影层（用于对比学习） self.vision_proj = nn.Linear(vision_width, embed_dim) self.text_proj = nn.Linear(text_width, embed_dim) # ITM分类头（判断图文是否匹配） self.itm_head = nn.Linear(text_width, 2) # 创建动量编码器（momentum encoder） self.visual_encoder_m, vision_width = create_vit(vit, image_size) self.vision_proj_m = nn.Linear(vision_width, embed_dim) self.text_encoder_m = BertModel(config=med_config, add_pooling_layer=False) self.text_proj_m = nn.Linear(text_width, embed_dim) # 配对编码器用于同步参数 self.model_pairs = [[self.visual_encoder, self.visual_encoder_m], [self.vision_proj, self.vision_proj_m], [self.text_encoder, self.text_encoder_m], [self.text_proj, self.text_proj_m]] self.copy_params() # 初始化对比学习的负样本队列 self.register_buffer(\\\"image_queue\\\", torch.randn(embed_dim, queue_size)) self.register_buffer(\\\"text_queue\\\", torch.randn(embed_dim, queue_size)) self.register_buffer(\\\"idx_queue\\\", torch.full((1, queue_size), -100)) self.register_buffer(\\\"ptr_queue\\\", torch.zeros(1, dtype=torch.long)) self.image_queue = nn.functional.normalize(self.image_queue, dim=0) self.text_queue = nn.functional.normalize(self.text_queue, dim=0) self.queue_size = queue_size self.momentum = momentum self.temp = nn.Parameter(0.07 * torch.ones([])) # 对比学习温度参数 self.negative_all_rank = negative_all_rank\",\"前向传播过程主要是为了计算两个训练目标的损失:\",\"图文对比目标（ITC）\",\"图文匹配目标（ITM）\",\"代码整体流程比较长，我们切分为多个步骤进行解析:\",\"提取图像和文本特征\",\" def forward(self, image, caption, alpha, idx): # 图像特征提取和投影 # image: (B, 3, H, W) -> image_embeds: (B, N, D) image_embeds = self.visual_encoder(image) image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device) # (B, N) image_feat = F.normalize(self.vision_proj(image_embeds[:, 0, :]), dim=-1) # 只取CLS Token做投影: (B, D_proj) # 文本特征提取和投影 text = self.tokenizer(caption, padding='max_length', truncation=True, max_length=35, return_tensors=\\\"pt\\\").to(image.device) # text.input_ids: (B, L) text_output = self.text_encoder(text.input_ids, attention_mask=text.attention_mask, return_dict=True, mode='text') text_feat = F.normalize(self.text_proj(text_output.last_hidden_state[:, 0, :]), dim=-1) # 只取CLS Token做投影: (B, D_proj)\",\"构造图文匹配矩阵 (Target)\",\" # 构造图文匹配矩阵 idx = idx.view(-1, 1) # (B, 1) idx_all = torch.cat([idx.t(), self.idx_queue.clone().detach()], dim=1) # (1, B + Q) pos_idx = torch.eq(idx, idx_all).float() # (B, B + Q) sim_targets = pos_idx / pos_idx.sum(1, keepdim=True) # (B, B + Q)\",\"这里需要和 MoCo 论文实现进行区分，MoCo 中采用的是每个 query（图像）只对应一个 positive（key），因此正负样本是 one-hot 编码，contrastive loss 是严格的一对一；而在 BLIP 中，由于图文对来自自然语言描述，可能存在多个正样本（即同一个图像可以有多个 caption），而且动量队列可能重复包含同一个样本（multi-hot），因此这里构造 sim_targets 时不是用 one-hot，而是通过 pos_idx 判断当前图文对与队列中哪些样本是正对（idx 相等），然后用行归一化将多个正样本平均分配权重，形成 soft target 分布，从而使对比学习更加稳健。\",\"举例:\",\"# 1. 环境 idx = [[7], [13], [20]] # B=3 idx_queue = [1, 7, 5, 13, 9, 30] # Q=6 idx_all = [7, 13, 20, 1, 7, 5, 13, 9, 30] # shape: (1, 9) # 2. 构造图文匹配矩阵 pos_idx = torch.eq(idx, idx_all) # 第1行: [1, 0, 0, 0, 1, 0, 0, 0, 0] # 第2行: [0, 1, 0, 0, 0, 0, 1, 0, 0] # 第3行: [0, 0, 1, 0, 0, 0, 0, 0, 0] # 3. 进行归一化 # 第1行: [0.5, 0, 0, 0, 0.5, 0, 0, 0, 0] # 第2行: [0, 0.5, 0, 0, 0, 0, 0.5, 0, 0] # 第3行: [0, 0, 1.0, 0, 0, 0, 0, 0, 0]\",\"动量慢更新 + 软标签计算\",\" # 使用动量编码器获取特征 with torch.no_grad(): self._momentum_update() image_embeds_m = self.visual_encoder_m(image) # (B, N, D) image_feat_m = F.normalize(self.vision_proj_m(image_embeds_m[:, 0, :]), dim=-1) # (B, D_proj) image_feat_m_all = torch.cat([image_feat_m.t(), self.image_queue.clone().detach()], dim=1) # (D_proj, B + Q) text_output_m = self.text_encoder_m(text.input_ids, attention_mask=text.attention_mask,return_dict=True, mode='text') text_feat_m = F.normalize(self.text_proj_m(text_output_m.last_hidden_state[:, 0, :]), dim=-1) # (B, D_proj) text_feat_m_all = torch.cat([text_feat_m.t(), self.text_queue.clone().detach()], dim=1) # (D_proj, B + Q) sim_i2t_m = image_feat_m @ text_feat_m_all / self.temp # (B, B + Q) sim_t2i_m = text_feat_m @ image_feat_m_all / self.temp # (B, B + Q) sim_i2t_targets = alpha * F.softmax(sim_i2t_m, dim=1) + (1 - alpha) * sim_targets # (B, B + Q) sim_t2i_targets = alpha * F.softmax(sim_t2i_m, dim=1) + (1 - alpha) * sim_targets # (B, B + Q)\",\"计算ITC损失 + 更新动量队列\",\" # 当前 batch 与动量队列的相似度 sim_i2t = image_feat @ text_feat_m_all / self.temp # (B, B + Q) sim_t2i = text_feat @ image_feat_m_all / self.temp # (B, B + Q) # 对比损失（InfoNCE） loss_i2t = -torch.sum(F.log_softmax(sim_i2t, dim=1) * sim_i2t_targets, dim=1).mean() loss_t2i = -torch.sum(F.log_softmax(sim_t2i, dim=1) * sim_t2i_targets, dim=1).mean() loss_ita = (loss_i2t + loss_t2i) / 2 # 更新队列 idxs = concat_all_gather(idx) # (B*, 1) self._dequeue_and_enqueue(image_feat_m, text_feat_m, idxs)\",\"正样本编码 + 难负样本采样\",\" ### 图文匹配任务 (ITM) ### encoder_input_ids = text.input_ids.clone() # (B, L) encoder_input_ids[:, 0] = self.tokenizer.enc_token_id bs = image.size(0) # 正样本编码 output_pos = self.text_encoder(encoder_input_ids, attention_mask=text.attention_mask, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, return_dict=True) # last_hidden_state: (B, L, D) # 采样难负样本（是否跨 GPU） if self.negative_all_rank: # 跨GPU部分实现，自信看源码进行学习 else: with torch.no_grad(): mask = torch.eq(idx, idx.t()) sim_i2t = image_feat @ text_feat.t() / self.temp sim_t2i = text_feat @ image_feat.t() / self.temp weights_i2t = F.softmax(sim_i2t,dim=1) weights_i2t.masked_fill_(mask, 0) weights_t2i = F.softmax(sim_t2i,dim=1) weights_t2i.masked_fill_(mask, 0) # select a negative image (from same rank) for each text image_embeds_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_t2i[b], 1).item() image_embeds_neg.append(image_embeds[neg_idx]) image_embeds_neg = torch.stack(image_embeds_neg,dim=0) # select a negative text (from same rank) for each image text_ids_neg = [] text_atts_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_i2t[b], 1).item() text_ids_neg.append(encoder_input_ids[neg_idx]) text_atts_neg.append(text.attention_mask[neg_idx]) text_ids_neg = torch.stack(text_ids_neg, dim=0) # (B, L) text_atts_neg = torch.stack(text_atts_neg, dim=0) # (B, L)\",\"idx 是当前 GPU 上 本地批次（batch）的样本索引（shape: (B, 1)）。\",\"idxs 是通过 concat_all_gather(idx) 得到的 所有 GPU 上所有样本索引的集合（shape: (total_batch_size, 1)）。\",\"构造两组负样本\",\" # [正样本，负样本] text_ids_all = torch.cat([encoder_input_ids, text_ids_neg], dim=0) # (2B, L) text_atts_all = torch.cat([text.attention_mask, text_atts_neg], dim=0) # (2B, L) # [负样本，正样本] image_embeds_all = torch.cat([image_embeds_neg, image_embeds], dim=0) # (2B, N, D) image_atts_all = torch.cat([image_atts, image_atts], dim=0) # (2B, N) # 两组负样本编码 output_neg = self.text_encoder(text_ids_all, attention_mask=text_atts_all, encoder_hidden_states=image_embeds_all, encoder_attention_mask=image_atts_all, return_dict=True)\",\"计算ITM损失 + 返回ITC损失和ITM损失\",\" # ITM 分类损失 vl_embeddings = torch.cat([output_pos.last_hidden_state[:, 0, :], output_neg.last_hidden_state[:, 0, :]], dim=0) # (3B, D) vl_output = self.itm_head(vl_embeddings) # (3B, 2) itm_labels = torch.cat([torch.ones(bs, dtype=torch.long), torch.zeros(2 * bs, dtype=torch.long)], dim=0).to(image.device) # (3B,) loss_itm = F.cross_entropy(vl_output, itm_labels) return loss_ita, loss_itm\"]},\"191\":{\"h\":\"过滤阶段\",\"t\":[\"当Filter模块在Coco数据集上，采用ITC和ITM目标执行完微调后，便得到了模态对齐好的图像编码器Vit 和 文本编码器Bert ， 然后我们便可以直接用训练好的Vit和Bert来做图文匹配和图文相似度计算了。\",\"class BLIP_ITM(nn.Module): def __init__(self, med_config = 'configs/med_config.json', image_size = 384, vit = 'base', vit_grad_ckpt = False, vit_ckpt_layer = 0, embed_dim = 256, ): self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer) self.tokenizer = init_tokenizer() self.text_encoder = BertModel(config=med_config, add_pooling_layer=False) self.vision_proj = nn.Linear(vision_width, embed_dim) self.text_proj = nn.Linear(text_width, embed_dim) self.itm_head = nn.Linear(text_width, 2) def forward(self, image, caption, match_head='itm'): image_embeds = self.visual_encoder(image) image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device) text = self.tokenizer(caption, padding='max_length', truncation=True, max_length=35, return_tensors=\\\"pt\\\").to(image.device) if match_head=='itm': output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask, encoder_hidden_states = image_embeds, encoder_attention_mask = image_atts, return_dict = True, ) itm_output = self.itm_head(output.last_hidden_state[:,0,:]) return itm_output elif match_head=='itc': text_output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask, return_dict = True, mode = 'text') image_feat = F.normalize(self.vision_proj(image_embeds[:,0,:]),dim=-1) text_feat = F.normalize(self.text_proj(text_output.last_hidden_state[:,0,:]),dim=-1) sim = image_feat @ text_feat.t() return sim\"]},\"192\":{\"h\":\"BLIP 预训练\",\"t\":[\"BLIP 模型基于 CapFilt 模块增强后的数据集上，采用ITC，ITM，LM三个目标进行训练，以下首先给出的是 BLIP 模型的训练代码:\",\"def train(model, data_loader, optimizer, epoch, device, config): for i, (image, caption) in data_loader: optimizer.zero_grad() # ramp up alpha in the first 2 epochs alpha = config['alpha']*min(1,(epoch*len(data_loader)+i)/(2*len(data_loader))) loss_ita, loss_itm, loss_lm = model(image, caption, alpha = alpha) loss = loss_ita + loss_itm + loss_lm loss.backward() optimizer.step() def main(args, config): #### Dataset #### datasets = [create_dataset('pretrain', config, min_scale=0.2)] # 返回的caption前不添加prompt data_loader = create_loader(datasets,samplers,batch_size=[config['batch_size']], num_workers=[4], is_trains=[True], collate_fns=[None])[0] #### Model #### model = blip_pretrain(image_size=config['image_size'], vit=config['vit'], vit_grad_ckpt=config['vit_grad_ckpt'], vit_ckpt_layer=config['vit_ckpt_layer'], queue_size=config['queue_size']) optimizer = torch.optim.AdamW(params=model.parameters(), lr=config['init_lr'], weight_decay=config['weight_decay']) for epoch in range(start_epoch, config['max_epoch']): train(model, data_loader, optimizer, epoch, device, config)\",\"BLIP 预训练代码实现部分参考Moco论文实现，采用动量慢更新策略，整体流程和ALBEF模型实现一致，下面首先给出的是 BLIP 模型的 init 初始化方法:\",\"class BLIP_Pretrain(nn.Module): def __init__(self, med_config='configs/bert_config.json', # 文本编码器配置 image_size=224, # 输入图像大小 vit='base', # 使用的 ViT 模型类型（如 base、large） vit_grad_ckpt=False, # 是否使用梯度检查点（节省显存） vit_ckpt_layer=0, # 从第几层开始启用 checkpoint embed_dim=256, # 图文共享表示的嵌入维度 queue_size=57600, # 对比学习中图文特征队列长度 momentum=0.995, # 动量编码器的更新参数 ): super().__init__() # 1. 创建主视觉编码器（ViT） self.visual_encoder, vision_width = create_vit( vit, image_size, vit_grad_ckpt, vit_ckpt_layer, 0 ) # 2. 创建文本编码器（BERT） self.tokenizer = init_tokenizer() # 加载 tokenizer（默认 BERT） self.text_encoder = BertModel.from_pretrained( 'bert-base-uncased', config=encoder_config, add_pooling_layer=False ) # 3. 视觉 / 文本 特征映射到共享空间 self.vision_proj = nn.Linear(vision_width, embed_dim) # (D_v → D_e) self.text_proj = nn.Linear(text_width, embed_dim) # (D_t → D_e) # 4. 图文匹配（ITM）任务的二分类头 self.itm_head = nn.Linear(text_width, 2) # ======================= 动量编码器（Momentum Encoder） ======================= # # 用于构造 InfoNCE 的 soft target，与主模型参数不同步，而是 EMA 滑动平均更新 self.visual_encoder_m, _ = create_vit(vit, image_size) # 动量视觉编码器 self.vision_proj_m = nn.Linear(vision_width, embed_dim) self.text_encoder_m = BertModel( config=encoder_config, add_pooling_layer=False ) # 动量文本编码器 self.text_proj_m = nn.Linear(text_width, embed_dim) # 将主模型和动量模型参数组织成配对，用于拷贝和更新 self.model_pairs = [ [self.visual_encoder, self.visual_encoder_m], [self.vision_proj, self.vision_proj_m], [self.text_encoder, self.text_encoder_m], [self.text_proj, self.text_proj_m], ] self.copy_params() # 初始化时直接复制参数（后续 EMA 更新） # ======================= 特征队列初始化 ======================= # # 队列用于 InfoNCE 对比学习中的负样本缓存（增强样本多样性） self.register_buffer(\\\"image_queue\\\", torch.randn(embed_dim, queue_size)) # 图像队列：(D_e, Q) self.register_buffer(\\\"text_queue\\\", torch.randn(embed_dim, queue_size)) # 文本队列：(D_e, Q) self.register_buffer(\\\"queue_ptr\\\", torch.zeros(1, dtype=torch.long)) # 队列指针（循环更新） # 初始化队列为单位向量（便于计算归一化相似度） self.image_queue = nn.functional.normalize(self.image_queue, dim=0) self.text_queue = nn.functional.normalize(self.text_queue, dim=0) self.queue_size = queue_size self.momentum = momentum # InfoNCE 温度参数（可学习） self.temp = nn.Parameter(0.07 * torch.ones([])) # ======================= 文本解码器（用于 LM 任务） ======================= # self.text_decoder = BertLMHeadModel.from_pretrained( 'bert-base-uncased', config=decoder_config )\",\"BLIP 模型的前向传播流程和ALBEF实现基本一致，这里不过多进行展开:\",\"def forward(self, image, caption, alpha): # ===================== 1. 图像与文本特征提取 ===================== # # 图像编码：提取视觉特征 image_embeds = self.visual_encoder(image) # (B, N, D_v) image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device) # (B, N) image_feat = F.normalize(self.vision_proj(image_embeds[:, 0, :]), dim=-1) # (B, D_e)，CLS特征 → 投影 → 归一化 # 文本编码：tokenize 文本 text = self.tokenizer(caption, padding='max_length', truncation=True, max_length=30, return_tensors=\\\"pt\\\").to(image.device) text_output = self.text_encoder(text.input_ids, attention_mask=text.attention_mask, return_dict=True, mode='text') text_feat = F.normalize(self.text_proj(text_output.last_hidden_state[:, 0, :]), dim=-1) # (B, D_e)，CLS特征 → 投影 → 归一化 # ===================== 2. 计算动量编码器输出，用于生成 soft target ===================== # with torch.no_grad(): self._momentum_update() # 图像动量编码器 image_embeds_m = self.visual_encoder_m(image) # (B, N, D_v) image_feat_m = F.normalize(self.vision_proj_m(image_embeds_m[:, 0, :]), dim=-1) # (B, D_e) # 构建图像所有对比特征 = 当前batch + 队列 image_feat_all = torch.cat([image_feat_m.T, self.image_queue.clone().detach()], dim=1) # (D_e, B+Q) # 文本动量编码器 text_output_m = self.text_encoder_m(text.input_ids, attention_mask=text.attention_mask, return_dict=True, mode='text') text_feat_m = F.normalize(self.text_proj_m(text_output_m.last_hidden_state[:, 0, :]), dim=-1) # (B, D_e) # 构建文本所有对比特征 = 当前batch + 队列 text_feat_all = torch.cat([text_feat_m.T, self.text_queue.clone().detach()], dim=1) # (D_e, B+Q) # 计算图 → 文本 和 文本 → 图 相似度（soft target） sim_i2t_m = image_feat_m @ text_feat_all / self.temp # (B, B+Q) sim_t2i_m = text_feat_m @ image_feat_all / self.temp # (B, B+Q) sim_targets = torch.zeros(sim_i2t_m.size()).to(image.device) sim_targets.fill_diagonal_(1) # 构造 hard target (对角线为正例) sim_i2t_targets = alpha * F.softmax(sim_i2t_m, dim=1) + (1 - alpha) * sim_targets # 软标签 + 硬标签混合 sim_t2i_targets = alpha * F.softmax(sim_t2i_m, dim=1) + (1 - alpha) * sim_targets # ===================== 3. 计算 InfoNCE 对比学习损失 (ITC) ===================== # sim_i2t = image_feat @ text_feat_all / self.temp # (B, B+Q) sim_t2i = text_feat @ image_feat_all / self.temp # (B, B+Q) loss_i2t = -torch.sum(F.log_softmax(sim_i2t, dim=1) * sim_i2t_targets, dim=1).mean() loss_t2i = -torch.sum(F.log_softmax(sim_t2i, dim=1) * sim_t2i_targets, dim=1).mean() loss_ita = (loss_i2t + loss_t2i) / 2 # 更新负样本队列 self._dequeue_and_enqueue(image_feat_m, text_feat_m) # ===================== 4. 图文匹配 (ITM) ===================== # # 用于多模态 cross-attention 编码的输入文本（替换 CLS） encoder_input_ids = text.input_ids.clone() encoder_input_ids[:, 0] = self.tokenizer.enc_token_id bs = image.size(0) # 正样本对 output_pos = self.text_encoder(encoder_input_ids, attention_mask=text.attention_mask, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, return_dict=True) with torch.no_grad(): # 为 ITM 任务采样负样本索引（从 sim 分布中采样，避免选到自己） weights_t2i = F.softmax(sim_t2i[:, :bs], dim=1) + 1e-4 weights_t2i.fill_diagonal_(0) weights_i2t = F.softmax(sim_i2t[:, :bs], dim=1) + 1e-4 weights_i2t.fill_diagonal_(0) # select a negative image for each text image_embeds_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_t2i[b], 1).item() image_embeds_neg.append(image_embeds[neg_idx]) image_embeds_neg = torch.stack(image_embeds_neg,dim=0) # select a negative text for each image text_ids_neg = [] text_atts_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_i2t[b], 1).item() text_ids_neg.append(encoder_input_ids[neg_idx]) text_atts_neg.append(text.attention_mask[neg_idx]) text_ids_neg = torch.stack(text_ids_neg,dim=0) text_atts_neg = torch.stack(text_atts_neg,dim=0) # 合并正负样本对 text_ids_all = torch.cat([encoder_input_ids, text_ids_neg], dim=0) # (2B, L) text_atts_all = torch.cat([text.attention_mask, text_atts_neg], dim=0) # (2B, L) image_embeds_all = torch.cat([image_embeds_neg, image_embeds], dim=0) # (2B, N, D_v) image_atts_all = torch.cat([image_atts, image_atts], dim=0) # (2B, N) output_neg = self.text_encoder(text_ids_all, attention_mask=text_atts_all, encoder_hidden_states=image_embeds_all, encoder_attention_mask=image_atts_all, return_dict=True) # 提取 [CLS] 融合特征，用于二分类匹配 vl_embeddings = torch.cat([output_pos.last_hidden_state[:, 0, :], output_neg.last_hidden_state[:, 0, :]], dim=0) # (3B, D_t) vl_output = self.itm_head(vl_embeddings) # (3B, 2)，匹配or不匹配 itm_labels = torch.cat([ torch.ones(bs, dtype=torch.long), torch.zeros(2 * bs, dtype=torch.long) ], dim=0).to(image.device) loss_itm = F.cross_entropy(vl_output, itm_labels) # ===================== 5. 文本生成任务（LM） ===================== # decoder_input_ids = text.input_ids.clone() decoder_input_ids[:, 0] = self.tokenizer.bos_token_id # 用 [BOS] 替换 [CLS] decoder_targets = decoder_input_ids.masked_fill(decoder_input_ids == self.tokenizer.pad_token_id, -100) # 忽略pad位loss decoder_output = self.text_decoder(decoder_input_ids, attention_mask=text.attention_mask, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, labels=decoder_targets, return_dict=True) loss_lm = decoder_output.loss # ===================== 6. 返回三个 loss ===================== # return loss_ita, loss_itm, loss_lm\"]},\"193\":{\"h\":\"ALBEF 论文\",\"t\":[\"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation 论文简析\",\"论文链接: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation 代码链接: https://github.com/salesforce/ALBEF\"]},\"194\":{\"h\":\"Introduction\",\"t\":[\"视觉-语言大规模预训练能提升多种视觉语言任务，但视觉和文本的token未对齐，导致多模态编码器难以有效建模图文交互。当前论文提出了一个基于对比损失的“先对齐后融合”（ALBEF）策略，通过对比学习先对齐图像和文本的表示，再用跨模态注意力融合，提升表示的准确性。并且ALBEF 不依赖目标检测器，也不需要高分辨率图像；同时通过引入动量蒸馏(momentum distillation)自训练方法，能更有效应对含噪声的网络数据，提高泛化能力。\",\"传统VLP方法依赖目标检测器提取区域特征，结合文本通过多模态编码器处理，面临：\",\"图像和文本特征空间不一致，交互难度大。\",\"目标检测器开销高，注释和计算开销大。\",\"网络图文数据噪声多，容易导致预训练过拟合，泛化差。\",\"ALBEF 采用无检测器的图像编码器和文本编码器独立编码，再用多模态编码器融合。设计图文对比（ITC）损失：\",\"对齐图文特征空间，简化跨模态融合。\",\"改善单模态编码语义理解。\",\"学习共同低维空间，促进硬负样本挖掘。\",\"提出动量蒸馏（MoD），用动量模型生成伪标签，缓解噪声影响，提升预训练及下游表现。\"]},\"195\":{\"h\":\"Related Work\",\"t\":[\"Vision-Language Representation Learning:\",\"当前视觉-语言表示学习主要分为两类：一类使用多模态 Transformer 编码器建模图文交互，适用于复杂推理任务但依赖目标检测器和高分辨率图像，计算开销大；另一类则采用独立的图像与文本编码器，通过对比学习在大规模图文对中对齐表示，虽在图文检索中效果出色，但难以处理复杂语义交互。ALBEF 融合两者优势，先用对比学习对齐图文表示，再通过跨模态注意力实现深度融合，同时摒弃目标检测器，在保证高性能的同时显著降低了计算成本，兼顾了效率与泛化能力。\",\"Knowledge Distillation:\",\"传统知识蒸馏通过教师模型指导学生模型提升性能，通常依赖预训练教师。近年来的在线蒸馏则使用多个同时训练的模型进行知识迁移。ALBEF 提出的 Momentum Distillation 属于自蒸馏的一种形式，通过使用自身参数的滑动平均作为教师，生成伪标签辅助训练。该方法无需额外模型，能缓解弱标注图文数据中的噪声问题，提升表示稳定性和泛化能力。\"]},\"196\":{\"h\":\"ALBEF\"},\"197\":{\"h\":\"Model Structure\",\"t\":[\"ALBEF 模型由三个主要部分组成：图像编码器、文本编码器和多模态编码器。图像编码器采用了预训练的 ViT-B/16（12 层视觉 Transformer），将输入图像编码为一系列嵌入向量。文本编码器和多模态编码器均为 6 层 Transformer，分别初始化自 BERTbase 的前 6 层和后 6 层。文本经过编码后生成的嵌入序列，会与图像嵌入一起送入多模态编码器进行融合。融合过程在多模态编码器的每一层中通过跨模态注意力（Cross Attention）实现，实现图文信息的深层交互。\"]},\"198\":{\"h\":\"Pre-training Objectives\",\"t\":[\"我们对 ALBEF 进行预训练，包含三个目标：在单模态编码器上进行图文对比学习（ITC），以及在多模态编码器上进行掩码语言模型（MLM）和图文匹配（ITM）。我们通过在线对比难样本挖掘来改进图文匹配（ITM）。\"]},\"199\":{\"h\":\"Image-Text Contrastive Learning\",\"t\":[\"图文对比学习旨在融合之前学习更好的单模态表示。它通过学习一个相似度函数\",\"使得配对的图文具有更高的相似度得分。这里 和 是线性变换，用于将 [CLS] 表征映射到归一化的低维（256维）向量。受 MoCo 启发，我们维护两个队列来存储动量单模态编码器最近的 个图文表示。动量编码器生成的归一化特征分别记为\",\"定义相似度函数为：\",\"对于每个图像和文本，我们计算归一化的图像到文本和文本到图像的 softmax 相似度：\",\"其中 是一个可学习的温度参数。令 和 分别表示真实的 one-hot 标签，负样本概率为 0，正样本概率为 1。图文对比损失定义为交叉熵 ：\"]},\"200\":{\"h\":\"Masked Language Modeling（MLM）\",\"t\":[\"Masked Language Modeling 利用图像和上下文文本共同预测被 mask 掉的单词。我们以 15% 的概率随机将输入文本中的 token 替换为特殊标记 [MASK]。设 表示被 mask 的文本， 表示模型预测的被 mask token 的概率分布。MLM 任务的目标是最小化交叉熵损失：\",\"其中 是 one-hot 词表分布，真实标签对应的概率为 1。\"]},\"201\":{\"h\":\"Image-Text Matching（ITM）\",\"t\":[\"Image-Text Matching 用于判断图文对是否匹配。我们使用多模态编码器输出的 [CLS] token 表征作为图文对的联合表示，接一个全连接层（FC），再通过 softmax 得到预测概率 ，最终计算 ITM 的交叉熵损失：\",\"其中 是二分类 one-hot 向量，表示图文对的真实匹配状态。\",\"我们提出了一种 零计算开销的 ITM 硬负样本采样策略：\",\"若图文语义相近但细节不同，则视为 hard negative；\",\"利用图文对比损失中的相似度作为度量，在 mini-batch 中为每张图像选择一个最相似的非匹配文本作为负样本；\",\"同样地，为每个文本选择一个最相似的非匹配图像。\",\"ALBEF 预训练总目标函数：\"]},\"202\":{\"h\":\"Momentum Distillation\",\"t\":[\"视觉-语言预训练所使用的图文对大多来自网页，因此存在较大的噪声。例如：正样本图文对往往关联性较弱，文本中可能包含与图像无关的信息，图像中也可能存在未被文本描述的实体。在对比学习（ITC）中，有些“负样本”文本可能实际上与图像语义一致；而在掩码语言建模（MLM）中，也可能存在多个与被 mask 单词同样合理甚至更好的替代词。但标准 one-hot 标签的监督会一律惩罚这些“非标答案”。\",\"为解决这一问题，ALBEF 提出使用 动量模型（Momentum Model）生成伪标签（pseudo-targets）进行蒸馏监督。动量模型是对主模型参数的滑动平均版本（exponential moving average），起到“老师模型”的作用。在训练中，主模型被训练去匹配动量模型的预测，从而提升鲁棒性和泛化能力。\",\"对比学习中的动量蒸馏:\",\"设动量编码器生成的相似度为：\",\"将其代入标准的对比学习 softmax 公式中，构造软标签（soft pseudo-target）。然后定义 ITC 的动量蒸馏损失为：\",\"这里的 KL 表示 Kullback-Leibler 散度，衡量模型预测分布与动量模型生成的软标签之间的差异。\",\"掩码语言建模中的动量蒸馏:\",\"设动量模型在图像 和被 mask 的文本 上预测得到的概率分布为 ，主模型的预测为 。对应的蒸馏损失定义为：\",\"这样的设计使得 MLM 模型不再被 one-hot 标签约束，可以学习更丰富的词汇表达，捕捉与图像内容相关的多种可能性。\",\"如论文中的图 2 所示，动量模型生成的伪标签往往比真实标签更具多样性和语义丰富性。例如：\",\"原始文本：\\\"polar bear in the [MASK]\\\" 真实标签：wild 伪标签前五名：zoo, pool, water, pond, wild\",\"这种伪标签不仅能补充视觉信息中的遗漏，还能提供更灵活的语义参考。\",\"通过引入动量蒸馏，ALBEF 能够：\",\"在噪声标签数据上提高学习效果；\",\"避免因 one-hot 标签过度惩罚合理预测；\",\"在多任务（如 ITC 和 MLM）中更稳定地训练；\",\"提高预训练模型在下游任务中的表现。\",\"动量蒸馏的总体损失是对原始监督信号与伪监督信号的加权组合，平衡其指导作用：\",\"其中 控制动量蒸馏信号的强度，实验中统一设为 0.4。\"]},\"203\":{\"h\":\"Code Implementation\"},\"204\":{\"h\":\"Train\",\"t\":[\"训练代码:\",\"def train(model, data_loader, optimizer, tokenizer, epoch, warmup_steps, device, scheduler, config): for i, (image, text) in enumerate(metric_logger.log_every(data_loader, print_freq, header)): optimizer.zero_grad() image = image.to(device,non_blocking=True) text_input = tokenizer(text, padding='longest', truncation=True, max_length=25, return_tensors=\\\"pt\\\").to(device) if epoch>0: alpha = config['alpha'] else: alpha = config['alpha']*min(1,i/len(data_loader)) loss_mlm, loss_ita, loss_itm = model(image, text_input, alpha = alpha) loss = loss_mlm + loss_ita + loss_itm loss.backward() optimizer.step()\",\"为何前期要让 α 慢慢增加？\",\"训练初期模型尚不稳定，动量分支的 soft label 不可靠。直接用 soft label 可能误导主模型。因此，先以 hard label 为主，逐渐引入 soft label 的指导。\",\"前期：alpha ≈ 0 → 以 one-hot 监督为主，训练稳定\",\"中期：alpha 上升 → soft label 引入更丰富的监督\",\"后期：alpha ≈ config['alpha'] → 强化多义性和类间相似度学习，提高泛化\",\"图中展示的是 ALBEF 模型训练过程中 alpha 参数的变化趋势：\",\"第一个 epoch（前 100 步）：alpha 线性从 0 增加到设定的最大值（如 0.5）。这种方式在训练初期让模型更多依赖于 one-hot 形式的监督信号，降低动量负样本带来的扰动。\",\"第二个 epoch 及之后：alpha 恒定为最大值（例如 0.5），意味着动量分布和 one-hot label 的加权比固定，开始充分利用动量编码器提供的软标签来训练。\"]},\"205\":{\"h\":\"Model Init\",\"t\":[\"ALBEF 模型初始化:\",\"def __init__(self, text_encoder = None, tokenizer = None, config = None, temp = 0.07, init_deit = True ): super().__init__() # 初始化 tokenizer（用于文本编码） self.tokenizer = tokenizer # MLM 任务中 mask 掉的 token 比例 self.mlm_probability = config['mlm_probability'] # 图文对比学习后的共同嵌入维度 embed_dim = config['embed_dim'] # 初始化视觉编码器（ViT backbone），输出: (B, N+1, 768) self.visual_encoder = VisionTransformer( img_size=config['image_res'], patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6)) vision_width = config['vision_width'] # ViT 输出维度，通常为 768 # 加载文本编码器 BertConfig 配置 bert_config = BertConfig.from_json_file(config['bert_config']) # 加载文本编码器，输出: (B, L, hidden_size)，默认 hidden_size = 768 self.text_encoder = BertForMaskedLM.from_pretrained(text_encoder, config=bert_config) text_width = self.text_encoder.config.hidden_size # Bert 输出维度（默认 768） # 图像特征 → 共享嵌入空间（Linear projection）：(B, vision_width) → (B, embed_dim) self.vision_proj = nn.Linear(vision_width, embed_dim) # 文本特征 → 共享嵌入空间（Linear projection）：(B, text_width) → (B, embed_dim) self.text_proj = nn.Linear(text_width, embed_dim) # 学习温度系数 temp ∈ [0.001, 0.5]，用于对比学习中的 softmax 除法 self.temp = nn.Parameter(torch.ones([]) * config['temp']) # 对比学习中的队列长度（如 65536） self.queue_size = config['queue_size'] # 动量更新系数（如 0.995） self.momentum = config['momentum'] # ITM 分类头：输入为 text_encoder 最后一层的 CLS 特征 → 输出为二分类 (B, 2) self.itm_head = nn.Linear(text_width, 2) # =============== 构建动量编码器（结构与主模型相同，仅参数使用 EMA 更新）=============== self.visual_encoder_m = VisionTransformer( img_size=config['image_res'], patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6)) self.vision_proj_m = nn.Linear(vision_width, embed_dim) self.text_encoder_m = BertForMaskedLM.from_pretrained(text_encoder, config=bert_config) self.text_proj_m = nn.Linear(text_width, embed_dim) # 将主编码器和动量编码器配对，用于 EMA 参数更新 self.model_pairs = [[self.visual_encoder,self.visual_encoder_m], [self.vision_proj,self.vision_proj_m], [self.text_encoder,self.text_encoder_m], [self.text_proj,self.text_proj_m], ] # 初始化动量编码器参数 = 主模型参数 self.copy_params() # =============== 初始化负样本队列 =============== # 图像特征队列：(embed_dim, queue_size) self.register_buffer(\\\"image_queue\\\", torch.randn(embed_dim, self.queue_size)) # 文本特征队列：(embed_dim, queue_size) self.register_buffer(\\\"text_queue\\\", torch.randn(embed_dim, self.queue_size)) # 当前入队位置指针，形状: (1,) self.register_buffer(\\\"queue_ptr\\\", torch.zeros(1, dtype=torch.long)) # 初始化队列特征为单位向量，便于之后相似度计算 self.image_queue = nn.functional.normalize(self.image_queue, dim=0) self.text_queue = nn.functional.normalize(self.text_queue, dim=0)\"]},\"206\":{\"h\":\"ITC\",\"t\":[\"ALBEF 模型前向传播中的 ITC 学习目标实现过程:\",\"def forward(self, image, text, alpha=0): # 1. 使用 ViT 对图像进行编码，输出图像特征 # image_embeds: (B, N+1, embed_dim)，N 个 patch + 1 个 CLS token image_embeds = self.visual_encoder(image) # 2. 构造图像的 attention mask，全部为1，表示无 padding # image_atts: (B, N+1)，与 image_embeds 保持一致 image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device) # 3. 取出 CLS Token（图像全局语义），进行线性变换 + 归一化 # image_feat: (B, D)，D为投影后的embedding维度（如256） image_feat = F.normalize(self.vision_proj(image_embeds[:, 0, :]), dim=-1) # 4. 文本编码器（BERT）对文本进行编码 # text_output.last_hidden_state: (B, L, H)，L 为文本长度，H 为hidden size text_output = self.text_encoder.bert(text.input_ids, attention_mask=text.attention_mask, return_dict=True, mode='text') text_embeds = text_output.last_hidden_state # 5. 取出 CLS Token（文本全局语义），线性变换 + 归一化 # text_feat: (B, D) text_feat = F.normalize(self.text_proj(text_embeds[:, 0, :]), dim=-1) # ========== 以下为动量编码器分支（momentum encoder），不参与反向传播 ========== with torch.no_grad(): # 6. 更新动量编码器参数（对主编码器做 EMA） self._momentum_update() # 7. 动量图像编码器输出特征 # image_embeds_m: (B, N+1, embed_dim) # image_feat_m: (B, D) image_embeds_m = self.visual_encoder_m(image) image_feat_m = F.normalize(self.vision_proj_m(image_embeds_m[:, 0, :]), dim=-1) # 8. 拼接当前动量图像特征和图像队列（K 个历史负样本） # image_feat_m.T: (D, B) # image_queue: (D, K) # image_feat_all: (D, B + K) image_feat_all = torch.cat([image_feat_m.t(), self.image_queue.clone().detach()], dim=1) # 9. 动量文本编码器输出特征 # text_output_m.last_hidden_state: (B, L, H) # text_feat_m: (B, D) text_output_m = self.text_encoder_m.bert(text.input_ids, attention_mask=text.attention_mask, return_dict=True, mode='text') text_feat_m = F.normalize(self.text_proj_m(text_output_m.last_hidden_state[:, 0, :]), dim=-1) # 10. 拼接当前动量文本特征和文本队列 # text_feat_all: (D, B + K) text_feat_all = torch.cat([text_feat_m.t(), self.text_queue.clone().detach()], dim=1) # 11. 图像特征与所有文本特征做内积，计算相似度（B, B+K） sim_i2t_m = image_feat_m @ text_feat_all / self.temp sim_t2i_m = text_feat_m @ image_feat_all / self.temp # 12. 构造一对一的匹配目标（对角为正样本） # sim_targets: (B, B+K)，对角为1，其他为0 sim_targets = torch.zeros(sim_i2t_m.size()).to(image.device) sim_targets.fill_diagonal_(1) # 13. 构造 soft label（平滑过的对比目标） # alpha = 0 则为 hard label，alpha 越大越 soft sim_i2t_targets = alpha * F.softmax(sim_i2t_m, dim=1) + (1 - alpha) * sim_targets sim_t2i_targets = alpha * F.softmax(sim_t2i_m, dim=1) + (1 - alpha) * sim_targets # ========== 当前主分支计算相似度，用于 loss 反向传播 ========== # 14. 使用主分支特征与队列拼接结果计算图像-文本相似度（B, B+K） sim_i2t = image_feat @ text_feat_all / self.temp sim_t2i = text_feat @ image_feat_all / self.temp # 15. 计算交叉熵损失（基于 soft label 的 KL loss） loss_i2t = -torch.sum(F.log_softmax(sim_i2t, dim=1) * sim_i2t_targets, dim=1).mean() loss_t2i = -torch.sum(F.log_softmax(sim_t2i, dim=1) * sim_t2i_targets, dim=1).mean() # 16. 图文对比损失（取双向平均） loss_ita = (loss_i2t + loss_t2i) / 2 # 17. 将当前动量特征送入队列，更新队列 self._dequeue_and_enqueue(image_feat_m, text_feat_m)\",\"加入动量队列中的样本作为负样本，是为了扩大负样本池，提升训练难度、判别性和稳定性，使模型能学到更强的图文对齐表示。\"]},\"207\":{\"h\":\"ITM\",\"t\":[\"ALBEF 模型前向传播中的 ITM 学习目标实现过程:\",\"###=================================### # 正向图文对的前向传播（正样本） output_pos = self.text_encoder.bert( encoder_embeds = text_embeds, # 输入文本的嵌入表示 attention_mask = text.attention_mask, # 文本的注意力掩码 encoder_hidden_states = image_embeds, # 图像特征作为 cross-attention 的 encoder hidden state encoder_attention_mask = image_atts, # 图像 attention mask（通常为全 1） return_dict = True, # 返回结构化输出（字典） mode = 'fusion', # 模态融合模式 ) # ================================= # # 计算 ITC 相似度生成的 soft label，用于选择难负样本 with torch.no_grad(): bs = image.size(0) # batch size # 图像到文本的相似度权重（归一化) , sim_i2t维度为(B, B+K) , 这里只取前B个样本, 不考虑从动量队列拿到的负样本 weights_i2t = F.softmax(sim_i2t[:, :bs], dim=1) # 文本到图像的相似度权重（归一化） weights_t2i = F.softmax(sim_t2i[:, :bs], dim=1) # 屏蔽对角线（避免采样到自己） weights_i2t.fill_diagonal_(0) weights_t2i.fill_diagonal_(0) # ================================= # # 采样每个文本对应的负图像（hard negative） image_embeds_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_t2i[b], 1).item() # 按权重从当前行采样一个负样本索引 image_embeds_neg.append(image_embeds[neg_idx]) # 获取对应的负图像嵌入 image_embeds_neg = torch.stack(image_embeds_neg, dim=0) # [B, D] # 采样每个图像对应的负文本（hard negative） text_embeds_neg = [] text_atts_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_i2t[b], 1).item() text_embeds_neg.append(text_embeds[neg_idx]) # 获取对应的负文本嵌入 text_atts_neg.append(text.attention_mask[neg_idx]) # 同时获取对应的 attention mask text_embeds_neg = torch.stack(text_embeds_neg, dim=0) text_atts_neg = torch.stack(text_atts_neg, dim=0) # 构造新的图文对：正文本 + 负文本，负图像 + 正图像 # 共有 2N 个图文对：N 个正样本 + N 个负样本（正文负图 + 正图负文） text_embeds_all = torch.cat([text_embeds, text_embeds_neg], dim=0) text_atts_all = torch.cat([text.attention_mask, text_atts_neg], dim=0) image_embeds_all = torch.cat([image_embeds_neg, image_embeds], dim=0) image_atts_all = torch.cat([image_atts, image_atts], dim=0) # 所有图文对的前向传播（用于 ITM 分类） output_neg = self.text_encoder.bert( encoder_embeds = text_embeds_all, attention_mask = text_atts_all, encoder_hidden_states = image_embeds_all, encoder_attention_mask = image_atts_all, return_dict = True, mode = 'fusion', ) # 提取 [CLS] token 表征作为跨模态图文对表示，输入到 ITM 头 vl_embeddings = torch.cat([ output_pos.last_hidden_state[:, 0, :], # 正样本 [CLS] output_neg.last_hidden_state[:, 0, :] # 负样本 [CLS] ], dim=0) # 二分类：匹配 or 不匹配 vl_output = self.itm_head(vl_embeddings) # shape: [3N, 2] # 构造 ground-truth 标签：前 N 个为正样本（1），后 2N 个为负样本（0） itm_labels = torch.cat([ torch.ones(bs, dtype=torch.long), # N 个正样本 torch.zeros(2*bs, dtype=torch.long) # 2N 个负样本（正图负文 + 正文负图） ], dim=0).to(image.device) # 计算 ITM 的交叉熵损失 loss_itm = F.cross_entropy(vl_output, itm_labels)\",\"补充说明:\",\"ALBEF 在进行 CrossAttention 时，image features 会作为 key 和 value ，而 text features 作为 query:\",\"class BertSelfAttention(nn.Module): def forward( self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False, ): # 1. text features 固定作为 query 计算来源 mixed_query_layer = self.query(hidden_states) # 2. 传入了 image features ，则做 cross attention is_cross_attention = encoder_hidden_states is not None if is_cross_attention: key_layer = self.transpose_for_scores(self.key(encoder_hidden_states)) value_layer = self.transpose_for_scores(self.value(encoder_hidden_states)) attention_mask = encoder_attention_mask else: key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) query_layer = self.transpose_for_scores(mixed_query_layer) ...\",\"ALBEF 采用加权随机采样而非直接取相似度最大的负样本（argmax），是为了在突出“难负样本”的同时保持训练的稳定性和多样性，避免模型过拟合于极端负样本或伪负样本，从而提升泛化能力和鲁棒性。\"]},\"208\":{\"h\":\"MLM\",\"t\":[\"ALBEF 模型前向传播中的 MLM 学习目标实现过程:\",\"##================= MLM ========================## # 克隆一份 input_ids 和 labels，作为 MLM 的输入和标签副本 input_ids = text.input_ids.clone() labels = input_ids.clone() # 构造一个与 input_ids 同形状的矩阵，值为 mask 概率（例如 0.15） probability_matrix = torch.full(labels.shape, self.mlm_probability) # 对 input_ids 按照给定概率进行 [MASK] 操作，同时将对应的 labels 保留为原始 token id，其余位置设为 -100（忽略） input_ids, labels = self.mask(input_ids, self.text_encoder.config.vocab_size, image.device, targets=labels, probability_matrix = probability_matrix) # ===== 使用动量编码器对 masked 输入做前向传播，获取 soft target（Teacher 网络） ===== # 注意：这一步不计算梯度，仅用于生成 soft label with torch.no_grad(): logits_m = self.text_encoder_m( input_ids, attention_mask = text.attention_mask, # 文本 attention mask encoder_hidden_states = image_embeds_m, # 动量视觉特征 encoder_attention_mask = image_atts, # 图像 attention mask return_dict = True, return_logits = True, # 返回 logits 用于 soft label ) # ===== 主网络进行 MLM 前向传播，并引入 soft label 监督 ===== mlm_output = self.text_encoder( input_ids, attention_mask = text.attention_mask, encoder_hidden_states = image_embeds, # 主视觉特征（非动量） encoder_attention_mask = image_atts, return_dict = True, labels = labels, # 用于 standard cross-entropy 监督（hard label） soft_labels = F.softmax(logits_m, dim=-1), # soft label 来自动量编码器（Teacher） alpha = alpha # 混合比：控制 hard 和 soft loss 的权重 ) # 最终的 masked language modeling 损失 loss_mlm = mlm_output.loss\",\"mask 方法代码实现:\",\"def mask(self, input_ids, vocab_size, device, targets=None, masked_indices=None, probability_matrix=None): # Step 1: 生成掩码位置 if masked_indices is None: # 若未指定掩码位置，则按给定的概率矩阵进行伯努利采样，得到每个 token 是否被 mask masked_indices = torch.bernoulli(probability_matrix).bool() # Step 2: 屏蔽不可 mask 的位置（例如 [PAD] 和 [CLS]） masked_indices[input_ids == self.tokenizer.pad_token_id] = False masked_indices[input_ids == self.tokenizer.cls_token_id] = False # Step 3: 构造目标标签（只对被 mask 的位置计算 loss） if targets is not None: targets[~masked_indices] = -100 # 非 mask 位置的标签设为 -100，表示 loss 忽略 # Step 4: 对被 mask 的 token 进行替换（按 BERT 策略） # 80% 的 mask token 被替换为 [MASK] indices_replaced = torch.bernoulli(torch.full(input_ids.shape, 0.8)).bool() & masked_indices input_ids[indices_replaced] = self.tokenizer.mask_token_id # 10% 的 mask token 被替换为随机 token（噪声） indices_random = torch.bernoulli(torch.full(input_ids.shape, 0.5)).bool() & masked_indices & ~indices_replaced random_words = torch.randint(vocab_size, input_ids.shape, dtype=torch.long).to(device) input_ids[indices_random] = random_words[indices_random] # 剩下的 10% 保持原样（不修改 token） # Step 5: 返回掩码后的 input_ids 和（可选的）标签 targets if targets is not None: return input_ids, targets else: return input_ids\",\"text_encoder 前向传播代码实现:\",\"def forward( self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_embeds=None, # 外部传入的 encoder embeddings（不常用） encoder_hidden_states=None, # 图像编码器输出（作为 cross-attn 的 K,V） encoder_attention_mask=None, # 图像部分的 attention mask labels=None, # MLM 标签（只在 MLM 模式中提供） output_attentions=None, output_hidden_states=None, return_dict=None, is_decoder=False, mode='multi_modal', # 模式控制，支持 'fusion'（图文融合）等 soft_labels=None, # 蒸馏 soft labels，来自 momentum encoder alpha=0, # 蒸馏损失的权重 return_logits=False, # 是否仅返回 logits（用于 momentum 计算） ): # Step 1: 调用 BERT 模型（支持 encoder-decoder 模式，支持 fusion 模式） outputs = self.bert( input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, encoder_embeds=encoder_embeds, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, is_decoder=is_decoder, mode=mode, ) # Step 2: 获取 transformer 输出的 token 表征（[B, L, D]） sequence_output = outputs[0] # Step 3: 计算每个 token 的预测分布（[B, L, vocab_size]） prediction_scores = self.cls(sequence_output) # Step 4: 若只需输出 logits（如 momentum 模型前向），直接返回 if return_logits: return prediction_scores # Step 5: 计算标准 MLM 交叉熵损失（仅对 label ≠ -100 的位置有效） masked_lm_loss = None if labels is not None: loss_fct = CrossEntropyLoss() # 忽略标签为 -100 的位置 masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1)) # Step 6: 若提供 soft labels（如知识蒸馏），计算 KL-style 蒸馏损失 if soft_labels is not None: # 蒸馏损失：soft label 和当前输出的 softmax 分布之间的 KL 散度 loss_distill = -torch.sum( F.log_softmax(prediction_scores, dim=-1) * soft_labels, dim=-1 ) loss_distill = loss_distill[labels != -100].mean() # 混合两种损失：标准 MLM loss 和 蒸馏 loss masked_lm_loss = (1 - alpha) * masked_lm_loss + alpha * loss_distill # Step 7: 根据 return_dict 控制输出格式（支持 tuple 或 dict） if not return_dict: output = (prediction_scores,) + outputs[2:] return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output # 标准化输出（MaskedLMOutput 是 huggingface 定义的一个结构体） return MaskedLMOutput( loss=masked_lm_loss, logits=prediction_scores, hidden_states=outputs.hidden_states, attentions=outputs.attentions, )\",\"注意:\",\"HuggingFace 的 CrossEntropyLoss 默认会忽略标签为 -100 的位置，这是 PyTorch 官方文档中的行为规范：\",\"class CrossEntropyLoss(_WeightedLoss): def __init__( self, weight: Optional[Tensor] = None, size_average=None, ignore_index: int = -100, reduce=None, reduction: str = \\\"mean\\\", label_smoothing: float = 0.0, )\"]},\"209\":{\"h\":\"BEIT2 论文\",\"t\":[\"BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers 论文解读\",\"论文链接: BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers 代码链接: https://github.com/microsoft/unilm/tree/master/beit2\"]},\"210\":{\"h\":\"引言\",\"t\":[\"掩码图像建模（MIM）通过恢复被掩码的图像块，能够在自监督学习中捕捉丰富的上下文信息，但大多数方法仅在低层像素上操作。\",\"现有重建目标可以分为三类：\",\"低层图像元素（如原始像素）\",\"手工特征（如 HOG 特征）\",\"视觉 token\",\"这些方法大多忽略了高层语义信息，而语言模型中的掩码词都是高层语义，这启发了 MIM 可以借助语义感知监督进行改进。\",\"BEIT V2 提出 向量量化知识蒸馏（VQ-KD），将连续的语义空间离散化为紧凑的视觉 token。VQ-KD 训练过程：\",\"编码器将输入图像转为离散 token，基于可学习码本（codebook）。\",\"解码器根据教师模型编码的语义特征重建图像特征。\",\"训练完成后，VQ-KD 的编码器被用作 BEIT V2 的语义视觉分词器，离散 token 作为监督信号进行 MIM 预训练。引入 图像块聚合策略，让 [CLS] token 聚合全局信息，解决传统 MIM 过度关注局部块重建而忽略全局表示的问题。\"]},\"211\":{\"h\":\"方法\"},\"212\":{\"h\":\"预训练阶段一: 向量量化知识蒸馏算法用于d-VAE预训练\",\"t\":[\"BEIT V2 继承了 BEIT 的掩码图像建模（Masked Image Modeling）框架，其核心思想是将每张图像通过视觉 tokenizer 转换为一组离散的视觉 token，然后训练模型去恢复被遮挡的 token。每个 token 对应图像中的一个 patch，从而实现对局部图像信息的建模（如图 2 所示）。训练过程中，引入了向量量化知识蒸馏（VQ-KD）算法，用于训练视觉 tokenizer，使其能够有效将图像映射到离散编码。\",\"图像表示部分，输入图像 会被划分为 个 patch ，每个 patch 大小为 ，在实验中 224 × 224 图像被划分为 14 × 14 个 patch，每个 patch 16 × 16。所有 patch 展平并线性映射得到 Transformer 的输入嵌入 ，用于后续编码。\",\"在 VQ-KD 训练中，视觉 tokenizer 由编码器和量化器组成：\",\"编码器将图像转换为 patch 表征 ；\",\"量化器在代码本 中查找每个 的最近邻进行量化，得到离散 token ，公式为：\",\"其中 表示 归一化，等价于基于余弦相似度查找最近代码。量化后的 归一化代码 输入解码器，解码器输出 尝试重建教师模型（如 DINO 或 CLIP）的语义特征 。训练目标最大化 decoder 输出与教师特征的余弦相似度，同时通过 stop-gradient 机制处理量化不可导问题，梯度从 decoder 输入传递到 encoder 输出。训练目标公式为：\",\"其中 表示停止梯度操作， 为训练图像数据集。\",\"向量量化训练中常见问题是代码本塌陷（codebook collapse），即只使用少量编码。为缓解此问题，VQ-KD 使用经验策略：\",\"查找最近邻时对代码本嵌入进行 归一化，并将维度降至 32；\",\"在输入 decoder 前将低维嵌入映射回高维空间；\",\"代码本嵌入使用指数移动平均（EMA）更新，EMA 能更稳定地追踪模型训练动态。\",\"整体而言，BEIT V2 结合视觉 tokenizer、VQ-KD 和 Transformer 架构，通过 patch 级别的离散表示学习与教师特征对齐，实现对图像语义信息的高效编码与预训练。\"]},\"213\":{\"h\":\"代码实现\",\"t\":[\"关于阶段一的预训练过程，我们先来看 cookbook 的代码实现，由于 BEiT-V2 采用 EMA(指数平均移动) 来对码本的状态参数进行缓慢更新，所以代码实现方面会维护一些额外的状态参数:\",\"class EmbeddingEMA(nn.Module): def __init__(self, num_tokens, codebook_dim, decay=0.99, eps=1e-5, kmeans_init=True, codebook_init_path=''): \\\"\\\"\\\" 向量量化的 codebook（码本）管理类，采用 EMA（指数滑动平均）进行更新。 参数: - num_tokens: 码本的向量个数（即字典大小） - codebook_dim: 每个向量的维度 - decay: EMA 的衰减系数 - eps: 避免数值错误的小常数 - kmeans_init: 是否使用 k-means 初始化 - codebook_init_path: 若提供，则从已有 checkpoint 加载初始化码本 \\\"\\\"\\\" super().__init__() self.num_tokens = num_tokens self.codebook_dim = codebook_dim self.decay = decay self.eps = eps # ========== 初始化权重 ========== if codebook_init_path == '': # 如果没有提供预训练的 codebook if not kmeans_init: # 随机初始化，并做 L2 归一化，保证每个 embedding 向量长度为 1 weight = torch.randn(num_tokens, codebook_dim) weight = l2norm(weight) else: # 若选择 kmeans_init，则先用全零矩阵占位，稍后再通过 k-means 初始化 weight = torch.zeros(num_tokens, codebook_dim) # 标记是否完成初始化（True=已初始化，False=未初始化） self.register_buffer('initted', torch.Tensor([not kmeans_init])) else: # 如果给定路径，则直接加载预训练的 codebook 权重 print(f\\\"load init codebook weight from {codebook_init_path}\\\") codebook_ckpt_weight = torch.load(codebook_init_path, map_location='cpu') weight = codebook_ckpt_weight.clone() self.register_buffer('initted', torch.Tensor([True])) # ========== 需要维护的参数 ========== # codebook 权重（不参与梯度更新，使用 EMA 更新） self.weight = nn.Parameter(weight, requires_grad = False) # 每个 cluster 的大小（计数），用来做 EMA 更新 self.cluster_size = nn.Parameter(torch.zeros(num_tokens), requires_grad = False) # 每个 cluster embedding 的均值，用于 EMA 更新 self.embed_avg = nn.Parameter(weight.clone(), requires_grad = False) # 是否启用更新 self.update = True\",\"cookbook 初始化过程大体含有两个阶段: 码本权重初始化; 状态参数初始化; BEiT-V2 针对码本权重的初始化还做了特别的优化，我们可以打开 kmeans_init 开关，让码本延迟到首次向量量化阶段，采用 k-means 算法对 encoder 输出的特征向量做聚类迭代，得到 num_tokens 个簇中心来作为码本的初始化向量。\",\"这样做的好处是:\",\"避免了随机初始化导致的码本塌陷问题;\",\"利用了 encoder 输出的特征分布信息，使得码本初始化更加合理。\",\"代码实现如下:\",\" @torch.jit.ignore def init_embed_(self, data): \\\"\\\"\\\" 用 k-means 对码本进行初始化。 - data: encoder 输出的样本数据 (N, D) \\\"\\\"\\\" if self.initted: # 若已初始化，则跳过 return print(\\\"Performing K-means init for codebook\\\") # 调用 kmeans 获取初始的 cluster 中心和 cluster 大小 embed, cluster_size = kmeans(data, self.num_tokens, 10, use_cosine_sim = True) # 更新权重和 cluster_size self.weight.data.copy_(embed) self.cluster_size.data.copy_(cluster_size) # 设置为已初始化状态 self.initted.data.copy_(torch.Tensor([True]))\",\"k-means 的计算步骤可以总结为以下几个核心环节：\",\"初始化簇中心\",\"从样本中随机选取 num_clusters 个向量作为初始中心，或者使用其他方法（如 k-means++）。\",\"计算样本与中心的距离/相似度\",\"对每个样本计算它与所有簇中心的距离（欧氏距离）或相似度（余弦相似度）。\",\"样本分配\",\"将每个样本分配到最近的簇（或相似度最高的簇），形成簇成员集合。\",\"统计簇信息\",\"统计每个簇的样本数量（用于更新中心和处理空簇）。\",\"更新簇中心\",\"对每个簇，将簇内样本向量求平均，得到新的中心。\",\"若某簇为空，则保留原中心不变。\",\"如果使用余弦相似度，更新后的中心需要做 L2 归一化。\",\"迭代\",\"重复步骤 2–5，直到达到预定迭代次数或收敛条件。\",\"输出结果\",\"返回最终的簇中心和每个簇的样本数。\",\"这整个过程就是 k-means 聚类的标准迭代流程：分配 → 更新 → 循环 , 具体代码实现如下:\",\"def kmeans(samples, num_clusters, num_iters = 10, use_cosine_sim = False): # samples: 输入样本，形状 (N, D)，N 是样本数，D 是维度 # num_clusters: 聚类簇数，即要分成多少类 # num_iters: k-means 的迭代次数 # use_cosine_sim: 是否用余弦相似度（默认用欧氏距离） # 提取样本维度、数据类型和设备 dim, dtype, device = samples.shape[-1], samples.dtype, samples.device # 从样本中随机选取 num_clusters 个向量作为初始中心 means = sample_vectors(samples, num_clusters) # 重复迭代更新聚类中心 for _ in range(num_iters): if use_cosine_sim: # 使用余弦相似度：直接点积即可（因为向量一般做过 l2norm） # 结果 shape: (N, K)，表示每个样本和每个中心的相似度 dists = samples @ means.t() else: # 使用欧氏距离： (x - μ)^2 # diffs: (N, 1, D) - (1, K, D) = (N, K, D) diffs = rearrange(samples, 'n d -> n () d') \\\\ - rearrange(means, 'c d -> () c d') # 计算平方距离并取负号（因为后面要用 max 来找最近中心） dists = -(diffs ** 2).sum(dim = -1) # shape: (N, K) # 找到每个样本最近的中心（或相似度最大的中心） # buckets: (N,) 每个样本对应的簇编号 buckets = dists.max(dim = -1).indices # 统计每个簇的样本数量 bins = torch.bincount(buckets, minlength = num_clusters) # (K,) # 标记哪些簇没有分配到样本（空簇） zero_mask = bins == 0 # 防止除以 0，把空簇的计数临时设为 1 bins_min_clamped = bins.masked_fill(zero_mask, 1) # 初始化新的簇中心 (K, D)，全部为 0 new_means = buckets.new_zeros(num_clusters, dim, dtype = dtype) # 把属于同一簇的样本向量加到对应的中心上 # repeat(buckets, 'n -> n d', d = dim): 把 (N,) 扩展成 (N, D)，方便 scatter_add new_means.scatter_add_(0, repeat(buckets, 'n -> n d', d = dim), samples) # 除以该簇的样本数，得到新的簇中心 new_means = new_means / bins_min_clamped[..., None] # 如果用余弦相似度，记得对中心做 l2norm 归一化 if use_cosine_sim: new_means = l2norm(new_means) # 更新簇中心： # - 如果该簇是空簇（zero_mask=True），保留旧的中心 # - 否则更新为新的中心 means = torch.where(zero_mask[..., None], means, new_means) # 返回最终的簇中心和每个簇的样本数 return means, bins\",\"向量量化器负责将连续的视觉特征映射到离散的视觉 token，该过程借助内部维护的 cookbook 完成，本节我们来详细解析一下它的实现逻辑:\",\"class NormEMAVectorQuantizer(nn.Module): def __init__(self, n_embed, embedding_dim, beta, decay=0.99, eps=1e-5, statistic_code_usage=True, kmeans_init=False, codebook_init_path=''): super().__init__() # codebook 向量的维度（即每个 embedding 的维数） self.codebook_dim = embedding_dim # codebook 的大小（有多少个离散 token） self.num_tokens = n_embed # commitment loss 的权重系数 self.beta = beta # EMA 更新的衰减系数 self.decay = decay # codebook，使用 EMA 更新（非梯度更新） # 这里的 EmbeddingEMA 类负责存储和更新 codebook 向量 # 参数： # - num_tokens: codebook 的大小 # - codebook_dim: 每个向量的维度 # - decay, eps: EMA 更新超参 # - kmeans_init: 是否用 k-means 初始化 codebook # - codebook_init_path: 是否从文件加载已有的 codebook self.embedding = EmbeddingEMA( self.num_tokens, self.codebook_dim, decay, eps, kmeans_init, codebook_init_path ) # 是否统计每个 code 的使用频率（防止 dead code） self.statistic_code_usage = statistic_code_usage if statistic_code_usage: # cluster_size 用来存储每个 code 的使用计数，注册为 buffer，随模型保存 self.register_buffer('cluster_size', torch.zeros(n_embed))\",\"向量量化器的前向传播过程负责将 encoder 编码得到的特征图 z 映射到离散的视觉 token 上，具体过程如下:\",\"def forward(self, z): \\\"\\\"\\\" 前向传播函数，实现向量量化（Vector Quantization）和 EMA 更新 参数: - z: 输入特征图, shape (batch, channel, height, width) 返回: - z_q: 量化后的特征图，shape 同输入 - loss: 量化损失 - encoding_indices: 每个向量对应的码本索引 \\\"\\\"\\\" # 将输入从 (B, C, H, W) 转换为 (B, H, W, C) 以便处理通道维 z = rearrange(z, 'b c h w -> b h w c') # L2 归一化 z = l2norm(z) # 展平特征图，每一行对应一个向量 (num_vectors, embedding_dim) z_flattened = z.reshape(-1, self.codebook_dim) # 初始化码本（如果需要） self.embedding.init_embed_(z_flattened) # 计算每个向量与码本中所有向量的欧氏距离平方 d = z_flattened.pow(2).sum(dim=1, keepdim=True) + \\\\ self.embedding.weight.pow(2).sum(dim=1) - 2 * \\\\ torch.einsum('bd,nd->bn', z_flattened, self.embedding.weight) # 'n d -> d n' # 为每个向量找到最近的码本索引 encoding_indices = torch.argmin(d, dim=1) # 将编码索引映射回码本向量并 reshape 成原来的特征图形状 z_q = self.embedding(encoding_indices).view(z.shape) # one-hot 编码 encodings = F.one_hot(encoding_indices, self.num_tokens).type(z.dtype) # 非训练模式下统计码本使用情况 if not self.training: with torch.no_grad(): cluster_size = encodings.sum(0) self.all_reduce_fn(cluster_size) # 分布式同步 ema_inplace(self.cluster_size, cluster_size, self.decay) # 训练模式下更新 EMA 码本 if self.training and self.embedding.update: bins = encodings.sum(0) self.all_reduce_fn(bins) # 更新 cluster_size 的 EMA ema_inplace(self.cluster_size, bins, self.decay) # 避免除零 zero_mask = (bins == 0) bins = bins.masked_fill(zero_mask, 1.) # 计算每个码本向量的累加特征 embed_sum = z_flattened.t() @ encodings self.all_reduce_fn(embed_sum) # 归一化并 L2 正则化 embed_normalized = (embed_sum / bins.unsqueeze(0)).t() embed_normalized = l2norm(embed_normalized) # 对未使用的码本向量保持原值 embed_normalized = torch.where(zero_mask[..., None], self.embedding.weight, embed_normalized) # 更新 EMA 码本权重 norm_ema_inplace(self.embedding.weight, embed_normalized, self.decay) # 量化损失 --- 只对encoder进行更新，cookbook不采用梯度回传更新 loss = self.beta * F.mse_loss(z_q.detach(), z) # 保留梯度 --- 用于后续重建损失，梯度可以沿着直接通路回传回encoder进行更新 z_q = z + (z_q - z).detach() # reshape 回原始输入形状 (B, C, H, W) z_q = rearrange(z_q, 'b h w c -> b c h w') return z_q, loss, encoding_indices\",\"前向传播的过程中，向量量化器还完成了对码本相关状态参数的更新，更新算法为: EMA(指数平均移动更新)。\",\"def ema_inplace(moving_avg, new, decay): moving_avg.data.mul_(decay).add_(new, alpha = (1 - decay))\",\"关于码本向量的更新流程就是 计算每个 codebook 的新的簇中心：\",\"用 encodings 把属于同一个 codebook 的样本挑出来\",\"把这些样本向量累加 → 得到 embed_sum\",\"除以该 codebook 的样本数量 → 得到均值\",\"均值向量就作为 新的 codebook 向量（簇中心）用于 EMA 更新\",\"简而言之，这一步就是 “统计簇内所有样本 → 计算簇中心” 的操作。\",\"最后讲解一下上面代码最后的两处梯度卸载操作:\",\" # 量化损失 --- 只对encoder进行更新，cookbook不采用梯度回传更新 loss = self.beta * F.mse_loss(z_q.detach(), z)\",\"MSE 公式展开如下：\",\"：encoder 输出\",\"：量化向量（detach 后不可导）\",\"反向梯度：\",\" 被视作常量 → 不产生梯度\",\"encoder 输出 可以接收梯度\",\"codebook 不通过梯度更新，EMA 更新独立\",\" # 保留梯度 --- 用于后续重建损失，梯度可以沿着直接通路回传回encoder进行更新 z_q = z + (z_q - z).detach()\",\"z_q 被更新后，后面参与了重建损失的计算，梯度回传的时候，(z_q - z).detach() 这部分被当作了常量，计算图认为对学习任务的贡献全部来源于 z ，但实际上前向传播阶段的贡献来源于z_q，这相当于使了一个障眼法。\",\"最后我们来看一下 VQKD 模型的完整结构，首先从它的初始化方法入手：\",\"class VQKD(nn.Module): \\\"\\\"\\\" VQKD (Vector-Quantized Knowledge Distillation) 模型 这是一个基于向量量化的知识蒸馏模型，用于学习图像的语义表示。 包含编码器、解码器、量化器和教师模型等组件。 \\\"\\\"\\\" def __init__(self, encoder_config, # 编码器配置参数 decoder_config, # 解码器配置参数 n_embed=8192, # 代码本大小（词汇表大小） embed_dim=32, # 嵌入维度 decay=0.99, # EMA衰减率 process_type='default', # 图像预处理类型 quantize_kmeans_init=True, # 是否使用k-means初始化量化器 teacher_model_type='clip', # 教师模型类型（clip或dino） decoder_out_dim=512, # 解码器输出维度 rec_loss_type='cosine', # 重建损失类型 **kwargs ): super().__init__() # 创建编码器和解码器 self.encoder = VisionTransformer(**encoder_config) # 使用Vision Transformer作为编码器 self.decoder = VisionTransformer(**decoder_config) # 使用Vision Transformer作为解码器 # 创建向量量化器 self.quantize = NormEMAVectorQuantizer( n_embed=n_embed, embedding_dim=embed_dim, beta=1.0, kmeans_init=quantize_kmeans_init, decay=decay, ) # 记录patch大小和token形状 self.patch_size = encoder_config['patch_size'] self.token_shape = (encoder_config['img_size'] // self.patch_size, encoder_config['img_size'] // self.patch_size) ## 教师模型设置 self.teacher_model_type = teacher_model_type self.decoder_out_dim = decoder_out_dim if self.teacher_model_type == 'clip': # 使用CLIP作为教师模型 self.scaling_layer = ScalingLayerForClip() # CLIP专用的缩放层 self.teacher_model, _ = clip.load(\\\"ViT-B/16\\\", device='cpu', jit=False) # 加载CLIP ViT-B/16模型 self.decoder_out_dim = 512 # CLIP输出维度为512 elif self.teacher_model_type == 'dino': # 使用DINO作为教师模型 self.scaling_layer = ScalingLayerForIM() # DINO专用的缩放层 self.teacher_model = get_dino_vit_base() # 加载DINO ViT-Base模型 self.decoder_out_dim = 768 # DINO输出维度为768 else: self.teacher_model = None if self.teacher_model is not None: # 冻结教师模型参数，不参与训练 for param in self.teacher_model.parameters(): param.requires_grad = False self.teacher_model.eval() # 设置为评估模式 self.teacher_input_size = kwargs.get('teacher_input_size', 224) # 任务特定层：用于调整特征维度 self.encode_task_layer = nn.Sequential( nn.Linear(encoder_config['embed_dim'], encoder_config['embed_dim']), # 线性变换 nn.Tanh(), # 激活函数 nn.Linear(encoder_config['embed_dim'], embed_dim) # 映射到量化器维度 ) self.decode_task_layer = nn.Sequential( nn.Linear(decoder_config['embed_dim'], decoder_config['embed_dim']), # 线性变换 nn.Tanh(), # 激活函数 nn.Linear(decoder_config['embed_dim'], self.decoder_out_dim), # 映射到教师模型输出维度 ) self.rec_loss_type = rec_loss_type print(f\\\"process type for VQKD: {process_type}\\\") self.process_type = process_type # 支持 'default', 'dall-e', 'imagenet_norm' self.logit_laplace_eps = 0.1 self.kwargs = kwargs # 初始化权重 self.encode_task_layer.apply(self._init_weights) self.decode_task_layer.apply(self._init_weights)\",\"VQKD 模型的前向传播流程负责具体落地知识蒸馏算法的实现，也就是让 d-VAE 模型学会从教师模型中学会编码图像高级语义信息的能力:\",\" def forward(self, x, **kwargs): \\\"\\\"\\\" 前向传播函数 Args: x: 输入图像，形状为 [B, 3, H, W]，值域为 [0, 1] Returns: loss: 总损失 log: 损失日志 \\\"\\\"\\\" x = self.pre_process(x) # 预处理图像到 [-1, 1] 范围 # 从教师模型获取目标特征 target = self.get_regress_target(x, **kwargs) # 编码和解码 quantize, embed_ind, emb_loss = self.encode(x) # 编码得到量化结果 xrec = self.decode(quantize) # 解码重建特征 # 计算重建损失 rec_loss = self.calculate_rec_loss(xrec, target) # 总损失 = 量化损失 + 重建损失 loss = emb_loss + rec_loss # 记录损失日志 log = {} split = \\\"train\\\" if self.training else \\\"val\\\" log[f'{split}/quant_loss'] = emb_loss.detach().mean() # 量化损失 log[f'{split}/rec_loss'] = rec_loss.detach().mean() # 重建损失 log[f'{split}/total_loss'] = loss.detach().mean() # 总损失 return loss, log\",\"从教师模型获取先验知识的过程，就是将图像送入预训练好的图像编码器，如: CLIP 或 DINO 中，得到编码后的图像特征输出:\",\" @torch.no_grad() def get_regress_target(self, x, **kwargs): \\\"\\\"\\\" 获取回归目标（从教师模型） Args: x: 输入图像 Returns: target: 教师模型的特征表示 \\\"\\\"\\\" # 使用缩放层预处理图像 norm_imgs = self.scaling_layer(x) if self.teacher_model_type == 'clip': # CLIP教师模型：编码图像并投影到特征空间 target = self.teacher_model.encode_image(norm_imgs, return_all_tokens=True) @ self.teacher_model.visual.proj elif self.teacher_model_type == 'dino': # DINO教师模型：前向传播获取特征 target = self.teacher_model.forward(norm_imgs, return_patch_tokens=True) else: raise NotImplementedError return target\",\"VQ-KD 模型提供了 encode 方法用于对输入图像进行编码，得到量化后的特征表示:\",\" def encode(self, x): \\\"\\\"\\\" 编码函数：将图像编码为量化的token Args: x: 输入图像 Returns: quantize: 量化后的特征 embed_ind: 嵌入索引 loss: 量化损失 \\\"\\\"\\\" # 使用编码器提取特征 encoder_features = self.encoder(x, return_patch_tokens=True) # 通过任务层调整特征维度 with torch.cuda.amp.autocast(enabled=False): to_quantizer_features = self.encode_task_layer(encoder_features.type_as(self.encode_task_layer[-1].weight)) # 重塑特征为空间维度 N = to_quantizer_features.shape[1] h, w = int(math.sqrt(N)), int(math.sqrt(N)) to_quantizer_features = rearrange(to_quantizer_features, 'b (h w) c -> b c h w', h=h, w=w) # 使用量化器进行向量量化 quantize, loss, embed_ind = self.quantize(to_quantizer_features) return quantize, embed_ind, loss\",\"对应的还有 decode 方法，用于将量化后的特征解码为原始图像:\",\" def decode(self, quantize, **kwargs): \\\"\\\"\\\" 解码函数：将量化的token解码为重建特征 Args: quantize: 量化的特征 Returns: rec: 重建的特征 \\\"\\\"\\\" # 使用解码器重建特征 decoder_features = self.decoder(quantize, return_patch_tokens=True) # 通过任务层调整输出维度 rec = self.decode_task_layer(decoder_features) return rec\",\"最后补充一下重建损失计算的代码实现，如下所示:\",\" def calculate_rec_loss(self, rec, target): \\\"\\\"\\\" 计算重建损失 Args: rec: 重建的特征 target: 目标特征 Returns: rec_loss: 重建损失值 \\\"\\\"\\\" if self.rec_loss_type == 'cosine': # 余弦相似度损失：将特征归一化后计算余弦距离 target = target / target.norm(dim=-1, keepdim=True) rec = rec / rec.norm(dim=-1, keepdim=True) rec_loss = (1 - (target * rec).sum(-1)).mean() else: raise NotImplementedError return rec_loss\"]},\"214\":{\"h\":\"预训练阶段二: 掩码图像建模学习目标用于BEiT预训练\",\"t\":[\"BEiT V2 预训练采用了 掩码图像建模（MIM） 的策略，类似于 BEIT 原论文（Bao et al., 2022）。给定一张输入图像 ，大约 40% 的图像 patch 会被随机选择并遮挡（mask），记被遮挡的位置为 。在这些位置，使用一个共享的可学习 embedding 替换原始的 patch embedding ：\",\"其中 是指示函数。随后在输入前加入一个可学习的 [CLS] token，形成 并输入视觉 Transformer。最终编码向量为 ，其中 对应 [CLS] token。MIM 头由一个全连接层构成，用于预测被遮挡位置的视觉 token：\",\"视觉 token 来自之前训练好的 tokenizer，为 MIM 自监督提供监督信号。MIM 的训练损失为：\",\"其中 为原图像对应的视觉 token， 为预训练图像集合。需要注意的是，本工作中视觉 token 的数量与图像 patch 数量相同。\",\"为了提升全局图像表征，BEiT V2 对 [CLS] token 进行了专门预训练，旨在缓解 patch 级预训练与图像级表示聚合之间的差异（如图3所示）。具体做法是为 [CLS] token 构建信息流 bottleneck，让其尽可能收集全局信息。对于 L 层 Transformer，第 层的 patch 向量记为 ，最后一层的 [CLS] token 为 。将其与中间层 的 patch 向量拼接形成 ：\",\"然后输入浅层（如两层） Transformer decoder，再次进行遮挡预测：\",\"MIM 头参数在两处共享，MIM 损失仍只在被遮挡位置计算。最终训练损失为两部分之和：第 L 层原始损失 + 浅层 decoder 的 MIM 损失。\",\"这种设计的直观效果是模型会倾向于把全局信息推送到 ，因为模型会尽可能利用第 层到第 L 层的参数，减少额外 MIM 损失。信息流 bottleneck 鼓励 [CLS] token 学到更可靠的全局表征，同时增强的表征有助于下游任务。需要注意的是，浅层 decoder 仅用于预训练 [CLS] token，预训练完成后会被丢弃。\",\"patch 聚合策略个人理解: 通过 CLS token 聚合全局信息并参与浅层 patch token 的遮挡预测，实现全局信息对局部特征学习的反哺，从而提升 MIM 任务的预训练效果。\",\"本节代码不算复杂，与 BEiT-V1 版本预训练过程有很多重合逻辑，因此这里就不再过多展开。\"]},\"215\":{\"h\":\"实验效果\",\"t\":[\"码本设置（Visual Tokenizer / VQ-KD）对效果的影响：\",\"使用 VQ-KD 训练视觉 tokenizer 时，码本大小（K）和向量维度（D）对重建性能和下游任务表现有直接影响。\",\"解码器深度：\",\"更深的解码器能够获得更好的重建质量，但会导致码本使用率下降，且下游任务性能降低\",\"浅层解码器在下游任务上表现更好，说明浅层 decoder 更注重保留有用的全局/局部特征\",\"码本维度调整：\",\"降低码本向量维度可以提高码本利用率（codebook utilization），避免部分码本向量长期未被使用\",\"教师模型影响：\",\"DINO 监督训练的 VQ-KD，BEIT V2 在 ImageNet 和 ADE20K 上性能分别达到 84.4% 和 49.2%，显著高于教师模型\",\"CLIP 监督训练也能带来一致提升，说明 VQ-KD 对不同教师模型具有良好的可扩展性\",\"码本可视化（Figure 4）：\",\"同一码本向量对应的图像 patch 语义一致，例如代码 7856 对应“眼睛”相关 patch\",\"码本和特征量化降低了对细节变化的敏感性，同时捕获高层语义，有助于预训练表征\",\"Patch 聚合策略对效果的影响：\",\"将中间层 patch 向量与最后一层 CLS token 拼接并输入浅层 decoder，有助于CLS token 聚合全局信息，从而提升遮挡 patch 的预测能力。\",\"浅层 vs 深层 Patch 聚合头：\",\"浅层头（1~2 层）比深层头（3 层）表现更好，原因是浅层更关注输入的 [CLS] token，有助于全局信息流向局部 patch\",\"提升线性探测（linear probing）性能，说明全局表示更有效\",\"MIM 头共享：\",\"同时使用共享的 MIM head 可以进一步提升下游任务性能，增强模型对 patch 和图像整体的表征能力\",\"总结：\",\"码本设置（大小、维度、解码器深度、教师模型）直接影响重建质量、码本使用率以及下游任务性能\",\"Patch 聚合策略通过 CLS token 汇聚全局信息并反哺 patch 预测，提高线性探测性能和下游任务效果\",\"两者结合，VQ-KD + Patch 聚合策略，使 BEIT V2 在 ImageNet-1K 分类和 ADE20K 语义分割上取得显著提升\"]},\"216\":{\"h\":\"相关工作\",\"t\":[\"视觉 Tokenizer：\",\"VQ-VAE（van den Oord 等，2017）将图像转换为离散码序列，然后基于这些码重建图像。\",\"DALL-E（Ramesh 等，2021）在量化过程中使用 Gumbel-softmax 松弛，而不是 VQ-VAE 的最近邻查找。\",\"VQGAN（Esser 等，2021）和 ViT-VQGAN（Yu 等，2021）引入 Transformer 模块训练更好的自编码器，以保持图像细节，并使用对抗损失和感知损失。\",\"ViT-VQGAN 还提出了分解式与 L2 归一化码本学习方法。\",\"与此相比，VQ-KD 的目标不是重建像素，而是从教师模型中重建语义知识，从而构建一个高度紧凑的语义码本，用于 MIM 任务。\",\"掩码图像建模（MIM）：\",\"MIM 方法在语言任务中取得成功（Devlin 等，2019；Dong 等，2019；Bao 等，2020），BEIT（Bao 等，2022）将其推广到计算机视觉，通过恢复离散视觉码作为预测目标。\",\"最近的工作探索了不同的 MIM 预测目标：\",\"MAE（He 等，2022）将 MIM 视为去噪的像素级重建任务。\",\"知识蒸馏（Wei 等，2021, 2022）与自蒸馏（Baevski 等，2022）通过模仿教师在遮挡位置的特征提供监督。\",\"PeCo（Dong 等，2021）利用 MoCo v3 作为 VQGAN 的感知模型，以训练更好的 tokenizer，用于 BEIT 预训练。\",\"尽管取得进展，大多数现有方法仍基于低级像素，本工作探索如何将 MIM 从像素级提升到语义级。\"]},\"217\":{\"h\":\"总结\",\"t\":[\"本文提出 向量量化知识蒸馏（VQ-KD），用于训练视觉 tokenizer 以支持 Vision Transformer 预训练。\",\"VQ-KD 将连续语义空间离散化，为掩码图像建模提供语义级监督，而不依赖图像像素。\",\"语义视觉 tokenizer 显著提升了 BEIT 的预训练效果，并在下游任务（如图像分类和语义分割）中提高了迁移性能。\",\"引入 Patch 聚合机制，显式鼓励模型生成全局图像表示，缩小了 patch 级预训练与图像级表示聚合之间的差距。\",\"未来工作方向：希望学习一个通用 tokenizer，将词与图像映射到同一词汇表，实现视觉-语言预训练的掩码预测。\"]},\"218\":{\"h\":\"补充\",\"t\":[\"BEiT-V2 模型完整架构如下所示:\"]},\"219\":{\"h\":\"BEIT3 论文\",\"t\":[\"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks 论文解读\",\"论文链接: Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks 代码链接: https://github.com/microsoft/unilm/tree/master/beit3\"]},\"220\":{\"h\":\"引言\",\"t\":[\"近年来，语言、视觉与多模态预训练正在出现“大融合”的趋势。研究者发现，只要在海量数据上进行大规模预训练，就可以把模型轻松迁移到各种下游任务中。一个理想的方向是：预训练一个通用基础模型，能够同时处理多种模态。\",\"BEiT-3 正是顺应这一趋势提出的，它在 视觉任务和 视觉-语言任务上都取得了最新的迁移性能；BEiT-3 核心贡献如下:\",\"1. 统一的骨干架构\",\"Transformer 的成功已经从 语言 扩展到了 视觉 和 多模态任务，这使得用统一网络结构来处理不同模态成为可能。不过，不同下游任务常常需要不同架构：\",\"双编码器 (dual-encoder)：用于高效检索（如跨模态检索）。\",\"编码器-解码器 (encoder-decoder)：用于生成任务（如图像描述）。\",\"融合编码器 (fusion-encoder)：用于图文联合表示学习。\",\"问题在于：大多数基础模型需要针对不同任务手动调整网络格式，且不同模态之间的参数往往难以有效共享。\",\"BEiT-3 引入了 Multiway Transformer（多路 Transformer），作为通用建模框架。它既能做模态特定的编码，也能实现跨模态深度融合，做到“一套架构适配所有下游任务”。\",\"2. 统一的预训练任务\",\"掩码建模（Masked Data Modeling）已在多种模态上取得成功：\",\"文本（Masked Language Modeling, MLM）\",\"图像（Masked Image Modeling, MIM）\",\"图文对（Masked Multimodal Modeling）\",\"现有视觉-语言基础模型通常需要 多任务训练（如图文匹配、对比学习），但这会导致扩展到大规模数据时效率低。\",\"BEiT-3 的做法是：只保留 单一任务——mask-then-predict（掩码预测）。\",\"把图像当作外语（Imglish），和文本用相同的方式建模。\",\"图文对被看作“平行句子”，用来学习模态间的对齐关系。\",\"这种方法虽然简单，却能学习到很强的可迁移表征，并在视觉与视觉-语言任务中取得了最新结果。\",\"3. 模型与数据的规模化\",\"扩大模型规模与数据规模，可以显著提高基础模型的泛化能力。\",\"BEiT-3 将模型扩展到了 数十亿参数级别。\",\"预训练数据规模也被扩大，但仅使用 公开数据集，保证学术可复现性。\",\"即使没有依赖私有数据，BEiT-3 依然超过了许多依赖私有大数据的基础模型。\",\"此外，将图像当作外语的方式还能直接复用大规模语言模型的训练管线，从而在规模化上进一步受益。\",\"BEiT-3 使用 Multiway Transformer，在 图像、文本和图文对上进行统一的掩码建模。\",\"在训练中，会随机掩码部分文本 token 或图像 patch。\",\"学习目标是恢复原始 token（文本 token 或视觉 token）。\",\"这是一个标准的自监督学习任务，使模型在预训练阶段就能获得通用性。 BEiT-3 在多种任务上都取得了最新性能，包括：\",\"视觉任务：目标检测（COCO）、实例分割（COCO）、语义分割（ADE20K）、图像分类（ImageNet）\",\"视觉-语言任务：视觉推理（NLVR2）、视觉问答（VQAv2）、图像描述（COCO）、跨模态检索（Flickr30K、COCO）\",\"结果显示：\",\"即使只使用公开数据，BEiT-3 依然超越了许多依赖私有数据的强大模型。\",\"它不仅在多模态任务上表现优异，在纯视觉任务中也能达到甚至超过专用模型的效果。\"]},\"221\":{\"h\":\"方法\",\"t\":[\"如图2所示，BEiT-3 通过在单模态与多模态数据上进行 掩码数据建模 来预训练，采用的是一个共享的 Multiway Transformer 网络。该模型可以迁移到多种视觉和视觉-语言下游任务中。\"]},\"222\":{\"h\":\"骨干网络：Multiway Transformers\",\"t\":[\"我们采用 Multiway Transformer 作为骨干模型来对不同模态进行编码。\",\"如图2所示，每一个 Multiway Transformer 块由一个共享的 自注意力模块 和一个 前馈网络池（即模态专家） 构成，不同模态使用不同的专家。我们会根据输入 token 的模态，将其路由到对应的专家。在实现中：\",\"每一层包含一个 视觉专家 和一个 语言专家。\",\"顶部三层还额外包含 视觉-语言专家，用于融合编码（fusion encoder）。\",\"见图3 (a)(b)(c) 了解更详细的结构布局\",\"使用一组模态专家可以鼓励模型更好地捕获模态特定的信息；而共享的自注意力模块则负责学习不同模态之间的对齐，并支持在多模态任务（如视觉-语言任务）中实现深度融合。\",\"如图3所示，这种统一架构使得 BEiT-3 能够支持多种下游任务。例如：\",\"BEiT-3 可以作为图像骨干网络，用于图像分类、目标检测、实例分割和语义分割等任务。\",\"它也可以微调用作 双编码器，用于高效的图文检索。\",\"还可以作为 融合模型，应用于多模态理解与生成任务。\"]},\"223\":{\"h\":\"预训练任务：掩码数据建模\",\"t\":[\"我们通过一个 统一的掩码数据建模（Masked Data Modeling, MDM） 目标来预训练 BEiT-3，适用于单模态数据（即图像和文本）以及多模态数据（即图文对）。\",\"在预训练过程中，我们会随机掩码一部分文本 token 或图像 patch，然后训练模型恢复被掩码的 token。这样一个统一的 mask-then-predict 任务不仅可以学习到表征，还能学到不同模态之间的对齐。\",\"文本数据：使用 SentencePiece tokenizer 进行分词。\",\"图像数据：使用 BEiT v2 的 tokenizer 获得离散化的视觉 token，作为重建目标。\",\"掩码策略如下：\",\"对单模态文本：随机掩码 15% 的 token。\",\"对图文对中的文本：随机掩码 50% 的 token。\",\"对图像：使用与 BEiT 相同的 block-wise 掩码策略，随机掩码 40% 的图像 patch。\",\"值得注意的是，我们只使用这一种预训练任务，使得训练过程更适合规模化。而之前的视觉-语言模型通常需要多个预训练目标（如图文对比、图文匹配、词-图像 patch/区域对齐等）。\",\"相比之下，我们的方法能使用更小的 batch size 进行训练。而基于对比学习的模型通常需要非常大的 batch size，这带来了工程上的挑战（如 GPU 内存消耗）。\"]},\"224\":{\"h\":\"模型与预训练规模化\",\"t\":[\"骨干网络\",\"BEiT-3 是一个 超大规模基础模型，其配置遵循 ViT-giant 。如表2所示，该模型包含：\",\"40 层 Multiway Transformer\",\"隐藏层大小为 1408\",\"中间层大小为 6144\",\"16 个注意力头\",\"每一层都包含视觉专家与语言专家，顶层三层还包含视觉-语言专家。自注意力模块在不同模态之间共享。\",\"参数规模：BEiT-3 总共有 19 亿参数：\",\"视觉专家：6.92 亿\",\"语言专家：6.92 亿\",\"视觉-语言专家：5200 万\",\"共享自注意力模块：3.17 亿\",\"当 BEiT-3 用作纯视觉编码器时，只有与视觉相关的参数会被激活，其规模大约与 ViT-giant 相当（约 10 亿参数）。\",\"预训练数据\",\"BEiT-3 在单模态和多模态数据上进行预训练（见表3）。\",\"多模态数据：约 1500 万图像和 2100 万图文对，来自 5 个公开数据集： Conceptual 12M (CC12M)、Conceptual Captions (CC3M) 、SBU Captions (SBU) 、COCO 和 Visual Genome (VG) 。\",\"单模态数据：来自 ImageNet-21K 的 1400 万图像，以及 160GB 文本语料 ，包括：English Wikipedia、BookCorpus 、OpenWebText3、CC-News 和 Stories 。\",\"预训练设置\",\"总共训练 100 万步。\",\"每个 batch 包含 6144 个样本（2048 图像 + 2048 文本 + 2048 图文对）。\",\"相比对比学习模型 ，所需 batch size 要小得多。\",\"图像处理：\",\"patch 大小为 ，输入分辨率 。\",\"数据增强与 BEiT 相同，包括随机裁剪、水平翻转、颜色扰动。\",\"文本处理：\",\"使用 64k 词表的 SentencePiece tokenizer。\",\"优化配置：\",\"优化器：AdamW ，超参数 ，，。\",\"学习率：cosine decay 调度器，峰值 ，线性 warmup 10000 步。\",\"权重衰减：0.05。\",\"随机深度 (stochastic depth) ：0.1。\",\"初始化：采用 BEiT 初始化算法稳定 Transformer 训练。\"]},\"225\":{\"h\":\"总结\",\"t\":[\"BEIT-3: 一个通用的多模态基础模型，在广泛的视觉和视觉-语言基准测试上都取得了 最新的最优性能。\",\"BEIT-3 的核心思想是：将图像视作一种外语，从而可以在图像、文本以及图文对上以统一方式进行 掩码“语言”建模。\",\"我们还展示了 Multiway Transformers 能够有效建模不同的视觉与视觉-语言任务，使其成为通用建模的一个有趣选择。BEIT-3 方法简单且高效，是 多模态基础模型规模化发展的有前景方向。\",\"更进一步：\",\"预训练 多语言版本的 BEIT-3，并加入更多模态（例如音频），以促进跨语言和跨模态迁移，推进大规模预训练在任务、语言和模态间的 大融合。\",\"结合 BEIT-3 与 MetaLM 的优势，探索为多模态基础模型赋予 上下文学习能力（in-context learning）。\",\"官方没有开源预训练阶段代码，所以本文就不再对代码进行讲解了。\"]},\"226\":{\"h\":\"BEiT 论文\",\"t\":[\"BEiT: BERT Pre-Training of Image Transformers\",\"论文链接: BEiT: BERT Pre-Training of Image Transformers 代码链接: https://github.com/microsoft/unilm/tree/master/beit\"]},\"227\":{\"h\":\"摘要\",\"t\":[\"BEiT（Bidirectional Encoder representation from Image Transformers）是一种基于自监督学习的视觉Transformer预训练模型，其核心思想借鉴了BERT的掩码语言建模任务，提出掩码图像建模(MIM)方法。\",\"具体而言，BEiT将图像表示为两种视图——图像块（如16×16像素的局部区域）和离散视觉标记（通过图像分词器生成），在预训练阶段随机掩码部分图像块并让模型预测原始视觉标记，而非直接回归像素值。\",\"实验表明，BEiT在图像分类和语义分割等下游任务中表现优异，且能加速微调收敛。该方法避免了传统像素级重建的局限性，通过高层语义的离散标记学习更有效的视觉表示，为视觉Transformer的自监督预训练提供了新思路。\"]},\"228\":{\"h\":\"简介\",\"t\":[\"Transformer 在计算机视觉领域展现出强大潜力，但视觉 Transformer 通常比卷积神经网络（CNN）需要更多的训练数据。为解决这一问题，自监督预训练成为利用大规模无标注图像数据的关键方法。目前，对比学习和自蒸馏等方法已被探索，但 BERT 风格的掩码建模在视觉领域的应用尚未充分研究。\",\"BEiT 提出了一种基于 掩码图像建模（MIM） 的自监督预训练方法，其核心挑战在于：\",\"缺乏预定义词汇：与 NLP 不同，图像块没有现成的词汇表，无法直接使用 softmax 分类器预测所有可能的候选块。\",\"像素回归的局限性：直接预测掩码块的原始像素会导致模型过度关注短程依赖和高频细节，而非高层语义。\",\"BEiT 的解决方案是：\",\"使用 双视图表示（图 1）：图像块（输入）和视觉标记（目标）。视觉标记通过离散变分自编码器（dVAE）学习，形成离散化的语义表示。\",\"在预训练时，随机掩码约 40% 的图像块，并让模型基于上下文预测原始视觉标记，而非像素值。\",\"实验表明，BEiT 在图像分类和语义分割任务上优于从零训练的模型和其他自监督方法。此外，BEiT 无需人工标注即可通过自注意力机制学习语义区域和物体边界（如图 2 所示），证明了其自动捕获高层视觉知识的能力。\",\"BEiT 的贡献包括：\",\"提出 MIM 任务，为视觉 Transformer 提供理论解释（基于变分自编码器视角）。\",\"在多个下游任务（如分类、分割）上验证了其有效性。\",\"揭示了自监督预训练中自注意力机制对语义理解的自动学习能力。\"]},\"229\":{\"h\":\"方法\",\"t\":[\"给定一张输入图像 ，BEiT 会将其编码为上下文相关的向量表示。如图 1 所示，BEiT 通过自监督学习的方式，在一个掩码图像建模（Masked Image Modeling, MIM）任务上进行预训练。MIM 的目标是基于编码向量还原被遮挡的图像 patch。对于下游任务（如图像分类、语义分割等），我们会在预训练好的 BEiT 基础上添加任务层，并在特定数据集上对参数进行微调。\"]},\"230\":{\"h\":\"图像表示\",\"t\":[\"我们的方法中将图像表示为两种形式，即：图像 patch 和 视觉 token。这两种表示方式分别作为预训练中的输入和输出。\"]},\"231\":{\"h\":\"图像 Patch\",\"t\":[\"将二维图像划分为一系列 patch，这样标准的 Transformer 就可以直接处理图像数据。形式化地，假设图像 ，我们将其划分为 个 patch，patch 表示为 ，其中 是通道数， 是输入图像尺寸， 是每个 patch 的大小。\",\"图像 patch 会被展平为向量并线性投影，这类似于 BERT 中的词嵌入 。图像 patch 保留的是原始像素数据，用作 BEiT 的输入特征。\",\"在实验中，我们将每张 的图像划分为 的 patch 网格，每个 patch 为 。\"]},\"232\":{\"h\":\"视觉 Token\",\"t\":[\"类似自然语言，我们将图像表示为由“图像 tokenizer”获得的一系列离散 token，而不是原始像素。具体来说，我们将图像 编码为 ，其中词表 包含离散 token 的索引。\",\"我们采用 离散变分自编码器_dVAE 训练得到的图像 tokenizer。视觉 token 的学习包含两个模块：tokenizer 和 解码器。tokenizer 根据视觉词汇表将图像像素 映射为离散 token ；解码器 根据视觉 token 重建原始图像 。其重建目标函数如下：\",\"由于潜在的视觉 token 是离散的，模型训练过程是不可微的，因此采用 Gumbel-Softmax 松弛方法 来优化模型参数。此外，在 dVAE 训练过程中对 加入了均匀先验。\",\"我们将每张图像 token 化为 的视觉 token 网格。请注意，对于一张图像，视觉 token 的数量与图像 patch 的数量相同。词表大小设置为 。\"]},\"233\":{\"h\":\"主干网络：图像 Transformer\",\"t\":[\"我们遵循 ViT 的方法，采用标准的 Transformer 作为主干网络，从而可以在网络结构上直接与已有方法进行对比。\",\"Transformer 的输入是一系列图像 patch，记为 。每个 patch 首先通过一个线性映射层转换为 patch 嵌入，表示为 ，其中 。我们在 patch 序列前添加一个特殊的 token [S]，并将标准的 1D 可学习位置编码 加到 patch 嵌入中。\",\"因此，输入向量为：\",\"这个向量序列随后被送入 Transformer，由 层 Transformer Block 逐层编码：\",\"最终输出为：\",\"其中， 表示第 个图像 patch 的最终编码表示。\"]},\"234\":{\"h\":\"BEiT 的预训练：掩码图像建模（Masked Image Modeling）\",\"t\":[\"我们提出了一种 掩码图像建模（Masked Image Modeling，MIM） 任务。该任务通过随机遮挡一部分图像 patch，然后预测这些被遮挡位置对应的视觉 token。\",\"图 1 展示了我们方法的整体流程: 给定一张输入图像 ，我们将其划分为 个图像 patch（记作 ），并将其编码为 个视觉 token（）。\",\"我们随机遮挡大约 40% 的图像 patch，遮挡位置记作集合 ，满足 。接着，我们用一个可学习的嵌入向量 替换这些被遮挡的 patch。\",\"我们构造损坏图像的 patch 序列：\",\"然后将其输入 层 Transformer 编码器中。最终输出的隐藏向量 被视为输入 patch 的编码表示。\",\"对于每一个被遮挡的位置 ，我们使用 softmax 分类器来预测对应的视觉 token，具体如下：\",\"其中， 是损坏后的图像输入，、 是分类器的参数， 为视觉 token 词表的大小。\",\"预训练目标是最大化在损坏图像条件下，预测正确视觉 token 的对数似然，即：\",\"最大化每个样本中被遮挡位置的预测准确性，使模型能够从上下文恢复出缺失的高层语义表示。\",\"其中， 表示训练语料库， 是随机选中的掩码位置集合， 表示按照掩码位置 所遮挡后的损坏图像。\",\"我们并不是简单地随机选择 patch 来作为掩码位置，而是采用了块状遮挡（blockwise masking）策略。其具体方法总结如下（详见算法 1）：每次遮挡图像中的一个 patch 区块。对于每个遮挡块，要求最小包含 16 个 patch，然后随机选择一个遮挡块的宽高比，重复上述步骤直到遮挡的 patch 数量达到总数的 40%，即 ，其中 是图像中 patch 的总数，0.4 是设定的遮挡比例。\",\"该 MIM 任务深受 掩码语言建模（Masked Language Modeling）的启发，MLM 是自然语言处理中最成功的预训练目标之一。此外，块状遮挡或 n-gram 掩码策略也在 BERT 类模型中广泛使用。\",\"然而，如果直接采用像素级自编码（pixel-level auto-encoding）的方式进行视觉预训练，即还原被遮挡 patch 的原始像素，会促使模型更关注短程依赖和高频细节。而 BEiT 通过预测离散视觉 token 的方式克服了这一问题，离散 token 能够对图像内容进行更高级别的抽象总结。\",\"论文在 3.3 节中的消融实验表明，所提出的方法在性能上显著优于像素级自编码方案。\"]},\"235\":{\"h\":\"从变分自编码器的视角\",\"t\":[\"BEIT 的预训练可以看作是变分自编码器（Variational Autoencoder，VAE）的训练。设 表示原始图像， 表示被遮挡（masked）的图像， 表示视觉令牌（visual tokens）。考虑对数似然 的证据下界（ELBO），即从被破坏的图像恢复原始图像：\",\"其中：\",\" 表示图像编码器（image tokenizer），用于获取视觉令牌；\",\" 表示解码器（decoder），根据输入的视觉令牌重建原始图像；\",\" 根据遮挡图像恢复视觉令牌，这就是我们的掩码图像建模（Masked Image Modeling, MIM）预训练任务。\",\"我们采用与 VQ-VAE 类似的两阶段训练流程。第一阶段，我们获得图像编码器作为离散变分自编码器（discrete VAE）。具体地，第一阶段通过最小化重建损失\",\"并假设先验为均匀分布，实现方程 (2) 中的目标。\",\"第一阶段只训练一个“离散 VAE”（也就是 VQ-VAE），不管掩码图像，也不训练 BEIT 主体，只关注如何把图像变成 token，再重建图像。\",\"在标准 VAE 中，latent 变量 是有一个先验分布 的（通常是高斯），用于计算 KL 散度项：\",\"但在 BEIT 的第一阶段中，我们使用 VQ-VAE 的思路，它的 latent 是 离散的、来自 codebook 的。我们通常令先验为 uniform，即假设 codebook 中的每个 token 等概率地可能出现。 这会导致 KL 散度为常数（或不参与优化），所以训练中我们就可以只最小化重建损失。\",\"第二阶段，在保持 和 不变的情况下，学习先验分布 。我们将 简化为单点分布，即取视觉令牌的最大概率值\",\"此时，方程 (2) 可以重写为：\",\"其中第一项为阶段一：视觉令牌重建（Visual Token Reconstruction），第二项即为我们的 BEIT 预训练目标：掩码图像建模（Masked Image Modeling）。\"]},\"236\":{\"h\":\"预训练设置（Pre-Training Setup）\",\"t\":[\"为了公平比较，BEIT 的网络结构遵循 ViT-Base 的配置。我们使用一个包含 12 层的 Transformer，隐藏层维度为 768，注意力头数为 12，前馈网络的中间层维度为 3072。输入图像被切分为默认的 16×16 大小的 patch。\",\"我们直接使用训练好的图像 tokenizer，其视觉 token 的词表大小为 8192。\",\"BEIT 在 ImageNet-1K 的训练集上进行预训练，该数据集包含约 120 万张图像。增强策略包括随机缩放裁剪、水平翻转和颜色抖动。注意，我们在自监督学习中不使用图像标签。\",\"实验中我们使用 224 × 224 的图像分辨率输入，因此图像会被划分为 14 × 14 个 patch，对应相同数量的视觉 token。在预训练过程中，我们最多随机遮挡 75 个 patch（即约占图像 patch 总数的 40%）。\",\"预训练共进行约 50 万步（约 800 个 epoch），使用 2000 的 batch size。优化器采用 Adam，超参数为 β₁ = 0.9、β₂ = 0.999。学习率设为 1.5e-3，前 10 个 epoch 进行 warmup，并采用 cosine 退火进行学习率衰减。权重衰减系数为 0.05。我们使用 stochastic depth，比例为 0.1，禁用了 dropout。\",\"整个 50 万步训练过程在 16 张 Nvidia Tesla V100（32GB 显存）GPU 上运行，约耗时五天。\",\"我们发现恰当的初始化对于 Transformer 的稳定训练非常关键，尤其是在大规模预训练时。我们首先将所有参数在一个较小范围内随机初始化，比如 [−0.02, 0.02]。随后，对于第 层 Transformer，我们将自注意力模块和前馈网络中子层最后的线性投影矩阵的输出按 进行缩放（rescale）。\"]},\"237\":{\"h\":\"在下游视觉任务上微调 BEIT\",\"t\":[\"在完成 BEIT 的预训练后，我们在 Transformer 结构的基础上添加一个任务层，并在下游任务上微调所有参数，这一过程类似于 BERT 的微调方式。我们在本文中以图像分类和语义分割为例。实际上，将这种“预训练再微调”（pre-training-then-fine-tuning）范式应用于其他视觉任务也非常直接。\"]},\"238\":{\"h\":\"图像分类（Image classification）\",\"t\":[\"对于图像分类任务，我们直接使用一个简单的线性分类器作为任务层。具体而言：\",\"使用 平均池化（average pooling） 来汇聚所有 patch 的表示；\",\"然后将全局表示送入 softmax 分类器。\",\"类别概率的计算方式如下：\",\"其中：\",\" 表示第 个图像 patch 在最后一层的表示向量；\",\" 是分类器的权重矩阵；\",\" 是类别数。\",\"我们通过最大化带标签数据的似然函数，联合优化 BEIT 模型和 softmax 分类器的参数。\"]},\"239\":{\"h\":\"语义分割（Semantic segmentation）\",\"t\":[\"对于语义分割任务，我们采用 SETR-PUP 中使用的任务层设计。具体地：\",\"使用预训练的 BEIT 作为 backbone 编码器；\",\"加入多个 反卷积层（deconvolution layers） 构成解码器，用于输出分割结果。\",\"该模型同样采用端到端微调的方式，类似于图像分类的训练流程。\"]},\"240\":{\"h\":\"中间微调（Intermediate fine-tuning）\",\"t\":[\"在完成自监督预训练后，我们可以在一个数据量更丰富的中间数据集（如 ImageNet-1K）上进一步训练 BEIT，然后再将模型迁移到具体的下游任务上进行微调。\",\"这种“中间微调”策略在 NLP 中已经成为常规做法，我们在 BEIT 中也直接采用了类似的方法。\"]},\"241\":{\"h\":\"实验\"},\"242\":{\"h\":\"消融实验（Ablation Studies）\",\"t\":[\"我们分析了 BEIT 各组成部分的重要性。所有模型都在 ImageNet（分类）和 ADE20K（分割）任务上评估，预训练步数为 300 epoch（约为主实验 800 epoch 的 37.5%）。\",\"表 4 展示了不同变体的实验结果：\",\"去除 blockwise masking：即随机选择 mask 的 patch，而非以 block 为单位遮挡。发现 blockwise masking 尤其对语义分割帮助很大。\",\"去除视觉 token，改为像素重建：预训练目标变成恢复原始像素，效果明显变差，甚至不如从零训练的 Transformer。\",\"同时去除视觉 token 和 blockwise masking：进一步加剧性能下降。\",\"预测所有 token（即不遮挡）：性能也有所下降，说明遮挡带来的表示学习能力是关键。\",\"延长预训练时间（800 epoch）：显著提升下游任务性能。\",\"结论：视觉 token 与遮挡机制是 BEIT 有效的关键组成。\"]},\"243\":{\"h\":\"自注意力图的分析\",\"t\":[\"我们发现 BEIT 的自注意力机制可以自动学会区分图像中的语义区域，尽管预训练时没有用任何人工标注。\",\"我们使用 COCO 图像做可视化（避免出现在预训练数据中），展示了不同 reference patch 的注意力图。具体方法是：\",\"取最后一层 self-attention 的 query-key 乘积；\",\"对某个 patch 为 query，显示它关注哪些其他 patch。\",\"结果表明，BEIT 的 attention head 能够自动关注对象边界或同类区域，这种内在学习能力可能是 BEIT 能在下游任务上泛化得更好的原因之一，特别是在小样本数据集上。\"]},\"244\":{\"h\":\"相关工作\"},\"245\":{\"h\":\"自监督视觉预训练\",\"t\":[\"自监督视觉预训练方法可以大致分为三类：\"]},\"246\":{\"h\":\"1. 对比学习（Contrastive Learning）\",\"t\":[\"对比学习通过拉近相似图像（如不同数据增强后的同一图像）之间的距离，并拉远其他图像。典型代表包括：\",\"MoCo、SimCLR、SwAV、BYOL、Barlow Twins；\",\"近期的方法如 DINO、MoCo v3 也可用于 Transformer；\",\"iBOT 综合了 MIM 和对比学习。\",\"这些方法需要构造正负样本对，并依赖于数据增强的设计。\"]},\"247\":{\"h\":\"2. 生成式预训练（Generative Pretraining）\",\"t\":[\"这类方法试图从原始图像中恢复遮挡部分：\",\"iGPT 使用像素序列建模；\",\"ViT 使用 patch 分类作为监督任务；\",\"GANs（如 BigGAN）可用于图像生成，但不适合表征学习。\",\"像素恢复任务通常难以训练，且容易聚焦低级细节而非语义结构。\"]},\"248\":{\"h\":\"3. 目标预训练（Pretext Task）\",\"t\":[\"设计特定任务作为学习信号：\",\"旋转预测；\",\"图像 jigsaw 拼图；\",\"DAE、MAE 等自动编码器结构；\",\"这些方法训练稳定，但有效性有限。\",\"BEIT 提供了新方向：将图像预训练目标设计为 “语义 token 预测”，而非像素或对比目标。\"]},\"249\":{\"h\":\"离散表示学习（Discrete Representation Learning）\",\"t\":[\"使用离散表示（token）可以使模型从输入中学习更抽象的语义信息。关键代表包括：\",\"VQ-VAE 提出了 vector quantization 机制，将连续表示映射为离散 codebook；\",\"VQ-VAE-2 和 dVAE 进一步改进重建质量与稳定性；\",\"这些方法常用于图像生成（PixelCNN/VQ-GAN）和音频建模；\",\"BEIT 借助 dVAE 提供的视觉 token，将离散化思想引入图像预训练任务。\"]},\"250\":{\"h\":\"BERT 式的预训练方法（BERT-style Pretraining）\",\"t\":[\"BERT 是 NLP 中最成功的预训练方法之一，基于 “mask 掉 token 并预测它” 的思想，启发了 BEIT 的设计。\",\"BEIT 类似于图像领域的 “BERT”，输入为 patch，输出为被 mask 掉的 token；\",\"iGPT 也借鉴了 BERT 思路，但直接在像素上建模，学习成本高；\",\"MAE 是 BEIT 的后续工作之一，直接预测图像 patch 的像素；\",\"BEIT 是第一个真正从 token 层面借用 BERT 的图像模型。\"]},\"251\":{\"h\":\"多模态预训练（Multimodal Pretraining）\",\"t\":[\"在图像与文本结合任务中，使用 token 化的表示也非常常见：\",\"DALL·E、CLIP 将图像编码为离散 token；\",\"ViLT、UNITER 等通过共享 Transformer 编码图文对；\",\"图像 tokenizer（如 dVAE、VQ-GAN）是许多多模态生成模型的关键组成；\",\"BEIT 借鉴这种 token 化方法，为纯视觉模型设计了自监督 token 预测任务。\"]},\"252\":{\"h\":\"结论\",\"t\":[\"我们提出了 BEIT（Bidirectional Encoder representation from Image Transformers），一种基于图像 Transformer 的自监督预训练方法。受 BERT 在自然语言处理中的成功启发，BEIT 通过遮挡图像 patch 并预测相应的视觉 token，将 BERT 式的 Masked Language Modeling 思想成功引入视觉领域。\",\"我们的工作首次提出以 图像视觉 token 为预测目标，训练一个 BERT 风格的视觉 Transformer，从而实现强大的图像表征学习能力。在多个视觉任务中，如图像分类与语义分割，BEIT 展示出卓越的性能，超越了基于监督学习或对比学习的主流方法。\",\"此外，我们也展示了通过联合 dVAE、合适的 masking 策略与稳定的训练流程，可以使大规模图像预训练具备更强的可扩展性，为更大模型（如 BEIT-L）提供了稳定基础。\",\"我们相信，本研究为 统一视觉预训练框架 打下了基础，也为未来探索更强大的视觉理解与生成模型提供了方向。\"]},\"253\":{\"h\":\"BEiT 模型代码解读\",\"t\":[\"BEiT 模型代码解读\",\"论文解读: BEiT 论文解读\"]},\"254\":{\"h\":\"dVAE 预训练\",\"t\":[\"BEiT 模型的代码实现中定义了 两种变分自编码器（VAE），分别对应：\",\"DiscreteVAE：自研的 VQ-VAE 实现，基于 lucidrains/DALLE-pytorch 改写。\",\"Dalle_VAE：封装了 OpenAI 提供的预训练好的 DALL·E 编码器和解码器，直接加载 encoder.pkl 和 decoder.pkl。\"]},\"255\":{\"h\":\"DiscreteVAE 初始化\",\"t\":[\"本节我们会针对 DiscreteVAE 的代码实现展开讲解 ，首先是 DiscreteVAE 类的 __init__ 方法：\",\"class DiscreteVAE(BasicVAE): def __init__( self, image_size = 256, # 输入图像尺寸，假设为正方形，如 256x256 num_tokens = 512, # codebook 中 token 的种类数（即离散编码类别数） codebook_dim = 512, # 每个 token 的嵌入维度 num_layers = 3, # 编码器/解码器的下采样/上采样层数 hidden_dim = 64, # 每层卷积的通道数 channels = 3, # 输入图像通道数，通常为3（RGB图像） smooth_l1_loss = False, # 是否使用 smooth_l1 作为重建损失（否则用 mse） temperature = 0.9, # Gumbel Softmax 的温度参数 straight_through = False, # 是否启用 straight-through 近似采样（硬采样但梯度可导） kl_div_loss_weight = 0. # KL 散度项的损失权重（用于保持 token 使用的均匀性） ): # 保存超参数 self.image_size = image_size self.num_tokens = num_tokens self.num_layers = num_layers self.temperature = temperature self.straight_through = straight_through # 创建 codebook：token 编号 → 向量（[num_tokens, codebook_dim]） self.codebook = nn.Embedding(num_tokens, codebook_dim) enc_layers = [] # 编码器层列表 dec_layers = [] # 解码器层列表 enc_in = channels # 编码器初始输入通道（RGB图像为3） dec_in = codebook_dim # 解码器初始输入通道（token 嵌入维度） # 构建多层编码器和解码器（对称结构） for layer_id in range(num_layers): # 编码器：4x4卷积下采样 + ReLU enc_layers.append( nn.Sequential( nn.Conv2d(enc_in, hidden_dim, kernel_size=4, stride=2, padding=1), nn.ReLU() ) ) # 编码器残差块（增强特征提取） enc_layers.append( ResBlock( chan_in=hidden_dim, hidden_size=hidden_dim, chan_out=hidden_dim ) ) enc_in = hidden_dim # 下次输入通道设为隐藏通道 # 解码器：反卷积上采样 + ReLU dec_layers.append( nn.Sequential( nn.ConvTranspose2d(dec_in, hidden_dim, kernel_size=4, stride=2, padding=1), nn.ReLU() ) ) # 解码器残差块 dec_layers.append( ResBlock( chan_in=hidden_dim, hidden_size=hidden_dim, chan_out=hidden_dim ) ) dec_in = hidden_dim # 下次输入通道设为隐藏通道 # 编码器最终输出层：将通道映射到 num_tokens，得到 token 分类 logits enc_layers.append(nn.Conv2d(hidden_dim, num_tokens, kernel_size=1)) # 解码器最终输出层：映射回原图像通道数（通常为 3） dec_layers.append(nn.Conv2d(hidden_dim, channels, kernel_size=1)) # 将所有子模块组合为完整的 encoder 和 decoder 网络 self.encoder = nn.Sequential(*enc_layers) self.decoder = nn.Sequential(*dec_layers) # 重建损失函数：使用 Smooth L1 或 MSE self.loss_fn = F.smooth_l1_loss if smooth_l1_loss else F.mse_loss # KL 散度正则项的权重（默认 0，不启用） self.kl_div_loss_weight = kl_div_loss_weight\",\"残差块实现如下:\",\"class ResBlock(nn.Module): def __init__(self, chan_in, hidden_size, chan_out): super().__init__() self.net = nn.Sequential( nn.Conv2d(chan_in, hidden_size, 3, padding=1), nn.ReLU(), nn.Conv2d(hidden_size, hidden_size, 3, padding=1), nn.ReLU(), nn.Conv2d(hidden_size, chan_out, 1) ) def forward(self, x): return self.net(x) + x\"]},\"256\":{\"h\":\"DiscreteVAE 前向传播\",\"t\":[\"下面给出的是 DiscreteVAE 类的 forward 方法实现:\",\"def forward( self, img, # 输入图像 [B, C, H, W] return_loss = False, # 是否返回 loss return_recons = False, # 是否返回重建图像 return_logits = False, # 是否仅返回 logits temp = None # 覆盖默认温度参数 ): device = img.device num_tokens = self.num_tokens image_size = self.image_size kl_div_loss_weight = self.kl_div_loss_weight # 编码器前向传播，得到每个像素位置的 logits logits = self.encoder(img) # shape: [B, num_tokens, H', W'] # 若仅获取 logits（例如用于 DALL-E 中提取离散 token 索引），则直接返回 if return_logits: return logits # 使用 Gumbel Softmax 对 logits 进行采样，得到 soft one-hot 编码 temp = temp if temp is not None else self.temperature soft_one_hot = F.gumbel_softmax(logits, tau=temp, dim=1, hard=self.straight_through) # shape: [B, num_tokens, H', W'] # 查找 codebook 向量，获得连续隐变量 # einsum 相当于 soft_one_hot @ codebook.weight sampled = einsum('b n h w, n d -> b d h w', soft_one_hot, self.codebook.weight) # shape: [B, codebook_dim, H', W'] # 解码器重建图像 out = self.decoder(sampled) # shape: [B, C, H, W] # 如果不需要 loss，只返回重建图像 if not return_loss: return out # 计算重建损失 recon_loss = self.loss_fn(img, out) # KL 散度损失（可选，衡量 q(y) 与 uniform 的差异） logits = rearrange(logits, 'b n h w -> b (h w) n') # 展平空间维度 qy = F.softmax(logits, dim=-1) # 每个位置的 token 概率分布 log_qy = torch.log(qy + 1e-10) # 避免 log(0) log_uniform = torch.log(torch.tensor([1. / num_tokens], device=device)) kl_div = F.kl_div(log_uniform, log_qy, None, None, reduction='batchmean', log_target=True) # 加权总损失 loss = recon_loss + (kl_div * kl_div_loss_weight) # 如果不需要重建图像，仅返回 loss if not return_recons: return loss # 否则返回 loss 和重建图像 return loss, out\"]},\"257\":{\"h\":\"Gumbel Softmax\",\"t\":[\"传统的离散变量采样（比如从多分类分布中采样一个类别）是非可微的，不能直接用反向传播训练神经网络。Gumbel Softmax（也叫 Concrete distribution）是一种连续的可微近似方法，允许在训练时对离散随机变量进行“软采样”，实现端到端的梯度传播。 训练时用“软采样”表示类别概率的加权和；推理时可以用硬采样（one-hot）恢复离散的类别。\",\"Gumbel Softmax的采样过程分为两步：\",\"对每个类别的 logits（未归一化的对数概率）加上 Gumbel 噪声（用来模拟采样的随机性）\",\"对加噪声后的 logits 使用 softmax，并用温度参数控制“分布的平滑度”\",\"数学表达式：\",\"其中：\",\" 是第 类的概率（或 logits 经过 softmax 的概率）\",\" 是从 Gumbel(0,1) 分布采样的噪声，定义为 ，其中 \",\" 是温度参数，控制分布的“尖锐度”。温度越低，采样越接近 one-hot；温度高时更平滑。\",\"PyTorch 提供了 F.gumbel_softmax 函数来实现上述过程：\",\"soft_one_hot = F.gumbel_softmax(logits, tau=temp, dim=1, hard=self.straight_through)\",\"logits：输入的未归一化的 logits 张量。\",\"tau：温度参数 temp，控制分布的平滑度。\",\"dim=1：在类别维度上执行 softmax。\",\"hard：布尔值，是否使用硬采样 + 直通梯度。\",\"hard=False 返回的是软概率分布（连续值，可微）。\",\"hard=True 返回 one-hot 编码，但梯度仍由软样本近似（Straight-Through Estimator）。\",\"Gumbel Softmax 使模型在训练时可以对这些 logits 进行采样，得到一个“软”的 one-hot 向量，代表隐空间的离散编码。这个 soft one-hot 向量乘以 codebook 的 embedding 权重，得到连续的隐向量表示，用于解码器重建图像。\",\"当 hard=True 时，可以模拟硬采样（one-hot向量），方便离散索引的推断，同时仍保证梯度流通。\"]},\"258\":{\"h\":\"hard=True时，如何实现的？\",\"t\":[\"当 hard=self.straight_through 为 True 时，Gumbel-Softmax 采样过程使用了一种称为 Straight-Through Gumbel-Softmax 的技巧，它使得：\",\"前向传播时是one-hot 向量（离散），\",\"反向传播时仍保持连续可导（通过 softmax）\",\"在 VAE 或 BEiT 的离散编码器中，我们希望：\",\"对图像进行离散 token 编码（便于 Transformer 训练）\",\"但同时又希望这个采样过程能反向传播梯度\",\"这就引出了 Straight-Through Gumbel-Softmax：\",\"采样过程详解:\",\"soft_one_hot = F.gumbel_softmax(logits, tau=temp, dim=1, hard=True)\",\"等价于：\",\"y_soft = softmax((logits + GumbelNoise) / tau) # 连续分布，用于反向传播 index = argmax(y_soft) # 找到最大概率的 one-hot 索引 y_hard = one_hot(index) # 得到一个离散的 one-hot 向量 # 关键一步：straight-through trick y = y_hard.detach() - y_soft.detach() + y_soft\",\"y = y_hard.detach() - y_soft.detach() + y_soft\",\"y_hard.detach()：将 one-hot 向量从计算图中分离出来（不可导）\",\"y_soft.detach()：也分离出来，表示不在反向传播中参与梯度计算\",\"+ y_soft：把 soft 向量加入回来，用于反向传播\",\"➡️ 整体效果：\",\"方向\",\"数据流\",\"梯度流\",\"forward\",\"使用 one-hot 离散 token\",\"——\",\"backward\",\"使用 softmax 的连续梯度\",\"保持可导，稳定训练\"]},\"259\":{\"h\":\"smooth_l1_loss\",\"t\":[\"smooth_l1_loss 是 PyTorch 中的一种 回归损失函数，也被称为 Huber Loss 的一种变体，它结合了均方误差（MSE）和平均绝对误差（MAE）的优点，在处理 异常值/离群点鲁棒性更强。\",\"对于预测值 、目标值 ，以及误差 ：\",\"这个函数在误差较小时近似于 MSELoss，误差较大时退化为 L1Loss，从而避免了 MSE 对异常值特别敏感的问题。\",\"相比 MSE，使用 smooth_l1_loss 能让 VAE：\",\"对单个像素偏差较大的情况更加宽容（防止训练不稳定）\",\"更容易在训练中收敛，因为梯度变化更平滑\",\"smooth_l1_loss 是一个融合了 MSE 的平滑性与 L1 的鲁棒性的损失函数，常用于图像回归与重建任务中，能更好处理异常误差。\"]},\"260\":{\"h\":\"KL散度计算\",\"t\":[\" # KL 散度损失（可选，衡量 q(y) 与 uniform 的差异） logits = rearrange(logits, 'b n h w -> b (h w) n') # 展平空间维度: [B, num_tokens, H, W] 变为 [B, H*W, num_tokens] qy = F.softmax(logits, dim=-1) # 每个位置的 token 概率分布 log_qy = torch.log(qy + 1e-10) # 避免 log(0) log_uniform = torch.log(torch.tensor([1. / num_tokens], device=device)) kl_div = F.kl_div(log_uniform, log_qy, None, None, reduction='batchmean', log_target=True)\",\"在这段代码中，我们计算的是编码器输出分布 与一个先验分布 之间的 Kullback-Leibler 散度，记作：\",\"其中：\",\"：先验分布（理想中我们希望 encoder 生成的分布接近它）\",\"：encoder 对图像每个 patch 给出的 softmax 分布\",\"：codebook 中的 token 数，即 num_tokens\",\"在本代码中：\",\"，即是均匀分布\",\"所以 \",\"带入公式得到：\",\"也就是：\",\"这个损失鼓励 q 趋近于均匀，从而避免编码器只用很少几个 token。\"]},\"261\":{\"h\":\"log_target 参数\",\"t\":[\"F.kl_div(input, target, log_target=False)\",\"此时，input 是 log 概率，target 是概率，计算公式为：\",\"即：\",\"F.kl_div(log_q, p, log_target=False)\",\"F.kl_div(log_p, log_q, log_target=True)\",\"此时认为 两个参数都是 log 概率，底层计算公式变为：\",\"也就是 PyTorch 自动执行：\",\"KL = (p * (log_p - log_q)).sum()\",\"其中：\",\"⚠️ 注意：这种方式需要我们手动把两个分布都以 log 形式传进去。\"]},\"262\":{\"h\":\"为什么先验分布设置为均匀分布？\",\"t\":[\"这是为了满足 信息瓶颈 或 高效利用 codebook 的目标：\",\"VQ-VAE 的典型问题：code collapse\",\"编码器如果训练不当，可能会只偏好极少数几个 code（比如 512 个 code 中只用 10 个），这是 codebook collapse。\",\"结果就是：虽然理论上有 512 种可能的图像 patch 表达，但实际只用了极少数，模型表达能力受限。\",\"使用均匀先验的好处\",\"均匀分布意味着我们希望所有 token 被“平等地使用”。\",\"加上 KL 散度约束后，编码器会被正则化为“尽可能平均地使用每个 token”。\",\"这样可以提高 codebook 的使用率，提升模型的表达多样性。\",\"总结为一句话：\",\"使用均匀先验是为了鼓励编码器生成的离散 token 分布更加均衡，避免 code collapse，从而充分利用整个 codebook 的表示能力。\"]},\"263\":{\"h\":\"块状遮挡（blockwise masking）策略\",\"t\":[\"块状遮挡通过遮盖图像中的连续 patch 区域，更真实地模拟自然场景中的遮挡，增强模型对上下文的理解能力，避免信息泄漏，同时实现简单且与基于patch的模型结构高度契合，因此比像素级别遮挡更有效和实用。\",\"遮挡方式：\",\"将图像划分为 个 patch\",\"每次遮挡一个矩形块区域，如 4×4 或 6×3 的 patch 区域\",\"最终总共遮掉 num_masking_patches 个 patch\",\"相比于逐 patch 独立遮挡（如 random token masking），这种遮挡方式：\",\"遮挡方式\",\"特点\",\"对比优势\",\"随机单 patch 遮挡\",\"每个 patch 独立被遮或不遮\",\"遮挡区域零碎\",\"✅ 块状遮挡\",\"遮一片连续矩形区域\",\"更符合图像结构、语义连续\",\"块状掩码策略的具体实现代码如下所示，首先给出的是掩码生成器的初始化方法，重点注意各个参数的含义:\",\"class MaskingGenerator: def __init__( self, input_size, # 输入图像的 patch 网格大小（如 14 表示 14x14 patch） num_masking_patches, # 最终要 mask 掉的 patch 总数量 min_num_patches=4, # 每次生成一个遮挡块时，最小 patch 数 max_num_patches=None, # 每次遮挡块最多的 patch 数；默认等于 num_masking_patches min_aspect=0.3, # 遮挡块的最小宽高比（例如 h/w = 0.3） max_aspect=None): # 最大宽高比，默认取 1 / min_aspect（对称处理） # 如果输入是整数，则构造成正方形大小的 patch 网格 if not isinstance(input_size, tuple): input_size = (input_size, ) * 2 self.height, self.width = input_size # patch 网格的高和宽（例如 14x14） self.num_patches = self.height * self.width # 总共可用 patch 数量 self.num_masking_patches = num_masking_patches # 需要被 mask 的 patch 总数 self.min_num_patches = min_num_patches # 单个遮挡块的最小 patch 数 # 若 max_num_patches 未指定，则设为总遮挡目标数（不限制） self.max_num_patches = num_masking_patches if max_num_patches is None else max_num_patches # 如果未指定最大宽高比，默认与 min_aspect 互为倒数，保持对称性 max_aspect = max_aspect or 1 / min_aspect # 记录宽高比范围的对数形式，便于采样（log 均匀采样 → 平滑控制长宽比例分布） self.log_aspect_ratio = (math.log(min_aspect), math.log(max_aspect))\",\"下面将展示正式执行块状遮挡策略前的准备工作:\",\"def __call__(self): # 初始化一个全零的遮挡掩码，大小为输入图像的patch数目（height x width） mask = np.zeros(shape=self.get_shape(), dtype=np.int) mask_count = 0 # 当前已经遮挡的patch数量 # 循环直到遮挡的patch数量达到指定的遮挡总数 while mask_count < self.num_masking_patches: # 计算本轮最多还能遮挡的patch数 max_mask_patches = self.num_masking_patches - mask_count # 限制本次遮挡的patch数量不超过最大遮挡数 max_mask_patches = min(max_mask_patches, self.max_num_patches) # 尝试生成一个遮挡块，返回本次新增遮挡的patch数量 delta = self._mask(mask, max_mask_patches) if delta == 0: # 如果没有新增遮挡（即无法再生成有效遮挡块），跳出循环 break else: # 更新已遮挡patch数量 mask_count += delta # 返回最终生成的遮挡掩码（0表示未遮挡，1表示遮挡） return mask\",\"执行块状遮挡策略的核心代码实现如下:\",\"def _mask(self, mask, max_mask_patches): delta = 0 # 记录本次新增遮挡的patch数量 for attempt in range(10): # 最多尝试10次生成遮挡块 # 随机采样目标遮挡面积（patch数量），范围在[min_num_patches, max_mask_patches]之间 # random.uniform 是均匀采样，即每一个值被采样到的可能性完全相等 target_area = random.uniform(self.min_num_patches, max_mask_patches) # 随机采样遮挡块的长宽比（在log空间均匀采样后exp还原） aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio)) # 根据面积和长宽比计算遮挡块的高度和宽度（向最近整数取整） # h * w ≈ target_area # h / w ≈ aspect_ratio # h^2 = target_area * aspect_ratio; w^2 = target_area / aspect_ratio h = int(round(math.sqrt(target_area * aspect_ratio))) w = int(round(math.sqrt(target_area / aspect_ratio))) # 检查遮挡块尺寸是否小于输入图像patch尺寸，确保遮挡块可放入图像范围内 if w < self.width and h < self.height: # 随机采样遮挡块在图像上的左上角位置，确保遮挡块不会越界 top = random.randint(0, self.height - h) left = random.randint(0, self.width - w) # 计算遮挡块区域内已被遮挡的patch数 num_masked = mask[top: top + h, left: left + w].sum() # 判断当前遮挡块的有效新增遮挡数量 # 必须新增遮挡patch数>0且不超过最大允许遮挡数 if 0 < h * w - num_masked <= max_mask_patches: # 遍历遮挡块区域，将未遮挡的patch设置为遮挡（1），累计新增遮挡数量 for i in range(top, top + h): for j in range(left, left + w): if mask[i, j] == 0: mask[i, j] = 1 delta += 1 # 如果本次成功新增了遮挡patch，跳出尝试循环 if delta > 0: break # 返回本次新增的遮挡patch数量 return delta\"]},\"264\":{\"h\":\"数据集加载\",\"t\":[\"数据集的加载，我们需要重点关注应用于图像之上的Transform操作，核心代码实现如下:\",\"class DataAugmentationForBEiT(object): def __init__(self, args): # 通用图像增强流程，包括颜色抖动、随机水平翻转和双图采样（生成两张不同视角图像） self.common_transform = transforms.Compose([ transforms.ColorJitter(0.4, 0.4, 0.4), # 随机调整亮度、对比度、饱和度 transforms.RandomHorizontalFlip(p=0.5), # 以 0.5 的概率水平翻转 RandomResizedCropAndInterpolationWithTwoPic( size=args.input_size, # 第一张图的目标尺寸 second_size=args.second_input_size, # 第二张图的目标尺寸 interpolation=args.train_interpolation, # 第一张图的插值方式 second_interpolation=args.second_interpolation, # 第二张图的插值方式 ), ]) # patch_transform：针对输入 encoder 的图像变换（标准 ToTensor 和 Normalize） self.patch_transform = transforms.Compose([ transforms.ToTensor(), # 将 PIL 图像转为 [0,1] 的张量 transforms.Normalize( mean=torch.tensor(mean), # 按照选定均值归一化 std=torch.tensor(std)) # 按照选定方差归一化 ]) # visual_token_transform：用于离散 VAE 编码器的图像变换 if args.discrete_vae_type == \\\"dall-e\\\": # 如果使用 DALL-E 风格的 tokenizer，需使用 map_pixels（将像素从 [0,1] 映射到 [-1,1]） self.visual_token_transform = transforms.Compose([ transforms.ToTensor(), map_pixels, # 特殊像素映射操作（DALL-E 需要） ]) elif args.discrete_vae_type == \\\"customized\\\": # 若是自定义 tokenizer，使用 Inception 风格归一化 self.visual_token_transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize( mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD, ), ]) else: raise NotImplementedError() # 不支持的 tokenizer 类型报错 # 初始化 MaskingGenerator，生成 block-wise 遮挡掩码 self.masked_position_generator = MaskingGenerator( args.window_size, # 图像分块尺寸（H, W） num_masking_patches=args.num_mask_patches, # 总共要 mask 掉的 patch 数量 max_num_patches=args.max_mask_patches_per_block, # 每个遮挡块最大 patch 数 min_num_patches=args.min_mask_patches_per_block, # 每个遮挡块最小 patch 数 ) def __call__(self, image): # 对输入图像做通用增强，返回两张不同视角图像 for_patches, for_visual_tokens = self.common_transform(image) # 分别对两张图做不同处理，生成 patch 图像、visual token 图像、遮挡掩码 return self.patch_transform(for_patches), self.visual_token_transform(for_visual_tokens), self.masked_position_generator() # 构建 BEiT 预训练用的数据集加载器 def build_beit_pretraining_dataset(args): transform = DataAugmentationForBEiT(args) # 实例化数据增强类 return ImageFolder(args.data_path, transform=transform) # 加载图像文件夹作为数据集，并应用 transform\",\"common_transform：共享的图像增强\",\"作用：对原始图像进行统一的图像增强（如颜色抖动、随机裁剪、水平翻转等），并生成两张不同的图像：\",\"一张用于喂给 Vision Transformer encoder（即 for_patches）\",\"一张用于传给视觉 tokenizer（即 for_visual_tokens）\",\"为什么要做两张图？\",\"是为了增强模型鲁棒性，但保持语义一致。BEiT 训练时，一边喂模型的是带遮挡的图像（encoder输入），一边是完整图像（tokenizer生成标签）。\",\"与 SimCLR、BYOL、DINO 等方法一样，图像增强是对比学习或自监督学习的基础。\",\"patch_transform：encoder 输入图像的标准化处理\",\"作用：将 for_patches 图像转换为 Tensor，并使用指定的均值和方差进行归一化处理，作为 Vision Transformer 的输入。\",\"为什么要归一化？\",\"Transformer 不像 CNN 有归一化操作，输入图像的数值尺度对训练稳定性和收敛非常重要。\",\"而 patch-wise transformer（如 ViT 或 BEiT）对输入的每个 patch 都很敏感，因此通常遵循 ImageNet 的 mean/std。\",\"visual_token_transform：VAE tokenizer 输入图像的标准化处理\",\"作用：处理 for_visual_tokens 图像，用于送入离散 VAE tokenizer，生成图像 patch 的 token 标签。\",\"如果使用 DALL·E 风格的 tokenizer，会使用 map_pixels 将图像从 [0, 1] 映射到 [−1, 1]。\",\"如果是自定义 tokenizer，使用 Inception 风格的均值方差进行归一化。\",\"为什么要特殊处理？\",\"视觉 tokenizer（如 DALL·E 的 VQ-VAE）对输入图像的数据分布非常敏感，训练和使用阶段都要一致。\",\"图像 token 是 BEiT 的训练目标（相当于 BERT 的词），其质量直接决定预训练效果。\"]},\"265\":{\"h\":\"BEiT主模型预训练\",\"t\":[\"下面给出的是BEiT主模型的预训练流程核心的代码实现:\",\"def main(args): # 1. 初始化 Vision Transformer 预训练模型 model = get_model(args) # 2. 构建 BEiT 预训练数据集（包括图像增强、遮挡、token 标签生成等） dataset_train = build_beit_pretraining_dataset(args) # 3. 加载离散 VAE 模型，用于将图像转换为离散的视觉 token（即 BEiT 的训练目标） d_vae = utils.create_d_vae( weight_path=args.discrete_vae_weight_path, # 离散VAE的权重路径 d_vae_type=args.discrete_vae_type, # 使用哪种类型的VAE（如\\\"dall-e\\\"或\\\"customized\\\"） device=device, # 使用的设备（GPU/CPU） image_size=args.second_input_size # 输入到VAE中的图像尺寸（通常小于主图像） ) # 4. 构建训练数据加载器 data_loader_train = torch.utils.data.DataLoader( dataset_train, sampler=sampler_train, # 用于分布式训练的采样器 batch_size=args.batch_size, # 每批次的图像数量 drop_last=True, # 如果最后一个batch数量不足，丢弃它 ) # 5. 训练过程：循环多个epoch for epoch in range(args.start_epoch, args.epochs): train_stats = train_one_epoch( model, # Vision Transformer 模型 d_vae, # 离散VAE，用于生成训练标签 data_loader_train, # 训练数据加载器 optimizer, # 优化器 device, # 设备 epoch, # 当前epoch数 loss_scaler, # 用于混合精度训练的loss scaler（如GradScaler） args.clip_grad, # 是否裁剪梯度 log_writer=log_writer, # 日志记录工具（如TensorBoard） start_steps=epoch * num_training_steps_per_epoch, # 当前epoch已训练的步数 lr_schedule_values=lr_schedule_values, # 当前epoch对应的学习率表 wd_schedule_values=wd_schedule_values, # 当前epoch对应的权重衰减表 )\",\"train_one_epoch 函数完成了 BEiT 预训练过程中的一个 epoch，包括以下步骤：\",\"生成视觉 token 作为标签（用 dVAE 编码器）；\",\"将图像送入模型并仅预测被 mask 的位置；\",\"使用交叉熵计算预测 token 和目标 token 的误差；\",\"反向传播并更新参数；\",\"def train_one_epoch(model: torch.nn.Module, d_vae: torch.nn.Module, data_loader: Iterable, optimizer: torch.optim.Optimizer, device: torch.device, epoch: int, loss_scaler, max_norm: float = 0, log_writer=None, lr_scheduler=None, start_steps=None, lr_schedule_values=None, wd_schedule_values=None): \\\"\\\"\\\" 执行模型的一个训练周期，包括前向传播、loss 计算、反向传播、参数更新。 \\\"\\\"\\\" for step, (batch, _) in enumerate(metric_logger.log_every(data_loader, print_freq, header)): # 每个 batch 包含： # - samples: 经过 patch_transform 的图像输入（供 Transformer 使用） # - images: 供离散 VAE 使用的图像输入（通常是不同尺寸的原图） # - bool_masked_pos: 遮挡掩码，表示哪些 patch 被 mask 掉了 samples, images, bool_masked_pos = batch with torch.no_grad(): # 使用 dVAE 将图像转为视觉 token（codebook 中的索引） # shape: [B, H*W] input_ids = d_vae.get_codebook_indices(images).flatten(1) # 展平掩码为 1D 向量，并转为布尔类型 bool_masked_pos = bool_masked_pos.flatten(1).to(torch.bool) # 提取被遮挡位置上的目标 token，作为训练标签 labels = input_ids[bool_masked_pos] with torch.cuda.amp.autocast(): # 前向传播： # - 输入为 samples（图像 patch） # - bool_masked_pos 指定模型只预测被遮挡的位置 # - return_all_tokens=False 表示只输出被 mask 的位置的预测结果 outputs = model(samples, bool_masked_pos=bool_masked_pos, return_all_tokens=False) # 计算交叉熵损失（只针对被 mask 的位置进行预测） loss = nn.CrossEntropyLoss()(input=outputs, target=labels) # 获取损失的标量值 loss_value = loss.item() # 如果 loss 出现 NaN 或 Inf，终止训练 if not math.isfinite(loss_value): print(\\\"Loss is {}, stopping training\\\".format(loss_value)) sys.exit(1) # 梯度清零 optimizer.zero_grad()\"]},\"266\":{\"h\":\"主模型代码实现\",\"t\":[\"VisionTransformerForMaskedImageModeling 的前向传播方法通过将输入图像切分为 patch 并编码为 token 表示，使用 [MASK] token 替换被遮挡的部分，然后加入位置编码并送入 Transformer 编码器提取上下文特征，最终通过分类头 lm_head 预测被遮挡位置的视觉 token，从而实现图像自监督预训练任务中的 masked image modeling。\",\"class VisionTransformerForMaskedImageModeling(nn.Module): def forward_features(self, x, bool_masked_pos): \\\"\\\"\\\" 前向特征提取： - 将图像转换为 patch embeddings - 用 [MASK] token 替换被遮挡的位置 - 加入 positional embedding - 送入 Transformer block 提取上下文特征 \\\"\\\"\\\" # 将输入图像 x 拆分为 patch，并投影为 token 表示（包括位置 mask） x = self.patch_embed(x, bool_masked_pos=bool_masked_pos) batch_size, seq_len, _ = x.size() # [B, N, D] # 准备 cls_token（每个样本一个） cls_tokens = self.cls_token.expand(batch_size, -1, -1) # [B, 1, D] # 准备 mask_token（用于替换被遮挡的位置） mask_token = self.mask_token.expand(batch_size, seq_len, -1) # [B, N, D] # 构造遮挡掩码 w（1 表示需要 mask 的位置），将被遮挡的 token 替换为 mask_token w = bool_masked_pos.unsqueeze(-1).type_as(mask_token) # [B, N, 1] x = x * (1 - w) + mask_token * w # 只在 mask 区域使用 mask_token，其它位置保留原 token # 拼接 cls_token 到序列最前面（ViT 默认行为） x = torch.cat((cls_tokens, x), dim=1) # [B, N+1, D] # 添加位置编码（如果存在） if self.pos_embed is not None: x = x + self.pos_embed # [B, N+1, D] # 应用 dropout（防止过拟合） x = self.pos_drop(x) # 计算相对位置偏置（如果定义了） rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None # 逐层通过 transformer block 处理 for blk in self.blocks: x = blk(x, rel_pos_bias=rel_pos_bias) # 最终归一化输出 return self.norm(x) def forward(self, x, bool_masked_pos, return_all_tokens=False): \\\"\\\"\\\" 前向主函数： - 获取 transformer 提取的特征 - 只保留 patch token（排除 cls_token） - 决定是否返回所有 token 的输出，或仅返回被 mask 的位置预测结果 \\\"\\\"\\\" # 获取 transformer 输出（包含 cls_token） x = self.forward_features(x, bool_masked_pos=bool_masked_pos) # [B, N+1, D] # 去掉 cls_token，只保留 patch token x = x[:, 1:] # [B, N, D] if return_all_tokens: # 返回所有 patch 的预测结果（例如：可视化使用） # self.lm_head = nn.Linear(embed_dim, vocab_size) return self.lm_head(x) # [B, N, vocab_size] else: # 仅返回被遮挡位置的预测结果，用于 loss 计算 return self.lm_head(x[bool_masked_pos]) # [num_masked_tokens, vocab_size]\",\"patch_embed 方法负责将图像裁剪为一系列固定大小的 patch，然后将每个 patch 映射为一个 embed_dim 维的向量，从而形成可供 Transformer 处理的 token 序列。\",\"class PatchEmbed(nn.Module): \\\"\\\"\\\" Image to Patch Embedding 将输入图像划分为 patch，并通过一个卷积操作将每个 patch 映射为一个 embedding 向量 \\\"\\\"\\\" def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768): super().__init__() # 将输入图像大小和 patch 大小转为 tuple 格式，例如 224 -> (224, 224) img_size = to_2tuple(img_size) patch_size = to_2tuple(patch_size) # 计算总 patch 数量 num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0]) # 保存 patch 的二维结构形状 (height, width)，例如 (14, 14) self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) self.img_size = img_size # 输入图像大小 self.patch_size = patch_size # 每个 patch 的大小 self.num_patches = num_patches # patch 的总数量 # 使用一个步长为 patch_size 的卷积操作来提取 patch 表示 # 输入维度为 in_chans（如3通道RGB），输出为 embed_dim（嵌入维度） self.proj = nn.Conv2d( in_chans, embed_dim, kernel_size=patch_size, stride=patch_size ) def forward(self, x, **kwargs): B, C, H, W = x.shape # 通过卷积将图像划分为 patch 并转换为 embedding 表示 # 输出形状: [B, embed_dim, H_patch, W_patch] x = self.proj(x) # 展平 patch 的空间维度为序列维度: [B, embed_dim, N_patches] -> [B, N_patches, embed_dim] x = x.flatten(2).transpose(1, 2) return x\"]},\"267\":{\"h\":\"CoCa 论文\",\"t\":[\"CoCa: Contrastive Captioners are Image-Text Foundation Models 论文简析\",\"论文链接: CoCa: Contrastive Captioners are Image-Text Foundation Models 代码链接: https://github.com/lucidrains/CoCa-pytorch\"]},\"268\":{\"h\":\"引言\",\"t\":[\"近年来，计算机视觉领域对大规模预训练基础模型的探索越来越重要，因为这些模型能够快速迁移到各种下游任务上。本论文提出了一种极简设计的 Contrastive Captioner (CoCa) 模型，它是一种图文编码-解码结构的基础模型，在训练时同时使用对比损失和生成式的描述损失。这样一来，它既能继承 CLIP 这类对比方法的能力，又能结合 SimVLM 这类生成方法的优点。\",\"与传统的 encoder-decoder 架构（解码器所有层都对编码器输出做 cross-attention）不同，CoCa 的解码器被一分为二：\",\"前半部分为单模态解码器（unimodal decoder），没有 cross-attention，只学习纯文本的表示；\",\"后半部分为多模态解码器（multimodal decoder），通过 cross-attention 融合图像和文本，得到跨模态的联合表示。\",\"在训练目标上，CoCa 同时引入：\",\"对比损失：用于图像编码器输出与文本单模态表示之间的对齐；\",\"描述损失（captioning loss）：作用在多模态解码器的输出上，要求其自回归地预测文本 token。\",\"这种共享计算图的方式，让两个目标可以在计算上高效结合，几乎没有额外开销。训练时，所有的标签（包括人工标注和网络噪声数据）都被统一当作文本，从而自然地融合了不同来源的监督信号。\",\"共享计算图: 一次forward完成两个损失目标值的计算。\",\"深度学习的发展，已经在语言领域涌现出 BERT、T5、GPT-3 等基础模型，它们通过大规模预训练展示出零样本、多任务、迁移学习的能力。相比专用模型，基础模型能在 amortized（摊销）成本上覆盖更多下游任务，推动规模化智能的发展。\",\"在视觉和视觉-语言任务中，已有三条研究路径：\",\"(1) 单编码器（Single-encoder）\",\"代表性工作在 ImageNet 等图像分类数据集上用交叉熵损失预训练。\",\"优点：提供通用的视觉特征，可迁移到图像和视频理解任务。\",\"缺点：仅依赖图像标注（类别标签），无法利用自然语言知识，因此在涉及图文结合的任务（如 VQA）上受限。\",\"(2) 双编码器（Dual-encoder，对比学习）\",\"通过图像编码器和文本编码器分别编码图像与文本，再用对比损失在共享的潜在空间对齐。\",\"优点：不仅能服务视觉任务，还能进行跨模态任务（如图文检索、零样本分类）。\",\"缺点：缺乏图文融合的联合表示，因此无法直接应用于复杂的多模态理解任务（如 VQA）。\",\"(3) 编码-解码（Encoder-decoder，生成式预训练）\",\"采用图像输入到编码器，解码器侧使用语言建模损失（LM loss 或 PrefixLM）进行训练。\",\"优点：能学到跨模态的联合表示，在多模态理解任务上表现突出。\",\"缺点：不能同时得到和图像对齐的纯文本表示，因此在跨模态对齐与检索方面不足。\",\"CoCa 融合并统一了以上三类范式，提出了一种改进的 encoder-decoder 架构：\",\"将解码器拆分为 单模态部分（仅学习文本特征）和 多模态部分（跨模态融合）。\",\"在单模态文本表示与图像表示之间施加对比目标，同时在多模态解码器输出上施加生成目标。\",\"在训练数据上，把所有标注（类别标签、自然描述、网络噪声文本）都视作文本，从而无缝整合了不同监督。\",\"这样一来，CoCa 的训练目标兼顾了：\",\"对比学习的优势（学习全局语义表征，适合检索和零样本分类）；\",\"生成学习的优势（对细粒度的区域特征建模，适合描述和理解任务）。\",\"CoCa 在多种任务上展现了强大的零样本和迁移能力：\",\"ImageNet 分类：\",\"零样本准确率 86.3%\",\"冻结编码器 + 学分类头：90.6%\",\"全模型微调：91.0%（SOTA）\",\"视频理解：\",\"Kinetics400/600/700：88.0% / 88.5% / 81.1%\",\"Moments-in-Time：47.4%\",\"跨模态检索：\",\"MSCOCO、Flickr30k：显著优于现有方法\",\"多模态理解：\",\"VQA：82.3%\",\"SNLI-VE、NLVR2：同样有优异表现\",\"图像描述生成：\",\"NoCaps：CIDEr 得分 120.6\",\"这些结果表明：一个统一的 CoCa 模型，能在无需大量任务特定微调的前提下，超越多个专用模型的性能。\"]},\"269\":{\"h\":\"相关工作\",\"t\":[\"视觉预训练\",\"早期的视觉模型大多依赖于在大规模标注数据（如 ImageNet、Instagram、JFT）上对卷积网络或 Transformer 进行预训练，从而解决分类、定位、分割、视频识别、跟踪等视觉识别任务。 近年来，自监督视觉预训练逐渐兴起：\",\"BEiT 借鉴 BERT 思路，提出了基于掩码图像建模的任务，并用量化后的视觉 token id 作为预测目标。\",\"MAE 和 SimMIM 移除了图像 tokenizer，直接使用轻量级解码器或投影层回归像素值。\",\"但这些方法的局限在于：它们只学习视觉模态模型，无法应用到需要图像与文本 联合推理 的任务。\",\"视觉-语言预训练（VLP）\",\"VLP 的目标是让模型能够在融合框架中联合建模视觉和语言。\",\"早期方法（LXMERT、UNITER、VinVL）依赖目标检测器（如 Faster R-CNN）提取图像特征。\",\"后续方法（ViLT、VLMo）则直接将视觉和语言 Transformer 统一起来，从零开始训练一个多模态 Transformer。\",\"图文基础模型\",\"最近的研究进一步提出了 图文基础模型，它们统一了视觉预训练和视觉-语言预训练：\",\"CLIP 和 ALIGN：利用噪声图文对数据，通过对比学习目标训练双编码器，学习到跨模态对齐能力，并能实现零样本图像分类。\",\"Florence：在 CLIP/ALIGN 的思路上提出统一的对比目标，并训练能适配于更广泛基准的基础模型。\",\"LiT 和 BASIC：先在大规模图像标注数据上用交叉熵训练，再在噪声图文对数据集上用对比损失微调，从而提升零样本图像分类性能。\",\"生成式方法（如 [16, 17, 34]）：采用编码器-解码器架构并引入生成式损失，在视觉-语言基准任务上表现优异，同时视觉编码器在图像分类上依然具备竞争力。\",\"与现有方法的对比\",\"本研究提出的 CoCa，专注于从零开始，在单一预训练阶段完成图文统一，从而避免多阶段训练（如 ALBEF 那样的先单模态、再多模态流程）。已有一些方法尝试过类似思路（如 ALBEF），但它们存在复杂的训练需求：\",\"计算开销：CoCa 在一批图文对上只需一次前向与反向传播，而 ALBEF 需要两次（一次使用被扰动输入，一次使用未扰动输入）。\",\"训练方式：CoCa 直接在两个目标函数下从零开始训练，而 ALBEF 依赖预训练的视觉和文本编码器，并需要额外的训练机制（如动量模块）。\",\"生成式优势：CoCa 的解码器架构结合生成式损失，更自然地支持图像描述任务，同时还能直接实现零样本学习。\"]},\"270\":{\"h\":\"方法\",\"t\":[\"我们首先回顾三类利用自然语言监督的基础模型家族：单编码器分类预训练、双编码器对比学习、编码器-解码器图像描述生成。随后介绍 Contrastive Captioners (CoCa)，它在一个简单的架构下融合了对比学习和图像到文本生成的优势。最后讨论 CoCa 模型如何通过零样本迁移或最小任务微调快速应用到下游任务。\"]},\"271\":{\"h\":\"自然语言监督\",\"t\":[\"单编码器分类（Single-Encoder Classification）\",\"经典的单编码器方法通过在大规模人工标注图像数据集（如 ImageNet、Instagram 或 JFT）上进行图像分类来预训练视觉编码器。注释文本的词汇通常是固定的，图像标注一般被映射为离散类别向量，并使用交叉熵损失训练：\",\"其中 是从真实标签 得到的 one-hot、多-hot 或平滑标签分布。训练完成后，学习到的图像编码器可作为通用视觉表示提取器，用于下游任务。\",\"双编码器对比学习（Dual-Encoder Contrastive Learning）\",\"相比单编码器分类需要人工标注和数据清理，双编码器方法利用大规模噪声文本描述，并引入可学习的文本编码器来编码自由形式文本。两个编码器通过对比目标联合优化：\",\"其中 和 分别是第 对图像和第 对文本的归一化嵌入， 是批量大小， 是温度参数。\",\"除了图像编码器，双编码器方法还学习了对齐的文本编码器，使得模型可以进行跨模态对齐应用，如图文检索和零样本图像分类。实证结果显示，这种零样本分类在受损或分布外图像上更加稳健。\",\"编码器-解码器图像描述生成（Encoder-Decoder Captioning）\",\"与双编码器整体编码文本不同，生成式方法（也称 captioner）追求更细粒度的表示，需要模型自回归地预测文本 的每个 token。遵循标准的编码器-解码器架构：\",\"图像编码器提供潜在编码特征（如使用 Vision Transformer 或卷积网络）。\",\"文本解码器通过最大化条件概率来学习配对文本的生成：\",\"编码器-解码器训练使用 teacher-forcing，以并行化计算并提高学习效率。\",\"与前述方法不同，captioner 不仅提供联合图文表示以用于视觉-语言理解任务，还可以直接应用于自然语言生成的图像描述任务。\",\"好的，我来帮你把 3.2 Contrastive Captioners Pretraining 这一节完整翻译成中文，保持通俗易懂，但同时尽量保留论文中的全部信息。\"]},\"272\":{\"h\":\"对比描述预训练（Contrastive Captioners Pretraining）\",\"t\":[\"整体架构（如图2所示）：我们提出的 CoCa（Contrastive Captioner）是一种简单的编码器-解码器方法，能够自然地融合三种训练范式。与标准的图文编码器-解码器模型类似，CoCa 使用神经网络编码器（默认采用 Vision Transformer (ViT) ，当然也可以是其他图像编码器，如 ConvNets ）将图像编码为潜在表示，并用带因果掩码的 Transformer 解码器对文本进行解码。\",\"与标准的 Transformer 解码器不同，CoCa 在解码器的前半部分省略了 cross-attention（交叉注意力），只保留单模态的文本表示学习；而在后半部分，则引入 cross-attention，与图像编码器交互，生成跨模态的图文联合表示。这样，CoCa 解码器可以同时产生单模态和多模态的文本表示，从而在同一架构下联合优化 对比损失 和 生成损失：\",\"其中， 和 是损失函数的加权系数。值得注意的是，单编码器的交叉熵分类目标其实可以看作是一种特殊的生成方法——只不过词汇表是标签名称集合。\",\"解耦的文本解码器与 CoCa 架构：\",\"描述生成任务要求模型优化条件概率 ，而对比学习任务则依赖无条件的文本表示。为了兼顾这两者，我们提出了一种解耦的解码器设计，将解码器分为单模态部分和多模态部分：\",\"底部 层（单模态解码器层）：只用因果掩码的自注意力机制来编码输入文本，不使用 cross-attention，得到单模态文本向量表示。\",\"顶部 层（多模态解码器层）：在因果掩码自注意力的基础上，再结合 cross-attention 与图像编码器输出交互，生成多模态表示。\",\"所有解码器层都禁止 token 关注未来的 token，因此自然适用于自回归式的生成目标 。而对于对比学习目标 ，我们在输入句子末尾附加一个可学习的 [CLS] token，并将其在单模态解码器输出中的向量作为文本嵌入。实验中我们将解码器平分为两部分，即 。\",\"在图像输入方面，我们遵循 ALIGN 的设定，使用 的图像分辨率和 的 patch size，得到 256 个图像 token。我们的最大模型 CoCa（简称 “CoCa”）采用 ViT-giant 结构，图像编码器有 10 亿参数，连同文本解码器在内总参数量为 21 亿。此外，我们还探索了两个更小的变体：“CoCa-Base”和“CoCa-Large”（见表1）。\",\"注意力池化（Attentional Poolers）：\",\"需要强调的是，对比损失只使用一个图像嵌入，而标准的图文编码-解码模型通常会利用整段图像 token 序列进行 cross-attention。我们的初步实验发现：\",\"使用单个池化的全局图像表示，有利于视觉识别类任务。\",\"使用更多图像 token（更细粒度表示），更适合多模态理解类任务，因为这类任务需要区域级别的特征。\",\"因此，CoCa 引入了任务特定的注意力池化（task-specific attentional pooling），为不同目标和下游任务定制图像表示。具体来说，pooler 是一个单层多头注意力结构，包含 个可学习查询，图像编码器的输出作为 key 和 value。通过这种机制，模型能够学习如何根据不同任务的需求，聚合出不同长度的图像嵌入（如图2所示）。\",\"在预训练中：\",\"生成损失：使用 ，保留更多图像 token。\",\"对比损失：使用 ，只提取全局嵌入。\",\"这样不仅能满足不同任务的需求，还让 pooler 自然地成为任务适配器。\",\"预训练效率：\",\"解耦的自回归解码器设计的一个关键优势在于：它能高效计算两种损失。因为单向语言模型在完整句子上用因果掩码训练，所以只需一次前向传播，就能同时获得对比损失和生成损失（相比双向方法需要两次前向传播）。这意味着两种损失的大部分计算是共享的，CoCa 相比标准的编码器-解码器模型只增加了极小的计算开销。\",\"另一方面，许多现有方法通常分阶段训练模型组件，且需要在不同数据源/模态之间切换。而 CoCa 则是从零开始进行端到端预训练，数据来源既包括人工标注的图像，也包括带噪声的网页 alt-text 图像，并且统一地将所有标签视为文本，兼顾对比和生成两类目标。\"]},\"273\":{\"h\":\"CoCa 在下游任务中的应用\",\"t\":[\"零样本迁移（Zero-shot Transfer）: 预训练好的 CoCa 模型能够直接在零样本设定下完成多种任务，充分利用图像和文本输入，包括：\",\"零样本图像分类\",\"零样本图文跨模态检索\",\"零样本文本-视频跨模态检索\",\"需要说明的是，这里的“零样本”与传统的零样本学习（zero-shot learning）不同。具体来说，在预训练阶段，模型可能会接触到一些与下游任务相关的监督信息，但在迁移过程中并没有使用任何下游任务的监督样本（这一点与 [12, 32] 中的定义一致）。此外，在预训练数据准备时，我们严格遵循 [13, 32] 提出的去重流程，过滤掉所有与下游任务领域高度相关的样本，避免数据泄漏。\",\"冻结特征评估（Frozen-feature Evaluation）: 如上一节所述，CoCa 使用任务特定的注意力池化（attentional pooling, 简称 pooler），在共享图像编码器的同时，为不同下游任务定制图像表示。这使得 CoCa 在作为冻结编码器（frozen encoder）时表现优异：我们只需学习一个新的 pooler 来聚合特征，而无需调整主干编码器。\",\"这种设计对多任务场景尤其有益：多个任务可以共享同一个冻结的图像编码器，而仅通过不同的 pooler 来适配任务目标。正如 [23] 所指出的，传统的线性评估（linear evaluation）往往难以准确衡量学到的表示能力，而我们发现注意力池化在实际应用中更具实用性。\",\"CoCa 在视频动作识别中的应用（CoCa for Video Action Recognition）: 为了让预训练好的 CoCa 适配视频动作识别任务，我们采用了一种简单的方法：\",\"从视频中抽取多个帧，并将每一帧分别送入共享的图像编码器（如图3所示）。\",\"对于冻结特征评估或微调场景，我们在空间和时间特征 token 上引入一个新的 pooler，并用 softmax 交叉熵损失进行训练。由于 pooler 只有一个 query token，所以它在所有空间和时间 token 上进行聚合时计算开销很小。\",\"对于零样本视频-文本检索，我们使用更简单的方法：对视频中 16 帧（从视频中均匀采样）的嵌入取平均，得到全局视频表示；在计算检索指标时，我们同时对每个视频的文本描述进行编码，并作为目标嵌入进行比对。\"]},\"274\":{\"h\":\"代码实现\",\"t\":[\"关于 CoCa 模型代码实现部分，我们先来看 Image Encoder 部分的代码实现:\",\"class CoCa(nn.Module): def embed_image(self, images=None, image_tokens=None): # encode images into embeddings # with the img_encoder passed in at init # it can also accept precomputed image tokens assert not (exists(images) and exists(image_tokens)) if exists(images): assert exists(self.img_encoder), 'img_encoder must be passed in for automatic image encoding' image_tokens = self.img_encoder(images) # attention pool image tokens # 默认情况下有 256 + 1 (cls query) 个 query token img_queries = repeat(self.img_queries, 'n d -> b n d', b=image_tokens.shape[0]) # image token 作为 key 和 value img_queries = self.img_attn_pool(img_queries, image_tokens) img_queries = self.img_attn_pool_norm(img_queries) # cls token query 和 image token query return img_queries[:, 0], img_queries[:, 1:]\",\"对文本端的编码过程，则分为了单模态编码和多模态融合两个阶段，下面给出的是单模态编码过程实现:\",\"class CoCa(nn.Module): def embed_text(self, text): # 获取 batch 大小和设备 batch, device = text.shape[0], text.device # 序列长度（句子 token 数量） seq = text.shape[1] # 将输入的 token id 映射到词向量 (embedding) # text: [batch, seq] -> text_tokens: [batch, seq, dim] text_tokens = self.token_emb(text) # 构造一个可学习的 [CLS] token，用于表示整句话的全局语义 # repeat 后扩展到 batch 大小: [dim] -> [batch, 1, dim] text_cls_tokens = repeat(self.text_cls_token, 'd -> b 1 d', b=batch) # 把 [CLS] token 拼接到文本序列末尾 # 原来: [batch, seq, dim] -> 拼接后: [batch, seq+1, dim] text_tokens = torch.cat((text_tokens, text_cls_tokens), dim=-2) # 构造 attention mask，防止 padding token 影响 [CLS] 学习 # (1) 先构造 padding mask，True 表示有效位置: [batch, seq] cls_mask = rearrange(text != self.pad_id, 'b j -> b 1 j') # [batch, 1, seq] # (2) 用 pad 把 mask 扩展到 [CLS] token # padding 方向解释: (last_dim_left, last_dim_right, second_last_dim_left, second_last_dim_right) # (0,1, seq,0) 表示：在最后一维右边加1（CLS），在倒数第二维左边加 seq # 得到最终 attn_mask: [batch, seq+1, seq+1] attn_mask = F.pad(cls_mask, (0, 1, seq, 0), value=True) # 依次通过所有的单模态（文本）transformer 层 # 每层会用 attn_mask 来避免对 padding 位置的注意力 for attn_ff in self.unimodal_layers: text_tokens = attn_ff(text_tokens, attn_mask=attn_mask) # 将拼接的 [CLS] token 和原始 token 序列分开 # text_tokens: [batch, seq, dim] # text_cls_tokens: [batch, dim] text_tokens, text_cls_tokens = text_tokens[:, :-1], text_tokens[:, -1] # 对 [CLS] token 进行 LayerNorm，作为文本的整体 embedding text_embeds = self.text_cls_norm(text_cls_tokens) # 返回两个结果： # 1. text_embeds: [batch, dim] —— 整个句子的全局语义向量 # 2. text_tokens: [batch, seq, dim] —— 每个 token 的表示，用于后续跨模态交互 return text_embeds, text_tokens\",\"CoCa 模型本身所提供的前向传播方法，则负责完成整个模型架构流程的实现:\",\"class CoCa(nn.Module): def forward( self, text, images=None, image_tokens=None, labels=None, return_loss=False, return_embeddings=False ): # 获取 batch 大小和设备信息 batch, device = text.shape[0], text.device # 如果要求计算 loss 且没有提供 labels，则自动构造 # 把输入的 text 向右平移一位，text 作为输入，labels 作为输出预测目标 if return_loss and not exists(labels): text, labels = text[:, :-1], text[:, 1:] # 编码文本，得到文本全局向量 (text_embeds) 和 token 序列表示 (text_tokens) text_embeds, text_tokens = self.embed_text(text) # 编码图像，得到图像全局向量 (image_embeds) 和 token 序列表示 (image_tokens) image_embeds, image_tokens = self.embed_image(images=images, image_tokens=image_tokens) # 如果用户只需要返回 embeddings，不需要走后续 loss 计算 if return_embeddings: return text_embeds, image_embeds # 多模态交互：文本 token 先经过自注意力层 (attn_ff)，再经过跨模态注意力层 (cross_attn)， # 与图像 token 进行交互 for attn_ff, cross_attn in self.multimodal_layers: text_tokens = attn_ff(text_tokens) text_tokens = cross_attn(text_tokens, image_tokens) # 将融合后的文本 token 映射到词表大小，得到 logits logits = self.to_logits(text_tokens) # 如果不需要 loss，直接返回 logits（用于推理） if not return_loss: return logits # 为了简写，cross_entropy 记作 ce ce = F.cross_entropy # -------- Caption Loss (字幕生成/文本生成损失) -------- # logits: [batch, seq, vocab] -> rearrange 为 [batch, vocab, seq] 符合 cross_entropy 输入要求 logits = rearrange(logits, 'b n c -> b c n') caption_loss = ce(logits, labels, ignore_index=self.pad_id) caption_loss = caption_loss * self.caption_loss_weight # -------- Contrastive Loss (图文对比损失) -------- # 将文本 / 图像 embedding 投影到 latent 空间 text_latents = self.text_to_latents(text_embeds) image_latents = self.img_to_latents(image_embeds) # 分布式训练时，把不同进程上的 embedding 聚合 if self.is_distributed: latents = torch.stack((text_latents, image_latents), dim = 1) latents = all_gather(latents) text_latents, image_latents = latents.unbind(dim = 1) # 计算文本和图像 latent 向量的相似度矩阵 (内积) # sim[i, j] 表示第 i 个文本与第 j 个图像的相似度 sim = einsum('i d, j d -> i j', text_latents, image_latents) sim = sim * self.temperature.exp() # 温度缩放，调节 softmax 分布的平滑度 # 构造对比学习的标签：第 i 个文本对应第 i 个图像 contrastive_labels = torch.arange(batch, device=device) # 对比损失：文本->图像 + 图像->文本，取平均 contrastive_loss = (ce(sim, contrastive_labels) + ce(sim.t(), contrastive_labels)) * 0.5 contrastive_loss = contrastive_loss * self.contrastive_loss_weight # 总损失 = caption loss + contrastive loss return caption_loss + contrastive_loss\"]},\"275\":{\"h\":\"消融实验\"},\"276\":{\"h\":\"\",\"t\":[\"生成式损失 vs 分类损失：\",\"使用图像描述的生成式损失训练的编码器-解码器模型，性能和传统的单编码器分类损失模型差不多\",\"说明生成式训练本身就包含了分类能力，CoCa 可以统一多种训练方法，而且不需要先单独预训练视觉编码器\",\"联合损失效果：\",\"同时使用对比损失和生成损失的 CoCa，比只用单一损失的模型表现更好\",\"生成损失不仅提升 VQA 的能力，也让跨模态对齐（图像和文本匹配）更准确\",\"两个损失共享计算资源，所以训练效率高，成本低\"]},\"277\":{\"h\":\"\",\"t\":[\"单模态和多模态解码器层：\",\"减少单模态文本层 → 零样本分类能力下降\",\"减少多模态层 → 多模态推理（如 VQA）能力下降\",\"将解码器拆成一半单模态、一半多模态 → 取得了较好平衡\",\"[CLS] token 使用：\",\"只用一个可学习的 [CLS] token 表示整句话，比把它和原输入拼接更好\",\"这种方式能减轻生成和对比损失之间的干扰，同时保证视觉和跨模态检索能力\"]},\"278\":{\"h\":\"\",\"t\":[\"预训练池化器方案：\",\"并行方案：同时提取对比和生成损失\",\"级联方案：先用生成池化器，再在其输出上做对比池化\",\"小模型实验表明，级联方案效果更好，所以 CoCa 默认采用\",\"查询数量：\",\"查询越多 → 使用更多图像 token 信息 → 对多模态理解任务更有利\",\"默认生成池化器长度设为 256，既能提升多模态理解，又保持冻结特征能力\"]},\"279\":{\"h\":\"DINO 论文\",\"t\":[\"Emerging Properties in Self-Supervised Vision Transformers 论文解读\",\"论文链接: Emerging Properties in Self-Supervised Vision Transformers 代码链接: https://github.com/facebookresearch/dino\"]},\"280\":{\"h\":\"引言\",\"t\":[\"Vision Transformer（ViT）近期成为卷积神经网络（convnets）的替代方案，在视觉识别任务中表现出竞争力。但与 convnets 相比，ViT 存在以下不足：\",\"计算开销更大\",\"需要更多训练数据\",\"提取的特征不具备独特优势\",\"研究者提出一个问题：是否是 监督式预训练 限制了 Transformer 在视觉中的潜力？在 NLP 任务中，Transformer 的成功很大程度上得益于 自监督学习（如 BERT 的掩码预测、GPT 的语言建模），这些预训练目标利用上下文信息提供了更丰富的学习信号，而不仅仅是单一标签。相比之下，图像监督学习往往把丰富的视觉信息压缩为一个类别标签，导致潜在信息损失。因此，研究者探索 自监督学习是否能为 ViT 带来新的特性。\",\"论文通过研究自监督预训练对 ViT 特征的影响，得到以下核心结论（部分如图1所示）：\",\"显式的语义分割信息: 自监督 ViT 特征中会自然出现场景布局与物体边界，这些信息可以直接在最后一个 Transformer block 的自注意力模块中读取。相比之下，监督 ViT 和 convnets 并不显式包含这些特征。\",\"优异的 k-NN 分类性能: 在完全不做微调、线性分类器训练或数据增强的情况下，单纯使用最近邻分类器（k-NN），自监督 ViT 在 ImageNet 上可达到 78.3% top-1 准确率。\",\"分割掩码的普遍性与关键条件: 分割掩码的涌现似乎是自监督方法的普遍属性，但要想在 k-NN 上取得良好性能，必须结合以下组件：\",\"动量编码器（momentum encoder）\",\"多视角裁剪增强（multi-crop augmentation）\",\"小 patch 的重要性: 使用更小的 patch 能显著提升 ViT 特征质量。\",\"基于以上发现，作者提出了 DINO（self-distillation with no labels），它可以被理解为一种 无标签的知识蒸馏：\",\"学生网络通过交叉熵损失，直接预测教师网络（由动量编码器构建）的输出。\",\"为避免模型塌缩，仅需在教师输出上应用 居中（centering）与锐化（sharpening）。\",\"相比之下，其他方法使用的复杂组件（如 predictor、先进归一化方式或对比损失）并未带来额外收益。\",\"性能表现：\",\"在 ImageNet 线性分类基准上，DINO + ViT-Base（小 patch）达到 80.1% top-1，显著超越之前的自监督方法。\",\"在 ResNet-50 上，DINO 的表现与最新的自监督系统相当。\",\"灵活性与通用性：\",\"DINO 同时适用于 ViT 和 convnets，无需对架构或归一化方式做修改。\",\"计算效率：\",\"在资源有限的场景下，仅需 2 台 8-GPU 服务器训练 3 天，DINO + ViT 就能在 ImageNet 线性基准上达到 76.1% top-1，超越了同规模 convnets 的自监督系统，同时计算开销更低。\"]},\"281\":{\"h\":\"相关工作\"},\"282\":{\"h\":\"\",\"t\":[\"自监督学习主要经历了以下几类方法：\",\"实例分类（Instance Classification）：把每张图像当作一个独立类别，训练模型去区分它们（允许数据增强后的一致性）。但问题是，当数据规模增大时，显式地学习分类器无法很好扩展。\",\"NCE（噪声对比估计）方法：通过比较图像特征而不是分类，避免了巨量类别的学习。但这种方法要求一次性比较大量样本，因此需要 超大 batch size 或 记忆库（memory bank）。\",\"聚类式方法：自动将实例分组，从而缓解对大规模比较的需求。\",\"无须区分图像的学习方法：近期研究发现，不必显式区分图像也能学到好特征。典型例子是 BYOL，它通过让学生特征去匹配由 动量编码器（momentum encoder） 生成的教师特征来学习。即使去掉动量编码器，BYOL 仍然能工作，但性能下降。\",\"在 BYOL 的启发下，出现了一系列扩展方向：\",\"匹配更复杂的表示\",\"将特征训练为匹配 均匀分布\",\"通过 白化（whitening） 来约束特征学习\",\"DINO 借鉴了 BYOL 的思想，但不同点在于：它使用 不同的相似性匹配损失，并且 学生和教师完全相同的架构。因此，DINO 补充了 BYOL 的观点，把自监督学习解释为 一种无标签的 Mean Teacher 自蒸馏（self-distillation）。\"]},\"283\":{\"h\":\"\",\"t\":[\"自训练（Self-training） 的目标是：利用少量标注，将知识传播到大量无标签数据中，以提升特征质量。这种传播方式有两类：\",\"硬标签分配（hard assignments）\",\"软标签分配（soft assignments）\",\"当使用软标签时，这种方法通常被称为 知识蒸馏（Knowledge Distillation），最初是为了让小模型模仿大模型的输出，从而实现模型压缩。\",\"Xie 等人 提出，蒸馏还能用于将软伪标签传播到无标签数据，这表明 自训练与知识蒸馏本质上是相关的。\",\"在此基础上，DINO 进一步发展：\",\"将知识蒸馏扩展到 无标签场景。\",\"与过去依赖 固定、预训练教师模型 的方法不同，DINO 的教师模型在训练过程中是 动态更新的。\",\"这样一来，知识蒸馏不再是自监督预训练之后的 后处理步骤，而是直接作为 自监督目标函数。\",\"此外，DINO 也与 协同蒸馏（Codistillation） 有关。协同蒸馏中，学生与教师使用相同架构，并在训练中相互蒸馏。但不同的是：在协同蒸馏里，教师也会从学生蒸馏；而在 DINO 中，教师参数是通过 学生参数的指数移动平均（EMA） 来更新的。\"]},\"284\":{\"h\":\"方法\"},\"285\":{\"h\":\"基于知识蒸馏的自监督学习\",\"t\":[\"DINO（Distillation with No Labels）整体框架与近年来的自监督学习方法类似（如SimCLR、BYOL等），但其核心思想借鉴了 知识蒸馏（Knowledge Distillation）。在该框架中，我们训练一个学生网络 去匹配一个教师网络 的输出，二者参数分别为 和 。\",\"对于输入图像 ，两者都会输出一个 维的概率分布，分别记为 和 。这些分布由网络输出经过 softmax 归一化得到：\",\"其中， 是温度参数，控制分布的平滑程度；教师网络也有类似公式，只是温度为 。\",\"在传统的知识蒸馏中，学生通过最小化交叉熵损失来学习匹配教师的分布：\",\"DINO在此基础上引入 多视角（multi-crop）策略：\",\"从同一张图片中生成多个视角（裁剪），形成集合 。\",\"包含两张分辨率较高的 全局视角，以及多张较低分辨率的 局部视角。\",\"学生网络处理所有视角，教师网络只处理全局视角，从而实现 局部-全局对齐。\",\"最终损失函数为：\",\"在默认参数设置下：\",\"全局视角分辨率为 ，覆盖原图超过50%区域。\",\"局部分辨率为 ，覆盖小于50%的区域。\",\"与传统知识蒸馏不同，DINO的教师网络不是固定的，而是由学生网络迭代生成：\",\"冻结教师：在一个epoch内固定教师参数，效果不错。\",\"直接拷贝学生参数：无法收敛。\",\"EMA（指数滑动平均）更新：效果最佳。\",\"更新规则为：\",\"其中 随训练进程按余弦调度从 0.996 增加到 1。这一机制类似 Mean Teacher，教师的表现始终优于学生，从而为学生提供更高质量的特征目标。\",\"网络 由两部分组成：\",\"Backbone ：可以是 ViT 或 ResNet。\",\"投影头 ：由三层 MLP（隐藏维度2048）、 归一化、以及一个权重归一化的全连接层组成。\",\"值得注意的是：\",\"DINO不使用预测器（predictor），因此学生和教师网络架构完全相同。\",\"ViT没有使用BN（Batch Normalization），因此整个系统BN-free。\",\"在自监督学习中，如果所有样本映射到相同的表示，会出现 坍塌问题。DINO采用以下机制来避免：\",\"居中（Centering）：防止某一维度主导表示，但容易导致均匀分布坍塌。\",\"锐化（Sharpening）：通过较低温度参数 增强分布尖锐性，但风险是过度集中。\",\"结合使用：居中与锐化效果互补，可以有效避免坍塌。\",\"其中，居中操作等价于给教师输出加一个偏置项 ，更新方式为：\",\"其中 为动量参数， 为batch size。该方法依赖于 一阶统计量，能适应不同batch大小。\"]},\"286\":{\"h\":\"实现细节\",\"t\":[\"Vision Transformer (ViT) 的机制\",\"ViT 将图像切分为不重叠的 patch（本文常用 或 ），每个 patch 通过线性层转为 embedding。序列中额外加入一个可学习的 token（称为 [CLS]），它用于聚合全局信息，并在其输出位置接上投影头 。虽然 [CLS] token 并不对应标签或监督，但为了与前人工作保持一致，依然称之为 class token。\",\"这些 patch token 和 [CLS] token 被输入到标准的 Transformer 网络，网络采用 pre-norm 的 LayerNorm。Transformer 由自注意力层与前馈层组成，并带有跳跃连接。自注意力层通过 attention 机制让每个 token 能够参考其他 token 的表示，进而更新自身表示。\",\"实现细节\",\"预训练在 无标签的 ImageNet 上进行。\",\"优化器：AdamW\",\"批量大小：1024（ViT-S/16 下分布在 16 个 GPU 上）\",\"学习率：前 10 个 epoch 内线性升至基础值，基础值遵循线性缩放规则：\",\"学习率调度：余弦退火\",\"权重衰减：同样采用余弦调度，从 0.04 增加到 0.4\",\"温度：学生温度 在前 30 个 epoch 内从 0.04 线性升到 0.07；教师温度 \",\"数据增强：采用 BYOL 的增强方式（颜色抖动、高斯模糊、光照变换 solarization），并结合 multi-crop；同时使用双三次插值（bicubic interpolation）调整位置编码以适配不同尺度\",\"评估协议\",\"自监督学习的常见评估方式有两类：\",\"线性评估：冻结特征，训练线性分类器；训练阶段使用随机裁剪缩放和水平翻转，测试阶段用中心裁剪来报告准确率。\",\"微调评估：以预训练权重初始化网络，并在训练过程中更新参数。\",\"但这两种方法对超参数较为敏感，例如学习率不同会导致准确率差异较大。因此，论文还引入了 k-NN 最近邻分类器 来简化特征质量评估：\",\"将预训练模型冻结，用它提取并存储下游任务训练集的特征\",\"对于输入图片，找到其特征的 个最近邻，由它们投票决定分类\",\"实验发现 的效果最好，且在大多数实验中表现稳定\",\"这种方法的优势在于：\",\"不需要超参数调节\",\"不依赖数据增强\",\"只需一次遍历下游数据集即可完成评估\",\"因此大大简化了特征评估的流程。\"]},\"287\":{\"h\":\"消融实验\"},\"288\":{\"h\":\"Patch大小的重要性\",\"t\":[\"研究发现，更小的Patch尺寸能显著提升ViT-S的k-NN分类性能（如表5所示）。例如：\",\"ViT-S/16 → ViT-S/8 → ViT-S/5，性能逐步提升\",\"ViT-B同样表现出这种趋势\",\"这种提升并不依赖增加模型参数，但会牺牲吞吐率：\",\"5×5 patch 的吞吐率仅 44 im/s\",\"8×8 patch 的吞吐率为 180 im/s\",\"因此，更小的Patch能带来准确率提升，但需要付出计算速度代价。\"]},\"289\":{\"h\":\"教师网络的选择\",\"t\":[\"在DINO中，教师网络扮演关键角色。实验结果表明：\",\"使用 上一轮迭代的学生网络 作为教师，模型无法收敛，除非增加额外的归一化。\",\"使用 前一个epoch的学生网络 作为教师，模型不会崩溃，k-NN结果与MoCo-v2、BYOL等框架接近。\",\"使用 动量教师 时效果最佳，显著优于上述方法。\",\"进一步分析（如图6所示）：\",\"动量教师在整个训练过程中始终优于学生模型，无论是ViT还是ResNet-50。\",\"这一现象在其他动量方法（如MoCo）中未被观察到。\",\"作者将动量教师解释为一种 指数衰减的Polyak-Ruppert平均，相当于在训练中持续进行模型集成，构建出性能更强的教师来指导学生学习。\"]},\"290\":{\"h\":\"避免崩溃\",\"t\":[\"DINO训练中可能出现两种形式的崩溃：\",\"均匀输出：模型对所有输入输出一致的分布\",\"单维主导：输出仅集中在一个维度\",\"DINO通过 居中（centering） 和 目标锐化（sharpening） 来避免这两类崩溃：\",\"居中操作抑制单维主导，但会推动输出趋向均匀\",\"锐化操作则产生相反作用，抑制均匀输出\",\"作者用交叉熵公式来解释：\",\"其中，若 ，则输出为常量，表示发生崩溃。实验（如图7所示）表明：\",\"缺少居中 → 收敛为0，出现单维崩溃\",\"缺少锐化 → 收敛为 ，出现均匀崩溃\",\"同时使用居中和锐化 → 两种崩溃被平衡并避免\"]},\"291\":{\"h\":\"计算需求\",\"t\":[\"在两台8-GPU服务器上训练ViT-S/16 DINO模型时，作者对比了不同的 multi-crop策略：\",\"无multi-crop（2×224²）：72.5%准确率，耗时46小时，显存9.3G\",\"multi-crop (2×224²+10×96²)：74.6%准确率，仅需24小时，显存15.4G\",\"可见：\",\"multi-crop 提升了准确率/训练时间的性价比\",\"更多视角数（如6×、10×96²）带来的提升逐渐减弱\",\"DINO最终在两台8-GPU、3天内达到了 76.1% top-1准确率，超越了同规模CNN的自监督方法，同时计算需求更低\"]},\"292\":{\"h\":\"小批量训练\",\"t\":[\"DINO同样可以在小batch下训练（如表9所示）：\",\"默认 batch size = 1024\",\"batch size = 128 时性能略低，但仍能达到较高水平\",\"batch size = 128 的实验仅需1张GPU即可运行\",\"batch size = 8 时，训练50个epoch能达到35.2%准确率，显示了在极小显存条件下训练大模型的潜力\",\"学习率采用线性缩放：\",\"不过，小batch训练可能需要重新调整超参数（如动量率）才能达到最佳效果。\"]},\"293\":{\"h\":\"代码解析\",\"t\":[\"从本节开始我们将对官方开源的 DINO 模型代码实现进行详细讲解，下图给出的是 DINO 模型的运行完整流程图:\",\"按照 DINO 模型的训练流程，第一步首先是对 输入图像 进行数据增强，生成 两张全局视角图像 + 若干局部视角图像， 该过程由 DataAugmentationDINO 类实现，代码如下:\",\"class DataAugmentationDINO(object): \\\"\\\"\\\" 数据增强类，用于 DINO 训练。 主要思想：从一张输入图像生成多个视角（multi-crop）， 包括 2 张全局裁剪图像（224x224）和若干张局部裁剪图像（96x96）。 \\\"\\\"\\\" def __init__(self, global_crops_scale, local_crops_scale, local_crops_number): # 基础增强：随机翻转 + 颜色抖动 + 随机灰度化 flip_and_color_jitter = transforms.Compose([ transforms.RandomHorizontalFlip(p=0.5), # 以 50% 概率水平翻转 transforms.RandomApply( # 以 80% 概率执行颜色抖动 [transforms.ColorJitter( brightness=0.4, # 亮度变化 contrast=0.4, # 对比度变化 saturation=0.2, # 饱和度变化 hue=0.1 # 色调变化 )], p=0.8 ), transforms.RandomGrayscale(p=0.2), # 以 20% 概率转为灰度图 ]) # 标准化（Imagenet 预训练均值/方差） normalize = transforms.Compose([ transforms.ToTensor(), # 转换为 Tensor transforms.Normalize( (0.485, 0.456, 0.406), # mean (0.229, 0.224, 0.225) # std ), ]) # ----------- 全局裁剪 1 ----------- self.global_transfo1 = transforms.Compose([ # 随机裁剪并缩放到 224x224 transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC), flip_and_color_jitter, utils.GaussianBlur(1.0), # 高斯模糊（概率 100%） normalize, ]) # ----------- 全局裁剪 2 ----------- self.global_transfo2 = transforms.Compose([ transforms.RandomResizedCrop(224, scale=global_crops_scale, interpolation=Image.BICUBIC), flip_and_color_jitter, utils.GaussianBlur(0.1), # 高斯模糊（概率 10%） utils.Solarization(0.2), # 以 20% 概率进行太阳化增强（反转亮部） normalize, ]) # ----------- 局部裁剪 ----------- self.local_crops_number = local_crops_number # 局部裁剪数量 self.local_transfo = transforms.Compose([ transforms.RandomResizedCrop(96, scale=local_crops_scale, interpolation=Image.BICUBIC), flip_and_color_jitter, utils.GaussianBlur(p=0.5), # 高斯模糊（概率 50%） normalize, ]) def __call__(self, image): \\\"\\\"\\\" 输入一张图像，返回多个增强后的 crop。 输出顺序： [全局视角1, 全局视角2, 局部视角1, 局部视角2, ...] \\\"\\\"\\\" # 先生成两张全局 crop crops = [self.global_transfo1(image), self.global_transfo2(image)] # 再生成若干张局部 crop for _ in range(self.local_crops_number): crops.append(self.local_transfo(image)) return crops\",\"特别注意初始化方法中传入的 global_crops_scale 和 local_crops_scale 参数 :\",\" # Multi-crop parser.add_argument('--global_crops_scale', type=float, nargs='+', default=(0.4, 1.0), help=\\\"Scale range for global crops.\\\") parser.add_argument('--local_crops_number', type=int, default=8, help=\\\"Number of local crops (0 disables multi-crop).\\\") parser.add_argument('--local_crops_scale', type=float, nargs='+', default=(0.05, 0.4), help=\\\"Scale range for local crops.\\\")\",\"*_crops_scale 是 随机裁剪区域的相对尺度范围，即原始图像面积的比例。\",\"参数形式：*_crops_scale = (min_scale, max_scale)\",\"作用：控制 随机裁剪区域的最小和最大面积比例。\",\"举例：\",\"如果 global_crops_scale = (0.4, 1.0)\",\"随机裁剪的区域面积 ∈ [40%, 100%] 原图面积之间。\",\"也就是说，有时裁掉少量边缘（接近原图），有时只取 40% 的图像内容（更聚焦）。\",\"裁剪后再统一缩放到 224 × 224，作为模型输入。\",\"由于 DINO 采用多视角图像输入，对于学生模型来说，一个批次图像经过增强后，会得到 2+n 个来自不同视角下的批次图像:\",\"为了同时处理多视角图像输入，DINO 使用了 装饰器模式 ，设计 MultiCropWrapper 类来将输入图像批次列表按照分辨率进行分组：\",\"[0,2) 区间对应 224 分辨率 ， [2,6) 对应 96 分辨率\",\"将分辨率相同的批次合并后输入模型，拼接结果:\",\"完整代码实现如下:\",\"class MultiCropWrapper(nn.Module): \\\"\\\"\\\" 一个封装类，用于处理多视角输入（multi-crop inputs）。 不同分辨率的输入会被分组，每一组在 backbone 中分别进行一次前向计算， 得到的特征拼接后再送入 head 中处理。 \\\"\\\"\\\" def __init__(self, backbone, head): super(MultiCropWrapper, self).__init__() # 去掉 backbone 中原本为 ImageNet 分类准备的全连接层（fc, head） # 因为这里是自监督学习，不需要类别分类器 backbone.fc, backbone.head = nn.Identity(), nn.Identity() self.backbone = backbone # 特征提取网络 self.head = head # 投影头（projection head），用于后续对比学习 def forward(self, x): # 保证输入是 list（多 crop 的场景可能传进来的是多个张量） if not isinstance(x, list): x = [x] # 获取每个输入 crop 的分辨率（最后一个维度 size） # torch.unique_consecutive 会返回连续相同值的唯一值及计数 # return_counts=True 表示返回每个唯一值的计数 # torch.cumsum 累积求和，得到每组 crop 的结束索引 idx_crops = torch.cumsum(torch.unique_consecutive( torch.tensor([inp.shape[-1] for inp in x]), # 取每个 crop 的宽度 return_counts=True, )[1], 0) # 初始化 start_idx = 0 # 创建一个空 tensor 用于保存所有特征，放到和输入相同的 device 上 output = torch.empty(0).to(x[0].device) # 遍历不同分辨率的分组 for end_idx in idx_crops: # 将相同分辨率的 crop 拼接（batch 化），一起送入 backbone _out = self.backbone(torch.cat(x[start_idx: end_idx])) # 有些 backbone（如 XCiT）返回的是 tuple，这里只取第一个元素（主特征） if isinstance(_out, tuple): _out = _out[0] # 将这一批特征拼接到输出中 output = torch.cat((output, _out)) # 更新下一个起始位置 start_idx = end_idx # 所有特征拼接完成后，送入 head 得到最终表示 return self.head(output)\",\"下面我们来看 DINO 模型完整的训练流程:\",\"def train_dino(args): \\\"\\\"\\\" 在单进程下运行完整的 DINO 训练循环 步骤： 1. 构建多视图增强 (Multi-crop augmentations) 和数据集 / DataLoader 2. 构建学生 (Student) / 教师 (Teacher) 网络，并附加 DINO 头 3. 用学生初始化教师网络，教师梯度冻结 4. 创建损失函数、优化器及余弦调度器 (学习率、权重衰减、EMA momentum) 5. 标准 float32 训练循环，每轮迭代更新 EMA 教师参数 \\\"\\\"\\\" # ===================== # 1. 数据准备 # ===================== # 构建 DINO 的多视图增强策略：2 个全局裁剪 + N 个局部裁剪 transform = DataAugmentationDINO( args.global_crops_scale, args.local_crops_scale, args.local_crops_number, ) # 使用 ImageFolder 数据集，要求目录结构为： # data_path/class_x/*.jpg # ImageFolder 会根据子文件夹名自动生成 class 索引，并返回 (PIL image, label) dataset = datasets.ImageFolder(args.data_path, transform=transform) # 创建 DataLoader data_loader = torch.utils.data.DataLoader( dataset, shuffle=True, # 打乱数据顺序 batch_size=args.batch_size, num_workers=args.num_workers, # 多线程加载数据 pin_memory=True, # CUDA 加速 drop_last=True, # 丢弃最后不足 batch 的数据 ) print(f\\\"数据加载完成: {len(dataset)} 张图片. Batch size: {args.batch_size}\\\") # ===================== # 2. 构建学生/教师网络 # ===================== # 使用相同的骨干网络 (Backbone) 构建学生和教师，并附加 DINO head student_backbone = vits.__dict__[args.arch]( patch_size=args.patch_size, drop_path_rate=args.drop_path_rate # DropPath 用于正则化 ) teacher_backbone = vits.__dict__[args.arch](patch_size=args.patch_size) embed_dim = student_backbone.embed_dim # ViT 输出 embedding 维度 # 构建学生网络 (Student) + DINO head student = utils.MultiCropWrapper( student_backbone, DINOHead( embed_dim, args.out_dim, use_bn=args.use_bn_in_head, # 是否在 head 使用 BN norm_last_layer=args.norm_last_layer, # 是否规范化最后一层 ) ) # 构建教师网络 (Teacher) + DINO head teacher = utils.MultiCropWrapper( teacher_backbone, DINOHead(embed_dim, args.out_dim, args.use_bn_in_head), ) # ===================== # 3. 初始化教师网络 # ===================== # 教师网络初始参数与学生网络相同 teacher.load_state_dict(student.state_dict()) # 教师网络不参与梯度更新，仅通过 EMA 更新参数 for p in teacher.parameters(): p.requires_grad = False print(f\\\"学生/教师网络构建完成: arch={args.arch}, embed_dim={embed_dim}\\\") # ===================== # 4. 构建 DINO 损失函数 # ===================== # DINOLoss 接收学生输出、教师输出和当前 epoch 信息 dino_loss = DINOLoss( args.out_dim, args.local_crops_number + 2, # 总视图数量: 2 个全局 + N 个局部 args.warmup_teacher_temp, args.teacher_temp, args.warmup_teacher_temp_epochs, args.epochs, ) # ===================== # 5. 构建优化器 # ===================== # 对参数进行分组：对 bias / norm 等不使用 weight decay params_groups = utils.get_params_groups(student) optimizer = torch.optim.AdamW(params_groups) # ===================== # 6. 学习率、权重衰减、EMA momentum 调度器 # ===================== # 使用余弦调度器，按迭代次数调整 base_lr = args.lr * (args.batch_size / 256.0) # 学习率按 batch_size 线性缩放 lr_schedule = utils.cosine_scheduler( base_lr, args.min_lr, args.epochs, len(data_loader), warmup_epochs=args.warmup_epochs, ) wd_schedule = utils.cosine_scheduler( args.weight_decay, args.weight_decay_end, args.epochs, len(data_loader), ) momentum_schedule = utils.cosine_scheduler( args.momentum_teacher, 1.0, args.epochs, len(data_loader) ) # ===================== # 7. 训练循环 # ===================== for epoch in range(args.epochs): student.train() # 学生网络训练模式 teacher.train() # 教师网络不更新梯度，但 train 模式保持 BN 行为 for it, (images, _) in enumerate(data_loader): # --------------------- # 每次迭代更新学习率和权重衰减 # --------------------- gid = it + epoch * len(data_loader) # 全局迭代索引 for i, pg in enumerate(optimizer.param_groups): pg[\\\"lr\\\"] = lr_schedule[gid] if i == 0: pg[\\\"weight_decay\\\"] = wd_schedule[gid] # --------------------- # 前向计算 + 损失 # 教师网络只看 2 个全局裁剪 # 学生网络看所有裁剪 (2 个全局 + N 个局部) # --------------------- teacher_output = teacher(images[:2]) # 仅前 2 个全局裁剪 student_output = student(images) # 所有裁剪 loss = dino_loss(student_output, teacher_output, epoch) # --------------------- # 反向传播 (仅学生网络) # --------------------- optimizer.zero_grad() loss.backward() # 训练初期冻结学生网络最后一层 utils.cancel_gradients_last_layer(epoch, student, args.freeze_last_layer) # 更新学生网络参数 optimizer.step() # --------------------- # EMA 更新教师网络参数 # --------------------- with torch.no_grad(): m = momentum_schedule[gid] # 当前迭代 EMA momentum for ps, pt in zip(student.parameters(), teacher.parameters()): pt.data.mul_(m).add_((1 - m) * ps.detach().data)\",\"DINOHead 将 [CLS] token 投影到 65536 维空间，是为了：\",\"增强训练稳定性（避免塌缩）。\",\"分离“表征空间”和“对比空间”。\",\"通过超大维度的“语义字典”让模型学习更细粒度、更丰富的表征。\",\"部分代码细节没有详细介绍，大家请自行阅读源代码\",\"最后来看一下 DINOLoss 类的实现:\",\"class DINOLoss(nn.Module): \\\"\\\"\\\" DINO 损失函数类，支持温度调度、中心化(center)和多视图(crop)处理。 功能说明： - 对教师输出进行温度锐化(sharpening)，并维护一个移动中心(center) 来稳定训练。 - 计算跨视图的交叉熵损失：每个教师的全局视图监督所有学生视图， 但不监督与其索引相同的学生视图。 \\\"\\\"\\\" def __init__(self, out_dim, ncrops, warmup_teacher_temp, teacher_temp, warmup_teacher_temp_epochs, nepochs, student_temp=0.1, center_momentum=0.9): super().__init__() self.student_temp = student_temp # 学生输出 softmax 的温度 self.center_momentum = center_momentum # 中心更新的 EMA 动量 self.ncrops = ncrops # 输入图像裁剪数量 self.register_buffer(\\\"center\\\", torch.zeros(1, out_dim)) # 初始化教师输出中心向量 # 教师温度调度表 # 前 warmup_teacher_temp_epochs 采用线性增长，从 warmup_teacher_temp 到 teacher_temp # 后续 epoch 固定为 teacher_temp self.teacher_temp_schedule = np.concatenate(( np.linspace(warmup_teacher_temp, teacher_temp, warmup_teacher_temp_epochs), np.ones(nepochs - warmup_teacher_temp_epochs) * teacher_temp )) def forward(self, student_output, teacher_output, epoch): \\\"\\\"\\\" 计算跨视图交叉熵损失。 输入： - student_output: 学生模型输出，包含所有裁剪的拼接结果 - teacher_output: 教师模型输出，仅包含全局视图 - epoch: 当前训练轮数，用于教师温度调度 处理流程： 1. 学生输出按温度缩放并拆分为每个裁剪的输出 2. 教师输出减去中心并进行温度锐化 3. 每个教师视图监督除同索引学生视图外的所有学生视图 \\\"\\\"\\\" # 学生输出按温度缩放，并拆分为 ncrops 个裁剪 student_out = student_output / self.student_temp student_out = student_out.chunk(self.ncrops) # 教师输出：减去中心并进行温度锐化 temp = self.teacher_temp_schedule[epoch] # 当前 epoch 的教师温度 teacher_out = F.softmax((teacher_output - self.center) / temp, dim=-1) teacher_out = teacher_out.detach().chunk(2) # 仅两张全局裁剪用于教师监督 total_loss = 0 # 总损失 n_loss_terms = 0 # 用于计算平均损失的项数 # 遍历每个教师视图 for iq, q in enumerate(teacher_out): # 遍历每个学生视图 for v in range(len(student_out)): if v == iq: # 不监督同索引的学生视图 continue # 计算交叉熵损失 loss = torch.sum(-q * F.log_softmax(student_out[v], dim=-1), dim=-1) total_loss += loss.mean() n_loss_terms += 1 # 对所有损失求平均 total_loss /= n_loss_terms # 更新教师输出中心 self.update_center(teacher_output) return total_loss @torch.no_grad() def update_center(self, teacher_output): \\\"\\\"\\\" 使用当前 batch 教师输出更新 EMA 中心 公式： center = center * center_momentum + batch_center * (1 - center_momentum) \\\"\\\"\\\" batch_center = teacher_output.mean(dim=0, keepdim=True) # 计算 batch 中心 self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\",\"DINO 不对同索引的学生视图进行监督，是为了 避免学生只学到平凡的输入匹配，强制学生跨视图对齐，才能让表示真正学到 语义一致性，而不是表面相似性。\"]},\"294\":{\"h\":\"书生·万象多模态大模型（InternVL 1.0）\",\"t\":[\"InternVL 1.0: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks 论文简析\",\"论文链接: https://arxiv.org/abs/2312.14238 代码链接: https://github.com/OpenGVLab/InternVL\"]},\"295\":{\"h\":\"摘要\",\"t\":[\"InternVL是一个大规模视觉-语言基础模型，旨在解决当前视觉与视觉-语言基础模型发展滞后于大型语言模型（LLMs）的问题。该模型通过将视觉基础模型扩展到60亿参数，并利用多源网络图像-文本数据进行渐进式对齐训练，成功实现了视觉与语言模型在参数规模和特征表示上的协调。InternVL在32个通用视觉-语言任务中表现出色，包括图像分类、语义分割、视频分类、图像/视频-文本检索以及多模态对话系统等，展现了强大的视觉能力和与LLMs的无缝集成潜力，为多模态大模型的发展提供了重要贡献。\"]},\"296\":{\"h\":\"简介\",\"t\":[\"研究背景与问题: 大型语言模型（LLMs）的快速发展推动了通用人工智能（AGI）系统的进步，但视觉和视觉-语言基础模型的发展却相对滞后。现有的视觉-语言大模型（VLLMs）通常使用轻量级的“胶水层”（如QFormer或线性投影）来对齐视觉和语言模型的特征，但这种方法存在三个主要限制：\",\"参数规模不匹配：LLMs的参数规模已达千亿级，而视觉编码器通常仅约10亿参数，限制了LLM的能力利用。\",\"表征不一致：视觉模型通常基于纯视觉数据或BERT系列模型训练，与LLMs的特征空间存在差异。\",\"低效连接：轻量级胶水层难以捕捉跨模态的复杂交互。\",\"解决方案与核心设计: 论文提出 InternVL，通过以下关键设计解决上述问题：\",\"参数平衡的视觉与语言组件：包含60亿参数的视觉编码器（InternViT-6B）和80亿参数的语言中间件（QLLaMA），后者作为强大的“胶水层”重组视觉特征。\",\"一致的表征对齐：使用多语言LLaMA初始化中间件，确保视觉编码器与LLMs的特征空间一致。\",\"渐进式图像-文本对齐策略：先在大规模噪声数据上对比学习，再在高质量数据上生成学习，逐步提升模型性能（如图1c所示）。\",\"模型优势\",\"多功能性：可作为独立视觉编码器或与语言中间件结合，支持感知、检索、生成和对话任务。\",\"强大性能：在ImageNet分类、ADE20K分割、视频检索等任务中达到SOTA（如图2所示）。\",\"LLM友好性：与LLaMA、Vicuna等LLMs无缝集成，推动多模态应用发展。\"]},\"297\":{\"h\":\"相关工作\"},\"298\":{\"h\":\"\",\"t\":[\"视觉基础模型在过去十年中经历了显著发展，从早期的AlexNet和CNN架构（如ResNet）到近年来的Vision Transformer（ViT）及其变体。ViT及其衍生模型（如ViT-G、EVA-02等）通过扩大模型规模和参数量，显著提升了视觉任务的性能。然而，当前广泛使用的视觉模型参数量仍停留在约10亿级别（如ViT-22B除外），远落后于LLMs的规模。此外，这些模型多基于纯视觉数据（如ImageNet、JFT）训练，或与BERT系列模型对齐，缺乏与LLMs的直接特征兼容性，限制了其在多模态任务中的表现。\"]},\"299\":{\"h\":\"\",\"t\":[\"LLMs（如GPT-3、LLaMA系列、Vicuna等）在自然语言处理领域取得了突破性进展，展示了强大的少样本和零样本学习能力。开源模型（如ChatGLM、Falcon等）的涌现进一步加速了多模态研究的进程。然而，LLMs本身缺乏视觉理解能力，如何将其与视觉模态结合成为关键挑战。\"]},\"300\":{\"h\":\"\",\"t\":[\"近期研究通过将视觉模型与LLMs结合，构建了多模态对话系统（如Flamingo、LLaVA、MiniGPT-4等）。这些工作主要依赖轻量级适配层（如QFormer、线性投影）连接视觉编码器和LLM，但受限于视觉模型的规模和对齐效率。部分模型（如KOSMOS-2、Qwen-VL）进一步引入了视觉定位能力，支持区域描述和问答。尽管如此，视觉基础模型的性能瓶颈仍是制约VLLMs发展的关键因素。\"]},\"301\":{\"h\":\"\",\"t\":[\"现有工作表明，视觉模型的规模不足和对齐低效是阻碍多模态能力提升的主要障碍。InternVL通过规模化视觉编码器和渐进式跨模态对齐，首次实现了视觉与语言模型在参数和特征空间的深度协同，填补了这一领域的空白。\"]},\"302\":{\"h\":\"方法\"},\"303\":{\"h\":\"\",\"t\":[\"InternVL的整体架构（如图3所示）突破了传统视觉模型（如ViT）和双塔模型（如CLIP）的局限，通过以下两个核心组件实现跨模态深度协同：\",\"InternViT-6B: 基于标准ViT架构的60亿参数视觉编码器，通过超参数搜索优化了深度（48层）、头数（25）和MLP比率（8），在模型规模（5.9B参数）与计算效率间取得平衡（详见表1）。其输出支持密集特征图（）或全局池化特征，适配分类、分割等任务。\",\"QLLaMA: 基于多语言LLaMA-7B初始化的80亿参数语言中间件，新增96个可学习查询和交叉注意力层（1B参数），作为视觉与LLMs之间的\\\"重型胶水层\\\"。相比QFormer等轻量适配器，其参数量提升42倍，能更有效地重组视觉特征为LLM兼容的序列（见图4b/d）。\",\"如图1所示，InternVL的架构设计显著区别于：\",\"(a) 纯视觉模型（如ResNet）：仅支持单模态任务，缺乏语言对齐。\",\"(b) 双塔模型（如CLIP）：独立编码图像/文本，依赖浅层相似度计算。\",\"(c) InternVL：通过QLLaMA实现动态特征交互，同时支持对比学习（如检索）和生成任务（如描述）。\",\"通过组合不同组件，InternVL可灵活切换为四种模式（图4）：\",\"纯视觉模式（图4a）：仅用InternViT-6B处理图像分类/分割。\",\"对比模式-InternVL-C（图4b）：视觉编码器+注意力池化，用于零样本分类/检索。\",\"对比模式-InternVL-G（图4b）：联合QLLaMA二次编码视觉特征，提升检索精度。\",\"对话模式（图4c/d）：连接LLM（如Vicuna），支持多模态问答。\"]},\"304\":{\"h\":\"\",\"t\":[\"大规模视觉编码器（InternViT-6B）\",\"InternViT-6B是一个基于Vision Transformer（ViT）的视觉编码器，参数量达到60亿，旨在与大型语言模型（LLM）的规模相匹配。\",\"通过超参数搜索（如模型深度、头维度和MLP比例），作者确定了在性能和效率之间取得平衡的最佳配置（表1）。实验发现，模型深度对速度的影响在GPU计算饱和后可以忽略，而参数数量相同时，不同配置对性能影响较小。最终选择了深度48、宽度3200、MLP比率12800的稳定配置。\",\"该编码器支持密集预测任务（如语义分割）和图像分类任务，并能生成全局或局部视觉特征（图4a/b）。\",\"语言中间件（QLLaMA）\",\"QLLaMA是一个80亿参数的语言中间件，基于多语言LLaMA-7B初始化，新增了96个可学习查询和交叉注意力层（1亿参数），用于对齐视觉与语言特征（图3）。\",\"相比传统轻量级“胶水层”（如QFormer或线性投影），QLLaMA的优势包括：\",\"通过预训练权重实现视觉特征到LLM表示的对齐；\",\"参数量是QFormer的42倍，即使冻结LLM解码器也能在多模态对话任务中表现优异；\",\"支持对比学习任务（如零样本图像分类和检索）。\",\"灵活的组合方式（“瑞士军刀”模型）\",\"InternVL通过组合视觉编码器和语言中间件，支持多种任务模式（图4）：\",\"视觉感知任务：直接使用InternViT-6B提取特征。\",\"对比任务（如检索）：通过注意力池化生成全局特征（InternVL-C或InternVL-G）。\",\"生成任务（如图像描述）：QLLaMA利用其大规模参数重组视觉表示并生成文本。\",\"多模态对话：连接LLM解码器（InternVL-Chat），支持两种配置（图4c/d）。\"]},\"305\":{\"h\":\"\",\"t\":[\"1. 视觉-语言对比训练（Vision-Language Contrastive Training）\",\"目标：初步对齐视觉编码器（InternViT-6B）和文本编码器（LLaMA-7B）。\",\"数据：使用大规模但噪声较多的公开网络图像-文本对（如 LAION-en、LAION-multi、COYO 等，共 4.98B 样本，表 2）。\",\"方法：\",\"采用 CLIP 风格的对比学习，最小化图像-文本对的对称交叉熵损失。\",\"初始阶段在较低分辨率（196×196）训练，并应用 50% 图像 token 掩码 以提高效率，后期切换至 224×224 分辨率。\",\"效果：使模型在零样本分类、图像-文本检索等对比任务上表现优异，并为后续阶段提供稳健的视觉表示。\",\"2. 视觉-语言生成训练（Vision-Language Generative Training）\",\"目标：增强模型生成能力，进一步对齐视觉与语言特征。\",\"数据：筛选高质量图像-文本数据（1.03B，表 2），去除低质量描述（如重复文本、无意义内容）。\",\"方法：\",\"冻结 InternViT-6B 和 QLLaMA 的预训练权重，仅训练新增的 可学习查询和交叉注意力层。\",\"结合 三种损失函数：\",\"ITC（图像-文本对比损失）\",\"ITM（图像-文本匹配损失）\",\"ITG（基于图像的文本生成损失）\",\"效果：使 QLLaMA 能够有效重组视觉特征，并生成连贯的文本描述（如表 10 的零样本图像描述结果）。\",\"3. 监督微调（Supervised Fine-tuning, SFT）\",\"目标：优化多模态对话能力，连接 LLM 解码器（如 Vicuna、InternLM）。\",\"数据：收集约 400 万高质量指令数据（表 3），涵盖图像描述、VQA、OCR、视觉定位等任务。\",\"方法：\",\"两种配置（图 4c/d）：\",\"仅使用 InternViT-6B，通过 MLP 层连接 LLM（类似 LLaVA）。\",\"使用完整 InternVL（InternViT + QLLaMA），利用其对齐的特征空间提升性能。\",\"由于 QLLaMA 与 LLM 特征空间一致，即使冻结 LLM 解码器，仅微调 MLP 层也能取得良好效果。\",\"效果：在 MME、POPE 等多模态对话基准上达到 SOTA（表 9）。\",\"这一渐进式策略确保模型 从粗粒度对齐过渡到细粒度优化，充分利用不同质量的数据，最终实现强大的多模态理解和生成能力。\"]},\"306\":{\"h\":\"实现细节\",\"t\":[\"第一阶段（Stage 1）\",\"在该阶段，图像编码器 InternViT-6B 是随机初始化的 7，而文本编码器 LLaMA-7B 则使用来自文献 32的预训练权重进行初始化。此阶段中，所有参数都是可训练的。\",\"第二阶段（Stage 2）\",\"在该阶段，InternViT-6B 和 QLLaMA 继承了第一阶段中学习到的权重，而 QLLaMA 中新加入的可学习查询（learnable queries）和跨注意力层（cross-attention layers）是随机初始化的。由于第一阶段中已获得了强大的表示能力，我们在该阶段冻结 InternViT-6B 和 QLLaMA，仅训练新引入的参数。\",\"基座是 LLaMA-7B：QLLaMA 继承了经过第一阶段对比训练后得到的 LLaMA-7B 权重；\",\"新增模块：\",\"96 个 learnable query 向量：用于从视觉特征中提取信息；\",\"Cross-Attention 层：插入到了 LLaMA 的每一层 decoder block 中（这是主流做法，如 BLIP-2 也是如此），使得语言模型具备视觉融合能力；\",\"参数量：新加入模块约为 10 亿参数，占 QLLaMA 总体 8B 的一部分；\",\"第三阶段（Stage 3）\",\"此阶段有两种不同的配置方式：\",\"一种是单独使用 InternViT-6B，如图 4(c) 所示；\",\"另一种是同时使用完整的 InternVL 模型，如图 4(d) 所示。\"]},\"307\":{\"h\":\"实验\"},\"308\":{\"h\":\"\",\"t\":[\"图像分类（Image Classification）：\",\"InternViT-6B 在 ImageNet-1K 及其多个变种（如 IN-A、IN-R、IN-V2 等）上进行线性探测评估。结果显示，其在冻结骨干网络的前提下，取得了领先的零样本分类准确率，平均精度达到 82.5%，超过了如 OpenCLIP-G、EVA-01-CLIP-g 等主流模型。\",\"语义分割（Semantic Segmentation）：\",\"在 ADE20K 上进行语义分割测试，在不同微调策略下（线性探测、Head Tuning、全量微调），InternViT-6B 都展现出更强的像素级感知能力。例如，在全参数微调下，mIoU 达到 58.9%，显著优于 ViT-22B（55.3%）。\"]},\"309\":{\"h\":\"\",\"t\":[\"零样本图像分类（Zero-Shot Image Classification）：\",\"在多语言版本的 ImageNet 上（EN, ZH, JP, AR, IT），InternVL-C 的表现优于 OpenCLIP-XLM-R 和其他多语言模型，展示了良好的语言泛化能力。\",\"零样本图像-文本检索（Image-Text Retrieval）：\",\"InternVL-C 和 InternVL-G 在英中双语的 Flickr30K / COCO / Flickr30K-CN / COCO-CN 上均取得 SoTA 表现，InternVL-G 的 Recall@1 在 COCO 图像→文本检索任务中达到 85.0%，在多语言图像→文本检索任务 XTD 中，Recall@10 平均可达 96.6%，显著超越现有方法。\",\"零样本图像字幕生成（Image Captioning）：\",\"InternVL-G 在不使用指令微调的前提下，仅通过 QLLaMA 即可生成高质量图像描述。例如在 COCO 测试集上，zero-shot CIDEr 得分达到 128.2，超越如 BLIP-2、Qwen-VL 等多模态生成模型。\"]},\"310\":{\"h\":\"\",\"t\":[\"InternVL-Chat 在多模态对话基准（如 MME、POPE）上超越了多个 SoTA 模型。比如，在 MME 综合指标上，InternVL-Chat（13B + QLLaMA）达到 1586.4 分，优于 LLaVA-1.5 和 InstructBLIP 等方法。\",\"此外，InternVL 的多模态对话能力还体现在：\",\"VQA 子任务上：GQA 得分达 59.5（优于 LLaVA-13B 的 63.3）；\",\"图像字幕、OCR、视觉推理任务中均表现稳定，兼具理解和生成能力。\"]},\"311\":{\"h\":\"\",\"t\":[\"视觉主干设计选择（InternViT-6B）：\",\"作者在不同模型深度、宽度、MLP 比例等超参数组合上进行对比试验，最终选择了参数约为 5.9B 的 variant 3 作为 InternViT-6B 版本，在计算成本和准确率之间取得了良好平衡。\",\"QLLaMA 的重要性验证：\",\"通过最小化配置（仅训练 MLP 层）进行对比，发现使用 QLLaMA 作为 glue 层明显优于传统 MLP 层或 QFormer，在对话任务（如 MME、OKVQA、GQA）上均有显著提升。例如 MME 得分从 1022.3（无 QLLaMA）提高至 1317.2（使用 QLLaMA 和 Vicuna-13B）。\"]},\"312\":{\"h\":\"总结\",\"t\":[\"InternVL 的实验结果充分证明了其设计策略的有效性：\",\"大型视觉编码器（InternViT-6B）具备极强的感知能力；\",\"QLLaMA 显著提升了视觉-语言对齐与生成能力；\",\"多阶段训练策略（对比 + 生成 + 指令微调）保障了模型的通用性与灵活性；\",\"在图像分类、文本检索、VQA、多模态对话等任务上全面领先于现有开源模型，是当前最具代表性的通用多模态基础模型之一。\"]},\"313\":{\"h\":\"结论\",\"t\":[\"通过将视觉基础模型扩展到 60 亿参数规模（InternViT-6B），并与一个由 LLaMA 初始化的语言中间件（QLLaMA）进行渐进式对齐，InternVL 构建了一个强大且通用的视觉-语言基础模型。借助海量图文数据和多阶段训练策略（对比、生成、微调），InternVL 实现了在图像分类、图文检索、图像描述、VQA、多模态对话等任务上的领先性能，成功弥合了视觉模型与大型语言模型之间的能力与表示鸿沟，推动了多模态大模型的发展。\"]},\"314\":{\"h\":\"详细训练设置(附录内容)\"},\"315\":{\"h\":\"\",\"t\":[\"如表20所示，在此阶段：\",\"图像编码器（InternViT-6B） 采用 BEiT 的初始化方法随机初始化，文本编码器（LLaMA-7B） 则加载多语言 LLaMA-7B 的预训练权重。所有参数均参与训练。\",\"优化器使用 AdamW，超参数为 β1=0.9、β2=0.95，权重衰减 0.1，学习率采用余弦退火策略（图像编码器初始 1e-3，文本编码器 1e-4）。\",\"采用 DropPath 率 0.2，总批量大小 164K，在 640 张 A100 GPU 上训练 175K 步，处理约 287 亿样本。\",\"为提升效率，初期在 196×196 分辨率 下训练并 掩码 50% 图像 token，后期切换至 224×224 分辨率 并取消掩码（最后 5 亿样本）。\"]},\"316\":{\"h\":\"\",\"t\":[\"InternViT-6B 和 QLLaMA 继承第一阶段权重，新增的可学习查询和交叉注意力层 随机初始化。\",\"冻结主干网络，仅训练新增参数，输入分辨率保持 224×224。\",\"优化器使用 AdamW（β1=0.9、β2=0.98，权重衰减 0.05），总批量大小 20K，在 160 张 A100 GPU 上训练 80K 步（含 2K 步热身），峰值学习率 5e-5。\"]},\"317\":{\"h\":\"\",\"t\":[\"提供两种配置：\",\"InternVL-Chat（不含 QLLaMA）\",\"类似 LLaVA-1.5，先以 LGS-558K 数据集 训练 MLP 层，再用 LLaVA-Mix-665K 微调 LLM，各训练 1 轮。\",\"InternVL-Chat（含 QLLaMA）\",\"分两步：先用自定义 SFT 数据训练 MLP 层，再微调 LLM。因数据量扩大，批量大小增至 512。\"]},\"318\":{\"h\":\"\",\"t\":[\"所有参数可训练，分别在 Flickr30K 和 Flickr30K-CN 上微调。\",\"分辨率 364×364，采用 分层学习率衰减（0.9） 和 DropPath 率 0.3。\",\"使用 AdamW，批量大小 1024，训练 10 轮。\"]},\"319\":{\"h\":\"\",\"t\":[\"遵循常规做法：\",\"用 BatchNorm 归一化特征，拼接平均池化的 patch token 和类别 token。\",\"线性分类头使用 SGD 训练 10 轮（批量 1024，峰值学习率 0.2，1 轮热身，无权重衰减），数据增强包括随机裁剪和翻转。\"]},\"320\":{\"h\":\"\",\"t\":[\"表23列出了三种配置的超参数：\",\"线性探测（Linear Probing）\",\"头部调优（Head Tuning）\",\"全参数调优（Full-parameter Tuning）\"]},\"321\":{\"h\":\"书生·万象多模态大模型（InternVL 1.5）\",\"t\":[\"How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites 论文简析\",\"论文链接: How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites\"]},\"322\":{\"h\":\"摘要\",\"t\":[\"InternVL 1.5 是一个开源的多模态大语言模型（MLLM），旨在缩小开源模型与商业多模态模型（如 GPT-4V）之间的性能差距。其核心改进包括以下三点：\",\"强大的视觉编码器：通过持续学习策略优化大规模视觉基础模型 InternViT-6B，提升其视觉理解能力，并使其能够适配不同的语言模型（LLMs）。\",\"动态高分辨率处理：根据输入图像的长宽比和分辨率，将其动态分割为 1 到 40 个 448×448 像素的图块，最高支持 4K 分辨率输入，同时保留全局缩略图以捕捉上下文信息。\",\"高质量双语数据集：精心构建了一个涵盖常见场景和文档图像的双语数据集（中英文问答对），显著提升了模型在 OCR 和中文相关任务中的表现。\",\"实验结果表明，InternVL 1.5 在 18 个多模态基准测试中表现优异，其中 8 项达到领先水平，尤其在 OCR 相关任务中表现突出。其性能与商业模型（如 GPT-4V、Gemini 系列等）相当，部分任务甚至超越商业模型。这一成果为开源多模态模型的发展提供了重要支持。\"]},\"323\":{\"h\":\"简介\",\"t\":[\"研究背景与问题: 大型语言模型（LLMs）在推动通用人工智能（AGI）方面发挥了重要作用，而多模态大型语言模型（MLLMs）进一步扩展了文本与视觉信息的交互能力。然而，开源模型与商业专有模型（如GPT-4V、Gemini系列和Qwen-VL-Max）之间仍存在显著差距，主要体现在三个方面：\",\"参数规模：商业模型通常具有超过1000亿参数，而开源模型的视觉基础模型（VFM）通常仅3亿参数，搭配70亿或130亿参数的LLMs。\",\"图像分辨率：商业模型支持动态分辨率以保留原始宽高比，而开源模型多采用固定分辨率（如336×336或448×448），限制了细节理解能力。\",\"多语言能力：商业模型通过多语言数据集训练，而开源模型主要依赖英语数据，其他语言任务表现较差（如OCR和中文场景理解）。\",\"解决方案与创新: 论文提出InternVL 1.5，通过以下改进缩小差距：\",\"强大的视觉编码器：基于InternViT-6B的持续学习策略，增强视觉理解能力并适配不同LLMs。\",\"动态高分辨率处理：将图像分割为1至40个448×448像素的区块（支持4K分辨率），并添加缩略图以保留全局上下文（见图4）。\",\"高质量双语数据集：涵盖常见场景和文档图像，通过中英文问答对标注，显著提升OCR和中文任务性能（见表1）。\",\"模型优势\",\"灵活分辨率：类似GPT-4V的“低/高”模式，用户可根据任务需求选择分辨率（如低分辨率用于场景描述，高分辨率用于文档分析）。\",\"双语能力：在中文任务中表现优于GPT-4V（见图1）。\",\"强视觉表征：InternViT-6B的大参数规模使其视觉表征能力媲美200亿参数的LLMs，实现多模态能力的协同提升（见图2）。\",\"性能验证: InternVL 1.5在18个多模态基准测试中表现优异，在8个任务中达到SOTA，尤其在OCR相关任务（如TextVQA、ChartQA）中超越商业模型（见表2）。研究团队开源模型权重，以促进MLLM社区发展。\"]},\"324\":{\"h\":\"相关工作\"},\"325\":{\"h\":\"\",\"t\":[\"商业模型在多模态领域占据领先地位，主要代表包括：\",\"GPT-4V（OpenAI）：扩展GPT-4的视觉能力，支持文本和图像输入。\",\"Gemini 系列（Google）：从1.0到1.5版本，支持文本、图像和音频，并扩展至100万tokens的上下文窗口。\",\"Qwen-VL-Plus/Max（阿里）：在无需OCR工具的情况下展现强大的多模态能力。\",\"Claude-3V、HPT Pro、MM1、Step-1V、Grok-1.5V 等新兴模型进一步推动多模态技术的发展。\",\"这些模型的优势在于大规模参数、动态分辨率支持和多语言优化，但通常不开源，限制了研究社区的应用和优化。\"]},\"326\":{\"h\":\"\",\"t\":[\"开源模型在视觉-语言任务中取得显著进展，代表性工作包括：\",\"LLaVA 系列、MiniGPT-4、Qwen-VL、CogVLM 等，主要采用固定分辨率（如336×336或448×448），导致在非常规宽高比或文档理解任务上表现受限。\",\"高分辨率优化方法：\",\"双分支视觉编码器（如LLaVA-HR、DeepSeek-VL），结合低分辨率和高分辨率特征。\",\"分块策略（如UReader），将高分辨率图像分割为多个低分辨率区块处理。\",\"尽管这些方法有所改进，但开源模型在文档、图表和场景文本理解方面仍显著落后于商业模型。\"]},\"327\":{\"h\":\"\",\"t\":[\"VFMs 是 MLLMs 的核心组件，当前研究重点包括：\",\"CLIP-ViT 和 SigLIP 是主流选择，但它们在非互联网图像（如文档）上的表现较差。\",\"混合特征方法（如结合CLIP和DINOv2）提升视觉表征能力。\",\"双编码器设计（如DeepSeek-VL 使用 SigLIP 和 SAM-B）优化不同分辨率输入。\",\"本文提出的 InternViT-6B 通过持续学习策略增强视觉理解能力，并适配不同LLMs，提升模型的泛化性。\",\"商业模型在规模和性能上领先，但开源模型通过高分辨率优化、数据增强和更强的视觉编码器（如InternViT-6B）逐步缩小差距。InternVL 1.5 的创新点在于动态分辨率、双语数据集和持续学习的视觉编码器，使其在OCR和中文任务上表现优异。\"]},\"328\":{\"h\":\"方法\"},\"329\":{\"h\":\"\",\"t\":[\"InternVL 1.5 采用经典的 \\\"ViT-MLP-LLM\\\" 架构（见图3），主要包含以下组件：\",\"视觉编码器：基于 InternViT-6B（45层），通过持续学习优化，支持高分辨率输入。\",\"语言模型：采用 InternLM2-20B（聊天版本），提供强大的语言理解能力。\",\"动态分辨率策略：训练时根据输入图像的宽高比和分辨率，将图像分割为 1~12个448×448区块，测试时可扩展至 40区块（4K分辨率），并引入缩略图保留全局信息。\",\"Token压缩：使用 Pixel Shuffle 操作将视觉Token数量减少至1/4（如448×448图像对应256个Token），提升计算效率。\",\"MLP投影层: 随机初始化\"]},\"330\":{\"h\":\"\",\"t\":[\"现有 MLLM 通常采用对比学习预训练的 ViT 模型作为视觉基础模型。然而，这些 ViT 模型通常在固定低分辨率（如224×224）的互联网爬取图像-文本对上训练，因此在处理高分辨率图像或非互联网来源图像（如文档图像）时性能会下降。\",\"InternViT-6B-448px-V1.2：\",\"为解决这一问题，我们在InternVL 1.2版本中对InternViT-6B模型进行了持续预训练。首先，我们发现倒数第四层的特征在多模态任务中表现最佳，因此直接移除了最后三层的权重，将模型层数从48层缩减至45层。\",\"随后，我们将分辨率从224提升至448，并将其与Nous-Hermes-2-Yi-34B结合。为赋予模型高分辨率处理和OCR能力，我们在训练中同时激活视觉编码器和MLP投影层，使用了混合的图像描述和OCR专用数据集。\",\"InternViT-6B-448px-V1.5：\",\"InternVL 1.5的开发基于InternViT-6B-448px-V1.2的强健基础进一步预训练。在此版本中，训练图像的分辨率从固定的448×448扩展为动态的448×448，其中基础图块大小为448×448，图块数量为1至12个。此外，我们还提升了预训练数据集的规模、质量和多样性，最终使1.5版本模型具备了强大的鲁棒性、OCR能力和高分辨率处理能力。\",\"语言模型从Nous-Hermes-2-Yi-34B更换为InternLM2-20B，但InternViT仍展现出与新语言模型的优秀兼容性和可移植性。这表明，InternViT-6B在MLLM预训练阶段学习到的视觉特征具有广泛适用性，并不依赖于特定的语言模型\"]},\"331\":{\"h\":\"\",\"t\":[\"受UReader启发，我们采用动态高分辨率训练策略，有效适应输入图像的不同分辨率和宽高比。该方法通过灵活分割图像图块，在保留细节信息的同时兼容多样化的图像分辨率。主要步骤如下：\",\"动态宽高比匹配：从35种预设宽高比中选择最接近输入图像的配置，避免过度拉伸。\",\"分块与缩略图：\",\"图像调整至目标分辨率（如800×1300 → 896×1344）后分割为448×448区块。\",\"额外添加448×448缩略图以保留全局上下文。\",\"训练与测试灵活性：训练时最多12区块（3,328 Token），测试时支持40区块（10,496 Token）。\"]},\"332\":{\"h\":\"\",\"t\":[\"预训练数据（53.9% 图像描述 + 32% OCR数据）：\",\"涵盖 Laion-EN/ZH、COYO、GRIT 等通用数据集，以及 Wukong-OCR、Common Crawl PDFs 等大规模OCR数据。\",\"使用PaddleOCR生成中英文文本标注，增强模型文字识别能力。\",\"微调数据：\",\"包括 TextCaps、ShareGPT4V（双语描述）、DocVQA、ChartQA 等任务专用数据。\",\"通过翻译管道（图5）将英文数据转为中文，提升多语言支持。\",\"InternVL 1.5 通过 强视觉编码器、动态分辨率策略 和 双语数据集，显著提升了开源模型在OCR、中文任务和高分辨率场景下的性能，缩小了与商业模型的差距。其模块化设计（如InternViT-6B的兼容性）为后续研究提供了灵活的基础。\"]},\"333\":{\"h\":\"实验\"},\"334\":{\"h\":\"\",\"t\":[\"InternVL 1.5 基于 InternViT-6B（视觉编码器）和 InternLM2-20B-Chat（聊天版语言模型）构建，采用 动态高分辨率策略：\",\"训练阶段：图像分割为 1~12个448×448图块\",\"测试阶段：支持零样本扩展至 40图块（4K分辨率）\",\"两阶段训练：\",\"（1）预训练视觉编码器+MLP投影器\",\"（2）全模型微调（260亿参数）\",\"技术配置：上下文长度4096，响应格式与 LLaVA 1.5 一致，评估工具 VLMEvalKit\"]},\"335\":{\"h\":\"\",\"t\":[\"（1）OCR相关任务\",\"文档理解（DocVQA）、图表解析（ChartQA）、场景文本（TextVQA）等任务表现优异\",\"关键优势：在ChartQA和OCRBench上超越所有商业模型（如GPT-4V、Gemini系列）\",\"（2）通用多模态任务\",\"中文能力突出：在MMBench-CN、CCBench等中文基准上大幅领先\",\"幻觉控制：HallusionBench分数最高\",\"科学理解：AI2D科学图表任务表现接近商业模型\",\"（3）数学推理\",\"MathVista基准：超越GPT-4V，展示强大的数学-视觉联合推理能力\",\"（4）多轮对话\",\"ConvBench评估显示：在开源模型中领先，但较GPT-4V仍有差距\"]},\"336\":{\"h\":\"\",\"t\":[\"（1）视觉编码器规模的影响\",\"实验证明：大语言模型（如34B参数）需搭配大规模视觉编码器（如6B参数），才能更好处理复杂多模态任务\",\"（2）动态分辨率的作用\",\"OCR任务（如DocVQA）：高分辨率（更多图块）显著提升性能\",\"通用任务（如MMMU）：分辨率过高可能略微降低效果\",\"灵活性：模型可自适应调整分辨率，平衡效率与精度\"]},\"337\":{\"h\":\"结论\",\"t\":[\"InternVL 1.5作为开源多模态大语言模型，通过持续优化的视觉编码器InternViT-6B、创新的动态高分辨率处理策略（支持4K输入）和高质量双语数据集，在18个多模态基准测试中展现出媲美商业模型的性能，尤其在OCR相关任务（TextVQA/ChartQA/DocVQA）和中文理解方面表现突出，部分能力甚至超越GPT-4V，其开源的模型权重和研究方法为多模态AI发展提供了重要基准，未来将持续优化对话和推理能力，推动技术民主化进程。\"]},\"338\":{\"h\":\"LLaVA 1.0(Large Language and Vision Assistant)\",\"t\":[\"LLaVA 1.0 : Large Language and Vision Assistant 论文简析\",\"论文链接: https://arxiv.org/abs/2304.08485 代码链接: https://github.com/haotian-liu/LLaVA\"]},\"339\":{\"h\":\"背景\",\"t\":[\"此前，大型语言模型（如 GPT-3、LLaMA）通过机器生成的指令数据进行调优，显著提升了零样本和少样本泛化能力（如 InstructGPT、FLAN-T5 等）。\",\"InstructGPT 是由 OpenAI 提出的一种通过 人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF） 来实现 指令调优（Instruction Tuning） 的方法。 其目标是让预训练语言模型（如 GPT-3）更好地理解和执行用户给出的自然语言指令，从而提升其在各种任务上的泛化能力，尤其是零样本（zero-shot）或多任务场景下的表现。 InstructGPT 的核心思想是：通过结合人工标注数据和强化学习，引导语言模型更好地遵循用户指令，并在多种任务上表现良好。 它不是单纯地“记住”训练数据中的例子，而是学会根据用户指令理解任务意图并生成合适的结果。 InstructGPT 的 instruction tuning 实现主要包括以下三个关键阶段： 步骤1：收集指令-响应对（Instruction-Following Data）\",\"OpenAI 收集了大量的人类编写的 指令（instruction） 和对应的 期望输出（response）。\",\"这些指令可以是开放式的（如“写一个关于猫的故事”），也可以是特定任务（如“翻译成中文”、“总结文章”）。\",\"数据来源包括：\",\"用户提交给 GPT-3 的 API 请求；\",\"内部标注人员手动构造的示例。\",\"目标：构建一个多样化的指令-响应数据集，用于训练或评估模型。\",\"步骤2：训练监督模型（Supervised Policy）\",\"使用标注好的指令-响应数据对模型进行微调（fine-tune）。\",\"输入是一个指令，输出是模型应该生成的响应。\",\"模型结构与原始 GPT-3 相同，只是参数经过调整以更好响应指令。\",\"步骤3：基于人类反馈的强化学习（RLHF）, 这是 InstructGPT 最具创新性的部分。具体分为三步:\",\"收集人类偏好数据\",\"对于同一个指令，让模型生成多个不同的回答；\",\"让人类标注者对这些回答进行排序，选出他们认为最好的答案。\",\"训练奖励模型（Reward Model）\",\"使用上述人类偏好数据，训练一个奖励模型（Reward Model），该模型的输入是一对（指令 + 回答），输出是对这个回答的评分（score）。\",\"奖励模型的目标是模拟人类的偏好判断。\",\"使用强化学习优化策略（Policy Optimization）\",\"使用 PPO（Proximal Policy Optimization） 等强化学习算法，以奖励模型为“环境”，进一步微调模型。\",\"在训练过程中，模型尝试生成尽可能高奖励的回答，从而更贴近人类期望。\",\"《 Visual Instruction Tuning 》 这篇论文首次尝试使用仅支持文本输入的 GPT-4 / ChatGPT 来生成图文结合的指令响应对（instruction-following data） ，并用这些数据训练一个端到端的视觉语言模型 LLaVA。\",\"论文核心创新点: 这是第一个系统性地将 NLP 中的指令调优思想引入多模态领域的研究。\"]},\"340\":{\"h\":\"方法\",\"t\":[\"作者将模型训练分为两个阶段 ：\",\"预训练阶段（Feature Alignment Pre-training）: 让视觉编码器提取的图像特征与语言模型的词嵌入空间对齐 , 也就是说：让模型理解图像和文本之间的语义关系, 这是后续指令调优的基础。\",\"微调阶段（End-to-End Fine-tuning）：在预训练的基础上，进一步训练模型理解和执行更复杂的视觉指令任务。\",\"多轮对话能力；\",\"复杂推理能力；\",\"科学问答等实际应用任务。\"]},\"341\":{\"h\":\"预训练\",\"t\":[\"预训练是 LLaVA 模型训练的第一阶段，目标让视觉编码器输出的图像特征与语言模型的词向量空间对齐 ，使得后续指令调优时，模型可以更好地理解和生成图文结合的内容。\",\"作者使用的是大规模图文对数据集 CC3M（Conceptual Captions 3M） ，包含约 300 万条图文对。\",\"为了提升数据质量，进行了以下筛选： 名词短语过滤（Noun Phrase Filtering）\",\"使用 Spacy 提取每条 caption 中的名词短语；\",\"统计每个名词短语出现的频率；\",\"去除频率小于 3 的短语（避免罕见组合）；\",\"对于频率大于 100 的短语，只保留最多 100 条描述（防止过拟合）；\",\"最终得到约 595,000 条高质量图文对 。\",\"数据构建方式: 为了模拟用户提问和模型回答的形式，将这些图文对转换为如下格式：\",\"Human: [指令] [图像描述] Assistant: [详细描述]\",\"其中：\",\"[指令] ：如“请描述这张图片。”、“图中有什么？” [图像描述] ：来自 caption 或 bounding box 的文本化表示； [详细描述] ：期望的回答，通常是图像内容的全面视觉描述。\",\"Caption: 图像的文字描述，从多个角度描述图像内容 , 如: \\\"A group of people standing outside of a black vehicle with various luggage.\\\" Bounding Box: 标注图像中的物体及其位置 , 如: person:[0.681, 0.242, 0.774, 0.694], backpack:[0.384, 0.696, 0.485, 0.914] .\",\"模型结构:\",\"视觉编码器 ：CLIP ViT-L/14（预训练好的）\",\"语言模型 ：Vicuna（基于 LLaMA 的指令调优版本）\",\"投影层 ：一个简单的线性层，连接视觉特征和语言嵌入空间\",\"LLaVA模型结构\",\"训练流程:\",\"输入图像 : 使用 CLIP 视觉编码器提取图像特征 。\",\"投影层 : 将 转换为语言模型可用的 token 序列 。\",\"训练目标: 使用交叉熵损失函数，最小化语言模型输出与真实答案之间的差异 。\",\"仅更新投影矩阵 ，保持视觉编码器和语言模型参数冻结。这个阶段相当于在语言模型的词空间中“训练出一个能看懂图的视觉分词器”。\",\"通过这个阶段训练后，模型已经具备基本的视觉理解能力，即：\",\"可以根据图像描述生成合理的文字解释；\",\"实现了图像与语言之间的初步语义对齐；\",\"为下一阶段的端到端微调提供了良好的初始化。\",\"虽然还不能执行复杂的推理任务，但已经可以处理基本的图文问答任务。\"]},\"342\":{\"h\":\"微调\",\"t\":[\"微调过程 是 LLaVA 模型训练的第二阶段，目标是让模型在预训练的基础上进一步掌握多模态指令理解与复杂推理能力 ，具体包括：\",\"支持多轮视觉对话（Multimodal Chat）\",\"理解并回答科学类问题（如 ScienceQA 数据集）\",\"执行复杂的视觉推理任务\",\"具备跨模态交互能力（图像 + 文本）\",\"这是实现“通用视觉助手”的关键一步。\",\"微调阶段使用的是作者自己构建的高质量多模态指令数据集：\",\"名称：LLaVA-Instruct-158K\",\"包含约 158,000 条图文对\",\"分为三种响应类型：\",\"对话型（Conversation） ：58,000 条\",\"详细描述型（Detailed Description） ：23,000 条\",\"复杂推理型（Complex Reasoning） ：77,000 条\",\"这些数据由 GPT-4 / ChatGPT 自动生成，涵盖多种任务类型，具有高度多样性和挑战性。\",\"微调阶段的数据组织方式如下：\",\"Xsystem-message <STOP> Human: X1instruct <STOP> Assistant: X1a <STOP> Human: X2instruct <STOP> Assistant: X2a <STOP> ...\",\"其中：\",\"Xsystem-message：系统提示语（如：“你是一个视觉助手”）； Xinstruct：用户提问或指令； Xa：期望的回答； <STOP>：分隔符，表示输入结束，开始输出回答。\",\"模型结构:\",\"视觉编码器 ：CLIP ViT-L/14（保持冻结）\",\"语言模型 ：Vicuna（基于 LLaMA 的指令调优版本）\",\"投影层 ：连接图像特征和语言嵌入空间的线性层\",\"训练流程:\",\"输入图像 : 使用 CLIP 提取图像特征 \",\"投影层 : 使用可训练的投影矩阵 将图像特征 转换为语言嵌入 \",\"训练目标: 最小化语言模型输出与真实答案之间的交叉熵损失。\",\"微调时保持视觉编码器参数不变，只更新投影层 和语言模型 Vicuna 的参数。\",\"论文中重点测试了以下两个应用场景：\",\"多模态聊天机器人（Multimodal Chatbot）： 使用 LLaVA-Instruct-158K 数据集进行训练；\",\"其中：\",\"对话型问答为多轮对话；\",\"其他两类为单轮对话；\",\"数据均匀采样，训练出一个能自然理解图像内容、并进行视觉对话的 AI 助手。\",\"科学问答（Science QA）：在 ScienceQA 数据集上进行迁移学习；\",\"每个问题包含文本或图像上下文；\",\"助手需要生成推理过程，并从多个选项中选择正确答案；\",\"在这个任务上，LLaVA 达到了 90.92% 准确率 ；\",\"当与 GPT-4 联合推理时，准确率达到 92.53% ，刷新该数据集 SOTA。\"]},\"343\":{\"h\":\"联合 GPT-4 的推理机制（Ensemble with GPT-4）\",\"t\":[\"作者还提出了一种创新方法，将 LLaVA 与 GPT-4 联合使用：\",\"方法一：GPT-4 补充\",\"当 GPT-4 无法回答时，使用 LLaVA 的预测结果；\",\"效果：准确率提升不大（仅 0.05%），说明 LLaVA 已经接近其上限。\",\"方法二：GPT-4 判断者（Judge）\",\"当 LLaVA 和 GPT-4 输出不一致时，再次用 GPT-4 做判断；\",\"效果：显著提升表现，最终准确率达到 92.53% ，刷新 ScienceQA 数据集的 SOTA。\",\"这是首次尝试将大语言模型用于模型集成（model ensemble）的研究。\"]},\"344\":{\"h\":\"ablation study（消融实验）\",\"t\":[\"论文中还进行了多项 ablation 实验，以分析不同训练策略的影响：\",\"训练策略\",\"准确率变化\",\"不做预训练\",\"-5.11%\",\"仅使用最后一层视觉特征\",\"-0.96%\",\"先生成答案再推理\",\"-1.15%\",\"使用较小的 7B 模型\",\"-1.08%\",\"这些实验表明：\",\"预训练阶段非常关键；\",\"使用倒数第二层视觉特征更有利于细节理解；\",\"推理优先（Reasoning First）有助于加快收敛；\",\"模型规模对性能有显著影响。\"]},\"345\":{\"h\":\"补充\"},\"346\":{\"h\":\"辨析 instruction tuning 和 prompt tuning\",\"t\":[\"Instruction Tuning（指令调优） 和 Prompt Tuning（提示调优） 是两种用于提升预训练语言模型（LLM）或视觉语言模型性能的技术，但它们的目标、方法和应用场景有显著区别。以下是两者的主要区别：\",\"定义与核心思想\",\"类别\",\"Instruction Tuning（指令调优）\",\"Prompt Tuning（提示调优）\",\"定义\",\"通过大量“指令-响应”对微调模型，使其更好地理解和执行用户给出的自然语言指令。\",\"在输入中添加可学习的前缀（prefix）或前缀/后缀（prompt），引导模型生成特定任务的结果，而不需要改变整个模型参数。\",\"核心思想\",\"模型要理解并遵循人类语言中的任务描述（如“总结一下这篇文章”）。\",\"模型通过在输入前后插入一些可训练的提示词来“唤醒”其已有的知识，完成特定任务。\",\"训练方式\",\"类别\",\"Instruction Tuning\",\"Prompt Tuning\",\"是否修改模型结构\",\"否（通常保留原始结构）\",\"否\",\"是否更新全部参数\",\"是（微调整个模型参数）\",\"否（仅更新插入的 prompt 参数，其余参数冻结）\",\"数据需求\",\"需要大量人工或机器生成的“指令-响应”对\",\"不需要额外标注数据，直接使用原始任务描述\",\"训练目标\",\"提升模型在各种任务上的泛化能力，尤其是零样本/少样本任务迁移\",\"让固定模型适应新任务，利用已有知识进行推理\",\"应用场景举例\",\"类别\",\"示例场景\",\"Instruction Tuning\",\"ChatGPT、InstructGPT、FLAN-T5、LLaVA（视觉+语言）等，能根据用户指令回答问题、写故事、编程、推理等。\",\"Prompt Tuning\",\"使用 [PROMPT] 前缀让 BERT 回答 QA 问题、分类任务；在图像识别中加入 learnable prefix 来适配不同类别。\",\"优缺点对比:\",\"对比维度\",\"Instruction Tuning\",\"Prompt Tuning\",\"优点\",\"- 更强的任务泛化能力- 更贴近真实用户交互- 可用于多模态任务\",\"- 参数效率高（只训练少量 prompt）- 可复用已有大模型权重\",\"缺点\",\"- 数据依赖性强（需要大量高质量指令数据）- 微调成本高（需训练整个模型）\",\"- 表达能力受限于 prompt 的设计- 泛化性不如 instruction tuning\",\"总结一句话： Instruction Tuning 是教会模型“听懂人话”，按指令做事；Prompt Tuning 是引导模型“激活已有知识”，通过提示词让它自己做任务。\"]},\"347\":{\"h\":\"MoCo 论文\",\"t\":[\"Momentum Contrast for Unsupervised Visual Representation Learning 论文简析\",\"论文链接: Momentum Contrast for Unsupervised Visual Representation Learning 代码链接: https://github.com/facebookresearch/moco\"]},\"348\":{\"h\":\"Introduction\",\"t\":[\"对比学习从2019年开始到现在一直都比较火，Moco是视觉领域使用对比学习一个里程碑的工作。\",\"Moco作为一个无监督的表征学习工作，不仅在分类任务上逼近了有监督的基线模型，在其他任务，如检测、分割、人体关键点检测上都超越了有监督的预训练模型，也就是ImageNet上的预训练模型；\",\"Moco证明了一点，无监督学习真的可行，我们并不需要大量标注好的数据；\"]},\"349\":{\"h\":\"What is contrast learning?\",\"t\":[\"首先说对比学习想要做到什么呢？我们现在有三张图，第一张图是人高兴，第二张图片是人悲伤，第三张图片是狗。\",\"我们想得到一个结果，就是我们不需要知道前两张图片是人这个类别，不需要知道第三张图片是狗这个类别。但是我们需要知道前两张图片是一个类别，第三张图片不是一个类别。\",\"换句话说，我们现在把这三张图片输入一个模型，得到三个表征，我们需要让这三个表征在特征空间中，前两张图片的表征距离比较近，第三张图片和它们的距离比较远。\",\"一句话说，我们希望在特征空间里，同一个类别的物体处于相邻的区域，不同类别的物体处于不相邻的区域。\",\"在这个过程中，我们需要知道的是，我们并没有用到标签信息，我们不需要知道第一张和第二张图片是人，第三张是狗。\",\"但是我们用到了另外一种信息，就是第一张图片和第二张图片是同一个类别，第三张图片不是同一个类别的信息。这其实也是一种标签信息。\",\"不过这种标签信息，我们可以使用一些代理任务，巧妙构造出来，而不需要人为地去标注这种标签信息。这些代理任务，会去定义一些规则，这些规则可以去定义哪些图片是相似的，哪些图片是不相似的，从而可以提供一些监督信号给到模型去训练。这个过程其实也是自监督训练的一个过程。\"]},\"350\":{\"h\":\"instance discrimination task\",\"t\":[\"一个最经典的代理任务就是：instance discrimination，叫做个体判别。\",\"这个代理任务是指，如果我们有一个没有标注的数据集，里面有n个图片。\",\"从这个数据集中，我们随机选择一个图片 ，对这个图片做随机裁剪（或者其他的数据增广操作，我们称之为transformation），从而得到另外两张图；\",\"一个是 一个是 ，这样我们会得到两个不太一样的照片。但是由于这两张图片是从同一个图片经过某种变化得到的，语义信息不应该发生变化。所以这两张图片就可以称之为正样本，也就是同一个类别的图片。\",\"这个代理任务，同时认为，这个数据集中剩余的所有图片都是负样本。\",\"为什么叫做个体判别呢？因为它认为每个图片自成一个类别，剩余的图片都不是同一个类别。\",\"这个粒度其实是很细，你在图片分类的时候很多照片是同一个类别，其余的照片又分为了很多类别，所以个体判别这个代理任务经过模型训练，表征会很细。\",\"对于ImageNet这个数据集来说，如果是个体判别任务，不是一千个类别，而是100多万个类别。\",\"所以个体判别这个代理任务定义了什么是正样本，什么是负样本，接下来就很简单了，我们只需要经过模型，然后使用一个对比学习的函数去训练模型就可以了，比如说NCEloss。\",\"在这个过程中，其实有一个很有意思的点，就是代理任务是多样性的，是很灵活的。只要你能够得到一个判断正样本和负样本的规律，后续的损失函数之类的训练就很常规了。\",\"比如说在视频领域，同一个视频里的任意两帧是正样本，其他视频里的帧是负样本；\"]},\"351\":{\"h\":\"Momentum Contrast\",\"t\":[\"Moco 这个名字就是来源于前两个单词的前两个字母，即基于动量的对比学习。\",\"动量是一种加权移动平均:\",\" 是上一个时刻的输出， 是动量超参数， 是当前时刻的输入。\",\"说白了，就是不想让当前时刻的输出只是依赖于当前时刻的输入，还希望和之前时刻的输出有关系。动量这个超参数是 的一个参数；如果 趋近于 ，那么 的改变会非常缓慢，因为 趋近于零。\",\"Moco 就是利用这个动量的特性，去缓慢地更新编码器，从而让中间学习到的字典特征尽可能保持一致（这句话没看懂没关系，一会详细讲）。\"]},\"352\":{\"h\":\"Abstract\",\"t\":[\"Moco 把对比学习看成了是一个字典查询的过程，构建了一个动态的字典。这个动态的字典分为两个部分：第一部分是一个队列，第二部分是一个移动平均的编码器。\",\"队列里的样本不需要进行梯度回传，因此我们可以往队列里放入很多负样本，从而让字典的规模变得很大。\",\"为什么还要使用一个移动平均的编码器呢？这是为了让字典里的特征尽可能保持一致。\",\"在训练过程中发现，拥有一个规模大且特征较为一致的字典，能让无监督的对比学习取得很好的效果。\",\"从实验结果来看，在 ImageNet 数据集上，如果采用 线性评估（Linear Probing） 进行测试，Moco 可以取得和之前最优的无监督方法相近甚至更好的结果。\",\"线性评估指的是，先预训练好一个骨干模型，然后将这个骨干网络参数冻结，只训练最后的全连接层，再查看在不同数据集上的表现结果。这样做其实类似于把骨干网络当成一个特征提取器，仅从其中提取特征，这和使用 ResNet 作为特征提取器的方式差不多。\",\"Moco 一个很大的优势在于，学习到的特征在下游任务上具有很好的迁移性。我们看重无监督学习的优点，就是它可以从大量未标注的数据上学习到特征，并迁移到标注数据较少的任务上。\",\"Moco 在 7 个下游任务（如分割、检测等）上超越了之前的有监督预训练模型。\"]},\"353\":{\"h\":\"Introduction\",\"t\":[\"GPT和BERT，已经证明无监督学习在NLP任务上是行得通的。但是在CV领域，有监督预训练还是占据主导地位；\",\"之前也有很多优秀的无监督工作，但是表现都会比有监督要差，作者认为这是因为CV领域和NLP领域的原始信号空间不同。\",\"对于NLP领域来说，它们是离散的信号，也就是原始的信号空间是由单词组成，或者更细一点，是由单词词缀组成的，所以我们可以很容易地去建立一个字典，然后让模型去学习特征。那么字典中的每个key就是一个类别，我们可以根据这个类别去学习模型（比如BERT最后进行的softmax操作，不就是分类操作吗）\",\"但是对于CV领域来讲，情况完全不一样。CV领域的信号是在一个连续而且高维的空间，它并不像单词那样有很强的语义信息，也没有浓缩得那么简洁；所以CV领域并不适合去建立一个字典来学习模型；如果没有这个字典，无监督就很难去建模。因此，在CV领域，无监督学习的表现往往不如有监督学习。\",\"在之前有很多优秀的对比学习工作，都可以归纳为一种字典查询的工作。\",\"我们接着来看图：\",\"两个编码器，一个是E11，一个是E12；然后我们将图片x1经过数据增强T1得到图片X11，再经过E11这个编码器，得到图片表征f11；同理，图片x1经过数据增强T2得到图片x12，然后经过E12这个编码器，得到表征f12。\",\"我们把X11这个图片叫做 anchor（锚点），x12叫做x11的正样本。\",\"什么是负样本呢？就是数据集中剩余的所有图片都是负样本。那么负样本走哪个编码器呢？走的是E12这个编码器，因为正样本和负样本都是相对于锚点来说的，所以正样本和负样本要走同一个编码器，从而让特征的获取过程保持一致性。于是，负样本x2、x3、x4等等也经过E12得到了真正的负样本表征f2、f3、fn。\",\"我们把f11叫做 query，把f12、f2、f3、fn叫做 key。\",\"对比学习的过程就是想要在特征空间里，让正样本的key和query距离近，其余的key离query远。\",\"我们其实可以把key集合看成字典。那么对比学习的过程，就是想得到一个模型，让query在字典中与自己匹配的正样本更近。\",\"如果把对比学习的过程看成一个动态字典的过程，若想要得到比较好的效果，那么字典最好需要满足两个条件：第一个是字典足够大，第二个是在训练的时候尽量保持一致性。\",\"首先，我们在做对比学习的时候，是一个batch一个batch地去做，所以如果key这个字典足够大，那么从中抽样的可能性组合就很多，模型的泛化性就很强。如果字典很小，泛化性就不足，相当于数据量不够。\",\"其次，保持一致性是因为我们需要字典中的特征尽可能使用同一个或者相近的编码器进行表征。因为如果不这样做，模型可能就只会学习到和query使用同样编码器的那个key，导致模型泛化性不足，走了捷径。\",\"所以Moco要做的就是：在对比学习框架中，提供一个又大又一致的字典；框架图如下：\",\"大字典是怎么做到的：维护一个队列，把每次训练的 batch-size 和队列大小分离开；具体来说就是这个队列可以很大，但是我们每次更新这个队列，是一点点地更新的，也就是说当我们用一个很小的 batchsize 时，把当前 batch 中的特征加入队列，将最老的 batch-size 的特征从队列中抽离；这样我们的队列就可以设置得很大，比如几万。通过这种方式，使用一个 GPU 也可以很好地训练模型。\",\"那么一致性是如何做到的？刚才说了，每次都是使用新的编码器更新 batch 大小的队列特征，除此之外的特征都是使用之前的编码器得到的，这样不就不一致了吗？这时使用动量更新即可。我们最开始右边分支的编码器是由左边初始化而来，后续更新时对右边这个编码器的参数进行动量更新，让 m 足够大，确保右边编码器更新得非常缓慢，从公式来说，就是这个图：\",\"可以看到，右边编码器会被之前的 编码，和当前时刻的 编码影响。当 足够大，无限接近于 时，就可以认为其无限被 控制，更新会非常缓慢。\",\"Moco 只是建立中间模型的一种方式，它非常灵活，可以和很多代理任务结合，这里使用的是之前讲过的个体判别任务。\",\"无监督学习最大的一个卖点，就是我们的模型在大量无标注的数据集上进行训练之后，得到的特征可以很好地迁移到下游任务中（比如标注数据很少的任务）。\"]},\"354\":{\"h\":\"Conclusion\",\"t\":[\"Moco 论文在 ImageNet 数据集上取得了很好的结果，在 Facebook 自家的数据集上同样效果良好，但提升幅度不大。当数据集规模从 100 万增长到 10 亿时，提升依然不明显。作者认为大规模数据没有被充分利用，可能采用一个更好的代理任务会取得更好的效果。所以作者提出，除了个体判别这个任务外，有没有可能将 MoCo 和 mask encoded 任务结合起来，也就是类似 BERT 的操作，使用 MLM（Masked Language Modeling） 自监督的方式进行学习。（这不就是 MAE 模型吗）\",\"开头提到过 CV 和 NLP 的信号空间不一致，直接照搬 BERT 的方法可能行不通，具体可参考 MAE 模型。\"]},\"355\":{\"h\":\"Related Work\",\"t\":[\"一般来说，自监督学习的方法可以从两个方面来进行优化或创新：\",\"在损失函数上做文章：设计或改进对比损失函数，使得模型能够更有效地学习到有判别性的表示。\",\"在代理任务上做文章：通过设计合理的预训练任务（如图像重建、上下文预测等）引导模型学习有用的特征。\",\"💡 注解：自监督学习其实是一种特殊形式的无监督学习，通过人为构造“标签”来训练模型。\",\"损失函数:\",\"NCE（Noise Contrastive Estimation）损失函数：\",\"最初用于语言模型（如word2vec）中，它的基本思想是把一个“超级大的多分类问题”（即从大量候选中选出正确样本）转化为一系列“二分类问题”（即判断某个样本是否是正样本）。\",\"因为直接在超大类别空间上做 softmax 计算代价太高，所以 NCE 提供了一种更可行的替代方法。\",\"类似于 word2vec 中的 negative sampling 技术。\",\"InfoNCE：\",\"是 NCE 的一种改进或变体，被广泛应用于对比学习中（如MoCo、SimCLR）。\",\"它的目标是在一个由一个正样本和多个负样本构成的集合中，最大化正样本的得分，使得模型能够区分正负样本对。\",\"温度系数（temperature hyperparameter）：\",\"在 InfoNCE 的 softmax 函数中，通常会加入一个 温度超参数 ，用于调节 logits（相似度）的“尖锐程度”：\",\" 越小，softmax 越尖锐，模型对正负样本差异更加敏感；\",\" 越大，softmax 趋于平滑，训练更稳定但区分度下降。\",\"在看 InfoNCE 的损失函数的时候，首先从 softmax 看起，这个是softmax的公式:\",\"然后我们加一个 就是 交叉熵损失函数：\",\"这个公式其实可以直接用在对比学习中。\",\"什么意思呢？ 交叉熵是一个多分类问题的损失函数，一个one-hot向量，和我真实输出做一个损失，目标是为了让真正标签的输出尽可能的大。\",\"那么有一个问题，如果把这个损失函数，直接套到对比学习中去，那么是什么意义呢？\",\"比如 ImageNet 100万个图片，那么我当前图片经过数据增强之后，经过编码器1得到了 锚点特征，经过编码器2得到了正样本，也就是我的 ground-truth；\",\"那么除了我当前这个图片外，其余100万 - 1个图片经过编码器2得到的表征都是负样本，也就是会得到这样一个向量：\",\"1 0 0 0 （1个1 , 100万 - 1个0）\",\"在这个向量上做交叉熵，其实就可以用在对比学习上。\",\"但是这样做 softmax 计算量太大了，像 BERT 这种模型，也就几万个类别，处理起来没啥问题，几百万个类别就太难了。\",\"这个时候 NCE（Noise Contrastive Estimation） 就是一种很好的解决方式，将问题转化为一个二分类问题，也就是现在只有两个类别，一个是正常样本，除此之外的都是噪声样本。\",\"但是这样做的逻辑不太清晰，所以 InfoNCE 就应运而生了。\",\"与其在整个数据集上计算损失，不如抽样一部分数据来计算损失。如果选取的抽样部分过少，就没什么意义，无法模拟整个数据集，所以抽样的部分还是要大一点。此时，字典的大小就很重要，也就是字典的大小就是分母下方的类别数量；在这个过程中，InfoNCE 把 NCE 的一系列二分类问题又转为了多分类问题。\",\" 就是我们的 query 表征，也就是 锚点 那个图片的特征， 就是正样本，分母累加那里的 ，就是我们的负样本数量，分类累加了 项，因为有 个负样本和一个正样本。\",\"温度参数 （希腊字母读音为 \\\"tao\\\"），在蒸馏相关内容里其实提到过，如果 很大，那么 softmax 分布会很平滑，看不出区别，就是把所有的负样本一视同仁，导致模型学习没有轻重；如果 很小，分布会更尖锐，会让模型只关注那个困难的负样本，其实那些负样本很有可能是潜在的正样本，如果模型过度关注这个困难的负样本，会导致模型很难收敛，或者 学习 到的特征不太好泛化。\",\"去除这个温度超参数，InfoNCE 本质就是一个交叉熵损失函数，只不过类别和所有样本相比，做了个近似，进行了一次随机抽样，抽样数量就是字典大小。Moco 伪代码里的 InfoNCE 直接使用的就是交叉熵损失函数代码。\"]},\"356\":{\"h\":\"Detail\",\"t\":[\"有个细节，为什么使用队列这种数据结构存储字典呢？\",\"因为先进先出，每次一个 batch 进来，最老的那个部分 batch 数据会出去，这部分数据是过时的，从而能够保持队列中的特征尽可能的 一致性。\",\"另一个细节：\",\"第二个分支不能随着走这一支的样本使用梯度回传进行更新，为什么呢？因为如果这样做了，第二个分支的编码器就更新得太快了，这些特征，我们是放到字典中去的，就会导致特征 不一致。\",\"为什么第二个分支不直接不更新，而是采用缓慢更新呢？（我自己理解，如果第二个分支一直不变，由于正样本的定义规则是经过编码器之后所在的语义空间才为正样本。在模型训练时可能会出现问题，因为到后来第一个分支和第二个分支编码器差距越来越大，原本是正样本的，损失也会很大，导致模型很难训练。）\",\"两个贡献：\",\"一个是构建很大的字典：计算损失的时候使用多分类，能够很近似在整个数据集上做的多分类损失。\",\"一个是保证字典内特征一致性，使用动量更新。\",\"需要注意的一点：就是 InfoNCE 损失计算的是整个字典做多分类。minibatch 大小和字典大小剥离开，batch 可以设置为 256，然后进来 256 个样本，每个样本都需要做一个 锚点，走一遍对比学习的流程。\",\"动量设置为了 0.99，这个值 很大 了。字典大小是 65536。\",\"在 Moco 之前的工作中， 字典大小和字典特征一致性经常不能同时满足。\"]},\"357\":{\"h\":\"Preview Work\",\"t\":[\"端到端的框架：\",\"端到端的框架 就是两个编码器都可以通过梯度回传进行更新，因为 和 都是从同一个 batch 中来的，我们通过一次 forward 就可以拿到所有样本的特征，我们直接梯度回传就可以了。这要求 batch 大小要足够的大，那么 InfoNCE 才能起作用，做到一个近似的全部数据集的多分类。SIMCLR 就是这个端到端的框架。这样字典是高度一致的。在这种情况下，batch 大小和字典大小是等价的。SIMCLR 就是用了 8192 作为 batch 大小。\",\"另一流派，更关注 字典的大，然后牺牲一些一致性，就是 memory bank；在这个流派只有一个编码器，就是 query 的编码器，可以进行梯度回传进行更新。对于 key 这边，是没有一个单独的编码器。\",\"memory bank 就是把整个数据集的特征，都存到了一起。对于 ImageNet 来说，这里就是 128 万个特征（作者说到，每个特征 128 维度，只需要 600M 的空间，还好。）\",\"然后每次训练的时候，从 memory bank 中随机抽样字典大小就可以了。右边的编码是在线下执行。\",\"在执行的时候，比如字典大小是 3，那么抽出三个来，左边做一次梯度回传之后，我们把字典中的 3 个用新的编码器做一个编码，放回到 memory bank 中去。\",\"（首先，我认为为了保持正样本的定义，肯定得更新样本特征）\",\"因为你这个更新操作，导致字典内编码特征不一致。\"]},\"358\":{\"h\":\"Code Implementation\"},\"359\":{\"h\":\"Train Code\",\"t\":[\"MoCo 训练代码如下所示:\",\"def main_worker(args): # 1. init MoCo Model model = deeplearning.cross_image_ssl.moco.builder.MoCo( models.__dict__[args.arch], args.moco_dim, args.moco_k, args.moco_m, args.moco_t, args.mlp, ) # 2. define loss function (criterion) and optimizer criterion = nn.CrossEntropyLoss().cuda(args.gpu) optimizer = torch.optim.SGD( model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay, ) # 3. Data loading code normalize = transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] ) augmentation = [ transforms.RandomResizedCrop(224, scale=(0.2, 1.0)), transforms.RandomGrayscale(p=0.2), transforms.ColorJitter(0.4, 0.4, 0.4, 0.4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize, ] train_dataset = datasets.ImageFolder( traindir, deeplearning.cross_image_ssl.moco.loader.TwoCropsTransform( transforms.Compose(augmentation) ), ) train_loader = torch.utils.data.DataLoader( train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=args.workers, pin_memory=True, sampler=train_sampler, drop_last=True, ) for epoch in range(args.start_epoch, args.epochs): # 4. train for one epoch train(train_loader, model, criterion, optimizer, epoch, args)\",\"这里的重点是 TwoCropsTransform 这个增强方法，它的作用是对一个样本进行两次增强，得到两个样本。\",\"class TwoCropsTransform: \\\"\\\"\\\"Take two random crops of one image as the query and key.\\\"\\\"\\\" def __init__(self, base_transform) -> None: self.base_transform = base_transform def __call__(self, x): q = self.base_transform(x) k = self.base_transform(x) return [q, k]\",\"此时 train_loader 返回的每个 batch 维度为: [2, batch_size, C, H, W] , 其中 batch[0] 代表 query , batch[1] 代表 positive key。\",\"def train(train_loader, model, criterion, optimizer, epoch, args) -> None: for i, (images, _) in enumerate(train_loader): # 1. compute output output, target = model(im_q=images[0], im_k=images[1]) loss = criterion(output, target) # 2. compute gradient and do SGD step optimizer.zero_grad() loss.backward() optimizer.step()\"]},\"360\":{\"h\":\"Model Implementation\"},\"361\":{\"h\":\"Model Init\",\"t\":[\"模型初始化代码如下所示:\",\"class MoCo(nn.Module): def __init__( self, base_encoder, # ResNet 模型 dim: int = 128, K: int = 65536, # 队列大小/字典大小/负样本数量 m: float = 0.999, # 动量更新参数 T: float = 0.07, # 温度参数 mlp: bool = False, ) -> None: \\\"\\\"\\\" dim: feature dimension (default: 128) K: queue size; number of negative keys (default: 65536) m: moco momentum of updating key encoder (default: 0.999) T: softmax temperature (default: 0.07) \\\"\\\"\\\" super(MoCo, self).__init__() self.K = K self.m = m self.T = T # 1. create the encoders: num_classes is the output fc dimension self.encoder_q = base_encoder(num_classes=dim) self.encoder_k = base_encoder(num_classes=dim) # 2. key encoder参数使用query encoder进行初始化，同时key encoder不参与梯度运算 for param_q, param_k in zip( self.encoder_q.parameters(), self.encoder_k.parameters() ): param_k.data.copy_(param_q.data) # initialize param_k.requires_grad = False # not update by gradient # 3. create the queue/dictionary and pointer self.register_buffer(\\\"queue\\\", torch.randn(dim, K)) self.queue = nn.functional.normalize(self.queue, dim=0) self.register_buffer(\\\"queue_ptr\\\", torch.zeros(1, dtype=torch.long)) # 队列指针，负责完成出队入队的信息记录\"]},\"362\":{\"h\":\"Model Forward\",\"t\":[\"模型前向传播代码如下所示:\",\" def forward(self, im_q, im_k): \\\"\\\"\\\" Input: im_q: a batch of query images im_k: a batch of key images Output: logits, targets \\\"\\\"\\\" # 1. compute query features q = self.encoder_q(im_q) # queries: NxC q = nn.functional.normalize(q, dim=1) # 2. compute key features with torch.no_grad(): # no gradient to keys self._momentum_update_key_encoder() # update the key encoder # shuffle for making use of BN im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k) k = self.encoder_k(im_k) # keys: NxC k = nn.functional.normalize(k, dim=1) # undo shuffle k = self._batch_unshuffle_ddp(k, idx_unshuffle) # 3. compute logits # Einstein sum is more intuitive # positive logits: Nx1 l_pos = torch.einsum(\\\"nc,nc->n\\\", [q, k]).unsqueeze(-1) # negative logits: NxK l_neg = torch.einsum(\\\"nc,ck->nk\\\", [q, self.queue.clone().detach()]) # 4. logits: Nx(1+K) logits = torch.cat([l_pos, l_neg], dim=1) # 5. apply temperature logits /= self.T # 6. labels: positive key indicators labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda() # 7. dequeue and enqueue self._dequeue_and_enqueue(k) return logits, labels\",\"论文采用 ResNet 作为编码器，其最后的全连接层（在全局平均池化之后）输出一个固定维度的向量（128 维）。这个输出向量会通过其 L2 范数进行归一化。该向量即表示一个 query（查询向量） 或 key（键向量） 的特征表示。\"]},\"363\":{\"h\":\"Momentum Update\",\"t\":[\"采用动量更新方式对 key encoder 进行缓慢更新的代码实现如下所示:\",\" @torch.no_grad() def _momentum_update_key_encoder(self) -> None: \\\"\\\"\\\" Momentum update of the key encoder \\\"\\\"\\\" for param_q, param_k in zip( self.encoder_q.parameters(), self.encoder_k.parameters() ): param_k.data = param_k.data * self.m + param_q.data * (1.0 - self.m)\"]},\"364\":{\"h\":\"Dequeue and Enqueue\",\"t\":[\"维护队列状态的代码实现如下所示：\",\" @torch.no_grad() def _dequeue_and_enqueue(self, keys) -> None: # 1. gather keys before updating queue keys = concat_all_gather(keys) # 如果你在多 GPU 上训练，每个 GPU 都会处理一部分 batch，该方法会将所有 GPU 上的 key 向量合并成一个完整的 batch batch_size = keys.shape[0] ptr = int(self.queue_ptr) assert self.K % batch_size == 0 # for simplicity # 2. replace the keys at ptr (dequeue and enqueue) self.queue[:, ptr : ptr + batch_size] = keys.T # queue 的形状是 (C, K)，每列是一个 key 向量（transpose 存储是为了快速矩阵乘） ptr = (ptr + batch_size) % self.K # move pointer self.queue_ptr[0] = ptr\"]},\"365\":{\"h\":\"多模态\"},\"366\":{\"h\":\"Unified-IO 论文\",\"t\":[\"Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks 论文简析\",\"论文链接: Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks 代码链接: https://unified-io.allenai.org/\"]},\"367\":{\"h\":\"VLMo 论文\",\"t\":[\"VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts 论文简析\",\"论文链接: VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts 代码链接: https://github.com/microsoft/unilm/tree/master/vlmo\"]},\"368\":{\"h\":\"Introduction\",\"t\":[\"视觉-语言（VL）预训练旨在从大规模图文对中学习通用的跨模态表示。现有模型通常通过图文匹配、图文对比学习、掩码区域分类/特征回归、词-区域/块对齐以及掩码语言建模等方法来聚合和对齐视觉与语言信息，然后在下游任务如图文检索、视觉问答（VQA）、视觉推理等进行微调。\",\"现有两类主流架构各有优缺点：\",\"双编码器架构（Dual-Encoder，如 CLIP、ALIGN）：\",\"图像和文本分别编码，模态间交互通过特征向量的余弦相似度进行。\",\"优点：检索任务高效，特征向量可提前计算存储，线性复杂度。\",\"缺点：交互浅，对复杂视觉-语言分类任务表现有限，如 CLIP 在视觉推理任务上准确率偏低。\",\"融合编码器架构（Fusion-Encoder）：\",\"使用多层 Transformer 通过跨模态注意力融合图像和文本表示。\",\"优点：在视觉-语言分类任务上性能优异。\",\"缺点：检索任务需对所有图文对联合编码，时间复杂度为二次方，推理速度慢。\",\"VLMO 的提出\",\"为兼顾双编码器和融合编码器的优势，论文提出了 统一视觉-语言预训练模型 VLMO，其特点如下：\",\"可作为双编码器用于图文检索，也可作为融合编码器处理图文对分类任务。\",\"核心组件为 Mixture-of-Modality-Experts (MOME) Transformer，一个 Transformer 块内可编码图像、文本及图文对。\",\"MOME 替换标准 Transformer 的前馈网络为模态专家池，捕获模态特定信息，同时共享自注意力层进行跨模态对齐。\",\"三类模态专家：视觉专家（图像编码）、语言专家（文本编码）、视觉-语言专家（图文融合）。\",\"模型灵活性高，可复用共享参数实现文本编码器、图像编码器和图文融合编码器。\",\"预训练任务与策略\",\"VLMO 采用三种联合预训练任务：\",\"图文对比学习（image-text contrastive learning）\",\"图文匹配（image-text matching）\",\"掩码语言建模（masked language modeling）\",\"同时提出 分阶段预训练策略，充分利用大规模图像单模态和文本单模态数据：\",\"在图像单模态数据上预训练视觉专家和自注意力模块，采用 BEIT 的掩码图像建模方法。\",\"在文本单模态数据上预训练语言专家，采用掩码语言建模方法。\",\"最终初始化视觉-语言预训练模型，解决图文对数量有限、描述短小的问题，从而学习更泛化的表示。\",\"实验结果与贡献\",\"在图文检索任务中，VLMO 作为双编码器比融合编码器更快，并且性能优于其他融合编码器模型。\",\"在视觉问答（VQA）和自然语言视觉推理（NLVR2）任务中，作为融合编码器的 VLMO 达到最先进性能。\",\"主要贡献：\",\"提出统一视觉-语言预训练模型 VLMO，可灵活用作融合编码器或双编码器。\",\"引入通用多模态 Transformer（MOME Transformer），通过模态专家捕获模态特定信息，并通过共享自注意力实现跨模态对齐。\",\"分阶段预训练策略利用大规模图像单模态和文本单模态数据，显著提升模型性能。\"]},\"369\":{\"h\":\"Related Work\",\"t\":[\"视觉-语言预训练方法大体可分为两类：\",\"双编码器（Dual Encoder）（如 CLIP [35], ALIGN [18]）：\",\"图像和文本分别编码，模态间交互通过余弦相似度或线性投影实现。\",\"通常使用图文对比学习优化模型。\",\"优点：对检索任务效果好，图像和文本特征可预计算。\",\"缺点：交互浅，不适合处理需要复杂推理的 VL 分类任务，如视觉问答或视觉推理。\",\"融合编码器（Fusion Encoder）（如 ALBEF [23], ViLT [20], Pixel-BERT [16]）：\",\"使用深度融合编码器，通过跨模态注意力建模图像和文本的交互。\",\"训练任务包括图文匹配、掩码语言建模、词-区域/块对齐、掩码区域分类和特征回归。\",\"优点：在分类任务上性能更好，可捕获深层交互。\",\"缺点：对所有图文对进行联合编码，推理速度慢；依赖目标检测器（如 Faster R-CNN [37]）获取区域特征时可扩展性差。\",\"改进方法：Pixel-BERT 去掉目标检测器，使用卷积网络提取网格特征；ALBEF 使用图像 Transformer 和文本 Transformer 获取表示再融合；ViLT 将图像 patch 与词嵌入拼接输入 Transformer 学习上下文表示。\",\"VLMO 的特点：\",\"采用共享的 MOME Transformer 统一预训练。\",\"可在检索任务中执行独立编码，也可联合编码图文对用于分类任务。\",\"优点：在保证性能的同时，检索和分类任务的推理速度更快。\"]},\"370\":{\"h\":\"Method\",\"t\":[\"给定图像-文本对，VLMO 通过 MOME Transformer 网络获得图像、文本以及图像-文本对的表示。如图 1 所示，统一的预训练通过以下任务优化共享的 MOME Transformer：\",\"对图像-only 和文本-only 表示进行图文对比学习（Image-Text Contrastive Learning）\",\"对图像-文本对表示进行图文匹配（Image-Text Matching）和掩码语言建模（Masked Language Modeling）\",\"得益于这种建模灵活性，模型在微调时可以作为双编码器（Dual Encoder）用于检索任务，单独编码图像和文本；也可以作为融合编码器（Fusion Encoder）用于分类任务，建模图像与文本的深层交互。\"]},\"371\":{\"h\":\"输入表示\",\"t\":[\"给定一个图像-文本对，我们将其编码为图像、文本和图像-文本向量表示，这些表示会被输入到 MOME Transformer 中以学习上下文表示，并对图像和文本特征进行对齐。\",\"图像表示\",\"遵循视觉 Transformer 方法，二维图像 被划分为 个 patch，，其中 为通道数， 为图像分辨率， 为 patch 分辨率。\",\"将图像 patch 拉平后线性投影获得 patch embedding，并在序列前添加可学习的特殊 token [I_CLS]。\",\"最终的图像输入表示为 patch embedding、可学习位置嵌入 和图像类型嵌入 的和：\",\"文本表示\",\"遵循 BERT 方法，将文本拆分为子词（WordPiece），并在序列前后添加 [T_CLS] 和 [T_SEP]。\",\"文本输入表示 为对应词向量、位置嵌入和类型嵌入之和：\",\"其中 为分词后的长度。\",\"图像-文本表示\",\"将图像和文本向量拼接得到图像-文本输入表示：\"]},\"372\":{\"h\":\"多模态专家混合 Transformer（MOME Transformer）\",\"t\":[\"受到 Mixture-of-Experts 网络启发，提出 MOME Transformer 用于多模态编码，将标准 Transformer 的前馈网络替换为“模态专家混合网络”（Mixture-of-Modality-Experts, MoME-FFN）。\",\"对上一层输出 ，每个 MOME Transformer 块通过切换不同模态专家捕捉模态特定信息，并通过多头自注意力（MSA）对齐视觉和语言内容。\",\"MoME-FFN 会根据输入模态选择不同专家：视觉专家（V-FFN）、语言专家（L-FFN）和视觉-语言专家（VL-FFN）。\",\"输入为图像-only 或文本-only 时，分别使用视觉或语言专家编码；输入为图像-文本对时，底层使用视觉和语言专家分别编码各自模态，顶层使用视觉-语言专家捕捉跨模态交互。\",\"最终得到图像-only、文本-only 和图像-文本的上下文表示。\"]},\"373\":{\"h\":\"预训练任务\",\"t\":[\"VLMO 使用共享参数同时进行以下预训练任务：\",\"图文对比学习（Image-Text Contrastive Learning）\",\"给定一个 batch 的 个图像-文本对，目标是从 个可能配对中识别正确配对，其中有 个负样本。\",\"使用 [I_CLS] 和 [T_CLS] 的输出向量作为图像和文本的聚合表示，通过线性投影和归一化得到 和 ，计算图像-文本相似度：\",\"其中 为可学习温度参数，使用交叉熵损失训练。\",\"掩码语言建模（Masked Language Modeling）\",\"随机选择文本序列中的 token 替换为 [MASK]（概率 15%），模型预测被掩码的 token，利用视觉信息辅助。\",\"使用 cross-entropy loss 对整个词表进行分类训练。\",\"图文匹配（Image-Text Matching）\",\"预测图像与文本是否匹配，使用 [T_CLS] 的最终隐藏向量作为表示输入分类器。\",\"采用 hard negative 采样策略：不同于 ALBEF 的局部采样（local hard negative），VLMO 提出全局 hard negative 采样（global hard negative），从所有 GPU 的训练样本中采样，能显著提升模型性能。\"]},\"374\":{\"h\":\"分阶段预训练（Stagewise Pre-Training）\",\"t\":[\"利用大规模图像-only 和文本-only 数据提升模型能力（图 2）。\",\"首先对图像-only 数据进行视觉预训练，训练 MOME Transformer 的注意力模块和视觉专家，使用 BEIT 预训练参数初始化。\",\"然后对文本-only 数据进行语言预训练，冻结注意力模块和视觉专家，仅训练语言专家进行掩码语言建模。\",\"这种方式比直接使用图像-文本对更容易收集数据，同时文本-only 数据通常较长、复杂，有助于提升对复杂图文对的泛化能力。\"]},\"375\":{\"h\":\"下游任务微调\",\"t\":[\"视觉-语言分类\",\"如视觉问答、视觉推理任务，VLMO 作为融合编码器使用，建模图像和文本的交互。\",\"使用 [T_CLS] 的最终编码向量表示图文对，输入任务专用分类器预测标签。\",\"视觉-语言检索\",\"VLMO 可作为双编码器，分别编码图像和文本，优化图文对比损失。\",\"推理阶段计算所有图像和文本表示，通过点积获得相似度。\",\"分别编码方式比融合编码器推理速度快得多（图 3）。\"]},\"376\":{\"h\":\"Ablation Studies\",\"t\":[\"阶段式预训练（Stagewise Pre-Training）\",\"我们首先进行了阶段式预训练的消融实验。ViLT 表明：使用在图像数据上预训练的 ViT 作为初始化，比使用在文本数据上预训练的 BERT 模型表现更好。因此，我们的实验从图像预训练开始。我们比较了两种初始化方式：\",\"图像预训练：直接使用 BEIT-Base 的参数初始化自注意力模块和所有模态专家。\",\"图像预训练 + 文本预训练：使用 BEIT-Base 的参数初始化 MOME Transformer 的视觉专家和自注意力模块，然后在文本语料上预训练语言专家。\",\"实验结果（见表4）显示：相比单独图像预训练，图像预训练 + 文本预训练进一步提升了视觉语言模型的性能。我们也尝试过随机初始化直接进行视觉语言预训练，但下游任务的准确率较低。阶段式预训练能够有效利用大规模图像语料与文本语料，从而提升视觉语言预训练效果。此外，考虑到我们使用的图文对数量有限，阶段式预训练能够缓解对大量图文对数据的依赖。\",\"MOME Transformer\",\"我们还进行了 MOME Transformer 的消融实验，使用 ViT-Base 作为模型初始化。实验结果（见表5）表明：\",\"使用 MOME Transformer 在检索和分类任务中都优于标准 Transformer。\",\"我们进一步分析了 MOME Transformer 中的视觉语言专家（VL-FFN）的贡献。当移除顶层 Transformer 层中的视觉语言专家时，模型性能下降，说明视觉语言专家有助于捕获更多的模态交互信息。\",\"共享的自注意力模块也对模型有积极贡献。关于共享自注意力模块的消融结果，详见附录 A。\",\"预训练任务（Pre-Training Tasks）\",\"我们对不同预训练任务的贡献进行了消融实验，结果见表5。\",\"仅使用图文对比损失训练的模型性能显著低于我们统一训练框架下的模型。\",\"引入带有 困难负样本挖掘（hard negative mining） 的图文匹配任务，显著提升了模型性能，验证了 MOME Transformer 统一训练框架的有效性。\",\"此外，实验结果显示 遮盖语言建模（MLM） 任务也能进一步提升模型效果。更多消融实验见附录。\",\"全局困难负样本挖掘（Global Hard Negative Mining）\",\"与 ALBEF [23] 不同，后者只从单个 GPU 的训练样本中采样困难负样本（称为 局部困难负样本挖掘）。我们的方法则从所有 GPU 的训练样本中采样困难负样本（称为 全局困难负样本挖掘）。实验结果（见表6）表明：全局困难负样本挖掘带来了显著的性能提升。\",\"共享自注意力机制的消融实验\",\"表7展示了在 MOME Transformer 中使用的共享自注意力模块的消融实验结果，该模块用于编码图像 patch 与文本 token。\",\"研究者将 共享自注意力（shared self-attention） 与 分离自注意力（separate self-attention） 进行对比。后者在前 层中，分别采用不同的注意力参数对图像 patch 和文本 token 进行编码。\",\"实验结果表明：\",\"使用 MOME 的共享自注意力效果更好。\",\"共享自注意力模块能够帮助 VLMO 学习不同模态之间的对齐关系，并在底层融合图像与文本，从而提升分类任务的表现。\"]},\"377\":{\"h\":\"Conclusion\",\"t\":[\"VLMO 是一个统一的视觉-语言预训练模型，它基于 MOME Transformer。MOME 引入了一组模态专家（modality experts），用于处理模态特定的信息，并通过共享的自注意力模块实现不同模态的对齐。\",\"因此，VLMO 可以同时支持两种不同的应用方式：\",\"作为 双编码器（dual encoder），分别编码图像和文本，并进行相似度计算，从而实现高效的视觉-语言检索任务；\",\"作为 融合编码器（fusion encoder），直接建模图像与文本的交互，用于更复杂的跨模态分类任务。\",\"研究表明，单靠有限规模的图文对数据进行预训练不足以获得最佳效果。VLMO 采用了阶段式预训练策略：\",\"先利用大规模 图像语料 和 文本语料 进行单模态预训练；\",\"再进行视觉-语言联合预训练。\",\"这种方式显著提升了模型的效果，并且缓解了对大规模图文对数据的依赖。\",\"未来工作方向：\",\"扩大模型规模：在预训练中使用更大规模的 VLMO 模型；\",\"支持生成任务：将 VLMO 微调用于图像描述（image captioning）等生成类任务，借鉴 UniLM [11] 的方法；\",\"跨模态互助：研究视觉与语言预训练在多大程度上能够互相促进，特别是 MOME 的共享骨干能够自然融合文本与图像表示；\",\"扩展到更多模态：例如语音、视频和结构化知识，以支持通用的多模态预训练。\"]},\"378\":{\"h\":\"VLMo 模型代码解读\",\"t\":[\"VLMO 模型代码解读\",\"论文链接: VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts 代码链接: https://github.com/microsoft/unilm/tree/master/vlmo\"]},\"379\":{\"h\":\"前置知识\",\"t\":[\"VLMO 模型的代码实现中主要使用了以下两个库，如果不提前了解一下库的基本用法，可能会导致读不懂代码实现：\",\"Sacred 实验管理框架\",\"PyTorch Lightning\",\"VLMo 模型代码实现是基于 ViLT 模型代码进行修改的，因此如果研究过 ViLT 代码实现的同学，对 VLMo 模型的代码实现应该比较亲切。\"]},\"380\":{\"h\":\"MOME（Mixture of Multimodal Experts）Transformer\",\"t\":[\"VLMO 论文中所提到的 MOME Transformer 的代码实现对应的类是 MultiWayTransformer , 本节我们将一点点完成该类代码的拆解; 首先，既然是 混合多模态专家模型, 那么它就需要具有同时处理图像和文本的能力；对于输入的图像，第一步需要完成图像的切片和嵌入，该功能由 PatchEmbed 类负责完成，具体代码实现如下:\",\"class PatchEmbed(nn.Module): \\\"\\\"\\\" Image to Patch Embedding 将输入的图像切分成小 patch，并通过卷积映射到指定的 embedding 维度。 这是 Vision Transformer (ViT) 中常用的图像嵌入方法。 \\\"\\\"\\\" def __init__( self, img_size=224, # 输入图像的高度和宽度（默认224x224） patch_size=16, # 每个patch的高度和宽度（默认16x16） in_chans=3, # 输入通道数，彩色图像通常为3 embed_dim=768, # 输出 embedding 的维度 no_patch_embed_bias=False, # 是否在卷积层中去掉偏置 ): super().__init__() # 将 img_size 和 patch_size 转换为 (height, width) 的 tuple img_size = to_2tuple(img_size) patch_size = to_2tuple(patch_size) # 计算图像能切分成多少个 patch num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0]) # 保存 patch 行列数，用于位置编码或其他处理 self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) self.img_size = img_size self.patch_size = patch_size self.num_patches = num_patches # 定义卷积层，将图像切分成 patch 并映射到 embedding 维度 # 注意：kernel_size = patch_size, stride = patch_size，这样每个卷积核对应一个 patch self.proj = nn.Conv2d( in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=False if no_patch_embed_bias else True, ) def forward(self, x): B, C, H, W = x.shape # 检查输入图像尺寸是否与初始化尺寸匹配 assert H == self.img_size[0] and W == self.img_size[1], \\\\ f\\\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\\\" # 卷积映射，将图像切分成 patch 并生成 embedding x = self.proj(x) # 输出 shape: [B, embed_dim, H_patch, W_patch] return x\",\"PatchEmbed 只完成了借助卷积对图像进行前置处理的步骤，MultiWayTransformer 类额外提供了 visual_embed 方法来完成与 文本Token 统一形式的 视觉Token 的构建:\",\"class MultiWayTransformer(nn.Module): def visual_embed(self, _x): \\\"\\\"\\\" 将输入的图像张量 _x 转换为视觉 token embedding。 步骤包括： 1. patch embedding 2. 展平并调整维度 3. 添加 cls token 4. 添加位置编码（可选） 5. 添加 dropout \\\"\\\"\\\" # 1. 将图像切分成 patch 并映射到 embedding 维度 x = self.patch_embed(_x) # shape: [B, embed_dim, H_patch, W_patch] # 2. 展平 patch 并调整维度，使其变为序列形式 [B, num_patches, embed_dim] x = x.flatten(2).transpose(1, 2) # flatten 从 H*W -> L，transpose 调整维度 B, L, _ = x.shape # B: batch size, L: patch 数量, _: embedding 维度 # 3. 扩展 cls_token 到 batch 大小，并与 patch embedding 拼接 cls_tokens = self.cls_token.expand(B, -1, -1) # shape: [B, 1, embed_dim] x = torch.cat((cls_tokens, x), dim=1) # 拼接后 shape: [B, L+1, embed_dim] # 4. 如果有位置编码，则加上 if self.pos_embed is not None: x = x + self.pos_embed # shape: [B, L+1, embed_dim] # 5. 添加 dropout，增加模型鲁棒性 x = self.pos_drop(x) # 6. 构建 mask，这里全 1 表示所有 token 都有效 x_mask = torch.ones(x.shape[0], x.shape[1]) # shape: [B, L+1] return x, x_mask # 返回 token embedding 和 mask\",\"MultiWayTransformer 类没有直接对外提供现成的 forward 方法实现，而是由调用方 VLMo 类负责完成前向传播流程的组织，所以下面我们将首先对其 init 方法进行分析，看看它内部包含哪些重要组件:\",\"class MultiWayTransformer(nn.Module): def __init__( self, img_size=224, # 输入图像尺寸 patch_size=16, # patch 大小 in_chans=3, # 输入通道数（例如 RGB 图像为 3） embed_dim=768, # embedding 维度 depth=12, # transformer block 层数 num_heads=12, # attention 头数 mlp_ratio=4.0, # MLP 隐层维度与 embedding 维度的比例 qkv_bias=True, # 是否在 QKV 上使用偏置 qk_scale=None, # 可手动设置 QK 缩放值 drop_rate=0.0, # dropout 概率 attn_drop_rate=0.0, # attention dropout 概率 drop_path_rate=0.0, # stochastic depth 概率 norm_layer=None, # normalization 层类型 need_relative_position_embed=True, # 是否使用相对位置编码 use_abs_pos_emb=False, # 是否使用绝对位置编码 layer_scale_init_values=0.1, # LayerScale 初始化值 vlffn_start_layer_index=10, # 从第几层开始使用 VL-FFN config=None, # 其他配置（如从 pytorch-lightning 传入） **kwargs, # 接收 timm 或其他传入的额外参数 ): \\\"\\\"\\\" MultiWayTransformer 构造函数，初始化视觉与文本 transformer 的参数。 \\\"\\\"\\\" super().__init__() # 如果传入 config，则覆盖 drop_path_rate drop_path_rate = drop_path_rate if config is None else config[\\\"drop_path_rate\\\"] # 保存是否使用绝对位置编码和相对位置编码的标志 self.use_abs_pos_emb = use_abs_pos_emb self.need_relative_position_embed = need_relative_position_embed # 记录 embedding 特征维度 self.num_features = (self.embed_dim) = embed_dim # num_features 与 embed_dim 保持一致 # 默认归一化层，如果未指定则使用 LayerNorm norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6) # PatchEmbedding，将图像切分为 patch 并映射到 embed_dim self.patch_embed = PatchEmbed( img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, ) # 记录 patch 数量和 patch 尺寸 num_patches = self.patch_embed.num_patches self.patch_size = patch_size self.num_heads = num_heads # VL-FFN 从哪一层开始 self.vlffn_start_layer_index = vlffn_start_layer_index # 针对 text-only pretraining，如果 textmlm loss 大于 0，则从最后一层开始使用 VL-FFN if config[\\\"loss_names\\\"][\\\"textmlm\\\"] > 0: self.vlffn_start_layer_index = depth # 类别 token 参数（用于全局聚合） self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 绝对位置编码参数（可选） self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim)) if self.use_abs_pos_emb else None # dropout 层 self.pos_drop = nn.Dropout(p=drop_rate) # stochastic depth，每层的 drop_path 概率线性增长 dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)] # 构建 transformer block 列表 self.blocks = nn.ModuleList( [ Block( dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, with_vlffn=(i >= self.vlffn_start_layer_index), # 超过起始层索引才启用 VL-FFN layer_scale_init_values=layer_scale_init_values, max_text_len=config[\\\"max_text_len\\\"], ) for i in range(depth) ] ) # transformer 最后的归一化层 self.norm = norm_layer(embed_dim) # 参数初始化 if self.pos_embed is not None: trunc_normal_(self.pos_embed, std=0.02) trunc_normal_(self.cls_token, std=0.02) self.apply(self._init_weights) # 初始化所有权重\",\"MultiWayTransformer 支持同时处理文本和图像模态，这个功能具体实现在其内部的 Transformer Block 中:\",\"class Block(nn.Module): def __init__( self, dim, # 输入特征维度 num_heads, # 多头注意力的头数 mlp_ratio=4.0, # MLP 隐藏层维度与输入维度的比例 qkv_bias=False, # QKV 是否使用偏置 qk_scale=None, # QK 缩放因子（覆盖默认 head_dim ** -0.5） drop=0.0, # Dropout 概率 attn_drop=0.0, # 注意力权重的 Dropout 概率 drop_path=0.0, # Stochastic Depth 概率 act_layer=nn.GELU, # 激活函数类型 norm_layer=nn.LayerNorm, # 归一化层类型 with_vlffn=False, # 是否使用跨模态 MLP（Vision-Language Feed-Forward Network） layer_scale_init_values=0.1, # LayerScale 初始化值 max_text_len=40, # 最大文本序列长度 ): super().__init__() # 第一个 LayerNorm（作用于注意力之前） self.norm1 = norm_layer(dim) # 多头注意力机制 self.attn = Attention( dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, ) # DropPath（随机丢弃整个残差分支）或恒等映射 self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity() # 第二阶段的 LayerNorm（针对文本和图像分别有独立的归一化层） self.norm2_text = norm_layer(dim) self.norm2_imag = norm_layer(dim) # MLP 隐藏层维度 mlp_hidden_dim = int(dim * mlp_ratio) # 文本模态的 MLP self.mlp_text = Mlp( in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop, ) # 图像模态的 MLP self.mlp_imag = Mlp( in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop, ) # 跨模态 MLP（仅在 with_vlffn=True 时使用） self.mlp_vl = None if with_vlffn: self.mlp_vl = Mlp( in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop, ) self.norm2_vl = norm_layer(dim) # LayerScale 参数（gamma_1 作用于注意力分支，gamma_2 作用于 MLP 分支） self.gamma_1 = ( nn.Parameter(layer_scale_init_values * torch.ones((dim)), requires_grad=True) if layer_scale_init_values is not None else 1.0 ) self.gamma_2 = ( nn.Parameter(layer_scale_init_values * torch.ones((dim)), requires_grad=True) if layer_scale_init_values is not None else 1.0 ) # 最大文本长度（在拆分多模态输入时使用） self.max_text_len = max_text_len def forward(self, x, mask=None, modality_type=None, relative_position_bias=None): \\\"\\\"\\\" Args: x: 输入特征 [B, L, C] mask: 注意力掩码（可选） modality_type: 输入模态类型 (\\\"image\\\", \\\"text\\\", 或 None 表示多模态） relative_position_bias: 相对位置编码偏置 \\\"\\\"\\\" # ====== 注意力子层（带残差连接 + LayerScale + DropPath）====== x = x + self.drop_path( self.gamma_1 * self.attn( self.norm1(x), mask=mask, relative_position_bias=relative_position_bias ) ) # ====== 前馈网络子层（根据模态类型选择不同 MLP）====== if modality_type == \\\"image\\\": # 仅图像模态 x = x + self.drop_path(self.gamma_2 * self.mlp_imag(self.norm2_imag(x))) elif modality_type == \\\"text\\\": # 仅文本模态 x = x + self.drop_path(self.gamma_2 * self.mlp_text(self.norm2_text(x))) else: # 多模态情况 if self.mlp_vl is None: # 分开处理文本和图像序列 x_text = x[:, : self.max_text_len] # 前 max_text_len 为文本 x_imag = x[:, self.max_text_len :] # 剩余部分为图像 x_text = x_text + self.drop_path(self.gamma_2 * self.mlp_text(self.norm2_text(x_text))) x_imag = x_imag + self.drop_path(self.gamma_2 * self.mlp_imag(self.norm2_imag(x_imag))) # 合并回一个序列 x = torch.cat([x_text, x_imag], dim=1) else: # 跨模态 MLP x = x + self.drop_path(self.gamma_2 * self.mlp_vl(self.norm2_vl(x))) return x\",\"LayerScale 技术: 在深层 Transformer 中，如果直接把残差相加，可能导致梯度爆炸或梯度消失; LayerScale 允许网络自己调节每一层的残差输出强度，从而改善训练稳定性。提高深层网络可训练性； 对深层 ViT（几十甚至上百层）非常有效，减少了训练前期的收敛难度。\",\"简单理解：\",\"x = x + γ1 * Attention(x) x = x + γ2 * MLP(x)\",\"γ1、γ2 = 可学习缩放因子\",\"作用 = 控制残差贡献，稳定训练\",\"为什么分开？因为 Attention 和 MLP 输出的统计特性不同，需要不同的缩放系数\",\"Attention 模块的代码属于模版代码，不涉及新技术的引入，代码实现如下所示:\",\"class Attention(nn.Module): def __init__( self, dim, # 输入特征的维度 (embedding dimension) num_heads=8, # 多头注意力的头数 qkv_bias=False, # 是否为 Q、K、V 添加可学习偏置 qk_scale=None, # QK 点积的缩放因子（可覆盖默认值） attn_drop=0.0, # 注意力权重的 Dropout 概率 proj_drop=0.0, # 输出投影层的 Dropout 概率 ): super().__init__() self.num_heads = num_heads head_dim = dim // num_heads # 每个注意力头的维度 # QK 缩放因子，默认为 1/sqrt(head_dim)，防止点积结果过大 self.scale = qk_scale or head_dim ** -0.5 # 线性层生成 Q、K、V（一次性计算 dim → 3*dim） self.qkv = nn.Linear(dim, dim * 3, bias=False) # 如果需要 Q、V 偏置，则单独为 Q 和 V 创建可学习参数 if qkv_bias: self.q_bias = nn.Parameter(torch.zeros(dim)) self.v_bias = nn.Parameter(torch.zeros(dim)) else: self.q_bias = None self.v_bias = None # 注意力权重的 Dropout self.attn_drop = nn.Dropout(attn_drop) # 注意力结果的输出投影层 self.proj = nn.Linear(dim, dim) # 输出投影后的 Dropout self.proj_drop = nn.Dropout(proj_drop) def forward(self, x, mask=None, relative_position_bias=None): \\\"\\\"\\\" Args: x: 输入张量 (B, N, C)， B=批大小，N=序列长度，C=通道数(embedding dim) mask: 注意力掩码 (B, N)，用于屏蔽无效位置 relative_position_bias: 相对位置编码 (num_heads, N, N) \\\"\\\"\\\" B, N, C = x.shape # 取出批大小、序列长度、通道数 # 处理 Q、K、V 偏置 qkv_bias = None if self.q_bias is not None: # 拼接 Q 偏置、K 偏置(全0)、V 偏置 qkv_bias = torch.cat(( self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias )) # 线性映射得到 Q、K、V（在这里一次性计算） qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias) # 变形为 (3, B, num_heads, N, head_dim)，并调整维度顺序 qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4) # 拆分 Q、K、V q, k, v = qkv[0], qkv[1], qkv[2] # 缩放 Q q = q * self.scale # QK^T 得到注意力分数矩阵 attn = (q.float() @ k.float().transpose(-2, -1)) # 如果有相对位置偏置，则加上 if relative_position_bias is not None: attn = attn + relative_position_bias.unsqueeze(0) # 如果有 mask（如解码器中的自回归屏蔽） if mask is not None: mask = mask.bool() attn = attn.masked_fill(~mask[:, None, None, :], float(\\\"-inf\\\")) # 对最后一维做 softmax 得到注意力权重 attn = attn.softmax(dim=-1).type_as(x) # 对注意力权重做 Dropout attn = self.attn_drop(attn) # 注意力加权 V，然后还原维度为 (B, N, C) x = (attn @ v).transpose(1, 2).reshape(B, N, C) # 输出投影 x = self.proj(x) # 投影结果 Dropout x = self.proj_drop(x) return x\"]},\"381\":{\"h\":\"VLMo\",\"t\":[\"主模型 VLMo 由于使用了 PyTorch Lightning 实验全流程管理框架，使得其代码看起来并不常规，但是其本质还是借助 模版方法设计模型 抽取出一套通用的模版流程，并通过在各个模版节点预留钩子函数的方式，使得用户可以在不改变模版流程的情况下，自定义模型的行为;\",\"因此，我们首先用一幅图理清楚 PyTorch Lightning 预留的这套模版流程是怎么设计的:\",\"图片取至 PytorchLightning : Model calls order\",\"1. 初始化阶段 ├─ 用户创建 LightningModule 和 Trainer (用户代码) ├─ LightningModule.configure_optimizers() (LightningModule) ├─ Trainer 配置 logger、callbacks、accelerator、分布式 (Trainer 内部) 2. 数据准备阶段 ├─ LightningDataModule.prepare_data() (LightningDataModule, global_rank=0) └─ LightningDataModule.setup(stage) (LightningDataModule, 每个进程, stage ∈ {'fit','validate','test','predict'}) 3. 数据加载阶段 ├─ LightningDataModule.train_dataloader() (LightningDataModule) ├─ LightningDataModule.val_dataloader() (LightningDataModule) └─ LightningDataModule.test_dataloader() (LightningDataModule) 4. 训练阶段（fit） ├─ Trainer.on_fit_start() (Trainer 调用所有 callbacks.on_fit_start) └─ Epoch 循环 (for epoch in max_epochs) ├─ Trainer.on_train_epoch_start() (Trainer callbacks) └─ Batch 循环 (for batch in train_dataloader) ├─ Trainer.on_train_batch_start(batch, batch_idx) (Trainer callbacks) ├─ LightningModule.training_step(batch, batch_idx) (LightningModule) ├─ Trainer.on_before_zero_grad(optimizer) (Trainer callbacks) ├─ optimizer.zero_grad() (PyTorch) ├─ loss.backward() (PyTorch) ├─ Trainer.on_after_backward() (Trainer callbacks) ├─ Trainer.on_before_optimizer_step(optimizer) (Trainer callbacks) ├─ optimizer.step() (PyTorch) └─ Trainer.on_train_batch_end(output, batch, batch_idx)(Trainer callbacks) ├─ LightningModule.training_epoch_end(outputs) (LightningModule) └─ Trainer.on_train_epoch_end() (Trainer callbacks) └─ 验证阶段（每个 epoch 后可选） ├─ Trainer.on_validation_start() (Trainer callbacks) ├─ model.eval(), torch.no_grad() (Trainer 内部) └─ 循环 val_dataloader ├─ LightningModule.validation_step(batch, batch_idx) (LightningModule) ├─ LightningModule.validation_step_end(output) (LightningModule) └─ 汇总 outputs ├─ LightningModule.validation_epoch_end(outputs) (LightningModule) └─ Trainer.on_validation_epoch_end() (Trainer callbacks) └─ Trainer.on_fit_end() (Trainer callbacks) 5. 测试阶段（test） ├─ Trainer.on_test_start() (Trainer callbacks) ├─ model.eval(), torch.no_grad() (Trainer 内部) └─ 循环 test_dataloader ├─ LightningModule.test_step(batch, batch_idx) (LightningModule) ├─ LightningModule.test_step_end(output) (LightningModule) └─ 汇总 outputs ├─ LightningModule.test_epoch_end(outputs) (LightningModule) └─ Trainer.on_test_end() (Trainer callbacks) 6. 预测阶段（predict） ├─ Trainer.on_predict_start() (Trainer callbacks) ├─ model.eval(), torch.no_grad() (Trainer 内部) └─ 循环 predict_dataloader ├─ LightningModule.predict_step(batch, batch_idx) (LightningModule) ├─ LightningModule.predict_step_end(output) (LightningModule) └─ 汇总 outputs ├─ LightningModule.predict_epoch_end(outputs) (LightningModule) └─ Trainer.on_predict_end() (Trainer callbacks)\",\"下面我们将结合上面的模版流程，分析一下 VLMo 在模版流程的各种阶段都做了什么:\"]},\"382\":{\"h\":\"数据模块\",\"t\":[\"一般模型训练都会加载多个来源不同的开源或私有数据集，VLMo 也不例外，因此 VLMo 提供了 MTDataModule 类用于完成多数据源加载的任务:\",\"class MTDataModule(LightningDataModule): def __init__(self, _config, dist=False): \\\"\\\"\\\" 多任务/多数据集 DataModule，负责管理多个子数据集 Args: _config: 配置字典，包含数据集 key 和其他参数 dist: 是否使用分布式采样 \\\"\\\"\\\" datamodule_keys = _config[\\\"datasets\\\"] assert len(datamodule_keys) > 0 super().__init__() # 保存数据集 key 和对应的数据模块实例 self.dm_keys = datamodule_keys self.dm_dicts = {key: _datamodules[key](_config) for key in datamodule_keys} self.dms = [v for k, v in self.dm_dicts.items()] # 从第一个数据模块读取通用配置 self.batch_size = self.dms[0].batch_size self.vocab_size = self.dms[0].vocab_size self.num_workers = self.dms[0].num_workers self.dist = dist # 是否使用分布式采样 def prepare_data(self): \\\"\\\"\\\" 数据准备阶段（只在主进程调用一次） 生命周期阶段: Trainer 调用 prepare_data() \\\"\\\"\\\" for dm in self.dms: dm.prepare_data() # 调用每个子数据模块的 prepare_data def setup(self, stage): \\\"\\\"\\\" 数据集构建阶段，每个进程都会调用 Args: stage: 'fit', 'validate', 'test', 'predict' 等 \\\"\\\"\\\" for dm in self.dms: dm.setup(stage) # 调用子数据模块的 setup # 合并各个子数据集 self.train_dataset = ConcatDataset([dm.train_dataset for dm in self.dms]) self.val_dataset = ConcatDataset([dm.val_dataset for dm in self.dms]) self.test_dataset = ConcatDataset([dm.test_dataset for dm in self.dms]) # 保存 tokenizer 和 collate 函数 self.tokenizer = self.dms[0].tokenizer self.collate = functools.partial( self.dms[0].train_dataset.collate, mlm_collator=self.dms[0].mlm_collator, ) # 分布式采样器 if self.dist and torch.distributed.is_initialized(): self.train_sampler = DistributedSampler(self.train_dataset, shuffle=True) self.val_sampler = DistributedSampler(self.val_dataset, shuffle=True) self.test_sampler = DistributedSampler(self.test_dataset, shuffle=False) else: self.train_sampler = None self.val_sampler = None self.test_sampler = None def train_dataloader(self): \\\"\\\"\\\" 返回训练 DataLoader 生命周期阶段: Trainer.fit() 内部调用 \\\"\\\"\\\" loader = DataLoader( self.train_dataset, batch_size=self.batch_size, sampler=self.train_sampler, num_workers=self.num_workers, collate_fn=self.collate, ) return loader def val_dataloader(self, batch_size=None): \\\"\\\"\\\" 返回验证 DataLoader 生命周期阶段: Trainer.validate() 或 Trainer.fit() 内部验证调用 \\\"\\\"\\\" loader = DataLoader( self.val_dataset, batch_size=batch_size if batch_size is not None else self.batch_size, sampler=self.val_sampler, num_workers=self.num_workers, collate_fn=self.collate, ) return loader def test_dataloader(self): \\\"\\\"\\\" 返回测试 DataLoader 生命周期阶段: Trainer.test() 内部调用 \\\"\\\"\\\" loader = DataLoader( self.test_dataset, batch_size=self.batch_size, sampler=self.test_sampler, num_workers=self.num_workers, collate_fn=self.collate, ) return loader\",\"_datamodules 字典中保存了 VLMo 所使用到的所有数据集对应的 DataModule 实现类:\",\"_datamodules = { \\\"vg\\\": VisualGenomeCaptionDataModule, \\\"f30k\\\": F30KCaptionKarpathyDataModule, \\\"coco\\\": CocoCaptionKarpathyDataModule, \\\"gcc\\\": ConceptualCaptionDataModule, \\\"sbu\\\": SBUCaptionDataModule, \\\"wikibk\\\": WikibkDataModule, \\\"vqa\\\": VQAv2DataModule, \\\"nlvr2\\\": NLVR2DataModule, }\",\"当子实现类比较多的时候，自然会存在一些重复性操作，因此 VLMo 模型的代码实现中额外抽取了一个抽象类 BaseDataModule 用于定义重复性的模版流程，以此来简化子实现类需要做的操作:\",\"class BaseDataModule(LightningDataModule): def __init__(self, _config): \\\"\\\"\\\" 基础 DataModule 类，支持图文/文本数据集 Args: _config: 配置字典，包含数据路径、batch_size、tokenizer 等信息 \\\"\\\"\\\" super().__init__() # 数据目录 self.data_dir = _config[\\\"data_root\\\"] # DataLoader 参数 self.num_workers = _config[\\\"num_workers\\\"] self.batch_size = _config[\\\"per_gpu_batchsize\\\"] self.eval_batch_size = self.batch_size # 数据处理参数 self.image_size = _config[\\\"image_size\\\"] self.max_text_len = _config[\\\"max_text_len\\\"] self.draw_false_image = _config[\\\"draw_false_image\\\"] self.draw_false_text = _config[\\\"draw_false_text\\\"] self.image_only = _config[\\\"image_only\\\"] self.text_only = _config[\\\"text_only\\\"] # 数据增强/transform 配置 self.train_transform_keys = ( [\\\"default_train\\\"] if len(_config[\\\"train_transform_keys\\\"]) == 0 else _config[\\\"train_transform_keys\\\"] ) self.val_transform_keys = ( [\\\"default_val\\\"] if len(_config[\\\"val_transform_keys\\\"]) == 0 else _config[\\\"val_transform_keys\\\"] ) # tokenizer tokenizer = _config[\\\"tokenizer\\\"] self.tokenizer = get_pretrained_tokenizer(tokenizer) self.vocab_size = self.tokenizer.vocab_size # collator: 用于 MLM（mask language model）训练 collator = ( DataCollatorForWholeWordMask if _config[\\\"whole_word_masking\\\"] else DataCollatorForLanguageModeling ) self.mlm_collator = collator( tokenizer=self.tokenizer, mlm=True, mlm_probability=_config[\\\"mlm_prob\\\"] ) # setup 状态标志，确保 setup 只执行一次 self.setup_flag = False @property def dataset_cls(self): \\\"\\\"\\\" 子类必须实现 返回 dataset 类（通常是 Dataset 子类） \\\"\\\"\\\" raise NotImplementedError(\\\"return tuple of dataset class\\\") @property def dataset_name(self): \\\"\\\"\\\" 子类必须实现 返回数据集名称 \\\"\\\"\\\" raise NotImplementedError(\\\"return name of dataset\\\") def set_train_dataset(self): \\\"\\\"\\\" 构建训练数据集 生命周期阶段: setup() 调用 \\\"\\\"\\\" self.train_dataset = self.dataset_cls( self.data_dir, self.train_transform_keys, split=\\\"train\\\", image_size=self.image_size, max_text_len=self.max_text_len, draw_false_image=self.draw_false_image, draw_false_text=self.draw_false_text, image_only=self.image_only, ) def set_val_dataset(self): \\\"\\\"\\\" 构建验证数据集 生命周期阶段: setup() 调用 \\\"\\\"\\\" self.val_dataset = self.dataset_cls( self.data_dir, self.val_transform_keys, split=\\\"val\\\", image_size=self.image_size, max_text_len=self.max_text_len, draw_false_image=self.draw_false_image, draw_false_text=self.draw_false_text, image_only=self.image_only, ) # 如果存在“无干扰”验证数据集类，额外构建 if hasattr(self, \\\"dataset_cls_no_false\\\"): self.val_dataset_no_false = self.dataset_cls_no_false( self.data_dir, self.val_transform_keys, split=\\\"val\\\", image_size=self.image_size, max_text_len=self.max_text_len, draw_false_image=0, draw_false_text=0, image_only=self.image_only, ) def make_no_false_val_dset(self, image_only=False): \\\"\\\"\\\" 构建无干扰验证数据集（用于评估） \\\"\\\"\\\" return self.dataset_cls_no_false( self.data_dir, self.val_transform_keys, split=\\\"val\\\", image_size=self.image_size, max_text_len=self.max_text_len, draw_false_image=0, draw_false_text=0, image_only=image_only, ) def make_no_false_test_dset(self, image_only=False): \\\"\\\"\\\" 构建无干扰测试数据集（用于评估） \\\"\\\"\\\" return self.dataset_cls_no_false( self.data_dir, self.val_transform_keys, split=\\\"test\\\", image_size=self.image_size, max_text_len=self.max_text_len, draw_false_image=0, draw_false_text=0, image_only=image_only, ) def set_test_dataset(self): \\\"\\\"\\\" 构建测试数据集 生命周期阶段: setup() 调用 \\\"\\\"\\\" self.test_dataset = self.dataset_cls( self.data_dir, self.val_transform_keys, split=\\\"test\\\", image_size=self.image_size, max_text_len=self.max_text_len, draw_false_image=self.draw_false_image, draw_false_text=self.draw_false_text, image_only=self.image_only, ) def setup(self, stage): \\\"\\\"\\\" 数据集构建钩子 生命周期阶段: Trainer.fit(), Trainer.validate(), Trainer.test() 内部调用 \\\"\\\"\\\" if not self.setup_flag: # 构建 train/val/test 数据集 self.set_train_dataset() self.set_val_dataset() self.set_test_dataset() # 给 dataset 注入 tokenizer self.train_dataset.tokenizer = self.tokenizer self.val_dataset.tokenizer = self.tokenizer self.test_dataset.tokenizer = self.tokenizer self.setup_flag = True # 标记 setup 已完成 def train_dataloader(self): \\\"\\\"\\\" 构建训练 DataLoader 生命周期阶段: Trainer.fit() 内部调用 \\\"\\\"\\\" loader = DataLoader( self.train_dataset, batch_size=self.batch_size, shuffle=True, # 训练集通常打乱 num_workers=self.num_workers, pin_memory=True, collate_fn=self.train_dataset.collate, ) return loader def val_dataloader(self): \\\"\\\"\\\" 构建验证 DataLoader 生命周期阶段: Trainer.validate() 或 Trainer.fit() 内部验证调用 \\\"\\\"\\\" loader = DataLoader( self.val_dataset, batch_size=self.eval_batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True, collate_fn=self.val_dataset.collate, ) return loader def test_dataloader(self): \\\"\\\"\\\" 构建测试 DataLoader 生命周期阶段: Trainer.test() 内部调用 \\\"\\\"\\\" loader = DataLoader( self.test_dataset, batch_size=self.eval_batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True, collate_fn=self.test_dataset.collate, ) return loader\",\"VLMo 在训练或验证数据集可能会加入一些“干扰样本”：\",\"draw_false_image=1：给文本配上错误图像\",\"draw_false_text=1：给图像配上错误文本\",\"这种策略有助于模型学习跨模态对齐能力，增强鲁棒性，但它会让数据本身有“噪声”。\",\"为什么需要无干扰数据集？\",\"在训练中，你希望模型看到“有干扰”的数据，提高判别能力；\",\"在评估阶段，你希望衡量模型在真实匹配样本上的性能，这时候就要去掉干扰，即 draw_false_image=0、draw_false_text=0；\",\"这保证了评估指标（如准确率、召回率等）反映的是模型对正确样本的能力，而不是对抗干扰样本的能力。\",\"具体实现:\",\"make_no_false_val_dset → 构建无干扰的验证集，保证验证指标真实可靠；\",\"make_no_false_test_dset → 构建无干扰的测试集，用于最终评估模型效果；\",\"可以选择 image_only=True 或 False 来控制是否只用图像作为输入。\",\"有了 BaseDataModule 类负责完成通用模版流程的抽取，子类需要做的事情就非常简单了，只需要告知父类自己的数据集名和数据集类的具体实现即可:\",\"class CocoCaptionKarpathyDataModule(BaseDataModule): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) @property def dataset_cls(self): return CocoCaptionKarpathyDataset @property def dataset_cls_no_false(self): return CocoCaptionKarpathyDataset @property def dataset_name(self): return \\\"coco\\\"\",\"当 VLMo 通过 LightningDataModule 完成 DataSet 的 prepare 和 set_up 后，下一步便可以通过 DataLoader 来正常获取一个批次的数据了，这里以 CocoCaptionKarpathyDataset 子实现类为例，看一下数据的形式:\",\"class CocoCaptionKarpathyDataset(BaseDataset): def __init__(self, *args, split=\\\"\\\", **kwargs): assert split in [\\\"train\\\", \\\"val\\\", \\\"test\\\"] self.split = split if split == \\\"train\\\": names = [\\\"coco_caption_karpathy_train\\\", \\\"coco_caption_karpathy_restval\\\"] elif split == \\\"val\\\": names = [\\\"coco_caption_karpathy_val\\\"] elif split == \\\"test\\\": names = [\\\"coco_caption_karpathy_test\\\"] super().__init__(*args, **kwargs, names=names, text_column_name=\\\"caption\\\") def __getitem__(self, index): suite = self.get_suite(index) if \\\"test\\\" in self.split: _index, _question_index = self.index_mapper[index] iid = self.table[\\\"image_id\\\"][_index].as_py() iid = int(iid.split(\\\".\\\")[0].split(\\\"_\\\")[-1]) suite.update({\\\"iid\\\": iid}) return suite\",\"通过 CocoCaptionKarpathyDataset 的 __getitem__ 方法，每次可以获取一条样本数据，具体形式如下:\",\"基类 BaseDataset 中提供了 collate 方法，用于 DataLoader 积攒起一批样本数据后，回调该钩子方法完成合适的批量数据格式组织:\",\"class BaseDataset(torch.utils.data.Dataset): def collate(self, batch, mlm_collator): batch_size = len(batch) ... return dict_batch\",\"该方法实现过程比较复杂，但其主要负责将输入的 batch 数据按 key 进行聚合 , 同时对输入的文本数据回调 mlm_collator 钩子方法，完成 Masked Language Modeling（MLM） 任务 , 生成两个新的 key : text_ids_mlm 和 text_labels_mlm 用于表示 MLM 后的 input_ids 和 mask标签。\",\"batch中只有一条数据\",\"按key进行聚合\",\"PyTorch 的 CrossEntropyLoss（尤其是 Hugging Face Transformers 的实现里）中，-100 会被当作 ignore_index，即这些位置不参与 loss 计算\"]},\"383\":{\"h\":\"模型实现\",\"t\":[\"从本节开始，我们将进入 VLMo 模型代码解析的核心部分，首先是其实现的钩子方法 training_step ，该方法负责完成具体的一轮训练实现:\",\"class VLMo(pl.LightningModule): def training_step(self, batch, batch_idx): # 记录一下本轮训练需要推进的学习任务有几个: ['itm', 'itc', 'mlm'] (预训练阶段有三个) vlmo_utils.set_task(self) # 调用 VLMo 的 forward 方法 output = self(batch) # 累加所有学习任务结束后的损失 total_loss = sum([v for k, v in output.items() if \\\"loss\\\" in k]) return total_loss\",\"def set_task(pl_module): pl_module.current_tasks = [ k for k, v in pl_module.hparams.config[\\\"loss_names\\\"].items() if v >= 1 ] return\",\"VLMo 模型的前向传播阶段会根据学习任务列表，分别进行多次独立的前向传播完成对应学习任务推进损失的计算，同时将损失记录在字典中:\",\" def forward(self, batch): ret = dict() if len(self.current_tasks) == 0: ret.update(self.infer(batch)) return ret # Masked Language Modeling if \\\"mlm\\\" in self.current_tasks: ret.update(objectives.compute_mlm(self, batch)) # Textonly Masked Language Modeling if \\\"textmlm\\\" in self.current_tasks: ret.update(objectives.compute_textonly_mlm(self, batch)) # Contrastive loss for pretraining if \\\"itc\\\" in self.current_tasks: ret.update(objectives.compute_itc(self, batch)) # Contrastive loss for finetuning if \\\"irtr\\\" in self.current_tasks: ret.update(objectives.compute_irtr(self, batch)) # Image Text Matching with global hard negative, must use with itc if \\\"itm\\\" in self.current_tasks: ret.update(objectives.compute_itm_hardneg(self, batch, ret[\\\"itc_i2t_logits\\\"], ret[\\\"itc_t2i_logits\\\"])) # Visual Question Answering if \\\"vqa\\\" in self.current_tasks: ret.update(objectives.compute_vqa(self, batch)) # Natural Language for Visual Reasoning 2 if \\\"nlvr2\\\" in self.current_tasks: ret.update(objectives.compute_nlvr2(self, batch)) return ret\",\"下面会分小节独立对每个学习任务的计算过程进行详解:\"]},\"384\":{\"h\":\"Masked Language Modeling\",\"t\":[\"第一个学习目标是 MLM 任务，该任务的学习目标是根据未被掩码的图像序列和文本序列，去预测被掩码的 Token 原来的标签; 具体代码实现如下所示:\",\"def infer( self, batch, mask_text=False, # 是否对文本做MLM掩码（Mask Language Modeling） mask_image=False, # 是否对图像做mask（当前代码未使用） image_token_type_idx=1, # 图像 token 类型的索引（用于 token_type_embeddings） image_embeds=None, # 可选：外部直接传入图像embedding image_masks=None, # 可选：外部直接传入图像mask ): # 1. 选择图像键名 (去除多视角图像选择逻辑) imgkey = \\\"image\\\" # 2. 确定是否使用 MLM 数据（_mlm 后缀） do_mlm = \\\"_mlm\\\" if mask_text else \\\"\\\" # 3. 取出文本相关的张量 text_ids = batch[f\\\"text_ids{do_mlm}\\\"] # 文本 token ID text_labels = batch[f\\\"text_labels{do_mlm}\\\"] # 文本标签（训练时可能是-100占位） text_masks = batch[f\\\"text_masks\\\"] # 文本 attention mask（padding位置为0） # 4. 将 token ID 转成嵌入向量 text_embeds = self.text_embeddings(text_ids) # 5. 取出图像并做视觉编码 img = batch[imgkey][0] # 图像张量（通常是 [B, C, H, W]） image_embeds, image_masks = self.transformer.visual_embed(img) # image_embeds: 图像的 patch embedding # image_masks: 图像的 attention mask # 6. 转成 long 类型（以防下游 embedding 索引出错） image_masks = image_masks.long() # 7. 给文本和图像 embedding 添加 token type embedding（区分模态） text_embeds, image_embeds = ( text_embeds + self.token_type_embeddings(torch.zeros_like(text_masks)), # 文本类型 idx=0 image_embeds + self.token_type_embeddings( torch.full_like(image_masks, image_token_type_idx) # 图像类型 idx=image_token_type_idx ), ) # 8. 将文本和图像序列拼接 co_embeds = torch.cat([text_embeds, image_embeds], dim=1) # 拼接 embedding co_masks = torch.cat([text_masks, image_masks], dim=1) # 拼接 mask # 9. 输入 transformer encoder x = co_embeds relative_position_bias_list = self.get_rel_pos_bias(self.text_imag_relative_position_index) for i, blk in enumerate(self.transformer.blocks): # 每个 block 都处理文本+图像的联合序列 x = blk( x, mask=co_masks, modality_type=\\\"vl\\\", relative_position_bias=relative_position_bias_list[i] ) # 10. LayerNorm 归一化 x = self.transformer.norm(x) # 11. 拆回文本特征和图像特征 text_feats, image_feats = ( x[:, : text_embeds.shape[1]], # 前半部分是文本 x[:, text_embeds.shape[1] :], # 后半部分是图像 ) # 12. 获取 CLS token 的池化特征 cls_feats = self.pooler(x) # 一般是 x[:,0] 做线性变换 # 但这里保留 raw_cls_feats 也就是未pooler的 # 13. 返回推理结果字典 ret = { \\\"text_feats\\\": text_feats, # 文本的序列特征 \\\"image_feats\\\": image_feats, # 图像的序列特征 \\\"cls_feats\\\": cls_feats, # CLS token 经过pooler的特征 \\\"raw_cls_feats\\\": x[:, 0], # CLS token的原始特征 \\\"image\\\": img, # 原始图像张量 \\\"text_labels\\\": text_labels, # 文本标签 \\\"text_ids\\\": text_ids, # 文本 token ID \\\"text_masks\\\": text_masks, # 文本 attention mask } return ret\",\"def compute_mlm(pl_module, batch): # 1. 调用 pl_module 的 infer 方法，开启 mask_text=True # 表示对输入文本做 MLM 掩码，mask_image=False 表示不对图像做掩码 # 返回的 infer 字典中包含文本/图像的特征、标签等信息 infer = pl_module.infer(batch, mask_text=True, mask_image=False) # 2. 将文本特征输入 MLM 预测头（mlm_score），得到预测的 token logits (上下文编码投影到词空间) # mlm_logits 形状通常为 [batch_size, seq_len, vocab_size] mlm_logits = pl_module.mlm_score(infer[\\\"text_feats\\\"]) # 3. 取出 MLM 任务的标签 # 标签中非 mask 位置一般是 -100（会被交叉熵忽略） mlm_labels = infer[\\\"text_labels\\\"] # 4. 计算 MLM 的交叉熵损失 # - 将 logits 展平为 [batch_size*seq_len, vocab_size] # - 将 labels 展平为 [batch_size*seq_len] # - ignore_index=-100 表示这些位置不计入损失 mlm_loss = F.cross_entropy( mlm_logits.view(-1, pl_module.hparams.config[\\\"vocab_size\\\"]), mlm_labels.view(-1), ignore_index=-100, ) # 5. 将 MLM 损失乘以权重 0.25（可能是多任务训练中给 MLM 任务的损失权重） ret = { \\\"mlm_loss\\\": mlm_loss * 0.25, \\\"mlm_logits\\\": mlm_logits, \\\"mlm_labels\\\": mlm_labels, \\\"mlm_ids\\\": infer[\\\"text_ids\\\"], # 原始文本 token id } # 6. 判断当前是训练阶段还是验证阶段 phase = \\\"train\\\" if pl_module.training else \\\"val\\\" # 7. 将 MLM 损失记录到 pl_module 的对应指标（train_mlm_loss 或 val_mlm_loss） loss = getattr(pl_module, f\\\"{phase}_mlm_loss\\\")(ret[\\\"mlm_loss\\\"]) # 8. 计算 MLM 预测准确率（忽略 -100 的位置） acc = getattr(pl_module, f\\\"{phase}_mlm_accuracy\\\")( ret[\\\"mlm_logits\\\"], ret[\\\"mlm_labels\\\"] ) # 9. 记录损失和准确率到日志（方便 TensorBoard / WandB 可视化） pl_module.log(f\\\"mlm/{phase}/loss\\\", loss) pl_module.log(f\\\"mlm/{phase}/accuracy\\\", acc) # 10. 返回结果字典（损失、预测值、标签、文本id） return ret\"]},\"385\":{\"h\":\"Contrastive loss for pretraining\",\"t\":[\"第二个学习目标是 ITC 任务，该任务的学习目标是采用对比学习策略，最大化相似度矩阵对角线的相似度得分，同时最小化非对角线的非匹配样本相似度得分。\",\"def infer_image( self, batch, mask_image=False, # 是否对图像进行 mask（传参保留，但当前实现未使用） image_token_type_idx=1, # 图像 token type id（区分 text=0, image=1） image_embeds=None, # 预计算好的图像 embedding（可选） image_masks=None, # 预计算好的图像 mask（可选） ): # ====== Step 1: 从 batch 取图像 ====== imgkey = \\\"image\\\" img = batch[imgkey][0] # batch[\\\"image\\\"] 是一个列表，取第 0 个元素作为输入图像 # ====== Step 2: 图像编码（patch embedding + mask） ====== # visual_embed 会把图像切成 patch，映射到 embedding 空间，并返回对应的 mask image_embeds, image_masks = self.transformer.visual_embed(img) # ====== Step 3: mask 处理 ====== # 将 mask 转为 long 类型，并放到图像所在的设备上 image_masks = image_masks.long().to(device=img.get_device()) # ====== Step 4: 加上 token type embedding ====== # 给图像 token embedding 加上类型 embedding（值固定为 image_token_type_idx=1） image_embeds = image_embeds + self.token_type_embeddings( torch.full_like(image_masks, image_token_type_idx) ) # ====== Step 5: 初始化 transformer 输入 ====== co_embeds = image_embeds # 融合输入 = 图像 embedding co_masks = image_masks # 融合 mask = 图像 mask x = co_embeds all_hidden_states = [] # 存储每层 hidden states，供后续 VL block 使用 # ====== Step 6: 计算相对位置偏置 ====== relative_position_bias_list = self.get_rel_pos_bias(self.relative_position_index) # ====== Step 7: 单模态图像编码（Image-only Blocks） ====== for i, blk in enumerate(self.transformer.blocks): x = blk( x, mask=co_masks, modality_type=\\\"image\\\", # 单模态模式 = 图像 relative_position_bias=relative_position_bias_list[i] ) all_hidden_states.append(x) # 保存该层输出 # ====== Step 8: 多模态编码（VL-Blocks） ====== # 从 vlffn_start_layer_index-1 层的输出作为起点 vlffn_hiddens = all_hidden_states[self.vlffn_start_layer_index - 1] # 从指定层开始，走 VL 模式（Vision-Language 融合） for vlffn_index in range(self.vlffn_start_layer_index, self.num_layers): vlffn_hiddens = self.transformer.blocks[vlffn_index]( vlffn_hiddens, mask=co_masks, modality_type=\\\"vl\\\", # 融合模式 relative_position_bias=relative_position_bias_list[vlffn_index] ) # ====== Step 9: 单模态最终输出 ====== vffn_hiddens = all_hidden_states[-1] # 最后一层（图像模式）hidden states vffn_hiddens = self.transformer.norm(vffn_hiddens) # LayerNorm 归一化 text_feats, image_feats = ( None, # 文本特征为空 vffn_hiddens, # 图像特征序列 ) # ====== Step 10: ITC 用的图像 CLS 特征 ====== cls_feats = self.itc_image_proj(vffn_hiddens[:, 0]) # 取 [CLS] token cls_feats = cls_feats / cls_feats.norm(dim=-1, keepdim=True) # L2 归一化 # ====== Step 11: VL 融合后的 CLS 特征 ====== vlffn_hiddens = self.transformer.norm(vlffn_hiddens) # 归一化 cls_vlffn_feats = self.itc_vl_image_proj(vlffn_hiddens[:, 0]) # 投影到 ITC 空间 cls_vlffn_feats = cls_vlffn_feats / cls_vlffn_feats.norm(dim=-1, keepdim=True) # ====== Step 12: 打包返回结果 ====== ret = { \\\"text_feats\\\": text_feats, # 文本特征（None） \\\"image_feats\\\": image_feats, # 图像特征（序列输出） \\\"cls_feats\\\": cls_feats, # 单模态 ITC CLS 特征 \\\"cls_vlffn_feats\\\": cls_vlffn_feats, # 跨模态 ITC CLS 特征 \\\"raw_cls_feats\\\": x[:, 0], # 原始 CLS token（最后一次 forward 的） \\\"image_masks\\\": image_masks, # 图像 mask \\\"text_labels\\\": None, # 文本标签（空） \\\"text_ids\\\": None, # 文本 ID（空） \\\"text_masks\\\": None, # 文本 mask（空） } return ret\",\"def infer_text( self, batch, mask_text=False, ): # 如果开启了 Masked Language Modeling（MLM），就从 batch 里取对应的 \\\"_mlm\\\" 字段 do_mlm = \\\"_mlm\\\" if mask_text else \\\"\\\" text_ids = batch[f\\\"text_ids{do_mlm}\\\"] # 输入 token id 序列 text_labels = batch[f\\\"text_labels{do_mlm}\\\"] # MLM 的标签（预测目标），正常推理时为 -100 text_masks = batch[f\\\"text_masks\\\"] # attention mask（哪些位置有效） # 将输入 token id 转换为 embedding 向量 text_embeds = self.text_embeddings(text_ids) # 加上 token_type_embeddings（区分模态，这里 text 全是 0） text_embeds = text_embeds + self.token_type_embeddings(torch.zeros_like(text_masks)) # co_* 表示“当前模态（text）的输入” co_embeds = text_embeds co_masks = text_masks # 初始化 transformer 输入 x = co_embeds all_hidden_states = [] # 获取相对位置偏置（transformer attention 中的 bias） relative_position_bias_list = self.get_rel_pos_bias(self.text_relative_position_index) # ====== 第一阶段：单模态 Transformer 编码 ====== # 遍历 transformer 的所有 Block，每个 block 按 \\\"text\\\" 模式运行 for i, blk in enumerate(self.transformer.blocks): x = blk( x, mask=co_masks, modality_type=\\\"text\\\", # 单模态模式（仅文本） relative_position_bias=relative_position_bias_list[i] ) all_hidden_states.append(x) # 记录每层输出，供后续 VL block 使用 # ====== 第二阶段：跨模态 Transformer 编码 ====== # 从 vlffn_start_layer_index-1 层的 hidden states 作为输入 vlffn_hiddens = all_hidden_states[self.vlffn_start_layer_index - 1] # 再次遍历后半部分 block，但这次切换成 \\\"vl\\\" 模式（允许跨模态交互） for vlffn_index in range(self.vlffn_start_layer_index, self.num_layers): vlffn_hiddens = self.transformer.blocks[vlffn_index]( vlffn_hiddens, mask=co_masks, modality_type=\\\"vl\\\", # 融合模式（Vision-Language） relative_position_bias=relative_position_bias_list[vlffn_index] ) # ====== 单模态输出 ====== lffn_hiddens = all_hidden_states[-1] # 最后一层（text 模式）的输出 lffn_hiddens = self.transformer.norm(lffn_hiddens) # 层归一化 text_feats, image_feats = ( lffn_hiddens, # 单模态文本特征 None, # 图像为空 ) # 提取 [CLS] token 的 embedding，并投影到对比学习空间（ITC） cls_feats = self.itc_text_proj(lffn_hiddens[:, 0]) cls_feats = cls_feats / cls_feats.norm(dim=-1, keepdim=True) # L2 归一化 # ====== 跨模态输出 ====== vlffn_hiddens = self.transformer.norm(vlffn_hiddens) # 层归一化 # 提取 VL 融合后的 [CLS] embedding，用于 ITM/VQA 等任务 cls_vlffn_feats = self.itc_vl_text_proj(vlffn_hiddens[:, 0]) cls_vlffn_feats = cls_vlffn_feats / cls_vlffn_feats.norm(dim=-1, keepdim=True) # ====== 打包返回结果 ====== ret = { \\\"text_feats\\\": text_feats, # 单模态文本特征序列 \\\"image_feats\\\": image_feats, # 图像特征（None） \\\"cls_feats\\\": cls_feats, # 单模态 [CLS] 特征（ITC 用） \\\"cls_vlffn_feats\\\": cls_vlffn_feats, # 融合 [CLS] 特征（ITM/VQA 用） \\\"raw_cls_feats\\\": x[:, 0], # 原始 [CLS] 输出（未投影） \\\"image_masks\\\": None, # 图像 mask（None） \\\"text_labels\\\": text_labels, # 文本标签（MLM 或 -100） \\\"text_ids\\\": text_ids, # 文本输入 ID \\\"text_masks\\\": text_masks, # 文本 mask } return ret\",\"# The implementation of image-text contrastive refers to open_clip (https://github.com/mlfoundations/open_clip) def compute_itc(pl_module, batch, aggregate=True): # 1. 分别对图像和文本做前向推理，提取 ITC 所需特征 infer_imag = pl_module.infer_image(batch, mask_image=False) # 图像推理 infer_text = pl_module.infer_text(batch, mask_text=False) # 文本推理 # 2. 取出图像和文本的 CLS 特征（单模态 ITC 特征） image_features = infer_imag[\\\"cls_feats\\\"] text_features = infer_text[\\\"cls_feats\\\"] # 3. 取出 ITC 训练时的对比缩放因子（logit_scale），并取指数保证其 > 0 logit_scale = pl_module.logit_scale.exp().mean() # 4. 取出经过 VL-FFN（跨模态融合层）的 CLS 特征（跨模态 ITC 特征） image_vlffn_features = infer_imag[\\\"cls_vlffn_feats\\\"] text_vlffn_features = infer_text[\\\"cls_vlffn_feats\\\"] # 5. 跨模态 ITC 的缩放因子 logit_vl_scale = pl_module.logit_vl_scale.exp().mean() # ========== 处理分布式训练下的负样本扩展 ========== if aggregate: ... else: # 7. 单机训练时，直接计算相似度 logits_per_image = logit_scale * image_features @ text_features.t() logits_per_text = logit_scale * text_features @ image_features.t() # 8. 构造 ground truth 标签：第 i 个样本对应第 i 个正样本 ground_truth = torch.arange(len(logits_per_image)).long().to(device=logits_per_image.get_device()) # 9. 计算单模态 ITC loss（对称 CE，image→text + text→image） itc_loss = ( F.cross_entropy(logits_per_image.float(), ground_truth) + F.cross_entropy(logits_per_text.float(), ground_truth) ) / 2 # 10. 计算跨模态 VL-FFN ITC loss (仅在分布式环境下启用) itc_vlffn_loss = ( F.cross_entropy(logits_per_vlffn_image.float(), ground_truth) + F.cross_entropy(logits_per_vlffn_text.float(), ground_truth) ) / 2 # 11. 总 loss：两者平均再乘权重 itc_total_loss = (itc_loss + itc_vlffn_loss) * 0.5 # 12. 返回结果字典 ret = { \\\"itc_loss\\\": itc_total_loss, # 总 ITC 损失 \\\"itc_i2t_logits\\\": logits_per_image, # image→text logits \\\"itc_t2i_logits\\\": logits_per_text, # text→image logits \\\"itc_labels\\\": ground_truth, # 标签 \\\"itc_logit_scale\\\": logit_scale, # 单模态 logit 缩放因子 \\\"itc_logit_vl_scale\\\": logit_vl_scale, # 跨模态 logit 缩放因子 } # ========== 训练/验证过程中的日志记录 ========== phase = \\\"train\\\" if pl_module.training else \\\"val\\\" # 13. 记录 loss 和缩放因子 loss = getattr(pl_module, f\\\"{phase}_itc_loss\\\")(ret[\\\"itc_loss\\\"]) scale = getattr(pl_module, f\\\"{phase}_itc_logit_scale\\\")(ret[\\\"itc_logit_scale\\\"]) # 14. 单模态 ITC 准确率 i2t_acc = getattr(pl_module, f\\\"{phase}_itc_i2t_accuracy\\\")(ret[\\\"itc_i2t_logits\\\"], ret[\\\"itc_labels\\\"]) t2i_acc = getattr(pl_module, f\\\"{phase}_itc_t2i_accuracy\\\")(ret[\\\"itc_t2i_logits\\\"], ret[\\\"itc_labels\\\"]) pl_module.log(f\\\"itc/{phase}/loss\\\", loss) pl_module.log(f\\\"itc/{phase}/logit_scale\\\", scale) pl_module.log(f\\\"itc/{phase}/i2t_accuracy\\\", i2t_acc) pl_module.log(f\\\"itc/{phase}/t2i_accuracy\\\", t2i_acc) # 15. 跨模态 VL-FFN ITC 的日志记录 vl_scale = getattr(pl_module, f\\\"{phase}_itc_vl_logit_scale\\\")(ret[\\\"itc_logit_vl_scale\\\"]) vl_i2t_acc = getattr(pl_module, f\\\"{phase}_itc_vl_i2t_accuracy\\\")(logits_per_vlffn_image, ret[\\\"itc_labels\\\"]) vl_t2i_acc = getattr(pl_module, f\\\"{phase}_itc_vl_t2i_accuracy\\\")(logits_per_vlffn_text, ret[\\\"itc_labels\\\"]) pl_module.log(f\\\"itc/{phase}/vl_logit_scale\\\", vl_scale) pl_module.log(f\\\"itc/{phase}/vl_i2t_accuracy\\\", vl_i2t_acc) pl_module.log(f\\\"itc/{phase}/vl_t2i_accuracy\\\", vl_t2i_acc) return ret\",\"跨模态 VL-FFN ITC Loss\",\"1. 为什么只在分布式环境下启用？\",\"代码实现逻辑：在 compute_itc 中，VL-FFN ITC loss 只在 aggregate=True 分支下计算，而 aggregate=True 对应分布式环境；单机模式（aggregate=False）默认不计算 VL-FFN ITC。\",\"主要动机：\",\"负样本数量：ITC loss 对负样本依赖大，VL-FFN ITC 特征较特殊，单机 batch 内负样本有限，训练不稳定；分布式环境通过 all_gather 扩展了负样本数量。\",\"资源开销：VL-FFN ITC 需要额外存储和计算 VL-FFN 特征，单机环境下显存/算力消耗较大；分布式环境充分利用多卡资源。\",\"理论上可在单机计算：禁用只是实现选择，可根据需要在单机环境加入 VL-FFN ITC 计算，但可能效果受限。\",\"2. 跨模态 VL-FFN ITC Loss 的作用\",\"单模态对齐：像 CLIP 那样，保证 encoder 自身就能学到基本的跨模态检索能力。\",\"跨模态对齐：在融合层之后，进一步对齐“混合表征”，使得 VL Blocks 也被直接监督，而不是只靠 ITM/MLM 任务间接学习。\"]},\"386\":{\"h\":\"Image-Text Matching\",\"t\":[\"第三个学习目标是 ITM 任务，该任务的学习目标是通过硬负样本策略，将正匹配的图文对与相似但不匹配的负样本区分开来，从而训练模型学会细粒度跨模态对齐。\",\" # Image Text Matching with global hard negative, must use with itc if \\\"itm\\\" in self.current_tasks: ret.update(objectives.compute_itm_hardneg(self, batch, ret[\\\"itc_i2t_logits\\\"], ret[\\\"itc_t2i_logits\\\"]))\",\"回顾 VLMo 模型的 forward 方法可知，再计算 ITM 学习目标之前，需要先完成 ITC 学习目标的计算，利用 ITC 提供的相似度矩阵完成 hard negative examples 的挖掘。\",\"def compute_itm_hardneg(pl_module, batch, sim_i2t, sim_t2i): # 获取正负样本数量及 batch size pos_len = batch[\\\"text_ids\\\"].size(0) neg_len = batch[\\\"text_ids\\\"].size(0) bsz = batch[\\\"text_ids\\\"].size(0) # 构建 ITM 标签：正样本为 1，负样本为 0 itm_labels = torch.cat([torch.ones(pos_len), torch.zeros(neg_len), torch.zeros(neg_len)]).to( pl_module.device ) # 拷贝 batch，防止原 batch 被修改 batch = {k: v for k, v in batch.items()} # 正样本推理 infer_pos = pl_module.infer(batch, mask_text=False, mask_image=False) batch_text_ids = infer_pos[\\\"text_ids\\\"] batch_text_masks = infer_pos[\\\"text_masks\\\"] batch_image = infer_pos[\\\"image\\\"] with torch.no_grad(): world_size = dist.get_world_size() rank = dist.get_rank() # 初始化各 GPU 上的 tensor，用于收集所有 GPU 的 batch 数据 gathered_text_ids = [ torch.zeros_like(batch_text_ids) for _ in range(world_size) ] gathered_text_masks = [ torch.zeros_like(batch_text_masks) for _ in range(world_size) ] gathered_image = [ torch.zeros_like(batch_image) for _ in range(world_size) ] # 分布式收集所有 GPU 的 tensor（多 GPU 环境） dist.all_gather(gathered_text_ids, batch_text_ids) dist.all_gather(gathered_text_masks, batch_text_masks) dist.all_gather(gathered_image, batch_image) # 合并收集的 tensor，排除当前 GPU 自己的部分 all_text_ids = torch.cat( [batch_text_ids] + gathered_text_ids[:rank] + gathered_text_ids[rank + 1 :] ) all_text_masks = torch.cat( [batch_text_masks] + gathered_text_masks[:rank] + gathered_text_masks[rank + 1 :] ) all_image = torch.cat( [batch_image] + gathered_image[:rank] + gathered_image[rank + 1 :] ) with torch.no_grad(): # 计算图像到文本、文本到图像的相似度权重 weights_i2t = F.softmax(sim_i2t[:bsz, :].float(), dim=1) weights_t2i = F.softmax(sim_t2i[:bsz, :].float(), dim=1) # 将对角线置零，避免选中正样本作为负样本 weights_i2t.fill_diagonal_(0) weights_t2i.fill_diagonal_(0) # 为每个图像选择一个负文本 images_neg = [] for b in range(bsz): neg_idx = torch.multinomial(weights_t2i[b], 1).item() images_neg.append(all_image[neg_idx]) images_neg = torch.stack(images_neg, dim=0) # 为每个文本选择一个负图像 text_ids_neg = [] text_masks_neg = [] for b in range(bsz): neg_idx = torch.multinomial(weights_i2t[b], 1).item() text_ids_neg.append(all_text_ids[neg_idx]) text_masks_neg.append(all_text_masks[neg_idx]) text_ids_neg = torch.stack(text_ids_neg, dim=0) text_masks_neg = torch.stack(text_masks_neg, dim=0) # 构建负样本 batch 并进行推理 batch_imgs_neg = {\\\"image\\\":[images_neg], \\\"text_ids\\\":batch[\\\"text_ids\\\"], \\\"text_labels\\\":batch[\\\"text_labels\\\"], \\\"text_masks\\\":batch[\\\"text_masks\\\"]} infer_imags_neg = pl_module.infer(batch_imgs_neg, mask_text=False, mask_image=False) batch_text_neg = {\\\"image\\\":batch[\\\"image\\\"], \\\"text_ids\\\":text_ids_neg, \\\"text_labels\\\":batch[\\\"text_labels\\\"], \\\"text_masks\\\":text_masks_neg} infer_text_neg = pl_module.infer(batch_text_neg, mask_text=False, mask_image=False) # 合并正负样本特征 all_cls_feats = torch.cat([infer_pos[\\\"cls_feats\\\"], infer_imags_neg[\\\"cls_feats\\\"], infer_text_neg[\\\"cls_feats\\\"]], dim=0) # 计算 ITM logits 和 loss itm_logits = pl_module.itm_score(all_cls_feats) itm_loss = F.cross_entropy(itm_logits, itm_labels.long()) # 构建返回字典 ret = { \\\"itm_loss\\\": itm_loss, \\\"itm_logits\\\": itm_logits, \\\"itm_labels\\\": itm_labels, } # 根据训练/验证阶段计算并记录 loss 与 accuracy phase = \\\"train\\\" if pl_module.training else \\\"val\\\" loss = getattr(pl_module, f\\\"{phase}_itm_loss\\\")(ret[\\\"itm_loss\\\"]) acc = getattr(pl_module, f\\\"{phase}_itm_accuracy\\\")( ret[\\\"itm_logits\\\"], ret[\\\"itm_labels\\\"] ) pl_module.log(f\\\"itm/{phase}/loss\\\", loss) pl_module.log(f\\\"itm/{phase}/accuracy\\\", acc) return ret\"]},\"387\":{\"h\":\"ViLT 论文\",\"t\":[\"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision 论文简析\",\"论文链接: ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision 代码链接: https://github.com/dandelin/vilt\"]},\"388\":{\"h\":\"Introduction\",\"t\":[\"视觉-语言预训练（VLP）领域中，传统的视觉特征提取主要有两种典型实现方案：\",\"Region Feature（区域特征）：通常使用预训练的目标检测器（如基于 Visual Genome 数据集训练的检测模型）来定位图像中的物体区域，并提取每个区域的特征。这种方法能够捕获较为精细的对象信息，是许多早期VLP模型的标准做法，但计算复杂且处理速度较慢。\",\"Grid Feature（网格特征）：用卷积神经网络（如 ResNet）对整张图像进行处理，将图像划分为固定大小的网格，通过卷积提取每个网格的视觉特征。这种方式避免了目标检测的步骤，提取速度相对更快，但仍依赖卷积架构，计算资源消耗仍然较大。\",\"ViLT模型提出了一种极简化的视觉嵌入方案，摒弃了传统的目标检测和卷积视觉嵌入器，采用无卷积的浅层线性投影直接将图像块（patch）嵌入，并与文本token一同输入transformer处理。这样，ViLT不仅极大降低了模型参数和计算负担，实现了比基于区域特征的模型快数十倍、比基于网格特征的模型快至少四倍的推理速度，还在多项视觉-语言任务中取得了竞争力甚至更优的性能。\",\"此外，ViLT首次引入了全词掩码和图像增强技术于视觉-语言预训练，进一步推动了模型的下游表现，展示了其轻量化设计在效率与性能上的优势。\",\"Contribution:\",\"第一个基于patch projection的多模态预训练模型，其是首个使用patch projection来做visual embedding的方法。\",\"证明了可以将BERT的方法和Vison Transformer结合起来用于多模态transformer。\",\"体现了全词掩码在预训练时以及图像增强在微调时的重要性。\"]},\"389\":{\"h\":\"Motivation\",\"t\":[\"目前参数量最小的多模态Transformer方法。ViLT使用预训练的ViT来初始化交互的transformer，这样就可以直接利用交互层来处理视觉特征，不需要额外增加一个视觉encoder（如Faster-RCNN）。\"]},\"390\":{\"h\":\"Method\",\"t\":[\"现有的视觉语言模型的三种结构类别：\",\"VE = Vision Embedding\",\"TE = Text Embedding\",\"MI = Modality Interaction\",\"上图是4种不同类型的VLP模型示意图。其中每个矩形的高表示相对计算量大小，VE、TE和MI分别是visual embedding、text embedding和modality interaction的简写。\",\"作者提出这4种类型的主要依据有两点：\",\"在参数或者计算上，两种模态是否保持平衡。\",\"在网络深层中，两种模态是否相互作用。\",\"VSE、VSE++和SCAN属于(a)类型。对图像和文本独立使用encoder，图像的更重，文本的更轻，使用简单的点积或者浅层attention层来表示两种模态特征的相似性。\",\"CLIP属于(b)类型。每个模态单独使用重的transformer encoder，使用池化后的图像特征点积计算特征相似性。\",\"ViLBERT、UNTER和Pixel-BERT属于(c)类型。这些方法使用深层transformer进行交互作用，但是由于VE仍然使用重的卷积网络进行特征抽取，导致计算量依然很大。\",\"作者提出的ViLT属于(d)类型。ViLT是首个将VE设计的如TE一样轻量的方法，该方法的主要计算量都集中在模态交互上。\"]},\"391\":{\"h\":\"Modality Interaction Schema\",\"t\":[\"模态交互部分可以分成两种方式：一种是single-stream(如BERT和UNITER)，另一种是dual-stream(如ViLBERT和LXMERT)。其中single-stream是对图像和文本concate然后进行交互操作，而dual-stream是不对图像和文本concate然后进行交互操作。ViLT延用single-stream的交互方式，因为dual-stream会引入额外的计算量。\",\"现有的VLP模型的text embedding基本上都使用类BERT结构(图1)，但是visual embedding存在着差异。在大多数情况下，visual embedding是现有VLP模型的瓶颈。visual embedding的方法总共有三大类，其中region feature方法通常采用Faster R-CNN二阶段检测器提取region的特征，grid feature方法直接使用CNN提取grid的特征，patch projection方法将输入图片切片投影提取特征。ViLT是首个使用patch projection来做visual embedding的方法。\"]},\"392\":{\"h\":\"Model Structure\",\"t\":[\"作者提出的ViLT可以认为是目前最简单的多模态Transformer方法。ViLT使用预训练的ViT来初始化交互的transformer，这样就可以直接利用交互层来处理视觉特征，不需要额外增加一个视觉encoder。\",\"文本特征输入部分，将文本看成一个词序列，通过word embedding matrix转化成word embedding，然后和position embedding进行相加，最后和modal-type embedding进行concate。\",\"图像特征输入部分，将图像切块看成一个图像块序列，通过linear projection转化成visual embedding，然后和postion embedding进行相加，最后和modal-type embedding进行concate。\",\"其中word embedding和visual embedding通过可学习的modal-type embedding标志位来区分，其中0标志位表示word embedding部分，1标志位表示visual embedding部分。\",\"wrod embedding和visual embedding分别都嵌入了一个额外的可学习 [class] embedding，方便和下游任务对接。\"]},\"393\":{\"h\":\"Pretraining Objectives\",\"t\":[\"ViLT预训练的优化目标有两个：一个是image text matching(ITM)，另一个是masked language modeling(MLM)。\",\"ImageText Matching：随机以0.5的概率将文本对应的图片替换成不同的图片，然后对文本标志位对应输出使用一个线性的ITM head将输出feature映射成一个二值logits，用来判断图像文本是否匹配。另外ViLT还设计了一个word patch alignment (WPA)来计算textual subset和visual subset的对齐分数。\",\"Masked Language Modeling：MLM的目标是通过文本的上下文信息去预测masked的文本tokens。随机以0.15的概率mask掉tokens，然后文本输出接两层MLP预测mask掉的tokens。\",\"Whole Word Masking：另外ViLT还使用了whole word masking技巧。whole word masking是将连续的子词tokens进行mask的技巧，避免了只通过单词上下文进行预测。比如将“giraffe”词tokenized成3个部分[“gi”, “##raf”, “##fe”]，可以mask成[“gi”, “[MASK]”, “##fe”]，模型会通过mask的上下文信息[“gi”，“##fe”]来预测mask的“##raf”，就会导致不利用图像信息。\"]},\"394\":{\"h\":\"Conclusion\",\"t\":[\"本文提出的方法在效率上大大提升且表现出相似的性能，相比于region feature的方法速度快了60倍，相比于grid feature的方法快了4倍，而且下游任务表现出相似甚至更好的性能。\",\"缺点：\",\"1、性能不够高，在一些数据集上的表现比不过C类方法，有可能因为对于现有的任务来说，因为数据集的bias，或者这个任务需要更多的视觉信息，因此需要更多得视觉部分，最后的效果才能好。\",\"2、虽然推理时间快，但是训练速度很慢。只是结构上简化了多模态学习，但一般人还是玩不起。\"]},\"395\":{\"h\":\"多模态常用改编Bert代码实现\",\"t\":[\"多模态论文中常用的改编版本的Bert代码实现记录\",\"本文改编Bert代码讲解基于BLIP项目展开，代码链接: BLIP/models/med.py\"]},\"396\":{\"h\":\"多模态 Bert 前向传播流程\",\"t\":[\"本节我们将对多模态Bert的前向传播基本流程进行讲解，所给代码删除了大量非核心逻辑，如需了解各类优化手段，请阅读源码进行学习。\"]},\"397\":{\"h\":\"1. 整体流程总览（BertModel）\",\"t\":[\"class BertModel(BertPreTrainedModel): def forward( self, input_ids=None, attention_mask=None, position_ids=None, encoder_hidden_states=None, # 图像模态特征 encoder_attention_mask=None, # 图像掩码 is_decoder=False, mode='multimodal', # 控制是否启用 cross-attention ): # 1. 词嵌入 + 位置编码 embedding_output = self.embeddings(input_ids=input_ids, position_ids=position_ids) # 2. 编码阶段（Text-only 或 Cross-modal） sequence_output = self.encoder( embedding_output, attention_mask=extended_attention_mask, # 可用于多头自注意力的文本 padding mask encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, # 可用于多头自注意力的图像 padding mask mode=mode, ) # 3. 池化输出（用于分类任务） pooled_output = self.pooler(sequence_output) if self.pooler is not None else None return BaseModelOutputWithPoolingAndCrossAttentions( last_hidden_state=sequence_output, pooler_output=pooled_output, )\",\"池化输出实现:\",\"class BertPooler(nn.Module): def __init__(self, config): super().__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) self.activation = nn.Tanh() def forward(self, hidden_states): # 1. 拿到能够代表整段文本或者整个多模态表示的 CLS Token first_token_tensor = hidden_states[:, 0] # 2. 非线性变换 pooled_output = self.dense(first_token_tensor) pooled_output = self.activation(pooled_output) return pooled_output\"]},\"398\":{\"h\":\"2. 编码器：BertEncoder\",\"t\":[\"class BertEncoder(nn.Module): def __init__(self, config): self.layer = nn.ModuleList([BertLayer(config, i) for i in range(config.num_hidden_layers)]) def forward( self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, mode='multimodal', ): for i in range(self.config.num_hidden_layers): layer_module = self.layer[i] hidden_states = layer_module( hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, mode=mode, ) return hidden_states\",\"多模态关键点：\",\"多模态时，每个 Layer 都有机会执行 cross-attention。\",\"encoder_hidden_states 来自视觉模型（如 ViT 的输出），将图像特征注入到文本流中。\"]},\"399\":{\"h\":\"3. Transformer 层：BertLayer\",\"t\":[\"class BertLayer(nn.Module): def forward( self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, mode=None, ): # 1. 自注意力（Self-Attention） attention_output = self.attention(hidden_states, attention_mask) # 2. 多模态交叉注意力（Cross-Attention） if mode == 'multimodal': attention_output = self.crossattention( attention_output, attention_mask, encoder_hidden_states, encoder_attention_mask, ) return attention_output\",\"多模态关键点：\",\"自注意力捕捉文本内部的依赖；\",\"跨模态注意力（CrossAttention）让文本 Query 关注图像 Key 和 Value，实现信息融合。\"]},\"400\":{\"h\":\"4. Attention 模块：BertAttention\",\"t\":[\"class BertAttention(nn.Module): def forward( self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, ): self_outputs = self.self( hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, ) # attention 后应用一个 MLP return self.output(self_outputs, hidden_states)\",\"MLP 实现:\",\"class BertSelfOutput(nn.Module): def __init__(self, config): super().__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, hidden_states, input_tensor): hidden_states = self.dense(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.LayerNorm(hidden_states + input_tensor) return hidden_states\"]},\"401\":{\"h\":\"5. 核心计算：BertSelfAttention\",\"t\":[\"class BertSelfAttention(nn.Module): def forward( self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, ): # 获取 Query mixed_query_layer = self.query(hidden_states) # 判断是否为 Cross Attention is_cross_attention = encoder_hidden_states is not None if is_cross_attention: key_layer = self.transpose_for_scores(self.key(encoder_hidden_states)) value_layer = self.transpose_for_scores(self.value(encoder_hidden_states)) attention_mask = encoder_attention_mask else: key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) query_layer = self.transpose_for_scores(mixed_query_layer) # 计算 Attention 分数（缩放点积注意力） attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) attention_scores = attention_scores / math.sqrt(self.attention_head_size) # 加 Mask if attention_mask is not None: attention_scores = attention_scores + attention_mask # Softmax 归一化为权重 attention_probs = nn.Softmax(dim=-1)(attention_scores) # Dropout（来自 Transformer 原始实现） attention_probs_dropped = self.dropout(attention_probs) # 应用注意力权重 context_layer = torch.matmul(attention_probs_dropped, value_layer) # Reshape context_layer = context_layer.permute(0, 2, 1, 3).contiguous() new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) return context_layer.view(*new_context_layer_shape)\"]},\"402\":{\"h\":\"6. 小结\",\"t\":[\"多模态交互核心(Cross Attention):\",\"项目\",\"说明\",\"Query\",\"来自文本（attention_output）\",\"Key/Value\",\"来自图像（encoder_hidden_states）\",\"作用\",\"让文本动态关注图像区域，建立 Token 与视觉 Patch 的对齐\",\"应用\",\"文本问图（VQA）、图文检索、图文生成等多模态任务\",\"总结:\",\" +--------------------------+ | Text Embeddings | +-----------+--------------+ | [Transformer Encoder] | ┌────────┴───────────┐ │ Self-Attention │ │ (Text <-> Text) │ └────────┬───────────┘ │ ┌────────▼───────────┐ │ Cross-Attention │ <--- 图像特征作为 Key / Value │ (Text <-> Image) │ └────────┬───────────┘ │ FeedForward + LayerNorm + Residual\"]},\"403\":{\"h\":\"自回归语言建模\",\"t\":[\"BertLMHeadModel 是基于 BERT 构建的 语言建模头（Language Modeling Head）模型，其主要用于 自回归语言建模（Causal Language Modeling, CLM），尤其是在 多模态生成任务中充当解码器。它通常用于像 UNITER、VLBERT、MiniGPT-4、BLIP 等多模态架构中的文本生成部分。\",\"class BertLMHeadModel(BertPreTrainedModel): def __init__(self, config): self.bert = BertModel(config, add_pooling_layer=False) self.cls = BertLMPredictionHead(config) def forward( self, input_ids=None, attention_mask=None, position_ids=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, is_decoder=True, reduction='mean', mode='multimodal', ): # 1. 调用BertModel outputs = self.bert( input_ids, attention_mask=attention_mask, position_ids=position_ids, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, is_decoder=is_decoder, mode=mode, ) # 2. 解码 sequence_output = outputs[0] prediction_scores = self.cls(sequence_output) # 3. 返回预测得分 if return_logits: return prediction_scores[:, :-1, :].contiguous() # 返回预测出来的: [x2,x3,...,xn] , 丢掉 X(n+1) # 4. 计算 next-token prediction 损失 lm_loss = None if labels is not None: # 4.1 模型预测出来的: [x2,x3,...,xn] , 丢掉 X(n+1) 和 标签: [x2,x3,...,xn] , 丢掉 X(1) shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous() labels = labels[:, 1:].contiguous() # 4.2 计算交叉熵损失 loss_fct = CrossEntropyLoss(reduction=reduction, label_smoothing=0.1) lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1)) if reduction=='none': lm_loss = lm_loss.view(prediction_scores.size(0),-1).sum(1) return CausalLMOutputWithCrossAttentions( loss=lm_loss, logits=prediction_scores, )\",\"# 对输入进行非线性变换: 投影 + 激活 + 归一化 class BertPredictionHeadTransform(nn.Module): def __init__(self, config): super().__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) # 默认采用GELU激活函数 if isinstance(config.hidden_act, str): self.transform_act_fn = ACT2FN[config.hidden_act] else: self.transform_act_fn = config.hidden_act self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) def forward(self, hidden_states): hidden_states = self.dense(hidden_states) hidden_states = self.transform_act_fn(hidden_states) hidden_states = self.LayerNorm(hidden_states) return hidden_states class BertLMPredictionHead(nn.Module): def __init__(self, config): self.transform = BertPredictionHeadTransform(config) self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False) def forward(self, hidden_states): # 1. 非线性变换 hidden_states = self.transform(hidden_states) # 2. 解码: 将(seq_len,hidden_size)中每个word映射到词空间 hidden_states = self.decoder(hidden_states) return hidden_states\"]},\"404\":{\"h\":\"庖丁解牛CLIP\",\"t\":[\"多模态模型CLIP原理与图片分类，文字搜索图像实战演练\",\"CLIP原始论文链接\"]},\"405\":{\"h\":\"引言\",\"t\":[\"2021 年可谓是视觉 Transformer（Vision Transformer）大放异彩的一年。自谷歌提出 ViT 之后，众多基于视觉 Transformer 的研究如潮水般涌来，广泛应用于各类计算机视觉任务。与此同时，OpenAI 在 2021 年 1 月发布的 DALL-E 和 CLIP，同样给计算机视觉领域带来了巨大影响。这两个模型都属于融合图像与文本的多模态模型，其中 DALL-E 是基于文本输入来生成图像的模型，而 CLIP 则是以文本作为监督信号，训练出具有可迁移能力的视觉模型。和 ViT 类似，DALL-E 和 CLIP 的出现也掀起了新一轮的研究热潮。\"]},\"406\":{\"h\":\"介绍\",\"t\":[\"CLIP的英文全称为Contrastive Language-Image Pre-training，它代表着一种基于对比文本-图像对的预训练方法，同时也指运用该方法构建的模型。CLIP属于基于对比学习的多模态模型。与计算机视觉（CV）领域中的一些对比学习方法，像MoCo和SimCLR有所不同，CLIP的训练数据采用的是文本-图像对，也就是一张图像搭配与之对应的文本描述。在训练过程中，借助对比学习机制，期望模型能够学习到文本和图像之间的匹配关系。\"]},\"407\":{\"h\":\"训练\",\"t\":[\"CLIP包含两个核心模型，分别是文本编码器（Text Encoder）和图像编码器（Image Encoder）。其中，文本编码器的作用是提取文本的特征，在实现时可采用自然语言处理（NLP）领域常用的文本Transformer模型；而图像编码器则用于提取图像的特征，在实际应用中可以选用常见的卷积神经网络（CNN）模型，也可以采用视觉Transformer模型。\",\"这里对提取的文本特征和图像特征进行对比学习。对于一个包含个文本-图像对的训练batch，将个文本特征和个图像特征两两组合，CLIP模型会预测出个可能的文本-图像对的相似度，这里的相似度直接计算文本特征和图像特征的余弦相似性（cosine similarity），即上图所示的矩阵。这里共有个正样本，即真正属于一对的文本和图像（矩阵中的对角线元素），而剩余的个文本-图像对为负样本，那么CLIP的训练目标就是最大个正样本的相似度，同时最小化个负样本的相似度，对应的伪代码实现如下所示：\",\"# image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter # 分别提取图像特征和文本特征 I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # 对两个特征进行线性投射，得到相同维度的特征，并进行l2归一化 I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # 计算缩放的余弦相似度：[n, n] logits = np.dot(I_e, T_e.T) * np.exp(t) # 对称的对比学习损失：等价于N个类别的cross_entropy_loss labels = np.arange(n) # 对角线元素的labels loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2\",\"为了训练CLIP模型，OpenAI从网络上收集了总计4亿对文本和图像，这些数据在论文中被称为WebImageText。若以文本单词数量来衡量，其规模与GPT-2训练时使用的WebText数据集相似。然而，从数据对的数量来看，它比谷歌的JFT-300M数据集还要多出1亿对，因此这是一个非常庞大的数据集。\",\"尽管CLIP是一个多模态模型，但其主要目的是训练可迁移的视觉模型。在论文中，文本编码器（Text Encoder）选择了一个包含6300万参数的Transformer模型，而图像编码器（Image Encoder）则采用了两种不同的架构：\",\"一种是常用的CNN架构ResNet。\",\"另一种是基于 Transformer 的ViT。\",\"ResNet包含五种不同尺寸的模型：ResNet50、ResNet101、RN50x4、RN50x16和RNx64（后三种模型是按照EfficientNet的缩放规则对ResNet分别放大4倍、16倍和64倍得到的），而ViT则选择了三种不同尺寸的模型：ViT-B/32、ViT-B/16和ViT-L/14。\",\"所有模型均训练了32个周期，使用AdamW优化器，并且在训练过程中采用了一个相对较大的批次大小：32768。由于数据量巨大，最大的ResNet模型RN50x64需要在592个V100 GPU上训练18天，而最大的ViT模型ViT-L/14则需要在256个V100 GPU上训练12天，这表明训练CLIP模型需要消耗大量的资源。对于ViT-L/14模型，还在336的分辨率下额外进行了一个周期的微调（finetune）以增强性能，论文发现这个模型的效果最佳，并将其标记为ViT-L/14@336，论文中进行对比实验的CLIP模型也采用了这一配置。\"]},\"408\":{\"h\":\"推理\",\"t\":[\"我们已经探讨了CLIP模型的运作机制，它由两个部分组成：一个视觉模型和一个文本模型。那么，如何将这个预训练的视觉模型应用到新的任务中呢？CLIP模型的一个显著优势是它能够进行zero-shot图像分类，这意味着它能够在没有任何特定任务训练数据的情况下，直接对图像进行分类。这不仅展示了CLIP的强大功能，也是其一大亮点。实现zero-shot分类的过程相当直接，可以概括为以下两个主要步骤：\",\"构建描述文本并提取特征：首先，根据任务的分类需求，为每个类别创建一个描述性的文本，例如“A photo of {label}”。这些文本随后被输入到文本编码器（Text Encoder）中，以生成相应的文本特征。如果有个类别，那么就会得到个文本特征。\",\"图像特征提取与分类：接下来，将待分类的图像输入到图像编码器（Image Encoder）中，以获取图像特征。然后，这些图像特征会与之前得到的个文本特征进行余弦相似度计算（这一过程与训练时相同）。最终，选择与图像特征相似度最高的文本所对应的类别，作为图像的分类预测结果。此外，这些相似度值可以被视为logits，通过softmax函数转换后，可以得到每个类别的预测概率。\",\"通过这种方式，CLIP模型能够在没有特定任务训练数据的情况下，直接对图像进行分类，这展示了其在图像分类任务中的灵活性和强大能力。\",\" 显然，我们通过利用CLIP模型的多模态能力，为特定任务动态构建了一个分类器。在这个过程中，文本编码器（Text Encoder）生成的文本特征相当于分类器的权重，而图像编码器（Image Encoder）提取的图像特征则是分类器的输入数据。以下是一个官方给出的CLIP模型的示例 ，该示例中的任务涉及8个类别:\",\"我们首先创建了各类别的文本描述，然后提取了相应的文本特征；\",\"然后我们读取要预测的图像，输入Image Encoder提取图像特征，并计算与文本特征的余弦相似度。\",\"# 1. 提取文本特征 texts = [ \\\"a page of text about segmentation\\\", \\\"a facial photo of a tabby cat\\\", \\\"a portrait of an astronaut with the American flag\\\", \\\"a rocket standing on a launchpad\\\", \\\"a red motorcycle standing in a garage\\\", \\\"a person looking at a camera on a tripod\\\", \\\"a black-and-white silhouette of a horse\\\", \\\"a cup of coffee on a saucer\\\" ] text_tokens = clip.tokenize([\\\"This is \\\" + desc for desc in texts]).cuda() with torch.no_grad(): text_features = model.encode_text(text_tokens).float() # 2. 提取图像特征 image_input = torch.tensor(np.stack(images)).cuda() with torch.no_grad(): image_features = model.encode_image(image_input).float() # 3. 计算余弦相似度 image_features /= image_features.norm(dim=-1, keepdim=True) text_features /= text_features.norm(dim=-1, keepdim=True) similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\",\"相似度如下所示，可以看到对于要预测的8个图像，按照最大相似度，其均能匹配到正确的文本标签：\",\"进一步地，我们也可以对得到的余弦相似度计算softmax，得到每个预测类别的概率值：\",\"text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1) top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)\",\"得到的预测概率如下所示，可以看到8个图像，CLIP模型均能够以较高的置信度给出正确的分类结果：\"]},\"409\":{\"h\":\"文本描述生成\",\"t\":[\"在使用CLIP模型进行zero-shot分类时，除了模型本身的应用，文本描述的生成也是一个关键环节。在之前的例子中，我们使用了“A photo of {label}”这样的格式来生成文本描述，但实际上，我们还有其他的选择。例如，我们可以直接使用类别标签作为文本描述。这种方法实际上与NLP领域的一个研究方向——prompt learning或prompt engineering——紧密相关。关于这一领域的详细综述，可以参考论文《Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing》。\",\"简单来说，prompt learning的核心思想是通过设计合适的prompt（提示），使得预训练模型能够直接应用于下游任务。这与传统的预训练加微调的方法有所不同。论文指出，如果我们直接使用类别标签作为文本描述，由于这些文本往往只是一个单词，缺乏具体的上下文，并且与CLIP模型的训练数据不完全一致，因此在效果上可能不如使用“A photo of {label}”这种格式（在ImageNet数据集上可以提升1.3%的效果）。\",\"此外，论文还实验了使用80个不同的prompt进行集成，结果发现在ImageNet数据集上能够带来3.5%的性能提升。具体的实验结果可以参考CLIP公开的notebook。\"]},\"410\":{\"h\":\"花卉图片分类\",\"t\":[\"本节我们将基于CLIP预训练模型实现Zero-Shot推理，训练使用到的数据集和AlexNet保持一致，因此这里就不再给出数据集下载链接了。\",\"图片分类实战 – 分别基于LeNet，AlexNet，VGG进行实现\",\"# 预训练模型名称 model_name = \\\"openai/clip-vit-large-patch14\\\" # 定义当前目录 current_dir = os.getcwd() model_dir = os.path.join(current_dir, model_name.replace(\\\"/\\\", \\\"_\\\")) # 检查当前目录是否有预训练权重文件，如果没有则下载 def download_pretrained_weights_if_needed(model_name, save_dir): if not os.path.exists(save_dir): try: print(f\\\"Downloading {model_name} to {save_dir}...\\\") snapshot_download(repo_id=model_name, local_dir=save_dir, local_dir_use_symlinks=False) print(f\\\"{model_name} downloaded successfully.\\\") except Exception as e: print(f\\\"Error downloading {model_name}: {e}\\\") download_pretrained_weights_if_needed(model_name, model_dir) # 加载模型和处理器 model = CLIPModel.from_pretrained(model_dir) processor = CLIPProcessor.from_pretrained(model_dir) device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\") model = model.to(device)\",\"在 openai/clip-vit-large-patch14 这个 CLIP 预训练模型中，图像编码器采用了 Vision Transformer（ViT）架构，具体使用的是 ViT-L/14 版本，文本编码器使用的是基于 Transformer 的架构。\",\"# 函数：生成文本嵌入 def text_embedding(texts): inputs = processor(text=texts, return_tensors=\\\"pt\\\", padding=True).to(device) with torch.no_grad(): embeddings = model.get_text_features(**inputs) return embeddings.cpu().numpy()\",\"这个函数的作用是将输入的文本转化为对应的嵌入表示（embedding）。它通过处理器对输入文本进行处理，使其符合模型的输入要求，然后利用模型获取文本特征，最后将结果转换为 numpy 数组格式返回，方便后续的计算和比较。\",\"def get_image_embeddings(image_paths): images = [] for image_path in image_paths: try: image = Image.open(image_path).convert(\\\"RGB\\\") images.append(image) except Exception as e: print(f\\\"Error loading image {e}\\\") if not images: return None inputs = processor(images=images, return_tensors=\\\"pt\\\").to(device) with torch.no_grad(): image_features = model.get_image_features(**inputs) return image_features.cpu().numpy()\",\"该函数作用是针对给定的图片路径，读取图片并将其转换为合适的格式后，通过模型获取图片的特征嵌入。如果在读取图片过程中出现错误，会进行相应的错误提示并返回 None。\",\"def cosine_similarity(vec1, vec2): vec1 = np.array(vec1) vec2 = np.array(vec2) return np.dot(vec1, vec2.T) / (np.linalg.norm(vec1, axis=1, keepdims=True) * np.linalg.norm(vec2, axis=1))\",\"在图文检索中，我们常常需要衡量文本嵌入和图片嵌入之间的相似度，这里采用了余弦相似度的计算方法。它将输入的向量转换为 numpy 数组后，按照余弦相似度的数学公式来计算两者的相似度数值。\",\"首先，我们需要根据上面给出的花卉数据集下载链接，将数据下载到当前项目目录下:\",\"其次，我们从flower_photos目录下读取出所有图片的路径:\",\"# 递归遍历目录获取所有图片路径 def get_all_image_paths(directory): image_paths = [] for root, _, files in os.walk(directory): for file in files: file_extension = os.path.splitext(file)[1].lower() if file_extension in ['.png', '.jpg', '.jpeg']: image_paths.append(os.path.join(root, file)) return image_paths image_paths = get_all_image_paths(\\\"./flower_photos\\\")\",\"同时将flower_photos下的子目录名作为我们的候选待匹配分类文本列表，并改造为a photo of 子目录名的格式，然后计算每个分类文本对应的文本嵌入向量:\",\"# 获取候选分类名列表 def get_candidates(directory): candidates = [] for sub_dir in os.listdir(directory): sub_dir_path = os.path.join(directory, sub_dir) if os.path.isdir(sub_dir_path): candidates.append(f\\\"a photo of {sub_dir}\\\") return candidates candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates)\",\"最后:\",\"分批次从图像列表中取出一批图像，获取其对应的图像嵌入向量列表\",\"计算这批图像嵌入向量中每一个图像嵌入向量和我们所有分类文本嵌入向量的余弦相似度\",\"针对每个图像嵌入向量取出和其相似度最高的那个文本嵌入向量的索引\",\"判断预测是否正确，统计正确率\",\"# 测试图片分类正确率 def accuracy(image_paths, candidates, text_embeddings, batch_size=64): correct_count = 0 total_count = len(image_paths) num_batches = (total_count + batch_size - 1) // batch_size # 分批次预测 for i in range(num_batches): start_idx = i * batch_size end_idx = min(start_idx + batch_size, total_count) # 取出当前批次的图像列表，并获得该批次图像列表对应的图像嵌入向量列表 batch_image_paths = image_paths[start_idx:end_idx] image_embeddings = get_image_embeddings(batch_image_paths) if image_embeddings is not None: # 计算图像嵌入向量列表中每个图像对应的嵌入向量和传入的分类文本嵌入向量的余弦相似度 similarities = cosine_similarity(image_embeddings, text_embeddings) # 针对每个图像嵌入向量获取和其相似度最高的分类文本嵌入向量索引下标 predicted_indices = np.argmax(similarities, axis=1) for j, predicted_index in enumerate(predicted_indices): # 针对每张图像，根据上述计算得到的和其相似度最高的分类文本索引，从候选分类文本集合中取出其分类名词 predicted_category = candidates[predicted_index].split(\\\" \\\")[-1] # 用当前图片外层目录的名字作为其分类名词 actual_category = os.path.basename(os.path.dirname(batch_image_paths[j])) # 比较两个分类名词是否相等 if predicted_category == actual_category: correct_count += 1 accuracy = correct_count / total_count return accuracy # 图片分类 def flowerClassify(): image_paths = get_all_image_paths(\\\"./flower_photos\\\") candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates) start_time = time.time() acc = accuracy(image_paths, candidates, text_embeddings, batch_size=64) end_time = time.time() print(f\\\"Time taken to test accuracy: {end_time - start_time:.2f} seconds\\\") print(f\\\"Accuracy: {acc * 100:.2f}%\\\")\",\"Time taken to test accuracy: 396.62 seconds Accuracy: 95.48%\"]},\"411\":{\"h\":\"文字搜索图像\",\"t\":[\"上面的花卉分类案例相当于使用图像去匹配最合适的文本描述，而这里我们将会反转这个逻辑，用文本描述去匹配最合适的图片内容。\",\"为了实现文字搜索图像的功能，我们只需要在计算出相似度得分矩阵后，以每个文本描述为一行，取出该行中得分最大的那一列，即为与当前文本描述相似度最高的那副图片，具体代码实现如下：\",\"# 找到与文本最匹配的图片 def find_most_matching_image(text, image_paths): text_emb = text_embedding([text]) image_embeddings = get_image_embeddings(image_paths) if image_embeddings is None: return None similarities = cosine_similarity(text_emb, image_embeddings) most_matching_index = np.argmax(similarities) return image_paths[most_matching_index]\",\"下面来实际展示一下效果，首先我们用data目录充当我们的图片库来源:\",\" 遍历data目录，拿到所有图片路径:\",\"# 遍历 data 目录获取所有图片路径 def get_images_from_data_dir(): data_dir = os.path.join(current_dir, 'data') if not os.path.exists(data_dir): print(f\\\"Data directory {data_dir} does not exist.\\\") return [] return get_all_image_paths(data_dir)\",\"这里以搜索向日葵花为例，我们首先获取图片库中所有图片，然后计算出和当前文本描述相似度最高的那副图片，并将图片展示出来:\",\"# 根据文字搜索图片 def searchPicByText(): image_paths = get_images_from_data_dir() query_text = \\\"a photo of a sunflowers\\\" most_matching_image = find_most_matching_image(query_text, image_paths) if most_matching_image: print(f\\\"The most matching image for '{query_text}' is: {most_matching_image}\\\") try: img = Image.open(most_matching_image) plt.imshow(img) plt.axis('off') plt.title(f\\\"Most matching image for '{query_text}'\\\") plt.show() except Exception as e: print(f\\\"Error opening image: {e}\\\") else: print(\\\"No matching image found.\\\")\",\"图片库中的图片： 运行上述代码，搜索出来的图片:\"]},\"412\":{\"h\":\"完整代码\",\"t\":[\"import time from matplotlib import pyplot as plt from transformers import CLIPProcessor, CLIPModel import torch from PIL import Image import numpy as np import warnings import os from huggingface_hub import snapshot_download warnings.filterwarnings(\\\"ignore\\\") # 模型名称 model_name = \\\"openai/clip-vit-large-patch14\\\" # 定义当前目录 current_dir = os.getcwd() model_dir = os.path.join(current_dir, model_name.replace(\\\"/\\\", \\\"_\\\")) # 检查当前目录是否有预训练权重文件，如果没有则下载 def download_pretrained_weights_if_needed(model_name, save_dir): if not os.path.exists(save_dir): try: print(f\\\"Downloading {model_name} to {save_dir}...\\\") snapshot_download(repo_id=model_name, local_dir=save_dir, local_dir_use_symlinks=False) print(f\\\"{model_name} downloaded successfully.\\\") except Exception as e: print(f\\\"Error downloading {model_name}: {e}\\\") download_pretrained_weights_if_needed(model_name, model_dir) # 加载模型和处理器 model = CLIPModel.from_pretrained(model_dir) processor = CLIPProcessor.from_pretrained(model_dir) device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\") model = model.to(device) # 函数：生成文本嵌入 def text_embedding(texts): inputs = processor(text=texts, return_tensors=\\\"pt\\\", padding=True).to(device) with torch.no_grad(): embeddings = model.get_text_features(**inputs) return embeddings.cpu().numpy() def get_image_embeddings(image_paths): images = [] for image_path in image_paths: try: image = Image.open(image_path).convert(\\\"RGB\\\") images.append(image) except Exception as e: print(f\\\"Error loading image {e}\\\") if not images: return None inputs = processor(images=images, return_tensors=\\\"pt\\\").to(device) with torch.no_grad(): image_features = model.get_image_features(**inputs) return image_features.cpu().numpy() def cosine_similarity(vec1, vec2): vec1 = np.array(vec1) vec2 = np.array(vec2) return np.dot(vec1, vec2.T) / (np.linalg.norm(vec1, axis=1, keepdims=True) * np.linalg.norm(vec2, axis=1)) # 递归遍历目录获取所有图片路径 def get_all_image_paths(directory): image_paths = [] for root, _, files in os.walk(directory): for file in files: file_extension = os.path.splitext(file)[1].lower() if file_extension in ['.png', '.jpg', '.jpeg']: image_paths.append(os.path.join(root, file)) return image_paths # 获取候选分类名列表 def get_candidates(directory): candidates = [] for sub_dir in os.listdir(directory): sub_dir_path = os.path.join(directory, sub_dir) if os.path.isdir(sub_dir_path): candidates.append(f\\\"a photo of {sub_dir}\\\") return candidates # 测试图片分类正确率 def accuracy(image_paths, candidates, text_embeddings, batch_size=64): correct_count = 0 total_count = len(image_paths) num_batches = (total_count + batch_size - 1) // batch_size for i in range(num_batches): start_idx = i * batch_size end_idx = min(start_idx + batch_size, total_count) batch_image_paths = image_paths[start_idx:end_idx] image_embeddings = get_image_embeddings(batch_image_paths) if image_embeddings is not None: similarities = cosine_similarity(image_embeddings, text_embeddings) predicted_indices = np.argmax(similarities, axis=1) for j, predicted_index in enumerate(predicted_indices): predicted_category = candidates[predicted_index].split(\\\" \\\")[-1] actual_category = os.path.basename(os.path.dirname(batch_image_paths[j])) if predicted_category == actual_category: correct_count += 1 accuracy = correct_count / total_count return accuracy # 图片分类 def flowerClassify(): image_paths = get_all_image_paths(\\\"./flower_photos\\\") candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates) start_time = time.time() acc = accuracy(image_paths, candidates, text_embeddings, batch_size=64) end_time = time.time() print(f\\\"Time taken to test accuracy: {end_time - start_time:.2f} seconds\\\") print(f\\\"Accuracy: {acc * 100:.2f}%\\\") ##################################################################################################3 # 遍历 data 目录获取所有图片路径 def get_images_from_data_dir(): data_dir = os.path.join(current_dir, 'data') if not os.path.exists(data_dir): print(f\\\"Data directory {data_dir} does not exist.\\\") return [] return get_all_image_paths(data_dir) # 找到与文本最匹配的图片 def find_most_matching_image(text, image_paths): text_emb = text_embedding([text]) image_embeddings = get_image_embeddings(image_paths) if image_embeddings is None: return None similarities = cosine_similarity(text_emb, image_embeddings) most_matching_index = np.argmax(similarities) return image_paths[most_matching_index] # 根据文字搜索图片 def searchPicByText(): image_paths = get_images_from_data_dir() query_text = \\\"a photo of a sunflowers\\\" most_matching_image = find_most_matching_image(query_text, image_paths) if most_matching_image: print(f\\\"The most matching image for '{query_text}' is: {most_matching_image}\\\") try: img = Image.open(most_matching_image) plt.imshow(img) plt.axis('off') plt.title(f\\\"Most matching image for '{query_text}'\\\") plt.show() except Exception as e: print(f\\\"Error opening image: {e}\\\") else: print(\\\"No matching image found.\\\")\"]},\"413\":{\"h\":\"小结\",\"t\":[\"在计算机视觉领域，常见的迁移学习方法是首先在大规模数据集（如ImageNet）上进行预训练，然后在具体的下游任务上进行微调。这种预训练通常是基于有监督学习的，需要大量的数据标注，因此成本较高。近年来，出现了一些基于自监督的方法，包括基于对比学习的方法（如MoCo和SimCLR）和基于图像掩码的方法（如MAE和BeiT）。自监督方法的优势在于不再需要标注数据。然而，无论是有监督还是自监督方法，在迁移到下游任务时，都需要进行有监督微调，无法实现zero-shot学习。对于有监督模型，由于它们在预训练数据集上采用固定类别数的分类器，因此在新的数据集上需要定义新的分类器来重新训练。对于自监督模型，代理任务通常是辅助进行表征学习，在迁移到其他数据集时也需要加上新的分类器进行有监督训练。\",\"然而，在NLP领域，基于自回归或语言掩码的预训练方法已经相对成熟，预训练模型很容易直接zero-shot迁移到下游任务，例如OpenAI的GPT-3。这种差异一方面是由于文本和图像属于两个完全不同的模态，另一个原因是NLP模型可以利用从互联网上收集的大量文本。因此，问题来了：能否基于互联网上的大量文本来预训练视觉模型？\",\"实际上，之前已经有一些研究工作探讨了使用文本作为监督信号来训练视觉模型。例如，2016年的工作《Learning Visual Features from Large Weakly Supervised Data》将这个问题转化为一个多标签分类任务，预测图像对应的文本的词袋模型；2017年的工作《Learning Visual N-Grams from Web Data》进一步扩展了这个方法来预测n-grams。最近的一些工作采用新的模型架构和预训练方法来从文本学习视觉特征，例如VirTex基于transformer的语言模型，ICMLM基于语言掩码的方法，ConVIRT基于对比学习的方法。总体来看，这方面的工作并不多，主要是因为这些方法难以实现较高的性能，例如2017年的那篇工作只在ImageNet上实现了11.5%的zero-shot性能，这远远低于ImageNet上的SOTA。此外，还有另一个方向，即基于文本弱监督来提升性能，例如谷歌的BiT和ViT基于JFT-300M数据集来预训练模型在ImageNet上取得SOTA。JFT-300M数据集是谷歌从互联网上收集的，通过一些自动化的手段将web text转化为18291个类别，但存在一定的噪声。尽管谷歌基于JFT-300M数据集取得了较好的结果，但这些模型仍然采用固定类别的softmax分类器进行预训练，这大大限制了它们的迁移能力和扩展性。\",\"谷歌采用的弱监督方法与以往方法的一个主要区别在于规模，或者说在于计算能力和数据集的规模。JFT-300M数据集的规模达到了上亿级别，谷歌利用强大的计算能力进行了预训练。相比之下，VirTex、ICMLM和ConVIRT仅在10万级别的数据上训练了几天。为了弥补数据规模上的差距，OpenAI从网络上收集了4亿条数据进行实验。然而，新的问题出现了：应该采用什么样的方法来进行训练。\",\"OpenAI首先尝试了VirTex模型，该模型联合训练一个CNN和文本transformer来预测图像的文本描述（image caption），但发现这种方法的训练效率（根据ImageNet数据集上的zero-shot性能评估）还不如直接预测词袋模型（bag of words），两者的训练效率相差3倍。如果进一步采用ConVIRT，即基于对比学习的方法，训练效率可以提高4倍。出现这种差异的原因不难理解，因为训练数据集中的文本-图像对是从互联网收集的，存在一定的噪声，即文本和图像可能不完全匹配。在这种情况下，适当降低训练目标反而可能取得更好的效果。\",\"从任务难度来看，排序为：Transformer Language Model > Bag of Words Prediction > Bag of Words Contrastive (CLIP)。由于训练数据量和模型计算量较大，训练效率成为一个至关重要的因素。因此，作者最终选择了对比学习方法来进行训练。\"]},\"414\":{\"h\":\"庖丁解牛BLIP2\",\"t\":[\"庖丁解牛BLIP2\",\"论文: https://arxiv.org/abs/2301.12597 代码: https://github.com/salesforce/LAVIS/tree/main/projects/blip2\"]},\"415\":{\"h\":\"背景\",\"t\":[\"多模态模型在过往发展的过程中，曾有一段时期一直在追求更大的网络架构（image encoder 和 text encoder/decoder）和 数据集，从而导致更大的训练代价。例如CLIP，400M数据，需要数百个GPU训练数十天，如何降低模型训练成本，同时具有很好的性能？\",\"这就是BLIP-2的起因，回顾下之前的多模态网络设计，三个模块（图像分支、文本分支、融合模块）:\",\"多模态网络设计\",\"(a) 早期的图文多模态：图像分支依赖目标检测器，模态融合比较弱，如VSE++。\",\"(b) 重点训练图像和文本特征提取，模态融合比较轻量，如CLIP。\",\"(c) 图像特征提取和模态融合都很重。\",\"(d) 侧重模态融合，特征提取网络相对轻量，如ViLT。\",\"模块\",\"(a)\",\"(b)\",\"(c)\",\"(d)\",\"理想情况\",\"视觉分支\",\"重\",\"重\",\"重\",\"轻\",\"重\",\"文本分支\",\"轻\",\"重\",\"轻\",\"轻\",\"重\",\"融合模块\",\"轻\",\"轻\",\"重\",\"重\",\"轻\",\"性能\",\"一般\",\"好\",\"好\",\"一般\",\"好\",\"训练代价\",\"中\",\"非常高\",\"非常高\",\"高\",\"中\",\"BLIP-2 基于 BLIP 架构，利用已有的ViT 和 LLM（均冻结）+ 一个的轻量Q-Former模块做模态融合，大幅降低训练成本。具有很强的zero-shot image-to-text generation能力，同时因LLM而具有了视觉推理能力。\"]},\"416\":{\"h\":\"模型结构\",\"t\":[\"BLIP-2 框架按照 Two-Stage 策略预训练轻量级查询 Transformer 以弥合模态差距。\",\"Stage 1: 不同模态数据的提取与融合。\",\"Stage 2: 把数据转换成LLM能识别的格式。\",\"Two-Stage流程\",\"从冻结的Image Encoder引到Vision-Language表征学习。\",\"从冻结的LLM引到Vision-Language生成学习，实现Zero Shot图文生成。\"]},\"417\":{\"h\":\"Stage 1: Representation Learning （表征学习）\",\"t\":[\"tage 1: Representation Learning （表征学习）\",\"Q-Former 由两个transformer模块组成，输入包含三部分：\",\"冻结参数的Image Encoder提取的图像embeddings\",\"Learned Queries\",\"Queries是一组可学习的embeddings，是第一个transformer模块的input，可认为是模型参数一部分\",\"推理时，Queries被用来从image encoder输出的embeddings里提取与input text最相关的视觉信息\",\"Input Text\",\"Stage 1 使用 图像-文本对 进行预训练，目标是训练好 Q-Former，以便 Queries 可以学习到如何更好地结合文本提取图片信息。\",\"对于Q-Former，一种比较好理解的方式：把Q-Former类比为一个Self-attention模块\",\"Q：learned queries\",\"K：input text\",\"V：image embeddings from Image Encoder\",\"Blip2Qformer核心代码实现如下:\",\"利用 query tokens 从 image embeddings 中提取与 text 最相关的视觉信息\",\"将输入的 input text 进行编码 , 然后使用第一个CLS Token 作为 input text representation\",\"class Blip2Qformer(Blip2Base): ... def forward(self, samples): image = samples[\\\"image\\\"] # (B,C,H,W) text = samples[\\\"text_input\\\"] # (B,seq_len) # frozen vit 将图片编码成 (B, seq_len, hidden_size) image_embeds = self.ln_vision(self.visual_encoder(image)) # 构建padding mask标注哪些image token是有效的 (B,seq_len) image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to( image.device ) # 初始化query tokens (B,seq_len,hidden_size) query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1) # query tokens 从 image embeddings 中提取与 text 最相关的视觉信息 # query_output (B,seq_len,hidden_size) query_output = self.Qformer.bert( query_embeds=query_tokens, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, use_cache=True, return_dict=True, ) image_feats = F.normalize( self.vision_proj(query_output.last_hidden_state), dim=-1 ) # 将input text 进行编码，维度为 (B,seq_len,hidden_size) text_tokens = self.tokenizer( text, padding=\\\"max_length\\\", truncation=True, max_length=self.max_txt_len, return_tensors=\\\"pt\\\", ).to(image.device) text_output = self.Qformer.bert( text_tokens.input_ids, attention_mask=text_tokens.attention_mask, # padding mask return_dict=True, ) # 取第一个cls token作为input text representation，维度为 (B,hidden_size) text_feat = F.normalize( self.text_proj(text_output.last_hidden_state[:, 0, :]), dim=-1 ) ...\",\"以上代码注释中统一用B代替image_batch和text_batch，以及seq_len和hidden_size也是同样处理手段，大家注意区分。\",\"为了训练好Q-Former，第一阶段设计了三个训练目标，分别如下:\"]},\"418\":{\"h\":\"1、Image-Text Contrastive Learning (ITC Loss, CLIP-like)\",\"t\":[\"目的: Image representation 与 Text representation，以最大化互信息\",\"自注意力掩码策略: Uni-modal Self-attention Mask（单模态自注意力）\",\"Queries 和 Text 仅能和自己的 tokens 做 attention（Query和Query、Text和Text）\",\"Uni-modal Self-attention Mask\",\"image_feats 中每个 image_feat 与 text_feat 计算一个 similarity score ，选择最大值作为这个图文对的相似度 :\",\"similarity score\",\"如何计算loss的: “in-batch negatives”，该方法正是CLIP在VLP领域发扬光大的。以下引用CLIP论文图做说明：\",\"in-batch negatives\",\"###============== Image-text Contrastive ===================### # 计算每个query token 和 text_feat 的相似度 , 得到相似度矩阵 (B,B,seq_len) # image_feats (B,seq_len,hidden_size) 变为 (B,1,seq_len,hidden_size) # text_feat (B,hidden_size) 变为 (B,hidden_size,1) sim_q2t = torch.matmul( image_feats.unsqueeze(1), text_feat.unsqueeze(-1) ).squeeze() # image-text similarity: aggregate across all query tokens # 保留和text_feat相似度最大的那个query token作为最后的相似度得分 , 维度为 (B,B) sim_i2t, _ = sim_q2t.max(-1) sim_i2t = sim_i2t / self.temp # 反过来计算text_feat 和 每个query token的相似度 , 得到相似度矩阵 (B,B,seq_len) # image_feats 维度变为 (B,hidden_size,seq_len) # text_feat (B,hidden_size) 变为 (B,1,1,hidden_size) sim_t2q = torch.matmul( text_feat.unsqueeze(1).unsqueeze(1), image_feats.permute(0, 2, 1) ).squeeze() # text-image similarity: aggregate across all query tokens # 保留和text_feat相似度最大的那个query token作为最后的相似度得分 , 维度为 (B,B) sim_t2i, _ = sim_t2q.max(-1) sim_t2i = sim_t2i / self.temp # 生成比标签 targets = torch.arange(image.size(0), device=image.device) # 计算 图文对比 Loss loss_itc = ( # sim_i2t 形状是 (B, B)，每一行表示一张图像和所有文本之间的相似度。 F.cross_entropy(sim_i2t, targets, label_smoothing=0.1) + F.cross_entropy(sim_t2i, targets, label_smoothing=0.1) ) / 2\"]},\"419\":{\"h\":\"2、Image-Text Matching (ITM Loss，二分类task)\",\"t\":[\"目的：通过学习image-text pair是否match，以细粒度对齐 Image representation 与 Text representation\",\"自注意力掩码策略: Bi-directional Self-attention Mask（双向自注意力）\",\"Queries 和Text都能和所有的tokens 做attention\",\"Bi-directional Self-attention Mask\",\"每个output query embedding送到二分类器中，得到一个logit；所有logits的平均作为最终的matching score:\",\"matching score\",\" ###============== Image-text Matching ===================### text_input_ids_world = text_tokens.input_ids text_attention_mask_world = text_tokens.attention_mask image_embeds_world = image_embeds with torch.no_grad(): # bs (batch size) ， diag_indices = [0,1,2,...,bs-1] diag_indices = torch.arange(bs, device=sim_t2i.device) # 把相似度矩阵对角线元素置为负无穷大，以避免模型将匹配图文对挑选为负样本 # (0,0) , (1,1) ... (bs-1,bs-1) 位置处设置为 -10000 sim_t2i[diag_indices, diag_indices] = -10000 sim_i2t[diag_indices, diag_indices] = -10000 weights_t2i = F.softmax(sim_t2i, dim=1) weights_i2t = F.softmax(sim_i2t, dim=1) # 为每个文本选择一个负样本图像 image_embeds_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_t2i[b], 1).item() image_embeds_neg.append(image_embeds_world[neg_idx]) image_embeds_neg = torch.stack(image_embeds_neg, dim=0) # 为每个图像选择一个负样本文本 text_ids_neg = [] text_atts_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_i2t[b], 1).item() text_ids_neg.append(text_input_ids_world[neg_idx]) text_atts_neg.append(text_attention_mask_world[neg_idx]) text_ids_neg = torch.stack(text_ids_neg, dim=0) text_atts_neg = torch.stack(text_atts_neg, dim=0) # 构建输入文本列表: [正样本batch，负样本batch1，负样本batch2] ，维度为 (3*bs,seq_len) text_ids_all = torch.cat( [text_tokens.input_ids, text_tokens.input_ids, text_ids_neg], dim=0 ) text_atts_all = torch.cat( [text_tokens.attention_mask, text_tokens.attention_mask, text_atts_neg], dim=0, ) # 构建query tokens列表: [正样本batch，负样本batch1，负样本batch2] ，维度为 (3*bs,seq_len,hidden_size) query_tokens_itm = self.query_tokens.expand(text_ids_all.shape[0], -1, -1) query_atts_itm = torch.ones(query_tokens_itm.size()[:-1], dtype=torch.long).to( image.device ) # 构建query和text的padding mask ，维度为 (3*bs,seq_len) attention_mask_all = torch.cat([query_atts_itm, text_atts_all], dim=1) # 构建输入图像列表: [正样本batch，负样本batch1，负样本batch2] ，维度为 (3*bs,seq_len,hidden_size) image_embeds_all = torch.cat( [image_embeds, image_embeds_neg, image_embeds], dim=0 ) image_atts_all = torch.ones(image_embeds_all.size()[:-1], dtype=torch.long).to( image.device ) # 1. 将输入文本转换为嵌入列表后和query tokens 在seq_len维度上拼接起来，维度为 (3*bs,text_seq_len + query_tokens_seq_len,hidden_size) # 2. 将文本和query tokens拼接得到的结果和图像嵌入进行cross attention计算，编码后得到输出的结果 output_itm = self.Qformer.bert( text_ids_all, query_embeds=query_tokens_itm, attention_mask=attention_mask_all, encoder_hidden_states=image_embeds_all, encoder_attention_mask=image_atts_all, return_dict=True, ) # 取 (3*bs,text_seq_len + query_tokens_seq_len,hidden_size) 中 query tokens部分的结果，维度为 (3*bs,query_tokens_seq_len,hidden_size) vl_embeddings = output_itm.last_hidden_state[:, : query_tokens_itm.size(1), :] # 把query tokens部分的每个位置都映射到2维匹配空间，维度为 (3*bs,query_tokens_seq_len,2) vl_output = self.itm_head(vl_embeddings) # 取每个位置的平均作为最终的匹配得分，维度为 (3*bs,2) logits = vl_output.mean(dim=1) # 构建匹配标签: [正样本batch=1，负样本batch1=0，负样本batch2=0] ，维度为 (3*bs) itm_labels = torch.cat( [torch.ones(bs, dtype=torch.long), torch.zeros(2 * bs, dtype=torch.long)], dim=0, ).to(image.device) # 计算交叉熵损失 loss_itm = F.cross_entropy(logits, itm_labels)\",\"当文本和query tokens同时输入BertModel时，BertEmbeddings会将text embeddings和query tokens的embeddings在seq_len维度上拼接起来。\",\"class BertEmbeddings(nn.Module): ... def forward( self, input_ids=None, position_ids=None, query_embeds=None, past_key_values_length=0, ): # 计算序列长度 if input_ids is not None: seq_length = input_ids.size()[1] else: seq_length = 0 # 如果未提供位置id，则自动生成 if position_ids is None: position_ids = self.position_ids[ :, past_key_values_length : seq_length + past_key_values_length ].clone() # 词嵌入与位置嵌入相加，若有query_embeds则拼接 if input_ids is not None: embeddings = self.word_embeddings(input_ids) if self.position_embedding_type == \\\"absolute\\\": position_embeddings = self.position_embeddings(position_ids) embeddings = embeddings + position_embeddings if query_embeds is not None: embeddings = torch.cat((query_embeds, embeddings), dim=1) else: embeddings = query_embeds embeddings = self.LayerNorm(embeddings) embeddings = self.dropout(embeddings) return embeddings\",\"下图展示了 Image-Text Matching 的完整计算流程，关于BertModel的代码解析部分，将会在下文进行详细讲解:\",\"Image-Text Matching\"]},\"420\":{\"h\":\"3、Image-Grounded Text Generation (ITG Loss, GPT-like)\",\"t\":[\"目的：让Q-Former学习“图生文”的能力，即给定Input Image，生成Text\",\"自注意力掩码策略：Multimodal Causal Self-attention Mask（多模态因果自监督）\",\"Queies 可以和所有自己的tokens做attention\",\"Text 可以和所有的query tokens 及 当前token之前的text tokens做attention\",\"Multimodal Causal Self-attention Mask\",\"视觉编码阶段:\",\"图像通过视觉编码器（如 ViT）编码为图像特征 image_embeds。Query tokens 通过 cross-attention 吸收图像特征，再通过 self-attention 生成压缩的视觉表示。缓存 query tokens 的 self-attention 的 past_key_values（而非 cross-attention 的 key/value）。\",\"QFormer 会使用 past_key_values 缓存和复用 EncoderLayer 中 self-attention 的 key/value :\",\"BertSelfAttention: 自注意力和交叉注意力流程统一化，每次计算后返回本次可能需要缓存的key & value\",\"class BertSelfAttention(nn.Module): ... def forward( self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False, ): # 判断是否为交叉注意力 is_cross_attention = encoder_hidden_states is not None # 交叉注意力则key和value都来自图像,key来自query tokens if is_cross_attention: key_layer = self.transpose_for_scores(self.key(encoder_hidden_states)) value_layer = self.transpose_for_scores(self.value(encoder_hidden_states)) attention_mask = encoder_attention_mask # 如果有缓存的key,value传入, 此时先用text embedding计算出key和value # 再和缓存的key,value在seq_len的维度拼接起来 elif past_key_value is not None: key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) key_layer = torch.cat([past_key_value[0], key_layer], dim=2) # (Batch,Heads,Seq_len,Hidden_size) value_layer = torch.cat([past_key_value[1], value_layer], dim=2) else: # 自注意力 key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) # 交叉注意力: 传入图像，则q来自query tokens # 自注意力: q来自query tokens 或者 text embedding mixed_query_layer = self.query(hidden_states) query_layer = self.transpose_for_scores(mixed_query_layer) # * 缓存key和value past_key_value = (key_layer, value_layer) # 计算注意力分数 attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) attention_scores = attention_scores / math.sqrt(self.attention_head_size) if attention_mask is not None: # 应用注意力掩码 attention_scores = attention_scores + attention_mask # softmax归一化得到注意力概率 attention_probs = nn.Softmax(dim=-1)(attention_scores) if is_cross_attention and self.save_attention: self.save_attention_map(attention_probs) attention_probs.register_hook(self.save_attn_gradients) # dropout防止过拟合 attention_probs_dropped = self.dropout(attention_probs) # 计算上下文表示 context_layer = torch.matmul(attention_probs_dropped, value_layer) context_layer = context_layer.permute(0, 2, 1, 3).contiguous() new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) context_layer = context_layer.view(*new_context_layer_shape) outputs = ( (context_layer, attention_probs) if output_attentions else (context_layer,) ) # outputs 列表最后一个记录了缓存的key和value outputs = outputs + (past_key_value,) return outputs\",\"BertLayer: 负责组织自注意力和交叉注意力的运算流程\",\"class BertLayer(nn.Module): ... def forward( self, hidden_states, # query tokens attention_mask=None, # query token padding mask head_mask=None, encoder_hidden_states=None, # image tokens encoder_attention_mask=None, # image padding mask past_key_value=None, output_attentions=False, query_length=0, ): self_attn_past_key_value = ( past_key_value[:2] if past_key_value is not None else None ) # 自注意力运算 self_attention_outputs = self.attention( hidden_states, attention_mask, head_mask, output_attentions=output_attentions, past_key_value=self_attn_past_key_value, # 缓存的key和value ) attention_output = self_attention_outputs[0] outputs = self_attention_outputs[1:-1] present_key_value = self_attention_outputs[-1] # 交叉注意力运算 if query_length > 0: query_attention_output = attention_output[:, :query_length, :] if self.has_cross_attention: cross_attention_outputs = self.crossattention( query_attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions=output_attentions, ) query_attention_output = cross_attention_outputs[0] outputs = ( outputs + cross_attention_outputs[1:-1] ) ... outputs = (layer_output,) + outputs outputs = outputs + (present_key_value,) # outputs 列表最后一个记录了缓存的key和value return outputs\",\"BertEncoder: 负责组织多个 BertLayer 叠加的运算流程\",\"class BertEncoder(nn.Module): ... def forward( self, hidden_states, # query tokens attention_mask=None, # query tokens padding mask head_mask=None, encoder_hidden_states=None, # images encoder_attention_mask=None, # images padding mask past_key_values=None, use_cache=None, output_attentions=False, output_hidden_states=False, return_dict=True, query_length=0, ): ... for i in range(self.config.num_hidden_layers): layer_module = self.layer[i] ... # 如果有缓存，则计算当前层BertLayer时，会从缓存中取出对应层先前缓存的key&value past_key_value = past_key_values[i] if past_key_values is not None else None layer_outputs = layer_module( hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, query_length, ) hidden_states = layer_outputs[0] # 每一层BertLayer产生的key&value都会进行缓存 if use_cache: next_decoder_cache += (layer_outputs[-1],) ... return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=next_decoder_cache, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, )\",\"Image-Grounded Text Generation 学习目标\",\" ... query_output = self.Qformer.bert( query_embeds=query_tokens, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, use_cache=True, # 缓存key&value return_dict=True, ) ... ##================= Image Captioning ========================## # 这一部分的目标是：根据图像特征，使用 Q-Former 解码器生成文本描述（caption） # Step 1: 准备 decoder 的输入 token IDs decoder_input_ids = text_tokens.input_ids.clone() # 将第一个 token 替换为 BOS（Begin Of Sentence）标记，表示“开始生成句子” decoder_input_ids[:, 0] = self.tokenizer.bos_token_id # Step 2: 构造训练目标 labels # 将 padding token 替换为 -100，这是 CrossEntropyLoss 默认忽略的标签值 labels = decoder_input_ids.masked_fill( decoder_input_ids == self.tokenizer.pad_token_id, -100 ) # Step 3: 构建 attention_mask（包含 query tokens 和 文本 token 的 mask） # query_atts 是 query tokens 的 attention mask，全为 1（因为都是有效 token） query_atts = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(image.device) # 将 query token 的 mask 和文本 token 的 mask 拼接在一起 attention_mask = torch.cat([query_atts, text_tokens.attention_mask], dim=1) # Step 4: 调用 Q-Former 解码器进行文本生成 lm_output = self.Qformer( decoder_input_ids, # 输入 token ID 序列（如 [BOS], dog, is...） attention_mask=attention_mask, # 指明哪些位置是有效的（非 padding） past_key_values=query_output.past_key_values, # 编码器输出的 key/value，包含图像信息 return_dict=True, # 返回字典格式结果 labels=labels, # 训练目标，用于计算 loss ) # Step 5: 提取语言模型损失 loss_lm = lm_output.loss # 使用交叉熵损失衡量生成与真实之间的差异\",\"文本生成阶段:\",\"将缓存的 past_key_values 作为文本解码器的初始状态。\",\"文本 token 在自回归生成时，通过 self-attention 复用缓存的视觉信息。\",\"BertLMHeadModel: 自回归语言建模任务（如文本生成）\",\"class BertLMHeadModel(BertPreTrainedModel): ... def forward( self, input_ids=None, attention_mask=None, position_ids=None, head_mask=None, query_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, past_key_values=None, use_cache=True, output_attentions=None, output_hidden_states=None, return_dict=None, return_logits=False, is_decoder=True, reduction=\\\"mean\\\", ): ... # 调用 BertModel 进行文本编码 (结合缓存的attention key&value) outputs = self.bert( input_ids, attention_mask=attention_mask, position_ids=position_ids, head_mask=head_mask, query_embeds=query_embeds, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, is_decoder=is_decoder, ) sequence_output = outputs[0] ... # self.cls 是一个分类头（BertOnlyMLMHead），它将每个 token 的向量映射到词汇表空间（logits） prediction_scores = self.cls(sequence_output) ... lm_loss = None if labels is not None: # 因为我们要预测下一个 token，所以把 logits 和 labels 错位对齐： # shifted_prediction_scores: 所有 token 的预测（除了最后一个） shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous() # labels: 所有 token 的真实值（从第二个开始） labels = labels[:, 1:].contiguous() loss_fct = CrossEntropyLoss(reduction=reduction, label_smoothing=0.1) lm_loss = loss_fct( shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1), ) if reduction == \\\"none\\\": lm_loss = lm_loss.view(prediction_scores.size(0), -1).sum(1) ... return CausalLMOutputWithCrossAttentions( loss=lm_loss, logits=prediction_scores, past_key_values=outputs.past_key_values, hidden_states=outputs.hidden_states, attentions=outputs.attentions, cross_attentions=outputs.cross_attentions, )\",\"BertModel 的 forward 方法中，当is_decoder=True时，会在get_extended_attention_mask方法中，构建一个下三角矩阵作为因果掩码矩阵。\"]},\"421\":{\"h\":\"Stage 2: Generative Learning（生成学习）\",\"t\":[\"Stage 2 是为了把 Q-Former 和冻结参数的 LLM 连接起来，以利用 LLM 的文本生成能力。\",\"支持两种LLM（decoder only、encoder-decoder based）:\",\"Generative Learning\",\"首先输入图片，直接输入冻结参数的 Image Encoder，得到图像的表征。\",\"然后图像的表征和 Queries 一起送入 Q-Former，得到 Queries 的输出 ，使用全连接 (FC) 层将 线性投影到与 LLM 的text embedding相同维度。\",\"后将投影后的 添加到 input text embeddings前面，Queries 的输出蕴含了视觉信息，送入LLM时，充当了soft visual prompts 。\",\"由于 Q-Former 已经过预训练以提取语言信息视觉表示，因此它有效地充当信息瓶颈，将最有用的信息提供给 LLM，同时删除不相关的视觉信息。这减少了LLM学习视觉语言对齐的负担，从而缓解了灾难性的遗忘问题。\",\"Blip2Qformer 的generate方法负责完成图像描述生成（图文到文本）:\",\"class Blip2Qformer(Blip2Base): ... def generate( self, samples, # 输入样本，包含图像和可选文本 use_nucleus_sampling=False, # 是否使用核采样（top-p采样） num_beams=3, # beam search的beam数量 max_length=30, # 生成文本的最大长度 min_length=10, # 生成文本的最小长度 top_p=0.9, # 核采样的概率阈值 repetition_penalty=1.0, # 重复惩罚系数 ): # 1. 图像编码阶段 image = samples[\\\"image\\\"] # 通过视觉编码器（如ViT）提取图像特征 (B, 257, D) image_embeds = self.ln_vision(self.visual_encoder(image)) # 2. 处理beam search扩展 if not use_nucleus_sampling: # 如果是beam search，需要复制图像特征以匹配beam数量 # (B, 257, D) -> (B*num_beams, 257, D) image_embeds = image_embeds.repeat_interleave(num_beams, dim=0) else: # 核采样时不扩展beam num_beams = 1 # 创建图像注意力掩码（全1，表示所有图像token有效） image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to( image.device ) # 3. 准备生成参数 model_kwargs = { \\\"encoder_hidden_states\\\": image_embeds, # 图像特征作为cross-attention的输入 \\\"encoder_attention_mask\\\": image_atts, # 图像注意力掩码 } # 4. 初始化文本输入（以BOS token开头） # 形状: (batch_size, 1)，初始为[BOS] input_ids = ( torch.LongTensor(image.size(0), 1) .fill_(self.tokenizer.bos_token_id) .to(image.device) ) # 5. 扩展可学习的query tokens # query_tokens形状: (batch_size, num_query_tokens, D) query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1) # 6. 调用Q-Former的生成方法 outputs = self.Qformer.generate( input_ids=input_ids, # 初始文本token [BOS] query_embeds=query_tokens, # 可学习query tokens max_length=max_length, # 最大生成长度 min_length=min_length, # 最小生成长度 num_beams=num_beams, # beam数量 do_sample=use_nucleus_sampling, # 是否采样 top_p=top_p, # 核采样参数 eos_token_id=self.tokenizer.sep_token_id, # 结束符 pad_token_id=self.tokenizer.pad_token_id, # 填充符 **model_kwargs # 图像特征和掩码 ) # 7. 解码生成的token id为文本 captions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True) return captions\"]},\"422\":{\"h\":\"庖丁解牛VIT\",\"t\":[\"多模态模型VIT原理与图片分类实战演练\",\"Vision Transformer是2021年谷歌在ICLR上提出的算法，它首次将NLP领域火热的Transformer模型架构移植到了CV领域，打破了这两个领域壁垒，并取得不错的成效。\",\"Vision Transformer的模型结构相比于Transformer来说更简单，在Transformer模型中，主要包含Encoder和Decoder结构，而ViT(Vision Transformer)仅借鉴了Encoder结构。\",\"ViT原论文中最核心的结论是: 当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果。\",\"归纳偏置:\",\"归纳偏置能够帮助学习算法缩小搜索范围，快速找到合适的模型。\",\"例如，在图像分类任务中，如果没有任何归纳偏置，学习算法需要在所有可能的函数空间中搜索最优模型，这几乎是不可能完成的任务。而通过引入特定的归纳偏置，如局部性和平移不变性（CNN 所具备的），可以将搜索范围限制在满足这些性质的模型子空间内，大大提高学习效率。\",\"但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些，因为Transformer和CNN相比缺少归纳偏置（inductive bias），即一种先验知识，提前做好的假设。CNN具有两种归纳偏置，一种是局部性（locality/two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征；一种是平移不变形（translation equivariance）。当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型。\"]},\"423\":{\"h\":\"原理\",\"t\":[\"本文将通过一个花卉分类的实战案例结合ViT原论文，来帮助大家梳理清楚Vision Transformer的核心流程实现。\"]},\"424\":{\"h\":\"0. 数据下载\",\"t\":[\"实验采用的是花蕊数据集，共5个类别，约4000多个样本。\",\"数据集下载：https://pan.baidu.com/s/137mO-7PY1jDq1Wp0NNyT3A?pwd=qvmq\",\"数据集加载代码:\",\"def read_split_data(root: str, val_rate: float = 0.2): random.seed(0) # 保证随机结果可复现 assert os.path.exists(root), \\\"dataset root: {} does not exist.\\\".format(root) # 遍历文件夹，一个文件夹对应一个类别 flower_class = [cla for cla in os.listdir(root) if os.path.isdir(os.path.join(root, cla))] # 排序，保证顺序一致 flower_class.sort() # 生成类别名称以及对应的数字索引 class_indices = dict((k, v) for v, k in enumerate(flower_class)) json_str = json.dumps(dict((val, key) for key, val in class_indices.items()), indent=4) with open('class_indices.json', 'w') as json_file: json_file.write(json_str) train_images_path = [] # 存储训练集的所有图片路径 train_images_label = [] # 存储训练集图片对应索引信息 val_images_path = [] # 存储验证集的所有图片路径 val_images_label = [] # 存储验证集图片对应索引信息 every_class_num = [] # 存储每个类别的样本总数 supported = [\\\".jpg\\\", \\\".JPG\\\", \\\".png\\\", \\\".PNG\\\"] # 支持的文件后缀类型 # 遍历每个文件夹下的文件 for cla in flower_class: cla_path = os.path.join(root, cla) # 遍历获取supported支持的所有文件路径 images = [os.path.join(root, cla, i) for i in os.listdir(cla_path) if os.path.splitext(i)[-1] in supported] # 获取该类别对应的索引 image_class = class_indices[cla] # 记录该类别的样本数量 every_class_num.append(len(images)) # 按比例随机采样验证样本 val_path = random.sample(images, k=int(len(images) * val_rate)) for img_path in images: if img_path in val_path: # 如果该路径在采样的验证集样本中则存入验证集 val_images_path.append(img_path) val_images_label.append(image_class) else: # 否则存入训练集 train_images_path.append(img_path) train_images_label.append(image_class) print(\\\"{} images were found in the dataset.\\\".format(sum(every_class_num))) print(\\\"{} images for training.\\\".format(len(train_images_path))) print(\\\"{} images for validation.\\\".format(len(val_images_path))) plot_image = True if plot_image: # 绘制每种类别个数柱状图 plt.bar(range(len(flower_class)), every_class_num, align='center') # 将横坐标0,1,2,3,4替换为相应的类别名称 plt.xticks(range(len(flower_class)), flower_class) # 在柱状图上添加数值标签 for i, v in enumerate(every_class_num): plt.text(x=i, y=v + 5, s=str(v), ha='center') # 设置x坐标 plt.xlabel('image class') # 设置y坐标 plt.ylabel('number of images') # 设置柱状图的标题 plt.title('flower class distribution') plt.show() return train_images_path, train_images_label, val_images_path, val_images_label\",\"自定义一个MyDataSet类来封装我们加载得到的数据集:\",\"from torch.utils.data import Dataset from PIL import Image import torch class MyDataSet(Dataset): \\\"\\\"\\\"自定义数据集\\\"\\\"\\\" def __init__(self, images_path: list, images_class: list, transform=None): \\\"\\\"\\\" 初始化自定义数据集类 :param images_path: 包含所有图像文件路径的列表 :param images_class: 包含所有图像对应类别的列表，与 images_path 中的图像一一对应 :param transform: 图像预处理的转换操作，默认为 None \\\"\\\"\\\" self.images_path = images_path self.images_class = images_class self.transform = transform def __len__(self): \\\"\\\"\\\" 返回数据集中图像的数量 :return: 数据集中图像的数量 \\\"\\\"\\\" return len(self.images_path) def __getitem__(self, item): \\\"\\\"\\\" 根据索引获取数据集中的图像和对应的标签 :param item: 图像的索引 :return: 经过预处理的图像和对应的标签 \\\"\\\"\\\" # 打开指定索引的图像文件 img = Image.open(self.images_path[item]) # RGB为彩色图片，L为灰度图片 # 检查图像是否为 RGB 模式，如果不是则抛出异常 if img.mode != 'RGB': raise ValueError(\\\"image: {} isn't RGB mode.\\\".format(self.images_path[item])) # 获取对应图像的标签 label = self.images_class[item] # 如果定义了图像预处理转换操作，则对图像进行处理 if self.transform is not None: img = self.transform(img) return img, label @staticmethod def collate_fn(batch): \\\"\\\"\\\" 自定义的批量数据处理函数，用于将一个批次的数据组合成一个张量 :param batch: 一个批次的数据，包含图像和对应的标签 :return: 组合后的图像张量和标签张量 \\\"\\\"\\\" # 官方实现的default_collate可以参考 # https://github.com/pytorch/pytorch/blob/67b7e751e6b5931a9f45274653f4f653a4e6cdf6/torch/utils/data/_utils/collate.py # 将一个批次的数据拆分为图像和标签两个元组 images, labels = tuple(zip(*batch)) # 将图像元组堆叠成一个四维张量，维度为 (batch_size, channels, height, width) images = torch.stack(images, dim=0) # 将标签元组转换为一个一维张量 labels = torch.as_tensor(labels) return images, labels\",\"两点注意:\",\"当使用 DataLoader 从数据集（Dataset）中加载数据时，它会将多个样本收集起来形成一个批次，但默认的组合方式可能不满足所有需求，这时就可以自定义 collate_fn 函数。\",\"@staticmethod 是 Python 中的一个装饰器，用于将一个方法定义为静态方法。静态方法是类中的一种特殊方法，它与类的实例和类本身都没有直接关联，可以直接通过类名调用，不需要创建类的实例。\"]},\"425\":{\"h\":\"1. 图片预处理\",\"t\":[\"预处理这个步骤在论文里并没有详细说明，但是对于ViT这个结构而言，输入的图片尺寸并不是自定义的，ViT-B/16为例，输入的图片尺寸必须为224x224。\",\"在 ViT - B/16 中，“B” 代表的是模型的基础（Base）版本 ，“16” 表示每个图像块的大小是 16x16 像素；ViT 通常在大规模数据集（如 ImageNet）上进行预训练，而预训练过程中使用的输入图像尺寸通常固定为 224x224。在预训练时，模型的参数是根据这个特定尺寸的输入数据进行优化和学习的。当我们在其他任务中使用预训练好的模型时，为了充分利用预训练的权重，也需要保持输入图像尺寸与预训练时一致，这样可以保证模型的特征提取能力和性能。\",\"因此，首先需要对输入图片进行尺寸变化，具体方式可以是直接缩放(Resize)，也可以进行随机裁剪(RandomResizedCrop)。\",\"对数据集和验证集划分之后，这里对训练集的处理方式是随机切成224x224像素的图片，然后进行水平翻转，再进行归一化和标准化处理；对验证集的处理方式是先Resize成256x256的图片，再从中心位置裁剪成224x224，再进行归一化和标准化处理。\",\"# 定义一个字典 data_transform，用于存储训练集和验证集的图像预处理转换操作 data_transform = { # 训练集的预处理转换操作 \\\"train\\\": transforms.Compose([ # 随机裁剪输入图像，将裁剪后的图像调整为 224x224 大小 # 这是一种数据增强的方式，通过随机裁剪可以增加训练数据的多样性，提高模型的泛化能力 transforms.RandomResizedCrop(224), # 以 0.5 的概率随机水平翻转图像 # 同样是数据增强的手段，增加了图像的多样性，有助于模型学习到不同方向的特征 transforms.RandomHorizontalFlip(), # 将 PIL 图像或 NumPy 数组转换为 PyTorch 的张量（Tensor） # 同时会将像素值从 [0, 255] 范围缩放到 [0, 1] 范围 transforms.ToTensor(), # 对图像进行归一化处理 # 第一个参数 [0.5, 0.5, 0.5] 是图像每个通道的均值，第二个参数 [0.5, 0.5, 0.5] 是图像每个通道的标准差 # 归一化有助于模型更快地收敛，提高训练的稳定性 transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) ]), # 验证集的预处理转换操作 \\\"val\\\": transforms.Compose([ # 将图像的短边缩放为 256 像素，长边按比例缩放 # 这一步是为了保证图像的整体比例不变，后续再进行裁剪操作 transforms.Resize(256), # 从图像的中心位置裁剪出 224x224 大小的图像 # 验证集不需要进行数据增强，只需要将图像调整到合适的大小 transforms.CenterCrop(224), # 将 PIL 图像或 NumPy 数组转换为 PyTorch 的张量（Tensor） # 同样会将像素值从 [0, 255] 范围缩放到 [0, 1] 范围 transforms.ToTensor(), # 对图像进行归一化处理，参数与训练集的归一化参数相同 # 保证训练集和验证集的数据处理方式一致 transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) ]) }\",\"下面我们将用于图片变换的transforms流水线和上面自定义的MyDataSet类都封装到DataLoader去。\",\"train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(data_path) # 实例化训练数据集 train_dataset = MyDataSet(images_path=train_images_path, images_class=train_images_label, transform=data_transform[\\\"train\\\"]) # 实例化验证数据集 val_dataset = MyDataSet(images_path=val_images_path, images_class=val_images_label, transform=data_transform[\\\"val\\\"]) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn) val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=val_dataset.collate_fn)\"]},\"426\":{\"h\":\"2. 图片切割\",\"t\":[\"Transformer需要输入的是一维的Token，对于二维的图像，一种朴素的想法就是把一个个像素点拉平，这样就成了一个一维序列。但是这样造成的一个后果是计算量太庞大，比如一张224x224的图片，变成1维度之后就成了50176，相当于直接输入一篇五万字的文章，模型难以计算。\",\"那么，一个改进的想法就是把一张图片分成nxn个Patch，每一个Patch作为一个Token，这样计算量就大大减小了。\",\"以ViT-B/16为例，将输入图片(224x224)按照16x16大小的Patch进行划分，划分后可以得到共个Patch。每个Patch是三通道的小图片，shape为(16, 16, 3)，将其展平就变成了一个长度为768的向量。\",\"每一个向量作为一个单独的输入，那样我们总共有196个向量，在代码中，可以变成一个[196,768]的矩阵，进行并行输入。\",\"这一步的操作在论文中是直接采用切割的处理办法，但是实际的代码实现中，采用了一种更巧妙的解决思路，就是利用一个卷积核大小为16x16，步距为16，卷积核个数为768的卷积层来进行实现。\",\"再来回顾我们的卷积层计算公式：\",\"输入为[224,244,3]，经过卷积层变成[14,14,768]，再映射为[196,768]。\",\"这样，就完成了从图片到Token之间的转换，我们通过自定义一个PatchEmbed类完成上述工作。\",\"class PatchEmbed(nn.Module): \\\"\\\"\\\" 2D Image to Patch Embedding 该类的作用是将二维图像分割成多个图像块（patch），并将这些图像块嵌入到一个低维向量空间中 \\\"\\\"\\\" def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None): \\\"\\\"\\\" 初始化 PatchEmbed 类 :param img_size: 输入图像的尺寸，默认为 224。如果传入一个整数，则表示图像是正方形，边长为该整数； :param patch_size: 每个图像块的尺寸，默认为 16。同样，如果传入一个整数，则表示图像块是正方形，边长为该整数； :param in_c: 输入图像的通道数，默认为 3（对应 RGB 图像） :param embed_dim: 嵌入维度，即每个图像块经过卷积操作后得到的特征向量的维度，默认为 768 :param norm_layer: 归一化层，默认为 None。如果传入一个归一化层类，则使用该层进行归一化；否则使用 nn.Identity() 表示不进行归一化 \\\"\\\"\\\" super().__init__() # 将 img_size 和 patch_size 转换为元组形式，如果传入的是整数，则将其转换为 (整数, 整数) 的形式 img_size = (img_size, img_size) patch_size = (patch_size, patch_size) self.img_size = img_size self.patch_size = patch_size # 计算网格大小，即图像在水平和垂直方向上分别可以划分的图像块数量 self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) # 计算图像块的总数，即网格大小的乘积 self.num_patches = self.grid_size[0] * self.grid_size[1] # 定义一个二维卷积层，用于将输入图像分割成多个图像块并进行嵌入 # in_c 是输入通道数，embed_dim 是输出通道数（也就是卷积核的数量） # kernel_size 是卷积核的大小，这里设置为图像块的大小 # stride 是卷积核的步长，这里设置为图像块的大小，确保卷积操作不会重叠 self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size) # 如果传入了归一化层类，则使用该层进行归一化；否则使用 nn.Identity() 表示不进行归一化 self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() def forward(self, x): \\\"\\\"\\\" 前向传播函数 :param x: 输入的图像张量，形状为 [B, C, H, W]，其中 B 是批量大小，C 是通道数，H 是图像高度，W 是图像宽度 :return: 经过处理后的图像块嵌入张量，形状为 [B, num_patches, embed_dim] \\\"\\\"\\\" # 获取输入图像张量的形状 B, C, H, W = x.shape # 注意下面的embed_dim代表的是卷积核的数量，也就是经过卷积后拼接得到的特征图(输出通道)数量 # H`和 W`代表输出特征图的宽和高 # 首先使用卷积层对输入图像进行处理，得到形状为 [B, embed_dim, H', W'] 的特征图 # 然后将特征图的最后两维展平为一维，得到形状为 [B, embed_dim, num_patches] 的张量 # 最后交换第 1 维和第 2 维，得到形状为 [B, num_patches, embed_dim] 的张量 # 这里的 num_patches 是图像块的总数 x = self.proj(x).flatten(2).transpose(1, 2) # 对处理后的张量进行归一化操作 x = self.norm(x) return x\",\"用一个简化版的例子说明上述过程:\",\"核心要点: 将卷积后的通道维数作为embedding的维度，卷积后剩余的长和宽相乘作为时间维度，由此把图片转换为序列的embedding形式。\"]},\"427\":{\"h\":\"3. 添加[class]token\",\"t\":[\"在上面的结构图中可以看到，输入Encoder的最左侧部分添加了一个0*这个Token，这个就是额外添加的一个[class]token，单独用来处理类别信息，经过Encoder之后，需要单独将这个Token再提取出来，输入到MLP Head之中再输出分类结果。\",\"这也是为什么结构图中MLP Head的位置是和这个[class]token对齐。\",\"这里简单介绍一下CLS TOKEN的作用:\",\"[CLS] Token 的作用是通过训练过程中损失值的降低，模型学会如何通过注意力机制将图像的有效信息汇聚到 [CLS] Token 中，从而完成图像分类任务。\",\"初始化： \",\"[CLS] Token 是一个随机初始化的向量，初始时没有任何语义信息。\",\"位置编码被添加到 patch 嵌入中，以保留图像的空间信息。\",\"前向传播： \",\"输入图像被分割成 patches，并通过线性变换映射到嵌入空间。\",\"[CLS] Token 被添加到 patch 嵌入序列的开头。\",\"通过多层 Transformer Encoder，模型计算每个 patch 嵌入（包括 [CLS] Token）与其他 patch 嵌入的关系。\",\"注意力汇聚： \",\"在每一层 Transformer 中，[CLS] Token 通过自注意力机制与其他 patch 嵌入交互。\",\"模型学会将图像中与分类任务相关的信息汇聚到 [CLS] Token 中。\",\"损失计算与反向传播： \",\"[CLS] Token 的输出向量被输入到分类头中，用于预测图像的类别。\",\"通过计算损失（如交叉熵损失），模型更新参数，使得 [CLS] Token 能够更好地聚合图像信息。\",\"收敛： \",\"随着训练的进行，损失值逐渐降低，模型学会如何通过注意力机制将图像的有效信息汇聚到 [CLS] Token 中。\",\"最终，[CLS] Token 的输出向量能够很好地表示图像的全局特征，用于分类任务。\",\"[CLS] Token 能起作用的原因在于：\",\"注意力机制的特性： \",\"自注意力机制能够捕捉图像中任意两个 patches 之间的关系。\",\"[CLS] Token 通过与其他 patches 的交互，能够动态地聚合图像信息。\",\"训练目标的引导： \",\"训练过程中，损失函数直接作用于 [CLS] Token 的输出。\",\"模型被强制学会将图像的有效信息汇聚到 [CLS] Token 中，以最小化损失。\",\"全局特征表示： \",\"[CLS] Token 位于序列的开头，能够通过多层 Transformer 逐步聚合全局信息。\",\"最终，[CLS] Token 的输出向量能够很好地表示图像的全局特征。\",\"class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, embed_layer=None): \\\"\\\"\\\" Args: img_size (int, tuple): 输入图像的尺寸 patch_size (int, tuple): 图像块的尺寸 in_c (int): 输入图像的通道数 num_classes (int): 分类任务的类别数 embed_dim (int): 嵌入维度 embed_layer (nn.Module): 图像块嵌入层 \\\"\\\"\\\" super(VisionTransformer, self).__init__() # 保存分类任务的类别数 self.num_classes = num_classes # 保存嵌入维度，同时作为特征数量，以保持与其他模型的一致性 self.num_features = self.embed_dim = embed_dim # 只有一个特殊标记（分类标记） self.num_tokens = 1 # 创建图像块嵌入层 self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # 计算图像块的数量 num_patches = self.patch_embed.num_patches # 创建可学习的分类标记 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 使用截断正态分布初始化分类标记 nn.init.trunc_normal_(self.cls_token, std=0.02) ... def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] # 将输入图像进行图像块嵌入 x = self.patch_embed(x) # [B, 196, 768] # [1, 1, 768] -> [B, 1, 768] # 扩展分类标记以匹配输入批次大小 cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将分类标记和图像块嵌入拼接 x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] ... # 返回分类标记对应的特征,x[:,0]对应维度为[B,1,768] return x[:,0]; def forward(self, x): # 提取输入图像的特征 x = self.forward_features(x) # 通过分类头 -- [B,1,768] x = self.head(x) return x\"]},\"428\":{\"h\":\"4. 添加位置编码\",\"t\":[\"在Transformer中，位置编码的作用是为了记忆输入的语序信息。ViT中，同样需要位置编码来记录各图像块之间的位置信息。\",\"这里主要有两种位置编码思路，一种思路是在转换之前(14,14)的图像块矩阵添加二维(2-D)位置编码，另一种思路是在转换后(196+1)这个维度上添加一维(1-D)位置编码。\",\"论文作者也对其做了实验，实验结果如下表所示：\",\" 可以看到，添加一维位置编码和二维位置编码并没有太大的差异。作者随后也对一维位置编码的结果进行了可视化，结果如下图所示：\",\" 上图中是每一个Patch中各位置的位置编码相似性度量，越接近黄色的位置代表越靠近位置编码的中心位置，可以看到，即使是一维位置编码，同样可以比较好地记录二维信息。\",\"class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, representation_size=None, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., embed_layer=None, norm_layer=None, act_layer=None): super(VisionTransformer, self).__init__() # 保存分类任务的类别数 self.num_classes = num_classes # 保存嵌入维度，同时作为特征数量，以保持与其他模型的一致性 self.num_features = self.embed_dim = embed_dim # 只有一个特殊标记（分类标记） self.num_tokens = 1 # 创建图像块嵌入层 self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # 计算图像块的数量 num_patches = self.patch_embed.num_patches # 创建可学习的分类标记 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 创建可学习的位置嵌入 self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim)) # 创建丢弃层，用于位置嵌入后的随机丢弃 self.pos_drop = nn.Dropout(p=drop_ratio) ... # 权重初始化 # 使用截断正态分布初始化位置嵌入 nn.init.trunc_normal_(self.pos_embed, std=0.02) # 使用截断正态分布初始化分类标记 nn.init.trunc_normal_(self.cls_token, std=0.02) ... def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] # 将输入图像进行图像块嵌入 x = self.patch_embed(x) # [B, 196, 768] # [1, 1, 768] -> [B, 1, 768] # 扩展分类标记以匹配输入批次大小 cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将分类标记和图像块嵌入拼接 x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] # 加上位置嵌入并进行随机丢弃 x = self.pos_drop(x + self.pos_embed) ... # 返回分类标记对应的特征 return x[:, 0] def forward(self, x): # 提取输入图像的特征 x = self.forward_features(x) # 通过分类头 x = self.head(x) return x\",\"上面代码实现中使用的是可学习位置嵌入，具体解释如下:\",\"可学习位置嵌入（learnable positional embedding）是指位置嵌入的参数是可以在模型训练过程中通过反向传播算法进行更新的。具体来说，在模型初始化时，位置嵌入会被初始化为一组特定的值（通常是随机初始化或者初始化为零），然后在训练过程中，这些值会根据模型的损失函数不断调整，以使得模型能够学习到最适合当前任务的位置表示。\"]},\"429\":{\"h\":\"5. Encoder\",\"t\":[\"ViT虽然采用的是Transformer Encoder的结构，但是和Transformer原始的Encoder还是有所区别，我将两者的结构进行对比，如下图所示，左侧为Transformer原始的Encoder结构。\",\" 可以看到，大致上两者结构是相同的，主要区别在于Norm层的顺序，原始Transformer的Norm层在多头注意力和前馈网络之后，而ViT将其放到前面，这里的原因，论文里没有做解释。\",\"关于Norm层，ViT仍是采用Transformer中用到Layer Normalization，计算公式如下：\",\"Norm层之后同样是多头注意力层(Multi-Head Attention)，和Transformer中的一样。\",\"后面的MLP是个单独的结构，就是两个线性层+GELU激活函数+Dropout的结构 ：\",\" MLP Block 中第一个线性层把输入特征投影到一个更高维度的空间后，不同特征之间能够进行更多样的组合。这有助于模型发现输入数据中更复杂的模式和关系。第二个线性层再把高维特征映射回原来的维度，这样就可以提取出对最终任务有帮助的特征组合。\",\"单一的线性层只能进行线性变换，其表达能力是有限的。在两个线性层之间通常会插入一个非线性激活函数（如 GELU），这样就能让 MLP 学习到输入数据的非线性特征。第一个线性层将输入特征映射到更高维度的空间，在这个高维空间里，数据的分布更加稀疏，也就为非线性激活函数提供了更多可以学习的特征组合，从而增强了模型的表达能力。\",\"class Block(nn.Module): def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm): super(Block, self).__init__() # 第一个归一化层，对输入进行归一化处理 self.norm1 = norm_layer(dim) # 多头自注意力层 self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio) # DropPath 层，用于随机深度，当 drop_path_ratio 大于 0 时使用，否则使用恒等映射 self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity() # 第二个归一化层，对经过注意力层的输出进行归一化处理 self.norm2 = norm_layer(dim) # 计算 MLP 的隐藏维度 mlp_hidden_dim = int(dim * mlp_ratio) # 创建 MLP 层 self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio) def forward(self, x): # 残差连接：输入加上经过归一化和注意力层处理后的输出 x = x + self.drop_path(self.attn(self.norm1(x))) # 残差连接：输入加上经过归一化和 MLP 层处理后的输出 x = x + self.drop_path(self.mlp(self.norm2(x))) return x\",\"class Mlp(nn.Module): \\\"\\\"\\\" MLP as used in Vision Transformer, MLP-Mixer and related networks \\\"\\\"\\\" def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.): super().__init__() # 如果未指定 out_features，则默认为 in_features out_features = out_features or in_features # 如果未指定 hidden_features，则默认为 in_features hidden_features = hidden_features or in_features # 第一个全连接层，将输入特征映射到隐藏特征空间 self.fc1 = nn.Linear(in_features, hidden_features) # 激活函数层，默认使用 GELU 激活函数 self.act = act_layer() # 第二个全连接层，将隐藏特征映射到输出特征空间 self.fc2 = nn.Linear(hidden_features, out_features) # Dropout 层，用于防止过拟合 self.drop = nn.Dropout(drop) def forward(self, x): # 通过第一个全连接层 x = self.fc1(x) # 通过激活函数层 x = self.act(x) # 应用 Dropout x = self.drop(x) # 通过第二个全连接层 x = self.fc2(x) # 再次应用 Dropout x = self.drop(x) return x\",\"一个block之后维度依然和输入相同，都是197 x 768 ，因此可以堆叠多个block。\"]},\"430\":{\"h\":\"6. 多头自注意力\",\"t\":[\"ViT中的多头自注意力模块实现逻辑和Transformer基本一致，主要的区别就是去掉了Paddding_Mask和Casual_Mask部分相关的掩码逻辑。\",\"下面所给出的代码实现，注意是通过一个线性层来同时计算qkv三个矩阵，这样可以提升计算效率。\",\"class Attention(nn.Module): def __init__(self, dim, # 嵌入层维度 num_heads=8, # 注意力头的数量，默认为8 qkv_bias=False, # 是否在生成Q、K、V时使用偏置，默认为False qk_scale=None, # 缩放因子，用于调整注意力分数，若为None则使用默认值 attn_drop_ratio=0., # 注意力矩阵的丢弃率，默认为0 proj_drop_ratio=0.): # 投影层的丢弃率，默认为0 super(Attention, self).__init__() self.num_heads = num_heads # 保存注意力头的数量 head_dim = dim // num_heads # 计算每个注意力头的维度 self.scale = qk_scale or head_dim ** -0.5 # 确定缩放因子，若qk_scale未指定，则使用默认的缩放因子 # 定义一个线性层，将输入的维度dim映射到dim * 3，用于同时生成查询（Q）、键（K）和值（V） self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) # 定义注意力矩阵的丢弃层，防止过拟合 self.attn_drop = nn.Dropout(attn_drop_ratio) # 定义投影层，将多头注意力的输出进行线性变换 self.proj = nn.Linear(dim, dim) # 定义投影层的丢弃层，防止过拟合 self.proj_drop = nn.Dropout(proj_drop_ratio) # 没有padding_mask, casual_mask def forward(self, x): # [batch_size, num_patches + 1, total_embed_dim] # 获取输入张量x的形状，B为批量大小，N为序列长度（包含分类token），C为输入token的总维度 B, N, C = x.shape # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim] # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head] # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head] # 通过qkv线性层将输入x映射到dim * 3的维度，然后调整形状并重新排列维度 # 下面的3是因为我们用一次矩阵运算得到了拼接在一起的Q,K,V矩阵，这里需要将其分离开来 qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) # [batch_size, num_heads, num_patches + 1, embed_dim_per_head] # 从qkv张量中分离出查询（Q）、键（K）和值（V） # 注意: Q,K,V计算来源相同,因此是自注意力 q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use tensor as tuple) # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1] # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1] # 将Q和K的转置相乘，得到注意力分数矩阵，再乘以缩放因子scale attn = (q @ k.transpose(-2, -1)) * self.scale # 对注意力分数矩阵应用softmax函数，得到注意力权重矩阵 attn = attn.softmax(dim=-1) # 对注意力权重矩阵应用丢弃层，防止过拟合 attn = self.attn_drop(attn) # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head] # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head] # reshape: -> [batch_size, num_patches + 1, total_embed_dim] # 将注意力权重矩阵与V相乘，得到每个注意力头的输出 # 对输出进行维度交换和形状调整，将多个注意力头的输出合并为一个张量 x = (attn @ v).transpose(1, 2).reshape(B, N, C) # 通过投影层对合并后的张量进行线性变换 x = self.proj(x) # 对投影后的结果应用丢弃层，防止过拟合 x = self.proj_drop(x) return x\",\"关于多头注意力机制流程不太清楚的，可以看这篇文章。\"]},\"431\":{\"h\":\"7. MLP Head\",\"t\":[\"在Transformer Encoder输出结果之后，需要再将第一个添加的Class Token提取出来，然后输入到MLP Head进行分类。在论文中，作者先是在ImageNet21K上进行预训练，MLP Head结构由Linear+tanh激活函数+Linear组成，但是迁移到其它数据集训练时，只需要用一个一个Linear即可。\",\"class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, representation_size=None, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., embed_layer=None, norm_layer=None, act_layer=None): super(VisionTransformer, self).__init__() # 保存分类任务的类别数 self.num_classes = num_classes # 保存嵌入维度，同时作为特征数量，以保持与其他模型的一致性 self.num_features = self.embed_dim = embed_dim # 只有一个特殊标记（分类标记） self.num_tokens = 1 # 如果没有提供归一化层，则使用默认的 LayerNorm，epsilon 为 1e-6 norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6) # 如果没有提供激活函数层，则使用 GELU 激活函数 act_layer = act_layer or nn.GELU # 创建图像块嵌入层 self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # 计算图像块的数量 num_patches = self.patch_embed.num_patches # 创建可学习的分类标记 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 创建可学习的位置嵌入 self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim)) # 创建丢弃层，用于位置嵌入后的随机丢弃 self.pos_drop = nn.Dropout(p=drop_ratio) # 创建Encoder Block块序列 self.blocks = nn.Sequential(*[ Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i], norm_layer=norm_layer, act_layer=act_layer) for i in range(depth) ]) # 创建归一化层 self.norm = norm_layer(embed_dim) ############################# MLP Head ############################################ # 更新特征数量为表示层的维度 self.num_features = representation_size # 创建预输出层，包含一个线性层和一个 Tanh 激活函数 self.pre_logits = nn.Sequential(OrderedDict([ (\\\"fc\\\", nn.Linear(embed_dim, representation_size)), (\\\"act\\\", nn.Tanh()) ])) # 分类头 # 如果类别数大于 0，则创建线性分类头，否则为恒等映射 self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity() ########################################################################### # 权重初始化 # 使用截断正态分布初始化位置嵌入 nn.init.trunc_normal_(self.pos_embed, std=0.02) # 使用截断正态分布初始化分类标记 nn.init.trunc_normal_(self.cls_token, std=0.02) # 应用自定义的权重初始化函数 self.apply(_init_vit_weights) def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] # 将输入图像进行图像块嵌入 x = self.patch_embed(x) # [B, 196, 768] # [1, 1, 768] -> [B, 1, 768] # 扩展分类标记以匹配输入批次大小 cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将分类标记和图像块嵌入拼接 x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] # 加上位置嵌入并进行随机丢弃 x = self.pos_drop(x + self.pos_embed) # 通过Encoder Block块序列 x = self.blocks(x) # 进行归一化 x = self.norm(x) # 返回分类标记对应的特征 -- 先交给预输出层进行处理 return self.pre_logits(x[:, 0]) def forward(self, x): # 提取输入图像的特征 x = self.forward_features(x) # 通过分类头 -- 映射到分类空间中去 x = self.head(x) return x\",\"self.pre_logits 模块可以看作是一个特征预处理模块，它位于最终分类头之前。通过将特征映射到特定的维度并进行非线性变换，该模块可以为后续的分类任务提供更具区分性和稳定性的特征表示，从而提高模型的分类性能。\",\"输出结果之后，再和真实标签做交叉熵损失，这样就可以完成ViT的训练过程。\",\"def train_one_epoch(model, optimizer, data_loader, device, epoch): ... # 遍历数据加载器中的每个批次数据 for step, data in enumerate(data_loader): # 解包数据，得到图像和对应的标签 images, labels = data # 累加当前批次的样本数到总样本数中 sample_num += images.shape[0] # 将图像数据移动到指定设备上，并通过模型进行前向传播，得到预测结果 pred = model(images.to(device)) # 从预测结果中找出每个样本预测概率最大的类别索引 pred_classes = torch.max(pred, dim=1)[1] # 计算预测正确的样本数，并累加到累计正确样本数中 accu_num += torch.eq(pred_classes, labels.to(device)).sum() # 计算预测结果与真实标签之间的交叉熵损失 loss = loss_function(pred, labels.to(device)) # 进行反向传播，计算梯度 loss.backward() ...\"]},\"432\":{\"h\":\"效果对比\",\"t\":[\"在论文中，作者将ViT和之前图像分类领域比较强的ResNet模型进行了对比测试，结果如下：\",\" 可以看到，右图中，作者使用了谷歌制作的JFT-300M数据集，当数据量小于30M时，ViT的效果表现不如ResNet，但是当数据量逐渐增大时，ViT才会慢慢超越ResNet。由此可见ViT工作的局限性，它必须要在超大数据集上进行预训练，然后再拿到其它数据集上做迁移学习，才会有好的效果。\",\"关于ViT模型的不同版本，论文里也做了说明： 其中的Layers就是Transformer Encoder中重复堆叠Encoder Block的次数，Hidden Size就是对应通过Embedding层后每个token的dim（向量的长度），MLP size是Transformer Encoder中MLP Block第一个全连接的节点个数（是Hidden Size的四倍），Heads代表Transformer中Multi-Head Attention的heads数。\",\"在深度学习领域，当提到模型参数量时，“M” 通常是 “million” 的缩写，代表 “百万”。所以参数量为 86M 就意味着模型大约有 86×1000000 = 8600000（八百六十万）个可训练参数。\",\"与之类似的还有 “B”，它是 “billion” 的缩写，代表 “十亿”。例如参数量为 1.2B 就表示模型大约有 1.2×1000000000 = 1200000000（十二亿）个可训练参数。\"]},\"433\":{\"h\":\"注意力可视化\",\"t\":[\"ViT这篇论文长达二十多页，里面包含了非常丰富的成果，其中包括注意力可视化。由于作者是首次将Transformer应用到图像领域，里面包含了注意力机制，那么作者就想把注意力得到的结果(也就是Q-K矩阵乘积)换源到图像上，得到结果如下图所示：\",\"可以看到，模型自动学习到了如果注意画面中的分类主体。\"]},\"434\":{\"h\":\"混合模型探索\",\"t\":[\"在论文的最后，作者又探索了一种混合模型(Hybrid)，就是将传统CNN和Transformer进行结合。\",\"下表中对比了ViT、ResNet和混合模型在不同图像分类数据集上的测试结果，可以看到当Epochs增大时，ResNet和混合模型的效果均不如ViT模型。\",\"混合模型的常见结合方式:\",\"CNN 作为特征提取器，Transformer 作为编码器 \",\"先用 CNN 对输入数据进行初步的特征提取，利用 CNN 的局部特征提取能力快速捕捉图像的底层特征。例如，在图像分类任务中，可以使用预训练的 ResNet 等 CNN 模型提取图像的特征图。\",\"然后将 CNN 提取的特征图转换为序列形式，输入到 Transformer 中进行进一步的处理。Transformer 可以利用其自注意力机制捕捉特征之间的长距离依赖关系，对特征进行更深入的建模。\",\"交错堆叠 CNN 和 Transformer 模块 \",\"在模型架构中，将 CNN 层和 Transformer 层交错堆叠。例如，先经过一层或多层 CNN 进行局部特征提取，然后再经过一层 Transformer 捕捉全局信息，如此反复。这样可以在模型的不同阶段交替利用 CNN 和 Transformer 的优势。\",\"在 Transformer 中引入卷积操作 \",\"在 Transformer 的架构中融入卷积操作，例如在多头自注意力机制或前馈网络中引入卷积层。这样可以为 Transformer 赋予局部特征提取的能力，同时保留其捕捉长距离依赖的优势。\"]},\"435\":{\"h\":\"加载预训练模型\",\"t\":[\"上面已经给出了数据集加载以及ViT模型核心代码实现了，下面我们将进入训练流程；首先说明，本次训练是基于预训练好的ViT-B/16这个模型进行微调，整体结构图如下：\",\"具体为vit_base_patch16_224_in21k这个模型:\",\"vit：代表 Vision Transformer。\",\"base：表示模型的规模。\",\"patch16：意味着在处理图像时，会将输入图像分割成大小为 16×16 像素的图像块（patches）。\",\"224：指的是输入图像的尺寸为 224×224 像素。在预训练和使用该模型时，需要将输入图像调整为这个固定的尺寸。\",\"in21k：该模型是在 ImageNet - 21k 数据集上进行预训练的。ImageNet - 21k 是一个大规模的图像数据集，包含大约 21000 个类别和 1.4 亿张图像。在如此大规模的数据集上进行预训练，模型能够学习到丰富的图像特征和模式，具有较强的泛化能力。\",\"def vit_base_patch16_224_in21k(num_classes: int = 21843, has_logits: bool = True): \\\"\\\"\\\" ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929). ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer. weights ported from official Google JAX impl: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch16_224_in21k-e5005f0a.pth \\\"\\\"\\\" model = VisionTransformer(img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, representation_size=768 if has_logits else None, num_classes=num_classes) return model # 加载预训练好的vit_base_patch16_224_in21k模型权重文件 model = vit_base_patch16_224_in21k(num_classes=5, has_logits=False).to(device) weights_dict = torch.load(args.weights, map_location=device) model.load_state_dict(weights_dict, strict=False)\",\"加载该模型后，训练了10个epoch，验证集上准确率达到了98.5%。整体模型还是比较大的，预训练权重大小为393MB，但是训练速度还是挺快的，因为在代码中有个冻结权重的操作，主干部分全部冻结，仅训练分类头。\",\"for name, para in model.named_parameters(): # 除head, pre_logits外，其他权重全部冻结 if \\\"head\\\" not in name and \\\"pre_logits\\\" not in name: para.requires_grad_(False) else: print(\\\"training {}\\\".format(name))\",\"训练与评估流程的代码为模版代码，考虑篇幅原因，这里不再贴出，大家可以自行拉取项目完整代码进行学习:\",\"https://pan.baidu.com/s/1rkdjdlR37O7gSr9j1mhjBg?pwd=vket\"]},\"436\":{\"h\":\"总结\",\"t\":[\"Vision Transformer证明了使用Transformer结构可以有效处理图像数据，并且取得了与卷积神经网络（CNN）相媲美的效果。\",\"统一多模态的可能性：使用Transformer架构为未来的多模态统一提供了可能性。\",\"图像到文本的桥梁：架起了图像空间到文本空间的桥梁。\",\"ViT核心：如何将二维图像转换为一维时间序列？通过将图像切成小片（Patches），并按行优先排序来实现。\"]},\"437\":{\"h\":\"开源项目\"},\"438\":{\"h\":\"API记录之Numpy篇\",\"t\":[\"API记录之Numpy篇\"]},\"439\":{\"h\":\"Numpy\"},\"440\":{\"h\":\"np.linspace\",\"t\":[\"np.linspace 是 NumPy 中生成等间隔数列的函数。基本用法：\",\"import numpy as np # 在 0 到 1 之间生成 5 个等间隔数 arr = np.linspace(0, 1, 5) print(arr) # 输出: [0. 0.25 0.5 0.75 1. ]\",\"参数说明：\",\"start：起始值\",\"stop：结束值\",\"num：生成的样本数量（默认 50）\",\"endpoint：是否包含 stop（默认 True）\",\"retstep：是否返回步长（True 返回 (array, step)）\"]},\"441\":{\"h\":\"np.concatenate\",\"t\":[\"np.concatenate 用于沿指定轴将多个数组拼接在一起。\",\"import numpy as np a = np.array([1, 2]) b = np.array([3, 4]) # 沿默认轴（axis=0）拼接 c = np.concatenate([a, b]) print(c) # 输出: [1 2 3 4] # 对二维数组沿不同轴拼接 a2 = np.array([[1, 2], [3, 4]]) b2 = np.array([[5, 6], [7, 8]]) # 沿行拼接（axis=0） c2 = np.concatenate([a2, b2], axis=0) # [[1 2] # [3 4] # [5 6] # [7 8]] # 沿列拼接（axis=1） c3 = np.concatenate([a2, b2], axis=1) # [[1 2 5 6] # [3 4 7 8]]\"]},\"442\":{\"h\":\"API记录之Python篇\",\"t\":[\"API记录之Python篇\"]},\"443\":{\"h\":\"Python\"},\"444\":{\"h\":\"作用域\",\"t\":[\"Python 的作用域遵循 LEGB（Local → Enclosing → Global → Built-in） 原则。\",\"Local：函数或代码块内部定义的名字。\",\"Enclosing：外层函数的作用域。\",\"Global：模块文件的顶层作用域。\",\"Built-in：Python 内置命名空间。\",\"关键点是：\",\"➡️ Python 没有像 C/C++ 那样的“块级作用域”。\",\"也就是说，在 if、for、while 这些代码块里定义的变量，并不会限制在这个代码块内部，而是直接存在于函数作用域里。\",\"Python 代码块与作用域对照表:\",\"代码块类型\",\"示例\",\"是否产生新作用域？\",\"说明\",\"模块（module）\",\"一个 .py 文件\",\"✅\",\"文件顶层的名字都在模块作用域内（全局作用域）。\",\"函数定义（def / lambda）\",\"def f(): ...\",\"✅\",\"每次调用函数都会创建一个新的局部作用域（Local）。\",\"类定义（class）\",\"class A: ...\",\"✅\",\"类体代码在独立的命名空间里执行，成员存入类的属性字典。\",\"if / else / elif\",\"if cond: x = 1\",\"❌\",\"不产生新作用域，变量提升到所在函数/模块作用域。\",\"for / while\",\"for i in range(3): ...\",\"❌\",\"循环变量在循环体外仍然可见。\",\"try / except / finally\",\"try: ... except: ...\",\"❌\",\"不产生新作用域，里面的变量外面也能用。\",\"with\",\"with open(...) as f:\",\"❌\",\"不产生新作用域，f 在外面仍然可见。\",\"推导式 (Python 3.x)\",\"[x for x in range(5)]\",\"✅（局部作用域）\",\"列表/字典/集合/生成器推导式里的循环变量 只在推导式内部有效，不会泄漏到外部（Python 2 会泄漏）。\"]},\"445\":{\"h\":\"位置参数与关键字参数\",\"t\":[\"位置参数（Positional Argument）：按 位置顺序 传入函数的参数。\",\"关键字参数（Keyword Argument）：用 key=value 的形式明确指定的参数。\",\"场景\",\"位置参数 *\",\"关键字参数 **\",\"调用时\",\"解包 tuple/list\",\"解包 dict\",\"定义时\",\"收集成 tuple\",\"收集成 dict\"]},\"446\":{\"h\":\"闭包与高阶导数\"},\"447\":{\"h\":\"什么是高阶函数？\",\"t\":[\"高阶函数（Higher-Order Function）满足以下两个条件之一即可：\",\"函数接收另一个函数作为参数；\",\"函数返回一个函数。\",\"Python 中的 map、sorted、functools.partial 都是高阶函数。\",\"比如这个函数就是高阶函数：\",\"def outer(func): # 接收函数作为参数 def inner(): print(\\\"调用前\\\") func() print(\\\"调用后\\\") return inner # 返回一个函数\"]},\"448\":{\"h\":\"什么是闭包？\",\"t\":[\"闭包是一个函数，它“记住”了它定义时的 外部作用域变量，即使外部函数已经执行完毕，这些变量依然存在。\",\"例如：\",\"def outer(): x = 10 def inner(): print(x) # inner 记住了 x return inner f = outer() f() # 输出 10\",\"这里 inner 是一个闭包，因为它引用了 outer 中的变量 x，而 outer 已经返回了。\",\"正常情况下：局部变量会在函数执行完后被释放; 但如果我们在内部函数中引用了外部函数的变量，Python 会自动把这些变量“绑定”到这个内部函数上，也就是形成闭包, 变量“被引用”而不会释放。\"]},\"449\":{\"h\":\"装饰器的实现用到了什么？\",\"t\":[\"现在看一个典型的装饰器例子：\",\"def my_decorator(func): # ✅ 高阶函数（接收函数并返回函数） def wrapper(*args, **kwargs): # ✅ wrapper 是闭包（记住了 func） print(\\\"Before call\\\") result = func(*args, **kwargs) print(\\\"After call\\\") return result return wrapper @my_decorator def greet(name): print(f\\\"Hello, {name}\\\")\",\"my_decorator 是 高阶函数，因为它接收 func 并返回 wrapper。\",\"wrapper 是 闭包，因为它访问了其外部作用域的变量 func，并在被调用时依然保留这个引用。\",\"“装饰器 = 高阶函数 + 闭包” 的意思是：\",\"一个装饰器的实现，必须用高阶函数（来接收和返回函数），而在返回的内部函数中，依赖闭包机制来记住原函数的引用，从而实现对原函数行为的增强或修改。\"]},\"450\":{\"h\":\"装饰器\",\"t\":[\"装饰器是 Python 中的一种语法结构，本质是一个 函数（或类），它接收一个函数或类作为参数，对其进行加工，并返回一个新的函数或类对象。\",\"简而言之：\",\"装饰器 = 高阶函数 + 闭包\",\"装饰器主要用于在 不修改原始函数代码的前提下，动态增加其功能，这在日志记录、性能测试、权限校验等场景中非常常见。\"]},\"451\":{\"h\":\"最基本的函数装饰器\",\"t\":[\"def my_decorator(func): def wrapper(): print(\\\"调用前\\\") func() print(\\\"调用后\\\") return wrapper @my_decorator def say_hello(): print(\\\"Hello\\\") say_hello()\",\"输出：\",\"调用前 Hello 调用后\",\"说明：\",\"@my_decorator 相当于：say_hello = my_decorator(say_hello)\",\"wrapper() 是闭包，持有对 func 的引用。\",\"返回的 wrapper 函数替代了原来的 say_hello 函数。\"]},\"452\":{\"h\":\"带参数的函数装饰器\",\"t\":[\"装饰器支持原函数有参数的情况：\",\"def my_decorator(func): def wrapper(*args, **kwargs): print(\\\"开始\\\") result = func(*args, **kwargs) print(\\\"结束\\\") return result return wrapper @my_decorator def add(a, b): return a + b print(add(3, 5))\",\"使用 *args 和 **kwargs 是为了支持任意参数签名。\"]},\"453\":{\"h\":\"带参数的装饰器（装饰器工厂）\",\"t\":[\"如果你希望装饰器 本身接受参数，则需要再多一层函数嵌套：\",\"def log(prefix): def decorator(func): def wrapper(*args, **kwargs): print(f\\\"{prefix} 开始调用 {func.__name__}\\\") result = func(*args, **kwargs) print(f\\\"{prefix} 结束调用 {func.__name__}\\\") return result return wrapper return decorator @log(\\\"DEBUG\\\") def multiply(a, b): return a * b\",\"执行顺序：\",\"@log(\\\"DEBUG\\\") 先返回 decorator\",\"然后 decorator(multiply) 返回 wrapper\"]},\"454\":{\"h\":\"使用 保留原函数元信息\",\"t\":[\"装饰器会改变函数的元信息:\",\"def my_decorator(func): def wrapper(*args, **kwargs): print(\\\"Before call\\\") return func(*args, **kwargs) return wrapper @my_decorator def greet(name): \\\"\\\"\\\"Say hello to someone\\\"\\\"\\\" print(f\\\"Hello, {name}\\\") print(greet.__name__) # ⚠️ 输出 wrapper，不是 greet print(greet.__doc__) # ⚠️ 输出 None，不是函数原文档\",\"@my_decorator 返回的是 wrapper 函数，所以 greet 实际上变成了 wrapper，它的名字和文档字符串也被覆盖了，所以使用装饰器会导致原函数的 __name__、__doc__ 等属性丢失。\",\"Python 提供了 functools.wraps(func) 装饰器，作用是：\",\"把原函数的 __name__、__doc__、__module__ 等元信息“复制”到 wrapper 函数上，让被装饰函数看起来仍然像原来的函数。\",\"import functools def my_decorator(func): @functools.wraps(func) # ✅ 这一步很关键 def wrapper(*args, **kwargs): print(\\\"Before call\\\") return func(*args, **kwargs) return wrapper @my_decorator def greet(name): \\\"\\\"\\\"Say hello to someone\\\"\\\"\\\" print(f\\\"Hello, {name}\\\") print(greet.__name__) # ✅ greet print(greet.__doc__) # ✅ Say hello to someone\",\"这在调试、文档生成、类型检查、元编程、反射中都非常重要。例如：\",\"help(greet)：没有 wraps 就看不到真实文档了\",\"使用 inspect 模块查看参数、注解、类型签名会失效\",\"多个装饰器嵌套时更容易出错\"]},\"455\":{\"h\":\"装饰类方法（普通方法 / 类方法 / 静态方法）\",\"t\":[\"def log_method(func): @wraps(func) def wrapper(*args, **kwargs): print(f\\\"调用方法 {func.__name__}\\\") return func(*args, **kwargs) return wrapper class MyClass: @log_method def hello(self): print(\\\"Hello from method\\\")\"]},\"456\":{\"h\":\"装饰整个类\",\"t\":[\"def decorate_class(cls): cls.version = \\\"1.0\\\" return cls @decorate_class class MyService: pass print(MyService.version) # 1.0\"]},\"457\":{\"h\":\"装饰器的底层原理与执行过程\",\"t\":[\"本质：装饰器 = 函数替换器\",\"一个装饰器：\",\"@decorator def func(): pass\",\"等价于：\",\"func = decorator(func)\",\"即：把 func 传给 decorator 函数，并用它的返回值替换 func 本身。\"]},\"458\":{\"h\":\"多个装饰器叠加时的执行顺序（从内到外）\",\"t\":[\"@d1 @d2 def func(): pass\",\"等价于：\",\"func = d1(d2(func))\",\"即，先应用最内层的 d2，再由外层 d1 包裹起来。\"]},\"459\":{\"h\":\"类装饰器\",\"t\":[\"类装饰器通常通过实现 __call__ 方法来模拟函数行为：\",\"class MyDecorator: def __init__(self, func): self.func = func def __call__(self, *args, **kwargs): print(\\\"调用前\\\") result = self.func(*args, **kwargs) print(\\\"调用后\\\") return result @MyDecorator def greet(name): print(f\\\"Hi, {name}\\\") greet(\\\"Alice\\\")\"]},\"460\":{\"h\":\"总结\",\"t\":[\"类型\",\"例子\",\"含义\",\"最基本装饰器\",\"@func\",\"f = func(f)\",\"装饰器工厂\",\"@decorator(x)\",\"f = decorator(x)(f)\",\"对象方法装饰器\",\"@obj.method\",\"f = obj.method(f)\",\"对象方法工厂\",\"@obj.method(args)\",\"f = obj.method(args)(f)\"]},\"461\":{\"h\":\"典型应用场景举例\",\"t\":[\"日志记录：\",\"def log(func): @wraps(func) def wrapper(*args, **kwargs): print(f\\\"调用 {func.__name__} 参数: {args}, {kwargs}\\\") return func(*args, **kwargs) return wrapper\",\"权限控制：\",\"def require_admin(func): @wraps(func) def wrapper(*args, **kwargs): if not user_is_admin(): raise PermissionError(\\\"需要管理员权限\\\") return func(*args, **kwargs) return wrapper\",\"性能测试（统计函数运行时间）：\",\"import time def timing(func): @wraps(func) def wrapper(*args, **kwargs): start = time.time() result = func(*args, **kwargs) print(f\\\"{func.__name__} 耗时: {time.time() - start:.4f}s\\\") return result return wrapper\"]},\"462\":{\"h\":\"地板除 “//”\",\"t\":[\"// 是 地板除（floor division） 运算符，表示向下取整的除法。\",\"表达式\",\"结果\",\"类型\",\"7 / 3\",\"2.333...\",\"float\",\"7 // 3\",\"2\",\"int\"]},\"463\":{\"h\":\"Ellipsis (...)\",\"t\":[\"Ellipsis（用 ... 表示）是 Python 中的语法，用于表示 多维索引中的省略维度。在多维数组或张量索引时，... 可以代替多个冒号 :，表示选择剩余所有维度。\",\"示例：\",\"import torch x = torch.randn(2, 3, 4, 5) # 取第 0 维第 1 个元素，后面所有维度都选中 y = x[1, ...] # 等价于 x[1, :, :, :] print(y.shape) # torch.Size([3, 4, 5]) # 在最后加维度 z = x[..., None] # shape: [2, 3, 4, 5, 1]\",\"总结：... 用于 简化多维索引 或 保留剩余维度。\"]},\"464\":{\"h\":\"API记录之Pytorch篇\",\"t\":[\"API记录之Pytorch篇\"]},\"465\":{\"h\":\"Pytorch\"},\"466\":{\"h\":\"stack\",\"t\":[\"torch.stack() 是 PyTorch 中用于将多个形状相同的张量沿一个新维度拼接的函数。\",\"torch.stack(tensors, dim=0, *, out=None)\",\"tensors：一个可迭代对象（如列表、元组），其中包含多个形状相同的 Tensor。\",\"dim：插入新维度的位置（默认是 0）。这个新维度就是拼接的那一维。\",\"out：可选输出张量，用于写入结果。\",\"例子如下:\",\"注意:\",\"所有张量必须具有完全相同的 shape。\",\"如果你想把一个 batch 中的多个样本打包成一个大 tensor，通常会用 torch.stack()。\"]},\"467\":{\"h\":\"transpose\",\"t\":[\"y = x.transpose(dim0, dim1)\",\"只交换两个指定维度，常用于 2D 或 3D 张量，如图像转置、RNN 输入调整等。\"]},\"468\":{\"h\":\"permute\",\"t\":[\"y = x.permute(dims)\",\"可以任意重新排列所有维度，是 transpose 的泛化，支持多维度同时交换。\",\"transpose() 和 permute() 返回的张量虽然是视图（view），但它们的 内存布局（strides）被改变。如果你接下来要对它们执行 .view() 或某些要求内存连续的操作，就必须先调用 .contiguous()。\",\"执行 transpose(0, 2) 后:\"]},\"469\":{\"h\":\"view\",\"t\":[\"view: 在不复制数据的前提下，返回具有新形状（shape）的张量视图（view）。\",\"new_tensor = x.view(shape)\",\".view() 只适用于连续内存的张量，某些操作（如 permute, transpose）会改变张量的 stride（内存步长），使其变得 非连续。此时必须先 .contiguous() 再 .view()：\",\"x = torch.randn(2, 3, 4) y = x.permute(0, 2, 1) # 改变维度顺序 z = y.contiguous().view(2, -1) # 否则可能报错\",\".view() 不会复制数据，是原张量的一个视图（共享内存）\"]},\"470\":{\"h\":\"reshape\",\"t\":[\"reshape: 返回具有新形状的张量。必要时会复制数据，否则返回视图。 相比 .view()，reshape() 不要求原始张量是连续的，这是它最大的优势。\",\"new_tensor = x.reshape(shape)\",\"在 PyTorch 中，reshape() 在多数情况下会返回原张量的视图（不复制数据），但当张量的内存布局不连续（例如经过了 permute()、transpose() 等操作），或新形状无法与原内存布局兼容时，reshape() 就会进行数据复制以创建新的张量。此外，如果张量来源于 expand()（广播视图），或者跨设备/特殊操作后的中间结果，也可能触发复制。因此，若希望确保内存效率，建议在 reshape 前使用 .is_contiguous() 检查，必要时用 .contiguous() 转为连续张量。\"]},\"471\":{\"h\":\"repeat\",\"t\":[\"tensor.repeat() 是 PyTorch 中用于沿指定维度重复张量内容的操作，它会复制数据，从而扩展张量的形状（不是视图）。\",\"repeated_tensor = x.repeat(repeat_1, repeat_2, ..., repeat_n)\",\"参数个数必须和 x 的维度数相同。\",\"每个 repeat_i 表示该维度上复制的次数。\",\"import torch x = torch.tensor([[1, 2], [3, 4]]) x = x.repeat(2,3) print(x) output: tensor([[1, 2, 1, 2, 1, 2], [3, 4, 3, 4, 3, 4], [1, 2, 1, 2, 1, 2], [3, 4, 3, 4, 3, 4]])\"]},\"472\":{\"h\":\"expand\",\"t\":[\"tensor.expand() 是 PyTorch 中用于扩展张量尺寸但不复制数据的一种高效方法，它通过广播（broadcasting）机制生成新的视图，节省内存。\",\"expanded_tensor = x.expand(size_1, size_2, ..., size_n)\",\"参数个数必须和 x.dim() 相同，或可以通过在前面添加维度来自动广播。\",\"某一维如果是 -1，表示保持原来的大小。\",\"x = torch.tensor([[1], [2], [3]]) # shape: [3, 1] x.expand(3, 4) # → 每行复制 4 次，但不占用额外内存 # tensor([[1, 1, 1, 1], # [2, 2, 2, 2], # [3, 3, 3, 3]])\",\"使用 -1 保留维度：\",\"x = torch.randn(3, 1, 5) # shape: [3, 1, 5] x.expand(-1, 4, -1) # shape → [3, 4, 5]\",\"核心原则：只有原始维度 = 1 的位置，才能通过 expand 变大；其他位置必须 相等。\",\"x = torch.tensor([[1, 2, 3]]) # shape: [1, 3] y = x.expand(2, 3) # ✅ 第 0 维是 1 → 可以扩展成 2 # ❌ 第 1 维是 3 → 目标仍是 3，虽然没变，但也不能写成 6！ x.expand(2, 6) # ❌ 报错！因为第 1 维是 3，不能变成 6\",\"特性\",\".expand()\",\".repeat()\",\"是否复制数据\",\"❌ 否（返回视图，节省内存）\",\"✅ 是（创建新张量，开销大）\",\"是否支持广播\",\"✅ 支持（自动按维度扩展）\",\"❌ 不支持，必须精确指定每维重复次数\",\"是否可用于改变维度\",\"❌ 否（维度必须兼容）\",\"✅ 是\",\"常用于\",\"高效广播，如 attention、masking 等\",\"实际复制，如构造重复输入\"]},\"473\":{\"h\":\"@torch.no_grad()\",\"t\":[\"在这个装饰器修饰的函数内，PyTorch 不会跟踪计算图，也不会计算梯度。\",\"这样可以减少内存使用和计算开销，因为不需要保存中间变量用于反向传播。\",\"适用于只需要前向推理且不需要更新模型参数的场景。\"]},\"474\":{\"h\":\"register_buffer\",\"t\":[\"# nn.Module 类中提供的方法 register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True)\",\"name (str)\",\"缓冲区的名称（字符串）。\",\"之后可以用 model.name 访问，比如 model.queue。\",\"tensor (torch.Tensor 或 None)\",\"要注册的张量。\",\"这个张量会成为模型的一个成员，但不会被视为可训练参数。\",\"也可以传 None，表示先占位，后面再赋值。\",\"persistent (bool，默认 True，PyTorch 1.8以后支持)\",\"如果为 True，该缓冲区会包含在 state_dict() 中，即会被保存和加载。\",\"如果为 False，缓冲区不会保存到 state_dict()，常用于临时缓存数据。\",\"register_buffer的作用和意义：\",\"它会把一个张量（tensor）作为模型的缓冲区注册，不会被当作模型的可训练参数（不会出现在model.parameters()里，也不会参与梯度计算或优化）。\",\"但是，缓冲区会被自动保存到模型的状态字典（state_dict）中，也会被加载（load）和保存（save）。\",\"常用于保存一些模型的状态信息，但这些信息不需要训练，比如：均值、方差、队列、掩码等。\"]},\"475\":{\"h\":\"einsum\",\"t\":[\"einsum 是 爱因斯坦求和约定（Einstein Summation） 的简写，是一个非常强大且直观的张量操作工具。\",\"相比 matmul、bmm、torch.matmul 这类 API，einsum 让你显式指定维度之间怎么相乘/求和/保留。\",\"torch.einsum(\\\"维度规则\\\", [tensor1, tensor2, ...])\",\"引号中是 对每个 tensor 的维度命名\",\"相同的维度字母表示要做 点积/求和\",\"没有重复的维度字母表示保留该维度\",\"einsum 表达式\",\"等价操作\",\"输出形状\",\"含义\",\"\\\"nc,nc->n\\\"\",\"(q * k).sum(dim=1)\",\"(N,)\",\"每个 query 与其正样本的点积\",\"\\\"nc,ck->nk\\\"\",\"torch.matmul(q, queue)\",\"(N, K)\",\"每个 query 与所有负样本的相似度\"]},\"476\":{\"h\":\"where\",\"t\":[\"torch.where(condition, x, y)\",\"condition：一个布尔型张量，用来判断条件是否成立。\",\"返回一个新张量：\",\"当 condition 对应位置为 True 时，取 x 中对应位置的元素；\",\"当 condition 对应位置为 False 时，取 y 中对应位置的元素。\"]},\"477\":{\"h\":\"torch.nn.functional.pad\",\"t\":[\"text = F.pad(text, (1, 0), value=0)\",\"text：待填充的张量，比如形状是 (batch_size, seq_len)。\",\"(1, 0)：指定填充的方式，这里是一个长度为2的元组 (padding_left, padding_right)，表示在最后一个维度的左侧填充1个元素，右侧填充0个元素。\",\"value=0：用来填充的数值，这里是用0填充。\",\" x = torch.tensor([1, 2, 3, 4, 5]) print(\\\"Original tensor:\\\", x) # 在最后一个维度左边填充1个0，右边不填充 padded_1 = F.pad(x, (1, 0), value=0) print(\\\"Pad (1, 0):\\\", padded_1) # 在最后一个维度左边不填充，右边填充2个9 padded_2 = F.pad(x, (0, 2), value=9) print(\\\"Pad (0, 2) with 9:\\\", padded_2) # 在最后一个维度两边各填充2个-1 padded_3 = F.pad(x, (2, 2), value=-1) print(\\\"Pad (2, 2) with -1:\\\", padded_3)\",\"output:\",\"Original tensor: tensor([1, 2, 3, 4, 5]) Pad (1, 0): tensor([0, 1, 2, 3, 4, 5]) Pad (0, 2) with 9: tensor([1, 2, 3, 4, 5, 9, 9]) Pad (2, 2) with -1: tensor([-1, -1, 1, 2, 3, 4, 5, -1, -1])\"]},\"478\":{\"h\":\"rearrange\",\"t\":[\"rearrange 是一个来自 einops（Einstein Operations）库的函数，用于对张量（Tensor）进行灵活、直观的重排、维度变换、转置、扩展等操作。\",\"from einops import rearrange output = rearrange(tensor, pattern)\",\"tensor 是输入张量。\",\"pattern 是一个字符串，描述输入和输出维度的对应关系，类似模式匹配。\",\"rearrange(x, 'b c h w -> b h w c') # 交换维度顺序 x = torch.randn(4) # shape (4,) y = rearrange(x, 'b -> b 1') # 变成 (4,1)，增加一个维度 x = torch.randn(2, 3, 4) y = rearrange(x, 'b c d -> b (c d)') # 把c和d合并成一个维度 x = torch.randn(2, 12) y = rearrange(x, 'b (c d) -> b c d', c=3) # 把12拆分成3和4\"]},\"479\":{\"h\":\"Tensor.uniform_\",\"t\":[\"Tensor.uniform_(from=0, to=1)\",\"把一个 已有的张量，用 均匀分布随机数填充。\",\"生成的值在 [from, to) 范围内，默认是 [0, 1)。\",\"加上 _ 说明是原地修改：直接在原张量上进行操作，不创建新张量。\"]},\"480\":{\"h\":\"torch.unique_consecutive\",\"t\":[\"作用：返回输入张量中 连续不重复的元素，类似于 NumPy 的 np.unique，但它只去掉 相邻重复值，而不是全局去重。\",\"torch.unique_consecutive( input, return_inverse=False, return_counts=False, dim=None ) -> (Tensor, Optional[Tensor], Optional[Tensor])\",\"input：输入张量。\",\"return_inverse：如果为 True，会额外返回一个张量，表示每个元素在唯一值张量中的索引。\",\"return_counts：如果为 True，会额外返回每个唯一值的 连续出现次数。\",\"dim：指定操作的维度。如果为 None，默认会展平为 1D 处理。\",\"示例1:\",\"import torch x = torch.tensor([1, 1, 2, 2, 3, 1, 1]) out = torch.unique_consecutive(x) print(out) # tensor([1, 2, 3, 1])\",\"这里没有去掉最后那个 1，因为它和前面的 3 不相邻。\",\"示例2：返回计数:\",\"out, counts = torch.unique_consecutive(x, return_counts=True) print(out) # tensor([1, 2, 3, 1]) print(counts) # tensor([2, 2, 1, 2])\",\"示例3: 返回反向索引:\",\"out, inverse = torch.unique_consecutive(x, return_inverse=True) print(out) # tensor([1, 2, 3, 1]) print(inverse) # tensor([0, 0, 1, 1, 2, 3, 3])\",\"示例4: 指定维度:\",\"x = torch.tensor([[1, 1, 2], [1, 2, 2], [3, 3, 3]]) out = torch.unique_consecutive(x, dim=0) print(out) # tensor([[1, 1, 2], # [1, 2, 2], # [3, 3, 3]])\",\"这里按 行 去重，只要相邻两行完全相同就会合并。\"]},\"481\":{\"h\":\"torch.cumsum\",\"t\":[\"作用：对张量沿指定维度做 累加求和（cumulative sum），返回一个新的张量。\",\"torch.cumsum(input, dim, *, dtype=None, out=None) -> Tensor\",\"input: 输入张量\",\"dim: 沿着哪个维度计算累积和\",\"dtype: 指定输出数据类型（可选），如果不指定就保持输入 dtype\",\"out: 输出张量（可选）\",\"返回值: 返回一个和 input 形状相同的张量，元素是按 dim 累加后的值。\",\"示例1: 一维张量\",\"import torch x = torch.tensor([1, 2, 3, 4]) y = torch.cumsum(x, dim=0) print(y) # tensor([ 1, 3, 6, 10])\",\"示例2: 二维张量\",\"x = torch.tensor([[1, 2, 3], [4, 5, 6]]) y = torch.cumsum(x, dim=0) # 沿着行方向 print(y) # tensor([[ 1, 2, 3], # [ 5, 7, 9]])\"]},\"482\":{\"h\":\"torch.Tensor 的 chunk 方法\",\"t\":[\"作用: 用于将张量沿指定维度 分块，基本用法如下：\",\"import torch x = torch.arange(8) # [0,1,2,3,4,5,6,7] # 将张量沿 dim=0 平均分成 4 块 chunks = x.chunk(4, dim=0) for c in chunks: print(c)\",\"输出：\",\"tensor([0, 1]) tensor([2, 3]) tensor([4, 5]) tensor([6, 7])\",\"参数说明:\",\"chunks：要分成的块数\",\"dim：沿哪个维度分块，默认 dim=0\",\"返回值：一个 tuple，包含切分后的张量块\",\"如果张量不能整除块数，前几个块会比后面的多一个元素。\",\"返回的是 tuple 而不是 list。\",\"举个二维例子：\",\"x = torch.arange(16).view(4, 4) chunks = x.chunk(2, dim=0) # 按行分成2块 for c in chunks: print(c)\",\"输出：\",\"tensor([[0, 1, 2, 3], [4, 5, 6, 7]]) tensor([[ 8, 9, 10, 11], [12, 13, 14, 15]])\"]},\"483\":{\"h\":\"torch.randperm\",\"t\":[\"torch.randperm(n) 返回一个长度为 n 的一维张量，包含 0 ~ n-1 的整数，顺序被随机打乱。常用于随机打乱索引，例如：\",\"idx = torch.randperm(5) # 可能输出: tensor([3, 0, 4, 1, 2])\"]},\"484\":{\"h\":\"torch.randint\",\"t\":[\"torch.randint(low, high, size) 返回在 [low, high) 区间内随机生成整数的张量，形状由 size 指定。示例：\",\"x = torch.randint(0, 10, (3, 2)) # 可能输出: tensor([[7, 1], # [3, 9], # [0, 4]])\"]},\"485\":{\"h\":\"torch.bincount\",\"t\":[\"torch.bincount(input, weights=None, minlength=0) 用于统计 非负整数张量input 中每个整数出现的次数，返回一个一维张量。\",\"参数:\",\"input：非负整数张量，一维。\",\"weights（可选）：与 input 同长度的浮点张量，用于加权计数。\",\"minlength（可选）：输出张量的最小长度，如果统计结果长度小于 minlength，在末尾补 0。\",\"返回值:\",\"一维张量 counts，counts[i] 表示整数 i 在 input 中的出现次数（或加权和，如果指定 weights）。\",\"例如:\",\"普通计数：\",\"x = torch.tensor([0, 1, 1, 3]) torch.bincount(x) # 输出: tensor([1, 2, 0, 1])\",\"加权计数：\",\"x = torch.tensor([0, 1, 1, 3]) w = torch.tensor([0.5, 1.0, 2.0, 1.5]) torch.bincount(x, weights=w) # 输出: tensor([0.5, 3.0, 0.0, 1.5])\",\"指定最小长度：\",\"x = torch.tensor([0, 1, 1]) torch.bincount(x, minlength=5) # 输出: tensor([1, 2, 0, 0, 0])\"]},\"486\":{\"h\":\"Tensor.new_zeros\",\"t\":[\"Tensor.new_zeros(*size, dtype=None, device=None) 是 PyTorch 的一个 张量创建方法，它根据已有张量的属性创建一个全零张量。\",\"作用:\",\"生成形状为 size 的全零张量。\",\"张量会和调用它的原张量 在同一设备上（CPU/GPU），并且默认继承原张量的数据类型，除非通过 dtype 指定。\",\"例子:\",\"x = torch.randn(3, 4, device='cuda') # 原张量在 GPU y = x.new_zeros(2, 5) # 在 GPU 上创建 2x5 的全零张量 print(y.device) # 输出: cuda:0\"]},\"487\":{\"h\":\"tensor.scatter_add_\",\"t\":[\"tensor.scatter_add_(dim, index, src)\",\"dim：指定沿哪一维累加。\",\"0 表示按行累加（不同样本累加到不同的簇行）。\",\"1 表示按列累加（按列索引累加元素）。\",\"index：与 src 同形状的整数张量，表示 src 中的每个元素要加到目标张量的哪个位置。\",\"如果 dim=0，index[i,j] 表示 src[i,j] 要加到 tensor[index[i,j], j]。\",\"如果 dim=1，index[i,j] 表示 src[i,j] 要加到 tensor[i, index[i,j]]。\"]},\"488\":{\"h\":\"torch.topk\",\"t\":[\"torch.topk() 是 PyTorch 中一个非常实用的函数，用于获取张量中最大或最小的 k 个值及其索引。\",\"torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None)\",\"input (必需)\",\"输入张量\",\"示例：distances 形状为 (m_batch, n_batch) 的距离矩阵\",\"k (必需)\",\"要返回的最大/最小值的数量\",\"示例：actual_nsample 实际需要的最近邻数量\",\"dim (可选)\",\"沿着哪个维度进行操作\",\"示例：dim=1 表示在每行中找 topk\",\"默认值：最后一个维度 (dim=-1)\",\"largest (可选)\",\"True: 返回最大的 k 个值\",\"False: 返回最小的 k 个值\",\"示例：largest=False 用于找最小距离（最近邻）\",\"sorted (可选)\",\"True: 返回的值按顺序排列\",\"False: 返回的值不保证顺序\",\"默认值：True\"]},\"489\":{\"h\":\"连续性\",\"t\":[\"“连续内存”: 一个多维张量在内存中实际上是以一维数组的形式存储的。\",\"内存连续：意味着按照张量的最右维度（最内层维度） 变化最快的方式（即行优先，Row Major）顺序，将其所有元素无间隔地、顺序地存储在一块内存中。\",\"内存不连续：意味着张量的元素在内存中的存储顺序与其逻辑上的维度顺序不匹配，或者内存中存在间隔（Stride）。\",\"stride 是一个元组，表示在每个维度上移动一个元素时，需要在内存中跳过多少个元素。它是理解连续性的关键。 对于一个形状为 (C, H, W) 的连续张量，其 stride 通常是 (H*W, W, 1)。\",\"在 C 维度上移动 1 位，需要在内存中跳过 H*W 个元素。\",\"在 H 维度上移动 1 位，需要在内存中跳过 W 个元素。\",\"在 W 维度上移动 1 位，只需要移动到下一个元素（1 个）。\",\"判断连续性的条件：当张量的 stride 与其 size 满足特定关系时（即 stride[i] == stride[i+1] * size[i+1]），该张量才是连续的。\"]},\"490\":{\"h\":\"tensor.is_contiguous()\",\"t\":[\"作用：判断当前张量的内存布局是否是连续的。\",\"返回值：一个布尔值（True 或 False）。\",\"特点：这是一个轻量级的检查操作，只检查元数据（stride, size），不复制任何数据。\",\"示例：\",\"import torch # 创建一个连续张量 x = torch.randn(2, 3, 4) print(x.is_contiguous()) # 输出: True print(x.stride()) # 输出: (12, 4, 1) -> (3*4, 4, 1) # 创建一个不连续张量的常见操作：转置（Transpose） y = x.transpose(0, 2) # 将维度0和维度2交换 print(y.shape) # 输出: torch.Size([4, 3, 2]) print(y.stride()) # 输出: (1, 4, 12) -> 与连续时的步长规则不符 print(y.is_contiguous()) # 输出: False\",\"像 transpose(), permute(), narrow(), expand(), t() 等操作通常会产生不连续的张量，因为它们只改变了视图（View），而没有实际重新排列内存中的数据。\"]},\"491\":{\"h\":\"tensor.contiguous()\",\"t\":[\"作用：返回一个内存连续的、数据内容相同的张量。\",\"返回值：一个新的张量。\",\"特点：\",\"如果原张量已经是连续的，则 contiguous()不会进行任何复制操作，直接返回原张量本身（self）。\",\"如果原张量不是连续的，则 contiguous()会分配一块新的连续内存，并将原张量的数据按照其逻辑顺序复制到这块新内存中。\",\"示例：\",\"import torch x = torch.randn(2, 3) print(f\\\"x is contiguous: {x.is_contiguous()}\\\") # True # 创建一个不连续的视图 y = x.t() # 转置操作 print(f\\\"y is contiguous: {y.is_contiguous()}\\\") # False # 对不连续的 y 调用 contiguous() z = y.contiguous() print(f\\\"z is contiguous: {z.is_contiguous()}\\\") # True # 验证内存地址和数据 print(f\\\"y data ptr: {y.storage().data_ptr()}\\\") # 与 x 相同 print(f\\\"z data ptr: {z.storage().data_ptr()}\\\") # 与 x/y 不同，是新分配的 # 验证数据内容是否一致 print(torch.all(y == z)) # 输出: True，数据值相同\"]},\"492\":{\"h\":\"为什么需要 ？\",\"t\":[\"许多 PyTorch 操作（尤其是底层由 CUDA/C++ 实现的操作）要求输入张量必须是内存连续的，否则会报错或得到错误的结果。最常见的场景包括：\",\"视图操作（View）：tensor.view()要求张量是连续的。\",\"y = x.t() # z = y.view(-1) # 这里会报错：RuntimeError: view size is not compatible with input tensor's size and stride... z = y.contiguous().view(-1) # 正确做法：先连续化，再改变视图\",\".data_ptr() 访问：如果你想获得底层数据存储区的指针，需要确保它是连续的。\",\"与外部库交互：例如将 PyTorch 张量转换为 NumPy 数组（tensor.numpy()）或传递给其他 C++ 扩展时，通常需要连续的内存布局。\",\"某些性能关键的操作：连续的内存访问模式对 CPU/GPU 缓存更友好，有时能提升计算效率。\"]},\"493\":{\"h\":\"归一化层对连续性的要求\",\"t\":[\"🔴 必须传入连续张量\",\"BatchNorm系列 (nn.BatchNorm1d/2d/3d)\",\"原因：底层CUDA实现严格依赖连续内存布局进行跨批次统计计算\",\"风险：直接传入不连续张量极高概率导致运行时错误或计算结果错误\",\"🟡 强烈建议传入连续张量\",\"LayerNorm (nn.LayerNorm)\",\"原因：虽然某些实现能处理不连续输入，但为保障跨平台一致性和最佳性能\",\"建议：总是使用 .contiguous() 确保稳定性和计算效率\",\"🟡 强烈建议传入连续张量\",\"InstanceNorm系列 (nn.InstanceNorm1d/2d/3d)\",\"原因：通道级别的统计计算同样受益于连续内存访问模式\",\"建议：预处理中确保张量连续性以避免潜在问题\",\"🟡 强烈建议传入连续张量\",\"GroupNorm (nn.GroupNorm)\",\"原因：分组统计计算需要高效的内存访问模式\",\"建议：保持连续性以获得最佳性能和正确性\",\"只有BatchNorm是\\\"必须\\\"的，其他都是\\\"强烈建议\\\"。但统一的最佳实践是：在所有归一化操作前都调用 .contiguous()，用微小的开销换取代码的健壮性和可维护性。\"]},\"494\":{\"h\":\"总结\",\"t\":[\"方法\",\"作用\",\"数据复制行为\",\"tensor.is_contiguous()\",\"检查张量内存是否连续\",\"绝不复制数据\",\"tensor.contiguous()\",\"确保返回一个连续的张量\",\"条件性复制（仅在原张量不连续时复制）\",\"最佳实践：当你对一个张量进行了 transpose, permute 等可能改变内存布局的操作后，如果后续需要用到 view 或者要将其传入某些特定函数，安全起见，先调用 .contiguous()。虽然有时不调用也能工作，但显式地调用可以避免难以调试的运行时错误。\"]},\"495\":{\"h\":\"API记录之杂类篇\",\"t\":[\"API记录之杂类篇\"]},\"496\":{\"h\":\"模型\"},\"497\":{\"h\":\"ResNet18\",\"t\":[\"ResNet18是一种深度残差网络，它由18层组成。它的结构包括一个输入层、四个残差块和一个输出层。每个残差块包含两个3x3的卷积层，每个卷积层后面都跟着一个Batch Normalization和ReLU激活函数。此外，每个残差块还包含一条跨层的连接线，将输入直接连接到输出。这种设计使得网络能够更好地处理深层特征，并且可以避免梯度消失问题。ResNet18在图像分类任务中表现出色，可以用于训练大型数据集，如ImageNet。\"]},\"498\":{\"h\":\"Bert\",\"t\":[\"pooler_output 的输出用于捕获整个句子的全局语义信息:\"]},\"499\":{\"h\":\"公式&定理\"},\"500\":{\"h\":\"通用近似定理\",\"t\":[\"以下内容来自: << 神经网络与深度学习 >> 4.3.1 通用近似定理\",\"根据通用近似定理，对于具有线性输出层和至少一个使用“挤压”性质的激活函数的隐藏层组成的前馈神经网络，只要其隐藏层神经元的数量足够，它可以以任意的精度来近似任何一个定义在实数空间中的有界闭集函数．所谓“挤压”性质的函数是指像Sigmoid函数的有界函数，但神经网络的通用近似性质也被证明对于其他类型的激活函数，比如ReLU，也都是适用的．\",\"个人对上述内容的理解\",\"通用近似定理中“隐藏层神经元的数量足够”这一条件，与多项式逼近（如泰勒展开）中 '增加阶数提高精度' 的思想有深刻的相似性，但神经网络的非线性基函数组合比传统多项式逼近更灵活。以下是具体分析：\",\"逼近方式\",\"多项式逼近（泰勒展开）\",\"神经网络逼近\",\"基函数\",\"单项式基 \",\"非线性激活后的基 \",\"组合方式\",\"线性加权和 \",\"线性加权和 \",\"逼近原理\",\"增加阶数 提高精度\",\"增加神经元数量 提高精度\",\"函数空间\",\"多项式函数空间\",\"自适应生成的非线性函数空间\",\"关键共同点：\",\"两者都通过增加基函数的数量（多项式阶数/神经元数量）来扩大逼近空间的容量，从而提升对目标函数的拟合精度。\",\"神经网络的独特优势:\",\"自适应基函数\",\"多项式逼近的基函数是固定的（如 ），而神经网络的基函数 的形状和位置（由权重 决定）可通过训练动态调整，更灵活适应目标函数。\",\"示例：拟合分段函数时，ReLU神经元可自动学习“转折点”，而多项式需极高阶数才能近似突变。\",\"维度诅咒的缓解\",\"在高维空间（）中，多项式逼近需要 项（指数增长），而神经网络通过非线性激活和分层结构，可能以 神经元实现相同精度。\",\"对非平滑函数的适应性\",\"泰勒展开要求函数无限可微，而神经网络（如使用ReLU）可逼近连续但不可微的函数（如 ）。\",\"案例：逼近区间 上的 \",\"多项式逼近: 需高阶泰勒展开 ，且高次项易导致震荡（龙格现象）。\",\"神经网络逼近: 仅需4个Tanh神经元即可高精度拟合，因基函数 能自适应频率和相位。\",\"理论限制的相似性:\",\"逼近精度与代价的权衡\",\"多项式：高阶项导致数值不稳定（如大数相减损失精度）。\",\"神经网络：神经元过多易过拟合，且训练难度增加（梯度消失/爆炸）。\",\"全局逼近 vs 局部逼近\",\"多项式：调整某一系数会影响全局拟合。\",\"神经网络：可通过局部神经元（如ReLU）实现分段逼近，更适应局部特征。\",\"现代深度学习的延伸: 深层神经网络通过函数复合（Function Composition）能够以指数级减少所需的神经元数量，核心原因在于层次化的函数构造方式比单层网络的线性组合更高效。这与多项式逼近等传统方法有本质区别，具体可以从以下几个方面理解：\",\"1. 函数复合 vs. 线性组合：数学本质对比\",\"单层网络（线性组合）：\",\"单隐藏层神经网络的输出形式为：\",\"它通过一组非线性基函数（(\\\\sigma)）的加权和逼近目标函数，类似于多项式逼近中的基函数组合。要逼近复杂函数，可能需要大量神经元（(N) 极大）。\",\"深层网络（函数复合）：\",\" 层网络的输出是多次复合的结果：\",\"每一层 都是一个非线性变换（如 ）。通过逐层抽象，深层网络可以逐步构造出更复杂的函数。\",\"关键区别：\",\"单层网络依赖基函数的数量（宽度）来增加表达能力。\",\"深层网络依赖函数的嵌套深度，通过分层组合简单函数，实现复杂功能。\",\"2. 为什么函数复合更高效？\",\"(1) 分治策略（Divide-and-Conquer）\",\"深层网络将复杂函数分解为多个简单步骤，每一层只需学习局部特征，最后组合成全局解。例如：\",\"目标函数：拟合一个“锯齿波”\",\"单层网络：需要大量神经元构造多个“转折点”。\",\"深层网络：每层学习一个转折点，通过复合实现指数级增长的分段线性区域（如 层ReLU网络可生成 个分段）。\",\"(2) 指数级表达能力\",\"理论结果：\",\"Telgarsky (2016) 证明：用深度 的ReLU网络可以构造具有 个线性区域的函数，而单层网络需要 个神经元才能达到相同效果。\",\"直观理解：每一层的非线性变换（如ReLU）相当于对输入空间进行一次“折叠”，深度叠加导致表达能力爆炸式增长。\",\"网络类型\",\"所需神经元/层数\",\"表达能力增长方式\",\"单层宽网络\",\" 神经元\",\"线性增长（基函数叠加）\",\"深层网络\",\" 层，每层 神经元\",\"指数增长（函数复合）\",\"(3) 参数复用与模块化\",\"深层网络通过共享参数（如卷积核）和模块化设计（如残差块），进一步减少冗余：\",\"示例：CNN中，同一卷积核在不同位置重复使用，避免为每个像素单独建模。\",\"3. 与多项式逼近的对比\",\"多项式逼近通过增加阶数（如泰勒展开）提升精度，但存在两大局限：\",\"全局性：调整某一系数会影响整个函数，难以局部修正。\",\"维度灾难：高维输入时，多项式项数 爆炸式增长。\",\"而神经网络的函数复合：\",\"局部性：每层聚焦不同抽象层次（如边缘→纹理→物体）。\",\"维度友好：通过分层降维（如池化）逐步压缩信息。\",\"4. 实例说明\",\"案例1：逼近“多次折叠”的函数\",\"目标函数：\",\"单层网络：需数百个神经元拟合嵌套正弦波。\",\"深层网络：3层即可，每层对应一个 操作。\",\"案例2：图像分类\",\"单层网络：需直接建模像素到类别的复杂映射，参数量极大。\",\"深层CNN：逐层提取边缘→纹理→部件→物体，参数量更少。\",\"5. 理论支持\",\"深度分离定理（Depth Separation Theorem）: 存在某些函数，用浅层网络逼近需要指数级神经元，而深层网络只需多项式数量（如 Eldan & Shamir, 2016）。\",\"电路理论类比: 深层网络类似布尔电路中的分层设计（如AND-OR门组合），比单层电路更高效。\",\"6. 深层网络的代价\",\"虽然深度减少了神经元数量，但带来了：\",\"优化难度：梯度消失/爆炸问题。\",\"过拟合风险：需正则化（如Dropout）。\",\"计算开销：并行化要求更高。\",\"总结:\",\"神经网络通过非线性激活函数生成的动态基函数组合，实现了比多项式逼近更高效的函数近似。虽然“增加神经元数量”与“提高多项式阶数”在思想上都体现了用更多自由度提升精度，但神经网络的自适应基函数和分层结构使其：\",\"对高维和非平滑函数更鲁棒\",\"避免了手工设计基函数的局限性\",\"在实践中通过梯度下降自动学习逼近策略\"]},\"501\":{\"h\":\"ROI Pooling\",\"t\":[\"在目标检测任务中，比如 Faster R-CNN，我们会从一张图片中生成多个候选区域（ROI），这些区域的大小各不相同。而神经网络的全连接层只能接受固定大小的输入，这就产生了一个问题：\",\"如何将不同尺寸的ROI特征，统一变为相同尺寸？\",\"ROI Pooling 的目标就是: 从不同大小的 ROI 区域中提取固定大小的特征（例如 7×7），同时保留最有代表性的空间信息。\",\"ROI Pooling 的操作流程可以分为三个步骤:\",\"映射 ROI 到特征图空间\",\"假设输入图像经过卷积得到一个特征图（例如从 ResNet 输出的特征图），而我们检测到一个 ROI（例如在原图上坐标为 ）。\",\"由于特征图的尺寸比原图小（通常是原图的 1/16），我们需要先将 ROI 坐标 映射到特征图上：\",\"其中 stride 是特征图相对于原图的缩放比例。\",\"将该 ROI 划分成固定数量的网格区域\",\"例如我们希望将每个 ROI 转换成 7×7 的特征图，那么就把该 ROI 分成 7 行 × 7 列的 小块（每一小块大小不同，但数目固定）。\",\"每个小块做 max pooling\",\"对每个小块区域做 最大池化（Max Pooling），取出该区域内的最大值，这样就将原本不定尺寸的 ROI 转换成一个固定大小的特征图（例如 7×7）。\",\"假设某个 ROI 映射到特征图上之后是一个大小为 14×14 的区域，我们希望输出一个 7×7 的固定大小特征图。\",\"将 14×14 区域划分为 7×7 的网格（每个网格是 2×2 大小）\",\"对每个 2×2 的小格子做最大池化 → 输出一个 7×7 特征图\",\"⚠️ ROI Pooling 有一个问题：量化误差。\",\"ROI Pooling 的划分方式中涉及到了取整（floor/ceil），这在某些场景下会导致位置偏差、信息丢失。\",\"为了更精确，Mask R-CNN 提出了更先进的方法：ROI Align，它使用双线性插值来避免量化误差，使得检测/分割性能更好。\"]},\"502\":{\"h\":\"ROI Align\",\"t\":[\"ROIAlign 是 Mask R-CNN 中为了解决 RoIPooling 引起的对齐误差问题而提出的关键组件。\",\"RoIPool（Region of Interest Pooling） 是 Faster R-CNN 中的标准组件，用于将任意大小的候选框（RoI）转换为固定大小（例如 7×7）的特征图，以便送入全连接层进行分类和回归。\",\"问题： RoIPool 在处理浮点型的 RoI 坐标时进行了两次量化（quantization）操作：\",\"RoI 边界坐标的量化（例如将 x/16 向下取整）；\",\"池化 bin 分割时的量化（每个 bin 的边界坐标再取整）。\",\"这会导致特征图上的空间对齐误差（misalignment），尤其对 像素级别任务如分割 影响显著。\",\"RoIAlign目标：消除量化误差，实现精确的像素级对齐。\",\"实现步骤如下：\",\"不进行任何量化\",\"保留浮点型的 RoI 坐标值（例如 x/16 而不是 [x/16]），也不对 bin 边界进行离散化。\",\"对每个 bin 采样多个点（如 2×2）\",\"将 RoI 分成固定数量的 bin（例如 7×7）。\",\"每个 bin 中选定若干个浮点坐标点（通常为4个采样点，中心或等距分布）。\",\"+----------+ | * * | | | | | | * * | +----------+\",\"每个 * 就是一个采样点，它们分布在 4 个角的中间位置，平均对称。\",\"使用双线性插值（Bilinear Interpolation）提取特征值\",\"由于坐标是浮点数，不对应实际的 feature map 网格点，因此使用四邻域双线性插值从特征图中获取精确的 feature 值。\",\"你要在 (3.6, 5.2) 点上取值： - 它离 (3,5) 的距离是 (1 - 0.6) * (1 - 0.2) = 0.4 * 0.8 = 0.32 - 它离 (4,5) 的距离是 0.6 * 0.8 = 0.48 - 它离 (3,6) 的距离是 0.4 * 0.2 = 0.08 - 它离 (4,6) 的距离是 0.6 * 0.2 = 0.12 于是你把这 4 个点的值按这个比例加起来，就得到了 (3.6, 5.2) 的值。 就像你在地图上两个村庄中间估算温度时，不会只看一个村，而是综合周围村子的情况加权得出。\",\"每个撒下去的小数点都用周围的4个整数点去“平均估计”（双线性插值）;\",\"对采样点的值进行聚合\",\"可以采用 max 或 average（论文推荐 average）。\",\"每个 bin 的最终输出为这些采样点值的聚合结果。\",\"图示（见论文 Figure 3）：\",\"实线为 RoI，虚线为 feature map 网格，黑点为采样点，通过插值获得值后聚合。\",\"RoIAlign 就是：\",\"先把目标区域平均切成小格子（比如 7×7）；\",\"在每个小格子里撒几个点（比如 2×2）；\",\"每个撒下去的小数点都用周围的4个整数点去“平均估计”（双线性插值）；\",\"最后把所有点的值求平均，就得到了这个格子的特征。\"]},\"503\":{\"h\":\"上采样\",\"t\":[\"torch.nn.functional.interpolate( input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None )\",\"参数\",\"说明\",\"input\",\"输入张量，形状 [N, C, H, W]（2D）或 [N, C, D, H, W]（3D）\",\"size\",\"输出的目标尺寸 (H_out, W_out)，与 scale_factor 互斥\",\"scale_factor\",\"缩放因子 (fH, fW)，用于等比例放大或缩小\",\"mode\",\"插值方式： 'nearest', 'bilinear', 'bicubic', 'trilinear', 'area'\",\"align_corners\",\"对双线性或双三次插值是否对齐角点，默认 None\",\"recompute_scale_factor\",\"是否重新计算缩放因子，默认 None\"]},\"504\":{\"h\":\"最近邻 (nearest) 插值\",\"t\":[\"最近邻插值是最简单的一种上采样方法：\",\"对于要插值的每个输出像素点，找到它在输入中的 最近的一个像素，直接把这个像素的值赋给输出像素。\",\"不会对像素值进行加权平均或平滑处理，所以原本的格子边界保持得很清晰。\",\"适合用于 离散格子 或 掩码、标签图，因为不会引入新的中间值。\",\"公式描述：\",\"假设输入特征图大小为 (H_in, W_in)，输出大小为 (H_out, W_out)：\",\"其中 round 表示取最接近的整数索引。\",\"一个简单的例子如下所示:\",\"输入 X: [[10, 20], [30, 40]] --放大两倍--> 输出 Y: [[10, 10, 20, 20], [10, 10, 20, 20], [30, 30, 40, 40], [30, 30, 40, 40]]\"]},\"505\":{\"h\":\"双线性 (bilinear) 插值\",\"t\":[\"双线性插值是一种常用的图像放大/缩小方法，它的核心思想是 对输出像素使用周围四个输入像素的加权平均，从而获得平滑的过渡效果。\",\"输入：一幅图像 H_in × W_in\",\"输出：目标大小 H_out × W_out\",\"对输出图像中的每个像素，找到它在输入图像对应的浮点坐标 (x, y)，并使用其周围的 2×2 像素进行插值。\",\"假设输出坐标映射到输入坐标 (x, y)，周围四个像素为：\",\"(x0, y0) ---- (x1, y0) | | | | (x0, y1) ---- (x1, y1)\",\"对应的像素值分别为 Q11, Q21, Q12, Q22\",\"插值公式：\",\"直观理解：x方向和y方向的线性插值的叠加 → 因此叫“双线性”\",\"特点\",\"说明\",\"平滑\",\"输出图像像素值是周围四个输入像素加权平均，边界过渡自然\",\"精度高于最近邻\",\"不会产生块状感，适合图像缩放\",\"适合连续值\",\"对热力图、注意力图、彩色图像非常友好\",\"较慢\",\"需要计算权重和加权平均，比最近邻计算量大\"]},\"506\":{\"h\":\"余弦相似度\",\"t\":[\"余弦相似度（Cosine Similarity）是一种常用的衡量两个向量相似度的方法，特别常用于文本向量、推荐系统、信息检索等场景。\",\"公式如下：\",\"其中：\",\" ：两个向量（维度相同）。\",\" ：向量的点积。\",\" ：向量 的欧几里得范数（长度）。\",\"。\",\" ：向量 和 之间的夹角。\",\"取值范围：\",\"，其中：\",\"越接近 1 表示两个向量方向越接近（相似度高）。\",\"越接近 0 表示两个向量几乎正交（不相似）。\",\"越接近 -1 表示两个向量方向相反。\"]},\"507\":{\"h\":\"L1 归一化（L1 Normalization）\",\"t\":[\"定义：把向量中每个元素除以向量元素的绝对值之和，使得归一化后的向量的 L1 范数 = 1。\",\"公式：\",\"特点：\",\"保留向量的方向信息，但缩放到总和为 1。\",\"常用于特征和概率分布处理，比如文本的 TF-IDF 向量。\"]},\"508\":{\"h\":\"L2 归一化（L2 Normalization）\",\"t\":[\"定义：把向量中每个元素除以向量的欧几里得长度，使得归一化后的向量的 L2 范数 = 1。\",\"公式：\",\"特点：\",\"保留向量方向，但长度固定为 1。\",\"在计算余弦相似度、梯度下降等场景中非常常用。\",\"L1 归一化 → 保证绝对值之和 = 1（稀疏向量处理好）。\",\"L2 归一化 → 保证长度 = 1（方向为主，长度固定）。\"]},\"509\":{\"h\":\"API记录之框架篇\",\"t\":[\"API记录之框架篇\"]},\"510\":{\"h\":\"timm 库\",\"t\":[\"timm 是 PyTorch Image Models 的缩写，是 Ross Wightman 开发和维护的一个 PyTorch 视觉模型库，在计算机视觉领域非常常用。它在科研与工业界都很受欢迎，因为它集合了大量常见与前沿的图像模型，同时提供了高质量的实现和训练权重。\",\"特点:\",\"模型丰富\",\"收录了数百种视觉模型，包括：\",\"经典模型：ResNet、DenseNet、EfficientNet、MobileNet\",\"Transformer 系列：ViT、DeiT、Swin Transformer、ConvNeXt\",\"最新论文模型：EVA、ConvNeXt V2、MaxViT 等\",\"你几乎可以把它当成 视觉模型的“模型仓库”。\",\"预训练权重\",\"提供了大量在 ImageNet-1k / ImageNet-21k 上训练好的权重，开箱即用。\",\"可以直接加载预训练模型用于 迁移学习 / finetune。\",\"统一接口\",\"使用简单，几乎所有模型都能通过同样的方式调用：\",\"import timm model = timm.create_model('resnet50', pretrained=True) x = torch.randn(1, 3, 224, 224) y = model(x)\",\"API 统一，降低了不同架构之间的切换成本。\",\"实用工具\",\"timm.data：包含数据增强（RandAugment、Mixup、CutMix 等）。\",\"timm.optim：包含优化器（AdamP、RAdam、Lookahead 等）。\",\"timm.scheduler：学习率调度器（CosineAnnealing、OneCycle、TanhDecay 等）。\",\"timm.loss：封装了多种损失函数（Label Smoothing、SoftTarget CrossEntropy 等）。\",\"这些设计让训练流程非常完整。\",\"高效实现\",\"很多模型在 timm 里做了 速度和显存优化，常常比官方实现更高效。\",\"支持混合精度训练、channels-last 等特性。\"]},\"511\":{\"h\":\"create_model 与 @register_model 装饰器\",\"t\":[\"create_model：timm 提供的统一入口，用于按名字实例化模型。\",\"model = timm.create_model('resnet50', pretrained=True)\",\"@register_model：用于将自定义模型注册到 timm 模型库，才能通过 create_model 调用。\",\"@register_model def my_model(pretrained=False, **kwargs): return MyModel(**kwargs)\",\"前者是用模型，后者是加模型。\"]},\"512\":{\"h\":\"scikit-learn 库\"},\"513\":{\"h\":\"train_test_split\",\"t\":[\"train_test_split（来自 sklearn.model_selection）用于把一个或多个并行数组按比例切分成训练集和测试集，常用于机器学习的数据准备。\",\"def train_test_split( *arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None, )\",\"*arrays：一个或多个数组（如 X, y），长度必须相同。返回值是按输入顺序交错的切分结果：X_train, X_test, y_train, y_test, ...。\",\"test_size：float（0~1，表示比例）或 int（样本数）或 None。若都为 None，默认 test_size=0.25。\",\"train_size：同 test_size，可用来显式指定训练集大小（优先级低于 test_size）。\",\"random_state：整数或 RandomState，用于可重复的随机化（只在 shuffle=True 时生效）。\",\"shuffle：是否先打乱样本（默认 True）。设为 False 时按原序切分。\",\"stratify：用于分层采样的标签数组（与输入长度相同），保证切分后各类比例与原始数据一致；如果提供了 stratify，必须 shuffle=True。\",\"简单例子:\",\"from sklearn.model_selection import train_test_split import numpy as np X = np.arange(10).reshape(10,1) # 10 个样本特征 y = np.array([0,0,0,0,1,1,1,1,1,1]) # 不平衡标签 X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42, stratify=y ) print(X_train.shape, X_test.shape) # (7,1) (3,1) print(np.bincount(y_train), np.bincount(y_test)) # 输出会显示训练/测试集中 0/1 类的比例与原始近似一致\",\"注意点:\",\"给多个数组（如 X, y, z）时，返回对应数量的切分结果。\",\"stratify 用于类别任务，能避免切分导致某类在测试集中缺失。\",\"若需固定切分可用 random_state；想保留原序列则 shuffle=False。\"]},\"514\":{\"h\":\"compute_class_weight\",\"t\":[\"compute_class_weight 是 scikit-learn 提供的一个函数，用于根据样本分布计算每个类别的权重，常用于 类别不平衡 的分类任务。\",\"sklearn.utils.class_weight.compute_class_weight( class_weight, classes, y )\",\"class_weight\",\"'balanced'：自动计算权重，和类别频率成反比。\",\"dict：手动指定某些类别的权重，如 {0: 1.0, 1: 5.0}。\",\"None：不计算，所有类别权重为 1。\",\"classes\",\"所有类别的 唯一标签数组（如 [0, 1, 2]）。\",\"y\",\"训练数据的标签数组（如 [0,0,1,2,2,2]）。\",\"返回值:\",\"weights：一维数组，长度与 classes 相同，表示每个类别的权重。\",\"计算公式（balanced 模式）：\",\"其中：\",\"：样本总数\",\"：类别数\",\"：第 j 类的样本数\",\"示例:\",\"from sklearn.utils.class_weight import compute_class_weight import numpy as np y = np.array([0, 0, 1, 2, 2, 2]) # 样本标签 classes = np.unique(y) weights = compute_class_weight('balanced', classes=classes, y=y) print(\\\"类别权重:\\\", dict(zip(classes, weights)))\",\"输出：\",\"类别权重: {0: 1.0, 1: 3.0, 2: 0.67}\",\"说明：\",\"类别 0 有 2 个样本 → 权重较低\",\"类别 1 只有 1 个样本 → 权重最高\",\"类别 2 有 3 个样本 → 权重最低\"]},\"515\":{\"h\":\"python 内置 collections 库\"},\"516\":{\"h\":\"Counter\",\"t\":[\"Counter 是 Python 内置库 collections 提供的一个计数器类，用于统计可迭代对象中各元素出现的次数。\",\"from collections import Counter Counter(iterable) # 输入一个可迭代对象 Counter(mapping) # 输入一个字典 Counter(a=2, b=3, ...) # 输入关键字参数\",\"返回的是一个字典的子类，键为元素，值为出现次数。\",\"查询一个未出现的元素时，计数为 0。\",\"支持常见的字典操作，还扩展了计数相关方法。\",\"常用方法:\",\"most_common(n)：返回出现次数最多的前 n 个元素及其频数。\",\"elements()：按出现次数依次返回元素（迭代器）。\",\"update(iterable)：更新计数。\",\"subtract(iterable)：减少计数。\",\"例子:\",\"from collections import Counter all_labels = [0, 1, 0, 2, 1, 0, 2, 2, 2] label_counts = Counter(all_labels) print(label_counts) # Counter({2: 4, 0: 3, 1: 2}) print(label_counts[2]) # 4 print(label_counts.most_common(1)) # [(2, 4)] print(list(label_counts.elements())) # [0, 0, 0, 1, 1, 2, 2, 2, 2]\"]},\"517\":{\"h\":\"pytorch 内置 采样库\"},\"518\":{\"h\":\"WeightedRandomSampler\",\"t\":[\"WeightedRandomSampler 是 PyTorch 提供的一个采样器，用于在构建 DataLoader 时 按权重采样样本，常用于类别不平衡的数据集。\",\"torch.utils.data.WeightedRandomSampler( weights, num_samples, replacement=True )\",\"weights\",\"一维数组/列表，长度等于样本数。\",\"每个元素表示对应样本被采样的概率权重。\",\"权重越大，被抽到的概率越高。\",\"num_samples\",\"采样的样本数（即每个 epoch 中从数据集中抽多少个样本）。\",\"通常设为 len(dataset) 或 len(train_labels)。\",\"replacement\",\"是否有放回采样：\",\"True：可以重复采样同一样本。\",\"False：无放回采样（但这时 num_samples 不能超过数据集大小）。\",\"举例:\",\"train_label_counts = Counter(train_labels) # 计算每个样本的权重：类别样本越少，权重越高 train_sample_weights = [1.0 / train_label_counts[label] for label in train_labels] # 构建加权随机采样器 train_sampler = WeightedRandomSampler( weights=train_sample_weights, num_samples=len(train_labels), # 每个epoch采样样本数=总样本数 replacement=True # 允许重复采样 )\",\"思路：类别数量少 → 权重大 → 更容易被采到。\",\"目的：让每个类别在训练过程中被抽到的机会接近均衡，从而缓解类别不平衡问题。\"]},\"519\":{\"h\":\"API记录之训练细节篇\",\"t\":[\"API记录之训练细节篇\"]},\"520\":{\"h\":\"os.environ[\\\"CUDA_VISIBLE_DEVICES\\\"]\",\"t\":[\"作用：限制进程可见的 GPU，隐藏未列出的设备。\",\"继承性：主进程设置后，所有子进程会继承同样的可见 GPU 配置。\",\"效果：\",\"torch.cuda.device_count() 返回的是“可见 GPU 数”，而非物理 GPU 总数。\",\"子进程无法访问未分配的 GPU。\",\"注意：子进程里修改只对该子进程生效，不会影响主进程或其他进程。\",\"设置方式: 你可以在运行程序前用命令行设置\",\"CUDA_VISIBLE_DEVICES=0,1 python train.py\",\"也可以在代码里设置：\",\"import os os.environ[\\\"CUDA_VISIBLE_DEVICES\\\"] = \\\"0,1\\\"\",\"一句话总结：CUDA_VISIBLE_DEVICES 用来指定程序和其子进程能看到哪些 GPU，相当于给进程建立一个“GPU 白名单”。\"]},\"521\":{\"h\":\"随机数种子与确定性\",\"t\":[\"作用：\",\"随机数种子（random seed）用于固定伪随机数生成器的初始状态。\",\"同样的种子保证每次生成的随机数序列完全相同，从而实现实验可重复性。\",\"随机数本质上仍是伪随机，只是可预测，不是一个常数。\",\"深度学习中通常固定的随机源：\",\"Python 内置 random 模块（数据增强等）\",\"NumPy 随机数生成器（数据预处理、初始化等）\",\"PyTorch CPU 随机数生成器（权重初始化、dropout 等）\",\"PyTorch GPU 随机数生成器（CUDA 上的操作，如 dropout、卷积初始化等）\",\"cuDNN 的随机算法（cudnn.deterministic=True）保证卷积等操作的可重复性\",\"常用设置如下:\",\"设置 Python 内置随机数种子：\",\"random.seed(args.manual_seed)\",\"控制 Python random 模块产生的随机数序列，确保每次运行生成相同的随机数。\",\"设置 NumPy 随机数种子：\",\"np.random.seed(args.manual_seed)\",\"控制 NumPy 的随机数生成，比如初始化权重、打乱数据顺序等。\",\"设置 PyTorch CPU 随机数种子：\",\"torch.manual_seed(args.manual_seed)\",\"控制 PyTorch 在 CPU 上的随机操作，例如权重初始化、dropout 等。\",\"设置 PyTorch CUDA 随机数种子：\",\"if torch.cuda.is_available(): torch.cuda.manual_seed(args.manual_seed) torch.cuda.manual_seed_all(args.manual_seed)\",\"manual_seed 只设置当前 GPU 的随机种子，manual_seed_all 则设置所有可见 GPU，保证多 GPU 情况下结果一致。\",\"控制 cuDNN 行为：\",\"cudnn.benchmark = False cudnn.deterministic = True\",\"cudnn.benchmark = False：禁止 cuDNN 自动寻找最优卷积算法（这个算法搜索过程会引入随机性）。\",\"cudnn.deterministic = True：强制使用确定性的卷积算法，保证每次运行结果一致。\"]},\"522\":{\"h\":\"偏置与归一化层的关系\",\"t\":[\"BatchNorm 会重新中心化数据\",\"# BatchNorm 的数学公式： y = (x - mean) / sqrt(var + eps) * gamma + beta # 其中： # - gamma: 缩放参数（可学习） # - beta: 偏置参数（可学习） # - mean, var: 批次的均值和方差\",\"Linear 层也有偏置\",\"# Linear 层的计算： z = x @ W.T + b # b是偏置项\",\"为什么此时 Linear 层的偏置是冗余的 ？\",\"输入 → Linear → BatchNorm 的计算链： output = BN(Linear(x)) = BN(x @ W.T + b) = [(x @ W.T + b - mean) / std] * gamma + beta = [x @ W.T / std + (b - mean)/std] * gamma + beta = (x @ W.T) * (gamma/std) + [gamma*(b-mean)/std + beta]\",\"可以看到:\",\"(x @ W.T) * (gamma/std)：有效的权重变换\",\"[gamma*(b-mean)/std + beta]：常数偏置项\",\"组件\",\"作用\",\"是否冗余\",\"Linear.bias\",\"添加常数偏移\",\"✅ 冗余\",\"BatchNorm.beta\",\"添加常数偏移\",\"✅ 唯一需要的\",\"BatchNorm.gamma\",\"缩放特征\",\"❌ 必要\",\"BatchNorm 的均值归一化\",\"中心化数据\",\"❌ 必要\",\"实际代码对比:\",\"错误做法（冗余）：\",\"# 浪费参数和计算 self.linear = nn.Linear(in_dim, out_dim, bias=True) # 有偏置 self.bn = nn.BatchNorm1d(out_dim) # 也有偏置beta\",\"正确做法（优化后）：\",\"# 参数和计算更高效 self.linear = nn.Linear(in_dim, out_dim, bias=False) # 无偏置 self.bn = nn.BatchNorm1d(out_dim) # 用BN的beta作为偏置\",\"如果不使用 BatchNorm，那么应该保留偏置：\",\"# 只有Linear层，没有BN self.linear = nn.Linear(in_dim, out_dim, bias=True) # 需要偏置 # 或者使用LayerNorm等其他归一化 self.linear = nn.Linear(in_dim, out_dim, bias=False) self.norm = nn.LayerNorm(out_dim) # LayerNorm也有偏置参数\",\"最佳实践总结:\",\"场景\",\"建议\",\"原因\",\"Linear + BatchNorm\",\"bias=False\",\"避免冗余，BN的beta足够\",\"只有Linear\",\"bias=True\",\"需要偏置来增加模型表达能力\",\"Linear + LayerNorm\",\"bias=False\",\"LayerNorm也有可学习的偏置\",\"Linear + InstanceNorm\",\"bias=False\",\"InstanceNorm有可学习的参数\",\"深度学习中的标准实践：\",\"CNN中：Conv2d + BatchNorm 时，conv.bias=False\",\"Transformer中：Linear + LayerNorm 时，linear.bias=False\",\"点云网络中：Linear + BatchNorm 时，linear.bias=False\"]},\"523\":{\"h\":\"Attention运算过程中维度变换的理解\",\"t\":[\"Attention运算过程中维度变换的理解\",\"在注意力机制（特别是 Transformer 中的 自注意力机制）中，Q（Query）、K（Key）、V（Value） 的维度对最终注意力输出的结果维度有直接影响。我们来一步步分析这个过程：\"]},\"524\":{\"h\":\"一、注意力机制的基本流程\",\"t\":[\"在标准的 缩放点积注意力（Scaled Dot-Product Attention） 中，计算公式如下：\",\"其中：\",\"：query的数量（如句子长度）\",\"：key/value的数量（也通常是句子长度）\",\"：每个 query 和 key 的维度\",\"：每个 value 的维度\"]},\"525\":{\"h\":\"二、Q、K、V 的初始维度对结果的影响\"},\"526\":{\"h\":\"1.\",\"t\":[\"这是注意力权重矩阵的来源。\",\"所以\",\"👉 这个矩阵表示的是每个 query 对应所有 key 的相似度（即注意力得分），共个值。\"]},\"527\":{\"h\":\"2.\",\"t\":[\"对每一行做 softmax，得到归一化的注意力权重：\",\"输入：\",\"输出：仍是\"]},\"528\":{\"h\":\"3.\",\"t\":[\"注意力权重：\",\"Value 矩阵：\",\"结果：\",\"👉 最终输出的维度是，也就是和输入的 query 数量一致，但每个输出向量的维度由 value 的维度决定。\"]},\"529\":{\"h\":\"三、总结：输入维度 → 输出维度\",\"t\":[\"输入\",\"维度\",\"含义\",\"Query (Q)\",\"查询向量，n 是序列长度\",\"Key (K)\",\"键向量，用于匹配查询\",\"Value (V)\",\"值向量，实际携带信息\",\"输出\",\"维度\",\"含义\",\"Attention Output\",\"每个 query 聚合了所有 value 的加权信息\"]},\"530\":{\"h\":\"四、如何理解这个过程？\",\"t\":[\"我们可以从以下角度理解：\"]},\"531\":{\"h\":\"✅ 1.\",\"t\":[\"每个 Query 都是在寻找最相关的 Key。\",\"根据相关性（注意力权重），从对应的 Value 中提取信息。\",\"最终每个 Query 得到一个融合了上下文信息的向量。\"]},\"532\":{\"h\":\"✅ 2.\",\"t\":[\"控制了相似度计算的维度，影响模型容量和梯度稳定性。\",\"决定了输出的信息维度，可以独立于设计。\",\"这种分离的设计让模型更灵活，比如多头注意力中可以分别控制每个 head 的表达能力。\"]},\"533\":{\"h\":\"✅ 3.\",\"t\":[\"类似数据库查询： \",\"Query 是你输入的问题；\",\"Key 是数据库中的索引；\",\"Value 是数据库中的内容；\",\"Attention 就是根据问题找到相关内容并返回。\"]},\"534\":{\"h\":\"五、例子说明（以 Transformer 为例）\",\"t\":[\"假设我们在 Transformer 中：\",\"输入是一个 batch of sequences，shape 为\",\"我们通过线性变换得到： \",\"那么最终输出为：\",\"如果使用多头注意力（Multi-head Attention），我们会拼接多个这样的头，最后再经过一个线性层映射回原始维度。\"]},\"535\":{\"h\":\"六、常见疑问解答\"},\"536\":{\"h\":\"❓Q: 为什么 和 可以不同？\",\"t\":[\"因为它们的作用不同：\",\"是用于计算相似度的维度；\",\"是用于信息表达的维度；\",\"两者解耦可以让模型更灵活地分配资源。\"]},\"537\":{\"h\":\"❓Q: 为什么要除以 ？\",\"t\":[\"防止内积过大导致 softmax 梯度消失。 当较大时，QK^T 的数值会很大，除以可以缓解这个问题。\"]},\"538\":{\"h\":\"七、可视化示意\",\"t\":[\"Q: [n x dk] K: [m x dk] V: [m x dv] ↓ ↓ ↓ Q @ K.T → [n x m] ↓ ↓ ↓ softmax → [n x m] V → [m x dv] ↓__________________________↓ ↓ Output → [n x dv]\"]},\"539\":{\"h\":\"Pytorch张量存储与访问原理\",\"t\":[\"Pytorch张量存储与访问原理\"]},\"540\":{\"h\":\"引言\",\"t\":[\"张量（Tensor）是数学和物理学中用于表示多维数组的一个概念，在机器学习、深度学习等领域也得到了广泛应用，简单来说，张量可以被视为一种广义的矩阵，它可以拥有任意数量的维度。我们生成一个3*3 的张量,如下所示:\",\"import torch t = torch.tensor([[1,2,3],[4,5,6],[7,8,9]]) print(t, t.shape) output: tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) torch.Size([3, 3])\",\"我们知道，计算机的内存（RAM）本质上是一个线性、一维的存储空间。每一个存储单元（通常是字节，byte）都有一个唯一的地址，这些地址从 0 开始顺序排列，形成一个连续的一维空间。因此不论张量是多少维的，最终都会被映射为一维。在映射机制里，涉及两个重要概念，连续性(contiguity rule)和步长(strides)。\"]},\"541\":{\"h\":\"连续性\",\"t\":[\"连续性: 数据元素在内存空间的排列顺序，主要包括行优先和列优先两类，Pytorch 默认使用行优先方式。\",\"在行优先顺序（Row-major order）中，，内存先存储第 0 行的所有元素，接着是第 1 行的所有元素，依此类推。对于上述代码中的矩阵，其在内存中行优先布局:\",\"1,2,3,4,5,6,7,8,9\",\"在列优先顺序（Column-major order）中，内存先存储第 0 列的所有元素，接着是第 1 列的所有元素，依此类推。对于上述代码中的矩阵，其在内存中列优先布局:\",\"1,4,7,2,5,8,3,6,9\"]},\"542\":{\"h\":\"步长\",\"t\":[\"步长解决的是如何将多维张量中的 (i, j) 或 (i, j, k, …) 索引映射到内存中的地址，比如在内存中从一个索引移动到另一个索引时，沿着某个特定维度需要跨越多少个元素（不一定是字节）。\",\"行优先存储模式的步长，如下例所示:\",\"import torch x = torch.tensor([[1, 2, 3], [4, 5, 6]]) print(\\\"Tensor:\\\") print(x) print(\\\"Strides:\\\", x.stride()) # 输出: (3, 1) # 行优先（Row-major）二维布局 - 形状（Shape）：`[2, 3]`（2 行，3 列） - 典型步长（Strides）：`[3, 1]` 解释： - 第一个维度（“行”维度）的步长是 `3`，意味着如果你从第 `i` 行移动到第 `i+1` 行，你需要在内存中跳过 `3` 个元素。 - 第二个维度（“列”维度）的步长是 `1`，意味着相邻列的元素在内存中也是相邻的。\",\"列优先存储模式的步长，如下例所示:\",\"import numpy as np import torch #创建一个 NumPy 的列优先（column-major）数组 a = np.array([[1, 2, 3], [4, 5, 6]], order='F') # 'F' 表示 Fortran order（列优先） x = torch.from_numpy(a) print(x) print(\\\"Strides:\\\", x.stride()) #输出：(1，2) #列优先（Column-major）二维布局 - 形状（Shape）：`[2, 3]`（2 行，3 列） - 典型步长（Strides）：`[1, 2]` 解释： - 第一个维度（“行”维度）的步长是 `1`，意味着从第 `i` 行移动到第 `i+1` 行只需要在内存中前进一步。 - 第二个维度（“列”维度）的步长是 `2`，意味着从第 `j` 列跳转到第 `j+1` 列时，你需要跳过 `2` 个元素\",\"再来看一个多维张量的例子:\",\"import torch t = torch.arange(0, 24).reshape(1, 2, 3, 4) print(t.stride()) torch.arange(0, 24) 创建了一个一维张量，包含从 0 到 23 的 24 个数字： [ 0, 1, 2, ..., 23] 然后 .reshape(1, 2, 3, 4) 将其重塑为一个 4 D 张量，其形状是： (第0维: batch=1, 第1维: channel=2, 第2维: height=3, 第3维: width=4) 也就是说这个张量可以看作是一个 batch size 为 1、有 2 个通道、每个通道有 3 行 4 列的数据。 tensor([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]]) 执行 print(t.stride())，输出为： (24, 12, 4, 1) 这表示在内存中访问该张量时，每个维度上的“步长”分别是： dim=0 (batch) 24 要跳到下一个 batch 需要移动 24 个元素（但因为 batch=1，实际上没用） dim=1 (channel) 12 要跳到下一个通道需要移动 12 个元素 dim=2 (height/行) 4 要跳到下一行需要移动 4 个元素 dim=3 (width/列) 1 要跳到下一列只需要移动 1 个元素\"]},\"543\":{\"h\":\"张量变换操作\"},\"544\":{\"h\":\"切片(Slice)\",\"t\":[\"切片，指从一个大的数据结构中提取出一部分连续的数据子集，如下所示:\",\"import torch # 创建一个张量 tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) # 获取第二行 print(tensor[1, :]) # 输出: tensor([4, 5, 6]) # 获取第二列 print(tensor[:, 1]) # 输出: tensor([2, 5, 8]) # 获取子张量：前两行、前两列 print(tensor[:2, :2]) # 输出: tensor([[1, 2], # [4, 5]]) # 每隔一行 + 每隔一列 print(tensor[::2, ::2]) # 输出: tensor([[1, 3], # [7, 9]])\",\"切片索引语法: tensor[start:stop:step]\",\"对一个张量（或矩阵）进行“切片”时，实际上并没有复制内存中的任何数据值。相反，这是通过“零拷贝”的方式，调整以下三个要素来创建一个新的视图（view）。\",\"这三个关键要素是：\",\"起始偏移量 (Base offset / Data pointer offset): 表示新视图从原始内存块中的哪个位置开始; 它是一个指向内存中某个元素的指针偏移量。\",\"形状 (Shape): 描述这个新视图的维度大小; 例如：原张量是 3x4，切片后可能是 2x3 或 1x4。\",\"步长 (Strides): 指明在每个维度上移动一个索引单位时，需要跨越多少个内存元素; 通过调整步长，可以实现按行、列或其他任意模式访问数据。\",\"具体的，假设有一个 3×4 的张量（即形状为 [3, 4]），在内存中是行优先存储：\",\"[[A00, A01, A02, A03], [A10, A11, A12, A13], [A20, A21, A22, A23]]\",\"对应的内存布局为一维数组：\",\"[A00, A01, A02, A03, A10, A11, A12, A13, A20, A21, A22, A23]\",\"如果我们做如下切片操作（取第1行到第3行，第1列到第3列）：\",\"sub_tensor = tensor[1:, 1:3]\",\"结果是一个形状为 [2, 2] 的新视图：\",\"[[A11, A12], [A21, A22]]\",\"但此时并没有复制任何数据，而是通过以下方式创建了一个新的“视图”，数据还是指向内存中已有的数据。\",\"属性\",\"值\",\"说明\",\"数据指针偏移\",\"指向 A11 的位置\",\"即内存地址偏移 5（从 A00 开始数）\",\"形状（shape）\",\"[2, 2]\",\"表示两行两列\",\"步长（strides）\",\"[4, 1]\",\"行步长为 4（跳过一行），列步长为 1（逐列）\",\"其下标索引公式可表示为:\"]},\"545\":{\"h\":\"转置(Transpose)\",\"t\":[\"转置,可以理解为对多维张量的各个维度（轴）进行重新排列。\",\"比如，对于一个二维矩阵（形状为 [m, n]），最简单的转置就是将行和列交换，得到一个新的形状为 [n, m] 的矩阵。对于更高维的张量，”转置” 通常意味着对各个轴进行更一般的排列组合。例如，将一个形状为 [D0, D1, D2] 的张量转置为 [D2, D0, D1]。\",\"转置包括逻辑转置和物理转置。\",\"基于步长的逻辑转置（零拷贝）：我们可以通改变对内存中数据缓冲区的解读方式来实现逻辑上的转置，而不是真正地重新排列数据。\",\"比如，在一个简单的二维情况（行优先布局）中：\",\"原始的步长可能是 strides = [n, 1]，对应一个形状为 [m, n] 的数组。\",\"当进行转置时，只需将这些步长交换为 [1, m]，就可以“按转置顺序”来读取数据。\",\"这种方法不需要复制数据，但要注意的是，如果原始数据是按行优先方式存储的，这种转置后的布局在缓存访问上可能效率较低。具体示例如下所示:\",\"import torch # 创建一个张量 t = torch.tensor([[1, 2, 3], [4, 5, 6]]) print(\\\"Original tensor:\\\") print(t) print(\\\"Strides:\\\", t.stride()) # 输出: (3, 1) # 逻辑转置 t_trans = t.t() # 或者使用 t.transpose(0, 1) print(\\\"\\\\nTransposed view:\\\") print(t_trans) print(\\\"Strides after transpose:\\\", t_trans.stride()) # 输出: (1, 3)\",\"物理转置（拷贝）：实际创建一个新的数据缓冲区，并将元素按照转置后的位置写入新内存，使转置后的数据在新的布局中是连续存储的。虽然这种方式需要花费时间和内存来完成拷贝，但它能为后续许多针对转置数据的操作带来更好的内存局部性（memory locality），比如在 TensorRT、ONNX Runtime等加速库中可以更好地利用缓存从而提升性能。\",\"import torch # 创建一个张量 t = torch.tensor([[1, 2, 3], [4, 5, 6]]) # 创建一个连续的转置副本 t_trans_contig = t.t().contiguous() print(\\\"\\\\nPhysically transposed and contiguous tensor:\\\") print(t_trans_contig) print(\\\"Strides after contiguous:\\\", t_trans_contig.stride()) # 输出: (2, 1) print(\\\"Is contiguous?\\\", t_trans_contig.is_contiguous()) # 应为 True\",\"值得注意的是，逻辑转置中，转置后的矩阵步长是(1,3)，而物理转置中，转置后的步长是(2,1)。\",\"这里解释一下，前提条件是行优先存储，在逻辑转置中，转置前\",\"[[1, 2, 3], [4, 5, 6]]\",\"底层数据存储为[1,2,3,4,5,6]，转置后\",\"[[1, 4], [2, 5], [3, 6]]\",\"底层内存中的数据并没有被重新排列！还是：[1, 2, 3, 4, 5, 6]，只是访问方式变了 —— 现在我们按“列优先”的方式去读这个数组，所以正确的strides是(1,3)。\",\"而在物理转置中，由于使用了.contiguous()，内存空间得以连续布局，数据在底层存储为[1, 4, 2, 5, 3, 6]，这个新内存布局使得每一行是连续的，因此步长为 (2,1)。\",\"我们可以把 .contiguous() 理解为：“我不管你现在怎么解读这块内存，我现在要把你的数据重新整理成行优先、连续存储的方式。”\"]},\"546\":{\"h\":\"广播(BroadCast)\",\"t\":[\"广播，是在张量（或数组）运算中非常常见且强大的一种机制，它允许不同形状的张量进行逐元素（element-wise）操作，而不需要显式复制数据。\",\"我们使用了一个示例来说明这一概念：对两个张量 a 和 b 进行相加操作，其中 a.shape = [4, 3]，b.shape = [1, 3]，如下图所示\",\"该图展示了 广播（Broadcast） 在数组运算中的底层实现原理，重点在于 如何通过调整步幅（strides）实现数据虚拟扩展，而非物理复制数据。\",\"数组 a：形状为 4×3，假设为常规行优先存储（如 strides=[3,1]）。\",\"数组 b：原始形状为 1×3，数据内容 [1, 2, 3]，初始步幅 strides=[3,1]。\",\"将 b 从 1×3虚拟扩展为 4×3，使其能与 a 进行逐元素运算（如加法）。将维度大小为 1 的轴扩展至目标大小（此处将行维度从 1 扩展到 4）。\",\"广播通过 设置步幅为 0 实现虚拟扩展：\",\"调整后的 b 属性：\",\"shape = [4, 3]，扩展后的逻辑形状。\",\"data = [1, 2, 3]，物理数据未改变。\",\"strides = [0, 1]，行维度步幅 0：表示在行方向移动时，内存地址不变（复用同一行数据），列维度步幅 1：与原始步幅一致，按元素大小（假设为 1 字节）步进。\",\"扩展后的 b 在逻辑上表现为每行都是 [1, 2, 3]，但物理内存中数据不复制。根据公式\",\"对任意行 i（0 ≤ i <4），代入调整后的步幅 strides=[0, 1]，所有行均指向原始数据的第 j 个元素，实现虚拟复制\"]},\"547\":{\"h\":\"维度问题\",\"t\":[\"3 轴张量，形状：[3, 2, 5] 的多种呈现方式\",\"四阶张量的例子\",\"TensorFlow/PyTorch中使用比较多的tensor的阶为4，shape为[Batch,Height,Weight,Features]\",\"n阶张量的排列规律如下图所示\",\"可以将规律总结为：从shape列表的最右边往左遍历，最开始三个阶按照“下-右-里”的顺序排列，然后打包成一个group，再将整个group按照“下-右-里”的顺序排列，满三次后再打包成一个group，如此往复循环。\",\"本部分图片来源: 张量简介\"]},\"548\":{\"h\":\"杂谈\"},\"549\":{\"h\":\"conda虚拟环境管理\",\"t\":[\"conda虚拟环境管理\"]},\"550\":{\"h\":\"一、创建新环境\",\"t\":[\"基本语法：\",\"conda create --name <环境名> [包名]\",\"可使用 -name（或 n）来命名环境。\",\"示例1：创建一个空环境（只包含 Python）\",\"conda create --name myenv\",\"示例2：创建环境时指定 Python 版本\",\"conda create --name myenv python=3.9\",\"示例3：创建环境并安装一些常用包\",\"conda create --name myenv python=3.8 numpy pandas\"]},\"551\":{\"h\":\"二、激活（切换）环境\",\"t\":[\"激活环境的命令：\",\"conda activate <环境名>\",\"示例：\",\"conda activate lmaffordance3d\",\"激活后，你的终端提示符通常会显示当前环境的名字，例如：\",\"(myenv) user@machine:~$\"]},\"552\":{\"h\":\"三、退出当前环境\",\"t\":[\"要退出当前激活的环境，返回 base 环境：\",\"conda deactivate\"]},\"553\":{\"h\":\"四、查看所有已创建的环境\",\"t\":[\"你可以使用以下命令查看你所有的 conda 环境：\",\"conda env list # 或者 conda info --envs\",\"输出示例：\",\"# conda environments: # base * /home/user/anaconda3 myenv /home/user/anaconda3/envs/myenv testenv /home/user/anaconda3/envs/testenv\",\"注：带星号 * 的表示当前激活的环境。\"]},\"554\":{\"h\":\"五、删除已创建的环境\",\"t\":[\"如果你想删除某个环境，可以使用：\",\"conda env remove -n myenv\",\"如需进一步帮助，可使用：\",\"conda create --help conda activate --help\"]},\"555\":{\"h\":\"六、查看当前激活的环境\",\"t\":[\"查看当前conda激活的环境:\",\"conda info\"]},\"556\":{\"h\":\"七、查看当前环境已安装的包\",\"t\":[\"查看当前环境已安装的包：\",\"conda list\"]},\"557\":{\"h\":\"八、在当前环境下安装包\",\"t\":[\"根据 requirements.txt 安装所需要的依赖包:\",\"conda activate 你的环境名 # 先激活你的conda环境 pip install -r requirements.txt\",\"重要说明：\",\"在激活的 Conda 环境中使用 pip install，包会安装到该环境的 site-packages 中，不会影响其他环境或系统 Python\",\"如果未激活任何环境时使用 pip install，包可能会安装到基础环境或系统 Python 中\",\"建议总是先激活 Conda 环境再使用 pip，以避免安装到错误的位置\",\"可以使用 which pip 或 where pip (Windows) 确认你使用的是 Conda 环境中的 pip\",\"pip install 安装失败的包，尝试使用conda install命令安装即可，再不行尝试源码编译安装(例如某些包在arm64系统上没有预先编译好的版本)。\",\"特性\",\"pip\",\"conda\",\"默认仓库\",\"PyPI（Python Package Index）\",\"Anaconda 官方仓库 / conda-forge\",\"包类型\",\"仅 Python 包（纯 Python 或源码）\",\"预编译的二进制包（含非 Python 依赖）\",\"非 Python 依赖\",\"不管理（如 FFmpeg、HDF5）\",\"自动安装（如 CUDA、MKL）\"]},\"558\":{\"h\":\"九、常见错误\",\"t\":[\"CondaError: Run 'conda init' before 'conda activate’\",\"conda init 如果是 bash： source ~/.bashrc 如果是 zsh： bash conda activate lavis\"]},\"559\":{\"h\":\"常用评估指标\",\"t\":[\"常用评估指标\"]},\"560\":{\"h\":\"二元分类场景\"},\"561\":{\"h\":\"混淆矩阵 (confusion_matrix)\",\"t\":[\"二元分类器的每个输出有四种可能的结果，如果我们将标准答案作为列，将模型的预测作为行，则会得到以下表格（称为混淆矩阵）：\",\"实际正例\",\"实际负例\",\"预测为正例\",\"真正例 (TP)：垃圾邮件被正确分类为垃圾邮件。\",\"假正例 (FP)：非垃圾邮件被误分类为垃圾邮件。\",\"预测为负例\",\"假负例 (FN)：垃圾邮件被误分类为非垃圾邮件。\",\"真负例 (TN)：非垃圾邮件被正确分类为非垃圾邮件。\",\"请注意，每行的总和表示所有预测正例 (TP + FP) 和所有预测负例 (FN + TN)，无论其有效性如何。与此同时，每个列中的总和会显示所有真实正例 (TP + FN) 和所有真实负例 (FP + TN)，而不会考虑模型分类。\",\"如果实际正例的总数与实际负例的总数不接近，则表示数据集不平衡。不平衡数据集的一个示例可能是一组数以千计的云彩照片，其中您感兴趣的罕见云彩类型（例如卷云）只出现了几次。\"]},\"562\":{\"h\":\"准确率 (accuracy)\",\"t\":[\"准确性是指所有分类（无论是正类还是负类）正确分类的比例。其数学定义为：\",\"在垃圾邮件分类示例中，准确率衡量的是所有电子邮件正确分类所占的比例。\",\"完美的模型没有假正例和假负例，因此准确率为 1.0，即 100%。\",\"由于准确率包含混淆矩阵中的所有四种结果（TP、FP、TN、FN），因此在类别数量相近且平衡的数据集的情况下，准确率可以作为衡量模型质量的粗略指标。\",\"不过，如果数据集不平衡，或者一种错误（假负例或假正例）的代价高于另一种错误（大多数实际应用中都是如此），则最好改为针对其他指标进行优化。\",\"对于严重不均衡的数据集（其中一个类别出现的频率非常低，例如 1%），如果模型 100% 都预测为负类别，则其准确性得分为 99%，尽管该模型毫无用处。\"]},\"563\":{\"h\":\"召回率 (recall) / 真正例率\",\"t\":[\"真正例率 (TPR)，即所有实际正例被正确分类为正例的比例，也称为召回率。\",\"在数学上，召回率的定义为：\",\"假负例是指被误分类为负例的实际正例，因此会出现在分母中。在垃圾邮件分类示例中，召回率衡量的是被正确分类为垃圾邮件的垃圾邮件电子邮件的比例。\",\"假设一个完美的模型不会出现假负例，因此其召回率 (TPR) 为 1.0，也就是说，检测率为 100%。\",\"在实际正例数量非常少的不平衡数据集中，召回率比准确率更有意义，因为它衡量的是模型正确识别所有正例实例的能力。对于疾病预测等应用，正确识别阳性病例至关重要。假负例通常比假正例的后果更严重。\"]},\"564\":{\"h\":\"误报概率 / 假正例率\",\"t\":[\"假正例率 (FPR) 是指被错误地归类为正例的所有实际负例所占的比例，也称为误报概率。其数学定义为：\",\"假正例是被错误分类的实际负例，因此会出现在分母中。在垃圾邮件分类示例中，FPR 用于衡量被错误分类为垃圾邮件的合法电子邮件的比例，或模型的误报率。\",\"完美的模型不会产生假正例，因此其假正例率为 0.0，也就是说，假正例率为 0%。\",\"在实际负例数量非常少（例如总共 1-2 个示例）的不平衡数据集中，FPR 作为一个指标就没有那么有意义和实用。\"]},\"565\":{\"h\":\"精确率\",\"t\":[\"精确率是指模型所有正类别分类中实际为正类别的分类所占的比例。在数学上，其定义为：\",\"在垃圾邮件分类示例中，精确率衡量的是被归类为垃圾邮件且实际上是垃圾邮件的电子邮件所占的比例。\",\"假设有一个完美的模型，则其假正例数为零，因此精确率为 1.0。\",\"在实际正例数量非常少（例如总共 1-2 个示例）的不平衡数据集中，精确率作为指标的意义和实用性较低。\",\"随着假正例的减少，精确率会提高；随着假负例的减少，召回率会提高。提高分类阈值往往会减少假正例的数量并增加假负例的数量，而降低阈值则会产生相反的效果。因此，精确率和召回率通常呈现反向关系，提高其中一个会降低另一个。\",\"分类阈值: 模型输出的概率值大于某个值时，模型才会将该样本分类为正类。\"]},\"566\":{\"h\":\"指标的选择和权衡\",\"t\":[\"在评估模型和选择阈值时，您选择优先考虑的指标取决于特定问题的成本、收益和风险。在垃圾邮件分类示例中，通常最好优先考虑召回率（抓取所有垃圾邮件）或准确率（尝试确保被标记为垃圾邮件的电子邮件实际上是垃圾邮件），或者在达到某个最低准确性水平的情况下，兼顾这两者。\",\"指标\",\"指南\",\"准确率\",\"作为平衡数据集的模型训练进度/收敛情况的粗略指标。对于模型效果，请仅与其他指标搭配使用。避免使用不平衡的数据集。考虑使用其他指标。\",\"召回率（真正例率）\",\"当假负例的代价高于假正例时使用，有病的人不能诊断为健康。\",\"假正例率\",\"当假正例的代价高于假负例时使用，误报很可怕。\",\"精确率\",\"当正例预测的准确性非常重要时，请使用此方法。\"]},\"567\":{\"h\":\"F1 得分\",\"t\":[\"F1 得分是精确率和召回率的调和平均数（一种平均值）。\",\"在数学上，它可按下式计算：\",\"此指标可平衡精确率和召回率的重要性，对于类别不平衡的数据集，优先于准确率。当精确率和召回率均为 1.0 的满分时，F1 得分也会为 1.0 的满分。更广泛地说，当精确率和召回率的值接近时，F1 得分也会接近它们的值。当精确率和召回率相差很大时，F1 将与较差的指标相似。\"]},\"568\":{\"h\":\"ROC 曲线和 AUC\",\"t\":[\"上一部分介绍了一系列模型指标，所有这些指标都是基于单个分类阈值值计算得出的。但是，如果您想评估模型在所有可能阈值下的质量，则需要使用不同的工具。\"]},\"569\":{\"h\":\"ROC (Receiver Operating Characteristic)\",\"t\":[\"ROC 曲线直观地显示了所有阈值下的模型性能。名称的长版本“接收器操作特性”源自二战雷达检测。\",\"绘制 ROC 曲线的方法是：计算每个可能的阈值（在实践中，是按选定的间隔）的真正例率 (TPR) 和假正例率 (FPR)，然后将 TPR 与 FPR 绘制到图表中。\",\"完美的模型在某个阈值下的 TPR 为 1.0，FPR 为 0.0，如果忽略所有其他阈值，则可以用 (0, 1) 点表示，也可以用以下方式表示：\",\"图 1. 假设的理想模型的 ROC 和 AUC\"]},\"570\":{\"h\":\"AUC （曲线下面积）\",\"t\":[\"ROC 曲线下面积 (AUC) 表示，如果给定随机选择的正例和负例，模型将正例排在负例之上的概率。\",\"上面的完美模型包含边长为 1 的正方形，其曲线下面积 (AUC) 为 1.0。这意味着，模型将随机选择的正例正确排在随机选择的负例之上的概率为 100%。\",\"更具体地说，AUC 为 1.0 的垃圾邮件分类器始终会为随机垃圾邮件分配比随机合规电子邮件更高的垃圾邮件概率。每封电子邮件的实际分类取决于您选择的阈值。\",\"对于二元分类器，如果模型的效果与随机猜测或抛硬币的效果完全一样，则其 ROC 曲线为从 (0,0) 到 (1,1) 的对角线。AUC 为 0.5，表示正确对随机正例和负例进行排名的概率为 50%。\",\"在垃圾邮件分类器示例中，AUC 为 0.5 的垃圾邮件分类器仅在 50% 的情况下会将随机垃圾邮件的垃圾邮件概率设为高于随机合法邮件的垃圾邮件概率。\",\"图 2. 完全随机猜测的 ROC 和 AUC\"]},\"571\":{\"h\":\"精确率与召回率曲线\",\"t\":[\"如果数据集在类别之间大致平衡，AUC 和 ROC 非常适合比较模型。当数据集不均衡时，准确率-召回率曲线 (PRC) 和这些曲线下的面积可以更好地直观比较模型性能。精确率/召回率曲线的创建方法是，在 y 轴上绘制精确率，在 x 轴上绘制所有阈值下的召回率。\",\"图 3. 精确率与召回率曲线\"]},\"572\":{\"h\":\"用于选择模型和阈值的 AUC 和 ROC\",\"t\":[\"AUC 是比较两个不同模型性能的有效衡量指标，前提是数据集大致平衡。曲线下面积较大的模型通常是更好的模型。\",\"图 4. 两个假设模型的 ROC 和 AUC。右侧曲线的 AUC 较高，表示该模型优于左侧曲线对应的模型。\",\"ROC 曲线上最接近 (0,1) 的点表示给定模型效果最佳的阈值范围。我们选择的阈值取决于哪个指标对特定用例而言最重要。请考虑下图中的点 A、B 和 C，每个点都代表一个阈值：\",\"图 5. 三个标记的点，表示阈值。\",\"如果假正例（误报）的代价很高，则可能有必要选择 FPR 较低的阈值（例如 A 点），即使 TPR 会降低也是如此。反之，如果假正例成本较低，而假负例（漏掉的真正例）成本较高，则点 C 的阈值（可最大限度地提高 TPR）可能更为合适。如果费用大致相当，点 B 在 TPR 和 FPR 之间可能提供最佳平衡。\"]},\"573\":{\"h\":\"数学知识点\",\"t\":[\"数学知识点\"]},\"574\":{\"h\":\"协方差矩阵\",\"t\":[\"协方差是两个变量“是否一起变化”的度量:\",\"如果两个变量 一起变大或一起变小，协方差是正的；\",\"如果一个变量变大时另一个变小，协方差是负的；\",\"如果两个变量 无关，协方差接近 0。\",\"数学定义（以两个变量为例）:\",\"直观上，它表示：\",\"“X 和 Y 的偏离平均值的乘积”的期望。\",\"扩展到多个变量：协方差矩阵\",\"如果你有 多个变量（如 ），你就可以把它们两两之间的协方差，组成一个 矩阵，这个矩阵就叫：\",\"协方差矩阵的结构，以三个变量为例（比如身高、体重、年龄）：\",\"注意：\",\"对角线上的元素：，也就是每个变量自己的方差。\",\"非对角线上的元素：表示变量之间的相关性（协方差）。\",\"这个矩阵是 对称的（因为 ）。\",\"图像直观理解（二维协方差矩阵），如果我们画出一个二维正态分布：\",\"当两个变量 不相关，协方差 = 0 → 分布是一个 圆形。\",\"当两个变量 正相关，协方差 > 0 → 分布是一个 沿对角线方向拉长的椭圆。\",\"当两个变量 负相关，协方差 < 0 → 椭圆朝反对角线方向倾斜。\",\"在深度学习与生成模型中，协方差矩阵可以用来：\",\"场景\",\"用法\",\"多元高斯分布\",\"表达不同维度之间的“联合关系”\",\"高斯混合模型（GMM）\",\"不同类别的“形状”和“方向”由协方差控制\",\"马氏距离\",\"衡量点与均值之间的距离，但考虑变量相关性\",\"主成分分析（PCA）\",\"通过协方差矩阵找出“主要变化方向”（特征值分解）\",\"高斯过程（GP）\",\"协方差函数定义样本之间的相似性结构\",\"一个例子帮助理解，假设我们有如下样本（身高和体重）：\",\"人\",\"身高（cm）\",\"体重（kg）\",\"A\",\"170\",\"65\",\"B\",\"180\",\"75\",\"C\",\"160\",\"55\",\"先计算每一维的均值，再计算协方差矩阵（不展开计算细节）后，你会得到：\",\"这意味着：\",\"身高和体重的方差都为 100；\",\"协方差为 100，说明它们强烈正相关。\",\"总结:\",\"名词\",\"含义\",\"协方差（cov）\",\"度量两个变量是否同步变化\",\"协方差矩阵（）\",\"所有变量两两之间协方差的矩阵表示\",\"对角线\",\"每个变量自己的方差\",\"非对角线\",\"表示不同变量之间的线性相关性\",\"应用\",\"多元高斯、高斯过程、GMM、PCA、马氏距离等\"]},\"575\":{\"h\":\"马氏距离\"},\"576\":{\"h\":\"欧几里得距离（Euclidean Distance）\",\"t\":[\"公式是：\",\"直观理解：\",\"它就是我们平时量两个点之间“直线距离”的方法。\",\"它对每个维度的偏差一视同仁，不考虑各维度数据的分布特征。\",\"换句话说，哪怕某个维度的数据本来波动很大（方差大），它在这个维度上的偏差也会被直接算入距离，导致整体距离变大。\"]},\"577\":{\"h\":\"马氏距离（Mahalanobis Distance）\",\"t\":[\"公式是：\",\"其中 是数据的协方差矩阵。\",\"直观理解：\",\"它不仅考虑两个点之间的差异，还考虑数据在各个维度上的方差大小和维度间的相关性。\",\" 是协方差矩阵的逆，起到了“标准化”的作用，把数据的不同尺度和相关性都考虑进来。\"]},\"578\":{\"h\":\"尺度差异性\",\"t\":[\"欧几里得距离计算方式：\",\"它直接对每个维度的偏差做平方加总。\",\"如果某个维度的数值范围很大（如年龄：0~100），另一个维度很小（如身高标准化后波动在 ），那么前者的变化会主导整个距离。\",\"这会导致尺度大的特征“支配”了距离判断，从而导致偏差。\",\"马氏距离定义如下：\",\"其中 是样本协方差矩阵， 是它的逆矩阵。\",\"假设我们有一个样本点 ，均值是 ，方差是 ，那么协方差矩阵就是：\",\"此时马氏距离变为：\",\"👉 这就是我们熟悉的 标准差单位距离（z-score 距离）。\",\"结论：马氏距离会自动把不同特征的偏差按标准差进行“标准化”。\",\"在高维空间中：\",\" 是一个 的协方差矩阵；\",\"它包含了每个特征的方差（主对角线），和特征间的协方差（非对角元素）；\",\" 相当于一个“加权标准化器”，对不同方向的偏差做缩放和正交旋转。\",\"设两维特征的协方差矩阵是：\",\"说明：\",\"第一个维度方差是 100，第二个维度方差是 1；\",\"马氏距离中对第一个维度的偏差乘上 ，惩罚少；\",\"第二个维度的偏差乘上 1，惩罚多。\",\"这就实现了 对每个维度根据尺度差异进行惩罚调整 —— 偏差大但常见的就不判定为“远”，偏差小但罕见的要严惩。\",\"你可以把马氏距离想成是：\",\"“在考虑数据分布形状后，重新拉直空间、拉平数据”的距离度量。\",\"原始空间中，数据可能沿某个方向拉长、压扁；\",\"马氏距离通过协方差矩阵逆变换，把这些方向“拉回正态”；\",\"变换后再用欧几里得距离度量 —— 就能反映“真实统计意义上的远近”。\"]},\"579\":{\"h\":\"总结\",\"t\":[\"欧几里得距离适合所有维度的尺度和方差差不多时，或者你不在意尺度差异。\",\"马氏距离适合不同维度尺度差别大、方差差异大且可能相关的情况，更能反映真实的“统计距离”。\"]},\"580\":{\"h\":\"注意力图可视化\",\"t\":[\"注意力图可视化\"]},\"581\":{\"h\":\"注意力图可视化\"},\"582\":{\"h\":\"ViT 模型\",\"t\":[\"本文基于 DINO 论文开源的代码实现进行讲解: visualize_attention.py\",\"步骤如下:\",\"下载预先训练的模型权重: https://github.com/facebookresearch/dino\",\"指定待处理图像路径后，对输入图像进行预处理\",\" # 图像预处理：调整大小、转换为张量、标准化 transform = pth_transforms.Compose([ pth_transforms.Resize(args.image_size), pth_transforms.ToTensor(), pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), ]) img = transform(img) # 确保图像尺寸能被 patch_size 整除（减去余数，确保可以整除) w, h = img.shape[1] - img.shape[1] % args.patch_size, img.shape[2] - img.shape[2] % args.patch_size img = img[:, :w, :h].unsqueeze(0) # 增加批次维度\",\"计算输出特征图的尺寸\",\" # 计算特征图的尺寸 w_featmap = img.shape[-2] // args.patch_size h_featmap = img.shape[-1] // args.patch_size\",\"获取最后一层的自注意力权重矩阵 & 获取 CLS Token 对其他 Token 的注意力权重\",\" # 获取最后一层的自注意力权重 attentions = model.get_last_selfattention(img.to(device)) nh = attentions.shape[1] # 注意力头数量 , [b,h,seq,seq] # 只保留输出 patch 的注意力权重 (排除 [CLS] token) # attentions[0, :, 0, 1:] 表示： # - 0: 批次索引 # - : : 所有注意力头 # - 0: 输出位置 (第一个 patch) # - 1: 从第二个位置开始 (排除 [CLS] token) attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\",\"class Attention(nn.Module): ... def forward(self, x): B, N, C = x.shape qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) q, k, v = qkv[0], qkv[1], qkv[2] attn = (q @ k.transpose(-2, -1)) * self.scale attn = attn.softmax(dim=-1) attn = self.attn_drop(attn) x = (attn @ v).transpose(1, 2).reshape(B, N, C) x = self.proj(x) x = self.proj_drop(x) return x, attn # 获取最后一层的自注意力权重矩阵 attn\",\"多头自注意力中每个Head会根据分配给自己的这部分特征维度，计算当前位置和其他位置的特征相似度，从而得到一个相似度权重矩阵。比如: Head1根据身高计算相似度，Head2根据体重计算相似度，Head3根据年龄计算相似度，分别得到三个从不同视角下计算出来的相似度矩阵。\",\"CLS Token 的目标是抽取有利于当前学习任务的图像重要特征信息汇总，因此我们重点关注每个头对应的注意力权重矩阵上，CLS Token 对图像中哪些 patch 投注了更高的注意力，就说明这些区域对当前学习任务特别重要 (每个头关注的区域或许不太一样，因为每个头只能根据分配给自己的这部分特征，决定哪些区域更重要)。\",\"生成注意力热力图\",\" # 重塑注意力权重为 2D 特征图 attentions = attentions.reshape(nh, w_featmap, h_featmap) # 插值到原始图像尺寸，使用最近邻插值保持 patch 边界清晰 attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=args.patch_size, mode=\\\"nearest\\\")[0].cpu().numpy()\",\"可视化出每个头对应的注意力热力图，这样可以看出每个头分别关注图像的哪部分区域\",\" # 保存每个注意力头的热力图 for j in range(nh): fname = os.path.join(args.output_dir, f\\\"attn-head{j}.png\\\") plt.imsave(fname=fname, arr=attentions[j], format='png') print(f\\\"{fname} 已保存。\\\")\",\"如果我们希望强化高注意力权重区域，我们便可以采样DINO模型代码中采用的注意力掩码矩阵技巧，将注意力图中注意力权重大于指定阈值的区域进行高亮显示; 注意力掩码矩阵生成逻辑如下所示:\",\" if args.threshold is not None: # 只保留一定百分比的注意力质量 val, idx = torch.sort(attentions) val /= torch.sum(val, dim=1, keepdim=True) # 归一化 cumval = torch.cumsum(val, dim=1) # 累积和 th_attn = cumval > (1 - args.threshold) # 阈值化 # 恢复原始顺序 idx2 = torch.argsort(idx) for head in range(nh): th_attn[head] = th_attn[head][idx2[head]] # 重塑为 2D 特征图 th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float() # 插值到原始图像尺寸(注意力掩码矩阵) th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=args.patch_size, mode=\\\"nearest\\\")[0].cpu().numpy()\",\"当执行完插值得到原始图像尺寸大小时，其实就得到了注意力掩码矩阵，最后便是根据注意力掩码矩阵应用在原图像上，即将掩码矩阵中值为1处，进行高亮即可，效果如下:\"]},\"583\":{\"h\":\"语义分割中常用的损失函数\",\"t\":[\"语义分割中常用的损失函数\"]},\"584\":{\"h\":\"语义分割\",\"t\":[\"语义分割是计算机视觉领域中的一项任务，旨在将图像中的每个像素分类为不同的语义类别。与对象检测任务不同，语义分割不仅需要识别图像中的物体，还需要对每个像素进行分类，从而实现对图像的细粒度理解和分析。\",\"语义分割可以被看作是像素级别的图像分割，其目标是为图像中的每个像素分配一个特定的语义类别标签。每个像素都被视为图像的基本单位，因此语义分割可以提供更详细和准确的图像分析结果。\",\"语义分割 vs 分类 :\",\"在语义分割任务中，由于需要对每个像素进行分类，因此需要使用像素级别的损失函数。\",\"语义分割任务中，图像中各个类别的像素数量通常不均衡，例如背景像素可能占据了大部分。\",\"语义分割任务需要对图像中的每个像素进行分类，同时保持空间连续性。\"]},\"585\":{\"h\":\"损失函数\"},\"586\":{\"h\":\"Dice Loss\",\"t\":[\"Dice Loss 是一种常用于语义分割任务的损失函数，尤其在目标区域较小、类别不平衡（class imbalance）的情况下表现优异。它来源于 Dice 系数（Dice Coefficient） ，又称为 Sørensen-Dice 系数 ，是衡量两个样本集合之间重叠程度的一种指标。\",\"Dice 系数衡量的是预测掩码与真实标签之间的相似性，公式如下：\",\"其中：\",\" ：模型预测出的功能区域（如经过 sigmoid 后的概率值）；\",\" ：Ground Truth 掩码（二值化或软标签）；\",\" ：预测为正类且实际也为正类的部分（交集）；\",\" ：预测和真实中所有正类区域之和；\",\"⚠️ 注意：Dice 系数范围是 [0, 1]，越大越好。\",\"Dice Loss 为了将其作为损失函数使用，我们通常取其补集：\",\"有时也会加入一个平滑项 ϵ 防止除以零：\",\"Dice Loss 的优势:\",\"优势\",\"描述\",\"对类别不平衡不敏感,更关注“有没有覆盖正确区域”，而不是“有多少点被正确分类”\",\"不像 BCE Loss 那样对负样本过多敏感\",\"直接优化 IoU 的替代指标\",\"Dice 和 IoU 表现类似，但更易梯度下降\",\"支持 soft mask 输入\",\"可处理连续概率值，不需要先 threshold\",\"更关注整体区域匹配\",\"而不是逐点分类\",\"代码实现:\",\"class DiceLoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数，支持加权和平均损失。 参数: weight (Tensor): 各类别的权重（可选） size_average (bool): 是否对 batch 中的样本取平均 loss \\\"\\\"\\\" super(DiceLoss, self).__init__() # 该参数未在当前代码中使用，但保留接口以备后续扩展 self.weight = weight # 控制是否对 batch 内 loss 取均值或求和 self.size_average = size_average def forward(self, inputs, targets, smooth=1): \\\"\\\"\\\" 前向传播函数，计算 Dice Loss。 参数: inputs (Tensor): 模型输出的预测值（logits 或 raw output），形状为 [B, N] targets (Tensor): 真实标签（ground truth mask），形状为 [B, N] smooth (float): 平滑项，防止除零错误，默认为 1 返回: dice_loss (Tensor): 计算得到的 Dice Loss \\\"\\\"\\\" # 如果你的模型最后没有 sigmoid 层，则需要在这里激活，否则应注释掉这行 inputs = F.sigmoid(inputs) # 将 logits 映射到 [0,1] 区间 # 将输入展平成一维张量，便于后续计算 # inputs: [B*N] # targets: [B*N] inputs = inputs.view(-1) targets = targets.view(-1) # 计算交集：预测与 GT 的重合部分 intersection = (inputs * targets).sum() # 计算 Dice Coefficient，加入 smooth 防止除以零 dice_score = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth) # 返回 Dice Loss，用 1 - Dice Coefficient # 值越小表示匹配越好 return 1 - dice_score\"]},\"587\":{\"h\":\"BCE-Dice Loss\",\"t\":[\"BCE-Dice Loss是将Dice Loss和标准的二元交叉熵（Binary Cross-Entropy, BCE）损失结合在一起的一种损失函数，通常用于分割模型中。它结合了两种 loss 的优点：\",\"BCE Loss ：关注每个点的分类误差；\",\"Dice Loss ：关注整体区域匹配度；\",\"Binary Cross Entropy Loss（BCE Loss）\",\"公式（逐点）：\",\"其中：\",\"：真实标签（binary 或 soft mask）；\",\"：模型输出的概率值；\",\"特点：\",\"对每个点单独计算分类误差；\",\"强调预测与 GT 的一致性；\",\"在类别平衡时效果好，但在前景远少于背景时容易偏向负样本；\",\"Dice Loss\",\"公式（简化版）：\",\"其中：\",\"：预测概率；\",\"：真实标签；\",\"：平滑项，防止除以零；\",\"特点：\",\"不依赖绝对数量，而是关注预测和 GT 的交并比；\",\"更适合前景极少的小区域识别；\",\"能缓解类别不平衡问题；\",\"为什么要把它们结合起来？\",\"模型\",\"缺陷\",\"补充方式\",\"BCE Loss\",\"对前景响应弱，易受类别不平衡影响\",\"加入 Dice Loss 增强区域匹配\",\"Dice Loss\",\"对单个点的分类精度不够敏感\",\"加入 BCE Loss 提高逐点判别能力\",\"组合后的优势：\",\"优势\",\"描述\",\"✔️ 抗类别不平衡能力强\",\"Dice Loss 起主导作用\",\"✔️ 对细节更敏感\",\"BCE Loss 提升边缘识别精度\",\"✔️ 支持 soft mask 输入\",\"可处理连续值掩码\",\"✔️ 更稳定地收敛\",\"两者互补，避免训练震荡\",\"代码实现:\",\"class DiceBCELoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数，构建一个组合损失函数 Dice + BCE。 参数: weight (Tensor): 可选参数，用于类别加权； size_average (bool): 是否对 batch 内样本取平均 loss（已弃用）； \\\"\\\"\\\" super(DiceBCELoss, self).__init__() # 这里暂时未使用 weight 和 size_average，保留接口以备扩展 def forward(self, inputs, targets, smooth=1): \\\"\\\"\\\" 前向传播函数，计算预测输出与真实标签之间的 Dice Loss 与 BCE Loss 的加权和。 参数: inputs (Tensor): 模型输出的 logits 或 raw 分数，形状为 [B, N] targets (Tensor): 真实掩码（ground truth mask），形状为 [B, N] smooth (float): 平滑项，防止除零错误，默认为 1 返回: Dice_BCE (Tensor): Dice + BCE 组合损失值 \\\"\\\"\\\" # 如果模型最后没有 sigmoid 层，这里需要激活 # 如果已经包含 sigmoid，则应注释掉这一行 inputs = F.sigmoid(inputs) # 将输入映射到概率空间 [0, 1] # 将输入和目标展平成一维张量，便于后续计算 # inputs: [B*N] # targets: [B*N] inputs = inputs.view(-1) targets = targets.view(-1) # 计算交集：预测值和真实值都为 1 的区域 intersection = (inputs * targets).sum() # 计算 Dice Loss： # Dice Coefficient = (2 * intersection) / (inputs_sum + targets_sum) # Dice Loss = 1 - Dice Coefficient inputs_sum = inputs.sum() targets_sum = targets.sum() dice_score = (2. * intersection + smooth) / (inputs_sum + targets_sum + smooth) dice_loss = 1 - dice_score # 计算 Binary Cross Entropy Loss（BCE） # 注意：F.binary_cross_entropy 默认要求 inputs 已经经过 sigmoid BCE = F.binary_cross_entropy(inputs, targets, reduction='mean') # 组合损失：BCE + Dice Loss Dice_BCE = BCE + dice_loss return Dice_BCE\"]},\"588\":{\"h\":\"Jaccard/Intersection over Union (IoU) Loss\",\"t\":[\"Jaccard Loss，也称为Intersection over Union (IoU) Loss，是一种常用的损失函数，用于语义分割任务中评估模型的分割结果与真实分割标签之间的相似性。它基于Jaccard指数（Jaccard Index），也称为 交并比（Intersection over Union, IoU）指标，用于度量两个集合之间的重叠程度。\",\"Jaccard Index（IoU）\",\"其中：\",\"：模型输出的概率值或二值化结果；\",\"：ground truth 掩码；\",\"分子是预测和 GT 的交集；\",\"分母是两者的并集；\",\"⚠️ IoU 值 ∈ [0, 1]，越大越好。\",\"Jaccard Loss（IoU Loss）\",\"为了将 IoU 转换为可优化的损失函数，我们取其补集：\",\"这样，损失越小表示预测越接近真实标签。\",\"为了避免除以零，通常加入平滑项 ：\",\"Jaccard Loss 有以下几个优点：\",\"特性\",\"描述\",\"✔️ 对类别不平衡不敏感\",\"不像 BCE Loss 那样偏向背景点\",\"✔️ 关注整体区域匹配\",\"强调预测与 GT 的空间一致性\",\"✔️ 更适合评估边界模糊区域\",\"如功能区域边缘不确定性较高\",\"与其他 Loss 的对比\",\"损失函数\",\"是否支持 soft mask\",\"是否对类别不平衡敏感\",\"是否直接优化 IoU\",\"输出范围\",\"BCE Loss\",\"❌ 否（需二值化）\",\"✅ 是\",\"❌ 否\",\"[0, ∞)\",\"Focal Loss\",\"✅ 是（加权）\",\"✅ 是（缓解）\",\"❌ 否\",\"[0, ∞)\",\"Dice Loss\",\"✅ 是\",\"✅ 是\",\"近似于 IoU\",\"[0, 1]\",\"Jaccard (IoU) Loss\",\"✅ 是\",\"✅ 是\",\"✅ 是\",\"[0, 1]\",\"虽然 Dice Loss 在实际训练中更稳定，但 Jaccard Loss 更贴近最终评估指标（IoU），适合在推理阶段作为验证标准。\",\"代码实现:\",\"class IoULoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数，构建一个基于 IoU（交并比）的损失函数。 参数: weight (Tensor): 可选参数，用于类别加权（未使用） size_average (bool): 是否对 batch 内样本取平均 loss（已弃用） \\\"\\\"\\\" super(IoULoss, self).__init__() # weight 和 size_average 在此实现中未使用，保留接口以备后续扩展 def forward(self, inputs, targets, smooth=1): \\\"\\\"\\\" 前向传播函数，计算预测输出与真实标签之间的 IoU Loss。 参数: inputs (Tensor): 模型输出的原始 logit 或经过 sigmoid 的概率值； 形状为 [B, N] targets (Tensor): ground truth 掩码，形状为 [B, N] smooth (float): 平滑项，防止除零错误，默认为 1 返回: iou_loss (Tensor): 计算得到的 IoU Loss \\\"\\\"\\\" # 如果模型最后没有 sigmoid 层，则在这里激活 # 如果已经包含 sigmoid，则应注释掉这一行 inputs = torch.sigmoid(inputs) # 将输入映射到 [0,1] 区间 # 将输入和目标展平成一维张量便于计算 # inputs: [B*N] # targets: [B*N] inputs = inputs.view(-1) targets = targets.view(-1) # 计算交集（Intersection），等价于 TP（True Positive） intersection = (inputs * targets).sum() # 计算并集：Union = input + target - intersection total = (inputs + targets).sum() union = total - intersection # 计算 IoU Score，加入平滑项防止除以零 iou_score = (intersection + smooth) / (union + smooth) # IoU Loss = 1 - IoU score，这样越接近 1，loss 越小 iou_loss = 1. - iou_score return iou_loss\"]},\"589\":{\"h\":\"Focal Loss\",\"t\":[\"Focal Loss 是一种针对类别不平衡（Class Imbalance）问题的损失函数改进方案，由何恺明团队在2017年论文《Focal Loss for Dense Object Detection》中提出，主要用于解决目标检测任务中前景-背景类别极端不平衡的问题（如1:1000）。其核心思想是通过调整难易样本的权重，使模型更关注难分类的样本。\",\"Focal Loss 基于交叉熵损失进行扩展，将样本的权重进行动态调整。与交叉熵损失函数相比，Focal Loss引入了一个衰减因子，其中 pt 是预测的概率值。这个衰减因子能够使得易分类的样本（ pt较高 ）的权重降低，从而减少对分类正确样本的贡献。\",\"核心思想:\",\"(1) 类别不平衡的问题\",\"在分类任务中（尤其是目标检测），负样本（背景）往往远多于正样本（目标），导致：\",\"模型被大量简单负样本主导，难以学习有效特征。\",\"简单样本的梯度贡献淹没难样本的梯度。\",\"(2) Focal Loss 的改进\",\"降低易分类样本的权重：对模型已经分类正确的样本（高置信度）减少损失贡献。\",\"聚焦难分类样本：对分类错误的样本（低置信度）保持高损失权重。\",\"Focal Loss 基于标准交叉熵损失（Cross-Entropy Loss）改进而来。\",\"(1) 标准交叉熵损失（CE Loss）\",\"其中：\",\"p 是模型预测的概率（经过sigmoid/softmax）。\",\"y 是真实标签（0或1）。\",\"(2) Focal Loss 定义\",\"：类别平衡权重（通常），用于平衡正负样本数量差异。\",\"：调节因子（通常），控制难易样本的权重衰减程度。\",\"γ 参数用于抑制容易分类的样本，而 α 参数用于平衡正负类别的权重。两者解决的是不同维度的问题：\",\"α：防止前景点（功能区域）被背景淹没，解决数据集中“类别数量不平衡”的问题（数据集级别）；\",\"γ：防止模型只关注简单样本，忽略难分类样本，解决模型训练时“简单样本主导梯度”的问题（样本级别）；\",\"综上，先通过 α 平衡类别数量，再通过 γ 抑制简单样本，两者协同提升模型性能。\",\"关键参数的作用:\",\"参数\",\"作用\",\"典型值\",\"控制难易样本权重：• ：退化为CE Loss• ：显著抑制简单样本\",\"0.5 ~ 5\",\"平衡正负样本数量：• ：正样本较少时增加权重\",\"0.25 ~ 0.75\",\"难样本vs易样:\",\"易分类样本（如 p=0.9 ）： 接近0，损失被大幅降低。\",\"难分类样本（如 p=0.1 ）： 接近1，损失几乎不受影响。\",\"假设两个正样本：\",\"易样本：（模型已自信分类）\",\"标准 CE Loss：\",\"Focal Loss（）：损失权重降低 100 倍！\",\"难样本：（模型分类错误）\",\"标准 CE Loss：\",\"Focal Loss（）：损失权重仅降低 20%。\",\"应用场景：\",\"目标检测（如RetinaNet）： 解决前景（目标）与背景的极端不平衡问题。\",\"医学图像分割： 病灶区域像素远少于正常组织。\",\"任何类别不平衡的分类任务： 如欺诈检测、罕见疾病诊断等。\",\"优缺点:\",\"优点\",\"缺点\",\"显著提升难样本的分类性能\",\"需调参（）\",\"抑制简单样本的梯度主导\",\"对噪声标签敏感\",\"兼容大多数分类模型\",\"计算量略高于CE Loss\",\"Focal Loss 通过 动态调整样本权重，使模型聚焦难分类样本。\",\"参数选择：\",\"：一般从2开始调优（值越大，简单样本抑制越强）。\",\"：根据正负样本比例调整（如正样本少则增大 ）。\",\"适用场景：类别不平衡越严重，Focal Loss 效果越显著。\",\"代码实现:\",\"# 设置全局参数（可调） ALPHA = 0.8 # 控制正样本（目标点）与负样本（非目标点）之间的损失权重； # 若前景点稀疏（如 grasping area），建议设为较高值（如 0.25~0.75）； GAMMA = 2 # 聚焦参数，用于抑制易分类样本，放大难分类样本的影响； class FocalLoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数，构建一个基于 BCE 的改进版 Focal Loss。 参数: weight (Tensor): 可选参数，用于类别加权（未使用）； size_average (bool): 是否对 batch 内样本取平均 loss（已弃用）； \\\"\\\"\\\" super(FocalLoss, self).__init__() # 当前实现未使用 weight 和 size_average，保留接口以备扩展 def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1): \\\"\\\"\\\" 前向传播函数，计算预测输出与真实标签之间的 Focal Loss。 参数: inputs (Tensor): 模型输出的原始 logit 或经过 sigmoid 的概率值； 形状为 [B, N]（batch_size × 点数） targets (Tensor): ground truth 掩码，形状为 [B, N] alpha (float): 平衡因子，控制正类（功能区域）和负类（非功能区域）之间的损失权重； 前景点少 → alpha 高（如 0.75），防止被背景淹没； gamma (float): 聚焦参数，抑制 easy examples，放大 hard examples； smooth (float): 平滑项，防止除零错误，默认为 1 返回: focal_loss (Tensor): 计算得到的 Focal Loss 值 \\\"\\\"\\\" # 如果模型最后没有 sigmoid 层，则在这里激活 inputs = torch.sigmoid(inputs) # 将输入展平便于后续计算 # inputs: [B*N], 表示每个点属于功能区域的概率； # targets: [B*N], 表示每个点是否属于目标功能区域（soft/hard label）； inputs = inputs.view(-1) targets = targets.view(-1) # Step 1: 计算 Binary Cross Entropy Loss（BCE） # 这里使用 'mean' reduction，表示对 batch 内取平均 ce_loss = F.binary_cross_entropy(inputs, targets, reduction='mean') # Step 2: 计算 pt = exp(-ce_loss)，即 e^{-ce_loss} pt = torch.exp(-ce_loss) # shape: scalar # Step 3: 按类别分配 alpha alpha = torch.where(targets == 1, alpha, 1 - alpha) # Step 4: 构建 Focal Weight： # focal_weight = α * (1 - pt)^γ # 目的是：让难分类样本获得更大的 loss 权重，从而引导模型学习更多语义信息 focal_weight = alpha * (1 - pt) ** gamma # Step 5: 最终 Focal Loss = focal_weight × ce_loss focal_loss = focal_weight * ce_loss return focal_loss\",\"关于计算 p_t（模型对真实类别的预测概率）代码解析:\",\"pt = torch.exp(-ce_loss) # p_t = softmax(output)[target_class]\",\"ce_loss = F.cross_entropy(...) → 这是交叉熵损失；\",\"-ce_loss → 负号；\",\"torch.exp(-ce_loss) → 求 exp（自然指数）；\",\"但实际上这行代码的意图是计算 ，即模型对真实类别的预测概率（confidence）, 这里采用的方法是一种“技巧性近似”。对于一个样本，交叉熵损失为：\",\"所以：\",\"pt = torch.exp(-ce_loss)\",\"这个表达式其实是通过 CE loss 反推出来的 ，因为：\"]},\"590\":{\"h\":\"Tversky Loss\",\"t\":[\"Tversky Loss的设计灵感来自Tversky指数（Tversky index），它是一种用于度量集合之间相似性的指标，同时也是 Dice Loss 的一种泛化形式，通过引入两个可调节参数来增强模型对假阳性（False Positives）和假阴性（False Negatives）的敏感度控制。\",\"Tversky Loss 的核心是 Tversky 系数：\",\"然后损失就是：\",\"其中：\",\"TP ：真阳性（True Positive）= 预测为正类，且真实也为正类的样本数\",\"FP ：假阳性（False Positive）= 预测为正类，但真实是负类的样本数\",\"FN ：假阴性（False Negative）= 预测为负类，但真实是正类的样本数\",\"α 和 β 是两个可调节的超参数\",\"α 越大，FP 的影响就越大 → 模型更不喜欢“误报”\",\"β 越大，FN 的影响就越大 → 模型更不喜欢“漏报”\",\"如果你设置 α>β ，说明你更讨厌“误检”\",\"如果你设置 β>α ，说明你更讨厌“漏检”\",\"分母中的 TP+α⋅FP+β⋅FN 构成了一个“加权惩罚项”\",\"例如：\",\"α=0.3, β=0.7 → 更重视召回率（Recall）\",\"α=0.7, β=0.3 → 更重视精确率（Precision）\",\"当 alpha=beta=0.5 时，Tversky指数简化为Dice系数，该系数也等于F1得分。\",\"当 alpha=beta=1 时，公式转化为Tanimoto系数，而当 alpha+beta=1 时，得到一组F-beta得分。\",\"# 设置默认参数：当 alpha = beta = 0.5 时，等价于 Dice Loss ALPHA = 0.5 # 控制假阳性（FP）的权重 BETA = 0.5 # 控制假阴性（FN）的权重 class TverskyLoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数 参数： weight: 可选，类别权重（用于处理类别不平衡） size_average: 如果为 True，则返回所有样本损失的平均值 \\\"\\\"\\\" super(TverskyLoss, self).__init__() # 本类中不直接使用 weight 和 size_average，但保留它们作为接口兼容 self.weight = weight self.size_average = size_average def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA): \\\"\\\"\\\" 前向传播计算损失值 参数： inputs: 模型输出的预测结果（logits），形状如 (N, H, W) 或 (N, C, H, W) targets: 真实标签（ground truth），形状与 inputs 相同 smooth: 平滑系数，防止除以零 alpha: FP 的惩罚权重 beta: FN 的惩罚权重 返回： loss: 计算得到的 Tversky Loss \\\"\\\"\\\" # 如果模型最后一层没有 Sigmoid 激活函数，请取消下面这行注释 # 对输出应用 Sigmoid 函数，将 logits 转换为概率 [0,1] inputs = F.sigmoid(inputs) # 将输入和目标张量展平为一维，便于后续计算 TP、FP、FN inputs = inputs.view(-1) targets = targets.view(-1) # 真阳性（True Positive）：预测为正且实际也为正的像素数量 TP = (inputs * targets).sum() # 假阳性（False Positive）：预测为正但实际为负的像素数量 FP = ((1 - targets) * inputs).sum() # 假阴性（False Negative）：预测为负但实际为正的像素数量 FN = (targets * (1 - inputs)).sum() # 计算 Tversky 系数（相似度指标） # 分母中：TP + α·FP + β·FN Tversky = (TP + smooth) / (TP + alpha * FP + beta * FN + smooth) # 最终损失是 1 - Tversky，这样在训练中最小化损失就等于最大化重叠度 return 1 - Tversky\"]},\"591\":{\"h\":\"Lovasz Hinge Loss\",\"t\":[\"Lovasz Hinge Loss的设计思想是，在计算IoU得分之前，根据预测误差对预测结果进行排序，然后累积计算每个误差对IoU得分的影响。然后，将该梯度向量与初始误差向量相乘，以最大程度地惩罚降低IoU得分的预测结果。\",\"https://github.com/bermanmaxim/LovaszSoftmax\"]},\"592\":{\"h\":\"Combo Loss\",\"t\":[\"Combo Loss 是一种结合了多个损失函数优点的混合损失函数，特别适用于图像分割任务。它将 Dice Loss 和 交叉熵损失（CrossEntropy Loss） 相结合，并引入一个可调节的权重参数，使得模型在训练过程中可以更灵活地平衡这两部分损失。\",\"核心思想：\",\"Combo Loss = α × CrossEntropy + (1 - α) × Dice Loss\",\"或者更广义地：\",\"Combo Loss = α × 分类误差（CE）+ β × 区域重叠误差（Dice）\",\"其中 α + β = 1，α 控制分类误差的重要性，β 控制区域匹配误差的重要性。\",\"数学定义:\",\"假设我们有预测概率图 ，真实标签 ，那么：\",\"交叉熵损失（Binary Cross Entropy）：\",\"Dice Loss：\",\"Combo Loss 定义为：\",\"其中：\",\"：控制两个损失之间的权重比例\",\"若 ：仅使用交叉熵损失\",\"若 ：仅使用 Dice Loss\",\"为什么使用 Combo Loss:\",\"优势\",\"描述\",\"✔️ 兼顾像素级精度和区域重叠度\",\"CE 关注每个像素的分类准确性，Dice 关注整体区域匹配程度\",\"✔️ 对类别不平衡问题鲁棒\",\"在前景像素远少于背景像素时表现良好（如医学图像）\",\"✔️ 更稳定的训练过程\",\"避免单一损失可能带来的训练不稳定性\",\"✔️ 可调性强\",\"通过调整 α 参数，适应不同任务需求\",\"对比其他损失函数：\",\"损失函数\",\"是否关注像素分类？\",\"是否关注区域匹配？\",\"是否可调？\",\"是否适合类别不平衡？\",\"CrossEntropy Loss\",\"✅\",\"❌\",\"❌\",\"❌\",\"Dice Loss\",\"❌\",\"✅\",\"❌\",\"✅\",\"Tversky Loss\",\"❌\",\"✅ ✅\",\"✅\",\"✅ ✅\",\"Combo Loss\",\"✅ ✅\",\"✅\",\"✅\",\"✅ ✅\",\"代码实现:\",\"# 超参数设置说明： ALPHA = 0.5 # 控制交叉熵中正负样本的权重 # 如果 ALPHA < 0.5：对假阳性（FP）惩罚更重（更关注精确率） # 如果 ALPHA > 0.5：对假阴性（FN）惩罚更重（更关注召回率） CE_RATIO = 0.5 # 控制交叉熵损失和 Dice 损失之间的权重分配 # CE_RATIO 越大，交叉熵在总损失中的占比越高 class ComboLoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数 参数： weight: 可选，类别权重（用于处理类别不平衡） size_average: 如果为 True，则返回所有样本损失的平均值 \\\"\\\"\\\" super(ComboLoss, self).__init__() # 这里不直接使用 weight 和 size_average，但保留作为接口兼容 self.weight = weight self.size_average = size_average def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, eps=1e-9): \\\"\\\"\\\" 前向传播计算 Combo Loss 参数： inputs: 模型输出的概率值（经过 Sigmoid），形状如 (N, H, W) targets: 真实标签，形状与 inputs 相同，值为 0 或 1 smooth: 平滑系数，防止除以零 alpha: 控制 FP/FN 的惩罚比例（用于交叉熵部分） eps: 防止 log(0) 出现的小常数 返回： combo_loss: 计算得到的 Combo Loss \\\"\\\"\\\" # 将输入和目标张量展平为一维，便于后续计算 inputs = inputs.view(-1) targets = targets.view(-1) # 计算 Dice Loss 所需的交集 intersection = (inputs * targets).sum() # Dice Score（区域匹配度） dice_score = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth) # 加入数值稳定性处理，防止 log(0) 出现 NaN # torch.clamp(x, min=a, max=b) 是 PyTorch 中的一个函数，用于将张量 x 中的每个元素限制在 [a, b] 区间内： # 这里把所有 inputs 中的值限制在区间 [eps, 1.0 - eps] 内，防止出现 0 或 1 的极端值。 inputs = torch.clamp(inputs, eps, 1.0 - eps) # 加权交叉熵损失（Weighted Cross Entropy） # 根据 ALPHA 参数调整正类和负类的权重 weighted_ce = - (ALPHA * targets * torch.log(inputs)) - ((1 - ALPHA) * (1 - targets) * torch.log(1 - inputs)) # 对损失求均值 weighted_ce = weighted_ce.mean() # Combo Loss 是交叉熵和 Dice Loss 的加权组合 # 注意：这里使用的是负的 Dice Score（因为要最小化损失） combo_loss = (CE_RATIO * weighted_ce) - ((1 - CE_RATIO) * dice_score) return combo_loss\",\"上面代码实现中使用的是加权交叉熵损失:\"]},\"593\":{\"h\":\"如何选择?\",\"t\":[\"任务需求：根据特定的分割任务的需求和特点，选择适合的损失函数。例如，对于类别不平衡的数据集，可以考虑使用Tversky Loss或Combo Loss等能够处理不平衡情况的损失函数。\",\"实验评估：在实验中，使用不同的损失函数进行训练，并评估它们在验证集或测试集上的性能。比较它们在IoU、准确率、召回率等指标上的表现，选择性能最佳的损失函数。\",\"超参数调整：一些损失函数具有额外的超参数，如Tversky Loss中的alpha和beta，可以通过调整这些超参数来进一步优化损失函数的性能。\"]},\"594\":{\"h\":\"通俗易懂解读BPE分词算法实现\",\"t\":[\"通俗易懂解读BPE分词算法实现\",\"BPE（Byte Pair Encoding，字节对编码）是一种基于频率统计的子词分词算法 ，广泛用于现代自然语言处理任务中，特别是在像 BERT、GPT 和 LLaMA 这样的大模型中。它的核心思想是通过不断合并最常见的字符对来构建一个高效的词汇表。\",\"BPE 的核心思想:\",\"从字符级别开始，逐步合并高频的字符对。\",\"最终生成一个既能表示常见单词，又能拆解未知词的子词词汇表 。\",\"可以有效控制词汇表大小，同时避免“未登录词”问题（OOV, Out-of-Vocabulary）。\"]},\"595\":{\"h\":\"预训练过程\",\"t\":[\"BPE 算法预训练工作流程:\",\"训练语料为: Hello World , Hey Wow\",\"1. 读取训练语料，同时完成断句分词任务\",\"# filepaths: 训练语料所在的文件列表 def create_vocab(filepaths: List[str]) -> Dict[str, int]: # 获取所有单词和每个单词的出现次数词典 vocab = defaultdict(int) for path in tqdm(filepaths, desc='Creating vocabulary'): text = open(path, 'r', encoding='utf-8-sig').read() # 利用NLTK库提供的sent_tokenize方法完成断句功能，即将原文本按照空格，句号等标点符号结合语义进行断句。 sentences = sent_tokenize(text) # 遍历句子列表 for sentence in sentences: # 利用NLTK库提供的wordpunct_tokenize方法完成分词功能 tokens = wordpunct_tokenize(sentence) # 记录每个词的出现次数 for token in tokens: vocab[token] += 1 # vocab: 记录每个词的出现次数的词典 return vocab\",\"2. 过滤掉vocab中的低频词\",\"def truncate_vocab(vocab: Dict[str, int], mincount: int) -> None: tokens = list(vocab.keys()) for token in tokens: if vocab[token] < mincount: del(vocab[token])\",\"示例中设置为了1，不会过滤掉任何词。\",\"3. 数据预处理\",\"将训练语料中的每个单词按字符拆分，并在结尾加上特殊标记 </w> 表示单词结束。\",\"def prepare_bpe_vocab(vocab: Dict[str, int]) -> Dict[str, int]: bpe_vocab = {} # 遍历vocab中所有词 for token in vocab: # 每个词的每个字符后都加上空格，同时末尾加上 </w> 表示单词结束 ntoken = ' '.join(list(token)) + ' </w>' bpe_vocab[ntoken] = vocab[token] return bpe_vocab\",\"4. 经历N次迭代，合并前N个最频繁的字符对\",\" # 一共合并merges个高频字符对后,才结束词汇表的构建 for i in trange(merges, desc='Merging'): # 1. 获取每个相邻字符对的出现次数 pairs = get_stats(vocab) # 2. 获取当前最高频的字符对 best = max(pairs, key=pairs.get) # 3. 合并当前最高频的字符对 vocab = merge_vocab(best, vocab) ######记录历史合并的最高频子词对及其频率(传统BPE算法没有这一步)###### merged_pair_freqs = defaultdict(int) # 一共合并merges个高频字符对后,才结束词汇表的构建 for _ in trange(merges, desc='Merging'): # 1. 获取每个相邻字符对的出现次数 pairs = get_stats(vocab) # 2. 获取当前最高频的字符对 best_pair = max(pairs.items(), key=lambda x: x[1]) ######记录该子词对的全局频率(传统BPE算法没有这一步)###### best_subword = ''.join(best_pair[0]) best_freq = best_pair[1] merged_pair_freqs[best_subword] += best_freq # 3. 合并当前最高频的字符对 vocab = merge_vocab(best_pair[0], vocab)\",\"4.1 获取每个相邻字符对的出现次数\",\"def get_stats(vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]: pairs = defaultdict(int) for word, freq in vocab.items(): # 对经过预处理的vocab中的每个词按空格进行切分 symbols = word.split() # 统计每个相邻字符对的出现次数 for i in range(len(symbols)-1): pairs[symbols[i],symbols[i+1]] += freq return pairs\",\"首轮统计展示\",\"4.2 获取当前最高频的字符对\",\"4.3 合并当前最高频的字符对\",\"def merge_vocab(pair: Tuple[str, str], v_in: Dict[str, int]) -> Dict[str, int]: # 1. 将传入的最高频字符对中的两个字符用空格拼接起来，如: \\\"H e\\\" bigram = re.escape(' '.join(pair)) v_out = {} # 2. 正则匹配含有“H e”的所有单词，并且“H”和“e”必须为两个独立的词，而不能为\\\"HH e\\\"或者\\\"H ee\\\"形式 p = re.compile(r'(?<!\\\\S)' + bigram + r'(?!\\\\S)') # 3. 遍历vocab中所有词 for word in v_in: # 3.1 用正则匹配并替换匹配上的 \\\"H e\\\" 为 “He” w_out = p.sub(''.join(pair), word) v_out[w_out] = v_in[word] # 4. 返回合并最高频字符对后的vocab return v_out\",\"5.根据N轮迭代合并后的Vocab来构建最终的频次表(每个子词的出现次数)\",\"def count_byte_freqs(vocab: Dict[str, int]) -> Dict[str, int]: freqs = defaultdict(int) for word in vocab: # 1. 按空格切分 bytes_ = word.split(' ') # 2. 每个子词出现次数加1 for byte in bytes_: freqs[byte] += 1 # 3. 添加一些特殊词 for token in ['<line/>', '</line>', '<pad>', '<unk>']: freqs[token] += 1 return freqs\",\"6.根据频次表构建最终的词汇表\",\"def create_vocab_maps(freqs: Dict[str, int]) -> (Dict[str, int], Dict[int, str]): # 1. 按照 词频从高到低 的顺序排序 ordered_freqs = sorted(freqs.items(), key=lambda x: x[1], reverse=True) vocab_to_idx, idx_to_vocab = {}, {} for i in range(len(ordered_freqs)): # 2. 构建词汇表 word, freq = ordered_freqs[i] vocab_to_idx[word] = i idx_to_vocab[i] = word return vocab_to_idx, idx_to_vocab\",\"7. freqs = 最终子词频率 + 历史最高频合并对的频率(传统BPE算法没有这一步)\",\" freqs.update(merged_pair_freqs)\",\"8. 通常最后会将预训练生成的频次表和词汇表写入文件保存\",\" def save(self, path: str) -> None: # 1. 频次表记录合并规则，也就是有哪些子词以及这些子词的出现次数，作为分词时的合并规则和优先选择权 with open(f'{path}/freqs.json', 'w', encoding='utf-8') as outfile: json.dump(self.freqs, outfile, indent=4, ensure_ascii=False) # 2. 常规的词汇表 with open(f'{path}/vocab_to_idx.json', 'w', encoding='utf-8') as outfile: json.dump(self.vocab_to_idx, outfile, indent=4, ensure_ascii=False) with open(f'{path}/idx_to_vocab.json', 'w', encoding='utf-8') as outfile: json.dump(self.idx_to_vocab, outfile, indent=4, ensure_ascii=False)\",\"BPE 算法预训练过程完整代码如下\",\" def train_bpe(filepaths: List[str], mincount: int, merges: int) -> 'BytePairTokenizer': vocab = create_vocab(filepaths) truncate_vocab(vocab, mincount) vocab = prepare_bpe_vocab(vocab) merged_pair_freqs = defaultdict(int) # (传统BPE算法没有这一步) for _ in trange(merges, desc='Merging'): pairs = get_stats(vocab) best_pair = max(pairs.items(), key=lambda x: x[1]) best_subword = ''.join(best_pair[0]) # (传统BPE算法没有这一步) best_freq = best_pair[1] # (传统BPE算法没有这一步) merged_pair_freqs[best_subword] += best_freq # (传统BPE算法没有这一步) vocab = merge_vocab(best_pair[0], vocab) freqs = count_byte_freqs(vocab) vocab_to_idx, idx_to_vocab = create_vocab_maps(freqs) freqs.update(merged_pair_freqs) # (传统BPE算法没有这一步) return BytePairTokenizer(freqs, vocab_to_idx, idx_to_vocab)\"]},\"596\":{\"h\":\"分词过程\",\"t\":[\"1.对输入的文本进行断句加分词\",\" # 使用NLTK库提供的sent_tokenize方法进行分词 lines = sent_tokenize(open(filepath, encoding='utf-8-sig').read()) tokens = [] # 遍历所有句子 for line in lines: if len(line) > 1: tokens += get_line_ids(line, tokenizer)\",\"def get_line_ids(line: str, tokenizer: BytePairTokenizer) -> List[int]: # 对每个句子进行分词 tokens = wordpunct_tokenize(line) # 将每个词从str转换为list列表形式，同时列表末尾追加</w> tokens = [list(t) + ['</w>'] for t in tokens] ...\",\"以输入 \\\"Hello World\\\" 为例\",\"2. 对当前句子中每个词进行子词合并加词ID映射，最后得到当前句子对应的Token列表\",\"def get_line_ids(line: str, tokenizer: BytePairTokenizer) -> List[int]: ... lineids = [] for token in tokens: # 2.1 对每个词进行子词合并，直到无法合并为止 token = tokenizer.merge_bytes(token) # 2.2 将当前词列表中每个子词映射为字典中对于的词ID ids = tokenizer.get_byte_ids(token) lineids += ids sol_id = tokenizer.get_byte_id('<line/>') eol_id = tokenizer.get_byte_id('</line>') lineids = [sol_id] + lineids + [eol_id] return lineids\",\"2.1 对每个词进行子词合并，直到无法合并为止\",\" # 对当前词的子词进行合并，直到无法合并为止 def merge_bytes(self, bytes_: List[str]) -> List[str]: bytes_, merged = self.merge_max_pair(bytes_) while merged: bytes_, merged = self.merge_max_pair(bytes_) return bytes_ def merge_max_pair(self, bytes_: List[str]) -> (List[str], bool): # 1. 取出出现次数最多的字符对 max_pair = self.get_max_pair_idxs(bytes_) merged = True if max_pair is not None else False if merged: # 2. 合并该字符对 bytes_ = bytes_[:max_pair[0]] + \\\\ [''.join(bytes_[max_pair[0]:max_pair[1]+1])] + \\\\ bytes_[max_pair[1]+1:] return bytes_, merged def get_max_pair_idxs(self, bytes_) -> Tuple[int, int]: pairs = {} # 1. 遍历所有相邻字符对的组合 for i in range(1, len(bytes_)): pair = ''.join(bytes_[i-1:i+1]) # 2. 判断每个字符对是否存在于频次表中，如果存在记录出现次数 if pair in self.freqs: pairs[(i-1, i)] = self.freqs[pair] # 3. 取出出现次数最多的字符对 return None if len(pairs) == 0 else max(pairs, key=pairs.get)\",\"2.2 将当前词列表中每个子词映射为字典中对于的词ID\",\" def get_byte_ids(self, bytes_): ids = [] for byte in bytes_: if byte in self.vocab_to_idx: ids.append(self.vocab_to_idx[byte]) else: ids.append(self.vocab_to_idx[self.unk]) return ids\"]},\"597\":{\"h\":\"附录\",\"t\":[\"BPE 分词器完整代码实现:\",\"from typing import Tuple, Dict, List from collections import defaultdict import json, re from nltk import wordpunct_tokenize, sent_tokenize from tqdm import trange, tqdm class BytePairTokenizer: def __init__(self, freqs: Dict[str, int], vocab_to_idx: Dict[str, int], idx_to_vocab: Dict[int, str]): \\\"\\\"\\\" Initialize byte pair tokenizer Args: freqs: frequency dictionary of vocabulary vocab_to_index: map of vocabulary words to indices index_to_vocab: map of vocabulary indices to words \\\"\\\"\\\" self.vocab_to_idx = vocab_to_idx self.idx_to_vocab = idx_to_vocab self.freqs = freqs self.sol = '<line/>' self.eol = '</line>' self.pad = '<pad>' self.unk = '<unk>' self.eow = '</w>' def get_sol(self) -> str: return self.sol def get_eol(self) -> str: return self.eol def get_pad(self) -> str: return self.pad def get_unk(self) -> str: return self.unk def get_eow(self) -> str: return self.eow def get_byte(self, byte_id: int) -> str: return self.idx_to_vocab[byte_id] def get_byte_id(self, byte: str) -> int: unk_id = self.vocab_to_idx[self.unk] bid = self.vocab_to_idx[byte] if byte in self.vocab_to_idx else unk_id return bid def get_byte_ids(self, bytes_): \\\"\\\"\\\" Get byte ids for each byte in provided list \\\"\\\"\\\" ids = [] for byte in bytes_: if byte in self.vocab_to_idx: ids.append(self.vocab_to_idx[byte]) else: ids.append(self.vocab_to_idx[self.unk]) return ids def get_bytes(self, byte_ids: List[int]) -> List[str]: \\\"\\\"\\\" Given a list of byte ids return corresponding bytes Args: byte_ids: list of byte ids Returns: (List[str]): list of bytes \\\"\\\"\\\" tokens = [] for byte_id in byte_ids: tokens.append(self.idx_to_vocab[byte_id]) return tokens def merge_bytes(self, bytes_: List[str]) -> List[str]: \\\"\\\"\\\" Return list of bytes with max pair merged Args: bytes_: list to merge max pair in Returns: (List[str]): list of bytes with all max pair occurrences merged \\\"\\\"\\\" bytes_, merged = self.merge_max_pair(bytes_) while merged: bytes_, merged = self.merge_max_pair(bytes_) return bytes_ def merge_max_pair(self, bytes_: List[str]) -> (List[str], bool): \\\"\\\"\\\" Takes in a list of bytes and merges the max pair if possible Args: bytes_: list of bytes to merge max pair in Returns: (bytes_): list of bytes with max pair merged (bool): flag indicating whether merge occurred \\\"\\\"\\\" max_pair = self.get_max_pair_idxs(bytes_) merged = True if max_pair is not None else False if merged: bytes_ = bytes_[:max_pair[0]] + \\\\ [''.join(bytes_[max_pair[0]:max_pair[1]+1])] + \\\\ bytes_[max_pair[1]+1:] return bytes_, merged def get_max_pair_idxs(self, bytes_) -> Tuple[int, int]: \\\"\\\"\\\" Get index of maximum byte pair in list of bytes Args: bytes_: list of bytes to find maximum pair from Returns: (Tuple[int, int]): maximum frequency byte pair \\\"\\\"\\\" pairs = {} for i in range(1, len(bytes_)): pair = ''.join(bytes_[i-1:i+1]) if pair in self.freqs: pairs[(i-1, i)] = self.freqs[pair] return None if len(pairs) == 0 else max(pairs, key=pairs.get) def save(self, path: str) -> None: with open(f'{path}/freqs.json', 'w', encoding='utf-8') as outfile: json.dump(self.freqs, outfile, indent=4, ensure_ascii=False) with open(f'{path}/vocab_to_idx.json', 'w', encoding='utf-8') as outfile: json.dump(self.vocab_to_idx, outfile, indent=4, ensure_ascii=False) with open(f'{path}/idx_to_vocab.json', 'w', encoding='utf-8') as outfile: json.dump(self.idx_to_vocab, outfile, indent=4, ensure_ascii=False) @staticmethod def load(path: str) -> 'BytePairTokenizer': with open(f'{path}/freqs.json', 'r', encoding='utf-8') as infile: freqs = json.load(infile) with open(f'{path}/vocab_to_idx.json', 'r', encoding='utf-8') as infile: vocab_to_idx = json.load(infile) with open(f'{path}/idx_to_vocab.json', 'r', encoding='utf-8') as infile: idx_to_vocab = json.load(infile) return BytePairTokenizer(freqs, vocab_to_idx, idx_to_vocab) @staticmethod def train_bpe(filepaths: List[str], mincount: int, merges: int) -> 'BytePairTokenizer': vocab = create_vocab(filepaths) truncate_vocab(vocab, mincount) vocab = prepare_bpe_vocab(vocab) merged_pair_freqs = defaultdict(int) for _ in trange(merges, desc='Merging'): pairs = get_stats(vocab) if not pairs: break best_pair = max(pairs.items(), key=lambda x: x[1]) best_subword = ''.join(best_pair[0]) best_freq = best_pair[1] merged_pair_freqs[best_subword] += best_freq vocab = merge_vocab(best_pair[0], vocab) freqs = count_byte_freqs(vocab) vocab_to_idx, idx_to_vocab = create_vocab_maps(freqs) freqs.update(merged_pair_freqs) return BytePairTokenizer(freqs, vocab_to_idx, idx_to_vocab) def create_vocab(filepaths: List[str]) -> Dict[str, int]: \\\"\\\"\\\" Create dictionary of vocabulary frequencies in given list of files Args: filepaths: list of filepaths to collect vocabulary from Returns: (Dict[str, int]): dictionary mapping vocabulary terms to their frequency \\\"\\\"\\\" vocab = defaultdict(int) for path in tqdm(filepaths, desc='Creating vocabulary'): text = open(path, 'r', encoding='utf-8-sig').read() sentences = sent_tokenize(text) for sentence in sentences: tokens = wordpunct_tokenize(sentence) for token in tokens: vocab[token] += 1 return vocab def truncate_vocab(vocab: Dict[str, int], mincount: int) -> None: \\\"\\\"\\\" Truncate vocabulary dictionary based on a minimum count Args: vocab: frequency mapping dictionary to truncate mincount: minimum count for members of dictionary (words with lower frequencies will be removed) \\\"\\\"\\\" tokens = list(vocab.keys()) for token in tokens: if vocab[token] < mincount: del(vocab[token]) def prepare_bpe_vocab(vocab: Dict[str, int]) -> Dict[str, int]: \\\"\\\"\\\" Prepare vocabulary frequency dictionary for byte-pair generation. End-of-word byte '</w>' added to words, every character separated by space Args: vocab: vocabulary frequency dictionary to prepare Returns: (Dict[str, int]): byte-pair ready vocabulary frequency dictionary \\\"\\\"\\\" bpe_vocab = {} for token in vocab: ntoken = ' '.join(list(token)) + ' </w>' bpe_vocab[ntoken] = vocab[token] return bpe_vocab def get_stats(vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]: \\\"\\\"\\\" Count all bytepairs in a dictionary containing vocabulary frequencies Args: vocab: dictionary mapping words to their frequency Returns: (Dict[Tuple[str, str], int]): dictionary containing byte pair frequencies \\\"\\\"\\\" pairs = defaultdict(int) for word, freq in vocab.items(): symbols = word.split() for i in range(len(symbols)-1): pairs[symbols[i],symbols[i+1]] += freq return pairs def merge_vocab(pair: Tuple[str, str], v_in: Dict[str, int]) -> Dict[str, int]: \\\"\\\"\\\" Merge all instances of given byte pair in vocabulary frequency dictionary Args: pair: byte pair to merge v_in: vocabulary to merge byte pair int Returns: (Dict[str, int]): resulting vocabulary with all instances of given byte pair merged \\\"\\\"\\\" bigram = re.escape(' '.join(pair)) v_out = {} p = re.compile(r'(?<!\\\\S)' + bigram + r'(?!\\\\S)') for word in v_in: w_out = p.sub(''.join(pair), word) v_out[w_out] = v_in[word] return v_out def count_byte_freqs(vocab: Dict[str, int]) -> Dict[str, int]: freqs = defaultdict(int) for word in vocab: bytes_ = word.split(' ') for byte in bytes_: freqs[byte] += 1 for token in ['<line/>', '</line>', '<pad>', '<unk>']: freqs[token] += 1 return freqs def create_vocab_maps(freqs: Dict[str, int]) -> (Dict[str, int], Dict[int, str]): \\\"\\\"\\\" Create map of vocabulary terms to indices and vice versa. Word indices are in order of their frequency in the provided vocabulary Args: freqs: dictionary mapping vocabulary terms to their frequencies Returns: (Dict[str, int]): dictionary mapping vocab to indices (Dict[int, str]): dictionary mapping indices to vocab \\\"\\\"\\\" ordered_freqs = sorted(freqs.items(), key=lambda x: x[1], reverse=True) vocab_to_idx, idx_to_vocab = {}, {} for i in range(len(ordered_freqs)): word, freq = ordered_freqs[i] vocab_to_idx[word] = i idx_to_vocab[i] = word return vocab_to_idx, idx_to_vocab\"]},\"598\":{\"h\":\"其他方向\"},\"599\":{\"h\":\"大模型微调(Fine Tuning)知识扫盲\",\"t\":[\"大模型微调(Fine Tuning)知识扫盲\"]},\"600\":{\"h\":\"什么是大模型 ？\",\"t\":[\"开始之前，为了方便大家理解，我们先对大模型做一个直观的抽象。\",\"本质上，现在的大模型要解决的问题，就是一个序列数据转换的问题：\",\"输入序列 X = [x1, x2, ..., xm]\",\"输出序列Y = [y1, y2, …, yn]\",\"X和Y之间的关系是：Y = WX。\",\"我们所说的“大模型”这个词：“大”是指用于训练模型的参数非常多，多达千亿、万亿；而“模型”指的就是上述公式中的矩阵W。\",\"在这里，矩阵W就是通过机器学习，得出的用来将X序列，转换成Y序列的权重参数组成的矩阵。\",\"需要特别说明：这里为了方便理解，做了大量的简化。在实际的模型中，会有多个用于不同目的的权重参数矩阵，也还有一些其它参数。\"]},\"601\":{\"h\":\"为什么要对大模型进行微调 ？\",\"t\":[\"通常，要对大模型进行微调，有以下一些原因：\",\"因为大模型的参数量非常大，训练成本非常高，每家公司都去从头训练一个自己的大模型，这个事情的性价比非常低；\",\"Prompt Engineering的方式是一种相对来说容易上手的使用大模型的方式，但是它的缺点也非常明显。因为通常大模型的实现原理，都会对输入序列的长度有限制，Prompt Engineering 的方式会把Prompt搞得很长。\",\"越长的Prompt，大模型的推理成本越高，因为推理成本是跟Prompt长度的平方正向相关的。\",\"另外，Prompt太长会因超过限制而被截断，进而导致大模型的输出质量打折口，这也是一个非常严重的问题。\",\"对于个人使用者而言，如果是解决自己日常生活、工作中的一些问题，直接用Prompt Engineering的方式，通常问题不大。\",\"但对于对外提供服务的企业来说，要想在自己的服务中接入大模型的能力，推理成本是不得不要考虑的一个因素，微调相对来说就是一个更优的方案。\",\"Prompt Engineering的效果达不到要求，企业又有比较好的自有数据，能够通过自有数据，更好的提升大模型在特定领域的能力。这时候微调就非常适用。\",\"要在个性化的服务中使用大模型的能力，这时候针对每个用户的数据，训练一个轻量级的微调模型，就是一个不错的方案。\",\"数据安全的问题。如果数据是不能传递给第三方大模型服务的，那么搭建自己的大模型就非常必要。通常这些开源的大模型都是需要用自有数据进行微调，才能够满足业务的需求，这时候也需要对大模型进行微调。\"]},\"602\":{\"h\":\"如何对大模型进行微调 ？\",\"t\":[\"从参数规模的角度，大模型的微调分成两条技术路线：\",\"一条是对全量的参数，进行全量的训练，这条路径叫全量微调FFT(Full Fine Tuning)。\",\"一条是只对部分的参数进行训练，这条路径叫PEFT(Parameter-Efficient Fine Tuning)。\",\"FFT的原理，就是用特定的数据，对大模型进行训练，将W变成，相比W ，最大的优点就是上述特定数据领域的表现会好很多。\",\"但FFT也会带来一些问题，影响比较大的问题，主要有以下两个：\",\"一个是训练的成本会比较高，因为微调的参数量跟预训练的是一样的多的；\",\"一个是叫灾难性遗忘(Catastrophic Forgetting)，用特定训练数据去微调可能会把这个领域的表现变好，但也可能会把原来表现好的别的领域的能力变差。\",\"PEFT主要想解决的问题，就是FFT存在的上述两个问题，PEFT也是目前比较主流的微调方案。\",\"从训练数据的来源、以及训练的方法的角度，大模型的微调有以下几条技术路线：\",\"监督式微调SFT(Supervised Fine Tuning) : 用人工标注的数据，用传统机器学习中监督学习的方法，对大模型进行微调；\",\"基于人类反馈的强化学习微调RLHF(Reinforcement Learning with Human Feedback) : 把人类的反馈，通过强化学习的方式，引入到对大模型的微调中去，让大模型生成的结果，更加符合人类的一些期望；\",\"基于AI反馈的强化学习微调RLAIF(Reinforcement Learning with AI Feedback) : 原理大致跟RLHF类似，但是反馈的来源是AI。这里是想解决反馈系统的效率问题，因为收集人类反馈，相对来说成本会比较高、效率比较低。\",\"不同的分类角度，只是侧重点不一样，对同一个大模型的微调，也不局限于某一个方案，可以多个方案一起。\",\"微调的最终目的，是能够在可控成本的前提下，尽可能地提升大模型在特定领域的能力。\"]},\"603\":{\"h\":\"常用的PEFT方案\",\"t\":[\"从成本和效果的角度综合考虑，PEFT是目前业界比较流行的微调方案。接下来介绍几种比较流行的PEFT微调方案。\"]},\"604\":{\"h\":\"Prompt Tuning\",\"t\":[\"Prompt Tuning的出发点，是基座模型(Foundation Model)的参数不变，为每个特定任务，训练一个少量参数的小模型，在具体执行特定任务的时候按需调用。\",\"Prompt Tuning的基本原理是在输入序列X之前，增加一些特定长度的特殊Token，以增大生成期望序列的概率。\",\"具体来说，就是将变成，。\",\"Prompt Tuning是发生在Embedding这个环节的。如果将大模型比做一个函数：，那么Prompt Tuning就是在保证函数本身不变的前提下，在X前面加上了一些特定的内容，而这些内容可以影响X生成期望中Y的概率。\",\"Prompt Tuning的具体细节,可以参见：The Power of Scale for Parameter-Efficient Prompt Tuning。\"]},\"605\":{\"h\":\"Prefix Tuning\",\"t\":[\"Prefix Tuning的灵感来源是，基于Prompt Engineering的实践表明，在不改变大模型的前提下，在Prompt上下文中添加适当的条件，可以引导大模型有更加出色的表现。\",\"Prefix Tuning的出发点，跟Prompt Tuning的是类似的，只不过它们的具体实现上有一些差异。\",\"Prompt Tuning是在Embedding环节，往输入序列X前面加特定的Token。而Prefix Tuning是在Transformer的Encoder和Decoder的网络中都加了一些特定的前缀。\",\"具体来说，就是将Y=WX中的W，变成。\",\"Prefix Tuning也保证了基座模型本身是没有变的，只是在推理的过程中，按需要在W前面拼接一些参数。\",\"Prefix Tuning的具体细节,可以参见：Prefix-Tuning: Optimizing Continuous Prompts for Generation。\"]},\"606\":{\"h\":\"LoRA\",\"t\":[\"LoRA是跟Prompt Tuning和Prefix Tuning完全不相同的另一条技术路线。\",\"LoRA背后有一个假设：我们现在看到的这些大语言模型，它们都是被过度参数化的。而过度参数化的大模型背后，都有一个低维的本质模型。\",\"通俗讲人话：大模型参数很多，但并不是所有的参数都是发挥同样作用的；大模型中有其中一部分参数，是非常重要的，是影响大模型生成结果的关键参数，这部分关键参数就是上面提到的低维的本质模型。\",\"LoRA的基本思路，包括以下几步：\",\"首先, 要适配特定的下游任务，要训练一个特定的模型，将Y=WX变成Y=(W+∆W)X，这里面∆W主是我们要微调得到的结果；\",\"其次，将∆W进行低维分解∆W=AB (∆W为m * n维，A为m * r维，B为r * n维，r就是上述假设中的低维)；\",\"接下来，用特定的训练数据，训练出A和B即可得到∆W，在推理的过程中直接将∆W加到W上去，再没有额外的成本。\",\"另外，如果要用LoRA适配不同的场景，切换也非常方便，做简单的矩阵加法即可：(W + ∆W) - ∆W + ∆W'。\",\"关于LoRA的具体细节,可以参见LoRA: Low-Rank Adaptation of Large Language Models。\"]},\"607\":{\"h\":\"QLoRA\",\"t\":[\"LoRA 效果已经非常好了，可以媲美全量微调的效果了，那为什么还要有个QLoRA呢？\",\"这里先简单介绍一下，量化（Quantization）。\",\"量化，是一种在保证模型效果基本不降低的前提下，通过降低参数的精度，来减少模型对于计算资源的需求的方法。\",\"量化的核心目标是降成本，降训练成本，特别是降后期的推理成本。\",\"QLoRA就是量化版的LoRA，它是在LoRA的基础上，进行了进一步的量化，将原本用16bit表示的参数，降为用4bit来表示，可以在保证模型效果的同时，极大地降低成本。\",\"论文中举的例子，65B的LLaMA的微调要780GB的GPU内存；而用了QLoRA之后，只需要48GB。效果相当惊人！\",\"关于QLoRA的具体细节,可以参见：QLoRA: Efficient Finetuning of Quantized LLMs。\",\"PEFT 的微调方法，还有很多种，限于篇幅原因，不再这里一一介绍。感兴趣的朋友，可以阅读这篇论文：Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning。\",\"相关阅读资料:\",\"近代自然语言处理技术发展的“第四范式”\",\"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\"]},\"608\":{\"h\":\"通俗易懂讲解LoRA微调\",\"t\":[\"通俗易懂讲解LoRA微调\",\"论文链接: LoRA: Low-Rank Adaptation of Large Language Models\"]},\"609\":{\"h\":\"符合认知的大模型微调流程\",\"t\":[\"符合我们直接观念所想的大模型微调流程为:\",\"准备与下游任务相关的数据集\",\"选择合适的预训练好的大模型\",\"在特定任务相关的数据集上执行有监督全量参数微调，将预训练模型的参数 调整为适合下游任务的 \",\"其中第三步通过反向传播全量更新模型参数的过程如下：\",\"图1: 反向传播更新模型参数过程\",\"上述的全量微调流程问题在于大模型的参数量往往特别大，也就是 占据了特别大的内存资源和计算资源，有没有办法能够减少 所占的内存资源和计算资源呢？\",\"图2: 低秩分解\",\"我们可以利用矩阵分解技术，将原始的 矩阵从 参数量降低到 级别的参数量，如下图所示:\",\"图3: 待微调的参数量下降到原来的9%\",\"这样一来，我们微调大模型的流程就变为了(前两步不变)：\",\"初始化低秩矩阵：对于需要微调的密集层，初始化两个低秩矩阵 和 ，其维度分别为 和 ，其中 是低秩的秩，远小于原始矩阵的维度。\",\"冻结预训练模型参数：在微调过程中，保持预训练模型的原始参数 不变，只对低秩矩阵 和 进行训练。\",\"执行微调训练：在准备好的数据集上，通过反向传播算法更新低秩矩阵 和 的参数，使得模型在下游任务上的表现逐渐优化。\",\"合并参数（可选）：在微调完成后，如果需要，可以将低秩矩阵 和 的更新量与原始参数 合并，得到最终适用于下游任务的模型参数 。\",\"这在LoRA这篇论文中也被称为低秩分解自适应技术。\",\"图4: 常规微调 VS LoRA微调\"]},\"610\":{\"h\":\"大模型微调大致发展历史\",\"t\":[\"大公司或者研究机构，都是有足够资源的来开发大模型，但是对于一般的小公司或者个人来说，要想开发自己的大模型几乎不可能，要知道像 ChatGPT 这样的大模型，一次训练的成本就在上千亿美元。\",\"那么那些小公司或者个人，又怎么能够利用这些开源的大模型，在自己的数据上继续训练，从而应用于自己的业务场景？有没有低成本的方法微调大模型？\",\"答案是有的。目前主流的方法包括2019年 Houlsby N 等人提出的 Adapter Tuning，2021年微软提出的 LORA，斯坦福提出的 Prefix-Tuning，谷歌提出的 Prompt Tuning，2022年清华提出的 P-tuning v2。\",\"这些方法都有各自的特点，从个人使用情况来说，LORA 的效果会好于其它几种方法。其它方法都有各自的一些问题：\",\"Adapter Tuning 增加了模型层数，引入了额外的推理延迟\",\"Prefix-Tuning 难于训练，且预留给 Prompt 的序列挤占了下游任务的输入序列空间，影响模型性能\",\"P-tuning v2 很容易导致旧知识遗忘，微调之后的模型，在之前的问题上表现明显变差\",\"基于上述背景，LORA 得益于前人的一些关于内在维度（intrinsic dimension）的发现：\",\"模型是过参数化的，它们有更小的内在维度，模型主要依赖于这个低的内在维度（low intrinsic dimension）去做任务适配。\",\"假设模型在任务适配过程中权重的改变量是低秩（low rank）的，由此提出低秩自适应（LoRA）方法。\",\"LoRA 允许我们通过优化适应过程中密集层变化的秩分解矩阵，来间接训练神经网络中的一些密集层，同时保持预先训练的权重不变。\"]},\"611\":{\"h\":\"LoRA 微调\",\"t\":[\"图5: LoRA 微调流程\",\"LoRA 的思想很简单:\",\"在 LoRA 论文中，在原始预训练语言模型（Pre - trained Language Model，简称 PLM）旁添加一条旁路，进行一次降维再升维的操作，以此来模拟所谓的内在秩（intrinsic rank）。\",\"训练的时候固定 PLM 的参数，只训练降维矩阵 A 与升维矩阵 B 。而模型的输入输出维度不变，输出时将 BA 与 PLM 的参数叠加。\",\"用随机高斯分布初始化 A ，用 0 矩阵初始化 B ，保证训练的开始此旁路矩阵依然是 0 矩阵。\",\"图6: 随机高斯分布初始化 A ，用 0 矩阵初始化 B\",\"假设要在下游任务微调一个预训练语言模型（如 GPT-3），则需要更新预训练模型参数，公式表示如下：\",\"其中， 是预训练模型初始化的参数， 就是需要更新的参数。如果是全参数微调，则它的参数量 （如果是 GPT-3，则 ）。从这可以看出要全参数微调大语言模型，代价是非常高的。\",\"而对于 LoRA 来说，只需要微调 。\",\"具体来看，假设预训练的矩阵为 ，它的更新可表示为：\",\"其中秩 。\",\"在 LoRA 的训练过程中， 是固定不变的，只有 和 是训练参数。\",\"在前向过程中， 与 都会乘以相同的输入 ，最后相加：\",\"LoRA 的这种思想有点类似于残差连接，同时使用这个旁路的更新来模拟 Full Fine-Tuning 的过程。并且，Full Fine-Tuning 可以被看作是 LoRA 的特例（当 等于 时）。\",\"在推理过程中，LoRA 也几乎未引入额外的 Inference Latency，只需要计算 即可。\",\"LoRA 与 Transformer 的结合也很简单，仅在 QKV Attention 的计算中增加一个旁路。\"]},\"612\":{\"h\":\"矩阵A和B为什么不能同时为零？\",\"t\":[\"在前面我们介绍了，用随机高斯分布初始化 ，用 0 矩阵初始化 。矩阵 为什么不也用 0 初始化？\",\"这主要是因为如果矩阵 也用 0 初始化，那么矩阵 的梯度就始终为 0，无法更新参数，导致 。这里简单推理一下。\",\"对于 ，设 ，则：\",\"因此：\",\"如果矩阵 也用 0 初始化，那么上面的梯度就变成了 0，所以矩阵 不能用 0 初始化。\",\"同样，我们看一下矩阵 初始化为 0 的影响。\",\"由于矩阵 的参数会发生更新，而 矩阵又不是 0 矩阵，因此后面 ，所以矩阵 可以用 0 初始化。\"]},\"613\":{\"h\":\"秩的选择\",\"t\":[\"论文实验结果显示，对于一般的任务， r=1,2,4,8 就足够了。而一些领域差距比较大的任务可能需要更大的 r 。\",\"同时，增加 r 值变大并不能提升微调的效果，这可能是因为参数量增加需要更多的语料。\",\"图7: 秩的选择\"]},\"614\":{\"h\":\"注意\",\"t\":[\"q进行 LoRA 高效的模型微调，重点是保持参数尺寸最小化。\",\"使用 PEFT 库来实现 LoRA，避免复杂的编码需求。\",\"将 LoRA 适应扩展到所有线性层，增强整体模型的能力。\",\"保持偏置和层归一化可训练，因为它们对模型的适应性至关重要，并且不需要低秩适应。\",\"应用量化低秩适应（Quantized LoRA，简称 QLoRA）以节省 GPU 显存并训练模型，从而能够训练更大的模型。\",\"量化是一种在深度学习领域用于减少模型内存占用和计算量的技术。在模型训练和推理过程中，神经网络的权重矩阵通常以高精度的浮点数（如 32 位浮点数）形式存储和计算，这会占用大量的内存资源并消耗较多的计算资源。量化通过将这些高精度的浮点数转换为低精度的整数（如 4 位或 8 位整数）来实现数据的压缩。\",\"在 LoRA 微调的场景中，QLoRA 就是利用量化技术的一个变体。它将权重矩阵量化为 4 位或 8 位整数，在不损失太多性能的情况下减少了模型的大小，使得模型可以在资源有限的设备上进行训练和部署，同时还能适应更多的参数。\",\"基于大模型的内在低秩特性，增加旁路矩阵来模拟全参数微调，LoRA 通过简单有效的方案来达成轻量微调的目的。它的应用自不必提，可以将现在的各种大模型通过轻量微调变成各个不同领域的专业模型。\",\"此外，考虑 OpenAI 对 GPT 模型的认知，GPT 的本质是对训练数据的有效压缩，从而发现数据内部的逻辑与联系，LoRA 的思想与之有相通之处，原模型虽大，但起核心作用的参数是低秩的，通过增加旁路，达到四两拨千斤的效果。\"]},\"615\":{\"h\":\"Prompt Engineering 知识扫盲\",\"t\":[\"Prompt Engineering 知识扫盲\"]},\"616\":{\"h\":\"什么是Prompt Engineering?\",\"t\":[\"Prompt (提示词) 是人类发给各种人工智能模型、用以完成特定任务的指令。\",\"Prompt Engineering (提示词工程) 是指我们为了让LLM能够更好地完成我们给它的任务，我们对Prompt进行优化、调整的过程。\",\"可能会有人这么问，LLM已经这么强了，直接丢给它个指令，让他去执行就好了，为什么还需要Prompt Engineering呢？\",\"确实像OpenAI的GPT4这样的LLM已经非常强了，很多简单的任务，我们直接用自然语言丢给他就去执行就好了。但是，对于一些复杂的问题，Prompt写得好不好，直接影响着大模型给出答案的正确与否。\",\"本质上，LLM是一个概率模型，它只是在给定的信息的前提下，给出概率最大的结果，它并不保证结果的合理性和正确性。\",\"要让LLM给出的结果尽可能地合理、正确，这是我们使用LLM的人的职责。\",\"这就是我们要去学习Prompt Engineering的原因。\"]},\"617\":{\"h\":\"如何写好Prompt?\"},\"618\":{\"h\":\"要明确,要具体\",\"t\":[\"我们发给LLM的批令，越明确、越具体，对于LLM越友好。\",\"举个例子，我们让LLM对一段文字进行总结：\",\"Prompt 2相比Prompt 1，对输出有了更加明确具体的要求，这样LLM输出的内容也会更加贴合我们的需求。另外，我们还用了'###'作为分隔符，进一步帮LLM明确要求。\",\"我们在给LLM发指令的时候，第一个关键点，就是我们要把给LLM做的任务尽可能细化，把要求尽可能明确、具体地描述出来。\"]},\"619\":{\"h\":\"给LLM更多的时间去思考\",\"t\":[\"《思考快与慢》这本书里介绍了我们人类大脑的“系统1”和“ 系统2”。\",\"系统1是快思考系统，反应很快，但可能会出错。\",\"系统2是慢思考系统，需要更长的反应时间，进行思考、推理，但结果会更加靠谱。\",\"默认情况下，LLM就像是一个快思考的系统，他利用自己已掌握的知识，快速给出答案，但并不能保证结果的正确性。\",\"为了让LLM给出的答案更加靠谱，我们需要通过Prompt Engineering 的方式，把LLM的慢思考调动起来。\",\"这就是“给LLM更多的时间去思考”背后的大致逻辑。\",\"给LLM更多的时间去思考，一个简单的技巧是在你的Prompt后面，加上这样一句话“Let’s think step by step”。这句话会引导LLM，会去分步骤思考，效果会比不加这句话要好。\",\"另一个技巧，在Prompt中加入一些例子，让LLM照着例子进行推理、思考。这一块的技巧性很强，我们在接下来的部分，介绍几种具体的技巧。\"]},\"620\":{\"h\":\"思维链技术：Chain-of-Thought\",\"t\":[\"这是《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》这篇论文里讲的一个Prompt Engineering的技巧。\",\"CoT(Chain-of-Thought) 的核心思想是，在Prompt中加入一些示例，来引导LLM展现出更好的推理能力。\",\"这里的关键是在Prompt中加入的示例，在这些示例中，我们会用自然语言描述一系列的推理过程，并最终引导出示例问题的正确结果。\",\"这个过程有点像，我们教小孩做应用题，我们先给小孩子分析讲解一些示例。然后再把新的问题让小孩子来解决。小孩子根据从示例中学习到的推理、分析能力，最终解出了新的问题。\",\"下面我们来看论文中给的CoT的例子：\",\"左侧是常规的Prompt，右侧是CoT Prompt\",\"蓝色标记出的部分是提供给LLM的示例。绿色标记出的部分是LLM输出的推理过程。\",\"在使用CoT这种Prompt Engineering技巧的时候，有几个注意点：\",\"CoT是LLM足够大（参数足够多，通常是在1000亿参数）时才涌现出来的能力。因此，在一些不够大的LLM上，CoT的效果并不明显。\",\"通常，在Prompt中加入的示例不是1条，而是多条。具体要考虑解决的问题类型，以及Prompt的长度（因为LLM的Prompt长度通常都是有长度限制的）。\"]},\"621\":{\"h\":\"自一致性技术：Self-Consistency\",\"t\":[\"这是《Self-Consistency Improves Chain of Thought Reasoning in Language Models》 这篇论文里讲的另一个Prompt Engineering的技巧。\",\"Self-Consistency技术是在CoT技术的基础之上，进行的进一步优化，目的是为了让LLM的推理能力能够更进一步提升。\",\"Self-Consistency的大致原理是这样：\",\"利用CoT Prompting技巧，写好Prompt；\",\"不要让LLM只生成最合适的唯一一个结果，而是利用LLM结果的多样性，生成多种不同推理路径所得的结果的集合；\",\"从结果集合中投票选择，选出投票最多的结果，做为最终的答案。\",\"这里有像我们人类解决问题的过程，如果我们用多种不同的方法去求解，大多数方法求解出来结果都一样的答案，那很有可能就是我们最终的答案。\",\"下面我们来看论文中给的Self-Consistency的例子：\",\"在上面的例子中，虚线之上是标准的CoT的过程，它得到的结果是错的。虚线之下是Self-Consistency的过程，得到的三个答案中，有1个是错的，有2个是正确的。最终答案是大多数投票的结果，是正确的。\"]},\"622\":{\"h\":\"从易至难技术：Least-to-Most\",\"t\":[\"这是《Least-to-Most Prompting Enables Complex Reasoning in Large Language Models》 这篇论文中介绍的方法。\",\"CoT的特点是同类型问题的迁移思考，因此，如果给的例子是比较简单的问题，而给的问题却是难度大很多的问题，这时候CoT的效果就不尽如人意。\",\"LtM(Least-to-Most)主是为了解决CoT这种从易到难的迁移能力不足而诞生的。\",\"LtM的核心思想是：教LLM把复杂问题，拆解成一系列的简单问题，通过解决这一系列的简单问题，来最终得到复杂问题的结果。\",\"LtM的过程包含两个阶段：\",\"分解阶段：把复杂问题分解成一系列的简单子问题。这个阶段的Prompt中要包含分解问题的示例，要和分解的问题；\",\"解决子问题阶段：这个阶段的Prompt中包含三部分内容：一是完整的LtM的例子；二是已解决的子问题及其答案列表；三是接下来要解答的子问题。\",\"这里也非常像我们人类学习解决复杂问题的过程，我们通过把复杂问题拆解成一个个的简单问题，通过把一个个的简单问题解决掉，最终把复杂问题也解决了。\",\"下面我们来看看论文中LtM的例子：\",\"从上图中，我们可以对LtM Prompting有一个直观的认知，通过引导LLM解决子问题，一步步引导LLM得出复杂问题的结果。\"]},\"623\":{\"h\":\"应用层\"},\"624\":{\"h\":\"GPT-1 论文\",\"t\":[\"GPT-1 论文\",\"论文: Improving Language Understanding by Generative Pre-Training\"]},\"625\":{\"h\":\"摘要\",\"t\":[\"自然语言理解包含了广泛的多样性任务，比如文本蕴涵，问答，语义相似度评估，文本分离。然而大规模的未标注的文本语料是丰富，而特定任务学习的标注数据有非常少，使得要充分做区分地训练模型非常有挑战性。作者证明通过在丰富的无标签文本语料库生成预训练generative pre-training语言模型，然后在每项具体任务上判别性微调discriminative fine-tuning，可以实现巨大的收益。对比之前的方法，作者在微调阶段使用任务感知的输入转换来实现有效的迁移，仅仅需要小小修改模型架构。通用的任务未知task-agnostic模型优于那些为每个任务精心设计的模型，在12个研究任务中9个提升到SOTA。如，作者在常识推理(Stories Close Test)上提升8.9%， 在问答上提升5.7%(RACE)，文本蕴含提升1.5%(MultiNLI)。\"]},\"626\":{\"h\":\"简介\",\"t\":[\"在NLP中，有效地从无标记的原始文本中学习的能力能减轻对监督学习的依赖。大部分深度学习方法需要大量人工标注的数据，这限制了它们在许多缺乏标记数据领域的适用性。在这种情况下，模型能从无标记数据中充分利用语义信息，为收集更多的标注数据提供了更多一个有价值的替代方案，标注数据昂贵又耗时。进一步来说，即便是那些有大量标注数据的场景，无监督学习得到的好的表示也能提供显著的提升。最有说服力的证据就是到目前为止大量使用预训练的词嵌入来提升一系列NLP任务表现。\",\"无论到什么程度，从无标注文本中充分利用词级别以外的信息是有挑战性的，有两个主要原因。\",\"不清楚在学习文本表示时，什么样的优化目标是最高效的迁移。近期研究考虑过各种各样的目标，如语言模型，机翻，语句连贯性，每种方法在不同任务上都优于其它方法。\",\"对于将这些学习到的表征迁移到目标任务的最有效方法，目前还没有达成共识。已有的技术涉及对模型架构进行特定任务的修改、使用复杂的学习方案以及添加辅助学习目标的组合。这些不确定性使得开发有效的语言处理半监督学习方法变得困难。\",\"在本文中，作者用无监督的预训练和监督的微调组合来探索关于语言理解任务半监督方法。目标是学习一个全局表示，迁移它来稍微适应一系列广泛的任务。作者假设采用一个大型无标记文本语料库和几个人工标记训练样本的数据集(目标任务)。该设置不需要这些目标任务和无标记语料库是一个领域的。并采用两段式训练流程。首先，在无标记数据上使用语言模型目标来学习神经网络初始化的参数。接着，使用对应特定任务的监督目标来调整这些参数。(预训练+微调)\",\"对于作者的模型架构，使用的是Transformer，它被证明在许多任务上有很强的表现，如机翻，文本生成，句法解析。该模型在文本上处理长期依赖提供了更结构化的内存，相比其他替代方案如RNN，Transformer跨各种各样任务的迁移性能更强。在迁移阶段，作者利用源于遍历式(traversal-style)方法的特定任务的输入改写，其将结构化文本输入处理为单一的连续字符序列。如作者在实验中证明的，这些改写使得在预训练模型架构上用最小的修改就会有效。\",\"作者在四种类型的语言理解任务评估作者的方法——自然语言推断NLI，问答，语义相识度，和文本分类。作者通用的任务未知task-agnostic模型优于那些为每个任务精心设计的模型，在12个研究任务中9个提升到SOTA。\",\"常识推理提升8.9%(Stories Cloze Test)\",\"问答提升5.6%(RACE)\",\"文本蕴含提升5.5%(MultiNLI)\",\"GLUE多任务提升5.5%.\",\"也分析了在四种不同设置下预训练模型的零次(zero-shot)表现，证明其确实为下游任务获取到了有用的语言知识。\"]},\"627\":{\"h\":\"相关工作\",\"t\":[\"NLP半监督学习: 预训练对于获取不同级别信息的需要，如从词级别信息到更高的(段落级别或者句子级别的)词嵌入。\",\"无监督预训练: 无监督预训练+监督微调方式，Transformer比LSTM能获取长距离信息。\",\"辅助训练目标: 添加一个无监督训练目标是半监督学习的一种替代形式。如POS tag，语义组块chuking, NER， 以及语言模型来提升标记的语义角色。\"]},\"628\":{\"h\":\"框架\",\"t\":[\"作者训练流程有两个阶段:\",\"在大规模文本语料上学习高容量的语言模型\",\"微调阶段，用标记的数据对特定任务微调模型\"]},\"629\":{\"h\":\"无监督预训练\",\"t\":[\"给定一个无监督学习的语料tokens ，使用标准的语言模型目标并最大化其似然：\",\"这里 是上下文窗口大小，条件概率 是参数 的神经网络模型。这些参数会以随机梯度下降训练。\",\"在作者的实验中，语言模型使用多层的 Transformer decoder（Transformer 的变种 ）。该模型在上下文 token 上使用多头自注意力操作，接一个逐位置的前馈层来生成目标字符的分布输出（比原本少了一个多头自注意力 ）：\",\"公式如下：\",\"这里 是上下文字符向量， 是层数， 是字符嵌入矩阵， 是位置嵌入矩阵。\"]},\"630\":{\"h\":\"有监督微调\",\"t\":[\"在训练公式 （ 1 ） 中的目标函数模型后，作者在监督学习目标任务上调整参数。假设有标记数据集 ，每个实例有输入字符的序列构成 ，对应着标签 。输入通过作者的预训练模型会得到最好的 transformer block 的激活状态 ，将其喂进一个参数为 的添加的线性输出层来预测 有：\",\"给出最大化的目标函数为：\",\"作者还发现加入语言模型作为辅助目标来微调有助于学习：(a) 提升监督模型的泛化能力；(b) 加速收敛。这跟之前的工作一样，观测发现用辅助目标能提升性能。尤其是，作者用以下优化（加权 ）目标：\",\"总之，作者额外要微调的参数只有 ，以及分割字符嵌入矩阵。\"]},\"631\":{\"h\":\"特定任务输入转换\",\"t\":[\"文本分类：直接微调模型\",\"问答或文本蕴含：输入是结构化的，如有序句子对，三元组（文档，问题和答案）\",\"因为作者的预训练模型是用连续的文本序列训练的，需要做些修改以便用在这些任务上。之前的工作提出了在迁移表征顶部学习特定任务的架构。这种方法重新引入了大量特定任务的定制化输入，并且不会对额外的架构组件使用迁移学习。相反，作者使用遍历式方法，就是将结构化输入转换为有序序列以便作者预训练能处理。这些输入转换使作者避免跨任务架构的大改。作者在下面部分和可视化插图 1 提供了这些输入的简洁描述。所有的转换包括添加随机初初始化的开始和结束标记 。\",\"文本蕴含：拼接前提文本 和假设 为 token 序列，用 $ 符来分隔两者。\",\"相似度：对于相似任务，两个比较的句子没有内在顺序。为了反映这点，作者修改输入序列来包含 2 种可能的顺序（用分隔符分隔），并独立地处理 2 个序列表示 ，逐元素相加然后送入线性输出层。\",\"问答和常识推理：对于这些任务，给定文档 ，一个问题 和一个可能答案集 。将文档和问题跟每个可能答案拼接起来，再在其中添加一个分隔符得到 。每个这些序列用作者的模型独立处理后通过一个 softmax 层归一化来生成所有可能答案的分布。\"]},\"632\":{\"h\":\"实验\"},\"633\":{\"h\":\"设置\",\"t\":[\"无监督预训练：BOOKS CORPUS 数据集预训练模型。长文本能让生成模型学习到长依赖信息的条件概率。ELMO 方法处理 1B Word benchmarks，在句子级别打乱顺序以破坏长距离结构信息，达到非常低的 18.4 困惑度。\",\"模型的具体配置:\",\"Transformer 架构：12 层有自注意力头（768 维隐藏层，12 个注意力头）transformer decoder 结构。\",\"逐位置前馈神经网络（position-wise feed-forward networks）：3072 维内部隐藏层。\",\"Adam 优化器方案：最大 lr=2.5e-4。开始 2000 次从 0 线性上升更新，再使用 cosine 方案退火到 0。\",\"采样与训练：从 512 连续 tokens 中随机采样得到 64 小批次样本，训练 100 轮。\",\"层归一化：改进版的 layerNorm，以 权重初始化。\",\"词汇表与正则化：40,000 合并的 BPE 词汇表，残差，嵌入和注意力层以 0.1 的 Dropout 来正则化。\",\"改进版 L2 正则：所有无偏差或增益权重设置为 。\",\"激活函数：GELU 作为激活函数。\",\"位置嵌入：使用学习的位置嵌入，而不是原始 Transformer 的正余弦曲线。\",\"数据清洗与分词：使用 ftfy 清洗原始 BooksCorpus，去掉字符和空格，再使用 spaCy tokenize。\",\"微调的细节:\",\"除非指定，使用无监督预训练超参数设置。分类层使用 0.1 的 Dropout。大部分任务，lr=6.25e-5，批大小为 32。在大部分任务中基本上 3 轮训练就足够了。lr 用以训练步数的 0.2% 预热衰减方案。 设置为 0.5。\"]},\"634\":{\"h\":\"监督微调\",\"t\":[\"微调任务和数据集如下：\",\"NLI 就是识别文本蕴含。涉及读取一对句子，判断它们之间的关系，是蕴含，矛盾或中立。因为存在各种变化现象，如词汇蕴含，共指，词汇和句法歧义，所以还是很有挑战性的。\",\"下表2是作者模型和之前SOTA模型NLI的结果比较：\",\"RTE数据集比较小，只有2490样本，只达到了56.0%准确率。\",\"问答和常识推理 结果如下表3，RACE数据集由初高中考试题构成。在Story Cloze和RACE提升明显。证明模型具有有效处理上下文长距离的信息的能力。\",\"语义相似度 语义相似度(或释义发现)任务涉及预测两个句子在语义上是否相等。挑战在于识别语句是否是概念改写，理解反面，处理句法歧义。使用的数据集：\",\"MRPC Microsoft Research Paraphrase corpus 是一些句子对，有的是同义的，有的是不同义的。\",\"QQP Quora Question Pairs 美国知识问答网站 Quora 上的问题答案数据集\",\"STS-B Semantic Textual similarity benchmark 语义文本相似度数据集，样本为文本对，评判两个文本语义信息的相似度，分数为1-5。\",\"在STS-B上有1个点的绝对提升，比Single-task BiLSTM + ELMo + Attn有4.2%的绝对提升。\",\"分类 两个不同分类任务的评估结果，也在上表4中。CoLA——Corpus of Linguistic Acceptability语言可接受性语料库，纽约大学发布的有关语法的数据集，该任务主要是对一个给定句子，判定其是否语法正确，因此CoLA属于单个句子的文本二分类任务。\",\"SST-2——The Stanford Sentiment Treebank, 主要针对电影评论来做情感分类，因此SST属于单个句子的文本分类任务（其中SST-2是二分类，SST-5是五分类，SST-5的情感极性区分的更细致)。\",\"CoLA上取得45.4，SST-2取得91.3的准确率，整体得分72.8。\",\"总体而言，在12个数据集上的9个取得SOTA结果，比许多情况下的ensemble模型要好。而且能适应不同大小数据集。\"]},\"635\":{\"h\":\"分析\",\"t\":[\"层数的迁移学习影响: 从预训练到微调迁移学习过程中，如下表2，在MultiNLI和RACE上的性能随着层数的变化而变化。作者观察标准结果，在MultiNLI上转移embedding能提升结果，每一层Transformer层能带来9%额外的提升。这表明预训练模型中的每一层都包含了解决目标问题有用的功能。\",\"零样本表现 最好要弄清楚为什么预训练模型会有效？一种假设是，与LSTMs相比，潜在生成式模型（underlying generative model）在应用到很多任务时可以提高语言建模的能力并且transformer更具结构化的注意力记忆（attentional memory）有助于迁移。在零样本上，LSTM表现高方差，表明在迁移中，Transformer架构导入偏差是有帮助的。\",\"对于CoLA（语言可接受性），样本的得分是用生成模型分配的tokens的平均对数概率，在阈值下进行预测的。\",\"对于SST-2(情感分析)，给每个实例样本加一个 very token,来限制语言模型的输出分布只有 positive和 nagative, 就是猜测被分配到高的概率值的token作为预测值。\",\"对于RACE(问答)，在文档和问题给定条件下，将生成模型分配的平均对数概率高的token作为答案。\",\"对于DPRD(威诺格拉德模式), 用两个可能的替换说法来代替定义的代词，在这之后用生成模型分配的剩下序列的平均token对数概率高的作为结果。\",\"消融研究 不同的消融研究如下表5.\",\"首先，作者在微调时用辅助的LM目标来检查作者模型的性能。在NLI和QQP任务上辅助LM目标有帮助。总之，就是大数据集有效，小数据集没有。\",\"其次，分析比较2048单元的单层LSTM和Transformer，二者都加同样的辅助LM，LSTM会掉5.6平均分数。\",\"最后，直接在监督学习任务上训练，不要预训练，这会导致在跨任务上性能降低14.8%.\"]},\"636\":{\"h\":\"结论\",\"t\":[\"作者介绍了一种框架，用任务未知的生成式预训练模型和判别式微调在自然语言理解上取得了很强的效果。通过在长篇连续文本的多样化语料库上预训练，作者模型获得了重要的世界知识和处理长距离依赖的能力，然后能成功迁移学习解决判别式任务，如问答，语义相似度评估，蕴含确定和文本分类，在12个的9个数据集取得了SOTA结果。\",\"使用无监督预训练来提升在判别式任务上的表现是机器学习研究的长期目标。作者的工作表明，实现显著的性能提升确实是可能的，并给出了提示Transformer类模型和长距离依赖的文本数据集最好用这种方法来训练。\"]},\"637\":{\"h\":\"GPT-2 论文\",\"t\":[\"GPT-2 论文\",\"论文链接: Language Models are Unsupervised Multitask Learners\"]},\"638\":{\"h\":\"摘要\",\"t\":[\"这篇论文《Language Models are Unsupervised Multitask Learners》由OpenAI团队提出，介绍了GPT-2模型，展示了大规模语言模型在无监督多任务学习中的潜力。GPT-2通过训练一个包含45百万网页链接的WebText数据集，能够在零样本（zero-shot）设置下完成多种自然语言处理任务，如问答、翻译、摘要和阅读理解等，无需任务特定的监督训练。研究发现，模型容量对任务性能至关重要，更大的模型在多个基准测试中达到了最先进水平。论文还探讨了模型泛化与记忆的关系，并指出GPT-2在生成连贯文本方面的能力，为构建通用语言处理系统提供了新思路。\"]},\"639\":{\"h\":\"简介\",\"t\":[\"当前机器学习系统的局限性\",\"当前的机器学习系统虽然在特定任务上表现出色，但依赖于大量标注数据和监督学习，导致其泛化能力有限。这些系统往往对数据分布或任务定义的微小变化非常敏感，表现为“狭窄的专家”而非通用的多任务处理者。作者指出，这种局限性部分源于单任务、单领域的数据集训练模式，限制了模型在多样化场景中的应用能力。\",\"多任务学习的挑战与机遇\",\"多任务学习（Multitask Learning）被视为提升模型通用性的潜在途径，但其在自然语言处理（NLP）领域的进展仍处于早期阶段。现有研究通常仅联合训练少量任务（如10-17个任务），而机器学习系统通常需要数百至数千个任务示例才能实现良好的泛化。作者认为，单纯依赖人工标注和设计任务目标难以满足多任务学习的规模化需求，因此需要探索更高效的学习范式。\",\"预训练与迁移学习的趋势\",\"近年来，预训练结合监督微调的方法在NLP任务中表现突出。从早期的词向量（如Word2Vec）到上下文感知的循环神经网络（如ELMo），再到基于自注意力机制的Transformer架构（如BERT、GPT），模型的迁移能力逐渐增强。然而，这些方法仍依赖监督数据。作者提出，语言模型本身可能通过无监督学习捕捉任务相关的知识，从而减少对显式监督的依赖。\",\"论文的核心假设与目标\",\"本文的核心假设是：足够大的语言模型在多样化文本训练下，能够通过预测任务的自然语言描述（如问答、翻译的文本示例）间接学习任务，而无需参数调整或架构修改。作者通过实验验证这一假设，证明GPT-2在零样本设置下能完成多种任务，部分任务性能接近或超越监督基线模型。这一发现为构建通用语言系统提供了新方向，同时揭示了模型容量与任务性能之间的紧密关联。\",\"研究意义\",\"论文强调，无监督任务学习是预训练技术成功的关键因素之一。尽管零样本性能尚不完美，但结果表明语言模型在无监督条件下已具备初步的多任务处理能力，为未来探索更通用的AI系统奠定了基础。\"]},\"640\":{\"h\":\"方法\",\"t\":[\"1. 语言建模的核心框架\",\"论文的核心方法是基于语言建模（Language Modeling, LM），即通过无监督学习估计文本序列的概率分布。给定一个符号序列 ((s_1, s_2, ..., s_n))，语言模型通过链式法则计算联合概率：\",\"这一框架允许模型不仅生成文本，还能计算任意条件概率，例如预测缺失的单词或句子。近年来，Transformer 架构（Vaswani et al., 2017）的引入显著提升了语言模型的表达能力，使其能够建模长距离依赖关系。\",\"2. 多任务学习的概率视角\",\"传统监督学习通常建模 ，而通用系统需要能够根据任务描述动态调整行为，即建模 。作者指出，McCann et al. (2018) 的MQAN（Multi-task Question Answering Network） 已经证明，任务可以通过自然语言描述（如“translate to French, English text, French text”）来指定。本文进一步假设，语言模型本身可以通过观察任务的自然语言演示（如问答对、翻译示例）来隐式学习任务，而无需显式监督。\",\"3. 训练数据集：WebText\",\"为了训练一个能够泛化到多种任务的语言模型，论文构建了一个新的数据集 WebText，其关键特点是：\",\"数据来源：从 Reddit 上爬取高赞（≥3 karma）的外链网页，确保内容经过人工筛选，质量高于 Common Crawl 等原始网络数据。\",\"规模与处理：初步版本包含约800万篇文档（40GB文本），去重并移除了 Wikipedia 数据以避免测试集污染。\",\"多样性目标：涵盖广泛的主题和写作风格，以增加模型接触不同任务（如翻译、摘要）自然演示的机会。\",\"4. 输入表示：改进的字节对编码（BPE）\",\"传统语言模型通常依赖单词或字符级输入，但存在词汇表限制或效率问题。本文采用字节级 BPE（Byte Pair Encoding），其优势包括：\",\"词汇表灵活性：基础词汇仅需256个字节，可表示任意 Unicode 字符串，避免传统 BPE 对 Unicode 编码的冗余扩展（如130,000+基词）。\",\"改进的合并策略：防止跨字符类别的合并（如“dog”与“dog!”被分开），减少词汇碎片化，同时允许空格合并以提高压缩效率。\",\"兼容性：支持对任何文本（无论预处理方式）直接计算概率，便于跨数据集评估。\",\"5. 模型架构：GPT-2 的改进\",\"GPT-2 基于 Transformer 架构，延续了 GPT-1（Radford et al., 2018）的设计，但进行了以下优化：\",\"层归一化调整：移至每个子模块的输入（类似预激活残差网络），并在最终自注意力块后增加额外层归一化。\",\"初始化优化：残差层权重按 缩放（ 为残差层数），缓解深层网络的梯度问题。\",\"扩展配置：词汇表增至50,257，上下文窗口从512扩展到1024 tokens，批大小提升至512。\",\"6. 实验设置与模型规模\",\"论文训练了 4种不同规模的模型（参数从117M到1.5B），以研究模型容量对性能的影响：\",\"最小模型（117M）与原始 GPT 相当，中等模型（345M）匹配 BERT-Large，最大模型 GPT-2（1.5B） 参数量远超 GPT-1。\",\"所有模型在 WebText 上仍表现欠拟合（held-out perplexity 持续下降），表明进一步扩大数据或模型可能提升性能。\",\"7. 任务执行的零样本机制\",\"GPT-2 的零样本能力依赖于任务提示（Task Prompting），即通过自然语言描述或示例引导模型生成目标输出。例如：\",\"翻译任务：输入“english sentence = french sentence”示例后，模型在“english sentence =”提示下生成翻译。\",\"摘要任务：在文章末尾添加“TL;DR:”触发摘要生成。\",\"问答任务：输入文档+对话历史+“A:”引导答案生成。\",\"这种方法无需微调，完全依赖语言模型对任务上下文的理解能力。\",\"总结\",\"无监督多任务学习的可行性证明：语言模型通过预测多样化文本中的任务演示（如翻译对、问答），隐式学习任务逻辑。\",\"数据与架构创新：WebText 的高质量数据、字节级 BPE 的通用性，以及 GPT-2 的规模化改进，共同支撑了零样本泛化能力。\",\"任务提示的关键作用：自然语言指令可作为隐式任务描述，激活模型的相关能力。\",\"这些设计使 GPT-2 成为首个在零样本设置下接近监督模型性能的大规模语言模型，为后续研究（如 GPT-3 的少样本学习）奠定了基础。\"]},\"641\":{\"h\":\"实验\",\"t\":[\"1. 实验设计与模型配置\",\"论文系统评估了不同规模的GPT-2模型（117M、345M、762M和1.5B参数）在多个NLP任务上的零样本（zero-shot）性能。所有模型均采用相同的架构，但层数、隐藏层维度和参数量不同（见表2）。实验的关键发现是：模型容量与任务性能呈对数线性关系，更大的模型在几乎所有任务上都显著优于小模型。作者特别指出，即使是最大的1.5B模型，在WebText验证集上仍未完全收敛（underfitting），表明进一步扩大模型和数据规模可能带来额外提升。\",\"2. 语言建模任务评估\",\"GPT-2在8个标准语言建模数据集上进行了测试，包括Penn Treebank（PTB）、WikiText-2、LAMBADA等。结果显示：\",\"跨领域泛化能力：GPT-2在7/8的数据集上刷新了零样本SOTA，特别是在小数据集（如PTB）和长程依赖任务（如LAMBADA）上提升显著。\",\"预处理的影响：由于GPT-2使用字节级BPE，可以避免传统tokenization的损失。通过可逆去token化（invertible detokenizers）处理，GPT-2的困惑度（perplexity）进一步降低了2.5-5点。\",\"例外情况：在One Billion Word Benchmark（1BW）上表现较差，作者归因于该数据集的句子级打乱破坏了长程依赖。\",\"3. 具体任务表现分析\",\"3.1 Children's Book Test（CBT）\",\"CBT测试模型对不同词类（命名实体、名词等）的预测能力。GPT-2在验证集（避免与WebText重叠）上达到：\",\"常见名词准确率93.3%（原SOTA 85.7%）\",\"命名实体准确率89.1%（原SOTA 82.3%）\",\"分析表明，模型容量增加直接缩小了与人类表现的差距（图2）。\",\"3.2 LAMBADA\",\"该任务要求预测句子的最后一个词，需要至少50个token的上下文理解。GPT-2将：\",\"困惑度从99.8降至8.6\",\"准确率从19%提升至52.66%\",\"通过添加停用词过滤器（避免生成非结尾词），准确率进一步提升至63.24%，超过此前需依赖上下文词约束的SOTA方法。\",\"3.3 Winograd Schema Challenge\",\"测试常识推理能力（共273个样例）。GPT-2以70.7%的准确率超越前SOTA（Trinh & Le, 2018）7个百分点。作者指出，尽管数据集小，但结果与人类表现（约95%）的差距已显著缩小。\",\"3.4 阅读理解（CoQA）\",\"在对话式问答数据集上，GPT-2仅通过文档+历史对话+“A:”提示生成答案，达到55 F1：\",\"匹配或超过3/4监督基线的性能\",\"但远低于人类（89 F1）和BERT SOTA\",\"错误分析显示，模型倾向于使用简单的检索启发式（如用文档中的人名回答\\\"who\\\"问题）。\",\"3.5 摘要生成（CNN/Daily Mail）\",\"通过“TL;DR:”提示+Top-k采样（k=2）生成摘要：\",\"ROUGE分数仅略高于随机选句基线\",\"但定性分析显示生成内容类似摘要, 移除提示后性能下降6.4点，证明自然语言指令可激活任务特定行为。\",\"3.6 翻译（WMT-14）\",\"尽管WebText几乎无平行语料，GPT-2通过示例提示：\",\"英→法：5 BLEU（低于词对齐基线）\",\"法→英：11.5 BLEU（超过部分无监督方法）\",\"作者推测英语语言模型的强大概率补偿了翻译知识的不足。\",\"3.7 问答（Natural Questions）\",\"在事实型问答测试中：\",\"精确匹配准确率4.1%（远低于监督系统30-50%）\",\"但对高置信度预测（top 1%），准确率达63.1%\",\"表明模型容量是限制因素（最小模型仅1.0%准确率）。\",\"4. 记忆与泛化分析\",\"4.1 数据重叠检测\",\"使用Bloom过滤器统计测试集与WebText的8-gram重叠率：\",\"平均重叠率3.2%（与常规训练-测试重叠率5.9%相比更低）\",\"极端案例：WikiText-103测试集1.6%重叠（因文章复用段落）\",\"对性能影响：LAMBADA去除重叠样本后，困惑度仅从8.6→8.7\",\"4.2 训练集与测试集性能对比\",\"图4显示，WebText训练集和测试集的困惑度同步下降，表明：\",\"即使1.5B模型仍欠拟合\",\"性能提升非源于记忆，而是真实泛化能力\",\"4.3 生成样本分析\",\"表13展示GPT-2在非分布数据（如“独角兽新闻”）上的生成能力：\",\"能生成连贯但虚构的内容\",\"证实模型并非简单记忆，而是组合学到的知识\",\"5. 实验结论\",\"规模定律：模型容量与零样本性能强相关，1.5B参数模型在多数任务上逼近或超越监督基线。\",\"任务通用性：单一语言模型可处理翻译、问答、摘要等多样化任务，仅需自然语言提示。\",\"数据质量：WebText的多样性和规模是关键，但数据重叠对结果影响有限。\",\"局限性：摘要、翻译等任务表现仍远逊于专业系统，显示无监督学习的当前边界。\",\"这些实验为后续研究（如GPT-3的少样本学习）提供了重要基准，证明无监督预训练在多任务迁移中的巨大潜力。\"]},\"642\":{\"h\":\"讨论\",\"t\":[\"无监督任务学习作为预训练技术成功的关键因素。研究表明，当语言模型在足够多样化的文本数据上训练时，能够通过预测任务的自然语言演示（如翻译对、问答示例）隐式学习任务逻辑，而无需显式监督。这一发现为理解当前预训练模型的有效性提供了新视角，并表明在极限情况下，语言模型可能直接学会执行任务。\",\"作者同时指出GPT-2的局限性：虽然在阅读理解等任务上接近监督基线，但在摘要、翻译等任务上的表现仍远未达到实用水平。这种性能差异揭示了当前方法的边界，表明模型容量和训练数据规模仍需进一步扩大。特别值得注意的是，GPT-2的完全抽象式输出（如问答时生成而非抽取答案）与传统指针网络方法形成鲜明对比，这为未来探索更灵活的文本生成方式提供了启示。\",\"未来研究方向，包括探索GPT-2的微调潜力（如在GLUE等基准上的表现），以及研究双向表示（如BERT）与单向语言模型的互补性。这些发现为后续GPT-3等更大规模模型的开发奠定了基础，推动学界重新思考语言模型在多任务学习中的角色。\"]},\"643\":{\"h\":\"总结\",\"t\":[\"本文通过GPT-2模型证明了大规模语言模型在无监督多任务学习中的强大潜力。当模型在足够大且多样化的文本数据（WebText）上训练时，仅通过语言建模目标就能在零样本设置下完成多种NLP任务，并在7/8的语言建模基准上达到SOTA水平。这一发现表明，高容量模型通过最大化文本序列的似然估计，可以自发地学习执行任务，而无需明确的监督信号。\",\"研究结果对构建通用语言系统具有重要意义：首先，它验证了单一模型架构通过规模扩展即可实现多任务处理的可能性；其次，展示了自然语言本身作为任务描述符的有效性。尽管当前零样本性能仍有限，但这一方向为减少对人工标注数据的依赖提供了新思路。\",\"最后，论文指出这仅是通向更通用AI系统的初步探索。作者开放了模型代码和小型预训练模型，鼓励后续研究继续探索更大规模语言模型的行为边界，以及如何更好地利用其隐含学习到的多任务能力。这项工作为后续GPT系列模型的发展奠定了理论基础和方法框架。\"]},\"644\":{\"h\":\"GPT-3 论文\",\"t\":[\"GPT-3 论文\",\"论文链接: Language Models are Few-Shot Learners\"]},\"645\":{\"h\":\"摘要\",\"t\":[\"这篇论文介绍了GPT-3，一个具有1750亿参数的自回归语言模型，通过大规模训练显著提升了少样本学习能力。GPT-3在多种自然语言处理任务中表现出色，包括翻译、问答和文本生成等，甚至在零样本和单样本设置下也能取得有竞争力的结果。研究还探讨了数据污染问题、模型局限性及其社会影响，如偏见和能源消耗。实验表明，模型规模的扩大带来了性能的持续提升，但某些任务仍存在挑战。论文强调了GPT-3在通用语言系统发展中的潜力及其可能带来的广泛社会影响。\"]},\"646\":{\"h\":\"简介\",\"t\":[\"1. 背景与动机\",\"近年来，自然语言处理（NLP）领域逐渐转向预训练语言模型，并采用更灵活的任务无关（task-agnostic）方法进行下游迁移学习。早期的模型（如词向量、RNN）依赖任务特定的架构，而现代模型（如Transformer）可直接微调，无需额外架构调整。然而，现有方法仍需要针对每个任务进行大规模监督数据微调，这限制了模型的广泛应用。相比之下，人类仅需少量示例或简单指令即可完成新任务，因此论文探索如何让语言模型具备类似的少样本学习能力。\",\"2. 现有方法的局限性\",\"当前基于微调的方法存在三个主要问题：\",\"数据需求高：每个任务需要数千至数十万标注样本，难以覆盖广泛的语言任务。\",\"泛化能力有限：模型容易过拟合训练数据的虚假相关性（spurious correlations），导致在分布外数据上表现不佳。\",\"与人类学习方式不匹配：人类可通过少量示例或自然语言指令快速适应新任务，而现有模型难以实现类似能力。\",\"3. 元学习与上下文学习的潜力\",\"论文提出通过元学习 (meta-learning) 提升模型的少样本学习能力，即在预训练阶段让模型隐式学习多种技能，并在推理时通过上下文（in-context learning）快速适应新任务。此前的研究（如GPT-2）已初步验证了上下文学习的可行性，但性能远低于微调方法。论文假设，模型规模的扩大可能显著提升上下文学习能力，因为更大容量的模型能吸收更多任务相关的模式。\",\"4. GPT-3 的目标与贡献\",\"论文训练了GPT-3（1750亿参数），比此前最大的非稀疏语言模型大10倍，并系统评估其在零样本（zero-shot）、单样本（one-shot）和少样本（few-shot）设置下的表现。实验覆盖了翻译、问答、常识推理等多样化任务，结果显示：\",\"在少样本设置下，GPT-3 接近或超越部分任务的微调模型性能。\",\"模型规模与少样本学习能力呈正相关，表明缩放定律（scaling laws）在此类任务中依然适用。\",\"同时，论文也分析了模型在自然语言推理（NLI）等任务上的局限性，并探讨了数据污染和社会影响等问题。\",\"5. 研究意义\",\"GPT-3 的成果表明，超大规模语言模型可以显著减少对任务特定数据的需求，推动更通用、灵活的语言系统发展。然而，其局限性（如计算成本、偏见问题）也提示了未来改进方向，如结合双向架构或多模态训练。论文最终强调，这一研究为探索语言模型的元学习机制和实际应用奠定了基础。\"]},\"647\":{\"h\":\"方法\",\"t\":[\"1. 四种任务设定方法的比较\",\"作者首先定义了语言模型执行任务的四种方式：\",\"Fine-tuning（微调）：在任务特定数据集上更新模型权重，通常需要数千到数十万个标注样本，性能最佳，但泛化能力弱，且每个任务都需新数据。\",\"Few-shot learning（少样本学习）：在推理时为模型提供10-100个任务示例作为上下文，无需参数更新，显著减少数据需求。\",\"One-shot learning（单样本学习）：提供一条示例和任务描述，有时更贴近人类学习习惯。\",\"Zero-shot learning（零样本学习）：仅提供任务描述，不给任何示例，是最具挑战也最通用的形式。\",\"如图 2.1 所示（Figure 2.1），这些方法在数据需求和任务适应能力之间形成一个光谱，GPT-3主要研究后三种方法，强调它们在无需微调的情况下就能取得良好效果，尤其是few-shot设定下的表现令人惊喜。\",\"2. 模型架构与规模设计\",\"GPT-3模型架构基本沿用GPT-2，包括预归一化、可逆tokenizer等设计，但采用稀疏注意力机制（Sparse Transformer）以提升效率。作者训练了从125M到175B参数的8个模型（见表 2.1），以研究性能与规模之间的关系。所有模型共享最大上下文窗口为2048 tokens。模型训练过程中采用混合并行策略以适应大规模参数训练。\",\"3. 数据集构成与过滤策略\",\"GPT-3的训练数据主要来自以下五个来源（见表 2.2）：\",\"Common Crawl（经过过滤，占比60%）\",\"WebText2、Books1、Books2、Wikipedia（合计40%）\",\"为保证数据质量，作者对Common Crawl执行了质量过滤和模糊去重，并引入高质量参考语料。重要的是，数据在训练中并非按体量采样，而是按质量设权重采样，高质量数据被重复使用，而Common Crawl这类数据在整个训练中只被读取一次左右。\",\"4. 训练过程与资源分配\",\"大模型使用较大的batch size和较小的学习率（详见表 2.1）。训练依赖微软提供的高带宽GPU集群，采用模型层间和矩阵级别的并行方式进行。所有模型都使用3000亿tokens进行训练，训练策略遵循了《Scaling Laws for Neural Language Models》一文的建议。\",\"如图 2.2 所示（Figure 2.2），GPT-3虽然模型更大，但实际训练所需的计算资源与较小模型相当，这得益于更高效的数据利用率。\",\"5. 评估方法与设定\",\"在few-shot设定下，模型的每个测试样本前会插入K条示例（K通常为10-100，取决于是否能容纳在2048 token窗口中）。对于没有训练集的数据集，示例从开发集提取；对于多选题，GPT-3比较不同答案的语言模型概率（归一化处理）；对于生成类任务，则使用beam search输出，并按F1、BLEU或精确匹配评估。最终结果在公开测试集或开发集上报告。\",\"总结\",\"GPT-3的研究方法基于“任务不可知”的设定，通过大规模预训练和精心设计的上下文输入，在不进行梯度更新的前提下实现任务适应。这种“以上下文为接口”的元学习方法，加上参数规模的扩展，使得GPT-3在多个任务上展现出超越以往fine-tuned方法的能力，为未来通用语言智能系统奠定了基础。\"]},\"648\":{\"h\":\"结果\",\"t\":[\"1. 模型性能随规模扩展而持续提升\",\"作者首先展示了8个不同规模的模型在训练过程中的表现，发现无论是在训练损失还是实际任务中的表现，都随着模型参数的增长而呈现平滑的幂律提升趋势（见图 3.1 和图 3.3）。这表明大模型能够更好地吸收语言知识和上下文信息。\",\"2. GPT-3在语言建模和完形填空任务中的表现\",\"GPT-3在传统语言建模任务（如PTB）中零样本设定下创下新SOTA（PPL 20.5），远优于此前结果（PPL 35.8）。在LAMBADA数据集上，few-shot设置下准确率达到86.4%，比原SOTA高出18%（见图 3.2）。此外，在StoryCloze和HellaSwag等故事完形任务中，GPT-3也表现出明显的few-shot优势。\",\"3. 在封闭式问答任务中接近甚至超越SOTA\",\"GPT-3在TriviaQA、WebQuestions 和 Natural Questions这三个问答任务中，在没有使用外部检索信息（closed-book）或微调的前提下，仅通过few-shot设定就达到了与微调SOTA模型相当甚至更优的水平。尤其在TriviaQA中，few-shot得分达到71.2%，超越了一些基于检索系统的模型（如RAG）。\",\"4. 多语言翻译能力显著提升\",\"尽管训练数据中非英语文本仅占7%，GPT-3在英法、英德、英罗等语言对的few-shot翻译任务中，已超越多项无监督NMT方法的表现（见表 3.4 和图 3.4）。尤其在翻译为英语的方向上，GPT-3展现出更强的语言建模优势。\",\"5. 常识推理与Winograd类任务\",\"GPT-3在Winograd Schema Challenge中零样本即可取得88.3%的准确率，接近人类水平，且在更具挑战性的Winogrande数据集上few-shot得分达到77.7%，逼近fine-tuned大型模型表现（见图 3.5）。但对于如WiC（语义一致性）任务，GPT-3表现较差（仅为49.4%），显示在一些语义比较任务上仍存在明显短板。\",\"6. 阅读理解与逻辑推理任务表现不一\",\"在阅读理解任务中（如CoQA、DROP、QuAC、SQuADv2），GPT-3在few-shot设定下表现优异，尤其在CoQA中few-shot得分（85.0 F1）仅比人类低几分（见图 3.7）。但在结构化或需要多步推理的任务中（如DROP、RACE、QuAC），表现则不及微调模型，显示GPT-3对复杂语义结构的掌握仍有提升空间。\",\"7. SuperGLUE整体表现良好，但有短板\",\"在SuperGLUE基准测试中，GPT-3在少样本（32个示例）设定下，在BoolQ、ReCoRD等任务上表现接近SOTA，在COPA任务中仅落后1-2分。但在如WiC、CB、MultiRC等任务上显著低于fine-tuned模型（见表 3.8 和图 3.8）。这说明GPT-3在识别细粒度语义差异上仍有明显不足。\",\"8. NLI和Adversarial推理任务仍具挑战性\",\"在自然语言推理任务（如RTE和ANLI）中，即使是GPT-3 175B也只能在few-shot设定下稍高于随机水平（约33%），表现远不如fine-tuned模型（见图 3.9）。尤其在ANLI这种对抗性构建的数据集上，GPT-3展示了推理能力的不足。\",\"9. 在合成任务和灵活性测试中展现强大泛化能力\",\"GPT-3在设计的算术、字母重排、新词使用、语法纠错等任务中，只需提供极少量的示例就能成功完成，这表明其具有一定程度的推理和快速适应能力。这些任务测试了GPT-3的few-shot元学习能力，显示其对“任务模式”的提取并非依赖微调。\",\"总结\",\"GPT-3在多数NLP任务中，在zero-, one-, few-shot设定下均展示了强大的任务适应能力，尤其在few-shot情境下，其表现多次逼近甚至超越传统fine-tuned模型。与此同时，一些任务（如对抗性推理、语义比较等）仍暴露出其推理深度与语言理解的局限，提示未来需在结构理解与逻辑泛化方面进一步改进。\"]},\"649\":{\"h\":\"局限性\",\"t\":[\"1. GPT-3 并非通用智能：能力分布不均\",\"尽管GPT-3在多个任务上取得了令人印象深刻的成绩，但作者明确指出，它并不是一个通用智能系统，其表现呈现出高度任务依赖性：在某些任务中可与SOTA模型媲美，但在其他任务（如自然语言推理、逻辑比较）中则表现平庸甚至接近随机。\",\"这种“选择性优势”意味着GPT-3更像是一个巨大的“模式匹配引擎”，而非真正理解语言和任务的系统。\",\"2. 缺乏鲁棒的系统性泛化能力\",\"GPT-3的few-shot能力主要依赖于识别任务格式和输出模式，而不是进行真正意义上的概念抽象和泛化。作者指出，目前尚不清楚模型在推理任务中是否“学会”了新知识，还是只是记住了相似的训练样本。这种 泛化机制的模糊性 是目前元学习方法的一个重要限制。\",\"3. Prompt依赖性强，输入微小变动影响大\",\"GPT-3对提示（prompt）形式和内容高度敏感。不同的措辞、问题格式甚至换行方式都可能造成性能大幅波动。\",\"这意味着few-shot效果难以稳定复现，缺乏可控性与鲁棒性，在实际部署中可能导致意外错误。\",\"4. 上下文窗口限制性能提升\",\"尽管GPT-3的上下文窗口扩大到2048 tokens，相比前代模型大幅提升，但这仍然限制了few-shot学习中可用的示例数量（尤其是在长文本任务中）。作者认为，有限上下文容量成为当前few-shot学习的“瓶颈”。\",\"5. 无法利用结构化监督信号\",\"GPT-3完全不依赖梯度更新，因此无法像微调方法那样从结构化监督中持续优化。在特定任务上（如NER、结构化问答、程序生成等），GPT-3的表现明显弱于专门微调过的模型。这表明它在需要长期优化和知识整合的任务中仍有较大局限。\",\"6. 推理与数学能力仍然有限\",\"GPT-3虽然能完成基础算术和简单逻辑题，但在 多步推理、抽象代数、数理一致性等方面 表现仍然较弱。这限制了其在金融、科研、工程等高精度领域的适用性。\",\"7. 模型不可解释性问题严重\",\"GPT-3的推理过程完全由大量参数和非线性变换组成，目前尚无有效方式解释它为何会给出某一答案。这种不可解释性限制了其在高风险领域的应用，如医疗、法律、金融决策等。\",\"总结\",\"虽然GPT-3在few-shot学习方面展现出极强的能力，但其本质仍是一个“超大规模、强记忆型的语言预测器”，而非具备深层理解与推理能力的系统。它面临的问题包括任务适应不均、prompt敏感性高、缺乏结构化监督利用能力、推理有限、以及缺乏透明性等。这些限制提示我们，在使用GPT-3及其衍生模型时，仍需谨慎评估其边界与适用性，并探索更强的系统性泛化能力和稳健性。\"]},\"650\":{\"h\":\"相关工作\",\"t\":[\"1. 从词向量到上下文表示的发展历程\",\"该部分首先回顾了自然语言处理（NLP）领域中语言表示学习的演进：\",\"早期方法（如 word2vec、GloVe）关注学习固定词向量；\",\"后续方法（如 ELMo、ULMFiT）引入上下文，支持基于上下文的词表示；\",\"Transformer 时代：BERT、GPT 系列、XLNet 等模型将预训练语言模型推向主流，支持更广泛的下游任务，通过微调在多个任务上实现了SOTA。\",\"GPT-3继承了这一发展路线，并将参数规模推至前所未有的高度，强化了“无任务特定架构”的方法论。\",\"2. 微调范式与任务适应能力的关系\",\"在GPT-3之前，大多数SOTA模型依赖于“预训练 + 微调”范式，即先在大语料上预训练，再在具体任务数据上进行监督微调（如BERT、T5）。这种方法虽然效果强大，但依赖大量任务标注数据，不利于迁移与泛化。\",\"GPT-3的核心创新之一，是系统性地探索 无梯度更新的few-shot学习（in-context learning），挑战了传统对“适应任务必须微调”的假设。\",\"3. 元学习与few-shot学习的启发\",\"作者借鉴了 元学习（meta-learning） 的理念，即模型在“外循环”中获得广泛能力，在“内循环”中快速适应新任务。GPT-3通过扩展模型容量，在预训练阶段学习泛化模式，在推理阶段用文本输入指定任务，实质是一种“隐式元学习”机制。\",\"这与Few-shot Learning领域中如MAML、Prototypical Networks、Matching Networks等方法异曲同工，但不同于它们使用结构明确的元任务，GPT-3完全通过文本学习并表达任务结构。\",\"4. 模型规模扩展趋势与“Scaling Laws”\",\"文中引用了Kaplan等人提出的“神经语言模型的规模定律（Scaling Laws）”，即验证集损失随着模型规模、数据量和计算量按幂律缩放。在这一理论指导下，GPT-3以175B参数扩展至前代模型的10倍以上。\",\"GPT-3验证了一个关键假设：few-shot能力会随着模型规模的增加而显著增强，补足了先前few-shot模型（如 GPT-2、CTRL、T5）表现不稳定的问题。\",\"5. 多任务与多语言学习的基础\",\"GPT-3并未对每个任务建立单独的模型，而是通过单一语言建模目标，实现任务统一与跨任务迁移，呼应了T5等模型的“文本到文本”框架。同时，它在某种程度上也具备一定的多语言能力，尽管其非英语性能仍有限。\",\"此外，文中提到了一些少量使用in-context设定的早期尝试（如 GPT-2 的zero-shot prompt），但GPT-3是首次系统性、大规模地在zero-, one-, few-shot条件下进行全面评估的工作。\",\"总结\",\"GPT-3站在了词向量、上下文建模、transformer架构、微调范式、元学习和模型扩展趋势等多个重要研究方向的交汇点上。它在技术上并非从零出发，而是有机融合并推升了这些已有成果，将预训练语言模型从“参数调优”时代推向了“推理即编程”的新范式。\"]},\"651\":{\"h\":\"结论\",\"t\":[\"作者指出，GPT-3 展示了强大的in-context learning（上下文学习）能力，在不进行任何梯度更新的前提下，仅通过自然语言提示和示例，即可在多种语言任务中实现从零样本到少样本的泛化，部分任务甚至达到或超越微调模型的水平。尽管仍存在局限，但结果表明：随着模型规模扩展，大规模语言模型在任务通用性与灵活性方面具有巨大潜力，为未来通用语言智能系统的发展提供了重要方向。\"]},\"652\":{\"h\":\"InstructGPT 论文\",\"t\":[\"InstructGPT 论文\",\"论文链接: Training language models to follow instructions with human feedback\"]},\"653\":{\"h\":\"摘要\",\"t\":[\"本研究指出，仅通过增加语言模型的规模，并不能显著提升其对用户意图的理解与遵循能力。为了解决这一问题，作者提出一种通过人类反馈对模型进行微调的方法，用以更好地对齐模型行为与用户意图。\",\"具体方法包括：首先利用人工演示数据对GPT-3进行监督学习微调；然后通过人类对多个模型输出的偏好进行排序，训练奖励模型，并结合强化学习进一步优化模型。最终所得的InstructGPT模型，即使参数量远小于原始GPT-3（例如1.3B对比175B），在用户偏好评估中仍表现更优。此外，InstructGPT在输出真实性、减少有害内容生成等方面也有所改进，且在公开NLP任务上的性能损失极小。研究表明，人类反馈微调是一种有效的模型对齐手段，尽管仍有提升空间。\"]},\"654\":{\"h\":\"简介\",\"t\":[\"作者指出，大型语言模型（如GPT-3）虽然具备强大的自然语言处理能力，但它们常常偏离用户意图，表现出诸如捏造事实、生成有害或无关文本、不遵循指令等问题。这是因为它们的训练目标是最大化互联网文本的下一个词预测概率，而非“安全且有用地遵循用户指令”，这造成了目标的不一致，即“对齐问题”（alignment problem）。\",\"为解决这一问题，本文提出了一种对齐语言模型与用户意图的策略：通过人类反馈进行强化学习（Reinforcement Learning from Human Feedback, RLHF）。该方法包括三个关键步骤：\",\"监督学习微调（SFT）：收集人类示范数据，微调预训练的GPT-3模型；\",\"奖励模型训练（RM）：收集人类对模型多个输出的排序偏好，训练出一个能预测人类偏好的奖励模型；\",\"强化学习微调（PPO）：使用奖励模型的反馈，采用Proximal Policy Optimization算法进一步优化模型行为。\",\"作者称这些过程使得模型输出更符合人类偏好，但强调这种对齐是相对于特定人群（即标注者和研究者）的偏好，并非广义上的“人类价值”。\",\"通过图1的结果可见，即便是只有1.3B参数的InstructGPT模型，其输出也比175B的原始GPT-3更受人类偏好，显示出这种人类反馈驱动的微调策略极具潜力。图1中显示的不同模型在人类偏好评估中的胜率清晰反映了该方法的有效性，表明训练目标的改变（从“预测下一个词”转向“优化人类偏好”）能带来质的改善。\",\"此外，作者采用了“有帮助（helpful）、诚实（honest）、无害（harmless）”三大原则来评估模型对齐效果，强调未来开发和部署语言模型时需格外关注其社会影响及安全性。\",\"总之，本文引入了一种有效的对齐方法，为语言模型行为与用户意图之间架起了桥梁，为AI安全和实用性的发展提供了关键路径。\"]},\"655\":{\"h\":\"相关工作\",\"t\":[\"一、基于人类反馈的模型对齐（Alignment via Human Feedback）\",\"InstructGPT 的核心技术基础是 “强化学习来自人类反馈（RLHF）”，旨在将模型输出行为与人类意图对齐。这一方法起初应用于强化学习场景：\",\"用人类偏好训练强化学习代理 : Christiano et al., 2017 提出了一种通过人类偏好比较训练代理的强化学习方法。\",\"在模拟环境中用人类反馈改进行为策略 : Ibarz et al., 2018将人类偏好学习应用于模仿学习。\",\"RLHF 后来被应用于语言任务，如摘要：\",\"风格延续任务中的偏好学习 : Ziegler et al., 2019\",\"文本摘要中的奖励建模与 PPO 微调 : Stiennon et al., 2020\",\"此外，该方向在对话系统（Jaques et al., 2019）、机器翻译（Bahdanau et al., 2016）、语义解析（Lawrence and Riezler, 2018）、故事生成（Zhou and Xu, 2020）等任务中也得到了广泛实践。\",\"InstructGPT 的工作属于对上述方法的泛化：将 RLHF 用于对齐语言模型在广泛任务分布下的行为。\",\"二、训练语言模型以遵循自然语言指令（Instruction Following）\",\"另一相关研究方向是使用自然语言指令训练模型以实现跨任务泛化：\",\"FLAN：使用数十个 NLP 数据集、配以自然语言任务说明进行微调。 （Wei et al., 2021）\",\"T0 / T0++：将 NLP 基准任务转换为指令格式，通过多任务微调训练语言模型。 （Sanh et al., 2021）\",\"Natural Instructions：探索指令格式变化对模型泛化能力的影响。 （Mishra et al., 2021）\",\"InstructGPT 与上述方法的不同之处在于其训练数据源真实 API 用户提交的指令，更具 任务多样性与实用性。\",\"图 1 支持这一点：即使参数量远小于 GPT-3（1.3B vs. 175B），InstructGPT 模型依然在用户指令任务中获得更高的偏好评分。\",\"三、评估语言模型的风险与危害\",\"InstructGPT 还借鉴了对语言模型潜在风险的研究，这些研究强调：\",\"语言模型会生成有害或偏见内容（Bender et al., 2021；Gehman et al., 2020）\",\"TruthfulQA 提供了一个用于测试模型生成信息真实性的基准数据集 （Lin et al., 2021）\",\"偏见评估数据集：包括 Winogender（性别偏见）和 CrowS-Pairs（社会偏见）（Rudinger et al., 2018,Nangia et al., 2020）\",\"InstructGPT 在实验部分也采用了这些基准（见论文第 4 节），并指出：在对毒性任务加入“请尊重”提示时，InstructGPT 比 GPT-3 更少生成有害内容（见 Figure 7）。\",\"四、模型行为干预与有害输出缓解策略\",\"文献中也探索了多种控制模型输出的策略，这些方法为 InstructGPT 所采用的 RLHF 方式提供了对照方案：\",\"微调小型数据集以嵌入价值观 （Solaiman and Dennison, 2021）\",\"通过触发短语过滤预训练语料，以降低毒性输出倾向 （Ngo et al., 2021）\",\"使用外部语言模型引导生成方向（如 Plug-and-Play Language Models） （Dathathri et al., 2019）\",\"用正则化或投影技术缓解嵌入空间中的偏见 （Liang et al., 2021）\",\"尽管 InstructGPT 并未直接采用这些方法，但在强化学习微调中加入 KL 约束、预训练梯度（PPO-ptx）等机制，实际上也体现了对 对齐损失(alignment tax) 的控制。\",\"以下是对 InstructGPT 论文中第 3 节 “Methods and Experimental Details” 的详细总结，内容结构遵循原文小节安排（3.1–3.6），并结合论文图表（如图 2）以增强理解。部分内容需分多段呈现以保留关键信息。\"]},\"656\":{\"h\":\"方法\",\"t\":[\"InstructGPT 的方法主要基于 Stiennon et al. (2020) 和 Ziegler et al. (2019) 提出的 三步训练框架，用以实现语言模型对人类意图的对齐。该流程可参见论文图 2 的三步训练流程：\",\"监督微调（Supervised Fine-Tuning, SFT）: 使用人类标注者示范的优质输出，微调预训练 GPT-3 模型，得到初始策略模型。\",\"奖励模型训练（Reward Model, RM）: 收集一组模型输出对（针对同一输入），由人类标注者根据偏好进行排序。将这些排序用作训练奖励模型（RM）的监督信号，使其学会预测哪一输出更受偏好。\",\"使用 PPO 强化学习（Proximal Policy Optimization）微调模型: 以奖励模型为环境反馈信号，对 SFT 模型进一步使用 PPO 算法进行强化学习优化，从而得到最终的 InstructGPT 模型。\",\"图 2（Figure 2） 明确展示了这三步流程之间的数据流和优化路径，是 InstructGPT 方法的核心概括图。\",\"数据集构建（Dataset）\",\"InstructGPT 的训练数据主要来自以下两个来源：\",\"真实用户在 OpenAI API Playground 提交的 prompt\",\"提取并去重后用于训练 SFT、RM 和 PPO 模型。为确保训练集与评估集分离，按用户 ID 进行划分。\",\"为防止泄露隐私，对训练数据进行了 PII 过滤。\",\"标注者创作的 prompt（主要用于冷启动训练）\",\"分为三类：\",\"Plain（开放任务）\",\"Few-shot（带示例的任务）\",\"User-based（模拟用户需求的任务）\",\"三类子数据集：\",\"SFT 数据集（~13k prompts）用于监督微调\",\"RM 数据集（~33k prompts）用于训练奖励模型\",\"PPO 数据集（~31k prompts）为 PPO 模型提供输入\",\"表 1 显示了 API prompt 的任务分布：约 46% 为生成类任务，QA 和聊天合计约 23%，突出了真实用户需求的多样性。\",\"任务类型（Tasks）\",\"训练任务覆盖广泛，包括但不限于：\",\"文本生成（如创作、补全）\",\"问答（开放型和封闭型）\",\"对话、重写、摘要、分类、抽取等\",\"大部分任务通过自然语言指令表达意图。少量则通过 few-shot 示例或文本上下文隐式表达。标注者在判断指令时需考虑信息准确性、避免偏见与毒性，这为 InstructGPT 模型“helpful, honest, harmless”标准提供训练信号。\",\"人类数据采集（Human Data Collection）\",\"OpenAI 雇佣了约 40 名标注者（通过 Upwork 与 ScaleAI）参与数据标注，执行以下任务：\",\"提供高质量示范（用于 SFT）\",\"对模型输出进行偏好排序（用于 RM）\",\"对最终模型进行评估\",\"为了保证标注质量，OpenAI 设计了 筛选测试 来挑选具有敏感内容识别能力的标注者。训练过程中的一些 prompt 包含争议性内容，故特别强调标注者的社会敏感性。\",\"在人类偏好标注中，inter-annotator agreement 达到 73±1.5%，说明标注者之间达成了较高的一致性。论文还进行了一组 held-out 标注者实验，显示 InstructGPT 模型能够泛化到新标注者的偏好。\",\"模型结构与训练细节（Models）\",\"所有模型都基于 GPT-3 架构 ，在三个参数规模（1.3B、6B、175B）下进行训练，训练策略如下：\",\"SFT 模型训练\",\"使用标注者示范数据，训练 16 个 epoch\",\"使用余弦学习率衰减，0.2 的残差 dropout\",\"用奖励模型得分选择最佳模型（而非验证 loss）\",\"奖励模型（RM）训练\",\"输入为 prompt 和 response，输出为标量奖励\",\"在每个 prompt 上收集 K（4–9）个响应，由标注者排序，训练时将所有配对作为一个 batch，防止过拟合\",\"使用如下 pairwise ranking loss：\",\"loss(θ) = − (1 / C(K,2)) * E[log(σ(r_θ(x, y_w) − r_θ(x, y_l)))]\",\"其中 、 分别为更受欢迎和较差的响应。\",\"PPO 和 PPO-ptx 模型训练\",\"PPO 使用 RM 作为奖励函数\",\"为缓解对奖励函数的过度优化，引入 KL 惩罚项\",\"PPO-ptx 版本进一步加入 pretraining 任务的 log-likelihood 更新项，以防对齐过程中性能退化（alignment tax）\",\"PPO-ptx 目标函数如下：\",\"Objective = E[r − β * KL + γ * logP_pretrain]\",\"评估方式（Evaluation）\",\"为了衡量模型的“对齐程度”，InstructGPT 使用了综合性评估框架：\",\"A. API prompt 分布评估\",\"使用 held-out 用户的 prompt\",\"人类评估输出的偏好、质量（Likert 1–7）、以及一系列元数据（如是否 hallucinate、是否尊重约束）\",\"图 4 展示了模型在是否遵循指令、幻觉率等多个维度的性能\",\"B. 公共 NLP 数据集评估\",\"涉及 TruthfulQA（真实性）、RealToxicityPrompts（毒性）、Winogender/CrowS-Pairs（偏见）\",\"还评估模型在 SQuAD、DROP、HellaSwag、WMT 2015 等任务上的零样本表现\",\"显示模型在强化学习过程中存在轻微性能损失，但 PPO-ptx 可有效缓解（见图 29–34，原文）\"]},\"657\":{\"h\":\"结果\",\"t\":[\"1. 在 API prompt 分布上的实验结果:\",\"InstructGPT 的核心实验基于真实用户提交的指令性 prompts，在这些任务中：\",\"人类评估者显著偏好 InstructGPT 输出\",\"Figure 1 显示：在用户任务分布中，1.3B 的 InstructGPT 模型比 175B GPT-3 更受偏好。\",\"即使是少样本提示增强的 GPT-3（few-shot GPT-3），也不及 InstructGPT。\",\"例如，175B InstructGPT 输出相较于标准 GPT-3 的偏好比为 85% ± 3%，相比 few-shot GPT-3 为 71% ± 4%。\",\"PPO-ptx 与 PPO 模型均优于 SFT 和 GPT-3\",\"Figure 3 显示，在两类提示分布（GPT-3 与 InstructGPT 用户提交）上，InstructGPT 在所有规模下均优于 GPT-3。\",\"并且该优势在训练标注者和 held-out 标注者之间都保持一致，说明偏好并非训练数据过拟合造成。\",\"更好地遵循指令，减少幻觉，更适合作为用户助手\",\"Figure 4 展示了模型输出的多维质量元数据对比：\",\"InstructGPT 更少“幻觉”（hallucination）\",\"更能遵守“指令中的显式约束”\",\"更常“尝试正确完成任务”\",\"更适合用于“客户助手场景”\",\"2. 在公开 NLP 数据集上的实验结果\",\"在 TruthfulQA 上更真实、更少编造\",\"Figure 6 显示，在 TruthfulQA 基准上，PPO 和 PPO-ptx 模型显著提升回答的真实性与信息性。\",\"例如，在加入指导性提示（instruction+QA）时，InstructGPT 倾向于不作伪答（如选择“I have no comment”），而 GPT-3 则容易自信地给出错误答案。\",\"InstructGPT 输出更少毒性内容，尤其在有“尊重”提示下\",\"使用 RealToxicityPrompts 数据集 + Perspective API 自动打分 + 人类评估。\",\"Figure 7 显示：\",\"有“请保持尊重”提示时，InstructGPT 显著比 GPT-3 更少生成有毒文本。\",\"无提示时，两者毒性差异减小。\",\"若刻意要求生成毒性内容，InstructGPT 反而更“有效”执行（更高毒性），说明其任务执行能力更强，但未具内置限制。\",\"在偏见测试中未表现出优势\",\"在 CrowS-Pairs 和 Winogender 数据集上，InstructGPT 和 GPT-3 偏见水平相当，有时更低 entropy 表示模型更“确信”其回答，但不一定更公正。\",\"使用 PPO-ptx 缓解了对齐损失（alignment tax）\",\"原始 PPO 模型在 SQuAD、DROP 等任务上表现退化。\",\"但通过在 RL 过程中混入预训练目标（PPO-ptx），可基本恢复甚至超越 GPT-3 性能（见附录图 29–34）。\",\"3. 定性分析与模型行为观察\",\"模型泛化能力强：能处理非训练分布指令\",\"InstructGPT 可：\",\"处理 非英语指令，如法语（尽管有时仍用英文回应）\",\"总结并解释 代码片段\",\"Figure 8 示例显示：\",\"GPT-3 未能回答“列表 C 的作用”问题，InstructGPT 给出较为合理的解释（虽然也不完全正确）\",\"模型仍存在简单错误与对“荒谬”指令的顺从\",\"InstructGPT 在面对带错误前提的指令时，可能不会质疑，而是“默认接受并执行”。\",\"它也倾向于过度规避风险，在回答简单问题时冗长解释或“过于中性”。\",\"Figure 9 展示：\",\"对“冥想后吃袜子有何用”这类指令，GPT-3 胡编乱造；InstructGPT 则写出听起来“认真合理”的答案，但仍在胡说。\",\"对“炮弹打南瓜”的问题，InstructGPT没能直接回答（如“炸碎”），而是列举可能性并犹豫。\",\"总结：InstructGPT 的结果证明了 RLHF 的有效性\",\"提升：输出更符合人类偏好，减少幻觉与毒性，对指令遵循度高。\",\"挑战：仍可生成有害内容，对荒谬命令未进行识别，任务复杂性上限未显现。\",\"泛化性：在代码、非英语指令等非监督数据上表现较好。\",\"控制手段：通过 PPO-ptx 控制 alignment tax，维持 NLP 性能。\"]},\"658\":{\"h\":\"讨论\",\"t\":[\"InstructGPT 是 OpenAI 迭代式对齐研究计划的一部分，目标是使现有模型更符合人类意图，同时构建适用于未来更强 AI 的通用方法。\",\"RLHF 是一种低成本高回报的对齐方法\",\"与预训练相比，使用 RLHF 对齐语言模型所需的计算成本极低：\",\"训练 GPT-3（175B）需约 3640 petaflop/s-days；\",\"而 InstructGPT 的 SFT 阶段只需 4.9 petaflop/s-days；\",\"PPO 微调也仅为 60 petaflop/s-days。\",\"与其训练更大的模型，不如在现有模型上投资对齐方法：例如，1.3B InstructGPT 的输出比 175B GPT-3 更受欢迎（见 Figure 1）。\",\"RLHF 能够泛化“指令跟随能力”\",\"模型在未明确训练的任务上也表现良好，如非英语任务、代码任务（见 Figure 8）。\",\"这意味着对齐方法不仅优化模型行为，还能提高其泛化能力，有助于构建更通用、适应性强的智能系统。\",\"可显著降低对齐带来的性能损失(alignment tax)\",\"原始 PPO 模型在一些公开 NLP 数据集上的性能下降（如 DROP、SQuAD）。\",\"但通过引入预训练梯度混合（PPO-ptx），可以在保持对齐的同时维持甚至提升性能（详见附录 Figure 29–34）。\",\"将抽象对齐技术成功应用于现实世界模型部署\",\"与以往在合成任务或小型模型上的研究不同，InstructGPT 将 RLHF 应用于真实的 API 模型中，验证了该技术在生产环境下的可行性和价值。\",\"我们到底在“对齐”谁？(Who Are We Aligning To ?)\",\"作者清晰指出当前模型对齐行为的“参考群体”是有局限的，实际对齐的是训练流程中的多重人为偏好叠加：\",\"标注者的偏好：训练数据和奖励信号均来自一组英语标注者（主要来自美国和东南亚），并非普遍“人类代表”。\",\"研究者的设计意图：OpenAI 研究团队定义了标注规则、标准与示例，标注者受其引导。\",\"API 用户行为：训练 prompt 来源于真实 API 用户提交，其任务形式和风格可能代表一类高频商业用途。\",\"用户 ≠ 社会：OpenAI API 用户为申请加入测试队列的群体，不代表所有潜在用户，更不代表所有受语言模型影响的人群。\",\"结论：当前对齐并非通用意义上的“人类价值对齐”，而是特定群体与目标下的实用性对齐。未来若需面向多元人群，可能需要模型具备多偏好条件控制能力。\",\"限制与盲点（Limitations）\",\"InstructGPT 在多个方面仍存在不足：\",\"模型行为问题：\",\"仍会生成有害、偏见或捏造内容。\",\"过度顺从错误指令：如“吃袜子”、“南瓜吸引炮弹”等（Figure 9）。\",\"复杂指令处理能力不足：多条件限制（如“用两句话总结 1930 年代法国电影”）仍表现不佳。\",\"数据收集问题：\",\"标注者人数有限（约40人），且偏好有偏，可能影响输出的一致性与代表性。\",\"多数比较数据仅有 1 位标注者进行判断，可能遗漏歧义与分歧点。\",\"语言多样性不足：训练数据主要为英文，非英语泛化能力未系统评估。\",\"作者建议未来采用更多元标注、歧义加权、以及特定群体优先原则（如针对少数群体敏感任务）。\",\"尚待探索的问题（Open Questions）\",\"作者列出多个值得进一步研究的问题：\",\"如何更有效缓解毒性与偏见？\",\"引入 adversarial 数据收集（如 Dinan et al., 2019b）；\",\"在预训练层面进行数据过滤（如 Ngo et al., 2021）；\",\"建立更强的拒绝机制以识别恶意请求。\",\"如何应对多价值体系的冲突？\",\"开发具备“偏好条件化能力”的模型（即对不同用户群体可调节输出风格/规范）；\",\"探索“社会契约式”对齐方法以处理价值多样性。\",\"如何建立更强的 reject 模型机制？\",\"当任务违反道德或逻辑前提时，模型应能自动识别并拒绝执行，而非“高质量完成”。\",\"社会影响与部署考量（Broader Impacts）\",\"正面影响：\",\"更符合用户指令、更具可控性、更少毒性，适合用于构建对话助手、总结系统、教育工具等。\",\"为“人类偏好引导型 AI”提供现实路径，降低部署风险。\",\"潜在风险：\",\"当前偏好群体有限，若未经适当调节可能导致某些群体观点被系统性排除；\",\"对齐本身可被滥用，尤其在军事、虚假宣传等敏感场景下；\",\"若拒绝机制不足，模型仍可能在对抗性攻击下暴露隐私、输出有害内容。\",\"作者强调，技术细节必须伴随规范治理与透明流程，否则对齐仅为形式上的“驯化”，而非本质的 AI 安全。\",\"总结:\",\"项目\",\"关键结论\",\"RLHF 价值\",\"成本低、泛化强、性能好，优于简单 scaling\",\"当前对齐对象\",\"并非“人类普遍价值”，而是 OpenAI 标注者 + 用户\",\"局限性\",\"模型顺从性过高、多样性不足、对抗性脆弱\",\"未来方向\",\"多群体条件对齐、拒绝模型、反毒性 adversarial 训练\",\"部署建议\",\"建议伴随伦理审查、偏好反馈机制、开放接口控制\"]},\"659\":{\"h\":\"KV-Cache 详解\",\"t\":[\"大模型加速技术之KV Cache详解\"]},\"660\":{\"h\":\"Why we need KV Cache ？\",\"t\":[\"生成式generative模型的推理过程很有特点，我们给一个输入文本，模型会输出一个回答（长度为N），其实该过程中执行了N次推理过程。即GPT类模型一次推理只输出一个token，输出token会与输入tokens 拼接在一起，然后作为下一次推理的输入，这样不断反复直到遇到终止符。\",\"如上描述是我们通常认知的GPT推理过程。代码描述如下：\",\"import torch from transformers import GPT2LMHeadModel, GPT2Tokenizer def main(): # 加载模型和 tokenizer model = GPT2LMHeadModel.from_pretrained(\\\"gpt2\\\").eval() tokenizer = GPT2Tokenizer.from_pretrained(\\\"gpt2\\\") # 初始输入 in_text = \\\"Open AI is a\\\" in_tokens = torch.tensor(tokenizer.encode(in_text)).unsqueeze(0) # [1, seq_len] token_eos = torch.tensor([198]) # line break symbol out_token = None i = 0 with torch.no_grad(): while out_token != token_eos: outputs = model(in_tokens) logits = outputs.logits out_token = torch.argmax(logits[0, -1, :], dim=-1, keepdim=True).unsqueeze(0) # [1, 1] in_tokens = torch.cat((in_tokens, out_token), dim=1) text = tokenizer.decode(in_tokens[0]) print(f'step {i} input: {text}', flush=True) i += 1 out_text = tokenizer.decode(in_tokens[0]) print(f'\\\\nInput: {in_text}') print(f'Output: {out_text}') if __name__ == \\\"__main__\\\": main()\",\"输出:\",\"step 0 input: Open AI is a new step 1 input: Open AI is a new way step 2 input: Open AI is a new way to step 3 input: Open AI is a new way to build step 4 input: Open AI is a new way to build AI step 5 input: Open AI is a new way to build AI that step 6 input: Open AI is a new way to build AI that is step 7 input: Open AI is a new way to build AI that is more step 8 input: Open AI is a new way to build AI that is more efficient step 9 input: Open AI is a new way to build AI that is more efficient and step 10 input: Open AI is a new way to build AI that is more efficient and more step 11 input: Open AI is a new way to build AI that is more efficient and more efficient step 12 input: Open AI is a new way to build AI that is more efficient and more efficient than step 13 input: Open AI is a new way to build AI that is more efficient and more efficient than traditional step 14 input: Open AI is a new way to build AI that is more efficient and more efficient than traditional AI step 15 input: Open AI is a new way to build AI that is more efficient and more efficient than traditional AI. step 16 input: Open AI is a new way to build AI that is more efficient and more efficient than traditional AI. Input: Open AI is a Output: Open AI is a new way to build AI that is more efficient and more efficient than traditional AI.\",\"在上面的推理过程中，每 step 内，输入一个 token序列，经过Embedding层将输入token序列变为一个三维张量 [b, s, h]，经过一通计算，最后经 logits 层将计算结果映射至词表空间，输出张量维度为 [b, s, vocab_size]。\",\"当前轮输出token与输入tokens拼接，并作为下一轮的输入tokens，反复多次。可以看出第 i+1 轮输入数据只比第 i 轮输入数据新增了一个 token，其他全部相同！\",\"因此第 i+1 轮推理时必然包含了第 i 轮的部分计算。KV Cache 的出发点就在这里，缓存当前轮可重复利用的计算结果，下一轮计算时直接读取缓存结果。\",\"上面所举例子并没有使用KV Cache进行推理,请注意。\"]},\"661\":{\"h\":\"Self-Attention Without Cache\",\"t\":[\"下图给出了无 Cache 情况下，类GPT式生成式模型进行推理的过程:\",\"这种方式的问题是: 每生成一个 token，就要重新计算所有之前 token 的 Q/K/V + Attention + FFN 。\"]},\"662\":{\"h\":\"Self-Attention With Cache\",\"t\":[\"下图给出了有 Cache 情况下，类GPT式生成式模型进行推理的过程:\"]},\"663\":{\"h\":\"Huggingface 官方代码实现\",\"t\":[\"本节将根据 Huggingface 官方代码实现进行 KV Cache 实现讲解 (只展示核心代码，移除了大量与本文无关的逻辑)。\",\"官方代码链接: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\",\"下面将给出使用了 KV Cache 进行推理的代码:\",\"import torch from transformers import GPT2Tokenizer, GPT2Config from modeling_gpt2 import GPT2LMHeadModel # copy from huggingface , 删除了大量无关代码 def generate_text(model, tokenizer, prompt, max_new_tokens=50, eos_token_id=198): model.eval() input_ids = tokenizer.encode(prompt, return_tensors=\\\"pt\\\") past_key_values = None output_ids = input_ids.clone() with torch.no_grad(): for step in range(max_new_tokens): outputs = model( input_ids=input_ids, past_key_values=past_key_values, use_cache=True ) logits = outputs.logits past_key_values = outputs.past_key_values next_token_logits = logits[:, -1, :] next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True) output_ids = torch.cat([output_ids, next_token], dim=-1) if next_token.item() == eos_token_id: break input_ids = next_token # 采用KV Cache后，推理过程修改的关键: 下一步只送入新 token print(f\\\"step {step}: {tokenizer.decode(output_ids[0])}\\\", flush=True) return tokenizer.decode(output_ids[0]) def main(): config = GPT2Config() tokenizer = GPT2Tokenizer.from_pretrained(\\\"gpt2\\\") model = GPT2LMHeadModel(config) prompt = \\\"Once upon a time\\\" output = generate_text(model, tokenizer, prompt) print(\\\"\\\\nFinal output:\\\") print(output) if __name__ == \\\"__main__\\\": main()\",\"KV Cache 的引入是为了加速自回归模型的推理速度，具体体现在:\",\"每轮推理时，只需要计算当前轮新增 token 的 Q/K/V，而不需要重新计算所有之前 token 的 Q/K/V。\",\"缓存当前轮计算结果，下一轮推理时直接读取缓存结果。\",\"在首轮推理的过程中，我们传入的是 promt 提示词列表，并且 KV Cache 此时为空，还未进行初始化。因此首轮推理过程需要完成 promt 提示词列表的 keys 和 values 的缓存；由于 GPT2 由多层 GPT2Block 堆叠而成，而每一层 GPT2Block 都有一个 GPT2Attention 模块， 因此 KV Cache 需要准备好每一层 GPT2Attention 模块的 keys 和 values 缓存 (分层Cache - legacy_cache)。\",\"class GPT2Model(GPT2PreTrainedModel): def forward( self, input_ids=None, past_key_values=None, cache_position=None, attention_mask=None, position_ids=None, head_mask=None, use_cache=None, ): return_legacy_cache = False if use_cache: # 1. 首轮推理，先进行 Legacy Cache 初始化 if past_key_values is None: return_legacy_cache = True past_key_values = DynamicCache() # 2. 后续推理，将模型以元组形式返回的缓存重新封装为Legacy Cache形式 elif not isinstance(past_key_values, Cache): return_legacy_cache = True past_key_values = DynamicCache.from_legacy_cache(past_key_values) # 3. 词嵌入 inputs_embeds = self.wte(input_ids) # 4. 位置编码计算 if cache_position is None: # 4.1 已经缓存的词序列长度 past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0 # 4.2 只为当前传入的词生成位置序列 cache_position = torch.arange( past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device ) if position_ids is None: position_ids = cache_position.unsqueeze(0) # 添加batch维度 # 4.3 生成位置编码 position_embeds = self.wpe(position_ids) # 5. 词嵌入 + 位置编码 hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device) # 6. 进入堆叠GPT2Block模块前向传播流程 for i, block in enumerate(self.h): hidden_states = block( hidden_states, past_key_values if not (self.gradient_checkpointing and self.training) else None, # 训练时，不启用KV Cache cache_position, causal_mask, use_cache=use_cache, ) hidden_states = self.ln_f(hidden_states) hidden_states = hidden_states.view(output_shape) # 7. 将KV Cache用元组的形式进行返回 past_key_values = past_key_values if use_cache else None if return_legacy_cache: past_key_values = past_key_values.to_legacy_cache() return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=past_key_values, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, )\",\"下图展示的是步骤7中以元组形式返回的KV Cache结构:\",\"下面将展示GPT2Block模块的实现逻辑，由于不涉及KV Cache的实现细节，所以不过多展开:\",\"class GPT2Block(GradientCheckpointingLayer): def forward( self, hidden_states: Optional[tuple[torch.FloatTensor]], past_key_value: Optional[Cache] = None, cache_position: Optional[torch.LongTensor] = None, attention_mask: Optional[torch.FloatTensor] = None, use_cache: Optional[bool] = False, ) -> Union[tuple[torch.Tensor], Optional[tuple[torch.Tensor, tuple[torch.FloatTensor, ...]]]]: # 1. 归一化 residual = hidden_states hidden_states = self.ln_1(hidden_states) # 2. 自注意力运算 attn_output, self_attn_weights = self.attn( hidden_states, past_key_value=past_key_value, cache_position=cache_position, attention_mask=attention_mask, use_cache=use_cache, ) # 3. residual connection hidden_states = attn_output + residual # 4. 归一化 + MLP + residual connection residual = hidden_states hidden_states = self.ln_2(hidden_states) feed_forward_hidden_states = self.mlp(hidden_states) hidden_states = residual + feed_forward_hidden_states return hidden_states\",\"推理时的常规流程（无 KV Cache）， 每生成一个新 token，都要：\",\"重新输入全部历史 token\",\"对所有历史 token 重新计算 key 和 value\",\"这意味着重复计算，效率低，计算开销线性增长\",\"有了 KV Cache 后的改进：\",\"第一次输入完整句子，计算并缓存其 key/value；\",\"后续每次生成新 token 时：\",\"只计算新 token 的 query、key、value；\",\"把新 token 的 key/value 插入缓存中（代码中用 past_key_value.update(...) 完成）；\",\"attention 直接使用「历史缓存 key/value + 当前新 token 的 key/value」来完成；\",\"整个注意力的 query 只有一个（当前 token），key/value 是历史缓存 + 当前 token。\",\"class GPT2Attention(nn.Module): def __init__(self, config, is_cross_attention=False, layer_idx=None): self.c_attn = Conv1D(3 * self.embed_dim, self.embed_dim) # 输入维度: (batch,seq_len,embed_dim) , 变换后的输出维度: (batch,seq_len,3*embed_dim) self.c_proj = Conv1D(self.embed_dim, self.embed_dim) def forward( self, hidden_states: Optional[tuple[torch.FloatTensor]], past_key_value: Optional[Cache] = None, cache_position: Optional[torch.LongTensor] = None, attention_mask: Optional[torch.FloatTensor] = None, ) -> tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]: # 1. 一维卷积进行线性变换和升维，然后切分成query，key，value query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) # 2. (batch,seq_len,-1,head_dim) , head_dim 是多头自注意力中每个头切分到的维度 shape_q = (*query_states.shape[:-1], -1, self.head_dim) shape_kv = (*key_states.shape[:-1], -1, self.head_dim) # 3. 维度统一: (batch,heads,seq_len,head_dim) query_states = query_states.view(shape_q).transpose(1, 2) key_states = key_states.view(shape_kv).transpose(1, 2) value_states = value_states.view(shape_kv).transpose(1, 2) # 4. KV Cache 不为空 if past_key_value is not None: # 4.1 cache_position 记录当前词对应输入词序列中的索引 cache_kwargs = {\\\"cache_position\\\": cache_position} # 4.2 将当前词的key和val进行缓存，根据所在GPTBlock层级(layer_idx说明)，和位于词序列的索引(cache_kwargs说明),插入对应层的list缓存中去，同时返回对应的key和val list key_states, value_states = past_key_value.update( key_states, value_states, self.layer_idx, cache_kwargs=cache_kwargs ) # 5. 进行经典的多头自注意力运算(不展开细聊) attn_output, attn_weights = attention_interface( self, query_states, # 当前输入词的query key_states, # cache key list + 输入词的key value_states, # cache val list + 输入词的val attention_mask, # padding mask dropout=self.attn_dropout.p if self.training else 0.0, ) attn_output = attn_output.reshape(*attn_output.shape[:-2], -1).contiguous() attn_output = self.c_proj(attn_output) attn_output = self.resid_dropout(attn_output) return attn_output, attn_weights\"]},\"664\":{\"h\":\"LLaMA-1论文\",\"t\":[\"LLaMA-1 论文\",\"论文链接: LLaMA: Open and Efficient Foundation Language Models\"]},\"665\":{\"h\":\"摘要\",\"t\":[\"LLaMA是一系列高效的基础语言模型，参数规模从7B到65B不等，其特点在于仅使用公开可用的数据集进行训练，而无需依赖专有数据。实验结果表明，LLaMA-13B在多数基准测试中优于GPT-3（175B），而LLaMA-65B则与Chinchilla-70B和PaLM-540B等顶尖模型表现相当。这些模型的发布旨在促进研究社区的开放访问和研究，部分模型甚至可以在单个GPU上运行。\"]},\"666\":{\"h\":\"简介\",\"t\":[\"模型规模与性能的重新思考\",\"论文指出传统观点认为模型参数越多性能越优（如GPT-3的175B参数），但Hoffmann等人（2022）的研究表明，在固定计算预算下，小模型+更多数据训练可能更优。例如，LLaMA-7B在1T tokens训练后性能持续提升（见图1训练损失曲线），而Hoffmann推荐的10B模型仅训练200B tokens即停止。这一发现挑战了单纯追求参数规模的范式。\",\"推理效率的核心目标\",\"LLaMA强调推理成本优化而非单纯训练速度。论文指出，虽然大模型训练更快达到目标性能，但小模型在长期训练后推理效率更高（如13B模型比GPT-3小10倍却性能更优）。这一设计理念直接反映在模型架构选择上（见表2的参数字段与学习率配置）。\",\"数据策略与开源兼容性\",\"与Chinchilla、PaLM等依赖未公开数据（如\\\"Books-2TB\\\"）不同，LLaMA仅使用公开数据（CommonCrawl 67%、C4 15%、GitHub 4.5%等，详见表1），使其完全可开源。这一策略虽限制数据量（总计1.4T tokens），但通过高效训练仍实现SOTA。\",\"性能验证与社会责任\",\"65B模型在常识推理（表3）、闭卷问答（表4-5）等任务上超越Chinchilla-70B\",\"代码生成（表8）和数学推理（表7）的竞争力\",\"同时分析模型偏见（表12-13）与毒性（表11），呼应AI伦理需求\"]},\"667\":{\"h\":\"方法\",\"t\":[\"1. 预训练数据与处理\",\"LLaMA采用纯公开数据混合，总规模1.4T tokens，主要来源包括：\",\"CommonCrawl（67%）：经CCNet流水线去重、语言识别（保留英文）和质量过滤（基于Wikipedia引用分类）。\",\"C4（15%）：补充多样性，启发式过滤低质量网页（如标点缺失）。\",\"代码与学术数据：GitHub（4.5%，MIT/Apache许可项目）、ArXiv（2.5%，移除宏定义和参考文献）、Stack Exchange（2%，按评分排序答案）。\",\"其他数据如Wikipedia（4.5%）和书籍（Gutenberg/Books3，4.5%）均经过严格去重（见表1的采样比例与磁盘大小）。\",\"Tokenizer：使用SentencePiece的BPE算法，数字拆分为独立字符，UTF-8回退到字节级处理。\",\"2. 模型架构改进\",\"基于Transformer的优化设计（对比原始架构）：\",\"预归一化（Pre-normalization）：采用RMSNorm对子层输入归一化（灵感来自GPT-3），提升训练稳定性。\",\"激活函数：替换ReLU为SwiGLU（PaLM方案），隐藏层维度设为 以平衡计算效率。\",\"位置编码：使用旋转位置嵌入（RoPE）（GPT-NeoX方案），替代绝对位置编码。\",\"详细参数配置见表2，例如65B模型维度为8192、64头注意力、80层。\",\"3. 训练优化策略\",\"优化器：AdamW（），余弦学习率调度（最终学习率为峰值10%），权重衰减0.1，梯度裁剪1.0。\",\"效率优化：\",\"内存管理：通过xformers库实现因果多头注意力的高效计算，避免存储注意力权重（参考Rabe & Staats 2021）。\",\"激活检查点（Checkpointing）：手动实现线性层反向传播，减少重计算（节省GPU内存）。\",\"并行策略：模型与序列并行（Korthikanti et al. 2022），重叠计算与GPU通信。\",\"如图1所示，65B模型在2048块A100（80GB）上训练速度达380 tokens/sec/GPU，1.4T tokens训练耗时约21天。\",\"总结\",\"LLaMA的方法论核心是通过数据质量优化（公开数据+严格过滤）、架构微调（SwiGLU/RoPE）和工程创新（内存/并行优化）实现高效训练。其设计始终围绕推理效率目标（如小模型长期训练），最终在多个基准测试中超越更大规模的闭源模型。\"]},\"668\":{\"h\":\"结果\",\"t\":[\"1. 常识推理（Common Sense Reasoning）\",\"零样本性能（表3）： LLaMA-65B在8个常识推理基准（如BoolQ、PIQA、ARC等）中全面超越Chinchilla-70B，并在多数任务上击败PaLM-540B（除BoolQ和WinoGrande）。例如：\",\"ARC挑战集：LLaMA-65B得分57.8，显著高于PaLM-540B的53.0。\",\"OpenBookQA：65B模型以60.2%准确率刷新SOTA。\",\"关键发现：LLaMA-13B性能优于GPT-3（175B），验证小模型+长训练的有效性。\",\"2. 闭卷问答（Closed-Book QA）\",\"NaturalQuestions（表4）与TriviaQA（表5）：\",\"65B模型在零样本和少样本（64-shot）设置下均达到SOTA（TriviaQA零样本68.2%，超越Chinchilla-70B的55.4%）。\",\"13B模型在单V100 GPU上推理时，性能仍优于GPT-3（如TriviaQA 64-shot 64.0% vs. GPT-3 57.2%）。\",\"训练动态：图2显示模型性能与训练token量强相关（如33B模型在1.4T tokens后HellaSwag分数提升至82.8）。\",\"3. 代码生成与数学推理\",\"代码生成（表8）: LLaMA-65B在HumanEval（pass@1 23.7%）和MBPP（37.7%）上超越未微调的PaLM-62B（15.9%/21.4%），接近PaLM-540B（26.2%/36.8%）。\",\"数学能力（表7）：\",\"GSM8k：65B模型未经数学微调即达50.9%（多数投票69.7%），优于Minerva-62B（52.4%）。\",\"MATH：65B模型（10.6%）表现接近PaLM-62B（8.8%），但远低于Minerva-540B（33.6%），凸显领域微调的重要性。\",\"4. 多任务理解（MMLU）与指令微调\",\"MMLU 5-shot（表9/16）: LLaMA-65B平均得分63.4%，落后于Chinchilla-70B（67.5%）和PaLM-540B（69.3%），主因是书籍数据量不足（仅177GB vs. 其他模型2TB）。\",\"指令微调（LLaMA-I）（表10）: 简单微调后，65B模型在MMLU上提升至68.9%，超越Flan-PaLM-62B（66.1%），证明指令适应的高效性。\",\"5. 偏见与毒性分析\",\"RealToxicityPrompts（表11）: 模型越大毒性倾向越高（65B Respectful类毒性分0.141 vs. 7B的0.081），与OPT等模型趋势一致。\",\"CrowS-Pairs（表12）: LLaMA-65B平均偏见得分66.6，优于OPT-175B（69.5），但宗教类别偏差显著（79.0）。\",\"WinoGender（表13）: 模型对非二元代词（their/them）的指代准确率（81.7%）高于性别化代词（his/him 72.1%），反映社会偏见。\",\"LLaMA的核心成果：\",\"效率突破：小模型（如13B）通过数据与训练优化达到大模型（GPT-3/Chinchilla）性能。\",\"多领域竞争力：在代码、数学等专业任务中，未微调模型即接近SOTA。\",\"可复现性：纯公开数据训练结果挑战了专有数据的必要性，但书籍/学术数据不足限制MMLU表现。\",\"责任缺陷：模型规模与毒性/偏见正相关，需后续治理（论文第5章重点讨论）。\"]},\"669\":{\"h\":\"指令微调\",\"t\":[\"方法与目标: LLaMA通过轻量级指令微调（遵循Chung et al., 2022的协议）优化LLaMA-65B，得到LLaMA-I，旨在提升任务泛化能力，无需复杂架构调整。\",\"关键性能提升（表10）\",\"MMLU 5-shot：微调后准确率从63.4%→68.9%，超越Flan-PaLM-62B（66.1%），但低于GPT-3.5（77.4%）。\",\"领域差异（表16 - 参考上文）：STEM（如Astronomy +9.2%）和人文任务（Philosophy +5.1%）提升显著。\",\"生成能力（附录D）\",\"代码生成：可输出规范代码（如HTML标签清理的正则表达式）。\",\"多轮交互：支持复杂对话（如象棋开局策略分析）。\",\"伦理响应：自动生成AI使用指南，强调责任约束。\",\"局限性与挑战\",\"数据不透明：微调数据规模/多样性未公开，可能限制泛化。\",\"逻辑缺陷：数学/推理任务仍存在幻觉（需后处理）。\",\"总结\",\"LLaMA-I证明小规模微调即可显著提升任务适应性，但透明性与可靠性仍需优化，为开源社区提供了可复现的基线（如后续Alpaca/Vicuna工作）。\"]},\"670\":{\"h\":\"Bias, Toxicity and Misinformation\",\"t\":[\"毒性生成评估（RealToxicityPrompts）\",\"使用PerspectiveAPI对100k提示生成内容进行毒性评分（0-1分）\",\"关键发现（表11）：\",\"模型规模与毒性正相关（65B毒性分0.141 vs 7B的0.081）\",\"\\\"Respectful\\\"提示仍可能触发毒性响应\",\"与Chinchiila（0.087）等模型趋势一致\",\"社会偏见分析\",\"CrowS-Pairs（表12）：\",\"平均偏见得分66.6（优于OPT-175B的69.5）\",\"宗教类别偏见最显著（79.0分）\",\"WinoGender（表13）：\",\"对非二元代词（their/them）指代准确率81.7%\",\"性别化代词（his/him）准确率低至72.1%\",\"\\\"gotcha\\\"测试显示职业性别刻板印象明显\",\"真实性缺陷（TruthfulQA）\",\"65B模型真实答案率仅57%（表14）\",\"在对抗性问题上易产生幻觉\",\"表现优于GPT-3但可靠性仍不足\",\"关键问题\",\"数据根源：CommonCrawl等网络数据隐含的社会偏见难以完全过滤\",\"规模悖论：能力提升伴随风险增加（如65B毒性最高）\",\"总结\",\"LLaMA呈现出与同类模型相似的偏见/毒性模式，凸显公开数据训练的固有挑战。需结合：\",\"1）更严格的数据清洗（如Wikipedia引用过滤）\",\"2）后处理技术（如perspectiveAPI过滤）\",\"3）社区治理框架\"]},\"671\":{\"h\":\"相关工作\",\"t\":[\"语言模型发展脉络\",\"从统计语言模型（n-gram）到神经网络（RNN/LSTM），最终演进至Transformer架构（Vaswani et al., 2017）\",\"关键里程碑：\",\"GPT系列（Radford et al., 2018, 2019, 2020）确立自回归范式\",\"BERT（Devlin et al., 2018）推动双向预训练\",\"T5（Raffel et al., 2020）统一文本到文本框架\",\"规模化研究\",\"计算律发现（Kaplan et al., 2020）揭示模型性能与规模的关系\",\"Chinchilla（Hoffmann et al., 2022）提出数据-计算最优平衡理论\",\"涌现能力研究（Wei et al., 2022）分析规模带来的质变\",\"开源模型进展\",\"OPT（Zhang et al., 2022）和BLOOM（Scao et al., 2022）推动开源大模型发展\",\"GPT-NeoX（Black et al., 2022）提供20B参数开源基线\"]},\"672\":{\"h\":\"总结\",\"t\":[\"LLaMA系列模型通过高效架构设计和纯公开数据训练，在多个基准测试中达到与更大规模专有模型相当的性能，同时保持开源可复现性，为AI研究的民主化提供了重要范例。\"]},\"673\":{\"h\":\"LLaMA-2论文\",\"t\":[\"LLaMA-2 论文\",\"论文链接: Llama 2: Open Foundation and Fine-Tuned Chat Models\"]},\"674\":{\"h\":\"摘要\"},\"675\":{\"h\":\"模型层\"},\"676\":{\"h\":\"RoBERTa 论文\",\"t\":[\"RoBERTa 论文\",\"论文链接: RoBERTa: A Robustly Optimized BERT Pretraining Approach\"]},\"677\":{\"h\":\"摘要\",\"t\":[\"RoBERTa是一项针对BERT预训练方法的优化研究，通过系统性的实验发现BERT存在训练不足的问题，并提出了一系列改进措施。这些改进包括更长的训练时间、更大的批次规模、更多的数据、移除下一句预测（NSP）目标、使用更长的序列以及动态调整掩码模式。实验结果表明，优化后的RoBERTa在多个基准测试（如GLUE、RACE和SQuAD）上达到了最先进的性能，甚至超越了后续提出的模型。研究强调了预训练中设计选择和数据规模的重要性，同时表明BERT的掩码语言模型目标在优化后仍具有竞争力。相关模型和代码已公开供进一步研究。\"]},\"678\":{\"h\":\"引言\",\"t\":[\"RoBERTa 是一项针对 BERT 预训练方法的复制研究，旨在通过系统性的实验评估不同超参数和数据规模对模型性能的影响。研究发现，BERT 的训练存在显著不足，通过优化训练策略（如延长训练时间、增大批次规模、使用更多数据等），RoBERTa 能够匹配甚至超越后续提出的多种模型（如 XLNet）。\",\"论文的主要改进包括：\",\"动态掩码（Dynamic Masking）（对比静态掩码，如表 1 显示动态掩码在 SQuAD 2.0 和 SST-2 任务上表现更优）；\",\"移除下一句预测（NSP）目标（实验表明 NSP 对性能影响有限，甚至可能损害模型表现，如表 2 对比不同输入格式）；\",\"更大批次训练（表 3 显示增大批次规模可提升模型困惑度和下游任务准确率）；\",\"更高效的字节级 BPE 编码（减少未知词影响）。\",\"此外，RoBERTa 引入了新数据集 CC-News（76GB），并验证了数据规模对预训练的关键作用。最终，RoBERTa 在 GLUE、SQuAD 和 RACE 上取得 SOTA 结果（如表 4、5、6），证明 BERT 的掩码语言模型目标在优化后仍具竞争力。\"]},\"679\":{\"h\":\"背景\",\"t\":[\"RoBERTa 基于 BERT 的架构和训练方法，但通过优化关键设计选择提升性能。BERT 采用 Transformer 结构，输入由两个文本片段（Segment）组成，并添加特殊标记（如 [CLS]、[SEP]）。其预训练任务包括：\",\"掩码语言模型（MLM）：随机选择 15% 的输入 token，其中 80% 替换为 [MASK]，10% 保持不变，10% 替换为随机 token。原始 BERT 使用静态掩码（即预处理时固定掩码模式），而 RoBERTa 改用动态掩码（每次输入时重新生成掩码），实验证明动态掩码效果更优（如表 1）。\",\"下一句预测（NSP）：判断两个片段是否连续。尽管 BERT 认为 NSP 对下游任务（如自然语言推理）有帮助，但 RoBERTa 的实验表明移除 NSP 可能提升性能（如表 2 对比不同输入格式）。\",\"优化策略：\",\"使用 Adam 优化器（, , ）。\",\"学习率采用线性预热（10,000 步）和衰减策略。\",\"原始 BERT 训练 1M 步，批次大小 256，序列长度 512。\",\"数据：BERT 使用 BookCorpus 和 Wikipedia（共 16GB），而 RoBERTa 扩展至更大规模数据（如 CC-News、OpenWebText 等，总计 160GB）。\",\"RoBERTa 通过调整这些关键因素（如动态掩码、移除 NSP、增大批次和数据规模），显著提升了 BERT 的预训练效率和下游任务表现。\"]},\"680\":{\"h\":\"实验步骤\",\"t\":[\"1. 模型实现与优化\",\"RoBERTa 基于 fairseq 工具包重新实现了 BERT，并优化了训练细节：\",\"学习率调整：相比原始 BERT 的固定学习率（1e-4），RoBERTa 针对不同设置调整峰值学习率和预热步数。\",\"Adam 优化器改进：发现 Adam 的 项对训练稳定性影响较大，调整 以提升大批次训练的稳定性（参考 Section 3.1）。\",\"序列长度：始终使用完整长度序列（512 tokens），而原始 BERT 会在训练初期使用较短序列。\",\"2. 训练硬件与效率\",\"采用 混合精度训练（FP16），在配备 8×32GB NVIDIA V100 GPU 的 DGX-1 机器上进行分布式训练，利用 Infiniband 互联提升效率。\",\"3. 数据配置\",\"RoBERTa 使用了 5 个英语语料库，总计超过 160GB 文本，包括：\",\"BookCorpus + Wikipedia（16GB，原始 BERT 数据）\",\"CC-News（76GB，新闻数据）\",\"OpenWebText（38GB，Reddit 高赞网页内容）\",\"Stories（31GB，故事类文本）\",\"通过控制数据规模（如对比 16GB vs. 160GB），RoBERTa 验证了更多数据能显著提升模型性能（参考 Section 5 和 Table 4）。\",\"4. 评估基准\",\"实验在三大基准任务上进行：\",\"GLUE：涵盖 9 项自然语言理解任务（如 MNLI、SST-2 等），采用单任务微调（非多任务学习）。\",\"SQuAD：\",\"V1.1：答案必存在于上下文中。\",\"V2.0：支持无答案问题，RoBERTa 增加了二分类器判断可答性（参考 Section 3.3）。\",\"RACE：长文本阅读理解任务，需从 4 个选项中选择正确答案，测试模型的长距离依赖能力。\"]},\"681\":{\"h\":\"训练步骤分析\",\"t\":[\"静态与动态掩码（Static vs. Dynamic Masking）\",\"原始BERT使用静态掩码，即在数据预处理阶段生成掩码模式并固定，通过复制数据来增加多样性。\",\"RoBERTa改为动态掩码，每次输入序列时生成新的掩码模式。实验表明，动态掩码性能略优于静态掩码（如表1所示），且更高效。因此，后续实验均采用动态掩码。\",\"在BERT和RoBERTa的预训练中，掩码（Masking） 是 Masked Language Modeling (MLM) 任务的核心步骤，即随机遮盖输入文本的部分单词，并让模型预测这些被遮盖的单词。\",\"1. 静态掩码（Static Masking）\",\"原始BERT的做法：\",\"在数据预处理阶段，一次性 对每个句子随机选择15%的单词进行掩码（其中80%替换为 [MASK]，10%保持不变，10%替换为随机单词）。\",\"由于BERT训练时会多次遍历数据（如40个epoch），为了避免每次训练时看到相同的掩码模式，BERT采用 数据复制 的方法：\",\"将训练数据复制 10份，每份采用不同的随机掩码模式。\",\"这样，每个句子在训练过程中会被看到 4次（40 epochs / 10 copies = 4次），但每次掩码不同。\",\"问题：\",\"数据复制增加了存储和计算开销。\",\"由于掩码模式是固定的（尽管有10种变体），模型可能过拟合这些特定的掩码模式，影响泛化能力。\",\"2. 动态掩码（Dynamic Masking）\",\"RoBERTa的改进：\",\"不再预先固定掩码模式，而是在 每次输入模型时动态生成掩码。\",\"例如，同一个句子在训练的不同批次（batch）中，可能会被掩码不同的单词。\",\"优势：\",\"减少存储开销：无需复制数据，节省内存。\",\"增加多样性：模型在训练过程中看到更多的掩码变体，提升泛化能力。\",\"更适合长训练周期：当训练步数远超过BERT的1M步时（如RoBERTa训练500K步），动态掩码能持续提供新的掩码模式，避免过拟合。\",\"输入格式与下一句预测（NSP）\",\"原始BERT使用“Segment-pair+NSP”输入格式，包含两个文档片段和NSP损失。\",\"RoBERTa对比了多种输入格式（如表2所示）：\",\"Sentence-pair+NSP：使用单句对，性能下降，可能因无法学习长距离依赖。\",\"Full-sentences：连续句子打包，去除NSP损失，性能优于原始BERT。\",\"Doc-sentences：限制输入来自同一文档，性能略优于Full-sentences，但因批次大小可变，最终选择Full-sentences格式。\",\"实验表明，去除NSP损失不仅未降低性能，反而有所提升，这与原始BERT的结论相反。\",\"大批量训练（Large Batch Training）\",\"原始BERT使用256的批次大小训练1M步。RoBERTa尝试增大批次至2K和8K，并调整学习率（如表3所示）。\",\"结果显示，大批量训练（如8K）在保持相同计算成本下，能提升掩码语言模型的困惑度和下游任务性能。因此，RoBERTa采用8K批次进行训练。\",\"文本编码（Text Encoding）\",\"原始BERT使用30K的字符级BPE词汇表。\",\"RoBERTa改用基于字节的BPE（50K词汇表），无需额外预处理。虽然早期实验显示性能略有下降，但其通用性优势使其成为最终选择。\",\"这些改进共同构成了RoBERTa的核心优化策略，显著提升了模型性能（如表4所示）。实验结果表明，BERT原始设计存在优化空间，而RoBERTa通过系统性的调整，在GLUE、SQuAD和RACE等任务上达到了新的 state-of-the-art 水平。\"]},\"682\":{\"h\":\"RoBERTa核心改进总结\",\"t\":[\"RoBERTa（Robustly Optimized BERT Approach）是对BERT预训练过程的系统性优化，通过调整训练策略、数据规模和模型设置，显著提升了性能。其主要改进包括：\"]},\"683\":{\"h\":\"\",\"t\":[\"动态掩码（Dynamic Masking）：\",\"原始BERT使用静态掩码（预处理阶段固定掩码模式），而RoBERTa改为每次输入时动态生成掩码，减少存储开销并提升泛化能力（见表1）。\",\"结果：动态掩码在SQuAD 2.0和SST-2任务上表现略优（F1 78.7 vs. 78.3）。\",\"移除NSP任务（Next Sentence Prediction）：\",\"BERT使用NSP任务（判断两个句子是否连续），但实验表明去除NSP后性能反而提升（见表2）。\",\"RoBERTa改用Full-sentences（连续句子打包，不跨文档）或Doc-sentences（单文档内句子打包），后者效果略优但计算复杂，最终选择Full-sentences。\",\"大批量训练（Large Batch Training）：\",\"BERT使用256的批次大小，RoBERTa增大至8K，并调整学习率（如1e-3）。\",\"结果：大批量训练提升MLM困惑度（3.77 vs. 3.99）和下游任务准确率（MNLI-m 84.6 vs. 84.7）（见表3）。\",\"字节级BPE（Byte-level BPE）：\",\"改用50K词汇表的字节级BPE编码，减少未登录词（OOV）问题，虽对部分任务性能略有影响，但通用性更强。\"]},\"684\":{\"h\":\"\",\"t\":[\"更大规模数据：\",\"BERT训练数据：16GB（BookCorpus + Wikipedia）。\",\"RoBERTa新增CC-News、OpenWebText等，总数据量达160GB。\",\"结果：数据量增加后，SQuAD 2.0 F1从87.3提升至87.7（见表4）。\",\"更长训练步数：\",\"BERT训练1M步，RoBERTa延长至300K~500K步（计算成本相当，因批次更大）。\",\"结果：500K步时，SQuAD 2.0 F1达89.4，超越XLNet（88.8）（见表4）。\"]},\"685\":{\"h\":\"\",\"t\":[\"GLUE基准：\",\"单任务微调：RoBERTa在9项任务中全面超越BERT和XLNet（MNLI-m 90.2 vs. 89.8）（见表5）。\",\"排行榜提交：以88.5平均分刷新SOTA，其中4项任务（MNLI、QNLI、RTE、STS-B）领先（见表5）。\",\"SQuAD 2.0：\",\"仅用SQuAD数据（无外部数据），F1达89.8，超越XLNet（89.1）（见表6）。\",\"RACE阅读理解：\",\"准确率83.2%，显著高于BERT（72.0）和XLNet（81.7）（见表7）。\"]},\"686\":{\"h\":\"\",\"t\":[\"BERT原始设计未充分优化：RoBERTa证明更长训练、更大批次、更多数据是关键。\",\"NSP任务非必要：去除后性能反而提升，与BERT结论相反。\",\"动态掩码与大批量训练：提升效率的同时改善泛化能力。\",\"开源贡献：发布模型、代码及新数据集CC-News。\",\"RoBERTa的改进表明，BERT的MLM目标本身足够强大，只需优化训练策略即可达到SOTA，无需复杂结构调整。\"]},\"687\":{\"h\":\"相关工作\",\"t\":[\"早期方法如ELMo、GPT和BERT通过不同训练目标（如语言建模、机器翻译、掩码语言建模）取得了显著进展，而后续工作通过多任务微调、实体嵌入、跨度预测和自回归预训练（如XLNet）进一步提升了性能。作者强调，这些改进通常依赖于更大模型和更多数据（如XLNet使用10倍于BERT的数据），而RoBERTa的目标是通过系统性地复现、简化和优化BERT的训练过程，为这些方法提供一个更清晰的性能基准，从而帮助社区更好地理解不同改进的相对贡献。\"]},\"688\":{\"h\":\"总结\",\"t\":[\"通过系统优化BERT的预训练策略（包括动态掩码、移除NSP任务、增大批次和训练数据、延长训练时间），RoBERTa在GLUE、SQuAD和RACE任务上实现了SOTA性能，证明了BERT原始设计的潜力尚未被充分挖掘；\",\"同时，研究揭示了模型性能提升的关键因素并非复杂结构改动，而是训练策略和数据规模的优化，相关代码、模型和CC-News数据集已开源以促进后续研究。\"]},\"689\":{\"h\":\"从\\\"零\\\"实现 Bert\",\"t\":[\"利用Pytorch从\\\"零\\\"实现Bert\",\"TinyBert 源码链接: https://github.com/BinaryOracle/TinyBert\"]},\"690\":{\"h\":\"Bert 是什么 ？\",\"t\":[\"BERT 全称为 Bidirectional Encoder Representation from Transformer，是 Google 以无监督的方式利用大量无标注文本「炼成」的语言模型，其架构为 Transformer 中的 Encoder（BERT = Encoder of Transformer）。\",\"以往为了解决不同的 NLP 任务，我们会为该任务设计一个最合适的神经网络架构并做训练，不同的 NLP 任务通常需要不同的模型，而设计这些模型并测试其 performance 是非常耗成本的（人力，时间，计算资源）。如果有一个能直接处理各式 NLP 任务的通用架构该有多好？\",\"随着时代演进，不少人很自然地有了这样子的想法，而 BERT 就是其中一个将此概念付诸实践的例子，Google 在预训练 BERT 时让它同时进行两个任务：\",\"漏字填空，即完型填空 (Masked Language Model)\",\"判断第 2 个句子在原始本文中是否跟第 1 个句子相接（Next Sentence Prediction）\"]},\"691\":{\"h\":\"Masked Language Model\",\"t\":[\"在 BERT 中，Masked LM（Masked Language Model）构建了语言模型，简单来说，就是随机遮盖或替换一句话里面的任意字或词，然后让模型通过上下文预测那一个被遮盖或替换的部分，之后做 Loss 的时候也只计算被遮盖部分的 Loss，这其实是一个很容易理解的任务，实际操作如下：\",\"随机把一句话中 15% 的 token（字或词）替换成以下内容：\",\"这些 token 有 80% 的几率被替换成 [MASK]，例如 my dog is hairy→my dog is [MASK]\",\"有 10% 的几率被替换成任意一个其它的 token，例如 my dog is hairy→my dog is apple\",\"有 10% 的几率原封不动，例如 my dog is hairy→my dog is hairy\",\"之后让模型预测和还原被遮盖掉或替换掉的部分，计算损失的时候，只计算在第 1 步里被随机遮盖或替换的部分，其余部分不做损失，其余部分无论输出什么东西，都无所谓。\",\"这样做的好处是，BERT 并不知道 [MASK] 替换的是哪一个词，而且任何一个词都有可能是被替换掉的，比如它看到的 apple 可能是被替换的词。这样强迫模型在编码当前时刻词的时候不能太依赖当前的词，而要考虑它的上下文，甚至根据上下文进行 \\\"纠错\\\"。比如上面的例子中，模型在编码 apple 时，根据上下文 my dog is，应该把 apple 编码成 hairy 的语义而不是 apple 的语义。\"]},\"692\":{\"h\":\"Next Sentence Prediction\",\"t\":[\"我们首先拿到属于上下文的一对句子，也就是两个句子，之后我们要在这两个句子中加一些特殊的 token：[CLS]上一句话[SEP]下一句话[SEP]。也就是在句子开头加一个 [CLS]，在两句话之间和句末加 [SEP]，具体地如下图所示：\",\"可以看到，上图中的两句话明显是连续的。如果现在有这么一句话 [CLS]我的狗很可爱[SEP]企鹅不擅长飞行[SEP]，可见这两句话就不是连续的。\",\"Token Embedding 就是正常的词向量，即 PyTorch 中的 nn.Embedding()\",\"Segment Embedding 的作用是用 embedding 的信息让模型分开上下句，我们给上句的 token 全 0，下句的 token 全 1，让模型得以判断上下句的起止位置，例如:\",\"[CLS]我的狗很可爱[SEP]企鹅不擅长飞行[SEP] 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\",\"Position Embedding 和 Transformer 中的不一样，不是三角函数，而是学习出来的。\"]},\"693\":{\"h\":\"Multi-Task Learning\",\"t\":[\"BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。\"]},\"694\":{\"h\":\"Fine-Tuning\",\"t\":[\"BERT 的 Fine-Tuning 共分为 4 中类型: 文本分类，Token分类，推理任务，问答任务。\",\"如果现在的任务是 文本分类，首先在输入句子的开头加一个代表分类的符号 [CLS]，然后将该位置的 output，丢给 Linear Classifier，让其 predict 一个 class 即可。整个过程中 Linear Classifier 的参数是需要从头开始学习的，而 BERT 中的参数微调就可以了\",\"为什么要用第一个位置，即 [CLS] 位置的 output，个人理解是因为 BERT 内部是 Transformer，而 Transformer 内部又是 Self-Attention，所以 [CLS] 的 output 里面肯定含有整句话的完整信息，这是毋庸置疑的。但是 Self-Attention 向量中，自己和自己的值其实是占大头的，现在假设使用 的 output 做分类，那么这个 output 中实际上会更加看重 ，而 又是一个有实际意义的字或词，这样难免会影响到最终的结果。但是 [CLS] 是没有任何实际意义的，只是一个占位符而已，所以就算 [CLS] 的 output 中自己的值占大头也无所谓。当然你也可以将所有词的 output 进行 concat，作为最终的 output。\",\"如果现在的任务是 Token分类，将句子中各个字对应位置的 output 分别送入不同的 Linear，预测出该字的标签。其实这本质上还是个分类问题，只不过是对每个字都要预测一个类别。\",\"如果现在的任务是 NLI（自然语言推理）。即给定一个前提，然后给出一个假设，模型要判断出这个假设是 正确、错误还是不知道。这本质上是一个三分类的问题，和 Case 1 差不多，对 [CLS] 的 output 进行预测即可\",\"如果现在的任务是 问答任务，举例来说，如上图，将一篇文章，和一个问题（这里的例子比较简单，答案一定会出现在文章中）送入模型中，模型会输出两个数 s,e，这两个数表示，这个问题的答案，落在文章的第 s 个词到第 e 个词。具体流程我们可以看下面这幅图:\",\"首先将问题和文章通过 [SEP] 分隔，送入 BERT 之后，得到上图中黄色的输出。此时我们还要训练两个 vector，即上图中橙色和黄色的向量。首先将橙色和所有的黄色向量进行 dot product，然后通过 softmax，看哪一个输出的值最大，例如上图中 对应的输出概率最大，那我们就认为 s=2。\",\"同样地，我们用蓝色的向量和所有黄色向量进行 dot product，最终预测得 的概率最大，因此 e=3。最终，答案就是 s=2,e=3。\",\"你可能会觉得这里面有个问题，假设最终的输出 s>e 怎么办，那不就矛盾了吗？其实在某些训练集里，有的问题就是没有答案的，因此此时的预测搞不好是对的，就是没有答案。\"]},\"695\":{\"h\":\"从 “零” 开始的预训练\",\"t\":[\"从本节开始，我们将从\\\"零\\\"开始，体验Bert的预训练过程是如何实现的；\"]},\"696\":{\"h\":\"数据清洗\",\"t\":[\"首先我们需要准备一个小型语料库，确保在单台机器上，仅使用CPU就能完成整个训练过程，这里采用的是 wikitext-2 和 wikitext-103 两个开源数据集:\",\"WikiText 英语词库数据（The WikiText Long Term Dependency Language Modeling Dataset）是一个包含1亿个词汇的英文词库数据，这些词汇是从Wikipedia的优质文章和标杆文章中提取得到，包括WikiText-2和WikiText-103两个版本，相比于著名的 Penn Treebank (PTB) 词库中的词汇数量，前者是其2倍，后者是其110倍。每个词汇还同时保留产生该词汇的原始文章，这尤其适合当需要长时依赖(longterm dependency)自然语言建模的场景。\",\"Wikitext-103是超过 1 亿个语句的数据合集，全部从维基百科的 Good 与 Featured 文章中提炼出来。广泛用于语言建模，当中包括 fastai 库和 ULMFiT 算法中经常用到的预训练模型。\",\"WikiText2是Wikitext-103 的子集，主要用于测试小型数据集的语言模型训练效果。\",\"WIKITEXT-2\",\"WIKITEXT-103\",\"下载地址\",\"https://s3.amazonaws.com/fast-ai-nlp/wikitext-103.tgz\",\"https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz\",\"WikiText-2 和 WikiText-103 是两个广泛用于语言模型训练和评估的英文维基百科语料数据集 ，由 Salesforce 提出并开源。它们在 NLP 领域（特别是语言建模、预训练任务）中非常经典。\",\"将数据集压缩包下载到dataset目录下，并解压到当前目录下，然后使用prepare_data文件所提供代码对原始数据格式进行解析，得到对应的JSON格式文件:\",\"相关核心代码实现如下:\",\"def process_csv(file_path): \\\"\\\"\\\"处理CSV文件,返回处理后的句子列表\\\"\\\"\\\" all_sentences = [] with open(file_path, 'r', encoding='utf-8') as f: reader = csv.reader(f) for row in reader: # 使用NLTK库，将一整段文本按“句子”切分成一个句子列表。 # 处理每行文本：去除前后空格，过滤无效行 paragraph = [line.strip() for line in sent_tokenize(row[0]) if line.strip() and not line.strip().startswith('=') and not all(c in string.punctuation for c in line.strip())] # 过滤掉句子数少于2的行 paragraph = [line for line in paragraph if len(line.split('. ')) >= 2] # 确保句子数为偶数 if len(paragraph) % 2 != 0: paragraph = paragraph[:-1] all_sentences.extend(paragraph) return all_sentences def main(): # 处理两个CSV文件 test_sentences = process_csv('wikitext-2/test.csv') train_sentences = process_csv('wikitext-2/train.csv') # 写入JSON文件 train_output_path = 'wikitext-2/train.json' os.makedirs(os.path.dirname(train_output_path), exist_ok=True) with open(train_output_path, 'w', encoding='utf-8') as f: json.dump(train_sentences, f, indent=4, ensure_ascii=False) print(f\\\"成功生成JSON文件: {train_output_path}\\\") test_output_path = 'wikitext-2/test.json' os.makedirs(os.path.dirname(test_output_path), exist_ok=True) with open(test_output_path, 'w', encoding='utf-8') as f: json.dump(test_sentences, f, indent=4, ensure_ascii=False) print(f\\\"成功生成JSON文件: {test_output_path}\\\") if __name__ == \\\"__main__\\\": main()\"]},\"697\":{\"h\":\"分词器实现\",\"t\":[\"分词器的实现较为简单，首先是其初始化方法中需要完成：字典初始化，数据预加载(可挪到其他地方实现)。\",\"class Tokenizer: def __init__(self, vocab_file = None): vocab_data = None if vocab_file is not None: with open(vocab_file, 'r') as f: vocab_data = json.load(f) # 定义字典保存路径 dict_path = 'dataset/vocab_dict.json' # 尝试加载已保存的字典 if os.path.exists(dict_path): with open(dict_path, 'r', encoding='utf-8') as f: saved_dict = json.load(f) self.word2idx = saved_dict['word2idx'] self.idx2word = {int(k): v for k, v in saved_dict['idx2word'].items()} self.vocab_size = len(self.word2idx) else: # 首先加入特殊标记：PAD, CLS, SEP, MASK , UNK , 这些是 BERT 模型中常用的特殊 token self.word2idx = {f'{name}': idx for idx, name in enumerate(['PAD', 'CLS', 'SEP', 'MASK' , 'UNK'])} # 处理vocab_data为列表形式的情况 if isinstance(vocab_data, list): # 将所有文本合并成一个字符串 all_text = ' '.join(vocab_data) # 临时替换特殊标记 ，然后对句子进行分词 temp_text = all_text.replace('<unk>', 'UNK') sentences = word_tokenize(temp_text) # 获取所有单词并去重 word_list = list(set(sentences)) # 给每个普通词分配索引，从4开始（前面是特殊token）, 当前已经有的词数（4个特殊词） hold_place = len(self.word2idx) for idx, word in enumerate(word_list): if word == 'UNK': continue self.word2idx[word] = idx + hold_place else: raise ValueError(\\\"vocab_data must be a list\\\") # 创建反向映射：索引 → 单词 self.idx2word = {idx: word for word, idx in self.word2idx.items()} # 总词汇量 self.vocab_size = len(self.word2idx) # 确保映射是一一对应的 assert len(self.word2idx) == len(self.idx2word) # 保存字典到文件 with open(dict_path, 'w') as f: json.dump({ 'word2idx': self.word2idx, 'idx2word': self.idx2word }, f, indent=4) # 对列表数据进行解析 self.max_len = 103 if isinstance(vocab_data, list): self.word_ids = [] # 两两配对遍历 for i in range(0, len(vocab_data), 2): sent_a = vocab_data[i] sent_b = vocab_data[i+1] tokens_a = self.encode(sent_a) tokens_b = self.encode(sent_b) # 如果任一句子长度超过50，跳过这对 if len(tokens_a) > 50 or len(tokens_b) > 50: continue # 否则保存这两个句子的 token ID 列表 self.word_ids.append(tokens_a) self.word_ids.append(tokens_b)\",\"字典的构建过程太过粗糙，导致最终构建得到的字典过大并且还有很多噪声，从而模型训练学习到每个词的含义需要更大量的数据集且最终效果也不会很好，可考虑换成 HuggingFace 的 BertTokenizer / WordPieceTokenizer 实现。\",\"上面优化方向很多，比如: 去除含有低频词的句对，因为低频词出现次数极少，模型很难学到它们的语义表示。\",\"对外提供的编码和解码两个方法实现如下:\",\" def encode(self, text): return self.tokenize(text) def decode(self, tokens): return self.detokenize(tokens) def tokenize(self, text): sentences = word_tokenize(text) tokens = [] for word in sentences: if word in self.word2idx: tokens.append(self.word2idx[word]) else: # 如果遇到不存在于字典中的word，则使用UNK替换 tokens.append(self.word2idx['UNK']) return tokens def detokenize(self, tokens): return ' '.join([self.idx2word[token] for token in tokens])\",\"实际实现过程中，出于方便，还将一个工具方法整合到了分词器的实现之中，它是用于执行Bert MLM任务掩码策略的方法:\",\" # 执行Bert的掩码策略: 掩码候选位置，输入序列，掩码符号 def masking_procedure(self,cand_pos, input_ids, masked_symb): masked_pos = [] masked_tokens = [] # 对于所有掩码候选位置执行掩码策略： 80% 概率替换为[MASK]，10% 概率替换为随机词，10% 概率保持不变 for pos in cand_pos: masked_pos.append(pos) # 记录被掩码的位置 masked_tokens.append(input_ids[pos]) # 记录被掩码的原token if random.random() < p_mask: # 80% 概率替换为[MASK] input_ids[pos] = masked_symb elif random.random() > (p_mask + p_replace): # 10% 概率替换为随机词 rand_word_idx = random.randint(4, self.vocab_size - 1) input_ids[pos] = rand_word_idx else: # 10% 概率保持不变 pass return masked_pos, masked_tokens\"]},\"698\":{\"h\":\"Batch数据准备\",\"t\":[\"有了分词器后，我们需要读取并构建Batch数据，用于我们的预训练任务，该过程由make_data方法实现，具体步骤为:\",\"收集相同数量的相邻句对和非相邻句对。\",\"对每个句对构建用于NSP任务的样本，形式为: [CLS] + A + [SEP] + B + [SEP]。\",\"对每个句对构建用于MLM任务的样本，首先将[CLS] + A + [SEP] + B + [SEP]句子中20%的词执行掩码策略，而针对这20%需要被掩码的词之上，再按照80%用MASK掩码替换，10%用随机词替换，10%保持原样不动的形式进行处理；最后返回两个列表: 20%执行掩码的词的位置列表，20%执行掩码的词的原Token列表。\",\"将所有输入序列填充到等长max_len。\",\"返回构建得到的单个样本列表: [被掩码后的输入序列, 句子分隔列表 , 20%执行掩码的词的位置列表, 20%执行掩码的词的原Token列表, 是否为连贯的上下句]。\",\"所有样本列表构成Batch数据返回。\",\"def make_data(tokenizer): sentences = tokenizer.word_ids batch_data = [] len_sentences = len(sentences) # Step 1: 收集相邻句对 adjacent_pairs = [] for i in range(len_sentences - 1): a, b = i, i + 1 if len(sentences[a]) <= 50 and len(sentences[b]) <= 50: adjacent_pairs.append((a, b)) # Step 2: 随机生成等量的非相邻句对 non_adjacent_pairs = [] valid_indices = [i for i in range(len_sentences) if len(sentences[i]) <= 50] for a in valid_indices: candidates = [b for b in valid_indices if abs(a - b) > 1] if candidates: b = random.choice(candidates) non_adjacent_pairs.append((a, b)) # 打乱顺序 random.shuffle(adjacent_pairs) random.shuffle(non_adjacent_pairs) # 保证数量一致 min_count = min(len(adjacent_pairs), len(non_adjacent_pairs)) adjacent_pairs = adjacent_pairs[:min_count] non_adjacent_pairs = non_adjacent_pairs[:min_count] # 构建样本 for a, b in adjacent_pairs: sample = prepare_sample(tokenizer, a, b, is_next=True) batch_data.append(sample) for a, b in non_adjacent_pairs: sample = prepare_sample(tokenizer, a, b, is_next=False) batch_data.append(sample) return batch_data def prepare_sample(tokenizer, tokens_a_idx, tokens_b_idx, is_next): sentences = tokenizer.word_ids tokens_a = sentences[tokens_a_idx] tokens_b = sentences[tokens_b_idx] # 拼接 [CLS] + A + [SEP] + B + [SEP] input_ids = [tokenizer.word2idx['CLS']] + tokens_a + [tokenizer.word2idx['SEP']] + tokens_b + [ tokenizer.word2idx['SEP']] segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (1 + len(tokens_b)) # MLM 准备 n_pred = min(max_pred, max(1, int(len(input_ids) * 0.2))) cand_pos = [ i for i, token in enumerate(input_ids) if token not in {tokenizer.word2idx['CLS'], tokenizer.word2idx['SEP'], tokenizer.word2idx['PAD'], tokenizer.word2idx['UNK']} ] random.shuffle(cand_pos) masked_pos, masked_tokens = tokenizer.masking_procedure(cand_pos[:n_pred], input_ids, tokenizer.word2idx['MASK']) # Padding def pad(seq, target_len, pad_value=tokenizer.word2idx['PAD']): seq += [pad_value] * (target_len - len(seq)) pad(input_ids, tokenizer.max_len) pad(segment_ids, tokenizer.max_len) if max_pred > n_pred: pad(masked_pos, max_pred) pad(masked_tokens, max_pred) return [input_ids, segment_ids, masked_tokens, masked_pos, is_next]\"]},\"699\":{\"h\":\"模型\",\"t\":[\"本文中的 Bert 模型整体实现也比较简单，其中关于BertEncoders编码并输出结果的整个过程如下图所示:\",\"NSP 任务会利用 CLS Token 作为整个输入序列的全局信息聚合表示，再经过非线性变换后，进行二分类任务，判断下一个句子是否是当前句子的后续句子，具体过程如下图所示:\",\"MLM 任务会利用 masked_pos 从BertEncoders编码输出结果中提取出被掩码的位置对应的嵌入向量，经过相同的非线性变换后，将这些掩码Token对应的嵌入向量映射到词向量空间中去，得到模型预测的这些掩码Token对应的真实词，具体过程如下图所示:\",\"核心代码实现如下:\",\"class BERT(nn.Module): def __init__(self, n_layers, vocab_size, max_len): \\\"\\\"\\\" 初始化一个简化版的 BERT 模型，支持 MLM（掩码语言建模） 和 NSP（下一句预测） 两个任务。 参数： n_layers: Transformer 编码器层数 vocab_size: 词表大小 max_len: 最大序列长度 \\\"\\\"\\\" super(BERT, self).__init__() # 1. 词嵌入 + 位置嵌入 + 句子嵌入 self.embedding = Embeddings(vocab_size, max_len) # 2. 多个 Transformer 编码器层堆叠 self.encoders = nn.ModuleList([ EncoderLayer() for _ in range(n_layers) ]) # 3. Pooler 层：用于提取 [CLS] token 的表示，用于 NSP 任务 self.pooler = Pooler() # 4. 下一句预测（NSP）分类器 self.next_cls = nn.Linear(d_model, 2) # 输出维度为 2，表示是否是连续句子 self.gelu = gelu # GELU 激活函数 # 5. 权重共享：Pooler 层与 FC 层共享权重 shared_weight = self.pooler.fc.weight # 获取 pooler 中的全连接层权重 self.fc = nn.Linear(d_model, d_model) # 创建新的线性层 self.fc.weight = shared_weight # 共享权重（weight tying） # 6. 权重共享：MLM 分类器共享词嵌入矩阵 shared_weight = self.embedding.word_emb.weight # 获取词嵌入层权重 self.word_classifier = nn.Linear(d_model, vocab_size, bias=False) self.word_classifier.weight = shared_weight # 权重共享（tie weights） def forward(self, tokens, segments, masked_pos): \\\"\\\"\\\" 前向传播逻辑 输入： tokens: [batch_size, seq_len]，token 的索引（已添加 [CLS], [SEP], [MASK] 等） segments: [batch_size, seq_len]，segment_id，区分句子 A 和 B masked_pos: [batch_size, max_pred]，记录被掩码的位置 输出： logits_cls: [batch_size, 2]，NSP 分类结果 logits_lm: [batch_size, max_pred, vocab_size]，MLM 预测结果 \\\"\\\"\\\" # 1. 词嵌入 + 位置嵌入 + 句子嵌入 output = self.embedding(tokens, segments) # shape: [batch_size, seq_len, d_model] # 2. 构造 padding mask（忽略填充部分） enc_self_pad_mask = get_pad_mask(tokens) # shape: [batch_size, seq_len, seq_len] # 3. 依次通过每个编码器层（Transformer Layer） for layer in self.encoders: output = layer(output, enc_self_pad_mask) # output shape: [batch_size, seq_len, d_model] # 4. NSP 任务：使用 [CLS] 标记进行下一句预测 hidden_pool = self.pooler(output[:, 0]) # 提取 [CLS] 位置的隐藏状态并池化 logits_cls = self.next_cls(hidden_pool) # 分类输出：[batch_size, 2] # 5. MLM 任务：恢复被掩码的词 # masked_pos: [batch_size, max_pred] # 扩展 masked_pos 到三维，便于从 output 中 gather 出被掩码位置的表示 masked_pos = masked_pos.unsqueeze(-1).expand(-1, -1, d_model) # shape: [batch, max_pred, d_model] # 使用 torch.gather 从 output 中取出被掩码位置的 token 表示 h_masked = torch.gather(output, dim=1, index=masked_pos) # shape: [batch_size, max_pred, d_model] # 通过全连接层 + GELU 激活函数 h_masked = self.gather(output, dim=1, index=masked_pos) # 再次提取被掩码位置的表示 h_masked = self.gelu(self.fc(h_masked)) # shape: [batch_size, max_pred, d_model] # 6. MLM 分类器：预测被掩码的词 logits_lm = self.word_classifier(h_masked) # shape: [batch_size, max_pred, vocab_size] # 返回两个任务的结果 return logits_cls, logits_lm\",\"完整的代码实现部分，大家参考仓库源码即可，本文不再全部Copy展示。\"]},\"700\":{\"h\":\"训练\",\"t\":[\"训练过程就比较常规了，有一点不同就是Bert预训练阶段的学习目标是: MLM Loss + NSP Loss ，具体核心代码实现如下:\",\"tokenizer = Tokenizer(\\\"dataset/wikitext-2/train.json\\\") batch_data = make_data(tokenizer) batch_tensor = [torch.LongTensor(ele) for ele in zip(*batch_data)] dataset = BERTDataset(*batch_tensor) dataloader = DataLoader(dataset, batch_size=32, shuffle=True) model = BERT(n_layers,tokenizer.vocab_size,tokenizer.max_len) lr = 1e-4 epochs = 100 # 优化器与学习率调度器 optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01) # 损失函数 + 标签平滑 criterion1 = nn.CrossEntropyLoss(label_smoothing=0.1) criterion2 = nn.CrossEntropyLoss(label_smoothing=0.1,ignore_index=0) # 加载检查点 checkpoint_path = 'best_model.pth' if os.path.exists(checkpoint_path): model.load_state_dict(torch.load(checkpoint_path, weights_only=True, map_location=device)) print('Loaded checkpoint from', checkpoint_path) model.to(device) best_loss = float('inf') # training total_batches = len(dataloader) for epoch in range(epochs): avg_loss = 0 for batch_idx, one_batch in enumerate(dataloader): input_ids, segment_ids, masked_tokens, masked_pos, is_next = [ele.to(device) for ele in one_batch] logits_cls, logits_lm = model(input_ids, segment_ids, masked_pos) # NSP 任务损失 loss_cls = criterion1(logits_cls, is_next) # MLM 任务损失 loss_lm = criterion2(logits_lm.view(-1, tokenizer.vocab_size), masked_tokens.view(-1)) loss_lm = (loss_lm.float()).mean() # 总损失 loss = loss_cls + loss_lm avg_loss += loss.item() if (epoch + 1) % 1 == 0: print(f'Epoch:{epoch + 1} Batch:{batch_idx + 1}/{total_batches} \\\\t loss: {loss:.6f}') loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) optimizer.step() avg_loss /= total_batches # 保存最优模型 if avg_loss < best_loss: best_loss = avg_loss torch.save(model.state_dict(), f'best_model.pth') print(f'Saved best model with loss: {best_loss:.6f}') # 效果评估 evaluate_model()\",\"由于模型输出的logits_cls是一个二分类值，因此我们只需要根据is_next取出索引0或者1下标对应的值即可知道我们是否预测正确，并且使用预测结果计算NSP任务损失值。\",\"对于MLM任务损失计算来说，我们只会计算被随机遮盖或替换的部分，其余部分不做损失，因此模型返回的logits_lm也只包含被掩码的Token对应的模型预测真实词，同时通过masked_tokens可知这些被掩码Token对应的真实词作为Label，从而计算交叉熵损失就很简单了。\",\"这里需要注意一点，对于MLM任务损失计算来说，我们需要在其对应的CrossEntropyLoss中指定ignore_index=0，即忽略掉PAD部分的损失计算；\",\"这里PAD部分指的是对于不同的句子，它们都是按照其序列长度的20%比例进行的掩码，而对于较短的句子，其掩码数量可能会偏少，因此为了确保masked_tokens列表中所有句子掩码数量一致，需要对掩码数量不足max_pred的进行PAD填充。\",\"模型返回的logits_lm中同样含有PAD部分，但是我们在计算损失时指定了ignore_index=0，即忽略掉PAD部分的损失计算，因此不会影响最终的损失值计算。\",\"gather函数比较灵活，它可以在指定维度上，根据索引矩阵，从源张量中提取特定位置的元素，构造一个新的张量。\",\"对于每一个输出位置 (i,j)，如果 dim=1（列方向），那么它从 input [index[i][j]][j] 中取值。\",\"对于每一个输出位置 (i,j)，如果 dim=0（列方向），那么它从 input [i][index[i][j]] 中取值。\"]},\"701\":{\"h\":\"效果\",\"t\":[\"本文所展示的Bert预训练属于教学级别的，最终的训练效果也是一般，仅供参考和学习:\",\"MLM Task: Correct / Total = 3167 / 9027 | Accuracy = 0.3508 (预测正确的掩码词数量/总掩码的词数量)\",\"NSP Task: Correct / Total = 504 / 960 | Accuracy = 0.5250 (预测正确的句对数量/总句对数量)\"]},\"702\":{\"h\":\"Details\",\"t\":[\"本节将会对Bert模型实现的部分细节进行说明。\"]},\"703\":{\"h\":\"Padding Mask 如何生成并起作用的 ？\",\"t\":[\"首先模型会根据传入的Tokens列表生成一个Pad Mask矩阵，该 矩阵维度 和 Q@K.T 后得到的注意力得分矩阵维度相同\",\"def get_pad_mask(tokens, pad_idx=0): ''' suppose index of [PAD] is zero in word2idx tokens: [batch, seq_len] ''' batch, seq_len = tokens.size() pad_mask = tokens.data.eq(pad_idx).unsqueeze(1) # （batch,1,seq_len) pad_mask = pad_mask.expand(batch, seq_len, seq_len) # （batch,seq_len,seq_len) return pad_mask\",\"假设输入的Token序列为: [A,B,C,PAD,PAD,PAD] , 则生成的Pad Mask模样为:\",\"在注意力得分矩阵计算完毕后，我们会使用Pad Mask矩阵将注意力得分矩阵中对应位置的得分设置为一个非常小的值，这样在后续的Softmax计算中，这些位置的概率就会接近0，从而在注意力机制中就不会考虑到这些PAD部分的Token了。\",\"class ScaledDotProductAttention(nn.Module): def forward(self, Q, K, V, attn_mask): scores = torch.matmul(Q, K.transpose(-1, -2) / msqrt(d_k)) # scores: [batch, n_heads, seq_len, seq_len] scores.masked_fill_(attn_mask, -1e9) attn = nn.Softmax(dim=-1)(scores) # context: [batch, n_heads, seq_len, d_v] context = torch.matmul(attn, V) return context\",\"横着看是计算某个词与全局序列中其他词的相关度，后续需要利用该相关度完成当前词的全局上下文信息融合，我们只需要确保对于某个词的上下文融合不被PAD词参与即可，而无需考虑PAD词的全局上下文信息是否需要进行计算。\"]},\"704\":{\"h\":\"位置编码\",\"t\":[\"位置编码\"]},\"705\":{\"h\":\"绝对位置编码（Absolute Positional Encoding, APE）\"},\"706\":{\"h\":\"正弦/余弦位置编码（Sinusoidal Positional Encoding）\",\"t\":[\"正弦余弦位置编码（Sinusoidal Positional Encoding）是一种无需训练的位置编码方法，它通过固定的周期性函数（正弦和余弦）来为序列的不同位置提供唯一的编码。对于每个位置 和每个维度 ，位置编码通过以下公式计算：\",\" 表示位置索引，表示计算哪个位置的编码\",\" 表示编码维度， 是编码空间的总维度\",\" 和 通过正弦和余弦函数分裂映射到偶数和奇数的维度\",\"假设：我们要计算输入序列第 2 个位置 Token 对应的位置编码，编码的维度设定为 4 ，则：\",\"最终，位置 2 的编码向量为：，我们把它加到第二个 Token 的词嵌入向量上，就相当于给其注入了顺序信息。\",\"计算起来是比较容易的，如何去理解这个位置编码？请看下面这张图：\",\"这三张图分别打印了 128 个位置向量第 2、6、12 维度的编码值的变化，我们发现这些值呈现周期性的变化。另外，我们也可以发现，向量维度越高，其周期就越长。\",\"上图，我们打印了位置 1、5 的编码向量中的 sin 和 cos 计算得到的编码值。我们可以发现，基于正弦和余弦函数得到的位置编码可以保证唯一性。另外，也可以看到，向量的维度越高，编码值的波动就越小，向量就越接近。\",\"简单总结下：\",\"因为正弦和余弦函数都是周期函数，编码在不同维度上具有不同的周期性\",\"位置编码向量是唯一的，因为不同位置的编码由不同的正弦和余弦值组成\",\"低维度的编码值波动性很大（周期短），高纬度的编码值波动性较小（周期长）\",\"所以，可以得到一个简单的结论：\",\"低维分量（小 i）的变化较快，主要捕捉局部位置关系\",\"高维分量（大 i）的变化较慢，可以用于编码全局信息\",\"这个怎么去理解？我们把某个位置的向量大概划分为两部分：低维向量部分 + 高维向量部分，低纬向量部分数值波动幅度很大，在一个周期内只能包含少量相邻的位置，并且一定程度上也表达了位置的局部的相对信息，这就是捕捉局部位置关系。那么，对于高纬向量部分而言，它的波动幅度很小，一个周期能够包含更多的位置信息，这也是我们理解的编码全局位置信息的含义。\",\"所以，对于一个基于正弦余弦编码的位置向量，可以理解为该向量中隐含了一些局部和全局的位置信息。使得 Transformer 既能感知局部相对位置，也能感知全局位置信息，从而弥补其原生结构中缺少位置感知能力的缺陷。\",\"当然，这种位置编码方法也存在以下一些不足之处：\",\"随着序列长度的增加，位置编码的周期性可能导致不同位置之间的区分度逐渐降低，难以准确表示极长序列中各个位置的独特信息。\",\"虽然正弦余弦位置编码能够隐含地表达一定的局部位置信息，但由于它是固定的、不可学习的，并没有专门针对局部依赖关系进行优化，因此在建模局部依赖关系时能力相对不足。\",\"正弦余弦位置编码是一种基于三角函数的固定编码方式，它是一种静态的位置信息表示。而注意力机制更关注的是文本中不同位置之间的动态语义关联。这两种信息在表示形式和语义侧重点上存在差异，导致在融合时可能无法很好地相互补充。\",\"正弦余弦位置编码通常是高维向量，其计算量会随着维度的增加不仅需要更多的计算时间，还可能占用大量的内存空间，影响模型的运行效率。\",\"正余弦位置编码通过在不同维度上引入不同波长的正余弦信号，使得低维对局部位置变化敏感，高维对全局位置变化敏感。虽然位置和语义在所有维度上混合，但在训练中，模型可能学到一种“低维更多位置，高维更多语义”的分工模式。\"]},\"707\":{\"h\":\"基于可学习的嵌入\",\"t\":[\"可学习的位置编码（Learnable Positional Encoding, LPE）是一种通过梯度下降自动学习位置编码的方法，不同于固定编码（如正弦/余弦函数编码），它不依赖任何手工设计的公式，而是直接让模型在训练过程中优化位置信息。对于一个长度为 𝐿 的输入序列，每个位置 𝑖 都对应一个可学习的向量。当训练或测试时，将输入 Token 的编码和对应位置的可学习位置编码向量相加，从而赋予 Token 相应的位置信息。\",\"这种位置编码方式能够根据具体的任务和数据特点，模型可以学习到更适合该类文本的位置表示方式，捕捉文本中位置相关的语义和结构信息，这是固定的位置编码（例如：正弦余弦位置编码）难以做到的。但是也存在一些不足之处，例如：\",\"如果训练时 max_len=512，测试时输入 1024 长度的序列，模型就无法处理了\",\"需要存储 max_len×d_model 维度的参数，可能导致大模型训练更难收敛\"]},\"708\":{\"h\":\"相对位置编码（Relative Position Encoding, RPE）\",\"t\":[\"相对位置编码直接对 两个 token 之间的距离 建模。例如：\",\"token i 关注 token j 时，注意力分数不仅取决于它们的内容，还取决于 i - j 的相对位置。\",\"如果两个位置的相对距离相同，那么它们的相对位置信息也是相同的（模型可以更好泛化到更长的序列）。\"]},\"709\":{\"h\":\"Relative Position Representations (Shaw et al., 2018)\",\"t\":[\"普通自注意力打分公式：\",\"其中：\",\"Shaw 相对位置编码改成：\",\"这里：\",\" 是和相对距离 对应的向量（可训练）。\",\"只要知道 ，就能从一个 embedding table 里查到对应的 向量。\",\"下面先给出完整代码实现，然后再进行详细解析:\",\"import torch import torch.nn as nn import math class RelPosAttention(nn.Module): def __init__(self, d_model, n_heads, max_len): super().__init__() self.n_heads = n_heads self.d_k = d_model // n_heads self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, d_model) self.W_v = nn.Linear(d_model, d_model) self.W_o = nn.Linear(d_model, d_model) self.rel_emb = nn.Embedding(2 * max_len - 1, self.d_k) # 相对位置向量表 self.max_len = max_len def forward(self, x): B, L, _ = x.size() assert L <= self.max_len, \\\"输入序列长度超过max_len\\\" Q = self.W_q(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2) # (B, h, L, d_k) K = self.W_k(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2) V = self.W_v(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2) # ===== 1. 普通注意力部分 ===== content_score = torch.matmul(Q, K.transpose(-2, -1)) # (B, h, L, L) # ===== 2. 相对位置部分 ===== # 相对位置索引矩阵 rel_pos = torch.arange(L, device=x.device).unsqueeze(1) - torch.arange(L, device=x.device).unsqueeze(0) rel_pos += self.max_len - 1 # shift到[0, 2L-2] R = self.rel_emb(rel_pos) # (L, L, d_k) # 使用爱因斯坦求和公式计算 Q_i ⋅ R_{i-j} pos_score = torch.einsum('bhld,lmd->bhlm', Q, R) # ===== 3. 合并并归一化 ===== scores = (content_score + pos_score) / math.sqrt(self.d_k) attn = torch.softmax(scores, dim=-1) out = torch.matmul(attn, V) # (B, h, L, d_k) out = out.transpose(1, 2).contiguous().view(B, L, -1) return self.W_o(out)\",\"首先来看一下 Shaw 相对位置编码中的相对位置矩阵的详解:\",\"生成相对位置差矩阵\",\"rel_pos = torch.arange(L, device=x.device).unsqueeze(1) - torch.arange(L, device=x.device).unsqueeze(0)\",\"假设序列长度 L = 4，torch.arange(L) 是：\",\"[0, 1, 2, 3]\",\"unsqueeze(1) 变成列向量 shape (4,1)\",\"unsqueeze(0) 变成行向量 shape (1,4)\",\"做减法（广播规则）：\",\"[[ 0-0, 0-1, 0-2, 0-3], [ 1-0, 1-1, 1-2, 1-3], [ 2-0, 2-1, 2-2, 2-3], [ 3-0, 3-1, 3-2, 3-3]]\",\"结果是：\",\"[[ 0, -1, -2, -3], [ 1, 0, -1, -2], [ 2, 1, 0, -1], [ 3, 2, 1, 0]]\",\"含义：第 i 行第 j 列的值就是 i - j，即 token i 与 token j 的相对距离。\",\"平移到正索引区间\",\"rel_pos += self.max_len - 1\",\"Embedding 的索引必须是 非负整数，所以要把负值平移到正数区间。如果 max_len=4，self.max_len - 1 = 3，加 3 后：\",\"[[3, 2, 1, 0], [4, 3, 2, 1], [5, 4, 3, 2], [6, 5, 4, 3]]\",\"值域范围是 [0, 2*max_len-2]。这正好对应 self.rel_emb 的 embedding 表大小 (2*max_len - 1, d_k)。\",\"查表得到相对位置向量\",\"R = self.rel_emb(rel_pos) # shape: (L, L, d_k)\",\"self.rel_emb 是一个可训练的 embedding 表，每个相对距离对应一个向量。rel_pos 的形状是 (L, L)，查表后：\",\"第一维 = 查询位置 i\",\"第二维 = 被关注位置 j\",\"第三维 = 对应的相对位置编码向量 ，长度 d_k\",\"所以 R 是一个 (L, L, d_k) 张量。\",\"用爱因斯坦求和公式计算位置分数\",\"pos_score = torch.einsum('bhld,lmd->bhlm', Q, R)\",\"先看两个输入的形状:\",\"Q: (B, h, L, d_k)\",\"B = batch 大小\",\"h = 注意力头数\",\"L = 序列长度（查询 token 的位置）\",\"d_k = 每个头的向量维度（Q 向量长度）\",\"R: (L, L, d_k)\",\"第 1 维：查询位置 index \",\"第 2 维：被关注位置 index \",\"第 3 维：与相对位置 对应的向量（长度 d_k）\",\"爱因斯坦求和规则: 'bhld,lmd->bhlm'\",\"左边 Q 的维度：b h l d\",\"右边 R 的维度：l m d\",\"两者中相同字母代表要“配对”的轴：\",\"l：查询位置 i → 保持不变（参与配对但保留在输出里）\",\"d：向量维度 → 相同字母且不出现在输出，表示要相乘后求和（点积）\",\"不同字母：\",\"m：来自 R 的“被关注位置”维度，出现在输出\",\"b、h：来自 Q 的 batch 和头维度，直接保留\",\"我们想算：\",\"固定 batch b、head h、查询位置 l、被关注位置 m\",\"从 Q 里取对应的查询向量 \",\"从 R 里取对应的相对位置向量 \",\"对它们做向量点积（沿 d 维求和）\",\"为什么相对位置矩阵 不直接加到注意力分数里，而是还要和 做点积？\",\"在标准自注意力中，打分是：\",\"其中：\",\"：查询 token i 的内容向量\",\"：被关注 token j 的内容向量\",\"这两个向量都在同一个向量空间中（维度 d_k），点积才能得到一个有意义的相似度分数。Shaw 的做法是：\",\"第二项的解释：\",\" 也是在 d_k 维的向量空间中。\",\"用 和 做点积，把位置信息向量投影到和 Q_i 一致的表示空间。\",\"这样得到的结果是一个标量，可以直接和 Q·K 的结果相加。\",\"换句话说：\",\"K 部分带来内容相关性\",\"R 部分带来位置相关性\",\"二者都要在同一个“Q 向量的视角”下衡量，所以都用 做点积\",\"可以把注意力打分看作“你(i)对别人(j)的关注程度”，它可能由两部分组成：\",\"内容相似度：你关心和你内容类似的人（Q·K）\",\"位置偏好：你关心离你近的人（Q·R）\"]},\"710\":{\"h\":\"T5 相对位置偏置（Relative Position Bias, RPB）\",\"t\":[\"在自注意力里，注意力打分是\",\"T5 在这个分数上再加一个与“相对距离”相关的标量偏置：\",\" 是查询位置到键位置的相对距离。\",\"为了参数更省、泛化更好，T5 不为每个距离单独学一个参数，而是把距离映射到少量“桶”（bucket）：近距离用细桶、远距离用粗桶（对数分桶）。\",\"每个桶学一个标量偏置，通常 按头（per-head）独立学习（形状：[num_buckets, n_heads]），再广播到 (B, n_heads, L_q, L_k)。\",\"这样既让模型“偏好临近”或“惩罚太远”，又几乎不增加计算量。\",\"Shaw RPR：相对位置像“额外的 Key 特征”，参与点积计算\",\"T5 RPB：相对位置像“分数修正表”，只在 attention 分数上加偏置\",\"偏置表（bias table）怎么理解 ?\",\"bias_table = nn.Parameter(torch.zeros(num_buckets, H))\",\"num_buckets：表示“相对位置的分组数量”。\",\"T5 并不对每个相对位置都单独存一个偏置，而是把相对距离压缩到若干个桶（bucket）里。\",\"比如小距离 1、2、3 可以映射到同一个桶，远距离可能映射到不同桶。\",\"H：表示注意力头数，每个头的偏置可以不同。\",\"不同头可以学习到不同的相对位置偏置模式，比如一个头关注短距离，一个头关注长距离。\",\"偏置表作用:\",\"偏置表里的每个元素是一个标量，它对应某个相对位置桶 + 注意力头的偏置。\",\"它不会参与点积运算，只是直接加到注意力分数上：\",\"通过加偏置，模型可以编码“相对位置信息”，比如让模型偏向关注附近的 token。\",\"可以想象成一个小表格：\",\"桶 (bucket)\",\"head0\",\"head1\",\"head2\",\"0\",\"0.1\",\"-0.2\",\"0.05\",\"1\",\"0.3\",\"0.0\",\"0.1\",\"2\",\"-0.1\",\"0.2\",\"0.0\",\"…\",\"…\",\"…\",\"…\",\"每一列 = 一个头的所有偏置\",\"每一行 = 一个相对距离桶的偏置\",\"当模型计算注意力分数时，它会根据 query-key 的相对距离找到对应的桶，然后查表取出偏置，加到该头的注意力分数上。\",\"桶的作用是什么 ？\",\"我们已经有了 相对位置矩阵relative_position[i,j] = j - i ，例如长度 8 的序列：\",\"i\\\\j 0 1 2 3 4 5 6 7 0 0 1 2 3 4 5 6 7 1 -1 0 1 2 3 4 5 6 2 -2 -1 0 1 2 3 4 5 3 -3 -2 -1 0 1 2 3 4 ...\",\"每个元素表示 query i 对 key j 的相对距离\",\"问题：序列很长时，如果直接为每个距离学习偏置，参数量会非常大。\",\"桶映射的目的:\",\"把相对距离压缩到固定数量的桶里\",\"T5 的做法：小距离用独立桶，大距离用对数压缩的桶\",\"这样：\",\"短距离：每个距离有自己的偏置（细粒度）\",\"长距离：距离较大的 token 共享同一桶（粗粒度）\",\"优点：\",\"节省参数\",\"保持模型关注短距离精细信息，同时对长距离不必过于精细\",\"假设 num_buckets = 4（简化）：\",\"relative_position ≤ 1 → bucket 0\",\"relative_position = 2 → bucket 1\",\"relative_position = 3~4 → bucket 2\",\"relative_position > 4 → bucket 3\",\"对序列 [A,B,C,D]：\",\"relative_position: i\\\\j 0 1 2 3 0 0 1 2 3 1 -1 0 1 2 2 -2 -1 0 1 3 -3 -2 -1 0 映射到桶 (bucket): i\\\\j 0 1 2 3 0 0 0 1 2 1 0 0 0 1 2 1 0 0 0 3 2 1 0 0\",\"每个元素都变成了 0~num_buckets-1 的整数\",\"这个整数就是偏置查表的索引\",\"核心理解：\",\"相对位置桶 = “距离分组”\",\"通过桶映射，可以让模型 对短距离敏感，对长距离粗略处理\",\"桶号最终用来查 bias_table，得到每个 query-key 对的偏置\",\"整体流程 ?\",\"假设：\",\"batch size: B\",\"序列长度: L\",\"注意力头数: H\",\"相对位置桶数: num_buckets\",\"(1) 偏置表\",\"bias_table = nn.Parameter(torch.zeros(num_buckets, H))\",\"shape: (num_buckets, H)\",\"每个桶每个头都有一个标量偏置\",\"(2) 相对位置矩阵\",\"relative_position = memory_position - context_position # (L,L)\",\"shape: (L, L)\",\"值是 j-i，表示 query i 对 key j 的相对位置\",\"(3) 桶映射\",\"relative_buckets = relative_position_bucket(relative_position) # (L,L)\",\"shape: (L,L)\",\"每个元素是 0~num_buckets-1\",\"(4) 查表得到偏置\",\"bias = bias_table[relative_buckets] # (L,L,H) bias = bias.permute(2,0,1) # (H,L,L)\",\"bias_table[relative_buckets] 用每个 (i,j) 桶号查表\",\"得到 (L,L,H)，然后换维度到 (H,L,L)，方便加到注意力分数\",\"(5) 广播 batch\",\"注意力分数：\",\"attn_scores = Q @ K.transpose(-1,-2) # (B,H,L,L) attn_scores = attn_scores + bias.unsqueeze(0) # (B,H,L,L)\",\"bias.unsqueeze(0) shape → (1,H,L,L)\",\"自动广播到 batch 维度 (B,H,L,L)\",\"每个头有独立偏置，不同 batch 共用\",\"(6) 相加过程总结\",\"先算 原始注意力分数：Q @ K^T → (B,H,L,L)\",\"查表得到 相对位置偏置：bias → (H,L,L)\",\"加到注意力分数：\",\"最终 shape 仍然 (B,H,L,L)，用于 softmax\",\"代码实现如下:\",\"import torch import torch.nn as nn class T5RelativePositionBias(nn.Module): def __init__(self, num_buckets=32, max_distance=128, n_heads=12): super().__init__() self.num_buckets = num_buckets self.max_distance = max_distance self.n_heads = n_heads # 偏置表：每个 bucket 对每个 attention head 存一个标量 self.relative_attention_bias = nn.Embedding(num_buckets, n_heads) def _relative_position_bucket(self, relative_position): \\\"\\\"\\\" 将相对位置映射到 [0, num_buckets-1] 的桶索引。 设计目的： - 相对位置可能范围很大（如 -512 ~ +512） - 如果直接给每个位置都分配一个独立参数，参数量会很大 - 解决方法： 1. 对短距离用线性分桶（精确表示） 2. 对长距离用对数分桶（粗略表示） 3. 可选地分开处理方向（左/右） 参数： relative_position: torch.Tensor - 形状 (Lq, Lk) 或 (..., Lq, Lk) - 元素为 key_position - query_position 负数：key 在 query 左边 正数：key 在 query 右边 返回： ret: torch.LongTensor - 形状与 relative_position 相同 - 每个元素是该相对位置对应的桶编号 \\\"\\\"\\\" num_buckets = self.num_buckets # 桶的总数量，例如 32 max_distance = self.max_distance # 能映射的最大距离（超过按最大处理） ret = 0 # 初始化桶编号 # 1. 取反（T5 定义相对位置为 memory_pos - context_pos，这里保持一致性） n = -relative_position # n < 0 表示 key 在 query 右边，n > 0 表示 key 在 query 左边 # 2. 方向分桶（可选） # 如果 key 在 query 左边 (n < 0)，桶号加 num_buckets//2 # 这样前一半桶表示左方向，后一半桶表示右方向 ret += (n < 0).to(torch.long) * num_buckets // 2 # 3. 只取绝对值（方向信息已在上一步编码） n = torch.abs(n) # 4. 定义短距离线性映射的阈值 # 前一半桶（num_buckets//2）用于精确表示短距离 max_exact = num_buckets // 2 # 例如 num_buckets=32 时，max_exact=16 # 5. 判断哪些是短距离 is_small = n < max_exact # 布尔张量 # 6. 对长距离做对数映射 # - 将距离范围 [max_exact, max_distance] 映射到桶 [max_exact, num_buckets-1] # - 对数映射可以把大范围的距离压缩到少量桶 val_if_large = max_exact + ( (torch.log(n.float() / max_exact) / # 距离归一化并取对数 torch.log(max_distance / max_exact)) # 对数分母：归一化最大距离 * (num_buckets - max_exact) # 映射到长距离桶区间 ).to(torch.long) # 7. 防止溢出（大于最大桶号的全部压到最后一个桶） val_if_large = torch.min( val_if_large, torch.full_like(val_if_large, num_buckets - 1) ) # 8. 根据距离类别（短/长）选择桶编号 ret += torch.where(is_small, n, val_if_large) return ret def forward(self, query_length, key_length): \\\"\\\"\\\" 返回 shape (1, n_heads, query_length, key_length) 的偏置矩阵 \\\"\\\"\\\" # 计算相对位置矩阵 (i - j) context_pos = torch.arange(query_length)[:, None] # (Lq, 1) memory_pos = torch.arange(key_length)[None, :] # (1, Lk) relative_position = memory_pos - context_pos # (Lq, Lk) # 映射到桶索引 rp_bucket = self._relative_position_bucket(relative_position) # 查表获取偏置值 (Lq, Lk, n_heads) values = self.relative_attention_bias(rp_bucket) # 调整维度 → (1, n_heads, Lq, Lk)，方便加到 attention scores 上 values = values.permute(2, 0, 1).unsqueeze(0) return values\",\"Step 1 — 偏置表的定义\",\"self.relative_attention_bias = nn.Embedding(num_buckets, n_heads)\",\"形状 = (num_buckets, n_heads)\",\"表示每个 桶 对应每个 注意力头 的一个标量偏置\",\"为什么是 Embedding：\",\"桶号是整数索引（0~num_buckets-1）\",\"Embedding 可以高效查表\",\"Step 2 — 构造相对位置矩阵\",\"context_pos = torch.arange(query_length)[:, None] # (Lq, 1) memory_pos = torch.arange(key_length)[None, :] # (1, Lk) relative_position = memory_pos - context_pos # (Lq, Lk)\",\"(i,j) 元素 = j - i\",\"例如 L=4：\",\"[[0, 1, 2, 3], [-1, 0, 1, 2], [-2, -1, 0, 1], [-3, -2, -1, 0]]\",\"Step 3 — 相对位置桶映射\",\"rp_bucket = self._relative_position_bucket(relative_position)\",\"目的：把 [-max_len, +max_len] 映射到 0 ~ num_buckets-1\",\"T5策略：\",\"短距离（|n| < max_exact）→ 线性映射，每个距离单独一个桶\",\"长距离 → 对数映射，多个距离共享一个桶\",\"正负方向可能分桶（方向信息保留）\",\"结果：桶矩阵 shape (Lq, Lk)，值是整数\",\"Step 4 — 查表获取偏置\",\"values = self.relative_attention_bias(rp_bucket) # (Lq, Lk, n_heads)\",\"对 (i,j) 的桶号 rp_bucket[i,j]，查表得到 shape (n_heads,) 的偏置\",\"最终得到 (Lq, Lk, n_heads)，即每个位置对每个 head 的偏置值\",\"Step 5 — 调整维度以便加到注意力分数\",\"values = values.permute(2, 0, 1).unsqueeze(0) # (1, n_heads, Lq, Lk)\",\"attention 分数 shape = (batch, n_heads, Lq, Lk)\",\"偏置加法公式：\",\"unsqueeze(0) 是为了 batch 维度可广播\",\"Step 6 — 加到注意力分数的时机\",\"在 T5 的多头注意力里，这一步通常是这样做的：\",\"scores = torch.matmul(Q, K.transpose(-1, -2)) # (B, H, Lq, Lk) scores += position_bias # (1, H, Lq, Lk) 广播相加\",\"区别于 Shaw 2018 的做法（它在 Q·K 之前，把位置向量加到 K 里），T5 是在 attention scores 已经计算完之后 再加偏置。\",\"一句话总结：\",\"T5 RPB 用 桶映射 + 查表 的方式，为每个 query-key 对加一个与相对位置有关的标量偏置（每个 head 单独学习），它是在注意力分数计算完之后加上去的，因此只需要二维标量表，不需要像 Shaw 那样存整个向量矩阵。\"]},\"711\":{\"h\":\"图解 Bert\",\"t\":[\"图解Bert & Bert文本分类实战\"]},\"712\":{\"h\":\"环境搭建\",\"t\":[\"按序执行以下命令完成环境搭建:\",\"git clone https://github.com/DA-southampton/Read_Bert_Code.git cd Read_Bert_Code conda create -n Read_Bert_Code python=3.9.22 conda activate Read_Bert_Code\",\"本文使用的是谷歌的中文预训练模型：chinese_L-12_H-768_A-12.zip，模型有点大，我就不上传了，如果本地不存在，就点击这里直接下载,或者直接命令行运行\",\"wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\",\"预训练模型下载下来之后，进行解压，然后将tf模型转为对应的pytorch版本即可。对应代码如下:\",\"export BERT_BASE_DIR=/Users/zhandaohong/Read_Bert_Code/chinese_L-12_H-768_A-12 python convert_tf_checkpoint_to_pytorch.py \\\\ --tf_checkpoint_path$BERT_BASE_DIR/bert_model.ckpt \\\\ --bert_config_file$BERT_BASE_DIR/bert_config.json \\\\ --pytorch_dump_path$BERT_BASE_DIR/pytorch_model.bin\",\"转化成功之后，将模型放入到仓库对应位置：\",\"Read_Bert_Code/bert_read_step_to_step/prev_trained_model/\",\"并重新命名为：\",\" bert-base-chinese\",\"其次是准备训练数据，这里我准备做一个文本分类任务，使用的是Tnews数据集，这个数据集来源是这里，分为训练，测试和开发集，我已经上传到了仓库中，具体位置在\",\"Read_Bert_Code/bert_read_step_to_step/chineseGLUEdatasets/tnews\",\"需要注意的一点是，因为我只是为了了解内部代码情况，所以准确度不是在我的考虑范围之内，所以我只是取其中的一部分数据，其中训练数据使用1k，测试数据使用1k，开发数据1k。\",\"准备就绪，使用pycharm导入项目，准备调试，我的调试文件是run_classifier.py文件，对应的参数为\",\"--model_type=bert --model_name_or_path=prev_trained_model/bert-base-chinese --task_name=\\\"tnews\\\" --do_train --do_eval --do_lower_case --data_dir=./chineseGLUEdatasets/tnews --max_seq_length=128 --per_gpu_train_batch_size=16 --per_gpu_eval_batch_size=16 --learning_rate=2e-5 --num_train_epochs=4.0 --logging_steps=100 --save_steps=100 --output_dir=./outputs/tnews_output/ --overwrite_output_dir\",\"然后启动 run_classifier.py 文件进行调试即可 , 所参考源仓库未提供requirements.txt文件，因此需要大家自行完成运行时缺失依赖包的安装。\"]},\"713\":{\"h\":\"数据预处理\",\"t\":[\"输入数据格式\",\"{ \\\"guid\\\": \\\"train-0\\\", \\\"label\\\": \\\"104\\\", // 文本分类任务: 文本对应的标签 \\\"text_a\\\": \\\"股票中的突破形态\\\", \\\"text_b\\\": null // NSP任务: 用于判断给出的两个句子是否连续 }\",\"NSP (Next Sentence Prediction)\",\"文本分词 & 借助字典映射为word id\",\"\\\"股票中的突破形态\\\" --> ['股', '票', '中', '的', '突', '破', '形', '态'] --> [5500, 4873, 704, 4638, 4960, 4788, 2501, 2578]\",\"对于字典中不存在的词 , 用 [UNK] 表示, 对应的id为 100\",\"过长截断策略\",\"添加特殊Token标记\",\"原序列添加特殊Token标记图\",\"[101, 5500, 4873, 704, 4638, 4960, 4788, 2501, 2578, 102]\",\"BertTokenizer中的特殊token id:\",\"[CLS]: 101\",\"[SEP]: 102\",\"[MASK]: 103\",\"[UNK]: 100\",\"[PAD]: 0\",\" # BertTokenizer def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None): if token_ids_1 is None: return [self.cls_token_id] + token_ids_0 + [self.sep_token_id] cls = [self.cls_token_id] sep = [self.sep_token_id] return cls + token_ids_0 + sep + token_ids_1 + sep\",\"创建句子辨识列表，用以区分不同的句子\",\"token_type_ids作用图解\",\" # BertTokenizer def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None): \\\"\\\"\\\" Creates a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence pair mask has the following format: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 | first sequence | second sequence if token_ids_1 is None, only returns the first portion of the mask (0's). \\\"\\\"\\\" sep = [self.sep_token_id] cls = [self.cls_token_id] if token_ids_1 is None: return len(cls + token_ids_0 + sep) * [0] return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\",\"创建用以区分special tokens部分的mask列表\",\"special_tokens_mask作用图解\",\" # BertTokenizer def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False): if token_ids_1 is not None: return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1] return [1] + ([0] * len(token_ids_0)) + [1]\",\"超长截断\",\" # PreTrainedTokenizer if max_length and len(encoded_inputs[\\\"input_ids\\\"]) > max_length: encoded_inputs[\\\"input_ids\\\"] = encoded_inputs[\\\"input_ids\\\"][:max_length] encoded_inputs[\\\"token_type_ids\\\"] = encoded_inputs[\\\"token_type_ids\\\"][:max_length] encoded_inputs[\\\"special_tokens_mask\\\"] = encoded_inputs[\\\"special_tokens_mask\\\"][:max_length]\",\"生成padding部分的mask列表\",\"attention_mask作用图解\",\" # 生成注意力掩码，真实token对应1，填充token对应0 attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\",\"所有序列都填充到max_length长度,不足长度用padding填充\",\"填充过程图\",\" # 记录输入长度 input_len = len(input_ids) # 计算需要填充的长度 --- 所有输入序列等长，都等于max_length padding_length = max_length - len(input_ids) # 右填充 input_ids = input_ids + ([pad_token] * padding_length) attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length) token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\",\"数据集中每一个样本最终都会解析得到一个InputFeatures\",\"InputFeatures组成图解\",\"features.append( InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=label, input_len=input_len))\",\"label 是当前文本对应的类别标签 input_len 是序列实际长度(含special tokens)\",\"数据集预处理完后，将InputFeatures List列表组装起来得到需要的DataSet\",\"dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_lens,all_labels)\"]},\"714\":{\"h\":\"模型架构\"},\"715\":{\"h\":\"DataLoader\",\"t\":[\" train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset) train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size,collate_fn=collate_fn)\",\"DataLoader 设置的回调方法cllote_fn负责对返回的一个batch，在返回前进行预处理:\",\"def collate_fn(batch): all_input_ids, all_attention_mask, all_token_type_ids, all_lens, all_labels = map(torch.stack, zip(*batch)) max_len = max(all_lens).item() # 计算当前批次中所有序列的实际最大长度 all_input_ids = all_input_ids[:, :max_len] # 按照本批次序列中最大长度进行截断: max_length --> max_len all_attention_mask = all_attention_mask[:, :max_len] all_token_type_ids = all_token_type_ids[:, :max_len] return all_input_ids, all_attention_mask, all_token_type_ids, all_labels\"]},\"716\":{\"h\":\"BertEmbeddings\",\"t\":[\"input embeddings = token embeddings + segmentation embeddings + position embeddings\",\"class BertEmbeddings(nn.Module): def __init__(self, config): super(BertEmbeddings, self).__init__() self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0) self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size) self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, input_ids, token_type_ids=None, position_ids=None): seq_length = input_ids.size(1) if position_ids is None: # 为当前批次中的每个序列样本生成一个位置序列: (1,2,3,4,5,...) , 构成一个位置序列矩阵 position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) position_ids = position_ids.unsqueeze(0).expand_as(input_ids) if token_type_ids is None: token_type_ids = torch.zeros_like(input_ids) words_embeddings = self.word_embeddings(input_ids) position_embeddings = self.position_embeddings(position_ids) # 位置编码为可学习的矩阵 token_type_embeddings = self.token_type_embeddings(token_type_ids) # 让模型自己学会区分不同的句子 embeddings = words_embeddings + position_embeddings + token_type_embeddings embeddings = self.LayerNorm(embeddings) embeddings = self.dropout(embeddings) return embeddings\",\"嵌入向量生成过程图\"]},\"717\":{\"h\":\"BertEncoder\"},\"718\":{\"h\":\"BertLayer\",\"t\":[\"BertLayer模型结构图\",\"class BertIntermediate(nn.Module): def __init__(self, config): super(BertIntermediate, self).__init__() self.dense = nn.Linear(config.hidden_size, config.intermediate_size) # (768,3072) # 激活函数 - GLEU if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)): self.intermediate_act_fn = ACT2FN[config.hidden_act] else: self.intermediate_act_fn = config.hidden_act def forward(self, hidden_states): hidden_states = self.dense(hidden_states) hidden_states = self.intermediate_act_fn(hidden_states) # 激活函数 - GLEU return hidden_states class BertOutput(nn.Module): def __init__(self, config): super(BertOutput, self).__init__() self.dense = nn.Linear(config.intermediate_size, config.hidden_size) # (3072,768) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, hidden_states, input_tensor): hidden_states = self.dense(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.LayerNorm(hidden_states + input_tensor) return hidden_states class BertLayer(nn.Module): def __init__(self, config): super(BertLayer, self).__init__() self.attention = BertAttention(config) self.intermediate = BertIntermediate(config) self.output = BertOutput(config) def forward(self, hidden_states, attention_mask=None): attention_output = self.attention(hidden_states, attention_mask) intermediate_output = self.intermediate(attention_output) layer_output = self.output(intermediate_output, attention_output) return layer_output\"]},\"719\":{\"h\":\"BertEncoder\",\"t\":[\"BertEncoder模型结构图\",\"class BertEncoder(nn.Module): def __init__(self, config): super(BertEncoder, self).__init__() self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)]) def forward(self, hidden_states, attention_mask=None, head_mask=None): for i, layer_module in enumerate(self.layer): hidden_states = layer_module(hidden_states, attention_mask, head_mask[i]) return hidden_states\"]},\"720\":{\"h\":\"BertPooler\",\"t\":[\"BertPooler模型结构图\",\"class BertPooler(nn.Module): def __init__(self, config): super(BertPooler, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) self.activation = nn.Tanh() def forward(self, hidden_states): # We \\\"pool\\\" the model by simply taking the hidden state corresponding # to the first token. first_token_tensor = hidden_states[:, 0] # CLS Token Context Embeddings pooled_output = self.dense(first_token_tensor) pooled_output = self.activation(pooled_output) return pooled_output\"]},\"721\":{\"h\":\"BertModel\",\"t\":[\"BertModel模型结构图\",\"class BertModel(BertPreTrainedModel): def __init__(self, config): super(BertModel, self).__init__(config) self.embeddings = BertEmbeddings(config) self.encoder = BertEncoder(config) self.pooler = BertPooler(config) self.init_weights() def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None): extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2) extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0 embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids) sequence_output = self.encoder(embedding_output, extended_attention_mask, # padding mask ) pooled_output = self.pooler(sequence_output) outputs = (sequence_output, pooled_output,) return outputs\"]},\"722\":{\"h\":\"BertForSequenceClassification\",\"t\":[\"BertForSequenceClassification模型结构图\",\"class BertForSequenceClassification(BertPreTrainedModel): def __init__(self, config): super(BertForSequenceClassification, self).__init__(config) self.num_labels = config.num_labels self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, self.config.num_labels) self.init_weights() def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None): outputs = self.bert(input_ids, attention_mask=attention_mask, # padding mask token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) # None ? pooled_output = outputs[1] # 对于分类任务来说，只需要去除CLS Token用于分类任务即可 pooled_output = self.dropout(pooled_output) logits = self.classifier(pooled_output) outputs = (logits,) + outputs[2:] # add hidden states and attention if they are here if labels is not None: if self.num_labels == 1: # We are doing regression loss_fct = MSELoss() loss = loss_fct(logits.view(-1), labels.view(-1)) else: loss_fct = CrossEntropyLoss() loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) outputs = (loss,) + outputs return outputs # (loss), logits, (hidden_states), (attentions)\"]},\"723\":{\"h\":\"BertAttention\"},\"724\":{\"h\":\"BertSelfAttention\",\"t\":[\"多头自注意力计算流程图\",\"class BertSelfAttention(nn.Module): def __init__(self, config): super(BertSelfAttention, self).__init__() self.output_attentions = config.output_attentions self.num_attention_heads = config.num_attention_heads self.attention_head_size = int(config.hidden_size / config.num_attention_heads) self.all_head_size = self.num_attention_heads * self.attention_head_size self.query = nn.Linear(config.hidden_size, self.all_head_size) self.key = nn.Linear(config.hidden_size, self.all_head_size) self.value = nn.Linear(config.hidden_size, self.all_head_size) self.dropout = nn.Dropout(config.attention_probs_dropout_prob) def transpose_for_scores(self, x): new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size) x = x.view(*new_x_shape) return x.permute(0, 2, 1, 3) def forward(self, hidden_states, attention_mask=None, head_mask=None): mixed_query_layer = self.query(hidden_states) mixed_key_layer = self.key(hidden_states) mixed_value_layer = self.value(hidden_states) # view 成多头格式: (batch,heads,seq_len,d_k) query_layer = self.transpose_for_scores(mixed_query_layer) key_layer = self.transpose_for_scores(mixed_key_layer) value_layer = self.transpose_for_scores(mixed_value_layer) # Take the dot product between \\\"query\\\" and \\\"key\\\" to get the raw attention scores. attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) # (batch,heads,d_k,seq_len) attention_scores = attention_scores / math.sqrt(self.attention_head_size) if attention_mask is not None: # Apply the attention mask is (precomputed for all layers in BertModel forward() function) attention_scores = attention_scores + attention_mask # Normalize the attention scores to probabilities. attention_probs = nn.Softmax(dim=-1)(attention_scores) # This is actually dropping out entire tokens to attend to, which might # seem a bit unusual, but is taken from the original Transformer paper. attention_probs = self.dropout(attention_probs) context_layer = torch.matmul(attention_probs, value_layer) context_layer = context_layer.permute(0, 2, 1, 3).contiguous() new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) context_layer = context_layer.view(*new_context_layer_shape) # 合并头结果 return context_layer\"]},\"725\":{\"h\":\"BertSelfOutput\",\"t\":[\"BertSelfOutput计算流程图\",\"class BertSelfOutput(nn.Module): def __init__(self, config): super(BertSelfOutput, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) # 残差链接 + 层归一化 def forward(self, hidden_states, input_tensor): hidden_states = self.dense(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.LayerNorm(hidden_states + input_tensor) return hidden_states\"]},\"726\":{\"h\":\"BertAttention\",\"t\":[\"BertAttention计算流程图\",\"class BertAttention(nn.Module): def __init__(self, config): super(BertAttention, self).__init__() self.self = BertSelfAttention(config) self.output = BertSelfOutput(config) def forward(self, input_tensor, attention_mask=None): self_outputs = self.self(input_tensor, attention_mask) # 多头自注意力机制 attention_output = self.output(self_outputs, input_tensor) return attention_output\"]},\"727\":{\"h\":\"预训练\",\"t\":[\"预训练与微调\"]},\"728\":{\"h\":\"BertPredictionHeadTransform\",\"t\":[\"BertPredictionHeadTransform结构图\",\"class BertPredictionHeadTransform(nn.Module): def __init__(self, config): super(BertPredictionHeadTransform, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)): self.transform_act_fn = ACT2FN[config.hidden_act] else: self.transform_act_fn = config.hidden_act self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) def forward(self, hidden_states): hidden_states = self.dense(hidden_states) hidden_states = self.transform_act_fn(hidden_states) hidden_states = self.LayerNorm(hidden_states) return hidden_states\"]},\"729\":{\"h\":\"BertLMPredictionHead\",\"t\":[\"BertLMPredictionHead结构图\",\"class BertLMPredictionHead(nn.Module): def __init__(self, config): super(BertLMPredictionHead, self).__init__() self.transform = BertPredictionHeadTransform(config) # The output weights are the same as the input embeddings, but there is # an output-only bias for each token. self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False) self.bias = nn.Parameter(torch.zeros(config.vocab_size)) def forward(self, hidden_states): hidden_states = self.transform(hidden_states) hidden_states = self.decoder(hidden_states) + self.bias return hidden_states\"]},\"730\":{\"h\":\"BertPreTrainingHeads\",\"t\":[\"BertPreTrainingHeads结构图\",\"class BertPreTrainingHeads(nn.Module): def __init__(self, config): super(BertPreTrainingHeads, self).__init__() self.predictions = BertLMPredictionHead(config) self.seq_relationship = nn.Linear(config.hidden_size, 2) def forward(self, sequence_output, pooled_output): prediction_scores = self.predictions(sequence_output) # seq_relationship_score = self.seq_relationship(pooled_output) # 两个句子是否为上下句关系 return prediction_scores, seq_relationship_score\"]},\"731\":{\"h\":\"BertForPreTraining\",\"t\":[\"BertForPreTraining结构图\",\"class BertForPreTraining(BertPreTrainedModel): def __init__(self, config): super(BertForPreTraining, self).__init__(config) self.bert = BertModel(config) self.cls = BertPreTrainingHeads(config) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, masked_lm_labels=None, next_sentence_label=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) sequence_output, pooled_output = outputs[:2] # 隐藏层输出,CLS Token Embeddings prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output) outputs = (prediction_scores, seq_relationship_score,) # 计算掩码语言损失 和 下一个句子预测损失 if masked_lm_labels is not None and next_sentence_label is not None: loss_fct = CrossEntropyLoss(ignore_index=-1) masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1)) next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1)) total_loss = masked_lm_loss + next_sentence_loss outputs = (total_loss,) + outputs return outputs # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)\"]},\"732\":{\"h\":\"其他下游任务\",\"t\":[\"Bert支持的下游任务图\"]},\"733\":{\"h\":\"问答任务\",\"t\":[\"在 BERT 的问答任务中，典型的输入是一个包含 问题（Question） 和 上下文（Context） 的文本对。例如：\",\"问题: “谁写了《哈姆雷特》？”上下文: “莎士比亚是英国文学史上最伟大的作家之一，他写了包括《哈姆雷特》、《麦克白》等著名悲剧。”\",\"输入格式（Tokenization 后的形式），在使用 BertTokenizer 编码后，输入会变成如下结构：\",\"[CLS] 问题 tokens [SEP] 上下文 tokens [SEP]\",\"BERT 的输出（Outputs），通过调用 self.bert(...)，你将得到一个包含多个元素的 tuple 输出：\",\"outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\",\"返回值形如：\",\"( sequence_output, # (batch_size, seq_length, hidden_size) pooled_output, # (batch_size, hidden_size) )\",\"主要输出项解释:\",\"✅ sequence_output: 最终每个 token 的表示\",\"形状：(batch_size, seq_length, hidden_size)\",\"是模型最后一层所有 token（包括问题和上下文）的隐藏状态。\",\"在问答任务中，我们主要使用它来预测答案的起始和结束位置。\",\"✅ pooled_output: 句子级别表示（不常用）\",\"形状：(batch_size, hidden_size)\",\"是 [CLS] token 经过一层全连接后的输出。\",\"在分类任务中更有用，在问答任务中一般不会使用这个输出。\",\"如何利用 BERT 输出做问答预测？\",\"在 BertForQuestionAnswering 中，使用了如下逻辑：\",\"logits = self.qa_outputs(sequence_output) # (batch_size, seq_length, 2) start_logits, end_logits = logits.split(1, dim=-1) # split into start and end start_logits = start_logits.squeeze(-1) # (batch_size, seq_length) end_logits = end_logits.squeeze(-1)\",\"qa_outputs 层的作用：\",\"是一个线性层：nn.Linear(config.hidden_size, 2)\",\"将每个 token 的 hidden_size 向量映射成两个分数：一个是该 token 作为答案开始的可能性，另一个是作为答案结束的可能性。\",\"输出解释：\",\"start_logits: 每个 token 是答案起点的得分（未归一化）。\",\"end_logits: 每个 token 是答案终点的得分。\",\"比如对于一个长度为 128 的序列，每个 token 都有一个对应的 start/end 分数：\",\"start_scores = torch.softmax(start_logits, dim=-1) # softmax 得到概率 end_scores = torch.softmax(end_logits, dim=-1) # 找出最可能是 start 和 end 的位置 start_index = torch.argmax(start_scores) end_index = torch.argmax(end_scores)\",\"如果 start_index <= end_index，那么可以组合这两个索引得到答案 span。\"]},\"734\":{\"h\":\"代码实现\",\"t\":[\"class BertForQuestionAnswering(BertPreTrainedModel): def __init__(self, config): super(BertForQuestionAnswering, self).__init__(config) self.num_labels = config.num_labels # 通常是 2，即 start 和 end self.bert = BertModel(config) self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, start_positions=None, end_positions=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids) sequence_output = outputs[0] # (batch,seq_len,hidden_size) ---> (batch,seq_len,2) logits = self.qa_outputs(sequence_output) start_logits, end_logits = logits.split(1, dim=-1) start_logits = start_logits.squeeze(-1) # (batch,seq_len) end_logits = end_logits.squeeze(-1) outputs = (start_logits, end_logits,) # 计算交叉熵损失 if start_positions is not None and end_positions is not None: # sometimes the start/end positions are outside our model inputs, we ignore these terms # ignored_index = seq_len ignored_index = start_logits.size(1) # clamp_ 是 PyTorch 中的一个方法，用于将张量中的值限制在指定的范围内。 # 它的语法是 tensor.clamp_(min, max) ，表示将张量中的值限制在 min 和 max 之间。 # 如果值小于 min ，则将其设置为 min ；如果值大于 max ，则将其设置为 max 。 start_positions.clamp_(0, ignored_index) end_positions.clamp_(0, ignored_index) # ignore_index: 用于指定在计算损失时忽略的标签索引。 loss_fct = CrossEntropyLoss(ignore_index=ignored_index) # 分别计算答案起始下标和结束下标预测得到的交叉熵损失 start_loss = loss_fct(start_logits, start_positions) end_loss = loss_fct(end_logits, end_positions) total_loss = (start_loss + end_loss) / 2 outputs = (total_loss,) + outputs return outputs # (loss), start_logits, end_logits\"]},\"735\":{\"h\":\"易混淆\",\"t\":[\"BERT 是一个 基于上下文编码（Contextual Encoder） 的模型，不是自回归生成器。它不会“生成”新的文本，而是对输入文本中每个 token 的角色进行分类（如判断哪个是答案的开始、结束）。所以最终的答案只能来自原始输入文本中的某一段子串。\",\"📚 详细解释\",\"✅ BERT 是一个 Encoder-only 模型\",\"BERT 只包含 Transformer 的 encoder 部分。\",\"它的作用是给定一个完整的句子（或两个句子），对每个 token 生成一个上下文相关的表示（contextualized representation）。\",\"它不具有生成能力，不能像 GPT 这样的 decoder-only 模型那样逐词生成新内容。\",\"🔍 QA 任务的本质：定位答案 span 而非生成答案\",\"在 SQuAD 这类抽取式问答任务中：\",\"答案必须是原文中的连续片段（span）。\",\"所以模型的任务是：\",\"给出问题和上下文；\",\"在上下文中找到最可能的答案起始位置和结束位置；\",\"最终答案就是上下文中这两个位置之间的字符串。\",\"BERT 做的就是这个定位任务，而不是重新生成一个新的答案。\",\"🧩 输入与输出的关系\",\"answer_tokens = input_ids[0][start_index : end_index + 1] answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\",\"这段代码的意思是：\",\"start_index 和 end_index 是模型预测出的答案的起始和结束位置。\",\"我们从原始输入的 input_ids 中取出对应的 token ID 子序列。\",\"使用 tokenizer 把这些 token ID 解码成自然语言文本。\",\"得到的就是答案。\",\"这其实就是在说：\",\"“根据你的理解，答案应该在这段文字中的第 X 到第 Y 个词之间，请把这部分原文告诉我。”\",\"🧪 举个例子\",\"假设原始上下文是：\",\"The capital of France is Paris.\",\"经过 Tokenizer 编码后可能是：\",\"[CLS] the capital of france is paris [SEP]\",\"如果模型预测 start_index=5，end_index=5，那么对应的就是单词 \\\"paris\\\"，这就是答案。\",\"⚠️ 注意事项\",\"不能超出上下文范围\",\"start/end positions 必须落在上下文部分（即 token_type_id == 1 的区域）。\",\"否则答案可能不合理（比如取到了问题部分的内容）。\",\"特殊 token 不计入答案\",\"[CLS], [SEP] 等会被 skip_special_tokens=True 自动跳过。\",\"无法处理不在原文中的答案\",\"如果正确答案没有出现在上下文中，BERT 无法“编造”出来。\",\"这是抽取式问答模型的局限性。\",\"💡 对比：生成式 vs 抽取式问答\",\"类型\",\"模型代表\",\"是否能生成新文本\",\"答案是否必须在原文中\",\"示例\",\"抽取式\",\"BERT\",\"❌\",\"✅\",\"答案是原文中的一段\",\"生成式\",\"T5 / BART / GPT\",\"✅\",\"❌\",\"答案可以是任意文本\",\"如果你希望模型能“自己写答案”，那就需要使用生成式模型。\",\"✅ 总结\",\"问题\",\"回答\",\"为什么答案来自 input_ids？\",\"因为 BERT 是编码器模型，只做抽取式问答，答案必须是原文中的一段文本。\",\"BERT 能不能自己生成答案？\",\"不能，BERT 不具备生成能力，只能对输入文本中的 token 做分类。\",\"如何获取答案？\",\"根据预测的 start/end index，从 input_ids 中提取 token，并用 tokenizer 解码成自然语言。\"]},\"736\":{\"h\":\"Token分类任务\",\"t\":[\"Token 分类任务是指对输入文本中的每个 token 进行分类，常见的应用场景包括：\",\"命名实体识别 (NER)\",\"词性标注 (POS)\",\"语义角色标注 (SRL)\",\"class BertForTokenClassification(BertPreTrainedModel): def __init__(self, config): super(BertForTokenClassification, self).__init__(config) self.num_labels = config.num_labels self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, config.num_labels) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) sequence_output = outputs[0] # (batch,seq_len,hidden_size) sequence_output = self.dropout(sequence_output) logits = self.classifier(sequence_output) # （batch,seq_len,num_labels） outputs = (logits,) if labels is not None: loss_fct = CrossEntropyLoss() # Only keep active parts of the loss if attention_mask is not None: active_loss = attention_mask.view(-1) == 1 active_logits = logits.view(-1, self.num_labels)[active_loss] active_labels = labels.view(-1)[active_loss] loss = loss_fct(active_logits, active_labels) else: loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) outputs = (loss,) + outputs return outputs # (loss), scores\"]},\"737\":{\"h\":\"多项选择任务\",\"t\":[\"多项选择任务是指给定一个问题和多个候选答案，模型需要从中选择最合适的答案。常见的应用场景包括：\",\"阅读理解任务\",\"问答系统中的候选答案选择\",\"对话系统中的候选回复选择\",\"在 多项选择题（Multiple Choice） 任务中，BERT 的输入组织形式与普通分类或问答任务略有不同。你需要为每个选项分别构造一个完整的 BERT 输入序列，并将它们组合成一个批次进行处理。\",\"✅ 假设你有一个问题 + 4 个选项：\",\"问题：谁写了《哈姆雷特》？ A. 雨果 B. 歌德 C. 莎士比亚 D. 托尔斯泰\",\"对于这样的多选问题，BERT 的输入方式是：\",\"对每一个选项，都单独构造一个 [CLS] + 问题 + [SEP] + 选项内容 + [SEP] 的输入序列。\",\"也就是说，模型会对每个选项分别编码 ，然后从中选出最合适的那个。\",\"class BertForMultipleChoice(BertPreTrainedModel): def __init__(self, config): super(BertForMultipleChoice, self).__init__(config) self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, 1) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None): # 获取选项个数 num_choices = input_ids.shape[1] # (batch_size, num_choices, seq_length) # 将选项展平，以便一起处理: (batch_size * num_choices, seq_length) input_ids = input_ids.view(-1, input_ids.size(-1)) attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) pooled_output = outputs[1] # (batch_size * num_choices, hidden_size) pooled_output = self.dropout(pooled_output) logits = self.classifier(pooled_output) # (batch_size * num_choices, 1) reshaped_logits = logits.view(-1, num_choices) # (batch_size , num_choices, 1) outputs = (reshaped_logits,) if labels is not None: loss_fct = CrossEntropyLoss() loss = loss_fct(reshaped_logits, labels) outputs = (loss,) + outputs return outputs # (loss), reshaped_logits, (hidden_states), (attentions)\",\"在前向传播中，会将这些输入展平，变成：\",\"input_ids.view(-1, seq_length) # (batch_size * num_choices, seq_length)\",\"这样就能让 BERT 对每个选项分别进行编码。\",\"BERT 输出后，再对每个选项做分类打分，最后重新 reshape 成 (batch_size, num_choices) 形式，用于计算交叉熵损失。\"]},\"738\":{\"h\":\"图解Transformer\",\"t\":[\"图解Transformer & 机器翻译实战\"]},\"739\":{\"h\":\"环境\",\"t\":[\"本文基于 The Annotated Transformer 所提供的代码展开进行讲解。\",\"环境搭建遵从如下步骤即可:\",\"git clone https://github.com/harvardnlp/annotated-transformer cd annotated-transformer conda create -n annotated-transformer python=3.9.22 conda activate annotated-transformer pip install -r requirements.txt\",\"MacOS 用户本地运行时，需要将 requirements.txt 文件中的 torch == 1.11.0+cu113 改为 torch==1.11.0，因为CUDA不支持MacOS。\"]},\"740\":{\"h\":\"背景\",\"t\":[\"RNN等模型的缺点是需要顺序计算，从而很难并行。因此出现了Extended Neural GPU、ByteNet和ConvS2S等网络模型。这些模型都是以CNN为基础，这比较容易并行。但是和RNN相比，它较难学习到长距离的依赖关系。\",\"本文的Transformer使用了Self-Attention机制，它在编码每一词的时候都能够注意(attend to)整个句子，从而可以解决长距离依赖的问题，同时计算Self-Attention可以用矩阵乘法一次计算所有的时刻，因此可以充分利用计算资源(CPU/GPU上的矩阵运算都是充分优化和高度并行的)。\"]},\"741\":{\"h\":\"模型架构\",\"t\":[\"Transformer 模型架构图\",\"Transformer 是一种基于自注意力机制(Self-Attention) 的神经网络架构,其由七大主要部分构成:\",\"Encoder-Decoder 结构\",\"编码器(Encoder)：将输入序列（如句子）转换为一系列高维向量表示。\",\"解码器(Decoder)：根据编码器的输出生成目标序列（如翻译后的句子）。\",\"多头自注意力机制（Multi-Head Self-Attention）\",\"自注意力机制是 Transformer 的核心，它允许模型在处理每个词时关注输入序列中的所有词。\",\"多头自注意力机制通过并行计算多个注意力头，捕捉不同子空间的信息，从而增强模型的表达能力。\",\"位置编码（Positional Encoding）\",\"由于 Transformer 不使用传统的循环或卷积结构，它通过位置编码将序列中词的位置信息注入到输入中。位置编码通常使用正弦和余弦函数生成。\",\"前馈神经网络（Feed-Forward Neural Network）\",\"在自注意力机制之后，每个位置的输出会通过一个独立的前馈神经网络进行进一步处理。\",\"残差连接与层归一化（Residual Connection & Layer Normalization）\",\"每个子层（如自注意力层和前馈层）都使用了残差连接和层归一化，以加速训练并提高模型的稳定性。\",\"掩码机制（Masking）\",\"在解码器中，使用掩码机制确保模型在生成目标序列时只能看到当前及之前的词，而不能看到未来的词。\",\"在输入序列长度不一致时，通过填充掩码（Padding Mask）屏蔽填充部分的信息。\",\"输出层\",\"解码器的最终输出通过一个线性层和 Softmax 函数生成目标序列的概率分布。\"]},\"742\":{\"h\":\"Encoder-Decoder 结构\",\"t\":[\"EncoderDecoder模型结构图\",\"class EncoderDecoder(nn.Module): def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): \\\"Take in and process masked src and target sequences.\\\" return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) def encode(self, src, src_mask): return self.encoder(self.src_embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\"]},\"743\":{\"h\":\"Generator\",\"t\":[\"Generator模型结构图\",\"class Generator(nn.Module): # 根据Decoder的隐状态输出一个词 # d_model是Decoder输出的大小，vocab是词典大小 def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) # 全连接再加上一个softmax def forward(self, x): return F.log_softmax(self.proj(x), dim=-1)\"]},\"744\":{\"h\":\"Encoder 结构\"},\"745\":{\"h\":\"SublayerConnection\",\"t\":[\"SublayerConnection模型结构图\",\"class SublayerConnection(nn.Module): \\\"\\\"\\\" LayerNorm + sublayer(Self-Attenion/Dense) + dropout + 残差连接 为了简单，把LayerNorm放到了前面，这和原始论文稍有不同，原始论文LayerNorm在最后。 \\\"\\\"\\\" def __init__(self, size, dropout): super(SublayerConnection, self).__init__() self.norm = LayerNorm(size) self.dropout = nn.Dropout(dropout) def forward(self, x, sublayer): \\\"sublayer是传入的参数，参考DecoderLayer，它可以当成函数调用，这个函数的有一个输入参数\\\" return x + self.dropout(sublayer(self.norm(x)))\"]},\"746\":{\"h\":\"EncoderLayer\",\"t\":[\"EncoderLayer模型结构图\",\"# 编码器层 = 自注意力子层 + 前馈层 class EncoderLayer(nn.Module): def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward # 自注意力子层 和 前馈层 各需要一个 self.sublayer = clones(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): \\\"Follow Figure 1 (left) for connections.\\\" x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) return self.sublayer[1](x, self.feed_forward)\"]},\"747\":{\"h\":\"Encoder\",\"t\":[\"Encoder模型结构图\",\"class Encoder(nn.Module): \\\"Core encoder is a stack of N layers\\\" def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): \\\"Pass the input (and mask) through each layer in turn.\\\" for layer in self.layers: x = layer(x, mask) return self.norm(x)\"]},\"748\":{\"h\":\"Decoder 结构\"},\"749\":{\"h\":\"DecoderLayer\",\"t\":[\"Decoder模型结构图\",\"# 解码器层 = 自注意力子层 + 源注意力子层 + 前馈层 class DecoderLayer(nn.Module): \\\"Decoder is made of self-attn, src-attn, and feed forward (defined below)\\\" def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward # 自注意力子层 + 源注意力子层 + 前馈层 各需要一个 self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): \\\"Follow Figure 1 (right) for connections.\\\" m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward)\"]},\"750\":{\"h\":\"Decoder\",\"t\":[\"Decoder模型结构图\",\"# 解码器 = N个解码器层 + 层归一化 class Decoder(nn.Module): \\\"Generic N layer decoder with masking.\\\" def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): # 输入,编码器隐藏层输出,源掩码,目标掩码 for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x)\"]},\"751\":{\"h\":\"多头自注意力\",\"t\":[\"多头自注意力计算流程图\",\"class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): \\\"Take in model size and number of heads.\\\" super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # We assume d_v always equals d_k self.d_k = d_model // h # 每个头64维 self.h = h # 8个头 self.linears = clones(nn.Linear(d_model, d_model), 4) # W_q,W_k,W_v,W_projection self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): \\\"Implements Figure 2\\\" if mask is not None: # Same mask applied to all h heads. mask = mask.unsqueeze(1) nbatches = query.size(0) # 1) Do all the linear projections in batch from d_model => h x d_k query, key, value = [ lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) # (batches,heads,seq_len,d_k) for lin, x in zip(self.linears, (query, key, value)) ] # 2) Apply attention on all the projected vectors in batch. x, self.attn = attention( query, key, value, mask=mask, dropout=self.dropout ) # 3) \\\"Concat\\\" using a view and apply a final linear. x = ( x.transpose(1, 2) .contiguous() .view(nbatches, -1, self.h * self.d_k) ) del query del key del value return self.linears[-1](x)\",\"def attention(query, key, value, mask=None, dropout=None): \\\"Compute 'Scaled Dot Product Attention'\\\" d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) # 广播: (1,1,1,10) ---> (1,8,10,10) p_attn = scores.softmax(dim=-1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn\"]},\"752\":{\"h\":\"🚀 从零构建深度学习框架（一）：计算图与自动微分的起点\",\"t\":[\"1.TinyPytorch 第一阶段: 计算图与自动微分\",\"仓库链接: https://github.com/BinaryOracle/TinyPytorch 本节代码:\"]},\"753\":{\"h\":\"引言：揭开深度学习框架的神秘面纱\",\"t\":[\"深度学习框架中蕴藏着惊人的技术和有趣的机制，而本系列旨在揭开这些技术和机制的神秘面纱，帮助读者正确理解技术，体会它们的有趣之处。为此，本系列将带领读者从零开始创建一个深度学习框架——TinyPytorch。TinyPytorch尽量用最少的代码实现了现代深度学习框架的功能。第一阶段共包含10个步骤，让我们逐步构建起TinyPytorch的基础功能。\"]},\"754\":{\"h\":\"步骤1：作为\\\"箱子\\\"的变量\"},\"755\":{\"h\":\"变量的基本概念\",\"t\":[\"变量是TinyPytorch最重要的组成部分，可将其比作存放数据的\\\"箱子\\\"。在编程中，变量的作用是存储数据，而TinyPytorch的变量实现为Variable类，其核心功能是保存和管理数据。\"]},\"756\":{\"h\":\"代码实现\",\"t\":[\"class Variable: def __init__(self, data): self.data = data\"]},\"757\":{\"h\":\"使用示例\",\"t\":[\"import numpy as np data = np.array(1.0) x = Variable(data) print(x.data) # 输出 1.0 x.data = np.array(2.0) print(x.data) # 输出: 2.0\"]},\"758\":{\"h\":\"关键要点\",\"t\":[\"Variable类封装了NumPy的多维数组（ndarray）\",\"数据存储在data属性中\",\"支持数据的修改和读取\"]},\"759\":{\"h\":\"步骤2：创建变量的函数\"},\"760\":{\"h\":\"函数与计算图\",\"t\":[\"函数定义了变量之间的对应关系，通过计算图可以直观地表示变量与函数的关系。计算图用圆框表示变量，用方框表示函数，节点和箭头展示计算流程。\"]},\"761\":{\"h\":\"函数类的设计\",\"t\":[\"设计Function类作为基类，具体函数继承该类并实现forward方法。__call__方法处理输入输出的变量封装。\"]},\"762\":{\"h\":\"代码实现\",\"t\":[\"class Function: # 设定传入的input和返回的output均为Variable类型 def __call__(self, input): x = input.data y = self.forward(x) output = Variable(y) return output def forward(self, x): raise NotImplementedError() class Square(Function): def forward(self, x): return x ** 2\"]},\"763\":{\"h\":\"辅助函数\",\"t\":[\"为了方便使用，将函数类封装为Python函数：\",\"def square(x): return Square()(x)\"]},\"764\":{\"h\":\"步骤3：函数的连续调用\"},\"765\":{\"h\":\"复合函数的计算\",\"t\":[\"多个函数可以连续调用，形成复合函数。例如，计算，可以通过连续使用平方函数和指数函数实现。\"]},\"766\":{\"h\":\"代码示例\",\"t\":[\"class Exp(Function): def forward(self, x): return np.exp(x) def exp(x): return Exp()(x) A = Square() B = Exp() C = Square() x = Variable(np.array(0.5)) a = A(x) b = B(a) y = C(b) print(y.data) # 输出: 1.648721270700128\"]},\"767\":{\"h\":\"计算图的意义\",\"t\":[\"复合函数的计算图展示了函数的组合过程，而反向传播算法可以高效地求出每个变量的导数，这为自动微分奠定了基础。\"]},\"768\":{\"h\":\"步骤4：数值微分\"},\"769\":{\"h\":\"导数的定义\",\"t\":[\"导数是变化率的表示，函数在处的导数定义为：\"]},\"770\":{\"h\":\"数值微分的实现\",\"t\":[\"使用中心差分近似计算数值微分，公式为：\",\"本部分感兴趣的可以回顾高数中微分定义部分内容，中心差分近似可以问问GPT。\"]},\"771\":{\"h\":\"代码实现\",\"t\":[\"def numerical_diff(f, x, eps=1e-4): x0 = Variable(x.data - eps) x1 = Variable(x.data + eps) y0 = f(x0) y1 = f(x1) return (y1.data - y0.data) / (2 * eps)\"]},\"772\":{\"h\":\"数值微分的问题\",\"t\":[\"结果包含误差（精度丢失）\",\"计算成本高（尤其对于多变量函数）\",\"可用于梯度检验，验证反向传播的正确性\"]},\"773\":{\"h\":\"步骤5：反向传播的理论知识\"},\"774\":{\"h\":\"链式法则\",\"t\":[\"反向传播的理论基础是链式法则，对于复合函数，若由、、组成，则：\"]},\"775\":{\"h\":\"反向传播的方向\",\"t\":[\"反向传播按从输出到输入的顺序计算导数，与正向传播方向相反。这种方式在输出为标量的情况下计算效率更高，适合机器学习中损失函数的优化。\",\"反向传播用于计算输入X对输出Y大小变化的影响，如果没有Function作用于X上，即Y=X，那么影响因子R恒为1；\",\"如果存在Function作用在X上，那么Function可能会放大或者缩小X对输出Y大小变化的影响，也就是改变X对于Y的影响因子，此时R= 1 * F放缩因子；\",\"如果存在多个Function先后作用在X上，即Y=Fn(...F2(F1(X)))，那么此时影响因子R= 1 * F1放缩因子 * F2放缩因子 * ... * Fn放缩因子；\",\"对于Y=F2(F1(x)) + F3(x) , 此时影响因子R = 1 * F1放缩因子 * F2放缩因子 + 1 * F3放缩因子；\",\"实际此处的函数放缩因子也称为函数的导数，多元场景下也称为偏导数。\"]},\"776\":{\"h\":\"计算图的反向传播\",\"t\":[\"反向传播过程中，导数从输出端向输入端传播，每个函数节点需要计算其导数，并将结果传递给前一层变量。\"]},\"777\":{\"h\":\"步骤6：手动进行反向传播\"},\"778\":{\"h\":\"扩展Variable类\",\"t\":[\"为Variable类添加grad属性，用于存储导数：\",\"class Variable: def __init__(self, data): self.data = data self.grad = None\"]},\"779\":{\"h\":\"扩展Function类\",\"t\":[\"为Function类添加反向传播方法backward，并在__call__方法中保存输入输出变量：\",\"class Function: def __call__(self, input): x = input.data y = self.forward(x) output = Variable(y) self.input = input self.output = output return output def backward(self, gy): raise NotImplementedError()\",\"反向传播的核心是依据链式法则，沿计算图反向推导各变量的导数。而链式法则的计算需要知晓每个函数在正向传播时的输入值和输出值。\",\"例如，对于函数 ，其导数 依赖于输入x的具体值（如平方函数 的导数 ，需要知道正向传播时的 值）。记录input后，反向传播时可直接获取这些值，避免重复计算或存储额外数据。\",\"再例如，对于函数 ，其导数依赖于输出值。 函数的表达式为 ，其导数 ，即需要知道正向传播时的输出值 才能计算导数。\"]},\"780\":{\"h\":\"具体函数的反向传播\",\"t\":[\"以平方函数为例：\",\"class Square(Function): def backward(self, gy): x = self.input.data gx = 2 * x * gy return gx\"]},\"781\":{\"h\":\"反向传播的执行\",\"t\":[\"按反向顺序调用各函数的backward方法，手动传递导数：\",\"y.grad = np.array(1.0) b.grad = C.backward(y.grad) a.grad = B.backward(b.grad) x.grad = A.backward(a.grad)\"]},\"782\":{\"h\":\"步骤7：反向传播的自动化\"},\"783\":{\"h\":\"建立变量与函数的连接\",\"t\":[\"在Variable类中添加creator属性，记录创建该变量的函数；在Function类的__call__方法中，设置变量的creator：\",\"class Variable: def set_creator(self, func): self.creator = func class Function: def __call__(self, input): # 其他代码... output.set_creator(self)\"]},\"784\":{\"h\":\"自动反向传播的实现\",\"t\":[\"在Variable类中添加backward方法，通过递归或循环遍历计算图，自动执行反向传播：\",\"class Variable: def backward(self): f = self.creator if f is not None: x = f.input x.grad = f.backward(self.grad) x.backward()\",\"你品，你细品 ~ 🤔 (目前实现的版本无法实现上图中计算图多分支的结构，只能实现一条竖线形状的计算图结构的反向传播)\"]},\"785\":{\"h\":\"步骤8：从递归到循环\"},\"786\":{\"h\":\"递归实现的问题\",\"t\":[\"递归实现的反向传播在计算图较深时可能导致栈溢出，且效率较低。\"]},\"787\":{\"h\":\"循环实现反向传播\",\"t\":[\"使用列表保存待处理的函数，通过循环替代递归：\",\"class Variable: def backward(self): funcs = [self.creator] while funcs: f = funcs.pop() # 弹出列表尾部元素 x, y = f.input, f.output x.grad = f.backward(y.grad) if x.creator is not None: funcs.append(x.creator) # 追加到列表尾部\",\"由于目前只支持竖线形状的计算图，因此func列表同一时刻最多只存在一个func\"]},\"788\":{\"h\":\"循环实现的优势\",\"t\":[\"避免栈溢出\",\"提高执行效率\",\"更容易处理复杂计算图\"]},\"789\":{\"h\":\"步骤9：让函数更易用\"},\"790\":{\"h\":\"函数的Python化\",\"t\":[\"将函数类封装为Python函数，便于直接调用：\",\"def square(x): return Square()(x) def exp(x): return Exp()(x)\"]},\"791\":{\"h\":\"自动设置梯度\",\"t\":[\"在Variable的backward方法中自动初始化梯度：\",\"class Variable: def backward(self): if self.grad is None: self.grad = np.ones_like(self.data) # 其他代码...\"]},\"792\":{\"h\":\"数据类型检查\",\"t\":[\"确保Variable只接受ndarray实例，提高代码健壮性：\",\"class Variable: def __init__(self, data): if data is not None: if not isinstance(data, np.ndarray): raise TypeError(f'{type(data)} is not supported') self.data = data # 其他代码...\"]},\"793\":{\"h\":\"步骤10：测试\"},\"794\":{\"h\":\"单元测试\",\"t\":[\"使用Python的unittest模块编写测试用例，验证函数的正向传播和反向传播：\",\"import unittest class SquareTest(unittest.TestCase): def test_forward(self): x = Variable(np.array(2.0)) y = square(x) self.assertEqual(y.data, np.array(4.0)) def test_backward(self): x = Variable(np.array(3.0)) y = square(x) y.backward() self.assertEqual(x.grad, np.array(6.0))\"]},\"795\":{\"h\":\"梯度检验\",\"t\":[\"将数值微分的结果与反向传播的结果进行比较，验证反向传播的正确性：\",\"def numerical_diff(f, x, eps=1e-4): # 实现如前所述 class SquareTest(unittest.TestCase): def test_gradient_check(self): x = Variable(np.random.rand(1)) y = square(x) y.backward() num_grad = numerical_diff(square, x) self.assertTrue(np.allclose(x.grad, num_grad))\"]},\"796\":{\"h\":\"测试的重要性\",\"t\":[\"确保代码功能正确性\",\"发现潜在bug\",\"支持代码重构和扩展\"]},\"797\":{\"h\":\"第一阶段总结\",\"t\":[\"通过第一阶段的10个步骤，我们从零开始构建了TinyPytorch框架的基础功能：\",\"实现了变量和函数的基本结构\",\"完成了自动微分的核心算法——反向传播\",\"实现了数值微分作为梯度检验工具\",\"优化了反向传播的实现，从递归改为循环\",\"提高了框架的易用性和健壮性\",\"建立了测试机制，确保代码质量\",\"此时的TinyPytorch已经具备了自动微分的能力，可以处理简单的计算图，并为后续的功能扩展奠定了坚实基础。接下来的阶段将进一步扩展TinyPytorch，使其支持更复杂的计算和神经网络的构建。\"]},\"798\":{\"h\":\"🧮 从零构建深度学习框架（二）：自动反向传播与计算图进阶\",\"t\":[\"1.TinyPytorch 第二阶段: 自动反向传播与框架基础能力提升\",\"仓库链接: https://github.com/BinaryOracle/TinyPytorch 本节代码:\"]},\"799\":{\"h\":\"引言：从自动微分迈向通用框架\",\"t\":[\"深度学习框架之所以强大，不仅因为其前向计算功能，更因为其背后复杂而精妙的自动微分系统。本系列文章试图揭开这些机制背后的本质，帮助读者从零搭建属于自己的深度学习引擎。\",\"第一阶段中，我们构建了变量（Variable）与函数（Function）类，实现了基础的计算图结构与反向传播流程，并通过链式法则自动推导了导数。\",\"第二阶段将从第11步延续，全面扩展 TinyPytorch 的核心能力。在第一阶段，我们完成了计算图与手动反向传播的雏形。而在本阶段，我们将继续揭开深度学习框架的核心机制：实现真正意义上的自动反向传播、多输入/输出处理、计算图遍历优化、梯度累加、配置控制等。通过这 14 个步骤，TinyPytorch 将蜕变为一个更通用、更高效、更接近真实框架的自动微分系统。\"]},\"800\":{\"h\":\"步骤11: 多输入与多输出\",\"t\":[\"现实中的神经网络操作往往不仅仅接受一个输入，也可能产生多个输出，例如加法、乘法、切片、拼接等操作。因此我们扩展 Function 类以支持 可变参数输入与输出列表。\",\"class Function: def __call__(self, *inputs): xs = [x.data for x in inputs] ys = self.forward(*xs) if not isinstance(ys, tuple): ys = (ys,) outputs = [Variable(as_array(y)) for y in ys] for output in outputs: output.set_creator(self) self.inputs = inputs self.outputs = outputs return outputs if len(outputs) > 1 else outputs[0] def forward(self, xs): raise NotImplementedError() def backward(self, gys): raise NotImplementedError()\",\"这一扩展使我们的函数定义更接近 NumPy 风格，支持多个输入与输出，提高了灵活性。\",\"为了更好地支持多输入函数，我们学习和利用了 Python 中的几个语法技巧：\",\"*args：接收任意个数的位置参数，用于函数的输入接口；\",\"*xs 解包语法：在调用如 forward(*xs) 时展开变量列表；\",\"tuple 判断：让返回值始终封装为元组，统一处理逻辑。\"]},\"801\":{\"h\":\"步骤12: backward 的多输入实现\",\"t\":[\"在 Variable.backward() 中支持多输出节点：\",\"class Variable: ... def backward(self): if self.grad is None: self.grad = np.ones_like(self.data) funcs = [self.creator] while funcs: f = funcs.pop() gys = [output.grad for output in f.outputs] gxs = f.backward(*gys) if not isinstance(gxs,tuple): gxs = (gxs,) for x, gx in zip(f.inputs, gxs): x.grad = gx if x.creator is not None: funcs.append(x.creator)\",\"这里 gxs 和 f.inputs 的每个元素都是一一对应的。准确来说, 如果有第i个元素, 那么f.input[i]的导数值对应于gxs[i]。于是代码中使用zip 函数和 for 循环来设置每一对的导数。以上就是Variable类的新 backward 方法。\"]},\"802\":{\"h\":\"步骤13: 重置导数\",\"t\":[\"当我们使用同一个变量分别进行多次计算时，我们希望每次计算都能得到正确的导数。为了实现这一点，我们需要在每次计算之前将导数重置为0。\",\"下面为 Variable 类提供一个新的方法，实现变量导数的重置。\",\"class Variable: ... def cleargrad(self): self.grad = None\"]},\"803\":{\"h\":\"步骤14: 共享变量与梯度累加\",\"t\":[\"当某个变量被多次用作输入时（例如 z = x + x），反向传播过程中它的梯度应累加。\",\"这是因为同一个变量对输出的影响路径有多条。如果我们不进行累加，而是直接覆盖梯度值，就会导致只有最后一条路径上的梯度被保留，其他路径上的梯度信息将丢失。\",\"例如：\",\"x = Variable(np.array(3.0)) y = add(x, x) # x 被用作两次输入\",\"在反向传播过程中，x 的梯度来自两个路径：一条是第一个 x 到 y，另一条是第二个 x 到 y。如果我们不对这两个梯度求和，只保留一个，那么 x 的真实导数就会被低估一半，最终影响训练结果。\",\"因此，在实现中应当如下处理：\",\"class Variable: ... def backward(self): if self.grad is None: self.grad = np.ones_like(self.data) funcs = [self.creator] while funcs: f = funcs.pop() gys = [output.grad for output in f.outputs] gxs = f.backward(*gys) if not isinstance(gxs,tuple): gxs = (gxs,) for x, gx in zip(f.inputs, gxs): if x.grad is None: x.grad = gx else: x.grad = x.grad + gx # 正确的累加方式 if x.creator is not None: funcs.append(x.creator)\",\"这样才能确保所有路径的贡献都被纳入最终的梯度值。\"]},\"804\":{\"h\":\"步骤15: 梯度重复累加的问题\",\"t\":[\"在步骤14中，我们通过变量梯度非空则进行累加的改动，解决了共享变量梯度重置的问题，但是这一改动也引发了另一个问题：梯度重复累加。 观察下图，由于目前Variable.backward()的实现逻辑总是将函数追加到待处理列表的末尾，同时又优先处理列表末尾的函数，为\\\"先进先出\\\"的实现逻辑，因此对于存在多分支的复杂计算图而言，它也总是会沿着某个分支进行DFS直到\\\"叶节点\\\"，这会导致如下图所示的共享变量a的梯度被重复累加，导致x变量梯度计算错误。\",\"例子如下:\",\"x = Variable(np.array(2.0)) a = square(x) b = square(a) c = square(a) y = add(b, c) y.backward()\",\"上面的问题本质是因为函数调用顺序错误导致的，对于共享变量，我们要先计算出其梯度后，才能继续计算其前向的梯度，其实也就是按照拓扑排序的方式去遍历计算图。\"]},\"805\":{\"h\":\"步骤16: \\\"辈分\\\"机制\",\"t\":[\"为了解决上述的问题，我们可以采用拓扑排序，但是这里为了方便理解，我们采用更加暴力的“辈分排序”机制确保函数调用顺序的正确执行。\",\"我们可以获取到哪个函数生成了哪个变量，这就构成了函数与变量的\\\"辈分\\\"关系；我们可以通过变量的辈分来设置其创建者函数的辈分，如下图所示:\",\"我们在Variable类和Function类中增加实例变量generation，用其来表示函数(或变量)属于哪一代。\",\"class Variable: def __init__ (self , data): if data is not None: if not isinstance(data , np.ndarray): raise TypeError( '{} is not supported' .format(type(data))) self.data = data self.grad = None self.creator = None self.generation = 0 def set_creator(self, func): self.creator = func self.generation = func.generation + 1 ...\",\"Variable 类将 generation 初始化为 0。之后, 当 set_creator 方法被调用时, 它将 generation 的值设置为父函数的 generation 值加1。\",\"Function 类的 generation 被设置为多个输入变量中最大的generation的值。\",\"class Function: def __call__(self, *inputs): xs = [x.data for x in inputs] ys = self.forward(*xs) if not isinstance(ys, tuple): ys = (ys,) outputs = [Variable(as_array(y)) for y in ys] self.generation = max([x.generation for x in inputs]) for output in outputs: output.set_creator(self) self.inputs = inputs self.outputs = outputs return outputs if len(outputs) > 1 else outputs[0]\",\"通过以上修改, 在进行普通计算(即正向传播)时, 变量和函数中会设置好 generation 的值，我们便可以通过\\\"辈分\\\"按序取出元素。\",\"如上图所示，此时我们可以通过辈分来确保函数B和C先于函数A取出；我们只需要如下修改Variable变量的backward方法即可完成按照辈分获取函数的逻辑:\",\"class Variable: ... def backward(self): if self.grad is None: self.grad = np.ones_like(self.data) funcs = [] seen_set = set() # 防止同一个函数被多次添加到func列表中，从而避免一个函数的backward方法被错误地多次调用 def add_func(f): if f not in seen_set: funcs.append(f) seen_set.add(f) funcs.sort(key=lambda x : x.generation) add_func(self.creator) while funcs: f = funcs.pop() # 每次取出辈分最大的函数 gys = [output.grad for output in f.outputs] gxs = f.backward(*gys) if not isinstance(gxs,tuple): gxs = (gxs,) for x, gx in zip(f.inputs, gxs): if x.grad is None: x.grad = gx else: x.grad = x.grad + gx # 正确的累加方式 if x.creator is not None: add_func(x.creator)\"]},\"806\":{\"h\":\"步骤17: 循环引用与内存释放\",\"t\":[\"Python的内存管理主要依靠两种机制：引用计数和分代垃圾回收（GC）。在深度学习框架中，合理的内存管理至关重要，尤其当处理大规模数据时，不当的内存管理可能导致内存泄漏或程序崩溃。\",\"引用计数：Python通过跟踪对象的引用次数来管理内存。当对象的引用计数为0时，会被立即回收。以下情况会增加引用计数：\",\"使用赋值运算符（如a = obj()）\",\"向函数传递参数（如f(a)）\",\"向容器对象（列表、元组等）添加元素。\",\"循环引用指对象之间相互引用，导致引用计数无法归零，从而无法被自动回收。例如：\",\"a = Obj() b = Obj() c = Obj() a.b = b b.c = c c.a = a a = b = c = None # 此时a、b、c的引用计数仍为1，无法回收\",\"这种情况下，尽管用户不再访问这些对象，但因循环引用，引用计数无法降至0，需依赖垃圾回收机制处理。\",\"虽然Python的垃圾回收（GC）机制可以处理循环引用对象，但在对内存敏感的场景下依然存在问题，主要原因如下：\",\"回收时机非即时性: GC是一种后台机制，通常在内存不足或满足特定条件时才会触发，而非实时回收循环引用对象。例如，当TinyPytorch处理大量神经网络计算时，若存在循环引用，GC可能无法及时释放内存，导致内存占用持续升高，甚至引发内存不足错误。\",\"性能开销较高: GC需要扫描整个对象图来检测循环引用，这一过程对大规模计算框架（如TinyPytorch）而言可能产生显著的性能损耗。尤其在神经网络训练中，频繁的GC操作会影响计算效率，而弱引用可通过避免循环引用直接解决问题，减少GC触发频率。\",\"TinyPytorch中的循环引用:\",\"当前TinyPytorch框架中，Function和Variable实例存在循环引用：\",\"Function实例引用输入和输出的Variable实例（self.inputs和self.outputs）。\",\"Variable实例通过creator属性引用创建它的Function实例。\",\"解决方案：弱引用\",\"弱引用的优势:\",\"避免强引用导致的内存滞留: Function和Variable之间原本存在强引用循环（Function引用Variable，Variable通过creator引用Function）。若使用强引用，即使计算图不再被用户访问，循环引用仍会导致对象无法释放。而弱引用不会增加对象的引用计数，当用户不再引用Variable时，对象可被立即回收，无需等待GC。\",\"符合框架设计需求: TinyPytorch的计算图需要动态构建和销毁，弱引用能确保计算图在使用完毕后自动释放内存。例如，当用户执行完一次前向传播和反向传播后，计算图中的中间变量（如Function和临时Variable）应被及时回收，以释放内存供后续计算使用。\",\"GC与弱引用的互补关系\",\"GC作为兜底机制：GC可处理开发者未显式解决的循环引用，但无法替代弱引用在框架设计中的针对性优化。\",\"弱引用作为主动优化：在TinyPytorch中，通过弱引用主动打破Function与Variable之间的循环引用，能更精准地控制内存释放时机，避免因GC延迟导致的内存问题。\",\"使用Python的weakref模块创建弱引用，避免增加对象的引用计数：\",\"修改Function类：\",\"import weakref class Function: def __call__(self, *inputs): # 原有代码... self.outputs = [weakref.ref(output) for output in outputs] # 将强引用改为弱引用\",\"弱引用不会增加对象的引用计数，当Variable实例不再被其他对象引用时，会被正常回收。\",\"修改Variable类的backward方法：\",\"class Variable: def backward(self): # 原有代码... gys = [output().grad for output in f.outputs] # 通过output()获取实际对象\",\"使用output()从弱引用中获取Variable实例，避免直接引用导致循环。\",\"总结:\",\"循环引用会导致Python对象无法被正常回收，需通过弱引用解决。\",\"在TinyPytorch中，将Function对Variable的引用改为弱引用，避免内存泄漏。\",\"优化后的内存管理确保框架在处理大规模计算时的稳定性和效率。\"]},\"807\":{\"h\":\"步骤18: 优化内存消耗\",\"t\":[\"优化反向传播的内存消耗: TinyPytorch当前的反向传播会保留所有变量的导数，但在实际应用中，仅终端变量的导数需要被保留，中间变量的导数往往无用。为此，我们引入释放中间变量导数的机制。\",\"修改Variable.backward方法: 添加retain_grad参数，若为False（默认），则反向传播后清除中间变量的导数。\",\" class Variable: def backward(self, retain_grad=False): if self.grad is None: self.grad = np.ones_like(self.data) funcs = [] seen_set = set() def add_func(f): if f not in seen_set: funcs.append(f) seen_set.add(f) funcs.sort(key=lambda x: x.generation) add_func(self.creator) while funcs: f = funcs.pop() gys = [output().grad for output in f.outputs] gxs = f.backward(*gys) if not isinstance(gxs, tuple): gxs = (gxs,) for x, gx in zip(f.inputs, gxs): if x.grad is None: x.grad = gx else: x.grad = x.grad + gx if x.creator is not None: add_func(x.creator) if not retain_grad: for y in f.outputs: y().grad = None # 清除中间变量的导数\",\"验证案例：\",\" x0 = Variable(np.array(1.0)) x1 = Variable(np.array(1.0)) t = add(x0, x1) y = add(x0, t) y.backward() # retain_grad默认False print(y.grad, t.grad) # 输出：None None（中间变量导数被清除） print(x0.grad, x1.grad) # 输出：2.0 1.0（终端变量导数保留）\",\"中间变量y和t的导数被立即释放，减少内存占用。\",\"禁用反向传播的模式优化: 在推理阶段（如模型预测），无需计算导数，可通过禁用反向传播模式进一步节省内存。\",\"创建配置类Config：\",\"class Config: enable_backprop = True # 控制反向传播是否启用\",\"修改Function.__call__方法： 仅当Config.enable_backprop为True时，保留反向传播所需的计算图连接：\",\"class Function: def __call__(self, *inputs): xs = [x.data for x in inputs] ys = self.forward(*xs) if not isinstance(ys, tuple): ys = (ys,) outputs = [Variable(as_array(y)) for y in ys] if Config.enable_backprop: self.generation = max([x.generation for x in inputs]) for output in outputs: output.set_creator(self) self.inputs = inputs self.outputs = [weakref.ref(output) for output in outputs] return outputs if len(outputs) > 1 else outputs[0]\",\"模式切换示例：\",\"# 启用反向传播（默认模式） x = Variable(np.ones((100, 100, 100))) y = square(square(square(x))) # 保留中间结果，内存占用高 # 禁用反向传播（推理模式） Config.enable_backprop = False x = Variable(np.ones((100, 100, 100))) y = square(square(square(x))) # 不保留中间结果，内存占用低\",\"禁用模式下，计算完成后中间变量立即释放，内存使用量大幅降低。\",\"使用with语句便捷切换模式: 为避免手动修改Config属性，可通过contextlib模块实现with语句上下文管理。\",\"实现using_config函数：\",\"import contextlib @contextlib.contextmanager def using_config(name, value): old_value = getattr(Config, name) setattr(Config, name, value) try: yield finally: setattr(Config, name, old_value)\",\"封装no_grad函数：\",\"def no_grad(): return using_config('enable_backprop', False) # 使用示例 with no_grad(): x = Variable(np.array(2.0)) y = square(x) # 禁用反向传播，不构建计算图\",\"退出with块后，模式自动恢复，避免因忘记重置配置导致的错误。\",\"优化效果总结:\",\"内存释放机制：通过retain_grad参数及时清除中间变量导数，避免内存长期占用。\",\"推理模式优化：禁用反向传播后，计算过程不保留计算图连接，适合无需梯度的场景（如模型预测）。\",\"工程实践：with no_grad()语法简洁，便于在训练和推理阶段灵活切换，提升代码可读性和鲁棒性。\"]},\"808\":{\"h\":\"步骤19: Variable 功能增强\",\"t\":[\"为了便于区分和调试，在Variable类中添加name属性，支持为变量设置自定义名称：\",\"class Variable: def __init__(self, data, name=None): if data is not None: if not isinstance(data, np.ndarray): raise TypeError(f'{type(data)} is not supported') self.data = data self.name = name # 新增名称属性 self.creator = None self.grad = None self.generation = 0\",\"使用示例：\",\"x = Variable(np.array(1.0), name='input_x') y = Variable(np.array(2.0), name='input_y')\",\"变量名称可在计算图可视化等场景中显示，提升调试效率。\",\"使Variable实例具备ndarray的行为特征，隐藏数据封装细节：\",\"添加shape、ndim、size、dtype属性:\",\"class Variable: @property def shape(self): return self.data.shape # 获取数据形状 @property def ndim(self): return self.data.ndim # 获取维度数 @property def size(self): return self.data.size # 获取元素总数 @property def dtype(self): return self.data.dtype # 获取数据类型\",\"示例验证：\",\"x = Variable(np.array([[1, 2, 3], [4, 5, 6]])) print(x.shape) # 输出：(2, 3) print(x.ndim) # 输出：2 print(x.size) # 输出：6 print(x.dtype) # 输出：int64\",\"变量实例可直接访问ndarray的核心属性，使用户无需关心data属性。\",\"实现__len__方法：\",\"class Variable: def __len__(self): return len(self.data) # 返回第1维度的元素数\",\"示例：\",\"x = Variable(np.array([1, 2, 3, 4])) print(len(x)) # 输出：4 y = Variable(np.array([[1, 2], [3, 4]])) print(len(y)) # 输出：2\",\"自定义打印格式：\",\" class Variable: def __repr__(self): if self.data is None: return 'variable(None)' data_str = str(self.data).replace('\\\\n', '\\\\n' + ' ' * 9) return f'variable({data_str})'\",\"输出效果：\",\"x = Variable(np.array([1, 2, 3])) print(x) # 输出：variable([1 2 3]) y = Variable(np.array([[1, 2], [3, 4]])) print(y) # 输出： # variable([[1 2] # [3 4]])\",\"打印时自动对齐多行数据，并标注“variable”前缀，便于识别变量类型。\",\"可继续添加ndarray的其他属性（如T转置、flat迭代器等），或实现__getitem__、__setitem__方法以支持数组索引，进一步强化变量的“透明箱子”特性。\"]},\"809\":{\"h\":\"步骤20–22: 运算符重载\",\"t\":[\"乘法运算的实现与运算符重载：\",\"在深度学习框架中，乘法运算是最基础的操作之一。为了让Variable实例支持自然的乘法表达式（如a * b），我们需要实现Mul类来处理正向传播和反向传播，并将其绑定到*运算符上。\",\"正向传播：计算两个输入变量的乘积，即y = x0 * x1。\",\"反向传播：根据导数公式，若y = x0 * x1，则，。因此，反向传播时需要将上游传来的梯度gy分别乘以x1和x0，传递给下游变量。\",\"class Mul(Function): def forward(self, x0, x1): y = x0 * x1 return y def backward(self, gy): x0, x1 = self.inputs[0].data, self.inputs[1].data return gy * x1, gy * x0 # 将梯度分别乘以x1和x0\",\"为了方便使用，我们将Mul类封装为Python函数mul，并通过Variable.__mul__和Variable.__rmul__绑定乘法运算符，使其支持左右操作数为Variable的情况。由于乘法满足交换律，__mul__和__rmul__可共用同一实现。\",\"def mul(x0, x1): x1 = as_array(x1) return Mul()(x0, x1) Variable.__mul__ = mul # 处理a * b Variable.__rmul__ = mul # 处理b * a\",\"加法运算的运算符重载:\",\"加法运算的Add类已在上文中实现，其反向传播逻辑为将上游梯度原封不动地传递给两个输入变量（因为，）。类似地，我们将Add类绑定到+运算符：\",\"def add(x0, x1): x1 = as_array(x1) return Add()(x0, x1) Variable.__add__ = add # 处理a + b Variable.__radd__ = add # 处理b + a\",\"复合运算的验证:\",\"通过组合加法和乘法运算符，我们可以验证框架是否支持复杂表达式的自动微分。例如，计算y = a * b + c并求导：\",\"a = Variable(np.array(3.0)) b = Variable(np.array(2.0)) c = Variable(np.array(1.0)) y = a * b + c # 等价于 add(mul(a, b), c) y.backward() print(y.data) # 输出：7.0（3*2+1） print(a.grad) # 输出：2.0（∂y/∂a = b） print(b.grad) # 输出：3.0（∂y/∂b = a） print(c.grad) # 输出：1.0（∂y/∂c = 1）\",\"此例中，反向传播正确计算了每个变量的梯度，证明运算符重载与自动微分机制的一致性。\",\"(可选部分) 在Python中，运算符重载需要同时考虑左右运算符（如__add__和__radd__），这是由Python的运算符调度机制决定的。当表达式中的左右操作数类型不同时，Python会根据操作数的类型选择不同的方法调用路径。Python中，运算符的调用顺序遵循以下规则：\",\"左操作数优先：当执行表达式a OP b时，Python首先尝试调用左操作数a的__OP__方法。\",\"右操作数 fallback：如果左操作数未实现__OP__方法，或返回NotImplemented，则尝试调用右操作数b的__rOP__方法。\",\"以加法a + b为例：\",\"首先调用a.__add__(b)；\",\"若a未实现__add__或返回NotImplemented，则调用b.__radd__(a)。\",\"当操作数类型不同时（如Variable与数值、ndarray混合运算），必须通过__rOP__处理右操作数为自定义类型的情况。例如：\",\"x + 3：左操作数x是Variable，调用x.__add__(3)，可正常转换3为Variable；\",\"3 + x：左操作数3是int，不具备__add__方法处理Variable，因此需调用x.__radd__(3)。\",\"在Python的运算符重载中，以def __add__(self, other)为例，self和other是两个关键入参:\",\"self：在表达式a + b中，self指代左操作数a，即调用__add__方法的实例。\",\"other：在表达式a + b中，other指代右操作数b，即调用__radd__方法的实例。\",\"在 Python 的运算符重载中，以def radd(self, other)为例，self和other是两个关键入参：\",\"在表达式a + b中，若左操作数a不支持__add__方法（或返回NotImplemented），则会调用右操作数b的__radd__方法。此时，self指代右操作数b，即调用__radd__方法的实例\",\"在__radd__方法中，other指代左操作数a，其类型可能是原生数值、ndarray或Variable。\",\"支持与ndarray及数值类型的混合运算:\",\"为了提升框架的易用性，我们需要让Variable实例能与NumPy数组（ndarray）、Python数值类型（如int、float）直接进行运算。关键在于实现类型转换工具函数as_variable，将非Variable对象转换为Variable实例：\",\"def as_variable(obj): if isinstance(obj, Variable): return obj return Variable(obj) # 将ndarray或数值转换为Variable\",\"同时，修改Function类的__call__方法，在接收输入时自动将参数转换为Variable：\",\"class Function: def __call__(self, *inputs): inputs = [as_variable(x) for x in inputs] # 统一转换为Variable xs = [x.data for x in inputs] # 后续计算逻辑...\",\"并且在所有重载运算符函数实现中，将other参数统一转换为ndarray：\",\"def add(self, other): other = as_array(other) return Add()(self, other) def as_array(x): if np.isscalar(x): return np.array(x) return x\",\"这样，当执行x + np.array(3.0)或x + 3.0时，右侧的ndarray或数值会被自动转换为Variable，确保运算正常进行。\",\"处理运算符的左右操作数差异:\",\"以乘法为例，当表达式为3.0 * x时，Python会调用x的__rmul__方法（右乘）。由于乘法满足交换律，__rmul__可复用__mul__的实现：\",\"Variable.__rmul__ = mul # 与__mul__共用逻辑，支持3.0 * x\",\"类似地，对于不满足交换律的运算符（如减法），需要分别处理左右操作数。例如，2.0 - x需要调用x的__rsub__方法，此时需交换操作数顺序并调用Sub类：\",\"def rsub(x0, x1): return Sub()(x1, x0) # 实现a - b = Sub(b, a) Variable.__rsub__ = rsub\",\"运算符优先级与类型转换:\",\"为了确保Variable实例在混合运算中优先被处理，我们为Variable类添加__array_priority__属性，设置其优先级高于ndarray（默认优先级为100）：\",\"class Variable: __array_priority__ = 200 # 高于ndarray的优先级，确保类型转换优先\",\"这使得当表达式为np.array([2.0]) + x时，x的__radd__方法会被优先调用，保证运算按预期执行。\",\"负数运算（-）的实现:\",\"负数运算y = -x的正向传播简单地对输入取反，反向传播时将上游梯度取反（因为）：\",\"class Neg(Function): def forward(self, x): return -x def backward(self, gy): return -gy # 梯度取反\",\"绑定-运算符到neg函数：\",\"def neg(x): return Neg()(x) Variable.__neg__ = neg # 支持y = -x\",\"减法运算（-）的完整实现:\",\"减法运算y = x0 - x1的反向传播中，，，因此反向传播时需将上游梯度gy分别乘以1和-1：\",\"class Sub(Function): def forward(self, x0, x1): return x0 - x1 def backward(self, gy): return gy, -gy # 梯度分别乘以1和-1\",\"由于减法不满足交换律，需分别实现__sub__（处理x0 - x1）和__rsub__（处理x1 - x0）：\",\"def sub(x0, x1): return Sub()(x0, x1) def rsub(x0, x1): return Sub()(x1, x0) # 交换操作数实现a - b Variable.__sub__ = sub # 支持x0 - x1 Variable.__rsub__ = rsub # 支持x1 - x0\",\"除法运算（/）的实现:\",\"除法运算y = x0 / x1的导数公式为，，反向传播时需按此计算梯度：\",\"class Div(Function): def forward(self, x0, x1): return x0 / x1 def backward(self, gy): x0, x1 = self.inputs[0].data, self.inputs[1].data gx0 = gy / x1 gx1 = -gy * x0 / (x1 ** 2) return gx0, gx1 # 按导数公式计算梯度\",\"同样，需处理左右操作数的除法运算：\",\"def div(x0, x1): return Div()(x0, x1) def rdiv(x0, x1): return Div()(x1, x0) # 交换操作数实现a / b Variable.__truediv__ = div # 支持x0 / x1 Variable.__rtruediv__ = rdiv # 支持x1 / x0\",\"幂运算（**）的实现:\",\"幂运算y = x ** c中，c为常数指数，其导数公式为。反向传播时需按此计算梯度：\",\"class Pow(Function): def __init__(self, c): self.c = c # 保存指数 def forward(self, x): return x ** self.c def backward(self, gy): x = self.inputs[0].data c = self.c gx = c * (x ** (c - 1)) * gy # 导数公式：c·x^(c-1)·gy return gx\",\"绑定**运算符到pow函数：\",\"def pow(x, c): return Pow(c)(x) Variable.__pow__ = pow # 支持x ** c\",\"通过步骤20-22的实现，TinyPytorch框架实现了完整的运算符重载体系，使开发者能以自然的数学表达式编写代码（如y = (x + 3) ** 2 / 2），而无需调用特定函数。这种“可微分编程”的方式不仅降低了学习成本，还确保了复杂表达式的自动微分正确性，为后续实现神经网络层和优化算法奠定了基础。\"]},\"810\":{\"h\":\"步骤23: 项目模块化结构\",\"t\":[\"在Python开发中，模块、包和库是组织代码的重要方式：\",\"模块（module）：单个Python文件，如core.py，用于封装功能。\",\"包（package）：多个模块的集合，以目录形式存在，需包含__init__.py文件。\",\"库（library）：多个包的集合，通常用于实现完整功能（如TinyPytorch框架）。\",\"为将 TinyPytorch 的代码组织为可复用的包，从本章开始为每个Chapter设计如下目录结构：\",\"__init__.py # 包初始化文件 core.py # 核心功能（简化版） functions.py # 具体函数实现 utils.py # 工具函数\",\"core.py，包括：\",\"类定义：Config、Variable、Function及运算符相关类（Add、Mul等）。\",\"函数定义：using_config、no_grad、as_array、as_variable等工具函数，以及add、mul等运算符函数。\",\"__init__.py中导入核心类并初始化运算符重载:\",\"from chapter2.core import Variable from chapter2.core import Function from chapter2.core import using_config from chapter2.core import no_grad from chapter2.core import as_array from chapter2.core import as_variable from chapter2.core import setup_variable setup_variable() # 初始化运算符重载\",\"其中，setup_variable()函数负责绑定运算符方法：\",\"def setup_variable(): Variable.__add__ = add Variable.__radd__ = add Variable.__mul__ = mul Variable.__rmul__ = mul # 其他运算符绑定...\"]},\"811\":{\"h\":\"步骤24：复杂函数的求导\",\"t\":[\"优化问题中常使用特定函数评估算法性能，这些函数被称为测试函数。通过对复杂测试函数求导，可验证TinyPytorch框架处理高阶微分的能力。本步骤选取3个经典测试函数，演示TinyPytorch的自动微分功能。\",\"Sphere函数求导\",\"函数定义：，是简单的平方和函数，用于验证基础微分逻辑。\",\"代码实现：\",\"def sphere(x, y): z = x ** 2 + y ** 2 return z x = Variable(np.array(1.0)) y = Variable(np.array(1.0)) z = sphere(x, y) z.backward() print(x.grad, y.grad) # 输出：2.0 2.0\",\"结果验证：根据导数公式 、，在 处导数为(2.0, 2.0)，与运行结果一致。\",\"Matyas函数求导\",\"函数定义：，是包含交叉项的二维函数。\",\"代码实现：\",\"def matyas(x, y): z = 0.26 * (x ** 2 + y ** 2) - 0.48 * x * y return z x = Variable(np.array(1.0)) y = Variable(np.array(1.0)) z = matyas(x, y) z.backward() print(x.grad, y.grad) # 输出：0.04 0.04（近似值）\",\"结果分析：通过运算符重载，可直接将数学表达式转译为代码。若不使用运算符，需编写繁琐的函数调用（如sub(mul(0.26, add(pow(x, 2), pow(y, 2))), mul(0.48, mul(x, y))），凸显运算符重载的可读性优势。\",\"Goldstein-Price函数求导\",\"函数定义：\",\"该函数形式复杂，包含高次项和交叉项，是验证框架能力的理想案例。\",\"代码实现：\",\"def goldstein(x, y): z = (1 + (x + y + 1)** 2 * (19 - 14*x + 3*x**2 - 14*y + 6*x*y + 3*y** 2)) * \\\\ (30 + (2*x - 3*y)** 2 * (18 - 32*x + 12*x**2 + 48*y - 36*x*y + 27*y** 2)) return z x = Variable(np.array(1.0)) y = Variable(np.array(1.0)) z = goldstein(x, y) z.backward() print(x.grad, y.grad) # 输出：-5376.0 8064.0\",\"结果验证：通过梯度检验可知结果正确。TinyPytorch框架能自动处理复杂表达式的微分，无需手动推导导数公式，体现了自动微分的优势。\",\"TinyPytorch的核心能力总结\",\"自然代码表达：支持将数学公式直接转译为Python代码，如z = (x + y + 1)**2 * ...，无需额外接口。\",\"复杂计算图处理：无论计算图结构多复杂（如多层嵌套、高次运算），均能正确构建反向传播路径。\",\"可微分编程：将普通数值计算转换为可微分计算，使深度学习框架具备自动求导能力，为优化算法和神经网络训练奠定基础。\",\"深度学习框架的计算图范式: TinyPytorch采用Define-by-Run（动态计算图） 模式，与Define-and-Run（静态计算图）的对比如下：\",\"Define-by-Run：计算与图构建同时进行，如TinyPytorch中每一步运算都会动态创建计算图链接，支持Python原生控制流（if、while），调试便捷。\",\"Define-and-Run：先定义计算图再执行，需使用领域特定语言（如TensorFlow 1.x的tf.cond），适合大规模优化但灵活性较低。\",\"TinyPytorch的动态计算图模式使其在易用性和灵活性上表现突出，尤其适合研究和快速开发场景。\"]},\"812\":{\"h\":\"第二阶段总结\",\"t\":[\"这一阶段，我们构建了如下关键功能：\",\"扩展DeZero以处理多输入多输出函数，支持用+、*等运算符自然表达计算。\",\"修改Function类，通过列表处理可变长参数，优化正向传播实现。\",\"实现多元函数反向传播，解决变量重复使用时的梯度累加问题。\",\"引入“辈分”机制，确保复杂计算图反向传播顺序正确。\",\"使用弱引用解决循环引用，通过Config类和no_grad模式优化内存管理。\",\"重载运算符，支持Variable与数值、数组混合运算，提升代码可读性。\",\"为Variable添加shape等属性，使其行为更接近NumPy数组，优化打印等交互体验。\"]},\"813\":{\"h\":\"🧠 从零构建深度学习框架（三）：动态图可视化与高阶导数构建\",\"t\":[\"1.TinyPytorch 第三阶段: 高阶导数与深度学习优化进阶\",\"仓库链接: https://github.com/BinaryOracle/TinyPytorch 本节代码:\"]},\"814\":{\"h\":\"引言：从自动微分走向“可视化 + 高阶导数 + 灵活控制”\",\"t\":[\"随着 TinyPytorch 框架核心功能的日益完善，我们开始迈入更深入也更贴近真实深度学习框架设计的阶段。在前一阶段，我们实现了自动构建计算图与反向传播的基本机制，使得模型训练具备了基础的“学习”能力。\",\"在第三阶段，我们将从第25步继续出发，围绕计算图可视化、高阶导数构建、动态图控制与框架灵活性展开一系列扩展与优化：\",\"引入 Graphviz 实现计算图的可视化渲染，帮助开发者直观理解前向与反向传播路径；\",\"实现 create_graph=True 支持高阶导数的构建；\",\"引入 sin、cos、tanh 等函数节点，扩展函数库并验证高阶导数；\",\"构建泰勒展开、牛顿法等经典函数逼近与优化示例；\",\"完善框架的模块结构，优化 Function 与 Variable 的内存管理与执行流程。\",\"通过这 10 个步骤，TinyPytorch 不仅具备了现代框架应有的可视化与控制能力，还能够处理更复杂的自动微分任务，为后续的神经网络模块与训练机制打下坚实基础。我们将看到，它不仅是“能跑起来”，而是真正朝着“易用、清晰、高效”的方向进化。\"]},\"815\":{\"h\":\"步骤25: 可视化计算图\",\"t\":[\"当前TinyPytorch已能将复杂式子转化为代码，但需直观呈现计算图全貌以辅助调试与理解。为此引入第三方工具Graphviz，其支持节点和箭头构成的数据结构可视化，可用于展示TinyPytorch计算图。\",\"macOS安装：通过Homebrew执行 brew install graphviz。\",\"Ubuntu安装：执行 sudo apt install graphviz。\",\"验证安装：运行 dot -V，若显示版本信息（如dot - graphviz version 2.40.1）则安装成功。\",\"文件转换命令：使用dot sample.dot -T png -o sample.png将DOT格式文件转换为PNG图像，其中-T指定输出格式，-o指定输出文件名。\",\"DOT语言基础语法:\",\"简单节点定义：定义包含节点x和y的有向图，节点间用换行分隔。\",\"digraph g { x y }\",\"节点属性设置：定义节点ID为1，标签为x，颜色橙色并填充；shape=box可将节点设为矩形。\",\"digraph g { 1 [label=\\\"x\\\", color=orange, style=filled] 2 [label=\\\"y\\\", color=orange, style=filled, shape=box] }\",\"节点连接：使用->表示箭头连接，如1->2表示从节点1到节点3的有向边。\",\"digraph g { 1 [label=\\\"x\\\", color=orange, style=filled] 2 [label=\\\"y\\\", color=orange, style=filled] 1 -> 2 }\",\"TinyPytorch计算图转换为DOT语言:\",\"import numpy as np from chapter3 import Variable from chapter3 import get_dot_graph x0 = Variable(np.array(1.0)) x1 = Variable(np.array(1.0)) y = x0 + x1 x0.name = 'x0' x1.name = 'x1' y.name = 'y' txt = get_dot_graph(y, verbose=False) print(txt) with open('sample.dot', 'w') as f: f.write(txt)\",\"代码将变量y的计算图转换为DOT语言字符串，并保存为文件。verbose参数控制是否显示详细信息。\",\"输出的DOT语言示例包含变量节点（橙色圆形）和函数节点（浅蓝色矩形），如：\",\"digraph g { 4847712112 [label=\\\"y\\\", color=orange, style=filled] 4847712064 [label=\\\"Add\\\", color=lightblue, style=filled, shape=box] 4775983056 -> 4847712064 4847711968 -> 4847712064 4847712064 -> 4847712112 4775983056 [label=\\\"x0\\\", color=orange, style=filled] 4847711968 [label=\\\"x1\\\", color=orange, style=filled] }\",\"转换后的图像展示x0 + x1的计算图，包含Add函数节点和变量连接。\",\"核心函数实现原理:\",\"_dot_var函数：生成变量节点的DOT描述，使用id(v)作为节点唯一ID，支持显示变量名、形状和数据类型：\",\"def _dot_var(v, verbose=False): dot_var = '{} [label=\\\"{}\\\", color=orange, style=filled]\\\\n' name = '' if v.name is None else v.name if verbose and v.data is not None: if v.name is not None: name += ': ' name += str(v.shape) + ' ' + str(v.dtype) return dot_var.format(id(v), name)\",\"示例输出：4423761088 [label=\\\"x: (2, 3) float64\\\", color=orange, style=filled]。\",\"_dot_func函数：生成函数节点的DOT描述，使用函数类名作为标签：\",\"def _dot_func(f): # for function dot_func = '{} [label=\\\"{}\\\", color=lightblue, style=filled, shape=box]\\\\n' ret = dot_func.format(id(f), f.__class__.__name__) # for edge dot_edge = '{} -> {}\\\\n' for x in f.inputs: ret += dot_edge.format(id(x), id(f)) for y in f.outputs: # y is weakref ret += dot_edge.format(id(f), id(y())) return ret\",\"示例输出：4423742632 [label=\\\"Add\\\", color=lightblue, style=filled, shape=box]。\",\"计算图遍历逻辑：与反向传播类似，从输出变量出发遍历所有节点（变量和函数），生成DOT语言字符串。通过seen_set避免重复处理节点，使用funcs.append(f)和funcs.pop()实现后序遍历。\",\"def get_dot_graph(output, verbose=True): txt = '' funcs = [] seen_set = set() def add_func(f): if f not in seen_set: funcs.append(f) # funcs.sort(key=lambda x: x.generation) seen_set.add(f) add_func(output.creator) txt += _dot_var(output, verbose) while funcs: func = funcs.pop() txt += _dot_func(func) for x in func.inputs: txt += _dot_var(x, verbose) if x.creator is not None: add_func(x.creator) return 'digraph g {\\\\n' + txt + '}'\",\"可视化工具封装:\",\"plot_dot_graph函数：自动执行DOT文件转换并显示图像，支持保存为PNG、PDF等格式：\",\"def plot_dot_graph(output, verbose=True, to_file='graph.png'): dot_graph = get_dot_graph(output, verbose) tmp_dir = os.path.join(os.path.expanduser('~'), '.dezero') if not os.path.exists(tmp_dir): os.mkdir(tmp_dir) graph_path = os.path.join(tmp_dir, 'tmp_graph.dot') with open(graph_path, 'w') as f: f.write(dot_graph) extension = os.path.splitext(to_file)[1][1:] # Extension(e.g. png, pdf) cmd = 'dot {} -T {} -o {}'.format(graph_path, extension, to_file) subprocess.run(cmd, shell=True) # Return the image as a Jupyter Image object, to be displayed in-line. try: from IPython import display return display.Image(filename=to_file) except: pass\",\"该函数自动调用系统命令转换文件，并支持在Jupyter Notebook中直接显示图像。\",\"复杂函数可视化示例: 以Goldstein-Price函数为例\",\"import numpy as np from chapter3 import plot_dot_graph, Variable def goldstein(x, y): z = (1 + (x + y + 1)**2 * (19 - 14*x + 3*x**2 - 14*y + 6*x*y + 3*y**2)) * \\\\ (30 + (2*x - 3*y)** 2 * (18 - 32*x + 12*x**2 + 48*y - 36*x*y + 27*y**2)) return z x = Variable(np.array(1.0)) y = Variable(np.array(1.0)) z = goldstein(x, y) z.backward() x.name = 'x' y.name = 'y' z.name = 'z' plot_dot_graph(z, to_file='goldstein.png')\",\"可视化结果显示复杂计算图，包含多层Pow、Mul、Add等操作节点，验证DeZero对复杂表达式的计算图构建能力。\"]},\"816\":{\"h\":\"步骤26: 寻找函数最优解\",\"t\":[\"本步骤将处理Rosenbrock函数，其式子为：\",\"该函数的形状如下图所示，若画出其“山”的等高线，会发现线的形状类似香蕉，因此Rosenbrock函数也被称为“香蕉函数”。\",\"本步骤的目标是找到使Rosenbrock函数输出值最小的和。已知Rosenbrock函数的最小值在处，接下来将使用TinyPytorch验证是否能找到该最小值。\",\"Rosenbrock函数的严格定义是，其中和是常数。上述例子是、时的Rosenbrock函数，该函数常作为优化问题的基准函数使用。\",\"首先求Rosenbrock函数在处的导数和，使用TinyPytorch实现的代码如下：\",\"import numpy as np from chapter3 import Variable def rosenbrock(x0, x1): y = 100 * (x1 - x0 ** 2) ** 2 + (x0 - 1) ** 2 return y x0 = Variable(np.array(0.0)) x1 = Variable(np.array(2.0)) y = rosenbrock(x0, x1) y.backward() print(x0.grad, x1.grad)\",\"运行结果为：\",\"-2.0 400.0\",\"这里将数值数据封装在Variable中，通过backward()方法求导。得到的导数为-2.0，导数为400.0。梯度展示了各点上函数输出值增加最快的方向，在点上，值增加最快的方向是(-2.0,400.0)，那么值减少最快的方向是(2.0,-400.0)。\",\"梯度下降法解决问题:\",\"对于形状复杂的函数，其最大值可能不在梯度指示方向，最小值也可能不在梯度反方向，但从局部看，梯度表示函数输出值最大的方向。重复向梯度方向移动一定距离，再求梯度，可逐渐接近目标位置，这就是梯度下降法。若从好的起点开始，使用梯度下降法能高效找到目标值。\",\"使用梯度下降法寻找Rosenbrock函数最小值的代码如下:\",\"x0 = Variable(np.array(0.0)) x1 = Variable(np.array(2.0)) lr = 0.001 iters = 1000 for i in range(iters): y = rosenbrock(x0, x1) x0.cleargrad() x1.cleargrad() y.backward() x0.data -= lr * x0.grad x1.data -= lr * x1.grad\",\"代码中，迭代次数设为iters（iters是iterations的缩写），与梯度相乘的值设为lr=0.001（lr是learning rate的缩写，即学习率）。\",\"由于for语句反复使用Variable实例x0和x1求导，而每次反向传播时导数会累加，所以在反向传播前需调用各变量的cleargrad方法重置导数。\",\"运行代码，从输出信息可看到(x0,x1)值的更新过程，部分结果如下：\",\"iter 992: x0 = 0.682166, x1 = 0.463833 iter 993: x0 = 0.682388, x1 = 0.464137 iter 994: x0 = 0.682609, x1 = 0.464440 iter 995: x0 = 0.682830, x1 = 0.464743 iter 996: x0 = 0.683051, x1 = 0.465046 iter 997: x0 = 0.683271, x1 = 0.465348 iter 998: x0 = 0.683492, x1 = 0.465651 iter 999: x0 = 0.683712, x1 = 0.465953\",\"将计算结果绘制在图上，如下图所示，从图中可看出逐渐接近星号所指的目的地位置，但尚未到达。\",\"增加迭代次数设为 iters =10000，结果如下图所示，此时离目的地更近，(x0,x1)的值为(0.99449622,0.98900063)。\",\"若再增加迭代次数到 iters =50000，就会抵达(1.0,1.0)。\",\"包含绘图的完整代码:\",\"import numpy as np from matplotlib import pyplot as plt from chapter3 import Variable def rosenbrock(x0, x1): y = 100 * (x1 - x0 ** 2) ** 2 + (x0 - 1) ** 2 return y x0 = Variable(np.array(0.0)) x1 = Variable(np.array(2.0)) lr = 0.001 iters = 50000 x0_list = [] x1_list = [] for i in range(iters): y = rosenbrock(x0, x1) x0.cleargrad() x1.cleargrad() y.backward() x0.data -= lr * x0.grad x1.data -= lr * x1.grad x0_list.append(x0.data.copy()) x1_list.append(x1.data.copy()) print('iter %d: x0 = %f, x1 = %f' % (i, x0.data, x1.data)) # 绘制等高线图 x = np.linspace(-2, 2, 400) y = np.linspace(-1, 3, 400) X, Y = np.meshgrid(x, y) Z = (1 - X)**2 + 100 * (Y - X**2)**2 plt.figure(figsize=(8, 6)) cp = plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 30), cmap='jet') plt.plot(x0_list, x1_list, 'o-', color='yellow', markersize=2, label='Gradient Descent Path') # 在 (1.0, 1.0) 处标记最优点 plt.plot(1.0, 1.0, marker='*', markersize=12, color='red', label='Minimum (1,1)') plt.xlabel('x0') plt.ylabel('x1') plt.title('Gradient Descent on Rosenbrock Function') plt.legend() plt.grid(True) plt.show()\",\"本步骤使用TinyPytorch实现了梯度下降法，找到了Rosenbrock函数最小值的位置，不过迭代次数较多，有5万次。实际上梯度下降法并不擅长处理Rosenbrock这种类型的函数，下一个步骤会介绍并实现另一种优化方法。\"]},\"817\":{\"h\":\"步骤27: 高阶导数\"},\"818\":{\"h\":\"🏗️ 从零构建深度学习框架（四）：计算图进阶与通用神经网络实现\",\"t\":[\"4.TinyPytorch 第四阶段: 通用网络层封装与模型训练流程构建\",\"仓库链接: https://github.com/BinaryOracle/TinyPytorch 本节代码:\"]},\"819\":{\"h\":\"引言：从自动微分迈向可训练的神经网络模型\",\"t\":[\"前三阶段的 TinyPytorch，已实现自动微分系统与基础函数操作。在第四阶段，我们将真正迈入“深度学习框架”的核心部分——从简单函数组合进化到模块化神经网络，实现可复用的层（Layer）、模型（Model）、优化器（Optimizer）等，最终完成一个能训练分类任务的通用框架。\",\"本阶段的目标是打造一个“小而全”的深度学习训练系统。我们将实现：\",\"网络层封装（如 Linear、ReLU 等）\",\"模型类 Model 与训练流程规范\",\"参数管理与清理机制\",\"SGD 优化器与 momentum 拓展\",\"批处理、数据加载器与数据集支持\",\"实际任务训练（分类任务 + MNIST 手写数字）\",\"第四阶段共 14 个步骤，从第44步到第57步，形成了一个具备如下特征的微型深度学习框架：\"]},\"820\":{\"h\":\"TinyPytorch\"},\"821\":{\"h\":\"1.前置知识\",\"t\":[\"智慧化知识库系统: 大语言模型应用开发基础知识速览。\"]},\"822\":{\"h\":\"大语言模型\",\"t\":[\"大语言模型（LLM，Large Language Model），也称大型语言模型，是一种旨在理解和生成人类语言的人工智能模型。\",\"LLM 通常指包含数百亿（或更多）参数的语言模型，它们在海量的文本数据上进行训练，从而获得对语言深层次的理解。目前，国外的知名 LLM 有 GPT、LLaMA、Gemini、Claude 和 Grok 等，国内的有 DeepSeek、通义千问、豆包、Kimi、文心一言、GLM 等。\",\"为了探索性能的极限，许多研究人员开始训练越来越庞大的语言模型，例如拥有 175B (1750 亿)参数的 GPT-3 和 540B（5400 亿）参数的 PaLM 。尽管这些大型语言模型与小型语言模型（例如 3.3 亿参数的 BERT 和 15 亿参数的 GPT-2）使用相似的架构和预训练任务，但它们展现出截然不同的能力，尤其在解决复杂任务时表现出了惊人的潜力，这被称为“涌现能力”。以 GPT-3 和 GPT-2 为例，GPT-3 可以通过学习上下文来解决少样本任务，而 GPT-2 在这方面表现较差。因此，科研界给这些庞大的语言模型起了个名字，称之为“大语言模型（LLM）”。LLM 的一个杰出应用就是 ChatGPT ，它是 GPT 系列 LLM 用于与人类对话式应用的大胆尝试，展现出了非常流畅和自然的表现。 语言建模的研究可以追溯到 20 世纪 90 年代，当时的研究主要集中在采用统计学习方法来预测词汇，通过分析前面的词汇来预测下一个词汇。但在理解复杂语言规则方面存在一定局限性。\",\"随后，研究人员不断尝试改进，2003 年深度学习先驱 Bengio 在他的经典论文 《A Neural Probabilistic Language Model》中，首次将深度学习的思想融入到语言模型中。强大的神经网络模型，相当于为计算机提供了强大的\\\"大脑\\\"来理解语言，让模型可以更好地捕捉和理解语言中的复杂关系。\",\"2018 年左右，Transformer 架构的神经网络模型开始崭露头角。通过大量文本数据训练这些模型，使它们能够通过阅读大量文本来深入理解语言规则和模式，就像让计算机阅读 整个互联网一样，对语言有了更深刻的理解，极大地提升了模型在各种自然语言处理任务上的表现。\",\"与此同时，研究人员发现，随着语言模型规模的扩大（增加模型大小或使用更多数据），模型展现出了一些惊人的能力，在各种任务中的表现均显著提升（Scaling Law）。这一发现标志着大型语言模型（LLM）时代的开启。\",\"通常大模型由三个阶段构成：预训练、后训练和在线推理。在 2024 年 9 月之前，大模型领域仅存在预训练阶段的 Scaling Law。然而，随着 OpenAI o1 的推出，后训练和在线推理阶段也各自拥有了 Scaling Law，即后训练阶段的强化学习 Scaling Law（RL Scaling Law）和在线推理阶段的 Inference Scaling Law（Test Time Scaling Law）。 随着各阶段计算量的增加，大模型的性能不断增长。\"]},\"823\":{\"h\":\"常见的LLM\",\"t\":[\"大语言模型的发展历程虽然只有短短不到五年的时间，但是发展速度相当惊人，截止 2024 年 6 月，国内外有超过百种大模型相继发布。下图按照时间线给出了 2019 年至 2024 年 6 月比较有影响力并且模型参数量超过 100 亿的大语言模型：\",\"接下来我们主要介绍几个国内外常见的大模型（包括开源和闭源）。\",\"OpenAI\",\"OpenAI 公司在 2018 年 提出的 GPT（Generative Pre-Training） 模型是典型的 生成式预训练语言模型 之一。 GPT 模型的基本原则是通过语言建模将世界知识压缩到仅解码器 (decoder-only) 的 Transformer 模型中，这样它就可以恢复(或记忆)世界知识的语义，并充当通用任务求解器。它能够成功的两个关键点：\",\"训练能够准确预测下一个单词的 decoder-only 的 Transformer 语言模型\",\"扩展语言模型的大小\",\"OpenAI 在 LLM 上的研究大致可以分为以下几个阶段：\",\"目前，GPT 系列已形成 知识型 与 推理型 两大技术分支。\",\"2022 年 11 月，OpenAI 发布了基于 GPT 模型（GPT-3.5 和 GPT-4）的会话应用 ChatGPT。由于与人类交流的出色能力，ChatGPT 自发布以来就引发了人工智能社区的兴奋。ChatGPT 本质上是一个 LLM 应用，是基于基座模型开发出来的，与基座模型有本质的区别。ChatGPT 上线后用户增长迅速，5 天注册人数突破 100 万，两个月后月活用户破亿，成为当时史上用户增长最快的消费级应用程序。\",\"随着不断迭代，ChatGPT 逐渐丰富了其功能：\",\"插件系统：允许开发者创建工具扩展 ChatGPT 的能力，实现网页浏览、数据分析和第三方服务调用\",\"实时语音和视频对话：用户可与 AI 进行自然的语音和视频交流，支持手势识别和情感表达\",\"多模态能力：能够分析和理解用户提供的图片、音频和视频，实现全面的多模态交互\",\"自定义指令与记忆功能：记住用户之前的交互习惯和偏好，提供个性化体验\",\"GPT 构建器平台：允许用户无需编程创建专用的 AI 助手，支持自定义知识库和行为模式\",\"数据分析与可视化：直接处理和分析上传的数据文件，生成图表和可视化报告\",\"知识型与推理型双模式：可在 GPT-4.5 (知识型) 和 o1/o3 (推理型) 之间切换，满足不同场景需求\",\"思维链展示：在推理型模型中可选择性展示思考过程，帮助用户理解推理步骤\",\"2023 年 3 月 发布的 GPT-4 引入了多模态能力，相比 GPT-3.5 的 1750 亿参数，GPT-4 规模显著扩大（推测约 1.8 万亿参数），在解决复杂任务和评估任务上展现出较大的性能提升。\",\"2024 年 5 月 发布的 GPT-4o（\\\"o\\\"代表\\\"omni\\\"全能）具备对文本、语音、图像三种模态的深度理解能力，主要特点包括：\",\"多模态融合：无缝理解和生成多种形式内容\",\"实时对话：响应速度比 GPT-4 快约 2 倍\",\"情感表达：在语音互动中传递更丰富的情感变化\",\"成本效益：API 定价降低约 50%\",\"2024 年 7 月 发布的 GPT-4o mini 是一款面向消费级应用的轻量级模型，价格更加亲民，适合日常对话和基础任务场景。\",\"2025 年 2 月 发布的 GPT-4.5 在知识广度、推理深度和创意表达方面有显著提升，特别强化了对客观事实的准确性，尤其是情商方面异常优秀。上下文长度扩展至 512K。是 OpenAI 的最后一个非思维链模型。\",\"主流知识型模型对比:\",\"模型名称\",\"上下文长度\",\"特点\",\"知识截止日期\",\"GPT-4\",\"16k\",\"经济，专门对话\",\"2021 年 9 月\",\"GPT-4o\",\"128k\",\"多模态，速度快\",\"2023 年 10 月\",\"GPT-4.5\",\"128k\",\"最强知识型，精准度高\",\"2023 年 10 月\",\"GPT-4o mini\",\"128k\",\"轻量知识型，性价比高\",\"2023 年 10 月\",\"2024 年 9 月 发布的 o1-mini、o1-preview 是专为复杂推理设计的模型，在回答前会先生成一段思维链（不公开），优先考虑精确性和推理步骤的正确性。\",\"超强推理能力：在数学、编程和逻辑推理等任务中表现卓越。\",\"解题过程可靠：注重解题中间步骤的正确性。\",\"问题分解能力：将复杂问题分解为可管理的子问题。\",\"自纠错机制：识别错误并主动纠正。\",\"2024 年 12 月 发布的 o1 比 o1-preview 可以在更快的时间内响应，思考的时间更短。\",\"2025 年 1 月 发布的 o3-mini 可以显示部分思维链，与 o1 相比，可以保持效果的情况下，响应速度更快。\",\"模型名称\",\"上下文长度\",\"特点\",\"知识截止日期\",\"o1\",\"128k\",\"强推理能力，慢\",\"2023 年 10 月\",\"o1 mini\",\"200k\",\"轻量推理，中速\",\"2023 年 10 月\",\"o3 mini\",\"200k\",\"超轻量推理，最快\",\"2023 年 10 月\",\"OpenAI 的模型战略形成了“知识型”和“推理型”两条互补产品线：\",\"知识型模型 专注于广泛知识覆盖和流畅对话体验。\",\"推理型模型 专注于精确推理和复杂问题求解，让用户可根据具体需求选择最适合的模型类型。\",\"Claude\",\"Claude 系列模型是由 OpenAI 离职人员创建的 Anthropic 公司开发的闭源语言大模型。\",\"最早的 Claude 于 2023 年 3 月 15 日发布。\",\"2024 年 3 月 4 日，更新至 Claude-3，包括 Claude 3 Haiku、Claude 3 Sonnet 和 Claude 3 Opus，它们的能力依次递增，旨在满足不同用户和应用场景的需求。\",\"2024 年 10 月，Anthropic 发布了 Claude 3.5 Sonnet，这是一款在推理和通用任务上有显著提升的模型。\",\"2025 年 5 月，Anthropic 又进一步发布了 Claude 4.0，包括了 Claude 4 Sonnet 和 Claude 4 Opus，均是混合推理模型，支持标准模式与推理思考模式，编码能力异常强大。支持多工具并行调用与精准指令解析，本地文件访问时内存管理升级，可规避捷径行为，强化复杂任务处理能力。\",\"模型名称\",\"上下文长度\",\"特点\",\"Claude 3.5 Haiku\",\"200k\",\"速度最快\",\"Claude 4 Sonnet\",\"200k\",\"最强性能，领先推理力\",\"Claude 4 Opus\",\"200k\",\"性能强大，费用最高\",\"Gemini\",\"Gemini 系列语言大模型由 Google 开发。\",\"2022 年 4 月，发布了初始版本（PaLM 后更名为 Gemini）。\",\"2025 年 2 月，Google 发布了 Gemini 2.0 系列模型，在性能和效率上有显著提升。包括 Gemini 2.0 Pro、Gemini 2.0 Flash、Gemini 2.0 Flash-Lite 是 Gemini 2.0 系列的三个版本，分别适用于不同的场景。同样，推出了其推理模型 Gemini 2.0 Flash Thinking。\",\"2025 年 3 月，Google 发布了 Gemini 2.5 Pro，性能有了进一步提升，推理能力和代码能力提升非常显著。\",\"模型名称\",\"上下文长度\",\"特点\",\"Gemini 2.5 Pro\",\"2M\",\"性能最强\",\"Gemini 2.0 Flash\",\"1M\",\"低延迟，性能强\",\"Gemini 2.0 Flash-Lite\",\"1M\",\"性价比最高\",\"Gemini 2.0 Flash Thinking\",\"1M\",\"思维链展示\",\"文心一言\",\"文心一言是基于百度文心大模型的知识增强语言大模型，于 2023 年 3 月 在国内率先开启邀测。文心一言的基础模型文心大模型于 2019 年发布 1.0 版，现已更新到 4.0 版本。更进一步划分，文心大模型包括 NLP 大模型、CV 大模型、跨模态大模型、生物计算大模型、行业大模型。文心一言的中文能力相对来说非常不错。 文心一言网页版分为 免费版 和 专业版：\",\"免费版 使用文心 3.5 版本，已经能够满足个人用户或小型企业的大部分需求。\",\"专业版 使用文心 4.0 版本，定价为 59.9 元/月，连续包月优惠价为 49.9 元/月。\",\"星火大模型\",\"讯飞星火认知大模型是科大讯飞发布的语言大模型，支持多种自然语言处理任务。\",\"2023 年 5 月，首次发布。\",\"2024年 10 月，讯飞星火发布模型 星火 4.0 Turbo。\",\"2025 年 1 月，讯飞发布了推理思考模型 讯飞星火 X1 和 星火语音同传模型。\",\"LLaMA\",\"LLaMA 系列模型是 Meta 开源的一组参数规模从 8B 到 405B 的基础语言模型。\",\"2023 年 2 月，发布 LLaMA。\",\"2023 年 7 月，发布了 LLaMA2 模型。\",\"2024 年 4 月，发布了 LLaMA3 模型。\",\"2024 年 7 月，发布了 LLaMA 3.1 模型。\",\"2024 年 12 月，发布了 LLaMA 3.3 模型（只开源了 70B 的指令模型）。\",\"它们都是在数万亿个字符上训练的，展示了如何仅使用公开可用的数据集来训练最先进的模型，而不需要依赖专有或不可访问的数据集。这些数据集包括 Common Crawl、Wikipedia、OpenWebText2、RealNews、Books 等。LLaMA 模型使用了大规模的数据过滤和清洗技术，以提高数据质量和多样性，减少噪声和偏见。LLaMA 模型还使用了高效的数据并行和流水线并行技术，以加速模型的训练和扩展其中 405B 参数模型是首个公开的千亿级开源模型，性能对标 GPT-4o 等商业闭源模型。 与 GPT 系列相同，LLaMA 模型也采用了 decoder-only 架构，同时结合了一些前人工作的改进。LLaMA 系列基本上是后续大模型的标杆：\",\"Pre-normalization（正则化）：为了提高训练稳定性，LLaMA 对每个 Transformer 子层的输入进行了 RMSNorm 归一化，这种归一化方法可以避免梯度爆炸和消失的问题，提高模型的收敛速度和性能。\",\"SwiGLU（激活函数）：将 ReLU 非线性替换为 SwiGLU 激活函数，增加网络的表达能力和非线性，同时减少参数量和计算量。\",\"旋转位置编码（RoPE, Rotary Position Embedding）：模型的输入不再使用位置编码，而是在网络的每一层添加了位置编码，RoPE 位置编码可以有效地捕捉输入序列中的相对位置信息，并且具有更好的泛化能力。\",\"分组查询注意力（GQA, Grouped-Query Attention）：通过将查询（query）分组并在组内共享键（key）和值（value），减少了计算量，同时保持了模型性能，提高了大型模型的推理效率（LLaMA2 只有 70B 采用）。\",\"LLaMA 3.1 于 2024 年 7 月 发布，提高了模型的性能和效率：\",\"更多的训练数据量：LLaMA3.1 在 15 万亿个 token 的数据上进行预训练，采用了更科学的数据配比。LLaMA3.1 接触到更多的文本信息，从而提高了其理解和生成文本的能力。\",\"更长的上下文长度：LLaMA 3.1 将上下文长度大幅提升至 128K token，支持处理极长的文档和对话历史，改善了对长文本的理解和生成能力，适用于更复杂的应用场景。\",\"分组查询注意力（GQA, Grouped-Query Attention）：通过将查询（query）分组并在组内共享键（key）和值（value），减少了计算量，同时保持了模型性能，提高了大型模型的推理效率（LLaMA2 只有 70B 采用）。\",\"更大的词表：LLaMA3.1 采用了 128K 的 tokenizer，是前两代 32K 的 4 倍，这使得其语义编码能力得到了极大的增强，从而显著提升了模型的性能。\",\"精细的指令遵循：通过改进的对齐技术，LLaMA 3.1 在遵循复杂指令、理解微妙提示方面表现更出色，使模型行为更可预测和可控。\",\"完善的工具使用：增强了 Function Calling 能力，使模型能够更准确地识别何时以及如何调用外部工具，提高了与外部系统集成的能力。 LLaMA 3.1 发布了 8B、70B 和 405B 三个规模的模型，分别提供基础版（Base）和指令微调版（Instruction），进一步扩展了 LLaMA 系列在开源社区的影响力和应用前景。\",\"DeepSeek\",\"DeepSeek 是由深度求索 (DeepSeek) 团队开发的开源大语言模型系列。首个版本于 2023 年 11 月 发布。DeepSeek 采用 decoder-only 架构，融合了 FlashAttention-2、RoPE 位置编码、SwiGLU 等先进技术，在多语言理解和代码生成等方面表现出色。\",\"2023 年 11 月 12 日：发布 DeepSeek 系列基础模型，包括 7B 和 67B 两种规模的 Base 和 Chat 版本。模型在 1.2 万亿 token 上进行训练，同时发布了 DeepSeek-Coder 专用代码生成模型。\",\"2024 年 3 月 15 日：发布 DeepSeek-V2 系列，提升了多语言能力、长文本理解和推理能力，同时发布了 DeepSeek-MoE 混合专家模型。\",\"2024 年 5 月 31 日：发布 DeepSeek-V2.5，性能得到进一步提升，上下文长度扩展至 128K tokens，并改进了工具调用和多模态能力。\",\"2024 年 10 月：发布 DeepSeek-V3，在推理能力、多语言理解和创意生成方面有显著提升，支持更复杂的系统提示词控制，并进一步提升了代码质量和多轮对话一致性。\",\"2025 年 2 月：\",\"DeepSeek-R1 推理型大模型：专注于复杂问题求解和精确推理能力，在数学、逻辑推理和结构化知识方面展现出卓越性能，类似于 OpenAI 的 o1 系列。并且是首个开源的推理型大模型，在多项基准测试中超越了 o1 系列。\",\"DeepSeek-R1-Zero：直接在大规模强化学习 (RL) 训练的模型，无需 SFT，在推理方面就十分出色。\",\"同时开源了用 Llama 和 Qwen 从 DeepSeek-R1 中蒸馏出的六个 dense 模型。其中 DeepSeek-R1-Distill-Qwen-32B 在各种基准测试中均优于 OpenAI-o1-mini。\",\"deepseek 目前采用的主要改进如下：\",\"多头潜在注意力 (MLA, Multi-head Latent Attention)：通过将键值 (KV) 缓存显著压缩为潜在向量来保证高效推理的同时不降低效果。\",\"DeepSeekMoE：通过稀疏计算以经济的成本训练强大的模型。\",\"一系列推理加速技术 借助 DeepSeekR1 的卓越能力，DeepSeek 成为了现象级爆火应用。7 天完成了 1 亿用户的增长，打破了 ChatGPT 的 2 个月的最快记录，成为史上增长最快的 AI 应用。\",\"通义千问\",\"通义千问是由阿里巴巴基于“通义”大模型研发，于 2023 年 4 月 正式发布。\",\"2023 年 9 月：阿里云开源了 Qwen（通义千问）系列工作。\",\"2024 年 6 月 6 日：正式开源了 Qwen2。\",\"2025 年 4 月 29 日：发布了全新升级的 Qwen3 系列模型。\",\"Qwen 系列均采用 decoder-only 架构，并结合 SwiGLU 激活、RoPE、GQA 等技术。中文能力相对来说是非常不错的开源模型。 目前，已经开源了 7 种模型大小：\",\"Dense 模型：0.6B、1.7B、4B、8B、14B、32B；\",\"MoE 模型：30B-A3B、235B-A22B。\",\"上下文长度：\",\"8B 以下模型的上下文长度为 32k；\",\"8B 以上模型的上下文长度为 128k。\",\"Qwen3 进一步增强了模型性能，改进了推理能力和指令遵循能力，同时保持了低资源部署的高效性，使其在长文本理解和复杂任务处理方面具有更强的优势。支持思考模式和非思考模式之间无缝切换，覆盖 119 种语言和方言。强化了模型的代码能力、Agent 能力，以及对 MCP 的支持。 同时还开源了代码模型和数学模型：\",\"Qwen2.5-Coder：1.5B、7B，以及即将推出的 32B。\",\"Qwen2.5-Math：1.5B、7B，以及 72B。\",\"在推理大模型方面：\",\"2024 年 11 月：发布并开源了 QwQ-32B-Preview 模型，仅用 32B 参数便在部分达到了 o1-mini 的推理水平。\",\"2025 年 3 月：发布并开源了 QwQ-32B，其性能可与具备 671B 参数（37B 激活参数）的 DeepSeek-R1 媲美。\",\"ChatGLM\",\"GLM系列模型是 清华大学和智谱 AI 等合作研发的语言大模型。\",\"2023 年 3 月，发布了 ChatGLM。\",\"2024 年 1 月，发布了 GLM4，并于 2024 年 6 月 正式开源。\",\"GLM-4-9B-Chat 支持多轮对话的同时，还具备网页浏览、代码执行、自定义工具调用（Function Call）和长文本推理（支持最大 128K 上下文）等功能。 开源了 对话模型 GLM-4-9B-Chat、基础模型 GLM-4-9B、长文本对话模型 GLM-4-9B-Chat-1M（支持 1M 上下文长度）、多模态模型 GLM-4V-9B 等全面对标 OpenAI。\"]},\"824\":{\"h\":\"LLM 的特点与能力\",\"t\":[\"大语言模型具有多种显著特点，这些特点使它们在自然语言处理和其他领域中引起了广泛的兴趣和研究。以下是大语言模型的一些主要特点：\",\"巨大的规模： LLM 通常具有巨大的参数规模，可以达到数十亿甚至数千亿个参数。这使得它们能够捕捉更多的语言知识和复杂的语法结构。\",\"预训练和微调： LLM 采用了预训练和微调的学习方法。首先在大规模文本数据上进行预训练（无标签数据），学习通用的语言表示和知识。然后通过微调（有标签数据）适应特定任务，从而在各种NLP 任务中表现出色。\",\"上下文感知： LLM 在处理文本时具有强大的上下文感知能力，能够理解和生成依赖于前文的文本内容。这使得它们在对话、文章生成和情境理解方面表现出色。\",\"多语言支持： LLM 可以用于多种语言，不仅限于英语。它们的多语言能力使得跨文化和跨语言的应用变得更加容易。\",\"多模态支持： 一些 LLM 已经扩展到支持多模态数据，包括文本、图像和声音。使得它们可以理解和生成不同媒体类型的内容，实现更多样化的应用。\",\"伦理和风险问题： 尽管 LLM 具有出色的能力，但它们也引发了伦理和风险问题，包括生成有害内容、隐私问题、认知偏差等。因此，研究和应用 LLM 需要谨慎。\",\"高计算资源需求： LLM 参数规模庞大，需要大量的计算资源进行训练和推理。通常需要使用高性能的 GPU 或 TPU 集群来实现。 大语言模型是一种具有强大语言处理能力的技术，已经在多个领域展示了潜力。它们为自然语言理解和生成任务提供了强大的工具，同时也引发了对其伦理和风险问题的关注。这些特点使 LLM 成为了当今计算机科学和人工智能领域的重要研究和应用方向。\"]},\"825\":{\"h\":\"\",\"t\":[\"区分大语言模型（LLM）与以前的预训练语言模型（PLM）最显著的特征之一是它们的 涌现能力。涌现能力是一种令人惊讶的能力，它在小型模型中不明显，但在大型模型中特别突出。类似物理学中的相变现象，涌现能力就像是模型性能随着规模增大而迅速提升，超过了随机水平，也就是我们常说的 量变引起质变。 涌现能力可以与某些复杂任务有关，但我们更关注的是其通用能力。接下来，我们简要介绍三个 LLM 典型的涌现能力：\",\"上下文学习：上下文学习能力是由 GPT-3 首次引入的。这种能力允许语言模型在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而无需额外的训练或参数更新。\",\"指令遵循：通过使用自然语言描述的多任务数据进行微调，也就是所谓的 指令微调。LLM 被证明在使用指令形式化描述的未见过的任务上表现良好。这意味着 LLM 能够根据任务指令执行任务，而无需事先见过具体示例，展示了其强大的泛化能力。\",\"逐步推理：小型语言模型通常难以解决涉及多个推理步骤的复杂任务，例如数学问题。然而，LLM 通过采用 思维链（CoT, Chain of Thought） 推理策略，利用包含中间推理步骤的提示机制来解决这些任务，从而得出最终答案。据推测，这种能力可能是通过对代码的训练获得的。 这些涌现能力让 LLM 在处理各种任务时表现出色，使它们成为了解决复杂问题和应用于多领域的强大工具。\"]},\"826\":{\"h\":\"\",\"t\":[\"在 2021 年，斯坦福大学等多所高校的研究人员提出了基座模型（foundation model）的概念，清晰了预训练模型的作用。这是一种全新的 AI 技术范式，借助于海量无标注数据的训练，获得可以适用于大量下游任务的大模型（单模态或者多模态）。这样，多个应用可以只依赖于一个或少数几个大模型进行统一建设。 大语言模型是这个新模式的典型例子，使用统一的大模型可以极大地提高研发效率。相比于每次开发单个模型的方式，这是一项本质上的进步。大型模型不仅可以缩短每个具体应用的开发周期，减少所需人力投入，也可以基于大模型的推理、常识和写作能力，获得更好的应用效果。因此，大模型可以成为 AI 应用开发的大一统基座模型，这是一个一举多得、全新的范式，值得大力推广。\"]},\"827\":{\"h\":\"\",\"t\":[\"让大语言模型真正火爆的契机，是基于对话聊天的 ChatGPT。业界很早就发现了用户对于对话交互的特殊偏好，陆奇在微软期间，就于 2016 年推进过“对话即平台（conversation as a platform）” 的战略。此外，苹果 Siri 、亚马逊 Echo 等基于语音对话的产品也非常受欢迎，反映出互联网用户对于聊天和对话这种交互模式的偏好。虽然之前的聊天机器人存在各种问题，但大型语言模型的出现再次让聊天机器人这种交互模式可以重新涌现。用户愈发期待像钢铁侠中“贾维斯”一样的人工智能，无所不能、无所不知。这引发我们对于 智能体（Agent） 类型应用前景的思考，Auto-GPT、微软 Jarvis 等项目已经出现并受到关注，相信未来会涌现出很多类似的以对话形态让助手完成各种具体工作的项目。\",\"LLM 已经在许多领域产生了深远的影响。在自然语言处理领域，它可以帮助计算机更好地理解和生成文本，包括写文章、回答问题、翻译语言等。在信息检索领域，它可以改进搜索引擎，让我们更轻松地找到所需的信息。在计算机视觉领域，研究人员还在努力让计算机理解图像和文字，以改善多媒体交互。 最重要的是，LLM 的出现让人们重新思考了 通用人工智能（AGI） 的可能性。AGI 是一种像人类一样思考和学习的人工智能。LLM 被认为是 AGI 的一种早期形式，这引发了对未来人工智能发展的许多思考和计划。\",\"总之，LLM 是一种令人兴奋的技术，它让计算机更好地理解和使用语言，正在改变着我们与技术互动的方式，同时也引发了对未来人工智能的无限探索。\"]},\"828\":{\"h\":\"检索增强生成（RAG, Retrieval-Augmented Generation）\",\"t\":[\"大型语言模型（LLM）相较于传统的语言模型具有更强大的能力，然而在某些情况下，它们仍可能无法提供准确的答案。为了解决大型语言模型在生成文本时面临的一系列挑战，提高模型的性能和输出质量，研究人员提出了一种新的模型架构：检索增强生成（RAG, Retrieval-Augmented Generation）。该架构巧妙地整合了从庞大知识库中检索到的相关信息，并以此为基础，指导大型语言模型生成更为精准的答案，从而显著提升了回答的准确性与深度。 目前 LLM 面临的主要问题有：\",\"信息偏差/幻觉： LLM 有时会产生与客观事实不符的信息，导致用户接收到的信息不准确。RAG 通过检索数据源，辅助模型生成过程，确保输出内容的精确性和可信度，减少信息偏差。\",\"知识更新滞后性： LLM 基于静态的数据集训练，这可能导致模型的知识更新滞后，无法及时反映最新的信息动态。RAG 通过实时检索最新数据，保持内容的时效性，确保信息的持续更新和准确性。\",\"内容不可追溯： LLM 生成的内容往往缺乏明确的信息来源，影响内容的可信度。RAG 将生成内容与检索到的原始资料建立链接，增强了内容的可追溯性，从而提升了用户对生成内容的信任度。\",\"领域专业知识能力欠缺： LLM 在处理特定领域的专业知识时，效果可能不太理想，这可能会影响到其在相关领域的回答质量。RAG 通过检索特定领域的相关文档，为模型提供丰富的上下文信息，从而提升了在专业领域内的问题回答质量和深度。\",\"推理能力限制： 面对复杂问题时，LLM 可能缺乏必要的推理能力，这影响了其对问题的理解和回答。RAG 结合检索到的信息和模型的生成能力，通过提供额外的背景知识和数据支持，增强了模型的推理和理解能力。\",\"应用场景适应性受限： LLM 需在多样化的应用场景中保持高效和准确，但单一模型可能难以全面适应所有场景。RAG 使得 LLM 能够通过检索对应应用场景数据的方式，灵活适应问答系统、推荐系统等多种应用场景。\",\"长文本处理能力较弱： LLM 在理解和生成长篇内容时受限于有限的上下文窗口，且必须按顺序处理内容，输入越长，速度越慢。RAG 通过检索和整合长文本信息，强化了模型对长上下文的理解和生成，有效突破了输入长度的限制，同时降低了调用成本，并提升了整体的处理效率。\"]},\"829\":{\"h\":\"工作流程\",\"t\":[\"RAG 是一个完整的系统，其工作流程可以简单地分为数据处理、检索、增强和生成四个阶段：\",\"数据处理阶段: 对原始数据进行清洗和处理; 将处理后的数据转化为检索模型可以使用的格式; 将处理后的数据存储在对应的数据库中。\",\"检索阶段: 将用户的问题输入到检索系统中，从数据库中检索相关信息。\",\"增强阶段: 对检索到的信息进行处理和增强，以便生成模型可以更好地理解和使用。\",\"生成阶段: 将增强后的信息输入到生成模型中，生成模型根据这些信息生成答案。\"]},\"830\":{\"h\":\"RAG VS Finetune\",\"t\":[\"在提升大语言模型效果中，RAG 和 微调（Finetune）是两种主流的方法。\",\"微调: 通过在特定数据集上进一步训练大语言模型，来提升模型在特定任务上的表现。\",\"RAG 和 微调的对比可以参考下表 :\",\"特征比较\",\"RAG\",\"微调\",\"知识更新\",\"直接更新检索知识库，无需重新训练。信息更新成本低，适合动态变化的数据。\",\"通常需要重新训练来保持知识和数据的更新。更新成本高，适合相对稳定的数据。\",\"数据处理\",\"对数据的处理和操作要求极低。\",\"依赖于构建高质量的数据集，有限的数据集可能无法显著提高性能。\",\"模型定制\",\"侧重于信息检索和融合外部知识，但可能无法充分定制模型行为或写作风格。\",\"可以根据特定风格或术语调整 LLM 行为、写作风格或特定领域知识。\",\"可解释性\",\"可以追溯到具体的数据来源，有较好的可解释性和可追踪性。\",\"黑盒子，可解释性相对较低。\",\"特征比较\",\"RAG\",\"微调\",\"计算资源\",\"需要额外的资源来支持检索机制和数据库的维护。\",\"依赖高质量的训练数\",\"推理延迟\",\"增加了检索步骤的耗时\",\"单纯 LLM 生成的耗时\",\"降低幻觉\",\"通过检索到的真实信息生成回答，降低了产生幻觉的概率。\",\"模型学习特定领域的数据有助于减少幻觉，但面对未见过的输入时仍可能出现幻觉。\",\"伦理隐私\",\"检索和使用外部数据可能引发伦理和隐私方面的问题。\",\"训练数据中的敏感信息需要妥善处理，以防泄露。\"]},\"831\":{\"h\":\"LangChain\",\"t\":[\"ChatGPT 的巨大成功激发了越来越多的开发者兴趣，他们希望利用 OpenAI 提供的 API 或者私有化模型，来开发基于大型语言模型的应用程序。尽管大型语言模型的调用相对简单，但要创建完整的应用程序，仍然需要大量的定制开发工作，包括 API 集成、互动逻辑、数据存储等等。\",\"为了解决这个问题，从 2022 年开始，许多机构和个人相继推出了多个开源项目，旨在帮助开发者们快速构建基于大型语言模型的端到端应用程序或工作流程。其中一个备受关注的项目就是 LangChain 框架。\",\"LangChain 框架是一个开源工具，充分利用了大型语言模型的强大能力，以便开发各种下游应用。它的目标是为各种大型语言模型应用提供通用接口，从而简化应用程序的开发流程。具体来说，LangChain 框架可以实现数据感知和环境互动，也就是说，它能够让语言模型与其他数据来源连接，并且允许语言模型与其所处的环境进行互动。\",\"利用 LangChain 框架，我们可以轻松地构建如下所示的 RAG 应用。在下图中，每个椭圆形代表了 LangChain 的一个模块，例如数据收集模块或预处理模块。每个矩形代表了一个数据状态，例如原始数据或预处理后的数据。箭头表示数据流的方向，从一个模块流向另一个模块。在每一步中，LangChain 都可以提供对应的解决方案，帮助我们处理各种任务。\"]},\"832\":{\"h\":\"核心组件\",\"t\":[\"LangChian 作为一个大语言模型开发框架，可以将 LLM 模型（对话模型、embedding 模型等）、向量数据库、交互层 Prompt、外部知识、外部代理工具整合到一起，进而可以自由构建 LLM 应用。 LangChain 主要由以下 6 个核心组件组成:\",\"模型输入/输出（Model I/O）：与语言模型交互的接口\",\"数据连接（Data connection）：与特定应用程序的数据进行交互的接口\",\"链（Chains）：将组件组合实现端到端应用。比如后续我们会将搭建检索问答链来完成检索问答。\",\"记忆（Memory）：用于链的多次运行之间持久化应用程序状态；\",\"代理（Agents）：扩展模型的推理能力。用于复杂的应用的调用序列；\",\"回调（Callbacks）：扩展模型的推理能力。用于复杂的应用的调用序列；\",\"在开发过程中，我们可以根据自身需求灵活地进行组合。\"]},\"833\":{\"h\":\"版本迭代\",\"t\":[\"在 LLM 技术领域的迅猛发展浪潮中，LangChain 作为一个不断进化的创新平台，持续推动着技术边界的拓展。2024 年 9 月 16 日，LangChain 正式发布了其稳定版本 v0.3，这一里程碑式的更新，为开发者带来了全面而强大的功能支持。其涵盖了模型的输入与输出处理、数据连接、链式操作、记忆机制、代理服务以及回调处理等关键组件，为 LLM 应用的开发和部署提供了坚实的基础。 同时，LangChain 的持续优化和功能迭代，未来将带来更多创新特性和性能提升。\",\"兼容性与支持：LangChain 兼顾了对 Python 和 JavaScript 的支持，同时保持了向后兼容性，确保开发者能够在升级过程中无缝过渡，享受到更加安全稳定的开发体验。\",\"架构改进：通过将核心组件 langchain-core 与合作伙伴包进行有效分离，LangChain 的架构设计变得更加条理清晰和稳固，为未来的系统化扩展和安全性提升奠定了坚实基础。\",\"可观察性：LangChain 通过与 LangSmith 的深度集成，提供了业界领先的调试和观测功能。这使得开发者能够对 LLM 应用中的每一步操作及其输入输出有一个清晰的认识，极大地简化了调试和问题排查的流程。\",\"广泛的集成：LangChain 拥有近 700 个集成，覆盖了从 LLM 到向量存储、工具和智能体（Agent）等多个技术领域，极大地降低了在各种技术栈上构建 LLM 应用的复杂度。\",\"可组合性：借助 LangChain 表达式语言（LCEL），开发者可以轻松地构建和定制 chain，充分利用数据编排框架的优势，包括批量处理、并行化操作和备选方案等高级功能。\",\"流式处理：LangChain 对流式处理进行了深度优化，确保所有利用 LCEL 创建的 chain 均能支持流式处理，包括中间步骤的数据流传输，从而为用户提供更加流畅的体验。\",\"输出解析：LangChain 提供了一系列强大的输出解析工具，确保 LLM 能够以结构化的格式返回信息，这对于 LLM 执行具体行动计划至关重要。\",\"检索能力：LangChain 引入了先进的检索技术，适用于生产环境，包括文本分割、检索机制和索引管道等，使得开发者能够轻松地将私有数据与 LLM 的能力相结合。\",\"工具使用与智能体：LangChain 提供了丰富的智能体和工具集合，并提供了定义工具的简便方法，支持智能体工作负载，包括让 LLM 调用函数或工具，以及如何高效地进行多次调用和推理，极大地提升了开发效率和应用性能。\"]},\"834\":{\"h\":\"生态圈\",\"t\":[\"LangChain Community: 专注于第三方集成，极大地丰富了 LangChain 的生态系统，使得开发者可以更容易地构建复杂和强大的应用程序，同时也促进了社区的合作和共享。\",\"LangChain Core: LangChain 框架的核心库、核心组件，提供了基础抽象和 LangChain 表达式语言（LCEL），提供基础架构和工具，用于构建、运行和与 LLM 交互的应用程序，为 LangChain 应用程序的开发提供了坚实的基础。我们后续会用到的处理文档、格式化 prompt、输出解析等都来自这个库。\",\"LangChain CLI: 命令行工具，使开发者能够通过终端与 LangChain 框架交互，执行项目初始化、测试、部署等任务。提高开发效率，让开发者能够通过简单的命令来管理整个应用程序的生命周期。\",\"LangServe: 部署服务，用于将 LangChain 应用程序部署到云端，提供可扩展、高可用的托管解决方案，并带有监控和日志功能。简化部署流程，让开发者可以专注于应用程序的开发，而不必担心底层的基础设施和运维工作。\",\"LangSmith: 开发者平台，专注于 LangChain 应用程序的开发、调试和测试，提供可视化界面和性能分析工具，旨在帮助开发者提高应用程序的质量，确保它们在部署前达到预期的性能和稳定性标准。\"]},\"835\":{\"h\":\"大模型开发\",\"t\":[\"我们将开发以大语言模型为功能核心、通过大语言模型的强大理解能力和生成能力、结合特殊的数据或业务逻辑来提供独特功能的应用称为大模型开发。开发大模型相关应用，其技术核心点虽然在大语言模型上，但一般通过调用 API 或开源模型来实现核心的理解与生成，通过 Prompt Enginnering 来实现大语言模型的控制，因此，虽然大模型是深度学习领域的集大成之作，大模型开发却更多是一个工程问题。\",\"在大模型开发中，我们一般不会去大幅度改动模型，而是将大模型作为一个调用工具，通过 Prompt Engineering、数据工程、业务逻辑分解等手段来充分发挥大模型能力，适配应用任务，而不会将精力聚焦在优化模型本身上。因此，作为大模型开发的初学者，我们并不需要深研大模型内部原理，而更需要掌握使用大模型的实践技巧。\",\"# 大语言模型 ## Prompt Engineering ## 数据工程 ## 业务逻辑分解 ## 验证迭代优化\",\"同时，以调用、发挥大模型为核心的大模型开发与传统的 AI 开发在整体思路上有着较大的不同。大语言模型的两个核心能力：指令遵循与文本生成提供了复杂业务逻辑的简单平替方案。\",\"传统的 AI 开发：首先需要将非常复杂的业务逻辑依次拆解，对于每一个子业务构造训练数据与验证数据，对于每一个子业务训练优化模型，最后形成完整的模型链路来解决整个业务逻辑。\",\"大模型开发：用 Prompt Engineering 来替代子模型的训练调优，通过 Prompt 链路组合来实现业务逻辑，用一个通用大模型 + 若干业务 Prompt 来解决任务，从而将传统的模型训练调优转变成了更简单、轻松、低成本的 Prompt 设计调优。\",\"同时，在评估思路上，大模型开发与传统 AI 开发也有质的差异。\",\"传统 AI 开发：需要首先构造训练集、测试集、验证集，通过在训练集上训练模型、在测试集上调优模型、在验证集上最终验证模型效果来实现性能的评估。\",\"大模型开发：流程更为灵活和敏捷。从实际业务需求出发构造小批量验证集，设计合理 Prompt 来满足验证集效果。然后，将不断从业务逻辑中收集当下 Prompt 的 Bad Case，并将 Bad Case 加入到验证集中，针对性优化 Prompt，最后实现较好的泛化效果。\"]},\"836\":{\"h\":\"基本流程\",\"t\":[\"结合上述分析，我们一般可以将大模型开发分解为以下几个流程：\",\"确定目标: 在进行开发前，我们首先需要确定开发的目标，即要开发的应用的应用场景、目标人群、核心价值。对于个体开发者或小型开发团队而言，一般应先设定最小化目标，从构建一个 MVP（最小可行性产品）开始，逐步进行完善和优化。\",\"设计功能: 在确定开发目标后，需要设计本应用所要提供的功能，以及每一个功能的大体实现逻辑。虽然我们通过使用大模型来简化了业务逻辑的拆解，但是越清晰、深入的业务逻辑理解往往也能带来更好的 Prompt 效果。同样，对于个体开发者或小型开发团队来说，首先要确定应用的核心功能，然后延展设计核心功能的上下游功能；\",\"搭建整体架构: 目前，绝大部分大模型应用都是采用的特定数据库 + Prompt + 通用大模型的架构。我们需要针对我们所设计的功能，搭建项目的整体架构，实现从用户输入到应用输出的全流程贯通。一般来说，我们推荐基于 LangChain 框架进行开发。LangChain 提供了 Chain、Tool 等架构的实现，我们可以基于 LangChain 进行个性化定制，实现从用户输入到数据库再到大模型最后输出的整体架构连接。\",\"搭建数据库: 个性化大模型应用需要有个性化数据库进行支撑。由于大模型应用需要进行向量语义检索，一般使用诸如 Chroma 的向量数据库。在该步骤中，我们需要收集数据并进行预处理，再向量化存储到数据库中。数据预处理一般包括从多种格式向纯文本的转化，例如 PDF、MarkDown、HTML、音视频等，以及对错误数据、异常数据、脏数据进行清洗。完成预处理后，需要进行切片、向量化构建出个性化数据库。\",\"Prompt Engineering: 优质的 Prompt 对大模型能力具有极大影响，我们需要逐步迭代构建优质的 Prompt Engineering 来提升应用性能。在该步中，我们首先应该明确 Prompt 设计的一般原则及技巧，构建出一个来源于实际业务的小型验证集，基于小型验证集设计满足基本要求、具备基本能力的 Prompt。\",\"验证迭代: 验证迭代在大模型开发中是极其重要的一步，一般指通过不断发现 Bad Case 并针对性改进 Prompt Engineering 来提升系统效果、应对边界情况。在完成上一步的初始化 Prompt 设计后，我们应该进行实际业务测试，探讨边界情况，找到 Bad Case，并针对性分析 Prompt 存在的问题，从而不断迭代优化，直到达到一个较为稳定、可以基本实现目标的 Prompt 版本。\",\"前后端搭建: 完成 Prompt Engineering 及其迭代优化之后，我们就完成了应用的核心功能，可以充分发挥大语言模型的强大能力。接下来我们需要搭建前后端，设计产品页面，让我们的应用能够上线成为产品。\",\"体验优化: 在完成前后端搭建之后，应用就可以上线体验了。接下来就需要进行长期的用户体验跟踪，记录 Bad Case 与用户负反馈，再针对性进行优化即可。\"]},\"837\":{\"h\":\"参考\",\"t\":[\"LLM 部分:\",\"A Survey of Large Language Models\",\"周枫：当我们谈论大模型时，应该关注哪些新能力？\",\"S型智能增长曲线：从Deepseek R1看Scaling Law的未来\",\"一文详尽之Scaling Law！\",\"QwQ: 思忖未知之界\",\"QwQ-32B: 领略强化学习之力\",\"RAG 部分:\",\"Retrieval-Augmented Generation for Large Language Models: A Survey\",\"面向大语言模型的检索增强生成技术：综述\"]},\"838\":{\"h\":\"2.大模型API使用\",\"t\":[\"智慧化知识库系统: LLM API的调用。\"]},\"839\":{\"h\":\"智慧化知识库系统\"},\"840\":{\"h\":\"AI Agent\"},\"841\":{\"h\":\"在线学习\"},\"842\":{\"h\":\"强化学习\"},\"843\":{\"h\":\"概率论基础知识\"},\"844\":{\"h\":\"概率论基础概念\",\"t\":[\"概率论基础概念(用到多少，学多少 =_=)\"]},\"845\":{\"h\":\"概率空间\",\"t\":[\"我们将概率空间定义为三元组 ，其中：\",\" 是样本空间，表示实验中所有可能的结果组成的集合；\",\" 是事件空间，即 的所有子集的集合；\",\" 是概率度量，是一个从事件 到 区间数值的映射（即 ），满足某些一致性要求。\"]},\"846\":{\"h\":\"离散随机变量\",\"t\":[\"最简单的情况是实验的结果是可数的。例如，掷一个三面骰子，其三个面分别标记为 “A”、“B”、“C”（为了简洁，我们用3面而不是6面）。此时：\",\"样本空间为 ，表示所有可能的实验结果；\",\"事件空间为 。\",\"其中每一个事件就是事件空间中的一个元素。例如：\",\"事件 表示骰子掷出面为 A 或 B；\",\"事件 表示骰子掷出面为 C。\",\"定义事件空间后，需要指定概率度量 ，即为事件空间中的每个集合赋予一个“权重”或“大小”。例如，设：\",\"，\",\"，\",\"。\",\"则复合事件的概率可通过求和得到，例如：\",\"。\",\"为简化记号，我们可以将每个样本空间中的结果映射为一个实数，这就定义了随机变量（random variable，记作 rv）：\",\"随机变量 = 一个把“事件结果”映射为“数值”的函数，它本身不随机，随机的是它作用的输入（样本 ）。\",\"，将每个结果 映射为实数 。\",\"例如，对三面骰子设：\",\"，\",\"，\",\"。\",\"再如，掷两次公平硬币，样本空间为：\",\"。\",\"设随机变量 表示“正面出现次数”，则：\",\"，\",\"，\",\"，\",\"。\",\"我们将随机变量可能的取值集合称为其状态空间，记作 。给定某个状态 ，定义：\",\"其中 ，称为 的原像。\",\"你有一个随机变量 ，它是一个函数，从样本空间 映射到实数；给定某个输出值 ，我们关心的是：随机变量等于这个值的概率是多少，即 。 但是：随机变量是函数，它本身不“随机”，真正随机的是实验结果 。所以，要知道“”的概率是多少，其实等价于问：\",\"有多少个 会导致 ，而这些 的总概率是多少？\",\"所以，我们这么定义：\",\"：是所有让 成立的样本点集合（这就是“原像”）；\",\"然后，：就是计算这些 的总概率。\",\"实验：投两次硬币\",\"定义随机变量 ：表示正面（H）的次数\",\"现在我们问：\",\"这等价于找出：\",\"哪些 会导致 ？\",\"答案是 \",\"如果每个 的概率都是 ，那么：\",\"这里， 称为概率质量函数（pmf，probability mass function）。继续上述例子，掷两次硬币的 pmf 为：\",\"，\",\"，\",\"。\",\"pmf 可用柱状图表示，也可用参数化函数表示。我们称 为随机变量 的概率分布。在上下文明确的情况下，常省略下标 。\",\"随机变量 把世界事件映射成数字；pmf 把这些数字映射成它们发生的概率。\"]},\"847\":{\"h\":\"连续随机变量\",\"t\":[\"我们也可以考虑结果为连续值的实验。这种情况下，假设样本空间是实数集合的子集：，并定义随机变量为恒等函数 。\",\"例如，测量某事件持续时间（单位：秒），设：\",\"。\",\"由于该集合是不可数的，无法像离散情形那样枚举所有子集。因此，我们需要借助Borel σ-代数（Borel sigma-field）来定义事件空间。其定义如下：\",\"集合 是一个 σ-代数（sigma-field）当且仅当：\",\"，且 ；\",\"若 ，则其补集 ；\",\"若 ，则 与 也属于 。\",\"Borel σ-代数是由半开区间 生成的最小 σ-代数。通过这些区间的并、交和补运算，我们可以得到：\",\"当我们讨论连续型随机变量（比如测量一个时间、距离或温度）时，它的样本空间是连续的，比如：\",\"在这种连续的空间里，所有可能的“事件”不是像离散情况那样简单地枚举出来的（比如 ），而可能是“无限多种可能的区间组合”。 比如我们可能想表示这些事件：\",\"“温度在 1 到 2 度之间” → 区间 (1, 2)\",\"“时间小于 5 秒” → 区间 \",\"“温度是 3 度或 7 度” → \",\"“测量值是无理数” → 这也算是一类事件！\",\"但是问题是：我们不能对“所有”这样的集合都定义概率！ 因为某些集合太“奇怪”或太“复杂”，会导致概率的定义出现矛盾或不收敛。 所以我们需要一个规则体系来规定“我们只对哪些集合定义概率” ——> 这个规则体系就是σ-代数（sigma-field）。\",\"σ-代数是一个集合的集合（简单理解：是你允许讨论的事件的全集合），它必须满足以下三条规则（你可以把它们理解成“合理事件空间”的要求）：\",\"包含整个样本空间和空集\",\"你总得允许“什么都不发生”（空事件）\",\"也得允许“一定会发生”（整个样本空间）\",\"如果你能谈某个事件，那它的补集你也得能谈\",\"比如：“温度小于 30 度” 这个事件存在，那“温度不小于 30 度”这个事件也应该存在\",\"如果你能谈一堆事件，那它们的并集和交集也得能谈\",\"比如你能谈“温度在 (0,1)”、“温度在 (1,2)”……，那“温度在 (0,2)”这种组合你也得能谈\",\"换句话说：σ-代数就是一种封闭的事件系统，允许你用基本事件构造更复杂事件，但不会跑出系统之外。 Borel σ-代数是专门为实数空间（）设计的一种 σ-代数，用来处理实数范围内的“正常”区间事件。 它的定义是：Borel σ-代数是由所有形如 的区间生成的最小 σ-代数。 也就是说，它从一些基本的“区间事件”出发，通过反复地做并集、交集、补集操作，构造出你所需要的所有“常见事件”。 比如：\",\"：开区间\",\"：闭区间\",\"、：半开区间\",\"：单点集\",\"任意有限/可数个区间并集交集……\",\"这些都属于 Borel σ-代数。 你可以把它理解成：我们定义概率，只在这些“结构正常的区间组合”上做，不碰那些太反直觉或病态的集合。 总结: Borel σ-代数是一种你可以安全地讨论概率的“事件集合体系”，它由一些基本区间（比如 ）出发，闭包得到所有“常规可测的集合”。\",\"在持续时间的例子中，可进一步限制事件空间只包含区间 ，其中 。\",\"为定义概率度量，我们为每个 赋一个非负权重 ，称为概率密度函数（pdf，probability density function）。\",\"对于事件 ，概率由积分给出：\",\"还可以定义累积分布函数（cdf）：\",\"从中可以计算区间概率：\",\"“概率分布”一词既可以指 pdf ，也可以指 cdf ，甚至指概率度量 本身。\",\"上述定义也可推广到多维空间 ，以及函数等更复杂的样本空间。\"]},\"848\":{\"h\":\"概率公理\",\"t\":[\"与事件空间相关联的概率规律，必须遵循概率公理（Kolmogorov 公理），具体如下：\",\"非负性（Non-negativity）：\",\"对任意事件 ，有\",\"规范性（Normalization）：\",\"整个样本空间的概率为 1：\",\"可加性（Additivity）：\",\"对于任意一列两两互不相交（即互斥）的事件 ，有\",\"在有限的情况下，比如只有两个互斥事件 和 ，上述公式简化为：\",\"这个公式对应的是“事件 或 发生”的概率（前提是这两个事件是互斥的）。\",\"从这些公理可以推导出一些常用结论：\",\"补集规则（Complement Rule）：\",\"其中， 表示事件 的补集。\",\"这个结论来自于：\",\"其他可推出的结论：\",\"：可通过反证法证明\",\"：可由补集规则推出，当 时，，所以 \",\"加法规则（Addition Rule）：\",\"对于任意两个事件（不要求互斥），有：\",\"这个公式适用于任意两个事件，即使它们可能有重叠。\"]},\"849\":{\"h\":\"条件概率\",\"t\":[\"考虑两个事件 和 。如果 ，则定义事件 在 已经发生的条件下的条件概率为：\",\"根据这个定义，可以得到乘法法则（multiplication rule）：\",\"条件概率衡量的是：在事件 已经发生的前提下，事件 发生的可能性有多大。\",\"然而，如果两个事件是无关的，那么一个事件的发生不会改变另一个事件的概率。更形式化地说，若满足以下条件，则称 与 是独立事件（independent events）：\",\"若 且 ，上式等价于：\",\"，或\",\"同理，若在某个事件 已知的条件下，事件 与 满足下式：\",\"则称 和 在给定 的条件下是条件独立的（conditionally independent）。\"]},\"850\":{\"h\":\"全概率公式（Law of total probability）\",\"t\":[\"根据条件概率的定义，还可以推导出全概率公式：\",\"若集合 构成样本空间 的一个划分（partition），那么对于任意事件 ，有：\",\"全概率公式: 一个事件总体概率 = 在不同情形下它发生的概率 × 各种情形本身的概率，加起来。 假设我们有一个检测疾病的筛查工具，我们要问：\",\"一个人检测为阳性的总体概率是多少？\",\"我们知道：\",\"人群中 1% 有病（记作 ），99% 无病（记作 ）；\",\"如果有病（），检测为阳性（）的概率是 0.9（即 ）；\",\"如果没病（），误报为阳性概率是 0.05（即 ）；\",\"问：一个人检测阳性的总体概率是多少？ 我们把“人是否患病”作为划分事件：\",\"：患病，\",\"：未患病，\",\"我们要算的事件是“检测为阳性”（）：\",\"所以，虽然你可能觉得“阳性概率应该很高”，但实际上总体阳性概率只有 5.85%，因为大多数人根本没病，而没病的人也有误报。\"]},\"851\":{\"h\":\"贝叶斯法则\",\"t\":[\"根据条件概率的定义，可以推导出贝叶斯法则（Bayes’ rule），也称为贝叶斯定理（Bayes’ theorem）。对于任意两个满足 且 的事件 和 ，有：\"]},\"852\":{\"h\":\"离散随机变量形式\",\"t\":[\"对于一个具有 个可能取值的离散随机变量 ，结合全概率公式，贝叶斯法则可以写为：\",\"其中：\",\"：称为先验概率（prior probability）\",\"：称为似然（likelihood）\",\"：称为后验概率（posterior probability）\",\"：为归一化常数，也叫作边缘似然（marginal likelihood）\"]},\"853\":{\"h\":\"连续随机变量形式\",\"t\":[\"对于一个连续型随机变量 ，贝叶斯法则写作：\",\"这就是贝叶斯法则在离散和连续两种情形下的表达方式，它提供了一种根据观测数据（事件 ）来更新我们对未知变量 的信念的机制，是整个贝叶斯推断的核心。\"]},\"854\":{\"h\":\"一些常见的概率分布\",\"t\":[\"在构建各种类型的模型时，我们会用到多种概率分布。以下小节总结了其中一些常见分布。\",\"交互式可视化网站： 🔗 https://ben18785.shinyapps.io/distribution-zoo/\"]},\"855\":{\"h\":\"离散分布\",\"t\":[\"本节讨论的是定义在（非负）整数子集上的一些离散型概率分布。\"]},\"856\":{\"h\":\"伯努利分布与二项分布（Bernoulli and Binomial distributions）\",\"t\":[\"设 ，二项分布（Binomial distribution）定义为：\",\"其中， 是从 个元素中选出 个的组合数（称为二项系数，读作 “N 选 x”）。\",\"如果 ，即 ，则二项分布退化为伯努利分布（Bernoulli distribution）：\",\"其中， 是分布的均值。\"]},\"857\":{\"h\":\"分类分布与多项分布（Categorical and Multinomial distributions）\",\"t\":[\"如果变量是多值离散型的（例如 ），我们可以使用分类分布（categorical distribution）：\",\"其中 表示选择类别 的概率， 是指示函数，表示当 时取 1，否则取 0。\",\"或者，也可以将 K 类变量 表示成一个 独热编码（one-hot）向量，这时分类分布可以写为：\",\"其中 表示当前样本属于第 类，其它元素为 0。\",\"如果 表示类别 在总共 次试验中出现的次数，那么就得到了多项分布（multinomial distribution）：\",\"其中，多项系数（multinomial coefficient）定义为：\"]},\"858\":{\"h\":\"泊松分布（Poisson distribution）\",\"t\":[\"设随机变量 。若 服从参数为 的泊松分布，记作 ，则其概率质量函数（pmf）为：\",\"其中， 是该分布的均值，同时也是方差，即：\"]},\"859\":{\"h\":\"负二项分布（Negative binomial distribution）\",\"t\":[\"假设我们有一个“盒子”（或称“容器”）中有 个球，其中：\",\" 个是红球\",\" 个是蓝球\",\"我们进行有放回抽样，直到抽出 个球。设 表示这 个球中蓝球的数量。可以证明：\",\"即： 服从二项分布。\"]},\"860\":{\"h\":\"转换视角：定义失败为红球，成功为蓝球\",\"t\":[\"现在我们重新定义抽红球为“失败”、抽蓝球为“成功”。我们继续抽球，直到观察到 次失败（红球）为止。\",\"设 表示在这过程中抽到的“成功”次数（即蓝球个数）。可以证明：\",\"也就是说， 服从 负二项分布（Negative binomial distribution），其概率质量函数定义为：\",\"其中 ，表示成功的次数。\",\"组合数 这表示从 次试验中选出 次成功的位置，剩下的是失败。 这里试验顺序重要，且第 次失败必须是第 次试验的结果（最后一次失败）。\",\"二项分布关注试验次数固定，成功次数随机。\",\"负二项分布关注成功次数固定，试验次数（失败次数）随机。\"]},\"861\":{\"h\":\"特殊情况说明\",\"t\":[\"如果 是实数，我们将组合数 替换为伽马函数表达式：\",\"利用了恒等式 。\"]},\"862\":{\"h\":\"数学期望与方差\",\"t\":[\"负二项分布的两个矩（均值和方差）为：\"]},\"863\":{\"h\":\"负二项分布的意义与优势\",\"t\":[\"这种含有两个参数的分布比泊松分布更具有建模灵活性，因为它可以单独控制均值与方差。这在模拟某些“传染性事件”时非常有用，例如某些事件之间是正相关的，它们的出现会导致比独立情形更大的方差。\",\"事实上，泊松分布是负二项分布的一个特例。可以证明：\",\"另一个特例是当 时，负二项分布变为几何分布（Geometric distribution）。\"]},\"864\":{\"h\":\"定义在实数上的连续分布\",\"t\":[\"在本节中，我们讨论一些定义在实数集合 上的一元连续分布，即 且 。\"]},\"865\":{\"h\":\"高斯分布（正态分布）\",\"t\":[\"最广泛使用的一元分布是高斯分布（Gaussian distribution），也叫正态分布（normal distribution）。\",\"高斯分布的概率密度函数（pdf）定义为：\",\"其中， 是归一化常数，用于确保整个密度函数的积分为 1。\",\"参数 表示分布的均值（mean），也是该分布的众数（mode）。\",\"参数 表示分布的方差（variance）。\",\"有时我们也会讨论高斯分布的精度（precision），即方差的倒数：。\",\"精度越高意味着分布越“窄”（即方差小），集中在 附近。\",\"高斯分布的累积分布函数（cdf）定义为：\",\"如果 、（即所谓的标准正态分布），我们简写为 。\"]},\"866\":{\"h\":\"半正态分布（Half-normal）\",\"t\":[\"在某些问题中，我们希望使用定义在非负实数上的分布。一种构造这类分布的方法是设定：\",\"由此诱导出的 的分布被称为半正态分布（half-normal distribution），其概率密度函数为：\",\"这个分布可以被看作是标准正态分布 在 0 处对折（folded over）后的结果。\"]},\"867\":{\"h\":\"学生 t 分布（Student t-distribution）\",\"t\":[\"高斯分布的一个问题在于它对离群点非常敏感，因为其概率密度随着与中心的平方距离增大而指数级衰减。一种更具鲁棒性的分布是 学生 t 分布（Student t-distribution），我们简称为Student 分布。它的概率密度函数（pdf）如下：\",\"其中：\",\" 是均值，\",\" 是尺度参数（注意：不是标准差），\",\" 被称为自由度（不过一个更恰当的术语可能是“正态程度” [Kru13]，因为当 趋于较大值时，该分布会表现得像高斯分布）。\",\"归一化常数 的表达式为：\",\"这里：\",\" 是Gamma 函数，定义为：\",\" 是Beta 函数，定义为：\"]},\"868\":{\"h\":\"柯西分布（Cauchy distribution）\",\"t\":[\"当 时，Student 分布被称为 柯西分布（Cauchy distribution） 或 洛伦兹分布（Lorentz distribution）。它的概率密度函数（pdf）定义为：\",\"其中：\",\"，即归一化常数。\",\"这个分布的一个显著特点是：它的尾部非常厚重（heavy tails），以至于定义均值的积分并不收敛（即没有期望值）。\",\"半柯西分布（half Cauchy distribution） 是一种基于均值为 0 的柯西分布进行“折叠”的版本，也就是说，它的概率密度函数全部集中在正实数轴上。\",\"因此，其形式为：\",\"更多分布使用到的时候再进行补充\"]},\"869\":{\"h\":\"高斯联合分布（Gaussian joint distributions）\",\"t\":[\"对于连续型随机变量，使用最广泛的联合概率分布是多元高斯分布（multivariate Gaussian 或 multivariate normal，简称 MVN）。\",\"这种分布之所以受欢迎，一方面是因为其数学处理非常方便，另一方面在许多实际问题中，高斯分布作为近似是相当合理的。事实上，在给定均值和协方差矩的约束下，高斯分布是熵最大的分布。鉴于它的重要性，本节将详细讨论高斯分布。\"]},\"870\":{\"h\":\"多元正态分布（The multivariate normal）\",\"t\":[\"在本节中，我们将详细介绍多元高斯分布，又称多元正态分布（MVN）。\"]},\"871\":{\"h\":\"定义（Definition）\",\"t\":[\"多元高斯分布的概率密度函数定义如下：\",\"其中：\",\" 是均值向量，\",\" 是 的协方差矩阵，\",\"归一化常数 保证整个 pdf 的积分为 1。\",\"指数中的表达式（忽略系数 -0.5）是数据向量 相对于均值 的马氏距离（Mahalanobis distance）平方，定义如下：\",\"在二维空间中，多元高斯分布被称为二维高斯分布（bivariate Gaussian distribution）。其概率密度函数可表示为：\",\"协方差矩阵形式为：\",\"其中：\",\" 是相关系数，定义为：\",\"图 2.8 展示了三种不同协方差矩阵下的二维多元高斯密度图：\",\"完全协方差矩阵（Full covariance matrix）: 有 个自由参数（由于 是对称的，所以除以 2）。\",\"对角协方差矩阵（Diagonal covariance matrix）: 只有 D 个自由参数，非对角线元素为 0，表示变量之间不相关。\",\"球形协方差矩阵（Spherical covariance matrix）: 也称为各向同性协方差矩阵（isotropic covariance matrix），只有一个自由参数 ，表示所有方向的方差相同。\",\"只有一个自由参数 ，表示所有方向的方差相同。\",\"当然可以，以下是你提供内容的逐段翻译与解释，保持原意清晰、结构一致：\"]},\"872\":{\"h\":\"高斯壳（Gaussian shells）\",\"t\":[\"在高维空间中，多元高斯分布的行为可能会显得非常反直觉。我们可以提出这样一个问题：\",\"如果我们从 中采样，其中 是维度数，我们应该预期这些样本大多会落在空间的哪里？\",\"由于概率密度函数的峰值（众数）位于原点 ，我们直觉上会认为：大多数样本应该靠近原点。\",\"然而，在高维空间中，高斯分布的“典型集合（typical set）”实际上是一个很薄的壳层或环带，其：\",\"与原点的距离为：\",\"壳层的厚度为：\"]},\"873\":{\"h\":\"直观解释如下：\",\"t\":[\"虽然密度函数以 的形式衰减 —— 即离原点越远，密度越小，但与此同时：\",\"球体的体积随半径 按 的速率快速增长；\",\"因为概率质量 = 密度 × 体积；\",\"所以，两者之间会出现一种“相互抵消的平衡点” —— 也就是在某个距离范围内，虽然密度在下降，但体积增加更快；\",\"大多数样本会集中在这个区域上 —— 也就是所谓的“高斯肥皂泡现象（Gaussian soap bubble phenomenon）”。\"]},\"874\":{\"h\":\"数学解释：为什么高斯样本集中在壳层上？\",\"t\":[\"考虑某个点 到原点的平方距离：\",\"期望平方距离为：\",\"方差为：\",\"相对标准差（变异系数）为：\",\"这意味着：\",\"尽管每个样本的距离是随机的；\",\"但当维度 越大时，它们的平方距离越来越集中在 附近；\",\"从而，距离本身越来越集中在 附近。\",\"这就是为什么我们说样本会集中在距离原点约为 的薄壳层上。\"]},\"875\":{\"h\":\"图像空间的含义（例如灰度图像）\",\"t\":[\"图 2.9b 展示了一些从如下高斯分布中采样的灰度图像：\",\"其中 是一张“全灰”的图片（每个像素亮度相同）。\",\"你可能以为既然是围绕全灰图采样，那么采样出来的图像应该也接近灰色。但事实恰恰相反：\",\"在高维图像空间中，几乎不可能采样到接近灰色的图像。\",\"这是因为样本几乎全部落在离 一定距离的“典型壳层”上，而不是密度最大的中心点（即灰色图像）。这非常反直觉，但却是高维高斯的真实现象。\"]},\"876\":{\"h\":\"概率论基础模型\",\"t\":[\"概率论基础模型(用到多少，学多少 =_=)\"]},\"877\":{\"h\":\"Bayes' rule\",\"t\":[\"Question One: What's the mean of Bayesian inference ?\",\"“推理”（inference）是指“从样本数据出发，得出带有一定置信度的一般性结论的行为”。术语“贝叶斯”（Bayesian）则用来指代那些使用概率理论来表示“置信度”（即确定程度）并利用 贝叶斯公式(Bayes’ rule) 根据观察数据更新置信度的方法。\",\"贝叶斯公式本身非常简单：它是一个用于计算在给定观测数据 情况下，某个未知（或隐藏）变量 可能取值的概率分布的公式：\",\"这个公式可以由以下恒等式直接推出：\",\"而这个恒等式又来自于概率的乘法法则（product rule）。\",\"在公式 (2.51) 中，术语 表示在我们看到任何数据之前，对 的可能取值的了解；这被称为先验分布（prior distribution）。如果 有 个可能的取值，那么 就是一个包含 个元素的向量，其中的概率和为 1。\",\"术语 表示在假设 的前提下，我们对可能出现的结果 的分布，这被称为观测分布（observation distribution）。当我们将其评估于实际观测结果 上时，就得到了函数 ，这被称为似然函数（likelihood）。需要注意的是，这其实是 的函数，因为 是已知的固定值，并且它不是一个概率分布，因为它的和不一定为 1 。\",\"将先验概率 与似然函数 相乘，可以得到未归一化的联合分布。我们可以通过除以 将其变为归一化分布，这个除数被称为边际似然（marginal likelihood），因为它是通过对未知量 进行边际化（即求和）得到的：\",\"通过对每个 计算 ，我们就得到了后验分布（posterior distribution），它表示我们在看到数据 之后，对 可能取值的最新信念状态。\",\"我们可以用一句话来总结贝叶斯公式：\",\"这里使用符号 （“正比于”）表示我们省略了分母，因为它只是一个与 无关的常数。\",\"使用贝叶斯公式，根据观测数据对某一感兴趣的未知量的分布进行更新的过程，被称为贝叶斯推理（Bayesian inference）或后验推理（posterior inference），也可以简称为概率推理（probabilistic inference）。\",\"Bayes 公式人话版本: “先有预期 + 接收信息 → 更新判断”\",\"：隐藏的“真相”或假设\",\"：你观测到的信息\",\"：你在没有观察任何信息前对 H 的先验信念\",\"：如果 H 是真的，你会看到这个信息的可能性\",\"：你在看到 Y 后对 H 的新判断（后验）\"]},\"878\":{\"h\":\"Inverse problems\",\"t\":[\"概率论的核心是：在已知世界状态 的前提下，预测某个结果 的分布。而逆概率问题关注的则是：通过观察结果 ，去推断世界的状态。我们可以把这看作是对 映射关系的反向求解。\",\"举个例子，设想我们要从一张二维图像 中推断出一个三维形状 。这是视觉场景理解中的一个经典问题。不幸的是，这是一个根本上的病态问题（ill-posed problem），如图 2.8 所示：同一个观测结果 ，可能对应多个潜在的隐藏状态 。同样地，我们也可以将自然语言理解看作是一个病态问题：听者必须从说话者表达出的（通常是模糊的）语言中，去推测其真正的意图 。\",\"为了解决这类反向问题，我们可以使用贝叶斯公式来计算后验概率，它描述了在观测到 的情况下，对各种可能世界状态 的概率分布。\",\"要实现这一点，需要给出：\",\"前向模型：描述在给定 的前提下，结果 是如何产生的；\",\"先验分布：用于排除或降低某些不太可能的世界状态。\"]},\"879\":{\"h\":\"组合分析\",\"t\":[\"组合分析\"]},\"880\":{\"h\":\"计数法则\",\"t\":[\"一共有 r 个实验 ， 第一个实验有 n1 种可能结果; 对应于第一个实验的每一种实验结果，第二个实验有 n2 种可能结果； 对应于头两个实验的每一种实验结果，第三个实验有 n3 种可能结果； 等等，那么，这 r 个实验一共有 n1 * n2 * ... * nr 种可能结果。\"]},\"881\":{\"h\":\"排列(考虑元素之间的顺序)\",\"t\":[\"n 个不同的元素，按任意顺序进行排列，总的排列方式共有: n*(n-1)(n-2)...32*1 = n!\",\"可用计数法则进行理解。\",\"n 个元素，如果其中 n1 个元素彼此相同，另 n2 个彼此相同，... ，nr 个也彼此相同，那么一共有 种不同的排列方式。\",\"总排列数 = 异排列数 * 每种异排列对应的重复排列数 --> 异排列数 = 总排列数 / 每种异排列对应的重复排列数\",\"例: 用 PEPPER 的 6 个字母进行排列，考察其中任一排列方式: PEPPER ，固定P,E,R的相对顺序不变，对P,E,R单独进行重排，会产生: 3! * 2! * 1! 种重复排列，如下所示:\"]},\"882\":{\"h\":\"组合(不考虑元素之间的顺序)\",\"t\":[\"n 个不同的元素中取 r 个\",\"总排列数 = 异排列数 * 每种异排列对应的全排列数 --> 异排列数 = 总排列数 / 每种异排列对应的全排列数\"]},\"883\":{\"h\":\"DALL·E 论文解读\",\"t\":[\"DALL·E 论文解读\",\"论文链接: Zero-Shot Text-to-Image Generation 第三方代码实现: DALL-E\"]},\"884\":{\"h\":\"引言\",\"t\":[\"文本生成图像任务传统上是在固定的数据集上，借助复杂的模型架构、辅助损失函数或额外信息（如物体部分标签、分割掩码等）进行训练。虽然这些方法提升了生成效果，但生成图像中仍常见严重问题：\",\"现存问题：\",\"物体扭曲，形状不自然\",\"物体摆放不合逻辑，空间关系混乱\",\"前景与背景融合不自然，视觉体验差\",\"这些问题限制了模型在实际应用中的表现。 从2015年起，研究者尝试用不同生成模型改善文本到图像的转换。最初，Mansimov 等人用变分自编码器 DRAW 模型进行尝试；随后，Reed 等人用生成对抗网络提升了图像质量和泛化能力。此后，研究不断融合新的方法：\",\"多尺度生成器架构\",\"引入注意力机制和辅助损失\",\"利用额外条件信息，如除文本外的其他标签\",\"此外，基于能量模型的方法和优化预训练跨模态模型输入的方式也出现，进一步提升了样本质量。尽管如此，生成的图像依然存在明显的瑕疵。\",\"近期，大规模模型和海量数据的结合，尤其是自回归 Transformer 架构，已在文本、图像和音频领域取得显著成果。相比之下，文本生成图像的研究多基于较小数据集（如 MS-COCO、CUB-200），模型规模和数据量可能是性能瓶颈。本研究训练了一个参数量为 亿的自回归 Transformer，使用了从互联网收集的 亿图文对，结果表明：\",\"该模型能够灵活生成高质量图像，且通过自然语言实现控制\",\"在 MS-COCO 数据集的零样本测试中表现优异，不依赖任何训练标签\",\"人类评估显示，生成图像在 的情况下优于之前专门训练的模型\",\"模型具备一定的图像到图像转换能力，这一复杂任务以前通常需要专门设计的方案才能实现\",\"这表明，大规模统一建模方法具备强大的泛化与多任务能力。\"]},\"885\":{\"h\":\"方法\",\"t\":[\"我们的目标是训练一个 Transformer，以自回归的方式将文本和图像 token 作为单一的数据流进行建模。然而，若直接使用像素作为图像 token，对于高分辨率图像将需要大量的内存。似然（Likelihood）目标倾向于优先建模像素之间的短程依赖，因此，大量的模型容量会花在捕捉高频细节上，而非低频结构上——而低频结构才是使得物体在视觉上可被我们辨认的关键。\",\"为了解决这些问题，我们采用了类似的两阶段训练流程：\",\"阶段 1：我们训练一个离散变分自编码器（dVAE）¹，将每个 256×256 的 RGB 图像压缩成一个 32×32 的图像 token 网格，其中每个元素可以取 8192 种可能的值。这使得 Transformer 的上下文长度减少了 192 倍，而视觉质量没有显著下降（见图 1）。\",\"阶段 2：我们将最多 256 个 BPE 编码的文本 token 与 32×32 = 1024 个图像 token 拼接起来，训练一个自回归 Transformer 来建模文本和图像 token 的联合分布。\",\"整体流程可以被视作在最大化模型分布在图像 、标题 以及 RGB 图像编码 token 上的联合似然的证据下界（ELB）。我们对该分布建模的因式分解为：\",\"其下界为：\",\"其中：\",\" 表示由 dVAE 编码器在给定 RGB 图像 时生成的 32×32 图像 token 的分布；\",\" 表示由 dVAE 解码器在给定图像 token 时生成 RGB 图像的分布；\",\" 表示由 Transformer 建模的文本和图像 token 的联合分布。\",\"需要注意的是，该下界仅在 时成立，而在实践中我们发现使用更大的 值更有帮助。接下来的小节将更详细地介绍这两个阶段。\",\"关于上面部分公式的补充解读:\",\"：原始 RGB 图像\",\"：文本标题（caption）\",\"：由 dVAE 编码得到的图像 token（32×32 网格，每个位置是 8192 维离散值）\",\"这样一来，整个生成过程可以想成：\",\"先从 Transformer 的联合先验 中，生成一段文字 token 和对应的图像 token；\",\"再用 dVAE 解码器 把图像 token 变成像素级别的 RGB 图像。\",\"意思是： 图像和文本的联合概率，可以拆成 “根据文本和图像 token 生成像素图像” × “文本和图像 token 的联合概率”。\",\"这样分开好处是：\",\" 专门学好像素生成（dVAE 解码器）\",\" 专门学好文本与图像 token 的联合分布（Transformer）\",\"互不干扰，分工明确。\",\"ELBO 公式的含义\",\"因为 是离散变量，我们没法直接优化 ，只能最大化它的证据下界（ELBO）：\",\"第一项 : 就是重构项，要求 dVAE 解码出来的图像尽量接近原图。\",\"第二项 : 是正则项，要求编码器的输出分布 要接近先验分布 。 控制这项的权重（ 是标准 ELBO，实际中用更大值鼓励更均匀的码本利用）。\",\"两阶段如何对应这个公式\",\"阶段一（学 ）: 先只看图像部分，训练 dVAE\",\"：编码器把图像变成 token；\",\"：解码器重构图像；这一步不动 。\",\"阶段二（学 ）: 固定 dVAE 编码器/解码器，直接用编码后的 作为离散符号，与文本 token 一起训练 Transformer 学 。\",\"ELBO 公式推导过程\",\"这里：\",\"：dVAE 解码器\",\"：Transformer 先验\",\"我们希望得到：\",\"但由于 是潜变量，这个求和很难直接算。在变分推断中，我们会给 引入一个可计算的近似后验：\",\"然后套入 Jensen 不等式 （ 是凹函数，所以 ）：\",\"注意到：\",\"如果把 也一起看作潜变量的一部分（这里他们写作 ），就会得到文中那种 KL 形式：\",\"然后他们在 KL 前乘了一个经验选择的系数 （即 β-VAE 思想）。本质上就是：\",\"用变分后验 近似真实后验 ；\",\"代入 Jensen 不等式得到下界；\",\"分离出“重构项”和“KL 正则项”；\",\"在 KL 项前加权得到 β-ELBO。\"]},\"886\":{\"h\":\"阶段一：学习视觉码本（Visual Codebook）\",\"t\":[\"在第一阶段训练中，我们最大化关于 和 的 ELB，这相当于在图像上单独训练一个 dVAE。我们将初始先验 设为对 个码本向量的均匀分类分布，并将 设为由编码器输出的 32×32 网格上每个空间位置的 8192 维 logits 所参数化的分类分布。\",\"此时，ELB 的优化变得困难：由于 是离散分布，我们无法使用重参数化梯度来最大化它。通过结合在线聚类分配过程与直通估计器可以解决这一问题，但我们则使用 Gumbel-Softmax 松弛，将对 的期望替换为对 的期望，其中当温度 时，该松弛会变得精确。 的似然使用 对数拉普拉斯分布 进行评估（推导见附录 A.3）。\",\"我们使用 Adam 优化该松弛的 ELB，并采用指数加权的迭代平均（iterate averaging）。附录 A.2 给出了完整的超参数描述，但我们发现以下几点对稳定训练尤其重要：\",\"松弛温度与步长的退火策略：我们将 退火至 ，这样即可缩小松弛后的验证 ELB 与使用 而非 的真实验证 ELB 之间的差距；\",\"在编码器末端与解码器开头使用 1×1 卷积：我们发现，在松弛操作附近减小卷积感受野有助于其对真实 ELB 的泛化；\",\"对编码器和解码器残差块的输出激活乘以一个较小的常数，以确保初始化时的稳定训练。\",\"我们还发现，将 KL 权重增加到 能促进更好的码本利用率，并最终在训练结束时得到更小的重构误差。\"]},\"887\":{\"h\":\"阶段二：学习先验\",\"t\":[\"在第二阶段中，我们固定 和 ，通过最大化关于 的 ELB 来学习文本与图像 token 的先验分布。在这里， 由一个 120 亿参数的稀疏 Transformer 表示。\",\"给定一对文本-图像数据，我们将小写化的标题用 BPE 编码，最多 256 个 token，词汇表大小为 16,384；图像则用 32×32 = 1024 个 token 表示，词汇表大小为 8192。图像 token 由 dVAE 编码器 logits 通过 argmax 采样得到（不添加 Gumbel 噪声）。最后，将文本和图像 token 拼接，并作为单一的数据流进行自回归建模。\",\"该 Transformer 是一个 仅解码器（decoder-only） 架构，其中每个图像 token 在其 64 层自注意力层中的任一层都可以访问所有文本 token。完整架构见附录 B.1。模型中使用了三种不同的自注意力掩码：\",\"文本到文本部分使用标准的因果掩码；\",\"图像到图像部分则使用行、列或卷积注意力掩码。\",\"我们将文本标题的最大长度限制为 256 个 token，但在最后一个文本 token 与“图像起始 token”之间的“填充”位置处理方式并不完全确定。一种方法是在自注意力运算中将这些 token 的 logits 设为 。但我们选择为每个 256 文本位置单独学习一个特殊的填充 token，仅在无对应文本 token 时使用。在 Conceptual Captions 的初步实验中，我们发现这种做法虽然验证集损失更高，但在分布外的标题上表现更好。\",\"我们将文本与图像 token 的交叉熵损失分别按批次中该类型 token 的总数进行归一化。由于我们主要关注图像建模，将文本的交叉熵损失乘以 ，图像的交叉熵损失乘以 。目标函数使用 Adam 优化，并采用指数加权迭代平均；训练细节见附录 B.2。我们保留了大约 60.6 万张图像作为验证集，在收敛时未发现过拟合迹象。\"]},\"888\":{\"h\":\"数据收集\",\"t\":[\"我们在 Conceptual Captions 数据集上进行了最高至 12 亿参数模型的初步实验，该数据集包含 330 万对文本-图像对，是 MS-COCO 的扩展版本。\",\"为了扩展到 120 亿参数规模，我们从互联网收集了 2.5 亿对文本-图像数据，构建了一个与 JFT-300M 规模相当的数据集。该数据集不包含 MS-COCO，但包含 Conceptual Captions 以及经过筛选的 YFCC100M 子集。由于 MS-COCO 是从 YFCC100M 创建的，因此我们的训练数据中包含了一部分 MS-COCO 验证集图像（但不包括其标题）。在第 3 节的定量结果中，我们对此进行了控制，发现这对结果没有显著影响。关于数据收集过程的更多细节见附录 C。\"]},\"889\":{\"h\":\"采样生成\",\"t\":[\"我们用一个预训练的对比模型（如: CLIP）来对从 Transformer 生成的图像进行重新排序。给定一条文字描述和一张候选图像，这个对比模型会根据图像和文字的匹配程度给出一个评分。\",\"图 6 展示了当我们从更多的样本数量 N 中选出排名前 k 的图像时，效果是如何变化的。这个过程可以理解为一种由语言引导的搜索，也类似于 Xu 等人（2018）提出的辅助文本-图像匹配的训练方法。\",\"除非特别说明，文中所有用于展示效果和评测的样本，都是在没有调整温度（即温度 t=1，图 2 除外）的情况下生成的，并且都采用了用 N=512 个样本进行重排序的策略。\",\"混合精度训练和分布式优化部分略过。\"]},\"890\":{\"h\":\"DALL·E 模型代码解读\",\"t\":[\"DALL·E 模型代码解读\",\"论文链接: Zero-Shot Text-to-Image Generation 第三方代码实现: DALL-E\"]},\"891\":{\"h\":\"代码实现\",\"t\":[\"DALL·E 将 文本-图像生成 问题建模为一个自回归语言建模任务，即将文本 token 和图像 token 拼接起来，作为一个统一的序列进行训练，从而学会生成图像的离散表示。 具体的流程如下图所示:\",\"DALL-E 模型前向传播整体流程\"]},\"892\":{\"h\":\"模型初始化\",\"t\":[\"我们需要通过 DALL-E 模型的初始化流程，来熟悉模型中使用到的一些参数及其含义:\",\"def __init__( self, *, dim, # Transformer 的隐藏维度 vae, # 编码图像的 VAE 模型（用于 image token 的提取） num_text_tokens = 10000, # 文本词表大小（不含 position padding token） text_seq_len = 256, # 文本序列最大长度 depth, # Transformer block 层数 heads = 8, # Attention 头数 dim_head = 64, # 每个 attention head 的维度 reversible = False, # 是否使用 reversible transformer attn_dropout = 0., # attention dropout 概率 ff_dropout = 0, # feedforward dropout 概率 sparse_attn = False, # 是否使用稀疏 attention attn_types = None, # 多种 attention 类型（可选） loss_img_weight = 7, # 图像损失在最终 loss 中的权重 stable = False, # 是否使用 numerically stable 的 norm sandwich_norm = False, # 是否采用 sandwich norm 策略（前中后都加 layernorm） shift_tokens = True, # 是否对输入 token 做 right shift（训练） rotary_emb = True, # 是否使用 rotary embedding（相对位置编码） shared_attn_ids = None, # 用于模块共享的 attention 层 ID（可选） shared_ff_ids = None, # 用于模块共享的 feedforward 层 ID（可选） share_input_output_emb = False, # 是否输入输出 embedding 权重共享 optimize_for_inference = False, # 是否为推理模式优化结构 ):\",\"这里关于 text_seq_len 参数和文本词空间的构成需要简单说明一下:\",\"图像 Token 相关计算:\",\" image_size = vae.image_size # 输入图像大小（例如 256x256） num_image_tokens = vae.num_tokens # 图像 token 的词表大小 image_fmap_size = (image_size // (2 ** vae.num_layers)) # 编码后 feature map 的大小 image_seq_len = image_fmap_size ** 2 # 图像 token 序列长度（flatten 之后）\",\"vae.num_layers 是 VAE 编码器中的卷积层个数，每层下采样一次（一般是 stride=2）。 图像经过 VAE 编码器下采样后，特征图的边长 = 原图边长 / 2^层数\",\"图像输入经过 VAE 编码后，变成了 image_fmap_size × image_fmap_size 的二维 token map，展平后是 image_seq_len 长度的一维序列，供 Transformer 使用。\",\"文本 token 总数调整（添加 padding token）:\",\" num_text_tokens = num_text_tokens + text_seq_len # 每个位置预留一个特殊 padding token\",\"位置编码设置 :\",\" self.text_pos_emb = nn.Embedding(text_seq_len + 1, dim) if not rotary_emb else always(0) # 文本位置编码（+1 是为了 <BOS> token），如果用 rotary 就返回 0 self.image_pos_emb = AxialPositionalEmbedding(dim, axial_shape=(image_fmap_size, image_fmap_size)) if not rotary_emb else always(0) # 图像使用二维 axial 位置编码（默认）\",\"保存配置参数 :\",\" self.num_text_tokens = num_text_tokens self.num_image_tokens = num_image_tokens self.text_seq_len = text_seq_len self.image_seq_len = image_seq_len seq_len = text_seq_len + image_seq_len # 总序列长度 total_tokens = num_text_tokens + num_image_tokens # 总词表大小 self.total_tokens = total_tokens self.total_seq_len = seq_len\",\"冻结 VAE 权重（不参与训练）:\",\" self.vae = vae set_requires_grad(self.vae, False)\",\"构造 Transformer 主体 :\",\" self.transformer = Transformer( dim = dim, causal = True, # 自回归模型 seq_len = seq_len, depth = depth, heads = heads, dim_head = dim_head, reversible = reversible, attn_dropout = attn_dropout, ff_dropout = ff_dropout, attn_types = attn_types, image_fmap_size = image_fmap_size, sparse_attn = sparse_attn, stable = stable, sandwich_norm = sandwich_norm, shift_tokens = shift_tokens, rotary_emb = rotary_emb, shared_attn_ids = shared_attn_ids, shared_ff_ids = shared_ff_ids, optimize_for_inference = optimize_for_inference, )\",\"因为为每个 padding 位置保留了唯一 token id，Transformer 不再需要外部的 pad mask。\",\"输出 projection 层（Logits）:\",\" self.to_logits = nn.Sequential( nn.LayerNorm(dim), nn.Linear(dim, self.total_tokens), # 输出维度为整个 text + image 的 token vocab )\",\"构造 token embedding 层（输入）:\",\" if share_input_output_emb: # 如果启用权重共享，将 to_logits 的 Linear 拆分作为共享矩阵 self.text_emb = SharedEmbedding(self.to_logits[1], 0, num_text_tokens) self.image_emb = SharedEmbedding(self.to_logits[1], num_text_tokens, total_tokens) else: self.text_emb = nn.Embedding(num_text_tokens, dim) self.image_emb = nn.Embedding(num_image_tokens, dim)\",\"构造 Logits Mask:\",\" seq_range = torch.arange(seq_len) # 序列中每个 token 的位置编号（0~seq_len-1） logits_range = torch.arange(total_tokens) # 总词表中的每个 token id（0~total_tokens-1） seq_range = rearrange(seq_range, 'n -> () n ()') # 变成 shape (1, seq_len, 1) logits_range = rearrange(logits_range, 'd -> () () d') # 变成 shape (1, 1, total_tokens) logits_mask = ( ((seq_range >= text_seq_len) & (logits_range < num_text_tokens)) | ((seq_range < text_seq_len) & (logits_range >= num_text_tokens)) ) # 如果位置在图像段（text_seq_len之后），却输出 text token → 屏蔽 # 如果位置在文本段（text_seq_len之前），却输出 image token → 屏蔽 self.register_buffer('logits_mask', logits_mask, persistent=False) # 保存 mask 到 buffer（不会被模型训练修改）\",\"由于文本token和图像token被拼接在一起，作为统一的序列输入Transformer进行编码，\",\"且文本词空间和图像离散视觉词空间也通过视觉词索引偏移的方式完成了统一，\",\"因此才有了Transformer可以一次性预测出每个位置对应的Next Token能力，\",\"但问题就在于属于某个文本Token位置处的预测结果向量中，其反映的实际是整个统一词空间上的概率分布，如果概率最高的那个Token是图像Token，那么就会导致模态混乱了，\",\"为了解决这个问题，作者引入了 Logits Mask , 如果当前待预测Token位置属于文本词，则将其概率分布中的离散视觉词索引空间对应的概率分布设置为0，\",\"反之，如果当前待预测Token位置属于离散视觉词，则将其概率分布中的文本词索引空间对应的概率分布设置为0，\",\"具体来说:\",\"import torch # 假设配置 text_seq_len = 4 # 输入文本序列长度 image_seq_len = 2 # 每个图像由两个离散视觉token进行表示 total_seq_len = text_seq_len + image_seq_len # 总输入序列长度 num_text_tokens = 4 # 文本词表大小 num_image_tokens = 5 # 离散视觉词表大小 total_tokens = num_text_tokens + num_image_tokens # 总词表大小 # 构造 logits_mask seq_range = torch.arange(total_seq_len).view(1, total_seq_len, 1) logits_range = torch.arange(total_tokens).view(1, 1, total_tokens) logits_mask = ((seq_range >= text_seq_len) & (logits_range < num_text_tokens)) | \\\\ ((seq_range < text_seq_len) & (logits_range >= num_text_tokens)) # 将 logits_mask 转为 int 展示（True->1, False->0） logits_mask_int = logits_mask.int()[0] # 只展示第一个 batch 维度 print(logits_mask_int)\",\"输出结果:\",\"# 前4个位置为文本token，后2个位置为图像token tensor([[0, 0, 0, 0, 1, 1, 1, 1, 1], # 对于每个token来说，统一词空间大小为9，其中前4维为词空间索引，后5维为离散视觉词空间索引 [0, 0, 0, 0, 1, 1, 1, 1, 1], # 对于文本token，将离散视觉词空间索引对应的概率分布设置为0 (这里设置为1，是为了后续乘上一个最小值) [0, 0, 0, 0, 1, 1, 1, 1, 1], [0, 0, 0, 0, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0, 0], # 对于图像token，将文本词索引空间对应的概率分布设置为0 (这里设置为1，是为了后续乘上一个最小值) [1, 1, 1, 1, 0, 0, 0, 0, 0]], dtype=torch.int32)\"]},\"893\":{\"h\":\"前向传播流程\",\"t\":[\"本节最开始给出的前向传播流程图已经清晰展示了 DALL·E 模型的前向传播流程，下面我们通过代码详细来看一下具体实现细节:\",\"随机对输入的文本条件进行 Dropout\",\"def forward( self, text, image=None, return_loss=False, null_cond_prob=0., cache=None, ): # 获取 batch size、device 和 transformer 的最大序列长度 batch, device, total_seq_len = text.shape[0], text.device, self.total_seq_len # 以一定概率随机删除文本条件（用于训练时的条件 dropout） if null_cond_prob > 0: null_mask = prob_mask_like((batch,), null_cond_prob, device=device) text *= rearrange(~null_mask, 'b -> b 1') # 如果 null_mask=True，则整条 text 设为 0（即无条件）\",\" def prob_mask_like(shape, prob, device): return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\",\"DALL·E 的目标不是只会“根据文本生成图像”，还希望它能：\",\"有条件生成（text → image）\",\"无条件生成（随机 → image）\",\"通过让一部分样本在训练时不给文本输入，让模型也能学到“如何仅靠图像生成图像”。\",\"为每一个padding token分配一个唯一的词索引\",\" # self.num_text_tokens - self.text_seq_len 是计算 padding token 在文本词索引空间中的起始索引 text_range = torch.arange(self.text_seq_len, device=device) + (self.num_text_tokens - self.text_seq_len) text = torch.where(text == 0, text_range, text) # 将 padding token 替换为唯一的 token ID\",\"文本序列开头加上 <bos> token , 作为自回归预测的开始标志\",\" # 在文本序列开头加上 <bos> token（值为0） text = F.pad(text, (1, 0), value=0)\",\"文本 token embedding 与 位置编码\",\" # 文本 token embedding 与位置编码 tokens = self.text_emb(text) tokens += self.text_pos_emb(torch.arange(text.shape[1], device=device)) seq_len = tokens.shape[1] # 当前 token 序列长度（仅包含文本部分）\",\"输入图像编码为离散的视觉Token，视觉Token embedding 与 位置编码 ，最后与文本Token embedding 拼接，作为送入 Transformer 的输入\",\" # 如果输入了图像（且非空），处理图像 embedding if exists(image) and not is_empty(image): is_raw_image = len(image.shape) == 4 # 如果是原始图像（B, C, H, W） if is_raw_image: image_size = self.vae.image_size channels = self.vae.channels # 确保图像尺寸正确 assert tuple(image.shape[1:]) == (channels, image_size, image_size), \\\\ f'invalid image of dimensions {image.shape} passed in during training' # 使用 VAE 将原始图像编码为离散 codebook indices (after flatten) image = self.vae.get_codebook_indices(image) image_len = image.shape[1] image_emb = self.image_emb(image) # 图像 token embedding image_emb += self.image_pos_emb(image_emb) # 图像位置编码 # 将文本和图像的 embedding 拼接 tokens = torch.cat((tokens, image_emb), dim=1) seq_len += image_len # 更新总长度\",\"\\\"右移\\\": 删除序列最后一个token，因为其不参与Next Token Prediction；(训练优化Trick不进行讲解)\",\" # 如果 token 总长度超过模型最大长度，则裁剪掉最后一个 token（训练时末尾 token 不需要预测） if tokens.shape[1] > total_seq_len: seq_len -= 1 tokens = tokens[:, :-1] # 如果启用了稳定训练策略（stabilization trick） if self.stable: alpha = 0.1 tokens = tokens * alpha + tokens.detach() * (1 - alpha) # 如果使用了 KV Cache（用于推理阶段），只保留最后一个 token if exists(cache) and cache.get('offset'): tokens = tokens[:, -1:] # 送入 transformer 主体 out = self.transformer(tokens, cache=cache)\",\"投影到统一词空间，应用 logits mask ，防止跨模态预测\",\" # 如果启用了稳定策略，对输出做归一化 if self.stable: out = self.norm_by_max(out) # 得到每个位置上的分类 logits（预测 token） logits = self.to_logits(out) # 构造 logits mask：限制哪些位置可以预测哪些 token（防止跨模态预测） logits_mask = self.logits_mask[:, :seq_len] if exists(cache) and cache.get('offset'): logits_mask = logits_mask[:, -1:] max_neg_value = -torch.finfo(logits.dtype).max # -inf 替代值 logits.masked_fill_(logits_mask, max_neg_value) # 用 -inf 屏蔽不合法预测\",\"是否提前中断返回 logits\",\" # 更新 KV Cache 的偏移量（用于增量推理） if exists(cache): cache['offset'] = cache.get('offset', 0) + logits.shape[1] # 如果不要求计算损失，直接返回 logits if not return_loss: return logits\",\"计算文本token和视觉token预测结果与原Label的交叉熵损失\",\" # 训练时必须提供图像（否则无法计算图像 token 的预测损失） assert exists(image), 'when training, image must be supplied' # 将图像 token 的索引整体加偏移（让图像 token ID 与文本 token 不重叠） offsetted_image = image + self.num_text_tokens # 构造预测标签：文本去掉 <bos>（text[:, 1:]），接上图像 token labels = torch.cat((text[:, 1:], offsetted_image), dim=1) # logits 维度从 [B, N, C] 变成 [B, C, N]，以匹配 cross_entropy 的输入格式 logits = rearrange(logits, 'b n c -> b c n') # 计算文本部分的 cross-entropy loss（前 self.text_seq_len 个 token） loss_text = F.cross_entropy(logits[:, :, :self.text_seq_len], labels[:, :self.text_seq_len]) # 计算图像部分的 cross-entropy loss loss_img = F.cross_entropy(logits[:, :, self.text_seq_len:], labels[:, self.text_seq_len:]) # 按照权重加权融合 loss（图像损失通常占更大比例） loss = (loss_text + self.loss_img_weight * loss_img) / (self.loss_img_weight + 1) return loss\",\"在 DALL·E 的训练中，文本 token 和图像 token 的数量差别很大（通常图像 token 远多于文本 token），如果直接把它们的交叉熵损失简单相加，图像部分的 loss 会“淹没”文本部分，导致模型过度关注图像而忽视文本。为了解决这个不平衡问题，DALL·E 在合并两部分损失时引入了一个 图像损失权重self.loss_img_weight（通常设置为 7），具体做法如下：\",\"loss = (loss_text + self.loss_img_weight * loss_img) / (self.loss_img_weight + 1)\",\"loss_text：文本部分的平均交叉熵损失\",\"loss_img ：图像部分的平均交叉熵损失\",\"self.loss_img_weight：图像损失在总损失中的相对重要性系数（>1 时放大图像 loss）\",\"当 loss_img_weight = 7 时，公式相当于：\",\"也就是把文本损失和图像损失当作 1:7 的比例来融合。除以 (self.loss_img_weight + 1) 可以 保持总损失的数值 scale 大致与单一部分损失相同，否则会直接把 loss 放大 8 倍，不利于学习率等超参数设置。例如：\",\"若不除以，合并后 loss 规模 ≈ \",\"除以后 loss 规模 ≈ ，整体仍在合理区间\",\"通过给图像损失设置更高的权重，平衡文本和图像两部分的训练目标，同时保持总损失数值稳定。\"]},\"894\":{\"h\":\"Classifier-Free Guidance（无条件引导技术）\",\"t\":[\"Classifier-Free Guidance（CFG）本质上是一种“在同一个模型内部做有条件与无条件两种预测，然后按比例混合”以强化条件信号的方法。它的核心思想可以分为三个步骤：\",\"无条件预测\",\"令模型忽略输入的条件（例如将 null_cond_prob=1.0），只靠自身学到的“图像先验”去预测下一个 token／像素。输出我们记作\",\"有条件预测\",\"再次用原始的条件（如文本描述）去预测，得到\",\"线性混合强化\",\"将两者按下式混合：\",\"其中 （cond_scale）是一个大于 1 的放大系数。这样做的意义在于：\",\" 正好捕捉了“条件对输出的额外影响”，\",\"放大这个差值就能让模型更“听话”地跟随条件（例如更准确地按照提示文本生成图像），\",\"而基础的“无条件”部分保证了生成的多样性与样本质量。\",\"为什么它能工作？\",\"单模型实现：不需要额外训练一个对比判别器或辅助网络，只利用模型自身“有条件/无条件”两种模式。\",\"稳定平衡： 时退化为普通有条件生成； 时增强条件影响；如果条件本身模糊，过大 会丧失多样性。\",\"实际效果：在图像或序列生成任务中，CFG 能显著提升条件相关性（如文本与生成图像的紧密契合度），同时保留一定的随机性和自然度。\",\"这种技术被广泛应用于扩散模型、Transformer-based 自回归模型（如 DALL·E）等条件生成场景，是当前最简单、最高效的“无判别器”引导方法。\",\"具体代码实现过程如下:\",\" def forward_with_cond_scale(self, *args, cond_scale = 1, cache = None, **kwargs): if cond_scale == 1: return self(*args, **kwargs) prev_cache = cache.copy() if exists(cache) else None logits = self(*args, cache = cache, **kwargs) # discovery by Katherine Crowson # https://twitter.com/RiversHaveWings/status/1478093658716966912 null_cond_logits = self(*args, null_cond_prob = 1., cache = prev_cache, **kwargs) return null_cond_logits + (logits - null_cond_logits) * cond_scale\"]},\"895\":{\"h\":\"推理过程: 图文联合生成图像\",\"t\":[\"DALL-E 的推理过程实际执行过程中，不仅可以传入文本条件，还可以传入初始图像条件，从而实现图文联合生成 (text + image condition) , 具体代码实现如下:\",\"@torch.no_grad() # 不计算梯度，用于推理模式，节省显存 @eval_decorator # 将模型切换到 eval 模式（如关闭 dropout、norm 统计冻结等），确保一致性 def generate_images( self, text, # 输入的文本 token 序列（已经 embed 好的 token id） *, clip = None, # 可选：用于对生成图像进行 CLIP 打分的模型 filter_thres = 0.5, # Top-k 采样时的阈值，控制生成 token 的多样性 temperature = 1., # Gumbel softmax 的温度参数，控制采样随机性 img = None, # 可选：用于 image priming 的起始图像 num_init_img_tokens = None,# 用于 priming 的起始 image token 数量 cond_scale = 1., # CFG 强化系数（1 表示不强化） use_cache = False, # 是否启用 KV 缓存加速 ): # 一些常用变量的引用 vae, text_seq_len, image_seq_len, num_text_tokens = ( self.vae, self.text_seq_len, self.image_seq_len, self.num_text_tokens ) total_len = text_seq_len + image_seq_len # 整个序列的总长度 text = text[:, :text_seq_len] # 限制输入文本长度不超过最大 text_seq_len out = text # 初始化输出 token 序列 # -------------------------- # Optional: 图像 priming # -------------------------- if exists(img): image_size = vae.image_size assert img.shape[1:] == (3, image_size, image_size), \\\\ f'input image must have the correct image size {image_size}' # 编码图像为 VQ token 序列 indices = vae.get_codebook_indices(img) # 默认采样前 14 × 32 = 448 个图像 token（约占 43.75%） num_img_tokens = default(num_init_img_tokens, int(0.4375 * image_seq_len)) assert num_img_tokens < image_seq_len, 'priming token 数不能超过图像 token 总长度' # 仅使用前 num_img_tokens 个 image token 来进行条件 priming indices = indices[:, :num_img_tokens] # 将这些图像 token 拼接到文本后面作为起始序列 out = torch.cat((out, indices), dim = -1)\",\"Image Priming for Conditional Image Generation 也可以理解为是一种 图像引导生成（Image Conditioning），就像 文本 prompt 一样引导生成内容，只不过它是 图像 prompt 。\",\"并且我们只拼接一部分（如前 14×32 个 token ≈ 左上角区域）：给出一定图像引导，让模型知道「风格/结构/颜色」，但留出空间继续生成图像后续内容。\",\"模式\",\"条件输入\",\"效果\",\"文本生成图像\",\"仅文本 token\",\"从零生成完整图像\",\"图像补全\",\"文本 token + 部分图像 token（来自真实图像）\",\"在已有图像基础上补全未提供区域\",\" # -------------------------- # 生成 token 序列（从起始长度到 total_len） # -------------------------- prev_cache = None cache = {} if use_cache else None # KV 缓存机制（可加速 transformer 推理） for cur_len in range(out.shape[1], total_len): is_image = cur_len >= text_seq_len # 当前 token 属于图像部分 # 每一步构造 text / image token 序列（注意有 padding） text, image = out[:, :text_seq_len], out[:, text_seq_len:] # 使用 CFG 技术进行条件引导预测 logits logits = self.forward_with_cond_scale(text, image, cond_scale=cond_scale, cache=cache) # 取当前时间步（只关心最后一个 token 的 logits） logits = logits[:, -1, :] # top-k 采样（过滤掉概率低的 token） filtered_logits = top_k(logits, thres=filter_thres) # 使用 gumbel softmax 进行随机采样，得到一个 token id sample = gumbel_sample(filtered_logits, temperature=temperature, dim=-1) # 如果是 image token，需要减去偏移（因为 logit 空间 = [text_vocab, image_vocab]） sample -= (num_text_tokens if is_image else 0) # 拼接新生成的 token out = torch.cat((out, sample[:, None]), dim=-1) # 拆分输出序列 text_seq = out[:, :text_seq_len] # 最终文本 token 序列 img_seq = out[:, -image_seq_len:] # 最终图像 token 序列（后 image_seq_len 个） # 解码图像 token 为实际图片 images = vae.decode(img_seq) # 若提供了 CLIP，则使用其打分 if exists(clip): scores = clip(text_seq, images, return_loss=False) return images, scores return images\"]},\"896\":{\"h\":\"Top-K 采样\",\"t\":[\"Top-K 采样是一种常用的生成模型采样方法，用于从模型输出的 logits 中选择概率最高的 K 个 token 作为下一个 token。\",\"def top_k(logits, thres=0.5): # 获取最后一维的大小，即 token 的数量 num_logits = logits.shape[-1] # 根据阈值计算 top-k 的 k 值，确保至少选一个 # 例如 thres=0.5 表示保留 top 50% 的 logits k = max(int((1 - thres) * num_logits), 1) # 从 logits 中获取 top-k 的值 val 及其对应的索引 ind val, ind = torch.topk(logits, k) # 构造一个与 logits 相同形状的 tensor，初始值为 -inf（负无穷） # 用于屏蔽非 top-k 的 logits probs = torch.full_like(logits, float('-inf')) # 将 top-k 的值 scatter 到对应的位置（其余位置仍为 -inf） probs.scatter_(1, ind, val) # 返回经过筛选后的 logits，非 top-k 的位置为 -inf return probs\"]},\"897\":{\"h\":\"Gumbel Sampling\",\"t\":[\"Gumbel Sampling（Gumbel-Max 采样），它是一个常用于离散分布中采样的技巧，尤其适用于生成模型中从 logits 中以概率方式采样一个 token，避免直接用 softmax → multinomial。\",\"其中 ，这是一个从 Gumbel 分布采样的噪声。\",\"def gumbel_sample(t, temperature=1., dim=-1): # 将 logits t 除以温度 temperature（控制随机性），加上 Gumbel 噪声后取 argmax 采样 return ((t / temperature) + gumbel_noise(t)).argmax(dim=dim)\",\"在代码中：\",\"t 是 logits，即模型输出的每个 token 的打分；\",\"gumbel_noise(t) 为每个位置生成 Gumbel(0,1) 噪声；\",\"(t / temperature) 是用温度控制 logits 的“平滑程度”；\",\"argmax(dim=dim) 就是从分布中采样一个 token。\",\"temperature 控制采样的随机程度：\",\"趋近 0 → 趋近贪心采样（最大值）；\",\"趋近 ∞ → 更加随机，平滑采样。\",\"gumbel_noise 的加入使得采样变为“有噪声的 argmax”，而不是简单地选最大值。\"]},\"898\":{\"h\":\"“语言建模能力”的回溯性验证\",\"t\":[\"DALL·E 是一个 文本-图像联合建模（joint modeling） 的 Transformer：\",\"它的输入是 text_tokens + image_tokens 拼接而成的序列；\",\"输出是对整个序列的预测（自回归建模）；\",\"模型头部输出 logits，既可用于预测文本 token，也可用于预测图像 token。\",\"generate_texts 方法就是在 复用这个模型的 text 生成能力，可以视作：\",\"🔸 “测试 DALL·E 是否真正学会了语言建模部分”，\",\"🔸 “是否理解 prompt 的语言结构”。\",\"@torch.no_grad() # 表示该函数中不进行梯度计算，节省内存，提高推理效率 @eval_decorator # 将模型设置为 evaluation 模式，禁用 dropout 等训练行为 def generate_texts( self, tokenizer, # 分词器对象，用于将输入文本编码为 token 序列 text = None, # 输入文本（可为空字符串） *, filter_thres = 0.5, # top-k 采样的阈值，控制保留多少 logits 值 temperature = 1. # Gumbel Softmax 的温度系数，调节随机性 ): text_seq_len = self.text_seq_len # 设定文本序列的最大长度（固定） # 如果没有输入文本，默认从 token_id 为 0 的 token 开始（如 [BOS]） if text is None or text == \\\"\\\": text_tokens = torch.tensor([[0]]).cuda() else: # 编码输入文本为 token 序列，并添加 batch 维度 text_tokens = torch.tensor(tokenizer.tokenizer.encode(text)).cuda().unsqueeze(0) # 自回归生成，逐 token 采样直到达到目标长度 for _ in range(text_tokens.shape[1], text_seq_len): device = text_tokens.device # 获取 token 的嵌入向量 tokens = self.text_emb(text_tokens) # 添加位置编码（相对或绝对），保持 token 顺序感知 tokens += self.text_pos_emb(torch.arange(text_tokens.shape[1], device=device)) seq_len = tokens.shape[1] # 当前序列长度 # 送入 Transformer 模型获取输出（每个位置的表征） output_transf = self.transformer(tokens) # 如果开启了 stable 模式，则归一化输出，避免极端数值 if self.stable: output_transf = self.norm_by_max(output_transf) # 映射至 logits（预测下一个 token 的概率分布） logits = self.to_logits(output_transf) # 屏蔽非法的预测位置： # 确保在生成文本的阶段，只能预测文本 token，而不是图像 token logits_mask = self.logits_mask[:, :seq_len] max_neg_value = -torch.finfo(logits.dtype).max logits.masked_fill_(logits_mask, max_neg_value) # 仅取最后一个位置的 logits（用于下一个 token 的采样） logits = logits[:, -1, :] # top-k 过滤：仅保留最可能的 k 个 logits，其余设置为 -inf filtered_logits = top_k(logits, thres=filter_thres) # 使用 Gumbel Softmax 技术从过滤后的 logits 中采样一个 token sample = gumbel_sample(filtered_logits, temperature=temperature, dim=-1) # 将采样到的新 token 拼接到已有序列后 text_tokens = torch.cat((text_tokens, sample[:, None]), dim=-1) # 构建 padding token 的集合，用于后续解码时跳过填充 token padding_tokens = set(np.arange(self.text_seq_len) + (self.num_text_tokens - self.text_seq_len)) # 将 token 序列解码为可读文本，自动去掉 padding token texts = [tokenizer.tokenizer.decode(text_token, pad_tokens=padding_tokens) for text_token in text_tokens] # 返回 token 序列和解码后的文本 return text_tokens, texts\"]},\"899\":{\"h\":\"DiscreteVAE 离散化变分自编码器\",\"t\":[\"从本节开始，我们将快速过一下 DiscreteVAE 离散化变分自编码器 和 CLIP 模型的代码实现。\",\"本节开始为扩展阅读内容，已有前置知识的同学，可以选择跳过。\",\"首先来看一下 DiscreteVAE 的初始化方法:\",\"class DiscreteVAE(nn.Module): def __init__( self, image_size = 256, # 输入图像尺寸（宽高），要求是 2 的幂 num_tokens = 512, # codebook 中的 token 数量（离散表示空间的大小） codebook_dim = 512, # codebook 中每个向量的维度 num_layers = 3, # 编码器 / 解码器的层数（每层是一次下采样或上采样） num_resnet_blocks = 0, # 残差块的数量（用于提升表达能力） hidden_dim = 64, # 编码器 / 解码器中卷积通道的基础维度 channels = 3, # 图像通道数（RGB = 3） smooth_l1_loss = False, # 是否使用 Smooth L1 损失（否则使用 MSE） temperature = 0.9, # Gumbel Softmax 温度，控制离散采样的平滑程度 straight_through = False, # 是否使用 straight-through estimator（用于反向传播离散 token） reinmax = False, # 是否使用 Reinmax（一种用于离散变量的采样技术） kl_div_loss_weight = 0., # KL 散度损失的权重（通常为 0 或很小） normalization = ((*((0.5,) * 3), 0), (*((0.5,) * 3), 1)) # 图像标准化参数 ): super().__init__() assert log2(image_size).is_integer(), 'image size must be a power of 2' assert num_layers >= 1, 'number of layers must be greater than or equal to 1' has_resblocks = num_resnet_blocks > 0 self.channels = channels self.image_size = image_size self.num_tokens = num_tokens self.num_layers = num_layers self.temperature = temperature self.straight_through = straight_through self.reinmax = reinmax # codebook：token_id 到向量的映射，大小为 (num_tokens, codebook_dim) self.codebook = nn.Embedding(num_tokens, codebook_dim) hdim = hidden_dim # 构造编码器与解码器的通道列表（每层的通道数） enc_chans = [hidden_dim] * num_layers dec_chans = list(reversed(enc_chans)) # 解码器通道反转 enc_chans = [channels, *enc_chans] # 编码器输入通道从图像开始 # 如果有残差块，解码器第一层输入通道来自 ResBlock 输出 dec_init_chan = codebook_dim if not has_resblocks else dec_chans[0] dec_chans = [dec_init_chan, *dec_chans] # 形如 [(in1, out1), (in2, out2), ...] enc_chans_io, dec_chans_io = map(lambda t: list(zip(t[:-1], t[1:])), (enc_chans, dec_chans)) enc_layers = [] # 编码器网络层列表 dec_layers = [] # 解码器网络层列表 # 构建编码器和解码器的每一层（卷积 / 转置卷积 + ReLU） for (enc_in, enc_out), (dec_in, dec_out) in zip(enc_chans_io, dec_chans_io): enc_layers.append( nn.Sequential(nn.Conv2d(enc_in, enc_out, kernel_size=4, stride=2, padding=1), nn.ReLU()) ) dec_layers.append( nn.Sequential(nn.ConvTranspose2d(dec_in, dec_out, kernel_size=4, stride=2, padding=1), nn.ReLU()) ) # 添加 ResBlock（如果有） for _ in range(num_resnet_blocks): dec_layers.insert(0, ResBlock(dec_chans[1])) # 解码器最前面插入 ResBlock enc_layers.append(ResBlock(enc_chans[-1])) # 编码器末尾追加 ResBlock # 如果使用 ResBlock，还需要额外将 codebook_dim 映射到 decoder 通道数 if num_resnet_blocks > 0: dec_layers.insert(0, nn.Conv2d(codebook_dim, dec_chans[1], kernel_size=1)) # 编码器最终输出 token logits，维度是 num_tokens（注意：非 softmax） enc_layers.append(nn.Conv2d(enc_chans[-1], num_tokens, kernel_size=1)) # 解码器最终输出图像，维度是原始图像的通道数 dec_layers.append(nn.Conv2d(dec_chans[-1], channels, kernel_size=1)) # 打包成 nn.Sequential 模块 self.encoder = nn.Sequential(*enc_layers) self.decoder = nn.Sequential(*dec_layers) # 设置重建损失函数：MSE 或 Smooth L1 self.loss_fn = F.smooth_l1_loss if smooth_l1_loss else F.mse_loss self.kl_div_loss_weight = kl_div_loss_weight # KL损失的权重（可用于 soft quantization） # 图像标准化（mean, std），用于输入预处理 self.normalization = tuple(map(lambda t: t[:channels], normalization))\",\"下面给出 DiscreteVAE 的 forward 方法：\",\"def forward( self, img, # 输入图像，形状为 (B, C, H, W) return_loss = False, # 是否返回损失（训练时设为 True） return_recons = False, # 是否返回重建图像（可用于可视化对比） return_logits = False, # 是否返回 token logits（DALL·E 训练时提取 token 用） temp = None # 温度参数，用于 Gumbel-Softmax，控制采样平滑程度 ): device = img.device num_tokens = self.num_tokens image_size = self.image_size kl_div_loss_weight = self.kl_div_loss_weight # 图像尺寸检查 assert img.shape[-1] == image_size and img.shape[-2] == image_size, f'input must have the correct image size {image_size}' # 归一化输入图像（和训练时保持一致） img = self.norm(img) # 编码器输出 logits，形状为 (B, num_tokens, H/2^L, W/2^L) logits = self.encoder(img) # 若仅需要 token logits（比如训练 DALL·E 时获取离散 token） if return_logits: return logits # 采样温度参数：如果没传入，就用默认的 self.temperature temp = default(temp, self.temperature) # Gumbel Softmax 采样：输出 one-hot 向量或 soft one-hot（取决于 straight_through） one_hot = F.gumbel_softmax(logits, tau = temp, dim = 1, hard = self.straight_through) # Reinmax（改进的 straight-through Gumbel Softmax）逻辑 if self.straight_through and self.reinmax: # Reinmax 来自 https://arxiv.org/abs/2304.08612，增强采样精度 # 论文算法2实现 one_hot = one_hot.detach() # 不反向传播梯度 π0 = logits.softmax(dim = 1) # 原始 softmax 分布 π1 = (one_hot + (logits / temp).softmax(dim = 1)) / 2 # 平均分布 π1 = ((log(π1) - logits).detach() + logits).softmax(dim = 1) # 加入梯度修正 π2 = 2 * π1 - 0.5 * π0 # Reinmax 分布 one_hot = π2 - π2.detach() + one_hot # 将梯度传递给 one_hot # 将 one-hot 与 codebook 进行矩阵乘法，获取嵌入向量图（图像 latent 表示） # einsum: b (token) h w, token d -> b d h w sampled = einsum('b n h w, n d -> b d h w', one_hot, self.codebook.weight) # 解码器将 latent 表示还原为图像 out = self.decoder(sampled) # 如果不需要返回 loss，直接返回解码图像 if not return_loss: return out # 计算重建损失（MSE 或 Smooth L1） recon_loss = self.loss_fn(img, out) # KL 散度部分（用于将 token 分布逼近 uniform） # logits shape: (B, num_tokens, H, W) -> (B, HW, num_tokens) logits = rearrange(logits, 'b n h w -> b (h w) n') log_qy = F.log_softmax(logits, dim = -1) # q(y|x)：预测分布 log_uniform = torch.log(torch.tensor([1. / num_tokens], device = device)) # p(y) ~ U kl_div = F.kl_div(log_uniform, log_qy, None, None, 'batchmean', log_target = True) # 总损失 = 重建损失 + KL散度损失（可选） loss = recon_loss + (kl_div * kl_div_loss_weight) # 如果不需要重建图像，直接返回 loss if not return_recons: return loss # 否则返回损失 + 解码图像 return loss, out\",\"关于本部分代码细节的详细解释，可以参考之前这篇文章: BEiT 模型代码解读\",\"DALL-E 模型使用的是训练好的 DiscreteVAE , 其中我们最常使用 get_codebook_indices 方法获取输入图像对应的离散视觉 token 索引。\",\"@torch.no_grad() # 禁用梯度计算（推理模式，提高效率，节省显存） @eval_decorator # 将模型暂时设为 eval 模式（关闭 Dropout、BatchNorm 的动效） def get_codebook_indices(self, images): # 编码器 + projection，得到每个位置对应的 logits（未 softmax） # logits 形状: (B, num_tokens, H', W')，H'=W'=原图尺寸 / 2^L logits = self(images, return_logits = True) # 取最大概率的 token 索引：在 dim=1（token 类别维）上 argmax # 得到形状: (B, H', W')，即每个图像中每个 patch 的 token 索引 codebook_indices = logits.argmax(dim = 1).flatten(1) # flatten(1): 将 (B, H', W') 展平为 (B, H'*W')，方便后续处理 return codebook_indices\",\"其次我们会调用 decode 方法实现从离散视觉Token索引到图像的重建过程:\",\"def decode( self, img_seq # 输入图像的离散 token 序列，形状：(B, N) ): # 通过 codebook 查表，把每个 token 转换为向量（embedding） # image_embeds 形状: (B, N, D) image_embeds = self.codebook(img_seq) # 获取维度信息：B 批次大小，N token 个数，D embedding 维度 b, n, d = image_embeds.shape # 假设图像是正方形的，N = H' × W'，计算边长 h = w = int(sqrt(n)) # 重新排列：从 (B, N, D) 转换为 (B, D, H', W')，用于 ConvTranspose 解码 image_embeds = rearrange(image_embeds, 'b (h w) d -> b d h w', h = h, w = w) # 解码还原图像：使用 Decoder（转置卷积等） # 输出图像形状: (B, C, H, W) images = self.decoder(image_embeds) return images\"]},\"900\":{\"h\":\"生成质量判别器: CLIP\",\"t\":[\"我们可以借助预训练好的CLIP模型，作为图文匹配的判别器，以此来评判DALL-E模型的文生图质量:\",\"class CLIP(nn.Module): def __init__( self, *, dim_text = 512, # 文本嵌入维度 dim_image = 512, # 图像嵌入维度 dim_latent = 512, # 公共对齐空间的维度（用于计算相似度） num_text_tokens = 10000, # 文本词表大小 text_enc_depth = 6, # 文本 Transformer 层数 text_seq_len = 256, # 文本序列最大长度 text_heads = 8, # 文本 Transformer 头数 num_visual_tokens = 512, # 图像 patch token 数（未使用） visual_enc_depth = 6, # 图像 Transformer 层数 visual_heads = 8, # 图像 Transformer 头数 visual_image_size = 256, # 输入图像尺寸 visual_patch_size = 32, # patch 尺寸 channels = 3 # 图像通道数 ): super().__init__() # --- 文本编码器 --- self.text_emb = nn.Embedding(num_text_tokens, dim_text) # 文本 token embedding self.text_pos_emb = nn.Embedding(text_seq_len, dim_text) # 文本位置 embedding self.text_transformer = Transformer( causal = False, seq_len = text_seq_len, dim = dim_text, depth = text_enc_depth, heads = text_heads, rotary_emb = False ) self.to_text_latent = nn.Linear(dim_text, dim_latent, bias = False) # 映射到公共 latent 空间 # --- 图像编码器 --- assert visual_image_size % visual_patch_size == 0, 'Image dimensions must be divisible by the patch size.' num_patches = (visual_image_size // visual_patch_size) ** 2 # 图像中 patch 的数量 patch_dim = channels * visual_patch_size ** 2 # 每个 patch 的展平维度 self.visual_patch_size = visual_patch_size self.to_visual_embedding = nn.Linear(patch_dim, dim_image) # 将 patch 嵌入图像 token 空间 self.visual_pos_emb = nn.Embedding(num_patches, dim_image) # 图像 patch 的位置 embedding self.visual_transformer = Transformer( causal = False, seq_len = num_patches, dim = dim_image, depth = visual_enc_depth, heads = visual_heads, rotary_emb = False ) self.to_visual_latent = nn.Linear(dim_image, dim_latent, bias = False) # 映射到公共 latent 空间 # 温度参数（可学习，用于缩放余弦相似度） self.temperature = nn.Parameter(torch.tensor(1.)) def forward( self, text, # 输入文本，shape: [B, T] image, # 输入图像，shape: [B, C, H, W] text_mask = None, # 文本掩码（用于处理 padding） return_loss = False # 是否返回对比损失（训练阶段） ): b, device, p = text.shape[0], text.device, self.visual_patch_size # --- 文本编码 --- text_emb = self.text_emb(text) # 文本 token embedding text_emb += self.text_pos_emb(torch.arange(text.shape[1], device = device)) # 加上位置 embedding enc_text = self.text_transformer(text_emb, mask = text_mask) # 经过 Transformer 编码 # --- 图像编码 --- # 将图像切分为 patch，并展平每个 patch image_patches = rearrange(image, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = p) image_emb = self.to_visual_embedding(image_patches) # 线性映射为 patch embedding image_emb += self.visual_pos_emb(torch.arange(image_emb.shape[1], device = device)) # 加位置 embedding enc_image = self.visual_transformer(image_emb) # 图像 Transformer 编码 # --- 获取图文 latent 表示 --- # 文本 latent: 取平均或 masked average（忽略 padding） if exists(text_mask): text_latents = masked_mean(enc_text, text_mask, dim = 1) else: text_latents = enc_text.mean(dim = 1) # 图像 latent: 直接平均所有 patch token image_latents = enc_image.mean(dim = 1) # 映射到公共 latent 空间 text_latents = self.to_text_latent(text_latents) image_latents = self.to_visual_latent(image_latents) # 对 latent 向量进行归一化（L2），以便计算余弦相似度 text_latents, image_latents = map(lambda t: F.normalize(t, p = 2, dim = -1), (text_latents, image_latents)) # 计算可学习温度值（以提升相似度分布的差异性） temp = self.temperature.exp() if not return_loss: # 推理模式下，计算每对 text-image 的相似度（点积 * 温度） sim = einsum('n d, n d -> n', text_latents, image_latents) * temp return sim # 训练模式下，计算所有样本之间的相似度矩阵（用于对比损失） sim = einsum('i d, j d -> i j', text_latents, image_latents) * temp # 构建标签，正样本在对角线（i==j） labels = torch.arange(b, device = device) # 双向对比损失：text -> image 和 image -> text，平均两个方向的 cross entropy loss = (F.cross_entropy(sim, labels) + F.cross_entropy(sim.t(), labels)) / 2 return loss\"]},\"901\":{\"h\":\"生成对抗网络 (GAN) 学习笔记\",\"t\":[\"生成对抗网络 (GAN) 学习笔记\"]},\"902\":{\"h\":\"前置知识\"},\"903\":{\"h\":\"最大似然估计（Maximum Likelihood Estimation, MLE）\",\"t\":[\"最大似然估计是一种利用观测数据反向推断模型参数的方法。直观地说，它假设数据生成过程已知（概率模型已定），但参数未知，我们通过已观测到的样本情况去“猜测”哪个参数值最合理。具体地，MLE 选取使得观测到的数据出现的概率（即似然）最大的参数值为估计结果。\",\"例如，如果我们抛硬币100次均出现正面，我们很自然地认为这枚硬币不是公平的，而最可能的是两面都为正面。这种根据实验结果推断最有可能的硬币属性，就是最大似然估计的直观思想。同样地，若10次抛硬币出现6次正面，我们倾向于估计硬币出现正面的概率为0.6，因为这个假设使得产生“6次正面”的可能性最大。更形式化地说，对于参数 的任意假设，定义似然函数，MLE 就是取使 最大的 。在统计学中，这个最大点称为参数的最大似然估计。\"]},\"904\":{\"h\":\"数学推导\",\"t\":[\"设样本数据为 ，假定其独立同分布于参数为 的某个概率分布。似然函数定义为在参数 给定下观测到该样本的联合概率或概率密度：\",\"若各观测独立，则\",\"其中 是单个样本的概率质量（或密度）函数。最大似然估计就是求解优化问题\",\"即寻找使似然函数取得最大值的参数。为了简化计算，通常取对数得到对数似然，由于对数是单调递增的， 的最大值处即对应 的最大值。对数似然展开为\",\"然后对 关于各参数求偏导并令其为零，即可得到极大似然估计的方程（似然方程），进而求出 。\",\"下面给出几个示例：\",\"离散分布（掷硬币）：设每次抛硬币结果 （1 表示正面），且独立同分布，。若 次抛掷中出现 次正面，则似然函数为 。取对数得 ，对 求导后令导数为零：，解得极大似然估计 。也就是说，硬币正面概率的 MLE 等于正面次数占总次数的比例。\",\"连续分布（正态分布）：假设 独立同分布于 ，似然函数为\",\"对 求导并令偏导数为零可知，MLE 解为样本均值和（无偏调整前的）样本方差：。换言之，在正态分布假设下，MLE 给出了直观的样本统计量作为参数估计。\"]},\"905\":{\"h\":\"信息论: 信息量，熵，交叉熵，KL散度\"},\"906\":{\"h\":\"1.\",\"t\":[\"信息论最核心的问题是：一个事件的发生给我们带来多少“惊讶”或“新信息”？\",\"公式：\",\"这里 是事件 发生的概率。\",\"概率越小 → 事件越罕见 → 信息量越大。\",\"对数底的选择：\",\"：信息单位是 bit（二进制比特）\",\"：信息单位是 nat（自然对数）\",\"例子：\",\"抛硬币得到正面（）：信息量 = bit\",\"抛硬币连续两次都正面（）：信息量 = bit\"]},\"907\":{\"h\":\"2.\",\"t\":[\"熵是平均信息量，用来衡量一个概率分布的不确定性。\",\"公式（离散分布）：\",\"熵越大，分布越“混乱”或不确定。\",\"如果事件概率全一样（均匀分布），熵最大。\",\"如果一个事件概率是 1（确定事件），熵为 0（没有不确定性）。\",\"例子：\",\"公平硬币： bit\",\"偏置硬币（正面 0.9）： bit\"]},\"908\":{\"h\":\"3.\",\"t\":[\"交叉熵衡量用分布 Q 去编码真实来自 P 的事件时的平均信息量。\",\"公式：\",\"解释：\",\"如果 ，交叉熵 = 熵，编码是最优的。\",\"如果 偏离 ，交叉熵会大于熵（编码变长）。\"]},\"909\":{\"h\":\"4.\",\"t\":[\"KL 散度就是交叉熵 - 熵：\",\"交叉熵可以看作是平均信息量 加上因为近似不准而多花的那部分成本，而那部分成本就是 KL 散度。\",\"也可以写成：\",\"含义：\",\"它表示：如果你用 Q 来近似 P，平均每个样本要多花多少信息量（比特 / nat）。\",\"它总是 （Gibbs 不等式），且当 时为 0。\",\"✅ 小结关系图：\",\"信息量 I(x) → 熵 H(P) = 平均 I(x) ↘ 交叉熵 H(P, Q) = 平均 -log Q(x) ↘ KL散度 = H(P, Q) - H(P) 注: 平均是指求期望\"]},\"910\":{\"h\":\"交叉熵损失（Cross-Entropy Loss）\",\"t\":[\"交叉熵损失是一种衡量两个概率分布差异的指标，常用在分类任务中，尤其是二分类和多分类问题。它用来衡量模型预测的概率分布 和真实标签分布 之间的距离。\",\"二分类交叉熵:\",\"对于标签 ，预测概率 ，交叉熵定义为：\",\"解释：\",\"如果真实是正类（），损失就是 ，预测越接近1损失越小。\",\"如果真实是负类（），损失就是 ，预测越接近0损失越小。\",\"多分类交叉熵(对单个样本):\",\"设真实标签是 one-hot 向量 ，预测概率分布是 ，其中 是类别数：\",\"即只对真实类别对应的概率取负对数。\"]},\"911\":{\"h\":\"JS散度（Jensen-Shannon divergence）\"},\"912\":{\"h\":\"1. JS散度是什么？（浅层直观）\",\"t\":[\"JS散度是衡量两个概率分布差异的一个方法。\",\"它是 KL散度的“对称改进版”，所以它总是非负且有限，且满足对称性：\",\"简单说，就是告诉你 和 两个分布相差多远。\"]},\"913\":{\"h\":\"2. 为什么要用JS散度而不是KL散度？\",\"t\":[\"KL散度不对称：\",\"KL散度有时会无穷大（如果 在 支持的区域为0，会导致 不存在）。\",\"JS散度解决了这些问题，变得对称且有界（最大值是 ）。\"]},\"914\":{\"h\":\"3. JS散度的数学定义\",\"t\":[\"给两个概率分布 和 ，定义它们的平均分布：\",\"JS散度定义为：\",\"其中， 是KL散度。\"]},\"915\":{\"h\":\"4. 直观理解JS散度\",\"t\":[\" 是 和 的“中间”分布。\",\"你先测量 与 的差异（KL散度），再测量 与 的差异，取平均。\",\"如果 和 很接近，那么它们都和 很接近，JS散度小。\",\"如果很不一样，JS散度就大。\",\"想象有两个人分别站在一条直线上的不同点，𝑃 和 Q 就是两个人的位置，M 是他们的中间点。你测量两个人到中间点的距离，取平均。这个平均距离越大，说明两个人相距越远。这样就公平、对称地反映了两人距离，而不是单方面去看某一个人的位置。\"]},\"916\":{\"h\":\"5. 举个简单例子\",\"t\":[\"假设：\",\"，\",\"（两个完全不同的分布，互斥事件）。\",\"那么：\",\"所以 \",\"这是最大值，表示两分布差异最大。\"]},\"917\":{\"h\":\"1-Lipschitz 函数\",\"t\":[\"一个函数 如果存在一个常数 （叫 Lipschitz 常数），使得：\",\"对任意 都成立，那么 叫做 K-Lipschitz 函数。\",\"K 越小：函数越平滑，变化越慢。\",\"K 越大：函数变化可能很陡，但仍有限制。\",\"当 时：\",\"这意味着：\",\"输入变化多少，输出的变化量最多等于输入变化量。\",\"相当于限制了函数的最大“斜率”是 1。\",\"想象你走山路：\",\"如果是 1-Lipschitz，走 1 米水平路，海拔最多变 1 米。\",\"如果是 2-Lipschitz，走 1 米水平路，海拔可能变 2 米，更陡。\",\"这个约束能防止函数的变化太快，让它比较“温和”。\"]},\"918\":{\"h\":\"原始 GAN\",\"t\":[\"GAN 是由 Ian Goodfellow 等人在2014年提出的一种生成模型，核心思想是通过两个神经网络之间的“对抗”训练，生成逼真的数据样本。\",\"GAN 里有两个角色：\",\"生成器（Generator，G）：负责从随机噪声生成“假数据”，目的是“骗过”判别器。\",\"判别器（Discriminator，D）：负责判断输入是真实数据还是生成器造出来的假数据。\",\"这两个网络互相对抗，判别器力求识别真假样本，生成器力求生成更逼真的样本“骗过”判别器。 GAN 是一个极大极小游戏，目标函数是：\",\"解释：\",\"：真实数据分布\",\"：随机噪声分布（通常是均匀或高斯）\",\"：判别器给输入 是真实数据的概率\",\"：生成器将噪声 转换成样本\",\"判别器想最大化识别真假的概率，生成器想最小化判别器识别生成样本为假的概率。\",\"传统GAN训练的完整流程:\",\"初始化生成器和判别器网络参数。\",\"训练判别器\",\"采样一批真实样本 。\",\"采样一批噪声 ，生成假样本 。\",\"计算判别器损失：\",\"用梯度下降更新判别器参数，增强它区分真假样本的能力。\",\"训练生成器\",\"再采样一批噪声 ，生成假样本 。\",\"计算生成器损失：\",\"这里生成器的目标是让判别器认为生成样本是真的（输出概率高）。\",\"用梯度下降更新生成器参数，使生成样本更逼真。\",\"重复步骤2和3，交替训练判别器和生成器，直到生成器能够生成看起来很真实的数据。\",\"代码实现:\",\"导包 + 参数准备\",\"import argparse import os import numpy as np import torchvision.transforms as transforms from torchvision.utils import save_image from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch os.makedirs(\\\"images\\\", exist_ok=True) parser = argparse.ArgumentParser() parser.add_argument(\\\"--n_epochs\\\", type=int, default=200, help=\\\"number of epochs of training\\\") parser.add_argument(\\\"--batch_size\\\", type=int, default=64, help=\\\"size of the batches\\\") parser.add_argument(\\\"--lr\\\", type=float, default=0.0002, help=\\\"adam: learning rate\\\") parser.add_argument(\\\"--b1\\\", type=float, default=0.5, help=\\\"adam: decay of first order momentum of gradient\\\") parser.add_argument(\\\"--b2\\\", type=float, default=0.999, help=\\\"adam: decay of first order momentum of gradient\\\") parser.add_argument(\\\"--n_cpu\\\", type=int, default=8, help=\\\"number of cpu threads to use during batch generation\\\") parser.add_argument(\\\"--latent_dim\\\", type=int, default=100, help=\\\"dimensionality of the latent space\\\") parser.add_argument(\\\"--img_size\\\", type=int, default=28, help=\\\"size of each image dimension\\\") parser.add_argument(\\\"--channels\\\", type=int, default=1, help=\\\"number of image channels\\\") parser.add_argument(\\\"--sample_interval\\\", type=int, default=400, help=\\\"interval betwen image samples\\\") opt = parser.parse_args() print(opt) img_shape = (opt.channels, opt.img_size, opt.img_size) cuda = True if torch.cuda.is_available() else False\",\"生成器代码实现\",\"class Generator(nn.Module): def __init__(self): super(Generator, self).__init__() def block(in_feat, out_feat, normalize=True): layers = [nn.Linear(in_feat, out_feat)] if normalize: layers.append(nn.BatchNorm1d(out_feat, 0.8)) layers.append(nn.LeakyReLU(0.2, inplace=True)) return layers self.model = nn.Sequential( *block(opt.latent_dim, 128, normalize=False), *block(128, 256), *block(256, 512), *block(512, 1024), # np.prod 是 NumPy 里的一个函数，用来计算一个数组或元组 所有元素的乘积 nn.Linear(1024, int(np.prod(img_shape))), nn.Tanh() ) def forward(self, z): img = self.model(z) img = img.view(img.size(0), *img_shape) return img\",\"判别器代码实现\",\"class Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.model = nn.Sequential( # np.prod 是 NumPy 里的一个函数，用来计算一个数组或元组 所有元素的乘积 nn.Linear(int(np.prod(img_shape)), 512), nn.LeakyReLU(0.2, inplace=True), nn.Linear(512, 256), nn.LeakyReLU(0.2, inplace=True), nn.Linear(256, 1), nn.Sigmoid(), ) def forward(self, img): img_flat = img.view(img.size(0), -1) validity = self.model(img_flat) return validity\",\"数据，模型，优化器准备\",\"# Loss function adversarial_loss = torch.nn.BCELoss() # Initialize generator and discriminator generator = Generator() discriminator = Discriminator() if cuda: generator.cuda() discriminator.cuda() adversarial_loss.cuda() # Configure data loader os.makedirs(\\\"./data/mnist\\\", exist_ok=True) dataloader = torch.utils.data.DataLoader( datasets.MNIST( \\\"./data/mnist\\\", train=True, download=True, transform=transforms.Compose( [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])] ), ), batch_size=opt.batch_size, shuffle=True, ) # Optimizers optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2)) optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2)) Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\",\"训练\",\"# ---------- # Training # ---------- for epoch in range(opt.n_epochs): for i, (imgs, _) in enumerate(dataloader): # Adversarial ground truths valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False) fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False) # Configure input real_imgs = Variable(imgs.type(Tensor)) # ----------------- # Train Generator # ----------------- optimizer_G.zero_grad() # Sample noise as generator input z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim)))) # Generate a batch of images gen_imgs = generator(z) # Loss measures generator's ability to fool the discriminator g_loss = adversarial_loss(discriminator(gen_imgs), valid) g_loss.backward() optimizer_G.step() # --------------------- # Train Discriminator # --------------------- optimizer_D.zero_grad() # Measure discriminator's ability to classify real from generated samples real_loss = adversarial_loss(discriminator(real_imgs), valid) fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake) d_loss = (real_loss + fake_loss) / 2 d_loss.backward() optimizer_D.step() print( \\\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\\\" % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item()) ) batches_done = epoch * len(dataloader) + i if batches_done % opt.sample_interval == 0: save_image(gen_imgs.data[:25], \\\"images/%d.png\\\" % batches_done, nrow=5, normalize=True)\",\"效果\"]},\"919\":{\"h\":\"推荐资料\",\"t\":[\"互怼的艺术：从零直达WGAN-GP\",\"能量视角下的GAN模型（一）：GAN＝“挖坑”＋“跳坑”\",\"WGAN的成功，可能跟Wasserstein距离没啥关系\"]},\"920\":{\"h\":\"PixelCNN 解读加代码实现\",\"t\":[\"PixelCNN 解读加代码实现\"]},\"921\":{\"h\":\"引言\",\"t\":[\"本文只涉及原始 PixelCNN 模型，不涉及后续的改进版本。 原始论文链接: Pixel Recurrent Neural Networks 改进版本: Conditional Image Generation with PixelCNN Decoders 继续改进: PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications\",\"PixelCNN 借用了 NLP 里的方法来生成图像。模型会根据前 i - 1 个像素输出第 i 个像素的概率分布。训练时，和多分类任务一样，要根据第 i 个像素的真值和预测的概率分布求交叉熵损失函数；采样时，直接从预测的概率分布里采样出第 i 个像素。\",\"这种模型最朴素的实现方法，是输入一幅图像的前 i - 1 个像素，输出第 i 个像素的概率分布，即第 i 个像素取某种颜色的概率的数组。为了方便讨论，我们先只考虑单通道图像，每个像素的颜色取值只有 256 种。因此，准确来说，模型的输出是 256 个经过 softmax 的概率。这样，我们得到了一个 V1.0 版本的模型。\",\"等等，模型不是叫「PixelCNN」吗？CNN 跑哪去了？的确，对于图像数据，最好还是使用 CNN，快捷又有效。因此，我们应该修改模型，令模型的输入为整幅图像和序号 i。我们根据序号 i，过滤掉 i 及 i 之后的像素，用 CNN 处理图像。输出部分还是保持一致。\",\"改进之后，V2.0 版本的模型确实能快速计算第i个像素的概率分布了。可是，CNN 是很擅长同时生成一个和原图像长宽相同的张量的，只算一个像素的概率分布还称不上高效。所以，我们可以让模型输入一幅图像，同时输出图像每一处的概率分布。\",\"这次的改进并不能加速采样。但是，在训练时，由于整幅训练图像已知，我们可以在一次前向传播后得到图像每一处的概率分布。假设图像有N个像素，我们就等于是在并行地训练N个样本，训练速度快了N倍！\",\"V3.0 版本的 PixelCNN 已经和论文里的 PixelCNN 非常接近了，我们来探讨一下网络的实现细节。相比普通的 CNN，PixelCNN 有一个特别的约束：第 i 个像素只能看到前 i-1 个像素的信息，不能看到第 i 个像素及后续像素的信息。对于 V2.0 版本只要输出一个概率分布的 PixelCNN ，我们可以通过一些简单处理过滤掉第 i 个像素之后的信息。而对于并行输出所有概率分布的 V3.0 版本，让每个像素都忽略后续像素的信息的方法就不是那么显然了。\"]},\"922\":{\"h\":\"掩码卷积\"},\"923\":{\"h\":\"空间掩码\",\"t\":[\"PixelCNN 论文里提出了一种掩码卷积机制，这种机制可以巧妙地掩盖住每个像素右侧和下侧的信息。具体来说，PixelCNN 使用了两类掩码卷积，我们把两类掩码卷积分别称为「A类」和「B类」。\",\"二者都是对卷积操作的卷积核做了掩码处理，使得卷积核的右下部分不产生贡献。A 类和 B 类的唯一区别在于卷积核的中心像素是否产生贡献。\",\"PixelCNN 的第一个卷积层使用 A 类掩码卷积，之后每一个卷积层都使用 B 类掩码卷积，如下图所示。\",\"为什么要先用一次 A 类掩码卷积，再每次使用 B 类掩码卷积呢？我们不妨来做一个实验。对于一个 7x7 的图像，我们先用 1 次 3x3 A 类掩码卷积，再用若干次 3x3 B 类掩码卷积。我们观察图像中心处的像素在每次卷积后的感受野（即输入图像中哪些像素的信息能够传递到中心像素上）。\",\"不难看出，经过了第一个 A 类掩码卷积后，每个像素就已经看不到自己位置上的输入信息了。再经过两次 B 类卷积，中心像素能够看到左上角大部分像素的信息。这满足 PixelCNN 的约束。\",\"而如果一直使用 A 类卷积，每次卷积后中心像素都会看漏一些信息（不妨对比下面这张示意图和上面那张示意图）。多卷几层后，中心像素的值就会和输入图像毫无关系。\",\"只是用 B 类卷积也是不行的。显然，如果第一层就使用 B 类卷积，中心像素还是能看到自己位置的输入信息。这打破了 PixelCNN 的约束。这下，我们能明白为什么只能先用一次 A 类卷积，再用若干次 B 类卷积了。\",\"利用两类掩码卷积，PixelCNN 满足了每个像素只能接受之前像素的信息这一约束。除此之外，PixelCNN 就没有什么特别的地方了。我们可以用任意一种 CNN 架构来实现PixelCNN。\"]},\"924\":{\"h\":\"通道掩码\",\"t\":[\"PixelCNN 是一种自回归生成模型，它的目标是在生成图像时，每个像素的值只能依赖于“它前面的像素”。 对于单通道灰度图像，这很简单：\",\"我们使用「空间掩码」，限制卷积时只能看到左边和上方的像素，不能看到当前像素和右下角的像素。\",\"但对于RGB 彩色图像来说，每个像素有 3 个子像素（R、G、B），我们还需要处理一个更复杂的问题：\",\"如何控制同一个像素内部的 R、G、B 三个通道之间的信息流动？\",\"这就引出了两类掩码的配合使用。\",\"掩码类型\",\"控制维度\",\"作用\",\"空间掩码\",\"高度 × 宽度 (h, w)\",\"限制某像素只能看到图像中“更早”的像素\",\"通道掩码\",\"输出通道 × 输入通道 (o, i)\",\"限制 RGB 通道之间的因果关系（如 G 不能看到 B）\",\"我们设卷积核的形状是 [o, i, h, w]，其中：\",\"o：输出通道数\",\"i：输入通道数\",\"h × w：卷积核的空间大小\",\"现在，假设你想生成一张 RGB 图像，我们将所有输入和输出通道都分成 3 组：\",\"通道编号\",\"含义\",\"i1 / o1\",\"对应 R 通道信息\",\"i2 / o2\",\"对应 G 通道信息\",\"i3 / o3\",\"对应 B 通道信息\",\"也就是说：\",\"输入通道 i 被拆分成 i1, i2, i3\",\"输出通道 o 被拆分成 o1, o2, o3\",\"Mask A（用于模型的第一层）：\",\"o1 (R) 不看任何输入（包括 i1/i2/i3）\",\"o2 (G) 只能看 i1 (R)\",\"o3 (B) 只能看 i1 和 i2 (R 和 G)\",\"也就是说，我们在生成当前像素的 R 通道时，不使用任何信息；生成 G 通道时，只能用 R 通道信息；生成 B 通道时，能用 R 和 G。\",\"Mask B（用于后续所有卷积层）：\",\"Mask B 是在 Mask A 的基础上允许通道看到自己当前值，更宽松。\",\"o1 可以看到 i1\",\"o2 可以看到 i1 + i2\",\"o3 可以看到 i1 + i2 + i3\",\"这让模型能更好地捕捉本通道的历史上下文，而不会因为约束太严而训练困难。\",\"空间上的掩码仍然使用传统 PixelCNN 的做法：\",\"只允许卷积核看到左边和上方的像素；\",\"对于 Mask A，还屏蔽掉卷积中心（不能看自己）；\",\"同时应用 Channel Mask A 和 Spatial Mask A\",\"对于 Mask B，保留卷积中心（允许看自己）。\",\"同时应用 Channel Mask B 和 Spatial Mask B\",\"这种掩码设计，能确保 PixelCNN 在生成每个像素时严格遵循「从左上到右下、从 R 到 G 到 B」的顺序，不泄露未来信息。\"]},\"925\":{\"h\":\"思考\",\"t\":[\"PixelCNN的核心思想是给图像的子像素定义一个先后顺序，之后让每个子像素的颜色取值分布由之前所有的子像素决定。实现PixelCNN时，可以用任意一种CNN架构，并注意两点：\",\"网络的输出是一个经softmax的概率分布。\",\"网络的所有卷积层要替换成带掩码的卷积层，第一个卷积层用A类掩码，后面的用B类掩码。\",\"学完了 PixelCNN，我们在闲暇之余来谈一谈 PixelCNN 和其他生成网络的对比情况。精通数学的人，会把图像生成问题看成学习一个图像的分布。每次生成一张图片，就是在图像分布里随机采样一张图。学习一个分布，最便捷的方法是定义一个带参数 的概率模型 ，最大化来自数据集的图像 的概率 。\",\"可问题来了：一个又方便采样，又能计算概率的模型不好设计。VAE 和 Diffusion 建模了把一个来自正态分布的向量变形成 的过程，并使用了统计学里的变分推理，求出了 的一个下界，再设法优化这个下界。GAN 干脆放弃了概率模型，直接拿一个神经网络来评价生成的图像好不好。\",\"PixelCNN 则正面挑战了建立概率模型这一任务。它把 定义为每个子像素出现概率的乘积，而每个子像素的概率仅由它之前的子像素决定。\",\"由于我们可以轻松地用神经网络建模每个子像素的概率分布并完成采样，PixelCNN 的采样也是很方便的。我们可以说 PixelCNN 是一个既方便采样，又能快速地求出图像概率的模型。\",\"相比与其他生成模型，PixelCNN 直接对 建模，在和概率相关的指标上表现优秀。很可惜，能最大化数据集的图像的出现概率，并不代表图像的生成质量就很优秀。因此，一直以来，以 PixelCNN 为代表的对概率直接建模的生成模型没有受到过太多的关注。可能只有少数必须要计算图像概率分布的任务才会用到 PixelCNN。\",\"除了能直接计算图像的概率外，PixelCNN 还有一个大特点：PixelCNN 能输出 离散的颜色值。VAE 和 GAN 这些模型都是把图像的颜色看成一个连续的浮点数，模型的输入和输出的取值范围都位于 -1 到 1 之间（有些模型是 0 到 1 之间）。而 PixelCNN 则输出的是像素取某个颜色的概率分布，它能描述的颜色是有限而确定的。假如我们是在生成 8 位单通道图像，那么网络就只输出 256 个离散的概率分布。能生成离散输出这一特性启发了后续很多生成模型。\",\"另外，这一特性也允许我们指定颜色的亮度级别。例如对于黑白手写数字数据集 MNIST，我们完全可以用黑、白两种颜色来描述图像，而不是非得用 256 个灰度级来描述图像。减少亮度级别后，网络的训练速度能快上很多。\",\"在后续的文献中，PixelCNN 被归类为了自回归生成模型。这是因为 PixelCNN 在生成图像时，要先输入空图像，得到第一个像素；把第一个像素填入空图像，输入进模型，得到第二个像素……也就是说，一个图像被不断地进模型、不断把上一个时刻的输出作为输入。这样用自己之前时刻的状态预测下一个状态的模型，在统计学里被称为自回归模型。\",\"如果你在其他图像生成文献中见到了「自回归模型」这个词，它大概率指的就是 PixelCNN 这种每次生成一个像素，该像素由之前所有像素决定的生成模型。\"]},\"926\":{\"h\":\"MINIST 数据集上的实战测试\",\"t\":[\"由于 MINIST 数据集中的图像都是单通道图像，也就意味着我们只需要实现空间掩码即可，简化了模型的实现细节。\",\"首先给出添加了空间掩码的卷积层实现:\",\"class MaskedCNN(nn.Conv2d): def __init__(self, mask_type, *args, **kwargs): self.mask_type = mask_type assert mask_type in ['A', 'B'], \\\"Unknown Mask Type\\\" # 断言检查 mask 类型必须是 A 或 B super(MaskedCNN, self).__init__(*args, **kwargs) # 调用父类 nn.Conv2d 的初始化函数 # 注册掩码张量为 buffer（不会作为可训练参数），初始值复制自卷积核的权重 self.register_buffer('mask', self.weight.data.clone()) # 获取卷积核尺寸：out_channels, in_channels, kernel_height, kernel_width _, depth, height, width = self.weight.size() # 将掩码全部置为 1（不屏蔽任何位置） self.mask.fill_(1) # 空间掩码设计，遮住当前像素及其右方、下方信息 if mask_type == 'A': # 对于 Mask A，屏蔽中心像素（即当前位置）及其右侧 self.mask[:, :, height // 2, width // 2:] = 0 # 屏蔽中心行上中心及其右侧的列 self.mask[:, :, height // 2 + 1:, :] = 0 # 屏蔽中心行以下所有行 else: # 对于 Mask B，保留中心像素，仅屏蔽其右侧 self.mask[:, :, height // 2, width // 2 + 1:] = 0 # 屏蔽中心行上中心右边的列 self.mask[:, :, height // 2 + 1:, :] = 0 # 屏蔽中心行以下所有行 def forward(self, x): # 每次前向传播时将掩码乘到卷积核上，强制屏蔽掉不允许看的区域 self.weight.data *= self.mask return super(MaskedCNN, self).forward(x)\",\"在每次前向传播前将不允许访问的卷积核位置置零，确保模型每个像素的输出 只能依赖于它左上方的像素（包括或不包括自己）。\",\"有了掩码卷积层后，我们便可以根据常规的卷积模型搭建策略来实现 PixelCNN 模型。\",\"import torch import torch.nn as nn class PixelCNN(nn.Module): \\\"\\\"\\\" PixelCNN 网络结构： 逐像素建模图像像素值的条件概率分布 P(x_i | x_{<i})。 使用 MaskedCNN 层控制像素生成顺序（A型和B型掩码），输出 256 类 softmax 概率。 \\\"\\\"\\\" def __init__(self, no_layers=8, kernel=7, channels=64, device=None): \\\"\\\"\\\" 参数说明： - no_layers: 卷积层总数（不包括输出层） - kernel: 卷积核大小（需为奇数，例如 7x7） - channels: 每层卷积的中间通道数 - device: 模型所在设备（可选） \\\"\\\"\\\" super(PixelCNN, self).__init__() self.no_layers = no_layers self.kernel = kernel self.channels = channels self.device = device # --- 第一层 --- # Mask 类型 A：不允许看到当前像素 # 输入通道为 1（灰度图），输出为 channels 维 # 通过设置 padding 来保证输出尺寸与输入一致 self.Conv2d_1 = MaskedCNN('A', 1, channels, kernel, stride=1, padding=kernel//2, bias=False) self.BatchNorm2d_1 = nn.BatchNorm2d(channels) self.ReLU_1 = nn.ReLU(True) # --- 后续几层 --- # Mask 类型 B：允许看到当前像素，但不允许看到未来像素（保持自回归结构） self.Conv2d_2 = MaskedCNN('B', channels, channels, kernel, 1, kernel//2, bias=False) self.BatchNorm2d_2 = nn.BatchNorm2d(channels) self.ReLU_2 = nn.ReLU(True) self.Conv2d_3 = MaskedCNN('B', channels, channels, kernel, 1, kernel//2, bias=False) self.BatchNorm2d_3 = nn.BatchNorm2d(channels) self.ReLU_3 = nn.ReLU(True) self.Conv2d_4 = MaskedCNN('B', channels, channels, kernel, 1, kernel//2, bias=False) self.BatchNorm2d_4 = nn.BatchNorm2d(channels) self.ReLU_4 = nn.ReLU(True) self.Conv2d_5 = MaskedCNN('B', channels, channels, kernel, 1, kernel//2, bias=False) self.BatchNorm2d_5 = nn.BatchNorm2d(channels) self.ReLU_5 = nn.ReLU(True) self.Conv2d_6 = MaskedCNN('B', channels, channels, kernel, 1, kernel//2, bias=False) self.BatchNorm2d_6 = nn.BatchNorm2d(channels) self.ReLU_6 = nn.ReLU(True) self.Conv2d_7 = MaskedCNN('B', channels, channels, kernel, 1, kernel//2, bias=False) self.BatchNorm2d_7 = nn.BatchNorm2d(channels) self.ReLU_7 = nn.ReLU(True) self.Conv2d_8 = MaskedCNN('B', channels, channels, kernel, 1, kernel//2, bias=False) self.BatchNorm2d_8 = nn.BatchNorm2d(channels) self.ReLU_8 = nn.ReLU(True) # --- 输出层 --- # 不再使用 Mask，使用 1x1 卷积将通道数映射为 256 类别 self.out = nn.Conv2d(channels, 256, kernel_size=1) def forward(self, x): \\\"\\\"\\\" 前向传播函数 输入： - x: shape = [batch_size, 1, height, width] （灰度图） 输出： - logits: shape = [batch_size, 256, height, width] 每个像素位置是一个 256 维的 logits，表示输出的 softmax 分布 \\\"\\\"\\\" x = self.Conv2d_1(x) x = self.BatchNorm2d_1(x) x = self.ReLU_1(x) x = self.Conv2d_2(x) x = self.BatchNorm2d_2(x) x = self.ReLU_2(x) x = self.Conv2d_3(x) x = self.BatchNorm2d_3(x) x = self.ReLU_3(x) x = self.Conv2d_4(x) x = self.BatchNorm2d_4(x) x = self.ReLU_4(x) x = self.Conv2d_5(x) x = self.BatchNorm2d_5(x) x = self.ReLU_5(x) x = self.Conv2d_6(x) x = self.BatchNorm2d_6(x) x = self.ReLU_6(x) x = self.Conv2d_7(x) x = self.BatchNorm2d_7(x) x = self.ReLU_7(x) x = self.Conv2d_8(x) x = self.BatchNorm2d_8(x) x = self.ReLU_8(x) return self.out(x)\",\"训练和无条件生成的代码实现如下所示:\",\"import torch import torch.nn as nn import torch.optim as optim from torchvision.utils import save_image from torchvision import datasets, transforms from torch.utils.data import DataLoader from Model import PixelCNN # 1. 数据预处理 transform = transforms.Compose([ transforms.ToTensor(), # 转为 [0, 1] transforms.Lambda(lambda x: (x * 255).long()) # 转为整数像素值 [0, 255] ]) train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform) train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # 2. 模型实例化 if torch.backends.mps.is_available(): device = torch.device(\\\"mps\\\") elif torch.cuda.is_available(): device = torch.device(\\\"cuda\\\") else: device = torch.device(\\\"cpu\\\") model = PixelCNN(no_layers=8, kernel=7, channels=64, device=device).to(device) # 3. 损失函数与优化器 criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=1e-3) # 4. 训练循环 for epoch in range(10): model.train() total_loss = 0 for batch_idx, (data, _) in enumerate(train_loader): data = data.to(device).float() # [B, 1, 28, 28] 归一化 target = data.squeeze(1).long() # [B, 28, 28] 像素标签 (0~255) # 前向传播 output = model(data / 255.0) # 归一化输入 output = output.permute(0, 2, 3, 1) # [B, H, W, 256] output = output.reshape(-1, 256) # 展平为分类输入 target = target.view(-1) # 展平为分类标签 loss = criterion(output, target) optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() print(f\\\"Epoch [{epoch+1}/10] Loss: {total_loss / len(train_loader):.4f}\\\") # 生成参数 model.eval() image_size = 28 channels = 1 num_samples = 64 device = next(model.parameters()).device # 初始化采样张量（全零） samples = torch.zeros(num_samples, channels, image_size, image_size).to(device) # 像素级逐步采样 with torch.no_grad(): for i in range(image_size): for j in range(image_size): # 前向传播，获得每个像素的概率分布 output = model(samples) # [B, 256, H, W] probs = torch.softmax(output[:, :, i, j], dim=-1) # [B, 256] sampled = torch.multinomial(probs, 1).squeeze(-1) # [B] samples[:, 0, i, j] = sampled.float() / 255.0 # 写入归一化像素值 # 保存生成图像 save_image(samples, \\\"pixelcnn_generated.png\\\", nrow=8, padding=2) print(\\\"图像已生成并保存为 pixelcnn_generated.png\\\")\"]},\"927\":{\"h\":\"摘录\",\"t\":[\"本文摘录至 https://zhuanlan.zhihu.com/p/632209862 , 并对其进行了一些修改。\"]},\"928\":{\"h\":\"Pytorch 实现 VAE 和 CVAE\",\"t\":[\"本文将使用 PyTorch 实现变分自编码器（VAE）和 条件变分自编码器(CVAE)，并在 MNIST 数据集上进行训练与评估\"]},\"929\":{\"h\":\"实现VAE\"},\"930\":{\"h\":\"1. 安装和导入依赖\",\"t\":[\"import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms from torch.utils.data import DataLoader import matplotlib.pyplot as plt\"]},\"931\":{\"h\":\"2. 定义 VAE 模型\",\"t\":[\"class VAE(nn.Module): def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20): super(VAE, self).__init__() # 编码器 self.fc1 = nn.Linear(input_dim, hidden_dim) self.fc_mu = nn.Linear(hidden_dim, latent_dim) self.fc_logvar = nn.Linear(hidden_dim, latent_dim) # 解码器 self.fc2 = nn.Linear(latent_dim, hidden_dim) self.fc3 = nn.Linear(hidden_dim, input_dim) def encode(self, x): h = F.relu(self.fc1(x)) return self.fc_mu(h), self.fc_logvar(h) def reparameterize(self, mu, logvar): std = torch.exp(0.5 * logvar) eps = torch.randn_like(std) return mu + eps * std def decode(self, z): h = F.relu(self.fc2(z)) return torch.sigmoid(self.fc3(h)) def forward(self, x): mu, logvar = self.encode(x) z = self.reparameterize(mu, logvar) return self.decode(z), mu, logvar\",\"重参数化技巧\",\"我们希望从一个高斯分布中采样隐变量：\",\"如果我们直接 z = torch.normal(mu, std) 这样采样，就不能反向传播到 mu 和 logvar，因为随机采样不可微！\",\"重参数化的关键思想: 将不可微的采样操作拆解为一个可微的确定性函数加一个随机变量。\",\"现在采样的是 ε（标准正态，和模型参数无关），而 μ 和 σ 参与的是可导的加法和乘法，可以进行梯度传播。\",\"为什么输出的是 log(σ²)（即对数方差）？\",\"确保方差 σ² 始终为正数\",\"方差必须 > 0，不能为负。但神经网络输出的是无约束的实数 ℝ，所以我们用一个变换把它映射为正数：\",\"无论 logvar 是什么，exp(logvar) 总是 > 0；\",\"避免了人为加 ReLU 或 clamp（数值不连续）等不稳定做法；\",\"模型可以自由学习 logvar ∈ ℝ，无需强行约束。\",\"数值稳定性更高\",\"对数空间中操作更稳定，比如在计算 KL 散度时：\",\"这里的 log σ² 正好可以直接来自模型输出，避免再求 log。\",\"为什么还要乘上 0.5？即 std = exp(0.5 * logvar)\",\"我们想要从 N(μ, σ²) 中重参数化采样，需要：\",\"但模型输出的是 logvar = log(σ²)，所以我们需要：\"]},\"932\":{\"h\":\"3. 定义损失函数（重构损失 + KL散度）\",\"t\":[\"def vae_loss(recon_x, x, mu, logvar): BCE = F.binary_cross_entropy(recon_x, x, reduction='sum') KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) return BCE + KLD\",\"变分自编码器（VAE）训练的核心目标函数，等式右边的两项是:\",\"这就是我们在优化的 变分下界（ELBO），即最大化：\",\"第一项：重建对数似然 , 这项衡量的是：给定隐变量 z，重建样本 X 的能力。在实际中要通过具体分布建模 P(X|z)，并写出其对数形式。\",\"我们假设图像每个像素的值是伯努利分布，并且相互独立。\",\"Step 1：伯努利分布的概率密度函数\",\"对于一个二元变量 ，其伯努利分布定义为：\",\"其中：\",\"：为预测像素点为 1 的概率（即 decoder 输出）\",\"：为真实像素值\",\"Step 2：图像整体建模为像素独立\",\"我们假设图像共有 个像素点，每个像素点是独立的伯努利分布，所以整个图像的条件概率为：\",\"Step 3：取对数得到 log-likelihood\",\"对上式取对数：\",\"这就是我们在 VAE 中用于训练的 重建项！\",\"Step 4：对应到 Binary Cross Entropy（BCE）\",\"这正是 binary cross entropy loss 的形式（取负号）：\",\"在 PyTorch 中：\",\"F.binary_cross_entropy(recon_x, x, reduction='sum') # 就是 -log P(X|z)\",\"第二项：KL 散度项\",\"衡量的是：我们学习的编码器 Q(z|X) 与先验 P(z)（通常为标准正态分布 ）之间的距离。\",\"对于高斯分布，它有一个闭式解：\"]},\"933\":{\"h\":\"4. 数据加载\",\"t\":[\"transform = transforms.ToTensor() train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True) train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\"]},\"934\":{\"h\":\"5. 训练模型\",\"t\":[\"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\") model = VAE().to(device) optimizer = optim.Adam(model.parameters(), lr=1e-3) epochs = 10 model.train() for epoch in range(epochs): train_loss = 0 for batch_idx, (data, _) in enumerate(train_loader): data = data.to(device).view(-1, 784) optimizer.zero_grad() recon_batch, mu, logvar = model(data) loss = vae_loss(recon_batch, data, mu, logvar) loss.backward() train_loss += loss.item() optimizer.step() print(f\\\"Epoch {epoch+1}, Loss: {train_loss / len(train_loader.dataset):.4f}\\\")\"]},\"935\":{\"h\":\"6. 模型评估\",\"t\":[\"model.eval() with torch.no_grad(): sample = next(iter(train_loader))[0].to(device)[:8] sample_flat = sample.view(-1, 784) recon, _, _ = model(sample_flat) recon = recon.view(-1, 1, 28, 28).cpu() # 显示原图和重建图像 fig, axs = plt.subplots(2, 8, figsize=(15, 4)) for i in range(8): axs[0, i].imshow(sample[i].cpu().squeeze(), cmap='gray') axs[0, i].axis('off') axs[1, i].imshow(recon[i].squeeze(), cmap='gray') axs[1, i].axis('off') axs[0, 0].set_title(\\\"Original\\\") axs[1, 0].set_title(\\\"Reconstruction\\\") plt.show()\",\"我们还可以从标准正态分布中采样一个向量 z ~ N(0, I)，然后送入 VAE 的 decode() 方法，生成图像。这是变分自编码器（VAE）最重要的能力之一：生成样本。\",\"VAE 的设计初衷之一就是在潜在空间中学习一个接近于标准正态分布 的分布。训练中，VAE 通过加入 KL 散度项让 q(z|x) 接近于标准正态分布。因此，在推理阶段可以从 中采样 z，并使用 decode(z) 生成新的图像。\",\"你可以在训练完成后加入如下代码来生成图像：\",\"with torch.no_grad(): # 从标准正态分布中采样 8 个 z 向量 z = torch.randn(8, 20).to(device) # 20 是 latent_dim 的大小 generated = model.decode(z).view(-1, 1, 28, 28).cpu() # 可视化生成图像 fig, axs = plt.subplots(1, 8, figsize=(15, 2)) for i in range(8): axs[i].imshow(generated[i].squeeze(), cmap='gray') axs[i].axis('off') plt.suptitle(\\\"Generated Images from z ~ N(0, I)\\\") plt.show()\",\"epoch 改为 100 个后的生成样本效果:\"]},\"936\":{\"h\":\"CVAE 实现\",\"t\":[\"条件变分自编码器（CVAE）是 VAE 的一个变种，它引入了条件变量，以实现对生成样本的控制。比如在 MNIST 中，c 可以是类别标签（0-9），使得模型能够生成指定数字的图像。\",\"CVAE 模型中的每一步都变成 条件化：\",\"部分\",\"普通 VAE\",\"CVAE 版本\",\"编码器输入\",\"解码器输入\",\"输出\",\"没有给出的步骤，均和VAE实现部分保持一致。\"]},\"937\":{\"h\":\"2. 定义 CVAE 模型\",\"t\":[\"class CVAE(nn.Module): def __init__(self, input_dim=784, label_dim=10, hidden_dim=400, latent_dim=20): super(CVAE, self).__init__() self.input_dim = input_dim self.label_dim = label_dim # 编码器：x 和 标签 c 连接 self.fc1 = nn.Linear(input_dim + label_dim, hidden_dim) self.fc_mu = nn.Linear(hidden_dim, latent_dim) self.fc_logvar = nn.Linear(hidden_dim, latent_dim) # 解码器：z 和 标签 c 连接 self.fc2 = nn.Linear(latent_dim + label_dim, hidden_dim) self.fc3 = nn.Linear(hidden_dim, input_dim) def encode(self, x, c): xc = torch.cat([x, c], dim=1) # concat image and condition h = F.relu(self.fc1(xc)) return self.fc_mu(h), self.fc_logvar(h) def reparameterize(self, mu, logvar): std = torch.exp(0.5 * logvar) eps = torch.randn_like(std) return mu + eps * std def decode(self, z, c): zc = torch.cat([z, c], dim=1) h = F.relu(self.fc2(zc)) return torch.sigmoid(self.fc3(h)) def forward(self, x, c): mu, logvar = self.encode(x, c) z = self.reparameterize(mu, logvar) return self.decode(z, c), mu, logvar\"]},\"938\":{\"h\":\"5. 训练过程\",\"t\":[\"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\") model = CVAE().to(device) optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) for epoch in range(10): model.train() train_loss = 0 for x, y in train_loader: x = x.view(-1, 784).to(device) c = F.one_hot(y, num_classes=10).float().to(device) optimizer.zero_grad() recon_x, mu, logvar = model(x, c) loss = loss_function(recon_x, x, mu, logvar) loss.backward() train_loss += loss.item() optimizer.step() print(f\\\"Epoch {epoch+1}, Loss: {train_loss / len(train_loader.dataset):.4f}\\\")\"]},\"939\":{\"h\":\"6. 条件生成图像（指定标签）\",\"t\":[\"model.eval() with torch.no_grad(): z = torch.randn(10, 20).to(device) labels = torch.arange(0, 10).long() c = F.one_hot(labels, num_classes=10).float().to(device) gen_imgs = model.decode(z, c).view(-1, 1, 28, 28).cpu() fig, axs = plt.subplots(1, 10, figsize=(15, 2)) for i in range(10): axs[i].imshow(gen_imgs[i].squeeze(), cmap='gray') axs[i].axis('off') plt.show()\"]},\"940\":{\"h\":\"生成模型学习\"},\"941\":{\"h\":\"Tutorial on Variational Autoencoders 论文\",\"t\":[\"Tutorial on Variational Autoencoders 论文\",\"论文链接: Tutorial on Variational Autoencoders\"]},\"942\":{\"h\":\"引言\",\"t\":[\"生成建模是机器学习中的一个重要领域，目标是建立对数据点 所在的高维空间 上分布 的模型。以图像为例，每张图像包含成千上万个像素，这些像素之间存在复杂的依赖关系，比如相邻像素颜色相似且共同构成物体。生成模型的任务就是捕捉这些像素间的依赖。\",\"模型的具体含义取决于我们想用它完成什么。最基础的生成模型能够数值计算 ，让看起来真实的图像具有较高概率，而随机噪声的图像概率较低。然而，这样的模型不一定实用，因为知道某个图像不太可能出现，并不能帮助我们生成新的、真实的图像。\",\"实际应用中，人们更关心的是能否基于已有数据库，生成更多类似但不完全相同的新样本。比如：\",\"从图像数据库合成新的未见图像；\",\"从三维植物模型生成更多植被以填充游戏场景；\",\"从手写文本生成更多手写内容。\",\"这些生成工具对设计师等用户非常有用。\",\"我们假设数据来自一个未知的真实分布 ，目标是学习一个生成模型 ，使得从 中采样的样本尽量和真实分布相似。\",\"训练这类模型长期以来面临三大难题：\",\"需要对数据结构做强假设；\",\"进行严重近似，导致模型性能不佳；\",\"依赖计算量大、效率低的推断方法，如马尔可夫链蒙特卡洛（MCMC）。\",\"近年来，借助反向传播训练神经网络这一强大函数逼近器的突破，出现了基于神经网络的生成模型框架。其中变分自编码器（VAE）是一种非常流行的方法，具有如下优点：\",\"假设较弱，不强制对数据结构做过多限制；\",\"训练速度快，可通过反向传播高效优化；\",\"引入近似但误差较小，尤其是在高容量模型下。\",\"这些特点使得 VAE 快速流行，成为研究和应用的热门工具。\",\"论文面向对生成模型感兴趣但不熟悉变分贝叶斯方法的读者，主要偏重计算机视觉领域。教程起源于加州大学伯克利分校和卡内基梅隆大学的读书分享。\"]},\"943\":{\"h\":\"预备知识: 潜变量模型\",\"t\":[\"生成模型训练时，维度间的复杂依赖会增加难度。以生成手写数字图像为例，模型若先随机选择一个数字 （潜变量），再生成对应像素，比直接逐像素生成效果更好。潜变量 是指生成过程中的隐藏决策，虽然我们只能看到最终生成的字符，但不知道具体哪个潜变量设置产生了它。\",\"为了让模型能生成接近训练数据的样本，必须确保对数据集中的每个样本 ，存在一个或多个潜变量设置 ，使得模型能生成与 相似的结果。数学上，假设潜变量 服从概率密度 ，通过一个确定性函数族 生成样本。优化目标是调整参数 ，使得从 采样后生成的样本 高概率地类似训练集中的数据。\",\"用概率分布形式表达即为最大化\",\"其中 表示给定潜变量 时生成样本 的条件概率。这种最大似然方法保证模型倾向于生成训练样本及其相似样本，而非随机噪声。\",\"在变分自编码器中，常用的 是均值为 、协方差为 的高斯分布：\",\"这个设计使得模型输出能“软匹配”训练样本，便于通过梯度下降逐渐调整参数，增加数据的生成概率。如果使用确定映射（即 ），优化将难以进行。\",\"输出分布不一定是高斯分布，例如二值数据可用伯努利分布。关键是 需要可计算且关于参数 连续。后续为了简洁，文中将省略函数 中的参数 。\"]},\"944\":{\"h\":\"变分自编码器\",\"t\":[\"虽然 VAE 含有编码器和解码器结构，看起来像传统的自编码器，但它的数学基础与稀疏自编码器或去噪自编码器并不相同。VAE 的目标是近似最大化训练样本的概率：\",\"它最大的优势在于可以直接从中采样，而不像早期方法那样依赖 MCMC 等复杂的采样技术。\",\"VAE 需要解决两个核心问题：\",\"如何定义潜变量 ，即 应该包含哪些信息？\",\"如何计算 中对 的积分？\",\"针对第一个问题，以手写数字为例，一个合理的 不仅要包含数字的类别信息，还要包含书写角度、笔画粗细、风格等。而这些属性往往是相关联的，例如书写快导致角度大、笔画细。\",\"我们不希望人为规定 每个维度的含义或其之间的关系，因此 VAE 采取的策略是：\",\"直接假设 来自一个简单的分布，通常是标准正态分布 。\",\"为什么这样可行？因为任意复杂分布都可以通过把简单分布输入到一个足够复杂的函数中得到。例如：\",\"若 ，则 会产生环形分布（如图2所示）。\",\"所以，使用神经网络等强函数逼近器，我们可以将标准正态的 映射为模型所需的潜在因子空间，再进一步映射为图像 。\",\"在 VAE 中，生成图像的过程被建模为高斯分布：\",\"其中 是多层神经网络。前几层将 映射成潜在属性（例如数字类别、风格等），后几层则将这些属性解码为图像。\",\"这意味着：\",\"网络会自动学习潜在结构，只要这样的结构有助于拟合训练数据（即使最大似然更大）。\",\"现在剩下的问题是如何优化上述积分。一个直接的近似方法是采样多个 ，然后用 Monte Carlo 平均近似 ：\",\"但在高维空间中，这种方式效率极低，需要非常多的样本 才能得到准确估计。\",\"图3 中说明了这个问题的本质：\",\"图3(a)：目标图像 ，我们希望估计其 。\",\"图3(b)：模型生成的图像，看起来不像“2”，质量很差。\",\"图3(c)：仅比 偏移半个像素，看起来很像原图。\",\"问题在于：\",\"图3(c) 与 的欧式距离为 0.2693\",\"图3(b) 与 的欧式距离为 0.0387\",\"由于 是高斯分布，其值由 控制。\",\"如果我们希望图3(b) 对 贡献很小，就必须让 足够小。但这会导致图3(c) 也被“排除”，因为其距离并不小。\",\"设随机向量 ，其分布为维度为 的多元高斯分布，记作：\",\"其概率密度函数为：\",\"在 VAE 里，我们通常假设协方差矩阵是对角的，甚至直接是单位矩阵的倍数：\",\"此时公式简化为：\",\"其中 是欧几里得距离的平方，表示 与生成样本的距离。\",\"现存问题（如图3所示）：\",\"要排除错误样本（图3b），需要 非常小\",\"但这样也会排除相似的好样本（图3c）\",\"采样效率低，需大量 才能采到接近目标图像的样本\",\"虽然可以尝试用更好的相似性度量代替欧氏距离，但在复杂视觉任务中，这类度量难以设计，也难以训练。为此，VAE 选择不改变相似性度量，而是优化采样方式，以提高效率。这为后续的变分推断方法奠定了基础。\"]},\"945\":{\"h\":\"构建目标函数\",\"t\":[\"当我们尝试通过采样计算公式 (1) 中的 时，有没有捷径可走呢？实际上，对于大多数 值， 都会非常接近于 0，因此对 的估计几乎没有贡献。\",\"变分自编码器（VAE）的关键思想是：尝试只采样那些很可能生成 的 值，并利用这些 来估计 。为此，我们引入一个新的分布函数 ，它能够根据输入 生成一组可能生成 的潜变量 。希望这个 下高概率的 空间远小于先验分布 下高概率的区域。\",\"这使得我们可以更容易地计算：\",\"但问题来了：如果 是从一个任意分布 （不是标准正态分布 ）中采样的，我们该如何优化 呢？\",\"我们先从变分贝叶斯方法的基础出发：Kullback-Leibler 散度（KL 散度），它衡量两个概率分布之间的距离。我们用它来衡量 和后验分布 之间的差异：\",\"为了将 和 引入这个式子，我们对 应用贝叶斯公式，有：\",\"注意： 不依赖于 ，因此可以移出期望符号。\",\"我们从 KL 散度的定义开始：\",\"接下来使用 贝叶斯公式 展开 ：\",\"带入上式中：\",\"对右侧进行对数展开：\",\"将其带入 KL 散度的期望中：\",\"因为 与 无关，是一个常数，可以移出期望符号：\",\"最终得到目标公式：\",\"接着，我们将等式两边都取负号、重新排列，并将期望中的部分转换为另一个 KL 散度，从而得到：\",\"其中：\",\"左边是我们想要的目标 减去一个误差项（让 逼近真实后验分布 ）；\",\"右边则是我们可以实际优化的目标。\",\"由于我们关心的是给定样本 的 ，所以我们自然希望选择一个依赖于 的 ，并让其尽可能逼近 。于是有：\",\"这个公式（式 5）正是变分自编码器的核心思想，非常值得深入理解：\",\"左边的 是我们希望最大化的目标；\",\"减去的 KL 散度项会鼓励 生成那些可以还原 的 ；\",\"如果我们能让 ，那么这个误差项趋近于 0，我们就真正最大化了 ；\",\"更重要的是，这样一来我们就绕过了后验分布 无法精确计算的问题，用 代替它。\",\"同时，右边的结构也变得非常像一个自编码器： 像编码器（encoder）将 映射到 ，而 像解码器（decoder）将 还原为 。\",\"我们会在后续进一步探索这种“看起来像自编码器”的结构。\"]},\"946\":{\"h\":\"优化目标函数\",\"t\":[\"那么我们该如何对公式（5）右边的目标函数使用随机梯度下降进行优化呢？首先，我们需要更具体地定义 的形式。通常的选择是将其设为：\",\"其中 和 是带参数 的任意可学习的确定性函数（后续公式中将省略 以简化表示）。在实际中， 和 都由神经网络实现，并且通常会约束 为对角矩阵。\",\"这样的设定具有计算上的优势，因为它可以明确地定义如何计算公式右边的两项。最后一项 现在就变成了两个多元高斯分布之间的 KL 散度，其可以写成如下闭式表达：\",\"其中 表示分布的维度。\",\"在我们的设定中， ，因此这个公式可以简化为：\",\"现在来看公式（5）右边的第一项： 。我们可以使用采样来估计这个期望，但要获得准确的估计，就需要多次对 采样并输入解码器 ，这在计算上代价很高。\",\"因此，和随机梯度下降中常用的做法一样，我们通常只对 采样一次，并用该 下的 作为对 的近似。\",\"毕竟，我们已经在对从数据集 中采样得到的不同 执行随机梯度下降。\",\"最终我们要优化的目标是：\",\"若我们对该公式求梯度，由于期望的线性性质，可以将梯度操作符移入期望内。这样我们只需采样一个 和一个从 得到的 ，然后计算以下表达式的梯度：\",\"我们可以对多个样本的 和 计算该函数的梯度并进行平均，最终会收敛到公式（8）的真实梯度。\",\"然而，公式（9）存在一个重要问题： 不仅依赖于生成器 的参数，还依赖于编码器 的参数，但在公式（9）中这种依赖“消失了”。\",\"为了让 VAE 正常工作，我们必须推动 学会生成让 能够成功解码的 编码结果。\",\"换种方式理解这个问题：公式（9）描述的神经网络结构类似图4左侧所示的网络。其前向传播没有问题，若对多个 和 样本取平均，其输出就是期望值。但我们还需要将误差反向传播到一个从 中采样的 ，而“采样”这一操作本质上是非连续的、不可微的。\",\"虽然反向传播可以应对带噪输入（例如 dropout），但无法直接穿过一个采样操作的节点反向传播梯度。\",\"解决方案来自 Kingma 等人在 [1] 中提出的 “重参数化技巧”（reparameterization trick）：\",\"我们将采样从网络内部移动到输入层。具体做法是：\",\"先从标准正态分布采样 \",\"再计算 \",\"这样我们就可以将期望写为如下形式：\",\"该结构如图4右侧所示。\",\"注意：现在所有的期望都关于不依赖模型参数的分布，因此我们可以安全地将梯度符号移入期望中而不会影响等式成立。换言之，在固定 和 的情况下，该函数对 和 的参数是连续且可导的，所以反向传播可以计算出有效梯度用于随机梯度下降。\",\"需要指出的一点是：重参数化技巧仅适用于连续变量的分布。为了能写成 ，其中 是不学习的随机变量， 是一个关于 的连续函数，才能使其可微。\",\"因此，如果 或 是离散分布，就无法使用该技巧。因为在这种情况下，函数 要么忽略 ，要么在某处发生跳跃，即出现不连续性，这会导致无法对模型参数求导。\"]},\"947\":{\"h\":\"测试已学习的模型\",\"t\":[\"在测试阶段，当我们希望从训练好的模型中生成新样本时，过程非常简单：只需从标准正态分布中采样 ，然后将这个 输入到解码器中即可生成图像。这时我们不再使用编码器，也不会进行变换（如均值和方差的加权），直接使用标准高斯采样值。这个测试时的网络结构在图5中有示意。\",\"当我们希望评估某个测试样本 的似然概率 时，事情会变得复杂。因为真实的 无法直接计算。不过我们知道 KL 散度 是非负的，因此，公式 (5) 的右侧是 的下界：\",\"尽管这个下界仍然包含一个关于 的期望项，无法闭式求解，但可以通过对 进行采样来估计。\",\"与直接从 采样相比，从 中采样可以提供更快、更稳定的估计收敛。这种下界估计虽然不是完全精确，但可以作为判断模型是否捕捉到了某个特定数据点 的一种有用指标。\",\"因此，在测试阶段，直接从标准正态分布采样 并输入解码器就能生成图像；而当需要评估样本概率时，可以使用训练过程中的变分下界（ELBO）作为近似指标。\",\"在 训练阶段，VAE 的工作流程是这样的：\",\"给一个真实样本 （比如一张图像）。\",\"用 编码器 生成潜在变量 的分布参数：均值 和方差 。\",\"从这个分布中采样出一个 （使用 reparameterization trick）。\",\"用 解码器 根据 生成图像 。\",\"同时优化两个目标：重建损失（ 与 的相似程度） + KL 散度（让 尽量接近 ）。\",\"训练完成后，编码器的目标是让 尽可能逼近标准正态分布 。也就是说，模型已经学会了将数据编码到一个“标准正态空间”中。 因此在 测试时，我们就不需要真实的 了。我们可以：\",\"直接从 中采样一个 （比如 ）\",\"把这个 传给 训练好的解码器\",\"解码器会输出一个图像，这就是一个新的、合理的样本\",\"因为 VAE 在训练过程中不断把 分布“推近”标准正态分布，所以训练好的解码器知道：从标准正态分布中采样 ，应该能生成“像样”的图像。这也是变分推断的精髓所在。 原始的变分自编码器（VAE） 只能学习如何根据某种隐变量 来重建训练数据 ，并不能直接学会如何根据文本或其他条件信息来生成对应图像。\"]},\"948\":{\"h\":\"解读目标函数\",\"t\":[\"到目前为止，你应该已经相信 VAE 的学习过程是可行的，并且它确实在对整个数据集上的 进行某种形式的优化。然而，我们实际上并没有在精确地优化 ，因此本节旨在更深入地理解这个目标函数究竟在做什么。\",\"我们将讨论三个关键主题：\",\"首先，我们会探讨：在优化 的同时，还优化了 ，这一步到底引入了多少误差？\",\"接着，我们会从信息论（Information Theory）的角度描述 VAE 框架，特别是分析等式 (5) 右侧的表达式，并将其与基于最小描述长度（Minimum Description Length，简称 MDL）等其他方法联系起来。\",\"最后，我们会调查：VAE 是否拥有类似于稀疏自编码器中稀疏性惩罚项那样的“正则化参数”，也就是说，是否存在一种类似于手动控制模型约束程度的机制？\"]},\"949\":{\"h\":\"项引入的误差分析\",\"t\":[\"本模型之所以具备可计算性，依赖于我们作出的一个假设：即 可以被建模为具有某个均值 和方差 的高斯分布。如果 ，那么 （即模型生成的分布）才会在分布意义上收敛于真实分布。\",\"然而，确保 收敛为零并不是一件容易的事。即使我们假设 和 具有任意高的表达能力，后验分布 在一般情况下也不一定是高斯分布，尤其是当我们用任意函数 去定义 时。\",\"如果 是固定的，这可能意味着 永远不会为零。\",\"如果 固定，且对应的 不是高斯分布，那么我们用高斯近似的 来学习会引入误差，导致从标准正态分布采样得到的 经 decoder 生成的图像 难以还原原始数据。\",\"不过，好消息是，只要我们拥有足够高容量（high-capacity）的神经网络，那么就存在很多种函数 ，它们都可以生成我们希望逼近的输出分布。而这些函数都可以同样好地最大化 。因此，我们只需要找到一种函数 ，它既能最大化 ，又能让 对所有 都是高斯分布。\",\"这里提到的函数 是指 decoder 端的神经网络，即从潜在变量 映射到图像 的那个函数。\",\"如果这样的函数存在，那么 KL 散度 就会推动模型向这一特定参数化方式靠拢。\",\"那么，是否对我们想要逼近的所有分布都存在这样一种函数 呢？目前为止，作者并不清楚是否已经有任何人从一般意义上证明了这一点。\",\"但幸运的是，作者指出，在某些特定情形下，可以证明这样的函数确实存在：前提是标准差 相较于真实分布累积分布函数（CDF）的曲率来说足够小（至少在一维情形下）。相关证明可见附录 A。\",\"虽然在实际中这样的小 可能会带来问题，比如导致梯度尺度失衡，从而影响现有的机器学习算法，但至少我们知道，在这个特定条件下，VAE 的近似误差可以为零。这一点提供了理论上的安慰。\",\"这一发现也暗示，未来的理论研究有望揭示 VAE 在更实际设定下到底存在多少近似误差。作者认为，附录 A 中的证明方法可能可以推广到多维情形，但这部分工作尚留待未来完成。\"]},\"950\":{\"h\":\"信息论视角\",\"t\":[\"我们可以从信息论的角度，特别是“最小描述长度（Minimum Description Length, MDL）”的原则，来理解公式（5）右边的意义。这种视角也正是许多 VAE 的前辈模型（如 Helmholtz 机器 [16]、Wake-Sleep 算法 [17]、深度置信网络 [18] 和玻尔兹曼机 [19]）的动机来源。\",\"在信息论中， 可以看作是用理想编码方式构建给定样本 所需的总比特数。公式（5）右侧将这个编码过程拆分为两个步骤：\",\"第一步是构建潜变量 ，这需要一定的信息量。请注意，KL 散度的单位就是比特（更准确地说是 nat）。具体地， 表示：为了将一个无信息的样本（来自先验分布 ）转化为更有信息的样本（来自后验近似 ），所需的平均信息量。这种解释也叫作 KL 散度的“信息增益”视角。换句话说，它度量了：与从 中采样相比，从 中采样可以获得多少关于 的额外信息（更多内容可参考文献 [20, 21] 中的 “bits back” 理论）。\",\"第二步是利用 还原出 。 描述了：在给定 的情况下，重建 所需的信息量（即压缩 还原所需的最小比特数）。\",\"因此，总的信息量（）等于上述两个步骤的总和：即\",\"不过这里要注意一点：由于我们用的是近似的编码分布 ，它不是理想的后验 ，所以还会有一个误差项 ，代表我们因为编码器不完美而付出的额外“编码代价”。\"]},\"951\":{\"h\":\"VAE 与正则化参数\",\"t\":[\"从公式（5）的角度看，将 看作一个正则化项是很有意思的，这与稀疏自编码器（sparse autoencoders）中的稀疏正则化很相似 [10]。从这个视角出发，我们可以思考一个问题：变分自编码器是否也存在某种“正则化参数”？\",\"在稀疏自编码器的目标函数中，确实存在一个正则化参数 ，该目标函数的形式如下：\",\"其中 和 分别是编码器和解码器函数， 是 范数，用于鼓励编码结果稀疏。这个 是需要手动设置的。\",\"相比之下，变分自编码器（VAE）通常并没有这样的正则化参数，这其实是一个好处：程序员就少了一个需要调节的超参数。\",\"但在某些模型中，我们可以人为“制造出”一个看起来像正则化参数的东西。有些人可能会认为：通过将潜变量的分布从 改为 ，就可以引入一个类似 的调节参数。\",\"然而，事实证明，这种修改不会改变模型本质。原因如下：\",\"我们可以通过以下重参数方式，把这个 吸收到 和 中：\",\"令 \",\"令 \",\"令 \",\"这样处理之后，右侧的损失函数值（也就是公式5的右边）将与原来 的情况完全一致。此外，用来从模型中采样 的方式也没变，因为 。换句话说，这种重参数化只是数学上的变换，不会引入真正意义上的正则化因子。\",\"但还有另一种方式可以引入“正则化参数”的作用。回忆一下：对于连续型数据，一个比较合适的输出分布是：\",\"其中 是我们人为设定的一个值。因此，重构损失项可以写作：\",\"这里的 是一个常数，与 无关，所以在优化过程中可以忽略。\",\"这意味着在 VAE 的完整优化目标中， 只出现在公式右边的第二项中，而不会影响 KL 散度项。从这个意义上来说， 的作用就像是一个正则化参数 ，它控制着两个损失项之间的权重。\",\"不过，这种参数的存在依赖于我们对 的分布假设。\",\"如果 是连续变量，我们就会使用上述的高斯分布，并引入 来控制建模精度。\",\"但如果 是二值变量，我们会使用 Bernoulli（伯努利）分布作为输出模型。在这种情况下，正则化参数 就消失了。\",\"如果仍然希望引入一个调节项，那只能靠“技巧”——比如把 的维度复制多次（相当于人为扩大重构误差项的权重）。\",\"从信息论的角度看，这种现象是合理的：\",\"当 是二值变量时，我们可以明确地“数清楚”编码 所需的比特数，两个损失项的单位是一致的（都是比特）。\",\"但当 是连续变量时，每个样本理论上都包含“无限信息”。在这种情况下，我们必须设定 ，以决定我们允许模型“多不精确”，这样才能让总的信息量变得有限（否则就永远不能完全编码一个实数向量）。\"]},\"952\":{\"h\":\"条件变分自编码器\",\"t\":[\"让我们回到之前生成手写数字的例子。假设我们不仅想生成新的数字，而是想在一个由同一个人写成的数字串中添加数字。这类似于计算机图形学中的一个实际问题——“补洞”：给定一张用户已经去除了不需要物体的图像，目标是用看起来合理的像素填补这个空洞。\",\"这两个问题的一个重要难点是，合理输出的空间是多模态的：下一个数字或者补充的像素有许多可能。标准的回归模型在这种情况下会失败，因为训练目标通常会惩罚预测结果与真实结果之间的距离。在面对这种问题时，回归模型的最佳结果往往是“介于多种可能之间”的某种平均值。就数字生成而言，这通常表现为模糊无意义的“平均图像”，融合了所有可能的数字和各种可能的书写风格。\",\"我们需要的是一种算法，能输入一个字符串或图像，输出一个复杂的、多模态的概率分布，并从中采样。条件变分自编码器（CVAE）应运而生，它通过简单地对整个生成过程加上条件限制来修改上一节中的数学模型。\",\"CVAE使我们能够解决输入到输出是一对多映射的问题，而无需显式指定输出分布的结构。\",\"给定输入 和输出 ，我们希望建立一个模型 ，最大化真实数据的概率（这里为了与标准机器学习的习惯保持一致，重新定义了 ）。我们通过引入潜变量 来定义模型：\",\"其中， 是一个我们可以从数据中学习的确定性函数。\",\"我们可以将之前的公式（公式2到5）改写为条件形式：\",\"注意， 仍然是标准正态分布 ，因为我们的模型假设在测试时 与 独立采样。\",\"该模型的结构如图6所示。测试时，我们只需从 采样，即可得到 的样本。\"]},\"953\":{\"h\":\"推荐阅读\",\"t\":[\"Introduction: Variational Auto - Encoder\"]},\"954\":{\"h\":\"VQ-VAE 论文解读与代码实现\",\"t\":[\"VQ-VAE 论文解读与代码实现\"]},\"955\":{\"h\":\"引言\",\"t\":[\"近两年，有许多图像生成类任务的前沿工作都使用了一种叫做\\\"codebook\\\"的机制。追溯起来，codebook机制最早是在VQ-VAE论文中提出的。相比于普通的VAE，VQ-VAE能利用codebook机制把图像编码成离散向量，为图像生成类任务提供了一种新的思路。VQ-VAE的这种建模方法启发了无数的后续工作，包括声名远扬的Stable Diffusion。\",\"在这篇文章中，我将先以易懂的逻辑带领大家一步一步领悟VQ-VAE的核心思想，再介绍VQ-VAE中关键算法的具体形式，最后把VQ-VAE的贡献及其对其他工作的影响做一个总结。通过阅读这篇文章，你不仅能理解VQ-VAE本身的原理，更能知道如何将VQ-VAE中的核心机制活学活用。\"]},\"956\":{\"h\":\"从 AE 到 VQ-VAE\",\"t\":[\"为什么VQ-VAE想要把图像编码成离散向量？让我们从最早的自编码器（Autoencoder, AE）开始一步一步谈起。AE是一类能够把图片压缩成较短的向量的神经网络模型，其结构如下图所示。AE包含一个编码器 和一个解码器。在训练时，输入图像 会被编码成一个较短的向量 ，再被解码回另一幅长得差不多的图像 。网络的学习目标是让重建出来的图像 和原图像 尽可能相似。\",\"解码器可以把一个向量解码成图片。换一个角度看，解码器就是一个图像生成模型，因为它可以根据向量来生成图片。那么，AE可不可以用来做图像生成呢？很可惜，AE的编码器编码出来的向量空间是不规整的。也就是说，解码器只认识经编码器编出来的向量，而不认识其他的向量。如果你把自己随机生成出来的向量输入给解码器，解码器是生成不出有意义的图片的。AE不能够随机生成图片，所以它不能很好地完成图像生成任务，只能起到把图像压缩的作用。\",\"AE离图像生成只差一步了。只要AE的编码空间比较规整，符合某个简单的数学分布（比如最常见的标准正态分布），那我们就可以从这个分布里随机采样向量，再让解码器根据这个向量来完成随机图片生成了。VAE就是这样一种改进版的AE。它用一些巧妙的方法约束了编码向量，使得满足标准正态分布。这样，解码器不仅认识编码器编出的向量，还认识其他来自标准正态分布的向量。训练完成后，我们就可以扔掉编码器，用来自标准正态分布的随机向量和解码器来实现随机图像生成了。\",\"VAE的实现细节就不在这里赘述了，是否理解它对理解VQ-VAE没有影响。我们只需知道VAE可以把图片编码成符合标准正态分布的向量即可。让向量符合标准正态分布的原因是方便随机采样。同时，需要强调的是，VAE编码出来的向量是连续向量，也就是向量的每一维都是浮点数。如果把向量的某一维稍微改动0.0001，解码器还是认得这个向量，并且会生成一张和原向量对应图片差不多的图片。\",\"但是，VAE生成出来的图片都不是很好看。VQ-VAE的作者认为，VAE的生成图片之所以质量不高，是因为图片被编码成了连续向量。而实际上，把图片编码成离散向量会更加自然。比如我们想让画家画一个人，我们会说这个是男是女，年龄是偏老还是偏年轻，体型是胖还是壮，而不会说这个人性别是0.5，年龄是0.6，体型是0.7。因此，VQ-VAE会把图片编码成离散向量，如下图所示。\",\"把图像编码成离散向量后，又会带来两个新的问题。第一个问题是，神经网络会默认输入满足一个连续的分布，而不善于处理离散的输入。如果你直接输入0, 1, 2这些数字，神经网络会默认1是一个处于0, 2中间的一种状态。为了解决这一问题，我们可以借鉴NLP中对于离散单词的处理方法。为了处理离散的输入单词，NLP模型的第一层一般都是词嵌入层，它可以把每个输入单词都映射到一个独一无二的连续向量上。这样，每个离散的数字都变成了一个特别的连续向量了。\",\"我们可以把类似的嵌入层加到VQ-VAE的解码器前。这个嵌入层在VQ-VAE里叫做\\\"embedding space（嵌入空间）\\\"，在后续文章中则被称作\\\"codebook\\\"。\",\"离散向量的另一个问题是它不好采样。回忆一下，VAE之所以把图片编码成符合正态分布的连续向量，就是为了能在图像生成时把编码器扔掉，让随机采样出的向量也能通过解码器变成图片。现在倒好，VQ-VAE把图片编码了一个离散向量，这个离散向量构成的空间是不好采样的。VQ-VAE不是面临着和AE一样的问题嘛。\",\"这个问题是无解的。没错！VQ-VAE根本不是一个图像生成模型。它和AE一样，只能很好地完成图像压缩，把图像变成一个短得多的向量，而不支持随机图像生成。VQ-VAE和AE的唯一区别，就是VQ-VAE会编码出离散向量，而AE会编码出连续向量。\",\"可为什么VQ-VAE会被归类到图像生成模型中呢？这是因为VQ-VAE的作者利用VQ-VAE能编码离散向量的特性，使用了一种特别的方法对VQ-VAE的离散编码空间采样。VQ-VAE的作者之前设计了一种图像生成网络，叫做PixelCNN。PixelCNN能拟合一个离散的分布。比如对于图像，PixelCNN能输出某个像素的某个颜色通道取0~255中某个值的概率分布。这不刚好嘛，VQ-VAE也是把图像编码成离散向量。换个更好理解的说法，VQ-VAE能把图像映射成一个「小图像」。我们可以把PixelCNN生成图像的方法搬过来，让PixelCNN学习生成「小图像」。这样，我们就可以用PixelCNN生成离散编码，再利用VQ-VAE的解码器把离散编码变成图像。\",\"让我们来整理一下VQ-VAE的工作过程。\",\"训练VQ-VAE的编码器和解码器，使得VQ-VAE能把图像变成「小图像」，也能把「小图像」变回图像。\",\"训练PixelCNN，让它学习怎么生成「小图像」。\",\"随机采样时，先用PixelCNN采样出「小图像」，再用VQ-VAE把「小图像」翻译成最终的生成图像。\",\"到这里，我们已经学完了VQ-VAE的核心思想。让我们来总结一下。VQ-VAE不是一个VAE，而是一个AE。它的目的是把图像压缩成离散向量。或者换个角度说，它提供了把大图像翻译成「小图像」的方法，也提供了把「小图像」翻译成大图像的方法。这样，一个随机生成大图像的问题，就被转换成了一个等价的随机生成一个较小的「图像」的问题。有一些图像生成模型，比如PixelCNN，更适合拟合离散分布。可以用它们来完成生成「小图像」的问题，填补上VQ-VAE生成图片的最后一片空缺。\"]},\"957\":{\"h\":\"VQ-VAE 实现细节\",\"t\":[\"在上一节中，我们虽然认识了VQ-VAE的核心思想，但略过了不少实现细节，比如：\",\"VQ-VAE的编码器怎么输出离散向量。\",\"VQ-VAE怎么优化编码器和解码器。\",\"VQ-VAE怎么优化嵌入空间。\",\"在这一节里，我们来详细探究这些细节。\"]},\"958\":{\"h\":\"输出离散编码\",\"t\":[\"想让神经网络输出一个整数，最简单的方法是和多分类模型一样，输出一个Softmax过的概率分布。之后，从概率分布里随机采样一个类别，这个类别的序号就是我们想要的整数。比如在下图中，我们想得到一个由3个整数构成的离散编码，就应该让编码器输出3组logit，再经过Softmax与采样，得到3个整数。\",\"但是，这么做不是最高效的。得到离散编码后，下一步我们又要根据嵌入空间把离散编码转回一个向量。可见，获取离散编码这一步有一点多余。能不能把编码器的输出张量（它之前的名字叫logit）、解码器的输入张量embedding、嵌入空间直接关联起来呢？\",\"VQ-VAE 使用了如下方式关联编码器的输出与解码器的输入：假设嵌入空间已经训练完毕，对于编码器的每个输出向量 ，找出它在嵌入空间里的最近邻 ，把 替换成 作为解码器的输入。\",\"求最近邻，即先计算向量与嵌入空间 个向量每个向量的距离，再对距离数组取一个 argmin，求出最近的下标（比如图中的 ），最后用下标去嵌入空间里取向量。下标构成的数组（比如图中的 ）也正是 VQ-VAE 的离散编码。\",\"就这样，我们知道了 VQ-VAE 是怎么生成离散编码的。VQ-VAE 的编码器其实不会显式地输出离散编码，而是输出了多个「假嵌入」 。之后，VQ-VAE 对每个 在嵌入空间里找最近邻，得到真正的嵌入 ，把 作为解码器的输入。\",\"虽然我们现在能把编码器和解码器拼接到一起，但现在又多出了一个问题：怎么让梯度从解码器的输入 传到 ? 从 到 的变换是一个从数组里取值的操作，这个操作是求不了导的。我们在下一小节里来详细探究一下怎么优化 VQ-VAE 的编码器和解码器。\"]},\"959\":{\"h\":\"优化编码器和解码器\",\"t\":[\"为了优化编码器和解码器，我们先来制订一下 VQ-VAE 的整体优化目标。由于 VQ-VAE 其实是一个 AE，误差函数里应该只有原图像和目标图像的重建误差。\",\"或者非要从 VAE 的角度说也行。VQ-VAE 相当于输出了一个 one-hot 离散分布。假设输入图像 的离散编码 是 ，则分布中仅有 ，。令离散编码 的先验分布是均匀分布（假设不知道输入图像 ，每个离散编码取到的概率是等同的），则先验分布 和后验分布 的 KL 散度是常量。因此，KL 散度项不用算入损失函数里。理解此处的数学推导意义不大，还不如直接理解成 VQ-VAE 其实是一个 AE。\",\"但直接拿这个误差来训练是不行的。误差中， 是解码器的输入。从编码器输出 到 这一步是不可导的，误差无法从解码器传递到编码器上。要是可以把 的梯度直接原封不动地复制到 上就好了。\",\"VQ-VAE 使用了一种叫做\\\"straight-through estimator\\\"的技术来完成梯度复制。这种技术是说，前向传播和反向传播的计算可以不对应。你可以为一个运算随意设计求梯度的方法。基于这一技术，VQ-VAE 使用了一种叫做 （stop gradient，停止梯度）的运算：\",\"也就是说，前向传播时，sg 里的值不变；反向传播时，sg 按值为 0 求导，即此次计算无梯度。（反向传播其实不会用到式子的值，只会用到式子的梯度。反向传播用到的 loss 值是在前向传播中算的）。\",\"基于这种运算，我们可以设计一个把梯度从 复制到 的误差：\",\"也就是说，前向传播时，就是拿解码器输入 来算梯度。\",\"而反向传播时，按下面这个公式求梯度，等价于把解码器的梯度全部传给 。\",\"这部分的 PyTorch 实现如下所示。在 PyTorch 里，(x).detach() 就是 ，它的值在前向传播时取 ，反向传播时取 。\",\"1 L = x - decoder(z_e + (z_q - z_e).detach())\",\"通过这一技巧，我们完成了梯度的传递，可以正常地训练编码器和解码器了。\"]},\"960\":{\"h\":\"优化嵌入空间\",\"t\":[\"到目前为止，我们的讨论都是建立在嵌入空间已经训练完毕的前提上的。现在，我们来讨论一下嵌入空间的训练方法。\",\"嵌入空间的优化目标是什么呢？嵌入空间的每一个向量应该能概括一类编码器输出的向量，比如一个表示「青年」的向量应该能概括所有 14–35 岁的人的照片的编码器输出。因此，嵌入空间的向量应该和其对应编码器输出尽可能接近。如下面的公式所示， 是编码器的输出向量， 是其在嵌入空间的最近邻向量。\",\"但作者认为，编码器和嵌入向量的学习速度应该不一样快。于是，他们再次使用了停止梯度的技术，把上面那个误差函数拆成了两部分。其中， 控制了编码器的相对学习速度。作者发现，算法对 的变化不敏感， 取 0.1~2.0 都差不多。\",\"其实，在论文中，作者分别讨论了上面公式里的两个误差。第一个误差来自字典学习算法里的经典算法 Vector Quantisation (VQ)，也就是 VQ-VAE 里的那个 VQ，它用于优化嵌入空间。第二个误差叫做专注误差，它用于约束编码器的输出，不让它跑到离嵌入空间里的向量太远的地方。\",\"这样，VQ-VAE 总体的损失函数可以写成：（由于算上了重建误差，我们多加一个 用于控制不同误差之间的比例）\"]},\"961\":{\"h\":\"总结\",\"t\":[\"VQ-VAE是一个把图像编码成离散向量的图像压缩模型。为了让神经网络理解离散编码，VQ-VAE借鉴了NLP的思想，让每个离散编码值对应一个嵌入，所有的嵌入都存储在一个嵌入空间（又称”codebook”）里。这样，VQ-VAE编码器的输出是若干个「假嵌入」，「假嵌入」会被替换成嵌入空间里最近的真嵌入，输入进解码器里。\",\"VQ-VAE的优化目标由两部分组成：重建误差和嵌入空间误差。重建误差为输入图片和重建图片的均方误差。为了让梯度从解码器传到编码器，作者使用了一种巧妙的停止梯度算子，让正向传播和反向传播按照不同的方式计算。嵌入空间误差为嵌入和其对应的编码器输出的均方误差。为了让嵌入和编码器以不同的速度优化，作者再次使用了停止梯度算子，把嵌入的更新和编码器的更新分开计算。\",\"训练完成后，为了实现随机图像生成，需要对VQ-VAE的离散分布采样，再把采样出来的离散向量对应的嵌入输入进解码器。VQ-VAE论文使用了PixelCNN来采样离散分布。实际上，PixelCNN不是唯一一种可用的拟合离散分布的模型。我们可以把它换成Transformer，甚至是diffusion模型。如果你当年看完VQ-VAE后立刻把PixelCNN换成了diffusion模型，那么恭喜你，你差不多提前设计出了Stable Diffusion。\",\"可见，VQ-VAE最大的贡献是提供了一种图像压缩思路，把生成大图像的问题转换成了一个更简单的生成「小图像」的问题。图像压缩成离散向量时主要借助了嵌入空间，或者说”codebook”这一工具。这种解决问题的思路可以应用到所有图像生成类任务上，比如超分辨率、图像修复、图像去模糊等。所以近两年我们能看到很多使用了codebook的图像生成类工作。\"]},\"962\":{\"h\":\"代码实现\"},\"963\":{\"h\":\"训练阶段\",\"t\":[\"VQ-VAE 的训练阶段分为五个阶段:\",\"我们将基于MINIST数据集进行训练演示\",\"步骤 1️⃣：编码输入图像\",\"用 Encoder 将输入图像 编码为连续隐变量 ，维度通常为 。\",\"这是一个普通的 CNN 编码过程，信息仍是连续的。\",\"# ============================= # 编码器 # ============================= class Encoder(nn.Module): def __init__(self, in_channels=1, hidden_channels=128, z_channels=64): super().__init__() self.conv1 = nn.Conv2d(in_channels, hidden_channels, 4, stride=2, padding=1) # 28x28 -> 14x14 self.conv2 = nn.Conv2d(hidden_channels, z_channels, 4, stride=2, padding=1) # 14x14 -> 7x7 def forward(self, x): x = F.relu(self.conv1(x)) x = self.conv2(x) return x\",\"在 VQ-VAE 中，一张输入图像最终被编码为一个 二维的“索引矩阵”，这个矩阵的每一个元素对应于 codebook 中的一个嵌入向量（embedding vector），表示该位置的图像特征。\",\"不是编码为“一个嵌入向量”的原因\",\"单一嵌入向量（如普通 VAE）：只能保留全局信息，比如图像的类别、姿态等，但无法表达空间结构。\",\"二维嵌入索引矩阵：是每个图像区域（patch/block）对应一个离散 token，可保留局部+空间结构，适合还原复杂细节。\",\"目标不同：压缩 vs 生成\",\"传统 AE/VAE 通常用作图像压缩或聚类，输出一个固定维度的表示。\",\"VQ-VAE 是为生成任务设计的：最终需要还原整张图像，因此不能只用一个全局向量（太少了），而必须保留空间布局。\",\"步骤 2️⃣：量化连续隐变量（离散化）\",\"将 中的每个空间位置的向量与 codebook 中的离散向量（embedding vectors）比较。\",\"选取最相近的向量索引（最近邻搜索），即：\",\"每个位置被替换为最接近的 codebook 向量，形成量化后的表示 。\",\"# ============================= # 向量量化器 # ============================= class VectorQuantizer(nn.Module): def __init__(self, num_embeddings, embedding_dim, commitment_cost): super().__init__() self.embedding_dim = embedding_dim self.num_embeddings = num_embeddings self.beta = commitment_cost # 编码字典 self.embedding = nn.Embedding(num_embeddings, embedding_dim) self.embedding.weight.data.uniform_(-1.0 / num_embeddings, 1.0 / num_embeddings) def forward(self, z): # z: [B, C, H, W] z_perm = z.permute(0, 2, 3, 1).contiguous() # [B, H, W, C] z_flattened = z_perm.view(-1, self.embedding_dim) # [BHW, C] # 欧氏距离计算 dist = ( torch.sum(z_flattened ** 2, dim=1, keepdim=True) - 2 * torch.matmul(z_flattened, self.embedding.weight.t()) + torch.sum(self.embedding.weight ** 2, dim=1) ) # [BHW, num_embeddings] encoding_indices = torch.argmin(dist, dim=1) # [BHW] quantized = self.embedding(encoding_indices) # [BHW, C] # 恢复为 [B, H, W, C] quantized = quantized.view(z_perm.shape) # [B, H, W, C] # 再 permute 回 [B, C, H, W] quantized = quantized.permute(0, 3, 1, 2).contiguous() # 向量量化损失 e_latent_loss = F.mse_loss(quantized.detach(), z) # <== 现在维度一致了 q_latent_loss = F.mse_loss(quantized, z.detach()) loss = q_latent_loss + self.beta * e_latent_loss # straight-through estimator quantized = z + (quantized - z).detach() encoding_indices = encoding_indices.view(z_perm.shape[0], z_perm.shape[1], z_perm.shape[2]) # [B, H, W] return quantized, loss, encoding_indices\",\"步骤 3️⃣：解码离散表示\",\"将 输入 Decoder，还原为重建图像 。\",\"由于 Decoder 处理的是向量索引映射后的嵌入向量，仍然是连续空间中的解码。\",\"# ============================= # 解码器 # ============================= class Decoder(nn.Module): def __init__(self, z_channels=64, hidden_channels=128, out_channels=1): super().__init__() self.conv1 = nn.ConvTranspose2d(z_channels, hidden_channels, 4, stride=2, padding=1) # 7x7 -> 14x14 self.conv2 = nn.ConvTranspose2d(hidden_channels, out_channels, 4, stride=2, padding=1) # 14x14 -> 28x28 def forward(self, z): z = F.relu(self.conv1(z)) z = torch.sigmoid(self.conv2(z)) return z\",\"步骤 4️⃣：计算损失函数（包括三项）\",\"总损失由三个部分组成：\",\"重建误差（图像像素级别 MSE 或 BCE）：\",\"codebook 损失（VQ loss）：鼓励 codebook 向量靠近 encoder 输出：\",\"承诺损失（commitment loss）：鼓励 encoder 输出靠近 codebook 向量：\",\"其中 表示 stop gradient，防止梯度传播到某部分。\",\"总损失为：\",\"# ============================= # VQ-VAE 组合模型 # ============================= class VQVAE(nn.Module): def __init__(self, in_channels=1, z_channels=64, num_embeddings=512, commitment_cost=0.25): super().__init__() self.encoder = Encoder(in_channels, z_channels=z_channels) self.vq = VectorQuantizer(num_embeddings, z_channels, commitment_cost) self.decoder = Decoder(z_channels) def forward(self, x): z = self.encoder(x) quantized, vq_loss, encoding_indices = self.vq(z) x_recon = self.decoder(quantized) return x_recon, vq_loss, encoding_indices\",\"步骤 5️⃣：反向传播与参数更新\",\"使用 straight-through estimator（STE）将编码器的梯度绕过非可导的 nearest neighbor 操作，近似回传。\",\"三部分参数更新：\",\"Encoder 更新：通过 STE 反向传播 的损失；\",\"Codebook 更新：更新嵌入向量（支持 EMA 或普通梯度更新）；\",\"Decoder 更新：通过正常反向传播重建误差更新。\",\"# 加载数据集 transform = transforms.Compose([transforms.ToTensor()]) train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True) train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True) # 初始化模型 model = VQVAE().to(device) optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) # 加载预训练模型（可选） model_loaded = False try: model.load_state_dict(torch.load(\\\"vqvae.pth\\\", map_location=device)) print(\\\"✅ 成功加载预训练的 VQ-VAE 模型权重\\\") model_loaded = True except FileNotFoundError: print(\\\"⚠️ 未找到预训练模型，开始从头训练\\\") # 训练模型 if not model_loaded: num_epochs = 10 for epoch in range(num_epochs): model.train() total_recon_loss = 0 total_vq_loss = 0 for x, _ in train_loader: x = x.to(device) x_recon, vq_loss, _ = model(x) recon_loss = F.mse_loss(x_recon, x) loss = recon_loss + vq_loss optimizer.zero_grad() loss.backward() optimizer.step() total_recon_loss += recon_loss.item() total_vq_loss += vq_loss.item() print(f\\\"Epoch [{epoch+1}/{num_epochs}], Recon Loss: {total_recon_loss:.4f}, VQ Loss: {total_vq_loss:.4f}\\\") # 保存模型 torch.save(model.state_dict(), \\\"vqvae.pth\\\") else: print(\\\"⏭️ 已加载预训练模型，跳过训练过程\\\")\"]},\"964\":{\"h\":\"生成阶段\",\"t\":[\"VQ-VAE：学习将图像压缩为离散 latent 表示，并能重建图像。\",\"PixelCNN：学习这些离散 latent 的分布，从而生成新的 latent 表示。\",\"解码阶段：用训练好的 VQ-VAE decoder，将 PixelCNN 采样的 latent 转换为图像。\",\"PixelCNN 模型实现如下(只有空间掩码卷积):\",\"class MaskedConv2d(nn.Conv2d): def __init__(self, mask_type, *args, **kwargs): super().__init__(*args, **kwargs) assert mask_type in ['A', 'B'] self.mask_type = mask_type self.register_buffer('mask', torch.ones_like(self.weight)) _, _, h, w = self.weight.size() yc, xc = h // 2, w // 2 self.mask[:, :, yc, xc+1:] = 0 self.mask[:, :, yc+1:] = 0 if mask_type == 'A': self.mask[:, :, yc, xc] = 0 def forward(self, x): self.weight.data *= self.mask return super().forward(x) class PixelCNN(nn.Module): def __init__(self, num_embeddings, in_channels=1, hidden_channels=64, num_layers=7): super().__init__() layers = [MaskedConv2d('A', in_channels, hidden_channels, kernel_size=7, padding=3), nn.ReLU()] for _ in range(num_layers - 2): layers.append(MaskedConv2d('B', hidden_channels, hidden_channels, 3, padding=1)) layers.append(nn.ReLU()) layers.append(nn.Conv2d(hidden_channels, num_embeddings, 1)) self.net = nn.Sequential(*layers) def forward(self, x): return self.net(x)\",\"基于已经训练好的VQ-VAE模型，再次扫描训练集，提取训练集中每个图像对应的离散 latent 索引列表\",\"# 假设：vqvae = VQVAE(...)，已经训练完毕或已加载权重 # 使用训练集提取离散 latent 索引 model.eval() all_indices = [] with torch.no_grad(): for img, _ in train_loader: z_e = model.encoder(img.to(device)) _, _, indices = model.vq(z_e) all_indices.append(indices.cpu()) # 拼接为 [B , H , W] all_indices = torch.cat(all_indices, dim=0)\",\"基于离散 latent 索引列表，训练 PixelCNN 模型，学习这些离散 latent 的分布规律：\",\"# PixelCNN 训练（学习 latent 索引分布） pixelcnn = PixelCNN(num_embeddings=512).to(device) optimizer = torch.optim.Adam(pixelcnn.parameters(), lr=1e-3) loss_fn = nn.CrossEntropyLoss() # 加载预训练 PixelCNN 模型（可选） pixelcnn_loaded = False try: pixelcnn.load_state_dict(torch.load(\\\"pixelcnn.pth\\\", map_location=device)) print(\\\"✅ 成功加载预训练的 PixelCNN 模型权重\\\") pixelcnn_loaded = True except FileNotFoundError: print(\\\"⚠️ 未找到预训练 PixelCNN 模型，开始从头训练\\\") if not pixelcnn_loaded: for epoch in range(10): total_loss = 0 for i in range(0, all_indices.size(0), 64): batch = all_indices[i:i+64].to(device) input = batch.unsqueeze(1).float() target = batch.long() # input 维度: (64,1,7,7) , logits 维度: (64,512,7,7) ，target 维度: (64,7,7) logits = pixelcnn(input) loss = loss_fn(logits, target) optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() print(f\\\"[PixelCNN] Epoch {epoch+1} Loss: {total_loss:.4f}\\\") # 保存模型 torch.save(pixelcnn.state_dict(), \\\"pixelcnn.pth\\\") else: print(\\\"⏭️ 已加载预训练 PixelCNN 模型，跳过训练过程\\\")\",\"PixelCNN 学习的是 latent index 的分布\",\"PixelCNN 生成 latent 索引 → VQ-VAE 解码成图像\",\"def sample_latent(pixelcnn, shape, num_embeddings): pixelcnn.eval() with torch.no_grad(): B, H, W = shape sample = torch.zeros((B, 1, H, W)).to(device) # 初始化 sample for i in range(H): for j in range(W): logits = pixelcnn(sample) probs = F.softmax(logits[:, :, i, j], dim=-1) sample[:, 0, i, j] = torch.multinomial(probs, 1).squeeze(-1) return sample.squeeze(1).long() # [B, H, W] # 从 PixelCNN 生成 latent 索引 sampled_indices = sample_latent(pixelcnn, shape=(8, 7, 7), num_embeddings=512) # 还原为 codebook 嵌入向量 embeddings = model.vq.embedding.weight quantized = embeddings[sampled_indices.view(-1)].view(8, 7, 7, -1).permute(0, 3, 1, 2).contiguous() # 解码成图像 with torch.no_grad(): recon = model.decoder(quantized).cpu() # 可视化 import matplotlib.pyplot as plt for i in range(8): plt.subplot(2, 4, i+1) plt.imshow(recon[i][0], cmap='gray') plt.axis('off') plt.suptitle(\\\"Generated Images from PixelCNN + VQ-VAE\\\") plt.show()\",\"由于训练轮次很少，所以效果也比较差:\"]},\"965\":{\"h\":\"\",\"t\":[\"本文转载至: 轻松理解 VQ-VAE：首个提出 codebook 机制的生成模型\"]},\"966\":{\"h\":\"WGAN 学习笔记\",\"t\":[\"WGAN 学习笔记\"]},\"967\":{\"h\":\"\",\"t\":[\"404 Not Found\"]}},\"dirtCount\":0,\"index\":[[\"⏭️\",{\"1\":{\"963\":1,\"964\":1}}],[\"承诺损失\",{\"1\":{\"963\":1}}],[\"太少了\",{\"1\":{\"963\":1}}],[\"太大\",{\"1\":{\"157\":1}}],[\"太大则可能导致不相关的点增多\",{\"1\":{\"135\":1}}],[\"姿态等\",{\"1\":{\"963\":1}}],[\"岁的人的照片的编码器输出\",{\"1\":{\"960\":1}}],[\"青年\",{\"1\":{\"960\":1}}],[\"停止梯度\",{\"1\":{\"959\":1}}],[\"没错\",{\"1\":{\"956\":1}}],[\"没有给出的步骤\",{\"1\":{\"936\":1}}],[\"没有不确定性\",{\"1\":{\"907\":1}}],[\"没有bn\",{\"1\":{\"522\":1}}],[\"没有重复的维度字母表示保留该维度\",{\"1\":{\"475\":1}}],[\"没有像\",{\"1\":{\"444\":1}}],[\"没有padding\",{\"1\":{\"430\":1}}],[\"没有\",{\"1\":{\"268\":1,\"454\":1}}],[\"没有固定顺序\",{\"1\":{\"149\":1}}],[\"介于多种可能之间\",{\"1\":{\"952\":1}}],[\"介绍了gpt\",{\"1\":{\"638\":1}}],[\"介绍几种具体的技巧\",{\"1\":{\"619\":1}}],[\"介绍\",{\"0\":{\"406\":1}}],[\"伯努利\",{\"1\":{\"951\":1}}],[\"伯努利分布的概率密度函数\",{\"1\":{\"932\":1}}],[\"伯努利分布与二项分布\",{\"0\":{\"856\":1}}],[\"吸收到\",{\"1\":{\"951\":1}}],[\"吸收图像特征\",{\"1\":{\"420\":1}}],[\"制造出\",{\"1\":{\"951\":1}}],[\"至少在一维情形下\",{\"1\":{\"949\":1}}],[\"端的神经网络\",{\"1\":{\"949\":1}}],[\"端到端的框架\",{\"1\":{\"357\":2}}],[\"永远不会为零\",{\"1\":{\"949\":1}}],[\"毕竟\",{\"1\":{\"946\":1}}],[\"式\",{\"1\":{\"945\":1}}],[\"式的\",{\"1\":{\"252\":1}}],[\"式的预训练方法\",{\"0\":{\"250\":1}}],[\"呢\",{\"1\":{\"945\":1,\"949\":1}}],[\"足够小\",{\"1\":{\"944\":1}}],[\"足够大的语言模型在多样化文本训练下\",{\"1\":{\"639\":1}}],[\"足够大\",{\"1\":{\"353\":2}}],[\"贡献很小\",{\"1\":{\"944\":1}}],[\"笔画细\",{\"1\":{\"944\":1}}],[\"笔画粗细\",{\"1\":{\"944\":1}}],[\"潜变量\",{\"1\":{\"943\":2}}],[\"潜变量模型\",{\"0\":{\"943\":1}}],[\"潜在风险\",{\"1\":{\"658\":1}}],[\"潜在生成式模型\",{\"1\":{\"635\":1}}],[\"潜在交互意图\",{\"1\":{\"52\":1}}],[\"ℝ\",{\"1\":{\"931\":2}}],[\"始终为正数\",{\"1\":{\"931\":1}}],[\"始终使用完整长度序列\",{\"1\":{\"680\":1}}],[\"ε\",{\"1\":{\"931\":1}}],[\"摘录\",{\"0\":{\"927\":1}}],[\"摘要等多样化任务\",{\"1\":{\"641\":1}}],[\"摘要生成\",{\"1\":{\"641\":1}}],[\"摘要任务\",{\"1\":{\"640\":1}}],[\"摘要和阅读理解等\",{\"1\":{\"638\":1}}],[\"摘要\",{\"0\":{\"29\":1,\"72\":1,\"227\":1,\"295\":1,\"322\":1,\"625\":1,\"638\":1,\"645\":1,\"653\":1,\"665\":1,\"674\":1,\"677\":1},\"1\":{\"640\":1,\"641\":1,\"656\":1}}],[\"灰度图\",{\"1\":{\"926\":2}}],[\"灰色\",{\"1\":{\"107\":1}}],[\"断言检查\",{\"1\":{\"926\":1}}],[\"干脆放弃了概率模型\",{\"1\":{\"925\":1}}],[\"干扰样本\",{\"1\":{\"382\":1}}],[\"跑哪去了\",{\"1\":{\"921\":1}}],[\"吗\",{\"1\":{\"921\":1}}],[\"＋\",{\"1\":{\"919\":1}}],[\"挖坑\",{\"1\":{\"919\":1}}],[\"挖掘并生成显式的\",{\"1\":{\"83\":1}}],[\"挖掘物体的不变几何属性\",{\"1\":{\"50\":1}}],[\"挖掘其他可能交互意图\",{\"1\":{\"36\":1}}],[\"挖掘不变几何属性与潜在交互意图\",{\"1\":{\"32\":1}}],[\"骗过\",{\"1\":{\"918\":2}}],[\"温和\",{\"1\":{\"917\":1}}],[\"温度不小于\",{\"1\":{\"847\":1}}],[\"温度小于\",{\"1\":{\"847\":1}}],[\"温度是\",{\"1\":{\"847\":1}}],[\"温度在\",{\"1\":{\"847\":4}}],[\"温度超参数\",{\"1\":{\"355\":1}}],[\"温度系数\",{\"1\":{\"355\":1}}],[\"温度\",{\"1\":{\"286\":1,\"899\":1,\"900\":1}}],[\"温度缩放\",{\"1\":{\"274\":1}}],[\"温度高时更平滑\",{\"1\":{\"257\":1}}],[\"温度越低\",{\"1\":{\"257\":1}}],[\"温度参数\",{\"1\":{\"192\":1,\"257\":1,\"355\":1,\"361\":1,\"899\":1,\"900\":1}}],[\"海拔可能变\",{\"1\":{\"917\":1}}],[\"海拔最多变\",{\"1\":{\"917\":1}}],[\"米\",{\"1\":{\"917\":2}}],[\"米水平路\",{\"1\":{\"917\":2}}],[\"斜率\",{\"1\":{\"917\":1}}],[\"叫做pixelcnn\",{\"1\":{\"956\":1}}],[\"叫做\",{\"1\":{\"917\":1}}],[\"叫做个体判别\",{\"1\":{\"350\":1}}],[\"叫\",{\"1\":{\"917\":1}}],[\"想让神经网络输出一个整数\",{\"1\":{\"958\":1}}],[\"想象你走山路\",{\"1\":{\"917\":1}}],[\"想象有两个人分别站在一条直线上的不同点\",{\"1\":{\"915\":1}}],[\"想保留原序列则\",{\"1\":{\"513\":1}}],[\"↘\",{\"1\":{\"909\":2}}],[\"抛硬币连续两次都正面\",{\"1\":{\"906\":1}}],[\"抛硬币得到正面\",{\"1\":{\"906\":1}}],[\"惊讶\",{\"1\":{\"906\":1}}],[\"熵为\",{\"1\":{\"907\":1}}],[\"熵最大\",{\"1\":{\"907\":1}}],[\"熵越大\",{\"1\":{\"907\":1}}],[\"熵是平均信息量\",{\"1\":{\"907\":1}}],[\"熵\",{\"0\":{\"905\":1},\"1\":{\"908\":1,\"909\":2}}],[\"π2\",{\"1\":{\"899\":3}}],[\"π1\",{\"1\":{\"899\":4}}],[\"π0\",{\"1\":{\"899\":2}}],[\"宽高\",{\"1\":{\"899\":1}}],[\"宽度\",{\"1\":{\"311\":1,\"500\":1,\"924\":1}}],[\"宽度3200\",{\"1\":{\"304\":1}}],[\"地跟随条件\",{\"1\":{\"894\":1}}],[\"地板除\",{\"0\":{\"462\":1},\"1\":{\"462\":1}}],[\"地板等\",{\"1\":{\"146\":1}}],[\"淹没\",{\"1\":{\"893\":1}}],[\"却输出\",{\"1\":{\"892\":2}}],[\"却能学习到很强的可迁移表征\",{\"1\":{\"220\":1}}],[\"¹\",{\"1\":{\"885\":1}}],[\"似然函数为\",{\"1\":{\"904\":1}}],[\"似然函数定义为在参数\",{\"1\":{\"904\":1}}],[\"似然方程\",{\"1\":{\"904\":1}}],[\"似然\",{\"1\":{\"885\":1}}],[\"异排列数\",{\"1\":{\"881\":2,\"882\":2}}],[\"异常数据\",{\"1\":{\"836\":1}}],[\"异常值\",{\"1\":{\"259\":1}}],[\"异常值等问题\",{\"1\":{\"149\":1}}],[\"异常检测\",{\"1\":{\"160\":1}}],[\"异常检测等特殊场景\",{\"1\":{\"160\":1}}],[\"异常点会影响分类和分割性能\",{\"1\":{\"157\":1}}],[\"听话\",{\"1\":{\"894\":1}}],[\"听者必须从说话者表达出的\",{\"1\":{\"878\":1}}],[\"听懂人话\",{\"1\":{\"346\":1}}],[\"置信度\",{\"1\":{\"877\":1}}],[\"贝叶斯公式本身非常简单\",{\"1\":{\"877\":1}}],[\"贝叶斯公式\",{\"1\":{\"877\":1,\"945\":1}}],[\"贝叶斯\",{\"1\":{\"877\":1}}],[\"贝叶斯法则写作\",{\"1\":{\"853\":1}}],[\"贝叶斯法则可以写为\",{\"1\":{\"852\":1}}],[\"贝叶斯法则\",{\"0\":{\"851\":1}}],[\"术语\",{\"1\":{\"877\":3}}],[\"密度\",{\"1\":{\"873\":1}}],[\"密度越小\",{\"1\":{\"873\":1}}],[\"密集连接局部邻域\",{\"1\":{\"110\":1}}],[\"壳层的厚度为\",{\"1\":{\"872\":1}}],[\"众数\",{\"1\":{\"872\":1}}],[\"众多基于视觉\",{\"1\":{\"405\":1}}],[\"球体的体积随半径\",{\"1\":{\"873\":1}}],[\"球形协方差矩阵\",{\"1\":{\"871\":1}}],[\"球查询\",{\"1\":{\"137\":2}}],[\"鉴于它的重要性\",{\"1\":{\"869\":1}}],[\"洛伦兹分布\",{\"1\":{\"868\":1}}],[\"柯西分布\",{\"0\":{\"868\":1},\"1\":{\"868\":1}}],[\"窄\",{\"1\":{\"865\":1}}],[\"事实证明\",{\"1\":{\"951\":1}}],[\"事实上\",{\"1\":{\"863\":1,\"869\":1}}],[\"事情会变得复杂\",{\"1\":{\"947\":1}}],[\"事件越罕见\",{\"1\":{\"906\":1}}],[\"事件集合体系\",{\"1\":{\"847\":1}}],[\"事件结果\",{\"1\":{\"846\":1}}],[\"事件\",{\"1\":{\"846\":2,\"847\":1,\"848\":1,\"849\":2,\"853\":1}}],[\"事件空间为\",{\"1\":{\"846\":1}}],[\"试验次数\",{\"1\":{\"860\":1}}],[\"红球\",{\"1\":{\"860\":1}}],[\"红色为剔除的文本\",{\"1\":{\"177\":1}}],[\"红色=功能区域\",{\"1\":{\"107\":1}}],[\"红色\",{\"1\":{\"107\":2}}],[\"红色区域为点云的可供性标注\",{\"1\":{\"41\":1}}],[\"失败次数\",{\"1\":{\"860\":1}}],[\"失败\",{\"1\":{\"860\":1}}],[\"盒子\",{\"1\":{\"859\":1}}],[\"服从概率密度\",{\"1\":{\"943\":1}}],[\"服从\",{\"1\":{\"860\":1}}],[\"服从二项分布\",{\"1\":{\"859\":1}}],[\"服从参数为\",{\"1\":{\"858\":1}}],[\"服务器训练\",{\"1\":{\"280\":1}}],[\"泊松分布是负二项分布的一个特例\",{\"1\":{\"863\":1}}],[\"泊松分布\",{\"0\":{\"858\":1}}],[\"读作\",{\"1\":{\"856\":1}}],[\"读取训练语料\",{\"1\":{\"595\":1}}],[\"读取图片并将其转换为合适的格式后\",{\"1\":{\"410\":1}}],[\"读取出所有点云文件路径\",{\"1\":{\"82\":1}}],[\"读取出所有图片路径\",{\"1\":{\"82\":1}}],[\"读取所有点云路径\",{\"1\":{\"53\":1}}],[\"读取所有图片路径\",{\"1\":{\"53\":1}}],[\"阳性概率应该很高\",{\"1\":{\"850\":1}}],[\"患病\",{\"1\":{\"850\":1}}],[\"半柯西分布\",{\"1\":{\"868\":1}}],[\"半正态分布\",{\"0\":{\"866\":1}}],[\"半开区间\",{\"1\":{\"847\":1}}],[\"半径太小可能无法有效捕获足够的局部详细\",{\"1\":{\"135\":1}}],[\"半径查询\",{\"1\":{\"125\":1}}],[\"奇怪\",{\"1\":{\"847\":1}}],[\"度\",{\"1\":{\"847\":3}}],[\"度或\",{\"1\":{\"847\":1}}],[\"度之间\",{\"1\":{\"847\":1}}],[\"度量两个变量是否同步变化\",{\"1\":{\"574\":1}}],[\"秒\",{\"1\":{\"847\":2}}],[\"测量值是无理数\",{\"1\":{\"847\":1}}],[\"测量某事件持续时间\",{\"1\":{\"847\":1}}],[\"测试已学习的模型\",{\"0\":{\"947\":1}}],[\"测试的重要性\",{\"0\":{\"796\":1}}],[\"测试数据使用1k\",{\"1\":{\"712\":1}}],[\"测试和开发集\",{\"1\":{\"712\":1}}],[\"测试模型的长距离依赖能力\",{\"1\":{\"680\":1}}],[\"测试显示职业性别刻板印象明显\",{\"1\":{\"670\":1}}],[\"测试重叠率5\",{\"1\":{\"641\":1}}],[\"测试常识推理能力\",{\"1\":{\"641\":1}}],[\"测试图片分类正确率\",{\"1\":{\"410\":1,\"412\":1}}],[\"测试阶段\",{\"1\":{\"334\":1,\"381\":1}}],[\"测试阶段用中心裁剪来报告准确率\",{\"1\":{\"286\":1}}],[\"测试时\",{\"1\":{\"947\":1,\"952\":1}}],[\"测试时输入\",{\"1\":{\"707\":1}}],[\"测试时支持40区块\",{\"1\":{\"331\":1}}],[\"测试时可扩展至\",{\"1\":{\"329\":1}}],[\"测试划分\",{\"1\":{\"187\":1}}],[\"测试集上\",{\"1\":{\"309\":1}}],[\"测试集最终评估\",{\"1\":{\"106\":1}}],[\"测试集\",{\"1\":{\"92\":1,\"104\":1,\"835\":1}}],[\"测试集中\",{\"1\":{\"513\":1}}],[\"测试集中的可供性类别未在训练集中出现\",{\"1\":{\"44\":1}}],[\"测试集中包含训练集中未出现的物体类别\",{\"1\":{\"44\":1}}],[\"测试\",{\"0\":{\"793\":1},\"1\":{\"53\":1,\"90\":1,\"92\":1,\"834\":1,\"898\":1}}],[\"掷硬币\",{\"1\":{\"904\":1}}],[\"掷两次硬币的\",{\"1\":{\"846\":1}}],[\"掷两次公平硬币\",{\"1\":{\"846\":1}}],[\"掷一个三面骰子\",{\"1\":{\"846\":1}}],[\"探讨边界情况\",{\"1\":{\"836\":1}}],[\"探索\",{\"1\":{\"658\":1}}],[\"探索指令格式变化对模型泛化能力的影响\",{\"1\":{\"655\":1}}],[\"探索为多模态基础模型赋予\",{\"1\":{\"225\":1}}],[\"探索图卷积深度在\",{\"1\":{\"110\":1}}],[\"脏数据进行清洗\",{\"1\":{\"836\":1}}],[\"音视频等\",{\"1\":{\"836\":1}}],[\"音频和视频\",{\"1\":{\"823\":1}}],[\"搭建数据库\",{\"1\":{\"836\":1}}],[\"搭建项目的整体架构\",{\"1\":{\"836\":1}}],[\"搭建整体架构\",{\"1\":{\"836\":1}}],[\"搭配70亿或130亿参数的llms\",{\"1\":{\"323\":1}}],[\"业务逻辑分解\",{\"1\":{\"835\":1}}],[\"业务逻辑分解等手段来充分发挥大模型能力\",{\"1\":{\"835\":1}}],[\"业界很早就发现了用户对于对话交互的特殊偏好\",{\"1\":{\"827\":1}}],[\"流式处理\",{\"1\":{\"833\":1}}],[\"流程更为灵活和敏捷\",{\"1\":{\"835\":1}}],[\"流程四步走\",{\"1\":{\"145\":1}}],[\"流程\",{\"1\":{\"116\":1}}],[\"拥有近\",{\"1\":{\"833\":1}}],[\"拥有一个规模大且特征较为一致的字典\",{\"1\":{\"352\":1}}],[\"享受到更加安全稳定的开发体验\",{\"1\":{\"833\":1}}],[\"链路组合来实现业务逻辑\",{\"1\":{\"835\":1}}],[\"链式操作\",{\"1\":{\"833\":1}}],[\"链式法则\",{\"0\":{\"774\":1}}],[\"链\",{\"1\":{\"832\":1}}],[\"箭头表示数据流的方向\",{\"1\":{\"831\":1}}],[\"黑盒子\",{\"1\":{\"830\":1}}],[\"黑点为采样点\",{\"1\":{\"502\":1}}],[\"微软\",{\"1\":{\"827\":1}}],[\"微调后准确率从63\",{\"1\":{\"669\":1}}],[\"微调也仅为\",{\"1\":{\"658\":1}}],[\"微调模型\",{\"1\":{\"656\":1}}],[\"微调预训练\",{\"1\":{\"656\":1}}],[\"微调预训练的gpt\",{\"1\":{\"654\":1}}],[\"微调小型数据集以嵌入价值观\",{\"1\":{\"655\":1}}],[\"微调范式\",{\"1\":{\"650\":1}}],[\"微调范式与任务适应能力的关系\",{\"1\":{\"650\":1}}],[\"微调任务和数据集如下\",{\"1\":{\"634\":1}}],[\"微调的对比可以参考下表\",{\"1\":{\"830\":1}}],[\"微调的细节\",{\"1\":{\"633\":1}}],[\"微调的场景中\",{\"1\":{\"614\":1}}],[\"微调的最终目的\",{\"1\":{\"602\":1}}],[\"微调流程\",{\"1\":{\"611\":1}}],[\"微调之后的模型\",{\"1\":{\"610\":1}}],[\"微调相对来说就是一个更优的方案\",{\"1\":{\"601\":1}}],[\"微调用于图像描述\",{\"1\":{\"377\":1}}],[\"微调成本高\",{\"1\":{\"346\":1}}],[\"微调整个模型参数\",{\"1\":{\"346\":1}}],[\"微调时保持视觉编码器参数不变\",{\"1\":{\"342\":1}}],[\"微调时提升为\",{\"1\":{\"176\":1}}],[\"微调过程\",{\"1\":{\"342\":1}}],[\"微调数据规模\",{\"1\":{\"669\":1}}],[\"微调数据\",{\"1\":{\"332\":1}}],[\"微调评估\",{\"1\":{\"286\":1}}],[\"微调阶段的数据组织方式如下\",{\"1\":{\"342\":1}}],[\"微调阶段使用的是作者自己构建的高质量多模态指令数据集\",{\"1\":{\"342\":1}}],[\"微调阶段\",{\"0\":{\"187\":1,\"190\":1},\"1\":{\"340\":1,\"628\":1}}],[\"微调\",{\"0\":{\"342\":1,\"611\":1},\"1\":{\"48\":1,\"313\":1,\"317\":1,\"647\":1,\"650\":1,\"655\":1,\"830\":4}}],[\"智能体\",{\"1\":{\"827\":1}}],[\"智慧化知识库系统\",{\"0\":{\"839\":1},\"1\":{\"821\":1,\"838\":1}}],[\"贾维斯\",{\"1\":{\"827\":1}}],[\"亚马逊\",{\"1\":{\"827\":1}}],[\"苹果\",{\"1\":{\"827\":1}}],[\"陆奇在微软期间\",{\"1\":{\"827\":1}}],[\"斯坦福大学等多所高校的研究人员提出了基座模型\",{\"1\":{\"826\":1}}],[\"斯坦福提出的\",{\"1\":{\"610\":1}}],[\"据推测\",{\"1\":{\"825\":1}}],[\"据作者所述\",{\"1\":{\"20\":1}}],[\"量变引起质变\",{\"1\":{\"825\":1}}],[\"量化连续隐变量\",{\"1\":{\"963\":1}}],[\"量化通过将这些高精度的浮点数转换为低精度的整数\",{\"1\":{\"614\":1}}],[\"量化是一种在深度学习领域用于减少模型内存占用和计算量的技术\",{\"1\":{\"614\":1}}],[\"量化的核心目标是降成本\",{\"1\":{\"607\":1}}],[\"量化的特征\",{\"1\":{\"213\":1}}],[\"量化\",{\"1\":{\"607\":2}}],[\"量化误差\",{\"1\":{\"501\":1}}],[\"量化器和教师模型等组件\",{\"1\":{\"213\":1}}],[\"量化器在代码本\",{\"1\":{\"212\":1}}],[\"量化向量\",{\"1\":{\"213\":1}}],[\"量化损失\",{\"1\":{\"213\":6}}],[\"量化后的特征\",{\"1\":{\"213\":1}}],[\"量化后的特征图\",{\"1\":{\"213\":1}}],[\"量化后的\",{\"1\":{\"212\":1}}],[\"量化分析\",{\"1\":{\"47\":1}}],[\"集中在\",{\"1\":{\"865\":1}}],[\"集成\",{\"1\":{\"831\":1}}],[\"集群来实现\",{\"1\":{\"824\":1}}],[\"集合\",{\"1\":{\"444\":1,\"847\":1}}],[\"集合函数近似器\",{\"1\":{\"160\":1}}],[\"集合抽象\",{\"1\":{\"143\":1}}],[\"巨大的规模\",{\"1\":{\"824\":1}}],[\"媲美\",{\"1\":{\"823\":1}}],[\"团队开发的开源大语言模型系列\",{\"1\":{\"823\":1}}],[\"讯飞发布了推理思考模型\",{\"1\":{\"823\":1}}],[\"讯飞星火\",{\"1\":{\"823\":1}}],[\"讯飞星火发布模型\",{\"1\":{\"823\":1}}],[\"讯飞星火认知大模型是科大讯飞发布的语言大模型\",{\"1\":{\"823\":1}}],[\"星火语音同传模型\",{\"1\":{\"823\":1}}],[\"星火\",{\"1\":{\"823\":1}}],[\"星火大模型\",{\"1\":{\"823\":1}}],[\"免费版\",{\"1\":{\"823\":2}}],[\"版\",{\"1\":{\"823\":1}}],[\"版本只要输出一个概率分布的\",{\"1\":{\"921\":1}}],[\"版本的\",{\"1\":{\"921\":1}}],[\"版本的模型确实能快速计算第i个像素的概率分布了\",{\"1\":{\"921\":1}}],[\"版本的模型\",{\"1\":{\"921\":1}}],[\"版本迭代\",{\"0\":{\"833\":1}}],[\"版本进一步加入\",{\"1\":{\"656\":1}}],[\"版本\",{\"1\":{\"311\":1,\"410\":1,\"425\":1,\"550\":1,\"823\":4,\"836\":1,\"921\":1,\"936\":1}}],[\"版本预训练过程有很多重合逻辑\",{\"1\":{\"214\":1}}],[\"版本代码\",{\"1\":{\"130\":2,\"147\":2}}],[\"费用最高\",{\"1\":{\"823\":1}}],[\"日\",{\"1\":{\"823\":6,\"833\":1}}],[\"日发布\",{\"1\":{\"823\":1}}],[\"日志记录\",{\"1\":{\"461\":1}}],[\"日志记录工具\",{\"1\":{\"265\":1}}],[\"于\",{\"1\":{\"823\":4}}],[\"于是有\",{\"1\":{\"945\":1}}],[\"于是代码中使用zip\",{\"1\":{\"801\":1}}],[\"于是你把这\",{\"1\":{\"502\":1}}],[\"于是\",{\"1\":{\"353\":1,\"960\":1}}],[\"慢\",{\"1\":{\"823\":1}}],[\"慢慢增加\",{\"1\":{\"204\":1}}],[\"速度越慢\",{\"1\":{\"828\":1}}],[\"速度最快\",{\"1\":{\"823\":1}}],[\"速度快\",{\"1\":{\"823\":1}}],[\"速度和显存优化\",{\"1\":{\"510\":1}}],[\"价格更加亲民\",{\"1\":{\"823\":1}}],[\"价值\",{\"1\":{\"658\":1}}],[\"快捷又有效\",{\"1\":{\"921\":1}}],[\"快约\",{\"1\":{\"823\":1}}],[\"快速流行\",{\"1\":{\"942\":1}}],[\"快速适应新任务\",{\"1\":{\"646\":1}}],[\"快速给出答案\",{\"1\":{\"619\":1}}],[\"快速找到合适的模型\",{\"1\":{\"422\":1}}],[\"截止\",{\"1\":{\"823\":1}}],[\"截断过长文本\",{\"1\":{\"64\":1}}],[\"月比较有影响力并且模型参数量超过\",{\"1\":{\"823\":1}}],[\"月\",{\"1\":{\"823\":50,\"833\":1}}],[\"月之前\",{\"1\":{\"822\":1}}],[\"月发布的\",{\"1\":{\"405\":1}}],[\"世界知识的语义\",{\"1\":{\"823\":1}}],[\"世界坐标系和局部坐标系\",{\"1\":{\"131\":1}}],[\"世纪\",{\"1\":{\"822\":1}}],[\"称之为\",{\"1\":{\"822\":1}}],[\"称为二项系数\",{\"1\":{\"856\":1}}],[\"称为后验概率\",{\"1\":{\"852\":1}}],[\"称为似然\",{\"1\":{\"852\":1}}],[\"称为先验概率\",{\"1\":{\"852\":1}}],[\"称为概率密度函数\",{\"1\":{\"847\":1}}],[\"称为概率质量函数\",{\"1\":{\"846\":1}}],[\"称为混淆矩阵\",{\"1\":{\"561\":1}}],[\"称为\",{\"1\":{\"286\":1,\"376\":2,\"846\":1}}],[\"称为随机输入丢弃\",{\"1\":{\"140\":1}}],[\"涌现能力可以与某些复杂任务有关\",{\"1\":{\"825\":1}}],[\"涌现能力就像是模型性能随着规模增大而迅速提升\",{\"1\":{\"825\":1}}],[\"涌现能力是一种令人惊讶的能力\",{\"1\":{\"825\":1}}],[\"涌现能力\",{\"1\":{\"822\":1,\"825\":1}}],[\"涌现能力研究\",{\"1\":{\"671\":1}}],[\"豆包\",{\"1\":{\"822\":1}}],[\"国内外有超过百种大模型相继发布\",{\"1\":{\"823\":1}}],[\"国内的有\",{\"1\":{\"822\":1}}],[\"国外的知名\",{\"1\":{\"822\":1}}],[\"拓展\",{\"1\":{\"819\":1}}],[\"拓扑复杂\",{\"1\":{\"159\":1}}],[\"🏗️\",{\"0\":{\"818\":1}}],[\"🌟\",{\"1\":{\"148\":1}}],[\"香蕉函数\",{\"1\":{\"816\":1}}],[\"山\",{\"1\":{\"816\":1}}],[\"寻找函数最优解\",{\"0\":{\"816\":1}}],[\"浅蓝色矩形\",{\"1\":{\"815\":1}}],[\"浅层直观\",{\"0\":{\"912\":1}}],[\"浅层头\",{\"1\":{\"215\":1}}],[\"浅层解码器在下游任务上表现更好\",{\"1\":{\"215\":1}}],[\"浅层\",{\"1\":{\"122\":1,\"214\":2,\"215\":1}}],[\"浅层细节特征\",{\"1\":{\"122\":1}}],[\"浅层特征\",{\"1\":{\"121\":1}}],[\"橙色圆形\",{\"1\":{\"815\":1}}],[\"牛顿法等经典函数逼近与优化示例\",{\"1\":{\"814\":1}}],[\"围绕计算图可视化\",{\"1\":{\"814\":1}}],[\"处对折\",{\"1\":{\"866\":1}}],[\"处标记最优点\",{\"1\":{\"816\":1}}],[\"处导数为\",{\"1\":{\"811\":1}}],[\"处理图像\",{\"1\":{\"893\":1,\"921\":1}}],[\"处理x1\",{\"1\":{\"809\":1}}],[\"处理x0\",{\"1\":{\"809\":1}}],[\"处理运算符的左右操作数差异\",{\"1\":{\"809\":1}}],[\"处理右操作数为自定义类型的情况\",{\"1\":{\"809\":1}}],[\"处理b\",{\"1\":{\"809\":2}}],[\"处理beam\",{\"1\":{\"421\":1}}],[\"处理a\",{\"1\":{\"809\":2}}],[\"处理vocab\",{\"1\":{\"697\":1}}],[\"处理两个csv文件\",{\"1\":{\"696\":1}}],[\"处理每行文本\",{\"1\":{\"696\":1}}],[\"处理csv文件\",{\"1\":{\"696\":1}}],[\"处理句法歧义\",{\"1\":{\"634\":1}}],[\"处理分布式训练下的负样本扩展\",{\"1\":{\"385\":1}}],[\"处理起来没啥问题\",{\"1\":{\"355\":1}}],[\"处理约\",{\"1\":{\"315\":1}}],[\"处理的是向量索引映射后的嵌入向量\",{\"1\":{\"963\":1}}],[\"处理的是点云数据\",{\"1\":{\"160\":1}}],[\"处理的\",{\"1\":{\"266\":1}}],[\"处理后\",{\"1\":{\"185\":1}}],[\"处理后的数据集包含比原始数据集更多的文本\",{\"1\":{\"181\":1}}],[\"处理后的\",{\"1\":{\"100\":1}}],[\"处理\",{\"1\":{\"159\":1,\"264\":1,\"266\":1,\"293\":1,\"380\":1,\"385\":1,\"480\":1,\"641\":1,\"657\":1}}],[\"处理点云数据\",{\"1\":{\"152\":1}}],[\"处理上采样后的特征\",{\"1\":{\"123\":1}}],[\"处理跳跃连接特征并进行上采样\",{\"1\":{\"122\":1}}],[\"处理当前层特征\",{\"1\":{\"122\":1}}],[\"处理维度匹配问题\",{\"1\":{\"120\":1}}],[\"处理过程\",{\"1\":{\"96\":1,\"97\":1,\"98\":1}}],[\"处理流程\",{\"1\":{\"78\":1,\"293\":1}}],[\"演示tinypytorch的自动微分功能\",{\"1\":{\"811\":1}}],[\"幂运算y\",{\"1\":{\"809\":1}}],[\"幂运算\",{\"1\":{\"809\":1}}],[\"∂c\",{\"1\":{\"809\":1}}],[\"∂b\",{\"1\":{\"809\":1}}],[\"∂a\",{\"1\":{\"809\":1}}],[\"∂y\",{\"1\":{\"809\":3}}],[\"透明箱子\",{\"1\":{\"808\":1}}],[\"封装no\",{\"1\":{\"807\":1}}],[\"封装了多种损失函数\",{\"1\":{\"510\":1}}],[\"封装了\",{\"1\":{\"254\":1}}],[\"修改function\",{\"1\":{\"807\":1}}],[\"修改function类的\",{\"1\":{\"809\":1}}],[\"修改function类\",{\"1\":{\"806\":1,\"812\":1}}],[\"修改variable\",{\"1\":{\"807\":1}}],[\"修改variable类的backward方法\",{\"1\":{\"806\":1}}],[\"修正意图理解\",{\"1\":{\"56\":1}}],[\"弱引用不会增加对象的引用计数\",{\"1\":{\"806\":1}}],[\"弱引用作为主动优化\",{\"1\":{\"806\":1}}],[\"弱引用能确保计算图在使用完毕后自动释放内存\",{\"1\":{\"806\":1}}],[\"弱引用的优势\",{\"1\":{\"806\":1}}],[\"弱引用\",{\"1\":{\"806\":1}}],[\"弱化\",{\"1\":{\"56\":1}}],[\"频繁的gc操作会影响计算效率\",{\"1\":{\"806\":1}}],[\"频次表记录合并规则\",{\"1\":{\"595\":1}}],[\"辈分排序\",{\"1\":{\"805\":1}}],[\"辈分\",{\"0\":{\"805\":1},\"1\":{\"805\":2,\"812\":1}}],[\"叶节点\",{\"1\":{\"804\":1}}],[\"乘法运算是最基础的操作之一\",{\"1\":{\"809\":1}}],[\"乘法运算的实现与运算符重载\",{\"1\":{\"809\":1}}],[\"乘法\",{\"1\":{\"800\":1}}],[\"乘积\",{\"1\":{\"160\":1,\"243\":1}}],[\"追溯起来\",{\"1\":{\"955\":1}}],[\"追加到列表尾部\",{\"1\":{\"787\":1}}],[\"追求更细粒度的表示\",{\"1\":{\"271\":1}}],[\"弹出列表尾部元素\",{\"1\":{\"787\":1}}],[\"递归实现的反向传播在计算图较深时可能导致栈溢出\",{\"1\":{\"786\":1}}],[\"递归实现的问题\",{\"0\":{\"786\":1}}],[\"递归遍历目录获取所有图片路径\",{\"1\":{\"410\":1,\"412\":1}}],[\"导包\",{\"1\":{\"918\":1}}],[\"导数为400\",{\"1\":{\"816\":1}}],[\"导数公式\",{\"1\":{\"809\":1}}],[\"导数从输出端向输入端传播\",{\"1\":{\"776\":1}}],[\"导数是变化率的表示\",{\"1\":{\"769\":1}}],[\"导数的定义\",{\"0\":{\"769\":1}}],[\"导致从标准正态分布采样得到的\",{\"1\":{\"949\":1}}],[\"导致用户接收到的信息不准确\",{\"1\":{\"828\":1}}],[\"导致内存占用持续升高\",{\"1\":{\"806\":1}}],[\"导致引用计数无法归零\",{\"1\":{\"806\":1}}],[\"导致x变量梯度计算错误\",{\"1\":{\"804\":1}}],[\"导致最终构建得到的字典过大并且还有很多噪声\",{\"1\":{\"697\":1}}],[\"导致其泛化能力有限\",{\"1\":{\"639\":1}}],[\"导致\",{\"1\":{\"589\":1,\"612\":1}}],[\"导致整体距离变大\",{\"1\":{\"576\":1}}],[\"导致计算量依然很大\",{\"1\":{\"390\":1}}],[\"导致字典内编码特征不一致\",{\"1\":{\"357\":1}}],[\"导致潜在信息损失\",{\"1\":{\"280\":1}}],[\"导致多模态编码器难以有效建模图文交互\",{\"1\":{\"194\":1}}],[\"导致性能下降\",{\"1\":{\"179\":1}}],[\"导致模型性能不佳\",{\"1\":{\"942\":1}}],[\"导致模型过度关注图像而忽视文本\",{\"1\":{\"893\":1}}],[\"导致模型很难训练\",{\"1\":{\"356\":1}}],[\"导致模型学习没有轻重\",{\"1\":{\"355\":1}}],[\"导致模型泛化性不足\",{\"1\":{\"353\":1}}],[\"导致模型无法捕捉到更细粒度的几何细节\",{\"1\":{\"157\":1}}],[\"导致模型在\",{\"1\":{\"26\":1}}],[\"导致所有的特征\",{\"1\":{\"131\":1}}],[\"导致预测不准确\",{\"1\":{\"117\":1}}],[\"导致本论文复现流程暂时终止\",{\"1\":{\"61\":1}}],[\"导致各项指标大幅下降\",{\"1\":{\"48\":1}}],[\"导致在融合时可能无法很好地相互补充\",{\"1\":{\"706\":1}}],[\"导致在分布外数据上表现不佳\",{\"1\":{\"646\":1}}],[\"导致在非常规宽高比或文档理解任务上表现受限\",{\"1\":{\"326\":1}}],[\"导致在\",{\"1\":{\"19\":1}}],[\"辅助损失函数或额外信息\",{\"1\":{\"884\":1}}],[\"辅助模型生成过程\",{\"1\":{\"828\":1}}],[\"辅助函数\",{\"0\":{\"763\":1}}],[\"辅助训练目标\",{\"1\":{\"627\":1}}],[\"箱子\",{\"0\":{\"754\":1},\"1\":{\"755\":1}}],[\"揭开深度学习框架的神秘面纱\",{\"0\":{\"753\":1}}],[\"揭示模型性能与规模的关系\",{\"1\":{\"671\":1}}],[\"揭示了自监督预训练中自注意力机制对语义理解的自动学习能力\",{\"1\":{\"228\":1}}],[\"揭示\",{\"1\":{\"83\":1}}],[\"仓库链接\",{\"1\":{\"752\":1,\"798\":1,\"813\":1,\"818\":1}}],[\"托尔斯泰\",{\"1\":{\"737\":1}}],[\"莎士比亚\",{\"1\":{\"737\":1}}],[\"莎士比亚是英国文学史上最伟大的作家之一\",{\"1\":{\"733\":1}}],[\"歌德\",{\"1\":{\"737\":1}}],[\"雨果\",{\"1\":{\"737\":1}}],[\"麦克白\",{\"1\":{\"733\":1}}],[\"哈姆雷特\",{\"1\":{\"733\":2,\"737\":1}}],[\"态\",{\"1\":{\"713\":1}}],[\"破\",{\"1\":{\"713\":1}}],[\"票\",{\"1\":{\"713\":1}}],[\"股\",{\"1\":{\"713\":1}}],[\"股票中的突破形态\",{\"1\":{\"713\":2}}],[\"短\",{\"1\":{\"710\":1}}],[\"短距离\",{\"1\":{\"710\":2}}],[\"布尔张量\",{\"1\":{\"710\":1}}],[\"布尔值\",{\"1\":{\"257\":1}}],[\"粗略表示\",{\"1\":{\"710\":1}}],[\"粗粒度\",{\"1\":{\"710\":1}}],[\"细粒度\",{\"1\":{\"710\":1}}],[\"细致地识别交互动作及其参与部位\",{\"1\":{\"52\":1}}],[\"怎么让梯度从解码器的输入\",{\"1\":{\"958\":1}}],[\"怎么理解\",{\"1\":{\"710\":1}}],[\"怎么办\",{\"1\":{\"694\":1}}],[\"桶矩阵\",{\"1\":{\"710\":1}}],[\"桶的总数量\",{\"1\":{\"710\":1}}],[\"桶的作用是什么\",{\"1\":{\"710\":1}}],[\"桶号是整数索引\",{\"1\":{\"710\":1}}],[\"桶号加\",{\"1\":{\"710\":1}}],[\"桶号查表\",{\"1\":{\"710\":1}}],[\"桶号最终用来查\",{\"1\":{\"710\":1}}],[\"桶映射\",{\"1\":{\"710\":2}}],[\"桶映射的目的\",{\"1\":{\"710\":1}}],[\"桶\",{\"1\":{\"710\":3}}],[\"爱因斯坦求和规则\",{\"1\":{\"709\":1}}],[\"爱因斯坦求和约定\",{\"1\":{\"475\":1}}],[\"⋅\",{\"1\":{\"709\":1}}],[\"𝑃\",{\"1\":{\"915\":1}}],[\"𝑖\",{\"1\":{\"707\":1}}],[\"𝐿\",{\"1\":{\"707\":1}}],[\"周枫\",{\"1\":{\"837\":1}}],[\"周期长\",{\"1\":{\"706\":1}}],[\"周期短\",{\"1\":{\"706\":1}}],[\"周围四个像素为\",{\"1\":{\"505\":1}}],[\"横着看是计算某个词与全局序列中其他词的相关度\",{\"1\":{\"703\":1}}],[\"临时替换特殊标记\",{\"1\":{\"697\":1}}],[\"落在文章的第\",{\"1\":{\"694\":1}}],[\"落后于chinchilla\",{\"1\":{\"668\":1}}],[\"差不多\",{\"1\":{\"694\":1}}],[\"企鹅不擅长飞行\",{\"1\":{\"692\":2}}],[\"企业又有比较好的自有数据\",{\"1\":{\"601\":1}}],[\"纠错\",{\"1\":{\"691\":1}}],[\"炼成\",{\"1\":{\"690\":1}}],[\"领略强化学习之力\",{\"1\":{\"837\":1}}],[\"领先推理力\",{\"1\":{\"823\":1}}],[\"领先\",{\"1\":{\"685\":1}}],[\"领域专业知识能力欠缺\",{\"1\":{\"828\":1}}],[\"领域差异\",{\"1\":{\"669\":1}}],[\"领域逐渐转向预训练语言模型\",{\"1\":{\"646\":1}}],[\"领域的进展仍处于早期阶段\",{\"1\":{\"639\":1}}],[\"领域常用的文本transformer模型\",{\"1\":{\"407\":1}}],[\"领域中语言表示学习的演进\",{\"1\":{\"650\":1}}],[\"领域中的一些对比学习方法\",{\"1\":{\"406\":1}}],[\"领域中\",{\"1\":{\"388\":1}}],[\"领域所在中心点查询特征\",{\"1\":{\"119\":1}}],[\"领域内最近邻键特征\",{\"1\":{\"119\":1}}],[\"领域情况\",{\"1\":{\"26\":2}}],[\"领域\",{\"1\":{\"3\":1,\"696\":1}}],[\"静态计算图\",{\"1\":{\"811\":1}}],[\"静态掩码\",{\"1\":{\"681\":1}}],[\"静态与动态掩码\",{\"1\":{\"681\":1}}],[\"静态方法\",{\"0\":{\"455\":1}}],[\"静态方法是类中的一种特殊方法\",{\"1\":{\"424\":1}}],[\"答案可以是任意文本\",{\"1\":{\"735\":1}}],[\"答案是\",{\"1\":{\"846\":1}}],[\"答案是原文中的一段\",{\"1\":{\"735\":1}}],[\"答案是否必须在原文中\",{\"1\":{\"735\":1}}],[\"答案是有的\",{\"1\":{\"610\":1}}],[\"答案应该在这段文字中的第\",{\"1\":{\"735\":1}}],[\"答案必须是原文中的一段文本\",{\"1\":{\"735\":1}}],[\"答案必须是原文中的连续片段\",{\"1\":{\"735\":1}}],[\"答案必存在于上下文中\",{\"1\":{\"680\":1}}],[\"答案就是\",{\"1\":{\"694\":1}}],[\"答案一定会出现在文章中\",{\"1\":{\"694\":1}}],[\"认知偏差等\",{\"1\":{\"824\":1}}],[\"认为\",{\"1\":{\"679\":1}}],[\"认真合理\",{\"1\":{\"657\":1}}],[\"社区治理框架\",{\"1\":{\"670\":1}}],[\"社会影响与部署考量\",{\"1\":{\"658\":1}}],[\"社会契约式\",{\"1\":{\"658\":1}}],[\"社会\",{\"1\":{\"658\":1}}],[\"社会偏见分析\",{\"1\":{\"670\":1}}],[\"社会偏见\",{\"1\":{\"655\":1}}],[\"凸显运算符重载的可读性优势\",{\"1\":{\"811\":1}}],[\"凸显公开数据训练的固有挑战\",{\"1\":{\"670\":1}}],[\"凸显领域微调的重要性\",{\"1\":{\"668\":1}}],[\"宗教类别偏见最显著\",{\"1\":{\"670\":1}}],[\"伦理隐私\",{\"1\":{\"830\":1}}],[\"伦理和风险问题\",{\"1\":{\"824\":1}}],[\"伦理响应\",{\"1\":{\"669\":1}}],[\"伦理风险\",{\"1\":{\"27\":1}}],[\"责任缺陷\",{\"1\":{\"668\":1}}],[\"灵感来自gpt\",{\"1\":{\"667\":1}}],[\"灵活适应问答系统\",{\"1\":{\"828\":1}}],[\"灵活控制\",{\"0\":{\"814\":1}}],[\"灵活的语言系统发展\",{\"1\":{\"646\":1}}],[\"灵活的组合方式\",{\"1\":{\"304\":1}}],[\"灵活性\",{\"1\":{\"336\":1}}],[\"灵活性与通用性\",{\"1\":{\"280\":1}}],[\"灵活分辨率\",{\"1\":{\"323\":1}}],[\"呼应ai伦理需求\",{\"1\":{\"666\":1}}],[\"呼应了t5等模型的\",{\"1\":{\"650\":1}}],[\"闭区间\",{\"1\":{\"847\":1}}],[\"闭卷问答\",{\"1\":{\"666\":1,\"668\":1}}],[\"闭包得到所有\",{\"1\":{\"847\":1}}],[\"闭包\",{\"1\":{\"449\":2,\"450\":1}}],[\"闭包是一个函数\",{\"1\":{\"448\":1}}],[\"闭包与高阶导数\",{\"0\":{\"446\":1}}],[\"历史缓存\",{\"1\":{\"663\":1}}],[\"历史最高频合并对的频率\",{\"1\":{\"595\":1}}],[\"堆叠而成\",{\"1\":{\"663\":1}}],[\"删除序列最后一个token\",{\"1\":{\"893\":1}}],[\"删除了大量无关代码\",{\"1\":{\"663\":1}}],[\"删除已创建的环境\",{\"0\":{\"554\":1}}],[\"部署服务\",{\"1\":{\"834\":1}}],[\"部署等任务\",{\"1\":{\"834\":1}}],[\"部署建议\",{\"1\":{\"658\":1}}],[\"部分图像\",{\"1\":{\"895\":1}}],[\"部分保证了生成的多样性与样本质量\",{\"1\":{\"894\":1}}],[\"部分结果如下\",{\"1\":{\"816\":1}}],[\"部分带来位置相关性\",{\"1\":{\"709\":1}}],[\"部分带来内容相关性\",{\"1\":{\"709\":1}}],[\"部分内容需分多段呈现以保留关键信息\",{\"1\":{\"655\":1}}],[\"部分任务甚至达到或超越微调模型的水平\",{\"1\":{\"651\":1}}],[\"部分任务甚至超越商业模型\",{\"1\":{\"322\":1}}],[\"部分任务性能接近或超越监督基线模型\",{\"1\":{\"639\":1}}],[\"部分能力甚至超越gpt\",{\"1\":{\"337\":1}}],[\"部分模型甚至可以在单个gpu上运行\",{\"1\":{\"665\":1}}],[\"部分模型\",{\"1\":{\"300\":1}}],[\"部分代码细节没有详细介绍\",{\"1\":{\"293\":1}}],[\"部分如图1所示\",{\"1\":{\"280\":1}}],[\"部分的代码实现\",{\"1\":{\"274\":1}}],[\"部分的翻译\",{\"1\":{\"179\":1}}],[\"部分翻译内容的总结\",{\"1\":{\"174\":1}}],[\"部分缓解了计算负担\",{\"1\":{\"109\":1}}],[\"部分\",{\"1\":{\"67\":2,\"70\":1,\"187\":1,\"735\":1,\"837\":2,\"936\":1}}],[\"部分研究开始映射语义功能到3d结构\",{\"1\":{\"31\":1}}],[\"部分工作通过语言理解在2d数据中定位功能区域\",{\"1\":{\"31\":1}}],[\"拒绝模型\",{\"1\":{\"658\":1}}],[\"驯化\",{\"1\":{\"658\":1}}],[\"虚假宣传等敏感场景下\",{\"1\":{\"658\":1}}],[\"虚线之下是self\",{\"1\":{\"621\":1}}],[\"虚线之上是标准的cot的过程\",{\"1\":{\"621\":1}}],[\"虚线为\",{\"1\":{\"502\":1}}],[\"规范性\",{\"1\":{\"848\":1}}],[\"规范\",{\"1\":{\"658\":1}}],[\"规模相当的数据集\",{\"1\":{\"888\":1}}],[\"规模显著扩大\",{\"1\":{\"823\":1}}],[\"规模化研究\",{\"1\":{\"671\":1}}],[\"规模悖论\",{\"1\":{\"670\":1}}],[\"规模定律\",{\"1\":{\"641\":1}}],[\"规模与处理\",{\"1\":{\"640\":1}}],[\"规模一致性\",{\"1\":{\"135\":1}}],[\"规模\",{\"1\":{\"30\":1,\"893\":2}}],[\"尚待探索的问题\",{\"1\":{\"658\":1}}],[\"歧义加权\",{\"1\":{\"658\":1}}],[\"南瓜吸引炮弹\",{\"1\":{\"658\":1}}],[\"吃袜子\",{\"1\":{\"658\":1}}],[\"炸碎\",{\"1\":{\"657\":1}}],[\"炮弹打南瓜\",{\"1\":{\"657\":1}}],[\"胡编乱造\",{\"1\":{\"657\":1}}],[\"冥想后吃袜子有何用\",{\"1\":{\"657\":1}}],[\"荒谬\",{\"1\":{\"657\":1}}],[\"尊重\",{\"1\":{\"657\":1}}],[\"倾向于不作伪答\",{\"1\":{\"657\":1}}],[\"倾斜曲面\",{\"1\":{\"30\":1}}],[\"客户助手场景\",{\"1\":{\"657\":1}}],[\"±\",{\"1\":{\"657\":2}}],[\"毒性模式\",{\"1\":{\"670\":1}}],[\"毒性生成评估\",{\"1\":{\"670\":1}}],[\"毒性\",{\"1\":{\"656\":1}}],[\"幻觉\",{\"1\":{\"657\":1,\"828\":1}}],[\"幻觉率等多个维度的性能\",{\"1\":{\"656\":1}}],[\"幻觉控制\",{\"1\":{\"335\":1}}],[\"σ²\",{\"1\":{\"931\":5}}],[\"σ\",{\"1\":{\"656\":1,\"847\":12,\"931\":1}}],[\"故事类文本\",{\"1\":{\"680\":1}}],[\"故事生成\",{\"1\":{\"655\":1}}],[\"故特别强调标注者的社会敏感性\",{\"1\":{\"656\":1}}],[\"雇佣了约\",{\"1\":{\"656\":1}}],[\"突\",{\"1\":{\"713\":1}}],[\"突出了真实用户需求的多样性\",{\"1\":{\"656\":1}}],[\"突破transformer缺少归纳偏置的限制\",{\"1\":{\"422\":1}}],[\"突破了传统视觉模型\",{\"1\":{\"303\":1}}],[\"明确展示了这三步流程之间的数据流和优化路径\",{\"1\":{\"656\":1}}],[\"诚实\",{\"1\":{\"654\":1}}],[\"奖励模型\",{\"1\":{\"656\":1}}],[\"奖励模型训练\",{\"1\":{\"654\":1,\"656\":1}}],[\"奖励模型的目标是模拟人类的偏好判断\",{\"1\":{\"339\":1}}],[\"补洞\",{\"1\":{\"952\":1}}],[\"补集规则\",{\"1\":{\"848\":1}}],[\"补集操作\",{\"1\":{\"847\":1}}],[\"补全\",{\"1\":{\"656\":1}}],[\"补足了先前few\",{\"1\":{\"650\":1}}],[\"补充多样性\",{\"1\":{\"667\":1}}],[\"补充方式\",{\"1\":{\"587\":1}}],[\"补充了\",{\"1\":{\"282\":1}}],[\"补充说明\",{\"1\":{\"207\":1}}],[\"补充实验中还使用了\",{\"1\":{\"176\":1}}],[\"补充的交互方式\",{\"1\":{\"52\":1}}],[\"补充\",{\"0\":{\"218\":1,\"345\":1},\"1\":{\"11\":1,\"343\":1}}],[\"金融决策等\",{\"1\":{\"649\":1}}],[\"工作流程\",{\"0\":{\"829\":1}}],[\"工作中的一些问题\",{\"1\":{\"601\":1}}],[\"工具使用与智能体\",{\"1\":{\"833\":1}}],[\"工具和智能体\",{\"1\":{\"833\":1}}],[\"工具函数\",{\"1\":{\"810\":1}}],[\"工具包重新实现了\",{\"1\":{\"680\":1}}],[\"工程实践\",{\"1\":{\"807\":1}}],[\"工程等高精度领域的适用性\",{\"1\":{\"649\":1}}],[\"科研界给这些庞大的语言模型起了个名字\",{\"1\":{\"822\":1}}],[\"科研\",{\"1\":{\"649\":1}}],[\"科学问答\",{\"1\":{\"342\":1}}],[\"科学问答等实际应用任务\",{\"1\":{\"340\":1}}],[\"科学理解\",{\"1\":{\"335\":1}}],[\"程序员就少了一个需要调节的超参数\",{\"1\":{\"951\":1}}],[\"程序生成等\",{\"1\":{\"649\":1}}],[\"程序向\",{\"1\":{\"107\":1}}],[\"瓶颈\",{\"1\":{\"649\":1}}],[\"瓶颈式架构\",{\"1\":{\"95\":1}}],[\"逻辑\",{\"1\":{\"899\":1}}],[\"逻辑推理和结构化知识方面展现出卓越性能\",{\"1\":{\"823\":1}}],[\"逻辑缺陷\",{\"1\":{\"669\":1}}],[\"逻辑比较\",{\"1\":{\"649\":1}}],[\"逻辑转置中\",{\"1\":{\"545\":1}}],[\"逻辑转置\",{\"1\":{\"545\":1}}],[\"甚至是diffusion模型\",{\"1\":{\"961\":1}}],[\"甚至直接是单位矩阵的倍数\",{\"1\":{\"944\":1}}],[\"甚至指概率度量\",{\"1\":{\"847\":1}}],[\"甚至引发内存不足错误\",{\"1\":{\"806\":1}}],[\"甚至根据上下文进行\",{\"1\":{\"691\":1}}],[\"甚至可能损害模型表现\",{\"1\":{\"678\":1}}],[\"甚至超越了后续提出的模型\",{\"1\":{\"677\":1}}],[\"甚至在零样本和单样本设置下也能取得有竞争力的结果\",{\"1\":{\"645\":1}}],[\"甚至不如从零训练的\",{\"1\":{\"242\":1}}],[\"讨论\",{\"0\":{\"642\":1,\"658\":1}}],[\"证实模型并非简单记忆\",{\"1\":{\"641\":1}}],[\"证明运算符重载与自动微分机制的一致性\",{\"1\":{\"809\":1}}],[\"证明指令适应的高效性\",{\"1\":{\"668\":1}}],[\"证明无监督预训练在多任务迁移中的巨大潜力\",{\"1\":{\"641\":1}}],[\"证明自然语言指令可激活任务特定行为\",{\"1\":{\"641\":1}}],[\"证明gpt\",{\"1\":{\"639\":1}}],[\"证明模型具有有效处理上下文长距离的信息的能力\",{\"1\":{\"634\":1}}],[\"证明其确实为下游任务获取到了有用的语言知识\",{\"1\":{\"626\":1}}],[\"证明\",{\"1\":{\"500\":1,\"678\":1}}],[\"证明了bert原始设计的潜力尚未被充分挖掘\",{\"1\":{\"688\":1}}],[\"证明了可以将bert的方法和vison\",{\"1\":{\"388\":1}}],[\"证明了其自动捕获高层视觉知识的能力\",{\"1\":{\"228\":1}}],[\"独热编码\",{\"1\":{\"857\":1}}],[\"独角兽新闻\",{\"1\":{\"641\":1}}],[\"独立采样\",{\"1\":{\"952\":1}}],[\"独立同分布于\",{\"1\":{\"904\":1}}],[\"独立学习\",{\"1\":{\"710\":1}}],[\"独立编码图像\",{\"1\":{\"303\":1}}],[\"独立被遮或不遮\",{\"1\":{\"263\":1}}],[\"独立遮挡\",{\"1\":{\"263\":1}}],[\"英语词库数据\",{\"1\":{\"696\":1}}],[\"英罗等语言对的few\",{\"1\":{\"648\":1}}],[\"英德\",{\"1\":{\"648\":1}}],[\"英→法\",{\"1\":{\"641\":1}}],[\"英文全称\",{\"1\":{\"106\":1}}],[\"阅读理解任务\",{\"1\":{\"737\":1}}],[\"阅读理解与逻辑推理任务表现不一\",{\"1\":{\"648\":1}}],[\"阅读理解\",{\"1\":{\"641\":1}}],[\"命名实体识别\",{\"1\":{\"736\":1}}],[\"命名实体准确率89\",{\"1\":{\"641\":1}}],[\"命名实体\",{\"1\":{\"641\":1}}],[\"命令行工具\",{\"1\":{\"834\":1}}],[\"命令后加上\",{\"1\":{\"107\":1}}],[\"命令中\",{\"1\":{\"107\":1}}],[\"奠定了基础\",{\"1\":{\"640\":1}}],[\"触发摘要生成\",{\"1\":{\"640\":1}}],[\"延长训练时间\",{\"1\":{\"688\":1}}],[\"延长预训练时间\",{\"1\":{\"242\":1}}],[\"延续了\",{\"1\":{\"640\":1}}],[\"≥3\",{\"1\":{\"640\":1}}],[\"间接学习任务\",{\"1\":{\"639\":1}}],[\"间存在功能冲突\",{\"1\":{\"179\":1}}],[\"狭窄的专家\",{\"1\":{\"639\":1}}],[\"翻译成大图像的方法\",{\"1\":{\"956\":1}}],[\"翻译成最终的生成图像\",{\"1\":{\"956\":1}}],[\"翻译成中文\",{\"1\":{\"339\":1}}],[\"翻译语言等\",{\"1\":{\"827\":1}}],[\"翻译等任务上的表现仍远未达到实用水平\",{\"1\":{\"642\":1}}],[\"翻译等任务表现仍远逊于专业系统\",{\"1\":{\"641\":1}}],[\"翻译任务\",{\"1\":{\"640\":1}}],[\"翻译示例\",{\"1\":{\"640\":1}}],[\"翻译的文本示例\",{\"1\":{\"639\":1}}],[\"翻译\",{\"1\":{\"638\":1,\"641\":1}}],[\"蕴含确定和文本分类\",{\"1\":{\"636\":1}}],[\"威诺格拉德模式\",{\"1\":{\"635\":1}}],[\"情感表达\",{\"1\":{\"823\":1}}],[\"情感分析\",{\"1\":{\"635\":1}}],[\"情况下\",{\"1\":{\"661\":1,\"662\":1,\"877\":1}}],[\"情况下结果一致\",{\"1\":{\"521\":1}}],[\"情况完全不一样\",{\"1\":{\"353\":1}}],[\"纽约大学发布的有关语法的数据集\",{\"1\":{\"634\":1}}],[\"评判两个文本语义信息的相似度\",{\"1\":{\"634\":1}}],[\"评估基准\",{\"1\":{\"680\":1}}],[\"评估方式\",{\"1\":{\"656\":1}}],[\"评估方法与设定\",{\"1\":{\"647\":1}}],[\"评估语言模型的风险与危害\",{\"1\":{\"655\":1}}],[\"评估工具\",{\"1\":{\"334\":1}}],[\"评估协议\",{\"1\":{\"286\":1}}],[\"评估阶段则是在验证集或测试集上评估模型的性能\",{\"1\":{\"106\":1}}],[\"评估\",{\"0\":{\"106\":1},\"1\":{\"103\":1}}],[\"评估集\",{\"1\":{\"92\":1}}],[\"评估时需要标准的单一样本对比\",{\"1\":{\"53\":1}}],[\"评估指标\",{\"1\":{\"46\":1}}],[\"美国知识问答网站\",{\"1\":{\"634\":1}}],[\"挑战\",{\"1\":{\"657\":1}}],[\"挑战了传统对\",{\"1\":{\"650\":1}}],[\"挑战在于识别语句是否是概念改写\",{\"1\":{\"634\":1}}],[\"挑战模型对可供性的泛化能力\",{\"1\":{\"43\":1}}],[\"矛盾或中立\",{\"1\":{\"634\":1}}],[\"涉及\",{\"1\":{\"656\":1}}],[\"涉及读取一对句子\",{\"1\":{\"634\":1}}],[\"涉及两个重要概念\",{\"1\":{\"540\":1}}],[\"困惑度仅从8\",{\"1\":{\"641\":1}}],[\"困惑度从99\",{\"1\":{\"641\":1}}],[\"困惑度\",{\"1\":{\"633\":1}}],[\"困难负样本挖掘\",{\"1\":{\"376\":1}}],[\"困难负样本挖掘策略\",{\"1\":{\"172\":1}}],[\"符来分隔两者\",{\"1\":{\"631\":1}}],[\"符合某个简单的数学分布\",{\"1\":{\"956\":1}}],[\"符合框架设计需求\",{\"1\":{\"806\":1}}],[\"符合我们直接观念所想的大模型微调流程为\",{\"1\":{\"609\":1}}],[\"符合认知的大模型微调流程\",{\"0\":{\"609\":1}}],[\"符合\",{\"1\":{\"274\":1}}],[\"$\",{\"1\":{\"631\":1}}],[\"观察下图\",{\"1\":{\"804\":1}}],[\"观察倒水场景时\",{\"1\":{\"30\":1}}],[\"观测发现用辅助目标能提升性能\",{\"1\":{\"630\":1}}],[\"段落级别或者句子级别的\",{\"1\":{\"627\":1}}],[\"迁移它来稍微适应一系列广泛的任务\",{\"1\":{\"626\":1}}],[\"迁移学习\",{\"1\":{\"510\":1}}],[\"迁移学习的能力\",{\"1\":{\"268\":1}}],[\"问\",{\"1\":{\"850\":1}}],[\"问答系统中的候选答案选择\",{\"1\":{\"737\":1}}],[\"问答和文本生成等\",{\"1\":{\"645\":1}}],[\"问答和常识推理\",{\"1\":{\"631\":1,\"634\":1}}],[\"问答示例\",{\"1\":{\"642\":1}}],[\"问答任务\",{\"0\":{\"733\":1},\"1\":{\"640\":1,\"694\":2}}],[\"问答或文本蕴含\",{\"1\":{\"631\":1}}],[\"问答提升5\",{\"1\":{\"626\":1}}],[\"问答\",{\"1\":{\"625\":1,\"626\":1,\"635\":1,\"640\":1,\"641\":2,\"646\":1,\"656\":1}}],[\"问题建模为一个自回归语言建模任务\",{\"1\":{\"891\":1}}],[\"问题分解能力\",{\"1\":{\"823\":1}}],[\"问题格式甚至换行方式都可能造成性能大幅波动\",{\"1\":{\"649\":1}}],[\"问题和答案\",{\"1\":{\"631\":1}}],[\"问题的损失函数改进方案\",{\"1\":{\"589\":1}}],[\"问题来了\",{\"1\":{\"413\":1}}],[\"问题\",{\"1\":{\"346\":1,\"502\":1,\"594\":1,\"641\":1,\"657\":1,\"681\":1,\"683\":1,\"710\":1,\"733\":3,\"735\":1,\"737\":2}}],[\"问题在于\",{\"1\":{\"220\":1,\"944\":1}}],[\"问题所在\",{\"1\":{\"157\":1}}],[\"问题嵌入\",{\"1\":{\"100\":1}}],[\"问题编码后的文本特征\",{\"1\":{\"96\":1}}],[\"问题条件化查询\",{\"1\":{\"94\":1}}],[\"问题文本不为空\",{\"1\":{\"92\":1}}],[\"问题文本\",{\"1\":{\"92\":1}}],[\"问题数\",{\"1\":{\"89\":1}}],[\"问题总数\",{\"1\":{\"89\":1,\"91\":1}}],[\"问题配对是固定的\",{\"1\":{\"90\":1}}],[\"问题配对\",{\"1\":{\"89\":1,\"91\":1,\"93\":1}}],[\"问题背景\",{\"1\":{\"53\":1,\"119\":1,\"148\":1}}],[\"教程起源于加州大学伯克利分校和卡内基梅隆大学的读书分享\",{\"1\":{\"942\":1}}],[\"教育工具等\",{\"1\":{\"658\":1}}],[\"教llm把复杂问题\",{\"1\":{\"622\":1}}],[\"教师输出更新\",{\"1\":{\"293\":1}}],[\"教师输出\",{\"1\":{\"293\":1}}],[\"教师输出减去中心并进行温度锐化\",{\"1\":{\"293\":1}}],[\"教师输出和当前\",{\"1\":{\"293\":1}}],[\"教师参数\",{\"1\":{\"293\":1}}],[\"教师参数是通过\",{\"1\":{\"283\":1}}],[\"教师梯度冻结\",{\"1\":{\"293\":1}}],[\"教师温度调度表\",{\"1\":{\"293\":1}}],[\"教师温度\",{\"1\":{\"286\":1}}],[\"教师的表现始终优于学生\",{\"1\":{\"285\":1}}],[\"教师网络只看\",{\"1\":{\"293\":1}}],[\"教师网络只处理全局视角\",{\"1\":{\"285\":1}}],[\"教师网络不更新梯度\",{\"1\":{\"293\":1}}],[\"教师网络不参与梯度更新\",{\"1\":{\"293\":1}}],[\"教师网络构建完成\",{\"1\":{\"293\":1}}],[\"教师网络初始参数与学生网络相同\",{\"1\":{\"293\":1}}],[\"教师网络\",{\"1\":{\"293\":1}}],[\"教师网络扮演关键角色\",{\"1\":{\"289\":1}}],[\"教师网络的选择\",{\"0\":{\"289\":1}}],[\"教师网络也有类似公式\",{\"1\":{\"285\":1}}],[\"教师也会从学生蒸馏\",{\"1\":{\"283\":1}}],[\"教师模型输出\",{\"1\":{\"293\":1}}],[\"教师模型\",{\"1\":{\"215\":1}}],[\"教师模型影响\",{\"1\":{\"215\":1}}],[\"教师模型的特征表示\",{\"1\":{\"213\":1}}],[\"教师模型设置\",{\"1\":{\"213\":1}}],[\"教师模型类型\",{\"1\":{\"213\":1}}],[\"教师\",{\"1\":{\"26\":1,\"168\":1,\"293\":1}}],[\"绿色标记出的部分是llm输出的推理过程\",{\"1\":{\"620\":1}}],[\"绿色为\",{\"1\":{\"177\":1}}],[\"背后的大致逻辑\",{\"1\":{\"619\":1}}],[\"背景类别极端不平衡的问题\",{\"1\":{\"589\":1}}],[\"背景知识扫盲\",{\"0\":{\"158\":1}}],[\"背景与动机\",{\"1\":{\"109\":1,\"646\":1}}],[\"背景点\",{\"1\":{\"102\":1}}],[\"背景特征\",{\"1\":{\"83\":1}}],[\"背景\",{\"0\":{\"112\":1,\"131\":1,\"339\":1,\"415\":1,\"679\":1,\"740\":1},\"1\":{\"83\":1,\"107\":1,\"589\":1}}],[\"背景区域特征图\",{\"1\":{\"83\":1}}],[\"背景区域特征图经过roi\",{\"1\":{\"83\":1}}],[\"背景区域特征过程的实现细节如下\",{\"1\":{\"83\":1}}],[\"背景区域特征\",{\"1\":{\"83\":1}}],[\"他们再次使用了停止梯度的技术\",{\"1\":{\"960\":1}}],[\"他们希望利用\",{\"1\":{\"831\":1}}],[\"他们之间的关系是独立的\",{\"1\":{\"55\":1}}],[\"他写了包括\",{\"1\":{\"733\":1}}],[\"他利用自己已掌握的知识\",{\"1\":{\"619\":1}}],[\"秩的选择\",{\"0\":{\"613\":1},\"1\":{\"613\":1}}],[\"旁添加一条旁路\",{\"1\":{\"611\":1}}],[\"感兴趣的朋友\",{\"1\":{\"607\":1}}],[\"感知在噪声和损坏条件下的鲁棒性\",{\"1\":{\"20\":1}}],[\"限于篇幅原因\",{\"1\":{\"607\":1}}],[\"限制\",{\"1\":{\"924\":1}}],[\"限制某像素只能看到图像中\",{\"1\":{\"924\":1}}],[\"限制卷积时只能看到左边和上方的像素\",{\"1\":{\"924\":1}}],[\"限制输入文本长度不超过最大\",{\"1\":{\"895\":1}}],[\"限制输入来自同一文档\",{\"1\":{\"681\":1}}],[\"限制哪些位置可以预测哪些\",{\"1\":{\"893\":1}}],[\"限制与盲点\",{\"1\":{\"658\":1}}],[\"限制进程可见的\",{\"1\":{\"520\":1}}],[\"限制了模型在多样化场景中的应用能力\",{\"1\":{\"639\":1}}],[\"限制了研究社区的应用和优化\",{\"1\":{\"325\":1}}],[\"限制了细节理解能力\",{\"1\":{\"323\":1}}],[\"限制了其在多模态任务中的表现\",{\"1\":{\"298\":1}}],[\"限制了llm的能力利用\",{\"1\":{\"296\":1}}],[\"限制了\",{\"1\":{\"280\":1}}],[\"限制了框架的普及性\",{\"1\":{\"27\":1}}],[\"限制本次遮挡的patch数量不超过最大遮挡数\",{\"1\":{\"263\":1}}],[\"∆w\",{\"1\":{\"606\":3}}],[\"∆w为m\",{\"1\":{\"606\":1}}],[\"往输入序列x前面加特定的token\",{\"1\":{\"605\":1}}],[\"往往远多于正样本\",{\"1\":{\"589\":1}}],[\"往往难以准确衡量学到的表示能力\",{\"1\":{\"273\":1}}],[\"跟prompt\",{\"1\":{\"605\":1}}],[\"跟踪等视觉识别任务\",{\"1\":{\"269\":1}}],[\"句法解析\",{\"1\":{\"626\":1}}],[\"句号等标点符号结合语义进行断句\",{\"1\":{\"595\":1}}],[\"句子级别表示\",{\"1\":{\"733\":1}}],[\"句子嵌入\",{\"1\":{\"699\":2}}],[\"句子分隔列表\",{\"1\":{\"698\":1}}],[\"句子中20\",{\"1\":{\"698\":1}}],[\"句子\",{\"1\":{\"274\":1,\"696\":1}}],[\"漏字填空\",{\"1\":{\"690\":1}}],[\"漏检\",{\"1\":{\"590\":1}}],[\"漏报\",{\"1\":{\"590\":1}}],[\"漏掉的真正例\",{\"1\":{\"572\":1}}],[\"技巧\",{\"1\":{\"951\":1}}],[\"技巧性近似\",{\"1\":{\"589\":1}}],[\"技术从过滤后的\",{\"1\":{\"898\":1}}],[\"技术进行条件引导预测\",{\"1\":{\"895\":1}}],[\"技术领域的迅猛发展浪潮中\",{\"1\":{\"833\":1}}],[\"技术范式\",{\"1\":{\"826\":1}}],[\"技术细节必须伴随规范治理与透明流程\",{\"1\":{\"658\":1}}],[\"技术\",{\"1\":{\"355\":1,\"380\":1}}],[\"技术配置\",{\"1\":{\"334\":1}}],[\"求出最近的下标\",{\"1\":{\"958\":1}}],[\"求出了\",{\"1\":{\"925\":1}}],[\"求最近邻\",{\"1\":{\"958\":1}}],[\"求导\",{\"1\":{\"959\":1}}],[\"求导并令偏导数为零可知\",{\"1\":{\"904\":1}}],[\"求导后令导数为零\",{\"1\":{\"904\":1}}],[\"求\",{\"1\":{\"589\":1}}],[\"求和\",{\"1\":{\"100\":1,\"160\":1,\"475\":2}}],[\"^γ\",{\"1\":{\"589\":1}}],[\"^2\",{\"1\":{\"213\":1}}],[\"罕见疾病诊断等\",{\"1\":{\"589\":1}}],[\"任意有限\",{\"1\":{\"847\":1}}],[\"任何类别不平衡的分类任务\",{\"1\":{\"589\":1}}],[\"任务损失\",{\"1\":{\"700\":2}}],[\"任务会利用\",{\"1\":{\"699\":2}}],[\"任务通常需要不同的模型\",{\"1\":{\"690\":1}}],[\"任务通用性\",{\"1\":{\"641\":1}}],[\"任务复杂性上限未显现\",{\"1\":{\"657\":1}}],[\"任务类型\",{\"1\":{\"656\":1}}],[\"任务多样性与实用性\",{\"1\":{\"655\":1}}],[\"任务模式\",{\"1\":{\"648\":1}}],[\"任务不可知\",{\"1\":{\"647\":1}}],[\"任务不仅可以学习到表征\",{\"1\":{\"223\":1}}],[\"任务提示的关键作用\",{\"1\":{\"640\":1}}],[\"任务执行的零样本机制\",{\"1\":{\"640\":1}}],[\"任务可以通过自然语言描述\",{\"1\":{\"640\":1}}],[\"任务涉及预测两个句子在语义上是否相等\",{\"1\":{\"634\":1}}],[\"任务需求\",{\"1\":{\"593\":1}}],[\"任务间接学习\",{\"1\":{\"385\":1}}],[\"任务也能进一步提升模型效果\",{\"1\":{\"376\":1}}],[\"任务结合起来\",{\"1\":{\"354\":1}}],[\"任务上表现更优\",{\"1\":{\"678\":1}}],[\"任务上评估\",{\"1\":{\"242\":1}}],[\"任务上进行预训练\",{\"1\":{\"229\":1}}],[\"任务深受\",{\"1\":{\"234\":1}}],[\"任务特定层\",{\"1\":{\"213\":1}}],[\"任务采样负样本索引\",{\"1\":{\"192\":1}}],[\"任务的本质\",{\"1\":{\"735\":1}}],[\"任务的通用架构该有多好\",{\"1\":{\"690\":1}}],[\"任务的核心步骤\",{\"1\":{\"681\":1}}],[\"任务的\",{\"1\":{\"656\":1}}],[\"任务的损失权重\",{\"1\":{\"384\":1}}],[\"任务的标签\",{\"1\":{\"384\":1}}],[\"任务的预训练效果\",{\"1\":{\"214\":1}}],[\"任务的目标是最小化交叉熵损失\",{\"1\":{\"200\":1}}],[\"任务的二分类头\",{\"1\":{\"192\":1}}],[\"任务的评估体系\",{\"1\":{\"106\":1}}],[\"任务\",{\"1\":{\"106\":1,\"192\":1,\"216\":1,\"228\":1,\"234\":1,\"382\":1,\"384\":1,\"385\":1,\"386\":1,\"648\":1,\"690\":1,\"699\":3}}],[\"任务中表现出色\",{\"1\":{\"824\":1}}],[\"任务中用于生成功能区域掩码的核心模块\",{\"1\":{\"100\":1}}],[\"任务中\",{\"1\":{\"95\":1,\"205\":1,\"280\":1,\"368\":1,\"737\":1}}],[\"任务设定\",{\"1\":{\"73\":1}}],[\"任务相比\",{\"1\":{\"19\":1}}],[\"病灶区域像素远少于正常组织\",{\"1\":{\"589\":1}}],[\"医学图像分割\",{\"1\":{\"589\":1}}],[\"倍\",{\"1\":{\"589\":1,\"823\":2,\"885\":1,\"893\":1}}],[\"综述\",{\"1\":{\"837\":1}}],[\"综上\",{\"1\":{\"589\":1}}],[\"综合指标上\",{\"1\":{\"310\":1}}],[\"综合了\",{\"1\":{\"246\":1}}],[\"综合最近几个熟人的意见\",{\"1\":{\"145\":1}}],[\"抗类别不平衡能力强\",{\"1\":{\"587\":1}}],[\"ϵ\",{\"1\":{\"586\":1}}],[\"阈值化\",{\"1\":{\"582\":1}}],[\"投两次硬币\",{\"1\":{\"846\":1}}],[\"投注了更高的注意力\",{\"1\":{\"582\":1}}],[\"投影结果\",{\"1\":{\"380\":1}}],[\"投影层的丢弃率\",{\"1\":{\"430\":1}}],[\"投影层\",{\"1\":{\"341\":2,\"342\":2}}],[\"投影头\",{\"1\":{\"285\":1,\"293\":1}}],[\"投影到统一词空间\",{\"1\":{\"893\":1}}],[\"投影到\",{\"1\":{\"274\":1,\"293\":1,\"385\":1}}],[\"投影到相同维度\",{\"1\":{\"120\":1}}],[\"投影会压缩几何信息\",{\"1\":{\"110\":1}}],[\"投影型网络\",{\"1\":{\"110\":1}}],[\"投影统一\",{\"1\":{\"83\":1}}],[\"投影\",{\"1\":{\"69\":5,\"192\":2,\"403\":1}}],[\"投影维度\",{\"1\":{\"69\":1}}],[\"压缩\",{\"1\":{\"963\":1}}],[\"压缩时间\",{\"1\":{\"70\":1}}],[\"压扁\",{\"1\":{\"578\":1}}],[\"偏移半个像素\",{\"1\":{\"944\":1}}],[\"偏离\",{\"1\":{\"908\":1}}],[\"偏好临近\",{\"1\":{\"710\":1}}],[\"偏好反馈机制\",{\"1\":{\"658\":1}}],[\"偏好条件化能力\",{\"1\":{\"658\":1}}],[\"偏见正相关\",{\"1\":{\"668\":1}}],[\"偏见与毒性分析\",{\"1\":{\"668\":1}}],[\"偏见或捏造内容\",{\"1\":{\"658\":1}}],[\"偏见水平相当\",{\"1\":{\"657\":1}}],[\"偏见\",{\"1\":{\"656\":1}}],[\"偏见评估数据集\",{\"1\":{\"655\":1}}],[\"偏见问题\",{\"1\":{\"646\":1}}],[\"偏差小但罕见的要严惩\",{\"1\":{\"578\":1}}],[\"偏差大但常见的就不判定为\",{\"1\":{\"578\":1}}],[\"偏置硬币\",{\"1\":{\"907\":1}}],[\"偏置加法公式\",{\"1\":{\"710\":1}}],[\"偏置表的定义\",{\"1\":{\"710\":1}}],[\"偏置表里的每个元素是一个标量\",{\"1\":{\"710\":1}}],[\"偏置表作用\",{\"1\":{\"710\":1}}],[\"偏置表\",{\"1\":{\"710\":3}}],[\"偏置参数\",{\"1\":{\"522\":1}}],[\"偏置与归一化层的关系\",{\"0\":{\"522\":1}}],[\"偏置\",{\"1\":{\"380\":5}}],[\"惩罚太远\",{\"1\":{\"710\":1}}],[\"惩罚项\",{\"1\":{\"656\":1}}],[\"惩罚更重\",{\"1\":{\"592\":2}}],[\"惩罚多\",{\"1\":{\"578\":1}}],[\"惩罚少\",{\"1\":{\"578\":1}}],[\"支配\",{\"1\":{\"578\":1}}],[\"支持的区域为0\",{\"1\":{\"913\":1}}],[\"支持的文件后缀类型\",{\"1\":{\"424\":1}}],[\"支持智能体工作负载\",{\"1\":{\"833\":1}}],[\"支持最大\",{\"1\":{\"823\":1}}],[\"支持思考模式和非思考模式之间无缝切换\",{\"1\":{\"823\":1}}],[\"支持更复杂的系统提示词控制\",{\"1\":{\"823\":1}}],[\"支持更广泛的下游任务\",{\"1\":{\"650\":1}}],[\"支持处理极长的文档和对话历史\",{\"1\":{\"823\":1}}],[\"支持标准模式与推理思考模式\",{\"1\":{\"823\":1}}],[\"支持自定义知识库和行为模式\",{\"1\":{\"823\":1}}],[\"支持手势识别和情感表达\",{\"1\":{\"823\":1}}],[\"支持保存为png\",{\"1\":{\"815\":1}}],[\"支持显示变量名\",{\"1\":{\"815\":1}}],[\"支持高阶导数的构建\",{\"1\":{\"814\":1}}],[\"支持高分辨率输入\",{\"1\":{\"329\":1}}],[\"支持variable与数值\",{\"1\":{\"812\":1}}],[\"支持用+\",{\"1\":{\"812\":1}}],[\"支持python原生控制流\",{\"1\":{\"811\":1}}],[\"支持将数学公式直接转译为python代码\",{\"1\":{\"811\":1}}],[\"支持x\",{\"1\":{\"809\":1}}],[\"支持x1\",{\"1\":{\"809\":2}}],[\"支持x0\",{\"1\":{\"809\":2}}],[\"支持y\",{\"1\":{\"809\":1}}],[\"支持3\",{\"1\":{\"809\":1}}],[\"支持与ndarray及数值类型的混合运算\",{\"1\":{\"809\":1}}],[\"支持为变量设置自定义名称\",{\"1\":{\"808\":1}}],[\"支持代码重构和扩展\",{\"1\":{\"796\":1}}],[\"支持数据的修改和读取\",{\"1\":{\"758\":1}}],[\"支持无答案问题\",{\"1\":{\"680\":1}}],[\"支持复杂对话\",{\"1\":{\"669\":1}}],[\"支持这一点\",{\"1\":{\"655\":1}}],[\"支持基于上下文的词表示\",{\"1\":{\"650\":1}}],[\"支持对任何文本\",{\"1\":{\"640\":1}}],[\"支持对比学习任务\",{\"1\":{\"304\":1}}],[\"支持加权和平均损失\",{\"1\":{\"586\":1}}],[\"支持常见的字典操作\",{\"1\":{\"516\":1}}],[\"支持混合精度训练\",{\"1\":{\"510\":1}}],[\"支持两种llm\",{\"1\":{\"421\":1}}],[\"支持两种配置\",{\"1\":{\"304\":1}}],[\"支持图文\",{\"1\":{\"382\":1}}],[\"支持同时处理文本和图像模态\",{\"1\":{\"380\":1}}],[\"支持生成任务\",{\"1\":{\"377\":1}}],[\"支持4k输入\",{\"1\":{\"337\":1}}],[\"支持4k分辨率\",{\"1\":{\"323\":1}}],[\"支持零样本扩展至\",{\"1\":{\"334\":1}}],[\"支持文本\",{\"1\":{\"325\":1}}],[\"支持文本和图像输入\",{\"1\":{\"325\":1}}],[\"支持多轮对话的同时\",{\"1\":{\"823\":1}}],[\"支持多轮视觉对话\",{\"1\":{\"342\":1}}],[\"支持多种自然语言处理任务\",{\"1\":{\"823\":1}}],[\"支持多种任务模式\",{\"1\":{\"304\":1}}],[\"支持多工具并行调用与精准指令解析\",{\"1\":{\"823\":1}}],[\"支持多个输入与输出\",{\"1\":{\"800\":1}}],[\"支持多维度同时交换\",{\"1\":{\"468\":1}}],[\"支持多模态问答\",{\"1\":{\"303\":1}}],[\"支持多粒度可供性推理\",{\"1\":{\"23\":1}}],[\"支持区域描述和问答\",{\"1\":{\"300\":1}}],[\"支持感知\",{\"1\":{\"296\":1}}],[\"支持温度调度\",{\"1\":{\"293\":1}}],[\"支持刚性变换标准化\",{\"1\":{\"157\":1}}],[\"支持原始点云\",{\"1\":{\"157\":1}}],[\"支持切片操作\",{\"1\":{\"123\":1}}],[\"支持通过爱因斯坦求和约定\",{\"1\":{\"100\":1}}],[\"支持\",{\"1\":{\"73\":1,\"106\":2,\"208\":4,\"213\":1,\"472\":1,\"586\":1,\"587\":1,\"699\":1,\"823\":1,\"963\":1}}],[\"尺度差异性\",{\"0\":{\"578\":1}}],[\"尺寸\",{\"1\":{\"380\":1,\"900\":1}}],[\"身高\",{\"1\":{\"574\":1}}],[\"身高和体重的方差都为\",{\"1\":{\"574\":1}}],[\"身高和体重\",{\"1\":{\"574\":1}}],[\"身份证号\",{\"1\":{\"137\":1}}],[\"马氏距离适合不同维度尺度差别大\",{\"1\":{\"579\":1}}],[\"马氏距离通过协方差矩阵逆变换\",{\"1\":{\"578\":1}}],[\"马氏距离中对第一个维度的偏差乘上\",{\"1\":{\"578\":1}}],[\"马氏距离会自动把不同特征的偏差按标准差进行\",{\"1\":{\"578\":1}}],[\"马氏距离定义如下\",{\"1\":{\"578\":1}}],[\"马氏距离等\",{\"1\":{\"574\":1}}],[\"马氏距离\",{\"0\":{\"575\":1,\"577\":1},\"1\":{\"574\":1}}],[\"椭圆朝反对角线方向倾斜\",{\"1\":{\"574\":1}}],[\"圆形\",{\"1\":{\"574\":1}}],[\"较低的阈值\",{\"1\":{\"572\":1}}],[\"较高\",{\"1\":{\"572\":1}}],[\"较慢\",{\"1\":{\"505\":1}}],[\"源掩码\",{\"1\":{\"750\":1}}],[\"源注意力子层\",{\"1\":{\"749\":2}}],[\"源码链接\",{\"1\":{\"689\":1}}],[\"源自二战雷达检测\",{\"1\":{\"569\":1}}],[\"源点云\",{\"1\":{\"122\":1}}],[\"源点云的特征\",{\"1\":{\"122\":1}}],[\"源点云坐标\",{\"1\":{\"122\":1}}],[\"您选择优先考虑的指标取决于特定问题的成本\",{\"1\":{\"566\":1}}],[\"召回率曲线的创建方法是\",{\"1\":{\"571\":1}}],[\"召回率曲线\",{\"1\":{\"571\":1}}],[\"召回率会提高\",{\"1\":{\"565\":1}}],[\"召回率比准确率更有意义\",{\"1\":{\"563\":1}}],[\"召回率衡量的是被正确分类为垃圾邮件的垃圾邮件电子邮件的比例\",{\"1\":{\"563\":1}}],[\"召回率的定义为\",{\"1\":{\"563\":1}}],[\"召回率\",{\"0\":{\"563\":1},\"1\":{\"566\":1}}],[\"召回率等指标上的表现\",{\"1\":{\"593\":1}}],[\"召回率等\",{\"1\":{\"382\":1}}],[\"垃圾邮件被误分类为非垃圾邮件\",{\"1\":{\"561\":1}}],[\"垃圾邮件被正确分类为垃圾邮件\",{\"1\":{\"561\":1}}],[\"假嵌入\",{\"1\":{\"958\":1,\"961\":2}}],[\"假如我们是在生成\",{\"1\":{\"925\":1}}],[\"假数据\",{\"1\":{\"918\":1}}],[\"假定其独立同分布于参数为\",{\"1\":{\"904\":1}}],[\"假阴性\",{\"1\":{\"590\":2}}],[\"假阳性\",{\"1\":{\"590\":2}}],[\"假负例通常比假正例的后果更严重\",{\"1\":{\"563\":1}}],[\"假负例是指被误分类为负例的实际正例\",{\"1\":{\"563\":1}}],[\"假负例或假正例\",{\"1\":{\"562\":1}}],[\"假负例\",{\"1\":{\"561\":1}}],[\"假正例是被错误分类的实际负例\",{\"1\":{\"564\":1}}],[\"假正例率为\",{\"1\":{\"564\":1}}],[\"假正例率\",{\"0\":{\"564\":1},\"1\":{\"564\":1,\"566\":1}}],[\"假正例\",{\"1\":{\"561\":1}}],[\"假设不知道输入图像\",{\"1\":{\"959\":1}}],[\"假设嵌入空间已经训练完毕\",{\"1\":{\"958\":1}}],[\"假设潜变量\",{\"1\":{\"943\":1}}],[\"假设较弱\",{\"1\":{\"942\":1}}],[\"假设你想生成一张\",{\"1\":{\"924\":1}}],[\"假设你有一个问题\",{\"1\":{\"737\":1}}],[\"假设配置\",{\"1\":{\"892\":1}}],[\"假设样本空间是实数集合的子集\",{\"1\":{\"847\":1}}],[\"假设原始上下文是\",{\"1\":{\"735\":1}}],[\"假设序列长度\",{\"1\":{\"709\":1}}],[\"假设\",{\"1\":{\"706\":1,\"710\":2,\"904\":1,\"916\":1,\"964\":1}}],[\"假设最终的输出\",{\"1\":{\"694\":1}}],[\"假设有标记数据集\",{\"1\":{\"630\":1}}],[\"假设有一个完美的模型\",{\"1\":{\"565\":1}}],[\"假设有一个\",{\"1\":{\"544\":1}}],[\"假设预训练的矩阵为\",{\"1\":{\"611\":1}}],[\"假设要在下游任务微调一个预训练语言模型\",{\"1\":{\"611\":1}}],[\"假设模型在任务适配过程中权重的改变量是低秩\",{\"1\":{\"610\":1}}],[\"假设两个正样本\",{\"1\":{\"589\":1}}],[\"假设我们不仅想生成新的数字\",{\"1\":{\"952\":1}}],[\"假设我们有一个\",{\"1\":{\"859\":1}}],[\"假设我们有一个检测疾病的筛查工具\",{\"1\":{\"850\":1}}],[\"假设我们有一个样本点\",{\"1\":{\"578\":1}}],[\"假设我们有预测概率图\",{\"1\":{\"592\":1}}],[\"假设我们有如下样本\",{\"1\":{\"574\":1}}],[\"假设我们在\",{\"1\":{\"534\":1}}],[\"假设的理想模型的\",{\"1\":{\"569\":1}}],[\"假设一个完美的模型不会出现假负例\",{\"1\":{\"563\":1}}],[\"假设为\",{\"1\":{\"546\":1}}],[\"假设为常规行优先存储\",{\"1\":{\"546\":1}}],[\"假设为正方形\",{\"1\":{\"255\":1}}],[\"假设输出坐标映射到输入坐标\",{\"1\":{\"505\":1}}],[\"假设输入图像\",{\"1\":{\"959\":1}}],[\"假设输入图像经过卷积得到一个特征图\",{\"1\":{\"501\":1}}],[\"假设输入的token序列为\",{\"1\":{\"703\":1}}],[\"假设输入特征图大小为\",{\"1\":{\"504\":1}}],[\"假设输入数据维度为\",{\"1\":{\"60\":1}}],[\"假设某个\",{\"1\":{\"501\":1}}],[\"假设图像有n个像素\",{\"1\":{\"921\":1}}],[\"假设图像是正方形的\",{\"1\":{\"899\":1}}],[\"假设图像\",{\"1\":{\"231\":1}}],[\"假设空间中所有区域的尺度或特征分布具有一定的一致性\",{\"1\":{\"135\":1}}],[\"真相\",{\"1\":{\"877\":1}}],[\"真正随机的是实验结果\",{\"1\":{\"846\":1}}],[\"真正例率\",{\"0\":{\"563\":1},\"1\":{\"563\":1,\"566\":1}}],[\"真正例\",{\"1\":{\"561\":1}}],[\"真阳性\",{\"1\":{\"590\":2}}],[\"真负例\",{\"1\":{\"561\":1}}],[\"真实数据分布\",{\"1\":{\"918\":1}}],[\"真实token对应1\",{\"1\":{\"713\":1}}],[\"真实性缺陷\",{\"1\":{\"670\":1}}],[\"真实性\",{\"1\":{\"656\":1}}],[\"真实用户在\",{\"1\":{\"656\":1}}],[\"真实掩码\",{\"1\":{\"587\":1}}],[\"真实统计意义上的远近\",{\"1\":{\"578\":1}}],[\"真实的图像\",{\"1\":{\"942\":1}}],[\"真实的\",{\"1\":{\"106\":1}}],[\"真实标签对应的概率为\",{\"1\":{\"200\":1}}],[\"真实标签\",{\"1\":{\"64\":1,\"106\":1,\"202\":1,\"586\":1,\"587\":2,\"590\":1,\"592\":2}}],[\"混乱\",{\"1\":{\"907\":1}}],[\"混淆矩阵\",{\"0\":{\"561\":1}}],[\"混合专家模型\",{\"1\":{\"823\":1}}],[\"混合精度训练和分布式优化部分略过\",{\"1\":{\"889\":1}}],[\"混合精度训练\",{\"1\":{\"680\":1}}],[\"混合模型的常见结合方式\",{\"1\":{\"434\":1}}],[\"混合模型探索\",{\"0\":{\"434\":1}}],[\"混合模型改进\",{\"1\":{\"157\":1}}],[\"混合表征\",{\"1\":{\"385\":1}}],[\"混合多模态专家模型\",{\"1\":{\"380\":1}}],[\"混合特征方法\",{\"1\":{\"327\":1}}],[\"混合两种损失\",{\"1\":{\"208\":1}}],[\"混合比\",{\"1\":{\"208\":1}}],[\"混合编码器\",{\"1\":{\"187\":1}}],[\"混合\",{\"1\":{\"95\":1}}],[\"纯公开数据训练结果挑战了专有数据的必要性\",{\"1\":{\"668\":1}}],[\"纯\",{\"1\":{\"557\":1}}],[\"纯视觉模式\",{\"1\":{\"303\":1}}],[\"纯视觉模型\",{\"1\":{\"303\":1}}],[\"安装和导入依赖\",{\"0\":{\"930\":1}}],[\"安装失败的包\",{\"1\":{\"557\":1}}],[\"安装所需要的依赖包\",{\"1\":{\"557\":1}}],[\"安全且有用地遵循用户指令\",{\"1\":{\"654\":1}}],[\"安全起见\",{\"1\":{\"494\":1}}],[\"安全\",{\"1\":{\"178\":1,\"658\":1}}],[\"退火至\",{\"1\":{\"886\":1}}],[\"退火进行学习率衰减\",{\"1\":{\"236\":1}}],[\"退出with块后\",{\"1\":{\"807\":1}}],[\"退出当前环境\",{\"0\":{\"552\":1}}],[\"退化为ce\",{\"1\":{\"589\":1}}],[\"杂谈\",{\"0\":{\"548\":1}}],[\"满三次后再打包成一个group\",{\"1\":{\"547\":1}}],[\"满足了每个像素只能接受之前像素的信息这一约束\",{\"1\":{\"923\":1}}],[\"满足下式\",{\"1\":{\"849\":1}}],[\"满足某些一致性要求\",{\"1\":{\"845\":1}}],[\"满足不同场景需求\",{\"1\":{\"823\":1}}],[\"满足特定关系时\",{\"1\":{\"489\":1}}],[\"满足以下两个条件之一即可\",{\"1\":{\"447\":1}}],[\"满足\",{\"1\":{\"234\":1}}],[\"≤\",{\"1\":{\"546\":1,\"710\":1}}],[\"物理数据未改变\",{\"1\":{\"546\":1}}],[\"物理转置\",{\"1\":{\"545\":1}}],[\"物体摆放不合逻辑\",{\"1\":{\"884\":1}}],[\"物体扭曲\",{\"1\":{\"884\":1}}],[\"物体在空间中移动时\",{\"1\":{\"161\":1}}],[\"物体分割和场景语义解析\",{\"1\":{\"148\":1}}],[\"物体名称匹配\",{\"1\":{\"92\":1}}],[\"物体组合\",{\"1\":{\"89\":1}}],[\"物体区域特征和点云特征的联合建模\",{\"1\":{\"83\":1}}],[\"物体区域特征图展平\",{\"1\":{\"83\":1}}],[\"物体的哪一部分因为主体和环境的作用而具备可交互潜能\",{\"1\":{\"83\":1}}],[\"物体点云的区域特征\",{\"1\":{\"83\":1}}],[\"物体几何结构文本\",{\"1\":{\"53\":2}}],[\"物体几何结构文本数据文件路径\",{\"1\":{\"53\":1}}],[\"物体几何知识特征\",{\"1\":{\"37\":1}}],[\"物体\",{\"1\":{\"50\":1,\"91\":1}}],[\"物体类别索引\",{\"1\":{\"92\":1}}],[\"物体类别数\",{\"1\":{\"89\":2}}],[\"物体类别\",{\"1\":{\"41\":1,\"91\":1,\"92\":1}}],[\"物体交互感知\",{\"1\":{\"35\":1}}],[\"物体可供性定位方法\",{\"1\":{\"50\":1}}],[\"物体可供性定位任务\",{\"1\":{\"40\":1}}],[\"物体可供性\",{\"1\":{\"32\":1}}],[\"物体数据集的出现\",{\"1\":{\"20\":1}}],[\"物体操作以及自主决策\",{\"1\":{\"19\":1}}],[\"轴上绘制所有阈值下的召回率\",{\"1\":{\"571\":1}}],[\"轴上绘制精确率\",{\"1\":{\"571\":1}}],[\"轴张量\",{\"1\":{\"547\":1}}],[\"轴\",{\"1\":{\"545\":1}}],[\"属于图像部分\",{\"1\":{\"895\":1}}],[\"属于哪一代\",{\"1\":{\"805\":1}}],[\"属于自蒸馏的一种形式\",{\"1\":{\"195\":1}}],[\"属性\",{\"1\":{\"544\":1,\"546\":1,\"809\":1}}],[\"❓q\",{\"0\":{\"536\":1,\"537\":1}}],[\"仍表现不佳\",{\"1\":{\"658\":1}}],[\"仍会生成有害\",{\"1\":{\"658\":1}}],[\"仍可生成有害内容\",{\"1\":{\"657\":1}}],[\"仍需谨慎评估其边界与适用性\",{\"1\":{\"649\":1}}],[\"仍暴露出其推理深度与语言理解的局限\",{\"1\":{\"648\":1}}],[\"仍是\",{\"1\":{\"527\":1}}],[\"仍然是连续空间中的解码\",{\"1\":{\"963\":1}}],[\"仍然是标准正态分布\",{\"1\":{\"952\":1}}],[\"仍然需要大量的定制开发工作\",{\"1\":{\"831\":1}}],[\"仍然\",{\"1\":{\"710\":1}}],[\"仍然能工作\",{\"1\":{\"282\":1}}],[\"仍然不如分块处理或多层级聚合模型高效\",{\"1\":{\"157\":1}}],[\"浪费参数和计算\",{\"1\":{\"522\":1}}],[\"唯一需要的\",{\"1\":{\"522\":1}}],[\"唯一标签数组\",{\"1\":{\"514\":1}}],[\"冗余\",{\"1\":{\"522\":2}}],[\"禁止\",{\"1\":{\"521\":1}}],[\"禁用梯度计算\",{\"1\":{\"899\":1}}],[\"禁用\",{\"1\":{\"898\":1}}],[\"禁用模式下\",{\"1\":{\"807\":1}}],[\"禁用反向传播后\",{\"1\":{\"807\":1}}],[\"禁用反向传播\",{\"1\":{\"807\":2}}],[\"禁用反向传播的模式优化\",{\"1\":{\"807\":1}}],[\"禁用只是实现选择\",{\"1\":{\"385\":1}}],[\"禁用了\",{\"1\":{\"236\":1}}],[\"库和\",{\"1\":{\"696\":1}}],[\"库来实现\",{\"1\":{\"614\":1}}],[\"库\",{\"0\":{\"510\":1,\"512\":1,\"515\":1},\"1\":{\"810\":1}}],[\"库的函数\",{\"1\":{\"478\":1}}],[\"长\",{\"1\":{\"710\":1}}],[\"长距离\",{\"1\":{\"710\":2}}],[\"长文本处理能力较弱\",{\"1\":{\"828\":1}}],[\"长文本对话模型\",{\"1\":{\"823\":1}}],[\"长文本理解和推理能力\",{\"1\":{\"823\":1}}],[\"长文本阅读理解任务\",{\"1\":{\"680\":1}}],[\"长文本能让生成模型学习到长依赖信息的条件概率\",{\"1\":{\"633\":1}}],[\"长度的一维序列\",{\"1\":{\"892\":1}}],[\"长度的序列\",{\"1\":{\"707\":1}}],[\"长度为n\",{\"1\":{\"660\":1}}],[\"长度等于样本数\",{\"1\":{\"518\":1}}],[\"长度与\",{\"1\":{\"514\":1}}],[\"长度必须相同\",{\"1\":{\"513\":1}}],[\"长度固定\",{\"1\":{\"508\":1}}],[\"长度\",{\"1\":{\"506\":1,\"709\":2}}],[\"长边按比例缩放\",{\"1\":{\"425\":1}}],[\"彩色图像来说\",{\"1\":{\"924\":1}}],[\"彩色图像非常友好\",{\"1\":{\"505\":1}}],[\"彩色图像通常为3\",{\"1\":{\"380\":1}}],[\"划分成固定数量的网格区域\",{\"1\":{\"501\":1}}],[\"划分后可以得到共个patch\",{\"1\":{\"426\":1}}],[\"电路理论类比\",{\"1\":{\"500\":1}}],[\"折叠\",{\"1\":{\"500\":1,\"868\":1}}],[\"锯齿波\",{\"1\":{\"500\":1}}],[\"拟合一个\",{\"1\":{\"500\":1}}],[\"拟合分段函数时\",{\"1\":{\"500\":1}}],[\"爆炸问题\",{\"1\":{\"500\":1}}],[\"爆炸式增长\",{\"1\":{\"500\":1}}],[\"爆炸\",{\"1\":{\"500\":1}}],[\"龙格现象\",{\"1\":{\"500\":1}}],[\"案例2\",{\"1\":{\"500\":1}}],[\"案例1\",{\"1\":{\"500\":1}}],[\"案例\",{\"1\":{\"500\":1}}],[\"神经语言模型的规模定律\",{\"1\":{\"650\":1}}],[\"神经元\",{\"1\":{\"500\":2}}],[\"神经元过多易过拟合\",{\"1\":{\"500\":1}}],[\"神经元实现相同精度\",{\"1\":{\"500\":1}}],[\"神经元数量\",{\"1\":{\"500\":1}}],[\"神经网络会默认1是一个处于0\",{\"1\":{\"956\":1}}],[\"神经网络会默认输入满足一个连续的分布\",{\"1\":{\"956\":1}}],[\"神经网络通过非线性激活函数生成的动态基函数组合\",{\"1\":{\"500\":1}}],[\"神经网络\",{\"1\":{\"500\":2}}],[\"神经网络的权重矩阵通常以高精度的浮点数\",{\"1\":{\"614\":1}}],[\"神经网络的独特优势\",{\"1\":{\"500\":1}}],[\"神经网络的输出在训练初期往往接近于零\",{\"1\":{\"152\":1}}],[\"神经网络逼近\",{\"1\":{\"500\":2}}],[\"神经网络与深度学习\",{\"1\":{\"500\":1}}],[\"神经网络模型只认识数字\",{\"1\":{\"92\":1}}],[\"逼近真实后验分布\",{\"1\":{\"945\":1}}],[\"逼近fine\",{\"1\":{\"648\":1}}],[\"逼近\",{\"1\":{\"500\":1}}],[\"逼近精度与代价的权衡\",{\"1\":{\"500\":1}}],[\"逼近区间\",{\"1\":{\"500\":1}}],[\"逼近原理\",{\"1\":{\"500\":1}}],[\"逼近方式\",{\"1\":{\"500\":1}}],[\"泰勒展开要求函数无限可微\",{\"1\":{\"500\":1}}],[\"泰勒展开\",{\"1\":{\"500\":1}}],[\"挤压\",{\"1\":{\"500\":2}}],[\"绝大部分大模型应用都是采用的特定数据库\",{\"1\":{\"836\":1}}],[\"绝对位置编码\",{\"0\":{\"705\":1}}],[\"绝对位置编码参数\",{\"1\":{\"380\":1}}],[\"绝不复制数据\",{\"1\":{\"494\":1}}],[\"风险\",{\"1\":{\"493\":1}}],[\"风格等\",{\"1\":{\"944\":2}}],[\"风格\",{\"1\":{\"800\":1,\"895\":1}}],[\"风格延续任务中的偏好学习\",{\"1\":{\"655\":1}}],[\"风格归一化\",{\"1\":{\"264\":1}}],[\"风格的对比学习\",{\"1\":{\"305\":1}}],[\"风格的均值方差进行归一化\",{\"1\":{\"264\":1}}],[\"风格的\",{\"1\":{\"264\":2}}],[\"风格的视觉\",{\"1\":{\"252\":1}}],[\"风格的掩码建模在视觉领域的应用尚未充分研究\",{\"1\":{\"228\":1}}],[\"风格的编码器\",{\"1\":{\"125\":1}}],[\"许多机构和个人相继推出了多个开源项目\",{\"1\":{\"831\":1}}],[\"许多研究人员开始训练越来越庞大的语言模型\",{\"1\":{\"822\":1}}],[\"许多\",{\"1\":{\"492\":1}}],[\"许多现有方法通常分阶段训练模型组件\",{\"1\":{\"272\":1}}],[\"顺序感知\",{\"1\":{\"898\":1}}],[\"顺序地存储在一块内存中\",{\"1\":{\"489\":1}}],[\"顺序\",{\"1\":{\"489\":1}}],[\"顺序被随机打乱\",{\"1\":{\"483\":1}}],[\"普通\",{\"1\":{\"936\":1}}],[\"普通注意力部分\",{\"1\":{\"709\":1}}],[\"普通自注意力打分公式\",{\"1\":{\"709\":1}}],[\"普通计数\",{\"1\":{\"485\":1}}],[\"普通方法\",{\"0\":{\"455\":1}}],[\"举个简单例子\",{\"0\":{\"916\":1}}],[\"举个例子\",{\"1\":{\"618\":1,\"735\":1,\"878\":1}}],[\"举个二维例子\",{\"1\":{\"482\":1}}],[\"举例来说\",{\"1\":{\"694\":1}}],[\"举例\",{\"1\":{\"190\":1,\"293\":1,\"518\":1}}],[\"举例说明\",{\"1\":{\"157\":1}}],[\"访问\",{\"1\":{\"474\":1,\"492\":1}}],[\"报错\",{\"1\":{\"472\":1}}],[\"必要\",{\"1\":{\"522\":2}}],[\"必要时用\",{\"1\":{\"470\":1}}],[\"必要时会复制数据\",{\"1\":{\"470\":1}}],[\"必需\",{\"1\":{\"488\":2}}],[\"必须确保对数据集中的每个样本\",{\"1\":{\"943\":1}}],[\"必须遵循概率公理\",{\"1\":{\"848\":1}}],[\"必须通过\",{\"1\":{\"809\":1}}],[\"必须落在上下文部分\",{\"1\":{\"735\":1}}],[\"必须为两个独立的词\",{\"1\":{\"595\":1}}],[\"必须\",{\"1\":{\"493\":1,\"513\":1}}],[\"必须传入连续张量\",{\"1\":{\"493\":1}}],[\"必须精确指定每维重复次数\",{\"1\":{\"472\":1}}],[\"必须用高阶函数\",{\"1\":{\"449\":1}}],[\"必须结合以下组件\",{\"1\":{\"280\":1}}],[\"必须新增遮挡patch数>0且不超过最大允许遮挡数\",{\"1\":{\"263\":1}}],[\"必须自己学会对各种姿态都识别准确\",{\"1\":{\"152\":1}}],[\"必须在两条分支同时加入相对位置编码\",{\"1\":{\"117\":1}}],[\"运算中非常常见且强大的一种机制\",{\"1\":{\"546\":1}}],[\"运算符到pow函数\",{\"1\":{\"809\":1}}],[\"运算符到neg函数\",{\"1\":{\"809\":1}}],[\"运算符优先级与类型转换\",{\"1\":{\"809\":1}}],[\"运算符的调用顺序遵循以下规则\",{\"1\":{\"809\":1}}],[\"运算符上\",{\"1\":{\"809\":1}}],[\"运算符重载需要同时考虑左右运算符\",{\"1\":{\"809\":1}}],[\"运算符重载\",{\"0\":{\"809\":1}}],[\"运算符\",{\"1\":{\"462\":1}}],[\"运行和与\",{\"1\":{\"834\":1}}],[\"运行代码\",{\"1\":{\"816\":1}}],[\"运行结果为\",{\"1\":{\"816\":1}}],[\"运行\",{\"1\":{\"815\":1}}],[\"运行上述代码\",{\"1\":{\"411\":1}}],[\"运行可视化\",{\"1\":{\"107\":1}}],[\"装饰整个类\",{\"0\":{\"456\":1}}],[\"装饰类方法\",{\"0\":{\"455\":1}}],[\"装饰器的底层原理与执行过程\",{\"0\":{\"457\":1}}],[\"装饰器的实现用到了什么\",{\"0\":{\"449\":1}}],[\"装饰器会改变函数的元信息\",{\"1\":{\"454\":1}}],[\"装饰器工厂\",{\"0\":{\"453\":1},\"1\":{\"460\":1}}],[\"装饰器支持原函数有参数的情况\",{\"1\":{\"452\":1}}],[\"装饰器主要用于在\",{\"1\":{\"450\":1}}],[\"装饰器是\",{\"1\":{\"450\":1}}],[\"装饰器\",{\"0\":{\"450\":1,\"511\":1},\"1\":{\"449\":1,\"450\":1,\"454\":1,\"457\":1}}],[\"装饰器模式\",{\"1\":{\"293\":1}}],[\"元\",{\"1\":{\"823\":2}}],[\"元素\",{\"1\":{\"710\":1}}],[\"元素为\",{\"1\":{\"710\":1}}],[\"元素是按\",{\"1\":{\"481\":1}}],[\"元学习和模型扩展趋势等多个重要研究方向的交汇点上\",{\"1\":{\"650\":1}}],[\"元学习\",{\"1\":{\"650\":1}}],[\"元学习与few\",{\"1\":{\"650\":1}}],[\"元学习与上下文学习的潜力\",{\"1\":{\"646\":1}}],[\"元编程\",{\"1\":{\"454\":1}}],[\"元组等\",{\"1\":{\"806\":1}}],[\"元组\",{\"1\":{\"120\":2,\"121\":2,\"122\":2,\"466\":1}}],[\"持续推动着技术边界的拓展\",{\"1\":{\"833\":1}}],[\"持续下降\",{\"1\":{\"640\":1}}],[\"持续追踪日志最新输出\",{\"1\":{\"107\":1}}],[\"持有对\",{\"1\":{\"451\":1}}],[\"权限控制\",{\"1\":{\"461\":1}}],[\"权限校验等场景中非常常见\",{\"1\":{\"450\":1}}],[\"权重增加到\",{\"1\":{\"886\":1}}],[\"权重共享\",{\"1\":{\"699\":3,\"892\":1}}],[\"权重大\",{\"1\":{\"518\":1}}],[\"权重越高\",{\"1\":{\"518\":1}}],[\"权重越大\",{\"1\":{\"145\":1,\"518\":1}}],[\"权重最低\",{\"1\":{\"514\":1}}],[\"权重最高\",{\"1\":{\"514\":1}}],[\"权重较低\",{\"1\":{\"514\":1}}],[\"权重初始化\",{\"1\":{\"428\":1,\"431\":1,\"521\":1,\"633\":1}}],[\"权重衰减0\",{\"1\":{\"667\":1}}],[\"权重衰减系数为\",{\"1\":{\"236\":1}}],[\"权重衰减\",{\"1\":{\"224\":1,\"286\":1,\"293\":2,\"315\":1,\"316\":1}}],[\"权重衰减为\",{\"1\":{\"176\":1}}],[\"权重由\",{\"1\":{\"160\":1}}],[\"权重归一化\",{\"1\":{\"145\":1}}],[\"权重\",{\"1\":{\"122\":1,\"145\":1,\"213\":2,\"257\":1,\"306\":1,\"589\":1,\"846\":1,\"892\":1}}],[\"权重向量\",{\"1\":{\"119\":1}}],[\"权重生成函数\",{\"1\":{\"119\":1}}],[\"权重项\",{\"1\":{\"102\":1}}],[\"绑定乘法运算符\",{\"1\":{\"809\":1}}],[\"绑定\",{\"1\":{\"448\":1,\"809\":2}}],[\"外循环\",{\"1\":{\"650\":1}}],[\"外层函数的作用域\",{\"1\":{\"444\":1}}],[\"外部代理工具整合到一起\",{\"1\":{\"832\":1}}],[\"外部知识\",{\"1\":{\"832\":1}}],[\"外部作用域变量\",{\"1\":{\"448\":1}}],[\"外部直接传入图像mask\",{\"1\":{\"384\":1}}],[\"外部直接传入图像embedding\",{\"1\":{\"384\":1}}],[\"外部传入的\",{\"1\":{\"208\":1}}],[\"沿计算图反向推导各变量的导数\",{\"1\":{\"779\":1}}],[\"沿\",{\"1\":{\"709\":1}}],[\"沿对角线方向拉长的椭圆\",{\"1\":{\"574\":1}}],[\"沿哪个维度分块\",{\"1\":{\"482\":1}}],[\"沿着某个特定维度需要跨越多少个元素\",{\"1\":{\"542\":1}}],[\"沿着哪个维度进行操作\",{\"1\":{\"488\":1}}],[\"沿着哪个维度计算累积和\",{\"1\":{\"481\":1}}],[\"沿着行方向\",{\"1\":{\"481\":1}}],[\"沿列拼接\",{\"1\":{\"441\":1}}],[\"沿行拼接\",{\"1\":{\"441\":1}}],[\"沿默认轴\",{\"1\":{\"441\":1}}],[\"沿nsample维度池化\",{\"1\":{\"121\":1}}],[\"架起了图像空间到文本空间的桥梁\",{\"1\":{\"436\":1}}],[\"架构来实现pixelcnn\",{\"1\":{\"923\":1}}],[\"架构改进\",{\"1\":{\"833\":1}}],[\"架构的神经网络模型开始崭露头角\",{\"1\":{\"822\":1}}],[\"架构微调\",{\"1\":{\"667\":1}}],[\"架构中\",{\"1\":{\"152\":1}}],[\"架构中的基本构建模块\",{\"1\":{\"120\":1}}],[\"架构\",{\"1\":{\"138\":1,\"144\":1,\"212\":1,\"268\":2,\"272\":1,\"329\":1,\"410\":1,\"415\":1,\"633\":1,\"640\":2,\"656\":1,\"823\":3,\"884\":1,\"887\":1}}],[\"赋一个非负权重\",{\"1\":{\"847\":1}}],[\"赋予局部特征提取的能力\",{\"1\":{\"434\":1}}],[\"赋给高斯颜色\",{\"1\":{\"22\":1}}],[\"换个更好理解的说法\",{\"1\":{\"956\":1}}],[\"换一个角度看\",{\"1\":{\"956\":1}}],[\"换种方式理解这个问题\",{\"1\":{\"946\":1}}],[\"换言之\",{\"1\":{\"904\":1,\"946\":1}}],[\"换源到图像上\",{\"1\":{\"433\":1}}],[\"换句话说\",{\"1\":{\"83\":1,\"145\":1,\"160\":1,\"349\":1,\"576\":1,\"709\":1,\"847\":1,\"950\":1,\"951\":1}}],[\"百万\",{\"1\":{\"432\":1}}],[\"右移\",{\"1\":{\"893\":1}}],[\"右乘\",{\"1\":{\"809\":1}}],[\"右操作数\",{\"1\":{\"809\":1}}],[\"右填充\",{\"1\":{\"713\":1}}],[\"右侧将这个编码过程拆分为两个步骤\",{\"1\":{\"950\":1}}],[\"右侧的损失函数值\",{\"1\":{\"951\":1}}],[\"右侧的表达式\",{\"1\":{\"948\":1}}],[\"右侧的ndarray或数值会被自动转换为variable\",{\"1\":{\"809\":1}}],[\"右侧是cot\",{\"1\":{\"620\":1}}],[\"右侧曲线的\",{\"1\":{\"572\":1}}],[\"右侧填充0个元素\",{\"1\":{\"477\":1}}],[\"右\",{\"1\":{\"547\":2,\"710\":1}}],[\"右图中\",{\"1\":{\"432\":1}}],[\"右边的意义\",{\"1\":{\"950\":1}}],[\"右边的第一项\",{\"1\":{\"946\":1}}],[\"右边的目标函数使用随机梯度下降进行优化呢\",{\"1\":{\"946\":1}}],[\"右边的结构也变得非常像一个自编码器\",{\"1\":{\"945\":1}}],[\"右边的编码是在线下执行\",{\"1\":{\"357\":1}}],[\"右边则是我们可以实际优化的目标\",{\"1\":{\"945\":1}}],[\"右边\",{\"1\":{\"709\":1,\"710\":2}}],[\"右边填充2个9\",{\"1\":{\"477\":1}}],[\"右边不填充\",{\"1\":{\"477\":1}}],[\"右边编码器会被之前的\",{\"1\":{\"353\":1}}],[\"键为元素\",{\"1\":{\"516\":1}}],[\"键\",{\"1\":{\"430\":2}}],[\"键向量\",{\"1\":{\"119\":1,\"362\":1,\"529\":1}}],[\"左上角区域\",{\"1\":{\"895\":1}}],[\"左操作数3是int\",{\"1\":{\"809\":1}}],[\"左操作数x是variable\",{\"1\":{\"809\":1}}],[\"左操作数优先\",{\"1\":{\"809\":1}}],[\"左\",{\"1\":{\"710\":1}}],[\"左边的\",{\"1\":{\"945\":1}}],[\"左边是我们想要的目标\",{\"1\":{\"945\":1}}],[\"左边\",{\"1\":{\"709\":1,\"710\":3}}],[\"左边做一次梯度回传之后\",{\"1\":{\"357\":1}}],[\"左侧是常规的prompt\",{\"1\":{\"620\":1}}],[\"左侧为transformer原始的encoder结构\",{\"1\":{\"429\":1}}],[\"边界清晰\",{\"1\":{\"582\":1}}],[\"边界过渡自然\",{\"1\":{\"505\":1}}],[\"边界进行离散化\",{\"1\":{\"502\":1}}],[\"边界坐标的量化\",{\"1\":{\"502\":1}}],[\"边长为该整数\",{\"1\":{\"426\":2}}],[\"边交互架构\",{\"1\":{\"110\":1}}],[\"归纳偏置能够帮助学习算法缩小搜索范围\",{\"1\":{\"422\":1}}],[\"归纳偏置\",{\"1\":{\"422\":1}}],[\"归一化输入\",{\"1\":{\"926\":1}}],[\"归一化输入图像\",{\"1\":{\"899\":1}}],[\"归一化常数\",{\"1\":{\"867\":1,\"871\":1}}],[\"归一化最大距离\",{\"1\":{\"710\":1}}],[\"归一化处理\",{\"1\":{\"647\":1}}],[\"归一化层对连续性的要求\",{\"0\":{\"493\":1}}],[\"归一化层\",{\"1\":{\"426\":1}}],[\"归一化层类型\",{\"1\":{\"380\":1}}],[\"归一化有助于模型更快地收敛\",{\"1\":{\"425\":1}}],[\"归一化为权重\",{\"1\":{\"401\":1}}],[\"归一化特征\",{\"1\":{\"319\":1}}],[\"归一化得到\",{\"1\":{\"285\":1}}],[\"归一化码本学习方法\",{\"1\":{\"216\":1}}],[\"归一化并\",{\"1\":{\"213\":1}}],[\"归一化代码\",{\"1\":{\"212\":1}}],[\"归一化权重\",{\"1\":{\"145\":1}}],[\"归一化因子\",{\"1\":{\"145\":1}}],[\"归一化注意力权重\",{\"1\":{\"119\":1}}],[\"归一化是必要的\",{\"1\":{\"117\":1}}],[\"归一化到单位球内\",{\"1\":{\"107\":1}}],[\"归一化后的点云数据\",{\"1\":{\"107\":1}}],[\"归一化\",{\"0\":{\"507\":1,\"508\":1},\"1\":{\"69\":1,\"107\":1,\"192\":2,\"206\":2,\"207\":2,\"212\":2,\"213\":4,\"285\":1,\"384\":1,\"385\":4,\"403\":1,\"508\":2,\"582\":1,\"663\":2,\"823\":1,\"926\":1}}],[\"充当了soft\",{\"1\":{\"421\":1}}],[\"充分利用数据编排框架的优势\",{\"1\":{\"833\":1}}],[\"充分利用了大型语言模型的强大能力\",{\"1\":{\"831\":1}}],[\"充分利用大规模图像单模态和文本单模态数据\",{\"1\":{\"368\":1}}],[\"充分利用不同质量的数据\",{\"1\":{\"305\":1}}],[\"充分利用图像和文本输入\",{\"1\":{\"273\":1}}],[\"充分吸收了高级区域抽象特征和文本特征\",{\"1\":{\"94\":1}}],[\"错误还是不知道\",{\"1\":{\"694\":1}}],[\"错误分析显示\",{\"1\":{\"641\":1}}],[\"错误分类为\",{\"1\":{\"30\":1}}],[\"错误做法\",{\"1\":{\"522\":1}}],[\"错位对齐\",{\"1\":{\"420\":1}}],[\"叠加的运算流程\",{\"1\":{\"420\":1}}],[\"叠加处理上采样后的特征\",{\"1\":{\"123\":1}}],[\"叠加处理下采样后的特征\",{\"1\":{\"123\":1}}],[\"缓冲区会被自动保存到模型的状态字典\",{\"1\":{\"474\":1}}],[\"缓冲区不会保存到\",{\"1\":{\"474\":1}}],[\"缓冲区的名称\",{\"1\":{\"474\":1}}],[\"缓存机制\",{\"1\":{\"895\":1}}],[\"缓存加速\",{\"1\":{\"895\":1}}],[\"缓存显著压缩为潜在向量来保证高效推理的同时不降低效果\",{\"1\":{\"823\":1}}],[\"缓存当前轮计算结果\",{\"1\":{\"663\":1}}],[\"缓存当前轮可重复利用的计算结果\",{\"1\":{\"660\":1}}],[\"缓存更友好\",{\"1\":{\"492\":1}}],[\"缓存key\",{\"1\":{\"420\":1}}],[\"缓存key和value\",{\"1\":{\"420\":1}}],[\"缓存的key和value\",{\"1\":{\"420\":1}}],[\"缓存和复用\",{\"1\":{\"420\":1}}],[\"缓存\",{\"1\":{\"420\":1,\"663\":1}}],[\"缓解了对齐损失\",{\"1\":{\"657\":1}}],[\"缓解深层网络的梯度问题\",{\"1\":{\"640\":1}}],[\"缓解\",{\"1\":{\"588\":1}}],[\"缓解噪声影响\",{\"1\":{\"194\":1}}],[\"缓解类别极度不平衡问题\",{\"1\":{\"102\":1}}],[\"好消息是\",{\"1\":{\"949\":1}}],[\"好\",{\"1\":{\"415\":3}}],[\"好的\",{\"1\":{\"271\":1,\"895\":1}}],[\"侧重于信息检索和融合外部知识\",{\"1\":{\"830\":1}}],[\"侧重于给这些点的特征分配更大的融合权重\",{\"1\":{\"96\":1}}],[\"侧重模态融合\",{\"1\":{\"415\":1}}],[\"曾有一段时期一直在追求更大的网络架构\",{\"1\":{\"415\":1}}],[\"庖丁解牛vit\",{\"0\":{\"422\":1}}],[\"庖丁解牛blip2\",{\"0\":{\"414\":1},\"1\":{\"414\":1}}],[\"庖丁解牛clip\",{\"0\":{\"404\":1}}],[\"谷歌提出的\",{\"1\":{\"610\":1}}],[\"谷歌利用强大的计算能力进行了预训练\",{\"1\":{\"413\":1}}],[\"谷歌采用的弱监督方法与以往方法的一个主要区别在于规模\",{\"1\":{\"413\":1}}],[\"搜索出来的图片\",{\"1\":{\"411\":1}}],[\"花卉图片分类\",{\"0\":{\"410\":1}}],[\"紧密相关\",{\"1\":{\"409\":1}}],[\"广泛的集成\",{\"1\":{\"833\":1}}],[\"广泛用于语言建模\",{\"1\":{\"696\":1}}],[\"广泛用于现代自然语言处理任务中\",{\"1\":{\"594\":1}}],[\"广泛应用于各类计算机视觉任务\",{\"1\":{\"405\":1}}],[\"广播相加\",{\"1\":{\"710\":1}}],[\"广播规则\",{\"1\":{\"709\":1}}],[\"广播通过\",{\"1\":{\"546\":1}}],[\"广播视图\",{\"1\":{\"470\":1}}],[\"广播\",{\"0\":{\"546\":1},\"1\":{\"157\":1,\"546\":2,\"710\":1,\"751\":1}}],[\"年开始\",{\"1\":{\"831\":1}}],[\"年推进过\",{\"1\":{\"827\":1}}],[\"年发布\",{\"1\":{\"823\":1}}],[\"年至\",{\"1\":{\"823\":1}}],[\"年左右\",{\"1\":{\"822\":1}}],[\"年深度学习先驱\",{\"1\":{\"822\":1}}],[\"年代\",{\"1\":{\"822\":1}}],[\"年代法国电影\",{\"1\":{\"658\":1}}],[\"年龄是0\",{\"1\":{\"956\":1}}],[\"年龄是偏老还是偏年轻\",{\"1\":{\"956\":1}}],[\"年龄\",{\"1\":{\"574\":1}}],[\"年\",{\"1\":{\"405\":1,\"822\":1,\"823\":49,\"826\":1,\"833\":1}}],[\"年可谓是视觉\",{\"1\":{\"405\":1}}],[\"年提出\",{\"1\":{\"97\":1}}],[\"┌────────▼───────────┐\",{\"1\":{\"402\":1}}],[\"┌────────┴───────────┐\",{\"1\":{\"402\":1}}],[\"│\",{\"1\":{\"402\":10}}],[\"拿到所有图片路径\",{\"1\":{\"411\":1}}],[\"拿到能够代表整段文本或者整个多模态表示的\",{\"1\":{\"397\":1}}],[\"拿到图片背景区域特征图\",{\"1\":{\"83\":1}}],[\"另\",{\"1\":{\"881\":1}}],[\"另外\",{\"1\":{\"601\":1,\"606\":1,\"618\":1,\"706\":2,\"925\":1}}],[\"另外vilt还使用了whole\",{\"1\":{\"393\":1}}],[\"另外vilt还设计了一个word\",{\"1\":{\"393\":1}}],[\"另一条是第二个\",{\"1\":{\"803\":1}}],[\"另一相关研究方向是使用自然语言指令训练模型以实现跨任务泛化\",{\"1\":{\"655\":1}}],[\"另一种思路是在转换后\",{\"1\":{\"428\":1}}],[\"另一种是基于\",{\"1\":{\"407\":1}}],[\"另一种是dual\",{\"1\":{\"391\":1}}],[\"另一种是同时使用完整的\",{\"1\":{\"306\":1}}],[\"另一流派\",{\"1\":{\"357\":1}}],[\"另一个特例是当\",{\"1\":{\"863\":1}}],[\"另一个是作为答案结束的可能性\",{\"1\":{\"733\":1}}],[\"另一个是masked\",{\"1\":{\"393\":1}}],[\"另一个技巧\",{\"1\":{\"619\":1}}],[\"另一个维度很小\",{\"1\":{\"578\":1}}],[\"另一个原因是nlp模型可以利用从互联网上收集的大量文本\",{\"1\":{\"413\":1}}],[\"另一个细节\",{\"1\":{\"356\":1}}],[\"另一个分支通过\",{\"1\":{\"19\":1}}],[\"另一方面在许多实际问题中\",{\"1\":{\"869\":1}}],[\"另一方面\",{\"1\":{\"272\":1}}],[\"另一类则采用独立的图像与文本编码器\",{\"1\":{\"195\":1}}],[\"另一部分特征是通过在当前分辨率直接对所有原始点应用单个pointnet得到的\",{\"1\":{\"142\":1}}],[\"摒弃了传统的目标检测和卷积视觉嵌入器\",{\"1\":{\"388\":1}}],[\"拷贝\",{\"1\":{\"386\":1,\"545\":1}}],[\"算力消耗较大\",{\"1\":{\"385\":1}}],[\"算法中经常用到的预训练模型\",{\"1\":{\"696\":1}}],[\"算法进行强化学习优化\",{\"1\":{\"656\":1}}],[\"算法预训练过程完整代码如下\",{\"1\":{\"595\":1}}],[\"算法预训练工作流程\",{\"1\":{\"595\":1}}],[\"算法对\",{\"1\":{\"213\":1,\"960\":1}}],[\"算法\",{\"1\":{\"121\":1,\"212\":1,\"950\":1}}],[\"算法流程\",{\"1\":{\"119\":1}}],[\"算法流程概述\",{\"1\":{\"119\":1}}],[\"算法在所有点集合中寻找最近的\",{\"1\":{\"119\":1}}],[\"资源开销\",{\"1\":{\"385\":1}}],[\"资源消耗高\",{\"1\":{\"27\":1}}],[\"里的那个\",{\"1\":{\"960\":1}}],[\"里的值不变\",{\"1\":{\"959\":1}}],[\"里的方法来生成图像\",{\"1\":{\"921\":1}}],[\"里的一个函数\",{\"1\":{\"918\":2}}],[\"里有两个角色\",{\"1\":{\"918\":1}}],[\"里查到对应的\",{\"1\":{\"709\":1}}],[\"里做了\",{\"1\":{\"510\":1}}],[\"里\",{\"1\":{\"474\":1,\"547\":2,\"710\":2,\"944\":1,\"959\":1,\"961\":1}}],[\"里面肯定含有整句话的完整信息\",{\"1\":{\"694\":1}}],[\"里面的变量外面也能用\",{\"1\":{\"444\":1}}],[\"里面包含了注意力机制\",{\"1\":{\"433\":1}}],[\"里面包含了非常丰富的成果\",{\"1\":{\"433\":1}}],[\"里面有n个图片\",{\"1\":{\"350\":1}}],[\"里取对应的相对位置向量\",{\"1\":{\"709\":1}}],[\"里取对应的查询向量\",{\"1\":{\"709\":1}}],[\"里取对应的\",{\"1\":{\"385\":1}}],[\"钩子方法\",{\"1\":{\"382\":1}}],[\"积攒起一批样本数据后\",{\"1\":{\"382\":1}}],[\"子集\",{\"1\":{\"888\":1}}],[\"子层的输入进行了\",{\"1\":{\"823\":1}}],[\"子序列\",{\"1\":{\"735\":1}}],[\"子进程里修改只对该子进程生效\",{\"1\":{\"520\":1}}],[\"子进程无法访问未分配的\",{\"1\":{\"520\":1}}],[\"子目录名的格式\",{\"1\":{\"410\":1}}],[\"子实现类为例\",{\"1\":{\"382\":1}}],[\"子类需要做的事情就非常简单了\",{\"1\":{\"382\":1}}],[\"子类\",{\"1\":{\"382\":1}}],[\"子类必须实现\",{\"1\":{\"382\":2}}],[\"子任务上\",{\"1\":{\"310\":1}}],[\"状态标志\",{\"1\":{\"382\":1}}],[\"状态参数初始化\",{\"1\":{\"213\":1}}],[\"生态圈\",{\"0\":{\"834\":1}}],[\"生物计算大模型\",{\"1\":{\"823\":1}}],[\"生命周期阶段\",{\"1\":{\"382\":11}}],[\"生成潜在变量\",{\"1\":{\"947\":1}}],[\"生成那些可以还原\",{\"1\":{\"945\":1}}],[\"生成更多类似但不完全相同的新样本\",{\"1\":{\"942\":1}}],[\"生成建模是机器学习中的一个重要领域\",{\"1\":{\"942\":1}}],[\"生成模型训练时\",{\"1\":{\"943\":1}}],[\"生成模型的任务就是捕捉这些像素间的依赖\",{\"1\":{\"942\":1}}],[\"生成模型学习\",{\"0\":{\"940\":1}}],[\"生成模型根据这些信息生成答案\",{\"1\":{\"829\":1}}],[\"生成新的图像\",{\"1\":{\"935\":1}}],[\"生成样本\",{\"1\":{\"935\":1,\"943\":1}}],[\"生成样本分析\",{\"1\":{\"641\":1}}],[\"生成参数\",{\"1\":{\"926\":1}}],[\"生成假样本\",{\"1\":{\"918\":2}}],[\"生成器代码实现\",{\"1\":{\"918\":1}}],[\"生成器想最小化判别器识别生成样本为假的概率\",{\"1\":{\"918\":1}}],[\"生成器将噪声\",{\"1\":{\"918\":1}}],[\"生成器力求生成更逼真的样本\",{\"1\":{\"918\":1}}],[\"生成器\",{\"1\":{\"918\":1}}],[\"生成器推导式里的循环变量\",{\"1\":{\"444\":1}}],[\"生成逼真的数据样本\",{\"1\":{\"918\":1}}],[\"生成对抗网络\",{\"0\":{\"901\":1},\"1\":{\"901\":1}}],[\"生成质量判别器\",{\"0\":{\"900\":1}}],[\"生成像素图像\",{\"1\":{\"885\":1}}],[\"生成图表和可视化报告\",{\"1\":{\"823\":1}}],[\"生成图像的过程被建模为高斯分布\",{\"1\":{\"944\":1}}],[\"生成图像的合成描述\",{\"1\":{\"165\":1}}],[\"生成图像在\",{\"1\":{\"884\":1}}],[\"生成图像\",{\"1\":{\"264\":1,\"935\":1,\"947\":1}}],[\"生成dot语言字符串\",{\"1\":{\"815\":1}}],[\"生成函数节点的dot描述\",{\"1\":{\"815\":1}}],[\"生成变量节点的dot描述\",{\"1\":{\"815\":1}}],[\"生成一组可能生成\",{\"1\":{\"945\":1}}],[\"生成一组动态卷积核\",{\"1\":{\"100\":2}}],[\"生成一段文字\",{\"1\":{\"885\":1}}],[\"生成一个上下文相关的表示\",{\"1\":{\"735\":1}}],[\"生成注意力掩码\",{\"1\":{\"713\":1}}],[\"生成注意力热力图\",{\"1\":{\"582\":1}}],[\"生成padding部分的mask列表\",{\"1\":{\"713\":1}}],[\"生成相对位置差矩阵\",{\"1\":{\"709\":1}}],[\"生成能力\",{\"1\":{\"669\":1,\"898\":1}}],[\"生成位置编码\",{\"1\":{\"663\":1}}],[\"生成有害或无关文本\",{\"1\":{\"654\":1}}],[\"生成摘要\",{\"1\":{\"641\":1}}],[\"生成多种不同推理路径所得的结果的集合\",{\"1\":{\"621\":1}}],[\"生成多模态表示\",{\"1\":{\"272\":1}}],[\"生成形状为\",{\"1\":{\"486\":1}}],[\"生成类别名称以及对应的数字索引\",{\"1\":{\"424\":1}}],[\"生成文本的最小长度\",{\"1\":{\"421\":1}}],[\"生成文本的最大长度\",{\"1\":{\"421\":1}}],[\"生成文本嵌入\",{\"1\":{\"410\":1,\"412\":1}}],[\"生成学习\",{\"0\":{\"421\":1}}],[\"生成学习的优势\",{\"1\":{\"268\":1}}],[\"生成压缩的视觉表示\",{\"1\":{\"420\":1}}],[\"生成text\",{\"1\":{\"420\":1}}],[\"生成比标签\",{\"1\":{\"418\":1}}],[\"生成两个新的\",{\"1\":{\"382\":1}}],[\"生成两张不同视角图像\",{\"1\":{\"264\":1}}],[\"生成任务\",{\"1\":{\"304\":1}}],[\"生成和对话任务\",{\"1\":{\"296\":1}}],[\"生成损失不仅提升\",{\"1\":{\"276\":1}}],[\"生成损失\",{\"1\":{\"272\":2}}],[\"生成跨模态的图文联合表示\",{\"1\":{\"272\":1}}],[\"生成式\",{\"1\":{\"735\":2}}],[\"生成式generative模型的推理过程很有特点\",{\"1\":{\"660\":1}}],[\"生成式损失\",{\"1\":{\"276\":1}}],[\"生成式优势\",{\"1\":{\"269\":1}}],[\"生成式方法\",{\"1\":{\"269\":1,\"271\":1}}],[\"生成式预训练语言模型\",{\"1\":{\"823\":1}}],[\"生成式预训练\",{\"0\":{\"247\":1},\"1\":{\"268\":1}}],[\"生成视觉\",{\"1\":{\"265\":1}}],[\"生成掩码位置\",{\"1\":{\"208\":1}}],[\"生成伪标签\",{\"1\":{\"202\":1}}],[\"生成伪标签辅助训练\",{\"1\":{\"195\":1}}],[\"生成伪标签应对噪声问题\",{\"1\":{\"17\":1}}],[\"生成阶段\",{\"0\":{\"188\":1,\"964\":1},\"1\":{\"829\":1}}],[\"生成合成文本\",{\"1\":{\"177\":1}}],[\"生成固定长度的特征向量\",{\"1\":{\"141\":1}}],[\"生成点集的划分\",{\"1\":{\"131\":1}}],[\"生成切平面图像\",{\"1\":{\"110\":1}}],[\"生成规则图像\",{\"1\":{\"110\":1}}],[\"生成融合特征\",{\"1\":{\"97\":1}}],[\"生成动态卷积核\",{\"1\":{\"94\":1}}],[\"生成的图像\",{\"1\":{\"949\":1}}],[\"生成的图像进行重新排序\",{\"1\":{\"889\":1}}],[\"生成的图像依然存在明显的瑕疵\",{\"1\":{\"884\":1}}],[\"生成的最小\",{\"1\":{\"847\":1}}],[\"生成的耗时\",{\"1\":{\"830\":1}}],[\"生成的内容往往缺乏明确的信息来源\",{\"1\":{\"828\":1}}],[\"生成的值在\",{\"1\":{\"479\":1}}],[\"生成的样本数量\",{\"1\":{\"440\":1}}],[\"生成的文本特征相当于分类器的权重\",{\"1\":{\"408\":1}}],[\"生成的文本更具有\",{\"1\":{\"178\":1}}],[\"生成的教师特征来学习\",{\"1\":{\"282\":1}}],[\"生成的分布接近它\",{\"1\":{\"260\":1}}],[\"生成的错误文本更难被\",{\"1\":{\"179\":1}}],[\"生成的多样化问题\",{\"1\":{\"93\":1}}],[\"生成的问题遵循以下三个关键原则\",{\"1\":{\"87\":1}}],[\"生成最终的\",{\"1\":{\"70\":1}}],[\"生成功能区域掩码\",{\"1\":{\"59\":1}}],[\"生成细粒度表示\",{\"1\":{\"36\":1}}],[\"生成\",{\"1\":{\"22\":1,\"106\":1,\"167\":1,\"188\":3,\"264\":2,\"293\":1,\"312\":1,\"313\":1,\"735\":1,\"895\":1,\"924\":2,\"963\":1,\"964\":2}}],[\"汇总\",{\"1\":{\"381\":3}}],[\"汇聚全局信息并反哺\",{\"1\":{\"215\":1}}],[\"└────────┬───────────┘\",{\"1\":{\"402\":2}}],[\"└─\",{\"1\":{\"381\":17}}],[\"├─\",{\"1\":{\"381\":32}}],[\"列或卷积注意力掩码\",{\"1\":{\"887\":1}}],[\"列或其他任意模式访问数据\",{\"1\":{\"544\":1}}],[\"列方向\",{\"1\":{\"700\":2}}],[\"列维度步幅\",{\"1\":{\"546\":1}}],[\"列步长为\",{\"1\":{\"544\":1}}],[\"列时\",{\"1\":{\"542\":1}}],[\"列跳转到第\",{\"1\":{\"542\":1}}],[\"列优先\",{\"1\":{\"542\":2,\"545\":1}}],[\"列优先存储模式的步长\",{\"1\":{\"542\":1}}],[\"列\",{\"1\":{\"542\":5}}],[\"列的值就是\",{\"1\":{\"709\":1}}],[\"列的数据\",{\"1\":{\"542\":1}}],[\"列的所有元素\",{\"1\":{\"541\":2}}],[\"列的\",{\"1\":{\"501\":1}}],[\"列表最后一个记录了缓存的key和value\",{\"1\":{\"420\":2}}],[\"列表\",{\"1\":{\"380\":1,\"444\":1,\"518\":1,\"657\":1,\"697\":1,\"806\":1}}],[\"列举两个该物体常见的其他交互方式\",{\"1\":{\"36\":1}}],[\"隐私问题\",{\"1\":{\"824\":1}}],[\"隐式元学习\",{\"1\":{\"650\":1}}],[\"隐式学习任务逻辑\",{\"1\":{\"640\":1,\"642\":1}}],[\"隐层维度与\",{\"1\":{\"380\":1}}],[\"隐藏的\",{\"1\":{\"877\":1}}],[\"隐藏数据封装细节\",{\"1\":{\"808\":1}}],[\"隐藏未列出的设备\",{\"1\":{\"520\":1}}],[\"隐藏维度2048\",{\"1\":{\"285\":1}}],[\"隐藏层输出\",{\"1\":{\"731\":1}}],[\"隐藏层神经元的数量足够\",{\"1\":{\"500\":1}}],[\"隐藏层维度设为\",{\"1\":{\"667\":1}}],[\"隐藏层维度和参数量不同\",{\"1\":{\"641\":1}}],[\"隐藏层维度\",{\"1\":{\"380\":1}}],[\"隐藏层维度与输入维度的比例\",{\"1\":{\"380\":1}}],[\"隐藏层维度为\",{\"1\":{\"236\":1}}],[\"隐藏层大小为\",{\"1\":{\"224\":1}}],[\"行业大模型\",{\"1\":{\"823\":1}}],[\"行第\",{\"1\":{\"709\":1}}],[\"行维度步幅\",{\"1\":{\"546\":1}}],[\"行步长为\",{\"1\":{\"544\":1}}],[\"行只需要在内存中前进一步\",{\"1\":{\"542\":1}}],[\"行移动到第\",{\"1\":{\"542\":2}}],[\"行优先布局\",{\"1\":{\"545\":1}}],[\"行优先\",{\"1\":{\"542\":1}}],[\"行优先存储模式的步长\",{\"1\":{\"542\":1}}],[\"行的所有元素\",{\"1\":{\"541\":2}}],[\"行\",{\"1\":{\"480\":1,\"501\":1,\"542\":7}}],[\"行列数\",{\"1\":{\"380\":1}}],[\"行为\",{\"1\":{\"293\":1,\"521\":1,\"830\":1}}],[\"肯定得更新样本特征\",{\"1\":{\"357\":1}}],[\"了新知识\",{\"1\":{\"649\":1}}],[\"了距离判断\",{\"1\":{\"578\":1}}],[\"了它定义时的\",{\"1\":{\"448\":1}}],[\"了\",{\"1\":{\"356\":1,\"947\":1}}],[\"了解更详细的结构布局\",{\"1\":{\"222\":1}}],[\"伪代码里的\",{\"1\":{\"355\":1}}],[\"伪标签前五名\",{\"1\":{\"202\":1}}],[\"抽蓝球为\",{\"1\":{\"860\":1}}],[\"抽取式\",{\"1\":{\"735\":1}}],[\"抽取式问答\",{\"1\":{\"735\":1}}],[\"抽取等\",{\"1\":{\"656\":1}}],[\"抽取出一套通用的模版流程\",{\"1\":{\"381\":1}}],[\"抽样数量就是字典大小\",{\"1\":{\"355\":1}}],[\"抽象代数\",{\"1\":{\"649\":1}}],[\"抽象\",{\"1\":{\"152\":1,\"154\":1}}],[\"抽象点集或局部特征\",{\"1\":{\"131\":2}}],[\"希望这个\",{\"1\":{\"945\":1}}],[\"希望学习一个通用\",{\"1\":{\"217\":1}}],[\"希腊字母读音为\",{\"1\":{\"355\":1}}],[\"像样\",{\"1\":{\"947\":1}}],[\"像解码器\",{\"1\":{\"945\":1}}],[\"像编码器\",{\"1\":{\"945\":1}}],[\"像moco和simclr有所不同\",{\"1\":{\"406\":1}}],[\"像\",{\"1\":{\"355\":1,\"385\":1,\"490\":1}}],[\"像素级逐步采样\",{\"1\":{\"926\":1}}],[\"像素级别任务如分割\",{\"1\":{\"502\":1}}],[\"像素标签\",{\"1\":{\"926\":1}}],[\"像素进行插值\",{\"1\":{\"505\":1}}],[\"像素的图像块\",{\"1\":{\"435\":1}}],[\"像素的图块\",{\"1\":{\"322\":1}}],[\"像素\",{\"1\":{\"425\":2,\"435\":1,\"894\":1}}],[\"像素恢复任务通常难以训练\",{\"1\":{\"247\":1}}],[\"像素回归的局限性\",{\"1\":{\"228\":1}}],[\"什么都不发生\",{\"1\":{\"847\":1}}],[\"什么样的优化目标是最高效的迁移\",{\"1\":{\"626\":1}}],[\"什么是prompt\",{\"0\":{\"616\":1}}],[\"什么是大模型\",{\"0\":{\"600\":1}}],[\"什么是闭包\",{\"0\":{\"448\":1}}],[\"什么是高阶函数\",{\"0\":{\"447\":1}}],[\"什么是负样本呢\",{\"1\":{\"353\":1}}],[\"什么是负样本\",{\"1\":{\"350\":1}}],[\"什么意思呢\",{\"1\":{\"355\":1}}],[\"看作一个正则化项是很有意思的\",{\"1\":{\"951\":1}}],[\"看哪一个输出的值最大\",{\"1\":{\"694\":1}}],[\"看一下数据的形式\",{\"1\":{\"382\":1}}],[\"看看它内部包含哪些重要组件\",{\"1\":{\"380\":1}}],[\"看起来像自编码器\",{\"1\":{\"945\":1}}],[\"看起来像传统的自编码器\",{\"1\":{\"944\":1}}],[\"看起来很像原图\",{\"1\":{\"944\":1}}],[\"看起来不像\",{\"1\":{\"944\":1}}],[\"看起\",{\"1\":{\"355\":1}}],[\"看不出区别\",{\"1\":{\"355\":1}}],[\"看不清细节\",{\"1\":{\"157\":1}}],[\"看不懂下面两行代码的话\",{\"1\":{\"137\":1}}],[\"趋近贪心采样\",{\"1\":{\"897\":1}}],[\"趋近\",{\"1\":{\"897\":2}}],[\"趋近于零\",{\"1\":{\"351\":1}}],[\"趋近于\",{\"1\":{\"351\":1}}],[\"趋近于均匀\",{\"1\":{\"260\":1}}],[\"趋于较大值时\",{\"1\":{\"867\":1}}],[\"趋于平滑\",{\"1\":{\"355\":1}}],[\"越具体\",{\"1\":{\"618\":1}}],[\"越明确\",{\"1\":{\"618\":1}}],[\"越长的prompt\",{\"1\":{\"601\":1}}],[\"越接近\",{\"1\":{\"506\":3}}],[\"越接近黄色的位置代表越靠近位置编码的中心位置\",{\"1\":{\"428\":1}}],[\"越大时\",{\"1\":{\"874\":1}}],[\"越大\",{\"1\":{\"355\":1,\"590\":2,\"592\":1,\"917\":1}}],[\"越大越好\",{\"1\":{\"586\":1,\"588\":1}}],[\"越大越\",{\"1\":{\"206\":1}}],[\"越尖锐\",{\"1\":{\"355\":1}}],[\"越小\",{\"1\":{\"355\":1,\"588\":1,\"917\":1}}],[\"尖锐程度\",{\"1\":{\"355\":1}}],[\"尖锐度\",{\"1\":{\"257\":1}}],[\"刚才说了\",{\"1\":{\"353\":1}}],[\"刚性运动\",{\"0\":{\"161\":1},\"1\":{\"161\":3}}],[\"走\",{\"1\":{\"385\":1,\"917\":2}}],[\"走一遍对比学习的流程\",{\"1\":{\"356\":1}}],[\"走了捷径\",{\"1\":{\"353\":1}}],[\"走的是e12这个编码器\",{\"1\":{\"353\":1}}],[\"锚点特征\",{\"1\":{\"355\":1}}],[\"锚点\",{\"1\":{\"353\":1,\"355\":1,\"356\":1}}],[\"巧妙构造出来\",{\"1\":{\"349\":1}}],[\"巧妙的view方式\",{\"1\":{\"119\":1}}],[\"写入归一化像素值\",{\"1\":{\"926\":1}}],[\"写入json文件\",{\"1\":{\"696\":1}}],[\"写作风格或特定领域知识\",{\"1\":{\"830\":1}}],[\"写好prompt\",{\"1\":{\"621\":1}}],[\"写故事\",{\"1\":{\"346\":1}}],[\"写一个关于猫的故事\",{\"1\":{\"339\":1}}],[\"唤醒\",{\"1\":{\"346\":1}}],[\"辨析\",{\"0\":{\"346\":1}}],[\"刷新\",{\"1\":{\"343\":1}}],[\"刷新该数据集\",{\"1\":{\"342\":1}}],[\"助手需要生成推理过程\",{\"1\":{\"342\":1}}],[\"助手\",{\"1\":{\"342\":1,\"823\":1}}],[\"你差不多提前设计出了stable\",{\"1\":{\"961\":1}}],[\"你不仅能理解vq\",{\"1\":{\"955\":1}}],[\"你应该已经相信\",{\"1\":{\"948\":1}}],[\"你测量两个人到中间点的距离\",{\"1\":{\"915\":1}}],[\"你先测量\",{\"1\":{\"915\":1}}],[\"你会看到这个信息的可能性\",{\"1\":{\"877\":1}}],[\"你会得到\",{\"1\":{\"574\":1}}],[\"你在看到\",{\"1\":{\"877\":1}}],[\"你在没有观察任何信息前对\",{\"1\":{\"877\":1}}],[\"你在图片分类的时候很多照片是同一个类别\",{\"1\":{\"350\":1}}],[\"你观测到的信息\",{\"1\":{\"877\":1}}],[\"你总得允许\",{\"1\":{\"847\":1}}],[\"你有一个随机变量\",{\"1\":{\"846\":1}}],[\"你细品\",{\"1\":{\"784\":1}}],[\"你品\",{\"1\":{\"784\":1}}],[\"你将得到一个包含多个元素的\",{\"1\":{\"733\":1}}],[\"你关心离你近的人\",{\"1\":{\"709\":1}}],[\"你关心和你内容类似的人\",{\"1\":{\"709\":1}}],[\"你\",{\"1\":{\"709\":1}}],[\"你可能以为既然是围绕全灰图采样\",{\"1\":{\"875\":1}}],[\"你可能会觉得这里面有个问题\",{\"1\":{\"694\":1}}],[\"你可以为一个运算随意设计求梯度的方法\",{\"1\":{\"959\":1}}],[\"你可以在训练完成后加入如下代码来生成图像\",{\"1\":{\"935\":1}}],[\"你可以在运行程序前用命令行设置\",{\"1\":{\"520\":1}}],[\"你可以把它理解成\",{\"1\":{\"847\":1}}],[\"你可以把它们理解成\",{\"1\":{\"847\":1}}],[\"你可以把马氏距离想成是\",{\"1\":{\"578\":1}}],[\"你可以使用以下命令查看你所有的\",{\"1\":{\"553\":1}}],[\"你就可以把它们两两之间的协方差\",{\"1\":{\"574\":1}}],[\"你的环境名\",{\"1\":{\"557\":1}}],[\"你的终端提示符通常会显示当前环境的名字\",{\"1\":{\"551\":1}}],[\"你需要为每个选项分别构造一个完整的\",{\"1\":{\"737\":1}}],[\"你需要跳过\",{\"1\":{\"542\":1}}],[\"你需要在内存中跳过\",{\"1\":{\"542\":1}}],[\"你几乎可以把它当成\",{\"1\":{\"510\":1}}],[\"你要在\",{\"1\":{\"502\":1}}],[\"你希望衡量模型在真实匹配样本上的性能\",{\"1\":{\"382\":1}}],[\"你希望模型看到\",{\"1\":{\"382\":1}}],[\"你是一个视觉助手\",{\"1\":{\"342\":1}}],[\"你知道吗\",{\"1\":{\"156\":1}}],[\"期望平方距离为\",{\"1\":{\"874\":1}}],[\"期望模型能够学习到文本和图像之间的匹配关系\",{\"1\":{\"406\":1}}],[\"期望的回答\",{\"1\":{\"341\":1,\"342\":1}}],[\"期望输出\",{\"1\":{\"339\":1}}],[\"条件生成图像\",{\"0\":{\"939\":1}}],[\"条件化\",{\"1\":{\"936\":1}}],[\"条件变分自编码器\",{\"0\":{\"952\":1},\"1\":{\"928\":1,\"936\":1,\"952\":1}}],[\"条件输入\",{\"1\":{\"895\":1}}],[\"条件对输出的额外影响\",{\"1\":{\"894\":1}}],[\"条件概率衡量的是\",{\"1\":{\"849\":1}}],[\"条件概率\",{\"0\":{\"849\":1},\"1\":{\"629\":1}}],[\"条件性复制\",{\"1\":{\"494\":1}}],[\"条\",{\"1\":{\"342\":3}}],[\"条图文对\",{\"1\":{\"342\":1}}],[\"条高质量图文对\",{\"1\":{\"341\":1}}],[\"条描述\",{\"1\":{\"341\":1}}],[\"名标注者\",{\"1\":{\"656\":1}}],[\"名词等\",{\"1\":{\"641\":1}}],[\"名词\",{\"1\":{\"574\":1}}],[\"名词短语过滤\",{\"1\":{\"341\":1}}],[\"名称的长版本\",{\"1\":{\"569\":1}}],[\"名称\",{\"1\":{\"106\":1,\"342\":1}}],[\"请把这部分原文告诉我\",{\"1\":{\"735\":1}}],[\"请看下面这张图\",{\"1\":{\"706\":1}}],[\"请保持尊重\",{\"1\":{\"657\":1}}],[\"请尊重\",{\"1\":{\"655\":1}}],[\"请取消下面这行注释\",{\"1\":{\"590\":1}}],[\"请考虑下图中的点\",{\"1\":{\"572\":1}}],[\"请使用此方法\",{\"1\":{\"566\":1}}],[\"请仅与其他指标搭配使用\",{\"1\":{\"566\":1}}],[\"请阅读源码进行学习\",{\"1\":{\"396\":1}}],[\"请描述这张图片\",{\"1\":{\"341\":1}}],[\"请求\",{\"1\":{\"339\":1}}],[\"请注意\",{\"1\":{\"232\":1,\"561\":1,\"660\":1,\"950\":1}}],[\"响应速度更快\",{\"1\":{\"823\":1}}],[\"响应速度比\",{\"1\":{\"823\":1}}],[\"响应\",{\"1\":{\"346\":2}}],[\"响应数据对模型进行微调\",{\"1\":{\"339\":1}}],[\"响应数据集\",{\"1\":{\"339\":1}}],[\"响应对\",{\"1\":{\"339\":1}}],[\"响应格式与\",{\"1\":{\"334\":1}}],[\"聊天版语言模型\",{\"1\":{\"334\":1}}],[\"聊天版本\",{\"1\":{\"329\":1}}],[\"阿里云开源了\",{\"1\":{\"823\":1}}],[\"阿里\",{\"1\":{\"325\":1}}],[\"商业模型在规模和性能上领先\",{\"1\":{\"327\":1}}],[\"商业模型在多模态领域占据领先地位\",{\"1\":{\"325\":1}}],[\"商业模型通过多语言数据集训练\",{\"1\":{\"323\":1}}],[\"商业模型通常具有超过1000亿参数\",{\"1\":{\"323\":1}}],[\"商业模型支持动态分辨率以保留原始宽高比\",{\"1\":{\"323\":1}}],[\"系数范围是\",{\"1\":{\"586\":1}}],[\"系数衡量的是预测掩码与真实标签之间的相似性\",{\"1\":{\"586\":1}}],[\"系数\",{\"1\":{\"586\":2,\"590\":2}}],[\"系统2是慢思考系统\",{\"1\":{\"619\":1}}],[\"系统2\",{\"1\":{\"619\":1}}],[\"系统1是快思考系统\",{\"1\":{\"619\":1}}],[\"系统1\",{\"1\":{\"619\":1}}],[\"系统提示语\",{\"1\":{\"342\":1}}],[\"系统的进步\",{\"1\":{\"296\":1}}],[\"系列均采用\",{\"1\":{\"823\":1}}],[\"系列工作\",{\"1\":{\"823\":1}}],[\"系列基础模型\",{\"1\":{\"823\":1}}],[\"系列基本上是后续大模型的标杆\",{\"1\":{\"823\":1}}],[\"系列在开源社区的影响力和应用前景\",{\"1\":{\"823\":1}}],[\"系列相同\",{\"1\":{\"823\":1}}],[\"系列的三个版本\",{\"1\":{\"823\":1}}],[\"系列模型是\",{\"1\":{\"823\":1}}],[\"系列模型是由\",{\"1\":{\"823\":1}}],[\"系列模型\",{\"1\":{\"823\":2}}],[\"系列语言大模型由\",{\"1\":{\"823\":1}}],[\"系列已形成\",{\"1\":{\"823\":1}}],[\"系列\",{\"1\":{\"325\":1,\"326\":1,\"510\":1,\"650\":1,\"822\":1,\"823\":3}}],[\"系列等\",{\"1\":{\"322\":1}}],[\"项引入的误差分析\",{\"0\":{\"949\":1}}],[\"项前加权得到\",{\"1\":{\"885\":1}}],[\"项自然语言理解任务\",{\"1\":{\"680\":1}}],[\"项对训练稳定性影响较大\",{\"1\":{\"680\":1}}],[\"项目模块化结构\",{\"0\":{\"810\":1}}],[\"项目\",{\"1\":{\"402\":1,\"658\":1}}],[\"项目的数据组织中\",{\"1\":{\"53\":1}}],[\"项\",{\"1\":{\"355\":1,\"500\":1}}],[\"项达到领先水平\",{\"1\":{\"322\":1}}],[\"精通数学的人\",{\"1\":{\"925\":1}}],[\"精细的指令遵循\",{\"1\":{\"823\":1}}],[\"精准度高\",{\"1\":{\"823\":1}}],[\"精确表示\",{\"1\":{\"710\":1}}],[\"精确匹配准确率4\",{\"1\":{\"641\":1}}],[\"精确率与召回率曲线\",{\"0\":{\"571\":1},\"1\":{\"571\":1}}],[\"精确率和召回率通常呈现反向关系\",{\"1\":{\"565\":1}}],[\"精确率会提高\",{\"1\":{\"565\":1}}],[\"精确率作为指标的意义和实用性较低\",{\"1\":{\"565\":1}}],[\"精确率衡量的是被归类为垃圾邮件且实际上是垃圾邮件的电子邮件所占的比例\",{\"1\":{\"565\":1}}],[\"精确率是指模型所有正类别分类中实际为正类别的分类所占的比例\",{\"1\":{\"565\":1}}],[\"精确率\",{\"0\":{\"565\":1},\"1\":{\"566\":1,\"571\":1}}],[\"精度越高意味着分布越\",{\"1\":{\"865\":1}}],[\"精度丢失\",{\"1\":{\"772\":1}}],[\"精度高于最近邻\",{\"1\":{\"505\":1}}],[\"精度受限\",{\"1\":{\"159\":1}}],[\"精心构建了一个涵盖常见场景和文档图像的双语数据集\",{\"1\":{\"322\":1}}],[\"轮的部分计算\",{\"1\":{\"660\":1}}],[\"轮推理时必然包含了第\",{\"1\":{\"660\":1}}],[\"轮输入数据新增了一个\",{\"1\":{\"660\":1}}],[\"轮输入数据只比第\",{\"1\":{\"660\":1}}],[\"轮训练就足够了\",{\"1\":{\"633\":1}}],[\"轮热身\",{\"1\":{\"319\":1}}],[\"轮\",{\"1\":{\"317\":1,\"318\":1,\"319\":1,\"633\":1}}],[\"率\",{\"1\":{\"315\":1,\"318\":1}}],[\"附近\",{\"1\":{\"865\":1,\"874\":2}}],[\"附录d\",{\"1\":{\"669\":1}}],[\"附录\",{\"0\":{\"597\":1},\"1\":{\"886\":1,\"949\":1}}],[\"附录内容\",{\"0\":{\"314\":1}}],[\"附加输入通道数\",{\"1\":{\"83\":1}}],[\"兼容性与支持\",{\"1\":{\"833\":1}}],[\"兼容性\",{\"1\":{\"640\":1}}],[\"兼容大多数分类模型\",{\"1\":{\"589\":1}}],[\"兼具理解和生成能力\",{\"1\":{\"310\":1}}],[\"兼顾了对\",{\"1\":{\"833\":1}}],[\"兼顾了效率与泛化能力\",{\"1\":{\"195\":1}}],[\"兼顾像素级精度和区域重叠度\",{\"1\":{\"592\":1}}],[\"兼顾这两者\",{\"1\":{\"566\":1}}],[\"兼顾对比和生成两类目标\",{\"1\":{\"272\":1}}],[\"兼顾理解与生成能力\",{\"1\":{\"165\":1}}],[\"占比60\",{\"1\":{\"647\":1}}],[\"占据了特别大的内存资源和计算资源\",{\"1\":{\"609\":1}}],[\"占\",{\"1\":{\"306\":1}}],[\"初步版本包含约800万篇文档\",{\"1\":{\"640\":1}}],[\"初步对齐视觉编码器\",{\"1\":{\"305\":1}}],[\"初期在\",{\"1\":{\"315\":1}}],[\"初始值复制自卷积核的权重\",{\"1\":{\"926\":1}}],[\"初始值为\",{\"1\":{\"896\":1}}],[\"初始输入\",{\"1\":{\"660\":1}}],[\"初始步幅\",{\"1\":{\"546\":1}}],[\"初始时没有任何语义信息\",{\"1\":{\"427\":1}}],[\"初始时随机选择一个点作为第一个中心点\",{\"1\":{\"137\":1}}],[\"初始文本token\",{\"1\":{\"421\":1}}],[\"初始为\",{\"1\":{\"421\":1}}],[\"初始阶段在较低分辨率\",{\"1\":{\"305\":1}}],[\"初始假设变换为恒等变换\",{\"1\":{\"152\":1}}],[\"初始设为一个极大值\",{\"1\":{\"137\":1}}],[\"初始全为false\",{\"1\":{\"121\":1}}],[\"初始化采样张量\",{\"1\":{\"926\":1}}],[\"初始化生成器和判别器网络参数\",{\"1\":{\"918\":1}}],[\"初始化输出\",{\"1\":{\"895\":1}}],[\"初始化输出张量\",{\"1\":{\"119\":1}}],[\"初始化运算符重载\",{\"1\":{\"810\":1}}],[\"初始化桶编号\",{\"1\":{\"710\":1}}],[\"初始化一个简化版的\",{\"1\":{\"699\":1}}],[\"初始化一个全零的遮挡掩码\",{\"1\":{\"263\":1}}],[\"初始化优化\",{\"1\":{\"640\":1}}],[\"初始化优化器\",{\"1\":{\"187\":1}}],[\"初始化为\",{\"1\":{\"612\":1,\"805\":1}}],[\"初始化两个低秩矩阵\",{\"1\":{\"609\":1}}],[\"初始化低秩矩阵\",{\"1\":{\"609\":1}}],[\"初始化等\",{\"1\":{\"521\":1}}],[\"初始化自定义数据集类\",{\"1\":{\"424\":1}}],[\"初始化query\",{\"1\":{\"417\":1}}],[\"初始化各\",{\"1\":{\"386\":1}}],[\"初始化阶段\",{\"1\":{\"381\":1}}],[\"初始化所有权重\",{\"1\":{\"380\":1}}],[\"初始化视觉与文本\",{\"1\":{\"380\":1}}],[\"初始化视觉编码器\",{\"1\":{\"187\":1,\"190\":1,\"205\":1}}],[\"初始化值\",{\"1\":{\"380\":2}}],[\"初始化的语言中间件\",{\"1\":{\"313\":1}}],[\"初始化教师输出中心向量\",{\"1\":{\"293\":1}}],[\"初始化教师网络\",{\"1\":{\"293\":1}}],[\"初始化算法稳定\",{\"1\":{\"224\":1}}],[\"初始化码本\",{\"1\":{\"213\":1}}],[\"初始化新的簇中心\",{\"1\":{\"213\":1}}],[\"初始化簇中心\",{\"1\":{\"213\":1}}],[\"初始化过程大体含有两个阶段\",{\"1\":{\"213\":1}}],[\"初始化权重\",{\"1\":{\"213\":2}}],[\"初始化队列特征为单位向量\",{\"1\":{\"205\":1}}],[\"初始化队列为单位向量\",{\"1\":{\"192\":1}}],[\"初始化负样本队列\",{\"1\":{\"205\":1}}],[\"初始化动量编码器参数\",{\"1\":{\"205\":1}}],[\"初始化时直接复制参数\",{\"1\":{\"192\":1}}],[\"初始化方法\",{\"1\":{\"192\":1}}],[\"初始化对比学习的负样本队列\",{\"1\":{\"190\":1}}],[\"初始化文本输入\",{\"1\":{\"421\":1}}],[\"初始化文本编码器和分词器\",{\"1\":{\"190\":1}}],[\"初始化文本解码器\",{\"1\":{\"187\":1}}],[\"初始化文本分词器\",{\"1\":{\"187\":1}}],[\"初始化插值后的特征\",{\"1\":{\"122\":1}}],[\"初始化上采样层\",{\"1\":{\"122\":1}}],[\"初始化距离数组\",{\"1\":{\"121\":1}}],[\"初始化下采样层\",{\"1\":{\"121\":1}}],[\"初始化函数\",{\"1\":{\"120\":1,\"145\":1,\"586\":1,\"587\":1,\"588\":1,\"589\":1,\"590\":1,\"592\":1}}],[\"初始化pointrefer模型\",{\"1\":{\"107\":1}}],[\"初始化\",{\"0\":{\"255\":1},\"1\":{\"106\":1,\"121\":1,\"146\":1,\"187\":1,\"205\":1,\"213\":3,\"224\":1,\"264\":1,\"265\":1,\"293\":1,\"385\":2,\"426\":1,\"427\":1,\"612\":5,\"663\":1,\"964\":1}}],[\"初始化损失函数\",{\"1\":{\"104\":1}}],[\"初始化模型和优化器\",{\"1\":{\"187\":1}}],[\"初始化模型\",{\"1\":{\"104\":1,\"963\":1}}],[\"初始化用于存储每个样本拼接后输入和\",{\"1\":{\"67\":1}}],[\"瑞士军刀\",{\"1\":{\"304\":1}}],[\"填补上vq\",{\"1\":{\"956\":1}}],[\"填补了这一领域的空白\",{\"1\":{\"301\":1}}],[\"填充\",{\"1\":{\"887\":1}}],[\"填充过程图\",{\"1\":{\"713\":1}}],[\"填充token对应0\",{\"1\":{\"713\":1}}],[\"填充符\",{\"1\":{\"421\":1}}],[\"填充至最长序列长度\",{\"1\":{\"64\":1}}],[\"远多于文本\",{\"1\":{\"893\":1}}],[\"远距离可能映射到不同桶\",{\"1\":{\"710\":1}}],[\"远距离用粗桶\",{\"1\":{\"710\":1}}],[\"远优于此前结果\",{\"1\":{\"648\":1}}],[\"远低于监督系统30\",{\"1\":{\"641\":1}}],[\"远低于解耦时的\",{\"1\":{\"179\":1}}],[\"远小于原始矩阵的维度\",{\"1\":{\"609\":1}}],[\"远\",{\"1\":{\"578\":1}}],[\"远落后于llms的规模\",{\"1\":{\"298\":1}}],[\"及\",{\"1\":{\"420\":1,\"921\":1}}],[\"及其右侧\",{\"1\":{\"926\":1}}],[\"及其对应的索引\",{\"1\":{\"896\":1}}],[\"及其迭代优化之后\",{\"1\":{\"836\":1}}],[\"及其多个变种\",{\"1\":{\"308\":1}}],[\"及其变体\",{\"1\":{\"298\":1}}],[\"及之后\",{\"1\":{\"204\":1}}],[\"渐进式图像\",{\"1\":{\"296\":1}}],[\"渐变\",{\"1\":{\"107\":1}}],[\"渐变颜色\",{\"1\":{\"107\":1}}],[\"轻松理解\",{\"1\":{\"965\":1}}],[\"轻松\",{\"1\":{\"835\":1}}],[\"轻量推理\",{\"1\":{\"823\":1}}],[\"轻量知识型\",{\"1\":{\"823\":1}}],[\"轻量级胶水层难以捕捉跨模态的复杂交互\",{\"1\":{\"296\":1}}],[\"轻\",{\"1\":{\"415\":7}}],[\"轻便\",{\"1\":{\"159\":1}}],[\"胶水层\",{\"1\":{\"296\":2,\"304\":1}}],[\"书生\",{\"0\":{\"294\":1,\"321\":1}}],[\"减法运算y\",{\"1\":{\"809\":1}}],[\"减法运算\",{\"1\":{\"809\":1}}],[\"减去的\",{\"1\":{\"945\":1}}],[\"减去一个误差项\",{\"1\":{\"945\":1}}],[\"减去余数\",{\"1\":{\"582\":1}}],[\"减去中心并进行温度锐化\",{\"1\":{\"293\":1}}],[\"减少亮度级别后\",{\"1\":{\"925\":1}}],[\"减少信息偏差\",{\"1\":{\"828\":1}}],[\"减少所需人力投入\",{\"1\":{\"826\":1}}],[\"减少了计算量\",{\"1\":{\"823\":2}}],[\"减少了训练前期的收敛难度\",{\"1\":{\"380\":1}}],[\"减少噪声和偏见\",{\"1\":{\"823\":1}}],[\"减少内存占用\",{\"1\":{\"807\":1}}],[\"减少gc触发频率\",{\"1\":{\"806\":1}}],[\"减少未登录词\",{\"1\":{\"683\":1}}],[\"减少未知词影响\",{\"1\":{\"678\":1}}],[\"减少存储开销并提升泛化能力\",{\"1\":{\"683\":1}}],[\"减少存储开销\",{\"1\":{\"681\":1}}],[\"减少重计算\",{\"1\":{\"667\":1}}],[\"减少幻觉与毒性\",{\"1\":{\"657\":1}}],[\"减少幻觉\",{\"1\":{\"657\":1}}],[\"减少有害内容生成等方面也有所改进\",{\"1\":{\"653\":1}}],[\"减少词汇碎片化\",{\"1\":{\"640\":1}}],[\"减少损失贡献\",{\"1\":{\"589\":1}}],[\"减少计数\",{\"1\":{\"516\":1}}],[\"减少计算量\",{\"1\":{\"119\":1}}],[\"减少多模态层\",{\"1\":{\"277\":1}}],[\"减少单模态文本层\",{\"1\":{\"277\":1}}],[\"减少额外\",{\"1\":{\"214\":1}}],[\"放回到\",{\"1\":{\"357\":1}}],[\"放到和输入相同的\",{\"1\":{\"293\":1}}],[\"放大这个差值就能让模型更\",{\"1\":{\"894\":1}}],[\"放大难分类样本的影响\",{\"1\":{\"589\":1}}],[\"放大难分类样本影响\",{\"1\":{\"102\":1}}],[\"放大两倍\",{\"1\":{\"504\":1}}],[\"放大\",{\"1\":{\"102\":1,\"589\":1,\"893\":1}}],[\"累积和\",{\"1\":{\"582\":1}}],[\"累积求和\",{\"1\":{\"293\":1}}],[\"累加后的值\",{\"1\":{\"481\":1}}],[\"累加求和\",{\"1\":{\"481\":1}}],[\"累加当前批次的样本数到总样本数中\",{\"1\":{\"431\":1}}],[\"累加所有学习任务结束后的损失\",{\"1\":{\"383\":1}}],[\"累计新增遮挡数量\",{\"1\":{\"263\":1}}],[\"色调变化\",{\"1\":{\"293\":1}}],[\"亮度变化\",{\"1\":{\"293\":1}}],[\"耗时\",{\"1\":{\"461\":1}}],[\"耗时46小时\",{\"1\":{\"291\":1}}],[\"耗时严重\",{\"1\":{\"73\":1}}],[\"收益和风险\",{\"1\":{\"566\":1}}],[\"收录了数百种视觉模型\",{\"1\":{\"510\":1}}],[\"收敛情况的粗略指标\",{\"1\":{\"566\":1}}],[\"收敛\",{\"1\":{\"427\":1}}],[\"收敛为零并不是一件容易的事\",{\"1\":{\"949\":1}}],[\"收敛为\",{\"1\":{\"290\":1}}],[\"收敛为0\",{\"1\":{\"290\":1}}],[\"收集相邻句对\",{\"1\":{\"698\":1}}],[\"收集相同数量的相邻句对和非相邻句对\",{\"1\":{\"698\":1}}],[\"收集一组模型输出对\",{\"1\":{\"656\":1}}],[\"收集人类对模型多个输出的排序偏好\",{\"1\":{\"654\":1}}],[\"收集人类示范数据\",{\"1\":{\"654\":1}}],[\"收集人类偏好数据\",{\"1\":{\"339\":1}}],[\"收集成\",{\"1\":{\"445\":2}}],[\"收集了大量的人类编写的\",{\"1\":{\"339\":1}}],[\"收集指令\",{\"1\":{\"339\":1}}],[\"收集约\",{\"1\":{\"305\":1}}],[\"收集所有样本的预测结果\",{\"1\":{\"106\":1}}],[\"光照变换\",{\"1\":{\"286\":1}}],[\"余弦函数编码\",{\"1\":{\"707\":1}}],[\"余弦位置编码\",{\"0\":{\"706\":1}}],[\"余弦学习率调度\",{\"1\":{\"667\":1}}],[\"余弦退火\",{\"1\":{\"286\":1}}],[\"余弦相似度损失\",{\"1\":{\"213\":1}}],[\"余弦相似度\",{\"0\":{\"506\":1},\"1\":{\"213\":1,\"506\":1}}],[\"锐化操作则产生相反作用\",{\"1\":{\"290\":1}}],[\"锐化\",{\"1\":{\"285\":1}}],[\"坍塌问题\",{\"1\":{\"285\":1}}],[\"裁剪后再统一缩放到\",{\"1\":{\"293\":1}}],[\"裁剪\",{\"1\":{\"285\":1}}],[\"裁剪等真实场景中的损坏方式\",{\"1\":{\"19\":1}}],[\"协同蒸馏中\",{\"1\":{\"283\":1}}],[\"协同蒸馏\",{\"1\":{\"283\":1}}],[\"协方差为\",{\"1\":{\"574\":1,\"943\":1}}],[\"协方差函数定义样本之间的相似性结构\",{\"1\":{\"574\":1}}],[\"协方差接近\",{\"1\":{\"574\":1}}],[\"协方差是负的\",{\"1\":{\"574\":1}}],[\"协方差是正的\",{\"1\":{\"574\":1}}],[\"协方差是两个变量\",{\"1\":{\"574\":1}}],[\"协方差矩阵形式为\",{\"1\":{\"871\":1}}],[\"协方差矩阵可以用来\",{\"1\":{\"574\":1}}],[\"协方差矩阵的结构\",{\"1\":{\"574\":1}}],[\"协方差矩阵\",{\"0\":{\"574\":1},\"1\":{\"574\":2}}],[\"协方差\",{\"1\":{\"22\":1,\"574\":5}}],[\"白两种颜色来描述图像\",{\"1\":{\"925\":1}}],[\"白名单\",{\"1\":{\"520\":1}}],[\"白化\",{\"1\":{\"282\":1}}],[\"白色背景\",{\"1\":{\"107\":1}}],[\"典型壳层\",{\"1\":{\"875\":1}}],[\"典型集合\",{\"1\":{\"872\":1}}],[\"典型的涌现能力\",{\"1\":{\"825\":1}}],[\"典型的输入是一个包含\",{\"1\":{\"733\":1}}],[\"典型值\",{\"1\":{\"589\":1}}],[\"典型步长\",{\"1\":{\"542\":2}}],[\"典型应用场景举例\",{\"0\":{\"461\":1}}],[\"典型例子是\",{\"1\":{\"282\":1}}],[\"典型代表包括\",{\"1\":{\"246\":1}}],[\"允许看到当前像素\",{\"1\":{\"926\":1}}],[\"允许看自己\",{\"1\":{\"924\":1}}],[\"允许你用基本事件构造更复杂事件\",{\"1\":{\"847\":1}}],[\"允许用户无需编程创建专用的\",{\"1\":{\"823\":1}}],[\"允许开发者创建工具扩展\",{\"1\":{\"823\":1}}],[\"允许我们通过优化适应过程中密集层变化的秩分解矩阵\",{\"1\":{\"610\":1}}],[\"允许重复采样\",{\"1\":{\"518\":1}}],[\"允许跨模态交互\",{\"1\":{\"385\":1}}],[\"允许网络自己调节每一层的残差输出强度\",{\"1\":{\"380\":1}}],[\"允许数据增强后的一致性\",{\"1\":{\"282\":1}}],[\"允许在训练时对离散随机变量进行\",{\"1\":{\"257\":1}}],[\"天完成了\",{\"1\":{\"823\":1}}],[\"天注册人数突破\",{\"1\":{\"823\":1}}],[\"天\",{\"1\":{\"280\":1}}],[\"台\",{\"1\":{\"280\":1}}],[\"达到55\",{\"1\":{\"641\":1}}],[\"达到非常低的\",{\"1\":{\"633\":1}}],[\"达到四两拨千斤的效果\",{\"1\":{\"614\":1}}],[\"达到最先进性能\",{\"1\":{\"368\":1}}],[\"达到了\",{\"1\":{\"342\":1}}],[\"达到\",{\"1\":{\"280\":1,\"308\":1,\"310\":1,\"656\":1}}],[\"达成最新最优性能\",{\"1\":{\"47\":1}}],[\"居中操作抑制单维主导\",{\"1\":{\"290\":1}}],[\"居中操作等价于给教师输出加一个偏置项\",{\"1\":{\"285\":1}}],[\"居中与锐化效果互补\",{\"1\":{\"285\":1}}],[\"居中\",{\"1\":{\"280\":1,\"285\":1,\"290\":1}}],[\"准确来说\",{\"1\":{\"801\":1,\"921\":1}}],[\"准确性是指所有分类\",{\"1\":{\"562\":1}}],[\"准确率83\",{\"1\":{\"685\":1}}],[\"准确率低至72\",{\"1\":{\"670\":1}}],[\"准确率刷新sota\",{\"1\":{\"668\":1}}],[\"准确率达63\",{\"1\":{\"641\":1}}],[\"准确率达到\",{\"1\":{\"342\":1}}],[\"准确率进一步提升至63\",{\"1\":{\"641\":1}}],[\"准确率从19\",{\"1\":{\"641\":1}}],[\"准确率可以作为衡量模型质量的粗略指标\",{\"1\":{\"562\":1}}],[\"准确率衡量的是所有电子邮件正确分类所占的比例\",{\"1\":{\"562\":1}}],[\"准确率变化\",{\"1\":{\"344\":1}}],[\"准确率提升不大\",{\"1\":{\"343\":1}}],[\"准确率\",{\"0\":{\"562\":1},\"1\":{\"280\":1,\"291\":2,\"292\":1,\"342\":1,\"385\":1,\"566\":1,\"571\":1,\"593\":1,\"634\":1,\"641\":1}}],[\"准备调试\",{\"1\":{\"712\":1}}],[\"准备就绪\",{\"1\":{\"712\":1}}],[\"准备与下游任务相关的数据集\",{\"1\":{\"609\":1}}],[\"准备生成参数\",{\"1\":{\"421\":1}}],[\"准备阶段主要完成数据集加载\",{\"1\":{\"104\":1}}],[\"准备\",{\"0\":{\"104\":1},\"1\":{\"103\":1,\"266\":2,\"420\":1,\"698\":1}}],[\"字或词\",{\"1\":{\"691\":1}}],[\"字母重排\",{\"1\":{\"648\":1}}],[\"字节级bpe\",{\"1\":{\"683\":1}}],[\"字节级\",{\"1\":{\"640\":1}}],[\"字节对编码\",{\"1\":{\"594\":1}}],[\"字节\",{\"1\":{\"546\":1}}],[\"字符串\",{\"1\":{\"474\":1,\"640\":1}}],[\"字段\",{\"1\":{\"385\":1}}],[\"字幕生成\",{\"1\":{\"274\":1}}],[\"字典的构建过程太过粗糙\",{\"1\":{\"697\":1}}],[\"字典的大\",{\"1\":{\"357\":1}}],[\"字典的大小就很重要\",{\"1\":{\"355\":1}}],[\"字典初始化\",{\"1\":{\"697\":1}}],[\"字典中包含文本\",{\"1\":{\"384\":1}}],[\"字典中保存了\",{\"1\":{\"382\":1}}],[\"字典大小\",{\"1\":{\"361\":1}}],[\"字典大小和字典特征一致性经常不能同时满足\",{\"1\":{\"356\":1}}],[\"字典大小是\",{\"1\":{\"356\":1}}],[\"字典\",{\"1\":{\"207\":1,\"444\":1}}],[\"帧\",{\"1\":{\"273\":1}}],[\"底层内存中的数据并没有被重新排列\",{\"1\":{\"545\":1}}],[\"底层数据存储为\",{\"1\":{\"545\":1}}],[\"底层cuda实现严格依赖连续内存布局进行跨批次统计计算\",{\"1\":{\"493\":1}}],[\"底层使用视觉和语言专家分别编码各自模态\",{\"1\":{\"372\":1}}],[\"底层计算公式变为\",{\"1\":{\"261\":1}}],[\"底部\",{\"1\":{\"272\":1}}],[\"经\",{\"1\":{\"949\":1}}],[\"经济\",{\"1\":{\"823\":1}}],[\"经ccnet流水线去重\",{\"1\":{\"667\":1}}],[\"经历n次迭代\",{\"1\":{\"595\":1}}],[\"经典模型\",{\"1\":{\"510\":1}}],[\"经典的单编码器方法通过在大规模人工标注图像数据集\",{\"1\":{\"271\":1}}],[\"经过了第一个\",{\"1\":{\"923\":1}}],[\"经过一层全连接后的输出\",{\"1\":{\"733\":1}}],[\"经过一通计算\",{\"1\":{\"660\":1}}],[\"经过相同的非线性变换后\",{\"1\":{\"699\":1}}],[\"经过embedding层将输入token序列变为一个三维张量\",{\"1\":{\"660\":1}}],[\"经过encoder之后\",{\"1\":{\"427\":1}}],[\"经过过滤\",{\"1\":{\"647\":1}}],[\"经过sigmoid\",{\"1\":{\"589\":1}}],[\"经过处理后的图像块嵌入张量\",{\"1\":{\"426\":1}}],[\"经过卷积层变成\",{\"1\":{\"426\":1}}],[\"经过预处理的图像和对应的标签\",{\"1\":{\"424\":1}}],[\"经过pooler的特征\",{\"1\":{\"384\":1}}],[\"经过编码器2得到了正样本\",{\"1\":{\"355\":1}}],[\"经过编码器1得到了\",{\"1\":{\"355\":1}}],[\"经过全局池化后得到一个全局特征向量\",{\"1\":{\"152\":1}}],[\"经过几层\",{\"1\":{\"145\":1}}],[\"经过mlp进一步提取和融合特征\",{\"1\":{\"145\":1}}],[\"经过\",{\"1\":{\"67\":1,\"69\":2,\"100\":1,\"119\":1,\"185\":1,\"257\":1,\"265\":1,\"592\":1,\"735\":1,\"900\":1}}],[\"专用代码生成模型\",{\"1\":{\"823\":1}}],[\"专业版\",{\"1\":{\"823\":2}}],[\"专注于\",{\"1\":{\"834\":1}}],[\"专注于第三方集成\",{\"1\":{\"834\":1}}],[\"专注于复杂问题求解和精确推理能力\",{\"1\":{\"823\":1}}],[\"专注于精确推理和复杂问题求解\",{\"1\":{\"823\":1}}],[\"专注于广泛知识覆盖和流畅对话体验\",{\"1\":{\"823\":1}}],[\"专注于从零开始\",{\"1\":{\"269\":1}}],[\"专门学好文本与图像\",{\"1\":{\"885\":1}}],[\"专门学好像素生成\",{\"1\":{\"885\":1}}],[\"专门对话\",{\"1\":{\"823\":1}}],[\"专门用于预测一个\",{\"1\":{\"152\":1}}],[\"零\",{\"0\":{\"689\":1,\"695\":1},\"1\":{\"689\":1,\"695\":1}}],[\"零拷贝\",{\"1\":{\"544\":1,\"545\":1}}],[\"零样本性能\",{\"1\":{\"668\":1}}],[\"零样本学习\",{\"1\":{\"647\":1}}],[\"零样本表现\",{\"1\":{\"635\":1}}],[\"零样本\",{\"1\":{\"273\":1}}],[\"零样本文本\",{\"1\":{\"273\":1}}],[\"零样本图像字幕生成\",{\"1\":{\"309\":1}}],[\"零样本图像\",{\"1\":{\"309\":1}}],[\"零样本图像分类\",{\"1\":{\"273\":1,\"309\":1}}],[\"零样本图文跨模态检索\",{\"1\":{\"273\":1}}],[\"零样本迁移\",{\"1\":{\"273\":1}}],[\"零样本准确率\",{\"1\":{\"268\":1}}],[\"零样本分类能力下降\",{\"1\":{\"277\":1}}],[\"零样本分类\",{\"1\":{\"268\":1}}],[\"零计算开销的\",{\"1\":{\"201\":1}}],[\"摊销\",{\"1\":{\"268\":1}}],[\"几乎不可能采样到接近灰色的图像\",{\"1\":{\"875\":1}}],[\"几乎所有模型都能通过同样的方式调用\",{\"1\":{\"510\":1}}],[\"几乎没有额外开销\",{\"1\":{\"268\":1}}],[\"几十甚至上百层\",{\"1\":{\"380\":1}}],[\"几百万个类别就太难了\",{\"1\":{\"355\":1}}],[\"几何属性\",{\"1\":{\"56\":1}}],[\"几何属性提取+意图类比\",{\"1\":{\"31\":1}}],[\"几何结构映射方法\",{\"1\":{\"73\":1}}],[\"几何结构信息与交互信息的融合\",{\"0\":{\"57\":1}}],[\"几何结构文本信息与点云信息进行融合\",{\"1\":{\"54\":1}}],[\"几何结构知识\",{\"1\":{\"52\":1}}],[\"几何结构推理\",{\"1\":{\"35\":1}}],[\"几何信息无法有效注入点云\",{\"1\":{\"48\":1}}],[\"几何推理\",{\"0\":{\"35\":1}}],[\"几何与意图利用不足\",{\"1\":{\"30\":1}}],[\"拆解成一系列的简单问题\",{\"1\":{\"622\":1}}],[\"拆回文本特征和图像特征\",{\"1\":{\"384\":1}}],[\"拆分输出序列\",{\"1\":{\"895\":1}}],[\"拆分作为共享矩阵\",{\"1\":{\"892\":1}}],[\"拆分\",{\"1\":{\"380\":1}}],[\"拆分为\",{\"1\":{\"266\":1}}],[\"拆成\",{\"1\":{\"83\":1}}],[\"终端变量导数保留\",{\"1\":{\"807\":1}}],[\"终止训练\",{\"1\":{\"265\":1}}],[\"终输出的\",{\"1\":{\"137\":1}}],[\"出发\",{\"1\":{\"847\":2}}],[\"出来\",{\"1\":{\"735\":1}}],[\"出被掩码位置的表示\",{\"1\":{\"699\":1}}],[\"出于方便\",{\"1\":{\"697\":1}}],[\"出现了基于神经网络的生成模型框架\",{\"1\":{\"942\":1}}],[\"出现了一些基于自监督的方法\",{\"1\":{\"413\":1}}],[\"出现了一系列扩展方向\",{\"1\":{\"282\":1}}],[\"出现在输出\",{\"1\":{\"709\":1}}],[\"出现的小常数\",{\"1\":{\"592\":1}}],[\"出现这种差异的原因不难理解\",{\"1\":{\"413\":1}}],[\"出现均匀崩溃\",{\"1\":{\"290\":1}}],[\"出现单维崩溃\",{\"1\":{\"290\":1}}],[\"出现\",{\"1\":{\"265\":1,\"592\":1}}],[\"出物体上可能的\",{\"1\":{\"83\":1}}],[\"丢给\",{\"1\":{\"694\":1}}],[\"丢掉\",{\"1\":{\"403\":3}}],[\"丢弃最后不足\",{\"1\":{\"293\":1}}],[\"丢弃它\",{\"1\":{\"265\":1}}],[\"丢失部分几何信息\",{\"1\":{\"159\":1}}],[\"饱和度变化\",{\"1\":{\"293\":1}}],[\"饱和度\",{\"1\":{\"264\":1}}],[\"检测为阳性\",{\"1\":{\"850\":2}}],[\"检测率为\",{\"1\":{\"563\":1}}],[\"检测等\",{\"1\":{\"352\":1}}],[\"检查张量内存是否连续\",{\"1\":{\"494\":1}}],[\"检查\",{\"1\":{\"470\":1}}],[\"检查图像是否为\",{\"1\":{\"424\":1}}],[\"检查当前目录是否有预训练权重文件\",{\"1\":{\"410\":1,\"412\":1}}],[\"检查输入图像尺寸是否与初始化尺寸匹配\",{\"1\":{\"380\":1}}],[\"检查遮挡块尺寸是否小于输入图像patch尺寸\",{\"1\":{\"263\":1}}],[\"检索机制和索引管道等\",{\"1\":{\"833\":1}}],[\"检索能力\",{\"1\":{\"833\":1}}],[\"检索和使用外部数据可能引发伦理和隐私方面的问题\",{\"1\":{\"830\":1}}],[\"检索和分类任务的推理速度更快\",{\"1\":{\"369\":1}}],[\"检索阶段\",{\"1\":{\"829\":1}}],[\"检索增强生成\",{\"0\":{\"828\":1},\"1\":{\"828\":1}}],[\"检索任务需对所有图文对联合编码\",{\"1\":{\"368\":1}}],[\"检索任务高效\",{\"1\":{\"368\":1}}],[\"检索\",{\"1\":{\"296\":1,\"303\":1,\"829\":1}}],[\"检索等任务\",{\"1\":{\"157\":1}}],[\"向容器对象\",{\"1\":{\"806\":1}}],[\"向函数传递参数\",{\"1\":{\"806\":1}}],[\"向下取整\",{\"1\":{\"502\":1}}],[\"向右平移一位\",{\"1\":{\"274\":1}}],[\"向最近整数取整\",{\"1\":{\"263\":1}}],[\"向量靠近\",{\"1\":{\"963\":1}}],[\"向量进行归一化\",{\"1\":{\"900\":1}}],[\"向量或\",{\"1\":{\"899\":1}}],[\"向量化构建出个性化数据库\",{\"1\":{\"836\":1}}],[\"向量化注意力\",{\"1\":{\"119\":1}}],[\"向量数据库\",{\"1\":{\"832\":1}}],[\"向量映射成两个分数\",{\"1\":{\"733\":1}}],[\"向量维度\",{\"1\":{\"709\":1}}],[\"向量维度越高\",{\"1\":{\"706\":1}}],[\"向量长度\",{\"1\":{\"709\":1}}],[\"向量长度为\",{\"1\":{\"213\":1}}],[\"向量就越接近\",{\"1\":{\"706\":1}}],[\"向量中\",{\"1\":{\"694\":1}}],[\"向量合并成一个完整的\",{\"1\":{\"364\":1}}],[\"向量的视角\",{\"1\":{\"709\":1}}],[\"向量的点积\",{\"1\":{\"506\":1}}],[\"向量的长度\",{\"1\":{\"432\":1}}],[\"向量的相似度矩阵\",{\"1\":{\"274\":1}}],[\"向量的维度越高\",{\"1\":{\"706\":1}}],[\"向量的维度\",{\"1\":{\"213\":1}}],[\"向量加入回来\",{\"1\":{\"258\":1}}],[\"向量从计算图中分离出来\",{\"1\":{\"258\":1}}],[\"向量乘以\",{\"1\":{\"257\":1}}],[\"向量与最后一层\",{\"1\":{\"215\":1}}],[\"向量拼接形成\",{\"1\":{\"214\":1}}],[\"向量记为\",{\"1\":{\"214\":1}}],[\"向量量化损失\",{\"1\":{\"963\":1}}],[\"向量量化器\",{\"1\":{\"963\":1}}],[\"向量量化器还完成了对码本相关状态参数的更新\",{\"1\":{\"213\":1}}],[\"向量量化器的前向传播过程负责将\",{\"1\":{\"213\":1}}],[\"向量量化器负责将连续的视觉特征映射到离散的视觉\",{\"1\":{\"213\":1}}],[\"向量量化的\",{\"1\":{\"213\":1}}],[\"向量量化训练中常见问题是代码本塌陷\",{\"1\":{\"212\":1}}],[\"向量量化知识蒸馏算法用于d\",{\"0\":{\"212\":1}}],[\"向量量化知识蒸馏\",{\"1\":{\"210\":1,\"217\":1}}],[\"向量\",{\"1\":{\"199\":1,\"201\":1,\"213\":2,\"255\":1,\"256\":1,\"257\":1,\"258\":2,\"265\":1,\"266\":1,\"306\":1,\"364\":1,\"385\":1,\"506\":2,\"507\":1,\"709\":1,\"857\":1,\"910\":1,\"935\":1,\"963\":2}}],[\"向量自注意力层\",{\"0\":{\"119\":1}}],[\"向量注意力的权重编码依赖\",{\"1\":{\"125\":1}}],[\"向量注意力能够更好地捕获3d空间中的几何关系\",{\"1\":{\"119\":1}}],[\"向量注意力由于支持逐通道自适应调制\",{\"1\":{\"117\":1}}],[\"向量注意力\",{\"1\":{\"112\":1,\"117\":1,\"125\":2}}],[\"跳坑\",{\"1\":{\"919\":1}}],[\"跳过训练过程\",{\"1\":{\"963\":1,\"964\":1}}],[\"跳过这对\",{\"1\":{\"697\":1}}],[\"跳过一行\",{\"1\":{\"544\":1}}],[\"跳出尝试循环\",{\"1\":{\"263\":1}}],[\"跳出循环\",{\"1\":{\"263\":1}}],[\"跳跃连接层\",{\"1\":{\"122\":1}}],[\"跳跃连接特征融合\",{\"1\":{\"122\":1}}],[\"跳跃连接来自编码器的点坐标\",{\"1\":{\"122\":1}}],[\"跳跃连接\",{\"1\":{\"122\":2}}],[\"跳跃连接融合\",{\"1\":{\"122\":1}}],[\"尝试只采样那些很可能生成\",{\"1\":{\"945\":1}}],[\"尝试加载已保存的字典\",{\"1\":{\"697\":1}}],[\"尝试正确完成任务\",{\"1\":{\"657\":1}}],[\"尝试确保被标记为垃圾邮件的电子邮件实际上是垃圾邮件\",{\"1\":{\"566\":1}}],[\"尝试使用conda\",{\"1\":{\"557\":1}}],[\"尝试生成一个遮挡块\",{\"1\":{\"263\":1}}],[\"尝试重建教师模型\",{\"1\":{\"212\":1}}],[\"遮住当前像素及其右方\",{\"1\":{\"926\":1}}],[\"遮盖语言建模\",{\"1\":{\"376\":1}}],[\"遮一片连续矩形区域\",{\"1\":{\"263\":1}}],[\"遮挡掩码\",{\"1\":{\"264\":2,\"265\":1}}],[\"遮挡块的最小宽高比\",{\"1\":{\"263\":1}}],[\"遮挡区域零碎\",{\"1\":{\"263\":1}}],[\"遮挡方式\",{\"1\":{\"263\":2}}],[\"遮挡位置记作集合\",{\"1\":{\"234\":1}}],[\"遮挡\",{\"1\":{\"12\":1,\"263\":1,\"265\":1}}],[\"尽量接近\",{\"1\":{\"947\":1}}],[\"尽可能相似\",{\"1\":{\"956\":1}}],[\"尽可能逼近标准正态分布\",{\"1\":{\"947\":1}}],[\"尽可能地提升大模型在特定领域的能力\",{\"1\":{\"602\":1}}],[\"尽可能平均地使用每个\",{\"1\":{\"262\":1}}],[\"尽管这个下界仍然包含一个关于\",{\"1\":{\"947\":1}}],[\"尽管这些大型语言模型与小型语言模型\",{\"1\":{\"822\":1}}],[\"尽管这些方法有所改进\",{\"1\":{\"326\":1}}],[\"尽管每个样本的距离是随机的\",{\"1\":{\"874\":1}}],[\"尽管大型语言模型的调用相对简单\",{\"1\":{\"831\":1}}],[\"尽管用户不再访问这些对象\",{\"1\":{\"806\":1}}],[\"尽管有10种变体\",{\"1\":{\"681\":1}}],[\"尽管有时仍用英文回应\",{\"1\":{\"657\":1}}],[\"尽管\",{\"1\":{\"655\":1,\"679\":1,\"824\":1}}],[\"尽管仍有提升空间\",{\"1\":{\"653\":1}}],[\"尽管仍存在局限\",{\"1\":{\"651\":1}}],[\"尽管其非英语性能仍有限\",{\"1\":{\"650\":1}}],[\"尽管gpt\",{\"1\":{\"649\":2}}],[\"尽管训练数据中非英语文本仅占7\",{\"1\":{\"648\":1}}],[\"尽管当前零样本性能仍有限\",{\"1\":{\"643\":1}}],[\"尽管webtext几乎无平行语料\",{\"1\":{\"641\":1}}],[\"尽管数据集小\",{\"1\":{\"641\":1}}],[\"尽管零样本性能尚不完美\",{\"1\":{\"639\":1}}],[\"尽管该模型毫无用处\",{\"1\":{\"562\":1}}],[\"尽管谷歌基于jft\",{\"1\":{\"413\":1}}],[\"尽管clip是一个多模态模型\",{\"1\":{\"407\":1}}],[\"尽管预训练时没有用任何人工标注\",{\"1\":{\"243\":1}}],[\"尽管取得进展\",{\"1\":{\"216\":1}}],[\"尽管使用简单的过滤规则\",{\"1\":{\"167\":1}}],[\"尽管如此\",{\"1\":{\"20\":1,\"300\":1,\"884\":1}}],[\"带入上式中\",{\"1\":{\"945\":1}}],[\"带入公式得到\",{\"1\":{\"260\":1}}],[\"带示例的任务\",{\"1\":{\"656\":1}}],[\"带星号\",{\"1\":{\"553\":1}}],[\"带参数的装饰器\",{\"0\":{\"453\":1}}],[\"带参数的函数装饰器\",{\"0\":{\"452\":1}}],[\"带残差连接\",{\"1\":{\"380\":1}}],[\"带来的提升逐渐减弱\",{\"1\":{\"291\":1}}],[\"带来新的特性\",{\"1\":{\"280\":1}}],[\"带曲线\",{\"1\":{\"52\":1}}],[\"误差无法从解码器传递到编码器上\",{\"1\":{\"959\":1}}],[\"误差中\",{\"1\":{\"959\":1}}],[\"误差函数里应该只有原图像和目标图像的重建误差\",{\"1\":{\"959\":1}}],[\"误差较大时退化为\",{\"1\":{\"259\":1}}],[\"误检\",{\"1\":{\"590\":1}}],[\"误报为阳性概率是\",{\"1\":{\"850\":1}}],[\"误报\",{\"1\":{\"572\":1,\"590\":1}}],[\"误报很可怕\",{\"1\":{\"566\":1}}],[\"误报概率\",{\"0\":{\"564\":1}}],[\"误分类为\",{\"1\":{\"31\":1}}],[\"回\",{\"1\":{\"963\":1}}],[\"回归模型的最佳结果往往是\",{\"1\":{\"952\":1}}],[\"回归损失函数\",{\"1\":{\"259\":1}}],[\"回忆一下\",{\"1\":{\"951\":1,\"956\":1}}],[\"回调\",{\"1\":{\"832\":1}}],[\"回调该钩子方法完成合适的批量数据格式组织\",{\"1\":{\"382\":1}}],[\"回收时机非即时性\",{\"1\":{\"806\":1}}],[\"回顾下之前的多模态网络设计\",{\"1\":{\"415\":1}}],[\"回顾\",{\"1\":{\"386\":1}}],[\"回答问题\",{\"1\":{\"827\":1}}],[\"回答\",{\"1\":{\"339\":1,\"346\":1,\"735\":1}}],[\"回原始输入形状\",{\"1\":{\"213\":1}}],[\"稳定平衡\",{\"1\":{\"894\":1}}],[\"稳定训练\",{\"1\":{\"258\":1,\"380\":1}}],[\"稳定的\",{\"1\":{\"152\":1}}],[\"➡️\",{\"1\":{\"258\":1,\"444\":1}}],[\"连同文本解码器在内总参数量为\",{\"1\":{\"272\":1}}],[\"连续随机变量形式\",{\"0\":{\"853\":1}}],[\"连续随机变量\",{\"0\":{\"847\":1}}],[\"连续包月优惠价为\",{\"1\":{\"823\":1}}],[\"连续句子打包\",{\"1\":{\"681\":1,\"683\":1}}],[\"连续\",{\"1\":{\"633\":1,\"943\":1}}],[\"连续存储的方式\",{\"1\":{\"545\":1}}],[\"连续的内存访问模式对\",{\"1\":{\"492\":1}}],[\"连续内存\",{\"1\":{\"489\":1}}],[\"连续性\",{\"0\":{\"489\":1,\"541\":1},\"1\":{\"540\":1,\"541\":1}}],[\"连续出现次数\",{\"1\":{\"480\":1}}],[\"连续不重复的元素\",{\"1\":{\"480\":1}}],[\"连续分布\",{\"1\":{\"258\":1,\"904\":1}}],[\"连续值\",{\"1\":{\"257\":1}}],[\"连接起来\",{\"1\":{\"421\":1}}],[\"连接图像特征和语言嵌入空间的线性层\",{\"1\":{\"342\":1}}],[\"连接视觉特征和语言嵌入空间\",{\"1\":{\"341\":1}}],[\"连接视觉编码器和llm\",{\"1\":{\"300\":1}}],[\"连接llm解码器\",{\"1\":{\"304\":1}}],[\"连接llm\",{\"1\":{\"303\":1}}],[\"连接\",{\"1\":{\"57\":1,\"305\":1,\"937\":2}}],[\"软匹配\",{\"1\":{\"943\":1}}],[\"软\",{\"1\":{\"257\":1}}],[\"软采样\",{\"1\":{\"257\":2}}],[\"软标签分配\",{\"1\":{\"283\":1}}],[\"软标签\",{\"1\":{\"192\":1}}],[\"软标签计算\",{\"1\":{\"190\":1}}],[\"软标签通常用于边界模糊区域\",{\"1\":{\"88\":1}}],[\"编造\",{\"1\":{\"735\":1}}],[\"编程和逻辑推理等任务中表现卓越\",{\"1\":{\"823\":1}}],[\"编程\",{\"1\":{\"346\":1}}],[\"编号\",{\"1\":{\"255\":1}}],[\"编码字典\",{\"1\":{\"963\":1}}],[\"编码输入图像\",{\"1\":{\"963\":1}}],[\"编码输入文本为\",{\"1\":{\"898\":1}}],[\"编码代价\",{\"1\":{\"950\":1}}],[\"编码结果\",{\"1\":{\"946\":1}}],[\"编码变长\",{\"1\":{\"908\":1}}],[\"编码是最优的\",{\"1\":{\"908\":1}}],[\"编码能力异常强大\",{\"1\":{\"823\":1}}],[\"编码后可能是\",{\"1\":{\"735\":1}}],[\"编码后\",{\"1\":{\"733\":1,\"892\":2}}],[\"编码后得到输出的结果\",{\"1\":{\"419\":1}}],[\"编码在不同维度上具有不同的周期性\",{\"1\":{\"706\":1}}],[\"编码值的波动就越小\",{\"1\":{\"706\":1}}],[\"编码成\",{\"1\":{\"691\":1}}],[\"编码的文本\",{\"1\":{\"885\":1}}],[\"编码的维度设定为\",{\"1\":{\"706\":1}}],[\"编码的冗余扩展\",{\"1\":{\"640\":1}}],[\"编码的输入文本\",{\"1\":{\"192\":1}}],[\"编码阶段\",{\"1\":{\"397\":1}}],[\"编码影响\",{\"1\":{\"353\":1}}],[\"编码文本\",{\"1\":{\"274\":1}}],[\"编码文本输入\",{\"1\":{\"187\":1}}],[\"编码图文对\",{\"1\":{\"251\":1}}],[\"编码图像为\",{\"1\":{\"895\":1}}],[\"编码图像的\",{\"1\":{\"892\":1}}],[\"编码图像并投影到特征空间\",{\"1\":{\"213\":1}}],[\"编码图像\",{\"1\":{\"83\":1,\"274\":1}}],[\"编码为连续隐变量\",{\"1\":{\"963\":1}}],[\"编码为图像特征\",{\"1\":{\"420\":1}}],[\"编码为\",{\"1\":{\"232\":1}}],[\"编码为两个特征\",{\"1\":{\"37\":1}}],[\"编码函数\",{\"1\":{\"213\":1}}],[\"编码和解码\",{\"1\":{\"213\":1}}],[\"编码过程\",{\"1\":{\"144\":1,\"963\":1}}],[\"编码\",{\"1\":{\"143\":1,\"190\":1,\"213\":1,\"256\":1,\"257\":1,\"258\":1,\"268\":1,\"353\":1,\"385\":2,\"678\":1,\"887\":1,\"900\":2,\"951\":1}}],[\"编码得到的图像\",{\"1\":{\"885\":1}}],[\"编码得到的特征图\",{\"1\":{\"213\":1}}],[\"编码得到量化结果\",{\"1\":{\"213\":1}}],[\"编码得到\",{\"1\":{\"96\":1}}],[\"编码点云特征\",{\"1\":{\"94\":1}}],[\"编码推理结果\",{\"1\":{\"32\":1}}],[\"编码器和嵌入向量的学习速度应该不一样快\",{\"1\":{\"960\":1}}],[\"编码器和解码器\",{\"1\":{\"254\":1}}],[\"编码器的目标是让\",{\"1\":{\"947\":1}}],[\"编码器的图像变换\",{\"1\":{\"264\":1}}],[\"编码器最终输出\",{\"1\":{\"899\":1}}],[\"编码器最终输出层\",{\"1\":{\"255\":1}}],[\"编码器末尾追加\",{\"1\":{\"899\":1}}],[\"编码器网络层列表\",{\"1\":{\"899\":1}}],[\"编码器输入\",{\"1\":{\"936\":1}}],[\"编码器输入通道从图像开始\",{\"1\":{\"899\":1}}],[\"编码器输出\",{\"1\":{\"899\":1}}],[\"编码器输出的\",{\"1\":{\"420\":1}}],[\"编码器下采样后\",{\"1\":{\"892\":1}}],[\"编码器把图像变成\",{\"1\":{\"885\":1}}],[\"编码器在给定\",{\"1\":{\"885\":1}}],[\"编码器隐藏层输出\",{\"1\":{\"750\":1}}],[\"编码器层\",{\"1\":{\"746\":1}}],[\"编码器层堆叠\",{\"1\":{\"699\":1}}],[\"编码器层数\",{\"1\":{\"699\":1}}],[\"编码器层列表\",{\"1\":{\"255\":1}}],[\"编码器提取上下文特征\",{\"1\":{\"266\":1}}],[\"编码器会被正则化为\",{\"1\":{\"262\":1}}],[\"编码器如果训练不当\",{\"1\":{\"262\":1}}],[\"编码器前向传播\",{\"1\":{\"256\":1}}],[\"编码器残差块\",{\"1\":{\"255\":1}}],[\"编码器初始输入通道\",{\"1\":{\"255\":1}}],[\"编码器中的卷积层个数\",{\"1\":{\"892\":1}}],[\"编码器中\",{\"1\":{\"234\":1}}],[\"编码器配置参数\",{\"1\":{\"213\":1}}],[\"编码器将图像转换为\",{\"1\":{\"212\":1}}],[\"编码器将输入图像转为离散\",{\"1\":{\"210\":1}}],[\"编码器建模图文交互\",{\"1\":{\"195\":1}}],[\"编码器模型适合理解类任务但难以生成文本\",{\"1\":{\"167\":1}}],[\"编码器模型不适合文本生成任务\",{\"1\":{\"165\":1}}],[\"编码器部分\",{\"1\":{\"146\":1}}],[\"编码器各层输出通道\",{\"1\":{\"123\":1}}],[\"编码器用于下采样和提取抽象特征\",{\"1\":{\"123\":1}}],[\"编码器不同层级的点云特征\",{\"1\":{\"70\":1}}],[\"编码器\",{\"0\":{\"179\":1,\"398\":1},\"1\":{\"22\":1,\"122\":2,\"123\":2,\"143\":2,\"144\":1,\"146\":1,\"165\":1,\"167\":1,\"220\":1,\"239\":1,\"255\":2,\"265\":1,\"270\":1,\"271\":2,\"741\":1,\"885\":1,\"887\":1,\"899\":3,\"931\":1,\"937\":1,\"947\":1,\"963\":1}}],[\"打包成\",{\"1\":{\"899\":1}}],[\"打包返回结果\",{\"1\":{\"385\":2}}],[\"打分的模型\",{\"1\":{\"895\":1}}],[\"打分是\",{\"1\":{\"709\":1}}],[\"打破了\",{\"1\":{\"823\":1}}],[\"打破了这两个领域壁垒\",{\"1\":{\"422\":1}}],[\"打印时自动对齐多行数据\",{\"1\":{\"808\":1}}],[\"打印当前性能指标\",{\"1\":{\"106\":1}}],[\"打乱顺序\",{\"1\":{\"698\":1}}],[\"打乱数据顺序等\",{\"1\":{\"521\":1}}],[\"打乱数据顺序\",{\"1\":{\"293\":1}}],[\"打开指定索引的图像文件\",{\"1\":{\"424\":1}}],[\"打下了基础\",{\"1\":{\"252\":1}}],[\"思想\",{\"1\":{\"885\":1}}],[\"思想成功引入视觉领域\",{\"1\":{\"252\":1}}],[\"思忖未知之界\",{\"1\":{\"837\":1}}],[\"思维链\",{\"1\":{\"825\":1}}],[\"思维链展示\",{\"1\":{\"823\":2}}],[\"思维链技术\",{\"0\":{\"620\":1}}],[\"思考的时间更短\",{\"1\":{\"823\":1}}],[\"思考\",{\"0\":{\"925\":1},\"1\":{\"619\":1}}],[\"思考快与慢\",{\"1\":{\"619\":1}}],[\"思路\",{\"1\":{\"250\":1,\"269\":1,\"518\":1}}],[\"借用了\",{\"1\":{\"921\":1}}],[\"借鉴了\",{\"1\":{\"282\":1}}],[\"借鉴\",{\"1\":{\"269\":1,\"377\":1}}],[\"借鉴这种\",{\"1\":{\"251\":1}}],[\"借助反向传播训练神经网络这一强大函数逼近器的突破\",{\"1\":{\"942\":1}}],[\"借助复杂的模型架构\",{\"1\":{\"884\":1}}],[\"借助于海量无标注数据的训练\",{\"1\":{\"826\":1}}],[\"借助字典映射为word\",{\"1\":{\"713\":1}}],[\"借助对比学习机制\",{\"1\":{\"406\":1}}],[\"借助海量图文数据和多阶段训练策略\",{\"1\":{\"313\":1}}],[\"借助\",{\"1\":{\"249\":1,\"823\":1,\"833\":1}}],[\"借助取反激活图片背景区域\",{\"1\":{\"83\":1}}],[\"化\",{\"1\":{\"293\":1}}],[\"化方法\",{\"1\":{\"251\":1}}],[\"化的表示也非常常见\",{\"1\":{\"251\":1}}],[\"化为\",{\"1\":{\"232\":1}}],[\"掉了\",{\"1\":{\"265\":1}}],[\"掉\",{\"1\":{\"250\":1}}],[\"掉的\",{\"1\":{\"205\":1,\"250\":1,\"263\":1,\"264\":1}}],[\"掉的单词\",{\"1\":{\"200\":1}}],[\"拼图\",{\"1\":{\"248\":1}}],[\"拼接为\",{\"1\":{\"964\":1}}],[\"拼接到已有序列后\",{\"1\":{\"898\":1}}],[\"拼接到文本后面作为起始序列\",{\"1\":{\"895\":1}}],[\"拼接到文本序列末尾\",{\"1\":{\"274\":1}}],[\"拼接而成的序列\",{\"1\":{\"898\":1}}],[\"拼接新生成的\",{\"1\":{\"895\":1}}],[\"拼接起来\",{\"1\":{\"885\":1,\"891\":1}}],[\"拼接等操作\",{\"1\":{\"800\":1}}],[\"拼接前提文本\",{\"1\":{\"631\":1}}],[\"拼接平均池化的\",{\"1\":{\"319\":1}}],[\"拼接结果\",{\"1\":{\"293\":1}}],[\"拼接并输入浅层\",{\"1\":{\"215\":1}}],[\"拼接当前动量文本特征和文本队列\",{\"1\":{\"206\":1}}],[\"拼接当前动量图像特征和图像队列\",{\"1\":{\"206\":1}}],[\"拼接机制不够精细\",{\"1\":{\"157\":1}}],[\"拼接方式缺乏动态调整机制\",{\"1\":{\"157\":1}}],[\"拼接相对坐标和特征\",{\"1\":{\"119\":1}}],[\"拼接\",{\"1\":{\"83\":1,\"266\":1,\"293\":1,\"380\":2,\"384\":2,\"441\":1,\"698\":1,\"887\":1,\"893\":2}}],[\"拼接嵌入向量\",{\"1\":{\"67\":1}}],[\"拼接在一起\",{\"1\":{\"67\":1,\"420\":1,\"660\":1}}],[\"拼接多模态嵌入与语言嵌入\",{\"0\":{\"67\":1},\"1\":{\"64\":1}}],[\"拼接后的输入嵌入\",{\"1\":{\"67\":1}}],[\"拼接后\",{\"1\":{\"58\":1,\"59\":1,\"60\":1,\"274\":1,\"380\":1}}],[\"拼接后送入解码器输出可供性预测\",{\"1\":{\"39\":1}}],[\"发生的概率\",{\"1\":{\"906\":1}}],[\"发生的可能性有多大\",{\"1\":{\"849\":1}}],[\"发生\",{\"1\":{\"848\":1}}],[\"发挥大模型为核心的大模型开发与传统的\",{\"1\":{\"835\":1}}],[\"发布并开源了\",{\"1\":{\"823\":2}}],[\"发布\",{\"1\":{\"823\":7}}],[\"发布的\",{\"1\":{\"823\":7}}],[\"发布模型\",{\"1\":{\"686\":1}}],[\"发布了全新升级的\",{\"1\":{\"823\":1}}],[\"发布了初始版本\",{\"1\":{\"823\":1}}],[\"发布了基于\",{\"1\":{\"823\":1}}],[\"发布了\",{\"1\":{\"183\":1,\"823\":10}}],[\"发现这对结果没有显著影响\",{\"1\":{\"888\":1}}],[\"发现潜在bug\",{\"1\":{\"796\":1}}],[\"发现无论是在训练损失还是实际任务中的表现\",{\"1\":{\"648\":1}}],[\"发现使用\",{\"1\":{\"311\":1}}],[\"发现\",{\"1\":{\"242\":1,\"680\":1}}],[\"各种情形本身的概率\",{\"1\":{\"850\":1}}],[\"各需要一个\",{\"1\":{\"746\":1,\"749\":1}}],[\"各类别的权重\",{\"1\":{\"586\":1}}],[\"各训练\",{\"1\":{\"317\":1}}],[\"各组成部分的重要性\",{\"1\":{\"242\":1}}],[\"各方法差别不大\",{\"1\":{\"47\":1}}],[\"张图片\",{\"1\":{\"293\":1}}],[\"张全局裁剪图像\",{\"1\":{\"293\":1}}],[\"张量简介\",{\"1\":{\"547\":1}}],[\"张量变换操作\",{\"0\":{\"543\":1}}],[\"张量可以被视为一种广义的矩阵\",{\"1\":{\"540\":1}}],[\"张量转换为\",{\"1\":{\"492\":1}}],[\"张量会和调用它的原张量\",{\"1\":{\"486\":1}}],[\"张量创建方法\",{\"1\":{\"486\":1}}],[\"张量\",{\"1\":{\"257\":1,\"467\":1,\"540\":1,\"542\":1,\"709\":1}}],[\"张\",{\"1\":{\"236\":1,\"315\":1,\"316\":1}}],[\"β=0\",{\"1\":{\"590\":2}}],[\"β>α\",{\"1\":{\"590\":1}}],[\"β\",{\"1\":{\"590\":3,\"592\":3,\"656\":1,\"885\":2}}],[\"β2=0\",{\"1\":{\"315\":1,\"316\":1}}],[\"β1=0\",{\"1\":{\"315\":1,\"316\":1}}],[\"β₂\",{\"1\":{\"236\":1}}],[\"β₁\",{\"1\":{\"236\":1}}],[\"此后\",{\"1\":{\"884\":1}}],[\"此例中\",{\"1\":{\"809\":1}}],[\"此指标可平衡精确率和召回率的重要性\",{\"1\":{\"567\":1}}],[\"此处将行维度从\",{\"1\":{\"546\":1}}],[\"此前的研究\",{\"1\":{\"646\":1}}],[\"此前\",{\"1\":{\"339\":1}}],[\"此阶段有两种不同的配置方式\",{\"1\":{\"306\":1}}],[\"此阶段中\",{\"1\":{\"306\":1}}],[\"此时公式简化为\",{\"1\":{\"944\":1}}],[\"此时离目的地更近\",{\"1\":{\"816\":1}}],[\"此时需交换操作数顺序并调用sub类\",{\"1\":{\"809\":1}}],[\"此时a\",{\"1\":{\"806\":1}}],[\"此时我们可以通过辈分来确保函数b和c先于函数a取出\",{\"1\":{\"805\":1}}],[\"此时我们还要训练两个\",{\"1\":{\"694\":1}}],[\"此时的tinypytorch已经具备了自动微分的能力\",{\"1\":{\"797\":1}}],[\"此时影响因子r\",{\"1\":{\"775\":1}}],[\"此时r=\",{\"1\":{\"775\":1}}],[\"此时为空\",{\"1\":{\"663\":1}}],[\"此时马氏距离变为\",{\"1\":{\"578\":1}}],[\"此时必须先\",{\"1\":{\"469\":1}}],[\"此时先用text\",{\"1\":{\"420\":1}}],[\"此时认为\",{\"1\":{\"261\":1}}],[\"此时\",{\"1\":{\"235\":1,\"261\":1,\"355\":1,\"359\":1,\"809\":1,\"846\":1,\"886\":1}}],[\"此外\",{\"1\":{\"29\":1,\"50\":1,\"72\":1,\"220\":1,\"228\":1,\"232\":1,\"234\":1,\"252\":1,\"272\":1,\"273\":1,\"283\":1,\"298\":1,\"310\":1,\"330\":1,\"376\":2,\"388\":1,\"408\":1,\"409\":1,\"413\":1,\"470\":1,\"497\":1,\"614\":1,\"648\":1,\"650\":1,\"653\":1,\"654\":1,\"655\":1,\"678\":1,\"827\":1,\"884\":1,\"951\":1}}],[\"考察其中任一排列方式\",{\"1\":{\"881\":1}}],[\"考察所有可能的\",{\"1\":{\"106\":1}}],[\"考虑元素之间的顺序\",{\"0\":{\"881\":1}}],[\"考虑某个点\",{\"1\":{\"874\":1}}],[\"考虑两个事件\",{\"1\":{\"849\":1}}],[\"考虑\",{\"1\":{\"614\":1}}],[\"考虑使用其他指标\",{\"1\":{\"566\":1}}],[\"考虑篇幅原因\",{\"1\":{\"435\":1}}],[\"考虑到我们使用的图文对数量有限\",{\"1\":{\"376\":1}}],[\"考虑对数似然\",{\"1\":{\"235\":1}}],[\"节的定量结果中\",{\"1\":{\"888\":1}}],[\"节点连接\",{\"1\":{\"815\":1}}],[\"节点属性设置\",{\"1\":{\"815\":1}}],[\"节点间用换行分隔\",{\"1\":{\"815\":1}}],[\"节点和箭头展示计算流程\",{\"1\":{\"760\":1}}],[\"节\",{\"1\":{\"655\":2}}],[\"节省参数\",{\"1\":{\"710\":1}}],[\"节省gpu内存\",{\"1\":{\"667\":1}}],[\"节省内存\",{\"1\":{\"472\":2,\"681\":1,\"898\":1}}],[\"节省显存\",{\"1\":{\"192\":1,\"895\":1,\"899\":1}}],[\"节中的消融实验表明\",{\"1\":{\"234\":1}}],[\"块\",{\"1\":{\"482\":1}}],[\"块级作用域\",{\"1\":{\"444\":1}}],[\"块通过切换不同模态专家捕捉模态特定信息\",{\"1\":{\"372\":1}}],[\"块对齐\",{\"1\":{\"369\":1}}],[\"块对齐以及掩码语言建模等方法来聚合和对齐视觉与语言信息\",{\"1\":{\"368\":1}}],[\"块内可编码图像\",{\"1\":{\"368\":1}}],[\"块状掩码策略的具体实现代码如下所示\",{\"1\":{\"263\":1}}],[\"块状遮挡通过遮盖图像中的连续\",{\"1\":{\"263\":1}}],[\"块状遮挡\",{\"0\":{\"263\":1},\"1\":{\"263\":1}}],[\"块状遮挡或\",{\"1\":{\"234\":1}}],[\"块由一个共享的\",{\"1\":{\"222\":1}}],[\"详解\",{\"0\":{\"659\":1}}],[\"详见表\",{\"1\":{\"647\":1}}],[\"详见表1\",{\"1\":{\"303\":1,\"666\":1}}],[\"详见附录\",{\"1\":{\"376\":1,\"658\":1}}],[\"详见算法\",{\"1\":{\"234\":1}}],[\"详细参数配置见表2\",{\"1\":{\"667\":1}}],[\"详细描述型\",{\"1\":{\"342\":1}}],[\"详细描述\",{\"1\":{\"341\":2}}],[\"详细描述交互\",{\"1\":{\"52\":1}}],[\"详细训练设置\",{\"0\":{\"314\":1}}],[\"详细解释\",{\"1\":{\"100\":1,\"735\":1}}],[\"详细交互行为\",{\"1\":{\"52\":1}}],[\"序列和解码后的文本\",{\"1\":{\"898\":1}}],[\"序列解码为可读文本\",{\"1\":{\"898\":1}}],[\"序列中每个\",{\"1\":{\"892\":1}}],[\"序列中额外加入一个可学习的\",{\"1\":{\"286\":1}}],[\"序列很长时\",{\"1\":{\"710\":1}}],[\"序列输出\",{\"1\":{\"385\":1}}],[\"序列表示\",{\"1\":{\"274\":2}}],[\"序列分开\",{\"1\":{\"274\":1}}],[\"序列长度\",{\"1\":{\"274\":1,\"380\":1,\"679\":1,\"680\":1,\"709\":1,\"710\":1,\"892\":1,\"893\":1}}],[\"序列进行\",{\"1\":{\"272\":1}}],[\"序列\",{\"1\":{\"234\":1,\"266\":1,\"341\":1,\"385\":1,\"420\":1,\"631\":1,\"895\":7,\"898\":2,\"899\":1}}],[\"序列前添加一个特殊的\",{\"1\":{\"233\":1}}],[\"序列为文本\",{\"1\":{\"188\":1}}],[\"峰值学习率\",{\"1\":{\"316\":1,\"319\":1}}],[\"峰值\",{\"1\":{\"224\":1}}],[\"水平\",{\"1\":{\"681\":1}}],[\"水平翻转等\",{\"1\":{\"264\":1}}],[\"水平翻转和颜色抖动\",{\"1\":{\"236\":1}}],[\"水平翻转\",{\"1\":{\"224\":1}}],[\"水壶的壶嘴\",{\"1\":{\"52\":1}}],[\"亿对文本\",{\"1\":{\"888\":1}}],[\"亿图文对\",{\"1\":{\"884\":1}}],[\"亿图像\",{\"1\":{\"176\":1}}],[\"亿的自回归\",{\"1\":{\"884\":1}}],[\"亿的大语言模型\",{\"1\":{\"823\":1}}],[\"亿用户的增长\",{\"1\":{\"823\":1}}],[\"亿个语句的数据合集\",{\"1\":{\"696\":1}}],[\"亿张图像\",{\"1\":{\"435\":1}}],[\"亿时\",{\"1\":{\"354\":1}}],[\"亿样本\",{\"1\":{\"315\":2}}],[\"亿\",{\"1\":{\"224\":3,\"272\":1,\"822\":2}}],[\"亿参数模型的初步实验\",{\"1\":{\"888\":1}}],[\"亿参数的稀疏\",{\"1\":{\"887\":1}}],[\"亿参数的\",{\"1\":{\"822\":2}}],[\"亿参数规模\",{\"1\":{\"313\":1,\"888\":1}}],[\"亿参数\",{\"1\":{\"224\":2,\"272\":1,\"306\":1,\"823\":1}}],[\"顶层使用视觉\",{\"1\":{\"372\":1}}],[\"顶层三层还包含视觉\",{\"1\":{\"224\":1}}],[\"顶部\",{\"1\":{\"272\":1}}],[\"顶部三层还额外包含\",{\"1\":{\"222\":1}}],[\"获得每个像素的概率分布\",{\"1\":{\"926\":1}}],[\"获得更好的应用效果\",{\"1\":{\"826\":1}}],[\"获得可以适用于大量下游任务的大模型\",{\"1\":{\"826\":1}}],[\"获得连续隐变量\",{\"1\":{\"256\":1}}],[\"获得的一系列离散\",{\"1\":{\"232\":1}}],[\"获得离散化的视觉\",{\"1\":{\"223\":1}}],[\"获取离散编码这一步有一点多余\",{\"1\":{\"958\":1}}],[\"获取卷积核尺寸\",{\"1\":{\"926\":1}}],[\"获取图文\",{\"1\":{\"900\":1}}],[\"获取图片\",{\"1\":{\"53\":1,\"82\":1}}],[\"获取维度信息\",{\"1\":{\"899\":1}}],[\"获取维度数\",{\"1\":{\"808\":1}}],[\"获取嵌入向量图\",{\"1\":{\"899\":1}}],[\"获取最后一维的大小\",{\"1\":{\"896\":1}}],[\"获取最后一层的自注意力权重\",{\"1\":{\"582\":1}}],[\"获取最后一层的自注意力权重矩阵\",{\"1\":{\"582\":2}}],[\"获取元素总数\",{\"1\":{\"808\":1}}],[\"获取实际对象\",{\"1\":{\"806\":1}}],[\"获取选项个数\",{\"1\":{\"737\":1}}],[\"获取词嵌入层权重\",{\"1\":{\"699\":1}}],[\"获取所有单词并去重\",{\"1\":{\"697\":1}}],[\"获取所有单词和每个单词的出现次数词典\",{\"1\":{\"595\":1}}],[\"获取子张量\",{\"1\":{\"544\":1}}],[\"获取第二列\",{\"1\":{\"544\":1}}],[\"获取第二行\",{\"1\":{\"544\":1}}],[\"获取输入张量x的形状\",{\"1\":{\"430\":1}}],[\"获取输入图像张量的形状\",{\"1\":{\"426\":1}}],[\"获取输入维度信息\",{\"1\":{\"64\":1}}],[\"获取对应图像的标签\",{\"1\":{\"424\":1}}],[\"获取对应的负文本嵌入\",{\"1\":{\"207\":1}}],[\"获取对应的负图像嵌入\",{\"1\":{\"207\":1}}],[\"获取该类别对应的索引\",{\"1\":{\"424\":1}}],[\"获取其对应的图像嵌入向量列表\",{\"1\":{\"410\":1}}],[\"获取候选分类名列表\",{\"1\":{\"410\":1,\"412\":1}}],[\"获取正负样本数量及\",{\"1\":{\"386\":1}}],[\"获取相对位置偏置\",{\"1\":{\"385\":1}}],[\"获取表示再融合\",{\"1\":{\"369\":1}}],[\"获取区域特征时可扩展性差\",{\"1\":{\"369\":1}}],[\"获取每个相邻字符对的出现次数\",{\"1\":{\"595\":3}}],[\"获取每个输入\",{\"1\":{\"293\":1}}],[\"获取每个查询点的邻居特征\",{\"1\":{\"119\":1}}],[\"获取每个查询点的邻居坐标\",{\"1\":{\"119\":1}}],[\"获取损失的标量值\",{\"1\":{\"265\":1}}],[\"获取回归目标\",{\"1\":{\"213\":1}}],[\"获取初始的\",{\"1\":{\"213\":1}}],[\"获取全局区域特征向量后\",{\"1\":{\"138\":1}}],[\"获取k个最近邻的索引和距离\",{\"1\":{\"119\":1}}],[\"获取当前最高频的字符对\",{\"1\":{\"595\":3}}],[\"获取当前\",{\"1\":{\"152\":1}}],[\"获取当前功能类型对应的索引值\",{\"1\":{\"92\":1}}],[\"获取当前样本对应的问题文本\",{\"1\":{\"92\":1}}],[\"获取当前样本对应的功能区域掩码\",{\"1\":{\"92\":1}}],[\"获取当前样本对应的功能类型\",{\"1\":{\"92\":1}}],[\"获取当前样本对应的物体类别\",{\"1\":{\"92\":1}}],[\"获取当前样本对应的点云id\",{\"1\":{\"92\":1}}],[\"获取当前样本的物体类别和物体信息值\",{\"1\":{\"92\":1}}],[\"获取当前样本中多模态嵌入的维度信息\",{\"1\":{\"67\":1}}],[\"获取样本的代码实现\",{\"1\":{\"92\":1}}],[\"获取两个注意力加权结果\",{\"1\":{\"69\":1}}],[\"获取\",{\"1\":{\"67\":1,\"69\":1,\"208\":2,\"266\":2,\"274\":2,\"384\":1,\"401\":1,\"582\":1,\"699\":1,\"893\":1,\"898\":1}}],[\"获取语言嵌入\",{\"1\":{\"64\":1}}],[\"获取设备信息\",{\"1\":{\"64\":1}}],[\"获取数据类型\",{\"1\":{\"808\":1}}],[\"获取数据形状\",{\"1\":{\"808\":1}}],[\"获取数据\",{\"1\":{\"53\":1,\"82\":1}}],[\"公平硬币\",{\"1\":{\"907\":1}}],[\"公理\",{\"1\":{\"848\":1}}],[\"公司开发的闭源语言大模型\",{\"1\":{\"823\":1}}],[\"公司在\",{\"1\":{\"823\":1}}],[\"公开数据+严格过滤\",{\"1\":{\"667\":1}}],[\"公开数据集\",{\"1\":{\"220\":1}}],[\"公共对齐空间的维度\",{\"1\":{\"900\":1}}],[\"公共\",{\"1\":{\"656\":1}}],[\"公式2到5\",{\"1\":{\"952\":1}}],[\"公式相当于\",{\"1\":{\"893\":1}}],[\"公式推导过程\",{\"1\":{\"885\":1}}],[\"公式的含义\",{\"1\":{\"885\":1}}],[\"公式人话版本\",{\"1\":{\"877\":1}}],[\"公式表示如下\",{\"1\":{\"611\":1}}],[\"公式转化为tanimoto系数\",{\"1\":{\"590\":1}}],[\"公式是\",{\"1\":{\"576\":1,\"577\":1}}],[\"公式描述\",{\"1\":{\"504\":1}}],[\"公式\",{\"0\":{\"499\":1},\"1\":{\"293\":1,\"507\":1,\"508\":1,\"587\":2,\"906\":1,\"907\":1,\"908\":1,\"946\":2,\"947\":1,\"950\":1}}],[\"公式展开如下\",{\"1\":{\"213\":1}}],[\"公式为\",{\"1\":{\"212\":1,\"770\":1}}],[\"公式中\",{\"1\":{\"202\":1}}],[\"公式来自论文\",{\"1\":{\"102\":1}}],[\"公式如下\",{\"1\":{\"37\":1,\"106\":1,\"506\":1,\"586\":1,\"629\":1}}],[\"扩大模型规模\",{\"1\":{\"377\":1}}],[\"扩大模型规模与数据规模\",{\"1\":{\"220\":1}}],[\"扩展模型的推理能力\",{\"1\":{\"832\":2}}],[\"扩展语言模型的大小\",{\"1\":{\"823\":1}}],[\"扩展函数库并验证高阶导数\",{\"1\":{\"814\":1}}],[\"扩展dezero以处理多输入多输出函数\",{\"1\":{\"812\":1}}],[\"扩展function类\",{\"0\":{\"779\":1}}],[\"扩展variable类\",{\"0\":{\"778\":1}}],[\"扩展至更大规模数据\",{\"1\":{\"679\":1}}],[\"扩展配置\",{\"1\":{\"640\":1}}],[\"扩展后的\",{\"1\":{\"546\":1}}],[\"扩展后的逻辑形状\",{\"1\":{\"546\":1}}],[\"扩展后便于广播乘法\",{\"1\":{\"145\":1}}],[\"扩展时\",{\"1\":{\"492\":1}}],[\"扩展等操作\",{\"1\":{\"478\":1}}],[\"扩展分类标记以匹配输入批次大小\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"扩展可学习的query\",{\"1\":{\"421\":1}}],[\"扩展了负样本数量\",{\"1\":{\"385\":1}}],[\"扩展\",{\"1\":{\"380\":1,\"699\":1}}],[\"扩展gpt\",{\"1\":{\"325\":1}}],[\"扩展到多个变量\",{\"1\":{\"574\":1}}],[\"扩展到更多模态\",{\"1\":{\"377\":1}}],[\"扩展到\",{\"1\":{\"274\":1,\"546\":1}}],[\"扩展到了\",{\"1\":{\"220\":1}}],[\"扩展成\",{\"1\":{\"213\":1}}],[\"扩展性\",{\"1\":{\"157\":1}}],[\"扩展维度后相乘\",{\"1\":{\"145\":1}}],[\"扩展每个点的特征表达能力\",{\"1\":{\"121\":1}}],[\"扩展生成更多问题\",{\"1\":{\"87\":1}}],[\"扩展构建了piadv2\",{\"1\":{\"30\":1}}],[\"缩小方法\",{\"1\":{\"505\":1}}],[\"缩小了与商业模型的差距\",{\"1\":{\"332\":1}}],[\"缩小了\",{\"1\":{\"217\":1}}],[\"缩放特征\",{\"1\":{\"522\":1}}],[\"缩放参数\",{\"1\":{\"522\":1}}],[\"缩放点积注意力\",{\"1\":{\"401\":1,\"524\":1}}],[\"缩放值\",{\"1\":{\"380\":1}}],[\"缩放因子\",{\"1\":{\"69\":1,\"380\":2,\"385\":2,\"430\":1,\"503\":1}}],[\"缩放\",{\"1\":{\"19\":1,\"161\":1,\"162\":1,\"380\":1,\"640\":1}}],[\"松弛温度与步长的退火策略\",{\"1\":{\"886\":1}}],[\"松弛方法\",{\"1\":{\"232\":1}}],[\"松弛\",{\"1\":{\"216\":1,\"886\":1}}],[\"眼睛\",{\"1\":{\"215\":1}}],[\"鼓励后续研究继续探索更大规模语言模型的行为边界\",{\"1\":{\"643\":1}}],[\"鼓励\",{\"1\":{\"214\":1,\"963\":2}}],[\"鼓励变换矩阵接近正交矩阵\",{\"1\":{\"153\":1}}],[\"级别的参数量\",{\"1\":{\"609\":1}}],[\"级别的离散表示学习与教师特征对齐\",{\"1\":{\"212\":1}}],[\"级联方案效果更好\",{\"1\":{\"278\":1}}],[\"级联方案\",{\"1\":{\"278\":1}}],[\"级预训练与图像级表示聚合之间的差距\",{\"1\":{\"217\":1}}],[\"级预训练与图像级表示聚合之间的差异\",{\"1\":{\"214\":1}}],[\"嵌入向量\",{\"1\":{\"964\":1}}],[\"嵌入向量生成过程图\",{\"1\":{\"716\":1}}],[\"嵌入空间误差为嵌入和其对应的编码器输出的均方误差\",{\"1\":{\"961\":1}}],[\"嵌入空间的向量应该和其对应编码器输出尽可能接近\",{\"1\":{\"960\":1}}],[\"嵌入空间的每一个向量应该能概括一类编码器输出的向量\",{\"1\":{\"960\":1}}],[\"嵌入空间的优化目标是什么呢\",{\"1\":{\"960\":1}}],[\"嵌入空间直接关联起来呢\",{\"1\":{\"958\":1}}],[\"嵌入空间\",{\"1\":{\"956\":1}}],[\"嵌入图像\",{\"1\":{\"900\":1}}],[\"嵌入和注意力层以\",{\"1\":{\"633\":1}}],[\"嵌入层维度\",{\"1\":{\"430\":1}}],[\"嵌入交互\",{\"1\":{\"427\":1}}],[\"嵌入的关系\",{\"1\":{\"427\":1}}],[\"嵌入序列的开头\",{\"1\":{\"427\":1}}],[\"嵌入中\",{\"1\":{\"233\":1,\"427\":1}}],[\"嵌入\",{\"1\":{\"233\":1,\"388\":1,\"427\":1}}],[\"嵌入索引\",{\"1\":{\"213\":1}}],[\"嵌入维度\",{\"1\":{\"213\":1,\"255\":1,\"266\":1,\"426\":1,\"427\":1}}],[\"范式\",{\"1\":{\"650\":1}}],[\"范式应用于其他视觉任务也非常直接\",{\"1\":{\"237\":1}}],[\"范围内\",{\"1\":{\"479\":1}}],[\"范围缩放到\",{\"1\":{\"425\":2}}],[\"范围在\",{\"1\":{\"263\":1}}],[\"范围\",{\"1\":{\"213\":1,\"425\":2}}],[\"范数进行归一化\",{\"1\":{\"362\":1}}],[\"范数\",{\"1\":{\"153\":2,\"507\":1,\"508\":1,\"951\":1}}],[\"词性标注\",{\"1\":{\"736\":1}}],[\"词库中的词汇数量\",{\"1\":{\"696\":1}}],[\"词汇和句法歧义\",{\"1\":{\"634\":1}}],[\"词汇表增至50\",{\"1\":{\"640\":1}}],[\"词汇表灵活性\",{\"1\":{\"640\":1}}],[\"词汇表\",{\"1\":{\"633\":1}}],[\"词汇表与正则化\",{\"1\":{\"633\":1}}],[\"词汇表大小为\",{\"1\":{\"887\":2}}],[\"词汇表大小\",{\"1\":{\"213\":1}}],[\"词频从高到低\",{\"1\":{\"595\":1}}],[\"词嵌入与位置嵌入相加\",{\"1\":{\"419\":1}}],[\"词嵌入\",{\"1\":{\"397\":1,\"627\":1,\"663\":2,\"699\":2}}],[\"词tokenized成3个部分\",{\"1\":{\"393\":1}}],[\"词表大小\",{\"1\":{\"699\":1}}],[\"词表大小设置为\",{\"1\":{\"232\":1}}],[\"词表的大小\",{\"1\":{\"234\":1}}],[\"词表的\",{\"1\":{\"224\":1}}],[\"词表分布\",{\"1\":{\"200\":1}}],[\"词\",{\"1\":{\"223\":1,\"368\":1,\"369\":1}}],[\"簇中心\",{\"1\":{\"213\":1}}],[\"记忆机制\",{\"1\":{\"833\":1}}],[\"记忆\",{\"1\":{\"832\":1}}],[\"记忆与泛化分析\",{\"1\":{\"641\":1}}],[\"记忆库\",{\"1\":{\"282\":1}}],[\"记住用户之前的交互习惯和偏好\",{\"1\":{\"823\":1}}],[\"记住了\",{\"1\":{\"448\":1,\"449\":1}}],[\"记住\",{\"1\":{\"339\":1,\"448\":1}}],[\"记作\",{\"1\":{\"234\":1,\"260\":1,\"274\":1,\"846\":2,\"850\":2,\"858\":1,\"944\":1}}],[\"记为\",{\"1\":{\"233\":1}}],[\"记被遮挡的位置为\",{\"1\":{\"214\":1}}],[\"记得对中心做\",{\"1\":{\"213\":1}}],[\"记录创建该变量的函数\",{\"1\":{\"783\":1}}],[\"记录input后\",{\"1\":{\"779\":1}}],[\"记录输入长度\",{\"1\":{\"713\":1}}],[\"记录被掩码的原token\",{\"1\":{\"697\":1}}],[\"记录被掩码的位置\",{\"1\":{\"697\":1,\"699\":1}}],[\"记录当前词对应输入词序列中的索引\",{\"1\":{\"663\":1}}],[\"记录该子词对的全局频率\",{\"1\":{\"595\":1}}],[\"记录该类别的样本数量\",{\"1\":{\"424\":1}}],[\"记录历史合并的最高频子词对及其频率\",{\"1\":{\"595\":1}}],[\"记录损失和准确率到日志\",{\"1\":{\"384\":1}}],[\"记录损失日志\",{\"1\":{\"213\":1}}],[\"记录一下本轮训练需要推进的学习任务有几个\",{\"1\":{\"383\":1}}],[\"记录\",{\"1\":{\"380\":2,\"385\":1,\"836\":1}}],[\"记录本次新增遮挡的patch数量\",{\"1\":{\"263\":1}}],[\"记录宽高比范围的对数形式\",{\"1\":{\"263\":1}}],[\"记录patch大小和token形状\",{\"1\":{\"213\":1}}],[\"记录每个词的出现次数的词典\",{\"1\":{\"595\":1}}],[\"记录每个词的出现次数\",{\"1\":{\"595\":1}}],[\"记录每个物体对应的点云索引下标区间\",{\"1\":{\"53\":1}}],[\"记录每层输出\",{\"1\":{\"385\":1}}],[\"记录每类物体对应的点云文件下标索引区间\",{\"1\":{\"82\":1}}],[\"记录物体边界框\",{\"1\":{\"82\":1}}],[\"空事件\",{\"1\":{\"847\":1}}],[\"空\",{\"1\":{\"385\":3}}],[\"空簇\",{\"1\":{\"213\":1}}],[\"空间远小于先验分布\",{\"1\":{\"945\":1}}],[\"空间上的掩码仍然使用传统\",{\"1\":{\"924\":1}}],[\"空间掩码设计\",{\"1\":{\"926\":1}}],[\"空间掩码\",{\"0\":{\"923\":1},\"1\":{\"924\":2}}],[\"空间关系混乱\",{\"1\":{\"884\":1}}],[\"空间\",{\"1\":{\"274\":1,\"385\":2,\"895\":1,\"900\":4}}],[\"空间中的位置\",{\"1\":{\"159\":1}}],[\"空间变换网络\",{\"1\":{\"148\":1,\"150\":1}}],[\"空间是均匀和各向同性的\",{\"1\":{\"135\":1}}],[\"空间均匀性\",{\"1\":{\"135\":1}}],[\"空间混合\",{\"1\":{\"97\":1}}],[\"空间先验对齐\",{\"1\":{\"76\":1}}],[\"空间维度\",{\"1\":{\"70\":1}}],[\"空间特征\",{\"1\":{\"69\":1}}],[\"空间更具挑战\",{\"1\":{\"20\":1}}],[\"μ\",{\"1\":{\"213\":1,\"931\":2}}],[\"循环引用会导致python对象无法被正常回收\",{\"1\":{\"806\":1}}],[\"循环引用仍会导致对象无法释放\",{\"1\":{\"806\":1}}],[\"循环引用指对象之间相互引用\",{\"1\":{\"806\":1}}],[\"循环引用与内存释放\",{\"0\":{\"806\":1}}],[\"循环来设置每一对的导数\",{\"1\":{\"801\":1}}],[\"循环实现的优势\",{\"0\":{\"788\":1}}],[\"循环实现反向传播\",{\"0\":{\"787\":1}}],[\"循环变量在循环体外仍然可见\",{\"1\":{\"444\":1}}],[\"循环多个epoch\",{\"1\":{\"265\":1}}],[\"循环直到遮挡的patch数量达到指定的遮挡总数\",{\"1\":{\"263\":1}}],[\"循环\",{\"1\":{\"213\":1,\"381\":5}}],[\"循环更新\",{\"1\":{\"192\":1}}],[\"迭代次数设为iters\",{\"1\":{\"816\":1}}],[\"迭代式对齐研究计划的一部分\",{\"1\":{\"658\":1}}],[\"迭代器\",{\"1\":{\"516\":1}}],[\"迭代\",{\"1\":{\"213\":1}}],[\"迭代选择\",{\"1\":{\"121\":1}}],[\"针对第一个问题\",{\"1\":{\"944\":1}}],[\"针对性优化\",{\"1\":{\"835\":1}}],[\"针对不同设置调整峰值学习率和预热步数\",{\"1\":{\"680\":1}}],[\"针对同一输入\",{\"1\":{\"656\":1}}],[\"针对每张图像\",{\"1\":{\"410\":1}}],[\"针对每个图像嵌入向量获取和其相似度最高的分类文本嵌入向量索引下标\",{\"1\":{\"410\":1}}],[\"针对每个图像嵌入向量取出和其相似度最高的那个文本嵌入向量的索引\",{\"1\":{\"410\":1}}],[\"针对文本和图像分别有独立的归一化层\",{\"1\":{\"380\":1}}],[\"针对\",{\"1\":{\"380\":1}}],[\"针对输入\",{\"1\":{\"264\":1}}],[\"针对码本权重的初始化还做了特别的优化\",{\"1\":{\"213\":1}}],[\"针对铰接物体\",{\"1\":{\"75\":1}}],[\"计数法则\",{\"0\":{\"880\":1}}],[\"计数为\",{\"1\":{\"516\":1}}],[\"计数\",{\"1\":{\"213\":1}}],[\"计算该函数的梯度并进行平均\",{\"1\":{\"946\":1}}],[\"计算生成器损失\",{\"1\":{\"918\":1}}],[\"计算判别器损失\",{\"1\":{\"918\":1}}],[\"计算所有样本之间的相似度矩阵\",{\"1\":{\"900\":1}}],[\"计算所有点到最新选中点的距离\",{\"1\":{\"121\":1}}],[\"计算所有点到已选点集的最小距离\",{\"1\":{\"121\":1}}],[\"计算每对\",{\"1\":{\"900\":1}}],[\"计算每个可能的阈值\",{\"1\":{\"569\":1}}],[\"计算每个样本的权重\",{\"1\":{\"518\":1}}],[\"计算每个注意力头的维度\",{\"1\":{\"430\":1}}],[\"计算每个query\",{\"1\":{\"418\":1}}],[\"计算每个码本向量的累加特征\",{\"1\":{\"213\":1}}],[\"计算每个向量与码本中所有向量的欧氏距离平方\",{\"1\":{\"213\":1}}],[\"计算每个\",{\"1\":{\"208\":1,\"213\":1}}],[\"计算每个原始点和下采样点之间的距离\",{\"1\":{\"145\":1}}],[\"计算每个查询点\",{\"1\":{\"137\":1}}],[\"计算每个批次下采样后的点数\",{\"1\":{\"121\":1}}],[\"计算可学习温度值\",{\"1\":{\"900\":1}}],[\"计算边长\",{\"1\":{\"899\":1}}],[\"计算文本部分的\",{\"1\":{\"893\":1}}],[\"计算文本token和视觉token预测结果与原label的交叉熵损失\",{\"1\":{\"893\":1}}],[\"计算文本和图像\",{\"1\":{\"274\":1}}],[\"计算与图构建同时进行\",{\"1\":{\"811\":1}}],[\"计算y\",{\"1\":{\"809\":1}}],[\"计算两个输入变量的乘积\",{\"1\":{\"809\":1}}],[\"计算过程不保留计算图连接\",{\"1\":{\"807\":1}}],[\"计算完成后中间变量立即释放\",{\"1\":{\"807\":1}}],[\"计算掩码语言损失\",{\"1\":{\"731\":1}}],[\"计算需要填充的长度\",{\"1\":{\"713\":1}}],[\"计算需求\",{\"0\":{\"291\":1}}],[\"计算起来是比较容易的\",{\"1\":{\"706\":1}}],[\"计算资源\",{\"1\":{\"690\":1,\"830\":1}}],[\"计算资源消耗仍然较大\",{\"1\":{\"388\":1}}],[\"计算最优平衡理论\",{\"1\":{\"671\":1}}],[\"计算最大半径\",{\"1\":{\"107\":1}}],[\"计算律发现\",{\"1\":{\"671\":1}}],[\"计算并缓存其\",{\"1\":{\"663\":1}}],[\"计算并集\",{\"1\":{\"588\":1}}],[\"计算交集\",{\"1\":{\"586\":1,\"587\":1,\"588\":1}}],[\"计算交叉熵损失\",{\"1\":{\"206\":1,\"265\":1,\"293\":1,\"403\":1,\"419\":1,\"734\":1}}],[\"计算得到的编码值\",{\"1\":{\"706\":1}}],[\"计算得到的\",{\"1\":{\"586\":1,\"588\":1,\"589\":1,\"590\":1,\"592\":1}}],[\"计算特征图的尺寸\",{\"1\":{\"582\":1}}],[\"计算特征图中每个点和点云每个点特征的相似度\",{\"1\":{\"83\":1}}],[\"计算输出特征图的尺寸\",{\"1\":{\"582\":1}}],[\"计算机的内存\",{\"1\":{\"540\":1}}],[\"计算机视觉领域对大规模预训练基础模型的探索越来越重要\",{\"1\":{\"268\":1}}],[\"计算梯度\",{\"1\":{\"431\":1}}],[\"计算预测输出与真实标签之间的\",{\"1\":{\"587\":1,\"588\":1,\"589\":1}}],[\"计算预测结果与真实标签之间的交叉熵损失\",{\"1\":{\"431\":1}}],[\"计算预测正确的样本数\",{\"1\":{\"431\":1}}],[\"计算公式\",{\"1\":{\"514\":1}}],[\"计算公式如下\",{\"1\":{\"429\":1,\"524\":1}}],[\"计算公式为\",{\"1\":{\"261\":1}}],[\"计算网格大小\",{\"1\":{\"426\":1}}],[\"计算上下文表示\",{\"1\":{\"420\":1}}],[\"计算注意力分数\",{\"1\":{\"420\":1}}],[\"计算注意力权重\",{\"1\":{\"119\":1}}],[\"计算序列长度\",{\"1\":{\"419\":1}}],[\"计算一个\",{\"1\":{\"418\":1}}],[\"计算这批图像嵌入向量中每一个图像嵌入向量和我们所有分类文本嵌入向量的余弦相似度\",{\"1\":{\"410\":1}}],[\"计算余弦相似度\",{\"1\":{\"408\":1}}],[\"计算缩放的余弦相似度\",{\"1\":{\"407\":1}}],[\"计算跨模态\",{\"1\":{\"385\":1}}],[\"计算跨视图交叉熵损失\",{\"1\":{\"293\":1}}],[\"计算跨视图的交叉熵损失\",{\"1\":{\"293\":1}}],[\"计算单模态\",{\"1\":{\"385\":1}}],[\"计算量略高于ce\",{\"1\":{\"589\":1}}],[\"计算量太大了\",{\"1\":{\"355\":1}}],[\"计算量大\",{\"1\":{\"148\":1}}],[\"计算代价太高\",{\"1\":{\"355\":1}}],[\"计算效率\",{\"1\":{\"280\":1}}],[\"计算效率问题\",{\"1\":{\"135\":1}}],[\"计算开销线性增长\",{\"1\":{\"663\":1}}],[\"计算开销更大\",{\"1\":{\"280\":1}}],[\"计算开销\",{\"1\":{\"269\":1,\"500\":1}}],[\"计算开销大\",{\"1\":{\"110\":1,\"195\":1}}],[\"计算总\",{\"1\":{\"266\":1}}],[\"计算遮挡块区域内已被遮挡的patch数\",{\"1\":{\"263\":1}}],[\"计算本轮最多还能遮挡的patch数\",{\"1\":{\"263\":1}}],[\"计算重建损失\",{\"1\":{\"213\":2,\"256\":1,\"899\":1}}],[\"计算簇中心\",{\"1\":{\"213\":1}}],[\"计算平方距离并取负号\",{\"1\":{\"213\":1}}],[\"计算平均\",{\"1\":{\"106\":1}}],[\"计算样本与中心的距离\",{\"1\":{\"213\":1}}],[\"计算标准\",{\"1\":{\"208\":1}}],[\"计算来源\",{\"1\":{\"207\":1}}],[\"计算图进阶与通用神经网络实现\",{\"0\":{\"818\":1}}],[\"计算图遍历逻辑\",{\"1\":{\"815\":1}}],[\"计算图遍历优化\",{\"1\":{\"799\":1}}],[\"计算图中的中间变量\",{\"1\":{\"806\":1}}],[\"计算图的反向传播\",{\"0\":{\"776\":1}}],[\"计算图的意义\",{\"0\":{\"767\":1}}],[\"计算图用圆框表示变量\",{\"1\":{\"760\":1}}],[\"计算图与自动微分\",{\"1\":{\"752\":1}}],[\"计算图与自动微分的起点\",{\"0\":{\"752\":1}}],[\"计算图像部分的\",{\"1\":{\"893\":1}}],[\"计算图像块的数量\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"计算图像块的总数\",{\"1\":{\"426\":1}}],[\"计算图像嵌入向量列表中每个图像对应的嵌入向量和传入的分类文本嵌入向量的余弦相似度\",{\"1\":{\"410\":1}}],[\"计算图像到文本\",{\"1\":{\"386\":1}}],[\"计算图像能切分成多少个\",{\"1\":{\"380\":1}}],[\"计算图像\",{\"1\":{\"373\":1}}],[\"计算图认为对学习任务的贡献全部来源于\",{\"1\":{\"213\":1}}],[\"计算图\",{\"1\":{\"192\":1}}],[\"计算动量编码器输出\",{\"1\":{\"192\":1}}],[\"计算itm损失\",{\"1\":{\"190\":1}}],[\"计算itc损失\",{\"1\":{\"190\":1}}],[\"计算提示词token长度\",{\"1\":{\"187\":1}}],[\"计算语言建模损失\",{\"1\":{\"187\":1}}],[\"计算时引入\",{\"1\":{\"172\":1}}],[\"计算点集的分布特性\",{\"1\":{\"160\":1}}],[\"计算点云质心\",{\"1\":{\"107\":1}}],[\"计算点云中每个点和特征图中每个点特征的相似度\",{\"1\":{\"83\":1}}],[\"计算复杂度高\",{\"1\":{\"159\":1}}],[\"计算变换矩阵与其转置相乘后与单位矩阵之间的距离\",{\"1\":{\"153\":1}}],[\"计算权重\",{\"1\":{\"145\":1}}],[\"计算原始点与下采样点之间的距离矩阵\",{\"1\":{\"145\":1}}],[\"计算成本高\",{\"1\":{\"772\":1}}],[\"计算成本相当\",{\"1\":{\"684\":1}}],[\"计算成本\",{\"1\":{\"134\":1}}],[\"计算距离的倒数\",{\"1\":{\"122\":1}}],[\"计算全局平均特征并变换\",{\"1\":{\"122\":1}}],[\"计算全局平均特征\",{\"1\":{\"122\":1}}],[\"计算欧氏距离平方\",{\"1\":{\"121\":1}}],[\"计算下采样后的批次索引\",{\"1\":{\"121\":1}}],[\"计算查询点与所有点之间的欧几里得距离平方\",{\"1\":{\"119\":1}}],[\"计算当前位置和其他位置的特征相似度\",{\"1\":{\"582\":1}}],[\"计算当前中心点与所有点之间的欧氏距离平方\",{\"1\":{\"137\":1}}],[\"计算当前批次中所有序列的实际最大长度\",{\"1\":{\"715\":1}}],[\"计算当前批次的起始\",{\"1\":{\"122\":1}}],[\"计算当前批次的结束索引\",{\"1\":{\"121\":1}}],[\"计算当前批次需要采样的点数\",{\"1\":{\"121\":1}}],[\"计算当前batch的结束索引\",{\"1\":{\"119\":1}}],[\"计算当前语言输入中有多少个有效\",{\"1\":{\"67\":1}}],[\"计算其与当前batch内所有候选点的欧几里得距离\",{\"1\":{\"119\":1}}],[\"计算相对位置矩阵\",{\"1\":{\"710\":1}}],[\"计算相对位置偏置\",{\"1\":{\"266\":1,\"385\":1}}],[\"计算相对坐标\",{\"1\":{\"119\":1}}],[\"计算相似度\",{\"1\":{\"206\":1}}],[\"计算相似度矩阵\",{\"1\":{\"83\":1}}],[\"计算方式为\",{\"1\":{\"112\":1}}],[\"计算曲线下面积\",{\"1\":{\"106\":1}}],[\"计算指标包括\",{\"1\":{\"106\":1}}],[\"计算损失函数\",{\"1\":{\"963\":1}}],[\"计算损失的时候\",{\"1\":{\"691\":1}}],[\"计算损失的时候使用多分类\",{\"1\":{\"356\":1}}],[\"计算损失\",{\"1\":{\"105\":1}}],[\"计算负类\",{\"1\":{\"102\":1}}],[\"计算正类\",{\"1\":{\"102\":1}}],[\"计算保持稳定\",{\"1\":{\"102\":1}}],[\"计算经过下采样得到的特征图相比于原始图片的缩小比例\",{\"1\":{\"83\":1}}],[\"计算密集跨模态相似性矩阵\",{\"1\":{\"78\":1}}],[\"计算\",{\"1\":{\"64\":1,\"69\":2,\"106\":4,\"192\":1,\"207\":2,\"208\":2,\"265\":1,\"266\":1,\"274\":1,\"293\":1,\"382\":1,\"384\":2,\"385\":1,\"386\":1,\"401\":1,\"403\":1,\"418\":1,\"429\":1,\"586\":2,\"587\":2,\"588\":1,\"589\":2,\"590\":1,\"592\":1,\"765\":1,\"877\":1}}],[\"稍后再通过\",{\"1\":{\"213\":1}}],[\"管理类\",{\"1\":{\"213\":1}}],[\"码本使用率以及下游任务性能\",{\"1\":{\"215\":1}}],[\"码本和特征量化降低了对细节变化的敏感性\",{\"1\":{\"215\":1}}],[\"码本可视化\",{\"1\":{\"215\":1}}],[\"码本维度调整\",{\"1\":{\"215\":1}}],[\"码本大小\",{\"1\":{\"215\":1}}],[\"码本设置\",{\"1\":{\"215\":2}}],[\"码本权重\",{\"1\":{\"213\":1}}],[\"码本权重初始化\",{\"1\":{\"213\":1}}],[\"码本的向量个数\",{\"1\":{\"213\":1}}],[\"码本\",{\"1\":{\"213\":2}}],[\"梯度表示函数输出值最大的方向\",{\"1\":{\"816\":1}}],[\"梯度下降法解决问题\",{\"1\":{\"816\":1}}],[\"梯度下降等场景中非常常用\",{\"1\":{\"508\":1}}],[\"梯度展示了各点上函数输出值增加最快的方向\",{\"1\":{\"816\":1}}],[\"梯度分别乘以1和\",{\"1\":{\"809\":1}}],[\"梯度取反\",{\"1\":{\"809\":1}}],[\"梯度重复累加\",{\"1\":{\"804\":1}}],[\"梯度重复累加的问题\",{\"0\":{\"804\":1}}],[\"梯度累加\",{\"1\":{\"799\":1}}],[\"梯度检验\",{\"0\":{\"795\":1}}],[\"梯度裁剪1\",{\"1\":{\"667\":1}}],[\"梯度消失\",{\"1\":{\"500\":2,\"537\":1}}],[\"梯度消失或爆炸\",{\"1\":{\"65\":1}}],[\"梯度清零\",{\"1\":{\"265\":1}}],[\"梯度流\",{\"1\":{\"258\":1}}],[\"梯度回传的时候\",{\"1\":{\"213\":1}}],[\"梯度可以沿着直接通路回传回encoder进行更新\",{\"1\":{\"213\":2}}],[\"梯度从\",{\"1\":{\"212\":1}}],[\"展开\",{\"1\":{\"945\":1}}],[\"展现出了非常流畅和自然的表现\",{\"1\":{\"822\":1}}],[\"展现了强大的视觉能力和与llms的无缝集成潜力\",{\"1\":{\"295\":1}}],[\"展平后是\",{\"1\":{\"892\":1}}],[\"展平为分类标签\",{\"1\":{\"926\":1}}],[\"展平为分类输入\",{\"1\":{\"926\":1}}],[\"展平为\",{\"1\":{\"384\":2,\"899\":1}}],[\"展平并调整维度\",{\"1\":{\"380\":1}}],[\"展平并线性映射得到\",{\"1\":{\"212\":1}}],[\"展平\",{\"1\":{\"266\":1,\"380\":1}}],[\"展平掩码为\",{\"1\":{\"265\":1}}],[\"展平空间维度\",{\"1\":{\"256\":1,\"260\":1}}],[\"展平特征图\",{\"1\":{\"213\":1}}],[\"展示\",{\"1\":{\"657\":1,\"892\":1}}],[\"展示强大的数学\",{\"1\":{\"335\":1}}],[\"展示出卓越的性能\",{\"1\":{\"252\":1}}],[\"展示同类物体的结构相似性可辅助对齐\",{\"1\":{\"73\":1}}],[\"展示了当我们从更多的样本数量\",{\"1\":{\"889\":1}}],[\"展示了一些从如下高斯分布中采样的灰度图像\",{\"1\":{\"875\":1}}],[\"展示了三种不同协方差矩阵下的二维多元高斯密度图\",{\"1\":{\"871\":1}}],[\"展示了其强大的泛化能力\",{\"1\":{\"825\":1}}],[\"展示了其轻量化设计在效率与性能上的优势\",{\"1\":{\"388\":1}}],[\"展示了如何仅使用公开可用的数据集来训练最先进的模型\",{\"1\":{\"823\":1}}],[\"展示了模型输出的多维质量元数据对比\",{\"1\":{\"657\":1}}],[\"展示了模型在是否遵循指令\",{\"1\":{\"656\":1}}],[\"展示了强大的in\",{\"1\":{\"651\":1}}],[\"展示了强大的少样本和零样本学习能力\",{\"1\":{\"299\":1}}],[\"展示了自然语言本身作为任务描述符的有效性\",{\"1\":{\"643\":1}}],[\"展示了大规模语言模型在无监督多任务学习中的潜力\",{\"1\":{\"638\":1}}],[\"展示了良好的语言泛化能力\",{\"1\":{\"309\":1}}],[\"展示了不同\",{\"1\":{\"243\":1}}],[\"展示了不同变体的实验结果\",{\"1\":{\"242\":1}}],[\"展示了我们方法的整体流程\",{\"1\":{\"234\":1}}],[\"展示了网页原始文本\",{\"1\":{\"177\":1}}],[\"展示了在大规模视觉\",{\"1\":{\"169\":1}}],[\"展示了对关键模块的消融实验结果\",{\"1\":{\"48\":1}}],[\"展示了各\",{\"1\":{\"43\":1}}],[\"展示了各类别的分布情况\",{\"1\":{\"41\":1}}],[\"继续改进\",{\"1\":{\"921\":1}}],[\"继续上述例子\",{\"1\":{\"846\":1}}],[\"继续训练的效果不如重新训练\",{\"1\":{\"182\":1}}],[\"继承性\",{\"1\":{\"520\":1}}],[\"继承第一阶段权重\",{\"1\":{\"316\":1}}],[\"继承了经过第一阶段对比训练后得到的\",{\"1\":{\"306\":1}}],[\"继承了第一阶段中学习到的权重\",{\"1\":{\"306\":1}}],[\"继承了\",{\"1\":{\"212\":1}}],[\"官方代码链接\",{\"1\":{\"663\":1}}],[\"官方代码实现进行\",{\"1\":{\"663\":1}}],[\"官方代码实现\",{\"0\":{\"663\":1}}],[\"官方代码库并没有非常清晰指明capfilt模块的实现代码位置\",{\"1\":{\"185\":1}}],[\"官方仓库\",{\"1\":{\"557\":1}}],[\"官方实现的default\",{\"1\":{\"424\":1}}],[\"官方没有开源预训练阶段代码\",{\"1\":{\"225\":1}}],[\"官方文档中的行为规范\",{\"1\":{\"208\":1}}],[\"≠\",{\"1\":{\"208\":1,\"658\":1}}],[\"蒸馏还能用于将软伪标签传播到无标签数据\",{\"1\":{\"283\":1}}],[\"蒸馏损失\",{\"1\":{\"208\":2}}],[\"蒸馏损失的权重\",{\"1\":{\"208\":1}}],[\"蒸馏\",{\"1\":{\"208\":2}}],[\"剩余部分为图像\",{\"1\":{\"380\":1}}],[\"剩余的图片都不是同一个类别\",{\"1\":{\"350\":1}}],[\"剩余的语言\",{\"1\":{\"67\":1}}],[\"剩下的是失败\",{\"1\":{\"860\":1}}],[\"剩下的\",{\"1\":{\"208\":1}}],[\"~null\",{\"1\":{\"893\":1}}],[\"~31k\",{\"1\":{\"656\":1}}],[\"~33k\",{\"1\":{\"656\":1}}],[\"~13k\",{\"1\":{\"656\":1}}],[\"~$\",{\"1\":{\"551\":1}}],[\"~\",{\"1\":{\"483\":1,\"558\":1,\"589\":2,\"710\":2,\"784\":1,\"815\":1,\"899\":1,\"935\":2}}],[\"~mask\",{\"1\":{\"380\":1}}],[\"~masked\",{\"1\":{\"208\":1}}],[\"~indices\",{\"1\":{\"208\":1}}],[\"~pred\",{\"1\":{\"107\":1}}],[\"值更有帮助\",{\"1\":{\"885\":1}}],[\"值得大力推广\",{\"1\":{\"826\":1}}],[\"值得注意的是\",{\"1\":{\"223\":1,\"272\":1,\"285\":1,\"545\":1}}],[\"值的更新过程\",{\"1\":{\"816\":1}}],[\"值增加最快的方向是\",{\"1\":{\"816\":1}}],[\"值加1\",{\"1\":{\"805\":1}}],[\"值是在前向传播中算的\",{\"1\":{\"959\":1}}],[\"值是整数\",{\"1\":{\"710\":1}}],[\"值是\",{\"1\":{\"710\":1}}],[\"值域范围是\",{\"1\":{\"709\":1}}],[\"值域为\",{\"1\":{\"213\":1}}],[\"值变大并不能提升微调的效果\",{\"1\":{\"613\":1}}],[\"值越大\",{\"1\":{\"589\":1}}],[\"值越小表示匹配越好\",{\"1\":{\"586\":1}}],[\"值\",{\"1\":{\"502\":1,\"544\":1,\"588\":1,\"589\":1,\"779\":1,\"896\":1,\"898\":1,\"945\":2}}],[\"值固定为\",{\"1\":{\"385\":1}}],[\"值为0\",{\"1\":{\"893\":1}}],[\"值为出现次数\",{\"1\":{\"516\":1}}],[\"值为\",{\"1\":{\"208\":1,\"592\":1}}],[\"值向量\",{\"1\":{\"119\":1,\"529\":1}}],[\"克隆一份\",{\"1\":{\"208\":1}}],[\"固定p\",{\"1\":{\"881\":1}}],[\"固定为\",{\"1\":{\"293\":1}}],[\"固定\",{\"1\":{\"283\":1,\"709\":1,\"885\":1,\"898\":1,\"949\":1}}],[\"固定作为\",{\"1\":{\"207\":1}}],[\"固定返回问题0\",{\"1\":{\"92\":1}}],[\"共用逻辑\",{\"1\":{\"809\":1}}],[\"共用\",{\"1\":{\"710\":1}}],[\"共分为\",{\"1\":{\"694\":1}}],[\"共273个样例\",{\"1\":{\"641\":1}}],[\"共同支撑了零样本泛化能力\",{\"1\":{\"640\":1}}],[\"共指\",{\"1\":{\"634\":1}}],[\"共个值\",{\"1\":{\"526\":1}}],[\"共5个类别\",{\"1\":{\"424\":1}}],[\"共\",{\"1\":{\"305\":1,\"679\":1}}],[\"共有\",{\"1\":{\"207\":1}}],[\"共享变量与梯度累加\",{\"0\":{\"803\":1}}],[\"共享同一桶\",{\"1\":{\"710\":1}}],[\"共享权重\",{\"1\":{\"699\":1}}],[\"共享内存\",{\"1\":{\"469\":1}}],[\"共享自注意力\",{\"1\":{\"376\":1}}],[\"共享自注意力机制的消融实验\",{\"1\":{\"376\":1}}],[\"共享自注意力模块能够帮助\",{\"1\":{\"376\":1}}],[\"共享自注意力模块\",{\"1\":{\"224\":1}}],[\"共享计算图\",{\"1\":{\"268\":1}}],[\"共享的自注意力模块也对模型有积极贡献\",{\"1\":{\"376\":1}}],[\"共享的图像增强\",{\"1\":{\"264\":1}}],[\"共享的批归一化和激活函数\",{\"1\":{\"121\":1}}],[\"共享嵌入空间\",{\"1\":{\"205\":2}}],[\"共享除了\",{\"1\":{\"171\":1}}],[\"屏蔽中心行上中心右边的列\",{\"1\":{\"926\":1}}],[\"屏蔽中心行上中心及其右侧的列\",{\"1\":{\"926\":1}}],[\"屏蔽中心行以下所有行\",{\"1\":{\"926\":2}}],[\"屏蔽中心像素\",{\"1\":{\"926\":1}}],[\"屏蔽非法的预测位置\",{\"1\":{\"898\":1}}],[\"屏蔽不合法预测\",{\"1\":{\"893\":1}}],[\"屏蔽不可\",{\"1\":{\"208\":1}}],[\"屏蔽\",{\"1\":{\"892\":2}}],[\"屏蔽填充部分的信息\",{\"1\":{\"741\":1}}],[\"屏蔽对角线\",{\"1\":{\"207\":1}}],[\"屏蔽掉\",{\"1\":{\"187\":1}}],[\"判别器代码实现\",{\"1\":{\"918\":1}}],[\"判别器想最大化识别真假的概率\",{\"1\":{\"918\":1}}],[\"判别器给输入\",{\"1\":{\"918\":1}}],[\"判别器力求识别真假样本\",{\"1\":{\"918\":1}}],[\"判别器\",{\"1\":{\"918\":3}}],[\"判别性和稳定性\",{\"1\":{\"206\":1}}],[\"判定其是否语法正确\",{\"1\":{\"634\":1}}],[\"判断\",{\"1\":{\"800\":1}}],[\"判断哪些是短距离\",{\"1\":{\"710\":1}}],[\"判断下一个句子是否是当前句子的后续句子\",{\"1\":{\"699\":1}}],[\"判断第\",{\"1\":{\"690\":1}}],[\"判断两个句子是否连续\",{\"1\":{\"683\":1}}],[\"判断两个片段是否连续\",{\"1\":{\"679\":1}}],[\"判断它们之间的关系\",{\"1\":{\"634\":1}}],[\"判断每个字符对是否存在于频次表中\",{\"1\":{\"596\":1}}],[\"判断连续性的条件\",{\"1\":{\"489\":1}}],[\"判断预测是否正确\",{\"1\":{\"410\":1}}],[\"判断是否为交叉注意力\",{\"1\":{\"420\":1}}],[\"判断是否为\",{\"1\":{\"401\":1}}],[\"判断是否匹配\",{\"1\":{\"173\":1}}],[\"判断者\",{\"1\":{\"343\":1}}],[\"判断当前张量的内存布局是否是连续的\",{\"1\":{\"490\":1}}],[\"判断当前是训练阶段还是验证阶段\",{\"1\":{\"384\":1}}],[\"判断当前遮挡块的有效新增遮挡数量\",{\"1\":{\"263\":1}}],[\"判断当前图文对与队列中哪些样本是正对\",{\"1\":{\"190\":1}}],[\"判断图文是否匹配\",{\"1\":{\"172\":1,\"190\":1}}],[\"判断模型是否能正确区分前景和背景\",{\"1\":{\"106\":1}}],[\"除此之外\",{\"1\":{\"923\":1}}],[\"除此之外的都是噪声样本\",{\"1\":{\"355\":1}}],[\"除此之外的特征都是使用之前的编码器得到的\",{\"1\":{\"353\":1}}],[\"除boolq和winogrande\",{\"1\":{\"668\":1}}],[\"除以温度\",{\"1\":{\"897\":1}}],[\"除以后\",{\"1\":{\"893\":1}}],[\"除以\",{\"1\":{\"893\":1}}],[\"除以可以缓解这个问题\",{\"1\":{\"537\":1}}],[\"除以该\",{\"1\":{\"213\":1}}],[\"除以该簇的样本数\",{\"1\":{\"213\":1}}],[\"除非特别说明\",{\"1\":{\"889\":1}}],[\"除非指定\",{\"1\":{\"633\":1}}],[\"除非通过\",{\"1\":{\"486\":1}}],[\"除非增加额外的归一化\",{\"1\":{\"289\":1}}],[\"除head\",{\"1\":{\"435\":1}}],[\"除了能直接计算图像的概率外\",{\"1\":{\"925\":1}}],[\"除了最后一个\",{\"1\":{\"420\":1}}],[\"除了模型本身的应用\",{\"1\":{\"409\":1}}],[\"除了个体判别这个任务外\",{\"1\":{\"354\":1}}],[\"除了图像编码器\",{\"1\":{\"271\":1}}],[\"除法运算y\",{\"1\":{\"809\":1}}],[\"除法运算\",{\"1\":{\"809\":1}}],[\"除法\",{\"1\":{\"205\":1}}],[\"除外\",{\"1\":{\"24\":1,\"889\":1}}],[\"配对\",{\"1\":{\"709\":1}}],[\"配对编码器用于同步参数\",{\"1\":{\"190\":1}}],[\"配以自然语言任务说明进行微调\",{\"1\":{\"655\":1}}],[\"配置控制等\",{\"1\":{\"799\":1}}],[\"配置字典\",{\"1\":{\"382\":2}}],[\"配置\",{\"1\":{\"205\":1,\"381\":1,\"382\":1,\"520\":1}}],[\"恒定为最大值\",{\"1\":{\"204\":1}}],[\"恒等映射变换\",{\"1\":{\"122\":1}}],[\"恒等映射\",{\"1\":{\"120\":1}}],[\"步里被随机遮盖或替换的部分\",{\"1\":{\"691\":1}}],[\"步进\",{\"1\":{\"546\":1}}],[\"步长解决的是如何将多维张量中的\",{\"1\":{\"542\":1}}],[\"步长\",{\"0\":{\"542\":1},\"1\":{\"542\":1,\"544\":2}}],[\"步距为16\",{\"1\":{\"426\":1}}],[\"步热身\",{\"1\":{\"316\":1}}],[\"步骤9\",{\"0\":{\"789\":1}}],[\"步骤8\",{\"0\":{\"785\":1}}],[\"步骤7\",{\"0\":{\"782\":1}}],[\"步骤6\",{\"0\":{\"777\":1}}],[\"步骤5\",{\"0\":{\"773\":1}}],[\"步骤4\",{\"0\":{\"768\":1}}],[\"步骤如下\",{\"1\":{\"582\":1}}],[\"步骤包括\",{\"1\":{\"380\":1}}],[\"步骤3\",{\"0\":{\"764\":1},\"1\":{\"339\":1}}],[\"步骤27\",{\"0\":{\"817\":1}}],[\"步骤26\",{\"0\":{\"816\":1}}],[\"步骤25\",{\"0\":{\"815\":1}}],[\"步骤24\",{\"0\":{\"811\":1}}],[\"步骤23\",{\"0\":{\"810\":1}}],[\"步骤20\",{\"0\":{\"809\":1}}],[\"步骤2\",{\"0\":{\"759\":1},\"1\":{\"339\":1}}],[\"步骤19\",{\"0\":{\"808\":1}}],[\"步骤18\",{\"0\":{\"807\":1}}],[\"步骤17\",{\"0\":{\"806\":1}}],[\"步骤16\",{\"0\":{\"805\":1}}],[\"步骤15\",{\"0\":{\"804\":1}}],[\"步骤14\",{\"0\":{\"803\":1}}],[\"步骤13\",{\"0\":{\"802\":1}}],[\"步骤12\",{\"0\":{\"801\":1}}],[\"步骤11\",{\"0\":{\"800\":1}}],[\"步骤10\",{\"0\":{\"793\":1}}],[\"步骤1\",{\"0\":{\"754\":1},\"1\":{\"339\":1}}],[\"步骤\",{\"1\":{\"293\":1,\"963\":5}}],[\"步\",{\"1\":{\"204\":1,\"224\":1,\"315\":1,\"316\":1,\"679\":2}}],[\"监督学习微调\",{\"1\":{\"654\":1}}],[\"监督式微调sft\",{\"1\":{\"602\":1}}],[\"监督式预训练\",{\"1\":{\"280\":1}}],[\"监督微调\",{\"0\":{\"634\":1},\"1\":{\"305\":1,\"656\":1}}],[\"监督训练也能带来一致提升\",{\"1\":{\"215\":1}}],[\"监督训练的\",{\"1\":{\"215\":1}}],[\"监督\",{\"1\":{\"208\":2,\"280\":1}}],[\"监督为主\",{\"1\":{\"204\":1}}],[\"监督信号不理想\",{\"1\":{\"165\":1}}],[\"捕捉不同子空间的信息\",{\"1\":{\"741\":1}}],[\"捕捉文本中位置相关的语义和结构信息\",{\"1\":{\"707\":1}}],[\"捕捉全局信息\",{\"1\":{\"434\":1}}],[\"捕捉与图像内容相关的多种可能性\",{\"1\":{\"202\":1}}],[\"捕获模态特定信息\",{\"1\":{\"368\":1}}],[\"捕获密集到稀疏采样区域内的多尺度信息\",{\"1\":{\"141\":1}}],[\"散度是常量\",{\"1\":{\"959\":1}}],[\"散度的\",{\"1\":{\"950\":1}}],[\"散度的单位就是比特\",{\"1\":{\"950\":1}}],[\"散度的期望中\",{\"1\":{\"945\":1}}],[\"散度的定义开始\",{\"1\":{\"945\":1}}],[\"散度时\",{\"1\":{\"931\":1}}],[\"散度就是交叉熵\",{\"1\":{\"909\":1}}],[\"散度部分\",{\"1\":{\"899\":1}}],[\"散度约束后\",{\"1\":{\"262\":1}}],[\"散度损失的权重\",{\"1\":{\"899\":1}}],[\"散度损失\",{\"1\":{\"256\":1,\"260\":1}}],[\"散度正则项的权重\",{\"1\":{\"255\":1}}],[\"散度为常数\",{\"1\":{\"235\":1}}],[\"散度项不用算入损失函数里\",{\"1\":{\"959\":1}}],[\"散度项会鼓励\",{\"1\":{\"945\":1}}],[\"散度项让\",{\"1\":{\"935\":1}}],[\"散度项的损失权重\",{\"1\":{\"255\":1}}],[\"散度项\",{\"1\":{\"235\":1,\"932\":1,\"951\":1}}],[\"散度\",{\"1\":{\"202\":1,\"208\":1,\"260\":1,\"909\":1,\"945\":3,\"946\":1,\"947\":2,\"949\":1}}],[\"老师模型\",{\"1\":{\"202\":1}}],[\"起主导作用\",{\"1\":{\"587\":1}}],[\"起始偏移量\",{\"1\":{\"544\":1}}],[\"起始值\",{\"1\":{\"440\":1}}],[\"起到了\",{\"1\":{\"577\":1}}],[\"起到\",{\"1\":{\"202\":1}}],[\"起来\",{\"1\":{\"57\":1}}],[\"硬币正面概率的\",{\"1\":{\"904\":1}}],[\"硬标签分配\",{\"1\":{\"283\":1}}],[\"硬标签混合\",{\"1\":{\"192\":1}}],[\"硬采样但梯度可导\",{\"1\":{\"255\":1}}],[\"硬负样本采样策略\",{\"1\":{\"201\":1}}],[\"令离散编码\",{\"1\":{\"959\":1}}],[\"令模型的输入为整幅图像和序号\",{\"1\":{\"921\":1}}],[\"令模型忽略输入的条件\",{\"1\":{\"894\":1}}],[\"令\",{\"1\":{\"199\":1,\"951\":3}}],[\"启发式过滤低质量网页\",{\"1\":{\"667\":1}}],[\"启发了\",{\"1\":{\"250\":1}}],[\"启发\",{\"1\":{\"199\":1}}],[\"启用反向传播\",{\"1\":{\"807\":1}}],[\"启用采样\",{\"1\":{\"188\":1}}],[\"启用\",{\"1\":{\"172\":3}}],[\"受到\",{\"1\":{\"372\":1}}],[\"受ureader启发\",{\"1\":{\"331\":1}}],[\"受\",{\"1\":{\"199\":1,\"252\":1}}],[\"受限于瓶颈维度\",{\"1\":{\"157\":1}}],[\"虽对部分任务性能略有影响\",{\"1\":{\"683\":1}}],[\"虽在图文检索中效果出色\",{\"1\":{\"195\":1}}],[\"虽然在实际中这样的小\",{\"1\":{\"949\":1}}],[\"虽然在阅读理解等任务上接近监督基线\",{\"1\":{\"642\":1}}],[\"虽然反向传播可以应对带噪输入\",{\"1\":{\"946\":1}}],[\"虽然可以尝试用更好的相似性度量代替欧氏距离\",{\"1\":{\"944\":1}}],[\"虽然我们现在能把编码器和解码器拼接到一起\",{\"1\":{\"958\":1}}],[\"虽然我们只能看到最终生成的字符\",{\"1\":{\"943\":1}}],[\"虽然我们通过使用大模型来简化了业务逻辑的拆解\",{\"1\":{\"836\":1}}],[\"虽然密度在下降\",{\"1\":{\"873\":1}}],[\"虽然密度函数以\",{\"1\":{\"873\":1}}],[\"虽然你可能觉得\",{\"1\":{\"850\":1}}],[\"虽然大模型是深度学习领域的集大成之作\",{\"1\":{\"835\":1}}],[\"虽然大模型训练更快达到目标性能\",{\"1\":{\"666\":1}}],[\"虽然之前的聊天机器人存在各种问题\",{\"1\":{\"827\":1}}],[\"虽然python的垃圾回收\",{\"1\":{\"806\":1}}],[\"虽然位置和语义在所有维度上混合\",{\"1\":{\"706\":1}}],[\"虽然正弦余弦位置编码能够隐含地表达一定的局部位置信息\",{\"1\":{\"706\":1}}],[\"虽然早期实验显示性能略有下降\",{\"1\":{\"681\":1}}],[\"虽然也不完全正确\",{\"1\":{\"657\":1}}],[\"虽然具备强大的自然语言处理能力\",{\"1\":{\"654\":1}}],[\"虽然gpt\",{\"1\":{\"649\":1}}],[\"虽然深度减少了神经元数量\",{\"1\":{\"500\":1}}],[\"虽然有时不调用也能工作\",{\"1\":{\"494\":1}}],[\"虽然某些实现能处理不连续输入\",{\"1\":{\"493\":1}}],[\"虽然没变\",{\"1\":{\"472\":1}}],[\"虽然推理时间快\",{\"1\":{\"394\":1}}],[\"虽然还不能执行复杂的推理任务\",{\"1\":{\"341\":1}}],[\"虽然理论上有\",{\"1\":{\"262\":1}}],[\"虽然规模大\",{\"1\":{\"167\":1}}],[\"虽然\",{\"1\":{\"157\":3,\"286\":1,\"500\":1,\"588\":1,\"944\":1}}],[\"虽然这些方法提升了生成效果\",{\"1\":{\"884\":1}}],[\"虽然这种方式需要花费时间和内存来完成拷贝\",{\"1\":{\"545\":1}}],[\"虽然这个点没有实际意义\",{\"1\":{\"137\":1}}],[\"虽然这里用的是\",{\"1\":{\"102\":1}}],[\"促进硬负样本挖掘\",{\"1\":{\"194\":1}}],[\"匹配或超过3\",{\"1\":{\"641\":1}}],[\"匹配更复杂的表示\",{\"1\":{\"282\":1}}],[\"匹配\",{\"1\":{\"207\":1,\"640\":1}}],[\"匹配or不匹配\",{\"1\":{\"192\":1}}],[\"匹配和生成等任务目标进行联合训练\",{\"1\":{\"174\":1}}],[\"队列大小\",{\"1\":{\"361\":1}}],[\"队列里的样本不需要进行梯度回传\",{\"1\":{\"352\":1}}],[\"队列\",{\"1\":{\"192\":2,\"474\":1}}],[\"队列指针\",{\"1\":{\"192\":1,\"361\":1}}],[\"队列用于\",{\"1\":{\"192\":1}}],[\"滑动平均更新\",{\"1\":{\"192\":1}}],[\"便得到了模态对齐好的图像编码器vit\",{\"1\":{\"191\":1}}],[\"便于通过梯度下降逐渐调整参数\",{\"1\":{\"943\":1}}],[\"便于识别变量类型\",{\"1\":{\"808\":1}}],[\"便于在训练和推理阶段灵活切换\",{\"1\":{\"807\":1}}],[\"便于直接调用\",{\"1\":{\"790\":1}}],[\"便于从\",{\"1\":{\"699\":1}}],[\"便于跨数据集评估\",{\"1\":{\"640\":1}}],[\"便于采样\",{\"1\":{\"263\":1}}],[\"便于\",{\"1\":{\"258\":1}}],[\"便于之后相似度计算\",{\"1\":{\"205\":1}}],[\"便于计算归一化相似度\",{\"1\":{\"192\":1}}],[\"便于广播到整个\",{\"1\":{\"153\":1}}],[\"便于广播\",{\"1\":{\"137\":1}}],[\"便于卷积操作\",{\"1\":{\"110\":1}}],[\"便于后续处理\",{\"1\":{\"152\":1}}],[\"便于后续计算\",{\"1\":{\"145\":1,\"586\":1,\"587\":1,\"590\":1,\"592\":1}}],[\"便于后续统一评估\",{\"1\":{\"106\":1}}],[\"便于后续\",{\"1\":{\"69\":1}}],[\"环境搭建遵从如下步骤即可\",{\"1\":{\"739\":1}}],[\"环境搭建\",{\"0\":{\"712\":1}}],[\"环境中的\",{\"1\":{\"557\":1}}],[\"环境中使用\",{\"1\":{\"557\":1}}],[\"环境再使用\",{\"1\":{\"557\":1}}],[\"环境\",{\"0\":{\"551\":1,\"739\":1},\"1\":{\"190\":1,\"339\":1,\"386\":1,\"552\":1,\"553\":1}}],[\"环境配置\",{\"0\":{\"62\":1}}],[\"束搜索\",{\"1\":{\"188\":1}}],[\"核采样参数\",{\"1\":{\"421\":1}}],[\"核采样时不扩展beam\",{\"1\":{\"421\":1}}],[\"核采样的概率阈值\",{\"1\":{\"421\":1}}],[\"核采样\",{\"1\":{\"188\":1}}],[\"核心价值\",{\"1\":{\"836\":1}}],[\"核心组件\",{\"0\":{\"832\":1},\"1\":{\"834\":1}}],[\"核心组件为\",{\"1\":{\"368\":1}}],[\"核心函数实现原理\",{\"1\":{\"815\":1}}],[\"核心功能\",{\"1\":{\"810\":1}}],[\"核心理解\",{\"1\":{\"710\":1}}],[\"核心原因在于层次化的函数构造方式比单层网络的线性组合更高效\",{\"1\":{\"500\":1}}],[\"核心原则\",{\"1\":{\"472\":1}}],[\"核心要点\",{\"1\":{\"426\":1}}],[\"核心计算\",{\"0\":{\"401\":1}}],[\"核心思想是通过两个神经网络之间的\",{\"1\":{\"918\":1}}],[\"核心思想\",{\"1\":{\"346\":1,\"589\":1,\"592\":1}}],[\"核心贡献如下\",{\"1\":{\"220\":1}}],[\"核心问题\",{\"1\":{\"157\":1}}],[\"核心操作包括\",{\"1\":{\"143\":1}}],[\"核心设计理念\",{\"1\":{\"119\":1}}],[\"核心\",{\"0\":{\"148\":1},\"1\":{\"115\":1,\"120\":1}}],[\"核心挑战\",{\"1\":{\"73\":1}}],[\"核心代码实现如下\",{\"1\":{\"52\":1,\"264\":1,\"699\":1}}],[\"核心目标\",{\"1\":{\"23\":1}}],[\"份\",{\"1\":{\"188\":1}}],[\"清华大学和智谱\",{\"1\":{\"823\":1}}],[\"清晰了预训练模型的作用\",{\"1\":{\"826\":1}}],[\"清晰\",{\"1\":{\"814\":1}}],[\"清除中间变量的导数\",{\"1\":{\"807\":1}}],[\"清除旧梯度\",{\"1\":{\"187\":1}}],[\"清洗原始\",{\"1\":{\"633\":1}}],[\"清洗\",{\"1\":{\"30\":1}}],[\"剔除\",{\"1\":{\"179\":1}}],[\"剔除原始和生成的低质量描述\",{\"1\":{\"165\":1}}],[\"阶段二\",{\"0\":{\"887\":1},\"1\":{\"885\":1}}],[\"阶段一\",{\"0\":{\"886\":1},\"1\":{\"885\":1}}],[\"阶段只需\",{\"1\":{\"658\":1}}],[\"阶段式预训练能够缓解对大量图文对数据的依赖\",{\"1\":{\"376\":1}}],[\"阶段式预训练能够有效利用大规模图像语料与文本语料\",{\"1\":{\"376\":1}}],[\"阶段式预训练\",{\"1\":{\"376\":1}}],[\"阶段\",{\"1\":{\"179\":1,\"885\":2}}],[\"双线性\",{\"0\":{\"505\":1},\"1\":{\"505\":1}}],[\"双线性插值是一种常用的图像放大\",{\"1\":{\"505\":1}}],[\"双线性插值\",{\"1\":{\"502\":2}}],[\"双向对比损失\",{\"1\":{\"900\":1}}],[\"双向自注意力\",{\"1\":{\"419\":1}}],[\"双向注意力\",{\"1\":{\"179\":1}}],[\"双语数据集\",{\"1\":{\"332\":1}}],[\"双语数据集和持续学习的视觉编码器\",{\"1\":{\"327\":1}}],[\"双语描述\",{\"1\":{\"332\":1}}],[\"双语能力\",{\"1\":{\"323\":1}}],[\"双分支视觉编码器\",{\"1\":{\"326\":1}}],[\"双分支架构\",{\"1\":{\"19\":1}}],[\"双塔模型\",{\"1\":{\"303\":1}}],[\"双视图表示\",{\"1\":{\"228\":1}}],[\"双编码器架构\",{\"1\":{\"368\":1}}],[\"双编码器设计\",{\"1\":{\"327\":1}}],[\"双编码器方法还学习了对齐的文本编码器\",{\"1\":{\"271\":1}}],[\"双编码器方法利用大规模噪声文本描述\",{\"1\":{\"271\":1}}],[\"双编码器对比学习\",{\"1\":{\"270\":1,\"271\":1}}],[\"双编码器\",{\"1\":{\"220\":1,\"222\":1,\"268\":1,\"369\":1,\"377\":1}}],[\"新信息\",{\"1\":{\"906\":1}}],[\"新闻数据\",{\"1\":{\"680\":1}}],[\"新词使用\",{\"1\":{\"648\":1}}],[\"新加入模块约为\",{\"1\":{\"306\":1}}],[\"新增名称属性\",{\"1\":{\"808\":1}}],[\"新增的可学习查询和交叉注意力层\",{\"1\":{\"316\":1}}],[\"新增模块\",{\"1\":{\"306\":1}}],[\"新增了96个可学习查询和交叉注意力层\",{\"1\":{\"304\":1}}],[\"新增96个可学习查询和交叉注意力层\",{\"1\":{\"303\":1}}],[\"新颖性与多样性\",{\"1\":{\"178\":1}}],[\"新的文本\",{\"1\":{\"735\":1}}],[\"新的问题出现了\",{\"1\":{\"413\":1}}],[\"新的\",{\"1\":{\"123\":1,\"213\":1}}],[\"新的鲁棒性基准\",{\"1\":{\"19\":1}}],[\"确实存在一个正则化参数\",{\"1\":{\"951\":1}}],[\"确实像openai的gpt4这样的llm已经非常强了\",{\"1\":{\"616\":1}}],[\"确立自回归范式\",{\"1\":{\"671\":1}}],[\"确信\",{\"1\":{\"657\":1}}],[\"确认你使用的是\",{\"1\":{\"557\":1}}],[\"确认偏差\",{\"1\":{\"179\":1}}],[\"确定事件\",{\"1\":{\"907\":1}}],[\"确定目标\",{\"1\":{\"836\":1}}],[\"确定缩放因子\",{\"1\":{\"430\":1}}],[\"确定是否使用\",{\"1\":{\"384\":1}}],[\"确定性搜索\",{\"1\":{\"178\":1}}],[\"确保方差\",{\"1\":{\"931\":1}}],[\"确保模型每个像素的输出\",{\"1\":{\"926\":1}}],[\"确保在生成文本的阶段\",{\"1\":{\"898\":1}}],[\"确保在单台机器上\",{\"1\":{\"696\":1}}],[\"确保至少选一个\",{\"1\":{\"896\":1}}],[\"确保一致性\",{\"1\":{\"895\":1}}],[\"确保图像尺寸正确\",{\"1\":{\"893\":1}}],[\"确保图像尺寸能被\",{\"1\":{\"582\":1}}],[\"确保它们在部署前达到预期的性能和稳定性标准\",{\"1\":{\"834\":1}}],[\"确保它们加起来是1\",{\"1\":{\"145\":1}}],[\"确保所有利用\",{\"1\":{\"833\":1}}],[\"确保开发者能够在升级过程中无缝过渡\",{\"1\":{\"833\":1}}],[\"确保信息的持续更新和准确性\",{\"1\":{\"828\":1}}],[\"确保输出内容的精确性和可信度\",{\"1\":{\"828\":1}}],[\"确保输入\",{\"1\":{\"122\":1}}],[\"确保输入张量是内存连续的\",{\"1\":{\"121\":1}}],[\"确保复杂计算图反向传播顺序正确\",{\"1\":{\"812\":1}}],[\"确保类型转换优先\",{\"1\":{\"809\":1}}],[\"确保运算正常进行\",{\"1\":{\"809\":1}}],[\"确保代码质量\",{\"1\":{\"797\":1}}],[\"确保代码功能正确性\",{\"1\":{\"796\":1}}],[\"确保variable只接受ndarray实例\",{\"1\":{\"792\":1}}],[\"确保映射是一一对应的\",{\"1\":{\"697\":1}}],[\"确保句子数为偶数\",{\"1\":{\"696\":1}}],[\"确保内容经过人工筛选\",{\"1\":{\"640\":1}}],[\"确保可以整除\",{\"1\":{\"582\":1}}],[\"确保每次运行生成相同的随机数\",{\"1\":{\"521\":1}}],[\"确保每个\",{\"1\":{\"100\":1}}],[\"确保返回一个连续的张量\",{\"1\":{\"494\":1}}],[\"确保稳定性和计算效率\",{\"1\":{\"493\":1}}],[\"确保卷积操作不会重叠\",{\"1\":{\"426\":1}}],[\"确保\",{\"1\":{\"382\":1,\"833\":1,\"949\":1}}],[\"确保右边编码器更新得非常缓慢\",{\"1\":{\"353\":1}}],[\"确保视觉编码器与llms的特征空间一致\",{\"1\":{\"296\":1}}],[\"确保遮挡块不会越界\",{\"1\":{\"263\":1}}],[\"确保遮挡块可放入图像范围内\",{\"1\":{\"263\":1}}],[\"确保索引在合法范围内\",{\"1\":{\"122\":1}}],[\"确保当前batch有数据需要处理\",{\"1\":{\"119\":1}}],[\"确保推理一致性\",{\"1\":{\"90\":1}}],[\"确保知识有效迁移\",{\"1\":{\"19\":1}}],[\"合理输出的空间是多模态的\",{\"1\":{\"952\":1}}],[\"合理的样本\",{\"1\":{\"947\":1}}],[\"合理的内存管理至关重要\",{\"1\":{\"806\":1}}],[\"合理事件空间\",{\"1\":{\"847\":1}}],[\"合计40\",{\"1\":{\"647\":1}}],[\"合适的\",{\"1\":{\"252\":1}}],[\"合成文本的多样性对性能的影响\",{\"0\":{\"178\":1}}],[\"合并后\",{\"1\":{\"893\":1}}],[\"合并头结果\",{\"1\":{\"724\":1}}],[\"合并并归一化\",{\"1\":{\"709\":1}}],[\"合并的\",{\"1\":{\"633\":1}}],[\"合并参数\",{\"1\":{\"609\":1}}],[\"合并该字符对\",{\"1\":{\"596\":1}}],[\"合并当前最高频的字符对\",{\"1\":{\"595\":3}}],[\"合并前n个最频繁的字符对\",{\"1\":{\"595\":1}}],[\"合并正负样本特征\",{\"1\":{\"386\":1}}],[\"合并正负样本对\",{\"1\":{\"192\":1}}],[\"合并收集的\",{\"1\":{\"386\":1}}],[\"合并各个子数据集\",{\"1\":{\"382\":1}}],[\"合并回一个序列\",{\"1\":{\"380\":1}}],[\"合并所有批次\",{\"1\":{\"122\":1}}],[\"合并\",{\"1\":{\"102\":1,\"609\":1}}],[\"性价比最高\",{\"1\":{\"823\":1}}],[\"性价比高\",{\"1\":{\"823\":1}}],[\"性别化代词\",{\"1\":{\"670\":1}}],[\"性别偏见\",{\"1\":{\"655\":1}}],[\"性质的函数是指像sigmoid函数的有界函数\",{\"1\":{\"500\":1}}],[\"性质的激活函数的隐藏层组成的前馈神经网络\",{\"1\":{\"500\":1}}],[\"性\",{\"1\":{\"177\":1}}],[\"性能得到进一步提升\",{\"1\":{\"823\":1}}],[\"性能对标\",{\"1\":{\"823\":1}}],[\"性能强\",{\"1\":{\"823\":1}}],[\"性能强大\",{\"1\":{\"823\":1}}],[\"性能有了进一步提升\",{\"1\":{\"823\":1}}],[\"性能有所提升\",{\"1\":{\"117\":1}}],[\"性能开销较高\",{\"1\":{\"806\":1}}],[\"性能略优于full\",{\"1\":{\"681\":1}}],[\"性能优于原始bert\",{\"1\":{\"681\":1}}],[\"性能优于纯\",{\"1\":{\"117\":1}}],[\"性能仍优于gpt\",{\"1\":{\"668\":1}}],[\"性能好\",{\"1\":{\"658\":1}}],[\"性能提升非源于记忆\",{\"1\":{\"641\":1}}],[\"性能测试\",{\"1\":{\"450\":1,\"461\":1}}],[\"性能不够高\",{\"1\":{\"394\":1}}],[\"性能验证与社会责任\",{\"1\":{\"666\":1}}],[\"性能验证\",{\"1\":{\"323\":1}}],[\"性能逐步提升\",{\"1\":{\"288\":1}}],[\"性能表现\",{\"1\":{\"280\":1}}],[\"性能和传统的单编码器分类损失模型差不多\",{\"1\":{\"276\":1}}],[\"性能也有所下降\",{\"1\":{\"242\":1}}],[\"性能\",{\"1\":{\"183\":1,\"215\":1,\"415\":1,\"641\":1,\"657\":2,\"668\":1}}],[\"性能反而下降\",{\"1\":{\"179\":1}}],[\"性能次优\",{\"1\":{\"179\":1}}],[\"性能最强\",{\"1\":{\"823\":1}}],[\"性能最优\",{\"1\":{\"117\":1}}],[\"性能最差\",{\"1\":{\"117\":1}}],[\"性能最佳\",{\"1\":{\"117\":1,\"647\":1}}],[\"性能下降\",{\"1\":{\"117\":1,\"681\":1}}],[\"性能明显下降\",{\"1\":{\"117\":1}}],[\"性能显著提升至\",{\"1\":{\"117\":1}}],[\"且对应的\",{\"1\":{\"949\":1}}],[\"且对多功能的相似结构易产生混淆\",{\"1\":{\"73\":1}}],[\"且满足对称性\",{\"1\":{\"912\":1}}],[\"且当\",{\"1\":{\"909\":1}}],[\"且独立同分布\",{\"1\":{\"904\":1}}],[\"且非空\",{\"1\":{\"893\":1}}],[\"且文本词空间和图像离散视觉词空间也通过视觉词索引偏移的方式完成了统一\",{\"1\":{\"892\":1}}],[\"且通过自然语言实现控制\",{\"1\":{\"884\":1}}],[\"且第\",{\"1\":{\"860\":1}}],[\"且\",{\"1\":{\"847\":1,\"849\":1,\"851\":1,\"864\":1}}],[\"且必须按顺序处理内容\",{\"1\":{\"828\":1}}],[\"且效率较低\",{\"1\":{\"786\":1}}],[\"且更高效\",{\"1\":{\"681\":1}}],[\"且偏好有偏\",{\"1\":{\"658\":1}}],[\"且在公开nlp任务上的性能损失极小\",{\"1\":{\"653\":1}}],[\"且在更具挑战性的winogrande数据集上few\",{\"1\":{\"648\":1}}],[\"且在大多数实验中表现稳定\",{\"1\":{\"286\":1}}],[\"且每个任务都需新数据\",{\"1\":{\"647\":1}}],[\"且预留给\",{\"1\":{\"610\":1}}],[\"且真实也为正类的样本数\",{\"1\":{\"590\":1}}],[\"且训练难度增加\",{\"1\":{\"500\":1}}],[\"且高次项易导致震荡\",{\"1\":{\"500\":1}}],[\"且没有提供\",{\"1\":{\"274\":1}}],[\"且需要在不同数据源\",{\"1\":{\"272\":1}}],[\"且容易聚焦低级细节而非语义结构\",{\"1\":{\"247\":1}}],[\"且能加速微调收敛\",{\"1\":{\"227\":1}}],[\"且不同模态之间的参数往往难以有效共享\",{\"1\":{\"220\":1}}],[\"且下游任务性能降低\",{\"1\":{\"215\":1}}],[\"且具备数据量和模型规模的可扩展性\",{\"1\":{\"177\":1}}],[\"遵循chung\",{\"1\":{\"669\":1}}],[\"遵循\",{\"1\":{\"371\":1}}],[\"遵循视觉\",{\"1\":{\"371\":1}}],[\"遵循常规做法\",{\"1\":{\"319\":1}}],[\"遵循标准的编码器\",{\"1\":{\"271\":1}}],[\"遵循您的格式规范\",{\"1\":{\"174\":1}}],[\"遵循一个\",{\"1\":{\"95\":1}}],[\"机器\",{\"1\":{\"950\":1}}],[\"机器上进行分布式训练\",{\"1\":{\"680\":1}}],[\"机器翻译实战\",{\"1\":{\"738\":1}}],[\"机器翻译\",{\"1\":{\"655\":1,\"687\":1}}],[\"机器人操作应用\",{\"1\":{\"75\":1}}],[\"机器人操作等任务中表现优异\",{\"1\":{\"31\":1}}],[\"机翻\",{\"1\":{\"626\":1}}],[\"机制的生成模型\",{\"1\":{\"965\":1}}],[\"机制可以处理循环引用对象\",{\"1\":{\"806\":1}}],[\"机制确保函数调用顺序的正确执行\",{\"1\":{\"805\":1}}],[\"机制生成新的视图\",{\"1\":{\"472\":1}}],[\"机制让每个\",{\"1\":{\"286\":1}}],[\"机制\",{\"0\":{\"805\":1},\"1\":{\"249\":1,\"650\":1,\"812\":1}}],[\"机制处理量化不可导问题\",{\"1\":{\"212\":1}}],[\"机制与重新初始化更契合\",{\"1\":{\"182\":1}}],[\"机制有效从网页图文对中挖掘高质量样本\",{\"1\":{\"174\":1}}],[\"统一词空间大小为9\",{\"1\":{\"892\":1}}],[\"统一转换为variable\",{\"1\":{\"809\":1}}],[\"统一处理逻辑\",{\"1\":{\"800\":1}}],[\"统一文本到文本框架\",{\"1\":{\"671\":1}}],[\"统一\",{\"1\":{\"510\":1}}],[\"统一接口\",{\"1\":{\"510\":1}}],[\"统一变为相同尺寸\",{\"1\":{\"501\":1}}],[\"统一多模态的可能性\",{\"1\":{\"436\":1}}],[\"统一形式的\",{\"1\":{\"380\":1}}],[\"统一训练框架的有效性\",{\"1\":{\"376\":1}}],[\"统一预训练\",{\"1\":{\"369\":1}}],[\"统一视觉\",{\"1\":{\"368\":1}}],[\"统一视觉预训练框架\",{\"1\":{\"252\":1}}],[\"统一起来\",{\"1\":{\"269\":1}}],[\"统一的预训练通过以下任务优化共享的\",{\"1\":{\"370\":1}}],[\"统一的预训练任务\",{\"1\":{\"220\":1}}],[\"统一的掩码数据建模\",{\"1\":{\"223\":1}}],[\"统一的骨干架构\",{\"1\":{\"220\":1}}],[\"统一的多任务架构\",{\"1\":{\"174\":1}}],[\"统一建模与数据增强的多模态预训练方法\",{\"1\":{\"174\":1}}],[\"统计冻结等\",{\"1\":{\"895\":1}}],[\"统计距离\",{\"1\":{\"579\":1}}],[\"统计函数运行时间\",{\"1\":{\"461\":1}}],[\"统计正确率\",{\"1\":{\"410\":1}}],[\"统计每个相邻字符对的出现次数\",{\"1\":{\"595\":1}}],[\"统计每个名词短语出现的频率\",{\"1\":{\"341\":1}}],[\"统计每个簇的样本数量\",{\"1\":{\"213\":2}}],[\"统计簇内所有样本\",{\"1\":{\"213\":1}}],[\"统计簇信息\",{\"1\":{\"213\":1}}],[\"统计量\",{\"1\":{\"160\":1}}],[\"统计分析\",{\"0\":{\"43\":1}}],[\"筛选测试\",{\"1\":{\"656\":1}}],[\"筛选高质量图像\",{\"1\":{\"305\":1}}],[\"筛选出的图文对与人工标注数据结合\",{\"1\":{\"173\":1}}],[\"筛选相关的几何特征\",{\"1\":{\"56\":1}}],[\"头部调优\",{\"1\":{\"320\":1}}],[\"头部处理\",{\"1\":{\"146\":1}}],[\"头维度和mlp比例\",{\"1\":{\"304\":1}}],[\"头数\",{\"1\":{\"303\":1,\"380\":1,\"892\":1,\"900\":2}}],[\"头共享\",{\"1\":{\"215\":1}}],[\"头参数在两处共享\",{\"1\":{\"214\":1}}],[\"头由一个全连接层构成\",{\"1\":{\"214\":1}}],[\"头\",{\"1\":{\"207\":1,\"293\":1}}],[\"头预测为不匹配\",{\"1\":{\"173\":1}}],[\"替换成\",{\"1\":{\"958\":1}}],[\"替换成以下内容\",{\"1\":{\"691\":1}}],[\"替换的是哪一个词\",{\"1\":{\"691\":1}}],[\"替换relu为swiglu\",{\"1\":{\"667\":1}}],[\"替换标准\",{\"1\":{\"368\":1}}],[\"替换被遮挡的位置\",{\"1\":{\"266\":1}}],[\"替换被遮挡的部分\",{\"1\":{\"266\":1}}],[\"替换这些被遮挡的\",{\"1\":{\"234\":1}}],[\"替换原始的\",{\"1\":{\"214\":1}}],[\"替换为唯一的\",{\"1\":{\"893\":1}}],[\"替换为伽马函数表达式\",{\"1\":{\"861\":1}}],[\"替换为随机单词\",{\"1\":{\"681\":1}}],[\"替换为随机\",{\"1\":{\"679\":1}}],[\"替换为\",{\"1\":{\"266\":1,\"373\":1,\"420\":2,\"679\":1,\"681\":1}}],[\"替换为特殊标记\",{\"1\":{\"200\":1}}],[\"替换为因果\",{\"1\":{\"171\":1}}],[\"替换\",{\"1\":{\"192\":2}}],[\"替代值\",{\"1\":{\"893\":1}}],[\"替代绝对位置编码\",{\"1\":{\"667\":1}}],[\"替代\",{\"1\":{\"97\":1}}],[\"位单通道图像\",{\"1\":{\"925\":1}}],[\"位标注者进行判断\",{\"1\":{\"658\":1}}],[\"位整数\",{\"1\":{\"614\":2}}],[\"位或\",{\"1\":{\"614\":2}}],[\"位浮点数\",{\"1\":{\"614\":1}}],[\"位\",{\"1\":{\"489\":3}}],[\"位于原点\",{\"1\":{\"872\":1}}],[\"位于序列的开头\",{\"1\":{\"427\":1}}],[\"位于\",{\"1\":{\"171\":1}}],[\"位置保留了唯一\",{\"1\":{\"892\":1}}],[\"位置处理方式并不完全确定\",{\"1\":{\"887\":1}}],[\"位置处设置为\",{\"1\":{\"419\":1}}],[\"位置偏好\",{\"1\":{\"709\":1}}],[\"位置顺序\",{\"1\":{\"445\":1}}],[\"位置参数\",{\"1\":{\"445\":2}}],[\"位置参数与关键字参数\",{\"0\":{\"445\":1}}],[\"位置嵌入\",{\"1\":{\"633\":1,\"699\":2}}],[\"位置嵌入会被初始化为一组特定的值\",{\"1\":{\"428\":1}}],[\"位置嵌入和类型嵌入之和\",{\"1\":{\"371\":1}}],[\"位置一般是\",{\"1\":{\"384\":1}}],[\"位置的隐藏状态并池化\",{\"1\":{\"699\":1}}],[\"位置的\",{\"1\":{\"694\":1}}],[\"位置的注意力\",{\"1\":{\"274\":1}}],[\"位置的标签设为\",{\"1\":{\"208\":1}}],[\"位置\",{\"1\":{\"137\":1,\"706\":1}}],[\"位置感知\",{\"1\":{\"119\":1}}],[\"位置编码设置\",{\"1\":{\"892\":1}}],[\"位置编码可以有效地捕捉输入序列中的相对位置信息\",{\"1\":{\"823\":1}}],[\"位置编码通常使用正弦和余弦函数生成\",{\"1\":{\"741\":1}}],[\"位置编码通过以下公式计算\",{\"1\":{\"706\":1}}],[\"位置编码为可学习的矩阵\",{\"1\":{\"716\":1}}],[\"位置编码的周期性可能导致不同位置之间的区分度逐渐降低\",{\"1\":{\"706\":1}}],[\"位置编码的作用是为了记忆输入的语序信息\",{\"1\":{\"428\":1}}],[\"位置编码向量是唯一的\",{\"1\":{\"706\":1}}],[\"位置编码计算\",{\"1\":{\"663\":1}}],[\"位置编码被添加到\",{\"1\":{\"427\":1}}],[\"位置编码乘子\",{\"1\":{\"125\":1}}],[\"位置编码对注意力生成分支和特征变换分支都很重要\",{\"1\":{\"114\":1}}],[\"位置编码让注意力能够感知局部空间结构\",{\"1\":{\"114\":1}}],[\"位置编码\",{\"0\":{\"114\":1,\"704\":1},\"1\":{\"117\":1,\"119\":2,\"397\":1,\"428\":2,\"663\":1,\"667\":1,\"704\":1,\"741\":1,\"823\":1,\"892\":1,\"893\":2}}],[\"位置信息作为元素属性处理\",{\"1\":{\"110\":1}}],[\"单模型实现\",{\"1\":{\"894\":1}}],[\"单模态或者多模态\",{\"1\":{\"826\":1}}],[\"单模态自注意力\",{\"1\":{\"418\":1}}],[\"单模态对齐\",{\"1\":{\"385\":1}}],[\"单模态文本特征序列\",{\"1\":{\"385\":1}}],[\"单模态文本特征\",{\"1\":{\"385\":1}}],[\"单模态输出\",{\"1\":{\"385\":1}}],[\"单模态\",{\"1\":{\"385\":6}}],[\"单模态最终输出\",{\"1\":{\"385\":1}}],[\"单模态模式\",{\"1\":{\"385\":2}}],[\"单模态图像编码\",{\"1\":{\"385\":1}}],[\"单模态和多模态解码器层\",{\"1\":{\"277\":1}}],[\"单模态解码器层\",{\"1\":{\"272\":1}}],[\"单模态部分\",{\"1\":{\"268\":1}}],[\"单模态数据\",{\"1\":{\"224\":1}}],[\"单模态编码器\",{\"1\":{\"171\":1}}],[\"单点集\",{\"1\":{\"847\":1}}],[\"单位\",{\"1\":{\"847\":1}}],[\"单个python文件\",{\"1\":{\"810\":1}}],[\"单个遮挡块的最小\",{\"1\":{\"263\":1}}],[\"单元测试\",{\"0\":{\"794\":1}}],[\"单词\",{\"1\":{\"697\":1}}],[\"单词同样合理甚至更好的替代词\",{\"1\":{\"202\":1}}],[\"单任务微调\",{\"1\":{\"685\":1}}],[\"单文档内句子打包\",{\"1\":{\"683\":1}}],[\"单样本学习\",{\"1\":{\"647\":1}}],[\"单样本\",{\"1\":{\"646\":1}}],[\"单纯\",{\"1\":{\"830\":1}}],[\"单纯依赖人工标注和设计任务目标难以满足多任务学习的规模化需求\",{\"1\":{\"639\":1}}],[\"单纯使用最近邻分类器\",{\"1\":{\"280\":1}}],[\"单领域的数据集训练模式\",{\"1\":{\"639\":1}}],[\"单层宽网络\",{\"1\":{\"500\":1}}],[\"单层网络依赖基函数的数量\",{\"1\":{\"500\":1}}],[\"单层网络\",{\"1\":{\"500\":4}}],[\"单隐藏层神经网络的输出形式为\",{\"1\":{\"500\":1}}],[\"单项式基\",{\"1\":{\"500\":1}}],[\"单一嵌入向量\",{\"1\":{\"963\":1}}],[\"单一语言模型可处理翻译\",{\"1\":{\"641\":1}}],[\"单一的线性层只能进行线性变换\",{\"1\":{\"429\":1}}],[\"单一任务\",{\"1\":{\"220\":1}}],[\"单机环境下显存\",{\"1\":{\"385\":1}}],[\"单机\",{\"1\":{\"385\":1}}],[\"单机模式\",{\"1\":{\"385\":1}}],[\"单机训练时\",{\"1\":{\"385\":1}}],[\"单靠有限规模的图文对数据进行预训练不足以获得最佳效果\",{\"1\":{\"377\":1}}],[\"单独学习\",{\"1\":{\"710\":1}}],[\"单独用来处理类别信息\",{\"1\":{\"427\":1}}],[\"单独编码图像和文本\",{\"1\":{\"370\":1}}],[\"单独使用\",{\"1\":{\"177\":1}}],[\"单维主导\",{\"1\":{\"290\":1}}],[\"单编码器的交叉熵分类目标其实可以看作是一种特殊的生成方法\",{\"1\":{\"272\":1}}],[\"单编码器分类\",{\"1\":{\"271\":1}}],[\"单编码器分类预训练\",{\"1\":{\"270\":1}}],[\"单编码器\",{\"1\":{\"268\":1}}],[\"单尺度分组\",{\"1\":{\"138\":1}}],[\"单尺度分组分类模型\",{\"0\":{\"138\":1}}],[\"被归类为了自回归生成模型\",{\"1\":{\"925\":1}}],[\"被拆分成\",{\"1\":{\"924\":2}}],[\"被称为贝叶斯推理\",{\"1\":{\"877\":1}}],[\"被称为自由度\",{\"1\":{\"867\":1}}],[\"被认为是\",{\"1\":{\"827\":1}}],[\"被证明在使用指令形式化描述的未见过的任务上表现良好\",{\"1\":{\"825\":1}}],[\"被设置为多个输入变量中最大的generation的值\",{\"1\":{\"805\":1}}],[\"被用作两次输入\",{\"1\":{\"803\":1}}],[\"被用作图像的全局特征\",{\"1\":{\"171\":1}}],[\"被关注\",{\"1\":{\"709\":1}}],[\"被关注位置\",{\"1\":{\"709\":4}}],[\"被掩码后的输入序列\",{\"1\":{\"698\":1}}],[\"被分开\",{\"1\":{\"640\":1}}],[\"被背景淹没\",{\"1\":{\"589\":1}}],[\"被抽到的概率越高\",{\"1\":{\"518\":1}}],[\"被改变\",{\"1\":{\"468\":1}}],[\"被引用\",{\"1\":{\"448\":1}}],[\"被添加到\",{\"1\":{\"427\":1}}],[\"被修改\",{\"1\":{\"386\":1}}],[\"被划分为\",{\"1\":{\"371\":1}}],[\"被广泛应用于对比学习中\",{\"1\":{\"355\":1}}],[\"被输入到标准的\",{\"1\":{\"286\":1}}],[\"被\",{\"1\":{\"262\":1,\"265\":1}}],[\"被视为提升模型通用性的潜在途径\",{\"1\":{\"639\":1}}],[\"被视为输入\",{\"1\":{\"234\":1}}],[\"被视作常量\",{\"1\":{\"213\":1}}],[\"被更新后\",{\"1\":{\"213\":1}}],[\"被替换为随机\",{\"1\":{\"208\":1}}],[\"被替换为\",{\"1\":{\"208\":1}}],[\"被送进\",{\"1\":{\"83\":1}}],[\"过大\",{\"1\":{\"894\":1}}],[\"过长截断策略\",{\"1\":{\"713\":1}}],[\"过度顺从错误指令\",{\"1\":{\"658\":1}}],[\"过度关注局部块重建而忽略全局表示的问题\",{\"1\":{\"210\":1}}],[\"过于中性\",{\"1\":{\"657\":1}}],[\"过拟合风险\",{\"1\":{\"500\":1}}],[\"过滤无效行\",{\"1\":{\"696\":1}}],[\"过滤掉\",{\"1\":{\"921\":1}}],[\"过滤掉概率低的\",{\"1\":{\"895\":1}}],[\"过滤掉句子数少于2的行\",{\"1\":{\"696\":1}}],[\"过滤掉vocab中的低频词\",{\"1\":{\"595\":1}}],[\"过滤掉所有与下游任务领域高度相关的样本\",{\"1\":{\"273\":1}}],[\"过滤掉与图像不匹配的原始和合成文本\",{\"1\":{\"185\":1}}],[\"过滤阶段\",{\"0\":{\"191\":1}}],[\"过滤对象既包括网页原始文本\",{\"1\":{\"173\":1}}],[\"过滤\",{\"1\":{\"167\":1,\"656\":1,\"898\":1}}],[\"过滤器\",{\"1\":{\"165\":1}}],[\"过程非常简单\",{\"1\":{\"947\":1}}],[\"过程中混入预训练目标\",{\"1\":{\"657\":1}}],[\"过程\",{\"1\":{\"94\":1}}],[\"剪切\",{\"1\":{\"162\":1}}],[\"弯曲\",{\"1\":{\"161\":1}}],[\"常数\",{\"1\":{\"917\":1}}],[\"常数偏置项\",{\"1\":{\"522\":1}}],[\"常省略下标\",{\"1\":{\"846\":1}}],[\"常识和写作能力\",{\"1\":{\"826\":1}}],[\"常识推理\",{\"1\":{\"668\":1}}],[\"常识推理与winograd类任务\",{\"1\":{\"648\":1}}],[\"常识推理等多样化任务\",{\"1\":{\"646\":1}}],[\"常识推理提升8\",{\"1\":{\"626\":1}}],[\"常规可测的集合\",{\"1\":{\"847\":1}}],[\"常规微调\",{\"1\":{\"609\":1}}],[\"常规的词汇表\",{\"1\":{\"595\":1}}],[\"常见事件\",{\"1\":{\"847\":1}}],[\"常见名词准确率93\",{\"1\":{\"641\":1}}],[\"常见错误\",{\"0\":{\"558\":1}}],[\"常见疑问解答\",{\"0\":{\"535\":1}}],[\"常见的llm\",{\"0\":{\"823\":1}}],[\"常见的应用场景包括\",{\"1\":{\"736\":1,\"737\":1}}],[\"常见的迁移学习方法是首先在大规模数据集\",{\"1\":{\"413\":1}}],[\"常见的对称函数\",{\"1\":{\"160\":1}}],[\"常常比官方实现更高效\",{\"1\":{\"510\":1}}],[\"常用的\",{\"1\":{\"943\":1}}],[\"常用的peft方案\",{\"0\":{\"603\":1}}],[\"常用在分类任务中\",{\"1\":{\"910\":1}}],[\"常用评估指标\",{\"0\":{\"559\":1},\"1\":{\"559\":1}}],[\"常用设置如下\",{\"1\":{\"521\":1}}],[\"常用方法\",{\"1\":{\"516\":1}}],[\"常用于类别不平衡的数据集\",{\"1\":{\"518\":1}}],[\"常用于机器学习的数据准备\",{\"1\":{\"513\":1}}],[\"常用于特征和概率分布处理\",{\"1\":{\"507\":1}}],[\"常用于随机打乱索引\",{\"1\":{\"483\":1}}],[\"常用于保存一些模型的状态信息\",{\"1\":{\"474\":1}}],[\"常用于临时缓存数据\",{\"1\":{\"474\":1}}],[\"常用于\",{\"1\":{\"467\":1,\"472\":1,\"514\":1}}],[\"常用于图像回归与重建任务中\",{\"1\":{\"259\":1}}],[\"常用正弦余弦或归一化坐标范围手工设计\",{\"1\":{\"114\":1}}],[\"才结束词汇表的构建\",{\"1\":{\"595\":2}}],[\"才会在分布意义上收敛于真实分布\",{\"1\":{\"949\":1}}],[\"才会有好的效果\",{\"1\":{\"432\":1}}],[\"才会将内容显示到屏幕上\",{\"1\":{\"107\":1}}],[\"才能使其可微\",{\"1\":{\"946\":1}}],[\"才能采到接近目标图像的样本\",{\"1\":{\"944\":1}}],[\"才能得到准确估计\",{\"1\":{\"944\":1}}],[\"才能继续计算其前向的梯度\",{\"1\":{\"804\":1}}],[\"才能计算导数\",{\"1\":{\"779\":1}}],[\"才能够满足业务的需求\",{\"1\":{\"601\":1}}],[\"才能通过\",{\"1\":{\"472\":1,\"511\":1}}],[\"才能起作用\",{\"1\":{\"357\":1}}],[\"才能更好处理复杂多模态任务\",{\"1\":{\"336\":1}}],[\"才能让表示真正学到\",{\"1\":{\"293\":1}}],[\"才能达到最佳效果\",{\"1\":{\"292\":1}}],[\"才能保证整个网络输出与输入点的顺序无关\",{\"1\":{\"160\":1}}],[\"适应特定任务\",{\"1\":{\"824\":1}}],[\"适应性强的智能系统\",{\"1\":{\"658\":1}}],[\"适应任务必须微调\",{\"1\":{\"650\":1}}],[\"适应扩展到所有线性层\",{\"1\":{\"614\":1}}],[\"适应不同任务需求\",{\"1\":{\"592\":1}}],[\"适用场景\",{\"1\":{\"589\":1}}],[\"适用于生产环境\",{\"1\":{\"833\":1}}],[\"适用于更复杂的应用场景\",{\"1\":{\"823\":1}}],[\"适用于只需要前向推理且不需要更新模型参数的场景\",{\"1\":{\"473\":1}}],[\"适用于单模态数据\",{\"1\":{\"223\":1}}],[\"适用于复杂推理任务但依赖目标检测器和高分辨率图像\",{\"1\":{\"195\":1}}],[\"适用于分类\",{\"1\":{\"148\":1}}],[\"适用于分类和密集预测任务\",{\"1\":{\"109\":1}}],[\"适用于\",{\"1\":{\"106\":1}}],[\"适用于连续响应值\",{\"1\":{\"106\":1}}],[\"适当降低训练目标反而可能取得更好的效果\",{\"1\":{\"413\":1}}],[\"适配应用任务\",{\"1\":{\"835\":1}}],[\"适配分类\",{\"1\":{\"303\":1}}],[\"适配视频动作识别任务\",{\"1\":{\"273\":1}}],[\"适合还原复杂细节\",{\"1\":{\"963\":1}}],[\"适合相对稳定的数据\",{\"1\":{\"830\":1}}],[\"适合动态变化的数据\",{\"1\":{\"830\":1}}],[\"适合日常对话和基础任务场景\",{\"1\":{\"823\":1}}],[\"适合大规模优化但灵活性较低\",{\"1\":{\"811\":1}}],[\"适合无需梯度的场景\",{\"1\":{\"807\":1}}],[\"适合机器学习中损失函数的优化\",{\"1\":{\"775\":1}}],[\"适合在推理阶段作为验证标准\",{\"1\":{\"588\":1}}],[\"适合连续值\",{\"1\":{\"505\":1}}],[\"适合图像缩放\",{\"1\":{\"505\":1}}],[\"适合用于构建对话助手\",{\"1\":{\"658\":1}}],[\"适合用于\",{\"1\":{\"504\":1}}],[\"适合描述和理解任务\",{\"1\":{\"268\":1}}],[\"适合检索和零样本分类\",{\"1\":{\"268\":1}}],[\"适合渲染\",{\"1\":{\"159\":1}}],[\"适合\",{\"1\":{\"159\":1}}],[\"法律\",{\"1\":{\"649\":1}}],[\"法→英\",{\"1\":{\"641\":1}}],[\"法向量等属性\",{\"1\":{\"159\":1}}],[\"法向量\",{\"1\":{\"159\":1}}],[\"法线等\",{\"1\":{\"137\":1}}],[\"忽视局部邻域关系\",{\"1\":{\"157\":1}}],[\"忽略系数\",{\"1\":{\"871\":1}}],[\"忽略填充部分\",{\"1\":{\"699\":1}}],[\"忽略难分类样本\",{\"1\":{\"589\":1}}],[\"忽略标签为\",{\"1\":{\"208\":1}}],[\"忽略\",{\"1\":{\"208\":2,\"384\":1,\"900\":1}}],[\"忽略pad位loss\",{\"1\":{\"192\":1}}],[\"忽略局部结构\",{\"1\":{\"157\":1}}],[\"忽略局部结构信息\",{\"1\":{\"157\":1}}],[\"忽略了局部邻域之间的结构关系\",{\"1\":{\"157\":1}}],[\"忽略坐标\",{\"1\":{\"122\":1}}],[\"易用\",{\"1\":{\"814\":1}}],[\"易混淆\",{\"0\":{\"735\":1}}],[\"易样本\",{\"1\":{\"589\":1}}],[\"易分类样本\",{\"1\":{\"589\":1}}],[\"易于扩展为检测\",{\"1\":{\"157\":1}}],[\"易受类别不平衡影响\",{\"1\":{\"587\":1}}],[\"易受\",{\"1\":{\"106\":1}}],[\"极端案例\",{\"1\":{\"641\":1}}],[\"极大地丰富了\",{\"1\":{\"834\":1}}],[\"极大地提升了开发效率和应用性能\",{\"1\":{\"833\":1}}],[\"极大地提升了模型在各种自然语言处理任务上的表现\",{\"1\":{\"822\":1}}],[\"极大地降低了在各种技术栈上构建\",{\"1\":{\"833\":1}}],[\"极大地降低成本\",{\"1\":{\"607\":1}}],[\"极大地简化了调试和问题排查的流程\",{\"1\":{\"833\":1}}],[\"极大\",{\"1\":{\"500\":1}}],[\"极其高效\",{\"1\":{\"157\":1}}],[\"极为重要\",{\"1\":{\"48\":1}}],[\"略逊于多视角\",{\"1\":{\"157\":1}}],[\"接上图像\",{\"1\":{\"893\":1}}],[\"接触到更多的文本信息\",{\"1\":{\"823\":1}}],[\"接一个逐位置的前馈层来生成目标字符的分布输出\",{\"1\":{\"629\":1}}],[\"接一个全连接层\",{\"1\":{\"201\":1}}],[\"接下来使用\",{\"1\":{\"945\":1}}],[\"接下来的小节将更详细地介绍这两个阶段\",{\"1\":{\"885\":1}}],[\"接下来的阶段将进一步扩展tinypytorch\",{\"1\":{\"797\":1}}],[\"接下来就需要进行长期的用户体验跟踪\",{\"1\":{\"836\":1}}],[\"接下来就很简单了\",{\"1\":{\"350\":1}}],[\"接下来我们需要搭建前后端\",{\"1\":{\"836\":1}}],[\"接下来我们主要介绍几个国内外常见的大模型\",{\"1\":{\"823\":1}}],[\"接下来将使用tinypytorch验证是否能找到该最小值\",{\"1\":{\"816\":1}}],[\"接下来介绍几种比较流行的peft微调方案\",{\"1\":{\"603\":1}}],[\"接下来\",{\"1\":{\"408\":1,\"606\":1,\"825\":1}}],[\"接收信息\",{\"1\":{\"877\":1}}],[\"接收任意个数的位置参数\",{\"1\":{\"800\":1}}],[\"接收器操作特性\",{\"1\":{\"569\":1}}],[\"接收函数并返回函数\",{\"1\":{\"449\":1}}],[\"接收函数作为参数\",{\"1\":{\"447\":1}}],[\"接收\",{\"1\":{\"380\":1}}],[\"接收学生输出\",{\"1\":{\"293\":1}}],[\"接收的文本\",{\"1\":{\"177\":1}}],[\"接着是第\",{\"1\":{\"541\":2}}],[\"接着\",{\"1\":{\"234\":1,\"626\":1,\"945\":1,\"948\":1}}],[\"接着通过多头可供性链式思维\",{\"1\":{\"32\":1}}],[\"接近于标准正态分布\",{\"1\":{\"935\":1}}],[\"接近palm\",{\"1\":{\"668\":1}}],[\"接近人类水平\",{\"1\":{\"648\":1}}],[\"接近或超越部分任务的微调模型性能\",{\"1\":{\"646\":1}}],[\"接近1\",{\"1\":{\"589\":1}}],[\"接近0\",{\"1\":{\"589\":1}}],[\"接近原图\",{\"1\":{\"293\":1}}],[\"接近\",{\"1\":{\"157\":1}}],[\"面向大语言模型的检索增强生成技术\",{\"1\":{\"837\":1}}],[\"面对复杂问题时\",{\"1\":{\"828\":1}}],[\"面对大量噪声点时效果较差\",{\"1\":{\"157\":1}}],[\"面临的主要问题有\",{\"1\":{\"828\":1}}],[\"面临\",{\"1\":{\"194\":1}}],[\"面部发生形变\",{\"1\":{\"157\":1}}],[\"尤其适用于生成模型中从\",{\"1\":{\"897\":1}}],[\"尤其适合研究和快速开发场景\",{\"1\":{\"811\":1}}],[\"尤其当处理大规模数据时\",{\"1\":{\"806\":1}}],[\"尤其对于多变量函数\",{\"1\":{\"772\":1}}],[\"尤其对\",{\"1\":{\"502\":1}}],[\"尤其对语义分割帮助很大\",{\"1\":{\"242\":1}}],[\"尤其是当我们用任意函数\",{\"1\":{\"949\":1}}],[\"尤其是二分类和多分类问题\",{\"1\":{\"910\":1}}],[\"尤其是自回归\",{\"1\":{\"884\":1}}],[\"尤其是情商方面异常优秀\",{\"1\":{\"823\":1}}],[\"尤其是few\",{\"1\":{\"647\":1}}],[\"尤其是目标检测\",{\"1\":{\"589\":1}}],[\"尤其是底层由\",{\"1\":{\"492\":1}}],[\"尤其是\",{\"1\":{\"382\":1,\"630\":1}}],[\"尤其是零样本\",{\"1\":{\"339\":1,\"346\":1}}],[\"尤其是在高容量模型下\",{\"1\":{\"942\":1}}],[\"尤其是在长文本任务中\",{\"1\":{\"649\":1}}],[\"尤其是在\",{\"1\":{\"403\":1}}],[\"尤其是在大规模预训练时\",{\"1\":{\"236\":1}}],[\"尤其是在低层次上对每个质心点运行局部pointnet时\",{\"1\":{\"142\":1}}],[\"尤其是在预测值接近极端值\",{\"1\":{\"102\":1}}],[\"尤其在解决复杂任务时表现出了惊人的潜力\",{\"1\":{\"822\":1}}],[\"尤其在神经网络训练中\",{\"1\":{\"806\":1}}],[\"尤其在军事\",{\"1\":{\"658\":1}}],[\"尤其在有\",{\"1\":{\"657\":1}}],[\"尤其在few\",{\"1\":{\"648\":1}}],[\"尤其在anli这种对抗性构建的数据集上\",{\"1\":{\"648\":1}}],[\"尤其在coqa中few\",{\"1\":{\"648\":1}}],[\"尤其在翻译为英语的方向上\",{\"1\":{\"648\":1}}],[\"尤其在triviaqa中\",{\"1\":{\"648\":1}}],[\"尤其在目标区域较小\",{\"1\":{\"586\":1}}],[\"尤其在ocr相关任务\",{\"1\":{\"323\":1,\"337\":1}}],[\"尤其在\",{\"1\":{\"322\":1}}],[\"尤其在图像分类与部分\",{\"1\":{\"168\":1}}],[\"尤其未训练时\",{\"1\":{\"157\":1}}],[\"十二亿\",{\"1\":{\"432\":1}}],[\"十亿\",{\"1\":{\"432\":1}}],[\"十一\",{\"1\":{\"157\":1}}],[\"十\",{\"1\":{\"157\":1}}],[\"又会带来两个新的问题\",{\"1\":{\"956\":1}}],[\"又会导致计算资源浪费\",{\"1\":{\"157\":1}}],[\"又称\",{\"1\":{\"961\":1}}],[\"又称多元正态分布\",{\"1\":{\"870\":1}}],[\"又称为\",{\"1\":{\"586\":1}}],[\"又进一步发布了\",{\"1\":{\"823\":1}}],[\"又几乎不增加计算量\",{\"1\":{\"710\":1}}],[\"又是一个有实际意义的字或词\",{\"1\":{\"694\":1}}],[\"又怎么能够利用这些开源的大模型\",{\"1\":{\"610\":1}}],[\"又能让\",{\"1\":{\"949\":1}}],[\"又能快速地求出图像概率的模型\",{\"1\":{\"925\":1}}],[\"又能计算概率的模型不好设计\",{\"1\":{\"925\":1}}],[\"又能拆解未知词的子词词汇表\",{\"1\":{\"594\":1}}],[\"又能结合\",{\"1\":{\"268\":1}}],[\"又保持冻结特征能力\",{\"1\":{\"278\":1}}],[\"又理解整体结构\",{\"1\":{\"150\":1}}],[\"九\",{\"0\":{\"558\":1},\"1\":{\"157\":1}}],[\"深入的业务逻辑理解往往也能带来更好的\",{\"1\":{\"836\":1}}],[\"深度置信网络\",{\"1\":{\"950\":1}}],[\"深度分离定理\",{\"1\":{\"500\":1}}],[\"深度叠加导致表达能力爆炸式增长\",{\"1\":{\"500\":1}}],[\"深度学习框架\",{\"1\":{\"819\":1}}],[\"深度学习框架的计算图范式\",{\"1\":{\"811\":1}}],[\"深度学习框架之所以强大\",{\"1\":{\"799\":1}}],[\"深度学习框架中蕴藏着惊人的技术和有趣的机制\",{\"1\":{\"753\":1}}],[\"深度学习等领域也得到了广泛应用\",{\"1\":{\"540\":1}}],[\"深度学习中的标准实践\",{\"1\":{\"522\":1}}],[\"深度学习中通常固定的随机源\",{\"1\":{\"521\":1}}],[\"深度学习的发展\",{\"1\":{\"268\":1}}],[\"深度学习模型\",{\"1\":{\"157\":1}}],[\"深层cnn\",{\"1\":{\"500\":1}}],[\"深层网络的代价\",{\"1\":{\"500\":1}}],[\"深层网络类似布尔电路中的分层设计\",{\"1\":{\"500\":1}}],[\"深层网络通过共享参数\",{\"1\":{\"500\":1}}],[\"深层网络将复杂函数分解为多个简单步骤\",{\"1\":{\"500\":1}}],[\"深层网络依赖函数的嵌套深度\",{\"1\":{\"500\":1}}],[\"深层网络可以逐步构造出更复杂的函数\",{\"1\":{\"500\":1}}],[\"深层网络\",{\"1\":{\"500\":4}}],[\"深层神经网络通过函数复合\",{\"1\":{\"500\":1}}],[\"深层\",{\"1\":{\"122\":1,\"215\":1}}],[\"深层抽象特征\",{\"1\":{\"122\":1}}],[\"深层特征\",{\"1\":{\"121\":1}}],[\"八百六十万\",{\"1\":{\"432\":1}}],[\"八\",{\"0\":{\"557\":1},\"1\":{\"157\":1}}],[\"七\",{\"0\":{\"538\":1,\"556\":1},\"1\":{\"157\":1}}],[\"七类损坏类型\",{\"1\":{\"25\":1}}],[\"六\",{\"0\":{\"535\":1,\"555\":1},\"1\":{\"157\":1}}],[\"很可惜\",{\"1\":{\"925\":1,\"956\":1}}],[\"很接近\",{\"1\":{\"915\":2}}],[\"很容易导致旧知识遗忘\",{\"1\":{\"610\":1}}],[\"很多简单的任务\",{\"1\":{\"616\":1}}],[\"很多模型在\",{\"1\":{\"510\":1}}],[\"很多点\",{\"1\":{\"121\":1}}],[\"很小\",{\"1\":{\"355\":1}}],[\"很大\",{\"1\":{\"355\":1,\"356\":1}}],[\"很难在这种情况下保持分类的一致性\",{\"1\":{\"157\":1}}],[\"拉回正态\",{\"1\":{\"578\":1}}],[\"拉伸等会导致形变的操作\",{\"1\":{\"162\":1}}],[\"拉伸等形变\",{\"1\":{\"157\":1}}],[\"拉伸\",{\"1\":{\"157\":1,\"161\":1}}],[\"拉平数据\",{\"1\":{\"578\":1}}],[\"拉平后线性投影获得\",{\"1\":{\"371\":1}}],[\"拉平\",{\"1\":{\"83\":1}}],[\"旋转位置编码\",{\"1\":{\"823\":1}}],[\"旋转预测\",{\"1\":{\"248\":1}}],[\"旋转\",{\"1\":{\"157\":1,\"161\":2,\"162\":2}}],[\"旋转等变性\",{\"1\":{\"110\":1}}],[\"五\",{\"0\":{\"534\":1,\"554\":1},\"1\":{\"157\":1}}],[\"四种任务设定方法的比较\",{\"1\":{\"647\":1}}],[\"四阶张量的例子\",{\"1\":{\"547\":1}}],[\"四个残差块和一个输出层\",{\"1\":{\"497\":1}}],[\"四个指标协同工作\",{\"1\":{\"106\":1}}],[\"四个指标对比总结\",{\"1\":{\"106\":1}}],[\"四\",{\"0\":{\"530\":1,\"553\":1,\"818\":1},\"1\":{\"157\":1,\"655\":1}}],[\"🧮\",{\"0\":{\"798\":1}}],[\"🤔\",{\"1\":{\"784\":1}}],[\"🧪\",{\"1\":{\"157\":1,\"735\":1}}],[\"🧩\",{\"1\":{\"157\":2,\"735\":1}}],[\"🧱\",{\"1\":{\"157\":3}}],[\"🧠\",{\"0\":{\"813\":1},\"1\":{\"153\":1,\"157\":1,\"160\":1}}],[\"曲率等细节\",{\"1\":{\"157\":1}}],[\"曲率\",{\"1\":{\"157\":1}}],[\"曲线上最接近\",{\"1\":{\"572\":1}}],[\"曲线为从\",{\"1\":{\"570\":1}}],[\"曲线的方法是\",{\"1\":{\"569\":1}}],[\"曲线直观地显示了所有阈值下的模型性能\",{\"1\":{\"569\":1}}],[\"曲线和\",{\"0\":{\"568\":1}}],[\"曲线\",{\"1\":{\"106\":1}}],[\"曲线下的面积\",{\"1\":{\"106\":1}}],[\"曲线下面积较大的模型通常是更好的模型\",{\"1\":{\"572\":1}}],[\"曲线下面积\",{\"0\":{\"570\":1},\"1\":{\"106\":1,\"570\":1}}],[\"座位\",{\"1\":{\"156\":1}}],[\"控制了编码器的相对学习速度\",{\"1\":{\"960\":1}}],[\"控制了相似度计算的维度\",{\"1\":{\"532\":1}}],[\"控制维度\",{\"1\":{\"924\":1}}],[\"控制离散采样的平滑程度\",{\"1\":{\"899\":1}}],[\"控制保留多少\",{\"1\":{\"898\":1}}],[\"控制采样平滑程度\",{\"1\":{\"899\":1}}],[\"控制采样的随机程度\",{\"1\":{\"897\":1}}],[\"控制采样随机性\",{\"1\":{\"895\":1}}],[\"控制随机性\",{\"1\":{\"897\":1}}],[\"控制生成\",{\"1\":{\"895\":1}}],[\"控制这项的权重\",{\"1\":{\"885\":1}}],[\"控制反向传播是否启用\",{\"1\":{\"807\":1}}],[\"控制手段\",{\"1\":{\"657\":1}}],[\"控制交叉熵损失和\",{\"1\":{\"592\":1}}],[\"控制交叉熵中正负样本的权重\",{\"1\":{\"592\":1}}],[\"控制两个损失之间的权重比例\",{\"1\":{\"592\":1}}],[\"控制区域匹配误差的重要性\",{\"1\":{\"592\":1}}],[\"控制分类误差的重要性\",{\"1\":{\"592\":1}}],[\"控制分布的平滑程度\",{\"1\":{\"285\":1}}],[\"控制分布的平滑度\",{\"1\":{\"257\":1}}],[\"控制分布的\",{\"1\":{\"257\":1}}],[\"控制假阴性\",{\"1\":{\"590\":1}}],[\"控制假阳性\",{\"1\":{\"590\":1}}],[\"控制正类\",{\"1\":{\"589\":1}}],[\"控制正样本\",{\"1\":{\"589\":1}}],[\"控制难易样本权重\",{\"1\":{\"589\":1}}],[\"控制难易样本的权重衰减程度\",{\"1\":{\"589\":1}}],[\"控制残差贡献\",{\"1\":{\"380\":1}}],[\"控制输出格式\",{\"1\":{\"208\":1}}],[\"控制\",{\"1\":{\"208\":1,\"293\":1,\"353\":1,\"521\":4,\"592\":1,\"657\":1,\"944\":1}}],[\"控制动量蒸馏信号的强度\",{\"1\":{\"202\":1}}],[\"控制是否对\",{\"1\":{\"586\":1}}],[\"控制是否启用\",{\"1\":{\"397\":1}}],[\"控制是否使用\",{\"1\":{\"154\":1}}],[\"控制是否输出全局特征\",{\"1\":{\"154\":1}}],[\"控制每个\",{\"1\":{\"53\":1}}],[\"❗而只有正交矩阵才能表示刚性变换\",{\"1\":{\"153\":1}}],[\"帮助我们处理各种任务\",{\"1\":{\"831\":1}}],[\"帮助用户理解推理步骤\",{\"1\":{\"823\":1}}],[\"帮助开发者直观理解前向与反向传播路径\",{\"1\":{\"814\":1}}],[\"帮助读者从零搭建属于自己的深度学习引擎\",{\"1\":{\"799\":1}}],[\"帮助读者正确理解技术\",{\"1\":{\"753\":1}}],[\"帮助训练稳定收敛\",{\"1\":{\"152\":1}}],[\"帮你在\",{\"1\":{\"83\":1}}],[\"摆正\",{\"1\":{\"152\":1,\"154\":1}}],[\"摆脱几何标注和固定场景限制\",{\"1\":{\"75\":1}}],[\"决定哪些区域更重要\",{\"1\":{\"582\":1}}],[\"决定了输出的信息维度\",{\"1\":{\"532\":1}}],[\"决定是否返回所有\",{\"1\":{\"266\":1}}],[\"决定\",{\"1\":{\"150\":1,\"500\":1}}],[\"小图像\",{\"1\":{\"956\":10,\"961\":1}}],[\"小型语言模型通常难以解决涉及多个推理步骤的复杂任务\",{\"1\":{\"825\":1}}],[\"小而全\",{\"1\":{\"819\":1}}],[\"小距离用独立桶\",{\"1\":{\"710\":1}}],[\"小模型\",{\"1\":{\"668\":1}}],[\"小模型+更多数据训练可能更优\",{\"1\":{\"666\":1}}],[\"小模型实验表明\",{\"1\":{\"278\":1}}],[\"小数据集没有\",{\"1\":{\"635\":1}}],[\"小批次样本\",{\"1\":{\"633\":1}}],[\"小批量训练\",{\"0\":{\"292\":1}}],[\"小孩子根据从示例中学习到的推理\",{\"1\":{\"620\":1}}],[\"小块\",{\"1\":{\"501\":1}}],[\"小batch训练可能需要重新调整超参数\",{\"1\":{\"292\":1}}],[\"小\",{\"1\":{\"280\":2,\"706\":1}}],[\"小结关系图\",{\"1\":{\"909\":1}}],[\"小结\",{\"0\":{\"174\":1,\"402\":1,\"413\":1}}],[\"小扰动不会改变函数输出\",{\"1\":{\"150\":1}}],[\"小红书\",{\"1\":{\"0\":1}}],[\"学完了\",{\"1\":{\"925\":1}}],[\"学\",{\"1\":{\"885\":3}}],[\"学多少\",{\"1\":{\"844\":1,\"876\":1}}],[\"学术数据不足限制mmlu表现\",{\"1\":{\"668\":1}}],[\"学会生成让\",{\"1\":{\"946\":1}}],[\"学会\",{\"1\":{\"649\":1}}],[\"学分类头\",{\"1\":{\"268\":1}}],[\"学到更可靠的全局表征\",{\"1\":{\"214\":1}}],[\"学到的是一个关键点集合\",{\"1\":{\"157\":1}}],[\"学到的是一个\",{\"1\":{\"150\":1}}],[\"学习的是\",{\"1\":{\"964\":1}}],[\"学习这些离散\",{\"1\":{\"964\":2}}],[\"学习将图像压缩为离散\",{\"1\":{\"964\":1}}],[\"学习一个分布\",{\"1\":{\"925\":1}}],[\"学习笔记\",{\"0\":{\"901\":1,\"966\":1},\"1\":{\"901\":1,\"966\":1}}],[\"学习先验\",{\"0\":{\"887\":1}}],[\"学习先验分布\",{\"1\":{\"235\":1}}],[\"学习视觉码本\",{\"0\":{\"886\":1}}],[\"学习通用的语言表示和知识\",{\"1\":{\"824\":1}}],[\"学习算法需要在所有可能的函数空间中搜索最优模型\",{\"1\":{\"422\":1}}],[\"学习不同模态之间的对齐关系\",{\"1\":{\"376\":1}}],[\"学习上下文表示\",{\"1\":{\"369\":1}}],[\"学习到输入数据的非线性特征\",{\"1\":{\"429\":1}}],[\"学习到的特征在下游任务上具有很好的迁移性\",{\"1\":{\"352\":1}}],[\"学习到的图像编码器可作为通用视觉表示提取器\",{\"1\":{\"271\":1}}],[\"学习到跨模态对齐能力\",{\"1\":{\"269\":1}}],[\"学习全局语义表征\",{\"1\":{\"268\":1}}],[\"学习成本高\",{\"1\":{\"250\":1}}],[\"学习\",{\"1\":{\"228\":1,\"274\":1,\"355\":1,\"814\":1,\"964\":1}}],[\"学习目标\",{\"1\":{\"420\":1}}],[\"学习目标的计算\",{\"1\":{\"386\":1}}],[\"学习目标之前\",{\"1\":{\"386\":1}}],[\"学习目标是恢复原始\",{\"1\":{\"220\":1}}],[\"学习目标实现过程\",{\"1\":{\"206\":1,\"207\":1,\"208\":1}}],[\"学习温度系数\",{\"1\":{\"205\":1}}],[\"学习共同低维空间\",{\"1\":{\"194\":1}}],[\"学习大模型\",{\"1\":{\"168\":1}}],[\"学习率调整\",{\"1\":{\"680\":1}}],[\"学习率调度\",{\"1\":{\"286\":1}}],[\"学习率调度器\",{\"1\":{\"104\":1,\"510\":1}}],[\"学习率调度器初始化等\",{\"1\":{\"104\":1}}],[\"学习率采用线性预热\",{\"1\":{\"679\":1}}],[\"学习率采用线性缩放\",{\"1\":{\"292\":1}}],[\"学习率采用余弦退火策略\",{\"1\":{\"315\":1}}],[\"学习率按\",{\"1\":{\"293\":1}}],[\"学习率设为\",{\"1\":{\"236\":1}}],[\"学习率预热后分别达到\",{\"1\":{\"176\":1}}],[\"学习率\",{\"1\":{\"34\":1,\"46\":1,\"224\":1,\"286\":1,\"293\":2}}],[\"学生模型输出\",{\"1\":{\"293\":1}}],[\"学生模型不能由教师模型直接初始化\",{\"1\":{\"182\":1}}],[\"学生输出按温度缩放\",{\"1\":{\"293\":1}}],[\"学生输出按温度缩放并拆分为每个裁剪的输出\",{\"1\":{\"293\":1}}],[\"学生输出\",{\"1\":{\"293\":1}}],[\"学生温度\",{\"1\":{\"286\":1}}],[\"学生网络看所有裁剪\",{\"1\":{\"293\":1}}],[\"学生网络训练模式\",{\"1\":{\"293\":1}}],[\"学生网络处理所有视角\",{\"1\":{\"285\":1}}],[\"学生网络通过交叉熵损失\",{\"1\":{\"280\":1}}],[\"学生通过最小化交叉熵损失来学习匹配教师的分布\",{\"1\":{\"285\":1}}],[\"学生参数的指数移动平均\",{\"1\":{\"283\":1}}],[\"学生与教师使用相同架构\",{\"1\":{\"283\":1}}],[\"学生和教师完全相同的架构\",{\"1\":{\"282\":1}}],[\"学生\",{\"0\":{\"867\":1},\"1\":{\"168\":1,\"293\":1,\"867\":1}}],[\"理想情况\",{\"1\":{\"415\":1}}],[\"理想中我们希望\",{\"1\":{\"260\":1}}],[\"理解此处的数学推导意义不大\",{\"1\":{\"959\":1}}],[\"理解微妙提示方面表现更出色\",{\"1\":{\"823\":1}}],[\"理解反面\",{\"1\":{\"634\":1}}],[\"理解为\",{\"1\":{\"545\":1}}],[\"理解并回答科学类问题\",{\"1\":{\"342\":1}}],[\"理解任务\",{\"1\":{\"109\":1}}],[\"理论\",{\"1\":{\"950\":1}}],[\"理论支持\",{\"1\":{\"500\":1}}],[\"理论结果\",{\"1\":{\"500\":1}}],[\"理论限制的相似性\",{\"1\":{\"500\":1}}],[\"理论上可在单机计算\",{\"1\":{\"385\":1}}],[\"理论上的限制\",{\"1\":{\"157\":1}}],[\"理论上证明\",{\"1\":{\"150\":1}}],[\"理论分析保证模型鲁棒性\",{\"1\":{\"150\":1}}],[\"矩阵维度\",{\"1\":{\"703\":1}}],[\"矩阵又不是\",{\"1\":{\"612\":1}}],[\"矩阵a和b为什么不能同时为零\",{\"0\":{\"612\":1}}],[\"矩阵初始化\",{\"1\":{\"611\":2,\"612\":1}}],[\"矩阵从\",{\"1\":{\"609\":1}}],[\"矩阵w就是通过机器学习\",{\"1\":{\"600\":1}}],[\"矩阵中的对角线元素\",{\"1\":{\"407\":1}}],[\"矩阵所有元素平方和开方\",{\"1\":{\"153\":1}}],[\"矩阵返回\",{\"1\":{\"152\":1}}],[\"矩阵\",{\"1\":{\"150\":2,\"528\":1,\"574\":1,\"611\":1,\"612\":2}}],[\"≈\",{\"1\":{\"150\":1,\"204\":2,\"263\":2,\"893\":2,\"895\":1}}],[\"稀疏向量处理好\",{\"1\":{\"508\":1}}],[\"稀疏性强\",{\"1\":{\"159\":1}}],[\"稀疏点云等任务中表现受限\",{\"1\":{\"157\":1}}],[\"稀疏点云下性能差\",{\"1\":{\"157\":1}}],[\"稀疏\",{\"1\":{\"149\":1}}],[\"稀疏卷积只计算非空体素\",{\"1\":{\"110\":1}}],[\"难于训练\",{\"1\":{\"610\":1}}],[\"难样本\",{\"1\":{\"589\":1}}],[\"难样本vs易样\",{\"1\":{\"589\":1}}],[\"难分类样本\",{\"1\":{\"589\":1}}],[\"难负样本\",{\"1\":{\"207\":1}}],[\"难负样本采样\",{\"1\":{\"190\":1}}],[\"难点\",{\"0\":{\"149\":1},\"1\":{\"150\":4}}],[\"难以还原原始数据\",{\"1\":{\"949\":1}}],[\"难以做到的\",{\"1\":{\"707\":1}}],[\"难以准确表示极长序列中各个位置的独特信息\",{\"1\":{\"706\":1}}],[\"难以覆盖广泛的语言任务\",{\"1\":{\"646\":1}}],[\"难以学习有效特征\",{\"1\":{\"589\":1}}],[\"难以局部修正\",{\"1\":{\"500\":1}}],[\"难以提升模型泛化能力\",{\"1\":{\"178\":1}}],[\"难以自动构建\",{\"1\":{\"159\":1}}],[\"难以用\",{\"1\":{\"159\":1}}],[\"难以建模更高维度的空间关系\",{\"1\":{\"157\":1}}],[\"难以捕捉非刚性变换下的不变性\",{\"1\":{\"157\":1}}],[\"难以区分语义相近但位置不同的区域\",{\"1\":{\"157\":1}}],[\"难以泛化到未见过的功能\",{\"1\":{\"30\":1}}],[\"难以识别和泛化与内部结构相关的可供性\",{\"1\":{\"27\":1}}],[\"那我们就可以从这个分布里随机采样向量\",{\"1\":{\"956\":1}}],[\"那我们就认为\",{\"1\":{\"694\":1}}],[\"那只能靠\",{\"1\":{\"951\":1}}],[\"那它们的并集和交集也得能谈\",{\"1\":{\"847\":1}}],[\"那它的补集你也得能谈\",{\"1\":{\"847\":1}}],[\"那\",{\"1\":{\"847\":2}}],[\"那就需要使用生成式模型\",{\"1\":{\"735\":1}}],[\"那就一起用\",{\"1\":{\"145\":1}}],[\"那不就矛盾了吗\",{\"1\":{\"694\":1}}],[\"那很有可能就是我们最终的答案\",{\"1\":{\"621\":1}}],[\"那为什么还要有个qlora呢\",{\"1\":{\"607\":1}}],[\"那个图片的特征\",{\"1\":{\"355\":1}}],[\"那么恭喜你\",{\"1\":{\"961\":1}}],[\"那么我们用高斯近似的\",{\"1\":{\"949\":1}}],[\"那么我们该如何对公式\",{\"1\":{\"946\":1}}],[\"那么我当前图片经过数据增强之后\",{\"1\":{\"355\":1}}],[\"那么网络就只输出\",{\"1\":{\"925\":1}}],[\"那么采样出来的图像应该也接近灰色\",{\"1\":{\"875\":1}}],[\"那么一共有\",{\"1\":{\"881\":1}}],[\"那么一个事件的发生不会改变另一个事件的概率\",{\"1\":{\"849\":1}}],[\"那么一致性是如何做到的\",{\"1\":{\"353\":1}}],[\"那么值减少最快的方向是\",{\"1\":{\"816\":1}}],[\"那么f\",{\"1\":{\"801\":1}}],[\"那么function可能会放大或者缩小x对输出y大小变化的影响\",{\"1\":{\"775\":1}}],[\"那么此时影响因子r=\",{\"1\":{\"775\":1}}],[\"那么影响因子r恒为1\",{\"1\":{\"775\":1}}],[\"那么对于任意事件\",{\"1\":{\"850\":1}}],[\"那么对应的就是单词\",{\"1\":{\"735\":1}}],[\"那么对比学习的过程\",{\"1\":{\"353\":1}}],[\"那么可以组合这两个索引得到答案\",{\"1\":{\"733\":1}}],[\"那么它们都和\",{\"1\":{\"915\":1}}],[\"那么它们的相对位置信息也是相同的\",{\"1\":{\"708\":1}}],[\"那么它从\",{\"1\":{\"700\":2}}],[\"那么它就需要具有同时处理图像和文本的能力\",{\"1\":{\"380\":1}}],[\"那么这个误差项趋近于\",{\"1\":{\"945\":1}}],[\"那么这个\",{\"1\":{\"694\":1}}],[\"那么上面的梯度就变成了\",{\"1\":{\"612\":1}}],[\"那么矩阵\",{\"1\":{\"612\":1}}],[\"那么那些小公司或者个人\",{\"1\":{\"610\":1}}],[\"那么prompt\",{\"1\":{\"604\":1}}],[\"那么搭建自己的大模型就非常必要\",{\"1\":{\"601\":1}}],[\"那么协方差矩阵就是\",{\"1\":{\"578\":1}}],[\"那么前者的变化会主导整个距离\",{\"1\":{\"578\":1}}],[\"那么最终输出为\",{\"1\":{\"534\":1}}],[\"那么应该保留偏置\",{\"1\":{\"522\":1}}],[\"那么作者就想把注意力得到的结果\",{\"1\":{\"433\":1}}],[\"那么就存在很多种函数\",{\"1\":{\"949\":1}}],[\"那么就会导致模态混乱了\",{\"1\":{\"892\":1}}],[\"那么就会得到个文本特征\",{\"1\":{\"408\":1}}],[\"那么就得到了多项分布\",{\"1\":{\"857\":1}}],[\"那么就把该\",{\"1\":{\"501\":1}}],[\"那么就是一个\",{\"1\":{\"160\":1}}],[\"那么clip的训练目标就是最大个正样本的相似度\",{\"1\":{\"407\":1}}],[\"那么抽出三个来\",{\"1\":{\"357\":1}}],[\"那么除了我当前这个图片外\",{\"1\":{\"355\":1}}],[\"那么是什么意义呢\",{\"1\":{\"355\":1}}],[\"那么有一个问题\",{\"1\":{\"355\":1}}],[\"那么从中抽样的可能性组合就很多\",{\"1\":{\"353\":1}}],[\"那么字典最好需要满足两个条件\",{\"1\":{\"353\":1}}],[\"那么字典中的每个key就是一个类别\",{\"1\":{\"353\":1}}],[\"那么负样本走哪个编码器呢\",{\"1\":{\"353\":1}}],[\"那么\",{\"1\":{\"351\":1,\"355\":1,\"357\":1,\"408\":1,\"426\":1,\"592\":1,\"706\":1,\"803\":1,\"846\":1,\"877\":1,\"880\":1,\"916\":1,\"917\":1,\"949\":3,\"956\":1}}],[\"那样存整个向量矩阵\",{\"1\":{\"710\":1}}],[\"那样偏向背景点\",{\"1\":{\"588\":1}}],[\"那样对负样本过多敏感\",{\"1\":{\"586\":1}}],[\"那样的\",{\"1\":{\"444\":1}}],[\"那样的先单模态\",{\"1\":{\"269\":1}}],[\"那样我们总共有196个向量\",{\"1\":{\"426\":1}}],[\"那样\",{\"1\":{\"385\":1}}],[\"那样逐层提取多层次的抽象特征\",{\"1\":{\"157\":1}}],[\"那样依赖\",{\"1\":{\"106\":1}}],[\"给一个真实样本\",{\"1\":{\"947\":1}}],[\"给两个概率分布\",{\"1\":{\"914\":1}}],[\"给每个普通词分配索引\",{\"1\":{\"697\":1}}],[\"给每个实例样本加一个\",{\"1\":{\"635\":1}}],[\"给每个点\",{\"1\":{\"157\":1}}],[\"给llm更多的时间去思考\",{\"0\":{\"619\":1},\"1\":{\"619\":2}}],[\"给出了直观的样本统计量作为参数估计\",{\"1\":{\"904\":1}}],[\"给出了完整的超参数描述\",{\"1\":{\"886\":1}}],[\"给出一定图像引导\",{\"1\":{\"895\":1}}],[\"给出问题和上下文\",{\"1\":{\"735\":1}}],[\"给出较为合理的解释\",{\"1\":{\"657\":1}}],[\"给出最大化的目标函数为\",{\"1\":{\"630\":1}}],[\"给出概率最大的结果\",{\"1\":{\"616\":1}}],[\"给出的\",{\"1\":{\"260\":1}}],[\"给多个数组\",{\"1\":{\"513\":1}}],[\"给图像\",{\"1\":{\"385\":1}}],[\"给图像配上错误文本\",{\"1\":{\"382\":1}}],[\"给文本和图像\",{\"1\":{\"384\":1}}],[\"给文本配上错误图像\",{\"1\":{\"382\":1}}],[\"给\",{\"1\":{\"382\":1}}],[\"给这3个邻近点分配权重\",{\"1\":{\"145\":1}}],[\"给定输入\",{\"1\":{\"952\":1}}],[\"给定隐变量\",{\"1\":{\"932\":1}}],[\"给定下观测到该样本的联合概率或概率密度\",{\"1\":{\"904\":1}}],[\"给定某个输出值\",{\"1\":{\"846\":1}}],[\"给定某个状态\",{\"1\":{\"846\":1}}],[\"给定文档\",{\"1\":{\"631\":1}}],[\"给定图像\",{\"1\":{\"370\":1}}],[\"给定一张用户已经去除了不需要物体的图像\",{\"1\":{\"952\":1}}],[\"给定一张输入图像\",{\"1\":{\"214\":1,\"229\":1,\"234\":1}}],[\"给定一条文字描述和一张候选图像\",{\"1\":{\"889\":1}}],[\"给定一对文本\",{\"1\":{\"887\":1}}],[\"给定一个符号序列\",{\"1\":{\"640\":1}}],[\"给定一个无监督学习的语料tokens\",{\"1\":{\"629\":1}}],[\"给定一个图像\",{\"1\":{\"371\":1}}],[\"给定一个点云\",{\"1\":{\"143\":1}}],[\"给定一个\",{\"1\":{\"94\":1,\"373\":1}}],[\"给定两个超参数\",{\"1\":{\"135\":1}}],[\"说白了\",{\"1\":{\"351\":1}}],[\"说明两个人相距越远\",{\"1\":{\"915\":1}}],[\"说明偏好并非训练数据过拟合造成\",{\"1\":{\"657\":1}}],[\"说明标注者之间达成了较高的一致性\",{\"1\":{\"656\":1}}],[\"说明你更讨厌\",{\"1\":{\"590\":2}}],[\"说明它们强烈正相关\",{\"1\":{\"574\":1}}],[\"说明是原地修改\",{\"1\":{\"479\":1}}],[\"说明视觉语言专家有助于捕获更多的模态交互信息\",{\"1\":{\"376\":1}}],[\"说明生成式训练本身就包含了分类能力\",{\"1\":{\"276\":1}}],[\"说明遮挡带来的表示学习能力是关键\",{\"1\":{\"242\":1}}],[\"说明全局表示更有效\",{\"1\":{\"215\":1}}],[\"说明\",{\"1\":{\"215\":1,\"343\":1,\"402\":1,\"444\":1,\"451\":1,\"503\":1,\"505\":1,\"514\":1,\"544\":1,\"578\":1}}],[\"说明浅层\",{\"1\":{\"215\":1}}],[\"说明其任务执行能力更强\",{\"1\":{\"657\":1}}],[\"说明其确实有效增强了语言\",{\"1\":{\"99\":1}}],[\"说明其具备良好的泛化能力与鲁棒性\",{\"1\":{\"49\":1}}],[\"说话越有分量\",{\"1\":{\"145\":1}}],[\"谁写了\",{\"1\":{\"733\":1,\"737\":1}}],[\"谁\",{\"1\":{\"658\":1}}],[\"谁离我越近\",{\"1\":{\"145\":1}}],[\"谁在和物体交互\",{\"1\":{\"83\":1}}],[\"维离散值\",{\"1\":{\"885\":1}}],[\"维求和\",{\"1\":{\"709\":1}}],[\"维持\",{\"1\":{\"657\":1}}],[\"维内部隐藏层\",{\"1\":{\"633\":1}}],[\"维隐藏层\",{\"1\":{\"633\":1}}],[\"维是\",{\"1\":{\"472\":3}}],[\"维第\",{\"1\":{\"463\":1}}],[\"维和第\",{\"1\":{\"426\":1}}],[\"维护队列状态的代码实现如下所示\",{\"1\":{\"364\":1}}],[\"维护一个队列\",{\"1\":{\"353\":1}}],[\"维\",{\"1\":{\"362\":1,\"426\":1,\"709\":3,\"886\":1,\"926\":1}}],[\"维空间\",{\"1\":{\"293\":1}}],[\"维的\",{\"1\":{\"926\":1}}],[\"维的概率分布\",{\"1\":{\"285\":1}}],[\"维的向量空间中\",{\"1\":{\"709\":1}}],[\"维的向量\",{\"1\":{\"266\":1}}],[\"维特征\",{\"1\":{\"145\":1}}],[\"维度通常为\",{\"1\":{\"963\":1}}],[\"维度间的复杂依赖会增加难度\",{\"1\":{\"943\":1}}],[\"维度是原始图像的通道数\",{\"1\":{\"899\":1}}],[\"维度是\",{\"1\":{\"899\":1}}],[\"维度从\",{\"1\":{\"893\":1}}],[\"维度可广播\",{\"1\":{\"710\":1}}],[\"维度的参数\",{\"1\":{\"707\":1}}],[\"维度的编码值的变化\",{\"1\":{\"706\":1}}],[\"维度的比例\",{\"1\":{\"380\":1}}],[\"维度统一\",{\"1\":{\"663\":1}}],[\"维度问题\",{\"0\":{\"547\":1}}],[\"维度相同\",{\"1\":{\"506\":1}}],[\"维度友好\",{\"1\":{\"500\":1}}],[\"维度灾难\",{\"1\":{\"500\":1}}],[\"维度诅咒的缓解\",{\"1\":{\"500\":1}}],[\"维度上移动\",{\"1\":{\"489\":3}}],[\"维度规则\",{\"1\":{\"475\":1}}],[\"维度必须兼容\",{\"1\":{\"472\":1}}],[\"维度变为\",{\"1\":{\"418\":1}}],[\"维度变换\",{\"1\":{\"122\":1,\"478\":1}}],[\"维度为\",{\"1\":{\"359\":1,\"417\":2,\"418\":2,\"419\":9,\"424\":1}}],[\"维度扩展系数\",{\"1\":{\"120\":1}}],[\"维度扩展到64\",{\"1\":{\"60\":1}}],[\"维度求和\",{\"1\":{\"100\":1}}],[\"维度\",{\"1\":{\"70\":1,\"91\":1,\"100\":3,\"145\":1,\"150\":1,\"157\":2,\"215\":1,\"293\":1,\"357\":1,\"380\":5,\"529\":2,\"542\":4,\"709\":2,\"710\":1,\"892\":1,\"898\":1,\"899\":1,\"964\":3}}],[\"维度同上\",{\"1\":{\"54\":1}}],[\"维度展平\",{\"1\":{\"54\":1}}],[\"欧几里得距离适合所有维度的尺度和方差差不多时\",{\"1\":{\"579\":1}}],[\"欧几里得距离计算方式\",{\"1\":{\"578\":1}}],[\"欧几里得距离\",{\"0\":{\"576\":1}}],[\"欧氏距离计算\",{\"1\":{\"963\":1}}],[\"欧氏距离\",{\"1\":{\"213\":1}}],[\"欧氏距离平方\",{\"1\":{\"145\":1}}],[\"欧式距离的均匀性假设\",{\"1\":{\"135\":1}}],[\"格式化\",{\"1\":{\"834\":1}}],[\"格式为\",{\"1\":{\"188\":1}}],[\"格式\",{\"1\":{\"145\":1,\"266\":1}}],[\"格式输出\",{\"1\":{\"64\":1}}],[\"←\",{\"1\":{\"144\":3}}],[\"少量则通过\",{\"1\":{\"656\":1}}],[\"少量点无法覆盖关键结构\",{\"1\":{\"157\":1}}],[\"少量点的坐标\",{\"1\":{\"145\":1}}],[\"少量点\",{\"1\":{\"121\":1}}],[\"少样本学习\",{\"1\":{\"647\":1}}],[\"少样本任务迁移\",{\"1\":{\"346\":1}}],[\"少\",{\"1\":{\"143\":1}}],[\"墙壁等\",{\"1\":{\"143\":1}}],[\"椅子\",{\"1\":{\"143\":1,\"146\":1}}],[\"椅子对应的点云一共1000个\",{\"1\":{\"53\":1}}],[\"要是可以把\",{\"1\":{\"959\":1}}],[\"要么在某处发生跳跃\",{\"1\":{\"946\":1}}],[\"要么忽略\",{\"1\":{\"946\":1}}],[\"要么对所有点做操作\",{\"1\":{\"131\":1}}],[\"要么对一个点做操作\",{\"1\":{\"131\":1}}],[\"要排除错误样本\",{\"1\":{\"944\":1}}],[\"要先输入空图像\",{\"1\":{\"925\":1}}],[\"要根据第\",{\"1\":{\"921\":1}}],[\"要接近先验分布\",{\"1\":{\"885\":1}}],[\"要实现这一点\",{\"1\":{\"878\":1}}],[\"要知道\",{\"1\":{\"846\":1}}],[\"要知道像\",{\"1\":{\"610\":1}}],[\"要和分解的问题\",{\"1\":{\"622\":1}}],[\"要具体\",{\"0\":{\"618\":1}}],[\"要明确\",{\"0\":{\"618\":1}}],[\"要让llm给出的结果尽可能地合理\",{\"1\":{\"616\":1}}],[\"要想开发自己的大模型几乎不可能\",{\"1\":{\"610\":1}}],[\"要想在自己的服务中接入大模型的能力\",{\"1\":{\"601\":1}}],[\"要训练一个特定的模型\",{\"1\":{\"606\":1}}],[\"要适配特定的下游任务\",{\"1\":{\"606\":1}}],[\"要在个性化的服务中使用大模型的能力\",{\"1\":{\"601\":1}}],[\"要对大模型进行微调\",{\"1\":{\"601\":1}}],[\"要退出当前激活的环境\",{\"1\":{\"552\":1}}],[\"要跳到下一列只需要移动\",{\"1\":{\"542\":1}}],[\"要跳到下一行需要移动\",{\"1\":{\"542\":1}}],[\"要跳到下一个通道需要移动\",{\"1\":{\"542\":1}}],[\"要跳到下一个\",{\"1\":{\"542\":1}}],[\"要逼近复杂函数\",{\"1\":{\"500\":1}}],[\"要返回的最大\",{\"1\":{\"488\":1}}],[\"要加到\",{\"1\":{\"487\":2}}],[\"要分成的块数\",{\"1\":{\"482\":1}}],[\"要注册的张量\",{\"1\":{\"474\":1}}],[\"要求是\",{\"1\":{\"899\":1}}],[\"要求编码器的输出分布\",{\"1\":{\"885\":1}}],[\"要求\",{\"1\":{\"885\":1}}],[\"要求张量是连续的\",{\"1\":{\"492\":1}}],[\"要求输入张量必须是内存连续的\",{\"1\":{\"492\":1}}],[\"要求目录结构为\",{\"1\":{\"293\":1}}],[\"要求其自回归地预测文本\",{\"1\":{\"268\":1}}],[\"要求最小包含\",{\"1\":{\"234\":1}}],[\"要小得多\",{\"1\":{\"224\":1}}],[\"要采样的质心点数量\",{\"1\":{\"141\":1}}],[\"之一\",{\"1\":{\"823\":1}}],[\"之前\",{\"1\":{\"710\":1}}],[\"之前的工作提出了在迁移表征顶部学习特定任务的架构\",{\"1\":{\"631\":1}}],[\"之前的工作中\",{\"1\":{\"356\":1}}],[\"之前已经有一些研究工作探讨了使用文本作为监督信号来训练视觉模型\",{\"1\":{\"413\":1}}],[\"之前也有很多优秀的无监督工作\",{\"1\":{\"353\":1}}],[\"之后让每个子像素的颜色取值分布由之前所有的子像素决定\",{\"1\":{\"925\":1}}],[\"之后让模型预测和还原被遮盖掉或替换掉的部分\",{\"1\":{\"691\":1}}],[\"之后每一个卷积层都使用\",{\"1\":{\"923\":1}}],[\"之后的像素\",{\"1\":{\"921\":1}}],[\"之后我们要在这两个句子中加一些特殊的\",{\"1\":{\"692\":1}}],[\"之后做\",{\"1\":{\"691\":1}}],[\"之后可以用\",{\"1\":{\"474\":1}}],[\"之后\",{\"1\":{\"405\":1,\"694\":1,\"805\":1,\"877\":1,\"892\":1,\"958\":2}}],[\"之后concat形成该区域提取的总特征\",{\"1\":{\"141\":1}}],[\"之后这些不同尺度上提取的特征被串联起来\",{\"1\":{\"140\":1}}],[\"之间切换\",{\"1\":{\"823\":1}}],[\"之间可能提供最佳平衡\",{\"1\":{\"572\":1}}],[\"之间生成\",{\"1\":{\"440\":1}}],[\"之间仍存在显著差距\",{\"1\":{\"323\":1}}],[\"之间的差异\",{\"1\":{\"945\":1}}],[\"之间的差距\",{\"1\":{\"886\":1}}],[\"之间的损失权重\",{\"1\":{\"589\":2}}],[\"之间的夹角\",{\"1\":{\"506\":1}}],[\"之间的关系\",{\"1\":{\"427\":1}}],[\"之间的性能差距\",{\"1\":{\"322\":1}}],[\"之间的\",{\"1\":{\"260\":1,\"887\":1}}],[\"之间的距离\",{\"1\":{\"246\":1,\"708\":1,\"910\":1,\"932\":1}}],[\"之间的平方欧氏距离\",{\"1\":{\"137\":1}}],[\"之间\",{\"1\":{\"171\":1,\"263\":1,\"734\":1,\"925\":2}}],[\"之间拉了一根线\",{\"1\":{\"83\":1}}],[\"zc\",{\"1\":{\"937\":2}}],[\"z|x\",{\"1\":{\"932\":1,\"935\":1}}],[\"ziegler\",{\"1\":{\"655\":1,\"656\":1}}],[\"zip\",{\"1\":{\"293\":1,\"361\":1,\"363\":1,\"424\":1,\"514\":1,\"700\":1,\"712\":2,\"715\":1,\"751\":1,\"801\":1,\"803\":1,\"805\":1,\"807\":1,\"899\":2}}],[\"zsh\",{\"1\":{\"558\":1}}],[\"zhihu\",{\"1\":{\"927\":1}}],[\"zhuanlan\",{\"1\":{\"927\":1}}],[\"zhandaohong\",{\"1\":{\"712\":1}}],[\"zhang\",{\"1\":{\"671\":1}}],[\"zhou\",{\"1\":{\"655\":1}}],[\"zh\",{\"1\":{\"309\":1,\"332\":1}}],[\"zoo\",{\"1\":{\"202\":1,\"854\":1}}],[\"z\",{\"1\":{\"138\":2,\"159\":2,\"213\":36,\"463\":1,\"469\":1,\"491\":6,\"492\":2,\"513\":1,\"522\":1,\"578\":1,\"803\":1,\"811\":12,\"815\":7,\"816\":2,\"918\":4,\"931\":5,\"932\":2,\"935\":7,\"937\":5,\"939\":2,\"959\":3,\"963\":32,\"964\":2}}],[\"zero\",{\"1\":{\"105\":1,\"187\":1,\"190\":1,\"192\":1,\"204\":1,\"213\":7,\"265\":1,\"273\":2,\"293\":1,\"309\":2,\"339\":1,\"359\":1,\"381\":2,\"626\":1,\"638\":1,\"641\":1,\"646\":1,\"647\":1,\"703\":1,\"713\":2,\"823\":1,\"883\":1,\"890\":1,\"918\":2,\"926\":1,\"934\":1,\"938\":1,\"963\":1,\"964\":1}}],[\"zeros\",{\"0\":{\"486\":1},\"1\":{\"83\":1,\"105\":2,\"106\":3,\"107\":3,\"119\":4,\"121\":2,\"122\":1,\"137\":2,\"190\":2,\"192\":3,\"205\":1,\"206\":1,\"207\":1,\"213\":4,\"263\":1,\"293\":1,\"361\":1,\"362\":1,\"380\":5,\"384\":1,\"385\":1,\"386\":5,\"419\":1,\"427\":1,\"428\":2,\"431\":2,\"486\":2,\"710\":2,\"716\":1,\"729\":1,\"893\":1,\"926\":1,\"964\":1}}],[\"质量很差\",{\"1\":{\"944\":1}}],[\"质量\",{\"1\":{\"656\":1}}],[\"质量高于\",{\"1\":{\"640\":1}}],[\"质量和多样性\",{\"1\":{\"330\":1}}],[\"质点数量\",{\"1\":{\"137\":1}}],[\"质心\",{\"1\":{\"137\":2}}],[\"我将先以易懂的逻辑带领大家一步一步领悟vq\",{\"1\":{\"955\":1}}],[\"我将两者的结构进行对比\",{\"1\":{\"429\":1}}],[\"我的调试文件是run\",{\"1\":{\"712\":1}}],[\"我的狗很可爱\",{\"1\":{\"692\":2}}],[\"我已经上传到了仓库中\",{\"1\":{\"712\":1}}],[\"我就不上传了\",{\"1\":{\"712\":1}}],[\"我现在要把你的数据重新整理成行优先\",{\"1\":{\"545\":1}}],[\"我不管你现在怎么解读这块内存\",{\"1\":{\"545\":1}}],[\"我不感兴趣\",{\"1\":{\"137\":1}}],[\"我认为为了保持正样本的定义\",{\"1\":{\"357\":1}}],[\"我自己理解\",{\"1\":{\"356\":1}}],[\"我来帮你把\",{\"1\":{\"271\":1}}],[\"我这个点最近的3个熟人是谁\",{\"1\":{\"145\":1}}],[\"我们多加一个\",{\"1\":{\"960\":1}}],[\"我们虽然认识了vq\",{\"1\":{\"957\":1}}],[\"我们必须设定\",{\"1\":{\"951\":1}}],[\"我们必须推动\",{\"1\":{\"946\":1}}],[\"我们实际上并没有在精确地优化\",{\"1\":{\"948\":1}}],[\"我们实现了自动构建计算图与反向传播的基本机制\",{\"1\":{\"814\":1}}],[\"我们该如何优化\",{\"1\":{\"945\":1}}],[\"我们假设数据来自一个未知的真实分布\",{\"1\":{\"942\":1}}],[\"我们假设图像共有\",{\"1\":{\"932\":1}}],[\"我们假设图像每个像素的值是伯努利分布\",{\"1\":{\"932\":1}}],[\"我们学习的编码器\",{\"1\":{\"932\":1}}],[\"我们学习和利用了\",{\"1\":{\"800\":1}}],[\"我们完成了梯度的传递\",{\"1\":{\"959\":1}}],[\"我们完成了计算图与手动反向传播的雏形\",{\"1\":{\"799\":1}}],[\"我们完全可以用黑\",{\"1\":{\"925\":1}}],[\"我们设卷积核的形状是\",{\"1\":{\"924\":1}}],[\"我们设计了一个新颖的框架\",{\"1\":{\"50\":1}}],[\"我们能明白为什么只能先用一次\",{\"1\":{\"923\":1}}],[\"我们观察图像中心处的像素在每次卷积后的感受野\",{\"1\":{\"923\":1}}],[\"我们来讨论一下嵌入空间的训练方法\",{\"1\":{\"960\":1}}],[\"我们来详细探究这些细节\",{\"1\":{\"957\":1}}],[\"我们来探讨一下网络的实现细节\",{\"1\":{\"921\":1}}],[\"我们来一步步分析这个过程\",{\"1\":{\"523\":1}}],[\"我们根据序号\",{\"1\":{\"921\":1}}],[\"我们得到了一个\",{\"1\":{\"921\":1}}],[\"我们倾向于估计硬币出现正面的概率为0\",{\"1\":{\"903\":1}}],[\"我们很自然地认为这枚硬币不是公平的\",{\"1\":{\"903\":1}}],[\"我们保留了大约\",{\"1\":{\"887\":1}}],[\"我们固定\",{\"1\":{\"887\":1}}],[\"我们无法使用重参数化梯度来最大化它\",{\"1\":{\"886\":1}}],[\"我们没法直接优化\",{\"1\":{\"885\":1}}],[\"我们训练一个离散变分自编码器\",{\"1\":{\"885\":1}}],[\"我们训练一个学生网络\",{\"1\":{\"285\":1}}],[\"我们直觉上会认为\",{\"1\":{\"872\":1}}],[\"我们直接用自然语言丢给他就去执行就好了\",{\"1\":{\"616\":1}}],[\"我们直接梯度回传就可以了\",{\"1\":{\"357\":1}}],[\"我们直接使用一个简单的线性分类器作为任务层\",{\"1\":{\"238\":1}}],[\"我们直接使用训练好的图像\",{\"1\":{\"236\":1}}],[\"我们应该修改模型\",{\"1\":{\"921\":1}}],[\"我们应该预期这些样本大多会落在空间的哪里\",{\"1\":{\"872\":1}}],[\"我们应该进行实际业务测试\",{\"1\":{\"836\":1}}],[\"我们简称为student\",{\"1\":{\"867\":1}}],[\"我们简写为\",{\"1\":{\"865\":1}}],[\"我们简要介绍三个\",{\"1\":{\"825\":1}}],[\"我们讨论一些定义在实数集合\",{\"1\":{\"864\":1}}],[\"我们继续抽球\",{\"1\":{\"860\":1}}],[\"我们进行有放回抽样\",{\"1\":{\"859\":1}}],[\"我们进一步分析了\",{\"1\":{\"376\":1}}],[\"我们为每个\",{\"1\":{\"847\":1}}],[\"我们为variable类添加\",{\"1\":{\"809\":1}}],[\"我们定义概率\",{\"1\":{\"847\":1}}],[\"我们不希望人为规定\",{\"1\":{\"944\":1}}],[\"我们不妨来做一个实验\",{\"1\":{\"923\":1}}],[\"我们不能对\",{\"1\":{\"847\":1}}],[\"我们不需要知道第一张和第二张图片是人\",{\"1\":{\"349\":1}}],[\"我们称\",{\"1\":{\"846\":1}}],[\"我们称之为transformation\",{\"1\":{\"350\":1}}],[\"我们这么定义\",{\"1\":{\"846\":1}}],[\"我们关心的是\",{\"1\":{\"846\":1}}],[\"我们就可以用pixelcnn生成离散编码\",{\"1\":{\"956\":1}}],[\"我们就可以扔掉编码器\",{\"1\":{\"956\":1}}],[\"我们就会使用上述的高斯分布\",{\"1\":{\"951\":1}}],[\"我们就不需要真实的\",{\"1\":{\"947\":1}}],[\"我们就真正最大化了\",{\"1\":{\"945\":1}}],[\"我们就等于是在并行地训练n个样本\",{\"1\":{\"921\":1}}],[\"我们就得到了后验分布\",{\"1\":{\"877\":1}}],[\"我们就完成了应用的核心功能\",{\"1\":{\"836\":1}}],[\"我们就用该查询点最近的那个点\",{\"1\":{\"137\":1}}],[\"我们推荐基于\",{\"1\":{\"836\":1}}],[\"我们一般可以将大模型开发分解为以下几个流程\",{\"1\":{\"836\":1}}],[\"我们一般不会去大幅度改动模型\",{\"1\":{\"835\":1}}],[\"我们后续会用到的处理文档\",{\"1\":{\"834\":1}}],[\"我们开始迈入更深入也更贴近真实深度学习框架设计的阶段\",{\"1\":{\"814\":1}}],[\"我们引入一个新的分布函数\",{\"1\":{\"945\":1}}],[\"我们引入释放中间变量导数的机制\",{\"1\":{\"807\":1}}],[\"我们引入了目前最大规模的\",{\"1\":{\"50\":1}}],[\"我们便可以根据常规的卷积模型搭建策略来实现\",{\"1\":{\"926\":1}}],[\"我们便可以通过\",{\"1\":{\"805\":1}}],[\"我们便可以采样dino模型代码中采用的注意力掩码矩阵技巧\",{\"1\":{\"582\":1}}],[\"我们要算的事件是\",{\"1\":{\"850\":1}}],[\"我们要问\",{\"1\":{\"850\":1}}],[\"我们要先计算出其梯度后\",{\"1\":{\"804\":1}}],[\"我们要计算输入序列第\",{\"1\":{\"706\":1}}],[\"我们构建了如下关键功能\",{\"1\":{\"812\":1}}],[\"我们构建了变量\",{\"1\":{\"799\":1}}],[\"我们构造损坏图像的\",{\"1\":{\"234\":1}}],[\"我们从\",{\"1\":{\"945\":1}}],[\"我们从互联网收集了\",{\"1\":{\"888\":1}}],[\"我们从零开始构建了tinypytorch框架的基础功能\",{\"1\":{\"797\":1}}],[\"我们从原始输入的\",{\"1\":{\"735\":1}}],[\"我们从flower\",{\"1\":{\"410\":1}}],[\"我们主要使用它来预测答案的起始和结束位置\",{\"1\":{\"733\":1}}],[\"我们已经学完了vq\",{\"1\":{\"956\":1}}],[\"我们已经在对从数据集\",{\"1\":{\"946\":1}}],[\"我们已经有了\",{\"1\":{\"710\":1}}],[\"我们已经探讨了clip模型的运作机制\",{\"1\":{\"408\":1}}],[\"我们想得到一个由3个整数构成的离散编码\",{\"1\":{\"958\":1}}],[\"我们想得到一个结果\",{\"1\":{\"349\":1}}],[\"我们想要从\",{\"1\":{\"931\":1}}],[\"我们想算\",{\"1\":{\"709\":1}}],[\"我们打印了位置\",{\"1\":{\"706\":1}}],[\"我们用它来衡量\",{\"1\":{\"945\":1}}],[\"我们用一个预训练的对比模型\",{\"1\":{\"889\":1}}],[\"我们用一个可学习的嵌入向量\",{\"1\":{\"234\":1}}],[\"我们用3面而不是6面\",{\"1\":{\"846\":1}}],[\"我们用蓝色的向量和所有黄色向量进行\",{\"1\":{\"694\":1}}],[\"我们给上句的\",{\"1\":{\"692\":1}}],[\"我们给一个输入文本\",{\"1\":{\"660\":1}}],[\"我们传入的是\",{\"1\":{\"663\":1}}],[\"我们到底在\",{\"1\":{\"658\":1}}],[\"我们教小孩做应用题\",{\"1\":{\"620\":1}}],[\"我们让llm对一段文字进行总结\",{\"1\":{\"618\":1}}],[\"我们发给llm的批令\",{\"1\":{\"618\":1}}],[\"我们发现这种做法虽然验证集损失更高\",{\"1\":{\"887\":1}}],[\"我们发现这些值呈现周期性的变化\",{\"1\":{\"706\":1}}],[\"我们发现倒数第四层的特征在多模态任务中表现最佳\",{\"1\":{\"330\":1}}],[\"我们发现\",{\"1\":{\"243\":1,\"886\":1}}],[\"我们发现恰当的初始化对于\",{\"1\":{\"236\":1}}],[\"我们看一下矩阵\",{\"1\":{\"612\":1}}],[\"我们看重无监督学习的优点\",{\"1\":{\"352\":1}}],[\"我们微调大模型的流程就变为了\",{\"1\":{\"609\":1}}],[\"我们所说的\",{\"1\":{\"600\":1}}],[\"我们先来制订一下\",{\"1\":{\"959\":1}}],[\"我们先来看\",{\"1\":{\"213\":1,\"274\":1}}],[\"我们先从变分贝叶斯方法的基础出发\",{\"1\":{\"945\":1}}],[\"我们先用\",{\"1\":{\"923\":1}}],[\"我们先只考虑单通道图像\",{\"1\":{\"921\":1}}],[\"我们先给小孩子分析讲解一些示例\",{\"1\":{\"620\":1}}],[\"我们先对大模型做一个直观的抽象\",{\"1\":{\"600\":1}}],[\"我们取其补集\",{\"1\":{\"588\":1}}],[\"我们选择的阈值取决于哪个指标对特定用例而言最重要\",{\"1\":{\"572\":1}}],[\"我们知道了\",{\"1\":{\"958\":1}}],[\"我们知道\",{\"1\":{\"540\":1,\"850\":1}}],[\"我们生成一个3\",{\"1\":{\"540\":1}}],[\"我们常常需要衡量文本嵌入和图片嵌入之间的相似度\",{\"1\":{\"410\":1}}],[\"我们也可以将自然语言理解看作是一个病态问题\",{\"1\":{\"878\":1}}],[\"我们也可以考虑结果为连续值的实验\",{\"1\":{\"847\":1}}],[\"我们也可以发现\",{\"1\":{\"706\":1}}],[\"我们也可以对得到的余弦相似度计算softmax\",{\"1\":{\"408\":1}}],[\"我们也尝试过随机初始化直接进行视觉语言预训练\",{\"1\":{\"376\":1}}],[\"我们也展示了通过联合\",{\"1\":{\"252\":1}}],[\"我们比较了两种初始化方式\",{\"1\":{\"376\":1}}],[\"我们是放到字典中去的\",{\"1\":{\"356\":1}}],[\"我们最大化关于\",{\"1\":{\"886\":1}}],[\"我们最开始右边分支的编码器是由左边初始化而来\",{\"1\":{\"353\":1}}],[\"我们最多随机遮挡\",{\"1\":{\"236\":1}}],[\"我们其实可以把key集合看成字典\",{\"1\":{\"353\":1}}],[\"我们把两类掩码卷积分别称为\",{\"1\":{\"923\":1}}],[\"我们把\",{\"1\":{\"850\":1}}],[\"我们把某个位置的向量大概划分为两部分\",{\"1\":{\"706\":1}}],[\"我们把它加到第二个\",{\"1\":{\"706\":1}}],[\"我们把字典中的\",{\"1\":{\"357\":1}}],[\"我们把f11叫做\",{\"1\":{\"353\":1}}],[\"我们把x11这个图片叫做\",{\"1\":{\"353\":1}}],[\"我们接着来看图\",{\"1\":{\"353\":1}}],[\"我们随机选择一个图片\",{\"1\":{\"350\":1}}],[\"我们随机遮挡大约\",{\"1\":{\"234\":1}}],[\"我们并没有用到标签信息\",{\"1\":{\"349\":1}}],[\"我们并不需要深研大模型内部原理\",{\"1\":{\"835\":1}}],[\"我们并不需要大量标注好的数据\",{\"1\":{\"348\":1}}],[\"我们并不是简单地随机选择\",{\"1\":{\"234\":1}}],[\"我们需要的是一种算法\",{\"1\":{\"952\":1}}],[\"我们需要更具体地定义\",{\"1\":{\"946\":1}}],[\"我们需要通过\",{\"1\":{\"892\":1}}],[\"我们需要通过prompt\",{\"1\":{\"619\":1}}],[\"我们需要借助borel\",{\"1\":{\"847\":1}}],[\"我们需要逐步迭代构建优质的\",{\"1\":{\"836\":1}}],[\"我们需要收集数据并进行预处理\",{\"1\":{\"836\":1}}],[\"我们需要针对我们所设计的功能\",{\"1\":{\"836\":1}}],[\"我们需要让variable实例能与numpy数组\",{\"1\":{\"809\":1}}],[\"我们需要让这三个表征在特征空间中\",{\"1\":{\"349\":1}}],[\"我们需要实现mul类来处理正向传播和反向传播\",{\"1\":{\"809\":1}}],[\"我们需要在每次计算之前将导数重置为0\",{\"1\":{\"802\":1}}],[\"我们需要在其对应的crossentropyloss中指定ignore\",{\"1\":{\"700\":1}}],[\"我们需要读取并构建batch数据\",{\"1\":{\"698\":1}}],[\"我们需要先将\",{\"1\":{\"501\":1}}],[\"我们需要根据上面给出的花卉数据集下载链接\",{\"1\":{\"410\":1}}],[\"我们需要知道的是\",{\"1\":{\"349\":1}}],[\"我们需要重点关注应用于图像之上的transform操作\",{\"1\":{\"264\":1}}],[\"我们现在看到的这些大语言模型\",{\"1\":{\"606\":1}}],[\"我们现在把这三张图片输入一个模型\",{\"1\":{\"349\":1}}],[\"我们现在有三张图\",{\"1\":{\"349\":1}}],[\"我们同时对每个视频的文本描述进行编码\",{\"1\":{\"273\":1}}],[\"我们只对哪些集合定义概率\",{\"1\":{\"847\":1}}],[\"我们只会计算被随机遮盖或替换的部分\",{\"1\":{\"700\":1}}],[\"我们只需知道vae可以把图片编码成符合标准正态分布的向量即可\",{\"1\":{\"956\":1}}],[\"我们只需从\",{\"1\":{\"952\":1}}],[\"我们只需要找到一种函数\",{\"1\":{\"949\":1}}],[\"我们只需要如下修改variable变量的backward方法即可完成按照辈分获取函数的逻辑\",{\"1\":{\"805\":1}}],[\"我们只需要确保对于某个词的上下文融合不被pad词参与即可\",{\"1\":{\"703\":1}}],[\"我们只需要在计算出相似度得分矩阵后\",{\"1\":{\"411\":1}}],[\"我们只需要经过模型\",{\"1\":{\"350\":1}}],[\"我们只需学习一个新的\",{\"1\":{\"273\":1}}],[\"我们只使用这一种预训练任务\",{\"1\":{\"223\":1}}],[\"我们严格遵循\",{\"1\":{\"273\":1}}],[\"我们还可以从标准正态分布中采样一个向量\",{\"1\":{\"935\":1}}],[\"我们还需要处理一个更复杂的问题\",{\"1\":{\"924\":1}}],[\"我们还发现\",{\"1\":{\"886\":1}}],[\"我们还用了\",{\"1\":{\"618\":1}}],[\"我们还有其他的选择\",{\"1\":{\"409\":1}}],[\"我们还进行了\",{\"1\":{\"376\":1}}],[\"我们还提升了预训练数据集的规模\",{\"1\":{\"330\":1}}],[\"我们还探索了两个更小的变体\",{\"1\":{\"272\":1}}],[\"我们还展示了\",{\"1\":{\"225\":1}}],[\"我们提出的\",{\"1\":{\"272\":1}}],[\"我们提出了一种解耦的解码器设计\",{\"1\":{\"272\":1}}],[\"我们提出了一种\",{\"1\":{\"201\":1,\"234\":1}}],[\"我们提出了一种开放词汇形式的\",{\"1\":{\"50\":1}}],[\"我们提出了\",{\"1\":{\"109\":1,\"252\":1}}],[\"我们首先应该明确\",{\"1\":{\"836\":1}}],[\"我们首先需要确定开发的目标\",{\"1\":{\"836\":1}}],[\"我们首先拿到属于上下文的一对句子\",{\"1\":{\"692\":1}}],[\"我们首先获取图片库中所有图片\",{\"1\":{\"411\":1}}],[\"我们首先创建了各类别的文本描述\",{\"1\":{\"408\":1}}],[\"我们首先用一幅图理清楚\",{\"1\":{\"381\":1}}],[\"我们首先进行了阶段式预训练的消融实验\",{\"1\":{\"376\":1}}],[\"我们首先回顾三类利用自然语言监督的基础模型家族\",{\"1\":{\"270\":1}}],[\"我们首先将所有参数在一个较小范围内随机初始化\",{\"1\":{\"236\":1}}],[\"我们希望建立一个模型\",{\"1\":{\"952\":1}}],[\"我们希望估计其\",{\"1\":{\"944\":1}}],[\"我们希望从一个高斯分布中采样隐变量\",{\"1\":{\"931\":1}}],[\"我们希望得到\",{\"1\":{\"885\":1}}],[\"我们希望使用定义在非负实数上的分布\",{\"1\":{\"866\":1}}],[\"我们希望每次计算都能得到正确的导数\",{\"1\":{\"802\":1}}],[\"我们希望输出一个\",{\"1\":{\"501\":1}}],[\"我们希望在特征空间里\",{\"1\":{\"349\":1}}],[\"我们希望\",{\"1\":{\"258\":1}}],[\"我们相信\",{\"1\":{\"252\":1}}],[\"我们相信该研究将为视觉可供性理解领域带来新的启发并推动其发展\",{\"1\":{\"50\":1}}],[\"我们的讨论都是建立在嵌入空间已经训练完毕的前提上的\",{\"1\":{\"960\":1}}],[\"我们的目标是训练一个\",{\"1\":{\"885\":1}}],[\"我们的实验从图像预训练开始\",{\"1\":{\"376\":1}}],[\"我们的初步实验发现\",{\"1\":{\"272\":1}}],[\"我们的最大模型\",{\"1\":{\"272\":1}}],[\"我们的工作首次提出以\",{\"1\":{\"252\":1}}],[\"我们的方法则从所有\",{\"1\":{\"376\":1}}],[\"我们的方法中将图像表示为两种形式\",{\"1\":{\"230\":1}}],[\"我们的方法能使用更小的\",{\"1\":{\"223\":1}}],[\"我们分析了\",{\"1\":{\"242\":1}}],[\"我们可以设计一个把梯度从\",{\"1\":{\"959\":1}}],[\"我们可以借鉴nlp中对于离散单词的处理方法\",{\"1\":{\"956\":1}}],[\"我们可以借助预训练好的clip模型\",{\"1\":{\"900\":1}}],[\"我们可以明确地\",{\"1\":{\"951\":1}}],[\"我们可以人为\",{\"1\":{\"951\":1}}],[\"我们可以思考一个问题\",{\"1\":{\"951\":1}}],[\"我们可以从信息论的角度\",{\"1\":{\"950\":1}}],[\"我们可以从以下角度理解\",{\"1\":{\"530\":1}}],[\"我们可以\",{\"1\":{\"947\":1}}],[\"我们可以对多个样本的\",{\"1\":{\"946\":1}}],[\"我们可以对ltm\",{\"1\":{\"622\":1}}],[\"我们可以将之前的公式\",{\"1\":{\"952\":1}}],[\"我们可以将标准正态的\",{\"1\":{\"944\":1}}],[\"我们可以将每个样本空间中的结果映射为一个实数\",{\"1\":{\"846\":1}}],[\"我们可以说\",{\"1\":{\"925\":1}}],[\"我们可以用任意一种\",{\"1\":{\"923\":1}}],[\"我们可以用一句话来总结贝叶斯公式\",{\"1\":{\"877\":1}}],[\"我们可以在一次前向传播后得到图像每一处的概率分布\",{\"1\":{\"921\":1}}],[\"我们可以在一个数据量更丰富的中间数据集\",{\"1\":{\"240\":1}}],[\"我们可以让模型输入一幅图像\",{\"1\":{\"921\":1}}],[\"我们可以提出这样一个问题\",{\"1\":{\"872\":1}}],[\"我们可以使用采样来估计这个期望\",{\"1\":{\"946\":1}}],[\"我们可以使用贝叶斯公式来计算后验概率\",{\"1\":{\"878\":1}}],[\"我们可以使用分类分布\",{\"1\":{\"857\":1}}],[\"我们可以使用一些代理任务\",{\"1\":{\"349\":1}}],[\"我们可以得到\",{\"1\":{\"847\":1}}],[\"我们可以基于\",{\"1\":{\"836\":1}}],[\"我们可以根据自身需求灵活地进行组合\",{\"1\":{\"832\":1}}],[\"我们可以根据这个类别去学习模型\",{\"1\":{\"353\":1}}],[\"我们可以轻松地构建如下所示的\",{\"1\":{\"831\":1}}],[\"我们可以验证框架是否支持复杂表达式的自动微分\",{\"1\":{\"809\":1}}],[\"我们可以通过以下重参数方式\",{\"1\":{\"951\":1}}],[\"我们可以通过一些简单处理过滤掉第\",{\"1\":{\"921\":1}}],[\"我们可以通过除以\",{\"1\":{\"877\":1}}],[\"我们可以通过变量的辈分来设置其创建者函数的辈分\",{\"1\":{\"805\":1}}],[\"我们可以通改变对内存中数据缓冲区的解读方式来实现逻辑上的转置\",{\"1\":{\"545\":1}}],[\"我们可以获取到哪个函数生成了哪个变量\",{\"1\":{\"805\":1}}],[\"我们可以采用拓扑排序\",{\"1\":{\"805\":1}}],[\"我们可以发现\",{\"1\":{\"706\":1}}],[\"我们可以利用矩阵分解技术\",{\"1\":{\"609\":1}}],[\"我们可以把它换成transformer\",{\"1\":{\"961\":1}}],[\"我们可以把pixelcnn生成图像的方法搬过来\",{\"1\":{\"956\":1}}],[\"我们可以把类似的嵌入层加到vq\",{\"1\":{\"956\":1}}],[\"我们可以把这看作是对\",{\"1\":{\"878\":1}}],[\"我们可以把\",{\"1\":{\"545\":1}}],[\"我们可以直接使用类别标签作为文本描述\",{\"1\":{\"409\":1}}],[\"我们可以打开\",{\"1\":{\"213\":1}}],[\"我们通常只对\",{\"1\":{\"946\":1}}],[\"我们通常假设协方差矩阵是对角的\",{\"1\":{\"944\":1}}],[\"我们通常取其补集\",{\"1\":{\"586\":1}}],[\"我们通常令先验为\",{\"1\":{\"235\":1}}],[\"我们通过引入潜变量\",{\"1\":{\"952\":1}}],[\"我们通过已观测到的样本情况去\",{\"1\":{\"903\":1}}],[\"我们通过变量梯度非空则进行累加的改动\",{\"1\":{\"804\":1}}],[\"我们通过把复杂问题拆解成一个个的简单问题\",{\"1\":{\"622\":1}}],[\"我们通过线性变换得到\",{\"1\":{\"534\":1}}],[\"我们通过自定义一个patchembed类完成上述工作\",{\"1\":{\"426\":1}}],[\"我们通过利用clip模型的多模态能力\",{\"1\":{\"408\":1}}],[\"我们通过一次\",{\"1\":{\"357\":1}}],[\"我们通过一个\",{\"1\":{\"223\":1}}],[\"我们通过最大化带标签数据的似然函数\",{\"1\":{\"238\":1}}],[\"我们通过在线对比难样本挖掘来改进图文匹配\",{\"1\":{\"198\":1}}],[\"我们获得图像编码器作为离散变分自编码器\",{\"1\":{\"235\":1}}],[\"我们使用了一个示例来说明这一概念\",{\"1\":{\"546\":1}}],[\"我们使用了\",{\"1\":{\"409\":1}}],[\"我们使用更简单的方法\",{\"1\":{\"273\":1}}],[\"我们使用一个包含\",{\"1\":{\"236\":1}}],[\"我们使用\",{\"1\":{\"234\":1,\"235\":1,\"236\":1,\"243\":1,\"886\":1,\"924\":1}}],[\"我们使用多模态编码器输出的\",{\"1\":{\"201\":1}}],[\"我们在下一小节里来详细探究一下怎么优化\",{\"1\":{\"958\":1}}],[\"我们在闲暇之余来谈一谈\",{\"1\":{\"925\":1}}],[\"我们在生成当前像素的\",{\"1\":{\"924\":1}}],[\"我们在variable类和function类中增加实例变量generation\",{\"1\":{\"805\":1}}],[\"我们在接下来的部分\",{\"1\":{\"619\":1}}],[\"我们在给llm发指令的时候\",{\"1\":{\"618\":1}}],[\"我们在做对比学习的时候\",{\"1\":{\"353\":1}}],[\"我们在训练中同时激活视觉编码器和mlp投影层\",{\"1\":{\"330\":1}}],[\"我们在internvl\",{\"1\":{\"330\":1}}],[\"我们在该阶段冻结\",{\"1\":{\"306\":1}}],[\"我们在空间和时间特征\",{\"1\":{\"273\":1}}],[\"我们在输入句子末尾附加一个可学习的\",{\"1\":{\"272\":1}}],[\"我们在本文中以图像分类和语义分割为例\",{\"1\":{\"237\":1}}],[\"我们在自监督学习中不使用图像标签\",{\"1\":{\"236\":1}}],[\"我们在\",{\"1\":{\"233\":1,\"237\":1,\"240\":1,\"888\":1}}],[\"我们遵循\",{\"1\":{\"233\":1,\"272\":1}}],[\"我们将基于minist数据集进行训练演示\",{\"1\":{\"963\":1}}],[\"我们将讨论三个关键主题\",{\"1\":{\"948\":1}}],[\"我们将采样从网络内部移动到输入层\",{\"1\":{\"946\":1}}],[\"我们将等式两边都取负号\",{\"1\":{\"945\":1}}],[\"我们将所有输入和输出通道都分成\",{\"1\":{\"924\":1}}],[\"我们将快速过一下\",{\"1\":{\"899\":1}}],[\"我们将文本与图像\",{\"1\":{\"887\":1}}],[\"我们将文本标题的最大长度限制为\",{\"1\":{\"887\":1}}],[\"我们将小写化的标题用\",{\"1\":{\"887\":1}}],[\"我们将初始先验\",{\"1\":{\"886\":1}}],[\"我们将最多\",{\"1\":{\"885\":1}}],[\"我们将详细介绍多元高斯分布\",{\"1\":{\"870\":1}}],[\"我们将组合数\",{\"1\":{\"861\":1}}],[\"我们将随机变量可能的取值集合称为其状态空间\",{\"1\":{\"846\":1}}],[\"我们将概率空间定义为三元组\",{\"1\":{\"845\":1}}],[\"我们将开发以大语言模型为功能核心\",{\"1\":{\"835\":1}}],[\"我们将实现\",{\"1\":{\"819\":1}}],[\"我们将真正迈入\",{\"1\":{\"819\":1}}],[\"我们将看到\",{\"1\":{\"814\":1}}],[\"我们将add类绑定到+运算符\",{\"1\":{\"809\":1}}],[\"我们将mul类封装为python函数mul\",{\"1\":{\"809\":1}}],[\"我们将继续揭开深度学习框架的核心机制\",{\"1\":{\"799\":1}}],[\"我们将从第25步继续出发\",{\"1\":{\"814\":1}}],[\"我们将从\",{\"1\":{\"695\":1}}],[\"我们将进入\",{\"1\":{\"383\":1}}],[\"我们将其编码为图像\",{\"1\":{\"371\":1}}],[\"我们将其划分为\",{\"1\":{\"231\":1,\"234\":1}}],[\"我们将分辨率从224提升至448\",{\"1\":{\"330\":1}}],[\"我们将自注意力模块和前馈网络中子层最后的线性投影矩阵的输出按\",{\"1\":{\"236\":1}}],[\"我们将\",{\"1\":{\"235\":1,\"886\":1}}],[\"我们将图像\",{\"1\":{\"232\":1}}],[\"我们将图像表示为由\",{\"1\":{\"232\":1}}],[\"我们将每张图像\",{\"1\":{\"232\":1}}],[\"我们将每张\",{\"1\":{\"231\":1}}],[\"我们将原始数据集中的网页文本复制\",{\"1\":{\"181\":1}}],[\"我们会说这个是男是女\",{\"1\":{\"956\":1}}],[\"我们会使用\",{\"1\":{\"951\":1}}],[\"我们会使用pad\",{\"1\":{\"703\":1}}],[\"我们会调查\",{\"1\":{\"948\":1}}],[\"我们会从信息论\",{\"1\":{\"948\":1}}],[\"我们会从一张图片中生成多个候选区域\",{\"1\":{\"501\":1}}],[\"我们会探讨\",{\"1\":{\"948\":1}}],[\"我们会在后续进一步探索这种\",{\"1\":{\"945\":1}}],[\"我们会在预训练好的\",{\"1\":{\"229\":1}}],[\"我们会给\",{\"1\":{\"885\":1}}],[\"我们会用到多种概率分布\",{\"1\":{\"854\":1}}],[\"我们会用自然语言描述一系列的推理过程\",{\"1\":{\"620\":1}}],[\"我们会为该任务设计一个最合适的神经网络架构并做训练\",{\"1\":{\"690\":1}}],[\"我们会拼接多个这样的头\",{\"1\":{\"534\":1}}],[\"我们会随机掩码一部分文本\",{\"1\":{\"223\":1}}],[\"我们会根据输入\",{\"1\":{\"222\":1}}],[\"我们采用了类似的两阶段训练流程\",{\"1\":{\"885\":1}}],[\"我们采用了一种简单的方法\",{\"1\":{\"273\":1}}],[\"我们采用更加暴力的\",{\"1\":{\"805\":1}}],[\"我们采用动态高分辨率训练策略\",{\"1\":{\"331\":1}}],[\"我们采用与\",{\"1\":{\"235\":1}}],[\"我们采用\",{\"1\":{\"222\":1,\"232\":1,\"239\":1}}],[\"我们以\",{\"1\":{\"200\":1}}],[\"我们计算的是编码器输出分布\",{\"1\":{\"260\":1}}],[\"我们计算归一化的图像到文本和文本到图像的\",{\"1\":{\"199\":1}}],[\"我们计划构建专用于推理的数据集\",{\"1\":{\"50\":1}}],[\"我们维护两个队列来存储动量单模态编码器最近的\",{\"1\":{\"199\":1}}],[\"我们切分为多个步骤进行解析\",{\"1\":{\"190\":1}}],[\"我们对此进行了控制\",{\"1\":{\"888\":1}}],[\"我们对该分布建模的因式分解为\",{\"1\":{\"885\":1}}],[\"我们对可能出现的结果\",{\"1\":{\"877\":1}}],[\"我们对prompt进行优化\",{\"1\":{\"616\":1}}],[\"我们对不同预训练任务的贡献进行了消融实验\",{\"1\":{\"376\":1}}],[\"我们对比了两种方式\",{\"1\":{\"182\":1}}],[\"我们对\",{\"1\":{\"34\":1,\"198\":1,\"945\":1}}],[\"代替它\",{\"1\":{\"945\":1}}],[\"代入\",{\"1\":{\"885\":1}}],[\"代入调整后的步幅\",{\"1\":{\"546\":1}}],[\"代数就是一种封闭的事件系统\",{\"1\":{\"847\":1}}],[\"代数是一种你可以安全地讨论概率的\",{\"1\":{\"847\":1}}],[\"代数是一个集合的集合\",{\"1\":{\"847\":1}}],[\"代数是由所有形如\",{\"1\":{\"847\":1}}],[\"代数是由半开区间\",{\"1\":{\"847\":1}}],[\"代数是专门为实数空间\",{\"1\":{\"847\":1}}],[\"代数\",{\"1\":{\"847\":7}}],[\"代理服务以及回调处理等关键组件\",{\"1\":{\"833\":1}}],[\"代理\",{\"1\":{\"832\":1}}],[\"代理任务通常是辅助进行表征学习\",{\"1\":{\"413\":1}}],[\"代价是非常高的\",{\"1\":{\"611\":1}}],[\"代表我们因为编码器不完美而付出的额外\",{\"1\":{\"950\":1}}],[\"代表的是模型的基础\",{\"1\":{\"425\":1}}],[\"代表\",{\"1\":{\"359\":2,\"432\":2,\"435\":1,\"823\":1}}],[\"代表性工作包括\",{\"1\":{\"326\":1}}],[\"代表性工作在\",{\"1\":{\"268\":1}}],[\"代表隐空间的离散编码\",{\"1\":{\"257\":1}}],[\"代表每个\",{\"1\":{\"137\":1}}],[\"代表原始点云中每个点的\",{\"1\":{\"137\":1}}],[\"代码执行\",{\"1\":{\"823\":1}}],[\"代码中\",{\"1\":{\"816\":1}}],[\"代码中用\",{\"1\":{\"663\":1}}],[\"代码将变量y的计算图转换为dot语言字符串\",{\"1\":{\"815\":1}}],[\"代码示例\",{\"0\":{\"766\":1}}],[\"代码及新数据集cc\",{\"1\":{\"686\":1}}],[\"代码与学术数据\",{\"1\":{\"667\":1}}],[\"代码生成与数学推理\",{\"1\":{\"668\":1}}],[\"代码生成\",{\"1\":{\"666\":1,\"668\":1,\"669\":1}}],[\"代码描述如下\",{\"1\":{\"660\":1}}],[\"代码任务\",{\"1\":{\"658\":1}}],[\"代码片段\",{\"1\":{\"657\":1}}],[\"代码块类型\",{\"1\":{\"444\":1}}],[\"代码块与作用域对照表\",{\"1\":{\"444\":1}}],[\"代码如下\",{\"1\":{\"293\":1}}],[\"代码解析\",{\"0\":{\"293\":1},\"1\":{\"589\":1}}],[\"代码本大小\",{\"1\":{\"213\":1}}],[\"代码本嵌入使用指数移动平均\",{\"1\":{\"212\":1}}],[\"代码整体流程比较长\",{\"1\":{\"190\":1}}],[\"代码链接\",{\"1\":{\"164\":1,\"193\":1,\"209\":1,\"219\":1,\"226\":1,\"267\":1,\"279\":1,\"294\":1,\"338\":1,\"347\":1,\"366\":1,\"367\":1,\"378\":1,\"387\":1,\"395\":1}}],[\"代码实现逻辑\",{\"1\":{\"385\":1}}],[\"代码实现的同学\",{\"1\":{\"379\":1}}],[\"代码实现如下所示\",{\"1\":{\"380\":1}}],[\"代码实现如下\",{\"1\":{\"138\":1,\"213\":1,\"710\":1}}],[\"代码实现\",{\"0\":{\"92\":1,\"118\":1,\"137\":1,\"144\":1,\"213\":1,\"274\":1,\"734\":1,\"756\":1,\"762\":1,\"771\":1,\"891\":1,\"962\":1},\"1\":{\"152\":1,\"586\":1,\"587\":1,\"588\":1,\"589\":1,\"592\":1,\"811\":3,\"918\":1}}],[\"代码体现\",{\"1\":{\"53\":1}}],[\"代码\",{\"0\":{\"51\":1,\"81\":1,\"151\":1},\"1\":{\"28\":1,\"61\":1,\"71\":1,\"84\":1,\"108\":1,\"124\":1,\"128\":1,\"129\":1,\"414\":1}}],[\"到这里\",{\"1\":{\"956\":1}}],[\"到这个内部函数上\",{\"1\":{\"448\":1}}],[\"到目前为止\",{\"1\":{\"948\":1,\"960\":1}}],[\"到向量的映射\",{\"1\":{\"899\":1}}],[\"到向量存储\",{\"1\":{\"833\":1}}],[\"到对应的位置\",{\"1\":{\"896\":1}}],[\"到原点的平方距离\",{\"1\":{\"874\":1}}],[\"到第\",{\"1\":{\"735\":1}}],[\"到三维\",{\"1\":{\"699\":1}}],[\"到神经网络\",{\"1\":{\"671\":1}}],[\"到上下文感知的循环神经网络\",{\"1\":{\"639\":1}}],[\"到特征图空间\",{\"1\":{\"501\":1}}],[\"到的特征不太好泛化\",{\"1\":{\"355\":1}}],[\"到近年来的vision\",{\"1\":{\"298\":1}}],[\"到序列最前面\",{\"1\":{\"266\":1}}],[\"到一个标准姿态\",{\"1\":{\"152\":1}}],[\"到\",{\"0\":{\"956\":1},\"1\":{\"137\":1,\"293\":1,\"322\":1,\"380\":1,\"440\":1,\"454\":1,\"542\":1,\"570\":1,\"803\":2,\"823\":1,\"845\":1,\"847\":1,\"892\":1,\"924\":2,\"925\":2,\"958\":1,\"959\":1}}],[\"到红色\",{\"1\":{\"107\":1}}],[\"次正面\",{\"1\":{\"904\":1}}],[\"次抛掷中出现\",{\"1\":{\"904\":1}}],[\"次试验的结果\",{\"1\":{\"860\":1}}],[\"次试验中选出\",{\"1\":{\"860\":1}}],[\"次试验中出现的次数\",{\"1\":{\"857\":1}}],[\"次成功的位置\",{\"1\":{\"860\":1}}],[\"次数\",{\"1\":{\"860\":1}}],[\"次失败必须是第\",{\"1\":{\"860\":1}}],[\"次失败\",{\"1\":{\"860\":1}}],[\"次从\",{\"1\":{\"633\":1}}],[\"次并与每个点的局部特征拼接\",{\"1\":{\"157\":1}}],[\"次并与每个点的局部特征\",{\"1\":{\"154\":1}}],[\"次\",{\"1\":{\"137\":1,\"156\":1,\"472\":1,\"923\":1}}],[\"找到\",{\"1\":{\"836\":1}}],[\"找到了rosenbrock函数最小值的位置\",{\"1\":{\"816\":1}}],[\"找到它在输入图像对应的浮点坐标\",{\"1\":{\"505\":1}}],[\"找到它在输入中的\",{\"1\":{\"504\":1}}],[\"找到它最近的\",{\"1\":{\"145\":1}}],[\"找到与文本最匹配的图片\",{\"1\":{\"411\":1,\"412\":1}}],[\"找到其特征的\",{\"1\":{\"286\":1}}],[\"找到最大概率的\",{\"1\":{\"258\":1}}],[\"找到最近的3个邻近点\",{\"1\":{\"145\":1}}],[\"找到每个样本最近的中心\",{\"1\":{\"213\":1}}],[\"找到邻居\",{\"1\":{\"145\":1}}],[\"找到源点云的\",{\"1\":{\"122\":1}}],[\"找出它在嵌入空间里的最近邻\",{\"1\":{\"958\":1}}],[\"找出它周围距离小于\",{\"1\":{\"137\":1}}],[\"找出最可能是\",{\"1\":{\"733\":1}}],[\"找出最近的3个邻近点\",{\"1\":{\"145\":1}}],[\"找出该尺度下每个质心点周围的邻近点\",{\"1\":{\"141\":1}}],[\"找出每个点的局部邻近点\",{\"1\":{\"137\":1}}],[\"距离本身越来越集中在\",{\"1\":{\"874\":1}}],[\"距离或温度\",{\"1\":{\"847\":1}}],[\"距离归一化并取对数\",{\"1\":{\"710\":1}}],[\"距离分组\",{\"1\":{\"710\":1}}],[\"距离较大的\",{\"1\":{\"710\":1}}],[\"距离\",{\"1\":{\"578\":1}}],[\"距离越近\",{\"1\":{\"145\":1}}],[\"距离直观性\",{\"1\":{\"135\":1}}],[\"距离的度量不受空间中位置的影响\",{\"1\":{\"135\":1}}],[\"文章生成和情境理解方面表现出色\",{\"1\":{\"824\":1}}],[\"文章中提炼出来\",{\"1\":{\"696\":1}}],[\"文心大模型包括\",{\"1\":{\"823\":1}}],[\"文心一言网页版分为\",{\"1\":{\"823\":1}}],[\"文心一言的中文能力相对来说非常不错\",{\"1\":{\"823\":1}}],[\"文心一言的基础模型文心大模型于\",{\"1\":{\"823\":1}}],[\"文心一言是基于百度文心大模型的知识增强语言大模型\",{\"1\":{\"823\":1}}],[\"文心一言\",{\"1\":{\"822\":1,\"823\":1}}],[\"文献中也探索了多种控制模型输出的策略\",{\"1\":{\"655\":1}}],[\"文档\",{\"1\":{\"631\":1}}],[\"文档生成\",{\"1\":{\"454\":1}}],[\"文档理解\",{\"1\":{\"335\":1}}],[\"文件转换命令\",{\"1\":{\"815\":1}}],[\"文件中的\",{\"1\":{\"739\":1}}],[\"文件进行调试即可\",{\"1\":{\"712\":1}}],[\"文件顶层的名字都在模块作用域内\",{\"1\":{\"444\":1}}],[\"文件\",{\"1\":{\"444\":1}}],[\"文字搜索图像\",{\"0\":{\"411\":1}}],[\"文字搜索图像实战演练\",{\"1\":{\"404\":1}}],[\"文中将省略函数\",{\"1\":{\"943\":1}}],[\"文中所有用于展示效果和评测的样本\",{\"1\":{\"889\":1}}],[\"文中提到了一些少量使用in\",{\"1\":{\"650\":1}}],[\"文中引用了kaplan等人提出的\",{\"1\":{\"650\":1}}],[\"文中的𝐶所表示的其他信息\",{\"1\":{\"136\":1}}],[\"文中作者通过ball\",{\"1\":{\"135\":1}}],[\"文本掩码\",{\"1\":{\"900\":1}}],[\"文本嵌入维度\",{\"1\":{\"900\":1}}],[\"文本部分的平均交叉熵损失\",{\"1\":{\"893\":1}}],[\"文本部分\",{\"1\":{\"893\":1}}],[\"文本去掉\",{\"1\":{\"893\":1}}],[\"文本序列开头加上\",{\"1\":{\"893\":1}}],[\"文本序列最大长度\",{\"1\":{\"892\":1,\"900\":1}}],[\"文本位置\",{\"1\":{\"900\":1}}],[\"文本位置编码\",{\"1\":{\"892\":1}}],[\"文本位置单独学习一个特殊的填充\",{\"1\":{\"887\":1}}],[\"文本词表大小\",{\"1\":{\"892\":2,\"900\":1}}],[\"文本标题\",{\"1\":{\"885\":1}}],[\"文本标签\",{\"1\":{\"384\":2,\"385\":2}}],[\"文本摘要中的奖励建模与\",{\"1\":{\"655\":1}}],[\"文本到文本部分使用标准的因果掩码\",{\"1\":{\"887\":1}}],[\"文本到文本\",{\"1\":{\"650\":1}}],[\"文本到图像的相似度权重\",{\"1\":{\"207\":1,\"386\":1}}],[\"文本蕴含\",{\"1\":{\"631\":1}}],[\"文本蕴含提升5\",{\"1\":{\"626\":1}}],[\"文本蕴含提升1\",{\"1\":{\"625\":1}}],[\"文本分词\",{\"1\":{\"713\":1}}],[\"文本分类任务\",{\"1\":{\"713\":1}}],[\"文本分类\",{\"1\":{\"631\":1,\"694\":2}}],[\"文本分离\",{\"1\":{\"625\":1}}],[\"文本分支\",{\"1\":{\"415\":2}}],[\"文本描述的生成也是一个关键环节\",{\"1\":{\"409\":1}}],[\"文本描述生成\",{\"0\":{\"409\":1}}],[\"文本问图\",{\"1\":{\"402\":1}}],[\"文本推理\",{\"1\":{\"385\":1}}],[\"文本id\",{\"1\":{\"384\":1}}],[\"文本类型\",{\"1\":{\"384\":1}}],[\"文本模态的\",{\"1\":{\"380\":1}}],[\"文本token\",{\"1\":{\"380\":1}}],[\"文本预训练进一步提升了视觉语言模型的性能\",{\"1\":{\"376\":1}}],[\"文本预训练\",{\"1\":{\"376\":1}}],[\"文本的更轻\",{\"1\":{\"390\":1}}],[\"文本的序列特征\",{\"1\":{\"384\":1}}],[\"文本的上下文表示\",{\"1\":{\"372\":1}}],[\"文本的注意力掩码\",{\"1\":{\"207\":1}}],[\"文本输入\",{\"1\":{\"385\":1}}],[\"文本输入表示\",{\"1\":{\"371\":2}}],[\"文本输入格式处理\",{\"1\":{\"107\":1}}],[\"文本表示\",{\"1\":{\"371\":2}}],[\"文本向量表示\",{\"1\":{\"371\":1}}],[\"文本和图像\",{\"1\":{\"371\":1,\"885\":1}}],[\"文本和图文对上进行统一的掩码建模\",{\"1\":{\"220\":1}}],[\"文本以及图像\",{\"1\":{\"370\":1}}],[\"文本以及图文对上以统一方式进行\",{\"1\":{\"225\":1}}],[\"文本及图文对\",{\"1\":{\"368\":1}}],[\"文本匹配损失\",{\"1\":{\"305\":1}}],[\"文本对应的标签\",{\"1\":{\"713\":1}}],[\"文本对更容易收集数据\",{\"1\":{\"374\":1}}],[\"文本对时\",{\"1\":{\"372\":1}}],[\"文本对表示进行图文匹配\",{\"1\":{\"370\":1}}],[\"文本对的表示\",{\"1\":{\"370\":1}}],[\"文本对的对称交叉熵损失\",{\"1\":{\"305\":1}}],[\"文本对上训练\",{\"1\":{\"330\":1}}],[\"文本对比损失\",{\"1\":{\"305\":1}}],[\"文本对\",{\"1\":{\"305\":1,\"370\":1,\"371\":1,\"373\":1,\"417\":1}}],[\"文本对齐策略\",{\"1\":{\"296\":1}}],[\"文本生成图像\",{\"1\":{\"895\":1}}],[\"文本生成图像的研究多基于较小数据集\",{\"1\":{\"884\":1}}],[\"文本生成图像任务传统上是在固定的数据集上\",{\"1\":{\"884\":1}}],[\"文本生成\",{\"1\":{\"626\":1,\"656\":1}}],[\"文本生成阶段\",{\"1\":{\"420\":1}}],[\"文本生成损失\",{\"1\":{\"274\":1}}],[\"文本生成任务\",{\"1\":{\"192\":1}}],[\"文本检索等对比任务上表现优异\",{\"1\":{\"305\":1}}],[\"文本检索以及多模态对话系统等\",{\"1\":{\"295\":1}}],[\"文本检索\",{\"1\":{\"273\":1,\"309\":1,\"312\":1}}],[\"文本处理\",{\"1\":{\"224\":1}}],[\"文本语料\",{\"1\":{\"224\":1,\"377\":1}}],[\"文本数据集\",{\"1\":{\"382\":1}}],[\"文本数据进行渐进式对齐训练\",{\"1\":{\"295\":1}}],[\"文本数据\",{\"1\":{\"223\":1,\"305\":1}}],[\"文本相似度\",{\"1\":{\"206\":1,\"373\":1}}],[\"文本全局语义\",{\"1\":{\"206\":1}}],[\"文本可能实际上与图像语义一致\",{\"1\":{\"202\":1}}],[\"文本中可能包含与图像无关的信息\",{\"1\":{\"202\":1}}],[\"文本经过编码后生成的嵌入序列\",{\"1\":{\"197\":1}}],[\"文本动量编码器\",{\"1\":{\"192\":1}}],[\"文本解码器通过最大化条件概率来学习配对文本的生成\",{\"1\":{\"271\":1}}],[\"文本解码器\",{\"1\":{\"192\":1}}],[\"文本队列\",{\"1\":{\"192\":1}}],[\"文本跨模态编码器\",{\"1\":{\"187\":1}}],[\"文本末尾添加一个\",{\"1\":{\"171\":1}}],[\"文本特征输入部分\",{\"1\":{\"392\":1}}],[\"文本特征为空\",{\"1\":{\"385\":1}}],[\"文本特征队列\",{\"1\":{\"205\":1}}],[\"文本特征提取和投影\",{\"1\":{\"190\":1}}],[\"文本特征\",{\"1\":{\"99\":2,\"205\":1,\"385\":1}}],[\"文本特征作为\",{\"1\":{\"24\":1}}],[\"文本引导的点特征分组\",{\"0\":{\"96\":1}}],[\"文本编码器的作用是提取文本的特征\",{\"1\":{\"407\":1}}],[\"文本编码器\",{\"1\":{\"206\":1,\"315\":2,\"407\":1,\"408\":1,\"900\":1}}],[\"文本编码器和多模态编码器均为\",{\"1\":{\"197\":1}}],[\"文本编码器和多模态编码器\",{\"1\":{\"197\":1}}],[\"文本编码器配置\",{\"1\":{\"192\":1}}],[\"文本编码器bert\",{\"1\":{\"191\":1}}],[\"文本编码器则基于\",{\"1\":{\"176\":1}}],[\"文本编码器使用的是基于\",{\"1\":{\"410\":1}}],[\"文本编码器使用\",{\"1\":{\"171\":1}}],[\"文本编码\",{\"0\":{\"55\":1},\"1\":{\"192\":1,\"368\":1,\"681\":1,\"900\":1}}],[\"文本信息增强视觉特征\",{\"1\":{\"23\":1}}],[\"文本融合\",{\"1\":{\"23\":1}}],[\"文本指令\",{\"1\":{\"19\":1,\"22\":1}}],[\"文本\",{\"0\":{\"5\":1,\"10\":1},\"1\":{\"178\":1,\"192\":4,\"208\":1,\"220\":2,\"224\":1,\"274\":3,\"303\":1,\"342\":1,\"372\":1,\"384\":4,\"385\":3,\"420\":2,\"680\":1,\"891\":1,\"892\":1,\"893\":3,\"895\":2,\"898\":1,\"900\":5}}],[\"低成本的\",{\"1\":{\"835\":1}}],[\"低延迟\",{\"1\":{\"823\":1}}],[\"低纬向量部分数值波动幅度很大\",{\"1\":{\"706\":1}}],[\"低维更多位置\",{\"1\":{\"706\":1}}],[\"低维向量部分\",{\"1\":{\"706\":1}}],[\"低维分量\",{\"1\":{\"706\":1}}],[\"低维度的编码值波动性很大\",{\"1\":{\"706\":1}}],[\"低于词对齐基线\",{\"1\":{\"641\":1}}],[\"低秩分解\",{\"1\":{\"609\":1}}],[\"低置信度\",{\"1\":{\"589\":1}}],[\"低\",{\"1\":{\"323\":1}}],[\"低效连接\",{\"1\":{\"296\":1}}],[\"低层图像元素\",{\"1\":{\"210\":1}}],[\"低密度区域则过于稀缺\",{\"1\":{\"134\":1}}],[\"低分辨率点云\",{\"1\":{\"122\":1}}],[\"低分辨率\",{\"1\":{\"121\":1,\"122\":2}}],[\"样本取平均\",{\"1\":{\"946\":1}}],[\"样本方差\",{\"1\":{\"904\":1}}],[\"样本空间为\",{\"1\":{\"846\":2}}],[\"样本的得分是用生成模型分配的tokens的平均对数概率\",{\"1\":{\"635\":1}}],[\"样本为文本对\",{\"1\":{\"634\":1}}],[\"样本级别\",{\"1\":{\"589\":1}}],[\"样本标签\",{\"1\":{\"514\":1}}],[\"样本总数\",{\"1\":{\"514\":1}}],[\"样本\",{\"1\":{\"305\":1,\"846\":1}}],[\"样本分配\",{\"1\":{\"213\":1}}],[\"样本分布偏差\",{\"1\":{\"134\":1}}],[\"样本数\",{\"1\":{\"89\":1,\"513\":1}}],[\"变回图像\",{\"1\":{\"956\":1}}],[\"变分下界\",{\"1\":{\"932\":1}}],[\"变分自编码器是否也存在某种\",{\"1\":{\"951\":1}}],[\"变分自编码器\",{\"0\":{\"944\":1},\"1\":{\"932\":1,\"945\":1,\"951\":1}}],[\"变化越慢\",{\"1\":{\"917\":1}}],[\"变化最快的方式\",{\"1\":{\"489\":1}}],[\"变得对称且有界\",{\"1\":{\"913\":1}}],[\"变异系数\",{\"1\":{\"874\":1}}],[\"变大\",{\"1\":{\"472\":1}}],[\"变为\",{\"1\":{\"260\":1,\"418\":3}}],[\"变量和函数\",{\"1\":{\"815\":1}}],[\"变量和函数中会设置好\",{\"1\":{\"805\":1}}],[\"变量实例可直接访问ndarray的核心属性\",{\"1\":{\"808\":1}}],[\"变量名称可在计算图可视化等场景中显示\",{\"1\":{\"808\":1}}],[\"变量的作用是存储数据\",{\"1\":{\"755\":1}}],[\"变量的基本概念\",{\"0\":{\"755\":1}}],[\"变量是tinypytorch最重要的组成部分\",{\"1\":{\"755\":1}}],[\"变量提升到所在函数\",{\"1\":{\"444\":1}}],[\"变量\",{\"1\":{\"235\":1,\"448\":1,\"877\":1}}],[\"变形为\",{\"1\":{\"380\":1}}],[\"变形\",{\"1\":{\"161\":1}}],[\"变换后再用欧几里得距离度量\",{\"1\":{\"578\":1}}],[\"变换后的输出维度\",{\"1\":{\"663\":1}}],[\"变换后的全局特征\",{\"1\":{\"122\":1}}],[\"变换后的点坐标\",{\"1\":{\"120\":1}}],[\"变换不变性\",{\"1\":{\"157\":1}}],[\"变换矩阵会通过\",{\"1\":{\"152\":1}}],[\"变成了\",{\"1\":{\"892\":1}}],[\"变成像素级别的\",{\"1\":{\"885\":1}}],[\"变成行向量\",{\"1\":{\"709\":1}}],[\"变成列向量\",{\"1\":{\"709\":1}}],[\"变成1维度之后就成了50176\",{\"1\":{\"426\":1}}],[\"变成\",{\"1\":{\"137\":2,\"156\":1,\"478\":1,\"605\":1,\"737\":1,\"892\":2,\"893\":1}}],[\"变成特征向量\",{\"1\":{\"132\":1}}],[\"互怼的艺术\",{\"1\":{\"919\":1}}],[\"互不干扰\",{\"1\":{\"885\":1}}],[\"互不重叠的分区\",{\"1\":{\"125\":1}}],[\"互动逻辑\",{\"1\":{\"831\":1}}],[\"互联提升效率\",{\"1\":{\"680\":1}}],[\"互斥事件\",{\"1\":{\"916\":1}}],[\"互斥\",{\"1\":{\"503\":1}}],[\"互为倒数\",{\"1\":{\"263\":1}}],[\"互优化机制\",{\"1\":{\"80\":1}}],[\"既能感知局部相对位置\",{\"1\":{\"706\":1}}],[\"既能提升多模态理解\",{\"1\":{\"278\":1}}],[\"既然是\",{\"1\":{\"380\":1}}],[\"既耗时又缺乏良好的空间对齐\",{\"1\":{\"125\":1}}],[\"既可用于预测文本\",{\"1\":{\"898\":1}}],[\"既可以在整个集合上作用\",{\"1\":{\"112\":1}}],[\"既可\",{\"1\":{\"73\":1}}],[\"网页收集\",{\"1\":{\"176\":1}}],[\"网格上每个空间位置的\",{\"1\":{\"886\":1}}],[\"网格点\",{\"1\":{\"502\":1}}],[\"网格特征\",{\"1\":{\"388\":1}}],[\"网格的高和宽\",{\"1\":{\"263\":1}}],[\"网格大小\",{\"1\":{\"263\":1}}],[\"网格\",{\"1\":{\"159\":1,\"231\":1,\"232\":1,\"263\":1,\"502\":1,\"885\":2}}],[\"网格采样\",{\"1\":{\"125\":1}}],[\"网络会自动学习潜在结构\",{\"1\":{\"944\":1}}],[\"网络结构\",{\"1\":{\"926\":1}}],[\"网络结构特点\",{\"1\":{\"146\":1}}],[\"网络层封装\",{\"1\":{\"819\":1}}],[\"网络类型\",{\"1\":{\"500\":1}}],[\"网络启发\",{\"1\":{\"372\":1}}],[\"网络获得图像\",{\"1\":{\"370\":1}}],[\"网络采用\",{\"1\":{\"286\":1}}],[\"网络噪声文本\",{\"1\":{\"268\":1}}],[\"网络\",{\"1\":{\"208\":1,\"221\":1,\"255\":1,\"285\":1,\"286\":1,\"293\":1}}],[\"网络图文数据噪声多\",{\"1\":{\"194\":1}}],[\"网络输出由一个有限子集\",{\"1\":{\"150\":1}}],[\"网络不是只捕获一个尺度上的局部特征\",{\"1\":{\"140\":1}}],[\"网络对于每个选定的形心点\",{\"1\":{\"140\":1}}],[\"网络对每一个点做低维到高维的映射\",{\"1\":{\"131\":1}}],[\"网络在训练时被呈现了不同稀疏度的点集\",{\"1\":{\"140\":1}}],[\"网络的学习目标是让重建出来的图像\",{\"1\":{\"956\":1}}],[\"网络的训练速度能快上很多\",{\"1\":{\"925\":1}}],[\"网络的所有卷积层要替换成带掩码的卷积层\",{\"1\":{\"925\":1}}],[\"网络的输出是一个经softmax的概率分布\",{\"1\":{\"925\":1}}],[\"网络的每一组set\",{\"1\":{\"132\":1}}],[\"网络的分割和分类模型\",{\"1\":{\"132\":1}}],[\"网络架构\",{\"0\":{\"116\":1}}],[\"容器\",{\"1\":{\"859\":1}}],[\"容易导致预训练过拟合\",{\"1\":{\"194\":1}}],[\"容易导致严重过拟合并限制模型深度\",{\"1\":{\"125\":1}}],[\"容易出错\",{\"1\":{\"157\":1}}],[\"容纳\",{\"1\":{\"27\":1}}],[\"切分成一个句子列表\",{\"1\":{\"696\":1}}],[\"切分后\",{\"1\":{\"55\":1}}],[\"切换也非常方便\",{\"1\":{\"606\":1}}],[\"切换\",{\"0\":{\"551\":1}}],[\"切片后可能是\",{\"1\":{\"544\":1}}],[\"切片索引语法\",{\"1\":{\"544\":1}}],[\"切片\",{\"0\":{\"544\":1},\"1\":{\"123\":1,\"544\":2,\"800\":1}}],[\"额外的\",{\"1\":{\"710\":1}}],[\"额外的提升\",{\"1\":{\"635\":1}}],[\"额外构建\",{\"1\":{\"382\":1}}],[\"额外添加448×448缩略图以保留全局上下文\",{\"1\":{\"331\":1}}],[\"额外特征\",{\"1\":{\"123\":1}}],[\"额外通道数\",{\"1\":{\"70\":1}}],[\"避免再求\",{\"1\":{\"931\":1}}],[\"避免极端数值\",{\"1\":{\"898\":1}}],[\"避免直接用\",{\"1\":{\"897\":1}}],[\"避免直接引用导致循环\",{\"1\":{\"806\":1}}],[\"避免内存长期占用\",{\"1\":{\"807\":1}}],[\"避免内存泄漏\",{\"1\":{\"806\":1}}],[\"避免增加对象的引用计数\",{\"1\":{\"806\":1}}],[\"避免强引用导致的内存滞留\",{\"1\":{\"806\":1}}],[\"避免栈溢出\",{\"1\":{\"788\":1}}],[\"避免重复计算或存储额外数据\",{\"1\":{\"779\":1}}],[\"避免重复选择\",{\"1\":{\"121\":1}}],[\"避免过拟合\",{\"1\":{\"681\":1}}],[\"避免过度拉伸\",{\"1\":{\"331\":1}}],[\"避免存储注意力权重\",{\"1\":{\"667\":1}}],[\"避免偏见与毒性\",{\"1\":{\"656\":1}}],[\"避免生成非结尾词\",{\"1\":{\"641\":1}}],[\"避免与webtext重叠\",{\"1\":{\"641\":1}}],[\"避免传统\",{\"1\":{\"640\":1}}],[\"避免复杂的编码需求\",{\"1\":{\"614\":1}}],[\"避免单一损失可能带来的训练不稳定性\",{\"1\":{\"592\":1}}],[\"避免训练震荡\",{\"1\":{\"587\":1}}],[\"避免使用不平衡的数据集\",{\"1\":{\"566\":1}}],[\"避免冗余\",{\"1\":{\"522\":1}}],[\"避免为每个像素单独建模\",{\"1\":{\"500\":1}}],[\"避免选中正样本作为负样本\",{\"1\":{\"386\":1}}],[\"避免选到自己\",{\"1\":{\"192\":1}}],[\"避免罕见组合\",{\"1\":{\"341\":1}}],[\"避免学生只学到平凡的输入匹配\",{\"1\":{\"293\":1}}],[\"避免塌缩\",{\"1\":{\"293\":1}}],[\"避免崩溃\",{\"0\":{\"290\":1}}],[\"避免数据泄漏\",{\"1\":{\"273\":1}}],[\"避免数值错误的小常数\",{\"1\":{\"213\":1}}],[\"避免信息泄漏\",{\"1\":{\"263\":1}}],[\"避免\",{\"1\":{\"256\":1,\"260\":1,\"262\":1}}],[\"避免出现在预训练数据中\",{\"1\":{\"243\":1}}],[\"避免部分码本向量长期未被使用\",{\"1\":{\"215\":1}}],[\"避免模型过拟合于极端负样本或伪负样本\",{\"1\":{\"207\":1}}],[\"避免采样到自己\",{\"1\":{\"207\":1}}],[\"避免因忘记重置配置导致的错误\",{\"1\":{\"807\":1}}],[\"避免因gc延迟导致的内存问题\",{\"1\":{\"806\":1}}],[\"避免因\",{\"1\":{\"202\":1}}],[\"避免了人为加\",{\"1\":{\"931\":1}}],[\"避免了手工设计基函数的局限性\",{\"1\":{\"500\":1}}],[\"避免了只通过单词上下文进行预测\",{\"1\":{\"393\":1}}],[\"避免了巨量类别的学习\",{\"1\":{\"282\":1}}],[\"避免了随机初始化导致的码本塌陷问题\",{\"1\":{\"213\":1}}],[\"避免了复杂的预处理\",{\"1\":{\"148\":1}}],[\"避免了传统方法对采样和邻域查询的依赖\",{\"1\":{\"125\":1}}],[\"避免除零\",{\"1\":{\"213\":1}}],[\"避免除零加一个小量\",{\"1\":{\"122\":1}}],[\"避免除以零\",{\"1\":{\"145\":1}}],[\"查表\",{\"1\":{\"710\":1,\"899\":1}}],[\"查表获取偏置\",{\"1\":{\"710\":1}}],[\"查表获取偏置值\",{\"1\":{\"710\":1}}],[\"查表得到\",{\"1\":{\"710\":2}}],[\"查表得到偏置\",{\"1\":{\"710\":1}}],[\"查表得到相对位置向量\",{\"1\":{\"709\":1}}],[\"查表后\",{\"1\":{\"709\":1}}],[\"查看当前环境已安装的包\",{\"0\":{\"556\":1},\"1\":{\"556\":1}}],[\"查看当前conda激活的环境\",{\"1\":{\"555\":1}}],[\"查看当前激活的环境\",{\"0\":{\"555\":1}}],[\"查看所有已创建的环境\",{\"0\":{\"553\":1}}],[\"查找\",{\"1\":{\"256\":1}}],[\"查找最近邻时对代码本嵌入进行\",{\"1\":{\"212\":1}}],[\"查找目标点云\",{\"1\":{\"122\":1}}],[\"查询\",{\"1\":{\"709\":2}}],[\"查询位置\",{\"1\":{\"709\":4}}],[\"查询一个未出现的元素时\",{\"1\":{\"516\":1}}],[\"查询越多\",{\"1\":{\"278\":1}}],[\"查询数量\",{\"1\":{\"278\":1}}],[\"查询点数量\",{\"1\":{\"137\":1}}],[\"查询点的位置\",{\"1\":{\"137\":1}}],[\"查询点的数量\",{\"1\":{\"119\":1}}],[\"查询点的坐标\",{\"1\":{\"119\":2}}],[\"查询相比\",{\"1\":{\"135\":1}}],[\"查询和分组\",{\"1\":{\"121\":1}}],[\"查询并分组函数\",{\"1\":{\"119\":1}}],[\"查询向量\",{\"1\":{\"119\":1,\"362\":1,\"529\":1}}],[\"恢复为\",{\"1\":{\"963\":1}}],[\"恢复被掩码的词\",{\"1\":{\"699\":1}}],[\"恢复原始顺序\",{\"1\":{\"582\":1}}],[\"恢复离散的类别\",{\"1\":{\"257\":1}}],[\"恢复张量格式为\",{\"1\":{\"145\":1}}],[\"恢复分辨率\",{\"1\":{\"122\":1}}],[\"恢复点云分辨率并融合不同层级的特征\",{\"1\":{\"122\":1}}],[\"恢复点云分辨率\",{\"1\":{\"122\":1}}],[\"恢复到原始点数\",{\"1\":{\"83\":1}}],[\"离职人员创建的\",{\"1\":{\"823\":1}}],[\"离群点鲁棒性更强\",{\"1\":{\"259\":1}}],[\"离散化\",{\"1\":{\"963\":1}}],[\"离散化变分自编码器\",{\"0\":{\"899\":1},\"1\":{\"899\":1}}],[\"离散向量的另一个问题是它不好采样\",{\"1\":{\"956\":1}}],[\"离散表示空间的大小\",{\"1\":{\"899\":1}}],[\"离散表示学习\",{\"0\":{\"249\":1}}],[\"离散视觉词表大小\",{\"1\":{\"892\":1}}],[\"离散分布\",{\"0\":{\"855\":1},\"1\":{\"904\":1,\"907\":1,\"959\":1}}],[\"离散随机变量形式\",{\"0\":{\"852\":1}}],[\"离散随机变量\",{\"0\":{\"846\":1}}],[\"离散格子\",{\"1\":{\"504\":1}}],[\"离散vae\",{\"1\":{\"265\":1}}],[\"离散vae的权重路径\",{\"1\":{\"265\":1}}],[\"离散的颜色值\",{\"1\":{\"925\":1}}],[\"离散的\",{\"1\":{\"235\":1}}],[\"离散变分自编码器\",{\"1\":{\"232\":1}}],[\"离散\",{\"1\":{\"210\":1,\"234\":1,\"235\":1,\"258\":2}}],[\"离散卷积\",{\"1\":{\"109\":1}}],[\"离已选点集最远的点\",{\"1\":{\"121\":1}}],[\"遍历所有相邻字符对的组合\",{\"1\":{\"596\":1}}],[\"遍历所有句子\",{\"1\":{\"596\":1}}],[\"遍历vocab中所有词\",{\"1\":{\"595\":2}}],[\"遍历句子列表\",{\"1\":{\"595\":1}}],[\"遍历数据加载器中的每个批次数据\",{\"1\":{\"431\":1}}],[\"遍历获取supported支持的所有文件路径\",{\"1\":{\"424\":1}}],[\"遍历文件夹\",{\"1\":{\"424\":1}}],[\"遍历data目录\",{\"1\":{\"411\":1}}],[\"遍历\",{\"1\":{\"385\":1,\"411\":1,\"412\":1}}],[\"遍历不同分辨率的分组\",{\"1\":{\"293\":1}}],[\"遍历遮挡块区域\",{\"1\":{\"263\":1}}],[\"遍历每个文件夹下的文件\",{\"1\":{\"424\":1}}],[\"遍历每个学生视图\",{\"1\":{\"293\":1}}],[\"遍历每个教师视图\",{\"1\":{\"293\":1}}],[\"遍历每个近邻点\",{\"1\":{\"122\":1}}],[\"遍历每个批次\",{\"1\":{\"121\":1}}],[\"遍历标注数据列表\",{\"1\":{\"92\":1}}],[\"重构损失项可以写作\",{\"1\":{\"951\":1}}],[\"重构损失\",{\"0\":{\"932\":1}}],[\"重构项\",{\"1\":{\"885\":1}}],[\"重参数化的关键思想\",{\"1\":{\"931\":1}}],[\"重参数化技巧仅适用于连续变量的分布\",{\"1\":{\"946\":1}}],[\"重参数化技巧\",{\"1\":{\"931\":1,\"946\":1}}],[\"重载运算符\",{\"1\":{\"812\":1}}],[\"重置导数\",{\"0\":{\"802\":1}}],[\"重新定义了\",{\"1\":{\"952\":1}}],[\"重新排列\",{\"1\":{\"899\":1,\"945\":1}}],[\"重新计算\",{\"1\":{\"663\":1}}],[\"重新输入全部历史\",{\"1\":{\"663\":1}}],[\"重新拉直空间\",{\"1\":{\"578\":1}}],[\"重写\",{\"1\":{\"656\":1}}],[\"重要的是\",{\"1\":{\"647\":1}}],[\"重要说明\",{\"1\":{\"557\":1}}],[\"重叠计算与gpu通信\",{\"1\":{\"667\":1}}],[\"重叠\",{\"1\":{\"641\":1}}],[\"重塑为\",{\"1\":{\"582\":1}}],[\"重塑注意力权重为\",{\"1\":{\"582\":1}}],[\"重塑特征为空间维度\",{\"1\":{\"213\":1}}],[\"重\",{\"1\":{\"415\":8}}],[\"重型胶水层\",{\"1\":{\"303\":1}}],[\"重组视觉特征\",{\"1\":{\"296\":1}}],[\"重点是保持参数尺寸最小化\",{\"1\":{\"614\":1}}],[\"重点是如何理解这里的分组\",{\"1\":{\"96\":1}}],[\"重点在于\",{\"1\":{\"546\":1}}],[\"重点训练图像和文本特征提取\",{\"1\":{\"415\":1}}],[\"重点注意各个参数的含义\",{\"1\":{\"263\":1}}],[\"重建误差\",{\"1\":{\"963\":1}}],[\"重建误差为输入图片和重建图片的均方误差\",{\"1\":{\"961\":1}}],[\"重建误差和嵌入空间误差\",{\"1\":{\"961\":1}}],[\"重建\",{\"1\":{\"950\":1}}],[\"重建项\",{\"1\":{\"932\":1}}],[\"重建样本\",{\"1\":{\"932\":1}}],[\"重建对数似然\",{\"1\":{\"932\":1}}],[\"重建原始图像\",{\"1\":{\"232\":1}}],[\"重建的特征\",{\"1\":{\"213\":2}}],[\"重建损失函数\",{\"1\":{\"255\":1}}],[\"重建损失值\",{\"1\":{\"213\":1}}],[\"重建损失\",{\"1\":{\"213\":2,\"899\":1,\"947\":1}}],[\"重建损失类型\",{\"1\":{\"213\":1}}],[\"重复向梯度方向移动一定距离\",{\"1\":{\"816\":1}}],[\"重复惩罚系数\",{\"1\":{\"421\":1}}],[\"重复惩罚项\",{\"1\":{\"188\":1}}],[\"重复上述步骤直到遮挡的\",{\"1\":{\"234\":1}}],[\"重复迭代更新聚类中心\",{\"1\":{\"213\":1}}],[\"重复步骤2和3\",{\"1\":{\"918\":1}}],[\"重复步骤\",{\"1\":{\"213\":1}}],[\"重复\",{\"1\":{\"137\":1}}],[\"重复直到选择足够多的点\",{\"1\":{\"121\":1}}],[\"选取最相近的向量索引\",{\"1\":{\"963\":1}}],[\"选取使得观测到的数据出现的概率\",{\"1\":{\"903\":1}}],[\"选取物体\",{\"1\":{\"87\":1}}],[\"选\",{\"1\":{\"856\":1}}],[\"选项内容\",{\"1\":{\"737\":1}}],[\"选出投票最多的结果\",{\"1\":{\"621\":1}}],[\"选出他们认为最好的答案\",{\"1\":{\"339\":1}}],[\"选出\",{\"1\":{\"141\":1}}],[\"选关键点\",{\"1\":{\"137\":1}}],[\"选择不改变相似性度量\",{\"1\":{\"944\":1}}],[\"选择桶编号\",{\"1\":{\"710\":1}}],[\"选择性优势\",{\"1\":{\"649\":1}}],[\"选择性能最佳的损失函数\",{\"1\":{\"593\":1}}],[\"选择合适的预训练好的大模型\",{\"1\":{\"609\":1}}],[\"选择适合的损失函数\",{\"1\":{\"593\":1}}],[\"选择最大值作为这个图文对的相似度\",{\"1\":{\"418\":1}}],[\"选择与图像特征相似度最高的文本所对应的类别\",{\"1\":{\"408\":1}}],[\"选择了一个包含6300万参数的transformer模型\",{\"1\":{\"407\":1}}],[\"选择图像键名\",{\"1\":{\"384\":1}}],[\"选择𝑁个点\",{\"1\":{\"134\":1}}],[\"选择距离最大的点\",{\"1\":{\"121\":2}}],[\"移至每个子模块的输入\",{\"1\":{\"640\":1}}],[\"移除nsp任务\",{\"1\":{\"683\":1,\"688\":1}}],[\"移除\",{\"1\":{\"679\":1}}],[\"移除下一句预测\",{\"1\":{\"677\":1,\"678\":1}}],[\"移除宏定义和参考文献\",{\"1\":{\"667\":1}}],[\"移除了大量与本文无关的逻辑\",{\"1\":{\"663\":1}}],[\"移除了图像\",{\"1\":{\"269\":1}}],[\"移除提示后性能下降6\",{\"1\":{\"641\":1}}],[\"移除最后一个\",{\"1\":{\"188\":1}}],[\"移除最后一个维度\",{\"1\":{\"121\":1}}],[\"移除无效数据的有效\",{\"1\":{\"177\":1}}],[\"移动到下一个批次\",{\"1\":{\"121\":1}}],[\"移动\",{\"1\":{\"73\":1,\"161\":1}}],[\"添加shape\",{\"1\":{\"808\":1}}],[\"添加retain\",{\"1\":{\"807\":1}}],[\"添加元素\",{\"1\":{\"806\":1}}],[\"添加特殊token标记\",{\"1\":{\"713\":1}}],[\"添加batch维度\",{\"1\":{\"663\":1}}],[\"添加一个无监督训练目标是半监督学习的一种替代形式\",{\"1\":{\"627\":1}}],[\"添加一些特殊词\",{\"1\":{\"595\":1}}],[\"添加一维位置编码和二维位置编码并没有太大的差异\",{\"1\":{\"428\":1}}],[\"添加常数偏移\",{\"1\":{\"522\":2}}],[\"添加到\",{\"1\":{\"421\":1}}],[\"添加可学习偏置\",{\"1\":{\"380\":1}}],[\"添加位置编码\",{\"0\":{\"428\":1},\"1\":{\"266\":1,\"380\":1,\"898\":1}}],[\"添加\",{\"0\":{\"427\":1},\"1\":{\"153\":1,\"380\":3,\"384\":1,\"892\":1,\"899\":1}}],[\"添加残差连接\",{\"1\":{\"120\":1}}],[\"添加更多上下文细节\",{\"1\":{\"87\":1}}],[\"升维或保持维度\",{\"1\":{\"120\":1}}],[\"批处理\",{\"1\":{\"819\":1}}],[\"批量\",{\"1\":{\"319\":1}}],[\"批量大小增至\",{\"1\":{\"317\":1}}],[\"批量大小\",{\"1\":{\"286\":1,\"318\":1}}],[\"批量大小为\",{\"1\":{\"176\":1}}],[\"批次大小\",{\"1\":{\"679\":1,\"899\":1}}],[\"批次的均值和方差\",{\"1\":{\"522\":1}}],[\"批次平均特征\",{\"1\":{\"122\":1}}],[\"批次数\",{\"1\":{\"121\":1}}],[\"批次索引\",{\"1\":{\"120\":1,\"121\":2,\"122\":2,\"137\":1,\"582\":1}}],[\"批归一化\",{\"1\":{\"120\":5,\"121\":2,\"122\":3}}],[\"批大小提升至512\",{\"1\":{\"640\":1}}],[\"批大小为\",{\"1\":{\"633\":1}}],[\"批大小\",{\"1\":{\"46\":1}}],[\"残差链接\",{\"1\":{\"725\":1}}],[\"残差层权重按\",{\"1\":{\"640\":1}}],[\"残差\",{\"1\":{\"633\":1}}],[\"残差块的数量\",{\"1\":{\"899\":1}}],[\"残差块实现如下\",{\"1\":{\"255\":1}}],[\"残差块\",{\"0\":{\"120\":1},\"1\":{\"120\":2}}],[\"残差连接与层归一化\",{\"1\":{\"741\":1}}],[\"残差连接\",{\"1\":{\"115\":1,\"120\":2,\"429\":2,\"745\":1}}],[\"<i\",{\"1\":{\"926\":1}}],[\"<bos>\",{\"1\":{\"892\":1,\"893\":3}}],[\"<unk>\",{\"1\":{\"595\":1,\"597\":2,\"697\":1}}],[\"<pad>\",{\"1\":{\"595\":1,\"597\":2}}],[\"<line\",{\"1\":{\"595\":1,\"596\":1,\"597\":2}}],[\"<环境名>\",{\"1\":{\"550\":1,\"551\":1}}],[\"<4\",{\"1\":{\"546\":1}}],[\"<<\",{\"1\":{\"500\":1}}],[\"<stop>\",{\"1\":{\"342\":6}}],[\"<seg>\",{\"1\":{\"7\":1}}],[\"<==\",{\"1\":{\"963\":1}}],[\"<=\",{\"1\":{\"263\":1,\"698\":3,\"709\":1,\"733\":1}}],[\"<eos>\",{\"1\":{\"171\":1}}],[\"<\",{\"1\":{\"119\":3,\"121\":1,\"122\":1,\"137\":1,\"263\":4,\"402\":3,\"574\":1,\"592\":1,\"595\":6,\"596\":2,\"597\":7,\"697\":1,\"700\":1,\"710\":5,\"892\":4,\"893\":1,\"895\":1}}],[\"按下面这个公式求梯度\",{\"1\":{\"959\":1}}],[\"按值为\",{\"1\":{\"959\":1}}],[\"按任意顺序进行排列\",{\"1\":{\"881\":1}}],[\"按导数公式计算梯度\",{\"1\":{\"809\":1}}],[\"按序取出元素\",{\"1\":{\"805\":1}}],[\"按序执行以下命令完成环境搭建\",{\"1\":{\"712\":1}}],[\"按反向顺序调用各函数的backward方法\",{\"1\":{\"781\":1}}],[\"按头\",{\"1\":{\"710\":1}}],[\"按评分排序答案\",{\"1\":{\"667\":1}}],[\"按用户\",{\"1\":{\"656\":1}}],[\"按需要在w前面拼接一些参数\",{\"1\":{\"605\":1}}],[\"按空格切分\",{\"1\":{\"595\":1}}],[\"按类别分配\",{\"1\":{\"589\":1}}],[\"按元素大小\",{\"1\":{\"546\":1}}],[\"按转置顺序\",{\"1\":{\"545\":1}}],[\"按权重采样样本\",{\"1\":{\"518\":1}}],[\"按权重从当前行采样一个负样本索引\",{\"1\":{\"207\":1}}],[\"按出现次数依次返回元素\",{\"1\":{\"516\":1}}],[\"按列索引累加元素\",{\"1\":{\"487\":1}}],[\"按行分成2块\",{\"1\":{\"482\":1}}],[\"按比例随机采样验证样本\",{\"1\":{\"424\":1}}],[\"按key进行聚合\",{\"1\":{\"382\":1}}],[\"按指令做事\",{\"1\":{\"346\":1}}],[\"按迭代次数调整\",{\"1\":{\"293\":1}}],[\"按\",{\"1\":{\"208\":1,\"385\":1,\"445\":1,\"873\":1}}],[\"按批次处理\",{\"1\":{\"122\":1}}],[\"按batch处理数据\",{\"1\":{\"119\":1}}],[\"按照权重加权融合\",{\"1\":{\"893\":1}}],[\"按照权重做加权平均\",{\"1\":{\"145\":1}}],[\"按照本批次序列中最大长度进行截断\",{\"1\":{\"715\":1}}],[\"按照16x16大小的patch进行划分\",{\"1\":{\"426\":1}}],[\"按照余弦相似度的数学公式来计算两者的相似度数值\",{\"1\":{\"410\":1}}],[\"按照最大相似度\",{\"1\":{\"408\":1}}],[\"按照\",{\"1\":{\"293\":1,\"595\":1}}],[\"按照选定方差归一化\",{\"1\":{\"264\":1}}],[\"按照选定均值归一化\",{\"1\":{\"264\":1}}],[\"按照给定概率进行\",{\"1\":{\"208\":1}}],[\"按照几个预定义的半径值来搜索周围的邻近点\",{\"1\":{\"140\":1}}],[\"按照不同的搜索半径或领域大小对点集进行分组\",{\"1\":{\"140\":1}}],[\"按照相似度完成信息融合\",{\"1\":{\"83\":1}}],[\"按照key的插入顺序返回的\",{\"1\":{\"53\":1}}],[\"只会用到式子的梯度\",{\"1\":{\"959\":1}}],[\"只出现在公式右边的第二项中\",{\"1\":{\"951\":1}}],[\"只出现了几次\",{\"1\":{\"561\":1}}],[\"只允许卷积核看到左边和上方的像素\",{\"1\":{\"924\":1}}],[\"只算一个像素的概率分布还称不上高效\",{\"1\":{\"921\":1}}],[\"只关心最后一个\",{\"1\":{\"895\":1}}],[\"只关注如何把图像变成\",{\"1\":{\"235\":1}}],[\"只关注全局结构\",{\"1\":{\"157\":1}}],[\"只利用模型自身\",{\"1\":{\"894\":1}}],[\"只靠自身学到的\",{\"1\":{\"894\":1}}],[\"只展示第一个\",{\"1\":{\"892\":1}}],[\"只展示核心代码\",{\"1\":{\"663\":1}}],[\"只开源了\",{\"1\":{\"823\":1}}],[\"只能保留全局信息\",{\"1\":{\"963\":1}}],[\"只能很好地完成图像压缩\",{\"1\":{\"956\":1}}],[\"只能起到把图像压缩的作用\",{\"1\":{\"956\":1}}],[\"只能学习如何根据某种隐变量\",{\"1\":{\"947\":1}}],[\"只能依赖于它左上方的像素\",{\"1\":{\"926\":1}}],[\"只能用\",{\"1\":{\"924\":1}}],[\"只能看\",{\"1\":{\"924\":2}}],[\"只能预测文本\",{\"1\":{\"898\":1}}],[\"只能最大化它的证据下界\",{\"1\":{\"885\":1}}],[\"只能实现一条竖线形状的计算图结构的反向传播\",{\"1\":{\"784\":1}}],[\"只能对输入文本中的\",{\"1\":{\"735\":1}}],[\"只能处理刚性变换\",{\"1\":{\"157\":1}}],[\"只做抽取式问答\",{\"1\":{\"735\":1}}],[\"只做特征变换\",{\"1\":{\"121\":2}}],[\"只计算在第\",{\"1\":{\"691\":1}}],[\"只计算新\",{\"1\":{\"663\":1}}],[\"只为当前传入的词生成位置序列\",{\"1\":{\"663\":1}}],[\"只达到了56\",{\"1\":{\"634\":1}}],[\"只包含\",{\"1\":{\"550\":1,\"735\":1}}],[\"只设置当前\",{\"1\":{\"521\":1}}],[\"只检查元数据\",{\"1\":{\"490\":1}}],[\"只适用于连续内存的张量\",{\"1\":{\"469\":1}}],[\"只交换两个指定维度\",{\"1\":{\"467\":1}}],[\"只执行一次\",{\"1\":{\"382\":1}}],[\"只完成了借助卷积对图像进行前置处理的步骤\",{\"1\":{\"380\":1}}],[\"只需从标准正态分布中采样\",{\"1\":{\"947\":1}}],[\"只需优化训练策略即可达到sota\",{\"1\":{\"686\":1}}],[\"只需提供极少量的示例就能成功完成\",{\"1\":{\"648\":1}}],[\"只需将这些步长交换为\",{\"1\":{\"545\":1}}],[\"只需要去除cls\",{\"1\":{\"722\":1}}],[\"只需要计算当前轮新增\",{\"1\":{\"663\":1}}],[\"只需要计算\",{\"1\":{\"611\":1}}],[\"只需要微调\",{\"1\":{\"611\":1}}],[\"只需要48gb\",{\"1\":{\"607\":1}}],[\"只需要移动到下一个元素\",{\"1\":{\"489\":1}}],[\"只需要用一个一个linear即可\",{\"1\":{\"431\":1}}],[\"只需要将图像调整到合适的大小\",{\"1\":{\"425\":1}}],[\"只需要告知父类自己的数据集名和数据集类的具体实现即可\",{\"1\":{\"382\":1}}],[\"只需要\",{\"1\":{\"357\":1}}],[\"只需一次遍历下游数据集即可完成评估\",{\"1\":{\"286\":1}}],[\"只不过它是\",{\"1\":{\"895\":1}}],[\"只不过它们的具体实现上有一些差异\",{\"1\":{\"605\":1}}],[\"只不过是对每个字都要预测一个类别\",{\"1\":{\"694\":1}}],[\"只不过类别和所有样本相比\",{\"1\":{\"355\":1}}],[\"只不过词汇表是标签名称集合\",{\"1\":{\"272\":1}}],[\"只训练降维矩阵\",{\"1\":{\"611\":1}}],[\"只训练最后的全连接层\",{\"1\":{\"352\":1}}],[\"只训练少量\",{\"1\":{\"346\":1}}],[\"只更新投影层\",{\"1\":{\"342\":1}}],[\"只是用\",{\"1\":{\"923\":1}}],[\"只是直接加到注意力分数上\",{\"1\":{\"710\":1}}],[\"只是一个占位符而已\",{\"1\":{\"694\":1}}],[\"只是在推理的过程中\",{\"1\":{\"605\":1}}],[\"只是侧重点不一样\",{\"1\":{\"602\":1}}],[\"只是访问方式变了\",{\"1\":{\"545\":1}}],[\"只是可预测\",{\"1\":{\"521\":1}}],[\"只是结构上简化了多模态学习\",{\"1\":{\"394\":1}}],[\"只是建立中间模型的一种方式\",{\"1\":{\"353\":1}}],[\"只是参数经过调整以更好响应指令\",{\"1\":{\"339\":1}}],[\"只是温度为\",{\"1\":{\"285\":1}}],[\"只用一个可学习的\",{\"1\":{\"277\":1}}],[\"只用因果掩码的自注意力机制来编码输入文本\",{\"1\":{\"272\":1}}],[\"只提取全局嵌入\",{\"1\":{\"272\":1}}],[\"只学习纯文本的表示\",{\"1\":{\"268\":1}}],[\"只学正交变换\",{\"1\":{\"157\":1}}],[\"只针对被\",{\"1\":{\"265\":1}}],[\"只返回重建图像\",{\"1\":{\"256\":1}}],[\"只返回特征\",{\"1\":{\"119\":1}}],[\"只有空间掩码卷积\",{\"1\":{\"964\":1}}],[\"只有2490样本\",{\"1\":{\"634\":1}}],[\"只有linear\",{\"1\":{\"522\":1}}],[\"只有linear层\",{\"1\":{\"522\":1}}],[\"只有\",{\"1\":{\"514\":1,\"611\":1,\"823\":2,\"871\":1}}],[\"只有batchnorm是\",{\"1\":{\"493\":1}}],[\"只有原始维度\",{\"1\":{\"472\":1}}],[\"只有一个自由参数\",{\"1\":{\"871\":2}}],[\"只有一个特殊标记\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"只有一个\",{\"1\":{\"273\":1,\"663\":1}}],[\"只有与视觉相关的参数会被激活\",{\"1\":{\"224\":1}}],[\"只有遇到换行符或者输出内容积累到一定大小时\",{\"1\":{\"107\":1}}],[\"只保留最后一个\",{\"1\":{\"893\":1}}],[\"只保留最多\",{\"1\":{\"341\":1}}],[\"只保留一个\",{\"1\":{\"803\":1}}],[\"只保留一定百分比的注意力质量\",{\"1\":{\"582\":1}}],[\"只保留输出\",{\"1\":{\"582\":1}}],[\"只保留单模态的文本表示学习\",{\"1\":{\"272\":1}}],[\"只保留\",{\"1\":{\"220\":1,\"266\":2}}],[\"只保留生成部分\",{\"1\":{\"188\":1}}],[\"只要ae的编码空间比较规整\",{\"1\":{\"956\":1}}],[\"只要我们拥有足够高容量\",{\"1\":{\"949\":1}}],[\"只要这样的结构有助于拟合训练数据\",{\"1\":{\"944\":1}}],[\"只要知道\",{\"1\":{\"709\":1}}],[\"只要其隐藏层神经元的数量足够\",{\"1\":{\"500\":1}}],[\"只要相邻两行完全相同就会合并\",{\"1\":{\"480\":1}}],[\"只要你能够得到一个判断正样本和负样本的规律\",{\"1\":{\"350\":1}}],[\"只要在海量数据上进行大规模预训练\",{\"1\":{\"220\":1}}],[\"只要关键点还在\",{\"1\":{\"150\":1}}],[\"只在这些\",{\"1\":{\"847\":1}}],[\"只在推导式内部有效\",{\"1\":{\"444\":1}}],[\"只在主进程调用一次\",{\"1\":{\"382\":1}}],[\"只在\",{\"1\":{\"208\":1,\"266\":1,\"385\":1,\"513\":1,\"710\":1}}],[\"只对低秩矩阵\",{\"1\":{\"609\":1}}],[\"只对encoder进行更新\",{\"1\":{\"213\":2}}],[\"只对被\",{\"1\":{\"208\":1}}],[\"只对应一个\",{\"1\":{\"190\":1}}],[\"只改变物体的方向\",{\"1\":{\"162\":1}}],[\"只改变位置和朝向\",{\"1\":{\"161\":1}}],[\"只通过\",{\"1\":{\"157\":1}}],[\"只取绝对值\",{\"1\":{\"710\":1}}],[\"只取cls\",{\"1\":{\"190\":2}}],[\"只取\",{\"1\":{\"146\":1}}],[\"只取特征和批次索引\",{\"1\":{\"122\":1}}],[\"只进行特征变换\",{\"1\":{\"121\":1}}],[\"组\",{\"1\":{\"924\":1}}],[\"组件\",{\"1\":{\"522\":1}}],[\"组合模型\",{\"1\":{\"963\":1}}],[\"组合\",{\"0\":{\"882\":1}}],[\"组合分析\",{\"0\":{\"879\":1},\"1\":{\"879\":1}}],[\"组合数\",{\"1\":{\"860\":1}}],[\"组合损失\",{\"1\":{\"587\":1}}],[\"组合损失值\",{\"1\":{\"587\":1}}],[\"组合后的优势\",{\"1\":{\"587\":1}}],[\"组合后的图像张量和标签张量\",{\"1\":{\"424\":1}}],[\"组合方式\",{\"1\":{\"500\":1}}],[\"组1\",{\"1\":{\"119\":2}}],[\"组0\",{\"1\":{\"119\":2}}],[\"组成一个\",{\"1\":{\"574\":1}}],[\"组成新的训练集\",{\"1\":{\"173\":1}}],[\"组成\",{\"1\":{\"39\":1,\"679\":1,\"774\":1}}],[\"α=0\",{\"1\":{\"590\":2}}],[\"α>β\",{\"1\":{\"590\":1}}],[\"α\",{\"1\":{\"119\":1,\"204\":1,\"589\":4,\"590\":3,\"592\":6}}],[\"ρ\",{\"1\":{\"119\":1}}],[\"ψ\",{\"1\":{\"119\":1}}],[\"φ\",{\"1\":{\"119\":1}}],[\"索引分布\",{\"1\":{\"964\":1}}],[\"索引列表\",{\"1\":{\"964\":2}}],[\"索引矩阵\",{\"1\":{\"963\":1}}],[\"索引矩阵和距离矩阵\",{\"1\":{\"119\":1}}],[\"索引映射到内存中的地址\",{\"1\":{\"542\":1}}],[\"索引出错\",{\"1\":{\"384\":1}}],[\"索引编号\",{\"1\":{\"187\":1}}],[\"索引保存下来\",{\"1\":{\"137\":1}}],[\"索引\",{\"1\":{\"119\":1,\"256\":1,\"258\":1,\"293\":1,\"697\":1,\"899\":3,\"964\":3}}],[\"γ2\",{\"1\":{\"380\":2}}],[\"γ1\",{\"1\":{\"380\":2}}],[\"γ\",{\"1\":{\"119\":3,\"150\":2,\"589\":3,\"656\":1}}],[\"−1\",{\"1\":{\"264\":1}}],[\"−0\",{\"1\":{\"236\":1}}],[\"−\",{\"1\":{\"119\":2,\"656\":3}}],[\"θ\",{\"1\":{\"119\":2,\"656\":3}}],[\"比直接逐像素生成效果更好\",{\"1\":{\"943\":1}}],[\"比特\",{\"1\":{\"909\":1}}],[\"比\",{\"1\":{\"655\":1,\"823\":1}}],[\"比原sota高出18\",{\"1\":{\"648\":1}}],[\"比原本少了一个多头自注意力\",{\"1\":{\"629\":1}}],[\"比此前最大的非稀疏语言模型大10倍\",{\"1\":{\"646\":1}}],[\"比许多情况下的ensemble模型要好\",{\"1\":{\"634\":1}}],[\"比single\",{\"1\":{\"634\":1}}],[\"比最近邻计算量大\",{\"1\":{\"505\":1}}],[\"比单层电路更高效\",{\"1\":{\"500\":1}}],[\"比较\",{\"1\":{\"963\":1}}],[\"比较它们在iou\",{\"1\":{\"593\":1}}],[\"比较两个分类名词是否相等\",{\"1\":{\"410\":1}}],[\"比较了两种文本生成方式\",{\"1\":{\"178\":1}}],[\"比基于网格特征的模型快至少四倍的推理速度\",{\"1\":{\"388\":1}}],[\"比使用在文本数据上预训练的\",{\"1\":{\"376\":1}}],[\"比把它和原输入拼接更好\",{\"1\":{\"277\":1}}],[\"比只用单一损失的模型表现更好\",{\"1\":{\"276\":1}}],[\"比深层头\",{\"1\":{\"215\":1}}],[\"比例进行的掩码\",{\"1\":{\"700\":1}}],[\"比例等超参数组合上进行对比试验\",{\"1\":{\"311\":1}}],[\"比例为\",{\"1\":{\"236\":1}}],[\"比例\",{\"1\":{\"205\":1}}],[\"比排序\",{\"1\":{\"150\":1}}],[\"比前两者更强\",{\"1\":{\"117\":1}}],[\"比如图像的类别\",{\"1\":{\"963\":1}}],[\"比如图中的\",{\"1\":{\"958\":2}}],[\"比如超分辨率\",{\"1\":{\"961\":1}}],[\"比如pixelcnn\",{\"1\":{\"956\":1}}],[\"比如对于图像\",{\"1\":{\"956\":1}}],[\"比如对于一个长度为\",{\"1\":{\"733\":1}}],[\"比如我们想让画家画一个人\",{\"1\":{\"956\":1}}],[\"比如我们可能想表示这些事件\",{\"1\":{\"847\":1}}],[\"比如最常见的标准正态分布\",{\"1\":{\"956\":1}}],[\"比如把\",{\"1\":{\"951\":1}}],[\"比如导致梯度尺度失衡\",{\"1\":{\"949\":1}}],[\"比如相邻像素颜色相似且共同构成物体\",{\"1\":{\"942\":1}}],[\"比如训练\",{\"1\":{\"899\":1}}],[\"比如只有两个互斥事件\",{\"1\":{\"848\":1}}],[\"比如你能谈\",{\"1\":{\"847\":1}}],[\"比如测量一个时间\",{\"1\":{\"847\":1}}],[\"比如后续我们会将搭建检索问答链来完成检索问答\",{\"1\":{\"832\":1}}],[\"比如取到了问题部分的内容\",{\"1\":{\"735\":1}}],[\"比如让模型偏向关注附近的\",{\"1\":{\"710\":1}}],[\"比如一个表示\",{\"1\":{\"960\":1}}],[\"比如一个头关注短距离\",{\"1\":{\"710\":1}}],[\"比如一张图像\",{\"1\":{\"947\":1}}],[\"比如一张224x224的图片\",{\"1\":{\"426\":1}}],[\"比如小距离\",{\"1\":{\"710\":1}}],[\"比如上面的例子中\",{\"1\":{\"691\":1}}],[\"比如它看到的\",{\"1\":{\"691\":1}}],[\"比如文本蕴涵\",{\"1\":{\"625\":1}}],[\"比如文本的\",{\"1\":{\"507\":1}}],[\"比如身高\",{\"1\":{\"574\":1}}],[\"比如在下图中\",{\"1\":{\"958\":1}}],[\"比如在计算\",{\"1\":{\"931\":1}}],[\"比如在\",{\"1\":{\"545\":1,\"936\":1}}],[\"比如在内存中从一个索引移动到另一个索引时\",{\"1\":{\"542\":1}}],[\"比如多头注意力中可以分别控制每个\",{\"1\":{\"532\":1}}],[\"比如初始化权重\",{\"1\":{\"521\":1}}],[\"比如relu\",{\"1\":{\"500\":1}}],[\"比如形状是\",{\"1\":{\"477\":1}}],[\"比如这个函数就是高阶函数\",{\"1\":{\"447\":1}}],[\"比如将\",{\"1\":{\"393\":1}}],[\"比如字典大小是\",{\"1\":{\"357\":1}}],[\"比如标注数据很少的任务\",{\"1\":{\"353\":1}}],[\"比如几万\",{\"1\":{\"353\":1}}],[\"比如bert最后进行的softmax操作\",{\"1\":{\"353\":1}}],[\"比如说在视频领域\",{\"1\":{\"350\":1}}],[\"比如说nceloss\",{\"1\":{\"350\":1}}],[\"比如从多分类分布中采样一个类别\",{\"1\":{\"257\":1}}],[\"比如边缘\",{\"1\":{\"157\":1}}],[\"比如椅子的腿\",{\"1\":{\"156\":1}}],[\"比如椅子朝向不同\",{\"1\":{\"152\":1}}],[\"比如法线\",{\"1\":{\"137\":1}}],[\"比如通过\",{\"1\":{\"137\":1}}],[\"比如颜色\",{\"1\":{\"137\":1}}],[\"比如人手\",{\"1\":{\"83\":1}}],[\"比如\",{\"1\":{\"53\":1,\"83\":2,\"122\":2,\"236\":1,\"262\":1,\"310\":1,\"355\":1,\"474\":2,\"501\":1,\"502\":2,\"545\":2,\"582\":1,\"697\":1,\"847\":5,\"942\":1,\"947\":1,\"957\":1}}],[\"比如机器人需要知道\",{\"1\":{\"19\":1}}],[\"效率突破\",{\"1\":{\"668\":1}}],[\"效率优化\",{\"1\":{\"667\":1}}],[\"效率低的推断方法\",{\"1\":{\"942\":1}}],[\"效率低\",{\"1\":{\"663\":1}}],[\"效率比较低\",{\"1\":{\"602\":1}}],[\"效率\",{\"1\":{\"157\":1}}],[\"效率极低\",{\"1\":{\"100\":1}}],[\"效果是如何变化的\",{\"1\":{\"889\":1}}],[\"效果可能不太理想\",{\"1\":{\"828\":1}}],[\"效果评估\",{\"1\":{\"700\":1}}],[\"效果会比不加这句话要好\",{\"1\":{\"619\":1}}],[\"效果相当惊人\",{\"1\":{\"607\":1}}],[\"效果已经非常好了\",{\"1\":{\"607\":1}}],[\"效果越显著\",{\"1\":{\"589\":1}}],[\"效果如下\",{\"1\":{\"582\":1}}],[\"效果对比\",{\"0\":{\"432\":1}}],[\"效果最佳\",{\"1\":{\"285\":1}}],[\"效果最好\",{\"1\":{\"117\":1}}],[\"效果不错\",{\"1\":{\"285\":1}}],[\"效果明显变差\",{\"1\":{\"242\":1}}],[\"效果验证\",{\"0\":{\"177\":1}}],[\"效果\",{\"0\":{\"701\":1},\"1\":{\"150\":3,\"305\":3,\"343\":2,\"520\":1,\"836\":1,\"895\":1,\"918\":1}}],[\"δ\",{\"1\":{\"117\":1,\"119\":4}}],[\"噪声后取\",{\"1\":{\"897\":1}}],[\"噪声对比估计\",{\"1\":{\"282\":1}}],[\"噪声\",{\"1\":{\"208\":1,\"257\":1,\"382\":1,\"887\":1,\"897\":1}}],[\"噪声比例仅\",{\"1\":{\"179\":1}}],[\"噪声比例稍高\",{\"1\":{\"178\":1}}],[\"噪声比例较低\",{\"1\":{\"178\":1}}],[\"噪声严重\",{\"1\":{\"173\":1}}],[\"噪声仍广泛存在\",{\"1\":{\"167\":1}}],[\"噪声增加\",{\"1\":{\"117\":1}}],[\"噪声干扰\",{\"1\":{\"19\":1}}],[\"邻近点\",{\"1\":{\"133\":1}}],[\"邻居点特征\",{\"1\":{\"122\":1}}],[\"邻居坐标减去查询点坐标\",{\"1\":{\"119\":1}}],[\"邻居数量\",{\"1\":{\"117\":1}}],[\"邻域采样数\",{\"1\":{\"121\":1}}],[\"邻域采样点数\",{\"1\":{\"121\":1}}],[\"邻域查询\",{\"1\":{\"119\":1}}],[\"邻域内的\",{\"1\":{\"119\":1}}],[\"邻域构建\",{\"1\":{\"119\":1}}],[\"邻域池化\",{\"1\":{\"117\":1}}],[\"邻域过大\",{\"1\":{\"117\":1}}],[\"邻域过小\",{\"1\":{\"117\":1}}],[\"三部分参数更新\",{\"1\":{\"963\":1}}],[\"三部分组成\",{\"1\":{\"133\":1}}],[\"三类子数据集\",{\"1\":{\"656\":1}}],[\"三类模态专家\",{\"1\":{\"368\":1}}],[\"三步训练框架\",{\"1\":{\"656\":1}}],[\"三大原则来评估模型对齐效果\",{\"1\":{\"654\":1}}],[\"三元组\",{\"1\":{\"631\":1}}],[\"三是接下来要解答的子问题\",{\"1\":{\"622\":1}}],[\"三种损失函数\",{\"1\":{\"305\":1}}],[\"三种设置下的对比表明\",{\"1\":{\"177\":1}}],[\"三\",{\"0\":{\"529\":1,\"552\":1,\"813\":1},\"1\":{\"157\":1,\"655\":1}}],[\"三次sample\",{\"1\":{\"138\":1}}],[\"三层分层特征学习结构\",{\"1\":{\"138\":1}}],[\"三维点云的位置信息比二维像素更关键\",{\"1\":{\"125\":1}}],[\"三线性插值\",{\"1\":{\"116\":1}}],[\"三个通道之间的信息流动\",{\"1\":{\"924\":1}}],[\"三个规模的模型\",{\"1\":{\"823\":1}}],[\"三个标记的点\",{\"1\":{\"572\":1}}],[\"三个模块\",{\"1\":{\"415\":1}}],[\"三个流程\",{\"1\":{\"103\":1}}],[\"三个关键步骤的实现\",{\"1\":{\"99\":1}}],[\"线性混合强化\",{\"1\":{\"894\":1}}],[\"线性映射为\",{\"1\":{\"900\":1}}],[\"线性映射\",{\"1\":{\"710\":1}}],[\"线性映射得到\",{\"1\":{\"380\":1}}],[\"线性上升更新\",{\"1\":{\"633\":1}}],[\"线性增长\",{\"1\":{\"500\":1}}],[\"线性组合\",{\"1\":{\"500\":2}}],[\"线性加权和\",{\"1\":{\"500\":2}}],[\"线性复杂度\",{\"1\":{\"368\":1}}],[\"线性探测\",{\"1\":{\"308\":1,\"320\":1}}],[\"线性投影到与\",{\"1\":{\"421\":1}}],[\"线性投影\",{\"1\":{\"300\":1}}],[\"线性投影或\",{\"1\":{\"112\":1}}],[\"线性缩放\",{\"1\":{\"293\":1}}],[\"线性评估指的是\",{\"1\":{\"352\":1}}],[\"线性评估\",{\"1\":{\"286\":1,\"352\":1}}],[\"线性升到\",{\"1\":{\"286\":1}}],[\"线性基准上达到\",{\"1\":{\"280\":1}}],[\"线性分类头使用\",{\"1\":{\"319\":1}}],[\"线性分类基准上\",{\"1\":{\"280\":1}}],[\"线性分类器训练或数据增强的情况下\",{\"1\":{\"280\":1}}],[\"线性\",{\"1\":{\"224\":1}}],[\"线性从\",{\"1\":{\"204\":1}}],[\"线性变换\",{\"1\":{\"120\":4,\"121\":1,\"206\":1,\"213\":2}}],[\"线性层生成\",{\"1\":{\"380\":1}}],[\"线性层\",{\"1\":{\"116\":2}}],[\"采取的策略是\",{\"1\":{\"944\":1}}],[\"采样相比\",{\"1\":{\"947\":1}}],[\"采样并输入解码器\",{\"1\":{\"946\":1}}],[\"采样效率低\",{\"1\":{\"944\":1}}],[\"采样后生成的样本\",{\"1\":{\"943\":1}}],[\"采样时\",{\"1\":{\"921\":1}}],[\"采样时的阈值\",{\"1\":{\"895\":1}}],[\"采样一次\",{\"1\":{\"946\":1}}],[\"采样一批噪声\",{\"1\":{\"918\":1}}],[\"采样一批真实样本\",{\"1\":{\"918\":1}}],[\"采样一些关键点\",{\"1\":{\"137\":1}}],[\"采样温度参数\",{\"1\":{\"899\":1}}],[\"采样直到达到目标长度\",{\"1\":{\"898\":1}}],[\"采样是一种常用的生成模型采样方法\",{\"1\":{\"896\":1}}],[\"采样生成\",{\"0\":{\"889\":1}}],[\"采样得到\",{\"1\":{\"887\":1}}],[\"采样得到的关键点坐标\",{\"1\":{\"137\":1}}],[\"采样与训练\",{\"1\":{\"633\":1}}],[\"采样库\",{\"0\":{\"517\":1}}],[\"采样多个点\",{\"1\":{\"502\":1}}],[\"采样\",{\"0\":{\"896\":1},\"1\":{\"373\":1,\"895\":1,\"897\":2,\"899\":1,\"946\":1,\"952\":1}}],[\"采样策略\",{\"1\":{\"373\":1}}],[\"采样过程详解\",{\"1\":{\"258\":1}}],[\"采样过程使用了一种称为\",{\"1\":{\"258\":1}}],[\"采样越接近\",{\"1\":{\"257\":1}}],[\"采样每个图像对应的负文本\",{\"1\":{\"207\":1}}],[\"采样每个文本对应的负图像\",{\"1\":{\"207\":1}}],[\"采样难负样本\",{\"1\":{\"190\":1}}],[\"采样的\",{\"1\":{\"964\":1}}],[\"采样的样本数\",{\"1\":{\"518\":1}}],[\"采样的阈值\",{\"1\":{\"188\":1,\"898\":1}}],[\"采样的关键点数量\",{\"1\":{\"137\":2}}],[\"采样半径\",{\"1\":{\"137\":1}}],[\"采样层\",{\"1\":{\"133\":1}}],[\"采样点数量\",{\"1\":{\"137\":1}}],[\"采样点的索引\",{\"1\":{\"121\":1}}],[\"采样点索引\",{\"1\":{\"121\":1}}],[\"采样子集\",{\"1\":{\"116\":1}}],[\"采用单任务微调\",{\"1\":{\"680\":1}}],[\"采用rmsnorm对子层输入归一化\",{\"1\":{\"667\":1}}],[\"采用kv\",{\"1\":{\"663\":1}}],[\"采用proximal\",{\"1\":{\"654\":1}}],[\"采用模型层间和矩阵级别的并行方式进行\",{\"1\":{\"647\":1}}],[\"采用了预训练和微调的学习方法\",{\"1\":{\"824\":1}}],[\"采用了\",{\"1\":{\"823\":1}}],[\"采用了更科学的数据配比\",{\"1\":{\"823\":1}}],[\"采用了一种更巧妙的解决思路\",{\"1\":{\"426\":1}}],[\"采用了阶段式预训练策略\",{\"1\":{\"377\":1}}],[\"采用无卷积的浅层线性投影直接将图像块\",{\"1\":{\"388\":1}}],[\"采用无检测器的图像编码器和文本编码器独立编码\",{\"1\":{\"194\":1}}],[\"采用共享的\",{\"1\":{\"369\":1}}],[\"采用掩码语言建模方法\",{\"1\":{\"368\":1}}],[\"采用三种联合预训练任务\",{\"1\":{\"368\":1}}],[\"采用经典的\",{\"1\":{\"329\":1}}],[\"采用线性增长\",{\"1\":{\"293\":1}}],[\"采用多视角图像输入\",{\"1\":{\"293\":1}}],[\"采用图像输入到编码器\",{\"1\":{\"268\":1}}],[\"采用标准的\",{\"1\":{\"233\":1}}],[\"采用的是一个共享的\",{\"1\":{\"221\":1}}],[\"采用加权随机采样而非直接取相似度最大的负样本\",{\"1\":{\"207\":1}}],[\"采用动量更新方式对\",{\"1\":{\"363\":1}}],[\"采用动量慢更新策略\",{\"1\":{\"192\":1}}],[\"采用动量编码器来生成表示\",{\"1\":{\"172\":1}}],[\"采用itc\",{\"1\":{\"192\":1}}],[\"采用itc和itm目标执行完微调后\",{\"1\":{\"191\":1}}],[\"采用moco的动量慢更新策略进行学习\",{\"1\":{\"190\":1}}],[\"采用bert分词器实现论文中的文本预处理\",{\"1\":{\"187\":1}}],[\"采用\",{\"1\":{\"172\":1,\"187\":1,\"213\":3,\"224\":1,\"272\":1,\"286\":1,\"305\":1,\"315\":2,\"318\":1,\"329\":1,\"334\":1,\"368\":1,\"373\":1,\"679\":1,\"680\":1,\"823\":3}}],[\"采用编码器\",{\"1\":{\"123\":1,\"269\":1}}],[\"采用批处理方式提高计算效率\",{\"1\":{\"119\":1}}],[\"采用减法关系并在注意力分支和特征分支都加入位置编码\",{\"1\":{\"113\":1}}],[\"近两年\",{\"1\":{\"955\":1}}],[\"近距离用细桶\",{\"1\":{\"710\":1}}],[\"近代自然语言处理技术发展的\",{\"1\":{\"607\":1}}],[\"近似回传\",{\"1\":{\"963\":1}}],[\"近似真实后验\",{\"1\":{\"885\":1}}],[\"近似值\",{\"1\":{\"811\":1}}],[\"近似于\",{\"1\":{\"588\":1}}],[\"近似采样\",{\"1\":{\"255\":1}}],[\"近期\",{\"1\":{\"884\":1}}],[\"近期研究考虑过各种各样的目标\",{\"1\":{\"626\":1}}],[\"近期研究通过将视觉模型与llms结合\",{\"1\":{\"300\":1}}],[\"近期研究发现\",{\"1\":{\"282\":1}}],[\"近期成为卷积神经网络\",{\"1\":{\"280\":1}}],[\"近期的方法如\",{\"1\":{\"246\":1}}],[\"近期一些研究进一步利用\",{\"1\":{\"20\":1}}],[\"近年来\",{\"1\":{\"220\":1,\"268\":1,\"269\":1,\"413\":1,\"639\":1,\"640\":1,\"646\":1,\"942\":1}}],[\"近年来的在线蒸馏则使用多个同时训练的模型进行知识迁移\",{\"1\":{\"195\":1}}],[\"近年来生成模型被用于文本任务的样本合成\",{\"1\":{\"169\":1}}],[\"近邻集合\",{\"1\":{\"113\":1}}],[\"聚焦难分类样本\",{\"1\":{\"589\":1}}],[\"聚焦参数\",{\"1\":{\"102\":1,\"589\":2}}],[\"聚类式方法\",{\"1\":{\"282\":1}}],[\"聚类簇数\",{\"1\":{\"213\":1}}],[\"聚类的标准迭代流程\",{\"1\":{\"213\":1}}],[\"聚合了所有\",{\"1\":{\"529\":1}}],[\"聚合出不同长度的图像嵌入\",{\"1\":{\"272\":1}}],[\"聚合机制\",{\"1\":{\"217\":1}}],[\"聚合头\",{\"1\":{\"215\":1}}],[\"聚合策略\",{\"1\":{\"215\":1}}],[\"聚合策略通过\",{\"1\":{\"215\":1}}],[\"聚合策略对效果的影响\",{\"1\":{\"215\":1}}],[\"聚合策略个人理解\",{\"1\":{\"214\":1}}],[\"聚合全局信息并参与浅层\",{\"1\":{\"214\":1}}],[\"聚合全局信息\",{\"1\":{\"210\":1,\"215\":1}}],[\"聚合所有点的信息\",{\"1\":{\"150\":1,\"157\":1}}],[\"聚合邻域特征\",{\"1\":{\"121\":1}}],[\"聚合邻域信息\",{\"1\":{\"121\":1}}],[\"聚合局部信息\",{\"1\":{\"121\":1}}],[\"聚合\",{\"1\":{\"116\":1,\"119\":1,\"274\":1}}],[\"聚合集合特征\",{\"1\":{\"110\":1}}],[\"优质的\",{\"1\":{\"836\":1}}],[\"优缺点\",{\"1\":{\"589\":1}}],[\"优缺点对比\",{\"1\":{\"346\":1}}],[\"优先考虑精确性和推理步骤的正确性\",{\"1\":{\"823\":1}}],[\"优先于准确率\",{\"1\":{\"567\":1}}],[\"优先级低于\",{\"1\":{\"513\":1}}],[\"优于opt\",{\"1\":{\"668\":1,\"670\":1}}],[\"优于minerva\",{\"1\":{\"668\":1}}],[\"优于简单\",{\"1\":{\"658\":1}}],[\"优于\",{\"1\":{\"310\":2}}],[\"优异的\",{\"1\":{\"280\":1}}],[\"优化嵌入空间\",{\"0\":{\"960\":1}}],[\"优化编码器和解码器\",{\"0\":{\"959\":1}}],[\"优化目标函数\",{\"0\":{\"946\":1}}],[\"优化目标是调整参数\",{\"1\":{\"943\":1}}],[\"优化将难以进行\",{\"1\":{\"943\":1}}],[\"优化该松弛的\",{\"1\":{\"886\":1}}],[\"优化\",{\"1\":{\"814\":1,\"887\":1}}],[\"优化打印等交互体验\",{\"1\":{\"812\":1}}],[\"优化正向传播实现\",{\"1\":{\"812\":1}}],[\"优化问题中常使用特定函数评估算法性能\",{\"1\":{\"811\":1}}],[\"优化效果总结\",{\"1\":{\"807\":1}}],[\"优化反向传播的内存消耗\",{\"1\":{\"807\":1}}],[\"优化内存消耗\",{\"0\":{\"807\":1}}],[\"优化了反向传播的实现\",{\"1\":{\"797\":1}}],[\"优化策略\",{\"1\":{\"679\":1}}],[\"优化llama\",{\"1\":{\"669\":1}}],[\"优化人类偏好\",{\"1\":{\"654\":1}}],[\"优化后的内存管理确保框架在处理大规模计算时的稳定性和效率\",{\"1\":{\"806\":1}}],[\"优化后的roberta在多个基准测试\",{\"1\":{\"677\":1}}],[\"优化后\",{\"1\":{\"522\":1}}],[\"优化难度\",{\"1\":{\"500\":1}}],[\"优化图文对比损失\",{\"1\":{\"375\":1}}],[\"优化不同分辨率输入\",{\"1\":{\"327\":1}}],[\"优化多模态对话能力\",{\"1\":{\"305\":1}}],[\"优化配置\",{\"1\":{\"224\":1}}],[\"优化器准备\",{\"1\":{\"918\":1}}],[\"优化器与\",{\"1\":{\"819\":1}}],[\"优化器与学习率调度器\",{\"1\":{\"700\":1}}],[\"优化器改进\",{\"1\":{\"680\":1}}],[\"优化器方案\",{\"1\":{\"633\":1}}],[\"优化器及余弦调度器\",{\"1\":{\"293\":1}}],[\"优化器采用\",{\"1\":{\"236\":1}}],[\"优化器使用\",{\"1\":{\"176\":1,\"315\":1,\"316\":1}}],[\"优化器设置\",{\"1\":{\"104\":1}}],[\"优化器\",{\"1\":{\"46\":1,\"104\":1,\"224\":1,\"265\":1,\"286\":1,\"667\":1,\"679\":1,\"819\":1}}],[\"优势\",{\"1\":{\"157\":1,\"586\":1,\"587\":1,\"592\":1,\"681\":1}}],[\"优点\",{\"1\":{\"110\":1,\"268\":3,\"346\":1,\"368\":2,\"369\":3,\"589\":1,\"710\":1}}],[\"体型是0\",{\"1\":{\"956\":1}}],[\"体型是胖还是壮\",{\"1\":{\"956\":1}}],[\"体积\",{\"1\":{\"873\":1}}],[\"体验优化\",{\"1\":{\"836\":1}}],[\"体验bert的预训练过程是如何实现的\",{\"1\":{\"695\":1}}],[\"体会它们的有趣之处\",{\"1\":{\"753\":1}}],[\"体重\",{\"1\":{\"574\":2}}],[\"体现了自动微分的优势\",{\"1\":{\"811\":1}}],[\"体现了全词掩码在预训练时以及图像增强在微调时的重要性\",{\"1\":{\"388\":1}}],[\"体现出模型对语义的灵活解析能力\",{\"1\":{\"49\":1}}],[\"体现出数据集在样本分布上的全面性和均衡性\",{\"1\":{\"43\":1}}],[\"体素网格\",{\"1\":{\"159\":1}}],[\"体素\",{\"1\":{\"157\":1}}],[\"体素化量化仍可能导致几何细节丢失\",{\"1\":{\"110\":1}}],[\"体素型网络\",{\"1\":{\"110\":1}}],[\"超轻量推理\",{\"1\":{\"823\":1}}],[\"超强推理能力\",{\"1\":{\"823\":1}}],[\"超长截断\",{\"1\":{\"713\":1}}],[\"超级大的多分类问题\",{\"1\":{\"355\":1}}],[\"超过了随机水平\",{\"1\":{\"825\":1}}],[\"超过了如\",{\"1\":{\"308\":1}}],[\"超过按最大处理\",{\"1\":{\"710\":1}}],[\"超过部分无监督方法\",{\"1\":{\"641\":1}}],[\"超过此前需依赖上下文词约束的sota方法\",{\"1\":{\"641\":1}}],[\"超过起始层索引才启用\",{\"1\":{\"380\":1}}],[\"超过历史最佳\",{\"1\":{\"106\":1}}],[\"超大规模\",{\"1\":{\"649\":1}}],[\"超大规模语言模型可以显著减少对任务特定数据的需求\",{\"1\":{\"646\":1}}],[\"超大规模基础模型\",{\"1\":{\"224\":1}}],[\"超大\",{\"1\":{\"282\":1}}],[\"超越xlnet\",{\"1\":{\"684\":1,\"685\":1}}],[\"超越flan\",{\"1\":{\"668\":1,\"669\":1}}],[\"超越chinchilla\",{\"1\":{\"668\":1}}],[\"超越gpt\",{\"1\":{\"335\":1}}],[\"超越如\",{\"1\":{\"309\":1}}],[\"超越了一些基于检索系统的模型\",{\"1\":{\"648\":1}}],[\"超越了同规模cnn的自监督方法\",{\"1\":{\"291\":1}}],[\"超越了同规模\",{\"1\":{\"280\":1}}],[\"超越了基于监督学习或对比学习的主流方法\",{\"1\":{\"252\":1}}],[\"超越多个专用模型的性能\",{\"1\":{\"268\":1}}],[\"超越大量先前方法\",{\"1\":{\"109\":1}}],[\"超参数调整\",{\"1\":{\"593\":1}}],[\"超参数设置说明\",{\"1\":{\"592\":1}}],[\"超参数为\",{\"1\":{\"236\":1,\"315\":1}}],[\"超参数\",{\"1\":{\"224\":1}}],[\"创新的动态高分辨率处理策略\",{\"1\":{\"337\":1}}],[\"创建的\",{\"1\":{\"833\":1,\"888\":1}}],[\"创建配置类config\",{\"1\":{\"807\":1}}],[\"创建变量的函数\",{\"0\":{\"759\":1}}],[\"创建用以区分special\",{\"1\":{\"713\":1}}],[\"创建句子辨识列表\",{\"1\":{\"713\":1}}],[\"创建反向映射\",{\"1\":{\"697\":1}}],[\"创建环境并安装一些常用包\",{\"1\":{\"550\":1}}],[\"创建环境时指定\",{\"1\":{\"550\":1}}],[\"创建新的线性层\",{\"1\":{\"699\":1}}],[\"创建新环境\",{\"0\":{\"550\":1}}],[\"创建新张量\",{\"1\":{\"472\":1}}],[\"创建了一个一维张量\",{\"1\":{\"542\":1}}],[\"创建预输出层\",{\"1\":{\"431\":1}}],[\"创建归一化层\",{\"1\":{\"431\":1}}],[\"创建encoder\",{\"1\":{\"431\":1}}],[\"创建丢弃层\",{\"1\":{\"428\":1,\"431\":1}}],[\"创建图像块嵌入层\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"创建图像注意力掩码\",{\"1\":{\"421\":1}}],[\"创建可学习的位置嵌入\",{\"1\":{\"428\":1,\"431\":1}}],[\"创建可学习的分类标记\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"创建可学习参数\",{\"1\":{\"380\":1}}],[\"创建可视化窗口\",{\"1\":{\"107\":1}}],[\"创建损失函数\",{\"1\":{\"293\":1}}],[\"创建一个连续的转置副本\",{\"1\":{\"545\":1}}],[\"创建一个连续张量\",{\"1\":{\"490\":1}}],[\"创建一个张量\",{\"1\":{\"544\":1,\"545\":2}}],[\"创建一个\",{\"1\":{\"542\":1}}],[\"创建一个不连续的视图\",{\"1\":{\"491\":1}}],[\"创建一个不连续张量的常见操作\",{\"1\":{\"490\":1}}],[\"创建一个空环境\",{\"1\":{\"550\":1}}],[\"创建一个空\",{\"1\":{\"293\":1}}],[\"创建一个全零点作为\",{\"1\":{\"137\":1}}],[\"创建\",{\"1\":{\"255\":1,\"293\":1,\"429\":1}}],[\"创建向量量化器\",{\"1\":{\"213\":1}}],[\"创建编码器和解码器\",{\"1\":{\"213\":1}}],[\"创建文本编码器\",{\"1\":{\"192\":1}}],[\"创建主视觉编码器\",{\"1\":{\"192\":1}}],[\"创建动量编码器\",{\"1\":{\"190\":1}}],[\"创建选择标记数组\",{\"1\":{\"121\":1}}],[\"创建输出索引张量\",{\"1\":{\"121\":1}}],[\"创下多个最先进水平\",{\"1\":{\"109\":1}}],[\"本模型之所以具备可计算性\",{\"1\":{\"949\":1}}],[\"本地文件访问时内存管理升级\",{\"1\":{\"823\":1}}],[\"本地批次\",{\"1\":{\"190\":1}}],[\"本阶段的目标是打造一个\",{\"1\":{\"819\":1}}],[\"本步骤使用tinypytorch实现了梯度下降法\",{\"1\":{\"816\":1}}],[\"本步骤的目标是找到使rosenbrock函数输出值最小的和\",{\"1\":{\"816\":1}}],[\"本步骤将处理rosenbrock函数\",{\"1\":{\"816\":1}}],[\"本步骤选取3个经典测试函数\",{\"1\":{\"811\":1}}],[\"本系列文章试图揭开这些机制背后的本质\",{\"1\":{\"799\":1}}],[\"本系列将带领读者从零开始创建一个深度学习框架\",{\"1\":{\"753\":1}}],[\"本部分感兴趣的可以回顾高数中微分定义部分内容\",{\"1\":{\"770\":1}}],[\"本部分图片来源\",{\"1\":{\"547\":1}}],[\"本类中不直接使用\",{\"1\":{\"590\":1}}],[\"本身\",{\"1\":{\"457\":1,\"847\":1}}],[\"本身接受参数\",{\"1\":{\"453\":1}}],[\"本次训练是基于预训练好的vit\",{\"1\":{\"435\":1}}],[\"本研究训练了一个参数量为\",{\"1\":{\"884\":1}}],[\"本研究指出\",{\"1\":{\"653\":1}}],[\"本研究提出的\",{\"1\":{\"269\":1}}],[\"本研究为\",{\"1\":{\"252\":1}}],[\"本论文提出了一种极简设计的\",{\"1\":{\"268\":1}}],[\"本工作探索如何将\",{\"1\":{\"216\":1}}],[\"本工作中视觉\",{\"1\":{\"214\":1}}],[\"本节开始为扩展阅读内容\",{\"1\":{\"899\":1}}],[\"本节最开始给出的前向传播流程图已经清晰展示了\",{\"1\":{\"893\":1}}],[\"本节讨论的是定义在\",{\"1\":{\"855\":1}}],[\"本节代码\",{\"1\":{\"752\":1,\"798\":1,\"813\":1,\"818\":1}}],[\"本节代码不算复杂\",{\"1\":{\"214\":1}}],[\"本节将详细讨论高斯分布\",{\"1\":{\"869\":1}}],[\"本节将会对bert模型实现的部分细节进行说明\",{\"1\":{\"702\":1}}],[\"本节将根据\",{\"1\":{\"663\":1}}],[\"本节我们将基于clip预训练模型实现zero\",{\"1\":{\"410\":1}}],[\"本节我们将对多模态bert的前向传播基本流程进行讲解\",{\"1\":{\"396\":1}}],[\"本节我们将一点点完成该类代码的拆解\",{\"1\":{\"380\":1}}],[\"本节我们会针对\",{\"1\":{\"255\":1}}],[\"本节我们来详细解析一下它的实现逻辑\",{\"1\":{\"213\":1}}],[\"本质上是一种\",{\"1\":{\"894\":1}}],[\"本质上是一个\",{\"1\":{\"823\":1}}],[\"本质上是一个线性\",{\"1\":{\"540\":1}}],[\"本质上就是\",{\"1\":{\"885\":1}}],[\"本质上\",{\"1\":{\"600\":1,\"616\":1}}],[\"本质上适合点云处理\",{\"1\":{\"109\":1}}],[\"本质\",{\"1\":{\"457\":1}}],[\"本质是一个\",{\"1\":{\"450\":1}}],[\"本质是一个二分类任务\",{\"1\":{\"172\":1}}],[\"本质就是一个交叉熵损失函数\",{\"1\":{\"355\":1}}],[\"本文转载至\",{\"1\":{\"965\":1}}],[\"本文将使用\",{\"1\":{\"928\":1}}],[\"本文将通过一个花卉分类的实战案例结合vit原论文\",{\"1\":{\"423\":1}}],[\"本文摘录至\",{\"1\":{\"927\":1}}],[\"本文只涉及原始\",{\"1\":{\"921\":1}}],[\"本文使用的是谷歌的中文预训练模型\",{\"1\":{\"712\":1}}],[\"本文使用的是训练了9个epoch后的模型权重进行的推理演示\",{\"1\":{\"107\":1}}],[\"本文所展示的bert预训练属于教学级别的\",{\"1\":{\"701\":1}}],[\"本文不再全部copy展示\",{\"1\":{\"699\":1}}],[\"本文中的\",{\"1\":{\"699\":1}}],[\"本文引入了一种有效的对齐方法\",{\"1\":{\"654\":1}}],[\"本文通过gpt\",{\"1\":{\"643\":1}}],[\"本文采用字节级\",{\"1\":{\"640\":1}}],[\"本文进一步假设\",{\"1\":{\"640\":1}}],[\"本文基于\",{\"1\":{\"582\":1,\"739\":1}}],[\"本文改编bert代码讲解基于blip项目展开\",{\"1\":{\"395\":1}}],[\"本文常用\",{\"1\":{\"286\":1}}],[\"本文提出了一种对齐语言模型与用户意图的策略\",{\"1\":{\"654\":1}}],[\"本文提出的方法在效率上大大提升且表现出相似的性能\",{\"1\":{\"394\":1}}],[\"本文提出的\",{\"1\":{\"327\":1}}],[\"本文提出的解决方案\",{\"1\":{\"19\":1}}],[\"本文提出\",{\"1\":{\"114\":1,\"217\":1}}],[\"本文方法创新点\",{\"1\":{\"110\":1}}],[\"本文创新\",{\"1\":{\"76\":1}}],[\"本文创新点\",{\"1\":{\"73\":1}}],[\"本文区别\",{\"1\":{\"75\":1}}],[\"本文构建了\",{\"1\":{\"40\":1}}],[\"本文是\",{\"1\":{\"20\":1}}],[\"本文的transformer使用了self\",{\"1\":{\"740\":1}}],[\"本文的核心假设是\",{\"1\":{\"639\":1}}],[\"本文的方法正是通过引入\",{\"1\":{\"20\":1}}],[\"本文的主要贡献可以总结如下\",{\"1\":{\"19\":1}}],[\"完善的工具使用\",{\"1\":{\"823\":1}}],[\"完善框架的模块结构\",{\"1\":{\"814\":1}}],[\"完美的模型在某个阈值下的\",{\"1\":{\"569\":1}}],[\"完美的模型不会产生假正例\",{\"1\":{\"564\":1}}],[\"完美的模型没有假正例和假负例\",{\"1\":{\"562\":1}}],[\"完全协方差矩阵\",{\"1\":{\"871\":1}}],[\"完全依赖语言模型对任务上下文的理解能力\",{\"1\":{\"640\":1}}],[\"完全随机猜测的\",{\"1\":{\"570\":1}}],[\"完全支持\",{\"1\":{\"157\":1}}],[\"完全由自注意力和逐点操作组成\",{\"1\":{\"109\":1}}],[\"完成预处理后\",{\"1\":{\"836\":1}}],[\"完成了自动微分的核心算法\",{\"1\":{\"797\":1}}],[\"完成特定任务\",{\"1\":{\"346\":1}}],[\"完成点云分割任务的过程是一个典型的\",{\"1\":{\"143\":1}}],[\"完成\",{\"1\":{\"142\":1,\"213\":1,\"382\":2,\"663\":1,\"836\":1}}],[\"完成领域值信息聚合过程\",{\"1\":{\"119\":1}}],[\"完成对同一个邻居点的所有通道分组应用相同权重分配的过程\",{\"1\":{\"119\":1}}],[\"完整架构见附录\",{\"1\":{\"887\":1}}],[\"完整代码\",{\"0\":{\"412\":1}}],[\"完整代码实现如下\",{\"1\":{\"119\":1,\"121\":1,\"293\":1}}],[\"完整代码实现如下所示\",{\"1\":{\"119\":1,\"122\":1}}],[\"完整的代码实现部分\",{\"1\":{\"699\":1}}],[\"完整的单尺度分组分类流程为\",{\"1\":{\"138\":1}}],[\"完整的向量自注意力计算代码实现如下\",{\"1\":{\"119\":1}}],[\"完整方法\",{\"1\":{\"106\":1}}],[\"池化输出实现\",{\"1\":{\"397\":1}}],[\"池化输出\",{\"1\":{\"397\":1}}],[\"池化\",{\"1\":{\"110\":1,\"502\":1}}],[\"池化或连续卷积直接在点上操作\",{\"1\":{\"109\":1}}],[\"池化后拼接\",{\"1\":{\"78\":1}}],[\"示例输出\",{\"1\":{\"815\":2}}],[\"示例验证\",{\"1\":{\"808\":1}}],[\"示例显示\",{\"1\":{\"657\":1}}],[\"示例或文本上下文隐式表达\",{\"1\":{\"656\":1}}],[\"示例从开发集提取\",{\"1\":{\"647\":1}}],[\"示例后\",{\"1\":{\"640\":1}}],[\"示例中设置为了1\",{\"1\":{\"595\":1}}],[\"示例4\",{\"1\":{\"480\":1}}],[\"示例3\",{\"1\":{\"480\":1,\"550\":1}}],[\"示例2\",{\"1\":{\"480\":1,\"481\":1,\"550\":1}}],[\"示例1\",{\"1\":{\"480\":1,\"481\":1,\"550\":1}}],[\"示例\",{\"1\":{\"444\":1,\"463\":1,\"484\":1,\"488\":4,\"490\":1,\"491\":1,\"500\":2,\"514\":1,\"551\":1,\"735\":1,\"808\":1}}],[\"示例场景\",{\"1\":{\"346\":1}}],[\"示例文本查询\",{\"1\":{\"107\":1}}],[\"示例数据\",{\"1\":{\"107\":1}}],[\"示例为\",{\"1\":{\"35\":2,\"36\":2}}],[\"蓝色标记出的部分是提供给llm的示例\",{\"1\":{\"620\":1}}],[\"蓝色\",{\"1\":{\"107\":1}}],[\"蓝色=背景\",{\"1\":{\"107\":1}}],[\"二进制比特\",{\"1\":{\"906\":1}}],[\"二项分布关注试验次数固定\",{\"1\":{\"860\":1}}],[\"二项分布\",{\"1\":{\"856\":1}}],[\"二是已解决的子问题及其答案列表\",{\"1\":{\"622\":1}}],[\"二值化或软标签\",{\"1\":{\"586\":1}}],[\"二值化显示\",{\"1\":{\"107\":1}}],[\"二元分类器的每个输出有四种可能的结果\",{\"1\":{\"561\":1}}],[\"二元分类场景\",{\"0\":{\"560\":1}}],[\"二维嵌入索引矩阵\",{\"1\":{\"963\":1}}],[\"二维的\",{\"1\":{\"963\":1}}],[\"二维协方差矩阵\",{\"1\":{\"574\":1}}],[\"二维布局\",{\"1\":{\"542\":2}}],[\"二维张量\",{\"1\":{\"481\":1}}],[\"二维图像\",{\"1\":{\"371\":1}}],[\"二者都是对卷积操作的卷积核做了掩码处理\",{\"1\":{\"923\":1}}],[\"二者都要在同一个\",{\"1\":{\"709\":1}}],[\"二者都加同样的辅助lm\",{\"1\":{\"635\":1}}],[\"二者参数分别为\",{\"1\":{\"285\":1}}],[\"二者分别从不同实例中采样\",{\"1\":{\"43\":1}}],[\"二分类交叉熵\",{\"1\":{\"910\":1}}],[\"二分类task\",{\"0\":{\"419\":1}}],[\"二分类问题\",{\"1\":{\"355\":1}}],[\"二分类\",{\"1\":{\"207\":1}}],[\"二\",{\"0\":{\"525\":1,\"551\":1,\"798\":1},\"1\":{\"157\":1,\"655\":1}}],[\"二次sample\",{\"1\":{\"138\":1}}],[\"增大批次和训练数据\",{\"1\":{\"688\":1}}],[\"增大批次和数据规模\",{\"1\":{\"679\":1}}],[\"增大批次规模\",{\"1\":{\"678\":1}}],[\"增加数据的生成概率\",{\"1\":{\"943\":1}}],[\"增加网络的表达能力和非线性\",{\"1\":{\"823\":1}}],[\"增加模型大小或使用更多数据\",{\"1\":{\"822\":1}}],[\"增加模型鲁棒性\",{\"1\":{\"380\":1}}],[\"增加迭代次数设为\",{\"1\":{\"816\":1}}],[\"增加多样性\",{\"1\":{\"681\":1}}],[\"增加旁路矩阵来模拟全参数微调\",{\"1\":{\"614\":1}}],[\"增加\",{\"1\":{\"613\":1}}],[\"增加了检索步骤的耗时\",{\"1\":{\"830\":1}}],[\"增加了二分类器判断可答性\",{\"1\":{\"680\":1}}],[\"增加了模型层数\",{\"1\":{\"610\":1}}],[\"增加了图像的多样性\",{\"1\":{\"425\":1}}],[\"增加一些特定长度的特殊token\",{\"1\":{\"604\":1}}],[\"增加一个维度\",{\"1\":{\"478\":1}}],[\"增加批次维度\",{\"1\":{\"582\":1}}],[\"增加神经元数量\",{\"1\":{\"500\":2}}],[\"增加阶数\",{\"1\":{\"500\":1}}],[\"增加阶数提高精度\",{\"1\":{\"500\":1}}],[\"增加到\",{\"1\":{\"285\":1,\"286\":1}}],[\"增加到设定的最大值\",{\"1\":{\"204\":1}}],[\"增加通道数\",{\"1\":{\"121\":1}}],[\"增加特征维度\",{\"1\":{\"121\":1,\"143\":1}}],[\"增加batch维度\",{\"1\":{\"107\":1}}],[\"增强它区分真假样本的能力\",{\"1\":{\"918\":1}}],[\"增强采样精度\",{\"1\":{\"899\":1}}],[\"增强阶段\",{\"1\":{\"829\":1}}],[\"增强和生成四个阶段\",{\"1\":{\"829\":1}}],[\"增强了模型的推理和理解能力\",{\"1\":{\"828\":1}}],[\"增强了内容的可追溯性\",{\"1\":{\"828\":1}}],[\"增强了\",{\"1\":{\"823\":1}}],[\"增强整体模型的能力\",{\"1\":{\"614\":1}}],[\"增强区域匹配\",{\"1\":{\"587\":1}}],[\"增强鲁棒性\",{\"1\":{\"382\":1}}],[\"增强视觉理解能力并适配不同llms\",{\"1\":{\"323\":1}}],[\"增强模型文字识别能力\",{\"1\":{\"332\":1}}],[\"增强模型生成能力\",{\"1\":{\"305\":1}}],[\"增强模型对上下文的理解能力\",{\"1\":{\"263\":1}}],[\"增强模型对\",{\"1\":{\"215\":1}}],[\"增强训练稳定性\",{\"1\":{\"293\":1}}],[\"增强分布尖锐性\",{\"1\":{\"285\":1}}],[\"增强特征提取\",{\"1\":{\"255\":1}}],[\"增强策略包括随机缩放裁剪\",{\"1\":{\"236\":1}}],[\"增强样本多样性\",{\"1\":{\"192\":1}}],[\"增强表示\",{\"1\":{\"122\":1}}],[\"增强现实和机器人等应用中\",{\"1\":{\"109\":1}}],[\"增强现实等领域\",{\"1\":{\"72\":1}}],[\"增强后的点特征进行卷积\",{\"1\":{\"100\":1}}],[\"增强的点特征图\",{\"1\":{\"98\":1}}],[\"增强点特征的语义判别能力\",{\"1\":{\"94\":1}}],[\"增强稳定性\",{\"1\":{\"69\":1}}],[\"增强空间特征表达\",{\"1\":{\"69\":1}}],[\"增强类比能力\",{\"1\":{\"36\":1}}],[\"增强\",{\"1\":{\"23\":1,\"69\":2,\"183\":1}}],[\"杀死训练进程\",{\"1\":{\"107\":1}}],[\"脚本时非常有用\",{\"1\":{\"107\":1}}],[\"会把图像生成问题看成学习一个图像的分布\",{\"1\":{\"925\":1}}],[\"会把图像切成\",{\"1\":{\"385\":1}}],[\"会丧失多样性\",{\"1\":{\"894\":1}}],[\"会\",{\"1\":{\"893\":1}}],[\"会产生环形分布\",{\"1\":{\"944\":1}}],[\"会产生\",{\"1\":{\"881\":1}}],[\"会发现线的形状类似香蕉\",{\"1\":{\"816\":1}}],[\"会去分步骤思考\",{\"1\":{\"619\":1}}],[\"会去定义一些规则\",{\"1\":{\"349\":1}}],[\"会有多个用于不同目的的权重参数矩阵\",{\"1\":{\"600\":1}}],[\"会降低也是如此\",{\"1\":{\"572\":1}}],[\"会重新中心化数据\",{\"1\":{\"522\":1}}],[\"会分配一块新的连续内存\",{\"1\":{\"491\":1}}],[\"会额外返回每个唯一值的\",{\"1\":{\"480\":1}}],[\"会额外返回一个张量\",{\"1\":{\"480\":1}}],[\"会改变张量的\",{\"1\":{\"469\":1}}],[\"会自动把这些变量\",{\"1\":{\"448\":1}}],[\"会泄漏\",{\"1\":{\"444\":1}}],[\"会将这些输入展平\",{\"1\":{\"737\":1}}],[\"会将输入图像分割成大小为\",{\"1\":{\"435\":1}}],[\"会将其编码为上下文相关的向量表示\",{\"1\":{\"229\":1}}],[\"会在训练初期使用较短序列\",{\"1\":{\"680\":1}}],[\"会在get\",{\"1\":{\"420\":1}}],[\"会在coco数据集每个样本原有caption的基础上添加一个prompt\",{\"1\":{\"187\":1}}],[\"会从缓存中取出对应层先前缓存的key\",{\"1\":{\"420\":1}}],[\"会进行相应的错误提示并返回\",{\"1\":{\"410\":1}}],[\"会进行推理能力结果更新\",{\"1\":{\"107\":1}}],[\"会导致概率的定义出现矛盾或不收敛\",{\"1\":{\"847\":1}}],[\"会导致\",{\"1\":{\"846\":2,\"913\":1}}],[\"会导致模型很难收敛\",{\"1\":{\"355\":1}}],[\"会导致非正交\",{\"1\":{\"152\":1}}],[\"会让模型只关注那个困难的负样本\",{\"1\":{\"355\":1}}],[\"会根据输入模态选择不同专家\",{\"1\":{\"372\":1}}],[\"会根据子文件夹名自动生成\",{\"1\":{\"293\":1}}],[\"会根据不同功能分支前向三次\",{\"1\":{\"172\":1}}],[\"会返回连续相同值的唯一值及计数\",{\"1\":{\"293\":1}}],[\"会得到\",{\"1\":{\"293\":1}}],[\"会出现\",{\"1\":{\"285\":1}}],[\"会使用\",{\"1\":{\"264\":1,\"420\":1}}],[\"会促使模型更关注短程依赖和高频细节\",{\"1\":{\"234\":1}}],[\"会随机掩码部分文本\",{\"1\":{\"220\":1}}],[\"会被替换成嵌入空间里最近的真嵌入\",{\"1\":{\"961\":1}}],[\"会被编码成一个较短的向量\",{\"1\":{\"956\":1}}],[\"会被正常回收\",{\"1\":{\"806\":1}}],[\"会被立即回收\",{\"1\":{\"806\":1}}],[\"会被交叉熵忽略\",{\"1\":{\"384\":1}}],[\"会被当作\",{\"1\":{\"382\":1}}],[\"会被展平为向量并线性投影\",{\"1\":{\"231\":1}}],[\"会被随机选择并遮挡\",{\"1\":{\"214\":1}}],[\"会被划分为\",{\"1\":{\"212\":1}}],[\"会作为\",{\"1\":{\"207\":1}}],[\"会与图像嵌入一起送入多模态编码器进行融合\",{\"1\":{\"197\":1}}],[\"会影响特征提取的一致性\",{\"1\":{\"152\":1}}],[\"会强制标准输出也像标准错误一样\",{\"1\":{\"107\":1}}],[\"会按照\",{\"1\":{\"55\":1}}],[\"当且仅当\",{\"1\":{\"847\":1}}],[\"当时的研究主要集中在采用统计学习方法来预测词汇\",{\"1\":{\"822\":1}}],[\"当表达式为3\",{\"1\":{\"809\":1}}],[\"当表达式中的左右操作数类型不同时\",{\"1\":{\"809\":1}}],[\"当操作数类型不同时\",{\"1\":{\"809\":1}}],[\"当执行x\",{\"1\":{\"809\":1}}],[\"当执行表达式a\",{\"1\":{\"809\":1}}],[\"当执行完插值得到原始图像尺寸大小时\",{\"1\":{\"582\":1}}],[\"当variable实例不再被其他对象引用时\",{\"1\":{\"806\":1}}],[\"当用户执行完一次前向传播和反向传播后\",{\"1\":{\"806\":1}}],[\"当用户不再引用variable时\",{\"1\":{\"806\":1}}],[\"当tinypytorch处理大量神经网络计算时\",{\"1\":{\"806\":1}}],[\"当对象的引用计数为0时\",{\"1\":{\"806\":1}}],[\"当某个变量被多次用作输入时\",{\"1\":{\"803\":1}}],[\"当我们希望评估某个测试样本\",{\"1\":{\"947\":1}}],[\"当我们希望从训练好的模型中生成新样本时\",{\"1\":{\"947\":1}}],[\"当我们尝试通过采样计算公式\",{\"1\":{\"945\":1}}],[\"当我们将其评估于实际观测结果\",{\"1\":{\"877\":1}}],[\"当我们讨论连续型随机变量\",{\"1\":{\"847\":1}}],[\"当我们谈论大模型时\",{\"1\":{\"837\":1}}],[\"当我们使用同一个变量分别进行多次计算时\",{\"1\":{\"802\":1}}],[\"当我们在其他任务中使用预训练好的模型时\",{\"1\":{\"425\":1}}],[\"当训练或测试时\",{\"1\":{\"707\":1}}],[\"当训练步数远超过bert的1m步时\",{\"1\":{\"681\":1}}],[\"当中包括\",{\"1\":{\"696\":1}}],[\"当然可以\",{\"1\":{\"871\":1}}],[\"当然\",{\"1\":{\"706\":1}}],[\"当然你也可以将所有词的\",{\"1\":{\"694\":1}}],[\"当然也可以是其他图像编码器\",{\"1\":{\"272\":1}}],[\"当任务违反道德或逻辑前提时\",{\"1\":{\"658\":1}}],[\"当模型计算注意力分数时\",{\"1\":{\"710\":1}}],[\"当模型在足够大且多样化的文本数据\",{\"1\":{\"643\":1}}],[\"当模型加深\",{\"1\":{\"125\":1}}],[\"当语言模型在足够多样化的文本数据上训练时\",{\"1\":{\"642\":1}}],[\"当两个变量\",{\"1\":{\"574\":3}}],[\"当精确率和召回率相差很大时\",{\"1\":{\"567\":1}}],[\"当精确率和召回率的值接近时\",{\"1\":{\"567\":1}}],[\"当精确率和召回率均为\",{\"1\":{\"567\":1}}],[\"当正例预测的准确性非常重要时\",{\"1\":{\"566\":1}}],[\"当假正例的代价高于假负例时使用\",{\"1\":{\"566\":1}}],[\"当假负例的代价高于假正例时使用\",{\"1\":{\"566\":1}}],[\"当进行转置时\",{\"1\":{\"545\":1}}],[\"当较大时\",{\"1\":{\"537\":1}}],[\"当你对一个张量进行了\",{\"1\":{\"494\":1}}],[\"当张量的\",{\"1\":{\"489\":1}}],[\"当提到模型参数量时\",{\"1\":{\"432\":1}}],[\"当使用\",{\"1\":{\"424\":1}}],[\"当使用软标签时\",{\"1\":{\"283\":1}}],[\"当cnn具有以上两种归纳偏置\",{\"1\":{\"422\":1}}],[\"当拥有足够多的数据进行预训练的时候\",{\"1\":{\"422\":1}}],[\"当is\",{\"1\":{\"420\":1}}],[\"当文本和query\",{\"1\":{\"419\":1}}],[\"当子实现类比较多的时候\",{\"1\":{\"382\":1}}],[\"当移除顶层\",{\"1\":{\"376\":1}}],[\"当数据集不均衡时\",{\"1\":{\"571\":1}}],[\"当数据集规模从\",{\"1\":{\"354\":1}}],[\"当数据量小于30m时\",{\"1\":{\"432\":1}}],[\"当数据规模增大时\",{\"1\":{\"282\":1}}],[\"当与\",{\"1\":{\"342\":1}}],[\"当filter模块在coco数据集上\",{\"1\":{\"191\":1}}],[\"当输入点云非常稀疏时\",{\"1\":{\"157\":1}}],[\"当局部区域的密度较高时\",{\"1\":{\"142\":1}}],[\"当局部区域的密度较低时\",{\"1\":{\"142\":1}}],[\"当stride≠1时\",{\"1\":{\"121\":1}}],[\"当\",{\"1\":{\"117\":3,\"188\":1,\"224\":1,\"257\":1,\"258\":1,\"343\":2,\"353\":1,\"382\":1,\"429\":1,\"476\":2,\"590\":3,\"611\":1,\"805\":1,\"848\":1,\"868\":1,\"893\":1,\"917\":1,\"951\":1}}],[\"当在\",{\"1\":{\"107\":1}}],[\"当前序列长度\",{\"1\":{\"898\":1}}],[\"当前tinypytorch已能将复杂式子转化为代码\",{\"1\":{\"815\":1}}],[\"当前tinypytorch框架中\",{\"1\":{\"806\":1}}],[\"当前token之前的text\",{\"1\":{\"420\":1}}],[\"当前已经有的词数\",{\"1\":{\"697\":1}}],[\"当前已经遮挡的patch数量\",{\"1\":{\"263\":1}}],[\"当前输入词的query\",{\"1\":{\"663\":1}}],[\"当前输入通道数初始化为in\",{\"1\":{\"145\":1}}],[\"当前新\",{\"1\":{\"663\":1}}],[\"当前轮输出token与输入tokens拼接\",{\"1\":{\"660\":1}}],[\"当前对齐对象\",{\"1\":{\"658\":1}}],[\"当前对齐并非通用意义上的\",{\"1\":{\"658\":1}}],[\"当前偏好群体有限\",{\"1\":{\"658\":1}}],[\"当前基于微调的方法存在三个主要问题\",{\"1\":{\"646\":1}}],[\"当前的机器学习系统虽然在特定任务上表现出色\",{\"1\":{\"639\":1}}],[\"当前机器学习系统的局限性\",{\"1\":{\"639\":1}}],[\"当前实现未使用\",{\"1\":{\"589\":1}}],[\"当前模态\",{\"1\":{\"385\":1}}],[\"当前代码未使用\",{\"1\":{\"384\":1}}],[\"当前研究重点包括\",{\"1\":{\"327\":1}}],[\"当前广泛使用的视觉模型参数量仍停留在约10亿级别\",{\"1\":{\"298\":1}}],[\"当前训练轮数\",{\"1\":{\"293\":1}}],[\"当前迭代\",{\"1\":{\"293\":1}}],[\"当前epoch对应的权重衰减表\",{\"1\":{\"265\":1}}],[\"当前epoch对应的学习率表\",{\"1\":{\"265\":1}}],[\"当前epoch已训练的步数\",{\"1\":{\"265\":1}}],[\"当前epoch数\",{\"1\":{\"265\":1}}],[\"当前主分支计算相似度\",{\"1\":{\"206\":1}}],[\"当前入队位置指针\",{\"1\":{\"205\":1}}],[\"当前论文提出了一个基于对比损失的\",{\"1\":{\"194\":1}}],[\"当前batch\",{\"1\":{\"192\":2}}],[\"当前batch的起始索引\",{\"1\":{\"119\":1}}],[\"当前视觉\",{\"1\":{\"165\":1,\"195\":1}}],[\"当前批次点数\",{\"1\":{\"122\":1}}],[\"当前批次的点数\",{\"1\":{\"121\":1}}],[\"当前批次的起始索引\",{\"1\":{\"121\":1}}],[\"当前层\",{\"1\":{\"122\":1}}],[\"当前层的点坐标\",{\"1\":{\"122\":1}}],[\"当前层特征\",{\"1\":{\"122\":3}}],[\"当前物体待预测的功能区域\",{\"1\":{\"107\":1}}],[\"当前物体类型\",{\"1\":{\"107\":1}}],[\"当前点云待预测的交互行为\",{\"1\":{\"53\":1}}],[\"当前是训练\",{\"1\":{\"53\":1}}],[\"当前交互\",{\"1\":{\"52\":1}}],[\"当前交互行为\",{\"1\":{\"52\":1}}],[\"当前\",{\"1\":{\"19\":1,\"106\":1,\"190\":1,\"293\":1,\"663\":2,\"893\":1,\"895\":1}}],[\"就这样\",{\"1\":{\"958\":1}}],[\"就应该让编码器输出3组logit\",{\"1\":{\"958\":1}}],[\"就应运而生了\",{\"1\":{\"355\":1}}],[\"就被转换成了一个等价的随机生成一个较小的\",{\"1\":{\"956\":1}}],[\"就数字生成而言\",{\"1\":{\"952\":1}}],[\"就消失了\",{\"1\":{\"951\":1}}],[\"就无法使用该技巧\",{\"1\":{\"946\":1}}],[\"就需要多次对\",{\"1\":{\"946\":1}}],[\"就必须让\",{\"1\":{\"944\":1}}],[\"就必须先调用\",{\"1\":{\"468\":1}}],[\"就不能反向传播到\",{\"1\":{\"931\":1}}],[\"就没有什么特别的地方了\",{\"1\":{\"923\":1}}],[\"就没什么意义\",{\"1\":{\"355\":1}}],[\"就用默认的\",{\"1\":{\"899\":1}}],[\"就返回\",{\"1\":{\"892\":1}}],[\"就于\",{\"1\":{\"827\":1}}],[\"就像\",{\"1\":{\"895\":1}}],[\"就像让计算机阅读\",{\"1\":{\"822\":1}}],[\"就像你在地图上两个村庄中间估算温度时\",{\"1\":{\"502\":1}}],[\"就点击这里直接下载\",{\"1\":{\"712\":1}}],[\"就相当于给其注入了顺序信息\",{\"1\":{\"706\":1}}],[\"就要重新计算所有之前\",{\"1\":{\"661\":1}}],[\"就足够了\",{\"1\":{\"613\":1}}],[\"就说明这些区域对当前学习任务特别重要\",{\"1\":{\"582\":1}}],[\"就得到了函数\",{\"1\":{\"877\":1}}],[\"就得到了这个格子的特征\",{\"1\":{\"502\":1}}],[\"就得到了\",{\"1\":{\"502\":1}}],[\"就看不到真实文档了\",{\"1\":{\"454\":1}}],[\"就表示模型大约有\",{\"1\":{\"432\":1}}],[\"就意味着模型大约有\",{\"1\":{\"432\":1}}],[\"就完成了从图片到token之间的转换\",{\"1\":{\"426\":1}}],[\"就有了很多先验信息\",{\"1\":{\"422\":1}}],[\"就从\",{\"1\":{\"385\":1}}],[\"就会推动模型向这一特定参数化方式靠拢\",{\"1\":{\"949\":1}}],[\"就会得到文中那种\",{\"1\":{\"885\":1}}],[\"就会抵达\",{\"1\":{\"816\":1}}],[\"就会进行数据复制以创建新的张量\",{\"1\":{\"470\":1}}],[\"就会导致只有最后一条路径上的梯度被保留\",{\"1\":{\"803\":1}}],[\"就会导致不利用图像信息\",{\"1\":{\"393\":1}}],[\"就会导致特征\",{\"1\":{\"356\":1}}],[\"就会立即在屏幕上显示一个字符\",{\"1\":{\"107\":1}}],[\"就能从一个\",{\"1\":{\"709\":1}}],[\"就能反映\",{\"1\":{\"578\":1}}],[\"就能在\",{\"1\":{\"280\":1}}],[\"就能同时获得对比损失和生成损失\",{\"1\":{\"272\":1}}],[\"就可以引入一个类似\",{\"1\":{\"951\":1}}],[\"就可以\",{\"1\":{\"545\":1}}],[\"就可以拿到所有样本的特征\",{\"1\":{\"357\":1}}],[\"就可以认为其无限被\",{\"1\":{\"353\":1}}],[\"就可以直接处理图像数据\",{\"1\":{\"231\":1}}],[\"就可以把模型轻松迁移到各种下游任务中\",{\"1\":{\"220\":1}}],[\"就是拿解码器输入\",{\"1\":{\"959\":1}}],[\"就是vq\",{\"1\":{\"956\":1}}],[\"就是为了能在图像生成时把编码器扔掉\",{\"1\":{\"956\":1}}],[\"就是在图像分布里随机采样一张图\",{\"1\":{\"925\":1}}],[\"就是告诉你\",{\"1\":{\"912\":1}}],[\"就是取使\",{\"1\":{\"903\":1}}],[\"就是最大似然估计的直观思想\",{\"1\":{\"903\":1}}],[\"就是从分布中采样一个\",{\"1\":{\"897\":1}}],[\"就是重构项\",{\"1\":{\"885\":1}}],[\"就是计算这些\",{\"1\":{\"846\":1}}],[\"就是没有答案\",{\"1\":{\"694\":1}}],[\"就是正常的词向量\",{\"1\":{\"692\":1}}],[\"就是正样本\",{\"1\":{\"355\":1}}],[\"就是随机遮盖或替换一句话里面的任意字或词\",{\"1\":{\"691\":1}}],[\"就是其中一个将此概念付诸实践的例子\",{\"1\":{\"690\":1}}],[\"就是大数据集有效\",{\"1\":{\"635\":1}}],[\"就是猜测被分配到高的概率值的token作为预测值\",{\"1\":{\"635\":1}}],[\"就是识别文本蕴含\",{\"1\":{\"634\":1}}],[\"就是需要更新的参数\",{\"1\":{\"611\":1}}],[\"就是fft存在的上述两个问题\",{\"1\":{\"602\":1}}],[\"就是用特定的数据\",{\"1\":{\"602\":1}}],[\"就是用了\",{\"1\":{\"357\":1}}],[\"就是根据问题找到相关内容并返回\",{\"1\":{\"533\":1}}],[\"就是一个包含\",{\"1\":{\"877\":1}}],[\"就是一个不错的方案\",{\"1\":{\"601\":1}}],[\"就是一个序列数据转换的问题\",{\"1\":{\"600\":1}}],[\"就是一个采样点\",{\"1\":{\"502\":1}}],[\"就是一种很好的解决方式\",{\"1\":{\"355\":1}}],[\"就是将结构化输入转换为有序序列以便作者预训练能处理\",{\"1\":{\"631\":1}}],[\"就是将y=wx中的w\",{\"1\":{\"605\":1}}],[\"就是将变成\",{\"1\":{\"604\":1}}],[\"就是将传统cnn和transformer进行结合\",{\"1\":{\"434\":1}}],[\"就是将图像送入预训练好的图像编码器\",{\"1\":{\"213\":1}}],[\"就是两个人的位置\",{\"1\":{\"915\":1}}],[\"就是两个线性层+gelu激活函数+dropout的结构\",{\"1\":{\"429\":1}}],[\"就是两个编码器都可以通过梯度回传进行更新\",{\"1\":{\"357\":1}}],[\"就是利用量化技术的一个变体\",{\"1\":{\"614\":1}}],[\"就是利用一个卷积核大小为16x16\",{\"1\":{\"426\":1}}],[\"就是利用这个动量的特性\",{\"1\":{\"351\":1}}],[\"就是把整个数据集的特征\",{\"1\":{\"357\":1}}],[\"就是把所有的负样本一视同仁\",{\"1\":{\"355\":1}}],[\"就是这个端到端的框架\",{\"1\":{\"357\":1}}],[\"就是这个图\",{\"1\":{\"353\":1}}],[\"就是\",{\"1\":{\"355\":1,\"356\":1,\"357\":2,\"502\":1,\"932\":1,\"959\":1}}],[\"就是我们要把给llm做的任务尽可能细化\",{\"1\":{\"618\":1}}],[\"就是我们的负样本数量\",{\"1\":{\"355\":1}}],[\"就是我们的\",{\"1\":{\"355\":1}}],[\"就是我们的模型在大量无标注的数据集上进行训练之后\",{\"1\":{\"353\":1}}],[\"就是我们不需要知道前两张图片是人这个类别\",{\"1\":{\"349\":1}}],[\"就是想得到一个模型\",{\"1\":{\"353\":1}}],[\"就是数据集中剩余的所有图片都是负样本\",{\"1\":{\"353\":1}}],[\"就是它可以从大量未标注的数据上学习到特征\",{\"1\":{\"352\":1}}],[\"就是不想让当前时刻的输出只是依赖于当前时刻的输入\",{\"1\":{\"351\":1}}],[\"就是代理任务是多样性的\",{\"1\":{\"350\":1}}],[\"就是第一张图片和第二张图片是同一个类别\",{\"1\":{\"349\":1}}],[\"就是经过\",{\"1\":{\"98\":1}}],[\"就拼接起来\",{\"1\":{\"145\":1}}],[\"就更新它\",{\"1\":{\"137\":1}}],[\"存一个标量\",{\"1\":{\"710\":1}}],[\"存在一个重要问题\",{\"1\":{\"946\":1}}],[\"存在一个或多个潜变量设置\",{\"1\":{\"943\":1}}],[\"存在一定的噪声\",{\"1\":{\"413\":1}}],[\"存在的问题\",{\"1\":{\"836\":1}}],[\"存在某些函数\",{\"1\":{\"500\":1}}],[\"存在以下不足\",{\"1\":{\"280\":1}}],[\"存在缓存机制\",{\"1\":{\"107\":1}}],[\"存储验证集图片对应索引信息\",{\"1\":{\"424\":1}}],[\"存储验证集的所有图片路径\",{\"1\":{\"424\":1}}],[\"存储训练集图片对应索引信息\",{\"1\":{\"424\":1}}],[\"存储训练集的所有图片路径\",{\"1\":{\"424\":1}}],[\"存储是为了快速矩阵乘\",{\"1\":{\"364\":1}}],[\"存储图像描述引导提示词\",{\"1\":{\"187\":1}}],[\"存储每个类别的样本总数\",{\"1\":{\"424\":1}}],[\"存储每个点到已选点集的最小距离\",{\"1\":{\"121\":1}}],[\"存储每层\",{\"1\":{\"385\":1}}],[\"存储每次选出的\",{\"1\":{\"137\":1}}],[\"存储处理后的每个批次特征\",{\"1\":{\"122\":1}}],[\"存储选中的索引\",{\"1\":{\"121\":1}}],[\"存储数组\",{\"1\":{\"106\":1}}],[\"存储了物体边界框文件路径\",{\"1\":{\"82\":1}}],[\"复合运算的验证\",{\"1\":{\"809\":1}}],[\"复合函数的计算图展示了函数的组合过程\",{\"1\":{\"767\":1}}],[\"复合函数的计算\",{\"0\":{\"765\":1}}],[\"复用这个模型的\",{\"1\":{\"898\":1}}],[\"复用同一行数据\",{\"1\":{\"546\":1}}],[\"复用缓存的视觉信息\",{\"1\":{\"420\":1}}],[\"复制到\",{\"1\":{\"959\":1}}],[\"复制\",{\"1\":{\"454\":1}}],[\"复杂函数可视化示例\",{\"1\":{\"815\":1}}],[\"复杂函数的求导\",{\"0\":{\"811\":1}}],[\"复杂计算图处理\",{\"1\":{\"811\":1}}],[\"复杂指令处理能力不足\",{\"1\":{\"658\":1}}],[\"复杂\",{\"1\":{\"374\":1,\"847\":1}}],[\"复杂推理型\",{\"1\":{\"342\":1}}],[\"复杂推理能力\",{\"1\":{\"340\":1}}],[\"复杂场景\",{\"1\":{\"19\":1}}],[\"复现\",{\"0\":{\"107\":1}}],[\"取负号\",{\"1\":{\"932\":1}}],[\"取对数得到\",{\"1\":{\"932\":1}}],[\"取对数得\",{\"1\":{\"904\":1}}],[\"取对应的索引\",{\"1\":{\"145\":1}}],[\"取决于\",{\"1\":{\"899\":1}}],[\"取决于是否能容纳在2048\",{\"1\":{\"647\":1}}],[\"取当前时间步\",{\"1\":{\"895\":1}}],[\"取反\",{\"1\":{\"710\":1}}],[\"取值范围\",{\"1\":{\"506\":1}}],[\"取\",{\"1\":{\"385\":1,\"419\":1,\"476\":2,\"960\":1}}],[\"取第1行到第3行\",{\"1\":{\"544\":1}}],[\"取第一个cls\",{\"1\":{\"417\":1}}],[\"取第\",{\"1\":{\"385\":1,\"463\":1}}],[\"取图像\",{\"1\":{\"385\":1}}],[\"取得了显著进展\",{\"1\":{\"687\":1}}],[\"取得了领先的零样本分类准确率\",{\"1\":{\"308\":1}}],[\"取得了较好平衡\",{\"1\":{\"277\":1}}],[\"取每个位置的平均作为最终的匹配得分\",{\"1\":{\"419\":1}}],[\"取每个\",{\"1\":{\"293\":1}}],[\"取平均或\",{\"1\":{\"900\":1}}],[\"取平均\",{\"1\":{\"274\":1,\"915\":2}}],[\"取最大概率的\",{\"1\":{\"899\":1}}],[\"取最后一层\",{\"1\":{\"243\":1}}],[\"取最小值\",{\"1\":{\"160\":1}}],[\"取最小的三个距离\",{\"1\":{\"145\":1}}],[\"取双向平均\",{\"1\":{\"206\":1}}],[\"取出出现次数最多的字符对\",{\"1\":{\"596\":2}}],[\"取出该区域内的最大值\",{\"1\":{\"501\":1}}],[\"取出该行中得分最大的那一列\",{\"1\":{\"411\":1}}],[\"取出经过\",{\"1\":{\"385\":1}}],[\"取出文本相关的张量\",{\"1\":{\"384\":1}}],[\"取出批大小\",{\"1\":{\"380\":1}}],[\"取出\",{\"1\":{\"206\":2,\"384\":1,\"385\":1}}],[\"取出图像和文本的\",{\"1\":{\"385\":1}}],[\"取出图像并做视觉编码\",{\"1\":{\"384\":1}}],[\"取出图像\",{\"1\":{\"187\":1}}],[\"取出当前批次的图像列表\",{\"1\":{\"410\":1}}],[\"取出当前最远点的坐标\",{\"1\":{\"137\":1}}],[\"取出当前样本对应的点云数据\",{\"1\":{\"92\":1}}],[\"取所有点的平均值\",{\"1\":{\"160\":1}}],[\"取所有点的最大值\",{\"1\":{\"160\":1}}],[\"取均值或求和\",{\"1\":{\"586\":1}}],[\"取均值\",{\"1\":{\"106\":1}}],[\"|n|\",{\"1\":{\"710\":1}}],[\"||i\",{\"1\":{\"150\":1}}],[\"|\",{\"1\":{\"106\":4,\"402\":4,\"502\":8,\"505\":4,\"701\":2,\"713\":2,\"892\":2,\"926\":1}}],[\"直到生成器能够生成看起来很真实的数据\",{\"1\":{\"918\":1}}],[\"直到观察到\",{\"1\":{\"860\":1}}],[\"直到抽出\",{\"1\":{\"859\":1}}],[\"直到达到一个较为稳定\",{\"1\":{\"836\":1}}],[\"直到达到预定迭代次数或收敛条件\",{\"1\":{\"213\":1}}],[\"直到无法合并为止\",{\"1\":{\"596\":3}}],[\"直线距离\",{\"1\":{\"576\":1}}],[\"直观地说\",{\"1\":{\"903\":1}}],[\"直观解释如下\",{\"0\":{\"873\":1}}],[\"直观上\",{\"1\":{\"574\":1}}],[\"直观理解js散度\",{\"0\":{\"915\":1}}],[\"直观理解\",{\"1\":{\"500\":1,\"505\":1,\"576\":1,\"577\":1}}],[\"直观的重排\",{\"1\":{\"478\":1}}],[\"直通梯度\",{\"1\":{\"257\":1}}],[\"直方图交集\",{\"1\":{\"106\":1}}],[\"直接从\",{\"1\":{\"947\":1}}],[\"直接从标准正态分布采样\",{\"1\":{\"947\":1}}],[\"直接从预测的概率分布里采样出第\",{\"1\":{\"921\":1}}],[\"直接假设\",{\"1\":{\"944\":1}}],[\"直接拿一个神经网络来评价生成的图像好不好\",{\"1\":{\"925\":1}}],[\"直接平均所有\",{\"1\":{\"900\":1}}],[\"直接更新检索知识库\",{\"1\":{\"830\":1}}],[\"直接进行运算\",{\"1\":{\"809\":1}}],[\"直接保留\",{\"1\":{\"709\":1}}],[\"直接计算概率\",{\"1\":{\"640\":1}}],[\"直接计算相似度\",{\"1\":{\"385\":1}}],[\"直接微调模型\",{\"1\":{\"631\":1}}],[\"直接影响着大模型给出答案的正确与否\",{\"1\":{\"616\":1}}],[\"直接影响重建质量\",{\"1\":{\"215\":1}}],[\"直接丢给它个指令\",{\"1\":{\"616\":1}}],[\"直接优化\",{\"1\":{\"586\":1}}],[\"直接把这个像素的值赋给输出像素\",{\"1\":{\"504\":1}}],[\"直接传入不连续张量极高概率导致运行时错误或计算结果错误\",{\"1\":{\"493\":1}}],[\"直接在大规模强化学习\",{\"1\":{\"823\":1}}],[\"直接在监督学习任务上训练\",{\"1\":{\"635\":1}}],[\"直接在原张量上进行操作\",{\"1\":{\"479\":1}}],[\"直接在两个目标函数下从零开始训练\",{\"1\":{\"269\":1}}],[\"直接建模图像与文本的交互\",{\"1\":{\"377\":1}}],[\"直接建模跨模态特征相似性\",{\"1\":{\"76\":1}}],[\"直接套到对比学习中去\",{\"1\":{\"355\":1}}],[\"直接照搬\",{\"1\":{\"354\":1}}],[\"直接拷贝学生参数\",{\"1\":{\"285\":1}}],[\"直接加载\",{\"1\":{\"254\":1}}],[\"直接预测教师网络\",{\"1\":{\"280\":1}}],[\"直接预测图像\",{\"1\":{\"250\":1}}],[\"直接预测掩码块的原始像素会导致模型过度关注短程依赖和高频细节\",{\"1\":{\"228\":1}}],[\"直接点积即可\",{\"1\":{\"213\":1}}],[\"直接返回解码图像\",{\"1\":{\"899\":1}}],[\"直接返回原张量本身\",{\"1\":{\"491\":1}}],[\"直接返回\",{\"1\":{\"208\":1,\"274\":1,\"893\":1,\"899\":1}}],[\"直接用编码后的\",{\"1\":{\"885\":1}}],[\"直接用prompt\",{\"1\":{\"601\":1}}],[\"直接用\",{\"1\":{\"204\":1}}],[\"直接以点集作为输入\",{\"1\":{\"148\":1}}],[\"直接复制其特征到所有原始点\",{\"1\":{\"145\":1}}],[\"直接处理和分析上传的数据文件\",{\"1\":{\"823\":1}}],[\"直接处理的原始点特征\",{\"1\":{\"142\":1}}],[\"直接处理嵌入连续空间的点云集合\",{\"1\":{\"110\":1}}],[\"直接提取的特征\",{\"1\":{\"142\":1}}],[\"直接对\",{\"1\":{\"925\":1}}],[\"直接对图像进行分类\",{\"1\":{\"408\":2}}],[\"直接对所有点进行特征提取\",{\"1\":{\"137\":1}}],[\"直接对整句进行编码\",{\"1\":{\"55\":1}}],[\"直接使用标准高斯采样值\",{\"1\":{\"947\":1}}],[\"直接使用的就是交叉熵损失函数代码\",{\"1\":{\"355\":1}}],[\"直接使用原始任务描述\",{\"1\":{\"346\":1}}],[\"直接使用internvit\",{\"1\":{\"304\":1}}],[\"直接使用轻量级解码器或投影层回归像素值\",{\"1\":{\"269\":1}}],[\"直接使用\",{\"1\":{\"123\":1,\"376\":1,\"663\":1}}],[\"直接输入冻结参数的\",{\"1\":{\"421\":1}}],[\"直接输入\",{\"1\":{\"106\":1}}],[\"直接评价分割精度\",{\"1\":{\"106\":1}}],[\"直接添加\",{\"1\":{\"92\":1}}],[\"直接关联结构与功能\",{\"1\":{\"75\":1}}],[\"直接拼接图像与点云特征作为输入\",{\"1\":{\"46\":1}}],[\"`j+1`\",{\"1\":{\"542\":1}}],[\"`j`\",{\"1\":{\"542\":1}}],[\"`2`\",{\"1\":{\"542\":2}}],[\"`1`\",{\"1\":{\"542\":2}}],[\"`i+1`\",{\"1\":{\"542\":2}}],[\"`i`\",{\"1\":{\"542\":2}}],[\"`3`\",{\"1\":{\"542\":2}}],[\"`\",{\"1\":{\"106\":1,\"542\":8}}],[\"角度\",{\"1\":{\"106\":1}}],[\"排列\",{\"0\":{\"881\":1}}],[\"排列不变性\",{\"1\":{\"157\":1}}],[\"排行榜提交\",{\"1\":{\"685\":1}}],[\"排除当前\",{\"1\":{\"386\":1}}],[\"排除\",{\"1\":{\"266\":1,\"582\":2,\"944\":1}}],[\"排除与几何矛盾的意图\",{\"1\":{\"56\":1}}],[\"排序为\",{\"1\":{\"413\":1}}],[\"排序\",{\"1\":{\"106\":1,\"424\":1}}],[\"∞\",{\"1\":{\"106\":1,\"588\":2,\"897\":1}}],[\"否\",{\"1\":{\"106\":8,\"346\":3,\"472\":2,\"588\":3}}],[\"否则就永远不能完全编码一个实数向量\",{\"1\":{\"951\":1}}],[\"否则会直接把\",{\"1\":{\"893\":1}}],[\"否则会报错或得到错误的结果\",{\"1\":{\"492\":1}}],[\"否则无法计算图像\",{\"1\":{\"893\":1}}],[\"否则取\",{\"1\":{\"857\":1}}],[\"否则答案可能不合理\",{\"1\":{\"735\":1}}],[\"否则保存这两个句子的\",{\"1\":{\"697\":1}}],[\"否则对齐仅为形式上的\",{\"1\":{\"658\":1}}],[\"否则应注释掉这行\",{\"1\":{\"586\":1}}],[\"否则可能报错\",{\"1\":{\"469\":1}}],[\"否则存入训练集\",{\"1\":{\"424\":1}}],[\"否则返回损失\",{\"1\":{\"899\":1}}],[\"否则返回视图\",{\"1\":{\"470\":1}}],[\"否则返回\",{\"1\":{\"256\":1}}],[\"否则用\",{\"1\":{\"255\":1}}],[\"否则更新为新的中心\",{\"1\":{\"213\":1}}],[\"否则\",{\"1\":{\"157\":1}}],[\"否则只有\",{\"1\":{\"138\":1}}],[\"否则使用恒等映射\",{\"1\":{\"429\":1}}],[\"否则使用\",{\"1\":{\"137\":1,\"426\":2,\"899\":1}}],[\"否则使用给定的\",{\"1\":{\"67\":1}}],[\"否则拼接额外特征\",{\"1\":{\"123\":1}}],[\"否则为恒等映射\",{\"1\":{\"431\":1}}],[\"否则为\",{\"1\":{\"119\":1}}],[\"否则模型性能显著下降\",{\"1\":{\"117\":1}}],[\"否则固定返回问题0\",{\"1\":{\"92\":1}}],[\"❌\",{\"1\":{\"106\":8,\"148\":1,\"161\":1,\"444\":4,\"472\":5,\"522\":2,\"588\":3,\"592\":6,\"735\":2}}],[\"✅\",{\"0\":{\"531\":1,\"532\":1,\"533\":1},\"1\":{\"106\":8,\"150\":5,\"152\":3,\"154\":2,\"156\":2,\"157\":12,\"160\":7,\"161\":1,\"263\":1,\"444\":4,\"449\":2,\"454\":3,\"472\":4,\"522\":2,\"588\":8,\"592\":14,\"733\":2,\"735\":4,\"737\":1,\"909\":1,\"963\":1,\"964\":1}}],[\"成立的样本点集合\",{\"1\":{\"846\":1}}],[\"成为研究和应用的热门工具\",{\"1\":{\"942\":1}}],[\"成为了当今计算机科学和人工智能领域的重要研究和应用方向\",{\"1\":{\"824\":1}}],[\"成为了现象级爆火应用\",{\"1\":{\"823\":1}}],[\"成为史上增长最快的\",{\"1\":{\"823\":1}}],[\"成为当时史上用户增长最快的消费级应用程序\",{\"1\":{\"823\":1}}],[\"成为首个在零样本设置下接近监督模型性能的大规模语言模型\",{\"1\":{\"640\":1}}],[\"成多头格式\",{\"1\":{\"724\":1}}],[\"成员存入类的属性字典\",{\"1\":{\"444\":1}}],[\"成功加载预训练的\",{\"1\":{\"963\":1,\"964\":1}}],[\"成功次数随机\",{\"1\":{\"860\":1}}],[\"成功\",{\"1\":{\"860\":2}}],[\"成功为蓝球\",{\"0\":{\"860\":1}}],[\"成功生成json文件\",{\"1\":{\"696\":2}}],[\"成功弥合了视觉模型与大型语言模型之间的能力与表示鸿沟\",{\"1\":{\"313\":1}}],[\"成功实现了视觉与语言模型在参数规模和特征表示上的协调\",{\"1\":{\"295\":1}}],[\"成本效益\",{\"1\":{\"823\":1}}],[\"成本较高\",{\"1\":{\"572\":1}}],[\"成本低\",{\"1\":{\"276\":1,\"658\":1}}],[\"成本上覆盖更多下游任务\",{\"1\":{\"268\":1}}],[\"成原来的特征图形状\",{\"1\":{\"213\":1}}],[\"成一个大的局部区域\",{\"1\":{\"137\":1}}],[\"成\",{\"1\":{\"106\":1,\"152\":1,\"737\":1}}],[\"影响内容的可信度\",{\"1\":{\"828\":1}}],[\"影响泛化能力\",{\"1\":{\"681\":1}}],[\"影响模型的运行效率\",{\"1\":{\"706\":1}}],[\"影响模型性能\",{\"1\":{\"610\":1}}],[\"影响模型容量和梯度稳定性\",{\"1\":{\"532\":1}}],[\"影响比较大的问题\",{\"1\":{\"602\":1}}],[\"影响显著\",{\"1\":{\"502\":1}}],[\"影响\",{\"1\":{\"106\":1,\"274\":1}}],[\"敏感度有限\",{\"1\":{\"106\":1}}],[\"绘制等高线图\",{\"1\":{\"816\":1}}],[\"绘制到图表中\",{\"1\":{\"569\":1}}],[\"绘制每种类别个数柱状图\",{\"1\":{\"424\":1}}],[\"绘制\",{\"1\":{\"106\":1,\"569\":1}}],[\"还不如直接理解成\",{\"1\":{\"959\":1}}],[\"还不如直接预测词袋模型\",{\"1\":{\"413\":1}}],[\"还认识其他来自标准正态分布的向量\",{\"1\":{\"956\":1}}],[\"还原所需的最小比特数\",{\"1\":{\"950\":1}}],[\"还原出\",{\"1\":{\"950\":1}}],[\"还原为重建图像\",{\"1\":{\"963\":1}}],[\"还原为\",{\"1\":{\"945\":1,\"964\":1}}],[\"还优化了\",{\"1\":{\"948\":1}}],[\"还依赖于编码器\",{\"1\":{\"946\":1}}],[\"还要包含书写角度\",{\"1\":{\"944\":1}}],[\"还屏蔽掉卷积中心\",{\"1\":{\"924\":1}}],[\"还需要额外将\",{\"1\":{\"899\":1}}],[\"还需要对每个像素进行分类\",{\"1\":{\"584\":1}}],[\"还希望它能\",{\"1\":{\"893\":1}}],[\"还希望和之前时刻的输出有关系\",{\"1\":{\"351\":1}}],[\"还具备网页浏览\",{\"1\":{\"823\":1}}],[\"还确保了复杂表达式的自动微分正确性\",{\"1\":{\"809\":1}}],[\"还取决于\",{\"1\":{\"708\":1}}],[\"还可能占用大量的内存空间\",{\"1\":{\"706\":1}}],[\"还可以传入初始图像条件\",{\"1\":{\"895\":1}}],[\"还可以推导出全概率公式\",{\"1\":{\"850\":1}}],[\"还可以定义累积分布函数\",{\"1\":{\"847\":1}}],[\"还可以直接应用于自然语言生成的图像描述任务\",{\"1\":{\"271\":1}}],[\"还可以作为\",{\"1\":{\"222\":1}}],[\"还将一个工具方法整合到了分词器的实现之中\",{\"1\":{\"697\":1}}],[\"还未进行初始化\",{\"1\":{\"663\":1}}],[\"还评估模型在\",{\"1\":{\"656\":1}}],[\"还借鉴了对语言模型潜在风险的研究\",{\"1\":{\"655\":1}}],[\"还有一个大特点\",{\"1\":{\"925\":1}}],[\"还有很多种\",{\"1\":{\"607\":1}}],[\"还有另一个方向\",{\"1\":{\"413\":1}}],[\"还考虑数据在各个维度上的方差大小和维度间的相关性\",{\"1\":{\"577\":1}}],[\"还考虑了从更低分辨率\",{\"1\":{\"142\":1}}],[\"还是只是记住了相似的训练样本\",{\"1\":{\"649\":1}}],[\"还是\",{\"1\":{\"545\":1}}],[\"还扩展了计数相关方法\",{\"1\":{\"516\":1}}],[\"还在336的分辨率下额外进行了一个周期的微调\",{\"1\":{\"407\":1}}],[\"还在多项视觉\",{\"1\":{\"388\":1}}],[\"还好\",{\"1\":{\"357\":1}}],[\"还让\",{\"1\":{\"272\":1}}],[\"还提出了分解式与\",{\"1\":{\"216\":1}}],[\"还小\",{\"1\":{\"137\":1}}],[\"还能够处理更复杂的自动微分任务\",{\"1\":{\"814\":1}}],[\"还能提高其泛化能力\",{\"1\":{\"658\":1}}],[\"还能提供更灵活的语义参考\",{\"1\":{\"202\":1}}],[\"还能计算任意条件概率\",{\"1\":{\"640\":1}}],[\"还能进行跨模态任务\",{\"1\":{\"268\":1}}],[\"还能学到不同模态之间的对齐\",{\"1\":{\"223\":1}}],[\"还能生成与\",{\"1\":{\"106\":1}}],[\"还能感知其他\",{\"1\":{\"100\":1}}],[\"还看响应强度分布\",{\"1\":{\"106\":1}}],[\"⚠️\",{\"1\":{\"106\":4,\"157\":1,\"160\":1,\"261\":1,\"454\":2,\"501\":1,\"586\":1,\"588\":1,\"735\":1,\"963\":1,\"964\":1}}],[\"衡量的是\",{\"1\":{\"932\":1}}],[\"衡量点与均值之间的距离\",{\"1\":{\"574\":1}}],[\"衡量\",{\"1\":{\"256\":1,\"260\":1}}],[\"衡量模型预测分布与动量模型生成的软标签之间的差异\",{\"1\":{\"202\":1}}],[\"衡量模型对二分类问题的判别能力\",{\"1\":{\"106\":1}}],[\"衡量空间重合度\",{\"1\":{\"106\":1}}],[\"衡量分类器整体判别能力\",{\"1\":{\"106\":1}}],[\"衡量分类器排序能力\",{\"1\":{\"106\":1}}],[\"衡量分布相似性\",{\"1\":{\"106\":2}}],[\"衡量逐点误差\",{\"1\":{\"106\":2}}],[\"衡量预测区域与真实标签之间的空间重合度\",{\"1\":{\"106\":1}}],[\"衡量预测掩码与\",{\"1\":{\"102\":1}}],[\"衡量整体分布一致性\",{\"1\":{\"106\":1}}],[\"✔️\",{\"1\":{\"106\":11,\"587\":4,\"588\":3,\"592\":4}}],[\"标题\",{\"1\":{\"885\":1}}],[\"标记进行下一句预测\",{\"1\":{\"699\":1}}],[\"标记\",{\"1\":{\"382\":1,\"420\":1}}],[\"标记哪些簇没有分配到样本\",{\"1\":{\"213\":1}}],[\"标记是否完成初始化\",{\"1\":{\"213\":1}}],[\"标记序列开始\",{\"1\":{\"171\":1}}],[\"标记为已选\",{\"1\":{\"121\":1}}],[\"标记为\",{\"1\":{\"106\":1}}],[\"标量注意力的\",{\"1\":{\"117\":1}}],[\"标量注意力\",{\"1\":{\"112\":1,\"117\":1}}],[\"标量注意力和向量注意力\",{\"1\":{\"112\":1}}],[\"标准的回归模型在这种情况下会失败\",{\"1\":{\"952\":1}}],[\"标准正态分布\",{\"1\":{\"947\":1}}],[\"标准正态空间\",{\"1\":{\"947\":1}}],[\"标准正态\",{\"1\":{\"931\":1}}],[\"标准与示例\",{\"1\":{\"658\":1}}],[\"标准提供训练信号\",{\"1\":{\"656\":1}}],[\"标准交叉熵损失\",{\"1\":{\"589\":1}}],[\"标准\",{\"1\":{\"208\":1,\"264\":1,\"293\":1,\"589\":2}}],[\"标准差单位距离\",{\"1\":{\"578\":1}}],[\"标准差\",{\"1\":{\"160\":1}}],[\"标准化\",{\"1\":{\"293\":1,\"577\":1,\"578\":1,\"582\":1}}],[\"标准化输出\",{\"1\":{\"208\":1}}],[\"标准化输入点云和特征空间\",{\"1\":{\"148\":1}}],[\"标准化的意义\",{\"1\":{\"152\":1}}],[\"标准错误\",{\"1\":{\"107\":1}}],[\"标准输出\",{\"1\":{\"107\":1}}],[\"标签为x\",{\"1\":{\"815\":1}}],[\"标签平滑\",{\"1\":{\"700\":1}}],[\"标签图\",{\"1\":{\"504\":1}}],[\"标签中非\",{\"1\":{\"384\":1}}],[\"标签等信息\",{\"1\":{\"384\":1}}],[\"标签生成等\",{\"1\":{\"265\":1}}],[\"标签过度惩罚合理预测\",{\"1\":{\"202\":1}}],[\"标签约束\",{\"1\":{\"202\":1}}],[\"标签的监督会一律惩罚这些\",{\"1\":{\"202\":1}}],[\"标签二值化\",{\"1\":{\"106\":1}}],[\"标签\",{\"1\":{\"106\":1,\"199\":1,\"207\":1,\"208\":2,\"264\":1,\"355\":1,\"384\":1,\"385\":2,\"386\":1,\"403\":1,\"937\":2}}],[\"标注者\",{\"1\":{\"658\":1}}],[\"标注者人数有限\",{\"1\":{\"658\":1}}],[\"标注者受其引导\",{\"1\":{\"658\":1}}],[\"标注者的偏好\",{\"1\":{\"658\":1}}],[\"标注者之间都保持一致\",{\"1\":{\"657\":1}}],[\"标注者实验\",{\"1\":{\"656\":1}}],[\"标注者在判断指令时需考虑信息准确性\",{\"1\":{\"656\":1}}],[\"标注者创作的\",{\"1\":{\"656\":1}}],[\"标注图像中的物体及其位置\",{\"1\":{\"341\":1}}],[\"标注数据昂贵又耗时\",{\"1\":{\"626\":1}}],[\"标注数据组织形式\",{\"1\":{\"92\":1}}],[\"标注数据有限\",{\"1\":{\"26\":1}}],[\"标注\",{\"0\":{\"88\":1}}],[\"标注策略\",{\"0\":{\"42\":1}}],[\"平方\",{\"1\":{\"871\":1}}],[\"平方距离矩阵\",{\"1\":{\"119\":1}}],[\"平等地使用\",{\"1\":{\"262\":1}}],[\"平行句子\",{\"1\":{\"220\":1}}],[\"平滑采样\",{\"1\":{\"897\":1}}],[\"平滑程度\",{\"1\":{\"897\":1}}],[\"平滑系数\",{\"1\":{\"590\":1,\"592\":1}}],[\"平滑项\",{\"1\":{\"586\":1,\"587\":2,\"588\":1,\"589\":1}}],[\"平滑\",{\"1\":{\"505\":1}}],[\"平滑控制长宽比例分布\",{\"1\":{\"263\":1}}],[\"平滑过的对比目标\",{\"1\":{\"206\":1}}],[\"平滑处理\",{\"1\":{\"160\":1}}],[\"平衡文本和图像两部分的训练目标\",{\"1\":{\"893\":1}}],[\"平衡正负样本数量\",{\"1\":{\"589\":1}}],[\"平衡类别数量\",{\"1\":{\"589\":1}}],[\"平衡效率与精度\",{\"1\":{\"336\":1}}],[\"平衡其指导作用\",{\"1\":{\"202\":1}}],[\"平衡因子\",{\"1\":{\"102\":1,\"589\":1}}],[\"平移到正索引区间\",{\"1\":{\"709\":1}}],[\"平移等变换具有鲁棒性\",{\"1\":{\"150\":1}}],[\"平移\",{\"1\":{\"149\":1,\"152\":1,\"161\":1}}],[\"平移中心到以关键点为原点的局部坐标系上\",{\"1\":{\"137\":1}}],[\"平面选择和三维遮挡可能影响识别性能\",{\"1\":{\"110\":1}}],[\"平均图像\",{\"1\":{\"952\":1}}],[\"平均近似\",{\"1\":{\"944\":1}}],[\"平均是指求期望\",{\"1\":{\"909\":1}}],[\"平均\",{\"1\":{\"909\":2}}],[\"平均每个样本要多花多少信息量\",{\"1\":{\"909\":1}}],[\"平均两个方向的\",{\"1\":{\"900\":1}}],[\"平均分布\",{\"1\":{\"899\":1}}],[\"平均分成\",{\"1\":{\"482\":1}}],[\"平均偏见得分66\",{\"1\":{\"670\":1}}],[\"平均重叠率3\",{\"1\":{\"641\":1}}],[\"平均估计\",{\"1\":{\"502\":2}}],[\"平均对称\",{\"1\":{\"502\":1}}],[\"平均可达\",{\"1\":{\"309\":1}}],[\"平均精度达到\",{\"1\":{\"308\":1}}],[\"平均池化\",{\"1\":{\"160\":1,\"238\":1}}],[\"平均值作为损失项\",{\"1\":{\"153\":1}}],[\"平均交并比\",{\"1\":{\"106\":1}}],[\"平均绝对误差\",{\"1\":{\"106\":1}}],[\"反复多次\",{\"1\":{\"660\":1}}],[\"反毒性\",{\"1\":{\"658\":1}}],[\"反而有所提升\",{\"1\":{\"681\":1}}],[\"反而更\",{\"1\":{\"657\":1}}],[\"反而降低性能\",{\"1\":{\"117\":1}}],[\"反应很快\",{\"1\":{\"619\":1}}],[\"反推出来的\",{\"1\":{\"589\":1}}],[\"反之\",{\"1\":{\"572\":1,\"892\":1}}],[\"反之亦然\",{\"1\":{\"139\":1}}],[\"反过来计算text\",{\"1\":{\"418\":1}}],[\"反转亮部\",{\"1\":{\"293\":1}}],[\"反卷积上采样\",{\"1\":{\"255\":1}}],[\"反卷积层\",{\"1\":{\"239\":1}}],[\"反向梯度\",{\"1\":{\"213\":1}}],[\"反向传播与参数更新\",{\"1\":{\"963\":1}}],[\"反向传播用到的\",{\"1\":{\"959\":1}}],[\"反向传播用于计算输入x对输出y大小变化的影响\",{\"1\":{\"775\":1}}],[\"反向传播其实不会用到式子的值\",{\"1\":{\"959\":1}}],[\"反向传播正确计算了每个变量的梯度\",{\"1\":{\"809\":1}}],[\"反向传播时取\",{\"1\":{\"959\":1}}],[\"反向传播时\",{\"1\":{\"959\":1}}],[\"反向传播时需按此计算梯度\",{\"1\":{\"809\":2}}],[\"反向传播时需要将上游传来的梯度gy分别乘以x1和x0\",{\"1\":{\"809\":1}}],[\"反向传播时将上游梯度取反\",{\"1\":{\"809\":1}}],[\"反向传播时可直接获取这些值\",{\"1\":{\"779\":1}}],[\"反向传播时仍保持连续可导\",{\"1\":{\"258\":1}}],[\"反向传播过程中它的梯度应累加\",{\"1\":{\"803\":1}}],[\"反向传播过程中\",{\"1\":{\"776\":1}}],[\"反向传播按从输出到输入的顺序计算导数\",{\"1\":{\"775\":1}}],[\"反向传播的自动化\",{\"0\":{\"782\":1}}],[\"反向传播的执行\",{\"0\":{\"781\":1}}],[\"反向传播的核心是依据链式法则\",{\"1\":{\"779\":1}}],[\"反向传播的方向\",{\"0\":{\"775\":1}}],[\"反向传播的理论基础是链式法则\",{\"1\":{\"774\":1}}],[\"反向传播的理论知识\",{\"0\":{\"773\":1}}],[\"反向传播更新模型参数过程\",{\"1\":{\"609\":1}}],[\"反向传播并更新参数\",{\"1\":{\"265\":1}}],[\"反向传播\",{\"1\":{\"105\":2,\"187\":1,\"206\":1,\"265\":1,\"293\":1,\"797\":1,\"809\":1,\"963\":1}}],[\"反射中都非常重要\",{\"1\":{\"454\":1}}],[\"反射\",{\"1\":{\"153\":1,\"157\":1,\"162\":1}}],[\"反距离加权插值\",{\"1\":{\"143\":1}}],[\"反距离加权\",{\"1\":{\"122\":1}}],[\"反映出互联网用户对于聊天和对话这种交互模式的偏好\",{\"1\":{\"827\":1}}],[\"反映社会偏见\",{\"1\":{\"668\":1}}],[\"反映的是模型对正确样本的能力\",{\"1\":{\"382\":1}}],[\"反映模型是否准确学习语言引导下的响应强度\",{\"1\":{\"106\":1}}],[\"反映点与功能核心区域的距离远近\",{\"1\":{\"88\":1}}],[\"更重要的是\",{\"1\":{\"945\":1}}],[\"更重视精确率\",{\"1\":{\"590\":1}}],[\"更重视召回率\",{\"1\":{\"590\":1}}],[\"更宽松\",{\"1\":{\"924\":1}}],[\"更早\",{\"1\":{\"924\":1}}],[\"更陡\",{\"1\":{\"917\":1}}],[\"更加随机\",{\"1\":{\"897\":1}}],[\"更加符合人类的一些期望\",{\"1\":{\"602\":1}}],[\"更形式化地说\",{\"1\":{\"849\":1,\"903\":1}}],[\"更长的上下文长度\",{\"1\":{\"823\":1}}],[\"更长训练步数\",{\"1\":{\"684\":1}}],[\"更接近真实框架的自动微分系统\",{\"1\":{\"799\":1}}],[\"更因为其背后复杂而精妙的自动微分系统\",{\"1\":{\"799\":1}}],[\"更大批次\",{\"1\":{\"686\":1}}],[\"更大批次训练\",{\"1\":{\"678\":1}}],[\"更大规模数据\",{\"1\":{\"684\":1}}],[\"更大的词表\",{\"1\":{\"823\":1}}],[\"更大的批次规模\",{\"1\":{\"677\":1}}],[\"更大的模型在几乎所有任务上都显著优于小模型\",{\"1\":{\"641\":1}}],[\"更大的模型在多个基准测试中达到了最先进水平\",{\"1\":{\"638\":1}}],[\"更严格的数据清洗\",{\"1\":{\"670\":1}}],[\"更符合用户指令\",{\"1\":{\"658\":1}}],[\"更符合图像结构\",{\"1\":{\"263\":1}}],[\"更不代表所有受语言模型影响的人群\",{\"1\":{\"658\":1}}],[\"更受欢迎\",{\"1\":{\"658\":1}}],[\"更受偏好\",{\"1\":{\"657\":1}}],[\"更常\",{\"1\":{\"657\":1}}],[\"更少毒性\",{\"1\":{\"658\":1}}],[\"更少生成有毒文本\",{\"1\":{\"657\":1}}],[\"更少生成有害内容\",{\"1\":{\"655\":1}}],[\"更少编造\",{\"1\":{\"657\":1}}],[\"更少\",{\"1\":{\"657\":1}}],[\"更具可控性\",{\"1\":{\"658\":1}}],[\"更具\",{\"1\":{\"655\":1}}],[\"更具体地说\",{\"1\":{\"570\":1}}],[\"更好的提升大模型在特定领域的能力\",{\"1\":{\"601\":1}}],[\"更好地遵循指令\",{\"1\":{\"657\":1}}],[\"更好地理解和执行用户给出的自然语言指令\",{\"1\":{\"339\":1}}],[\"更好地应对\",{\"1\":{\"102\":1}}],[\"更稳定的估计收敛\",{\"1\":{\"947\":1}}],[\"更稳定的训练过程\",{\"1\":{\"592\":1}}],[\"更稳定地收敛\",{\"1\":{\"587\":1}}],[\"更贴近最终评估指标\",{\"1\":{\"588\":1}}],[\"更贴近真实用户交互\",{\"1\":{\"346\":1}}],[\"更能知道如何将vq\",{\"1\":{\"955\":1}}],[\"更能遵守\",{\"1\":{\"657\":1}}],[\"更能反映真实的\",{\"1\":{\"579\":1}}],[\"更能增强模型将视觉信息转化为自然语言的能力\",{\"1\":{\"172\":1}}],[\"更广泛地说\",{\"1\":{\"567\":1}}],[\"更适应局部特征\",{\"1\":{\"500\":1}}],[\"更适合拟合离散分布\",{\"1\":{\"956\":1}}],[\"更适合长训练周期\",{\"1\":{\"681\":1}}],[\"更适合用于\",{\"1\":{\"657\":1}}],[\"更适合作为用户助手\",{\"1\":{\"657\":1}}],[\"更适合前景极少的小区域识别\",{\"1\":{\"587\":1}}],[\"更适合多模态理解类任务\",{\"1\":{\"272\":1}}],[\"更适合评估边界模糊区域\",{\"1\":{\"588\":1}}],[\"更适合评估\",{\"1\":{\"106\":1}}],[\"更灵活适应目标函数\",{\"1\":{\"500\":1}}],[\"更关注召回率\",{\"1\":{\"592\":1}}],[\"更关注精确率\",{\"1\":{\"592\":1}}],[\"更关注整体区域匹配\",{\"1\":{\"586\":1}}],[\"更关注\",{\"1\":{\"357\":1,\"586\":1}}],[\"更强的任务泛化能力\",{\"1\":{\"346\":1}}],[\"更强的视觉主干\",{\"1\":{\"177\":1}}],[\"更多内容可参考文献\",{\"1\":{\"950\":1}}],[\"更多分布使用到的时候再进行补充\",{\"1\":{\"868\":1}}],[\"更多的训练数据量\",{\"1\":{\"823\":1}}],[\"更多的数据\",{\"1\":{\"677\":1}}],[\"更多数据是关键\",{\"1\":{\"686\":1}}],[\"更多消融实验见附录\",{\"1\":{\"376\":1}}],[\"更多图块\",{\"1\":{\"336\":1}}],[\"更多视角数\",{\"1\":{\"291\":1}}],[\"更丰富的表征\",{\"1\":{\"293\":1}}],[\"更聚焦\",{\"1\":{\"293\":1}}],[\"更小的patch能带来准确率提升\",{\"1\":{\"288\":1}}],[\"更小的patch尺寸能显著提升vit\",{\"1\":{\"288\":1}}],[\"更准确地说是\",{\"1\":{\"950\":1}}],[\"更准确\",{\"1\":{\"276\":1}}],[\"更细粒度表示\",{\"1\":{\"272\":1}}],[\"更自然地支持图像描述任务\",{\"1\":{\"269\":1}}],[\"更真实地模拟自然场景中的遮挡\",{\"1\":{\"263\":1}}],[\"更容易处理复杂计算图\",{\"1\":{\"788\":1}}],[\"更容易被采到\",{\"1\":{\"518\":1}}],[\"更容易在训练中收敛\",{\"1\":{\"259\":1}}],[\"更容易训练\",{\"1\":{\"152\":1}}],[\"更进一步划分\",{\"1\":{\"823\":1}}],[\"更进一步\",{\"1\":{\"225\":1}}],[\"更注重保留有用的全局\",{\"1\":{\"215\":1}}],[\"更深的解码器能够获得更好的重建质量\",{\"1\":{\"215\":1}}],[\"更倾向于生成数据集中常见的\",{\"1\":{\"178\":1}}],[\"更复杂的对称函数建模\",{\"1\":{\"160\":1}}],[\"更有效\",{\"1\":{\"150\":1}}],[\"更高效\",{\"1\":{\"799\":1}}],[\"更高效的字节级\",{\"1\":{\"678\":1}}],[\"更高效的性能\",{\"1\":{\"50\":1}}],[\"更高毒性\",{\"1\":{\"657\":1}}],[\"更高分辨率\",{\"1\":{\"142\":1}}],[\"更易于管理的子集\",{\"1\":{\"131\":1}}],[\"更精准的空间对齐\",{\"1\":{\"125\":1}}],[\"更新嵌入向量\",{\"1\":{\"963\":1}}],[\"更新总长度\",{\"1\":{\"893\":1}}],[\"更新判断\",{\"1\":{\"877\":1}}],[\"更新成本高\",{\"1\":{\"830\":1}}],[\"更新至\",{\"1\":{\"823\":1}}],[\"更新项\",{\"1\":{\"656\":1}}],[\"更新计数\",{\"1\":{\"516\":1}}],[\"更新特征数量为表示层的维度\",{\"1\":{\"431\":1}}],[\"更新会非常缓慢\",{\"1\":{\"353\":1}}],[\"更新教师输出中心\",{\"1\":{\"293\":1}}],[\"更新教师网络参数\",{\"1\":{\"293\":1}}],[\"更新学生网络参数\",{\"1\":{\"293\":1}}],[\"更新参数\",{\"1\":{\"293\":1}}],[\"更新方式为\",{\"1\":{\"285\":1}}],[\"更新规则为\",{\"1\":{\"285\":1}}],[\"更新已遮挡patch数量\",{\"1\":{\"263\":1}}],[\"更新独立\",{\"1\":{\"213\":1}}],[\"更新算法为\",{\"1\":{\"213\":1}}],[\"更新超参\",{\"1\":{\"213\":1}}],[\"更新的衰减系数\",{\"1\":{\"213\":1}}],[\"更新后的中心需要做\",{\"1\":{\"213\":1}}],[\"更新簇中心\",{\"1\":{\"213\":2}}],[\"更新权重和\",{\"1\":{\"213\":1}}],[\"更新动量编码器参数\",{\"1\":{\"206\":1}}],[\"更新动量队列\",{\"1\":{\"190\":1}}],[\"更新负样本队列\",{\"1\":{\"192\":1}}],[\"更新\",{\"1\":{\"192\":1,\"205\":1,\"212\":1,\"213\":9,\"285\":1,\"893\":1,\"963\":3}}],[\"更新队列\",{\"1\":{\"190\":1,\"206\":1}}],[\"更新模型参数\",{\"1\":{\"187\":1}}],[\"更新下一个起始位置\",{\"1\":{\"293\":1}}],[\"更新下一个batch的起始索引\",{\"1\":{\"119\":1}}],[\"更新下一层的输入通道数\",{\"1\":{\"145\":1}}],[\"更新距离\",{\"1\":{\"121\":1}}],[\"更新点和批次信息\",{\"1\":{\"121\":1}}],[\"更新索引\",{\"1\":{\"106\":1}}],[\"抑制简单样本的梯度主导\",{\"1\":{\"589\":1}}],[\"抑制简单样本\",{\"1\":{\"589\":1}}],[\"抑制均匀输出\",{\"1\":{\"290\":1}}],[\"抑制\",{\"1\":{\"102\":1,\"589\":1}}],[\"正好可以直接来自模型输出\",{\"1\":{\"931\":1}}],[\"正好捕捉了\",{\"1\":{\"894\":1}}],[\"正好为resnet18最后生成的特征图的分辨率\",{\"1\":{\"83\":1}}],[\"正比于\",{\"1\":{\"877\":1}}],[\"正态程度\",{\"1\":{\"867\":1}}],[\"正态分布\",{\"0\":{\"865\":1},\"1\":{\"904\":1}}],[\"正面\",{\"1\":{\"907\":1}}],[\"正面出现次数\",{\"1\":{\"846\":1}}],[\"正面影响\",{\"1\":{\"658\":1}}],[\"正在改变着我们与技术互动的方式\",{\"1\":{\"827\":1}}],[\"正式开源\",{\"1\":{\"823\":1}}],[\"正式开源了\",{\"1\":{\"823\":1}}],[\"正式发布了其稳定版本\",{\"1\":{\"833\":1}}],[\"正式发布\",{\"1\":{\"823\":1}}],[\"正向传播\",{\"1\":{\"809\":1}}],[\"正向图文对的前向传播\",{\"1\":{\"207\":1}}],[\"正负方向可能分桶\",{\"1\":{\"710\":1}}],[\"正数\",{\"1\":{\"710\":1}}],[\"正余弦位置编码通过在不同维度上引入不同波长的正余弦信号\",{\"1\":{\"706\":1}}],[\"正弦和余弦\",{\"1\":{\"706\":1}}],[\"正弦余弦位置编码通常是高维向量\",{\"1\":{\"706\":1}}],[\"正弦余弦位置编码是一种基于三角函数的固定编码方式\",{\"1\":{\"706\":1}}],[\"正弦余弦位置编码\",{\"1\":{\"706\":1,\"707\":1}}],[\"正弦\",{\"0\":{\"706\":1}}],[\"正则项\",{\"1\":{\"885\":1}}],[\"正则\",{\"1\":{\"633\":1}}],[\"正则匹配含有\",{\"1\":{\"595\":1}}],[\"正则化参数\",{\"1\":{\"948\":1,\"951\":3}}],[\"正则化\",{\"1\":{\"213\":1,\"823\":1}}],[\"正则化损失\",{\"0\":{\"153\":1}}],[\"正则化的作用\",{\"1\":{\"117\":1}}],[\"正相关\",{\"1\":{\"574\":1}}],[\"正确的累加方式\",{\"1\":{\"803\":1,\"805\":1}}],[\"正确\",{\"1\":{\"616\":1,\"694\":1}}],[\"正确识别阳性病例至关重要\",{\"1\":{\"563\":1}}],[\"正确分类的比例\",{\"1\":{\"562\":1}}],[\"正确做法\",{\"1\":{\"492\":1,\"522\":1}}],[\"正常工作\",{\"1\":{\"946\":1}}],[\"正常\",{\"1\":{\"847\":1}}],[\"正常情况下\",{\"1\":{\"448\":1}}],[\"正常推理时为\",{\"1\":{\"385\":1}}],[\"正如\",{\"1\":{\"273\":1}}],[\"正是变分自编码器的核心思想\",{\"1\":{\"945\":1}}],[\"正是顺应这一趋势提出的\",{\"1\":{\"220\":1}}],[\"正是对这一缺陷的改进\",{\"1\":{\"157\":1}}],[\"正图负文\",{\"1\":{\"207\":2}}],[\"正图像\",{\"1\":{\"207\":1}}],[\"正文负图\",{\"1\":{\"207\":2}}],[\"正文本\",{\"1\":{\"207\":1}}],[\"正样本在对角线\",{\"1\":{\"900\":1}}],[\"正样本较少时增加权重\",{\"1\":{\"589\":1}}],[\"正样本batch=1\",{\"1\":{\"419\":1}}],[\"正样本batch\",{\"1\":{\"419\":3}}],[\"正样本推理\",{\"1\":{\"386\":1}}],[\"正样本为\",{\"1\":{\"386\":1}}],[\"正样本图文对往往关联性较弱\",{\"1\":{\"202\":1}}],[\"正样本概率为\",{\"1\":{\"199\":1}}],[\"正样本对\",{\"1\":{\"192\":1}}],[\"正样本\",{\"1\":{\"190\":2,\"207\":2}}],[\"正样本编码\",{\"1\":{\"190\":2}}],[\"正交变换包括\",{\"1\":{\"162\":1}}],[\"正交变换的本质是\",{\"1\":{\"162\":1}}],[\"正交变换\",{\"0\":{\"162\":1}}],[\"正类\",{\"1\":{\"102\":1}}],[\"负无穷\",{\"1\":{\"896\":1}}],[\"负二项分布变为几何分布\",{\"1\":{\"863\":1}}],[\"负二项分布的意义与优势\",{\"0\":{\"863\":1}}],[\"负二项分布的两个矩\",{\"1\":{\"862\":1}}],[\"负二项分布关注成功次数固定\",{\"1\":{\"860\":1}}],[\"负二项分布\",{\"0\":{\"859\":1},\"1\":{\"860\":1}}],[\"负数运算y\",{\"1\":{\"809\":1}}],[\"负数运算\",{\"1\":{\"809\":1}}],[\"负数\",{\"1\":{\"710\":1}}],[\"负号\",{\"1\":{\"589\":1}}],[\"负相关\",{\"1\":{\"574\":1}}],[\"负图像\",{\"1\":{\"207\":1}}],[\"负文本\",{\"1\":{\"207\":1}}],[\"负样本batch2=0\",{\"1\":{\"419\":1}}],[\"负样本batch2\",{\"1\":{\"419\":3}}],[\"负样本batch1=0\",{\"1\":{\"419\":1}}],[\"负样本batch1\",{\"1\":{\"419\":3}}],[\"负样本为\",{\"1\":{\"386\":1}}],[\"负样本数量\",{\"1\":{\"361\":1,\"385\":1}}],[\"负样本x2\",{\"1\":{\"353\":1}}],[\"负样本概率为\",{\"1\":{\"199\":1}}],[\"负样本\",{\"1\":{\"190\":2,\"202\":1,\"207\":1,\"589\":1}}],[\"负类\",{\"1\":{\"102\":1}}],[\"负责判断输入是真实数据还是生成器造出来的假数据\",{\"1\":{\"918\":1}}],[\"负责从随机噪声生成\",{\"1\":{\"918\":1}}],[\"负责组织多个\",{\"1\":{\"420\":1}}],[\"负责组织自注意力和交叉注意力的运算流程\",{\"1\":{\"420\":1}}],[\"负责组内信息混合\",{\"1\":{\"97\":2}}],[\"负责管理多个子数据集\",{\"1\":{\"382\":1}}],[\"负责完成出队入队的信息记录\",{\"1\":{\"361\":1}}],[\"负责通道间信息混合\",{\"1\":{\"97\":2}}],[\"掩码类型\",{\"1\":{\"924\":1}}],[\"掩码卷积\",{\"0\":{\"922\":1}}],[\"掩码机制\",{\"1\":{\"741\":1}}],[\"掩码符号\",{\"1\":{\"697\":1}}],[\"掩码候选位置\",{\"1\":{\"697\":1}}],[\"掩码语言模型\",{\"1\":{\"679\":1}}],[\"掩码语言建模\",{\"1\":{\"234\":1,\"368\":1,\"369\":1,\"373\":1,\"687\":1,\"699\":1}}],[\"掩码语言建模中的动量蒸馏\",{\"1\":{\"202\":1}}],[\"掩码等\",{\"1\":{\"474\":1}}],[\"掩码区域分类和特征回归\",{\"1\":{\"369\":1}}],[\"掩码区域分类\",{\"1\":{\"368\":1}}],[\"掩码策略也在\",{\"1\":{\"234\":1}}],[\"掩码策略\",{\"1\":{\"223\":1}}],[\"掩码策略如下\",{\"1\":{\"223\":1}}],[\"掩码数据建模\",{\"0\":{\"223\":1},\"1\":{\"221\":1}}],[\"掩码预测\",{\"1\":{\"220\":1}}],[\"掩码建模\",{\"1\":{\"220\":1}}],[\"掩码图像建模学习目标用于beit预训练\",{\"0\":{\"214\":1}}],[\"掩码图像建模\",{\"0\":{\"234\":1},\"1\":{\"210\":1,\"214\":1,\"216\":1,\"228\":1,\"234\":1,\"235\":1}}],[\"掩码高度匹配的功能区域\",{\"1\":{\"106\":1}}],[\"掩码之间的逐点偏差\",{\"1\":{\"106\":1}}],[\"掩码\",{\"1\":{\"102\":1,\"225\":1,\"305\":1,\"315\":1,\"384\":1,\"504\":1,\"586\":1,\"588\":2,\"589\":1,\"681\":1}}],[\"或普通梯度更新\",{\"1\":{\"963\":1}}],[\"或不确定\",{\"1\":{\"907\":1}}],[\"或不参与优化\",{\"1\":{\"235\":1}}],[\"或密度\",{\"1\":{\"904\":1}}],[\"或很小\",{\"1\":{\"899\":1}}],[\"或假设\",{\"1\":{\"877\":1}}],[\"或后验推理\",{\"1\":{\"877\":1}}],[\"或隐藏\",{\"1\":{\"877\":1}}],[\"或称\",{\"1\":{\"859\":1}}],[\"或太\",{\"1\":{\"847\":1}}],[\"或开源模型来实现核心的理解与生成\",{\"1\":{\"835\":1}}],[\"或记忆\",{\"1\":{\"823\":1}}],[\"或更多\",{\"1\":{\"822\":1}}],[\"或x\",{\"1\":{\"809\":1}}],[\"或返回notimplemented\",{\"1\":{\"809\":3}}],[\"或实现\",{\"1\":{\"808\":1}}],[\"或变量\",{\"1\":{\"805\":1}}],[\"或两个句子\",{\"1\":{\"735\":1}}],[\"或doc\",{\"1\":{\"683\":1}}],[\"或微调的前提下\",{\"1\":{\"648\":1}}],[\"或释义发现\",{\"1\":{\"634\":1}}],[\"或准确率\",{\"1\":{\"566\":1}}],[\"或模型的误报率\",{\"1\":{\"564\":1}}],[\"或源码\",{\"1\":{\"557\":1}}],[\"或数组\",{\"1\":{\"546\":1}}],[\"或矩阵\",{\"1\":{\"544\":1}}],[\"或传递给其他\",{\"1\":{\"492\":1}}],[\"或加权和\",{\"1\":{\"485\":1}}],[\"或可以通过在前面添加维度来自动广播\",{\"1\":{\"472\":1}}],[\"或新形状无法与原内存布局兼容时\",{\"1\":{\"470\":1}}],[\"或某些要求内存连续的操作\",{\"1\":{\"468\":1}}],[\"或类\",{\"1\":{\"450\":1}}],[\"或恒等映射\",{\"1\":{\"380\":1}}],[\"或其他传入的额外参数\",{\"1\":{\"380\":1}}],[\"或文本\",{\"1\":{\"372\":1}}],[\"或者说\",{\"1\":{\"961\":1}}],[\"或者说在于计算能力和数据集的规模\",{\"1\":{\"413\":1}}],[\"或者非要从\",{\"1\":{\"959\":1}}],[\"或者换个角度说\",{\"1\":{\"956\":1}}],[\"或者私有化模型\",{\"1\":{\"831\":1}}],[\"或者直接命令行运行\",{\"1\":{\"712\":1}}],[\"或者更广义地\",{\"1\":{\"592\":1}}],[\"或者更细一点\",{\"1\":{\"353\":1}}],[\"或者你不在意尺度差异\",{\"1\":{\"579\":1}}],[\"或者在达到某个最低准确性水平的情况下\",{\"1\":{\"566\":1}}],[\"或者一种错误\",{\"1\":{\"562\":1}}],[\"或者使用\",{\"1\":{\"545\":1}}],[\"或者使用layernorm等其他归一化\",{\"1\":{\"522\":1}}],[\"或者使用其他方法\",{\"1\":{\"213\":1}}],[\"或者要将其传入某些特定函数\",{\"1\":{\"494\":1}}],[\"或者内存中存在间隔\",{\"1\":{\"489\":1}}],[\"或者跨设备\",{\"1\":{\"470\":1}}],[\"或者这个任务需要更多的视觉信息\",{\"1\":{\"394\":1}}],[\"或者\",{\"1\":{\"355\":1,\"420\":1,\"553\":1,\"595\":1,\"857\":1}}],[\"或者其他的数据增广操作\",{\"1\":{\"350\":1}}],[\"或前缀\",{\"1\":{\"346\":1}}],[\"或多任务场景下的表现\",{\"1\":{\"339\":1}}],[\"或全局池化特征\",{\"1\":{\"303\":1}}],[\"或与bert系列模型对齐\",{\"1\":{\"298\":1}}],[\"或卷积网络\",{\"1\":{\"271\":1}}],[\"或平滑标签分布\",{\"1\":{\"271\":1}}],[\"或平均池化\",{\"1\":{\"100\":1}}],[\"或仅返回被\",{\"1\":{\"266\":1}}],[\"或视觉语言模型性能的技术\",{\"1\":{\"346\":1}}],[\"或视觉\",{\"1\":{\"220\":1}}],[\"或相似度最大的中心\",{\"1\":{\"213\":1}}],[\"或相似度最高的簇\",{\"1\":{\"213\":1}}],[\"或相似度\",{\"1\":{\"213\":1}}],[\"或图像\",{\"1\":{\"220\":1,\"223\":1}}],[\"或图像视图\",{\"1\":{\"148\":1}}],[\"或图结构增强局部建模能力\",{\"1\":{\"157\":1}}],[\"或转置卷积\",{\"1\":{\"145\":1}}],[\"或每组邻域大小\",{\"1\":{\"140\":1}}],[\"或局部区域\",{\"1\":{\"131\":1}}],[\"或\",{\"1\":{\"102\":1,\"106\":1,\"107\":1,\"117\":2,\"131\":1,\"143\":1,\"177\":1,\"208\":1,\"212\":1,\"213\":1,\"255\":1,\"257\":1,\"258\":1,\"262\":1,\"263\":1,\"264\":1,\"265\":2,\"268\":1,\"271\":1,\"282\":1,\"285\":1,\"286\":1,\"341\":1,\"362\":1,\"380\":1,\"382\":3,\"384\":1,\"385\":1,\"397\":1,\"463\":1,\"467\":1,\"474\":1,\"490\":1,\"502\":1,\"503\":1,\"504\":1,\"513\":2,\"518\":1,\"542\":1,\"544\":1,\"550\":1,\"557\":1,\"586\":1,\"587\":2,\"590\":1,\"592\":2,\"710\":2,\"824\":1,\"846\":2,\"848\":1,\"849\":1,\"868\":1,\"869\":1,\"899\":2,\"906\":1,\"926\":1,\"931\":1,\"946\":1,\"963\":1}}],[\"或经过\",{\"1\":{\"102\":1,\"588\":1,\"589\":1}}],[\"得益于前人的一些关于内在维度\",{\"1\":{\"610\":1}}],[\"得益于这种建模灵活性\",{\"1\":{\"370\":1}}],[\"得出带有一定置信度的一般性结论的行为\",{\"1\":{\"877\":1}}],[\"得出的用来将x序列\",{\"1\":{\"600\":1}}],[\"得出\",{\"1\":{\"160\":1}}],[\"得分也会接近它们的值\",{\"1\":{\"567\":1}}],[\"得分也会为\",{\"1\":{\"567\":1}}],[\"得分是精确率和召回率的调和平均数\",{\"1\":{\"567\":1}}],[\"得分从\",{\"1\":{\"311\":1}}],[\"得分达\",{\"1\":{\"310\":1}}],[\"得分达到\",{\"1\":{\"309\":1}}],[\"得分\",{\"0\":{\"567\":1},\"1\":{\"100\":1,\"268\":1}}],[\"得到真正的嵌入\",{\"1\":{\"958\":1}}],[\"得到3个整数\",{\"1\":{\"958\":1}}],[\"得到第二个像素\",{\"1\":{\"925\":1}}],[\"得到第一个像素\",{\"1\":{\"925\":1}}],[\"得到形状\",{\"1\":{\"899\":1}}],[\"得到形状为\",{\"1\":{\"426\":3}}],[\"得到概率\",{\"1\":{\"733\":1}}],[\"得到模型预测的这些掩码token对应的真实词\",{\"1\":{\"699\":1}}],[\"得到对应的json格式文件\",{\"1\":{\"696\":1}}],[\"得到上图中黄色的输出\",{\"1\":{\"694\":1}}],[\"得到llama\",{\"1\":{\"669\":1}}],[\"得到初始策略模型\",{\"1\":{\"656\":1}}],[\"得到一组f\",{\"1\":{\"590\":1}}],[\"得到一个新的形状为\",{\"1\":{\"545\":1}}],[\"得到一个融合了上下文信息的向量\",{\"1\":{\"531\":1}}],[\"得到一个logit\",{\"1\":{\"419\":1}}],[\"得到一个离散的\",{\"1\":{\"258\":1}}],[\"得到一个\",{\"1\":{\"257\":1,\"895\":1}}],[\"得到一个全局语义向量\",{\"1\":{\"70\":1}}],[\"得到归一化的注意力权重\",{\"1\":{\"527\":1}}],[\"得到归一化因子\",{\"1\":{\"145\":1}}],[\"得到结果如下图所示\",{\"1\":{\"433\":1}}],[\"得到相似度矩阵\",{\"1\":{\"418\":2}}],[\"得到相同维度的特征\",{\"1\":{\"407\":1}}],[\"得到注意力分数矩阵\",{\"1\":{\"380\":1,\"430\":1}}],[\"得到注意力权重矩阵\",{\"1\":{\"430\":1}}],[\"得到注意力权重\",{\"1\":{\"119\":1,\"380\":1}}],[\"得到两个样本\",{\"1\":{\"359\":1}}],[\"得到表征f12\",{\"1\":{\"353\":1}}],[\"得到图像和对应的标签\",{\"1\":{\"431\":1}}],[\"得到图像的表征\",{\"1\":{\"421\":1}}],[\"得到图像全局向量\",{\"1\":{\"274\":1}}],[\"得到图片表征f11\",{\"1\":{\"353\":1}}],[\"得到三个表征\",{\"1\":{\"349\":1}}],[\"得到每组\",{\"1\":{\"293\":1}}],[\"得到每个位置对应的\",{\"1\":{\"899\":1}}],[\"得到每个位置上的分类\",{\"1\":{\"893\":1}}],[\"得到每个注意力头的输出\",{\"1\":{\"430\":1}}],[\"得到每个预测类别的概率值\",{\"1\":{\"408\":1}}],[\"得到每个像素位置的\",{\"1\":{\"256\":1}}],[\"得到每个原始点的插值特征\",{\"1\":{\"145\":1}}],[\"得到每个点的最终特征\",{\"1\":{\"121\":1}}],[\"得到每个\",{\"1\":{\"100\":1,\"208\":1,\"710\":1}}],[\"得到以下核心结论\",{\"1\":{\"280\":1}}],[\"得到以查询点为原点的局部相对坐标系\",{\"1\":{\"119\":1}}],[\"得到文本全局向量\",{\"1\":{\"274\":1}}],[\"得到最终适用于下游任务的模型参数\",{\"1\":{\"609\":1}}],[\"得到最终表示\",{\"1\":{\"293\":1}}],[\"得到最终\",{\"1\":{\"274\":1}}],[\"得到最终的掩码\",{\"1\":{\"100\":1}}],[\"得到全局视频表示\",{\"1\":{\"273\":1}}],[\"得到单模态文本向量表示\",{\"1\":{\"272\":1}}],[\"得到跨模态的联合表示\",{\"1\":{\"268\":1}}],[\"得到连续的隐向量表示\",{\"1\":{\"257\":1}}],[\"得到量化后的特征表示\",{\"1\":{\"213\":1}}],[\"得到编码后的图像特征输出\",{\"1\":{\"213\":1}}],[\"得到均值\",{\"1\":{\"213\":1}}],[\"得到新的簇中心\",{\"1\":{\"213\":1}}],[\"得到新的中心\",{\"1\":{\"213\":1}}],[\"得到离散编码后\",{\"1\":{\"958\":1}}],[\"得到离散\",{\"1\":{\"212\":1}}],[\"得到预测结果\",{\"1\":{\"431\":1}}],[\"得到预测概率\",{\"1\":{\"201\":1}}],[\"得到预测的\",{\"1\":{\"106\":1,\"384\":1}}],[\"得到该区域的特征\",{\"1\":{\"143\":1}}],[\"得到该区域的固定长度特征表示\",{\"1\":{\"137\":1}}],[\"得到的导数为\",{\"1\":{\"816\":1}}],[\"得到的就是答案\",{\"1\":{\"735\":1}}],[\"得到的三个答案中\",{\"1\":{\"621\":1}}],[\"得到的预测概率如下所示\",{\"1\":{\"408\":1}}],[\"得到的特征可以很好地迁移到下游任务中\",{\"1\":{\"353\":1}}],[\"得到的特征可能无法覆盖整个物体\",{\"1\":{\"157\":1}}],[\"得到的特征拼接后再送入\",{\"1\":{\"293\":1}}],[\"得到的\",{\"1\":{\"190\":1,\"271\":1,\"877\":1,\"946\":1}}],[\"得到的质心\",{\"1\":{\"137\":1}}],[\"得到的几何属性描述与\",{\"1\":{\"37\":1}}],[\"得到目标点特征\",{\"1\":{\"122\":1}}],[\"得到目标物体区域掩码\",{\"1\":{\"83\":1}}],[\"得到目标物体区域特征\",{\"1\":{\"83\":2}}],[\"得到\",{\"1\":{\"119\":1,\"138\":3,\"156\":1,\"213\":2,\"255\":1,\"256\":1,\"272\":1,\"274\":1,\"421\":1,\"710\":1,\"894\":1}}],[\"得到分割掩码\",{\"1\":{\"100\":1}}],[\"得到融合特征\",{\"1\":{\"32\":1}}],[\"保障了模型的通用性与灵活性\",{\"1\":{\"312\":1}}],[\"保存模型\",{\"1\":{\"963\":1,\"964\":1}}],[\"保存生成图像\",{\"1\":{\"926\":1}}],[\"保存配置参数\",{\"1\":{\"892\":1}}],[\"保存指数\",{\"1\":{\"809\":1}}],[\"保存最优模型\",{\"1\":{\"700\":1}}],[\"保存字典到文件\",{\"1\":{\"697\":1}}],[\"保存每个注意力头的热力图\",{\"1\":{\"582\":1}}],[\"保存注意力头的数量\",{\"1\":{\"430\":1}}],[\"保存嵌入维度\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"保存分类任务的类别数\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"保存该层输出\",{\"1\":{\"385\":1}}],[\"保存数据集\",{\"1\":{\"382\":1}}],[\"保存是否使用绝对位置编码和相对位置编码的标志\",{\"1\":{\"380\":1}}],[\"保存\",{\"1\":{\"266\":1,\"380\":1,\"382\":1,\"892\":1}}],[\"保存超参数\",{\"1\":{\"255\":1}}],[\"保存原始输入用于残差连接\",{\"1\":{\"120\":1}}],[\"保证整个\",{\"1\":{\"871\":1}}],[\"保证运算按预期执行\",{\"1\":{\"809\":1}}],[\"保证数量一致\",{\"1\":{\"698\":1}}],[\"保证训练的开始此旁路矩阵依然是\",{\"1\":{\"611\":1}}],[\"保证训练集和验证集的数据处理方式一致\",{\"1\":{\"425\":1}}],[\"保证每次运行结果一致\",{\"1\":{\"521\":1}}],[\"保证每个\",{\"1\":{\"213\":1}}],[\"保证多\",{\"1\":{\"521\":1}}],[\"保证卷积等操作的可重复性\",{\"1\":{\"521\":1}}],[\"保证切分后各类比例与原始数据一致\",{\"1\":{\"513\":1}}],[\"保证长度\",{\"1\":{\"508\":1}}],[\"保证绝对值之和\",{\"1\":{\"508\":1}}],[\"保证顺序一致\",{\"1\":{\"424\":1}}],[\"保证随机结果可复现\",{\"1\":{\"424\":1}}],[\"保证\",{\"1\":{\"385\":1}}],[\"保证验证指标真实可靠\",{\"1\":{\"382\":1}}],[\"保证输入是\",{\"1\":{\"293\":1}}],[\"保证学术可复现性\",{\"1\":{\"220\":1}}],[\"保证变换是刚性的\",{\"1\":{\"152\":1}}],[\"保留中心像素\",{\"1\":{\"926\":1}}],[\"保留中间结果\",{\"1\":{\"807\":1}}],[\"保留卷积中心\",{\"1\":{\"924\":1}}],[\"保留反向传播所需的计算图连接\",{\"1\":{\"807\":1}}],[\"保留英文\",{\"1\":{\"667\":1}}],[\"保留接口以备后续扩展\",{\"1\":{\"588\":1}}],[\"保留接口以备扩展\",{\"1\":{\"587\":1,\"589\":1}}],[\"保留向量方向\",{\"1\":{\"508\":1}}],[\"保留向量的方向信息\",{\"1\":{\"507\":1}}],[\"保留浮点型的\",{\"1\":{\"502\":1}}],[\"保留\",{\"1\":{\"475\":1}}],[\"保留维度\",{\"1\":{\"472\":1}}],[\"保留剩余维度\",{\"1\":{\"463\":1}}],[\"保留原函数元信息\",{\"0\":{\"454\":1}}],[\"保留原始几何信息\",{\"1\":{\"159\":1}}],[\"保留和text\",{\"1\":{\"418\":2}}],[\"保留更多图像\",{\"1\":{\"272\":1}}],[\"保留的是原始像素数据\",{\"1\":{\"231\":1}}],[\"保留梯度\",{\"1\":{\"213\":2}}],[\"保留旧的中心\",{\"1\":{\"213\":1}}],[\"保留为原始\",{\"1\":{\"208\":1}}],[\"保留下来\",{\"1\":{\"100\":2}}],[\"保持自回归结构\",{\"1\":{\"926\":1}}],[\"保持总损失的数值\",{\"1\":{\"893\":1}}],[\"保持原意清晰\",{\"1\":{\"871\":1}}],[\"保持原样不动的形式进行处理\",{\"1\":{\"698\":1}}],[\"保持原样\",{\"1\":{\"208\":1}}],[\"保持内容的时效性\",{\"1\":{\"828\":1}}],[\"保持模型关注短距离精细信息\",{\"1\":{\"710\":1}}],[\"保持偏置和层归一化可训练\",{\"1\":{\"614\":1}}],[\"保持预训练模型的原始参数\",{\"1\":{\"609\":1}}],[\"保持高损失权重\",{\"1\":{\"589\":1}}],[\"保持连续性以获得最佳性能和正确性\",{\"1\":{\"493\":1}}],[\"保持冻结\",{\"1\":{\"342\":1}}],[\"保持视觉编码器和语言模型参数冻结\",{\"1\":{\"341\":1}}],[\"保持通俗易懂\",{\"1\":{\"271\":1}}],[\"保持对称性\",{\"1\":{\"263\":1}}],[\"保持可导\",{\"1\":{\"258\":1}}],[\"保持\",{\"1\":{\"188\":1,\"898\":1}}],[\"保持每个点的最小距离\",{\"1\":{\"121\":1}}],[\"保持批处理信息\",{\"1\":{\"121\":1}}],[\"保持几何结构\",{\"1\":{\"121\":1}}],[\"保持不变\",{\"1\":{\"100\":1,\"679\":1,\"681\":1,\"709\":1}}],[\"保持评估完整性\",{\"1\":{\"90\":1}}],[\"保持一致性是因为我们需要字典中的特征尽可能使用同一个或者相近的编码器进行表征\",{\"1\":{\"353\":1}}],[\"保持一致\",{\"1\":{\"44\":1,\"206\":1,\"380\":1}}],[\"保持其预训练能力不变\",{\"1\":{\"26\":1}}],[\"去定义\",{\"1\":{\"949\":1}}],[\"去编码真实来自\",{\"1\":{\"908\":1}}],[\"去预测\",{\"1\":{\"894\":1}}],[\"去预测下一个\",{\"1\":{\"894\":1}}],[\"去预测被掩码的\",{\"1\":{\"384\":1}}],[\"去推测其真正的意图\",{\"1\":{\"878\":1}}],[\"去推断世界的状态\",{\"1\":{\"878\":1}}],[\"去做任务适配\",{\"1\":{\"610\":1}}],[\"去重并移除了\",{\"1\":{\"640\":1}}],[\"去重\",{\"1\":{\"480\":1}}],[\"去缓慢地更新编码器\",{\"1\":{\"351\":1}}],[\"去匹配一个教师网络\",{\"1\":{\"285\":1}}],[\"去掉字符和空格\",{\"1\":{\"633\":1}}],[\"去掉目标检测器\",{\"1\":{\"369\":1}}],[\"去掉\",{\"1\":{\"188\":1,\"266\":1,\"293\":1}}],[\"去除含有低频词的句对\",{\"1\":{\"697\":1}}],[\"去除前后空格\",{\"1\":{\"696\":1}}],[\"去除后性能反而提升\",{\"1\":{\"686\":1}}],[\"去除nsp损失不仅未降低性能\",{\"1\":{\"681\":1}}],[\"去除nsp损失\",{\"1\":{\"681\":1}}],[\"去除多视角图像选择逻辑\",{\"1\":{\"384\":1}}],[\"去除这个温度超参数\",{\"1\":{\"355\":1}}],[\"去除频率小于\",{\"1\":{\"341\":1}}],[\"去除低质量描述\",{\"1\":{\"305\":1}}],[\"去除视觉\",{\"1\":{\"242\":1}}],[\"去除\",{\"1\":{\"242\":1}}],[\"去除噪声文本\",{\"1\":{\"177\":1}}],[\"去除batch维度\",{\"1\":{\"107\":1}}],[\"去填充这些空缺\",{\"1\":{\"137\":1}}],[\"去卷积点云特征\",{\"1\":{\"100\":1}}],[\"去关注点云中最相关的区域\",{\"1\":{\"96\":1}}],[\"区别于\",{\"1\":{\"710\":1}}],[\"区分大语言模型\",{\"1\":{\"825\":1}}],[\"区分句子\",{\"1\":{\"699\":1}}],[\"区分\",{\"1\":{\"385\":1}}],[\"区分模态\",{\"1\":{\"384\":1,\"385\":1}}],[\"区块\",{\"1\":{\"234\":1}}],[\"区间事件\",{\"1\":{\"847\":2}}],[\"区间数值的映射\",{\"1\":{\"845\":1}}],[\"区间内\",{\"1\":{\"592\":1}}],[\"区间内随机生成整数的张量\",{\"1\":{\"484\":1}}],[\"区间对应\",{\"1\":{\"293\":1}}],[\"区间\",{\"1\":{\"100\":1,\"586\":1,\"588\":1,\"847\":2}}],[\"区域匹配度\",{\"1\":{\"592\":1}}],[\"区域匹配误差\",{\"1\":{\"102\":1}}],[\"区域重叠误差\",{\"1\":{\"592\":1}}],[\"区域重合度\",{\"1\":{\"106\":1}}],[\"区域划分为\",{\"1\":{\"501\":1}}],[\"区域中提取固定大小的特征\",{\"1\":{\"501\":1}}],[\"区域特征\",{\"1\":{\"388\":1}}],[\"区域使用\",{\"1\":{\"266\":1}}],[\"区域对齐等\",{\"1\":{\"223\":1}}],[\"区域预测\",{\"1\":{\"83\":1}}],[\"区域定位\",{\"1\":{\"78\":1}}],[\"区域\",{\"1\":{\"30\":1,\"31\":1,\"49\":2,\"83\":2,\"263\":2,\"285\":1,\"368\":1,\"369\":1}}],[\"扫描角度不同等\",{\"1\":{\"152\":1}}],[\"扫描\",{\"1\":{\"100\":1}}],[\"猜测\",{\"1\":{\"903\":1}}],[\"猜出我的特征\",{\"1\":{\"145\":1}}],[\"猜\",{\"1\":{\"100\":1}}],[\"后几层则将这些属性解码为图像\",{\"1\":{\"944\":1}}],[\"后5维为离散视觉词空间索引\",{\"1\":{\"892\":1}}],[\"后2个位置为图像token\",{\"1\":{\"892\":1}}],[\"后验分布\",{\"1\":{\"949\":1}}],[\"后验\",{\"1\":{\"877\":1}}],[\"后对\",{\"1\":{\"877\":1}}],[\"后更名为\",{\"1\":{\"823\":1}}],[\"后训练和在线推理阶段也各自拥有了\",{\"1\":{\"822\":1}}],[\"后训练和在线推理\",{\"1\":{\"822\":1}}],[\"后一半桶表示右方向\",{\"1\":{\"710\":1}}],[\"后得到的注意力得分矩阵维度相同\",{\"1\":{\"703\":1}}],[\"后处理技术\",{\"1\":{\"670\":1}}],[\"后处理步骤\",{\"1\":{\"283\":1}}],[\"后来被应用于语言任务\",{\"1\":{\"655\":1}}],[\"后来研究通过示例性的\",{\"1\":{\"20\":1}}],[\"后面的用b类掩码\",{\"1\":{\"925\":1}}],[\"后面的mlp是个单独的结构\",{\"1\":{\"429\":1}}],[\"后面再赋值\",{\"1\":{\"474\":1}}],[\"后面所有维度都选中\",{\"1\":{\"463\":1}}],[\"后面参与了重建损失的计算\",{\"1\":{\"213\":1}}],[\"后将投影后的\",{\"1\":{\"421\":1}}],[\"后三种模型是按照efficientnet的缩放规则对resnet分别放大4倍\",{\"1\":{\"407\":1}}],[\"后应用一个\",{\"1\":{\"400\":1}}],[\"后半部分是图像\",{\"1\":{\"384\":1}}],[\"后半部分为多模态解码器\",{\"1\":{\"268\":1}}],[\"后的结果\",{\"1\":{\"866\":1}}],[\"后的形式\",{\"1\":{\"733\":1}}],[\"后的改进\",{\"1\":{\"663\":1}}],[\"后的概率值\",{\"1\":{\"586\":1}}],[\"后的\",{\"1\":{\"382\":1}}],[\"后可选\",{\"1\":{\"381\":1}}],[\"后缀\",{\"1\":{\"346\":1,\"384\":1}}],[\"后分割为448×448区块\",{\"1\":{\"331\":1}}],[\"后者是其110倍\",{\"1\":{\"696\":1}}],[\"后者是加模型\",{\"1\":{\"511\":1}}],[\"后者效果略优但计算复杂\",{\"1\":{\"683\":1}}],[\"后者在前\",{\"1\":{\"376\":1}}],[\"后者只从单个\",{\"1\":{\"376\":1}}],[\"后者作为强大的\",{\"1\":{\"296\":1}}],[\"后者为因果\",{\"1\":{\"171\":1}}],[\"后扩展到\",{\"1\":{\"274\":1}}],[\"后不可导\",{\"1\":{\"213\":1}}],[\"后期切换至\",{\"1\":{\"305\":1,\"315\":1}}],[\"后期\",{\"1\":{\"204\":1}}],[\"后续公式中将省略\",{\"1\":{\"946\":1}}],[\"后续为了简洁\",{\"1\":{\"943\":1}}],[\"后续几层\",{\"1\":{\"926\":1}}],[\"后续计算逻辑\",{\"1\":{\"809\":1}}],[\"后续需要利用该相关度完成当前词的全局上下文信息融合\",{\"1\":{\"703\":1}}],[\"后续实验均采用动态掩码\",{\"1\":{\"681\":1}}],[\"后续每次生成新\",{\"1\":{\"663\":1}}],[\"后续推理\",{\"1\":{\"663\":1}}],[\"后续再进行裁剪操作\",{\"1\":{\"425\":1}}],[\"后续更新时对右边这个编码器的参数进行动量更新\",{\"1\":{\"353\":1}}],[\"后续方法\",{\"1\":{\"269\":1,\"650\":1}}],[\"后续的损失函数之类的训练就很常规了\",{\"1\":{\"350\":1}}],[\"后续的\",{\"1\":{\"157\":1}}],[\"后续的特征提取更稳定\",{\"1\":{\"152\":1}}],[\"后续发展\",{\"1\":{\"157\":1}}],[\"后续改进方向\",{\"1\":{\"157\":1}}],[\"后续改进\",{\"1\":{\"157\":1}}],[\"后续\",{\"1\":{\"123\":2,\"192\":1,\"293\":1}}],[\"后续批次\",{\"1\":{\"121\":1,\"122\":1}}],[\"后续训练完50个epoch后\",{\"1\":{\"107\":1}}],[\"后与\",{\"1\":{\"119\":1}}],[\"后台运行\",{\"1\":{\"107\":1}}],[\"后\",{\"1\":{\"99\":1,\"119\":1,\"152\":1,\"207\":1,\"382\":1,\"468\":1,\"574\":1,\"709\":1,\"895\":1}}],[\"↓\",{\"1\":{\"99\":4,\"121\":4,\"143\":5,\"144\":8,\"538\":9}}],[\"──┐\",{\"1\":{\"99\":1}}],[\"两阶段如何对应这个公式\",{\"1\":{\"885\":1}}],[\"两阶段训练\",{\"1\":{\"334\":1}}],[\"两条互补产品线\",{\"1\":{\"823\":1}}],[\"两大技术分支\",{\"1\":{\"823\":1}}],[\"两两配对遍历\",{\"1\":{\"697\":1}}],[\"两点注意\",{\"1\":{\"424\":1}}],[\"两点的距离反映了这两点的实际相似度或关联度\",{\"1\":{\"135\":1}}],[\"两张全局视角图像\",{\"1\":{\"293\":1}}],[\"两组负样本编码\",{\"1\":{\"190\":1}}],[\"两者之间会出现一种\",{\"1\":{\"873\":1}}],[\"两者中相同字母代表要\",{\"1\":{\"709\":1}}],[\"两者毒性差异减小\",{\"1\":{\"657\":1}}],[\"两者协同提升模型性能\",{\"1\":{\"589\":1}}],[\"两者解决的是不同维度的问题\",{\"1\":{\"589\":1}}],[\"两者解耦可以让模型更灵活地分配资源\",{\"1\":{\"536\":1}}],[\"两者互补\",{\"1\":{\"587\":1}}],[\"两者都通过增加基函数的数量\",{\"1\":{\"500\":1}}],[\"两者都会输出一个\",{\"1\":{\"285\":1}}],[\"两者的训练效率相差3倍\",{\"1\":{\"413\":1}}],[\"两者平均再乘权重\",{\"1\":{\"385\":1}}],[\"两者结合\",{\"1\":{\"215\":1}}],[\"两者结合可以\",{\"1\":{\"102\":1}}],[\"两者联合使用时效果最佳\",{\"1\":{\"177\":1}}],[\"两种规模的\",{\"1\":{\"823\":1}}],[\"两种模态是否相互作用\",{\"1\":{\"390\":1}}],[\"两种模态是否保持平衡\",{\"1\":{\"390\":1}}],[\"两种模式\",{\"1\":{\"89\":1,\"122\":1,\"894\":1}}],[\"两种配置\",{\"1\":{\"305\":1}}],[\"两种崩溃被平衡并避免\",{\"1\":{\"290\":1}}],[\"两种变分自编码器\",{\"1\":{\"254\":1}}],[\"两种工作模式\",{\"1\":{\"122\":1}}],[\"两个损失项的单位是一致的\",{\"1\":{\"951\":1}}],[\"两个损失共享计算资源\",{\"1\":{\"276\":1}}],[\"两个完全不同的分布\",{\"1\":{\"916\":1}}],[\"两个分布相差多远\",{\"1\":{\"912\":1}}],[\"两个月后月活用户破亿\",{\"1\":{\"823\":1}}],[\"两个句子是否为上下句关系\",{\"1\":{\"730\":1}}],[\"两个任务\",{\"1\":{\"699\":1}}],[\"两个开源数据集\",{\"1\":{\"696\":1}}],[\"两个不同分类任务的评估结果\",{\"1\":{\"634\":1}}],[\"两个比较的句子没有内在顺序\",{\"1\":{\"631\":1}}],[\"两个假设模型的\",{\"1\":{\"572\":1}}],[\"两个向量\",{\"1\":{\"506\":1}}],[\"两个贡献\",{\"1\":{\"356\":1}}],[\"两个编码器\",{\"1\":{\"353\":1}}],[\"两个编码器通过对比目标联合优化\",{\"1\":{\"271\":1}}],[\"两个参数都是\",{\"1\":{\"261\":1}}],[\"两个点之间的直线距离被认为是相似度或连接强度的直观表示\",{\"1\":{\"135\":1}}],[\"两个问题是相关联的\",{\"1\":{\"131\":1}}],[\"两个\",{\"1\":{\"97\":1,\"708\":1}}],[\"操作将视觉token数量减少至1\",{\"1\":{\"329\":1}}],[\"操作\",{\"1\":{\"97\":1,\"100\":2,\"143\":1,\"208\":1,\"492\":1,\"500\":1,\"502\":1,\"546\":1,\"963\":1}}],[\"知道了哪些点跟自身的相关度更大\",{\"1\":{\"96\":1}}],[\"知识更新\",{\"1\":{\"830\":1}}],[\"知识更新滞后性\",{\"1\":{\"828\":1}}],[\"知识截止日期\",{\"1\":{\"823\":2}}],[\"知识型模型\",{\"1\":{\"823\":1}}],[\"知识型与推理型双模式\",{\"1\":{\"823\":1}}],[\"知识型\",{\"1\":{\"823\":3}}],[\"知识扫盲\",{\"0\":{\"599\":1,\"615\":1},\"1\":{\"599\":1,\"615\":1}}],[\"知识蒸馏不再是自监督预训练之后的\",{\"1\":{\"283\":1}}],[\"知识蒸馏让小模型\",{\"1\":{\"168\":1}}],[\"知识蒸馏\",{\"0\":{\"168\":1},\"1\":{\"216\":1,\"283\":1,\"285\":1}}],[\"知识迁移\",{\"1\":{\"23\":2}}],[\"知识星球\",{\"1\":{\"0\":1}}],[\"某个未知\",{\"1\":{\"877\":1}}],[\"某一维如果是\",{\"1\":{\"472\":1}}],[\"某一层次𝐿𝑖的区域特征是通过将来自下一级𝐿𝑖−1的子区域特征总结后的向量与直接处理该局部区域所有原始点的单个pointnet得到的特征向量进行concat得到的\",{\"1\":{\"142\":1}}],[\"某一层解码器输出的点特征\",{\"1\":{\"96\":1}}],[\"某些性能关键的操作\",{\"1\":{\"492\":1}}],[\"某些操作\",{\"1\":{\"469\":1}}],[\"某些功能类型在特定物体类别下会从训练集中省略\",{\"1\":{\"89\":1}}],[\"骨干网络结构\",{\"1\":{\"116\":1}}],[\"骨干网络\",{\"0\":{\"222\":1},\"1\":{\"94\":1,\"224\":1}}],[\"也正是\",{\"1\":{\"958\":1}}],[\"也提供了把\",{\"1\":{\"956\":1}}],[\"也提示了未来改进方向\",{\"1\":{\"646\":1}}],[\"也难以训练\",{\"1\":{\"944\":1}}],[\"也类似于\",{\"1\":{\"889\":1}}],[\"也一起看作潜变量的一部分\",{\"1\":{\"885\":1}}],[\"也一并加入\",{\"1\":{\"137\":1,\"141\":1}}],[\"也一并提取\",{\"1\":{\"137\":1}}],[\"也得允许\",{\"1\":{\"847\":1}}],[\"也属于\",{\"1\":{\"847\":1}}],[\"也能把\",{\"1\":{\"956\":1}}],[\"也能感知全局位置信息\",{\"1\":{\"706\":1}}],[\"也能实现跨模态深度融合\",{\"1\":{\"220\":1}}],[\"也在上表4中\",{\"1\":{\"634\":1}}],[\"也分析了在四种不同设置下预训练模型的零次\",{\"1\":{\"626\":1}}],[\"也分离出来\",{\"1\":{\"258\":1}}],[\"也用\",{\"1\":{\"612\":2}}],[\"也几乎未引入额外的\",{\"1\":{\"611\":1}}],[\"也还有一些其它参数\",{\"1\":{\"600\":1}}],[\"也通常是句子长度\",{\"1\":{\"524\":1}}],[\"也有偏置beta\",{\"1\":{\"522\":1}}],[\"也都是适用的\",{\"1\":{\"500\":1}}],[\"也会被加载\",{\"1\":{\"474\":1}}],[\"也需要保持输入图像尺寸与预训练时一致\",{\"1\":{\"425\":1}}],[\"也就意味着我们只需要实现空间掩码即可\",{\"1\":{\"926\":1}}],[\"也就为非线性激活函数提供了更多可以学习的特征组合\",{\"1\":{\"429\":1}}],[\"也就几万个类别\",{\"1\":{\"355\":1}}],[\"也就是向量的每一维都是浮点数\",{\"1\":{\"956\":1}}],[\"也就是公式5的右边\",{\"1\":{\"951\":1}}],[\"也就是把文本损失和图像损失当作\",{\"1\":{\"893\":1}}],[\"也就是在某个距离范围内\",{\"1\":{\"873\":1}}],[\"也就是在句子开头加一个\",{\"1\":{\"692\":1}}],[\"也就是所谓的\",{\"1\":{\"825\":1,\"873\":1}}],[\"也就是我们常说的\",{\"1\":{\"825\":1}}],[\"也就是我的\",{\"1\":{\"355\":1}}],[\"也就是改变x对于y的影响因子\",{\"1\":{\"775\":1}}],[\"也就是两个句子\",{\"1\":{\"692\":1}}],[\"也就是有哪些子词以及这些子词的出现次数\",{\"1\":{\"595\":1}}],[\"也就是每个变量自己的方差\",{\"1\":{\"574\":1}}],[\"也就是和输入的\",{\"1\":{\"528\":1}}],[\"也就是形成闭包\",{\"1\":{\"448\":1}}],[\"也就是q\",{\"1\":{\"433\":1}}],[\"也就是经过卷积后拼接得到的特征图\",{\"1\":{\"426\":1}}],[\"也就是卷积核的数量\",{\"1\":{\"426\":1}}],[\"也就是一张图像搭配与之对应的文本描述\",{\"1\":{\"406\":1}}],[\"也就是未pooler的\",{\"1\":{\"384\":1}}],[\"也就是字典的大小就是分母下方的类别数量\",{\"1\":{\"355\":1}}],[\"也就是现在只有两个类别\",{\"1\":{\"355\":1}}],[\"也就是会得到这样一个向量\",{\"1\":{\"355\":1}}],[\"也就是类似\",{\"1\":{\"354\":1}}],[\"也就是原始的信号空间是由单词组成\",{\"1\":{\"353\":1}}],[\"也就是同一个类别的图片\",{\"1\":{\"350\":1}}],[\"也就是imagenet上的预训练模型\",{\"1\":{\"348\":1}}],[\"也就是\",{\"1\":{\"235\":1,\"260\":1,\"261\":1,\"355\":1,\"609\":1,\"960\":1}}],[\"也就是让\",{\"1\":{\"213\":1}}],[\"也就是说这个张量可以看作是一个\",{\"1\":{\"542\":1}}],[\"也就是说当我们用一个很小的\",{\"1\":{\"353\":1}}],[\"也就是说\",{\"1\":{\"100\":1,\"293\":1,\"340\":1,\"444\":1,\"563\":1,\"564\":1,\"737\":1,\"831\":1,\"847\":1,\"860\":1,\"868\":1,\"904\":1,\"924\":2,\"925\":1,\"947\":1,\"948\":1,\"956\":1,\"959\":2}}],[\"也就是取出上面随机选择的问题文本\",{\"1\":{\"92\":1}}],[\"也没有浓缩得那么简洁\",{\"1\":{\"353\":1}}],[\"也是该分布的众数\",{\"1\":{\"865\":1}}],[\"也是在\",{\"1\":{\"709\":1}}],[\"也是其一大亮点\",{\"1\":{\"408\":1}}],[\"也是如此\",{\"1\":{\"306\":1}}],[\"也是输出维度\",{\"1\":{\"120\":1}}],[\"也与\",{\"1\":{\"283\":1}}],[\"也让跨模态对齐\",{\"1\":{\"276\":1}}],[\"也包括带噪声的网页\",{\"1\":{\"272\":1}}],[\"也包括合成描述\",{\"1\":{\"173\":1}}],[\"也称大型语言模型\",{\"1\":{\"822\":1}}],[\"也称为各向同性协方差矩阵\",{\"1\":{\"871\":1}}],[\"也称为贝叶斯定理\",{\"1\":{\"851\":1}}],[\"也称为\",{\"1\":{\"588\":1}}],[\"也称为intersection\",{\"1\":{\"588\":1}}],[\"也称为误报概率\",{\"1\":{\"564\":1}}],[\"也称为召回率\",{\"1\":{\"563\":1}}],[\"也称\",{\"1\":{\"271\":1}}],[\"也被\",{\"1\":{\"944\":1}}],[\"也被直接监督\",{\"1\":{\"385\":1}}],[\"也被称为\",{\"1\":{\"259\":1}}],[\"也被近年来的工作所采纳\",{\"1\":{\"171\":1}}],[\"也叫正态分布\",{\"1\":{\"865\":1}}],[\"也叫作边缘似然\",{\"1\":{\"852\":1}}],[\"也叫\",{\"1\":{\"257\":1}}],[\"也为未来探索更强大的视觉理解与生成模型提供了方向\",{\"1\":{\"252\":1}}],[\"也借鉴了\",{\"1\":{\"250\":1}}],[\"也不及\",{\"1\":{\"657\":1}}],[\"也不局限于某一个方案\",{\"1\":{\"602\":1}}],[\"也不对\",{\"1\":{\"502\":1}}],[\"也不会进行变换\",{\"1\":{\"947\":1}}],[\"也不会参与梯度计算或优化\",{\"1\":{\"474\":1}}],[\"也不会计算梯度\",{\"1\":{\"473\":1}}],[\"也不例外\",{\"1\":{\"382\":1}}],[\"也不训练\",{\"1\":{\"235\":1}}],[\"也不需要高分辨率图像\",{\"1\":{\"194\":1}}],[\"也不需要事先体素化\",{\"1\":{\"109\":1}}],[\"也应该以一种方式被处理\",{\"1\":{\"131\":1}}],[\"也启发了\",{\"1\":{\"110\":1}}],[\"也可用参数化函数表示\",{\"1\":{\"846\":1}}],[\"也可用于预测图像\",{\"1\":{\"898\":1}}],[\"也可用于\",{\"1\":{\"246\":1}}],[\"也可联合编码图文对用于分类任务\",{\"1\":{\"369\":1}}],[\"也可作为融合编码器处理图文对分类任务\",{\"1\":{\"368\":1}}],[\"也可能产生多个输出\",{\"1\":{\"800\":1}}],[\"也可能触发复制\",{\"1\":{\"470\":1}}],[\"也可能存在多个与被\",{\"1\":{\"202\":1}}],[\"也可能导致错误分类\",{\"1\":{\"157\":1}}],[\"也可以写成\",{\"1\":{\"909\":1}}],[\"也可以理解为是一种\",{\"1\":{\"895\":1}}],[\"也可以简称为概率推理\",{\"1\":{\"877\":1}}],[\"也可以将\",{\"1\":{\"857\":1}}],[\"也可以指\",{\"1\":{\"847\":1}}],[\"也可以基于大模型的推理\",{\"1\":{\"826\":1}}],[\"也可以看到\",{\"1\":{\"706\":1}}],[\"也可以用以下方式表示\",{\"1\":{\"569\":1}}],[\"也可以在代码里设置\",{\"1\":{\"520\":1}}],[\"也可以传\",{\"1\":{\"474\":1}}],[\"也可以进行随机裁剪\",{\"1\":{\"425\":1}}],[\"也可以采用视觉transformer模型\",{\"1\":{\"407\":1}}],[\"也可以作为融合编码器\",{\"1\":{\"370\":1}}],[\"也可以很好地训练模型\",{\"1\":{\"353\":1}}],[\"也可以是特定任务\",{\"1\":{\"339\":1}}],[\"也可以是软标签\",{\"1\":{\"88\":1}}],[\"也可以只在局部子集上作用\",{\"1\":{\"112\":1}}],[\"也可以归一化为\",{\"1\":{\"106\":1}}],[\"也可\",{\"1\":{\"73\":1}}],[\"种\",{\"1\":{\"921\":1}}],[\"种重复排列\",{\"1\":{\"881\":1}}],[\"种不同的排列方式\",{\"1\":{\"881\":1}}],[\"种可能结果\",{\"1\":{\"880\":4}}],[\"种可能的值\",{\"1\":{\"885\":1}}],[\"种可能的顺序\",{\"1\":{\"631\":1}}],[\"种可能的图像\",{\"1\":{\"262\":1}}],[\"种语言和方言\",{\"1\":{\"823\":1}}],[\"种模型大小\",{\"1\":{\"823\":1}}],[\"种唯一组合\",{\"1\":{\"91\":1}}],[\"种物体\",{\"1\":{\"87\":1}}],[\"内循环\",{\"1\":{\"650\":1}}],[\"内取平均\",{\"1\":{\"589\":1}}],[\"内样本取平均\",{\"1\":{\"587\":1,\"588\":1,\"589\":1}}],[\"内\",{\"1\":{\"586\":1,\"592\":1,\"660\":1}}],[\"内置随机数种子\",{\"1\":{\"521\":1}}],[\"内置库\",{\"1\":{\"516\":1}}],[\"内置\",{\"0\":{\"515\":1,\"517\":1},\"1\":{\"521\":1}}],[\"内置命名空间\",{\"1\":{\"444\":1}}],[\"内存释放机制\",{\"1\":{\"807\":1}}],[\"内存使用量大幅降低\",{\"1\":{\"807\":1}}],[\"内存占用低\",{\"1\":{\"807\":1}}],[\"内存占用高\",{\"1\":{\"807\":1}}],[\"内存\",{\"1\":{\"667\":1}}],[\"内存管理\",{\"1\":{\"667\":1}}],[\"内存地址不变\",{\"1\":{\"546\":1}}],[\"内存空间得以连续布局\",{\"1\":{\"545\":1}}],[\"内存先存储第\",{\"1\":{\"541\":2}}],[\"内存不连续\",{\"1\":{\"489\":1}}],[\"内存连续\",{\"1\":{\"489\":1}}],[\"内存步长\",{\"1\":{\"469\":1}}],[\"内存布局\",{\"1\":{\"468\":1}}],[\"内存消耗\",{\"1\":{\"223\":1}}],[\"内负样本有限\",{\"1\":{\"385\":1}}],[\"内从\",{\"1\":{\"286\":1}}],[\"内线性升至基础值\",{\"1\":{\"286\":1}}],[\"内积\",{\"1\":{\"274\":1}}],[\"内容不可追溯\",{\"1\":{\"828\":1}}],[\"内容相似度\",{\"1\":{\"709\":1}}],[\"内容结构遵循原文小节安排\",{\"1\":{\"655\":1}}],[\"内容\",{\"1\":{\"91\":1}}],[\"内部又是\",{\"1\":{\"694\":1}}],[\"内部是\",{\"1\":{\"694\":1}}],[\"内部验证调用\",{\"1\":{\"382\":2}}],[\"内部调用\",{\"1\":{\"382\":5}}],[\"内部标注人员手动构造的示例\",{\"1\":{\"339\":1}}],[\"内部\",{\"1\":{\"57\":1,\"97\":2,\"381\":4}}],[\"内部或\",{\"1\":{\"57\":1}}],[\"内部可供性不足\",{\"1\":{\"27\":1}}],[\"抓取所有垃圾邮件\",{\"1\":{\"566\":1}}],[\"抓取耳机\",{\"1\":{\"89\":1}}],[\"抓握中部\",{\"1\":{\"52\":1}}],[\"目的\",{\"1\":{\"418\":1,\"419\":1,\"420\":1,\"518\":1,\"710\":1}}],[\"目的是为了让llm的推理能力能够更进一步提升\",{\"1\":{\"621\":1}}],[\"目的是\",{\"1\":{\"589\":1,\"918\":1}}],[\"目的是让点云\",{\"1\":{\"152\":1}}],[\"目的是从一个大的数据集中选出一组代表性强的点\",{\"1\":{\"134\":1}}],[\"目的是测试模型对新组合的泛化能力\",{\"1\":{\"89\":1}}],[\"目的是评估模型在熟悉场景下的表现\",{\"1\":{\"89\":1}}],[\"目录获取所有图片路径\",{\"1\":{\"411\":1,\"412\":1}}],[\"目前为止\",{\"1\":{\"949\":1}}],[\"目前采用的主要改进如下\",{\"1\":{\"823\":1}}],[\"目前实现的版本无法实现上图中计算图多分支的结构\",{\"1\":{\"784\":1}}],[\"目前尚无有效方式解释它为何会给出某一答案\",{\"1\":{\"649\":1}}],[\"目前尚不清楚模型在推理任务中是否\",{\"1\":{\"649\":1}}],[\"目前还没有达成共识\",{\"1\":{\"626\":1}}],[\"目前主流的方法包括2019年\",{\"1\":{\"610\":1}}],[\"目前参数量最小的多模态transformer方法\",{\"1\":{\"389\":1}}],[\"目前\",{\"1\":{\"228\":1,\"822\":1,\"823\":2,\"828\":1,\"836\":1}}],[\"目标不同\",{\"1\":{\"963\":1}}],[\"目标图像\",{\"1\":{\"944\":1}}],[\"目标倾向于优先建模像素之间的短程依赖\",{\"1\":{\"885\":1}}],[\"目标人群\",{\"1\":{\"836\":1}}],[\"目标掩码\",{\"1\":{\"750\":1}}],[\"目标任务\",{\"1\":{\"626\":1}}],[\"目标点\",{\"1\":{\"589\":1}}],[\"目标点云\",{\"1\":{\"122\":1}}],[\"目标点云坐标\",{\"1\":{\"122\":1}}],[\"目标大小\",{\"1\":{\"505\":1}}],[\"目标函数是\",{\"1\":{\"918\":1}}],[\"目标函数使用\",{\"1\":{\"887\":1}}],[\"目标函数如下\",{\"1\":{\"656\":1}}],[\"目标函数\",{\"1\":{\"500\":2}}],[\"目标仍是\",{\"1\":{\"472\":1}}],[\"目标让视觉编码器输出的图像特征与语言模型的词向量空间对齐\",{\"1\":{\"341\":1}}],[\"目标锐化\",{\"1\":{\"290\":1}}],[\"目标值\",{\"1\":{\"259\":1}}],[\"目标预训练\",{\"0\":{\"248\":1}}],[\"目标来预训练\",{\"1\":{\"223\":1}}],[\"目标检测\",{\"1\":{\"220\":1,\"222\":1,\"589\":1}}],[\"目标检测器开销高\",{\"1\":{\"194\":1}}],[\"目标特征\",{\"1\":{\"213\":1}}],[\"目标进行微调\",{\"1\":{\"173\":1}}],[\"目标在\",{\"1\":{\"173\":1}}],[\"目标是用看起来合理的像素填补这个空洞\",{\"1\":{\"952\":1}}],[\"目标是学习一个生成模型\",{\"1\":{\"942\":1}}],[\"目标是学习一个全局表示\",{\"1\":{\"626\":1}}],[\"目标是建立对数据点\",{\"1\":{\"942\":1}}],[\"目标是使现有模型更符合人类意图\",{\"1\":{\"658\":1}}],[\"目标是使正样本的图文对在特征空间中接近\",{\"1\":{\"172\":1}}],[\"目标是训练好\",{\"1\":{\"417\":1}}],[\"目标是从\",{\"1\":{\"373\":1}}],[\"目标是为了让真正标签的输出尽可能的大\",{\"1\":{\"355\":1}}],[\"目标是让模型在预训练的基础上进一步掌握多模态指令理解与复杂推理能力\",{\"1\":{\"342\":1}}],[\"目标是优化模型\",{\"1\":{\"32\":1}}],[\"目标\",{\"1\":{\"143\":1,\"228\":1,\"305\":3,\"339\":1,\"589\":2,\"630\":1,\"677\":1,\"678\":1}}],[\"目标批次索引\",{\"1\":{\"121\":1}}],[\"目标功能区域\",{\"1\":{\"102\":1}}],[\"目标区域的特征\",{\"1\":{\"83\":1}}],[\"目标物体框\",{\"1\":{\"82\":2}}],[\"🔸\",{\"1\":{\"898\":2}}],[\"🔗\",{\"1\":{\"854\":1}}],[\"🚀\",{\"0\":{\"752\":1}}],[\"📚\",{\"1\":{\"735\":1}}],[\"👉\",{\"1\":{\"526\":1,\"528\":1,\"578\":1}}],[\"🟡\",{\"1\":{\"493\":3}}],[\"🔴\",{\"1\":{\"493\":1}}],[\"📌\",{\"1\":{\"157\":1}}],[\"📐\",{\"1\":{\"157\":1}}],[\"📊\",{\"1\":{\"157\":1}}],[\"📈\",{\"1\":{\"157\":2}}],[\"📉\",{\"1\":{\"157\":2}}],[\"🔍\",{\"1\":{\"157\":6,\"735\":1}}],[\"📦\",{\"1\":{\"145\":1,\"160\":1}}],[\"🔹\",{\"1\":{\"89\":2}}],[\"🔹目标\",{\"1\":{\"52\":4}}],[\"💡\",{\"1\":{\"88\":1,\"157\":1,\"355\":1,\"735\":1}}],[\"构成样本空间\",{\"1\":{\"850\":1}}],[\"构成一个位置序列矩阵\",{\"1\":{\"716\":1}}],[\"构成了一个\",{\"1\":{\"590\":1}}],[\"构成解码器\",{\"1\":{\"239\":1}}],[\"构成\",{\"1\":{\"222\":1}}],[\"构造编码器与解码器的通道列表\",{\"1\":{\"899\":1}}],[\"构造预测标签\",{\"1\":{\"893\":1}}],[\"构造相对位置矩阵\",{\"1\":{\"710\":1}}],[\"构造训练目标\",{\"1\":{\"420\":1}}],[\"构造函数\",{\"1\":{\"380\":1}}],[\"构造对比学习的标签\",{\"1\":{\"274\":1}}],[\"构造对应的数据加载器\",{\"1\":{\"187\":1}}],[\"构造遮挡掩码\",{\"1\":{\"266\":1}}],[\"构造目标标签\",{\"1\":{\"208\":1}}],[\"构造新的图文对\",{\"1\":{\"207\":1}}],[\"构造一对一的匹配目标\",{\"1\":{\"206\":1}}],[\"构造一个新的张量\",{\"1\":{\"700\":1}}],[\"构造一个可学习的\",{\"1\":{\"274\":1}}],[\"构造一个与\",{\"1\":{\"208\":1,\"896\":1}}],[\"构造一个单位矩阵\",{\"1\":{\"153\":1}}],[\"构造一个从\",{\"1\":{\"137\":1}}],[\"构造软标签\",{\"1\":{\"202\":1}}],[\"构造两组负样本\",{\"1\":{\"190\":1}}],[\"构造图文匹配矩阵\",{\"1\":{\"190\":3}}],[\"构造图像的\",{\"1\":{\"206\":1}}],[\"构造图像\",{\"1\":{\"188\":1}}],[\"构造输入的\",{\"1\":{\"188\":1}}],[\"构造\",{\"1\":{\"188\":1,\"192\":1,\"206\":1,\"207\":1,\"274\":1,\"385\":1,\"699\":1,\"892\":4,\"893\":1}}],[\"构造出你所需要的所有\",{\"1\":{\"847\":1}}],[\"构造出\",{\"1\":{\"93\":1}}],[\"构造出对应的二值掩码\",{\"1\":{\"88\":1}}],[\"构建目标函数\",{\"0\":{\"945\":1}}],[\"构建标签\",{\"1\":{\"900\":1}}],[\"构建编码器和解码器的每一层\",{\"1\":{\"899\":1}}],[\"构建编码器层\",{\"1\":{\"123\":1}}],[\"构建出一个来源于实际业务的小型验证集\",{\"1\":{\"836\":1}}],[\"构建出性能更强的教师来指导学生学习\",{\"1\":{\"289\":1}}],[\"构建器平台\",{\"1\":{\"823\":1}}],[\"构建泰勒展开\",{\"1\":{\"814\":1}}],[\"构建样本\",{\"1\":{\"698\":1}}],[\"构建词汇表\",{\"1\":{\"595\":1}}],[\"构建加权随机采样器\",{\"1\":{\"518\":1}}],[\"构建一个基于\",{\"1\":{\"588\":1,\"589\":1}}],[\"构建一个组合损失函数\",{\"1\":{\"587\":1}}],[\"构建一个下三角矩阵作为因果掩码矩阵\",{\"1\":{\"420\":1}}],[\"构建一个多样化的指令\",{\"1\":{\"339\":1}}],[\"构建匹配标签\",{\"1\":{\"419\":1}}],[\"构建输入图像列表\",{\"1\":{\"419\":1}}],[\"构建输入文本列表\",{\"1\":{\"419\":1}}],[\"构建query和text的padding\",{\"1\":{\"419\":1}}],[\"构建query\",{\"1\":{\"419\":1}}],[\"构建padding\",{\"1\":{\"417\":1}}],[\"构建描述文本并提取特征\",{\"1\":{\"408\":1}}],[\"构建的\",{\"1\":{\"403\":1}}],[\"构建返回字典\",{\"1\":{\"386\":1}}],[\"构建负样本\",{\"1\":{\"386\":1}}],[\"构建测试\",{\"1\":{\"382\":1}}],[\"构建测试数据集\",{\"1\":{\"382\":1}}],[\"构建验证\",{\"1\":{\"382\":1}}],[\"构建验证数据集\",{\"1\":{\"382\":1}}],[\"构建训练\",{\"1\":{\"382\":1}}],[\"构建训练数据集\",{\"1\":{\"382\":1}}],[\"构建训练数据加载器\",{\"1\":{\"265\":1}}],[\"构建无干扰的测试集\",{\"1\":{\"382\":1}}],[\"构建无干扰的验证集\",{\"1\":{\"382\":1}}],[\"构建无干扰测试数据集\",{\"1\":{\"382\":1}}],[\"构建无干扰验证数据集\",{\"1\":{\"382\":1}}],[\"构建优化器\",{\"1\":{\"293\":1}}],[\"构建教师网络\",{\"1\":{\"293\":1}}],[\"构建学生网络\",{\"1\":{\"293\":1}}],[\"构建学生和教师\",{\"1\":{\"293\":1}}],[\"构建学生\",{\"1\":{\"293\":2}}],[\"构建多视图增强\",{\"1\":{\"293\":1}}],[\"构建多层编码器和解码器\",{\"1\":{\"255\":1}}],[\"构建信息流\",{\"1\":{\"214\":1}}],[\"构建动量编码器\",{\"1\":{\"205\":1}}],[\"构建文本所有对比特征\",{\"1\":{\"192\":1}}],[\"构建图像所有对比特征\",{\"1\":{\"192\":1}}],[\"构建语言建模标签\",{\"1\":{\"187\":1}}],[\"构建预训练语料\",{\"1\":{\"183\":1}}],[\"构建全局特征向量\",{\"1\":{\"160\":1}}],[\"构建点云的层次化表示\",{\"1\":{\"157\":1}}],[\"构建点之间的邻接图\",{\"1\":{\"157\":1}}],[\"构建mlp层\",{\"1\":{\"145\":1}}],[\"构建用于特征传播\",{\"1\":{\"145\":1}}],[\"构建它们的局部邻域区域\",{\"1\":{\"137\":1}}],[\"构建局部邻域的半径\",{\"1\":{\"137\":2}}],[\"构建解码器层\",{\"1\":{\"123\":1}}],[\"构建解码器路径\",{\"1\":{\"122\":1}}],[\"构建邻域\",{\"1\":{\"119\":1}}],[\"构建层级点\",{\"1\":{\"110\":1}}],[\"构建了语言模型\",{\"1\":{\"691\":1}}],[\"构建了一个与\",{\"1\":{\"888\":1}}],[\"构建了一个动态的字典\",{\"1\":{\"352\":1}}],[\"构建了一个强大且通用的视觉\",{\"1\":{\"313\":1}}],[\"构建了多模态对话系统\",{\"1\":{\"300\":1}}],[\"构建了基于\",{\"1\":{\"109\":1}}],[\"构建了高表达能力的网络骨干\",{\"1\":{\"109\":1}}],[\"构建问题\",{\"0\":{\"87\":1}}],[\"构建两个基于损坏的评测基准\",{\"1\":{\"19\":1}}],[\"构建\",{\"1\":{\"19\":1,\"67\":2,\"102\":2,\"264\":1,\"265\":1,\"293\":2,\"334\":1,\"380\":2,\"382\":1,\"386\":1,\"420\":1,\"589\":1,\"898\":1}}],[\"陈述句等\",{\"1\":{\"87\":1}}],[\"疑问句\",{\"1\":{\"87\":1}}],[\"例外情况\",{\"1\":{\"641\":1}}],[\"例子说明\",{\"0\":{\"534\":1}}],[\"例子如下\",{\"1\":{\"466\":1,\"804\":1}}],[\"例子\",{\"1\":{\"460\":1,\"486\":1,\"516\":1,\"906\":1,\"907\":1}}],[\"例\",{\"1\":{\"87\":1,\"881\":1}}],[\"例如书写快导致角度大\",{\"1\":{\"944\":1}}],[\"例如二值数据可用伯努利分布\",{\"1\":{\"943\":1}}],[\"例如对于黑白手写数字数据集\",{\"1\":{\"925\":1}}],[\"例如更准确地按照提示文本生成图像\",{\"1\":{\"894\":1}}],[\"例如灰度图像\",{\"0\":{\"875\":1}}],[\"例如某些事件之间是正相关的\",{\"1\":{\"863\":1}}],[\"例如某些包在arm64系统上没有预先编译好的版本\",{\"1\":{\"557\":1}}],[\"例如原始数据或预处理后的数据\",{\"1\":{\"831\":1}}],[\"例如数字类别\",{\"1\":{\"944\":1}}],[\"例如数据收集模块或预处理模块\",{\"1\":{\"831\":1}}],[\"例如数学问题\",{\"1\":{\"825\":1}}],[\"例如拥有\",{\"1\":{\"822\":1}}],[\"例如加法\",{\"1\":{\"800\":1}}],[\"例如长度\",{\"1\":{\"710\":1}}],[\"例如上图中\",{\"1\":{\"694\":1}}],[\"例如65b模型维度为8192\",{\"1\":{\"667\":1}}],[\"例如1\",{\"1\":{\"653\":1}}],[\"例如预测缺失的单词或句子\",{\"1\":{\"640\":1}}],[\"例如背景像素可能占据了大部分\",{\"1\":{\"584\":1}}],[\"例如总共\",{\"1\":{\"564\":1,\"565\":1}}],[\"例如卷云\",{\"1\":{\"561\":1}}],[\"例如权重初始化\",{\"1\":{\"521\":1}}],[\"例如我们希望将每个\",{\"1\":{\"501\":1}}],[\"例如从\",{\"1\":{\"501\":1}}],[\"例如将\",{\"1\":{\"492\":1,\"502\":1,\"894\":1}}],[\"例如经过了\",{\"1\":{\"470\":1}}],[\"例如参数量为\",{\"1\":{\"432\":1}}],[\"例如clip\",{\"1\":{\"415\":1}}],[\"例如谷歌的bit和vit基于jft\",{\"1\":{\"413\":1}}],[\"例如2017年的那篇工作只在imagenet上实现了11\",{\"1\":{\"413\":1}}],[\"例如virtex基于transformer的语言模型\",{\"1\":{\"413\":1}}],[\"例如openai的gpt\",{\"1\":{\"413\":1}}],[\"例如语音\",{\"1\":{\"377\":1}}],[\"例如在原图上坐标为\",{\"1\":{\"501\":1}}],[\"例如在多头自注意力机制或前馈网络中引入卷积层\",{\"1\":{\"434\":1}}],[\"例如在\",{\"1\":{\"309\":1}}],[\"例如学习率不同会导致准确率差异较大\",{\"1\":{\"286\":1}}],[\"例如用于\",{\"1\":{\"256\":1}}],[\"例如音频\",{\"1\":{\"225\":1}}],[\"例如代码\",{\"1\":{\"215\":1}}],[\"例如法向量\",{\"1\":{\"70\":1}}],[\"例如不同形状的椅子\",{\"1\":{\"53\":1}}],[\"例如\",{\"1\":{\"30\":1,\"31\":3,\"69\":1,\"86\":1,\"89\":1,\"94\":1,\"107\":1,\"143\":1,\"145\":1,\"202\":2,\"204\":1,\"208\":2,\"222\":1,\"263\":2,\"266\":3,\"288\":1,\"308\":1,\"311\":1,\"380\":1,\"408\":1,\"409\":1,\"413\":1,\"422\":1,\"434\":2,\"448\":1,\"454\":1,\"483\":1,\"485\":1,\"500\":1,\"501\":2,\"502\":3,\"544\":1,\"545\":1,\"551\":1,\"562\":1,\"572\":1,\"590\":1,\"593\":1,\"640\":1,\"657\":2,\"658\":1,\"666\":1,\"668\":1,\"681\":1,\"691\":3,\"692\":1,\"707\":2,\"708\":1,\"710\":3,\"733\":1,\"765\":1,\"779\":1,\"803\":2,\"806\":3,\"809\":3,\"822\":1,\"836\":1,\"846\":5,\"847\":1,\"857\":1,\"892\":1,\"893\":1,\"896\":1,\"903\":1,\"926\":1,\"944\":1,\"946\":1}}],[\"例如图像\",{\"1\":{\"19\":1}}],[\"原图边长\",{\"1\":{\"892\":1}}],[\"原图面积之间\",{\"1\":{\"293\":1}}],[\"原像\",{\"1\":{\"846\":1}}],[\"原有代码\",{\"1\":{\"806\":2}}],[\"原序列添加特殊token标记图\",{\"1\":{\"713\":1}}],[\"原文\",{\"1\":{\"656\":1}}],[\"原sota\",{\"1\":{\"641\":2}}],[\"原模型虽大\",{\"1\":{\"614\":1}}],[\"原张量是\",{\"1\":{\"544\":1}}],[\"原张量在\",{\"1\":{\"486\":1}}],[\"原本是正样本的\",{\"1\":{\"356\":1}}],[\"原来的标签\",{\"1\":{\"384\":1}}],[\"原来\",{\"1\":{\"274\":1}}],[\"原因如下\",{\"1\":{\"951\":1}}],[\"原因\",{\"1\":{\"493\":4,\"522\":1}}],[\"原因是浅层更关注输入的\",{\"1\":{\"215\":1}}],[\"原因分析\",{\"1\":{\"157\":2}}],[\"原论文\",{\"1\":{\"214\":1}}],[\"原理大致跟rlhf类似\",{\"1\":{\"602\":1}}],[\"原理\",{\"0\":{\"423\":1}}],[\"原理回顾\",{\"1\":{\"157\":1}}],[\"原理说明\",{\"1\":{\"150\":1}}],[\"原地操作节省内存\",{\"1\":{\"120\":1,\"121\":1}}],[\"原则\",{\"1\":{\"87\":1,\"444\":1}}],[\"原始的变分自编码器\",{\"1\":{\"947\":1}}],[\"原始的步长可能是\",{\"1\":{\"545\":1}}],[\"原始论文链接\",{\"1\":{\"921\":1}}],[\"原始论文layernorm在最后\",{\"1\":{\"745\":1}}],[\"原始注意力分数\",{\"1\":{\"710\":1}}],[\"原始bert使用30k的字符级bpe词汇表\",{\"1\":{\"681\":1}}],[\"原始bert使用256的批次大小训练1m步\",{\"1\":{\"681\":1}}],[\"原始bert使用\",{\"1\":{\"681\":1}}],[\"原始bert使用静态掩码\",{\"1\":{\"681\":1,\"683\":1}}],[\"原始bert的做法\",{\"1\":{\"681\":1}}],[\"原始空间中\",{\"1\":{\"578\":1}}],[\"原始形状为\",{\"1\":{\"546\":1}}],[\"原始transformer的norm层在多头注意力和前馈网络之后\",{\"1\":{\"429\":1}}],[\"原始实现\",{\"1\":{\"401\":1}}],[\"原始\",{\"0\":{\"918\":1},\"1\":{\"385\":2,\"657\":1,\"658\":1,\"679\":2,\"680\":1,\"885\":1,\"899\":1}}],[\"原始图像张量\",{\"1\":{\"384\":1}}],[\"原始文本\",{\"1\":{\"202\":1,\"384\":1}}],[\"原始批次索引\",{\"1\":{\"121\":1}}],[\"原始特征\",{\"1\":{\"121\":1}}],[\"原始点集合\",{\"1\":{\"159\":1}}],[\"原始点数量\",{\"1\":{\"145\":1}}],[\"原始点对应的特征数据\",{\"1\":{\"145\":1}}],[\"原始点坐标数据\",{\"1\":{\"145\":1}}],[\"原始点坐标\",{\"1\":{\"143\":1}}],[\"原始点特征\",{\"1\":{\"98\":1,\"143\":1}}],[\"原始点云数据\",{\"1\":{\"138\":1}}],[\"原始点云数量\",{\"1\":{\"70\":2}}],[\"原始点云点数\",{\"1\":{\"83\":1}}],[\"原始交互文本\",{\"1\":{\"55\":1}}],[\"手写数字\",{\"1\":{\"819\":1}}],[\"手动传递导数\",{\"1\":{\"781\":1}}],[\"手动进行反向传播\",{\"0\":{\"777\":1}}],[\"手动实现线性层反向传播\",{\"1\":{\"667\":1}}],[\"手动指定某些类别的权重\",{\"1\":{\"514\":1}}],[\"手工特征\",{\"1\":{\"210\":1}}],[\"手工设计问题\",{\"1\":{\"87\":1}}],[\"手臂\",{\"1\":{\"83\":1}}],[\"相较于真实分布累积分布函数\",{\"1\":{\"949\":1}}],[\"相较于传统的语言模型具有更强大的能力\",{\"1\":{\"828\":1}}],[\"相互抵消的平衡点\",{\"1\":{\"873\":1}}],[\"相信未来会涌现出很多类似的以对话形态让助手完成各种具体工作的项目\",{\"1\":{\"827\":1}}],[\"相应的位置信息\",{\"1\":{\"707\":1}}],[\"相加过程总结\",{\"1\":{\"710\":1}}],[\"相加\",{\"1\":{\"693\":1}}],[\"相结合\",{\"1\":{\"592\":1}}],[\"相邻重复值\",{\"1\":{\"480\":1}}],[\"相邻阶段通过转换模块连接\",{\"1\":{\"116\":1}}],[\"相媲美的效果\",{\"1\":{\"436\":1}}],[\"相同形状的\",{\"1\":{\"896\":1}}],[\"相同字母且不出现在输出\",{\"1\":{\"709\":1}}],[\"相同\",{\"1\":{\"224\":1,\"339\":1,\"472\":1,\"491\":1,\"514\":1,\"590\":1,\"592\":1,\"710\":1}}],[\"相同的维度字母表示要做\",{\"1\":{\"475\":1}}],[\"相同的\",{\"1\":{\"223\":1}}],[\"相当\",{\"1\":{\"224\":1,\"322\":1,\"640\":1}}],[\"相当于输出了一个\",{\"1\":{\"959\":1}}],[\"相当于人为扩大重构误差项的权重\",{\"1\":{\"951\":1}}],[\"相当于限制了函数的最大\",{\"1\":{\"917\":1}}],[\"相当于为计算机提供了强大的\",{\"1\":{\"822\":1}}],[\"相当于一个\",{\"1\":{\"578\":1}}],[\"相当于给进程建立一个\",{\"1\":{\"520\":1}}],[\"相当于对输入空间进行一次\",{\"1\":{\"500\":1}}],[\"相当于直接输入一篇五万字的文章\",{\"1\":{\"426\":1}}],[\"相当于数据量不够\",{\"1\":{\"353\":1}}],[\"相当于在训练中持续进行模型集成\",{\"1\":{\"289\":1}}],[\"相当于\",{\"1\":{\"256\":1,\"264\":1,\"451\":1}}],[\"相当于告诉模型\",{\"1\":{\"156\":1}}],[\"相当于加了一个\",{\"1\":{\"152\":1}}],[\"相当于cnn中的\",{\"1\":{\"121\":1,\"122\":1}}],[\"相当于跨空间位置的信息交换\",{\"1\":{\"97\":1}}],[\"相关证明可见附录\",{\"1\":{\"949\":1}}],[\"相关计算\",{\"1\":{\"892\":1}}],[\"相关的标量偏置\",{\"1\":{\"710\":1}}],[\"相关核心代码实现如下\",{\"1\":{\"696\":1}}],[\"相关代码\",{\"1\":{\"688\":1}}],[\"相关模型和代码已公开供进一步研究\",{\"1\":{\"677\":1}}],[\"相关阅读资料\",{\"1\":{\"607\":1}}],[\"相关任务中表现突出\",{\"1\":{\"322\":1}}],[\"相关\",{\"1\":{\"215\":1}}],[\"相关工作\",{\"0\":{\"20\":1,\"31\":1,\"74\":1,\"110\":1,\"216\":1,\"244\":1,\"269\":1,\"281\":1,\"297\":1,\"324\":1,\"627\":1,\"650\":1,\"655\":1,\"671\":1,\"687\":1}}],[\"相似的结果\",{\"1\":{\"943\":1}}],[\"相似度指标\",{\"1\":{\"590\":1}}],[\"相似度高\",{\"1\":{\"506\":1}}],[\"相似度如下所示\",{\"1\":{\"408\":1}}],[\"相似度生成的\",{\"1\":{\"207\":1}}],[\"相似度\",{\"1\":{\"192\":1,\"199\":1,\"213\":1,\"355\":1,\"631\":1}}],[\"相似性得分\",{\"1\":{\"106\":1}}],[\"相等\",{\"1\":{\"190\":1,\"472\":1}}],[\"相比与其他生成模型\",{\"1\":{\"925\":1}}],[\"相比普通的\",{\"1\":{\"921\":1}}],[\"相比原始\",{\"1\":{\"680\":1}}],[\"相比前代模型大幅提升\",{\"1\":{\"649\":1}}],[\"相比更低\",{\"1\":{\"641\":1}}],[\"相比其他替代方案如rnn\",{\"1\":{\"626\":1}}],[\"相比w\",{\"1\":{\"602\":1}}],[\"相比单独图像预训练\",{\"1\":{\"376\":1}}],[\"相比单编码器分类需要人工标注和数据清理\",{\"1\":{\"271\":1}}],[\"相比传统轻量级\",{\"1\":{\"304\":1}}],[\"相比qformer等轻量适配器\",{\"1\":{\"303\":1}}],[\"相比标准的编码器\",{\"1\":{\"272\":1}}],[\"相比双向方法需要两次前向传播\",{\"1\":{\"272\":1}}],[\"相比专用模型\",{\"1\":{\"268\":1}}],[\"相比\",{\"1\":{\"259\":1,\"280\":1,\"470\":1,\"475\":1,\"657\":1,\"823\":2}}],[\"相比对比学习模型\",{\"1\":{\"224\":1}}],[\"相比于普通的vae\",{\"1\":{\"955\":1}}],[\"相比于每次开发单个模型的方式\",{\"1\":{\"826\":1}}],[\"相比于著名的\",{\"1\":{\"696\":1}}],[\"相比于grid\",{\"1\":{\"394\":1}}],[\"相比于region\",{\"1\":{\"394\":1}}],[\"相比于逐\",{\"1\":{\"263\":1}}],[\"相比于传统的\",{\"1\":{\"172\":1}}],[\"相比于随机采样\",{\"1\":{\"134\":1}}],[\"相比使用预训练目标检测器\",{\"1\":{\"171\":1}}],[\"相比之下\",{\"1\":{\"157\":1,\"223\":1,\"280\":3,\"413\":1,\"646\":1,\"884\":1,\"951\":1}}],[\"相反\",{\"1\":{\"135\":1,\"142\":1,\"544\":1,\"631\":1}}],[\"相对或绝对\",{\"1\":{\"898\":1}}],[\"相对标准差\",{\"1\":{\"874\":1}}],[\"相对于均值\",{\"1\":{\"871\":1}}],[\"相对距离\",{\"1\":{\"710\":1}}],[\"相对位置可能范围很大\",{\"1\":{\"710\":1}}],[\"相对位置矩阵\",{\"1\":{\"710\":1}}],[\"相对位置矩阵relative\",{\"1\":{\"710\":1}}],[\"相对位置桶映射\",{\"1\":{\"710\":1}}],[\"相对位置桶数\",{\"1\":{\"710\":1}}],[\"相对位置桶\",{\"1\":{\"710\":1}}],[\"相对位置信息\",{\"1\":{\"710\":1}}],[\"相对位置的分组数量\",{\"1\":{\"710\":1}}],[\"相对位置像\",{\"1\":{\"710\":2}}],[\"相对位置偏置\",{\"0\":{\"710\":1},\"1\":{\"710\":1}}],[\"相对位置索引矩阵\",{\"1\":{\"709\":1}}],[\"相对位置部分\",{\"1\":{\"709\":1}}],[\"相对位置向量表\",{\"1\":{\"709\":1}}],[\"相对位置编码中的相对位置矩阵的详解\",{\"1\":{\"709\":1}}],[\"相对位置编码改成\",{\"1\":{\"709\":1}}],[\"相对位置编码直接对\",{\"1\":{\"708\":1}}],[\"相对位置编码偏置\",{\"1\":{\"380\":1}}],[\"相对位置编码\",{\"0\":{\"708\":1},\"1\":{\"119\":1,\"380\":1,\"892\":1}}],[\"相对来说成本会比较高\",{\"1\":{\"602\":1}}],[\"相对坐标\",{\"1\":{\"121\":1}}],[\"相对坐标计算\",{\"1\":{\"119\":1}}],[\"相乘\",{\"1\":{\"83\":1,\"877\":1}}],[\"前几层将\",{\"1\":{\"944\":1}}],[\"前几个块会比后面的多一个元素\",{\"1\":{\"482\":1}}],[\"前4个位置为文本token\",{\"1\":{\"892\":1}}],[\"前中后都加\",{\"1\":{\"892\":1}}],[\"前乘了一个经验选择的系数\",{\"1\":{\"885\":1}}],[\"前景与背景融合不自然\",{\"1\":{\"884\":1}}],[\"前景点少\",{\"1\":{\"589\":1}}],[\"前景点\",{\"1\":{\"102\":1}}],[\"前后端搭建\",{\"1\":{\"836\":1}}],[\"前三阶段的\",{\"1\":{\"819\":1}}],[\"前缀\",{\"1\":{\"808\":1}}],[\"前缀让\",{\"1\":{\"346\":1}}],[\"前馈层\",{\"1\":{\"746\":2,\"749\":2}}],[\"前馈神经网络\",{\"1\":{\"741\":1}}],[\"前馈网络子层\",{\"1\":{\"380\":1}}],[\"前馈网络的中间层维度为\",{\"1\":{\"236\":1}}],[\"前馈网络池\",{\"1\":{\"222\":1}}],[\"前一半桶\",{\"1\":{\"710\":1}}],[\"前一个epoch的学生网络\",{\"1\":{\"289\":1}}],[\"前面是特殊token\",{\"1\":{\"697\":1}}],[\"前提是标准差\",{\"1\":{\"949\":1}}],[\"前提是这两个事件是互斥的\",{\"1\":{\"848\":1}}],[\"前提是数据集大致平衡\",{\"1\":{\"572\":1}}],[\"前提条件是行优先存储\",{\"1\":{\"545\":1}}],[\"前者是其2倍\",{\"1\":{\"696\":1}}],[\"前者是用模型\",{\"1\":{\"511\":1}}],[\"前者为双向\",{\"1\":{\"171\":1}}],[\"前使用\",{\"1\":{\"470\":1}}],[\"前半部分是文本\",{\"1\":{\"384\":1}}],[\"前半部分为单模态解码器\",{\"1\":{\"268\":1}}],[\"前置知识\",{\"0\":{\"379\":1,\"821\":1,\"902\":1}}],[\"前两步不变\",{\"1\":{\"609\":1}}],[\"前两列\",{\"1\":{\"544\":1}}],[\"前两行\",{\"1\":{\"544\":1}}],[\"前两张图片的表征距离比较近\",{\"1\":{\"349\":1}}],[\"前两种与\",{\"1\":{\"44\":1}}],[\"前向模型\",{\"1\":{\"878\":1}}],[\"前向计算\",{\"1\":{\"293\":1}}],[\"前向主函数\",{\"1\":{\"266\":1}}],[\"前向特征提取\",{\"1\":{\"266\":1}}],[\"前向传播时\",{\"1\":{\"959\":2}}],[\"前向传播时是one\",{\"1\":{\"258\":1}}],[\"前向传播和反向传播的计算可以不对应\",{\"1\":{\"959\":1}}],[\"前向传播逻辑\",{\"1\":{\"699\":1}}],[\"前向传播计算\",{\"1\":{\"592\":1}}],[\"前向传播计算损失值\",{\"1\":{\"590\":1}}],[\"前向传播流程\",{\"0\":{\"396\":1,\"893\":1}}],[\"前向传播获取特征\",{\"1\":{\"213\":1}}],[\"前向传播的过程中\",{\"1\":{\"213\":1}}],[\"前向传播代码实现\",{\"1\":{\"208\":1}}],[\"前向传播函数\",{\"1\":{\"145\":1,\"146\":1,\"213\":2,\"426\":1,\"586\":1,\"587\":1,\"588\":1,\"589\":1,\"926\":1}}],[\"前向传播过程主要是为了计算两个训练目标的损失\",{\"1\":{\"190\":1}}],[\"前向传播过程\",{\"1\":{\"105\":1}}],[\"前向传播过程如下\",{\"1\":{\"94\":1}}],[\"前向传播\",{\"0\":{\"256\":1},\"1\":{\"83\":1,\"106\":1,\"120\":1,\"121\":2,\"122\":1,\"123\":1,\"187\":1,\"208\":1,\"265\":1,\"427\":1,\"926\":2}}],[\"前将低维嵌入映射回高维空间\",{\"1\":{\"212\":1}}],[\"前\",{\"1\":{\"204\":1,\"207\":1,\"236\":1,\"286\":1,\"293\":1,\"380\":1,\"893\":1}}],[\"前期\",{\"1\":{\"204\":1}}],[\"前的某一层\",{\"1\":{\"143\":1}}],[\"参与的是可导的加法和乘法\",{\"1\":{\"931\":1}}],[\"参与点积计算\",{\"1\":{\"710\":1}}],[\"参与配对但保留在输出里\",{\"1\":{\"709\":1}}],[\"参与数据标注\",{\"1\":{\"656\":1}}],[\"参数准备\",{\"1\":{\"918\":1}}],[\"参数和文本词空间的构成需要简单说明一下\",{\"1\":{\"892\":1}}],[\"参数和计算更高效\",{\"1\":{\"522\":1}}],[\"参数便在部分达到了\",{\"1\":{\"823\":1}}],[\"参数模型是首个公开的千亿级开源模型\",{\"1\":{\"823\":1}}],[\"参数的\",{\"1\":{\"822\":2}}],[\"参数的语言模型\",{\"1\":{\"822\":1}}],[\"参数的变化趋势\",{\"1\":{\"204\":1}}],[\"参数管理与清理机制\",{\"1\":{\"819\":1}}],[\"参数调优\",{\"1\":{\"650\":1}}],[\"参数调整正类和负类的权重\",{\"1\":{\"592\":1}}],[\"参数从117m到1\",{\"1\":{\"640\":1}}],[\"参数足够多\",{\"1\":{\"620\":1}}],[\"参数选择\",{\"1\":{\"589\":1}}],[\"参数用于平衡正负类别的权重\",{\"1\":{\"589\":1}}],[\"参数用于抑制容易分类的样本\",{\"1\":{\"589\":1}}],[\"参数复用与模块化\",{\"1\":{\"500\":1}}],[\"参数个数必须和\",{\"1\":{\"471\":1,\"472\":1}}],[\"参数说明\",{\"1\":{\"440\":1,\"482\":1,\"926\":1}}],[\"参数与训练集的归一化参数相同\",{\"1\":{\"425\":1}}],[\"参数初始化\",{\"1\":{\"380\":1}}],[\"参数效率高\",{\"1\":{\"346\":1}}],[\"参数量会很大\",{\"1\":{\"710\":1}}],[\"参数量会非常大\",{\"1\":{\"710\":1}}],[\"参数量远超\",{\"1\":{\"640\":1}}],[\"参数量降低到\",{\"1\":{\"609\":1}}],[\"参数量更少\",{\"1\":{\"500\":1}}],[\"参数量极大\",{\"1\":{\"500\":1}}],[\"参数量\",{\"1\":{\"306\":1}}],[\"参数量是qformer的42倍\",{\"1\":{\"304\":1}}],[\"参数量达到60亿\",{\"1\":{\"304\":1}}],[\"参数量急剧膨胀\",{\"1\":{\"125\":1}}],[\"参数平衡的视觉与语言组件\",{\"1\":{\"296\":1}}],[\"参数形式\",{\"1\":{\"293\":1}}],[\"参数规模庞大\",{\"1\":{\"824\":1}}],[\"参数规模从7b到65b不等\",{\"1\":{\"665\":1}}],[\"参数规模不匹配\",{\"1\":{\"296\":1}}],[\"参数规模\",{\"1\":{\"224\":1,\"323\":1}}],[\"参数更新\",{\"1\":{\"205\":1,\"265\":1}}],[\"参数更新等\",{\"1\":{\"105\":1}}],[\"参数共享\",{\"1\":{\"157\":1}}],[\"参数依赖\",{\"1\":{\"134\":1}}],[\"参数来标记每个batch中点云的边界范围\",{\"1\":{\"119\":1}}],[\"参数\",{\"0\":{\"261\":1},\"1\":{\"83\":1,\"102\":1,\"107\":1,\"137\":1,\"145\":2,\"146\":1,\"213\":3,\"293\":1,\"346\":1,\"380\":2,\"382\":1,\"461\":1,\"485\":1,\"503\":1,\"586\":2,\"587\":2,\"588\":2,\"589\":3,\"590\":2,\"592\":3,\"699\":1,\"710\":1,\"823\":1,\"865\":2}}],[\"参考decoderlayer\",{\"1\":{\"745\":1}}],[\"参考上文\",{\"1\":{\"669\":1}}],[\"参考上文的\",{\"1\":{\"92\":1}}],[\"参考rabe\",{\"1\":{\"667\":1}}],[\"参考群体\",{\"1\":{\"658\":1}}],[\"参考点解码器\",{\"0\":{\"100\":1},\"1\":{\"94\":1}}],[\"参考\",{\"0\":{\"837\":1},\"1\":{\"17\":1,\"46\":1,\"176\":1,\"680\":3}}],[\"点数\",{\"1\":{\"589\":1}}],[\"点\",{\"1\":{\"572\":2}}],[\"点表示\",{\"1\":{\"569\":1}}],[\"点上取值\",{\"1\":{\"502\":1}}],[\"点积才能得到一个有意义的相似度分数\",{\"1\":{\"709\":1}}],[\"点积\",{\"1\":{\"475\":1,\"709\":1,\"900\":1}}],[\"点积的缩放因子\",{\"1\":{\"380\":1}}],[\"点之间的相互作用\",{\"1\":{\"150\":1}}],[\"点之间存在相互作用\",{\"1\":{\"149\":1}}],[\"点与点之间有空间关系\",{\"1\":{\"149\":1}}],[\"点额外特征\",{\"1\":{\"137\":1}}],[\"点变换模块类型\",{\"1\":{\"123\":1}}],[\"点的数量\",{\"1\":{\"156\":1}}],[\"点的特征数据\",{\"1\":{\"137\":1}}],[\"点的特征\",{\"1\":{\"119\":1}}],[\"点的坐标\",{\"1\":{\"119\":1}}],[\"点的坐标天然可用作位置编码\",{\"1\":{\"114\":1}}],[\"点级变换和池化组成\",{\"1\":{\"116\":1}}],[\"点级别标注\",{\"1\":{\"92\":1}}],[\"点级别\",{\"1\":{\"83\":2}}],[\"点级别的\",{\"1\":{\"83\":1}}],[\"点型网络\",{\"1\":{\"110\":1}}],[\"点大小\",{\"1\":{\"107\":1}}],[\"点集抽象层\",{\"1\":{\"137\":1}}],[\"点集的划分必须产生跨分区的共同结构\",{\"1\":{\"131\":1}}],[\"点集划分是指如何将一个大的点云分割成更小的\",{\"1\":{\"131\":1}}],[\"点集特征集合\",{\"1\":{\"99\":1}}],[\"点集集合中每个点的特征和文本特征信息进行融合\",{\"1\":{\"94\":1}}],[\"点特征作为键\",{\"1\":{\"96\":1}}],[\"点特征\",{\"1\":{\"83\":1,\"120\":2,\"121\":2,\"123\":1}}],[\"点坐标\",{\"1\":{\"83\":1,\"120\":2,\"121\":2,\"123\":1,\"137\":1}}],[\"点云网络中\",{\"1\":{\"522\":1}}],[\"点云模型\",{\"1\":{\"157\":1}}],[\"点云是点的集合\",{\"1\":{\"149\":1}}],[\"点云是三维几何数据的一种重要表示形式\",{\"1\":{\"148\":1}}],[\"点云语义分割模型\",{\"0\":{\"146\":1}}],[\"点云语义分割\",{\"0\":{\"143\":1}}],[\"点云的姿态可能各不相同\",{\"1\":{\"152\":1}}],[\"点云的无序性\",{\"1\":{\"149\":1,\"150\":1}}],[\"点云的额外特征\",{\"1\":{\"137\":1}}],[\"点云的不规则分布给池化带来挑战\",{\"1\":{\"125\":1}}],[\"点云的跨模态点云形状补全方法\",{\"1\":{\"46\":1}}],[\"点云上采样\",{\"1\":{\"123\":1}}],[\"点云上采样过渡层\",{\"1\":{\"122\":1}}],[\"点云下采样\",{\"1\":{\"123\":1}}],[\"点云下采样过渡层\",{\"1\":{\"121\":1}}],[\"点云天然就是不规则的集合\",{\"1\":{\"113\":1}}],[\"点云与二维图像不同\",{\"1\":{\"110\":1}}],[\"点云广泛存在于自动驾驶\",{\"1\":{\"109\":1}}],[\"点云坐标数据\",{\"1\":{\"137\":2}}],[\"点云坐标\",{\"1\":{\"107\":1}}],[\"点云预处理\",{\"1\":{\"107\":1}}],[\"点云分割中最常用的指标之一\",{\"1\":{\"106\":1}}],[\"点云分支提取到的是\",{\"1\":{\"83\":1}}],[\"点云分支\",{\"1\":{\"78\":1}}],[\"点云维度\",{\"1\":{\"100\":1}}],[\"点云对象和一个自然语言问题\",{\"1\":{\"94\":1}}],[\"点云对象组成\",{\"1\":{\"40\":1}}],[\"点云id\",{\"1\":{\"92\":2}}],[\"点云来源\",{\"1\":{\"91\":1}}],[\"点云中点的顺序不影响整体形状\",{\"1\":{\"160\":1}}],[\"点云中点的数量\",{\"1\":{\"106\":1}}],[\"点云中\",{\"1\":{\"114\":1}}],[\"点云中的每个点被标注为支持一个或多个功能类型\",{\"1\":{\"86\":1}}],[\"点云中相关点\",{\"1\":{\"65\":1}}],[\"点云每个点的\",{\"1\":{\"83\":1}}],[\"点云子集数量\",{\"1\":{\"83\":1}}],[\"点云联合特征\",{\"1\":{\"83\":1}}],[\"点云里的几何部分\",{\"1\":{\"83\":1}}],[\"点云里对应的这部分结构\",{\"1\":{\"83\":1}}],[\"点云几何局部\",{\"1\":{\"83\":1}}],[\"点云功能区域索引列表\",{\"1\":{\"82\":1}}],[\"点云功能区域掩码列表\",{\"1\":{\"82\":1}}],[\"点云列表\",{\"1\":{\"82\":1}}],[\"点云配对数据的数据集piad\",{\"1\":{\"72\":1}}],[\"点云等\",{\"1\":{\"67\":1}}],[\"点云编码器\",{\"1\":{\"64\":1}}],[\"点云特征插值函数\",{\"1\":{\"122\":1}}],[\"点云特征图\",{\"1\":{\"100\":1}}],[\"点云特征与几何结构特征的融合\",{\"0\":{\"60\":1}}],[\"点云特征和几何结构特征做特征融合\",{\"1\":{\"59\":1}}],[\"点云特征\",{\"1\":{\"54\":1}}],[\"点云样本列表\",{\"1\":{\"53\":1}}],[\"点云索引下标区间\",{\"1\":{\"53\":1}}],[\"点云索引文件路径\",{\"1\":{\"53\":1}}],[\"点云数据归一化处理\",{\"1\":{\"107\":1}}],[\"点云数据组织形式\",{\"1\":{\"92\":1}}],[\"点云数据\",{\"1\":{\"64\":1,\"92\":1,\"107\":1,\"146\":1}}],[\"点云数量是前者的五倍\",{\"1\":{\"41\":1}}],[\"点云数\",{\"1\":{\"41\":1}}],[\"点云部分主要来自以下开源数据源\",{\"1\":{\"41\":1}}],[\"点云关联\",{\"1\":{\"31\":1}}],[\"点云转化为更具语义信息的\",{\"1\":{\"26\":1}}],[\"点云本身稀疏\",{\"1\":{\"26\":1}}],[\"点云和\",{\"1\":{\"22\":1}}],[\"点云和指令文本中预测物体各点的可供性分数\",{\"1\":{\"21\":1}}],[\"点云\",{\"0\":{\"5\":1,\"8\":1,\"10\":1,\"159\":1},\"1\":{\"19\":1,\"53\":2,\"91\":1,\"159\":2}}],[\"再把采样出来的离散向量对应的嵌入输入进解码器\",{\"1\":{\"961\":1}}],[\"再对距离数组取一个\",{\"1\":{\"958\":1}}],[\"再对每个选项做分类打分\",{\"1\":{\"737\":1}}],[\"再利用vq\",{\"1\":{\"956\":1}}],[\"再让解码器根据这个向量来完成随机图片生成了\",{\"1\":{\"956\":1}}],[\"再被解码回另一幅长得差不多的图像\",{\"1\":{\"956\":1}}],[\"再介绍vq\",{\"1\":{\"955\":1}}],[\"再进一步映射为图像\",{\"1\":{\"944\":1}}],[\"再进行归一化和标准化处理\",{\"1\":{\"425\":2}}],[\"再进行视觉\",{\"1\":{\"377\":1}}],[\"再进行集成\",{\"1\":{\"183\":1}}],[\"再进行多视角融合\",{\"1\":{\"110\":1}}],[\"再生成对应像素\",{\"1\":{\"943\":1}}],[\"再生成若干张局部\",{\"1\":{\"293\":1}}],[\"再设法优化这个下界\",{\"1\":{\"925\":1}}],[\"再每次使用\",{\"1\":{\"923\":1}}],[\"再采样一批噪声\",{\"1\":{\"918\":1}}],[\"再测量\",{\"1\":{\"915\":1}}],[\"再如\",{\"1\":{\"846\":1}}],[\"再针对性进行优化即可\",{\"1\":{\"836\":1}}],[\"再向量化存储到数据库中\",{\"1\":{\"836\":1}}],[\"再求梯度\",{\"1\":{\"816\":1}}],[\"再例如\",{\"1\":{\"779\":1}}],[\"再加偏置\",{\"1\":{\"710\":1}}],[\"再广播到\",{\"1\":{\"710\":1}}],[\"再按照80\",{\"1\":{\"698\":1}}],[\"再到基于自注意力机制的transformer架构\",{\"1\":{\"639\":1}}],[\"再使用\",{\"1\":{\"633\":2}}],[\"再没有额外的成本\",{\"1\":{\"606\":1}}],[\"再不行尝试源码编译安装\",{\"1\":{\"557\":1}}],[\"再将整个group按照\",{\"1\":{\"547\":1}}],[\"再将融合信息返回点空间\",{\"1\":{\"99\":1}}],[\"再来看一个多维张量的例子\",{\"1\":{\"542\":1}}],[\"再来回顾我们的卷积层计算公式\",{\"1\":{\"426\":1}}],[\"再改变视图\",{\"1\":{\"492\":1}}],[\"再\",{\"1\":{\"469\":1,\"963\":1}}],[\"再和真实标签做交叉熵损失\",{\"1\":{\"431\":1}}],[\"再和缓存的key\",{\"1\":{\"420\":1}}],[\"再乘以缩放因子scale\",{\"1\":{\"430\":1}}],[\"再映射为\",{\"1\":{\"426\":1}}],[\"再从中心位置裁剪成224x224\",{\"1\":{\"425\":1}}],[\"再计算协方差矩阵\",{\"1\":{\"574\":1}}],[\"再计算\",{\"1\":{\"386\":1,\"946\":1}}],[\"再经过softmax与采样\",{\"1\":{\"958\":1}}],[\"再经过两次\",{\"1\":{\"923\":1}}],[\"再经过非线性变换后\",{\"1\":{\"699\":1}}],[\"再经过e11这个编码器\",{\"1\":{\"353\":1}}],[\"再经过跨模态注意力层\",{\"1\":{\"274\":1}}],[\"再查看在不同数据集上的表现结果\",{\"1\":{\"352\":1}}],[\"再次扫描训练集\",{\"1\":{\"964\":1}}],[\"再次提取被掩码位置的表示\",{\"1\":{\"699\":1}}],[\"再次应用\",{\"1\":{\"429\":1}}],[\"再次遍历后半部分\",{\"1\":{\"385\":1}}],[\"再次用原始的条件\",{\"1\":{\"894\":1}}],[\"再次用\",{\"1\":{\"343\":1}}],[\"再次进行遮挡预测\",{\"1\":{\"214\":1}}],[\"再微调\",{\"1\":{\"317\":1}}],[\"再结合\",{\"1\":{\"272\":1}}],[\"再结合原始点云逐点特征\",{\"1\":{\"83\":1}}],[\"再多模态流程\",{\"1\":{\"269\":1}}],[\"再在具体任务数据上进行监督微调\",{\"1\":{\"650\":1}}],[\"再在其中添加一个分隔符得到\",{\"1\":{\"631\":1}}],[\"再在其输出上做对比池化\",{\"1\":{\"278\":1}}],[\"再在高质量数据上生成学习\",{\"1\":{\"296\":1}}],[\"再在噪声图文对数据集上用对比损失微调\",{\"1\":{\"269\":1}}],[\"再在三维网格上进行卷积\",{\"1\":{\"110\":1}}],[\"再重建图像\",{\"1\":{\"235\":1}}],[\"再通过跨模态注意力实现深度融合\",{\"1\":{\"195\":1}}],[\"再通过\",{\"1\":{\"185\":1,\"201\":1,\"420\":1,\"589\":1}}],[\"再通过第一个卷积层提取初始特征\",{\"1\":{\"154\":1}}],[\"再由外层\",{\"1\":{\"458\":1}}],[\"再由\",{\"1\":{\"177\":1}}],[\"再与每个点的局部特征拼接\",{\"1\":{\"156\":1}}],[\"再用vq\",{\"1\":{\"956\":1}}],[\"再用若干次\",{\"1\":{\"923\":2}}],[\"再用对比损失在共享的潜在空间对齐\",{\"1\":{\"268\":1}}],[\"再用多模态编码器融合\",{\"1\":{\"194\":1}}],[\"再用跨模态注意力融合\",{\"1\":{\"194\":1}}],[\"再用mlp提提神\",{\"1\":{\"145\":1}}],[\"再用\",{\"1\":{\"137\":1,\"317\":1,\"885\":1}}],[\"再决定要使用什么文本查询\",{\"1\":{\"107\":1}}],[\"再上采样统一分辨率\",{\"1\":{\"23\":1}}],[\"送入模型中\",{\"1\":{\"694\":1}}],[\"送入llm时\",{\"1\":{\"421\":1}}],[\"送入\",{\"1\":{\"83\":1,\"266\":1,\"293\":1,\"694\":1,\"893\":1,\"898\":1}}],[\"哪个参数值最合理\",{\"1\":{\"903\":1}}],[\"哪怕某个维度的数据本来波动很大\",{\"1\":{\"576\":1}}],[\"哪些\",{\"1\":{\"846\":1}}],[\"哪些位置有效\",{\"1\":{\"385\":1}}],[\"哪些图片是不相似的\",{\"1\":{\"349\":1}}],[\"哪些区域应该被激活\",{\"1\":{\"83\":1}}],[\"哪里能按下按钮\",{\"1\":{\"19\":1}}],[\"哪里能抓住把手\",{\"1\":{\"19\":1}}],[\"告诉\",{\"1\":{\"83\":1}}],[\"↔\",{\"1\":{\"83\":1,\"119\":1}}],[\"显然\",{\"1\":{\"408\":1,\"923\":1}}],[\"显存并训练模型\",{\"1\":{\"614\":1}}],[\"显存15\",{\"1\":{\"291\":1}}],[\"显存9\",{\"1\":{\"291\":1}}],[\"显存\",{\"1\":{\"236\":1}}],[\"显示原图和重建图像\",{\"1\":{\"935\":1}}],[\"显示增大批次规模可提升模型困惑度和下游任务准确率\",{\"1\":{\"678\":1}}],[\"显示动态掩码在\",{\"1\":{\"678\":1}}],[\"显示模型在强化学习过程中存在轻微性能损失\",{\"1\":{\"656\":1}}],[\"显示了\",{\"1\":{\"656\":1}}],[\"显示了在极小显存条件下训练大模型的潜力\",{\"1\":{\"292\":1}}],[\"显示其对\",{\"1\":{\"648\":1}}],[\"显示gpt\",{\"1\":{\"648\":1}}],[\"显示在一些语义比较任务上仍存在明显短板\",{\"1\":{\"648\":1}}],[\"显示无监督学习的当前边界\",{\"1\":{\"641\":1}}],[\"显示它关注哪些其他\",{\"1\":{\"243\":1}}],[\"显示\",{\"1\":{\"179\":1,\"182\":1,\"376\":1,\"656\":1,\"657\":4}}],[\"显示出这种人类反馈驱动的微调策略极具潜力\",{\"1\":{\"654\":1}}],[\"显示出数据集对交互多样性和类别多样性的全面覆盖\",{\"1\":{\"41\":1}}],[\"显示出强大的适应性和鲁棒性\",{\"1\":{\"19\":1}}],[\"显著高于bert\",{\"1\":{\"685\":1}}],[\"显著高于palm\",{\"1\":{\"668\":1}}],[\"显著高于教师模型\",{\"1\":{\"215\":1}}],[\"显著比\",{\"1\":{\"657\":1}}],[\"显著减少数据需求\",{\"1\":{\"647\":1}}],[\"显著抑制简单样本\",{\"1\":{\"589\":1}}],[\"显著超越现有方法\",{\"1\":{\"309\":1}}],[\"显著超越之前的自监督方法\",{\"1\":{\"280\":1}}],[\"显著优于\",{\"1\":{\"308\":1}}],[\"显著优于上述方法\",{\"1\":{\"289\":1}}],[\"显著优于现有方法\",{\"1\":{\"268\":1}}],[\"显著提升难样本的分类性能\",{\"1\":{\"589\":1}}],[\"显著提升表现\",{\"1\":{\"343\":1}}],[\"显著提升性能\",{\"1\":{\"336\":1}}],[\"显著提升ocr和中文任务性能\",{\"1\":{\"323\":1}}],[\"显著提升下游任务性能\",{\"1\":{\"242\":1}}],[\"显著提升模型性能\",{\"1\":{\"177\":1,\"368\":1}}],[\"显著提升了性能\",{\"1\":{\"682\":1}}],[\"显著提升了模型性能\",{\"1\":{\"376\":1,\"681\":1}}],[\"显著提升了模型在\",{\"1\":{\"322\":1}}],[\"显著提升了零样本和少样本泛化能力\",{\"1\":{\"339\":1}}],[\"显著提升了开源模型在ocr\",{\"1\":{\"332\":1}}],[\"显著提升了视觉\",{\"1\":{\"312\":1}}],[\"显著提升了视觉任务的性能\",{\"1\":{\"298\":1}}],[\"显著提升了\",{\"1\":{\"217\":1,\"679\":1}}],[\"显著提升了分割性能\",{\"1\":{\"150\":1}}],[\"显著提升了所有指标\",{\"1\":{\"99\":1}}],[\"显著扩展了训练数据的规模和质量\",{\"1\":{\"174\":1}}],[\"显式的语义分割信息\",{\"1\":{\"280\":1}}],[\"显式鼓励模型生成全局图像表示\",{\"1\":{\"217\":1}}],[\"显式区域挖掘\",{\"1\":{\"83\":1}}],[\"显式地学习分类器无法很好扩展\",{\"1\":{\"282\":1}}],[\"显式地\",{\"1\":{\"83\":1}}],[\"自回归生成\",{\"1\":{\"898\":1}}],[\"自回归建模\",{\"1\":{\"898\":1}}],[\"自回归模型\",{\"1\":{\"892\":1,\"894\":1,\"925\":1}}],[\"自回归语言建模任务\",{\"1\":{\"420\":1}}],[\"自回归语言建模\",{\"0\":{\"403\":1},\"1\":{\"403\":1}}],[\"自纠错机制\",{\"1\":{\"823\":1}}],[\"自发布以来就引发了人工智能社区的兴奋\",{\"1\":{\"823\":1}}],[\"自己写答案\",{\"1\":{\"735\":1}}],[\"自己和自己的值其实是占大头的\",{\"1\":{\"694\":1}}],[\"自己的部分\",{\"1\":{\"386\":1}}],[\"自一致性技术\",{\"0\":{\"621\":1}}],[\"自适应基函数\",{\"1\":{\"500\":1}}],[\"自适应生成的非线性函数空间\",{\"1\":{\"500\":1}}],[\"自适应融合模块\",{\"0\":{\"95\":1,\"99\":1},\"1\":{\"94\":1}}],[\"自定义工具调用\",{\"1\":{\"823\":1}}],[\"自定义指令与记忆功能\",{\"1\":{\"823\":1}}],[\"自定义打印格式\",{\"1\":{\"808\":1}}],[\"自定义的批量数据处理函数\",{\"1\":{\"424\":1}}],[\"自定义数据集\",{\"1\":{\"424\":1}}],[\"自定义一个mydataset类来封装我们加载得到的数据集\",{\"1\":{\"424\":1}}],[\"自定义模型的行为\",{\"1\":{\"381\":1}}],[\"自谷歌提出\",{\"1\":{\"405\":1}}],[\"自身就能学到基本的跨模态检索能力\",{\"1\":{\"385\":1}}],[\"自家的数据集上同样效果良好\",{\"1\":{\"354\":1}}],[\"自训练与知识蒸馏本质上是相关的\",{\"1\":{\"283\":1}}],[\"自训练\",{\"1\":{\"283\":1}}],[\"自训练方法\",{\"1\":{\"194\":1}}],[\"自蒸馏\",{\"1\":{\"282\":1}}],[\"自蒸馏也取得了不错效果\",{\"1\":{\"168\":1}}],[\"自动去掉\",{\"1\":{\"898\":1}}],[\"自动反向传播与框架基础能力提升\",{\"1\":{\"798\":1}}],[\"自动反向传播与计算图进阶\",{\"0\":{\"798\":1}}],[\"自动反向传播的实现\",{\"0\":{\"784\":1}}],[\"自动设置梯度\",{\"0\":{\"791\":1}}],[\"自动跳过\",{\"1\":{\"735\":1}}],[\"自动广播到\",{\"1\":{\"710\":1}}],[\"自动打分\",{\"1\":{\"657\":1}}],[\"自动安装\",{\"1\":{\"557\":1}}],[\"自动寻找最优卷积算法\",{\"1\":{\"521\":1}}],[\"自动计算权重\",{\"1\":{\"514\":1}}],[\"自动按维度扩展\",{\"1\":{\"472\":1}}],[\"自动生成ai使用指南\",{\"1\":{\"669\":1}}],[\"自动生成\",{\"1\":{\"342\":1}}],[\"自动将实例分组\",{\"1\":{\"282\":1}}],[\"自动执行dot文件转换并显示图像\",{\"1\":{\"815\":1}}],[\"自动执行反向传播\",{\"1\":{\"784\":1}}],[\"自动执行\",{\"1\":{\"261\":1}}],[\"自然对数\",{\"1\":{\"906\":1}}],[\"自然代码表达\",{\"1\":{\"811\":1}}],[\"自然演示的机会\",{\"1\":{\"640\":1}}],[\"自然指数\",{\"1\":{\"589\":1}}],[\"自然会存在一些重复性操作\",{\"1\":{\"382\":1}}],[\"自然地成为任务适配器\",{\"1\":{\"272\":1}}],[\"自然语言建模的场景\",{\"1\":{\"696\":1}}],[\"自然语言推理\",{\"1\":{\"694\":1}}],[\"自然语言推断nli\",{\"1\":{\"626\":1}}],[\"自然语言处理\",{\"1\":{\"646\":1}}],[\"自然语言理解包含了广泛的多样性任务\",{\"1\":{\"625\":1}}],[\"自然语言监督\",{\"0\":{\"271\":1}}],[\"自然语言指令可作为隐式任务描述\",{\"1\":{\"640\":1}}],[\"自然语言指令\",{\"1\":{\"64\":1}}],[\"自然描述\",{\"1\":{\"268\":1}}],[\"自研的\",{\"1\":{\"254\":1}}],[\"自监督方法的优势在于不再需要标注数据\",{\"1\":{\"413\":1}}],[\"自监督的方式进行学习\",{\"1\":{\"354\":1}}],[\"自监督目标函数\",{\"1\":{\"283\":1}}],[\"自监督\",{\"1\":{\"280\":2}}],[\"自监督学习其实是一种特殊形式的无监督学习\",{\"1\":{\"355\":1}}],[\"自监督学习的方法可以从两个方面来进行优化或创新\",{\"1\":{\"355\":1}}],[\"自监督学习的常见评估方式有两类\",{\"1\":{\"286\":1}}],[\"自监督学习主要经历了以下几类方法\",{\"1\":{\"282\":1}}],[\"自监督学习是否能为\",{\"1\":{\"280\":1}}],[\"自监督学习\",{\"1\":{\"280\":1}}],[\"自监督视觉预训练逐渐兴起\",{\"1\":{\"269\":1}}],[\"自监督视觉预训练方法可以大致分为三类\",{\"1\":{\"245\":1}}],[\"自监督视觉预训练\",{\"0\":{\"245\":1}}],[\"自监督预训练成为利用大规模无标注图像数据的关键方法\",{\"1\":{\"228\":1}}],[\"自监督提供监督信号\",{\"1\":{\"214\":1}}],[\"自信看源码进行学习\",{\"1\":{\"190\":1}}],[\"自注意力子层\",{\"1\":{\"746\":2,\"749\":2}}],[\"自注意力运算\",{\"1\":{\"420\":1,\"663\":1}}],[\"自注意力和交叉注意力流程统一化\",{\"1\":{\"420\":1}}],[\"自注意力掩码策略\",{\"1\":{\"418\":1,\"419\":1,\"420\":1}}],[\"自注意力捕捉文本内部的依赖\",{\"1\":{\"399\":1}}],[\"自注意力图的分析\",{\"0\":{\"243\":1}}],[\"自注意力模块在不同模态之间共享\",{\"1\":{\"224\":1}}],[\"自注意力模块\",{\"1\":{\"222\":1}}],[\"自注意力层通过\",{\"1\":{\"286\":1}}],[\"自注意力层\",{\"1\":{\"120\":1}}],[\"自注意力分为两种\",{\"1\":{\"112\":1}}],[\"自注意力本质上是集合操作\",{\"1\":{\"110\":1}}],[\"自注意力机制是\",{\"1\":{\"741\":1}}],[\"自注意力机制能够捕捉图像中任意两个\",{\"1\":{\"427\":1}}],[\"自注意力机制\",{\"1\":{\"100\":1,\"523\":1}}],[\"自注意力\",{\"1\":{\"83\":1,\"120\":1,\"399\":1,\"420\":2}}],[\"自注意力完成内部信息建模\",{\"1\":{\"83\":1}}],[\"消失了\",{\"1\":{\"946\":1}}],[\"消融研究\",{\"1\":{\"635\":1}}],[\"消融实验\",{\"0\":{\"117\":1,\"242\":1,\"275\":1,\"287\":1,\"344\":1}}],[\"消融项分析\",{\"1\":{\"48\":1}}],[\"消除量化误差\",{\"1\":{\"502\":1}}],[\"消除尺度差异\",{\"1\":{\"92\":1}}],[\"消除模态差异\",{\"1\":{\"83\":1}}],[\"人们更关心的是能否基于已有数据库\",{\"1\":{\"942\":1}}],[\"人是否患病\",{\"1\":{\"850\":1}}],[\"人群中\",{\"1\":{\"850\":1}}],[\"人力\",{\"1\":{\"690\":1}}],[\"人\",{\"1\":{\"574\":1}}],[\"人体关键点检测上都超越了有监督的预训练模型\",{\"1\":{\"348\":1}}],[\"人工标注\",{\"1\":{\"176\":1}}],[\"人手接触杯子的边缘\",{\"1\":{\"83\":1}}],[\"人类普遍价值\",{\"1\":{\"658\":1}}],[\"人类偏好引导型\",{\"1\":{\"658\":1}}],[\"人类代表\",{\"1\":{\"658\":1}}],[\"人类评估显示\",{\"1\":{\"884\":1}}],[\"人类评估\",{\"1\":{\"657\":1}}],[\"人类评估者显著偏好\",{\"1\":{\"657\":1}}],[\"人类评估输出的偏好\",{\"1\":{\"656\":1}}],[\"人类数据采集\",{\"1\":{\"656\":1}}],[\"人类价值对齐\",{\"1\":{\"658\":1}}],[\"人类价值\",{\"1\":{\"654\":1}}],[\"人类反馈微调是一种有效的模型对齐手段\",{\"1\":{\"653\":1}}],[\"人类反馈强化学习\",{\"1\":{\"339\":1}}],[\"人类可通过少量示例或自然语言指令快速适应新任务\",{\"1\":{\"646\":1}}],[\"人类仅需少量示例或简单指令即可完成新任务\",{\"1\":{\"646\":1}}],[\"人类通过同时分析物体的\",{\"1\":{\"56\":1}}],[\"人类通过多步推理和类比思维解决复杂任务\",{\"1\":{\"30\":1}}],[\"人类交互文本\",{\"1\":{\"53\":1}}],[\"人类交互文本数据文件路径\",{\"1\":{\"53\":1}}],[\"人类会\",{\"1\":{\"30\":1}}],[\"人类认知启发\",{\"1\":{\"30\":1}}],[\"让正向传播和反向传播按照不同的方式计算\",{\"1\":{\"961\":1}}],[\"让正样本的key和query距离近\",{\"1\":{\"353\":1}}],[\"让它学习怎么生成\",{\"1\":{\"956\":1}}],[\"让它比较\",{\"1\":{\"917\":1}}],[\"让pixelcnn学习生成\",{\"1\":{\"956\":1}}],[\"让随机采样出的向量也能通过解码器变成图片\",{\"1\":{\"956\":1}}],[\"让向量符合标准正态分布的原因是方便随机采样\",{\"1\":{\"956\":1}}],[\"让看起来真实的图像具有较高概率\",{\"1\":{\"942\":1}}],[\"让图像\",{\"1\":{\"893\":1}}],[\"让开发者可以专注于应用程序的开发\",{\"1\":{\"834\":1}}],[\"让开发者能够通过简单的命令来管理整个应用程序的生命周期\",{\"1\":{\"834\":1}}],[\"让我们来总结一下\",{\"1\":{\"956\":1}}],[\"让我们来整理一下vq\",{\"1\":{\"956\":1}}],[\"让我们从最早的自编码器\",{\"1\":{\"956\":1}}],[\"让我们回到之前生成手写数字的例子\",{\"1\":{\"952\":1}}],[\"让我们的应用能够上线成为产品\",{\"1\":{\"836\":1}}],[\"让我们更轻松地找到所需的信息\",{\"1\":{\"827\":1}}],[\"让我们逐步构建起tinypytorch的基础功能\",{\"1\":{\"753\":1}}],[\"让大语言模型真正火爆的契机\",{\"1\":{\"827\":1}}],[\"让大模型生成的结果\",{\"1\":{\"602\":1}}],[\"让用户可根据具体需求选择最适合的模型类型\",{\"1\":{\"823\":1}}],[\"让返回值始终封装为元组\",{\"1\":{\"800\":1}}],[\"让函数更易用\",{\"0\":{\"789\":1}}],[\"让其\",{\"1\":{\"694\":1}}],[\"让其尽可能收集全局信息\",{\"1\":{\"214\":1}}],[\"让llm照着例子进行推理\",{\"1\":{\"619\":1}}],[\"让他去执行就好了\",{\"1\":{\"616\":1}}],[\"让难分类样本获得更大的\",{\"1\":{\"589\":1}}],[\"让你显式指定维度之间怎么相乘\",{\"1\":{\"475\":1}}],[\"让被装饰函数看起来仍然像原来的函数\",{\"1\":{\"454\":1}}],[\"让q\",{\"1\":{\"420\":1}}],[\"让query在字典中与自己匹配的正样本更近\",{\"1\":{\"353\":1}}],[\"让文本动态关注图像区域\",{\"1\":{\"402\":1}}],[\"让文本\",{\"1\":{\"399\":1}}],[\"让固定模型适应新任务\",{\"1\":{\"346\":1}}],[\"让视觉编码器提取的图像特征与语言模型的词嵌入空间对齐\",{\"1\":{\"340\":1}}],[\"让人类标注者对这些回答进行排序\",{\"1\":{\"339\":1}}],[\"让两个目标可以在计算上高效结合\",{\"1\":{\"268\":1}}],[\"让码本延迟到首次向量量化阶段\",{\"1\":{\"213\":1}}],[\"让变换矩阵从一个恒等变换开始学习\",{\"1\":{\"152\":1}}],[\"让网络从一个小扰动开始学习\",{\"1\":{\"152\":1}}],[\"让网络明确知道哪些区域真正具备\",{\"1\":{\"83\":1}}],[\"让模型知道\",{\"1\":{\"895\":1}}],[\"让模型也能学到\",{\"1\":{\"893\":1}}],[\"让模型可以更好地捕捉和理解语言中的复杂关系\",{\"1\":{\"822\":1}}],[\"让模型自己学会区分不同的句子\",{\"1\":{\"716\":1}}],[\"让模型得以判断上下句的起止位置\",{\"1\":{\"692\":1}}],[\"让模型理解图像和文本之间的语义关系\",{\"1\":{\"340\":1}}],[\"让模型生成多个不同的回答\",{\"1\":{\"339\":1}}],[\"让模型学习更细粒度\",{\"1\":{\"293\":1}}],[\"让模型更容易训练和泛化\",{\"1\":{\"152\":1}}],[\"让模型既关注局部细节\",{\"1\":{\"150\":1}}],[\"让模型同时关注逐点分类精度和整体区域匹配\",{\"1\":{\"102\":1}}],[\"让每个离散编码值对应一个嵌入\",{\"1\":{\"961\":1}}],[\"让每个像素都忽略后续像素的信息的方法就不是那么显然了\",{\"1\":{\"921\":1}}],[\"让每个类别在训练过程中被抽到的机会接近均衡\",{\"1\":{\"518\":1}}],[\"让每个点都能看到上下文信息\",{\"1\":{\"154\":1,\"156\":1}}],[\"让每个\",{\"1\":{\"100\":1}}],[\"让每个语言\",{\"1\":{\"96\":1}}],[\"让\",{\"1\":{\"83\":1,\"210\":1,\"353\":1,\"945\":1,\"947\":1}}],[\"局部性\",{\"1\":{\"500\":1}}],[\"局部逼近\",{\"1\":{\"500\":1}}],[\"局部变量会在函数执行完后被释放\",{\"1\":{\"448\":1}}],[\"局部作用域\",{\"1\":{\"444\":1}}],[\"局部困难负样本挖掘\",{\"1\":{\"376\":1}}],[\"局部裁剪数量\",{\"1\":{\"293\":1}}],[\"局部裁剪\",{\"1\":{\"293\":1}}],[\"局部分辨率为\",{\"1\":{\"285\":1}}],[\"局部\",{\"1\":{\"285\":1}}],[\"局部视角2\",{\"1\":{\"293\":1}}],[\"局部视角1\",{\"1\":{\"293\":1}}],[\"局部视角\",{\"1\":{\"285\":1}}],[\"局部建模能力弱\",{\"1\":{\"157\":1}}],[\"局部特征\",{\"1\":{\"215\":1}}],[\"局部特征实现上下文感知\",{\"1\":{\"157\":1}}],[\"局部特征编码\",{\"1\":{\"137\":1}}],[\"局部特征学习器\",{\"1\":{\"131\":1}}],[\"局部区域\",{\"1\":{\"137\":1}}],[\"局部区域中的每个点将相对于形心所在位置进行调整\",{\"1\":{\"136\":1}}],[\"局部区域中的点转换成相对于形心的局部坐标系\",{\"1\":{\"136\":1}}],[\"局部区域级别\",{\"1\":{\"83\":1}}],[\"局部感受野\",{\"1\":{\"121\":1}}],[\"局部坐标系转换\",{\"1\":{\"136\":1}}],[\"局部坐标系\",{\"1\":{\"119\":1}}],[\"局部邻域处理\",{\"1\":{\"119\":1}}],[\"局部邻域\",{\"1\":{\"113\":1}}],[\"局部邻域应用方式及位置编码方法的设计\",{\"1\":{\"109\":1}}],[\"局部自注意力\",{\"1\":{\"83\":1}}],[\"局限性与挑战\",{\"1\":{\"669\":1}}],[\"局限性与未来工作\",{\"1\":{\"50\":1}}],[\"局限性\",{\"0\":{\"27\":1,\"649\":1},\"1\":{\"641\":1,\"658\":1}}],[\"根据阈值计算\",{\"1\":{\"896\":1}}],[\"根据文本生成图像\",{\"1\":{\"893\":1}}],[\"根据文本和图像\",{\"1\":{\"885\":1}}],[\"根据文字搜索图片\",{\"1\":{\"411\":1,\"412\":1}}],[\"根据观测数据对某一感兴趣的未知量的分布进行更新的过程\",{\"1\":{\"877\":1}}],[\"根据观察数据更新置信度的方法\",{\"1\":{\"877\":1}}],[\"根据条件概率的定义\",{\"1\":{\"850\":1,\"851\":1}}],[\"根据这个定义\",{\"1\":{\"849\":1}}],[\"根据导数公式\",{\"1\":{\"809\":1,\"811\":1}}],[\"根据decoder的隐状态输出一个词\",{\"1\":{\"743\":1}}],[\"根据编码器的输出生成目标序列\",{\"1\":{\"741\":1}}],[\"根据你的理解\",{\"1\":{\"735\":1}}],[\"根据上下文\",{\"1\":{\"691\":1}}],[\"根据上述计算得到的和其相似度最高的分类文本索引\",{\"1\":{\"410\":1}}],[\"根据所在gptblock层级\",{\"1\":{\"663\":1}}],[\"根据频次表构建最终的词汇表\",{\"1\":{\"595\":1}}],[\"根据n轮迭代合并后的vocab来构建最终的频次表\",{\"1\":{\"595\":1}}],[\"根据特定的分割任务的需求和特点\",{\"1\":{\"593\":1}}],[\"根据预测的\",{\"1\":{\"735\":1}}],[\"根据预测误差对预测结果进行排序\",{\"1\":{\"591\":1}}],[\"根据预测值从灰色\",{\"1\":{\"107\":1}}],[\"根据正负样本比例调整\",{\"1\":{\"589\":1}}],[\"根据公式\",{\"1\":{\"546\":1}}],[\"根据相关性\",{\"1\":{\"531\":1}}],[\"根据通用近似定理\",{\"1\":{\"500\":1}}],[\"根据索引矩阵\",{\"1\":{\"700\":1}}],[\"根据索引获取数据集中的图像和对应的标签\",{\"1\":{\"424\":1}}],[\"根据索引分组特征\",{\"1\":{\"119\":1}}],[\"根据索引分组坐标\",{\"1\":{\"119\":1}}],[\"根据图像特征\",{\"1\":{\"420\":1}}],[\"根据imagenet数据集上的zero\",{\"1\":{\"413\":1}}],[\"根据任务的分类需求\",{\"1\":{\"408\":1}}],[\"根据训练\",{\"1\":{\"386\":1}}],[\"根据模态类型选择不同\",{\"1\":{\"380\":1}}],[\"根据输入图像的长宽比和分辨率\",{\"1\":{\"322\":1}}],[\"根据输入的视觉令牌重建原始图像\",{\"1\":{\"235\":1}}],[\"根据面积和长宽比计算遮挡块的高度和宽度\",{\"1\":{\"263\":1}}],[\"根据遮挡图像恢复视觉令牌\",{\"1\":{\"235\":1}}],[\"根据视觉\",{\"1\":{\"232\":1}}],[\"根据视觉词汇表将图像像素\",{\"1\":{\"232\":1}}],[\"根据注意力机制加权求和\",{\"1\":{\"160\":1}}],[\"根据距离类别\",{\"1\":{\"710\":1}}],[\"根据距离反比加权\",{\"1\":{\"145\":1}}],[\"根据距离分配权重\",{\"1\":{\"145\":1}}],[\"根据反距离加权分配权重\",{\"1\":{\"122\":1}}],[\"根据use\",{\"1\":{\"119\":1}}],[\"根据邻居点索引\",{\"1\":{\"119\":1}}],[\"根据\",{\"1\":{\"119\":1,\"208\":1,\"557\":1,\"592\":1,\"947\":1}}],[\"根据坐标构建卷积核\",{\"1\":{\"110\":1}}],[\"根据样本索引取出样本数据\",{\"1\":{\"92\":1}}],[\"根据自然语言问题找出与之相关的功能区域\",{\"1\":{\"86\":1}}],[\"根据交互主体框\",{\"1\":{\"83\":1}}],[\"根据目标物体框\",{\"1\":{\"83\":1}}],[\"联合关系\",{\"1\":{\"574\":1}}],[\"联合使用\",{\"1\":{\"343\":1}}],[\"联合\",{\"0\":{\"343\":1}}],[\"联合qllama二次编码视觉特征\",{\"1\":{\"303\":1}}],[\"联合损失效果\",{\"1\":{\"276\":1}}],[\"联合推理时\",{\"1\":{\"342\":1}}],[\"联合推理\",{\"1\":{\"269\":1}}],[\"联合优化\",{\"1\":{\"238\":1}}],[\"联合训练三种任务\",{\"1\":{\"165\":1}}],[\"联合建模\",{\"1\":{\"83\":1}}],[\"联合注意力生成对齐特征\",{\"1\":{\"78\":1}}],[\"联合区域对齐\",{\"1\":{\"78\":1}}],[\"早期的模型\",{\"1\":{\"646\":1}}],[\"早期的图文多模态\",{\"1\":{\"415\":1}}],[\"早期的视觉模型大多依赖于在大规模标注数据\",{\"1\":{\"269\":1}}],[\"早期方法如elmo\",{\"1\":{\"687\":1}}],[\"早期方法\",{\"1\":{\"269\":1,\"650\":1}}],[\"早期方法主要在图像或视频中识别交互区域\",{\"1\":{\"20\":1}}],[\"早期工作\",{\"1\":{\"75\":1}}],[\"意思是\",{\"1\":{\"885\":1}}],[\"意味着gpt\",{\"1\":{\"649\":1}}],[\"意味着从第\",{\"1\":{\"542\":2}}],[\"意味着相邻列的元素在内存中也是相邻的\",{\"1\":{\"542\":1}}],[\"意味着如果你从第\",{\"1\":{\"542\":1}}],[\"意味着张量的元素在内存中的存储顺序与其逻辑上的维度顺序不匹配\",{\"1\":{\"489\":1}}],[\"意味着按照张量的最右维度\",{\"1\":{\"489\":1}}],[\"意味着在处理图像时\",{\"1\":{\"435\":1}}],[\"意味着动量分布和\",{\"1\":{\"204\":1}}],[\"意义\",{\"1\":{\"73\":1}}],[\"意图协同推理\",{\"1\":{\"31\":1}}],[\"场景文本\",{\"1\":{\"335\":1}}],[\"场景理解中的优势\",{\"1\":{\"110\":1}}],[\"场景理解的通用骨干\",{\"1\":{\"109\":1}}],[\"场景\",{\"1\":{\"80\":1,\"102\":1,\"445\":1,\"522\":1,\"574\":1}}],[\"场景特征\",{\"1\":{\"78\":1}}],[\"场景的评估\",{\"1\":{\"73\":1}}],[\"场景交互以揭示功能\",{\"1\":{\"73\":1}}],[\"场景复杂性\",{\"1\":{\"20\":1}}],[\"盛放\",{\"1\":{\"73\":1}}],[\"握持\",{\"1\":{\"73\":1}}],[\"握柄倒水\",{\"1\":{\"30\":1}}],[\"杯子的边缘曲面\",{\"1\":{\"83\":1}}],[\"杯子\",{\"1\":{\"73\":1}}],[\"需大量\",{\"1\":{\"944\":1}}],[\"需为奇数\",{\"1\":{\"926\":1}}],[\"需在多样化的应用场景中保持高效和准确\",{\"1\":{\"828\":1}}],[\"需编写繁琐的函数调用\",{\"1\":{\"811\":1}}],[\"需包含\",{\"1\":{\"810\":1}}],[\"需处理左右操作数的除法运算\",{\"1\":{\"809\":1}}],[\"需分别实现\",{\"1\":{\"809\":1}}],[\"需通过弱引用解决\",{\"1\":{\"806\":1}}],[\"需通过交互上下文建模解决\",{\"1\":{\"73\":1}}],[\"需依赖垃圾回收机制处理\",{\"1\":{\"806\":1}}],[\"需从\",{\"1\":{\"680\":1}}],[\"需后处理\",{\"1\":{\"669\":1}}],[\"需后续治理\",{\"1\":{\"668\":1}}],[\"需约\",{\"1\":{\"658\":1}}],[\"需调参\",{\"1\":{\"589\":1}}],[\"需二值化\",{\"1\":{\"588\":1}}],[\"需正则化\",{\"1\":{\"500\":1}}],[\"需直接建模像素到类别的复杂映射\",{\"1\":{\"500\":1}}],[\"需数百个神经元拟合嵌套正弦波\",{\"1\":{\"500\":1}}],[\"需高阶泰勒展开\",{\"1\":{\"500\":1}}],[\"需训练整个模型\",{\"1\":{\"346\":1}}],[\"需搭配大规模视觉编码器\",{\"1\":{\"336\":1}}],[\"需使用领域特定语言\",{\"1\":{\"811\":1}}],[\"需使用\",{\"1\":{\"264\":1}}],[\"需先\",{\"1\":{\"106\":2}}],[\"需结合\",{\"1\":{\"106\":1,\"670\":1}}],[\"需严格的空间对应\",{\"1\":{\"76\":1}}],[\"需跨源对齐区域\",{\"1\":{\"73\":1}}],[\"需要指出的一点是\",{\"1\":{\"946\":1}}],[\"需要指定概率度量\",{\"1\":{\"846\":1}}],[\"需要非常多的样本\",{\"1\":{\"944\":1}}],[\"需要解决两个核心问题\",{\"1\":{\"944\":1}}],[\"需要可计算且关于参数\",{\"1\":{\"943\":1}}],[\"需要减去偏移\",{\"1\":{\"895\":1}}],[\"需要给出\",{\"1\":{\"878\":1}}],[\"需要进行切片\",{\"1\":{\"836\":1}}],[\"需要设计本应用所要提供的功能\",{\"1\":{\"836\":1}}],[\"需要首先构造训练集\",{\"1\":{\"835\":1}}],[\"需要额外的资源来支持检索机制和数据库的维护\",{\"1\":{\"830\":1}}],[\"需要额外存储和计算\",{\"1\":{\"385\":1}}],[\"需要谨慎\",{\"1\":{\"824\":1}}],[\"需要分别处理左右操作数\",{\"1\":{\"809\":1}}],[\"需要知道正向传播时的\",{\"1\":{\"779\":1}}],[\"需要存储\",{\"1\":{\"707\":1}}],[\"需要对vq\",{\"1\":{\"961\":1}}],[\"需要对数据结构做强假设\",{\"1\":{\"942\":1}}],[\"需要对掩码数量不足max\",{\"1\":{\"700\":1}}],[\"需要对哪个物体做归一化呢\",{\"1\":{\"131\":1}}],[\"需要准备好每一层\",{\"1\":{\"663\":1}}],[\"需要至少50个token的上下文理解\",{\"1\":{\"641\":1}}],[\"需要做些修改以便用在这些任务上\",{\"1\":{\"631\":1}}],[\"需要更长的反应时间\",{\"1\":{\"619\":1}}],[\"需要更多训练数据\",{\"1\":{\"280\":1}}],[\"需要更多的训练数据\",{\"1\":{\"228\":1}}],[\"需要特别说明\",{\"1\":{\"600\":1}}],[\"需要跨越多少个内存元素\",{\"1\":{\"544\":1}}],[\"需要移动\",{\"1\":{\"542\":1}}],[\"需要偏置来增加模型表达能力\",{\"1\":{\"522\":1}}],[\"需要偏置\",{\"1\":{\"522\":1}}],[\"需要计算权重和加权平均\",{\"1\":{\"505\":1}}],[\"需要确保它是连续的\",{\"1\":{\"492\":1}}],[\"需要确保这些划分具有一定的一致性或共同结构\",{\"1\":{\"131\":1}}],[\"需要在内存中跳过\",{\"1\":{\"489\":2}}],[\"需要在内存中跳过多少个元素\",{\"1\":{\"489\":1}}],[\"需要管理员权限\",{\"1\":{\"461\":1}}],[\"需要再将第一个添加的class\",{\"1\":{\"431\":1}}],[\"需要单独将这个token再提取出来\",{\"1\":{\"427\":1}}],[\"需要相对少的数据就可以学习一个比较好的模型\",{\"1\":{\"422\":1}}],[\"需要复制图像特征以匹配beam数量\",{\"1\":{\"421\":1}}],[\"需要数百个gpu训练数十天\",{\"1\":{\"415\":1}}],[\"需要先完成\",{\"1\":{\"386\":1}}],[\"需要先将\",{\"1\":{\"106\":1}}],[\"需要不同的缩放系数\",{\"1\":{\"380\":1}}],[\"需要大量的计算资源进行训练和推理\",{\"1\":{\"824\":1}}],[\"需要大量的数据标注\",{\"1\":{\"413\":1}}],[\"需要大量神经元构造多个\",{\"1\":{\"500\":1}}],[\"需要大量高质量指令数据\",{\"1\":{\"346\":1}}],[\"需要大量人工或机器生成的\",{\"1\":{\"346\":1}}],[\"需要说明的是\",{\"1\":{\"273\":1}}],[\"需要强调的是\",{\"1\":{\"272\":1,\"956\":1}}],[\"需要模型自回归地预测文本\",{\"1\":{\"271\":1}}],[\"需要两次\",{\"1\":{\"269\":1}}],[\"需要\",{\"1\":{\"264\":1,\"931\":1,\"944\":1}}],[\"需要被掩码的词之上\",{\"1\":{\"698\":1}}],[\"需要被\",{\"1\":{\"263\":1}}],[\"需要注意的一点是\",{\"1\":{\"712\":1}}],[\"需要注意的一点\",{\"1\":{\"356\":1}}],[\"需要注意的是\",{\"1\":{\"214\":2,\"877\":1,\"885\":1}}],[\"需要注意在对交互文本进行编码时\",{\"1\":{\"55\":1}}],[\"需要维护的参数\",{\"1\":{\"213\":1}}],[\"需要捕捉局部结构\",{\"1\":{\"149\":1}}],[\"需要加上批次的起始偏移\",{\"1\":{\"121\":1}}],[\"需要是内存连续的\",{\"1\":{\"121\":1}}],[\"需要处理坐标和特征\",{\"1\":{\"121\":1}}],[\"需要处理维度匹配\",{\"1\":{\"120\":1}}],[\"需要投影层\",{\"1\":{\"120\":1}}],[\"需要查询的最近邻数量\",{\"1\":{\"119\":1}}],[\"需要转置\",{\"1\":{\"119\":1}}],[\"需要将输入图像调整为这个固定的尺寸\",{\"1\":{\"435\":1}}],[\"需要将\",{\"1\":{\"53\":1,\"739\":1}}],[\"需要建立完善的伦理规范\",{\"1\":{\"27\":1}}],[\"坐标值\",{\"1\":{\"502\":1}}],[\"坐标时进行了两次量化\",{\"1\":{\"502\":1}}],[\"坐标信息\",{\"1\":{\"159\":1}}],[\"坐标\",{\"1\":{\"115\":1,\"146\":1,\"159\":1,\"501\":1}}],[\"坐标与热力图形式的可供性值\",{\"1\":{\"42\":1}}],[\"坐\",{\"1\":{\"73\":1}}],[\"概念\",{\"1\":{\"73\":1}}],[\"概率越小\",{\"1\":{\"906\":1}}],[\"概率模型已定\",{\"1\":{\"903\":1}}],[\"概率论的核心是\",{\"1\":{\"878\":1}}],[\"概率论基础模型\",{\"0\":{\"876\":1},\"1\":{\"876\":1}}],[\"概率论基础概念\",{\"0\":{\"844\":1},\"1\":{\"844\":1}}],[\"概率论基础知识\",{\"0\":{\"843\":1}}],[\"概率公理\",{\"0\":{\"848\":1}}],[\"概率由积分给出\",{\"1\":{\"847\":1}}],[\"概率空间\",{\"0\":{\"845\":1}}],[\"概率保持不变\",{\"1\":{\"697\":2}}],[\"概率替换为随机词\",{\"1\":{\"697\":2}}],[\"概率替换为\",{\"1\":{\"697\":2}}],[\"概率线性增长\",{\"1\":{\"380\":1}}],[\"概率进行太阳化增强\",{\"1\":{\"293\":1}}],[\"概率转为灰度图\",{\"1\":{\"293\":1}}],[\"概率执行颜色抖动\",{\"1\":{\"293\":1}}],[\"概率水平翻转\",{\"1\":{\"293\":1}}],[\"概率\",{\"1\":{\"83\":1,\"208\":1,\"261\":2,\"293\":3,\"373\":1,\"380\":8,\"892\":2,\"926\":1}}],[\"概率值\",{\"1\":{\"70\":1}}],[\"概率分布\",{\"1\":{\"64\":1,\"156\":1,\"256\":1,\"260\":1,\"847\":1}}],[\"yc+1\",{\"1\":{\"964\":1}}],[\"yc\",{\"1\":{\"964\":3}}],[\"y|x\",{\"1\":{\"899\":1}}],[\"yfcc100m\",{\"1\":{\"888\":2}}],[\"yellow\",{\"1\":{\"816\":1}}],[\"ys\",{\"1\":{\"800\":5,\"805\":5,\"807\":5}}],[\"yn\",{\"1\":{\"600\":1}}],[\"y2\",{\"1\":{\"600\":1}}],[\"y=y\",{\"1\":{\"514\":1}}],[\"y=v\",{\"1\":{\"424\":1}}],[\"y1\",{\"1\":{\"505\":2,\"600\":1,\"771\":2}}],[\"y0\",{\"1\":{\"505\":2,\"771\":2}}],[\"ylabel\",{\"1\":{\"424\":1,\"816\":1}}],[\"yl3800\",{\"1\":{\"84\":1}}],[\"yield\",{\"1\":{\"807\":1}}],[\"yi\",{\"1\":{\"330\":2}}],[\"yu\",{\"1\":{\"216\":1}}],[\"y\",{\"1\":{\"138\":2,\"159\":2,\"256\":1,\"258\":14,\"260\":1,\"463\":2,\"467\":1,\"468\":1,\"469\":2,\"472\":1,\"476\":2,\"478\":3,\"481\":4,\"486\":2,\"490\":4,\"491\":9,\"492\":3,\"504\":1,\"505\":2,\"510\":1,\"513\":10,\"514\":4,\"522\":1,\"571\":1,\"574\":1,\"589\":1,\"600\":1,\"656\":2,\"735\":1,\"762\":2,\"766\":2,\"779\":2,\"781\":2,\"787\":2,\"794\":4,\"795\":2,\"800\":2,\"803\":3,\"804\":2,\"805\":2,\"807\":10,\"808\":6,\"809\":5,\"811\":26,\"815\":24,\"816\":15,\"877\":1,\"899\":1,\"938\":2}}],[\"yanx27\",{\"1\":{\"130\":1}}],[\"yaml\",{\"1\":{\"107\":3}}],[\"yawen\",{\"1\":{\"28\":1}}],[\"you\",{\"1\":{\"94\":1}}],[\"yyvhang\",{\"1\":{\"71\":1}}],[\"做的就是这个定位任务\",{\"1\":{\"735\":1}}],[\"做点积\",{\"1\":{\"709\":3}}],[\"做减法\",{\"1\":{\"709\":1}}],[\"做分类\",{\"1\":{\"694\":1,\"735\":1}}],[\"做为最终的答案\",{\"1\":{\"621\":1}}],[\"做简单的矩阵加法即可\",{\"1\":{\"606\":1}}],[\"做了大量的简化\",{\"1\":{\"600\":1}}],[\"做了个近似\",{\"1\":{\"355\":1}}],[\"做attention\",{\"1\":{\"419\":1}}],[\"做\",{\"1\":{\"418\":1,\"892\":1}}],[\"做线性变换\",{\"1\":{\"384\":1}}],[\"做判断\",{\"1\":{\"343\":1}}],[\"做到一个近似的全部数据集的多分类\",{\"1\":{\"357\":1}}],[\"做到\",{\"1\":{\"220\":1}}],[\"做通道分组\",{\"1\":{\"119\":1}}],[\"做掩码操作\",{\"1\":{\"100\":1}}],[\"做全局池化\",{\"1\":{\"70\":1}}],[\"做回答\",{\"1\":{\"52\":1}}],[\"执行随机梯度下降\",{\"1\":{\"946\":1}}],[\"执行项目初始化\",{\"1\":{\"834\":1}}],[\"执行具体行动计划至关重要\",{\"1\":{\"833\":1}}],[\"执行掩码的词的原token列表\",{\"1\":{\"698\":2}}],[\"执行掩码的词的位置列表\",{\"1\":{\"698\":2}}],[\"执行bert的掩码策略\",{\"1\":{\"697\":1}}],[\"执行以下任务\",{\"1\":{\"656\":1}}],[\"执行微调训练\",{\"1\":{\"609\":1}}],[\"执行顺序\",{\"1\":{\"453\":1}}],[\"执行复杂的视觉推理任务\",{\"1\":{\"342\":1}}],[\"执行模型的一个训练周期\",{\"1\":{\"265\":1}}],[\"执行块状遮挡策略的核心代码实现如下\",{\"1\":{\"263\":1}}],[\"执行语言建模训练\",{\"1\":{\"187\":1}}],[\"执行一次训练\",{\"1\":{\"187\":1}}],[\"执行一个\",{\"1\":{\"187\":1}}],[\"执行多轮训练\",{\"1\":{\"187\":1}}],[\"执行最远点采样迭代\",{\"1\":{\"121\":1}}],[\"执行最远点采样算法\",{\"1\":{\"121\":1}}],[\"执行预测和可视化\",{\"1\":{\"107\":1}}],[\"执行\",{\"1\":{\"70\":1,\"100\":1,\"468\":1,\"542\":1,\"657\":1,\"815\":1}}],[\"执行交叉注意力机制\",{\"1\":{\"69\":1}}],[\"全灰\",{\"1\":{\"875\":1}}],[\"全概率公式\",{\"0\":{\"850\":1},\"1\":{\"850\":1}}],[\"全新的范式\",{\"1\":{\"826\":1}}],[\"全能\",{\"1\":{\"823\":1}}],[\"全面扩展\",{\"1\":{\"799\":1}}],[\"全连接再加上一个softmax\",{\"1\":{\"743\":1}}],[\"全连接层预测变换矩阵\",{\"1\":{\"152\":1}}],[\"全部从维基百科的\",{\"1\":{\"696\":1}}],[\"全部为\",{\"1\":{\"213\":1}}],[\"全部为1\",{\"1\":{\"206\":1}}],[\"全\",{\"1\":{\"692\":2}}],[\"全称为\",{\"1\":{\"690\":1}}],[\"全1\",{\"1\":{\"421\":1}}],[\"全是\",{\"1\":{\"385\":1}}],[\"全0\",{\"1\":{\"380\":1}}],[\"全参数调优\",{\"1\":{\"320\":1}}],[\"全量微调\",{\"1\":{\"308\":1}}],[\"全模型微调\",{\"1\":{\"268\":1,\"334\":1}}],[\"全零\",{\"1\":{\"122\":1,\"926\":1}}],[\"全局性\",{\"1\":{\"500\":1}}],[\"全局逼近\",{\"1\":{\"500\":1}}],[\"全局作用域\",{\"1\":{\"444\":1}}],[\"全局困难负样本挖掘带来了显著的性能提升\",{\"1\":{\"376\":1}}],[\"全局困难负样本挖掘\",{\"1\":{\"376\":2}}],[\"全局迭代索引\",{\"1\":{\"293\":1}}],[\"全局裁剪\",{\"1\":{\"293\":2}}],[\"全局对齐\",{\"1\":{\"285\":1}}],[\"全局视角2\",{\"1\":{\"293\":1}}],[\"全局视角1\",{\"1\":{\"293\":1}}],[\"全局视角分辨率为\",{\"1\":{\"285\":1}}],[\"全局视角\",{\"1\":{\"285\":1}}],[\"全局信息\",{\"1\":{\"156\":1}}],[\"全局信息融合机制\",{\"1\":{\"150\":1}}],[\"全局最大池化\",{\"1\":{\"152\":1}}],[\"全局质心点\",{\"1\":{\"137\":1}}],[\"全局特征表示\",{\"1\":{\"427\":1}}],[\"全局特征不能很好地反映每个点的上下文\",{\"1\":{\"157\":1}}],[\"全局特征不能直接用于分割\",{\"1\":{\"156\":1}}],[\"全局特征只有一份\",{\"1\":{\"156\":1}}],[\"全局特征\",{\"1\":{\"154\":1}}],[\"全局特征开关\",{\"1\":{\"154\":1}}],[\"全局特征都不一样了\",{\"1\":{\"131\":1}}],[\"全局特征变换\",{\"1\":{\"122\":1}}],[\"全局特征增强\",{\"1\":{\"122\":2}}],[\"全局特征向量\",{\"1\":{\"116\":1}}],[\"全局\",{\"1\":{\"83\":3}}],[\"全局池化\",{\"1\":{\"83\":2}}],[\"全局自注意力\",{\"1\":{\"83\":1}}],[\"全局平均池化层\",{\"1\":{\"70\":1}}],[\"全为\",{\"1\":{\"67\":1,\"188\":1,\"420\":1}}],[\"数清楚\",{\"1\":{\"951\":1}}],[\"数不能超过图像\",{\"1\":{\"895\":1}}],[\"数值稳定性更高\",{\"1\":{\"931\":1}}],[\"数值不连续\",{\"1\":{\"931\":1}}],[\"数值\",{\"1\":{\"846\":1}}],[\"数值微分的问题\",{\"0\":{\"772\":1}}],[\"数值微分的实现\",{\"0\":{\"770\":1}}],[\"数值微分\",{\"0\":{\"768\":1}}],[\"数值可以是\",{\"1\":{\"88\":1}}],[\"数字拆分为独立字符\",{\"1\":{\"667\":1}}],[\"数理一致性等方面\",{\"1\":{\"649\":1}}],[\"数\",{\"1\":{\"260\":1,\"263\":3,\"264\":2,\"520\":1,\"900\":1}}],[\"数学上\",{\"1\":{\"943\":1}}],[\"数学推导\",{\"0\":{\"904\":1}}],[\"数学推理\",{\"1\":{\"335\":1}}],[\"数学解释\",{\"0\":{\"874\":1}}],[\"数学期望与方差\",{\"0\":{\"862\":1}}],[\"数学\",{\"1\":{\"669\":1}}],[\"数学等专业任务中\",{\"1\":{\"668\":1}}],[\"数学能力\",{\"1\":{\"668\":1}}],[\"数学知识点\",{\"0\":{\"573\":1},\"1\":{\"573\":1}}],[\"数学本质对比\",{\"1\":{\"500\":1}}],[\"数学表达式\",{\"1\":{\"257\":1}}],[\"数学定义\",{\"1\":{\"160\":1,\"574\":1,\"592\":1}}],[\"数十亿参数级别\",{\"1\":{\"220\":1}}],[\"数下\",{\"1\":{\"181\":1}}],[\"数组混合运算\",{\"1\":{\"812\":1}}],[\"数组转换为\",{\"1\":{\"425\":2}}],[\"数组后\",{\"1\":{\"410\":1}}],[\"数组格式返回\",{\"1\":{\"410\":1}}],[\"数组\",{\"1\":{\"107\":1,\"492\":1,\"542\":1,\"546\":2}}],[\"数量一致\",{\"1\":{\"528\":1}}],[\"数量和\",{\"1\":{\"380\":1}}],[\"数量达到总数的\",{\"1\":{\"234\":1}}],[\"数量相同\",{\"1\":{\"214\":1}}],[\"数量有限\",{\"1\":{\"173\":1}}],[\"数量列表\",{\"1\":{\"123\":1}}],[\"数量\",{\"1\":{\"70\":2,\"188\":1,\"263\":1,\"264\":1,\"266\":1,\"274\":1,\"380\":1,\"426\":1,\"895\":1,\"899\":1}}],[\"数据工程\",{\"1\":{\"835\":2}}],[\"数据连接\",{\"1\":{\"832\":1,\"833\":1}}],[\"数据存储等等\",{\"1\":{\"831\":1}}],[\"数据存储在data属性中\",{\"1\":{\"758\":1}}],[\"数据分析与可视化\",{\"1\":{\"823\":1}}],[\"数据分析和第三方服务调用\",{\"1\":{\"823\":1}}],[\"数据类型检查\",{\"0\":{\"792\":1}}],[\"数据类型和设备\",{\"1\":{\"213\":1}}],[\"数据预加载\",{\"1\":{\"697\":1}}],[\"数据预处理一般包括从多种格式向纯文本的转化\",{\"1\":{\"836\":1}}],[\"数据预处理\",{\"0\":{\"713\":1},\"1\":{\"107\":1,\"521\":1,\"595\":1,\"926\":1}}],[\"数据清洗\",{\"0\":{\"696\":1}}],[\"数据清洗与分词\",{\"1\":{\"633\":1}}],[\"数据量增加后\",{\"1\":{\"684\":1}}],[\"数据量和计算量按幂律缩放\",{\"1\":{\"650\":1}}],[\"数据规模和模型设置\",{\"1\":{\"682\":1}}],[\"数据复制增加了存储和计算开销\",{\"1\":{\"681\":1}}],[\"数据复制\",{\"1\":{\"681\":1}}],[\"数据复制行为\",{\"1\":{\"494\":1}}],[\"数据配置\",{\"1\":{\"680\":1}}],[\"数据根源\",{\"1\":{\"670\":1}}],[\"数据不透明\",{\"1\":{\"669\":1}}],[\"数据策略与开源兼容性\",{\"1\":{\"666\":1}}],[\"数据在训练中并非按体量采样\",{\"1\":{\"647\":1}}],[\"数据在底层存储为\",{\"1\":{\"545\":1}}],[\"数据质量\",{\"1\":{\"641\":1}}],[\"数据质量差\",{\"1\":{\"165\":1}}],[\"数据重叠检测\",{\"1\":{\"641\":1}}],[\"数据与架构创新\",{\"1\":{\"640\":1}}],[\"数据以避免测试集污染\",{\"1\":{\"640\":1}}],[\"数据安全的问题\",{\"1\":{\"601\":1}}],[\"数据可能沿某个方向拉长\",{\"1\":{\"578\":1}}],[\"数据内容\",{\"1\":{\"546\":1}}],[\"数据内容相同的张量\",{\"1\":{\"491\":1}}],[\"数据指针偏移\",{\"1\":{\"544\":1}}],[\"数据还是指向内存中已有的数据\",{\"1\":{\"544\":1}}],[\"数据元素在内存空间的排列顺序\",{\"1\":{\"541\":1}}],[\"数据值相同\",{\"1\":{\"491\":1}}],[\"数据的分布更加稀疏\",{\"1\":{\"429\":1}}],[\"数据的标注非常有限\",{\"1\":{\"19\":1}}],[\"数据下载\",{\"0\":{\"424\":1}}],[\"数据按\",{\"1\":{\"382\":1}}],[\"数据目录\",{\"1\":{\"382\":1}}],[\"数据模块\",{\"0\":{\"382\":1}}],[\"数据加载\",{\"0\":{\"933\":1}}],[\"数据加载器与数据集支持\",{\"1\":{\"819\":1}}],[\"数据加载阶段\",{\"1\":{\"381\":1}}],[\"数据加载完成\",{\"1\":{\"293\":1}}],[\"数据通常较长\",{\"1\":{\"374\":1}}],[\"数据进行语言预训练\",{\"1\":{\"374\":1}}],[\"数据进行视觉预训练\",{\"1\":{\"374\":1}}],[\"数据提升模型能力\",{\"1\":{\"374\":1}}],[\"数据会出去\",{\"1\":{\"356\":1}}],[\"数据依赖性强\",{\"1\":{\"346\":1}}],[\"数据需求高\",{\"1\":{\"646\":1}}],[\"数据需求\",{\"1\":{\"346\":1}}],[\"数据均匀采样\",{\"1\":{\"342\":1}}],[\"数据构建方式\",{\"1\":{\"341\":1}}],[\"数据准备阶段\",{\"1\":{\"381\":1,\"382\":1}}],[\"数据准备\",{\"1\":{\"293\":1}}],[\"数据来源\",{\"1\":{\"640\":1}}],[\"数据来源包括\",{\"1\":{\"339\":1}}],[\"数据来源既包括人工标注的图像\",{\"1\":{\"272\":1}}],[\"数据来预测\",{\"1\":{\"20\":1}}],[\"数据流\",{\"1\":{\"258\":1}}],[\"数据从头训练一个新模型\",{\"1\":{\"182\":1}}],[\"数据训练\",{\"1\":{\"182\":1,\"317\":1}}],[\"数据增强等\",{\"1\":{\"521\":1}}],[\"数据增强和更强的视觉编码器\",{\"1\":{\"327\":1}}],[\"数据增强包括随机裁剪和翻转\",{\"1\":{\"319\":1}}],[\"数据增强类\",{\"1\":{\"293\":1}}],[\"数据增强与\",{\"1\":{\"224\":1}}],[\"数据增强与配对策略\",{\"0\":{\"90\":1}}],[\"数据增强\",{\"0\":{\"169\":1},\"1\":{\"286\":1,\"382\":1}}],[\"数据自举方法\",{\"1\":{\"165\":1}}],[\"数据处理\",{\"1\":{\"830\":1}}],[\"数据处理阶段\",{\"1\":{\"829\":1}}],[\"数据处理参数\",{\"1\":{\"382\":1}}],[\"数据处理尤为有利\",{\"1\":{\"117\":1}}],[\"数据处理错误\",{\"1\":{\"20\":1}}],[\"数据路径设置\",{\"1\":{\"107\":1}}],[\"数据组织形式\",{\"1\":{\"92\":1}}],[\"数据总量\",{\"1\":{\"89\":1}}],[\"数据划分方式\",{\"1\":{\"89\":1}}],[\"数据划分\",{\"0\":{\"44\":1}}],[\"数据收集问题\",{\"1\":{\"658\":1}}],[\"数据收集\",{\"0\":{\"41\":1,\"888\":1},\"1\":{\"658\":1}}],[\"数据集预处理完后\",{\"1\":{\"713\":1}}],[\"数据集预训练模型\",{\"1\":{\"633\":1}}],[\"数据集评估\",{\"1\":{\"656\":1}}],[\"数据集构成与过滤策略\",{\"1\":{\"647\":1}}],[\"数据集构建\",{\"1\":{\"656\":1}}],[\"数据集构建钩子\",{\"1\":{\"382\":1}}],[\"数据集构建阶段\",{\"1\":{\"382\":1}}],[\"数据集级别\",{\"1\":{\"589\":1}}],[\"数据集下载\",{\"1\":{\"424\":1}}],[\"数据集训练的检测模型\",{\"1\":{\"388\":1}}],[\"数据集进行训练\",{\"1\":{\"342\":1}}],[\"数据集进行微调\",{\"1\":{\"187\":1,\"190\":1}}],[\"数据集加载代码\",{\"1\":{\"424\":1}}],[\"数据集加载\",{\"0\":{\"264\":1}}],[\"数据集处理方面\",{\"1\":{\"190\":1}}],[\"数据集上的实战测试\",{\"0\":{\"926\":1}}],[\"数据集上的实验结果\",{\"1\":{\"657\":1}}],[\"数据集上的性能下降\",{\"1\":{\"658\":1}}],[\"数据集上取得了很好的结果\",{\"1\":{\"354\":1}}],[\"数据集上\",{\"1\":{\"352\":1,\"657\":1}}],[\"数据集上进行训练与评估\",{\"1\":{\"928\":1}}],[\"数据集上进行训练后\",{\"1\":{\"188\":1}}],[\"数据集上进行了最高至\",{\"1\":{\"888\":1}}],[\"数据集上进行预训练的\",{\"1\":{\"435\":1}}],[\"数据集上进行迁移学习\",{\"1\":{\"342\":1}}],[\"数据集上开展了系统性的实验评估\",{\"1\":{\"45\":1}}],[\"数据集的零样本测试中表现优异\",{\"1\":{\"884\":1}}],[\"数据集的\",{\"1\":{\"343\":1}}],[\"数据集的加载\",{\"1\":{\"264\":1}}],[\"数据集的训练\",{\"1\":{\"187\":1}}],[\"数据集的初始化\",{\"1\":{\"53\":1}}],[\"数据集应当用于重新训练一个新模型\",{\"1\":{\"182\":1}}],[\"数据集重新训练模型\",{\"0\":{\"182\":1}}],[\"数据集一致\",{\"1\":{\"181\":1}}],[\"数据集中的图像都是单通道图像\",{\"1\":{\"926\":1}}],[\"数据集中每一个样本最终都会解析得到一个inputfeatures\",{\"1\":{\"713\":1}}],[\"数据集中图像的数量\",{\"1\":{\"424\":1}}],[\"数据集中\",{\"1\":{\"102\":1}}],[\"数据集类型\",{\"1\":{\"92\":1}}],[\"数据集存放目录\",{\"1\":{\"92\":1}}],[\"数据集统计信息\",{\"0\":{\"91\":1}}],[\"数据集设置\",{\"1\":{\"89\":1}}],[\"数据集组织方式\",{\"0\":{\"89\":1}}],[\"数据集基于\",{\"1\":{\"86\":1,\"93\":1}}],[\"数据集初始化的核心代码实现如下\",{\"1\":{\"92\":1}}],[\"数据集初始化\",{\"1\":{\"82\":1}}],[\"数据集目录下的组织方式\",{\"1\":{\"82\":1}}],[\"数据集贡献\",{\"1\":{\"30\":1}}],[\"数据集\",{\"0\":{\"40\":1,\"53\":1,\"82\":1,\"85\":1},\"1\":{\"28\":1,\"71\":1,\"183\":1,\"293\":1,\"317\":1,\"342\":1,\"382\":1,\"415\":1,\"655\":1,\"656\":3,\"657\":1}}],[\"数据损坏基准\",{\"0\":{\"25\":1}}],[\"数据退化的情况下依然精确识别交互区域\",{\"1\":{\"20\":1}}],[\"数据\",{\"1\":{\"20\":1,\"176\":1,\"305\":3,\"384\":1,\"386\":1,\"679\":1,\"680\":1,\"918\":1}}],[\"数据稀缺\",{\"1\":{\"19\":1}}],[\"热图回归损失\",{\"1\":{\"79\":1}}],[\"热图\",{\"1\":{\"70\":1}}],[\"指导大型语言模型生成更为精准的答案\",{\"1\":{\"828\":1}}],[\"指导抓取和运动规划\",{\"1\":{\"75\":1}}],[\"指代准确率81\",{\"1\":{\"670\":1}}],[\"指的就是上述公式中的矩阵w\",{\"1\":{\"600\":1}}],[\"指的是输入图像的尺寸为\",{\"1\":{\"435\":1}}],[\"指南\",{\"1\":{\"566\":1}}],[\"指向\",{\"1\":{\"544\":1}}],[\"指明在每个维度上移动一个索引单位时\",{\"1\":{\"544\":1}}],[\"指明哪些位置是有效的\",{\"1\":{\"420\":1}}],[\"指从一个大的数据结构中提取出一部分连续的数据子集\",{\"1\":{\"544\":1}}],[\"指令遵循与文本生成提供了复杂业务逻辑的简单平替方案\",{\"1\":{\"835\":1}}],[\"指令遵循\",{\"1\":{\"825\":1}}],[\"指令跟随能力\",{\"1\":{\"658\":1}}],[\"指令的顺从\",{\"1\":{\"657\":1}}],[\"指令中的显式约束\",{\"1\":{\"657\":1}}],[\"指令\",{\"1\":{\"339\":2,\"341\":2,\"346\":2}}],[\"指令调优\",{\"1\":{\"339\":1,\"346\":2}}],[\"指令微调\",{\"0\":{\"669\":1},\"1\":{\"312\":1,\"668\":1,\"825\":1}}],[\"指令理解特征\",{\"1\":{\"69\":1}}],[\"指定标签\",{\"0\":{\"939\":1}}],[\"指定待处理图像路径后\",{\"1\":{\"582\":1}}],[\"指定沿哪一维累加\",{\"1\":{\"487\":1}}],[\"指定最小长度\",{\"1\":{\"485\":1}}],[\"指定\",{\"1\":{\"484\":1,\"486\":1}}],[\"指定输出数据类型\",{\"1\":{\"481\":1}}],[\"指定维度\",{\"1\":{\"480\":1}}],[\"指定操作的维度\",{\"1\":{\"480\":1}}],[\"指定填充的方式\",{\"1\":{\"477\":1}}],[\"指定模型只预测被遮挡的位置\",{\"1\":{\"265\":1}}],[\"指定的列\",{\"1\":{\"92\":1}}],[\"指数中的表达式\",{\"1\":{\"871\":1}}],[\"指数级表达能力\",{\"1\":{\"500\":1}}],[\"指数增长\",{\"1\":{\"500\":2}}],[\"指数衰减的polyak\",{\"1\":{\"289\":1}}],[\"指数滑动平均\",{\"1\":{\"213\":1,\"285\":1}}],[\"指数平均移动更新\",{\"1\":{\"213\":1}}],[\"指数平均移动\",{\"1\":{\"213\":1}}],[\"指标的选择和权衡\",{\"0\":{\"566\":1}}],[\"指标\",{\"1\":{\"106\":2,\"566\":1,\"588\":1}}],[\"指出图像中物体与人交互的部分\",{\"1\":{\"35\":1}}],[\"定价为\",{\"1\":{\"823\":1}}],[\"定价降低约\",{\"1\":{\"823\":1}}],[\"定性分析与模型行为观察\",{\"1\":{\"657\":1}}],[\"定理\",{\"0\":{\"499\":1}}],[\"定理表明\",{\"1\":{\"150\":1}}],[\"定义损失函数\",{\"0\":{\"932\":1}}],[\"定义它们的平均分布\",{\"1\":{\"914\":1}}],[\"定义似然函数\",{\"1\":{\"903\":1}}],[\"定义如下\",{\"1\":{\"871\":1}}],[\"定义在实数上的连续分布\",{\"0\":{\"864\":1}}],[\"定义失败为红球\",{\"0\":{\"860\":1}}],[\"定义随机变量\",{\"1\":{\"846\":1}}],[\"定义事件空间后\",{\"1\":{\"846\":1}}],[\"定义节点id为1\",{\"1\":{\"815\":1}}],[\"定义包含节点x和y的有向图\",{\"1\":{\"815\":1}}],[\"定义短距离线性映射的阈值\",{\"1\":{\"710\":1}}],[\"定义相对位置为\",{\"1\":{\"710\":1}}],[\"定义相似度函数为\",{\"1\":{\"199\":1}}],[\"定义字典保存路径\",{\"1\":{\"697\":1}}],[\"定义时\",{\"1\":{\"445\":1}}],[\"定义注意力矩阵的丢弃层\",{\"1\":{\"430\":1}}],[\"定义一个线性层\",{\"1\":{\"430\":1}}],[\"定义一个二维卷积层\",{\"1\":{\"426\":1}}],[\"定义一个字典\",{\"1\":{\"425\":1}}],[\"定义当前目录\",{\"1\":{\"410\":1,\"412\":1}}],[\"定义卷积层\",{\"1\":{\"380\":1}}],[\"定义\",{\"0\":{\"871\":1,\"931\":1,\"937\":1},\"1\":{\"346\":1,\"507\":1,\"508\":1,\"589\":1,\"846\":1}}],[\"定义与核心思想\",{\"1\":{\"346\":1}}],[\"定义为每个子像素出现概率的乘积\",{\"1\":{\"925\":1}}],[\"定义为\",{\"1\":{\"257\":1,\"592\":1,\"856\":1,\"857\":1,\"865\":2,\"867\":2,\"868\":1,\"871\":1}}],[\"定义的一个结构体\",{\"1\":{\"208\":1}}],[\"定义局部区域的形心\",{\"1\":{\"133\":1}}],[\"定义投影层的丢弃层\",{\"1\":{\"430\":1}}],[\"定义投影层\",{\"1\":{\"69\":1,\"430\":1}}],[\"定位答案\",{\"1\":{\"735\":1}}],[\"定位交互部位\",{\"1\":{\"52\":1}}],[\"定位交互部件并分析几何结构\",{\"1\":{\"30\":1}}],[\"定位交互发生的对象区域\",{\"1\":{\"52\":1}}],[\"定位精度显著更高\",{\"1\":{\"47\":1}}],[\"定位\",{\"1\":{\"31\":1,\"269\":1}}],[\"调试和测试\",{\"1\":{\"834\":1}}],[\"调试便捷\",{\"1\":{\"811\":1}}],[\"调节随机性\",{\"1\":{\"898\":1}}],[\"调节因子\",{\"1\":{\"589\":1}}],[\"调节\",{\"1\":{\"274\":1}}],[\"调度器\",{\"1\":{\"224\":1,\"293\":1}}],[\"调整的过程\",{\"1\":{\"616\":1}}],[\"调整为适合下游任务的\",{\"1\":{\"609\":1}}],[\"调整大小\",{\"1\":{\"582\":1}}],[\"调整后的\",{\"1\":{\"546\":1}}],[\"调整以下三个要素来创建一个新的视图\",{\"1\":{\"544\":1}}],[\"调整某一系数会影响整个函数\",{\"1\":{\"500\":1}}],[\"调整某一系数会影响全局拟合\",{\"1\":{\"500\":1}}],[\"调整位置编码以适配不同尺度\",{\"1\":{\"286\":1}}],[\"调整维度以便加到注意力分数\",{\"1\":{\"710\":1}}],[\"调整维度\",{\"1\":{\"146\":1,\"380\":1,\"710\":1}}],[\"调整到最终输出维度\",{\"1\":{\"120\":1}}],[\"调整输出格式为\",{\"1\":{\"69\":1}}],[\"调整\",{\"1\":{\"69\":1,\"680\":1}}],[\"调用父类\",{\"1\":{\"926\":1}}],[\"调用函数或工具\",{\"1\":{\"833\":1}}],[\"调用x\",{\"1\":{\"809\":1}}],[\"调用方法\",{\"1\":{\"455\":1}}],[\"调用后\",{\"1\":{\"447\":1,\"451\":2,\"459\":1}}],[\"调用前\",{\"1\":{\"447\":1,\"451\":2,\"459\":1}}],[\"调用时\",{\"1\":{\"445\":1}}],[\"调用q\",{\"1\":{\"421\":1}}],[\"调用bertmodel\",{\"1\":{\"403\":1}}],[\"调用子数据模块的\",{\"1\":{\"382\":1}}],[\"调用每个子数据模块的\",{\"1\":{\"382\":1}}],[\"调用所有\",{\"1\":{\"381\":1}}],[\"调用跨模态解码器\",{\"1\":{\"187\":1}}],[\"调用\",{\"1\":{\"64\":1,\"156\":1,\"208\":1,\"213\":1,\"382\":4,\"383\":1,\"384\":1,\"420\":2,\"461\":1,\"491\":1,\"511\":1}}],[\"都差不多\",{\"1\":{\"960\":1}}],[\"都由神经网络实现\",{\"1\":{\"946\":1}}],[\"都成立\",{\"1\":{\"917\":1}}],[\"都可以提供对应的解决方案\",{\"1\":{\"831\":1}}],[\"都可以归纳为一种字典查询的工作\",{\"1\":{\"353\":1}}],[\"都使用了残差连接和层归一化\",{\"1\":{\"741\":1}}],[\"都单独构造一个\",{\"1\":{\"737\":1}}],[\"都等于max\",{\"1\":{\"713\":1}}],[\"都对应一个可学习的向量\",{\"1\":{\"707\":1}}],[\"都无所谓\",{\"1\":{\"691\":1}}],[\"都要\",{\"1\":{\"663\":1}}],[\"都随着模型参数的增长而呈现平滑的幂律提升趋势\",{\"1\":{\"648\":1}}],[\"都预测为负类别\",{\"1\":{\"562\":1}}],[\"都需要进行有监督微调\",{\"1\":{\"413\":1}}],[\"都处理文本+图像的联合序列\",{\"1\":{\"384\":1}}],[\"都存到了一起\",{\"1\":{\"357\":1}}],[\"都是比特\",{\"1\":{\"951\":1}}],[\"都是高斯分布\",{\"1\":{\"949\":1}}],[\"都是高阶函数\",{\"1\":{\"447\":1}}],[\"都是在没有调整温度\",{\"1\":{\"889\":1}}],[\"都是在寻找最相关的\",{\"1\":{\"531\":1}}],[\"都是有足够资源的来开发大模型\",{\"1\":{\"610\":1}}],[\"都是有效的\",{\"1\":{\"67\":1}}],[\"都是一个非线性变换\",{\"1\":{\"500\":1}}],[\"都是197\",{\"1\":{\"429\":1}}],[\"都是从同一个\",{\"1\":{\"357\":1}}],[\"都展现出更强的像素级感知能力\",{\"1\":{\"308\":1}}],[\"都视作文本\",{\"1\":{\"268\":1}}],[\"都被统一当作文本\",{\"1\":{\"268\":1}}],[\"都很敏感\",{\"1\":{\"264\":1}}],[\"都能接收相同的图像信息\",{\"1\":{\"188\":1}}],[\"都有一个对应的\",{\"1\":{\"733\":1}}],[\"都有一个\",{\"1\":{\"663\":1}}],[\"都有一个低维的本质模型\",{\"1\":{\"606\":1}}],[\"都有一个唯一的地址\",{\"1\":{\"540\":1}}],[\"都有机会执行\",{\"1\":{\"398\":1}}],[\"都有\",{\"1\":{\"160\":1}}],[\"都有效\",{\"1\":{\"67\":1,\"380\":1}}],[\"都尝试引入更复杂的结构来提升建模能力\",{\"1\":{\"157\":1}}],[\"都会非常接近于\",{\"1\":{\"945\":1}}],[\"都会乘以相同的输入\",{\"1\":{\"611\":1}}],[\"都会对输入序列的长度有限制\",{\"1\":{\"601\":1}}],[\"都会处理一部分\",{\"1\":{\"364\":1}}],[\"都会被压缩为一个固定长度的特征向量\",{\"1\":{\"137\":1}}],[\"都会基于其语言语义\",{\"1\":{\"100\":1}}],[\"都与所有点\",{\"1\":{\"100\":1}}],[\"都带有原始语言上下文\",{\"1\":{\"100\":1}}],[\"则先验分布\",{\"1\":{\"959\":1}}],[\"则先用全零矩阵占位\",{\"1\":{\"213\":1}}],[\"则分布中仅有\",{\"1\":{\"959\":1}}],[\"则分为了单模态编码和多模态融合两个阶段\",{\"1\":{\"274\":1}}],[\"则正面挑战了建立概率模型这一任务\",{\"1\":{\"925\":1}}],[\"则似然函数为\",{\"1\":{\"904\":1}}],[\"则归一化输出\",{\"1\":{\"898\":1}}],[\"则裁剪掉最后一个\",{\"1\":{\"893\":1}}],[\"则整条\",{\"1\":{\"893\":1}}],[\"则用来指代那些使用概率理论来表示\",{\"1\":{\"877\":1}}],[\"则二项分布退化为伯努利分布\",{\"1\":{\"856\":1}}],[\"则称\",{\"1\":{\"849\":2}}],[\"则定义事件\",{\"1\":{\"849\":1}}],[\"则复合事件的概率可通过求和得到\",{\"1\":{\"846\":1}}],[\"则安装成功\",{\"1\":{\"815\":1}}],[\"则调用b\",{\"1\":{\"809\":1}}],[\"则调用knn查询函数计算\",{\"1\":{\"119\":1}}],[\"则尝试调用右操作数b的\",{\"1\":{\"809\":1}}],[\"则反向传播后清除中间变量的导数\",{\"1\":{\"807\":1}}],[\"则将其概率分布中的文本词索引空间对应的概率分布设置为0\",{\"1\":{\"892\":1}}],[\"则将其概率分布中的离散视觉词索引空间对应的概率分布设置为0\",{\"1\":{\"892\":1}}],[\"则将其设置为\",{\"1\":{\"734\":2}}],[\"则将其转换为\",{\"1\":{\"426\":1}}],[\"则生成的pad\",{\"1\":{\"703\":1}}],[\"则写出听起来\",{\"1\":{\"657\":1}}],[\"则容易自信地给出错误答案\",{\"1\":{\"657\":1}}],[\"则它的参数量\",{\"1\":{\"611\":1}}],[\"则在这里激活\",{\"1\":{\"588\":1,\"589\":1}}],[\"则应注释掉这一行\",{\"1\":{\"587\":1,\"588\":1}}],[\"则点\",{\"1\":{\"572\":1}}],[\"则可能有必要选择\",{\"1\":{\"572\":1}}],[\"则可以用\",{\"1\":{\"569\":1}}],[\"则其概率质量函数\",{\"1\":{\"858\":1}}],[\"则其补集\",{\"1\":{\"847\":1}}],[\"则其\",{\"1\":{\"570\":1}}],[\"则其假正例数为零\",{\"1\":{\"565\":1}}],[\"则其准确性得分为\",{\"1\":{\"562\":1}}],[\"则最好改为针对其他指标进行优化\",{\"1\":{\"562\":1}}],[\"则表示数据集不平衡\",{\"1\":{\"561\":1}}],[\"则表示图像块是正方形\",{\"1\":{\"426\":1}}],[\"则表示图像是正方形\",{\"1\":{\"426\":1}}],[\"则会调用右操作数b的\",{\"1\":{\"809\":1}}],[\"则会得到以下表格\",{\"1\":{\"561\":1}}],[\"则会因\",{\"1\":{\"179\":1}}],[\"则设置所有可见\",{\"1\":{\"521\":1}}],[\"则设为总遮挡目标数\",{\"1\":{\"263\":1}}],[\"则\",{\"1\":{\"491\":2,\"611\":1,\"612\":1,\"706\":1,\"774\":1,\"809\":1,\"846\":1,\"847\":1,\"904\":1,\"944\":1}}],[\"则需要更新预训练模型参数\",{\"1\":{\"611\":1}}],[\"则需要在这里激活\",{\"1\":{\"586\":1}}],[\"则需要使用不同的工具\",{\"1\":{\"568\":1}}],[\"则需要再多一层函数嵌套\",{\"1\":{\"453\":1}}],[\"则需要将图像特征复制\",{\"1\":{\"188\":1}}],[\"则创建线性分类头\",{\"1\":{\"431\":1}}],[\"则默认为\",{\"1\":{\"429\":2}}],[\"则默认多模态\",{\"1\":{\"67\":1}}],[\"则计算当前层bertlayer时\",{\"1\":{\"420\":1}}],[\"则q来自query\",{\"1\":{\"420\":1}}],[\"则自动生成\",{\"1\":{\"419\":1}}],[\"则自动构造\",{\"1\":{\"274\":1}}],[\"则采用了两种不同的架构\",{\"1\":{\"407\":1}}],[\"则是以文本作为监督信号\",{\"1\":{\"405\":1}}],[\"则是从零开始进行端到端预训练\",{\"1\":{\"272\":1}}],[\"则单独为\",{\"1\":{\"380\":1}}],[\"则覆盖\",{\"1\":{\"380\":1}}],[\"则加上\",{\"1\":{\"380\":2}}],[\"则加载多语言\",{\"1\":{\"315\":1}}],[\"则使用其打分\",{\"1\":{\"895\":1}}],[\"则使用unk替换\",{\"1\":{\"697\":1}}],[\"则使用beam\",{\"1\":{\"647\":1}}],[\"则使用\",{\"1\":{\"431\":1}}],[\"则使用默认的\",{\"1\":{\"431\":1}}],[\"则使用默认的缩放因子\",{\"1\":{\"430\":1}}],[\"则使用该层进行归一化\",{\"1\":{\"426\":2}}],[\"则使用来自文献\",{\"1\":{\"306\":1}}],[\"则使用所有点作为查询点\",{\"1\":{\"119\":1}}],[\"则输出的是像素取某个颜色的概率分布\",{\"1\":{\"925\":1}}],[\"则输出为常量\",{\"1\":{\"290\":1}}],[\"则输出维度与输入相同\",{\"1\":{\"122\":1}}],[\"则负责完成整个模型架构流程的实现\",{\"1\":{\"274\":1}}],[\"则引入\",{\"1\":{\"272\":1}}],[\"则构造成正方形大小的\",{\"1\":{\"263\":1}}],[\"则直接将视觉和语言\",{\"1\":{\"269\":1}}],[\"则直接返回\",{\"1\":{\"256\":1}}],[\"则直接加载预训练的\",{\"1\":{\"213\":1}}],[\"则保留原中心不变\",{\"1\":{\"213\":1}}],[\"则保存\",{\"1\":{\"106\":1}}],[\"则跳过\",{\"1\":{\"213\":1}}],[\"则从最后一层开始使用\",{\"1\":{\"380\":1}}],[\"则从已有\",{\"1\":{\"213\":1}}],[\"则从问题1～15中随机选择一个问题\",{\"1\":{\"92\":1}}],[\"则按给定的概率矩阵进行伯努利采样\",{\"1\":{\"208\":1}}],[\"则做\",{\"1\":{\"207\":1}}],[\"则为\",{\"1\":{\"206\":1}}],[\"则视为\",{\"1\":{\"201\":1}}],[\"则基于高质量人工标注的数据集如\",{\"1\":{\"185\":1}}],[\"则模型体积大\",{\"1\":{\"179\":1}}],[\"则认为是噪声文本\",{\"1\":{\"173\":1}}],[\"则返回所有样本损失的平均值\",{\"1\":{\"590\":1,\"592\":1}}],[\"则返回\",{\"1\":{\"154\":2}}],[\"则返回该问题文本\",{\"1\":{\"92\":1}}],[\"则拼接起来\",{\"1\":{\"145\":1}}],[\"则对图像进行处理\",{\"1\":{\"424\":1}}],[\"则对整个点云做全局特征提取\",{\"1\":{\"137\":1}}],[\"则对所有点进行自查询\",{\"1\":{\"119\":1}}],[\"则进行额外处理\",{\"1\":{\"64\":1}}],[\"插件系统\",{\"1\":{\"823\":1}}],[\"插值到原始图像尺寸\",{\"1\":{\"582\":2}}],[\"插值到目标点上的特征\",{\"1\":{\"122\":1}}],[\"插值公式\",{\"1\":{\"505\":1}}],[\"插值方式\",{\"1\":{\"503\":1}}],[\"插值并融合后的特征\",{\"1\":{\"145\":1}}],[\"插值得到的密集特征\",{\"1\":{\"143\":1}}],[\"插值时选取的近邻点个数\",{\"1\":{\"122\":1}}],[\"插值\",{\"0\":{\"504\":1,\"505\":1},\"1\":{\"122\":1}}],[\"插值阶段\",{\"1\":{\"54\":1}}],[\"插入对应层的list缓存中去\",{\"1\":{\"663\":1}}],[\"插入缓存中\",{\"1\":{\"663\":1}}],[\"插入新维度的位置\",{\"1\":{\"466\":1}}],[\"插入到了\",{\"1\":{\"306\":1}}],[\"插入异常点\",{\"1\":{\"157\":1}}],[\"插入批次索引\",{\"1\":{\"83\":1}}],[\"插入的多模态嵌入\",{\"1\":{\"67\":1}}],[\"非负\",{\"1\":{\"855\":1}}],[\"非负性\",{\"1\":{\"848\":1}}],[\"非负整数\",{\"1\":{\"709\":1}}],[\"非负整数张量\",{\"1\":{\"485\":1}}],[\"非负整数张量input\",{\"1\":{\"485\":1}}],[\"非多任务学习\",{\"1\":{\"680\":1}}],[\"非英语泛化能力未系统评估\",{\"1\":{\"658\":1}}],[\"非英语指令等非监督数据上表现较好\",{\"1\":{\"657\":1}}],[\"非英语指令\",{\"1\":{\"657\":1}}],[\"非功能区域\",{\"1\":{\"589\":1}}],[\"非目标点\",{\"1\":{\"589\":1}}],[\"非目标区域匹配度\",{\"1\":{\"102\":1}}],[\"非对角元素\",{\"1\":{\"578\":1}}],[\"非对角线元素为\",{\"1\":{\"871\":1}}],[\"非对角线\",{\"1\":{\"574\":1}}],[\"非对角线上的元素\",{\"1\":{\"574\":1}}],[\"非垃圾邮件被正确分类为非垃圾邮件\",{\"1\":{\"561\":1}}],[\"非垃圾邮件被误分类为垃圾邮件\",{\"1\":{\"561\":1}}],[\"非连续\",{\"1\":{\"469\":1}}],[\"非常值得深入理解\",{\"1\":{\"945\":1}}],[\"非常小\",{\"1\":{\"944\":1}}],[\"非常接近了\",{\"1\":{\"921\":1}}],[\"非常适合比较模型\",{\"1\":{\"571\":1}}],[\"非常高\",{\"1\":{\"415\":2}}],[\"非常有效\",{\"1\":{\"380\":1}}],[\"非线性替换为\",{\"1\":{\"823\":1}}],[\"非线性激活后的基\",{\"1\":{\"500\":1}}],[\"非线性变换\",{\"1\":{\"397\":1,\"403\":1}}],[\"非线性增强和空间对齐\",{\"1\":{\"65\":2}}],[\"非训练模式下统计码本使用情况\",{\"1\":{\"213\":1}}],[\"非梯度更新\",{\"1\":{\"213\":1}}],[\"非动量\",{\"1\":{\"208\":1}}],[\"非标答案\",{\"1\":{\"202\":1}}],[\"非均匀\",{\"1\":{\"161\":1}}],[\"非均匀密度下稳定的特征学习\",{\"0\":{\"139\":1}}],[\"非刚性运动\",{\"1\":{\"161\":1}}],[\"非刚性变形\",{\"1\":{\"157\":1}}],[\"非结构化\",{\"1\":{\"159\":1}}],[\"非\",{\"1\":{\"67\":1,\"208\":1,\"420\":1,\"557\":1,\"896\":1,\"899\":1}}],[\"供后续\",{\"1\":{\"385\":2}}],[\"供离散\",{\"1\":{\"265\":1}}],[\"供\",{\"1\":{\"67\":1,\"265\":1,\"892\":1}}],[\"默认从\",{\"1\":{\"898\":1}}],[\"默认采样前\",{\"1\":{\"895\":1}}],[\"默认采用gelu激活函数\",{\"1\":{\"403\":1}}],[\"默认采用\",{\"1\":{\"272\":1,\"278\":1}}],[\"默认优先级为100\",{\"1\":{\"809\":1}}],[\"默认模式\",{\"1\":{\"807\":1}}],[\"默认接受并执行\",{\"1\":{\"657\":1}}],[\"默认情况下\",{\"1\":{\"619\":1}}],[\"默认情况下有\",{\"1\":{\"274\":1}}],[\"默认要求\",{\"1\":{\"587\":1}}],[\"默认仓库\",{\"1\":{\"557\":1}}],[\"默认值\",{\"1\":{\"488\":2}}],[\"默认会展平为\",{\"1\":{\"480\":1}}],[\"默认会忽略标签为\",{\"1\":{\"208\":1}}],[\"默认是\",{\"1\":{\"466\":1,\"479\":1}}],[\"默认使用行优先方式\",{\"1\":{\"541\":1}}],[\"默认使用\",{\"1\":{\"429\":1}}],[\"默认忽略的标签值\",{\"1\":{\"420\":1}}],[\"默认不计算\",{\"1\":{\"385\":1}}],[\"默认归一化层\",{\"1\":{\"380\":1}}],[\"默认16x16\",{\"1\":{\"380\":1}}],[\"默认224x224\",{\"1\":{\"380\":1}}],[\"默认生成池化器长度设为\",{\"1\":{\"278\":1}}],[\"默认行为\",{\"1\":{\"266\":1}}],[\"默认与\",{\"1\":{\"263\":1}}],[\"默认取\",{\"1\":{\"263\":1}}],[\"默认等于\",{\"1\":{\"263\":1}}],[\"默认用欧氏距离\",{\"1\":{\"213\":1}}],[\"默认\",{\"1\":{\"192\":1,\"205\":2,\"255\":1,\"292\":1,\"440\":2,\"474\":1,\"482\":1,\"503\":2,\"513\":2,\"807\":1,\"892\":1}}],[\"默认为0\",{\"1\":{\"430\":2}}],[\"默认为false\",{\"1\":{\"430\":1}}],[\"默认为8\",{\"1\":{\"430\":1}}],[\"默认为\",{\"1\":{\"187\":1,\"380\":1,\"424\":1,\"426\":5,\"586\":1,\"587\":1,\"588\":1,\"589\":1}}],[\"默认为全\",{\"1\":{\"67\":1}}],[\"默认有\",{\"1\":{\"138\":1}}],[\"默认3\",{\"1\":{\"122\":1}}],[\"默认图像注意力掩码为空\",{\"1\":{\"64\":1}}],[\"形如\",{\"1\":{\"899\":1}}],[\"形\",{\"1\":{\"713\":1}}],[\"形式和内容高度敏感\",{\"1\":{\"649\":1}}],[\"形式存储和计算\",{\"1\":{\"614\":1}}],[\"形式\",{\"1\":{\"595\":1,\"737\":1,\"885\":1}}],[\"形式传进去\",{\"1\":{\"261\":1}}],[\"形式化地\",{\"1\":{\"231\":1}}],[\"形式的监督信号\",{\"1\":{\"204\":1}}],[\"形式为\",{\"1\":{\"102\":1,\"698\":1}}],[\"形心点的坐标来实现\",{\"1\":{\"136\":1}}],[\"形成量化后的表示\",{\"1\":{\"963\":1}}],[\"形成了一个具备如下特征的微型深度学习框架\",{\"1\":{\"819\":1}}],[\"形成复合函数\",{\"1\":{\"765\":1}}],[\"形成集合\",{\"1\":{\"285\":1}}],[\"形成离散化的语义表示\",{\"1\":{\"228\":1}}],[\"形成簇成员集合\",{\"1\":{\"213\":1}}],[\"形成多尺度特征表示\",{\"1\":{\"143\":1}}],[\"形成最终的局部特征表示\",{\"1\":{\"141\":1}}],[\"形成最终的局部区域表示\",{\"1\":{\"137\":1}}],[\"形成\",{\"1\":{\"137\":1,\"190\":1,\"214\":1}}],[\"形成一个连续的一维空间\",{\"1\":{\"540\":1}}],[\"形成一个特征向量\",{\"1\":{\"142\":1}}],[\"形成一个综合的多尺度特征表示\",{\"1\":{\"140\":1}}],[\"形成一个新的子集\",{\"1\":{\"137\":1}}],[\"形成一个类似于\",{\"1\":{\"97\":1}}],[\"形成每个查询点的邻域特征集合\",{\"1\":{\"119\":1}}],[\"形成更稳定的联合表示\",{\"1\":{\"65\":1}}],[\"形状不自然\",{\"1\":{\"884\":1}}],[\"形状和数据类型\",{\"1\":{\"815\":1}}],[\"形状如\",{\"1\":{\"590\":1,\"592\":1}}],[\"形状由\",{\"1\":{\"484\":1}}],[\"形状相同的张量\",{\"1\":{\"481\":1}}],[\"形状是\",{\"1\":{\"418\":1}}],[\"形状通常为\",{\"1\":{\"384\":1}}],[\"形状与\",{\"1\":{\"143\":1,\"590\":1,\"592\":1,\"710\":1}}],[\"形状与pxo1中的特征相同或变换后的维度\",{\"1\":{\"122\":1}}],[\"形状也为\",{\"1\":{\"102\":1}}],[\"形状\",{\"1\":{\"100\":4,\"120\":4,\"121\":3,\"145\":5,\"146\":2,\"205\":1,\"213\":1,\"421\":1,\"503\":1,\"542\":2,\"544\":2,\"547\":1,\"574\":1,\"710\":3,\"733\":2,\"899\":3}}],[\"形状多样\",{\"1\":{\"95\":1}}],[\"形状的一维数组\",{\"1\":{\"88\":1}}],[\"形状为\",{\"1\":{\"67\":1,\"102\":1,\"107\":1,\"121\":4,\"213\":1,\"426\":2,\"488\":1,\"545\":1,\"546\":1,\"586\":2,\"587\":2,\"588\":2,\"589\":2,\"899\":2}}],[\"形态学\",{\"1\":{\"11\":1}}],[\"它之前的名字叫logit\",{\"1\":{\"958\":1}}],[\"它提供了把大图像翻译成\",{\"1\":{\"956\":1}}],[\"它提供了一种根据观测数据\",{\"1\":{\"853\":1}}],[\"它和ae一样\",{\"1\":{\"956\":1}}],[\"它控制着两个损失项之间的权重\",{\"1\":{\"951\":1}}],[\"它度量了\",{\"1\":{\"950\":1}}],[\"它衡量两个概率分布之间的距离\",{\"1\":{\"945\":1}}],[\"它最大的优势在于可以直接从中采样\",{\"1\":{\"944\":1}}],[\"它引入了条件变量\",{\"1\":{\"936\":1}}],[\"它引入了局部区域搜索\",{\"1\":{\"157\":1}}],[\"它有一个闭式解\",{\"1\":{\"932\":1}}],[\"它大概率指的就是\",{\"1\":{\"925\":1}}],[\"它把\",{\"1\":{\"925\":1}}],[\"它前面的像素\",{\"1\":{\"924\":1}}],[\"它用于约束编码器的输出\",{\"1\":{\"960\":1}}],[\"它用于优化嵌入空间\",{\"1\":{\"960\":1}}],[\"它用于聚合全局信息\",{\"1\":{\"286\":1}}],[\"它用一些巧妙的方法约束了编码向量\",{\"1\":{\"956\":1}}],[\"它用来衡量模型预测的概率分布\",{\"1\":{\"910\":1}}],[\"它总是\",{\"1\":{\"909\":1}}],[\"它假设数据生成过程已知\",{\"1\":{\"903\":1}}],[\"它描述了在观测到\",{\"1\":{\"878\":1}}],[\"它从一些基本的\",{\"1\":{\"847\":1}}],[\"它必须满足以下三条规则\",{\"1\":{\"847\":1}}],[\"它必须要在超大数据集上进行预训练\",{\"1\":{\"432\":1}}],[\"它本身不\",{\"1\":{\"846\":1}}],[\"它本身不随机\",{\"1\":{\"846\":1}}],[\"它让计算机更好地理解和使用语言\",{\"1\":{\"827\":1}}],[\"它允许模型在处理每个词时关注输入序列中的所有词\",{\"1\":{\"741\":1}}],[\"它允许不同形状的张量进行逐元素\",{\"1\":{\"546\":1}}],[\"它较难学习到长距离的依赖关系\",{\"1\":{\"740\":1}}],[\"它对应某个相对位置桶\",{\"1\":{\"710\":1}}],[\"它对每个维度的偏差一视同仁\",{\"1\":{\"576\":1}}],[\"它也总是会沿着某个分支进行dfs直到\",{\"1\":{\"804\":1}}],[\"它也倾向于过度规避风险\",{\"1\":{\"657\":1}}],[\"它也可以微调用作\",{\"1\":{\"222\":1}}],[\"它面临的问题包括任务适应不均\",{\"1\":{\"649\":1}}],[\"它验证了单一模型架构通过规模扩展即可实现多任务处理的可能性\",{\"1\":{\"643\":1}}],[\"它被证明在许多任务上有很强的表现\",{\"1\":{\"626\":1}}],[\"它得到的结果是错的\",{\"1\":{\"621\":1}}],[\"它并不是一个通用智能系统\",{\"1\":{\"649\":1}}],[\"它并不保证结果的合理性和正确性\",{\"1\":{\"616\":1}}],[\"它并不像单词那样有很强的语义信息\",{\"1\":{\"353\":1}}],[\"它只是在给定的信息的前提下\",{\"1\":{\"616\":1}}],[\"它来源于\",{\"1\":{\"586\":1}}],[\"它直接对每个维度的偏差做平方加总\",{\"1\":{\"578\":1}}],[\"它就是我们平时量两个点之间\",{\"1\":{\"576\":1}}],[\"它表示我们在看到数据\",{\"1\":{\"877\":1}}],[\"它表示\",{\"1\":{\"574\":1,\"909\":1}}],[\"它可能由两部分组成\",{\"1\":{\"709\":1}}],[\"它可按下式计算\",{\"1\":{\"567\":1}}],[\"它可以把每个输入单词都映射到一个独一无二的连续向量上\",{\"1\":{\"956\":1}}],[\"它可以改进搜索引擎\",{\"1\":{\"827\":1}}],[\"它可以帮助计算机更好地理解和生成文本\",{\"1\":{\"827\":1}}],[\"它可以当成函数调用\",{\"1\":{\"745\":1}}],[\"它可以在指定维度上\",{\"1\":{\"700\":1}}],[\"它可以拥有任意数量的维度\",{\"1\":{\"540\":1}}],[\"它可以以任意的精度来近似任何一个定义在实数空间中的有界闭集函数\",{\"1\":{\"500\":1}}],[\"它可以被理解为一种\",{\"1\":{\"280\":1}}],[\"它离\",{\"1\":{\"502\":4}}],[\"它根据已有张量的属性创建一个全零张量\",{\"1\":{\"486\":1}}],[\"它会根据\",{\"1\":{\"710\":1}}],[\"它会把一个张量\",{\"1\":{\"474\":1}}],[\"它会复制数据\",{\"1\":{\"471\":1}}],[\"它会将多个样本收集起来形成一个批次\",{\"1\":{\"424\":1}}],[\"它会将图像切分为\",{\"1\":{\"171\":1}}],[\"它接收一个函数或类作为参数\",{\"1\":{\"450\":1}}],[\"它接收来自编码器和解码器的特征\",{\"1\":{\"70\":1}}],[\"它\",{\"1\":{\"448\":1}}],[\"它位于最终分类头之前\",{\"1\":{\"431\":1}}],[\"它与类的实例和类本身都没有直接关联\",{\"1\":{\"424\":1}}],[\"它首次将nlp领域火热的transformer模型架构移植到了cv领域\",{\"1\":{\"422\":1}}],[\"它将权重矩阵量化为\",{\"1\":{\"614\":1}}],[\"它将\",{\"1\":{\"592\":1,\"805\":1}}],[\"它将每个\",{\"1\":{\"420\":1}}],[\"它将输入的向量转换为\",{\"1\":{\"410\":1}}],[\"它由一些基本区间\",{\"1\":{\"847\":1}}],[\"它由18层组成\",{\"1\":{\"497\":1}}],[\"它由两个部分组成\",{\"1\":{\"408\":1}}],[\"它由大量带有位置信息的点组成\",{\"1\":{\"159\":1}}],[\"它比谷歌的jft\",{\"1\":{\"407\":1}}],[\"它代表着一种基于对比文本\",{\"1\":{\"406\":1}}],[\"它通常用于像\",{\"1\":{\"403\":1}}],[\"它通过简单地对整个生成过程加上条件限制来修改上一节中的数学模型\",{\"1\":{\"952\":1}}],[\"它通过位置编码将序列中词的位置信息注入到输入中\",{\"1\":{\"741\":1}}],[\"它通过固定的周期性函数\",{\"1\":{\"706\":1}}],[\"它通过一组非线性基函数\",{\"1\":{\"500\":1}}],[\"它通过广播\",{\"1\":{\"472\":1}}],[\"它通过处理器对输入文本进行处理\",{\"1\":{\"410\":1}}],[\"它通过让学生特征去匹配由\",{\"1\":{\"282\":1}}],[\"它通过学习一个相似度函数\",{\"1\":{\"199\":1}}],[\"它基于jaccard指数\",{\"1\":{\"588\":1}}],[\"它基于\",{\"1\":{\"377\":1}}],[\"它基于前面的特征提取模块\",{\"1\":{\"155\":1,\"156\":1}}],[\"它非常灵活\",{\"1\":{\"353\":1}}],[\"它能描述的颜色是有限而确定的\",{\"1\":{\"925\":1}}],[\"它能够根据输入\",{\"1\":{\"945\":1}}],[\"它能够让语言模型与其他数据来源连接\",{\"1\":{\"831\":1}}],[\"它能够成功的两个关键点\",{\"1\":{\"823\":1}}],[\"它能够从每个子集中提取有用的信息或特征\",{\"1\":{\"131\":1}}],[\"它能高效计算两种损失\",{\"1\":{\"272\":1}}],[\"它们都可以生成我们希望逼近的输出分布\",{\"1\":{\"949\":1}}],[\"它们都是在数万亿个字符上训练的\",{\"1\":{\"823\":1}}],[\"它们都是按照其序列长度的20\",{\"1\":{\"700\":1}}],[\"它们都是被过度参数化的\",{\"1\":{\"606\":1}}],[\"它们仍可能无法提供准确的答案\",{\"1\":{\"828\":1}}],[\"它们为自然语言理解和生成任务提供了强大的工具\",{\"1\":{\"824\":1}}],[\"它们的平方距离越来越集中在\",{\"1\":{\"874\":1}}],[\"它们的出现会导致比独立情形更大的方差\",{\"1\":{\"863\":1}}],[\"它们的多语言能力使得跨文化和跨语言的应用变得更加容易\",{\"1\":{\"824\":1}}],[\"它们的能力依次递增\",{\"1\":{\"823\":1}}],[\"它们在海量的文本数据上进行训练\",{\"1\":{\"822\":1}}],[\"它们在\",{\"1\":{\"696\":1}}],[\"它们有更小的内在维度\",{\"1\":{\"610\":1}}],[\"它们分布在\",{\"1\":{\"502\":1}}],[\"它们是离散的信号\",{\"1\":{\"353\":1}}],[\"它们统一了视觉预训练和视觉\",{\"1\":{\"269\":1}}],[\"它们只学习视觉模态模型\",{\"1\":{\"269\":1}}],[\"它们通过大规模预训练展示出零样本\",{\"1\":{\"268\":1}}],[\"它们共同帮助我们判断模型是否真正理解语言引导下的功能区域语义\",{\"1\":{\"106\":1}}],[\"它既能最大化\",{\"1\":{\"949\":1}}],[\"它既能继承\",{\"1\":{\"268\":1}}],[\"它既能做模态特定的编码\",{\"1\":{\"220\":1}}],[\"它使得\",{\"1\":{\"258\":1}}],[\"它使用双线性插值来避免量化误差\",{\"1\":{\"501\":1}}],[\"它使用\",{\"1\":{\"155\":2,\"282\":1}}],[\"它在小型模型中不明显\",{\"1\":{\"825\":1}}],[\"它在编码每一词的时候都能够注意\",{\"1\":{\"740\":1}}],[\"它在技术上并非从零出发\",{\"1\":{\"650\":1}}],[\"它在某种程度上也具备一定的多语言能力\",{\"1\":{\"650\":1}}],[\"它在这个维度上的偏差也会被直接算入距离\",{\"1\":{\"576\":1}}],[\"它在科研与工业界都很受欢迎\",{\"1\":{\"510\":1}}],[\"它在一个简单的架构下融合了对比学习和图像到文本生成的优势\",{\"1\":{\"270\":1}}],[\"它在\",{\"1\":{\"220\":1,\"710\":1}}],[\"它负责从输入点云中提取出可用于分类或分割的特征\",{\"1\":{\"154\":1}}],[\"它具有以下特点\",{\"1\":{\"152\":1}}],[\"它是在注意力分数计算完之后加上去的\",{\"1\":{\"710\":1}}],[\"它是在lora的基础上\",{\"1\":{\"607\":1}}],[\"它是用于执行bert\",{\"1\":{\"697\":1}}],[\"它是理解连续性的关键\",{\"1\":{\"489\":1}}],[\"它是\",{\"1\":{\"432\":1,\"822\":1,\"912\":1}}],[\"它是一种静态的位置信息表示\",{\"1\":{\"706\":1}}],[\"它是一种用于度量集合之间相似性的指标\",{\"1\":{\"590\":1}}],[\"它是一种图文编码\",{\"1\":{\"268\":1}}],[\"它是一个常用于离散分布中采样的技巧\",{\"1\":{\"897\":1}}],[\"它是一个用于计算在给定观测数据\",{\"1\":{\"877\":1}}],[\"它是一个函数\",{\"1\":{\"846\":1}}],[\"它是一个指向内存中某个元素的指针偏移量\",{\"1\":{\"544\":1}}],[\"它是一个非常强大且灵活的张量操作函数\",{\"1\":{\"100\":1}}],[\"它是无序点云数据特征提取的高效算法\",{\"1\":{\"131\":1}}],[\"它包含了每个特征的方差\",{\"1\":{\"578\":1}}],[\"它包含了这个区域内所有点的信息\",{\"1\":{\"137\":1}}],[\"它包含\",{\"1\":{\"115\":1}}],[\"它常用于图像检索\",{\"1\":{\"106\":1}}],[\"它结合了两种\",{\"1\":{\"587\":1}}],[\"它结合了均方误差\",{\"1\":{\"259\":1}}],[\"它结合了\",{\"1\":{\"102\":1}}],[\"它的值在前向传播时取\",{\"1\":{\"959\":1}}],[\"它的目的是把图像压缩成离散向量\",{\"1\":{\"956\":1}}],[\"它的目标是在生成图像时\",{\"1\":{\"924\":1}}],[\"它的目标是在一个由一个正样本和多个负样本构成的集合中\",{\"1\":{\"355\":1}}],[\"它的目标是为各种大型语言模型应用提供通用接口\",{\"1\":{\"831\":1}}],[\"它的目标是\",{\"1\":{\"152\":1}}],[\"它的输入是\",{\"1\":{\"898\":1}}],[\"它的核心思想可以分为三个步骤\",{\"1\":{\"894\":1}}],[\"它的核心思想是通过不断合并最常见的字符对来构建一个高效的词汇表\",{\"1\":{\"594\":1}}],[\"它的核心思想是\",{\"1\":{\"137\":1,\"505\":1}}],[\"它的尾部非常厚重\",{\"1\":{\"868\":1}}],[\"它的概率密度函数全部集中在正实数轴上\",{\"1\":{\"868\":1}}],[\"它的概率密度函数\",{\"1\":{\"867\":1,\"868\":1}}],[\"它的定义是\",{\"1\":{\"847\":1}}],[\"它的样本空间是连续的\",{\"1\":{\"847\":1}}],[\"它的语法是\",{\"1\":{\"734\":1}}],[\"它的波动幅度很小\",{\"1\":{\"706\":1}}],[\"它的应用自不必提\",{\"1\":{\"614\":1}}],[\"它的更新可表示为\",{\"1\":{\"611\":1}}],[\"它的结构包括一个输入层\",{\"1\":{\"497\":1}}],[\"它的名字和文档字符串也被覆盖了\",{\"1\":{\"454\":1}}],[\"它的作用是给定一个完整的句子\",{\"1\":{\"735\":1}}],[\"它的作用是对一个样本进行两次增强\",{\"1\":{\"359\":1}}],[\"它的作用是负责从输入的点云数据中采样关键点\",{\"1\":{\"137\":1}}],[\"它的基本思想是把一个\",{\"1\":{\"355\":1}}],[\"它的\",{\"1\":{\"235\":1}}],[\"它的本质是\",{\"1\":{\"100\":1}}],[\"它的主要目标是\",{\"1\":{\"100\":1}}],[\"它的设计核心包括\",{\"1\":{\"19\":1}}],[\"它不具有生成能力\",{\"1\":{\"735\":1}}],[\"它不会\",{\"1\":{\"735\":1}}],[\"它不会参与点积运算\",{\"1\":{\"710\":1}}],[\"它不依赖任何手工设计的公式\",{\"1\":{\"707\":1}}],[\"它不仅是\",{\"1\":{\"814\":1}}],[\"它不仅考虑两个点之间的差异\",{\"1\":{\"577\":1}}],[\"它不仅在多模态任务上表现优异\",{\"1\":{\"220\":1}}],[\"它不是理想的后验\",{\"1\":{\"950\":1}}],[\"它不是单纯地\",{\"1\":{\"339\":1}}],[\"它不是直接包含原始图像\",{\"1\":{\"65\":1}}],[\"它不使用任何注意力机制\",{\"1\":{\"97\":1}}],[\"它要回答\",{\"1\":{\"83\":1}}],[\"应运而生\",{\"1\":{\"952\":1}}],[\"应对边界情况\",{\"1\":{\"836\":1}}],[\"应被及时回收\",{\"1\":{\"806\":1}}],[\"应为\",{\"1\":{\"545\":1}}],[\"应使用\",{\"0\":{\"182\":1}}],[\"应该能生成\",{\"1\":{\"947\":1}}],[\"应该包含哪些信息\",{\"1\":{\"944\":1}}],[\"应该把\",{\"1\":{\"691\":1}}],[\"应该采用什么样的方法来进行训练\",{\"1\":{\"413\":1}}],[\"应该更高地加权第二个向量\",{\"1\":{\"142\":1}}],[\"应该关注哪些新能力\",{\"1\":{\"837\":1}}],[\"应该关注哪些点云点\",{\"1\":{\"65\":1}}],[\"应该关注图像中的哪些位置\",{\"1\":{\"65\":1}}],[\"应用贝叶斯公式\",{\"1\":{\"945\":1}}],[\"应用就可以上线体验了\",{\"1\":{\"836\":1}}],[\"应用程序的开发\",{\"1\":{\"834\":1}}],[\"应用程序的开发提供了坚实的基础\",{\"1\":{\"834\":1}}],[\"应用程序部署到云端\",{\"1\":{\"834\":1}}],[\"应用的复杂度\",{\"1\":{\"833\":1}}],[\"应用的开发和部署提供了坚实的基础\",{\"1\":{\"833\":1}}],[\"应用中的每一步操作及其输入输出有一个清晰的认识\",{\"1\":{\"833\":1}}],[\"应用开发的大一统基座模型\",{\"1\":{\"826\":1}}],[\"应用于真实的\",{\"1\":{\"658\":1}}],[\"应用于多模态理解与生成任务\",{\"1\":{\"222\":1}}],[\"应用层\",{\"0\":{\"623\":1}}],[\"应用量化低秩适应\",{\"1\":{\"614\":1}}],[\"应用自定义的权重初始化函数\",{\"1\":{\"431\":1}}],[\"应用自注意力\",{\"1\":{\"120\":1}}],[\"应用注意力掩码\",{\"1\":{\"420\":1}}],[\"应用注意力权重\",{\"1\":{\"401\":1}}],[\"应用\",{\"1\":{\"266\":1,\"402\":1,\"429\":1,\"574\":1,\"823\":2,\"831\":1,\"832\":1,\"893\":1}}],[\"应用场景适应性受限\",{\"1\":{\"828\":1}}],[\"应用场景举例\",{\"1\":{\"346\":1}}],[\"应用场景\",{\"1\":{\"160\":1,\"589\":1}}],[\"应用到原始点云上\",{\"1\":{\"152\":1}}],[\"应用改良的交叉注意力机制\",{\"1\":{\"60\":1}}],[\"防止梯度传播到某部分\",{\"1\":{\"963\":1}}],[\"防止跨模态预测\",{\"1\":{\"893\":2}}],[\"防止跨字符类别的合并\",{\"1\":{\"640\":1}}],[\"防止同一个函数被多次添加到func列表中\",{\"1\":{\"805\":1}}],[\"防止溢出\",{\"1\":{\"710\":1}}],[\"防止出现\",{\"1\":{\"592\":1}}],[\"防止被背景淹没\",{\"1\":{\"589\":1}}],[\"防止模型只关注简单样本\",{\"1\":{\"589\":1}}],[\"防止模型偏向特定句式或长度\",{\"1\":{\"87\":1}}],[\"防止前景点\",{\"1\":{\"589\":1}}],[\"防止除零错误\",{\"1\":{\"586\":1,\"587\":1,\"588\":1,\"589\":1}}],[\"防止除以零\",{\"1\":{\"586\":2,\"587\":1,\"590\":1,\"592\":1}}],[\"防止除以\",{\"1\":{\"213\":1}}],[\"防止内积过大导致\",{\"1\":{\"537\":1}}],[\"防止原\",{\"1\":{\"386\":1}}],[\"防止点积结果过大\",{\"1\":{\"380\":1}}],[\"防止某一维度主导表示\",{\"1\":{\"285\":1}}],[\"防止过拟合\",{\"1\":{\"266\":1,\"341\":1,\"430\":4,\"656\":1}}],[\"防止训练不稳定\",{\"1\":{\"259\":1}}],[\"防止重复生成\",{\"1\":{\"188\":1}}],[\"防止忽略小区域\",{\"1\":{\"102\":1}}],[\"防止其影响后续计算\",{\"1\":{\"100\":1}}],[\"防止\",{\"1\":{\"65\":1,\"213\":1,\"274\":1,\"592\":2}}],[\"仅比\",{\"1\":{\"944\":1}}],[\"仅比人类低几分\",{\"1\":{\"648\":1}}],[\"仅屏蔽其右侧\",{\"1\":{\"926\":1}}],[\"仅保留最可能的\",{\"1\":{\"898\":1}}],[\"仅取最后一个位置的\",{\"1\":{\"898\":1}}],[\"仅包含文本部分\",{\"1\":{\"893\":1}}],[\"仅包含全局视图\",{\"1\":{\"293\":1}}],[\"仅解码器\",{\"1\":{\"887\":1}}],[\"仅当config\",{\"1\":{\"807\":1}}],[\"仅终端变量的导数需要被保留\",{\"1\":{\"807\":1}}],[\"仅供参考和学习\",{\"1\":{\"701\":1}}],[\"仅177gb\",{\"1\":{\"668\":1}}],[\"仅为49\",{\"1\":{\"648\":1}}],[\"仅提供任务描述\",{\"1\":{\"647\":1}}],[\"仅仅需要小小修改模型架构\",{\"1\":{\"625\":1}}],[\"仅借鉴了encoder结构\",{\"1\":{\"422\":1}}],[\"仅能和自己的\",{\"1\":{\"418\":1}}],[\"仅文本\",{\"1\":{\"385\":1,\"895\":1}}],[\"仅文本模态\",{\"1\":{\"380\":1}}],[\"仅图像模态\",{\"1\":{\"380\":1}}],[\"仅从其中提取特征\",{\"1\":{\"352\":1}}],[\"仅\",{\"1\":{\"343\":1,\"557\":1}}],[\"仅训练分类头\",{\"1\":{\"435\":1}}],[\"仅训练语言专家进行掩码语言建模\",{\"1\":{\"374\":1}}],[\"仅训练\",{\"1\":{\"311\":1}}],[\"仅训练新增参数\",{\"1\":{\"316\":1}}],[\"仅训练新增的\",{\"1\":{\"305\":1}}],[\"仅训练新引入的参数\",{\"1\":{\"306\":1}}],[\"仅微调\",{\"1\":{\"305\":1}}],[\"仅用\",{\"1\":{\"823\":1}}],[\"仅用squad数据\",{\"1\":{\"685\":1}}],[\"仅用internvit\",{\"1\":{\"303\":1}}],[\"仅用于预训练\",{\"1\":{\"214\":1}}],[\"仅用于生成\",{\"1\":{\"208\":1}}],[\"仅支持单模态任务\",{\"1\":{\"303\":1}}],[\"仅两张全局裁剪用于教师监督\",{\"1\":{\"293\":1}}],[\"仅学生网络\",{\"1\":{\"293\":1}}],[\"仅学习文本特征\",{\"1\":{\"268\":1}}],[\"仅前\",{\"1\":{\"293\":1}}],[\"仅通过增加语言模型的规模\",{\"1\":{\"653\":1}}],[\"仅通过自然语言提示和示例\",{\"1\":{\"651\":1}}],[\"仅通过few\",{\"1\":{\"648\":1}}],[\"仅通过语言建模目标就能在零样本设置下完成多种nlp任务\",{\"1\":{\"643\":1}}],[\"仅通过\",{\"1\":{\"293\":1,\"309\":1}}],[\"仅通过扩充原始数据进行更长时间训练\",{\"1\":{\"181\":1}}],[\"仅需自然语言提示\",{\"1\":{\"641\":1}}],[\"仅需4个tanh神经元即可高精度拟合\",{\"1\":{\"500\":1}}],[\"仅需24小时\",{\"1\":{\"291\":1}}],[\"仅需\",{\"1\":{\"280\":1}}],[\"仅需在教师输出上应用\",{\"1\":{\"280\":1}}],[\"仅依赖图像标注\",{\"1\":{\"268\":1}}],[\"仅返回被遮挡位置的预测结果\",{\"1\":{\"266\":1}}],[\"仅返回\",{\"1\":{\"256\":1}}],[\"仅返回预测结果\",{\"1\":{\"64\":1}}],[\"仅对\",{\"1\":{\"208\":1}}],[\"仅参数使用\",{\"1\":{\"205\":1}}],[\"仅在无对应文本\",{\"1\":{\"887\":1}}],[\"仅在原张量不连续时复制\",{\"1\":{\"494\":1}}],[\"仅在分布式环境下启用\",{\"1\":{\"385\":1}}],[\"仅在\",{\"1\":{\"179\":1,\"380\":1,\"611\":1}}],[\"仅在注意力生成分支或特征变换分支单独加入相对位置编码\",{\"1\":{\"117\":1}}],[\"仅靠\",{\"1\":{\"157\":1}}],[\"仅靠局部特征很难判断某个点属于哪个部件\",{\"1\":{\"156\":1}}],[\"仅使用前\",{\"1\":{\"895\":1}}],[\"仅使用cpu就能完成整个训练过程\",{\"1\":{\"696\":1}}],[\"仅使用交叉熵损失\",{\"1\":{\"592\":1}}],[\"仅使用图文对比损失训练的模型性能显著低于我们统一训练框架下的模型\",{\"1\":{\"376\":1}}],[\"仅使用最后一层视觉特征\",{\"1\":{\"344\":1}}],[\"仅使用\",{\"1\":{\"305\":1,\"592\":1}}],[\"仅使用当前层特征进行自增强\",{\"1\":{\"122\":1}}],[\"仅使用全局特征增强当前层特征\",{\"1\":{\"122\":1}}],[\"仅更新插入的\",{\"1\":{\"346\":1}}],[\"仅更新投影矩阵\",{\"1\":{\"341\":1}}],[\"仅更新\",{\"1\":{\"34\":1}}],[\"语音\",{\"1\":{\"823\":1}}],[\"语法简洁\",{\"1\":{\"807\":1}}],[\"语法纠错等任务中\",{\"1\":{\"648\":1}}],[\"语句连贯性\",{\"1\":{\"626\":1}}],[\"语言中\",{\"1\":{\"878\":1}}],[\"语言中间件\",{\"1\":{\"304\":1}}],[\"语言识别\",{\"1\":{\"667\":1}}],[\"语言多样性不足\",{\"1\":{\"658\":1}}],[\"语言建模能力\",{\"0\":{\"898\":1}}],[\"语言建模的研究可以追溯到\",{\"1\":{\"822\":1}}],[\"语言建模的核心框架\",{\"1\":{\"640\":1}}],[\"语言建模任务评估\",{\"1\":{\"641\":1}}],[\"语言建模头\",{\"1\":{\"403\":1}}],[\"语言可接受性\",{\"1\":{\"635\":1}}],[\"语言联合预训练\",{\"1\":{\"377\":1}}],[\"语言检索任务\",{\"1\":{\"377\":1}}],[\"语言检索\",{\"1\":{\"375\":1}}],[\"语言分类\",{\"1\":{\"375\":1}}],[\"语言分类任务上性能优异\",{\"1\":{\"368\":1}}],[\"语言分类任务表现有限\",{\"1\":{\"368\":1}}],[\"语言对齐与生成能力\",{\"1\":{\"312\":1}}],[\"语言对比训练\",{\"1\":{\"305\":1}}],[\"语言生成训练\",{\"1\":{\"305\":1}}],[\"语言大模型\",{\"1\":{\"296\":1}}],[\"语言大规模预训练能提升多种视觉语言任务\",{\"1\":{\"194\":1}}],[\"语言理解任务\",{\"1\":{\"271\":1}}],[\"语言和模态间的\",{\"1\":{\"225\":1}}],[\"语言基础模型的发展却相对滞后\",{\"1\":{\"296\":1}}],[\"语言基础模型发展滞后于大型语言模型\",{\"1\":{\"295\":1}}],[\"语言基础模型\",{\"1\":{\"295\":1,\"313\":1}}],[\"语言基础模型通常需要\",{\"1\":{\"220\":1}}],[\"语言基准任务上表现优异\",{\"1\":{\"269\":1}}],[\"语言基准测试上都取得了\",{\"1\":{\"225\":1}}],[\"语言专家捕捉跨模态交互\",{\"1\":{\"372\":1}}],[\"语言专家\",{\"1\":{\"222\":2,\"224\":3,\"368\":2,\"372\":2}}],[\"语言下游任务中\",{\"1\":{\"221\":1}}],[\"语言任务中取得了竞争力甚至更优的性能\",{\"1\":{\"388\":1}}],[\"语言任务中取得了最新结果\",{\"1\":{\"220\":1}}],[\"语言任务中取得显著进展\",{\"1\":{\"326\":1}}],[\"语言任务中表现出色\",{\"1\":{\"295\":1}}],[\"语言任务中\",{\"1\":{\"268\":1}}],[\"语言任务\",{\"1\":{\"220\":1,\"222\":1,\"225\":1}}],[\"语言任务上都取得了最新的迁移性能\",{\"1\":{\"220\":1}}],[\"语言任务上以零样本方式迁移也表现优异\",{\"1\":{\"165\":1}}],[\"语言表示学习主要分为两类\",{\"1\":{\"195\":1}}],[\"语言研究的发展\",{\"1\":{\"183\":1}}],[\"语言预训练方法大体可分为两类\",{\"1\":{\"369\":1}}],[\"语言预训练模型\",{\"1\":{\"368\":3,\"377\":1}}],[\"语言预训练的掩码预测\",{\"1\":{\"217\":1}}],[\"语言预训练所使用的图文对大多来自网页\",{\"1\":{\"202\":1}}],[\"语言预训练框架\",{\"1\":{\"183\":1}}],[\"语言预训练中使用合成图像描述的独特优势\",{\"1\":{\"169\":1}}],[\"语言预训练\",{\"0\":{\"167\":1},\"1\":{\"165\":1,\"269\":2,\"388\":2}}],[\"语言查询特征\",{\"1\":{\"100\":1}}],[\"语言含义\",{\"1\":{\"100\":1}}],[\"语言引导的点特征筛选\",{\"1\":{\"96\":1}}],[\"语言引导的\",{\"1\":{\"94\":1,\"106\":1}}],[\"语言引导下的\",{\"1\":{\"84\":1}}],[\"语言辅助方法\",{\"1\":{\"75\":1}}],[\"语言后半段\",{\"1\":{\"67\":1}}],[\"语言前半段\",{\"1\":{\"67\":1}}],[\"语言部分的\",{\"1\":{\"67\":1}}],[\"语言\",{\"1\":{\"67\":1,\"220\":2,\"225\":1,\"368\":1}}],[\"语言模型发展脉络\",{\"1\":{\"671\":1}}],[\"语言模型会生成有害或偏见内容\",{\"1\":{\"655\":1}}],[\"语言模型可能直接学会执行任务\",{\"1\":{\"642\":1}}],[\"语言模型本身可以通过观察任务的自然语言演示\",{\"1\":{\"640\":1}}],[\"语言模型本身可能通过无监督学习捕捉任务相关的知识\",{\"1\":{\"639\":1}}],[\"语言模型通过预测多样化文本中的任务演示\",{\"1\":{\"640\":1}}],[\"语言模型通过链式法则计算联合概率\",{\"1\":{\"640\":1}}],[\"语言模型通常需要多个预训练目标\",{\"1\":{\"223\":1}}],[\"语言模型使用多层的\",{\"1\":{\"629\":1}}],[\"语言模型从nous\",{\"1\":{\"330\":1}}],[\"语言模型\",{\"1\":{\"66\":1,\"329\":1,\"341\":1,\"342\":1,\"823\":1}}],[\"语言指令理解特征\",{\"1\":{\"64\":1}}],[\"语义角色标注\",{\"1\":{\"736\":1}}],[\"语义解析\",{\"1\":{\"655\":1}}],[\"语义比较等\",{\"1\":{\"648\":1}}],[\"语义文本相似度数据集\",{\"1\":{\"634\":1}}],[\"语义组块chuking\",{\"1\":{\"627\":1}}],[\"语义相似度\",{\"1\":{\"634\":2}}],[\"语义相似度评估\",{\"1\":{\"625\":1,\"636\":1}}],[\"语义相识度\",{\"1\":{\"626\":1}}],[\"语义信息不应该发生变化\",{\"1\":{\"350\":1}}],[\"语义信息表达能力弱\",{\"1\":{\"26\":1}}],[\"语义字典\",{\"1\":{\"293\":1}}],[\"语义连续\",{\"1\":{\"263\":1}}],[\"语义\",{\"1\":{\"248\":1}}],[\"语义视觉\",{\"1\":{\"217\":1}}],[\"语义一致性\",{\"1\":{\"293\":1,\"648\":1}}],[\"语义一致\",{\"1\":{\"215\":1}}],[\"语义分割任务需要对图像中的每个像素进行分类\",{\"1\":{\"584\":1}}],[\"语义分割任务中\",{\"1\":{\"584\":1}}],[\"语义分割可以被看作是像素级别的图像分割\",{\"1\":{\"584\":1}}],[\"语义分割不仅需要识别图像中的物体\",{\"1\":{\"584\":1}}],[\"语义分割是计算机视觉领域中的一项任务\",{\"1\":{\"584\":1}}],[\"语义分割中常用的损失函数\",{\"0\":{\"583\":1},\"1\":{\"583\":1}}],[\"语义分割等\",{\"1\":{\"229\":1}}],[\"语义分割上取得显著提升\",{\"1\":{\"215\":1}}],[\"语义分割\",{\"0\":{\"239\":1,\"584\":1},\"1\":{\"116\":2,\"220\":1,\"295\":1,\"308\":1,\"584\":1}}],[\"语义电流\",{\"1\":{\"83\":1}}],[\"语义表示\",{\"1\":{\"83\":1}}],[\"语义对齐\",{\"1\":{\"57\":1}}],[\"语义空间受限\",{\"1\":{\"30\":1}}],[\"降为用4bit来表示\",{\"1\":{\"607\":1}}],[\"降训练成本\",{\"1\":{\"607\":1}}],[\"降低了产生幻觉的概率\",{\"1\":{\"830\":1}}],[\"降低了不同架构之间的切换成本\",{\"1\":{\"510\":1}}],[\"降低幻觉\",{\"1\":{\"830\":1}}],[\"降低部署风险\",{\"1\":{\"658\":1}}],[\"降低易分类样本的权重\",{\"1\":{\"589\":1}}],[\"降低码本向量维度可以提高码本利用率\",{\"1\":{\"215\":1}}],[\"降低动量负样本带来的扰动\",{\"1\":{\"204\":1}}],[\"降低分辨率\",{\"1\":{\"121\":1}}],[\"降低点云分辨率同时增加特征维度\",{\"1\":{\"121\":1}}],[\"降低点云分辨率\",{\"1\":{\"121\":1}}],[\"降维\",{\"1\":{\"122\":1}}],[\"降维的线性投影\",{\"1\":{\"115\":1}}],[\"降维适配器\",{\"0\":{\"68\":1},\"1\":{\"64\":1,\"68\":1}}],[\"降到\",{\"1\":{\"48\":1}}],[\"返回经过筛选后的\",{\"1\":{\"896\":1}}],[\"返回第1维度的元素数\",{\"1\":{\"808\":1}}],[\"返回构建得到的单个样本列表\",{\"1\":{\"698\":1}}],[\"返回处理后的句子列表\",{\"1\":{\"696\":1}}],[\"返回合并最高频字符对后的vocab\",{\"1\":{\"595\":1}}],[\"返回出现次数最多的前\",{\"1\":{\"516\":1}}],[\"返回对应数量的切分结果\",{\"1\":{\"513\":1}}],[\"返回在\",{\"1\":{\"484\":1}}],[\"返回值形如\",{\"1\":{\"733\":1}}],[\"返回值是按输入顺序交错的切分结果\",{\"1\":{\"513\":1}}],[\"返回值\",{\"1\":{\"481\":1,\"482\":1,\"485\":1,\"490\":1,\"491\":1,\"514\":1}}],[\"返回反向索引\",{\"1\":{\"480\":1}}],[\"返回计数\",{\"1\":{\"480\":1}}],[\"返回输入张量中\",{\"1\":{\"480\":1}}],[\"返回一个内存连续的\",{\"1\":{\"491\":1}}],[\"返回一个一维张量\",{\"1\":{\"485\":1}}],[\"返回一个长度为\",{\"1\":{\"483\":1}}],[\"返回一个和\",{\"1\":{\"481\":1}}],[\"返回一个新的张量\",{\"1\":{\"481\":1}}],[\"返回一个新张量\",{\"1\":{\"476\":1}}],[\"返回一个函数\",{\"1\":{\"447\":1}}],[\"返回视图\",{\"1\":{\"472\":1}}],[\"返回具有新形状的张量\",{\"1\":{\"470\":1}}],[\"返回具有新形状\",{\"1\":{\"469\":1}}],[\"返回分类标记对应的特征\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"返回数据集中图像的数量\",{\"1\":{\"424\":1}}],[\"返回数据集名称\",{\"1\":{\"382\":1}}],[\"返回字典格式结果\",{\"1\":{\"420\":1}}],[\"返回预测出来的\",{\"1\":{\"403\":1}}],[\"返回预测得分\",{\"1\":{\"403\":1}}],[\"返回结果字典\",{\"1\":{\"384\":1,\"385\":1}}],[\"返回结构化输出\",{\"1\":{\"207\":1}}],[\"返回推理结果字典\",{\"1\":{\"384\":1}}],[\"返回测试\",{\"1\":{\"382\":1}}],[\"返回验证\",{\"1\":{\"382\":1}}],[\"返回训练\",{\"1\":{\"382\":1}}],[\"返回多个增强后的\",{\"1\":{\"293\":1}}],[\"返回两个任务的结果\",{\"1\":{\"699\":1}}],[\"返回两个结果\",{\"1\":{\"274\":1}}],[\"返回两张不同视角图像\",{\"1\":{\"264\":1}}],[\"返回所有\",{\"1\":{\"266\":1}}],[\"返回本次新增的遮挡patch数量\",{\"1\":{\"263\":1}}],[\"返回本次新增遮挡的patch数量\",{\"1\":{\"263\":1}}],[\"返回最小的\",{\"1\":{\"488\":1}}],[\"返回最大的\",{\"1\":{\"488\":1}}],[\"返回最终生成的遮挡掩码\",{\"1\":{\"263\":1}}],[\"返回最终的簇中心和每个簇的样本数\",{\"1\":{\"213\":2}}],[\"返回最近邻索引和实际距离\",{\"1\":{\"119\":1}}],[\"返回最近邻点的索引和对应的距离值\",{\"1\":{\"119\":1}}],[\"返回掩码后的\",{\"1\":{\"208\":1}}],[\"返回三个\",{\"1\":{\"192\":1}}],[\"返回itc损失和itm损失\",{\"1\":{\"190\":1}}],[\"返回每个点的分类结果和抽象特征\",{\"1\":{\"146\":1}}],[\"返回位置索引\",{\"1\":{\"137\":1}}],[\"返回的值不保证顺序\",{\"1\":{\"488\":1}}],[\"返回的值按顺序排列\",{\"1\":{\"488\":1}}],[\"返回的张量虽然是视图\",{\"1\":{\"468\":1}}],[\"返回的\",{\"1\":{\"384\":1,\"451\":1}}],[\"返回的每个\",{\"1\":{\"359\":1}}],[\"返回的caption前不添加prompt\",{\"1\":{\"192\":1}}],[\"返回的是一个字典的子类\",{\"1\":{\"516\":1}}],[\"返回的是软概率分布\",{\"1\":{\"257\":1}}],[\"返回的是\",{\"1\":{\"123\":1,\"293\":1,\"454\":1,\"482\":1,\"520\":1}}],[\"返回的高维隐向量维度为\",{\"1\":{\"54\":1}}],[\"返回相同格式的数据\",{\"1\":{\"120\":1,\"121\":1}}],[\"返回目标物体区域特征图\",{\"1\":{\"83\":1}}],[\"返回拼接好的输入和\",{\"1\":{\"67\":1}}],[\"返回\",{\"1\":{\"64\":1,\"83\":1,\"92\":1,\"102\":1,\"137\":1,\"145\":1,\"146\":2,\"152\":1,\"154\":1,\"208\":1,\"213\":1,\"257\":1,\"380\":1,\"382\":1,\"440\":1,\"453\":1,\"552\":1,\"586\":2,\"587\":1,\"588\":1,\"589\":1,\"590\":1,\"592\":1,\"710\":2,\"898\":1}}],[\"函数变化可能很陡\",{\"1\":{\"917\":1}}],[\"函数越平滑\",{\"1\":{\"917\":1}}],[\"函数负责绑定运算符方法\",{\"1\":{\"810\":1}}],[\"函数和\",{\"1\":{\"801\":1}}],[\"函数在处的导数定义为\",{\"1\":{\"769\":1}}],[\"函数的python化\",{\"0\":{\"790\":1}}],[\"函数的表达式为\",{\"1\":{\"779\":1}}],[\"函数的连续调用\",{\"0\":{\"764\":1}}],[\"函数的作用是将整个点云视为一个\",{\"1\":{\"137\":1}}],[\"函数类的设计\",{\"0\":{\"761\":1}}],[\"函数与计算图\",{\"0\":{\"760\":1}}],[\"函数生成目标序列的概率分布\",{\"1\":{\"741\":1}}],[\"函数复合\",{\"1\":{\"500\":3}}],[\"函数空间\",{\"1\":{\"500\":1}}],[\"函数替换器\",{\"1\":{\"457\":1}}],[\"函数替代了原来的\",{\"1\":{\"451\":1}}],[\"函数上\",{\"1\":{\"454\":1}}],[\"函数返回一个函数\",{\"1\":{\"447\":1}}],[\"函数接收另一个函数作为参数\",{\"1\":{\"447\":1}}],[\"函数定义了变量之间的对应关系\",{\"1\":{\"760\":1}}],[\"函数定义\",{\"1\":{\"444\":1,\"810\":1,\"811\":3}}],[\"函数定义见\",{\"1\":{\"106\":1}}],[\"函数或代码块内部定义的名字\",{\"1\":{\"444\":1}}],[\"函数中\",{\"1\":{\"355\":1}}],[\"函数完成了\",{\"1\":{\"265\":1}}],[\"函数来实现上述过程\",{\"1\":{\"257\":1}}],[\"函数选出距离最小的\",{\"1\":{\"119\":1}}],[\"函数\",{\"0\":{\"917\":1},\"1\":{\"64\":1,\"100\":1,\"150\":1,\"160\":1,\"382\":1,\"410\":1,\"412\":1,\"424\":1,\"450\":1,\"451\":1,\"454\":1,\"457\":1,\"590\":1,\"867\":2,\"904\":1,\"917\":1,\"946\":1}}],[\"设随机向量\",{\"1\":{\"944\":1}}],[\"设随机变量\",{\"1\":{\"846\":1,\"858\":1}}],[\"设真实标签是\",{\"1\":{\"910\":1}}],[\"设每次抛硬币结果\",{\"1\":{\"904\":1}}],[\"设样本数据为\",{\"1\":{\"904\":1}}],[\"设想我们要从一张二维图像\",{\"1\":{\"878\":1}}],[\"设定文本序列的最大长度\",{\"1\":{\"898\":1}}],[\"设定传入的input和返回的output均为variable类型\",{\"1\":{\"762\":1}}],[\"设定下\",{\"1\":{\"648\":1}}],[\"设两维特征的协方差矩阵是\",{\"1\":{\"578\":1}}],[\"设备\",{\"1\":{\"265\":1}}],[\"设动量模型在图像\",{\"1\":{\"202\":1}}],[\"设动量编码器生成的相似度为\",{\"1\":{\"202\":1}}],[\"设\",{\"1\":{\"200\":1,\"235\":1,\"612\":1,\"846\":1,\"847\":1,\"856\":1,\"859\":1,\"860\":1}}],[\"设为由编码器输出的\",{\"1\":{\"886\":1}}],[\"设为对\",{\"1\":{\"886\":1}}],[\"设为\",{\"1\":{\"188\":1,\"513\":1,\"887\":1,\"893\":1}}],[\"设是一个函数\",{\"1\":{\"160\":1}}],[\"设置重建损失函数\",{\"1\":{\"899\":1}}],[\"设置其优先级高于ndarray\",{\"1\":{\"809\":1}}],[\"设置变量的creator\",{\"1\":{\"783\":1}}],[\"设置的回调方法cllote\",{\"1\":{\"715\":1}}],[\"设置下均达到sota\",{\"1\":{\"668\":1}}],[\"设置下的表现\",{\"1\":{\"646\":1}}],[\"设置下完成多种自然语言处理任务\",{\"1\":{\"638\":1}}],[\"设置默认参数\",{\"1\":{\"590\":1}}],[\"设置全局参数\",{\"1\":{\"589\":1}}],[\"设置步幅为\",{\"1\":{\"546\":1}}],[\"设置方式\",{\"1\":{\"520\":1}}],[\"设置柱状图的标题\",{\"1\":{\"424\":1}}],[\"设置y坐标\",{\"1\":{\"424\":1}}],[\"设置x坐标\",{\"1\":{\"424\":1}}],[\"设置为\",{\"1\":{\"633\":1}}],[\"设置为已初始化状态\",{\"1\":{\"213\":1}}],[\"设置为评估模式\",{\"1\":{\"107\":1,\"213\":1}}],[\"设置渲染参数\",{\"1\":{\"107\":1}}],[\"设置颜色\",{\"1\":{\"107\":1}}],[\"设置训练轮数\",{\"1\":{\"107\":1}}],[\"设置学习率\",{\"1\":{\"107\":1}}],[\"设置batch\",{\"1\":{\"107\":1}}],[\"设置后台运行\",{\"1\":{\"107\":1}}],[\"设置\",{\"0\":{\"633\":1},\"1\":{\"64\":1,\"102\":1,\"521\":4}}],[\"设计的一种\",{\"1\":{\"847\":1}}],[\"设计的一般原则及技巧\",{\"1\":{\"836\":1}}],[\"设计产品页面\",{\"1\":{\"836\":1}}],[\"设计后\",{\"1\":{\"836\":1}}],[\"设计功能\",{\"1\":{\"836\":1}}],[\"设计功能热图\",{\"1\":{\"75\":1}}],[\"设计合理\",{\"1\":{\"835\":1}}],[\"设计合理的自注意力网络可扩展到复杂大规模场景\",{\"1\":{\"110\":1}}],[\"设计调优\",{\"1\":{\"835\":1}}],[\"设计function类作为基类\",{\"1\":{\"761\":1}}],[\"设计目的\",{\"1\":{\"710\":1}}],[\"设计或改进对比损失函数\",{\"1\":{\"355\":1}}],[\"设计特定任务作为学习信号\",{\"1\":{\"248\":1}}],[\"设计图文对比\",{\"1\":{\"194\":1}}],[\"设计局部\",{\"1\":{\"150\":1}}],[\"设计中的关键选择\",{\"1\":{\"117\":1}}],[\"设计了\",{\"1\":{\"656\":1}}],[\"设计了一个统一架构\",{\"1\":{\"148\":1}}],[\"设计了高度表达能力的\",{\"1\":{\"109\":1}}],[\"设计了多头部功能链式思维\",{\"1\":{\"29\":1}}],[\"设计\",{\"1\":{\"19\":1,\"293\":1}}],[\"∈\",{\"1\":{\"64\":2,\"96\":2,\"97\":1,\"100\":1,\"106\":1,\"205\":1,\"293\":1,\"381\":1,\"588\":1,\"931\":1}}],[\"输入进解码器里\",{\"1\":{\"961\":1}}],[\"输入进模型\",{\"1\":{\"925\":1}}],[\"输入变化多少\",{\"1\":{\"917\":1}}],[\"输入越长\",{\"1\":{\"828\":1}}],[\"输入与输出的关系\",{\"1\":{\"735\":1}}],[\"输入会变成如下结构\",{\"1\":{\"733\":1}}],[\"输入数据格式\",{\"1\":{\"713\":1}}],[\"输入格式\",{\"1\":{\"681\":1,\"733\":1}}],[\"输入格式与下一句预测\",{\"1\":{\"681\":1}}],[\"输入由两个文本片段\",{\"1\":{\"679\":1}}],[\"输入词的val\",{\"1\":{\"663\":1}}],[\"输入词的key\",{\"1\":{\"663\":1}}],[\"输入微小变动影响大\",{\"1\":{\"649\":1}}],[\"输入文本\",{\"1\":{\"898\":1,\"900\":1}}],[\"输入文本序列长度\",{\"1\":{\"892\":1}}],[\"输入文本的嵌入表示\",{\"1\":{\"207\":1}}],[\"输入文档+对话历史+\",{\"1\":{\"640\":1}}],[\"输入通道为\",{\"1\":{\"926\":1}}],[\"输入通道\",{\"1\":{\"924\":2}}],[\"输入通道数\",{\"1\":{\"380\":2,\"924\":1}}],[\"输入通过作者的预训练模型会得到最好的\",{\"1\":{\"630\":1}}],[\"输入序列长度超过max\",{\"1\":{\"709\":1}}],[\"输入序列\",{\"1\":{\"600\":1,\"697\":1,\"737\":1}}],[\"输入维度\",{\"0\":{\"529\":1},\"1\":{\"663\":1}}],[\"输入维度为\",{\"1\":{\"266\":1}}],[\"输入关键字参数\",{\"1\":{\"516\":1}}],[\"输入一个\",{\"1\":{\"660\":1}}],[\"输入一个字典\",{\"1\":{\"516\":1}}],[\"输入一个可迭代对象\",{\"1\":{\"516\":1}}],[\"输入一张图像\",{\"1\":{\"293\":1}}],[\"输入调整等\",{\"1\":{\"467\":1}}],[\"输入加上经过归一化和\",{\"1\":{\"429\":1}}],[\"输入加上经过归一化和注意力层处理后的输出\",{\"1\":{\"429\":1}}],[\"输入encoder的最左侧部分添加了一个0\",{\"1\":{\"427\":1}}],[\"输入的文本\",{\"1\":{\"895\":1}}],[\"输入的图像张量\",{\"1\":{\"426\":1}}],[\"输入的图片尺寸必须为224x224\",{\"1\":{\"425\":1}}],[\"输入的图片尺寸并不是自定义的\",{\"1\":{\"425\":1}}],[\"输入的未归一化的\",{\"1\":{\"257\":1}}],[\"输入包含三部分\",{\"1\":{\"417\":1}}],[\"输入image\",{\"1\":{\"408\":1}}],[\"输入张量\",{\"1\":{\"380\":1,\"480\":1,\"481\":1,\"488\":1,\"503\":1}}],[\"输入模态类型\",{\"1\":{\"380\":1}}],[\"输入任务专用分类器预测标签\",{\"1\":{\"375\":1}}],[\"输入表示\",{\"0\":{\"371\":1},\"1\":{\"640\":1}}],[\"输入要求\",{\"1\":{\"274\":1}}],[\"输入向量为\",{\"1\":{\"233\":1}}],[\"输入分辨率保持\",{\"1\":{\"316\":1}}],[\"输入分辨率\",{\"1\":{\"224\":1}}],[\"输入样本\",{\"1\":{\"213\":1,\"421\":1}}],[\"输入传递到\",{\"1\":{\"212\":1}}],[\"输入解码器\",{\"1\":{\"212\":1}}],[\"输入做前向传播\",{\"1\":{\"208\":1}}],[\"输入到解码器中即可生成图像\",{\"1\":{\"947\":1}}],[\"输入到mlp\",{\"1\":{\"427\":1}}],[\"输入到vae中的图像尺寸\",{\"1\":{\"265\":1}}],[\"输入到\",{\"1\":{\"207\":1,\"434\":1}}],[\"输入网页图像\",{\"1\":{\"173\":1}}],[\"输入形式\",{\"1\":{\"157\":1}}],[\"输入标准化\",{\"0\":{\"152\":1}}],[\"输入特征的维度\",{\"1\":{\"380\":1}}],[\"输入特征的通道数\",{\"1\":{\"145\":1}}],[\"输入特征\",{\"1\":{\"380\":1}}],[\"输入特征图\",{\"1\":{\"213\":1}}],[\"输入特征维度\",{\"1\":{\"69\":1,\"120\":1,\"121\":1,\"122\":1,\"380\":1}}],[\"输入输出示例\",{\"1\":{\"143\":1}}],[\"输入点的特征维度\",{\"1\":{\"137\":1}}],[\"输入点特征维度\",{\"1\":{\"123\":1}}],[\"输入点云变换矩阵\",{\"1\":{\"154\":1}}],[\"输入点云变换网络\",{\"1\":{\"154\":1}}],[\"输入点云可能来自不同角度\",{\"1\":{\"152\":1}}],[\"输入点云可能缺失或含有异常点\",{\"1\":{\"150\":1}}],[\"输入点云可能缺失或包含噪声\",{\"1\":{\"149\":1}}],[\"输入点云坐标\",{\"1\":{\"121\":1}}],[\"输入点云数据\",{\"1\":{\"107\":2}}],[\"输入位置编码\",{\"1\":{\"119\":1}}],[\"输入是结构化的\",{\"1\":{\"631\":1}}],[\"输入是一个\",{\"1\":{\"534\":1}}],[\"输入是一个指令\",{\"1\":{\"339\":1}}],[\"输入是一组点的特征向量及其\",{\"1\":{\"115\":1}}],[\"输入是一组特征向量\",{\"1\":{\"112\":1}}],[\"输入是原始点云\",{\"1\":{\"152\":1}}],[\"输入是相对坐标\",{\"1\":{\"119\":1}}],[\"输入\",{\"1\":{\"83\":2,\"96\":1,\"97\":1,\"98\":1,\"102\":1,\"106\":2,\"119\":2,\"121\":2,\"122\":1,\"143\":1,\"145\":1,\"146\":1,\"228\":1,\"293\":1,\"384\":1,\"385\":3,\"420\":1,\"504\":1,\"505\":1,\"522\":1,\"527\":1,\"529\":1,\"586\":1,\"587\":1,\"640\":1,\"699\":1,\"750\":1,\"892\":1,\"926\":1,\"963\":1}}],[\"输入为图像\",{\"1\":{\"372\":2}}],[\"输入为\",{\"1\":{\"70\":1,\"205\":1,\"250\":1,\"265\":1,\"426\":1,\"656\":1}}],[\"输入图像编码为离散的视觉token\",{\"1\":{\"893\":1}}],[\"输入图像被分割成\",{\"1\":{\"427\":1}}],[\"输入图像被切分为默认的\",{\"1\":{\"236\":1}}],[\"输入图像裁剪数量\",{\"1\":{\"293\":1}}],[\"输入图像的离散\",{\"1\":{\"899\":1}}],[\"输入图像的通道数\",{\"1\":{\"426\":1,\"427\":1}}],[\"输入图像的尺寸\",{\"1\":{\"426\":1,\"427\":1}}],[\"输入图像的高度和宽度\",{\"1\":{\"380\":1}}],[\"输入图像的数值尺度对训练稳定性和收敛非常重要\",{\"1\":{\"264\":1}}],[\"输入图像的标准化处理\",{\"1\":{\"264\":2}}],[\"输入图像的\",{\"1\":{\"263\":1}}],[\"输入图像通道数\",{\"1\":{\"255\":1}}],[\"输入图像大小\",{\"1\":{\"192\":1,\"266\":1,\"892\":1}}],[\"输入图像尺寸\",{\"1\":{\"187\":1,\"255\":1,\"380\":1,\"899\":1,\"900\":1}}],[\"输入图像及文本\",{\"1\":{\"173\":1}}],[\"输入图像\",{\"1\":{\"64\":1,\"212\":1,\"213\":3,\"256\":1,\"293\":1,\"341\":1,\"342\":1,\"899\":1,\"900\":1,\"956\":1}}],[\"输出靠近\",{\"1\":{\"963\":1}}],[\"输出离散编码\",{\"0\":{\"958\":1}}],[\"输出分布不一定是高斯分布\",{\"1\":{\"943\":1}}],[\"输出分组标记\",{\"1\":{\"96\":1}}],[\"输出部分还是保持一致\",{\"1\":{\"921\":1}}],[\"输出第\",{\"1\":{\"921\":1}}],[\"输出概率高\",{\"1\":{\"918\":1}}],[\"输出我们记作\",{\"1\":{\"894\":1}}],[\"输出解析等都来自这个库\",{\"1\":{\"834\":1}}],[\"输出解析\",{\"1\":{\"833\":1}}],[\"输出解释\",{\"1\":{\"733\":1}}],[\"输出效果\",{\"1\":{\"808\":1}}],[\"输出处理\",{\"1\":{\"799\":1}}],[\"输出层\",{\"1\":{\"741\":1,\"926\":1}}],[\"输出后\",{\"1\":{\"737\":1}}],[\"输出做问答预测\",{\"1\":{\"733\":1}}],[\"输出token会与输入tokens\",{\"1\":{\"660\":1}}],[\"输出有害内容\",{\"1\":{\"658\":1}}],[\"输出更符合人类偏好\",{\"1\":{\"657\":1}}],[\"输出更少毒性内容\",{\"1\":{\"657\":1}}],[\"输出相较于标准\",{\"1\":{\"657\":1}}],[\"输出时将\",{\"1\":{\"611\":1}}],[\"输出序列y\",{\"1\":{\"600\":1}}],[\"输出位置\",{\"1\":{\"582\":1}}],[\"输出示例\",{\"1\":{\"553\":1}}],[\"输出会显示训练\",{\"1\":{\"513\":1}}],[\"输出图像形状\",{\"1\":{\"899\":1}}],[\"输出图像像素值是周围四个输入像素加权平均\",{\"1\":{\"505\":1}}],[\"输出图像特征\",{\"1\":{\"206\":1}}],[\"输出大小为\",{\"1\":{\"504\":1}}],[\"输出通道数\",{\"1\":{\"924\":1}}],[\"输出通道\",{\"1\":{\"426\":1,\"924\":2}}],[\"输出投影\",{\"1\":{\"380\":1}}],[\"输出投影后的\",{\"1\":{\"380\":1}}],[\"输出投影层的\",{\"1\":{\"380\":1}}],[\"输出不一致时\",{\"1\":{\"343\":1}}],[\"输出顺序\",{\"1\":{\"293\":1}}],[\"输出仅集中在一个维度\",{\"1\":{\"290\":1}}],[\"输出与教师特征的余弦相似度\",{\"1\":{\"212\":1}}],[\"输出合成描述\",{\"1\":{\"173\":1}}],[\"输出结果之后\",{\"1\":{\"431\":1}}],[\"输出结果都保持不变\",{\"1\":{\"160\":1}}],[\"输出结果\",{\"1\":{\"145\":1,\"213\":1,\"892\":1}}],[\"输出最终的插值后特征\",{\"1\":{\"145\":1}}],[\"输出类别概率\",{\"1\":{\"143\":1}}],[\"输出类别数\",{\"1\":{\"138\":1}}],[\"输出维度为整个\",{\"1\":{\"892\":1}}],[\"输出维度为\",{\"1\":{\"699\":1}}],[\"输出维度为3+in\",{\"1\":{\"121\":1}}],[\"输出维度\",{\"0\":{\"529\":1},\"1\":{\"205\":2}}],[\"输出维度与输入相同\",{\"1\":{\"122\":1}}],[\"输出形状\",{\"1\":{\"121\":1,\"266\":1,\"475\":1}}],[\"输出形状为\",{\"1\":{\"119\":2,\"137\":2}}],[\"输出特征维度\",{\"1\":{\"121\":1,\"122\":1}}],[\"输出是对整个序列的预测\",{\"1\":{\"898\":1}}],[\"输出是对这个回答的评分\",{\"1\":{\"339\":1}}],[\"输出是模型应该生成的响应\",{\"1\":{\"339\":1}}],[\"输出是一个变换矩阵\",{\"1\":{\"152\":1}}],[\"输出是与\",{\"1\":{\"119\":1}}],[\"输出是更新后的点特征\",{\"1\":{\"115\":1}}],[\"输出点数依次为\",{\"1\":{\"116\":1}}],[\"输出一个固定维度的表示\",{\"1\":{\"963\":1}}],[\"输出一个固定维度的向量\",{\"1\":{\"362\":1}}],[\"输出一个softmax过的概率分布\",{\"1\":{\"958\":1}}],[\"输出一个复杂的\",{\"1\":{\"952\":1}}],[\"输出一个\",{\"1\":{\"501\":1}}],[\"输出一个字符\",{\"1\":{\"107\":1}}],[\"输出一个更抽象的\",{\"1\":{\"83\":1}}],[\"输出张量维度为\",{\"1\":{\"660\":1}}],[\"输出张量的最小长度\",{\"1\":{\"485\":1}}],[\"输出张量\",{\"1\":{\"100\":1,\"481\":1}}],[\"输出增强后的点特征\",{\"1\":{\"99\":1}}],[\"输出增强文本特征\",{\"1\":{\"24\":1}}],[\"输出头\",{\"1\":{\"83\":1,\"116\":1}}],[\"输出的变化量最多等于输入变化量\",{\"1\":{\"917\":1}}],[\"输出的dot语言示例包含变量节点\",{\"1\":{\"815\":1}}],[\"输出的目标尺寸\",{\"1\":{\"503\":1}}],[\"输出的统计特性不同\",{\"1\":{\"380\":1}}],[\"输出的样本数据\",{\"1\":{\"213\":1}}],[\"输出的特征图\",{\"1\":{\"501\":1}}],[\"输出的特征分布信息\",{\"1\":{\"213\":1}}],[\"输出的特征向量做聚类迭代\",{\"1\":{\"213\":1}}],[\"输出的特征送入一个小型\",{\"1\":{\"143\":1}}],[\"输出的\",{\"1\":{\"208\":1}}],[\"输出的数据集是经过图文对齐质量优化的图文对集合\",{\"1\":{\"185\":1}}],[\"输出的该\",{\"1\":{\"171\":1}}],[\"输出的层次化点特征恢复到原始点数\",{\"1\":{\"83\":1}}],[\"输出的隐藏状态映射回原始嵌入维度\",{\"1\":{\"68\":1}}],[\"输出逐点\",{\"1\":{\"83\":1}}],[\"输出范围\",{\"1\":{\"70\":1,\"106\":1,\"588\":1}}],[\"输出为标量奖励\",{\"1\":{\"656\":1}}],[\"输出为被\",{\"1\":{\"250\":1}}],[\"输出为二分类\",{\"1\":{\"205\":1}}],[\"输出为\",{\"1\":{\"70\":1,\"146\":1,\"266\":1,\"542\":1,\"926\":1}}],[\"输出每个点的类别概率\",{\"1\":{\"146\":1}}],[\"输出每个点的\",{\"1\":{\"70\":2,\"123\":1}}],[\"输出每个点云点的\",{\"1\":{\"70\":1}}],[\"输出映射回合适维度\",{\"1\":{\"64\":1}}],[\"输出\",{\"1\":{\"32\":1,\"38\":1,\"64\":1,\"70\":1,\"83\":7,\"98\":1,\"112\":1,\"121\":2,\"122\":3,\"136\":1,\"143\":1,\"145\":1,\"154\":1,\"205\":2,\"212\":1,\"213\":2,\"266\":1,\"293\":1,\"380\":2,\"385\":1,\"440\":1,\"441\":1,\"448\":1,\"451\":1,\"454\":2,\"482\":2,\"485\":3,\"486\":1,\"490\":5,\"491\":1,\"504\":1,\"505\":1,\"514\":1,\"527\":1,\"529\":1,\"542\":2,\"544\":4,\"545\":3,\"657\":1,\"660\":1,\"699\":1,\"733\":1,\"757\":2,\"766\":1,\"807\":2,\"808\":8,\"809\":4,\"811\":3,\"832\":1,\"892\":1,\"899\":2,\"926\":2,\"932\":1,\"936\":1,\"963\":1}}],[\"建议伴随伦理审查\",{\"1\":{\"658\":1}}],[\"建议设为较高值\",{\"1\":{\"589\":1}}],[\"建议总是先激活\",{\"1\":{\"557\":1}}],[\"建议\",{\"1\":{\"493\":3,\"522\":1}}],[\"建议在\",{\"1\":{\"470\":1}}],[\"建议用linux或者windows系统进行测试\",{\"1\":{\"62\":1}}],[\"建模了把一个来自正态分布的向量变形成\",{\"1\":{\"925\":1}}],[\"建模的文本和图像\",{\"1\":{\"885\":1}}],[\"建模图像和文本的交互\",{\"1\":{\"375\":1}}],[\"建模图像与文本的深层交互\",{\"1\":{\"370\":1}}],[\"建模\",{\"1\":{\"225\":1,\"708\":1,\"925\":1}}],[\"建模交互上下文以明确功能区域\",{\"1\":{\"72\":1}}],[\"建立了测试机制\",{\"1\":{\"797\":1}}],[\"建立变量与函数的连接\",{\"0\":{\"783\":1}}],[\"建立更强的拒绝机制以识别恶意请求\",{\"1\":{\"658\":1}}],[\"建立物体类型和功能类型的索引映射关系\",{\"1\":{\"92\":1}}],[\"建立目标物体局部交互区域到点云局部区域的特征对应映射关系\",{\"1\":{\"83\":1}}],[\"建立\",{\"1\":{\"22\":1,\"402\":1}}],[\"待微调的参数量下降到原来的9\",{\"1\":{\"609\":1}}],[\"待填充的张量\",{\"1\":{\"477\":1}}],[\"待完善\",{\"0\":{\"62\":1}}],[\"待预测功能区域类型列表\",{\"1\":{\"53\":1}}],[\"待预测功能区域类型\",{\"1\":{\"53\":1}}],[\"待预测功能区域类型全部隐含在了样本对应的文件路径中\",{\"1\":{\"53\":1}}],[\"由一个\",{\"1\":{\"887\":1}}],[\"由多层\",{\"1\":{\"663\":1}}],[\"由标注者排序\",{\"1\":{\"656\":1}}],[\"由人类标注者根据偏好进行排序\",{\"1\":{\"656\":1}}],[\"由openai团队提出\",{\"1\":{\"638\":1}}],[\"由何恺明团队在2017年论文\",{\"1\":{\"589\":1}}],[\"由协方差控制\",{\"1\":{\"574\":1}}],[\"由权重\",{\"1\":{\"500\":1}}],[\"由此诱导出的\",{\"1\":{\"866\":1}}],[\"由此提出低秩自适应\",{\"1\":{\"610\":1}}],[\"由此可见vit工作的局限性\",{\"1\":{\"432\":1}}],[\"由此把图片转换为序列的embedding形式\",{\"1\":{\"426\":1}}],[\"由两个transformer模块组成\",{\"1\":{\"417\":1}}],[\"由两部分组成\",{\"1\":{\"285\":1}}],[\"由它们投票决定分类\",{\"1\":{\"286\":1}}],[\"由自注意力层与前馈层组成\",{\"1\":{\"286\":1}}],[\"由三层\",{\"1\":{\"285\":1}}],[\"由三角形面片组成的\",{\"1\":{\"159\":1}}],[\"由动量编码器构建\",{\"1\":{\"280\":1}}],[\"由编码器和量化器组成\",{\"1\":{\"212\":1}}],[\"由\",{\"1\":{\"96\":1,\"97\":1,\"133\":1,\"233\":1,\"696\":1,\"885\":1,\"887\":1}}],[\"由于训练轮次很少\",{\"1\":{\"964\":1}}],[\"由于训练数据量和模型计算量较大\",{\"1\":{\"413\":1}}],[\"由于算上了重建误差\",{\"1\":{\"960\":1}}],[\"由于期望的线性性质\",{\"1\":{\"946\":1}}],[\"由于我们用的是近似的编码分布\",{\"1\":{\"950\":1}}],[\"由于我们关心的是给定样本\",{\"1\":{\"945\":1}}],[\"由于我们可以轻松地用神经网络建模每个子像素的概率分布并完成采样\",{\"1\":{\"925\":1}}],[\"由于我们主要关注图像建模\",{\"1\":{\"887\":1}}],[\"由于整幅训练图像已知\",{\"1\":{\"921\":1}}],[\"由于对数是单调递增的\",{\"1\":{\"904\":1}}],[\"由于文本token和图像token被拼接在一起\",{\"1\":{\"892\":1}}],[\"由于概率密度函数的峰值\",{\"1\":{\"872\":1}}],[\"由于该集合是不可数的\",{\"1\":{\"847\":1}}],[\"由于大模型应用需要进行向量语义检索\",{\"1\":{\"836\":1}}],[\"由于与人类交流的出色能力\",{\"1\":{\"823\":1}}],[\"由于for语句反复使用variable实例x0和x1求导\",{\"1\":{\"816\":1}}],[\"由于减法不满足交换律\",{\"1\":{\"809\":1}}],[\"由于乘法满足交换律\",{\"1\":{\"809\":2}}],[\"由于目前variable\",{\"1\":{\"804\":1}}],[\"由于目前只支持竖线形状的计算图\",{\"1\":{\"787\":1}}],[\"由于目标功能区域的尺度\",{\"1\":{\"95\":1}}],[\"由于模型输出的logits\",{\"1\":{\"700\":1}}],[\"由于掩码模式是固定的\",{\"1\":{\"681\":1}}],[\"由于bert训练时会多次遍历数据\",{\"1\":{\"681\":1}}],[\"由于不涉及kv\",{\"1\":{\"663\":1}}],[\"由于gpt\",{\"1\":{\"641\":1}}],[\"由于矩阵\",{\"1\":{\"612\":1}}],[\"由于需要对每个像素进行分类\",{\"1\":{\"584\":1}}],[\"由于准确率包含混淆矩阵中的所有四种结果\",{\"1\":{\"562\":1}}],[\"由于坐标是浮点数\",{\"1\":{\"502\":1}}],[\"由于特征图的尺寸比原图小\",{\"1\":{\"501\":1}}],[\"由于作者是首次将transformer应用到图像领域\",{\"1\":{\"433\":1}}],[\"由于它们在预训练数据集上采用固定类别数的分类器\",{\"1\":{\"413\":1}}],[\"由于这些文本往往只是一个单词\",{\"1\":{\"409\":1}}],[\"由于数据量巨大\",{\"1\":{\"407\":1}}],[\"由于使用了\",{\"1\":{\"381\":1,\"545\":1}}],[\"由于正样本的定义规则是经过编码器之后所在的语义空间才为正样本\",{\"1\":{\"356\":1}}],[\"由于第一阶段中已获得了强大的表示能力\",{\"1\":{\"306\":1}}],[\"由于潜在的视觉\",{\"1\":{\"232\":1}}],[\"由于\",{\"1\":{\"213\":1,\"273\":1,\"293\":1,\"305\":1,\"421\":1,\"663\":1,\"741\":1,\"871\":1,\"886\":1,\"888\":1,\"926\":1,\"944\":1,\"959\":1,\"963\":1}}],[\"由于图文对来自自然语言描述\",{\"1\":{\"190\":1}}],[\"由于经过\",{\"1\":{\"181\":1}}],[\"由于高质量人工标注图文对\",{\"1\":{\"173\":1}}],[\"由于表情不同\",{\"1\":{\"157\":1}}],[\"由于子区域在计算第一个向量时包含的点更稀疏\",{\"1\":{\"142\":1}}],[\"由于点集在不同区域可能会有不同的采样密度\",{\"1\":{\"139\":1}}],[\"由于论文数据集还未开源\",{\"1\":{\"61\":1}}],[\"由成对的\",{\"1\":{\"40\":1}}],[\"加位置\",{\"1\":{\"900\":1}}],[\"加起来\",{\"1\":{\"850\":1}}],[\"加法规则\",{\"1\":{\"848\":1}}],[\"加法运算的add类已在上文中实现\",{\"1\":{\"809\":1}}],[\"加法运算的运算符重载\",{\"1\":{\"809\":1}}],[\"加\",{\"1\":{\"401\":1,\"709\":1}}],[\"加到注意力分数的时机\",{\"1\":{\"710\":1}}],[\"加到注意力分数\",{\"1\":{\"710\":1}}],[\"加到该头的注意力分数上\",{\"1\":{\"710\":1}}],[\"加到\",{\"1\":{\"233\":1}}],[\"加单位矩阵\",{\"1\":{\"152\":1}}],[\"加速收敛\",{\"1\":{\"630\":1}}],[\"加速\",{\"1\":{\"293\":1}}],[\"加速训练\",{\"1\":{\"120\":1}}],[\"加速计算\",{\"1\":{\"115\":1}}],[\"加入梯度修正\",{\"1\":{\"899\":1}}],[\"加入到验证集中\",{\"1\":{\"835\":1}}],[\"加入数值稳定性处理\",{\"1\":{\"592\":1}}],[\"加入数据增强后缓解\",{\"1\":{\"157\":1}}],[\"加入平滑项防止除以零\",{\"1\":{\"588\":1}}],[\"加入多个\",{\"1\":{\"239\":1}}],[\"加入了均匀先验\",{\"1\":{\"232\":1}}],[\"加入动量队列中的样本作为负样本\",{\"1\":{\"206\":1}}],[\"加入正则项约束变换矩阵接近正交\",{\"1\":{\"150\":1}}],[\"加入\",{\"1\":{\"99\":2,\"152\":1,\"266\":1,\"586\":1,\"587\":2}}],[\"加上因为近似不准而多花的那部分成本\",{\"1\":{\"909\":1}}],[\"加上位置\",{\"1\":{\"900\":1}}],[\"加上位置嵌入并进行随机丢弃\",{\"1\":{\"428\":1,\"431\":1}}],[\"加上参数规模的扩展\",{\"1\":{\"647\":1}}],[\"加上这样一句话\",{\"1\":{\"619\":1}}],[\"加上类型\",{\"1\":{\"385\":1}}],[\"加上\",{\"1\":{\"257\":1,\"262\":1,\"385\":2,\"479\":1,\"897\":1}}],[\"加上全局特征后\",{\"1\":{\"156\":1}}],[\"加上单位矩阵作为初始偏置\",{\"1\":{\"152\":1}}],[\"加上小常数避免数值不稳定\",{\"1\":{\"119\":1}}],[\"加上原始\",{\"1\":{\"96\":1}}],[\"加上经典的\",{\"1\":{\"60\":1}}],[\"加权惩罚项\",{\"1\":{\"590\":1}}],[\"加权\",{\"1\":{\"588\":1,\"630\":1}}],[\"加权标准化器\",{\"1\":{\"578\":1}}],[\"加权计数\",{\"1\":{\"485\":1}}],[\"加权和\",{\"1\":{\"160\":1}}],[\"加权平均插值\",{\"1\":{\"145\":1}}],[\"加权平均系数\",{\"1\":{\"145\":1}}],[\"加权累加特征\",{\"1\":{\"122\":1}}],[\"加权交叉熵损失\",{\"1\":{\"592\":1}}],[\"加权交叉熵\",{\"1\":{\"102\":1}}],[\"加权后的输出\",{\"1\":{\"69\":2}}],[\"加权总损失\",{\"1\":{\"64\":1,\"256\":1}}],[\"加之原本在github上开源的代码后续被下架\",{\"1\":{\"61\":1}}],[\"加载检查点\",{\"1\":{\"700\":1}}],[\"加载该模型后\",{\"1\":{\"435\":1}}],[\"加载模型和\",{\"1\":{\"660\":1}}],[\"加载模型和处理器\",{\"1\":{\"410\":1,\"412\":1}}],[\"加载模型配置\",{\"1\":{\"107\":1}}],[\"加载离散\",{\"1\":{\"265\":1}}],[\"加载图像文件夹作为数据集\",{\"1\":{\"264\":1}}],[\"加载图像数据\",{\"1\":{\"52\":1}}],[\"加载dino\",{\"1\":{\"213\":1}}],[\"加载clip\",{\"1\":{\"213\":1}}],[\"加载初始化码本\",{\"1\":{\"213\":1}}],[\"加载文本编码器\",{\"1\":{\"205\":2}}],[\"加载\",{\"1\":{\"187\":1,\"192\":1}}],[\"加载数据集\",{\"1\":{\"963\":1}}],[\"加载数据\",{\"1\":{\"187\":1}}],[\"加载预训练\",{\"1\":{\"964\":1}}],[\"加载预训练好的vit\",{\"1\":{\"435\":1}}],[\"加载预训练模型\",{\"0\":{\"435\":1},\"1\":{\"963\":1}}],[\"加载预训练视觉编码器与文本解码器\",{\"1\":{\"187\":1}}],[\"加载预训练权重\",{\"1\":{\"107\":1}}],[\"加载预训练多模态大模型\",{\"1\":{\"52\":1}}],[\"加载训练集\",{\"1\":{\"104\":1}}],[\"加载的标注数据中每个样本的组织形式如下\",{\"1\":{\"92\":1}}],[\"加载58种物体\",{\"1\":{\"92\":1}}],[\"加载标注数据\",{\"1\":{\"92\":1,\"107\":1}}],[\"加载点云数据\",{\"1\":{\"92\":1,\"107\":1}}],[\"加载点云数据和功能区域掩码\",{\"1\":{\"53\":1}}],[\"加载点云\",{\"1\":{\"82\":1}}],[\"加载点云样本\",{\"1\":{\"53\":1}}],[\"加载列表中所有点云样本\",{\"1\":{\"53\":1}}],[\"传到\",{\"1\":{\"958\":1}}],[\"传染性事件\",{\"1\":{\"863\":1}}],[\"传递给下游变量\",{\"1\":{\"809\":1}}],[\"传给\",{\"1\":{\"457\":1,\"947\":1}}],[\"传参保留\",{\"1\":{\"385\":1}}],[\"传统gan训练的完整流程\",{\"1\":{\"918\":1}}],[\"传统\",{\"1\":{\"835\":1,\"963\":1}}],[\"传统语言模型通常依赖单词或字符级输入\",{\"1\":{\"640\":1}}],[\"传统监督学习通常建模\",{\"1\":{\"640\":1}}],[\"传统bpe算法没有这一步\",{\"1\":{\"595\":8}}],[\"传统的\",{\"1\":{\"835\":1}}],[\"传统的视觉特征提取主要有两种典型实现方案\",{\"1\":{\"388\":1}}],[\"传统的线性评估\",{\"1\":{\"273\":1}}],[\"传统的离散变量采样\",{\"1\":{\"257\":1}}],[\"传统知识蒸馏通过教师模型指导学生模型提升性能\",{\"1\":{\"195\":1}}],[\"传统vlp方法依赖目标检测器提取区域特征\",{\"1\":{\"194\":1}}],[\"传统方法的缺陷\",{\"1\":{\"148\":1}}],[\"传统方法难以适应不同情况\",{\"1\":{\"95\":1}}],[\"传统卷积神经网络难以直接处理\",{\"1\":{\"148\":1}}],[\"传入函数的参数\",{\"1\":{\"445\":1}}],[\"传入图像\",{\"1\":{\"420\":1}}],[\"传入图像编码信息\",{\"1\":{\"188\":1}}],[\"传入\",{\"1\":{\"380\":1}}],[\"传入了\",{\"1\":{\"207\":1}}],[\"传入的点集特征集合经过转置处理后的维度为\",{\"1\":{\"94\":1}}],[\"传入question文本\",{\"1\":{\"94\":1}}],[\"传入数据维度\",{\"1\":{\"59\":1}}],[\"传感器误差\",{\"1\":{\"20\":1}}],[\"传感器误差或数据损坏\",{\"1\":{\"19\":1}}],[\"逐像素建模图像像素值的条件概率分布\",{\"1\":{\"926\":1}}],[\"逐\",{\"1\":{\"898\":1}}],[\"逐位置前馈神经网络\",{\"1\":{\"633\":1}}],[\"逐列\",{\"1\":{\"544\":1}}],[\"逐渐丰富了其功能\",{\"1\":{\"823\":1}}],[\"逐渐引入\",{\"1\":{\"204\":1}}],[\"逐渐恢复点数\",{\"1\":{\"143\":1}}],[\"逐元素相加然后送入线性输出层\",{\"1\":{\"631\":1}}],[\"逐元素相加\",{\"1\":{\"122\":1}}],[\"逐元素相乘\",{\"1\":{\"119\":1}}],[\"逐点乘法\",{\"1\":{\"122\":1}}],[\"逐点\",{\"1\":{\"110\":1,\"117\":2,\"152\":1,\"587\":1}}],[\"逐点误差\",{\"1\":{\"106\":1}}],[\"逐步进行完善和优化\",{\"1\":{\"836\":1}}],[\"逐步推理\",{\"1\":{\"825\":1}}],[\"逐步合并高频的字符对\",{\"1\":{\"594\":1}}],[\"逐步压缩信息\",{\"1\":{\"500\":1}}],[\"逐步聚合全局信息\",{\"1\":{\"427\":1}}],[\"逐步缩小差距\",{\"1\":{\"327\":1}}],[\"逐步提升模型性能\",{\"1\":{\"296\":1}}],[\"逐步向正交矩阵靠拢\",{\"1\":{\"152\":1}}],[\"逐步减少点的数量\",{\"1\":{\"143\":1}}],[\"逐步以自上而下的方式细化点特征图\",{\"1\":{\"95\":1}}],[\"逐步分析交互图像中的几何属性和交互意图\",{\"1\":{\"29\":1}}],[\"逐级做点集抽象得到的每层的点集坐标和点集特征集合\",{\"1\":{\"94\":1}}],[\"逐级恢复点云分辨率\",{\"1\":{\"83\":1}}],[\"逐层提取边缘→纹理→部件→物体\",{\"1\":{\"500\":1}}],[\"逐层通过\",{\"1\":{\"266\":1}}],[\"逐层编码\",{\"1\":{\"233\":1}}],[\"逐层融合上下文信息\",{\"1\":{\"146\":1}}],[\"逐层恢复到原始点数\",{\"1\":{\"144\":1}}],[\"逐层将特征插值回原始点数量\",{\"1\":{\"143\":1}}],[\"逐层抽象后融合成全局特征\",{\"1\":{\"141\":1}}],[\"逐层上采样\",{\"1\":{\"83\":1}}],[\"逐层点云特征列表\",{\"1\":{\"59\":1}}],[\"逐个拼接\",{\"1\":{\"67\":1}}],[\"通义\",{\"1\":{\"823\":1}}],[\"通义千问是由阿里巴巴基于\",{\"1\":{\"823\":1}}],[\"通义千问\",{\"1\":{\"822\":1,\"823\":2}}],[\"通俗易懂讲解lora微调\",{\"0\":{\"608\":1},\"1\":{\"608\":1}}],[\"通俗易懂解读bpe分词算法实现\",{\"0\":{\"594\":1},\"1\":{\"594\":1}}],[\"通俗讲人话\",{\"1\":{\"606\":1}}],[\"通用大模型的架构\",{\"1\":{\"836\":1}}],[\"通用人工智能\",{\"1\":{\"827\":1}}],[\"通用网络层封装与模型训练流程构建\",{\"1\":{\"818\":1}}],[\"通用的任务未知task\",{\"1\":{\"625\":1}}],[\"通用近似定理中\",{\"1\":{\"500\":1}}],[\"通用近似定理\",{\"0\":{\"500\":1},\"1\":{\"500\":1}}],[\"通用视觉助手\",{\"1\":{\"342\":1}}],[\"通用任务\",{\"1\":{\"336\":1}}],[\"通用多模态任务\",{\"1\":{\"335\":1}}],[\"通用图像增强流程\",{\"1\":{\"264\":1}}],[\"通常并没有这样的正则化参数\",{\"1\":{\"951\":1}}],[\"通常的选择是将其设为\",{\"1\":{\"946\":1}}],[\"通常取对数得到对数似然\",{\"1\":{\"904\":1}}],[\"通常设置为\",{\"1\":{\"893\":1}}],[\"通常设为\",{\"1\":{\"518\":1}}],[\"通常图像\",{\"1\":{\"893\":1}}],[\"通常具有巨大的参数规模\",{\"1\":{\"824\":1}}],[\"通常大模型由三个阶段构成\",{\"1\":{\"822\":1}}],[\"通常指包含数百亿\",{\"1\":{\"822\":1}}],[\"通常在内存不足或满足特定条件时才会触发\",{\"1\":{\"806\":1}}],[\"通常在大规模数据集\",{\"1\":{\"425\":1}}],[\"通常需要重新训练来保持知识和数据的更新\",{\"1\":{\"830\":1}}],[\"通常需要使用高性能的\",{\"1\":{\"824\":1}}],[\"通常需要数千到数十万个标注样本\",{\"1\":{\"647\":1}}],[\"通常需要连续的内存布局\",{\"1\":{\"492\":1}}],[\"通常这些开源的大模型都是需要用自有数据进行微调\",{\"1\":{\"601\":1}}],[\"通常问题不大\",{\"1\":{\"601\":1}}],[\"通常最后会将预训练生成的频次表和词汇表写入文件保存\",{\"1\":{\"595\":1}}],[\"通常最好优先考虑召回率\",{\"1\":{\"566\":1}}],[\"通常\",{\"1\":{\"589\":2,\"601\":1,\"620\":1,\"710\":1}}],[\"通常加入平滑项\",{\"1\":{\"588\":1}}],[\"通常意味着对各个轴进行更一般的排列组合\",{\"1\":{\"545\":1}}],[\"通常使用预训练的目标检测器\",{\"1\":{\"388\":1}}],[\"通常使用图文对比学习优化模型\",{\"1\":{\"369\":1}}],[\"通常使用轻量级的\",{\"1\":{\"296\":1}}],[\"通常会用\",{\"1\":{\"466\":1}}],[\"通常会加入一个\",{\"1\":{\"355\":1}}],[\"通常会设置一个上限k\",{\"1\":{\"135\":1}}],[\"通常保留原始结构\",{\"1\":{\"346\":1}}],[\"通常采用对比学习预训练的\",{\"1\":{\"330\":1}}],[\"通常仅3亿参数\",{\"1\":{\"323\":1}}],[\"通常小于主图像\",{\"1\":{\"265\":1}}],[\"通常比卷积神经网络\",{\"1\":{\"228\":1}}],[\"通常为标准正态分布\",{\"1\":{\"932\":1}}],[\"通常为4个采样点\",{\"1\":{\"502\":1}}],[\"通常为3\",{\"1\":{\"255\":1}}],[\"通常为全\",{\"1\":{\"207\":1}}],[\"通常为\",{\"1\":{\"205\":1,\"255\":1,\"899\":1}}],[\"通常依赖预训练教师\",{\"1\":{\"195\":1}}],[\"通常是标准正态分布\",{\"1\":{\"944\":1}}],[\"通常是均匀或高斯\",{\"1\":{\"918\":1}}],[\"通常是模糊的\",{\"1\":{\"878\":1}}],[\"通常是在1000亿参数\",{\"1\":{\"620\":1}}],[\"通常是字节\",{\"1\":{\"540\":1}}],[\"通常是原图的\",{\"1\":{\"501\":1}}],[\"通常是随机初始化或者初始化为零\",{\"1\":{\"428\":1}}],[\"通常是图像内容的全面视觉描述\",{\"1\":{\"341\":1}}],[\"通常是不同尺寸的原图\",{\"1\":{\"265\":1}}],[\"通常是高斯\",{\"1\":{\"235\":1}}],[\"通常是旋转或反射\",{\"1\":{\"152\":1}}],[\"通常是\",{\"1\":{\"123\":1,\"382\":1,\"384\":1,\"432\":1,\"489\":1,\"734\":1}}],[\"通常特征更抽象\",{\"1\":{\"122\":1}}],[\"通常分辨率更高\",{\"1\":{\"122\":1}}],[\"通常用作图像压缩或聚类\",{\"1\":{\"963\":1}}],[\"通常用于实现完整功能\",{\"1\":{\"810\":1}}],[\"通常用于分割模型中\",{\"1\":{\"587\":1}}],[\"通常用于连接编码器和解码器\",{\"1\":{\"122\":1}}],[\"通常用于解码器中间层\",{\"1\":{\"122\":1}}],[\"通常用在解码器中\",{\"1\":{\"122\":1}}],[\"通常用在层次化架构中\",{\"1\":{\"121\":1}}],[\"通常我们会使用多个\",{\"1\":{\"106\":1}}],[\"通道时\",{\"1\":{\"924\":3}}],[\"通道信息\",{\"1\":{\"924\":4}}],[\"通道编号\",{\"1\":{\"924\":1}}],[\"通道之间的因果关系\",{\"1\":{\"924\":1}}],[\"通道掩码\",{\"0\":{\"924\":1},\"1\":{\"924\":1}}],[\"通道级别的统计计算同样受益于连续内存访问模式\",{\"1\":{\"493\":1}}],[\"通道数\",{\"1\":{\"380\":1,\"899\":1}}],[\"通道数增加时\",{\"1\":{\"125\":1}}],[\"通道2\",{\"1\":{\"119\":1}}],[\"通道0\",{\"1\":{\"119\":1}}],[\"通道分组数\",{\"1\":{\"120\":1}}],[\"通道分组\",{\"1\":{\"119\":1}}],[\"通道维度特征提取阶段\",{\"1\":{\"152\":1}}],[\"通道维度完成拼接后\",{\"1\":{\"83\":1}}],[\"通道维度作信息融合\",{\"1\":{\"60\":1}}],[\"通道维度上进行特征融合\",{\"1\":{\"58\":1,\"59\":1}}],[\"通过正常反向传播重建误差更新\",{\"1\":{\"963\":1}}],[\"通过正弦和余弦函数分裂映射到偶数和奇数的维度\",{\"1\":{\"706\":1}}],[\"通过阅读这篇文章\",{\"1\":{\"955\":1}}],[\"通过加入\",{\"1\":{\"935\":1}}],[\"通过加偏置\",{\"1\":{\"710\":1}}],[\"通过设置\",{\"1\":{\"926\":1}}],[\"通过设计合理的预训练任务\",{\"1\":{\"355\":1}}],[\"通过给图像损失设置更高的权重\",{\"1\":{\"893\":1}}],[\"通过让一部分样本在训练时不给文本输入\",{\"1\":{\"893\":1}}],[\"通过观察结果\",{\"1\":{\"878\":1}}],[\"通过反复地做并集\",{\"1\":{\"847\":1}}],[\"通过反向传播算法更新低秩矩阵\",{\"1\":{\"609\":1}}],[\"通过与\",{\"1\":{\"833\":1}}],[\"通过与其他\",{\"1\":{\"427\":1}}],[\"通过提供额外的背景知识和数据支持\",{\"1\":{\"828\":1}}],[\"通过提示词让它自己做任务\",{\"1\":{\"346\":1}}],[\"通过检索到的真实信息生成回答\",{\"1\":{\"830\":1}}],[\"通过检索和整合长文本信息\",{\"1\":{\"828\":1}}],[\"通过检索特定领域的相关文档\",{\"1\":{\"828\":1}}],[\"通过检索数据源\",{\"1\":{\"828\":1}}],[\"通过实时检索最新数据\",{\"1\":{\"828\":1}}],[\"通过采用\",{\"1\":{\"825\":1}}],[\"通过使用自然语言描述的多任务数据进行微调\",{\"1\":{\"825\":1}}],[\"通过使用自身参数的滑动平均作为教师\",{\"1\":{\"195\":1}}],[\"通过理解上下文并生成相应输出的方式来执行任务\",{\"1\":{\"825\":1}}],[\"通过稀疏计算以经济的成本训练强大的模型\",{\"1\":{\"823\":1}}],[\"通过改进的对齐技术\",{\"1\":{\"823\":1}}],[\"通过backward\",{\"1\":{\"816\":1}}],[\"通过seen\",{\"1\":{\"815\":1}}],[\"通过softmax函数转换后\",{\"1\":{\"408\":1}}],[\"通过homebrew执行\",{\"1\":{\"815\":1}}],[\"通过config类和no\",{\"1\":{\"812\":1}}],[\"通过cmafm模块将几何属性\",{\"1\":{\"30\":1}}],[\"通过列表处理可变长参数\",{\"1\":{\"812\":1}}],[\"通过梯度检验可知结果正确\",{\"1\":{\"811\":1}}],[\"通过运算符重载\",{\"1\":{\"811\":1}}],[\"通过步骤20\",{\"1\":{\"809\":1}}],[\"通过组合加法和乘法运算符\",{\"1\":{\"809\":1}}],[\"通过组合不同组件\",{\"1\":{\"303\":1}}],[\"通过retain\",{\"1\":{\"807\":1}}],[\"通过roi\",{\"1\":{\"78\":1}}],[\"通过output\",{\"1\":{\"806\":1}}],[\"通过弱引用主动打破function与variable之间的循环引用\",{\"1\":{\"806\":1}}],[\"通过以上修改\",{\"1\":{\"805\":1}}],[\"通过以下改进缩小差距\",{\"1\":{\"323\":1}}],[\"通过以下两个核心组件实现跨模态深度协同\",{\"1\":{\"303\":1}}],[\"通过以下关键设计解决上述问题\",{\"1\":{\"296\":1}}],[\"通过循环替代递归\",{\"1\":{\"787\":1}}],[\"通过递归或循环遍历计算图\",{\"1\":{\"784\":1}}],[\"通过计算图可以直观地表示变量与函数的关系\",{\"1\":{\"760\":1}}],[\"通过计算损失\",{\"1\":{\"427\":1}}],[\"通过填充掩码\",{\"1\":{\"741\":1}}],[\"通过调用\",{\"1\":{\"733\":1}}],[\"通过调整训练策略\",{\"1\":{\"682\":1}}],[\"通过调整这些关键因素\",{\"1\":{\"679\":1}}],[\"通过调整\",{\"1\":{\"592\":1}}],[\"通过调整步长\",{\"1\":{\"544\":1}}],[\"通过桶映射\",{\"1\":{\"710\":1}}],[\"通过系统优化bert的预训练策略\",{\"1\":{\"688\":1}}],[\"通过系统性的实验发现bert存在训练不足的问题\",{\"1\":{\"677\":1}}],[\"通过复制数据来增加多样性\",{\"1\":{\"681\":1}}],[\"通过复合实现指数级增长的分段线性区域\",{\"1\":{\"500\":1}}],[\"通过控制数据规模\",{\"1\":{\"680\":1}}],[\"通过优化训练策略\",{\"1\":{\"678\":1}}],[\"通过数据与训练优化达到大模型\",{\"1\":{\"668\":1}}],[\"通过xformers库实现因果多头注意力的高效计算\",{\"1\":{\"667\":1}}],[\"通过触发短语过滤预训练语料\",{\"1\":{\"655\":1}}],[\"通过图1的结果可见\",{\"1\":{\"654\":1}}],[\"通过图像编码器和文本编码器分别编码图像与文本\",{\"1\":{\"268\":1}}],[\"通过图像分词器生成\",{\"1\":{\"227\":1}}],[\"通过人类反馈进行强化学习\",{\"1\":{\"654\":1}}],[\"通过人为构造\",{\"1\":{\"355\":1}}],[\"通过微调在多个任务上实现了sota\",{\"1\":{\"650\":1}}],[\"通过大语言模型的强大理解能力和生成能力\",{\"1\":{\"835\":1}}],[\"通过大规模预训练和精心设计的上下文输入\",{\"1\":{\"647\":1}}],[\"通过大规模训练显著提升了少样本学习能力\",{\"1\":{\"645\":1}}],[\"通过大量文本数据训练这些模型\",{\"1\":{\"822\":1}}],[\"通过大量\",{\"1\":{\"346\":1}}],[\"通过添加停用词过滤器\",{\"1\":{\"641\":1}}],[\"通过可逆去token化\",{\"1\":{\"641\":1}}],[\"通过在训练集上训练模型\",{\"1\":{\"835\":1}}],[\"通过在特定数据集上进一步训练大语言模型\",{\"1\":{\"830\":1}}],[\"通过在长篇连续文本的多样化语料库上预训练\",{\"1\":{\"636\":1}}],[\"通过在单模态与多模态数据上进行\",{\"1\":{\"221\":1}}],[\"通过引导llm解决子问题\",{\"1\":{\"622\":1}}],[\"通过引入两个可调节参数来增强模型对假阳性\",{\"1\":{\"590\":1}}],[\"通过引入动量蒸馏\",{\"1\":{\"202\":1}}],[\"通过把一个个的简单问题解决掉\",{\"1\":{\"622\":1}}],[\"通过解决这一系列的简单问题\",{\"1\":{\"622\":1}}],[\"通过增加旁路\",{\"1\":{\"614\":1}}],[\"通过简单有效的方案来达成轻量微调的目的\",{\"1\":{\"614\":1}}],[\"通过降低参数的精度\",{\"1\":{\"607\":1}}],[\"通过强化学习的方式\",{\"1\":{\"602\":1}}],[\"通过协方差矩阵找出\",{\"1\":{\"574\":1}}],[\"通过插值获得值后聚合\",{\"1\":{\"502\":1}}],[\"通过逐层抽象\",{\"1\":{\"500\":1}}],[\"通过encoder\",{\"1\":{\"431\":1}}],[\"通过投影层对合并后的张量进行线性变换\",{\"1\":{\"430\":1}}],[\"通过qkv线性层将输入x映射到dim\",{\"1\":{\"430\":1}}],[\"通过qllama实现动态特征交互\",{\"1\":{\"303\":1}}],[\"通过第一阶段的10个步骤\",{\"1\":{\"797\":1}}],[\"通过第一个全连接层\",{\"1\":{\"429\":1}}],[\"通过第二个全连接层\",{\"1\":{\"429\":1}}],[\"通过激活函数层\",{\"1\":{\"429\":1}}],[\"通过分析前面的词汇来预测下一个词汇\",{\"1\":{\"822\":1}}],[\"通过分层降维\",{\"1\":{\"500\":1}}],[\"通过分层组合简单函数\",{\"1\":{\"500\":1}}],[\"通过分类头\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"通过分组实现参数共享\",{\"1\":{\"119\":1}}],[\"通过随机裁剪可以增加训练数据的多样性\",{\"1\":{\"425\":1}}],[\"通过随机丢弃来模拟不同密度的采样\",{\"1\":{\"141\":1}}],[\"通过视觉编码器\",{\"1\":{\"421\":1}}],[\"通过视觉编码器提取图像的视觉特征\",{\"1\":{\"188\":1}}],[\"通过学习image\",{\"1\":{\"419\":1}}],[\"通过一个确定性函数族\",{\"1\":{\"943\":1}}],[\"通过一些自动化的手段将web\",{\"1\":{\"413\":1}}],[\"通过一致性约束继承老师的能力\",{\"1\":{\"26\":1}}],[\"通过linear\",{\"1\":{\"392\":1}}],[\"通过word\",{\"1\":{\"392\":1}}],[\"通过卷积提取每个网格的视觉特征\",{\"1\":{\"388\":1}}],[\"通过卷积将图像划分为\",{\"1\":{\"266\":1}}],[\"通过点积获得相似度\",{\"1\":{\"375\":1}}],[\"通过点云插值\",{\"1\":{\"122\":1}}],[\"通过线性投影和归一化得到\",{\"1\":{\"373\":1}}],[\"通过线性层转为\",{\"1\":{\"286\":1}}],[\"通过模型获取图片的特征嵌入\",{\"1\":{\"410\":1}}],[\"通过模态专家捕获模态特定信息\",{\"1\":{\"368\":1}}],[\"通过模仿教师在遮挡位置的特征提供监督\",{\"1\":{\"216\":1}}],[\"通过跨模态注意力建模图像和文本的交互\",{\"1\":{\"369\":1}}],[\"通过跨模态注意力融合图像和文本表示\",{\"1\":{\"368\":1}}],[\"通过跨模态相似性\",{\"1\":{\"80\":1}}],[\"通过这一技巧\",{\"1\":{\"959\":1}}],[\"通过这些区间的并\",{\"1\":{\"847\":1}}],[\"通过这\",{\"1\":{\"799\":1,\"814\":1}}],[\"通过这种方式\",{\"1\":{\"353\":1,\"408\":1}}],[\"通过这种机制\",{\"1\":{\"272\":1}}],[\"通过这个阶段训练后\",{\"1\":{\"341\":1}}],[\"通过结合在线聚类分配过程与直通估计器可以解决这一问题\",{\"1\":{\"886\":1}}],[\"通过结合人工标注数据和强化学习\",{\"1\":{\"339\":1}}],[\"通过结合描述交互的图像或语言与3d几何结构引入外部先验\",{\"1\":{\"30\":1}}],[\"通过机器生成的指令数据进行调优\",{\"1\":{\"339\":1}}],[\"通过持续优化的视觉编码器internvit\",{\"1\":{\"337\":1}}],[\"通过持续学习优化\",{\"1\":{\"329\":1}}],[\"通过持续学习策略增强视觉理解能力\",{\"1\":{\"327\":1}}],[\"通过持续学习策略优化大规模视觉基础模型\",{\"1\":{\"322\":1}}],[\"通过翻译管道\",{\"1\":{\"332\":1}}],[\"通过中英文问答对标注\",{\"1\":{\"323\":1}}],[\"通过将潜变量的分布从\",{\"1\":{\"951\":1}}],[\"通过将核心组件\",{\"1\":{\"833\":1}}],[\"通过将键值\",{\"1\":{\"823\":1}}],[\"通过将查询\",{\"1\":{\"823\":2}}],[\"通过将图像切成小片\",{\"1\":{\"436\":1}}],[\"通过将特征映射到特定的维度并进行非线性变换\",{\"1\":{\"431\":1}}],[\"通过将视觉基础模型扩展到\",{\"1\":{\"313\":1}}],[\"通过将局部区域中的每个点\",{\"1\":{\"136\":1}}],[\"通过最大化关于\",{\"1\":{\"887\":1}}],[\"通过最小化配置\",{\"1\":{\"311\":1}}],[\"通过最远点采样选择最具代表性的点\",{\"1\":{\"121\":1}}],[\"通过注意力池化生成全局特征\",{\"1\":{\"304\":1}}],[\"通过预训练权重实现视觉特征到llm表示的对齐\",{\"1\":{\"304\":1}}],[\"通过预测离散视觉\",{\"1\":{\"234\":1}}],[\"通过超参数搜索\",{\"1\":{\"304\":1}}],[\"通过超参数搜索优化了深度\",{\"1\":{\"303\":1}}],[\"通过超大维度的\",{\"1\":{\"293\":1}}],[\"通过扩大模型规模和参数量\",{\"1\":{\"298\":1}}],[\"通过比较图像特征而不是分类\",{\"1\":{\"282\":1}}],[\"通过遮挡图像\",{\"1\":{\"252\":1}}],[\"通过高层语义的离散标记学习更有效的视觉表示\",{\"1\":{\"227\":1}}],[\"通过恢复离散视觉码作为预测目标\",{\"1\":{\"216\":1}}],[\"通过恢复被掩码的图像块\",{\"1\":{\"210\":1}}],[\"通过任务层调整输出维度\",{\"1\":{\"213\":1}}],[\"通过任务层调整特征维度\",{\"1\":{\"213\":1}}],[\"通过对每个\",{\"1\":{\"877\":1}}],[\"通过对复杂测试函数求导\",{\"1\":{\"811\":1}}],[\"通过对比学习目标训练双编码器\",{\"1\":{\"269\":1}}],[\"通过对比学习在大规模图文对中对齐表示\",{\"1\":{\"195\":1}}],[\"通过对比学习先对齐图像和文本的表示\",{\"1\":{\"194\":1}}],[\"通过对自注意力算子形式\",{\"1\":{\"109\":1}}],[\"通过pointnet获取每个形心多尺度信息\",{\"1\":{\"141\":1}}],[\"通过较低温度参数\",{\"1\":{\"285\":1}}],[\"通过较大的邻域尺度避免过度稀疏的问题\",{\"1\":{\"140\":1}}],[\"通过较小的邻域尺度捕获细节\",{\"1\":{\"140\":1}}],[\"通过全连接层\",{\"1\":{\"699\":1}}],[\"通过全连接层进行分类\",{\"1\":{\"138\":1}}],[\"通过全局上下文信息增强局部特征\",{\"1\":{\"122\":1}}],[\"通过多任务微调训练语言模型\",{\"1\":{\"655\":1}}],[\"通过多个局部区域球查询提取不同尺度的局部特征\",{\"1\":{\"141\":1}}],[\"通过多层\",{\"1\":{\"138\":1,\"427\":1}}],[\"通过多头可供性链式思维\",{\"1\":{\"50\":1}}],[\"通过查找形心点周围的\",{\"1\":{\"133\":1}}],[\"通过局部特征学习器\",{\"1\":{\"131\":1}}],[\"通过局部邻域的自注意力传播信息\",{\"1\":{\"109\":1}}],[\"通过位置编码函数θ将3d相对坐标信息融入注意力计算\",{\"1\":{\"119\":1}}],[\"通过消息传递进行信息传播\",{\"1\":{\"109\":1}}],[\"通过共享mlp迫使图像和点云特征在相同空间分布\",{\"1\":{\"83\":1}}],[\"通过双路交叉注意力分别建模物体\",{\"1\":{\"80\":1}}],[\"通过特征传播层上采样\",{\"1\":{\"78\":1}}],[\"通过自注意力机制与其他\",{\"1\":{\"427\":1}}],[\"通过自注意力建模模态内结构关系\",{\"1\":{\"78\":1}}],[\"通过自监督学习的方式\",{\"1\":{\"229\":1}}],[\"通过自适应权重融合不同尺度特征\",{\"1\":{\"23\":1}}],[\"通过智能体主动交互学习\",{\"1\":{\"75\":1}}],[\"通过智能体在虚拟环境中主动交互学习功能\",{\"1\":{\"73\":1}}],[\"通过密集跨模态相似性\",{\"1\":{\"73\":1}}],[\"通过标注物体交互区域建立几何结构与功能的固定关联\",{\"1\":{\"73\":1}}],[\"通过联合区域对齐模块\",{\"1\":{\"72\":1}}],[\"通过2d图像中的交互信息来预测3d物体的功能区域\",{\"1\":{\"72\":1}}],[\"通过适配器层将\",{\"1\":{\"64\":1}}],[\"通过建立意图与几何的显式关联\",{\"1\":{\"56\":1}}],[\"通过为每张图像配对多个点云\",{\"1\":{\"53\":1}}],[\"通过交叉注意力建模物体\",{\"1\":{\"73\":1}}],[\"通过交叉注意力层\",{\"1\":{\"37\":1}}],[\"通过交叉注意力机制计算对象几何特征\",{\"1\":{\"32\":1}}],[\"通过文本条件查询分割功能区域\",{\"1\":{\"31\":1}}],[\"通过轻量语言模型提取文本特征\",{\"1\":{\"22\":1}}],[\"通过\",{\"1\":{\"19\":1,\"100\":2,\"150\":1,\"152\":1,\"167\":1,\"174\":1,\"212\":1,\"214\":1,\"258\":1,\"268\":1,\"282\":1,\"305\":1,\"332\":1,\"370\":1,\"382\":2,\"420\":2,\"589\":1,\"641\":1,\"656\":1,\"657\":1,\"835\":3,\"887\":1,\"899\":1,\"963\":1}}],[\"通过嵌入到\",{\"1\":{\"19\":1}}],[\"泛化更好\",{\"1\":{\"710\":1}}],[\"泛化强\",{\"1\":{\"658\":1}}],[\"泛化机制的模糊性\",{\"1\":{\"649\":1}}],[\"泛化差\",{\"1\":{\"194\":1}}],[\"泛化能力有限\",{\"1\":{\"646\":1}}],[\"泛化能力\",{\"1\":{\"89\":1}}],[\"泛化\",{\"1\":{\"57\":1}}],[\"泛化性就不足\",{\"1\":{\"353\":1}}],[\"泛化性不如\",{\"1\":{\"346\":1}}],[\"泛化性不足\",{\"1\":{\"31\":1}}],[\"泛化性差\",{\"1\":{\"75\":1}}],[\"泛化性下降明显\",{\"1\":{\"48\":1}}],[\"泛化性和鲁棒性\",{\"1\":{\"26\":1}}],[\"泛化性\",{\"1\":{\"19\":1,\"657\":1}}],[\"整除\",{\"1\":{\"582\":1}}],[\"整数子集上的一些离散型概率分布\",{\"1\":{\"855\":1}}],[\"整数或\",{\"1\":{\"513\":1}}],[\"整数\",{\"1\":{\"426\":2}}],[\"整个序列的总长度\",{\"1\":{\"895\":1}}],[\"整个生成过程可以想成\",{\"1\":{\"885\":1}}],[\"整个样本空间的概率为\",{\"1\":{\"848\":1}}],[\"整个样本空间\",{\"1\":{\"847\":1}}],[\"整个互联网一样\",{\"1\":{\"822\":1}}],[\"整个句子\",{\"1\":{\"740\":1}}],[\"整个句子的全局语义向量\",{\"1\":{\"274\":1}}],[\"整个过程中\",{\"1\":{\"694\":1}}],[\"整个注意力的\",{\"1\":{\"663\":1}}],[\"整个\",{\"1\":{\"236\":1}}],[\"整个函数\",{\"1\":{\"150\":1}}],[\"整个网络完全由\",{\"1\":{\"116\":1}}],[\"整幅图像\",{\"1\":{\"112\":1}}],[\"整理\",{\"1\":{\"57\":1}}],[\"整体仍在合理区间\",{\"1\":{\"893\":1}}],[\"整体得分72\",{\"1\":{\"634\":1}}],[\"整体模型还是比较大的\",{\"1\":{\"435\":1}}],[\"整体结构图如下\",{\"1\":{\"435\":1}}],[\"整体框架与近年来的自监督学习方法类似\",{\"1\":{\"285\":1}}],[\"整体架构\",{\"1\":{\"272\":1}}],[\"整体效果\",{\"1\":{\"258\":1}}],[\"整体而言\",{\"1\":{\"212\":1}}],[\"整体来看\",{\"1\":{\"174\":1}}],[\"整体包括三大模块\",{\"1\":{\"21\":1}}],[\"整体流程可以被视作在最大化模型分布在图像\",{\"1\":{\"885\":1}}],[\"整体流程总览\",{\"0\":{\"397\":1}}],[\"整体流程和albef模型实现一致\",{\"1\":{\"192\":1}}],[\"整体流程\",{\"1\":{\"21\":1,\"710\":1}}],[\"第四阶段共\",{\"1\":{\"819\":1}}],[\"第四阶段\",{\"1\":{\"818\":1}}],[\"第四范式\",{\"1\":{\"607\":1}}],[\"第3维\",{\"1\":{\"542\":1}}],[\"第3行\",{\"1\":{\"190\":2}}],[\"第2维\",{\"1\":{\"542\":1}}],[\"第2行\",{\"1\":{\"190\":2}}],[\"第1列到第3列\",{\"1\":{\"544\":1}}],[\"第1维\",{\"1\":{\"542\":1}}],[\"第1行\",{\"1\":{\"190\":2}}],[\"第0维\",{\"1\":{\"542\":1}}],[\"第三方代码实现\",{\"1\":{\"883\":1,\"890\":1}}],[\"第三个实验有\",{\"1\":{\"880\":1}}],[\"第三个学习目标是\",{\"1\":{\"386\":1}}],[\"第三维\",{\"1\":{\"709\":1}}],[\"第三张是狗\",{\"1\":{\"349\":1}}],[\"第三张图片不是同一个类别的信息\",{\"1\":{\"349\":1}}],[\"第三张图片不是一个类别\",{\"1\":{\"349\":1}}],[\"第三张图片和它们的距离比较远\",{\"1\":{\"349\":1}}],[\"第三张图片是狗\",{\"1\":{\"349\":1}}],[\"第三阶段\",{\"1\":{\"306\":1,\"813\":1}}],[\"第三步\",{\"1\":{\"143\":1}}],[\"第三层\",{\"1\":{\"120\":2}}],[\"第\",{\"1\":{\"122\":1,\"214\":2,\"274\":1,\"385\":1,\"472\":2,\"514\":1,\"709\":4,\"921\":1}}],[\"第500个样本\",{\"1\":{\"107\":1}}],[\"第二项\",{\"1\":{\"885\":1,\"932\":1}}],[\"第二项的解释\",{\"1\":{\"709\":1}}],[\"第二项即为我们的\",{\"1\":{\"235\":1}}],[\"第二维\",{\"1\":{\"709\":1}}],[\"第二部分是一个移动平均的编码器\",{\"1\":{\"352\":1}}],[\"第二张图片是人悲伤\",{\"1\":{\"349\":1}}],[\"第二张图的插值方式\",{\"1\":{\"264\":1}}],[\"第二张图的目标尺寸\",{\"1\":{\"264\":1}}],[\"第二步是利用\",{\"1\":{\"950\":1}}],[\"第二步\",{\"1\":{\"143\":1}}],[\"第二层\",{\"1\":{\"120\":1}}],[\"第二个误差叫做专注误差\",{\"1\":{\"960\":1}}],[\"第二个实验有\",{\"1\":{\"880\":1}}],[\"第二个维度的偏差乘上\",{\"1\":{\"578\":1}}],[\"第二个维度方差是\",{\"1\":{\"578\":1}}],[\"第二个维度\",{\"1\":{\"542\":2}}],[\"第二个全连接层\",{\"1\":{\"429\":1}}],[\"第二个归一化层\",{\"1\":{\"429\":1}}],[\"第二个线性层再把高维特征映射回原来的维度\",{\"1\":{\"429\":1}}],[\"第二个参数\",{\"1\":{\"425\":1}}],[\"第二个学习目标是\",{\"1\":{\"385\":1}}],[\"第二个分支的编码器就更新得太快了\",{\"1\":{\"356\":1}}],[\"第二个分支不能随着走这一支的样本使用梯度回传进行更新\",{\"1\":{\"356\":1}}],[\"第二个是在训练的时候尽量保持一致性\",{\"1\":{\"353\":1}}],[\"第二个点云2048个点\",{\"1\":{\"119\":1}}],[\"第二个\",{\"1\":{\"69\":1,\"204\":1}}],[\"第二分支\",{\"1\":{\"69\":1}}],[\"第二阶段总结\",{\"0\":{\"812\":1}}],[\"第二阶段将从第11步延续\",{\"1\":{\"799\":1}}],[\"第二阶段的\",{\"1\":{\"380\":1}}],[\"第二阶段\",{\"1\":{\"57\":1,\"235\":1,\"306\":1,\"385\":1,\"798\":1}}],[\"第一项\",{\"1\":{\"885\":1,\"932\":1}}],[\"第一维\",{\"1\":{\"709\":1}}],[\"第一次输入完整句子\",{\"1\":{\"663\":1}}],[\"第一部分是一个队列\",{\"1\":{\"352\":1}}],[\"第一张图是人高兴\",{\"1\":{\"349\":1}}],[\"第一张图的插值方式\",{\"1\":{\"264\":1}}],[\"第一张图的目标尺寸\",{\"1\":{\"264\":1}}],[\"第一步是构建潜变量\",{\"1\":{\"950\":1}}],[\"第一步需要完成图像的切片和嵌入\",{\"1\":{\"380\":1}}],[\"第一步首先是对\",{\"1\":{\"293\":1}}],[\"第一步\",{\"1\":{\"143\":1}}],[\"第一层是一个叫做\",{\"1\":{\"152\":1}}],[\"第一层\",{\"1\":{\"120\":2,\"926\":1}}],[\"第一个误差来自字典学习算法里的经典算法\",{\"1\":{\"960\":1}}],[\"第一个问题是\",{\"1\":{\"956\":1}}],[\"第一个卷积层用a类掩码\",{\"1\":{\"925\":1}}],[\"第一个实验有\",{\"1\":{\"880\":1}}],[\"第一个关键点\",{\"1\":{\"618\":1}}],[\"第一个维度方差是\",{\"1\":{\"578\":1}}],[\"第一个维度\",{\"1\":{\"542\":2}}],[\"第一个全连接层\",{\"1\":{\"429\":1}}],[\"第一个归一化层\",{\"1\":{\"429\":1}}],[\"第一个线性层将输入特征映射到更高维度的空间\",{\"1\":{\"429\":1}}],[\"第一个参数\",{\"1\":{\"425\":1}}],[\"第一个基于patch\",{\"1\":{\"388\":1}}],[\"第一个学习目标是\",{\"1\":{\"384\":1}}],[\"第一个是字典足够大\",{\"1\":{\"353\":1}}],[\"第一个向量提供了更细致的信息\",{\"1\":{\"142\":1}}],[\"第一个点\",{\"1\":{\"137\":1}}],[\"第一个批次\",{\"1\":{\"121\":1,\"122\":1}}],[\"第一个\",{\"1\":{\"69\":1,\"204\":1,\"380\":1,\"582\":1}}],[\"第一分支\",{\"1\":{\"69\":1}}],[\"第一阶段中\",{\"1\":{\"799\":1}}],[\"第一阶段总结\",{\"0\":{\"797\":1}}],[\"第一阶段共包含10个步骤\",{\"1\":{\"753\":1}}],[\"第一阶段设计了三个训练目标\",{\"1\":{\"417\":1}}],[\"第一阶段只训练一个\",{\"1\":{\"235\":1}}],[\"第一阶段通过最小化重建损失\",{\"1\":{\"235\":1}}],[\"第一阶段\",{\"1\":{\"57\":1,\"235\":1,\"306\":1,\"385\":1,\"752\":1}}],[\"信息仍是连续的\",{\"1\":{\"963\":1}}],[\"信息增益\",{\"1\":{\"950\":1}}],[\"信息单位是\",{\"1\":{\"906\":2}}],[\"信息量越大\",{\"1\":{\"906\":1}}],[\"信息量\",{\"0\":{\"905\":1},\"1\":{\"906\":2,\"909\":1}}],[\"信息论视角\",{\"0\":{\"950\":1}}],[\"信息论最核心的问题是\",{\"1\":{\"906\":1}}],[\"信息论\",{\"0\":{\"905\":1}}],[\"信息更新成本低\",{\"1\":{\"830\":1}}],[\"信息偏差\",{\"1\":{\"828\":1}}],[\"信息检索等场景\",{\"1\":{\"506\":1}}],[\"信息丢失\",{\"1\":{\"501\":1}}],[\"信息瓶颈\",{\"1\":{\"262\":1}}],[\"信息流\",{\"1\":{\"214\":1}}],[\"信息\",{\"1\":{\"57\":1,\"278\":1,\"293\":1}}],[\"把生成大图像的问题转换成了一个更简单的生成\",{\"1\":{\"961\":1}}],[\"把嵌入的更新和编码器的更新分开计算\",{\"1\":{\"961\":1}}],[\"把上面那个误差函数拆成了两部分\",{\"1\":{\"960\":1}}],[\"把图片编码成离散向量会更加自然\",{\"1\":{\"956\":1}}],[\"把图像变成一个短得多的向量\",{\"1\":{\"956\":1}}],[\"把图像编码成离散向量后\",{\"1\":{\"956\":1}}],[\"把图像\",{\"1\":{\"885\":1}}],[\"把图像当作外语\",{\"1\":{\"220\":1}}],[\"把这个\",{\"1\":{\"947\":1,\"951\":1}}],[\"把这些数字映射成它们发生的概率\",{\"1\":{\"846\":1}}],[\"把这些\",{\"1\":{\"735\":1}}],[\"把这些方向\",{\"1\":{\"578\":1}}],[\"把这些样本向量累加\",{\"1\":{\"213\":1}}],[\"把这些点的坐标归一化到以质心为中心的局部坐标系下\",{\"1\":{\"141\":1}}],[\"把这些抽象语义转化为\",{\"1\":{\"83\":1}}],[\"把第一个像素填入空图像\",{\"1\":{\"925\":1}}],[\"把世界事件映射成数字\",{\"1\":{\"846\":1}}],[\"把layernorm放到了前面\",{\"1\":{\"745\":1}}],[\"把llm的慢思考调动起来\",{\"1\":{\"619\":1}}],[\"把位置向量加到\",{\"1\":{\"710\":1}}],[\"把位置信息向量投影到和\",{\"1\":{\"709\":1}}],[\"把相对距离压缩到固定数量的桶里\",{\"1\":{\"710\":1}}],[\"把相似度矩阵对角线元素置为负无穷大\",{\"1\":{\"419\":1}}],[\"把新\",{\"1\":{\"663\":1}}],[\"把复杂问题分解成一系列的简单子问题\",{\"1\":{\"622\":1}}],[\"把要求尽可能明确\",{\"1\":{\"618\":1}}],[\"把人类的反馈\",{\"1\":{\"602\":1}}],[\"把数据的不同尺度和相关性都考虑进来\",{\"1\":{\"577\":1}}],[\"把数据转换成llm能识别的格式\",{\"1\":{\"416\":1}}],[\"把向量中每个元素除以向量的欧几里得长度\",{\"1\":{\"508\":1}}],[\"把向量中每个元素除以向量元素的绝对值之和\",{\"1\":{\"507\":1}}],[\"把一个\",{\"1\":{\"479\":1}}],[\"把12拆分成3和4\",{\"1\":{\"478\":1}}],[\"把c和d合并成一个维度\",{\"1\":{\"478\":1}}],[\"把原函数的\",{\"1\":{\"454\":1}}],[\"把原始点云\",{\"1\":{\"137\":1}}],[\"把query\",{\"1\":{\"419\":1}}],[\"把q\",{\"1\":{\"417\":1}}],[\"把当前\",{\"1\":{\"353\":1}}],[\"把每个\",{\"1\":{\"899\":1}}],[\"把每次训练的\",{\"1\":{\"353\":1}}],[\"把每张图像当作一个独立类别\",{\"1\":{\"282\":1}}],[\"把f12\",{\"1\":{\"353\":1}}],[\"把对比学习看成了是一个字典查询的过程\",{\"1\":{\"352\":1}}],[\"把自监督学习解释为\",{\"1\":{\"282\":1}}],[\"把不同进程上的\",{\"1\":{\"274\":1}}],[\"把不同尺度学到的特征拼接在一起\",{\"1\":{\"141\":1}}],[\"把输入的\",{\"1\":{\"274\":1}}],[\"把所有标注\",{\"1\":{\"268\":1}}],[\"把属于同一个\",{\"1\":{\"213\":1}}],[\"把属于同一簇的样本向量加到对应的中心上\",{\"1\":{\"213\":1}}],[\"把空簇的计数临时设为\",{\"1\":{\"213\":1}}],[\"把全局特征复制\",{\"1\":{\"154\":1}}],[\"把它们\",{\"1\":{\"152\":1}}],[\"把它们相对于关键点的位置进行归一化\",{\"1\":{\"137\":1}}],[\"把邻域点的数据整理成适合卷积的格式\",{\"1\":{\"137\":1}}],[\"把邻近点的坐标和特征拼接在一起\",{\"1\":{\"137\":1}}],[\"把距离超过\",{\"1\":{\"137\":1}}],[\"把刚才找到的邻近点的坐标提取出来\",{\"1\":{\"137\":1}}],[\"把他的大小归一化到一个球中\",{\"1\":{\"131\":1}}],[\"把\",{\"1\":{\"57\":1,\"213\":1,\"258\":1,\"274\":2,\"355\":1,\"457\":1,\"710\":1,\"958\":2}}],[\"把手\",{\"1\":{\"56\":1}}],[\"➜\",{\"1\":{\"57\":2}}],[\"qwq\",{\"1\":{\"823\":2,\"837\":2}}],[\"qwen3\",{\"1\":{\"823\":2}}],[\"qwen2\",{\"1\":{\"823\":3}}],[\"qwen\",{\"1\":{\"300\":1,\"309\":1,\"325\":1,\"326\":1,\"823\":4}}],[\"qnli\",{\"1\":{\"685\":1}}],[\"qqp\",{\"1\":{\"634\":1}}],[\"q进行\",{\"1\":{\"614\":1}}],[\"qlora就是量化版的lora\",{\"1\":{\"607\":1}}],[\"qlora\",{\"0\":{\"607\":1},\"1\":{\"607\":1,\"614\":2}}],[\"qllama利用其大规模参数重组视觉表示并生成文本\",{\"1\":{\"304\":1}}],[\"qllama的优势包括\",{\"1\":{\"304\":1}}],[\"qllama是一个80亿参数的语言中间件\",{\"1\":{\"304\":1}}],[\"qllama\",{\"1\":{\"296\":1,\"303\":1,\"304\":1,\"305\":4,\"306\":5,\"309\":1,\"310\":1,\"311\":4,\"312\":1,\"313\":1,\"316\":1,\"317\":2}}],[\"q12\",{\"1\":{\"505\":1}}],[\"q11\",{\"1\":{\"505\":1}}],[\"q22\",{\"1\":{\"505\":1}}],[\"q21\",{\"1\":{\"505\":1}}],[\"q2t\",{\"1\":{\"418\":2}}],[\"q来自query\",{\"1\":{\"420\":1}}],[\"qk^t\",{\"1\":{\"380\":1,\"537\":1}}],[\"qk\",{\"1\":{\"380\":10,\"428\":1,\"429\":2,\"430\":2,\"431\":2}}],[\"qkv\",{\"1\":{\"205\":2,\"380\":18,\"428\":1,\"429\":2,\"430\":8,\"431\":2,\"582\":5,\"611\":1}}],[\"q=images\",{\"1\":{\"359\":1}}],[\"q=6\",{\"1\":{\"190\":1}}],[\"qa\",{\"1\":{\"342\":1,\"346\":1,\"656\":1,\"668\":1,\"733\":2,\"734\":2,\"735\":1}}],[\"qy\",{\"1\":{\"256\":4,\"260\":4,\"899\":2}}],[\"quac\",{\"1\":{\"648\":2}}],[\"quantisation\",{\"1\":{\"960\":1}}],[\"quantizer\",{\"1\":{\"213\":5}}],[\"quantize\",{\"1\":{\"213\":11}}],[\"quantized\",{\"1\":{\"209\":2,\"213\":1,\"607\":1,\"614\":1,\"963\":12,\"964\":2}}],[\"quantization\",{\"1\":{\"213\":1,\"249\":1,\"502\":1,\"607\":1,\"899\":1}}],[\"quant\",{\"1\":{\"213\":1}}],[\"quora\",{\"1\":{\"634\":2}}],[\"queies\",{\"1\":{\"420\":1}}],[\"queue\",{\"1\":{\"190\":20,\"192\":16,\"205\":13,\"206\":3,\"361\":6,\"362\":1,\"364\":5,\"474\":1,\"475\":1}}],[\"queries被用来从image\",{\"1\":{\"417\":1}}],[\"queries是一组可学习的embeddings\",{\"1\":{\"417\":1}}],[\"queries\",{\"1\":{\"94\":1,\"100\":3,\"274\":8,\"306\":1,\"362\":1,\"417\":3,\"418\":1,\"419\":1,\"421\":3}}],[\"query的数量\",{\"1\":{\"524\":1}}],[\"query的响应图进行平均池化\",{\"1\":{\"100\":1}}],[\"query和query\",{\"1\":{\"418\":1}}],[\"query是基于欧氏距离的均匀性假设\",{\"1\":{\"135\":1}}],[\"query通过确保每个局部区域都有一个固定的尺度\",{\"1\":{\"135\":1}}],[\"query通过固定区域尺度而不是固定邻居数量来定义邻域\",{\"1\":{\"135\":1}}],[\"query找到该查询点在半径为𝑟范围内点\",{\"1\":{\"135\":1}}],[\"query来查询形心的邻居点\",{\"1\":{\"135\":1}}],[\"queryandgroup\",{\"1\":{\"119\":4,\"121\":1}}],[\"query=self\",{\"1\":{\"100\":1}}],[\"query=x\",{\"1\":{\"99\":1}}],[\"query=q\",{\"1\":{\"99\":1}}],[\"query\",{\"1\":{\"24\":1,\"38\":1,\"69\":13,\"94\":1,\"96\":8,\"98\":8,\"100\":10,\"119\":1,\"137\":7,\"141\":2,\"143\":1,\"190\":1,\"207\":6,\"243\":2,\"273\":1,\"274\":4,\"306\":1,\"353\":1,\"355\":1,\"357\":1,\"359\":2,\"362\":3,\"399\":1,\"401\":6,\"402\":1,\"411\":4,\"412\":4,\"417\":8,\"418\":2,\"419\":17,\"420\":30,\"421\":5,\"475\":2,\"523\":1,\"524\":1,\"526\":1,\"528\":1,\"529\":2,\"531\":2,\"533\":1,\"663\":7,\"710\":15,\"724\":7,\"751\":9,\"823\":4}}],[\"questions这三个问答任务中\",{\"1\":{\"648\":1}}],[\"questions\",{\"1\":{\"641\":1,\"658\":1}}],[\"question数据可视化图\",{\"1\":{\"92\":1}}],[\"question数据可视化\",{\"1\":{\"87\":1}}],[\"question\",{\"0\":{\"87\":1},\"1\":{\"92\":6,\"100\":1,\"105\":2,\"106\":2,\"382\":1,\"383\":1,\"634\":1,\"640\":1,\"733\":1,\"877\":1}}],[\"question4\",{\"1\":{\"52\":2}}],[\"question3\",{\"1\":{\"52\":2}}],[\"question2\",{\"1\":{\"52\":2}}],[\"question1\",{\"1\":{\"52\":2}}],[\"qid\",{\"1\":{\"92\":5}}],[\"qformer\",{\"1\":{\"64\":1,\"311\":1,\"417\":2,\"419\":1,\"420\":3,\"421\":1}}],[\"q\",{\"0\":{\"525\":1},\"1\":{\"56\":8,\"64\":1,\"69\":9,\"94\":1,\"96\":6,\"98\":3,\"99\":5,\"100\":2,\"119\":9,\"190\":11,\"192\":2,\"213\":14,\"256\":1,\"260\":2,\"261\":3,\"293\":2,\"359\":2,\"361\":4,\"362\":9,\"363\":3,\"380\":17,\"417\":3,\"420\":2,\"421\":3,\"430\":5,\"475\":2,\"523\":1,\"529\":1,\"538\":2,\"582\":2,\"661\":1,\"663\":4,\"703\":3,\"709\":17,\"710\":5,\"751\":1,\"899\":1,\"908\":1,\"909\":4,\"915\":1,\"932\":1,\"935\":1,\"959\":1,\"963\":2}}],[\"强推理能力\",{\"1\":{\"823\":1}}],[\"强记忆型的语言预测器\",{\"1\":{\"649\":1}}],[\"强烈建议\",{\"1\":{\"493\":1}}],[\"强烈建议传入连续张量\",{\"1\":{\"493\":3}}],[\"强视觉编码器\",{\"1\":{\"332\":1}}],[\"强视觉表征\",{\"1\":{\"323\":1}}],[\"强大的神经网络模型\",{\"1\":{\"822\":1}}],[\"强大的视觉编码器\",{\"1\":{\"322\":1,\"323\":1}}],[\"强大性能\",{\"1\":{\"296\":1}}],[\"强度\",{\"1\":{\"159\":1}}],[\"强调责任约束\",{\"1\":{\"669\":1}}],[\"强调未来开发和部署语言模型时需格外关注其社会影响及安全性\",{\"1\":{\"654\":1}}],[\"强调它们在无需微调的情况下就能取得良好效果\",{\"1\":{\"647\":1}}],[\"强调预测与\",{\"1\":{\"587\":1,\"588\":1}}],[\"强调位置编码的重要性\",{\"1\":{\"110\":1}}],[\"强调分布匹配\",{\"1\":{\"106\":1}}],[\"强调正类\",{\"1\":{\"102\":1}}],[\"强化系数\",{\"1\":{\"895\":1}}],[\"强化复杂任务处理能力\",{\"1\":{\"823\":1}}],[\"强化学习\",{\"0\":{\"842\":1},\"1\":{\"656\":1}}],[\"强化学习来自人类反馈\",{\"1\":{\"655\":1}}],[\"强化学习微调\",{\"1\":{\"654\":1}}],[\"强化学习方法\",{\"1\":{\"73\":1,\"75\":1}}],[\"强化了模型对长上下文的理解和生成\",{\"1\":{\"828\":1}}],[\"强化了模型的代码能力\",{\"1\":{\"823\":1}}],[\"强化了\",{\"1\":{\"650\":1}}],[\"强化多义性和类间相似度学习\",{\"1\":{\"204\":1}}],[\"强化三维点的空间关系\",{\"1\":{\"125\":1}}],[\"强化人类意图在物体语义推理中的引导作用\",{\"1\":{\"56\":1}}],[\"强化\",{\"1\":{\"56\":1}}],[\"强制屏蔽掉不允许看的区域\",{\"1\":{\"926\":1}}],[\"强制使用确定性的卷积算法\",{\"1\":{\"521\":1}}],[\"强制学生跨视图对齐\",{\"1\":{\"293\":1}}],[\"强制学习正交变换矩阵\",{\"1\":{\"157\":1}}],[\"强制将每个样本开头的\",{\"1\":{\"188\":1}}],[\"强制\",{\"1\":{\"26\":1}}],[\"交替训练判别器和生成器\",{\"1\":{\"918\":1}}],[\"交替作用\",{\"1\":{\"97\":1}}],[\"交和补运算\",{\"1\":{\"847\":1}}],[\"交并比\",{\"1\":{\"588\":2}}],[\"交换操作数实现a\",{\"1\":{\"809\":2}}],[\"交换维度顺序\",{\"1\":{\"478\":1}}],[\"交换张量的第1维和第2维\",{\"1\":{\"83\":1}}],[\"交错堆叠\",{\"1\":{\"434\":1}}],[\"交集\",{\"1\":{\"106\":1,\"586\":1,\"847\":1}}],[\"交叉熵定义为\",{\"1\":{\"910\":1}}],[\"交叉熵可以看作是平均信息量\",{\"1\":{\"909\":1}}],[\"交叉熵会大于熵\",{\"1\":{\"908\":1}}],[\"交叉熵衡量用分布\",{\"1\":{\"908\":1}}],[\"交叉熵\",{\"0\":{\"905\":1},\"1\":{\"908\":1,\"909\":1}}],[\"交叉熵在总损失中的占比越高\",{\"1\":{\"592\":1}}],[\"交叉熵是一个多分类问题的损失函数\",{\"1\":{\"355\":1}}],[\"交叉熵损失是一种衡量两个概率分布差异的指标\",{\"1\":{\"910\":1}}],[\"交叉熵损失为\",{\"1\":{\"589\":1}}],[\"交叉熵损失函数\",{\"1\":{\"355\":1}}],[\"交叉熵损失进行训练\",{\"1\":{\"273\":1}}],[\"交叉熵损失\",{\"0\":{\"910\":1},\"1\":{\"208\":1,\"592\":2}}],[\"交叉熵损失监督\",{\"1\":{\"79\":1}}],[\"交叉注意力运算\",{\"1\":{\"420\":1}}],[\"交叉注意力则key和value都来自图像\",{\"1\":{\"420\":1}}],[\"交叉注意力\",{\"1\":{\"272\":1,\"420\":1}}],[\"交叉注意力建模交互上下文\",{\"1\":{\"78\":1}}],[\"交叉注意力模拟了这种双向推理过程\",{\"1\":{\"56\":1}}],[\"交互式可视化网站\",{\"1\":{\"854\":1}}],[\"交互的应用程序\",{\"1\":{\"834\":1}}],[\"交互层\",{\"1\":{\"832\":1}}],[\"交互浅\",{\"1\":{\"368\":1,\"369\":1}}],[\"交互难度大\",{\"1\":{\"194\":1}}],[\"交互区域特征\",{\"1\":{\"83\":1}}],[\"交互图片里的区域\",{\"1\":{\"83\":1}}],[\"交互图像\",{\"1\":{\"53\":1}}],[\"交互图像与\",{\"1\":{\"40\":1}}],[\"交互发生的场景背景\",{\"1\":{\"83\":1}}],[\"交互信号能准确落到\",{\"1\":{\"83\":1}}],[\"交互信息与图像特征的融合\",{\"0\":{\"58\":1}}],[\"交互信息文本\",{\"1\":{\"53\":1}}],[\"交互主体区域特征图\",{\"1\":{\"83\":1}}],[\"交互主体区域特征\",{\"1\":{\"83\":2}}],[\"交互主体框\",{\"1\":{\"82\":2}}],[\"交互\",{\"1\":{\"80\":1,\"100\":1}}],[\"交互文本信息与图像信息进行融合\",{\"1\":{\"54\":1}}],[\"交互文本和几何结构文本的信息通过改良的交叉注意力机制进行交互融合\",{\"1\":{\"54\":1}}],[\"交互行为名\",{\"1\":{\"53\":1}}],[\"交互知识\",{\"1\":{\"52\":1}}],[\"交互部位\",{\"1\":{\"52\":1}}],[\"交互类比推理\",{\"1\":{\"36\":1}}],[\"交互细节描述\",{\"1\":{\"36\":1}}],[\"改善了对长文本的理解和生成能力\",{\"1\":{\"823\":1}}],[\"改善单模态编码语义理解\",{\"1\":{\"194\":1}}],[\"改用50k词汇表的字节级bpe编码\",{\"1\":{\"683\":1}}],[\"改用动态掩码\",{\"1\":{\"679\":1}}],[\"改写为条件形式\",{\"1\":{\"952\":1}}],[\"改写\",{\"1\":{\"254\":1}}],[\"改变维度顺序\",{\"1\":{\"469\":1}}],[\"改变\",{\"1\":{\"161\":1}}],[\"改变输出维度\",{\"1\":{\"122\":1}}],[\"改进之后\",{\"1\":{\"921\":1}}],[\"改进了推理能力和指令遵循能力\",{\"1\":{\"823\":1}}],[\"改进了可供性定位精度\",{\"1\":{\"20\":1}}],[\"改进的\",{\"1\":{\"899\":1}}],[\"改进的合并策略\",{\"1\":{\"640\":1}}],[\"改进的字节对编码\",{\"1\":{\"640\":1}}],[\"改进的位置编码机制\",{\"1\":{\"125\":1}}],[\"改进版本\",{\"1\":{\"921\":1}}],[\"改进版\",{\"1\":{\"633\":1}}],[\"改进版的\",{\"1\":{\"633\":1}}],[\"改进而来\",{\"1\":{\"589\":1}}],[\"改进方法\",{\"1\":{\"369\":1}}],[\"改进\",{\"1\":{\"157\":6}}],[\"改为像素重建\",{\"1\":{\"242\":1}}],[\"改为\",{\"1\":{\"87\":1,\"739\":1,\"935\":1,\"951\":1}}],[\"改良的交叉注意力\",{\"0\":{\"56\":1}}],[\"均和vae实现部分保持一致\",{\"1\":{\"936\":1}}],[\"均能支持流式处理\",{\"1\":{\"833\":1}}],[\"均能正确构建反向传播路径\",{\"1\":{\"811\":1}}],[\"均是混合推理模型\",{\"1\":{\"823\":1}}],[\"均经过严格去重\",{\"1\":{\"667\":1}}],[\"均冻结\",{\"1\":{\"415\":1}}],[\"均匀输出\",{\"1\":{\"290\":1}}],[\"均匀分布随机数填充\",{\"1\":{\"479\":1}}],[\"均匀分布\",{\"1\":{\"282\":1,\"907\":1}}],[\"均匀分布意味着我们希望所有\",{\"1\":{\"262\":1}}],[\"均匀采样\",{\"1\":{\"263\":1}}],[\"均匀性假设\",{\"1\":{\"135\":1}}],[\"均值和方差\",{\"1\":{\"862\":1}}],[\"均值是\",{\"1\":{\"578\":1}}],[\"均值\",{\"1\":{\"474\":1,\"947\":1}}],[\"均值向量就作为\",{\"1\":{\"213\":1}}],[\"均值作为最终评估指标\",{\"1\":{\"106\":1}}],[\"均有性能提升\",{\"1\":{\"177\":1}}],[\"均保持不变\",{\"1\":{\"54\":1}}],[\"均优于现有方法\",{\"1\":{\"19\":1}}],[\"7x7\",{\"1\":{\"923\":1,\"926\":1,\"963\":2}}],[\"79\",{\"1\":{\"668\":1,\"670\":1}}],[\"71\",{\"1\":{\"657\":1}}],[\"73±1\",{\"1\":{\"656\":1}}],[\"7个百分点\",{\"1\":{\"641\":1}}],[\"7×7\",{\"1\":{\"501\":6,\"502\":3}}],[\"7py1jdq1wp0nnyt3a\",{\"1\":{\"424\":1}}],[\"77\",{\"1\":{\"342\":1,\"669\":1,\"683\":1}}],[\"774\",{\"1\":{\"341\":1}}],[\"7b的0\",{\"1\":{\"668\":1,\"670\":1}}],[\"7b在1t\",{\"1\":{\"666\":1}}],[\"7b\",{\"1\":{\"305\":1,\"306\":3,\"315\":2,\"344\":1,\"823\":4}}],[\"7b初始化\",{\"1\":{\"304\":1}}],[\"7b初始化的80亿参数语言中间件\",{\"1\":{\"303\":1}}],[\"74\",{\"1\":{\"291\":1}}],[\"784\",{\"1\":{\"934\":1,\"935\":1,\"938\":1}}],[\"78\",{\"1\":{\"280\":1,\"683\":2}}],[\"7856\",{\"1\":{\"215\":1}}],[\"75\",{\"1\":{\"236\":1,\"440\":1,\"574\":1,\"589\":3,\"895\":1}}],[\"751\",{\"1\":{\"89\":1,\"91\":1,\"93\":1}}],[\"7章\",{\"1\":{\"179\":1}}],[\"76gb\",{\"1\":{\"678\":1,\"680\":1}}],[\"762m和1\",{\"1\":{\"641\":1}}],[\"76\",{\"1\":{\"117\":1,\"280\":1,\"291\":1}}],[\"768\",{\"1\":{\"59\":2,\"70\":2,\"83\":2,\"94\":1,\"205\":4,\"213\":1,\"236\":1,\"426\":4,\"427\":6,\"428\":4,\"429\":1,\"431\":4,\"633\":1,\"712\":3,\"718\":2}}],[\"704\",{\"1\":{\"713\":2}}],[\"70b的55\",{\"1\":{\"668\":1}}],[\"70b\",{\"1\":{\"666\":1,\"668\":2,\"823\":4}}],[\"70b和palm\",{\"1\":{\"665\":1}}],[\"700\",{\"1\":{\"268\":1,\"833\":1}}],[\"70\",{\"1\":{\"117\":2}}],[\"72b\",{\"1\":{\"823\":1}}],[\"72\",{\"1\":{\"117\":1,\"291\":1,\"668\":1,\"685\":1}}],[\"7\",{\"0\":{\"92\":1,\"431\":1},\"1\":{\"54\":2,\"64\":1,\"82\":1,\"83\":5,\"99\":1,\"106\":1,\"110\":1,\"150\":1,\"190\":4,\"206\":1,\"208\":1,\"293\":1,\"306\":1,\"352\":1,\"362\":1,\"384\":2,\"385\":2,\"421\":1,\"441\":3,\"462\":2,\"481\":1,\"482\":3,\"484\":1,\"501\":2,\"513\":1,\"540\":2,\"541\":2,\"542\":1,\"544\":2,\"590\":2,\"595\":1,\"625\":1,\"640\":1,\"641\":4,\"648\":3,\"649\":1,\"655\":1,\"656\":1,\"657\":1,\"660\":1,\"663\":1,\"668\":4,\"670\":1,\"683\":2,\"684\":1,\"685\":1,\"710\":3,\"809\":1,\"823\":6,\"847\":1,\"892\":1,\"893\":3,\"926\":6,\"956\":1,\"964\":10}}],[\"不让它跑到离嵌入空间里的向量太远的地方\",{\"1\":{\"960\":1}}],[\"不强制对数据结构做过多限制\",{\"1\":{\"942\":1}}],[\"不允许看到当前像素\",{\"1\":{\"926\":1}}],[\"不屏蔽任何位置\",{\"1\":{\"926\":1}}],[\"不断把上一个时刻的输出作为输入\",{\"1\":{\"925\":1}}],[\"不断对点云进行下采样\",{\"1\":{\"144\":1}}],[\"不泄露未来信息\",{\"1\":{\"924\":1}}],[\"不看任何输入\",{\"1\":{\"924\":1}}],[\"不妨对比下面这张示意图和上面那张示意图\",{\"1\":{\"923\":1}}],[\"不难看出\",{\"1\":{\"923\":1}}],[\"不涉及后续的改进版本\",{\"1\":{\"921\":1}}],[\"不涉及新技术的引入\",{\"1\":{\"380\":1}}],[\"不存在\",{\"1\":{\"913\":1}}],[\"不反向传播梯度\",{\"1\":{\"899\":1}}],[\"不重叠\",{\"1\":{\"893\":1}}],[\"不添加\",{\"1\":{\"887\":1}}],[\"不等式得到下界\",{\"1\":{\"885\":1}}],[\"不等式\",{\"1\":{\"885\":1,\"909\":1}}],[\"不幸的是\",{\"1\":{\"878\":1}}],[\"不碰那些太反直觉或病态的集合\",{\"1\":{\"847\":1}}],[\"不公开\",{\"1\":{\"823\":1}}],[\"不具备\",{\"1\":{\"809\":1}}],[\"不具备生成能力\",{\"1\":{\"735\":1}}],[\"不构建计算图\",{\"1\":{\"807\":1}}],[\"不保留中间结果\",{\"1\":{\"807\":1}}],[\"不当的内存管理可能导致内存泄漏或程序崩溃\",{\"1\":{\"806\":1}}],[\"不计入答案\",{\"1\":{\"735\":1}}],[\"不计算梯度\",{\"1\":{\"895\":1}}],[\"不计算\",{\"1\":{\"514\":1}}],[\"不足长度用padding填充\",{\"1\":{\"713\":1}}],[\"不为每个距离单独学一个参数\",{\"1\":{\"710\":1}}],[\"不为空\",{\"1\":{\"663\":1}}],[\"不直接加到注意力分数里\",{\"1\":{\"709\":1}}],[\"不直接优化最终目标\",{\"1\":{\"106\":1}}],[\"不少人很自然地有了这样子的想法\",{\"1\":{\"690\":1}}],[\"不跨文档\",{\"1\":{\"683\":1}}],[\"不再使用\",{\"1\":{\"926\":1}}],[\"不再需要外部的\",{\"1\":{\"892\":1}}],[\"不再预先固定掩码模式\",{\"1\":{\"681\":1}}],[\"不再这里一一介绍\",{\"1\":{\"607\":1}}],[\"不展开细聊\",{\"1\":{\"663\":1}}],[\"不展开计算细节\",{\"1\":{\"574\":1}}],[\"不代表所有潜在用户\",{\"1\":{\"658\":1}}],[\"不如在现有模型上投资对齐方法\",{\"1\":{\"658\":1}}],[\"不如抽样一部分数据来计算损失\",{\"1\":{\"355\":1}}],[\"不遵循指令等问题\",{\"1\":{\"654\":1}}],[\"不利于学习率等超参数设置\",{\"1\":{\"893\":1}}],[\"不利于迁移与泛化\",{\"1\":{\"650\":1}}],[\"不利于高维空间建模\",{\"1\":{\"157\":1}}],[\"不给任何示例\",{\"1\":{\"647\":1}}],[\"不清楚在学习文本表示时\",{\"1\":{\"626\":1}}],[\"不要求互斥\",{\"1\":{\"848\":1}}],[\"不要求原始张量是连续的\",{\"1\":{\"470\":1}}],[\"不要预训练\",{\"1\":{\"635\":1}}],[\"不要让llm只生成最合适的唯一一个结果\",{\"1\":{\"621\":1}}],[\"不变\",{\"1\":{\"609\":1}}],[\"不变的情况下\",{\"1\":{\"235\":1}}],[\"不平衡数据集的一个示例可能是一组数以千计的云彩照片\",{\"1\":{\"561\":1}}],[\"不平衡标签\",{\"1\":{\"513\":1}}],[\"不管理\",{\"1\":{\"557\":1}}],[\"不管掩码图像\",{\"1\":{\"235\":1}}],[\"不一定是字节\",{\"1\":{\"542\":1}}],[\"不一致\",{\"1\":{\"356\":1}}],[\"不一致则重新随机选\",{\"1\":{\"53\":1}}],[\"不相关\",{\"1\":{\"574\":1}}],[\"不相似\",{\"1\":{\"506\":1}}],[\"不相邻\",{\"1\":{\"480\":1}}],[\"不对应实际的\",{\"1\":{\"502\":1}}],[\"不对同索引的学生视图进行监督\",{\"1\":{\"293\":1}}],[\"不复制任何数据\",{\"1\":{\"490\":1}}],[\"不复制数据\",{\"1\":{\"470\":1}}],[\"不创建新张量\",{\"1\":{\"479\":1}}],[\"不会引入真正意义上的正则化因子\",{\"1\":{\"951\":1}}],[\"不会作为可训练参数\",{\"1\":{\"926\":1}}],[\"不会被模型训练修改\",{\"1\":{\"892\":1}}],[\"不会被当作模型的可训练参数\",{\"1\":{\"474\":1}}],[\"不会过滤掉任何词\",{\"1\":{\"595\":1}}],[\"不会影响其他环境或系统\",{\"1\":{\"557\":1}}],[\"不会影响主进程或其他进程\",{\"1\":{\"520\":1}}],[\"不会产生块状感\",{\"1\":{\"505\":1}}],[\"不会对像素值进行加权平均或平滑处理\",{\"1\":{\"504\":1}}],[\"不会只看一个村\",{\"1\":{\"502\":1}}],[\"不会进行任何复制操作\",{\"1\":{\"491\":1}}],[\"不会出现在model\",{\"1\":{\"474\":1}}],[\"不会跟踪计算图\",{\"1\":{\"473\":1}}],[\"不会复制数据\",{\"1\":{\"469\":1}}],[\"不会泄漏到外部\",{\"1\":{\"444\":1}}],[\"不产生新作用域\",{\"1\":{\"444\":3}}],[\"不产生梯度\",{\"1\":{\"213\":1}}],[\"不适合处理需要复杂推理的\",{\"1\":{\"369\":1}}],[\"不适合大规模场景\",{\"1\":{\"110\":1}}],[\"不就是分类操作吗\",{\"1\":{\"353\":1}}],[\"不是编码为\",{\"1\":{\"963\":1}}],[\"不是高斯分布\",{\"1\":{\"949\":1}}],[\"不是标准正态分布\",{\"1\":{\"945\":1}}],[\"不是标准差\",{\"1\":{\"867\":1}}],[\"不是像离散情况那样简单地枚举出来的\",{\"1\":{\"847\":1}}],[\"不是自回归生成器\",{\"1\":{\"735\":1}}],[\"不是三角函数\",{\"1\":{\"692\":1}}],[\"不是一个常数\",{\"1\":{\"521\":1}}],[\"不是一千个类别\",{\"1\":{\"350\":1}}],[\"不是视图\",{\"1\":{\"471\":1}}],[\"不是函数原文档\",{\"1\":{\"454\":1}}],[\"不是\",{\"1\":{\"454\":1}}],[\"不是在序列维度上做注意力\",{\"1\":{\"119\":1}}],[\"不做预训练\",{\"1\":{\"344\":1}}],[\"不做任何变化\",{\"1\":{\"152\":1}}],[\"不含\",{\"1\":{\"317\":1,\"892\":1}}],[\"不监督同索引的学生视图\",{\"1\":{\"293\":1}}],[\"不必显式区分图像也能学到好特征\",{\"1\":{\"282\":1}}],[\"不限制\",{\"1\":{\"263\":1}}],[\"不可微的\",{\"1\":{\"946\":1}}],[\"不可学习的\",{\"1\":{\"706\":1}}],[\"不可导\",{\"1\":{\"258\":1}}],[\"不可靠\",{\"1\":{\"204\":1}}],[\"不启用kv\",{\"1\":{\"663\":1}}],[\"不启用\",{\"1\":{\"255\":1}}],[\"不过这里要注意一点\",{\"1\":{\"950\":1}}],[\"不过这种标签信息\",{\"1\":{\"349\":1}}],[\"不过我们知道\",{\"1\":{\"947\":1}}],[\"不过一个更恰当的术语可能是\",{\"1\":{\"867\":1}}],[\"不过迭代次数较多\",{\"1\":{\"816\":1}}],[\"不过\",{\"1\":{\"220\":1,\"292\":1,\"562\":1,\"949\":1,\"951\":1}}],[\"不通过梯度更新\",{\"1\":{\"213\":1}}],[\"不通过缓存直接将内容打印到屏幕上\",{\"1\":{\"107\":1}}],[\"不参与训练\",{\"1\":{\"213\":1,\"892\":1}}],[\"不参与梯度更新\",{\"1\":{\"213\":1}}],[\"不参与反向传播\",{\"1\":{\"206\":1}}],[\"不修改原始函数代码的前提下\",{\"1\":{\"450\":1}}],[\"不修改\",{\"1\":{\"208\":1}}],[\"不匹配\",{\"1\":{\"207\":1}}],[\"不考虑元素之间的顺序\",{\"0\":{\"882\":1}}],[\"不考虑各维度数据的分布特征\",{\"1\":{\"576\":1}}],[\"不考虑从动量队列拿到的负样本\",{\"1\":{\"207\":1}}],[\"不考虑空间邻域关系\",{\"1\":{\"152\":1}}],[\"不包括输出层\",{\"1\":{\"926\":1}}],[\"不包括\",{\"1\":{\"162\":1}}],[\"不改变形状和大小\",{\"1\":{\"162\":1}}],[\"不改变物体形状和内部结构\",{\"1\":{\"161\":1}}],[\"不改变\",{\"1\":{\"161\":1}}],[\"不常用\",{\"1\":{\"160\":1,\"208\":1,\"733\":1}}],[\"不够大\",{\"1\":{\"157\":1}}],[\"不够精细\",{\"1\":{\"157\":1}}],[\"不稳定\",{\"1\":{\"152\":1}}],[\"不灵活等问题\",{\"1\":{\"148\":1}}],[\"不带法向量或其他属性\",{\"1\":{\"146\":1}}],[\"不能为负\",{\"1\":{\"931\":1}}],[\"不能看自己\",{\"1\":{\"924\":1}}],[\"不能看到\",{\"1\":{\"924\":1}}],[\"不能看到当前像素和右下角的像素\",{\"1\":{\"924\":1}}],[\"不能看到第\",{\"1\":{\"921\":1}}],[\"不能\",{\"1\":{\"735\":1}}],[\"不能超出上下文范围\",{\"1\":{\"735\":1}}],[\"不能超过数据集大小\",{\"1\":{\"518\":1}}],[\"不能像\",{\"1\":{\"735\":1}}],[\"不能用\",{\"1\":{\"612\":1}}],[\"不能变成\",{\"1\":{\"472\":1}}],[\"不能同时得到和图像对齐的纯文本表示\",{\"1\":{\"268\":1}}],[\"不能直接用反向传播训练神经网络\",{\"1\":{\"257\":1}}],[\"不能直接使用这些操作\",{\"1\":{\"145\":1}}],[\"不能作为\",{\"1\":{\"106\":1}}],[\"不进行任何量化\",{\"1\":{\"502\":1}}],[\"不进行采样\",{\"1\":{\"137\":1}}],[\"不进行\",{\"1\":{\"123\":2}}],[\"不规则的点云数据设计\",{\"1\":{\"121\":1}}],[\"不同头可以学习到不同的相对位置偏置模式\",{\"1\":{\"710\":1}}],[\"不同字母\",{\"1\":{\"709\":1}}],[\"不同的\",{\"1\":{\"690\":1}}],[\"不同的措辞\",{\"1\":{\"649\":1}}],[\"不同的消融研究如下表5\",{\"1\":{\"635\":1}}],[\"不同的分类角度\",{\"1\":{\"602\":1}}],[\"不同的相似性匹配损失\",{\"1\":{\"282\":1}}],[\"不同类别的\",{\"1\":{\"574\":1}}],[\"不同类别的物体处于不相邻的区域\",{\"1\":{\"349\":1}}],[\"不同样本累加到不同的簇行\",{\"1\":{\"487\":1}}],[\"不同样本的点云可能包含不同数量的点\",{\"1\":{\"119\":1}}],[\"不同特征之间能够进行更多样的组合\",{\"1\":{\"429\":1}}],[\"不同模态数据的提取与融合\",{\"1\":{\"416\":1}}],[\"不同模态使用不同的专家\",{\"1\":{\"222\":1}}],[\"不同于固定编码\",{\"1\":{\"707\":1}}],[\"不同于\",{\"1\":{\"373\":1}}],[\"不同配置对性能影响较小\",{\"1\":{\"304\":1}}],[\"不同分辨率的输入会被分组\",{\"1\":{\"293\":1}}],[\"不同\",{\"1\":{\"228\":1,\"268\":1,\"273\":1,\"376\":1,\"491\":1,\"666\":1,\"710\":1}}],[\"不同下游任务常常需要不同架构\",{\"1\":{\"220\":1}}],[\"不同位置\",{\"1\":{\"152\":1}}],[\"不同尺度的查询半径列表\",{\"1\":{\"141\":1}}],[\"不同尺度的特征被串联形成多尺度特征向量\",{\"1\":{\"140\":1}}],[\"不同粒度下准确回答可供性相关问题\",{\"1\":{\"19\":1}}],[\"不使用任何信息\",{\"1\":{\"924\":1}}],[\"不使用传统的循环或卷积结构\",{\"1\":{\"741\":1}}],[\"不使用\",{\"1\":{\"117\":1,\"272\":1}}],[\"不支持的\",{\"1\":{\"264\":1}}],[\"不支持\",{\"1\":{\"106\":1,\"472\":1}}],[\"不像\",{\"1\":{\"106\":1,\"264\":1,\"586\":1,\"588\":1}}],[\"不仅依赖于生成器\",{\"1\":{\"946\":1}}],[\"不仅要包含数字的类别信息\",{\"1\":{\"944\":1}}],[\"不仅可以传入文本条件\",{\"1\":{\"895\":1}}],[\"不仅限于英语\",{\"1\":{\"824\":1}}],[\"不仅具备了现代框架应有的可视化与控制能力\",{\"1\":{\"814\":1}}],[\"不仅因为其前向计算功能\",{\"1\":{\"799\":1}}],[\"不仅在分类任务上逼近了有监督的基线模型\",{\"1\":{\"348\":1}}],[\"不仅提供联合图文表示以用于视觉\",{\"1\":{\"271\":1}}],[\"不仅能服务视觉任务\",{\"1\":{\"268\":1}}],[\"不仅考虑从当前分辨率下抽象得到的特征\",{\"1\":{\"142\":1}}],[\"不仅理解语言指令\",{\"1\":{\"106\":1}}],[\"不仅理解自己的语义\",{\"1\":{\"100\":1}}],[\"不仅看交集\",{\"1\":{\"106\":1}}],[\"不需要预测\",{\"1\":{\"893\":1}}],[\"不需要像\",{\"1\":{\"710\":1}}],[\"不需要先\",{\"1\":{\"586\":1}}],[\"不需要创建类的实例\",{\"1\":{\"424\":1}}],[\"不需要额外训练一个对比判别器或辅助网络\",{\"1\":{\"894\":1}}],[\"不需要额外增加一个视觉encoder\",{\"1\":{\"389\":1,\"392\":1}}],[\"不需要额外标注数据\",{\"1\":{\"346\":1}}],[\"不需要知道第三张图片是狗这个类别\",{\"1\":{\"349\":1}}],[\"不需要类别分类器\",{\"1\":{\"293\":1}}],[\"不需要超参数调节\",{\"1\":{\"286\":1}}],[\"不需要走后续\",{\"1\":{\"274\":1}}],[\"不需要\",{\"1\":{\"106\":1}}],[\"不依赖于\",{\"1\":{\"945\":1}}],[\"不依赖任何训练标签\",{\"1\":{\"884\":1}}],[\"不依赖绝对数量\",{\"1\":{\"587\":1}}],[\"不依赖数据增强\",{\"1\":{\"286\":1}}],[\"不依赖目标检测器\",{\"1\":{\"194\":1}}],[\"不依赖卷积\",{\"1\":{\"116\":1}}],[\"不依赖卷积操作\",{\"1\":{\"109\":1}}],[\"不依赖特定阈值\",{\"1\":{\"106\":1}}],[\"不依赖\",{\"1\":{\"106\":1}}],[\"不加\",{\"1\":{\"99\":1}}],[\"不在训练中透露\",{\"1\":{\"90\":1}}],[\"不方便进行处理\",{\"1\":{\"62\":1}}],[\"中这种依赖\",{\"1\":{\"946\":1}}],[\"中说明了这个问题的本质\",{\"1\":{\"944\":1}}],[\"中对\",{\"1\":{\"944\":1}}],[\"中对应位置的元素\",{\"1\":{\"476\":2}}],[\"中重参数化采样\",{\"1\":{\"931\":1}}],[\"中以概率方式采样一个\",{\"1\":{\"897\":1}}],[\"中以学习上下文表示\",{\"1\":{\"371\":1}}],[\"中获取\",{\"1\":{\"896\":1}}],[\"中获得广泛能力\",{\"1\":{\"650\":1}}],[\"中推断出一个三维形状\",{\"1\":{\"878\":1}}],[\"中采样可以获得多少关于\",{\"1\":{\"950\":1}}],[\"中采样可以提供更快\",{\"1\":{\"947\":1}}],[\"中采样相比\",{\"1\":{\"950\":1}}],[\"中采样得到的不同\",{\"1\":{\"946\":1}}],[\"中采样的\",{\"1\":{\"945\":1,\"946\":1}}],[\"中采样的样本尽量和真实分布相似\",{\"1\":{\"942\":1}}],[\"中采样一个\",{\"1\":{\"898\":1,\"947\":1}}],[\"中采样\",{\"1\":{\"872\":1,\"935\":1}}],[\"中采用的是每个\",{\"1\":{\"190\":1}}],[\"中有\",{\"1\":{\"859\":1}}],[\"中蒸馏出的六个\",{\"1\":{\"823\":1}}],[\"中速\",{\"1\":{\"823\":1}}],[\"中支持多输出节点\",{\"1\":{\"801\":1}}],[\"中取出对应的\",{\"1\":{\"735\":1}}],[\"中取出被掩码位置的\",{\"1\":{\"699\":1}}],[\"中取值\",{\"1\":{\"700\":2}}],[\"中非常经典\",{\"1\":{\"696\":1}}],[\"中自己的值占大头也无所谓\",{\"1\":{\"694\":1}}],[\"中实际上会更加看重\",{\"1\":{\"694\":1}}],[\"中实现深度融合\",{\"1\":{\"222\":1}}],[\"中类型\",{\"1\":{\"694\":1}}],[\"中全面超越chinchilla\",{\"1\":{\"668\":1}}],[\"中快速适应新任务\",{\"1\":{\"650\":1}}],[\"中则表现平庸甚至接近随机\",{\"1\":{\"649\":1}}],[\"中零样本设定下创下新sota\",{\"1\":{\"648\":1}}],[\"中等模型\",{\"1\":{\"640\":1}}],[\"中等点\",{\"1\":{\"121\":1}}],[\"中随机采样得到\",{\"1\":{\"633\":1}}],[\"中随机抽样字典大小就可以了\",{\"1\":{\"357\":1}}],[\"中从数据集中抽多少个样本\",{\"1\":{\"518\":1}}],[\"中选择概率最高的\",{\"1\":{\"896\":1}}],[\"中选出排名前\",{\"1\":{\"889\":1}}],[\"中选定若干个浮点坐标点\",{\"1\":{\"502\":1}}],[\"中选取了\",{\"1\":{\"87\":1}}],[\"中为了解决\",{\"1\":{\"502\":1}}],[\"中为每张图像选择一个最相似的非匹配文本作为负样本\",{\"1\":{\"201\":1}}],[\"中一个非常实用的函数\",{\"1\":{\"488\":1}}],[\"中生成等间隔数列的函数\",{\"1\":{\"440\":1}}],[\"中引入卷积操作\",{\"1\":{\"434\":1}}],[\"中进行进一步的处理\",{\"1\":{\"434\":1}}],[\"中加载数据时\",{\"1\":{\"424\":1}}],[\"中每个向量的维度\",{\"1\":{\"899\":1}}],[\"中每个整数出现的次数\",{\"1\":{\"485\":1}}],[\"中每个\",{\"1\":{\"418\":1}}],[\"中每个word映射到词空间\",{\"1\":{\"403\":1}}],[\"中常用的图像嵌入方法\",{\"1\":{\"380\":1}}],[\"中去\",{\"1\":{\"357\":1}}],[\"中来的\",{\"1\":{\"357\":1}}],[\"中文能力相对来说是非常不错的开源模型\",{\"1\":{\"823\":1}}],[\"中文能力突出\",{\"1\":{\"335\":1}}],[\"中文任务和高分辨率场景下的性能\",{\"1\":{\"332\":1}}],[\"中超越商业模型\",{\"1\":{\"323\":1}}],[\"中英文问答对\",{\"1\":{\"322\":1}}],[\"中新加入的可学习查询\",{\"1\":{\"306\":1}}],[\"中原本为\",{\"1\":{\"293\":1}}],[\"中处理\",{\"1\":{\"293\":1}}],[\"中分别进行一次前向计算\",{\"1\":{\"293\":1}}],[\"中分辨率\",{\"1\":{\"121\":1,\"122\":2}}],[\"中未被观察到\",{\"1\":{\"289\":1}}],[\"中只用\",{\"1\":{\"262\":1}}],[\"中只有一组\",{\"1\":{\"137\":1}}],[\"中最成功的预训练方法之一\",{\"1\":{\"250\":1}}],[\"中也直接采用了类似的方法\",{\"1\":{\"240\":1}}],[\"中已经成为常规做法\",{\"1\":{\"240\":1}}],[\"中查找每个\",{\"1\":{\"212\":1}}],[\"中期\",{\"1\":{\"204\":1}}],[\"中更稳定地训练\",{\"1\":{\"202\":1}}],[\"中第一个线性层把输入特征投影到一个更高维度的空间后\",{\"1\":{\"429\":1}}],[\"中第\",{\"1\":{\"179\":1}}],[\"中优先选择对比相似度高的负样本来增强训练信号\",{\"1\":{\"172\":1}}],[\"中添加一层\",{\"1\":{\"171\":1}}],[\"中心像素还是能看到自己位置的输入信息\",{\"1\":{\"923\":1}}],[\"中心像素的值就会和输入图像毫无关系\",{\"1\":{\"923\":1}}],[\"中心像素能够看到左上角大部分像素的信息\",{\"1\":{\"923\":1}}],[\"中心差分近似可以问问gpt\",{\"1\":{\"770\":1}}],[\"中心或等距分布\",{\"1\":{\"502\":1}}],[\"中心更新的\",{\"1\":{\"293\":1}}],[\"中心和\",{\"1\":{\"213\":1}}],[\"中心\",{\"1\":{\"157\":1,\"293\":2}}],[\"中心化数据\",{\"1\":{\"522\":1}}],[\"中心化\",{\"1\":{\"107\":1,\"293\":1}}],[\"中用于训练的\",{\"1\":{\"932\":1}}],[\"中用于扩展张量尺寸但不复制数据的一种高效方法\",{\"1\":{\"472\":1}}],[\"中用于沿指定维度重复张量内容的操作\",{\"1\":{\"471\":1}}],[\"中用于将多个形状相同的张量沿一个新维度拼接的函数\",{\"1\":{\"466\":1}}],[\"中用于约束变换矩阵接近正交性的正则化损失函数\",{\"1\":{\"153\":1}}],[\"中用于点云\",{\"1\":{\"145\":1}}],[\"中用于从点云中选择具有代表性的采样点的一种策略\",{\"1\":{\"137\":1}}],[\"中均能有效提取特征\",{\"1\":{\"140\":1}}],[\"中找到最大的那个距离对应的点\",{\"1\":{\"137\":2}}],[\"中层特征\",{\"1\":{\"121\":1}}],[\"中间\",{\"1\":{\"915\":1}}],[\"中间变量y和t的导数被立即释放\",{\"1\":{\"807\":1}}],[\"中间变量导数被清除\",{\"1\":{\"807\":1}}],[\"中间变量的导数往往无用\",{\"1\":{\"807\":1}}],[\"中间微调\",{\"0\":{\"240\":1},\"1\":{\"240\":1}}],[\"中间层大小为\",{\"1\":{\"224\":1}}],[\"中间特征维度\",{\"1\":{\"120\":1}}],[\"中间通道数\",{\"1\":{\"119\":1}}],[\"中提出的\",{\"1\":{\"946\":1}}],[\"中提出\",{\"1\":{\"589\":1}}],[\"中提供了\",{\"1\":{\"382\":1}}],[\"中提高了迁移性能\",{\"1\":{\"217\":1}}],[\"中提到\",{\"1\":{\"102\":1}}],[\"中提取信息\",{\"1\":{\"531\":1}}],[\"中提取与\",{\"1\":{\"417\":2}}],[\"中提取离散\",{\"1\":{\"256\":1}}],[\"中提取对应的点\",{\"1\":{\"137\":1}}],[\"中提取\",{\"1\":{\"70\":1,\"735\":1}}],[\"中提取共性的几何属性\",{\"1\":{\"53\":1}}],[\"中的离散向量\",{\"1\":{\"963\":1}}],[\"中的稀疏正则化很相似\",{\"1\":{\"951\":1}}],[\"中的证明方法可能可以推广到多维情形\",{\"1\":{\"949\":1}}],[\"中的参数\",{\"1\":{\"943\":1}}],[\"中的参数微调就可以了\",{\"1\":{\"694\":1}}],[\"中的权重\",{\"1\":{\"892\":1}}],[\"中的几个语法技巧\",{\"1\":{\"800\":1}}],[\"中的全连接层权重\",{\"1\":{\"699\":1}}],[\"中的全局特征学习\",{\"1\":{\"137\":1}}],[\"中的不一样\",{\"1\":{\"692\":1}}],[\"中的值限制在区间\",{\"1\":{\"592\":1}}],[\"中的样本取平均\",{\"1\":{\"586\":1}}],[\"中的标准组件\",{\"1\":{\"502\":1}}],[\"中的出现次数\",{\"1\":{\"485\":1}}],[\"中的多个样本打包成一个大\",{\"1\":{\"466\":1}}],[\"中的语法\",{\"1\":{\"463\":1}}],[\"中的语言引导\",{\"1\":{\"102\":1}}],[\"中的变量\",{\"1\":{\"448\":1}}],[\"中的一个嵌入向量\",{\"1\":{\"963\":1}}],[\"中的一个方法\",{\"1\":{\"734\":1}}],[\"中的一个函数\",{\"1\":{\"592\":1}}],[\"中的一个装饰器\",{\"1\":{\"424\":1}}],[\"中的一种语法结构\",{\"1\":{\"450\":1}}],[\"中的一种\",{\"1\":{\"259\":1}}],[\"中的图像一一对应\",{\"1\":{\"424\":1}}],[\"中的视觉语言专家\",{\"1\":{\"376\":1}}],[\"中的特征加入队列\",{\"1\":{\"353\":1}}],[\"中的名词短语\",{\"1\":{\"341\":1}}],[\"中的指令调优思想引入多模态领域的研究\",{\"1\":{\"339\":1}}],[\"中的定义一致\",{\"1\":{\"273\":1}}],[\"中的索引\",{\"1\":{\"265\":1}}],[\"中的每个空间位置的向量与\",{\"1\":{\"963\":1}}],[\"中的每个元素限制在\",{\"1\":{\"592\":1}}],[\"中的每个元素要加到目标张量的哪个位置\",{\"1\":{\"487\":1}}],[\"中的每个\",{\"1\":{\"235\":1}}],[\"中的目标函数模型后\",{\"1\":{\"630\":1}}],[\"中的目标\",{\"1\":{\"235\":1}}],[\"中的词嵌入\",{\"1\":{\"231\":1}}],[\"中的核心操作\",{\"1\":{\"160\":1}}],[\"中的核心模块\",{\"1\":{\"137\":1}}],[\"中的原始特征\",{\"1\":{\"143\":1}}],[\"中的\",{\"1\":{\"141\":1,\"260\":1,\"355\":1,\"385\":1,\"447\":1,\"523\":1,\"690\":1,\"692\":1,\"899\":1,\"945\":1,\"950\":1}}],[\"中的数据\",{\"1\":{\"100\":1}}],[\"中的空间卷积\",{\"1\":{\"97\":1}}],[\"中的自注意力机制\",{\"1\":{\"97\":1}}],[\"中筛选出同时满足\",{\"1\":{\"92\":1}}],[\"中椅子的\",{\"1\":{\"73\":1}}],[\"中使用的共享自注意力模块的消融实验结果\",{\"1\":{\"376\":1}}],[\"中使用的任务层设计\",{\"1\":{\"239\":1}}],[\"中使用\",{\"1\":{\"69\":1,\"160\":1}}],[\"中\",{\"1\":{\"53\":1,\"106\":1,\"119\":1,\"122\":1,\"123\":1,\"153\":1,\"190\":1,\"202\":2,\"213\":1,\"235\":1,\"255\":1,\"283\":1,\"306\":1,\"309\":1,\"355\":1,\"380\":2,\"382\":1,\"385\":1,\"408\":2,\"415\":2,\"419\":1,\"420\":1,\"425\":1,\"427\":5,\"470\":1,\"474\":2,\"500\":2,\"523\":1,\"524\":1,\"534\":1,\"541\":2,\"545\":1,\"557\":2,\"648\":1,\"681\":1,\"691\":1,\"699\":1,\"713\":1,\"733\":1,\"822\":1,\"877\":1,\"885\":1,\"932\":1,\"936\":1,\"944\":1,\"947\":1,\"951\":1,\"963\":1}}],[\"中小型组织或技术基础设施不足的地区难以承担\",{\"1\":{\"27\":1}}],[\">0\",{\"1\":{\"892\":1}}],[\">1\",{\"1\":{\"892\":1,\"893\":1}}],[\">1024\",{\"1\":{\"94\":1}}],[\">表示箭头连接\",{\"1\":{\"815\":1}}],[\">>\",{\"1\":{\"500\":1}}],[\">nk\",{\"1\":{\"362\":1,\"475\":1}}],[\">n\",{\"1\":{\"362\":1,\"475\":1}}],[\">文本\",{\"1\":{\"274\":1}}],[\">图像\",{\"1\":{\"274\":1}}],[\">bhlm\",{\"1\":{\"709\":3}}],[\">bn\",{\"1\":{\"213\":1}}],[\">bln\",{\"1\":{\"94\":1,\"100\":3}}],[\">=\",{\"1\":{\"106\":2,\"122\":1,\"380\":1,\"383\":1,\"696\":1,\"892\":4,\"895\":1,\"899\":1}}],[\">2表示从节点1到节点3的有向边\",{\"1\":{\"815\":1}}],[\">2048\",{\"1\":{\"94\":1}}],[\">256\",{\"1\":{\"94\":1}}],[\">512\",{\"1\":{\"94\":1}}],[\">object\",{\"1\":{\"59\":1}}],[\">\",{\"1\":{\"53\":1,\"58\":3,\"59\":4,\"60\":3,\"64\":2,\"69\":3,\"70\":2,\"83\":14,\"106\":1,\"107\":2,\"119\":6,\"121\":1,\"137\":1,\"145\":1,\"156\":1,\"190\":1,\"213\":8,\"256\":2,\"260\":1,\"263\":1,\"266\":2,\"274\":9,\"359\":2,\"361\":1,\"363\":1,\"364\":1,\"380\":3,\"382\":1,\"385\":1,\"402\":2,\"413\":2,\"420\":1,\"421\":1,\"427\":2,\"428\":2,\"429\":1,\"430\":8,\"431\":3,\"478\":4,\"480\":1,\"481\":1,\"490\":2,\"504\":1,\"574\":1,\"582\":1,\"592\":1,\"595\":10,\"596\":7,\"597\":23,\"663\":2,\"697\":3,\"698\":2,\"710\":2,\"713\":3,\"715\":1,\"734\":1,\"751\":1,\"800\":1,\"805\":1,\"807\":1,\"815\":5,\"847\":1,\"881\":1,\"882\":1,\"892\":2,\"893\":4,\"899\":7,\"900\":5,\"931\":2,\"963\":4}}],[\"验证迭代在大模型开发中是极其重要的一步\",{\"1\":{\"836\":1}}],[\"验证迭代\",{\"1\":{\"836\":1}}],[\"验证迭代优化\",{\"1\":{\"835\":1}}],[\"验证dezero对复杂表达式的计算图构建能力\",{\"1\":{\"815\":1}}],[\"验证安装\",{\"1\":{\"815\":1}}],[\"验证案例\",{\"1\":{\"807\":1}}],[\"验证函数的正向传播和反向传播\",{\"1\":{\"794\":1}}],[\"验证反向传播的正确性\",{\"1\":{\"772\":1,\"795\":1}}],[\"验证小模型+长训练的有效性\",{\"1\":{\"668\":1}}],[\"验证数据内容是否一致\",{\"1\":{\"491\":1}}],[\"验证数据集类\",{\"1\":{\"382\":1}}],[\"验证内存地址和数据\",{\"1\":{\"491\":1}}],[\"验证过程中的日志记录\",{\"1\":{\"385\":1}}],[\"验证阶段计算并记录\",{\"1\":{\"386\":1}}],[\"验证阶段\",{\"1\":{\"381\":1}}],[\"验证集图像\",{\"1\":{\"888\":1}}],[\"验证集上准确率达到了98\",{\"1\":{\"435\":1}}],[\"验证集上进行评估的核心代码实现如下\",{\"1\":{\"106\":1}}],[\"验证集不需要进行数据增强\",{\"1\":{\"425\":1}}],[\"验证集的预处理转换操作\",{\"1\":{\"425\":1}}],[\"验证集\",{\"1\":{\"104\":1,\"835\":1}}],[\"验证\",{\"1\":{\"90\":1,\"92\":1,\"187\":1}}],[\"验证环境\",{\"1\":{\"53\":1}}],[\"验证了更多数据能显著提升模型性能\",{\"1\":{\"680\":1}}],[\"验证了该技术在生产环境下的可行性和价值\",{\"1\":{\"658\":1}}],[\"验证了任务设定的可行性和方法的有效性\",{\"1\":{\"72\":1}}],[\"验证了\",{\"1\":{\"19\":1,\"177\":1,\"181\":1,\"376\":1}}],[\"先验\",{\"1\":{\"885\":1}}],[\"先验分布\",{\"1\":{\"260\":1,\"878\":1}}],[\"先只看图像部分\",{\"1\":{\"885\":1}}],[\"先从标准正态分布采样\",{\"1\":{\"946\":1}}],[\"先从\",{\"1\":{\"885\":1}}],[\"先有预期\",{\"1\":{\"877\":1}}],[\"先定义计算图再执行\",{\"1\":{\"811\":1}}],[\"先算\",{\"1\":{\"710\":1}}],[\"先看两个输入的形状\",{\"1\":{\"709\":1}}],[\"先进先出\",{\"1\":{\"804\":1}}],[\"先进行\",{\"1\":{\"663\":1}}],[\"先进归一化方式或对比损失\",{\"1\":{\"280\":1}}],[\"先通过\",{\"1\":{\"589\":1}}],[\"先计算每一维的均值\",{\"1\":{\"574\":1}}],[\"先激活你的conda环境\",{\"1\":{\"557\":1}}],[\"先把目标区域平均切成小格子\",{\"1\":{\"502\":1}}],[\"先调用\",{\"1\":{\"494\":1}}],[\"先连续化\",{\"1\":{\"492\":1}}],[\"先应用最内层的\",{\"1\":{\"458\":1}}],[\"先返回\",{\"1\":{\"453\":1}}],[\"先经过一层或多层\",{\"1\":{\"434\":1}}],[\"先经过自注意力层\",{\"1\":{\"274\":1}}],[\"先交给预输出层进行处理\",{\"1\":{\"431\":1}}],[\"先利用大规模\",{\"1\":{\"377\":1}}],[\"先预训练好一个骨干模型\",{\"1\":{\"352\":1}}],[\"先生成答案再推理\",{\"1\":{\"344\":1}}],[\"先生成两张全局\",{\"1\":{\"293\":1}}],[\"先在大规模噪声数据上对比学习\",{\"1\":{\"296\":1}}],[\"先在大规模图像标注数据上用交叉熵训练\",{\"1\":{\"269\":1}}],[\"先用pixelcnn采样出\",{\"1\":{\"956\":1}}],[\"先用\",{\"1\":{\"434\":1}}],[\"先用自定义\",{\"1\":{\"317\":1}}],[\"先用生成池化器\",{\"1\":{\"278\":1}}],[\"先用对比学习对齐图文表示\",{\"1\":{\"195\":1}}],[\"先构造\",{\"1\":{\"274\":1}}],[\"先以\",{\"1\":{\"204\":1,\"317\":1}}],[\"先对齐后融合\",{\"1\":{\"194\":1}}],[\"先将特征维度转到最后\",{\"1\":{\"121\":1}}],[\"先降维\",{\"1\":{\"60\":1}}],[\"先了解一下great项目对应的数据集目录结构\",{\"1\":{\"53\":1}}],[\"先训练\",{\"1\":{\"24\":1}}],[\"推近\",{\"1\":{\"947\":1}}],[\"推荐阅读\",{\"0\":{\"953\":1}}],[\"推荐资料\",{\"0\":{\"919\":1}}],[\"推荐系统等多种应用场景\",{\"1\":{\"828\":1}}],[\"推荐系统\",{\"1\":{\"506\":1}}],[\"推导见附录\",{\"1\":{\"886\":1}}],[\"推导式\",{\"1\":{\"444\":1}}],[\"推出了其推理模型\",{\"1\":{\"823\":1}}],[\"推测约\",{\"1\":{\"823\":1}}],[\"推测其他交互\",{\"1\":{\"52\":1}}],[\"推进大规模预训练在任务\",{\"1\":{\"225\":1}}],[\"推动开源大模型发展\",{\"1\":{\"671\":1}}],[\"推动双向预训练\",{\"1\":{\"671\":1}}],[\"推动更通用\",{\"1\":{\"646\":1}}],[\"推动学界重新思考语言模型在多任务学习中的角色\",{\"1\":{\"642\":1}}],[\"推动技术民主化进程\",{\"1\":{\"337\":1}}],[\"推动了多模态大模型的发展\",{\"1\":{\"313\":1}}],[\"推动多模态应用发展\",{\"1\":{\"296\":1}}],[\"推动规模化智能的发展\",{\"1\":{\"268\":1}}],[\"推动\",{\"1\":{\"93\":1}}],[\"推理过程\",{\"0\":{\"895\":1}}],[\"推理过程修改的关键\",{\"1\":{\"663\":1}}],[\"推理延迟\",{\"1\":{\"830\":1}}],[\"推理策略\",{\"1\":{\"825\":1}}],[\"推理能力限制\",{\"1\":{\"828\":1}}],[\"推理能力和代码能力提升非常显著\",{\"1\":{\"823\":1}}],[\"推理能力受限\",{\"1\":{\"48\":1}}],[\"推理深度和创意表达方面有显著提升\",{\"1\":{\"823\":1}}],[\"推理型大模型\",{\"1\":{\"823\":1}}],[\"推理型模型\",{\"1\":{\"823\":1}}],[\"推理型\",{\"1\":{\"823\":3}}],[\"推理模式下\",{\"1\":{\"900\":1}}],[\"推理模式优化\",{\"1\":{\"807\":1}}],[\"推理模式\",{\"1\":{\"807\":1,\"899\":1}}],[\"推理任务\",{\"1\":{\"694\":1}}],[\"推理任务仍存在幻觉\",{\"1\":{\"669\":1}}],[\"推理效率的核心目标\",{\"1\":{\"666\":1}}],[\"推理即编程\",{\"1\":{\"650\":1}}],[\"推理有限\",{\"1\":{\"649\":1}}],[\"推理与数学能力仍然有限\",{\"1\":{\"649\":1}}],[\"推理成本是不得不要考虑的一个因素\",{\"1\":{\"601\":1}}],[\"推理时的常规流程\",{\"1\":{\"663\":1}}],[\"推理时\",{\"1\":{\"417\":1}}],[\"推理时可以用硬采样\",{\"1\":{\"257\":1}}],[\"推理速度慢\",{\"1\":{\"368\":1,\"369\":1}}],[\"推理等\",{\"1\":{\"346\":1}}],[\"推理优先\",{\"1\":{\"344\":1}}],[\"推理阶段计算所有图像和文本表示\",{\"1\":{\"375\":1}}],[\"推理阶段\",{\"1\":{\"90\":1}}],[\"推理阶段仅使用\",{\"1\":{\"24\":1}}],[\"推理或训练分支\",{\"1\":{\"64\":1}}],[\"推理几何结构\",{\"1\":{\"52\":1}}],[\"推理几何形态支持该交互\",{\"1\":{\"52\":1}}],[\"推理除了当前交互以外\",{\"1\":{\"52\":1}}],[\"推理\",{\"0\":{\"408\":1},\"1\":{\"50\":1,\"619\":1,\"877\":1,\"895\":1}}],[\"推理的交互描述被\",{\"1\":{\"37\":1}}],[\"推理潜在意图\",{\"1\":{\"30\":1}}],[\"开区间\",{\"1\":{\"847\":1}}],[\"开发也有质的差异\",{\"1\":{\"835\":1}}],[\"开发在整体思路上有着较大的不同\",{\"1\":{\"835\":1}}],[\"开发大模型相关应用\",{\"1\":{\"835\":1}}],[\"开发者平台\",{\"1\":{\"834\":1}}],[\"开发者可以轻松地构建和定制\",{\"1\":{\"833\":1}}],[\"开发\",{\"1\":{\"823\":1,\"835\":2}}],[\"开发数据1k\",{\"1\":{\"712\":1}}],[\"开发具备\",{\"1\":{\"658\":1}}],[\"开发和维护的一个\",{\"1\":{\"510\":1}}],[\"开放接口控制\",{\"1\":{\"658\":1}}],[\"开放型和封闭型\",{\"1\":{\"656\":1}}],[\"开放任务\",{\"1\":{\"656\":1}}],[\"开箱即用\",{\"1\":{\"510\":1}}],[\"开销大\",{\"1\":{\"472\":1}}],[\"开启\",{\"1\":{\"384\":1}}],[\"开头提到过\",{\"1\":{\"354\":1}}],[\"开始从头训练\",{\"1\":{\"963\":1,\"964\":1}}],[\"开始一步一步谈起\",{\"1\":{\"956\":1}}],[\"开始的预训练\",{\"0\":{\"695\":1}}],[\"开始之前\",{\"1\":{\"600\":1}}],[\"开始数\",{\"1\":{\"544\":1}}],[\"开始顺序排列\",{\"1\":{\"540\":1}}],[\"开始调用\",{\"1\":{\"453\":1}}],[\"开始\",{\"1\":{\"452\":1,\"633\":1,\"695\":1,\"836\":1,\"898\":1}}],[\"开始生成句子\",{\"1\":{\"420\":1}}],[\"开始输出回答\",{\"1\":{\"342\":1}}],[\"开始充分利用动量编码器提供的软标签来训练\",{\"1\":{\"204\":1}}],[\"开关\",{\"1\":{\"213\":1}}],[\"开壶盖\",{\"1\":{\"52\":1}}],[\"开源了\",{\"1\":{\"823\":1}}],[\"开源的一组参数规模从\",{\"1\":{\"823\":1}}],[\"开源贡献\",{\"1\":{\"686\":1}}],[\"开源项目\",{\"0\":{\"437\":1}}],[\"开源模型进展\",{\"1\":{\"671\":1}}],[\"开源模型在视觉\",{\"1\":{\"326\":1}}],[\"开源模型与商业专有模型\",{\"1\":{\"323\":1}}],[\"开源模型\",{\"1\":{\"299\":1}}],[\"开源网络库\",{\"1\":{\"2\":1}}],[\"开源框架\",{\"1\":{\"2\":1}}],[\"用训练好的\",{\"1\":{\"964\":1}}],[\"用训练好的模型权重\",{\"1\":{\"107\":1}}],[\"用概率分布形式表达即为最大化\",{\"1\":{\"943\":1}}],[\"用梯度下降更新生成器参数\",{\"1\":{\"918\":1}}],[\"用梯度下降更新判别器参数\",{\"1\":{\"918\":1}}],[\"用变分后验\",{\"1\":{\"885\":1}}],[\"用到多少\",{\"1\":{\"844\":1,\"876\":1}}],[\"用其来表示函数\",{\"1\":{\"805\":1}}],[\"用方框表示函数\",{\"1\":{\"760\":1}}],[\"用每个\",{\"1\":{\"710\":1}}],[\"用爱因斯坦求和公式计算位置分数\",{\"1\":{\"709\":1}}],[\"用随机词替换\",{\"1\":{\"698\":1}}],[\"用随机高斯分布初始化\",{\"1\":{\"611\":1,\"612\":1}}],[\"用mask掩码替换\",{\"1\":{\"698\":1}}],[\"用两句话总结\",{\"1\":{\"658\":1}}],[\"用两个可能的替换说法来代替定义的代词\",{\"1\":{\"635\":1}}],[\"用奖励模型得分选择最佳模型\",{\"1\":{\"656\":1}}],[\"用正则化或投影技术缓解嵌入空间中的偏见\",{\"1\":{\"655\":1}}],[\"用正则匹配并替换匹配上的\",{\"1\":{\"595\":1}}],[\"用人类偏好训练强化学习代理\",{\"1\":{\"655\":1}}],[\"用人工标注的数据\",{\"1\":{\"602\":1}}],[\"用任务未知的生成式预训练模型和判别式微调在自然语言理解上取得了很强的效果\",{\"1\":{\"636\":1}}],[\"用以区分不同的句子\",{\"1\":{\"713\":1}}],[\"用以实现语言模型对人类意图的对齐\",{\"1\":{\"656\":1}}],[\"用以更好地对齐模型行为与用户意图\",{\"1\":{\"653\":1}}],[\"用以训练步数的\",{\"1\":{\"633\":1}}],[\"用以完成特定任务的指令\",{\"1\":{\"616\":1}}],[\"用分隔符分隔\",{\"1\":{\"631\":1}}],[\"用标记的数据对特定任务微调模型\",{\"1\":{\"628\":1}}],[\"用特定的训练数据\",{\"1\":{\"606\":1}}],[\"用特定训练数据去微调可能会把这个领域的表现变好\",{\"1\":{\"602\":1}}],[\"用传统机器学习中监督学习的方法\",{\"1\":{\"602\":1}}],[\"用法\",{\"1\":{\"574\":1}}],[\"用bn的beta作为偏置\",{\"1\":{\"522\":1}}],[\"用浅层网络逼近需要指数级神经元\",{\"1\":{\"500\":1}}],[\"用深度\",{\"1\":{\"500\":1}}],[\"用微小的开销换取代码的健壮性和可维护性\",{\"1\":{\"493\":1}}],[\"用一个通用大模型\",{\"1\":{\"835\":1}}],[\"用一个简化版的例子说明上述过程\",{\"1\":{\"426\":1}}],[\"用一组由语言引导的动态卷积核\",{\"1\":{\"100\":1}}],[\"用文本描述去匹配最合适的图片内容\",{\"1\":{\"411\":1}}],[\"用当前图片外层目录的名字作为其分类名词\",{\"1\":{\"410\":1}}],[\"用卷积神经网络\",{\"1\":{\"388\":1}}],[\"用的图像\",{\"1\":{\"385\":1}}],[\"用户愈发期待像钢铁侠中\",{\"1\":{\"827\":1}}],[\"用户可与\",{\"1\":{\"823\":1}}],[\"用户可根据任务需求选择分辨率\",{\"1\":{\"323\":1}}],[\"用户本地运行时\",{\"1\":{\"739\":1}}],[\"用户为申请加入测试队列的群体\",{\"1\":{\"658\":1}}],[\"用户\",{\"1\":{\"658\":2}}],[\"用户行为\",{\"1\":{\"658\":1}}],[\"用户的\",{\"1\":{\"656\":1}}],[\"用户代码\",{\"1\":{\"381\":1}}],[\"用户创建\",{\"1\":{\"381\":1}}],[\"用户提交\",{\"1\":{\"657\":1,\"658\":1}}],[\"用户提交的指令\",{\"1\":{\"655\":1}}],[\"用户提交给\",{\"1\":{\"339\":1}}],[\"用户提问或指令\",{\"1\":{\"342\":1}}],[\"用学生初始化教师网络\",{\"1\":{\"293\":1}}],[\"用它提取并存储下游任务训练集的特征\",{\"1\":{\"286\":1}}],[\"用作\",{\"1\":{\"231\":1}}],[\"用作纯视觉编码器时\",{\"1\":{\"224\":1}}],[\"用来自标准正态分布的随机向量和解码器来实现随机图像生成了\",{\"1\":{\"956\":1}}],[\"用来从模型中采样\",{\"1\":{\"951\":1}}],[\"用来计算一个数组或元组\",{\"1\":{\"918\":2}}],[\"用来衡量一个概率分布的不确定性\",{\"1\":{\"907\":1}}],[\"用来处理实数范围内的\",{\"1\":{\"847\":1}}],[\"用来指定程序和其子进程能看到哪些\",{\"1\":{\"520\":1}}],[\"用来填充的数值\",{\"1\":{\"477\":1}}],[\"用来判断条件是否成立\",{\"1\":{\"476\":1}}],[\"用来判断图像文本是否匹配\",{\"1\":{\"393\":1}}],[\"用来模拟采样的随机性\",{\"1\":{\"257\":1}}],[\"用来学习模态间的对齐关系\",{\"1\":{\"220\":1}}],[\"用来存储每个\",{\"1\":{\"213\":1}}],[\"用来做\",{\"1\":{\"213\":1}}],[\"用来对注意力权重归一化\",{\"1\":{\"119\":1}}],[\"用动量模型生成伪标签\",{\"1\":{\"194\":1}}],[\"用全连接层逐步压缩到\",{\"1\":{\"152\":1}}],[\"用邻居特征加权求和\",{\"1\":{\"122\":1}}],[\"用二维卷积处理\",{\"1\":{\"110\":1}}],[\"用不同\",{\"1\":{\"106\":1}}],[\"用增强后的语言查询去\",{\"1\":{\"100\":1}}],[\"用语言引导点特征分组\",{\"1\":{\"99\":1}}],[\"用\",{\"1\":{\"97\":1,\"182\":1,\"192\":1,\"213\":2,\"265\":1,\"266\":1,\"274\":1,\"319\":1,\"385\":2,\"445\":1,\"463\":1,\"479\":1,\"586\":1,\"611\":2,\"612\":1,\"631\":1,\"709\":1,\"710\":1,\"713\":1,\"835\":1,\"881\":1,\"893\":1,\"899\":1,\"921\":1,\"945\":1,\"947\":2,\"963\":1}}],[\"用几何结构\",{\"1\":{\"56\":1}}],[\"用意图文本\",{\"1\":{\"56\":1}}],[\"用resnet18对图像进行编码\",{\"1\":{\"54\":1}}],[\"用手握住壶把倒水\",{\"1\":{\"52\":1}}],[\"用于控制不同误差之间的比例\",{\"1\":{\"960\":1}}],[\"用于鼓励编码结果稀疏\",{\"1\":{\"951\":1}}],[\"用于模型的第一层\",{\"1\":{\"924\":1}}],[\"用于模块共享的\",{\"1\":{\"892\":2}}],[\"用于缩放余弦相似度\",{\"1\":{\"900\":1}}],[\"用于输入预处理\",{\"1\":{\"899\":1}}],[\"用于输出分割结果\",{\"1\":{\"239\":1}}],[\"用于下一个\",{\"1\":{\"898\":1}}],[\"用于下游任务\",{\"1\":{\"271\":1}}],[\"用于屏蔽非\",{\"1\":{\"896\":1}}],[\"用于屏蔽无效位置\",{\"1\":{\"380\":1}}],[\"用于增量推理\",{\"1\":{\"893\":1}}],[\"用于排除或降低某些不太可能的世界状态\",{\"1\":{\"878\":1}}],[\"用于确保整个密度函数的积分为\",{\"1\":{\"865\":1}}],[\"用于构建\",{\"1\":{\"834\":1}}],[\"用于构造\",{\"1\":{\"192\":1}}],[\"用于复杂的应用的调用序列\",{\"1\":{\"832\":2}}],[\"用于链的多次运行之间持久化应用程序状态\",{\"1\":{\"832\":1}}],[\"用于与人类对话式应用的大胆尝试\",{\"1\":{\"822\":1}}],[\"用于验证基础微分逻辑\",{\"1\":{\"811\":1}}],[\"用于封装功能\",{\"1\":{\"810\":1}}],[\"用于函数的输入接口\",{\"1\":{\"800\":1}}],[\"用于指定在计算损失时忽略的标签索引\",{\"1\":{\"734\":1}}],[\"用于判断给出的两个句子是否连续\",{\"1\":{\"713\":1}}],[\"用于判断图文对是否匹配\",{\"1\":{\"201\":1}}],[\"用于精确表示短距离\",{\"1\":{\"710\":1}}],[\"用于提升表达能力\",{\"1\":{\"899\":1}}],[\"用于提升图文语料的质量\",{\"1\":{\"173\":1}}],[\"用于提取\",{\"1\":{\"699\":1}}],[\"用于我们的预训练任务\",{\"1\":{\"698\":1}}],[\"用于监督微调\",{\"1\":{\"656\":1}}],[\"用于交叉熵部分\",{\"1\":{\"592\":1}}],[\"用于抑制易分类样本\",{\"1\":{\"589\":1}}],[\"用于平衡正负样本数量差异\",{\"1\":{\"589\":1}}],[\"用于平衡训练效率与显存使用\",{\"1\":{\"187\":1}}],[\"用于度量两个集合之间的重叠程度\",{\"1\":{\"588\":1}}],[\"用于语义分割任务中评估模型的分割结果与真实分割标签之间的相似性\",{\"1\":{\"588\":1}}],[\"用于语言引导下的功能区域分割\",{\"1\":{\"102\":1}}],[\"用于类别加权\",{\"1\":{\"587\":1,\"588\":1,\"589\":1}}],[\"用于类别任务\",{\"1\":{\"513\":1}}],[\"用于选择模型和阈值的\",{\"0\":{\"572\":1}}],[\"用于选择难负样本\",{\"1\":{\"207\":1}}],[\"用于匹配查询\",{\"1\":{\"529\":1}}],[\"用于固定伪随机数生成器的初始状态\",{\"1\":{\"521\":1}}],[\"用于在构建\",{\"1\":{\"518\":1}}],[\"用于根据样本分布计算每个类别的权重\",{\"1\":{\"514\":1}}],[\"用于可重复的随机化\",{\"1\":{\"513\":1}}],[\"用于把一个或多个并行数组按比例切分成训练集和测试集\",{\"1\":{\"513\":1}}],[\"用于按名字实例化模型\",{\"1\":{\"511\":1}}],[\"用于等比例放大或缩小\",{\"1\":{\"503\":1}}],[\"用于找最小距离\",{\"1\":{\"488\":1}}],[\"用于获取张量中最大或最小的\",{\"1\":{\"488\":1}}],[\"用于获取视觉令牌\",{\"1\":{\"235\":1}}],[\"用于加权计数\",{\"1\":{\"485\":1}}],[\"用于统计可迭代对象中各元素出现的次数\",{\"1\":{\"516\":1}}],[\"用于统计\",{\"1\":{\"485\":1}}],[\"用于写入结果\",{\"1\":{\"466\":1}}],[\"用于沿指定轴将多个数组拼接在一起\",{\"1\":{\"441\":1}}],[\"用于同时生成查询\",{\"1\":{\"430\":1}}],[\"用于防止过拟合\",{\"1\":{\"429\":1}}],[\"用于随机深度\",{\"1\":{\"429\":1}}],[\"用于位置嵌入后的随机丢弃\",{\"1\":{\"428\":1,\"431\":1}}],[\"用于位置编码或其他处理\",{\"1\":{\"380\":1}}],[\"用于存储导数\",{\"1\":{\"778\":1}}],[\"用于存储训练集和验证集的图像预处理转换操作\",{\"1\":{\"425\":1}}],[\"用于存储反向传播需要的信息\",{\"1\":{\"121\":1}}],[\"用于收集所有\",{\"1\":{\"386\":1}}],[\"用于表示\",{\"1\":{\"382\":1,\"463\":1}}],[\"用于表示整句话的全局语义\",{\"1\":{\"274\":1}}],[\"用于最终评估模型效果\",{\"1\":{\"382\":1}}],[\"用于评估\",{\"1\":{\"382\":2}}],[\"用于评测模型在真实场景中的鲁棒性\",{\"1\":{\"25\":1}}],[\"用于定义重复性的模版流程\",{\"1\":{\"382\":1}}],[\"用于全局聚合\",{\"1\":{\"380\":1}}],[\"用于更复杂的跨模态分类任务\",{\"1\":{\"377\":1}}],[\"用于更新中心和处理空簇\",{\"1\":{\"213\":1}}],[\"用于处理\",{\"1\":{\"900\":1}}],[\"用于处理类别不平衡\",{\"1\":{\"590\":1,\"592\":1}}],[\"用于处理模态特定的信息\",{\"1\":{\"377\":1}}],[\"用于处理多视角输入\",{\"1\":{\"293\":1}}],[\"用于分层采样的标签数组\",{\"1\":{\"513\":1}}],[\"用于分类任务\",{\"1\":{\"370\":1,\"397\":1,\"427\":1}}],[\"用于分布式训练的采样器\",{\"1\":{\"265\":1}}],[\"用于检索任务\",{\"1\":{\"370\":1}}],[\"用于调整注意力分数\",{\"1\":{\"430\":1}}],[\"用于调整特征维度\",{\"1\":{\"213\":1}}],[\"用于调节\",{\"1\":{\"355\":1}}],[\"用于从模型输出的\",{\"1\":{\"896\":1}}],[\"用于从视觉特征中提取信息\",{\"1\":{\"306\":1}}],[\"用于从点云中选择分布最均匀的点的子集\",{\"1\":{\"121\":1}}],[\"用于零样本分类\",{\"1\":{\"303\":1}}],[\"用于教师温度调度\",{\"1\":{\"293\":1}}],[\"用于正则化\",{\"1\":{\"293\":1}}],[\"用于推理模式\",{\"1\":{\"895\":1}}],[\"用于推理阶段\",{\"1\":{\"893\":1}}],[\"用于推理\",{\"1\":{\"274\":1}}],[\"用于替换被遮挡的位置\",{\"1\":{\"266\":1}}],[\"用于混合精度训练的loss\",{\"1\":{\"265\":1}}],[\"用于送入离散\",{\"1\":{\"264\":1}}],[\"用于离散\",{\"1\":{\"264\":1}}],[\"用于反向传播离散\",{\"1\":{\"899\":1}}],[\"用于反向传播\",{\"1\":{\"258\":2}}],[\"用于解码器重建图像\",{\"1\":{\"257\":1}}],[\"用于解决一个新颖的任务\",{\"1\":{\"94\":1}}],[\"用于解决类别不平衡并提升分割精度\",{\"1\":{\"24\":1}}],[\"用于保存所有特征\",{\"1\":{\"293\":1}}],[\"用于保存卷积层和批归一化层\",{\"1\":{\"145\":1}}],[\"用于保持\",{\"1\":{\"255\":1}}],[\"用于计算相似度\",{\"1\":{\"900\":1}}],[\"用于计算交叉熵损失\",{\"1\":{\"737\":1}}],[\"用于计算平均损失的项数\",{\"1\":{\"293\":1}}],[\"用于计算\",{\"1\":{\"235\":1,\"420\":1}}],[\"用于高效的图文检索\",{\"1\":{\"222\":1}}],[\"用于高效检索\",{\"1\":{\"220\":1}}],[\"用于融合编码\",{\"1\":{\"222\":1}}],[\"用于融合来自语言模型的不同语义信息\",{\"1\":{\"69\":1}}],[\"用于图像编码器输出与文本单模态表示之间的对齐\",{\"1\":{\"268\":1}}],[\"用于图像分类\",{\"1\":{\"222\":1}}],[\"用于图像字幕生成\",{\"1\":{\"187\":1}}],[\"用于图文联合表示学习\",{\"1\":{\"220\":1}}],[\"用于预测图像的类别\",{\"1\":{\"427\":1}}],[\"用于预测被遮挡位置的视觉\",{\"1\":{\"214\":1}}],[\"用于预训练新模型\",{\"1\":{\"185\":1}}],[\"用于学习图像的语义表示\",{\"1\":{\"213\":1}}],[\"用于学习图文之间的细粒度对齐关系\",{\"1\":{\"172\":1}}],[\"用于训练时的条件\",{\"1\":{\"893\":1}}],[\"用于训练奖励模型\",{\"1\":{\"656\":1}}],[\"用于训练或评估模型\",{\"1\":{\"339\":1}}],[\"用于训练视觉\",{\"1\":{\"212\":1,\"217\":1}}],[\"用于训练下一个更强的模型\",{\"1\":{\"173\":1}}],[\"用于文本编码\",{\"1\":{\"205\":1}}],[\"用于将输入文本编码为\",{\"1\":{\"898\":1}}],[\"用于将输入图像分割成多个图像块并进行嵌入\",{\"1\":{\"426\":1}}],[\"用于将张量中的值限制在指定的范围内\",{\"1\":{\"734\":1}}],[\"用于将张量\",{\"1\":{\"592\":1}}],[\"用于将张量沿指定维度\",{\"1\":{\"482\":1}}],[\"用于将自定义模型注册到\",{\"1\":{\"511\":1}}],[\"用于将任意大小的候选框\",{\"1\":{\"502\":1}}],[\"用于将一个方法定义为静态方法\",{\"1\":{\"424\":1}}],[\"用于将一个批次的数据组合成一个张量\",{\"1\":{\"424\":1}}],[\"用于将图像转换为离散的视觉\",{\"1\":{\"265\":1}}],[\"用于将量化后的特征解码为原始图像\",{\"1\":{\"213\":1}}],[\"用于将\",{\"1\":{\"199\":1,\"834\":1,\"899\":1}}],[\"用于二分类匹配\",{\"1\":{\"192\":1}}],[\"用于多模态编码\",{\"1\":{\"372\":1}}],[\"用于多模态\",{\"1\":{\"192\":1}}],[\"用于生成训练标签\",{\"1\":{\"265\":1}}],[\"用于生成任务\",{\"1\":{\"220\":1}}],[\"用于生成\",{\"1\":{\"192\":1}}],[\"用于拷贝和更新\",{\"1\":{\"192\":1}}],[\"用于对比损失\",{\"1\":{\"900\":1}}],[\"用于对比学习中的\",{\"1\":{\"205\":1}}],[\"用于对比学习\",{\"1\":{\"190\":1}}],[\"用于对生成图像进行\",{\"1\":{\"895\":1}}],[\"用于对张量\",{\"1\":{\"478\":1}}],[\"用于对齐语言模型在广泛任务分布下的行为\",{\"1\":{\"655\":1}}],[\"用于对齐视觉与语言特征\",{\"1\":{\"304\":1}}],[\"用于对齐图像和文本的表示空间\",{\"1\":{\"172\":1}}],[\"用于论文3\",{\"1\":{\"187\":1}}],[\"用于其他任务\",{\"1\":{\"146\":1}}],[\"用于后续所有卷积层\",{\"1\":{\"924\":1}}],[\"用于后续解码时跳过填充\",{\"1\":{\"898\":1}}],[\"用于后续解码时区分提示与生成文本\",{\"1\":{\"187\":1}}],[\"用于后续对比学习\",{\"1\":{\"293\":1}}],[\"用于后续跨模态交互\",{\"1\":{\"274\":1}}],[\"用于后续重建损失\",{\"1\":{\"213\":2}}],[\"用于后续编码\",{\"1\":{\"212\":1}}],[\"用于后续比较\",{\"1\":{\"153\":1}}],[\"用于后续计算其他点到该点的距离\",{\"1\":{\"137\":1}}],[\"用于后续分割掩码预测\",{\"1\":{\"98\":1}}],[\"用于快速访问每个\",{\"1\":{\"137\":1}}],[\"用于点云语义分割的网络\",{\"1\":{\"123\":1}}],[\"用于局部特征聚合\",{\"1\":{\"121\":1}}],[\"用于knn搜索\",{\"1\":{\"120\":1}}],[\"用于减少计算量\",{\"1\":{\"120\":1}}],[\"用于衡量被错误分类为垃圾邮件的合法电子邮件的比例\",{\"1\":{\"564\":1}}],[\"用于衡量两个概率分布之间的匹配程度\",{\"1\":{\"106\":1}}],[\"用于衡量模型输出的\",{\"1\":{\"106\":1}}],[\"用于衡量预测掩码与真实标签之间的空间重合度\",{\"1\":{\"102\":1}}],[\"用于缓解类别不平衡问题\",{\"1\":{\"102\":1}}],[\"用于linear\",{\"1\":{\"83\":1}}],[\"用于支持该任务\",{\"1\":{\"72\":1}}],[\"用于稳定训练过程\",{\"1\":{\"69\":1}}],[\"用于\",{\"1\":{\"69\":1,\"83\":1,\"137\":1,\"192\":1,\"205\":1,\"206\":1,\"207\":1,\"208\":3,\"213\":2,\"216\":2,\"266\":1,\"293\":1,\"382\":2,\"384\":1,\"385\":1,\"463\":1,\"656\":2,\"699\":1,\"710\":1,\"892\":1,\"895\":2,\"899\":2}}],[\"用于降低计算复杂度\",{\"1\":{\"69\":1}}],[\"框架进行开发\",{\"1\":{\"836\":1}}],[\"框架交互\",{\"1\":{\"834\":1}}],[\"框架的核心库\",{\"1\":{\"834\":1}}],[\"框架可以实现数据感知和环境互动\",{\"1\":{\"831\":1}}],[\"框架是一个开源工具\",{\"1\":{\"831\":1}}],[\"框架核心功能的日益完善\",{\"1\":{\"814\":1}}],[\"框架按照\",{\"1\":{\"416\":1}}],[\"框架图如下\",{\"1\":{\"353\":1}}],[\"框架\",{\"0\":{\"628\":1},\"1\":{\"165\":1,\"212\":1,\"650\":1,\"831\":2,\"948\":1}}],[\"框架在多项评估指标上具有显著优势\",{\"1\":{\"50\":1}}],[\"框架旨在从\",{\"1\":{\"21\":1}}],[\"万对文本\",{\"1\":{\"888\":1}}],[\"万亿个\",{\"1\":{\"823\":1}}],[\"万亿参数\",{\"1\":{\"823\":1}}],[\"万亿\",{\"1\":{\"600\":1,\"823\":1}}],[\"万个特征\",{\"1\":{\"357\":1}}],[\"万个标注完整的\",{\"1\":{\"50\":1}}],[\"万增长到\",{\"1\":{\"354\":1}}],[\"万条图文对\",{\"1\":{\"341\":1}}],[\"万高质量指令数据\",{\"1\":{\"305\":1}}],[\"万象多模态大模型\",{\"0\":{\"294\":1,\"321\":1}}],[\"万步训练过程在\",{\"1\":{\"236\":1}}],[\"万步\",{\"1\":{\"224\":1,\"236\":1}}],[\"万图像\",{\"1\":{\"224\":1}}],[\"万图像和\",{\"1\":{\"224\":1}}],[\"万图文对\",{\"1\":{\"224\":1}}],[\"万\",{\"1\":{\"224\":1,\"823\":1}}],[\"万张图像作为验证集\",{\"1\":{\"887\":1}}],[\"万张图像\",{\"1\":{\"176\":1,\"236\":1}}],[\"万张交互图像与超过\",{\"1\":{\"50\":1}}],[\"涵盖广泛的主题和写作风格\",{\"1\":{\"640\":1}}],[\"涵盖多种任务类型\",{\"1\":{\"342\":1}}],[\"涵盖常见场景和文档图像\",{\"1\":{\"323\":1}}],[\"涵盖图像描述\",{\"1\":{\"305\":1}}],[\"涵盖\",{\"1\":{\"50\":1,\"332\":1,\"680\":1}}],[\"同\",{\"1\":{\"513\":1}}],[\"同形状的整数张量\",{\"1\":{\"487\":1}}],[\"同形状的矩阵\",{\"1\":{\"208\":1}}],[\"同长度的浮点张量\",{\"1\":{\"485\":1}}],[\"同理\",{\"1\":{\"353\":1,\"849\":1}}],[\"同一卷积核在不同位置重复使用\",{\"1\":{\"500\":1}}],[\"同一个观测结果\",{\"1\":{\"878\":1}}],[\"同一个句子在训练的不同批次\",{\"1\":{\"681\":1}}],[\"同一个视频里的任意两帧是正样本\",{\"1\":{\"350\":1}}],[\"同一个类别的物体处于相邻的区域\",{\"1\":{\"349\":1}}],[\"同一码本向量对应的图像\",{\"1\":{\"215\":1}}],[\"同一物体区域可能支持多功能\",{\"1\":{\"73\":1}}],[\"同一物体在不同交互图像中被推理出不同的\",{\"1\":{\"49\":1}}],[\"同输入\",{\"1\":{\"213\":1}}],[\"同样的种子保证每次生成的随机数序列完全相同\",{\"1\":{\"521\":1}}],[\"同样可以比较好地记录二维信息\",{\"1\":{\"428\":1}}],[\"同样需要位置编码来记录各图像块之间的位置信息\",{\"1\":{\"428\":1}}],[\"同样\",{\"1\":{\"426\":1,\"612\":1,\"809\":1,\"823\":1,\"836\":1}}],[\"同样会将像素值从\",{\"1\":{\"425\":1}}],[\"同样会为coco数据集中每个样本的caption前添加固定长度的prompt\",{\"1\":{\"190\":1}}],[\"同样是数据增强的手段\",{\"1\":{\"425\":1}}],[\"同样给计算机视觉领域带来了巨大影响\",{\"1\":{\"405\":1}}],[\"同样采用余弦调度\",{\"1\":{\"286\":1}}],[\"同样有优异表现\",{\"1\":{\"268\":1}}],[\"同样地\",{\"1\":{\"201\":1,\"694\":1,\"878\":1,\"903\":1}}],[\"同时优化两个目标\",{\"1\":{\"947\":1}}],[\"同时优化三个预训练目标\",{\"1\":{\"172\":1}}],[\"同时应用\",{\"1\":{\"924\":2}}],[\"同时输出图像每一处的概率分布\",{\"1\":{\"921\":1}}],[\"同时降低了调用成本\",{\"1\":{\"828\":1}}],[\"同时降维\",{\"1\":{\"58\":1,\"59\":1}}],[\"同时还开源了代码模型和数学模型\",{\"1\":{\"823\":1}}],[\"同时还能适应更多的参数\",{\"1\":{\"614\":1}}],[\"同时还能直接实现零样本学习\",{\"1\":{\"269\":1}}],[\"同时开源了用\",{\"1\":{\"823\":1}}],[\"同时发布了\",{\"1\":{\"823\":2}}],[\"同时减少参数量和计算量\",{\"1\":{\"823\":1}}],[\"同时又优先处理列表末尾的函数\",{\"1\":{\"804\":1}}],[\"同时对长距离不必过于精细\",{\"1\":{\"710\":1}}],[\"同时对输入的文本数据回调\",{\"1\":{\"382\":1}}],[\"同时进行\",{\"1\":{\"693\":1}}],[\"同时表明bert的掩码语言模型目标在优化后仍具有竞争力\",{\"1\":{\"677\":1}}],[\"同时分析模型偏见\",{\"1\":{\"666\":1}}],[\"同时返回对应的key和val\",{\"1\":{\"663\":1}}],[\"同时构建适用于未来更强\",{\"1\":{\"658\":1}}],[\"同时允许空格合并以提高压缩效率\",{\"1\":{\"640\":1}}],[\"同时揭示了模型容量与任务性能之间的紧密关联\",{\"1\":{\"639\":1}}],[\"同时列表末尾追加<\",{\"1\":{\"596\":1}}],[\"同时末尾加上\",{\"1\":{\"595\":1}}],[\"同时完成断句分词任务\",{\"1\":{\"595\":1}}],[\"同时避免\",{\"1\":{\"594\":1}}],[\"同时也促进了社区的合作和共享\",{\"1\":{\"834\":1}}],[\"同时也引发了对未来人工智能的无限探索\",{\"1\":{\"827\":1}}],[\"同时也引发了对其伦理和风险问题的关注\",{\"1\":{\"824\":1}}],[\"同时也是方差\",{\"1\":{\"858\":1}}],[\"同时也是\",{\"1\":{\"590\":1}}],[\"同时也指运用该方法构建的模型\",{\"1\":{\"406\":1}}],[\"同时作为特征数量\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"同时会将像素值从\",{\"1\":{\"425\":1}}],[\"同时删除不相关的视觉信息\",{\"1\":{\"421\":1}}],[\"同时因llm而具有了视觉推理能力\",{\"1\":{\"415\":1}}],[\"同时具有很好的性能\",{\"1\":{\"415\":1}}],[\"同时最小化个负样本的相似度\",{\"1\":{\"407\":1}}],[\"同时最小化非对角线的非匹配样本相似度得分\",{\"1\":{\"385\":1}}],[\"同时文本\",{\"1\":{\"374\":1}}],[\"同时提供了高质量的实现和训练权重\",{\"1\":{\"510\":1}}],[\"同时提出\",{\"1\":{\"368\":1}}],[\"同时提取对比和生成损失\",{\"1\":{\"278\":1}}],[\"同时共享自注意力层进行跨模态对齐\",{\"1\":{\"368\":1}}],[\"同时key\",{\"1\":{\"361\":1}}],[\"同时认为\",{\"1\":{\"350\":1}}],[\"同时支持对比学习\",{\"1\":{\"303\":1}}],[\"同时支持编码器\",{\"1\":{\"165\":1}}],[\"同时计算self\",{\"1\":{\"740\":1}}],[\"同时计算需求更低\",{\"1\":{\"291\":1}}],[\"同时计算开销更低\",{\"1\":{\"280\":1}}],[\"同时适用于\",{\"1\":{\"280\":1}}],[\"同时保持总损失数值稳定\",{\"1\":{\"893\":1}}],[\"同时保持了向后兼容性\",{\"1\":{\"833\":1}}],[\"同时保持了低资源部署的高效性\",{\"1\":{\"823\":1}}],[\"同时保持了模型性能\",{\"1\":{\"823\":2}}],[\"同时保持开源可复现性\",{\"1\":{\"672\":1}}],[\"同时保持预先训练的权重不变\",{\"1\":{\"610\":1}}],[\"同时保持空间连续性\",{\"1\":{\"584\":1}}],[\"同时保持丰富的特征表示\",{\"1\":{\"122\":1}}],[\"同时保留一定的随机性和自然度\",{\"1\":{\"894\":1}}],[\"同时保留最有代表性的空间信息\",{\"1\":{\"501\":1}}],[\"同时保留其捕捉长距离依赖的优势\",{\"1\":{\"434\":1}}],[\"同时保留全局缩略图以捕捉上下文信息\",{\"1\":{\"322\":1}}],[\"同时保证视觉和跨模态检索能力\",{\"1\":{\"277\":1}}],[\"同时使用这个旁路的更新来模拟\",{\"1\":{\"611\":1}}],[\"同时使用居中和锐化\",{\"1\":{\"290\":1}}],[\"同时使用双三次插值\",{\"1\":{\"286\":1}}],[\"同时使用对比损失和生成损失的\",{\"1\":{\"276\":1}}],[\"同时使用共享的\",{\"1\":{\"215\":1}}],[\"同时视觉编码器在图像分类上依然具备竞争力\",{\"1\":{\"269\":1}}],[\"同时在多模态解码器输出上施加生成目标\",{\"1\":{\"268\":1}}],[\"同时引入\",{\"1\":{\"268\":1}}],[\"同时实现简单且与基于patch的模型结构高度契合\",{\"1\":{\"263\":1}}],[\"同时仍保证梯度流通\",{\"1\":{\"257\":1}}],[\"同时去除视觉\",{\"1\":{\"242\":1}}],[\"同时捕获高层语义\",{\"1\":{\"215\":1}}],[\"同时增强的表征有助于下游任务\",{\"1\":{\"214\":1}}],[\"同时通过masked\",{\"1\":{\"700\":1}}],[\"同时通过\",{\"1\":{\"212\":1}}],[\"同时通过引入动量蒸馏\",{\"1\":{\"194\":1}}],[\"同时将flower\",{\"1\":{\"410\":1}}],[\"同时将损失记录在字典中\",{\"1\":{\"383\":1}}],[\"同时将对应的\",{\"1\":{\"208\":1}}],[\"同时将运行时输出写入日志\",{\"1\":{\"107\":1}}],[\"同时获取对应的\",{\"1\":{\"207\":1}}],[\"同时获取裁剪后的物体框\",{\"1\":{\"82\":1}}],[\"同时摒弃目标检测器\",{\"1\":{\"195\":1}}],[\"同时结合了一些前人工作的改进\",{\"1\":{\"823\":1}}],[\"同时结合图文对比\",{\"1\":{\"174\":1}}],[\"同时结合跨模态特征对齐\",{\"1\":{\"50\":1}}],[\"同时区分负样本\",{\"1\":{\"172\":1}}],[\"同时兼顾理解与生成任务\",{\"1\":{\"167\":1}}],[\"同时确保这些分区的处理方式允许在它们之间共享模型权重\",{\"1\":{\"131\":1}}],[\"同时缺少\",{\"1\":{\"131\":1}}],[\"同时缺乏高质量内部可供性数据进一步限制了表现\",{\"1\":{\"27\":1}}],[\"同时包含了\",{\"1\":{\"125\":1}}],[\"同时包含部分新物体类别\",{\"1\":{\"44\":1}}],[\"同时处理点云数量不一致的情况\",{\"1\":{\"119\":1}}],[\"同时等比例对物体框做同样的缩放\",{\"1\":{\"82\":1}}],[\"同时记录每类物体共对应多少不同的点云\",{\"1\":{\"82\":1}}],[\"同时记录每类物体对应的样本总量\",{\"1\":{\"53\":1}}],[\"同时判断是否与当前图片交互行为一致\",{\"1\":{\"53\":1}}],[\"同时\",{\"1\":{\"20\":1,\"165\":1,\"613\":1,\"646\":1,\"650\":1,\"688\":1,\"809\":1,\"833\":1,\"835\":2,\"945\":1,\"956\":1}}],[\"时为\",{\"1\":{\"909\":1}}],[\"时获取离散\",{\"1\":{\"899\":1}}],[\"时增强条件影响\",{\"1\":{\"894\":1}}],[\"时退化为普通有条件生成\",{\"1\":{\"894\":1}}],[\"时放大图像\",{\"1\":{\"893\":1}}],[\"时成立\",{\"1\":{\"885\":1}}],[\"时生成样本\",{\"1\":{\"943\":1}}],[\"时生成\",{\"1\":{\"885\":1}}],[\"时生成的\",{\"1\":{\"885\":1}}],[\"时生效\",{\"1\":{\"513\":1}}],[\"时非常有用\",{\"1\":{\"863\":1}}],[\"时取\",{\"1\":{\"857\":1}}],[\"时的rosenbrock函数\",{\"1\":{\"816\":1}}],[\"时展开变量列表\",{\"1\":{\"800\":1}}],[\"时让它同时进行两个任务\",{\"1\":{\"690\":1}}],[\"时代的开启\",{\"1\":{\"822\":1}}],[\"时代推向了\",{\"1\":{\"650\":1}}],[\"时代\",{\"1\":{\"650\":1}}],[\"时才涌现出来的能力\",{\"1\":{\"620\":1}}],[\"时按原序切分\",{\"1\":{\"513\":1}}],[\"时使用\",{\"1\":{\"380\":1,\"429\":1,\"887\":1}}],[\"时间小于\",{\"1\":{\"847\":1}}],[\"时间\",{\"1\":{\"690\":1}}],[\"时间复杂度为二次方\",{\"1\":{\"368\":1}}],[\"时间戳等\",{\"1\":{\"159\":1}}],[\"时性能会下降\",{\"1\":{\"330\":1}}],[\"时性能略低\",{\"1\":{\"292\":1}}],[\"时性能最佳\",{\"1\":{\"117\":1}}],[\"时效果最佳\",{\"1\":{\"289\":1}}],[\"时表现优异\",{\"1\":{\"273\":1}}],[\"时表示输出层\",{\"1\":{\"123\":1}}],[\"时不是用\",{\"1\":{\"190\":1}}],[\"时\",{\"1\":{\"48\":1,\"102\":1,\"117\":4,\"207\":1,\"215\":1,\"257\":1,\"258\":1,\"292\":1,\"353\":2,\"372\":1,\"476\":2,\"513\":1,\"518\":1,\"522\":3,\"544\":1,\"590\":4,\"611\":1,\"657\":1,\"663\":1,\"691\":1,\"708\":1,\"710\":1,\"805\":1,\"847\":1,\"848\":1,\"863\":1,\"868\":1,\"886\":1,\"893\":1,\"917\":1,\"945\":1,\"947\":1,\"949\":1}}],[\"缺少锐化\",{\"1\":{\"290\":1}}],[\"缺少居中\",{\"1\":{\"290\":1}}],[\"缺陷对比\",{\"1\":{\"157\":1}}],[\"缺陷类型\",{\"1\":{\"157\":1}}],[\"缺陷\",{\"0\":{\"157\":1},\"1\":{\"157\":1,\"587\":1}}],[\"缺乏结构化监督利用能力\",{\"1\":{\"649\":1}}],[\"缺乏可控性与鲁棒性\",{\"1\":{\"649\":1}}],[\"缺乏鲁棒的系统性泛化能力\",{\"1\":{\"649\":1}}],[\"缺乏具体的上下文\",{\"1\":{\"409\":1}}],[\"缺乏语言对齐\",{\"1\":{\"303\":1}}],[\"缺乏与llms的直接特征兼容性\",{\"1\":{\"298\":1}}],[\"缺乏图文融合的联合表示\",{\"1\":{\"268\":1}}],[\"缺乏预定义词汇\",{\"1\":{\"228\":1}}],[\"缺乏精细建模\",{\"1\":{\"157\":1}}],[\"缺乏层次化\",{\"1\":{\"157\":1}}],[\"缺乏层次化特征提取机制\",{\"1\":{\"157\":1}}],[\"缺乏动态上下文感知\",{\"1\":{\"157\":1}}],[\"缺乏足够上下文\",{\"1\":{\"117\":1}}],[\"缺乏纹理\",{\"1\":{\"26\":1}}],[\"缺点\",{\"1\":{\"110\":2,\"268\":3,\"346\":1,\"368\":2,\"369\":2,\"394\":1,\"589\":1}}],[\"缺失\",{\"1\":{\"48\":1}}],[\"若对多个\",{\"1\":{\"946\":1}}],[\"若我们对该公式求梯度\",{\"1\":{\"946\":1}}],[\"若各观测独立\",{\"1\":{\"904\":1}}],[\"若10次抛硬币出现6次正面\",{\"1\":{\"903\":1}}],[\"若仅需要\",{\"1\":{\"899\":1}}],[\"若仅获取\",{\"1\":{\"256\":1}}],[\"若不除以\",{\"1\":{\"893\":1}}],[\"若不使用运算符\",{\"1\":{\"811\":1}}],[\"若直接使用像素作为图像\",{\"1\":{\"885\":1}}],[\"若集合\",{\"1\":{\"850\":1}}],[\"若在某个事件\",{\"1\":{\"849\":1}}],[\"若满足以下条件\",{\"1\":{\"849\":1}}],[\"若干业务\",{\"1\":{\"835\":1}}],[\"若干局部视角图像\",{\"1\":{\"293\":1}}],[\"若再增加迭代次数到\",{\"1\":{\"816\":1}}],[\"若从好的起点开始\",{\"1\":{\"816\":1}}],[\"若画出其\",{\"1\":{\"816\":1}}],[\"若显示版本信息\",{\"1\":{\"815\":1}}],[\"若左操作数a不支持\",{\"1\":{\"809\":1}}],[\"若a未实现\",{\"1\":{\"809\":1}}],[\"若y\",{\"1\":{\"809\":1}}],[\"若为false\",{\"1\":{\"807\":1}}],[\"若为none则使用默认值\",{\"1\":{\"430\":1}}],[\"若使用强引用\",{\"1\":{\"806\":1}}],[\"若存在循环引用\",{\"1\":{\"806\":1}}],[\"若由\",{\"1\":{\"774\":1}}],[\"若拒绝机制不足\",{\"1\":{\"658\":1}}],[\"若未经适当调节可能导致某些群体观点被系统性排除\",{\"1\":{\"658\":1}}],[\"若未指定掩码位置\",{\"1\":{\"208\":1}}],[\"若刻意要求生成毒性内容\",{\"1\":{\"657\":1}}],[\"若前景点稀疏\",{\"1\":{\"589\":1}}],[\"若需固定切分可用\",{\"1\":{\"513\":1}}],[\"若都为\",{\"1\":{\"513\":1}}],[\"若希望确保内存效率\",{\"1\":{\"470\":1}}],[\"若qk\",{\"1\":{\"430\":1}}],[\"若有query\",{\"1\":{\"419\":1}}],[\"若以文本单词数量来衡量\",{\"1\":{\"407\":1}}],[\"若想要得到比较好的效果\",{\"1\":{\"353\":1}}],[\"若是自定义\",{\"1\":{\"264\":1}}],[\"若某簇为空\",{\"1\":{\"213\":1}}],[\"若已初始化\",{\"1\":{\"213\":1}}],[\"若选择\",{\"1\":{\"213\":1}}],[\"若提供了\",{\"1\":{\"895\":1}}],[\"若提供\",{\"1\":{\"208\":1,\"213\":1}}],[\"若只需输出\",{\"1\":{\"208\":1}}],[\"若图文语义相近但细节不同\",{\"1\":{\"201\":1}}],[\"若\",{\"1\":{\"173\":1,\"263\":1,\"290\":1,\"592\":2,\"847\":2,\"849\":1,\"858\":1,\"904\":1,\"944\":1}}],[\"若缺失\",{\"1\":{\"48\":1}}],[\"若技术被滥用可能侵犯隐私或导致缺乏问责\",{\"1\":{\"27\":1}}],[\"无偏调整前的\",{\"1\":{\"904\":1}}],[\"无偏置\",{\"1\":{\"120\":1,\"121\":1,\"522\":1}}],[\"无判别器\",{\"1\":{\"894\":1}}],[\"无条件\",{\"1\":{\"894\":2}}],[\"无条件预测\",{\"1\":{\"894\":1}}],[\"无条件引导技术\",{\"0\":{\"894\":1}}],[\"无条件生成\",{\"1\":{\"893\":1}}],[\"无病\",{\"1\":{\"850\":1}}],[\"无限信息\",{\"1\":{\"951\":1}}],[\"无限多种可能的区间组合\",{\"1\":{\"847\":1}}],[\"无限接近于\",{\"1\":{\"353\":1}}],[\"无所不知\",{\"1\":{\"827\":1}}],[\"无所不能\",{\"1\":{\"827\":1}}],[\"无缝理解和生成多种形式内容\",{\"1\":{\"823\":1}}],[\"无外部数据\",{\"1\":{\"685\":1}}],[\"无提示时\",{\"1\":{\"657\":1}}],[\"无害\",{\"1\":{\"654\":1}}],[\"无梯度更新的few\",{\"1\":{\"650\":1}}],[\"无任务特定架构\",{\"1\":{\"650\":1}}],[\"无关的常数\",{\"1\":{\"877\":1}}],[\"无关\",{\"1\":{\"574\":1,\"945\":1,\"951\":1}}],[\"无放回采样\",{\"1\":{\"518\":1}}],[\"无干扰\",{\"1\":{\"382\":1}}],[\"无监督任务学习作为预训练技术成功的关键因素\",{\"1\":{\"642\":1}}],[\"无监督任务学习是预训练技术成功的关键因素之一\",{\"1\":{\"639\":1}}],[\"无监督多任务学习的可行性证明\",{\"1\":{\"640\":1}}],[\"无监督预训练+监督微调方式\",{\"1\":{\"627\":1}}],[\"无监督预训练\",{\"0\":{\"629\":1},\"1\":{\"627\":1,\"633\":1}}],[\"无监督学习得到的好的表示也能提供显著的提升\",{\"1\":{\"626\":1}}],[\"无监督学习最大的一个卖点\",{\"1\":{\"353\":1}}],[\"无监督学习的表现往往不如有监督学习\",{\"1\":{\"353\":1}}],[\"无监督学习真的可行\",{\"1\":{\"348\":1}}],[\"无监督就很难去建模\",{\"1\":{\"353\":1}}],[\"无权重衰减\",{\"1\":{\"319\":1}}],[\"无意义内容\",{\"1\":{\"305\":1}}],[\"无意图推理\",{\"1\":{\"48\":1}}],[\"无multi\",{\"1\":{\"291\":1}}],[\"无论\",{\"1\":{\"931\":1}}],[\"无论计算图结构多复杂\",{\"1\":{\"811\":1}}],[\"无论预处理方式\",{\"1\":{\"640\":1}}],[\"无论到什么程度\",{\"1\":{\"626\":1}}],[\"无论其有效性如何\",{\"1\":{\"561\":1}}],[\"无论是正类还是负类\",{\"1\":{\"562\":1}}],[\"无论是有监督还是自监督方法\",{\"1\":{\"413\":1}}],[\"无论是vit还是resnet\",{\"1\":{\"289\":1}}],[\"无论你如何打乱输入元素的顺序\",{\"1\":{\"160\":1}}],[\"无标签数据\",{\"1\":{\"824\":1}}],[\"无标签的\",{\"1\":{\"286\":1}}],[\"无标签的知识蒸馏\",{\"1\":{\"280\":1}}],[\"无标签场景\",{\"1\":{\"283\":1}}],[\"无须区分图像的学习方法\",{\"1\":{\"282\":1}}],[\"无序性\",{\"1\":{\"159\":1}}],[\"无局部聚合机制\",{\"1\":{\"157\":1}}],[\"无效\",{\"1\":{\"137\":1}}],[\"无跳跃连接\",{\"1\":{\"122\":2}}],[\"无下采样模式\",{\"1\":{\"121\":2}}],[\"无邻域交互\",{\"1\":{\"117\":1}}],[\"无位置编码\",{\"1\":{\"117\":1}}],[\"无需强行约束\",{\"1\":{\"931\":1}}],[\"无需重新训练\",{\"1\":{\"830\":1}}],[\"无需\",{\"1\":{\"823\":1}}],[\"无需手动推导导数公式\",{\"1\":{\"811\":1}}],[\"无需计算导数\",{\"1\":{\"807\":1}}],[\"无需等待gc\",{\"1\":{\"806\":1}}],[\"无需额外接口\",{\"1\":{\"811\":1}}],[\"无需额外预处理\",{\"1\":{\"681\":1}}],[\"无需额外架构调整\",{\"1\":{\"646\":1}}],[\"无需复杂结构调整\",{\"1\":{\"686\":1}}],[\"无需复杂架构调整\",{\"1\":{\"669\":1}}],[\"无需复制数据\",{\"1\":{\"681\":1}}],[\"无需参数更新\",{\"1\":{\"647\":1}}],[\"无需任务特定的监督训练\",{\"1\":{\"638\":1}}],[\"无需对架构或归一化方式做修改\",{\"1\":{\"280\":1}}],[\"无需人工标注即可通过自注意力机制学习语义区域和物体边界\",{\"1\":{\"228\":1}}],[\"无需预处理\",{\"1\":{\"157\":1}}],[\"无需量化或投影\",{\"1\":{\"110\":1}}],[\"无需切分\",{\"1\":{\"55\":1}}],[\"无相机参数方法\",{\"1\":{\"76\":1}}],[\"无\",{\"1\":{\"48\":1,\"311\":1,\"663\":1}}],[\"无跨模态融合\",{\"1\":{\"48\":1}}],[\"无几何推理\",{\"1\":{\"48\":1}}],[\"无法闭式求解\",{\"1\":{\"947\":1}}],[\"无法精确计算的问题\",{\"1\":{\"945\":1}}],[\"无法及时反映最新的信息动态\",{\"1\":{\"828\":1}}],[\"无法回收\",{\"1\":{\"806\":1}}],[\"无法回答时\",{\"1\":{\"343\":1}}],[\"无法\",{\"1\":{\"735\":1}}],[\"无法利用结构化监督信号\",{\"1\":{\"649\":1}}],[\"无法利用自然语言知识\",{\"1\":{\"268\":1}}],[\"无法更新参数\",{\"1\":{\"612\":1}}],[\"无法实现zero\",{\"1\":{\"413\":1}}],[\"无法模拟整个数据集\",{\"1\":{\"355\":1}}],[\"无法收敛\",{\"1\":{\"285\":1}}],[\"无法应用到需要图像与文本\",{\"1\":{\"269\":1}}],[\"无法应对弯曲\",{\"1\":{\"157\":1}}],[\"无法直接计算\",{\"1\":{\"947\":1}}],[\"无法直接使用\",{\"1\":{\"228\":1}}],[\"无法直接用于每个点\",{\"1\":{\"156\":1}}],[\"无法充分利用\",{\"1\":{\"157\":1}}],[\"无法区分顺序信息\",{\"1\":{\"157\":1}}],[\"无法有效利用局部结构\",{\"1\":{\"157\":1}}],[\"无法捕捉边缘\",{\"1\":{\"157\":1}}],[\"无法像离散情形那样枚举所有子集\",{\"1\":{\"847\":1}}],[\"无法像\",{\"1\":{\"157\":1}}],[\"无法处理不在原文中的答案\",{\"1\":{\"735\":1}}],[\"无法处理非刚性变形\",{\"1\":{\"157\":1}}],[\"无法处理非刚性形变\",{\"1\":{\"157\":2}}],[\"无法处理开放词汇场景\",{\"1\":{\"31\":1}}],[\"无法很好地捕捉全局语义\",{\"1\":{\"19\":1}}],[\"✗\",{\"1\":{\"48\":4}}],[\"表大小\",{\"1\":{\"709\":1}}],[\"表9\",{\"1\":{\"668\":1}}],[\"表5\",{\"1\":{\"668\":1}}],[\"表7\",{\"1\":{\"666\":1,\"668\":1}}],[\"表7展示了在\",{\"1\":{\"376\":1}}],[\"表8\",{\"1\":{\"666\":1,\"668\":1}}],[\"表征学习\",{\"0\":{\"417\":1},\"1\":{\"417\":1}}],[\"表征会很细\",{\"1\":{\"350\":1}}],[\"表征不一致\",{\"1\":{\"296\":1}}],[\"表征空间\",{\"1\":{\"293\":1}}],[\"表征\",{\"1\":{\"208\":1,\"212\":1,\"355\":1}}],[\"表征作为跨模态图文对表示\",{\"1\":{\"207\":1}}],[\"表征作为图文对的联合表示\",{\"1\":{\"201\":1}}],[\"表征映射到归一化的低维\",{\"1\":{\"199\":1}}],[\"表\",{\"1\":{\"182\":1,\"242\":1,\"305\":4,\"656\":1,\"678\":1,\"709\":1}}],[\"表4\",{\"1\":{\"179\":1,\"666\":1,\"668\":1}}],[\"表23列出了三种配置的超参数\",{\"1\":{\"320\":1}}],[\"表2\",{\"1\":{\"178\":1}}],[\"表14\",{\"1\":{\"670\":1}}],[\"表16\",{\"1\":{\"669\":1}}],[\"表13\",{\"1\":{\"668\":1,\"670\":1}}],[\"表13展示gpt\",{\"1\":{\"641\":1}}],[\"表10\",{\"1\":{\"668\":1,\"669\":1}}],[\"表11\",{\"1\":{\"666\":1,\"668\":1,\"670\":1}}],[\"表12\",{\"1\":{\"666\":1,\"668\":1,\"670\":1}}],[\"表1\",{\"1\":{\"177\":1,\"304\":1}}],[\"表现优于gpt\",{\"1\":{\"670\":1}}],[\"表现接近palm\",{\"1\":{\"668\":1}}],[\"表现出诸如捏造事实\",{\"1\":{\"654\":1}}],[\"表现不稳定的问题\",{\"1\":{\"650\":1}}],[\"表现不错\",{\"1\":{\"157\":1}}],[\"表现仍然较弱\",{\"1\":{\"649\":1}}],[\"表现远不如fine\",{\"1\":{\"648\":1}}],[\"表现则不及微调模型\",{\"1\":{\"648\":1}}],[\"表现为\",{\"1\":{\"639\":1}}],[\"表现类似\",{\"1\":{\"586\":1}}],[\"表现更好\",{\"1\":{\"215\":1}}],[\"表现\",{\"1\":{\"183\":1,\"309\":1,\"626\":1}}],[\"表现良好\",{\"1\":{\"157\":1}}],[\"表明训练目标的改变\",{\"1\":{\"654\":1}}],[\"表明缩放定律\",{\"1\":{\"646\":1}}],[\"表明模型容量和训练数据规模仍需进一步扩大\",{\"1\":{\"642\":1}}],[\"表明模型容量是限制因素\",{\"1\":{\"641\":1}}],[\"表明进一步扩大模型和数据规模可能带来额外提升\",{\"1\":{\"641\":1}}],[\"表明进一步扩大数据或模型可能提升性能\",{\"1\":{\"640\":1}}],[\"表明在迁移中\",{\"1\":{\"635\":1}}],[\"表明\",{\"1\":{\"157\":1,\"290\":1,\"376\":3,\"641\":1}}],[\"表明交互意图推理对泛化至新\",{\"1\":{\"48\":1}}],[\"表面纹理等\",{\"1\":{\"157\":1}}],[\"表达不同维度之间的\",{\"1\":{\"574\":1}}],[\"表达\",{\"1\":{\"262\":1}}],[\"表达能力增长方式\",{\"1\":{\"500\":1}}],[\"表达能力受限于\",{\"1\":{\"157\":1,\"346\":1}}],[\"表达能力更强\",{\"1\":{\"117\":1}}],[\"表达式语言\",{\"1\":{\"833\":1,\"834\":1}}],[\"表达式\",{\"1\":{\"100\":1,\"462\":1,\"475\":1}}],[\"表3\",{\"1\":{\"48\":1,\"179\":1,\"666\":1,\"668\":1}}],[\"表示给定潜变量\",{\"1\":{\"943\":1}}],[\"表示还原为图像\",{\"1\":{\"899\":1}}],[\"表示保留\",{\"1\":{\"896\":1}}],[\"表示保持原来的大小\",{\"1\":{\"472\":1}}],[\"表示由\",{\"1\":{\"885\":3}}],[\"表示我们省略了分母\",{\"1\":{\"877\":1}}],[\"表示变量之间不相关\",{\"1\":{\"871\":1}}],[\"表示变量之间的相关性\",{\"1\":{\"574\":1}}],[\"表示分布的维度\",{\"1\":{\"946\":1}}],[\"表示分布的方差\",{\"1\":{\"865\":1}}],[\"表示分布的均值\",{\"1\":{\"865\":1}}],[\"表示成功的次数\",{\"1\":{\"860\":1}}],[\"表示成一个\",{\"1\":{\"857\":1}}],[\"表示这\",{\"1\":{\"859\":1}}],[\"表示这些位置不计入损失\",{\"1\":{\"384\":1}}],[\"表示类别\",{\"1\":{\"857\":1}}],[\"表示类别概率的加权和\",{\"1\":{\"257\":1}}],[\"表示当前样本属于第\",{\"1\":{\"857\":1}}],[\"表示当\",{\"1\":{\"857\":1}}],[\"表示选择类别\",{\"1\":{\"857\":1}}],[\"表示选择剩余所有维度\",{\"1\":{\"463\":1}}],[\"表示事件\",{\"1\":{\"848\":1}}],[\"表示正面\",{\"1\":{\"846\":1,\"904\":1}}],[\"表示正确对随机正例和负例进行排名的概率为\",{\"1\":{\"570\":1}}],[\"表示骰子掷出面为\",{\"1\":{\"846\":2}}],[\"表示实验中所有可能的结果组成的集合\",{\"1\":{\"845\":1}}],[\"表示将张量中的值限制在\",{\"1\":{\"734\":1}}],[\"表示注意力头数\",{\"1\":{\"710\":1}}],[\"表示要相乘后求和\",{\"1\":{\"709\":1}}],[\"表示编码维度\",{\"1\":{\"706\":1}}],[\"表示计算哪个位置的编码\",{\"1\":{\"706\":1}}],[\"表示位置索引\",{\"1\":{\"706\":1}}],[\"表示是否是连续句子\",{\"1\":{\"699\":1}}],[\"表示单词结束\",{\"1\":{\"595\":2}}],[\"表示对\",{\"1\":{\"589\":1}}],[\"表示对输入文本做\",{\"1\":{\"384\":1}}],[\"表示阈值\",{\"1\":{\"572\":1}}],[\"表示两分布差异最大\",{\"1\":{\"916\":1}}],[\"表示两行两列\",{\"1\":{\"544\":1}}],[\"表示两个向量方向相反\",{\"1\":{\"506\":1}}],[\"表示两个向量方向越接近\",{\"1\":{\"506\":1}}],[\"表示两个向量几乎正交\",{\"1\":{\"506\":1}}],[\"表示新视图从原始内存块中的哪个位置开始\",{\"1\":{\"544\":1}}],[\"表示比例\",{\"1\":{\"513\":1}}],[\"表示取最接近的整数索引\",{\"1\":{\"504\":1}}],[\"表示在假设\",{\"1\":{\"877\":1}}],[\"表示在我们看到任何数据之前\",{\"1\":{\"877\":1}}],[\"表示在这过程中抽到的\",{\"1\":{\"860\":1}}],[\"表示在行方向移动时\",{\"1\":{\"546\":1}}],[\"表示在每个维度上移动一个元素时\",{\"1\":{\"489\":1}}],[\"表示在每行中找\",{\"1\":{\"488\":1}}],[\"表示在最后一个维度的左侧填充1个元素\",{\"1\":{\"477\":1}}],[\"表示按列累加\",{\"1\":{\"487\":1}}],[\"表示按行累加\",{\"1\":{\"487\":1}}],[\"表示按照掩码位置\",{\"1\":{\"234\":1}}],[\"表示整数\",{\"1\":{\"485\":1}}],[\"表示整句话\",{\"1\":{\"277\":1}}],[\"表示先占位\",{\"1\":{\"474\":1}}],[\"表示向下取整的除法\",{\"1\":{\"462\":1}}],[\"表示模型更\",{\"1\":{\"657\":1}}],[\"表示模型的规模\",{\"1\":{\"435\":1}}],[\"表示模型预测的被\",{\"1\":{\"200\":1}}],[\"表示不强化\",{\"1\":{\"895\":1}}],[\"表示不同变量之间的线性相关性\",{\"1\":{\"574\":1}}],[\"表示不进行归一化\",{\"1\":{\"426\":2}}],[\"表示不对图像做掩码\",{\"1\":{\"384\":1}}],[\"表示不在反向传播中参与梯度计算\",{\"1\":{\"258\":1}}],[\"表示多模态\",{\"1\":{\"380\":1}}],[\"表示所有方向的方差相同\",{\"1\":{\"871\":2}}],[\"表示所有可能的实验结果\",{\"1\":{\"846\":1}}],[\"表示所有图像token有效\",{\"1\":{\"421\":1}}],[\"表示所有\",{\"1\":{\"380\":1}}],[\"表示进行图文对比学习\",{\"1\":{\"370\":1}}],[\"表示输出的\",{\"1\":{\"926\":1}}],[\"表示输出头\",{\"1\":{\"123\":1}}],[\"表示输入结束\",{\"1\":{\"342\":1}}],[\"表示返回每个唯一值的计数\",{\"1\":{\"293\":1}}],[\"表示发生崩溃\",{\"1\":{\"290\":1}}],[\"表示需要\",{\"1\":{\"266\":1}}],[\"表示只输出被\",{\"1\":{\"265\":1}}],[\"表示只在通道维度操作\",{\"1\":{\"152\":1}}],[\"表示哪些\",{\"1\":{\"265\":1}}],[\"表示哪些点属于目标功能区域\",{\"1\":{\"94\":1}}],[\"表示解码器\",{\"1\":{\"235\":1}}],[\"表示视觉令牌\",{\"1\":{\"235\":1}}],[\"表示原始图像\",{\"1\":{\"235\":1}}],[\"表示训练语料库\",{\"1\":{\"234\":1}}],[\"表示第\",{\"1\":{\"233\":1,\"238\":1,\"274\":1}}],[\"表示为\",{\"1\":{\"231\":1,\"233\":1}}],[\"表示停止梯度操作\",{\"1\":{\"212\":1}}],[\"表示无\",{\"1\":{\"206\":1}}],[\"表示被遮挡\",{\"1\":{\"235\":1}}],[\"表示被\",{\"1\":{\"200\":1}}],[\"表示图像编码器\",{\"1\":{\"235\":1}}],[\"表示图像特征没有被\",{\"1\":{\"188\":1}}],[\"表示图文对的真实匹配状态\",{\"1\":{\"201\":1}}],[\"表示图文对的多模态表示\",{\"1\":{\"171\":1}}],[\"表示结束\",{\"1\":{\"171\":1}}],[\"表示形式\",{\"1\":{\"159\":1}}],[\"表示最原始的点云\",{\"1\":{\"146\":1}}],[\"表示每一层mlp的输出通道数\",{\"1\":{\"145\":1}}],[\"表示每个类别的权重\",{\"1\":{\"514\":1}}],[\"表示每个元素在唯一值张量中的索引\",{\"1\":{\"480\":1}}],[\"表示每个图像块的大小是\",{\"1\":{\"425\":1}}],[\"表示每个样本和每个中心的相似度\",{\"1\":{\"213\":1}}],[\"表示每个\",{\"1\":{\"100\":1,\"710\":1}}],[\"表示每个点是否属于目标功能区域\",{\"1\":{\"589\":1}}],[\"表示每个点是否具有可操作性\",{\"1\":{\"70\":1}}],[\"表示每个点是否具有特定可操作性的概率\",{\"1\":{\"64\":1}}],[\"表示每个点属于功能区域的概率\",{\"1\":{\"589\":1}}],[\"表示每个点属于目标功能区域的概率\",{\"1\":{\"100\":1}}],[\"表示其属于目标功能区域的概率\",{\"1\":{\"100\":1}}],[\"表示该位置的图像特征\",{\"1\":{\"963\":1}}],[\"表示该函数中不进行梯度计算\",{\"1\":{\"898\":1}}],[\"表示该模型优于左侧曲线对应的模型\",{\"1\":{\"572\":1}}],[\"表示该维度上复制的次数\",{\"1\":{\"471\":1}}],[\"表示该层点数\",{\"1\":{\"96\":1}}],[\"表示该点是否具有可操作性\",{\"1\":{\"70\":1}}],[\"表示点属于该功能区域的概率\",{\"1\":{\"88\":1}}],[\"表示概率\",{\"1\":{\"70\":1}}],[\"表示填充\",{\"1\":{\"67\":1}}],[\"表示有效位置\",{\"1\":{\"274\":1}}],[\"表示有效\",{\"1\":{\"67\":1}}],[\"表示\",{\"1\":{\"65\":2,\"83\":1,\"100\":1,\"137\":1,\"152\":1,\"154\":1,\"202\":1,\"208\":1,\"212\":1,\"263\":1,\"266\":4,\"274\":1,\"385\":1,\"420\":1,\"463\":1,\"487\":3,\"542\":1,\"570\":1,\"582\":1,\"699\":1,\"710\":4,\"713\":1,\"846\":1,\"887\":2,\"899\":1,\"900\":1,\"944\":1,\"950\":1,\"963\":1,\"964\":2}}],[\"表示池化后扩展为\",{\"1\":{\"38\":1}}],[\"表示的一致映射\",{\"1\":{\"22\":1}}],[\"能不能把编码器的输出张量\",{\"1\":{\"958\":1}}],[\"能不能自己生成答案\",{\"1\":{\"735\":1}}],[\"能输入一个字符串或图像\",{\"1\":{\"952\":1}}],[\"能输出\",{\"1\":{\"925\":1}}],[\"能生成离散输出这一特性启发了后续很多生成模型\",{\"1\":{\"925\":1}}],[\"能生成连贯但虚构的内容\",{\"1\":{\"641\":1}}],[\"能最大化数据集的图像的出现概率\",{\"1\":{\"925\":1}}],[\"能确保\",{\"1\":{\"924\":1}}],[\"能用\",{\"1\":{\"924\":1}}],[\"能量视角下的gan模型\",{\"1\":{\"919\":1}}],[\"能促进更好的码本利用率\",{\"1\":{\"886\":1}}],[\"能跑起来\",{\"1\":{\"814\":1}}],[\"能映射的最大距离\",{\"1\":{\"710\":1}}],[\"能提升掩码语言模型的困惑度和下游任务性能\",{\"1\":{\"681\":1}}],[\"能处理非训练分布指令\",{\"1\":{\"657\":1}}],[\"能带来质的改善\",{\"1\":{\"654\":1}}],[\"能力提升伴随风险增加\",{\"1\":{\"670\":1}}],[\"能力\",{\"1\":{\"651\":1,\"814\":1,\"823\":2}}],[\"能力分布不均\",{\"1\":{\"649\":1}}],[\"能力下降\",{\"1\":{\"277\":1}}],[\"能缓解类别不平衡问题\",{\"1\":{\"587\":1}}],[\"能缓解弱标注图文数据中的噪声问题\",{\"1\":{\"195\":1}}],[\"能避免切分导致某类在测试集中缺失\",{\"1\":{\"513\":1}}],[\"能自适应频率和相位\",{\"1\":{\"500\":1}}],[\"能起作用的原因在于\",{\"1\":{\"427\":1}}],[\"能否基于互联网上的大量文本来预训练视觉模型\",{\"1\":{\"413\":1}}],[\"能根据用户指令回答问题\",{\"1\":{\"346\":1}}],[\"能适应不同batch大小\",{\"1\":{\"285\":1}}],[\"能显著提升条件相关性\",{\"1\":{\"894\":1}}],[\"能显著提升模型性能\",{\"1\":{\"373\":1}}],[\"能显著提升\",{\"1\":{\"280\":1}}],[\"能在无需大量任务特定微调的前提下\",{\"1\":{\"268\":1}}],[\"能在下游任务上泛化得更好的原因之一\",{\"1\":{\"243\":1}}],[\"能学到跨模态的联合表示\",{\"1\":{\"268\":1}}],[\"能让无监督的对比学习取得很好的效果\",{\"1\":{\"352\":1}}],[\"能让\",{\"1\":{\"259\":1}}],[\"能更精准地控制内存释放时机\",{\"1\":{\"806\":1}}],[\"能更有效地重组视觉特征为llm兼容的序列\",{\"1\":{\"303\":1}}],[\"能更有效应对含噪声的网络数据\",{\"1\":{\"194\":1}}],[\"能更好处理异常误差\",{\"1\":{\"259\":1}}],[\"能更稳定地追踪模型训练动态\",{\"1\":{\"212\":1}}],[\"能反映边缘响应质量\",{\"1\":{\"106\":1}}],[\"能够成功解码的\",{\"1\":{\"946\":1}}],[\"能够以结构化的格式返回信息\",{\"1\":{\"833\":1}}],[\"能够以指数级减少所需的神经元数量\",{\"1\":{\"500\":1}}],[\"能够根据任务指令执行任务\",{\"1\":{\"825\":1}}],[\"能够理解和生成依赖于前文的文本内容\",{\"1\":{\"824\":1}}],[\"能够分析和理解用户提供的图片\",{\"1\":{\"823\":1}}],[\"能够匹配甚至超越后续提出的多种模型\",{\"1\":{\"678\":1}}],[\"能够泛化\",{\"1\":{\"658\":1}}],[\"能够通过检索对应应用场景数据的方式\",{\"1\":{\"828\":1}}],[\"能够通过预测任务的自然语言演示\",{\"1\":{\"642\":1}}],[\"能够通过预测任务的自然语言描述\",{\"1\":{\"639\":1}}],[\"能够通过自有数据\",{\"1\":{\"601\":1}}],[\"能够通过多层\",{\"1\":{\"427\":1}}],[\"能够动态地聚合图像信息\",{\"1\":{\"427\":1}}],[\"能够更好地聚合图像信息\",{\"1\":{\"427\":1}}],[\"能够很近似在整个数据集上做的多分类损失\",{\"1\":{\"356\":1}}],[\"能够很好地支持\",{\"1\":{\"86\":1}}],[\"能够有效重组视觉特征\",{\"1\":{\"305\":1}}],[\"能够有效建模不同的视觉与视觉\",{\"1\":{\"225\":1}}],[\"能够参考其他\",{\"1\":{\"286\":1}}],[\"能够自然地融合三种训练范式\",{\"1\":{\"272\":1}}],[\"能够自动关注对象边界或同类区域\",{\"1\":{\"243\":1}}],[\"能够自适应地选择最适合的特征尺度进行组合\",{\"1\":{\"141\":1}}],[\"能够对图像内容进行更高级别的抽象总结\",{\"1\":{\"234\":1}}],[\"能够同时处理多种模态\",{\"1\":{\"220\":1}}],[\"能够在零样本\",{\"1\":{\"638\":1}}],[\"能够在自监督学习中捕捉丰富的上下文信息\",{\"1\":{\"210\":1}}],[\"能够在开放场景下支持可供性理解\",{\"1\":{\"50\":1}}],[\"能够\",{\"1\":{\"202\":1}}],[\"能够编码更丰富的空间信息\",{\"1\":{\"119\":1}}],[\"能够处理点云的排列和数量不变性\",{\"1\":{\"109\":1}}],[\"能够突破预定义样本空间的限制\",{\"1\":{\"50\":1}}],[\"能够支持多种下游任务\",{\"1\":{\"222\":1}}],[\"能够支持\",{\"1\":{\"19\":1}}],[\"能正确捕捉如\",{\"1\":{\"47\":1}}],[\"约占\",{\"1\":{\"895\":1}}],[\"约40人\",{\"1\":{\"658\":1}}],[\"约4000多个样本\",{\"1\":{\"424\":1}}],[\"约束\",{\"1\":{\"655\":1}}],[\"约33\",{\"1\":{\"648\":1}}],[\"约95\",{\"1\":{\"641\":1}}],[\"约为主实验\",{\"1\":{\"242\":1}}],[\"约耗时五天\",{\"1\":{\"236\":1}}],[\"约\",{\"1\":{\"47\":2,\"224\":2,\"236\":1,\"656\":1}}],[\"高概率地类似训练集中的数据\",{\"1\":{\"943\":1}}],[\"高度\",{\"1\":{\"924\":1}}],[\"高可用的托管解决方案\",{\"1\":{\"834\":1}}],[\"高计算资源需求\",{\"1\":{\"824\":1}}],[\"高次运算\",{\"1\":{\"811\":1}}],[\"高于ndarray的优先级\",{\"1\":{\"809\":1}}],[\"高于性别化代词\",{\"1\":{\"668\":1}}],[\"高维更多语义\",{\"1\":{\"706\":1}}],[\"高维对全局位置变化敏感\",{\"1\":{\"706\":1}}],[\"高维向量部分\",{\"1\":{\"706\":1}}],[\"高维分量\",{\"1\":{\"706\":1}}],[\"高维输入时\",{\"1\":{\"500\":1}}],[\"高纬度的编码值波动性较小\",{\"1\":{\"706\":1}}],[\"高赞网页内容\",{\"1\":{\"680\":1}}],[\"高质量完成\",{\"1\":{\"658\":1}}],[\"高质量数据被重复使用\",{\"1\":{\"647\":1}}],[\"高质量双语数据集\",{\"1\":{\"322\":1,\"323\":1}}],[\"高容量模型通过最大化文本序列的似然估计\",{\"1\":{\"643\":1}}],[\"高置信度\",{\"1\":{\"589\":1}}],[\"高阶导数构建\",{\"1\":{\"814\":1}}],[\"高阶导数\",{\"0\":{\"814\":1,\"817\":1}}],[\"高阶导数与深度学习优化进阶\",{\"1\":{\"813\":1}}],[\"高阶项导致数值不稳定\",{\"1\":{\"500\":1}}],[\"高阶函数\",{\"1\":{\"447\":1,\"449\":3,\"450\":1}}],[\"高效\",{\"1\":{\"814\":1}}],[\"高效的模型微调\",{\"1\":{\"614\":1}}],[\"高效实现\",{\"1\":{\"510\":1}}],[\"高效广播\",{\"1\":{\"472\":1}}],[\"高效利用\",{\"1\":{\"262\":1}}],[\"高\",{\"1\":{\"323\":1,\"415\":1,\"589\":1}}],[\"高分辨率优化方法\",{\"1\":{\"326\":1}}],[\"高分辨率用于文档分析\",{\"1\":{\"323\":1}}],[\"高分辨率点云\",{\"1\":{\"122\":1}}],[\"高分辨率\",{\"1\":{\"121\":1,\"122\":2,\"336\":1}}],[\"高出\",{\"1\":{\"47\":2}}],[\"高斯肥皂泡现象\",{\"1\":{\"873\":1}}],[\"高斯壳\",{\"0\":{\"872\":1}}],[\"高斯联合分布\",{\"0\":{\"869\":1}}],[\"高斯分布是熵最大的分布\",{\"1\":{\"869\":1}}],[\"高斯分布作为近似是相当合理的\",{\"1\":{\"869\":1}}],[\"高斯分布的\",{\"1\":{\"872\":1}}],[\"高斯分布的一个问题在于它对离群点非常敏感\",{\"1\":{\"867\":1}}],[\"高斯分布的累积分布函数\",{\"1\":{\"865\":1}}],[\"高斯分布的概率密度函数\",{\"1\":{\"865\":1}}],[\"高斯分布\",{\"0\":{\"865\":1}}],[\"高斯过程\",{\"1\":{\"574\":2}}],[\"高斯混合模型\",{\"1\":{\"574\":1}}],[\"高斯模糊\",{\"1\":{\"286\":1,\"293\":3}}],[\"高斯散点渲染\",{\"1\":{\"26\":1}}],[\"高斯基元参数包括位置\",{\"1\":{\"22\":1}}],[\"依次通过每个编码器层\",{\"1\":{\"699\":1}}],[\"依次通过所有的单模态\",{\"1\":{\"274\":1}}],[\"依此类推\",{\"1\":{\"541\":2}}],[\"依然称之为\",{\"1\":{\"286\":1}}],[\"依然超越了许多依赖私有数据的强大模型\",{\"1\":{\"220\":1}}],[\"依然超过了许多依赖私有大数据的基础模型\",{\"1\":{\"220\":1}}],[\"依赖计算量大\",{\"1\":{\"942\":1}}],[\"依赖高质量的训练数\",{\"1\":{\"830\":1}}],[\"依赖于我们作出的一个假设\",{\"1\":{\"949\":1}}],[\"依赖于构建高质量的数据集\",{\"1\":{\"830\":1}}],[\"依赖于输入x的具体值\",{\"1\":{\"779\":1}}],[\"依赖任务特定的架构\",{\"1\":{\"646\":1}}],[\"依赖\",{\"1\":{\"557\":2}}],[\"依赖闭包机制来记住原函数的引用\",{\"1\":{\"449\":1}}],[\"依赖浅层相似度计算\",{\"1\":{\"303\":1}}],[\"依赖预训练的视觉和文本编码器\",{\"1\":{\"269\":1}}],[\"依赖预定义类别\",{\"1\":{\"30\":1}}],[\"依赖目标检测器\",{\"1\":{\"269\":1,\"369\":1}}],[\"依赖视角选择\",{\"1\":{\"159\":1}}],[\"依赖初始点和距离度量方式的选择\",{\"1\":{\"134\":1}}],[\"依旧表现出色\",{\"1\":{\"47\":1}}],[\"下标构成的数组\",{\"1\":{\"958\":1}}],[\"下高概率的区域\",{\"1\":{\"945\":1}}],[\"下高概率的\",{\"1\":{\"945\":1}}],[\"下方信息\",{\"1\":{\"926\":1}}],[\"下衡量\",{\"1\":{\"709\":1}}],[\"下载地址\",{\"1\":{\"696\":1}}],[\"下载预先训练的模型权重\",{\"1\":{\"582\":1}}],[\"下句的\",{\"1\":{\"692\":1}}],[\"下一个数字或者补充的像素有许多可能\",{\"1\":{\"952\":1}}],[\"下一个步骤会介绍并实现另一种优化方法\",{\"1\":{\"816\":1}}],[\"下一个句子预测损失\",{\"1\":{\"731\":1}}],[\"下一句话\",{\"1\":{\"692\":1}}],[\"下一句预测\",{\"1\":{\"679\":1,\"699\":2}}],[\"下一轮推理时直接读取缓存结果\",{\"1\":{\"663\":1}}],[\"下一轮计算时直接读取缓存结果\",{\"1\":{\"660\":1}}],[\"下一步我们又要根据嵌入空间把离散编码转回一个向量\",{\"1\":{\"958\":1}}],[\"下一步只送入新\",{\"1\":{\"663\":1}}],[\"下一步便可以通过\",{\"1\":{\"382\":1}}],[\"下进行训练\",{\"1\":{\"656\":1}}],[\"下\",{\"1\":{\"547\":2}}],[\"下表2是作者模型和之前sota模型nli的结果比较\",{\"1\":{\"634\":1}}],[\"下表中对比了vit\",{\"1\":{\"434\":1}}],[\"下表现不佳\",{\"1\":{\"19\":1}}],[\"下图按照时间线给出了\",{\"1\":{\"823\":1}}],[\"下图展示的是步骤7中以元组形式返回的kv\",{\"1\":{\"663\":1}}],[\"下图展示了\",{\"1\":{\"419\":1}}],[\"下图给出了有\",{\"1\":{\"662\":1}}],[\"下图给出了无\",{\"1\":{\"661\":1}}],[\"下图给出的是\",{\"1\":{\"293\":1}}],[\"下游任务微调\",{\"0\":{\"375\":1}}],[\"下训练并\",{\"1\":{\"315\":1}}],[\"下分布在\",{\"1\":{\"286\":1}}],[\"下次输入通道设为隐藏通道\",{\"1\":{\"255\":2}}],[\"下采样点的特征\",{\"1\":{\"145\":1}}],[\"下采样点数量\",{\"1\":{\"145\":1}}],[\"下采样点对应的特征数据\",{\"1\":{\"145\":1}}],[\"下采样点特征\",{\"1\":{\"143\":1}}],[\"下采样点坐标\",{\"1\":{\"143\":1}}],[\"下采样比例与邻居点数\",{\"1\":{\"123\":1}}],[\"下采样后的点坐标数据\",{\"1\":{\"145\":1}}],[\"下采样后的点坐标\",{\"1\":{\"121\":2}}],[\"下采样后的点云仍然保持原始点云的几何形状\",{\"1\":{\"121\":1}}],[\"下采样模式\",{\"1\":{\"121\":2}}],[\"下采样率\",{\"1\":{\"121\":1}}],[\"下采样率为\",{\"1\":{\"116\":1}}],[\"下采样步长\",{\"1\":{\"121\":1}}],[\"下采样层\",{\"0\":{\"121\":1}}],[\"下的\",{\"1\":{\"106\":1,\"946\":1}}],[\"下的表现\",{\"1\":{\"106\":1}}],[\"下面给出几个示例\",{\"1\":{\"904\":1}}],[\"下面给出\",{\"1\":{\"899\":1}}],[\"下面给出的是单模态编码过程实现\",{\"1\":{\"274\":1}}],[\"下面给出的是beit主模型的预训练流程核心的代码实现\",{\"1\":{\"265\":1}}],[\"下面给出的是\",{\"1\":{\"256\":1}}],[\"下面给出的是一个基于pointnet++的点云语义分割模型定义\",{\"1\":{\"146\":1}}],[\"下面为\",{\"1\":{\"802\":1}}],[\"下面的3是因为我们用一次矩阵运算得到了拼接在一起的q\",{\"1\":{\"430\":1}}],[\"下面所给出的代码实现\",{\"1\":{\"430\":1}}],[\"下面来实际展示一下效果\",{\"1\":{\"411\":1}}],[\"下面会分小节独立对每个学习任务的计算过程进行详解\",{\"1\":{\"383\":1}}],[\"下面我们通过代码详细来看一下具体实现细节\",{\"1\":{\"893\":1}}],[\"下面我们将进入训练流程\",{\"1\":{\"435\":1}}],[\"下面我们将用于图片变换的transforms流水线和上面自定义的mydataset类都封装到dataloader去\",{\"1\":{\"425\":1}}],[\"下面我们将结合上面的模版流程\",{\"1\":{\"381\":1}}],[\"下面我们来看看论文中ltm的例子\",{\"1\":{\"622\":1}}],[\"下面我们来看论文中给的self\",{\"1\":{\"621\":1}}],[\"下面我们来看论文中给的cot的例子\",{\"1\":{\"620\":1}}],[\"下面我们来看\",{\"1\":{\"293\":1}}],[\"下面将展示gpt2block模块的实现逻辑\",{\"1\":{\"663\":1}}],[\"下面将展示正式执行块状遮挡策略前的准备工作\",{\"1\":{\"263\":1}}],[\"下面将给出使用了\",{\"1\":{\"663\":1}}],[\"下面将给出captioner模块基于coco数据集\",{\"1\":{\"187\":1}}],[\"下面首先给出的是\",{\"1\":{\"192\":1}}],[\"下面先给出完整代码实现\",{\"1\":{\"709\":1}}],[\"下面先给出\",{\"1\":{\"190\":1}}],[\"下面针对上面部分代码进行进一步说明\",{\"1\":{\"119\":1}}],[\"下面详细介绍一下动态卷机核卷积的过程\",{\"1\":{\"100\":1}}],[\"下面只需要把以上的三个步骤按流程组织起来即可得到afm模块的完整实现了\",{\"1\":{\"99\":1}}],[\"下降了\",{\"1\":{\"48\":1}}],[\"下均显著优于现有方法\",{\"1\":{\"47\":1}}],[\"xc+1\",{\"1\":{\"964\":1}}],[\"xc\",{\"1\":{\"937\":2,\"964\":2}}],[\"xcit\",{\"1\":{\"293\":1}}],[\"x|z\",{\"1\":{\"932\":2}}],[\"x^\",{\"1\":{\"809\":1}}],[\"x的tf\",{\"1\":{\"811\":1}}],[\"x的正向传播简单地对输入取反\",{\"1\":{\"809\":1}}],[\"x的\",{\"1\":{\"809\":1}}],[\"x需要调用x的\",{\"1\":{\"809\":1}}],[\"x时\",{\"1\":{\"809\":2}}],[\"xs\",{\"1\":{\"800\":5,\"805\":2,\"807\":2,\"809\":1}}],[\"xsystem\",{\"1\":{\"342\":2}}],[\"xu\",{\"1\":{\"655\":1,\"889\":1}}],[\"x和y之间的关系是\",{\"1\":{\"600\":1}}],[\"xm\",{\"1\":{\"600\":1}}],[\"xmf\",{\"1\":{\"46\":1}}],[\"x方向和y方向的线性插值的叠加\",{\"1\":{\"505\":1}}],[\"xlnet\",{\"1\":{\"650\":1,\"678\":1}}],[\"xlabel\",{\"1\":{\"424\":1,\"816\":1}}],[\"xlm\",{\"1\":{\"309\":1}}],[\"x=i\",{\"1\":{\"424\":1}}],[\"xticks\",{\"1\":{\"424\":1}}],[\"xtd\",{\"1\":{\"309\":1}}],[\"xa\",{\"1\":{\"342\":1}}],[\"xrec\",{\"1\":{\"213\":2}}],[\"xn\",{\"1\":{\"150\":3,\"403\":3}}],[\"x5\",{\"1\":{\"123\":4}}],[\"x4等等也经过e12得到了真正的负样本表征f2\",{\"1\":{\"353\":1}}],[\"x4\",{\"1\":{\"123\":5}}],[\"x3\",{\"1\":{\"123\":5,\"353\":1,\"403\":3}}],[\"x0\",{\"1\":{\"123\":6,\"505\":2,\"771\":2,\"807\":4,\"809\":31,\"815\":5,\"816\":36}}],[\"x2a\",{\"1\":{\"342\":1}}],[\"x2instruct\",{\"1\":{\"342\":1}}],[\"x2\",{\"1\":{\"122\":6,\"123\":5,\"403\":3,\"600\":1}}],[\"x1的计算图\",{\"1\":{\"815\":1}}],[\"x1的导数公式为\",{\"1\":{\"809\":1}}],[\"x1的反向传播中\",{\"1\":{\"809\":1}}],[\"x12叫做x11的正样本\",{\"1\":{\"353\":1}}],[\"x1a\",{\"1\":{\"342\":1}}],[\"x1instruct\",{\"1\":{\"342\":1}}],[\"x1\",{\"1\":{\"122\":4,\"123\":5,\"150\":3,\"505\":2,\"600\":1,\"771\":2,\"807\":3,\"809\":34,\"815\":5,\"816\":34,\"823\":1}}],[\"xj+δ\",{\"1\":{\"119\":1}}],[\"xj\",{\"1\":{\"119\":1}}],[\"xinstruct\",{\"1\":{\"342\":1}}],[\"xie\",{\"1\":{\"283\":1}}],[\"xi\",{\"1\":{\"119\":1,\"150\":1}}],[\"xx\",{\"1\":{\"107\":1}}],[\"xyz2\",{\"1\":{\"143\":2,\"145\":6}}],[\"xyz1\",{\"1\":{\"143\":3,\"145\":6}}],[\"xyz标志决定输出格式\",{\"1\":{\"119\":1}}],[\"xyz=false\",{\"1\":{\"119\":1}}],[\"xyz=true则为\",{\"1\":{\"119\":1}}],[\"xyz=true\",{\"1\":{\"119\":2,\"121\":1}}],[\"xyz\",{\"1\":{\"54\":4,\"83\":4,\"94\":5,\"100\":1,\"119\":37,\"121\":6,\"122\":11,\"123\":2,\"137\":60,\"138\":11,\"141\":31,\"143\":2,\"146\":22}}],[\"x\",{\"1\":{\"53\":2,\"58\":2,\"59\":2,\"60\":3,\"82\":2,\"83\":7,\"92\":2,\"96\":4,\"97\":10,\"98\":5,\"99\":3,\"100\":2,\"119\":29,\"120\":25,\"121\":23,\"122\":18,\"123\":3,\"138\":12,\"141\":10,\"146\":9,\"152\":29,\"154\":32,\"155\":10,\"156\":20,\"159\":2,\"213\":13,\"255\":3,\"263\":1,\"266\":30,\"293\":8,\"359\":3,\"380\":69,\"384\":10,\"385\":10,\"403\":3,\"426\":8,\"427\":14,\"428\":15,\"429\":21,\"430\":9,\"431\":19,\"444\":4,\"448\":4,\"460\":2,\"463\":4,\"467\":1,\"468\":1,\"469\":3,\"470\":1,\"471\":6,\"472\":9,\"476\":2,\"477\":5,\"478\":7,\"480\":6,\"481\":4,\"482\":4,\"484\":1,\"485\":6,\"486\":2,\"490\":4,\"491\":6,\"492\":1,\"502\":3,\"504\":1,\"505\":2,\"510\":2,\"513\":10,\"522\":8,\"538\":7,\"542\":6,\"571\":1,\"574\":1,\"582\":9,\"592\":2,\"595\":6,\"597\":4,\"600\":1,\"606\":1,\"656\":2,\"709\":5,\"724\":7,\"735\":1,\"743\":2,\"745\":3,\"746\":8,\"747\":4,\"749\":12,\"750\":4,\"751\":7,\"757\":4,\"762\":5,\"763\":2,\"766\":6,\"771\":3,\"775\":3,\"779\":2,\"780\":2,\"781\":1,\"784\":3,\"787\":4,\"790\":4,\"794\":5,\"795\":5,\"800\":2,\"801\":4,\"803\":17,\"804\":2,\"805\":13,\"807\":19,\"808\":11,\"809\":24,\"811\":26,\"815\":25,\"816\":6,\"856\":1,\"909\":3,\"926\":57,\"931\":4,\"932\":7,\"937\":5,\"938\":7,\"959\":2,\"963\":17,\"964\":4}}],[\"x9x\",{\"1\":{\"28\":1}}],[\"包初始化文件\",{\"1\":{\"810\":1}}],[\"包和库是组织代码的重要方式\",{\"1\":{\"810\":1}}],[\"包\",{\"1\":{\"557\":1,\"810\":1}}],[\"包类型\",{\"1\":{\"557\":1}}],[\"包可能会安装到基础环境或系统\",{\"1\":{\"557\":1}}],[\"包会安装到该环境的\",{\"1\":{\"557\":1}}],[\"包名\",{\"1\":{\"550\":1}}],[\"包裹起来\",{\"1\":{\"458\":1}}],[\"包括三项\",{\"1\":{\"963\":1}}],[\"包括声名远扬的stable\",{\"1\":{\"955\":1}}],[\"包括或不包括自己\",{\"1\":{\"926\":1}}],[\"包括让\",{\"1\":{\"833\":1}}],[\"包括中间步骤的数据流传输\",{\"1\":{\"833\":1}}],[\"包括批量处理\",{\"1\":{\"833\":1}}],[\"包括写文章\",{\"1\":{\"827\":1}}],[\"包括生成有害内容\",{\"1\":{\"824\":1}}],[\"包括文本分割\",{\"1\":{\"833\":1}}],[\"包括文本\",{\"1\":{\"824\":1}}],[\"包括了\",{\"1\":{\"823\":1}}],[\"包括开源和闭源\",{\"1\":{\"823\":1}}],[\"包括问题和上下文\",{\"1\":{\"733\":1}}],[\"包括wikitext\",{\"1\":{\"696\":1}}],[\"包括动态掩码\",{\"1\":{\"688\":1}}],[\"包括但不限于\",{\"1\":{\"656\":1}}],[\"包括预归一化\",{\"1\":{\"647\":1}}],[\"包括翻译\",{\"1\":{\"645\":1}}],[\"包括探索gpt\",{\"1\":{\"642\":1}}],[\"包括penn\",{\"1\":{\"641\":1}}],[\"包括基于对比学习的方法\",{\"1\":{\"413\":1}}],[\"包括图像分类\",{\"1\":{\"295\":1}}],[\"包括图像增强\",{\"1\":{\"265\":1}}],[\"包括人工标注和网络噪声数据\",{\"1\":{\"268\":1}}],[\"包括位置\",{\"1\":{\"266\":1}}],[\"包括以下几步\",{\"1\":{\"606\":1}}],[\"包括以下步骤\",{\"1\":{\"265\":1}}],[\"包括以下核心模块\",{\"1\":{\"94\":1}}],[\"包括颜色抖动\",{\"1\":{\"264\":1}}],[\"包括随机裁剪\",{\"1\":{\"224\":1}}],[\"包括\",{\"1\":{\"185\":1,\"220\":1,\"224\":1,\"273\":1,\"293\":1,\"332\":1,\"427\":1,\"510\":1,\"655\":1,\"680\":1,\"810\":1,\"823\":3,\"831\":1,\"924\":1}}],[\"包括理解类任务和生成类任务\",{\"1\":{\"183\":1}}],[\"包括前向传播\",{\"1\":{\"105\":1,\"265\":1}}],[\"包括与多个先进方法的对比以及消融实验和可视化分析\",{\"1\":{\"45\":1}}],[\"包含整个样本空间和空集\",{\"1\":{\"847\":1}}],[\"包含绘图的完整代码\",{\"1\":{\"816\":1}}],[\"包含多层pow\",{\"1\":{\"815\":1}}],[\"包含add函数节点和变量连接\",{\"1\":{\"815\":1}}],[\"包含高次项和交叉项\",{\"1\":{\"811\":1}}],[\"包含争议性内容\",{\"1\":{\"656\":1}}],[\"包含从\",{\"1\":{\"542\":1}}],[\"包含优化器\",{\"1\":{\"510\":1}}],[\"包含切分后的张量块\",{\"1\":{\"482\":1}}],[\"包含大约\",{\"1\":{\"435\":1}}],[\"包含一个线性层和一个\",{\"1\":{\"431\":1}}],[\"包含分类token\",{\"1\":{\"430\":1}}],[\"包含所有图像对应类别的列表\",{\"1\":{\"424\":1}}],[\"包含所有图像文件路径的列表\",{\"1\":{\"424\":1}}],[\"包含所有裁剪的拼接结果\",{\"1\":{\"293\":1}}],[\"包含图像和对应的标签\",{\"1\":{\"424\":1}}],[\"包含图像和可选文本\",{\"1\":{\"421\":1}}],[\"包含图像信息\",{\"1\":{\"420\":1}}],[\"包含数据增强\",{\"1\":{\"510\":1}}],[\"包含数据路径\",{\"1\":{\"382\":1}}],[\"包含数据集\",{\"1\":{\"382\":1}}],[\"包含约\",{\"1\":{\"341\":1,\"342\":1}}],[\"包含60亿参数的视觉编码器\",{\"1\":{\"296\":1}}],[\"包含离散\",{\"1\":{\"232\":1}}],[\"包含编码器\",{\"1\":{\"213\":1}}],[\"包含三个目标\",{\"1\":{\"198\":1}}],[\"包含三个关键步骤\",{\"1\":{\"95\":1}}],[\"包含两个文档片段和nsp损失\",{\"1\":{\"681\":1}}],[\"包含两个模块\",{\"1\":{\"173\":1}}],[\"包含两张分辨率较高的\",{\"1\":{\"285\":1}}],[\"包含两种适应性特征学习层\",{\"1\":{\"139\":1}}],[\"包含表面细节\",{\"1\":{\"159\":1}}],[\"包含\",{\"1\":{\"121\":1,\"224\":1,\"265\":1,\"266\":1,\"272\":1,\"420\":1,\"483\":1}}],[\"包含相对坐标和\",{\"1\":{\"119\":1}}],[\"包含了几何结构\",{\"1\":{\"83\":1}}],[\"包含了交互提示\",{\"1\":{\"83\":1}}],[\"包含7\",{\"1\":{\"73\":1}}],[\"包含jra模块\",{\"1\":{\"73\":1}}],[\"包含物体的坐标\",{\"1\":{\"32\":1}}],[\"包含15k交互图像和38k标注的3d物体实例\",{\"1\":{\"29\":1}}],[\"多不精确\",{\"1\":{\"951\":1}}],[\"多卷几层后\",{\"1\":{\"923\":1}}],[\"多分类交叉熵\",{\"1\":{\"910\":1}}],[\"多分辨率分组\",{\"0\":{\"142\":1},\"1\":{\"142\":1}}],[\"多种\",{\"1\":{\"892\":1}}],[\"多种可供性\",{\"1\":{\"49\":1}}],[\"多输入与多输出\",{\"0\":{\"800\":1}}],[\"多输入\",{\"1\":{\"799\":1}}],[\"多元正态分布\",{\"0\":{\"870\":1}}],[\"多元场景下也称为偏导数\",{\"1\":{\"775\":1}}],[\"多元高斯\",{\"1\":{\"574\":1}}],[\"多元高斯分布的行为可能会显得非常反直觉\",{\"1\":{\"872\":1}}],[\"多元高斯分布的概率密度函数定义如下\",{\"1\":{\"871\":1}}],[\"多元高斯分布被称为二维高斯分布\",{\"1\":{\"871\":1}}],[\"多元高斯分布\",{\"1\":{\"574\":1}}],[\"多项系数\",{\"1\":{\"857\":1}}],[\"多项选择题\",{\"1\":{\"737\":1}}],[\"多项选择任务是指给定一个问题和多个候选答案\",{\"1\":{\"737\":1}}],[\"多项选择任务\",{\"0\":{\"737\":1}}],[\"多项式项数\",{\"1\":{\"500\":1}}],[\"多项式\",{\"1\":{\"500\":2}}],[\"多项式阶数\",{\"1\":{\"500\":1}}],[\"多项式函数空间\",{\"1\":{\"500\":1}}],[\"多项式逼近通过增加阶数\",{\"1\":{\"500\":1}}],[\"多项式逼近需要\",{\"1\":{\"500\":1}}],[\"多项式逼近的基函数是固定的\",{\"1\":{\"500\":1}}],[\"多项式逼近\",{\"1\":{\"500\":2}}],[\"多领域竞争力\",{\"1\":{\"668\":1}}],[\"多群体条件对齐\",{\"1\":{\"658\":1}}],[\"多数投票69\",{\"1\":{\"668\":1}}],[\"多数比较数据仅有\",{\"1\":{\"658\":1}}],[\"多数据集\",{\"1\":{\"382\":1}}],[\"多条件限制\",{\"1\":{\"658\":1}}],[\"多步推理\",{\"1\":{\"649\":1}}],[\"多达千亿\",{\"1\":{\"600\":1}}],[\"多次折叠\",{\"1\":{\"500\":1}}],[\"多维索引中的省略维度\",{\"1\":{\"463\":1}}],[\"多轮交互\",{\"1\":{\"669\":1}}],[\"多轮对话能力\",{\"1\":{\"340\":1}}],[\"多轮对话\",{\"1\":{\"335\":1}}],[\"多轮数据集\",{\"1\":{\"183\":1}}],[\"多语言支持\",{\"1\":{\"824\":1}}],[\"多语言理解和创意生成方面有显著提升\",{\"1\":{\"823\":1}}],[\"多语言翻译能力显著提升\",{\"1\":{\"648\":1}}],[\"多语言能力\",{\"1\":{\"323\":1}}],[\"多语言版本的\",{\"1\":{\"225\":1}}],[\"多阶段训练策略\",{\"1\":{\"312\":1}}],[\"多阶段编码\",{\"1\":{\"94\":1}}],[\"多功能性\",{\"1\":{\"296\":1}}],[\"多线程加载数据\",{\"1\":{\"293\":1}}],[\"多任务理解\",{\"1\":{\"668\":1}}],[\"多任务与多语言学习的基础\",{\"1\":{\"650\":1}}],[\"多任务学习的概率视角\",{\"1\":{\"640\":1}}],[\"多任务学习的挑战与机遇\",{\"1\":{\"639\":1}}],[\"多任务学习\",{\"1\":{\"639\":1}}],[\"多任务\",{\"1\":{\"268\":1,\"382\":1}}],[\"多任务训练\",{\"1\":{\"220\":1}}],[\"多路\",{\"1\":{\"220\":1}}],[\"多视角裁剪增强\",{\"1\":{\"280\":1}}],[\"多视角图像\",{\"1\":{\"159\":1}}],[\"多视角\",{\"1\":{\"157\":2,\"285\":1}}],[\"多\",{\"1\":{\"143\":1,\"271\":1,\"293\":1,\"386\":1}}],[\"多个应用可以只依赖于一个或少数几个大模型进行统一建设\",{\"1\":{\"826\":1}}],[\"多个包的集合\",{\"1\":{\"810\":1}}],[\"多个模块的集合\",{\"1\":{\"810\":1}}],[\"多个函数可以连续调用\",{\"1\":{\"765\":1}}],[\"多个距离共享一个桶\",{\"1\":{\"710\":1}}],[\"多个变量\",{\"1\":{\"574\":1}}],[\"多个装饰器叠加时的执行顺序\",{\"0\":{\"458\":1}}],[\"多个装饰器嵌套时更容易出错\",{\"1\":{\"454\":1}}],[\"多个任务可以共享同一个冻结的图像编码器\",{\"1\":{\"273\":1}}],[\"多个合成描述\",{\"1\":{\"183\":1}}],[\"多个\",{\"1\":{\"143\":2,\"699\":1}}],[\"多个物体场景\",{\"1\":{\"49\":1}}],[\"多头潜在注意力\",{\"1\":{\"823\":1}}],[\"多头自注意力机制通过并行计算多个注意力头\",{\"1\":{\"741\":1}}],[\"多头自注意力机制\",{\"1\":{\"726\":1,\"741\":1}}],[\"多头自注意力计算流程图\",{\"1\":{\"724\":1,\"751\":1}}],[\"多头自注意力中每个head会根据分配给自己的这部分特征维度\",{\"1\":{\"582\":1}}],[\"多头自注意力\",{\"0\":{\"430\":1,\"751\":1}}],[\"多头自注意力层\",{\"1\":{\"429\":1}}],[\"多头注意力机制\",{\"1\":{\"380\":1}}],[\"多头注意力的头数\",{\"1\":{\"380\":2}}],[\"多头注意力\",{\"1\":{\"125\":1}}],[\"多头重组回原貌\",{\"1\":{\"119\":1}}],[\"多尺度生成器架构\",{\"1\":{\"884\":1}}],[\"多尺度建模能力\",{\"1\":{\"157\":1}}],[\"多尺度聚合机制\",{\"1\":{\"157\":1}}],[\"多尺度特征提取机制\",{\"1\":{\"141\":1}}],[\"多尺度特征融合\",{\"1\":{\"122\":1}}],[\"多尺度分组分类模型\",{\"0\":{\"141\":1}}],[\"多尺度分组\",{\"0\":{\"140\":1},\"1\":{\"139\":1,\"140\":1,\"141\":1}}],[\"多尺度上采样模块\",{\"1\":{\"70\":1}}],[\"多阈值下的\",{\"1\":{\"106\":1}}],[\"多阈值评估更稳定\",{\"1\":{\"106\":1}}],[\"多层感知机\",{\"1\":{\"97\":1}}],[\"多形状的功能区域的感知能力\",{\"1\":{\"95\":1}}],[\"多模态的概率分布\",{\"1\":{\"952\":1}}],[\"多模态支持\",{\"1\":{\"824\":1}}],[\"多模态融合\",{\"1\":{\"823\":1}}],[\"多模态能力\",{\"1\":{\"823\":1}}],[\"多模态因果自监督\",{\"1\":{\"420\":1}}],[\"多模态网络设计\",{\"1\":{\"415\":1}}],[\"多模态模型\",{\"1\":{\"823\":1}}],[\"多模态模型vit原理与图片分类实战演练\",{\"1\":{\"422\":1}}],[\"多模态模型在过往发展的过程中\",{\"1\":{\"415\":1}}],[\"多模态模型clip原理与图片分类\",{\"1\":{\"404\":1}}],[\"多模态生成任务中充当解码器\",{\"1\":{\"403\":1}}],[\"多模态时\",{\"1\":{\"398\":1}}],[\"多模态关键点\",{\"1\":{\"398\":1,\"399\":1}}],[\"多模态论文中常用的改编版本的bert代码实现记录\",{\"1\":{\"395\":1}}],[\"多模态常用改编bert代码实现\",{\"0\":{\"395\":1}}],[\"多模态编码\",{\"1\":{\"385\":1}}],[\"多模态情况\",{\"1\":{\"380\":1}}],[\"多模态专家混合\",{\"0\":{\"372\":1}}],[\"多模态\",{\"0\":{\"365\":1,\"396\":1},\"1\":{\"823\":1}}],[\"多模态聊天机器人\",{\"1\":{\"342\":1}}],[\"多模态对话等任务上的领先性能\",{\"1\":{\"313\":1}}],[\"多模态对话等任务上全面领先于现有开源模型\",{\"1\":{\"312\":1}}],[\"多模态对话\",{\"1\":{\"304\":1}}],[\"多模态推理\",{\"1\":{\"277\":1}}],[\"多模态交叉注意力\",{\"1\":{\"399\":1}}],[\"多模态交叉注意力模块\",{\"1\":{\"69\":1}}],[\"多模态交互核心\",{\"1\":{\"402\":1}}],[\"多模态交互\",{\"1\":{\"274\":1}}],[\"多模态解码器层\",{\"1\":{\"272\":1}}],[\"多模态理解\",{\"1\":{\"268\":1}}],[\"多模态部分\",{\"1\":{\"268\":1}}],[\"多模态预训练\",{\"0\":{\"251\":1}}],[\"多模态基础模型规模化发展的有前景方向\",{\"1\":{\"225\":1}}],[\"多模态数据\",{\"1\":{\"224\":1}}],[\"多模态数据的\",{\"1\":{\"67\":1}}],[\"多模态任务\",{\"1\":{\"220\":1}}],[\"多模态混合的编码器\",{\"1\":{\"171\":1}}],[\"多模态嵌入\",{\"1\":{\"67\":2}}],[\"多模态特征投影到语言语义空间\",{\"0\":{\"66\":1},\"1\":{\"64\":1}}],[\"多样化描述更有利于学习\",{\"1\":{\"165\":1}}],[\"多样化的几何变体\",{\"1\":{\"53\":1}}],[\"多样性未公开\",{\"1\":{\"669\":1}}],[\"多样性不足\",{\"1\":{\"658\":1}}],[\"多样性目标\",{\"1\":{\"640\":1}}],[\"多样性\",{\"1\":{\"30\":1}}],[\"多实例鲁棒性\",{\"1\":{\"49\":1}}],[\"多对多关系分析\",{\"1\":{\"43\":1}}],[\"含有编码器和解码器结构\",{\"1\":{\"944\":1}}],[\"含special\",{\"1\":{\"713\":1}}],[\"含非\",{\"1\":{\"557\":1}}],[\"含义\",{\"1\":{\"100\":2,\"460\":1,\"475\":1,\"529\":2,\"574\":1,\"709\":1,\"909\":1,\"924\":1}}],[\"含\",{\"1\":{\"42\":1,\"316\":1,\"317\":1}}],[\"每张图像包含成千上万个像素\",{\"1\":{\"942\":1}}],[\"每张图像生成一个序列\",{\"1\":{\"188\":1}}],[\"每份采用不同的随机掩码模式\",{\"1\":{\"681\":1}}],[\"每生成一个新\",{\"1\":{\"663\":1}}],[\"每生成一个\",{\"1\":{\"661\":1}}],[\"每\",{\"1\":{\"660\":1}}],[\"每种异排列对应的全排列数\",{\"1\":{\"882\":2}}],[\"每种异排列对应的重复排列数\",{\"1\":{\"881\":2}}],[\"每种方法在不同任务上都优于其它方法\",{\"1\":{\"626\":1}}],[\"每种物体可以对应多种形状实例和功能类别\",{\"1\":{\"92\":2}}],[\"每家公司都去从头训练一个自己的大模型\",{\"1\":{\"601\":1}}],[\"每封电子邮件的实际分类取决于您选择的阈值\",{\"1\":{\"570\":1}}],[\"每隔一列\",{\"1\":{\"544\":1}}],[\"每隔一行\",{\"1\":{\"544\":1}}],[\"每行的总和表示所有预测正例\",{\"1\":{\"561\":1}}],[\"每行复制\",{\"1\":{\"472\":1}}],[\"每行是一个\",{\"1\":{\"65\":1}}],[\"每轮推理时\",{\"1\":{\"663\":1}}],[\"每轮迭代更新\",{\"1\":{\"293\":1}}],[\"每轮只使用\",{\"1\":{\"176\":1}}],[\"每批次的图像数量\",{\"1\":{\"265\":1}}],[\"每层卷积的中间通道数\",{\"1\":{\"926\":1}}],[\"每层卷积的通道数\",{\"1\":{\"255\":1}}],[\"每层是一次下采样或上采样\",{\"1\":{\"899\":1}}],[\"每层下采样一次\",{\"1\":{\"892\":1}}],[\"每层对应一个\",{\"1\":{\"500\":1}}],[\"每层聚焦不同抽象层次\",{\"1\":{\"500\":1}}],[\"每层\",{\"1\":{\"500\":1}}],[\"每层学习一个转折点\",{\"1\":{\"500\":1}}],[\"每层的通道数\",{\"1\":{\"899\":1}}],[\"每层的\",{\"1\":{\"380\":1}}],[\"每层会用\",{\"1\":{\"274\":1}}],[\"每层逐步下采样\",{\"1\":{\"146\":1}}],[\"每次前向传播时将掩码乘到卷积核上\",{\"1\":{\"926\":1}}],[\"每次生成一张图片\",{\"1\":{\"925\":1}}],[\"每次生成一个遮挡块时\",{\"1\":{\"263\":1}}],[\"每次卷积后中心像素都会看漏一些信息\",{\"1\":{\"923\":1}}],[\"每次取出辈分最大的函数\",{\"1\":{\"805\":1}}],[\"每次输入模型时动态生成掩码\",{\"1\":{\"681\":1}}],[\"每次输入序列时生成新的掩码模式\",{\"1\":{\"681\":1}}],[\"每次输入时重新生成掩码\",{\"1\":{\"679\":1}}],[\"每次调用函数都会创建一个新的局部作用域\",{\"1\":{\"444\":1}}],[\"每次计算后返回本次可能需要缓存的key\",{\"1\":{\"420\":1}}],[\"每次可以获取一条样本数据\",{\"1\":{\"382\":1}}],[\"每次一个\",{\"1\":{\"356\":1}}],[\"每次都是使用新的编码器更新\",{\"1\":{\"353\":1}}],[\"每次迭代更新学习率和权重衰减\",{\"1\":{\"293\":1}}],[\"每次迭代中\",{\"1\":{\"90\":1}}],[\"每次遮挡块最多的\",{\"1\":{\"263\":1}}],[\"每次遮挡一个矩形块区域\",{\"1\":{\"263\":1}}],[\"每次遮挡图像中的一个\",{\"1\":{\"234\":1}}],[\"每一步构造\",{\"1\":{\"895\":1}}],[\"每一列\",{\"1\":{\"710\":1}}],[\"每一小块大小不同\",{\"1\":{\"501\":1}}],[\"每一行\",{\"1\":{\"710\":1}}],[\"每一行表示一张图像和所有文本之间的相似度\",{\"1\":{\"418\":1}}],[\"每一行对应一个向量\",{\"1\":{\"213\":1}}],[\"每一组在\",{\"1\":{\"293\":1}}],[\"每一层transformer层能带来9\",{\"1\":{\"635\":1}}],[\"每一层的非线性变换\",{\"1\":{\"500\":1}}],[\"每一层只需学习局部特征\",{\"1\":{\"500\":1}}],[\"每一层\",{\"1\":{\"500\":1}}],[\"每一层bertlayer产生的key\",{\"1\":{\"420\":1}}],[\"每一层都包含视觉专家与语言专家\",{\"1\":{\"224\":1}}],[\"每一层包含一个\",{\"1\":{\"222\":1}}],[\"每一层包含\",{\"1\":{\"123\":1}}],[\"每一个存储单元\",{\"1\":{\"540\":1}}],[\"每一个向量作为一个单独的输入\",{\"1\":{\"426\":1}}],[\"每一个patch作为一个token\",{\"1\":{\"426\":1}}],[\"每一个\",{\"1\":{\"222\":1}}],[\"每对图文样本只需通过一次视觉\",{\"1\":{\"172\":1}}],[\"每组共享注意力权重\",{\"1\":{\"125\":1}}],[\"每组2个通道\",{\"1\":{\"119\":1}}],[\"每点分类\",{\"1\":{\"116\":1}}],[\"每列是一个\",{\"1\":{\"65\":1,\"364\":1}}],[\"每个离散编码取到的概率是等同的\",{\"1\":{\"959\":1}}],[\"每个离散的数字都变成了一个特别的连续向量了\",{\"1\":{\"956\":1}}],[\"每个维度的含义或其之间的关系\",{\"1\":{\"944\":1}}],[\"每个维度上的\",{\"1\":{\"542\":1}}],[\"每个像素点是独立的伯努利分布\",{\"1\":{\"932\":1}}],[\"每个像素位置是一个\",{\"1\":{\"926\":1}}],[\"每个像素有\",{\"1\":{\"924\":1}}],[\"每个像素的值只能依赖于\",{\"1\":{\"924\":1}}],[\"每个像素的颜色取值只有\",{\"1\":{\"921\":1}}],[\"每个像素就已经看不到自己位置上的输入信息了\",{\"1\":{\"923\":1}}],[\"每个像素亮度相同\",{\"1\":{\"875\":1}}],[\"每个像素都被视为图像的基本单位\",{\"1\":{\"584\":1}}],[\"每个矩形代表了一个数据状态\",{\"1\":{\"831\":1}}],[\"每个椭圆形代表了\",{\"1\":{\"831\":1}}],[\"每个函数节点需要计算其导数\",{\"1\":{\"776\":1}}],[\"每个距离单独一个桶\",{\"1\":{\"710\":1}}],[\"每个距离有自己的偏置\",{\"1\":{\"710\":1}}],[\"每个桶每个头都有一个标量偏置\",{\"1\":{\"710\":1}}],[\"每个桶学一个标量偏置\",{\"1\":{\"710\":1}}],[\"每个元素是该相对位置对应的桶编号\",{\"1\":{\"710\":1}}],[\"每个元素是\",{\"1\":{\"710\":1}}],[\"每个元素都变成了\",{\"1\":{\"710\":1}}],[\"每个元素表示\",{\"1\":{\"710\":1}}],[\"每个元素表示对应样本被采样的概率权重\",{\"1\":{\"518\":1}}],[\"每个元素表示下采样后该批次点的结束位置\",{\"1\":{\"121\":1}}],[\"每个元素表示该批次点的结束位置\",{\"1\":{\"121\":1}}],[\"每个元素表示该批次点的结束索引\",{\"1\":{\"121\":1}}],[\"每个头64维\",{\"1\":{\"751\":1}}],[\"每个头有独立偏置\",{\"1\":{\"710\":1}}],[\"每个头的偏置可以不同\",{\"1\":{\"710\":1}}],[\"每个头的向量维度\",{\"1\":{\"709\":1}}],[\"每个头关注的区域或许不太一样\",{\"1\":{\"582\":1}}],[\"每个相对距离对应一个向量\",{\"1\":{\"709\":1}}],[\"每个位置被替换为最接近的\",{\"1\":{\"963\":1}}],[\"每个位置预留一个特殊\",{\"1\":{\"892\":1}}],[\"每个位置是\",{\"1\":{\"885\":1}}],[\"每个位置\",{\"1\":{\"707\":1}}],[\"每个位置的表征\",{\"1\":{\"898\":1}}],[\"每个位置的输出会通过一个独立的前馈神经网络进行进一步处理\",{\"1\":{\"741\":1}}],[\"每个位置的\",{\"1\":{\"256\":1,\"260\":1}}],[\"每个词汇还同时保留产生该词汇的原始文章\",{\"1\":{\"696\":1}}],[\"每个词的每个字符后都加上空格\",{\"1\":{\"595\":1}}],[\"每个句子在训练过程中会被看到\",{\"1\":{\"681\":1}}],[\"每个任务需要数千至数十万标注样本\",{\"1\":{\"646\":1}}],[\"每个这些序列用作者的模型独立处理后通过一个\",{\"1\":{\"631\":1}}],[\"每个实例有输入字符的序列构成\",{\"1\":{\"630\":1}}],[\"每个子层\",{\"1\":{\"741\":1}}],[\"每个子词出现次数加1\",{\"1\":{\"595\":1}}],[\"每个子词的出现次数\",{\"1\":{\"595\":1}}],[\"每个子步骤都由一个\",{\"1\":{\"52\":1}}],[\"每个变量自己的方差\",{\"1\":{\"574\":1}}],[\"每个列中的总和会显示所有真实正例\",{\"1\":{\"561\":1}}],[\"每个通道有\",{\"1\":{\"542\":1}}],[\"每个epoch采样样本数=总样本数\",{\"1\":{\"518\":1}}],[\"每个撒下去的小数点都用周围的4个整数点去\",{\"1\":{\"502\":2}}],[\"每个网格是\",{\"1\":{\"501\":1}}],[\"每个小块做\",{\"1\":{\"501\":1}}],[\"每个残差块还包含一条跨层的连接线\",{\"1\":{\"497\":1}}],[\"每个残差块包含两个3x3的卷积层\",{\"1\":{\"497\":1}}],[\"每个卷积层后面都跟着一个batch\",{\"1\":{\"497\":1}}],[\"每个卷积层后跟一个\",{\"1\":{\"154\":1}}],[\"每个图像由两个离散视觉token进行表示\",{\"1\":{\"892\":1}}],[\"每个图像块的尺寸\",{\"1\":{\"426\":1}}],[\"每个图像位置对应的所有点云点\",{\"1\":{\"65\":1}}],[\"每个patch是三通道的小图片\",{\"1\":{\"426\":1}}],[\"每个patch的高度和宽度\",{\"1\":{\"380\":1}}],[\"每个output\",{\"1\":{\"419\":1}}],[\"每个query\",{\"1\":{\"418\":1}}],[\"每个query通道维度做特征融合\",{\"1\":{\"100\":1}}],[\"每个模态单独使用重的transformer\",{\"1\":{\"390\":1}}],[\"每个进程都会调用\",{\"1\":{\"382\":1}}],[\"每个进程\",{\"1\":{\"381\":1}}],[\"每个注意力头的维度\",{\"1\":{\"380\":1}}],[\"每个特征\",{\"1\":{\"357\":1}}],[\"每个问题包含文本或图像上下文\",{\"1\":{\"342\":1}}],[\"每个教师视图监督除同索引学生视图外的所有学生视图\",{\"1\":{\"293\":1}}],[\"每个教师的全局视图监督所有学生视图\",{\"1\":{\"293\":1}}],[\"每个遮挡块最小\",{\"1\":{\"264\":1}}],[\"每个遮挡块最大\",{\"1\":{\"264\":1}}],[\"每个向量对应的码本索引\",{\"1\":{\"213\":1}}],[\"每个向量的维度\",{\"1\":{\"213\":2}}],[\"每个样本理论上都包含\",{\"1\":{\"951\":1}}],[\"每个样本都需要做一个\",{\"1\":{\"356\":1}}],[\"每个样本一个\",{\"1\":{\"266\":1}}],[\"每个样本对应的簇编号\",{\"1\":{\"213\":1}}],[\"每个样本为一个\",{\"1\":{\"42\":1}}],[\"每个格子表示是否有物体\",{\"1\":{\"159\":1}}],[\"每个原始点都有了一个新的特征向量\",{\"1\":{\"145\":1}}],[\"每个原始点都有了它最近的\",{\"1\":{\"145\":1}}],[\"每个原始点对应的\",{\"1\":{\"145\":1}}],[\"每个层是一个conv1d\",{\"1\":{\"145\":1}}],[\"每个结构中心点不变\",{\"1\":{\"141\":1}}],[\"每个关键点的多尺度特征表示\",{\"1\":{\"141\":1}}],[\"每个关键点对应的局部区域点和特征\",{\"1\":{\"137\":1}}],[\"每个尺度可以有不同的网络深度和宽度\",{\"1\":{\"141\":1}}],[\"每个尺度\",{\"1\":{\"140\":1}}],[\"每个半径定义了一个局部邻域的大小\",{\"1\":{\"140\":1}}],[\"每个邻域内采样的关键点数量\",{\"1\":{\"137\":2}}],[\"每个区域中点的数量𝐾和query的半径𝑟\",{\"1\":{\"135\":1}}],[\"每个查询点点局部特征\",{\"1\":{\"137\":1}}],[\"每个查询点对所有原始点的距离\",{\"1\":{\"137\":1}}],[\"每个查询点到最近邻点的平方距离\",{\"1\":{\"119\":1}}],[\"每个查询点的最近邻点索引\",{\"1\":{\"119\":1}}],[\"每个batch的查询点的结束索引\",{\"1\":{\"119\":2}}],[\"每个batch的点的结束索引\",{\"1\":{\"119\":2}}],[\"每个\",{\"1\":{\"100\":5,\"122\":2,\"123\":1,\"187\":1,\"212\":3,\"213\":2,\"224\":1,\"231\":1,\"233\":1,\"255\":1,\"263\":1,\"265\":1,\"266\":1,\"274\":1,\"286\":1,\"364\":1,\"372\":1,\"381\":1,\"384\":1,\"385\":1,\"398\":1,\"471\":1,\"475\":2,\"502\":4,\"524\":2,\"529\":1,\"531\":1,\"710\":2,\"733\":3,\"892\":1,\"900\":1}}],[\"每个文本token询问所有点key后\",{\"1\":{\"96\":1}}],[\"每个形状实例随机匹配一个与其功能类型一致的问题\",{\"1\":{\"90\":1}}],[\"每个物体类别可有多个形状实例\",{\"1\":{\"89\":1}}],[\"每个物体都以点云形式表示\",{\"1\":{\"86\":1}}],[\"每个点通常包含\",{\"1\":{\"159\":1}}],[\"每个点都代表一个阈值\",{\"1\":{\"572\":1}}],[\"每个点都需要全局上下文\",{\"1\":{\"156\":1}}],[\"每个点都能感知整个点云的全局信息\",{\"1\":{\"122\":1}}],[\"每个点有\",{\"1\":{\"145\":1,\"152\":1,\"159\":1}}],[\"每个点到当前所有已选中心点的最小距离\",{\"1\":{\"137\":1}}],[\"每个点输出\",{\"1\":{\"123\":1}}],[\"每个点是否属于当前问题描述的功能区域\",{\"1\":{\"88\":1}}],[\"每个点的分类结果\",{\"1\":{\"146\":1}}],[\"每个点的三个邻近点的权重\",{\"1\":{\"145\":1}}],[\"每个点的类别预测\",{\"1\":{\"123\":1}}],[\"每个点的邻居数量\",{\"1\":{\"120\":1}}],[\"每个点的值\",{\"1\":{\"100\":1}}],[\"每个点的\",{\"1\":{\"70\":1,\"83\":1}}],[\"每个点云约\",{\"1\":{\"91\":1}}],[\"每个点云点对应的所有图像位置\",{\"1\":{\"65\":1}}],[\"每个点云点与图像中每个位置之间的相似度得分\",{\"1\":{\"65\":1}}],[\"每个点云实例按可供性类别标注\",{\"1\":{\"42\":1}}],[\"每个点包括\",{\"1\":{\"42\":1}}],[\"每类\",{\"1\":{\"25\":1}}],[\"8个头\",{\"1\":{\"751\":1}}],[\"8×32gb\",{\"1\":{\"680\":1}}],[\"8×8\",{\"1\":{\"288\":1}}],[\"8回退到字节级处理\",{\"1\":{\"667\":1}}],[\"8的语言建模基准上达到sota水平\",{\"1\":{\"643\":1}}],[\"8的数据集上刷新了零样本sota\",{\"1\":{\"641\":1}}],[\"8降至8\",{\"1\":{\"641\":1}}],[\"8以后支持\",{\"1\":{\"474\":1}}],[\"8b\",{\"1\":{\"306\":1,\"823\":5}}],[\"8bit=true\",{\"1\":{\"52\":1}}],[\"88\",{\"1\":{\"268\":2,\"684\":1}}],[\"889\",{\"1\":{\"41\":1}}],[\"84\",{\"1\":{\"215\":1,\"683\":2}}],[\"8600000\",{\"1\":{\"432\":1}}],[\"86×1000000\",{\"1\":{\"432\":1}}],[\"86m\",{\"1\":{\"432\":1}}],[\"86\",{\"1\":{\"185\":1,\"268\":1}}],[\"85\",{\"1\":{\"157\":1,\"309\":1,\"641\":1,\"648\":1,\"657\":1,\"850\":1}}],[\"896×1344\",{\"1\":{\"331\":1}}],[\"89\",{\"1\":{\"117\":1,\"157\":1,\"641\":1,\"685\":2}}],[\"890\",{\"1\":{\"25\":1}}],[\"87\",{\"1\":{\"99\":1,\"106\":1}}],[\"870\",{\"1\":{\"87\":2,\"89\":1,\"91\":1}}],[\"82\",{\"1\":{\"99\":1,\"268\":1,\"308\":1,\"641\":1}}],[\"8064\",{\"1\":{\"811\":1}}],[\"80gb\",{\"1\":{\"667\":1}}],[\"80层\",{\"1\":{\"667\":1}}],[\"80k\",{\"1\":{\"316\":1}}],[\"800\",{\"1\":{\"236\":1,\"242\":2}}],[\"80\",{\"1\":{\"83\":5,\"208\":1,\"280\":1,\"293\":1,\"679\":1,\"691\":1,\"697\":2}}],[\"8\",{\"0\":{\"68\":1,\"93\":1},\"1\":{\"50\":1,\"58\":3,\"59\":6,\"64\":1,\"70\":2,\"83\":10,\"99\":1,\"104\":1,\"106\":2,\"117\":3,\"119\":9,\"122\":1,\"123\":2,\"141\":1,\"145\":1,\"146\":1,\"179\":1,\"206\":1,\"208\":1,\"280\":1,\"288\":1,\"292\":1,\"293\":1,\"303\":1,\"322\":1,\"384\":2,\"385\":2,\"441\":3,\"482\":2,\"502\":2,\"540\":2,\"541\":2,\"542\":1,\"544\":2,\"550\":1,\"589\":1,\"595\":5,\"596\":1,\"597\":7,\"613\":1,\"614\":2,\"634\":1,\"635\":1,\"648\":4,\"657\":1,\"658\":1,\"660\":1,\"668\":5,\"684\":1,\"685\":2,\"696\":3,\"697\":1,\"710\":2,\"751\":1,\"816\":1,\"823\":1,\"871\":1,\"878\":1,\"892\":1,\"893\":1,\"900\":2,\"918\":1,\"925\":1,\"926\":6,\"935\":7,\"946\":1,\"964\":3}}],[\"8192\",{\"1\":{\"236\":1,\"357\":1,\"885\":2,\"886\":1,\"887\":1}}],[\"81\",{\"1\":{\"47\":1,\"268\":1,\"668\":1,\"685\":1}}],[\"总的信息量\",{\"1\":{\"950\":1}}],[\"总的排列方式共有\",{\"1\":{\"881\":1}}],[\"总是\",{\"1\":{\"931\":1}}],[\"总是使用\",{\"1\":{\"493\":1}}],[\"总长度\",{\"1\":{\"895\":1}}],[\"总长度超过模型最大长度\",{\"1\":{\"893\":1}}],[\"总输入序列长度\",{\"1\":{\"892\":1}}],[\"总词表中的每个\",{\"1\":{\"892\":1}}],[\"总词表大小\",{\"1\":{\"892\":2}}],[\"总词汇量\",{\"1\":{\"697\":1}}],[\"总序列长度\",{\"1\":{\"892\":1}}],[\"总排列数\",{\"1\":{\"881\":2,\"882\":2}}],[\"总句对数量\",{\"1\":{\"701\":1}}],[\"总掩码的词数量\",{\"1\":{\"701\":1}}],[\"总规模1\",{\"1\":{\"667\":1}}],[\"总之\",{\"1\":{\"630\":1,\"635\":1,\"654\":1,\"827\":1}}],[\"总\",{\"1\":{\"385\":2}}],[\"总批量大小\",{\"1\":{\"315\":1,\"316\":1}}],[\"总视图数量\",{\"1\":{\"293\":1}}],[\"总数调整\",{\"1\":{\"892\":1}}],[\"总数据量达160gb\",{\"1\":{\"684\":1}}],[\"总数\",{\"1\":{\"263\":1,\"520\":1}}],[\"总数量\",{\"1\":{\"263\":1}}],[\"总数的\",{\"1\":{\"236\":1}}],[\"总共要\",{\"1\":{\"264\":1}}],[\"总共可用\",{\"1\":{\"263\":1}}],[\"总共训练\",{\"1\":{\"224\":1}}],[\"总共有\",{\"1\":{\"224\":1}}],[\"总共得到\",{\"1\":{\"87\":1}}],[\"总点数\",{\"1\":{\"121\":1}}],[\"总体的损失函数可以写成\",{\"1\":{\"960\":1}}],[\"总体而言\",{\"1\":{\"634\":1}}],[\"总体来看\",{\"1\":{\"413\":1}}],[\"总体\",{\"1\":{\"306\":1}}],[\"总体效果\",{\"1\":{\"110\":1}}],[\"总体数据统计\",{\"1\":{\"41\":1}}],[\"总损失为\",{\"1\":{\"963\":1}}],[\"总损失由三个部分组成\",{\"1\":{\"963\":1}}],[\"总损失\",{\"1\":{\"102\":1,\"213\":3,\"274\":1,\"293\":1,\"700\":1,\"899\":1}}],[\"总损失包含三项\",{\"1\":{\"79\":1}}],[\"总样本数\",{\"1\":{\"89\":1}}],[\"总结系统\",{\"1\":{\"658\":1}}],[\"总结并解释\",{\"1\":{\"657\":1}}],[\"总结一句话\",{\"1\":{\"346\":1}}],[\"总结一下这篇文章\",{\"1\":{\"346\":1}}],[\"总结文章\",{\"1\":{\"339\":1}}],[\"总结为一句话\",{\"1\":{\"262\":1}}],[\"总结表格\",{\"1\":{\"157\":1}}],[\"总结\",{\"0\":{\"93\":1,\"217\":1,\"225\":1,\"312\":1,\"436\":1,\"460\":1,\"494\":1,\"529\":1,\"579\":1,\"643\":1,\"672\":1,\"688\":1,\"961\":1},\"1\":{\"57\":1,\"135\":1,\"150\":1,\"215\":1,\"402\":1,\"463\":1,\"500\":1,\"574\":1,\"640\":1,\"647\":1,\"648\":1,\"649\":1,\"650\":1,\"657\":1,\"658\":1,\"667\":1,\"669\":1,\"670\":1,\"735\":1,\"806\":1,\"847\":1}}],[\"总训练轮次\",{\"1\":{\"46\":1}}],[\"总计超过\",{\"1\":{\"680\":1}}],[\"总计1\",{\"1\":{\"666\":1}}],[\"总计\",{\"1\":{\"25\":1,\"679\":1}}],[\"而必须保留空间布局\",{\"1\":{\"963\":1}}],[\"而反向传播时\",{\"1\":{\"959\":1}}],[\"而反向传播算法可以高效地求出每个变量的导数\",{\"1\":{\"767\":1}}],[\"而ae会编码出连续向量\",{\"1\":{\"956\":1}}],[\"而实际上\",{\"1\":{\"956\":1}}],[\"而随机噪声的图像概率较低\",{\"1\":{\"942\":1}}],[\"而如果一直使用\",{\"1\":{\"923\":1}}],[\"而那部分成本就是\",{\"1\":{\"909\":1}}],[\"而最可能的是两面都为正面\",{\"1\":{\"903\":1}}],[\"而最大的vit模型vit\",{\"1\":{\"407\":1}}],[\"而基础的\",{\"1\":{\"894\":1}}],[\"而基于对比学习的模型通常需要非常大的\",{\"1\":{\"223\":1}}],[\"而视觉质量没有显著下降\",{\"1\":{\"885\":1}}],[\"而视觉编码器通常仅约10亿参数\",{\"1\":{\"296\":1}}],[\"而低频结构才是使得物体在视觉上可被我们辨认的关键\",{\"1\":{\"885\":1}}],[\"而逆概率问题关注的则是\",{\"1\":{\"878\":1}}],[\"而没病的人也有误报\",{\"1\":{\"850\":1}}],[\"而没有实际重新排列内存中的数据\",{\"1\":{\"490\":1}}],[\"而没有建模点与点之间的局部几何关系\",{\"1\":{\"157\":1}}],[\"而可能是\",{\"1\":{\"847\":1}}],[\"而更需要掌握使用大模型的实践技巧\",{\"1\":{\"835\":1}}],[\"而每个子像素的概率仅由它之前的子像素决定\",{\"1\":{\"925\":1}}],[\"而每次反向传播时导数会累加\",{\"1\":{\"816\":1}}],[\"而每一层\",{\"1\":{\"663\":1}}],[\"而弱引用不会增加对象的引用计数\",{\"1\":{\"806\":1}}],[\"而弱引用可通过避免循环引用直接解决问题\",{\"1\":{\"806\":1}}],[\"而言可能产生显著的性能损耗\",{\"1\":{\"806\":1}}],[\"而链式法则的计算需要知晓每个函数在正向传播时的输入值和输出值\",{\"1\":{\"779\":1}}],[\"而tinypytorch的变量实现为variable类\",{\"1\":{\"755\":1}}],[\"而本系列旨在揭开这些技术和机制的神秘面纱\",{\"1\":{\"753\":1}}],[\"而注意力机制更关注的是文本中不同位置之间的动态语义关联\",{\"1\":{\"706\":1}}],[\"而针对这20\",{\"1\":{\"698\":1}}],[\"而要考虑它的上下文\",{\"1\":{\"691\":1}}],[\"而设计这些模型并测试其\",{\"1\":{\"690\":1}}],[\"而后续工作通过多任务微调\",{\"1\":{\"687\":1}}],[\"而roberta的目标是通过系统性地复现\",{\"1\":{\"687\":1}}],[\"而roberta改为每次输入时动态生成掩码\",{\"1\":{\"683\":1}}],[\"而roberta通过系统性的调整\",{\"1\":{\"681\":1}}],[\"而原始\",{\"1\":{\"680\":1}}],[\"而hoffmann推荐的10b模型仅训练200b\",{\"1\":{\"666\":1}}],[\"而llama\",{\"1\":{\"665\":1}}],[\"而common\",{\"1\":{\"647\":1}}],[\"而现有模型难以实现类似能力\",{\"1\":{\"646\":1}}],[\"而现代模型\",{\"1\":{\"646\":1}}],[\"而通用系统需要能够根据任务描述动态调整行为\",{\"1\":{\"640\":1}}],[\"而通过引入特定的归纳偏置\",{\"1\":{\"422\":1}}],[\"而无需显式指定输出分布的结构\",{\"1\":{\"952\":1}}],[\"而无需显式监督\",{\"1\":{\"640\":1,\"642\":1}}],[\"而无需事先见过具体示例\",{\"1\":{\"825\":1}}],[\"而无需额外的训练或参数更新\",{\"1\":{\"825\":1}}],[\"而无需调用特定函数\",{\"1\":{\"809\":1}}],[\"而无需调整主干编码器\",{\"1\":{\"273\":1}}],[\"而无需考虑pad词的全局上下文信息是否需要进行计算\",{\"1\":{\"703\":1}}],[\"而无需依赖专有数据\",{\"1\":{\"665\":1}}],[\"而无需明确的监督信号\",{\"1\":{\"643\":1}}],[\"而无需参数调整或架构修改\",{\"1\":{\"639\":1}}],[\"而机器学习系统通常需要数百至数千个任务示例才能实现良好的泛化\",{\"1\":{\"639\":1}}],[\"而特定任务学习的标注数据有非常少\",{\"1\":{\"625\":1}}],[\"而给的问题却是难度大很多的问题\",{\"1\":{\"622\":1}}],[\"而一些领域差距比较大的任务可能需要更大的\",{\"1\":{\"613\":1}}],[\"而模型的输入输出维度不变\",{\"1\":{\"611\":1}}],[\"而用了qlora之后\",{\"1\":{\"607\":1}}],[\"而过度参数化的大模型背后\",{\"1\":{\"606\":1}}],[\"而prefix\",{\"1\":{\"605\":1}}],[\"而这个恒等式又来自于概率的乘法法则\",{\"1\":{\"877\":1}}],[\"而这些函数都可以同样好地最大化\",{\"1\":{\"949\":1}}],[\"而这些属性往往是相关联的\",{\"1\":{\"944\":1}}],[\"而这些\",{\"1\":{\"846\":1}}],[\"而这些内容可以影响x生成期望中y的概率\",{\"1\":{\"604\":1}}],[\"而这里我们将会反转这个逻辑\",{\"1\":{\"411\":1}}],[\"而当需要评估样本概率时\",{\"1\":{\"947\":1}}],[\"而当\",{\"1\":{\"590\":1}}],[\"而假负例\",{\"1\":{\"572\":1}}],[\"而降低阈值则会产生相反的效果\",{\"1\":{\"565\":1}}],[\"而物理转置中\",{\"1\":{\"545\":1}}],[\"而我们检测到一个\",{\"1\":{\"501\":1}}],[\"而我们发现注意力池化在实际应用中更具实用性\",{\"1\":{\"273\":1}}],[\"而深层网络只需多项式数量\",{\"1\":{\"500\":1}}],[\"而单层网络需要\",{\"1\":{\"500\":1}}],[\"而神经网络的全连接层只能接受固定大小的输入\",{\"1\":{\"501\":1}}],[\"而神经网络的函数复合\",{\"1\":{\"500\":1}}],[\"而神经网络的基函数\",{\"1\":{\"500\":1}}],[\"而神经网络\",{\"1\":{\"500\":1}}],[\"而神经网络通过非线性激活和分层结构\",{\"1\":{\"500\":1}}],[\"而多项式需极高阶数才能近似突变\",{\"1\":{\"500\":1}}],[\"而多模态大型语言模型\",{\"1\":{\"323\":1}}],[\"而预训练过程中使用的输入图像尺寸通常固定为\",{\"1\":{\"425\":1}}],[\"而vit将其放到前面\",{\"1\":{\"429\":1}}],[\"而vit\",{\"1\":{\"422\":1}}],[\"而vit则选择了三种不同尺寸的模型\",{\"1\":{\"407\":1}}],[\"而图像编码器\",{\"1\":{\"407\":1,\"408\":1}}],[\"而图像编码器则用于提取图像的特征\",{\"1\":{\"407\":1}}],[\"而剩余的个文本\",{\"1\":{\"407\":1}}],[\"而dual\",{\"1\":{\"391\":1}}],[\"而开源模型主要依赖英语数据\",{\"1\":{\"323\":1}}],[\"而开源模型多采用固定分辨率\",{\"1\":{\"323\":1}}],[\"而开源模型的视觉基础模型\",{\"1\":{\"323\":1}}],[\"而文本编码器\",{\"1\":{\"306\":1}}],[\"而参数数量相同时\",{\"1\":{\"304\":1}}],[\"而且任何一个词都有可能是被替换掉的\",{\"1\":{\"691\":1}}],[\"而且能适应不同大小数据集\",{\"1\":{\"634\":1}}],[\"而且下游任务表现出相似甚至更好的性能\",{\"1\":{\"394\":1}}],[\"而且不需要先单独预训练视觉编码器\",{\"1\":{\"276\":1}}],[\"而且动量队列可能重复包含同一个样本\",{\"1\":{\"190\":1}}],[\"而仅通过不同的\",{\"1\":{\"273\":1}}],[\"而标准的图文编码\",{\"1\":{\"272\":1}}],[\"而对于并行输出所有概率分布的\",{\"1\":{\"921\":1}}],[\"而对于较短的句子\",{\"1\":{\"700\":1}}],[\"而对于\",{\"1\":{\"611\":1}}],[\"而对于对比学习目标\",{\"1\":{\"272\":1}}],[\"而对比学习任务则依赖无条件的文本表示\",{\"1\":{\"272\":1}}],[\"而非随机噪声\",{\"1\":{\"943\":1}}],[\"而非低频结构上\",{\"1\":{\"885\":1}}],[\"而非实时回收循环引用对象\",{\"1\":{\"806\":1}}],[\"而非生成答案\",{\"1\":{\"735\":1}}],[\"而非本质的\",{\"1\":{\"658\":1}}],[\"而非验证\",{\"1\":{\"656\":1}}],[\"而非具备深层理解与推理能力的系统\",{\"1\":{\"649\":1}}],[\"而非真正理解语言和任务的系统\",{\"1\":{\"649\":1}}],[\"而非通用的多任务处理者\",{\"1\":{\"639\":1}}],[\"而非物理复制数据\",{\"1\":{\"546\":1}}],[\"而非物理\",{\"1\":{\"520\":1}}],[\"而非\",{\"1\":{\"420\":1,\"654\":1,\"658\":1,\"886\":1}}],[\"而非像素或对比目标\",{\"1\":{\"248\":1}}],[\"而非像素值\",{\"1\":{\"228\":1}}],[\"而非以\",{\"1\":{\"242\":1}}],[\"而非高层语义\",{\"1\":{\"228\":1}}],[\"而非直接回归像素值\",{\"1\":{\"227\":1}}],[\"而非训练时间变长\",{\"1\":{\"181\":1}}],[\"而之前的视觉\",{\"1\":{\"223\":1}}],[\"而共享的自注意力模块则负责学习不同模态之间的对齐\",{\"1\":{\"222\":1}}],[\"而语言模型中的掩码词都是高层语义\",{\"1\":{\"210\":1}}],[\"而在实践中我们发现使用更大的\",{\"1\":{\"885\":1}}],[\"而在本阶段\",{\"1\":{\"799\":1}}],[\"而在物理转置中\",{\"1\":{\"545\":1}}],[\"而在返回的内部函数中\",{\"1\":{\"449\":1}}],[\"而在后半部分\",{\"1\":{\"272\":1}}],[\"而在掩码语言建模\",{\"1\":{\"202\":1}}],[\"而在\",{\"1\":{\"190\":1,\"283\":1}}],[\"而自动爬取的网页图文对\",{\"1\":{\"173\":1}}],[\"而嵌入层\",{\"1\":{\"171\":1}}],[\"而点云是无序集合\",{\"1\":{\"160\":1}}],[\"而点云本质上就是带位置属性的点集合\",{\"1\":{\"110\":1}}],[\"而点云本质上就是集合结构\",{\"1\":{\"109\":1}}],[\"而不支持随机图像生成\",{\"1\":{\"956\":1}}],[\"而不善于处理离散的输入\",{\"1\":{\"956\":1}}],[\"而不认识其他的向量\",{\"1\":{\"956\":1}}],[\"而不像早期方法那样依赖\",{\"1\":{\"944\":1}}],[\"而不必担心底层的基础设施和运维工作\",{\"1\":{\"834\":1}}],[\"而不能看到未来的词\",{\"1\":{\"741\":1}}],[\"而不能为\",{\"1\":{\"595\":1}}],[\"而不会说这个人性别是0\",{\"1\":{\"956\":1}}],[\"而不会影响\",{\"1\":{\"951\":1}}],[\"而不会因为约束太严而训练困难\",{\"1\":{\"924\":1}}],[\"而不会将精力聚焦在优化模型本身上\",{\"1\":{\"835\":1}}],[\"而不会考虑模型分类\",{\"1\":{\"561\":1}}],[\"而不会释放\",{\"1\":{\"448\":1}}],[\"而不需要依赖专有或不可访问的数据集\",{\"1\":{\"823\":1}}],[\"而不需要重新计算所有之前\",{\"1\":{\"663\":1}}],[\"而不需要显式复制数据\",{\"1\":{\"546\":1}}],[\"而不需要人为地去标注这种标签信息\",{\"1\":{\"349\":1}}],[\"而不需要改变整个模型参数\",{\"1\":{\"346\":1}}],[\"而不仅仅是单一标签\",{\"1\":{\"280\":1}}],[\"而不仅仅依赖单一实例\",{\"1\":{\"53\":1}}],[\"而不依赖图像像素\",{\"1\":{\"217\":1}}],[\"而不是非得用\",{\"1\":{\"925\":1}}],[\"而不是单方面去看某一个人的位置\",{\"1\":{\"915\":1}}],[\"而不是图像\",{\"1\":{\"898\":1}}],[\"而不是简单地选最大值\",{\"1\":{\"897\":1}}],[\"而不是密度最大的中心点\",{\"1\":{\"875\":1}}],[\"而不是重新生成一个新的答案\",{\"1\":{\"735\":1}}],[\"而不是进行真正意义上的概念抽象和泛化\",{\"1\":{\"649\":1}}],[\"而不是原始\",{\"1\":{\"633\":1}}],[\"而不是原始像素\",{\"1\":{\"232\":1}}],[\"而不是逐点分类\",{\"1\":{\"586\":1}}],[\"而不是真正地重新排列数据\",{\"1\":{\"545\":1}}],[\"而不是全局去重\",{\"1\":{\"480\":1}}],[\"而不是只靠\",{\"1\":{\"385\":1}}],[\"而不是对抗干扰样本的能力\",{\"1\":{\"382\":1}}],[\"而不是表面相似性\",{\"1\":{\"293\":1}}],[\"而不是\",{\"1\":{\"216\":1,\"482\":1,\"502\":1,\"586\":1}}],[\"而不是元组\",{\"1\":{\"123\":1}}],[\"而是输出了多个\",{\"1\":{\"958\":1}}],[\"而是一个ae\",{\"1\":{\"956\":1}}],[\"而是想在一个由同一个人写成的数字串中添加数字\",{\"1\":{\"952\":1}}],[\"而是优化采样方式\",{\"1\":{\"944\":1}}],[\"而是将大模型作为一个调用工具\",{\"1\":{\"835\":1}}],[\"而是真正朝着\",{\"1\":{\"814\":1}}],[\"而是真实泛化能力\",{\"1\":{\"641\":1}}],[\"而是对输入文本中每个\",{\"1\":{\"735\":1}}],[\"而是把相对距离压缩到若干个桶\",{\"1\":{\"710\":1}}],[\"而是把距离映射到少量\",{\"1\":{\"710\":1}}],[\"而是还要和\",{\"1\":{\"709\":1}}],[\"而是学习出来的\",{\"1\":{\"692\":1}}],[\"而是学会根据用户指令理解任务意图并生成合适的结果\",{\"1\":{\"339\":1}}],[\"而是训练策略和数据规模的优化\",{\"1\":{\"688\":1}}],[\"而是在网络的每一层添加了位置编码\",{\"1\":{\"823\":1}}],[\"而是在\",{\"1\":{\"681\":1}}],[\"而是在通道维度上做注意力\",{\"1\":{\"119\":1}}],[\"而是特定群体与目标下的实用性对齐\",{\"1\":{\"658\":1}}],[\"而是列举可能性并犹豫\",{\"1\":{\"657\":1}}],[\"而是有机融合并推升了这些已有成果\",{\"1\":{\"650\":1}}],[\"而是按质量设权重采样\",{\"1\":{\"647\":1}}],[\"而是组合学到的知识\",{\"1\":{\"641\":1}}],[\"而是利用llm结果的多样性\",{\"1\":{\"621\":1}}],[\"而是多条\",{\"1\":{\"620\":1}}],[\"而是关注预测和\",{\"1\":{\"587\":1}}],[\"而是综合周围村子的情况加权得出\",{\"1\":{\"502\":1}}],[\"而是直接覆盖梯度值\",{\"1\":{\"803\":1}}],[\"而是直接让模型在训练过程中优化位置信息\",{\"1\":{\"707\":1}}],[\"而是直接存在于函数作用域里\",{\"1\":{\"444\":1}}],[\"而是直接作为\",{\"1\":{\"283\":1}}],[\"而是由调用方\",{\"1\":{\"380\":1}}],[\"而是由学生网络迭代生成\",{\"1\":{\"285\":1}}],[\"而是采用缓慢更新呢\",{\"1\":{\"356\":1}}],[\"而是采用了块状遮挡\",{\"1\":{\"234\":1}}],[\"而是100多万个类别\",{\"1\":{\"350\":1}}],[\"而是从教师模型中重建语义知识\",{\"1\":{\"216\":1}}],[\"而是\",{\"1\":{\"192\":1,\"657\":1,\"658\":1}}],[\"而是能够捕获多个尺度上的局部特征\",{\"1\":{\"140\":1}}],[\"而是向量形式\",{\"1\":{\"119\":1}}],[\"而是通过单一语言建模目标\",{\"1\":{\"650\":1}}],[\"而是通过以下方式创建了一个新的\",{\"1\":{\"544\":1}}],[\"而是通过\",{\"1\":{\"65\":1,\"97\":1,\"190\":1}}],[\"而是通过监督点级热图\",{\"1\":{\"39\":1}}],[\"而向量注意力达到\",{\"1\":{\"117\":1}}],[\"而先前方法通常忽略\",{\"1\":{\"110\":1}}],[\"而\",{\"1\":{\"102\":1,\"107\":1,\"178\":1,\"207\":1,\"234\":1,\"264\":1,\"269\":2,\"272\":1,\"306\":1,\"385\":1,\"405\":1,\"448\":1,\"589\":1,\"600\":1,\"612\":1,\"657\":1,\"658\":1,\"679\":2,\"690\":1,\"694\":3,\"822\":1,\"925\":1,\"931\":1,\"945\":1,\"946\":1}}],[\"而下面这行代码实现的是一个\",{\"1\":{\"100\":1}}],[\"而几何结构文本则是单一连贯的几何描述\",{\"1\":{\"55\":1}}],[\"而great通过几何\",{\"1\":{\"31\":1}}],[\"激活参数\",{\"1\":{\"823\":1}}],[\"激活检查点\",{\"1\":{\"667\":1}}],[\"激活模型的相关能力\",{\"1\":{\"640\":1}}],[\"激活后\",{\"1\":{\"551\":1}}],[\"激活环境的命令\",{\"1\":{\"551\":1}}],[\"激活已有知识\",{\"1\":{\"346\":1}}],[\"激活函数层\",{\"1\":{\"429\":1}}],[\"激活函数类型\",{\"1\":{\"380\":1}}],[\"激活函数\",{\"1\":{\"120\":1,\"143\":1,\"213\":2,\"429\":1,\"431\":2,\"590\":1,\"633\":1,\"667\":1,\"699\":2,\"718\":2,\"823\":2}}],[\"激活概率\",{\"1\":{\"83\":1}}],[\"激活交互主体所在区域\",{\"1\":{\"83\":1}}],[\"激活\",{\"0\":{\"551\":1},\"1\":{\"39\":1,\"120\":1,\"122\":1,\"403\":1,\"823\":1}}],[\"卷积将通道数映射为\",{\"1\":{\"926\":1}}],[\"卷积层总数\",{\"1\":{\"926\":1}}],[\"卷积核大小\",{\"1\":{\"926\":1}}],[\"卷积核的空间大小\",{\"1\":{\"924\":1}}],[\"卷积核个数为768的卷积层来进行实现\",{\"1\":{\"426\":1}}],[\"卷积初始化等\",{\"1\":{\"521\":1}}],[\"卷积后剩余的长和宽相乘作为时间维度\",{\"1\":{\"426\":1}}],[\"卷积映射\",{\"1\":{\"380\":1}}],[\"卷积\",{\"1\":{\"38\":1,\"97\":1,\"152\":1,\"886\":1,\"899\":1}}],[\"跨度预测和自回归预训练\",{\"1\":{\"687\":1}}],[\"跨领域泛化能力\",{\"1\":{\"641\":1}}],[\"跨gpu部分实现\",{\"1\":{\"190\":1}}],[\"跨注意力融合公式为\",{\"1\":{\"38\":1}}],[\"跨模态大模型\",{\"1\":{\"823\":1}}],[\"跨模态对齐\",{\"1\":{\"385\":1}}],[\"跨模态输出\",{\"1\":{\"385\":1}}],[\"跨模态\",{\"1\":{\"380\":2,\"385\":8}}],[\"跨模态互助\",{\"1\":{\"377\":1}}],[\"跨模态检索\",{\"1\":{\"220\":1,\"268\":1}}],[\"跨模态注意力\",{\"1\":{\"399\":1}}],[\"跨模态注意力机制\",{\"1\":{\"100\":1}}],[\"跨模态注意力矩阵\",{\"1\":{\"65\":1}}],[\"跨模态相似性匹配\",{\"1\":{\"83\":1}}],[\"跨模态融合层\",{\"1\":{\"385\":1}}],[\"跨模态融合\",{\"1\":{\"30\":1,\"268\":1}}],[\"跨模态一致性对齐\",{\"0\":{\"23\":1},\"1\":{\"21\":1}}],[\"具备基本能力的\",{\"1\":{\"836\":1}}],[\"具备对文本\",{\"1\":{\"823\":1}}],[\"具备跨模态交互能力\",{\"1\":{\"342\":1}}],[\"具备极强的感知能力\",{\"1\":{\"312\":1}}],[\"具备较强的泛化能力和鲁棒性\",{\"1\":{\"19\":1}}],[\"具有任意高的表达能力\",{\"1\":{\"949\":1}}],[\"具有如下优点\",{\"1\":{\"942\":1}}],[\"具有出色的能力\",{\"1\":{\"824\":1}}],[\"具有较强的泛化能力\",{\"1\":{\"435\":1}}],[\"具有较强的实用性与拓展性\",{\"1\":{\"174\":1}}],[\"具有很强的zero\",{\"1\":{\"415\":1}}],[\"具有高度多样性和挑战性\",{\"1\":{\"342\":1}}],[\"具有以下优势\",{\"1\":{\"102\":1}}],[\"具有语义意义\",{\"1\":{\"86\":1}}],[\"具体代码实现过程如下\",{\"1\":{\"894\":1}}],[\"具体代码实现如下所示\",{\"1\":{\"384\":1}}],[\"具体代码实现如下\",{\"1\":{\"213\":1,\"380\":1,\"411\":1,\"895\":1}}],[\"具体函数实现\",{\"1\":{\"810\":1}}],[\"具体函数的反向传播\",{\"0\":{\"780\":1}}],[\"具体函数继承该类并实现forward方法\",{\"1\":{\"761\":1}}],[\"具体位置在\",{\"1\":{\"712\":1}}],[\"具体核心代码实现如下\",{\"1\":{\"700\":1}}],[\"具体步骤为\",{\"1\":{\"698\":1}}],[\"具体流程我们可以看下面这幅图\",{\"1\":{\"694\":1}}],[\"具体流程如下\",{\"1\":{\"119\":1}}],[\"具体体现在\",{\"1\":{\"663\":1}}],[\"具体任务表现分析\",{\"1\":{\"641\":1}}],[\"具体要考虑解决的问题类型\",{\"1\":{\"620\":1}}],[\"具体来看\",{\"1\":{\"611\":1}}],[\"具体来说就是这个队列可以很大\",{\"1\":{\"353\":1}}],[\"具体来说\",{\"1\":{\"83\":1,\"140\":1,\"232\":1,\"272\":1,\"273\":1,\"428\":1,\"604\":1,\"605\":1,\"831\":1,\"892\":1,\"923\":1}}],[\"具体示例如下所示\",{\"1\":{\"545\":1}}],[\"具体的流程如下图所示\",{\"1\":{\"891\":1}}],[\"具体的\",{\"1\":{\"544\":1}}],[\"具体的实验结果可以参考clip公开的notebook\",{\"1\":{\"409\":1}}],[\"具体可以从以下几个方面理解\",{\"1\":{\"500\":1}}],[\"具体可参考\",{\"1\":{\"354\":1}}],[\"具体为vit\",{\"1\":{\"435\":1}}],[\"具体解释如下\",{\"1\":{\"428\":1}}],[\"具体方法包括\",{\"1\":{\"653\":1}}],[\"具体方法是\",{\"1\":{\"243\":1}}],[\"具体方式可以是直接缩放\",{\"1\":{\"425\":1}}],[\"具体使用的是\",{\"1\":{\"410\":1}}],[\"具体形式如下\",{\"1\":{\"382\":1}}],[\"具体实现\",{\"1\":{\"382\":1}}],[\"具体实现包含以下几个关键步骤\",{\"1\":{\"119\":1}}],[\"具体包括\",{\"1\":{\"342\":1}}],[\"具体分为三步\",{\"1\":{\"339\":1}}],[\"具体如下\",{\"1\":{\"234\":1,\"848\":1}}],[\"具体过程如下图所示\",{\"1\":{\"699\":2}}],[\"具体过程如下\",{\"1\":{\"213\":1}}],[\"具体而言\",{\"1\":{\"142\":1,\"227\":1,\"238\":1}}],[\"具体做法是\",{\"1\":{\"946\":1}}],[\"具体做法是为\",{\"1\":{\"214\":1}}],[\"具体做法如下\",{\"1\":{\"893\":1}}],[\"具体做法\",{\"1\":{\"135\":1}}],[\"具体选择多少个中心点以及邻域内的数量由超参数确定\",{\"1\":{\"134\":1}}],[\"具体地如下图所示\",{\"1\":{\"692\":1}}],[\"具体地描述出来\",{\"1\":{\"618\":1}}],[\"具体地\",{\"1\":{\"38\":1,\"235\":1,\"239\":1,\"903\":1,\"950\":1}}],[\"融合了所有可能的数字和各种可能的书写风格\",{\"1\":{\"952\":1}}],[\"融合了\",{\"1\":{\"823\":1}}],[\"融合后的\",{\"1\":{\"385\":2}}],[\"融合后的文本特征\",{\"1\":{\"98\":1}}],[\"融合模块\",{\"1\":{\"415\":2}}],[\"融合模式\",{\"1\":{\"385\":2}}],[\"融合模型\",{\"1\":{\"222\":1}}],[\"融合输入\",{\"1\":{\"385\":1}}],[\"融合并统一了以上三类范式\",{\"1\":{\"268\":1}}],[\"融合图像和文本\",{\"1\":{\"268\":1}}],[\"融合过程在多模态编码器的每一层中通过跨模态注意力\",{\"1\":{\"197\":1}}],[\"融合两者优势\",{\"1\":{\"195\":1}}],[\"融合特征\",{\"1\":{\"192\":1}}],[\"融合+增强\",{\"1\":{\"145\":1}}],[\"融合深层特征和浅层特征\",{\"1\":{\"122\":1}}],[\"融合编码器架构\",{\"1\":{\"368\":1}}],[\"融合编码器\",{\"1\":{\"122\":2,\"220\":1,\"369\":1,\"377\":1}}],[\"融合来自编码器的跳跃连接特征\",{\"1\":{\"116\":1}}],[\"融合所有\",{\"1\":{\"100\":1}}],[\"融合到\",{\"1\":{\"83\":1}}],[\"融合交互主体和环境的语义线索\",{\"1\":{\"83\":1}}],[\"融合生成功能表征\",{\"1\":{\"78\":1}}],[\"融合不同来源的信息\",{\"1\":{\"69\":1}}],[\"融合通道信息\",{\"1\":{\"69\":1}}],[\"融合\",{\"1\":{\"64\":1,\"83\":1,\"122\":1,\"123\":6,\"385\":3}}],[\"融合语言与视觉特征\",{\"1\":{\"64\":1}}],[\"融合多尺度特征\",{\"1\":{\"122\":1}}],[\"融合多尺度视觉特征和文本特征\",{\"1\":{\"23\":1}}],[\"融合多个\",{\"1\":{\"100\":1}}],[\"融合多模态空间特征\",{\"0\":{\"65\":1},\"1\":{\"64\":1}}],[\"融合表示为\",{\"1\":{\"38\":1}}],[\"融合至\",{\"1\":{\"38\":1}}],[\"描述了\",{\"1\":{\"950\":1}}],[\"描述的神经网络结构类似图4左侧所示的网络\",{\"1\":{\"946\":1}}],[\"描述在给定\",{\"1\":{\"878\":1}}],[\"描述这个新视图的维度大小\",{\"1\":{\"544\":1}}],[\"描述输入和输出维度的对应关系\",{\"1\":{\"478\":1}}],[\"描述短小的问题\",{\"1\":{\"368\":1}}],[\"描述生成任务要求模型优化条件概率\",{\"1\":{\"272\":1}}],[\"描述损失\",{\"1\":{\"268\":1}}],[\"描述器与过滤器的组合能显著提升性能\",{\"1\":{\"165\":1}}],[\"描述器\",{\"1\":{\"165\":1}}],[\"描述功能的文本提示\",{\"1\":{\"107\":1}}],[\"描述\",{\"1\":{\"87\":1,\"106\":4,\"157\":1,\"160\":1,\"586\":1,\"587\":1,\"588\":1,\"592\":1}}],[\"描述图像中人与物体的交互方式\",{\"1\":{\"36\":1}}],[\"描述图像中人与物体之间的完整交互过程\",{\"1\":{\"36\":1}}],[\"描述实际交互\",{\"1\":{\"30\":1}}],[\"类卷积了\",{\"1\":{\"923\":1}}],[\"类卷积也是不行的\",{\"1\":{\"923\":1}}],[\"类卷积\",{\"1\":{\"923\":4}}],[\"类掩码卷积后\",{\"1\":{\"923\":1}}],[\"类掩码卷积呢\",{\"1\":{\"923\":1}}],[\"类掩码卷积\",{\"1\":{\"923\":5}}],[\"类和\",{\"1\":{\"923\":1}}],[\"类变量\",{\"1\":{\"857\":1}}],[\"类将\",{\"1\":{\"805\":1}}],[\"类提供一个新的方法\",{\"1\":{\"802\":1}}],[\"类以支持\",{\"1\":{\"800\":1}}],[\"类gpt式生成式模型进行推理的过程\",{\"1\":{\"661\":1,\"662\":1}}],[\"类装饰器通常通过实现\",{\"1\":{\"459\":1}}],[\"类装饰器\",{\"0\":{\"459\":1}}],[\"类方法\",{\"0\":{\"455\":1}}],[\"类体代码在独立的命名空间里执行\",{\"1\":{\"444\":1}}],[\"类定义\",{\"1\":{\"444\":1,\"810\":1}}],[\"类型必须是\",{\"1\":{\"926\":1}}],[\"类型应用前景的思考\",{\"1\":{\"827\":1}}],[\"类型签名会失效\",{\"1\":{\"454\":1}}],[\"类型检查\",{\"1\":{\"454\":1}}],[\"类型\",{\"1\":{\"384\":1,\"385\":1,\"390\":4,\"460\":1,\"462\":1,\"735\":1,\"892\":1,\"926\":2}}],[\"类型的索引\",{\"1\":{\"384\":1}}],[\"类型报错\",{\"1\":{\"264\":1}}],[\"类用于完成多数据源加载的任务\",{\"1\":{\"382\":1}}],[\"类没有直接对外提供现成的\",{\"1\":{\"380\":1}}],[\"类额外提供了\",{\"1\":{\"380\":1}}],[\"类负责完成通用模版流程的抽取\",{\"1\":{\"382\":1}}],[\"类负责完成前向传播流程的组织\",{\"1\":{\"380\":1}}],[\"类负责完成\",{\"1\":{\"380\":1}}],[\"类负责存储和更新\",{\"1\":{\"213\":1}}],[\"类来将输入图像批次列表按照分辨率进行分组\",{\"1\":{\"293\":1}}],[\"类实现\",{\"1\":{\"293\":1}}],[\"类实现了k近邻查询算法\",{\"1\":{\"119\":1}}],[\"类实现了point\",{\"1\":{\"119\":1}}],[\"类的唯一区别在于卷积核的中心像素是否产生贡献\",{\"1\":{\"923\":1}}],[\"类的样本数\",{\"1\":{\"514\":1}}],[\"类的比例与原始近似一致\",{\"1\":{\"513\":1}}],[\"类的实现\",{\"1\":{\"293\":1}}],[\"类的概率\",{\"1\":{\"257\":1}}],[\"类的\",{\"1\":{\"255\":1,\"256\":1,\"805\":1}}],[\"类模型中广泛使用\",{\"1\":{\"234\":1}}],[\"类中提供的方法\",{\"1\":{\"474\":1}}],[\"类中\",{\"1\":{\"152\":1}}],[\"类得分\",{\"1\":{\"123\":1}}],[\"类是\",{\"1\":{\"120\":1}}],[\"类似物理学中的相变现象\",{\"1\":{\"825\":1}}],[\"类似地\",{\"1\":{\"809\":2}}],[\"类似预激活残差网络\",{\"1\":{\"640\":1}}],[\"类似数据库查询\",{\"1\":{\"533\":1}}],[\"类似模式匹配\",{\"1\":{\"478\":1}}],[\"类似gpt\",{\"1\":{\"323\":1}}],[\"类似自然语言\",{\"1\":{\"232\":1}}],[\"类似\",{\"1\":{\"123\":1,\"305\":1,\"317\":1,\"405\":1}}],[\"类似多头注意力\",{\"1\":{\"119\":1}}],[\"类似于多项式逼近中的基函数组合\",{\"1\":{\"500\":1}}],[\"类似于图像领域的\",{\"1\":{\"250\":1}}],[\"类似于图像分类的训练流程\",{\"1\":{\"239\":1}}],[\"类似于在cnn中权重共享的概念\",{\"1\":{\"131\":1}}],[\"类似于cnn中的上采样\",{\"1\":{\"122\":1}}],[\"类似于cnn中的上采样层\",{\"1\":{\"122\":1}}],[\"类似于cnn中的下采样层\",{\"1\":{\"121\":1}}],[\"类似于传统的全连接层或\",{\"1\":{\"97\":1}}],[\"类似于\",{\"1\":{\"97\":1,\"214\":1,\"355\":1,\"480\":1,\"823\":1}}],[\"类似的两阶段训练流程\",{\"1\":{\"235\":1}}],[\"类似的\",{\"1\":{\"11\":1}}],[\"类\",{\"1\":{\"89\":2,\"91\":2,\"382\":2,\"426\":1,\"799\":1,\"857\":1,\"926\":1}}],[\"类比\",{\"1\":{\"52\":1}}],[\"类比额外交互\",{\"1\":{\"52\":1}}],[\"类比推理\",{\"0\":{\"36\":1}}],[\"类别维\",{\"1\":{\"899\":1}}],[\"类别平衡权重\",{\"1\":{\"589\":1}}],[\"类别样本越少\",{\"1\":{\"518\":1}}],[\"类别权重\",{\"1\":{\"514\":2,\"590\":1,\"592\":1}}],[\"类别数量不平衡\",{\"1\":{\"589\":1}}],[\"类别数量少\",{\"1\":{\"518\":1}}],[\"类别数\",{\"1\":{\"514\":1}}],[\"类别不平衡越严重\",{\"1\":{\"589\":1}}],[\"类别不平衡的问题\",{\"1\":{\"589\":1}}],[\"类别不平衡\",{\"1\":{\"514\":1,\"586\":1}}],[\"类别\",{\"1\":{\"346\":3,\"380\":1,\"514\":3,\"926\":1}}],[\"类别标签\",{\"1\":{\"268\":2}}],[\"类别概率的计算方式如下\",{\"1\":{\"238\":1}}],[\"类别预测\",{\"1\":{\"83\":1}}],[\"类别下图像与点云的数量比例\",{\"1\":{\"43\":1}}],[\"类别间存在明显的多对多关系\",{\"1\":{\"43\":1}}],[\"类平衡分析\",{\"1\":{\"43\":1}}],[\"从编码器输出\",{\"1\":{\"959\":1}}],[\"从概率分布里随机采样一个类别\",{\"1\":{\"958\":1}}],[\"从信息论的角度看\",{\"1\":{\"951\":1}}],[\"从公式\",{\"1\":{\"951\":1}}],[\"从公式来说\",{\"1\":{\"353\":1}}],[\"从手写文本生成更多手写内容\",{\"1\":{\"942\":1}}],[\"从三维植物模型生成更多植被以填充游戏场景\",{\"1\":{\"942\":1}}],[\"从标准正态分布中采样\",{\"1\":{\"935\":1,\"947\":1}}],[\"从左上到右下\",{\"1\":{\"924\":1}}],[\"从起始长度到\",{\"1\":{\"895\":1}}],[\"从2015年起\",{\"1\":{\"884\":1}}],[\"从中可以计算区间概率\",{\"1\":{\"847\":1}}],[\"从样本数据出发\",{\"1\":{\"877\":1}}],[\"从样本空间\",{\"1\":{\"846\":1}}],[\"从样本中随机选取\",{\"1\":{\"213\":2}}],[\"从deepseek\",{\"1\":{\"837\":1}}],[\"从构建一个\",{\"1\":{\"836\":1}}],[\"从实际业务需求出发构造小批量验证集\",{\"1\":{\"835\":1}}],[\"从实验结果来看\",{\"1\":{\"352\":1}}],[\"从一个模块流向另一个模块\",{\"1\":{\"831\":1}}],[\"从一张输入图像生成多个视角\",{\"1\":{\"293\":1}}],[\"从简单函数组合进化到模块化神经网络\",{\"1\":{\"819\":1}}],[\"从输出信息可看到\",{\"1\":{\"816\":1}}],[\"从输出变量出发遍历所有节点\",{\"1\":{\"815\":1}}],[\"从输入点云\",{\"1\":{\"137\":1}}],[\"从输入点中选取一组点\",{\"1\":{\"133\":1}}],[\"从自动微分迈向可训练的神经网络模型\",{\"0\":{\"819\":1}}],[\"从自动微分迈向通用框架\",{\"0\":{\"799\":1}}],[\"从自动微分走向\",{\"0\":{\"814\":1}}],[\"从弱引用中获取variable实例\",{\"1\":{\"806\":1}}],[\"从递归改为循环\",{\"1\":{\"797\":1}}],[\"从递归到循环\",{\"0\":{\"785\":1}}],[\"从零直达wgan\",{\"1\":{\"919\":1}}],[\"从零生成完整图像\",{\"1\":{\"895\":1}}],[\"从零构建深度学习框架\",{\"0\":{\"752\":1,\"798\":1,\"813\":1,\"818\":1}}],[\"从零开始训练一个多模态\",{\"1\":{\"269\":1}}],[\"从源张量中提取特定位置的元素\",{\"1\":{\"700\":1}}],[\"从bertencoders编码输出结果中提取出被掩码的位置对应的嵌入向量\",{\"1\":{\"699\":1}}],[\"从4开始\",{\"1\":{\"697\":1}}],[\"从统计语言模型\",{\"1\":{\"671\":1}}],[\"从词向量到上下文表示的发展历程\",{\"1\":{\"650\":1}}],[\"从早期的词向量\",{\"1\":{\"639\":1}}],[\"从早期的alexnet和cnn架构\",{\"1\":{\"298\":1}}],[\"从预训练到微调迁移学习过程中\",{\"1\":{\"635\":1}}],[\"从预测结果中找出每个样本预测概率最大的类别索引\",{\"1\":{\"431\":1}}],[\"从无标注文本中充分利用词级别以外的信息是有挑战性的\",{\"1\":{\"626\":1}}],[\"从上图中\",{\"1\":{\"622\":1}}],[\"从易至难技术\",{\"0\":{\"622\":1}}],[\"从结果集合中投票选择\",{\"1\":{\"621\":1}}],[\"从这个意义上来说\",{\"1\":{\"951\":1}}],[\"从这个视角出发\",{\"1\":{\"951\":1}}],[\"从这个分布中采样出一个\",{\"1\":{\"947\":1}}],[\"从这个数据集中\",{\"1\":{\"350\":1}}],[\"从这些公理可以推导出一些常用结论\",{\"1\":{\"848\":1}}],[\"从这可以看出要全参数微调大语言模型\",{\"1\":{\"611\":1}}],[\"从个人使用情况来说\",{\"1\":{\"610\":1}}],[\"从成本和效果的角度综合考虑\",{\"1\":{\"603\":1}}],[\"从训练数据的来源\",{\"1\":{\"602\":1}}],[\"从参数规模的角度\",{\"1\":{\"602\":1}}],[\"从字符级别开始\",{\"1\":{\"594\":1}}],[\"从shape列表的最右边往左遍历\",{\"1\":{\"547\":1}}],[\"从对应的\",{\"1\":{\"531\":1}}],[\"从不同大小的\",{\"1\":{\"501\":1}}],[\"从内到外\",{\"0\":{\"458\":1}}],[\"从qkv张量中分离出查询\",{\"1\":{\"430\":1}}],[\"从数据库中检索相关信息\",{\"1\":{\"829\":1}}],[\"从数据集\",{\"1\":{\"424\":1}}],[\"从数据对的数量来看\",{\"1\":{\"407\":1}}],[\"从冻结的llm引到vision\",{\"1\":{\"416\":1}}],[\"从冻结的image\",{\"1\":{\"416\":1}}],[\"从任务难度来看\",{\"1\":{\"413\":1}}],[\"从候选分类文本集合中取出其分类名词\",{\"1\":{\"410\":1}}],[\"从指定层开始\",{\"1\":{\"385\":1}}],[\"从第44步到第57步\",{\"1\":{\"819\":1}}],[\"从第二个位置开始\",{\"1\":{\"582\":1}}],[\"从第二个开始\",{\"1\":{\"420\":1}}],[\"从第一个数据模块读取通用配置\",{\"1\":{\"382\":1}}],[\"从第几层开始使用\",{\"1\":{\"380\":1}}],[\"从第几层开始启用\",{\"1\":{\"192\":1}}],[\"从哪一层开始\",{\"1\":{\"380\":1}}],[\"从所有\",{\"1\":{\"373\":1}}],[\"从所有点中选出每个通道的最大响应值\",{\"1\":{\"152\":1,\"154\":1}}],[\"从多个角度描述图像内容\",{\"1\":{\"341\":1}}],[\"从多个角度渲染点云或\",{\"1\":{\"159\":1}}],[\"从35种预设宽高比中选择最接近输入图像的配置\",{\"1\":{\"331\":1}}],[\"从1\",{\"1\":{\"325\":1}}],[\"从粗粒度对齐过渡到细粒度优化\",{\"1\":{\"305\":1}}],[\"从本章开始为每个chapter设计如下目录结构\",{\"1\":{\"810\":1}}],[\"从本节开始\",{\"1\":{\"383\":1,\"695\":1,\"899\":1}}],[\"从本节开始我们将对官方开源的\",{\"1\":{\"293\":1}}],[\"从本质上来说\",{\"1\":{\"131\":1}}],[\"从同一张图片中生成多个视角\",{\"1\":{\"285\":1}}],[\"从视频中均匀采样\",{\"1\":{\"273\":1}}],[\"从视频中抽取多个帧\",{\"1\":{\"273\":1}}],[\"从变分自编码器的视角\",{\"0\":{\"235\":1}}],[\"从像素级提升到语义级\",{\"1\":{\"216\":1}}],[\"从教师模型\",{\"1\":{\"213\":1}}],[\"从教师模型获取先验知识的过程\",{\"1\":{\"213\":1}}],[\"从教师模型获取目标特征\",{\"1\":{\"213\":1}}],[\"从稀疏点恢复到原始点密度\",{\"1\":{\"146\":1}}],[\"从下采样点中取出每个原始点对应的最近邻点的特征\",{\"1\":{\"145\":1}}],[\"从最稀疏的点开始\",{\"1\":{\"143\":1,\"144\":1}}],[\"从点云中选出有代表性的点作为中心点\",{\"1\":{\"143\":1}}],[\"从点云中根据索引提取特定点\",{\"1\":{\"137\":1}}],[\"从点云中找出最相关的功能区域\",{\"1\":{\"100\":1}}],[\"从原始点云中选出\",{\"1\":{\"137\":1}}],[\"从原始点云中选择代表性点\",{\"1\":{\"121\":1}}],[\"从目标主体区域特征和背景特征中提取相关信息分别单独加到自己身上\",{\"1\":{\"83\":1}}],[\"从文件路径中提取物体名\",{\"1\":{\"82\":1}}],[\"从图中可看出逐渐接近星号所指的目的地位置\",{\"1\":{\"816\":1}}],[\"从图像数据库合成新的未见图像\",{\"1\":{\"942\":1}}],[\"从图像的中心位置裁剪出\",{\"1\":{\"425\":1}}],[\"从图像\",{\"1\":{\"75\":1}}],[\"从图片路径中截取得到物体名\",{\"1\":{\"53\":1}}],[\"从索引区间中随机采样pair\",{\"1\":{\"53\":1}}],[\"从\",{\"0\":{\"689\":1,\"695\":1,\"956\":1},\"1\":{\"37\":1,\"48\":1,\"70\":1,\"87\":1,\"92\":1,\"116\":1,\"137\":1,\"152\":1,\"192\":1,\"286\":1,\"293\":1,\"357\":1,\"380\":1,\"385\":3,\"417\":2,\"544\":1,\"546\":1,\"633\":1,\"640\":1,\"654\":1,\"699\":1,\"709\":2,\"735\":1,\"823\":1,\"831\":1,\"896\":1,\"899\":1,\"924\":1,\"947\":1,\"950\":1,\"958\":1,\"964\":1}}],[\"从几何结构解释该部位可以交互的原因\",{\"1\":{\"35\":1}}],[\"从而生成新的\",{\"1\":{\"964\":1}}],[\"从而影响现有的机器学习算法\",{\"1\":{\"949\":1}}],[\"从而\",{\"1\":{\"874\":1}}],[\"从而不断迭代优化\",{\"1\":{\"836\":1}}],[\"从而将传统的模型训练调优转变成了更简单\",{\"1\":{\"835\":1}}],[\"从而简化应用程序的开发流程\",{\"1\":{\"831\":1}}],[\"从而显著提升了回答的准确性与深度\",{\"1\":{\"828\":1}}],[\"从而显著提升了模型的性能\",{\"1\":{\"823\":1}}],[\"从而得出最终答案\",{\"1\":{\"825\":1}}],[\"从而得到\",{\"1\":{\"945\":1}}],[\"从而得到最终的\",{\"1\":{\"656\":1}}],[\"从而得到一个相似度权重矩阵\",{\"1\":{\"582\":1}}],[\"从而得到另外两张图\",{\"1\":{\"350\":1}}],[\"从而无法被自动回收\",{\"1\":{\"806\":1}}],[\"从而无缝整合了不同监督\",{\"1\":{\"268\":1}}],[\"从而增强模型的表达能力\",{\"1\":{\"741\":1}}],[\"从而增强了模型的表达能力\",{\"1\":{\"429\":1}}],[\"从而很难并行\",{\"1\":{\"740\":1}}],[\"从而赋予\",{\"1\":{\"707\":1}}],[\"从而弥补其原生结构中缺少位置感知能力的缺陷\",{\"1\":{\"706\":1}}],[\"从而弥补自身在稀疏点云上的不足\",{\"1\":{\"26\":1}}],[\"从而计算交叉熵损失就很简单了\",{\"1\":{\"700\":1}}],[\"从而模型训练学习到每个词的含义需要更大量的数据集且最终效果也不会很好\",{\"1\":{\"697\":1}}],[\"从而帮助社区更好地理解不同改进的相对贡献\",{\"1\":{\"687\":1}}],[\"从而发现数据内部的逻辑与联系\",{\"1\":{\"614\":1}}],[\"从而应用于自己的业务场景\",{\"1\":{\"610\":1}}],[\"从而引导模型学习更多语义信息\",{\"1\":{\"589\":1}}],[\"从而导致偏差\",{\"1\":{\"578\":1}}],[\"从而导致更大的训练代价\",{\"1\":{\"415\":1}}],[\"从而扩展张量的形状\",{\"1\":{\"471\":1}}],[\"从而完成图像分类任务\",{\"1\":{\"427\":1}}],[\"从而缓解类别不平衡问题\",{\"1\":{\"518\":1}}],[\"从而缓解了灾难性的遗忘问题\",{\"1\":{\"421\":1}}],[\"从而缓解对大规模比较的需求\",{\"1\":{\"282\":1}}],[\"从而训练模型学会细粒度跨模态对齐\",{\"1\":{\"386\":1}}],[\"从而训练网络在面对实际应用中可能遇到的各种采样密度时\",{\"1\":{\"141\":1}}],[\"从而改善训练稳定性\",{\"1\":{\"380\":1}}],[\"从而学会生成图像的离散表示\",{\"1\":{\"891\":1}}],[\"从而学会根据输入数据的变化自适应地加权不同尺度上检测到的模式\",{\"1\":{\"140\":1}}],[\"从而学习更泛化的表示\",{\"1\":{\"368\":1}}],[\"从而能够训练更大的模型\",{\"1\":{\"614\":1}}],[\"从而能够保持队列中的特征尽可能的\",{\"1\":{\"356\":1}}],[\"从而能够更好地捕捉点云的局部结构和层次信息\",{\"1\":{\"157\":1}}],[\"从而让特征的获取过程保持一致性\",{\"1\":{\"353\":1}}],[\"从而让字典的规模变得很大\",{\"1\":{\"352\":1}}],[\"从而让中间学习到的字典特征尽可能保持一致\",{\"1\":{\"351\":1}}],[\"从而可以解决长距离依赖的问题\",{\"1\":{\"740\":1}}],[\"从而可以提供一些监督信号给到模型去训练\",{\"1\":{\"349\":1}}],[\"从而可以在网络结构上直接与已有方法进行对比\",{\"1\":{\"233\":1}}],[\"从而可以在图像\",{\"1\":{\"225\":1}}],[\"从而更贴近人类期望\",{\"1\":{\"339\":1}}],[\"从而为用户提供更加流畅的体验\",{\"1\":{\"833\":1}}],[\"从而为学生提供更高质量的特征目标\",{\"1\":{\"285\":1}}],[\"从而为后续的动态卷积和掩码预测提供基础\",{\"1\":{\"100\":1}}],[\"从而解决分类\",{\"1\":{\"269\":1}}],[\"从而自然地融合了不同来源的监督信号\",{\"1\":{\"268\":1}}],[\"从而形成可供\",{\"1\":{\"266\":1}}],[\"从而形成更完整的语言上下文理解\",{\"1\":{\"100\":1}}],[\"从而充分利用整个\",{\"1\":{\"262\":1}}],[\"从而避免一个函数的backward方法被错误地多次调用\",{\"1\":{\"805\":1}}],[\"从而避免多阶段训练\",{\"1\":{\"269\":1}}],[\"从而避免编码器只用很少几个\",{\"1\":{\"260\":1}}],[\"从而避免了\",{\"1\":{\"259\":1}}],[\"从而在各种nlp\",{\"1\":{\"824\":1}}],[\"从而在注意力机制中就不会考虑到这些pad部分的token了\",{\"1\":{\"703\":1}}],[\"从而在同一架构下联合优化\",{\"1\":{\"272\":1}}],[\"从而在规模化上进一步受益\",{\"1\":{\"220\":1}}],[\"从而在实际应用中实现更快\",{\"1\":{\"50\":1}}],[\"从而构建一个高度紧凑的语义码本\",{\"1\":{\"216\":1}}],[\"从而实现图文联合生成\",{\"1\":{\"895\":1}}],[\"从而实现图像自监督预训练任务中的\",{\"1\":{\"266\":1}}],[\"从而实现实验可重复性\",{\"1\":{\"521\":1}}],[\"从而实现对图像的细粒度理解和分析\",{\"1\":{\"584\":1}}],[\"从而实现对原函数行为的增强或修改\",{\"1\":{\"449\":1}}],[\"从而实现对局部图像信息的建模\",{\"1\":{\"212\":1}}],[\"从而实现高效的视觉\",{\"1\":{\"377\":1}}],[\"从而实现\",{\"1\":{\"285\":1}}],[\"从而实现模型压缩\",{\"1\":{\"283\":1}}],[\"从而实现强大的图像表征学习能力\",{\"1\":{\"252\":1}}],[\"从而实现点云的分层特征学习\",{\"1\":{\"137\":1}}],[\"从而使对比学习更加稳健\",{\"1\":{\"190\":1}}],[\"从而考虑负样本中的潜在正样本\",{\"1\":{\"172\":1}}],[\"从而保证变换是刚性的\",{\"1\":{\"152\":1}}],[\"从而插值得到该点的特征\",{\"1\":{\"145\":1}}],[\"从而减少对显式监督的依赖\",{\"1\":{\"639\":1}}],[\"从而减少对分类正确样本的贡献\",{\"1\":{\"589\":1}}],[\"从而减少对人工标注的依赖\",{\"1\":{\"20\":1}}],[\"从而减少参数量\",{\"1\":{\"125\":1}}],[\"从而减少计算复杂度并保持性能\",{\"1\":{\"97\":1}}],[\"从而提高了其理解和生成文本的能力\",{\"1\":{\"823\":1}}],[\"从而提高模型的分类性能\",{\"1\":{\"431\":1}}],[\"从而提升了在专业领域内的问题回答质量和深度\",{\"1\":{\"828\":1}}],[\"从而提升了用户对生成内容的信任度\",{\"1\":{\"828\":1}}],[\"从而提升对目标函数的拟合精度\",{\"1\":{\"500\":1}}],[\"从而提升分类任务的表现\",{\"1\":{\"376\":1}}],[\"从而提升视觉语言预训练效果\",{\"1\":{\"376\":1}}],[\"从而提升其在各种任务上的泛化能力\",{\"1\":{\"339\":1}}],[\"从而提升零样本图像分类性能\",{\"1\":{\"269\":1}}],[\"从而提升遮挡\",{\"1\":{\"215\":1}}],[\"从而提升\",{\"1\":{\"214\":1}}],[\"从而提升泛化能力和鲁棒性\",{\"1\":{\"207\":1}}],[\"从而提升鲁棒性和泛化能力\",{\"1\":{\"202\":1}}],[\"从而提升模型对多尺度\",{\"1\":{\"95\":1}}],[\"从而提供一个标准化的评测平台\",{\"1\":{\"19\":1}}],[\"从而获得对语言深层次的理解\",{\"1\":{\"822\":1}}],[\"从而获得平滑的过渡效果\",{\"1\":{\"505\":1}}],[\"从而获得\",{\"1\":{\"52\":1}}],[\"从而建立一致的\",{\"1\":{\"19\":1}}],[\"进入堆叠gpt2block模块前向传播流程\",{\"1\":{\"663\":1}}],[\"进入pointnet++经典的特征传播阶段\",{\"1\":{\"59\":1}}],[\"进而求出\",{\"1\":{\"904\":1}}],[\"进而可以自由构建\",{\"1\":{\"832\":1}}],[\"进而导致大模型的输出质量打折口\",{\"1\":{\"601\":1}}],[\"进而更新自身表示\",{\"1\":{\"286\":1}}],[\"进来\",{\"1\":{\"356\":1}}],[\"进一步增强了模型性能\",{\"1\":{\"823\":1}}],[\"进一步增强特征\",{\"1\":{\"146\":1}}],[\"进一步强化变量的\",{\"1\":{\"808\":1}}],[\"进一步提升了样本质量\",{\"1\":{\"884\":1}}],[\"进一步提升了性能\",{\"1\":{\"687\":1}}],[\"进一步提取和融合特征\",{\"1\":{\"145\":1}}],[\"进一步提取各自模态内部的语义一致性与结构关系\",{\"1\":{\"65\":1}}],[\"进一步降低了2\",{\"1\":{\"641\":1}}],[\"进一步来说\",{\"1\":{\"626\":1}}],[\"进一步帮llm明确要求\",{\"1\":{\"618\":1}}],[\"进一步减少冗余\",{\"1\":{\"500\":1}}],[\"进一步地\",{\"1\":{\"408\":1}}],[\"进一步推动了模型的下游表现\",{\"1\":{\"388\":1}}],[\"进一步对齐\",{\"1\":{\"385\":1}}],[\"进一步对齐视觉与语言特征\",{\"1\":{\"305\":1}}],[\"进一步训练模型理解和执行更复杂的视觉指令任务\",{\"1\":{\"340\":1}}],[\"进一步微调模型\",{\"1\":{\"339\":1}}],[\"进一步扩展了\",{\"1\":{\"823\":1}}],[\"进一步扩展了这个方法来预测n\",{\"1\":{\"413\":1}}],[\"进一步扩展了文本与视觉信息的交互能力\",{\"1\":{\"323\":1}}],[\"进一步扩大语料规模\",{\"1\":{\"183\":1}}],[\"进一步引入了视觉定位能力\",{\"1\":{\"300\":1}}],[\"进一步分析\",{\"1\":{\"289\":1}}],[\"进一步发展\",{\"1\":{\"283\":1}}],[\"进一步改进重建质量与稳定性\",{\"1\":{\"249\":1}}],[\"进一步加剧性能下降\",{\"1\":{\"242\":1}}],[\"进一步处理后\",{\"1\":{\"83\":1}}],[\"进一步从几何结构角度推理为什么该部位适合交互\",{\"1\":{\"35\":1}}],[\"进行某种形式的优化\",{\"1\":{\"948\":1}}],[\"进行严重近似\",{\"1\":{\"942\":1}}],[\"进行矩阵乘法\",{\"1\":{\"899\":1}}],[\"进行随机采样\",{\"1\":{\"895\":1}}],[\"进行评估\",{\"1\":{\"886\":1}}],[\"进行边际化\",{\"1\":{\"877\":1}}],[\"进行个性化定制\",{\"1\":{\"836\":1}}],[\"进行自然的语音和视频交流\",{\"1\":{\"823\":1}}],[\"进行解压\",{\"1\":{\"712\":1}}],[\"进行二分类任务\",{\"1\":{\"699\":1}}],[\"进行预测即可\",{\"1\":{\"694\":1}}],[\"进行预训练\",{\"1\":{\"198\":1,\"269\":1,\"417\":1}}],[\"进行经典的多头自注意力运算\",{\"1\":{\"663\":1}}],[\"进行划分\",{\"1\":{\"656\":1}}],[\"进行的进一步优化\",{\"1\":{\"621\":1}}],[\"进行思考\",{\"1\":{\"619\":1}}],[\"进行一次降维再升维的操作\",{\"1\":{\"611\":1}}],[\"进行全量的训练\",{\"1\":{\"602\":1}}],[\"进行高亮即可\",{\"1\":{\"582\":1}}],[\"进行逐元素运算\",{\"1\":{\"546\":1}}],[\"进行逐级上采样\",{\"1\":{\"70\":1}}],[\"进行相加操作\",{\"1\":{\"546\":1}}],[\"进行重新排列\",{\"1\":{\"545\":1}}],[\"进行灵活\",{\"1\":{\"478\":1}}],[\"进行局部特征提取\",{\"1\":{\"434\":1}}],[\"进行反向传播\",{\"1\":{\"431\":1}}],[\"进行并行输入\",{\"1\":{\"426\":1}}],[\"进行文本编码\",{\"1\":{\"420\":1}}],[\"进行聚合\",{\"1\":{\"382\":1}}],[\"进行单模态预训练\",{\"1\":{\"377\":1}}],[\"进行编码\",{\"1\":{\"376\":1,\"417\":2}}],[\"进行缓慢更新的代码实现如下所示\",{\"1\":{\"363\":1}}],[\"进行测试\",{\"1\":{\"352\":1}}],[\"进行了进一步的量化\",{\"1\":{\"607\":1}}],[\"进行了一次随机抽样\",{\"1\":{\"355\":1}}],[\"进行了以下筛选\",{\"1\":{\"341\":1}}],[\"进行了专门预训练\",{\"1\":{\"214\":1}}],[\"进行渐进式对齐\",{\"1\":{\"313\":1}}],[\"进行对比\",{\"1\":{\"311\":1,\"376\":1}}],[\"进行数据增强\",{\"1\":{\"293\":1}}],[\"进行交互\",{\"1\":{\"274\":1}}],[\"进行采样来估计\",{\"1\":{\"947\":1}}],[\"进行采样\",{\"1\":{\"256\":1,\"257\":1}}],[\"进行缩放\",{\"1\":{\"236\":1}}],[\"进行\",{\"1\":{\"236\":1,\"274\":1,\"544\":1,\"694\":1}}],[\"进行训练\",{\"1\":{\"223\":1,\"268\":1,\"609\":1,\"884\":1}}],[\"进行训练和评估\",{\"1\":{\"185\":1}}],[\"进行更新\",{\"1\":{\"213\":1}}],[\"进行替换\",{\"1\":{\"208\":1}}],[\"进行线性变换\",{\"1\":{\"206\":1}}],[\"进行线性映射形成\",{\"1\":{\"38\":1}}],[\"进行蒸馏监督\",{\"1\":{\"202\":1}}],[\"进行归一化\",{\"1\":{\"190\":1,\"431\":1}}],[\"进行分词\",{\"1\":{\"223\":1}}],[\"进行分词并转为\",{\"1\":{\"188\":1}}],[\"进行分类\",{\"1\":{\"157\":1,\"736\":1}}],[\"进行增强\",{\"1\":{\"185\":1}}],[\"进行特征插值和上采样\",{\"1\":{\"146\":1}}],[\"进行特征学习\",{\"1\":{\"131\":1}}],[\"进行多尺度特征提取和下采样\",{\"1\":{\"146\":1}}],[\"进行填充\",{\"1\":{\"119\":1}}],[\"进行消息传递或图卷积\",{\"1\":{\"110\":1}}],[\"进行推理的代码\",{\"1\":{\"663\":1}}],[\"进行推理\",{\"1\":{\"107\":1}}],[\"进行内积操作\",{\"1\":{\"100\":1}}],[\"进行组内和通道间的信息混合\",{\"0\":{\"97\":1}}],[\"进行联合推理\",{\"1\":{\"64\":1}}],[\"进行信息压缩\",{\"1\":{\"60\":1}}],[\"进行微调\",{\"1\":{\"34\":1}}],[\"识别错误并主动纠正\",{\"1\":{\"823\":1}}],[\"识别点云中的功能区域\",{\"1\":{\"95\":1}}],[\"识别交互部位\",{\"1\":{\"52\":1}}],[\"识别交互部件\",{\"1\":{\"30\":1}}],[\"识别图像中物体与人发生交互的部分\",{\"1\":{\"35\":1}}],[\"该目标函数的形式如下\",{\"1\":{\"951\":1}}],[\"该结构如图4右侧所示\",{\"1\":{\"946\":1}}],[\"该像素由之前所有像素决定的生成模型\",{\"1\":{\"925\":1}}],[\"该松弛会变得精确\",{\"1\":{\"886\":1}}],[\"该下界仅在\",{\"1\":{\"885\":1}}],[\"该分布会表现得像高斯分布\",{\"1\":{\"867\":1}}],[\"该架构巧妙地整合了从庞大知识库中检索到的相关信息\",{\"1\":{\"828\":1}}],[\"该架构支持以下三种功能模式\",{\"1\":{\"171\":1}}],[\"该函数对\",{\"1\":{\"946\":1}}],[\"该函数常作为优化问题的基准函数使用\",{\"1\":{\"816\":1}}],[\"该函数的形状如下图所示\",{\"1\":{\"816\":1}}],[\"该函数自动调用系统命令转换文件\",{\"1\":{\"815\":1}}],[\"该函数形式复杂\",{\"1\":{\"811\":1}}],[\"该函数作用是针对给定的图片路径\",{\"1\":{\"410\":1}}],[\"该流程可参见论文图\",{\"1\":{\"656\":1}}],[\"该方向在对话系统\",{\"1\":{\"655\":1}}],[\"该方法包括三个关键步骤\",{\"1\":{\"654\":1}}],[\"该方法正是clip在vlp领域发扬光大的\",{\"1\":{\"418\":1}}],[\"该方法的主要计算量都集中在模态交互上\",{\"1\":{\"390\":1}}],[\"该方法的核心作用是将无序的点云数据转换为有序的局部邻域结构\",{\"1\":{\"119\":1}}],[\"该方法负责完成具体的一轮训练实现\",{\"1\":{\"383\":1}}],[\"该方法实现过程比较复杂\",{\"1\":{\"382\":1}}],[\"该方法会将所有\",{\"1\":{\"364\":1}}],[\"该方法通过灵活分割图像图块\",{\"1\":{\"331\":1}}],[\"该方法依赖于\",{\"1\":{\"285\":1}}],[\"该方法避免了传统像素级重建的局限性\",{\"1\":{\"227\":1}}],[\"该方法无需额外模型\",{\"1\":{\"195\":1}}],[\"该方法能够有效从噪声图文对中学习\",{\"1\":{\"170\":1}}],[\"该方法能更好的覆盖整个点集\",{\"1\":{\"134\":1}}],[\"该方法摆脱了对几何标注或固定场景的依赖\",{\"1\":{\"73\":1}}],[\"该方法在piad数据集上表现优异\",{\"1\":{\"72\":1}}],[\"该方法从交互图像中进行推理\",{\"1\":{\"50\":1}}],[\"该设置不需要这些目标任务和无标记语料库是一个领域的\",{\"1\":{\"626\":1}}],[\"该系数也等于f1得分\",{\"1\":{\"590\":1}}],[\"该参数未在当前代码中使用\",{\"1\":{\"586\":1}}],[\"该图展示了\",{\"1\":{\"546\":1}}],[\"该张量才是连续的\",{\"1\":{\"489\":1}}],[\"该缓冲区会包含在\",{\"1\":{\"474\":1}}],[\"该类的作用是将二维图像分割成多个图像块\",{\"1\":{\"426\":1}}],[\"该示例中的任务涉及8个类别\",{\"1\":{\"408\":1}}],[\"该任务要求预测句子的最后一个词\",{\"1\":{\"641\":1}}],[\"该任务主要是对一个给定句子\",{\"1\":{\"634\":1}}],[\"该任务的学习目标是通过硬负样本策略\",{\"1\":{\"386\":1}}],[\"该任务的学习目标是采用对比学习策略\",{\"1\":{\"385\":1}}],[\"该任务的学习目标是根据未被掩码的图像序列和文本序列\",{\"1\":{\"384\":1}}],[\"该任务通过随机遮挡一部分图像\",{\"1\":{\"234\":1}}],[\"该功能由\",{\"1\":{\"380\":1}}],[\"该向量即表示一个\",{\"1\":{\"362\":1}}],[\"该编码器支持密集预测任务\",{\"1\":{\"304\":1}}],[\"该过程由make\",{\"1\":{\"698\":1}}],[\"该过程由\",{\"1\":{\"293\":1}}],[\"该过程借助内部维护的\",{\"1\":{\"213\":1}}],[\"该数据集不包含\",{\"1\":{\"888\":1}}],[\"该数据集包含\",{\"1\":{\"888\":1}}],[\"该数据集包含约\",{\"1\":{\"236\":1}}],[\"该数据集大大超越了前作\",{\"1\":{\"41\":1}}],[\"该\",{\"1\":{\"234\":1,\"703\":1,\"887\":1}}],[\"该模块可以为后续的分类任务提供更具区分性和稳定性的特征表示\",{\"1\":{\"431\":1}}],[\"该模块用于编码图像\",{\"1\":{\"376\":1}}],[\"该模块在特征内容和三维空间布局上都能自适应进行信息聚合\",{\"1\":{\"115\":1}}],[\"该模型的结构如图6所示\",{\"1\":{\"952\":1}}],[\"该模型的输入是一对\",{\"1\":{\"339\":1}}],[\"该模型能够灵活生成高质量图像\",{\"1\":{\"884\":1}}],[\"该模型在上下文\",{\"1\":{\"629\":1}}],[\"该模型在文本上处理长期依赖提供了更结构化的内存\",{\"1\":{\"626\":1}}],[\"该模型是在\",{\"1\":{\"435\":1}}],[\"该模型联合训练一个cnn和文本transformer来预测图像的文本描述\",{\"1\":{\"413\":1}}],[\"该模型通过将视觉基础模型扩展到60亿参数\",{\"1\":{\"295\":1}}],[\"该模型同样采用端到端微调的方式\",{\"1\":{\"239\":1}}],[\"该模型包含\",{\"1\":{\"224\":1}}],[\"该模型可以迁移到多种视觉和视觉\",{\"1\":{\"221\":1}}],[\"该范围确保局部区域的尺度是固定的\",{\"1\":{\"135\":1}}],[\"该插值流程就是\",{\"1\":{\"122\":1}}],[\"该机制是点云处理领域的重要创新\",{\"1\":{\"119\":1}}],[\"该部分首先回顾了自然语言处理\",{\"1\":{\"650\":1}}],[\"该部分包含\",{\"1\":{\"35\":1,\"36\":1}}],[\"该部位的几何属性推理\",{\"1\":{\"52\":1}}],[\"该物体常见的其他交互\",{\"1\":{\"52\":1}}],[\"该框架结合了多模态大语言模型\",{\"1\":{\"29\":1}}],[\"其结构如下图所示\",{\"1\":{\"956\":1}}],[\"其前向传播没有问题\",{\"1\":{\"946\":1}}],[\"其可以写成如下闭式表达\",{\"1\":{\"946\":1}}],[\"其分布为维度为\",{\"1\":{\"944\":1}}],[\"其值由\",{\"1\":{\"944\":1}}],[\"其伯努利分布定义为\",{\"1\":{\"932\":1}}],[\"其反映的实际是整个统一词空间上的概率分布\",{\"1\":{\"892\":1}}],[\"其反向传播逻辑为将上游梯度原封不动地传递给两个输入变量\",{\"1\":{\"809\":1}}],[\"其下界为\",{\"1\":{\"885\":1}}],[\"其下标索引公式可表示为\",{\"1\":{\"544\":1}}],[\"其形式为\",{\"1\":{\"868\":1}}],[\"其形状是\",{\"1\":{\"542\":1}}],[\"其形状和大小保持不变的运动方式\",{\"1\":{\"161\":1}}],[\"其概率密度函数可表示为\",{\"1\":{\"871\":1}}],[\"其概率密度函数为\",{\"1\":{\"866\":1,\"944\":1}}],[\"其概率质量函数定义为\",{\"1\":{\"860\":1}}],[\"其定义如下\",{\"1\":{\"847\":1}}],[\"其定义为\",{\"1\":{\"565\":1}}],[\"其三个面分别标记为\",{\"1\":{\"846\":1}}],[\"其技术核心点虽然在大语言模型上\",{\"1\":{\"835\":1}}],[\"其涵盖了模型的输入与输出处理\",{\"1\":{\"833\":1}}],[\"其工作流程可以简单地分为数据处理\",{\"1\":{\"829\":1}}],[\"其性能可与具备\",{\"1\":{\"823\":1}}],[\"其性能与商业模型\",{\"1\":{\"322\":1}}],[\"其最大值可能不在梯度指示方向\",{\"1\":{\"816\":1}}],[\"其最后的全连接层\",{\"1\":{\"362\":1}}],[\"其式子为\",{\"1\":{\"816\":1}}],[\"其支持节点和箭头构成的数据结构可视化\",{\"1\":{\"815\":1}}],[\"其类型可能是原生数值\",{\"1\":{\"809\":1}}],[\"其导数公式为\",{\"1\":{\"809\":1}}],[\"其导数依赖于输出值\",{\"1\":{\"779\":1}}],[\"其导数\",{\"1\":{\"779\":2}}],[\"其由七大主要部分构成\",{\"1\":{\"741\":1}}],[\"其计算量会随着维度的增加不仅需要更多的计算时间\",{\"1\":{\"706\":1}}],[\"其周期就越长\",{\"1\":{\"706\":1}}],[\"其掩码数量可能会偏少\",{\"1\":{\"700\":1}}],[\"其架构为\",{\"1\":{\"690\":1}}],[\"其预训练任务包括\",{\"1\":{\"679\":1}}],[\"其设计始终围绕推理效率目标\",{\"1\":{\"667\":1}}],[\"其特点在于仅使用公开可用的数据集进行训练\",{\"1\":{\"665\":1}}],[\"其特点如下\",{\"1\":{\"368\":1}}],[\"其任务形式和风格可能代表一类高频商业用途\",{\"1\":{\"658\":1}}],[\"其回答\",{\"1\":{\"657\":1}}],[\"其输出就是期望值\",{\"1\":{\"946\":1}}],[\"其输出也比175b的原始gpt\",{\"1\":{\"654\":1}}],[\"其输出支持密集特征图\",{\"1\":{\"303\":1}}],[\"其表现呈现出高度任务依赖性\",{\"1\":{\"649\":1}}],[\"其表现多次逼近甚至超越传统fine\",{\"1\":{\"648\":1}}],[\"其表达能力是有限的\",{\"1\":{\"429\":1}}],[\"其局限性\",{\"1\":{\"646\":1}}],[\"其优势包括\",{\"1\":{\"640\":1}}],[\"其关键特点是\",{\"1\":{\"640\":1}}],[\"其将结构化文本输入处理为单一的连续字符序列\",{\"1\":{\"626\":1}}],[\"其它元素为\",{\"1\":{\"857\":1}}],[\"其它方法都有各自的一些问题\",{\"1\":{\"610\":1}}],[\"其它位置保留原\",{\"1\":{\"266\":1}}],[\"其维度分别为\",{\"1\":{\"609\":1}}],[\"其目标是为图像中的每个像素分配一个特定的语义类别标签\",{\"1\":{\"584\":1}}],[\"其目标是让预训练语言模型\",{\"1\":{\"339\":1}}],[\"其曲线下面积\",{\"1\":{\"570\":1}}],[\"其数学定义为\",{\"1\":{\"562\":1,\"564\":1}}],[\"其在内存中列优先布局\",{\"1\":{\"541\":1}}],[\"其在内存中行优先布局\",{\"1\":{\"541\":1}}],[\"其在冻结骨干网络的前提下\",{\"1\":{\"308\":1}}],[\"其\",{\"1\":{\"489\":1,\"872\":1}}],[\"其均能匹配到正确的文本标签\",{\"1\":{\"408\":1}}],[\"其规模与gpt\",{\"1\":{\"407\":1}}],[\"其规模大约与\",{\"1\":{\"224\":1}}],[\"其主要改进包括\",{\"1\":{\"682\":1}}],[\"其主要用于\",{\"1\":{\"403\":1}}],[\"其主要功能是\",{\"1\":{\"146\":1}}],[\"其主要功能是为每个查询点寻找最近的邻居点\",{\"1\":{\"119\":1}}],[\"其是首个使用patch\",{\"1\":{\"388\":1}}],[\"其实\",{\"1\":{\"960\":1}}],[\"其实是一个\",{\"1\":{\"959\":2}}],[\"其实等价于问\",{\"1\":{\"846\":1}}],[\"其实也就是按照拓扑排序的方式去遍历计算图\",{\"1\":{\"804\":1}}],[\"其实在某些训练集里\",{\"1\":{\"694\":1}}],[\"其实这本质上还是个分类问题\",{\"1\":{\"694\":1}}],[\"其实该过程中执行了n次推理过程\",{\"1\":{\"660\":1}}],[\"其实就得到了注意力掩码矩阵\",{\"1\":{\"582\":1}}],[\"其实就可以用在对比学习上\",{\"1\":{\"355\":1}}],[\"其实那些负样本很有可能是潜在的正样本\",{\"1\":{\"355\":1}}],[\"其实有一个很有意思的点\",{\"1\":{\"350\":1}}],[\"其次我们会调用\",{\"1\":{\"899\":1}}],[\"其次是准备训练数据\",{\"1\":{\"712\":1}}],[\"其次\",{\"1\":{\"353\":1,\"410\":1,\"606\":1,\"635\":1,\"643\":1}}],[\"其已有的知识\",{\"1\":{\"346\":1}}],[\"其开源的模型权重和研究方法为多模态ai发展提供了重要基准\",{\"1\":{\"337\":1}}],[\"其模块化设计\",{\"1\":{\"332\":1}}],[\"其参数量提升42倍\",{\"1\":{\"303\":1}}],[\"其质量直接决定预训练效果\",{\"1\":{\"264\":1}}],[\"其视觉\",{\"1\":{\"236\":1}}],[\"其具体方法总结如下\",{\"1\":{\"234\":1}}],[\"其重建目标函数如下\",{\"1\":{\"232\":1}}],[\"其核心功能是保存和管理数据\",{\"1\":{\"755\":1}}],[\"其核心改进包括以下三点\",{\"1\":{\"322\":1}}],[\"其核心挑战在于\",{\"1\":{\"228\":1}}],[\"其核心思想是通过调整难易样本的权重\",{\"1\":{\"589\":1}}],[\"其核心思想是将每张图像通过视觉\",{\"1\":{\"212\":1}}],[\"其核心思想借鉴了bert的掩码语言建模任务\",{\"1\":{\"227\":1}}],[\"其配置遵循\",{\"1\":{\"224\":1}}],[\"其余设置为\",{\"1\":{\"898\":1}}],[\"其余位置仍为\",{\"1\":{\"896\":1}}],[\"其余位置设为\",{\"1\":{\"208\":1}}],[\"其余部分无论输出什么东西\",{\"1\":{\"691\":1}}],[\"其余部分不做损失\",{\"1\":{\"691\":1,\"700\":1}}],[\"其余部分共享\",{\"1\":{\"179\":1}}],[\"其余100万\",{\"1\":{\"355\":1}}],[\"其余的key离query远\",{\"1\":{\"353\":1}}],[\"其余的照片又分为了很多类别\",{\"1\":{\"350\":1}}],[\"其余参数冻结\",{\"1\":{\"346\":1}}],[\"其余参数保持冻结\",{\"1\":{\"34\":1}}],[\"其两大创新点\",{\"1\":{\"165\":1}}],[\"其代码实现如下所示\",{\"1\":{\"120\":1}}],[\"其他可推出的结论\",{\"1\":{\"848\":1}}],[\"其他运算符绑定\",{\"1\":{\"810\":1}}],[\"其他路径上的梯度信息将丢失\",{\"1\":{\"803\":1}}],[\"其他代码\",{\"1\":{\"783\":1,\"791\":1,\"792\":1}}],[\"其他下游任务\",{\"0\":{\"732\":1}}],[\"其他模型2tb\",{\"1\":{\"668\":1}}],[\"其他数据如wikipedia\",{\"1\":{\"667\":1}}],[\"其他全部相同\",{\"1\":{\"660\":1}}],[\"其他方向\",{\"0\":{\"598\":1}}],[\"其他方法使用的复杂组件\",{\"1\":{\"280\":1}}],[\"其他方法倾向于错误地预测为训练集中频繁出现的\",{\"1\":{\"47\":1}}],[\"其他都是\",{\"1\":{\"493\":1}}],[\"其他位置必须\",{\"1\":{\"472\":1}}],[\"其他权重全部冻结\",{\"1\":{\"435\":1}}],[\"其他配置\",{\"1\":{\"380\":1}}],[\"其他视频里的帧是负样本\",{\"1\":{\"350\":1}}],[\"其他两类为单轮对话\",{\"1\":{\"342\":1}}],[\"其他语言任务表现较差\",{\"1\":{\"323\":1}}],[\"其他为0\",{\"1\":{\"206\":1}}],[\"其他开源许可网站\",{\"1\":{\"41\":1}}],[\"其图像数量是前者的三倍\",{\"1\":{\"41\":1}}],[\"其中变分自编码器\",{\"1\":{\"942\":1}}],[\"其中我们最常使用\",{\"1\":{\"899\":1}}],[\"其中前4维为词空间索引\",{\"1\":{\"892\":1}}],[\"其中当温度\",{\"1\":{\"886\":1}}],[\"其中的概率和为\",{\"1\":{\"877\":1}}],[\"其中的layers就是transformer\",{\"1\":{\"432\":1}}],[\"其中每个图像\",{\"1\":{\"887\":1}}],[\"其中每个元素可以取\",{\"1\":{\"885\":1}}],[\"其中每个矩形的高表示相对计算量大小\",{\"1\":{\"390\":1}}],[\"其中每一个事件就是事件空间中的一个元素\",{\"1\":{\"846\":1}}],[\"其中一个备受关注的项目就是\",{\"1\":{\"831\":1}}],[\"其中一个类别出现的频率非常低\",{\"1\":{\"562\":1}}],[\"其中和是常数\",{\"1\":{\"816\":1}}],[\"其中训练数据使用1k\",{\"1\":{\"712\":1}}],[\"其中关于bertencoders编码并输出结果的整个过程如下图所示\",{\"1\":{\"699\":1}}],[\"其中4项任务\",{\"1\":{\"685\":1}}],[\"其中80\",{\"1\":{\"681\":1}}],[\"其中sst\",{\"1\":{\"634\":1}}],[\"其中single\",{\"1\":{\"391\":1}}],[\"其中秩\",{\"1\":{\"611\":1}}],[\"其中第三步通过反向传播全量更新模型参数的过程如下\",{\"1\":{\"609\":1}}],[\"其中第一项为阶段一\",{\"1\":{\"235\":1}}],[\"其中您感兴趣的罕见云彩类型\",{\"1\":{\"561\":1}}],[\"其中包含多个形状相同的\",{\"1\":{\"466\":1}}],[\"其中包括注意力可视化\",{\"1\":{\"433\":1}}],[\"其中包括两个理解任务和一个生成任务\",{\"1\":{\"172\":1}}],[\"其中0标志位表示word\",{\"1\":{\"392\":1}}],[\"其中word\",{\"1\":{\"392\":1}}],[\"其中region\",{\"1\":{\"391\":1}}],[\"其中有\",{\"1\":{\"373\":1}}],[\"其中基础图块大小为448×448\",{\"1\":{\"330\":1}}],[\"其中词表\",{\"1\":{\"232\":1}}],[\"其中默认使用\",{\"1\":{\"176\":1}}],[\"其中额外添加的\",{\"1\":{\"171\":1}}],[\"其中\",{\"1\":{\"32\":2,\"38\":1,\"39\":1,\"52\":1,\"78\":1,\"88\":1,\"96\":2,\"97\":1,\"98\":1,\"106\":2,\"112\":2,\"113\":1,\"114\":1,\"121\":1,\"143\":1,\"150\":1,\"199\":1,\"200\":1,\"201\":1,\"202\":1,\"212\":2,\"214\":3,\"231\":1,\"233\":2,\"234\":3,\"235\":1,\"238\":1,\"257\":2,\"260\":1,\"261\":1,\"271\":2,\"272\":1,\"285\":4,\"290\":1,\"322\":1,\"341\":1,\"342\":2,\"359\":1,\"371\":2,\"373\":1,\"405\":1,\"407\":1,\"426\":1,\"501\":1,\"504\":1,\"506\":2,\"514\":1,\"522\":1,\"524\":1,\"546\":1,\"577\":1,\"578\":1,\"586\":1,\"587\":2,\"588\":1,\"589\":2,\"590\":1,\"592\":2,\"609\":1,\"611\":1,\"656\":1,\"679\":1,\"709\":2,\"810\":1,\"815\":1,\"823\":1,\"845\":1,\"846\":1,\"847\":1,\"848\":1,\"852\":1,\"856\":2,\"857\":3,\"858\":1,\"859\":1,\"860\":1,\"865\":1,\"867\":1,\"868\":1,\"871\":2,\"872\":1,\"875\":1,\"885\":1,\"894\":1,\"897\":1,\"904\":1,\"910\":1,\"914\":1,\"924\":1,\"932\":1,\"943\":1,\"944\":2,\"945\":1,\"946\":3,\"951\":2,\"952\":1,\"960\":1,\"963\":1}}],[\"见附录图\",{\"1\":{\"657\":1}}],[\"见论文第\",{\"1\":{\"655\":1}}],[\"见论文\",{\"1\":{\"502\":1}}],[\"见表7\",{\"1\":{\"685\":1}}],[\"见表\",{\"1\":{\"647\":2,\"648\":2}}],[\"见表6\",{\"1\":{\"376\":1,\"685\":1}}],[\"见表5\",{\"1\":{\"376\":1,\"685\":2}}],[\"见表4\",{\"1\":{\"376\":1,\"684\":2}}],[\"见表2的参数字段与学习率配置\",{\"1\":{\"666\":1}}],[\"见表2\",{\"1\":{\"323\":1,\"641\":1,\"683\":1}}],[\"见表1的采样比例与磁盘大小\",{\"1\":{\"667\":1}}],[\"见表1\",{\"1\":{\"272\":1,\"323\":1,\"683\":1}}],[\"见表3\",{\"1\":{\"224\":1,\"683\":1}}],[\"见过\",{\"1\":{\"89\":1}}],[\"见图\",{\"1\":{\"648\":5,\"656\":1,\"885\":1}}],[\"见图2\",{\"1\":{\"323\":1}}],[\"见图1训练损失曲线\",{\"1\":{\"666\":1}}],[\"见图1\",{\"1\":{\"323\":1}}],[\"见图4\",{\"1\":{\"323\":1}}],[\"见图4b\",{\"1\":{\"303\":1}}],[\"见图3\",{\"1\":{\"222\":1,\"329\":1}}],[\"见图8\",{\"1\":{\"49\":1}}],[\"见图7\",{\"1\":{\"49\":1}}],[\"见图6\",{\"1\":{\"49\":1}}],[\"见图5\",{\"1\":{\"48\":1}}],[\"见\",{\"1\":{\"32\":3,\"53\":1,\"655\":1,\"658\":2}}],[\"随训练进程按余弦调度从\",{\"1\":{\"285\":1}}],[\"随模型保存\",{\"1\":{\"213\":1}}],[\"随后介绍\",{\"1\":{\"270\":1}}],[\"随后\",{\"1\":{\"236\":1,\"330\":1,\"822\":1,\"884\":1}}],[\"随后在输入前加入一个可学习的\",{\"1\":{\"214\":1}}],[\"随后线性衰减\",{\"1\":{\"176\":1}}],[\"随后将\",{\"1\":{\"32\":1}}],[\"随机噪声分布\",{\"1\":{\"918\":1}}],[\"随机对输入的文本条件进行\",{\"1\":{\"893\":1}}],[\"随机\",{\"1\":{\"846\":1,\"860\":1,\"893\":1}}],[\"随机的是它作用的输入\",{\"1\":{\"846\":1}}],[\"随机变量是函数\",{\"1\":{\"846\":1}}],[\"随机变量等于这个值的概率是多少\",{\"1\":{\"846\":1}}],[\"随机变量\",{\"1\":{\"846\":2}}],[\"随机生成等量的非相邻句对\",{\"1\":{\"698\":1}}],[\"随机把一句话中\",{\"1\":{\"691\":1}}],[\"随机高斯分布初始化\",{\"1\":{\"611\":1}}],[\"随机数生成器\",{\"1\":{\"521\":3}}],[\"随机数本质上仍是伪随机\",{\"1\":{\"521\":1}}],[\"随机数种子\",{\"1\":{\"521\":4}}],[\"随机数种子与确定性\",{\"0\":{\"521\":1}}],[\"随机以0\",{\"1\":{\"393\":2}}],[\"随机丢弃整个残差分支\",{\"1\":{\"380\":1}}],[\"随机灰度化\",{\"1\":{\"293\":1}}],[\"随机翻转\",{\"1\":{\"293\":1}}],[\"随机裁剪输入图像\",{\"1\":{\"425\":1}}],[\"随机裁剪的区域面积\",{\"1\":{\"293\":1}}],[\"随机裁剪区域的最小和最大面积比例\",{\"1\":{\"293\":1}}],[\"随机裁剪区域的相对尺度范围\",{\"1\":{\"293\":1}}],[\"随机裁剪并缩放到\",{\"1\":{\"293\":1}}],[\"随机裁剪\",{\"1\":{\"264\":1}}],[\"随机裁剪图片\",{\"1\":{\"82\":1}}],[\"随机调整亮度\",{\"1\":{\"264\":1}}],[\"随机水平翻转和双图采样\",{\"1\":{\"264\":1}}],[\"随机单\",{\"1\":{\"263\":1}}],[\"随机深度\",{\"1\":{\"224\":1}}],[\"随机掩码约\",{\"1\":{\"228\":1}}],[\"随机掩码\",{\"1\":{\"223\":3}}],[\"随机初始化\",{\"1\":{\"213\":1,\"316\":1,\"329\":1}}],[\"随机采样时\",{\"1\":{\"956\":1}}],[\"随机采样遮挡块在图像上的左上角位置\",{\"1\":{\"263\":1}}],[\"随机采样遮挡块的长宽比\",{\"1\":{\"263\":1}}],[\"随机采样目标遮挡面积\",{\"1\":{\"263\":1}}],[\"随机采样\",{\"1\":{\"178\":1}}],[\"随机选择\",{\"1\":{\"679\":1}}],[\"随机选择文本序列中的\",{\"1\":{\"373\":1}}],[\"随机选择第一个点\",{\"1\":{\"121\":1}}],[\"随机选择一个起始点\",{\"1\":{\"121\":1}}],[\"随机选\",{\"1\":{\"92\":1}}],[\"随机配对使模型暴露于各种语义上下文中\",{\"1\":{\"90\":1}}],[\"随着不断迭代\",{\"1\":{\"823\":1}}],[\"随着各阶段计算量的增加\",{\"1\":{\"822\":1}}],[\"随着语言模型规模的扩大\",{\"1\":{\"822\":1}}],[\"随着\",{\"1\":{\"814\":1,\"822\":1}}],[\"随着序列长度的增加\",{\"1\":{\"706\":1}}],[\"随着时代演进\",{\"1\":{\"690\":1}}],[\"随着模型规模扩展\",{\"1\":{\"651\":1}}],[\"随着假负例的减少\",{\"1\":{\"565\":1}}],[\"随着假正例的减少\",{\"1\":{\"565\":1}}],[\"随着训练的进行\",{\"1\":{\"427\":1}}],[\"随着3d数据集\",{\"1\":{\"31\":1}}],[\"随着大规模\",{\"1\":{\"20\":1}}],[\"98900063\",{\"1\":{\"816\":1}}],[\"98\",{\"1\":{\"316\":1}}],[\"98b\",{\"1\":{\"305\":1}}],[\"95\",{\"1\":{\"315\":1,\"410\":1}}],[\"9b\",{\"1\":{\"311\":1,\"823\":5,\"875\":1}}],[\"9b参数\",{\"1\":{\"303\":1}}],[\"914\",{\"1\":{\"341\":1}}],[\"91\",{\"1\":{\"268\":1}}],[\"92\",{\"1\":{\"224\":2,\"342\":2,\"343\":1}}],[\"998\",{\"1\":{\"816\":1}}],[\"997\",{\"1\":{\"816\":1}}],[\"99449622\",{\"1\":{\"816\":1}}],[\"994\",{\"1\":{\"816\":1}}],[\"993\",{\"1\":{\"816\":1}}],[\"992\",{\"1\":{\"816\":1}}],[\"996\",{\"1\":{\"285\":1,\"816\":1}}],[\"99\",{\"1\":{\"213\":3,\"356\":1,\"562\":1,\"683\":1,\"850\":1}}],[\"995\",{\"1\":{\"190\":1,\"192\":1,\"205\":1,\"816\":1}}],[\"999\",{\"1\":{\"104\":1,\"236\":1,\"361\":2,\"816\":1,\"918\":1}}],[\"960\",{\"1\":{\"701\":1}}],[\"96x96\",{\"1\":{\"293\":1}}],[\"96\",{\"1\":{\"141\":1,\"293\":2,\"306\":1,\"309\":1,\"344\":1}}],[\"9027\",{\"1\":{\"701\":1}}],[\"90\",{\"1\":{\"76\":1,\"117\":1,\"157\":1,\"268\":1,\"342\":1,\"685\":1,\"822\":1}}],[\"9\",{\"0\":{\"69\":1},\"1\":{\"32\":1,\"46\":1,\"47\":1,\"64\":1,\"83\":1,\"104\":1,\"106\":1,\"110\":1,\"152\":4,\"157\":1,\"188\":1,\"190\":3,\"206\":1,\"236\":1,\"255\":1,\"293\":1,\"305\":1,\"308\":1,\"315\":1,\"316\":1,\"318\":1,\"332\":1,\"384\":2,\"385\":2,\"421\":1,\"477\":4,\"481\":1,\"482\":1,\"484\":1,\"540\":2,\"541\":2,\"542\":1,\"544\":2,\"550\":1,\"589\":1,\"592\":1,\"625\":1,\"626\":1,\"641\":1,\"648\":2,\"656\":1,\"657\":1,\"658\":2,\"660\":1,\"668\":3,\"669\":1,\"680\":1,\"712\":1,\"739\":1,\"808\":1,\"822\":1,\"823\":5,\"833\":1,\"850\":1,\"899\":1,\"907\":1,\"936\":1,\"946\":3}}],[\"首个提出\",{\"1\":{\"965\":1}}],[\"首个版本于\",{\"1\":{\"823\":1}}],[\"首个专门针对\",{\"1\":{\"20\":1}}],[\"首轮推理\",{\"1\":{\"663\":1}}],[\"首轮统计展示\",{\"1\":{\"595\":1}}],[\"首先给出添加了空间掩码的卷积层实现\",{\"1\":{\"926\":1}}],[\"首先给出的是掩码生成器的初始化方法\",{\"1\":{\"263\":1}}],[\"首先要确定应用的核心功能\",{\"1\":{\"836\":1}}],[\"首先需要将非常复杂的业务逻辑依次拆解\",{\"1\":{\"835\":1}}],[\"首先需要对输入图片进行尺寸变化\",{\"1\":{\"425\":1}}],[\"首先在大规模文本数据上进行预训练\",{\"1\":{\"824\":1}}],[\"首先在输入句子的开头加一个代表分类的符号\",{\"1\":{\"694\":1}}],[\"首先求rosenbrock函数在处的导数和\",{\"1\":{\"816\":1}}],[\"首先调用a\",{\"1\":{\"809\":1}}],[\"首先来看一下\",{\"1\":{\"709\":1,\"899\":1}}],[\"首先模型会根据传入的tokens列表生成一个pad\",{\"1\":{\"703\":1}}],[\"首先加入特殊标记\",{\"1\":{\"697\":1}}],[\"首先我们需要准备一个小型语料库\",{\"1\":{\"696\":1}}],[\"首先我们用data目录充当我们的图片库来源\",{\"1\":{\"411\":1}}],[\"首先将\",{\"1\":{\"698\":1}}],[\"首先将橙色和所有的黄色向量进行\",{\"1\":{\"694\":1}}],[\"首先将问题和文章通过\",{\"1\":{\"694\":1}}],[\"首先利用人工演示数据对gpt\",{\"1\":{\"653\":1}}],[\"首先说明\",{\"1\":{\"435\":1}}],[\"首先说对比学习想要做到什么呢\",{\"1\":{\"349\":1}}],[\"首先输入图片\",{\"1\":{\"421\":1}}],[\"首先对图像\",{\"1\":{\"374\":1}}],[\"首先从\",{\"1\":{\"355\":1}}],[\"首先从它的初始化方法入手\",{\"1\":{\"213\":1}}],[\"首先是其初始化方法中需要完成\",{\"1\":{\"697\":1}}],[\"首先是其实现的钩子方法\",{\"1\":{\"383\":1}}],[\"首先是\",{\"1\":{\"255\":1}}],[\"首先通过一个线性映射层转换为\",{\"1\":{\"233\":1}}],[\"首先通过\",{\"1\":{\"185\":1}}],[\"首先\",{\"1\":{\"142\":1,\"330\":1,\"353\":1,\"357\":1,\"380\":1,\"408\":1,\"410\":1,\"606\":1,\"626\":1,\"635\":1,\"643\":1,\"946\":1,\"948\":1}}],[\"首先使用卷积层对输入图像进行处理\",{\"1\":{\"426\":1}}],[\"首先使用\",{\"1\":{\"32\":1}}],[\"首次引入的\",{\"1\":{\"825\":1}}],[\"首次发布\",{\"1\":{\"823\":1}}],[\"首次将深度学习的思想融入到语言模型中\",{\"1\":{\"822\":1}}],[\"首次将自注意力网络引入三维点云理解\",{\"1\":{\"125\":1}}],[\"首次实现了视觉与语言模型在参数和特征空间的深度协同\",{\"1\":{\"301\":1}}],[\"首次sample\",{\"1\":{\"138\":1}}],[\"首次通过非配对的2d\",{\"1\":{\"75\":1}}],[\"首次提出通过2d交互图像预测3d物体功能区域\",{\"1\":{\"73\":1}}],[\"即此次计算无梯度\",{\"1\":{\"959\":1}}],[\"即先计算向量与嵌入空间\",{\"1\":{\"958\":1}}],[\"即先在大语料上预训练\",{\"1\":{\"650\":1}}],[\"即压缩\",{\"1\":{\"950\":1}}],[\"即出现不连续性\",{\"1\":{\"946\":1}}],[\"即最大化\",{\"1\":{\"932\":1}}],[\"即最远的点\",{\"1\":{\"121\":1}}],[\"即当前位置\",{\"1\":{\"926\":1}}],[\"即输入图像中哪些像素的信息能够传递到中心像素上\",{\"1\":{\"923\":1}}],[\"即第\",{\"1\":{\"921\":1}}],[\"即寻找使似然函数取得最大值的参数\",{\"1\":{\"904\":1}}],[\"即似然\",{\"1\":{\"903\":1}}],[\"即温度\",{\"1\":{\"889\":1}}],[\"即求和\",{\"1\":{\"877\":1}}],[\"即确定程度\",{\"1\":{\"877\":1}}],[\"即灰色图像\",{\"1\":{\"875\":1}}],[\"即离原点越远\",{\"1\":{\"873\":1}}],[\"即离散编码类别数\",{\"1\":{\"255\":1}}],[\"即没有期望值\",{\"1\":{\"868\":1}}],[\"即归一化常数\",{\"1\":{\"868\":1}}],[\"即所谓的标准正态分布\",{\"1\":{\"865\":1}}],[\"即所有实际正例被正确分类为正例的比例\",{\"1\":{\"563\":1}}],[\"即所有\",{\"1\":{\"67\":1}}],[\"即方差小\",{\"1\":{\"865\":1}}],[\"即方差的倒数\",{\"1\":{\"865\":1}}],[\"即蓝球个数\",{\"1\":{\"860\":1}}],[\"即互斥\",{\"1\":{\"848\":1}}],[\"即为事件空间中的每个集合赋予一个\",{\"1\":{\"846\":1}}],[\"即为与当前文本描述相似度最高的那副图片\",{\"1\":{\"411\":1}}],[\"即要开发的应用的应用场景\",{\"1\":{\"836\":1}}],[\"即要分成多少类\",{\"1\":{\"213\":1}}],[\"即后训练阶段的强化学习\",{\"1\":{\"822\":1}}],[\"即学习率\",{\"1\":{\"816\":1}}],[\"即调用\",{\"1\":{\"809\":3}}],[\"即y\",{\"1\":{\"809\":1}}],[\"即y=fn\",{\"1\":{\"775\":1}}],[\"即y=x\",{\"1\":{\"775\":1}}],[\"即正向传播\",{\"1\":{\"805\":1}}],[\"即需要知道正向传播时的输出值\",{\"1\":{\"779\":1}}],[\"即忽略掉pad部分的损失计算\",{\"1\":{\"700\":2}}],[\"即给定一个前提\",{\"1\":{\"694\":1}}],[\"即给定input\",{\"1\":{\"420\":1}}],[\"即完型填空\",{\"1\":{\"690\":1}}],[\"即预处理时固定掩码模式\",{\"1\":{\"679\":1}}],[\"即gpt类模型一次推理只输出一个token\",{\"1\":{\"660\":1}}],[\"即对数方差\",{\"1\":{\"931\":1}}],[\"即对不同用户群体可调节输出风格\",{\"1\":{\"658\":1}}],[\"即对点顺序不敏感\",{\"1\":{\"150\":1}}],[\"即便是只有1\",{\"1\":{\"654\":1}}],[\"即便是那些有大量标注数据的场景\",{\"1\":{\"626\":1}}],[\"即标注者和研究者\",{\"1\":{\"654\":1}}],[\"即验证集损失随着模型规模\",{\"1\":{\"650\":1}}],[\"即通过自然语言描述或示例引导模型生成目标输出\",{\"1\":{\"640\":1}}],[\"即通过无监督学习估计文本序列的概率分布\",{\"1\":{\"640\":1}}],[\"即建模\",{\"1\":{\"640\":1}}],[\"即将文本\",{\"1\":{\"891\":1}}],[\"即将原文本按照空格\",{\"1\":{\"595\":1}}],[\"即将掩码矩阵中值为1处\",{\"1\":{\"582\":1}}],[\"即模型生成的分布\",{\"1\":{\"949\":1}}],[\"即模型输出的每个\",{\"1\":{\"897\":1}}],[\"即模型在\",{\"1\":{\"650\":1}}],[\"即模型对真实类别的预测概率\",{\"1\":{\"589\":1}}],[\"即模态专家\",{\"1\":{\"222\":1}}],[\"即内存地址偏移\",{\"1\":{\"544\":1}}],[\"即形状为\",{\"1\":{\"544\":1}}],[\"即注意力得分\",{\"1\":{\"526\":1}}],[\"即行优先\",{\"1\":{\"489\":1}}],[\"即会被保存和加载\",{\"1\":{\"474\":1}}],[\"即网格大小的乘积\",{\"1\":{\"426\":1}}],[\"即一种先验知识\",{\"1\":{\"422\":1}}],[\"即文本和图像可能不完全匹配\",{\"1\":{\"413\":1}}],[\"即基于对比学习的方法\",{\"1\":{\"413\":1}}],[\"即基于文本弱监督来提升性能\",{\"1\":{\"413\":1}}],[\"即基于动量的对比学习\",{\"1\":{\"351\":1}}],[\"即真正属于一对的文本和图像\",{\"1\":{\"407\":1}}],[\"即上图中橙色和黄色的向量\",{\"1\":{\"694\":1}}],[\"即上图所示的矩阵\",{\"1\":{\"407\":1}}],[\"即上一层级\",{\"1\":{\"142\":1}}],[\"即这些位置不参与\",{\"1\":{\"382\":1}}],[\"即判断某个样本是否是正样本\",{\"1\":{\"355\":1}}],[\"即从潜在变量\",{\"1\":{\"949\":1}}],[\"即从大量候选中选出正确样本\",{\"1\":{\"355\":1}}],[\"即从被破坏的图像恢复原始图像\",{\"1\":{\"235\":1}}],[\"即可得到\",{\"1\":{\"952\":1}}],[\"即可得到极大似然估计的方程\",{\"1\":{\"904\":1}}],[\"即可在多种语言任务中实现从零样本到少样本的泛化\",{\"1\":{\"651\":1}}],[\"即可\",{\"1\":{\"611\":1,\"694\":1}}],[\"即可生成高质量图像描述\",{\"1\":{\"309\":1}}],[\"即可用于生成图像描述\",{\"1\":{\"188\":1}}],[\"即原始图像面积的比例\",{\"1\":{\"293\":1}}],[\"即每一个值被采样到的可能性完全相等\",{\"1\":{\"263\":1}}],[\"即每个图像中每个\",{\"1\":{\"899\":1}}],[\"即每个图像块经过卷积操作后得到的特征向量的维度\",{\"1\":{\"426\":1}}],[\"即每个位置对每个\",{\"1\":{\"710\":1}}],[\"即每个\",{\"1\":{\"213\":1,\"518\":1}}],[\"即每个点的各个类别得分\",{\"1\":{\"156\":1}}],[\"即每个点都有一个类别预测\",{\"1\":{\"146\":1}}],[\"即每个点对应的\",{\"1\":{\"64\":1}}],[\"即每个查询点有一个特征向量\",{\"1\":{\"137\":1}}],[\"即每个局部邻域内的点数量维度\",{\"1\":{\"137\":1}}],[\"即无条件\",{\"1\":{\"893\":1}}],[\"即无法再生成有效遮挡块\",{\"1\":{\"263\":1}}],[\"即无功能区域\",{\"1\":{\"106\":1}}],[\"即是均匀分布\",{\"1\":{\"260\":1}}],[\"即不遮挡\",{\"1\":{\"242\":1}}],[\"即随机遮盖输入文本的部分单词\",{\"1\":{\"681\":1}}],[\"即随机选择\",{\"1\":{\"242\":1}}],[\"即随机移除一部分输入点\",{\"1\":{\"141\":1}}],[\"即约占图像\",{\"1\":{\"236\":1}}],[\"即取视觉令牌的最大概率值\",{\"1\":{\"235\":1}}],[\"即假设\",{\"1\":{\"235\":1}}],[\"即还原被遮挡\",{\"1\":{\"234\":1}}],[\"即图片上相邻的区域具有相似的特征\",{\"1\":{\"422\":1}}],[\"即图文对\",{\"1\":{\"223\":1}}],[\"即图像在水平和垂直方向上分别可以划分的图像块数量\",{\"1\":{\"426\":1}}],[\"即图像和文本\",{\"1\":{\"223\":1}}],[\"即图像特征作为\",{\"1\":{\"188\":1}}],[\"即字典大小\",{\"1\":{\"213\":1}}],[\"即同一个图像可以有多个\",{\"1\":{\"190\":1}}],[\"即在数据预处理阶段生成掩码模式并固定\",{\"1\":{\"681\":1}}],[\"即在预训练阶段让模型隐式学习多种技能\",{\"1\":{\"646\":1}}],[\"即在\",{\"1\":{\"172\":1}}],[\"即在欧氏空间中\",{\"1\":{\"135\":1}}],[\"即只对真实类别对应的概率取负对数\",{\"1\":{\"910\":1}}],[\"即只使用少量编码\",{\"1\":{\"212\":1}}],[\"即只改变物体的方向而不改变形状和大小\",{\"1\":{\"153\":1}}],[\"即只依赖一小部分关键点就能判断整体形状\",{\"1\":{\"150\":1}}],[\"即使我们假设\",{\"1\":{\"949\":1}}],[\"即使最大似然更大\",{\"1\":{\"944\":1}}],[\"即使它们可能有重叠\",{\"1\":{\"848\":1}}],[\"即使计算图不再被用户访问\",{\"1\":{\"806\":1}}],[\"即使参数量远小于\",{\"1\":{\"655\":1}}],[\"即使参数量远小于原始gpt\",{\"1\":{\"653\":1}}],[\"即使1\",{\"1\":{\"641\":1}}],[\"即使是少样本提示增强的\",{\"1\":{\"657\":1}}],[\"即使是gpt\",{\"1\":{\"648\":1}}],[\"即使是最大的1\",{\"1\":{\"641\":1}}],[\"即使是一维位置编码\",{\"1\":{\"428\":1}}],[\"即使外部函数已经执行完毕\",{\"1\":{\"448\":1}}],[\"即使冻结\",{\"1\":{\"305\":1}}],[\"即使冻结llm解码器也能在多模态对话任务中表现优异\",{\"1\":{\"304\":1}}],[\"即使去掉动量编码器\",{\"1\":{\"282\":1}}],[\"即使只使用公开数据\",{\"1\":{\"220\":1}}],[\"即使没有依赖私有数据\",{\"1\":{\"220\":1}}],[\"即使其他点都在\",{\"1\":{\"157\":1}}],[\"即使\",{\"1\":{\"150\":1,\"572\":1}}],[\"即使丢失一些点或加入异常点\",{\"1\":{\"150\":1}}],[\"即根据最近的几个邻近点的距离进行加权平均\",{\"1\":{\"143\":1}}],[\"即空间中的任何距离值具有相似的含义\",{\"1\":{\"135\":1}}],[\"即任何方向上的度量都是等价的\",{\"1\":{\"135\":1}}],[\"即任务是找到点云集中的局部区域的中心点\",{\"1\":{\"134\":1}}],[\"即生成一个二值掩码\",{\"1\":{\"94\":1}}],[\"即物体支持的交互可能性\",{\"1\":{\"73\":1}}],[\"即\",{\"1\":{\"32\":1,\"100\":1,\"106\":1,\"154\":1,\"157\":1,\"160\":1,\"230\":1,\"234\":2,\"260\":1,\"261\":1,\"264\":2,\"265\":1,\"272\":1,\"341\":1,\"382\":1,\"457\":1,\"458\":1,\"489\":1,\"562\":1,\"589\":1,\"654\":1,\"692\":1,\"694\":1,\"709\":1,\"734\":1,\"735\":1,\"845\":2,\"846\":1,\"850\":2,\"856\":1,\"858\":1,\"859\":1,\"864\":1,\"885\":1,\"896\":1,\"931\":1,\"932\":1,\"943\":1,\"944\":1,\"949\":1,\"950\":1,\"963\":1}}],[\"关闭\",{\"1\":{\"899\":1}}],[\"关系\",{\"1\":{\"805\":1}}],[\"关键是\",{\"1\":{\"943\":1}}],[\"关键在于实现类型转换工具函数as\",{\"1\":{\"809\":1}}],[\"关键要点\",{\"0\":{\"758\":1}}],[\"关键里程碑\",{\"1\":{\"671\":1}}],[\"关键性能提升\",{\"1\":{\"669\":1}}],[\"关键发现\",{\"1\":{\"668\":1,\"670\":1}}],[\"关键结论\",{\"1\":{\"658\":1}}],[\"关键参数的作用\",{\"1\":{\"589\":1}}],[\"关键区别\",{\"1\":{\"500\":1}}],[\"关键共同点\",{\"1\":{\"500\":1}}],[\"关键字参数\",{\"1\":{\"445\":2}}],[\"关键优势\",{\"1\":{\"335\":1}}],[\"关键一步\",{\"1\":{\"258\":1}}],[\"关键代表包括\",{\"1\":{\"249\":1}}],[\"关键点是\",{\"1\":{\"444\":1}}],[\"关键点可能丢失\",{\"1\":{\"157\":1}}],[\"关键点集\",{\"1\":{\"150\":1}}],[\"关键问题\",{\"1\":{\"31\":1,\"670\":1}}],[\"关注\",{\"1\":{\"708\":1}}],[\"关注学习固定词向量\",{\"1\":{\"650\":1}}],[\"关注每个像素的分类准确性\",{\"1\":{\"592\":1}}],[\"关注每个点的分类误差\",{\"1\":{\"587\":1}}],[\"关注整体区域匹配程度\",{\"1\":{\"592\":1}}],[\"关注整体区域匹配\",{\"1\":{\"588\":1}}],[\"关注整体区域匹配度\",{\"1\":{\"587\":1}}],[\"关注整体掩码匹配度\",{\"1\":{\"102\":1}}],[\"关注图像\",{\"1\":{\"399\":1}}],[\"关注未来的\",{\"1\":{\"272\":1}}],[\"关注排序能力\",{\"1\":{\"106\":1}}],[\"关于各参数求偏导并令其为零\",{\"1\":{\"904\":1}}],[\"关于本部分代码细节的详细解释\",{\"1\":{\"899\":1}}],[\"关于数据收集过程的更多细节见附录\",{\"1\":{\"888\":1}}],[\"关于上面部分公式的补充解读\",{\"1\":{\"885\":1}}],[\"关于qlora的具体细节\",{\"1\":{\"607\":1}}],[\"关于lora的具体细节\",{\"1\":{\"606\":1}}],[\"关于计算\",{\"1\":{\"589\":1}}],[\"关于vit模型的不同版本\",{\"1\":{\"432\":1}}],[\"关于多头注意力机制流程不太清楚的\",{\"1\":{\"430\":1}}],[\"关于norm层\",{\"1\":{\"429\":1}}],[\"关于bertmodel的代码解析部分\",{\"1\":{\"419\":1}}],[\"关于这一领域的详细综述\",{\"1\":{\"409\":1}}],[\"关于共享自注意力模块的消融结果\",{\"1\":{\"376\":1}}],[\"关于\",{\"1\":{\"274\":1}}],[\"关于码本向量的更新流程就是\",{\"1\":{\"213\":1}}],[\"关于阶段一的预训练过程\",{\"1\":{\"213\":1}}],[\"关于利用roi\",{\"1\":{\"83\":1}}],[\"关于我们\",{\"0\":{\"1\":1}}],[\"结束调用\",{\"1\":{\"453\":1}}],[\"结束\",{\"1\":{\"452\":1,\"735\":1}}],[\"结束值\",{\"1\":{\"440\":1}}],[\"结束符\",{\"1\":{\"421\":1}}],[\"结束索引和点数\",{\"1\":{\"122\":1}}],[\"结果分析\",{\"1\":{\"811\":1}}],[\"结果验证\",{\"1\":{\"811\":2}}],[\"结果包含误差\",{\"1\":{\"772\":1}}],[\"结果是\",{\"1\":{\"709\":1}}],[\"结果是一个形状为\",{\"1\":{\"544\":1}}],[\"结果发现在imagenet数据集上能够带来3\",{\"1\":{\"409\":1}}],[\"结果见表5\",{\"1\":{\"376\":1}}],[\"结果就是\",{\"1\":{\"262\":1}}],[\"结果就不会变\",{\"1\":{\"150\":1}}],[\"结果\",{\"0\":{\"648\":1,\"657\":1,\"668\":1},\"1\":{\"213\":1,\"462\":1,\"528\":1,\"678\":1,\"683\":2,\"684\":2,\"710\":1,\"878\":1}}],[\"结果显示\",{\"1\":{\"157\":1,\"220\":1,\"308\":1,\"641\":1,\"646\":1,\"681\":1}}],[\"结果中的当前写入位置\",{\"1\":{\"121\":1}}],[\"结果如下表3\",{\"1\":{\"634\":1}}],[\"结果如下图所示\",{\"1\":{\"428\":1,\"816\":1}}],[\"结果如下\",{\"1\":{\"117\":1,\"432\":1}}],[\"结果表明\",{\"1\":{\"99\":1,\"243\":1,\"884\":1}}],[\"结合全概率公式\",{\"1\":{\"852\":1}}],[\"结合全局池化增强语义表达\",{\"1\":{\"70\":1}}],[\"结合上述分析\",{\"1\":{\"836\":1}}],[\"结合特殊的数据或业务逻辑来提供独特功能的应用称为大模型开发\",{\"1\":{\"835\":1}}],[\"结合特征差值和位置编码生成向量化注意力权重\",{\"1\":{\"119\":1}}],[\"结合检索到的信息和模型的生成能力\",{\"1\":{\"828\":1}}],[\"结合缓存的attention\",{\"1\":{\"420\":1}}],[\"结合低分辨率和高分辨率特征\",{\"1\":{\"326\":1}}],[\"结合使用\",{\"1\":{\"285\":1}}],[\"结合视觉\",{\"1\":{\"212\":1}}],[\"结合文本通过多模态编码器处理\",{\"1\":{\"194\":1}}],[\"结合文本描述提升语义理解\",{\"1\":{\"75\":1}}],[\"结合\",{\"1\":{\"150\":1,\"225\":1,\"305\":1}}],[\"结合了层级特征提取和多尺度融合机制\",{\"1\":{\"143\":1}}],[\"结合高层语义信息和底层几何细节\",{\"1\":{\"122\":1}}],[\"结合论文理解这些指标的意义\",{\"1\":{\"106\":1}}],[\"结合人工+gpt\",{\"1\":{\"93\":1}}],[\"结合其对应的功能类型和原始点云标注信息\",{\"1\":{\"88\":1}}],[\"结合cot在目标检测\",{\"1\":{\"31\":1}}],[\"结构一致\",{\"1\":{\"871\":1}}],[\"结构正常的区间组合\",{\"1\":{\"847\":1}}],[\"结构化问答\",{\"1\":{\"649\":1}}],[\"结构的基础上添加一个任务层\",{\"1\":{\"237\":1}}],[\"结构的隐式关联\",{\"1\":{\"76\":1}}],[\"结构与主模型相同\",{\"1\":{\"205\":1}}],[\"结构规整\",{\"1\":{\"159\":1}}],[\"结构单一\",{\"1\":{\"157\":1}}],[\"结构简单\",{\"1\":{\"157\":1}}],[\"结构如图3所示\",{\"1\":{\"116\":1}}],[\"结构如图2所示\",{\"1\":{\"113\":1}}],[\"结构多样性\",{\"1\":{\"87\":1}}],[\"结构融合\",{\"1\":{\"57\":1}}],[\"结构\",{\"0\":{\"742\":1,\"744\":1,\"748\":1},\"1\":{\"56\":1,\"143\":1,\"272\":1,\"633\":1,\"679\":1,\"741\":1,\"895\":1}}],[\"结构上\",{\"1\":{\"20\":1}}],[\"结论如下\",{\"1\":{\"179\":1}}],[\"结论\",{\"0\":{\"50\":1,\"252\":1,\"313\":1,\"337\":1,\"636\":1,\"651\":1},\"1\":{\"117\":3,\"242\":1,\"578\":1,\"658\":1}}],[\"解读目标函数\",{\"0\":{\"948\":1}}],[\"解读加代码实现\",{\"0\":{\"920\":1},\"1\":{\"920\":1}}],[\"解为样本均值和\",{\"1\":{\"904\":1}}],[\"解得极大似然估计\",{\"1\":{\"904\":1}}],[\"解题过程可靠\",{\"1\":{\"823\":1}}],[\"解释\",{\"1\":{\"542\":2,\"908\":1,\"910\":1,\"918\":1}}],[\"解耦的自回归解码器设计的一个关键优势在于\",{\"1\":{\"272\":1}}],[\"解耦的文本解码器与\",{\"1\":{\"272\":1}}],[\"解包语法\",{\"1\":{\"800\":1}}],[\"解包数据\",{\"1\":{\"431\":1}}],[\"解包\",{\"1\":{\"120\":1,\"121\":1,\"122\":1,\"445\":2}}],[\"解包编码器输出的不同层级特征\",{\"1\":{\"70\":1}}],[\"解组\",{\"1\":{\"95\":1}}],[\"解码成图像\",{\"1\":{\"964\":2}}],[\"解码成自然语言\",{\"1\":{\"735\":1}}],[\"解码成自然语言文本\",{\"1\":{\"735\":1}}],[\"解码离散表示\",{\"1\":{\"963\":1}}],[\"解码还原图像\",{\"1\":{\"899\":1}}],[\"解码图像\",{\"1\":{\"895\":1,\"899\":1}}],[\"解码出来的图像尽量接近原图\",{\"1\":{\"885\":1}}],[\"解码模型通常会利用整段图像\",{\"1\":{\"272\":1}}],[\"解码结构的基础模型\",{\"1\":{\"268\":1}}],[\"解码结构提取多尺度点特征\",{\"1\":{\"94\":1}}],[\"解码函数\",{\"1\":{\"213\":1}}],[\"解码重建特征\",{\"1\":{\"213\":1}}],[\"解码生成的token\",{\"1\":{\"421\":1}}],[\"解码生成的\",{\"1\":{\"188\":1}}],[\"解码过程\",{\"1\":{\"144\":1}}],[\"解码过程中点特征的语言引导能力\",{\"1\":{\"95\":1}}],[\"解码\",{\"1\":{\"83\":1,\"143\":1,\"268\":1,\"403\":2,\"899\":1}}],[\"解码输出\",{\"1\":{\"78\":1}}],[\"解码阶段\",{\"0\":{\"59\":1},\"1\":{\"964\":1}}],[\"解码器还是认得这个向量\",{\"1\":{\"956\":1}}],[\"解码器不仅认识编码器编出的向量\",{\"1\":{\"956\":1}}],[\"解码器不同\",{\"1\":{\"272\":1}}],[\"解码器是生成不出有意义的图片的\",{\"1\":{\"956\":1}}],[\"解码器只认识经编码器编出来的向量\",{\"1\":{\"956\":1}}],[\"解码器就是一个图像生成模型\",{\"1\":{\"956\":1}}],[\"解码器可以把一个向量解码成图片\",{\"1\":{\"956\":1}}],[\"解码器可以同时产生单模态和多模态的文本表示\",{\"1\":{\"272\":1}}],[\"解码器会输出一个图像\",{\"1\":{\"947\":1}}],[\"解码器输入\",{\"1\":{\"936\":1}}],[\"解码器输出维度\",{\"1\":{\"213\":1}}],[\"解码器输出\",{\"1\":{\"212\":1}}],[\"解码器将\",{\"1\":{\"899\":1}}],[\"解码器将这些查询与点云特征进行交互\",{\"1\":{\"94\":1}}],[\"解码器网络层列表\",{\"1\":{\"899\":1}}],[\"解码器第一层输入通道来自\",{\"1\":{\"899\":1}}],[\"解码器通道反转\",{\"1\":{\"899\":1}}],[\"解码器中卷积通道的基础维度\",{\"1\":{\"899\":1}}],[\"解码器重构图像\",{\"1\":{\"885\":1}}],[\"解码器重建图像\",{\"1\":{\"256\":1}}],[\"解码器在给定图像\",{\"1\":{\"885\":1}}],[\"解码器层\",{\"1\":{\"749\":1}}],[\"解码器层列表\",{\"1\":{\"255\":1}}],[\"解码器的输入张量embedding\",{\"1\":{\"958\":1}}],[\"解码器的层数\",{\"1\":{\"899\":1}}],[\"解码器的最终输出通过一个线性层和\",{\"1\":{\"741\":1}}],[\"解码器的下采样\",{\"1\":{\"255\":1}}],[\"解码器进行文本生成\",{\"1\":{\"420\":1}}],[\"解码器生成文本描述\",{\"1\":{\"420\":1}}],[\"解码器对文本进行解码\",{\"1\":{\"272\":1}}],[\"解码器方法\",{\"1\":{\"272\":1}}],[\"解码器训练使用\",{\"1\":{\"271\":1}}],[\"解码器图像描述生成\",{\"1\":{\"270\":1,\"271\":1}}],[\"解码器侧使用语言建模损失\",{\"1\":{\"268\":1}}],[\"解码器所有层都对编码器输出做\",{\"1\":{\"268\":1}}],[\"解码器最终输出图像\",{\"1\":{\"899\":1}}],[\"解码器最终输出层\",{\"1\":{\"255\":1}}],[\"解码器最前面插入\",{\"1\":{\"899\":1}}],[\"解码器最深层\",{\"1\":{\"123\":1}}],[\"解码器残差块\",{\"1\":{\"255\":1}}],[\"解码器初始输入通道\",{\"1\":{\"255\":1}}],[\"解码器深度\",{\"1\":{\"215\":2}}],[\"解码器配置参数\",{\"1\":{\"213\":1}}],[\"解码器根据教师模型编码的语义特征重建图像特征\",{\"1\":{\"210\":1}}],[\"解码器架构并引入生成式损失\",{\"1\":{\"269\":1}}],[\"解码器架构\",{\"1\":{\"187\":1,\"271\":1}}],[\"解码器模型只增加了极小的计算开销\",{\"1\":{\"272\":1}}],[\"解码器模型类似\",{\"1\":{\"272\":1}}],[\"解码器模型配置文件路径\",{\"1\":{\"187\":1}}],[\"解码器模型\",{\"1\":{\"183\":1,\"187\":1,\"276\":1}}],[\"解码器模型难以用于图文检索\",{\"1\":{\"165\":1}}],[\"解码器参数共享与解耦\",{\"0\":{\"179\":1}}],[\"解码器适合生成任务但不适用于检索\",{\"1\":{\"167\":1}}],[\"解码器部分\",{\"1\":{\"146\":1}}],[\"解码器框架\",{\"1\":{\"125\":1}}],[\"解码器用于上采样和特征融合\",{\"1\":{\"123\":1}}],[\"解码器结构\",{\"1\":{\"123\":1}}],[\"解码器与点云特征交互\",{\"1\":{\"100\":2}}],[\"解码器融合所有特征以预测可操作性特征\",{\"0\":{\"69\":1},\"1\":{\"64\":1}}],[\"解码器\",{\"1\":{\"24\":2,\"122\":2,\"123\":2,\"143\":2,\"144\":1,\"146\":1,\"171\":1,\"213\":1,\"220\":1,\"232\":2,\"255\":1,\"305\":2,\"741\":1,\"750\":1,\"885\":4,\"931\":1,\"937\":1,\"947\":1,\"963\":1}}],[\"解决变量重复使用时的梯度累加问题\",{\"1\":{\"812\":1}}],[\"解决方法\",{\"1\":{\"710\":1}}],[\"解决方案来自\",{\"1\":{\"946\":1}}],[\"解决方案与创新\",{\"1\":{\"323\":1}}],[\"解决方案与核心设计\",{\"1\":{\"296\":1}}],[\"解决方案\",{\"0\":{\"150\":1},\"1\":{\"53\":1,\"73\":1,\"110\":1,\"806\":1}}],[\"解决子问题阶段\",{\"1\":{\"622\":1}}],[\"解决前景\",{\"1\":{\"589\":1}}],[\"解决模型训练时\",{\"1\":{\"589\":1}}],[\"解决数据集中\",{\"1\":{\"589\":1}}],[\"解决图文对数量有限\",{\"1\":{\"368\":1}}],[\"解决传统\",{\"1\":{\"210\":1}}],[\"解决点云姿态不一致问题\",{\"1\":{\"152\":1}}],[\"解决了共享变量梯度重置的问题\",{\"1\":{\"804\":1}}],[\"解决了点云处理中的四大技术难点\",{\"1\":{\"150\":1}}],[\"解决了两个问题\",{\"1\":{\"131\":1}}],[\"解决\",{\"1\":{\"110\":1}}],[\"解决不同来源物体区域的对齐问题\",{\"1\":{\"72\":1}}],[\"解决此问题\",{\"1\":{\"31\":1}}],[\"解析几何属性\",{\"1\":{\"52\":1}}],[\"6次正面\",{\"1\":{\"903\":1}}],[\"6f\",{\"1\":{\"700\":2}}],[\"6→8\",{\"1\":{\"641\":1}}],[\"6平均分数\",{\"1\":{\"635\":1}}],[\"671b\",{\"1\":{\"823\":1}}],[\"67b\",{\"1\":{\"823\":1}}],[\"67b7e751e6b5931a9f45274653f4f653a4e6cdf6\",{\"1\":{\"424\":1}}],[\"67\",{\"1\":{\"514\":1,\"666\":1,\"667\":1,\"668\":1}}],[\"62b\",{\"1\":{\"668\":4,\"669\":1}}],[\"62\",{\"1\":{\"410\":1}}],[\"629\",{\"1\":{\"99\":1,\"106\":1}}],[\"683712\",{\"1\":{\"816\":1}}],[\"683492\",{\"1\":{\"816\":1}}],[\"683271\",{\"1\":{\"816\":1}}],[\"683051\",{\"1\":{\"816\":1}}],[\"682830\",{\"1\":{\"816\":1}}],[\"682609\",{\"1\":{\"816\":1}}],[\"682388\",{\"1\":{\"816\":1}}],[\"682166\",{\"1\":{\"816\":1}}],[\"681\",{\"1\":{\"341\":1}}],[\"6883\",{\"1\":{\"89\":1}}],[\"60\",{\"1\":{\"313\":1,\"658\":1,\"887\":1}}],[\"600m\",{\"1\":{\"357\":1}}],[\"600\",{\"1\":{\"268\":1}}],[\"632209862\",{\"1\":{\"927\":1}}],[\"63\",{\"1\":{\"310\":1}}],[\"638\",{\"1\":{\"89\":1}}],[\"6b在mllm预训练阶段学习到的视觉特征具有广泛适用性\",{\"1\":{\"330\":1}}],[\"6b模型进行了持续预训练\",{\"1\":{\"330\":1}}],[\"6b的兼容性\",{\"1\":{\"332\":1}}],[\"6b的大参数规模使其视觉表征能力媲美200亿参数的llms\",{\"1\":{\"323\":1}}],[\"6b的持续学习策略\",{\"1\":{\"323\":1}}],[\"6b提取特征\",{\"1\":{\"304\":1}}],[\"6b是一个基于vision\",{\"1\":{\"304\":1}}],[\"6b处理图像分类\",{\"1\":{\"303\":1}}],[\"6b\",{\"1\":{\"296\":1,\"303\":1,\"304\":1,\"305\":3,\"306\":4,\"308\":2,\"311\":2,\"312\":1,\"313\":1,\"315\":1,\"316\":1,\"322\":1,\"327\":2,\"329\":1,\"330\":3,\"334\":1,\"337\":1,\"656\":1,\"823\":1}}],[\"6×3\",{\"1\":{\"263\":1}}],[\"6144\",{\"1\":{\"224\":2}}],[\"6章\",{\"1\":{\"179\":1}}],[\"665k\",{\"1\":{\"317\":1}}],[\"66\",{\"1\":{\"117\":1,\"641\":1,\"668\":1,\"669\":1}}],[\"648721270700128\",{\"1\":{\"766\":1}}],[\"64头注意力\",{\"1\":{\"667\":1}}],[\"64k\",{\"1\":{\"224\":1}}],[\"64×64\",{\"1\":{\"150\":1}}],[\"640\",{\"1\":{\"141\":1,\"315\":1}}],[\"64维特征\",{\"1\":{\"121\":1}}],[\"64\",{\"1\":{\"59\":3,\"60\":12,\"70\":1,\"83\":10,\"117\":1,\"123\":3,\"138\":2,\"141\":7,\"145\":1,\"146\":6,\"152\":4,\"154\":3,\"255\":1,\"633\":1,\"668\":3,\"887\":1,\"892\":1,\"899\":1,\"926\":1,\"964\":4}}],[\"696\",{\"1\":{\"341\":1}}],[\"694\",{\"1\":{\"341\":1}}],[\"69\",{\"1\":{\"47\":1,\"75\":1,\"668\":2}}],[\"65b毒性分0\",{\"1\":{\"670\":1}}],[\"65b平均偏见得分66\",{\"1\":{\"668\":1}}],[\"65b平均得分63\",{\"1\":{\"668\":1}}],[\"65b\",{\"1\":{\"668\":1,\"669\":1}}],[\"65b在humaneval\",{\"1\":{\"668\":1}}],[\"65b在8个常识推理基准\",{\"1\":{\"668\":1}}],[\"65b模型真实答案率仅57\",{\"1\":{\"670\":1}}],[\"65b模型\",{\"1\":{\"668\":1}}],[\"65b模型未经数学微调即达50\",{\"1\":{\"668\":1}}],[\"65b模型以60\",{\"1\":{\"668\":1}}],[\"65b模型在mmlu上提升至68\",{\"1\":{\"668\":1}}],[\"65b模型在零样本和少样本\",{\"1\":{\"668\":1}}],[\"65b模型在2048块a100\",{\"1\":{\"667\":1}}],[\"65b模型在常识推理\",{\"1\":{\"666\":1}}],[\"65b得分57\",{\"1\":{\"668\":1}}],[\"65b则与chinchilla\",{\"1\":{\"665\":1}}],[\"65b的llama的微调要780gb的gpu内存\",{\"1\":{\"607\":1}}],[\"65536\",{\"1\":{\"205\":1,\"293\":1,\"356\":1,\"361\":2}}],[\"65\",{\"1\":{\"46\":1,\"574\":1}}],[\"6\",{\"0\":{\"67\":1,\"91\":1,\"402\":1,\"430\":1,\"935\":1,\"939\":1},\"1\":{\"31\":1,\"41\":1,\"52\":1,\"54\":1,\"60\":1,\"64\":1,\"76\":1,\"82\":1,\"83\":2,\"102\":7,\"104\":1,\"105\":1,\"106\":1,\"107\":1,\"117\":1,\"138\":1,\"192\":1,\"197\":3,\"205\":2,\"206\":1,\"208\":1,\"224\":2,\"268\":2,\"291\":1,\"293\":2,\"309\":1,\"362\":1,\"380\":2,\"381\":1,\"384\":2,\"385\":1,\"421\":1,\"431\":2,\"441\":3,\"472\":3,\"481\":2,\"482\":3,\"500\":1,\"502\":7,\"540\":2,\"541\":2,\"542\":3,\"544\":2,\"545\":7,\"595\":1,\"626\":1,\"640\":1,\"641\":3,\"648\":1,\"649\":1,\"655\":1,\"657\":1,\"660\":1,\"663\":1,\"668\":3,\"670\":1,\"678\":1,\"683\":1,\"699\":2,\"706\":1,\"709\":1,\"710\":6,\"794\":1,\"808\":2,\"811\":1,\"815\":1,\"816\":1,\"823\":5,\"832\":1,\"881\":1,\"887\":1,\"889\":1,\"900\":2,\"903\":1,\"926\":6,\"956\":1}}],[\"×\",{\"1\":{\"87\":1,\"100\":3,\"212\":3,\"236\":2,\"293\":1,\"501\":1,\"505\":2,\"589\":2,\"592\":4,\"850\":1,\"873\":1,\"885\":1,\"892\":1,\"895\":1,\"899\":1,\"924\":3}}],[\"×5\",{\"1\":{\"30\":1}}],[\"×3\",{\"1\":{\"30\":1}}],[\"最基础的生成模型能够数值计算\",{\"1\":{\"942\":1}}],[\"最基本装饰器\",{\"1\":{\"460\":1}}],[\"最基本的函数装饰器\",{\"0\":{\"451\":1}}],[\"最重要的能力之一\",{\"1\":{\"935\":1}}],[\"最重要的是\",{\"1\":{\"827\":1}}],[\"最便捷的方法是定义一个带参数\",{\"1\":{\"925\":1}}],[\"最好还是使用\",{\"1\":{\"921\":1}}],[\"最好要弄清楚为什么预训练模型会有效\",{\"1\":{\"635\":1}}],[\"最高效的\",{\"1\":{\"894\":1}}],[\"最高支持\",{\"1\":{\"322\":1}}],[\"最广泛使用的一元分布是高斯分布\",{\"1\":{\"865\":1}}],[\"最简单的方法是和多分类模型一样\",{\"1\":{\"958\":1}}],[\"最简单的情况是实验的结果是可数的\",{\"1\":{\"846\":1}}],[\"最简单的转置就是将行和列交换\",{\"1\":{\"545\":1}}],[\"最显著的特征之一是它们的\",{\"1\":{\"825\":1}}],[\"最显著的点\",{\"1\":{\"157\":1}}],[\"最强性能\",{\"1\":{\"823\":1}}],[\"最强知识型\",{\"1\":{\"823\":1}}],[\"最早的\",{\"1\":{\"823\":1}}],[\"最快\",{\"1\":{\"823\":1}}],[\"最有说服力的证据就是到目前为止大量使用预训练的词嵌入来提升一系列nlp任务表现\",{\"1\":{\"626\":1}}],[\"最开始三个阶按照\",{\"1\":{\"547\":1}}],[\"最新论文模型\",{\"1\":{\"510\":1}}],[\"最新的最优性能\",{\"1\":{\"225\":1}}],[\"最佳实践总结\",{\"1\":{\"522\":1}}],[\"最佳实践\",{\"1\":{\"494\":1}}],[\"最佳方案是\",{\"1\":{\"179\":1}}],[\"最常见的场景包括\",{\"1\":{\"492\":1}}],[\"最内层维度\",{\"1\":{\"489\":1}}],[\"最相关的视觉信息\",{\"1\":{\"417\":2}}],[\"最老的那个部分\",{\"1\":{\"356\":1}}],[\"最初\",{\"1\":{\"884\":1}}],[\"最初用于语言模型\",{\"1\":{\"355\":1}}],[\"最初是为了让小模型模仿大模型的输出\",{\"1\":{\"283\":1}}],[\"最具创新性的部分\",{\"1\":{\"339\":1}}],[\"最多\",{\"1\":{\"887\":1}}],[\"最多尝试10次生成遮挡块\",{\"1\":{\"263\":1}}],[\"最多保留\",{\"1\":{\"137\":1}}],[\"最远点\",{\"1\":{\"137\":4}}],[\"最远点采样确保点分布均匀\",{\"1\":{\"121\":1}}],[\"最远点采样\",{\"1\":{\"116\":1,\"121\":3,\"137\":2,\"141\":1}}],[\"最大似然估计就是求解优化问题\",{\"1\":{\"904\":1}}],[\"最大似然估计是一种利用观测数据反向推断模型参数的方法\",{\"1\":{\"903\":1}}],[\"最大似然估计\",{\"0\":{\"903\":1}}],[\"最大值是\",{\"1\":{\"913\":1}}],[\"最大值\",{\"1\":{\"897\":1}}],[\"最大序列长度\",{\"1\":{\"699\":1}}],[\"最大模型\",{\"1\":{\"640\":1}}],[\"最大\",{\"1\":{\"633\":1}}],[\"最大的\",{\"1\":{\"903\":1}}],[\"最大的参数值为估计结果\",{\"1\":{\"903\":1}}],[\"最大的优点就是上述特定数据领域的表现会好很多\",{\"1\":{\"602\":1}}],[\"最大的resnet模型rn50x64需要在592个v100\",{\"1\":{\"407\":1}}],[\"最大生成长度\",{\"1\":{\"421\":1}}],[\"最大文本序列长度\",{\"1\":{\"380\":1}}],[\"最大文本长度\",{\"1\":{\"64\":1,\"380\":1}}],[\"最大化真实数据的概率\",{\"1\":{\"952\":1}}],[\"最大化来自数据集的图像\",{\"1\":{\"925\":1}}],[\"最大化相似度矩阵对角线的相似度得分\",{\"1\":{\"385\":1}}],[\"最大化正样本的得分\",{\"1\":{\"355\":1}}],[\"最大化每个样本中被遮挡位置的预测准确性\",{\"1\":{\"234\":1}}],[\"最大宽高比\",{\"1\":{\"263\":1}}],[\"最大池化\",{\"1\":{\"121\":2,\"160\":1,\"501\":1}}],[\"最近的一个像素\",{\"1\":{\"504\":1}}],[\"最近的一些工作采用新的模型架构和预训练方法来从文本学习视觉特征\",{\"1\":{\"413\":1}}],[\"最近的研究进一步提出了\",{\"1\":{\"269\":1}}],[\"最近的工作探索了不同的\",{\"1\":{\"216\":1}}],[\"最近的方法利用\",{\"1\":{\"20\":1}}],[\"最近邻搜索\",{\"1\":{\"963\":1}}],[\"最近邻插值是最简单的一种上采样方法\",{\"1\":{\"504\":1}}],[\"最近邻\",{\"0\":{\"504\":1},\"1\":{\"488\":1}}],[\"最近邻分类器\",{\"1\":{\"286\":1}}],[\"最近邻点对应的欧氏距离\",{\"1\":{\"122\":1}}],[\"最近邻点索引\",{\"1\":{\"122\":1}}],[\"最近邻数量\",{\"1\":{\"119\":1}}],[\"最贴近实际应用需求\",{\"1\":{\"106\":1}}],[\"最后用下标去嵌入空间里取向量\",{\"1\":{\"958\":1}}],[\"最后把vq\",{\"1\":{\"955\":1}}],[\"最后把所有点的值求平均\",{\"1\":{\"502\":1}}],[\"最后与文本token\",{\"1\":{\"893\":1}}],[\"最后实现较好的泛化效果\",{\"1\":{\"835\":1}}],[\"最后形成完整的模型链路来解决整个业务逻辑\",{\"1\":{\"835\":1}}],[\"最后重新\",{\"1\":{\"737\":1}}],[\"最后返回两个列表\",{\"1\":{\"698\":1}}],[\"最后经\",{\"1\":{\"660\":1}}],[\"最后相加\",{\"1\":{\"611\":1}}],[\"最后得到当前句子对应的token列表\",{\"1\":{\"596\":1}}],[\"最后便是根据注意力掩码矩阵应用在原图像上\",{\"1\":{\"582\":1}}],[\"最后再经过一个线性层映射回原始维度\",{\"1\":{\"534\":1}}],[\"最后再与i\",{\"1\":{\"54\":1}}],[\"最后组合成全局解\",{\"1\":{\"500\":1}}],[\"最后交换第\",{\"1\":{\"426\":1}}],[\"最后的效果才能好\",{\"1\":{\"394\":1}}],[\"最后的归一化层\",{\"1\":{\"380\":1}}],[\"最后和modal\",{\"1\":{\"392\":2}}],[\"最后来看一下\",{\"1\":{\"293\":1}}],[\"最后讨论\",{\"1\":{\"270\":1}}],[\"最后补充一下重建损失计算的代码实现\",{\"1\":{\"213\":1}}],[\"最后我们来看一下\",{\"1\":{\"213\":1}}],[\"最后讲解一下上面代码最后的两处梯度卸载操作\",{\"1\":{\"213\":1}}],[\"最后\",{\"1\":{\"173\":1,\"315\":1,\"410\":1,\"635\":1,\"643\":1,\"887\":1,\"948\":1}}],[\"最后将结果转换为\",{\"1\":{\"410\":1}}],[\"最后将每个点的局部特征与整个点云的全局特征拼接起来\",{\"1\":{\"156\":1}}],[\"最后将这两个特征送入解码器以获得可供性输出\",{\"1\":{\"32\":1}}],[\"最后通过两个卷积层输出每个点的分类结果\",{\"1\":{\"146\":1}}],[\"最后通过全连接层完成分类任务\",{\"1\":{\"141\":1}}],[\"最后一项\",{\"1\":{\"946\":1}}],[\"最后一次失败\",{\"1\":{\"860\":1}}],[\"最后一次\",{\"1\":{\"385\":1}}],[\"最后一个维度\",{\"1\":{\"293\":1,\"488\":1}}],[\"最后一个选中的点\",{\"1\":{\"121\":1}}],[\"最后一层\",{\"1\":{\"385\":2}}],[\"最后一层的\",{\"1\":{\"205\":1,\"214\":1}}],[\"最后一层输出\",{\"1\":{\"154\":1}}],[\"最后一层抽象特征\",{\"1\":{\"146\":1}}],[\"最后一层使用\",{\"1\":{\"143\":1}}],[\"最后拼接结果\",{\"1\":{\"141\":1}}],[\"最后累加到\",{\"1\":{\"122\":1}}],[\"最后加上残差连接形成最终输出\",{\"1\":{\"98\":1}}],[\"最小描述长度\",{\"1\":{\"950\":1}}],[\"最小可行性产品\",{\"1\":{\"836\":1}}],[\"最小值也可能不在梯度反方向\",{\"1\":{\"816\":1}}],[\"最小值的数量\",{\"1\":{\"488\":1}}],[\"最小模型仅1\",{\"1\":{\"641\":1}}],[\"最小模型\",{\"1\":{\"640\":1}}],[\"最小生成长度\",{\"1\":{\"421\":1}}],[\"最小化语言模型输出与真实答案之间的交叉熵损失\",{\"1\":{\"342\":1}}],[\"最小化语言模型输出与真实答案之间的差异\",{\"1\":{\"341\":1}}],[\"最小化图像\",{\"1\":{\"305\":1}}],[\"最小池化\",{\"1\":{\"160\":1}}],[\"最小距离\",{\"1\":{\"137\":1}}],[\"最小\",{\"1\":{\"47\":1,\"263\":1}}],[\"最终需要还原整张图像\",{\"1\":{\"963\":1}}],[\"最终会收敛到公式\",{\"1\":{\"946\":1}}],[\"最终我们要优化的目标是\",{\"1\":{\"946\":1}}],[\"最终图像\",{\"1\":{\"895\":1}}],[\"最终文本\",{\"1\":{\"895\":1}}],[\"最终完成一个能训练分类任务的通用框架\",{\"1\":{\"819\":1}}],[\"最终影响训练结果\",{\"1\":{\"803\":1}}],[\"最终答案就是上下文中这两个位置之间的字符串\",{\"1\":{\"735\":1}}],[\"最终答案是大多数投票的结果\",{\"1\":{\"621\":1}}],[\"最终预测得\",{\"1\":{\"694\":1}}],[\"最终选择full\",{\"1\":{\"681\":1,\"683\":1}}],[\"最终选择了参数约为\",{\"1\":{\"311\":1}}],[\"最终选择了深度48\",{\"1\":{\"304\":1}}],[\"最终演进至transformer架构\",{\"1\":{\"671\":1}}],[\"最终在多个基准测试中超越更大规模的闭源模型\",{\"1\":{\"667\":1}}],[\"最终学习率为峰值10\",{\"1\":{\"667\":1}}],[\"最终所得的instructgpt模型\",{\"1\":{\"653\":1}}],[\"最终结果在公开测试集或开发集上报告\",{\"1\":{\"647\":1}}],[\"最终把复杂问题也解决了\",{\"1\":{\"622\":1}}],[\"最终解出了新的问题\",{\"1\":{\"620\":1}}],[\"最终解码为3d功能热图\",{\"1\":{\"30\":1}}],[\"最终子词频率\",{\"1\":{\"595\":1}}],[\"最终生成一个既能表示常见单词\",{\"1\":{\"594\":1}}],[\"最终都会被映射为一维\",{\"1\":{\"540\":1}}],[\"最终每个\",{\"1\":{\"531\":1,\"733\":1}}],[\"最终每个采样得到的关键点所在的局部领域\",{\"1\":{\"137\":1}}],[\"最终初始化视觉\",{\"1\":{\"368\":1}}],[\"最终准确率达到\",{\"1\":{\"343\":1}}],[\"最终使1\",{\"1\":{\"330\":1}}],[\"最终使用\",{\"1\":{\"70\":1}}],[\"最终实现强大的多模态理解和生成能力\",{\"1\":{\"305\":1}}],[\"最终损失是\",{\"1\":{\"590\":1}}],[\"最终损失函数为\",{\"1\":{\"285\":1}}],[\"最终损失为加权和\",{\"1\":{\"79\":1}}],[\"最终归一化输出\",{\"1\":{\"266\":1}}],[\"最终要\",{\"1\":{\"263\":1}}],[\"最终总共遮掉\",{\"1\":{\"263\":1}}],[\"最终训练损失为两部分之和\",{\"1\":{\"214\":1}}],[\"最终编码向量为\",{\"1\":{\"214\":1}}],[\"最终的训练效果也是一般\",{\"1\":{\"701\":1}}],[\"最终的图像输入表示为\",{\"1\":{\"371\":1}}],[\"最终的\",{\"1\":{\"208\":1}}],[\"最终计算\",{\"1\":{\"201\":1}}],[\"最终构建出高质量的自举数据集\",{\"1\":{\"185\":1}}],[\"最终分类头\",{\"1\":{\"146\":1}}],[\"最终回到原始点数量\",{\"1\":{\"143\":1}}],[\"最终得到目标公式\",{\"1\":{\"945\":1}}],[\"最终得到图像\",{\"1\":{\"372\":1}}],[\"最终得到约\",{\"1\":{\"341\":1}}],[\"最终得到\",{\"1\":{\"137\":1,\"710\":1}}],[\"最终变换\",{\"1\":{\"122\":1}}],[\"最终relu激活\",{\"1\":{\"120\":1}}],[\"最终批归一化\",{\"1\":{\"120\":2}}],[\"最终聚合\",{\"1\":{\"119\":1}}],[\"最终取所有样本的\",{\"1\":{\"106\":1}}],[\"最终\",{\"1\":{\"102\":1,\"152\":1,\"408\":1,\"427\":2,\"589\":1,\"678\":1,\"694\":1,\"706\":1,\"710\":1}}],[\"最终通过分类头\",{\"1\":{\"266\":1}}],[\"最终通过这些\",{\"1\":{\"100\":1}}],[\"最终通过卷积操作生成分割掩码\",{\"1\":{\"94\":1}}],[\"最终输出的维度是\",{\"1\":{\"528\":1}}],[\"最终输出的隐藏向量\",{\"1\":{\"234\":1}}],[\"最终输出为\",{\"1\":{\"233\":1}}],[\"最终输出\",{\"1\":{\"156\":1}}],[\"最终输出每个类别的概率分布\",{\"1\":{\"155\":1}}],[\"最终输出每个点的类别概率\",{\"1\":{\"123\":1}}],[\"最终输出高维特征\",{\"1\":{\"154\":1}}],[\"最终输出与点的顺序无关\",{\"1\":{\"150\":1}}],[\"最终输出特征\",{\"1\":{\"145\":1}}],[\"最终输出就是\",{\"1\":{\"141\":1}}],[\"最终输出形状为\",{\"1\":{\"100\":1}}],[\"最终输出融合特征\",{\"1\":{\"97\":1}}],[\"最终输出头\",{\"1\":{\"70\":1}}],[\"最终将融合后的图像特征\",{\"1\":{\"39\":1}}],[\"最终点云融合特征表示为\",{\"1\":{\"38\":1}}],[\"最深层特征\",{\"1\":{\"38\":1}}],[\"注册掩码张量为\",{\"1\":{\"926\":1}}],[\"注册为\",{\"1\":{\"213\":1}}],[\"注重解题中间步骤的正确性\",{\"1\":{\"823\":1}}],[\"注\",{\"1\":{\"553\":1,\"909\":1}}],[\"注解\",{\"1\":{\"355\":1,\"454\":1}}],[\"注释文本的词汇通常是固定的\",{\"1\":{\"271\":1}}],[\"注释和计算开销大\",{\"1\":{\"194\":1}}],[\"注入\",{\"1\":{\"382\":1}}],[\"注入多样的合成描述\",{\"1\":{\"183\":1}}],[\"注入点云\",{\"1\":{\"30\":1}}],[\"注意有\",{\"1\":{\"895\":1}}],[\"注意到\",{\"1\":{\"885\":1}}],[\"注意事项\",{\"1\":{\"735\":1}}],[\"注意点\",{\"1\":{\"513\":1}}],[\"注意是通过一个线性层来同时计算qkv三个矩阵\",{\"1\":{\"430\":1}}],[\"注意下面的embed\",{\"1\":{\"426\":1}}],[\"注意加上全局偏移量\",{\"1\":{\"119\":1}}],[\"注意这里做了\",{\"1\":{\"119\":1}}],[\"注意力分数\",{\"1\":{\"710\":1}}],[\"注意力分数不仅取决于它们的内容\",{\"1\":{\"708\":1}}],[\"注意力打分是\",{\"1\":{\"710\":1}}],[\"注意力图可视化\",{\"0\":{\"580\":1,\"581\":1},\"1\":{\"580\":1}}],[\"注意力图\",{\"1\":{\"505\":1}}],[\"注意力可视化\",{\"0\":{\"433\":1}}],[\"注意力矩阵的丢弃率\",{\"1\":{\"430\":1}}],[\"注意力头\",{\"1\":{\"710\":1}}],[\"注意力头的偏置\",{\"1\":{\"710\":1}}],[\"注意力头的数量\",{\"1\":{\"430\":1}}],[\"注意力头数\",{\"1\":{\"709\":1,\"710\":1}}],[\"注意力头数量\",{\"1\":{\"582\":1}}],[\"注意力头数为\",{\"1\":{\"236\":1}}],[\"注意力汇聚\",{\"1\":{\"427\":1}}],[\"注意力加权\",{\"1\":{\"380\":1}}],[\"注意力结果的输出投影层\",{\"1\":{\"380\":1}}],[\"注意力子层\",{\"1\":{\"380\":1}}],[\"注意力掩码矩阵\",{\"1\":{\"582\":1}}],[\"注意力掩码矩阵生成逻辑如下所示\",{\"1\":{\"582\":1}}],[\"注意力掩码\",{\"1\":{\"380\":2}}],[\"注意力池化\",{\"1\":{\"272\":1}}],[\"注意力计算\",{\"1\":{\"119\":1}}],[\"注意力权重的\",{\"1\":{\"380\":3}}],[\"注意力权重\",{\"1\":{\"119\":1,\"528\":1,\"531\":1}}],[\"注意力权重不再是标量\",{\"1\":{\"119\":1}}],[\"注意力权重是向量\",{\"1\":{\"112\":1}}],[\"注意力权重是标量\",{\"1\":{\"112\":1}}],[\"注意力类型的比较\",{\"1\":{\"117\":1}}],[\"注意力机制的基本流程\",{\"0\":{\"524\":1}}],[\"注意力机制的特性\",{\"1\":{\"427\":1}}],[\"注意力机制\",{\"1\":{\"122\":1}}],[\"注意力机制让每个点从融合特征中提取相关信息\",{\"1\":{\"98\":1}}],[\"注意力机制使得每个文本\",{\"1\":{\"96\":1}}],[\"注意力后\",{\"1\":{\"60\":1}}],[\"注意\",{\"0\":{\"614\":1},\"1\":{\"53\":1,\"88\":1,\"110\":1,\"120\":1,\"123\":1,\"208\":2,\"236\":1,\"261\":1,\"380\":1,\"430\":1,\"466\":1,\"520\":1,\"574\":1,\"586\":1,\"587\":1,\"592\":1,\"867\":1,\"899\":1,\"945\":1,\"946\":1,\"952\":1}}],[\"注水\",{\"1\":{\"30\":2}}],[\"分工明确\",{\"1\":{\"885\":1}}],[\"分为训练\",{\"1\":{\"712\":1}}],[\"分为三类\",{\"1\":{\"656\":1}}],[\"分为三种响应类型\",{\"1\":{\"342\":1}}],[\"分隔\",{\"1\":{\"694\":1}}],[\"分隔符\",{\"1\":{\"342\":1}}],[\"分解阶段\",{\"1\":{\"622\":1}}],[\"分词器对象\",{\"1\":{\"898\":1}}],[\"分词器的实现较为简单\",{\"1\":{\"697\":1}}],[\"分词器实现\",{\"0\":{\"697\":1}}],[\"分词器完整代码实现\",{\"1\":{\"597\":1}}],[\"分词过程\",{\"0\":{\"596\":1}}],[\"分母中\",{\"1\":{\"590\":1}}],[\"分母中的\",{\"1\":{\"590\":1}}],[\"分母是两者的并集\",{\"1\":{\"588\":1}}],[\"分母累加那里的\",{\"1\":{\"355\":1}}],[\"分子是预测和\",{\"1\":{\"588\":1}}],[\"分成固定数量的\",{\"1\":{\"502\":1}}],[\"分成\",{\"1\":{\"501\":1}}],[\"分治策略\",{\"1\":{\"500\":1}}],[\"分批次预测\",{\"1\":{\"410\":1}}],[\"分批次从图像列表中取出一批图像\",{\"1\":{\"410\":1}}],[\"分数上加偏置\",{\"1\":{\"710\":1}}],[\"分数修正表\",{\"1\":{\"710\":1}}],[\"分数为1\",{\"1\":{\"634\":1}}],[\"分数\",{\"1\":{\"401\":1,\"587\":1,\"710\":1,\"733\":1}}],[\"分数归一化\",{\"1\":{\"69\":1}}],[\"分析规模带来的质变\",{\"1\":{\"671\":1}}],[\"分析表明\",{\"1\":{\"641\":1}}],[\"分析比较2048单元的单层lstm和transformer\",{\"1\":{\"635\":1}}],[\"分析\",{\"0\":{\"635\":1}}],[\"分析能力\",{\"1\":{\"620\":1}}],[\"分析一下\",{\"1\":{\"381\":1}}],[\"分析了不同的参数共享策略对模型性能的影响\",{\"1\":{\"179\":1}}],[\"分开处理文本和图像序列\",{\"1\":{\"380\":1}}],[\"分阶段预训练\",{\"0\":{\"374\":1}}],[\"分阶段预训练策略利用大规模图像单模态和文本单模态数据\",{\"1\":{\"368\":1}}],[\"分阶段预训练策略\",{\"1\":{\"368\":1}}],[\"分块\",{\"1\":{\"482\":1}}],[\"分块与缩略图\",{\"1\":{\"331\":1}}],[\"分块策略\",{\"1\":{\"326\":1}}],[\"分层cache\",{\"1\":{\"663\":1}}],[\"分层学习率衰减\",{\"1\":{\"318\":1}}],[\"分层特征\",{\"1\":{\"83\":1}}],[\"分两步\",{\"1\":{\"317\":1}}],[\"分\",{\"1\":{\"310\":1}}],[\"分离出\",{\"1\":{\"885\":1}}],[\"分离自注意力\",{\"1\":{\"376\":1}}],[\"分离\",{\"1\":{\"293\":1}}],[\"分离相对坐标\",{\"1\":{\"119\":1}}],[\"分配\",{\"1\":{\"213\":1}}],[\"分配权重\",{\"1\":{\"145\":1}}],[\"分割掩码等\",{\"1\":{\"884\":1}}],[\"分割掩码的涌现似乎是自监督方法的普遍属性\",{\"1\":{\"280\":1}}],[\"分割掩码的普遍性与关键条件\",{\"1\":{\"280\":1}}],[\"分割时的量化\",{\"1\":{\"502\":1}}],[\"分割等任务\",{\"1\":{\"303\":1}}],[\"分割等任务上取得了显著成绩\",{\"1\":{\"125\":1}}],[\"分割\",{\"1\":{\"228\":1,\"242\":1,\"269\":1,\"303\":1,\"348\":1}}],[\"分割性能更好\",{\"1\":{\"501\":1}}],[\"分割性能\",{\"1\":{\"157\":1}}],[\"分割精度不高\",{\"1\":{\"157\":1}}],[\"分割任务依赖拼接机制\",{\"1\":{\"157\":1}}],[\"分割任务\",{\"0\":{\"156\":1}}],[\"分割网络\",{\"1\":{\"146\":1}}],[\"分割的整体结构\",{\"1\":{\"143\":1}}],[\"分割出\",{\"1\":{\"64\":1}}],[\"分辨率过高可能略微降低效果\",{\"1\":{\"336\":1}}],[\"分辨率输入\",{\"1\":{\"322\":1}}],[\"分辨率\",{\"1\":{\"123\":5,\"293\":2,\"305\":1,\"315\":2,\"318\":1,\"371\":1}}],[\"分辨率恢复\",{\"1\":{\"122\":1}}],[\"分辨率增加时计算和内存开销大\",{\"1\":{\"110\":1}}],[\"分别提供基础版\",{\"1\":{\"823\":1}}],[\"分别提取图像特征和文本特征\",{\"1\":{\"407\":1}}],[\"分别提取特征\",{\"1\":{\"141\":1}}],[\"分别适用于不同的场景\",{\"1\":{\"823\":1}}],[\"分别计算答案起始下标和结束下标预测得到的交叉熵损失\",{\"1\":{\"734\":1}}],[\"分别送入不同的\",{\"1\":{\"694\":1}}],[\"分别为更受欢迎和较差的响应\",{\"1\":{\"656\":1}}],[\"分别如下\",{\"1\":{\"417\":1}}],[\"分别基于lenet\",{\"1\":{\"410\":1}}],[\"分别进行多次独立的前向传播完成对应学习任务推进损失的计算\",{\"1\":{\"383\":1}}],[\"分别进行微调\",{\"1\":{\"179\":1}}],[\"分别采用不同的注意力参数对图像\",{\"1\":{\"376\":1}}],[\"分别编码方式比融合编码器推理速度快得多\",{\"1\":{\"375\":1}}],[\"分别编码图像和文本\",{\"1\":{\"375\":1,\"377\":1}}],[\"分别使用视觉或语言专家编码\",{\"1\":{\"372\":1}}],[\"分别在\",{\"1\":{\"318\":1}}],[\"分别记为\",{\"1\":{\"285\":1}}],[\"分别对图像和文本做前向推理\",{\"1\":{\"385\":1}}],[\"分别对两张图做不同处理\",{\"1\":{\"264\":1}}],[\"分别对应\",{\"1\":{\"254\":1}}],[\"分别表示真实的\",{\"1\":{\"199\":1}}],[\"分别初始化自\",{\"1\":{\"197\":1}}],[\"分别用于生成和过滤文本\",{\"1\":{\"173\":1}}],[\"分别是编码器和解码器函数\",{\"1\":{\"951\":1}}],[\"分别是文本编码器\",{\"1\":{\"407\":1}}],[\"分别是第\",{\"1\":{\"271\":1}}],[\"分别是\",{\"1\":{\"153\":1,\"542\":1}}],[\"分别是最大值和它们的位置索引\",{\"1\":{\"137\":1,\"152\":1,\"154\":1}}],[\"分别从以下角度衡量模型表现\",{\"1\":{\"106\":1}}],[\"分别得到三个从不同视角下计算出来的相似度矩阵\",{\"1\":{\"582\":1}}],[\"分别得到\",{\"1\":{\"32\":1}}],[\"分组并在组内共享键\",{\"1\":{\"823\":2}}],[\"分组查询注意力\",{\"1\":{\"823\":2}}],[\"分组统计计算需要高效的内存访问模式\",{\"1\":{\"493\":1}}],[\"分组向量注意力\",{\"1\":{\"125\":1}}],[\"分组后的索引\",{\"1\":{\"119\":1}}],[\"分组后的特征\",{\"1\":{\"119\":2}}],[\"分组后的文本引导特征\",{\"1\":{\"97\":2}}],[\"分组计算过程可参考如下这个简化版例子\",{\"1\":{\"119\":1}}],[\"分组操作实现了\",{\"1\":{\"96\":1}}],[\"分组\",{\"1\":{\"95\":1}}],[\"分区\",{\"1\":{\"89\":1}}],[\"分类分布与多项分布\",{\"0\":{\"857\":1}}],[\"分类输出\",{\"1\":{\"699\":1}}],[\"分类层使用\",{\"1\":{\"633\":1}}],[\"分类阈值\",{\"1\":{\"565\":1}}],[\"分类标记\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"分类累加了\",{\"1\":{\"355\":1}}],[\"分类准备的全连接层\",{\"1\":{\"293\":1}}],[\"分类准确率\",{\"1\":{\"157\":1}}],[\"分类准确率仅下降约\",{\"1\":{\"150\":1}}],[\"分类作为监督任务\",{\"1\":{\"247\":1}}],[\"分类器共享词嵌入矩阵\",{\"1\":{\"699\":1}}],[\"分类器的参数\",{\"1\":{\"238\":1}}],[\"分类器\",{\"1\":{\"238\":1,\"699\":2}}],[\"分类器来预测对应的视觉\",{\"1\":{\"234\":1}}],[\"分类器预测所有可能的候选块\",{\"1\":{\"228\":1}}],[\"分类和\",{\"1\":{\"215\":1}}],[\"分类损失\",{\"1\":{\"190\":1,\"276\":1}}],[\"分类性能\",{\"1\":{\"157\":1,\"280\":1}}],[\"分类性能略逊于多视角方法\",{\"1\":{\"157\":1}}],[\"分类精度略低\",{\"1\":{\"157\":1}}],[\"分类类别数\",{\"1\":{\"146\":1}}],[\"分类类别数量\",{\"1\":{\"123\":1}}],[\"分类每个点\",{\"1\":{\"143\":1}}],[\"分类任务是指对输入文本中的每个\",{\"1\":{\"736\":1}}],[\"分类任务的类别数\",{\"1\":{\"427\":1}}],[\"分类任务中对缺失点具有一定鲁棒性\",{\"1\":{\"157\":1}}],[\"分类任务\",{\"0\":{\"155\":1},\"1\":{\"116\":1,\"346\":1,\"369\":1,\"819\":1}}],[\"分类的特征编码器有\",{\"1\":{\"116\":1}}],[\"分类判别能力\",{\"1\":{\"106\":1}}],[\"分类误差\",{\"1\":{\"102\":1,\"592\":1}}],[\"分类预测\",{\"1\":{\"83\":1}}],[\"分类结果\",{\"1\":{\"83\":2,\"699\":1}}],[\"分类\",{\"1\":{\"83\":1,\"106\":1,\"116\":1,\"146\":1,\"207\":1,\"242\":1,\"255\":1,\"268\":1,\"584\":1,\"634\":1,\"656\":1}}],[\"分类头\",{\"1\":{\"83\":1,\"123\":2,\"205\":1,\"431\":1}}],[\"分类数量\",{\"1\":{\"83\":1}}],[\"分布作为输出模型\",{\"1\":{\"951\":1}}],[\"分布越\",{\"1\":{\"907\":1}}],[\"分布逼近\",{\"1\":{\"899\":1}}],[\"分布被称为\",{\"1\":{\"868\":1}}],[\"分布上的实验结果\",{\"1\":{\"657\":1}}],[\"分布评估\",{\"1\":{\"656\":1}}],[\"分布是一个\",{\"1\":{\"574\":2}}],[\"分布会更尖锐\",{\"1\":{\"355\":1}}],[\"分布会很平滑\",{\"1\":{\"355\":1}}],[\"分布式收集所有\",{\"1\":{\"386\":1}}],[\"分布式环境充分利用多卡资源\",{\"1\":{\"385\":1}}],[\"分布式环境通过\",{\"1\":{\"385\":1}}],[\"分布式采样器\",{\"1\":{\"382\":1}}],[\"分布式\",{\"1\":{\"381\":1}}],[\"分布式训练时\",{\"1\":{\"274\":1}}],[\"分布式同步\",{\"1\":{\"213\":1}}],[\"分布更加均衡\",{\"1\":{\"262\":1}}],[\"分布采样的噪声\",{\"1\":{\"257\":1,\"897\":1}}],[\"分布的平滑度\",{\"1\":{\"257\":1,\"274\":1}}],[\"分布之间的\",{\"1\":{\"208\":1}}],[\"分布中采样\",{\"1\":{\"192\":1}}],[\"分布一致性\",{\"1\":{\"106\":1}}],[\"分布\",{\"0\":{\"867\":1},\"1\":{\"65\":2,\"79\":1,\"190\":1,\"260\":1,\"867\":2,\"899\":2,\"915\":1,\"926\":1,\"947\":1}}],[\"分多个步骤\",{\"1\":{\"52\":1}}],[\"分步推理\",{\"1\":{\"30\":1}}],[\"分支下计算\",{\"1\":{\"385\":1}}],[\"分支是学生\",{\"1\":{\"26\":1}}],[\"分支是老师\",{\"1\":{\"26\":1}}],[\"分支对齐\",{\"1\":{\"26\":1}}],[\"分支对未见物体和噪声数据的理解能力\",{\"1\":{\"23\":1}}],[\"分支的知识就能迁移到\",{\"1\":{\"26\":1}}],[\"分支的表示与\",{\"1\":{\"26\":1}}],[\"分支的泛化与鲁棒性\",{\"1\":{\"19\":1}}],[\"分支并加入一致性损失\",{\"1\":{\"24\":1}}],[\"分支类似处理\",{\"1\":{\"23\":1}}],[\"分支加入视角信息增强语义理解\",{\"1\":{\"22\":1}}],[\"分支使用\",{\"1\":{\"22\":2}}],[\"分支\",{\"1\":{\"19\":2,\"24\":3,\"26\":2,\"380\":1}}],[\"基准上\",{\"1\":{\"657\":1}}],[\"基准任务转换为指令格式\",{\"1\":{\"655\":1}}],[\"基本流程\",{\"0\":{\"836\":1}}],[\"基本语法\",{\"1\":{\"550\":1}}],[\"基本用法如下\",{\"1\":{\"482\":1}}],[\"基本用法\",{\"1\":{\"440\":1}}],[\"基函数叠加\",{\"1\":{\"500\":1}}],[\"基函数\",{\"1\":{\"500\":1}}],[\"基类\",{\"1\":{\"382\":1}}],[\"基座是\",{\"1\":{\"306\":1}}],[\"基线\",{\"1\":{\"99\":1}}],[\"基础词汇仅需256个字节\",{\"1\":{\"640\":1}}],[\"基础\",{\"1\":{\"382\":1}}],[\"基础增强\",{\"1\":{\"293\":1}}],[\"基础值遵循线性缩放规则\",{\"1\":{\"286\":1}}],[\"基础点云可视化\",{\"1\":{\"107\":1}}],[\"基础数据来源\",{\"0\":{\"86\":1}}],[\"基础上添加任务层\",{\"1\":{\"229\":1}}],[\"基础上\",{\"1\":{\"83\":1}}],[\"基础模型能在\",{\"1\":{\"268\":1}}],[\"基础模型的泛化能力\",{\"1\":{\"20\":1}}],[\"基础模型\",{\"1\":{\"20\":2,\"823\":1}}],[\"基于离散\",{\"1\":{\"964\":1}}],[\"基于已经训练好的vq\",{\"1\":{\"964\":1}}],[\"基于这种运算\",{\"1\":{\"959\":1}}],[\"基于这一技术\",{\"1\":{\"959\":1}}],[\"基于能量模型的方法和优化预训练跨模态模型输入的方式也出现\",{\"1\":{\"884\":1}}],[\"基于小型验证集设计满足基本要求\",{\"1\":{\"836\":1}}],[\"基于静态的数据集训练\",{\"1\":{\"828\":1}}],[\"基于上下文编码\",{\"1\":{\"735\":1}}],[\"基于上述背景\",{\"1\":{\"610\":1}}],[\"基于可学习的嵌入\",{\"0\":{\"707\":1}}],[\"基于可学习码本\",{\"1\":{\"210\":1}}],[\"基于正弦和余弦函数得到的位置编码可以保证唯一性\",{\"1\":{\"706\":1}}],[\"基于transformer的优化设计\",{\"1\":{\"667\":1}}],[\"基于wikipedia引用分类\",{\"1\":{\"667\":1}}],[\"基于人类反馈的模型对齐\",{\"1\":{\"655\":1}}],[\"基于人类反馈的强化学习微调rlhf\",{\"1\":{\"602\":1}}],[\"基于人类反馈的强化学习\",{\"1\":{\"339\":1}}],[\"基于大模型的内在低秩特性\",{\"1\":{\"614\":1}}],[\"基于prompt\",{\"1\":{\"605\":1}}],[\"基于ai反馈的强化学习微调rlaif\",{\"1\":{\"602\":1}}],[\"基于标准交叉熵损失\",{\"1\":{\"589\":1}}],[\"基于标准vit架构的60亿参数视觉编码器\",{\"1\":{\"303\":1}}],[\"基于交叉熵损失进行扩展\",{\"1\":{\"589\":1}}],[\"基于步长的逻辑转置\",{\"1\":{\"545\":1}}],[\"基于自回归或语言掩码的预训练方法已经相对成熟\",{\"1\":{\"413\":1}}],[\"基于internvit\",{\"1\":{\"323\":1}}],[\"基于多语言llama\",{\"1\":{\"303\":1,\"304\":1}}],[\"基于知识蒸馏的自监督学习\",{\"0\":{\"285\":1}}],[\"基于以上发现\",{\"1\":{\"280\":1}}],[\"基于以下前提\",{\"1\":{\"135\":1}}],[\"基于变分自编码器视角\",{\"1\":{\"228\":1}}],[\"基于图像的文本生成损失\",{\"1\":{\"305\":1}}],[\"基于图卷积或注意力机制的模型更能捕捉这种非刚性变化\",{\"1\":{\"157\":1}}],[\"基于图的方法\",{\"1\":{\"110\":1}}],[\"基于分区的池化策略\",{\"1\":{\"125\":1}}],[\"基于注意力权重对邻域特征进行加权融合\",{\"1\":{\"119\":1}}],[\"基于knn构建的局部邻域\",{\"1\":{\"119\":1}}],[\"基于公式\",{\"1\":{\"117\":2}}],[\"基于连续卷积的方法\",{\"1\":{\"110\":1}}],[\"基于学习的方法主要分为三类\",{\"1\":{\"110\":1}}],[\"基于\",{\"1\":{\"97\":1,\"122\":1,\"187\":1,\"206\":1,\"250\":1,\"254\":1,\"329\":1,\"334\":1,\"341\":1,\"342\":1,\"415\":1,\"640\":1,\"679\":1,\"680\":1}}],[\"基于相机参数\",{\"1\":{\"76\":1}}],[\"基于几何映射的方法\",{\"1\":{\"75\":1}}],[\"基于语言引导的3d可供性分割\",{\"1\":{\"46\":1}}],[\"基于微调的mllm\",{\"1\":{\"30\":1}}],[\"倒水\",{\"1\":{\"30\":1,\"56\":1}}],[\"壶嘴的形状\",{\"1\":{\"56\":1}}],[\"壶嘴上开口狭窄\",{\"1\":{\"52\":1}}],[\"壶嘴\",{\"1\":{\"30\":1,\"56\":1}}],[\"未找到预训练\",{\"1\":{\"964\":1}}],[\"未找到预训练模型\",{\"1\":{\"963\":1}}],[\"未\",{\"1\":{\"899\":1}}],[\"未患病\",{\"1\":{\"850\":1}}],[\"未归一化\",{\"1\":{\"733\":1}}],[\"未归一化的对数概率\",{\"1\":{\"257\":1}}],[\"未微调模型即接近sota\",{\"1\":{\"668\":1}}],[\"未能回答\",{\"1\":{\"657\":1}}],[\"未能充分利用三维坐标中的几何特性\",{\"1\":{\"125\":1}}],[\"未登录词\",{\"1\":{\"594\":1}}],[\"未使用\",{\"1\":{\"588\":1,\"589\":1,\"900\":1}}],[\"未投影\",{\"1\":{\"385\":1}}],[\"未指定\",{\"1\":{\"263\":1}}],[\"未经扰动训练时\",{\"1\":{\"157\":1}}],[\"未经roi\",{\"1\":{\"83\":1}}],[\"未见\",{\"1\":{\"73\":1,\"89\":1}}],[\"未见类别以及带有噪声\",{\"1\":{\"19\":1}}],[\"未来的理论研究有望揭示\",{\"1\":{\"949\":1}}],[\"未来将带来更多创新特性和性能提升\",{\"1\":{\"833\":1}}],[\"未来将持续优化对话和推理能力\",{\"1\":{\"337\":1}}],[\"未来方向\",{\"1\":{\"658\":1}}],[\"未来若需面向多元人群\",{\"1\":{\"658\":1}}],[\"未来研究方向\",{\"1\":{\"642\":1}}],[\"未来工作方向\",{\"1\":{\"217\":1,\"377\":1}}],[\"未来可进一步探索以下方向以提升\",{\"1\":{\"183\":1}}],[\"未来\",{\"1\":{\"50\":1}}],[\"未充分挖掘物体间共享的几何不变性\",{\"1\":{\"30\":1}}],[\"所在的高维空间\",{\"1\":{\"942\":1}}],[\"所参数化的分类分布\",{\"1\":{\"886\":1}}],[\"所参考源仓库未提供requirements\",{\"1\":{\"712\":1}}],[\"所提供的代码展开进行讲解\",{\"1\":{\"739\":1}}],[\"所提出的方法在性能上显著优于像素级自编码方案\",{\"1\":{\"234\":1}}],[\"所采用的\",{\"1\":{\"655\":1}}],[\"所占的内存资源和计算资源呢\",{\"1\":{\"609\":1}}],[\"所谓\",{\"1\":{\"500\":1}}],[\"所具备的\",{\"1\":{\"422\":1}}],[\"所给代码删除了大量非核心逻辑\",{\"1\":{\"396\":1}}],[\"所使用到的所有数据集对应的\",{\"1\":{\"382\":1}}],[\"所使用的训练数据总共包含约\",{\"1\":{\"176\":1}}],[\"所使用的隐藏状态空间维度\",{\"1\":{\"66\":1}}],[\"所指出的\",{\"1\":{\"273\":1}}],[\"所遮挡后的损坏图像\",{\"1\":{\"234\":1}}],[\"所需神经元\",{\"1\":{\"500\":1}}],[\"所需特征\",{\"1\":{\"385\":1}}],[\"所需\",{\"1\":{\"224\":1}}],[\"所需的比特数\",{\"1\":{\"951\":1}}],[\"所需的信息量\",{\"1\":{\"950\":1}}],[\"所需的平均信息量\",{\"1\":{\"950\":1}}],[\"所需的总比特数\",{\"1\":{\"950\":1}}],[\"所需的交集\",{\"1\":{\"592\":1}}],[\"所需的关键词参数\",{\"1\":{\"188\":1}}],[\"所需的输入格式\",{\"1\":{\"67\":1}}],[\"所支持的23种物体类型和17种功能类型\",{\"1\":{\"92\":1}}],[\"所有元素的乘积\",{\"1\":{\"918\":2}}],[\"所有可能的\",{\"1\":{\"847\":1}}],[\"所有输入序列等长\",{\"1\":{\"713\":1}}],[\"所有序列都填充到max\",{\"1\":{\"713\":1}}],[\"所有样本列表构成batch数据返回\",{\"1\":{\"698\":1}}],[\"所有无偏差或增益权重设置为\",{\"1\":{\"633\":1}}],[\"所有注意力头\",{\"1\":{\"582\":1}}],[\"所有变量两两之间协方差的矩阵表示\",{\"1\":{\"574\":1}}],[\"所有这些指标都是基于单个分类阈值值计算得出的\",{\"1\":{\"568\":1}}],[\"所有行均指向原始数据的第\",{\"1\":{\"546\":1}}],[\"所有子进程会继承同样的可见\",{\"1\":{\"520\":1}}],[\"所有类别的\",{\"1\":{\"514\":1}}],[\"所有类别权重为\",{\"1\":{\"514\":1}}],[\"所有张量必须具有完全相同的\",{\"1\":{\"466\":1}}],[\"所有logits的平均作为最终的matching\",{\"1\":{\"419\":1}}],[\"所有模型都基于\",{\"1\":{\"656\":1}}],[\"所有模型都使用3000亿tokens进行训练\",{\"1\":{\"647\":1}}],[\"所有模型都在\",{\"1\":{\"242\":1}}],[\"所有模型共享最大上下文窗口为2048\",{\"1\":{\"647\":1}}],[\"所有模型均采用相同的架构\",{\"1\":{\"641\":1}}],[\"所有模型均训练了32个周期\",{\"1\":{\"407\":1}}],[\"所有模型在\",{\"1\":{\"640\":1}}],[\"所有参数可训练\",{\"1\":{\"318\":1}}],[\"所有参数均参与训练\",{\"1\":{\"315\":1}}],[\"所有参数都是可训练的\",{\"1\":{\"306\":1}}],[\"所有裁剪\",{\"1\":{\"293\":1}}],[\"所有特征拼接完成后\",{\"1\":{\"293\":1}}],[\"所有解码器层都禁止\",{\"1\":{\"272\":1}}],[\"所有的嵌入都存储在一个嵌入空间\",{\"1\":{\"961\":1}}],[\"所有的转换包括添加随机初初始化的开始和结束标记\",{\"1\":{\"631\":1}}],[\"所有的标签\",{\"1\":{\"268\":1}}],[\"所有的数据都将发生变化\",{\"1\":{\"131\":1}}],[\"所有图文对的前向传播\",{\"1\":{\"207\":1}}],[\"所有\",{\"1\":{\"190\":1,\"212\":1,\"420\":2,\"847\":1}}],[\"所有卷积和\",{\"1\":{\"152\":1}}],[\"所有尺度的特征保存到\",{\"1\":{\"141\":1}}],[\"所有尺度的网络并行运行\",{\"1\":{\"141\":1}}],[\"所有点相乘\",{\"1\":{\"160\":1}}],[\"所有点相加\",{\"1\":{\"160\":1}}],[\"所有点经过共享参数的\",{\"1\":{\"150\":1}}],[\"所有点组成的局部区域\",{\"1\":{\"137\":1}}],[\"所有点的特征\",{\"1\":{\"119\":1}}],[\"所有点的坐标\",{\"1\":{\"119\":2}}],[\"所有邻居点特征进行求和\",{\"1\":{\"119\":1}}],[\"所有通道共享聚合权重\",{\"1\":{\"110\":1}}],[\"所有问题专属于评估阶段\",{\"1\":{\"90\":1}}],[\"所有物体几何结构文本数据\",{\"1\":{\"53\":1}}],[\"所有人类交互文本数据\",{\"1\":{\"53\":1}}],[\"所示\",{\"1\":{\"30\":1,\"41\":1,\"43\":1,\"115\":1,\"181\":1,\"202\":1,\"212\":1,\"228\":1,\"229\":1,\"306\":2,\"370\":1,\"647\":2,\"878\":1}}],[\"所以效果也比较差\",{\"1\":{\"964\":1}}],[\"所以近两年我们能看到很多使用了codebook的图像生成类工作\",{\"1\":{\"961\":1}}],[\"所以在优化过程中可以忽略\",{\"1\":{\"951\":1}}],[\"所以在反向传播前需调用各变量的cleargrad方法重置导数\",{\"1\":{\"816\":1}}],[\"所以还会有一个误差项\",{\"1\":{\"950\":1}}],[\"所以还是很有挑战性的\",{\"1\":{\"634\":1}}],[\"所以反向传播可以计算出有效梯度用于随机梯度下降\",{\"1\":{\"946\":1}}],[\"所以整个图像的条件概率为\",{\"1\":{\"932\":1}}],[\"所以它不能很好地完成图像生成任务\",{\"1\":{\"956\":1}}],[\"所以它总是非负且有限\",{\"1\":{\"912\":1}}],[\"所以它在所有空间和时间\",{\"1\":{\"273\":1}}],[\"所以除以\",{\"1\":{\"871\":1}}],[\"所以模型的任务是\",{\"1\":{\"735\":1}}],[\"所以模型必须具有对点顺序的不变性\",{\"1\":{\"160\":1}}],[\"所以最终的答案只能来自原始输入文本中的某一段子串\",{\"1\":{\"735\":1}}],[\"所以我只是取其中的一部分数据\",{\"1\":{\"712\":1}}],[\"所以我们自然希望选择一个依赖于\",{\"1\":{\"945\":1}}],[\"所以我们用一个变换把它映射为正数\",{\"1\":{\"931\":1}}],[\"所以我们需要\",{\"1\":{\"931\":1}}],[\"所以我们需要一个规则体系来规定\",{\"1\":{\"847\":1}}],[\"所以我们需要加一个正则化项来鼓励变换矩阵接近正交矩阵\",{\"1\":{\"153\":1}}],[\"所以我们可以很容易地去建立一个字典\",{\"1\":{\"353\":1}}],[\"所以准确度不是在我的考虑范围之内\",{\"1\":{\"712\":1}}],[\"所以都用\",{\"1\":{\"709\":1}}],[\"所以要把负值平移到正数区间\",{\"1\":{\"709\":1}}],[\"所以要把它复制\",{\"1\":{\"156\":1}}],[\"所以就算\",{\"1\":{\"694\":1}}],[\"所以不过多展开\",{\"1\":{\"663\":1}}],[\"所以矩阵\",{\"1\":{\"612\":2}}],[\"所以正确的strides是\",{\"1\":{\"545\":1}}],[\"所以正样本和负样本要走同一个编码器\",{\"1\":{\"353\":1}}],[\"所以原本的格子边界保持得很清晰\",{\"1\":{\"504\":1}}],[\"所以使用装饰器会导致原函数的\",{\"1\":{\"454\":1}}],[\"所以参数量为\",{\"1\":{\"432\":1}}],[\"所以把\",{\"1\":{\"420\":1}}],[\"所以下面我们将首先对其\",{\"1\":{\"380\":1}}],[\"所以抽样的部分还是要大一点\",{\"1\":{\"355\":1}}],[\"所以作者提出\",{\"1\":{\"354\":1}}],[\"所以moco要做的就是\",{\"1\":{\"353\":1}}],[\"所以如果key这个字典足够大\",{\"1\":{\"353\":1}}],[\"所以cv领域并不适合去建立一个字典来学习模型\",{\"1\":{\"353\":1}}],[\"所以个体判别这个代理任务定义了什么是正样本\",{\"1\":{\"350\":1}}],[\"所以个体判别这个代理任务经过模型训练\",{\"1\":{\"350\":1}}],[\"所以这两张图片就可以称之为正样本\",{\"1\":{\"350\":1}}],[\"所以这个表达式的含义是\",{\"1\":{\"100\":1}}],[\"所以训练好的解码器知道\",{\"1\":{\"947\":1}}],[\"所以训练效率高\",{\"1\":{\"276\":1}}],[\"所以训练中我们就可以只最小化重建损失\",{\"1\":{\"235\":1}}],[\"所以只需一次前向传播\",{\"1\":{\"272\":1}}],[\"所以本文就不再对代码进行讲解了\",{\"1\":{\"225\":1}}],[\"所以代码实现方面会维护一些额外的状态参数\",{\"1\":{\"213\":1}}],[\"所以小的才是有效点\",{\"1\":{\"137\":1}}],[\"所以\",{\"1\":{\"26\":1,\"162\":1,\"260\":1,\"278\":1,\"355\":2,\"454\":1,\"526\":1,\"589\":1,\"694\":1,\"706\":2,\"709\":1,\"846\":2,\"848\":1,\"850\":1,\"873\":1,\"885\":1,\"916\":1,\"921\":1,\"944\":1}}],[\"简单说\",{\"1\":{\"912\":1}}],[\"简单节点定义\",{\"1\":{\"815\":1}}],[\"简单总结下\",{\"1\":{\"706\":1}}],[\"简单微调后\",{\"1\":{\"668\":1}}],[\"简单样本抑制越强\",{\"1\":{\"589\":1}}],[\"简单样本主导梯度\",{\"1\":{\"589\":1}}],[\"简单样本的梯度贡献淹没难样本的梯度\",{\"1\":{\"589\":1}}],[\"简单例子\",{\"1\":{\"513\":1}}],[\"简单来说\",{\"1\":{\"409\":1,\"540\":1,\"691\":1}}],[\"简单理解\",{\"1\":{\"380\":1,\"847\":1}}],[\"简而言之\",{\"1\":{\"213\":1,\"450\":1}}],[\"简洁\",{\"1\":{\"159\":1}}],[\"简洁表达\",{\"1\":{\"87\":1}}],[\"简析pointnet网络模型及其背后原理\",{\"1\":{\"147\":1}}],[\"简析pointnet\",{\"0\":{\"147\":1}}],[\"简析pointnet++\",{\"0\":{\"130\":1},\"1\":{\"130\":1}}],[\"简化了模型的实现细节\",{\"1\":{\"926\":1}}],[\"简化部署流程\",{\"1\":{\"834\":1}}],[\"简化\",{\"1\":{\"710\":1}}],[\"简化和优化bert的训练过程\",{\"1\":{\"687\":1}}],[\"简化版\",{\"1\":{\"587\":1,\"810\":1}}],[\"简化多维索引\",{\"1\":{\"463\":1}}],[\"简化为单点分布\",{\"1\":{\"235\":1}}],[\"简化跨模态融合\",{\"1\":{\"194\":1}}],[\"简化处理\",{\"1\":{\"119\":1}}],[\"简化成一句话\",{\"1\":{\"26\":1}}],[\"简称\",{\"1\":{\"84\":1,\"272\":1,\"273\":1,\"611\":1,\"614\":1,\"869\":1,\"948\":1}}],[\"简介\",{\"0\":{\"30\":1,\"73\":1,\"228\":1,\"296\":1,\"323\":1,\"626\":1,\"639\":1,\"646\":1,\"654\":1,\"666\":1}}],[\"策略预训练轻量级查询\",{\"1\":{\"416\":1}}],[\"策略与稳定的训练流程\",{\"1\":{\"252\":1}}],[\"策略在\",{\"1\":{\"240\":1}}],[\"策略对交互图像进行推理\",{\"1\":{\"32\":1}}],[\"策略\",{\"0\":{\"263\":1},\"1\":{\"29\":1,\"141\":1,\"194\":1,\"208\":1,\"234\":1,\"285\":1,\"892\":1}}],[\"以决定我们允许模型\",{\"1\":{\"951\":1}}],[\"以简化表示\",{\"1\":{\"946\":1}}],[\"以手写数字为例\",{\"1\":{\"944\":1}}],[\"以生成手写数字图像为例\",{\"1\":{\"943\":1}}],[\"以生成相应的文本特征\",{\"1\":{\"408\":1}}],[\"以图像为例\",{\"1\":{\"942\":1}}],[\"以强化条件信号的方法\",{\"1\":{\"894\":1}}],[\"以匹配\",{\"1\":{\"893\":1}}],[\"以一定概率随机删除文本条件\",{\"1\":{\"893\":1}}],[\"以确保初始化时的稳定训练\",{\"1\":{\"886\":1}}],[\"以确保问题多样性和语义丰富性\",{\"1\":{\"87\":1}}],[\"以自回归的方式将文本和图像\",{\"1\":{\"885\":1}}],[\"以至于定义均值的积分并不收敛\",{\"1\":{\"868\":1}}],[\"以调用\",{\"1\":{\"835\":1}}],[\"以改善多媒体交互\",{\"1\":{\"827\":1}}],[\"以goldstein\",{\"1\":{\"815\":1}}],[\"以目录形式存在\",{\"1\":{\"810\":1}}],[\"以乘法为例\",{\"1\":{\"809\":1}}],[\"以def\",{\"1\":{\"809\":2}}],[\"以加速模型的训练和扩展其中\",{\"1\":{\"823\":1}}],[\"以加速训练并提高模型的稳定性\",{\"1\":{\"741\":1}}],[\"以加法a\",{\"1\":{\"809\":1}}],[\"以释放内存供后续计算使用\",{\"1\":{\"806\":1}}],[\"以平方函数为例\",{\"1\":{\"780\":1}}],[\"以平衡计算效率\",{\"1\":{\"667\":1}}],[\"以往为了解决不同的\",{\"1\":{\"690\":1}}],[\"以往方法依赖采样\",{\"1\":{\"125\":1}}],[\"以无监督的方式利用大量无标注文本\",{\"1\":{\"690\":1}}],[\"以88\",{\"1\":{\"685\":1}}],[\"以防泄露\",{\"1\":{\"830\":1}}],[\"以防对齐过程中性能退化\",{\"1\":{\"656\":1}}],[\"以防下游\",{\"1\":{\"384\":1}}],[\"以降低毒性输出倾向\",{\"1\":{\"655\":1}}],[\"以上模型的上下文长度为\",{\"1\":{\"823\":1}}],[\"以上就是variable类的新\",{\"1\":{\"801\":1}}],[\"以上下文为接口\",{\"1\":{\"647\":1}}],[\"以上代码注释中统一用b代替image\",{\"1\":{\"417\":1}}],[\"以研究性能与规模之间的关系\",{\"1\":{\"647\":1}}],[\"以研究模型容量对性能的影响\",{\"1\":{\"640\":1}}],[\"以节省\",{\"1\":{\"614\":1}}],[\"以此来评判dall\",{\"1\":{\"900\":1}}],[\"以此来模拟所谓的内在秩\",{\"1\":{\"611\":1}}],[\"以此来简化子实现类需要做的操作\",{\"1\":{\"382\":1}}],[\"以增加模型接触不同任务\",{\"1\":{\"640\":1}}],[\"以增大生成期望序列的概率\",{\"1\":{\"604\":1}}],[\"以增强理解\",{\"1\":{\"655\":1}}],[\"以增强性能\",{\"1\":{\"407\":1}}],[\"以增强\",{\"1\":{\"95\":1}}],[\"以增强泛化能力\",{\"1\":{\"43\":1}}],[\"以输入\",{\"1\":{\"596\":1}}],[\"以三个变量为例\",{\"1\":{\"574\":1}}],[\"以两个变量为例\",{\"1\":{\"574\":1}}],[\"以避免安装到错误的位置\",{\"1\":{\"557\":1}}],[\"以避免模型将匹配图文对挑选为负样本\",{\"1\":{\"419\":1}}],[\"以使得模型能够学习到最适合当前任务的位置表示\",{\"1\":{\"428\":1}}],[\"以最大程度地惩罚降低iou得分的预测结果\",{\"1\":{\"591\":1}}],[\"以最大化互信息\",{\"1\":{\"418\":1}}],[\"以最小化损失\",{\"1\":{\"427\":1}}],[\"以保留图像的空间信息\",{\"1\":{\"427\":1}}],[\"以保持与其他模型的一致性\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"以保持图像细节\",{\"1\":{\"216\":1}}],[\"以保持计算的可管理性\",{\"1\":{\"135\":1}}],[\"以保持原始模型识别能力的同时增强其推理能力\",{\"1\":{\"34\":1}}],[\"以vit\",{\"1\":{\"426\":1}}],[\"以bos\",{\"1\":{\"421\":1}}],[\"以利用\",{\"1\":{\"421\":1}}],[\"以细粒度对齐\",{\"1\":{\"419\":1}}],[\"以弥合模态差距\",{\"1\":{\"416\":1}}],[\"以每个文本描述为一行\",{\"1\":{\"411\":1}}],[\"以获取图像特征\",{\"1\":{\"408\":1}}],[\"以获得更强的局部几何感知能力\",{\"1\":{\"141\":1}}],[\"以分析不同训练策略的影响\",{\"1\":{\"344\":1}}],[\"以奖励模型为环境反馈信号\",{\"1\":{\"656\":1}}],[\"以奖励模型为\",{\"1\":{\"339\":1}}],[\"以提高数据质量和多样性\",{\"1\":{\"823\":1}}],[\"以提高效率\",{\"1\":{\"305\":1,\"944\":1}}],[\"以提升相似度分布的差异性\",{\"1\":{\"900\":1}}],[\"以提升大批次训练的稳定性\",{\"1\":{\"680\":1}}],[\"以提升效率\",{\"1\":{\"647\":1}}],[\"以提升特征质量\",{\"1\":{\"283\":1}}],[\"以预训练权重初始化网络\",{\"1\":{\"286\":1}}],[\"以并行化计算并提高学习效率\",{\"1\":{\"271\":1}}],[\"以促进mllm社区发展\",{\"1\":{\"323\":1}}],[\"以促进跨语言和跨模态迁移\",{\"1\":{\"225\":1}}],[\"以促进视觉\",{\"1\":{\"183\":1}}],[\"以支持通用的多模态预训练\",{\"1\":{\"377\":1}}],[\"以支持\",{\"1\":{\"217\":1}}],[\"以支持训练阶段的匹配与推理\",{\"1\":{\"42\":1}}],[\"以训练更好的\",{\"1\":{\"216\":1}}],[\"以\",{\"0\":{\"534\":1},\"1\":{\"204\":1,\"264\":1,\"293\":4,\"425\":1,\"633\":1,\"822\":1,\"925\":1}}],[\"以下小节总结了其中一些常见分布\",{\"1\":{\"854\":1}}],[\"以下模型的上下文长度为\",{\"1\":{\"823\":1}}],[\"以下情况会增加引用计数\",{\"1\":{\"806\":1}}],[\"以下内容来自\",{\"1\":{\"500\":1}}],[\"以下引用clip论文图做说明\",{\"1\":{\"418\":1}}],[\"以下为动量编码器分支\",{\"1\":{\"206\":1}}],[\"以下首先给出的是\",{\"1\":{\"192\":1}}],[\"以下是你提供内容的逐段翻译与解释\",{\"1\":{\"871\":1}}],[\"以下是大语言模型的一些主要特点\",{\"1\":{\"824\":1}}],[\"以下是具体分析\",{\"1\":{\"500\":1}}],[\"以下是一个官方给出的clip模型的示例\",{\"1\":{\"408\":1}}],[\"以下是两者的主要区别\",{\"1\":{\"346\":1}}],[\"以下是论文\",{\"1\":{\"179\":1}}],[\"以下是对\",{\"1\":{\"174\":1,\"655\":1}}],[\"以下代码是我自己写的一个测试代码\",{\"1\":{\"107\":1}}],[\"以计算以下三种损失\",{\"1\":{\"172\":1}}],[\"以实现对生成样本的控制\",{\"1\":{\"936\":1}}],[\"以实现生成任务\",{\"1\":{\"171\":1}}],[\"以实现最佳的性能\",{\"1\":{\"141\":1}}],[\"以及经过筛选的\",{\"1\":{\"888\":1}}],[\"以及函数等更复杂的样本空间\",{\"1\":{\"847\":1}}],[\"以及每一个功能的大体实现逻辑\",{\"1\":{\"836\":1}}],[\"以及即将推出的\",{\"1\":{\"823\":1}}],[\"以及对错误数据\",{\"1\":{\"836\":1}}],[\"以及对\",{\"1\":{\"823\":1}}],[\"以及add\",{\"1\":{\"810\":1}}],[\"以及特定群体优先原则\",{\"1\":{\"658\":1}}],[\"以及一系列元数据\",{\"1\":{\"656\":1}}],[\"以及一个权重归一化的全连接层组成\",{\"1\":{\"285\":1}}],[\"以及缺乏透明性等\",{\"1\":{\"649\":1}}],[\"以及如何高效地进行多次调用和推理\",{\"1\":{\"833\":1}}],[\"以及如何更好地利用其隐含学习到的多任务能力\",{\"1\":{\"643\":1}}],[\"以及如何通过局部特征学习器\",{\"1\":{\"131\":1}}],[\"以及研究双向表示\",{\"1\":{\"642\":1}}],[\"以及分割字符嵌入矩阵\",{\"1\":{\"630\":1}}],[\"以及语言模型来提升标记的语义角色\",{\"1\":{\"627\":1}}],[\"以及prompt的长度\",{\"1\":{\"620\":1}}],[\"以及训练的方法的角度\",{\"1\":{\"602\":1}}],[\"以及seq\",{\"1\":{\"417\":1}}],[\"以及多张较低分辨率的\",{\"1\":{\"285\":1}}],[\"以及多模态数据\",{\"1\":{\"223\":1}}],[\"以及误差\",{\"1\":{\"259\":1}}],[\"以及\",{\"1\":{\"224\":1,\"332\":1,\"640\":1,\"823\":1,\"885\":1}}],[\"以及在多模态编码器上进行掩码语言模型\",{\"1\":{\"198\":1}}],[\"以及数据集自举策略\",{\"1\":{\"170\":1}}],[\"以适配后面的卷积操作\",{\"1\":{\"145\":1}}],[\"以便计算余弦相似度\",{\"1\":{\"900\":1}}],[\"以便开发各种下游应用\",{\"1\":{\"831\":1}}],[\"以便生成模型可以更好地理解和使用\",{\"1\":{\"829\":1}}],[\"以便一起处理\",{\"1\":{\"737\":1}}],[\"以便送入全连接层进行分类和回归\",{\"1\":{\"502\":1}}],[\"以便\",{\"1\":{\"417\":1}}],[\"以便处理通道维\",{\"1\":{\"213\":1}}],[\"以便和特征相乘\",{\"1\":{\"145\":1}}],[\"以便可以在这些分区上独立地学习特征\",{\"1\":{\"131\":1}}],[\"以便可以在这些区域上应用局部操作\",{\"1\":{\"131\":1}}],[\"以结合来自不同尺度的特征\",{\"1\":{\"140\":1}}],[\"以反映其相对位置\",{\"1\":{\"136\":1}}],[\"以限制每个局部区域中考虑的点的数量\",{\"1\":{\"135\":1}}],[\"以产生一个更少元素的新集合\",{\"1\":{\"133\":1}}],[\"以二维欧几里得空间为例\",{\"1\":{\"132\":1}}],[\"以开放词汇的方式定位3d物体的功能区域\",{\"1\":{\"29\":1}}],[\"以捕捉复杂空间关系\",{\"1\":{\"20\":1}}],[\"是为生成任务设计的\",{\"1\":{\"963\":1}}],[\"是为了后续乘上一个最小值\",{\"1\":{\"892\":2}}],[\"是为了支持任意参数签名\",{\"1\":{\"452\":1}}],[\"是为了把\",{\"1\":{\"421\":1}}],[\"是为了\",{\"1\":{\"293\":2,\"710\":1,\"892\":1}}],[\"是为了增强模型鲁棒性\",{\"1\":{\"264\":1}}],[\"是为了在突出\",{\"1\":{\"207\":1}}],[\"是为了扩大负样本池\",{\"1\":{\"206\":1}}],[\"是为了扩展成\",{\"1\":{\"153\":1}}],[\"是其在嵌入空间的最近邻向量\",{\"1\":{\"960\":1}}],[\"是解码器的输入\",{\"1\":{\"959\":1}}],[\"是怎么生成离散编码的\",{\"1\":{\"958\":1}}],[\"是因为图片被编码成了连续向量\",{\"1\":{\"956\":1}}],[\"是二值变量时\",{\"1\":{\"951\":1}}],[\"是二值变量\",{\"1\":{\"951\":1}}],[\"是二分类\",{\"1\":{\"201\":1}}],[\"是连续变量时\",{\"1\":{\"951\":1}}],[\"是连续变量\",{\"1\":{\"951\":1}}],[\"是连接具身智能体感知与操作的关键\",{\"1\":{\"73\":1}}],[\"是我们人为设定的一个值\",{\"1\":{\"951\":1}}],[\"是我们希望最大化的目标\",{\"1\":{\"945\":1}}],[\"是需要手动设置的\",{\"1\":{\"951\":1}}],[\"是固定的\",{\"1\":{\"949\":1}}],[\"是固定不变的\",{\"1\":{\"611\":1}}],[\"是不学习的随机变量\",{\"1\":{\"946\":1}}],[\"是带参数\",{\"1\":{\"946\":1}}],[\"是欧几里得距离的平方\",{\"1\":{\"944\":1}}],[\"是高斯分布\",{\"1\":{\"944\":1}}],[\"是多层神经网络\",{\"1\":{\"944\":1}}],[\"是多头自注意力中每个头切分到的维度\",{\"1\":{\"663\":1}}],[\"是很擅长同时生成一个和原图像长宽相同的张量的\",{\"1\":{\"921\":1}}],[\"是很灵活的\",{\"1\":{\"350\":1}}],[\"是他们的中间点\",{\"1\":{\"915\":1}}],[\"是kl散度\",{\"1\":{\"914\":1}}],[\"是事件\",{\"1\":{\"906\":1}}],[\"是事件空间\",{\"1\":{\"845\":1}}],[\"是单个样本的概率质量\",{\"1\":{\"904\":1}}],[\"是用温度控制\",{\"1\":{\"897\":1}}],[\"是用于信息表达的维度\",{\"1\":{\"536\":1}}],[\"是用于计算相似度的维度\",{\"1\":{\"536\":1}}],[\"是计算\",{\"1\":{\"893\":1}}],[\"是凹函数\",{\"1\":{\"885\":1}}],[\"是潜变量\",{\"1\":{\"885\":1}}],[\"是标准\",{\"1\":{\"885\":1}}],[\"是正则项\",{\"1\":{\"885\":1}}],[\"是正确的\",{\"1\":{\"621\":1}}],[\"是离散分布\",{\"1\":{\"886\":1,\"946\":1}}],[\"是离散变量\",{\"1\":{\"885\":1}}],[\"是离散的\",{\"1\":{\"232\":1}}],[\"是如何产生的\",{\"1\":{\"878\":1}}],[\"是真实数据的概率\",{\"1\":{\"918\":1}}],[\"是真实标签\",{\"1\":{\"589\":1}}],[\"是真的\",{\"1\":{\"877\":1}}],[\"是已知的固定值\",{\"1\":{\"877\":1}}],[\"是相关系数\",{\"1\":{\"871\":1}}],[\"是beta\",{\"1\":{\"867\":1}}],[\"是gamma\",{\"1\":{\"867\":1}}],[\"是尺度参数\",{\"1\":{\"867\":1}}],[\"是均值为\",{\"1\":{\"943\":1}}],[\"是均值向量\",{\"1\":{\"871\":1}}],[\"是均值\",{\"1\":{\"867\":1}}],[\"是均匀采样\",{\"1\":{\"263\":1}}],[\"是实数\",{\"1\":{\"861\":1}}],[\"是该分布的均值\",{\"1\":{\"858\":1}}],[\"是分布的均值\",{\"1\":{\"856\":1}}],[\"是分类器的权重矩阵\",{\"1\":{\"238\":1}}],[\"是分类器的参数\",{\"1\":{\"234\":1}}],[\"是整个贝叶斯推断的核心\",{\"1\":{\"853\":1}}],[\"是独立事件\",{\"1\":{\"849\":1}}],[\"是你允许讨论的事件的全集合\",{\"1\":{\"847\":1}}],[\"是你输入的问题\",{\"1\":{\"533\":1}}],[\"是所有让\",{\"1\":{\"846\":1}}],[\"是前两代\",{\"1\":{\"823\":1}}],[\"是专为复杂推理设计的模型\",{\"1\":{\"823\":1}}],[\"是验证框架能力的理想案例\",{\"1\":{\"811\":1}}],[\"是包含交叉项的二维函数\",{\"1\":{\"811\":1}}],[\"是简单的平方和函数\",{\"1\":{\"811\":1}}],[\"是编码器的输出向量\",{\"1\":{\"960\":1}}],[\"是编码器模型\",{\"1\":{\"735\":1}}],[\"是编码空间的总维度\",{\"1\":{\"706\":1}}],[\"是答案终点的得分\",{\"1\":{\"733\":1}}],[\"是答案起点的得分\",{\"1\":{\"733\":1}}],[\"是模型预测出的答案的起始和结束位置\",{\"1\":{\"735\":1}}],[\"是模型预测的概率\",{\"1\":{\"589\":1}}],[\"是模型最后一层所有\",{\"1\":{\"733\":1}}],[\"是序列实际长度\",{\"1\":{\"713\":1}}],[\"是序列长度\",{\"1\":{\"529\":1}}],[\"是在\",{\"1\":{\"710\":1,\"924\":1}}],[\"是在张量\",{\"1\":{\"546\":1}}],[\"是查询位置到键位置的相对距离\",{\"1\":{\"710\":1}}],[\"是和相对距离\",{\"1\":{\"709\":1}}],[\"是没有任何实际意义的\",{\"1\":{\"694\":1}}],[\"是没有一个单独的编码器\",{\"1\":{\"357\":1}}],[\"是什么\",{\"0\":{\"690\":1},\"1\":{\"931\":1}}],[\"是对bert预训练过程的系统性优化\",{\"1\":{\"682\":1}}],[\"是对称的\",{\"1\":{\"150\":1,\"871\":1}}],[\"是历史缓存\",{\"1\":{\"663\":1}}],[\"是系统性地探索\",{\"1\":{\"650\":1}}],[\"是目前元学习方法的一个重要限制\",{\"1\":{\"649\":1}}],[\"是蕴含\",{\"1\":{\"634\":1}}],[\"是位置嵌入矩阵\",{\"1\":{\"629\":1}}],[\"是位置编码\",{\"1\":{\"112\":1}}],[\"是字符嵌入矩阵\",{\"1\":{\"629\":1}}],[\"是层数\",{\"1\":{\"629\":1}}],[\"是参数\",{\"1\":{\"629\":1}}],[\"是上下文字符向量\",{\"1\":{\"629\":1}}],[\"是上下文窗口大小\",{\"1\":{\"629\":1}}],[\"是上一个时刻的输出\",{\"1\":{\"351\":1}}],[\"是人类发给各种人工智能模型\",{\"1\":{\"616\":1}}],[\"是训练参数\",{\"1\":{\"611\":1}}],[\"是预训练模型初始化的参数\",{\"1\":{\"611\":1}}],[\"是预测的概率值\",{\"1\":{\"589\":1}}],[\"是预测值与真实值之间的平均绝对误差\",{\"1\":{\"106\":1}}],[\"是低秩的秩\",{\"1\":{\"609\":1}}],[\"是影响大模型生成结果的关键参数\",{\"1\":{\"606\":1}}],[\"是非负的\",{\"1\":{\"947\":1}}],[\"是非常耗成本的\",{\"1\":{\"690\":1}}],[\"是非常重要的\",{\"1\":{\"606\":1}}],[\"是非可微的\",{\"1\":{\"257\":1}}],[\"是基座模型\",{\"1\":{\"604\":1}}],[\"是基于对话聊天的\",{\"1\":{\"827\":1}}],[\"是基于基座模型开发出来的\",{\"1\":{\"823\":1}}],[\"是基于文本输入来生成图像的模型\",{\"1\":{\"405\":1}}],[\"是基于\",{\"1\":{\"403\":1}}],[\"是能够在可控成本的前提下\",{\"1\":{\"602\":1}}],[\"是交叉熵和\",{\"1\":{\"592\":1}}],[\"是两种主流的方法\",{\"1\":{\"830\":1}}],[\"是两种用于提升预训练语言模型\",{\"1\":{\"346\":1}}],[\"是两个广泛用于语言模型训练和评估的英文维基百科语料数据集\",{\"1\":{\"696\":1}}],[\"是两个可调节的超参数\",{\"1\":{\"590\":1}}],[\"是衡量两个样本集合之间重叠程度的一种指标\",{\"1\":{\"586\":1}}],[\"是它的逆矩阵\",{\"1\":{\"578\":1}}],[\"是样本空间\",{\"1\":{\"845\":1}}],[\"是样本协方差矩阵\",{\"1\":{\"578\":1}}],[\"是样本数\",{\"1\":{\"213\":1}}],[\"是协方差矩阵的逆\",{\"1\":{\"577\":1}}],[\"是比较两个不同模型性能的有效衡量指标\",{\"1\":{\"572\":1}}],[\"是按选定的间隔\",{\"1\":{\"569\":1}}],[\"是数据向量\",{\"1\":{\"871\":1}}],[\"是数据的协方差矩阵\",{\"1\":{\"577\":1}}],[\"是数据库中的内容\",{\"1\":{\"533\":1}}],[\"是数据库中的索引\",{\"1\":{\"533\":1}}],[\"是数学和物理学中用于表示多维数组的一个概念\",{\"1\":{\"540\":1}}],[\"是特征图相对于原图的缩放比例\",{\"1\":{\"501\":1}}],[\"是新分配的\",{\"1\":{\"491\":1}}],[\"是原张量的一个视图\",{\"1\":{\"469\":1}}],[\"是闭包\",{\"1\":{\"449\":1,\"451\":1}}],[\"是hidden\",{\"1\":{\"432\":1}}],[\"是卷积核的步长\",{\"1\":{\"426\":1}}],[\"是卷积核的大小\",{\"1\":{\"426\":1}}],[\"是输出通道数\",{\"1\":{\"426\":1}}],[\"是输入一幅图像的前\",{\"1\":{\"921\":1}}],[\"是输入张量\",{\"1\":{\"478\":1}}],[\"是输入通道数\",{\"1\":{\"426\":1}}],[\"是输入图像尺寸\",{\"1\":{\"231\":1}}],[\"是许多早期vlp模型的标准做法\",{\"1\":{\"388\":1}}],[\"是许多多模态生成模型的关键组成\",{\"1\":{\"251\":1}}],[\"是动量超参数\",{\"1\":{\"351\":1}}],[\"是引导模型\",{\"1\":{\"346\":1}}],[\"是教会模型\",{\"1\":{\"346\":1}}],[\"是由深度求索\",{\"1\":{\"823\":1}}],[\"是由单词词缀组成的\",{\"1\":{\"353\":1}}],[\"是由\",{\"1\":{\"339\":1,\"918\":1}}],[\"是主流选择\",{\"1\":{\"327\":1}}],[\"是随机初始化的\",{\"1\":{\"306\":2}}],[\"是随机选中的掩码位置集合\",{\"1\":{\"234\":1}}],[\"是损失函数的加权系数\",{\"1\":{\"272\":1}}],[\"是损坏后的图像输入\",{\"1\":{\"234\":1}}],[\"是批量大小\",{\"1\":{\"271\":1,\"426\":1}}],[\"是概率度量\",{\"1\":{\"845\":1}}],[\"是概率\",{\"1\":{\"261\":1}}],[\"是温度参数\",{\"1\":{\"257\":1,\"271\":1,\"285\":1}}],[\"是从一个任意分布\",{\"1\":{\"945\":1}}],[\"是从真实标签\",{\"1\":{\"271\":1}}],[\"是从\",{\"1\":{\"257\":1,\"856\":1,\"888\":1}}],[\"是第一个transformer模块的input\",{\"1\":{\"417\":1}}],[\"是第一个真正从\",{\"1\":{\"250\":1}}],[\"是第\",{\"1\":{\"257\":1}}],[\"是自然语言处理中最成功的预训练目标之一\",{\"1\":{\"234\":1}}],[\"是设定的遮挡比例\",{\"1\":{\"234\":1}}],[\"是通道数\",{\"1\":{\"231\":1,\"426\":1}}],[\"是通过\",{\"1\":{\"190\":1}}],[\"是维度数\",{\"1\":{\"872\":1}}],[\"是维度\",{\"1\":{\"213\":1}}],[\"是当前最简单\",{\"1\":{\"894\":1}}],[\"是当前最具代表性的通用多模态基础模型之一\",{\"1\":{\"312\":1}}],[\"是当前文本对应的类别标签\",{\"1\":{\"713\":1}}],[\"是当前时刻的输入\",{\"1\":{\"351\":1}}],[\"是当前\",{\"1\":{\"190\":1}}],[\"是当前规模最大的同类数据集\",{\"1\":{\"40\":1}}],[\"是严格的一对一\",{\"1\":{\"190\":1}}],[\"是每个图像区域\",{\"1\":{\"963\":1}}],[\"是每个图像对应的索引编号\",{\"1\":{\"190\":1}}],[\"是每个\",{\"1\":{\"231\":1}}],[\"是每个点的高维特征\",{\"1\":{\"150\":1}}],[\"是每个点云局部区域关键点引导下提取的图像信息增强后的点云关键点局部区域特征\",{\"1\":{\"65\":1}}],[\"是指生成过程中的隐藏决策\",{\"1\":{\"943\":1}}],[\"是指我们为了让llm能够更好地完成我们给它的任务\",{\"1\":{\"616\":1}}],[\"是指用于训练模型的参数非常多\",{\"1\":{\"600\":1}}],[\"是指被错误地归类为正例的所有实际负例所占的比例\",{\"1\":{\"564\":1}}],[\"是指位置嵌入的参数是可以在模型训练过程中通过反向传播算法进行更新的\",{\"1\":{\"428\":1}}],[\"是指示函数\",{\"1\":{\"214\":1,\"857\":1}}],[\"是指\",{\"1\":{\"161\":1,\"877\":1,\"949\":1}}],[\"是指物体或环境的属性决定了观察者可以执行的潜在动作\",{\"1\":{\"20\":1}}],[\"是最具挑战也最通用的形式\",{\"1\":{\"647\":1}}],[\"是最大可容忍的点云范围\",{\"1\":{\"150\":1}}],[\"是最终的\",{\"1\":{\"39\":1}}],[\"是关键点集合\",{\"1\":{\"150\":1}}],[\"是关系函数\",{\"1\":{\"112\":1}}],[\"是后续的全连接网络\",{\"1\":{\"150\":1}}],[\"是类别数\",{\"1\":{\"143\":1,\"238\":1,\"910\":1}}],[\"是生成向量权重的\",{\"1\":{\"112\":1}}],[\"是归一化常数\",{\"1\":{\"865\":1}}],[\"是归一化\",{\"1\":{\"112\":1}}],[\"是无序分散在三维空间的点集合\",{\"1\":{\"110\":1}}],[\"是无缓存的\",{\"1\":{\"107\":1}}],[\"是嵌入在连续空间的集合\",{\"1\":{\"109\":1}}],[\"是有局限的\",{\"1\":{\"658\":1}}],[\"是有一个先验分布\",{\"1\":{\"235\":1}}],[\"是有缓存的\",{\"1\":{\"107\":1}}],[\"是有效的\",{\"1\":{\"67\":1}}],[\"是否存在一种类似于手动控制模型约束程度的机制\",{\"1\":{\"948\":1}}],[\"是否拥有类似于稀疏自编码器中稀疏性惩罚项那样的\",{\"1\":{\"948\":1}}],[\"是否理解它对理解vq\",{\"1\":{\"956\":1}}],[\"是否理解\",{\"1\":{\"898\":1}}],[\"是否真正学会了语言建模部分\",{\"1\":{\"898\":1}}],[\"是否提前中断返回\",{\"1\":{\"893\":1}}],[\"是否输入输出\",{\"1\":{\"892\":1}}],[\"是否采用\",{\"1\":{\"892\":1}}],[\"是否采样\",{\"1\":{\"421\":1}}],[\"是否能生成新文本\",{\"1\":{\"735\":1}}],[\"是否尊重约束\",{\"1\":{\"656\":1}}],[\"是否适合类别不平衡\",{\"1\":{\"592\":1}}],[\"是否直接优化\",{\"1\":{\"588\":1}}],[\"是否一起变化\",{\"1\":{\"574\":1}}],[\"是否冗余\",{\"1\":{\"522\":1}}],[\"是否有放回采样\",{\"1\":{\"518\":1}}],[\"是否先打乱样本\",{\"1\":{\"513\":1}}],[\"是否重新计算缩放因子\",{\"1\":{\"503\":1}}],[\"是否可调\",{\"1\":{\"592\":1}}],[\"是否可用于改变维度\",{\"1\":{\"472\":1}}],[\"是否可微\",{\"1\":{\"160\":1}}],[\"是否复制数据\",{\"1\":{\"472\":1}}],[\"是否产生新作用域\",{\"1\":{\"444\":1}}],[\"是否包含\",{\"1\":{\"440\":1}}],[\"是否包含法线信息\",{\"1\":{\"138\":1}}],[\"是否对我们想要逼近的所有分布都存在这样一种函数\",{\"1\":{\"949\":1}}],[\"是否对输入\",{\"1\":{\"892\":1}}],[\"是否对类别不平衡敏感\",{\"1\":{\"588\":1}}],[\"是否对\",{\"1\":{\"586\":1,\"587\":1,\"588\":1,\"589\":1}}],[\"是否对图像进行\",{\"1\":{\"385\":1}}],[\"是否对图像做mask\",{\"1\":{\"384\":1}}],[\"是否对文本做mlm掩码\",{\"1\":{\"384\":1}}],[\"是否为连贯的上下句\",{\"1\":{\"698\":1}}],[\"是否为\",{\"1\":{\"380\":1}}],[\"是否为推理模式优化结构\",{\"1\":{\"892\":1}}],[\"是否为推理模式\",{\"1\":{\"64\":1}}],[\"是否更新全部参数\",{\"1\":{\"346\":1}}],[\"是否修改模型结构\",{\"1\":{\"346\":1}}],[\"是否规范化最后一层\",{\"1\":{\"293\":1}}],[\"是否在生成q\",{\"1\":{\"430\":1}}],[\"是否在卷积层中去掉偏置\",{\"1\":{\"380\":1}}],[\"是否在\",{\"1\":{\"293\":1,\"380\":1}}],[\"是否在输出中包含相对坐标信息\",{\"1\":{\"119\":1}}],[\"是否是\",{\"1\":{\"280\":1}}],[\"是否裁剪梯度\",{\"1\":{\"265\":1}}],[\"是否返回对比损失\",{\"1\":{\"900\":1}}],[\"是否返回损失\",{\"1\":{\"899\":1}}],[\"是否返回步长\",{\"1\":{\"440\":1}}],[\"是否返回重建图像\",{\"1\":{\"256\":1,\"899\":1}}],[\"是否返回\",{\"1\":{\"256\":1,\"899\":1}}],[\"是否启用\",{\"1\":{\"255\":1,\"895\":1}}],[\"是否启用更新\",{\"1\":{\"213\":1}}],[\"是否统计每个\",{\"1\":{\"213\":1}}],[\"是否从文件加载已有的\",{\"1\":{\"213\":1}}],[\"是否用\",{\"1\":{\"213\":1}}],[\"是否用余弦相似度\",{\"1\":{\"213\":1}}],[\"是否使用稀疏\",{\"1\":{\"892\":1}}],[\"是否使用核采样\",{\"1\":{\"421\":1}}],[\"是否使用分布式采样\",{\"1\":{\"382\":2}}],[\"是否使用跨模态\",{\"1\":{\"380\":1}}],[\"是否使用偏置\",{\"1\":{\"380\":1}}],[\"是否使用绝对位置编码\",{\"1\":{\"380\":1}}],[\"是否使用相对位置编码\",{\"1\":{\"380\":1}}],[\"是否使用硬采样\",{\"1\":{\"257\":1}}],[\"是否使用k\",{\"1\":{\"213\":1}}],[\"是否使用\",{\"1\":{\"213\":1,\"255\":1,\"892\":3,\"899\":3}}],[\"是否使用梯度检查点\",{\"1\":{\"192\":1}}],[\"是否使用梯度检查点优化vit显存占用\",{\"1\":{\"187\":1}}],[\"是否仅返回\",{\"1\":{\"208\":1,\"256\":1}}],[\"是否被\",{\"1\":{\"208\":1}}],[\"是否被后续模型改进\",{\"1\":{\"157\":1}}],[\"是否跨\",{\"1\":{\"190\":1}}],[\"是否关注区域匹配\",{\"1\":{\"592\":1}}],[\"是否关注像素分类\",{\"1\":{\"592\":1}}],[\"是否关注空间重合度\",{\"1\":{\"106\":1}}],[\"是否关注分布相似性\",{\"1\":{\"106\":1}}],[\"是否依赖\",{\"1\":{\"106\":1}}],[\"是否支持广播\",{\"1\":{\"472\":1}}],[\"是否支持\",{\"1\":{\"106\":1,\"588\":1}}],[\"是语言增强后的点特征\",{\"1\":{\"98\":1}}],[\"是线性变换\",{\"1\":{\"98\":1,\"199\":1}}],[\"是roberta编码后的文本特征\",{\"1\":{\"96\":1}}],[\"是一张\",{\"1\":{\"875\":1}}],[\"是一款面向消费级应用的轻量级模型\",{\"1\":{\"823\":1}}],[\"是一项针对\",{\"1\":{\"678\":1}}],[\"是一些句子对\",{\"1\":{\"634\":1}}],[\"是一点点地更新的\",{\"1\":{\"353\":1}}],[\"是一个我们可以从数据中学习的确定性函数\",{\"1\":{\"952\":1}}],[\"是一个关于\",{\"1\":{\"946\":1}}],[\"是一个常数\",{\"1\":{\"945\":1,\"951\":1}}],[\"是一个既方便采样\",{\"1\":{\"925\":1}}],[\"是一个极大极小游戏\",{\"1\":{\"918\":1}}],[\"是一个大于\",{\"1\":{\"894\":1}}],[\"是一个大规模的图像数据集\",{\"1\":{\"435\":1}}],[\"是一个从事件\",{\"1\":{\"845\":1}}],[\"是一个完整的系统\",{\"1\":{\"829\":1}}],[\"是一个线性层\",{\"1\":{\"733\":1}}],[\"是一个线性变换\",{\"1\":{\"96\":1}}],[\"是一个可训练的\",{\"1\":{\"709\":1}}],[\"是一个可学习的温度参数\",{\"1\":{\"199\":1}}],[\"是一个包含1亿个词汇的英文词库数据\",{\"1\":{\"696\":1}}],[\"是一个元组\",{\"1\":{\"489\":1}}],[\"是一个字符串\",{\"1\":{\"478\":1}}],[\"是一个来自\",{\"1\":{\"478\":1}}],[\"是一个非常强大且直观的张量操作工具\",{\"1\":{\"475\":1}}],[\"是一个闭包\",{\"1\":{\"448\":1}}],[\"是一个随机初始化的向量\",{\"1\":{\"427\":1}}],[\"是一个分类头\",{\"1\":{\"420\":1}}],[\"是一个列表\",{\"1\":{\"385\":1}}],[\"是一个统一的视觉\",{\"1\":{\"377\":1}}],[\"是一个batch一个batch地去做\",{\"1\":{\"353\":1}}],[\"是一个开源的多模态大语言模型\",{\"1\":{\"322\":1}}],[\"是一个单层多头注意力结构\",{\"1\":{\"272\":1}}],[\"是一个融合了\",{\"1\":{\"259\":1}}],[\"是一个\",{\"1\":{\"224\":1,\"578\":1,\"709\":1,\"735\":2,\"847\":1,\"887\":1,\"898\":1}}],[\"是一个新的视觉\",{\"1\":{\"183\":1}}],[\"是一个新颖的\",{\"1\":{\"165\":1}}],[\"是一个兼顾理解与生成\",{\"1\":{\"174\":1}}],[\"是一个图像引导的文本编码器\",{\"1\":{\"173\":1}}],[\"是一个图像引导的文本解码器\",{\"1\":{\"173\":1}}],[\"是一个小型神经网络\",{\"1\":{\"152\":1}}],[\"是一个两层线性层\",{\"1\":{\"113\":1,\"114\":1}}],[\"是一种非常流行的方法\",{\"1\":{\"942\":1}}],[\"是一种自回归生成模型\",{\"1\":{\"924\":1}}],[\"是一种令人兴奋的技术\",{\"1\":{\"827\":1}}],[\"是一种像人类一样思考和学习的人工智能\",{\"1\":{\"827\":1}}],[\"是一种旨在理解和生成人类语言的人工智能模型\",{\"1\":{\"822\":1}}],[\"是一种通过梯度下降自动学习位置编码的方法\",{\"1\":{\"707\":1}}],[\"是一种无需训练的位置编码方法\",{\"1\":{\"706\":1}}],[\"是一种低成本高回报的对齐方法\",{\"1\":{\"658\":1}}],[\"是一种在保证模型效果基本不降低的前提下\",{\"1\":{\"607\":1}}],[\"是一种结合了多个损失函数优点的混合损失函数\",{\"1\":{\"592\":1}}],[\"是一种针对类别不平衡\",{\"1\":{\"589\":1}}],[\"是一种常用的损失函数\",{\"1\":{\"588\":1}}],[\"是一种常用的衡量两个向量相似度的方法\",{\"1\":{\"506\":1}}],[\"是一种常用于语义分割任务的损失函数\",{\"1\":{\"586\":1}}],[\"是一种简单的编码器\",{\"1\":{\"272\":1}}],[\"是一种连续的可微近似方法\",{\"1\":{\"257\":1}}],[\"是一种基于均值为\",{\"1\":{\"868\":1}}],[\"是一种基于自注意力机制\",{\"1\":{\"741\":1}}],[\"是一种基于自监督学习的视觉transformer预训练模型\",{\"1\":{\"227\":1}}],[\"是一种基于频率统计的子词分词算法\",{\"1\":{\"594\":1}}],[\"是一种基于直方图交集的相似性指标\",{\"1\":{\"106\":1}}],[\"是一种对输入顺序不敏感的函数\",{\"1\":{\"160\":1}}],[\"是一种表示三维空间中物体或场景的方式\",{\"1\":{\"159\":1}}],[\"是一种单尺度网络\",{\"1\":{\"157\":1}}],[\"是一种\",{\"1\":{\"97\":1}}],[\"是一种残差连接\",{\"1\":{\"96\":1}}],[\"是一种新颖的框架\",{\"1\":{\"29\":1}}],[\"是点级特征变换\",{\"1\":{\"112\":1}}],[\"是点数\",{\"1\":{\"88\":1}}],[\"是点云\",{\"1\":{\"32\":1}}],[\"是\",{\"1\":{\"88\":1,\"100\":1,\"102\":1,\"106\":9,\"107\":1,\"113\":1,\"120\":1,\"123\":2,\"137\":2,\"145\":1,\"150\":1,\"153\":1,\"160\":7,\"188\":1,\"200\":1,\"208\":1,\"225\":1,\"235\":1,\"250\":2,\"259\":1,\"261\":1,\"264\":1,\"293\":1,\"327\":1,\"342\":1,\"346\":1,\"355\":1,\"420\":1,\"424\":1,\"440\":1,\"449\":2,\"462\":1,\"463\":1,\"466\":1,\"468\":1,\"471\":1,\"472\":3,\"475\":1,\"486\":1,\"488\":1,\"502\":2,\"510\":2,\"514\":1,\"516\":1,\"518\":1,\"588\":8,\"592\":1,\"656\":1,\"658\":1,\"681\":1,\"690\":1,\"709\":1,\"733\":1,\"734\":1,\"823\":2,\"871\":1,\"888\":1,\"892\":1,\"897\":1,\"915\":1,\"917\":1,\"918\":2,\"935\":1,\"936\":1,\"951\":1,\"959\":1}}],[\"是图像块的总数\",{\"1\":{\"426\":1}}],[\"是图像宽度\",{\"1\":{\"426\":1}}],[\"是图像高度\",{\"1\":{\"426\":1}}],[\"是图像每个通道的标准差\",{\"1\":{\"425\":1}}],[\"是图像每个通道的均值\",{\"1\":{\"425\":1}}],[\"是图像中\",{\"1\":{\"234\":1}}],[\"是图像\",{\"1\":{\"65\":1,\"106\":1}}],[\"功能增强\",{\"0\":{\"808\":1}}],[\"功能说明\",{\"1\":{\"293\":1}}],[\"功能属性匹配\",{\"1\":{\"92\":1}}],[\"功能类别\",{\"1\":{\"92\":2}}],[\"功能类别预测\",{\"1\":{\"78\":1}}],[\"功能类型索引\",{\"1\":{\"92\":1}}],[\"功能类型\",{\"1\":{\"91\":1}}],[\"功能类型数\",{\"1\":{\"89\":1}}],[\"功能组合的标注数据\",{\"1\":{\"92\":1}}],[\"功能组合\",{\"1\":{\"87\":2,\"91\":1}}],[\"功能分类损失\",{\"1\":{\"79\":1}}],[\"功能揭示\",{\"1\":{\"78\":1}}],[\"功能模糊性\",{\"1\":{\"73\":1}}],[\"功能可供性\",{\"1\":{\"73\":1}}],[\"功能意图\",{\"1\":{\"56\":1}}],[\"功能区域\",{\"1\":{\"107\":1,\"589\":2}}],[\"功能区域预测结果\",{\"1\":{\"107\":1}}],[\"功能区域识别任务中\",{\"1\":{\"106\":1}}],[\"功能区域分割\",{\"1\":{\"93\":1}}],[\"功能区域掩码\",{\"1\":{\"92\":3}}],[\"功能区域类型\",{\"1\":{\"53\":1}}],[\"功能区域热力图列表\",{\"1\":{\"53\":1}}],[\"功能区域热力图\",{\"1\":{\"53\":2}}],[\"功能\",{\"1\":{\"27\":1,\"73\":1,\"121\":1,\"122\":1}}],[\"引用计数无法降至0\",{\"1\":{\"806\":1}}],[\"引用计数\",{\"1\":{\"806\":1}}],[\"引用计数和分代垃圾回收\",{\"1\":{\"806\":1}}],[\"引起的对齐误差问题而提出的关键组件\",{\"1\":{\"502\":1}}],[\"引号中是\",{\"1\":{\"475\":1}}],[\"引导方法\",{\"1\":{\"894\":1}}],[\"引导答案生成\",{\"1\":{\"640\":1}}],[\"引导模型学习有用的特征\",{\"1\":{\"355\":1}}],[\"引导模型生成特定任务的结果\",{\"1\":{\"346\":1}}],[\"引导语言模型更好地遵循用户指令\",{\"1\":{\"339\":1}}],[\"引导下提取的点云信息增强后的图像特征\",{\"1\":{\"65\":1}}],[\"引导\",{\"1\":{\"52\":1}}],[\"引导的3d可供性方法\",{\"1\":{\"46\":1}}],[\"引入这个式子\",{\"1\":{\"945\":1}}],[\"引入近似但误差较小\",{\"1\":{\"942\":1}}],[\"引入一个可计算的近似后验\",{\"1\":{\"885\":1}}],[\"引入一组可学习的\",{\"1\":{\"94\":1}}],[\"引入注意力机制和辅助损失\",{\"1\":{\"884\":1}}],[\"引入上下文\",{\"1\":{\"650\":1}}],[\"引入到对大模型的微调中去\",{\"1\":{\"602\":1}}],[\"引入带有\",{\"1\":{\"376\":1}}],[\"引入通用多模态\",{\"1\":{\"368\":1}}],[\"引入了先进的检索技术\",{\"1\":{\"833\":1}}],[\"引入了多模态能力\",{\"1\":{\"823\":1}}],[\"引入了新数据集\",{\"1\":{\"678\":1}}],[\"引入了额外的推理延迟\",{\"1\":{\"610\":1}}],[\"引入了一组模态专家\",{\"1\":{\"377\":1}}],[\"引入了任务特定的注意力池化\",{\"1\":{\"272\":1}}],[\"引入了\",{\"1\":{\"220\":1}}],[\"引入了向量量化知识蒸馏\",{\"1\":{\"212\":1}}],[\"引入了两个变换网络\",{\"1\":{\"153\":1}}],[\"引入更丰富的监督\",{\"1\":{\"204\":1}}],[\"引入多尺度采样等\",{\"1\":{\"157\":1}}],[\"引入两个空间变换网络\",{\"1\":{\"150\":1}}],[\"引入全局上下文\",{\"1\":{\"122\":1}}],[\"引入过多不相关点\",{\"1\":{\"117\":1}}],[\"引入交互先验\",{\"1\":{\"31\":1}}],[\"引入\",{\"1\":{\"26\":1,\"148\":1,\"150\":1,\"157\":1,\"210\":1,\"216\":1,\"217\":1,\"656\":1,\"658\":1,\"812\":1,\"814\":2}}],[\"引言\",{\"0\":{\"19\":1,\"109\":1,\"125\":1,\"210\":1,\"220\":1,\"268\":1,\"280\":1,\"405\":1,\"540\":1,\"678\":1,\"753\":1,\"799\":1,\"814\":1,\"819\":1,\"884\":1,\"921\":1,\"942\":1,\"955\":1}}],[\"视角\",{\"1\":{\"950\":1}}],[\"视为去噪的像素级重建任务\",{\"1\":{\"216\":1}}],[\"视频和结构化知识\",{\"1\":{\"377\":1}}],[\"视频检索等任务中达到sota\",{\"1\":{\"296\":1}}],[\"视频分类\",{\"1\":{\"295\":1}}],[\"视频跨模态检索\",{\"1\":{\"273\":1}}],[\"视频识别\",{\"1\":{\"269\":1}}],[\"视频理解\",{\"1\":{\"268\":1}}],[\"视频中分割功能区域\",{\"1\":{\"75\":1}}],[\"视频\",{\"1\":{\"31\":1,\"295\":1}}],[\"视图操作\",{\"1\":{\"492\":1}}],[\"视图\",{\"1\":{\"26\":1,\"544\":1}}],[\"视觉体验差\",{\"1\":{\"884\":1}}],[\"视觉编码阶段\",{\"1\":{\"420\":1}}],[\"视觉编码器提取图像特征\",{\"1\":{\"341\":1}}],[\"视觉编码器规模的影响\",{\"1\":{\"336\":1}}],[\"视觉编码器\",{\"1\":{\"329\":1,\"334\":1,\"341\":1,\"342\":1}}],[\"视觉编码器+注意力池化\",{\"1\":{\"303\":1}}],[\"视觉分支\",{\"1\":{\"415\":1}}],[\"视觉token\",{\"1\":{\"380\":1,\"893\":1}}],[\"视觉transformer模型大小\",{\"1\":{\"187\":1}}],[\"视觉+语言\",{\"1\":{\"346\":1}}],[\"视觉联合推理能力\",{\"1\":{\"335\":1}}],[\"视觉主干设计选择\",{\"1\":{\"311\":1}}],[\"视觉定位等任务\",{\"1\":{\"305\":1}}],[\"视觉感知任务\",{\"1\":{\"304\":1}}],[\"视觉模型的\",{\"1\":{\"510\":1}}],[\"视觉模型的规模不足和对齐低效是阻碍多模态能力提升的主要障碍\",{\"1\":{\"301\":1}}],[\"视觉模型库\",{\"1\":{\"510\":1}}],[\"视觉模型通常基于纯视觉数据或bert系列模型训练\",{\"1\":{\"296\":1}}],[\"视觉基础模型的性能瓶颈仍是制约vllms发展的关键因素\",{\"1\":{\"300\":1}}],[\"视觉基础模型在过去十年中经历了显著发展\",{\"1\":{\"298\":1}}],[\"视觉预训练\",{\"1\":{\"269\":1}}],[\"视觉令牌重建\",{\"1\":{\"235\":1}}],[\"视觉标记通过离散变分自编码器\",{\"1\":{\"228\":1}}],[\"视觉专家\",{\"1\":{\"222\":1,\"224\":1,\"368\":1,\"372\":1}}],[\"视觉问答\",{\"1\":{\"220\":1,\"368\":1}}],[\"视觉推理任务\",{\"1\":{\"375\":1}}],[\"视觉推理任务中均表现稳定\",{\"1\":{\"310\":1}}],[\"视觉推理等进行微调\",{\"1\":{\"368\":1}}],[\"视觉推理\",{\"1\":{\"220\":1}}],[\"视觉任务\",{\"1\":{\"220\":1}}],[\"视觉任务和\",{\"1\":{\"220\":1}}],[\"视觉任务中\",{\"1\":{\"31\":1}}],[\"视觉与多模态预训练正在出现\",{\"1\":{\"220\":1}}],[\"视觉与大语言模型\",{\"1\":{\"93\":1}}],[\"视觉\",{\"0\":{\"167\":1,\"232\":1},\"1\":{\"171\":1,\"192\":1,\"194\":1,\"202\":1,\"210\":1,\"212\":1,\"214\":1,\"216\":1,\"220\":3,\"222\":1,\"224\":1,\"230\":1,\"232\":2,\"242\":1,\"264\":1,\"269\":1,\"305\":2,\"368\":2,\"369\":1,\"375\":2,\"388\":1}}],[\"视觉的跨模态交互能力\",{\"1\":{\"99\":1}}],[\"视觉语义特征\",{\"1\":{\"64\":1,\"69\":1}}],[\"视觉特征作为\",{\"1\":{\"24\":1}}],[\"视觉和语言模型进行开放词汇\",{\"1\":{\"20\":1}}],[\"利用两类掩码卷积\",{\"1\":{\"923\":1}}],[\"利用额外条件信息\",{\"1\":{\"884\":1}}],[\"利用包含中间推理步骤的提示机制来解决这些任务\",{\"1\":{\"825\":1}}],[\"利用pytorch从\",{\"1\":{\"689\":1}}],[\"利用cot\",{\"1\":{\"621\":1}}],[\"利用clip编码器实现文本\",{\"1\":{\"31\":1}}],[\"利用nltk库提供的wordpunct\",{\"1\":{\"595\":1}}],[\"利用nltk库提供的sent\",{\"1\":{\"595\":1}}],[\"利用已有的vit\",{\"1\":{\"415\":1}}],[\"利用已有知识进行推理\",{\"1\":{\"346\":1}}],[\"利用大规模图像\",{\"1\":{\"374\":1}}],[\"利用视觉信息辅助\",{\"1\":{\"373\":1}}],[\"利用其对齐的特征空间提升性能\",{\"1\":{\"305\":1}}],[\"利用少量标注\",{\"1\":{\"283\":1}}],[\"利用噪声图文对数据\",{\"1\":{\"269\":1}}],[\"利用了恒等式\",{\"1\":{\"861\":1}}],[\"利用了\",{\"1\":{\"213\":1}}],[\"利用图文对比损失中的相似度作为度量\",{\"1\":{\"201\":1}}],[\"利用图像和上下文文本共同预测被\",{\"1\":{\"200\":1}}],[\"利用对称函数\",{\"1\":{\"148\":1}}],[\"利用相对坐标与点特征相结合的方式可以捕获局部区域中点与点之间的关系\",{\"1\":{\"136\":1}}],[\"利用上一步得到的中心点将点集划分成若干个区域\",{\"1\":{\"132\":1}}],[\"利用广播后做逐元素相乘\",{\"1\":{\"119\":1}}],[\"利用knn算法为每个点构建局部邻域\",{\"1\":{\"119\":1}}],[\"利用稀疏性\",{\"1\":{\"110\":1}}],[\"利用一组问题条件化的\",{\"1\":{\"100\":2}}],[\"利用1x1卷积完成通道维度上的信息融合\",{\"1\":{\"83\":1}}],[\"利用联合建模特征作为query\",{\"1\":{\"83\":1}}],[\"利用roi\",{\"1\":{\"83\":3}}],[\"利用功能\",{\"1\":{\"76\":1}}],[\"利用物体结构辅助推断更多人类交互意图\",{\"1\":{\"56\":1}}],[\"利用跨模态自适应融合模块\",{\"1\":{\"32\":1}}],[\"利用2d交互语义指导3d功能定位\",{\"1\":{\"31\":1}}],[\"利用\",{\"1\":{\"26\":1,\"94\":1,\"119\":1,\"216\":1,\"386\":1,\"417\":1,\"434\":1,\"680\":1,\"831\":1}}],[\"已加载预训练\",{\"1\":{\"964\":1}}],[\"已加载预训练模型\",{\"1\":{\"963\":1}}],[\"已在文本\",{\"1\":{\"884\":1}}],[\"已在多种模态上取得成功\",{\"1\":{\"220\":1}}],[\"已知的条件下\",{\"1\":{\"849\":1}}],[\"已知rosenbrock函数的最小值在处\",{\"1\":{\"816\":1}}],[\"已实现自动微分系统与基础函数操作\",{\"1\":{\"819\":1}}],[\"已添加\",{\"1\":{\"699\":1}}],[\"已超越多项无监督nmt方法的表现\",{\"1\":{\"648\":1}}],[\"已初步验证了上下文学习的可行性\",{\"1\":{\"646\":1}}],[\"已弃用\",{\"1\":{\"587\":1,\"588\":1,\"589\":1}}],[\"已保存\",{\"1\":{\"582\":1}}],[\"已完成\",{\"1\":{\"382\":1}}],[\"已有前置知识的同学\",{\"1\":{\"899\":1}}],[\"已有的技术涉及对模型架构进行特定任务的修改\",{\"1\":{\"626\":1}}],[\"已有的张量\",{\"1\":{\"479\":1}}],[\"已有一些方法尝试过类似思路\",{\"1\":{\"269\":1}}],[\"已有三条研究路径\",{\"1\":{\"268\":1}}],[\"已经训练完毕或已加载权重\",{\"1\":{\"964\":1}}],[\"已经和论文里的\",{\"1\":{\"921\":1}}],[\"已经\",{\"1\":{\"895\":1}}],[\"已经发生的前提下\",{\"1\":{\"849\":1}}],[\"已经发生的条件下的条件概率为\",{\"1\":{\"849\":1}}],[\"已经在许多领域产生了深远的影响\",{\"1\":{\"827\":1}}],[\"已经在多个领域展示了潜力\",{\"1\":{\"824\":1}}],[\"已经在语言领域涌现出\",{\"1\":{\"268\":1}}],[\"已经扩展到支持多模态数据\",{\"1\":{\"824\":1}}],[\"已经开源了\",{\"1\":{\"823\":1}}],[\"已经能够满足个人用户或小型企业的大部分需求\",{\"1\":{\"823\":1}}],[\"已经计算完之后\",{\"1\":{\"710\":1}}],[\"已经缓存的词序列长度\",{\"1\":{\"663\":1}}],[\"已经证明\",{\"1\":{\"640\":1}}],[\"已经证明无监督学习在nlp任务上是行得通的\",{\"1\":{\"353\":1}}],[\"已经经过\",{\"1\":{\"587\":1}}],[\"已经返回了\",{\"1\":{\"448\":1}}],[\"已经过预训练以提取语言信息视觉表示\",{\"1\":{\"421\":1}}],[\"已经接近其上限\",{\"1\":{\"343\":1}}],[\"已经把\",{\"1\":{\"83\":1}}],[\"已经相对成熟\",{\"1\":{\"26\":1}}],[\"已见类别\",{\"1\":{\"19\":1}}],[\"等其他方法联系起来\",{\"1\":{\"948\":1}}],[\"等复杂的采样技术\",{\"1\":{\"944\":1}}],[\"等式右边的两项是\",{\"1\":{\"932\":1}}],[\"等不稳定做法\",{\"1\":{\"931\":1}}],[\"等不使用\",{\"1\":{\"293\":1}}],[\"等训练行为\",{\"1\":{\"898\":1}}],[\"等条件生成场景\",{\"1\":{\"894\":1}}],[\"等等\",{\"1\":{\"880\":1,\"921\":1}}],[\"等架构的实现\",{\"1\":{\"836\":1}}],[\"等多个技术领域\",{\"1\":{\"833\":1}}],[\"等多模态架构中的文本生成部分\",{\"1\":{\"403\":1}}],[\"等多模态生成模型\",{\"1\":{\"309\":1}}],[\"等多模态对话基准上达到\",{\"1\":{\"305\":1}}],[\"等项目已经出现并受到关注\",{\"1\":{\"827\":1}}],[\"等全面对标\",{\"1\":{\"823\":1}}],[\"等功能\",{\"1\":{\"823\":1}}],[\"等合作研发的语言大模型\",{\"1\":{\"823\":1}}],[\"等技术\",{\"1\":{\"823\":1}}],[\"等先进技术\",{\"1\":{\"823\":1}}],[\"等商业闭源模型\",{\"1\":{\"823\":1}}],[\"等函数节点\",{\"1\":{\"814\":1}}],[\"等运算符自然表达计算\",{\"1\":{\"812\":1}}],[\"等会被\",{\"1\":{\"735\":1}}],[\"等著名悲剧\",{\"1\":{\"733\":1}}],[\"等模型趋势一致\",{\"1\":{\"670\":1}}],[\"等模型将预训练语言模型推向主流\",{\"1\":{\"650\":1}}],[\"等机制\",{\"1\":{\"655\":1}}],[\"等原始网络数据\",{\"1\":{\"640\":1}}],[\"等于上述两个步骤的总和\",{\"1\":{\"950\":1}}],[\"等于正面次数占总次数的比例\",{\"1\":{\"904\":1}}],[\"等于\",{\"1\":{\"611\":1}}],[\"等特性\",{\"1\":{\"510\":1}}],[\"等可能改变内存布局的操作后\",{\"1\":{\"494\":1}}],[\"等价操作\",{\"1\":{\"475\":1}}],[\"等价于把解码器的梯度全部传给\",{\"1\":{\"959\":1}}],[\"等价于n个类别的cross\",{\"1\":{\"407\":1}}],[\"等价于\",{\"1\":{\"258\":1,\"457\":1,\"458\":1,\"463\":1,\"588\":1,\"590\":1,\"809\":1}}],[\"等价于基于余弦相似度查找最近代码\",{\"1\":{\"212\":1}}],[\"等操作通常会产生不连续的张量\",{\"1\":{\"490\":1}}],[\"等操作\",{\"1\":{\"470\":1}}],[\"等元信息\",{\"1\":{\"454\":1}}],[\"等属性丢失\",{\"1\":{\"454\":1}}],[\"等信息\",{\"1\":{\"382\":1}}],[\"等生成类任务\",{\"1\":{\"377\":1}}],[\"等强化学习算法\",{\"1\":{\"339\":1}}],[\"等大规模ocr数据\",{\"1\":{\"332\":1}}],[\"等通用数据集\",{\"1\":{\"332\":1}}],[\"等通过共享\",{\"1\":{\"251\":1}}],[\"等新兴模型进一步推动多模态技术的发展\",{\"1\":{\"325\":1}}],[\"等方法\",{\"1\":{\"310\":1}}],[\"等方法一样\",{\"1\":{\"264\":1}}],[\"等主流模型\",{\"1\":{\"308\":1}}],[\"等人在\",{\"1\":{\"946\":1}}],[\"等人在2014年提出的一种生成模型\",{\"1\":{\"918\":1}}],[\"等人用生成对抗网络提升了图像质量和泛化能力\",{\"1\":{\"884\":1}}],[\"等人用变分自编码器\",{\"1\":{\"884\":1}}],[\"等人提出的\",{\"1\":{\"610\":1}}],[\"等人\",{\"1\":{\"283\":1,\"889\":1}}],[\"等图像分类数据集上用交叉熵损失预训练\",{\"1\":{\"268\":1}}],[\"等基于语音对话的产品也非常受欢迎\",{\"1\":{\"827\":1}}],[\"等基于图像的方法\",{\"1\":{\"157\":1}}],[\"等基础模型\",{\"1\":{\"268\":1}}],[\"等自动编码器结构\",{\"1\":{\"248\":1}}],[\"等概率地可能出现\",{\"1\":{\"235\":1}}],[\"等将连续卷积应用于粒子流体动力学\",{\"1\":{\"110\":1}}],[\"等研究局部谱图卷积\",{\"1\":{\"110\":1}}],[\"等指标更关注重合度\",{\"1\":{\"106\":1}}],[\"等\",{\"1\":{\"26\":1,\"86\":1,\"87\":1,\"91\":2,\"102\":1,\"165\":1,\"208\":1,\"216\":12,\"305\":1,\"308\":1,\"326\":1,\"339\":1,\"346\":1,\"382\":1,\"434\":1,\"472\":1,\"510\":5,\"521\":2,\"658\":1,\"666\":1,\"679\":1,\"680\":1,\"699\":1,\"819\":2,\"822\":2,\"823\":1}}],[\"等任务上超越chinchilla\",{\"1\":{\"666\":1}}],[\"等任务上表现退化\",{\"1\":{\"657\":1}}],[\"等任务上的零样本表现\",{\"1\":{\"656\":1}}],[\"等任务上的局限性\",{\"1\":{\"646\":1}}],[\"等任务中也得到了广泛实践\",{\"1\":{\"655\":1}}],[\"等任务表现优异\",{\"1\":{\"335\":1}}],[\"等任务专用数据\",{\"1\":{\"332\":1}}],[\"等任务\",{\"1\":{\"19\":1,\"385\":1}}],[\"如普通\",{\"1\":{\"963\":1}}],[\"如均值和方差的加权\",{\"1\":{\"947\":1}}],[\"如马尔可夫链蒙特卡洛\",{\"1\":{\"942\":1}}],[\"如前\",{\"1\":{\"895\":1}}],[\"如关闭\",{\"1\":{\"895\":1}}],[\"如除文本外的其他标签\",{\"1\":{\"884\":1}}],[\"如物体部分标签\",{\"1\":{\"884\":1}}],[\"如多层嵌套\",{\"1\":{\"811\":1}}],[\"如z\",{\"1\":{\"811\":1}}],[\"如sub\",{\"1\":{\"811\":1}}],[\"如simclr\",{\"1\":{\"285\":1}}],[\"如y\",{\"1\":{\"809\":1}}],[\"如模型预测\",{\"1\":{\"807\":2}}],[\"如模型深度\",{\"1\":{\"304\":1}}],[\"如平方函数\",{\"1\":{\"779\":1}}],[\"如自注意力层和前馈层\",{\"1\":{\"741\":1}}],[\"如自然语言推理\",{\"1\":{\"649\":1,\"679\":1}}],[\"如判断哪个是答案的开始\",{\"1\":{\"735\":1}}],[\"如正弦\",{\"1\":{\"707\":1}}],[\"如正样本少则增大\",{\"1\":{\"589\":1}}],[\"如xlnet使用10倍于bert的数据\",{\"1\":{\"687\":1}}],[\"如xlnet\",{\"1\":{\"687\":1}}],[\"如8k\",{\"1\":{\"681\":1}}],[\"如800×1300\",{\"1\":{\"331\":1}}],[\"如对比\",{\"1\":{\"680\":1}}],[\"如对抗性推理\",{\"1\":{\"648\":1}}],[\"如动态掩码\",{\"1\":{\"679\":1}}],[\"如动量率\",{\"1\":{\"292\":1}}],[\"如动量模块\",{\"1\":{\"269\":1}}],[\"如延长训练时间\",{\"1\":{\"678\":1}}],[\"如wikipedia引用过滤\",{\"1\":{\"670\":1}}],[\"如word2vec\",{\"1\":{\"355\":1,\"639\":1}}],[\"如后续alpaca\",{\"1\":{\"669\":1}}],[\"如象棋开局策略分析\",{\"1\":{\"669\":1}}],[\"如html标签清理的正则表达式\",{\"1\":{\"669\":1}}],[\"如a\",{\"1\":{\"806\":1,\"809\":1}}],[\"如astronomy\",{\"1\":{\"669\":1}}],[\"如and\",{\"1\":{\"500\":1}}],[\"如boolq\",{\"1\":{\"668\":1}}],[\"如bert\",{\"1\":{\"639\":1,\"642\":1,\"650\":1}}],[\"如bert和uniter\",{\"1\":{\"391\":1}}],[\"如bed\",{\"1\":{\"92\":1}}],[\"如小模型长期训练\",{\"1\":{\"667\":1}}],[\"如标点缺失\",{\"1\":{\"667\":1}}],[\"如上图所示\",{\"1\":{\"805\":1}}],[\"如上图\",{\"1\":{\"694\":1}}],[\"如上描述是我们通常认知的gpt推理过程\",{\"1\":{\"660\":1}}],[\"如上一节所述\",{\"1\":{\"273\":1}}],[\"如针对少数群体敏感任务\",{\"1\":{\"658\":1}}],[\"如非英语任务\",{\"1\":{\"658\":1}}],[\"如法语\",{\"1\":{\"657\":1}}],[\"如法线\",{\"1\":{\"137\":1}}],[\"如选择\",{\"1\":{\"657\":1}}],[\"如是否\",{\"1\":{\"656\":1}}],[\"如创作\",{\"1\":{\"656\":1}}],[\"如摘要\",{\"1\":{\"655\":1}}],[\"如医疗\",{\"1\":{\"649\":1}}],[\"如医学图像\",{\"1\":{\"592\":1}}],[\"如ner\",{\"1\":{\"649\":1}}],[\"如roberta训练500k步\",{\"1\":{\"681\":1}}],[\"如rte和anli\",{\"1\":{\"648\":1}}],[\"如rag\",{\"1\":{\"648\":1}}],[\"如retinanet\",{\"1\":{\"589\":1}}],[\"如relu\",{\"1\":{\"500\":2}}],[\"如resnet\",{\"1\":{\"298\":1,\"303\":1}}],[\"如结合双向架构或多模态训练\",{\"1\":{\"646\":1}}],[\"如结合clip和dinov2\",{\"1\":{\"327\":1}}],[\"如计算成本\",{\"1\":{\"646\":1}}],[\"如词向量\",{\"1\":{\"646\":1}}],[\"如词汇蕴含\",{\"1\":{\"634\":1}}],[\"如偏见和能源消耗\",{\"1\":{\"645\":1}}],[\"如在glue等基准上的表现\",{\"1\":{\"642\":1}}],[\"如用文档中的人名回答\",{\"1\":{\"641\":1}}],[\"如perspectiveapi过滤\",{\"1\":{\"670\":1}}],[\"如ptb\",{\"1\":{\"641\":1,\"648\":1}}],[\"如pos\",{\"1\":{\"627\":1}}],[\"如翻译后的句子\",{\"1\":{\"741\":1}}],[\"如翻译对\",{\"1\":{\"640\":1,\"642\":1}}],[\"如翻译\",{\"1\":{\"640\":1}}],[\"如elmo\",{\"1\":{\"639\":1}}],[\"如问答时生成而非抽取答案\",{\"1\":{\"642\":1}}],[\"如问答对\",{\"1\":{\"640\":1}}],[\"如问答\",{\"1\":{\"636\":1,\"638\":1,\"639\":1}}],[\"如有序句子对\",{\"1\":{\"631\":1}}],[\"如作者在实验中证明的\",{\"1\":{\"626\":1}}],[\"如机翻\",{\"1\":{\"626\":1}}],[\"如语言建模\",{\"1\":{\"687\":1}}],[\"如语言模型\",{\"1\":{\"626\":1}}],[\"如语义分割\",{\"1\":{\"304\":1}}],[\"如t转置\",{\"1\":{\"808\":1}}],[\"如tinypytorch中每一步运算都会动态创建计算图链接\",{\"1\":{\"811\":1}}],[\"如tinypytorch框架\",{\"1\":{\"810\":1}}],[\"如tinypytorch\",{\"1\":{\"806\":1}}],[\"如triviaqa\",{\"1\":{\"668\":1}}],[\"如transformer\",{\"1\":{\"646\":1}}],[\"如tversky\",{\"1\":{\"593\":1}}],[\"如tensorflow\",{\"1\":{\"811\":1}}],[\"如tensorboard\",{\"1\":{\"265\":1}}],[\"如textvqa\",{\"1\":{\"323\":1}}],[\"如欺诈检测\",{\"1\":{\"589\":1}}],[\"如功能区域边缘不确定性较高\",{\"1\":{\"588\":1}}],[\"如经过\",{\"1\":{\"586\":1}}],[\"如经过下采样后的点\",{\"1\":{\"145\":1}}],[\"如身高标准化后波动在\",{\"1\":{\"578\":1}}],[\"如年龄\",{\"1\":{\"578\":1}}],[\"如需进一步帮助\",{\"1\":{\"554\":1}}],[\"如需了解各类优化手段\",{\"1\":{\"396\":1}}],[\"如此往复循环\",{\"1\":{\"547\":1}}],[\"如此反复\",{\"1\":{\"434\":1}}],[\"如加法\",{\"1\":{\"546\":1}}],[\"如池化\",{\"1\":{\"500\":1}}],[\"如池化层\",{\"1\":{\"121\":1}}],[\"如边缘→纹理→物体\",{\"1\":{\"500\":1}}],[\"如残差块\",{\"1\":{\"500\":1}}],[\"如卷积核\",{\"1\":{\"500\":1}}],[\"如大数相减损失精度\",{\"1\":{\"500\":1}}],[\"如泰勒展开\",{\"1\":{\"500\":2}}],[\"如构造重复输入\",{\"1\":{\"472\":1}}],[\"如列表\",{\"1\":{\"466\":1}}],[\"如下面的公式所示\",{\"1\":{\"960\":1}}],[\"如下\",{\"1\":{\"867\":1}}],[\"如下表2\",{\"1\":{\"635\":1}}],[\"如下例所示\",{\"1\":{\"542\":2}}],[\"如下图所示\",{\"1\":{\"429\":1,\"546\":1,\"609\":1,\"805\":1,\"816\":1,\"923\":1,\"956\":1}}],[\"如下所示\",{\"1\":{\"213\":1,\"540\":1,\"544\":1,\"881\":1}}],[\"如交叉熵损失\",{\"1\":{\"427\":1}}],[\"如局部性和平移不变性\",{\"1\":{\"422\":1}}],[\"如variable与数值\",{\"1\":{\"809\":1}}],[\"如vse++\",{\"1\":{\"415\":1}}],[\"如vilt\",{\"1\":{\"415\":1}}],[\"如vilbert和lxmert\",{\"1\":{\"391\":1}}],[\"如vicuna\",{\"1\":{\"303\":1}}],[\"如vit\",{\"1\":{\"298\":2,\"303\":1,\"421\":1}}],[\"如function和临时variable\",{\"1\":{\"806\":1}}],[\"如f\",{\"1\":{\"806\":1}}],[\"如faster\",{\"1\":{\"389\":1}}],[\"如flamingo\",{\"1\":{\"300\":1}}],[\"如基于\",{\"1\":{\"388\":1}}],[\"如准确率\",{\"1\":{\"382\":1}}],[\"如解码器中的自回归屏蔽\",{\"1\":{\"380\":1}}],[\"如从词级别信息到更高的\",{\"1\":{\"627\":1}}],[\"如从\",{\"1\":{\"380\":1}}],[\"如分割\",{\"1\":{\"352\":1}}],[\"如分类\",{\"1\":{\"228\":1}}],[\"如检测\",{\"1\":{\"348\":1}}],[\"如检索\",{\"1\":{\"303\":1,\"304\":1}}],[\"如mae和beit\",{\"1\":{\"413\":1}}],[\"如mmmu\",{\"1\":{\"336\":1}}],[\"如moco和simclr\",{\"1\":{\"413\":1}}],[\"如moco\",{\"1\":{\"289\":1,\"355\":1}}],[\"如dot\",{\"1\":{\"815\":1}}],[\"如docvqa\",{\"1\":{\"336\":1}}],[\"如drop\",{\"1\":{\"648\":1}}],[\"如dropout\",{\"1\":{\"500\":1}}],[\"如deepseek\",{\"1\":{\"327\":1}}],[\"如224×224\",{\"1\":{\"330\":1}}],[\"如256\",{\"1\":{\"206\":1}}],[\"如40个epoch\",{\"1\":{\"681\":1}}],[\"如448×448图像对应256个token\",{\"1\":{\"329\":1}}],[\"如48\",{\"1\":{\"75\":1}}],[\"如文本与生成图像的紧密契合度\",{\"1\":{\"894\":1}}],[\"如文本描述\",{\"1\":{\"894\":1}}],[\"如文本生成\",{\"1\":{\"420\":1}}],[\"如文本或图像\",{\"1\":{\"31\":1}}],[\"如文档图像\",{\"1\":{\"330\":1}}],[\"如文档\",{\"1\":{\"327\":1}}],[\"如ureader\",{\"1\":{\"326\":1}}],[\"如lambada\",{\"1\":{\"641\":1}}],[\"如lay\",{\"1\":{\"92\":1}}],[\"如llava\",{\"1\":{\"326\":1}}],[\"如低分辨率用于场景描述\",{\"1\":{\"323\":1}}],[\"如ocr和中文场景理解\",{\"1\":{\"323\":1}}],[\"如重复文本\",{\"1\":{\"305\":1}}],[\"如零样本图像分类和检索\",{\"1\":{\"304\":1}}],[\"如描述\",{\"1\":{\"303\":1}}],[\"如core\",{\"1\":{\"810\":1}}],[\"如coqa\",{\"1\":{\"648\":1}}],[\"如clip\",{\"1\":{\"303\":2,\"415\":1}}],[\"如chatglm\",{\"1\":{\"299\":1}}],[\"如kosmos\",{\"1\":{\"300\":1}}],[\"如qformer\",{\"1\":{\"300\":1}}],[\"如qformer或线性投影\",{\"1\":{\"296\":1,\"304\":1}}],[\"如glue\",{\"1\":{\"677\":1}}],[\"如gpt\",{\"1\":{\"299\":1,\"323\":1,\"335\":1,\"641\":1,\"646\":1,\"654\":1,\"666\":1}}],[\"如gradscaler\",{\"1\":{\"265\":1}}],[\"如65b毒性最高\",{\"1\":{\"670\":1}}],[\"如6b参数\",{\"1\":{\"336\":1}}],[\"如6×\",{\"1\":{\"291\":1}}],[\"如68\",{\"1\":{\"76\":1}}],[\"如使用relu\",{\"1\":{\"500\":1}}],[\"如使用\",{\"1\":{\"271\":1}}],[\"如33b模型在1\",{\"1\":{\"668\":1}}],[\"如336×336或448×448\",{\"1\":{\"323\":1,\"326\":1}}],[\"如34b参数\",{\"1\":{\"336\":1}}],[\"如3通道rgb\",{\"1\":{\"266\":1}}],[\"如36\",{\"1\":{\"75\":1}}],[\"如颜色抖动\",{\"1\":{\"264\":1}}],[\"如不同数据增强后的同一图像\",{\"1\":{\"246\":1}}],[\"如视觉问答\",{\"1\":{\"375\":1}}],[\"如视觉问答或视觉推理\",{\"1\":{\"369\":1}}],[\"如视觉\",{\"1\":{\"222\":1}}],[\"如跨模态检索\",{\"1\":{\"220\":1}}],[\"如两层\",{\"1\":{\"214\":1}}],[\"如原始像素\",{\"1\":{\"210\":1}}],[\"如知识蒸馏\",{\"1\":{\"208\":1}}],[\"如论文中的图\",{\"1\":{\"202\":1}}],[\"如方差\",{\"1\":{\"160\":1}}],[\"如时间序列点云\",{\"1\":{\"157\":1}}],[\"如只有几十个点\",{\"1\":{\"157\":1}}],[\"如弯曲\",{\"1\":{\"157\":1}}],[\"如人体姿态变化\",{\"1\":{\"157\":1}}],[\"如椅子腿和桌面连接处\",{\"1\":{\"157\":1}}],[\"如旋转\",{\"1\":{\"149\":1,\"152\":1,\"153\":1}}],[\"如最大池化\",{\"1\":{\"148\":1}}],[\"如最远点采样\",{\"1\":{\"125\":1}}],[\"如桌子边缘\",{\"1\":{\"157\":1}}],[\"如桌子\",{\"1\":{\"143\":1,\"146\":1}}],[\"如何计算\",{\"1\":{\"944\":1}}],[\"如何计算loss的\",{\"1\":{\"418\":1}}],[\"如何定义潜变量\",{\"1\":{\"944\":1}}],[\"如何控制同一个像素内部的\",{\"1\":{\"924\":1}}],[\"如何仅靠图像生成图像\",{\"1\":{\"893\":1}}],[\"如何获取答案\",{\"1\":{\"735\":1}}],[\"如何利用\",{\"1\":{\"733\":1}}],[\"如何去理解这个位置编码\",{\"1\":{\"706\":1}}],[\"如何生成并起作用的\",{\"0\":{\"703\":1}}],[\"如何生成点集的划分\",{\"1\":{\"131\":1}}],[\"如何建立更强的\",{\"1\":{\"658\":1}}],[\"如何应对多价值体系的冲突\",{\"1\":{\"658\":1}}],[\"如何更有效缓解毒性与偏见\",{\"1\":{\"658\":1}}],[\"如何写好prompt\",{\"0\":{\"617\":1}}],[\"如何对大模型进行微调\",{\"0\":{\"602\":1}}],[\"如何选择\",{\"0\":{\"593\":1}}],[\"如何通过调整步幅\",{\"1\":{\"546\":1}}],[\"如何理解这个过程\",{\"0\":{\"530\":1}}],[\"如何降低模型训练成本\",{\"1\":{\"415\":1}}],[\"如何将不同尺寸的roi特征\",{\"1\":{\"501\":1}}],[\"如何将二维图像转换为一维时间序列\",{\"1\":{\"436\":1}}],[\"如何将这个预训练的视觉模型应用到新的任务中呢\",{\"1\":{\"408\":1}}],[\"如何将其与视觉模态结合成为关键挑战\",{\"1\":{\"299\":1}}],[\"如何实现的\",{\"0\":{\"258\":1}}],[\"如何设计一个能够从这些局部分区中学习有用特征的机制\",{\"1\":{\"131\":1}}],[\"如何有效地对点云进行分区\",{\"1\":{\"131\":1}}],[\"如转置卷积\",{\"1\":{\"122\":1}}],[\"如第一个点云1024个点\",{\"1\":{\"119\":1}}],[\"如句子长度\",{\"1\":{\"524\":1}}],[\"如句子\",{\"1\":{\"112\":1,\"741\":1}}],[\"如减法\",{\"1\":{\"112\":1,\"809\":1}}],[\"如1e\",{\"1\":{\"683\":1}}],[\"如13b\",{\"1\":{\"668\":1}}],[\"如13b模型比gpt\",{\"1\":{\"666\":1}}],[\"如130\",{\"1\":{\"640\":1}}],[\"如10\",{\"1\":{\"639\":1}}],[\"如16×16像素的局部区域\",{\"1\":{\"227\":1}}],[\"如1\",{\"1\":{\"76\":1,\"589\":1,\"815\":1}}],[\"如12\",{\"1\":{\"75\":1}}],[\"如11\",{\"1\":{\"73\":1,\"75\":1}}],[\"如表4所示\",{\"1\":{\"681\":1}}],[\"如表3所示\",{\"1\":{\"681\":1}}],[\"如表20所示\",{\"1\":{\"315\":1}}],[\"如表2所示\",{\"1\":{\"31\":1,\"47\":1,\"224\":1,\"681\":1}}],[\"如表9所示\",{\"1\":{\"292\":1}}],[\"如表5所示\",{\"1\":{\"288\":1}}],[\"如表\",{\"1\":{\"181\":1,\"305\":1,\"678\":3,\"679\":2}}],[\"如表1所示\",{\"1\":{\"75\":1,\"681\":1}}],[\"如果仍然希望引入一个调节项\",{\"1\":{\"951\":1}}],[\"如果这样的函数存在\",{\"1\":{\"949\":1}}],[\"如果这些关键点缺失或被遮挡\",{\"1\":{\"157\":1}}],[\"如果第一层就使用\",{\"1\":{\"923\":1}}],[\"如果第二个分支一直不变\",{\"1\":{\"356\":1}}],[\"如果很不一样\",{\"1\":{\"915\":1}}],[\"如果真实是负类\",{\"1\":{\"910\":1}}],[\"如果真实是正类\",{\"1\":{\"910\":1}}],[\"如果一个事件概率是\",{\"1\":{\"907\":1}}],[\"如果一个变量变大时另一个变小\",{\"1\":{\"574\":1}}],[\"如果事件概率全一样\",{\"1\":{\"907\":1}}],[\"如果条件本身模糊\",{\"1\":{\"894\":1}}],[\"如果启用了稳定策略\",{\"1\":{\"893\":1}}],[\"如果启用了稳定训练策略\",{\"1\":{\"893\":1}}],[\"如果启用权重共享\",{\"1\":{\"892\":1}}],[\"如果概率最高的那个token是图像token\",{\"1\":{\"892\":1}}],[\"如果位置在文本段\",{\"1\":{\"892\":1}}],[\"如果位置在图像段\",{\"1\":{\"892\":1}}],[\"如果其中\",{\"1\":{\"881\":1}}],[\"如果变量是多值离散型的\",{\"1\":{\"857\":1}}],[\"如果没传入\",{\"1\":{\"899\":1}}],[\"如果没病\",{\"1\":{\"850\":1}}],[\"如果没有输入文本\",{\"1\":{\"898\":1}}],[\"如果没有function作用于x上\",{\"1\":{\"775\":1}}],[\"如果没有任何归纳偏置\",{\"1\":{\"422\":1}}],[\"如果没有则下载\",{\"1\":{\"410\":1,\"412\":1}}],[\"如果没有这个字典\",{\"1\":{\"353\":1}}],[\"如果没有新增遮挡\",{\"1\":{\"263\":1}}],[\"如果没有\",{\"1\":{\"152\":1}}],[\"如果没有指定查询点\",{\"1\":{\"119\":2}}],[\"如果没有提供激活函数层\",{\"1\":{\"431\":1}}],[\"如果没有提供归一化层\",{\"1\":{\"431\":1}}],[\"如果没有提供预训练的\",{\"1\":{\"213\":1}}],[\"如果没有提供预计算的索引\",{\"1\":{\"119\":1}}],[\"如果没有提供\",{\"1\":{\"67\":1}}],[\"如果每个\",{\"1\":{\"846\":1}}],[\"如果左操作数未实现\",{\"1\":{\"809\":1}}],[\"如果正确答案没有出现在上下文中\",{\"1\":{\"735\":1}}],[\"如果值大于\",{\"1\":{\"734\":1}}],[\"如果值小于\",{\"1\":{\"734\":1}}],[\"如果本地不存在\",{\"1\":{\"712\":1}}],[\"如果本次成功新增了遮挡patch\",{\"1\":{\"263\":1}}],[\"如果两个事件是无关的\",{\"1\":{\"849\":1}}],[\"如果两个位置的相对距离相同\",{\"1\":{\"708\":1}}],[\"如果两个变量\",{\"1\":{\"574\":2}}],[\"如果训练时\",{\"1\":{\"707\":1}}],[\"如果训练过程中加入了噪声\",{\"1\":{\"157\":1}}],[\"如果遇到不存在于字典中的word\",{\"1\":{\"697\":1}}],[\"如果任一句子长度超过50\",{\"1\":{\"697\":1}}],[\"如果现在的任务是\",{\"1\":{\"694\":4}}],[\"如果现在有这么一句话\",{\"1\":{\"692\":1}}],[\"如果给的例子是比较简单的问题\",{\"1\":{\"622\":1}}],[\"如果给定随机选择的正例和负例\",{\"1\":{\"570\":1}}],[\"如果给定路径\",{\"1\":{\"213\":1}}],[\"如果矩阵\",{\"1\":{\"612\":1}}],[\"如果要用lora适配不同的场景\",{\"1\":{\"606\":1}}],[\"如果要求计算\",{\"1\":{\"274\":1}}],[\"如果将大模型比做一个函数\",{\"1\":{\"604\":1}}],[\"如果数据是不能传递给第三方大模型服务的\",{\"1\":{\"601\":1}}],[\"如果数据集在类别之间大致平衡\",{\"1\":{\"571\":1}}],[\"如果数据集不平衡\",{\"1\":{\"562\":1}}],[\"如果已经包含\",{\"1\":{\"587\":1,\"588\":1}}],[\"如果费用大致相当\",{\"1\":{\"572\":1}}],[\"如果假正例成本较低\",{\"1\":{\"572\":1}}],[\"如果假正例\",{\"1\":{\"572\":1}}],[\"如果忽略所有其他阈值\",{\"1\":{\"569\":1}}],[\"如果您想评估模型在所有可能阈值下的质量\",{\"1\":{\"568\":1}}],[\"如果模型预测\",{\"1\":{\"735\":1}}],[\"如果模型最后一层没有\",{\"1\":{\"590\":1}}],[\"如果模型最后没有\",{\"1\":{\"587\":1,\"588\":1,\"589\":1}}],[\"如果模型的效果与随机猜测或抛硬币的效果完全一样\",{\"1\":{\"570\":1}}],[\"如果模型\",{\"1\":{\"562\":1}}],[\"如果模型过度关注这个困难的负样本\",{\"1\":{\"355\":1}}],[\"如果实际正例的总数与实际负例的总数不接近\",{\"1\":{\"561\":1}}],[\"如果实际邻居数量小于要求的nsample\",{\"1\":{\"119\":1}}],[\"如果提供了\",{\"1\":{\"513\":1}}],[\"如果后续需要用到\",{\"1\":{\"494\":1}}],[\"如果原始数据是按行优先方式存储的\",{\"1\":{\"545\":1}}],[\"如果原始点有自己的特征\",{\"1\":{\"145\":1}}],[\"如果原始点有特征\",{\"1\":{\"145\":1}}],[\"如果原张量不是连续的\",{\"1\":{\"491\":1}}],[\"如果原张量已经是连续的\",{\"1\":{\"491\":1}}],[\"如果指定\",{\"1\":{\"485\":1}}],[\"如果统计结果长度小于\",{\"1\":{\"485\":1}}],[\"如果张量不能整除块数\",{\"1\":{\"482\":1}}],[\"如果张量来源于\",{\"1\":{\"470\":1}}],[\"如果为\",{\"1\":{\"474\":2,\"480\":3,\"590\":1,\"592\":1}}],[\"如果为none\",{\"1\":{\"122\":1}}],[\"如果为none则使用xyz\",{\"1\":{\"119\":1}}],[\"如果为none则重新计算\",{\"1\":{\"119\":1}}],[\"如果类别数大于\",{\"1\":{\"431\":1}}],[\"如果该路径在采样的验证集样本中则存入验证集\",{\"1\":{\"424\":1}}],[\"如果该簇是空簇\",{\"1\":{\"213\":1}}],[\"如果未激活任何环境时使用\",{\"1\":{\"557\":1}}],[\"如果未提供位置id\",{\"1\":{\"419\":1}}],[\"如果未指定\",{\"1\":{\"429\":2}}],[\"如果未指定则使用\",{\"1\":{\"380\":1}}],[\"如果未指定最大宽高比\",{\"1\":{\"263\":1}}],[\"如果进一步采用convirt\",{\"1\":{\"413\":1}}],[\"如果在读取图片过程中出现错误\",{\"1\":{\"410\":1}}],[\"如果开启了\",{\"1\":{\"385\":1,\"898\":1}}],[\"如果传入了归一化层类\",{\"1\":{\"426\":1}}],[\"如果传入的是整数\",{\"1\":{\"426\":1}}],[\"如果传入一个归一化层类\",{\"1\":{\"426\":1}}],[\"如果传入一个整数\",{\"1\":{\"426\":2}}],[\"如果传入\",{\"1\":{\"380\":1}}],[\"如果有残差块\",{\"1\":{\"899\":1}}],[\"如果有病\",{\"1\":{\"850\":1}}],[\"如果有第i个元素\",{\"1\":{\"801\":1}}],[\"如果有一个能直接处理各式\",{\"1\":{\"690\":1}}],[\"如果有缓存\",{\"1\":{\"420\":1}}],[\"如果有缓存的key\",{\"1\":{\"420\":1}}],[\"如果有个类别\",{\"1\":{\"408\":1}}],[\"如果有\",{\"1\":{\"380\":1,\"899\":1}}],[\"如果有相对位置偏置\",{\"1\":{\"380\":1}}],[\"如果有位置编码\",{\"1\":{\"380\":1}}],[\"如果有额外特征\",{\"1\":{\"137\":1,\"141\":1}}],[\"如果有额外的点特征\",{\"1\":{\"137\":1}}],[\"如果你当年看完vq\",{\"1\":{\"961\":1}}],[\"如果你直接输入0\",{\"1\":{\"956\":1}}],[\"如果你把自己随机生成出来的向量输入给解码器\",{\"1\":{\"956\":1}}],[\"如果你在其他图像生成文献中见到了\",{\"1\":{\"925\":1}}],[\"如果你在多\",{\"1\":{\"364\":1}}],[\"如果你用\",{\"1\":{\"909\":1}}],[\"如果你能谈一堆事件\",{\"1\":{\"847\":1}}],[\"如果你能谈某个事件\",{\"1\":{\"847\":1}}],[\"如果你希望模型能\",{\"1\":{\"735\":1}}],[\"如果你希望装饰器\",{\"1\":{\"453\":1}}],[\"如果你设置\",{\"1\":{\"590\":2}}],[\"如果你的模型最后没有\",{\"1\":{\"586\":1}}],[\"如果你有\",{\"1\":{\"574\":1}}],[\"如果你有一张人脸的点云\",{\"1\":{\"157\":1}}],[\"如果你想删除某个环境\",{\"1\":{\"554\":1}}],[\"如果你想获得底层数据存储区的指针\",{\"1\":{\"492\":1}}],[\"如果你想把一个\",{\"1\":{\"466\":1}}],[\"如果你接下来要对它们执行\",{\"1\":{\"468\":1}}],[\"如果选取的抽样部分过少\",{\"1\":{\"355\":1}}],[\"如果把向量的某一维稍微改动0\",{\"1\":{\"956\":1}}],[\"如果把\",{\"1\":{\"885\":1}}],[\"如果把这个损失函数\",{\"1\":{\"355\":1}}],[\"如果把对比学习的过程看成一个动态字典的过程\",{\"1\":{\"353\":1}}],[\"如果字典很小\",{\"1\":{\"353\":1}}],[\"如果采用\",{\"1\":{\"352\":1}}],[\"如果我们能让\",{\"1\":{\"945\":1}}],[\"如果我们希望图3\",{\"1\":{\"944\":1}}],[\"如果我们希望强化高注意力权重区域\",{\"1\":{\"582\":1}}],[\"如果我们直接\",{\"1\":{\"931\":1}}],[\"如果我们直接使用类别标签作为文本描述\",{\"1\":{\"409\":1}}],[\"如果我们抛硬币100次均出现正面\",{\"1\":{\"903\":1}}],[\"如果我们从\",{\"1\":{\"872\":1}}],[\"如果我们不对这两个梯度求和\",{\"1\":{\"803\":1}}],[\"如果我们不进行累加\",{\"1\":{\"803\":1}}],[\"如果我们用多种不同的方法去求解\",{\"1\":{\"621\":1}}],[\"如果我们画出一个二维正态分布\",{\"1\":{\"574\":1}}],[\"如果我们将标准答案作为列\",{\"1\":{\"561\":1}}],[\"如果我们做如下切片操作\",{\"1\":{\"544\":1}}],[\"如果我们有一个没有标注的数据集\",{\"1\":{\"350\":1}}],[\"如果我本来就有特征\",{\"1\":{\"145\":1}}],[\"如果所有样本映射到相同的表示\",{\"1\":{\"285\":1}}],[\"如果用\",{\"1\":{\"892\":1}}],[\"如果用户只需要返回\",{\"1\":{\"274\":1}}],[\"如果用余弦相似度\",{\"1\":{\"213\":1}}],[\"如果定义了图像预处理转换操作\",{\"1\":{\"424\":1}}],[\"如果定义了\",{\"1\":{\"266\":1}}],[\"如果存在一个常数\",{\"1\":{\"917\":1}}],[\"如果存在多个function先后作用在x上\",{\"1\":{\"775\":1}}],[\"如果存在function作用在x上\",{\"1\":{\"775\":1}}],[\"如果存在记录出现次数\",{\"1\":{\"596\":1}}],[\"如果存在\",{\"1\":{\"266\":1,\"382\":1}}],[\"如果最后一个batch数量不足\",{\"1\":{\"265\":1}}],[\"如果是原始图像\",{\"1\":{\"893\":1}}],[\"如果是全参数微调\",{\"1\":{\"611\":1}}],[\"如果是解决自己日常生活\",{\"1\":{\"601\":1}}],[\"如果是\",{\"1\":{\"558\":2,\"611\":1,\"895\":1,\"917\":2}}],[\"如果是beam\",{\"1\":{\"421\":1}}],[\"如果是个体判别任务\",{\"1\":{\"350\":1}}],[\"如果是自定义\",{\"1\":{\"264\":1}}],[\"如果是分割任务\",{\"1\":{\"154\":2}}],[\"如果是分类任务\",{\"1\":{\"154\":2}}],[\"如果输入了图像\",{\"1\":{\"893\":1}}],[\"如果输入是整数\",{\"1\":{\"263\":1}}],[\"如果输入特征只有\",{\"1\":{\"123\":1}}],[\"如果直接把它们的交叉熵损失简单相加\",{\"1\":{\"893\":1}}],[\"如果直接把残差相加\",{\"1\":{\"380\":1}}],[\"如果直接给每个位置都分配一个独立参数\",{\"1\":{\"710\":1}}],[\"如果直接为每个距离学习偏置\",{\"1\":{\"710\":1}}],[\"如果直接采用像素级自编码\",{\"1\":{\"234\":1}}],[\"如果直接作为变换矩阵\",{\"1\":{\"152\":1}}],[\"如果二者参数共享\",{\"1\":{\"179\":1}}],[\"如果共享\",{\"1\":{\"179\":1}}],[\"如果完全不共享参数\",{\"1\":{\"179\":1}}],[\"如果对于任意排列\",{\"1\":{\"160\":1}}],[\"如果点太少\",{\"1\":{\"157\":1}}],[\"如果不要求计算损失\",{\"1\":{\"893\":1}}],[\"如果不使用\",{\"1\":{\"522\":1}}],[\"如果不指定就保持输入\",{\"1\":{\"481\":1}}],[\"如果不是则抛出异常\",{\"1\":{\"424\":1}}],[\"如果不提前了解一下库的基本用法\",{\"1\":{\"379\":1}}],[\"如果不需要返回\",{\"1\":{\"899\":1}}],[\"如果不需要重建图像\",{\"1\":{\"256\":1,\"899\":1}}],[\"如果不需要\",{\"1\":{\"256\":1,\"274\":1}}],[\"如果不加处理\",{\"1\":{\"152\":1}}],[\"如果不够就重复最近的点来填充\",{\"1\":{\"137\":1}}],[\"如果只有1个下采样点\",{\"1\":{\"145\":1}}],[\"如果只用\",{\"1\":{\"100\":1}}],[\"如果某个维度的数值范围很大\",{\"1\":{\"578\":1}}],[\"如果某个查询点附近的点太少\",{\"1\":{\"137\":1}}],[\"如果某个点到新中心点的距离比之前记录的\",{\"1\":{\"137\":1}}],[\"如果需要\",{\"1\":{\"120\":1,\"213\":1,\"380\":1,\"609\":1}}],[\"如果\",{\"1\":{\"120\":1,\"137\":1,\"157\":2,\"265\":1,\"293\":1,\"351\":1,\"355\":2,\"380\":1,\"487\":2,\"592\":2,\"700\":2,\"709\":1,\"710\":1,\"733\":1,\"849\":1,\"856\":1,\"857\":1,\"861\":1,\"865\":1,\"877\":2,\"893\":2,\"908\":2,\"913\":1,\"915\":1,\"945\":1,\"946\":1,\"949\":3,\"951\":1}}],[\"如果use\",{\"1\":{\"119\":1}}],[\"如果包含强度值\",{\"1\":{\"107\":1}}],[\"如果当前待预测token位置属于离散视觉词\",{\"1\":{\"892\":1}}],[\"如果当前待预测token位置属于文本词\",{\"1\":{\"892\":1}}],[\"如果当前\",{\"1\":{\"106\":1}}],[\"如果当前样本没有正类\",{\"1\":{\"106\":1}}],[\"如果当前是训练模式\",{\"1\":{\"92\":1}}],[\"如果当前物体类别在排序后的字典中\",{\"1\":{\"92\":1}}],[\"如果当前物体类别不在排序后的字典中\",{\"1\":{\"92\":1}}],[\"如果使用确定映射\",{\"1\":{\"943\":1}}],[\"如果使用了\",{\"1\":{\"893\":1}}],[\"如果使用多头注意力\",{\"1\":{\"534\":1}}],[\"如果使用余弦相似度\",{\"1\":{\"213\":1}}],[\"如果使用\",{\"1\":{\"64\":1,\"188\":1,\"264\":2,\"899\":1}}],[\"如54\",{\"1\":{\"73\":1,\"75\":1}}],[\"如5\",{\"1\":{\"31\":1}}],[\"如int\",{\"1\":{\"809\":1}}],[\"如internvit\",{\"1\":{\"327\":1,\"332\":1}}],[\"如internvl\",{\"1\":{\"30\":1,\"31\":1}}],[\"如imagenet\",{\"1\":{\"298\":1,\"413\":1,\"497\":1}}],[\"如iagnet\",{\"1\":{\"30\":1}}],[\"如手柄的抓握属性\",{\"1\":{\"30\":1}}],[\"如将\",{\"1\":{\"30\":1,\"31\":1}}],[\"如图7所示\",{\"1\":{\"290\":1}}],[\"如图6所示\",{\"1\":{\"289\":1}}],[\"如图文检索和零样本图像分类\",{\"1\":{\"271\":1}}],[\"如图文检索\",{\"1\":{\"268\":1}}],[\"如图文对比\",{\"1\":{\"223\":1}}],[\"如图文匹配\",{\"1\":{\"220\":1}}],[\"如图\",{\"1\":{\"212\":1,\"228\":1,\"229\":1,\"306\":2,\"370\":1,\"647\":2,\"655\":1,\"878\":1}}],[\"如图4\",{\"1\":{\"115\":1}}],[\"如图4所示\",{\"1\":{\"47\":1}}],[\"如图3所示\",{\"1\":{\"78\":1,\"214\":1,\"222\":1,\"273\":1,\"303\":1,\"944\":1}}],[\"如图3\",{\"1\":{\"41\":1,\"43\":1}}],[\"如图像转置\",{\"1\":{\"467\":1}}],[\"如图像重建\",{\"1\":{\"355\":1}}],[\"如图像分类与语义分割\",{\"1\":{\"252\":1}}],[\"如图像分类\",{\"1\":{\"229\":1}}],[\"如图像分类和语义分割\",{\"1\":{\"217\":1}}],[\"如图像描述\",{\"1\":{\"220\":1,\"304\":1}}],[\"如图像或点云特征\",{\"1\":{\"67\":1}}],[\"如图像\",{\"1\":{\"31\":1,\"67\":1,\"112\":1}}],[\"如图1c所示\",{\"1\":{\"296\":1}}],[\"如图15所示\",{\"1\":{\"80\":1}}],[\"如图1所示\",{\"1\":{\"31\":1,\"109\":1,\"303\":1,\"667\":1}}],[\"如图1\",{\"1\":{\"30\":1,\"73\":1}}],[\"如图2\",{\"1\":{\"73\":1,\"76\":1}}],[\"如图2所示\",{\"1\":{\"32\":1,\"109\":1,\"221\":1,\"222\":1,\"272\":2,\"296\":1,\"944\":1}}],[\"如图2右侧\",{\"0\":{\"23\":1}}],[\"如图2左侧\",{\"0\":{\"22\":1}}],[\"如瓶子的\",{\"1\":{\"27\":1}}],[\"如\",{\"1\":{\"26\":1,\"30\":3,\"47\":1,\"48\":1,\"52\":5,\"53\":1,\"56\":4,\"69\":1,\"70\":3,\"73\":1,\"87\":1,\"91\":2,\"95\":1,\"102\":1,\"106\":1,\"110\":1,\"112\":1,\"125\":1,\"145\":2,\"157\":3,\"160\":1,\"171\":1,\"173\":1,\"177\":1,\"192\":1,\"202\":1,\"204\":1,\"205\":2,\"208\":1,\"210\":1,\"212\":1,\"213\":2,\"223\":1,\"240\":1,\"247\":1,\"251\":1,\"252\":1,\"255\":1,\"263\":3,\"264\":2,\"265\":1,\"268\":2,\"269\":5,\"271\":1,\"272\":1,\"277\":1,\"280\":2,\"293\":1,\"305\":2,\"306\":1,\"308\":1,\"310\":1,\"311\":1,\"322\":2,\"339\":5,\"341\":3,\"342\":2,\"346\":1,\"368\":2,\"369\":3,\"388\":1,\"398\":1,\"420\":2,\"425\":1,\"429\":1,\"469\":1,\"472\":1,\"500\":5,\"502\":1,\"513\":2,\"514\":3,\"521\":1,\"546\":1,\"557\":2,\"574\":1,\"589\":5,\"595\":1,\"611\":1,\"614\":2,\"625\":1,\"640\":3,\"641\":1,\"650\":4,\"655\":1,\"657\":1,\"658\":5,\"666\":1,\"678\":1,\"679\":2,\"680\":1,\"710\":1,\"809\":1,\"815\":1,\"819\":1,\"884\":1,\"889\":1,\"894\":1,\"898\":1,\"924\":1,\"950\":1}}],[\"有许多图像生成类任务的前沿工作都使用了一种叫做\",{\"1\":{\"955\":1}}],[\"有噪声的\",{\"1\":{\"897\":1}}],[\"有条件\",{\"1\":{\"894\":1}}],[\"有条件预测\",{\"1\":{\"894\":1}}],[\"有条件生成\",{\"1\":{\"893\":1}}],[\"有病\",{\"1\":{\"850\":1}}],[\"有病的人不能诊断为健康\",{\"1\":{\"566\":1}}],[\"有较好的可解释性和可追踪性\",{\"1\":{\"830\":1}}],[\"有限的数据集可能无法显著提高性能\",{\"1\":{\"830\":1}}],[\"有限上下文容量成为当前few\",{\"1\":{\"649\":1}}],[\"有标签数据\",{\"1\":{\"824\":1}}],[\"有5万次\",{\"1\":{\"816\":1}}],[\"有一些图像生成模型\",{\"1\":{\"956\":1}}],[\"有一个特别的约束\",{\"1\":{\"921\":1}}],[\"有一个问题\",{\"1\":{\"501\":1}}],[\"有一点不同就是bert预训练阶段的学习目标是\",{\"1\":{\"700\":1}}],[\"有的问题就是没有答案的\",{\"1\":{\"694\":1}}],[\"有的是不同义的\",{\"1\":{\"634\":1}}],[\"有的是同义的\",{\"1\":{\"634\":1}}],[\"有帮助\",{\"1\":{\"654\":1,\"679\":1}}],[\"有监督微调\",{\"0\":{\"630\":1}}],[\"有监督预训练还是占据主导地位\",{\"1\":{\"353\":1}}],[\"有两个主要原因\",{\"1\":{\"626\":1}}],[\"有2个是正确的\",{\"1\":{\"621\":1}}],[\"有1个是错的\",{\"1\":{\"621\":1}}],[\"有几个注意点\",{\"1\":{\"620\":1}}],[\"有以下一些原因\",{\"1\":{\"601\":1}}],[\"有以下几个优点\",{\"1\":{\"588\":1}}],[\"有没有捷径可走呢\",{\"1\":{\"945\":1}}],[\"有没有低成本的方法微调大模型\",{\"1\":{\"610\":1}}],[\"有没有办法能够减少\",{\"1\":{\"609\":1}}],[\"有没有覆盖正确区域\",{\"1\":{\"586\":1}}],[\"有没有可能将\",{\"1\":{\"354\":1}}],[\"有偏置\",{\"1\":{\"522\":1}}],[\"有\",{\"1\":{\"514\":2,\"542\":1,\"630\":1,\"657\":1,\"691\":3,\"822\":1,\"848\":3,\"850\":1,\"851\":1,\"871\":1,\"877\":1,\"945\":1}}],[\"有可能因为对于现有的任务来说\",{\"1\":{\"394\":1}}],[\"有了掩码卷积层后\",{\"1\":{\"926\":1}}],[\"有了分词器后\",{\"1\":{\"698\":1}}],[\"有了\",{\"1\":{\"382\":1,\"663\":1}}],[\"有了以上\",{\"1\":{\"99\":1}}],[\"有干扰\",{\"1\":{\"382\":1}}],[\"有个细节\",{\"1\":{\"356\":1}}],[\"有时我们也会讨论高斯分布的精度\",{\"1\":{\"865\":1}}],[\"有时会产生与客观事实不符的信息\",{\"1\":{\"828\":1}}],[\"有时更低\",{\"1\":{\"657\":1}}],[\"有时更贴近人类学习习惯\",{\"1\":{\"647\":1}}],[\"有时也会加入一个平滑项\",{\"1\":{\"586\":1}}],[\"有时能提升计算效率\",{\"1\":{\"492\":1}}],[\"有时只取\",{\"1\":{\"293\":1}}],[\"有时裁掉少量边缘\",{\"1\":{\"293\":1}}],[\"有关\",{\"1\":{\"283\":1}}],[\"有利于视觉识别类任务\",{\"1\":{\"272\":1}}],[\"有归一化操作\",{\"1\":{\"264\":1}}],[\"有助于构建更通用\",{\"1\":{\"658\":1}}],[\"有助于迁移\",{\"1\":{\"635\":1}}],[\"有助于模型学习到不同方向的特征\",{\"1\":{\"425\":1}}],[\"有助于提升对复杂图文对的泛化能力\",{\"1\":{\"374\":1}}],[\"有助于加快收敛\",{\"1\":{\"344\":1}}],[\"有助于全局信息流向局部\",{\"1\":{\"215\":1}}],[\"有助于cls\",{\"1\":{\"215\":1}}],[\"有助于预训练表征\",{\"1\":{\"215\":1}}],[\"有多少个\",{\"1\":{\"846\":1}}],[\"有多少个离散\",{\"1\":{\"213\":1}}],[\"有多少点被正确分类\",{\"1\":{\"586\":1}}],[\"有多少组点云\",{\"1\":{\"152\":1}}],[\"有些人可能会认为\",{\"1\":{\"951\":1}}],[\"有些模型是\",{\"1\":{\"925\":1}}],[\"有些\",{\"1\":{\"202\":1,\"293\":1}}],[\"有些位置被标记为\",{\"1\":{\"137\":1}}],[\"有跳跃连接\",{\"1\":{\"122\":1}}],[\"有信息交互但无注意力机制\",{\"1\":{\"117\":1}}],[\"有效突破了输入长度的限制\",{\"1\":{\"828\":1}}],[\"有效\",{\"1\":{\"657\":1}}],[\"有效地从无标记的原始文本中学习的能力能减轻对监督学习的依赖\",{\"1\":{\"626\":1}}],[\"有效适应输入图像的不同分辨率和宽高比\",{\"1\":{\"331\":1}}],[\"有效的权重变换\",{\"1\":{\"522\":1}}],[\"有效的关键组成\",{\"1\":{\"242\":1}}],[\"有效的部分\",{\"1\":{\"67\":1}}],[\"有效提升了下游任务中的表现\",{\"1\":{\"185\":1}}],[\"有效性检查\",{\"1\":{\"122\":1}}],[\"有效语言部分\",{\"1\":{\"67\":1}}],[\"有望提升机器人在未知环境中的自主交互能力\",{\"1\":{\"50\":1}}],[\"有丰富的大规模数据和成熟的预训练基础模型\",{\"1\":{\"26\":1}}],[\"有问题需要咨询的小伙伴\",{\"1\":{\"2\":1}}],[\"0时\",{\"1\":{\"809\":1}}],[\"0+cu113\",{\"1\":{\"739\":1}}],[\"0~255\",{\"1\":{\"926\":1}}],[\"0~total\",{\"1\":{\"892\":1}}],[\"0~seq\",{\"1\":{\"892\":1}}],[\"0~num\",{\"1\":{\"710\":3}}],[\"0~100\",{\"1\":{\"578\":1}}],[\"0~1\",{\"1\":{\"70\":1,\"83\":1,\"513\":1}}],[\"0和sst\",{\"1\":{\"683\":1}}],[\"0分\",{\"1\":{\"670\":1}}],[\"0或1\",{\"1\":{\"589\":1}}],[\"08612\",{\"1\":{\"899\":1}}],[\"087\",{\"1\":{\"670\":1}}],[\"081\",{\"1\":{\"668\":1,\"670\":1}}],[\"08\",{\"1\":{\"344\":1,\"502\":1}}],[\"08485\",{\"1\":{\"338\":1}}],[\"0到1\",{\"1\":{\"325\":1}}],[\"01\",{\"1\":{\"308\":1,\"700\":1}}],[\"012个点云和5\",{\"1\":{\"73\":1}}],[\"04\",{\"1\":{\"286\":2,\"811\":2}}],[\"04744\",{\"1\":{\"61\":1}}],[\"047\",{\"1\":{\"25\":1}}],[\"0表示未遮挡\",{\"1\":{\"263\":1}}],[\"02等\",{\"1\":{\"298\":1}}],[\"02\",{\"1\":{\"236\":2,\"380\":2,\"427\":1,\"428\":2,\"431\":2}}],[\"02413\",{\"1\":{\"130\":1}}],[\"07\",{\"1\":{\"190\":1,\"192\":1,\"205\":1,\"286\":1,\"361\":2}}],[\"001\",{\"1\":{\"205\":1,\"816\":3}}],[\"00593\",{\"1\":{\"147\":1}}],[\"0001\",{\"1\":{\"956\":1}}],[\"0002\",{\"1\":{\"918\":1}}],[\"000+基词\",{\"1\":{\"640\":1}}],[\"000\",{\"1\":{\"7\":1,\"341\":1,\"342\":4,\"633\":1,\"679\":1}}],[\"093\",{\"1\":{\"99\":1,\"106\":1}}],[\"0387\",{\"1\":{\"944\":1}}],[\"03b\",{\"1\":{\"305\":1}}],[\"03\",{\"1\":{\"48\":1,\"712\":1}}],[\"0\",{\"0\":{\"294\":1,\"338\":1,\"424\":1},\"1\":{\"47\":2,\"53\":46,\"54\":1,\"56\":2,\"58\":1,\"59\":14,\"60\":2,\"65\":3,\"67\":4,\"69\":2,\"70\":12,\"82\":25,\"83\":22,\"88\":1,\"92\":4,\"94\":11,\"99\":4,\"100\":2,\"102\":4,\"104\":2,\"105\":2,\"106\":17,\"107\":9,\"119\":30,\"121\":13,\"122\":11,\"123\":16,\"137\":12,\"138\":2,\"141\":14,\"143\":1,\"145\":6,\"146\":2,\"152\":8,\"153\":1,\"154\":1,\"156\":1,\"172\":1,\"176\":1,\"187\":3,\"188\":3,\"190\":61,\"191\":4,\"192\":14,\"199\":1,\"202\":1,\"204\":4,\"205\":4,\"206\":5,\"207\":6,\"208\":6,\"213\":11,\"224\":2,\"234\":1,\"236\":5,\"255\":3,\"256\":1,\"257\":1,\"260\":1,\"263\":9,\"264\":7,\"265\":1,\"266\":4,\"268\":2,\"274\":9,\"285\":1,\"286\":4,\"293\":27,\"294\":1,\"309\":1,\"315\":2,\"316\":1,\"318\":2,\"319\":1,\"338\":1,\"341\":8,\"343\":1,\"344\":1,\"355\":3,\"356\":1,\"359\":14,\"361\":4,\"362\":1,\"363\":1,\"364\":3,\"380\":29,\"382\":10,\"383\":1,\"384\":5,\"385\":11,\"386\":6,\"397\":1,\"401\":1,\"403\":2,\"408\":1,\"410\":1,\"412\":1,\"417\":2,\"418\":2,\"419\":5,\"420\":9,\"421\":3,\"424\":2,\"425\":23,\"426\":3,\"427\":3,\"428\":3,\"429\":2,\"430\":3,\"431\":6,\"440\":6,\"456\":2,\"463\":1,\"466\":1,\"468\":1,\"469\":1,\"472\":1,\"477\":9,\"479\":1,\"480\":2,\"482\":3,\"483\":2,\"484\":2,\"485\":15,\"486\":1,\"487\":1,\"490\":1,\"502\":14,\"506\":1,\"513\":5,\"514\":13,\"516\":8,\"518\":1,\"520\":1,\"540\":1,\"541\":2,\"542\":5,\"545\":1,\"546\":5,\"562\":1,\"563\":1,\"564\":3,\"565\":1,\"567\":2,\"569\":4,\"570\":6,\"572\":1,\"574\":4,\"582\":19,\"586\":2,\"587\":1,\"588\":6,\"589\":6,\"590\":4,\"592\":10,\"595\":4,\"596\":3,\"597\":5,\"611\":3,\"612\":10,\"633\":6,\"634\":1,\"641\":1,\"648\":1,\"656\":1,\"660\":7,\"663\":6,\"667\":1,\"668\":3,\"670\":2,\"678\":1,\"680\":1,\"684\":2,\"685\":2,\"692\":9,\"696\":2,\"697\":1,\"698\":2,\"699\":1,\"700\":3,\"701\":2,\"709\":19,\"710\":56,\"712\":1,\"713\":29,\"716\":1,\"718\":1,\"720\":1,\"721\":2,\"724\":2,\"728\":1,\"734\":3,\"735\":1,\"736\":1,\"739\":1,\"746\":1,\"749\":1,\"751\":3,\"757\":4,\"766\":1,\"781\":1,\"794\":4,\"800\":1,\"803\":1,\"804\":1,\"805\":3,\"807\":6,\"808\":3,\"809\":15,\"811\":18,\"815\":4,\"816\":43,\"823\":15,\"847\":2,\"850\":2,\"857\":2,\"866\":1,\"868\":1,\"871\":2,\"892\":33,\"893\":8,\"894\":1,\"895\":3,\"897\":2,\"898\":4,\"899\":13,\"900\":2,\"907\":2,\"909\":1,\"918\":16,\"921\":5,\"925\":1,\"926\":11,\"931\":5,\"932\":1,\"934\":1,\"935\":8,\"936\":1,\"937\":1,\"938\":1,\"939\":1,\"944\":2,\"945\":2,\"959\":1,\"960\":2,\"963\":7,\"964\":9}}],[\"05\",{\"1\":{\"47\":1,\"176\":1,\"224\":1,\"236\":1,\"293\":1,\"316\":1,\"343\":1,\"710\":1,\"850\":1}}],[\"覆盖了从\",{\"1\":{\"833\":1}}],[\"覆盖默认\",{\"1\":{\"380\":1}}],[\"覆盖默认温度参数\",{\"1\":{\"256\":1}}],[\"覆盖小于50\",{\"1\":{\"285\":1}}],[\"覆盖原图超过50\",{\"1\":{\"285\":1}}],[\"覆盖以下数据集\",{\"1\":{\"176\":1}}],[\"覆盖23类物体和17种功能\",{\"1\":{\"73\":1}}],[\"覆盖类别\",{\"1\":{\"41\":1}}],[\"覆盖多对多关联\",{\"1\":{\"30\":1}}],[\"覆盖\",{\"1\":{\"25\":1,\"823\":1}}],[\"对右侧进行对数展开\",{\"1\":{\"945\":1}}],[\"对抗\",{\"1\":{\"918\":1}}],[\"对抗性脆弱\",{\"1\":{\"658\":1}}],[\"对编码器和解码器残差块的输出激活乘以一个较小的常数\",{\"1\":{\"886\":1}}],[\"对p\",{\"1\":{\"881\":1}}],[\"对各种可能世界状态\",{\"1\":{\"878\":1}}],[\"对任意\",{\"1\":{\"917\":1}}],[\"对任意事件\",{\"1\":{\"848\":1}}],[\"对任意行\",{\"1\":{\"546\":1}}],[\"对三面骰子设\",{\"1\":{\"846\":1}}],[\"对大模型能力具有极大影响\",{\"1\":{\"836\":1}}],[\"对大模型进行微调\",{\"1\":{\"602\":1}}],[\"对大模型进行训练\",{\"1\":{\"602\":1}}],[\"对流式处理进行了深度优化\",{\"1\":{\"833\":1}}],[\"对检索到的信息进行处理和增强\",{\"1\":{\"829\":1}}],[\"对检索任务效果好\",{\"1\":{\"369\":1}}],[\"对语言有了更深刻的理解\",{\"1\":{\"822\":1}}],[\"对语言指令进行分词\",{\"1\":{\"64\":1}}],[\"对加一个与相对位置有关的标量偏置\",{\"1\":{\"710\":1}}],[\"对加噪声后的\",{\"1\":{\"257\":1}}],[\"对长距离做对数映射\",{\"1\":{\"710\":1}}],[\"对长距离用对数分桶\",{\"1\":{\"710\":1}}],[\"对长距离粗略处理\",{\"1\":{\"710\":1}}],[\"对短距离用线性分桶\",{\"1\":{\"710\":1}}],[\"对短距离敏感\",{\"1\":{\"710\":1}}],[\"对的偏置\",{\"1\":{\"710\":1}}],[\"对序列\",{\"1\":{\"710\":1}}],[\"对别人\",{\"1\":{\"709\":1}}],[\"对它们做向量点积\",{\"1\":{\"709\":1}}],[\"对外提供的编码和解码两个方法实现如下\",{\"1\":{\"697\":1}}],[\"对列表数据进行解析\",{\"1\":{\"697\":1}}],[\"对下游任务\",{\"1\":{\"679\":1}}],[\"对非二元代词\",{\"1\":{\"670\":1}}],[\"对非平滑函数的适应性\",{\"1\":{\"500\":1}}],[\"对荒谬命令未进行识别\",{\"1\":{\"657\":1}}],[\"对指令遵循度高\",{\"1\":{\"657\":1}}],[\"对最终模型进行评估\",{\"1\":{\"656\":1}}],[\"对最后一维做\",{\"1\":{\"380\":1}}],[\"对模型输出进行偏好排序\",{\"1\":{\"656\":1}}],[\"对模型已经分类正确的样本\",{\"1\":{\"589\":1}}],[\"对训练数据进行了\",{\"1\":{\"656\":1}}],[\"对性能影响有限\",{\"1\":{\"678\":1}}],[\"对性能影响\",{\"1\":{\"641\":1}}],[\"对同一个大模型的微调\",{\"1\":{\"602\":1}}],[\"对当前词的子词进行合并\",{\"1\":{\"596\":1}}],[\"对当前句子中每个词进行子词合并加词id映射\",{\"1\":{\"596\":1}}],[\"对经过预处理的vocab中的每个词按空格进行切分\",{\"1\":{\"595\":1}}],[\"对经过注意力层的输出进行归一化处理\",{\"1\":{\"429\":1}}],[\"对损失求均值\",{\"1\":{\"592\":1}}],[\"对假阴性\",{\"1\":{\"592\":1}}],[\"对假阳性\",{\"1\":{\"592\":1}}],[\"对类别不平衡问题鲁棒\",{\"1\":{\"592\":1}}],[\"对类别不平衡不敏感\",{\"1\":{\"586\":1,\"588\":1}}],[\"对噪声标签敏感\",{\"1\":{\"589\":1}}],[\"对噪声点敏感\",{\"1\":{\"157\":2}}],[\"对分类错误的样本\",{\"1\":{\"589\":1}}],[\"对细节更敏感\",{\"1\":{\"587\":1}}],[\"对细粒度的区域特征建模\",{\"1\":{\"268\":1}}],[\"对前景响应弱\",{\"1\":{\"587\":1}}],[\"对其他\",{\"1\":{\"582\":1}}],[\"对其进行加工\",{\"1\":{\"450\":1}}],[\"对两个张量\",{\"1\":{\"546\":1}}],[\"对两个特征进行线性投射\",{\"1\":{\"407\":1}}],[\"对一个张量\",{\"1\":{\"544\":1}}],[\"对热力图\",{\"1\":{\"505\":1}}],[\"对双线性或双三次插值是否对齐角点\",{\"1\":{\"503\":1}}],[\"对采样点的值进行聚合\",{\"1\":{\"502\":1}}],[\"对高维和非平滑函数更鲁棒\",{\"1\":{\"500\":1}}],[\"对不连续的\",{\"1\":{\"491\":1}}],[\"对不同方向的偏差做缩放和正交旋转\",{\"1\":{\"578\":1}}],[\"对不同教师模型具有良好的可扩展性\",{\"1\":{\"215\":1}}],[\"对不同阈值计算\",{\"1\":{\"106\":1}}],[\"对张量沿指定维度做\",{\"1\":{\"481\":1}}],[\"对二维数组沿不同轴拼接\",{\"1\":{\"441\":1}}],[\"对特征进行更深入的建模\",{\"1\":{\"434\":1}}],[\"对特征空间进行变换\",{\"1\":{\"154\":1}}],[\"对特征空间做变换\",{\"1\":{\"150\":1,\"153\":1}}],[\"对投影后的结果应用丢弃层\",{\"1\":{\"430\":1}}],[\"对输出做归一化\",{\"1\":{\"893\":1}}],[\"对输出有了更加明确具体的要求\",{\"1\":{\"618\":1}}],[\"对输出应用\",{\"1\":{\"590\":1}}],[\"对输出图像中的每个像素\",{\"1\":{\"505\":1}}],[\"对输出像素使用周围四个输入像素的加权平均\",{\"1\":{\"505\":1}}],[\"对输出进行维度交换和形状调整\",{\"1\":{\"430\":1}}],[\"对输入数据进行初步的特征提取\",{\"1\":{\"434\":1}}],[\"对输入进行归一化处理\",{\"1\":{\"429\":1}}],[\"对输入进行非线性变换\",{\"1\":{\"403\":1}}],[\"对输入图像进行预处理\",{\"1\":{\"582\":1}}],[\"对输入图像的数据分布非常敏感\",{\"1\":{\"264\":1}}],[\"对输入图像做通用增强\",{\"1\":{\"264\":1}}],[\"对输入的文本进行断句加分词\",{\"1\":{\"596\":1}}],[\"对输入的每个\",{\"1\":{\"264\":1}}],[\"对输入的点云做刚性变换\",{\"1\":{\"152\":1}}],[\"对输入点云做刚性变换\",{\"1\":{\"150\":1}}],[\"对输入点云中的每个点进行分类\",{\"1\":{\"146\":1}}],[\"对输入点进行采样\",{\"1\":{\"132\":1}}],[\"对注意力权重矩阵应用丢弃层\",{\"1\":{\"430\":1}}],[\"对注意力权重做\",{\"1\":{\"380\":1}}],[\"对注意力分数矩阵应用softmax函数\",{\"1\":{\"430\":1}}],[\"对处理后的张量进行归一化操作\",{\"1\":{\"426\":1}}],[\"对验证集的处理方式是先resize成256x256的图片\",{\"1\":{\"425\":1}}],[\"对数空间中操作更稳定\",{\"1\":{\"931\":1}}],[\"对数底的选择\",{\"1\":{\"906\":1}}],[\"对数似然展开为\",{\"1\":{\"904\":1}}],[\"对数拉普拉斯分布\",{\"1\":{\"886\":1}}],[\"对数据的处理和操作要求极低\",{\"1\":{\"830\":1}}],[\"对数据集和验证集划分之后\",{\"1\":{\"425\":1}}],[\"对数映射\",{\"1\":{\"710\":1}}],[\"对数映射可以把大范围的距离压缩到少量桶\",{\"1\":{\"710\":1}}],[\"对数分母\",{\"1\":{\"710\":1}}],[\"对数分桶\",{\"1\":{\"710\":1}}],[\"对数值变化敏感\",{\"1\":{\"160\":1}}],[\"对整张图像进行处理\",{\"1\":{\"388\":1}}],[\"对整个词表进行分类训练\",{\"1\":{\"373\":1}}],[\"对负样本依赖大\",{\"1\":{\"385\":1}}],[\"对深层\",{\"1\":{\"380\":1}}],[\"对上式取对数\",{\"1\":{\"932\":1}}],[\"对上一层输出\",{\"1\":{\"372\":1}}],[\"对上述得到的每个区域进行编码\",{\"1\":{\"132\":1}}],[\"对复杂视觉\",{\"1\":{\"368\":1}}],[\"对这个图片做随机裁剪\",{\"1\":{\"350\":1}}],[\"对微调模型\",{\"1\":{\"346\":1}}],[\"对话即平台\",{\"1\":{\"827\":1}}],[\"对话模型\",{\"1\":{\"823\":1,\"832\":1}}],[\"对话模式\",{\"1\":{\"303\":1}}],[\"对话系统中的候选回复选择\",{\"1\":{\"737\":1}}],[\"对话\",{\"1\":{\"656\":1}}],[\"对话型问答为多轮对话\",{\"1\":{\"342\":1}}],[\"对话型\",{\"1\":{\"342\":1}}],[\"对教师输出进行温度锐化\",{\"1\":{\"293\":1}}],[\"对参数进行分组\",{\"1\":{\"293\":1}}],[\"对参数选择依赖性高\",{\"1\":{\"135\":1}}],[\"对多模态理解任务更有利\",{\"1\":{\"278\":1}}],[\"对多个大规模噪声网页图文数据集\",{\"1\":{\"185\":1}}],[\"对视频中\",{\"1\":{\"273\":1}}],[\"对文本端的编码过程\",{\"1\":{\"274\":1}}],[\"对文本的归一化嵌入\",{\"1\":{\"271\":1}}],[\"对文本进行编码\",{\"1\":{\"206\":1}}],[\"对原始数据进行清洗和处理\",{\"1\":{\"829\":1}}],[\"对原始图像进行统一的图像增强\",{\"1\":{\"264\":1}}],[\"对原始点云做刚性变换\",{\"1\":{\"153\":1}}],[\"对单个样本\",{\"1\":{\"910\":1}}],[\"对单个点的分类精度不够敏感\",{\"1\":{\"587\":1}}],[\"对单个像素偏差较大的情况更加宽容\",{\"1\":{\"259\":1}}],[\"对单模态文本\",{\"1\":{\"223\":1}}],[\"对异常值特别敏感的问题\",{\"1\":{\"259\":1}}],[\"对异常点鲁棒性差\",{\"1\":{\"157\":1}}],[\"对异常点也有一定容忍能力\",{\"1\":{\"150\":1}}],[\"对称地反映了两人距离\",{\"1\":{\"915\":1}}],[\"对称改进版\",{\"1\":{\"912\":1}}],[\"对称的\",{\"1\":{\"574\":1}}],[\"对称的对比学习损失\",{\"1\":{\"407\":1}}],[\"对称\",{\"1\":{\"385\":1}}],[\"对称处理\",{\"1\":{\"263\":1}}],[\"对称结构\",{\"1\":{\"255\":1}}],[\"对称函数\",{\"0\":{\"160\":1},\"1\":{\"160\":2}}],[\"对某个\",{\"1\":{\"243\":1}}],[\"对重建性能和下游任务表现有直接影响\",{\"1\":{\"215\":1}}],[\"对效果的影响\",{\"1\":{\"215\":1}}],[\"对未使用的码本向量保持原值\",{\"1\":{\"213\":1}}],[\"对码本进行初始化\",{\"1\":{\"213\":1}}],[\"对被\",{\"1\":{\"208\":1}}],[\"对角协方差矩阵\",{\"1\":{\"871\":1}}],[\"对角线\",{\"1\":{\"574\":1}}],[\"对角线上的元素\",{\"1\":{\"574\":1}}],[\"对角线元素的labels\",{\"1\":{\"407\":1}}],[\"对角线为正例\",{\"1\":{\"192\":1}}],[\"对角为1\",{\"1\":{\"206\":1}}],[\"对角为正样本\",{\"1\":{\"206\":1}}],[\"对主编码器做\",{\"1\":{\"206\":1}}],[\"对少量异常点有一定鲁棒性\",{\"1\":{\"157\":1}}],[\"对稀疏点云敏感\",{\"1\":{\"157\":1}}],[\"对局部形状变化敏感\",{\"1\":{\"157\":1}}],[\"对局部点云组做最大池化或平均池化\",{\"1\":{\"143\":1}}],[\"对局部点云进行变换\",{\"1\":{\"143\":1}}],[\"对几何变换的不变性\",{\"1\":{\"149\":1,\"150\":1}}],[\"对有效邻居点\",{\"1\":{\"122\":1}}],[\"对权重进行归一化\",{\"1\":{\"122\":1}}],[\"对无序点云重新排序\",{\"1\":{\"110\":1}}],[\"对无效\",{\"1\":{\"100\":1}}],[\"对排列和数量不敏感\",{\"1\":{\"109\":1}}],[\"对所有历史\",{\"1\":{\"663\":1}}],[\"对所有图文对进行联合编码\",{\"1\":{\"369\":1}}],[\"对所有损失求平均\",{\"1\":{\"293\":1}}],[\"对所有\",{\"1\":{\"106\":1,\"949\":1}}],[\"对所有点\",{\"1\":{\"97\":1}}],[\"对边界敏感\",{\"1\":{\"106\":1}}],[\"对边界模糊区域友好\",{\"1\":{\"106\":1}}],[\"对边界模糊区域不敏感\",{\"1\":{\"106\":1}}],[\"对affordance\",{\"1\":{\"100\":1}}],[\"对每一个选项\",{\"1\":{\"737\":1}}],[\"对每一行做\",{\"1\":{\"527\":1}}],[\"对每种组合手工编写\",{\"1\":{\"87\":1}}],[\"对每个选项分别进行编码\",{\"1\":{\"737\":1}}],[\"对每个句对构建用于mlm任务的样本\",{\"1\":{\"698\":1}}],[\"对每个句对构建用于nsp任务的样本\",{\"1\":{\"698\":1}}],[\"对每个句子随机选择15\",{\"1\":{\"681\":1}}],[\"对每个句子进行分词\",{\"1\":{\"596\":1}}],[\"对每个句子独立进行编码\",{\"1\":{\"55\":1}}],[\"对每个词进行子词合并\",{\"1\":{\"596\":2}}],[\"对每个维度根据尺度差异进行惩罚调整\",{\"1\":{\"578\":1}}],[\"对每个小块区域做\",{\"1\":{\"501\":1}}],[\"对每个类别的\",{\"1\":{\"257\":1}}],[\"对每个簇\",{\"1\":{\"213\":1}}],[\"对每个样本计算它与所有簇中心的距离\",{\"1\":{\"213\":1}}],[\"对每个样本单独处理\",{\"1\":{\"67\":1}}],[\"对每个原始点\",{\"1\":{\"145\":1}}],[\"对每个尺度的局部点集应用对应的\",{\"1\":{\"141\":1}}],[\"对每个半径\",{\"1\":{\"141\":1}}],[\"对每个局部区域内所有点的最大响应值进行池化\",{\"1\":{\"137\":1}}],[\"对每个查询点的邻近点按索引排序\",{\"1\":{\"137\":1}}],[\"对每个目标点\",{\"1\":{\"122\":1}}],[\"对每个中心点的所有邻居点的特征在特征维度上进行分组\",{\"1\":{\"119\":1}}],[\"对每个点单独计算分类误差\",{\"1\":{\"587\":1}}],[\"对每个点独立处理\",{\"1\":{\"157\":1}}],[\"对每个点进行特征提取\",{\"1\":{\"154\":1}}],[\"对每个点取预测值和真实值中的较小者\",{\"1\":{\"106\":1}}],[\"对每个点的3个权重求和\",{\"1\":{\"145\":1}}],[\"对每个点的特征做一个简单的分类器\",{\"1\":{\"143\":1}}],[\"对每个点的响应值\",{\"1\":{\"100\":1}}],[\"对每个点的关注程度\",{\"1\":{\"100\":1}}],[\"对每个点的关注响应\",{\"1\":{\"100\":1}}],[\"对每个\",{\"1\":{\"97\":1,\"475\":1,\"501\":1,\"502\":1,\"710\":1,\"735\":1,\"823\":1,\"958\":1}}],[\"对象可被立即回收\",{\"1\":{\"806\":1}}],[\"对象方法工厂\",{\"1\":{\"460\":1}}],[\"对象方法装饰器\",{\"1\":{\"460\":1}}],[\"对象\",{\"1\":{\"123\":1}}],[\"对象功能区域分割\",{\"1\":{\"94\":1}}],[\"对象上\",{\"1\":{\"83\":1}}],[\"对象的交互区域\",{\"1\":{\"52\":1}}],[\"对交互主体框位置进行精细调整\",{\"1\":{\"83\":1}}],[\"对交互文本和几何结构文本进行编码这块\",{\"1\":{\"55\":1}}],[\"对交互文本和几何结构文本进行编码\",{\"1\":{\"54\":1}}],[\"对图文对中的文本\",{\"1\":{\"223\":1}}],[\"对图像中哪些\",{\"1\":{\"582\":1}}],[\"对图像和文本独立使用encoder\",{\"1\":{\"390\":1}}],[\"对图像和第\",{\"1\":{\"271\":1}}],[\"对图像和点云特征进行\",{\"1\":{\"65\":2}}],[\"对图像每个\",{\"1\":{\"260\":1}}],[\"对图像进行归一化处理\",{\"1\":{\"425\":2}}],[\"对图像进行离散\",{\"1\":{\"258\":1}}],[\"对图像进行编码\",{\"1\":{\"206\":1}}],[\"对图像\",{\"1\":{\"223\":1,\"370\":2}}],[\"对图片进行缩放\",{\"1\":{\"82\":1}}],[\"对\",{\"1\":{\"70\":1,\"78\":1,\"91\":1,\"100\":1,\"106\":1,\"117\":1,\"188\":1,\"208\":1,\"214\":1,\"256\":1,\"274\":1,\"293\":1,\"346\":1,\"379\":1,\"614\":1,\"640\":1,\"656\":1,\"657\":2,\"694\":1,\"710\":3,\"877\":2,\"900\":1,\"904\":2,\"944\":1}}],[\"对点特征全局平均池化\",{\"1\":{\"116\":1}}],[\"对点数维度做\",{\"1\":{\"70\":1}}],[\"对点积结果进行缩放\",{\"1\":{\"65\":1}}],[\"对点云的旋转\",{\"1\":{\"150\":1}}],[\"对点云进行下采样\",{\"1\":{\"143\":1}}],[\"对点云进行编码\",{\"1\":{\"54\":1}}],[\"对点云密度变换较为敏感\",{\"1\":{\"135\":1}}],[\"对点云数据做平移操作后\",{\"1\":{\"131\":1}}],[\"对点云数据进行转置操作\",{\"1\":{\"92\":1}}],[\"对点云数据进行归一化处理\",{\"1\":{\"92\":1}}],[\"对点云特征\",{\"1\":{\"38\":1}}],[\"对自然语言指令进行\",{\"1\":{\"64\":1}}],[\"对应到\",{\"1\":{\"932\":1}}],[\"对应于头两个实验的每一种实验结果\",{\"1\":{\"880\":1}}],[\"对应于第一个实验的每一种实验结果\",{\"1\":{\"880\":1}}],[\"对应代码如下\",{\"1\":{\"712\":1}}],[\"对应每个\",{\"1\":{\"710\":1}}],[\"对应着标签\",{\"1\":{\"630\":1}}],[\"对应所有\",{\"1\":{\"526\":1}}],[\"对应位置为\",{\"1\":{\"476\":2}}],[\"对应维度为\",{\"1\":{\"427\":1}}],[\"对应分布式环境\",{\"1\":{\"385\":1}}],[\"对应相同数量的视觉\",{\"1\":{\"236\":1}}],[\"对应\",{\"1\":{\"214\":1,\"215\":1,\"293\":1,\"426\":1,\"924\":3}}],[\"对应图像中的一个\",{\"1\":{\"212\":1}}],[\"对应论文图1中的视觉transformer\",{\"1\":{\"187\":1}}],[\"对应论文3\",{\"1\":{\"187\":2}}],[\"对应一个离散\",{\"1\":{\"963\":1}}],[\"对应一个形状为\",{\"1\":{\"545\":1}}],[\"对应一个\",{\"1\":{\"152\":1}}],[\"对应一组相关的点特征\",{\"1\":{\"96\":1}}],[\"对应半径下最多取多少邻近点\",{\"1\":{\"141\":1}}],[\"对应层\",{\"1\":{\"122\":1}}],[\"对应指标\",{\"1\":{\"106\":1}}],[\"对应起来\",{\"1\":{\"83\":1}}],[\"对应的id为\",{\"1\":{\"713\":1}}],[\"对应的参数为\",{\"1\":{\"712\":1}}],[\"对应的相对位置编码向量\",{\"1\":{\"709\":1}}],[\"对应的向量\",{\"1\":{\"709\":2}}],[\"对应的位置编码\",{\"1\":{\"706\":1}}],[\"对应的输出概率最大\",{\"1\":{\"694\":1}}],[\"对应的内存布局为一维数组\",{\"1\":{\"544\":1}}],[\"对应的像素值分别为\",{\"1\":{\"505\":1}}],[\"对应的伪代码实现如下所示\",{\"1\":{\"407\":1}}],[\"对应的还有\",{\"1\":{\"213\":1}}],[\"对应的蒸馏损失定义为\",{\"1\":{\"202\":1}}],[\"对应的\",{\"1\":{\"100\":1,\"187\":1}}],[\"对应的注意力掩码\",{\"1\":{\"67\":1}}],[\"对应的几何属性\",{\"1\":{\"52\":1}}],[\"对应多少个\",{\"1\":{\"53\":1}}],[\"对比不同输入格式\",{\"1\":{\"678\":1,\"679\":1}}],[\"对比静态掩码\",{\"1\":{\"678\":1}}],[\"对比原始架构\",{\"1\":{\"667\":1}}],[\"对比之前的方法\",{\"1\":{\"625\":1}}],[\"对比其他损失函数\",{\"1\":{\"592\":1}}],[\"对比维度\",{\"1\":{\"346\":1}}],[\"对比\",{\"1\":{\"312\":1,\"313\":1,\"735\":1}}],[\"对比任务\",{\"1\":{\"304\":1}}],[\"对比模式\",{\"1\":{\"303\":2}}],[\"对比空间\",{\"1\":{\"293\":1}}],[\"对比描述预训练\",{\"0\":{\"272\":1}}],[\"对比度变化\",{\"1\":{\"293\":1}}],[\"对比度\",{\"1\":{\"264\":1}}],[\"对比优势\",{\"1\":{\"263\":1}}],[\"对比学习的过程就是想要在特征空间里\",{\"1\":{\"353\":1}}],[\"对比学习的优势\",{\"1\":{\"268\":1}}],[\"对比学习从2019年开始到现在一直都比较火\",{\"1\":{\"348\":1}}],[\"对比学习通过拉近相似图像\",{\"1\":{\"246\":1}}],[\"对比学习和自蒸馏等方法已被探索\",{\"1\":{\"228\":1}}],[\"对比学习\",{\"0\":{\"246\":1},\"1\":{\"220\":1,\"268\":1}}],[\"对比学习损失\",{\"1\":{\"192\":1}}],[\"对比学习中的队列长度\",{\"1\":{\"205\":1}}],[\"对比学习中的动量蒸馏\",{\"1\":{\"202\":1}}],[\"对比学习中的负样本缓存\",{\"1\":{\"192\":1}}],[\"对比学习中图文特征队列长度\",{\"1\":{\"192\":1}}],[\"对比学习温度参数\",{\"1\":{\"190\":1}}],[\"对比损失只使用一个图像嵌入\",{\"1\":{\"272\":1}}],[\"对比损失\",{\"1\":{\"190\":1,\"268\":1,\"272\":2,\"274\":1}}],[\"对比方法\",{\"1\":{\"46\":1}}],[\"对比见表1\",{\"1\":{\"30\":1}}],[\"对于编码器的每个输出向量\",{\"1\":{\"958\":1}}],[\"对于连续型数据\",{\"1\":{\"951\":1}}],[\"对于连续型随机变量\",{\"1\":{\"869\":1}}],[\"对于大多数\",{\"1\":{\"945\":1}}],[\"对于单通道灰度图像\",{\"1\":{\"924\":1}}],[\"对于单个的物体还好\",{\"1\":{\"131\":1}}],[\"对于标签\",{\"1\":{\"910\":1}}],[\"对于参数\",{\"1\":{\"903\":1}}],[\"对于文本token\",{\"1\":{\"892\":1}}],[\"对于高斯分布\",{\"1\":{\"932\":1}}],[\"对于高分辨率图像将需要大量的内存\",{\"1\":{\"885\":1}}],[\"对于高纬向量部分而言\",{\"1\":{\"706\":1}}],[\"对于任意两个满足\",{\"1\":{\"851\":1}}],[\"对于任意两个事件\",{\"1\":{\"848\":1}}],[\"对于任意一列两两互不相交\",{\"1\":{\"848\":1}}],[\"对于事件\",{\"1\":{\"847\":1}}],[\"对于个体开发者或小型开发团队来说\",{\"1\":{\"836\":1}}],[\"对于个体开发者或小型开发团队而言\",{\"1\":{\"836\":1}}],[\"对于个人使用者而言\",{\"1\":{\"601\":1}}],[\"对于形状复杂的函数\",{\"1\":{\"816\":1}}],[\"对于不满足交换律的运算符\",{\"1\":{\"809\":1}}],[\"对于共享变量\",{\"1\":{\"804\":1}}],[\"对于函数\",{\"1\":{\"779\":2}}],[\"对于y=f2\",{\"1\":{\"775\":1}}],[\"对于复合函数\",{\"1\":{\"774\":1}}],[\"对于这样的多选问题\",{\"1\":{\"737\":1}}],[\"对于这些任务\",{\"1\":{\"631\":1}}],[\"对于分类任务来说\",{\"1\":{\"722\":1}}],[\"对于字典中不存在的词\",{\"1\":{\"713\":1}}],[\"对于mlm任务损失计算来说\",{\"1\":{\"700\":2}}],[\"对于所有掩码候选位置执行掩码策略\",{\"1\":{\"697\":1}}],[\"对于生成类任务\",{\"1\":{\"647\":1}}],[\"对于没有训练集的数据集\",{\"1\":{\"647\":1}}],[\"对于dprd\",{\"1\":{\"635\":1}}],[\"对于race\",{\"1\":{\"635\":1}}],[\"对于sst\",{\"1\":{\"635\":1}}],[\"对于cola\",{\"1\":{\"635\":1}}],[\"对于相似任务\",{\"1\":{\"631\":1}}],[\"对于作者的模型架构\",{\"1\":{\"626\":1}}],[\"对于将这些学习到的表征迁移到目标任务的最有效方法\",{\"1\":{\"626\":1}}],[\"对于llm越友好\",{\"1\":{\"618\":1}}],[\"对于需要微调的密集层\",{\"1\":{\"609\":1}}],[\"对于二元分类器\",{\"1\":{\"570\":1}}],[\"对于二维的图像\",{\"1\":{\"426\":1}}],[\"对于类别不平衡的数据集\",{\"1\":{\"567\":1,\"593\":1}}],[\"对于模型效果\",{\"1\":{\"566\":1}}],[\"对于疾病预测等应用\",{\"1\":{\"563\":1}}],[\"对于严重不均衡的数据集\",{\"1\":{\"562\":1}}],[\"对于更高维的张量\",{\"1\":{\"545\":1}}],[\"对于上述代码中的矩阵\",{\"1\":{\"541\":2}}],[\"对于要插值的每个输出像素点\",{\"1\":{\"504\":1}}],[\"对于具有线性输出层和至少一个使用\",{\"1\":{\"500\":1}}],[\"对于q\",{\"1\":{\"417\":1}}],[\"对于自监督模型\",{\"1\":{\"413\":1}}],[\"对于有监督模型\",{\"1\":{\"413\":1}}],[\"对于vit\",{\"1\":{\"407\":1}}],[\"对于一些复杂的问题\",{\"1\":{\"616\":1}}],[\"对于一般的任务\",{\"1\":{\"613\":1}}],[\"对于一个二元变量\",{\"1\":{\"932\":1}}],[\"对于一个二维矩阵\",{\"1\":{\"545\":1}}],[\"对于一个\",{\"1\":{\"923\":1}}],[\"对于一个连续型随机变量\",{\"1\":{\"853\":1}}],[\"对于一个具有\",{\"1\":{\"852\":1}}],[\"对于一个长度为\",{\"1\":{\"707\":1}}],[\"对于一个基于正弦余弦编码的位置向量\",{\"1\":{\"706\":1}}],[\"对于一个样本\",{\"1\":{\"589\":1}}],[\"对于一个形状为\",{\"1\":{\"489\":1}}],[\"对于一个包含个文本\",{\"1\":{\"407\":1}}],[\"对于一张图像\",{\"1\":{\"232\":1}}],[\"对于输入的图像\",{\"1\":{\"380\":1}}],[\"对于输入图片\",{\"1\":{\"286\":1}}],[\"对于输入图像\",{\"1\":{\"285\":1}}],[\"对于nlp领域来说\",{\"1\":{\"353\":1}}],[\"对于imagenet这个数据集来说\",{\"1\":{\"350\":1}}],[\"对于频率大于\",{\"1\":{\"341\":1}}],[\"对于同一个指令\",{\"1\":{\"339\":1}}],[\"对于学生模型来说\",{\"1\":{\"293\":1}}],[\"对于零样本视频\",{\"1\":{\"273\":1}}],[\"对于冻结特征评估或微调场景\",{\"1\":{\"273\":1}}],[\"对于预测值\",{\"1\":{\"259\":1}}],[\"对于语义分割任务\",{\"1\":{\"239\":1}}],[\"对于第\",{\"1\":{\"236\":1}}],[\"对于下游任务\",{\"1\":{\"229\":1}}],[\"对于\",{\"1\":{\"214\":1,\"357\":2,\"612\":1,\"921\":1,\"924\":2,\"926\":2}}],[\"对于非刚性变形\",{\"1\":{\"157\":1}}],[\"对于多选题\",{\"1\":{\"647\":1}}],[\"对于多标签\",{\"1\":{\"143\":1}}],[\"对于多分类\",{\"1\":{\"143\":1}}],[\"对于某个形心\",{\"1\":{\"135\":1}}],[\"对于查询点集合中的每个点\",{\"1\":{\"119\":1}}],[\"对于每个token来说\",{\"1\":{\"892\":1}}],[\"对于每个位置\",{\"1\":{\"706\":1}}],[\"对于每个遮挡块\",{\"1\":{\"234\":1}}],[\"对于每个图像和文本\",{\"1\":{\"199\":1}}],[\"对于每个局部区域\",{\"1\":{\"141\":1}}],[\"对于每个质心点\",{\"1\":{\"140\":1}}],[\"对于每个选中的关键点\",{\"1\":{\"137\":1}}],[\"对于每个查询点\",{\"1\":{\"119\":1}}],[\"对于每个问题\",{\"1\":{\"88\":1}}],[\"对于每一个子业务训练优化模型\",{\"1\":{\"835\":1}}],[\"对于每一个子业务构造训练数据与验证数据\",{\"1\":{\"835\":1}}],[\"对于每一个输出位置\",{\"1\":{\"700\":2}}],[\"对于每一个被遮挡的位置\",{\"1\":{\"234\":1}}],[\"对于每一个\",{\"1\":{\"100\":1}}],[\"对于点云中的每一个点\",{\"1\":{\"65\":1}}],[\"对于点云实例\",{\"1\":{\"42\":1}}],[\"对于图像数据\",{\"1\":{\"921\":1}}],[\"对于图像token\",{\"1\":{\"892\":1}}],[\"对于图像分类任务\",{\"1\":{\"238\":1}}],[\"对于图像中的每一个位置\",{\"1\":{\"65\":1}}],[\"对于图像\",{\"1\":{\"42\":1}}],[\"对齐本身可被滥用\",{\"1\":{\"658\":1}}],[\"对齐方法以处理价值多样性\",{\"1\":{\"658\":1}}],[\"对齐语言模型所需的计算成本极低\",{\"1\":{\"658\":1}}],[\"对齐程度\",{\"1\":{\"656\":1}}],[\"对齐损失\",{\"1\":{\"655\":1}}],[\"对齐问题\",{\"1\":{\"654\":1}}],[\"对齐视觉和语言内容\",{\"1\":{\"372\":1}}],[\"对齐图文特征空间\",{\"1\":{\"194\":1}}],[\"对齐做相加\",{\"1\":{\"119\":1}}],[\"对齐的特征\",{\"1\":{\"119\":1}}],[\"对齐后的特征\",{\"1\":{\"83\":1}}],[\"对齐得到的图像\",{\"1\":{\"83\":1}}],[\"对齐\",{\"1\":{\"83\":1,\"152\":1,\"658\":1}}],[\"对齐特征\",{\"1\":{\"83\":1}}],[\"对齐特征并使用\",{\"1\":{\"23\":1}}],[\"对齐区域\",{\"1\":{\"73\":1}}],[\"对齐模糊性\",{\"1\":{\"73\":1}}],[\"对齐二者\",{\"1\":{\"37\":1}}],[\"对机器人感知与操作至关重要\",{\"1\":{\"30\":1}}],[\"对对象\",{\"1\":{\"25\":1}}],[\"个向量每个向量的距离\",{\"1\":{\"958\":1}}],[\"个向量作为初始中心\",{\"1\":{\"213\":2}}],[\"个后的生成样本效果\",{\"1\":{\"935\":1}}],[\"个灰度级来描述图像\",{\"1\":{\"925\":1}}],[\"个离散的概率分布\",{\"1\":{\"925\":1}}],[\"个子像素\",{\"1\":{\"924\":1}}],[\"个经过\",{\"1\":{\"921\":1}}],[\"个像素点\",{\"1\":{\"932\":1}}],[\"个像素之后的信息\",{\"1\":{\"921\":1}}],[\"个像素及后续像素的信息\",{\"1\":{\"921\":1}}],[\"个像素只能看到前\",{\"1\":{\"921\":1}}],[\"个像素取某种颜色的概率的数组\",{\"1\":{\"921\":1}}],[\"个像素\",{\"1\":{\"921\":2}}],[\"个像素的信息\",{\"1\":{\"921\":1}}],[\"个像素的真值和预测的概率分布求交叉熵损失函数\",{\"1\":{\"921\":1}}],[\"个像素的概率分布\",{\"1\":{\"921\":2}}],[\"个像素输出第\",{\"1\":{\"921\":1}}],[\"个数\",{\"1\":{\"899\":1}}],[\"个数字\",{\"1\":{\"542\":1}}],[\"个码本向量的均匀分类分布\",{\"1\":{\"886\":1}}],[\"个字母进行排列\",{\"1\":{\"881\":1}}],[\"个也彼此相同\",{\"1\":{\"881\":1}}],[\"个彼此相同\",{\"1\":{\"881\":1}}],[\"个不同的元素中取\",{\"1\":{\"882\":1}}],[\"个不同的元素\",{\"1\":{\"881\":1}}],[\"个不同形状\",{\"1\":{\"25\":1}}],[\"个实验一共有\",{\"1\":{\"880\":1}}],[\"个实验\",{\"1\":{\"880\":1}}],[\"个自由参数\",{\"1\":{\"871\":2}}],[\"个是蓝球\",{\"1\":{\"859\":1}}],[\"个是红球\",{\"1\":{\"859\":1}}],[\"个球中蓝球的数量\",{\"1\":{\"859\":1}}],[\"个球\",{\"1\":{\"859\":2}}],[\"个的组合数\",{\"1\":{\"856\":1}}],[\"个性化大模型应用需要有个性化数据库进行支撑\",{\"1\":{\"836\":1}}],[\"个集成\",{\"1\":{\"833\":1}}],[\"个核心组件组成\",{\"1\":{\"832\":1}}],[\"个月的最快记录\",{\"1\":{\"823\":1}}],[\"个步骤\",{\"1\":{\"799\":1,\"814\":1,\"819\":1}}],[\"个选项\",{\"1\":{\"737\":1}}],[\"个选项中选择正确答案\",{\"1\":{\"680\":1}}],[\"个位置向量第\",{\"1\":{\"706\":1}}],[\"个位置\",{\"1\":{\"706\":1}}],[\"个词之间\",{\"1\":{\"735\":1}}],[\"个词\",{\"1\":{\"694\":1}}],[\"个词到第\",{\"1\":{\"694\":1}}],[\"个句子相接\",{\"1\":{\"690\":1}}],[\"个句子在原始本文中是否跟第\",{\"1\":{\"690\":1}}],[\"个英语语料库\",{\"1\":{\"680\":1}}],[\"个响应\",{\"1\":{\"656\":1}}],[\"个序列表示\",{\"1\":{\"631\":1}}],[\"个示例\",{\"1\":{\"564\":1,\"565\":1}}],[\"个通道\",{\"1\":{\"542\":1}}],[\"个角的中间位置\",{\"1\":{\"502\":1}}],[\"个神经元才能达到相同效果\",{\"1\":{\"500\":1}}],[\"个线性区域的函数\",{\"1\":{\"500\":1}}],[\"个分段\",{\"1\":{\"500\":1}}],[\"个分布尽可能均匀的采样点索引\",{\"1\":{\"137\":1}}],[\"个人对上述内容的理解\",{\"1\":{\"500\":1}}],[\"个人理解是因为\",{\"1\":{\"694\":1}}],[\"个人理解是为了确保维度对齐\",{\"1\":{\"119\":1}}],[\"个人理解\",{\"0\":{\"26\":1}}],[\"个值\",{\"1\":{\"488\":2}}],[\"个值及其索引\",{\"1\":{\"488\":1}}],[\"个元素彼此相同\",{\"1\":{\"881\":1}}],[\"个元素的向量\",{\"1\":{\"877\":1}}],[\"个元素中选出\",{\"1\":{\"856\":1}}],[\"个元素及其频数\",{\"1\":{\"516\":1}}],[\"个元素\",{\"1\":{\"463\":1,\"489\":2,\"542\":6,\"546\":1,\"881\":1}}],[\"个元素作为输入图像\",{\"1\":{\"385\":1}}],[\"个等间隔数\",{\"1\":{\"440\":1}}],[\"个类别和\",{\"1\":{\"435\":1}}],[\"个类别得分\",{\"1\":{\"123\":1}}],[\"个用新的编码器做一个编码\",{\"1\":{\"357\":1}}],[\"个下游任务\",{\"1\":{\"352\":1}}],[\"个多模态基准测试中表现优异\",{\"1\":{\"322\":1}}],[\"个裁剪\",{\"1\":{\"293\":1}}],[\"个局部\",{\"1\":{\"293\":2}}],[\"个局部裁剪\",{\"1\":{\"293\":1}}],[\"个全局\",{\"1\":{\"293\":2}}],[\"个全局裁剪\",{\"1\":{\"293\":3}}],[\"个来自不同视角下的批次图像\",{\"1\":{\"293\":1}}],[\"个文本对应第\",{\"1\":{\"274\":1}}],[\"个文本与第\",{\"1\":{\"274\":1}}],[\"个可能的取值\",{\"1\":{\"877\":1}}],[\"个可能取值的离散随机变量\",{\"1\":{\"852\":1}}],[\"个可能配对中识别正确配对\",{\"1\":{\"373\":1}}],[\"个可训练参数\",{\"1\":{\"432\":2}}],[\"个可学习查询\",{\"1\":{\"272\":1}}],[\"个可供性类别\",{\"1\":{\"25\":1}}],[\"个视觉\",{\"1\":{\"234\":1}}],[\"个视角的\",{\"1\":{\"22\":1}}],[\"个图像的相似度\",{\"1\":{\"274\":1}}],[\"个图像\",{\"1\":{\"233\":1,\"234\":1,\"238\":1,\"272\":1,\"274\":1,\"373\":1,\"885\":1,\"895\":1}}],[\"个图文对\",{\"1\":{\"207\":1}}],[\"个图文表示\",{\"1\":{\"199\":1}}],[\"个样本进行重排序的策略\",{\"1\":{\"889\":1}}],[\"个样本特征\",{\"1\":{\"513\":1}}],[\"个样本对应第\",{\"1\":{\"385\":1}}],[\"个样本\",{\"1\":{\"224\":1,\"356\":1,\"514\":3}}],[\"个公开数据集\",{\"1\":{\"224\":1}}],[\"个注意力头\",{\"1\":{\"224\":1,\"633\":1}}],[\"个簇中心来作为码本的初始化向量\",{\"1\":{\"213\":1}}],[\"个为负样本\",{\"1\":{\"207\":1}}],[\"个为正样本\",{\"1\":{\"207\":1}}],[\"个负样本和一个正样本\",{\"1\":{\"355\":1}}],[\"个负样本\",{\"1\":{\"207\":2,\"373\":1}}],[\"个正样本\",{\"1\":{\"207\":2,\"385\":1}}],[\"个历史负样本\",{\"1\":{\"206\":1}}],[\"个输出\",{\"1\":{\"152\":1}}],[\"个坐标值\",{\"1\":{\"152\":1}}],[\"个邻近点的特征\",{\"1\":{\"145\":1}}],[\"个邻近点\",{\"1\":{\"145\":1}}],[\"个邻居的索引\",{\"1\":{\"122\":1}}],[\"个邻居点\",{\"1\":{\"119\":1}}],[\"个关键点作为局部区域中心\",{\"1\":{\"141\":1}}],[\"个关键点对应的全局区域特征向量\",{\"1\":{\"138\":1}}],[\"个关键点对应的局部区域特征向量\",{\"1\":{\"138\":2}}],[\"个关键点的坐标\",{\"1\":{\"138\":3}}],[\"个维度\",{\"1\":{\"137\":1}}],[\"个具有代表性的点\",{\"1\":{\"137\":1}}],[\"个最近邻点索引\",{\"1\":{\"145\":1}}],[\"个最近邻\",{\"1\":{\"122\":2,\"286\":1}}],[\"个百分点\",{\"1\":{\"117\":1}}],[\"个阶段\",{\"1\":{\"116\":1}}],[\"个定制化问题\",{\"1\":{\"91\":1}}],[\"个专家设计的问题\",{\"1\":{\"87\":1,\"89\":1}}],[\"个问题\",{\"1\":{\"87\":1}}],[\"个代表性问题\",{\"1\":{\"87\":1}}],[\"个同类别点云\",{\"1\":{\"53\":1}}],[\"个点的值按这个比例加起来\",{\"1\":{\"502\":1}}],[\"个点的子集决定\",{\"1\":{\"157\":1}}],[\"个点云\",{\"1\":{\"89\":1,\"93\":1}}],[\"个点\",{\"1\":{\"42\":1,\"91\":1,\"119\":1,\"137\":2,\"145\":3}}],[\"个\",{\"1\":{\"34\":1,\"206\":2,\"212\":2,\"231\":1,\"234\":1,\"236\":4,\"262\":2,\"263\":2,\"274\":1,\"286\":3,\"306\":1,\"322\":1,\"371\":1,\"489\":1,\"656\":1,\"882\":1,\"885\":1,\"887\":3,\"893\":1,\"895\":3,\"896\":1,\"898\":1,\"935\":1}}],[\"个对象类别\",{\"1\":{\"25\":1}}],[\"个严重度\",{\"1\":{\"25\":1}}],[\"5️⃣\",{\"1\":{\"963\":1}}],[\"5平均分刷新sota\",{\"1\":{\"685\":1}}],[\"5点\",{\"1\":{\"641\":1}}],[\"5b模型仍欠拟合\",{\"1\":{\"641\":1}}],[\"5b模型\",{\"1\":{\"641\":1}}],[\"5b参数模型在多数任务上逼近或超越监督基线\",{\"1\":{\"641\":1}}],[\"5b参数\",{\"1\":{\"641\":1}}],[\"5b\",{\"1\":{\"640\":2,\"823\":2}}],[\"5是五分类\",{\"1\":{\"634\":1}}],[\"5的情感极性区分的更细致\",{\"1\":{\"634\":1}}],[\"5的概率将文本对应的图片替换成不同的图片\",{\"1\":{\"393\":1}}],[\"5的开发基于internvit\",{\"1\":{\"330\":1}}],[\"5作为开源多模态大语言模型\",{\"1\":{\"337\":1}}],[\"5v\",{\"1\":{\"325\":1}}],[\"5版本模型具备了强大的鲁棒性\",{\"1\":{\"330\":1}}],[\"5版本\",{\"1\":{\"325\":1}}],[\"5在18个多模态基准测试中表现优异\",{\"1\":{\"323\":1}}],[\"595\",{\"1\":{\"341\":1}}],[\"59\",{\"1\":{\"310\":1,\"823\":1}}],[\"5500\",{\"1\":{\"713\":2}}],[\"55\",{\"1\":{\"308\":1,\"574\":1}}],[\"558k\",{\"1\":{\"317\":1}}],[\"558\",{\"1\":{\"99\":1}}],[\"5×5\",{\"1\":{\"288\":1}}],[\"5e\",{\"1\":{\"236\":1,\"316\":1,\"633\":1}}],[\"5376\",{\"1\":{\"811\":1}}],[\"53\",{\"1\":{\"107\":1,\"332\":1,\"342\":1,\"343\":1}}],[\"504\",{\"1\":{\"701\":1}}],[\"50k词汇表\",{\"1\":{\"681\":1}}],[\"50000\",{\"1\":{\"816\":1}}],[\"500k步时\",{\"1\":{\"684\":1}}],[\"500个点\",{\"1\":{\"121\":1,\"122\":1}}],[\"500\",{\"1\":{\"107\":3}}],[\"50\",{\"1\":{\"107\":1,\"110\":1,\"150\":1,\"223\":1,\"236\":2,\"280\":1,\"289\":1,\"293\":2,\"305\":1,\"315\":1,\"440\":1,\"570\":2,\"641\":1,\"697\":2,\"698\":3,\"823\":1,\"896\":1}}],[\"51\",{\"1\":{\"877\":1}}],[\"516\",{\"1\":{\"89\":1}}],[\"512k\",{\"1\":{\"823\":1}}],[\"512\",{\"1\":{\"54\":6,\"58\":11,\"59\":19,\"60\":11,\"65\":1,\"70\":6,\"83\":34,\"123\":1,\"138\":6,\"141\":5,\"144\":4,\"146\":1,\"152\":3,\"155\":3,\"156\":3,\"213\":1,\"255\":2,\"262\":2,\"317\":1,\"633\":1,\"679\":1,\"680\":1,\"710\":1,\"899\":2,\"900\":4,\"918\":4,\"964\":1}}],[\"58\",{\"1\":{\"87\":2,\"89\":1,\"91\":1,\"308\":1,\"342\":1}}],[\"5400\",{\"1\":{\"822\":1}}],[\"540b的53\",{\"1\":{\"668\":1}}],[\"540b\",{\"1\":{\"668\":4,\"822\":1}}],[\"540b等顶尖模型表现相当\",{\"1\":{\"665\":1}}],[\"54\",{\"1\":{\"46\":1}}],[\"5250\",{\"1\":{\"701\":1}}],[\"5200\",{\"1\":{\"224\":1}}],[\"52\",{\"1\":{\"46\":1,\"668\":1}}],[\"56\",{\"1\":{\"41\":1,\"44\":1,\"46\":1}}],[\"57\",{\"1\":{\"41\":1,\"668\":1}}],[\"5\",{\"0\":{\"26\":1,\"46\":1,\"47\":1,\"48\":1,\"49\":1,\"90\":1,\"321\":1,\"401\":1,\"429\":1,\"916\":1,\"934\":1,\"938\":1},\"1\":{\"25\":1,\"34\":1,\"41\":1,\"50\":1,\"52\":1,\"54\":1,\"59\":1,\"60\":1,\"64\":1,\"65\":1,\"69\":1,\"82\":1,\"83\":13,\"87\":1,\"100\":1,\"102\":3,\"104\":1,\"105\":1,\"106\":2,\"107\":2,\"116\":1,\"117\":3,\"119\":5,\"141\":1,\"143\":1,\"146\":1,\"176\":1,\"190\":6,\"192\":1,\"204\":2,\"205\":1,\"206\":1,\"208\":3,\"213\":3,\"224\":1,\"242\":1,\"264\":2,\"265\":1,\"268\":1,\"274\":1,\"288\":1,\"291\":1,\"293\":4,\"303\":1,\"308\":1,\"310\":2,\"311\":1,\"315\":1,\"316\":1,\"317\":1,\"322\":2,\"323\":1,\"327\":1,\"329\":1,\"330\":1,\"332\":1,\"334\":2,\"344\":1,\"362\":1,\"380\":4,\"381\":1,\"384\":2,\"385\":3,\"408\":1,\"409\":1,\"413\":1,\"420\":1,\"421\":1,\"424\":1,\"425\":19,\"430\":1,\"435\":1,\"440\":3,\"441\":3,\"444\":1,\"452\":1,\"463\":3,\"472\":3,\"477\":5,\"481\":2,\"482\":3,\"483\":1,\"485\":4,\"486\":1,\"500\":1,\"502\":4,\"514\":1,\"540\":2,\"541\":2,\"542\":3,\"544\":5,\"545\":7,\"547\":1,\"570\":2,\"572\":1,\"589\":3,\"590\":4,\"592\":4,\"595\":1,\"625\":1,\"626\":2,\"633\":2,\"634\":1,\"640\":1,\"641\":5,\"646\":1,\"647\":1,\"648\":3,\"649\":1,\"650\":1,\"656\":1,\"660\":1,\"663\":2,\"666\":2,\"667\":4,\"668\":4,\"669\":2,\"670\":1,\"678\":1,\"680\":2,\"699\":2,\"706\":1,\"709\":2,\"710\":7,\"712\":1,\"716\":1,\"766\":1,\"808\":1,\"823\":18,\"847\":1,\"850\":1,\"871\":1,\"888\":1,\"892\":1,\"895\":1,\"896\":2,\"898\":1,\"899\":3,\"918\":3,\"926\":6,\"931\":3,\"932\":1,\"937\":1,\"945\":1,\"946\":2,\"947\":1,\"948\":1,\"950\":2,\"951\":1,\"956\":1}}],[\"js散度就大\",{\"1\":{\"915\":1}}],[\"js散度小\",{\"1\":{\"915\":1}}],[\"js散度定义为\",{\"1\":{\"914\":1}}],[\"js散度的数学定义\",{\"0\":{\"914\":1}}],[\"js散度解决了这些问题\",{\"1\":{\"913\":1}}],[\"js散度是衡量两个概率分布差异的一个方法\",{\"1\":{\"912\":1}}],[\"js散度是什么\",{\"0\":{\"912\":1}}],[\"js散度\",{\"0\":{\"911\":1}}],[\"json\",{\"1\":{\"187\":2,\"190\":1,\"191\":1,\"192\":1,\"205\":1,\"424\":6,\"595\":6,\"597\":13,\"696\":4,\"697\":4,\"700\":1,\"712\":1}}],[\"jensen\",{\"0\":{\"911\":1},\"1\":{\"885\":2}}],[\"jet\",{\"1\":{\"816\":1}}],[\"jx\",{\"1\":{\"435\":1}}],[\"javascript\",{\"1\":{\"833\":1}}],[\"javaer\",{\"1\":{\"2\":1}}],[\"jarvis\",{\"1\":{\"827\":1}}],[\"jaques\",{\"1\":{\"655\":1}}],[\"jaccard\",{\"0\":{\"588\":1},\"1\":{\"588\":7}}],[\"jax\",{\"1\":{\"435\":1}}],[\"jupyter\",{\"1\":{\"815\":1}}],[\"judge\",{\"1\":{\"343\":1}}],[\"just\",{\"1\":{\"52\":1,\"83\":1}}],[\"jpeg\",{\"1\":{\"410\":1,\"412\":1}}],[\"jp\",{\"1\":{\"309\":1}}],[\"jpg\",{\"1\":{\"52\":1,\"293\":1,\"410\":1,\"412\":1,\"424\":2}}],[\"jft\",{\"1\":{\"269\":1,\"271\":1,\"298\":1,\"413\":2,\"888\":1}}],[\"jigsaw\",{\"1\":{\"248\":1}}],[\"jit=false\",{\"1\":{\"213\":1}}],[\"jit\",{\"1\":{\"213\":1}}],[\"jitter\",{\"1\":{\"25\":1,\"293\":4}}],[\"join\",{\"1\":{\"92\":3,\"107\":2,\"187\":1,\"410\":3,\"411\":1,\"412\":4,\"424\":3,\"582\":1,\"595\":5,\"596\":2,\"597\":6,\"697\":2,\"815\":2}}],[\"joint\",{\"0\":{\"869\":1},\"1\":{\"60\":2,\"65\":4,\"69\":2,\"83\":10,\"898\":1}}],[\"jra模块\",{\"1\":{\"78\":1,\"80\":1}}],[\"jra\",{\"1\":{\"72\":1,\"83\":6}}],[\"j\",{\"1\":{\"59\":5,\"83\":16,\"121\":1,\"141\":3,\"263\":3,\"274\":6,\"410\":2,\"412\":2,\"487\":7,\"514\":1,\"542\":2,\"546\":1,\"582\":3,\"700\":5,\"708\":2,\"709\":7,\"710\":14,\"900\":2,\"926\":3,\"964\":3}}],[\"训练pixelcnn\",{\"1\":{\"956\":1}}],[\"训练vq\",{\"1\":{\"956\":1}}],[\"训练好的解码器\",{\"1\":{\"947\":1}}],[\"训练样本\",{\"1\":{\"943\":1}}],[\"训练速度快\",{\"1\":{\"942\":1}}],[\"训练速度快了n倍\",{\"1\":{\"921\":1}}],[\"训练这类模型长期以来面临三大难题\",{\"1\":{\"942\":1}}],[\"训练生成器\",{\"1\":{\"918\":1}}],[\"训练判别器\",{\"1\":{\"918\":1}}],[\"训练优化trick不进行讲解\",{\"1\":{\"893\":1}}],[\"训练优化策略\",{\"1\":{\"667\":1}}],[\"训练细节见附录\",{\"1\":{\"887\":1}}],[\"训练的核心目标函数\",{\"1\":{\"932\":1}}],[\"训练的模型\",{\"1\":{\"823\":1}}],[\"训练的时候固定\",{\"1\":{\"611\":1}}],[\"训练能够准确预测下一个单词的\",{\"1\":{\"823\":1}}],[\"训练步骤分析\",{\"0\":{\"681\":1}}],[\"训练硬件与效率\",{\"1\":{\"680\":1}}],[\"训练动态\",{\"1\":{\"668\":1}}],[\"训练任务覆盖广泛\",{\"1\":{\"656\":1}}],[\"训练任务包括图文匹配\",{\"1\":{\"369\":1}}],[\"训练语言模型以遵循自然语言指令\",{\"1\":{\"655\":1}}],[\"训练语料所在的文件列表\",{\"1\":{\"595\":1}}],[\"训练语料为\",{\"1\":{\"595\":1}}],[\"训练依赖微软提供的高带宽gpu集群\",{\"1\":{\"647\":1}}],[\"训练成本非常高\",{\"1\":{\"601\":1}}],[\"训练与评估流程的代码为模版代码\",{\"1\":{\"435\":1}}],[\"训练与测试灵活性\",{\"1\":{\"331\":1}}],[\"训练了10个epoch\",{\"1\":{\"435\":1}}],[\"训练代价\",{\"1\":{\"415\":1}}],[\"训练代码如下所示\",{\"1\":{\"359\":1}}],[\"训练代码\",{\"1\":{\"204\":1}}],[\"训练效率成为一个至关重要的因素\",{\"1\":{\"413\":1}}],[\"训练效率可以提高4倍\",{\"1\":{\"413\":1}}],[\"训练使用到的数据集和alexnet保持一致\",{\"1\":{\"410\":1}}],[\"训练出a和b即可得到∆w\",{\"1\":{\"606\":1}}],[\"训练出具有可迁移能力的视觉模型\",{\"1\":{\"405\":1}}],[\"训练出一个能预测人类偏好的奖励模型\",{\"1\":{\"654\":1}}],[\"训练出一个能自然理解图像内容\",{\"1\":{\"342\":1}}],[\"训练出一个能看懂图的视觉分词器\",{\"1\":{\"341\":1}}],[\"训练不稳定\",{\"1\":{\"385\":1}}],[\"训练更稳定但区分度下降\",{\"1\":{\"355\":1}}],[\"训练策略如下\",{\"1\":{\"656\":1}}],[\"训练策略遵循了\",{\"1\":{\"647\":1}}],[\"训练策略\",{\"1\":{\"344\":1}}],[\"训练流程\",{\"1\":{\"341\":1,\"342\":1}}],[\"训练奖励模型\",{\"1\":{\"339\":1,\"653\":1}}],[\"训练监督模型\",{\"1\":{\"339\":1}}],[\"训练数据中的敏感信息需要妥善处理\",{\"1\":{\"830\":1}}],[\"训练数据中的例子\",{\"1\":{\"339\":1}}],[\"训练数据主要为英文\",{\"1\":{\"658\":1}}],[\"训练数据和奖励信号均来自一组英语标注者\",{\"1\":{\"658\":1}}],[\"训练数据集\",{\"1\":{\"640\":1}}],[\"训练数据的标签数组\",{\"1\":{\"514\":1}}],[\"训练数据加载器\",{\"1\":{\"265\":1}}],[\"训练图像的分辨率从固定的448×448扩展为动态的448×448\",{\"1\":{\"330\":1}}],[\"训练初期冻结学生网络最后一层\",{\"1\":{\"293\":1}}],[\"训练初期模型尚不稳定\",{\"1\":{\"204\":1}}],[\"训练循环\",{\"1\":{\"293\":3,\"926\":1}}],[\"训练50个epoch能达到35\",{\"1\":{\"292\":1}}],[\"训练线性分类器\",{\"1\":{\"286\":1}}],[\"训练方式\",{\"1\":{\"269\":1,\"346\":1}}],[\"训练一个自回归\",{\"1\":{\"885\":1}}],[\"训练一个少量参数的小模型\",{\"1\":{\"604\":1}}],[\"训练一个轻量级的微调模型\",{\"1\":{\"601\":1}}],[\"训练一个奖励模型\",{\"1\":{\"339\":1}}],[\"训练一个\",{\"1\":{\"252\":1}}],[\"训练得到的图像\",{\"1\":{\"232\":1}}],[\"训练视觉\",{\"1\":{\"215\":1}}],[\"训练模式下\",{\"1\":{\"900\":1}}],[\"训练模式下更新\",{\"1\":{\"213\":1}}],[\"训练模型\",{\"0\":{\"934\":1},\"1\":{\"963\":1}}],[\"训练模型去区分它们\",{\"1\":{\"282\":1}}],[\"训练模型以自回归方式生成文本\",{\"1\":{\"172\":1}}],[\"训练模型根据图像生成文本描述\",{\"1\":{\"172\":1}}],[\"训练目标的引导\",{\"1\":{\"427\":1}}],[\"训练目标\",{\"1\":{\"341\":1,\"342\":1,\"346\":1,\"420\":1}}],[\"训练目标公式为\",{\"1\":{\"212\":1}}],[\"训练目标最大化\",{\"1\":{\"212\":1}}],[\"训练中\",{\"1\":{\"212\":1,\"935\":1}}],[\"训练完成后\",{\"1\":{\"210\":1,\"271\":1,\"947\":1,\"956\":1,\"961\":1}}],[\"训练过程就比较常规了\",{\"1\":{\"700\":1}}],[\"训练过程与资源分配\",{\"1\":{\"647\":1}}],[\"训练过程中的一些\",{\"1\":{\"656\":1}}],[\"训练过程中对\",{\"1\":{\"232\":1}}],[\"训练过程中\",{\"1\":{\"212\":1,\"427\":1}}],[\"训练过程\",{\"0\":{\"938\":1},\"1\":{\"210\":1,\"265\":1}}],[\"训练过程代码实现基本遵循moco论文中所提出的动量慢更新对比学习代码实现\",{\"1\":{\"190\":1}}],[\"训练稳定\",{\"1\":{\"204\":1}}],[\"训练函数\",{\"1\":{\"187\":1}}],[\"训练多个模型\",{\"1\":{\"183\":1}}],[\"训练时提取\",{\"1\":{\"899\":1}}],[\"训练时设为\",{\"1\":{\"899\":1}}],[\"训练时必须提供图像\",{\"1\":{\"893\":1}}],[\"训练时末尾\",{\"1\":{\"893\":1}}],[\"训练时将所有配对作为一个\",{\"1\":{\"656\":1}}],[\"训练时的对比缩放因子\",{\"1\":{\"385\":1}}],[\"训练时可能是\",{\"1\":{\"384\":1}}],[\"训练时最多12区块\",{\"1\":{\"331\":1}}],[\"训练时根据输入图像的宽高比和分辨率\",{\"1\":{\"329\":1}}],[\"训练时间的性价比\",{\"1\":{\"291\":1}}],[\"训练时间会更长\",{\"1\":{\"181\":1}}],[\"训练时\",{\"1\":{\"264\":1,\"268\":1,\"663\":1,\"921\":1}}],[\"训练时用\",{\"1\":{\"257\":1}}],[\"训练时会对每个图像随机采样\",{\"1\":{\"53\":1}}],[\"训练轮数为\",{\"1\":{\"176\":1}}],[\"训练配置如下\",{\"1\":{\"176\":1}}],[\"训练集与测试集性能对比\",{\"1\":{\"641\":1}}],[\"训练集与测试集中的物体与可供性类别相同\",{\"1\":{\"44\":1}}],[\"训练集的预处理转换操作\",{\"1\":{\"425\":1}}],[\"训练集通常打乱\",{\"1\":{\"382\":1}}],[\"训练集\",{\"1\":{\"92\":1}}],[\"训练阶段使用随机裁剪缩放和水平翻转\",{\"1\":{\"286\":1}}],[\"训练阶段唯一需要注意的一点就是数据集的构造过程中\",{\"1\":{\"187\":1}}],[\"训练阶段则是模型的核心迭代过程\",{\"1\":{\"105\":1}}],[\"训练阶段\",{\"0\":{\"963\":1},\"1\":{\"90\":1,\"334\":1,\"381\":1,\"900\":1,\"947\":1}}],[\"训练和无条件生成的代码实现如下所示\",{\"1\":{\"926\":1}}],[\"训练和使用阶段都要一致\",{\"1\":{\"264\":1}}],[\"训练和测试阶段共享相似的物体类别和功能类型的分布\",{\"1\":{\"89\":1}}],[\"训练和部署需要大量计算资源\",{\"1\":{\"27\":1}}],[\"训练\",{\"0\":{\"103\":1,\"105\":1,\"407\":1,\"700\":1},\"1\":{\"24\":1,\"92\":1,\"103\":1,\"224\":1,\"258\":1,\"293\":1,\"298\":1,\"305\":1,\"317\":1,\"318\":1,\"319\":1,\"374\":1,\"382\":1,\"385\":1,\"633\":1,\"656\":2,\"658\":3,\"679\":1,\"885\":1,\"892\":1,\"918\":2,\"964\":2}}],[\"训练分两阶段\",{\"1\":{\"24\":1}}],[\"冻结预训练模型参数\",{\"1\":{\"609\":1}}],[\"冻结参数的image\",{\"1\":{\"417\":1}}],[\"冻结注意力模块和视觉专家\",{\"1\":{\"374\":1}}],[\"冻结主干网络\",{\"1\":{\"316\":1}}],[\"冻结特征\",{\"1\":{\"286\":1}}],[\"冻结特征评估\",{\"1\":{\"273\":1}}],[\"冻结教师\",{\"1\":{\"285\":1}}],[\"冻结教师模型参数\",{\"1\":{\"213\":1}}],[\"冻结编码器\",{\"1\":{\"268\":1}}],[\"冻结\",{\"1\":{\"24\":1,\"305\":1,\"892\":1}}],[\"作用是\",{\"1\":{\"454\":1}}],[\"作用域\",{\"0\":{\"444\":1}}],[\"作用于\",{\"1\":{\"380\":1}}],[\"作用于注意力分支\",{\"1\":{\"380\":1}}],[\"作用于注意力之前\",{\"1\":{\"380\":1}}],[\"作用于点云特征图\",{\"1\":{\"100\":1}}],[\"作用\",{\"1\":{\"264\":3,\"293\":1,\"380\":1,\"402\":1,\"480\":1,\"481\":1,\"482\":1,\"486\":1,\"490\":1,\"491\":1,\"494\":1,\"520\":1,\"521\":1,\"522\":1,\"589\":1,\"924\":1}}],[\"作用在多模态解码器的输出上\",{\"1\":{\"268\":1}}],[\"作用在\",{\"1\":{\"119\":1}}],[\"作者再次使用了停止梯度算子\",{\"1\":{\"961\":1}}],[\"作者分别讨论了上面公式里的两个误差\",{\"1\":{\"960\":1}}],[\"作者发现\",{\"1\":{\"960\":1}}],[\"作者并不清楚是否已经有任何人从一般意义上证明了这一点\",{\"1\":{\"949\":1}}],[\"作者引入了\",{\"1\":{\"892\":1}}],[\"作者强调\",{\"1\":{\"658\":1,\"687\":1}}],[\"作者列出多个值得进一步研究的问题\",{\"1\":{\"658\":1}}],[\"作者建议未来采用更多元标注\",{\"1\":{\"658\":1}}],[\"作者清晰指出当前模型对齐行为的\",{\"1\":{\"658\":1}}],[\"作者采用了\",{\"1\":{\"654\":1}}],[\"作者称这些过程使得模型输出更符合人类偏好\",{\"1\":{\"654\":1}}],[\"作者借鉴了\",{\"1\":{\"650\":1}}],[\"作者首先展示了8个不同规模的模型在训练过程中的表现\",{\"1\":{\"648\":1}}],[\"作者首先定义了语言模型执行任务的四种方式\",{\"1\":{\"647\":1}}],[\"作者对common\",{\"1\":{\"647\":1}}],[\"作者对比了不同的\",{\"1\":{\"291\":1}}],[\"作者训练了从125m到175b参数的8个模型\",{\"1\":{\"647\":1}}],[\"作者训练流程有两个阶段\",{\"1\":{\"628\":1}}],[\"作者开放了模型代码和小型预训练模型\",{\"1\":{\"643\":1}}],[\"作者同时指出gpt\",{\"1\":{\"642\":1}}],[\"作者归因于该数据集的句子级打乱破坏了长程依赖\",{\"1\":{\"641\":1}}],[\"作者特别指出\",{\"1\":{\"641\":1}}],[\"作者指出\",{\"1\":{\"639\":1,\"640\":1,\"641\":1,\"649\":1,\"651\":1,\"654\":1,\"949\":1}}],[\"作者的工作表明\",{\"1\":{\"636\":1}}],[\"作者模型获得了重要的世界知识和处理长距离依赖的能力\",{\"1\":{\"636\":1}}],[\"作者介绍了一种框架\",{\"1\":{\"636\":1}}],[\"作者观察标准结果\",{\"1\":{\"635\":1}}],[\"作者修改输入序列来包含\",{\"1\":{\"631\":1}}],[\"作者额外要微调的参数只有\",{\"1\":{\"630\":1}}],[\"作者通过实验验证这一假设\",{\"1\":{\"639\":1}}],[\"作者通过fps来抽样点集中较为重要的点\",{\"1\":{\"134\":1}}],[\"作者通用的任务未知task\",{\"1\":{\"626\":1}}],[\"作者利用源于遍历式\",{\"1\":{\"626\":1}}],[\"作者假设采用一个大型无标记文本语料库和几个人工标记训练样本的数据集\",{\"1\":{\"626\":1}}],[\"作者用以下优化\",{\"1\":{\"630\":1}}],[\"作者用无监督的预训练和监督的微调组合来探索关于语言理解任务半监督方法\",{\"1\":{\"626\":1}}],[\"作者用交叉熵公式来解释\",{\"1\":{\"290\":1}}],[\"作者证明通过在丰富的无标签文本语料库生成预训练generative\",{\"1\":{\"625\":1}}],[\"作者又探索了一种混合模型\",{\"1\":{\"434\":1}}],[\"作者先是在imagenet21k上进行预训练\",{\"1\":{\"431\":1}}],[\"作者随后也对一维位置编码的结果进行了可视化\",{\"1\":{\"428\":1}}],[\"作者最终选择了对比学习方法来进行训练\",{\"1\":{\"413\":1}}],[\"作者提出一种通过人类反馈对模型进行微调的方法\",{\"1\":{\"653\":1}}],[\"作者提出\",{\"1\":{\"639\":1}}],[\"作者提出的vilt可以认为是目前最简单的多模态transformer方法\",{\"1\":{\"392\":1}}],[\"作者提出的vilt属于\",{\"1\":{\"390\":1}}],[\"作者提出这4种类型的主要依据有两点\",{\"1\":{\"390\":1}}],[\"作者提出了\",{\"1\":{\"280\":1}}],[\"作者提出了一个新的数据处理流程\",{\"1\":{\"173\":1}}],[\"作者提出了一个多任务模型架构\",{\"1\":{\"171\":1}}],[\"作者提出了一个统一的视觉语言预训练框架\",{\"1\":{\"170\":1}}],[\"作者提出了新的\",{\"1\":{\"125\":1}}],[\"作者说到\",{\"1\":{\"357\":1}}],[\"作者认为\",{\"1\":{\"639\":1,\"649\":1,\"949\":1}}],[\"作者认为大规模数据没有被充分利用\",{\"1\":{\"354\":1}}],[\"作者认为这是因为cv领域和nlp领域的原始信号空间不同\",{\"1\":{\"353\":1}}],[\"作者认为编码和解码之间的主要差异体现在\",{\"1\":{\"171\":1}}],[\"作者还发现加入语言模型作为辅助目标来微调有助于学习\",{\"1\":{\"630\":1}}],[\"作者还提出了一种创新方法\",{\"1\":{\"343\":1}}],[\"作者还构建了一个包含图像\",{\"1\":{\"72\":1}}],[\"作者使用遍历式方法\",{\"1\":{\"631\":1}}],[\"作者使用了一种巧妙的停止梯度算子\",{\"1\":{\"961\":1}}],[\"作者使用了谷歌制作的jft\",{\"1\":{\"432\":1}}],[\"作者使用了四个核心评估指标来衡量模型对语言引导下功能区域的识别能力\",{\"1\":{\"106\":1}}],[\"作者使用的是大规模图文对数据集\",{\"1\":{\"341\":1}}],[\"作者确定了在性能和效率之间取得平衡的最佳配置\",{\"1\":{\"304\":1}}],[\"作者将vit和之前图像分类领域比较强的resnet模型进行了对比测试\",{\"1\":{\"432\":1}}],[\"作者将模型训练分为两个阶段\",{\"1\":{\"340\":1}}],[\"作者将动量教师解释为一种\",{\"1\":{\"289\":1}}],[\"作者将通过\",{\"1\":{\"173\":1}}],[\"作者推测英语语言模型的强大概率补偿了翻译知识的不足\",{\"1\":{\"641\":1}}],[\"作者推测\",{\"1\":{\"178\":1}}],[\"作者在微调时用辅助的lm目标来检查作者模型的性能\",{\"1\":{\"635\":1}}],[\"作者在微调阶段使用任务感知的输入转换来实现有效的迁移\",{\"1\":{\"625\":1}}],[\"作者在下面部分和可视化插图\",{\"1\":{\"631\":1}}],[\"作者在监督学习目标任务上调整参数\",{\"1\":{\"630\":1}}],[\"作者在四种类型的语言理解任务评估作者的方法\",{\"1\":{\"626\":1}}],[\"作者在常识推理\",{\"1\":{\"625\":1}}],[\"作者在不同模型深度\",{\"1\":{\"311\":1}}],[\"作者在模型中采用了\",{\"1\":{\"171\":1}}],[\"作者在提出的\",{\"1\":{\"45\":1}}],[\"作者进行了多组消融实验\",{\"1\":{\"117\":1}}],[\"作者设计了\",{\"1\":{\"95\":1}}],[\"作者设计了一个名为iag\",{\"1\":{\"72\":1}}],[\"作者设计了多个分析实验\",{\"1\":{\"49\":1}}],[\"作者构建了两个新的数据集\",{\"1\":{\"19\":1}}],[\"作为解码器的输入\",{\"1\":{\"958\":2}}],[\"作为近似指标\",{\"1\":{\"947\":1}}],[\"作为对\",{\"1\":{\"946\":1}}],[\"作为对称函数\",{\"1\":{\"150\":1}}],[\"作为图文匹配的判别器\",{\"1\":{\"900\":1}}],[\"作为图像的分类预测结果\",{\"1\":{\"408\":1}}],[\"作为图像编码器\",{\"1\":{\"171\":1}}],[\"作为下一个\",{\"1\":{\"896\":1}}],[\"作为送入\",{\"1\":{\"893\":1}}],[\"作为自回归预测的开始标志\",{\"1\":{\"893\":1}}],[\"作为统一的序列输入transformer进行编码\",{\"1\":{\"892\":1}}],[\"作为离散符号\",{\"1\":{\"885\":1}}],[\"作为单一的数据流进行建模\",{\"1\":{\"885\":1}}],[\"作为划分事件\",{\"1\":{\"850\":1}}],[\"作为大模型开发的初学者\",{\"1\":{\"835\":1}}],[\"作为一个统一的序列进行训练\",{\"1\":{\"891\":1}}],[\"作为一个不断进化的创新平台\",{\"1\":{\"833\":1}}],[\"作为一个大语言模型开发框架\",{\"1\":{\"832\":1}}],[\"作为一个指标就没有那么有意义和实用\",{\"1\":{\"564\":1}}],[\"作为节点唯一id\",{\"1\":{\"815\":1}}],[\"作为答案开始的可能性\",{\"1\":{\"733\":1}}],[\"作为整个输入序列的全局信息聚合表示\",{\"1\":{\"699\":1}}],[\"作为整个点云的\",{\"1\":{\"152\":1,\"154\":1}}],[\"作为最终的\",{\"1\":{\"694\":1}}],[\"作为奖励函数\",{\"1\":{\"656\":1}}],[\"作为激活函数\",{\"1\":{\"633\":1}}],[\"作为分隔符\",{\"1\":{\"618\":1}}],[\"作为分词时的合并规则和优先选择权\",{\"1\":{\"595\":1}}],[\"作为平衡数据集的模型训练进度\",{\"1\":{\"566\":1}}],[\"作为特征提取器\",{\"1\":{\"434\":1}}],[\"作为特征提取器的方式差不多\",{\"1\":{\"352\":1}}],[\"作为文本解码器的初始状态\",{\"1\":{\"420\":1}}],[\"作为文本的整体\",{\"1\":{\"274\":1}}],[\"作为模型的缓冲区注册\",{\"1\":{\"474\":1}}],[\"作为模型初始化\",{\"1\":{\"376\":1}}],[\"作为模型输入\",{\"1\":{\"293\":1}}],[\"作为初始化\",{\"1\":{\"376\":1}}],[\"作为初始输入\",{\"1\":{\"100\":1}}],[\"作为融合编码器使用\",{\"1\":{\"375\":1}}],[\"作为融合编码器的\",{\"1\":{\"368\":1}}],[\"作为双编码器比融合编码器更快\",{\"1\":{\"368\":1}}],[\"作为编码器\",{\"1\":{\"362\":1,\"434\":1}}],[\"作为视觉与llms之间的\",{\"1\":{\"303\":1}}],[\"作为教师\",{\"1\":{\"289\":2}}],[\"作为输出预测目标\",{\"1\":{\"274\":1}}],[\"作为输入\",{\"1\":{\"274\":1,\"385\":1}}],[\"作为预测目标\",{\"1\":{\"269\":1}}],[\"作为训练标签\",{\"1\":{\"265\":1}}],[\"作为标签\",{\"1\":{\"265\":1}}],[\"作为重建损失\",{\"1\":{\"255\":1}}],[\"作为重建目标\",{\"1\":{\"223\":1}}],[\"作为主干网络\",{\"1\":{\"233\":1}}],[\"作为骨干模型来对不同模态进行编码\",{\"1\":{\"222\":1}}],[\"作为通用建模框架\",{\"1\":{\"220\":1}}],[\"作为监督信号进行\",{\"1\":{\"210\":1}}],[\"作为结束标记\",{\"1\":{\"188\":1}}],[\"作为第二个\",{\"1\":{\"69\":1}}],[\"作为第一个\",{\"1\":{\"69\":1}}],[\"作为\",{\"0\":{\"754\":1},\"1\":{\"69\":1,\"188\":2,\"207\":1,\"208\":2,\"216\":1,\"239\":1,\"264\":1,\"274\":1,\"311\":2,\"357\":1,\"377\":2,\"417\":1}}],[\"作为稳定的语义参考\",{\"1\":{\"26\":1}}],[\"作为动态卷积核预测可供性分数\",{\"1\":{\"24\":1}}],[\"kru13\",{\"1\":{\"867\":1}}],[\"kolmogorov\",{\"1\":{\"848\":1}}],[\"korthikanti\",{\"1\":{\"667\":1}}],[\"k^t\",{\"1\":{\"710\":1}}],[\"katherine\",{\"1\":{\"894\":1}}],[\"kaplan\",{\"1\":{\"671\":1}}],[\"karma\",{\"1\":{\"640\":1}}],[\"karpathy\",{\"1\":{\"187\":1,\"382\":4}}],[\"kv\",{\"0\":{\"659\":1,\"660\":1},\"1\":{\"660\":1,\"663\":11,\"823\":1,\"893\":2,\"895\":2}}],[\"k通常为10\",{\"1\":{\"647\":1}}],[\"k采样\",{\"1\":{\"641\":1}}],[\"kg\",{\"1\":{\"574\":1}}],[\"k矩阵乘积\",{\"1\":{\"433\":1}}],[\"kmeans\",{\"1\":{\"213\":15}}],[\"kd\",{\"1\":{\"210\":3,\"212\":4,\"213\":1,\"215\":5,\"216\":1,\"217\":2}}],[\"kullback\",{\"1\":{\"202\":1,\"260\":1,\"945\":1}}],[\"kinetics400\",{\"1\":{\"268\":1}}],[\"kingma\",{\"1\":{\"946\":1}}],[\"king\",{\"1\":{\"128\":1}}],[\"kimi\",{\"1\":{\"822\":1}}],[\"kim\",{\"1\":{\"171\":1}}],[\"k×k\",{\"1\":{\"153\":1}}],[\"k=int\",{\"1\":{\"424\":1}}],[\"k=images\",{\"1\":{\"359\":1}}],[\"k=2\",{\"1\":{\"155\":1,\"641\":1}}],[\"k=64\",{\"1\":{\"154\":1}}],[\"k=13\",{\"1\":{\"123\":1}}],[\"k=3\",{\"1\":{\"122\":2}}],[\"kpconv\",{\"1\":{\"110\":1}}],[\"kcnet\",{\"1\":{\"110\":1}}],[\"kwargs=cache\",{\"1\":{\"663\":1}}],[\"kwargs说明\",{\"1\":{\"663\":1}}],[\"kwargs\",{\"1\":{\"92\":1,\"97\":1,\"188\":3,\"213\":8,\"266\":1,\"380\":1,\"382\":4,\"421\":2,\"449\":2,\"452\":3,\"453\":2,\"454\":4,\"455\":2,\"459\":2,\"461\":7,\"511\":2,\"663\":2,\"894\":4,\"926\":2,\"964\":2}}],[\"k\",{\"0\":{\"525\":1,\"896\":1},\"1\":{\"83\":1,\"96\":3,\"98\":3,\"100\":2,\"117\":1,\"119\":18,\"122\":9,\"123\":5,\"141\":3,\"150\":1,\"155\":1,\"156\":11,\"157\":5,\"206\":4,\"208\":1,\"213\":16,\"215\":1,\"280\":3,\"286\":1,\"289\":1,\"359\":3,\"361\":10,\"362\":13,\"363\":4,\"364\":3,\"380\":8,\"382\":1,\"383\":4,\"386\":2,\"417\":1,\"424\":2,\"430\":7,\"475\":2,\"488\":5,\"523\":1,\"529\":1,\"538\":2,\"542\":1,\"582\":2,\"656\":2,\"661\":1,\"663\":2,\"697\":2,\"703\":4,\"709\":26,\"710\":5,\"724\":2,\"751\":9,\"857\":1,\"889\":1,\"895\":3,\"896\":11,\"898\":4,\"917\":3}}],[\"kld\",{\"1\":{\"932\":2}}],[\"kl损失的权重\",{\"1\":{\"899\":1}}],[\"kl散度有时会无穷大\",{\"1\":{\"913\":1}}],[\"kl散度不对称\",{\"1\":{\"913\":1}}],[\"kl散度的\",{\"1\":{\"912\":1}}],[\"kl散度\",{\"0\":{\"905\":1,\"932\":1},\"1\":{\"909\":1,\"915\":1}}],[\"kl散度损失\",{\"1\":{\"899\":1}}],[\"kl散度计算\",{\"0\":{\"260\":1}}],[\"kl散度约束\",{\"1\":{\"79\":1}}],[\"kl\",{\"1\":{\"83\":2,\"202\":1,\"206\":1,\"208\":2,\"235\":2,\"255\":5,\"256\":7,\"260\":3,\"261\":4,\"262\":1,\"655\":1,\"656\":2,\"885\":4,\"886\":1,\"899\":11,\"909\":2,\"931\":1,\"932\":1,\"935\":1,\"945\":5,\"946\":1,\"947\":2,\"949\":1,\"950\":2,\"951\":1,\"959\":2}}],[\"knn查询寻找最近的k个邻居\",{\"1\":{\"135\":1}}],[\"knn查询的前向传播函数\",{\"1\":{\"119\":1}}],[\"knnquery\",{\"1\":{\"119\":3,\"122\":1}}],[\"knn\",{\"1\":{\"110\":1,\"116\":1,\"119\":12,\"122\":1,\"125\":1,\"135\":1}}],[\"knife\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"knowledge\",{\"0\":{\"37\":1,\"168\":1},\"1\":{\"11\":2,\"14\":1,\"52\":2,\"56\":2,\"59\":1,\"195\":1,\"213\":1,\"283\":1,\"285\":1}}],[\"keep\",{\"1\":{\"736\":1}}],[\"keepdims=true\",{\"1\":{\"410\":1,\"412\":1}}],[\"keepdim=true\",{\"1\":{\"122\":2,\"145\":1,\"152\":1,\"154\":1,\"190\":1,\"213\":3,\"293\":1,\"385\":4,\"408\":2,\"582\":1,\"660\":1,\"663\":1,\"963\":1}}],[\"kernel=7\",{\"1\":{\"926\":2}}],[\"kernel\",{\"1\":{\"100\":1,\"152\":1,\"255\":4,\"266\":1,\"380\":2,\"426\":2,\"899\":5,\"926\":21,\"964\":1}}],[\"kernels\",{\"1\":{\"94\":1,\"100\":2}}],[\"kettle\",{\"1\":{\"48\":1,\"52\":12,\"53\":1,\"55\":6}}],[\"keyword\",{\"1\":{\"445\":1}}],[\"key来自query\",{\"1\":{\"420\":1}}],[\"key=pairs\",{\"1\":{\"595\":1,\"596\":1,\"597\":1}}],[\"key=value\",{\"1\":{\"445\":1}}],[\"key=self\",{\"1\":{\"100\":1}}],[\"key=gt\",{\"1\":{\"99\":1}}],[\"key=x\",{\"1\":{\"99\":1}}],[\"key=lambda\",{\"1\":{\"92\":1,\"595\":3,\"597\":2,\"805\":1,\"807\":1,\"815\":1}}],[\"key和value为融合后的文本特征\",{\"1\":{\"98\":1}}],[\"key和value都是点云特征\",{\"1\":{\"96\":1}}],[\"keys\",{\"1\":{\"53\":1,\"82\":1,\"361\":1,\"362\":2,\"364\":7,\"382\":17,\"595\":1,\"597\":1,\"663\":2}}],[\"keyboard\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"key\",{\"1\":{\"24\":1,\"38\":1,\"56\":8,\"69\":19,\"87\":1,\"92\":4,\"94\":1,\"96\":8,\"98\":11,\"99\":1,\"100\":7,\"119\":1,\"190\":1,\"207\":6,\"243\":1,\"272\":1,\"274\":1,\"353\":1,\"357\":1,\"359\":2,\"361\":2,\"362\":6,\"363\":3,\"364\":2,\"382\":7,\"399\":1,\"401\":5,\"402\":2,\"419\":3,\"420\":44,\"424\":2,\"523\":1,\"524\":2,\"526\":1,\"529\":1,\"531\":1,\"533\":1,\"663\":43,\"710\":16,\"724\":7,\"751\":7,\"823\":2}}],[\"层控制像素生成顺序\",{\"1\":{\"926\":1}}],[\"层自注意力层中的任一层都可以访问所有文本\",{\"1\":{\"887\":1}}],[\"层共享权重\",{\"1\":{\"699\":1}}],[\"层与\",{\"1\":{\"699\":1}}],[\"层有自注意力头\",{\"1\":{\"633\":1}}],[\"层也有偏置\",{\"1\":{\"522\":1}}],[\"层也能取得良好效果\",{\"1\":{\"305\":1}}],[\"层relu网络可生成\",{\"1\":{\"500\":1}}],[\"层网络的输出是多次复合的结果\",{\"1\":{\"500\":1}}],[\"层交错堆叠\",{\"1\":{\"434\":1}}],[\"层和\",{\"1\":{\"434\":1}}],[\"层和后\",{\"1\":{\"197\":1}}],[\"层处理后的输出\",{\"1\":{\"429\":1}}],[\"层类型\",{\"1\":{\"380\":1}}],[\"层数的迁移学习影响\",{\"1\":{\"635\":1}}],[\"层数\",{\"1\":{\"380\":1,\"500\":1,\"892\":1,\"900\":2}}],[\"层中\",{\"1\":{\"376\":1}}],[\"层中的视觉语言专家时\",{\"1\":{\"376\":1}}],[\"层或\",{\"1\":{\"311\":1}}],[\"层明显优于传统\",{\"1\":{\"311\":1}}],[\"层连接\",{\"1\":{\"305\":1}}],[\"层面借用\",{\"1\":{\"250\":1}}],[\"层到第\",{\"1\":{\"214\":1}}],[\"层原始损失\",{\"1\":{\"214\":1}}],[\"层的作用\",{\"1\":{\"733\":1}}],[\"层的偏置是冗余的\",{\"1\":{\"522\":1}}],[\"层的计算\",{\"1\":{\"522\":1}}],[\"层的输出作为起点\",{\"1\":{\"385\":1}}],[\"层的参数\",{\"1\":{\"214\":1}}],[\"层的\",{\"1\":{\"214\":1,\"236\":1,\"385\":1}}],[\"层的高性能网络\",{\"1\":{\"109\":1}}],[\"层视觉\",{\"1\":{\"197\":1}}],[\"层不共享参数\",{\"1\":{\"179\":1}}],[\"层则可以共享\",{\"1\":{\"171\":1}}],[\"层以外的所有参数\",{\"1\":{\"171\":1}}],[\"层维度\",{\"1\":{\"157\":1}}],[\"层进一步融合局部\",{\"1\":{\"156\":1}}],[\"层后面都加了\",{\"1\":{\"152\":1}}],[\"层后的结果\",{\"1\":{\"67\":1}}],[\"层层插值并融合特征\",{\"1\":{\"146\":1}}],[\"层层下采样并提取特征\",{\"1\":{\"146\":1}}],[\"层堆叠\",{\"1\":{\"143\":2}}],[\"层构成了一个\",{\"1\":{\"138\":1}}],[\"层提取特征\",{\"1\":{\"137\":1}}],[\"层次化结构由多个set\",{\"1\":{\"133\":1}}],[\"层次化点集特征学习\",{\"0\":{\"133\":1}}],[\"层基于向量自注意力\",{\"1\":{\"113\":1}}],[\"层点数\",{\"1\":{\"83\":1}}],[\"层归一化调整\",{\"1\":{\"640\":1}}],[\"层归一化来生成所有可能答案的分布\",{\"1\":{\"631\":1}}],[\"层归一化\",{\"1\":{\"69\":1,\"385\":2,\"633\":1,\"725\":1,\"750\":1}}],[\"层将计算结果映射至词表空间\",{\"1\":{\"660\":1}}],[\"层将\",{\"1\":{\"64\":1,\"421\":1}}],[\"层\",{\"0\":{\"113\":1,\"399\":1},\"1\":{\"24\":1,\"109\":2,\"115\":1,\"144\":2,\"146\":4,\"154\":1,\"171\":1,\"179\":1,\"197\":2,\"214\":1,\"215\":2,\"224\":1,\"233\":1,\"234\":1,\"236\":1,\"272\":2,\"274\":1,\"306\":1,\"311\":1,\"317\":2,\"380\":1,\"429\":3,\"500\":1,\"586\":1,\"587\":1,\"588\":1,\"589\":1,\"699\":1,\"892\":4}}],[\"特定任务输入转换\",{\"0\":{\"631\":1}}],[\"特殊情况说明\",{\"0\":{\"861\":1}}],[\"特殊\",{\"1\":{\"735\":1}}],[\"特殊操作后的中间结果\",{\"1\":{\"470\":1}}],[\"特殊像素映射操作\",{\"1\":{\"264\":1}}],[\"特别强化了对客观事实的准确性\",{\"1\":{\"823\":1}}],[\"特别值得注意的是\",{\"1\":{\"642\":1}}],[\"特别适用于图像分割任务\",{\"1\":{\"592\":1}}],[\"特别常用于文本向量\",{\"1\":{\"506\":1}}],[\"特别是分析等式\",{\"1\":{\"948\":1}}],[\"特别是语言建模\",{\"1\":{\"696\":1}}],[\"特别是降后期的推理成本\",{\"1\":{\"607\":1}}],[\"特别是\",{\"1\":{\"377\":1,\"523\":1,\"950\":1}}],[\"特别是在小数据集\",{\"1\":{\"641\":1}}],[\"特别是在小样本数据集上\",{\"1\":{\"243\":1}}],[\"特别是在像\",{\"1\":{\"594\":1}}],[\"特别是在遮挡严重的情况下\",{\"1\":{\"157\":1}}],[\"特别注意初始化方法中传入的\",{\"1\":{\"293\":1}}],[\"特性\",{\"1\":{\"106\":4,\"472\":1,\"557\":1,\"588\":1,\"808\":1}}],[\"特征比较\",{\"1\":{\"830\":2}}],[\"特征值分解\",{\"1\":{\"574\":1}}],[\"特征图的边长\",{\"1\":{\"892\":1}}],[\"特征图\",{\"1\":{\"501\":1,\"582\":2}}],[\"特征较特殊\",{\"1\":{\"385\":1}}],[\"特征向量可提前计算存储\",{\"1\":{\"368\":1}}],[\"特征回归\",{\"1\":{\"368\":1}}],[\"特征质量\",{\"1\":{\"280\":1}}],[\"特征中会自然出现场景布局与物体边界\",{\"1\":{\"280\":1}}],[\"特征的影响\",{\"1\":{\"280\":1}}],[\"特征队列初始化\",{\"1\":{\"192\":1}}],[\"特征映射到共享空间\",{\"1\":{\"192\":1}}],[\"特征投影层\",{\"1\":{\"190\":1}}],[\"特征空间一致\",{\"1\":{\"305\":1}}],[\"特征空间变换矩阵\",{\"1\":{\"154\":1}}],[\"特征空间对齐\",{\"1\":{\"76\":1}}],[\"特征插值方式\",{\"1\":{\"143\":1}}],[\"特征编码\",{\"1\":{\"136\":1}}],[\"特征升维\",{\"1\":{\"123\":1}}],[\"特征融合\",{\"1\":{\"122\":1,\"123\":1,\"160\":1}}],[\"特征和批次索引\",{\"1\":{\"120\":1,\"121\":1}}],[\"特征分组\",{\"1\":{\"119\":1}}],[\"特征分布对齐损失\",{\"1\":{\"79\":1}}],[\"特征聚合\",{\"1\":{\"119\":1}}],[\"特征变换开关\",{\"1\":{\"154\":1}}],[\"特征变换\",{\"1\":{\"119\":1}}],[\"特征通道维度\",{\"1\":{\"100\":1}}],[\"特征通过高斯映射投影到\",{\"1\":{\"23\":1}}],[\"特征传播\",{\"1\":{\"143\":1,\"145\":1}}],[\"特征传播阶段结束\",{\"1\":{\"94\":1}}],[\"特征传播阶段\",{\"1\":{\"94\":1}}],[\"特征传播层\",{\"0\":{\"145\":1},\"1\":{\"83\":1}}],[\"特征池化\",{\"1\":{\"83\":1}}],[\"特征拆分为\",{\"1\":{\"83\":1}}],[\"特征压缩\",{\"1\":{\"83\":1}}],[\"特征嵌入维度\",{\"1\":{\"83\":1}}],[\"特征增强\",{\"1\":{\"83\":1,\"122\":1,\"160\":1}}],[\"特征提取网络相对轻量\",{\"1\":{\"415\":1}}],[\"特征提取网络\",{\"1\":{\"293\":1}}],[\"特征提取\",{\"0\":{\"154\":1},\"1\":{\"78\":1}}],[\"特征维度\",{\"1\":{\"70\":1,\"380\":1}}],[\"特征从原始嵌入维度\",{\"1\":{\"66\":1}}],[\"特征\",{\"1\":{\"64\":2,\"83\":1,\"119\":3,\"122\":2,\"123\":1,\"145\":1,\"205\":1,\"210\":1,\"385\":11,\"710\":1}}],[\"特征保留几何信息\",{\"1\":{\"23\":1}}],[\"特征保留语义\",{\"1\":{\"23\":1}}],[\"特点与作用\",{\"1\":{\"106\":4}}],[\"特点\",{\"1\":{\"6\":1,\"7\":1,\"9\":1,\"11\":1,\"12\":1,\"14\":1,\"263\":1,\"490\":1,\"491\":1,\"505\":1,\"507\":1,\"508\":1,\"510\":1,\"587\":2,\"823\":4}}],[\"然后用\",{\"1\":{\"944\":1}}],[\"然后用行归一化将多个正样本平均分配权重\",{\"1\":{\"190\":1}}],[\"然后送入\",{\"1\":{\"935\":1}}],[\"然后按比例混合\",{\"1\":{\"894\":1}}],[\"然后他们在\",{\"1\":{\"885\":1}}],[\"然后套入\",{\"1\":{\"885\":1}}],[\"然后延展设计核心功能的上下游功能\",{\"1\":{\"836\":1}}],[\"然后从中选出最合适的那个\",{\"1\":{\"737\":1}}],[\"然后启动\",{\"1\":{\"712\":1}}],[\"然后换维度到\",{\"1\":{\"710\":1}}],[\"然后查表取出偏置\",{\"1\":{\"710\":1}}],[\"然后给出一个假设\",{\"1\":{\"694\":1}}],[\"然后让模型通过上下文预测那一个被遮盖或替换的部分\",{\"1\":{\"691\":1}}],[\"然后让模型去学习特征\",{\"1\":{\"353\":1}}],[\"然后切分成query\",{\"1\":{\"663\":1}}],[\"然后作为下一次推理的输入\",{\"1\":{\"660\":1}}],[\"然后能成功迁移学习解决判别式任务\",{\"1\":{\"636\":1}}],[\"然后累积计算每个误差对iou得分的影响\",{\"1\":{\"591\":1}}],[\"然后损失就是\",{\"1\":{\"590\":1}}],[\"然后打包成一个group\",{\"1\":{\"547\":1}}],[\"然后再进行详细解析\",{\"1\":{\"709\":1}}],[\"然后再把新的问题让小孩子来解决\",{\"1\":{\"620\":1}}],[\"然后再经过一层\",{\"1\":{\"434\":1}}],[\"然后再拿到其它数据集上做迁移学习\",{\"1\":{\"432\":1}}],[\"然后再将模型迁移到具体的下游任务上进行微调\",{\"1\":{\"240\":1}}],[\"然后输入到mlp\",{\"1\":{\"431\":1}}],[\"然后输入浅层\",{\"1\":{\"214\":1}}],[\"然后调整形状并重新排列维度\",{\"1\":{\"430\":1}}],[\"然后图像的表征和\",{\"1\":{\"421\":1}}],[\"然后计算以下表达式的梯度\",{\"1\":{\"946\":1}}],[\"然后计算出和当前文本描述相似度最高的那副图片\",{\"1\":{\"411\":1}}],[\"然后计算每个分类文本对应的文本嵌入向量\",{\"1\":{\"410\":1}}],[\"然后利用模型获取文本特征\",{\"1\":{\"410\":1}}],[\"然后提取了相应的文本特征\",{\"1\":{\"408\":1}}],[\"然后文本输出接两层mlp预测mask掉的tokens\",{\"1\":{\"393\":1}}],[\"然后和postion\",{\"1\":{\"392\":1}}],[\"然后和position\",{\"1\":{\"392\":1}}],[\"然后还原维度为\",{\"1\":{\"380\":1}}],[\"然后对\",{\"1\":{\"904\":1}}],[\"然后对句子进行分词\",{\"1\":{\"697\":1}}],[\"然后对文本标志位对应输出使用一个线性的itm\",{\"1\":{\"393\":1}}],[\"然后对文本\",{\"1\":{\"374\":1}}],[\"然后对它们的特征做加权平均\",{\"1\":{\"145\":1}}],[\"然后在每项具体任务上判别性微调discriminative\",{\"1\":{\"625\":1}}],[\"然后在训练过程中\",{\"1\":{\"428\":1}}],[\"然后在具体的下游任务上进行微调\",{\"1\":{\"413\":1}}],[\"然后在文本语料上预训练语言专家\",{\"1\":{\"376\":1}}],[\"然后在下游任务如图文检索\",{\"1\":{\"368\":1}}],[\"然后在后续会进行如下处理\",{\"1\":{\"100\":1}}],[\"然后每次训练的时候\",{\"1\":{\"357\":1}}],[\"然后牺牲一些一致性\",{\"1\":{\"357\":1}}],[\"然后进行水平翻转\",{\"1\":{\"425\":1}}],[\"然后进来\",{\"1\":{\"356\":1}}],[\"然后进入pointnet++的特征传播阶段\",{\"1\":{\"54\":1}}],[\"然后经过e12这个编码器\",{\"1\":{\"353\":1}}],[\"然后我们读取要预测的图像\",{\"1\":{\"408\":1}}],[\"然后我们加一个\",{\"1\":{\"355\":1}}],[\"然后我们将图片x1经过数据增强t1得到图片x11\",{\"1\":{\"353\":1}}],[\"然后我们便可以直接用训练好的vit和bert来做图文匹配和图文相似度计算了\",{\"1\":{\"191\":1}}],[\"然后加入位置编码并送入\",{\"1\":{\"266\":1}}],[\"然后随机选择一个遮挡块的宽高比\",{\"1\":{\"234\":1}}],[\"然后将tf模型转为对应的pytorch版本即可\",{\"1\":{\"712\":1}}],[\"然后将该位置的\",{\"1\":{\"694\":1}}],[\"然后将所有的\",{\"1\":{\"693\":1}}],[\"然后将\",{\"1\":{\"434\":1,\"569\":1}}],[\"然后将特征图的最后两维展平为一维\",{\"1\":{\"426\":1}}],[\"然后将这个\",{\"1\":{\"947\":1}}],[\"然后将这个骨干网络参数冻结\",{\"1\":{\"352\":1}}],[\"然后将这些不同尺度的特征拼接在一起\",{\"1\":{\"141\":1}}],[\"然后将每个\",{\"1\":{\"266\":1}}],[\"然后将全局表示送入\",{\"1\":{\"238\":1}}],[\"然后将其输入\",{\"1\":{\"234\":1}}],[\"然后预测这些被遮挡位置对应的视觉\",{\"1\":{\"234\":1}}],[\"然后训练模型恢复被掩码的\",{\"1\":{\"223\":1}}],[\"然后训练模型去恢复被遮挡的\",{\"1\":{\"212\":1}}],[\"然后基于这些码重建图像\",{\"1\":{\"216\":1}}],[\"然后定义\",{\"1\":{\"202\":1}}],[\"然后取\",{\"1\":{\"153\":1}}],[\"然后取平均得到\",{\"1\":{\"106\":1}}],[\"然后通过微调\",{\"1\":{\"824\":1}}],[\"然后通过人类对多个模型输出的偏好进行排序\",{\"1\":{\"653\":1}}],[\"然后通过全连接层\",{\"1\":{\"155\":2}}],[\"然后通过\",{\"1\":{\"143\":1,\"694\":1}}],[\"然后通过对应的pointnets提取每个尺度上的特征来捕获多尺度模式\",{\"1\":{\"140\":1}}],[\"然后只保留前\",{\"1\":{\"137\":1}}],[\"然后复制这个索引数组到每个\",{\"1\":{\"137\":1}}],[\"然后把所有点映射到高维的特征通过最大池化最终表示全局特征\",{\"1\":{\"131\":1}}],[\"然后使用prepare\",{\"1\":{\"696\":1}}],[\"然后使用第一个cls\",{\"1\":{\"417\":1}}],[\"然后使用一个对比学习的函数去训练模型就可以了\",{\"1\":{\"350\":1}}],[\"然后使用全局最大池化\",{\"1\":{\"157\":1}}],[\"然后使用\",{\"1\":{\"110\":1,\"157\":1}}],[\"然后求和\",{\"1\":{\"106\":1}}],[\"然后\",{\"1\":{\"32\":1,\"408\":1,\"453\":1,\"542\":1,\"591\":1,\"835\":1,\"846\":1}}],[\"然后融合多尺度特征\",{\"1\":{\"23\":1}}],[\"然而在某些情况下\",{\"1\":{\"828\":1}}],[\"然而大规模的未标注的文本语料是丰富\",{\"1\":{\"625\":1}}],[\"然而\",{\"1\":{\"19\":1,\"234\":1,\"298\":1,\"299\":1,\"323\":1,\"330\":1,\"407\":1,\"413\":3,\"639\":1,\"646\":2,\"822\":1,\"825\":1,\"849\":1,\"872\":1,\"885\":1,\"942\":1,\"946\":1,\"948\":1,\"949\":1,\"951\":1}}],[\"粒度自适应视觉\",{\"1\":{\"23\":1}}],[\"粒度自适应融合模块\",{\"1\":{\"19\":2}}],[\"图解transformer\",{\"0\":{\"738\":1},\"1\":{\"738\":1}}],[\"图解bert\",{\"1\":{\"711\":1}}],[\"图解\",{\"0\":{\"711\":1}}],[\"图7\",{\"1\":{\"613\":1}}],[\"图6\",{\"1\":{\"611\":1}}],[\"图示\",{\"1\":{\"502\":1}}],[\"图生文\",{\"1\":{\"420\":1}}],[\"图中有什么\",{\"1\":{\"341\":1}}],[\"图中展示的是\",{\"1\":{\"204\":1}}],[\"图表解析\",{\"1\":{\"335\":1}}],[\"图表和场景文本理解方面仍显著落后于商业模型\",{\"1\":{\"326\":1}}],[\"图5\",{\"1\":{\"332\":1,\"611\":1}}],[\"图块数量为1至12个\",{\"1\":{\"330\":1}}],[\"图\",{\"1\":{\"192\":1,\"228\":1,\"234\":1,\"305\":1,\"374\":1,\"375\":1,\"569\":1,\"570\":1,\"571\":1,\"572\":2,\"655\":1,\"656\":2,\"871\":1,\"875\":1,\"889\":2}}],[\"图文联合生成图像\",{\"0\":{\"895\":1}}],[\"图文到文本\",{\"1\":{\"421\":1}}],[\"图文生成等多模态任务\",{\"1\":{\"402\":1}}],[\"图文基础模型\",{\"1\":{\"269\":2}}],[\"图文对被看作\",{\"1\":{\"220\":1}}],[\"图文对\",{\"1\":{\"220\":1,\"224\":1}}],[\"图文对比\",{\"1\":{\"418\":1}}],[\"图文对比损失\",{\"1\":{\"206\":1,\"274\":1}}],[\"图文对比损失定义为交叉熵\",{\"1\":{\"199\":1}}],[\"图文对比目标\",{\"1\":{\"190\":1}}],[\"图文对比学习后的共同嵌入维度\",{\"1\":{\"205\":1}}],[\"图文对比学习旨在融合之前学习更好的单模态表示\",{\"1\":{\"199\":1}}],[\"图文对比学习\",{\"1\":{\"165\":1,\"368\":2,\"373\":1}}],[\"图文融合\",{\"1\":{\"208\":1,\"368\":1}}],[\"图文共享表示的嵌入维度\",{\"1\":{\"192\":1}}],[\"图文数据的自举式清洗机制\",{\"0\":{\"173\":1}}],[\"图文检索\",{\"1\":{\"165\":1,\"313\":1,\"402\":1}}],[\"图文匹配任务\",{\"1\":{\"190\":1}}],[\"图文匹配目标\",{\"1\":{\"190\":1}}],[\"图文匹配\",{\"1\":{\"165\":1,\"192\":2,\"223\":1,\"368\":1,\"373\":1}}],[\"图卷积\",{\"1\":{\"157\":1}}],[\"图池化\",{\"1\":{\"110\":1}}],[\"图上进行图卷积\",{\"1\":{\"110\":1}}],[\"图4显示\",{\"1\":{\"641\":1}}],[\"图4a\",{\"1\":{\"303\":1,\"304\":1}}],[\"图4c\",{\"1\":{\"116\":1,\"303\":1,\"304\":1}}],[\"图4b\",{\"1\":{\"116\":1,\"303\":2}}],[\"图4\",{\"1\":{\"73\":1,\"177\":1,\"303\":1,\"304\":1,\"609\":1}}],[\"图2显示模型性能与训练token量强相关\",{\"1\":{\"668\":1}}],[\"图2\",{\"1\":{\"73\":2,\"609\":1,\"641\":1}}],[\"图片切割\",{\"0\":{\"426\":1}}],[\"图片预处理\",{\"0\":{\"425\":1}}],[\"图片库中的图片\",{\"1\":{\"411\":1}}],[\"图片分类\",{\"1\":{\"410\":1,\"412\":1}}],[\"图片分类实战\",{\"1\":{\"410\":1}}],[\"图片取至\",{\"1\":{\"381\":1}}],[\"图片x1经过数据增强t2得到图片x12\",{\"1\":{\"353\":1}}],[\"图片\",{\"1\":{\"53\":1,\"82\":1}}],[\"图片索引文件路径\",{\"1\":{\"53\":1}}],[\"图1中显示的不同模型在人类偏好评估中的胜率清晰反映了该方法的有效性\",{\"1\":{\"654\":1}}],[\"图1\",{\"1\":{\"30\":2,\"391\":1,\"609\":1}}],[\"图3c\",{\"1\":{\"944\":1}}],[\"图3b\",{\"1\":{\"944\":1}}],[\"图3\",{\"0\":{\"23\":1},\"1\":{\"30\":1,\"41\":1,\"43\":1,\"304\":1,\"609\":1,\"944\":6}}],[\"图像像素级别\",{\"1\":{\"963\":1}}],[\"图像去模糊等\",{\"1\":{\"961\":1}}],[\"图像修复\",{\"1\":{\"961\":1}}],[\"图像压缩成离散向量时主要借助了嵌入空间\",{\"1\":{\"961\":1}}],[\"图像压缩成一个\",{\"1\":{\"885\":1}}],[\"图像整体建模为像素独立\",{\"1\":{\"932\":1}}],[\"图像已生成并保存为\",{\"1\":{\"926\":1}}],[\"图像嵌入维度\",{\"1\":{\"900\":1}}],[\"图像尺寸检查\",{\"1\":{\"899\":1}}],[\"图像标准化\",{\"1\":{\"899\":1}}],[\"图像标准化参数\",{\"1\":{\"899\":1}}],[\"图像标注一般被映射为离散类别向量\",{\"1\":{\"271\":1}}],[\"图像通道数\",{\"1\":{\"899\":1,\"900\":1}}],[\"图像通过视觉编码器\",{\"1\":{\"420\":1}}],[\"图像联合建模\",{\"1\":{\"898\":1}}],[\"图像补全\",{\"1\":{\"895\":1}}],[\"图像引导生成\",{\"1\":{\"895\":1}}],[\"图像引导的文本解码器\",{\"1\":{\"171\":1}}],[\"图像引导的文本编码器\",{\"1\":{\"171\":1}}],[\"图像先验\",{\"1\":{\"894\":1}}],[\"图像损失在总损失中的相对重要性系数\",{\"1\":{\"893\":1}}],[\"图像损失在最终\",{\"1\":{\"892\":1}}],[\"图像损失权重self\",{\"1\":{\"893\":1}}],[\"图像损失通常占更大比例\",{\"1\":{\"893\":1}}],[\"图像位置编码\",{\"1\":{\"893\":1}}],[\"图像使用二维\",{\"1\":{\"892\":1}}],[\"图像输入经过\",{\"1\":{\"892\":1}}],[\"图像经过\",{\"1\":{\"892\":1}}],[\"图像生成\",{\"1\":{\"891\":1}}],[\"图像匹配的训练方法\",{\"1\":{\"889\":1}}],[\"图像起始\",{\"1\":{\"887\":1}}],[\"图像到图像部分则使用行\",{\"1\":{\"887\":1}}],[\"图像到文本的桥梁\",{\"1\":{\"436\":1}}],[\"图像到文本的相似度权重\",{\"1\":{\"207\":1}}],[\"图像则用\",{\"1\":{\"887\":1}}],[\"图像空间的含义\",{\"0\":{\"875\":1}}],[\"图像三种模态的深度理解能力\",{\"1\":{\"823\":1}}],[\"图像直观理解\",{\"1\":{\"574\":1}}],[\"图像或\",{\"1\":{\"425\":2}}],[\"图像注意力掩码\",{\"1\":{\"421\":1}}],[\"图像对是从互联网收集的\",{\"1\":{\"413\":1}}],[\"图像对为负样本\",{\"1\":{\"407\":1}}],[\"图像对的相似度\",{\"1\":{\"407\":1}}],[\"图像对的训练batch\",{\"1\":{\"407\":1}}],[\"图像对的预训练方法\",{\"1\":{\"406\":1}}],[\"图像对\",{\"1\":{\"406\":1,\"888\":1}}],[\"图像掩码\",{\"1\":{\"397\":1}}],[\"图像推理\",{\"1\":{\"385\":1}}],[\"图像模态特征\",{\"1\":{\"397\":1}}],[\"图像模态的\",{\"1\":{\"380\":1}}],[\"图像模式\",{\"1\":{\"385\":1}}],[\"图像类型\",{\"1\":{\"384\":1}}],[\"图像的交叉熵损失乘以\",{\"1\":{\"887\":1}}],[\"图像的分布\",{\"1\":{\"885\":1}}],[\"图像的索引\",{\"1\":{\"424\":1}}],[\"图像的更重\",{\"1\":{\"390\":1}}],[\"图像的特征\",{\"1\":{\"384\":1}}],[\"图像的序列特征\",{\"1\":{\"384\":1}}],[\"图像的\",{\"1\":{\"384\":2}}],[\"图像的文字描述\",{\"1\":{\"341\":1}}],[\"图像张量\",{\"1\":{\"384\":1}}],[\"图像为空\",{\"1\":{\"385\":1}}],[\"图像为\",{\"1\":{\"380\":1}}],[\"图像语料\",{\"1\":{\"377\":1}}],[\"图像调整至目标分辨率\",{\"1\":{\"331\":1}}],[\"图像和声音\",{\"1\":{\"824\":1}}],[\"图像和音频领域取得显著成果\",{\"1\":{\"884\":1}}],[\"图像和音频\",{\"1\":{\"325\":1}}],[\"图像和文本的联合概率\",{\"1\":{\"885\":1}}],[\"图像和文本匹配\",{\"1\":{\"276\":1}}],[\"图像和文本特征可预计算\",{\"1\":{\"369\":1}}],[\"图像和文本特征空间不一致\",{\"1\":{\"194\":1}}],[\"图像和文本特征统一到共享嵌入空间\",{\"1\":{\"23\":1}}],[\"图像和文本分别编码\",{\"1\":{\"171\":1,\"368\":1,\"369\":1}}],[\"图像字幕\",{\"1\":{\"310\":1}}],[\"图像→文本检索任务中达到\",{\"1\":{\"309\":1}}],[\"图像监督学习往往把丰富的视觉信息压缩为一个类别标签\",{\"1\":{\"280\":1}}],[\"图像转换为\",{\"1\":{\"264\":1}}],[\"图像转为\",{\"1\":{\"264\":1}}],[\"图像增强是对比学习或自监督学习的基础\",{\"1\":{\"264\":1}}],[\"图像视觉\",{\"1\":{\"252\":1}}],[\"图像做可视化\",{\"1\":{\"243\":1}}],[\"图像表示\",{\"0\":{\"230\":1},\"1\":{\"371\":1}}],[\"图像表示部分\",{\"1\":{\"212\":1}}],[\"图像块嵌入层\",{\"1\":{\"427\":1}}],[\"图像块的尺寸\",{\"1\":{\"427\":1}}],[\"图像块没有现成的词汇表\",{\"1\":{\"228\":1}}],[\"图像块\",{\"1\":{\"227\":1,\"228\":1}}],[\"图像块聚合策略\",{\"1\":{\"210\":1}}],[\"图像处理\",{\"1\":{\"224\":1}}],[\"图像处理或其他数据集中用于抽样的算法\",{\"1\":{\"134\":1}}],[\"图像预处理\",{\"1\":{\"582\":1}}],[\"图像预处理的转换操作\",{\"1\":{\"424\":1}}],[\"图像预处理类型\",{\"1\":{\"213\":1}}],[\"图像预训练\",{\"1\":{\"376\":3}}],[\"图像预训练分辨率为\",{\"1\":{\"176\":1}}],[\"图像被划分为\",{\"1\":{\"212\":1}}],[\"图像部分的平均交叉熵损失\",{\"1\":{\"893\":1}}],[\"图像部分的\",{\"1\":{\"208\":1,\"893\":1}}],[\"图像部分主要来源于\",{\"1\":{\"41\":1}}],[\"图像全局语义\",{\"1\":{\"206\":1}}],[\"图像动量编码器\",{\"1\":{\"192\":1}}],[\"图像编码阶段\",{\"1\":{\"421\":1}}],[\"图像编码\",{\"1\":{\"192\":1,\"368\":1,\"385\":1,\"885\":1,\"900\":1}}],[\"图像编码器采用了\",{\"1\":{\"410\":1}}],[\"图像编码器采用了预训练的\",{\"1\":{\"197\":1}}],[\"图像编码器和图文融合编码器\",{\"1\":{\"368\":1}}],[\"图像编码器初始\",{\"1\":{\"315\":1}}],[\"图像编码器的输出作为\",{\"1\":{\"272\":1}}],[\"图像编码器有\",{\"1\":{\"272\":1}}],[\"图像编码器提供潜在编码特征\",{\"1\":{\"271\":1}}],[\"图像编码器输出\",{\"1\":{\"208\":1}}],[\"图像编码器基于在\",{\"1\":{\"176\":1}}],[\"图像编码器\",{\"1\":{\"64\":1,\"197\":1,\"306\":1,\"315\":1,\"900\":1}}],[\"图像与文本特征提取\",{\"1\":{\"192\":1}}],[\"图像与点云之间不需要一一对应\",{\"1\":{\"43\":1}}],[\"图像队列\",{\"1\":{\"192\":1}}],[\"图像任务中数据增强广泛应用\",{\"1\":{\"169\":1}}],[\"图像描述生成\",{\"1\":{\"268\":1}}],[\"图像描述生成的引导提示词\",{\"1\":{\"187\":1}}],[\"图像描述\",{\"1\":{\"165\":1,\"220\":1,\"313\":1,\"332\":1,\"341\":2}}],[\"图像条件语言建模\",{\"1\":{\"165\":1}}],[\"图像条件解码器三种模式\",{\"1\":{\"165\":1}}],[\"图像条件编码器\",{\"1\":{\"165\":1}}],[\"图像中各个类别的像素数量通常不均衡\",{\"1\":{\"584\":1}}],[\"图像中也可能存在未被文本描述的实体\",{\"1\":{\"202\":1}}],[\"图像中\",{\"1\":{\"114\":1,\"900\":1}}],[\"图像识别\",{\"1\":{\"110\":1}}],[\"图像分割为\",{\"1\":{\"334\":1}}],[\"图像分割等任务\",{\"1\":{\"106\":1}}],[\"图像分辨率\",{\"1\":{\"323\":1}}],[\"图像分块尺寸\",{\"1\":{\"264\":1}}],[\"图像分类\",{\"0\":{\"238\":1},\"1\":{\"220\":1,\"308\":1,\"500\":1}}],[\"图像分支依赖目标检测器\",{\"1\":{\"415\":1}}],[\"图像分支提取到的是\",{\"1\":{\"83\":1}}],[\"图像分支\",{\"1\":{\"78\":1,\"415\":1}}],[\"图像交互提示区域\",{\"1\":{\"83\":1}}],[\"图像交互信息与点云特征做融合\",{\"1\":{\"59\":1}}],[\"图像里交互的这部分\",{\"1\":{\"83\":1}}],[\"图像样本\",{\"1\":{\"53\":1}}],[\"图像所属的物体名\",{\"1\":{\"52\":1}}],[\"图像融合两阶段3d检测框架\",{\"1\":{\"46\":1}}],[\"图像按可供性类别进行分类\",{\"1\":{\"42\":1}}],[\"图像数据\",{\"1\":{\"223\":1,\"887\":1,\"888\":1}}],[\"图像数\",{\"1\":{\"41\":1}}],[\"图像特征和掩码\",{\"1\":{\"421\":1}}],[\"图像特征提取和模态融合都很重\",{\"1\":{\"415\":1}}],[\"图像特征提取和投影\",{\"1\":{\"190\":1}}],[\"图像特征提取与分类\",{\"1\":{\"408\":1}}],[\"图像特征输入部分\",{\"1\":{\"392\":1}}],[\"图像特征序列\",{\"1\":{\"385\":1}}],[\"图像特征作为cross\",{\"1\":{\"421\":1}}],[\"图像特征作为\",{\"1\":{\"207\":1,\"402\":1}}],[\"图像特征与所有文本特征做内积\",{\"1\":{\"206\":1}}],[\"图像特征队列\",{\"1\":{\"205\":1}}],[\"图像特征\",{\"1\":{\"38\":1,\"205\":1,\"385\":2}}],[\"图像\",{\"0\":{\"8\":1,\"10\":1,\"231\":1,\"233\":1},\"1\":{\"19\":1,\"22\":1,\"46\":1,\"159\":1,\"190\":1,\"207\":1,\"208\":1,\"220\":2,\"223\":1,\"224\":1,\"230\":1,\"231\":2,\"232\":1,\"248\":1,\"251\":1,\"264\":4,\"265\":1,\"272\":1,\"274\":2,\"295\":1,\"305\":4,\"315\":1,\"342\":1,\"371\":1,\"384\":1,\"385\":6,\"417\":1,\"426\":1,\"885\":4,\"887\":1,\"892\":3,\"893\":1,\"895\":2,\"899\":1,\"900\":6,\"924\":1,\"956\":1}}],[\"提交的\",{\"1\":{\"656\":1}}],[\"提前做好的假设\",{\"1\":{\"422\":1}}],[\"提示仍可能触发毒性响应\",{\"1\":{\"670\":1}}],[\"提示下\",{\"1\":{\"657\":1}}],[\"提示下生成翻译\",{\"1\":{\"640\":1}}],[\"提示时\",{\"1\":{\"655\":1,\"657\":1}}],[\"提示未来需在结构理解与逻辑泛化方面进一步改进\",{\"1\":{\"648\":1}}],[\"提示+top\",{\"1\":{\"641\":1}}],[\"提示生成答案\",{\"1\":{\"641\":1}}],[\"提示词列表的\",{\"1\":{\"663\":1}}],[\"提示词列表\",{\"1\":{\"663\":1}}],[\"提示词工程\",{\"1\":{\"616\":1}}],[\"提示词\",{\"1\":{\"616\":1}}],[\"提示\",{\"1\":{\"409\":1}}],[\"提示调优\",{\"1\":{\"346\":2}}],[\"提高效率\",{\"1\":{\"899\":1}}],[\"提高推理效率\",{\"1\":{\"898\":1}}],[\"提高开发效率\",{\"1\":{\"834\":1}}],[\"提高了与外部系统集成的能力\",{\"1\":{\"823\":1}}],[\"提高了大型模型的推理效率\",{\"1\":{\"823\":2}}],[\"提高了灵活性\",{\"1\":{\"800\":1}}],[\"提高了框架的易用性和健壮性\",{\"1\":{\"797\":1}}],[\"提高了模型的性能和效率\",{\"1\":{\"823\":1}}],[\"提高了模型的泛化性和稳健性\",{\"1\":{\"141\":1}}],[\"提高了模型在空间上的泛化能力\",{\"1\":{\"135\":1}}],[\"提高代码健壮性\",{\"1\":{\"792\":1}}],[\"提高执行效率\",{\"1\":{\"788\":1}}],[\"提高逐点判别能力\",{\"1\":{\"587\":1}}],[\"提高其中一个会降低另一个\",{\"1\":{\"565\":1}}],[\"提高分类阈值往往会减少假正例的数量并增加假负例的数量\",{\"1\":{\"565\":1}}],[\"提高多项式阶数\",{\"1\":{\"500\":1}}],[\"提高训练的稳定性\",{\"1\":{\"425\":1}}],[\"提高判别能力\",{\"1\":{\"382\":1}}],[\"提高深层网络可训练性\",{\"1\":{\"380\":1}}],[\"提高至\",{\"1\":{\"311\":1}}],[\"提高线性探测性能和下游任务效果\",{\"1\":{\"215\":1}}],[\"提高预训练模型在下游任务中的表现\",{\"1\":{\"202\":1}}],[\"提高泛化\",{\"1\":{\"204\":1}}],[\"提高泛化能力\",{\"1\":{\"194\":1}}],[\"提高泛化性与鲁棒性\",{\"1\":{\"23\":1}}],[\"提高算法鲁棒性\",{\"1\":{\"141\":1}}],[\"提高模型的性能和输出质量\",{\"1\":{\"828\":1}}],[\"提高模型的收敛速度和性能\",{\"1\":{\"823\":1}}],[\"提高模型的泛化能力\",{\"1\":{\"425\":1}}],[\"提高模型的泛化性能\",{\"1\":{\"141\":1}}],[\"提高模型对语言指令下功能区域的理解能力\",{\"1\":{\"102\":1}}],[\"提高精度\",{\"1\":{\"110\":1,\"500\":2}}],[\"提高计算效率\",{\"1\":{\"110\":1,\"122\":1}}],[\"提升代码可读性\",{\"1\":{\"812\":1}}],[\"提升代码可读性和鲁棒性\",{\"1\":{\"807\":1}}],[\"提升调试效率\",{\"1\":{\"808\":1}}],[\"提升显著\",{\"1\":{\"669\":1}}],[\"提升训练稳定性\",{\"1\":{\"667\":1}}],[\"提升训练难度\",{\"1\":{\"206\":1}}],[\"提升至52\",{\"1\":{\"641\":1}}],[\"提升监督模型的泛化能力\",{\"1\":{\"630\":1}}],[\"提升边缘识别精度\",{\"1\":{\"587\":1}}],[\"提升边界识别能力\",{\"1\":{\"102\":1}}],[\"提升精度\",{\"1\":{\"500\":1}}],[\"提升依然不明显\",{\"1\":{\"354\":1}}],[\"提升多语言支持\",{\"1\":{\"332\":1}}],[\"提升计算效率\",{\"1\":{\"329\":1}}],[\"提升视觉表征能力\",{\"1\":{\"327\":1}}],[\"提升其视觉理解能力\",{\"1\":{\"322\":1}}],[\"提升检索精度\",{\"1\":{\"303\":1}}],[\"提升模型在各种任务上的泛化能力\",{\"1\":{\"346\":1}}],[\"提升模型的少样本学习能力\",{\"1\":{\"646\":1}}],[\"提升模型的泛化性\",{\"1\":{\"327\":1}}],[\"提升模型的表达多样性\",{\"1\":{\"262\":1}}],[\"提升模型鲁棒性\",{\"1\":{\"152\":2}}],[\"提升线性探测\",{\"1\":{\"215\":1}}],[\"提升表示稳定性和泛化能力\",{\"1\":{\"195\":1}}],[\"提升表示的准确性\",{\"1\":{\"194\":1}}],[\"提升预训练及下游表现\",{\"1\":{\"194\":1}}],[\"提升了多语言能力\",{\"1\":{\"823\":1}}],[\"提升了多模态学习效果\",{\"1\":{\"169\":1}}],[\"提升了准确率\",{\"1\":{\"291\":1}}],[\"提升了模型的泛化能力和稳定性\",{\"1\":{\"150\":1}}],[\"提升效率的同时改善泛化能力\",{\"1\":{\"686\":1}}],[\"提升效率\",{\"1\":{\"125\":1}}],[\"提升\",{\"1\":{\"117\":1,\"657\":1}}],[\"提升泛化能力\",{\"1\":{\"31\":1,\"90\":1,\"167\":1,\"681\":1}}],[\"提炼问题本质\",{\"1\":{\"87\":1}}],[\"提供可视化界面和性能分析工具\",{\"1\":{\"834\":1}}],[\"提供可扩展\",{\"1\":{\"834\":1}}],[\"提供基础架构和工具\",{\"1\":{\"834\":1}}],[\"提供个性化体验\",{\"1\":{\"823\":1}}],[\"提供20b参数开源基线\",{\"1\":{\"671\":1}}],[\"提供现实路径\",{\"1\":{\"658\":1}}],[\"提供高质量示范\",{\"1\":{\"656\":1}}],[\"提供一条示例和任务描述\",{\"1\":{\"647\":1}}],[\"提供一个又大又一致的字典\",{\"1\":{\"353\":1}}],[\"提供强大的语言理解能力\",{\"1\":{\"329\":1}}],[\"提供强大的语义泛化能力\",{\"1\":{\"26\":1}}],[\"提供两种配置\",{\"1\":{\"317\":1}}],[\"提供通用的视觉特征\",{\"1\":{\"268\":1}}],[\"提供了基础抽象和\",{\"1\":{\"834\":1}}],[\"提供了丰富的智能体和工具集合\",{\"1\":{\"833\":1}}],[\"提供了业界领先的调试和观测功能\",{\"1\":{\"833\":1}}],[\"提供了一系列强大的输出解析工具\",{\"1\":{\"833\":1}}],[\"提供了一个用于测试模型生成信息真实性的基准数据集\",{\"1\":{\"655\":1}}],[\"提供了一种更可行的替代方法\",{\"1\":{\"355\":1}}],[\"提供了重要基准\",{\"1\":{\"641\":1}}],[\"提供了这些输入的简洁描述\",{\"1\":{\"631\":1}}],[\"提供了大量在\",{\"1\":{\"510\":1}}],[\"提供了\",{\"1\":{\"257\":1,\"382\":1,\"454\":1,\"836\":1}}],[\"提供了稳定基础\",{\"1\":{\"252\":1}}],[\"提供了新方向\",{\"1\":{\"248\":1}}],[\"提供的\",{\"1\":{\"831\":1}}],[\"提供的一个采样器\",{\"1\":{\"518\":1}}],[\"提供的一个计数器类\",{\"1\":{\"516\":1}}],[\"提供的一个函数\",{\"1\":{\"514\":1}}],[\"提供的统一入口\",{\"1\":{\"511\":1}}],[\"提供的相似度矩阵完成\",{\"1\":{\"386\":1}}],[\"提供的预训练好的\",{\"1\":{\"254\":1}}],[\"提供的视觉\",{\"1\":{\"249\":1}}],[\"提供的点云和功能区域标注构建\",{\"1\":{\"86\":1}}],[\"提供理论解释\",{\"1\":{\"228\":1}}],[\"提供更多额外信息\",{\"1\":{\"178\":1}}],[\"提供新描述\",{\"1\":{\"177\":1}}],[\"提供语义指导\",{\"1\":{\"83\":1}}],[\"提供\",{\"1\":{\"83\":2}}],[\"提供支撑\",{\"1\":{\"83\":1}}],[\"提供三种标准划分方式\",{\"1\":{\"44\":1}}],[\"提取训练集中每个图像对应的离散\",{\"1\":{\"964\":1}}],[\"提取并去重后用于训练\",{\"1\":{\"656\":1}}],[\"提取输入图像的特征\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"提取语言模型损失\",{\"1\":{\"420\":1}}],[\"提取语言建模损失\",{\"1\":{\"187\":1}}],[\"提取文本特征\",{\"1\":{\"408\":1}}],[\"提取速度相对更快\",{\"1\":{\"388\":1}}],[\"提取每条\",{\"1\":{\"341\":1}}],[\"提取每一点的特征向量\",{\"1\":{\"152\":1}}],[\"提取的图像特征则是分类器的输入数据\",{\"1\":{\"408\":1}}],[\"提取的特征图转换为序列形式\",{\"1\":{\"434\":1}}],[\"提取的特征不具备独特优势\",{\"1\":{\"280\":1}}],[\"提取的特征\",{\"1\":{\"266\":1}}],[\"提取的关键点集合\",{\"1\":{\"157\":1}}],[\"提取上下文特征\",{\"1\":{\"266\":1}}],[\"提取被遮挡位置上的目标\",{\"1\":{\"265\":1}}],[\"提取样本维度\",{\"1\":{\"213\":1}}],[\"提取视觉特征\",{\"1\":{\"192\":1}}],[\"提取全局特征\",{\"1\":{\"155\":2}}],[\"提取更高维的特征\",{\"1\":{\"154\":1}}],[\"提取更高级的特征表示\",{\"1\":{\"97\":1}}],[\"提取邻近点的特征\",{\"1\":{\"145\":1}}],[\"提取局部特征\",{\"1\":{\"144\":1}}],[\"提取点云的层次化特征\",{\"1\":{\"138\":1}}],[\"提取这些区域的高维特征\",{\"1\":{\"137\":1}}],[\"提取这些局部区域中的点及其特征\",{\"1\":{\"137\":1}}],[\"提取当前批次的特征\",{\"1\":{\"122\":1}}],[\"提取当前batch的点坐标和查询点坐标\",{\"1\":{\"119\":1}}],[\"提取对应的特征向量\",{\"1\":{\"119\":1}}],[\"提取\",{\"1\":{\"64\":2,\"192\":1,\"207\":1,\"385\":3,\"699\":1}}],[\"提取图像和文本特征\",{\"1\":{\"190\":1}}],[\"提取图像和点云的特征\",{\"1\":{\"64\":1}}],[\"提取图像特征表示\",{\"1\":{\"187\":1}}],[\"提取图像特征\",{\"1\":{\"22\":1,\"269\":1,\"342\":1,\"408\":1,\"421\":1}}],[\"提取特征值\",{\"1\":{\"502\":1}}],[\"提取特征\",{\"1\":{\"32\":1,\"110\":1,\"143\":1,\"150\":1,\"156\":1,\"157\":2}}],[\"提取几何属性\",{\"1\":{\"30\":1}}],[\"提取多尺度点云特征\",{\"1\":{\"22\":1}}],[\"提出并开源\",{\"1\":{\"696\":1}}],[\"提出数据\",{\"1\":{\"671\":1}}],[\"提出全局\",{\"1\":{\"373\":1}}],[\"提出统一视觉\",{\"1\":{\"368\":1}}],[\"提出了更先进的方法\",{\"1\":{\"501\":1}}],[\"提出了基于掩码图像建模的任务\",{\"1\":{\"269\":1}}],[\"提出了\",{\"1\":{\"249\":1}}],[\"提出了一种通过人类偏好比较训练代理的强化学习方法\",{\"1\":{\"655\":1}}],[\"提出了一种改进的\",{\"1\":{\"268\":1}}],[\"提出了一种基于\",{\"1\":{\"228\":1}}],[\"提出了一种针对性的解决方案\",{\"1\":{\"20\":1}}],[\"提出掩码图像建模\",{\"1\":{\"227\":1}}],[\"提出使用\",{\"1\":{\"202\":1}}],[\"提出动量蒸馏\",{\"1\":{\"194\":1}}],[\"提出的辅助文本\",{\"1\":{\"889\":1}}],[\"提出的一种通过\",{\"1\":{\"339\":1}}],[\"提出的去重流程\",{\"1\":{\"273\":1}}],[\"提出的方法\",{\"1\":{\"125\":1}}],[\"提出的\",{\"1\":{\"73\":1,\"172\":1,\"195\":1,\"656\":1,\"823\":1}}],[\"提出\",{\"1\":{\"19\":1,\"167\":2,\"210\":1,\"228\":1,\"283\":1,\"372\":1}}],[\"渲染公式\",{\"1\":{\"22\":1}}],[\"颜色橙色并填充\",{\"1\":{\"815\":1}}],[\"颜色抖动\",{\"1\":{\"286\":1,\"293\":1}}],[\"颜色扰动\",{\"1\":{\"224\":1}}],[\"颜色等\",{\"1\":{\"137\":1}}],[\"颜色等信息\",{\"1\":{\"70\":1}}],[\"颜色映射\",{\"1\":{\"107\":1}}],[\"颜色\",{\"1\":{\"22\":1,\"137\":1,\"159\":1,\"895\":1}}],[\"使生成样本更逼真\",{\"1\":{\"918\":1}}],[\"使开发者能够通过终端与\",{\"1\":{\"834\":1}}],[\"使开发者能以自然的数学表达式编写代码\",{\"1\":{\"809\":1}}],[\"使它们成为了解决复杂问题和应用于多领域的强大工具\",{\"1\":{\"825\":1}}],[\"使它们能够通过阅读大量文本来深入理解语言规则和模式\",{\"1\":{\"822\":1}}],[\"使深度学习框架具备自动求导能力\",{\"1\":{\"811\":1}}],[\"使variable实例具备ndarray的行为特征\",{\"1\":{\"808\":1}}],[\"使转置后的数据在新的布局中是连续存储的\",{\"1\":{\"545\":1}}],[\"使\",{\"1\":{\"215\":1,\"305\":1}}],[\"使特征分布更稳定\",{\"1\":{\"153\":1}}],[\"使网络能够应对实际中各种密度变换的情况\",{\"1\":{\"141\":1}}],[\"使网络能学习不同采样密度下局部点云特征的提取\",{\"1\":{\"141\":1}}],[\"使网络可扩展到百万点大场景\",{\"1\":{\"110\":1}}],[\"使局部特征的表示不够精确\",{\"1\":{\"135\":1}}],[\"使得vq\",{\"1\":{\"956\":1}}],[\"使得满足标准正态分布\",{\"1\":{\"956\":1}}],[\"使得从\",{\"1\":{\"942\":1,\"943\":1}}],[\"使得卷积核的右下部分不产生贡献\",{\"1\":{\"923\":1}}],[\"使得开发者可以更容易地构建复杂和强大的应用程序\",{\"1\":{\"834\":1}}],[\"使得开发者能够轻松地将私有数据与\",{\"1\":{\"833\":1}}],[\"使得它们可以理解和生成不同媒体类型的内容\",{\"1\":{\"824\":1}}],[\"使得它们能够在统一的语义空间中进行有效的跨模态交互\",{\"1\":{\"65\":2}}],[\"使得低维对局部位置变化敏感\",{\"1\":{\"706\":1}}],[\"使得gpt\",{\"1\":{\"647\":1}}],[\"使得要充分做区分地训练模型非常有挑战性\",{\"1\":{\"625\":1}}],[\"使得归一化后的向量的\",{\"1\":{\"507\":1,\"508\":1}}],[\"使得检测\",{\"1\":{\"501\":1}}],[\"使得预训练模型能够直接应用于下游任务\",{\"1\":{\"409\":1}}],[\"使得\",{\"1\":{\"385\":1,\"427\":1,\"706\":1,\"828\":1,\"917\":1}}],[\"使得用户可以在不改变模版流程的情况下\",{\"1\":{\"381\":1}}],[\"使得其代码看起来并不常规\",{\"1\":{\"381\":1}}],[\"使得模型能生成与\",{\"1\":{\"943\":1}}],[\"使得模型能够生成指定数字的图像\",{\"1\":{\"936\":1}}],[\"使得模型能够区分正负样本对\",{\"1\":{\"355\":1}}],[\"使得模型能够更有效地学习到有判别性的表示\",{\"1\":{\"355\":1}}],[\"使得模型训练具备了基础的\",{\"1\":{\"814\":1}}],[\"使得模型可以在资源有限的设备上进行训练和部署\",{\"1\":{\"614\":1}}],[\"使得模型可以进行跨模态对齐应用\",{\"1\":{\"271\":1}}],[\"使得模型在下游任务上的表现逐渐优化\",{\"1\":{\"609\":1}}],[\"使得模型在训练过程中可以更灵活地平衡这两部分损失\",{\"1\":{\"592\":1}}],[\"使得后续指令调优时\",{\"1\":{\"341\":1}}],[\"使得语言模型具备视觉融合能力\",{\"1\":{\"306\":1}}],[\"使得训练过程更适合规模化\",{\"1\":{\"223\":1}}],[\"使得码本初始化更加合理\",{\"1\":{\"213\":1}}],[\"使得配对的图文具有更高的相似度得分\",{\"1\":{\"199\":1}}],[\"使得每个\",{\"1\":{\"181\":1}}],[\"使得每个点的权重之和为1\",{\"1\":{\"145\":1}}],[\"使得采样点在整个点云空间中分布尽可能均匀\",{\"1\":{\"137\":1}}],[\"使得在它们之间可以共享学习到的特征表示的权重\",{\"1\":{\"131\":1}}],[\"使模型行为更可预测和可控\",{\"1\":{\"823\":1}}],[\"使模型聚焦难分类样本\",{\"1\":{\"589\":1}}],[\"使模型更关注难分类的样本\",{\"1\":{\"589\":1}}],[\"使模型更好地利用点云的几何信息\",{\"1\":{\"125\":1}}],[\"使模型在零样本分类\",{\"1\":{\"305\":1}}],[\"使模型在训练时可以对这些\",{\"1\":{\"257\":1}}],[\"使模型在预训练阶段就能获得通用性\",{\"1\":{\"220\":1}}],[\"使模型能够更准确地识别何时以及如何调用外部工具\",{\"1\":{\"823\":1}}],[\"使模型能够从上下文恢复出缺失的高层语义表示\",{\"1\":{\"234\":1}}],[\"使模型能学到更强的图文对齐表示\",{\"1\":{\"206\":1}}],[\"使模型能在不同尺度\",{\"1\":{\"19\":1}}],[\"使每个点的权重和为\",{\"1\":{\"122\":1}}],[\"使问题更具体地连接目标对象的功能\",{\"1\":{\"87\":1}}],[\"使功能表征与对齐特征相互增强\",{\"1\":{\"80\":1}}],[\"使其行为更接近numpy数组\",{\"1\":{\"812\":1}}],[\"使其支持左右操作数为variable的情况\",{\"1\":{\"809\":1}}],[\"使其支持更复杂的计算和神经网络的构建\",{\"1\":{\"797\":1}}],[\"使其完全可开源\",{\"1\":{\"666\":1}}],[\"使其学会预测哪一输出更受偏好\",{\"1\":{\"656\":1}}],[\"使其能够建模长距离依赖关系\",{\"1\":{\"640\":1}}],[\"使其能够有效将图像映射到离散编码\",{\"1\":{\"212\":1}}],[\"使其能与\",{\"1\":{\"546\":1}}],[\"使其变得\",{\"1\":{\"469\":1}}],[\"使其变为序列形式\",{\"1\":{\"380\":1}}],[\"使其符合模型的输入要求\",{\"1\":{\"410\":1}}],[\"使其更好地理解和执行用户给出的自然语言指令\",{\"1\":{\"346\":1}}],[\"使其在长文本理解和复杂任务处理方面具有更强的优势\",{\"1\":{\"823\":1}}],[\"使其在ocr和中文任务上表现优异\",{\"1\":{\"327\":1}}],[\"使其在泛化性和鲁棒性上大幅提升\",{\"1\":{\"26\":1}}],[\"使其成为通用建模的一个有趣选择\",{\"1\":{\"225\":1}}],[\"使其标准化\",{\"1\":{\"153\":1}}],[\"使其姿态统一\",{\"1\":{\"152\":1}}],[\"使其简短但仍有意义\",{\"1\":{\"87\":1}}],[\"使其专注于特定领域\",{\"1\":{\"50\":1}}],[\"使视觉特征保留空间结构的同时关注指令相关区域\",{\"1\":{\"23\":1}}],[\"使用训练集提取离散\",{\"1\":{\"964\":1}}],[\"使用训练好的\",{\"1\":{\"165\":1}}],[\"使用贝叶斯公式\",{\"1\":{\"877\":1}}],[\"使用统一的大模型可以极大地提高研发效率\",{\"1\":{\"826\":1}}],[\"使用文心\",{\"1\":{\"823\":2}}],[\"使用梯度下降法寻找rosenbrock函数最小值的代码如下\",{\"1\":{\"816\":1}}],[\"使用梯度下降法能高效找到目标值\",{\"1\":{\"816\":1}}],[\"使用tinypytorch实现的代码如下\",{\"1\":{\"816\":1}}],[\"使用transformer架构为未来的多模态统一提供了可能性\",{\"1\":{\"436\":1}}],[\"使用funcs\",{\"1\":{\"815\":1}}],[\"使用farthest\",{\"1\":{\"134\":1}}],[\"使用函数类名作为标签\",{\"1\":{\"815\":1}}],[\"使用id\",{\"1\":{\"815\":1}}],[\"使用dot\",{\"1\":{\"815\":1}}],[\"使用dino作为教师模型\",{\"1\":{\"213\":1}}],[\"使用弱引用解决循环引用\",{\"1\":{\"812\":1}}],[\"使用户无需关心data属性\",{\"1\":{\"808\":1}}],[\"使用with语句便捷切换模式\",{\"1\":{\"807\":1}}],[\"使用output\",{\"1\":{\"806\":1}}],[\"使用赋值运算符\",{\"1\":{\"806\":1}}],[\"使用列表保存待处理的函数\",{\"1\":{\"787\":1}}],[\"使用中心差分近似计算数值微分\",{\"1\":{\"770\":1}}],[\"使用示例\",{\"0\":{\"757\":1},\"1\":{\"807\":1,\"808\":1}}],[\"使用掩码机制确保模型在生成目标序列时只能看到当前及之前的词\",{\"1\":{\"741\":1}}],[\"使用爱因斯坦求和公式计算\",{\"1\":{\"709\":1}}],[\"使用nltk库\",{\"1\":{\"696\":1}}],[\"使用nltk库提供的sent\",{\"1\":{\"596\":1}}],[\"使用单句对\",{\"1\":{\"681\":1}}],[\"使用单个池化的全局图像表示\",{\"1\":{\"272\":1}}],[\"使用静态掩码\",{\"1\":{\"679\":1}}],[\"使用旋转位置嵌入\",{\"1\":{\"667\":1}}],[\"使用sentencepiece的bpe算法\",{\"1\":{\"667\":1}}],[\"使用如下\",{\"1\":{\"656\":1}}],[\"使用人类标注者示范的优质输出\",{\"1\":{\"656\":1}}],[\"使用外部语言模型引导生成方向\",{\"1\":{\"655\":1}}],[\"使用数十个\",{\"1\":{\"655\":1}}],[\"使用奖励模型的反馈\",{\"1\":{\"654\":1}}],[\"使用bloom过滤器统计测试集与webtext的8\",{\"1\":{\"641\":1}}],[\"使用无监督预训练来提升在判别式任务上的表现是机器学习研究的长期目标\",{\"1\":{\"636\":1}}],[\"使用无监督预训练超参数设置\",{\"1\":{\"633\":1}}],[\"使用学习的位置嵌入\",{\"1\":{\"633\":1}}],[\"使用对应特定任务的监督目标来调整这些参数\",{\"1\":{\"626\":1}}],[\"使用对称函数\",{\"1\":{\"150\":1}}],[\"使用复杂的学习方案以及添加辅助学习目标的组合\",{\"1\":{\"626\":1}}],[\"使用最广泛的联合概率分布是多元高斯分布\",{\"1\":{\"869\":1}}],[\"使用最近邻插值保持\",{\"1\":{\"582\":1}}],[\"使用最大池化聚合局部信息\",{\"1\":{\"141\":1}}],[\"使用最大池化提取最显著的特征\",{\"1\":{\"121\":1}}],[\"使用简单\",{\"1\":{\"510\":1}}],[\"使用简单的点积或者浅层attention层来表示两种模态特征的相似性\",{\"1\":{\"390\":1}}],[\"使用双线性插值\",{\"1\":{\"502\":1}}],[\"使用截断正态分布初始化位置嵌入\",{\"1\":{\"428\":1,\"431\":1}}],[\"使用截断正态分布初始化分类标记\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"使用全连接\",{\"1\":{\"421\":1}}],[\"使用adamw优化器\",{\"1\":{\"407\":1}}],[\"使用池化后的图像特征点积计算特征相似性\",{\"1\":{\"390\":1}}],[\"使用在图像数据上预训练的\",{\"1\":{\"376\":1}}],[\"使用共享参数同时进行以下预训练任务\",{\"1\":{\"373\":1}}],[\"使用卷积网络提取网格特征\",{\"1\":{\"369\":1}}],[\"使用深度融合编码器\",{\"1\":{\"369\":1}}],[\"使用深度图和颜色图生成\",{\"1\":{\"22\":1}}],[\"使用倒数第二层视觉特征更有利于细节理解\",{\"1\":{\"344\":1}}],[\"使用较小的\",{\"1\":{\"344\":1}}],[\"使用可训练的投影矩阵\",{\"1\":{\"342\":1}}],[\"使用可学习的\",{\"1\":{\"34\":1}}],[\"使用强化学习优化策略\",{\"1\":{\"339\":1}}],[\"使用上述人类偏好数据\",{\"1\":{\"339\":1}}],[\"使用标注者示范数据\",{\"1\":{\"656\":1}}],[\"使用标注好的指令\",{\"1\":{\"339\":1}}],[\"使用标准的语言模型目标并最大化其似然\",{\"1\":{\"629\":1}}],[\"使用标量点积\",{\"1\":{\"110\":1}}],[\"使用python的weakref模块创建弱引用\",{\"1\":{\"806\":1}}],[\"使用python的unittest模块编写测试用例\",{\"1\":{\"794\":1}}],[\"使用pycharm导入项目\",{\"1\":{\"712\":1}}],[\"使用perspectiveapi对100k提示生成内容进行毒性评分\",{\"1\":{\"670\":1}}],[\"使用paddleocr生成中英文文本标注\",{\"1\":{\"332\":1}}],[\"使用pointnet++编码点云\",{\"1\":{\"94\":1}}],[\"使用完整\",{\"1\":{\"305\":1}}],[\"使用大规模但噪声较多的公开网络图像\",{\"1\":{\"305\":1}}],[\"使用当前\",{\"1\":{\"293\":1}}],[\"使用余弦学习率衰减\",{\"1\":{\"656\":1}}],[\"使用余弦调度器\",{\"1\":{\"293\":1}}],[\"使用余弦相似度\",{\"1\":{\"213\":1}}],[\"使用相似的架构和预训练任务\",{\"1\":{\"822\":1}}],[\"使用相同的骨干网络\",{\"1\":{\"293\":1}}],[\"使用相对位置编码\",{\"1\":{\"117\":1}}],[\"使用了如下方式关联编码器的输出与解码器的输入\",{\"1\":{\"958\":1}}],[\"使用了如下逻辑\",{\"1\":{\"733\":1}}],[\"使用了一种叫做\",{\"1\":{\"959\":2}}],[\"使用了一种特别的方法对vq\",{\"1\":{\"956\":1}}],[\"使用了一个偏置项\",{\"1\":{\"102\":1}}],[\"使用了两类掩码卷积\",{\"1\":{\"923\":1}}],[\"使用了从互联网收集的\",{\"1\":{\"884\":1}}],[\"使用了综合性评估框架\",{\"1\":{\"656\":1}}],[\"使用了混合的图像描述和ocr专用数据集\",{\"1\":{\"330\":1}}],[\"使用了\",{\"1\":{\"293\":1,\"680\":1}}],[\"使用更多数据等\",{\"1\":{\"678\":1}}],[\"使用更多图像\",{\"1\":{\"272\":1,\"278\":1}}],[\"使用更长的序列以及动态调整掩码模式\",{\"1\":{\"677\":1}}],[\"使用更小的\",{\"1\":{\"280\":1}}],[\"使用图像\",{\"1\":{\"369\":1}}],[\"使用图像描述的生成式损失训练的编码器\",{\"1\":{\"276\":1}}],[\"使用图注意力卷积\",{\"1\":{\"110\":1}}],[\"使用任务特定的注意力池化\",{\"1\":{\"273\":1}}],[\"使用神经网络等强函数逼近器\",{\"1\":{\"944\":1}}],[\"使用神经网络编码器\",{\"1\":{\"272\":1}}],[\"使用神经网络直接学习对称函数\",{\"1\":{\"160\":1}}],[\"使用交叉熵计算预测\",{\"1\":{\"265\":1}}],[\"使用交叉熵损失衡量生成与真实之间的差异\",{\"1\":{\"420\":1}}],[\"使用交叉熵损失训练\",{\"1\":{\"373\":1}}],[\"使用交叉熵损失函数\",{\"1\":{\"341\":1}}],[\"使用交叉熵损失\",{\"1\":{\"172\":1}}],[\"使用哪种类型的vae\",{\"1\":{\"265\":1}}],[\"使用均匀先验是为了鼓励编码器生成的离散\",{\"1\":{\"262\":1}}],[\"使用均匀先验的好处\",{\"1\":{\"262\":1}}],[\"使用离散表示\",{\"1\":{\"249\":1}}],[\"使用像素序列建模\",{\"1\":{\"247\":1}}],[\"使用预训练的\",{\"1\":{\"239\":1}}],[\"使用与\",{\"1\":{\"223\":1}}],[\"使用一组模态专家可以鼓励模型更好地捕获模态特定的信息\",{\"1\":{\"222\":1}}],[\"使用一个\",{\"1\":{\"353\":1}}],[\"使用一个步长为\",{\"1\":{\"266\":1}}],[\"使用一个共享的可学习\",{\"1\":{\"214\":1}}],[\"使用一个线性分类头进行预测\",{\"1\":{\"172\":1}}],[\"使用一个轻量级的交叉注意力模块\",{\"1\":{\"96\":1}}],[\"使用解码器重建特征\",{\"1\":{\"213\":1}}],[\"使用量化器进行向量量化\",{\"1\":{\"213\":1}}],[\"使用编码器提取特征\",{\"1\":{\"213\":1}}],[\"使用缩放层预处理图像\",{\"1\":{\"213\":1}}],[\"使用clip作为教师模型\",{\"1\":{\"213\":1}}],[\"使用vision\",{\"1\":{\"213\":2}}],[\"使用欧氏距离\",{\"1\":{\"213\":1}}],[\"使用经验策略\",{\"1\":{\"212\":1}}],[\"使用主分支特征与队列拼接结果计算图像\",{\"1\":{\"206\":1}}],[\"使用的是tnews数据集\",{\"1\":{\"712\":1}}],[\"使用的是transformer\",{\"1\":{\"626\":1}}],[\"使用的数据集\",{\"1\":{\"634\":1}}],[\"使用的图像输入\",{\"1\":{\"265\":1}}],[\"使用的设备\",{\"1\":{\"265\":1}}],[\"使用的均匀性\",{\"1\":{\"255\":1}}],[\"使用的\",{\"1\":{\"192\":1}}],[\"使用动量更新\",{\"1\":{\"356\":1}}],[\"使用动量编码器对\",{\"1\":{\"208\":1}}],[\"使用动量编码器获取特征\",{\"1\":{\"190\":1}}],[\"使用动态边条件卷积\",{\"1\":{\"110\":1}}],[\"使用分块策略\",{\"1\":{\"157\":1}}],[\"使用分割头预测最终的\",{\"0\":{\"70\":1},\"1\":{\"64\":1}}],[\"使用反距离加权\",{\"1\":{\"145\":1}}],[\"使用mini\",{\"1\":{\"133\":1}}],[\"使用下采样后的点坐标和批次索引\",{\"1\":{\"121\":1}}],[\"使用0填充索引和距离矩阵\",{\"1\":{\"119\":1}}],[\"使用广播机制计算坐标差\",{\"1\":{\"119\":1}}],[\"使用绝对位置编码\",{\"1\":{\"117\":1}}],[\"使用向量注意力\",{\"1\":{\"110\":1}}],[\"使用坐标插值生成卷积权重\",{\"1\":{\"110\":1}}],[\"使用多语言llama初始化中间件\",{\"1\":{\"296\":1}}],[\"使用多模态混合的编码器\",{\"1\":{\"183\":1}}],[\"使用多层\",{\"1\":{\"156\":1,\"368\":1}}],[\"使用多个不同的\",{\"1\":{\"183\":1}}],[\"使用多个不同大小的邻域球\",{\"1\":{\"141\":1}}],[\"使用多个\",{\"1\":{\"137\":1}}],[\"使用多个阈值计算\",{\"1\":{\"106\":1}}],[\"使用多项式函数族定义卷积核权重\",{\"1\":{\"110\":1}}],[\"使用核相关\",{\"1\":{\"110\":1}}],[\"使用超级点图表示上下文关系\",{\"1\":{\"110\":1}}],[\"使用排列不变操作\",{\"1\":{\"110\":1}}],[\"使用不同的损失函数进行训练\",{\"1\":{\"593\":1}}],[\"使用不同句式结构\",{\"1\":{\"87\":1}}],[\"使用不平衡八叉树\",{\"1\":{\"110\":1}}],[\"使用稀疏卷积\",{\"1\":{\"109\":1}}],[\"使用第4列作为默认掩码\",{\"1\":{\"107\":1}}],[\"使用另一个注意力模块\",{\"1\":{\"98\":2}}],[\"使用robert编码文本\",{\"1\":{\"94\":1}}],[\"使用残差连接\",{\"1\":{\"69\":1}}],[\"使用自注意力机制提炼两个模态之间的语义一致性\",{\"1\":{\"65\":1}}],[\"使用\",{\"0\":{\"454\":1},\"1\":{\"22\":1,\"24\":1,\"32\":1,\"55\":1,\"64\":4,\"67\":1,\"69\":4,\"70\":1,\"87\":2,\"94\":1,\"97\":1,\"100\":2,\"106\":2,\"117\":1,\"119\":1,\"137\":2,\"141\":1,\"143\":2,\"146\":2,\"150\":1,\"152\":1,\"154\":2,\"156\":1,\"157\":2,\"171\":2,\"173\":2,\"185\":1,\"188\":4,\"206\":1,\"213\":2,\"215\":1,\"220\":1,\"223\":2,\"224\":1,\"228\":1,\"236\":1,\"238\":1,\"247\":1,\"251\":1,\"255\":1,\"256\":1,\"257\":1,\"258\":2,\"259\":1,\"264\":2,\"265\":2,\"266\":1,\"272\":3,\"277\":1,\"289\":3,\"293\":2,\"311\":1,\"318\":1,\"327\":1,\"329\":1,\"339\":1,\"341\":2,\"342\":2,\"343\":1,\"346\":1,\"354\":1,\"373\":3,\"374\":1,\"375\":1,\"376\":4,\"385\":2,\"417\":1,\"420\":1,\"452\":1,\"454\":1,\"472\":1,\"614\":1,\"633\":1,\"656\":3,\"657\":2,\"658\":1,\"679\":2,\"699\":2,\"735\":1,\"815\":1,\"892\":1,\"893\":1,\"895\":2,\"898\":1,\"899\":1,\"926\":2,\"947\":1,\"963\":1}}],[\"→68\",{\"1\":{\"669\":1}}],[\"→\",{\"0\":{\"529\":1},\"1\":{\"22\":1,\"23\":2,\"69\":3,\"70\":4,\"83\":5,\"106\":1,\"116\":9,\"117\":4,\"120\":7,\"121\":10,\"122\":18,\"143\":1,\"144\":9,\"146\":4,\"150\":4,\"152\":4,\"192\":8,\"204\":3,\"205\":5,\"213\":6,\"255\":1,\"263\":1,\"277\":3,\"278\":2,\"288\":2,\"290\":3,\"331\":1,\"380\":1,\"382\":2,\"444\":3,\"472\":4,\"501\":1,\"505\":1,\"508\":2,\"514\":3,\"518\":2,\"522\":2,\"538\":4,\"574\":3,\"589\":4,\"590\":4,\"697\":1,\"709\":2,\"710\":10,\"847\":4,\"877\":1,\"892\":2,\"893\":2,\"897\":3,\"906\":2,\"909\":1,\"964\":1}}],[\"预备知识\",{\"0\":{\"943\":1}}],[\"预归一化\",{\"1\":{\"667\":1}}],[\"预热衰减方案\",{\"1\":{\"633\":1}}],[\"预编译的二进制包\",{\"1\":{\"557\":1}}],[\"预计算好的图像\",{\"1\":{\"385\":2}}],[\"预计算的最近邻索引\",{\"1\":{\"119\":1}}],[\"预留的这套模版流程是怎么设计的\",{\"1\":{\"381\":1}}],[\"预处理阶段固定掩码模式\",{\"1\":{\"683\":1}}],[\"预处理的影响\",{\"1\":{\"641\":1}}],[\"预处理中确保张量连续性以避免潜在问题\",{\"1\":{\"493\":1}}],[\"预处理这个步骤在论文里并没有详细说明\",{\"1\":{\"425\":1}}],[\"预处理图像到\",{\"1\":{\"213\":1}}],[\"预处理层\",{\"1\":{\"152\":1}}],[\"预训练和微调\",{\"1\":{\"824\":1}}],[\"预训练与微调\",{\"1\":{\"727\":1}}],[\"预训练与迁移学习的趋势\",{\"1\":{\"639\":1}}],[\"预训练方法的复制研究\",{\"1\":{\"678\":1}}],[\"预训练梯度\",{\"1\":{\"655\":1}}],[\"预训练结合监督微调的方法在nlp任务中表现突出\",{\"1\":{\"639\":1}}],[\"预训练对于获取不同级别信息的需要\",{\"1\":{\"627\":1}}],[\"预训练+微调\",{\"1\":{\"626\":1}}],[\"预训练过程\",{\"0\":{\"595\":1}}],[\"预训练过程中的一个\",{\"1\":{\"265\":1}}],[\"预训练权重\",{\"1\":{\"510\":1}}],[\"预训练权重大小为393mb\",{\"1\":{\"435\":1}}],[\"预训练参数初始化\",{\"1\":{\"374\":1}}],[\"预训练旨在从大规模图文对中学习通用的跨模态表示\",{\"1\":{\"368\":1}}],[\"预训练是\",{\"1\":{\"341\":1}}],[\"预训练视觉编码器+mlp投影器\",{\"1\":{\"334\":1}}],[\"预训练均值\",{\"1\":{\"293\":1}}],[\"预训练在\",{\"1\":{\"286\":1}}],[\"预训练教师模型\",{\"1\":{\"283\":1}}],[\"预训练池化器方案\",{\"1\":{\"278\":1}}],[\"预训练好的\",{\"1\":{\"273\":1,\"341\":1}}],[\"预训练效率\",{\"1\":{\"272\":1}}],[\"预训练用的数据集加载器\",{\"1\":{\"264\":1}}],[\"预训练步数为\",{\"1\":{\"242\":1}}],[\"预训练再微调\",{\"1\":{\"237\":1}}],[\"预训练共进行约\",{\"1\":{\"236\":1}}],[\"预训练设置\",{\"0\":{\"236\":1},\"1\":{\"224\":1}}],[\"预训练数据与处理\",{\"1\":{\"667\":1}}],[\"预训练数据集\",{\"1\":{\"265\":1}}],[\"预训练数据\",{\"1\":{\"224\":1,\"332\":1}}],[\"预训练数据规模也被扩大\",{\"1\":{\"220\":1}}],[\"预训练任务与策略\",{\"1\":{\"368\":1}}],[\"预训练任务\",{\"0\":{\"223\":1,\"373\":1},\"1\":{\"235\":1,\"376\":1,\"696\":1}}],[\"预训练一个通用基础模型\",{\"1\":{\"220\":1}}],[\"预训练完成后会被丢弃\",{\"1\":{\"214\":1}}],[\"预训练采用了\",{\"1\":{\"214\":1}}],[\"预训练阶段实际上是将上述两个任务结合起来\",{\"1\":{\"693\":1}}],[\"预训练阶段有三个\",{\"1\":{\"383\":1}}],[\"预训练阶段非常关键\",{\"1\":{\"344\":1}}],[\"预训练阶段\",{\"1\":{\"340\":1}}],[\"预训练阶段二\",{\"0\":{\"214\":1}}],[\"预训练阶段一\",{\"0\":{\"212\":1}}],[\"预训练总目标函数\",{\"1\":{\"201\":1}}],[\"预训练代码实现部分参考moco论文实现\",{\"1\":{\"192\":1}}],[\"预训练\",{\"0\":{\"192\":1,\"254\":1,\"341\":1,\"727\":1},\"1\":{\"210\":1,\"216\":1,\"217\":1,\"225\":1,\"650\":1,\"822\":1}}],[\"预训练环境为两个16\",{\"1\":{\"176\":1}}],[\"预训练细节\",{\"0\":{\"176\":1}}],[\"预训练目标变成恢复原始像素\",{\"1\":{\"242\":1}}],[\"预训练目标是最大化在损坏图像条件下\",{\"1\":{\"234\":1}}],[\"预训练目标\",{\"0\":{\"172\":1},\"1\":{\"170\":1,\"235\":1}}],[\"预训练模型下载下来之后\",{\"1\":{\"712\":1}}],[\"预训练模型很容易直接zero\",{\"1\":{\"413\":1}}],[\"预训练模型中\",{\"1\":{\"410\":1}}],[\"预训练模型名称\",{\"1\":{\"410\":1}}],[\"预训练模型\",{\"1\":{\"265\":1}}],[\"预训练模型路径\",{\"1\":{\"107\":1}}],[\"预训练模型具有强泛化能力和丰富语义知识\",{\"1\":{\"22\":1}}],[\"预测越接近0损失越小\",{\"1\":{\"910\":1}}],[\"预测越接近1损失越小\",{\"1\":{\"910\":1}}],[\"预测分布\",{\"1\":{\"899\":1}}],[\"预测下一个\",{\"1\":{\"898\":1}}],[\"预测下一个词\",{\"1\":{\"654\":1}}],[\"预测某个结果\",{\"1\":{\"878\":1}}],[\"预测正确的句对数量\",{\"1\":{\"701\":1}}],[\"预测正确的掩码词数量\",{\"1\":{\"701\":1}}],[\"预测正确视觉\",{\"1\":{\"234\":1}}],[\"预测被掩码的词\",{\"1\":{\"699\":1}}],[\"预测被遮挡位置的视觉\",{\"1\":{\"266\":1}}],[\"预测出该字的标签\",{\"1\":{\"694\":1}}],[\"预测出一个变换矩阵\",{\"1\":{\"154\":1}}],[\"预测概率分布是\",{\"1\":{\"910\":1}}],[\"预测概率\",{\"1\":{\"587\":1,\"910\":1}}],[\"预测和真实中所有正类区域之和\",{\"1\":{\"586\":1}}],[\"预测为负但实际为正的像素数量\",{\"1\":{\"590\":1}}],[\"预测为负类\",{\"1\":{\"590\":1}}],[\"预测为负例\",{\"1\":{\"561\":1}}],[\"预测为正但实际为负的像素数量\",{\"1\":{\"590\":1}}],[\"预测为正且实际也为正的像素数量\",{\"1\":{\"590\":1}}],[\"预测为正类\",{\"1\":{\"590\":2}}],[\"预测为正类且实际也为正类的部分\",{\"1\":{\"586\":1}}],[\"预测为正例\",{\"1\":{\"561\":1}}],[\"预测图像对应的文本的词袋模型\",{\"1\":{\"413\":1}}],[\"预测图像与文本是否匹配\",{\"1\":{\"373\":1}}],[\"预测值和真实值都为\",{\"1\":{\"587\":1}}],[\"预测值\",{\"1\":{\"384\":1}}],[\"预测准确率\",{\"1\":{\"384\":1}}],[\"预测阶段\",{\"1\":{\"381\":1}}],[\"预测任务\",{\"1\":{\"251\":1}}],[\"预测所有\",{\"1\":{\"242\":1}}],[\"预测目标\",{\"1\":{\"216\":1,\"385\":1}}],[\"预测头\",{\"1\":{\"143\":1,\"384\":1}}],[\"预测掩码\",{\"1\":{\"107\":1}}],[\"预测结果可视化\",{\"1\":{\"107\":1}}],[\"预测结果\",{\"1\":{\"107\":1,\"699\":1}}],[\"预测点云的功能区域掩码\",{\"1\":{\"107\":1}}],[\"预测的\",{\"1\":{\"106\":1}}],[\"预测与\",{\"1\":{\"102\":1,\"586\":1}}],[\"预测\",{\"1\":{\"70\":1,\"83\":1,\"215\":1,\"248\":1,\"893\":1}}],[\"预测倾向训练集中已有的\",{\"1\":{\"48\":1}}],[\"预测能力\",{\"1\":{\"19\":1}}],[\"方式提供了对照方案\",{\"1\":{\"655\":1}}],[\"方案退火到\",{\"1\":{\"633\":1}}],[\"方面发挥了重要作用\",{\"1\":{\"323\":1}}],[\"方差必须\",{\"1\":{\"931\":1}}],[\"方差为\",{\"1\":{\"874\":1}}],[\"方差差异大且可能相关的情况\",{\"1\":{\"579\":1}}],[\"方差是\",{\"1\":{\"578\":1}}],[\"方差大\",{\"1\":{\"576\":1}}],[\"方差\",{\"1\":{\"293\":1,\"474\":1}}],[\"方程\",{\"1\":{\"235\":1}}],[\"方便后续处理\",{\"1\":{\"899\":1}}],[\"方便后续的计算和比较\",{\"1\":{\"410\":1}}],[\"方便加到\",{\"1\":{\"710\":1}}],[\"方便加到注意力分数\",{\"1\":{\"710\":1}}],[\"方便和下游任务对接\",{\"1\":{\"392\":1}}],[\"方便离散索引的推断\",{\"1\":{\"257\":1}}],[\"方便\",{\"1\":{\"83\":1,\"213\":1,\"384\":1}}],[\"方法获取输入图像对应的离散视觉\",{\"1\":{\"899\":1}}],[\"方法就是在\",{\"1\":{\"898\":1}}],[\"方法求导\",{\"1\":{\"816\":1}}],[\"方法会被优先调用\",{\"1\":{\"809\":1}}],[\"方法以支持数组索引\",{\"1\":{\"808\":1}}],[\"方法被调用时\",{\"1\":{\"805\":1}}],[\"方法与目标\",{\"1\":{\"669\":1}}],[\"方法的实例\",{\"1\":{\"809\":3}}],[\"方法的核心概括图\",{\"1\":{\"656\":1}}],[\"方法的特定任务的输入改写\",{\"1\":{\"626\":1}}],[\"方法处理variable\",{\"1\":{\"809\":1}}],[\"方法处理输入输出的变量封装\",{\"1\":{\"761\":1}}],[\"方法处理\",{\"1\":{\"633\":1}}],[\"方法来模拟函数行为\",{\"1\":{\"459\":1}}],[\"方法来完成与\",{\"1\":{\"380\":1}}],[\"方法中保存输入输出变量\",{\"1\":{\"779\":1}}],[\"方法中\",{\"1\":{\"420\":1,\"783\":1,\"809\":1}}],[\"方法中已开始尝试\",{\"1\":{\"168\":1}}],[\"方法可知\",{\"1\":{\"386\":1}}],[\"方法进行下游迁移学习\",{\"1\":{\"646\":1}}],[\"方法进行分析\",{\"1\":{\"380\":1}}],[\"方法进行训练的代码实现\",{\"1\":{\"187\":1}}],[\"方法和应用场景有显著区别\",{\"1\":{\"346\":1}}],[\"方法二\",{\"1\":{\"343\":1}}],[\"方法一\",{\"1\":{\"343\":1}}],[\"方法负责将图像裁剪为一系列固定大小的\",{\"1\":{\"266\":1}}],[\"方法简单且高效\",{\"1\":{\"225\":1}}],[\"方法在语言任务中取得成功\",{\"1\":{\"216\":1}}],[\"方法用于对输入图像进行编码\",{\"1\":{\"213\":1}}],[\"方法代码实现\",{\"1\":{\"208\":1}}],[\"方法实现从离散视觉token索引到图像的重建过程\",{\"1\":{\"899\":1}}],[\"方法实现\",{\"1\":{\"190\":1,\"256\":1,\"380\":1}}],[\"方法实现了点云的邻域查询和特征分组功能\",{\"1\":{\"119\":1}}],[\"方法参考\",{\"1\":{\"172\":1}}],[\"方法部分主要分为三个内容\",{\"1\":{\"170\":1}}],[\"方法依赖从网络抓取的图文对数据\",{\"1\":{\"167\":1}}],[\"方法虽然在多模态任务上取得进展\",{\"1\":{\"165\":1}}],[\"方法设计\",{\"1\":{\"109\":1}}],[\"方法创新\",{\"1\":{\"30\":1}}],[\"方法\",{\"0\":{\"21\":1,\"32\":1,\"77\":1,\"111\":1,\"126\":1,\"211\":1,\"221\":1,\"229\":1,\"270\":1,\"284\":1,\"302\":1,\"328\":1,\"340\":1,\"482\":1,\"640\":1,\"647\":1,\"656\":1,\"667\":1,\"885\":1},\"1\":{\"22\":1,\"106\":1,\"157\":1,\"159\":1,\"213\":1,\"227\":1,\"255\":1,\"282\":1,\"305\":3,\"371\":2,\"382\":2,\"383\":1,\"384\":1,\"494\":1,\"610\":1,\"801\":1,\"807\":1,\"808\":1,\"809\":8,\"899\":1,\"935\":1}}],[\"方向信息保留\",{\"1\":{\"710\":1}}],[\"方向信息已在上一步编码\",{\"1\":{\"710\":1}}],[\"方向分桶\",{\"1\":{\"710\":1}}],[\"方向为主\",{\"1\":{\"508\":1}}],[\"方向解释\",{\"1\":{\"274\":1}}],[\"方向\",{\"1\":{\"64\":1,\"258\":1,\"574\":1}}],[\"方向复盘\",{\"0\":{\"4\":1},\"1\":{\"4\":1}}],[\"方向研究\",{\"1\":{\"2\":1}}],[\"旨在帮助开发者提高应用程序的质量\",{\"1\":{\"834\":1}}],[\"旨在帮助开发者们快速构建基于大型语言模型的端到端应用程序或工作流程\",{\"1\":{\"831\":1}}],[\"旨在满足不同用户和应用场景的需求\",{\"1\":{\"823\":1}}],[\"旨在提升任务泛化能力\",{\"1\":{\"669\":1}}],[\"旨在提升模型在各种复杂环境下的可靠性\",{\"1\":{\"20\":1}}],[\"旨在将模型输出行为与人类意图对齐\",{\"1\":{\"655\":1}}],[\"旨在将图像中的每个像素分类为不同的语义类别\",{\"1\":{\"584\":1}}],[\"旨在缩小开源模型与商业多模态模型\",{\"1\":{\"322\":1}}],[\"旨在与大型语言模型\",{\"1\":{\"304\":1}}],[\"旨在解决当前视觉与视觉\",{\"1\":{\"295\":1}}],[\"旨在缓解\",{\"1\":{\"214\":1}}],[\"旨在实现语言引导下的\",{\"1\":{\"93\":1}}],[\"旨在推动\",{\"1\":{\"84\":1}}],[\"旨在为具身智能体建立感知与操作之间的联系\",{\"1\":{\"72\":1}}],[\"旨在通过系统性的实验评估不同超参数和数据规模对模型性能的影响\",{\"1\":{\"678\":1}}],[\"旨在通过任意指令定位物体上支持特定交互的\",{\"1\":{\"30\":1}}],[\"旨在通过挖掘物体的不变几何属性和潜在交互意图\",{\"1\":{\"29\":1}}],[\"旨在同时提升\",{\"1\":{\"19\":1}}],[\"来算梯度\",{\"1\":{\"959\":1}}],[\"来控制建模精度\",{\"1\":{\"951\":1}}],[\"来控制是否只用图像作为输入\",{\"1\":{\"382\":1}}],[\"来理解公式\",{\"1\":{\"950\":1}}],[\"来理解语言\",{\"1\":{\"822\":1}}],[\"来重建训练数据\",{\"1\":{\"947\":1}}],[\"来估计\",{\"1\":{\"945\":1}}],[\"来保证输出尺寸与输入一致\",{\"1\":{\"926\":1}}],[\"来近似\",{\"1\":{\"909\":1}}],[\"来进行条件\",{\"1\":{\"895\":1}}],[\"来熟悉模型中使用到的一些参数及其含义\",{\"1\":{\"892\":1}}],[\"来学习会引入误差\",{\"1\":{\"949\":1}}],[\"来学习文本与图像\",{\"1\":{\"887\":1}}],[\"来学习优化的策略\",{\"1\":{\"140\":1}}],[\"来建模文本和图像\",{\"1\":{\"885\":1}}],[\"来定义模型\",{\"1\":{\"952\":1}}],[\"来定义事件空间\",{\"1\":{\"847\":1}}],[\"来定位图像中的物体区域\",{\"1\":{\"388\":1}}],[\"来满足验证集效果\",{\"1\":{\"835\":1}}],[\"来解决任务\",{\"1\":{\"835\":1}}],[\"来替代子模型的训练调优\",{\"1\":{\"835\":1}}],[\"来开发基于大型语言模型的应用程序\",{\"1\":{\"831\":1}}],[\"来为序列的不同位置提供唯一的编码\",{\"1\":{\"706\":1}}],[\"来完成\",{\"1\":{\"663\":1}}],[\"来完成点云物体分割任务\",{\"1\":{\"156\":1}}],[\"来完成点云分类任务\",{\"1\":{\"155\":1}}],[\"来挑选具有敏感内容识别能力的标注者\",{\"1\":{\"656\":1}}],[\"来隐式学习任务\",{\"1\":{\"640\":1}}],[\"来指定\",{\"1\":{\"640\":1}}],[\"来限制语言模型的输出分布只有\",{\"1\":{\"635\":1}}],[\"来正则化\",{\"1\":{\"633\":1}}],[\"来正常获取一个批次的数据了\",{\"1\":{\"382\":1}}],[\"来最终得到复杂问题的结果\",{\"1\":{\"622\":1}}],[\"来引导llm展现出更好的推理能力\",{\"1\":{\"620\":1}}],[\"来间接训练神经网络中的一些密集层\",{\"1\":{\"610\":1}}],[\"来减少模型对于计算资源的需求的方法\",{\"1\":{\"607\":1}}],[\"来命名环境\",{\"1\":{\"550\":1}}],[\"来读取数据\",{\"1\":{\"545\":1}}],[\"来增加表达能力\",{\"1\":{\"500\":1}}],[\"来扩大逼近空间的容量\",{\"1\":{\"500\":1}}],[\"来接收和返回函数\",{\"1\":{\"449\":1}}],[\"来帮助大家梳理清楚vision\",{\"1\":{\"423\":1}}],[\"来预测mask的\",{\"1\":{\"393\":1}}],[\"来预训练\",{\"1\":{\"221\":1}}],[\"来计算textual\",{\"1\":{\"393\":1}}],[\"来说\",{\"1\":{\"357\":1,\"611\":1}}],[\"来训练模型\",{\"1\":{\"355\":1}}],[\"来适配不同类别\",{\"1\":{\"346\":1}}],[\"来适配任务目标\",{\"1\":{\"273\":1}}],[\"来生成图文结合的指令响应对\",{\"1\":{\"339\":1}}],[\"来实现大语言模型的控制\",{\"1\":{\"835\":1}}],[\"来实现数据的压缩\",{\"1\":{\"614\":1}}],[\"来实现\",{\"1\":{\"339\":1}}],[\"来实现全局信息建模\",{\"1\":{\"97\":1}}],[\"来对从\",{\"1\":{\"889\":1}}],[\"来对齐视觉和语言模型的特征\",{\"1\":{\"296\":1}}],[\"来对码本的状态参数进行缓慢更新\",{\"1\":{\"213\":1}}],[\"来稳定训练\",{\"1\":{\"293\":1}}],[\"来避免这两类崩溃\",{\"1\":{\"290\":1}}],[\"来避免对\",{\"1\":{\"274\":1}}],[\"来简化特征质量评估\",{\"1\":{\"286\":1}}],[\"来更新我们对未知变量\",{\"1\":{\"853\":1}}],[\"来更新的\",{\"1\":{\"283\":1}}],[\"来更新分组特征\",{\"1\":{\"97\":1}}],[\"来约束特征学习\",{\"1\":{\"282\":1}}],[\"来聚合特征\",{\"1\":{\"273\":1}}],[\"来汇聚所有\",{\"1\":{\"238\":1}}],[\"来作为掩码位置\",{\"1\":{\"234\":1}}],[\"来优化模型参数\",{\"1\":{\"232\":1}}],[\"来找最近中心\",{\"1\":{\"213\":1}}],[\"来表示整句话的语义\",{\"1\":{\"171\":1}}],[\"来表达各种线性代数运算\",{\"1\":{\"100\":1}}],[\"来模拟不同的采样密度\",{\"1\":{\"141\":1}}],[\"来模拟噪声和破坏\",{\"1\":{\"19\":1}}],[\"来构建局部区域点集\",{\"1\":{\"133\":1}}],[\"来自后验近似\",{\"1\":{\"950\":1}}],[\"来自先验分布\",{\"1\":{\"950\":1}}],[\"来自一个简单的分布\",{\"1\":{\"944\":1}}],[\"来自真实图像\",{\"1\":{\"895\":1}}],[\"来自图像\",{\"1\":{\"402\":1}}],[\"来自文本\",{\"1\":{\"402\":1}}],[\"来自视觉模型\",{\"1\":{\"398\":1}}],[\"来自之前训练好的\",{\"1\":{\"214\":1}}],[\"来自动量编码器\",{\"1\":{\"208\":1}}],[\"来自下一级的特征\",{\"1\":{\"142\":1}}],[\"来自论文的理论分析\",{\"1\":{\"157\":1}}],[\"来自论文\",{\"1\":{\"106\":1}}],[\"来自论文图3\",{\"0\":{\"91\":1}}],[\"来自\",{\"1\":{\"70\":2,\"91\":1,\"143\":1,\"208\":1,\"224\":2,\"235\":1,\"341\":1,\"401\":1,\"513\":1,\"709\":2,\"899\":1}}],[\"来源于真实\",{\"1\":{\"658\":1}}],[\"来源之二\",{\"1\":{\"69\":1}}],[\"来源之一\",{\"1\":{\"69\":1}}],[\"来源\",{\"1\":{\"69\":1}}],[\"来标记哪些多模态\",{\"1\":{\"67\":1}}],[\"来推断交互可能性\",{\"1\":{\"56\":1}}],[\"来提升系统效果\",{\"1\":{\"836\":1}}],[\"来提升应用性能\",{\"1\":{\"836\":1}}],[\"来提升模型在特定任务上的表现\",{\"1\":{\"830\":1}}],[\"来提升\",{\"1\":{\"20\":1}}],[\"研究不断融合新的方法\",{\"1\":{\"884\":1}}],[\"研究和应用\",{\"1\":{\"824\":1}}],[\"研究人员提出了一种新的模型架构\",{\"1\":{\"828\":1}}],[\"研究人员还在努力让计算机理解图像和文字\",{\"1\":{\"827\":1}}],[\"研究人员发现\",{\"1\":{\"822\":1}}],[\"研究人员不断尝试改进\",{\"1\":{\"822\":1}}],[\"研究揭示了模型性能提升的关键因素并非复杂结构改动\",{\"1\":{\"688\":1}}],[\"研究强调了预训练中设计选择和数据规模的重要性\",{\"1\":{\"677\":1}}],[\"研究团队定义了标注规则\",{\"1\":{\"658\":1}}],[\"研究团队开源模型权重\",{\"1\":{\"323\":1}}],[\"研究还探讨了数据污染问题\",{\"1\":{\"645\":1}}],[\"研究还提出了目前最大的3d功能数据集piadv2\",{\"1\":{\"29\":1}}],[\"研究结果对构建通用语言系统具有重要意义\",{\"1\":{\"643\":1}}],[\"研究意义\",{\"1\":{\"639\":1,\"646\":1}}],[\"研究视觉与语言预训练在多大程度上能够互相促进\",{\"1\":{\"377\":1}}],[\"研究背景与问题\",{\"1\":{\"296\":1,\"323\":1}}],[\"研究背景与动机\",{\"1\":{\"19\":1}}],[\"研究发现\",{\"1\":{\"288\":1,\"638\":1,\"678\":1}}],[\"研究者尝试用不同生成模型改善文本到图像的转换\",{\"1\":{\"884\":1}}],[\"研究者的设计意图\",{\"1\":{\"658\":1}}],[\"研究者将\",{\"1\":{\"376\":1}}],[\"研究者探索\",{\"1\":{\"280\":1}}],[\"研究者提出一个问题\",{\"1\":{\"280\":1}}],[\"研究者发现\",{\"1\":{\"220\":1}}],[\"研究者开始直接将可供性映射到\",{\"1\":{\"20\":1}}],[\"研究表明\",{\"1\":{\"30\":1,\"377\":1,\"642\":1,\"653\":1}}],[\"但作者认为\",{\"1\":{\"960\":1}}],[\"但作者明确指出\",{\"1\":{\"649\":1}}],[\"但直接拿这个误差来训练是不行的\",{\"1\":{\"959\":1}}],[\"但直接在像素上建模\",{\"1\":{\"250\":1}}],[\"但现在又多出了一个问题\",{\"1\":{\"958\":1}}],[\"但略过了不少实现细节\",{\"1\":{\"957\":1}}],[\"但还有另一种方式可以引入\",{\"1\":{\"951\":1}}],[\"但还没有明确说\",{\"1\":{\"83\":1}}],[\"但至少我们知道\",{\"1\":{\"949\":1}}],[\"但幸运的是\",{\"1\":{\"949\":1}}],[\"但模型输出的是\",{\"1\":{\"931\":1}}],[\"但神经网络输出的是无约束的实数\",{\"1\":{\"931\":1}}],[\"但神经网络的自适应基函数和分层结构使其\",{\"1\":{\"500\":1}}],[\"但神经网络的非线性基函数组合比传统多项式逼近更灵活\",{\"1\":{\"500\":1}}],[\"但神经网络的通用近似性质也被证明对于其他类型的激活函数\",{\"1\":{\"500\":1}}],[\"但参数未知\",{\"1\":{\"903\":1}}],[\"但留出空间继续生成图像后续内容\",{\"1\":{\"895\":1}}],[\"但问题来了\",{\"1\":{\"945\":1}}],[\"但问题就在于属于某个文本token位置处的预测结果向量中\",{\"1\":{\"892\":1}}],[\"但问题是\",{\"1\":{\"282\":1}}],[\"但包含\",{\"1\":{\"888\":1}}],[\"但包含大量噪声文本\",{\"1\":{\"167\":1}}],[\"但我们还需要将误差反向传播到一个从\",{\"1\":{\"946\":1}}],[\"但我们选择为每个\",{\"1\":{\"887\":1}}],[\"但我们发现以下几点对稳定训练尤其重要\",{\"1\":{\"886\":1}}],[\"但我们则使用\",{\"1\":{\"886\":1}}],[\"但我们更关注的是其通用能力\",{\"1\":{\"825\":1}}],[\"但生成图像中仍常见严重问题\",{\"1\":{\"884\":1}}],[\"但却是高维高斯的真实现象\",{\"1\":{\"875\":1}}],[\"但事实恰恰相反\",{\"1\":{\"875\":1}}],[\"但体积增加更快\",{\"1\":{\"873\":1}}],[\"但一般通过调用\",{\"1\":{\"835\":1}}],[\"但一般人还是玩不起\",{\"1\":{\"394\":1}}],[\"但面对未见过的输入时仍可能出现幻觉\",{\"1\":{\"830\":1}}],[\"但单一模型可能难以全面适应所有场景\",{\"1\":{\"828\":1}}],[\"但大型语言模型的出现再次让聊天机器人这种交互模式可以重新涌现\",{\"1\":{\"827\":1}}],[\"但大多数方法仅在低层像素上操作\",{\"1\":{\"210\":1}}],[\"但尚未到达\",{\"1\":{\"816\":1}}],[\"但从局部看\",{\"1\":{\"816\":1}}],[\"但需直观呈现计算图全貌以辅助调试与理解\",{\"1\":{\"815\":1}}],[\"但需要付出计算速度代价\",{\"1\":{\"288\":1}}],[\"但无法表达空间结构\",{\"1\":{\"963\":1}}],[\"但无法直接穿过一个采样操作的节点反向传播梯度\",{\"1\":{\"946\":1}}],[\"但无法替代弱引用在框架设计中的针对性优化\",{\"1\":{\"806\":1}}],[\"但无法定位具体交互部位\",{\"1\":{\"75\":1}}],[\"但实验表明去除nsp后性能反而提升\",{\"1\":{\"683\":1}}],[\"但实际训练所需的计算资源与较小模型相当\",{\"1\":{\"647\":1}}],[\"但实际上总体阳性概率只有\",{\"1\":{\"850\":1}}],[\"但实际上这行代码的意图是计算\",{\"1\":{\"589\":1}}],[\"但实际上\",{\"1\":{\"409\":1}}],[\"但实际上前向传播阶段的贡献来源于z\",{\"1\":{\"213\":1}}],[\"但实际只用了极少数\",{\"1\":{\"262\":1}}],[\"但因循环引用\",{\"1\":{\"806\":1}}],[\"但因批次大小可变\",{\"1\":{\"681\":1}}],[\"但因为\",{\"1\":{\"542\":1}}],[\"但每次掩码不同\",{\"1\":{\"681\":1}}],[\"但每个输出向量的维度由\",{\"1\":{\"528\":1}}],[\"但透明性与可靠性仍需优化\",{\"1\":{\"669\":1}}],[\"但低于gpt\",{\"1\":{\"669\":1}}],[\"但书籍\",{\"1\":{\"668\":1}}],[\"但宗教类别偏差显著\",{\"1\":{\"668\":1}}],[\"但远低于minerva\",{\"1\":{\"668\":1}}],[\"但远低于人类\",{\"1\":{\"641\":1}}],[\"但小模型在长期训练后推理效率更高\",{\"1\":{\"666\":1}}],[\"但hoffmann等人\",{\"1\":{\"666\":1}}],[\"但通用性更强\",{\"1\":{\"683\":1}}],[\"但通过优化关键设计选择提升性能\",{\"1\":{\"679\":1}}],[\"但通过高效训练仍实现sota\",{\"1\":{\"666\":1}}],[\"但通过引入预训练梯度混合\",{\"1\":{\"658\":1}}],[\"但通过在\",{\"1\":{\"657\":1}}],[\"但通常不开源\",{\"1\":{\"325\":1}}],[\"但未具内置限制\",{\"1\":{\"657\":1}}],[\"但强调这种对齐是相对于特定人群\",{\"1\":{\"654\":1}}],[\"但gpt\",{\"1\":{\"650\":1}}],[\"但有短板\",{\"1\":{\"648\":1}}],[\"但有效性有限\",{\"1\":{\"248\":1}}],[\"但采用稀疏注意力机制\",{\"1\":{\"647\":1}}],[\"但采用图文检索和图文匹配作为训练目标\",{\"1\":{\"190\":1}}],[\"但泛化能力弱\",{\"1\":{\"647\":1}}],[\"但泛化性受限\",{\"1\":{\"73\":1}}],[\"但某些任务仍存在挑战\",{\"1\":{\"645\":1}}],[\"但数据重叠对结果影响有限\",{\"1\":{\"641\":1}}],[\"但数目固定\",{\"1\":{\"501\":1}}],[\"但对于rgb\",{\"1\":{\"924\":1}}],[\"但对于如wic\",{\"1\":{\"648\":1}}],[\"但对于对外提供服务的企业来说\",{\"1\":{\"601\":1}}],[\"但对高置信度预测\",{\"1\":{\"641\":1}}],[\"但定性分析显示生成内容类似摘要\",{\"1\":{\"641\":1}}],[\"但层数\",{\"1\":{\"641\":1}}],[\"但进行了以下优化\",{\"1\":{\"640\":1}}],[\"但结果表明\",{\"1\":{\"651\":1}}],[\"但结果表明语言模型在无监督条件下已具备初步的多任务处理能力\",{\"1\":{\"639\":1}}],[\"但结果与人类表现\",{\"1\":{\"641\":1}}],[\"但结果会更加靠谱\",{\"1\":{\"619\":1}}],[\"但依赖大量任务标注数据\",{\"1\":{\"650\":1}}],[\"但依赖于大量标注数据和监督学习\",{\"1\":{\"639\":1}}],[\"但依赖切平面估计\",{\"1\":{\"110\":1}}],[\"但并不能保证结果的正确性\",{\"1\":{\"619\":1}}],[\"但并不是所有的参数都是发挥同样作用的\",{\"1\":{\"606\":1}}],[\"但起核心作用的参数是低秩的\",{\"1\":{\"614\":1}}],[\"但也可能会把原来表现好的别的领域的能力变差\",{\"1\":{\"602\":1}}],[\"但也不能写成\",{\"1\":{\"472\":1}}],[\"但fft也会带来一些问题\",{\"1\":{\"602\":1}}],[\"但真实是正类的样本数\",{\"1\":{\"590\":1}}],[\"但真实是负类的样本数\",{\"1\":{\"590\":1}}],[\"但保留作为接口兼容\",{\"1\":{\"592\":1}}],[\"但保留它们作为接口兼容\",{\"1\":{\"590\":1}}],[\"但保留接口以备后续扩展\",{\"1\":{\"586\":1}}],[\"但保持语义一致\",{\"1\":{\"264\":1}}],[\"但更易梯度下降\",{\"1\":{\"586\":1}}],[\"但考虑变量相关性\",{\"1\":{\"574\":1}}],[\"但物理内存中数据不复制\",{\"1\":{\"546\":1}}],[\"但要获得准确的估计\",{\"1\":{\"946\":1}}],[\"但要创建完整的应用程序\",{\"1\":{\"831\":1}}],[\"但要注意的是\",{\"1\":{\"545\":1}}],[\"但要想在\",{\"1\":{\"280\":1}}],[\"但此时并没有复制任何数据\",{\"1\":{\"544\":1}}],[\"但长度固定为\",{\"1\":{\"508\":1}}],[\"但缩放到总和为\",{\"1\":{\"507\":1}}],[\"但带来了\",{\"1\":{\"500\":1}}],[\"但显式地调用可以避免难以调试的运行时错误\",{\"1\":{\"494\":1}}],[\"但统一的最佳实践是\",{\"1\":{\"493\":1}}],[\"但为保障跨平台一致性和最佳性能\",{\"1\":{\"493\":1}}],[\"但为了与前人工作保持一致\",{\"1\":{\"286\":1}}],[\"但当\",{\"1\":{\"951\":1}}],[\"但当维度\",{\"1\":{\"874\":1}}],[\"但当张量的内存布局不连续\",{\"1\":{\"470\":1}}],[\"但当前实现未使用\",{\"1\":{\"385\":1}}],[\"但如果\",{\"1\":{\"951\":1}}],[\"但如果我们在内部函数中引用了外部函数的变量\",{\"1\":{\"448\":1}}],[\"但如果训练时没有加入扰动\",{\"1\":{\"157\":1}}],[\"但默认的组合方式可能不满足所有需求\",{\"1\":{\"424\":1}}],[\"但发现这种方法的训练效率\",{\"1\":{\"413\":1}}],[\"但存在词汇表限制或效率问题\",{\"1\":{\"640\":1}}],[\"但存在两大局限\",{\"1\":{\"500\":1}}],[\"但存在一定的噪声\",{\"1\":{\"413\":1}}],[\"但存在以下局限性\",{\"1\":{\"30\":1}}],[\"但仍有限制\",{\"1\":{\"917\":1}}],[\"但仍在胡说\",{\"1\":{\"657\":1}}],[\"但仍依赖卷积架构\",{\"1\":{\"388\":1}}],[\"但仍能达到较高水平\",{\"1\":{\"292\":1}}],[\"但计算复杂且处理速度较慢\",{\"1\":{\"388\":1}}],[\"但计算和内存开销大\",{\"1\":{\"109\":1}}],[\"但下游任务的准确率较低\",{\"1\":{\"376\":1}}],[\"但提升幅度不大\",{\"1\":{\"354\":1}}],[\"但已经可以处理基本的图文问答任务\",{\"1\":{\"341\":1}}],[\"但已有方法大多借鉴二维的编码方式\",{\"1\":{\"125\":1}}],[\"但较gpt\",{\"1\":{\"335\":1}}],[\"但internvit仍展现出与新语言模型的优秀兼容性和可移植性\",{\"1\":{\"330\":1}}],[\"但开源模型通过高分辨率优化\",{\"1\":{\"327\":1}}],[\"但开源模型在文档\",{\"1\":{\"326\":1}}],[\"但受限于视觉模型的规模和对齐效率\",{\"1\":{\"300\":1}}],[\"但受限于预定义类别\",{\"1\":{\"31\":1}}],[\"但会推动输出趋向均匀\",{\"1\":{\"290\":1}}],[\"但会牺牲吞吐率\",{\"1\":{\"288\":1}}],[\"但会导致码本使用率下降\",{\"1\":{\"215\":1}}],[\"但风险是过度集中\",{\"1\":{\"285\":1}}],[\"但容易导致均匀分布坍塌\",{\"1\":{\"285\":1}}],[\"但其通用性优势使其成为最终选择\",{\"1\":{\"681\":1}}],[\"但其本质仍是一个\",{\"1\":{\"649\":1}}],[\"但其在自然语言处理\",{\"1\":{\"639\":1}}],[\"但其主要目的是训练可迁移的视觉模型\",{\"1\":{\"407\":1}}],[\"但其主要负责将输入的\",{\"1\":{\"382\":1}}],[\"但其核心思想借鉴了\",{\"1\":{\"285\":1}}],[\"但其仍存在一些不足\",{\"1\":{\"125\":1}}],[\"但不知道具体哪个潜变量设置产生了它\",{\"1\":{\"943\":1}}],[\"但不允许看到未来像素\",{\"1\":{\"926\":1}}],[\"但不包括其标题\",{\"1\":{\"888\":1}}],[\"但不会跑出系统之外\",{\"1\":{\"847\":1}}],[\"但不会被视为可训练参数\",{\"1\":{\"474\":1}}],[\"但不一定更公正\",{\"1\":{\"657\":1}}],[\"但不占用额外内存\",{\"1\":{\"472\":1}}],[\"但不监督与其索引相同的学生视图\",{\"1\":{\"293\":1}}],[\"但不同于它们使用结构明确的元任务\",{\"1\":{\"650\":1}}],[\"但不同的是\",{\"1\":{\"283\":1}}],[\"但不同点在于\",{\"1\":{\"282\":1}}],[\"但不适合表征学习\",{\"1\":{\"247\":1}}],[\"但性能远低于微调方法\",{\"1\":{\"646\":1}}],[\"但性能下降\",{\"1\":{\"282\":1}}],[\"但性能全面超越\",{\"1\":{\"178\":1}}],[\"但与此同时\",{\"1\":{\"873\":1}}],[\"但与\",{\"1\":{\"280\":1}}],[\"但同时尽量保留论文中的全部信息\",{\"1\":{\"271\":1}}],[\"但同时又希望这个采样过程能反向传播梯度\",{\"1\":{\"258\":1}}],[\"但同一物体的不同实例可能有几何差异\",{\"1\":{\"53\":1}}],[\"但梯度仍由软样本近似\",{\"1\":{\"257\":1}}],[\"但\",{\"1\":{\"228\":1,\"293\":1,\"588\":1,\"656\":1,\"679\":1}}],[\"但视觉和视觉\",{\"1\":{\"296\":1}}],[\"但视觉和文本的token未对齐\",{\"1\":{\"194\":1}}],[\"但视觉\",{\"1\":{\"228\":1}}],[\"但仅使用\",{\"1\":{\"220\":1}}],[\"但标准\",{\"1\":{\"202\":1}}],[\"但难以处理复杂语义交互\",{\"1\":{\"195\":1}}],[\"但难以精确定位交互点位\",{\"1\":{\"20\":1}}],[\"但文本\",{\"1\":{\"172\":1}}],[\"但多用于低资源语言场景\",{\"1\":{\"169\":1}}],[\"但语言任务的数据增强较困难\",{\"1\":{\"169\":1}}],[\"但普遍存在两个问题\",{\"1\":{\"165\":1}}],[\"但可以作为判断模型是否捕捉到了某个特定数据点\",{\"1\":{\"947\":1}}],[\"但可以通过对\",{\"1\":{\"947\":1}}],[\"但可能无法充分定制模型行为或写作风格\",{\"1\":{\"830\":1}}],[\"但可能会出错\",{\"1\":{\"619\":1}}],[\"但可能效果受限\",{\"1\":{\"385\":1}}],[\"但可用于特定任务\",{\"1\":{\"160\":1}}],[\"但可供性类别相同\",{\"1\":{\"44\":1}}],[\"但可供性学习要求在\",{\"1\":{\"20\":1}}],[\"但这部分工作尚留待未来完成\",{\"1\":{\"949\":1}}],[\"但这样也会排除相似的好样本\",{\"1\":{\"944\":1}}],[\"但这会导致图3\",{\"1\":{\"944\":1}}],[\"但这会导致扩展到大规模数据时效率低\",{\"1\":{\"220\":1}}],[\"但这仍然限制了few\",{\"1\":{\"649\":1}}],[\"但这一方向为减少对人工标注数据的依赖提供了新思路\",{\"1\":{\"643\":1}}],[\"但这时\",{\"1\":{\"518\":1}}],[\"但这些信息不需要训练\",{\"1\":{\"474\":1}}],[\"但这些模型仍然采用固定类别的softmax分类器进行预训练\",{\"1\":{\"413\":1}}],[\"但这些方法的局限在于\",{\"1\":{\"269\":1}}],[\"但这次切换成\",{\"1\":{\"385\":1}}],[\"但这里保留\",{\"1\":{\"384\":1}}],[\"但这两种方法对超参数较为敏感\",{\"1\":{\"286\":1}}],[\"但这种方法存在三个主要限制\",{\"1\":{\"296\":1}}],[\"但这种方法要求一次性比较大量样本\",{\"1\":{\"282\":1}}],[\"但这种方式表达能力有限\",{\"1\":{\"157\":1}}],[\"但这可能导致所选邻域的实际尺寸随点的密度变化而变化\",{\"1\":{\"135\":1}}],[\"但遇到遮挡严重或点分布不均匀时性能下降明显\",{\"1\":{\"157\":1}}],[\"但由于\",{\"1\":{\"885\":1}}],[\"但由于它是固定的\",{\"1\":{\"706\":1}}],[\"但由于它们是神经网络直接预测出来的\",{\"1\":{\"153\":1}}],[\"但由于其无序性和非规则性\",{\"1\":{\"148\":1}}],[\"但在某些模型中\",{\"1\":{\"951\":1}}],[\"但在公式\",{\"1\":{\"946\":1}}],[\"但在复杂视觉任务中\",{\"1\":{\"944\":1}}],[\"但在高维空间中\",{\"1\":{\"944\":1}}],[\"但在分布外的标题上表现更好\",{\"1\":{\"887\":1}}],[\"但在最后一个文本\",{\"1\":{\"887\":1}}],[\"但在大型模型中特别突出\",{\"1\":{\"825\":1}}],[\"但在理解复杂语言规则方面存在一定局限性\",{\"1\":{\"822\":1}}],[\"但在实际应用中\",{\"1\":{\"807\":1}}],[\"但在对内存敏感的场景下依然存在问题\",{\"1\":{\"806\":1}}],[\"但在训练中\",{\"1\":{\"706\":1}}],[\"但在强化学习微调中加入\",{\"1\":{\"655\":1}}],[\"但在其他任务\",{\"1\":{\"649\":1}}],[\"但在如wic\",{\"1\":{\"648\":1}}],[\"但在结构化或需要多步推理的任务中\",{\"1\":{\"648\":1}}],[\"但在摘要\",{\"1\":{\"642\":1}}],[\"但在前景远少于背景时容易偏向负样本\",{\"1\":{\"587\":1}}],[\"但在迁移过程中并没有使用任何下游任务的监督样本\",{\"1\":{\"273\":1}}],[\"但在\",{\"1\":{\"235\":1,\"649\":1}}],[\"但在处理超大规模点云时\",{\"1\":{\"157\":1}}],[\"但在精度上仍略逊一筹\",{\"1\":{\"157\":1}}],[\"但在一些复杂区域\",{\"1\":{\"157\":1}}],[\"但在点云这种非结构化数据中\",{\"1\":{\"145\":1}}],[\"但在计算上可能非常昂贵\",{\"1\":{\"142\":1}}],[\"但在测试集中保留\",{\"1\":{\"89\":1}}],[\"但是问题是\",{\"1\":{\"847\":1}}],[\"但是越清晰\",{\"1\":{\"836\":1}}],[\"但是发展速度相当惊人\",{\"1\":{\"823\":1}}],[\"但是这里为了方便理解\",{\"1\":{\"805\":1}}],[\"但是这一改动也引发了另一个问题\",{\"1\":{\"804\":1}}],[\"但是这样造成的一个后果是计算量太庞大\",{\"1\":{\"426\":1}}],[\"但是这样做的逻辑不太清晰\",{\"1\":{\"355\":1}}],[\"但是这样做\",{\"1\":{\"355\":1}}],[\"但是和rnn相比\",{\"1\":{\"740\":1}}],[\"但是和transformer原始的encoder还是有所区别\",{\"1\":{\"429\":1}}],[\"但是也存在一些不足之处\",{\"1\":{\"707\":1}}],[\"但是反馈的来源是ai\",{\"1\":{\"602\":1}}],[\"但是它的缺点也非常明显\",{\"1\":{\"601\":1}}],[\"但是\",{\"1\":{\"474\":1,\"568\":1,\"616\":1,\"694\":2,\"846\":1,\"921\":1,\"956\":1,\"958\":1}}],[\"但是训练速度还是挺快的\",{\"1\":{\"435\":1}}],[\"但是训练速度很慢\",{\"1\":{\"394\":1}}],[\"但是当数据量逐渐增大时\",{\"1\":{\"432\":1}}],[\"但是当训练数据集不够大的时候\",{\"1\":{\"422\":1}}],[\"但是迁移到其它数据集训练时\",{\"1\":{\"431\":1}}],[\"但是实际的代码实现中\",{\"1\":{\"426\":1}}],[\"但是对于一般的小公司或者个人来说\",{\"1\":{\"610\":1}}],[\"但是对于vit这个结构而言\",{\"1\":{\"425\":1}}],[\"但是对于cv领域来讲\",{\"1\":{\"353\":1}}],[\"但是visual\",{\"1\":{\"391\":1}}],[\"但是由于ve仍然使用重的卷积网络进行特征抽取\",{\"1\":{\"390\":1}}],[\"但是由于这两张图片是从同一个图片经过某种变化得到的\",{\"1\":{\"350\":1}}],[\"但是其本质还是借助\",{\"1\":{\"381\":1}}],[\"但是表现都会比有监督要差\",{\"1\":{\"353\":1}}],[\"但是在cv领域\",{\"1\":{\"353\":1}}],[\"但是在一个场景中有多个物体时则不好办\",{\"1\":{\"131\":1}}],[\"但是我们在计算损失时指定了ignore\",{\"1\":{\"700\":1}}],[\"但是我们每次更新这个队列\",{\"1\":{\"353\":1}}],[\"但是我们用到了另外一种信息\",{\"1\":{\"349\":1}}],[\"但是我们需要知道前两张图片是一个类别\",{\"1\":{\"349\":1}}],[\"但是官方仓库的issue给出了明确答复\",{\"1\":{\"185\":1}}],[\"但是尺度不同\",{\"1\":{\"141\":1}}],[\"但是作用不完全相同\",{\"1\":{\"119\":1}}],[\"但专为点云设计\",{\"1\":{\"122\":1}}],[\"但专门为无序\",{\"1\":{\"121\":1}}],[\"但专门为点云数据设计\",{\"1\":{\"121\":1,\"122\":2}}],[\"但它的数学基础与稀疏自编码器或去噪自编码器并不相同\",{\"1\":{\"944\":1}}],[\"但它能为后续许多针对转置数据的操作带来更好的内存局部性\",{\"1\":{\"545\":1}}],[\"但它只去掉\",{\"1\":{\"480\":1}}],[\"但它会让数据本身有\",{\"1\":{\"382\":1}}],[\"但它们也引发了伦理和风险问题\",{\"1\":{\"824\":1}}],[\"但它们展现出截然不同的能力\",{\"1\":{\"822\":1}}],[\"但它们常常偏离用户意图\",{\"1\":{\"654\":1}}],[\"但它们的\",{\"1\":{\"468\":1}}],[\"但它们的目标\",{\"1\":{\"346\":1}}],[\"但它们在非互联网图像\",{\"1\":{\"327\":1}}],[\"但它们存在复杂的训练需求\",{\"1\":{\"269\":1}}],[\"但它们与图像不同\",{\"1\":{\"109\":1}}],[\"但它也为后续模型奠定了基础\",{\"1\":{\"157\":1}}],[\"但它是为了统一接口设计的一个占位符\",{\"1\":{\"137\":1}}],[\"但它本质上是\",{\"1\":{\"102\":1}}],[\"但完全不使用注意力机制\",{\"1\":{\"97\":1}}],[\"但测试时要求\",{\"1\":{\"89\":1}}],[\"但效率低\",{\"1\":{\"75\":1}}],[\"但搜索空间大\",{\"1\":{\"73\":1}}],[\"但动态功能特性使得mllms难以直接从交互图像推理3d功能\",{\"1\":{\"31\":1}}],[\"但机器人操作需要3d信息\",{\"1\":{\"31\":1}}],[\"但先在每个尺度进行对齐\",{\"1\":{\"23\":1}}],[\"但缺乏对物体可供性相关部分的精确定位\",{\"1\":{\"20\":1}}],[\"因批次更大\",{\"1\":{\"684\":1}}],[\"因文章复用段落\",{\"1\":{\"641\":1}}],[\"因基函数\",{\"1\":{\"500\":1}}],[\"因数据量扩大\",{\"1\":{\"317\":1}}],[\"因果注意力\",{\"1\":{\"179\":1}}],[\"因其在编码\",{\"1\":{\"179\":1}}],[\"因为训练目标通常会惩罚预测结果与真实结果之间的距离\",{\"1\":{\"952\":1}}],[\"因为训练数据集中的文本\",{\"1\":{\"413\":1}}],[\"因为真实的\",{\"1\":{\"947\":1}}],[\"因为在这种情况下\",{\"1\":{\"946\":1}}],[\"因为在代码中有个冻结权重的操作\",{\"1\":{\"435\":1}}],[\"因为任意复杂分布都可以通过把简单分布输入到一个足够复杂的函数中得到\",{\"1\":{\"944\":1}}],[\"因为知道某个图像不太可能出现\",{\"1\":{\"942\":1}}],[\"因为随机采样不可微\",{\"1\":{\"931\":1}}],[\"因为其距离并不小\",{\"1\":{\"944\":1}}],[\"因为其不参与next\",{\"1\":{\"893\":1}}],[\"因为其概率密度随着与中心的平方距离增大而指数级衰减\",{\"1\":{\"867\":1}}],[\"因为为每个\",{\"1\":{\"892\":1}}],[\"因为概率质量\",{\"1\":{\"873\":1}}],[\"因为当\",{\"1\":{\"867\":1}}],[\"因为大多数人根本没病\",{\"1\":{\"850\":1}}],[\"因为大模型的参数量非常大\",{\"1\":{\"601\":1}}],[\"因为某些集合太\",{\"1\":{\"847\":1}}],[\"因为cuda不支持macos\",{\"1\":{\"739\":1}}],[\"因为我们的模型假设在测试时\",{\"1\":{\"952\":1}}],[\"因为我们要预测下一个\",{\"1\":{\"420\":1}}],[\"因为我只是为了了解内部代码情况\",{\"1\":{\"712\":1}}],[\"因为正弦和余弦函数都是周期函数\",{\"1\":{\"706\":1}}],[\"因为正样本和负样本都是相对于锚点来说的\",{\"1\":{\"353\":1}}],[\"因为低频词出现次数极少\",{\"1\":{\"697\":1}}],[\"因为更大容量的模型能吸收更多任务相关的模式\",{\"1\":{\"646\":1}}],[\"因为存在各种变化现象\",{\"1\":{\"634\":1}}],[\"因为作者的预训练模型是用连续的文本序列训练的\",{\"1\":{\"631\":1}}],[\"因为llm的prompt长度通常都是有长度限制的\",{\"1\":{\"620\":1}}],[\"因为收集人类反馈\",{\"1\":{\"602\":1}}],[\"因为微调的参数量跟预训练的是一样的多的\",{\"1\":{\"602\":1}}],[\"因为推理成本是跟prompt长度的平方正向相关的\",{\"1\":{\"601\":1}}],[\"因为通常大模型的实现原理\",{\"1\":{\"601\":1}}],[\"因为要最小化损失\",{\"1\":{\"592\":1}}],[\"因为每个头只能根据分配给自己的这部分特征\",{\"1\":{\"582\":1}}],[\"因为不同位置的编码由不同的正弦和余弦值组成\",{\"1\":{\"706\":1}}],[\"因为不会引入新的中间值\",{\"1\":{\"504\":1}}],[\"因为不需要保存中间变量用于反向传播\",{\"1\":{\"473\":1}}],[\"因为第\",{\"1\":{\"472\":1}}],[\"因为transformer和cnn相比缺少归纳偏置\",{\"1\":{\"422\":1}}],[\"因为都是有效\",{\"1\":{\"420\":1}}],[\"因为数据集的bias\",{\"1\":{\"394\":1}}],[\"因为dual\",{\"1\":{\"391\":1}}],[\"因为你这个更新操作\",{\"1\":{\"357\":1}}],[\"因为到后来第一个分支和第二个分支编码器差距越来越大\",{\"1\":{\"356\":1}}],[\"因为如果这样做了\",{\"1\":{\"356\":1}}],[\"因为如果不这样做\",{\"1\":{\"353\":1}}],[\"因为先进先出\",{\"1\":{\"356\":1}}],[\"因为有\",{\"1\":{\"355\":1}}],[\"因为直接在超大类别空间上做\",{\"1\":{\"355\":1}}],[\"因为单向语言模型在完整句子上用因果掩码训练\",{\"1\":{\"272\":1}}],[\"因为这个假设使得产生\",{\"1\":{\"903\":1}}],[\"因为这里是自监督学习\",{\"1\":{\"293\":1}}],[\"因为这类任务需要区域级别的特征\",{\"1\":{\"272\":1}}],[\"因为这些模型能够快速迁移到各种下游任务上\",{\"1\":{\"268\":1}}],[\"因为梯度变化更平滑\",{\"1\":{\"259\":1}}],[\"因为模型会尽可能利用第\",{\"1\":{\"214\":1}}],[\"因为后面要用\",{\"1\":{\"213\":1}}],[\"因为后面有bn\",{\"1\":{\"120\":1,\"121\":1}}],[\"因为向量一般做过\",{\"1\":{\"213\":1}}],[\"因为前面有\",{\"1\":{\"137\":1}}],[\"因为学习到的特征和权重可以在多个局部区域中复用\",{\"1\":{\"131\":1}}],[\"因为\",{\"1\":{\"131\":1,\"351\":1,\"357\":1,\"380\":1,\"574\":1,\"589\":1,\"735\":1,\"809\":2,\"877\":1,\"885\":1,\"895\":1,\"945\":1,\"947\":1,\"951\":1}}],[\"因为expansion=1\",{\"1\":{\"120\":1}}],[\"因为它可以根据向量来生成图片\",{\"1\":{\"956\":1}}],[\"因为它可以明确地定义如何计算公式右边的两项\",{\"1\":{\"946\":1}}],[\"因为它可以单独控制均值与方差\",{\"1\":{\"863\":1}}],[\"因为它只是一个与\",{\"1\":{\"877\":1}}],[\"因为它是通过对未知量\",{\"1\":{\"877\":1}}],[\"因为它的和不一定为\",{\"1\":{\"877\":1}}],[\"因为它衡量的是模型正确识别所有正例实例的能力\",{\"1\":{\"563\":1}}],[\"因为它们对模型的适应性至关重要\",{\"1\":{\"614\":1}}],[\"因为它们的作用不同\",{\"1\":{\"536\":1}}],[\"因为它们只改变了视图\",{\"1\":{\"490\":1}}],[\"因为它集合了大量常见与前沿的图像模型\",{\"1\":{\"510\":1}}],[\"因为它和前面的\",{\"1\":{\"480\":1}}],[\"因为它访问了其外部作用域的变量\",{\"1\":{\"449\":1}}],[\"因为它接收\",{\"1\":{\"449\":1}}],[\"因为它引用了\",{\"1\":{\"448\":1}}],[\"因为它认为每个图片自成一个类别\",{\"1\":{\"350\":1}}],[\"因为它能够在更低层次上递归地检视更高分辨率\",{\"1\":{\"142\":1}}],[\"因为它对输入的排列和数量不敏感\",{\"1\":{\"109\":1}}],[\"因为它提供了高质量的点云和功能标注\",{\"1\":{\"86\":1}}],[\"因为背景区域大小等于特征图大小\",{\"1\":{\"83\":1}}],[\"因为需要精确的空间和深度信息\",{\"1\":{\"20\":1}}],[\"因此本节旨在更深入地理解这个目标函数究竟在做什么\",{\"1\":{\"948\":1}}],[\"因此对\",{\"1\":{\"945\":1}}],[\"因此对于存在多分支的复杂计算图而言\",{\"1\":{\"804\":1}}],[\"因此才有了transformer可以一次性预测出每个位置对应的next\",{\"1\":{\"892\":1}}],[\"因此rosenbrock函数也被称为\",{\"1\":{\"816\":1}}],[\"因此反向传播时需将上游梯度gy分别乘以1和\",{\"1\":{\"809\":1}}],[\"因此需调用x\",{\"1\":{\"809\":1}}],[\"因此需要大家自行完成运行时缺失依赖包的安装\",{\"1\":{\"712\":1}}],[\"因此需要探索更高效的学习范式\",{\"1\":{\"639\":1}}],[\"因此需要更多得视觉部分\",{\"1\":{\"394\":1}}],[\"因此需要\",{\"1\":{\"282\":1}}],[\"因此需要使用像素级别的损失函数\",{\"1\":{\"584\":1}}],[\"因此需要使用\",{\"1\":{\"119\":1}}],[\"因此func列表同一时刻最多只存在一个func\",{\"1\":{\"787\":1}}],[\"因此出现了extended\",{\"1\":{\"740\":1}}],[\"因此只需要二维标量表\",{\"1\":{\"710\":1}}],[\"因此不能只用一个全局向量\",{\"1\":{\"963\":1}}],[\"因此不会影响最终的损失值计算\",{\"1\":{\"700\":1}}],[\"因此不论张量是多少维的\",{\"1\":{\"540\":1}}],[\"因此为了确保masked\",{\"1\":{\"700\":1}}],[\"因此模型返回的logits\",{\"1\":{\"700\":1}}],[\"因此此时的预测搞不好是对的\",{\"1\":{\"694\":1}}],[\"因此首轮推理过程需要完成\",{\"1\":{\"663\":1}}],[\"因此第\",{\"1\":{\"660\":1}}],[\"因此无法像微调方法那样从结构化监督中持续优化\",{\"1\":{\"649\":1}}],[\"因此无法直接应用于复杂的多模态理解任务\",{\"1\":{\"268\":1}}],[\"因此论文探索如何让语言模型具备类似的少样本学习能力\",{\"1\":{\"646\":1}}],[\"因此sst属于单个句子的文本分类任务\",{\"1\":{\"634\":1}}],[\"因此cola属于单个句子的文本二分类任务\",{\"1\":{\"634\":1}}],[\"因此后面\",{\"1\":{\"612\":1}}],[\"因此语义分割可以提供更详细和准确的图像分析结果\",{\"1\":{\"584\":1}}],[\"因此我们可以安全地将梯度符号移入期望中而不会影响等式成立\",{\"1\":{\"946\":1}}],[\"因此我们可以往队列里放入很多负样本\",{\"1\":{\"352\":1}}],[\"因此我们的训练数据中包含了一部分\",{\"1\":{\"888\":1}}],[\"因此我们扩展\",{\"1\":{\"800\":1}}],[\"因此我们只需要根据is\",{\"1\":{\"700\":1}}],[\"因此我们重点关注每个头对应的注意力权重矩阵上\",{\"1\":{\"582\":1}}],[\"因此精确率为\",{\"1\":{\"565\":1}}],[\"因此其假正例率为\",{\"1\":{\"564\":1}}],[\"因此其召回率\",{\"1\":{\"563\":1}}],[\"因此会出现在分母中\",{\"1\":{\"563\":1,\"564\":1}}],[\"因此准确率为\",{\"1\":{\"562\":1}}],[\"因此步长为\",{\"1\":{\"545\":1}}],[\"因此叫\",{\"1\":{\"505\":1}}],[\"因此使用四邻域双线性插值从特征图中获取精确的\",{\"1\":{\"502\":1}}],[\"因此是自注意力\",{\"1\":{\"430\":1}}],[\"因此可以移出期望符号\",{\"1\":{\"945\":1}}],[\"因此可以充分利用计算资源\",{\"1\":{\"740\":1}}],[\"因此可以堆叠多个block\",{\"1\":{\"429\":1}}],[\"因此可能比第二个向量更不可靠\",{\"1\":{\"142\":1}}],[\"因此它有效地充当信息瓶颈\",{\"1\":{\"421\":1}}],[\"因此成本较高\",{\"1\":{\"413\":1}}],[\"因此这个公式可以简化为\",{\"1\":{\"946\":1}}],[\"因此这是一个非常庞大的数据集\",{\"1\":{\"407\":1}}],[\"因此这里就不再给出数据集下载链接了\",{\"1\":{\"410\":1}}],[\"因此这里就不再过多展开\",{\"1\":{\"214\":1}}],[\"因此这里构造\",{\"1\":{\"190\":1}}],[\"因此如果研究过\",{\"1\":{\"379\":1}}],[\"因此直接移除了最后三层的权重\",{\"1\":{\"330\":1}}],[\"因此大大简化了特征评估的流程\",{\"1\":{\"286\":1}}],[\"因此整个系统bn\",{\"1\":{\"285\":1}}],[\"因此学生和教师网络架构完全相同\",{\"1\":{\"285\":1}}],[\"因此自然适用于自回归式的生成目标\",{\"1\":{\"272\":1}}],[\"因此自注意力特别适合点云\",{\"1\":{\"113\":1}}],[\"因此通常遵循\",{\"1\":{\"264\":1}}],[\"因此通过\",{\"1\":{\"22\":1}}],[\"因此比像素级别遮挡更有效和实用\",{\"1\":{\"263\":1}}],[\"因此图像会被划分为\",{\"1\":{\"236\":1}}],[\"因此采用\",{\"1\":{\"232\":1}}],[\"因此存在较大的噪声\",{\"1\":{\"202\":1}}],[\"因此正负样本是\",{\"1\":{\"190\":1}}],[\"因此在\",{\"1\":{\"947\":1}}],[\"因此在建模局部依赖关系时能力相对不足\",{\"1\":{\"706\":1}}],[\"因此在类别数量相近且平衡的数据集的情况下\",{\"1\":{\"562\":1}}],[\"因此在新的数据集上需要定义新的分类器来重新训练\",{\"1\":{\"413\":1}}],[\"因此在效果上可能不如使用\",{\"1\":{\"409\":1}}],[\"因此在处理高分辨率图像或非互联网来源图像\",{\"1\":{\"330\":1}}],[\"因此在跨模态对齐与检索方面不足\",{\"1\":{\"268\":1}}],[\"因此在涉及图文结合的任务\",{\"1\":{\"268\":1}}],[\"因此在相同的\",{\"1\":{\"181\":1}}],[\"因此在公式\",{\"1\":{\"114\":1}}],[\"因此每个质心将根据这些不同的半径值与其周围点形成多个点集群\",{\"1\":{\"140\":1}}],[\"因此\",{\"1\":{\"139\":1,\"204\":1,\"233\":1,\"272\":1,\"280\":1,\"282\":1,\"286\":1,\"288\":1,\"353\":1,\"376\":1,\"377\":1,\"381\":1,\"382\":2,\"413\":2,\"425\":1,\"470\":1,\"565\":1,\"612\":1,\"620\":1,\"622\":1,\"663\":1,\"681\":2,\"694\":1,\"803\":1,\"809\":1,\"822\":1,\"824\":1,\"826\":1,\"835\":2,\"847\":1,\"868\":1,\"885\":1,\"921\":2,\"925\":1,\"935\":1,\"944\":1,\"946\":2,\"947\":2,\"949\":1,\"950\":1,\"951\":1,\"956\":1,\"959\":1,\"960\":1}}],[\"因此传统卷积网络难以直接应用\",{\"1\":{\"110\":1}}],[\"因此传统基于卷积的视觉网络难以直接应用\",{\"1\":{\"109\":1}}],[\"因此加权融合的时候\",{\"1\":{\"96\":1}}],[\"因此仍然很难实现稳健泛化\",{\"1\":{\"20\":1}}],[\"因此泛化性不足\",{\"1\":{\"19\":1}}],[\"将编码器的梯度绕过非可导的\",{\"1\":{\"963\":1}}],[\"将编码索引映射回码本向量并\",{\"1\":{\"213\":1}}],[\"将与原来\",{\"1\":{\"951\":1}}],[\"将与较差的指标相似\",{\"1\":{\"567\":1}}],[\"将掩码全部置为\",{\"1\":{\"926\":1}}],[\"将掩码图像中目标物体所在区域激活\",{\"1\":{\"83\":1}}],[\"将梯度传递给\",{\"1\":{\"899\":1}}],[\"将梯度分别乘以x1和x0\",{\"1\":{\"809\":1}}],[\"将采样到的新\",{\"1\":{\"898\":1}}],[\"将两者按下式混合\",{\"1\":{\"894\":1}}],[\"将两个方向的\",{\"1\":{\"102\":1}}],[\"将两个注意力输出拼接在一起\",{\"1\":{\"69\":1}}],[\"将离散视觉词空间索引对应的概率分布设置为0\",{\"1\":{\"892\":1}}],[\"将离散化思想引入图像预训练任务\",{\"1\":{\"249\":1}}],[\"将对\",{\"1\":{\"886\":1}}],[\"将对角线置零\",{\"1\":{\"386\":1}}],[\"将先验概率\",{\"1\":{\"877\":1}}],[\"将不可微的采样操作拆解为一个可微的确定性函数加一个随机变量\",{\"1\":{\"931\":1}}],[\"将不断从业务逻辑中收集当下\",{\"1\":{\"835\":1}}],[\"将不规则点云转化为规则表示\",{\"1\":{\"110\":1}}],[\"将组件组合实现端到端应用\",{\"1\":{\"832\":1}}],[\"将增强后的信息输入到生成模型中\",{\"1\":{\"829\":1}}],[\"将用户的问题输入到检索系统中\",{\"1\":{\"829\":1}}],[\"将处理后的数据存储在对应的数据库中\",{\"1\":{\"829\":1}}],[\"将处理后的数据转化为检索模型可以使用的格式\",{\"1\":{\"829\":1}}],[\"将生成内容与检索到的原始资料建立链接\",{\"1\":{\"828\":1}}],[\"将生成模型分配的平均对数概率高的token作为答案\",{\"1\":{\"635\":1}}],[\"将上下文长度大幅提升至\",{\"1\":{\"823\":1}}],[\"将复杂问题分解为可管理的子问题\",{\"1\":{\"823\":1}}],[\"将计算结果绘制在图上\",{\"1\":{\"816\":1}}],[\"将普通数值计算转换为可微分计算\",{\"1\":{\"811\":1}}],[\"将other参数统一转换为ndarray\",{\"1\":{\"809\":1}}],[\"将ndarray或数值转换为variable\",{\"1\":{\"809\":1}}],[\"将非variable对象转换为variable实例\",{\"1\":{\"809\":1}}],[\"将function对variable的引用改为弱引用\",{\"1\":{\"806\":1}}],[\"将强引用改为弱引用\",{\"1\":{\"806\":1}}],[\"将蜕变为一个更通用\",{\"1\":{\"799\":1}}],[\"将数值微分的结果与反向传播的结果进行比较\",{\"1\":{\"795\":1}}],[\"将数据集压缩包下载到dataset目录下\",{\"1\":{\"696\":1}}],[\"将数据下载到当前项目目录下\",{\"1\":{\"410\":1}}],[\"将函数类封装为python函数\",{\"1\":{\"763\":1,\"790\":1}}],[\"将选项展平\",{\"1\":{\"737\":1}}],[\"将距离范围\",{\"1\":{\"710\":1}}],[\"将距离转换为\",{\"1\":{\"145\":1}}],[\"将所有输入序列填充到等长max\",{\"1\":{\"698\":1}}],[\"将所有文本合并成一个字符串\",{\"1\":{\"697\":1}}],[\"将所有子模块组合为完整的\",{\"1\":{\"255\":1}}],[\"将一整段文本按\",{\"1\":{\"696\":1}}],[\"将一篇文章\",{\"1\":{\"694\":1}}],[\"将一个形状为\",{\"1\":{\"545\":1}}],[\"将一个批次的数据拆分为图像和标签两个元组\",{\"1\":{\"424\":1}}],[\"将句子中各个字对应位置的\",{\"1\":{\"694\":1}}],[\"将训练数据复制\",{\"1\":{\"681\":1}}],[\"将训练语料中的每个单词按字符拆分\",{\"1\":{\"595\":1}}],[\"将kv\",{\"1\":{\"663\":1}}],[\"将抽象对齐技术成功应用于现实世界模型部署\",{\"1\":{\"658\":1}}],[\"将预训练语言模型从\",{\"1\":{\"650\":1}}],[\"将预训练模型的参数\",{\"1\":{\"609\":1}}],[\"将预训练模型冻结\",{\"1\":{\"286\":1}}],[\"将文档和问题跟每个可能答案拼接起来\",{\"1\":{\"631\":1}}],[\"将文本词索引空间对应的概率分布设置为0\",{\"1\":{\"892\":1}}],[\"将文本的交叉熵损失乘以\",{\"1\":{\"887\":1}}],[\"将文本和图像的\",{\"1\":{\"893\":1}}],[\"将文本和图像\",{\"1\":{\"887\":1}}],[\"将文本和图像序列拼接\",{\"1\":{\"384\":1}}],[\"将文本和query\",{\"1\":{\"419\":1}}],[\"将文本看成一个词序列\",{\"1\":{\"392\":1}}],[\"将文本特征输入\",{\"1\":{\"384\":1}}],[\"将文本特征作为查询\",{\"1\":{\"96\":1}}],[\"将文本拆分为子词\",{\"1\":{\"371\":1}}],[\"将文本\",{\"1\":{\"274\":1}}],[\"将文本语义信息与点云特征进行跨模态融合\",{\"1\":{\"95\":1}}],[\"将文本切分为多个句子\",{\"1\":{\"55\":1}}],[\"将原始图像编码为离散\",{\"1\":{\"893\":1}}],[\"将原始的\",{\"1\":{\"609\":1}}],[\"将原始点云\",{\"1\":{\"154\":1}}],[\"将原本用16bit表示的参数\",{\"1\":{\"607\":1}}],[\"将∆w进行低维分解∆w=ab\",{\"1\":{\"606\":1}}],[\"将y=wx变成y=\",{\"1\":{\"606\":1}}],[\"将w变成\",{\"1\":{\"602\":1}}],[\"将传入的最高频字符对中的两个字符用空格拼接起来\",{\"1\":{\"595\":1}}],[\"将样本的权重进行动态调整\",{\"1\":{\"589\":1}}],[\"将注意力图中注意力权重大于指定阈值的区域进行高亮显示\",{\"1\":{\"582\":1}}],[\"将注意力权重矩阵与v相乘\",{\"1\":{\"430\":1}}],[\"将维度大小为\",{\"1\":{\"546\":1}}],[\"将维度0和维度2交换\",{\"1\":{\"490\":1}}],[\"将该梯度向量与初始误差向量相乘\",{\"1\":{\"591\":1}}],[\"将该\",{\"1\":{\"501\":1}}],[\"将该点的距离设为0\",{\"1\":{\"121\":1}}],[\"将张量沿\",{\"1\":{\"482\":1}}],[\"将多个注意力头的输出合并为一个张量\",{\"1\":{\"430\":1}}],[\"将多头注意力的输出进行线性变换\",{\"1\":{\"430\":1}}],[\"将q和k的转置相乘\",{\"1\":{\"430\":1}}],[\"将隐藏特征映射到输出特征空间\",{\"1\":{\"429\":1}}],[\"将分类标记和图像块嵌入拼接\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"将分辨率相同的批次合并后输入模型\",{\"1\":{\"293\":1}}],[\"将卷积后的通道维数作为embedding的维度\",{\"1\":{\"426\":1}}],[\"将卷积核表示为\",{\"1\":{\"110\":1}}],[\"将裁剪后的图像调整为\",{\"1\":{\"425\":1}}],[\"将标签元组转换为一个一维张量\",{\"1\":{\"424\":1}}],[\"将标准\",{\"1\":{\"372\":1}}],[\"将横坐标0\",{\"1\":{\"424\":1}}],[\"将缓存的\",{\"1\":{\"420\":1}}],[\"将第一个\",{\"1\":{\"420\":1}}],[\"将会在下文进行详细讲解\",{\"1\":{\"419\":1}}],[\"将inputfeatures\",{\"1\":{\"713\":1}}],[\"将input\",{\"1\":{\"417\":1}}],[\"将图片编码成\",{\"1\":{\"417\":1}}],[\"将图像数据移动到指定设备上\",{\"1\":{\"431\":1}}],[\"将图像的短边缩放为\",{\"1\":{\"425\":1}}],[\"将图像元组堆叠成一个四维张量\",{\"1\":{\"424\":1}}],[\"将图像切块看成一个图像块序列\",{\"1\":{\"392\":1}}],[\"将图像切分为\",{\"1\":{\"380\":1,\"900\":1}}],[\"将图像切分为不重叠的\",{\"1\":{\"286\":1}}],[\"将图像切分成\",{\"1\":{\"380\":3}}],[\"将图像和文本向量拼接得到图像\",{\"1\":{\"371\":1}}],[\"将图像\",{\"1\":{\"369\":1,\"371\":1,\"893\":1}}],[\"将图像特征注入到文本流中\",{\"1\":{\"398\":1}}],[\"将图像特征\",{\"1\":{\"342\":1}}],[\"将图像分割为\",{\"1\":{\"329\":1}}],[\"将图像分割为1至40个448×448像素的区块\",{\"1\":{\"323\":1}}],[\"将图像转换为\",{\"1\":{\"266\":1}}],[\"将图像转换为离散码序列\",{\"1\":{\"216\":1}}],[\"将图像转为视觉\",{\"1\":{\"265\":1}}],[\"将图像送入模型并仅预测被\",{\"1\":{\"265\":1}}],[\"将图像从\",{\"1\":{\"264\":1}}],[\"将图像划分为固定大小的网格\",{\"1\":{\"388\":1}}],[\"将图像划分为\",{\"1\":{\"263\":1}}],[\"将图像编码为潜在表示\",{\"1\":{\"272\":1}}],[\"将图像编码为离散\",{\"1\":{\"251\":1}}],[\"将图像编码为量化的token\",{\"1\":{\"213\":1}}],[\"将图像预训练目标设计为\",{\"1\":{\"248\":1}}],[\"将图像视作一种外语\",{\"1\":{\"225\":1}}],[\"将图像当作外语的方式还能直接复用大规模语言模型的训练管线\",{\"1\":{\"220\":1}}],[\"将图像patch和点云点拼接成一个统一的token序列\",{\"1\":{\"65\":1}}],[\"将图像+点云特征插入语言嵌入中\",{\"1\":{\"64\":1}}],[\"将待分类的图像输入到图像编码器\",{\"1\":{\"408\":1}}],[\"将个文本特征和个图像特征两两组合\",{\"1\":{\"407\":1}}],[\"将正匹配的图文对与相似但不匹配的负样本区分开来\",{\"1\":{\"386\":1}}],[\"将问题转化为一个二分类问题\",{\"1\":{\"355\":1}}],[\"将最有用的信息提供给\",{\"1\":{\"421\":1}}],[\"将最老的\",{\"1\":{\"353\":1}}],[\"将最后一层\",{\"1\":{\"143\":1}}],[\"将英文数据转为中文\",{\"1\":{\"332\":1}}],[\"将模型暂时设为\",{\"1\":{\"899\":1}}],[\"将模型设置为\",{\"1\":{\"898\":1}}],[\"将模型切换到\",{\"1\":{\"895\":1}}],[\"将模型放入到仓库对应位置\",{\"1\":{\"712\":1}}],[\"将模型以元组形式返回的缓存重新封装为legacy\",{\"1\":{\"663\":1}}],[\"将模型的预测作为行\",{\"1\":{\"561\":1}}],[\"将模型层数从48层缩减至45层\",{\"1\":{\"330\":1}}],[\"将模型扩展到了\",{\"1\":{\"220\":1}}],[\"将高分辨率图像分割为多个低分辨率区块处理\",{\"1\":{\"326\":1}}],[\"将相对位置映射到\",{\"1\":{\"710\":1}}],[\"将相对坐标\",{\"1\":{\"119\":1}}],[\"将相对坐标通过mlp网络映射到高维特征空间\",{\"1\":{\"119\":1}}],[\"将相同分辨率的\",{\"1\":{\"293\":1}}],[\"将知识蒸馏扩展到\",{\"1\":{\"283\":1}}],[\"将知识传播到大量无标签数据中\",{\"1\":{\"283\":1}}],[\"将拼接的\",{\"1\":{\"274\":1}}],[\"将拼接后的特征映射回原维度\",{\"1\":{\"122\":1}}],[\"将解码器拆成一半单模态\",{\"1\":{\"277\":1}}],[\"将解码器拆分为\",{\"1\":{\"268\":1}}],[\"将解码器分为单模态部分和多模态部分\",{\"1\":{\"272\":1}}],[\"将被遮挡的\",{\"1\":{\"266\":1}}],[\"将像素从\",{\"1\":{\"264\":1}}],[\"将未遮挡的patch设置为遮挡\",{\"1\":{\"263\":1}}],[\"将通道映射到\",{\"1\":{\"255\":1}}],[\"将这个问题转化为一个多标签分类任务\",{\"1\":{\"413\":1}}],[\"将这一批特征拼接到输出中\",{\"1\":{\"293\":1}}],[\"将这种\",{\"1\":{\"237\":1}}],[\"将这些图像\",{\"1\":{\"895\":1}}],[\"将这些图文对转换为如下格式\",{\"1\":{\"341\":1}}],[\"将这些掩码token对应的嵌入向量映射到词向量空间中去\",{\"1\":{\"699\":1}}],[\"将这些排序用作训练奖励模型\",{\"1\":{\"656\":1}}],[\"将这些特征映射到类别空间\",{\"1\":{\"155\":2}}],[\"将这些信息和\",{\"1\":{\"83\":1}}],[\"将这些知识注入点云特征并与图像特征融合\",{\"1\":{\"32\":1}}],[\"将这些知识与点云和图像特征结合\",{\"1\":{\"29\":1}}],[\"将二维图像划分为一系列\",{\"1\":{\"231\":1}}],[\"将词与图像映射到同一词汇表\",{\"1\":{\"217\":1}}],[\"将连续表示映射为离散\",{\"1\":{\"249\":1}}],[\"将连续语义空间离散化\",{\"1\":{\"217\":1}}],[\"将连续的语义空间离散化为紧凑的视觉\",{\"1\":{\"210\":1}}],[\"将中间层\",{\"1\":{\"215\":1}}],[\"将其带入\",{\"1\":{\"945\":1}}],[\"将其变为归一化分布\",{\"1\":{\"877\":1}}],[\"将其喂进一个参数为\",{\"1\":{\"630\":1}}],[\"将其重塑为一个\",{\"1\":{\"542\":1}}],[\"将其所有元素无间隔地\",{\"1\":{\"489\":1}}],[\"将其展平就变成了一个长度为768的向量\",{\"1\":{\"426\":1}}],[\"将其动态分割为\",{\"1\":{\"322\":1}}],[\"将其路由到对应的专家\",{\"1\":{\"222\":1}}],[\"将其推广到计算机视觉\",{\"1\":{\"216\":1}}],[\"将其与中间层\",{\"1\":{\"214\":1}}],[\"将其代入标准的对比学习\",{\"1\":{\"202\":1}}],[\"将特征训练为匹配\",{\"1\":{\"282\":1}}],[\"将特征归一化后计算余弦距离\",{\"1\":{\"213\":1}}],[\"将特征映射到\",{\"1\":{\"70\":1}}],[\"将量化的token解码为重建特征\",{\"1\":{\"213\":1}}],[\"将簇内样本向量求平均\",{\"1\":{\"213\":1}}],[\"将主编码器和动量编码器配对\",{\"1\":{\"205\":1}}],[\"将主模型和动量模型参数组织成配对\",{\"1\":{\"192\":1}}],[\"将空间划分成立方体格子\",{\"1\":{\"159\":1}}],[\"将之前计算好的权重扩展维度\",{\"1\":{\"145\":1}}],[\"将坐标和特征从\",{\"1\":{\"145\":1}}],[\"将稀疏点集points2插值到密集点集xyz1的位置上\",{\"1\":{\"145\":1}}],[\"将稀疏点集的特征插值回原始点集的位置上\",{\"1\":{\"145\":1}}],[\"将稀疏点云渲染成逼真的\",{\"1\":{\"19\":1}}],[\"将邻域点组合成局部点云组\",{\"1\":{\"143\":1}}],[\"将来自下一级\",{\"1\":{\"142\":1}}],[\"将view\",{\"1\":{\"137\":1}}],[\"将转换后的坐标以及点的附加特征\",{\"1\":{\"136\":1}}],[\"将向量注意力划分为多个组\",{\"1\":{\"125\":1}}],[\"将p2位置的特征插值到p1位置\",{\"1\":{\"122\":1}}],[\"将深层特征上采样到浅层分辨率\",{\"1\":{\"122\":1}}],[\"将全局特征与每个点的局部特征拼接起来\",{\"1\":{\"150\":1}}],[\"将全局特征复制到每个点\",{\"1\":{\"122\":1}}],[\"将全局语义向量扩展回原始点云数量\",{\"1\":{\"70\":1}}],[\"将低分辨率特征上采样到高分辨率\",{\"1\":{\"122\":1}}],[\"将当前词的key和val进行缓存\",{\"1\":{\"663\":1}}],[\"将当前词列表中每个子词映射为字典中对于的词id\",{\"1\":{\"596\":2}}],[\"将当前动量特征送入队列\",{\"1\":{\"206\":1}}],[\"将当前选中的\",{\"1\":{\"137\":1}}],[\"将当前batch的结果存入总输出中\",{\"1\":{\"119\":1}}],[\"将当前样本的物体信息值追加到对应列表中\",{\"1\":{\"92\":1}}],[\"将每个结果\",{\"1\":{\"846\":1}}],[\"将每个\",{\"1\":{\"733\":1,\"885\":1}}],[\"将每个词从str转换为list列表形式\",{\"1\":{\"596\":1}}],[\"将每个样本分配到最近的簇\",{\"1\":{\"213\":1}}],[\"将每个样本属于的物体类型\",{\"1\":{\"53\":1}}],[\"将每个点的局部特征与整个点云的全局特征拼接起来\",{\"1\":{\"154\":1}}],[\"将每个查询点的邻居点坐标减去查询点自身坐标\",{\"1\":{\"119\":1}}],[\"将局部表面几何投影到切平面\",{\"1\":{\"110\":1}}],[\"将点集构建成图\",{\"1\":{\"110\":1}}],[\"将点集构建成图结构\",{\"1\":{\"109\":1}}],[\"将点云转换为体素网格\",{\"1\":{\"148\":1}}],[\"将点云划分为\",{\"1\":{\"125\":1}}],[\"将点云体素化\",{\"1\":{\"110\":1}}],[\"将点云体素化后应用\",{\"1\":{\"109\":1}}],[\"将点云投影到二维平面\",{\"1\":{\"110\":1}}],[\"将点云投影到图像平面\",{\"1\":{\"76\":1}}],[\"将点云\",{\"1\":{\"23\":1}}],[\"将点云表示为高斯基元\",{\"1\":{\"22\":1}}],[\"将响应值映射到\",{\"1\":{\"100\":1}}],[\"将融合特征重新分配给每个点\",{\"1\":{\"98\":2}}],[\"将融合特征映射回点空间\",{\"0\":{\"98\":1}}],[\"将融合后的文本\",{\"1\":{\"274\":1}}],[\"将融合后的\",{\"1\":{\"66\":1}}],[\"将融合后的空间特征通过适配器上采样到与语言模型匹配的维度\",{\"1\":{\"64\":1}}],[\"将交互主体区域框在特征图中框出的区域\",{\"1\":{\"83\":1}}],[\"将交互主体框和目标物体框等比例缩小\",{\"1\":{\"83\":1}}],[\"将目标物体区域框在特征图中框出的区域\",{\"1\":{\"83\":1}}],[\"将输入序列\",{\"1\":{\"741\":1}}],[\"将输入和目标张量展平为一维\",{\"1\":{\"590\":1,\"592\":1}}],[\"将输入和目标展平成一维张量便于计算\",{\"1\":{\"588\":1}}],[\"将输入和目标展平成一维张量\",{\"1\":{\"587\":1}}],[\"将输入展平便于后续计算\",{\"1\":{\"589\":1}}],[\"将输入展平成一维张量\",{\"1\":{\"586\":1}}],[\"将输入映射到\",{\"1\":{\"588\":1}}],[\"将输入映射到概率空间\",{\"1\":{\"587\":1}}],[\"将输入映射到低维空间以进行\",{\"1\":{\"69\":1}}],[\"将输入直接连接到输出\",{\"1\":{\"497\":1}}],[\"将输入特征映射到隐藏特征空间\",{\"1\":{\"429\":1}}],[\"将输入特征通过线性层生成q\",{\"1\":{\"119\":1}}],[\"将输入图片\",{\"1\":{\"426\":1}}],[\"将输入图像进行图像块嵌入\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"将输入图像大小和\",{\"1\":{\"266\":1}}],[\"将输入图像划分为\",{\"1\":{\"266\":1}}],[\"将输入图像\",{\"1\":{\"266\":1,\"963\":1}}],[\"将输入图像编码为一系列嵌入向量\",{\"1\":{\"197\":1}}],[\"将输入文本转换为嵌入列表后和query\",{\"1\":{\"419\":1}}],[\"将输入\",{\"1\":{\"385\":1,\"707\":1}}],[\"将输入的维度dim映射到dim\",{\"1\":{\"430\":1}}],[\"将输入的图像张量\",{\"1\":{\"380\":1}}],[\"将输入的图像切分成小\",{\"1\":{\"380\":1}}],[\"将输入的\",{\"1\":{\"274\":1,\"417\":1}}],[\"将输入从\",{\"1\":{\"213\":1}}],[\"将输入分别投影到低维空间\",{\"1\":{\"69\":1}}],[\"将语言嵌入\",{\"1\":{\"67\":1}}],[\"将\",{\"1\":{\"23\":1,\"26\":1,\"38\":1,\"39\":1,\"67\":1,\"68\":1,\"83\":3,\"87\":1,\"106\":1,\"116\":1,\"119\":1,\"216\":1,\"252\":1,\"258\":1,\"264\":2,\"293\":1,\"341\":1,\"343\":1,\"377\":1,\"380\":1,\"384\":5,\"385\":1,\"403\":1,\"420\":2,\"425\":2,\"426\":1,\"434\":1,\"501\":1,\"502\":1,\"546\":1,\"586\":1,\"590\":1,\"614\":1,\"655\":2,\"658\":1,\"823\":1,\"886\":1,\"891\":1,\"892\":2,\"893\":1,\"896\":1,\"897\":1,\"898\":1,\"899\":2,\"900\":1,\"945\":2,\"951\":1,\"963\":2,\"964\":1}}],[\"将可供性分数\",{\"1\":{\"22\":1}}],[\"将可供性检测扩展到\",{\"1\":{\"20\":1}}],[\"将可供性检测推广到新颖物体和不同视角\",{\"1\":{\"20\":1}}],[\"为对角矩阵\",{\"1\":{\"946\":1}}],[\"为对应词向量\",{\"1\":{\"371\":1}}],[\"为真实像素值\",{\"1\":{\"932\":1}}],[\"为代表的对概率直接建模的生成模型没有受到过太多的关注\",{\"1\":{\"925\":1}}],[\"为实际图片\",{\"1\":{\"895\":1}}],[\"为实现这一目标\",{\"1\":{\"50\":1}}],[\"为止\",{\"1\":{\"860\":1}}],[\"为归一化常数\",{\"1\":{\"852\":1}}],[\"为定义概率度量\",{\"1\":{\"847\":1}}],[\"为随机变量\",{\"1\":{\"846\":1}}],[\"为简化记号\",{\"1\":{\"846\":1}}],[\"为开发者带来了全面而强大的功能支持\",{\"1\":{\"833\":1}}],[\"为开源社区提供了可复现的基线\",{\"1\":{\"669\":1}}],[\"为模型提供丰富的上下文信息\",{\"1\":{\"828\":1}}],[\"为variable添加shape等属性\",{\"1\":{\"812\":1}}],[\"为variable类添加grad属性\",{\"1\":{\"778\":1}}],[\"为优化算法和神经网络训练奠定基础\",{\"1\":{\"811\":1}}],[\"为将\",{\"1\":{\"810\":1}}],[\"为避免手动修改config属性\",{\"1\":{\"807\":1}}],[\"为避免模型塌缩\",{\"1\":{\"280\":1}}],[\"为function类添加反向传播方法backward\",{\"1\":{\"779\":1}}],[\"为当前批次中的每个序列样本生成一个位置序列\",{\"1\":{\"716\":1}}],[\"为这些方法提供一个更清晰的性能基准\",{\"1\":{\"687\":1}}],[\"为ai研究的民主化提供了重要范例\",{\"1\":{\"672\":1}}],[\"为ai安全和实用性的发展提供了关键路径\",{\"1\":{\"654\":1}}],[\"为缓解对奖励函数的过度优化\",{\"1\":{\"656\":1}}],[\"为缓解此问题\",{\"1\":{\"212\":1}}],[\"为生成类任务\",{\"1\":{\"656\":1}}],[\"为防止泄露隐私\",{\"1\":{\"656\":1}}],[\"为确保训练集与评估集分离\",{\"1\":{\"656\":1}}],[\"为语言模型行为与用户意图之间架起了桥梁\",{\"1\":{\"654\":1}}],[\"为未来的系统化扩展和安全性提升奠定了坚实基础\",{\"1\":{\"833\":1}}],[\"为未来通用语言智能系统的发展提供了重要方向\",{\"1\":{\"651\":1}}],[\"为未来通用语言智能系统奠定了基础\",{\"1\":{\"647\":1}}],[\"为未来探索更通用的ai系统奠定了基础\",{\"1\":{\"639\":1}}],[\"为保证数据质量\",{\"1\":{\"647\":1}}],[\"为残差层数\",{\"1\":{\"640\":1}}],[\"为构建通用语言处理系统提供了新思路\",{\"1\":{\"638\":1}}],[\"为收集更多的标注数据提供了更多一个有价值的替代方案\",{\"1\":{\"626\":1}}],[\"为例\",{\"0\":{\"534\":1},\"1\":{\"596\":1,\"809\":2,\"822\":1}}],[\"为特定任务动态构建了一个分类器\",{\"1\":{\"408\":1}}],[\"为文本\",{\"1\":{\"380\":1}}],[\"为文本长度\",{\"1\":{\"206\":1}}],[\"为可学习温度参数\",{\"1\":{\"373\":1}}],[\"为分词后的长度\",{\"1\":{\"371\":1}}],[\"为通道数\",{\"1\":{\"371\":1}}],[\"为兼顾双编码器和融合编码器的优势\",{\"1\":{\"368\":1}}],[\"为下一阶段的端到端微调提供了良好的初始化\",{\"1\":{\"341\":1}}],[\"为赋予模型高分辨率处理和ocr能力\",{\"1\":{\"330\":1}}],[\"为提升效率\",{\"1\":{\"315\":1}}],[\"为多模态大模型的发展提供了重要贡献\",{\"1\":{\"295\":1}}],[\"为batch\",{\"1\":{\"285\":1}}],[\"为动量参数\",{\"1\":{\"285\":1}}],[\"为不同下游任务定制图像表示\",{\"1\":{\"273\":1}}],[\"为不同目标和下游任务定制图像表示\",{\"1\":{\"272\":1}}],[\"为更大模型\",{\"1\":{\"252\":1}}],[\"为预测像素点为\",{\"1\":{\"932\":1}}],[\"为预测目标\",{\"1\":{\"252\":1}}],[\"为预训练图像集合\",{\"1\":{\"214\":1}}],[\"为纯视觉模型设计了自监督\",{\"1\":{\"251\":1}}],[\"为单位遮挡\",{\"1\":{\"242\":1}}],[\"为视觉\",{\"1\":{\"228\":1,\"234\":1}}],[\"为视觉transformer的自监督预训练提供了新思路\",{\"1\":{\"227\":1}}],[\"为掩码图像建模提供语义级监督\",{\"1\":{\"217\":1}}],[\"为原图像对应的视觉\",{\"1\":{\"214\":1}}],[\"为训练图像数据集\",{\"1\":{\"212\":1}}],[\"为hidden\",{\"1\":{\"206\":1}}],[\"为主\",{\"1\":{\"204\":1}}],[\"为何前期要让\",{\"1\":{\"204\":1}}],[\"为解决这一问题\",{\"1\":{\"202\":1,\"228\":1,\"330\":1,\"654\":1}}],[\"为每一个padding\",{\"1\":{\"893\":1}}],[\"为每张图像生成\",{\"1\":{\"183\":1}}],[\"为每个位置生成\",{\"1\":{\"897\":1}}],[\"为每个\",{\"1\":{\"710\":1}}],[\"为每个特定任务\",{\"1\":{\"604\":1}}],[\"为每个图像选择一个负样本文本\",{\"1\":{\"419\":1}}],[\"为每个图像选择一个负文本\",{\"1\":{\"386\":1}}],[\"为每个类别创建一个描述性的文本\",{\"1\":{\"408\":1}}],[\"为每个文本选择一个负样本图像\",{\"1\":{\"419\":1}}],[\"为每个文本选择一个负图像\",{\"1\":{\"386\":1}}],[\"为每个文本选择一个最相似的非匹配图像\",{\"1\":{\"201\":1}}],[\"为每个向量找到最近的码本索引\",{\"1\":{\"213\":1}}],[\"为每个原始点\",{\"1\":{\"145\":1}}],[\"为每个中心点找到其邻域内的点\",{\"1\":{\"143\":1}}],[\"为每个尺度构建一个独立的小型\",{\"1\":{\"141\":1}}],[\"为每个关键点构建局部邻域\",{\"1\":{\"137\":1}}],[\"为每个采样点找到邻域并聚合特征\",{\"1\":{\"121\":1}}],[\"为每个采样点聚合其邻域内的特征信息\",{\"1\":{\"121\":1}}],[\"为每个查询点找到最近邻并分组其特征\",{\"1\":{\"119\":1}}],[\"为每个点生成特征\",{\"1\":{\"116\":1}}],[\"为每个组合额外生成\",{\"1\":{\"87\":1}}],[\"为点云\",{\"1\":{\"98\":1}}],[\"为点云坐标\",{\"1\":{\"78\":1}}],[\"为此引入第三方工具graphviz\",{\"1\":{\"815\":1}}],[\"为此\",{\"1\":{\"95\":1,\"142\":1,\"753\":1,\"807\":1,\"944\":1,\"945\":1}}],[\"为后续实现神经网络层和优化算法奠定了基础\",{\"1\":{\"809\":1}}],[\"为后续研究\",{\"1\":{\"640\":1}}],[\"为后续研究提供了灵活的基础\",{\"1\":{\"332\":1}}],[\"为后续三维深度学习奠定了基础\",{\"1\":{\"150\":1}}],[\"为后续的神经网络模块与训练机制打下坚实基础\",{\"1\":{\"814\":1}}],[\"为后续的处理步骤提供信息\",{\"1\":{\"142\":1}}],[\"为后续的注意力计算提供空间上下文信息\",{\"1\":{\"119\":1}}],[\"为后续\",{\"1\":{\"83\":1}}],[\"为功能类别标签\",{\"1\":{\"78\":1}}],[\"为rgb图像\",{\"1\":{\"78\":1}}],[\"为机器人操作\",{\"1\":{\"73\":1}}],[\"为坐标\",{\"1\":{\"59\":1}}],[\"为什么vq\",{\"1\":{\"956\":1}}],[\"为什么这样可行\",{\"1\":{\"944\":1}}],[\"为什么输出的是\",{\"1\":{\"931\":1}}],[\"为什么它能工作\",{\"1\":{\"894\":1}}],[\"为什么高斯样本集中在壳层上\",{\"0\":{\"874\":1}}],[\"为什么答案来自\",{\"1\":{\"735\":1}}],[\"为什么是\",{\"1\":{\"710\":1}}],[\"为什么相对位置矩阵\",{\"1\":{\"709\":1}}],[\"为什么还要乘上\",{\"1\":{\"931\":1}}],[\"为什么还要使用一个移动平均的编码器呢\",{\"1\":{\"352\":1}}],[\"为什么还需要prompt\",{\"1\":{\"616\":1}}],[\"为什么不也用\",{\"1\":{\"612\":1}}],[\"为什么\",{\"0\":{\"536\":1}}],[\"为什么此时\",{\"1\":{\"522\":1}}],[\"为什么函数复合更高效\",{\"1\":{\"500\":1}}],[\"为什么只在分布式环境下启用\",{\"1\":{\"385\":1}}],[\"为什么需要\",{\"0\":{\"492\":1}}],[\"为什么需要无干扰数据集\",{\"1\":{\"382\":1}}],[\"为什么需要这个正则化项\",{\"1\":{\"153\":1}}],[\"为什么分开\",{\"1\":{\"380\":1}}],[\"为什么第二个分支不直接不更新\",{\"1\":{\"356\":1}}],[\"为什么呢\",{\"1\":{\"356\":1}}],[\"为什么叫做个体判别呢\",{\"1\":{\"350\":1}}],[\"为什么要先用一次\",{\"1\":{\"923\":1}}],[\"为什么要用js散度而不是kl散度\",{\"0\":{\"913\":1}}],[\"为什么要用第一个位置\",{\"1\":{\"694\":1}}],[\"为什么要对大模型进行微调\",{\"0\":{\"601\":1}}],[\"为什么要把它们结合起来\",{\"1\":{\"587\":1}}],[\"为什么要除以\",{\"0\":{\"537\":1}}],[\"为什么要特殊处理\",{\"1\":{\"264\":1}}],[\"为什么要归一化\",{\"1\":{\"264\":1}}],[\"为什么要做两张图\",{\"1\":{\"264\":1}}],[\"为什么先验分布设置为均匀分布\",{\"0\":{\"262\":1}}],[\"为什么使用队列这种数据结构存储字典呢\",{\"1\":{\"356\":1}}],[\"为什么使用\",{\"1\":{\"86\":1,\"592\":1}}],[\"为什么我们需要pair\",{\"1\":{\"53\":1}}],[\"为什么壶嘴适合倒水\",{\"1\":{\"30\":1}}],[\"为进一步评估模型的理解与泛化能力\",{\"1\":{\"49\":1}}],[\"为验证所提方法\",{\"1\":{\"45\":1}}],[\"为支撑开放词汇\",{\"1\":{\"40\":1}}],[\"为输出头\",{\"1\":{\"39\":1}}],[\"为全连接层\",{\"1\":{\"38\":1}}],[\"为了优化编码器和解码器\",{\"1\":{\"959\":1}}],[\"为了处理离散的输入单词\",{\"1\":{\"956\":1}}],[\"为了能写成\",{\"1\":{\"946\":1}}],[\"为了能够在不同的局部子集上共享权重\",{\"1\":{\"131\":1}}],[\"为了扩展到\",{\"1\":{\"888\":1}}],[\"为了提高训练稳定性\",{\"1\":{\"823\":1}}],[\"为了提升框架的易用性\",{\"1\":{\"809\":1}}],[\"为了提升数据质量\",{\"1\":{\"341\":1}}],[\"为了提升全局图像表征\",{\"1\":{\"214\":1}}],[\"为了提升模型对点云姿态变化的鲁棒性\",{\"1\":{\"153\":1}}],[\"为了探索性能的极限\",{\"1\":{\"822\":1}}],[\"为了确保variable实例在混合运算中优先被处理\",{\"1\":{\"809\":1}}],[\"为了便于区分和调试\",{\"1\":{\"808\":1}}],[\"为了解决这个不平衡问题\",{\"1\":{\"893\":1}}],[\"为了解决这个问题\",{\"1\":{\"831\":1,\"892\":1}}],[\"为了解决这些问题\",{\"1\":{\"885\":1}}],[\"为了解决这类反向问题\",{\"1\":{\"878\":1}}],[\"为了解决这一问题\",{\"1\":{\"139\":1,\"653\":1,\"956\":1}}],[\"为了解决大型语言模型在生成文本时面临的一系列挑战\",{\"1\":{\"828\":1}}],[\"为了解决上述的问题\",{\"1\":{\"805\":1}}],[\"为了更好地支持多输入函数\",{\"1\":{\"800\":1}}],[\"为了更精确\",{\"1\":{\"501\":1}}],[\"为了方便讨论\",{\"1\":{\"921\":1}}],[\"为了方便使用\",{\"1\":{\"763\":1,\"809\":1}}],[\"为了方便大家理解\",{\"1\":{\"600\":1}}],[\"为了简化计算\",{\"1\":{\"904\":1}}],[\"为了简洁\",{\"1\":{\"846\":1}}],[\"为了简单\",{\"1\":{\"745\":1}}],[\"为了简写\",{\"1\":{\"274\":1}}],[\"为了参数更省\",{\"1\":{\"710\":1}}],[\"为了避免每次训练时看到相同的掩码模式\",{\"1\":{\"681\":1}}],[\"为了避免除以零\",{\"1\":{\"588\":1}}],[\"为了衡量模型的\",{\"1\":{\"656\":1}}],[\"为了保证标注质量\",{\"1\":{\"656\":1}}],[\"为了反映这点\",{\"1\":{\"631\":1}}],[\"为了让嵌入和编码器以不同的速度优化\",{\"1\":{\"961\":1}}],[\"为了让梯度从解码器传到编码器\",{\"1\":{\"961\":1}}],[\"为了让神经网络理解离散编码\",{\"1\":{\"961\":1}}],[\"为了让\",{\"1\":{\"946\":1}}],[\"为了让模型能生成接近训练数据的样本\",{\"1\":{\"943\":1}}],[\"为了让variable实例支持自然的乘法表达式\",{\"1\":{\"809\":1}}],[\"为了让llm给出的答案更加靠谱\",{\"1\":{\"619\":1}}],[\"为了让预训练好的\",{\"1\":{\"273\":1}}],[\"为了将一个无信息的样本\",{\"1\":{\"950\":1}}],[\"为了将\",{\"1\":{\"588\":1,\"945\":1}}],[\"为了将其作为损失函数使用\",{\"1\":{\"586\":1}}],[\"为了将几何属性与点云特征更好地对齐融合\",{\"1\":{\"38\":1}}],[\"为了充分利用预训练的权重\",{\"1\":{\"425\":1}}],[\"为了弥补数据规模上的差距\",{\"1\":{\"413\":1}}],[\"为了弥补现有研究缺乏鲁棒性评测的不足\",{\"1\":{\"19\":1}}],[\"为了实现随机图像生成\",{\"1\":{\"961\":1}}],[\"为了实现这一点\",{\"1\":{\"802\":1}}],[\"为了实现文字搜索图像的功能\",{\"1\":{\"411\":1}}],[\"为了实现多任务高效预训练\",{\"1\":{\"171\":1}}],[\"为了训练一个能够泛化到多种任务的语言模型\",{\"1\":{\"640\":1}}],[\"为了训练一个既具理解能力又具生成能力的统一模型\",{\"1\":{\"171\":1}}],[\"为了训练好q\",{\"1\":{\"417\":1}}],[\"为了训练clip模型\",{\"1\":{\"407\":1}}],[\"为了模拟用户提问和模型回答的形式\",{\"1\":{\"341\":1}}],[\"为了模仿传统卷积网络中的权重共享机制以提高学习效率和模型的泛化能力\",{\"1\":{\"131\":1}}],[\"为了同时处理多视角图像输入\",{\"1\":{\"293\":1}}],[\"为了兼顾这两者\",{\"1\":{\"272\":1}}],[\"为了公平比较\",{\"1\":{\"236\":1}}],[\"为了验证\",{\"1\":{\"117\":1,\"181\":1}}],[\"为了获得对物体可供性更深入的理解\",{\"1\":{\"34\":1}}],[\"为\",{\"1\":{\"32\":1,\"34\":1,\"38\":1,\"39\":1,\"83\":1,\"117\":1,\"187\":1,\"192\":1,\"214\":2,\"231\":1,\"243\":1,\"258\":1,\"274\":1,\"371\":1,\"431\":1,\"534\":1,\"542\":1,\"563\":1,\"569\":2,\"570\":4,\"595\":1,\"631\":1,\"656\":1,\"657\":1,\"658\":1,\"804\":1,\"833\":1,\"834\":1,\"846\":1,\"858\":1,\"862\":1,\"874\":1,\"898\":1}}],[\"为图像生成类任务提供了一种新的思路\",{\"1\":{\"955\":1}}],[\"为图像生成合成文本\",{\"1\":{\"185\":1}}],[\"为图像分辨率\",{\"1\":{\"371\":1}}],[\"为图像中主体和物体的边界框\",{\"1\":{\"78\":1}}],[\"为图像\",{\"1\":{\"32\":1}}],[\"为社区提供了一个衡量\",{\"1\":{\"19\":1}}],[\"上就好了\",{\"1\":{\"959\":1}}],[\"上分布\",{\"1\":{\"942\":1}}],[\"上时\",{\"1\":{\"877\":1}}],[\"上式等价于\",{\"1\":{\"849\":1}}],[\"上做\",{\"1\":{\"847\":1}}],[\"上做最大池化\",{\"1\":{\"137\":1}}],[\"上线后用户增长迅速\",{\"1\":{\"823\":1}}],[\"上超越未微调的palm\",{\"1\":{\"668\":1}}],[\"上超越了之前的有监督预训练模型\",{\"1\":{\"352\":1}}],[\"上超越了多个\",{\"1\":{\"310\":1}}],[\"上更真实\",{\"1\":{\"657\":1}}],[\"上收集\",{\"1\":{\"656\":1}}],[\"上达到了最先进的性能\",{\"1\":{\"677\":1}}],[\"上达到\",{\"1\":{\"641\":1}}],[\"上表现较差\",{\"1\":{\"641\":1}}],[\"上表现不佳\",{\"1\":{\"26\":1}}],[\"上提升显著\",{\"1\":{\"641\":1}}],[\"上提升8\",{\"1\":{\"625\":1}}],[\"上仍表现欠拟合\",{\"1\":{\"640\":1}}],[\"上爬取高赞\",{\"1\":{\"640\":1}}],[\"上使用多头自注意力操作\",{\"1\":{\"629\":1}}],[\"上使用偏置\",{\"1\":{\"380\":1}}],[\"上述公式简化为\",{\"1\":{\"848\":1}}],[\"上述定义也可推广到多维空间\",{\"1\":{\"847\":1}}],[\"上述例子是\",{\"1\":{\"816\":1}}],[\"上述的全量微调流程问题在于大模型的参数量往往特别大\",{\"1\":{\"609\":1}}],[\"上述即为pointnet++设计中的两个核心挑战\",{\"1\":{\"131\":1}}],[\"上创建\",{\"1\":{\"486\":1}}],[\"上面优化方向很多\",{\"1\":{\"697\":1}}],[\"上面所举例子并没有使用kv\",{\"1\":{\"660\":1}}],[\"上面代码实现中使用的是加权交叉熵损失\",{\"1\":{\"592\":1}}],[\"上面代码实现中使用的是可学习位置嵌入\",{\"1\":{\"428\":1}}],[\"上面的问题本质是因为函数调用顺序错误导致的\",{\"1\":{\"804\":1}}],[\"上面的完美模型包含边长为\",{\"1\":{\"570\":1}}],[\"上面的花卉分类案例相当于使用图像去匹配最合适的文本描述\",{\"1\":{\"411\":1}}],[\"上面已经给出了数据集加载以及vit模型核心代码实现了\",{\"1\":{\"435\":1}}],[\"上图\",{\"1\":{\"706\":1}}],[\"上图中的两句话明显是连续的\",{\"1\":{\"692\":1}}],[\"上图中是每一个patch中各位置的位置编码相似性度量\",{\"1\":{\"428\":1}}],[\"上图是4种不同类型的vlp模型示意图\",{\"1\":{\"390\":1}}],[\"上的联合似然的证据下界\",{\"1\":{\"885\":1}}],[\"上的一元连续分布\",{\"1\":{\"864\":1}}],[\"上的研究大致可以分为以下几个阶段\",{\"1\":{\"823\":1}}],[\"上的生成能力\",{\"1\":{\"641\":1}}],[\"上的问题答案数据集\",{\"1\":{\"634\":1}}],[\"上的随机操作\",{\"1\":{\"521\":1}}],[\"上的操作\",{\"1\":{\"521\":1}}],[\"上的\",{\"1\":{\"364\":1,\"386\":1,\"500\":1}}],[\"上的表现较差\",{\"1\":{\"327\":1}}],[\"上微调\",{\"1\":{\"318\":1}}],[\"上训练速度达380\",{\"1\":{\"667\":1}}],[\"上训练时\",{\"1\":{\"643\":1}}],[\"上训练好的权重\",{\"1\":{\"510\":1}}],[\"上训练\",{\"1\":{\"315\":1,\"316\":1,\"364\":1}}],[\"上均有显著提升\",{\"1\":{\"311\":1}}],[\"上均取得\",{\"1\":{\"309\":1}}],[\"上一句话\",{\"1\":{\"692\":1}}],[\"上一部分介绍了一系列模型指标\",{\"1\":{\"568\":1}}],[\"上一轮迭代的学生网络\",{\"1\":{\"289\":1}}],[\"上一层点集中的点特征重建过程中\",{\"1\":{\"94\":1}}],[\"上可达到\",{\"1\":{\"280\":1}}],[\"上引入一个新的\",{\"1\":{\"273\":1}}],[\"上进行训练\",{\"1\":{\"823\":1}}],[\"上进行预训练\",{\"1\":{\"413\":1,\"425\":1}}],[\"上进行语义分割测试\",{\"1\":{\"308\":1}}],[\"上进行线性探测评估\",{\"1\":{\"308\":1}}],[\"上进行\",{\"1\":{\"286\":1}}],[\"上进行聚合时计算开销很小\",{\"1\":{\"273\":1}}],[\"上进行图像分类来预训练视觉编码器\",{\"1\":{\"271\":1}}],[\"上进一步训练\",{\"1\":{\"240\":1}}],[\"上对卷积网络或\",{\"1\":{\"269\":1}}],[\"上受限\",{\"1\":{\"268\":1}}],[\"上运行\",{\"1\":{\"236\":1}}],[\"上验证了其有效性\",{\"1\":{\"228\":1}}],[\"上性能分别达到\",{\"1\":{\"215\":1}}],[\"上升\",{\"1\":{\"204\":1}}],[\"上预测得到的概率分布为\",{\"1\":{\"202\":1}}],[\"上预训练的\",{\"1\":{\"176\":1}}],[\"上所有样本索引的集合\",{\"1\":{\"190\":1}}],[\"上轻量微调\",{\"1\":{\"173\":1}}],[\"上取得\",{\"1\":{\"678\":1}}],[\"上取得良好性能\",{\"1\":{\"280\":1}}],[\"上取得最先进性能\",{\"1\":{\"165\":1}}],[\"上取得巨大成功\",{\"1\":{\"110\":1}}],[\"上略低于\",{\"1\":{\"157\":1}}],[\"上也具有泛化能力\",{\"1\":{\"106\":1}}],[\"上下文感知\",{\"1\":{\"824\":1}}],[\"上下文长度\",{\"1\":{\"823\":6}}],[\"上下文长度扩展至\",{\"1\":{\"823\":2}}],[\"上下文长度4096\",{\"1\":{\"334\":1}}],[\"上下文\",{\"1\":{\"733\":3,\"823\":1}}],[\"上下文学习\",{\"1\":{\"651\":1,\"825\":1}}],[\"上下文学习能力是由\",{\"1\":{\"825\":1}}],[\"上下文学习能力\",{\"1\":{\"225\":1}}],[\"上下文建模\",{\"1\":{\"650\":1}}],[\"上下文窗口限制性能提升\",{\"1\":{\"649\":1}}],[\"上下文窗口从512扩展到1024\",{\"1\":{\"640\":1}}],[\"上下文编码投影到词空间\",{\"1\":{\"384\":1}}],[\"上下文预测等\",{\"1\":{\"355\":1}}],[\"上下文对象\",{\"1\":{\"121\":1}}],[\"上下文丰富化\",{\"1\":{\"87\":1}}],[\"上下文注入\",{\"1\":{\"83\":1}}],[\"上采样后的特征\",{\"1\":{\"122\":1}}],[\"上采样后的点云特征\",{\"1\":{\"100\":1}}],[\"上采样\",{\"0\":{\"503\":1},\"1\":{\"122\":1,\"145\":2}}],[\"上采样的编码器特征\",{\"1\":{\"122\":2}}],[\"上采样层数\",{\"1\":{\"255\":1}}],[\"上采样层\",{\"0\":{\"122\":1},\"1\":{\"123\":1}}],[\"上采样特征\",{\"1\":{\"83\":1}}],[\"上采样过程中\",{\"1\":{\"94\":1}}],[\"上采样过程\",{\"1\":{\"70\":1}}],[\"上采样至原始点数后记为\",{\"1\":{\"38\":1}}],[\"上\",{\"1\":{\"19\":1,\"119\":1,\"157\":1,\"190\":1,\"213\":1,\"280\":1,\"286\":1,\"293\":1,\"309\":1,\"657\":1,\"710\":1,\"875\":1,\"899\":1}}],[\"损坏的数据\",{\"1\":{\"19\":1}}],[\"损失就是\",{\"1\":{\"910\":2}}],[\"损失之间的权重分配\",{\"1\":{\"592\":1}}],[\"损失权重仅降低\",{\"1\":{\"589\":1}}],[\"损失权重降低\",{\"1\":{\"589\":1}}],[\"损失几乎不受影响\",{\"1\":{\"589\":1}}],[\"损失被大幅降低\",{\"1\":{\"589\":1}}],[\"损失越小表示预测越接近真实标签\",{\"1\":{\"588\":1}}],[\"损失结合在一起的一种损失函数\",{\"1\":{\"587\":1}}],[\"损失值逐渐降低\",{\"1\":{\"427\":1}}],[\"损失记录到\",{\"1\":{\"384\":1}}],[\"损失乘以权重\",{\"1\":{\"384\":1}}],[\"损失也会很大\",{\"1\":{\"356\":1}}],[\"损失仍只在被遮挡位置计算\",{\"1\":{\"214\":1}}],[\"损失日志\",{\"1\":{\"213\":1}}],[\"损失计算与反向传播\",{\"1\":{\"427\":1}}],[\"损失计算的是整个字典做多分类\",{\"1\":{\"356\":1}}],[\"损失计算\",{\"1\":{\"105\":1}}],[\"损失约束\",{\"1\":{\"23\":1}}],[\"损失\",{\"1\":{\"14\":1,\"194\":1,\"208\":1,\"214\":2,\"293\":1,\"384\":1,\"385\":1,\"403\":1,\"899\":1,\"963\":1}}],[\"损失函数与优化器\",{\"1\":{\"926\":1}}],[\"损失函数直接作用于\",{\"1\":{\"427\":1}}],[\"损失函数类\",{\"1\":{\"293\":1}}],[\"损失函数定义\",{\"1\":{\"104\":1}}],[\"损失函数由\",{\"1\":{\"39\":1}}],[\"损失函数\",{\"0\":{\"101\":1,\"585\":1},\"1\":{\"6\":1,\"7\":1,\"9\":1,\"11\":1,\"12\":1,\"14\":1,\"24\":1,\"293\":1,\"355\":2,\"588\":1,\"592\":1,\"700\":1}}],[\"大致与单一部分损失相同\",{\"1\":{\"893\":1}}],[\"大致上两者结构是相同的\",{\"1\":{\"429\":1}}],[\"大量的模型容量会花在捕捉高频细节上\",{\"1\":{\"885\":1}}],[\"大量实验验证了我们提出的\",{\"1\":{\"50\":1}}],[\"大量实验表明\",{\"1\":{\"19\":1}}],[\"大脑\",{\"1\":{\"822\":1}}],[\"大距离用对数压缩的桶\",{\"1\":{\"710\":1}}],[\"大批量训练提升mlm困惑度\",{\"1\":{\"683\":1}}],[\"大批量训练\",{\"1\":{\"681\":2,\"683\":1}}],[\"大部分任务通过自然语言指令表达意图\",{\"1\":{\"656\":1}}],[\"大部分任务\",{\"1\":{\"633\":1}}],[\"大部分深度学习方法需要大量人工标注的数据\",{\"1\":{\"626\":1}}],[\"大公司或者研究机构\",{\"1\":{\"610\":1}}],[\"大\",{\"1\":{\"600\":1,\"706\":1}}],[\"大模型api使用\",{\"0\":{\"838\":1}}],[\"大模型开发与传统\",{\"1\":{\"835\":1}}],[\"大模型开发却更多是一个工程问题\",{\"1\":{\"835\":1}}],[\"大模型开发\",{\"0\":{\"835\":1},\"1\":{\"835\":2}}],[\"大模型可以成为\",{\"1\":{\"826\":1}}],[\"大模型研发\",{\"1\":{\"823\":1}}],[\"大模型领域仅存在预训练阶段的\",{\"1\":{\"822\":1}}],[\"大模型加速技术之kv\",{\"1\":{\"659\":1}}],[\"大模型使用较大的batch\",{\"1\":{\"647\":1}}],[\"大模型中有其中一部分参数\",{\"1\":{\"606\":1}}],[\"大模型参数很多\",{\"1\":{\"606\":1}}],[\"大模型的性能不断增长\",{\"1\":{\"822\":1}}],[\"大模型的微调有以下几条技术路线\",{\"1\":{\"602\":1}}],[\"大模型的微调分成两条技术路线\",{\"1\":{\"602\":1}}],[\"大模型的推理成本越高\",{\"1\":{\"601\":1}}],[\"大模型\",{\"1\":{\"600\":1,\"823\":2}}],[\"大模型微调大致发展历史\",{\"0\":{\"610\":1}}],[\"大模型微调\",{\"0\":{\"599\":1},\"1\":{\"599\":1}}],[\"大大提高学习效率\",{\"1\":{\"422\":1}}],[\"大家参考仓库源码即可\",{\"1\":{\"699\":1}}],[\"大家可以自行拉取项目完整代码进行学习\",{\"1\":{\"435\":1}}],[\"大家注意区分\",{\"1\":{\"417\":1}}],[\"大家请自行阅读源代码\",{\"1\":{\"293\":1}}],[\"大幅降低训练成本\",{\"1\":{\"415\":1}}],[\"大幅超越所有基线\",{\"1\":{\"47\":1}}],[\"大放异彩的一年\",{\"1\":{\"405\":1}}],[\"大于最大桶号的全部压到最后一个桶\",{\"1\":{\"710\":1}}],[\"大于\",{\"1\":{\"380\":1,\"429\":1}}],[\"大字典是怎么做到的\",{\"1\":{\"353\":1}}],[\"大型模型不仅可以缩短每个具体应用的开发周期\",{\"1\":{\"826\":1}}],[\"大型视觉编码器\",{\"1\":{\"312\":1}}],[\"大型语言模型\",{\"1\":{\"296\":1,\"323\":1,\"339\":1,\"654\":1,\"828\":1}}],[\"大融合\",{\"1\":{\"220\":1,\"225\":1}}],[\"大多数样本会集中在这个区域上\",{\"1\":{\"873\":1}}],[\"大多数样本应该靠近原点\",{\"1\":{\"872\":1}}],[\"大多数sota模型依赖于\",{\"1\":{\"650\":1}}],[\"大多数方法求解出来结果都一样的答案\",{\"1\":{\"621\":1}}],[\"大多数实际应用中都是如此\",{\"1\":{\"562\":1}}],[\"大多数基础模型需要针对不同任务手动调整网络格式\",{\"1\":{\"220\":1}}],[\"大多数现有方法仍基于低级像素\",{\"1\":{\"216\":1}}],[\"大多使用从网络收集的嘈杂图文对作为训练数据\",{\"1\":{\"165\":1}}],[\"大约\",{\"1\":{\"214\":1}}],[\"大语言模型的两个核心能力\",{\"1\":{\"835\":1}}],[\"大语言模型的发展历程虽然只有短短不到五年的时间\",{\"1\":{\"823\":1}}],[\"大语言模型是这个新模式的典型例子\",{\"1\":{\"826\":1}}],[\"大语言模型是一种具有强大语言处理能力的技术\",{\"1\":{\"824\":1}}],[\"大语言模型具有多种显著特点\",{\"1\":{\"824\":1}}],[\"大语言模型应用开发基础知识速览\",{\"1\":{\"821\":1}}],[\"大语言模型\",{\"0\":{\"163\":1,\"822\":1},\"1\":{\"336\":1,\"822\":2,\"835\":1}}],[\"大小要足够的大\",{\"1\":{\"357\":1}}],[\"大小和字典大小是等价的\",{\"1\":{\"357\":1}}],[\"大小和字典大小剥离开\",{\"1\":{\"356\":1}}],[\"大小和设备信息\",{\"1\":{\"274\":1}}],[\"大小和设备\",{\"1\":{\"274\":1}}],[\"大小转为\",{\"1\":{\"266\":1}}],[\"大小的图像\",{\"1\":{\"425\":1}}],[\"大小的队列特征\",{\"1\":{\"353\":1}}],[\"大小的\",{\"1\":{\"236\":1}}],[\"大小\",{\"1\":{\"213\":1,\"215\":1,\"274\":1,\"357\":1,\"380\":2,\"425\":1,\"501\":1,\"709\":1,\"846\":1}}],[\"大小不超过\",{\"1\":{\"150\":1}}],[\"大小为输入图像的patch数目\",{\"1\":{\"263\":1}}],[\"大小为下采样后的总点数\",{\"1\":{\"121\":1}}],[\"大小为\",{\"1\":{\"83\":1,\"212\":1,\"224\":1,\"899\":1}}],[\"大区域\",{\"1\":{\"137\":1}}],[\"大局部区域\",{\"1\":{\"137\":1}}],[\"大规模统一建模方法具备强大的泛化与多任务能力\",{\"1\":{\"884\":1}}],[\"大规模模型和海量数据的结合\",{\"1\":{\"884\":1}}],[\"大规模语言模型在任务通用性与灵活性方面具有巨大潜力\",{\"1\":{\"651\":1}}],[\"大规模地在zero\",{\"1\":{\"650\":1}}],[\"大规模视觉编码器\",{\"1\":{\"304\":1}}],[\"大规模\",{\"1\":{\"20\":1}}],[\"大规模预训练模型可以将视觉特征与可供性相关的文本描述对齐\",{\"1\":{\"20\":1}}],[\"大规模预训练\",{\"1\":{\"19\":1}}],[\"实时对话\",{\"1\":{\"823\":1}}],[\"实时语音和视频对话\",{\"1\":{\"823\":1}}],[\"实体嵌入\",{\"1\":{\"687\":1}}],[\"实质是一种\",{\"1\":{\"650\":1}}],[\"实用工具\",{\"1\":{\"510\":1}}],[\"实线为\",{\"1\":{\"502\":1}}],[\"实证结果显示\",{\"1\":{\"271\":1}}],[\"实例说明\",{\"1\":{\"500\":1}}],[\"实例化验证数据集\",{\"1\":{\"425\":1}}],[\"实例化训练数据集\",{\"1\":{\"425\":1}}],[\"实例化数据增强类\",{\"1\":{\"264\":1}}],[\"实例分类\",{\"1\":{\"282\":1}}],[\"实例分割和语义分割等任务\",{\"1\":{\"222\":1}}],[\"实例分割\",{\"1\":{\"220\":1}}],[\"实际应用中\",{\"1\":{\"942\":1}}],[\"实际效果\",{\"1\":{\"894\":1}}],[\"实际中用更大值鼓励更均匀的码本利用\",{\"1\":{\"885\":1}}],[\"实际任务训练\",{\"1\":{\"819\":1}}],[\"实际此处的函数放缩因子也称为函数的导数\",{\"1\":{\"775\":1}}],[\"实际实现过程中\",{\"1\":{\"697\":1}}],[\"实际操作如下\",{\"1\":{\"691\":1}}],[\"实际对齐的是训练流程中的多重人为偏好叠加\",{\"1\":{\"658\":1}}],[\"实际负例\",{\"1\":{\"561\":1}}],[\"实际正例\",{\"1\":{\"561\":1}}],[\"实际创建一个新的数据缓冲区\",{\"1\":{\"545\":1}}],[\"实际携带信息\",{\"1\":{\"529\":1}}],[\"实际代码对比\",{\"1\":{\"522\":1}}],[\"实际需要的最近邻数量\",{\"1\":{\"488\":1}}],[\"实际复制\",{\"1\":{\"472\":1}}],[\"实际采集的点云常有遮挡\",{\"1\":{\"149\":1}}],[\"实际上梯度下降法并不擅长处理rosenbrock这种类型的函数\",{\"1\":{\"816\":1}}],[\"实际上也体现了对\",{\"1\":{\"655\":1}}],[\"实际上并没有复制内存中的任何数据值\",{\"1\":{\"544\":1}}],[\"实际上没用\",{\"1\":{\"542\":1}}],[\"实际上没有局部的概念\",{\"1\":{\"131\":1}}],[\"实际上变成了\",{\"1\":{\"454\":1}}],[\"实际上\",{\"1\":{\"237\":1,\"413\":1,\"945\":1,\"961\":1}}],[\"实际上是一个很薄的壳层或环带\",{\"1\":{\"872\":1}}],[\"实际上是一个动态生成的卷积核\",{\"1\":{\"100\":1}}],[\"实际上是用一个固定大小的全局特征去\",{\"1\":{\"157\":1}}],[\"实际可用的最近邻数量\",{\"1\":{\"119\":1}}],[\"实验在三大基准任务上进行\",{\"1\":{\"680\":1}}],[\"实验步骤\",{\"0\":{\"680\":1}}],[\"实验覆盖了翻译\",{\"1\":{\"646\":1}}],[\"实验结论\",{\"1\":{\"641\":1}}],[\"实验结果如下表所示\",{\"1\":{\"428\":1}}],[\"实验结果\",{\"1\":{\"376\":3}}],[\"实验结果与贡献\",{\"1\":{\"368\":1}}],[\"实验结果与贡献总结\",{\"1\":{\"19\":1}}],[\"实验结果与表现\",{\"1\":{\"165\":1}}],[\"实验结果显示\",{\"1\":{\"117\":1,\"376\":1}}],[\"实验结果表明\",{\"1\":{\"72\":1,\"289\":1,\"322\":1,\"376\":1,\"665\":1,\"677\":1,\"681\":1}}],[\"实验的关键发现是\",{\"1\":{\"641\":1}}],[\"实验设计与模型配置\",{\"1\":{\"641\":1}}],[\"实验设置与模型规模\",{\"1\":{\"640\":1}}],[\"实验评估\",{\"1\":{\"593\":1}}],[\"实验采用的是花蕊数据集\",{\"1\":{\"424\":1}}],[\"实验采用以下评估指标评估\",{\"1\":{\"46\":1}}],[\"实验全流程管理框架\",{\"1\":{\"381\":1}}],[\"实验管理框架\",{\"1\":{\"379\":1}}],[\"实验中我们将解码器平分为两部分\",{\"1\":{\"272\":1}}],[\"实验中我们使用\",{\"1\":{\"236\":1}}],[\"实验中统一设为\",{\"1\":{\"202\":1}}],[\"实验表明\",{\"1\":{\"227\":1,\"228\":1,\"645\":1,\"678\":1,\"681\":2}}],[\"实验效果\",{\"0\":{\"215\":1}}],[\"实验观察\",{\"1\":{\"157\":1}}],[\"实验验证\",{\"1\":{\"150\":1,\"157\":1}}],[\"实验证明动态掩码效果更优\",{\"1\":{\"679\":1}}],[\"实验证明\",{\"1\":{\"150\":1,\"165\":1,\"336\":1}}],[\"实验证明了great在开放词汇场景下的有效性和优越性\",{\"1\":{\"29\":1}}],[\"实验发现\",{\"1\":{\"114\":1,\"286\":1,\"304\":1}}],[\"实验\",{\"0\":{\"45\":1,\"241\":1,\"307\":1,\"333\":1,\"632\":1,\"641\":1},\"1\":{\"290\":1,\"344\":1,\"846\":1}}],[\"实现如下所示\",{\"1\":{\"959\":1}}],[\"实现如前所述\",{\"1\":{\"795\":1}}],[\"实现vae\",{\"0\":{\"929\":1}}],[\"实现pixelcnn时\",{\"1\":{\"925\":1}}],[\"实现从用户输入到数据库再到大模型最后输出的整体架构连接\",{\"1\":{\"836\":1}}],[\"实现从用户输入到应用输出的全流程贯通\",{\"1\":{\"836\":1}}],[\"实现全面的多模态交互\",{\"1\":{\"823\":1}}],[\"实现全局信息对局部特征学习的反哺\",{\"1\":{\"214\":1}}],[\"实现网页浏览\",{\"1\":{\"823\":1}}],[\"实现可复用的层\",{\"1\":{\"819\":1}}],[\"实现后序遍历\",{\"1\":{\"815\":1}}],[\"实现计算图的可视化渲染\",{\"1\":{\"814\":1}}],[\"实现a\",{\"1\":{\"809\":1}}],[\"实现using\",{\"1\":{\"807\":1}}],[\"实现变分自编码器\",{\"1\":{\"928\":1}}],[\"实现变量导数的重置\",{\"1\":{\"802\":1}}],[\"实现变换不变性\",{\"1\":{\"150\":1}}],[\"实现真正意义上的自动反向传播\",{\"1\":{\"799\":1}}],[\"实现bert\",{\"1\":{\"689\":1}}],[\"实现讲解\",{\"1\":{\"663\":1}}],[\"实现任务统一与跨任务迁移\",{\"1\":{\"650\":1}}],[\"实现显著的性能提升确实是可能的\",{\"1\":{\"636\":1}}],[\"实现虚拟复制\",{\"1\":{\"546\":1}}],[\"实现虚拟扩展\",{\"1\":{\"546\":1}}],[\"实现数据虚拟扩展\",{\"1\":{\"546\":1}}],[\"实现步骤如下\",{\"1\":{\"502\":1}}],[\"实现精确的像素级对齐\",{\"1\":{\"502\":1}}],[\"实现精准的3d功能定位\",{\"1\":{\"29\":1}}],[\"实现复杂功能\",{\"1\":{\"500\":1}}],[\"实现分段逼近\",{\"1\":{\"500\":1}}],[\"实现的操作\",{\"1\":{\"492\":1}}],[\"实现zero\",{\"1\":{\"408\":1,\"416\":1}}],[\"实现信息融合\",{\"1\":{\"399\":1}}],[\"实现类\",{\"1\":{\"382\":1}}],[\"实现类似人类的类比推理能力\",{\"1\":{\"56\":1}}],[\"实现主要包括以下三个关键阶段\",{\"1\":{\"339\":1}}],[\"实现多元函数反向传播\",{\"1\":{\"812\":1}}],[\"实现多模态能力的协同提升\",{\"1\":{\"323\":1}}],[\"实现多任务预训练与灵活迁移\",{\"1\":{\"165\":1}}],[\"实现了基础的计算图结构与反向传播流程\",{\"1\":{\"799\":1}}],[\"实现了数值微分作为梯度检验工具\",{\"1\":{\"797\":1}}],[\"实现了变量和函数的基本结构\",{\"1\":{\"797\":1}}],[\"实现了比多项式逼近更高效的函数近似\",{\"1\":{\"500\":1}}],[\"实现了比基于区域特征的模型快数十倍\",{\"1\":{\"388\":1}}],[\"实现了图像与语言之间的初步语义对齐\",{\"1\":{\"341\":1}}],[\"实现了在图像分类\",{\"1\":{\"313\":1}}],[\"实现了更高效\",{\"1\":{\"125\":1}}],[\"实现端到端的梯度传播\",{\"1\":{\"257\":1}}],[\"实现视觉\",{\"1\":{\"217\":1}}],[\"实现向量量化\",{\"1\":{\"213\":1}}],[\"实现图文信息的深层交互\",{\"1\":{\"197\":1}}],[\"实现论文3\",{\"1\":{\"187\":1}}],[\"实现论文中提出的图像\",{\"1\":{\"187\":1}}],[\"实现点集顺序不变性\",{\"1\":{\"148\":1}}],[\"实现方程\",{\"1\":{\"235\":1}}],[\"实现方式\",{\"1\":{\"143\":1}}],[\"实现方法\",{\"1\":{\"136\":1}}],[\"实现特征上采样\",{\"1\":{\"122\":1}}],[\"实现特点\",{\"1\":{\"119\":1}}],[\"实现预激活\",{\"1\":{\"120\":1}}],[\"实现高效训练\",{\"1\":{\"667\":1}}],[\"实现高效的点云特征聚合\",{\"1\":{\"119\":1}}],[\"实现高效预测\",{\"1\":{\"24\":1}}],[\"实现跨模态信息的充分交互\",{\"1\":{\"97\":1}}],[\"实现语言引导下的跨模态融合\",{\"1\":{\"94\":1}}],[\"实现无先验对齐\",{\"1\":{\"80\":1}}],[\"实现对图像语义信息的高效编码与预训练\",{\"1\":{\"212\":1}}],[\"实现对称性\",{\"1\":{\"150\":1}}],[\"实现对\",{\"1\":{\"50\":1}}],[\"实现细节\",{\"0\":{\"286\":1,\"306\":1,\"957\":1},\"1\":{\"46\":1,\"286\":1}}],[\"实现开放词汇功能定位\",{\"1\":{\"31\":1}}],[\"实现\",{\"0\":{\"689\":1,\"928\":1,\"936\":1},\"1\":{\"23\":2,\"70\":1,\"102\":1,\"176\":1,\"197\":1,\"254\":1,\"400\":1,\"697\":1,\"808\":1,\"814\":1}}],[\"实现更多样化的应用\",{\"1\":{\"824\":1}}],[\"实现更好的泛化\",{\"1\":{\"20\":1}}],[\"实现更加稳健的学习\",{\"1\":{\"17\":1}}],[\"并从中采样\",{\"1\":{\"952\":1}}],[\"并从多个选项中选择正确答案\",{\"1\":{\"342\":1}}],[\"并输入解码器就能生成图像\",{\"1\":{\"947\":1}}],[\"并输入视觉\",{\"1\":{\"214\":1}}],[\"并让其尽可能逼近\",{\"1\":{\"945\":1}}],[\"并让模型预测这些被遮盖的单词\",{\"1\":{\"681\":1}}],[\"并让模型基于上下文预测原始视觉标记\",{\"1\":{\"228\":1}}],[\"并写出其对数形式\",{\"1\":{\"932\":1}}],[\"并注意两点\",{\"1\":{\"925\":1}}],[\"并展平每个\",{\"1\":{\"900\":1}}],[\"并定义随机变量为恒等函数\",{\"1\":{\"847\":1}}],[\"并针对性分析\",{\"1\":{\"836\":1}}],[\"并针对性改进\",{\"1\":{\"836\":1}}],[\"并带有监控和日志功能\",{\"1\":{\"834\":1}}],[\"并带有跳跃连接\",{\"1\":{\"286\":1}}],[\"并以此为基础\",{\"1\":{\"828\":1}}],[\"并于\",{\"1\":{\"823\":1}}],[\"并进一步提升了代码质量和多轮对话一致性\",{\"1\":{\"823\":1}}],[\"并进行l2归一化\",{\"1\":{\"407\":1}}],[\"并进行推理\",{\"1\":{\"386\":1}}],[\"并进行相似度计算\",{\"1\":{\"377\":1}}],[\"并进行视觉对话的\",{\"1\":{\"342\":1}}],[\"并改进了工具调用和多模态能力\",{\"1\":{\"823\":1}}],[\"并改造为a\",{\"1\":{\"410\":1}}],[\"并充当通用任务求解器\",{\"1\":{\"823\":1}}],[\"并支持在jupyter\",{\"1\":{\"815\":1}}],[\"并支持在多模态任务\",{\"1\":{\"222\":1}}],[\"并保存为文件\",{\"1\":{\"815\":1}}],[\"并标注\",{\"1\":{\"808\":1}}],[\"并为后续的功能扩展奠定了坚实基础\",{\"1\":{\"797\":1}}],[\"并为后续阶段提供稳健的视觉表示\",{\"1\":{\"305\":1}}],[\"并重新命名为\",{\"1\":{\"712\":1}}],[\"并没有专门针对局部依赖关系进行优化\",{\"1\":{\"706\":1}}],[\"并解压到当前目录下\",{\"1\":{\"696\":1}}],[\"并调整学习率\",{\"1\":{\"681\":1,\"683\":1}}],[\"并调整维度顺序\",{\"1\":{\"380\":1}}],[\"并调整维度\",{\"1\":{\"380\":1}}],[\"并优化了训练细节\",{\"1\":{\"680\":1}}],[\"并添加\",{\"1\":{\"898\":1}}],[\"并添加特殊标记\",{\"1\":{\"679\":1}}],[\"并添加缩略图以保留全局上下文\",{\"1\":{\"323\":1}}],[\"并验证了数据规模对预训练的关键作用\",{\"1\":{\"678\":1}}],[\"并作为单一的数据流进行自回归建模\",{\"1\":{\"887\":1}}],[\"并作为下一轮的输入tokens\",{\"1\":{\"660\":1}}],[\"并作为目标嵌入进行比对\",{\"1\":{\"273\":1}}],[\"并指出\",{\"1\":{\"655\":1}}],[\"并指出gpt\",{\"1\":{\"638\":1}}],[\"并非\",{\"1\":{\"658\":1}}],[\"并非普遍\",{\"1\":{\"658\":1}}],[\"并非广义上的\",{\"1\":{\"654\":1}}],[\"并非通用智能\",{\"1\":{\"649\":1}}],[\"并探索更强的系统性泛化能力和稳健性\",{\"1\":{\"649\":1}}],[\"并探讨了数据污染和社会影响等问题\",{\"1\":{\"646\":1}}],[\"并按f1\",{\"1\":{\"647\":1}}],[\"并按行优先排序来实现\",{\"1\":{\"436\":1}}],[\"并系统评估其在零样本\",{\"1\":{\"646\":1}}],[\"并表明在极限情况下\",{\"1\":{\"642\":1}}],[\"并给出了提示transformer类模型和长距离依赖的文本数据集最好用这种方法来训练\",{\"1\":{\"636\":1}}],[\"并独立地处理\",{\"1\":{\"631\":1}}],[\"并最终在训练结束时得到更小的重构误差\",{\"1\":{\"886\":1}}],[\"并最终引导出示例问题的正确结果\",{\"1\":{\"620\":1}}],[\"并最终输出分类结果\",{\"1\":{\"138\":1}}],[\"并评估它们在验证集或测试集上的性能\",{\"1\":{\"593\":1}}],[\"并累加到累计正确样本数中\",{\"1\":{\"431\":1}}],[\"并获得该批次图像列表对应的图像嵌入向量列表\",{\"1\":{\"410\":1}}],[\"并计算与文本特征的余弦相似度\",{\"1\":{\"408\":1}}],[\"并投影到对比学习空间\",{\"1\":{\"385\":1}}],[\"并投影为\",{\"1\":{\"266\":1}}],[\"并放到图像所在的设备上\",{\"1\":{\"385\":1}}],[\"并映射到\",{\"1\":{\"380\":3}}],[\"并对其进行了一些修改\",{\"1\":{\"927\":1}}],[\"并对图像和文本特征进行对齐\",{\"1\":{\"371\":1}}],[\"并对潜在交互方式进行类比推理\",{\"1\":{\"50\":1}}],[\"并迁移到标注数据较少的任务上\",{\"1\":{\"352\":1}}],[\"并适配不同llms\",{\"1\":{\"327\":1}}],[\"并扩展至100万tokens的上下文窗口\",{\"1\":{\"325\":1}}],[\"并使其能够适配不同的语言模型\",{\"1\":{\"322\":1}}],[\"并使用\",{\"1\":{\"935\":1}}],[\"并使用了统计学里的变分推理\",{\"1\":{\"925\":1}}],[\"并使用其周围的\",{\"1\":{\"505\":1}}],[\"并使用交叉熵损失训练\",{\"1\":{\"271\":1}}],[\"并使用指定的均值和方差进行归一化处理\",{\"1\":{\"264\":1}}],[\"并使用对抗损失和感知损失\",{\"1\":{\"216\":1}}],[\"并能重建图像\",{\"1\":{\"964\":1}}],[\"并能生成全局或局部视觉特征\",{\"1\":{\"304\":1}}],[\"并能实现零样本图像分类\",{\"1\":{\"269\":1}}],[\"并拆分为\",{\"1\":{\"293\":1}}],[\"并维护一个移动中心\",{\"1\":{\"293\":1}}],[\"并附加\",{\"1\":{\"293\":2}}],[\"并不代表图像的生成质量就很优秀\",{\"1\":{\"925\":1}}],[\"并不对每个相对位置都单独存一个偏置\",{\"1\":{\"710\":1}}],[\"并不对应标签或监督\",{\"1\":{\"286\":1}}],[\"并不知道\",{\"1\":{\"691\":1}}],[\"并不能直接学会如何根据文本或其他条件信息来生成对应图像\",{\"1\":{\"947\":1}}],[\"并不能帮助我们生成新的\",{\"1\":{\"942\":1}}],[\"并不能显著提升其对用户意图的理解与遵循能力\",{\"1\":{\"653\":1}}],[\"并不能保证这些矩阵是正交矩阵\",{\"1\":{\"153\":1}}],[\"并不会限制在这个代码块内部\",{\"1\":{\"444\":1}}],[\"并不依赖于特定的语言模型\",{\"1\":{\"330\":1}}],[\"并不显式包含这些特征\",{\"1\":{\"280\":1}}],[\"并行化操作和备选方案等高级功能\",{\"1\":{\"833\":1}}],[\"并行化要求更高\",{\"1\":{\"500\":1}}],[\"并行策略\",{\"1\":{\"667\":1}}],[\"并行方案\",{\"1\":{\"278\":1}}],[\"并行优化\",{\"1\":{\"157\":1,\"667\":1}}],[\"并需要额外的训练机制\",{\"1\":{\"269\":1}}],[\"并训练能适配于更广泛基准的基础模型\",{\"1\":{\"269\":1}}],[\"并转换为\",{\"1\":{\"266\":1}}],[\"并转为布尔类型\",{\"1\":{\"265\":1}}],[\"并编码为\",{\"1\":{\"266\":1}}],[\"并编码为一系列嵌入表示\",{\"1\":{\"171\":1}}],[\"并应用\",{\"1\":{\"264\":1,\"305\":1}}],[\"并用该\",{\"1\":{\"946\":1}}],[\"并用它的返回值替换\",{\"1\":{\"457\":1}}],[\"并用这些数据训练一个端到端的视觉语言模型\",{\"1\":{\"339\":1}}],[\"并用\",{\"1\":{\"273\":1,\"735\":1}}],[\"并用带因果掩码的\",{\"1\":{\"272\":1}}],[\"并用量化后的视觉\",{\"1\":{\"269\":1}}],[\"并用温度参数控制\",{\"1\":{\"257\":1}}],[\"并用其产生的软标签作为训练目标\",{\"1\":{\"172\":1}}],[\"并预测相应的视觉\",{\"1\":{\"252\":1}}],[\"并预测它\",{\"1\":{\"250\":1}}],[\"并依赖于数据增强的设计\",{\"1\":{\"246\":1}}],[\"并拉远其他图像\",{\"1\":{\"246\":1}}],[\"并采用指数加权迭代平均\",{\"1\":{\"887\":1}}],[\"并采用指数加权的迭代平均\",{\"1\":{\"886\":1}}],[\"并采用更灵活的任务无关\",{\"1\":{\"646\":1}}],[\"并采用两段式训练流程\",{\"1\":{\"626\":1}}],[\"并采用\",{\"1\":{\"236\":1}}],[\"并假设先验为均匀分布\",{\"1\":{\"235\":1}}],[\"并加入更多模态\",{\"1\":{\"225\":1}}],[\"并做\",{\"1\":{\"213\":1}}],[\"并引入高质量参考语料\",{\"1\":{\"647\":1}}],[\"并引入一个可调节的权重参数\",{\"1\":{\"592\":1}}],[\"并引入缩略图保留全局信息\",{\"1\":{\"329\":1}}],[\"并引入可学习的文本编码器来编码自由形式文本\",{\"1\":{\"271\":1}}],[\"并引入\",{\"1\":{\"208\":1,\"951\":1}}],[\"并替换开头\",{\"1\":{\"187\":1}}],[\"并剔除低质量描述\",{\"1\":{\"183\":1}}],[\"并未直接采用这些方法\",{\"1\":{\"655\":1}}],[\"并未带来额外收益\",{\"1\":{\"280\":1}}],[\"并未带来性能提升\",{\"1\":{\"181\":1}}],[\"并未充分利用点云稀疏性\",{\"1\":{\"109\":1}}],[\"并有利于多任务学习\",{\"1\":{\"171\":1}}],[\"并有望应用于机器人操作\",{\"1\":{\"72\":1}}],[\"并提供了定义工具的简便方法\",{\"1\":{\"833\":1}}],[\"并提出了一系列改进措施\",{\"1\":{\"677\":1}}],[\"并提取每个区域的特征\",{\"1\":{\"388\":1}}],[\"并提取更高级别的局部特征\",{\"1\":{\"146\":1}}],[\"并提升了整体的处理效率\",{\"1\":{\"828\":1}}],[\"并提升\",{\"1\":{\"19\":1}}],[\"并直接在同一区域内融合点信息\",{\"1\":{\"125\":1}}],[\"并且会生成一张和原向量对应图片差不多的图片\",{\"1\":{\"956\":1}}],[\"并且它确实在对整个数据集上的\",{\"1\":{\"948\":1}}],[\"并且它不是一个概率分布\",{\"1\":{\"877\":1}}],[\"并且通常会约束\",{\"1\":{\"946\":1}}],[\"并且相互独立\",{\"1\":{\"932\":1}}],[\"并且我们只拼接一部分\",{\"1\":{\"895\":1}}],[\"并且都采用了用\",{\"1\":{\"889\":1}}],[\"并且允许语言模型与其所处的环境进行互动\",{\"1\":{\"831\":1}}],[\"并且是首个开源的推理型大模型\",{\"1\":{\"823\":1}}],[\"并且具有更好的泛化能力\",{\"1\":{\"823\":1}}],[\"并且一定程度上也表达了位置的局部的相对信息\",{\"1\":{\"706\":1}}],[\"并且使用预测结果计算nsp任务损失值\",{\"1\":{\"700\":1}}],[\"并且该优势在训练标注者和\",{\"1\":{\"657\":1}}],[\"并且不会对额外的架构组件使用迁移学习\",{\"1\":{\"631\":1}}],[\"并且不需要低秩适应\",{\"1\":{\"614\":1}}],[\"并且可以避免梯度消失问题\",{\"1\":{\"497\":1}}],[\"并且默认继承原张量的数据类型\",{\"1\":{\"486\":1}}],[\"并且取得了与卷积神经网络\",{\"1\":{\"436\":1}}],[\"并且与clip模型的训练数据不完全一致\",{\"1\":{\"409\":1}}],[\"并且缓解了对大规模图文对数据的依赖\",{\"1\":{\"377\":1}}],[\"并且性能优于其他融合编码器模型\",{\"1\":{\"368\":1}}],[\"并且\",{\"1\":{\"282\":1,\"595\":1,\"611\":1,\"663\":1}}],[\"并且统一地将所有标签视为文本\",{\"1\":{\"272\":1}}],[\"并且albef\",{\"1\":{\"194\":1}}],[\"并且二者都可以看作是\",{\"1\":{\"125\":1}}],[\"并且在所有重载运算符函数实现中\",{\"1\":{\"809\":1}}],[\"并且在训练过程中采用了一个相对较大的批次大小\",{\"1\":{\"407\":1}}],[\"并且在\",{\"1\":{\"106\":1}}],[\"并结合论文图表\",{\"1\":{\"655\":1}}],[\"并结合强化学习进一步优化模型\",{\"1\":{\"653\":1}}],[\"并结合正则化损失\",{\"1\":{\"152\":1}}],[\"并结合\",{\"1\":{\"125\":1,\"286\":1,\"823\":1}}],[\"并与文本token一同输入transformer处理\",{\"1\":{\"388\":1}}],[\"并与\",{\"1\":{\"380\":1}}],[\"并与一个由\",{\"1\":{\"313\":1}}],[\"并与原始特征拼接\",{\"1\":{\"122\":1}}],[\"并与图像特征联合用于预测\",{\"1\":{\"38\":1}}],[\"并返回一个新的函数或类对象\",{\"1\":{\"450\":1}}],[\"并返回对应的\",{\"1\":{\"385\":1}}],[\"并返回\",{\"1\":{\"293\":1,\"449\":1}}],[\"并返回这些邻居点的索引\",{\"1\":{\"119\":1}}],[\"并返回局部邻域的特征\",{\"1\":{\"119\":1}}],[\"并将期望中的部分转换为另一个\",{\"1\":{\"945\":1}}],[\"并将结果传递给前一层变量\",{\"1\":{\"776\":1}}],[\"并将它们组合成一个批次进行处理\",{\"1\":{\"737\":1}}],[\"并将它们组织称为局部区域集\",{\"1\":{\"135\":1}}],[\"并将参数规模推至前所未有的高度\",{\"1\":{\"650\":1}}],[\"并将元素按照转置后的位置写入新内存\",{\"1\":{\"545\":1}}],[\"并将原张量的数据按照其逻辑顺序复制到这块新内存中\",{\"1\":{\"491\":1}}],[\"并将这些图像块嵌入到一个低维向量空间中\",{\"1\":{\"426\":1}}],[\"并将图片展示出来\",{\"1\":{\"411\":1}}],[\"并将每一帧分别送入共享的图像编码器\",{\"1\":{\"273\":1}}],[\"并将其与基于最小描述长度\",{\"1\":{\"948\":1}}],[\"并将其与nous\",{\"1\":{\"330\":1}}],[\"并将其绑定到\",{\"1\":{\"809\":1}}],[\"并将其标记为vit\",{\"1\":{\"407\":1}}],[\"并将其在单模态解码器输出中的向量作为文本嵌入\",{\"1\":{\"272\":1}}],[\"并将其编码为\",{\"1\":{\"234\":1}}],[\"并将标准的\",{\"1\":{\"233\":1}}],[\"并将维度降至\",{\"1\":{\"212\":1}}],[\"并将\",{\"1\":{\"116\":1,\"835\":1,\"886\":1}}],[\"并显著提升点云理解性能\",{\"1\":{\"110\":1}}],[\"并集\",{\"1\":{\"102\":1,\"106\":1}}],[\"并取得不错的成效\",{\"1\":{\"422\":1}}],[\"并取指数保证其\",{\"1\":{\"385\":1}}],[\"并取消掩码\",{\"1\":{\"315\":1}}],[\"并取最小值\",{\"1\":{\"121\":1}}],[\"并取\",{\"1\":{\"102\":1}}],[\"并在\",{\"1\":{\"779\":1,\"928\":1}}],[\"并在多数任务上击败palm\",{\"1\":{\"668\":1}}],[\"并在多种任务上表现良好\",{\"1\":{\"339\":1}}],[\"并在推理时通过上下文\",{\"1\":{\"646\":1}}],[\"并在7\",{\"1\":{\"643\":1}}],[\"并在最终自注意力块后增加额外层归一化\",{\"1\":{\"640\":1}}],[\"并在结尾加上特殊标记\",{\"1\":{\"595\":1}}],[\"并在被调用时依然保留这个引用\",{\"1\":{\"449\":1}}],[\"并在底层融合图像与文本\",{\"1\":{\"376\":1}}],[\"并在序列前后添加\",{\"1\":{\"371\":1}}],[\"并在序列前添加可学习的特殊\",{\"1\":{\"371\":1}}],[\"并在训练过程中更新参数\",{\"1\":{\"286\":1}}],[\"并在训练中相互蒸馏\",{\"1\":{\"283\":1}}],[\"并在其输出位置接上投影头\",{\"1\":{\"286\":1}}],[\"并在特定数据集上对参数进行微调\",{\"1\":{\"229\":1}}],[\"并在视觉与视觉\",{\"1\":{\"220\":1}}],[\"并在下游任务上微调所有参数\",{\"1\":{\"237\":1}}],[\"并在下游任务\",{\"1\":{\"217\":1}}],[\"并在每个局部区域提取特征\",{\"1\":{\"143\":1}}],[\"并在此基础上强化自身的语义表达\",{\"1\":{\"96\":1}}],[\"并在新场景下提升可供性预测能力\",{\"1\":{\"20\":1}}],[\"并仅保留\",{\"1\":{\"92\":1}}],[\"并生成连贯的文本描述\",{\"1\":{\"305\":1}}],[\"并生成两张不同的图像\",{\"1\":{\"264\":1}}],[\"并生成\",{\"1\":{\"64\":1,\"380\":1}}],[\"并利用这些\",{\"1\":{\"945\":1}}],[\"并利用这些数据集对多模态模型进行知识蒸馏\",{\"1\":{\"50\":1}}],[\"并利用\",{\"1\":{\"877\":1}}],[\"并利用多源网络图像\",{\"1\":{\"295\":1}}],[\"并利用预训练\",{\"1\":{\"19\":1}}],[\"并推广至未见场景\",{\"1\":{\"50\":1}}],[\"并联想潜在意图\",{\"1\":{\"30\":1}}],[\"并通过variable\",{\"1\":{\"809\":1}}],[\"并通过链式法则自动推导了导数\",{\"1\":{\"799\":1}}],[\"并通过模型进行前向传播\",{\"1\":{\"431\":1}}],[\"并通过线性变换映射到嵌入空间\",{\"1\":{\"427\":1}}],[\"并通过在各个模版节点预留钩子函数的方式\",{\"1\":{\"381\":1}}],[\"并通过卷积映射到指定的\",{\"1\":{\"380\":1}}],[\"并通过共享的自注意力模块实现不同模态的对齐\",{\"1\":{\"377\":1}}],[\"并通过共享自注意力实现跨模态对齐\",{\"1\":{\"368\":1}}],[\"并通过多头自注意力\",{\"1\":{\"372\":1}}],[\"并通过多尺度上采样与融合\",{\"1\":{\"70\":1}}],[\"并通过一个卷积操作将每个\",{\"1\":{\"266\":1}}],[\"并通过一个小型\",{\"1\":{\"137\":1}}],[\"并通过对大规模嘈杂图文数据进行\",{\"1\":{\"183\":1}}],[\"并通过局部+全局特征融合机制实现强大的点云建模能力\",{\"1\":{\"150\":1}}],[\"并通过功能揭示模块\",{\"1\":{\"72\":1}}],[\"并通过复合损失优化整个流程\",{\"1\":{\"32\":1}}],[\"并通过跨模态自适应融合模块\",{\"1\":{\"29\":1}}],[\"并通过引入\",{\"1\":{\"19\":1}}],[\"的运算\",{\"1\":{\"959\":1}}],[\"的运算符重载中\",{\"1\":{\"809\":1}}],[\"的技术来完成梯度复制\",{\"1\":{\"959\":1}}],[\"的技巧\",{\"1\":{\"258\":1}}],[\"的离散编码\",{\"1\":{\"958\":1,\"959\":1}}],[\"的离散编码器中\",{\"1\":{\"258\":1}}],[\"的某种平均值\",{\"1\":{\"952\":1}}],[\"的某个概率分布\",{\"1\":{\"904\":1}}],[\"的情况完全一致\",{\"1\":{\"951\":1}}],[\"的情况下生成的\",{\"1\":{\"889\":1}}],[\"的情况下优于之前专门训练的模型\",{\"1\":{\"884\":1}}],[\"的情况下\",{\"1\":{\"878\":1,\"946\":1,\"950\":1}}],[\"的情况下表现优异\",{\"1\":{\"586\":1}}],[\"的情况下会将随机垃圾邮件的垃圾邮件概率设为高于随机合法邮件的垃圾邮件概率\",{\"1\":{\"570\":1}}],[\"的调节参数\",{\"1\":{\"951\":1}}],[\"的额外信息\",{\"1\":{\"950\":1}}],[\"的曲率来说足够小\",{\"1\":{\"949\":1}}],[\"的那个函数\",{\"1\":{\"949\":1}}],[\"的角度说也行\",{\"1\":{\"959\":1}}],[\"的角度看\",{\"1\":{\"951\":1}}],[\"的角度描述\",{\"1\":{\"948\":1}}],[\"的角色进行分类\",{\"1\":{\"735\":1}}],[\"的同时\",{\"1\":{\"948\":1}}],[\"的同时保持训练的稳定性和多样性\",{\"1\":{\"207\":1}}],[\"的学习过程是可行的\",{\"1\":{\"948\":1}}],[\"的学习包含两个模块\",{\"1\":{\"232\":1}}],[\"的工作流程是这样的\",{\"1\":{\"947\":1}}],[\"的工作属于对上述方法的泛化\",{\"1\":{\"655\":1}}],[\"的下界\",{\"1\":{\"947\":1}}],[\"的右侧是\",{\"1\":{\"947\":1}}],[\"的似然概率\",{\"1\":{\"947\":1}}],[\"的似然使用\",{\"1\":{\"886\":1}}],[\"的近似误差可以为零\",{\"1\":{\"949\":1}}],[\"的近似\",{\"1\":{\"946\":1}}],[\"的潜变量\",{\"1\":{\"945\":1}}],[\"的估计几乎没有贡献\",{\"1\":{\"945\":1}}],[\"的欧式距离为\",{\"1\":{\"944\":2}}],[\"的欧几里得范数\",{\"1\":{\"506\":1}}],[\"的积分\",{\"1\":{\"944\":1}}],[\"的积分为\",{\"1\":{\"871\":1}}],[\"的顺序\",{\"1\":{\"924\":1}}],[\"的顺序排序\",{\"1\":{\"595\":1}}],[\"的顺序排列\",{\"1\":{\"547\":2}}],[\"的约束\",{\"1\":{\"923\":2}}],[\"的确\",{\"1\":{\"921\":1}}],[\"的任意可学习的确定性函数\",{\"1\":{\"946\":1}}],[\"的任意假设\",{\"1\":{\"903\":1}}],[\"的任务分布\",{\"1\":{\"656\":1}}],[\"的任务\",{\"1\":{\"269\":1}}],[\"的任务是\",{\"1\":{\"83\":1}}],[\"的展平维度\",{\"1\":{\"900\":1}}],[\"的动机来源\",{\"1\":{\"950\":1}}],[\"的动效\",{\"1\":{\"899\":1}}],[\"的动量蒸馏损失为\",{\"1\":{\"202\":1}}],[\"的幂\",{\"1\":{\"899\":1}}],[\"的集合\",{\"1\":{\"898\":1}}],[\"的采样也是很方便的\",{\"1\":{\"925\":1}}],[\"的采样\",{\"1\":{\"898\":1}}],[\"的回溯性验证\",{\"0\":{\"898\":1}}],[\"的回答\",{\"1\":{\"52\":2}}],[\"的打分\",{\"1\":{\"897\":1}}],[\"的起始\",{\"1\":{\"895\":1}}],[\"的起始图像\",{\"1\":{\"895\":1}}],[\"的放大系数\",{\"1\":{\"894\":1}}],[\"的比例来融合\",{\"1\":{\"893\":1}}],[\"的二维\",{\"1\":{\"892\":1}}],[\"的二维结构形状\",{\"1\":{\"266\":1}}],[\"的扩展版本\",{\"1\":{\"888\":1}}],[\"的初步实验中\",{\"1\":{\"887\":1}}],[\"的初始化函数\",{\"1\":{\"926\":1}}],[\"的初始化方法\",{\"1\":{\"899\":1}}],[\"的初始化方法随机初始化\",{\"1\":{\"315\":1}}],[\"的初始维度对结果的影响\",{\"0\":{\"525\":1}}],[\"的先验分布是均匀分布\",{\"1\":{\"959\":1}}],[\"的先验分布\",{\"1\":{\"887\":1}}],[\"的先验信念\",{\"1\":{\"877\":1}}],[\"的联合概率\",{\"1\":{\"885\":1}}],[\"的联合先验\",{\"1\":{\"885\":1}}],[\"的联合分布\",{\"1\":{\"885\":3}}],[\"的上下文长度减少了\",{\"1\":{\"885\":1}}],[\"的薄壳层上\",{\"1\":{\"874\":1}}],[\"的速率快速增长\",{\"1\":{\"873\":1}}],[\"的马氏距离\",{\"1\":{\"871\":1}}],[\"的版本\",{\"1\":{\"868\":1}}],[\"的柯西分布进行\",{\"1\":{\"868\":1}}],[\"的泊松分布\",{\"1\":{\"858\":1}}],[\"的条件概率\",{\"1\":{\"943\":1}}],[\"的条件下是条件独立的\",{\"1\":{\"849\":1}}],[\"的条件输入\",{\"1\":{\"188\":1}}],[\"的补集\",{\"1\":{\"848\":1}}],[\"的事件时的平均信息量\",{\"1\":{\"908\":1}}],[\"的事件\",{\"1\":{\"848\":1,\"851\":1}}],[\"的区间生成的最小\",{\"1\":{\"847\":1}}],[\"的区域\",{\"1\":{\"285\":1,\"501\":1,\"587\":1,\"735\":1}}],[\"的要求\",{\"1\":{\"847\":1}}],[\"的次数\",{\"1\":{\"846\":1}}],[\"的总概率\",{\"1\":{\"846\":1}}],[\"的总概率是多少\",{\"1\":{\"846\":1}}],[\"的总数进行归一化\",{\"1\":{\"887\":1}}],[\"的总数量\",{\"1\":{\"266\":1}}],[\"的总数\",{\"1\":{\"234\":1}}],[\"的原因\",{\"1\":{\"963\":1}}],[\"的原则\",{\"1\":{\"950\":1}}],[\"的原像\",{\"1\":{\"846\":1}}],[\"的原始像素\",{\"1\":{\"234\":1}}],[\"的向量应该能概括所有\",{\"1\":{\"960\":1}}],[\"的向量数据库\",{\"1\":{\"836\":1}}],[\"的向量映射到词汇表空间\",{\"1\":{\"420\":1}}],[\"的生态系统\",{\"1\":{\"834\":1}}],[\"的生成逻辑\",{\"1\":{\"53\":1}}],[\"的持续优化和功能迭代\",{\"1\":{\"833\":1}}],[\"的巨大成功激发了越来越多的开发者兴趣\",{\"1\":{\"831\":1}}],[\"的可能取值的了解\",{\"1\":{\"877\":1}}],[\"的可能性最大\",{\"1\":{\"903\":1}}],[\"的可能性\",{\"1\":{\"827\":1}}],[\"的可操作性特征\",{\"1\":{\"70\":1}}],[\"的战略\",{\"1\":{\"827\":1}}],[\"的概念\",{\"1\":{\"826\":1}}],[\"的概率模型\",{\"1\":{\"925\":1}}],[\"的概率是\",{\"1\":{\"850\":1}}],[\"的概率是多少\",{\"1\":{\"846\":1}}],[\"的概率都是\",{\"1\":{\"846\":1}}],[\"的概率最大\",{\"1\":{\"694\":1}}],[\"的概率随机水平翻转图像\",{\"1\":{\"425\":1}}],[\"的概率随机将输入文本中的\",{\"1\":{\"200\":1}}],[\"的概率水平翻转\",{\"1\":{\"264\":1}}],[\"的概率\",{\"1\":{\"257\":1,\"848\":1,\"857\":1,\"921\":1,\"925\":1,\"932\":1}}],[\"的概率分布\",{\"1\":{\"200\":1,\"846\":1,\"878\":1,\"898\":1}}],[\"的概率值\",{\"1\":{\"70\":1,\"102\":1,\"588\":1,\"589\":1}}],[\"的支持\",{\"1\":{\"823\":1,\"833\":1}}],[\"的卓越能力\",{\"1\":{\"823\":1}}],[\"的基础上允许通道看到自己当前值\",{\"1\":{\"924\":1}}],[\"的基础上将双向\",{\"1\":{\"171\":1}}],[\"的基础语言模型\",{\"1\":{\"823\":1}}],[\"的会话应用\",{\"1\":{\"823\":1}}],[\"的推出\",{\"1\":{\"822\":1}}],[\"的推理过程实际执行过程中\",{\"1\":{\"895\":1}}],[\"的推理水平\",{\"1\":{\"823\":1}}],[\"的推理机制\",{\"0\":{\"343\":1}}],[\"的推理能力\",{\"1\":{\"29\":1}}],[\"的深度集成\",{\"1\":{\"833\":1}}],[\"的深度学习训练系统\",{\"1\":{\"819\":1}}],[\"的深度融合\",{\"1\":{\"93\":1}}],[\"的等高线\",{\"1\":{\"816\":1}}],[\"的内存管理与执行流程\",{\"1\":{\"814\":1}}],[\"的内容向量\",{\"1\":{\"709\":2}}],[\"的完整优化目标中\",{\"1\":{\"951\":1}}],[\"的完整实现\",{\"1\":{\"809\":1}}],[\"的完整计算流程\",{\"1\":{\"419\":1}}],[\"的梯度直接原封不动地复制到\",{\"1\":{\"959\":1}}],[\"的梯度来自两个路径\",{\"1\":{\"803\":1}}],[\"的梯度就始终为\",{\"1\":{\"612\":1}}],[\"的导数值对应于gxs\",{\"1\":{\"801\":1}}],[\"的导数\",{\"1\":{\"779\":1}}],[\"的神经网络\",{\"1\":{\"949\":1}}],[\"的神经网络架构\",{\"1\":{\"741\":1}}],[\"的神经网络模型\",{\"1\":{\"629\":1}}],[\"的隐藏状态\",{\"1\":{\"733\":1}}],[\"的隐藏维度\",{\"1\":{\"429\":1,\"892\":1}}],[\"的问答任务中\",{\"1\":{\"733\":1}}],[\"的问题\",{\"1\":{\"295\":1,\"589\":2,\"657\":1,\"956\":2,\"961\":1}}],[\"的桶号\",{\"1\":{\"710\":1}}],[\"的桶索引\",{\"1\":{\"710\":1}}],[\"的做法\",{\"1\":{\"710\":2,\"924\":1}}],[\"的做法是\",{\"1\":{\"220\":1,\"709\":1}}],[\"的序列\",{\"1\":{\"710\":1,\"733\":1}}],[\"的序列挤占了下游任务的输入序列空间\",{\"1\":{\"610\":1}}],[\"的轴\",{\"1\":{\"709\":1}}],[\"的轴扩展至目标大小\",{\"1\":{\"546\":1}}],[\"的子集\",{\"1\":{\"696\":1}}],[\"的几率原封不动\",{\"1\":{\"691\":1}}],[\"的几率被替换成任意一个其它的\",{\"1\":{\"691\":1}}],[\"的几率被替换成\",{\"1\":{\"691\":1}}],[\"的时候也只计算被遮盖部分的\",{\"1\":{\"691\":1}}],[\"的时间复杂度虽然是\",{\"1\":{\"157\":1}}],[\"的单词进行掩码\",{\"1\":{\"681\":1}}],[\"的固定学习率\",{\"1\":{\"680\":1}}],[\"的固定大小特征图\",{\"1\":{\"501\":1}}],[\"的竞争力\",{\"1\":{\"666\":1}}],[\"的缓存\",{\"1\":{\"663\":1}}],[\"的出发点就在这里\",{\"1\":{\"660\":1}}],[\"的出现让人们重新思考了\",{\"1\":{\"827\":1}}],[\"的出现也掀起了新一轮的研究热潮\",{\"1\":{\"405\":1}}],[\"的出现\",{\"1\":{\"31\":1}}],[\"的通用方法\",{\"1\":{\"658\":1}}],[\"的通用性\",{\"1\":{\"640\":1}}],[\"的答案\",{\"1\":{\"657\":1}}],[\"的残差\",{\"1\":{\"656\":1}}],[\"的残差连接结构\",{\"1\":{\"120\":1}}],[\"的监督信号\",{\"1\":{\"656\":1}}],[\"的三步训练流程\",{\"1\":{\"656\":1}}],[\"的三合一操作\",{\"1\":{\"121\":1}}],[\"的详细总结\",{\"1\":{\"655\":1}}],[\"的控制\",{\"1\":{\"655\":1}}],[\"的偏移量\",{\"1\":{\"893\":1}}],[\"的偏置值\",{\"1\":{\"710\":1}}],[\"的偏置\",{\"1\":{\"710\":1}}],[\"的偏置矩阵\",{\"1\":{\"710\":1}}],[\"的偏好比为\",{\"1\":{\"657\":1}}],[\"的偏好\",{\"1\":{\"654\":1}}],[\"的偏离平均值的乘积\",{\"1\":{\"574\":1}}],[\"的理念\",{\"1\":{\"650\":1}}],[\"的假设\",{\"1\":{\"650\":1}}],[\"的提取\",{\"1\":{\"892\":1}}],[\"的提取并非依赖微调\",{\"1\":{\"648\":1}}],[\"的提出\",{\"1\":{\"368\":1}}],[\"的准确率\",{\"1\":{\"648\":1}}],[\"的准确率超越前sota\",{\"1\":{\"641\":1}}],[\"的元学习方法\",{\"1\":{\"647\":1}}],[\"的成果表明\",{\"1\":{\"646\":1}}],[\"的成功很大程度上得益于\",{\"1\":{\"280\":1}}],[\"的成功已经从\",{\"1\":{\"220\":1}}],[\"的差距已显著缩小\",{\"1\":{\"641\":1}}],[\"的差异\",{\"1\":{\"256\":1,\"260\":1,\"915\":2}}],[\"的少样本学习\",{\"1\":{\"640\":1}}],[\"的规模化改进\",{\"1\":{\"640\":1}}],[\"的规模相匹配\",{\"1\":{\"304\":1}}],[\"的高斯分布\",{\"1\":{\"943\":1,\"949\":1}}],[\"的高斯基元中实现\",{\"1\":{\"19\":1}}],[\"的高质量数据\",{\"1\":{\"640\":1}}],[\"的零样本能力依赖于任务提示\",{\"1\":{\"640\":1}}],[\"的零样本图像描述结果\",{\"1\":{\"305\":1}}],[\"的外链网页\",{\"1\":{\"640\":1}}],[\"的mqan\",{\"1\":{\"640\":1}}],[\"的mlp层\",{\"1\":{\"145\":1}}],[\"的引入是为了加速自回归模型的推理速度\",{\"1\":{\"663\":1}}],[\"的引入显著提升了语言模型的表达能力\",{\"1\":{\"640\":1}}],[\"的引用\",{\"1\":{\"451\":1}}],[\"的绝对提升\",{\"1\":{\"634\":1}}],[\"的正余弦曲线\",{\"1\":{\"633\":1}}],[\"的正方形\",{\"1\":{\"570\":1}}],[\"的添加的线性输出层来预测\",{\"1\":{\"630\":1}}],[\"的激活状态\",{\"1\":{\"630\":1}}],[\"的变化不敏感\",{\"1\":{\"960\":1}}],[\"的变化较慢\",{\"1\":{\"706\":1}}],[\"的变化较快\",{\"1\":{\"706\":1}}],[\"的变换是一个从数组里取值的操作\",{\"1\":{\"958\":1}}],[\"的变换矩阵\",{\"1\":{\"152\":2}}],[\"的变量\",{\"0\":{\"754\":1}}],[\"的变种\",{\"1\":{\"629\":1}}],[\"的本质是对训练数据的有效压缩\",{\"1\":{\"614\":1}}],[\"的过程\",{\"1\":{\"611\":1,\"925\":1}}],[\"的这种思想有点类似于残差连接\",{\"1\":{\"611\":1}}],[\"的发现\",{\"1\":{\"610\":1}}],[\"的更新量与原始参数\",{\"1\":{\"609\":1}}],[\"的微调方法\",{\"1\":{\"607\":1}}],[\"的微调方式\",{\"1\":{\"237\":1}}],[\"的极端值\",{\"1\":{\"592\":1}}],[\"的惩罚比例\",{\"1\":{\"592\":1}}],[\"的惩罚权重\",{\"1\":{\"590\":2}}],[\"的敏感度控制\",{\"1\":{\"590\":1}}],[\"的替代指标\",{\"1\":{\"586\":1}}],[\"的替代方案\",{\"1\":{\"280\":1}}],[\"的协方差矩阵\",{\"1\":{\"578\":1,\"871\":1}}],[\"的期望项\",{\"1\":{\"947\":1}}],[\"的期望替换为对\",{\"1\":{\"886\":1}}],[\"的期望\",{\"1\":{\"574\":1,\"886\":1}}],[\"的度量\",{\"1\":{\"574\":1}}],[\"的阈值\",{\"1\":{\"572\":1}}],[\"的垃圾邮件分类器仅在\",{\"1\":{\"570\":1}}],[\"的垃圾邮件分类器始终会为随机垃圾邮件分配比随机合规电子邮件更高的垃圾邮件概率\",{\"1\":{\"570\":1}}],[\"的满分\",{\"1\":{\"567\":1}}],[\"的满分时\",{\"1\":{\"567\":1}}],[\"的不平衡数据集中\",{\"1\":{\"564\":1,\"565\":1}}],[\"的代码组织为可复用的包\",{\"1\":{\"810\":1}}],[\"的代码实现对应的类是\",{\"1\":{\"380\":1}}],[\"的代码实现展开讲解\",{\"1\":{\"255\":1}}],[\"的代码实现\",{\"1\":{\"213\":1}}],[\"的代价很高\",{\"1\":{\"572\":1}}],[\"的代价高于另一种错误\",{\"1\":{\"562\":1}}],[\"的列优先\",{\"1\":{\"542\":1}}],[\"的列表\",{\"1\":{\"67\":1}}],[\"的步长是\",{\"1\":{\"542\":4}}],[\"的随机种子\",{\"1\":{\"521\":1}}],[\"的随机数生成\",{\"1\":{\"521\":1}}],[\"的随机算法\",{\"1\":{\"521\":1}}],[\"的值为\",{\"1\":{\"816\":1}}],[\"的值设置为父函数的\",{\"1\":{\"805\":1}}],[\"的值\",{\"1\":{\"502\":1,\"805\":1,\"896\":2}}],[\"的距离度量\",{\"1\":{\"578\":1}}],[\"的距离是\",{\"1\":{\"502\":4}}],[\"的距离矩阵\",{\"1\":{\"488\":1}}],[\"的边界坐标再取整\",{\"1\":{\"502\":1}}],[\"的划分方式中涉及到了取整\",{\"1\":{\"501\":1}}],[\"的小格子做最大池化\",{\"1\":{\"501\":1}}],[\"的网格\",{\"1\":{\"501\":1}}],[\"的网络结构遵循\",{\"1\":{\"236\":1}}],[\"的网络结构可视化理解\",{\"1\":{\"99\":1}}],[\"的函数\",{\"1\":{\"500\":1,\"846\":1,\"877\":1}}],[\"的relu网络可以构造具有\",{\"1\":{\"500\":1}}],[\"的连续函数\",{\"1\":{\"946\":1}}],[\"的连续张量\",{\"1\":{\"489\":1}}],[\"的连续梯度\",{\"1\":{\"258\":1}}],[\"的全零张量\",{\"1\":{\"486\":2}}],[\"的全局特征来自于\",{\"1\":{\"157\":1}}],[\"的整体优化目标\",{\"1\":{\"959\":1}}],[\"的整体结构是一个典型的\",{\"1\":{\"144\":1}}],[\"的整数\",{\"1\":{\"483\":1,\"710\":1}}],[\"的简写\",{\"1\":{\"475\":1}}],[\"的泛化\",{\"1\":{\"468\":1,\"886\":1}}],[\"的意思是\",{\"1\":{\"449\":1}}],[\"的形式衰减\",{\"1\":{\"873\":1}}],[\"的形式明确指定的参数\",{\"1\":{\"445\":1}}],[\"的形式\",{\"1\":{\"426\":1,\"932\":1,\"946\":1}}],[\"的形状和位置\",{\"1\":{\"500\":1}}],[\"的形状是\",{\"1\":{\"364\":1,\"709\":1}}],[\"的形状为\",{\"1\":{\"69\":1}}],[\"的generate方法负责完成图像描述生成\",{\"1\":{\"421\":1}}],[\"的text\",{\"1\":{\"421\":1}}],[\"的真实梯度\",{\"1\":{\"946\":1}}],[\"的真实验证\",{\"1\":{\"886\":1}}],[\"的真实导数就会被低估一半\",{\"1\":{\"803\":1}}],[\"的真实值\",{\"1\":{\"420\":1}}],[\"的真正例率\",{\"1\":{\"569\":1}}],[\"的真正价值\",{\"1\":{\"181\":1}}],[\"的相似程度\",{\"1\":{\"947\":1}}],[\"的相似度\",{\"1\":{\"418\":1,\"526\":1,\"900\":1}}],[\"的相对距离找到对应的桶\",{\"1\":{\"710\":1}}],[\"的相对距离\",{\"1\":{\"709\":1,\"710\":1}}],[\"的相对位置\",{\"1\":{\"708\":1,\"710\":1}}],[\"的相同通道进行混合\",{\"1\":{\"97\":1}}],[\"的zero\",{\"1\":{\"413\":1,\"650\":1}}],[\"的架构设计变得更加条理清晰和稳固\",{\"1\":{\"833\":1}}],[\"的架构和训练方法\",{\"1\":{\"679\":1}}],[\"的架构中融入卷积操作\",{\"1\":{\"434\":1}}],[\"的架构\",{\"1\":{\"410\":1}}],[\"的vit\",{\"1\":{\"407\":1}}],[\"的挖掘\",{\"1\":{\"386\":1}}],[\"的日志记录\",{\"1\":{\"385\":1}}],[\"的缩放因子\",{\"1\":{\"385\":1}}],[\"的缩写\",{\"1\":{\"107\":1,\"432\":2,\"510\":1}}],[\"的标签\",{\"1\":{\"385\":1}}],[\"的标签平滑\",{\"1\":{\"172\":1}}],[\"的池化特征\",{\"1\":{\"384\":1}}],[\"的构建\",{\"1\":{\"380\":1}}],[\"的共享骨干能够自然融合文本与图像表示\",{\"1\":{\"377\":1}}],[\"的共享自注意力效果更好\",{\"1\":{\"376\":1}}],[\"的共享结构特征\",{\"1\":{\"53\":1}}],[\"的图片\",{\"1\":{\"875\":1}}],[\"的图文匹配任务\",{\"1\":{\"376\":1}}],[\"的图像时\",{\"1\":{\"889\":1}}],[\"的图像内容\",{\"1\":{\"293\":1}}],[\"的图像分辨率和\",{\"1\":{\"272\":1}}],[\"的图像分辨率输入\",{\"1\":{\"236\":1}}],[\"的图像输入\",{\"1\":{\"265\":1}}],[\"的图像变换\",{\"1\":{\"264\":1}}],[\"的图像模型\",{\"1\":{\"250\":1}}],[\"的图像划分为\",{\"1\":{\"231\":1}}],[\"的图像块矩阵添加二维\",{\"1\":{\"428\":1}}],[\"的图像块\",{\"1\":{\"228\":1}}],[\"的图像\",{\"1\":{\"214\":1,\"223\":1,\"234\":1,\"235\":1,\"885\":1,\"923\":1,\"947\":1}}],[\"的消融实验\",{\"1\":{\"376\":1}}],[\"的注意力权重\",{\"1\":{\"582\":2}}],[\"的注意力模块和视觉专家\",{\"1\":{\"374\":1}}],[\"的注意力图\",{\"1\":{\"243\":1}}],[\"的局部特征提取能力快速捕捉图像的底层特征\",{\"1\":{\"434\":1}}],[\"的局部采样\",{\"1\":{\"373\":1}}],[\"的局限\",{\"1\":{\"303\":1}}],[\"的和\",{\"1\":{\"371\":1}}],[\"的空间一致性\",{\"1\":{\"588\":1}}],[\"的空间\",{\"1\":{\"357\":1}}],[\"的空间维度为序列维度\",{\"1\":{\"266\":1}}],[\"的信念的机制\",{\"1\":{\"853\":1}}],[\"的信号空间不一致\",{\"1\":{\"354\":1}}],[\"的信息让模型分开上下句\",{\"1\":{\"692\":1}}],[\"的信息\",{\"1\":{\"100\":1}}],[\"的改变会非常缓慢\",{\"1\":{\"351\":1}}],[\"的改进\",{\"1\":{\"167\":1,\"589\":1,\"640\":1}}],[\"的改进版\",{\"1\":{\"102\":1,\"589\":1}}],[\"的参数是连续且可导的\",{\"1\":{\"946\":1}}],[\"的参数是需要从头开始学习的\",{\"1\":{\"694\":1}}],[\"的参数会发生更新\",{\"1\":{\"612\":1}}],[\"的参数叠加\",{\"1\":{\"611\":1}}],[\"的参数不变\",{\"1\":{\"604\":1}}],[\"的参数初始化\",{\"1\":{\"376\":1}}],[\"的参数初始化自注意力模块和所有模态专家\",{\"1\":{\"376\":1}}],[\"的参数\",{\"1\":{\"342\":1,\"380\":1,\"609\":1,\"611\":1,\"946\":2}}],[\"的指令模型\",{\"1\":{\"823\":1}}],[\"的指令调优版本\",{\"1\":{\"341\":1,\"342\":1}}],[\"的指代准确率\",{\"1\":{\"668\":1}}],[\"的指导\",{\"1\":{\"204\":1}}],[\"的短语\",{\"1\":{\"341\":2}}],[\"的互联网爬取图像\",{\"1\":{\"330\":1}}],[\"的多元高斯分布\",{\"1\":{\"944\":1}}],[\"的多样性\",{\"1\":{\"895\":1}}],[\"的多输入实现\",{\"0\":{\"801\":1}}],[\"的多头注意力里\",{\"1\":{\"710\":1}}],[\"的多种呈现方式\",{\"1\":{\"547\":1}}],[\"的多模态对话能力还体现在\",{\"1\":{\"310\":1}}],[\"的多视图增强策略\",{\"1\":{\"293\":1}}],[\"的一致性\",{\"1\":{\"587\":1}}],[\"的一个变种\",{\"1\":{\"936\":1}}],[\"的一个下界\",{\"1\":{\"925\":1}}],[\"的一个划分\",{\"1\":{\"850\":1}}],[\"的一个模块\",{\"1\":{\"831\":1}}],[\"的一个杰出应用就是\",{\"1\":{\"822\":1}}],[\"的一个标量偏置\",{\"1\":{\"710\":1}}],[\"的一个\",{\"1\":{\"486\":1}}],[\"的一个参数\",{\"1\":{\"351\":1}}],[\"的一维张量\",{\"1\":{\"483\":1}}],[\"的一系列二分类问题又转为了多分类问题\",{\"1\":{\"355\":1}}],[\"的一种有用指标\",{\"1\":{\"947\":1}}],[\"的一种早期形式\",{\"1\":{\"827\":1}}],[\"的一种泛化形式\",{\"1\":{\"590\":1}}],[\"的一种改进或变体\",{\"1\":{\"355\":1}}],[\"的一种变体\",{\"1\":{\"259\":1}}],[\"的一部分\",{\"1\":{\"306\":1}}],[\"的每一层\",{\"1\":{\"306\":1}}],[\"的每个元素都是一一对应的\",{\"1\":{\"801\":1}}],[\"的每个\",{\"1\":{\"171\":1,\"271\":1}}],[\"的涌现进一步加速了多模态研究的进程\",{\"1\":{\"299\":1}}],[\"的快速发展推动了通用人工智能\",{\"1\":{\"296\":1}}],[\"的教师温度\",{\"1\":{\"293\":1}}],[\"的教师模型在训练过程中是\",{\"1\":{\"283\":1}}],[\"的温度系数\",{\"1\":{\"898\":1}}],[\"的温度\",{\"1\":{\"293\":1}}],[\"的温度参数\",{\"1\":{\"255\":1,\"895\":1}}],[\"的数组\",{\"1\":{\"545\":1}}],[\"的数值会很大\",{\"1\":{\"537\":1}}],[\"的数学公式\",{\"1\":{\"522\":1}}],[\"的数据上进行预训练\",{\"1\":{\"823\":1}}],[\"的数据\",{\"1\":{\"293\":1,\"382\":1}}],[\"的数量\",{\"1\":{\"896\":1,\"900\":1}}],[\"的数量差别很大\",{\"1\":{\"893\":1}}],[\"的数量相同\",{\"1\":{\"232\":1}}],[\"的数量与图像\",{\"1\":{\"214\":1,\"232\":1}}],[\"的宽度\",{\"1\":{\"293\":1}}],[\"的场景可能传进来的是多个张量\",{\"1\":{\"293\":1}}],[\"的吞吐率为\",{\"1\":{\"288\":1}}],[\"的吞吐率仅\",{\"1\":{\"288\":1}}],[\"的增强方式\",{\"1\":{\"286\":1}}],[\"的机制\",{\"1\":{\"286\":1,\"955\":1}}],[\"的观点\",{\"1\":{\"282\":1}}],[\"的重合部分\",{\"1\":{\"586\":1}}],[\"的重合度\",{\"1\":{\"102\":1}}],[\"的重要性验证\",{\"1\":{\"311\":1}}],[\"的重要性\",{\"1\":{\"280\":1}}],[\"的语言结构\",{\"1\":{\"898\":1}}],[\"的语言模型\",{\"1\":{\"690\":1}}],[\"的语言建模\",{\"1\":{\"280\":1}}],[\"的语义而不是\",{\"1\":{\"691\":1}}],[\"的语义特征\",{\"1\":{\"212\":1}}],[\"的语义视觉分词器\",{\"1\":{\"210\":1}}],[\"的语义\",{\"1\":{\"65\":2,\"691\":1}}],[\"的语义知识与泛化能力来增强\",{\"1\":{\"19\":1}}],[\"的掩码语言模型目标在优化后仍具竞争力\",{\"1\":{\"678\":1}}],[\"的掩码预测\",{\"1\":{\"280\":1}}],[\"的掩码图像建模方法\",{\"1\":{\"368\":1}}],[\"的掩码图像建模\",{\"1\":{\"212\":1}}],[\"的能力相结合\",{\"1\":{\"833\":1}}],[\"的能力\",{\"1\":{\"276\":1,\"420\":1,\"823\":1,\"932\":1}}],[\"的嵌入向量\",{\"1\":{\"898\":1}}],[\"的嵌入取平均\",{\"1\":{\"273\":1}}],[\"的嵌入维度\",{\"1\":{\"255\":1}}],[\"的设定\",{\"1\":{\"272\":1,\"647\":1}}],[\"的设计初衷之一就是在潜在空间中学习一个接近于标准正态分布\",{\"1\":{\"935\":1}}],[\"的设计\",{\"1\":{\"250\":1,\"346\":1,\"640\":1}}],[\"的解码器架构结合生成式损失\",{\"1\":{\"269\":1}}],[\"的解码器被一分为二\",{\"1\":{\"268\":1}}],[\"的解决方案是\",{\"1\":{\"228\":1}}],[\"的卷积操作来提取\",{\"1\":{\"266\":1}}],[\"的误差\",{\"1\":{\"265\":1,\"959\":1}}],[\"的词表大小\",{\"1\":{\"892\":1}}],[\"的词表大小为\",{\"1\":{\"236\":1}}],[\"的词嵌入向量上\",{\"1\":{\"706\":1}}],[\"的词执行掩码策略\",{\"1\":{\"698\":1}}],[\"的词\",{\"1\":{\"264\":1}}],[\"的张量转置为\",{\"1\":{\"545\":1}}],[\"的张量视图\",{\"1\":{\"469\":1}}],[\"的张量\",{\"1\":{\"264\":1,\"425\":2,\"426\":2,\"540\":1,\"544\":1}}],[\"的典型问题\",{\"1\":{\"262\":1}}],[\"的鲁棒性的损失函数\",{\"1\":{\"259\":1}}],[\"的平滑性与\",{\"1\":{\"259\":1}}],[\"的优化变得困难\",{\"1\":{\"886\":1}}],[\"的优点\",{\"1\":{\"259\":1,\"587\":1}}],[\"的优势\",{\"1\":{\"125\":1,\"157\":1,\"225\":1,\"434\":1,\"586\":1}}],[\"的种类数\",{\"1\":{\"255\":1}}],[\"的像素\",{\"1\":{\"250\":1,\"924\":1}}],[\"的后续工作之一\",{\"1\":{\"250\":1}}],[\"的思想与之有相通之处\",{\"1\":{\"614\":1}}],[\"的思想很简单\",{\"1\":{\"611\":1}}],[\"的思想有深刻的相似性\",{\"1\":{\"500\":1}}],[\"的思想\",{\"1\":{\"250\":1,\"282\":1}}],[\"的思路上提出统一的对比目标\",{\"1\":{\"269\":1}}],[\"的思路\",{\"1\":{\"235\":1}}],[\"的稳定训练非常关键\",{\"1\":{\"236\":1}}],[\"的配置\",{\"1\":{\"236\":1}}],[\"的证据下界\",{\"1\":{\"235\":1}}],[\"的启发下\",{\"1\":{\"282\":1}}],[\"的启发\",{\"1\":{\"234\":1}}],[\"的编码和对应位置的可学习位置编码向量相加\",{\"1\":{\"707\":1}}],[\"的编码向量中的\",{\"1\":{\"706\":1}}],[\"的编码向量为\",{\"1\":{\"706\":1}}],[\"的编码器和解码器\",{\"1\":{\"958\":1}}],[\"的编码器其实不会显式地输出离散编码\",{\"1\":{\"958\":1}}],[\"的编码器\",{\"1\":{\"357\":1}}],[\"的编码器被用作\",{\"1\":{\"210\":1}}],[\"的编码表示\",{\"1\":{\"234\":1}}],[\"的方向进化\",{\"1\":{\"814\":1}}],[\"的方法主要基于\",{\"1\":{\"656\":1}}],[\"的方法论\",{\"1\":{\"650\":1}}],[\"的方法可能行不通\",{\"1\":{\"354\":1}}],[\"的方法不同\",{\"1\":{\"283\":1}}],[\"的方法\",{\"1\":{\"233\":1,\"339\":1,\"377\":1,\"576\":1,\"681\":1,\"956\":1}}],[\"的方式也没变\",{\"1\":{\"951\":1}}],[\"的方式不仅降低了学习成本\",{\"1\":{\"809\":1}}],[\"的方式会把prompt搞得很长\",{\"1\":{\"601\":1}}],[\"的方式去读这个数组\",{\"1\":{\"545\":1}}],[\"的方式克服了这一问题\",{\"1\":{\"234\":1}}],[\"的方式进行视觉预训练\",{\"1\":{\"234\":1}}],[\"的方式优化数据质量\",{\"1\":{\"167\":1}}],[\"的方式\",{\"1\":{\"152\":1,\"544\":1,\"619\":1,\"710\":1}}],[\"的方式重构图像\",{\"1\":{\"65\":1}}],[\"的视觉专家和自注意力模块\",{\"1\":{\"376\":1}}],[\"的视觉编码器\",{\"1\":{\"304\":1}}],[\"的视觉\",{\"1\":{\"232\":1}}],[\"的视觉模型架构\",{\"1\":{\"97\":1}}],[\"的模型战略形成了\",{\"1\":{\"823\":1}}],[\"的模型\",{\"1\":{\"658\":1,\"735\":1,\"942\":1}}],[\"的模态\",{\"1\":{\"222\":1}}],[\"的模块\",{\"1\":{\"152\":1}}],[\"的趋势\",{\"1\":{\"220\":1}}],[\"的预训练效率和下游任务表现\",{\"1\":{\"679\":1}}],[\"的预训练效果\",{\"1\":{\"217\":1}}],[\"的预训练权重\",{\"1\":{\"305\":1,\"315\":1}}],[\"的预训练后\",{\"1\":{\"237\":1}}],[\"的预训练可以看作是变分自编码器\",{\"1\":{\"235\":1}}],[\"的预训练\",{\"0\":{\"234\":1}}],[\"的预测损失\",{\"1\":{\"893\":1}}],[\"的预测\",{\"1\":{\"420\":1}}],[\"的预测能力\",{\"1\":{\"215\":1,\"641\":1}}],[\"的预测分布\",{\"1\":{\"208\":1}}],[\"的预测结果\",{\"1\":{\"168\":1,\"266\":1,\"343\":1}}],[\"的感知模型\",{\"1\":{\"216\":1}}],[\"的遮挡预测\",{\"1\":{\"214\":1}}],[\"的训练阶段分为五个阶段\",{\"1\":{\"963\":1}}],[\"的训练中\",{\"1\":{\"893\":1}}],[\"的训练存在显著不足\",{\"1\":{\"678\":1}}],[\"的训练数据主要来自以下两个来源\",{\"1\":{\"656\":1}}],[\"的训练过程中\",{\"1\":{\"611\":1}}],[\"的训练样本中采样困难负样本\",{\"1\":{\"376\":2}}],[\"的训练样本中采样\",{\"1\":{\"373\":1}}],[\"的训练目标兼顾了\",{\"1\":{\"268\":1}}],[\"的训练目标\",{\"1\":{\"264\":1,\"265\":1}}],[\"的训练集上进行预训练\",{\"1\":{\"236\":1}}],[\"的训练\",{\"1\":{\"235\":1}}],[\"的训练损失为\",{\"1\":{\"214\":1}}],[\"的训练流程\",{\"1\":{\"187\":1}}],[\"的策略\",{\"1\":{\"214\":1}}],[\"的操作流程可以分为三个步骤\",{\"1\":{\"501\":1}}],[\"的操作\",{\"1\":{\"213\":1,\"354\":1}}],[\"的新判断\",{\"1\":{\"877\":1}}],[\"的新范式\",{\"1\":{\"650\":1}}],[\"的新视图\",{\"1\":{\"544\":1}}],[\"的新视角\",{\"1\":{\"168\":1}}],[\"的新的簇中心\",{\"1\":{\"213\":1}}],[\"的使用率\",{\"1\":{\"262\":1}}],[\"的使用计数\",{\"1\":{\"213\":1}}],[\"的使用频率\",{\"1\":{\"213\":1}}],[\"的使用更具计算效率\",{\"1\":{\"171\":1}}],[\"的维度复制多次\",{\"1\":{\"951\":1}}],[\"的维度决定\",{\"1\":{\"528\":1}}],[\"的维度对最终注意力输出的结果维度有直接影响\",{\"1\":{\"523\":1}}],[\"的维度命名\",{\"1\":{\"475\":1}}],[\"的维度数相同\",{\"1\":{\"471\":1}}],[\"的维度\",{\"1\":{\"380\":1,\"524\":2,\"709\":2,\"892\":1}}],[\"的维度问题\",{\"1\":{\"119\":1}}],[\"的维数\",{\"1\":{\"213\":1}}],[\"的迭代次数\",{\"1\":{\"213\":1}}],[\"的均值归一化\",{\"1\":{\"522\":1}}],[\"的均值\",{\"1\":{\"213\":1}}],[\"的衰减系数\",{\"1\":{\"213\":1}}],[\"的最大值\",{\"1\":{\"904\":1}}],[\"的最大值处即对应\",{\"1\":{\"904\":1}}],[\"的最大序列长度\",{\"1\":{\"893\":1}}],[\"的最大缺陷在于它\",{\"1\":{\"157\":1}}],[\"的最后一个非思维链模型\",{\"1\":{\"823\":1}}],[\"的最终输出为这些采样点值的聚合结果\",{\"1\":{\"502\":1}}],[\"的最终编码向量表示图文对\",{\"1\":{\"375\":1}}],[\"的最终编码表示\",{\"1\":{\"233\":1}}],[\"的最终隐藏向量作为表示输入分类器\",{\"1\":{\"373\":1}}],[\"的最近邻查找\",{\"1\":{\"216\":1}}],[\"的最近邻进行量化\",{\"1\":{\"212\":1}}],[\"的位置为\",{\"1\":{\"896\":1}}],[\"的位置编号\",{\"1\":{\"892\":1}}],[\"的位置预测结果\",{\"1\":{\"266\":1}}],[\"的位置进行预测\",{\"1\":{\"265\":1}}],[\"的位置的预测结果\",{\"1\":{\"265\":1}}],[\"的位置有效\",{\"1\":{\"208\":1}}],[\"的位置计算\",{\"1\":{\"208\":1}}],[\"的位置\",{\"1\":{\"208\":3,\"265\":1,\"266\":1,\"384\":1,\"472\":1,\"544\":1,\"709\":1,\"733\":1,\"900\":1}}],[\"的权重降低\",{\"1\":{\"589\":1}}],[\"的权重系数\",{\"1\":{\"213\":1}}],[\"的权重\",{\"1\":{\"208\":1,\"590\":2}}],[\"的交并比\",{\"1\":{\"587\":1}}],[\"的交互\",{\"1\":{\"427\":1}}],[\"的交叉熵损失分别按批次中该类型\",{\"1\":{\"887\":1}}],[\"的交叉熵损失\",{\"1\":{\"201\":1,\"207\":1,\"384\":1}}],[\"的交集\",{\"1\":{\"102\":1,\"588\":1}}],[\"的文本对\",{\"1\":{\"733\":1}}],[\"的文本生成能力\",{\"1\":{\"421\":1}}],[\"的文本化表示\",{\"1\":{\"341\":1}}],[\"的文本\",{\"1\":{\"200\":1,\"202\":1}}],[\"的前辈模型\",{\"1\":{\"950\":1}}],[\"的前提下\",{\"1\":{\"877\":1,\"878\":2}}],[\"的前馈网络替换为\",{\"1\":{\"372\":1}}],[\"的前馈网络为模态专家池\",{\"1\":{\"368\":1}}],[\"的前向传播方法通过将输入图像切分为\",{\"1\":{\"266\":1}}],[\"的前\",{\"1\":{\"197\":1}}],[\"的前缀\",{\"1\":{\"188\":1}}],[\"的样本\",{\"1\":{\"952\":1}}],[\"的样本挑出来\",{\"1\":{\"213\":1}}],[\"的样本索引\",{\"1\":{\"190\":1}}],[\"的样本数量\",{\"1\":{\"213\":1}}],[\"的样本数量与\",{\"1\":{\"181\":1}}],[\"的样本数\",{\"1\":{\"106\":1}}],[\"的效果会好于其它几种方法\",{\"1\":{\"610\":1}}],[\"的效果最好\",{\"1\":{\"286\":1}}],[\"的效果\",{\"1\":{\"183\":1,\"409\":1}}],[\"的性能提升\",{\"1\":{\"409\":1}}],[\"的性能提升并非源于更长的训练时间\",{\"0\":{\"181\":1}}],[\"的性能会显著下降\",{\"1\":{\"157\":1}}],[\"的对角线\",{\"1\":{\"570\":1}}],[\"的对齐\",{\"1\":{\"402\":1}}],[\"的对应指标\",{\"1\":{\"384\":1}}],[\"的对应关系\",{\"1\":{\"83\":1}}],[\"的对数似然\",{\"1\":{\"234\":1}}],[\"的对比如下\",{\"1\":{\"811\":1}}],[\"的对比\",{\"1\":{\"177\":1,\"588\":1}}],[\"的大规模网页数据集\",{\"1\":{\"176\":1}}],[\"的大小\",{\"1\":{\"152\":1,\"213\":3,\"231\":1,\"266\":1,\"892\":1,\"935\":1}}],[\"的贡献包括\",{\"1\":{\"228\":1}}],[\"的贡献\",{\"1\":{\"169\":1,\"376\":1}}],[\"的主要缺陷\",{\"1\":{\"157\":1}}],[\"的主要局限在于其多步推理机制带来了较高的计算复杂度\",{\"1\":{\"50\":1}}],[\"的实验表明移除\",{\"1\":{\"679\":1}}],[\"的实验结果充分证明了其设计策略的有效性\",{\"1\":{\"312\":1}}],[\"的实验仅需1张gpu即可运行\",{\"1\":{\"292\":1}}],[\"的实验\",{\"1\":{\"157\":1}}],[\"的实现逻辑\",{\"1\":{\"804\":1}}],[\"的实现逻辑总是将函数追加到待处理列表的末尾\",{\"1\":{\"804\":1}}],[\"的实现里\",{\"1\":{\"382\":1}}],[\"的实现\",{\"1\":{\"96\":1,\"97\":1,\"809\":4}}],[\"的输出比\",{\"1\":{\"658\":1}}],[\"的输出用于捕获整个句子的全局语义信息\",{\"1\":{\"498\":1}}],[\"的输出向量能够很好地表示图像的全局特征\",{\"1\":{\"427\":2}}],[\"的输出向量被输入到分类头中\",{\"1\":{\"427\":1}}],[\"的输出向量作为图像和文本的聚合表示\",{\"1\":{\"373\":1}}],[\"的输出蕴含了视觉信息\",{\"1\":{\"421\":1}}],[\"的输出\",{\"1\":{\"266\":1,\"280\":1,\"285\":1,\"385\":1,\"398\":1,\"421\":1,\"427\":1,\"733\":1}}],[\"的输出仅由一个不超过\",{\"1\":{\"157\":1}}],[\"的输入格式\",{\"1\":{\"893\":1}}],[\"的输入方式是\",{\"1\":{\"737\":1}}],[\"的输入组织形式与普通分类或问答任务略有不同\",{\"1\":{\"737\":1}}],[\"的输入序列\",{\"1\":{\"707\":1,\"737\":1}}],[\"的输入\",{\"1\":{\"264\":1,\"385\":1,\"420\":1,\"679\":1,\"893\":1}}],[\"的输入是一系列图像\",{\"1\":{\"233\":1}}],[\"的输入特征\",{\"1\":{\"231\":1}}],[\"的输入嵌入\",{\"1\":{\"212\":1}}],[\"的输入和标签副本\",{\"1\":{\"208\":1}}],[\"的输入起点\",{\"1\":{\"188\":1}}],[\"的输入为\",{\"1\":{\"32\":1}}],[\"的限制\",{\"1\":{\"157\":1}}],[\"的表达式为\",{\"1\":{\"867\":1}}],[\"的表达能力\",{\"1\":{\"532\":1}}],[\"的表达能力受\",{\"1\":{\"157\":1}}],[\"的表现优于\",{\"1\":{\"309\":1}}],[\"的表现与最新的自监督系统相当\",{\"1\":{\"280\":1}}],[\"的表现不如基于图结构的模型\",{\"1\":{\"157\":1}}],[\"的表示当前激活的环境\",{\"1\":{\"553\":1}}],[\"的表示能力\",{\"1\":{\"262\":1}}],[\"的表示\",{\"1\":{\"238\":1,\"274\":1,\"286\":1,\"699\":1,\"733\":1}}],[\"的分布规律\",{\"1\":{\"964\":1}}],[\"的分布假设\",{\"1\":{\"951\":1}}],[\"的分布参数\",{\"1\":{\"947\":1}}],[\"的分布\",{\"1\":{\"877\":1,\"878\":1,\"885\":1,\"935\":1,\"964\":2}}],[\"的分布被称为半正态分布\",{\"1\":{\"866\":1}}],[\"的分工模式\",{\"1\":{\"706\":1}}],[\"的分辨率\",{\"1\":{\"293\":1}}],[\"的分类任务\",{\"1\":{\"514\":1}}],[\"的分类准确率略低于\",{\"1\":{\"157\":1}}],[\"的分类模块\",{\"1\":{\"155\":1}}],[\"的分割网络将全局特征复制\",{\"1\":{\"157\":1}}],[\"的分割模块通过拼接全局特征\",{\"1\":{\"157\":1}}],[\"的分割模块\",{\"1\":{\"156\":1}}],[\"的作用就像是一个正则化参数\",{\"1\":{\"951\":1}}],[\"的作用是用\",{\"1\":{\"692\":1}}],[\"的作用是通过训练过程中损失值的降低\",{\"1\":{\"427\":1}}],[\"的作用域遵循\",{\"1\":{\"444\":1}}],[\"的作用\",{\"1\":{\"153\":1,\"202\":1,\"385\":1,\"577\":1,\"657\":1,\"951\":1}}],[\"的创新点在于动态分辨率\",{\"1\":{\"327\":1}}],[\"的创新点\",{\"1\":{\"148\":1}}],[\"的核心部分\",{\"1\":{\"819\":1}}],[\"的核心能力\",{\"1\":{\"799\":1}}],[\"的核心\",{\"1\":{\"741\":1}}],[\"的核心实验基于真实用户提交的指令性\",{\"1\":{\"657\":1}}],[\"的核心技术基础是\",{\"1\":{\"655\":1}}],[\"的核心思想\",{\"1\":{\"594\":1}}],[\"的核心思想是\",{\"1\":{\"97\":1,\"225\":1,\"339\":1,\"620\":1}}],[\"的核心是\",{\"1\":{\"590\":1}}],[\"的核心组件\",{\"1\":{\"327\":1}}],[\"的核心特征提取模块\",{\"1\":{\"154\":1}}],[\"的核心模块\",{\"1\":{\"145\":1}}],[\"的核心就是逐层提取局部特征\",{\"1\":{\"138\":1}}],[\"的第一个卷积层使用\",{\"1\":{\"923\":1}}],[\"的第一阶段中\",{\"1\":{\"235\":1}}],[\"的第\",{\"1\":{\"137\":1}}],[\"的索引整体加偏移\",{\"1\":{\"893\":1}}],[\"的索引必须是\",{\"1\":{\"709\":1}}],[\"的索引数组\",{\"1\":{\"137\":1}}],[\"的索引\",{\"1\":{\"137\":1,\"232\":1,\"699\":1}}],[\"的所有子集的集合\",{\"1\":{\"845\":1}}],[\"的所有单词\",{\"1\":{\"595\":1}}],[\"的所有\",{\"1\":{\"385\":1}}],[\"的所有邻近点\",{\"1\":{\"137\":1}}],[\"的所有通道进行处理\",{\"1\":{\"97\":1}}],[\"的特点与能力\",{\"0\":{\"824\":1}}],[\"的特点\",{\"1\":{\"369\":1}}],[\"的特例\",{\"1\":{\"125\":1,\"611\":1}}],[\"的特征图\",{\"1\":{\"426\":1,\"501\":1,\"502\":1}}],[\"的特征表示\",{\"1\":{\"362\":1}}],[\"的特征从队列中抽离\",{\"1\":{\"353\":1}}],[\"的特征进行汇总\",{\"1\":{\"142\":1}}],[\"的特征进行对齐\",{\"1\":{\"53\":1}}],[\"的特征映射回更高分辨率点集\",{\"1\":{\"116\":1}}],[\"的特征汇聚到\",{\"1\":{\"116\":1}}],[\"的特征\",{\"1\":{\"83\":1,\"122\":1,\"131\":1}}],[\"的结束索引\",{\"1\":{\"293\":1}}],[\"的结构过于简单\",{\"1\":{\"157\":1}}],[\"的结构\",{\"1\":{\"137\":1,\"945\":1}}],[\"的结合也很简单\",{\"1\":{\"611\":1}}],[\"的结合\",{\"1\":{\"125\":1}}],[\"的结果相加\",{\"1\":{\"709\":1}}],[\"的结果证明了\",{\"1\":{\"657\":1}}],[\"的结果\",{\"1\":{\"106\":1}}],[\"的点表示给定模型效果最佳的阈值范围\",{\"1\":{\"572\":1}}],[\"的点缺失\",{\"1\":{\"150\":1}}],[\"的点集群都将独立地送入对应的pointnet网络进行特征提取\",{\"1\":{\"140\":1}}],[\"的点及其特征\",{\"1\":{\"137\":1}}],[\"的点全部替换为\",{\"1\":{\"137\":1}}],[\"的点\",{\"1\":{\"137\":1}}],[\"的点累积偏移\",{\"1\":{\"123\":1}}],[\"的点数累积和\",{\"1\":{\"122\":2}}],[\"的点云和功能标注\",{\"1\":{\"93\":1}}],[\"的组合操作\",{\"1\":{\"122\":1}}],[\"的组合形式\",{\"1\":{\"102\":1}}],[\"的自监督系统\",{\"1\":{\"280\":1}}],[\"的自监督预训练方法\",{\"1\":{\"228\":1,\"252\":1}}],[\"的自注意力模块中读取\",{\"1\":{\"280\":1}}],[\"的自注意力机制可以自动学会区分图像中的语义区域\",{\"1\":{\"243\":1}}],[\"的自注意力机制天然适合点云\",{\"1\":{\"109\":1}}],[\"的自定义pytorch函数\",{\"1\":{\"121\":1}}],[\"的线性变换\",{\"1\":{\"119\":1}}],[\"的选择\",{\"1\":{\"117\":1}}],[\"的影响就越大\",{\"1\":{\"590\":2}}],[\"的影响\",{\"1\":{\"117\":1,\"612\":1}}],[\"的两个分支中都加了\",{\"1\":{\"114\":1}}],[\"的计算中增加一个旁路\",{\"1\":{\"611\":1}}],[\"的计算链\",{\"1\":{\"522\":1}}],[\"的计算步骤可以总结为以下几个核心环节\",{\"1\":{\"213\":1}}],[\"的计算公式为\",{\"1\":{\"112\":1}}],[\"的计算流程如下\",{\"1\":{\"106\":1}}],[\"的加入使得采样变为\",{\"1\":{\"897\":1}}],[\"的加入是为了让\",{\"1\":{\"102\":1}}],[\"的加权组合\",{\"1\":{\"592\":1}}],[\"的加权信息\",{\"1\":{\"529\":1}}],[\"的加权比固定\",{\"1\":{\"204\":1}}],[\"的加权和逼近目标函数\",{\"1\":{\"500\":1}}],[\"的加权和\",{\"1\":{\"102\":1,\"587\":1}}],[\"的关键思想是\",{\"1\":{\"945\":1}}],[\"的关键一步\",{\"1\":{\"342\":1}}],[\"的关注程度\",{\"1\":{\"709\":1}}],[\"的关注信息\",{\"1\":{\"100\":1}}],[\"的关系想成\",{\"1\":{\"83\":1}}],[\"的得分结果\",{\"1\":{\"100\":1}}],[\"的有效性是否真正来自其机制本身\",{\"1\":{\"181\":1}}],[\"的有效性\",{\"1\":{\"99\":1,\"657\":1}}],[\"的有效性与泛化能力\",{\"1\":{\"45\":1}}],[\"的堆叠结构\",{\"1\":{\"97\":1}}],[\"的行\",{\"1\":{\"92\":1}}],[\"的目标不是只会\",{\"1\":{\"893\":1}}],[\"的目标不是重建像素\",{\"1\":{\"216\":1}}],[\"的目标与贡献\",{\"1\":{\"646\":1}}],[\"的目标就是\",{\"1\":{\"501\":1}}],[\"的目标是近似最大化训练样本的概率\",{\"1\":{\"944\":1}}],[\"的目标是抽取有利于当前学习任务的图像重要特征信息汇总\",{\"1\":{\"582\":1}}],[\"的目标是让模型能够在融合框架中联合建模视觉和语言\",{\"1\":{\"269\":1}}],[\"的目标是基于编码向量还原被遮挡的图像\",{\"1\":{\"229\":1}}],[\"的目标是\",{\"1\":{\"95\":1,\"283\":1}}],[\"的目标是预测出与该问题相关的点云区域\",{\"1\":{\"94\":1}}],[\"的目标\",{\"1\":{\"86\":1,\"262\":1}}],[\"的框架\",{\"1\":{\"72\":1}}],[\"的拼接特征\",{\"1\":{\"70\":1}}],[\"的损失函数\",{\"1\":{\"588\":1}}],[\"的损失函数的时候\",{\"1\":{\"355\":1}}],[\"的损失\",{\"1\":{\"64\":1,\"963\":1}}],[\"的\",{\"0\":{\"482\":1},\"1\":{\"48\":2,\"64\":2,\"69\":5,\"100\":1,\"107\":1,\"113\":2,\"114\":1,\"122\":1,\"138\":1,\"141\":1,\"188\":1,\"192\":1,\"206\":1,\"207\":1,\"208\":5,\"213\":1,\"214\":2,\"223\":3,\"224\":1,\"231\":1,\"235\":2,\"236\":1,\"242\":2,\"243\":2,\"257\":2,\"263\":2,\"264\":3,\"272\":1,\"286\":1,\"309\":1,\"310\":1,\"311\":1,\"339\":2,\"355\":2,\"373\":1,\"380\":1,\"382\":3,\"383\":1,\"384\":1,\"385\":3,\"386\":2,\"420\":9,\"480\":1,\"493\":1,\"542\":1,\"610\":1,\"633\":2,\"657\":1,\"658\":1,\"661\":1,\"663\":5,\"680\":2,\"691\":1,\"694\":5,\"697\":1,\"709\":3,\"713\":1,\"733\":1,\"735\":1,\"823\":8,\"835\":1,\"881\":1,\"885\":1,\"886\":1,\"887\":2,\"892\":3,\"895\":1,\"896\":3,\"897\":1,\"898\":2,\"899\":2,\"915\":1,\"935\":1,\"945\":4,\"959\":1}}],[\"的矩阵\",{\"1\":{\"42\":1,\"153\":1,\"426\":1,\"545\":1}}],[\"的研究表明\",{\"1\":{\"666\":1}}],[\"的研究如潮水般涌来\",{\"1\":{\"405\":1}}],[\"的研究\",{\"1\":{\"20\":1,\"343\":1}}],[\"模拟用户需求的任务\",{\"1\":{\"656\":1}}],[\"模拟人类通过观察学习物体功能的能力\",{\"1\":{\"73\":1}}],[\"模拟人类对交互方式的联想\",{\"1\":{\"36\":1}}],[\"模拟人观察交互图像时的思维链条\",{\"1\":{\"52\":1}}],[\"模版方法设计模型\",{\"1\":{\"381\":1}}],[\"模态融合比较轻量\",{\"1\":{\"415\":1}}],[\"模态融合比较弱\",{\"1\":{\"415\":1}}],[\"模态融合模式\",{\"1\":{\"207\":1}}],[\"模态交互部分可以分成两种方式\",{\"1\":{\"391\":1}}],[\"模态专家混合网络\",{\"1\":{\"372\":1}}],[\"模态间交互通过余弦相似度或线性投影实现\",{\"1\":{\"369\":1}}],[\"模态间交互通过特征向量的余弦相似度进行\",{\"1\":{\"368\":1}}],[\"模态之间切换\",{\"1\":{\"272\":1}}],[\"模态的可靠对应关系\",{\"1\":{\"19\":1}}],[\"模式自动恢复\",{\"1\":{\"807\":1}}],[\"模式切换示例\",{\"1\":{\"807\":1}}],[\"模式匹配引擎\",{\"1\":{\"649\":1}}],[\"模式运行\",{\"1\":{\"385\":1}}],[\"模式保持\",{\"1\":{\"293\":1}}],[\"模式\",{\"1\":{\"208\":2,\"323\":1,\"385\":3,\"424\":1,\"514\":1,\"811\":1,\"895\":2,\"898\":2,\"899\":1}}],[\"模式控制\",{\"1\":{\"208\":1}}],[\"模式中提供\",{\"1\":{\"208\":1}}],[\"模式2\",{\"1\":{\"122\":3}}],[\"模式1\",{\"1\":{\"122\":3}}],[\"模块产生的随机数序列\",{\"1\":{\"521\":1}}],[\"模块查看参数\",{\"1\":{\"454\":1}}],[\"模块作用域\",{\"1\":{\"444\":1}}],[\"模块文件的顶层作用域\",{\"1\":{\"444\":1}}],[\"模块可以看作是一个特征预处理模块\",{\"1\":{\"431\":1}}],[\"模块训练更好的自编码器\",{\"1\":{\"216\":1}}],[\"模块增强后的数据集上\",{\"1\":{\"192\":1}}],[\"模块同样也是基于\",{\"1\":{\"190\":1}}],[\"模块在\",{\"1\":{\"188\":1}}],[\"模块微调阶段\",{\"1\":{\"185\":1}}],[\"模块实现\",{\"0\":{\"185\":1}}],[\"模块用生成的语义丰富描述进行蒸馏\",{\"1\":{\"168\":1}}],[\"模块用于最终的\",{\"1\":{\"70\":1}}],[\"模块是一个残差结构\",{\"1\":{\"115\":1}}],[\"模块的\",{\"1\":{\"663\":1}}],[\"模块的代码属于模版代码\",{\"1\":{\"380\":1}}],[\"模块的作用是将\",{\"1\":{\"83\":1}}],[\"模块的前向传播\",{\"1\":{\"70\":1}}],[\"模块通过剔除噪声文本完成隐式知识过滤\",{\"1\":{\"168\":1}}],[\"模块通过\",{\"1\":{\"83\":1,\"177\":1}}],[\"模块就是在图像和点云之间建立\",{\"1\":{\"83\":1}}],[\"模块\",{\"0\":{\"115\":1,\"186\":1,\"189\":1,\"400\":1},\"1\":{\"34\":1,\"83\":1,\"95\":1,\"116\":1,\"415\":1,\"434\":1,\"444\":1,\"521\":1,\"663\":1,\"810\":2,\"899\":1}}],[\"模型权重\",{\"1\":{\"963\":1,\"964\":1}}],[\"模型生成的图像\",{\"1\":{\"944\":1}}],[\"模型若先随机选择一个数字\",{\"1\":{\"943\":1}}],[\"模型评估\",{\"0\":{\"935\":1}}],[\"模型实例化\",{\"1\":{\"926\":1}}],[\"模型实现如下\",{\"1\":{\"964\":1}}],[\"模型实现与优化\",{\"1\":{\"680\":1}}],[\"模型实现\",{\"0\":{\"94\":1,\"383\":1}}],[\"模型所在设备\",{\"1\":{\"926\":1}}],[\"模型获取输出\",{\"1\":{\"898\":1}}],[\"模型头部输出\",{\"1\":{\"898\":1}}],[\"模型具备一定的图像到图像转换能力\",{\"1\":{\"884\":1}}],[\"模型进行尝试\",{\"1\":{\"884\":1}}],[\"模型进一步使用\",{\"1\":{\"656\":1}}],[\"模型输入\",{\"1\":{\"832\":1}}],[\"模型输出应不受刚性变换影响\",{\"1\":{\"149\":1}}],[\"模型输出的预测结果\",{\"1\":{\"590\":1}}],[\"模型输出的预测值\",{\"1\":{\"586\":1}}],[\"模型输出的\",{\"1\":{\"587\":1}}],[\"模型输出的概率值或二值化结果\",{\"1\":{\"588\":1}}],[\"模型输出的概率值大于某个值时\",{\"1\":{\"565\":1}}],[\"模型输出的概率值\",{\"1\":{\"106\":1,\"587\":1,\"592\":1}}],[\"模型输出的原始\",{\"1\":{\"102\":1,\"588\":1,\"589\":1}}],[\"模型等\",{\"1\":{\"832\":1}}],[\"模型学习特定领域的数据有助于减少幻觉\",{\"1\":{\"830\":1}}],[\"模型学会将图像中与分类任务相关的信息汇聚到\",{\"1\":{\"427\":1}}],[\"模型学会如何通过注意力机制将图像的有效信息汇聚到\",{\"1\":{\"427\":2}}],[\"模型学会从教师模型中学会编码图像高级语义信息的能力\",{\"1\":{\"213\":1}}],[\"模型定制\",{\"1\":{\"830\":1}}],[\"模型也采用了\",{\"1\":{\"823\":1}}],[\"模型还使用了高效的数据并行和流水线并行技术\",{\"1\":{\"823\":1}}],[\"模型是典型的\",{\"1\":{\"823\":1}}],[\"模型是过参数化的\",{\"1\":{\"610\":1}}],[\"模型展现出了一些惊人的能力\",{\"1\":{\"822\":1}}],[\"模型类\",{\"1\":{\"819\":1}}],[\"模型类型\",{\"1\":{\"192\":1}}],[\"模型代表\",{\"1\":{\"735\":1}}],[\"模型代码解析的核心部分\",{\"1\":{\"383\":1}}],[\"模型代码解读\",{\"0\":{\"253\":1,\"378\":1,\"890\":1},\"1\":{\"253\":1,\"378\":1,\"890\":1,\"899\":1}}],[\"模型代码解读与复现\",{\"0\":{\"61\":1,\"84\":1}}],[\"模型代码进行修改的\",{\"1\":{\"379\":1}}],[\"模型代码实现是基于\",{\"1\":{\"379\":1}}],[\"模型代码实现进行详细讲解\",{\"1\":{\"293\":1}}],[\"模型代码实现部分\",{\"1\":{\"274\":1}}],[\"模型那样逐词生成新内容\",{\"1\":{\"735\":1}}],[\"模型有点大\",{\"1\":{\"712\":1}}],[\"模型就无法处理了\",{\"1\":{\"707\":1}}],[\"模型就能理解\",{\"1\":{\"83\":1}}],[\"模型返回的logits\",{\"1\":{\"700\":1}}],[\"模型整体实现也比较简单\",{\"1\":{\"699\":1}}],[\"模型很难学到它们的语义表示\",{\"1\":{\"697\":1}}],[\"模型要判断出这个假设是\",{\"1\":{\"694\":1}}],[\"模型要理解并遵循人类语言中的任务描述\",{\"1\":{\"346\":1}}],[\"模型层\",{\"0\":{\"675\":1}}],[\"模型越大毒性倾向越高\",{\"1\":{\"668\":1}}],[\"模型会根据前\",{\"1\":{\"921\":1}}],[\"模型会对每个选项分别编码\",{\"1\":{\"737\":1}}],[\"模型会输出两个数\",{\"1\":{\"694\":1}}],[\"模型会输出一个回答\",{\"1\":{\"660\":1}}],[\"模型会通过mask的上下文信息\",{\"1\":{\"393\":1}}],[\"模型顺从性过高\",{\"1\":{\"658\":1}}],[\"模型应能自动识别并拒绝执行\",{\"1\":{\"658\":1}}],[\"模型机制\",{\"1\":{\"658\":1}}],[\"模型行为问题\",{\"1\":{\"658\":1}}],[\"模型行为干预与有害输出缓解策略\",{\"1\":{\"655\":1}}],[\"模型中的每一步都变成\",{\"1\":{\"936\":1}}],[\"模型中使用了三种不同的自注意力掩码\",{\"1\":{\"887\":1}}],[\"模型中常用的特殊\",{\"1\":{\"697\":1}}],[\"模型中\",{\"1\":{\"658\":1,\"823\":1}}],[\"模型仍可能在对抗性攻击下暴露隐私\",{\"1\":{\"658\":1}}],[\"模型仍存在简单错误与对\",{\"1\":{\"657\":1}}],[\"模型仍然缺乏\",{\"1\":{\"20\":1}}],[\"模型泛化能力强\",{\"1\":{\"657\":1}}],[\"模型显著提升回答的真实性与信息性\",{\"1\":{\"657\":1}}],[\"模型均优于\",{\"1\":{\"657\":1}}],[\"模型比\",{\"1\":{\"657\":1}}],[\"模型依然在用户指令任务中获得更高的偏好评分\",{\"1\":{\"655\":1}}],[\"模型依然能稳定预测合理的交互区域\",{\"1\":{\"49\":1}}],[\"模型性能随规模扩展而持续提升\",{\"1\":{\"648\":1}}],[\"模型性能下降\",{\"1\":{\"376\":1}}],[\"模型容易过拟合训练数据的虚假相关性\",{\"1\":{\"646\":1}}],[\"模型容量与零样本性能强相关\",{\"1\":{\"641\":1}}],[\"模型容量与任务性能呈对数线性关系\",{\"1\":{\"641\":1}}],[\"模型容量增加直接缩小了与人类表现的差距\",{\"1\":{\"641\":1}}],[\"模型容量对任务性能至关重要\",{\"1\":{\"638\":1}}],[\"模型规模和数据量可能是性能瓶颈\",{\"1\":{\"884\":1}}],[\"模型规模与毒性正相关\",{\"1\":{\"670\":1}}],[\"模型规模与毒性\",{\"1\":{\"668\":1}}],[\"模型规模与性能的重新思考\",{\"1\":{\"666\":1}}],[\"模型规模与少样本学习能力呈正相关\",{\"1\":{\"646\":1}}],[\"模型规模扩展趋势与\",{\"1\":{\"650\":1}}],[\"模型规模的扩大可能显著提升上下文学习能力\",{\"1\":{\"646\":1}}],[\"模型规模的扩大带来了性能的持续提升\",{\"1\":{\"645\":1}}],[\"模型规模对性能有显著影响\",{\"1\":{\"344\":1}}],[\"模型局限性及其社会影响\",{\"1\":{\"645\":1}}],[\"模型倾向于使用简单的检索启发式\",{\"1\":{\"641\":1}}],[\"模型主要依赖于这个低的内在维度\",{\"1\":{\"610\":1}}],[\"模型主要依赖点云的外部表面信息\",{\"1\":{\"27\":1}}],[\"模型更不喜欢\",{\"1\":{\"590\":2}}],[\"模型更新参数\",{\"1\":{\"427\":1}}],[\"模型分类错误\",{\"1\":{\"589\":1}}],[\"模型已经学会了将数据编码到一个\",{\"1\":{\"947\":1}}],[\"模型已经具备基本的视觉理解能力\",{\"1\":{\"341\":1}}],[\"模型已自信分类\",{\"1\":{\"589\":1}}],[\"模型被大量简单负样本主导\",{\"1\":{\"589\":1}}],[\"模型被强制学会将图像的有效信息汇聚到\",{\"1\":{\"427\":1}}],[\"模型将随机选择的正例正确排在随机选择的负例之上的概率为\",{\"1\":{\"570\":1}}],[\"模型将正例排在负例之上的概率\",{\"1\":{\"570\":1}}],[\"模型将完全依赖随机初始化的参数去\",{\"1\":{\"100\":1}}],[\"模型才会将该样本分类为正类\",{\"1\":{\"565\":1}}],[\"模型库\",{\"1\":{\"511\":1}}],[\"模型仓库\",{\"1\":{\"510\":1}}],[\"模型丰富\",{\"1\":{\"510\":1}}],[\"模型提供输入\",{\"1\":{\"656\":1}}],[\"模型提供了\",{\"1\":{\"213\":1}}],[\"模型提取图像的特征图\",{\"1\":{\"434\":1}}],[\"模型自动学习到了如果注意画面中的分类主体\",{\"1\":{\"433\":1}}],[\"模型计算每个\",{\"1\":{\"427\":1}}],[\"模型难以计算\",{\"1\":{\"426\":1}}],[\"模型名称\",{\"1\":{\"412\":1,\"823\":4}}],[\"模型预测出的功能区域\",{\"1\":{\"586\":1}}],[\"模型预测出来的\",{\"1\":{\"403\":1}}],[\"模型预测被掩码的\",{\"1\":{\"373\":1}}],[\"模型预测该点属于功能区域的概率\",{\"1\":{\"106\":1}}],[\"模型在编码\",{\"1\":{\"691\":1}}],[\"模型在训练过程中看到更多的掩码变体\",{\"1\":{\"681\":1}}],[\"模型在训练期间学会了抓取包和杯子\",{\"1\":{\"89\":1}}],[\"模型在一些公开\",{\"1\":{\"658\":1}}],[\"模型在未明确训练的任务上也表现良好\",{\"1\":{\"658\":1}}],[\"模型在\",{\"1\":{\"640\":1,\"657\":1,\"823\":1}}],[\"模型在微调时可以作为双编码器\",{\"1\":{\"370\":1}}],[\"模型灵活性高\",{\"1\":{\"368\":1}}],[\"模型吗\",{\"1\":{\"354\":1}}],[\"模型通过在输入前后插入一些可训练的提示词来\",{\"1\":{\"346\":1}}],[\"模型通常在固定低分辨率\",{\"1\":{\"330\":1}}],[\"模型训练\",{\"1\":{\"656\":2}}],[\"模型训练的第二阶段\",{\"1\":{\"342\":1}}],[\"模型训练的第一阶段\",{\"1\":{\"341\":1}}],[\"模型训练过程是不可微的\",{\"1\":{\"232\":1}}],[\"模型训练过程中采用混合并行策略以适应大规模参数训练\",{\"1\":{\"647\":1}}],[\"模型训练过程中\",{\"1\":{\"204\":1}}],[\"模型尝试生成尽可能高奖励的回答\",{\"1\":{\"339\":1}}],[\"模型可以自由学习\",{\"1\":{\"931\":1}}],[\"模型可以编码\",{\"1\":{\"710\":1}}],[\"模型可以更好泛化到更长的序列\",{\"1\":{\"708\":1}}],[\"模型可以更好地理解和生成图文结合的内容\",{\"1\":{\"341\":1}}],[\"模型可以学习到更适合该类文本的位置表示方式\",{\"1\":{\"707\":1}}],[\"模型可能学到一种\",{\"1\":{\"706\":1}}],[\"模型可能过拟合这些特定的掩码模式\",{\"1\":{\"681\":1}}],[\"模型可能就只会学习到和query使用同样编码器的那个key\",{\"1\":{\"353\":1}}],[\"模型可能会接触到一些与下游任务相关的监督信息\",{\"1\":{\"273\":1}}],[\"模型可自适应调整分辨率\",{\"1\":{\"336\":1}}],[\"模型作为视觉基础模型\",{\"1\":{\"330\":1}}],[\"模型深度对速度的影响在gpu计算饱和后可以忽略\",{\"1\":{\"304\":1}}],[\"模型完整的训练流程\",{\"1\":{\"293\":1}}],[\"模型完整架构如下所示\",{\"1\":{\"218\":1}}],[\"模型对非二元代词\",{\"1\":{\"668\":1}}],[\"模型对真实类别的预测概率\",{\"1\":{\"589\":1}}],[\"模型对正负样本差异更加敏感\",{\"1\":{\"355\":1}}],[\"模型对所有输入输出一致的分布\",{\"1\":{\"290\":1}}],[\"模型对物体关键交互区域的识别能力下降\",{\"1\":{\"48\":1}}],[\"模型不是叫\",{\"1\":{\"921\":1}}],[\"模型不可解释性问题严重\",{\"1\":{\"649\":1}}],[\"模型不会崩溃\",{\"1\":{\"289\":1}}],[\"模型不再被\",{\"1\":{\"202\":1}}],[\"模型本身所提供的前向传播方法\",{\"1\":{\"274\":1}}],[\"模型如何通过零样本迁移或最小任务微调快速应用到下游任务\",{\"1\":{\"270\":1}}],[\"模型表现更好\",{\"1\":{\"376\":1}}],[\"模型表现良好\",{\"1\":{\"157\":1}}],[\"模型表达能力受限\",{\"1\":{\"262\":1}}],[\"模型和cc\",{\"1\":{\"688\":1}}],[\"模型和\",{\"1\":{\"238\":1}}],[\"模型与序列并行\",{\"1\":{\"667\":1}}],[\"模型与预训练规模化\",{\"0\":{\"224\":1}}],[\"模型与数据的规模化\",{\"1\":{\"220\":1}}],[\"模型前向传播整体流程\",{\"1\":{\"891\":1}}],[\"模型前向传播代码如下所示\",{\"1\":{\"362\":1}}],[\"模型前向传播中的\",{\"1\":{\"206\":1,\"207\":1,\"208\":1}}],[\"模型前向\",{\"1\":{\"208\":1}}],[\"模型由三个主要部分组成\",{\"1\":{\"197\":1}}],[\"模型基于\",{\"1\":{\"192\":1}}],[\"模型使用的是训练好的\",{\"1\":{\"899\":1}}],[\"模型使用了大规模的数据过滤和清洗技术\",{\"1\":{\"823\":1}}],[\"模型使用\",{\"1\":{\"176\":1}}],[\"模型设计上\",{\"1\":{\"171\":1}}],[\"模型架构图\",{\"1\":{\"741\":1}}],[\"模型架构改进\",{\"1\":{\"667\":1}}],[\"模型架构与规模设计\",{\"1\":{\"647\":1}}],[\"模型架构\",{\"0\":{\"171\":1,\"714\":1,\"741\":1},\"1\":{\"170\":1,\"640\":1}}],[\"模型构建两个模块\",{\"1\":{\"165\":1}}],[\"模型限制\",{\"1\":{\"165\":1}}],[\"模型为\",{\"1\":{\"159\":1}}],[\"模型必须对输入点的排列顺序不敏感\",{\"1\":{\"149\":1}}],[\"模型需要从中选择最合适的答案\",{\"1\":{\"737\":1}}],[\"模型需要为每个点预测一个类别标签\",{\"1\":{\"143\":1}}],[\"模型需要根据自然语言问题识别点云中最相关的功能区域\",{\"1\":{\"102\":1}}],[\"模型需要根据自然语言问题\",{\"1\":{\"95\":1}}],[\"模型推理\",{\"1\":{\"107\":1}}],[\"模型初始化代码如下所示\",{\"1\":{\"361\":1}}],[\"模型初始化\",{\"0\":{\"892\":1},\"1\":{\"104\":1,\"205\":1}}],[\"模型的具体含义取决于我们想用它完成什么\",{\"1\":{\"942\":1}}],[\"模型的具体配置\",{\"1\":{\"633\":1}}],[\"模型的输入和输出的取值范围都位于\",{\"1\":{\"925\":1}}],[\"模型的输入不再使用位置编码\",{\"1\":{\"823\":1}}],[\"模型的输出是\",{\"1\":{\"921\":1}}],[\"模型的初始化流程\",{\"1\":{\"892\":1}}],[\"模型的基本原则是通过语言建模将世界知识压缩到仅解码器\",{\"1\":{\"823\":1}}],[\"模型的每个测试样本前会插入k条示例\",{\"1\":{\"647\":1}}],[\"模型的迁移能力逐渐增强\",{\"1\":{\"639\":1}}],[\"模型的认知\",{\"1\":{\"614\":1}}],[\"模型的参数是根据这个特定尺寸的输入数据进行优化和学习的\",{\"1\":{\"425\":1}}],[\"模型的前向传播阶段会根据学习任务列表\",{\"1\":{\"383\":1}}],[\"模型的前向传播流程\",{\"1\":{\"893\":1}}],[\"模型的前向传播流程负责具体落地知识蒸馏算法的实现\",{\"1\":{\"213\":1}}],[\"模型的前向传播流程和albef实现基本一致\",{\"1\":{\"192\":1}}],[\"模型的代码实现\",{\"1\":{\"899\":1}}],[\"模型的代码实现应该比较亲切\",{\"1\":{\"379\":1}}],[\"模型的代码实现中额外抽取了一个抽象类\",{\"1\":{\"382\":1}}],[\"模型的代码实现中主要使用了以下两个库\",{\"1\":{\"379\":1}}],[\"模型的代码实现中定义了\",{\"1\":{\"254\":1}}],[\"模型的泛化性就很强\",{\"1\":{\"353\":1}}],[\"模型的运行完整流程图\",{\"1\":{\"293\":1}}],[\"模型的完整结构\",{\"1\":{\"213\":1}}],[\"模型的\",{\"1\":{\"192\":1,\"386\":1}}],[\"模型的训练流程\",{\"1\":{\"293\":1}}],[\"模型的训练代码\",{\"1\":{\"192\":1}}],[\"模型的训练过程大体分为了\",{\"1\":{\"103\":1}}],[\"模型的监督信号\",{\"1\":{\"102\":1}}],[\"模型的语义知识提升\",{\"1\":{\"19\":1}}],[\"模型变体包括\",{\"1\":{\"176\":1}}],[\"模型变体\",{\"1\":{\"99\":1}}],[\"模型目标\",{\"1\":{\"94\":1}}],[\"模型结构与训练细节\",{\"1\":{\"656\":1}}],[\"模型结构与原始\",{\"1\":{\"339\":1}}],[\"模型结构图\",{\"1\":{\"63\":1}}],[\"模型结构\",{\"0\":{\"63\":1,\"132\":1,\"416\":1},\"1\":{\"165\":1,\"167\":1,\"341\":1,\"342\":1}}],[\"模型能从无标记数据中充分利用语义信息\",{\"1\":{\"626\":1}}],[\"模型能够泛化到新标注者的偏好\",{\"1\":{\"656\":1}}],[\"模型能够直接在零样本设定下完成多种任务\",{\"1\":{\"273\":1}}],[\"模型能够学习到丰富的图像特征和模式\",{\"1\":{\"435\":1}}],[\"模型能够学习如何根据不同任务的需求\",{\"1\":{\"272\":1}}],[\"模型能够学习从\",{\"1\":{\"53\":1}}],[\"模型能准确对每个对象生成独立的\",{\"1\":{\"49\":1}}],[\"模型无法收敛\",{\"1\":{\"289\":1}}],[\"模型无法精确聚焦于关键交互部位\",{\"1\":{\"48\":1}}],[\"模型无法进行类比推理\",{\"1\":{\"48\":1}}],[\"模型优势\",{\"1\":{\"22\":1,\"296\":1,\"323\":1}}],[\"模型\",{\"0\":{\"54\":1,\"83\":1,\"496\":1,\"582\":1,\"699\":1,\"931\":1,\"937\":1},\"1\":{\"19\":1,\"159\":1,\"190\":1,\"208\":1,\"213\":1,\"265\":2,\"268\":2,\"304\":1,\"306\":1,\"310\":1,\"344\":1,\"354\":1,\"361\":1,\"377\":1,\"403\":1,\"407\":1,\"587\":1,\"600\":1,\"656\":4,\"699\":1,\"735\":1,\"819\":1,\"823\":9,\"832\":1,\"892\":1,\"918\":1,\"921\":1,\"926\":1,\"964\":4}}],[\"模型大多依赖几何与位置编码\",{\"1\":{\"19\":1}}],[\"在嵌入空间里找最近邻\",{\"1\":{\"958\":1}}],[\"在后续文章中则被称作\",{\"1\":{\"956\":1}}],[\"在后续的文献中\",{\"1\":{\"925\":1}}],[\"在面对这种问题时\",{\"1\":{\"952\":1}}],[\"在面对带错误前提的指令时\",{\"1\":{\"657\":1}}],[\"在稀疏自编码器的目标函数中\",{\"1\":{\"951\":1}}],[\"在信息论中\",{\"1\":{\"950\":1}}],[\"在信息检索领域\",{\"1\":{\"827\":1}}],[\"在更实际设定下到底存在多少近似误差\",{\"1\":{\"949\":1}}],[\"在优化\",{\"1\":{\"948\":1}}],[\"在测试阶段\",{\"1\":{\"947\":2}}],[\"在测试集上调优模型\",{\"1\":{\"835\":1}}],[\"在固定\",{\"1\":{\"946\":1}}],[\"在固定计算预算下\",{\"1\":{\"666\":1}}],[\"在我们的设定中\",{\"1\":{\"946\":1}}],[\"在变分自编码器中\",{\"1\":{\"943\":1}}],[\"在变分推断中\",{\"1\":{\"885\":1}}],[\"在统计学里被称为自回归模型\",{\"1\":{\"925\":1}}],[\"在统计学中\",{\"1\":{\"903\":1}}],[\"在生成图像时\",{\"1\":{\"925\":1}}],[\"在生成每个像素时严格遵循\",{\"1\":{\"924\":1}}],[\"在和概率相关的指标上表现优秀\",{\"1\":{\"925\":1}}],[\"在正态分布假设下\",{\"1\":{\"904\":1}}],[\"在已有图像基础上补全未提供区域\",{\"1\":{\"895\":1}}],[\"在已知世界状态\",{\"1\":{\"878\":1}}],[\"在合并两部分损失时引入了一个\",{\"1\":{\"893\":1}}],[\"在合成任务和灵活性测试中展现强大泛化能力\",{\"1\":{\"648\":1}}],[\"在收敛时未发现过拟合迹象\",{\"1\":{\"887\":1}}],[\"在其\",{\"1\":{\"887\":1}}],[\"在其他任务\",{\"1\":{\"348\":1}}],[\"在松弛操作附近减小卷积感受野有助于其对真实\",{\"1\":{\"886\":1}}],[\"在编码器末端与解码器开头使用\",{\"1\":{\"886\":1}}],[\"在编程中\",{\"1\":{\"755\":1}}],[\"在公式\",{\"1\":{\"877\":1}}],[\"在公开\",{\"1\":{\"657\":1}}],[\"在高维图像空间中\",{\"1\":{\"875\":1}}],[\"在高维空间中\",{\"1\":{\"578\":1,\"872\":2}}],[\"在高维空间\",{\"1\":{\"500\":1}}],[\"在二维空间中\",{\"1\":{\"871\":1}}],[\"在总共\",{\"1\":{\"857\":1}}],[\"在构建各种类型的模型时\",{\"1\":{\"854\":1}}],[\"在给定均值和协方差矩的约束下\",{\"1\":{\"869\":1}}],[\"在给定\",{\"1\":{\"849\":1,\"950\":1}}],[\"在事件\",{\"1\":{\"849\":1}}],[\"在事实型问答测试中\",{\"1\":{\"641\":1}}],[\"在有限的情况下\",{\"1\":{\"848\":1}}],[\"在持续时间的例子中\",{\"1\":{\"847\":1}}],[\"在线学习\",{\"0\":{\"841\":1}}],[\"在确定开发目标后\",{\"1\":{\"836\":1}}],[\"在验证集上最终验证模型效果来实现性能的评估\",{\"1\":{\"835\":1}}],[\"在开发过程中\",{\"1\":{\"832\":1}}],[\"在开源模型中领先\",{\"1\":{\"335\":1}}],[\"在下图中\",{\"1\":{\"831\":1}}],[\"在下游任务中的应用\",{\"0\":{\"273\":1}}],[\"在下游视觉任务上微调\",{\"0\":{\"237\":1}}],[\"在提升大语言模型效果中\",{\"1\":{\"830\":1}}],[\"在理解和生成长篇内容时受限于有限的上下文窗口\",{\"1\":{\"828\":1}}],[\"在各种基准测试中均优于\",{\"1\":{\"823\":1}}],[\"在各种任务中的表现均显著提升\",{\"1\":{\"822\":1}}],[\"在遵循复杂指令\",{\"1\":{\"823\":1}}],[\"在国内率先开启邀测\",{\"1\":{\"823\":1}}],[\"在性能和效率上有显著提升\",{\"1\":{\"823\":1}}],[\"在回答前会先生成一段思维链\",{\"1\":{\"823\":1}}],[\"在回答简单问题时冗长解释或\",{\"1\":{\"657\":1}}],[\"在知识广度\",{\"1\":{\"823\":1}}],[\"在语音互动中传递更丰富的情感变化\",{\"1\":{\"823\":1}}],[\"在语义分割任务中\",{\"1\":{\"584\":1}}],[\"在解决复杂任务和评估任务上展现出较大的性能提升\",{\"1\":{\"823\":1}}],[\"在解码器中\",{\"1\":{\"741\":1}}],[\"在解码器的前半部分省略了\",{\"1\":{\"272\":1}}],[\"在解码阶段\",{\"1\":{\"116\":1}}],[\"在他的经典论文\",{\"1\":{\"822\":1}}],[\"在点上\",{\"1\":{\"816\":1}}],[\"在点云中逐步选择离已选点尽可能远的点\",{\"1\":{\"137\":1}}],[\"在第\",{\"1\":{\"888\":1}}],[\"在第二阶段中\",{\"1\":{\"887\":1}}],[\"在第四阶段\",{\"1\":{\"819\":1}}],[\"在第三阶段\",{\"1\":{\"814\":1}}],[\"在第一阶段训练中\",{\"1\":{\"886\":1}}],[\"在第一阶段\",{\"1\":{\"799\":1}}],[\"在接收输入时自动将参数转换为variable\",{\"1\":{\"809\":1}}],[\"在表达式a\",{\"1\":{\"809\":3}}],[\"在tinypytorch中\",{\"1\":{\"806\":2}}],[\"在transformer\",{\"1\":{\"431\":1}}],[\"在transformer中\",{\"1\":{\"428\":1}}],[\"在transformer模型中\",{\"1\":{\"422\":1}}],[\"在步骤14中\",{\"1\":{\"804\":1}}],[\"在反向传播过程中\",{\"1\":{\"803\":1}}],[\"在调用如\",{\"1\":{\"800\":1}}],[\"在variable的backward方法中自动初始化梯度\",{\"1\":{\"791\":1}}],[\"在variable类中添加name属性\",{\"1\":{\"808\":1}}],[\"在variable类中添加backward方法\",{\"1\":{\"784\":1}}],[\"在variable类中添加creator属性\",{\"1\":{\"783\":1}}],[\"在function类的\",{\"1\":{\"783\":1}}],[\"在few\",{\"1\":{\"647\":1}}],[\"在上一节中\",{\"1\":{\"957\":1}}],[\"在上下文明确的情况下\",{\"1\":{\"846\":1}}],[\"在上下文中找到最可能的答案起始位置和结束位置\",{\"1\":{\"735\":1}}],[\"在上面的推理过程中\",{\"1\":{\"660\":1}}],[\"在上面的例子中\",{\"1\":{\"621\":1}}],[\"在上面的结构图中可以看到\",{\"1\":{\"427\":1}}],[\"在问答任务中一般不会使用这个输出\",{\"1\":{\"733\":1}}],[\"在问答任务中\",{\"1\":{\"733\":1}}],[\"在问答上提升5\",{\"1\":{\"625\":1}}],[\"在返回前进行预处理\",{\"1\":{\"715\":1}}],[\"在注意力得分矩阵计算完毕后\",{\"1\":{\"703\":1}}],[\"在注意力机制\",{\"1\":{\"523\":1}}],[\"在glue\",{\"1\":{\"681\":1}}],[\"在gpt\",{\"1\":{\"650\":1}}],[\"在bert和roberta的预训练中\",{\"1\":{\"681\":1}}],[\"在boolq\",{\"1\":{\"648\":1}}],[\"在配备\",{\"1\":{\"680\":1}}],[\"在首轮推理的过程中\",{\"1\":{\"663\":1}}],[\"在偏见测试中未表现出优势\",{\"1\":{\"657\":1}}],[\"在加入指导性提示\",{\"1\":{\"657\":1}}],[\"在用户任务分布中\",{\"1\":{\"657\":1}}],[\"在用户偏好评估中仍表现更优\",{\"1\":{\"653\":1}}],[\"在三个参数规模\",{\"1\":{\"656\":1}}],[\"在三维物体上找出可以交互的区域\",{\"1\":{\"19\":1}}],[\"在人类偏好标注中\",{\"1\":{\"656\":1}}],[\"在使用\",{\"1\":{\"733\":1}}],[\"在使用gpt\",{\"1\":{\"649\":1}}],[\"在使用cot这种prompt\",{\"1\":{\"620\":1}}],[\"在使用clip模型进行zero\",{\"1\":{\"409\":1}}],[\"在某些特定情形下\",{\"1\":{\"949\":1}}],[\"在某些问题中\",{\"1\":{\"866\":1}}],[\"在某些任务中可与sota模型媲美\",{\"1\":{\"649\":1}}],[\"在某些标准数据集\",{\"1\":{\"157\":1}}],[\"在zero\",{\"1\":{\"648\":1}}],[\"在阅读理解任务中\",{\"1\":{\"648\":1}}],[\"在没有使用外部检索信息\",{\"1\":{\"648\":1}}],[\"在封闭式问答任务中接近甚至超越sota\",{\"1\":{\"648\":1}}],[\"在lambada数据集上\",{\"1\":{\"648\":1}}],[\"在log空间均匀采样后exp还原\",{\"1\":{\"263\":1}}],[\"在任务特定数据集上更新模型权重\",{\"1\":{\"647\":1}}],[\"在少样本设置下\",{\"1\":{\"646\":1}}],[\"在one\",{\"1\":{\"641\":1}}],[\"在webtext验证集上仍未完全收敛\",{\"1\":{\"641\":1}}],[\"在nli和qqp任务上辅助lm目标有帮助\",{\"1\":{\"635\":1}}],[\"在nlp中\",{\"1\":{\"626\":1}}],[\"在nlp领域\",{\"1\":{\"413\":1}}],[\"在文章末尾添加\",{\"1\":{\"640\":1}}],[\"在文档和问题给定条件下\",{\"1\":{\"635\":1}}],[\"在文本序列开头加上\",{\"1\":{\"893\":1}}],[\"在文本词索引空间中的起始索引\",{\"1\":{\"893\":1}}],[\"在文本单模态数据上预训练语言专家\",{\"1\":{\"368\":1}}],[\"在文本前添加\",{\"1\":{\"171\":1}}],[\"在阈值下进行预测的\",{\"1\":{\"635\":1}}],[\"在零样本上\",{\"1\":{\"635\":1}}],[\"在应用到很多任务时可以提高语言建模的能力并且transformer更具结构化的注意力记忆\",{\"1\":{\"635\":1}}],[\"在superglue基准测试中\",{\"1\":{\"648\":1}}],[\"在sts\",{\"1\":{\"634\":1}}],[\"在storycloze和hellaswag等故事完形任务中\",{\"1\":{\"648\":1}}],[\"在story\",{\"1\":{\"634\":1}}],[\"在seq\",{\"1\":{\"419\":1}}],[\"在句子级别打乱顺序以破坏长距离结构信息\",{\"1\":{\"633\":1}}],[\"在作者的实验中\",{\"1\":{\"629\":1}}],[\"在作为冻结编码器\",{\"1\":{\"273\":1}}],[\"在迁移阶段\",{\"1\":{\"626\":1}}],[\"在迁移到其他数据集时也需要加上新的分类器进行有监督训练\",{\"1\":{\"413\":1}}],[\"在迁移到下游任务时\",{\"1\":{\"413\":1}}],[\"在12个的9个数据集取得了sota结果\",{\"1\":{\"636\":1}}],[\"在12个数据集上的9个取得sota结果\",{\"1\":{\"634\":1}}],[\"在12个研究任务中9个提升到sota\",{\"1\":{\"625\":1,\"626\":1}}],[\"在18个多模态基准测试中展现出媲美商业模型的性能\",{\"1\":{\"337\":1}}],[\"在原始预训练语言模型\",{\"1\":{\"611\":1}}],[\"在原始点数量下的每个点都拥有一个合理的特征向量\",{\"1\":{\"145\":1}}],[\"在微调完成后\",{\"1\":{\"609\":1}}],[\"在微调过程中\",{\"1\":{\"609\":1}}],[\"在准备好的数据集上\",{\"1\":{\"609\":1}}],[\"在特定任务上\",{\"1\":{\"649\":1}}],[\"在特定任务相关的数据集上执行有监督全量参数微调\",{\"1\":{\"609\":1}}],[\"在特征层面建立\",{\"1\":{\"19\":1}}],[\"在推理大模型方面\",{\"1\":{\"823\":1}}],[\"在推理方面就十分出色\",{\"1\":{\"823\":1}}],[\"在推理能力\",{\"1\":{\"823\":1}}],[\"在推理型模型中可选择性展示思考过程\",{\"1\":{\"823\":1}}],[\"在推理阶段可以从\",{\"1\":{\"935\":1}}],[\"在推理阶段\",{\"1\":{\"807\":1}}],[\"在推理阶段用文本输入指定任务\",{\"1\":{\"650\":1}}],[\"在推理时为模型提供10\",{\"1\":{\"647\":1}}],[\"在推理过程中\",{\"1\":{\"611\":1}}],[\"在推理的过程中直接将∆w加到w上去\",{\"1\":{\"606\":1}}],[\"在推动通用人工智能\",{\"1\":{\"323\":1}}],[\"在python开发中\",{\"1\":{\"810\":1}}],[\"在python的运算符重载中\",{\"1\":{\"809\":1}}],[\"在python中\",{\"1\":{\"809\":1}}],[\"在prompt中加入的示例不是1条\",{\"1\":{\"620\":1}}],[\"在prompt中加入一些示例\",{\"1\":{\"620\":1}}],[\"在prompt中加入一些例子\",{\"1\":{\"619\":1}}],[\"在prompt上下文中添加适当的条件\",{\"1\":{\"605\":1}}],[\"在pointnet中\",{\"1\":{\"131\":1}}],[\"在x前面加上了一些特定的内容\",{\"1\":{\"604\":1}}],[\"在具体执行特定任务的时候按需调用\",{\"1\":{\"604\":1}}],[\"在类别平衡时效果好\",{\"1\":{\"587\":1}}],[\"在类别维度上执行\",{\"1\":{\"257\":1}}],[\"在考虑数据分布形状后\",{\"1\":{\"578\":1}}],[\"在垃圾邮件分类器示例中\",{\"1\":{\"570\":1}}],[\"在垃圾邮件分类示例中\",{\"1\":{\"562\":1,\"563\":1,\"564\":1,\"565\":1,\"566\":1}}],[\"在评估思路上\",{\"1\":{\"835\":1}}],[\"在评估模型和选择阈值时\",{\"1\":{\"566\":1}}],[\"在评估阶段\",{\"1\":{\"382\":1}}],[\"在激活的\",{\"1\":{\"557\":1}}],[\"在当前环境下安装包\",{\"0\":{\"557\":1}}],[\"在逻辑上表现为每行都是\",{\"1\":{\"546\":1}}],[\"在逻辑转置中\",{\"1\":{\"545\":1}}],[\"在数学\",{\"1\":{\"823\":2}}],[\"在数学上\",{\"1\":{\"563\":1,\"565\":1,\"567\":1}}],[\"在数据预处理阶段\",{\"1\":{\"681\":1}}],[\"在数据方面\",{\"1\":{\"174\":1}}],[\"在数组运算中的底层实现原理\",{\"1\":{\"546\":1}}],[\"在内存中是行优先存储\",{\"1\":{\"544\":1}}],[\"在内存中是连续存放的\",{\"1\":{\"122\":1}}],[\"在列优先顺序\",{\"1\":{\"541\":1}}],[\"在行优先顺序\",{\"1\":{\"541\":1}}],[\"在映射机制里\",{\"1\":{\"540\":1}}],[\"在机器学习\",{\"1\":{\"540\":1}}],[\"在机器翻译和\",{\"1\":{\"110\":1}}],[\"在目标检测任务中\",{\"1\":{\"501\":1}}],[\"在目标物体区域掩码之上\",{\"1\":{\"83\":1}}],[\"在思想上都体现了用更多自由度提升精度\",{\"1\":{\"500\":1}}],[\"在所有规模下均优于\",{\"1\":{\"657\":1}}],[\"在所有归一化操作前都调用\",{\"1\":{\"493\":1}}],[\"在所有划分\",{\"1\":{\"47\":1}}],[\"在同一个模型内部做有条件与无条件两种预测\",{\"1\":{\"894\":1}}],[\"在同一设备上\",{\"1\":{\"486\":1}}],[\"在同一张交互图像中存在多个物体时\",{\"1\":{\"49\":1}}],[\"在末尾补\",{\"1\":{\"485\":1}}],[\"在最后加维度\",{\"1\":{\"463\":1}}],[\"在最后一个维度两边各填充2个\",{\"1\":{\"477\":1}}],[\"在最后一个维度左边不填充\",{\"1\":{\"477\":1}}],[\"在最后一个维度左边填充1个0\",{\"1\":{\"477\":1}}],[\"在最后一维右边加1\",{\"1\":{\"274\":1}}],[\"在最后一层的表示向量\",{\"1\":{\"238\":1}}],[\"在外面仍然可见\",{\"1\":{\"444\":1}}],[\"在如此大规模的数据集上进行预训练\",{\"1\":{\"435\":1}}],[\"在深度学习框架中\",{\"1\":{\"806\":1,\"809\":1}}],[\"在深度学习与生成模型中\",{\"1\":{\"574\":1}}],[\"在深度学习领域\",{\"1\":{\"432\":1}}],[\"在深层\",{\"1\":{\"380\":1}}],[\"在每次前向传播前将不允许访问的卷积核位置置零\",{\"1\":{\"926\":1}}],[\"在每一步中\",{\"1\":{\"831\":1}}],[\"在每一层\",{\"1\":{\"427\":1}}],[\"在每个\",{\"1\":{\"656\":1}}],[\"在每个小格子里撒几个点\",{\"1\":{\"502\":1}}],[\"在每个层上\",{\"1\":{\"133\":1}}],[\"在代码\",{\"1\":{\"657\":1,\"668\":1}}],[\"在代码中\",{\"1\":{\"426\":1,\"897\":1}}],[\"在代理任务上做文章\",{\"1\":{\"355\":1}}],[\"在柱状图上添加数值标签\",{\"1\":{\"424\":1}}],[\"在imagenet数据集上可以提升1\",{\"1\":{\"409\":1}}],[\"在imagenet分类\",{\"1\":{\"296\":1}}],[\"在之前的问题上表现明显变差\",{\"1\":{\"610\":1}}],[\"在之前的例子中\",{\"1\":{\"409\":1}}],[\"在之前有很多优秀的对比学习工作\",{\"1\":{\"353\":1}}],[\"在大模型开发中\",{\"1\":{\"835\":1}}],[\"在大部分任务中基本上\",{\"1\":{\"633\":1}}],[\"在大多数情况下\",{\"1\":{\"391\":1}}],[\"在大规模文本语料上学习高容量的语言模型\",{\"1\":{\"628\":1}}],[\"在大规模场景理解任务中表现一般\",{\"1\":{\"157\":1}}],[\"在大规模或实时应用中可能成为瓶颈\",{\"1\":{\"50\":1}}],[\"在网络深层中\",{\"1\":{\"390\":1}}],[\"在网络设计上\",{\"1\":{\"109\":1}}],[\"在参数或者计算上\",{\"1\":{\"390\":1}}],[\"在融合层之后\",{\"1\":{\"385\":1}}],[\"在模拟环境中用人类反馈改进行为策略\",{\"1\":{\"655\":1}}],[\"在模版流程的各种阶段都做了什么\",{\"1\":{\"381\":1}}],[\"在模型训练和推理过程中\",{\"1\":{\"614\":1}}],[\"在模型训练时可能会出现问题\",{\"1\":{\"356\":1}}],[\"在模型架构中\",{\"1\":{\"434\":1}}],[\"在模型初始化时\",{\"1\":{\"428\":1}}],[\"在模型规模\",{\"1\":{\"303\":1}}],[\"在模型设计上提出了一个灵活\",{\"1\":{\"174\":1}}],[\"在拆分多模态输入时使用\",{\"1\":{\"380\":1}}],[\"在检索和分类任务中都优于标准\",{\"1\":{\"376\":1}}],[\"在图文检索中\",{\"1\":{\"410\":1}}],[\"在图文检索任务中\",{\"1\":{\"368\":1}}],[\"在图像或序列生成任务中\",{\"1\":{\"894\":1}}],[\"在图像单模态数据上预训练视觉专家和自注意力模块\",{\"1\":{\"368\":1}}],[\"在图像识别中加入\",{\"1\":{\"346\":1}}],[\"在图像分类任务中\",{\"1\":{\"422\":1,\"434\":1}}],[\"在图像分类\",{\"1\":{\"312\":1}}],[\"在图像分类和语义分割任务上优于从零训练的模型和其他自监督方法\",{\"1\":{\"228\":1}}],[\"在图像输入方面\",{\"1\":{\"272\":1}}],[\"在图像与文本结合任务中\",{\"1\":{\"251\":1}}],[\"在全局平均池化之后\",{\"1\":{\"362\":1}}],[\"在全参数微调下\",{\"1\":{\"308\":1}}],[\"在执行的时候\",{\"1\":{\"357\":1}}],[\"在蒸馏相关内容里其实提到过\",{\"1\":{\"355\":1}}],[\"在看\",{\"1\":{\"355\":1}}],[\"在损失函数上做文章\",{\"1\":{\"355\":1}}],[\"在copa任务中仅落后1\",{\"1\":{\"648\":1}}],[\"在cv领域\",{\"1\":{\"353\":1}}],[\"在chartqa和ocrbench上超越所有商业模型\",{\"1\":{\"335\":1}}],[\"在caption前添加prompt\",{\"1\":{\"187\":1}}],[\"在8个任务中达到sota\",{\"1\":{\"323\":1}}],[\"在中文任务中表现优于gpt\",{\"1\":{\"323\":1}}],[\"在此类任务中依然适用\",{\"1\":{\"646\":1}}],[\"在此实现中未使用\",{\"1\":{\"588\":1}}],[\"在此版本中\",{\"1\":{\"330\":1}}],[\"在此阶段\",{\"1\":{\"315\":1}}],[\"在此基础上\",{\"1\":{\"283\":1}}],[\"在不进行任何梯度更新的前提下\",{\"1\":{\"651\":1}}],[\"在不进行梯度更新的前提下实现任务适应\",{\"1\":{\"647\":1}}],[\"在不损失太多性能的情况下减少了模型的大小\",{\"1\":{\"614\":1}}],[\"在不改变大模型的前提下\",{\"1\":{\"605\":1}}],[\"在不复制数据的前提下\",{\"1\":{\"469\":1}}],[\"在不使用指令微调的前提下\",{\"1\":{\"309\":1}}],[\"在不同情形下它发生的概率\",{\"1\":{\"850\":1}}],[\"在不同微调策略下\",{\"1\":{\"308\":1}}],[\"在不同解码阶段注入语言线索\",{\"1\":{\"95\":1}}],[\"在不同解码层注入语言信息\",{\"1\":{\"94\":1}}],[\"在英中双语的\",{\"1\":{\"309\":1}}],[\"在该步中\",{\"1\":{\"836\":1}}],[\"在该步骤中\",{\"1\":{\"836\":1}}],[\"在该阶段\",{\"1\":{\"306\":2}}],[\"在该框架中\",{\"1\":{\"285\":1}}],[\"在两句话之间和句末加\",{\"1\":{\"692\":1}}],[\"在两类提示分布\",{\"1\":{\"657\":1}}],[\"在两个线性层之间通常会插入一个非线性激活函数\",{\"1\":{\"429\":1}}],[\"在两个视频\",{\"1\":{\"165\":1}}],[\"在两台8\",{\"1\":{\"291\":1}}],[\"在dino中\",{\"1\":{\"289\":1}}],[\"在前一阶段\",{\"1\":{\"814\":1}}],[\"在前向传播中\",{\"1\":{\"737\":1}}],[\"在前向过程中\",{\"1\":{\"611\":1}}],[\"在前面我们介绍了\",{\"1\":{\"612\":1}}],[\"在前景像素远少于背景像素时表现良好\",{\"1\":{\"592\":1}}],[\"在前\",{\"1\":{\"286\":1}}],[\"在自注意力机制之后\",{\"1\":{\"741\":1}}],[\"在自注意力里\",{\"1\":{\"710\":1}}],[\"在自然语言推理任务\",{\"1\":{\"648\":1}}],[\"在自然语言处理领域\",{\"1\":{\"827\":1}}],[\"在自然语言处理领域取得了突破性进展\",{\"1\":{\"299\":1}}],[\"在自然语言处理中的成功启发\",{\"1\":{\"252\":1}}],[\"在自己的数据上继续训练\",{\"1\":{\"610\":1}}],[\"在自回归生成时\",{\"1\":{\"420\":1}}],[\"在自监督学习中\",{\"1\":{\"285\":1}}],[\"在默认参数设置下\",{\"1\":{\"285\":1}}],[\"在传统的知识蒸馏中\",{\"1\":{\"285\":1}}],[\"在协同蒸馏里\",{\"1\":{\"283\":1}}],[\"在资源有限的场景下\",{\"1\":{\"280\":1}}],[\"在完全不做微调\",{\"1\":{\"280\":1}}],[\"在完成前后端搭建之后\",{\"1\":{\"836\":1}}],[\"在完成上一步的初始化\",{\"1\":{\"836\":1}}],[\"在完成自监督预训练后\",{\"1\":{\"240\":1}}],[\"在完成\",{\"1\":{\"237\":1}}],[\"在倒数第二维左边加\",{\"1\":{\"274\":1}}],[\"在计算iou得分之前\",{\"1\":{\"591\":1}}],[\"在计算余弦相似度\",{\"1\":{\"508\":1}}],[\"在计算机视觉领域非常常用\",{\"1\":{\"510\":1}}],[\"在计算机视觉领域\",{\"1\":{\"413\":1,\"827\":1}}],[\"在计算机视觉领域展现出强大潜力\",{\"1\":{\"228\":1}}],[\"在计算成本和准确率之间取得了良好平衡\",{\"1\":{\"311\":1}}],[\"在计算检索指标时\",{\"1\":{\"273\":1}}],[\"在视频动作识别中的应用\",{\"1\":{\"273\":1}}],[\"在视觉问答\",{\"1\":{\"368\":1}}],[\"在视觉推理任务上准确率偏低\",{\"1\":{\"368\":1}}],[\"在视觉中的潜力\",{\"1\":{\"280\":1}}],[\"在视觉识别任务中表现出竞争力\",{\"1\":{\"280\":1}}],[\"在视觉\",{\"1\":{\"269\":1,\"368\":1}}],[\"在视觉和视觉\",{\"1\":{\"268\":1}}],[\"在共享图像编码器的同时\",{\"1\":{\"273\":1}}],[\"在共享空间里把图像的局部区域和点云的局部区域对应起来\",{\"1\":{\"83\":1}}],[\"在因果掩码自注意力的基础上\",{\"1\":{\"272\":1}}],[\"在一般情况下也不一定是高斯分布\",{\"1\":{\"949\":1}}],[\"在一些不够大的llm上\",{\"1\":{\"620\":1}}],[\"在一些数据集上的表现比不过c类方法\",{\"1\":{\"394\":1}}],[\"在一个周期内只能包含少量相邻的位置\",{\"1\":{\"706\":1}}],[\"在一个简单的二维情况\",{\"1\":{\"545\":1}}],[\"在一个epoch内固定教师参数\",{\"1\":{\"285\":1}}],[\"在一个掩码图像建模\",{\"1\":{\"229\":1}}],[\"在一批图文对上只需一次前向与反向传播\",{\"1\":{\"269\":1}}],[\"在单进程下运行完整的\",{\"1\":{\"293\":1}}],[\"在单一预训练阶段完成图文统一\",{\"1\":{\"269\":1}}],[\"在单模态文本表示与图像表示之间施加对比目标\",{\"1\":{\"268\":1}}],[\"在单模态和多模态数据上进行预训练\",{\"1\":{\"224\":1}}],[\"在单模态编码器上进行图文对比学习\",{\"1\":{\"198\":1}}],[\"在本节中\",{\"1\":{\"864\":1,\"870\":1}}],[\"在本代码中\",{\"1\":{\"260\":1}}],[\"在本文中\",{\"1\":{\"170\":1,\"626\":1}}],[\"在处理特定领域的专业知识时\",{\"1\":{\"828\":1}}],[\"在处理各种任务时表现出色\",{\"1\":{\"825\":1}}],[\"在处理文本时具有强大的上下文感知能力\",{\"1\":{\"824\":1}}],[\"在处理浮点型的\",{\"1\":{\"502\":1}}],[\"在处理\",{\"1\":{\"259\":1}}],[\"在标准自注意力中\",{\"1\":{\"709\":1}}],[\"在标准的\",{\"1\":{\"524\":1}}],[\"在标准\",{\"1\":{\"235\":1}}],[\"在广泛的视觉和视觉\",{\"1\":{\"225\":1}}],[\"在预训练\",{\"1\":{\"690\":1}}],[\"在预训练层面进行数据过滤\",{\"1\":{\"658\":1}}],[\"在预训练和使用该模型时\",{\"1\":{\"435\":1}}],[\"在预训练的基础上\",{\"1\":{\"340\":1}}],[\"在预训练数据准备时\",{\"1\":{\"273\":1}}],[\"在预训练阶段学习泛化模式\",{\"1\":{\"650\":1}}],[\"在预训练阶段\",{\"1\":{\"273\":1}}],[\"在预训练阶段随机掩码部分图像块并让模型预测原始视觉标记\",{\"1\":{\"227\":1}}],[\"在预训练中使用更大规模的\",{\"1\":{\"377\":1}}],[\"在预训练中\",{\"1\":{\"272\":1}}],[\"在预训练时\",{\"1\":{\"228\":1,\"425\":1}}],[\"在预训练过程中\",{\"1\":{\"223\":1,\"236\":1}}],[\"在预训练模型基础上继续使用\",{\"1\":{\"182\":1}}],[\"在纯视觉任务中也能达到甚至超过专用模型的效果\",{\"1\":{\"220\":1}}],[\"在量化过程中使用\",{\"1\":{\"216\":1}}],[\"在输入序列长度不一致时\",{\"1\":{\"741\":1}}],[\"在输入中添加可学习的前缀\",{\"1\":{\"346\":1}}],[\"在输入\",{\"1\":{\"212\":1}}],[\"在实验部分也采用了这些基准\",{\"1\":{\"655\":1}}],[\"在实验中\",{\"1\":{\"212\":1,\"231\":1,\"593\":1}}],[\"在实践中\",{\"1\":{\"569\":1}}],[\"在实践中通过梯度下降自动学习逼近策略\",{\"1\":{\"500\":1}}],[\"在实际中\",{\"1\":{\"946\":1}}],[\"在实际中要通过具体分布建模\",{\"1\":{\"932\":1}}],[\"在实际部署中可能导致意外错误\",{\"1\":{\"649\":1}}],[\"在实际的模型中\",{\"1\":{\"600\":1}}],[\"在实际训练中更稳定\",{\"1\":{\"588\":1}}],[\"在实际正例数量非常少\",{\"1\":{\"565\":1}}],[\"在实际正例数量非常少的不平衡数据集中\",{\"1\":{\"563\":1}}],[\"在实际负例数量非常少\",{\"1\":{\"564\":1}}],[\"在实际应用中可以选用常见的卷积神经网络\",{\"1\":{\"407\":1}}],[\"在实现中应当如下处理\",{\"1\":{\"803\":1}}],[\"在实现中\",{\"1\":{\"222\":1}}],[\"在实现时可采用自然语言处理\",{\"1\":{\"407\":1}}],[\"在实现时\",{\"1\":{\"135\":1}}],[\"在进行开发前\",{\"1\":{\"836\":1}}],[\"在进行普通计算\",{\"1\":{\"805\":1}}],[\"在进行\",{\"1\":{\"207\":1}}],[\"在噪声标签数据上提高学习效果\",{\"1\":{\"202\":1}}],[\"在对抗性问题上易产生幻觉\",{\"1\":{\"670\":1}}],[\"在对毒性任务加入\",{\"1\":{\"655\":1}}],[\"在对话式问答数据集上\",{\"1\":{\"641\":1}}],[\"在对话任务\",{\"1\":{\"311\":1}}],[\"在对比学习框架中\",{\"1\":{\"353\":1}}],[\"在对比学习\",{\"1\":{\"202\":1}}],[\"在对齐后的\",{\"1\":{\"83\":1}}],[\"在众多下游任务中都实现了最先进\",{\"1\":{\"183\":1}}],[\"在保证性能的同时\",{\"1\":{\"369\":1}}],[\"在保证高性能的同时显著降低了计算成本\",{\"1\":{\"195\":1}}],[\"在保留细节信息的同时兼容多样化的图像分辨率\",{\"1\":{\"331\":1}}],[\"在保留信息的同时提升训练数据质量\",{\"1\":{\"165\":1}}],[\"在保持相同计算成本下\",{\"1\":{\"681\":1}}],[\"在保持\",{\"1\":{\"235\":1}}],[\"在保持预训练高效的前提下\",{\"1\":{\"167\":1}}],[\"在速度和效率上占优\",{\"1\":{\"157\":1}}],[\"在部件分割任务中\",{\"1\":{\"157\":1}}],[\"在通道维度进行拼接\",{\"1\":{\"154\":1}}],[\"在multinli上转移embedding能提升结果\",{\"1\":{\"635\":1}}],[\"在multinli和race上的性能随着层数的变化而变化\",{\"1\":{\"635\":1}}],[\"在mmbench\",{\"1\":{\"335\":1}}],[\"在mrg中\",{\"1\":{\"142\":1}}],[\"在msg中\",{\"1\":{\"140\":1}}],[\"在密集采样的区域中学到的特征可能无法很好地泛化到稀疏采样的区域\",{\"1\":{\"139\":1}}],[\"在屋里空间或某些特定的抽象空间中\",{\"1\":{\"135\":1}}],[\"在平移不变性上也有局限性\",{\"1\":{\"131\":1}}],[\"在关系向量中额外引入\",{\"1\":{\"125\":1}}],[\"在分割任务中\",{\"1\":{\"150\":1}}],[\"在分类任务中更有用\",{\"1\":{\"733\":1}}],[\"在分类任务中\",{\"1\":{\"589\":1}}],[\"在分类任务上性能更好\",{\"1\":{\"369\":1}}],[\"在分类\",{\"1\":{\"125\":1}}],[\"在分组内进行信息交换\",{\"1\":{\"99\":1}}],[\"在源点云\",{\"1\":{\"122\":1}}],[\"在u\",{\"1\":{\"122\":1}}],[\"在邻域维度上池化\",{\"1\":{\"121\":1}}],[\"在批处理点云数据时\",{\"1\":{\"119\":1}}],[\"在局部应用自注意力\",{\"1\":{\"110\":1}}],[\"在层级空间结构中增加对局部几何布局的敏感性\",{\"1\":{\"110\":1}}],[\"在多项基准测试中超越了\",{\"1\":{\"823\":1}}],[\"在多数情况下会返回原张量的视图\",{\"1\":{\"470\":1}}],[\"在多维数组或张量索引时\",{\"1\":{\"463\":1}}],[\"在多模态对话基准\",{\"1\":{\"310\":1}}],[\"在多模态理解任务上表现突出\",{\"1\":{\"268\":1}}],[\"在多语言理解和代码生成等方面表现出色\",{\"1\":{\"823\":1}}],[\"在多语言图像→文本检索任务\",{\"1\":{\"309\":1}}],[\"在多语言版本的\",{\"1\":{\"309\":1}}],[\"在多种任务上展现了强大的零样本和迁移能力\",{\"1\":{\"268\":1}}],[\"在多种任务上都取得了最新性能\",{\"1\":{\"220\":1}}],[\"在多种条件下均能保持优秀性能\",{\"1\":{\"19\":1}}],[\"在多任务\",{\"1\":{\"202\":1}}],[\"在多个基准测试中达到与更大规模专有模型相当的性能\",{\"1\":{\"672\":1}}],[\"在多个方面仍存在不足\",{\"1\":{\"658\":1}}],[\"在多个方面改进了\",{\"1\":{\"125\":1}}],[\"在多个nlp任务上的零样本\",{\"1\":{\"641\":1}}],[\"在多个视觉任务中\",{\"1\":{\"252\":1}}],[\"在多个下游任务\",{\"1\":{\"228\":1}}],[\"在多个任务\",{\"1\":{\"165\":1}}],[\"在多个领域和数据集上进行了广泛实验\",{\"1\":{\"109\":1}}],[\"在论文的最后\",{\"1\":{\"434\":1}}],[\"在论文中\",{\"1\":{\"407\":1,\"431\":1,\"432\":1,\"960\":1}}],[\"在论文中提到\",{\"1\":{\"157\":1}}],[\"在论文\",{\"1\":{\"102\":1}}],[\"在训练公式\",{\"1\":{\"630\":1}}],[\"在训练或验证数据集可能会加入一些\",{\"1\":{\"382\":1}}],[\"在训练过程中不断把\",{\"1\":{\"947\":1}}],[\"在训练过程中发现\",{\"1\":{\"352\":1}}],[\"在训练过程中\",{\"1\":{\"339\":1,\"406\":1}}],[\"在训练数据上\",{\"1\":{\"268\":1}}],[\"在训练目标上\",{\"1\":{\"268\":1}}],[\"在训练时\",{\"1\":{\"921\":1,\"956\":1}}],[\"在训练时同时使用对比损失和生成式的描述损失\",{\"1\":{\"268\":1}}],[\"在训练时引入随机丢弃形心来模拟不同密度情况\",{\"1\":{\"141\":1}}],[\"在训练时引入不同密度的点集情况\",{\"1\":{\"141\":1}}],[\"在训练中\",{\"1\":{\"202\":1,\"220\":1,\"382\":1}}],[\"在训练中通过\",{\"1\":{\"26\":1}}],[\"在训练和测试中不作为显式监督信号\",{\"1\":{\"88\":1}}],[\"在扩展过程中\",{\"1\":{\"87\":1}}],[\"在这一节里\",{\"1\":{\"957\":1}}],[\"在这一理论指导下\",{\"1\":{\"650\":1}}],[\"在这篇文章中\",{\"1\":{\"955\":1}}],[\"在这种连续的空间里\",{\"1\":{\"847\":1}}],[\"在这种情况下\",{\"1\":{\"142\":1,\"357\":1,\"413\":1,\"626\":1,\"951\":2}}],[\"在这方面表现较差\",{\"1\":{\"822\":1}}],[\"在这之后用生成模型分配的剩下序列的平均token对数概率高的作为结果\",{\"1\":{\"635\":1}}],[\"在这里\",{\"1\":{\"600\":1,\"887\":1}}],[\"在这里一次性计算\",{\"1\":{\"380\":1}}],[\"在这个特定条件下\",{\"1\":{\"949\":1}}],[\"在这个分数上再加一个与\",{\"1\":{\"710\":1}}],[\"在这个装饰器修饰的函数内\",{\"1\":{\"473\":1}}],[\"在这个高维空间里\",{\"1\":{\"429\":1}}],[\"在这个流派只有一个编码器\",{\"1\":{\"357\":1}}],[\"在这个向量上做交叉熵\",{\"1\":{\"355\":1}}],[\"在这个过程中\",{\"1\":{\"349\":1,\"350\":1,\"355\":1,\"408\":1}}],[\"在这个任务上\",{\"1\":{\"342\":1}}],[\"在这个交互场景里\",{\"1\":{\"83\":1}}],[\"在这段代码中\",{\"1\":{\"260\":1}}],[\"在这些任务中\",{\"1\":{\"657\":1}}],[\"在这些示例中\",{\"1\":{\"620\":1}}],[\"在这些位置\",{\"1\":{\"214\":1}}],[\"在这些点中选出若干个中心点\",{\"1\":{\"132\":1}}],[\"在这根线的两端加上\",{\"1\":{\"83\":1}}],[\"在无标记数据上使用语言模型目标来学习神经网络初始化的参数\",{\"1\":{\"626\":1}}],[\"在无需ocr工具的情况下展现强大的多模态能力\",{\"1\":{\"325\":1}}],[\"在无需固定标签集的情况下增强泛化能力\",{\"1\":{\"20\":1}}],[\"在无空间先验下实现跨源特征对齐\",{\"1\":{\"76\":1}}],[\"在跨模态融合后\",{\"1\":{\"65\":1}}],[\"在混合精度下运行\",{\"1\":{\"64\":1}}],[\"在几何形状变化显著的同类物体中\",{\"1\":{\"49\":1}}],[\"在监控或自主决策等场景中\",{\"1\":{\"27\":1}}],[\"在真实世界中\",{\"1\":{\"20\":1}}],[\"在主流与损坏基准上进行大量实验\",{\"1\":{\"19\":1}}],[\"在双分支架构下实现跨模态知识整合与传播\",{\"1\":{\"19\":1}}],[\"在\",{\"1\":{\"19\":1,\"47\":1,\"53\":1,\"57\":1,\"69\":1,\"95\":1,\"97\":1,\"100\":1,\"102\":1,\"106\":2,\"107\":1,\"110\":1,\"114\":2,\"123\":1,\"137\":3,\"152\":1,\"153\":1,\"157\":1,\"171\":2,\"179\":1,\"185\":1,\"201\":1,\"212\":1,\"215\":2,\"220\":1,\"232\":1,\"236\":1,\"258\":1,\"269\":1,\"280\":4,\"282\":1,\"305\":1,\"308\":2,\"309\":1,\"310\":1,\"315\":1,\"316\":1,\"322\":1,\"342\":1,\"352\":2,\"354\":1,\"355\":1,\"356\":1,\"385\":1,\"405\":1,\"410\":1,\"425\":1,\"434\":2,\"440\":1,\"444\":1,\"470\":1,\"485\":1,\"486\":1,\"489\":3,\"521\":1,\"571\":2,\"572\":1,\"611\":2,\"614\":1,\"650\":1,\"657\":4,\"678\":1,\"691\":1,\"710\":6,\"733\":2,\"735\":1,\"737\":1,\"801\":1,\"809\":2,\"811\":1,\"816\":1,\"822\":1,\"823\":2,\"826\":1,\"833\":1,\"849\":1,\"866\":1,\"884\":1,\"885\":1,\"887\":1,\"893\":1,\"899\":1,\"913\":1,\"932\":1,\"944\":2,\"947\":1,\"959\":1,\"963\":1}}],[\"动量更新参数\",{\"1\":{\"361\":1}}],[\"动量更新系数\",{\"1\":{\"205\":1}}],[\"动量设置为了\",{\"1\":{\"356\":1}}],[\"动量这个超参数是\",{\"1\":{\"351\":1}}],[\"动量是一种加权移动平均\",{\"1\":{\"351\":1}}],[\"动量\",{\"1\":{\"293\":1}}],[\"动量教师在整个训练过程中始终优于学生模型\",{\"1\":{\"289\":1}}],[\"动量教师\",{\"1\":{\"289\":1}}],[\"动量视觉特征\",{\"1\":{\"208\":1}}],[\"动量视觉编码器\",{\"1\":{\"192\":1}}],[\"动量图像编码器输出特征\",{\"1\":{\"206\":1}}],[\"动量分支的\",{\"1\":{\"204\":1}}],[\"动量蒸馏的总体损失是对原始监督信号与伪监督信号的加权组合\",{\"1\":{\"202\":1}}],[\"动量模型生成的伪标签往往比真实标签更具多样性和语义丰富性\",{\"1\":{\"202\":1}}],[\"动量模型是对主模型参数的滑动平均版本\",{\"1\":{\"202\":1}}],[\"动量模型\",{\"1\":{\"202\":1}}],[\"动量文本编码器输出特征\",{\"1\":{\"206\":1}}],[\"动量文本编码器\",{\"1\":{\"192\":1}}],[\"动量编码器生成的归一化特征分别记为\",{\"1\":{\"199\":1}}],[\"动量编码器\",{\"1\":{\"192\":1,\"280\":1,\"282\":1}}],[\"动量编码器的更新参数\",{\"1\":{\"192\":1}}],[\"动量慢更新\",{\"1\":{\"190\":1}}],[\"动态图控制与框架灵活性展开一系列扩展与优化\",{\"1\":{\"814\":1}}],[\"动态图可视化与高阶导数构建\",{\"0\":{\"813\":1}}],[\"动态计算图\",{\"1\":{\"811\":1}}],[\"动态掩码与大批量训练\",{\"1\":{\"686\":1}}],[\"动态掩码在squad\",{\"1\":{\"683\":1}}],[\"动态掩码能持续提供新的掩码模式\",{\"1\":{\"681\":1}}],[\"动态掩码性能略优于静态掩码\",{\"1\":{\"681\":1}}],[\"动态掩码\",{\"1\":{\"678\":1,\"681\":1,\"683\":1}}],[\"动态调整样本权重\",{\"1\":{\"589\":1}}],[\"动态增加其功能\",{\"1\":{\"450\":1}}],[\"动态高分辨率策略\",{\"1\":{\"334\":1}}],[\"动态高分辨率处理\",{\"1\":{\"322\":1,\"323\":1}}],[\"动态宽高比匹配\",{\"1\":{\"331\":1}}],[\"动态分辨率的作用\",{\"1\":{\"336\":1}}],[\"动态分辨率策略\",{\"1\":{\"329\":1,\"332\":1}}],[\"动态分辨率支持和多语言优化\",{\"1\":{\"325\":1}}],[\"动态更新的\",{\"1\":{\"283\":1}}],[\"动态卷积核\",{\"1\":{\"100\":1}}],[\"动态卷积\",{\"1\":{\"100\":2}}],[\"动态融合多层次的视觉与文本特征\",{\"1\":{\"19\":1}}],[\"动作可能性\",{\"1\":{\"30\":1,\"31\":1}}],[\"动作预测\",{\"1\":{\"19\":1}}],[\"动机\",{\"1\":{\"22\":1}}],[\"映射成潜在属性\",{\"1\":{\"944\":1}}],[\"映射至\",{\"1\":{\"898\":1}}],[\"映射关系的反向求解\",{\"1\":{\"878\":1}}],[\"映射回原图像通道数\",{\"1\":{\"255\":1}}],[\"映射为模型所需的潜在因子空间\",{\"1\":{\"944\":1}}],[\"映射为实数\",{\"1\":{\"846\":1}}],[\"映射为\",{\"1\":{\"846\":1}}],[\"映射为一个\",{\"1\":{\"266\":2}}],[\"映射为离散\",{\"1\":{\"232\":1}}],[\"映射为4\",{\"1\":{\"83\":2}}],[\"映射函数\",{\"1\":{\"113\":1}}],[\"映射到图像\",{\"1\":{\"949\":1}}],[\"映射到公共\",{\"1\":{\"900\":3}}],[\"映射到实数\",{\"1\":{\"846\":1}}],[\"映射到长距离桶区间\",{\"1\":{\"710\":1}}],[\"映射到桶索引\",{\"1\":{\"710\":1}}],[\"映射到桶\",{\"1\":{\"710\":2}}],[\"映射到特征图上之后是一个大小为\",{\"1\":{\"501\":1}}],[\"映射到特征图上\",{\"1\":{\"501\":1}}],[\"映射到分类空间中去\",{\"1\":{\"431\":1}}],[\"映射到词表大小\",{\"1\":{\"274\":1}}],[\"映射到词向量\",{\"1\":{\"274\":1}}],[\"映射到教师模型输出维度\",{\"1\":{\"213\":1}}],[\"映射到量化器维度\",{\"1\":{\"213\":1}}],[\"映射到离散的视觉\",{\"1\":{\"213\":1}}],[\"映射到\",{\"1\":{\"66\":1,\"264\":2,\"385\":1,\"586\":1,\"710\":1,\"899\":1,\"945\":1}}],[\"映射\",{\"0\":{\"22\":1},\"1\":{\"19\":1,\"21\":1,\"22\":1,\"501\":1}}],[\"一直以来\",{\"1\":{\"925\":1}}],[\"一样引导生成内容\",{\"1\":{\"895\":1}}],[\"一样的人工智能\",{\"1\":{\"827\":1}}],[\"一共有\",{\"1\":{\"880\":1}}],[\"一共合并merges个高频字符对后\",{\"1\":{\"595\":2}}],[\"一定距离的\",{\"1\":{\"875\":1}}],[\"一定会发生\",{\"1\":{\"847\":1}}],[\"一方面是因为其数学处理非常方便\",{\"1\":{\"869\":1}}],[\"一词既可以指\",{\"1\":{\"847\":1}}],[\"一文详尽之scaling\",{\"1\":{\"837\":1}}],[\"一文的建议\",{\"1\":{\"647\":1}}],[\"一系列推理加速技术\",{\"1\":{\"823\":1}}],[\"一步步引导llm得出复杂问题的结果\",{\"1\":{\"622\":1}}],[\"一步步重建回原始点数量\",{\"1\":{\"94\":1}}],[\"一是完整的ltm的例子\",{\"1\":{\"622\":1}}],[\"一条是第一个\",{\"1\":{\"803\":1}}],[\"一条是只对部分的参数进行训练\",{\"1\":{\"602\":1}}],[\"一条是对全量的参数\",{\"1\":{\"602\":1}}],[\"一些常用变量的引用\",{\"1\":{\"895\":1}}],[\"一些常见的概率分布\",{\"0\":{\"854\":1}}],[\"一些\",{\"1\":{\"824\":1}}],[\"一些任务\",{\"1\":{\"648\":1}}],[\"一些损失函数具有额外的超参数\",{\"1\":{\"593\":1}}],[\"一些方法尝试使用\",{\"1\":{\"20\":1}}],[\"一起训练\",{\"1\":{\"885\":1}}],[\"一起变大或一起变小\",{\"1\":{\"574\":1}}],[\"一起送入\",{\"1\":{\"293\":1,\"421\":1}}],[\"一起送入mini\",{\"1\":{\"136\":1}}],[\"一幅图像\",{\"1\":{\"505\":1}}],[\"一维卷积进行线性变换和升维\",{\"1\":{\"663\":1}}],[\"一维的存储空间\",{\"1\":{\"540\":1}}],[\"一维数组\",{\"1\":{\"514\":1,\"518\":1}}],[\"一维\",{\"1\":{\"485\":1}}],[\"一维张量\",{\"1\":{\"481\":1,\"485\":1}}],[\"一般指通过不断发现\",{\"1\":{\"836\":1}}],[\"一般使用诸如\",{\"1\":{\"836\":1}}],[\"一般应先设定最小化目标\",{\"1\":{\"836\":1}}],[\"一般从2开始调优\",{\"1\":{\"589\":1}}],[\"一般\",{\"1\":{\"415\":2}}],[\"一般是\",{\"1\":{\"384\":1,\"892\":1}}],[\"一般模型训练都会加载多个来源不同的开源或私有数据集\",{\"1\":{\"382\":1}}],[\"一般来说\",{\"1\":{\"355\":1,\"836\":1}}],[\"一会详细讲\",{\"1\":{\"351\":1}}],[\"一句话说\",{\"1\":{\"349\":1}}],[\"一句话总结\",{\"1\":{\"157\":1,\"520\":1,\"710\":1}}],[\"一阶统计量\",{\"1\":{\"285\":1}}],[\"一半多模态\",{\"1\":{\"277\":1}}],[\"一次性\",{\"1\":{\"681\":1}}],[\"一次性计算\",{\"1\":{\"380\":1}}],[\"一次训练的成本就在上千亿美元\",{\"1\":{\"610\":1}}],[\"一次使用未扰动输入\",{\"1\":{\"269\":1}}],[\"一次使用被扰动输入\",{\"1\":{\"269\":1}}],[\"一次forward完成两个损失目标值的计算\",{\"1\":{\"268\":1}}],[\"一边是完整图像\",{\"1\":{\"264\":1}}],[\"一边喂模型的是带遮挡的图像\",{\"1\":{\"264\":1}}],[\"一张输入图像最终被编码为一个\",{\"1\":{\"963\":1}}],[\"一张用于传给视觉\",{\"1\":{\"264\":1}}],[\"一张用于喂给\",{\"1\":{\"264\":1}}],[\"一张图对应一条生成的\",{\"1\":{\"173\":1}}],[\"一张图片对应多张同物体但形状不同的点云图片\",{\"1\":{\"82\":1}}],[\"一种用于离散变量的采样技术\",{\"1\":{\"899\":1}}],[\"一种用于通用化\",{\"1\":{\"19\":1}}],[\"一种方法是在自注意力运算中将这些\",{\"1\":{\"887\":1}}],[\"一种更具鲁棒性的分布是\",{\"1\":{\"867\":1}}],[\"一种构造这类分布的方法是设定\",{\"1\":{\"866\":1}}],[\"一种假设是\",{\"1\":{\"635\":1}}],[\"一种平均值\",{\"1\":{\"567\":1}}],[\"一种思路是在转换之前\",{\"1\":{\"428\":1}}],[\"一种朴素的想法就是把一个个像素点拉平\",{\"1\":{\"426\":1}}],[\"一种比较好理解的方式\",{\"1\":{\"417\":1}}],[\"一种是平移不变形\",{\"1\":{\"422\":1}}],[\"一种是局部性\",{\"1\":{\"422\":1}}],[\"一种是常用的cnn架构resnet\",{\"1\":{\"407\":1}}],[\"一种是single\",{\"1\":{\"391\":1}}],[\"一种是单独使用\",{\"1\":{\"306\":1}}],[\"一种无标签的\",{\"1\":{\"282\":1}}],[\"一种基于图像\",{\"1\":{\"252\":1}}],[\"一套架构适配所有下游任务\",{\"1\":{\"220\":1}}],[\"一类使用多模态\",{\"1\":{\"195\":1}}],[\"一\",{\"0\":{\"524\":1,\"550\":1,\"752\":1},\"1\":{\"157\":1,\"655\":1,\"919\":1}}],[\"一致的表示空间\",{\"1\":{\"709\":1}}],[\"一致的表征对齐\",{\"1\":{\"296\":1}}],[\"一致\",{\"1\":{\"143\":1,\"334\":1}}],[\"一致性\",{\"1\":{\"356\":1}}],[\"一致性约束\",{\"1\":{\"26\":1}}],[\"一致性对齐\",{\"1\":{\"23\":1}}],[\"一致性对齐模块\",{\"1\":{\"19\":2}}],[\"一组新的关键点位置\",{\"1\":{\"141\":1}}],[\"一组点云被处理和抽象\",{\"1\":{\"133\":1}}],[\"一旦点云被划分成小的子集\",{\"1\":{\"131\":1}}],[\"一个嵌入向量\",{\"1\":{\"963\":1}}],[\"一个随机生成大图像的问题\",{\"1\":{\"956\":1}}],[\"一个比较合适的输出分布是\",{\"1\":{\"951\":1}}],[\"一个看起来像正则化参数的东西\",{\"1\":{\"951\":1}}],[\"一个直接的近似方法是采样多个\",{\"1\":{\"944\":1}}],[\"一个合理的\",{\"1\":{\"944\":1}}],[\"一个图像被不断地进模型\",{\"1\":{\"925\":1}}],[\"一个又方便采样\",{\"1\":{\"925\":1}}],[\"一个函数\",{\"1\":{\"917\":1}}],[\"一个事件的发生给我们带来多少\",{\"1\":{\"906\":1}}],[\"一个事件总体概率\",{\"1\":{\"850\":1}}],[\"一个人检测阳性的总体概率是多少\",{\"1\":{\"850\":1}}],[\"一个人检测为阳性的总体概率是多少\",{\"1\":{\"850\":1}}],[\"一个把\",{\"1\":{\"846\":1}}],[\"一个相对距离桶的偏置\",{\"1\":{\"710\":1}}],[\"一个头的所有偏置\",{\"1\":{\"710\":1}}],[\"一个头关注长距离\",{\"1\":{\"710\":1}}],[\"一个周期能够包含更多的位置信息\",{\"1\":{\"706\":1}}],[\"一个具有1750亿参数的自回归语言模型\",{\"1\":{\"645\":1}}],[\"一个问题\",{\"1\":{\"631\":1}}],[\"一个问题可以作用于多个物体类别\",{\"1\":{\"89\":1}}],[\"一个例子帮助理解\",{\"1\":{\"574\":1}}],[\"一个或多个数组\",{\"1\":{\"513\":1}}],[\"一个简单的技巧是在你的prompt后面\",{\"1\":{\"619\":1}}],[\"一个简单的例子如下所示\",{\"1\":{\"504\":1}}],[\"一个简单的线性层\",{\"1\":{\"341\":1}}],[\"一个新的张量\",{\"1\":{\"491\":1}}],[\"一个布尔值\",{\"1\":{\"490\":1}}],[\"一个布尔型张量\",{\"1\":{\"476\":1}}],[\"一个多维张量在内存中实际上是以一维数组的形式存储的\",{\"1\":{\"489\":1}}],[\"一个可迭代对象\",{\"1\":{\"466\":1}}],[\"一个装饰器\",{\"1\":{\"457\":1}}],[\"一个装饰器的实现\",{\"1\":{\"449\":1}}],[\"一个block之后维度依然和输入相同\",{\"1\":{\"429\":1}}],[\"一个改进的想法就是把一张图片分成nxn个patch\",{\"1\":{\"426\":1}}],[\"一个批次的数据\",{\"1\":{\"424\":1}}],[\"一个批次图像经过增强后\",{\"1\":{\"293\":1}}],[\"一个文件夹对应一个类别\",{\"1\":{\"424\":1}}],[\"一个的轻量q\",{\"1\":{\"415\":1}}],[\"一个视觉模型和一个文本模型\",{\"1\":{\"408\":1}}],[\"一个\",{\"1\":{\"368\":1,\"444\":1,\"482\":1,\"694\":1}}],[\"一个one\",{\"1\":{\"355\":1}}],[\"一个很大的优势在于\",{\"1\":{\"352\":1}}],[\"一个是该\",{\"1\":{\"733\":1}}],[\"一个是叫灾难性遗忘\",{\"1\":{\"602\":1}}],[\"一个是训练的成本会比较高\",{\"1\":{\"602\":1}}],[\"一个是image\",{\"1\":{\"393\":1}}],[\"一个是保证字典内特征一致性\",{\"1\":{\"356\":1}}],[\"一个是构建很大的字典\",{\"1\":{\"356\":1}}],[\"一个是正常样本\",{\"1\":{\"355\":1}}],[\"一个是e12\",{\"1\":{\"353\":1}}],[\"一个是e11\",{\"1\":{\"353\":1}}],[\"一个是\",{\"1\":{\"350\":2}}],[\"一个最经典的代理任务就是\",{\"1\":{\"350\":1}}],[\"一个封装类\",{\"1\":{\"293\":1}}],[\"一个统一的\",{\"1\":{\"268\":1}}],[\"一个通用的多模态基础模型\",{\"1\":{\"225\":1}}],[\"一个理想的方向是\",{\"1\":{\"220\":1}}],[\"一个列表\",{\"1\":{\"145\":1}}],[\"一个非法索引\",{\"1\":{\"137\":1}}],[\"一个元组\",{\"1\":{\"137\":1,\"152\":1,\"154\":1}}],[\"一个分支处理\",{\"1\":{\"19\":1}}],[\"一名普通但十分热爱探索技术的coder\",{\"1\":{\"2\":1}}],[\"鲁棒性不足\",{\"1\":{\"22\":1}}],[\"鲁棒性\",{\"1\":{\"19\":1}}],[\"和原图像\",{\"1\":{\"956\":1}}],[\"和原始\",{\"1\":{\"274\":1}}],[\"和输出\",{\"1\":{\"952\":1}}],[\"和玻尔兹曼机\",{\"1\":{\"950\":1}}],[\"和方差\",{\"1\":{\"947\":1,\"949\":1}}],[\"和随机梯度下降中常用的做法一样\",{\"1\":{\"946\":1}}],[\"和后验分布\",{\"1\":{\"945\":1,\"959\":1}}],[\"和模型参数无关\",{\"1\":{\"931\":1}}],[\"和模块化设计\",{\"1\":{\"500\":1}}],[\"和真实标签分布\",{\"1\":{\"910\":1}}],[\"和训练时保持一致\",{\"1\":{\"899\":1}}],[\"和长文本推理\",{\"1\":{\"823\":1}}],[\"和长程依赖任务\",{\"1\":{\"641\":1}}],[\"和指令微调版\",{\"1\":{\"823\":1}}],[\"和在线推理阶段的\",{\"1\":{\"822\":1}}],[\"和funcs\",{\"1\":{\"815\":1}}],[\"和函数节点\",{\"1\":{\"815\":1}}],[\"和variable\",{\"1\":{\"809\":1}}],[\"和头维度\",{\"1\":{\"709\":1}}],[\"和每个维度\",{\"1\":{\"706\":1}}],[\"和每个查询点上\",{\"1\":{\"137\":1}}],[\"和xlnet\",{\"1\":{\"685\":1}}],[\"和下游任务准确率\",{\"1\":{\"683\":1}}],[\"和衰减策略\",{\"1\":{\"679\":1}}],[\"和bloom\",{\"1\":{\"671\":1}}],[\"和bert\",{\"1\":{\"641\":1}}],[\"和人文任务\",{\"1\":{\"669\":1}}],[\"和palm\",{\"1\":{\"668\":1}}],[\"和mbpp\",{\"1\":{\"668\":1}}],[\"和mlp比率\",{\"1\":{\"303\":1}}],[\"和工程创新\",{\"1\":{\"667\":1}}],[\"和书籍\",{\"1\":{\"667\":1}}],[\"和质量过滤\",{\"1\":{\"667\":1}}],[\"和数学推理\",{\"1\":{\"666\":1}}],[\"和数据集\",{\"1\":{\"293\":1}}],[\"和位于词序列的索引\",{\"1\":{\"663\":1}}],[\"和聊天合计约\",{\"1\":{\"656\":1}}],[\"和少样本\",{\"1\":{\"646\":1}}],[\"和假设\",{\"1\":{\"631\":1}}],[\"和假阴性\",{\"1\":{\"590\":1}}],[\"和假正例率\",{\"1\":{\"569\":1}}],[\"和负类\",{\"1\":{\"589\":1}}],[\"和特征间的协方差\",{\"1\":{\"578\":1}}],[\"和这些曲线下的面积可以更好地直观比较模型性能\",{\"1\":{\"571\":1}}],[\"和所有真实负例\",{\"1\":{\"561\":1}}],[\"和所有预测负例\",{\"1\":{\"561\":1}}],[\"和步长\",{\"1\":{\"540\":1}}],[\"和保存\",{\"1\":{\"474\":1}}],[\"和transformer中的一样\",{\"1\":{\"429\":1}}],[\"和text都能和所有的tokens\",{\"1\":{\"419\":1}}],[\"和冻结参数的\",{\"1\":{\"421\":1}}],[\"和基于图像掩码的方法\",{\"1\":{\"413\":1}}],[\"和缩放因子\",{\"1\":{\"385\":1}}],[\"和视觉\",{\"1\":{\"372\":1}}],[\"和视觉标记\",{\"1\":{\"228\":1}}],[\"和掩码语言建模\",{\"1\":{\"370\":1}}],[\"和我真实输出做一个损失\",{\"1\":{\"355\":1}}],[\"和当前时刻的\",{\"1\":{\"353\":1}}],[\"和当前输出的\",{\"1\":{\"208\":1}}],[\"和队列大小分离开\",{\"1\":{\"353\":1}}],[\"和语言模型\",{\"1\":{\"342\":1}}],[\"和对应的图像\",{\"1\":{\"885\":1}}],[\"和对应的数据模块实例\",{\"1\":{\"382\":1}}],[\"和对应的\",{\"1\":{\"339\":1}}],[\"和对比学习\",{\"1\":{\"246\":1}}],[\"和中文理解方面表现突出\",{\"1\":{\"337\":1}}],[\"和中文相关任务中的表现\",{\"1\":{\"322\":1}}],[\"和高质量双语数据集\",{\"1\":{\"337\":1}}],[\"和类别频率成反比\",{\"1\":{\"514\":1}}],[\"和类别\",{\"1\":{\"319\":1}}],[\"和其他生成网络的对比情况\",{\"1\":{\"925\":1}}],[\"和其他参数\",{\"1\":{\"382\":1}}],[\"和其他多语言模型\",{\"1\":{\"309\":1}}],[\"和其对应的\",{\"1\":{\"32\":1}}],[\"和跨注意力层\",{\"1\":{\"306\":1}}],[\"和文本分类\",{\"1\":{\"626\":1}}],[\"和文本\",{\"1\":{\"369\":1,\"370\":1,\"374\":1,\"376\":1,\"420\":1}}],[\"和文本编码器\",{\"1\":{\"305\":1}}],[\"和文本用相同的方式建模\",{\"1\":{\"220\":1}}],[\"和生成任务\",{\"1\":{\"303\":1}}],[\"和双塔模型\",{\"1\":{\"303\":1}}],[\"和80亿参数的语言中间件\",{\"1\":{\"296\":1}}],[\"和多分类任务一样\",{\"1\":{\"921\":1}}],[\"和多分辨率分组\",{\"1\":{\"139\":1}}],[\"和多视图\",{\"1\":{\"293\":1}}],[\"和若干张局部裁剪图像\",{\"1\":{\"293\":1}}],[\"和目标\",{\"1\":{\"265\":1}}],[\"和平均绝对误差\",{\"1\":{\"259\":1}}],[\"和重建图像\",{\"1\":{\"256\":1}}],[\"和音频建模\",{\"1\":{\"249\":1}}],[\"和离散视觉标记\",{\"1\":{\"227\":1}}],[\"和一个解码器\",{\"1\":{\"956\":1}}],[\"和一个从\",{\"1\":{\"946\":1}}],[\"和一个问题\",{\"1\":{\"694\":1}}],[\"和一个可能答案集\",{\"1\":{\"631\":1}}],[\"和一个\",{\"1\":{\"222\":2}}],[\"和向量维度\",{\"1\":{\"215\":1}}],[\"和被\",{\"1\":{\"202\":1}}],[\"和图\",{\"1\":{\"648\":3}}],[\"和图文匹配\",{\"1\":{\"198\":1}}],[\"和图像编码器\",{\"1\":{\"407\":1}}],[\"和图像\",{\"1\":{\"372\":1,\"891\":1,\"893\":1}}],[\"和图像类型嵌入\",{\"1\":{\"371\":1}}],[\"和图像分类任务\",{\"1\":{\"304\":1}}],[\"和图像分析中已经非常成功\",{\"1\":{\"112\":1}}],[\"和图像整体的表征能力\",{\"1\":{\"215\":1}}],[\"和图像特征\",{\"1\":{\"30\":1}}],[\"和第\",{\"1\":{\"179\":1}}],[\"和稀疏采样的区域\",{\"1\":{\"140\":1}}],[\"和解码器\",{\"1\":{\"122\":2}}],[\"和邻域内的\",{\"1\":{\"119\":1}}],[\"和点维度的平均\",{\"1\":{\"102\":1}}],[\"和点云特征\",{\"1\":{\"39\":1}}],[\"和通道混合\",{\"1\":{\"97\":1}}],[\"和通道维度\",{\"1\":{\"69\":1}}],[\"和值\",{\"1\":{\"96\":1,\"430\":2,\"823\":2}}],[\"和物体\",{\"1\":{\"80\":1}}],[\"和联合注意力\",{\"1\":{\"80\":1}}],[\"和arm模块\",{\"1\":{\"73\":1}}],[\"和自然语言视觉推理\",{\"1\":{\"368\":1}}],[\"和自然语言理解出发\",{\"1\":{\"31\":1}}],[\"和自注意力在\",{\"1\":{\"112\":1}}],[\"和自注意力机制融合图像与点云特征\",{\"1\":{\"64\":1}}],[\"和知识特征\",{\"1\":{\"38\":1}}],[\"和可供性意图特征\",{\"1\":{\"32\":1}}],[\"和38k\",{\"1\":{\"30\":1}}],[\"和同一物体的多交互意图关联\",{\"1\":{\"30\":1}}],[\"和不透明度\",{\"1\":{\"22\":1}}],[\"和\",{\"0\":{\"346\":1,\"536\":1,\"572\":1,\"928\":1},\"1\":{\"19\":2,\"25\":1,\"31\":1,\"32\":2,\"56\":1,\"64\":3,\"66\":1,\"69\":5,\"70\":1,\"78\":1,\"83\":4,\"92\":1,\"94\":1,\"106\":3,\"110\":1,\"119\":1,\"133\":1,\"152\":1,\"153\":1,\"171\":3,\"173\":1,\"176\":4,\"183\":1,\"185\":2,\"187\":1,\"191\":1,\"192\":1,\"199\":2,\"202\":1,\"207\":1,\"208\":5,\"212\":1,\"213\":1,\"215\":2,\"216\":1,\"220\":1,\"224\":2,\"230\":1,\"232\":1,\"235\":1,\"242\":2,\"249\":1,\"254\":1,\"255\":1,\"264\":1,\"268\":1,\"269\":3,\"271\":1,\"272\":4,\"274\":4,\"280\":2,\"285\":2,\"286\":1,\"290\":1,\"293\":2,\"305\":1,\"306\":2,\"309\":1,\"310\":1,\"311\":1,\"316\":1,\"318\":2,\"327\":2,\"332\":1,\"334\":1,\"343\":1,\"346\":1,\"354\":2,\"357\":1,\"371\":1,\"373\":2,\"377\":1,\"380\":4,\"381\":1,\"382\":4,\"386\":1,\"399\":1,\"403\":1,\"405\":3,\"415\":3,\"418\":3,\"420\":2,\"426\":1,\"434\":2,\"452\":1,\"468\":1,\"506\":1,\"524\":1,\"546\":1,\"569\":1,\"570\":1,\"571\":1,\"572\":3,\"574\":2,\"586\":1,\"587\":1,\"588\":1,\"589\":1,\"590\":2,\"592\":2,\"594\":1,\"595\":1,\"609\":5,\"611\":1,\"619\":1,\"648\":1,\"655\":1,\"656\":4,\"657\":4,\"663\":3,\"678\":2,\"679\":1,\"680\":1,\"692\":1,\"694\":1,\"696\":2,\"699\":2,\"703\":1,\"706\":2,\"709\":1,\"731\":1,\"733\":2,\"734\":2,\"735\":1,\"746\":1,\"801\":1,\"809\":3,\"822\":4,\"823\":11,\"830\":2,\"833\":1,\"848\":1,\"849\":2,\"851\":1,\"885\":1,\"886\":1,\"887\":1,\"893\":1,\"899\":1,\"900\":1,\"912\":1,\"914\":1,\"915\":3,\"923\":1,\"924\":5,\"925\":2,\"928\":1,\"931\":2,\"937\":2,\"945\":1,\"946\":6,\"949\":1,\"951\":2}}],[\"这么做不是最高效的\",{\"1\":{\"958\":1}}],[\"这通常表现为模糊无意义的\",{\"1\":{\"952\":1}}],[\"这需要一定的信息量\",{\"1\":{\"950\":1}}],[\"这需要一个\",{\"1\":{\"131\":1}}],[\"这正是\",{\"1\":{\"932\":1}}],[\"这正好对应\",{\"1\":{\"709\":1}}],[\"这项衡量的是\",{\"1\":{\"932\":1}}],[\"这项工作为后续gpt系列模型的发展奠定了理论基础和方法框架\",{\"1\":{\"643\":1}}],[\"这让模型能更好地捕捉本通道的历史上下文\",{\"1\":{\"924\":1}}],[\"这很简单\",{\"1\":{\"924\":1}}],[\"这下\",{\"1\":{\"923\":1}}],[\"这打破了\",{\"1\":{\"923\":1}}],[\"这满足\",{\"1\":{\"923\":1}}],[\"这次的改进并不能加速采样\",{\"1\":{\"921\":1}}],[\"这相当于在图像上单独训练一个\",{\"1\":{\"886\":1}}],[\"这相当于使了一个障眼法\",{\"1\":{\"213\":1}}],[\"这\",{\"1\":{\"880\":1}}],[\"这非常反直觉\",{\"1\":{\"875\":1}}],[\"这等价于找出\",{\"1\":{\"846\":1}}],[\"这对于\",{\"1\":{\"833\":1}}],[\"这影响了其对问题的理解和回答\",{\"1\":{\"828\":1}}],[\"这可能意味着\",{\"1\":{\"949\":1}}],[\"这可能会影响到其在相关领域的回答质量\",{\"1\":{\"828\":1}}],[\"这可能导致模型的知识更新滞后\",{\"1\":{\"828\":1}}],[\"这可能是因为参数量增加需要更多的语料\",{\"1\":{\"613\":1}}],[\"这引发了对未来人工智能发展的许多思考和计划\",{\"1\":{\"827\":1}}],[\"这引发我们对于\",{\"1\":{\"827\":1}}],[\"这被称为似然函数\",{\"1\":{\"877\":1}}],[\"这被称为观测分布\",{\"1\":{\"877\":1}}],[\"这被称为先验分布\",{\"1\":{\"877\":1}}],[\"这被称为\",{\"1\":{\"822\":1}}],[\"这和原始论文稍有不同\",{\"1\":{\"745\":1}}],[\"这和使用\",{\"1\":{\"352\":1}}],[\"这比较容易并行\",{\"1\":{\"740\":1}}],[\"这段代码的意思是\",{\"1\":{\"735\":1}}],[\"这三张图分别打印了\",{\"1\":{\"706\":1}}],[\"这三个关键要素是\",{\"1\":{\"544\":1}}],[\"这三个步骤构成了一个完整的跨模态融合流程\",{\"1\":{\"95\":1}}],[\"这尤其适合当需要长时依赖\",{\"1\":{\"696\":1}}],[\"这本质上是一个三分类的问题\",{\"1\":{\"694\":1}}],[\"这本书里介绍了我们人类大脑的\",{\"1\":{\"619\":1}}],[\"这其实是一个好处\",{\"1\":{\"951\":1}}],[\"这其实是一个很容易理解的任务\",{\"1\":{\"691\":1}}],[\"这其实是\",{\"1\":{\"877\":1}}],[\"这其实就是在说\",{\"1\":{\"735\":1}}],[\"这其实也是一种标签信息\",{\"1\":{\"349\":1}}],[\"这为后续的变分推断方法奠定了基础\",{\"1\":{\"944\":1}}],[\"这为自动微分奠定了基础\",{\"1\":{\"767\":1}}],[\"这为\",{\"1\":{\"656\":1}}],[\"这为未来探索更灵活的文本生成方式提供了启示\",{\"1\":{\"642\":1}}],[\"这造成了目标的不一致\",{\"1\":{\"654\":1}}],[\"这说明gpt\",{\"1\":{\"648\":1}}],[\"这得益于更高效的数据利用率\",{\"1\":{\"647\":1}}],[\"这限制了其在金融\",{\"1\":{\"649\":1}}],[\"这限制了模型的广泛应用\",{\"1\":{\"646\":1}}],[\"这限制了它们在许多缺乏标记数据领域的适用性\",{\"1\":{\"626\":1}}],[\"这跟之前的工作一样\",{\"1\":{\"630\":1}}],[\"这句话会引导llm\",{\"1\":{\"619\":1}}],[\"这句话没看懂没关系\",{\"1\":{\"351\":1}}],[\"这会占用大量的内存资源并消耗较多的计算资源\",{\"1\":{\"614\":1}}],[\"这会导致无法对模型参数求导\",{\"1\":{\"946\":1}}],[\"这会导致如下图所示的共享变量a的梯度被重复累加\",{\"1\":{\"804\":1}}],[\"这会导致在跨任务上性能降低14\",{\"1\":{\"635\":1}}],[\"这会导致尺度大的特征\",{\"1\":{\"578\":1}}],[\"这会导致特征图上的空间对齐误差\",{\"1\":{\"502\":1}}],[\"这会导致\",{\"1\":{\"235\":1}}],[\"这主要是因为如果矩阵\",{\"1\":{\"612\":1}}],[\"这条路径叫peft\",{\"1\":{\"602\":1}}],[\"这条路径叫全量微调fft\",{\"1\":{\"602\":1}}],[\"这表示从\",{\"1\":{\"860\":1}}],[\"这表示在内存中访问该张量时\",{\"1\":{\"542\":1}}],[\"这表明它在需要长期优化和知识整合的任务中仍有较大局限\",{\"1\":{\"649\":1}}],[\"这表明其具有一定程度的推理和快速适应能力\",{\"1\":{\"648\":1}}],[\"这表明大模型能够更好地吸收语言知识和上下文信息\",{\"1\":{\"648\":1}}],[\"这表明预训练模型中的每一层都包含了解决目标问题有用的功能\",{\"1\":{\"635\":1}}],[\"这表明训练clip模型需要消耗大量的资源\",{\"1\":{\"407\":1}}],[\"这表明\",{\"1\":{\"283\":1,\"330\":1,\"884\":1}}],[\"这有助于模型发现输入数据中更复杂的模式和关系\",{\"1\":{\"429\":1}}],[\"这也算是一类事件\",{\"1\":{\"847\":1}}],[\"这也是变分推断的精髓所在\",{\"1\":{\"947\":1}}],[\"这也是我们理解的编码全局位置信息的含义\",{\"1\":{\"706\":1}}],[\"这也是一个非常严重的问题\",{\"1\":{\"601\":1}}],[\"这也是为什么结构图中mlp\",{\"1\":{\"427\":1}}],[\"这也间接说明\",{\"1\":{\"182\":1}}],[\"这几乎是不可能完成的任务\",{\"1\":{\"422\":1}}],[\"这减少了llm学习视觉语言对齐的负担\",{\"1\":{\"421\":1}}],[\"这大大限制了它们的迁移能力和扩展性\",{\"1\":{\"413\":1}}],[\"这远远低于imagenet上的sota\",{\"1\":{\"413\":1}}],[\"这方面的工作并不多\",{\"1\":{\"413\":1}}],[\"这展示了其在图像分类任务中的灵活性和强大能力\",{\"1\":{\"408\":1}}],[\"这不刚好嘛\",{\"1\":{\"956\":1}}],[\"这不仅展示了clip的强大功能\",{\"1\":{\"408\":1}}],[\"这不就是\",{\"1\":{\"354\":1}}],[\"这保证了评估指标\",{\"1\":{\"382\":1}}],[\"这时我们不再使用编码器\",{\"1\":{\"947\":1}}],[\"这时分类分布可以写为\",{\"1\":{\"857\":1}}],[\"这时候cot的效果就不尽如人意\",{\"1\":{\"622\":1}}],[\"这时候也需要对大模型进行微调\",{\"1\":{\"601\":1}}],[\"这时候针对每个用户的数据\",{\"1\":{\"601\":1}}],[\"这时候微调就非常适用\",{\"1\":{\"601\":1}}],[\"这时候就要去掉干扰\",{\"1\":{\"382\":1}}],[\"这时就可以自定义\",{\"1\":{\"424\":1}}],[\"这时使用动量更新即可\",{\"1\":{\"353\":1}}],[\"这边\",{\"1\":{\"357\":1}}],[\"这要求\",{\"1\":{\"357\":1}}],[\"这篇论文介绍了gpt\",{\"1\":{\"645\":1}}],[\"这篇论文\",{\"1\":{\"638\":1}}],[\"这篇论文中介绍的方法\",{\"1\":{\"622\":1}}],[\"这篇论文里讲的另一个prompt\",{\"1\":{\"621\":1}}],[\"这篇论文里讲的一个prompt\",{\"1\":{\"620\":1}}],[\"这篇论文首次尝试使用仅支持文本输入的\",{\"1\":{\"339\":1}}],[\"这篇论文提出了一项新的任务和一个配套的数据集\",{\"1\":{\"84\":1}}],[\"这篇论文提出了一种新颖的任务设定\",{\"1\":{\"72\":1}}],[\"这带来了工程上的挑战\",{\"1\":{\"223\":1}}],[\"这使得我们可以更容易地计算\",{\"1\":{\"945\":1}}],[\"这使得开发者能够对\",{\"1\":{\"833\":1}}],[\"这使得它们在对话\",{\"1\":{\"824\":1}}],[\"这使得它们能够捕捉更多的语言知识和复杂的语法结构\",{\"1\":{\"824\":1}}],[\"这使得它在细粒度识别\",{\"1\":{\"157\":1}}],[\"这使得其语义编码能力得到了极大的增强\",{\"1\":{\"823\":1}}],[\"这使得当表达式为np\",{\"1\":{\"809\":1}}],[\"这使得\",{\"1\":{\"273\":1,\"885\":1}}],[\"这使得用统一网络结构来处理不同模态成为可能\",{\"1\":{\"220\":1}}],[\"这部分的\",{\"1\":{\"959\":1}}],[\"这部分关键参数就是上面提到的低维的本质模型\",{\"1\":{\"606\":1}}],[\"这部分数据是过时的\",{\"1\":{\"356\":1}}],[\"这部分被当作了常量\",{\"1\":{\"213\":1}}],[\"这部分代码实现如下\",{\"1\":{\"96\":1,\"97\":1}}],[\"这整个过程就是\",{\"1\":{\"213\":1}}],[\"这启发了\",{\"1\":{\"210\":1}}],[\"这与稀疏自编码器\",{\"1\":{\"951\":1}}],[\"这与原始bert的结论相反\",{\"1\":{\"681\":1}}],[\"这与few\",{\"1\":{\"650\":1}}],[\"这与多项式逼近等传统方法有本质区别\",{\"1\":{\"500\":1}}],[\"这与传统的预训练加微调的方法有所不同\",{\"1\":{\"409\":1}}],[\"这与知识蒸馏领域的常见做法一致\",{\"1\":{\"182\":1}}],[\"这与在传统cnn中学习图像局部区域特征的过程相似\",{\"1\":{\"131\":1}}],[\"这就定义了随机变量\",{\"1\":{\"846\":1}}],[\"这就构成了函数与变量的\",{\"1\":{\"805\":1}}],[\"这就实现了\",{\"1\":{\"578\":1}}],[\"这就产生了一个问题\",{\"1\":{\"501\":1}}],[\"这就引出了两类掩码的配合使用\",{\"1\":{\"924\":1}}],[\"这就引出了\",{\"1\":{\"258\":1}}],[\"这就要求网络中的某些关键操作必须是对称函数\",{\"1\":{\"160\":1}}],[\"这就是一个新的\",{\"1\":{\"947\":1}}],[\"这就是为什么我们说样本会集中在距离原点约为\",{\"1\":{\"874\":1}}],[\"这就是贝叶斯法则在离散和连续两种情形下的表达方式\",{\"1\":{\"853\":1}}],[\"这就是梯度下降法\",{\"1\":{\"816\":1}}],[\"这就是答案\",{\"1\":{\"735\":1}}],[\"这就是捕捉局部位置关系\",{\"1\":{\"706\":1}}],[\"这就是我们在\",{\"1\":{\"932\":1}}],[\"这就是我们在优化的\",{\"1\":{\"932\":1}}],[\"这就是我们要去学习prompt\",{\"1\":{\"616\":1}}],[\"这就是我们熟悉的\",{\"1\":{\"578\":1}}],[\"这就是我们的掩码图像建模\",{\"1\":{\"235\":1}}],[\"这就是blip\",{\"1\":{\"415\":1}}],[\"这就是\",{\"1\":{\"153\":1,\"619\":1,\"846\":1}}],[\"这就是下一个\",{\"1\":{\"137\":2}}],[\"这在计算上代价很高\",{\"1\":{\"946\":1}}],[\"这在模拟某些\",{\"1\":{\"863\":1}}],[\"这在lora这篇论文中也被称为低秩分解自适应技术\",{\"1\":{\"609\":1}}],[\"这在某些场景下会导致位置偏差\",{\"1\":{\"501\":1}}],[\"这在调试\",{\"1\":{\"454\":1}}],[\"这在日志记录\",{\"1\":{\"450\":1}}],[\"这在处理非均匀采样的数据时可能不是最优的选择\",{\"1\":{\"135\":1}}],[\"这在使用\",{\"1\":{\"107\":1}}],[\"这实际上是一种分组通道注意力\",{\"1\":{\"119\":1}}],[\"这两种信息在表示形式和语义侧重点上存在差异\",{\"1\":{\"706\":1}}],[\"这两种表示方式分别作为预训练中的输入和输出\",{\"1\":{\"230\":1}}],[\"这两种特征被concat为一个复合特征向量\",{\"1\":{\"142\":1}}],[\"这两类自注意力算子本质上都是集合算子\",{\"1\":{\"112\":1}}],[\"这两个问题的一个重要难点是\",{\"1\":{\"952\":1}}],[\"这两个网络互相对抗\",{\"1\":{\"918\":1}}],[\"这两个网络输出的是变换矩阵\",{\"1\":{\"153\":1}}],[\"这两个向量都在同一个向量空间中\",{\"1\":{\"709\":1}}],[\"这两个数表示\",{\"1\":{\"694\":1}}],[\"这两个模型都属于融合图像与文本的多模态模型\",{\"1\":{\"405\":1}}],[\"这两个操作交替进行\",{\"1\":{\"97\":1}}],[\"这两个基准数据集基于常用的\",{\"1\":{\"19\":1}}],[\"这意味着在\",{\"1\":{\"951\":1}}],[\"这意味着重复计算\",{\"1\":{\"663\":1}}],[\"这意味着对齐方法不仅优化模型行为\",{\"1\":{\"658\":1}}],[\"这意味着few\",{\"1\":{\"649\":1}}],[\"这意味着它能够在没有任何特定任务训练数据的情况下\",{\"1\":{\"408\":1}}],[\"这意味着两种损失的大部分计算是共享的\",{\"1\":{\"272\":1}}],[\"这意味着即使是不同的局部子集\",{\"1\":{\"131\":1}}],[\"这意味着\",{\"1\":{\"106\":1,\"157\":1,\"570\":1,\"574\":1,\"825\":1,\"874\":1,\"917\":1,\"944\":1}}],[\"这是变分自编码器\",{\"1\":{\"935\":1}}],[\"这是最大值\",{\"1\":{\"916\":1}}],[\"这是视觉场景理解中的一个经典问题\",{\"1\":{\"878\":1}}],[\"这是由python的运算符调度机制决定的\",{\"1\":{\"809\":1}}],[\"这是抽取式问答模型的局限性\",{\"1\":{\"735\":1}}],[\"这是固定的位置编码\",{\"1\":{\"707\":1}}],[\"这是毋庸置疑的\",{\"1\":{\"694\":1}}],[\"这是因为vq\",{\"1\":{\"956\":1}}],[\"这是因为\",{\"1\":{\"925\":1}}],[\"这是因为样本几乎全部落在离\",{\"1\":{\"875\":1}}],[\"这是因为同一个变量对输出的影响路径有多条\",{\"1\":{\"803\":1}}],[\"这是因为它们的训练目标是最大化互联网文本的下一个词预测概率\",{\"1\":{\"654\":1}}],[\"这是因为在实际采集过程中\",{\"1\":{\"152\":1}}],[\"这是我们使用llm的人的职责\",{\"1\":{\"616\":1}}],[\"这是交叉熵损失\",{\"1\":{\"589\":1}}],[\"这是通过\",{\"1\":{\"544\":1}}],[\"这是注意力权重矩阵的来源\",{\"1\":{\"526\":1}}],[\"这是它最大的优势\",{\"1\":{\"470\":1}}],[\"这是一项本质上的进步\",{\"1\":{\"826\":1}}],[\"这是一种全新的\",{\"1\":{\"826\":1}}],[\"这是一种数据增强的方式\",{\"1\":{\"425\":1}}],[\"这是一款在推理和通用任务上有显著提升的模型\",{\"1\":{\"823\":1}}],[\"这是一个普通的\",{\"1\":{\"963\":1}}],[\"这是一个从\",{\"1\":{\"897\":1}}],[\"这是一个根本上的病态问题\",{\"1\":{\"878\":1}}],[\"这是一个一举多得\",{\"1\":{\"826\":1}}],[\"这是一个轻量级的检查操作\",{\"1\":{\"490\":1}}],[\"这是一个标准的自监督学习任务\",{\"1\":{\"220\":1}}],[\"这是一个基于向量量化的知识蒸馏模型\",{\"1\":{\"213\":1}}],[\"这是一个椅子\",{\"1\":{\"156\":1}}],[\"这是首次尝试将大语言模型用于模型集成\",{\"1\":{\"343\":1}}],[\"这是实现\",{\"1\":{\"342\":1}}],[\"这是后续指令调优的基础\",{\"1\":{\"340\":1}}],[\"这是第一个系统性地将\",{\"1\":{\"339\":1}}],[\"这是主流做法\",{\"1\":{\"306\":1}}],[\"这是为了让字典里的特征尽可能保持一致\",{\"1\":{\"352\":1}}],[\"这是为了满足\",{\"1\":{\"262\":1}}],[\"这是为了每个\",{\"1\":{\"188\":1}}],[\"这是\",{\"1\":{\"100\":1,\"137\":1,\"208\":1,\"262\":1,\"339\":1,\"380\":1,\"420\":1,\"620\":1,\"621\":1,\"622\":1}}],[\"这是训练中未曾遇到过的功能\",{\"1\":{\"89\":1}}],[\"这里为了与标准机器学习的习惯保持一致\",{\"1\":{\"952\":1}}],[\"这里为了方便理解\",{\"1\":{\"600\":1}}],[\"这里提到的函数\",{\"1\":{\"949\":1}}],[\"这里生成器的目标是让判别器认为生成样本是真的\",{\"1\":{\"918\":1}}],[\"这里设置为1\",{\"1\":{\"892\":2}}],[\"这里设置为图像块的大小\",{\"1\":{\"426\":2}}],[\"这里关于\",{\"1\":{\"892\":1}}],[\"这里他们写作\",{\"1\":{\"885\":1}}],[\"这里试验顺序重要\",{\"1\":{\"860\":1}}],[\"这里将数值数据封装在variable中\",{\"1\":{\"816\":1}}],[\"这里我准备做一个文本分类任务\",{\"1\":{\"712\":1}}],[\"这里保持一致性\",{\"1\":{\"710\":1}}],[\"这里pad部分指的是对于不同的句子\",{\"1\":{\"700\":1}}],[\"这里也非常像我们人类学习解决复杂问题的过程\",{\"1\":{\"622\":1}}],[\"这里有像我们人类解决问题的过程\",{\"1\":{\"621\":1}}],[\"这里先简单介绍一下\",{\"1\":{\"607\":1}}],[\"这里面∆w主是我们要微调得到的结果\",{\"1\":{\"606\":1}}],[\"这里把所有\",{\"1\":{\"592\":1}}],[\"这里采用的是\",{\"1\":{\"696\":1}}],[\"这里采用的方法是一种\",{\"1\":{\"589\":1}}],[\"这里采用了余弦相似度的计算方法\",{\"1\":{\"410\":1}}],[\"这里使用符号\",{\"1\":{\"877\":1}}],[\"这里使用的是负的\",{\"1\":{\"592\":1}}],[\"这里使用的是之前讲过的个体判别任务\",{\"1\":{\"353\":1}}],[\"这里使用\",{\"1\":{\"589\":1}}],[\"这里暂时未使用\",{\"1\":{\"587\":1}}],[\"这里解释一下\",{\"1\":{\"545\":1}}],[\"这里会报错\",{\"1\":{\"492\":1}}],[\"这里按\",{\"1\":{\"480\":1}}],[\"这里没有去掉最后那个\",{\"1\":{\"480\":1}}],[\"这里是想解决反馈系统的效率问题\",{\"1\":{\"602\":1}}],[\"这里是用0填充\",{\"1\":{\"477\":1}}],[\"这里是一个长度为2的元组\",{\"1\":{\"477\":1}}],[\"这里不直接使用\",{\"1\":{\"592\":1}}],[\"这里不再贴出\",{\"1\":{\"435\":1}}],[\"这里不过多进行展开\",{\"1\":{\"192\":1}}],[\"这里需要注意一点\",{\"1\":{\"700\":1}}],[\"这里需要激活\",{\"1\":{\"587\":1}}],[\"这里需要将其分离开来\",{\"1\":{\"430\":1}}],[\"这里需要和\",{\"1\":{\"190\":1}}],[\"这里主要有两种位置编码思路\",{\"1\":{\"428\":1}}],[\"这里简单推理一下\",{\"1\":{\"612\":1}}],[\"这里简单介绍一下cls\",{\"1\":{\"427\":1}}],[\"这里简化为2维\",{\"1\":{\"119\":1}}],[\"这里对训练集的处理方式是随机切成224x224像素的图片\",{\"1\":{\"425\":1}}],[\"这里对提取的文本特征和图像特征进行对比学习\",{\"1\":{\"407\":1}}],[\"这里共有个正样本\",{\"1\":{\"407\":1}}],[\"这里以搜索向日葵花为例\",{\"1\":{\"411\":1}}],[\"这里以\",{\"1\":{\"382\":1}}],[\"这里全\",{\"1\":{\"380\":1}}],[\"这里只取第一个元素\",{\"1\":{\"293\":1}}],[\"这里只取前b个样本\",{\"1\":{\"207\":1}}],[\"这里的例子比较简单\",{\"1\":{\"694\":1}}],[\"这里的关键是在prompt中加入的示例\",{\"1\":{\"620\":1}}],[\"这里的原因\",{\"1\":{\"429\":1}}],[\"这里的相似度直接计算文本特征和图像特征的余弦相似性\",{\"1\":{\"407\":1}}],[\"这里的重点是\",{\"1\":{\"359\":1}}],[\"这里的\",{\"1\":{\"202\":1,\"213\":1,\"273\":1,\"426\":1,\"931\":1,\"951\":1}}],[\"这里的三个\",{\"1\":{\"138\":1}}],[\"这里\",{\"1\":{\"199\":1,\"385\":1,\"448\":1,\"629\":2,\"709\":1,\"801\":1,\"846\":1,\"867\":1,\"885\":1,\"906\":1}}],[\"这里默认\",{\"1\":{\"122\":1}}],[\"这里固定选择第一个点\",{\"1\":{\"121\":1}}],[\"这里应该有残差连接的shortcut处理\",{\"1\":{\"120\":1}}],[\"这里因为\",{\"1\":{\"119\":1}}],[\"这里直接等于\",{\"1\":{\"119\":1}}],[\"这里tgt就是roberta编码得到的文本特征嵌入向量\",{\"1\":{\"100\":1}}],[\"这里就是\",{\"1\":{\"83\":1,\"357\":1}}],[\"这个操作是求不了导的\",{\"1\":{\"958\":1}}],[\"这个操作被称为\",{\"1\":{\"137\":1}}],[\"这个类别的序号就是我们想要的整数\",{\"1\":{\"958\":1}}],[\"这个问题是无解的\",{\"1\":{\"956\":1}}],[\"这个问题的答案\",{\"1\":{\"694\":1}}],[\"这个离散向量构成的空间是不好采样的\",{\"1\":{\"956\":1}}],[\"这个嵌入层在vq\",{\"1\":{\"956\":1}}],[\"这个测试时的网络结构在图5中有示意\",{\"1\":{\"947\":1}}],[\"这个设计使得模型输出能\",{\"1\":{\"943\":1}}],[\"这个约束能防止函数的变化太快\",{\"1\":{\"917\":1}}],[\"这个平均距离越大\",{\"1\":{\"915\":1}}],[\"这个最大点称为参数的最大似然估计\",{\"1\":{\"903\":1}}],[\"这个对比模型会根据图像和文字的匹配程度给出一个评分\",{\"1\":{\"889\":1}}],[\"这个求和很难直接算\",{\"1\":{\"885\":1}}],[\"这个除数被称为边际似然\",{\"1\":{\"877\":1}}],[\"这个分布的一个显著特点是\",{\"1\":{\"868\":1}}],[\"这个分布可以被看作是标准正态分布\",{\"1\":{\"866\":1}}],[\"这个结论来自于\",{\"1\":{\"848\":1}}],[\"这个公式\",{\"1\":{\"945\":1}}],[\"这个公式可以由以下恒等式直接推出\",{\"1\":{\"877\":1}}],[\"这个公式适用于任意两个事件\",{\"1\":{\"848\":1}}],[\"这个公式对应的是\",{\"1\":{\"848\":1}}],[\"这个公式其实可以直接用在对比学习中\",{\"1\":{\"355\":1}}],[\"这个事件也应该存在\",{\"1\":{\"847\":1}}],[\"这个事件存在\",{\"1\":{\"847\":1}}],[\"这个事情的性价比非常低\",{\"1\":{\"601\":1}}],[\"这个规则体系就是σ\",{\"1\":{\"847\":1}}],[\"这个数据集来源是这里\",{\"1\":{\"712\":1}}],[\"这个数据集中剩余的所有图片都是负样本\",{\"1\":{\"350\":1}}],[\"这个整数就是偏置查表的索引\",{\"1\":{\"710\":1}}],[\"这个怎么去理解\",{\"1\":{\"706\":1}}],[\"这个阶段的prompt中包含三部分内容\",{\"1\":{\"622\":1}}],[\"这个阶段的prompt中要包含分解问题的示例\",{\"1\":{\"622\":1}}],[\"这个阶段相当于在语言模型的词空间中\",{\"1\":{\"341\":1}}],[\"这个词\",{\"1\":{\"600\":1,\"925\":1}}],[\"这个表达式其实是通过\",{\"1\":{\"589\":1}}],[\"这个衰减因子能够使得易分类的样本\",{\"1\":{\"589\":1}}],[\"这个矩阵的每一个元素对应于\",{\"1\":{\"963\":1}}],[\"这个矩阵是\",{\"1\":{\"574\":1}}],[\"这个矩阵就叫\",{\"1\":{\"574\":1}}],[\"这个矩阵表示的是每个\",{\"1\":{\"526\":1}}],[\"这个矩阵表示对点云所做的变换\",{\"1\":{\"152\":1}}],[\"这个新内存布局使得每一行是连续的\",{\"1\":{\"545\":1}}],[\"这个新维度就是拼接的那一维\",{\"1\":{\"466\":1}}],[\"这个算法搜索过程会引入随机性\",{\"1\":{\"521\":1}}],[\"这个张量会成为模型的一个成员\",{\"1\":{\"474\":1}}],[\"这个维度上添加一维\",{\"1\":{\"428\":1}}],[\"这个就是额外添加的一个\",{\"1\":{\"427\":1}}],[\"这个token\",{\"1\":{\"427\":1}}],[\"这个功能具体实现在其内部的\",{\"1\":{\"380\":1}}],[\"这个输出向量会通过其\",{\"1\":{\"362\":1}}],[\"这个增强方法\",{\"1\":{\"359\":1}}],[\"这个值\",{\"1\":{\"356\":1}}],[\"这个时候\",{\"1\":{\"355\":1}}],[\"这个是softmax的公式\",{\"1\":{\"355\":1}}],[\"这个动态的字典分为两个部分\",{\"1\":{\"352\":1}}],[\"这个名字就是来源于前两个单词的前两个字母\",{\"1\":{\"351\":1}}],[\"这个粒度其实是很细\",{\"1\":{\"350\":1}}],[\"这个代理任务\",{\"1\":{\"350\":1}}],[\"这个代理任务是指\",{\"1\":{\"350\":1}}],[\"这个损失鼓励\",{\"1\":{\"260\":1}}],[\"这个向量序列随后被送入\",{\"1\":{\"233\":1}}],[\"这个变换矩阵是近似正交的\",{\"1\":{\"152\":1}}],[\"这个模型使用了\",{\"1\":{\"141\":1}}],[\"这个模块实现了\",{\"1\":{\"141\":1}}],[\"这个特征向量代表了这个局部区域的高维特征\",{\"1\":{\"137\":1}}],[\"这个函数的有一个输入参数\",{\"1\":{\"745\":1}}],[\"这个函数的作用是将输入的文本转化为对应的嵌入表示\",{\"1\":{\"410\":1}}],[\"这个函数的作用是从点云中找出每个查询点周围一定半径范围内的邻近点索引\",{\"1\":{\"137\":1}}],[\"这个函数的作用是从输入点云中\",{\"1\":{\"137\":1}}],[\"这个函数在误差较小时近似于\",{\"1\":{\"259\":1}}],[\"这个函数实现的是根据给定的索引\",{\"1\":{\"137\":1}}],[\"这个函数实现的是最远点采样\",{\"1\":{\"137\":1}}],[\"这个过程可以理解为一种由语言引导的搜索\",{\"1\":{\"889\":1}}],[\"这个过程有点像\",{\"1\":{\"620\":1}}],[\"这个过程其实也是自监督训练的一个过程\",{\"1\":{\"349\":1}}],[\"这个过程类似于在传统的卷积神经网络中如何处理图像的小区域\",{\"1\":{\"131\":1}}],[\"这个过程发生在数据集准备阶段\",{\"1\":{\"52\":1}}],[\"这个\",{\"1\":{\"98\":1,\"257\":1,\"410\":1,\"951\":1}}],[\"这一工具\",{\"1\":{\"961\":1}}],[\"这一点提供了理论上的安慰\",{\"1\":{\"949\":1}}],[\"这一点与\",{\"1\":{\"273\":1}}],[\"这一操作本质上是非连续的\",{\"1\":{\"946\":1}}],[\"这一特性也允许我们指定颜色的亮度级别\",{\"1\":{\"925\":1}}],[\"这一复杂任务以前通常需要专门设计的方案才能实现\",{\"1\":{\"884\":1}}],[\"这一里程碑式的更新\",{\"1\":{\"833\":1}}],[\"这一阶段\",{\"1\":{\"812\":1}}],[\"这一扩展使我们的函数定义更接近\",{\"1\":{\"800\":1}}],[\"这一策略虽限制数据量\",{\"1\":{\"666\":1}}],[\"这一设计理念直接反映在模型架构选择上\",{\"1\":{\"666\":1}}],[\"这一方法起初应用于强化学习场景\",{\"1\":{\"655\":1}}],[\"这一研究为探索语言模型的元学习机制和实际应用奠定了基础\",{\"1\":{\"646\":1}}],[\"这一研究为功能学习领域提供了新的视角\",{\"1\":{\"72\":1}}],[\"这一发现也暗示\",{\"1\":{\"949\":1}}],[\"这一发现标志着大型语言模型\",{\"1\":{\"822\":1}}],[\"这一发现挑战了单纯追求参数规模的范式\",{\"1\":{\"666\":1}}],[\"这一发现表明\",{\"1\":{\"643\":1}}],[\"这一发现为理解当前预训练模型的有效性提供了新视角\",{\"1\":{\"642\":1}}],[\"这一发现为构建通用语言系统提供了新方向\",{\"1\":{\"639\":1}}],[\"这一框架允许模型不仅生成文本\",{\"1\":{\"640\":1}}],[\"这一块的技巧性很强\",{\"1\":{\"619\":1}}],[\"这一条件\",{\"1\":{\"500\":1}}],[\"这一部分的目标是\",{\"1\":{\"420\":1}}],[\"这一成果为开源多模态模型的发展提供了重要支持\",{\"1\":{\"322\":1}}],[\"这一渐进式策略确保模型\",{\"1\":{\"305\":1}}],[\"这一现象在其他动量方法\",{\"1\":{\"289\":1}}],[\"这一机制类似\",{\"1\":{\"285\":1}}],[\"这一节完整翻译成中文\",{\"1\":{\"271\":1}}],[\"这一过程对大规模计算框架\",{\"1\":{\"806\":1}}],[\"这一过程与训练时相同\",{\"1\":{\"408\":1}}],[\"这一过程类似于\",{\"1\":{\"237\":1}}],[\"这一过程通过对每个子区域应用集合抽象层\",{\"1\":{\"142\":1}}],[\"这一步是不可导的\",{\"1\":{\"959\":1}}],[\"这一步是为了保证图像的整体比例不变\",{\"1\":{\"425\":1}}],[\"这一步到底引入了多少误差\",{\"1\":{\"948\":1}}],[\"这一步通常是这样做的\",{\"1\":{\"710\":1}}],[\"这一步很关键\",{\"1\":{\"454\":1}}],[\"这一步的操作在论文中是直接采用切割的处理办法\",{\"1\":{\"426\":1}}],[\"这一步就是\",{\"1\":{\"213\":1}}],[\"这一步不动\",{\"1\":{\"885\":1}}],[\"这一步不计算梯度\",{\"1\":{\"208\":1}}],[\"这一步不在\",{\"1\":{\"152\":1}}],[\"这一步相当于图像任务中的\",{\"1\":{\"145\":1}}],[\"这一最具挑战性的设置下\",{\"1\":{\"47\":1}}],[\"这类度量难以设计\",{\"1\":{\"944\":1}}],[\"这类抽取式问答任务中\",{\"1\":{\"735\":1}}],[\"这类指令\",{\"1\":{\"657\":1}}],[\"这类生成方法的优点\",{\"1\":{\"268\":1}}],[\"这类对比方法的能力\",{\"1\":{\"268\":1}}],[\"这类方法试图从原始图像中恢复遮挡部分\",{\"1\":{\"247\":1}}],[\"这类似于计算机图形学中的一个实际问题\",{\"1\":{\"952\":1}}],[\"这类似于\",{\"1\":{\"231\":1}}],[\"这类\",{\"1\":{\"47\":1,\"475\":1}}],[\"这种解决问题的思路可以应用到所有图像生成类任务上\",{\"1\":{\"961\":1}}],[\"这种解释也叫作\",{\"1\":{\"950\":1}}],[\"这种技术是说\",{\"1\":{\"959\":1}}],[\"这种技术被广泛应用于扩散模型\",{\"1\":{\"894\":1}}],[\"这种现象是合理的\",{\"1\":{\"951\":1}}],[\"这种参数的存在依赖于我们对\",{\"1\":{\"951\":1}}],[\"这种重参数化只是数学上的变换\",{\"1\":{\"951\":1}}],[\"这种修改不会改变模型本质\",{\"1\":{\"951\":1}}],[\"这种视角也正是许多\",{\"1\":{\"950\":1}}],[\"这种下界估计虽然不是完全精确\",{\"1\":{\"947\":1}}],[\"这种最大似然方法保证模型倾向于生成训练样本及其相似样本\",{\"1\":{\"943\":1}}],[\"这种每次生成一个像素\",{\"1\":{\"925\":1}}],[\"这种掩码设计\",{\"1\":{\"924\":1}}],[\"这种机制可以巧妙地掩盖住每个像素右侧和下侧的信息\",{\"1\":{\"923\":1}}],[\"这种根据实验结果推断最有可能的硬币属性\",{\"1\":{\"903\":1}}],[\"这种分布之所以受欢迎\",{\"1\":{\"869\":1}}],[\"这种分离的设计让模型更灵活\",{\"1\":{\"532\":1}}],[\"这种含有两个参数的分布比泊松分布更具有建模灵活性\",{\"1\":{\"863\":1}}],[\"这种组合你也得能谈\",{\"1\":{\"847\":1}}],[\"这种能力可能是通过对代码的训练获得的\",{\"1\":{\"825\":1}}],[\"这种能力允许语言模型在提供自然语言指令或多个任务示例的情况下\",{\"1\":{\"825\":1}}],[\"这种能力对机器人学和人机交互非常重要\",{\"1\":{\"19\":1}}],[\"这种归一化方法可以避免梯度爆炸和消失的问题\",{\"1\":{\"823\":1}}],[\"这种情况下\",{\"1\":{\"806\":1,\"847\":1}}],[\"这种位置编码方式能够根据具体的任务和数据特点\",{\"1\":{\"707\":1}}],[\"这种位置编码方法也存在以下一些不足之处\",{\"1\":{\"706\":1}}],[\"这种不可解释性限制了其在高风险领域的应用\",{\"1\":{\"649\":1}}],[\"这种性能差异揭示了当前方法的边界\",{\"1\":{\"642\":1}}],[\"这种局限性部分源于单任务\",{\"1\":{\"639\":1}}],[\"这种转置后的布局在缓存访问上可能效率较低\",{\"1\":{\"545\":1}}],[\"这种差异一方面是由于文本和图像属于两个完全不同的模态\",{\"1\":{\"413\":1}}],[\"这种预训练通常是基于有监督学习的\",{\"1\":{\"413\":1}}],[\"这种格式\",{\"1\":{\"409\":1}}],[\"这种策略有助于模型学习跨模态对齐能力\",{\"1\":{\"382\":1}}],[\"这种模型最朴素的实现方法\",{\"1\":{\"921\":1}}],[\"这种模型\",{\"1\":{\"355\":1}}],[\"这种提升并不依赖增加模型参数\",{\"1\":{\"288\":1}}],[\"这种传播方式有两类\",{\"1\":{\"283\":1}}],[\"这种零样本分类在受损或分布外图像上更加稳健\",{\"1\":{\"271\":1}}],[\"这种共享计算图的方式\",{\"1\":{\"268\":1}}],[\"这种遮挡方式\",{\"1\":{\"263\":1}}],[\"这种内在学习能力可能是\",{\"1\":{\"243\":1}}],[\"这种\",{\"1\":{\"240\":1,\"647\":1,\"649\":2,\"809\":1}}],[\"这种统一架构使得\",{\"1\":{\"222\":1}}],[\"这种方式效率极低\",{\"1\":{\"944\":1}}],[\"这种方式在输出为标量的情况下计算效率更高\",{\"1\":{\"775\":1}}],[\"这种方式在训练初期让模型更多依赖于\",{\"1\":{\"204\":1}}],[\"这种方式的问题是\",{\"1\":{\"661\":1}}],[\"这种方式避免了目标检测的步骤\",{\"1\":{\"388\":1}}],[\"这种方式显著提升了模型的效果\",{\"1\":{\"377\":1}}],[\"这种方式比直接使用图像\",{\"1\":{\"374\":1}}],[\"这种方式能减轻生成和对比损失之间的干扰\",{\"1\":{\"277\":1}}],[\"这种方式需要我们手动把两个分布都以\",{\"1\":{\"261\":1}}],[\"这种方法虽然效果强大\",{\"1\":{\"650\":1}}],[\"这种方法虽然简单\",{\"1\":{\"220\":1}}],[\"这种方法无需微调\",{\"1\":{\"640\":1}}],[\"这种方法重新引入了大量特定任务的定制化输入\",{\"1\":{\"631\":1}}],[\"这种方法不需要复制数据\",{\"1\":{\"545\":1}}],[\"这种方法实际上与nlp领域的一个研究方向\",{\"1\":{\"409\":1}}],[\"这种方法能够捕获较为精细的对象信息\",{\"1\":{\"388\":1}}],[\"这种方法的优势在于\",{\"1\":{\"286\":1}}],[\"这种方法通常被称为\",{\"1\":{\"283\":1}}],[\"这种方法大大增强了网络处理非均匀采样数据的能力\",{\"1\":{\"141\":1}}],[\"这种方法使得网络能够在细节丰富的区域\",{\"1\":{\"140\":1}}],[\"这种方法使网络能够通过在训练期间随机丢弃输入点\",{\"1\":{\"140\":1}}],[\"这种伪标签不仅能补充视觉信息中的遗漏\",{\"1\":{\"202\":1}}],[\"这种非均匀性为点集特征学习带来了显著挑战\",{\"1\":{\"139\":1}}],[\"这种类别不平衡\",{\"1\":{\"106\":1}}],[\"这种设计对多任务场景尤其有益\",{\"1\":{\"273\":1}}],[\"这种设计的直观效果是模型会倾向于把全局信息推送到\",{\"1\":{\"214\":1}}],[\"这种设计使得网络能够更好地处理深层特征\",{\"1\":{\"497\":1}}],[\"这种设计使得网络只关注\",{\"1\":{\"157\":1}}],[\"这种设计使得语言信息能有效地指导点特征的学习过程\",{\"1\":{\"99\":1}}],[\"这种设计无需依赖具体的可供性分类标签\",{\"1\":{\"39\":1}}],[\"这样才能让总的信息量变得有限\",{\"1\":{\"951\":1}}],[\"这样才能确保所有路径的贡献都被纳入最终的梯度值\",{\"1\":{\"803\":1}}],[\"这样处理之后\",{\"1\":{\"951\":1}}],[\"这样采样\",{\"1\":{\"931\":1}}],[\"这样用自己之前时刻的状态预测下一个状态的模型\",{\"1\":{\"925\":1}}],[\"这样即可缩小松弛后的验证\",{\"1\":{\"886\":1}}],[\"这样分开好处是\",{\"1\":{\"885\":1}}],[\"这样它就可以恢复\",{\"1\":{\"823\":1}}],[\"这样前一半桶表示左方向\",{\"1\":{\"710\":1}}],[\"这样既让模型\",{\"1\":{\"710\":1}}],[\"这样得到的结果是一个标量\",{\"1\":{\"709\":1}}],[\"这样在后续的softmax计算中\",{\"1\":{\"703\":1}}],[\"这样在训练中最小化损失就等于最大化重叠度\",{\"1\":{\"590\":1}}],[\"这样难免会影响到最终的结果\",{\"1\":{\"694\":1}}],[\"这样强迫模型在编码当前时刻词的时候不能太依赖当前的词\",{\"1\":{\"691\":1}}],[\"这样llm输出的内容也会更加贴合我们的需求\",{\"1\":{\"618\":1}}],[\"这样越接近\",{\"1\":{\"588\":1}}],[\"这样计算量就大大减小了\",{\"1\":{\"426\":1}}],[\"这样就公平\",{\"1\":{\"915\":1}}],[\"这样就将原本不定尺寸的\",{\"1\":{\"501\":1}}],[\"这样就能让\",{\"1\":{\"429\":1,\"737\":1}}],[\"这样就可以完成vit的训练过程\",{\"1\":{\"431\":1}}],[\"这样就可以提取出对最终任务有帮助的特征组合\",{\"1\":{\"429\":1}}],[\"这样就可以直接利用交互层来处理视觉特征\",{\"1\":{\"389\":1,\"392\":1}}],[\"这样就成了一个一维序列\",{\"1\":{\"426\":1}}],[\"这样每个卷积核对应一个\",{\"1\":{\"380\":1}}],[\"这样每个点在预测标签时都能看到整个物体的上下文\",{\"1\":{\"150\":1}}],[\"这样字典是高度一致的\",{\"1\":{\"357\":1}}],[\"这样不断反复直到遇到终止符\",{\"1\":{\"660\":1}}],[\"这样不就不一致了吗\",{\"1\":{\"353\":1}}],[\"这样不仅能满足不同任务的需求\",{\"1\":{\"272\":1}}],[\"这样我们就可以将期望写为如下形式\",{\"1\":{\"946\":1}}],[\"这样我们只需采样一个\",{\"1\":{\"946\":1}}],[\"这样我们的队列就可以设置得很大\",{\"1\":{\"353\":1}}],[\"这样我们会得到两个不太一样的照片\",{\"1\":{\"350\":1}}],[\"这样做其实类似于把骨干网络当成一个特征提取器\",{\"1\":{\"352\":1}}],[\"这样做的意义在于\",{\"1\":{\"894\":1}}],[\"这样做的好处是\",{\"1\":{\"213\":1,\"691\":1}}],[\"这样做的目的是提高模型的效率和泛化能力\",{\"1\":{\"131\":1}}],[\"这样做的原因是因为交互文本由当前图片反映的交互行为和模型额外补充的当前物体存在的其他交互行为构成\",{\"1\":{\"55\":1}}],[\"这样一来我们就绕过了后验分布\",{\"1\":{\"945\":1}}],[\"这样一来\",{\"1\":{\"268\":2,\"283\":1,\"609\":1,\"885\":1}}],[\"这样一个统一的\",{\"1\":{\"223\":1}}],[\"这样标准的\",{\"1\":{\"231\":1}}],[\"这样的设定具有计算上的优势\",{\"1\":{\"946\":1}}],[\"这样的设计使得\",{\"1\":{\"202\":1}}],[\"这样的模型不一定实用\",{\"1\":{\"942\":1}}],[\"这样的集合都定义概率\",{\"1\":{\"847\":1}}],[\"这样的\",{\"1\":{\"735\":1}}],[\"这样的大模型\",{\"1\":{\"610\":1}}],[\"这样的大模型中\",{\"1\":{\"594\":1}}],[\"这样的格式来生成文本描述\",{\"1\":{\"409\":1}}],[\"这样的共享设计能够提升训练效率\",{\"1\":{\"171\":1}}],[\"这样模型就能根据上下文更准确地做出判断\",{\"1\":{\"156\":1}}],[\"这样可以看出每个头分别关注图像的哪部分区域\",{\"1\":{\"582\":1}}],[\"这样可以减少内存使用和计算开销\",{\"1\":{\"473\":1}}],[\"这样可以为\",{\"1\":{\"434\":1}}],[\"这样可以在模型的不同阶段交替利用\",{\"1\":{\"434\":1}}],[\"这样可以在每个部分上独立地学习特征\",{\"1\":{\"131\":1}}],[\"这样可以提升计算效率\",{\"1\":{\"430\":1}}],[\"这样可以提高\",{\"1\":{\"262\":1}}],[\"这样可以保证模型的特征提取能力和性能\",{\"1\":{\"425\":1}}],[\"这样可以利用\",{\"1\":{\"19\":1}}],[\"这样\",{\"1\":{\"26\":1,\"83\":1,\"140\":2,\"152\":1,\"272\":1,\"388\":1,\"426\":1,\"588\":1,\"681\":1,\"710\":1,\"809\":1,\"826\":1,\"921\":1,\"956\":4,\"960\":1,\"961\":1}}],[\"这些生成工具对设计师等用户非常有用\",{\"1\":{\"942\":1}}],[\"这些像素之间存在复杂的依赖关系\",{\"1\":{\"942\":1}}],[\"这些问题限制了模型在实际应用中的表现\",{\"1\":{\"884\":1}}],[\"这些问题导致现有方法在鲁棒性和适应性上均受限制\",{\"1\":{\"19\":1}}],[\"这些都属于\",{\"1\":{\"847\":1}}],[\"这些涌现能力让\",{\"1\":{\"825\":1}}],[\"这些特点使得\",{\"1\":{\"942\":1}}],[\"这些特点使\",{\"1\":{\"824\":1}}],[\"这些特点使它们在自然语言处理和其他领域中引起了广泛的兴趣和研究\",{\"1\":{\"824\":1}}],[\"这些特征\",{\"1\":{\"356\":1}}],[\"这些函数被称为测试函数\",{\"1\":{\"811\":1}}],[\"这些位置的概率就会接近0\",{\"1\":{\"703\":1}}],[\"这些是\",{\"1\":{\"697\":1}}],[\"这些词汇是从wikipedia的优质文章和标杆文章中提取得到\",{\"1\":{\"696\":1}}],[\"这些改进通常依赖于更大模型和更多数据\",{\"1\":{\"687\":1}}],[\"这些改进共同构成了roberta的核心优化策略\",{\"1\":{\"681\":1}}],[\"这些改进包括更长的训练时间\",{\"1\":{\"677\":1}}],[\"这些改写使得在预训练模型架构上用最小的修改就会有效\",{\"1\":{\"626\":1}}],[\"这些研究强调\",{\"1\":{\"655\":1}}],[\"这些限制提示我们\",{\"1\":{\"649\":1}}],[\"这些任务测试了gpt\",{\"1\":{\"648\":1}}],[\"这些发现为后续gpt\",{\"1\":{\"642\":1}}],[\"这些实验为后续研究\",{\"1\":{\"641\":1}}],[\"这些实验表明\",{\"1\":{\"344\":1}}],[\"这些设计使\",{\"1\":{\"640\":1}}],[\"这些设计让训练流程非常完整\",{\"1\":{\"510\":1}}],[\"这些系统往往对数据分布或任务定义的微小变化非常敏感\",{\"1\":{\"639\":1}}],[\"这些输入转换使作者避免跨任务架构的大改\",{\"1\":{\"631\":1}}],[\"这些参数会以随机梯度下降训练\",{\"1\":{\"629\":1}}],[\"这些不确定性使得开发有效的语言处理半监督学习方法变得困难\",{\"1\":{\"626\":1}}],[\"这些地址从\",{\"1\":{\"540\":1}}],[\"这些区域的大小各不相同\",{\"1\":{\"501\":1}}],[\"这些变量依然存在\",{\"1\":{\"448\":1}}],[\"这些代码块里定义的变量\",{\"1\":{\"444\":1}}],[\"这些代理任务\",{\"1\":{\"349\":1}}],[\"这些值会根据模型的损失函数不断调整\",{\"1\":{\"428\":1}}],[\"这些相似度值可以被视为logits\",{\"1\":{\"408\":1}}],[\"这些图像特征会与之前得到的个文本特征进行余弦相似度计算\",{\"1\":{\"408\":1}}],[\"这些文本随后被输入到文本编码器\",{\"1\":{\"408\":1}}],[\"这些数据集包括\",{\"1\":{\"823\":1}}],[\"这些数据在论文中被称为webimagetext\",{\"1\":{\"407\":1}}],[\"这些数据由\",{\"1\":{\"342\":1}}],[\"这些表示会被输入到\",{\"1\":{\"371\":1}}],[\"这些规则可以去定义哪些图片是相似的\",{\"1\":{\"349\":1}}],[\"这些指令可以是开放式的\",{\"1\":{\"339\":1}}],[\"这些指标共同构成了\",{\"1\":{\"106\":1}}],[\"这些工作主要依赖轻量级适配层\",{\"1\":{\"300\":1}}],[\"这些模型都是把图像的颜色看成一个连续的浮点数\",{\"1\":{\"925\":1}}],[\"这些模型都是以cnn为基础\",{\"1\":{\"740\":1}}],[\"这些模型的发布旨在促进研究社区的开放访问和研究\",{\"1\":{\"665\":1}}],[\"这些模型的优势在于大规模参数\",{\"1\":{\"325\":1}}],[\"这些模型多基于纯视觉数据\",{\"1\":{\"298\":1}}],[\"这些模型具备很强的语义理解与泛化能力\",{\"1\":{\"26\":1}}],[\"这些\",{\"1\":{\"286\":1,\"330\":1,\"691\":1}}],[\"这些分布由网络输出经过\",{\"1\":{\"285\":1}}],[\"这些信息可以直接在最后一个\",{\"1\":{\"280\":1}}],[\"这些预训练目标利用上下文信息提供了更丰富的学习信号\",{\"1\":{\"280\":1}}],[\"这些结果表明\",{\"1\":{\"268\":1}}],[\"这些点大致构成物体的骨架\",{\"1\":{\"157\":1}}],[\"这些点彼此之间的最小距离尽可能大\",{\"1\":{\"134\":1}}],[\"这些方法为\",{\"1\":{\"655\":1}}],[\"这些方法在数据需求和任务适应能力之间形成一个光谱\",{\"1\":{\"647\":1}}],[\"这些方法仍依赖监督数据\",{\"1\":{\"639\":1}}],[\"这些方法仍受限于训练语义空间\",{\"1\":{\"31\":1}}],[\"这些方法都有各自的特点\",{\"1\":{\"610\":1}}],[\"这些方法使用深层transformer进行交互作用\",{\"1\":{\"390\":1}}],[\"这些方法常用于图像生成\",{\"1\":{\"249\":1}}],[\"这些方法训练稳定\",{\"1\":{\"248\":1}}],[\"这些方法需要构造正负样本对\",{\"1\":{\"246\":1}}],[\"这些方法大多忽略了高层语义信息\",{\"1\":{\"210\":1}}],[\"这些方法会导致信息损失\",{\"1\":{\"148\":1}}],[\"这些人离我太远了\",{\"1\":{\"137\":1}}],[\"这些功能标签仅用于构造问题和定位正确功能区域\",{\"1\":{\"88\":1}}],[\"这些功能标注是人工标注的\",{\"1\":{\"86\":1}}],[\"这些损坏可能源自\",{\"1\":{\"20\":1}}],[\"主因是书籍数据量不足\",{\"1\":{\"668\":1}}],[\"主是为了解决cot这种从易到难的迁移能力不足而诞生的\",{\"1\":{\"622\":1}}],[\"主对角线\",{\"1\":{\"578\":1}}],[\"主成分分析\",{\"1\":{\"574\":1}}],[\"主进程设置后\",{\"1\":{\"520\":1}}],[\"主干部分全部冻结\",{\"1\":{\"435\":1}}],[\"主干网络\",{\"0\":{\"233\":1}}],[\"主干网络受限\",{\"1\":{\"19\":1}}],[\"主特征\",{\"1\":{\"293\":1}}],[\"主视觉特征\",{\"1\":{\"208\":1}}],[\"主网络进行\",{\"1\":{\"208\":1}}],[\"主流知识型模型对比\",{\"1\":{\"823\":1}}],[\"主流程\",{\"1\":{\"187\":1}}],[\"主流\",{\"1\":{\"167\":1}}],[\"主要偏重计算机视觉领域\",{\"1\":{\"942\":1}}],[\"主要由以下\",{\"1\":{\"832\":1}}],[\"主要特点包括\",{\"1\":{\"823\":1}}],[\"主要原因如下\",{\"1\":{\"806\":1}}],[\"主要输出项解释\",{\"1\":{\"733\":1}}],[\"主要捕捉局部位置关系\",{\"1\":{\"706\":1}}],[\"主要来源包括\",{\"1\":{\"667\":1}}],[\"主要来自美国和东南亚\",{\"1\":{\"658\":1}}],[\"主要用于测试小型数据集的语言模型训练效果\",{\"1\":{\"696\":1}}],[\"主要用于冷启动训练\",{\"1\":{\"656\":1}}],[\"主要用于解决目标检测任务中前景\",{\"1\":{\"589\":1}}],[\"主要针对电影评论来做情感分类\",{\"1\":{\"634\":1}}],[\"主要有以下两个\",{\"1\":{\"602\":1}}],[\"主要变化方向\",{\"1\":{\"574\":1}}],[\"主要包括行优先和列优先两类\",{\"1\":{\"541\":1}}],[\"主要包含encoder和decoder结构\",{\"1\":{\"422\":1}}],[\"主要包含以下组件\",{\"1\":{\"329\":1}}],[\"主要的区别就是去掉了paddding\",{\"1\":{\"430\":1}}],[\"主要区别在于norm层的顺序\",{\"1\":{\"429\":1}}],[\"主要是因为这些方法难以实现较高的性能\",{\"1\":{\"413\":1}}],[\"主要动机\",{\"1\":{\"385\":1}}],[\"主要步骤如下\",{\"1\":{\"331\":1}}],[\"主要采用固定分辨率\",{\"1\":{\"326\":1}}],[\"主要代表包括\",{\"1\":{\"325\":1}}],[\"主要体现在三个方面\",{\"1\":{\"323\":1}}],[\"主要思想\",{\"1\":{\"293\":1}}],[\"主要作用是\",{\"1\":{\"145\":1}}],[\"主要贡献\",{\"1\":{\"109\":1,\"368\":1}}],[\"主模型代码实现\",{\"0\":{\"266\":1}}],[\"主模型参数\",{\"1\":{\"205\":1}}],[\"主模型的预测为\",{\"1\":{\"202\":1}}],[\"主模型被训练去匹配动量模型的预测\",{\"1\":{\"202\":1}}],[\"主模型\",{\"0\":{\"123\":1},\"1\":{\"381\":1}}],[\"主体\",{\"1\":{\"73\":1,\"78\":1,\"80\":1,\"83\":1,\"235\":1,\"892\":1,\"893\":1}}],[\"主页\",{\"0\":{\"0\":1}}],[\"与正则化参数\",{\"0\":{\"951\":1}}],[\"与正向传播方向相反\",{\"1\":{\"775\":1}}],[\"与从\",{\"1\":{\"950\":1}}],[\"与直接从\",{\"1\":{\"947\":1}}],[\"与生成样本的距离\",{\"1\":{\"944\":1}}],[\"与先验\",{\"1\":{\"932\":1}}],[\"与位置编码\",{\"1\":{\"893\":1}}],[\"与使用\",{\"1\":{\"886\":1}}],[\"与似然函数\",{\"1\":{\"877\":1}}],[\"与原点的距离为\",{\"1\":{\"872\":1}}],[\"与原始\",{\"1\":{\"640\":1}}],[\"与原始步幅一致\",{\"1\":{\"546\":1}}],[\"与原始点\",{\"1\":{\"137\":1}}],[\"与事件空间相关联的概率规律\",{\"1\":{\"848\":1}}],[\"与用户负反馈\",{\"1\":{\"836\":1}}],[\"与合作伙伴包进行有效分离\",{\"1\":{\"833\":1}}],[\"与合成文本\",{\"1\":{\"177\":1}}],[\"与特定应用程序的数据进行交互的接口\",{\"1\":{\"832\":1}}],[\"与语言模型交互的接口\",{\"1\":{\"832\":1}}],[\"与以前的预训练语言模型\",{\"1\":{\"825\":1}}],[\"与以往在合成任务或小型模型上的研究不同\",{\"1\":{\"658\":1}}],[\"与基座模型有本质的区别\",{\"1\":{\"823\":1}}],[\"与训练流程规范\",{\"1\":{\"819\":1}}],[\"与梯度相乘的值设为lr=0\",{\"1\":{\"816\":1}}],[\"与反向传播类似\",{\"1\":{\"815\":1}}],[\"与define\",{\"1\":{\"811\":1}}],[\"与运行结果一致\",{\"1\":{\"811\":1}}],[\"与函数\",{\"1\":{\"799\":1}}],[\"与相对位置\",{\"1\":{\"709\":1}}],[\"与bert结论相反\",{\"1\":{\"686\":1}}],[\"与chinchiila\",{\"1\":{\"670\":1}}],[\"与chinchilla\",{\"1\":{\"666\":1}}],[\"与opt等模型趋势一致\",{\"1\":{\"668\":1}}],[\"与指令微调\",{\"1\":{\"668\":1}}],[\"与triviaqa\",{\"1\":{\"668\":1}}],[\"与毒性\",{\"1\":{\"666\":1}}],[\"与预训练相比\",{\"1\":{\"658\":1}}],[\"与上述方法的不同之处在于其训练数据源真实\",{\"1\":{\"655\":1}}],[\"与人类学习方式不匹配\",{\"1\":{\"646\":1}}],[\"与单向语言模型的互补性\",{\"1\":{\"642\":1}}],[\"与常规训练\",{\"1\":{\"641\":1}}],[\"与lstms相比\",{\"1\":{\"635\":1}}],[\"与llama\",{\"1\":{\"296\":1}}],[\"与llms的特征空间存在差异\",{\"1\":{\"296\":1}}],[\"与升维矩阵\",{\"1\":{\"611\":1}}],[\"与负样本\",{\"1\":{\"589\":1}}],[\"与背景的极端不平衡问题\",{\"1\":{\"589\":1}}],[\"与交叉熵损失函数相比\",{\"1\":{\"589\":1}}],[\"与交互主体和环境语义结合\",{\"1\":{\"83\":1}}],[\"与交互意图\",{\"1\":{\"30\":1}}],[\"与对象检测任务不同\",{\"1\":{\"584\":1}}],[\"与输入长度相同\",{\"1\":{\"513\":1}}],[\"与多项式逼近的对比\",{\"1\":{\"500\":1}}],[\"与多项式逼近\",{\"1\":{\"500\":1}}],[\"与多模态嵌入\",{\"1\":{\"67\":1}}],[\"与多模态大模型\",{\"1\":{\"31\":1}}],[\"与外部库交互\",{\"1\":{\"492\":1}}],[\"与连续时的步长规则不符\",{\"1\":{\"490\":1}}],[\"与所有负样本的相似度\",{\"1\":{\"475\":1}}],[\"与之类似的还有\",{\"1\":{\"432\":1}}],[\"与其训练更大的模型\",{\"1\":{\"658\":1}}],[\"与其\",{\"1\":{\"489\":1}}],[\"与其正样本的点积\",{\"1\":{\"475\":1}}],[\"与其他\",{\"1\":{\"427\":1,\"588\":1}}],[\"与其在整个数据集上计算损失\",{\"1\":{\"355\":1}}],[\"与计算机视觉\",{\"1\":{\"406\":1}}],[\"与计算效率间取得平衡\",{\"1\":{\"303\":1}}],[\"与此同时\",{\"1\":{\"405\":1,\"561\":1,\"648\":1,\"822\":1}}],[\"与此相比\",{\"1\":{\"216\":1}}],[\"与视觉\",{\"1\":{\"402\":1}}],[\"与文本\",{\"1\":{\"376\":1,\"885\":1,\"893\":1}}],[\"与词嵌入拼接输入\",{\"1\":{\"369\":1}}],[\"与传统指针网络方法形成鲜明对比\",{\"1\":{\"642\":1}}],[\"与传统知识蒸馏不同\",{\"1\":{\"285\":1}}],[\"与传统的零样本学习\",{\"1\":{\"273\":1}}],[\"与传统的\",{\"1\":{\"268\":1}}],[\"与传统的标量注意力不同\",{\"1\":{\"119\":1}}],[\"与过去依赖\",{\"1\":{\"283\":1}}],[\"与锐化\",{\"1\":{\"280\":1}}],[\"与图像\",{\"1\":{\"274\":1}}],[\"与图像编码器输出交互\",{\"1\":{\"272\":1}}],[\"与图像编码器交互\",{\"1\":{\"272\":1}}],[\"与标准的\",{\"1\":{\"272\":1}}],[\"与标准的图文编码器\",{\"1\":{\"272\":1}}],[\"与前述方法不同\",{\"1\":{\"271\":1}}],[\"与双编码器整体编码文本不同\",{\"1\":{\"271\":1}}],[\"与现有方法的对比\",{\"1\":{\"269\":1}}],[\"与一个先验分布\",{\"1\":{\"260\":1}}],[\"与遮挡机制是\",{\"1\":{\"242\":1}}],[\"与自蒸馏\",{\"1\":{\"216\":1}}],[\"与自注意力层\",{\"1\":{\"37\":1}}],[\"与主模型参数不同步\",{\"1\":{\"192\":1}}],[\"与动量队列的相似度\",{\"1\":{\"190\":1}}],[\"与解码\",{\"1\":{\"179\":1}}],[\"与k最近邻\",{\"1\":{\"135\":1}}],[\"与邻域查询\",{\"1\":{\"125\":1}}],[\"与邻域对齐\",{\"1\":{\"119\":1}}],[\"与编码器的transitiondown对应\",{\"1\":{\"122\":1}}],[\"与每个点特征拼接\",{\"1\":{\"122\":1}}],[\"与整个网络一起端到端训练\",{\"1\":{\"114\":1}}],[\"与意图特征\",{\"1\":{\"38\":1}}],[\"与\",{\"0\":{\"511\":1},\"1\":{\"19\":3,\"39\":1,\"43\":1,\"53\":1,\"57\":1,\"73\":2,\"79\":1,\"106\":1,\"123\":4,\"125\":2,\"179\":1,\"206\":1,\"214\":1,\"225\":1,\"228\":1,\"256\":1,\"260\":1,\"264\":1,\"305\":1,\"343\":1,\"376\":2,\"380\":1,\"386\":1,\"418\":2,\"419\":1,\"424\":1,\"485\":1,\"487\":1,\"491\":2,\"500\":1,\"503\":1,\"569\":1,\"587\":1,\"611\":3,\"640\":1,\"656\":1,\"657\":2,\"696\":1,\"709\":1,\"809\":1,\"814\":1,\"823\":3,\"847\":1,\"849\":2,\"885\":1,\"887\":1,\"893\":2,\"899\":1,\"915\":2,\"944\":2,\"945\":1,\"947\":1,\"951\":1,\"952\":1}}],[\"可保留局部+空间结构\",{\"1\":{\"963\":1}}],[\"可问题来了\",{\"1\":{\"925\":1}}],[\"可是\",{\"1\":{\"921\":1}}],[\"可加速\",{\"1\":{\"895\":1}}],[\"可加性\",{\"1\":{\"848\":1}}],[\"可由补集规则推出\",{\"1\":{\"848\":1}}],[\"可进一步限制事件空间只包含区间\",{\"1\":{\"847\":1}}],[\"可进一步增强性能\",{\"1\":{\"177\":1}}],[\"可数个区间并集交集\",{\"1\":{\"847\":1}}],[\"可组合性\",{\"1\":{\"833\":1}}],[\"可观察性\",{\"1\":{\"833\":1}}],[\"可解释性相对较低\",{\"1\":{\"830\":1}}],[\"可解释性\",{\"1\":{\"830\":1}}],[\"可规避捷径行为\",{\"1\":{\"823\":1}}],[\"可在\",{\"1\":{\"823\":1}}],[\"可在检索任务中执行独立编码\",{\"1\":{\"369\":1}}],[\"可逐渐接近目标位置\",{\"1\":{\"816\":1}}],[\"可直接将数学表达式转译为代码\",{\"1\":{\"811\":1}}],[\"可直接微调\",{\"1\":{\"646\":1}}],[\"可验证tinypytorch框架处理高阶微分的能力\",{\"1\":{\"811\":1}}],[\"可正常转换3为variable\",{\"1\":{\"809\":1}}],[\"可共用同一实现\",{\"1\":{\"809\":1}}],[\"可继续添加ndarray的其他属性\",{\"1\":{\"808\":1}}],[\"可变参数输入与输出列表\",{\"1\":{\"800\":1}}],[\"可将其比作存放数据的\",{\"1\":{\"755\":1}}],[\"可训练\",{\"1\":{\"709\":1}}],[\"可训练的参数化位置编码\",{\"1\":{\"114\":1}}],[\"可考虑换成\",{\"1\":{\"697\":1}}],[\"可挪到其他地方实现\",{\"1\":{\"697\":1}}],[\"可输出规范代码\",{\"1\":{\"669\":1}}],[\"可复现性\",{\"1\":{\"668\":1}}],[\"可复用\",{\"1\":{\"809\":1}}],[\"可复用共享参数实现文本编码器\",{\"1\":{\"368\":1}}],[\"可复用已有大模型权重\",{\"1\":{\"346\":1}}],[\"可显著降低对齐带来的性能损失\",{\"1\":{\"658\":1}}],[\"可\",{\"1\":{\"657\":1}}],[\"可基本恢复甚至超越\",{\"1\":{\"657\":1}}],[\"可有效缓解\",{\"1\":{\"656\":1}}],[\"可逆tokenizer等设计\",{\"1\":{\"647\":1}}],[\"可表示任意\",{\"1\":{\"640\":1}}],[\"可调性强\",{\"1\":{\"592\":1}}],[\"可调\",{\"1\":{\"589\":1}}],[\"可处理连续值掩码\",{\"1\":{\"587\":1}}],[\"可处理连续概率值\",{\"1\":{\"586\":1}}],[\"可最大限度地提高\",{\"1\":{\"572\":1}}],[\"可使用\",{\"1\":{\"550\":1,\"554\":1}}],[\"可使用成熟的\",{\"1\":{\"159\":1}}],[\"可用计数法则进行理解\",{\"1\":{\"881\":1}}],[\"可用柱状图表示\",{\"1\":{\"846\":1}}],[\"可用来显式指定训练集大小\",{\"1\":{\"513\":1}}],[\"可用于可视化对比\",{\"1\":{\"899\":1}}],[\"可用于\",{\"1\":{\"899\":1}}],[\"可用于展示tinypytorch计算图\",{\"1\":{\"815\":1}}],[\"可用于梯度检验\",{\"1\":{\"772\":1}}],[\"可用于多头自注意力的图像\",{\"1\":{\"397\":1}}],[\"可用于多头自注意力的文本\",{\"1\":{\"397\":1}}],[\"可用于多模态任务\",{\"1\":{\"346\":1}}],[\"可用于多种\",{\"1\":{\"109\":1}}],[\"可用于图像生成\",{\"1\":{\"247\":1}}],[\"可通过反向传播高效优化\",{\"1\":{\"942\":1}}],[\"可通过反证法证明\",{\"1\":{\"848\":1}}],[\"可通过contextlib模块实现with语句上下文管理\",{\"1\":{\"807\":1}}],[\"可通过禁用反向传播模式进一步节省内存\",{\"1\":{\"807\":1}}],[\"可通过局部神经元\",{\"1\":{\"500\":1}}],[\"可通过训练动态调整\",{\"1\":{\"500\":1}}],[\"可逼近连续但不可微的函数\",{\"1\":{\"500\":1}}],[\"可认为是模型参数一部分\",{\"1\":{\"417\":1}}],[\"可根据需要在单机环境加入\",{\"1\":{\"385\":1}}],[\"可覆盖默认值\",{\"1\":{\"380\":1}}],[\"可手动设置\",{\"1\":{\"380\":1}}],[\"可捕获深层交互\",{\"1\":{\"369\":1}}],[\"可灵活用作融合编码器或双编码器\",{\"1\":{\"368\":1}}],[\"可迁移到图像和视频理解任务\",{\"1\":{\"268\":1}}],[\"可微分编程\",{\"1\":{\"809\":1,\"811\":1}}],[\"可微\",{\"1\":{\"257\":1}}],[\"可学习query\",{\"1\":{\"421\":1}}],[\"可学习缩放因子\",{\"1\":{\"380\":1}}],[\"可学习位置嵌入\",{\"1\":{\"371\":1,\"428\":1}}],[\"可学习位置编码\",{\"1\":{\"233\":1}}],[\"可学习查询和交叉注意力层\",{\"1\":{\"305\":1}}],[\"可学习\",{\"1\":{\"192\":1,\"522\":2,\"900\":1}}],[\"可学习的位置编码\",{\"1\":{\"100\":1,\"707\":1}}],[\"可视为一种结构化的知识蒸馏方式\",{\"1\":{\"168\":1}}],[\"可视化生成图像\",{\"1\":{\"935\":1}}],[\"可视化结果显示复杂计算图\",{\"1\":{\"815\":1}}],[\"可视化工具封装\",{\"1\":{\"815\":1}}],[\"可视化计算图\",{\"0\":{\"815\":1}}],[\"可视化出每个头对应的注意力热力图\",{\"1\":{\"582\":1}}],[\"可视化示意\",{\"0\":{\"538\":1}}],[\"可视化\",{\"0\":{\"814\":1},\"1\":{\"384\":1,\"964\":1}}],[\"可视化使用\",{\"1\":{\"266\":1}}],[\"可视化当前物体点云\",{\"1\":{\"107\":1}}],[\"可视化功能区域预测结果\",{\"1\":{\"107\":1}}],[\"可视化支持\",{\"1\":{\"48\":1}}],[\"可视化分析\",{\"1\":{\"47\":1}}],[\"可为什么vq\",{\"1\":{\"956\":1}}],[\"可为\",{\"1\":{\"145\":1}}],[\"可为空字符串\",{\"1\":{\"898\":1}}],[\"可为空\",{\"1\":{\"143\":1}}],[\"可改进的地方\",{\"1\":{\"135\":1}}],[\"可结合高效采样策略\",{\"1\":{\"110\":1}}],[\"可能只有少数必须要计算图像概率分布的任务才会用到\",{\"1\":{\"925\":1}}],[\"可能跟wasserstein距离没啥关系\",{\"1\":{\"919\":1}}],[\"可能对应多个潜在的隐藏状态\",{\"1\":{\"878\":1}}],[\"可能取值的最新信念状态\",{\"1\":{\"877\":1}}],[\"可能取值的概率分布的公式\",{\"1\":{\"877\":1}}],[\"可能缺乏必要的推理能力\",{\"1\":{\"828\":1}}],[\"可能因无法学习长距离依赖\",{\"1\":{\"681\":1}}],[\"可能提升性能\",{\"1\":{\"679\":1}}],[\"可能限制泛化\",{\"1\":{\"669\":1}}],[\"可能遗漏歧义与分歧点\",{\"1\":{\"658\":1}}],[\"可能遗漏重要细节\",{\"1\":{\"157\":1}}],[\"可能影响输出的一致性与代表性\",{\"1\":{\"658\":1}}],[\"可能需要模型具备多偏好条件控制能力\",{\"1\":{\"658\":1}}],[\"可能需要大量神经元\",{\"1\":{\"500\":1}}],[\"可能不会质疑\",{\"1\":{\"657\":1}}],[\"可能不足以代表复杂的局部结构\",{\"1\":{\"157\":1}}],[\"可能更为合适\",{\"1\":{\"572\":1}}],[\"可能以\",{\"1\":{\"500\":1}}],[\"可能输出\",{\"1\":{\"483\":1,\"484\":1}}],[\"可能是被替换的词\",{\"1\":{\"691\":1}}],[\"可能是多任务训练中给\",{\"1\":{\"384\":1}}],[\"可能是经验设定\",{\"1\":{\"102\":1}}],[\"可能导致大模型训练更难收敛\",{\"1\":{\"707\":1}}],[\"可能导致梯度爆炸或梯度消失\",{\"1\":{\"380\":1}}],[\"可能导致样本在高密度区域内过度集中\",{\"1\":{\"134\":1}}],[\"可能会带来问题\",{\"1\":{\"949\":1}}],[\"可能会被掩码不同的单词\",{\"1\":{\"681\":1}}],[\"可能会有人这么问\",{\"1\":{\"616\":1}}],[\"可能会导致读不懂代码实现\",{\"1\":{\"379\":1}}],[\"可能会只偏好极少数几个\",{\"1\":{\"262\":1}}],[\"可能采用一个更好的代理任务会取得更好的效果\",{\"1\":{\"354\":1}}],[\"可能误导主模型\",{\"1\":{\"204\":1}}],[\"可能存在多个正样本\",{\"1\":{\"190\":1}}],[\"可能存在的问题\",{\"1\":{\"134\":1}}],[\"可能无法捕捉重要的几何细节\",{\"1\":{\"134\":1}}],[\"可能未充分利用点云稀疏性\",{\"1\":{\"110\":1}}],[\"可作为双编码器\",{\"1\":{\"375\":1}}],[\"可作为双编码器用于图文检索\",{\"1\":{\"368\":1}}],[\"可作为独立视觉编码器或与语言中间件结合\",{\"1\":{\"296\":1}}],[\"可作为\",{\"1\":{\"109\":1}}],[\"可选部分\",{\"1\":{\"809\":1}}],[\"可选地分开处理方向\",{\"1\":{\"710\":1}}],[\"可选地拼接\",{\"1\":{\"143\":1}}],[\"可选参数\",{\"1\":{\"587\":1,\"588\":1,\"589\":1}}],[\"可选输出张量\",{\"1\":{\"466\":1}}],[\"可选的\",{\"1\":{\"208\":1}}],[\"可选颜色\",{\"1\":{\"159\":1}}],[\"可选属性\",{\"1\":{\"159\":1}}],[\"可选\",{\"0\":{\"158\":1},\"1\":{\"107\":1,\"122\":1,\"137\":1,\"154\":1,\"256\":1,\"260\":1,\"380\":3,\"384\":2,\"385\":2,\"481\":2,\"485\":2,\"488\":3,\"586\":1,\"590\":1,\"592\":1,\"609\":1,\"710\":1,\"892\":3,\"895\":2,\"899\":1,\"926\":1,\"963\":1,\"964\":1}}],[\"可见这两句话就不是连续的\",{\"1\":{\"692\":1}}],[\"可见\",{\"1\":{\"73\":1,\"291\":1,\"520\":1,\"958\":1,\"961\":1}}],[\"可操作性\",{\"1\":{\"70\":1}}],[\"可操作性热图\",{\"0\":{\"70\":1},\"1\":{\"64\":1}}],[\"可抓握\",{\"1\":{\"53\":1}}],[\"可以正常地训练编码器和解码器了\",{\"1\":{\"959\":1}}],[\"可以被建模为具有某个均值\",{\"1\":{\"949\":1}}],[\"可以被看作是\",{\"1\":{\"611\":1}}],[\"可以移出期望符号\",{\"1\":{\"945\":1}}],[\"可以视作\",{\"1\":{\"898\":1}}],[\"可以\",{\"1\":{\"893\":1}}],[\"可以拆成\",{\"1\":{\"885\":1}}],[\"可以证明这样的函数确实存在\",{\"1\":{\"949\":1}}],[\"可以证明\",{\"1\":{\"859\":1,\"860\":1,\"863\":1}}],[\"可以推导出贝叶斯法则\",{\"1\":{\"851\":1}}],[\"可以充分发挥大语言模型的强大能力\",{\"1\":{\"836\":1}}],[\"可以基本实现目标的\",{\"1\":{\"836\":1}}],[\"可以追溯到具体的数据来源\",{\"1\":{\"830\":1}}],[\"可以根据特定风格或术语调整\",{\"1\":{\"830\":1}}],[\"可以根据图像描述生成合理的文字解释\",{\"1\":{\"341\":1}}],[\"可以达到数十亿甚至数千亿个参数\",{\"1\":{\"824\":1}}],[\"可以保持效果的情况下\",{\"1\":{\"823\":1}}],[\"可以显示部分思维链\",{\"1\":{\"823\":1}}],[\"可以显著提高基础模型的泛化能力\",{\"1\":{\"220\":1}}],[\"可以处理简单的计算图\",{\"1\":{\"797\":1}}],[\"可以处理任意顺序的点集\",{\"1\":{\"150\":1}}],[\"可以通过学习上下文来解决少样本任务\",{\"1\":{\"822\":1}}],[\"可以通过连续使用平方函数和指数函数实现\",{\"1\":{\"765\":1}}],[\"可以通过调整这些超参数来进一步优化损失函数的性能\",{\"1\":{\"593\":1}}],[\"可以高效查表\",{\"1\":{\"710\":1}}],[\"可以让模型\",{\"1\":{\"710\":1}}],[\"可以想象成一个小表格\",{\"1\":{\"710\":1}}],[\"可以映射到同一个桶\",{\"1\":{\"710\":1}}],[\"可以得到未归一化的联合分布\",{\"1\":{\"877\":1}}],[\"可以得到乘法法则\",{\"1\":{\"849\":1}}],[\"可以得到一个简单的结论\",{\"1\":{\"706\":1}}],[\"可以得到每个类别的预测概率\",{\"1\":{\"408\":1}}],[\"可以自发地学习执行任务\",{\"1\":{\"643\":1}}],[\"可以实现巨大的收益\",{\"1\":{\"625\":1}}],[\"可以实现按行\",{\"1\":{\"544\":1}}],[\"可以用它们来完成生成\",{\"1\":{\"956\":1}}],[\"可以用任意一种cnn架构\",{\"1\":{\"925\":1}}],[\"可以用于多种语言\",{\"1\":{\"824\":1}}],[\"可以用于编码全局信息\",{\"1\":{\"706\":1}}],[\"可以用于训练大型数据集\",{\"1\":{\"497\":1}}],[\"可以用\",{\"1\":{\"612\":1}}],[\"可以阅读这篇论文\",{\"1\":{\"607\":1}}],[\"可以在更快的时间内响应\",{\"1\":{\"823\":1}}],[\"可以在保持对齐的同时维持甚至提升性能\",{\"1\":{\"658\":1}}],[\"可以在保证模型效果的同时\",{\"1\":{\"607\":1}}],[\"可以在下游任务中获得较好的迁移效果\",{\"1\":{\"422\":1}}],[\"可以媲美全量微调的效果了\",{\"1\":{\"607\":1}}],[\"可以引导大模型有更加出色的表现\",{\"1\":{\"605\":1}}],[\"可以参考之前这篇文章\",{\"1\":{\"899\":1}}],[\"可以参考论文\",{\"1\":{\"409\":1}}],[\"可以参见lora\",{\"1\":{\"606\":1}}],[\"可以参见\",{\"1\":{\"604\":1,\"605\":1,\"607\":1}}],[\"可以多个方案一起\",{\"1\":{\"602\":1}}],[\"可以有效控制词汇表大小\",{\"1\":{\"594\":1}}],[\"可以有效避免坍塌\",{\"1\":{\"285\":1}}],[\"可以考虑使用tversky\",{\"1\":{\"593\":1}}],[\"可以不同\",{\"0\":{\"536\":1}}],[\"可以独立于设计\",{\"1\":{\"532\":1}}],[\"可以重复采样同一样本\",{\"1\":{\"518\":1}}],[\"可以重写为\",{\"1\":{\"235\":1}}],[\"可以直接和\",{\"1\":{\"709\":1}}],[\"可以直接加载预训练模型用于\",{\"1\":{\"510\":1}}],[\"可以直接通过类名调用\",{\"1\":{\"424\":1}}],[\"可以采用\",{\"1\":{\"502\":1}}],[\"可以扩展成\",{\"1\":{\"472\":1}}],[\"可以任意重新排列所有维度\",{\"1\":{\"468\":1}}],[\"可以代替多个冒号\",{\"1\":{\"463\":1}}],[\"可以看作是用理想编码方式构建给定样本\",{\"1\":{\"950\":1}}],[\"可以看出第\",{\"1\":{\"660\":1}}],[\"可以看这篇文章\",{\"1\":{\"430\":1}}],[\"可以看到当epochs增大时\",{\"1\":{\"434\":1}}],[\"可以看到8个图像\",{\"1\":{\"408\":1}}],[\"可以看到对于要预测的8个图像\",{\"1\":{\"408\":1}}],[\"可以看到\",{\"1\":{\"353\":1,\"428\":2,\"429\":1,\"432\":1,\"433\":1,\"522\":1,\"692\":1,\"924\":3}}],[\"可以变成一个\",{\"1\":{\"426\":1}}],[\"可以将梯度操作符移入期望内\",{\"1\":{\"946\":1}}],[\"可以将\",{\"1\":{\"832\":1}}],[\"可以将现在的各种大模型通过轻量微调变成各个不同领域的专业模型\",{\"1\":{\"614\":1}}],[\"可以将低秩矩阵\",{\"1\":{\"609\":1}}],[\"可以将规律总结为\",{\"1\":{\"547\":1}}],[\"可以将搜索范围限制在满足这些性质的模型子空间内\",{\"1\":{\"422\":1}}],[\"可以将其平移到坐标系的中心\",{\"1\":{\"131\":1}}],[\"可以和所有的query\",{\"1\":{\"420\":1}}],[\"可以和所有自己的tokens做attention\",{\"1\":{\"420\":1}}],[\"可以和很多代理任务结合\",{\"1\":{\"353\":1}}],[\"可以学习到如何更好地结合文本提取图片信息\",{\"1\":{\"417\":1}}],[\"可以学习更丰富的词汇表达\",{\"1\":{\"202\":1}}],[\"可以概括为以下两个主要步骤\",{\"1\":{\"408\":1}}],[\"可以mask成\",{\"1\":{\"393\":1}}],[\"可以选择跳过\",{\"1\":{\"899\":1}}],[\"可以选择\",{\"1\":{\"382\":1}}],[\"可以同时支持两种不同的应用方式\",{\"1\":{\"377\":1}}],[\"可以进行梯度传播\",{\"1\":{\"931\":1}}],[\"可以进行梯度回传进行更新\",{\"1\":{\"357\":1}}],[\"可以进一步提升下游任务性能\",{\"1\":{\"215\":1}}],[\"可以设置为\",{\"1\":{\"356\":1}}],[\"可以取得和之前最优的无监督方法相近甚至更好的结果\",{\"1\":{\"352\":1}}],[\"可以统一多种训练方法\",{\"1\":{\"276\":1}}],[\"可以模拟硬采样\",{\"1\":{\"257\":1}}],[\"可以使用训练过程中的变分下界\",{\"1\":{\"947\":1}}],[\"可以使用\",{\"1\":{\"554\":1,\"557\":1}}],[\"可以使用预训练的\",{\"1\":{\"434\":1}}],[\"可以使大规模图像预训练具备更强的可扩展性\",{\"1\":{\"252\":1}}],[\"可以使模型从输入中学习更抽象的语义信息\",{\"1\":{\"249\":1}}],[\"可以作为图像骨干网络\",{\"1\":{\"222\":1}}],[\"可以接收梯度\",{\"1\":{\"213\":1}}],[\"可以借助语义感知监督进行改进\",{\"1\":{\"210\":1}}],[\"可以先去了解一下python中的高级索引机制\",{\"1\":{\"137\":1}}],[\"可以理解为该向量中隐含了一些局部和全局的位置信息\",{\"1\":{\"706\":1}}],[\"可以理解为对多维张量的各个维度\",{\"1\":{\"545\":1}}],[\"可以理解为\",{\"1\":{\"131\":1}}],[\"可以对不同通道单独调制\",{\"1\":{\"112\":1}}],[\"可以避免传统tokenization的损失\",{\"1\":{\"641\":1}}],[\"可以避免因为输出缓存导致日志卡在某一行不输出的问题\",{\"1\":{\"107\":1}}],[\"可以避免仅凭物体几何去猜\",{\"1\":{\"83\":1}}],[\"可以是类别标签\",{\"1\":{\"936\":1}}],[\"可以是\",{\"1\":{\"106\":1,\"285\":1}}],[\"可以确保语言语义不会丢失\",{\"1\":{\"96\":1}}],[\"可以把注意力打分看作\",{\"1\":{\"709\":1}}],[\"可以把\",{\"1\":{\"83\":1}}],[\"可以利用其自注意力机制捕捉特征之间的长距离依赖关系\",{\"1\":{\"434\":1}}],[\"可以利用\",{\"1\":{\"22\":1}}],[\"可以加微信备注来意\",{\"1\":{\"2\":1}}],[\"可供性数据集\",{\"1\":{\"50\":1}}],[\"可供性数据集构建\",{\"1\":{\"19\":1}}],[\"可供性类别\",{\"1\":{\"41\":1}}],[\"可供性与交互图像直接联系起来\",{\"1\":{\"39\":1}}],[\"可供性预测质量\",{\"1\":{\"46\":1}}],[\"可供性预测\",{\"1\":{\"39\":1}}],[\"可供性意图知识特征\",{\"1\":{\"37\":1}}],[\"可供性标注\",{\"1\":{\"32\":1}}],[\"可供性解码与训练\",{\"0\":{\"24\":1}}],[\"可供性解码与预测\",{\"1\":{\"21\":1}}],[\"可供性掩码\",{\"1\":{\"22\":1}}],[\"可供性检测\",{\"1\":{\"20\":1}}],[\"可供性区域的精准定位\",{\"1\":{\"50\":1}}],[\"可供性区域\",{\"1\":{\"20\":1}}],[\"可供性\",{\"1\":{\"20\":1,\"25\":1}}],[\"可供性方法鲁棒性的标准\",{\"1\":{\"19\":1}}],[\"可供性学习泛化性差\",{\"1\":{\"22\":1}}],[\"可供性学习鲁棒性\",{\"1\":{\"20\":1}}],[\"可供性学习容易受到点云损坏影响\",{\"1\":{\"20\":1}}],[\"可供性学习\",{\"1\":{\"20\":2}}],[\"可供性学习的鲁棒性\",{\"1\":{\"20\":1}}],[\"可供性学习的泛化性\",{\"1\":{\"20\":1}}],[\"可供性学习的新方法\",{\"1\":{\"19\":1}}],[\"可供性学习的目标是根据语义线索\",{\"1\":{\"19\":1}}],[\"可供性学习方法存在几个主要问题\",{\"1\":{\"19\":1}}],[\"论文面向对生成模型感兴趣但不熟悉变分贝叶斯方法的读者\",{\"1\":{\"942\":1}}],[\"论文算法2实现\",{\"1\":{\"899\":1}}],[\"论文第5章重点讨论\",{\"1\":{\"668\":1}}],[\"论文最终强调\",{\"1\":{\"646\":1}}],[\"论文也分析了模型在自然语言推理\",{\"1\":{\"646\":1}}],[\"论文假设\",{\"1\":{\"646\":1}}],[\"论文系统评估了不同规模的gpt\",{\"1\":{\"641\":1}}],[\"论文训练了gpt\",{\"1\":{\"646\":1}}],[\"论文训练了\",{\"1\":{\"640\":1}}],[\"论文构建了一个新的数据集\",{\"1\":{\"640\":1}}],[\"论文强调了gpt\",{\"1\":{\"645\":1}}],[\"论文强调\",{\"1\":{\"639\":1}}],[\"论文的主要改进包括\",{\"1\":{\"678\":1}}],[\"论文的核心方法是基于语言建模\",{\"1\":{\"640\":1}}],[\"论文的核心假设与目标\",{\"1\":{\"639\":1}}],[\"论文的策略\",{\"1\":{\"26\":1}}],[\"论文实验结果显示\",{\"1\":{\"613\":1}}],[\"论文实现进行区分\",{\"1\":{\"190\":1}}],[\"论文开源的代码实现进行讲解\",{\"1\":{\"582\":1}}],[\"论文推荐\",{\"1\":{\"502\":1}}],[\"论文里提出了一种掩码卷积机制\",{\"1\":{\"923\":1}}],[\"论文里也做了说明\",{\"1\":{\"432\":1}}],[\"论文里没有做解释\",{\"1\":{\"429\":1}}],[\"论文作者也对其做了实验\",{\"1\":{\"428\":1}}],[\"论文还进行了一组\",{\"1\":{\"656\":1}}],[\"论文还探讨了模型泛化与记忆的关系\",{\"1\":{\"638\":1}}],[\"论文还实验了使用80个不同的prompt进行集成\",{\"1\":{\"409\":1}}],[\"论文还引入了\",{\"1\":{\"286\":1}}],[\"论文指出传统观点认为模型参数越多性能越优\",{\"1\":{\"666\":1}}],[\"论文指出这仅是通向更通用ai系统的初步探索\",{\"1\":{\"643\":1}}],[\"论文指出\",{\"1\":{\"409\":1,\"666\":1}}],[\"论文发现这个模型的效果最佳\",{\"1\":{\"407\":1}}],[\"论文采用\",{\"1\":{\"362\":1}}],[\"论文核心创新点\",{\"1\":{\"339\":1}}],[\"论文提出通过元学习\",{\"1\":{\"646\":1}}],[\"论文提出了\",{\"1\":{\"368\":1}}],[\"论文提出了一个全新的模型\",{\"1\":{\"94\":1}}],[\"论文提出internvl\",{\"1\":{\"323\":1}}],[\"论文提出\",{\"1\":{\"296\":1}}],[\"论文通过研究自监督预训练对\",{\"1\":{\"280\":1}}],[\"论文在\",{\"1\":{\"234\":1,\"354\":1}}],[\"论文简析\",{\"1\":{\"193\":1,\"267\":1,\"294\":1,\"321\":1,\"338\":1,\"347\":1,\"366\":1,\"367\":1,\"387\":1}}],[\"论文附录a中提到的训练优化策略\",{\"1\":{\"187\":1}}],[\"论文4\",{\"1\":{\"187\":1}}],[\"论文中第\",{\"1\":{\"655\":1}}],[\"论文中\",{\"1\":{\"611\":1}}],[\"论文中举的例子\",{\"1\":{\"607\":1}}],[\"论文中进行对比实验的clip模型也采用了这一配置\",{\"1\":{\"407\":1}}],[\"论文中所提到的\",{\"1\":{\"380\":1}}],[\"论文中所给的模型架构图中的decoder\",{\"1\":{\"94\":1}}],[\"论文中所给的模型架构图中的encoder\",{\"1\":{\"94\":1}}],[\"论文中还进行了多项\",{\"1\":{\"344\":1}}],[\"论文中重点测试了以下两个应用场景\",{\"1\":{\"342\":1}}],[\"论文中采用vit\",{\"1\":{\"187\":1}}],[\"论文中做了\",{\"1\":{\"157\":1}}],[\"论文中的验证\",{\"1\":{\"157\":1}}],[\"论文中也进行了大量消融实验来验证\",{\"1\":{\"99\":1}}],[\"论文代码解读与复现\",{\"1\":{\"61\":1,\"84\":1}}],[\"论文解读与代码实现\",{\"0\":{\"954\":1},\"1\":{\"954\":1}}],[\"论文解读\",{\"0\":{\"28\":1,\"71\":1,\"883\":1},\"1\":{\"28\":1,\"71\":1,\"164\":1,\"209\":1,\"219\":1,\"253\":2,\"279\":1,\"883\":1}}],[\"论文链接\",{\"1\":{\"18\":1,\"164\":1,\"193\":1,\"209\":1,\"219\":1,\"226\":1,\"267\":1,\"279\":1,\"294\":1,\"321\":1,\"338\":1,\"347\":1,\"366\":1,\"367\":1,\"378\":1,\"387\":1,\"608\":1,\"637\":1,\"644\":1,\"652\":1,\"664\":1,\"673\":1,\"676\":1,\"883\":1,\"890\":1,\"941\":1}}],[\"论文\",{\"0\":{\"18\":1,\"108\":1,\"124\":1,\"128\":1,\"129\":1,\"164\":1,\"193\":1,\"209\":1,\"219\":1,\"226\":1,\"267\":1,\"279\":1,\"347\":1,\"366\":1,\"367\":1,\"387\":1,\"624\":1,\"637\":1,\"644\":1,\"652\":1,\"676\":1,\"941\":1},\"1\":{\"18\":1,\"28\":1,\"61\":1,\"71\":1,\"84\":1,\"108\":2,\"119\":1,\"124\":2,\"128\":2,\"129\":2,\"130\":1,\"147\":1,\"414\":1,\"624\":2,\"637\":1,\"644\":1,\"652\":1,\"664\":1,\"673\":1,\"676\":1,\"941\":1}}],[\"ubuntu安装\",{\"1\":{\"815\":1}}],[\"ulmfit\",{\"1\":{\"650\":1,\"696\":1}}],[\"utf\",{\"1\":{\"595\":4,\"596\":1,\"597\":7,\"667\":1,\"696\":3,\"697\":1}}],[\"utm\",{\"1\":{\"185\":1}}],[\"utilization\",{\"1\":{\"215\":1}}],[\"utility\",{\"1\":{\"107\":4}}],[\"util\",{\"1\":{\"107\":1}}],[\"utils\",{\"1\":{\"106\":1,\"107\":1,\"265\":2,\"293\":12,\"359\":1,\"382\":1,\"383\":1,\"424\":3,\"425\":2,\"514\":2,\"518\":1,\"700\":1,\"810\":1,\"918\":2,\"926\":2,\"930\":1}}],[\"ummenhofer\",{\"1\":{\"110\":1}}],[\"umbrella\",{\"1\":{\"53\":1}}],[\"u\",{\"1\":{\"107\":4,\"123\":1,\"125\":1,\"899\":1}}],[\"upon\",{\"1\":{\"663\":1}}],[\"upwork\",{\"1\":{\"656\":1}}],[\"updating\",{\"1\":{\"361\":1,\"364\":1}}],[\"update\",{\"0\":{\"363\":1},\"1\":{\"190\":1,\"192\":1,\"206\":1,\"213\":2,\"293\":2,\"361\":1,\"362\":2,\"363\":2,\"382\":1,\"383\":8,\"386\":1,\"516\":1,\"595\":2,\"597\":1,\"663\":2}}],[\"up\",{\"1\":{\"59\":6,\"64\":1,\"66\":1,\"70\":6,\"83\":7,\"94\":11,\"100\":7,\"116\":1,\"192\":1,\"294\":1,\"382\":1,\"607\":1}}],[\"upsampled\",{\"1\":{\"122\":2}}],[\"upsample\",{\"1\":{\"59\":1,\"145\":1}}],[\"usage\",{\"1\":{\"213\":3}}],[\"usage=true\",{\"1\":{\"52\":1,\"213\":1}}],[\"using\",{\"1\":{\"52\":6,\"55\":6,\"751\":1,\"807\":2,\"810\":2}}],[\"users\",{\"1\":{\"712\":1}}],[\"user\",{\"1\":{\"461\":1,\"551\":1,\"553\":3,\"656\":1}}],[\"used\",{\"1\":{\"429\":1,\"713\":1}}],[\"use\",{\"1\":{\"11\":1,\"52\":1,\"119\":5,\"121\":1,\"213\":5,\"293\":3,\"362\":1,\"380\":4,\"383\":1,\"386\":1,\"410\":1,\"412\":1,\"417\":1,\"420\":5,\"421\":2,\"430\":1,\"663\":7,\"895\":2,\"918\":1}}],[\"unusual\",{\"1\":{\"724\":1}}],[\"unknown\",{\"1\":{\"926\":1}}],[\"unk\",{\"1\":{\"596\":1,\"597\":7,\"697\":5,\"698\":1,\"713\":2}}],[\"unter和pixel\",{\"1\":{\"390\":1}}],[\"undo\",{\"1\":{\"362\":1}}],[\"underfitting\",{\"1\":{\"641\":1}}],[\"underlying\",{\"1\":{\"635\":1}}],[\"understanding\",{\"1\":{\"164\":2,\"179\":1,\"624\":1}}],[\"under\",{\"1\":{\"14\":1,\"46\":1,\"106\":3,\"149\":1}}],[\"unbind\",{\"1\":{\"274\":1}}],[\"unbuffered\",{\"1\":{\"107\":1}}],[\"uncased\",{\"1\":{\"192\":2}}],[\"unittest\",{\"1\":{\"794\":2,\"795\":1}}],[\"uniter\",{\"1\":{\"251\":1,\"269\":1,\"403\":1}}],[\"unicode\",{\"1\":{\"640\":2,\"718\":1,\"728\":1}}],[\"uni\",{\"1\":{\"418\":2}}],[\"unique\",{\"0\":{\"480\":1},\"1\":{\"293\":2,\"480\":6,\"514\":1}}],[\"uniform\",{\"0\":{\"479\":1},\"1\":{\"235\":1,\"256\":3,\"260\":3,\"263\":3,\"479\":1,\"893\":1,\"899\":3,\"963\":1}}],[\"unified\",{\"0\":{\"366\":1},\"1\":{\"164\":2,\"179\":1,\"366\":5,\"367\":2,\"378\":1}}],[\"unilm\",{\"1\":{\"209\":1,\"219\":1,\"226\":1,\"367\":1,\"377\":1,\"378\":1}}],[\"unimodal\",{\"1\":{\"171\":1,\"172\":1,\"268\":1,\"274\":1}}],[\"union\",{\"0\":{\"588\":1},\"1\":{\"46\":1,\"106\":6,\"588\":5,\"663\":2}}],[\"unordered\",{\"1\":{\"149\":1,\"160\":1}}],[\"un\",{\"1\":{\"99\":1}}],[\"ungroup\",{\"1\":{\"99\":2}}],[\"ungrouping阶段\",{\"1\":{\"99\":1}}],[\"ungrouping\",{\"0\":{\"98\":1},\"1\":{\"95\":1,\"99\":3}}],[\"unshuffle\",{\"1\":{\"362\":3}}],[\"unsupervised\",{\"1\":{\"347\":2,\"637\":1,\"638\":1}}],[\"unsqueeze\",{\"1\":{\"94\":2,\"99\":1,\"100\":3,\"106\":2,\"107\":1,\"119\":8,\"122\":2,\"213\":1,\"266\":1,\"362\":1,\"380\":1,\"418\":4,\"582\":3,\"660\":2,\"663\":1,\"699\":1,\"703\":1,\"709\":6,\"710\":5,\"716\":1,\"721\":2,\"751\":1,\"898\":1,\"964\":1}}],[\"unseen\",{\"1\":{\"44\":2,\"47\":5,\"48\":1,\"89\":1,\"106\":1}}],[\"n=512\",{\"1\":{\"889\":1}}],[\"n=序列长度\",{\"1\":{\"380\":1}}],[\"nrow=8\",{\"1\":{\"926\":1}}],[\"nrow=5\",{\"1\":{\"918\":1}}],[\"nr\",{\"1\":{\"880\":1,\"881\":1}}],[\"n3\",{\"1\":{\"880\":1}}],[\"nbatches\",{\"1\":{\"751\":3}}],[\"n个解码器层\",{\"1\":{\"750\":1}}],[\"nfinal\",{\"1\":{\"663\":1}}],[\"ninput\",{\"1\":{\"660\":1}}],[\"ngo\",{\"1\":{\"655\":1,\"658\":1}}],[\"n维\",{\"1\":{\"606\":2}}],[\"ntoken\",{\"1\":{\"595\":2,\"597\":2}}],[\"ntransposed\",{\"1\":{\"545\":1}}],[\"nh\",{\"1\":{\"582\":6}}],[\"n阶张量的排列规律如下图所示\",{\"1\":{\"547\":1}}],[\"n为序列长度\",{\"1\":{\"430\":1}}],[\"nc\",{\"1\":{\"362\":3,\"475\":3}}],[\"ncrops\",{\"1\":{\"293\":5}}],[\"nce\",{\"1\":{\"282\":1,\"355\":5}}],[\"nvidia\",{\"1\":{\"236\":1,\"680\":1}}],[\"ndim\",{\"1\":{\"808\":4}}],[\"ndarray或variable\",{\"1\":{\"809\":1}}],[\"ndarray混合运算\",{\"1\":{\"809\":1}}],[\"ndarray\",{\"1\":{\"758\":1,\"792\":1,\"805\":1,\"808\":1,\"809\":1}}],[\"nd\",{\"1\":{\"213\":1}}],[\"n+1\",{\"1\":{\"205\":1,\"206\":3,\"266\":3,\"403\":2}}],[\"null\",{\"1\":{\"713\":1,\"893\":5,\"894\":5}}],[\"nucleus\",{\"1\":{\"178\":2,\"188\":2,\"421\":3}}],[\"numerically\",{\"1\":{\"892\":1}}],[\"numerical\",{\"1\":{\"771\":1,\"795\":2}}],[\"numpy\",{\"0\":{\"439\":1},\"1\":{\"107\":4,\"152\":1,\"408\":2,\"410\":4,\"412\":3,\"425\":2,\"440\":2,\"441\":1,\"480\":1,\"492\":2,\"513\":1,\"514\":1,\"521\":3,\"542\":3,\"550\":1,\"582\":2,\"757\":1,\"800\":1,\"815\":2,\"816\":2,\"918\":3}}],[\"nums\",{\"1\":{\"106\":2}}],[\"num个点云样本\",{\"1\":{\"53\":1}}],[\"num参数\",{\"1\":{\"53\":1}}],[\"number\",{\"1\":{\"53\":6,\"82\":6,\"83\":1,\"137\":2,\"293\":8,\"361\":1,\"424\":1,\"751\":1,\"899\":1,\"918\":3}}],[\"num\",{\"1\":{\"53\":4,\"64\":1,\"65\":6,\"82\":2,\"83\":15,\"97\":3,\"104\":7,\"105\":1,\"106\":11,\"107\":4,\"121\":3,\"138\":3,\"141\":2,\"143\":1,\"146\":8,\"187\":1,\"188\":5,\"190\":1,\"192\":1,\"205\":2,\"213\":24,\"255\":11,\"256\":5,\"260\":4,\"263\":22,\"264\":4,\"265\":1,\"266\":4,\"293\":2,\"359\":1,\"361\":3,\"380\":22,\"382\":16,\"385\":2,\"398\":2,\"410\":2,\"412\":2,\"420\":1,\"421\":6,\"424\":5,\"426\":5,\"427\":9,\"428\":11,\"429\":2,\"430\":24,\"431\":18,\"435\":4,\"440\":1,\"518\":4,\"582\":2,\"710\":25,\"712\":1,\"719\":1,\"722\":5,\"724\":5,\"734\":3,\"736\":6,\"737\":9,\"795\":2,\"892\":25,\"893\":3,\"895\":9,\"896\":2,\"898\":1,\"899\":23,\"900\":6,\"926\":2,\"938\":1,\"939\":1,\"963\":12,\"964\":7}}],[\"num=12\",{\"1\":{\"52\":1}}],[\"nz\",{\"1\":{\"138\":1}}],[\"ny\",{\"1\":{\"138\":1}}],[\"nxk\",{\"1\":{\"362\":1}}],[\"nx1\",{\"1\":{\"362\":1}}],[\"nxc\",{\"1\":{\"362\":2}}],[\"nx\",{\"1\":{\"138\":1,\"362\":1}}],[\"n2\",{\"1\":{\"122\":1,\"880\":2,\"881\":1}}],[\"n1\",{\"1\":{\"122\":1,\"880\":2,\"881\":1}}],[\"nli和adversarial推理任务仍具挑战性\",{\"1\":{\"648\":1}}],[\"nli\",{\"1\":{\"634\":1,\"646\":1,\"694\":1}}],[\"nltk\",{\"1\":{\"597\":1}}],[\"nlvr2datamodule\",{\"1\":{\"382\":1}}],[\"nlvr2\",{\"1\":{\"220\":1,\"268\":1,\"368\":1,\"382\":1,\"383\":2}}],[\"nlp模型的第一层一般都是词嵌入层\",{\"1\":{\"956\":1}}],[\"nlp半监督学习\",{\"1\":{\"627\":1}}],[\"nlp\",{\"1\":{\"110\":1,\"112\":1,\"114\":1,\"228\":1,\"240\":1,\"250\":1,\"280\":1,\"339\":1,\"354\":1,\"407\":1,\"639\":1,\"646\":1,\"650\":1,\"655\":2,\"656\":1,\"657\":2,\"658\":1,\"690\":3,\"696\":3,\"823\":1,\"921\":1}}],[\"nl=token数\",{\"1\":{\"64\":1}}],[\"nl\",{\"1\":{\"64\":4}}],[\"nsp任务\",{\"1\":{\"713\":1}}],[\"nsp任务非必要\",{\"1\":{\"686\":1}}],[\"nsp\",{\"1\":{\"677\":1,\"678\":2,\"679\":4,\"681\":1,\"699\":6,\"700\":2,\"701\":1,\"713\":1}}],[\"nsample=none\",{\"1\":{\"138\":1}}],[\"nsample=nsample\",{\"1\":{\"123\":12}}],[\"nsample=64\",{\"1\":{\"138\":1}}],[\"nsample=32\",{\"1\":{\"138\":1,\"146\":4}}],[\"nsample=16\",{\"1\":{\"119\":1,\"120\":1,\"121\":1,\"123\":2}}],[\"nsample\",{\"1\":{\"119\":47,\"120\":2,\"121\":10,\"123\":2,\"137\":23,\"141\":4,\"488\":1}}],[\"ns\",{\"1\":{\"64\":5,\"150\":1}}],[\"nphysically\",{\"1\":{\"545\":1}}],[\"npoint=64\",{\"1\":{\"146\":1}}],[\"npoint=256\",{\"1\":{\"146\":1}}],[\"npoint=16\",{\"1\":{\"146\":1}}],[\"npoint=1024\",{\"1\":{\"146\":1}}],[\"npoint=128\",{\"1\":{\"138\":1}}],[\"npoint=none\",{\"1\":{\"138\":1}}],[\"npoint=512\",{\"1\":{\"138\":1}}],[\"npoint\",{\"1\":{\"70\":2,\"83\":3,\"94\":2,\"137\":35,\"141\":4}}],[\"np\",{\"0\":{\"440\":1,\"441\":1},\"1\":{\"64\":1,\"92\":5,\"106\":15,\"107\":15,\"152\":2,\"263\":1,\"293\":3,\"407\":5,\"408\":1,\"410\":6,\"411\":1,\"412\":8,\"440\":3,\"441\":9,\"480\":1,\"513\":5,\"514\":3,\"521\":1,\"542\":2,\"757\":3,\"766\":2,\"781\":1,\"791\":1,\"792\":1,\"794\":4,\"795\":2,\"801\":1,\"803\":2,\"804\":1,\"805\":2,\"807\":6,\"808\":8,\"809\":6,\"811\":6,\"815\":6,\"816\":11,\"898\":1,\"918\":6}}],[\"n\",{\"1\":{\"54\":1,\"56\":2,\"59\":1,\"60\":2,\"64\":1,\"65\":10,\"67\":6,\"69\":29,\"70\":21,\"83\":69,\"88\":2,\"94\":4,\"96\":1,\"98\":1,\"99\":1,\"100\":12,\"102\":2,\"104\":8,\"106\":3,\"107\":14,\"119\":26,\"120\":7,\"121\":19,\"122\":14,\"123\":8,\"137\":30,\"141\":4,\"143\":4,\"145\":29,\"146\":7,\"152\":1,\"154\":5,\"156\":9,\"157\":3,\"190\":5,\"192\":5,\"206\":1,\"207\":4,\"213\":26,\"234\":1,\"256\":4,\"260\":2,\"266\":7,\"274\":4,\"293\":6,\"380\":9,\"407\":7,\"413\":1,\"430\":3,\"471\":1,\"472\":1,\"475\":2,\"483\":3,\"488\":1,\"500\":1,\"503\":2,\"516\":2,\"529\":1,\"538\":4,\"545\":4,\"550\":1,\"554\":1,\"582\":3,\"586\":4,\"587\":4,\"588\":4,\"589\":4,\"590\":2,\"592\":1,\"610\":1,\"640\":1,\"671\":1,\"698\":3,\"699\":3,\"700\":1,\"703\":2,\"709\":7,\"710\":26,\"712\":1,\"739\":1,\"747\":3,\"750\":3,\"808\":2,\"815\":4,\"856\":1,\"881\":6,\"882\":1,\"889\":1,\"892\":2,\"893\":4,\"899\":11,\"900\":3,\"918\":4,\"931\":1,\"935\":2}}],[\"nn结果与moco\",{\"1\":{\"289\":1}}],[\"nn分类性能\",{\"1\":{\"288\":1}}],[\"nn\",{\"0\":{\"477\":1},\"1\":{\"54\":1,\"56\":1,\"57\":1,\"58\":11,\"59\":17,\"60\":15,\"65\":8,\"66\":4,\"68\":4,\"69\":12,\"70\":8,\"83\":31,\"94\":1,\"96\":1,\"97\":15,\"98\":1,\"99\":1,\"100\":2,\"102\":4,\"104\":1,\"119\":17,\"120\":11,\"121\":6,\"122\":16,\"123\":10,\"137\":5,\"138\":8,\"141\":15,\"143\":6,\"145\":5,\"146\":5,\"152\":13,\"154\":7,\"155\":8,\"156\":8,\"187\":1,\"190\":9,\"191\":4,\"192\":9,\"205\":10,\"207\":1,\"213\":14,\"255\":18,\"265\":3,\"266\":4,\"274\":3,\"280\":3,\"286\":1,\"293\":4,\"359\":1,\"361\":2,\"362\":2,\"380\":20,\"397\":3,\"398\":2,\"399\":1,\"400\":5,\"401\":2,\"403\":5,\"419\":1,\"420\":4,\"426\":5,\"427\":4,\"428\":6,\"429\":6,\"430\":5,\"431\":14,\"474\":1,\"493\":4,\"503\":1,\"522\":7,\"582\":3,\"586\":1,\"587\":1,\"588\":1,\"589\":1,\"590\":1,\"592\":1,\"663\":1,\"692\":1,\"699\":5,\"700\":3,\"703\":2,\"709\":8,\"710\":7,\"716\":5,\"718\":6,\"719\":2,\"720\":3,\"722\":2,\"724\":6,\"725\":3,\"726\":1,\"728\":2,\"729\":3,\"730\":2,\"733\":1,\"734\":1,\"736\":2,\"737\":2,\"742\":1,\"743\":2,\"745\":2,\"746\":1,\"747\":1,\"749\":1,\"750\":1,\"751\":3,\"892\":6,\"899\":14,\"900\":8,\"918\":18,\"926\":25,\"930\":3,\"931\":6,\"937\":6,\"963\":9,\"964\":7}}],[\"noun\",{\"1\":{\"341\":1}}],[\"nocaps\",{\"1\":{\"268\":1}}],[\"non\",{\"1\":{\"204\":1,\"698\":7,\"848\":1}}],[\"none\",{\"1\":{\"64\":1,\"67\":1,\"99\":1,\"100\":6,\"107\":2,\"119\":5,\"121\":1,\"122\":3,\"123\":1,\"137\":3,\"138\":1,\"141\":6,\"145\":2,\"146\":1,\"153\":2,\"154\":1,\"187\":3,\"190\":3,\"192\":1,\"205\":3,\"207\":1,\"208\":8,\"213\":5,\"256\":4,\"260\":2,\"263\":1,\"266\":3,\"359\":3,\"361\":1,\"363\":1,\"364\":1,\"380\":17,\"382\":4,\"385\":9,\"397\":2,\"401\":2,\"403\":3,\"410\":3,\"411\":2,\"412\":4,\"419\":4,\"420\":10,\"424\":2,\"426\":1,\"435\":1,\"454\":1,\"463\":1,\"474\":2,\"480\":1,\"503\":2,\"513\":2,\"514\":1,\"582\":1,\"595\":2,\"596\":2,\"597\":4,\"660\":1,\"663\":14,\"697\":3,\"710\":4,\"713\":4,\"716\":2,\"722\":2,\"724\":1,\"731\":2,\"734\":2,\"736\":2,\"737\":7,\"751\":4,\"778\":1,\"784\":1,\"787\":1,\"791\":1,\"792\":1,\"801\":2,\"802\":1,\"803\":3,\"805\":6,\"806\":1,\"807\":6,\"808\":5,\"815\":4,\"892\":3,\"894\":2,\"895\":6,\"898\":3,\"899\":3,\"900\":1}}],[\"no\",{\"0\":{\"473\":1},\"1\":{\"190\":2,\"192\":2,\"206\":1,\"207\":1,\"208\":1,\"213\":2,\"265\":1,\"280\":1,\"285\":1,\"293\":2,\"362\":2,\"363\":1,\"364\":1,\"380\":2,\"381\":3,\"382\":10,\"386\":2,\"408\":2,\"410\":2,\"411\":1,\"412\":3,\"419\":1,\"657\":1,\"660\":1,\"663\":1,\"807\":3,\"810\":2,\"895\":1,\"898\":1,\"899\":1,\"926\":6,\"935\":2,\"939\":1,\"964\":3}}],[\"nohup\",{\"1\":{\"107\":2}}],[\"notebook中直接显示图像\",{\"1\":{\"815\":1}}],[\"notes\",{\"1\":{\"70\":1}}],[\"notimplementederror\",{\"1\":{\"92\":1,\"213\":2,\"264\":1,\"382\":2,\"762\":1,\"779\":1,\"800\":2}}],[\"not\",{\"1\":{\"92\":2,\"99\":1,\"104\":1,\"137\":3,\"141\":2,\"145\":1,\"188\":1,\"207\":1,\"208\":6,\"213\":4,\"256\":3,\"263\":1,\"265\":1,\"266\":2,\"274\":3,\"293\":1,\"361\":1,\"380\":7,\"382\":2,\"397\":1,\"401\":2,\"403\":1,\"410\":3,\"411\":2,\"412\":5,\"419\":3,\"420\":6,\"421\":1,\"424\":2,\"435\":2,\"461\":1,\"492\":1,\"582\":1,\"596\":1,\"597\":2,\"663\":4,\"696\":2,\"697\":1,\"698\":1,\"713\":1,\"722\":1,\"724\":1,\"731\":2,\"734\":2,\"736\":2,\"737\":4,\"751\":3,\"784\":1,\"787\":1,\"792\":3,\"800\":1,\"801\":2,\"803\":2,\"805\":7,\"807\":5,\"808\":3,\"815\":5,\"892\":2,\"893\":2,\"899\":3,\"900\":1,\"963\":1,\"964\":1,\"967\":1}}],[\"norm=1\",{\"1\":{\"700\":1}}],[\"norm层之后同样是多头注意力层\",{\"1\":{\"429\":1}}],[\"normemavectorquantizer\",{\"1\":{\"213\":2}}],[\"normalization和relu激活函数\",{\"1\":{\"497\":1}}],[\"normalization\",{\"0\":{\"507\":1,\"508\":1},\"1\":{\"285\":1,\"380\":1,\"429\":1,\"667\":1,\"741\":1,\"823\":1,\"848\":1,\"899\":3}}],[\"normalize=false\",{\"1\":{\"918\":1}}],[\"normalize=true\",{\"1\":{\"918\":2}}],[\"normalized\",{\"1\":{\"213\":6}}],[\"normalize\",{\"1\":{\"53\":4,\"82\":2,\"92\":1,\"107\":2,\"190\":6,\"191\":2,\"192\":6,\"205\":2,\"206\":4,\"264\":3,\"293\":5,\"359\":3,\"361\":1,\"362\":2,\"407\":2,\"417\":2,\"425\":2,\"582\":1,\"724\":1,\"900\":1,\"918\":2}}],[\"normal\",{\"0\":{\"866\":1,\"870\":1},\"1\":{\"138\":6,\"141\":5,\"159\":1,\"380\":2,\"427\":1,\"428\":2,\"431\":2,\"865\":1,\"866\":1,\"869\":1,\"918\":1,\"931\":1}}],[\"norm3\",{\"1\":{\"100\":1}}],[\"norm\",{\"1\":{\"96\":3,\"98\":3,\"122\":2,\"137\":3,\"138\":3,\"141\":3,\"145\":2,\"153\":1,\"205\":2,\"213\":7,\"265\":1,\"266\":1,\"274\":2,\"286\":1,\"293\":3,\"380\":11,\"384\":1,\"385\":8,\"400\":1,\"403\":1,\"408\":2,\"410\":2,\"412\":2,\"426\":6,\"428\":1,\"429\":3,\"431\":7,\"522\":1,\"700\":1,\"716\":1,\"718\":1,\"725\":1,\"728\":1,\"745\":2,\"747\":2,\"750\":2,\"892\":5,\"893\":1,\"895\":1,\"898\":1,\"899\":1}}],[\"norm2\",{\"1\":{\"60\":1,\"97\":2,\"98\":1,\"100\":1,\"380\":8,\"429\":2}}],[\"norm1\",{\"1\":{\"60\":1,\"97\":2,\"100\":1,\"380\":2,\"429\":2}}],[\"noise\",{\"1\":{\"14\":1,\"355\":2,\"897\":3,\"918\":1}}],[\"noisy\",{\"1\":{\"14\":1,\"149\":1}}],[\"nat\",{\"1\":{\"906\":1,\"909\":1,\"950\":1}}],[\"naturalquestions\",{\"1\":{\"668\":1}}],[\"natural\",{\"1\":{\"383\":1,\"409\":1,\"607\":1,\"641\":1,\"648\":1,\"655\":1}}],[\"nagative\",{\"1\":{\"635\":1}}],[\"nargs=\",{\"1\":{\"293\":2}}],[\"narrow\",{\"1\":{\"52\":2,\"490\":1}}],[\"nangia\",{\"1\":{\"655\":1}}],[\"nanmean\",{\"1\":{\"106\":2}}],[\"nan\",{\"1\":{\"106\":3,\"265\":1,\"592\":1}}],[\"na\",{\"1\":{\"64\":1}}],[\"names=names\",{\"1\":{\"382\":1}}],[\"names\",{\"1\":{\"380\":1,\"382\":3,\"383\":1}}],[\"name=none\",{\"1\":{\"808\":1}}],[\"name=\",{\"1\":{\"107\":2,\"382\":1,\"712\":1,\"808\":2}}],[\"named\",{\"1\":{\"104\":2,\"435\":1}}],[\"name\",{\"1\":{\"53\":4,\"82\":2,\"92\":2,\"107\":1,\"382\":3,\"410\":8,\"412\":8,\"435\":4,\"449\":2,\"453\":2,\"454\":8,\"455\":1,\"459\":2,\"461\":2,\"474\":3,\"550\":5,\"660\":1,\"663\":1,\"696\":1,\"697\":2,\"712\":1,\"807\":4,\"808\":2,\"815\":14}}],[\"neighbor\",{\"1\":{\"963\":1}}],[\"neighborhood\",{\"1\":{\"422\":1}}],[\"neox\",{\"1\":{\"671\":1}}],[\"neox方案\",{\"1\":{\"667\":1}}],[\"neural\",{\"1\":{\"647\":1,\"740\":1,\"741\":1,\"822\":1,\"921\":1}}],[\"ner\",{\"1\":{\"627\":1,\"736\":1}}],[\"nearest\",{\"0\":{\"504\":1},\"1\":{\"503\":2,\"582\":2,\"963\":1}}],[\"needed\",{\"1\":{\"410\":2,\"412\":2}}],[\"need\",{\"0\":{\"660\":1},\"1\":{\"380\":3}}],[\"nepochs\",{\"1\":{\"293\":2}}],[\"negativity\",{\"1\":{\"848\":1}}],[\"negatives\",{\"1\":{\"418\":2,\"590\":1}}],[\"negative\",{\"0\":{\"859\":1},\"1\":{\"102\":6,\"190\":8,\"192\":2,\"201\":1,\"207\":2,\"355\":1,\"361\":1,\"362\":1,\"373\":4,\"376\":2,\"383\":1,\"386\":2,\"590\":2,\"860\":1}}],[\"neg\",{\"1\":{\"190\":22,\"192\":22,\"207\":22,\"362\":2,\"386\":31,\"419\":20,\"809\":5,\"893\":2,\"898\":2}}],[\"next取出索引0或者1下标对应的值即可知道我们是否预测正确\",{\"1\":{\"700\":1}}],[\"next=false\",{\"1\":{\"698\":1}}],[\"next=true\",{\"1\":{\"698\":1}}],[\"next\",{\"0\":{\"692\":1},\"1\":{\"121\":3,\"187\":1,\"403\":1,\"420\":1,\"663\":6,\"683\":1,\"690\":1,\"698\":2,\"699\":2,\"700\":2,\"713\":1,\"731\":5,\"926\":1,\"935\":1}}],[\"net\",{\"1\":{\"123\":1,\"125\":1,\"143\":1,\"148\":1,\"150\":2,\"152\":3,\"157\":2,\"255\":2,\"964\":2}}],[\"net类架构中逐步恢复空间分辨率\",{\"1\":{\"122\":1}}],[\"networks等方法异曲同工\",{\"1\":{\"650\":1}}],[\"networks\",{\"1\":{\"429\":1,\"633\":1,\"650\":1,\"921\":1}}],[\"network\",{\"1\":{\"72\":1,\"152\":1,\"380\":1,\"640\":1,\"741\":1}}],[\"netpoll\",{\"1\":{\"2\":1}}],[\"news数据集已开源以促进后续研究\",{\"1\":{\"688\":1}}],[\"news\",{\"1\":{\"224\":1,\"678\":1,\"679\":1,\"680\":1,\"684\":1,\"686\":1}}],[\"new\",{\"0\":{\"486\":1},\"1\":{\"14\":1,\"119\":38,\"121\":9,\"122\":14,\"137\":46,\"141\":16,\"145\":9,\"213\":10,\"401\":2,\"420\":2,\"469\":1,\"470\":1,\"486\":2,\"660\":18,\"663\":2,\"724\":4}}],[\"r单独进行重排\",{\"1\":{\"881\":1}}],[\"r的相对顺序不变\",{\"1\":{\"881\":1}}],[\"rv\",{\"1\":{\"846\":1}}],[\"r1看scaling\",{\"1\":{\"837\":1}}],[\"r1\",{\"1\":{\"823\":5}}],[\"rtruediv\",{\"1\":{\"809\":1}}],[\"rte\",{\"1\":{\"685\":1}}],[\"rte数据集比较小\",{\"1\":{\"634\":1}}],[\"rdiv\",{\"1\":{\"809\":2}}],[\"rsub\",{\"1\":{\"809\":8}}],[\"rl\",{\"1\":{\"657\":1,\"822\":1,\"823\":1}}],[\"rlhf\",{\"1\":{\"339\":2,\"654\":1,\"655\":4,\"657\":1,\"658\":5}}],[\"rmsnorm\",{\"1\":{\"823\":1}}],[\"rmul\",{\"1\":{\"809\":6,\"810\":1}}],[\"rm\",{\"1\":{\"654\":1,\"656\":7}}],[\"r=1\",{\"1\":{\"613\":1}}],[\"r就是上述假设中的低维\",{\"1\":{\"606\":1}}],[\"r维\",{\"1\":{\"606\":1}}],[\"rwightman\",{\"1\":{\"435\":1}}],[\"rn50x16和rnx64\",{\"1\":{\"407\":1}}],[\"rn50x4\",{\"1\":{\"407\":1}}],[\"rnn等模型的缺点是需要顺序计算\",{\"1\":{\"740\":1}}],[\"rnn\",{\"1\":{\"150\":1,\"467\":1,\"646\":1,\"671\":1}}],[\"rcnn\",{\"1\":{\"389\":1}}],[\"rudinger\",{\"1\":{\"655\":1}}],[\"rule\",{\"0\":{\"877\":1},\"1\":{\"540\":1,\"848\":2,\"849\":1,\"851\":1,\"877\":2}}],[\"ruppert平均\",{\"1\":{\"289\":1}}],[\"runtime等加速库中可以更好地利用缓存从而提升性能\",{\"1\":{\"545\":1}}],[\"runtimeerror\",{\"1\":{\"492\":1}}],[\"runs\",{\"1\":{\"107\":1}}],[\"run\",{\"1\":{\"53\":7,\"82\":8,\"107\":1,\"558\":1,\"712\":1,\"811\":4,\"815\":1}}],[\"rp\",{\"1\":{\"710\":5}}],[\"rpr\",{\"1\":{\"710\":1}}],[\"rpb\",{\"0\":{\"710\":1},\"1\":{\"710\":2}}],[\"rpe\",{\"0\":{\"708\":1}}],[\"rpo\",{\"0\":{\"100\":1}}],[\"rpd\",{\"1\":{\"94\":1,\"100\":2}}],[\"r^\",{\"1\":{\"96\":2,\"97\":1}}],[\"rb\",{\"1\":{\"92\":2,\"107\":2}}],[\"rb×cp×np\",{\"1\":{\"64\":1}}],[\"rb×ci×h×w\",{\"1\":{\"64\":1}}],[\"rag\",{\"0\":{\"828\":1,\"830\":1},\"1\":{\"828\":8,\"829\":1,\"830\":4,\"831\":1,\"837\":1}}],[\"race阅读理解\",{\"1\":{\"685\":1}}],[\"race和squad\",{\"1\":{\"677\":1}}],[\"race数据集由初高中考试题构成\",{\"1\":{\"634\":1}}],[\"race\",{\"1\":{\"625\":1,\"626\":1,\"648\":1,\"678\":1,\"680\":1}}],[\"radd\",{\"1\":{\"809\":10,\"810\":1}}],[\"radford\",{\"1\":{\"640\":1,\"671\":1}}],[\"radam\",{\"1\":{\"510\":1}}],[\"radius=none\",{\"1\":{\"138\":1}}],[\"radius=0\",{\"1\":{\"138\":2,\"146\":4}}],[\"radius^2\",{\"1\":{\"137\":1}}],[\"radius\",{\"1\":{\"137\":12,\"141\":7}}],[\"raffel\",{\"1\":{\"671\":1}}],[\"raf\",{\"1\":{\"393\":2}}],[\"ram\",{\"1\":{\"540\":1}}],[\"ramesh\",{\"1\":{\"216\":1}}],[\"ramp\",{\"1\":{\"192\":1}}],[\"ratio=dpr\",{\"1\":{\"431\":1}}],[\"ratio=drop\",{\"1\":{\"429\":1,\"431\":1}}],[\"ratio=attn\",{\"1\":{\"429\":1,\"431\":1}}],[\"ratio=0\",{\"1\":{\"428\":3,\"429\":3,\"430\":2,\"431\":3}}],[\"ratio=mlp\",{\"1\":{\"380\":1,\"431\":1}}],[\"ratio=4\",{\"1\":{\"83\":2,\"205\":2,\"380\":2,\"428\":1,\"429\":1,\"431\":1}}],[\"ratio\",{\"1\":{\"263\":8,\"380\":2,\"428\":1,\"429\":7,\"430\":2,\"431\":4,\"592\":4}}],[\"rate的缩写\",{\"1\":{\"816\":1}}],[\"rate=2e\",{\"1\":{\"712\":1}}],[\"rate=0\",{\"1\":{\"380\":3}}],[\"rate=args\",{\"1\":{\"293\":1}}],[\"rate\",{\"1\":{\"104\":1,\"293\":1,\"380\":8,\"424\":2,\"918\":1}}],[\"raise\",{\"1\":{\"92\":1,\"213\":2,\"264\":1,\"382\":2,\"424\":1,\"461\":1,\"697\":1,\"762\":1,\"779\":1,\"792\":1,\"800\":2,\"805\":1,\"808\":1}}],[\"raw=dict\",{\"1\":{\"104\":1,\"107\":1}}],[\"raw\",{\"1\":{\"70\":10,\"83\":7,\"104\":1,\"107\":1,\"384\":2,\"385\":2,\"586\":1,\"587\":1,\"724\":1,\"893\":2}}],[\"rand\",{\"1\":{\"697\":2,\"795\":1}}],[\"randaugment\",{\"1\":{\"510\":1}}],[\"randperm\",{\"0\":{\"483\":1},\"1\":{\"483\":2}}],[\"randn\",{\"1\":{\"190\":2,\"192\":2,\"205\":2,\"213\":1,\"361\":1,\"463\":1,\"469\":1,\"472\":1,\"478\":3,\"486\":1,\"490\":1,\"491\":1,\"510\":1,\"931\":1,\"935\":1,\"937\":1,\"939\":1}}],[\"randint\",{\"0\":{\"484\":1},\"1\":{\"53\":1,\"92\":1,\"137\":1,\"208\":1,\"263\":2,\"484\":2,\"697\":1}}],[\"randomsampler\",{\"1\":{\"715\":1}}],[\"randomstate\",{\"1\":{\"513\":1}}],[\"randomresizedcrop\",{\"1\":{\"293\":3,\"359\":1,\"425\":2}}],[\"randomresizedcropandinterpolationwithtwopic\",{\"1\":{\"264\":1}}],[\"randomgrayscale\",{\"1\":{\"293\":1,\"359\":1}}],[\"randomapply\",{\"1\":{\"293\":1}}],[\"randomhorizontalflip\",{\"1\":{\"264\":1,\"293\":1,\"359\":1,\"425\":1}}],[\"random\",{\"1\":{\"53\":2,\"82\":1,\"92\":1,\"140\":1,\"208\":5,\"263\":6,\"359\":1,\"424\":2,\"513\":4,\"521\":5,\"697\":5,\"698\":4,\"795\":1,\"846\":1,\"918\":1}}],[\"range\",{\"1\":{\"53\":6,\"67\":1,\"82\":4,\"83\":2,\"105\":1,\"106\":2,\"119\":1,\"121\":3,\"122\":2,\"123\":2,\"137\":1,\"141\":2,\"187\":1,\"190\":3,\"192\":3,\"207\":2,\"213\":1,\"255\":1,\"263\":3,\"265\":1,\"293\":5,\"359\":1,\"380\":1,\"385\":2,\"386\":5,\"398\":2,\"410\":1,\"412\":1,\"419\":2,\"420\":1,\"424\":2,\"431\":1,\"444\":2,\"582\":2,\"595\":2,\"596\":1,\"597\":3,\"663\":1,\"697\":1,\"698\":2,\"699\":1,\"700\":1,\"719\":1,\"816\":2,\"892\":16,\"893\":2,\"895\":1,\"898\":1,\"899\":1,\"918\":1,\"926\":3,\"934\":1,\"935\":2,\"938\":1,\"939\":1,\"963\":1,\"964\":6}}],[\"ranking\",{\"1\":{\"656\":1}}],[\"rank=0\",{\"1\":{\"381\":1}}],[\"rank=false\",{\"1\":{\"190\":1}}],[\"rank=config\",{\"1\":{\"190\":1}}],[\"rank\",{\"1\":{\"34\":1,\"190\":6,\"386\":8,\"606\":1,\"608\":1,\"610\":1,\"611\":1,\"715\":1}}],[\"rivershavewings\",{\"1\":{\"894\":1}}],[\"riezler\",{\"1\":{\"655\":1}}],[\"rigid\",{\"1\":{\"153\":1,\"161\":1}}],[\"right\",{\"1\":{\"64\":1,\"274\":2,\"477\":1,\"749\":1,\"892\":1}}],[\"ride\",{\"1\":{\"53\":1}}],[\"r\",{\"1\":{\"60\":2,\"119\":15,\"269\":1,\"308\":1,\"309\":1,\"369\":1,\"391\":1,\"501\":2,\"502\":2,\"557\":1,\"595\":3,\"597\":6,\"613\":2,\"656\":3,\"696\":1,\"697\":2,\"709\":12,\"739\":1,\"880\":2,\"882\":1,\"924\":10}}],[\"rgb为彩色图片\",{\"1\":{\"424\":1}}],[\"rgb图像为3\",{\"1\":{\"255\":1}}],[\"rgb图像\",{\"1\":{\"255\":1}}],[\"rgb\",{\"1\":{\"53\":1,\"82\":1,\"159\":1,\"187\":1,\"380\":1,\"410\":1,\"412\":1,\"424\":3,\"426\":1,\"885\":6,\"899\":1,\"924\":2}}],[\"rotary\",{\"1\":{\"823\":1,\"892\":7,\"900\":2}}],[\"rotate\",{\"1\":{\"25\":1}}],[\"rotated\",{\"1\":{\"12\":1}}],[\"rotation\",{\"1\":{\"12\":1}}],[\"rosenbrock\",{\"1\":{\"816\":6}}],[\"rosenbrock函数的严格定义是\",{\"1\":{\"816\":1}}],[\"ross\",{\"1\":{\"510\":1}}],[\"rop\",{\"1\":{\"809\":2}}],[\"rope\",{\"1\":{\"667\":2,\"823\":4}}],[\"rouge分数仅略高于随机选句基线\",{\"1\":{\"641\":1}}],[\"round\",{\"1\":{\"263\":2,\"504\":1}}],[\"row\",{\"1\":{\"489\":1,\"541\":1,\"542\":1,\"696\":2}}],[\"rocket\",{\"1\":{\"408\":1}}],[\"roc\",{\"0\":{\"568\":1,\"569\":1,\"572\":1},\"1\":{\"106\":4,\"569\":3,\"570\":3,\"571\":1,\"572\":2}}],[\"root\",{\"1\":{\"92\":3,\"107\":3,\"187\":1,\"382\":1,\"410\":2,\"412\":2,\"424\":8}}],[\"root=\",{\"1\":{\"92\":1,\"926\":1,\"933\":1,\"963\":1}}],[\"roipool\",{\"1\":{\"502\":2}}],[\"roipooling\",{\"1\":{\"502\":1}}],[\"roialign目标\",{\"1\":{\"502\":1}}],[\"roialign\",{\"1\":{\"502\":2}}],[\"roi\",{\"0\":{\"501\":1,\"502\":1},\"1\":{\"83\":24,\"501\":15,\"502\":6}}],[\"robustly\",{\"1\":{\"676\":1,\"682\":1}}],[\"robustness\",{\"1\":{\"14\":1}}],[\"roberta在glue\",{\"1\":{\"688\":1}}],[\"roberta在9项任务中全面超越bert和xlnet\",{\"1\":{\"685\":1}}],[\"roberta证明更长训练\",{\"1\":{\"686\":1}}],[\"roberta延长至300k~500k步\",{\"1\":{\"684\":1}}],[\"roberta新增cc\",{\"1\":{\"684\":1}}],[\"roberta增大至8k\",{\"1\":{\"683\":1}}],[\"roberta核心改进总结\",{\"0\":{\"682\":1}}],[\"roberta改用full\",{\"1\":{\"683\":1}}],[\"roberta改用基于字节的bpe\",{\"1\":{\"681\":1}}],[\"roberta改为动态掩码\",{\"1\":{\"681\":1}}],[\"roberta采用8k批次进行训练\",{\"1\":{\"681\":1}}],[\"roberta尝试增大批次至2k和8k\",{\"1\":{\"681\":1}}],[\"roberta对比了多种输入格式\",{\"1\":{\"681\":1}}],[\"roberta的改进表明\",{\"1\":{\"686\":1}}],[\"roberta的改进\",{\"1\":{\"681\":1}}],[\"roberta是一项针对bert预训练方法的优化研究\",{\"1\":{\"677\":1}}],[\"roberta\",{\"0\":{\"676\":1},\"1\":{\"32\":1,\"37\":1,\"54\":1,\"55\":1,\"96\":1,\"676\":2,\"678\":4,\"679\":5,\"680\":5,\"682\":1}}],[\"reinmax\",{\"1\":{\"899\":8}}],[\"reinforcement\",{\"1\":{\"339\":1,\"602\":2,\"654\":1}}],[\"reed\",{\"1\":{\"884\":1}}],[\"reject\",{\"1\":{\"658\":1}}],[\"reversed\",{\"1\":{\"899\":1}}],[\"reverse=true\",{\"1\":{\"595\":1,\"597\":1}}],[\"reversible\",{\"1\":{\"892\":4}}],[\"revealed\",{\"1\":{\"83\":1}}],[\"removed\",{\"1\":{\"597\":1}}],[\"remove\",{\"1\":{\"554\":1}}],[\"remote\",{\"1\":{\"52\":2}}],[\"requirements\",{\"1\":{\"557\":2,\"739\":2}}],[\"require\",{\"1\":{\"461\":1}}],[\"requires\",{\"1\":{\"104\":2,\"213\":4,\"293\":1,\"361\":1,\"380\":3,\"435\":1,\"892\":1,\"918\":2}}],[\"reddit\",{\"1\":{\"640\":1,\"680\":1}}],[\"red\",{\"1\":{\"408\":1,\"816\":1}}],[\"reduce\",{\"1\":{\"213\":3}}],[\"reduce=none\",{\"1\":{\"208\":1}}],[\"reduction==\",{\"1\":{\"403\":1}}],[\"reduction=reduction\",{\"1\":{\"403\":1,\"420\":1}}],[\"reduction=\",{\"1\":{\"256\":1,\"260\":1,\"403\":1,\"420\":1,\"587\":1,\"589\":1,\"932\":2}}],[\"reduction\",{\"1\":{\"208\":1,\"420\":1,\"589\":1}}],[\"reward\",{\"1\":{\"339\":2,\"656\":1}}],[\"retain\",{\"1\":{\"807\":3}}],[\"retstep\",{\"1\":{\"440\":1}}],[\"ret\",{\"1\":{\"383\":13,\"384\":7,\"385\":15,\"386\":8,\"710\":5,\"815\":4}}],[\"retrieval\",{\"0\":{\"828\":1},\"1\":{\"190\":4,\"309\":1,\"828\":1,\"837\":1}}],[\"returnfps\",{\"1\":{\"137\":1}}],[\"returnfps=false\",{\"1\":{\"137\":1}}],[\"returns\",{\"1\":{\"67\":1,\"69\":1,\"70\":1,\"107\":2,\"120\":1,\"121\":2,\"122\":2,\"123\":1,\"213\":5,\"597\":9,\"713\":1}}],[\"return\",{\"1\":{\"52\":4,\"53\":2,\"54\":1,\"56\":1,\"57\":1,\"58\":2,\"59\":2,\"60\":2,\"64\":4,\"65\":1,\"67\":1,\"69\":2,\"70\":1,\"82\":2,\"83\":8,\"92\":3,\"94\":1,\"96\":2,\"97\":1,\"98\":2,\"99\":1,\"100\":2,\"102\":1,\"107\":2,\"119\":4,\"120\":1,\"121\":2,\"122\":2,\"123\":3,\"137\":13,\"138\":1,\"141\":3,\"143\":1,\"145\":1,\"146\":1,\"152\":1,\"153\":1,\"154\":2,\"155\":1,\"156\":1,\"187\":4,\"188\":3,\"190\":6,\"191\":5,\"192\":7,\"204\":1,\"206\":2,\"207\":2,\"208\":14,\"213\":12,\"255\":1,\"256\":10,\"263\":2,\"264\":2,\"265\":2,\"266\":6,\"274\":10,\"293\":5,\"359\":1,\"362\":1,\"380\":4,\"382\":15,\"383\":4,\"384\":2,\"385\":3,\"386\":1,\"397\":2,\"398\":1,\"399\":1,\"400\":2,\"401\":1,\"403\":5,\"410\":9,\"411\":4,\"412\":13,\"417\":3,\"419\":2,\"420\":10,\"421\":1,\"424\":7,\"426\":2,\"427\":2,\"428\":2,\"429\":2,\"430\":1,\"431\":2,\"435\":1,\"447\":1,\"448\":1,\"449\":2,\"451\":1,\"452\":3,\"453\":4,\"454\":4,\"455\":2,\"456\":1,\"459\":1,\"461\":6,\"480\":6,\"511\":1,\"582\":1,\"586\":1,\"587\":1,\"588\":1,\"589\":1,\"590\":1,\"592\":1,\"595\":7,\"596\":5,\"597\":22,\"663\":9,\"696\":1,\"697\":5,\"698\":2,\"699\":1,\"703\":2,\"709\":1,\"710\":2,\"713\":6,\"715\":1,\"716\":1,\"718\":3,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"724\":2,\"725\":1,\"726\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"734\":1,\"736\":1,\"737\":1,\"742\":3,\"743\":1,\"745\":1,\"746\":1,\"747\":1,\"749\":1,\"750\":1,\"751\":2,\"762\":2,\"763\":1,\"766\":2,\"771\":1,\"779\":1,\"780\":1,\"790\":2,\"800\":1,\"805\":1,\"807\":2,\"808\":7,\"809\":24,\"811\":3,\"815\":6,\"816\":2,\"893\":5,\"894\":2,\"895\":3,\"896\":1,\"897\":1,\"898\":1,\"899\":13,\"900\":4,\"918\":3,\"926\":2,\"931\":4,\"932\":1,\"937\":4,\"963\":4,\"964\":3}}],[\"regression\",{\"1\":{\"722\":1}}],[\"regress\",{\"1\":{\"213\":2}}],[\"register\",{\"0\":{\"474\":1,\"511\":1},\"1\":{\"190\":4,\"192\":3,\"205\":3,\"213\":3,\"293\":1,\"361\":2,\"420\":1,\"474\":2,\"511\":2,\"892\":1,\"926\":1,\"964\":1}}],[\"region\",{\"1\":{\"6\":1,\"11\":1,\"83\":1,\"106\":1,\"137\":2,\"387\":2,\"388\":1,\"502\":1}}],[\"regularizer\",{\"1\":{\"153\":3}}],[\"reg\",{\"1\":{\"150\":1}}],[\"recurrent\",{\"1\":{\"921\":1}}],[\"recall\",{\"0\":{\"563\":1},\"1\":{\"309\":2,\"590\":1}}],[\"record等任务上表现接近sota\",{\"1\":{\"648\":1}}],[\"recompute\",{\"1\":{\"503\":2}}],[\"recognition\",{\"1\":{\"273\":1}}],[\"recon\",{\"1\":{\"256\":2,\"899\":2,\"932\":3,\"934\":2,\"935\":4,\"938\":2,\"963\":11,\"964\":2}}],[\"recons\",{\"1\":{\"256\":2,\"899\":2}}],[\"reconstruction\",{\"1\":{\"235\":1,\"935\":1}}],[\"recovered\",{\"1\":{\"143\":1}}],[\"rec\",{\"1\":{\"213\":22}}],[\"recip\",{\"1\":{\"122\":3,\"145\":3}}],[\"receiver\",{\"0\":{\"569\":1},\"1\":{\"106\":1}}],[\"rendering\",{\"1\":{\"148\":1}}],[\"render\",{\"1\":{\"107\":1}}],[\"reparameterization\",{\"1\":{\"946\":1,\"947\":1}}],[\"reparameterize\",{\"1\":{\"931\":2,\"937\":2}}],[\"repr\",{\"1\":{\"808\":1}}],[\"representations\",{\"0\":{\"709\":1}}],[\"representation\",{\"0\":{\"249\":1,\"417\":1},\"1\":{\"193\":2,\"195\":1,\"227\":1,\"252\":1,\"347\":2,\"417\":3,\"418\":2,\"419\":2,\"428\":1,\"431\":3,\"435\":1,\"690\":1,\"735\":1}}],[\"repo\",{\"1\":{\"410\":1,\"412\":1}}],[\"replacement\",{\"1\":{\"518\":1}}],[\"replacement=true\",{\"1\":{\"518\":2}}],[\"replace\",{\"1\":{\"364\":1,\"410\":1,\"412\":1,\"697\":2,\"808\":1}}],[\"replaced\",{\"1\":{\"208\":3}}],[\"repetition\",{\"1\":{\"188\":3,\"421\":1}}],[\"repeated\",{\"1\":{\"471\":1}}],[\"repeat\",{\"0\":{\"471\":1},\"1\":{\"60\":1,\"122\":1,\"137\":6,\"145\":1,\"152\":1,\"154\":1,\"188\":1,\"213\":2,\"274\":3,\"421\":1,\"471\":7,\"472\":1}}],[\"rephrase\",{\"1\":{\"92\":3}}],[\"relposattention\",{\"1\":{\"709\":1}}],[\"rel\",{\"1\":{\"266\":4,\"384\":1,\"385\":2,\"709\":12}}],[\"relative\",{\"0\":{\"708\":1,\"709\":1,\"710\":1},\"1\":{\"380\":10,\"384\":3,\"385\":8,\"710\":26}}],[\"relationship\",{\"1\":{\"730\":4,\"731\":4}}],[\"relation\",{\"1\":{\"83\":3}}],[\"related\",{\"0\":{\"166\":1,\"195\":1,\"355\":1,\"369\":1},\"1\":{\"429\":1}}],[\"relu神经元可自动学习\",{\"1\":{\"500\":1}}],[\"relu激活函数\",{\"1\":{\"121\":1}}],[\"relu激活\",{\"1\":{\"120\":2,\"121\":2,\"122\":4}}],[\"relu\",{\"1\":{\"58\":2,\"59\":3,\"60\":2,\"65\":2,\"66\":1,\"68\":1,\"69\":1,\"70\":2,\"83\":5,\"113\":1,\"114\":1,\"116\":2,\"119\":3,\"120\":7,\"121\":6,\"122\":4,\"123\":1,\"137\":2,\"138\":2,\"141\":5,\"143\":1,\"145\":4,\"146\":1,\"152\":8,\"154\":2,\"155\":4,\"156\":3,\"255\":6,\"819\":1,\"823\":1,\"899\":3,\"926\":24,\"931\":3,\"937\":2,\"963\":2,\"964\":2}}],[\"releases\",{\"1\":{\"435\":1}}],[\"release\",{\"1\":{\"6\":1,\"12\":1,\"14\":2}}],[\"re\",{\"1\":{\"53\":1,\"595\":2,\"597\":3}}],[\"ref\",{\"1\":{\"806\":1,\"807\":1}}],[\"refers\",{\"1\":{\"385\":1}}],[\"reference\",{\"1\":{\"107\":2,\"243\":1}}],[\"referred\",{\"1\":{\"94\":2,\"100\":3}}],[\"referring\",{\"1\":{\"7\":1}}],[\"refrigerator\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"respectful\",{\"1\":{\"670\":1}}],[\"respectful类毒性分0\",{\"1\":{\"668\":1}}],[\"response\",{\"1\":{\"339\":1,\"656\":1}}],[\"response4\",{\"1\":{\"52\":2}}],[\"response3\",{\"1\":{\"52\":2}}],[\"response2\",{\"1\":{\"52\":2}}],[\"response1\",{\"1\":{\"52\":2}}],[\"restval\",{\"1\":{\"382\":1}}],[\"resblocks\",{\"1\":{\"899\":2}}],[\"resblock\",{\"1\":{\"255\":3,\"899\":7}}],[\"rescale\",{\"1\":{\"236\":1}}],[\"res\",{\"1\":{\"205\":2}}],[\"resolution\",{\"0\":{\"142\":1},\"1\":{\"139\":1,\"143\":1}}],[\"research\",{\"1\":{\"97\":1,\"129\":1,\"435\":1,\"634\":1}}],[\"resid\",{\"1\":{\"663\":1}}],[\"residual\",{\"1\":{\"96\":1,\"98\":1,\"402\":1,\"663\":6,\"741\":1}}],[\"resize\",{\"1\":{\"53\":2,\"82\":2,\"425\":2,\"582\":1,\"918\":1}}],[\"resulting\",{\"1\":{\"597\":1}}],[\"result\",{\"1\":{\"52\":1,\"92\":5,\"119\":1,\"121\":4,\"449\":2,\"452\":2,\"453\":2,\"459\":2,\"461\":2}}],[\"results\",{\"0\":{\"47\":1},\"1\":{\"105\":1,\"106\":3}}],[\"reshaped\",{\"1\":{\"737\":4}}],[\"reshape后\",{\"1\":{\"58\":1,\"59\":1}}],[\"reshape\",{\"0\":{\"470\":1},\"1\":{\"32\":1,\"58\":2,\"59\":2,\"119\":1,\"137\":1,\"152\":1,\"213\":3,\"380\":2,\"401\":1,\"430\":4,\"470\":6,\"513\":1,\"542\":2,\"582\":5,\"663\":1,\"737\":1,\"926\":1}}],[\"resnet和混合模型的效果均不如vit模型\",{\"1\":{\"434\":1}}],[\"resnet和混合模型在不同图像分类数据集上的测试结果\",{\"1\":{\"434\":1}}],[\"resnet101\",{\"1\":{\"407\":1}}],[\"resnet18在图像分类任务中表现出色\",{\"1\":{\"497\":1}}],[\"resnet18是一种深度残差网络\",{\"1\":{\"497\":1}}],[\"resnet18\",{\"0\":{\"497\":1},\"1\":{\"46\":1,\"64\":1,\"83\":1}}],[\"resnet50\",{\"1\":{\"407\":1,\"510\":1,\"511\":1}}],[\"resnet包含五种不同尺寸的模型\",{\"1\":{\"407\":1}}],[\"resnet提取特征\",{\"1\":{\"78\":1}}],[\"resnet\",{\"1\":{\"32\":1,\"280\":1,\"285\":1,\"352\":1,\"361\":1,\"362\":1,\"388\":1,\"407\":1,\"434\":1,\"501\":1,\"510\":1,\"899\":4}}],[\"rearrange\",{\"0\":{\"478\":1},\"1\":{\"213\":5,\"256\":1,\"260\":1,\"274\":3,\"478\":7,\"892\":2,\"893\":2,\"899\":2,\"900\":1}}],[\"reader\",{\"1\":{\"696\":3}}],[\"ready\",{\"1\":{\"597\":1}}],[\"read\",{\"1\":{\"53\":5,\"82\":4,\"92\":1,\"107\":2,\"424\":1,\"425\":1,\"595\":1,\"596\":1,\"597\":1,\"712\":9}}],[\"realnews\",{\"1\":{\"823\":1}}],[\"realtoxicityprompts\",{\"1\":{\"656\":1,\"657\":1,\"668\":1,\"670\":1}}],[\"real\",{\"1\":{\"12\":1,\"14\":1,\"918\":5}}],[\"reasoning\",{\"0\":{\"35\":1,\"36\":1},\"1\":{\"7\":1,\"35\":1,\"36\":1,\"52\":4,\"128\":2,\"342\":1,\"344\":1,\"383\":1,\"620\":1,\"621\":1,\"622\":1,\"668\":1}}],[\"4b\",{\"1\":{\"823\":1}}],[\"4o\",{\"1\":{\"823\":5}}],[\"4个特殊词\",{\"1\":{\"697\":1}}],[\"4次\",{\"1\":{\"681\":2}}],[\"4t\",{\"1\":{\"666\":1,\"667\":2,\"668\":1}}],[\"4点\",{\"1\":{\"641\":1}}],[\"4监督基线的性能\",{\"1\":{\"641\":1}}],[\"4种不同规模的模型\",{\"1\":{\"640\":1}}],[\"4×3\",{\"1\":{\"546\":2}}],[\"4×4\",{\"1\":{\"263\":1}}],[\"4f\",{\"1\":{\"461\":1,\"926\":1,\"934\":1,\"938\":1,\"963\":2,\"964\":1}}],[\"4替换为相应的类别名称\",{\"1\":{\"424\":1}}],[\"4的视觉能力\",{\"1\":{\"325\":1}}],[\"4k分辨率\",{\"1\":{\"329\":1,\"334\":1}}],[\"4k\",{\"1\":{\"322\":1}}],[\"4v仍有差距\",{\"1\":{\"335\":1}}],[\"4v的\",{\"1\":{\"323\":1}}],[\"4v\",{\"1\":{\"321\":2,\"322\":2,\"323\":2,\"325\":1,\"335\":2,\"337\":1,\"823\":1}}],[\"4c\",{\"1\":{\"305\":1}}],[\"4等\",{\"1\":{\"300\":1}}],[\"4g\",{\"1\":{\"291\":1}}],[\"4423742632\",{\"1\":{\"815\":1}}],[\"4423761088\",{\"1\":{\"815\":1}}],[\"448\",{\"1\":{\"895\":1}}],[\"448px\",{\"1\":{\"330\":3}}],[\"448×448\",{\"1\":{\"322\":1}}],[\"44\",{\"1\":{\"288\":1}}],[\"4x4卷积下采样\",{\"1\":{\"255\":1}}],[\"404\",{\"1\":{\"967\":1}}],[\"405b\",{\"1\":{\"823\":3}}],[\"40gb文本\",{\"1\":{\"640\":1}}],[\"40图块\",{\"1\":{\"334\":1}}],[\"40区块\",{\"1\":{\"329\":1}}],[\"400m数据\",{\"1\":{\"415\":1}}],[\"400\",{\"1\":{\"305\":1,\"816\":5}}],[\"406\",{\"1\":{\"293\":1,\"359\":1,\"582\":1}}],[\"40\",{\"1\":{\"214\":1,\"223\":1,\"224\":1,\"228\":1,\"234\":2,\"236\":1,\"293\":2,\"322\":1,\"504\":5,\"633\":1,\"656\":1,\"681\":1,\"815\":1}}],[\"4096\",{\"1\":{\"69\":1,\"146\":2}}],[\"465953\",{\"1\":{\"816\":1}}],[\"465651\",{\"1\":{\"816\":1}}],[\"465348\",{\"1\":{\"816\":1}}],[\"465046\",{\"1\":{\"816\":1}}],[\"464743\",{\"1\":{\"816\":1}}],[\"464440\",{\"1\":{\"816\":1}}],[\"464137\",{\"1\":{\"816\":1}}],[\"463833\",{\"1\":{\"816\":1}}],[\"4638\",{\"1\":{\"713\":2}}],[\"46\",{\"1\":{\"110\":1,\"656\":1}}],[\"4️⃣\",{\"0\":{\"99\":1},\"1\":{\"145\":1,\"963\":1}}],[\"416\",{\"1\":{\"89\":1}}],[\"4大小的特征图\",{\"1\":{\"83\":3}}],[\"4960\",{\"1\":{\"713\":2}}],[\"496\",{\"1\":{\"331\":1}}],[\"49\",{\"1\":{\"54\":2,\"58\":6,\"59\":5,\"215\":1,\"823\":1}}],[\"4847711968\",{\"1\":{\"815\":2}}],[\"4847712064\",{\"1\":{\"815\":4}}],[\"4847712112\",{\"1\":{\"815\":2}}],[\"4873\",{\"1\":{\"713\":2}}],[\"48层\",{\"1\":{\"303\":1}}],[\"485\",{\"1\":{\"293\":1,\"341\":1,\"359\":1,\"582\":1}}],[\"48\",{\"1\":{\"48\":1,\"410\":1,\"502\":1,\"811\":3,\"815\":1}}],[\"4775983056\",{\"1\":{\"815\":2}}],[\"4788\",{\"1\":{\"713\":2}}],[\"47\",{\"1\":{\"46\":1,\"268\":1}}],[\"45层\",{\"1\":{\"329\":1}}],[\"456\",{\"1\":{\"293\":1,\"359\":1,\"582\":1}}],[\"45\",{\"1\":{\"46\":1}}],[\"4e\",{\"1\":{\"34\":1}}],[\"4375\",{\"1\":{\"895\":1}}],[\"43类\",{\"1\":{\"41\":1}}],[\"43类物体\",{\"1\":{\"30\":1}}],[\"43\",{\"1\":{\"11\":1,\"32\":1,\"46\":1,\"53\":1,\"895\":1}}],[\"4\",{\"0\":{\"25\":1,\"39\":1,\"49\":1,\"89\":1,\"343\":2,\"400\":1,\"428\":1,\"909\":1,\"915\":1,\"933\":1},\"1\":{\"11\":1,\"25\":1,\"32\":1,\"34\":1,\"46\":1,\"52\":5,\"53\":2,\"54\":1,\"59\":1,\"60\":1,\"64\":1,\"65\":1,\"82\":1,\"83\":28,\"87\":3,\"93\":1,\"100\":2,\"102\":3,\"105\":1,\"106\":2,\"107\":3,\"117\":2,\"119\":8,\"121\":1,\"123\":11,\"138\":3,\"141\":3,\"146\":1,\"150\":1,\"187\":3,\"190\":3,\"192\":5,\"202\":1,\"206\":1,\"208\":2,\"215\":2,\"234\":1,\"242\":1,\"264\":3,\"265\":1,\"268\":1,\"286\":1,\"293\":7,\"305\":1,\"306\":2,\"310\":1,\"315\":1,\"326\":1,\"329\":1,\"335\":1,\"339\":1,\"342\":2,\"343\":6,\"359\":5,\"362\":1,\"380\":3,\"381\":1,\"384\":2,\"385\":2,\"403\":4,\"420\":1,\"421\":1,\"430\":1,\"435\":1,\"441\":5,\"463\":3,\"469\":1,\"471\":7,\"472\":4,\"477\":5,\"478\":4,\"481\":2,\"482\":7,\"483\":1,\"484\":1,\"486\":1,\"490\":6,\"500\":2,\"502\":6,\"516\":3,\"540\":2,\"541\":2,\"542\":10,\"544\":6,\"545\":7,\"546\":3,\"572\":1,\"582\":1,\"589\":1,\"595\":5,\"613\":1,\"614\":2,\"633\":2,\"634\":1,\"640\":1,\"641\":5,\"646\":1,\"647\":1,\"648\":5,\"649\":1,\"650\":1,\"655\":1,\"656\":2,\"657\":2,\"658\":1,\"660\":1,\"663\":8,\"666\":1,\"667\":3,\"668\":5,\"669\":2,\"678\":1,\"680\":4,\"684\":1,\"694\":1,\"697\":1,\"699\":2,\"700\":2,\"706\":1,\"709\":6,\"710\":10,\"716\":1,\"737\":1,\"751\":1,\"771\":1,\"794\":1,\"795\":1,\"808\":6,\"818\":1,\"823\":26,\"892\":2,\"893\":1,\"926\":7,\"932\":1,\"935\":1,\"963\":4,\"964\":1}}],[\"3~4\",{\"1\":{\"710\":1}}],[\"31\",{\"1\":{\"823\":1}}],[\"3167\",{\"1\":{\"701\":1}}],[\"31gb\",{\"1\":{\"680\":1}}],[\"3提升至87\",{\"1\":{\"684\":1}}],[\"3但可靠性仍不足\",{\"1\":{\"670\":1}}],[\"36\",{\"1\":{\"668\":1,\"811\":1,\"815\":1}}],[\"3640\",{\"1\":{\"658\":1}}],[\"364×364\",{\"1\":{\"318\":1}}],[\"3小10倍却性能更优\",{\"1\":{\"666\":1}}],[\"3更受人类偏好\",{\"1\":{\"654\":1}}],[\"3更像是一个巨大的\",{\"1\":{\"649\":1}}],[\"3模型\",{\"1\":{\"654\":1}}],[\"3模型架构基本沿用gpt\",{\"1\":{\"647\":1}}],[\"3进行监督学习微调\",{\"1\":{\"653\":1}}],[\"3站在了词向量\",{\"1\":{\"650\":1}}],[\"3是首次系统性\",{\"1\":{\"650\":1}}],[\"3并未对每个任务建立单独的模型\",{\"1\":{\"650\":1}}],[\"3验证了一个关键假设\",{\"1\":{\"650\":1}}],[\"3以175b参数扩展至前代模型的10倍以上\",{\"1\":{\"650\":1}}],[\"3完全通过文本学习并表达任务结构\",{\"1\":{\"650\":1}}],[\"3完全不依赖梯度更新\",{\"1\":{\"649\":1}}],[\"3通过扩展模型容量\",{\"1\":{\"650\":1}}],[\"3之前\",{\"1\":{\"650\":1}}],[\"3继承了这一发展路线\",{\"1\":{\"650\":1}}],[\"3及其衍生模型时\",{\"1\":{\"649\":1}}],[\"3虽然能完成基础算术和简单逻辑题\",{\"1\":{\"649\":1}}],[\"3虽然模型更大\",{\"1\":{\"647\":1}}],[\"3对提示\",{\"1\":{\"649\":1}}],[\"3对复杂语义结构的掌握仍有提升空间\",{\"1\":{\"648\":1}}],[\"3展示了推理能力的不足\",{\"1\":{\"648\":1}}],[\"3展现出更强的语言建模优势\",{\"1\":{\"648\":1}}],[\"3表现较差\",{\"1\":{\"648\":1}}],[\"3也表现出明显的few\",{\"1\":{\"648\":1}}],[\"3比较不同答案的语言模型概率\",{\"1\":{\"647\":1}}],[\"3主要研究后三种方法\",{\"1\":{\"647\":1}}],[\"3在设计的算术\",{\"1\":{\"648\":1}}],[\"3在识别细粒度语义差异上仍有明显不足\",{\"1\":{\"648\":1}}],[\"3在少样本\",{\"1\":{\"648\":1}}],[\"3在few\",{\"1\":{\"648\":1,\"649\":1}}],[\"3在winograd\",{\"1\":{\"648\":1}}],[\"3在英法\",{\"1\":{\"648\":1}}],[\"3在triviaqa\",{\"1\":{\"648\":1}}],[\"3在传统语言建模任务\",{\"1\":{\"648\":1}}],[\"3在语言建模和完形填空任务中的表现\",{\"1\":{\"648\":1}}],[\"3在多个任务上取得了令人印象深刻的成绩\",{\"1\":{\"649\":1}}],[\"3在多个任务上展现出超越以往fine\",{\"1\":{\"647\":1}}],[\"3在多数nlp任务中\",{\"1\":{\"648\":1}}],[\"3在多种自然语言处理任务中表现出色\",{\"1\":{\"645\":1}}],[\"3在通用语言系统发展中的潜力及其可能带来的广泛社会影响\",{\"1\":{\"645\":1}}],[\"3等更大规模模型的开发奠定了基础\",{\"1\":{\"642\":1}}],[\"3的175b参数\",{\"1\":{\"666\":1}}],[\"3的核心创新之一\",{\"1\":{\"650\":1}}],[\"3的推理过程完全由大量参数和非线性变换组成\",{\"1\":{\"649\":1}}],[\"3的表现明显弱于专门微调过的模型\",{\"1\":{\"649\":1}}],[\"3的上下文窗口扩大到2048\",{\"1\":{\"649\":1}}],[\"3的few\",{\"1\":{\"648\":1,\"649\":1}}],[\"3的研究方法基于\",{\"1\":{\"647\":1}}],[\"3的训练数据主要来自以下五个来源\",{\"1\":{\"647\":1}}],[\"3的少样本学习\",{\"1\":{\"641\":1}}],[\"3的准确率\",{\"1\":{\"634\":1}}],[\"3的维度\",{\"1\":{\"430\":1}}],[\"3×4\",{\"1\":{\"544\":1}}],[\"3×3\",{\"1\":{\"150\":1,\"152\":1,\"153\":1}}],[\"3x4\",{\"1\":{\"544\":1}}],[\"3x3\",{\"1\":{\"152\":2,\"923\":2}}],[\"3层即可\",{\"1\":{\"500\":1}}],[\"330\",{\"1\":{\"888\":1}}],[\"33\",{\"1\":{\"668\":1}}],[\"333\",{\"1\":{\"462\":1}}],[\"336\",{\"1\":{\"407\":1}}],[\"396\",{\"1\":{\"410\":1}}],[\"3508\",{\"1\":{\"701\":1}}],[\"35\",{\"1\":{\"369\":1,\"648\":1,\"960\":1}}],[\"3m\",{\"1\":{\"341\":1}}],[\"3v\",{\"1\":{\"325\":1}}],[\"3天内达到了\",{\"1\":{\"291\":1}}],[\"3g\",{\"1\":{\"291\":1}}],[\"3n\",{\"1\":{\"207\":1}}],[\"3b参数的instructgpt模型\",{\"1\":{\"654\":1}}],[\"3b对比175b\",{\"1\":{\"653\":1}}],[\"3b\",{\"1\":{\"190\":3,\"192\":2,\"655\":1,\"656\":1,\"657\":1,\"658\":1}}],[\"30b\",{\"1\":{\"823\":1}}],[\"300m\",{\"1\":{\"888\":1}}],[\"300m数据集\",{\"1\":{\"432\":1}}],[\"300m数据集的规模达到了上亿级别\",{\"1\":{\"413\":1}}],[\"300m数据集取得了较好的结果\",{\"1\":{\"413\":1}}],[\"300m数据集是谷歌从互联网上收集的\",{\"1\":{\"413\":1}}],[\"300m数据集来预训练模型在imagenet上取得sota\",{\"1\":{\"413\":1}}],[\"300m数据集还要多出1亿对\",{\"1\":{\"407\":1}}],[\"300\",{\"1\":{\"242\":1,\"341\":1}}],[\"3072\",{\"1\":{\"236\":1,\"633\":1,\"718\":2}}],[\"30\",{\"1\":{\"190\":2,\"286\":1,\"504\":5,\"811\":1,\"815\":1,\"816\":1,\"847\":2}}],[\"3节中的条件生成任务\",{\"1\":{\"187\":1}}],[\"3+d\",{\"1\":{\"137\":3}}],[\"3+c\",{\"1\":{\"119\":3,\"121\":2}}],[\"3️⃣\",{\"0\":{\"98\":1},\"1\":{\"145\":1,\"963\":1}}],[\"345m\",{\"1\":{\"640\":1,\"641\":1}}],[\"34b更换为internlm2\",{\"1\":{\"330\":1}}],[\"34b结合\",{\"1\":{\"330\":1}}],[\"34\",{\"1\":{\"47\":1,\"269\":1,\"656\":1,\"657\":1,\"658\":1}}],[\"38gb\",{\"1\":{\"680\":1}}],[\"384\",{\"1\":{\"187\":1,\"191\":1,\"341\":1,\"887\":1}}],[\"38\",{\"1\":{\"41\":1,\"48\":1}}],[\"38k\",{\"1\":{\"11\":1}}],[\"32×32\",{\"1\":{\"885\":4,\"886\":1,\"887\":1}}],[\"32b\",{\"1\":{\"823\":6,\"837\":1}}],[\"32k\",{\"1\":{\"823\":2}}],[\"32个示例\",{\"1\":{\"648\":1}}],[\"32768\",{\"1\":{\"407\":1}}],[\"328\",{\"1\":{\"331\":1}}],[\"32的预训练权重进行初始化\",{\"1\":{\"306\":1}}],[\"32gb\",{\"1\":{\"236\":1}}],[\"320\",{\"1\":{\"59\":1,\"83\":1,\"141\":1}}],[\"32\",{\"1\":{\"41\":1,\"119\":1,\"123\":1,\"141\":4,\"144\":2,\"146\":2,\"212\":1,\"273\":2,\"332\":1,\"407\":1,\"502\":1,\"614\":1,\"633\":1,\"710\":1,\"811\":1,\"815\":1,\"881\":1,\"895\":1,\"900\":1}}],[\"37b\",{\"1\":{\"823\":1}}],[\"37\",{\"1\":{\"39\":1,\"242\":1,\"369\":1,\"668\":1}}],[\"3\",{\"0\":{\"24\":1,\"33\":1,\"38\":2,\"39\":1,\"48\":1,\"66\":1,\"88\":1,\"248\":1,\"399\":1,\"420\":1,\"427\":1,\"528\":1,\"533\":1,\"644\":1,\"908\":1,\"914\":1,\"932\":1},\"1\":{\"11\":1,\"24\":1,\"31\":2,\"32\":4,\"50\":1,\"52\":4,\"53\":3,\"54\":4,\"58\":6,\"59\":7,\"60\":1,\"64\":3,\"70\":2,\"82\":2,\"83\":12,\"92\":2,\"94\":9,\"99\":1,\"100\":8,\"102\":1,\"104\":1,\"106\":5,\"107\":12,\"110\":1,\"114\":1,\"117\":2,\"119\":31,\"120\":1,\"121\":7,\"122\":4,\"123\":8,\"137\":14,\"138\":5,\"141\":6,\"143\":2,\"144\":2,\"145\":16,\"146\":1,\"150\":2,\"152\":9,\"154\":1,\"155\":1,\"187\":1,\"190\":2,\"192\":2,\"206\":1,\"208\":2,\"213\":1,\"215\":1,\"220\":10,\"221\":1,\"222\":2,\"223\":1,\"224\":5,\"225\":5,\"234\":2,\"236\":1,\"255\":5,\"263\":2,\"265\":1,\"268\":4,\"271\":1,\"280\":2,\"293\":3,\"299\":1,\"305\":2,\"306\":1,\"308\":1,\"310\":1,\"311\":2,\"315\":1,\"318\":1,\"331\":1,\"335\":1,\"339\":4,\"341\":1,\"357\":2,\"359\":1,\"361\":1,\"362\":1,\"375\":1,\"380\":8,\"381\":1,\"384\":2,\"385\":2,\"397\":1,\"401\":1,\"403\":1,\"408\":1,\"409\":1,\"412\":1,\"413\":1,\"419\":10,\"420\":2,\"421\":1,\"424\":1,\"426\":3,\"430\":7,\"441\":5,\"444\":2,\"452\":1,\"462\":2,\"463\":3,\"469\":1,\"471\":8,\"472\":16,\"477\":7,\"478\":1,\"480\":13,\"481\":4,\"482\":3,\"483\":1,\"484\":2,\"485\":3,\"486\":1,\"490\":3,\"491\":1,\"500\":3,\"502\":5,\"510\":1,\"513\":2,\"514\":2,\"516\":1,\"540\":5,\"541\":2,\"542\":12,\"544\":4,\"545\":11,\"546\":8,\"547\":2,\"571\":1,\"582\":2,\"589\":1,\"590\":2,\"595\":7,\"596\":1,\"611\":2,\"633\":1,\"640\":2,\"641\":12,\"644\":1,\"645\":1,\"646\":5,\"647\":1,\"648\":14,\"649\":2,\"650\":1,\"651\":1,\"653\":1,\"654\":1,\"655\":5,\"656\":2,\"657\":17,\"658\":2,\"660\":1,\"663\":6,\"665\":1,\"667\":2,\"668\":6,\"669\":1,\"670\":1,\"678\":1,\"680\":4,\"683\":4,\"699\":2,\"709\":19,\"710\":22,\"716\":1,\"724\":2,\"749\":1,\"751\":1,\"794\":1,\"803\":1,\"808\":8,\"809\":10,\"811\":3,\"815\":4,\"816\":2,\"822\":5,\"823\":24,\"825\":1,\"833\":1,\"847\":1,\"881\":1,\"886\":1,\"888\":1,\"895\":1,\"899\":5,\"900\":1,\"924\":2,\"926\":9,\"932\":1,\"934\":1,\"938\":1,\"963\":3,\"964\":3}}],[\"3d`\",{\"1\":{\"106\":1}}],[\"3d对象功能区域分割\",{\"1\":{\"84\":1}}],[\"3d对象功能定位\",{\"1\":{\"30\":1}}],[\"3d功能热图\",{\"1\":{\"78\":1}}],[\"3d功能定位\",{\"1\":{\"75\":1}}],[\"3d数据学习功能\",{\"1\":{\"75\":1}}],[\"3daffordance\",{\"1\":{\"54\":2,\"59\":4,\"83\":10,\"94\":6,\"100\":12}}],[\"3d点云样本\",{\"1\":{\"53\":1}}],[\"3dir\",{\"1\":{\"41\":1}}],[\"3d实例\",{\"1\":{\"30\":1}}],[\"3dgs\",{\"0\":{\"13\":1},\"1\":{\"19\":3}}],[\"3d\",{\"0\":{\"4\":1,\"13\":1,\"22\":1,\"70\":1,\"127\":1},\"1\":{\"2\":1,\"4\":1,\"6\":1,\"7\":3,\"11\":2,\"12\":1,\"14\":2,\"18\":2,\"19\":17,\"20\":11,\"21\":2,\"22\":5,\"23\":7,\"24\":2,\"26\":6,\"28\":1,\"31\":1,\"32\":2,\"39\":2,\"40\":2,\"41\":1,\"46\":2,\"49\":1,\"50\":4,\"53\":1,\"61\":1,\"64\":2,\"66\":1,\"70\":2,\"71\":1,\"72\":1,\"83\":6,\"84\":3,\"86\":2,\"87\":1,\"91\":1,\"93\":3,\"94\":4,\"105\":2,\"106\":6,\"109\":4,\"110\":3,\"114\":1,\"115\":1,\"117\":1,\"119\":1,\"128\":2,\"152\":1,\"154\":1,\"157\":1,\"159\":4,\"467\":1,\"493\":2,\"503\":1}}],[\"2中间的一种状态\",{\"1\":{\"956\":1}}],[\"2这些数字\",{\"1\":{\"956\":1}}],[\"2^l\",{\"1\":{\"899\":3}}],[\"2^层数\",{\"1\":{\"892\":1}}],[\"2m\",{\"1\":{\"823\":1}}],[\"2+1\",{\"1\":{\"809\":1}}],[\"2+n\",{\"1\":{\"293\":1}}],[\"2l\",{\"1\":{\"709\":1}}],[\"2和wikitext\",{\"1\":{\"696\":1}}],[\"2任务上表现略优\",{\"1\":{\"683\":1}}],[\"2论文\",{\"0\":{\"673\":1}}],[\"2tb\",{\"1\":{\"666\":1}}],[\"2tuple\",{\"1\":{\"266\":2,\"380\":2}}],[\"2分\",{\"1\":{\"648\":1}}],[\"2通过示例提示\",{\"1\":{\"641\":1}}],[\"2通过训练一个包含45百万网页链接的webtext数据集\",{\"1\":{\"638\":1}}],[\"2仅通过文档+历史对话+\",{\"1\":{\"641\":1}}],[\"2以70\",{\"1\":{\"641\":1}}],[\"2将\",{\"1\":{\"641\":1}}],[\"2使用字节级bpe\",{\"1\":{\"641\":1}}],[\"2在非分布数据\",{\"1\":{\"641\":1}}],[\"2在验证集\",{\"1\":{\"641\":1}}],[\"2在7\",{\"1\":{\"641\":1}}],[\"2在8个标准语言建模数据集上进行了测试\",{\"1\":{\"641\":1}}],[\"2在零样本设置下能完成多种任务\",{\"1\":{\"639\":1}}],[\"2在生成连贯文本方面的能力\",{\"1\":{\"638\":1}}],[\"2模型证明了大规模语言模型在无监督多任务学习中的强大潜力\",{\"1\":{\"643\":1}}],[\"2模型\",{\"1\":{\"638\":1,\"641\":1}}],[\"2取得91\",{\"1\":{\"634\":1}}],[\"2是二分类\",{\"1\":{\"634\":1}}],[\"2相比prompt\",{\"1\":{\"618\":1}}],[\"2x3\",{\"1\":{\"544\":1}}],[\"2x5\",{\"1\":{\"486\":1}}],[\"2×2\",{\"1\":{\"501\":2,\"502\":2,\"505\":1}}],[\"2×224²+10×96²\",{\"1\":{\"291\":1}}],[\"2×224²\",{\"1\":{\"291\":1}}],[\"2×1000000000\",{\"1\":{\"432\":1}}],[\"2的微调潜力\",{\"1\":{\"642\":1}}],[\"2的完全抽象式输出\",{\"1\":{\"642\":1}}],[\"2的局限性\",{\"1\":{\"642\":1}}],[\"2的困惑度\",{\"1\":{\"641\":1}}],[\"2的起因\",{\"1\":{\"415\":1}}],[\"2的强健基础进一步预训练\",{\"1\":{\"330\":1}}],[\"2f\",{\"1\":{\"410\":2,\"412\":2}}],[\"2训练时使用的webtext数据集相似\",{\"1\":{\"407\":1}}],[\"2版本中对internvit\",{\"1\":{\"330\":1}}],[\"2k\",{\"1\":{\"316\":1}}],[\"2n\",{\"1\":{\"207\":3}}],[\"2b\",{\"1\":{\"190\":4,\"192\":4,\"432\":1}}],[\"2节中使用的prompt\",{\"1\":{\"187\":1}}],[\"2组\",{\"1\":{\"119\":1}}],[\"27\",{\"1\":{\"110\":1,\"811\":1,\"815\":1}}],[\"2>\",{\"1\":{\"107\":1}}],[\"2️⃣\",{\"0\":{\"97\":1},\"1\":{\"145\":1,\"963\":1}}],[\"22的实现\",{\"1\":{\"809\":1}}],[\"22b\",{\"1\":{\"308\":1}}],[\"22b除外\",{\"1\":{\"298\":1}}],[\"225\",{\"1\":{\"293\":1,\"359\":1,\"582\":1}}],[\"229\",{\"1\":{\"293\":1,\"359\":1,\"582\":1}}],[\"22\",{\"0\":{\"809\":1},\"1\":{\"73\":1,\"542\":1,\"712\":1,\"739\":1}}],[\"224×224\",{\"1\":{\"305\":1,\"315\":1,\"316\":1,\"435\":1}}],[\"224x224\",{\"1\":{\"293\":2,\"425\":3,\"426\":1,\"435\":1}}],[\"224\",{\"1\":{\"53\":2,\"82\":2,\"212\":2,\"213\":1,\"236\":2,\"266\":3,\"293\":6,\"359\":2,\"425\":2,\"426\":2,\"435\":6,\"510\":2,\"582\":1}}],[\"2c\",{\"1\":{\"69\":1,\"83\":3}}],[\"20b\",{\"1\":{\"329\":1,\"330\":1,\"334\":1}}],[\"20k\",{\"1\":{\"316\":1}}],[\"2015\",{\"1\":{\"656\":1}}],[\"2018将人类偏好学习应用于模仿学习\",{\"1\":{\"655\":1}}],[\"2018\",{\"0\":{\"709\":1},\"1\":{\"640\":2,\"641\":1,\"655\":2,\"671\":2,\"710\":1,\"712\":1,\"822\":1,\"823\":1,\"889\":1}}],[\"2016\",{\"1\":{\"500\":2,\"655\":1,\"827\":1}}],[\"2016年的工作\",{\"1\":{\"413\":1}}],[\"2010\",{\"1\":{\"435\":1}}],[\"2017年的工作\",{\"1\":{\"413\":1}}],[\"2017\",{\"1\":{\"176\":1,\"216\":1,\"640\":1,\"655\":1,\"671\":1}}],[\"2019b\",{\"1\":{\"658\":1}}],[\"2019\",{\"1\":{\"171\":1,\"176\":1,\"216\":2,\"655\":3,\"656\":1,\"671\":1,\"823\":2}}],[\"2014\",{\"1\":{\"73\":1}}],[\"200k\",{\"1\":{\"823\":5}}],[\"2003\",{\"1\":{\"822\":1}}],[\"2000\",{\"1\":{\"236\":1,\"633\":1}}],[\"200\",{\"1\":{\"119\":8,\"809\":1,\"884\":1}}],[\"20\",{\"1\":{\"99\":1,\"106\":3,\"176\":1,\"190\":2,\"293\":2,\"369\":1,\"504\":5,\"542\":1,\"589\":1,\"648\":1,\"698\":4,\"822\":1,\"935\":2,\"939\":1,\"950\":1}}],[\"2025\",{\"1\":{\"823\":9}}],[\"2022的协议\",{\"1\":{\"669\":1}}],[\"2022年清华提出的\",{\"1\":{\"610\":1}}],[\"2022\",{\"1\":{\"214\":1,\"216\":4,\"666\":1,\"667\":1,\"671\":5,\"823\":2,\"831\":1}}],[\"2020\",{\"1\":{\"171\":1,\"176\":1,\"216\":1,\"655\":4,\"656\":1,\"671\":3}}],[\"2021年微软提出的\",{\"1\":{\"610\":1}}],[\"2021a\",{\"1\":{\"171\":1,\"172\":2}}],[\"2021\",{\"1\":{\"97\":1,\"171\":2,\"176\":2,\"216\":5,\"405\":2,\"655\":8,\"658\":1,\"667\":1,\"823\":1,\"826\":1}}],[\"2024年\",{\"1\":{\"823\":1}}],[\"2024\",{\"1\":{\"46\":1,\"84\":1,\"822\":1,\"823\":19,\"833\":1}}],[\"2023\",{\"1\":{\"46\":1,\"823\":17}}],[\"2048\",{\"1\":{\"42\":1,\"53\":2,\"54\":1,\"59\":7,\"64\":3,\"70\":2,\"83\":2,\"91\":1,\"92\":2,\"94\":1,\"105\":2,\"107\":1,\"224\":3}}],[\"290\",{\"1\":{\"47\":1}}],[\"29\",{\"1\":{\"46\":1,\"48\":1,\"107\":1,\"656\":1,\"657\":1,\"658\":1,\"823\":1}}],[\"2501\",{\"1\":{\"713\":2}}],[\"2504\",{\"1\":{\"61\":1}}],[\"25e\",{\"1\":{\"633\":1}}],[\"25~0\",{\"1\":{\"589\":1}}],[\"2578\",{\"1\":{\"713\":2}}],[\"257\",{\"1\":{\"421\":3,\"640\":1}}],[\"255\",{\"1\":{\"107\":2,\"425\":2,\"926\":4}}],[\"256×256\",{\"1\":{\"885\":1}}],[\"256x256\",{\"1\":{\"255\":1,\"892\":1}}],[\"256维\",{\"1\":{\"199\":1}}],[\"256\",{\"1\":{\"60\":3,\"123\":2,\"138\":5,\"141\":6,\"145\":1,\"146\":11,\"152\":3,\"155\":3,\"156\":3,\"191\":1,\"255\":1,\"272\":1,\"274\":1,\"278\":1,\"293\":1,\"356\":2,\"425\":2,\"679\":1,\"885\":1,\"887\":3,\"892\":1,\"899\":1,\"900\":2,\"918\":4,\"921\":2,\"925\":2,\"926\":9}}],[\"25\",{\"1\":{\"46\":1,\"102\":1,\"178\":1,\"179\":1,\"303\":1,\"384\":2,\"440\":1,\"513\":1,\"589\":1,\"918\":1,\"963\":1}}],[\"2693\",{\"1\":{\"944\":1}}],[\"260亿参数\",{\"1\":{\"334\":1}}],[\"26\",{\"1\":{\"39\":1,\"668\":1,\"811\":2}}],[\"28x28\",{\"1\":{\"963\":2}}],[\"287\",{\"1\":{\"315\":1}}],[\"2880\",{\"1\":{\"176\":1}}],[\"28\",{\"1\":{\"32\":1,\"926\":5,\"935\":4,\"939\":2}}],[\"21843\",{\"1\":{\"435\":1}}],[\"21k\",{\"1\":{\"224\":1,\"435\":3,\"510\":1}}],[\"21000\",{\"1\":{\"435\":1}}],[\"2100\",{\"1\":{\"224\":1}}],[\"215\",{\"1\":{\"89\":1}}],[\"213\",{\"1\":{\"41\":1}}],[\"21\",{\"1\":{\"31\":1,\"272\":1,\"542\":1,\"668\":1,\"950\":1}}],[\"235b\",{\"1\":{\"823\":1}}],[\"2301\",{\"1\":{\"414\":1}}],[\"2304\",{\"1\":{\"338\":1,\"899\":1}}],[\"2303\",{\"1\":{\"71\":1}}],[\"2312\",{\"1\":{\"294\":1}}],[\"23\",{\"1\":{\"25\":1,\"89\":1,\"91\":1,\"107\":1,\"273\":1,\"342\":1,\"369\":1,\"376\":1,\"542\":3,\"656\":1,\"668\":1}}],[\"244\",{\"1\":{\"426\":1}}],[\"242\",{\"1\":{\"341\":1}}],[\"2400\",{\"1\":{\"176\":1}}],[\"24类\",{\"1\":{\"41\":1}}],[\"24类功能\",{\"1\":{\"30\":1}}],[\"2411\",{\"1\":{\"28\":1}}],[\"24\",{\"1\":{\"11\":1,\"53\":1,\"58\":3,\"542\":6,\"641\":1}}],[\"2\",{\"0\":{\"23\":1,\"33\":1,\"47\":1,\"65\":1,\"87\":1,\"247\":1,\"398\":1,\"419\":1,\"421\":1,\"426\":1,\"527\":1,\"532\":1,\"637\":1,\"838\":1,\"907\":1,\"913\":1,\"931\":1,\"937\":1},\"1\":{\"11\":1,\"25\":1,\"31\":1,\"32\":1,\"52\":4,\"53\":5,\"54\":2,\"56\":6,\"58\":3,\"59\":10,\"60\":8,\"64\":1,\"65\":5,\"69\":9,\"70\":4,\"82\":1,\"83\":31,\"89\":1,\"94\":17,\"97\":2,\"100\":3,\"102\":5,\"104\":1,\"106\":4,\"107\":4,\"119\":16,\"121\":4,\"122\":4,\"123\":11,\"137\":8,\"138\":1,\"141\":7,\"144\":2,\"145\":6,\"146\":2,\"150\":1,\"152\":2,\"153\":2,\"154\":6,\"156\":4,\"157\":2,\"190\":6,\"191\":1,\"192\":9,\"202\":1,\"205\":2,\"206\":2,\"207\":3,\"208\":3,\"212\":1,\"213\":5,\"215\":1,\"220\":1,\"228\":1,\"235\":2,\"249\":1,\"263\":1,\"265\":1,\"266\":2,\"268\":1,\"271\":1,\"274\":3,\"280\":1,\"292\":1,\"293\":18,\"300\":1,\"305\":3,\"306\":2,\"309\":2,\"311\":1,\"315\":1,\"319\":1,\"330\":3,\"334\":1,\"335\":1,\"336\":1,\"359\":5,\"361\":1,\"362\":1,\"364\":1,\"374\":1,\"380\":15,\"381\":1,\"383\":1,\"384\":2,\"385\":5,\"394\":1,\"397\":2,\"399\":1,\"401\":3,\"403\":3,\"407\":1,\"408\":1,\"415\":1,\"416\":2,\"418\":2,\"419\":5,\"420\":5,\"421\":2,\"424\":2,\"426\":3,\"428\":1,\"430\":4,\"441\":5,\"444\":1,\"462\":2,\"463\":2,\"468\":1,\"469\":3,\"471\":9,\"472\":10,\"477\":16,\"478\":2,\"480\":15,\"481\":3,\"482\":4,\"483\":1,\"484\":1,\"485\":3,\"486\":1,\"490\":3,\"491\":1,\"500\":2,\"502\":5,\"514\":10,\"516\":12,\"540\":2,\"541\":2,\"542\":13,\"544\":11,\"545\":10,\"546\":3,\"547\":1,\"564\":1,\"565\":1,\"570\":1,\"582\":7,\"586\":1,\"587\":2,\"589\":4,\"592\":1,\"595\":8,\"596\":9,\"613\":1,\"631\":2,\"633\":1,\"634\":2,\"635\":1,\"637\":1,\"640\":8,\"641\":5,\"646\":2,\"647\":12,\"648\":3,\"649\":1,\"650\":3,\"655\":1,\"656\":5,\"657\":1,\"660\":1,\"663\":10,\"667\":3,\"668\":5,\"669\":1,\"670\":1,\"673\":2,\"678\":3,\"679\":1,\"680\":2,\"681\":1,\"683\":1,\"684\":2,\"685\":3,\"690\":1,\"696\":10,\"697\":1,\"698\":2,\"699\":6,\"700\":1,\"703\":1,\"706\":3,\"709\":28,\"710\":41,\"716\":1,\"718\":1,\"721\":1,\"722\":1,\"724\":4,\"728\":1,\"730\":1,\"731\":2,\"733\":2,\"734\":3,\"746\":1,\"749\":1,\"751\":5,\"757\":2,\"762\":1,\"771\":1,\"780\":1,\"794\":1,\"804\":1,\"807\":2,\"808\":11,\"809\":7,\"811\":18,\"815\":12,\"816\":18,\"822\":3,\"823\":19,\"847\":4,\"871\":2,\"875\":1,\"877\":1,\"878\":1,\"881\":2,\"885\":1,\"886\":1,\"887\":1,\"888\":1,\"889\":1,\"892\":3,\"899\":5,\"900\":4,\"917\":2,\"918\":4,\"926\":22,\"932\":2,\"935\":2,\"939\":1,\"944\":1,\"963\":6,\"964\":5}}],[\"2d功能检测\",{\"1\":{\"75\":1}}],[\"2d演示与3d物体来自不同实例\",{\"1\":{\"73\":1}}],[\"2d方法难以直接迁移\",{\"1\":{\"31\":1}}],[\"2d\",{\"0\":{\"22\":1},\"1\":{\"6\":1,\"14\":2,\"19\":9,\"20\":6,\"21\":1,\"22\":8,\"23\":5,\"24\":2,\"26\":8,\"40\":1,\"46\":2,\"53\":1,\"64\":1,\"66\":1,\"71\":1,\"83\":2,\"110\":2,\"159\":2,\"426\":1,\"467\":1,\"493\":2,\"503\":1,\"582\":2}}],[\"1×1\",{\"1\":{\"886\":1}}],[\"1×3虚拟扩展为\",{\"1\":{\"546\":1}}],[\"1×3\",{\"1\":{\"546\":1}}],[\"1=none\",{\"1\":{\"713\":3}}],[\"1m\",{\"1\":{\"679\":1,\"823\":5}}],[\"1分\",{\"1\":{\"670\":1}}],[\"1论文\",{\"0\":{\"664\":1}}],[\"1bw\",{\"1\":{\"641\":1}}],[\"1b\",{\"1\":{\"633\":1}}],[\"1b参数\",{\"1\":{\"303\":1}}],[\"1x4\",{\"1\":{\"544\":1}}],[\"1x1\",{\"1\":{\"97\":1,\"926\":1}}],[\"1rkdjdlr37o7gsr9j1mhjbg\",{\"1\":{\"435\":1}}],[\"1标志位表示visual\",{\"1\":{\"392\":1}}],[\"1+k\",{\"1\":{\"362\":1}}],[\"1~12个448×448图块\",{\"1\":{\"334\":1}}],[\"1~12个448×448区块\",{\"1\":{\"329\":1}}],[\"1~2\",{\"1\":{\"215\":1,\"960\":1}}],[\"1v\",{\"1\":{\"325\":1}}],[\"1亿参数\",{\"1\":{\"304\":1}}],[\"1准确率\",{\"1\":{\"291\":1}}],[\"1表示遮挡\",{\"1\":{\"263\":1}}],[\"1表示输出维度与输入维度相同\",{\"1\":{\"120\":1}}],[\"1k\",{\"1\":{\"215\":1,\"236\":1,\"240\":1,\"308\":1,\"510\":1}}],[\"14×32\",{\"1\":{\"895\":1}}],[\"14×14\",{\"1\":{\"501\":2}}],[\"1478093658716966912\",{\"1\":{\"894\":1}}],[\"14b\",{\"1\":{\"823\":1}}],[\"141\",{\"1\":{\"668\":1,\"670\":1}}],[\"14模型\",{\"1\":{\"407\":1}}],[\"14则需要在256个v100\",{\"1\":{\"407\":1}}],[\"14238\",{\"1\":{\"294\":1}}],[\"14x14\",{\"1\":{\"263\":2,\"963\":4}}],[\"1408\",{\"1\":{\"224\":1}}],[\"1400\",{\"1\":{\"176\":1,\"224\":1}}],[\"14\",{\"1\":{\"206\":1,\"212\":2,\"236\":2,\"263\":1,\"266\":2,\"341\":1,\"342\":1,\"385\":1,\"407\":2,\"410\":1,\"426\":2,\"428\":2,\"482\":1,\"542\":1,\"641\":1,\"660\":1,\"799\":1,\"811\":2,\"815\":2,\"819\":1,\"895\":1,\"960\":1}}],[\"1节中的跨模态解码器\",{\"1\":{\"187\":1}}],[\"1节中提到的多模态融合模块配置\",{\"1\":{\"187\":1}}],[\"1节实验设置中使用384x384\",{\"1\":{\"187\":1}}],[\"137mo\",{\"1\":{\"424\":1}}],[\"1317\",{\"1\":{\"311\":1}}],[\"13b模型在单v100\",{\"1\":{\"668\":1}}],[\"13b性能优于gpt\",{\"1\":{\"668\":1}}],[\"13b在多数基准测试中优于gpt\",{\"1\":{\"665\":1}}],[\"13b\",{\"1\":{\"310\":2,\"311\":1}}],[\"13\",{\"1\":{\"182\":1,\"190\":4,\"206\":1,\"273\":1,\"384\":1,\"385\":1,\"482\":1,\"542\":1,\"660\":1,\"666\":1}}],[\"1d\",{\"1\":{\"152\":1,\"233\":1,\"265\":1,\"480\":1}}],[\"1个0\",{\"1\":{\"355\":1}}],[\"1个1\",{\"1\":{\"355\":1}}],[\"1个图片经过编码器2得到的表征都是负样本\",{\"1\":{\"355\":1}}],[\"1个邻居\",{\"1\":{\"119\":1}}],[\"1个点\",{\"1\":{\"119\":1}}],[\"1️⃣\",{\"0\":{\"96\":1},\"1\":{\"145\":1,\"963\":1}}],[\"1f242tsdxjrzkkqotibsin2u6rjagrz2w\",{\"1\":{\"71\":1}}],[\"119\",{\"1\":{\"823\":1}}],[\"11929\",{\"1\":{\"435\":1}}],[\"117m\",{\"1\":{\"640\":1,\"641\":1}}],[\"110\",{\"1\":{\"99\":1}}],[\"11\",{\"1\":{\"64\":1,\"106\":1,\"110\":1,\"206\":1,\"344\":1,\"377\":1,\"384\":1,\"385\":2,\"482\":1,\"542\":1,\"641\":1,\"660\":1,\"712\":1,\"739\":2,\"823\":4}}],[\"12597\",{\"1\":{\"414\":1}}],[\"12m\",{\"1\":{\"176\":1,\"224\":1}}],[\"1200000000\",{\"1\":{\"432\":1}}],[\"120\",{\"1\":{\"89\":1,\"236\":1,\"268\":1,\"887\":1,\"888\":1}}],[\"128k\",{\"1\":{\"823\":9}}],[\"128维特征\",{\"1\":{\"121\":1}}],[\"128\",{\"1\":{\"59\":2,\"83\":1,\"94\":1,\"97\":1,\"123\":2,\"138\":5,\"141\":10,\"143\":4,\"144\":4,\"145\":1,\"146\":12,\"152\":4,\"154\":3,\"156\":3,\"292\":2,\"309\":1,\"357\":2,\"361\":2,\"362\":1,\"706\":1,\"733\":1,\"918\":2}}],[\"127\",{\"1\":{\"47\":1}}],[\"12\",{\"1\":{\"47\":1,\"48\":1,\"106\":1,\"181\":1,\"197\":1,\"206\":1,\"236\":2,\"273\":1,\"384\":1,\"385\":2,\"478\":1,\"482\":1,\"490\":2,\"502\":1,\"542\":4,\"633\":2,\"660\":1,\"706\":1,\"712\":6,\"811\":1,\"815\":1,\"823\":3,\"888\":1}}],[\"1e9\",{\"1\":{\"703\":1,\"751\":1}}],[\"1e10\",{\"1\":{\"137\":2}}],[\"1e\",{\"1\":{\"46\":1,\"102\":7,\"107\":1,\"119\":1,\"122\":1,\"145\":1,\"192\":2,\"256\":1,\"260\":1,\"315\":2,\"431\":1,\"680\":1,\"700\":1}}],[\"15的概率mask掉tokens\",{\"1\":{\"393\":1}}],[\"158\",{\"1\":{\"342\":1}}],[\"158k\",{\"1\":{\"342\":2}}],[\"1586\",{\"1\":{\"310\":1}}],[\"1500\",{\"1\":{\"224\":1}}],[\"15\",{\"1\":{\"41\":1,\"87\":1,\"92\":1,\"107\":1,\"176\":1,\"200\":1,\"206\":1,\"208\":1,\"223\":1,\"344\":1,\"373\":1,\"385\":1,\"482\":1,\"542\":1,\"660\":1,\"666\":1,\"667\":1,\"668\":1,\"679\":1,\"691\":1,\"822\":1,\"823\":3,\"935\":2,\"939\":1}}],[\"15k交互图像\",{\"1\":{\"30\":1}}],[\"15k\",{\"1\":{\"11\":1}}],[\"18\",{\"1\":{\"41\":1,\"322\":1,\"369\":1,\"542\":1,\"633\":1,\"811\":1,\"815\":1,\"950\":1}}],[\"180\",{\"1\":{\"7\":1,\"288\":1,\"574\":1}}],[\"16k\",{\"1\":{\"823\":1}}],[\"16gb\",{\"1\":{\"679\":1,\"680\":2,\"684\":1}}],[\"16这个模型进行微调\",{\"1\":{\"435\":1}}],[\"16x16\",{\"1\":{\"425\":1}}],[\"16为例\",{\"1\":{\"425\":1,\"426\":1}}],[\"16和vit\",{\"1\":{\"407\":1}}],[\"16倍和64倍得到的\",{\"1\":{\"407\":1}}],[\"160\",{\"1\":{\"316\":1,\"574\":1}}],[\"160gb\",{\"1\":{\"224\":1,\"679\":1,\"680\":2}}],[\"164k\",{\"1\":{\"315\":1}}],[\"16×16\",{\"1\":{\"236\":1,\"435\":1}}],[\"16模型\",{\"1\":{\"213\":1}}],[\"1612\",{\"1\":{\"147\":1}}],[\"16维权重\",{\"1\":{\"119\":1}}],[\"162张图像\",{\"1\":{\"73\":1}}],[\"16\",{\"1\":{\"34\":1,\"46\":1,\"83\":8,\"89\":1,\"97\":1,\"107\":1,\"123\":5,\"141\":1,\"146\":1,\"176\":2,\"197\":1,\"206\":1,\"212\":2,\"213\":1,\"224\":1,\"234\":1,\"236\":1,\"269\":1,\"273\":1,\"286\":2,\"288\":1,\"291\":1,\"369\":1,\"425\":2,\"426\":3,\"435\":1,\"482\":1,\"501\":1,\"502\":3,\"542\":1,\"656\":1,\"660\":1,\"668\":1,\"833\":1,\"887\":1,\"950\":1}}],[\"101\",{\"1\":{\"713\":2}}],[\"104\",{\"1\":{\"713\":1}}],[\"10437\",{\"1\":{\"71\":1}}],[\"10份\",{\"1\":{\"681\":1}}],[\"103是超过\",{\"1\":{\"696\":1}}],[\"103两个版本\",{\"1\":{\"696\":1}}],[\"103\",{\"1\":{\"696\":5,\"697\":1,\"713\":1}}],[\"103测试集1\",{\"1\":{\"641\":1}}],[\"1035\",{\"1\":{\"89\":1}}],[\"102\",{\"1\":{\"713\":2}}],[\"1022\",{\"1\":{\"311\":1}}],[\"1024+64\",{\"1\":{\"154\":1}}],[\"1024维\",{\"1\":{\"152\":1,\"154\":1,\"155\":2}}],[\"1024\",{\"1\":{\"58\":1,\"60\":1,\"138\":3,\"141\":3,\"144\":2,\"145\":1,\"146\":3,\"152\":9,\"154\":7,\"155\":1,\"156\":2,\"286\":1,\"292\":1,\"318\":1,\"319\":1,\"707\":1,\"885\":1,\"887\":1,\"918\":2}}],[\"10×96²\",{\"1\":{\"291\":1}}],[\"100个任务示例作为上下文\",{\"1\":{\"647\":1}}],[\"100占位\",{\"1\":{\"384\":1}}],[\"100万\",{\"1\":{\"355\":1}}],[\"100万个图片\",{\"1\":{\"355\":1}}],[\"1000\",{\"1\":{\"589\":1,\"816\":1}}],[\"10000\",{\"1\":{\"224\":1,\"419\":3,\"721\":1,\"892\":1,\"900\":1}}],[\"1000个点\",{\"1\":{\"121\":1,\"122\":1}}],[\"100\",{\"1\":{\"187\":2,\"190\":1,\"192\":1,\"204\":1,\"208\":8,\"224\":1,\"293\":2,\"341\":2,\"354\":1,\"382\":1,\"384\":4,\"385\":2,\"408\":1,\"410\":1,\"412\":1,\"420\":2,\"562\":2,\"563\":1,\"570\":1,\"574\":2,\"578\":1,\"589\":1,\"633\":1,\"647\":1,\"700\":1,\"713\":2,\"807\":6,\"816\":3,\"823\":2,\"935\":1}}],[\"1088\",{\"1\":{\"154\":2,\"156\":1}}],[\"10\",{\"0\":{\"70\":1},\"1\":{\"34\":2,\"64\":1,\"87\":1,\"106\":2,\"206\":1,\"208\":2,\"213\":2,\"224\":1,\"236\":1,\"256\":1,\"260\":1,\"262\":1,\"263\":1,\"272\":1,\"286\":1,\"293\":1,\"305\":1,\"306\":1,\"309\":1,\"318\":1,\"319\":1,\"331\":1,\"354\":1,\"384\":2,\"385\":2,\"448\":2,\"481\":1,\"482\":1,\"484\":1,\"504\":5,\"513\":3,\"542\":1,\"660\":1,\"668\":1,\"679\":3,\"681\":3,\"691\":2,\"697\":4,\"698\":2,\"751\":3,\"814\":1,\"823\":9,\"926\":2,\"934\":1,\"938\":1,\"939\":4,\"951\":1,\"963\":1,\"964\":1}}],[\"192\",{\"1\":{\"885\":1}}],[\"198\",{\"1\":{\"660\":1}}],[\"1980\",{\"1\":{\"30\":1}}],[\"1930\",{\"1\":{\"658\":1}}],[\"197\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"196+1\",{\"1\":{\"428\":1}}],[\"196\",{\"1\":{\"426\":2,\"427\":1,\"428\":1,\"431\":1}}],[\"196×196\",{\"1\":{\"305\":1,\"315\":1}}],[\"19626\",{\"1\":{\"28\":1}}],[\"190\",{\"1\":{\"107\":3}}],[\"19\",{\"1\":{\"89\":1,\"91\":1,\"93\":1,\"178\":1,\"224\":1,\"542\":1,\"811\":1,\"815\":1,\"950\":1}}],[\"1asow2t2mltykaia\",{\"1\":{\"28\":1}}],[\"1n\",{\"1\":{\"28\":1}}],[\"1750\",{\"1\":{\"822\":1,\"823\":1}}],[\"1750亿参数\",{\"1\":{\"646\":1}}],[\"175b的69\",{\"1\":{\"670\":1}}],[\"175b\",{\"1\":{\"655\":1,\"656\":1,\"657\":2,\"658\":2,\"665\":1,\"668\":2,\"822\":1}}],[\"175b也只能在few\",{\"1\":{\"648\":1}}],[\"175k\",{\"1\":{\"315\":1}}],[\"17个任务\",{\"1\":{\"639\":1}}],[\"170\",{\"1\":{\"574\":1}}],[\"1706\",{\"1\":{\"130\":1}}],[\"174\",{\"1\":{\"89\":1}}],[\"17\",{\"1\":{\"25\":1,\"89\":1,\"91\":1,\"99\":1,\"206\":1,\"224\":1,\"269\":1,\"542\":1,\"950\":1}}],[\"1\",{\"0\":{\"22\":1,\"46\":1,\"86\":1,\"246\":1,\"294\":1,\"321\":1,\"338\":1,\"397\":1,\"417\":1,\"418\":1,\"425\":1,\"526\":1,\"531\":1,\"624\":1,\"821\":1,\"906\":1,\"912\":1,\"917\":1,\"930\":1},\"1\":{\"11\":1,\"31\":1,\"46\":1,\"48\":1,\"50\":1,\"52\":5,\"53\":6,\"54\":5,\"56\":8,\"58\":4,\"59\":37,\"60\":28,\"64\":4,\"65\":10,\"67\":3,\"69\":14,\"70\":16,\"82\":2,\"83\":52,\"88\":1,\"89\":1,\"92\":1,\"94\":29,\"97\":2,\"99\":2,\"100\":14,\"102\":13,\"104\":1,\"105\":2,\"106\":12,\"107\":10,\"117\":1,\"119\":39,\"120\":1,\"121\":14,\"122\":12,\"123\":23,\"137\":34,\"138\":3,\"141\":9,\"143\":2,\"144\":2,\"145\":17,\"146\":4,\"150\":1,\"152\":12,\"153\":4,\"154\":13,\"156\":8,\"157\":1,\"172\":1,\"176\":2,\"187\":2,\"188\":4,\"190\":30,\"191\":3,\"192\":14,\"199\":1,\"200\":1,\"204\":1,\"205\":2,\"206\":10,\"207\":5,\"208\":8,\"213\":21,\"220\":1,\"224\":1,\"228\":1,\"229\":1,\"234\":2,\"236\":2,\"255\":1,\"256\":2,\"257\":1,\"260\":2,\"263\":5,\"264\":6,\"265\":4,\"266\":14,\"268\":2,\"274\":17,\"280\":5,\"285\":1,\"291\":1,\"293\":21,\"294\":1,\"305\":2,\"306\":1,\"309\":1,\"310\":1,\"315\":1,\"317\":2,\"319\":1,\"322\":3,\"323\":2,\"325\":1,\"327\":1,\"329\":1,\"330\":2,\"332\":1,\"334\":3,\"335\":1,\"336\":1,\"337\":1,\"338\":1,\"344\":2,\"355\":1,\"359\":5,\"361\":2,\"362\":2,\"363\":1,\"364\":1,\"370\":1,\"380\":32,\"381\":1,\"382\":1,\"383\":1,\"384\":6,\"385\":13,\"386\":6,\"394\":1,\"397\":2,\"399\":1,\"401\":3,\"403\":12,\"405\":1,\"408\":5,\"410\":4,\"412\":4,\"416\":1,\"417\":7,\"418\":13,\"419\":15,\"420\":20,\"421\":7,\"424\":2,\"425\":2,\"426\":5,\"427\":10,\"428\":10,\"430\":16,\"431\":10,\"432\":2,\"435\":2,\"440\":3,\"441\":5,\"444\":1,\"456\":2,\"463\":4,\"469\":2,\"471\":8,\"472\":19,\"474\":1,\"477\":20,\"478\":2,\"479\":1,\"480\":20,\"481\":4,\"482\":3,\"483\":2,\"484\":1,\"485\":12,\"487\":1,\"488\":1,\"489\":5,\"490\":3,\"492\":2,\"500\":3,\"501\":1,\"502\":2,\"506\":2,\"507\":2,\"508\":4,\"510\":1,\"513\":10,\"514\":10,\"516\":6,\"518\":1,\"520\":2,\"540\":2,\"541\":4,\"542\":14,\"544\":9,\"545\":17,\"546\":12,\"562\":2,\"563\":1,\"564\":1,\"565\":2,\"567\":2,\"569\":3,\"570\":5,\"572\":1,\"578\":2,\"582\":14,\"586\":7,\"587\":7,\"588\":10,\"589\":11,\"590\":7,\"592\":12,\"595\":18,\"596\":11,\"597\":12,\"618\":1,\"624\":1,\"630\":1,\"631\":1,\"633\":2,\"640\":5,\"641\":8,\"646\":1,\"647\":5,\"648\":2,\"649\":1,\"650\":1,\"655\":3,\"656\":4,\"657\":3,\"658\":3,\"660\":7,\"663\":19,\"664\":1,\"667\":3,\"668\":4,\"669\":2,\"670\":2,\"678\":1,\"679\":1,\"680\":4,\"681\":1,\"685\":1,\"690\":1,\"691\":1,\"692\":9,\"694\":1,\"696\":2,\"697\":1,\"698\":9,\"699\":5,\"700\":8,\"703\":4,\"706\":1,\"709\":36,\"710\":61,\"713\":26,\"715\":1,\"716\":2,\"721\":2,\"722\":6,\"724\":5,\"731\":5,\"733\":6,\"734\":5,\"735\":2,\"736\":6,\"737\":15,\"739\":1,\"743\":1,\"746\":2,\"749\":2,\"751\":15,\"752\":1,\"757\":2,\"766\":1,\"775\":4,\"781\":1,\"795\":1,\"798\":1,\"800\":1,\"805\":2,\"807\":4,\"808\":8,\"809\":9,\"811\":10,\"813\":1,\"815\":12,\"816\":13,\"823\":18,\"847\":4,\"848\":1,\"850\":1,\"857\":1,\"865\":1,\"871\":1,\"877\":2,\"881\":3,\"885\":2,\"887\":1,\"892\":41,\"893\":21,\"894\":4,\"895\":9,\"896\":4,\"897\":2,\"898\":7,\"899\":20,\"900\":7,\"904\":1,\"907\":1,\"917\":5,\"918\":6,\"921\":3,\"923\":1,\"925\":3,\"926\":32,\"932\":3,\"934\":1,\"935\":9,\"938\":1,\"939\":3,\"945\":1,\"946\":1,\"956\":1,\"959\":1,\"963\":6,\"964\":11}}],[\"=原图尺寸\",{\"1\":{\"899\":1}}],[\"=w\",{\"1\":{\"899\":1}}],[\"=50000\",{\"1\":{\"816\":1}}],[\"=10000\",{\"1\":{\"816\":1}}],[\"=>\",{\"1\":{\"751\":1}}],[\"=self\",{\"1\":{\"54\":1}}],[\"======\",{\"1\":{\"380\":4,\"385\":34}}],[\"==============\",{\"1\":{\"418\":1,\"419\":1}}],[\"===============\",{\"1\":{\"205\":4}}],[\"===================\",{\"1\":{\"418\":1,\"419\":1}}],[\"=====================\",{\"1\":{\"192\":12,\"293\":14}}],[\"=============================\",{\"1\":{\"963\":8}}],[\"=================================\",{\"1\":{\"207\":3}}],[\"========================\",{\"1\":{\"208\":1,\"420\":1}}],[\"=======================\",{\"1\":{\"192\":6}}],[\"====================\",{\"1\":{\"121\":4,\"122\":4}}],[\"=================\",{\"1\":{\"123\":6,\"208\":1,\"420\":1}}],[\"==========\",{\"1\":{\"123\":10,\"206\":4,\"213\":4,\"385\":4}}],[\"=====\",{\"1\":{\"208\":4,\"709\":6}}],[\"==\",{\"1\":{\"53\":5,\"64\":1,\"82\":4,\"92\":3,\"106\":1,\"107\":2,\"119\":2,\"121\":1,\"122\":1,\"123\":1,\"137\":1,\"145\":1,\"187\":1,\"192\":1,\"208\":2,\"213\":8,\"263\":2,\"264\":2,\"293\":2,\"364\":1,\"380\":4,\"382\":5,\"383\":1,\"399\":1,\"410\":1,\"412\":1,\"419\":1,\"420\":2,\"489\":1,\"491\":1,\"589\":1,\"596\":1,\"597\":1,\"660\":1,\"663\":2,\"696\":1,\"697\":2,\"700\":1,\"715\":1,\"718\":1,\"722\":1,\"728\":1,\"735\":1,\"736\":1,\"739\":1,\"751\":2,\"893\":3,\"894\":1,\"895\":1,\"898\":1,\"899\":2,\"900\":1,\"918\":1,\"926\":1,\"964\":1}}],[\"=\",{\"1\":{\"11\":4,\"52\":15,\"53\":47,\"54\":7,\"56\":18,\"57\":3,\"58\":9,\"59\":21,\"60\":19,\"64\":20,\"65\":24,\"66\":1,\"67\":7,\"68\":1,\"69\":36,\"70\":18,\"82\":37,\"83\":93,\"87\":1,\"92\":25,\"94\":15,\"96\":4,\"97\":8,\"98\":5,\"99\":4,\"100\":27,\"102\":16,\"104\":15,\"105\":10,\"106\":35,\"107\":45,\"117\":1,\"119\":58,\"120\":24,\"121\":45,\"122\":28,\"123\":32,\"137\":63,\"138\":24,\"141\":53,\"143\":2,\"145\":24,\"146\":26,\"150\":2,\"152\":25,\"153\":5,\"154\":30,\"155\":13,\"156\":20,\"161\":1,\"187\":32,\"188\":12,\"190\":81,\"191\":28,\"192\":77,\"204\":7,\"205\":28,\"206\":23,\"207\":41,\"208\":44,\"213\":115,\"236\":2,\"255\":27,\"256\":20,\"257\":1,\"258\":6,\"260\":5,\"261\":1,\"263\":25,\"264\":7,\"265\":13,\"266\":25,\"274\":39,\"292\":4,\"293\":59,\"359\":12,\"361\":12,\"362\":11,\"363\":1,\"364\":6,\"380\":92,\"382\":61,\"383\":4,\"384\":27,\"385\":71,\"386\":38,\"390\":3,\"397\":8,\"398\":3,\"399\":2,\"400\":7,\"401\":16,\"403\":22,\"407\":9,\"408\":10,\"410\":40,\"411\":9,\"412\":46,\"417\":10,\"418\":8,\"419\":40,\"420\":56,\"421\":10,\"424\":25,\"425\":6,\"426\":11,\"427\":12,\"428\":15,\"429\":19,\"430\":16,\"431\":28,\"432\":2,\"435\":5,\"440\":1,\"441\":7,\"444\":1,\"448\":2,\"449\":2,\"450\":1,\"451\":1,\"452\":1,\"453\":1,\"456\":1,\"457\":2,\"458\":1,\"459\":2,\"460\":4,\"461\":2,\"463\":3,\"467\":1,\"468\":1,\"469\":4,\"470\":1,\"471\":3,\"472\":6,\"474\":1,\"477\":5,\"478\":7,\"480\":6,\"481\":4,\"482\":4,\"483\":1,\"484\":1,\"485\":4,\"486\":2,\"490\":2,\"491\":3,\"492\":3,\"502\":5,\"507\":1,\"508\":3,\"510\":3,\"511\":1,\"513\":3,\"514\":3,\"516\":2,\"518\":3,\"520\":1,\"521\":4,\"522\":14,\"540\":1,\"542\":4,\"544\":2,\"545\":5,\"546\":5,\"574\":1,\"582\":29,\"586\":7,\"587\":12,\"588\":10,\"589\":17,\"590\":16,\"592\":15,\"595\":42,\"596\":19,\"597\":55,\"600\":3,\"640\":2,\"656\":2,\"660\":14,\"663\":60,\"681\":1,\"690\":1,\"696\":11,\"697\":30,\"698\":22,\"699\":21,\"700\":23,\"701\":4,\"703\":6,\"709\":32,\"710\":45,\"713\":14,\"715\":7,\"716\":16,\"718\":17,\"719\":2,\"720\":5,\"721\":10,\"722\":14,\"724\":25,\"725\":6,\"726\":4,\"728\":7,\"729\":5,\"730\":4,\"731\":11,\"733\":9,\"734\":17,\"735\":2,\"736\":16,\"737\":17,\"742\":5,\"743\":1,\"745\":2,\"746\":6,\"747\":3,\"749\":9,\"750\":4,\"751\":15,\"756\":1,\"757\":3,\"762\":3,\"766\":7,\"771\":4,\"775\":1,\"778\":2,\"779\":5,\"780\":2,\"781\":4,\"783\":1,\"784\":3,\"787\":4,\"791\":1,\"792\":1,\"794\":4,\"795\":3,\"800\":6,\"801\":7,\"802\":1,\"803\":11,\"804\":5,\"805\":22,\"806\":12,\"807\":30,\"808\":13,\"809\":44,\"810\":4,\"811\":13,\"815\":28,\"816\":44,\"844\":2,\"846\":1,\"850\":1,\"873\":1,\"876\":2,\"881\":3,\"882\":2,\"885\":1,\"887\":1,\"892\":75,\"893\":36,\"894\":8,\"895\":33,\"896\":4,\"898\":20,\"899\":86,\"900\":69,\"906\":2,\"908\":1,\"909\":3,\"918\":28,\"926\":88,\"931\":14,\"932\":2,\"933\":3,\"934\":8,\"935\":8,\"937\":15,\"938\":8,\"939\":5,\"959\":1,\"963\":44,\"964\":34}}],[\"html\",{\"1\":{\"836\":1}}],[\"https\",{\"1\":{\"28\":3,\"61\":2,\"71\":3,\"84\":2,\"108\":1,\"124\":1,\"128\":1,\"129\":1,\"130\":3,\"147\":3,\"164\":1,\"185\":1,\"193\":1,\"209\":1,\"219\":1,\"226\":1,\"267\":1,\"279\":1,\"294\":2,\"338\":2,\"347\":1,\"366\":1,\"367\":1,\"378\":1,\"385\":1,\"387\":1,\"414\":2,\"424\":2,\"435\":4,\"582\":1,\"591\":1,\"663\":1,\"689\":1,\"696\":2,\"712\":2,\"739\":1,\"752\":1,\"798\":1,\"813\":1,\"818\":1,\"854\":1,\"894\":1,\"899\":1,\"927\":1}}],[\"hh\",{\"1\":{\"595\":1}}],[\"h`和\",{\"1\":{\"426\":1}}],[\"hyperparameter\",{\"1\":{\"355\":1}}],[\"hybrid\",{\"0\":{\"102\":1},\"1\":{\"102\":1,\"434\":1}}],[\"hr\",{\"1\":{\"326\":1}}],[\"hparams\",{\"1\":{\"383\":1,\"384\":1}}],[\"hpt\",{\"1\":{\"325\":1}}],[\"hpein\",{\"1\":{\"110\":1}}],[\"h^2\",{\"1\":{\"263\":1}}],[\"hoffmann\",{\"1\":{\"671\":1}}],[\"honest\",{\"1\":{\"654\":1,\"656\":1}}],[\"houlsby\",{\"1\":{\"610\":1}}],[\"home\",{\"1\":{\"553\":3}}],[\"hook\",{\"1\":{\"420\":1}}],[\"horse\",{\"1\":{\"408\":1}}],[\"how\",{\"1\":{\"321\":2}}],[\"hog\",{\"1\":{\"210\":1}}],[\"hot向量\",{\"1\":{\"257\":1,\"355\":1}}],[\"hot\",{\"1\":{\"190\":3,\"199\":1,\"200\":1,\"201\":1,\"202\":3,\"204\":3,\"213\":2,\"256\":4,\"257\":6,\"258\":7,\"271\":2,\"857\":1,\"899\":11,\"910\":1,\"938\":1,\"939\":1,\"959\":1}}],[\"hold\",{\"1\":{\"107\":2,\"697\":2}}],[\"holyoak\",{\"1\":{\"30\":1}}],[\"hub\",{\"1\":{\"412\":1}}],[\"huber\",{\"1\":{\"259\":1}}],[\"hugging\",{\"1\":{\"382\":1}}],[\"huggingface\",{\"0\":{\"663\":1},\"1\":{\"208\":2,\"412\":1,\"663\":3,\"697\":1}}],[\"hue=0\",{\"1\":{\"293\":1}}],[\"hutter\",{\"1\":{\"176\":1}}],[\"human\",{\"1\":{\"52\":1,\"53\":2,\"54\":2,\"56\":1,\"57\":2,\"339\":1,\"341\":1,\"342\":2,\"602\":1,\"652\":1,\"654\":1,\"655\":1,\"656\":1}}],[\"hw\",{\"1\":{\"65\":1,\"69\":3,\"83\":1,\"899\":1}}],[\"hm\",{\"0\":{\"102\":1},\"1\":{\"64\":6,\"102\":3,\"104\":2,\"105\":3}}],[\"hey\",{\"1\":{\"595\":1}}],[\"helmholtz\",{\"1\":{\"950\":1}}],[\"hellaswag\",{\"1\":{\"656\":1}}],[\"hello\",{\"1\":{\"449\":1,\"451\":7,\"454\":5,\"455\":2,\"595\":1,\"596\":1}}],[\"held\",{\"1\":{\"640\":1,\"656\":2,\"657\":1}}],[\"helpful\",{\"1\":{\"654\":1,\"656\":1}}],[\"help\",{\"1\":{\"454\":1,\"554\":2}}],[\"help=\",{\"1\":{\"293\":3,\"918\":10}}],[\"hermes\",{\"1\":{\"330\":2}}],[\"here\",{\"1\":{\"97\":1,\"722\":1}}],[\"he\",{\"1\":{\"216\":1,\"595\":1}}],[\"heavy\",{\"1\":{\"868\":1}}],[\"heatmap\",{\"1\":{\"64\":1,\"70\":2}}],[\"head2\",{\"1\":{\"710\":1}}],[\"head2根据体重计算相似度\",{\"1\":{\"582\":1}}],[\"head1\",{\"1\":{\"710\":1}}],[\"head1根据身高计算相似度\",{\"1\":{\"582\":1}}],[\"head0\",{\"1\":{\"710\":1}}],[\"head3根据年龄计算相似度\",{\"1\":{\"582\":1}}],[\"head结构由linear+tanh激活函数+linear组成\",{\"1\":{\"431\":1}}],[\"head进行分类\",{\"1\":{\"431\":1}}],[\"head的位置是和这个\",{\"1\":{\"427\":1}}],[\"head之中再输出分类结果\",{\"1\":{\"427\":1}}],[\"head将输出feature映射成一个二值logits\",{\"1\":{\"393\":1}}],[\"header\",{\"1\":{\"204\":1,\"265\":1}}],[\"head==\",{\"1\":{\"191\":2}}],[\"head=\",{\"1\":{\"191\":1}}],[\"head=false\",{\"1\":{\"123\":1}}],[\"head=true\",{\"1\":{\"123\":3}}],[\"heads代表transformer中multi\",{\"1\":{\"432\":1}}],[\"heads=8\",{\"1\":{\"380\":1,\"430\":1}}],[\"heads=num\",{\"1\":{\"380\":2,\"429\":1,\"431\":1}}],[\"heads=12\",{\"1\":{\"205\":2,\"380\":1,\"428\":1,\"431\":1,\"435\":1,\"710\":1}}],[\"heads=dict\",{\"1\":{\"104\":1,\"107\":1}}],[\"heads\",{\"1\":{\"65\":6,\"83\":6,\"104\":1,\"107\":1,\"380\":11,\"420\":1,\"429\":2,\"430\":12,\"431\":1,\"582\":2,\"663\":1,\"703\":2,\"709\":7,\"710\":15,\"724\":7,\"751\":3,\"892\":3,\"900\":6}}],[\"head\",{\"0\":{\"33\":1,\"35\":1,\"36\":1,\"52\":1,\"431\":1},\"1\":{\"11\":1,\"30\":2,\"37\":2,\"50\":1,\"59\":2,\"64\":1,\"70\":6,\"83\":5,\"123\":1,\"143\":1,\"144\":1,\"190\":2,\"191\":2,\"192\":2,\"205\":1,\"207\":2,\"208\":2,\"215\":1,\"243\":1,\"266\":4,\"293\":15,\"308\":1,\"320\":1,\"380\":5,\"401\":2,\"403\":1,\"419\":1,\"420\":10,\"427\":1,\"428\":1,\"429\":1,\"430\":8,\"431\":3,\"432\":1,\"435\":1,\"532\":1,\"534\":1,\"582\":5,\"663\":6,\"709\":1,\"710\":4,\"719\":2,\"721\":1,\"722\":2,\"724\":10,\"731\":2,\"734\":1,\"736\":2,\"737\":2,\"741\":1,\"823\":1,\"892\":4}}],[\"height=3\",{\"1\":{\"542\":1}}],[\"height=600\",{\"1\":{\"107\":1}}],[\"height\",{\"1\":{\"64\":1,\"213\":1,\"263\":5,\"266\":1,\"380\":1,\"424\":1,\"542\":1,\"547\":1,\"926\":8}}],[\"hezhu\",{\"1\":{\"61\":1}}],[\"him\",{\"1\":{\"668\":1,\"670\":1}}],[\"his\",{\"1\":{\"668\":1,\"670\":1}}],[\"history=\",{\"1\":{\"52\":2}}],[\"history=history\",{\"1\":{\"52\":3}}],[\"history=true\",{\"1\":{\"52\":4}}],[\"history=none\",{\"1\":{\"52\":1}}],[\"history\",{\"1\":{\"52\":2}}],[\"hinge\",{\"0\":{\"591\":1},\"1\":{\"591\":1}}],[\"high\",{\"1\":{\"484\":2,\"949\":1}}],[\"higher\",{\"1\":{\"447\":1}}],[\"hi\",{\"1\":{\"459\":1}}],[\"hiddens\",{\"1\":{\"385\":22}}],[\"hidden\",{\"1\":{\"64\":4,\"66\":2,\"67\":3,\"68\":3,\"69\":1,\"187\":1,\"188\":1,\"190\":7,\"191\":3,\"192\":7,\"205\":3,\"206\":4,\"207\":13,\"208\":10,\"255\":14,\"380\":7,\"385\":11,\"397\":8,\"398\":9,\"399\":4,\"400\":17,\"401\":8,\"402\":1,\"403\":25,\"417\":8,\"418\":7,\"419\":7,\"420\":34,\"421\":1,\"429\":9,\"432\":1,\"663\":26,\"699\":2,\"716\":5,\"718\":24,\"719\":5,\"720\":5,\"722\":4,\"724\":8,\"725\":12,\"728\":15,\"729\":7,\"730\":1,\"731\":1,\"733\":6,\"734\":2,\"736\":3,\"737\":4,\"899\":3,\"931\":6,\"937\":6,\"963\":6,\"964\":5}}],[\"hierarchy\",{\"1\":{\"59\":1}}],[\"hv\",{\"1\":{\"56\":1}}],[\"hq\",{\"1\":{\"56\":1,\"128\":1}}],[\"h进行交互融合\",{\"1\":{\"54\":1}}],[\"h=h\",{\"1\":{\"213\":1}}],[\"h=\",{\"1\":{\"54\":1}}],[\"h\",{\"1\":{\"54\":9,\"57\":5,\"58\":6,\"59\":5,\"64\":3,\"65\":2,\"67\":1,\"83\":4,\"150\":5,\"190\":1,\"206\":3,\"213\":11,\"256\":9,\"260\":4,\"263\":9,\"264\":1,\"265\":1,\"266\":2,\"359\":1,\"380\":6,\"384\":1,\"407\":1,\"417\":1,\"426\":4,\"427\":1,\"428\":1,\"431\":1,\"478\":2,\"489\":4,\"503\":3,\"504\":2,\"505\":2,\"582\":6,\"590\":2,\"592\":1,\"595\":5,\"660\":1,\"663\":1,\"699\":5,\"709\":8,\"710\":18,\"712\":3,\"751\":9,\"846\":1,\"877\":3,\"893\":1,\"899\":22,\"900\":3,\"909\":4,\"924\":3,\"926\":2,\"931\":5,\"937\":5,\"963\":6,\"964\":7}}],[\"hdim\",{\"1\":{\"899\":1}}],[\"hdf5\",{\"1\":{\"557\":1}}],[\"hd\",{\"1\":{\"53\":3}}],[\"hk\",{\"1\":{\"53\":4,\"56\":20,\"57\":2}}],[\"half\",{\"0\":{\"866\":1},\"1\":{\"866\":1,\"868\":1}}],[\"hallucination\",{\"1\":{\"657\":1}}],[\"hallucinate\",{\"1\":{\"656\":1}}],[\"hallusionbench分数最高\",{\"1\":{\"335\":1}}],[\"haiku\",{\"1\":{\"823\":2}}],[\"hairy\",{\"1\":{\"691\":2}}],[\"hairy→my\",{\"1\":{\"691\":3}}],[\"have\",{\"1\":{\"657\":1,\"895\":1,\"899\":1}}],[\"happy\",{\"1\":{\"430\":1}}],[\"ha=\",{\"1\":{\"424\":1}}],[\"haotian\",{\"1\":{\"338\":1}}],[\"harvardnlp\",{\"1\":{\"739\":1}}],[\"harmless\",{\"1\":{\"654\":1,\"656\":1}}],[\"hardneg\",{\"1\":{\"383\":1,\"386\":2}}],[\"hard=true时\",{\"0\":{\"258\":1}}],[\"hard=true\",{\"1\":{\"257\":2,\"258\":1}}],[\"hard=false\",{\"1\":{\"257\":1}}],[\"hard=self\",{\"1\":{\"256\":1,\"257\":1,\"258\":1}}],[\"hard\",{\"1\":{\"102\":1,\"106\":1,\"192\":1,\"201\":1,\"204\":1,\"206\":1,\"207\":2,\"208\":2,\"257\":1,\"258\":4,\"283\":1,\"373\":4,\"376\":2,\"383\":1,\"386\":2,\"589\":2,\"899\":1}}],[\"harsh\",{\"1\":{\"14\":1}}],[\"hasattr\",{\"1\":{\"382\":1}}],[\"has\",{\"1\":{\"64\":1,\"420\":1,\"435\":3,\"713\":2,\"899\":2}}],[\"hammer\",{\"1\":{\"53\":1}}],[\"hat\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"hands\",{\"1\":{\"107\":2}}],[\"hand\",{\"1\":{\"52\":4,\"55\":4}}],[\"handle\",{\"1\":{\"6\":1,\"11\":1,\"52\":1,\"55\":2,\"56\":1}}],[\"v0\",{\"1\":{\"435\":1,\"833\":1}}],[\"v计算来源相同\",{\"1\":{\"430\":1}}],[\"v矩阵\",{\"1\":{\"430\":1}}],[\"v时使用偏置\",{\"1\":{\"430\":1}}],[\"vffn\",{\"1\":{\"385\":5}}],[\"vfms\",{\"1\":{\"327\":1}}],[\"vfm\",{\"1\":{\"323\":1}}],[\"verbose=true\",{\"1\":{\"815\":2}}],[\"verbose=false\",{\"1\":{\"815\":2}}],[\"verbose\",{\"1\":{\"815\":4}}],[\"verbose参数控制是否显示详细信息\",{\"1\":{\"815\":1}}],[\"very\",{\"1\":{\"635\":1}}],[\"versa\",{\"1\":{\"597\":1}}],[\"version\",{\"1\":{\"456\":2,\"718\":1,\"728\":1,\"815\":1}}],[\"vec2\",{\"1\":{\"410\":5,\"412\":5}}],[\"vec1\",{\"1\":{\"410\":5,\"412\":5}}],[\"vectorquantizer\",{\"1\":{\"963\":2}}],[\"vectors\",{\"1\":{\"143\":1,\"213\":2,\"751\":1,\"963\":1}}],[\"vector\",{\"1\":{\"124\":1,\"125\":1,\"209\":2,\"213\":2,\"249\":1,\"694\":1,\"960\":1,\"963\":1}}],[\"vector3dvector\",{\"1\":{\"107\":4}}],[\"vehicle\",{\"1\":{\"341\":1}}],[\"ve\",{\"1\":{\"268\":1,\"390\":2}}],[\"vgg进行实现\",{\"1\":{\"410\":1}}],[\"vg\",{\"1\":{\"224\":1,\"382\":1}}],[\"v3\",{\"1\":{\"216\":1,\"246\":1,\"823\":1,\"921\":2}}],[\"v100\",{\"1\":{\"236\":1,\"680\":1}}],[\"v1\",{\"1\":{\"214\":1,\"330\":3,\"680\":1,\"921\":1}}],[\"vqvae\",{\"1\":{\"963\":4,\"964\":2}}],[\"vqgan\",{\"1\":{\"216\":4}}],[\"vqkd\",{\"1\":{\"213\":5}}],[\"vq\",{\"0\":{\"954\":1,\"956\":1,\"957\":1},\"1\":{\"210\":3,\"212\":4,\"213\":1,\"215\":5,\"216\":3,\"217\":2,\"235\":3,\"249\":3,\"251\":1,\"254\":1,\"262\":1,\"264\":1,\"895\":1,\"954\":1,\"955\":2,\"956\":10,\"957\":3,\"958\":6,\"959\":6,\"960\":4,\"961\":6,\"963\":17,\"964\":6,\"965\":1}}],[\"vqav2datamodule\",{\"1\":{\"382\":1}}],[\"vqav2\",{\"1\":{\"220\":1}}],[\"vqa\",{\"1\":{\"165\":1,\"268\":3,\"276\":1,\"277\":1,\"305\":1,\"310\":1,\"312\":1,\"313\":1,\"368\":2,\"382\":1,\"383\":2,\"385\":2,\"402\":1}}],[\"vse++和scan属于\",{\"1\":{\"390\":1}}],[\"vse\",{\"1\":{\"390\":1}}],[\"vs\",{\"0\":{\"830\":1},\"1\":{\"156\":1,\"157\":2,\"215\":1,\"276\":1,\"500\":2,\"584\":1,\"609\":1,\"655\":1,\"668\":3,\"670\":1,\"680\":1,\"681\":1,\"683\":3,\"685\":1,\"735\":1,\"963\":1}}],[\"vocab是词典大小\",{\"1\":{\"743\":1}}],[\"vocab\",{\"1\":{\"208\":5,\"266\":3,\"274\":2,\"382\":4,\"384\":3,\"403\":2,\"420\":1,\"595\":58,\"596\":3,\"597\":76,\"660\":1,\"697\":16,\"699\":6,\"700\":2,\"716\":2,\"729\":2,\"731\":1,\"743\":2,\"892\":1,\"895\":2}}],[\"vocabulary\",{\"1\":{\"11\":1,\"20\":1,\"28\":1,\"30\":1,\"31\":1,\"594\":1,\"595\":1,\"597\":18}}],[\"voxnet\",{\"1\":{\"157\":1}}],[\"voxel\",{\"1\":{\"148\":1,\"159\":1}}],[\"v三元组\",{\"1\":{\"119\":1}}],[\"v\",{\"0\":{\"525\":1},\"1\":{\"96\":2,\"98\":2,\"119\":12,\"192\":4,\"208\":1,\"293\":3,\"372\":1,\"380\":15,\"382\":2,\"383\":4,\"386\":2,\"417\":1,\"424\":4,\"430\":4,\"523\":1,\"529\":1,\"538\":2,\"582\":2,\"595\":6,\"597\":7,\"661\":1,\"663\":2,\"697\":2,\"703\":3,\"709\":4,\"751\":2,\"815\":10}}],[\"vr等应用提供了更通用的功能理解范式\",{\"1\":{\"73\":1}}],[\"vaswani\",{\"1\":{\"640\":1,\"671\":1}}],[\"vase\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"var函数\",{\"1\":{\"815\":1}}],[\"var\",{\"1\":{\"522\":2,\"815\":5}}],[\"various\",{\"1\":{\"341\":1}}],[\"variance\",{\"1\":{\"865\":1}}],[\"variant\",{\"1\":{\"311\":1}}],[\"variational\",{\"0\":{\"941\":1},\"1\":{\"235\":1,\"941\":2,\"953\":1}}],[\"variable等工具函数\",{\"1\":{\"810\":1}}],[\"variable通过creator引用function\",{\"1\":{\"806\":1}}],[\"variable实例通过creator属性引用创建它的function实例\",{\"1\":{\"806\":1}}],[\"variable类封装了numpy的多维数组\",{\"1\":{\"758\":1}}],[\"variable\",{\"0\":{\"808\":1},\"1\":{\"152\":1,\"756\":1,\"757\":1,\"762\":1,\"766\":1,\"771\":2,\"778\":1,\"779\":1,\"783\":1,\"784\":1,\"787\":1,\"791\":1,\"792\":1,\"794\":2,\"795\":1,\"799\":1,\"800\":1,\"801\":2,\"802\":2,\"803\":2,\"804\":1,\"805\":4,\"806\":1,\"807\":7,\"808\":16,\"809\":21,\"810\":11,\"811\":6,\"814\":1,\"815\":6,\"816\":8,\"846\":1,\"918\":5}}],[\"van\",{\"1\":{\"216\":1}}],[\"vae模型\",{\"1\":{\"964\":1}}],[\"vae最大的贡献是提供了一种图像压缩思路\",{\"1\":{\"961\":1}}],[\"vae后立刻把pixelcnn换成了diffusion模型\",{\"1\":{\"961\":1}}],[\"vae论文使用了pixelcnn来采样离散分布\",{\"1\":{\"961\":1}}],[\"vae论文中提出的\",{\"1\":{\"955\":1}}],[\"vae编码器的输出是若干个\",{\"1\":{\"961\":1}}],[\"vae编码出来的向量是连续向量\",{\"1\":{\"956\":1}}],[\"vae借鉴了nlp的思想\",{\"1\":{\"961\":1}}],[\"vae是一个把图像编码成离散向量的图像压缩模型\",{\"1\":{\"961\":1}}],[\"vae怎么优化嵌入空间\",{\"1\":{\"957\":1}}],[\"vae怎么优化编码器和解码器\",{\"1\":{\"957\":1}}],[\"vae生成图片的最后一片空缺\",{\"1\":{\"956\":1}}],[\"vae生成出来的图片都不是很好看\",{\"1\":{\"956\":1}}],[\"vae不是一个vae\",{\"1\":{\"956\":1}}],[\"vae不是面临着和ae一样的问题嘛\",{\"1\":{\"956\":1}}],[\"vae把\",{\"1\":{\"956\":1}}],[\"vae把图片编码了一个离散向量\",{\"1\":{\"956\":1}}],[\"vae也是把图像编码成离散向量\",{\"1\":{\"956\":1}}],[\"vae能把图像变成\",{\"1\":{\"956\":1}}],[\"vae能把图像映射成一个\",{\"1\":{\"956\":1}}],[\"vae能编码离散向量的特性\",{\"1\":{\"956\":1}}],[\"vae能利用codebook机制把图像编码成离散向量\",{\"1\":{\"955\":1}}],[\"vae会被归类到图像生成模型中呢\",{\"1\":{\"956\":1}}],[\"vae会编码出离散向量\",{\"1\":{\"956\":1}}],[\"vae会把图片编码成离散向量\",{\"1\":{\"956\":1}}],[\"vae和ae的唯一区别\",{\"1\":{\"956\":1}}],[\"vae根本不是一个图像生成模型\",{\"1\":{\"956\":1}}],[\"vae之所以把图片编码成符合正态分布的连续向量\",{\"1\":{\"956\":1}}],[\"vae里叫做\",{\"1\":{\"956\":1}}],[\"vae没有影响\",{\"1\":{\"956\":1}}],[\"vae就是这样一种改进版的ae\",{\"1\":{\"956\":1}}],[\"vae想要把图像编码成离散向量\",{\"1\":{\"956\":1}}],[\"vae中的核心机制活学活用\",{\"1\":{\"955\":1}}],[\"vae中关键算法的具体形式\",{\"1\":{\"955\":1}}],[\"vae本身的原理\",{\"1\":{\"955\":1}}],[\"vae的离散分布采样\",{\"1\":{\"961\":1}}],[\"vae的离散编码空间采样\",{\"1\":{\"956\":1}}],[\"vae的优化目标由两部分组成\",{\"1\":{\"961\":1}}],[\"vae的编码器怎么输出离散向量\",{\"1\":{\"957\":1}}],[\"vae的编码器和解码器\",{\"1\":{\"956\":1}}],[\"vae的工作过程\",{\"1\":{\"956\":1}}],[\"vae的解码器把离散编码变成图像\",{\"1\":{\"956\":1}}],[\"vae的解码器前\",{\"1\":{\"956\":1}}],[\"vae的作者之前设计了一种图像生成网络\",{\"1\":{\"956\":1}}],[\"vae的作者利用vq\",{\"1\":{\"956\":1}}],[\"vae的作者认为\",{\"1\":{\"956\":1}}],[\"vae的生成图片之所以质量不高\",{\"1\":{\"956\":1}}],[\"vae的实现细节就不在这里赘述了\",{\"1\":{\"956\":1}}],[\"vae的贡献及其对其他工作的影响做一个总结\",{\"1\":{\"955\":1}}],[\"vae的核心思想\",{\"1\":{\"955\":1,\"956\":1,\"957\":1}}],[\"vae的这种建模方法启发了无数的后续工作\",{\"1\":{\"955\":1}}],[\"vae\",{\"0\":{\"928\":1,\"931\":1,\"951\":1,\"954\":1,\"956\":1,\"957\":1},\"1\":{\"213\":1,\"216\":2,\"235\":7,\"249\":2,\"254\":3,\"258\":1,\"259\":1,\"262\":1,\"264\":6,\"265\":10,\"885\":1,\"892\":13,\"893\":4,\"895\":5,\"925\":2,\"928\":1,\"931\":2,\"932\":3,\"934\":2,\"935\":4,\"936\":2,\"942\":2,\"944\":7,\"945\":1,\"946\":1,\"947\":3,\"948\":3,\"949\":2,\"950\":1,\"951\":2,\"954\":1,\"958\":6,\"959\":7,\"960\":2,\"963\":7,\"964\":4,\"965\":1}}],[\"vae预训练\",{\"0\":{\"212\":1}}],[\"validity\",{\"1\":{\"918\":2}}],[\"validation\",{\"1\":{\"381\":5,\"424\":1}}],[\"validate\",{\"1\":{\"381\":1,\"382\":4}}],[\"valid\",{\"1\":{\"122\":5,\"698\":3,\"918\":3}}],[\"val\",{\"1\":{\"53\":1,\"82\":2,\"89\":1,\"104\":4,\"105\":2,\"106\":1,\"107\":2,\"187\":3,\"190\":3,\"213\":1,\"381\":2,\"382\":34,\"384\":2,\"385\":1,\"386\":1,\"424\":13,\"425\":7,\"582\":4,\"663\":1,\"710\":5,\"896\":3}}],[\"value的数量\",{\"1\":{\"524\":1}}],[\"valueerror\",{\"1\":{\"424\":1,\"697\":1}}],[\"value都会进行缓存\",{\"1\":{\"420\":1}}],[\"value在seq\",{\"1\":{\"420\":1}}],[\"value传入\",{\"1\":{\"420\":1}}],[\"value=past\",{\"1\":{\"663\":1}}],[\"value=\",{\"1\":{\"477\":1}}],[\"value=9\",{\"1\":{\"477\":1}}],[\"value=0\",{\"1\":{\"477\":3,\"893\":1}}],[\"value=self\",{\"1\":{\"420\":1}}],[\"value=tokenizer\",{\"1\":{\"698\":1}}],[\"value=true\",{\"1\":{\"274\":1}}],[\"value=tgt\",{\"1\":{\"100\":1}}],[\"value=none\",{\"1\":{\"207\":1,\"420\":2}}],[\"value=memory\",{\"1\":{\"100\":1}}],[\"value=gt\",{\"1\":{\"99\":1}}],[\"value=x\",{\"1\":{\"99\":1}}],[\"values=outputs\",{\"1\":{\"420\":1}}],[\"values=past\",{\"1\":{\"420\":1,\"663\":2}}],[\"values=query\",{\"1\":{\"420\":1}}],[\"values=next\",{\"1\":{\"420\":1}}],[\"values=none\",{\"1\":{\"265\":2,\"420\":2,\"663\":1}}],[\"values=layer\",{\"1\":{\"380\":1}}],[\"values=lr\",{\"1\":{\"265\":1}}],[\"values=0\",{\"1\":{\"380\":2}}],[\"values=wd\",{\"1\":{\"265\":1}}],[\"values\",{\"1\":{\"52\":5,\"137\":1,\"152\":1,\"154\":1,\"265\":2,\"380\":5,\"419\":3,\"420\":8,\"663\":19,\"710\":7}}],[\"value\",{\"1\":{\"24\":1,\"38\":1,\"56\":8,\"69\":19,\"92\":3,\"96\":7,\"98\":6,\"119\":1,\"207\":5,\"265\":3,\"272\":1,\"274\":1,\"399\":1,\"401\":5,\"402\":2,\"420\":30,\"523\":1,\"524\":1,\"528\":2,\"529\":2,\"531\":1,\"533\":1,\"663\":20,\"698\":1,\"724\":6,\"751\":7,\"807\":4,\"823\":2,\"893\":2,\"898\":2}}],[\"v2\",{\"0\":{\"124\":1},\"1\":{\"40\":1,\"124\":2,\"125\":1,\"209\":2,\"210\":2,\"212\":2,\"213\":2,\"214\":2,\"215\":2,\"218\":1,\"223\":1,\"289\":1,\"308\":1,\"510\":1,\"610\":2,\"680\":1,\"823\":2,\"921\":2}}],[\"vice\",{\"1\":{\"597\":1}}],[\"vicuna工作\",{\"1\":{\"669\":1}}],[\"vicuna\",{\"1\":{\"305\":1,\"311\":1,\"341\":1,\"342\":2}}],[\"vicuna等\",{\"1\":{\"299\":1}}],[\"vicuna等llms无缝集成\",{\"1\":{\"296\":1}}],[\"virtex\",{\"1\":{\"413\":1}}],[\"vilbert\",{\"1\":{\"390\":1}}],[\"vilt预训练的优化目标有两个\",{\"1\":{\"393\":1}}],[\"vilt是首个使用patch\",{\"1\":{\"391\":1}}],[\"vilt是首个将ve设计的如te一样轻量的方法\",{\"1\":{\"390\":1}}],[\"vilt延用single\",{\"1\":{\"391\":1}}],[\"vilt使用预训练的vit来初始化交互的transformer\",{\"1\":{\"389\":1,\"392\":1}}],[\"vilt首次引入了全词掩码和图像增强技术于视觉\",{\"1\":{\"388\":1}}],[\"vilt不仅极大降低了模型参数和计算负担\",{\"1\":{\"388\":1}}],[\"vilt模型提出了一种极简化的视觉嵌入方案\",{\"1\":{\"388\":1}}],[\"vilt\",{\"0\":{\"387\":1},\"1\":{\"251\":1,\"269\":1,\"369\":2,\"376\":1,\"379\":2,\"387\":3}}],[\"video\",{\"1\":{\"273\":1}}],[\"vinvl\",{\"1\":{\"269\":1}}],[\"vit核心\",{\"1\":{\"436\":1}}],[\"vitjx\",{\"1\":{\"435\":1}}],[\"vit这篇论文长达二十多页\",{\"1\":{\"433\":1}}],[\"vit才会慢慢超越resnet\",{\"1\":{\"432\":1}}],[\"vit的效果表现不如resnet\",{\"1\":{\"432\":1}}],[\"vit的表现通常比同等大小的resnets要差一些\",{\"1\":{\"422\":1}}],[\"vit的表现就会超过cnn\",{\"1\":{\"422\":1}}],[\"vit仍是采用transformer中用到layer\",{\"1\":{\"429\":1}}],[\"vit虽然采用的是transformer\",{\"1\":{\"429\":1}}],[\"vit中的多头自注意力模块实现逻辑和transformer基本一致\",{\"1\":{\"430\":1}}],[\"vit中\",{\"1\":{\"428\":1}}],[\"vit原论文中最核心的结论是\",{\"1\":{\"422\":1}}],[\"vit及其衍生模型\",{\"1\":{\"298\":1}}],[\"vits\",{\"1\":{\"293\":2}}],[\"vit没有使用bn\",{\"1\":{\"285\":1}}],[\"vit=\",{\"1\":{\"190\":1,\"192\":1}}],[\"vit=config\",{\"1\":{\"187\":1,\"190\":1,\"192\":1}}],[\"vit梯度检查点层数\",{\"1\":{\"187\":1}}],[\"vit\",{\"0\":{\"582\":1},\"1\":{\"171\":2,\"176\":8,\"177\":1,\"187\":15,\"190\":13,\"191\":7,\"192\":15,\"197\":1,\"205\":2,\"206\":1,\"213\":4,\"216\":2,\"224\":2,\"233\":1,\"236\":1,\"247\":1,\"264\":1,\"266\":1,\"272\":2,\"280\":11,\"285\":1,\"286\":3,\"288\":4,\"293\":1,\"298\":1,\"304\":1,\"308\":1,\"327\":1,\"329\":1,\"330\":2,\"341\":1,\"342\":1,\"376\":2,\"380\":2,\"398\":1,\"405\":2,\"407\":2,\"410\":4,\"412\":1,\"417\":1,\"420\":1,\"425\":3,\"431\":1,\"435\":6,\"510\":1}}],[\"via\",{\"1\":{\"128\":2,\"655\":1}}],[\"visible\",{\"0\":{\"520\":1},\"1\":{\"520\":3}}],[\"visiontransformerformaskedimagemodeling\",{\"1\":{\"266\":2}}],[\"visiontransformer\",{\"1\":{\"205\":2,\"213\":2,\"427\":2,\"428\":2,\"431\":2,\"435\":1}}],[\"vision\",{\"0\":{\"127\":1,\"338\":1},\"1\":{\"64\":1,\"67\":1,\"70\":1,\"97\":1,\"164\":2,\"179\":1,\"187\":2,\"190\":10,\"191\":4,\"192\":9,\"193\":2,\"195\":1,\"205\":9,\"206\":2,\"217\":1,\"219\":4,\"264\":2,\"265\":2,\"271\":1,\"272\":1,\"279\":2,\"280\":1,\"286\":1,\"294\":1,\"305\":2,\"338\":1,\"366\":2,\"367\":2,\"378\":1,\"380\":2,\"385\":2,\"387\":2,\"390\":1,\"405\":1,\"407\":1,\"410\":1,\"417\":2,\"421\":1,\"422\":3,\"429\":1,\"435\":2,\"436\":1}}],[\"vis\",{\"1\":{\"107\":6}}],[\"visualgenomecaptiondatamodule\",{\"1\":{\"382\":1}}],[\"visualization\",{\"1\":{\"107\":3}}],[\"visualizer\",{\"1\":{\"107\":1}}],[\"visualize\",{\"1\":{\"107\":4,\"582\":1}}],[\"visual\",{\"0\":{\"886\":1},\"1\":{\"12\":1,\"23\":1,\"61\":1,\"176\":1,\"185\":1,\"187\":2,\"188\":1,\"190\":6,\"191\":2,\"192\":6,\"205\":4,\"206\":2,\"209\":2,\"213\":1,\"215\":1,\"224\":1,\"235\":2,\"264\":10,\"294\":1,\"339\":1,\"347\":2,\"380\":2,\"383\":2,\"384\":1,\"385\":2,\"388\":1,\"391\":2,\"413\":2,\"417\":1,\"421\":2,\"900\":23}}],[\"view\",{\"0\":{\"469\":1},\"1\":{\"12\":2,\"54\":1,\"65\":1,\"69\":2,\"83\":4,\"119\":10,\"137\":11,\"138\":1,\"141\":2,\"145\":2,\"148\":1,\"152\":3,\"154\":2,\"156\":2,\"159\":1,\"190\":1,\"208\":2,\"213\":1,\"384\":2,\"401\":1,\"403\":3,\"420\":4,\"468\":2,\"469\":7,\"470\":1,\"482\":1,\"490\":1,\"492\":5,\"494\":1,\"544\":1,\"545\":1,\"586\":2,\"587\":2,\"588\":2,\"589\":2,\"590\":2,\"592\":2,\"663\":4,\"700\":2,\"709\":4,\"722\":4,\"724\":3,\"731\":4,\"736\":5,\"737\":6,\"751\":3,\"892\":2,\"918\":2,\"926\":1,\"934\":1,\"935\":3,\"938\":1,\"939\":1,\"963\":3,\"964\":2}}],[\"vlbert\",{\"1\":{\"403\":1}}],[\"vlffn=true\",{\"1\":{\"380\":1}}],[\"vlffn=false\",{\"1\":{\"380\":1}}],[\"vlffn=\",{\"1\":{\"380\":1}}],[\"vlffn\",{\"1\":{\"380\":6,\"385\":46}}],[\"vllms\",{\"1\":{\"296\":1}}],[\"vlp\",{\"0\":{\"167\":1},\"1\":{\"165\":2,\"167\":1,\"168\":1,\"269\":2,\"388\":1}}],[\"vlmevalkit\",{\"1\":{\"334\":1}}],[\"vlmo\",{\"0\":{\"367\":1,\"378\":1,\"381\":1},\"1\":{\"269\":1,\"367\":3,\"368\":6,\"369\":1,\"370\":1,\"373\":2,\"375\":2,\"376\":1,\"377\":5,\"378\":3,\"379\":3,\"380\":2,\"381\":2,\"382\":6,\"383\":5,\"386\":1}}],[\"vlm\",{\"1\":{\"67\":1}}],[\"vl\",{\"1\":{\"2\":1,\"190\":4,\"192\":4,\"207\":4,\"300\":1,\"309\":1,\"323\":1,\"325\":1,\"326\":2,\"327\":1,\"368\":1,\"369\":1,\"372\":1,\"376\":1,\"380\":10,\"384\":1,\"385\":40,\"419\":4}}],[\"wgan\",{\"0\":{\"966\":1},\"1\":{\"966\":1}}],[\"wgan的成功\",{\"1\":{\"919\":1}}],[\"wget\",{\"1\":{\"712\":1}}],[\"wpe\",{\"1\":{\"663\":1}}],[\"wpa\",{\"1\":{\"393\":1}}],[\"wte\",{\"1\":{\"663\":1}}],[\"wmt\",{\"1\":{\"641\":1,\"656\":1}}],[\"w+∆w\",{\"1\":{\"606\":1}}],[\"wx\",{\"1\":{\"600\":1}}],[\"w>\",{\"1\":{\"595\":3,\"596\":2,\"597\":3}}],[\"w`代表输出特征图的宽和高\",{\"1\":{\"426\":1}}],[\"wukong\",{\"1\":{\"332\":1}}],[\"wd\",{\"1\":{\"265\":2,\"293\":2}}],[\"write\",{\"1\":{\"424\":1,\"815\":2}}],[\"writer=none\",{\"1\":{\"265\":1}}],[\"writer=log\",{\"1\":{\"265\":1}}],[\"writer\",{\"1\":{\"265\":1}}],[\"wrod\",{\"1\":{\"392\":1}}],[\"wraps\",{\"1\":{\"454\":3,\"455\":1,\"461\":3}}],[\"wrapper\",{\"1\":{\"449\":5,\"451\":4,\"452\":2,\"453\":3,\"454\":8,\"455\":2,\"461\":6}}],[\"wrap\",{\"1\":{\"92\":1}}],[\"wrapgrasp\",{\"1\":{\"53\":1,\"82\":1}}],[\"w^2\",{\"1\":{\"263\":1}}],[\"w=w\",{\"1\":{\"213\":1}}],[\"wake\",{\"1\":{\"950\":1}}],[\"way\",{\"1\":{\"660\":17}}],[\"warnings\",{\"1\":{\"412\":2}}],[\"warmup\",{\"1\":{\"204\":1,\"224\":1,\"236\":1,\"293\":11}}],[\"walk\",{\"1\":{\"410\":1,\"412\":1}}],[\"water\",{\"1\":{\"202\":1}}],[\"wandb\",{\"1\":{\"384\":1}}],[\"wang\",{\"1\":{\"110\":1}}],[\"want\",{\"1\":{\"107\":3}}],[\"wait\",{\"1\":{\"6\":1,\"12\":1,\"14\":2}}],[\"were\",{\"1\":{\"424\":1}}],[\"webquestions\",{\"1\":{\"648\":1}}],[\"webtext2\",{\"1\":{\"647\":1}}],[\"webtext的多样性和规模是关键\",{\"1\":{\"641\":1}}],[\"webtext训练集和测试集的困惑度同步下降\",{\"1\":{\"641\":1}}],[\"webtext\",{\"1\":{\"640\":4,\"643\":1}}],[\"web\",{\"1\":{\"413\":1}}],[\"weakref\",{\"1\":{\"806\":2,\"807\":1,\"815\":1}}],[\"weakly\",{\"1\":{\"413\":1}}],[\"wear\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"wei\",{\"1\":{\"216\":1,\"655\":1,\"671\":1}}],[\"weight=none\",{\"1\":{\"586\":1,\"587\":1,\"588\":1,\"589\":1,\"590\":1,\"592\":1}}],[\"weight=self\",{\"1\":{\"380\":1}}],[\"weighted\",{\"1\":{\"592\":5}}],[\"weightedrandomsampler\",{\"0\":{\"518\":1},\"1\":{\"518\":3}}],[\"weightedloss\",{\"1\":{\"208\":1}}],[\"weights=train\",{\"1\":{\"518\":1}}],[\"weights=w\",{\"1\":{\"485\":1}}],[\"weights=none\",{\"1\":{\"485\":1}}],[\"weights\",{\"1\":{\"119\":2,\"190\":6,\"192\":6,\"207\":6,\"213\":2,\"380\":1,\"386\":6,\"410\":2,\"412\":2,\"419\":4,\"431\":1,\"435\":5,\"485\":2,\"514\":3,\"518\":4,\"663\":3,\"699\":1,\"700\":1,\"721\":1,\"722\":1,\"729\":1}}],[\"weight\",{\"0\":{\"514\":1},\"1\":{\"104\":1,\"122\":3,\"145\":4,\"187\":2,\"190\":2,\"192\":2,\"208\":1,\"213\":17,\"255\":3,\"256\":5,\"265\":2,\"274\":2,\"293\":4,\"359\":2,\"380\":1,\"514\":8,\"547\":1,\"586\":3,\"587\":2,\"588\":2,\"589\":7,\"590\":4,\"592\":4,\"699\":9,\"700\":1,\"892\":1,\"893\":8,\"899\":7,\"926\":3,\"963\":3,\"964\":4}}],[\"we\",{\"0\":{\"660\":1},\"1\":{\"102\":1,\"321\":2,\"658\":1,\"720\":1,\"722\":1,\"734\":1,\"751\":1}}],[\"w₂\",{\"1\":{\"98\":1}}],[\"wow\",{\"1\":{\"595\":1}}],[\"word2idx\",{\"1\":{\"697\":14,\"698\":9,\"703\":1}}],[\"word2vec\",{\"1\":{\"355\":1,\"650\":1}}],[\"wordpunct\",{\"1\":{\"595\":1,\"596\":1,\"597\":2}}],[\"wordpiecetokenizer\",{\"1\":{\"697\":1}}],[\"wordpiece\",{\"1\":{\"371\":1}}],[\"word\",{\"1\":{\"382\":1,\"393\":3,\"419\":1,\"595\":10,\"597\":12,\"633\":1,\"641\":1,\"697\":17,\"698\":2,\"699\":4,\"716\":2}}],[\"words\",{\"1\":{\"187\":1,\"208\":2,\"413\":3,\"597\":5,\"716\":2}}],[\"worker\",{\"1\":{\"359\":1}}],[\"workers\",{\"1\":{\"293\":1,\"359\":1,\"382\":10}}],[\"workers=self\",{\"1\":{\"382\":6}}],[\"workers=args\",{\"1\":{\"293\":1,\"359\":1}}],[\"workers=\",{\"1\":{\"187\":1,\"190\":1,\"192\":1}}],[\"workers=8\",{\"1\":{\"104\":3}}],[\"work\",{\"0\":{\"166\":1,\"195\":1,\"355\":1,\"357\":1,\"369\":1}}],[\"world\",{\"1\":{\"12\":1,\"386\":5,\"419\":6,\"595\":1,\"596\":1}}],[\"would\",{\"1\":{\"94\":1}}],[\"winogender\",{\"1\":{\"655\":1,\"656\":1,\"657\":1,\"668\":1,\"670\":1}}],[\"winograd\",{\"1\":{\"641\":1}}],[\"windows\",{\"1\":{\"557\":1}}],[\"window\",{\"1\":{\"107\":4,\"264\":1}}],[\"will\",{\"1\":{\"597\":1}}],[\"wild\",{\"1\":{\"202\":2}}],[\"wightman\",{\"1\":{\"510\":1}}],[\"wikitext2是wikitext\",{\"1\":{\"696\":1}}],[\"wikitext\",{\"1\":{\"641\":2,\"696\":15,\"700\":1}}],[\"wikibkdatamodule\",{\"1\":{\"382\":1}}],[\"wikibk\",{\"1\":{\"382\":1}}],[\"wikipedia\",{\"1\":{\"224\":1,\"640\":1,\"647\":1,\"679\":1,\"680\":1,\"684\":1,\"823\":1}}],[\"width=4\",{\"1\":{\"542\":1}}],[\"width=600\",{\"1\":{\"107\":1}}],[\"width\",{\"1\":{\"64\":1,\"187\":3,\"190\":7,\"191\":4,\"192\":6,\"205\":10,\"213\":1,\"263\":5,\"266\":1,\"380\":1,\"424\":1,\"542\":1,\"926\":6}}],[\"wise\",{\"1\":{\"54\":2,\"70\":1,\"83\":4,\"94\":2,\"223\":1,\"264\":2,\"546\":1,\"633\":1}}],[\"without\",{\"0\":{\"661\":1},\"1\":{\"387\":2}}],[\"with\",{\"0\":{\"343\":1,\"662\":1},\"1\":{\"7\":1,\"11\":1,\"12\":1,\"14\":1,\"18\":2,\"52\":4,\"61\":1,\"64\":1,\"92\":2,\"100\":3,\"107\":4,\"190\":2,\"192\":2,\"193\":2,\"206\":1,\"207\":1,\"208\":1,\"209\":2,\"213\":2,\"265\":2,\"274\":1,\"280\":1,\"285\":1,\"293\":1,\"321\":2,\"341\":1,\"362\":1,\"367\":2,\"378\":1,\"380\":4,\"383\":2,\"386\":4,\"408\":3,\"410\":2,\"412\":2,\"419\":1,\"424\":1,\"444\":2,\"477\":4,\"492\":1,\"595\":3,\"597\":11,\"602\":2,\"652\":1,\"660\":1,\"663\":1,\"696\":3,\"697\":3,\"700\":1,\"713\":3,\"750\":1,\"807\":2,\"815\":2,\"894\":1,\"895\":1,\"921\":2,\"926\":1,\"935\":2,\"939\":1,\"964\":3}}],[\"w\",{\"1\":{\"54\":3,\"58\":1,\"64\":4,\"65\":2,\"83\":4,\"119\":14,\"190\":1,\"213\":11,\"256\":9,\"260\":4,\"263\":9,\"264\":1,\"265\":1,\"266\":6,\"359\":1,\"380\":6,\"384\":1,\"407\":5,\"417\":1,\"424\":1,\"426\":4,\"427\":1,\"428\":1,\"431\":1,\"478\":2,\"485\":1,\"489\":6,\"503\":3,\"504\":2,\"505\":2,\"522\":6,\"582\":5,\"590\":2,\"592\":1,\"595\":5,\"597\":5,\"606\":1,\"656\":1,\"696\":2,\"697\":1,\"709\":8,\"751\":4,\"815\":2,\"893\":1,\"899\":21,\"900\":3,\"924\":3,\"926\":2,\"963\":6,\"964\":7}}],[\"who\",{\"1\":{\"641\":1,\"658\":1}}],[\"whole\",{\"1\":{\"382\":1,\"393\":2}}],[\"when\",{\"1\":{\"893\":1}}],[\"whether\",{\"1\":{\"597\":1}}],[\"where\",{\"0\":{\"476\":1},\"1\":{\"94\":1,\"95\":1,\"121\":2,\"213\":2,\"476\":1,\"557\":1,\"589\":1,\"710\":1,\"893\":1}}],[\"what\",{\"0\":{\"349\":1},\"1\":{\"877\":1}}],[\"white\",{\"1\":{\"408\":1}}],[\"whitening\",{\"1\":{\"282\":1}}],[\"while\",{\"1\":{\"53\":1,\"263\":1,\"444\":2,\"596\":1,\"597\":1,\"660\":1,\"787\":1,\"801\":1,\"803\":1,\"805\":1,\"807\":1,\"811\":1,\"815\":1}}],[\"which\",{\"1\":{\"52\":2,\"107\":3,\"557\":1,\"724\":1}}],[\"why\",{\"0\":{\"660\":1},\"1\":{\"52\":2}}],[\"e模型的文生图质量\",{\"1\":{\"900\":1}}],[\"echo\",{\"1\":{\"827\":1}}],[\"ecc\",{\"1\":{\"110\":1}}],[\"edge\",{\"1\":{\"815\":4}}],[\"e=3\",{\"1\":{\"694\":2}}],[\"efficient\",{\"1\":{\"602\":1,\"604\":1,\"607\":2,\"660\":17,\"664\":1}}],[\"efficientnet\",{\"1\":{\"510\":1}}],[\"eow\",{\"1\":{\"597\":3}}],[\"eol\",{\"1\":{\"596\":2,\"597\":3}}],[\"eos\",{\"1\":{\"188\":2,\"421\":1,\"660\":2,\"663\":2}}],[\"ee\",{\"1\":{\"595\":1}}],[\"e^\",{\"1\":{\"589\":1}}],[\"euclidean\",{\"0\":{\"576\":1}}],[\"einops\",{\"1\":{\"478\":2}}],[\"einstein\",{\"1\":{\"100\":1,\"362\":1,\"475\":1,\"478\":1}}],[\"einsum\",{\"0\":{\"475\":1},\"1\":{\"94\":1,\"100\":5,\"213\":1,\"256\":2,\"274\":1,\"362\":2,\"475\":4,\"709\":2,\"899\":2,\"900\":2}}],[\"e5005f0a\",{\"1\":{\"435\":1}}],[\"escape\",{\"1\":{\"595\":1,\"597\":1}}],[\"estimation\",{\"0\":{\"903\":1},\"1\":{\"355\":2}}],[\"estimations\",{\"1\":{\"12\":1}}],[\"estimator\",{\"1\":{\"257\":1,\"899\":1,\"959\":1,\"963\":2}}],[\"esser\",{\"1\":{\"216\":1}}],[\"et\",{\"0\":{\"709\":1},\"1\":{\"171\":5,\"172\":2,\"176\":4,\"214\":1,\"640\":3,\"655\":17,\"656\":2,\"658\":2,\"667\":1,\"669\":1,\"671\":10}}],[\"eta\",{\"1\":{\"104\":1}}],[\"eye\",{\"1\":{\"153\":1}}],[\"equal\",{\"1\":{\"899\":1}}],[\"equals\",{\"1\":{\"751\":1}}],[\"equivariance\",{\"1\":{\"422\":1}}],[\"eq\",{\"1\":{\"119\":3,\"190\":3,\"431\":1,\"703\":1}}],[\"epsilon\",{\"1\":{\"431\":1}}],[\"eps=config\",{\"1\":{\"400\":1,\"403\":1,\"716\":1,\"718\":1,\"725\":1,\"728\":1}}],[\"eps=1e\",{\"1\":{\"104\":1,\"205\":2,\"213\":2,\"380\":1,\"431\":1,\"592\":1,\"771\":1,\"795\":1}}],[\"eps\",{\"1\":{\"213\":6,\"400\":1,\"403\":1,\"522\":1,\"592\":5,\"716\":1,\"718\":1,\"725\":1,\"728\":1,\"771\":3,\"931\":2,\"937\":2}}],[\"epochs=4\",{\"1\":{\"712\":1}}],[\"epochs=args\",{\"1\":{\"293\":1}}],[\"epochs\",{\"1\":{\"192\":1,\"265\":1,\"293\":11,\"359\":1,\"381\":1,\"681\":1,\"700\":2,\"918\":4,\"934\":2,\"963\":3}}],[\"epoch>0\",{\"1\":{\"190\":1,\"204\":1}}],[\"epoch+1\",{\"1\":{\"105\":1,\"926\":1,\"934\":1,\"938\":1,\"963\":1,\"964\":1}}],[\"epoch\",{\"1\":{\"34\":1,\"104\":1,\"105\":2,\"106\":2,\"107\":1,\"181\":2,\"187\":5,\"190\":4,\"192\":6,\"204\":3,\"236\":2,\"242\":3,\"265\":9,\"286\":2,\"293\":10,\"359\":5,\"381\":10,\"431\":2,\"518\":1,\"656\":1,\"700\":4,\"918\":4,\"926\":2,\"934\":2,\"935\":1,\"938\":2,\"963\":2,\"964\":2}}],[\"each\",{\"1\":{\"190\":2,\"192\":2,\"597\":1,\"729\":1,\"747\":1,\"918\":1}}],[\"easy\",{\"1\":{\"102\":1,\"589\":1}}],[\"earphone\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"emerging\",{\"1\":{\"279\":2}}],[\"ema衰减率\",{\"1\":{\"213\":1}}],[\"ema\",{\"1\":{\"192\":2,\"205\":2,\"206\":1,\"212\":2,\"213\":20,\"283\":1,\"285\":1,\"293\":8,\"963\":1}}],[\"employ\",{\"1\":{\"102\":1}}],[\"empty\",{\"1\":{\"92\":1,\"293\":1,\"893\":1}}],[\"emb=false\",{\"1\":{\"380\":1}}],[\"embed=true\",{\"1\":{\"380\":1}}],[\"embed=n\",{\"1\":{\"213\":1}}],[\"embed=8192\",{\"1\":{\"213\":1}}],[\"embed\",{\"1\":{\"97\":7,\"100\":3,\"190\":7,\"191\":3,\"192\":7,\"205\":14,\"206\":2,\"213\":31,\"266\":12,\"274\":4,\"293\":6,\"380\":31,\"384\":1,\"385\":2,\"407\":2,\"426\":9,\"427\":13,\"428\":15,\"430\":9,\"431\":17,\"435\":1,\"663\":6,\"742\":8,\"895\":1}}],[\"embeds则拼接\",{\"1\":{\"419\":1}}],[\"embeds=query\",{\"1\":{\"417\":1,\"419\":1,\"420\":2,\"421\":1}}],[\"embeds=encoder\",{\"1\":{\"208\":1}}],[\"embeds=inputs\",{\"1\":{\"208\":1}}],[\"embeds=none\",{\"1\":{\"208\":2,\"384\":1,\"385\":1,\"419\":1,\"420\":1}}],[\"embeds=llm\",{\"1\":{\"64\":1}}],[\"embeds\",{\"1\":{\"64\":4,\"67\":10,\"187\":3,\"188\":5,\"190\":16,\"191\":4,\"192\":16,\"206\":10,\"207\":22,\"208\":4,\"274\":11,\"384\":13,\"385\":12,\"417\":4,\"419\":16,\"420\":3,\"421\":6,\"663\":7,\"899\":6}}],[\"embedding相同维度\",{\"1\":{\"421\":1}}],[\"embedding计算出key和value\",{\"1\":{\"420\":1}}],[\"embedding送到二分类器中\",{\"1\":{\"419\":1}}],[\"embedding分别都嵌入了一个额外的可学习\",{\"1\":{\"392\":1}}],[\"embedding部分\",{\"1\":{\"392\":2}}],[\"embedding标志位来区分\",{\"1\":{\"392\":1}}],[\"embedding通过可学习的modal\",{\"1\":{\"392\":1}}],[\"embedding和visual\",{\"1\":{\"392\":2}}],[\"embedding和modality\",{\"1\":{\"390\":1}}],[\"embedding进行concate\",{\"1\":{\"392\":2}}],[\"embedding进行相加\",{\"1\":{\"392\":2}}],[\"embedding是现有vlp模型的瓶颈\",{\"1\":{\"391\":1}}],[\"embedding存在着差异\",{\"1\":{\"391\":1}}],[\"embedding基本上都使用类bert结构\",{\"1\":{\"391\":1}}],[\"embedding的方法总共有三大类\",{\"1\":{\"391\":1}}],[\"embedding的方法\",{\"1\":{\"388\":1,\"391\":1}}],[\"embeddingema\",{\"1\":{\"213\":3}}],[\"embeddings=512\",{\"1\":{\"963\":1,\"964\":2}}],[\"embeddings=false\",{\"1\":{\"274\":1}}],[\"embeddings前面\",{\"1\":{\"421\":1}}],[\"embeddings和query\",{\"1\":{\"419\":1}}],[\"embeddings\",{\"1\":{\"64\":1,\"67\":1,\"100\":2,\"188\":1,\"190\":2,\"192\":2,\"207\":2,\"208\":1,\"266\":1,\"274\":3,\"384\":4,\"385\":4,\"397\":1,\"402\":1,\"410\":12,\"411\":4,\"412\":15,\"417\":3,\"419\":17,\"699\":1,\"716\":23,\"720\":1,\"721\":2,\"729\":1,\"731\":1,\"963\":8,\"964\":5}}],[\"embedding\",{\"1\":{\"59\":1,\"64\":1,\"67\":1,\"69\":1,\"70\":1,\"100\":1,\"213\":15,\"214\":2,\"255\":1,\"257\":1,\"266\":4,\"274\":4,\"286\":1,\"293\":1,\"371\":2,\"380\":16,\"384\":5,\"385\":10,\"390\":3,\"392\":4,\"397\":2,\"410\":4,\"411\":1,\"412\":3,\"419\":1,\"420\":1,\"426\":1,\"428\":1,\"692\":5,\"699\":3,\"709\":5,\"710\":4,\"716\":3,\"721\":2,\"823\":1,\"832\":1,\"892\":6,\"893\":7,\"899\":3,\"900\":12,\"956\":1,\"963\":13,\"964\":1}}],[\"emb\",{\"1\":{\"54\":1,\"58\":6,\"59\":12,\"60\":16,\"65\":14,\"66\":4,\"68\":2,\"69\":20,\"70\":14,\"83\":35,\"94\":3,\"104\":2,\"107\":2,\"213\":3,\"274\":1,\"380\":3,\"411\":2,\"412\":2,\"699\":1,\"709\":5,\"892\":13,\"893\":8,\"898\":2,\"900\":15}}],[\"events\",{\"1\":{\"849\":1}}],[\"every\",{\"1\":{\"204\":1,\"265\":1,\"424\":5,\"597\":1}}],[\"eva\",{\"1\":{\"298\":1,\"308\":1,\"510\":1}}],[\"evaluation\",{\"1\":{\"273\":2,\"656\":1,\"898\":1}}],[\"evaluating\",{\"1\":{\"106\":1}}],[\"evaluate\",{\"1\":{\"14\":1,\"106\":1,\"700\":1}}],[\"eval\",{\"1\":{\"52\":1,\"106\":1,\"107\":1,\"213\":1,\"381\":3,\"382\":3,\"660\":1,\"663\":1,\"712\":2,\"895\":2,\"898\":1,\"899\":2,\"926\":1,\"935\":1,\"939\":1,\"964\":2}}],[\"ev\",{\"1\":{\"69\":2}}],[\"ek\",{\"1\":{\"69\":2}}],[\"e\",{\"0\":{\"883\":1,\"890\":1},\"1\":{\"64\":1,\"69\":7,\"83\":11,\"122\":4,\"192\":10,\"213\":1,\"216\":1,\"251\":1,\"254\":1,\"256\":1,\"264\":5,\"265\":1,\"405\":3,\"407\":6,\"410\":4,\"411\":2,\"412\":6,\"595\":5,\"656\":2,\"694\":2,\"815\":1,\"881\":2,\"883\":2,\"890\":2,\"891\":2,\"892\":1,\"893\":4,\"894\":1,\"895\":1,\"898\":2,\"899\":3,\"959\":2,\"963\":2,\"964\":2}}],[\"elb\",{\"1\":{\"885\":1,\"886\":6,\"887\":1}}],[\"elbo\",{\"1\":{\"235\":1,\"885\":5,\"932\":1,\"947\":1}}],[\"ele\",{\"1\":{\"700\":4}}],[\"element\",{\"1\":{\"546\":1}}],[\"elements\",{\"1\":{\"516\":2}}],[\"elmo\",{\"1\":{\"633\":1,\"634\":1,\"650\":1}}],[\"elicits\",{\"1\":{\"620\":1}}],[\"elif\",{\"1\":{\"191\":1,\"213\":2,\"264\":1,\"380\":1,\"382\":2,\"420\":1,\"444\":1,\"663\":1,\"697\":1,\"926\":1}}],[\"eldan\",{\"1\":{\"500\":1}}],[\"ellipsis\",{\"0\":{\"463\":1},\"1\":{\"463\":1}}],[\"else\",{\"1\":{\"53\":5,\"64\":2,\"67\":1,\"82\":4,\"92\":3,\"96\":2,\"98\":2,\"106\":1,\"107\":1,\"119\":5,\"120\":1,\"121\":4,\"122\":3,\"123\":2,\"137\":4,\"138\":2,\"141\":3,\"145\":2,\"154\":2,\"188\":1,\"190\":2,\"204\":1,\"207\":1,\"208\":2,\"213\":7,\"255\":1,\"256\":1,\"263\":2,\"264\":1,\"266\":2,\"380\":9,\"382\":5,\"384\":2,\"385\":3,\"386\":1,\"397\":1,\"401\":1,\"403\":1,\"410\":1,\"411\":1,\"412\":2,\"419\":2,\"420\":4,\"421\":1,\"424\":1,\"426\":1,\"429\":1,\"431\":1,\"435\":2,\"444\":1,\"596\":3,\"597\":4,\"663\":4,\"697\":4,\"713\":2,\"715\":1,\"718\":1,\"722\":1,\"728\":1,\"736\":1,\"737\":3,\"800\":1,\"803\":1,\"805\":2,\"807\":2,\"815\":1,\"892\":3,\"894\":1,\"895\":2,\"898\":1,\"899\":2,\"900\":1,\"918\":2,\"926\":2,\"934\":1,\"938\":1,\"963\":1,\"964\":1}}],[\"elowen\",{\"0\":{\"3\":1}}],[\"exact=16\",{\"1\":{\"710\":1}}],[\"exact\",{\"1\":{\"710\":9}}],[\"examples\",{\"1\":{\"102\":2,\"386\":1,\"589\":2}}],[\"exchange\",{\"1\":{\"667\":1}}],[\"exception\",{\"1\":{\"410\":2,\"411\":1,\"412\":3}}],[\"except\",{\"1\":{\"410\":2,\"411\":1,\"412\":3,\"444\":2,\"815\":1,\"963\":1,\"964\":1}}],[\"extend\",{\"1\":{\"696\":1}}],[\"extended\",{\"1\":{\"397\":1,\"420\":1,\"721\":6}}],[\"extension\",{\"1\":{\"410\":2,\"412\":2,\"815\":3}}],[\"extract\",{\"1\":{\"53\":2,\"82\":1}}],[\"exist\",{\"1\":{\"411\":1,\"412\":1,\"424\":1,\"696\":2,\"918\":2}}],[\"exists\",{\"1\":{\"274\":5,\"410\":1,\"411\":1,\"412\":2,\"424\":1,\"697\":1,\"700\":1,\"815\":1,\"893\":5,\"894\":1,\"895\":2,\"900\":1}}],[\"exit\",{\"1\":{\"265\":1}}],[\"export\",{\"1\":{\"712\":1}}],[\"exponential\",{\"1\":{\"202\":1}}],[\"experimental\",{\"1\":{\"655\":1}}],[\"experiments\",{\"0\":{\"175\":1},\"1\":{\"174\":1}}],[\"experts\",{\"0\":{\"380\":1},\"1\":{\"367\":2,\"368\":1,\"372\":2,\"377\":1,\"378\":1}}],[\"exp\",{\"1\":{\"263\":1,\"274\":1,\"385\":2,\"407\":1,\"589\":6,\"766\":5,\"790\":2,\"900\":1,\"931\":3,\"932\":1,\"937\":1}}],[\"expansion\",{\"1\":{\"97\":4,\"120\":8,\"123\":4}}],[\"expanduser\",{\"1\":{\"815\":1}}],[\"expanded\",{\"1\":{\"472\":1}}],[\"expand\",{\"0\":{\"472\":1},\"1\":{\"70\":1,\"83\":2,\"266\":2,\"380\":1,\"417\":1,\"419\":1,\"421\":1,\"427\":1,\"428\":1,\"431\":1,\"470\":1,\"472\":8,\"490\":1,\"699\":1,\"703\":1,\"716\":1}}],[\"explicit\",{\"1\":{\"83\":1}}],[\"explain\",{\"1\":{\"52\":2}}],[\"execution\",{\"1\":{\"7\":1}}],[\"error\",{\"1\":{\"46\":1,\"106\":4,\"410\":2,\"411\":1,\"412\":3}}],[\"entire\",{\"1\":{\"724\":1}}],[\"entropy\",{\"0\":{\"910\":1},\"1\":{\"7\":2,\"102\":1,\"190\":1,\"192\":1,\"207\":1,\"208\":1,\"274\":3,\"373\":1,\"384\":1,\"385\":4,\"386\":1,\"407\":3,\"418\":2,\"419\":1,\"587\":5,\"589\":4,\"592\":2,\"657\":1,\"893\":5,\"900\":3,\"932\":4}}],[\"enable\",{\"1\":{\"807\":5}}],[\"enables\",{\"1\":{\"622\":1}}],[\"enabled=false\",{\"1\":{\"213\":1}}],[\"envs\",{\"1\":{\"553\":3}}],[\"env\",{\"1\":{\"553\":1,\"554\":1}}],[\"environments\",{\"1\":{\"553\":1}}],[\"environ\",{\"0\":{\"520\":1},\"1\":{\"520\":1}}],[\"ensemble\",{\"0\":{\"343\":1},\"1\":{\"343\":1}}],[\"ensure\",{\"1\":{\"87\":1,\"595\":3,\"597\":3,\"696\":2}}],[\"en\",{\"1\":{\"305\":1,\"309\":1,\"332\":1}}],[\"enginnering\",{\"1\":{\"835\":1}}],[\"engineering技巧的时候\",{\"1\":{\"620\":1}}],[\"engineering技术\",{\"1\":{\"187\":1}}],[\"engineering呢\",{\"1\":{\"616\":1}}],[\"engineering的技巧\",{\"1\":{\"620\":1,\"621\":1}}],[\"engineering的原因\",{\"1\":{\"616\":1}}],[\"engineering的实践表明\",{\"1\":{\"605\":1}}],[\"engineering的效果达不到要求\",{\"1\":{\"601\":1}}],[\"engineering的方式\",{\"1\":{\"601\":1}}],[\"engineering的方式是一种相对来说容易上手的使用大模型的方式\",{\"1\":{\"601\":1}}],[\"engineering\",{\"0\":{\"615\":1,\"616\":1},\"1\":{\"409\":1,\"601\":1,\"615\":1,\"616\":1,\"619\":1,\"835\":3,\"836\":4}}],[\"english\",{\"1\":{\"224\":1,\"640\":3}}],[\"enqueue\",{\"0\":{\"364\":1},\"1\":{\"190\":1,\"192\":1,\"206\":1,\"362\":2,\"364\":2}}],[\"enclosing\",{\"1\":{\"444\":2}}],[\"enc5\",{\"1\":{\"123\":3}}],[\"enc4\",{\"1\":{\"123\":3}}],[\"enc3\",{\"1\":{\"123\":3}}],[\"enc2\",{\"1\":{\"123\":3}}],[\"enc\",{\"1\":{\"123\":6,\"190\":1,\"192\":1,\"255\":8,\"699\":2,\"899\":18,\"900\":9}}],[\"enc1\",{\"1\":{\"123\":3}}],[\"encoded\",{\"1\":{\"354\":1,\"713\":7}}],[\"encode\",{\"1\":{\"171\":1,\"213\":8,\"274\":1,\"408\":2,\"660\":1,\"663\":1,\"697\":3,\"742\":2,\"898\":1,\"931\":2,\"937\":2}}],[\"encoder模型结构图\",{\"1\":{\"747\":1}}],[\"encoderdecoder\",{\"1\":{\"742\":2}}],[\"encoderdecoder模型结构图\",{\"1\":{\"742\":1}}],[\"encoder中mlp\",{\"1\":{\"432\":1}}],[\"encoder中重复堆叠encoder\",{\"1\":{\"432\":1}}],[\"encoder的结构\",{\"1\":{\"429\":1}}],[\"encoderlayer模型结构图\",{\"1\":{\"746\":1}}],[\"encoderlayer\",{\"0\":{\"746\":1},\"1\":{\"420\":1,\"699\":1,\"746\":2}}],[\"encoder输出结果之后\",{\"1\":{\"431\":1}}],[\"encoder输出的embeddings里提取与input\",{\"1\":{\"417\":1}}],[\"encoder输入\",{\"1\":{\"264\":1}}],[\"encoder提取的图像embeddings\",{\"1\":{\"417\":1}}],[\"encoder提取图像特征\",{\"1\":{\"408\":1}}],[\"encoder引到vision\",{\"1\":{\"416\":1}}],[\"encoder不参与梯度运算\",{\"1\":{\"361\":1}}],[\"encoder进行初始化\",{\"1\":{\"361\":1}}],[\"encoder参数使用query\",{\"1\":{\"361\":1}}],[\"encoders\",{\"1\":{\"361\":1,\"699\":2}}],[\"encoder2\",{\"1\":{\"54\":1}}],[\"encoder\",{\"0\":{\"171\":1,\"429\":1,\"742\":1,\"744\":1,\"747\":1},\"1\":{\"17\":1,\"54\":3,\"59\":3,\"64\":2,\"70\":3,\"83\":7,\"94\":1,\"104\":2,\"122\":1,\"144\":1,\"165\":1,\"171\":4,\"172\":2,\"183\":1,\"187\":5,\"188\":4,\"190\":24,\"191\":7,\"192\":26,\"205\":13,\"206\":5,\"207\":15,\"208\":17,\"212\":1,\"213\":18,\"220\":3,\"222\":1,\"227\":1,\"252\":1,\"254\":1,\"255\":2,\"256\":1,\"260\":2,\"264\":3,\"268\":5,\"271\":3,\"273\":1,\"274\":5,\"280\":1,\"282\":1,\"361\":8,\"362\":4,\"363\":5,\"368\":2,\"369\":2,\"370\":2,\"377\":2,\"384\":1,\"385\":1,\"390\":1,\"397\":5,\"398\":5,\"399\":4,\"400\":4,\"401\":6,\"402\":2,\"403\":4,\"407\":8,\"408\":4,\"415\":2,\"417\":4,\"419\":2,\"420\":20,\"421\":5,\"427\":1,\"690\":3,\"721\":2,\"735\":3,\"741\":2,\"742\":4,\"747\":3,\"899\":2,\"945\":1,\"953\":1,\"963\":8,\"964\":1}}],[\"encoding=\",{\"1\":{\"595\":4,\"596\":1,\"597\":7,\"696\":3,\"697\":1}}],[\"encodings\",{\"1\":{\"213\":5}}],[\"encoding\",{\"0\":{\"37\":1,\"705\":1,\"706\":1,\"708\":1},\"1\":{\"94\":3,\"213\":5,\"234\":1,\"274\":1,\"594\":1,\"640\":1,\"681\":1,\"706\":1,\"707\":1,\"741\":1,\"963\":7}}],[\"endpoint\",{\"1\":{\"440\":1}}],[\"end\",{\"1\":{\"119\":10,\"121\":3,\"293\":4,\"340\":2,\"381\":13,\"410\":4,\"412\":4,\"597\":1,\"733\":12,\"734\":14,\"735\":5}}],[\"enrichment\",{\"1\":{\"87\":1}}],[\"enhance\",{\"1\":{\"65\":6,\"83\":4}}],[\"enumerate\",{\"1\":{\"53\":1,\"92\":2,\"105\":1,\"106\":1,\"107\":1,\"119\":2,\"137\":1,\"141\":1,\"145\":1,\"204\":1,\"265\":1,\"293\":3,\"359\":1,\"384\":1,\"385\":2,\"410\":1,\"412\":1,\"424\":2,\"431\":1,\"663\":1,\"697\":2,\"698\":1,\"700\":1,\"719\":1,\"918\":1,\"926\":1,\"934\":1}}],[\"pmf\",{\"1\":{\"846\":4,\"858\":1}}],[\"ppl\",{\"1\":{\"648\":2}}],[\"ppo\",{\"1\":{\"339\":1,\"654\":1,\"655\":2,\"656\":11,\"657\":8,\"658\":3}}],[\"pwd=vket\",{\"1\":{\"435\":1}}],[\"pwd=qvmq\",{\"1\":{\"424\":1}}],[\"p采样\",{\"1\":{\"421\":1}}],[\"png将dot格式文件转换为png图像\",{\"1\":{\"815\":1}}],[\"png\",{\"1\":{\"410\":1,\"412\":1,\"424\":2,\"582\":2,\"815\":4,\"918\":1,\"926\":2}}],[\"plug\",{\"1\":{\"655\":1}}],[\"plus\",{\"1\":{\"325\":1}}],[\"plm\",{\"1\":{\"611\":3,\"825\":1}}],[\"plot\",{\"1\":{\"424\":2,\"815\":4,\"816\":2}}],[\"plt\",{\"1\":{\"411\":4,\"412\":5,\"424\":7,\"582\":1,\"816\":11,\"930\":1,\"935\":5,\"939\":2,\"964\":6}}],[\"pl\",{\"1\":{\"383\":4,\"384\":11,\"385\":20,\"386\":11}}],[\"platform\",{\"1\":{\"827\":1}}],[\"place\",{\"1\":{\"697\":2}}],[\"plain\",{\"1\":{\"656\":1}}],[\"planes=none\",{\"1\":{\"122\":1}}],[\"planes=8\",{\"1\":{\"119\":1,\"120\":1,\"123\":2}}],[\"planes\",{\"1\":{\"119\":37,\"120\":28,\"121\":19,\"122\":23,\"123\":45}}],[\"playground\",{\"1\":{\"656\":1}}],[\"play\",{\"1\":{\"53\":1,\"655\":1}}],[\"ps\",{\"1\":{\"293\":2}}],[\"pseudo\",{\"1\":{\"202\":2}}],[\"pg\",{\"1\":{\"293\":3}}],[\"p=dropout\",{\"1\":{\"751\":1}}],[\"p=drop\",{\"1\":{\"380\":1,\"428\":1,\"431\":1}}],[\"p=top\",{\"1\":{\"188\":1,\"421\":1}}],[\"p=0\",{\"1\":{\"155\":1,\"188\":1,\"264\":1,\"293\":4,\"359\":1,\"421\":1,\"589\":2}}],[\"p4\",{\"1\":{\"123\":7}}],[\"p5\",{\"1\":{\"123\":6}}],[\"pxo2\",{\"1\":{\"122\":5}}],[\"pxo2=none\",{\"1\":{\"122\":1}}],[\"pxo1\",{\"1\":{\"122\":4}}],[\"pxo\",{\"1\":{\"119\":2,\"120\":3,\"121\":3,\"123\":3}}],[\"pj\",{\"1\":{\"119\":1}}],[\"pkill\",{\"1\":{\"107\":1}}],[\"pkl\",{\"1\":{\"92\":2,\"107\":2,\"254\":2}}],[\"py中导入核心类并初始化运算符重载\",{\"1\":{\"810\":1}}],[\"py文件\",{\"1\":{\"712\":1,\"810\":1}}],[\"pypi\",{\"1\":{\"557\":1}}],[\"pyplot\",{\"1\":{\"412\":1,\"816\":1,\"930\":1,\"964\":1}}],[\"py\",{\"1\":{\"107\":3,\"138\":1,\"141\":1,\"382\":1,\"395\":1,\"424\":1,\"444\":1,\"520\":1,\"582\":1,\"663\":1,\"712\":2,\"810\":6}}],[\"python会调用x的\",{\"1\":{\"809\":1}}],[\"python会根据操作数的类型选择不同的方法调用路径\",{\"1\":{\"809\":1}}],[\"python数值类型\",{\"1\":{\"809\":1}}],[\"python首先尝试调用左操作数a的\",{\"1\":{\"809\":1}}],[\"python中\",{\"1\":{\"809\":1}}],[\"python通过跟踪对象的引用次数来管理内存\",{\"1\":{\"806\":1}}],[\"python的内存管理主要依靠两种机制\",{\"1\":{\"806\":1}}],[\"python=3\",{\"1\":{\"550\":2,\"712\":1,\"739\":1}}],[\"python\",{\"0\":{\"443\":1,\"515\":1},\"1\":{\"107\":7,\"123\":1,\"424\":1,\"444\":6,\"447\":1,\"448\":1,\"450\":1,\"454\":1,\"463\":1,\"516\":1,\"520\":1,\"521\":3,\"550\":2,\"557\":7,\"712\":1,\"800\":1,\"809\":1,\"833\":1}}],[\"pytorch中使用比较多的tensor的阶为4\",{\"1\":{\"547\":1}}],[\"pytorch张量存储与访问原理\",{\"0\":{\"539\":1},\"1\":{\"539\":1}}],[\"pytorchlightning\",{\"1\":{\"381\":1}}],[\"pytorch版本\",{\"0\":{\"151\":1}}],[\"pytorch\",{\"0\":{\"465\":1,\"517\":1,\"928\":1},\"1\":{\"100\":1,\"130\":2,\"147\":2,\"176\":1,\"208\":1,\"254\":1,\"257\":1,\"259\":1,\"261\":1,\"267\":1,\"379\":1,\"380\":1,\"381\":5,\"382\":1,\"424\":2,\"425\":2,\"435\":1,\"466\":1,\"470\":1,\"471\":1,\"472\":1,\"473\":1,\"474\":1,\"486\":1,\"488\":1,\"492\":2,\"510\":2,\"518\":1,\"521\":5,\"541\":1,\"592\":1,\"692\":1,\"712\":3,\"734\":1,\"928\":1,\"932\":1,\"959\":2}}],[\"pd\",{\"1\":{\"92\":1}}],[\"pdf等格式\",{\"1\":{\"815\":1}}],[\"pdfs\",{\"1\":{\"332\":1}}],[\"pdf\",{\"1\":{\"84\":1,\"815\":1,\"836\":1,\"847\":2,\"865\":1,\"867\":1,\"868\":1,\"871\":1}}],[\"phenomenon\",{\"1\":{\"873\":1}}],[\"photos下的子目录名作为我们的候选待匹配分类文本列表\",{\"1\":{\"410\":1}}],[\"photos\",{\"1\":{\"410\":4,\"412\":2}}],[\"photos目录下读取出所有图片的路径\",{\"1\":{\"410\":1}}],[\"photo\",{\"1\":{\"408\":2,\"409\":2,\"410\":2,\"411\":1,\"412\":2}}],[\"phase\",{\"1\":{\"384\":5,\"385\":15,\"386\":5}}],[\"phrase\",{\"1\":{\"341\":1}}],[\"phrasing\",{\"1\":{\"87\":1}}],[\"philosophy\",{\"1\":{\"669\":1}}],[\"phi\",{\"1\":{\"65\":7,\"83\":7}}],[\"p3\",{\"1\":{\"70\":1,\"83\":1,\"123\":7}}],[\"p2\",{\"1\":{\"70\":1,\"83\":1,\"122\":2,\"123\":7,\"900\":3}}],[\"p1\",{\"1\":{\"70\":1,\"83\":1,\"122\":2,\"123\":5,\"900\":3}}],[\"p0\",{\"1\":{\"70\":1,\"83\":1,\"123\":7}}],[\"p+n\",{\"1\":{\"65\":1,\"83\":1}}],[\"ptx\",{\"1\":{\"655\":1,\"656\":4,\"657\":5,\"658\":1}}],[\"ptb\",{\"1\":{\"641\":1,\"696\":1}}],[\"pt较高\",{\"1\":{\"589\":1}}],[\"pth\",{\"1\":{\"435\":1,\"582\":4,\"700\":2,\"963\":2,\"964\":2}}],[\"ptr\",{\"1\":{\"190\":1,\"192\":1,\"205\":1,\"361\":1,\"364\":9,\"491\":4,\"492\":1}}],[\"pts\",{\"1\":{\"154\":2,\"156\":2}}],[\"ptv2\",{\"1\":{\"125\":1}}],[\"ptv1\",{\"1\":{\"125\":2}}],[\"pt\",{\"1\":{\"64\":1,\"106\":1,\"107\":1,\"187\":1,\"188\":1,\"190\":1,\"191\":1,\"192\":1,\"204\":1,\"293\":2,\"410\":2,\"412\":2,\"417\":1,\"589\":7,\"663\":1}}],[\"pca\",{\"1\":{\"574\":2}}],[\"pccn\",{\"1\":{\"110\":1}}],[\"pcd\",{\"1\":{\"107\":4}}],[\"pc\",{\"1\":{\"53\":2,\"82\":1,\"92\":1,\"107\":11}}],[\"punctuation\",{\"1\":{\"696\":1}}],[\"pup\",{\"1\":{\"239\":1}}],[\"pull\",{\"1\":{\"53\":1,\"92\":1}}],[\"push\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"p\",{\"1\":{\"53\":3,\"54\":2,\"59\":22,\"60\":7,\"65\":15,\"69\":6,\"70\":27,\"82\":3,\"83\":65,\"94\":19,\"96\":1,\"98\":4,\"99\":1,\"104\":6,\"106\":6,\"119\":23,\"120\":6,\"121\":12,\"122\":2,\"188\":1,\"261\":4,\"293\":2,\"421\":1,\"589\":3,\"595\":2,\"597\":2,\"610\":2,\"663\":1,\"697\":3,\"751\":5,\"899\":1,\"900\":4,\"908\":1,\"909\":5,\"926\":1,\"927\":1,\"932\":3}}],[\"piqa\",{\"1\":{\"668\":1}}],[\"pii\",{\"1\":{\"656\":1}}],[\"pip\",{\"1\":{\"557\":9,\"739\":1}}],[\"pin\",{\"1\":{\"293\":1,\"359\":1,\"382\":3}}],[\"pil\",{\"1\":{\"264\":1,\"293\":1,\"412\":1,\"424\":1,\"425\":2}}],[\"picture\",{\"1\":{\"187\":2,\"188\":2,\"190\":1}}],[\"pickle\",{\"1\":{\"92\":2,\"107\":3}}],[\"pi\",{\"1\":{\"119\":1}}],[\"pixels\",{\"1\":{\"264\":3}}],[\"pixelcnn不是唯一一种可用的拟合离散分布的模型\",{\"1\":{\"961\":1}}],[\"pixelcnn能输出某个像素的某个颜色通道取0~255中某个值的概率分布\",{\"1\":{\"956\":1}}],[\"pixelcnn能拟合一个离散的分布\",{\"1\":{\"956\":1}}],[\"pixelcnn的核心思想是给图像的子像素定义一个先后顺序\",{\"1\":{\"925\":1}}],[\"pixelcnn++\",{\"1\":{\"921\":1}}],[\"pixelcnn\",{\"0\":{\"920\":1},\"1\":{\"249\":1,\"920\":1,\"921\":9,\"923\":7,\"924\":3,\"925\":14,\"926\":8,\"964\":30}}],[\"pixel\",{\"1\":{\"52\":5,\"234\":1,\"329\":1,\"369\":2,\"921\":1}}],[\"piad数据集\",{\"1\":{\"73\":1}}],[\"piad\",{\"1\":{\"19\":2,\"25\":1,\"41\":1,\"44\":1,\"53\":1,\"82\":1}}],[\"piadv2\",{\"1\":{\"11\":1,\"40\":1,\"44\":1,\"45\":1,\"50\":1}}],[\"pepper\",{\"1\":{\"881\":2}}],[\"penn\",{\"1\":{\"696\":1}}],[\"penalty\",{\"1\":{\"188\":1}}],[\"penalty=repetition\",{\"1\":{\"188\":1}}],[\"penalty=1\",{\"1\":{\"188\":2,\"421\":1}}],[\"petaflop\",{\"1\":{\"658\":3}}],[\"peft\",{\"1\":{\"607\":1,\"614\":1}}],[\"peft是目前业界比较流行的微调方案\",{\"1\":{\"603\":1}}],[\"peft也是目前比较主流的微调方案\",{\"1\":{\"602\":1}}],[\"peft主要想解决的问题\",{\"1\":{\"602\":1}}],[\"peco\",{\"1\":{\"216\":1}}],[\"people\",{\"1\":{\"52\":6,\"55\":4,\"341\":1}}],[\"perplexity\",{\"1\":{\"640\":1,\"641\":1}}],[\"perm\",{\"1\":{\"963\":6}}],[\"permissionerror\",{\"1\":{\"461\":1}}],[\"permutation\",{\"1\":{\"149\":1,\"160\":2}}],[\"permute维度\",{\"1\":{\"60\":2}}],[\"permute后\",{\"1\":{\"59\":1}}],[\"permute\",{\"0\":{\"468\":1},\"1\":{\"54\":2,\"56\":2,\"58\":1,\"59\":2,\"60\":2,\"65\":2,\"69\":1,\"83\":3,\"137\":4,\"141\":4,\"145\":5,\"146\":1,\"380\":1,\"401\":1,\"418\":1,\"420\":1,\"430\":2,\"468\":2,\"469\":2,\"470\":1,\"490\":1,\"494\":1,\"582\":1,\"710\":3,\"724\":2,\"926\":1,\"963\":3,\"964\":1}}],[\"per\",{\"1\":{\"143\":1,\"144\":2,\"264\":2,\"265\":1,\"382\":1,\"385\":12,\"430\":6,\"710\":1,\"712\":2}}],[\"performing\",{\"1\":{\"213\":1}}],[\"performs\",{\"1\":{\"52\":2}}],[\"performance\",{\"0\":{\"49\":1},\"1\":{\"690\":1}}],[\"perspective\",{\"1\":{\"657\":1}}],[\"perspectives\",{\"1\":{\"12\":1}}],[\"persistent=false\",{\"1\":{\"892\":1}}],[\"persistent\",{\"1\":{\"474\":2}}],[\"person\",{\"1\":{\"52\":8,\"55\":2,\"341\":1,\"408\":1}}],[\"perception\",{\"1\":{\"35\":1,\"52\":2}}],[\"poisson\",{\"0\":{\"858\":1}}],[\"pointer\",{\"1\":{\"361\":1,\"364\":1,\"544\":1}}],[\"pointfeat\",{\"1\":{\"154\":2}}],[\"pointtransformerv2\",{\"1\":{\"124\":1}}],[\"pointtransformerseg\",{\"1\":{\"123\":1}}],[\"pointtransformerblock\",{\"1\":{\"120\":3}}],[\"pointtransformerlayer\",{\"1\":{\"119\":2,\"120\":1}}],[\"pointops\",{\"1\":{\"119\":2,\"121\":2,\"122\":1}}],[\"pointcept\",{\"1\":{\"124\":1}}],[\"pointcnn\",{\"1\":{\"110\":1,\"157\":1}}],[\"pointconv\",{\"1\":{\"110\":1}}],[\"pointcloud\",{\"1\":{\"107\":2,\"137\":2}}],[\"pointweb\",{\"1\":{\"110\":1}}],[\"point点云数据\",{\"1\":{\"94\":1}}],[\"pointrefer模型结构图\",{\"1\":{\"94\":1}}],[\"pointrefer\",{\"1\":{\"94\":5,\"100\":1,\"102\":1,\"104\":1,\"106\":2,\"107\":4}}],[\"pointnetdensecls\",{\"1\":{\"156\":3}}],[\"pointnetcls\",{\"1\":{\"155\":3}}],[\"pointnetfeat\",{\"1\":{\"154\":3,\"155\":4,\"156\":3}}],[\"pointnetfeaturepropagation\",{\"1\":{\"59\":3,\"70\":5,\"83\":3,\"145\":3,\"146\":4}}],[\"pointnet网络模型结构图\",{\"1\":{\"151\":1}}],[\"pointnet后\",{\"1\":{\"138\":3}}],[\"pointnetsetabstractionmsg\",{\"1\":{\"141\":5}}],[\"pointnetsetabstraction\",{\"1\":{\"137\":3,\"138\":3,\"141\":1,\"146\":4}}],[\"pointnet来提取局部区域中的特征\",{\"1\":{\"136\":1}}],[\"pointnet将局部区域编码为特征向量\",{\"1\":{\"133\":1}}],[\"pointnet2\",{\"1\":{\"130\":2,\"138\":1,\"141\":2}}],[\"pointnet\",{\"0\":{\"136\":1},\"1\":{\"110\":1,\"130\":1,\"132\":1,\"133\":2,\"137\":2,\"141\":1,\"143\":1,\"147\":2,\"148\":1,\"150\":4,\"152\":4,\"153\":2,\"154\":1,\"155\":1,\"156\":1,\"157\":26,\"160\":2}}],[\"pointnet编码点云\",{\"1\":{\"83\":1}}],[\"pointnet++提出了密度自适应pointnet层\",{\"1\":{\"139\":1}}],[\"pointnet++提取特征\",{\"1\":{\"78\":1}}],[\"pointnet++应用pointnet递归地对输入集进行嵌套分区\",{\"1\":{\"131\":1}}],[\"pointnet++选择pointnet作为局部特征学习器\",{\"1\":{\"131\":1}}],[\"pointnet++在进行点集划分时\",{\"1\":{\"131\":1}}],[\"pointnet++的下一个任务是学习这些子集\",{\"1\":{\"131\":1}}],[\"pointnet++需要一种方法来有效地将点云分割成多个部分\",{\"1\":{\"131\":1}}],[\"pointnet++\",{\"1\":{\"22\":1,\"32\":1,\"38\":1,\"46\":1,\"54\":1,\"64\":1,\"83\":2,\"94\":3,\"95\":1,\"110\":1,\"131\":1,\"137\":3,\"138\":2,\"141\":2,\"143\":2,\"144\":1,\"145\":1,\"146\":1,\"157\":4}}],[\"points2\",{\"1\":{\"143\":2,\"145\":7}}],[\"points1\",{\"1\":{\"143\":2,\"145\":7}}],[\"points\",{\"1\":{\"12\":1,\"53\":9,\"59\":1,\"64\":1,\"82\":9,\"87\":1,\"107\":18,\"137\":68,\"138\":7,\"141\":39,\"143\":2,\"144\":7,\"145\":15,\"146\":27,\"149\":2,\"152\":1}}],[\"point\",{\"0\":{\"108\":1,\"113\":1,\"115\":1,\"120\":1,\"123\":1,\"124\":1},\"1\":{\"7\":3,\"40\":1,\"52\":3,\"53\":28,\"54\":1,\"64\":8,\"65\":6,\"70\":3,\"82\":14,\"83\":3,\"92\":6,\"94\":3,\"100\":3,\"105\":3,\"106\":6,\"107\":15,\"108\":3,\"109\":3,\"113\":1,\"115\":2,\"116\":1,\"117\":1,\"120\":5,\"121\":4,\"123\":3,\"124\":2,\"125\":2,\"134\":1,\"137\":7,\"141\":2,\"143\":2,\"144\":2,\"150\":1,\"152\":2,\"157\":2,\"159\":1}}],[\"pop\",{\"1\":{\"787\":1,\"801\":1,\"803\":1,\"805\":1,\"807\":1,\"815\":2}}],[\"pope\",{\"1\":{\"305\":1,\"310\":1}}],[\"portion\",{\"1\":{\"713\":1}}],[\"ported\",{\"1\":{\"435\":1}}],[\"portrait\",{\"1\":{\"408\":1}}],[\"policy\",{\"1\":{\"339\":3,\"654\":1,\"656\":1}}],[\"polar\",{\"1\":{\"202\":1}}],[\"power\",{\"1\":{\"604\":1,\"899\":1}}],[\"pow\",{\"1\":{\"213\":2,\"809\":5,\"811\":2,\"932\":1}}],[\"pond\",{\"1\":{\"202\":1}}],[\"posed\",{\"1\":{\"878\":1}}],[\"posterior\",{\"1\":{\"852\":1,\"877\":2}}],[\"postech\",{\"1\":{\"108\":1}}],[\"possible\",{\"1\":{\"597\":1}}],[\"pos=bool\",{\"1\":{\"265\":1,\"266\":2}}],[\"pos=self\",{\"1\":{\"94\":1,\"100\":1}}],[\"positive和\",{\"1\":{\"635\":1}}],[\"positives\",{\"1\":{\"590\":1}}],[\"positive\",{\"1\":{\"102\":6,\"190\":1,\"359\":1,\"362\":2,\"588\":1,\"590\":4}}],[\"positions\",{\"1\":{\"734\":7,\"735\":1}}],[\"positions=none\",{\"1\":{\"734\":2}}],[\"position=cache\",{\"1\":{\"663\":1}}],[\"position=none\",{\"1\":{\"663\":1}}],[\"positional\",{\"0\":{\"705\":1,\"706\":1},\"1\":{\"266\":1,\"428\":1,\"445\":1,\"706\":1,\"707\":1,\"741\":1}}],[\"position\",{\"0\":{\"708\":1,\"709\":1,\"710\":1},\"1\":{\"52\":2,\"100\":1,\"137\":5,\"141\":2,\"208\":2,\"264\":2,\"380\":11,\"384\":4,\"385\":12,\"397\":2,\"403\":2,\"419\":9,\"420\":2,\"633\":1,\"663\":16,\"692\":1,\"710\":25,\"716\":12,\"721\":2,\"722\":2,\"731\":2,\"734\":2,\"736\":2,\"737\":6,\"823\":1,\"892\":1}}],[\"pos\",{\"1\":{\"100\":8,\"190\":7,\"192\":2,\"207\":2,\"265\":7,\"266\":14,\"362\":2,\"380\":11,\"384\":1,\"385\":2,\"386\":7,\"428\":5,\"431\":5,\"697\":10,\"698\":6,\"699\":9,\"700\":2,\"709\":10,\"710\":10,\"736\":1,\"892\":2,\"893\":2,\"898\":1,\"900\":4}}],[\"pos1d\",{\"1\":{\"94\":1,\"100\":1}}],[\"po\",{\"1\":{\"60\":6}}],[\"pooled\",{\"1\":{\"397\":5,\"720\":4,\"721\":2,\"722\":4,\"730\":2,\"731\":2,\"733\":2,\"737\":4}}],[\"pooler\",{\"1\":{\"272\":2,\"273\":5,\"384\":1,\"397\":3,\"498\":1,\"699\":7,\"721\":2}}],[\"poolers\",{\"1\":{\"272\":1}}],[\"pooling\",{\"0\":{\"501\":1},\"1\":{\"116\":1,\"124\":1,\"137\":1,\"143\":1,\"150\":5,\"152\":1,\"157\":7,\"160\":3,\"190\":2,\"191\":1,\"192\":2,\"238\":1,\"272\":1,\"273\":1,\"403\":1,\"501\":6,\"502\":1}}],[\"pool后\",{\"1\":{\"60\":1}}],[\"pool\",{\"1\":{\"60\":2,\"70\":4,\"83\":9,\"121\":2,\"202\":1,\"274\":3,\"699\":2,\"720\":1}}],[\"pour\",{\"1\":{\"30\":1,\"31\":1,\"47\":1,\"52\":4,\"53\":1,\"55\":2,\"56\":1,\"82\":1,\"92\":1}}],[\"palm\",{\"1\":{\"668\":1,\"669\":1,\"822\":1,\"823\":1}}],[\"palm方案\",{\"1\":{\"667\":1}}],[\"palm等依赖未公开数据\",{\"1\":{\"666\":1}}],[\"package\",{\"1\":{\"557\":1,\"810\":1}}],[\"packages\",{\"1\":{\"557\":1}}],[\"pandas\",{\"1\":{\"550\":1}}],[\"pan\",{\"1\":{\"424\":1,\"435\":1}}],[\"page\",{\"1\":{\"408\":1}}],[\"pass\",{\"1\":{\"456\":1,\"457\":1,\"458\":1,\"668\":1,\"697\":1,\"747\":1,\"815\":1}}],[\"passed\",{\"1\":{\"274\":2,\"713\":1,\"893\":1}}],[\"past\",{\"1\":{\"207\":1,\"419\":3,\"420\":27,\"663\":27}}],[\"padded\",{\"1\":{\"477\":6}}],[\"padding位置为0\",{\"1\":{\"384\":1}}],[\"padding=3\",{\"1\":{\"964\":1}}],[\"padding=2\",{\"1\":{\"926\":1}}],[\"padding=kernel\",{\"1\":{\"926\":1}}],[\"padding=true\",{\"1\":{\"410\":1,\"412\":1}}],[\"padding=1\",{\"1\":{\"255\":4,\"899\":2,\"963\":4,\"964\":1}}],[\"padding=\",{\"1\":{\"64\":1,\"187\":1,\"190\":1,\"191\":1,\"192\":1,\"204\":1,\"417\":1}}],[\"padding\",{\"0\":{\"703\":1},\"1\":{\"64\":2,\"67\":3,\"94\":1,\"98\":4,\"99\":1,\"100\":8,\"119\":2,\"187\":1,\"188\":1,\"206\":1,\"274\":4,\"397\":2,\"417\":1,\"420\":6,\"477\":2,\"663\":1,\"698\":1,\"699\":1,\"713\":6,\"716\":1,\"721\":1,\"722\":1,\"741\":1,\"892\":4,\"893\":2,\"895\":1,\"898\":3,\"900\":2,\"926\":1}}],[\"pad\",{\"0\":{\"477\":1},\"1\":{\"187\":1,\"188\":5,\"192\":1,\"208\":2,\"274\":4,\"420\":1,\"421\":2,\"477\":10,\"597\":3,\"697\":2,\"698\":9,\"699\":3,\"703\":11,\"713\":3,\"892\":1,\"893\":1,\"898\":1}}],[\"parse\",{\"1\":{\"918\":1}}],[\"parser\",{\"1\":{\"293\":3,\"918\":12}}],[\"paris\",{\"1\":{\"735\":3}}],[\"paragraph\",{\"1\":{\"696\":7}}],[\"paraphrase\",{\"1\":{\"634\":1}}],[\"para\",{\"1\":{\"435\":2}}],[\"parameter\",{\"1\":{\"190\":1,\"192\":1,\"205\":1,\"213\":3,\"320\":1,\"380\":6,\"407\":1,\"427\":1,\"428\":2,\"431\":2,\"602\":1,\"604\":1,\"607\":1,\"710\":2,\"729\":1,\"900\":1}}],[\"parameters\",{\"1\":{\"104\":2,\"187\":1,\"190\":1,\"192\":1,\"213\":1,\"293\":3,\"359\":1,\"361\":2,\"363\":2,\"435\":1,\"474\":1,\"700\":2,\"721\":1,\"918\":2,\"926\":2,\"934\":1,\"938\":1,\"963\":1,\"964\":1}}],[\"params=model\",{\"1\":{\"187\":1,\"190\":1,\"192\":1}}],[\"params\",{\"1\":{\"104\":3,\"190\":1,\"192\":1,\"205\":1,\"293\":3}}],[\"param\",{\"1\":{\"104\":2,\"213\":2,\"293\":1,\"361\":5,\"363\":5,\"424\":5,\"426\":6}}],[\"part\",{\"1\":{\"52\":11,\"83\":6,\"107\":3}}],[\"partitioning\",{\"1\":{\"131\":2}}],[\"partition\",{\"1\":{\"124\":1,\"850\":1}}],[\"partitions\",{\"0\":{\"44\":1}}],[\"partial\",{\"1\":{\"12\":1,\"380\":1,\"382\":1,\"431\":1,\"447\":1}}],[\"parts\",{\"1\":{\"11\":2,\"736\":1}}],[\"paper\",{\"1\":{\"84\":1,\"435\":1,\"724\":1}}],[\"papers\",{\"1\":{\"84\":1}}],[\"pa\",{\"1\":{\"70\":5,\"83\":7}}],[\"pattern\",{\"1\":{\"478\":2}}],[\"patch16\",{\"1\":{\"435\":6}}],[\"patch14\",{\"1\":{\"410\":2,\"412\":1}}],[\"patch大小的重要性\",{\"0\":{\"288\":1}}],[\"patchembedding\",{\"1\":{\"380\":1}}],[\"patchembed\",{\"1\":{\"266\":1,\"380\":4,\"426\":2}}],[\"patches=args\",{\"1\":{\"264\":3}}],[\"patches=none\",{\"1\":{\"263\":1}}],[\"patches=4\",{\"1\":{\"263\":1}}],[\"patches\",{\"1\":{\"97\":3,\"131\":1,\"263\":26,\"264\":7,\"266\":5,\"380\":7,\"426\":5,\"427\":6,\"428\":4,\"430\":11,\"431\":4,\"435\":1,\"436\":1,\"900\":5}}],[\"patch数量\",{\"1\":{\"263\":1}}],[\"patch\",{\"0\":{\"231\":1},\"1\":{\"65\":5,\"83\":2,\"97\":9,\"112\":1,\"171\":1,\"205\":2,\"206\":1,\"212\":8,\"213\":7,\"214\":8,\"215\":11,\"217\":2,\"220\":1,\"223\":3,\"224\":1,\"229\":1,\"230\":1,\"231\":8,\"232\":1,\"233\":6,\"234\":12,\"236\":4,\"238\":2,\"242\":1,\"243\":3,\"247\":1,\"250\":2,\"252\":1,\"260\":1,\"262\":1,\"263\":17,\"264\":11,\"265\":3,\"266\":34,\"272\":1,\"280\":3,\"286\":3,\"288\":2,\"293\":4,\"319\":1,\"369\":1,\"371\":5,\"376\":2,\"380\":42,\"384\":1,\"385\":2,\"388\":1,\"391\":1,\"393\":1,\"402\":1,\"426\":12,\"427\":11,\"428\":5,\"431\":5,\"435\":1,\"582\":11,\"899\":1,\"900\":20,\"963\":1}}],[\"path$bert\",{\"1\":{\"712\":2}}],[\"paths\",{\"1\":{\"410\":17,\"411\":6,\"412\":21}}],[\"path=prev\",{\"1\":{\"712\":1}}],[\"path=val\",{\"1\":{\"425\":1}}],[\"path=train\",{\"1\":{\"425\":1}}],[\"path=0\",{\"1\":{\"380\":1}}],[\"path=dpr\",{\"1\":{\"380\":1}}],[\"path=args\",{\"1\":{\"265\":1}}],[\"path=\",{\"1\":{\"213\":2}}],[\"path\",{\"1\":{\"52\":6,\"53\":30,\"82\":24,\"92\":3,\"106\":3,\"107\":5,\"187\":3,\"213\":6,\"264\":1,\"265\":1,\"293\":4,\"380\":16,\"410\":12,\"411\":2,\"412\":14,\"424\":30,\"425\":5,\"428\":1,\"429\":7,\"431\":2,\"582\":1,\"595\":6,\"597\":10,\"696\":12,\"697\":5,\"700\":5,\"815\":8,\"816\":1}}],[\"pair+nsp\",{\"1\":{\"681\":2}}],[\"pairwise\",{\"1\":{\"656\":1}}],[\"pair是否match\",{\"1\":{\"419\":1}}],[\"pair=2\",{\"1\":{\"53\":1,\"82\":1}}],[\"pairs\",{\"1\":{\"7\":1,\"91\":1,\"190\":1,\"192\":1,\"205\":1,\"595\":9,\"596\":4,\"597\":10,\"634\":1,\"655\":1,\"656\":1,\"657\":1,\"668\":1,\"670\":1,\"698\":14}}],[\"pair\",{\"1\":{\"7\":1,\"53\":5,\"82\":3,\"594\":1,\"595\":17,\"596\":14,\"597\":42,\"640\":1,\"713\":2}}],[\"priming\",{\"1\":{\"895\":6}}],[\"prior\",{\"1\":{\"852\":1,\"877\":1}}],[\"priority\",{\"1\":{\"809\":2}}],[\"price函数为例\",{\"1\":{\"815\":1}}],[\"price函数求导\",{\"1\":{\"811\":1}}],[\"print\",{\"1\":{\"52\":4,\"106\":1,\"107\":2,\"204\":1,\"213\":3,\"265\":2,\"293\":2,\"410\":6,\"411\":4,\"412\":10,\"424\":3,\"435\":1,\"440\":1,\"441\":1,\"447\":2,\"448\":1,\"449\":3,\"451\":3,\"452\":3,\"453\":2,\"454\":8,\"455\":2,\"456\":1,\"459\":3,\"461\":2,\"463\":1,\"471\":1,\"477\":4,\"480\":6,\"481\":2,\"482\":2,\"486\":1,\"490\":5,\"491\":6,\"513\":2,\"514\":1,\"516\":4,\"540\":1,\"542\":7,\"544\":4,\"545\":10,\"582\":1,\"660\":3,\"663\":3,\"696\":2,\"700\":3,\"757\":2,\"766\":1,\"807\":2,\"808\":8,\"809\":4,\"811\":3,\"815\":1,\"816\":2,\"892\":1,\"918\":2,\"926\":2,\"934\":1,\"938\":1,\"963\":4,\"964\":4}}],[\"prc\",{\"1\":{\"571\":1}}],[\"prod\",{\"1\":{\"918\":4}}],[\"product\",{\"1\":{\"160\":1,\"524\":1,\"694\":2,\"724\":1,\"751\":1,\"877\":1}}],[\"procedure\",{\"1\":{\"697\":1,\"698\":1}}],[\"processor\",{\"1\":{\"410\":3,\"412\":3}}],[\"processing\",{\"1\":{\"409\":1,\"607\":1}}],[\"process\",{\"1\":{\"213\":6,\"696\":3,\"742\":1}}],[\"promt\",{\"1\":{\"663\":2}}],[\"prompt敏感性高\",{\"1\":{\"649\":1}}],[\"prompt依赖性强\",{\"1\":{\"649\":1}}],[\"prompt写得好不好\",{\"1\":{\"616\":1}}],[\"prompt太长会因超过限制而被截断\",{\"1\":{\"601\":1}}],[\"prompting有一个直观的认知\",{\"1\":{\"622\":1}}],[\"prompting技巧\",{\"1\":{\"621\":1}}],[\"prompting\",{\"1\":{\"409\":1,\"607\":1,\"620\":1,\"622\":1,\"640\":1}}],[\"prompt+pre\",{\"1\":{\"187\":1}}],[\"prompt=config\",{\"1\":{\"187\":1}}],[\"prompt\",{\"0\":{\"346\":1,\"604\":1,\"615\":1},\"1\":{\"11\":4,\"35\":2,\"36\":2,\"52\":9,\"187\":10,\"188\":8,\"346\":11,\"409\":3,\"601\":3,\"604\":5,\"605\":1,\"607\":1,\"610\":2,\"615\":1,\"616\":2,\"618\":1,\"620\":1,\"649\":1,\"650\":1,\"656\":8,\"657\":1,\"658\":1,\"663\":4,\"832\":1,\"834\":1,\"835\":10,\"836\":12,\"895\":2,\"898\":1}}],[\"prompts\",{\"1\":{\"11\":1,\"421\":1,\"605\":1,\"656\":3,\"657\":1}}],[\"prototypical\",{\"1\":{\"650\":1}}],[\"proximal\",{\"1\":{\"339\":1,\"656\":1}}],[\"pro\",{\"1\":{\"325\":1,\"823\":3}}],[\"prob=1\",{\"1\":{\"894\":1}}],[\"prob=0\",{\"1\":{\"893\":1}}],[\"probabilistic\",{\"1\":{\"822\":1,\"877\":1}}],[\"probabilities\",{\"1\":{\"724\":1}}],[\"probability=\",{\"1\":{\"382\":1}}],[\"probability\",{\"0\":{\"850\":1},\"1\":{\"205\":2,\"208\":6,\"846\":1,\"847\":1,\"852\":2}}],[\"problems\",{\"0\":{\"878\":1}}],[\"problem\",{\"1\":{\"654\":1,\"878\":1}}],[\"probs\",{\"1\":{\"401\":4,\"408\":3,\"420\":7,\"724\":5,\"896\":3,\"926\":2,\"964\":2}}],[\"prob\",{\"1\":{\"382\":1,\"400\":1,\"716\":1,\"718\":1,\"722\":1,\"724\":1,\"725\":1,\"736\":1,\"737\":1,\"893\":6,\"894\":1}}],[\"probing\",{\"1\":{\"215\":1,\"320\":1,\"352\":1}}],[\"provided\",{\"1\":{\"122\":1,\"597\":2}}],[\"projected\",{\"1\":{\"751\":1}}],[\"projects\",{\"1\":{\"414\":1}}],[\"projections\",{\"1\":{\"751\":1}}],[\"projection转化成visual\",{\"1\":{\"392\":1}}],[\"projection方法将输入图片切片投影提取特征\",{\"1\":{\"391\":1}}],[\"projection来做visual\",{\"1\":{\"388\":1,\"391\":1}}],[\"projection的多模态预训练模型\",{\"1\":{\"388\":1}}],[\"projection\",{\"1\":{\"205\":2,\"293\":1,\"751\":1,\"892\":1,\"899\":1}}],[\"proj\",{\"1\":{\"56\":6,\"58\":3,\"59\":4,\"60\":5,\"69\":32,\"83\":5,\"104\":2,\"107\":2,\"190\":18,\"191\":4,\"192\":12,\"205\":8,\"206\":4,\"213\":1,\"266\":2,\"380\":9,\"385\":4,\"407\":2,\"417\":2,\"426\":2,\"429\":1,\"430\":6,\"582\":2,\"663\":2,\"743\":2}}],[\"property\",{\"1\":{\"382\":5,\"808\":4}}],[\"properties\",{\"1\":{\"11\":1,\"279\":2}}],[\"propagation\",{\"1\":{\"83\":1,\"143\":4,\"144\":1,\"145\":1,\"146\":2}}],[\"propose\",{\"1\":{\"7\":1}}],[\"prev\",{\"1\":{\"712\":1,\"894\":2,\"895\":1}}],[\"preview\",{\"0\":{\"357\":1},\"1\":{\"823\":3}}],[\"precision\",{\"1\":{\"590\":1,\"865\":1}}],[\"precomputed\",{\"1\":{\"274\":1,\"724\":1}}],[\"present\",{\"1\":{\"420\":2}}],[\"press\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"prepare\",{\"1\":{\"381\":1,\"382\":5,\"595\":2,\"597\":4,\"698\":3}}],[\"preparation\",{\"1\":{\"11\":1}}],[\"pretext\",{\"0\":{\"248\":1}}],[\"pretraining\",{\"0\":{\"247\":1,\"250\":1,\"251\":1,\"272\":1,\"385\":1,\"393\":1},\"1\":{\"219\":2,\"264\":1,\"265\":1,\"271\":1,\"380\":1,\"383\":1,\"656\":1,\"676\":1}}],[\"pretrain\",{\"1\":{\"192\":3,\"656\":1}}],[\"pretrainedtokenizer\",{\"1\":{\"713\":1}}],[\"pretrained=false\",{\"1\":{\"511\":1}}],[\"pretrained=true\",{\"1\":{\"510\":1,\"511\":1}}],[\"pretrained=config\",{\"1\":{\"187\":1,\"190\":1}}],[\"pretrained\",{\"1\":{\"52\":2,\"187\":1,\"190\":1,\"192\":2,\"205\":2,\"382\":1,\"410\":4,\"412\":4,\"660\":2,\"663\":1}}],[\"prefixlm\",{\"1\":{\"268\":1}}],[\"prefix\",{\"0\":{\"605\":1},\"1\":{\"188\":1,\"346\":2,\"453\":3,\"605\":5,\"610\":2}}],[\"pred的进行pad填充\",{\"1\":{\"700\":1}}],[\"predictor\",{\"1\":{\"280\":1,\"285\":1}}],[\"predicted\",{\"1\":{\"107\":1,\"410\":6,\"412\":6}}],[\"predict\",{\"1\":{\"107\":2,\"187\":1,\"220\":1,\"223\":1,\"381\":8,\"382\":1,\"409\":1,\"607\":1,\"694\":1}}],[\"predictions\",{\"1\":{\"730\":2}}],[\"prediction\",{\"0\":{\"692\":1},\"1\":{\"7\":1,\"14\":1,\"102\":1,\"107\":1,\"208\":5,\"403\":7,\"413\":1,\"420\":6,\"683\":1,\"690\":1,\"713\":1,\"730\":2,\"731\":4,\"893\":1}}],[\"pred\",{\"1\":{\"102\":10,\"106\":4,\"107\":24,\"431\":5,\"698\":7,\"699\":7}}],[\"pre\",{\"0\":{\"198\":1,\"236\":1,\"374\":1},\"1\":{\"14\":1,\"120\":1,\"164\":2,\"165\":1,\"179\":1,\"213\":1,\"226\":2,\"237\":1,\"286\":1,\"340\":1,\"367\":2,\"376\":2,\"378\":1,\"406\":1,\"409\":1,\"431\":3,\"435\":2,\"607\":1,\"611\":1,\"624\":1,\"625\":1,\"667\":1,\"823\":2}}],[\"i+64\",{\"1\":{\"964\":1}}],[\"i+1\",{\"1\":{\"489\":2,\"595\":1,\"596\":1,\"597\":2,\"660\":2,\"697\":1,\"964\":1}}],[\"i3\",{\"1\":{\"924\":4}}],[\"i==j\",{\"1\":{\"900\":1}}],[\"ill\",{\"1\":{\"878\":1}}],[\"iloc\",{\"1\":{\"92\":2}}],[\"ipython\",{\"1\":{\"815\":1}}],[\"i证明小规模微调即可显著提升任务适应性\",{\"1\":{\"669\":1}}],[\"ibarz\",{\"1\":{\"655\":1}}],[\"ibot\",{\"1\":{\"246\":1}}],[\"icmlm和convirt仅在10万级别的数据上训练了几天\",{\"1\":{\"413\":1}}],[\"icmlm基于语言掩码的方法\",{\"1\":{\"413\":1}}],[\"irtr\",{\"1\":{\"383\":2}}],[\"iid\",{\"1\":{\"382\":5}}],[\"iq\",{\"1\":{\"293\":2}}],[\"igpt\",{\"1\":{\"247\":1,\"250\":1}}],[\"ignored\",{\"1\":{\"734\":4}}],[\"ignore\",{\"1\":{\"208\":1,\"213\":1,\"274\":1,\"382\":1,\"384\":2,\"412\":1,\"700\":1,\"731\":1,\"734\":3}}],[\"itg\",{\"0\":{\"420\":1},\"1\":{\"305\":1}}],[\"it\",{\"1\":{\"274\":1,\"293\":2,\"309\":1}}],[\"iterate\",{\"1\":{\"886\":1}}],[\"iterable\",{\"1\":{\"265\":1,\"516\":3}}],[\"iter\",{\"1\":{\"816\":9,\"935\":1}}],[\"iters是iterations的缩写\",{\"1\":{\"816\":1}}],[\"iters\",{\"1\":{\"213\":3,\"816\":6}}],[\"items\",{\"1\":{\"382\":1,\"383\":2,\"386\":1,\"424\":1,\"595\":4,\"597\":3,\"697\":2}}],[\"item\",{\"1\":{\"92\":5,\"106\":1,\"121\":8,\"122\":4,\"190\":2,\"192\":2,\"207\":2,\"265\":1,\"380\":1,\"386\":2,\"419\":2,\"424\":5,\"663\":1,\"700\":1,\"715\":1,\"918\":2,\"926\":1,\"934\":1,\"938\":1,\"963\":2,\"964\":1}}],[\"ita\",{\"1\":{\"190\":4,\"192\":4,\"204\":2,\"206\":1}}],[\"itm分类头\",{\"1\":{\"190\":1}}],[\"itm\",{\"0\":{\"172\":1,\"201\":1,\"207\":1,\"419\":1},\"1\":{\"172\":1,\"173\":2,\"190\":11,\"191\":7,\"192\":12,\"198\":2,\"201\":2,\"204\":2,\"205\":2,\"207\":8,\"305\":1,\"383\":3,\"385\":3,\"386\":26,\"393\":1,\"419\":12}}],[\"itc\",{\"0\":{\"172\":1,\"206\":1,\"418\":1},\"1\":{\"172\":1,\"173\":1,\"190\":1,\"191\":1,\"192\":1,\"194\":1,\"198\":1,\"202\":3,\"206\":1,\"207\":1,\"305\":1,\"383\":6,\"385\":66,\"386\":5,\"418\":1}}],[\"io\",{\"0\":{\"366\":1},\"1\":{\"129\":1,\"366\":3,\"854\":1,\"899\":4}}],[\"iouloss\",{\"1\":{\"588\":2}}],[\"iou\",{\"0\":{\"588\":1},\"1\":{\"106\":27,\"586\":2,\"588\":21}}],[\"ian\",{\"1\":{\"918\":1}}],[\"ia\",{\"1\":{\"83\":6}}],[\"ia^t\",{\"1\":{\"83\":1}}],[\"iag网络输入为四元组\",{\"1\":{\"78\":1}}],[\"iag框架\",{\"1\":{\"73\":1}}],[\"iag\",{\"1\":{\"46\":1,\"47\":1,\"83\":1}}],[\"iagnet\",{\"0\":{\"71\":1},\"1\":{\"31\":1,\"71\":1}}],[\"i2t维度为\",{\"1\":{\"207\":1}}],[\"i2t\",{\"1\":{\"190\":13,\"192\":13,\"206\":9,\"207\":4,\"383\":1,\"385\":10,\"386\":6,\"418\":5,\"419\":4}}],[\"i2\",{\"1\":{\"56\":4,\"69\":4,\"924\":6}}],[\"i1\",{\"1\":{\"56\":4,\"69\":4,\"924\":8}}],[\"idf\",{\"1\":{\"507\":1}}],[\"id为文本\",{\"1\":{\"421\":1}}],[\"id=198\",{\"1\":{\"663\":1}}],[\"id=model\",{\"1\":{\"410\":1,\"412\":1}}],[\"id=self\",{\"1\":{\"188\":4,\"421\":2}}],[\"idw\",{\"1\":{\"143\":1,\"145\":2}}],[\"iden\",{\"1\":{\"152\":4}}],[\"identity\",{\"1\":{\"120\":5,\"293\":2,\"380\":1,\"426\":3,\"429\":1,\"431\":1}}],[\"identify\",{\"1\":{\"87\":1}}],[\"idea\",{\"0\":{\"17\":1}}],[\"ids作用图解\",{\"1\":{\"713\":1}}],[\"ids=position\",{\"1\":{\"208\":1,\"397\":1,\"403\":1,\"420\":1,\"721\":1,\"722\":1,\"731\":1,\"734\":1,\"736\":1,\"737\":1}}],[\"ids=token\",{\"1\":{\"208\":1,\"713\":1,\"721\":1,\"722\":1,\"731\":1,\"733\":1,\"734\":1,\"736\":1,\"737\":1}}],[\"ids=none\",{\"1\":{\"208\":3,\"397\":2,\"403\":2,\"419\":2,\"420\":2,\"663\":2,\"716\":2,\"721\":2,\"722\":2,\"731\":2,\"734\":2,\"736\":2,\"737\":2}}],[\"ids=input\",{\"1\":{\"188\":2,\"397\":1,\"421\":1,\"663\":1,\"713\":1}}],[\"ids\",{\"1\":{\"64\":1,\"187\":7,\"188\":8,\"190\":16,\"191\":2,\"192\":21,\"206\":2,\"208\":24,\"265\":2,\"382\":2,\"384\":7,\"385\":6,\"386\":22,\"397\":2,\"403\":2,\"417\":1,\"419\":21,\"420\":9,\"421\":2,\"596\":11,\"597\":11,\"663\":13,\"697\":7,\"698\":11,\"700\":4,\"713\":36,\"715\":8,\"716\":14,\"721\":4,\"722\":4,\"731\":4,\"733\":2,\"734\":4,\"735\":4,\"736\":4,\"737\":17,\"892\":6}}],[\"id\",{\"1\":{\"53\":2,\"64\":2,\"82\":2,\"92\":6,\"107\":1,\"187\":5,\"188\":5,\"190\":1,\"192\":3,\"208\":4,\"255\":1,\"269\":1,\"274\":3,\"382\":1,\"384\":4,\"385\":5,\"420\":3,\"421\":3,\"596\":6,\"597\":7,\"656\":1,\"663\":1,\"697\":1,\"699\":1,\"713\":9,\"735\":3,\"815\":6,\"892\":4,\"893\":2,\"895\":2,\"898\":1,\"899\":1}}],[\"idx说明\",{\"1\":{\"663\":1}}],[\"idx2word\",{\"1\":{\"697\":7}}],[\"idx2\",{\"1\":{\"582\":2}}],[\"idx=none\",{\"1\":{\"663\":1}}],[\"idx=image\",{\"1\":{\"384\":1}}],[\"idx=idx\",{\"1\":{\"190\":1}}],[\"idx=0\",{\"1\":{\"384\":1,\"703\":1,\"716\":1}}],[\"idx=1\",{\"1\":{\"384\":1,\"385\":2}}],[\"idxs\",{\"1\":{\"190\":3,\"596\":2,\"597\":2}}],[\"idx\",{\"1\":{\"53\":10,\"82\":2,\"119\":38,\"121\":17,\"122\":3,\"137\":21,\"141\":3,\"145\":5,\"190\":32,\"192\":5,\"207\":5,\"293\":8,\"362\":2,\"381\":6,\"383\":1,\"384\":2,\"385\":1,\"386\":5,\"410\":5,\"412\":5,\"419\":5,\"483\":1,\"582\":2,\"595\":14,\"596\":3,\"597\":34,\"663\":1,\"697\":8,\"698\":4,\"700\":2,\"703\":1,\"926\":1,\"934\":1}}],[\"i\",{\"1\":{\"53\":4,\"54\":7,\"56\":8,\"58\":7,\"59\":10,\"65\":17,\"67\":10,\"69\":18,\"70\":2,\"82\":2,\"83\":69,\"92\":4,\"105\":1,\"106\":12,\"107\":10,\"119\":9,\"121\":10,\"122\":17,\"137\":4,\"141\":7,\"145\":2,\"153\":5,\"187\":1,\"190\":2,\"192\":1,\"204\":2,\"263\":3,\"274\":6,\"293\":2,\"359\":1,\"371\":1,\"373\":1,\"380\":3,\"384\":2,\"385\":6,\"398\":4,\"407\":12,\"410\":2,\"412\":2,\"420\":3,\"424\":4,\"431\":2,\"444\":1,\"471\":1,\"485\":2,\"487\":7,\"489\":1,\"542\":2,\"546\":2,\"595\":7,\"596\":4,\"597\":10,\"657\":1,\"660\":5,\"663\":1,\"668\":1,\"669\":1,\"697\":2,\"698\":8,\"700\":5,\"706\":2,\"708\":2,\"709\":10,\"710\":14,\"719\":2,\"801\":2,\"816\":3,\"832\":1,\"900\":2,\"909\":2,\"918\":3,\"921\":15,\"924\":4,\"926\":4,\"935\":13,\"939\":4,\"964\":7}}],[\"isotropic\",{\"1\":{\"871\":1}}],[\"isscalar\",{\"1\":{\"809\":1}}],[\"issues\",{\"1\":{\"185\":1}}],[\"isn\",{\"1\":{\"424\":1}}],[\"isdir\",{\"1\":{\"410\":1,\"412\":1,\"424\":1}}],[\"isfinite\",{\"1\":{\"265\":1}}],[\"isinstance\",{\"1\":{\"263\":1,\"293\":2,\"403\":1,\"663\":1,\"697\":2,\"718\":2,\"728\":2,\"792\":1,\"800\":1,\"801\":1,\"803\":1,\"805\":3,\"807\":2,\"808\":1,\"809\":1}}],[\"is\",{\"0\":{\"349\":1,\"490\":1},\"1\":{\"52\":1,\"67\":1,\"96\":2,\"98\":2,\"99\":1,\"107\":2,\"119\":8,\"121\":1,\"122\":6,\"123\":5,\"137\":3,\"141\":2,\"145\":1,\"152\":1,\"153\":1,\"187\":1,\"190\":1,\"192\":1,\"207\":3,\"208\":8,\"213\":1,\"256\":1,\"263\":1,\"265\":1,\"266\":2,\"274\":1,\"359\":1,\"361\":1,\"362\":1,\"380\":9,\"382\":2,\"397\":2,\"401\":4,\"403\":3,\"408\":1,\"410\":2,\"411\":2,\"412\":4,\"419\":4,\"420\":12,\"424\":1,\"461\":1,\"470\":1,\"490\":2,\"491\":6,\"492\":1,\"494\":1,\"521\":1,\"545\":2,\"582\":1,\"596\":1,\"597\":1,\"660\":32,\"663\":6,\"691\":7,\"697\":1,\"698\":4,\"700\":2,\"703\":1,\"710\":2,\"713\":4,\"716\":2,\"722\":1,\"724\":4,\"729\":1,\"731\":2,\"734\":2,\"735\":2,\"736\":2,\"737\":4,\"747\":1,\"749\":1,\"751\":3,\"784\":1,\"787\":1,\"791\":1,\"792\":2,\"801\":2,\"803\":3,\"805\":5,\"807\":3,\"808\":3,\"815\":5,\"893\":3,\"895\":2,\"898\":1,\"899\":1,\"918\":1,\"926\":2,\"934\":1,\"938\":1}}],[\"if\",{\"1\":{\"52\":1,\"53\":6,\"64\":2,\"67\":1,\"82\":5,\"92\":3,\"96\":2,\"98\":2,\"99\":1,\"104\":2,\"106\":2,\"107\":7,\"119\":10,\"120\":1,\"121\":5,\"122\":4,\"123\":2,\"137\":5,\"138\":2,\"141\":4,\"145\":2,\"152\":1,\"153\":1,\"154\":3,\"188\":2,\"190\":2,\"191\":1,\"204\":1,\"207\":1,\"208\":8,\"213\":13,\"255\":1,\"256\":4,\"263\":7,\"264\":1,\"265\":1,\"266\":3,\"274\":5,\"293\":4,\"380\":16,\"382\":9,\"383\":10,\"384\":2,\"385\":3,\"386\":2,\"397\":1,\"399\":1,\"401\":2,\"403\":4,\"410\":9,\"411\":3,\"412\":12,\"419\":5,\"420\":11,\"421\":1,\"424\":6,\"426\":1,\"429\":1,\"431\":1,\"435\":2,\"444\":3,\"461\":1,\"521\":1,\"582\":1,\"595\":1,\"596\":6,\"597\":9,\"660\":1,\"663\":12,\"696\":4,\"697\":8,\"698\":6,\"700\":3,\"710\":5,\"713\":7,\"715\":1,\"716\":2,\"718\":1,\"722\":3,\"724\":1,\"728\":1,\"731\":1,\"734\":1,\"736\":2,\"737\":4,\"751\":3,\"784\":1,\"787\":1,\"791\":1,\"792\":2,\"800\":2,\"801\":3,\"803\":4,\"805\":9,\"807\":9,\"808\":3,\"809\":2,\"811\":1,\"815\":6,\"892\":3,\"893\":10,\"894\":2,\"895\":4,\"898\":2,\"899\":7,\"900\":2,\"918\":5,\"926\":2,\"934\":1,\"938\":1,\"963\":1,\"964\":2}}],[\"in2\",{\"1\":{\"899\":1}}],[\"in21k模型权重文件\",{\"1\":{\"435\":1}}],[\"in21k\",{\"1\":{\"435\":4}}],[\"in21k这个模型\",{\"1\":{\"435\":1}}],[\"in1\",{\"1\":{\"899\":1}}],[\"invalid\",{\"1\":{\"893\":1}}],[\"invariance\",{\"1\":{\"149\":1,\"160\":1}}],[\"invariant\",{\"1\":{\"149\":1}}],[\"invertible\",{\"1\":{\"641\":1}}],[\"inverse=true\",{\"1\":{\"480\":1}}],[\"inverse=false\",{\"1\":{\"480\":1}}],[\"inverse\",{\"0\":{\"878\":1},\"1\":{\"480\":3}}],[\"inception\",{\"1\":{\"264\":4}}],[\"including\",{\"1\":{\"52\":2}}],[\"includes\",{\"1\":{\"12\":1}}],[\"in=hidden\",{\"1\":{\"255\":2}}],[\"independent\",{\"1\":{\"849\":2}}],[\"indent=4\",{\"1\":{\"424\":1,\"595\":3,\"597\":3,\"696\":2,\"697\":1}}],[\"index=5\",{\"1\":{\"735\":2}}],[\"index=ignored\",{\"1\":{\"734\":1}}],[\"index=0\",{\"1\":{\"700\":3}}],[\"index=masked\",{\"1\":{\"699\":2}}],[\"index=\",{\"1\":{\"384\":2,\"731\":1}}],[\"index=10\",{\"1\":{\"380\":1}}],[\"index=self\",{\"1\":{\"274\":1}}],[\"indexed\",{\"1\":{\"137\":1}}],[\"index\",{\"1\":{\"53\":14,\"82\":13,\"92\":3,\"137\":8,\"141\":3,\"145\":2,\"187\":2,\"208\":1,\"258\":2,\"380\":4,\"382\":8,\"384\":1,\"385\":14,\"410\":2,\"411\":2,\"412\":4,\"487\":6,\"557\":1,\"588\":2,\"590\":1,\"597\":3,\"700\":2,\"703\":1,\"709\":2,\"733\":4,\"734\":6,\"735\":5,\"964\":1}}],[\"inductive\",{\"1\":{\"422\":1}}],[\"indicating\",{\"1\":{\"597\":1}}],[\"indicators\",{\"1\":{\"362\":1}}],[\"indices=none\",{\"1\":{\"208\":1}}],[\"indices\",{\"1\":{\"121\":4,\"122\":5,\"137\":5,\"152\":1,\"154\":1,\"208\":12,\"213\":6,\"265\":1,\"410\":2,\"412\":2,\"419\":6,\"424\":4,\"597\":6,\"698\":3,\"893\":2,\"895\":5,\"899\":4,\"963\":7,\"964\":10}}],[\"ind\",{\"1\":{\"213\":4,\"896\":3}}],[\"indoor\",{\"1\":{\"129\":2}}],[\"inp\",{\"1\":{\"293\":2}}],[\"inplace\",{\"1\":{\"213\":4}}],[\"inplace=true\",{\"1\":{\"119\":3,\"120\":1,\"121\":1,\"122\":4,\"123\":1,\"146\":1,\"918\":3}}],[\"inputfeatures\",{\"1\":{\"713\":1}}],[\"inputfeatures组成图解\",{\"1\":{\"713\":1}}],[\"input=x\",{\"1\":{\"380\":1}}],[\"input=outputs\",{\"1\":{\"265\":1}}],[\"inputs和self\",{\"1\":{\"806\":1}}],[\"inputs\",{\"1\":{\"64\":6,\"67\":6,\"208\":2,\"293\":1,\"410\":4,\"412\":4,\"586\":9,\"587\":14,\"588\":9,\"589\":8,\"590\":10,\"592\":12,\"663\":4,\"713\":8,\"734\":1,\"800\":4,\"801\":2,\"803\":1,\"805\":6,\"806\":1,\"807\":6,\"809\":9,\"815\":2}}],[\"input\",{\"1\":{\"64\":7,\"67\":19,\"119\":2,\"137\":11,\"140\":1,\"141\":3,\"143\":1,\"144\":1,\"187\":5,\"188\":8,\"190\":9,\"191\":2,\"192\":14,\"204\":2,\"206\":2,\"208\":23,\"213\":2,\"261\":2,\"263\":5,\"264\":2,\"265\":3,\"362\":1,\"380\":1,\"382\":1,\"397\":2,\"400\":2,\"403\":2,\"408\":2,\"417\":6,\"419\":10,\"420\":8,\"421\":3,\"480\":2,\"481\":3,\"485\":4,\"488\":2,\"492\":1,\"503\":2,\"588\":1,\"660\":19,\"663\":6,\"697\":4,\"698\":6,\"700\":4,\"713\":13,\"715\":4,\"716\":6,\"718\":2,\"721\":2,\"722\":2,\"725\":2,\"726\":3,\"729\":1,\"731\":2,\"733\":1,\"734\":2,\"735\":4,\"736\":2,\"737\":7,\"747\":1,\"762\":2,\"779\":4,\"780\":1,\"783\":1,\"784\":1,\"787\":1,\"801\":1,\"808\":2,\"892\":2,\"895\":1,\"899\":1,\"918\":2,\"931\":3,\"937\":5,\"964\":3}}],[\"inner\",{\"1\":{\"96\":2,\"98\":2,\"447\":2,\"448\":4}}],[\"injection\",{\"1\":{\"83\":1}}],[\"inherent\",{\"1\":{\"83\":3}}],[\"inspect\",{\"1\":{\"454\":1}}],[\"insert\",{\"1\":{\"83\":2,\"899\":2}}],[\"instruct\",{\"1\":{\"342\":2}}],[\"instructgpt没能直接回答\",{\"1\":{\"657\":1}}],[\"instructgpt在输出真实性\",{\"1\":{\"653\":1}}],[\"instructgpt\",{\"0\":{\"652\":1},\"1\":{\"339\":5,\"346\":1,\"652\":1,\"655\":10,\"656\":7,\"657\":18,\"658\":5}}],[\"instructblip\",{\"1\":{\"310\":1}}],[\"instruction+qa\",{\"1\":{\"657\":1}}],[\"instructional\",{\"1\":{\"64\":3,\"69\":3}}],[\"instructions\",{\"1\":{\"12\":2,\"61\":1,\"652\":1,\"655\":1}}],[\"instruction\",{\"0\":{\"346\":1},\"1\":{\"7\":3,\"64\":1,\"339\":6,\"346\":7,\"655\":1,\"823\":1}}],[\"install命令安装即可\",{\"1\":{\"557\":1}}],[\"install\",{\"1\":{\"557\":4,\"739\":1,\"815\":2}}],[\"instancenorm有可学习的参数\",{\"1\":{\"522\":1}}],[\"instancenorm\",{\"1\":{\"522\":1}}],[\"instancenorm1d\",{\"1\":{\"493\":1}}],[\"instancenorm系列\",{\"1\":{\"493\":1}}],[\"instance\",{\"0\":{\"350\":1},\"1\":{\"282\":1,\"350\":1}}],[\"instances\",{\"1\":{\"49\":1,\"597\":2}}],[\"instagram\",{\"1\":{\"269\":1,\"271\":1}}],[\"initialized\",{\"1\":{\"382\":1}}],[\"initialize\",{\"1\":{\"361\":1,\"597\":1,\"918\":1}}],[\"init=quantize\",{\"1\":{\"213\":1}}],[\"init=false\",{\"1\":{\"213\":1}}],[\"init=true\",{\"1\":{\"213\":2}}],[\"initted\",{\"1\":{\"213\":4}}],[\"init\",{\"0\":{\"205\":1,\"361\":1},\"1\":{\"53\":2,\"58\":4,\"59\":4,\"60\":4,\"65\":2,\"69\":4,\"70\":2,\"82\":2,\"83\":12,\"92\":1,\"97\":2,\"102\":2,\"119\":2,\"120\":2,\"121\":2,\"122\":2,\"123\":2,\"137\":2,\"138\":2,\"141\":4,\"145\":2,\"146\":2,\"152\":2,\"154\":2,\"155\":2,\"156\":2,\"187\":4,\"190\":4,\"191\":2,\"192\":5,\"205\":3,\"208\":1,\"213\":28,\"255\":4,\"263\":1,\"264\":1,\"266\":2,\"274\":1,\"293\":5,\"359\":2,\"361\":2,\"380\":18,\"382\":8,\"397\":2,\"398\":1,\"400\":2,\"403\":4,\"424\":1,\"426\":2,\"427\":3,\"428\":4,\"429\":4,\"430\":2,\"431\":5,\"459\":1,\"558\":2,\"586\":2,\"587\":2,\"588\":2,\"589\":2,\"590\":2,\"592\":2,\"597\":1,\"663\":1,\"697\":1,\"699\":2,\"709\":2,\"710\":2,\"716\":2,\"718\":6,\"719\":2,\"720\":2,\"721\":3,\"722\":3,\"724\":2,\"725\":2,\"726\":2,\"728\":2,\"729\":2,\"730\":2,\"731\":2,\"734\":2,\"736\":2,\"737\":2,\"742\":2,\"743\":2,\"745\":2,\"746\":2,\"747\":2,\"749\":2,\"750\":2,\"751\":2,\"756\":1,\"778\":1,\"792\":1,\"805\":1,\"808\":1,\"809\":1,\"810\":3,\"892\":1,\"895\":2,\"899\":4,\"900\":2,\"918\":4,\"926\":4,\"931\":2,\"937\":2,\"963\":8,\"964\":4}}],[\"infiniband\",{\"1\":{\"680\":1}}],[\"infile\",{\"1\":{\"597\":6}}],[\"infer\",{\"1\":{\"383\":1,\"384\":8,\"385\":10,\"386\":12}}],[\"inference\",{\"1\":{\"11\":1,\"28\":1,\"29\":1,\"64\":3,\"611\":1,\"822\":1,\"877\":5,\"892\":3}}],[\"info\",{\"1\":{\"553\":1,\"555\":1,\"718\":1,\"728\":1}}],[\"infonce\",{\"1\":{\"190\":1,\"192\":4,\"355\":7,\"356\":1,\"357\":1}}],[\"information\",{\"1\":{\"12\":1,\"948\":1}}],[\"inf\",{\"1\":{\"121\":1,\"265\":1,\"380\":1,\"700\":1,\"893\":2,\"896\":4,\"898\":1}}],[\"int32\",{\"1\":{\"892\":1}}],[\"intrinsic\",{\"1\":{\"610\":2,\"611\":1}}],[\"introduction\",{\"0\":{\"165\":1,\"194\":1,\"348\":1,\"353\":1,\"368\":1,\"388\":1},\"1\":{\"953\":1}}],[\"intuitive\",{\"1\":{\"362\":1}}],[\"inttensor\",{\"1\":{\"121\":1}}],[\"int64\",{\"1\":{\"92\":2,\"808\":1}}],[\"int\",{\"1\":{\"83\":8,\"97\":2,\"106\":2,\"122\":1,\"187\":2,\"208\":1,\"213\":2,\"263\":3,\"265\":1,\"361\":2,\"364\":1,\"380\":1,\"382\":1,\"427\":5,\"429\":1,\"435\":1,\"462\":1,\"513\":1,\"595\":21,\"596\":4,\"597\":37,\"697\":1,\"698\":1,\"724\":1,\"892\":4,\"895\":1,\"896\":1,\"899\":1,\"918\":2}}],[\"integer\",{\"1\":{\"899\":1}}],[\"integration\",{\"0\":{\"37\":1}}],[\"intensity\",{\"1\":{\"159\":1}}],[\"intention\",{\"1\":{\"28\":1,\"29\":1}}],[\"interval\",{\"1\":{\"918\":3}}],[\"interface\",{\"1\":{\"663\":1}}],[\"inter\",{\"1\":{\"656\":1}}],[\"interest\",{\"1\":{\"502\":1}}],[\"intermediate\",{\"0\":{\"240\":1},\"1\":{\"718\":9}}],[\"interleave\",{\"1\":{\"188\":1,\"421\":1}}],[\"interpolate\",{\"1\":{\"503\":1,\"582\":2}}],[\"interpolated\",{\"1\":{\"143\":1,\"145\":4}}],[\"interpolation=image\",{\"1\":{\"293\":3}}],[\"interpolation=args\",{\"1\":{\"264\":2}}],[\"interpolation\",{\"1\":{\"122\":2,\"264\":2,\"286\":1,\"502\":1}}],[\"interpcnn\",{\"1\":{\"110\":1}}],[\"intersect\",{\"1\":{\"106\":3}}],[\"intersection\",{\"0\":{\"588\":1},\"1\":{\"46\":1,\"102\":4,\"106\":3,\"586\":2,\"587\":3,\"588\":6,\"592\":2}}],[\"internlm2\",{\"1\":{\"329\":1,\"334\":1}}],[\"internlm\",{\"1\":{\"305\":1}}],[\"internvit\",{\"1\":{\"296\":1,\"303\":1,\"304\":2,\"305\":4,\"306\":4,\"308\":2,\"311\":2,\"312\":1,\"313\":1,\"315\":1,\"316\":1,\"322\":1,\"323\":1,\"327\":1,\"329\":1,\"330\":3,\"334\":1}}],[\"internvl通过组合视觉编码器和语言中间件\",{\"1\":{\"304\":1}}],[\"internvl通过规模化视觉编码器和渐进式跨模态对齐\",{\"1\":{\"301\":1}}],[\"internvl可灵活切换为四种模式\",{\"1\":{\"303\":1}}],[\"internvl的架构设计显著区别于\",{\"1\":{\"303\":1}}],[\"internvl的整体架构\",{\"1\":{\"303\":1}}],[\"internvl在32个通用视觉\",{\"1\":{\"295\":1}}],[\"internvl是一个大规模视觉\",{\"1\":{\"295\":1}}],[\"internvl\",{\"0\":{\"294\":1,\"321\":1},\"1\":{\"34\":1,\"52\":1,\"294\":2,\"296\":1,\"303\":3,\"304\":2,\"305\":1,\"306\":1,\"309\":5,\"310\":3,\"312\":1,\"313\":2,\"317\":2,\"322\":2,\"323\":1,\"327\":1,\"329\":1,\"330\":1,\"332\":1,\"334\":1,\"337\":1}}],[\"internet\",{\"1\":{\"52\":1}}],[\"interact\",{\"1\":{\"52\":4}}],[\"interacts\",{\"1\":{\"52\":2}}],[\"interactive\",{\"1\":{\"36\":1,\"52\":2,\"83\":2}}],[\"interaction的简写\",{\"1\":{\"390\":1}}],[\"interactions\",{\"1\":{\"52\":4,\"61\":1,\"71\":1}}],[\"interaction\",{\"0\":{\"391\":1},\"1\":{\"11\":9,\"12\":1,\"35\":1,\"36\":1,\"52\":12,\"72\":1,\"149\":1,\"390\":1}}],[\"into\",{\"1\":{\"7\":1,\"12\":1,\"274\":1,\"733\":1}}],[\"in\",{\"1\":{\"11\":1,\"12\":1,\"52\":7,\"53\":3,\"59\":3,\"67\":1,\"70\":3,\"71\":1,\"82\":2,\"83\":5,\"92\":4,\"104\":4,\"105\":2,\"106\":5,\"107\":1,\"119\":7,\"120\":8,\"121\":11,\"122\":16,\"123\":11,\"137\":6,\"138\":4,\"141\":8,\"145\":5,\"146\":8,\"187\":2,\"188\":1,\"190\":4,\"192\":5,\"202\":1,\"204\":1,\"207\":2,\"213\":2,\"225\":1,\"255\":9,\"263\":3,\"265\":2,\"266\":4,\"268\":1,\"274\":4,\"279\":2,\"293\":12,\"308\":3,\"359\":2,\"361\":1,\"363\":1,\"380\":9,\"381\":2,\"382\":9,\"383\":10,\"384\":1,\"385\":4,\"386\":7,\"398\":2,\"408\":2,\"409\":1,\"410\":7,\"412\":7,\"418\":2,\"419\":2,\"420\":1,\"424\":10,\"426\":4,\"427\":3,\"428\":2,\"429\":8,\"431\":4,\"435\":3,\"444\":4,\"482\":2,\"504\":2,\"505\":2,\"518\":1,\"522\":4,\"582\":2,\"595\":18,\"596\":7,\"597\":33,\"607\":1,\"620\":1,\"621\":1,\"622\":1,\"646\":1,\"650\":1,\"660\":9,\"663\":2,\"696\":5,\"697\":9,\"698\":8,\"699\":2,\"700\":4,\"703\":1,\"713\":1,\"719\":2,\"724\":1,\"742\":1,\"747\":2,\"750\":1,\"751\":4,\"800\":3,\"801\":2,\"803\":2,\"805\":7,\"806\":2,\"807\":9,\"809\":2,\"815\":5,\"816\":2,\"893\":1,\"895\":1,\"898\":2,\"899\":6,\"918\":4,\"926\":6,\"934\":2,\"935\":2,\"938\":2,\"939\":1,\"963\":6,\"964\":10}}],[\"imsave\",{\"1\":{\"582\":1}}],[\"imshow\",{\"1\":{\"411\":1,\"412\":1,\"935\":3,\"939\":1,\"964\":1}}],[\"imags\",{\"1\":{\"386\":2}}],[\"imag\",{\"1\":{\"380\":11,\"384\":1,\"385\":3}}],[\"imagetext\",{\"1\":{\"393\":1}}],[\"image→text\",{\"1\":{\"385\":2}}],[\"image=none\",{\"1\":{\"893\":1}}],[\"image=false\",{\"1\":{\"384\":3,\"385\":2,\"386\":3}}],[\"image=1\",{\"1\":{\"382\":1,\"385\":1}}],[\"image=0\",{\"1\":{\"382\":4}}],[\"image=self\",{\"1\":{\"382\":3}}],[\"imagefolder\",{\"1\":{\"264\":1,\"293\":3,\"359\":1}}],[\"imagenet\",{\"1\":{\"176\":1,\"213\":1,\"215\":2,\"220\":1,\"224\":1,\"236\":1,\"240\":1,\"242\":1,\"264\":1,\"268\":2,\"269\":1,\"271\":1,\"280\":3,\"286\":1,\"293\":2,\"308\":1,\"309\":1,\"352\":1,\"354\":1,\"355\":1,\"357\":1,\"425\":1,\"435\":3,\"510\":2}}],[\"image\",{\"0\":{\"199\":1,\"201\":1,\"234\":1,\"238\":1,\"386\":1,\"418\":1,\"419\":1,\"420\":1},\"1\":{\"40\":1,\"52\":9,\"53\":2,\"64\":2,\"67\":6,\"82\":1,\"83\":7,\"164\":2,\"165\":1,\"171\":2,\"172\":4,\"179\":1,\"187\":25,\"188\":13,\"190\":50,\"191\":14,\"192\":47,\"201\":1,\"204\":4,\"205\":5,\"206\":30,\"207\":19,\"208\":5,\"209\":2,\"212\":1,\"213\":1,\"219\":2,\"220\":1,\"226\":2,\"227\":1,\"229\":1,\"234\":1,\"235\":3,\"252\":1,\"255\":3,\"256\":2,\"264\":2,\"265\":1,\"266\":2,\"267\":2,\"274\":25,\"293\":5,\"308\":1,\"309\":3,\"359\":3,\"368\":2,\"370\":2,\"373\":2,\"377\":1,\"380\":4,\"382\":35,\"383\":1,\"384\":20,\"385\":43,\"386\":15,\"402\":1,\"406\":1,\"407\":5,\"408\":10,\"410\":31,\"411\":21,\"412\":51,\"413\":1,\"415\":2,\"417\":14,\"418\":11,\"419\":20,\"420\":7,\"421\":15,\"424\":9,\"426\":1,\"435\":1,\"510\":1,\"582\":1,\"815\":3,\"883\":1,\"890\":1,\"892\":31,\"893\":31,\"895\":28,\"898\":1,\"899\":17,\"900\":29,\"918\":5,\"921\":1,\"926\":7,\"937\":1}}],[\"images=images\",{\"1\":{\"274\":1,\"410\":1,\"412\":1}}],[\"images=none\",{\"1\":{\"274\":2}}],[\"images\",{\"1\":{\"11\":1,\"12\":1,\"71\":1,\"159\":1,\"265\":3,\"274\":4,\"293\":3,\"359\":1,\"362\":2,\"386\":5,\"407\":1,\"408\":1,\"410\":3,\"411\":2,\"412\":5,\"420\":2,\"424\":40,\"425\":12,\"431\":3,\"895\":5,\"899\":4,\"918\":3,\"935\":1,\"964\":1}}],[\"im\",{\"1\":{\"288\":2,\"359\":2,\"362\":8}}],[\"impacts\",{\"1\":{\"658\":1}}],[\"improving\",{\"1\":{\"624\":1,\"921\":1}}],[\"improves\",{\"1\":{\"621\":1}}],[\"improve\",{\"1\":{\"6\":1,\"11\":1}}],[\"implements\",{\"1\":{\"751\":1}}],[\"implementation\",{\"0\":{\"184\":1,\"203\":1,\"358\":1,\"360\":1},\"1\":{\"385\":1}}],[\"impl\",{\"1\":{\"435\":1}}],[\"import\",{\"1\":{\"102\":3,\"107\":7,\"412\":9,\"424\":3,\"440\":1,\"441\":1,\"454\":1,\"461\":1,\"463\":1,\"471\":1,\"478\":1,\"480\":1,\"481\":1,\"482\":1,\"490\":1,\"491\":1,\"510\":1,\"513\":2,\"514\":2,\"516\":2,\"520\":1,\"540\":1,\"542\":4,\"544\":1,\"545\":2,\"597\":5,\"660\":2,\"663\":3,\"709\":3,\"710\":2,\"757\":1,\"794\":1,\"806\":1,\"807\":1,\"810\":7,\"815\":6,\"816\":5,\"892\":1,\"918\":9,\"926\":9,\"930\":7,\"964\":1}}],[\"imgkey\",{\"1\":{\"384\":2,\"385\":2}}],[\"imglish\",{\"1\":{\"220\":1}}],[\"imgs\",{\"1\":{\"213\":3,\"386\":2,\"918\":11,\"939\":2}}],[\"img\",{\"1\":{\"53\":28,\"54\":5,\"58\":1,\"64\":8,\"65\":7,\"82\":25,\"83\":14,\"187\":2,\"205\":2,\"213\":2,\"256\":4,\"266\":9,\"274\":15,\"380\":16,\"384\":3,\"385\":2,\"411\":2,\"412\":2,\"424\":9,\"426\":10,\"427\":3,\"428\":2,\"431\":2,\"435\":1,\"582\":11,\"892\":1,\"893\":12,\"895\":12,\"899\":10,\"918\":18,\"964\":2}}],[\"imbalance\",{\"1\":{\"6\":1,\"11\":1,\"586\":1,\"589\":1}}],[\"bhw\",{\"1\":{\"963\":4}}],[\"bhld\",{\"1\":{\"709\":3}}],[\"b类\",{\"1\":{\"923\":1}}],[\"b1\",{\"1\":{\"918\":3}}],[\"b中\",{\"1\":{\"809\":3}}],[\"b时\",{\"1\":{\"809\":1}}],[\"b上有1个点的绝对提升\",{\"1\":{\"634\":1}}],[\"b为例\",{\"1\":{\"809\":1}}],[\"b为r\",{\"1\":{\"606\":1}}],[\"b为批量大小\",{\"1\":{\"430\":1}}],[\"bpe\",{\"1\":{\"594\":2,\"595\":8,\"597\":7,\"633\":1,\"640\":4,\"678\":1,\"683\":1,\"885\":1,\"887\":1}}],[\"b是偏置项\",{\"1\":{\"522\":1}}],[\"b2\",{\"1\":{\"441\":3,\"918\":3}}],[\"b同样表现出这种趋势\",{\"1\":{\"288\":1}}],[\"b=批大小\",{\"1\":{\"380\":1}}],[\"b=batch\",{\"1\":{\"274\":1}}],[\"b=image\",{\"1\":{\"274\":1}}],[\"b=3\",{\"1\":{\"190\":1,\"516\":1}}],[\"bd\",{\"1\":{\"213\":1}}],[\"b+k\",{\"1\":{\"206\":3,\"207\":1}}],[\"b+q\",{\"1\":{\"192\":6}}],[\"bubble\",{\"1\":{\"873\":1}}],[\"but\",{\"1\":{\"724\":1,\"729\":1}}],[\"built\",{\"1\":{\"444\":2}}],[\"builder\",{\"1\":{\"359\":1}}],[\"build\",{\"1\":{\"264\":1,\"265\":1,\"660\":15,\"713\":1}}],[\"buffer的作用和意义\",{\"1\":{\"474\":1}}],[\"buffer\",{\"0\":{\"474\":1},\"1\":{\"190\":4,\"192\":3,\"205\":3,\"213\":4,\"293\":1,\"361\":2,\"474\":1,\"892\":2,\"926\":2,\"964\":1}}],[\"buckets=32\",{\"1\":{\"710\":2}}],[\"buckets\",{\"1\":{\"213\":6,\"710\":29}}],[\"bucket\",{\"1\":{\"53\":1,\"710\":18}}],[\"bcn\",{\"1\":{\"94\":1,\"100\":3}}],[\"bceloss\",{\"1\":{\"918\":1}}],[\"bce\",{\"0\":{\"587\":1},\"1\":{\"14\":1,\"24\":1,\"102\":3,\"586\":1,\"587\":17,\"588\":2,\"589\":2,\"932\":3,\"963\":1}}],[\"bleu或精确匹配评估\",{\"1\":{\"647\":1}}],[\"bleu\",{\"1\":{\"641\":2}}],[\"blob\",{\"1\":{\"424\":1,\"663\":1}}],[\"block第一个全连接的节点个数\",{\"1\":{\"432\":1}}],[\"block的次数\",{\"1\":{\"432\":1}}],[\"block块序列\",{\"1\":{\"431\":2}}],[\"blockwise\",{\"0\":{\"263\":1},\"1\":{\"234\":1,\"242\":3}}],[\"blocking=true\",{\"1\":{\"204\":1}}],[\"blocks\",{\"1\":{\"123\":11,\"141\":7,\"266\":1,\"380\":1,\"384\":1,\"385\":7,\"431\":2,\"899\":4}}],[\"block\",{\"1\":{\"123\":25,\"171\":1,\"223\":1,\"233\":1,\"242\":1,\"264\":3,\"266\":2,\"280\":1,\"306\":1,\"380\":5,\"384\":1,\"385\":5,\"429\":3,\"431\":1,\"630\":1,\"663\":2,\"892\":1,\"918\":5,\"963\":1}}],[\"black\",{\"1\":{\"341\":1,\"408\":1,\"671\":1}}],[\"blk\",{\"1\":{\"266\":2,\"384\":2,\"385\":4}}],[\"blip2qformer\",{\"1\":{\"417\":1,\"421\":2}}],[\"blip2qformer核心代码实现如下\",{\"1\":{\"417\":1}}],[\"blip2\",{\"1\":{\"414\":1}}],[\"blip2base\",{\"1\":{\"64\":1,\"417\":1,\"421\":1}}],[\"blip\",{\"0\":{\"164\":1,\"192\":1},\"1\":{\"164\":3,\"165\":2,\"167\":1,\"168\":1,\"169\":1,\"170\":1,\"172\":1,\"174\":2,\"176\":1,\"179\":1,\"183\":3,\"185\":3,\"187\":4,\"190\":4,\"191\":1,\"192\":7,\"306\":1,\"309\":1,\"395\":1,\"403\":1,\"415\":2,\"416\":1}}],[\"blc\",{\"1\":{\"94\":1,\"100\":3}}],[\"bn的beta足够\",{\"1\":{\"522\":1}}],[\"bn的维度匹配\",{\"1\":{\"83\":1}}],[\"bn=args\",{\"1\":{\"293\":1}}],[\"bn5\",{\"1\":{\"152\":2}}],[\"bn4\",{\"1\":{\"152\":2}}],[\"bns\",{\"1\":{\"137\":3,\"141\":3,\"145\":3}}],[\"bn要求通道维度在前\",{\"1\":{\"121\":1}}],[\"bn3\",{\"1\":{\"120\":2,\"152\":2,\"154\":2,\"156\":2}}],[\"bn2\",{\"1\":{\"120\":2,\"138\":2,\"141\":2,\"152\":2,\"154\":2,\"155\":2,\"156\":2}}],[\"bn1\",{\"1\":{\"120\":2,\"138\":2,\"141\":2,\"146\":2,\"152\":2,\"154\":2,\"155\":2,\"156\":2}}],[\"bn\",{\"1\":{\"70\":1,\"116\":2,\"120\":3,\"121\":5,\"137\":2,\"141\":7,\"145\":4,\"152\":1,\"293\":4,\"362\":1,\"522\":4}}],[\"bsz\",{\"1\":{\"386\":5}}],[\"bs\",{\"1\":{\"67\":2,\"107\":1,\"190\":5,\"192\":7,\"207\":7,\"419\":19}}],[\"bmm\",{\"1\":{\"56\":4,\"65\":3,\"69\":4,\"83\":3,\"152\":1,\"153\":1,\"154\":2,\"475\":1}}],[\"brew\",{\"1\":{\"815\":1}}],[\"break\",{\"1\":{\"53\":1,\"263\":2,\"597\":1,\"660\":1,\"663\":1}}],[\"broader\",{\"1\":{\"658\":1}}],[\"broadcast\",{\"0\":{\"546\":1},\"1\":{\"546\":1}}],[\"broadcasting\",{\"1\":{\"472\":1}}],[\"broom\",{\"1\":{\"53\":1}}],[\"brightness=0\",{\"1\":{\"293\":1}}],[\"borel\",{\"1\":{\"847\":6}}],[\"bos\",{\"1\":{\"187\":2,\"188\":2,\"192\":2,\"420\":3,\"421\":3,\"898\":1}}],[\"book\",{\"1\":{\"641\":1,\"648\":1,\"668\":1}}],[\"books3\",{\"1\":{\"667\":1}}],[\"books2\",{\"1\":{\"647\":1}}],[\"books1\",{\"1\":{\"647\":1}}],[\"bookscorpus\",{\"1\":{\"633\":1}}],[\"books\",{\"1\":{\"633\":1,\"666\":1,\"823\":1}}],[\"bookcorpus\",{\"1\":{\"224\":1,\"679\":1,\"680\":1,\"684\":1}}],[\"bootstrapped\",{\"0\":{\"182\":1},\"1\":{\"181\":1,\"182\":1,\"183\":1,\"185\":1}}],[\"bootstrapping\",{\"1\":{\"164\":2,\"165\":1,\"179\":1,\"183\":2}}],[\"bool\",{\"1\":{\"121\":1,\"187\":1,\"208\":3,\"265\":8,\"266\":6,\"361\":1,\"380\":1,\"435\":1,\"474\":2,\"586\":1,\"587\":1,\"588\":1,\"589\":1,\"596\":1,\"597\":2,\"663\":1}}],[\"bounding\",{\"1\":{\"83\":2,\"341\":2}}],[\"box框文件路径\",{\"1\":{\"82\":1}}],[\"box\",{\"1\":{\"82\":17,\"83\":44,\"341\":2}}],[\"bottleneck\",{\"1\":{\"95\":1,\"150\":1,\"157\":1,\"214\":2}}],[\"bottle\",{\"1\":{\"53\":1,\"64\":1,\"82\":1,\"92\":1}}],[\"bowl\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"body\",{\"1\":{\"52\":1}}],[\"bivariate\",{\"1\":{\"871\":1}}],[\"bits\",{\"1\":{\"950\":1}}],[\"bit\",{\"1\":{\"724\":1,\"906\":3,\"907\":2}}],[\"bid\",{\"1\":{\"597\":2}}],[\"bidirectional\",{\"1\":{\"227\":1,\"252\":1,\"690\":1}}],[\"bigram\",{\"1\":{\"595\":2,\"597\":2}}],[\"biggan\",{\"1\":{\"247\":1}}],[\"bilstm\",{\"1\":{\"634\":1}}],[\"bilinear\",{\"0\":{\"505\":1},\"1\":{\"502\":1,\"503\":1}}],[\"billion\",{\"1\":{\"432\":1,\"641\":1}}],[\"bi\",{\"1\":{\"419\":2}}],[\"bicubic\",{\"1\":{\"286\":1,\"293\":3,\"503\":1}}],[\"bicycle\",{\"1\":{\"53\":1}}],[\"bias\",{\"0\":{\"670\":1,\"710\":1},\"1\":{\"266\":4,\"293\":1,\"380\":20,\"384\":3,\"385\":9,\"422\":1,\"429\":1,\"430\":1,\"431\":1,\"522\":1,\"710\":17,\"729\":3,\"900\":2}}],[\"bias=none\",{\"1\":{\"380\":2}}],[\"bias=qkv\",{\"1\":{\"380\":3,\"429\":1,\"430\":1,\"431\":1}}],[\"bias=relative\",{\"1\":{\"380\":1,\"384\":1,\"385\":4}}],[\"bias=rel\",{\"1\":{\"266\":1}}],[\"bias=true\",{\"1\":{\"205\":2,\"380\":1,\"428\":1,\"431\":1,\"522\":3}}],[\"bias=false\",{\"1\":{\"97\":2,\"120\":3,\"121\":2,\"380\":5,\"403\":1,\"429\":1,\"430\":1,\"522\":8,\"699\":1,\"729\":1,\"926\":8}}],[\"binomial\",{\"0\":{\"856\":1,\"859\":1},\"1\":{\"856\":1,\"860\":1}}],[\"bin\",{\"1\":{\"502\":7,\"712\":1}}],[\"bincount\",{\"0\":{\"485\":1},\"1\":{\"213\":1,\"485\":4,\"513\":2}}],[\"bins\",{\"1\":{\"213\":13}}],[\"binary\",{\"0\":{\"2\":1},\"1\":{\"0\":1,\"7\":1,\"88\":1,\"102\":1,\"106\":11,\"587\":6,\"589\":2,\"592\":1,\"932\":4}}],[\"binaryoracle\",{\"1\":{\"0\":1,\"689\":1,\"752\":1,\"798\":1,\"813\":1,\"818\":1}}],[\"bfloat16\",{\"1\":{\"52\":2}}],[\"bayesian\",{\"1\":{\"877\":3}}],[\"bayes\",{\"0\":{\"877\":1},\"1\":{\"851\":2,\"877\":2}}],[\"bad\",{\"1\":{\"835\":2,\"836\":3}}],[\"bahdanau\",{\"1\":{\"655\":1}}],[\"ba\",{\"1\":{\"611\":1}}],[\"balanced\",{\"1\":{\"514\":3}}],[\"ball\",{\"1\":{\"135\":4,\"137\":6,\"141\":2,\"143\":1}}],[\"bart\",{\"1\":{\"735\":1}}],[\"bar\",{\"1\":{\"424\":1}}],[\"barlow\",{\"1\":{\"246\":1}}],[\"baidu\",{\"1\":{\"424\":1,\"435\":1}}],[\"bank\",{\"1\":{\"282\":1,\"357\":4}}],[\"bashrc\",{\"1\":{\"558\":1}}],[\"bash\",{\"1\":{\"558\":2}}],[\"basic\",{\"1\":{\"269\":1}}],[\"basicvae\",{\"1\":{\"255\":1}}],[\"basemodeloutputwithpastandcrossattentions\",{\"1\":{\"420\":1,\"663\":1}}],[\"basemodeloutputwithpoolingandcrossattentions\",{\"1\":{\"397\":1}}],[\"basename\",{\"1\":{\"410\":1,\"412\":1}}],[\"base模型\",{\"1\":{\"213\":1}}],[\"base作为默认视觉编码器\",{\"1\":{\"187\":1}}],[\"base\",{\"1\":{\"187\":1,\"190\":1,\"191\":1,\"192\":4,\"213\":1,\"236\":1,\"272\":1,\"280\":1,\"293\":2,\"359\":5,\"361\":3,\"376\":3,\"425\":1,\"435\":7,\"544\":1,\"552\":1,\"553\":1,\"712\":6,\"823\":2}}],[\"basedataset\",{\"1\":{\"382\":3}}],[\"basedatamodule\",{\"1\":{\"382\":4}}],[\"based\",{\"1\":{\"124\":1,\"157\":2,\"160\":1,\"421\":1,\"597\":1,\"656\":1,\"894\":1}}],[\"baseballbat\",{\"1\":{\"53\":1}}],[\"baseline\",{\"1\":{\"46\":1}}],[\"baevski\",{\"1\":{\"216\":1}}],[\"bao\",{\"1\":{\"214\":1,\"216\":2}}],[\"batch数据准备\",{\"0\":{\"698\":1}}],[\"batch=1\",{\"1\":{\"542\":2}}],[\"batch和text\",{\"1\":{\"417\":1}}],[\"batch中只有一条数据\",{\"1\":{\"382\":1}}],[\"batchmean\",{\"1\":{\"256\":1,\"260\":1,\"899\":1}}],[\"batchsize\",{\"1\":{\"152\":2,\"153\":1,\"156\":2,\"353\":1,\"382\":1}}],[\"batch索引\",{\"1\":{\"120\":1}}],[\"batches\",{\"1\":{\"105\":1,\"410\":2,\"412\":2,\"700\":3,\"751\":1,\"918\":4}}],[\"batched\",{\"1\":{\"67\":1}}],[\"batchnorm系列\",{\"1\":{\"493\":1}}],[\"batchnorm2d\",{\"1\":{\"137\":1,\"141\":1,\"926\":24}}],[\"batchnorm\",{\"1\":{\"70\":1,\"119\":1,\"137\":1,\"154\":1,\"319\":1,\"522\":10,\"899\":1}}],[\"batchnorm1d\",{\"1\":{\"58\":2,\"59\":3,\"60\":3,\"65\":2,\"69\":1,\"70\":1,\"83\":7,\"119\":3,\"120\":4,\"121\":1,\"122\":3,\"123\":1,\"138\":2,\"141\":2,\"143\":1,\"145\":2,\"146\":1,\"152\":5,\"154\":3,\"155\":2,\"156\":3,\"493\":1,\"522\":2,\"918\":1}}],[\"batch\",{\"1\":{\"54\":8,\"58\":11,\"59\":3,\"64\":2,\"67\":7,\"69\":1,\"83\":4,\"94\":1,\"100\":2,\"102\":1,\"104\":3,\"106\":1,\"119\":10,\"121\":7,\"122\":2,\"123\":1,\"137\":13,\"152\":6,\"153\":2,\"172\":1,\"187\":2,\"190\":6,\"192\":2,\"201\":1,\"207\":1,\"213\":1,\"223\":2,\"224\":2,\"236\":1,\"265\":5,\"266\":3,\"274\":20,\"282\":1,\"285\":1,\"292\":4,\"293\":13,\"353\":4,\"356\":3,\"357\":4,\"359\":6,\"362\":4,\"364\":6,\"373\":1,\"380\":2,\"381\":16,\"382\":25,\"383\":12,\"384\":10,\"385\":13,\"386\":34,\"410\":9,\"412\":9,\"417\":1,\"418\":2,\"419\":1,\"420\":1,\"421\":3,\"424\":4,\"425\":2,\"430\":10,\"466\":1,\"477\":1,\"488\":2,\"534\":1,\"542\":3,\"547\":1,\"586\":2,\"587\":1,\"588\":1,\"589\":3,\"656\":1,\"663\":4,\"681\":2,\"683\":1,\"698\":4,\"699\":14,\"700\":10,\"703\":7,\"709\":3,\"710\":6,\"712\":2,\"715\":4,\"724\":2,\"733\":6,\"734\":3,\"736\":2,\"737\":7,\"751\":2,\"892\":1,\"893\":3,\"898\":1,\"918\":6,\"926\":4,\"933\":1,\"934\":3,\"963\":1,\"964\":3}}],[\"backends\",{\"1\":{\"926\":1}}],[\"backprop为true时\",{\"1\":{\"807\":1}}],[\"backprop\",{\"1\":{\"807\":4}}],[\"backpack\",{\"1\":{\"53\":1,\"341\":1}}],[\"background\",{\"1\":{\"107\":1}}],[\"back\",{\"1\":{\"107\":3,\"950\":1}}],[\"backward方法\",{\"1\":{\"807\":1}}],[\"backward\",{\"0\":{\"801\":1},\"1\":{\"105\":1,\"187\":1,\"190\":1,\"192\":1,\"204\":1,\"258\":1,\"293\":1,\"359\":1,\"381\":2,\"431\":1,\"700\":1,\"779\":1,\"780\":1,\"781\":3,\"784\":3,\"787\":2,\"791\":1,\"794\":2,\"795\":1,\"800\":1,\"801\":4,\"803\":2,\"804\":2,\"805\":2,\"806\":1,\"807\":3,\"809\":6,\"811\":3,\"815\":1,\"816\":3,\"918\":2,\"926\":1,\"934\":1,\"938\":1,\"963\":1,\"964\":1}}],[\"backbone\",{\"1\":{\"46\":2,\"94\":3,\"205\":1,\"239\":1,\"285\":1,\"293\":16}}],[\"bag\",{\"1\":{\"53\":1,\"82\":1,\"92\":1,\"413\":3}}],[\"b\",{\"1\":{\"30\":1,\"41\":1,\"48\":1,\"54\":4,\"56\":2,\"59\":2,\"64\":15,\"65\":11,\"67\":1,\"69\":28,\"70\":10,\"73\":2,\"82\":2,\"83\":104,\"94\":9,\"96\":2,\"97\":2,\"98\":2,\"99\":2,\"100\":17,\"102\":2,\"106\":3,\"119\":5,\"120\":1,\"121\":8,\"122\":7,\"123\":1,\"137\":56,\"138\":2,\"141\":11,\"143\":4,\"145\":33,\"146\":7,\"152\":3,\"154\":4,\"156\":7,\"176\":4,\"190\":39,\"192\":15,\"197\":1,\"205\":7,\"206\":15,\"207\":6,\"208\":2,\"213\":12,\"222\":1,\"256\":9,\"260\":4,\"265\":1,\"266\":13,\"274\":6,\"303\":1,\"304\":1,\"327\":1,\"380\":19,\"384\":1,\"386\":4,\"390\":1,\"407\":2,\"415\":2,\"417\":8,\"418\":17,\"419\":4,\"421\":3,\"425\":3,\"426\":8,\"427\":7,\"428\":5,\"430\":3,\"431\":5,\"432\":1,\"435\":2,\"441\":2,\"452\":2,\"453\":2,\"478\":8,\"522\":6,\"546\":6,\"572\":2,\"574\":1,\"582\":4,\"586\":4,\"587\":4,\"588\":4,\"589\":4,\"592\":1,\"611\":3,\"630\":1,\"634\":1,\"656\":1,\"660\":2,\"685\":1,\"697\":5,\"698\":20,\"699\":1,\"703\":1,\"709\":13,\"710\":9,\"713\":1,\"737\":1,\"766\":4,\"781\":3,\"804\":2,\"806\":6,\"809\":14,\"846\":2,\"887\":2,\"893\":7,\"899\":23,\"900\":6,\"923\":8,\"924\":12,\"926\":17,\"944\":3,\"963\":6,\"964\":6}}],[\"below\",{\"1\":{\"749\":1}}],[\"ben18785\",{\"1\":{\"854\":1}}],[\"bengio\",{\"1\":{\"822\":1}}],[\"bender\",{\"1\":{\"655\":1}}],[\"benchmarks\",{\"1\":{\"633\":1}}],[\"benchmark\",{\"0\":{\"46\":1},\"1\":{\"14\":1,\"521\":2,\"634\":1,\"641\":1}}],[\"begin\",{\"1\":{\"420\":1}}],[\"beginning\",{\"1\":{\"188\":1}}],[\"be\",{\"1\":{\"274\":1,\"597\":1,\"697\":1,\"713\":1,\"815\":1,\"893\":1,\"899\":2,\"900\":1}}],[\"beit主模型预训练\",{\"0\":{\"265\":1}}],[\"beit在图像分类和语义分割等下游任务中表现优异\",{\"1\":{\"227\":1}}],[\"beit将图像表示为两种视图\",{\"1\":{\"227\":1}}],[\"beit3\",{\"0\":{\"219\":1},\"1\":{\"219\":1}}],[\"beit\",{\"0\":{\"226\":1,\"234\":1,\"237\":1,\"253\":1},\"1\":{\"209\":2,\"210\":2,\"212\":3,\"213\":2,\"214\":4,\"215\":2,\"216\":2,\"217\":1,\"218\":1,\"219\":2,\"220\":9,\"221\":1,\"222\":2,\"223\":3,\"224\":6,\"225\":5,\"226\":3,\"227\":1,\"228\":5,\"229\":3,\"231\":1,\"234\":1,\"235\":4,\"236\":2,\"237\":1,\"238\":1,\"239\":1,\"240\":2,\"242\":2,\"243\":3,\"248\":1,\"249\":1,\"250\":4,\"251\":1,\"252\":4,\"253\":2,\"254\":1,\"258\":1,\"264\":5,\"265\":4,\"269\":1,\"315\":1,\"368\":1,\"374\":1,\"376\":2,\"899\":1}}],[\"beit2\",{\"0\":{\"209\":1},\"1\":{\"209\":1}}],[\"bermanmaxim\",{\"1\":{\"591\":1}}],[\"bernoulli\",{\"0\":{\"856\":1},\"1\":{\"208\":3,\"856\":1,\"951\":1}}],[\"bert支持的下游任务图\",{\"1\":{\"732\":1}}],[\"bertformultiplechoice\",{\"1\":{\"737\":2}}],[\"bertformaskedlm\",{\"1\":{\"205\":2}}],[\"bertfortokenclassification\",{\"1\":{\"736\":2}}],[\"bertforquestionanswering\",{\"1\":{\"733\":1,\"734\":2}}],[\"bertforpretraining结构图\",{\"1\":{\"731\":1}}],[\"bertforpretraining\",{\"0\":{\"731\":1},\"1\":{\"731\":2}}],[\"bertforsequenceclassification模型结构图\",{\"1\":{\"722\":1}}],[\"bertforsequenceclassification\",{\"0\":{\"722\":1},\"1\":{\"722\":2}}],[\"bertoutput\",{\"1\":{\"718\":3}}],[\"bertonlymlmhead\",{\"1\":{\"420\":1}}],[\"bertintermediate\",{\"1\":{\"718\":3}}],[\"bert文本分类实战\",{\"1\":{\"711\":1}}],[\"bertdataset\",{\"1\":{\"700\":1}}],[\"berttokenizer中的特殊token\",{\"1\":{\"713\":1}}],[\"berttokenizer\",{\"1\":{\"697\":1,\"713\":3,\"733\":1}}],[\"bert的mlm目标本身足够强大\",{\"1\":{\"686\":1}}],[\"bert原始设计未充分优化\",{\"1\":{\"686\":1}}],[\"bert原始设计存在优化空间\",{\"1\":{\"681\":1}}],[\"bert训练1m步\",{\"1\":{\"684\":1}}],[\"bert训练数据\",{\"1\":{\"684\":1}}],[\"bert使用256的批次大小\",{\"1\":{\"683\":1}}],[\"bert使用nsp任务\",{\"1\":{\"683\":1}}],[\"bert采用\",{\"1\":{\"681\":1}}],[\"bertembeddings\",{\"0\":{\"716\":1},\"1\":{\"419\":1,\"716\":2,\"721\":1}}],[\"bertembeddings会将text\",{\"1\":{\"419\":1}}],[\"bertencoder模型结构图\",{\"1\":{\"719\":1}}],[\"bertencoder\",{\"0\":{\"398\":1,\"717\":1,\"719\":1},\"1\":{\"398\":1,\"420\":2,\"719\":2,\"721\":1}}],[\"bertselfoutput计算流程图\",{\"1\":{\"725\":1}}],[\"bertselfoutput\",{\"0\":{\"725\":1},\"1\":{\"400\":1,\"725\":2,\"726\":1}}],[\"bertselfattention\",{\"0\":{\"401\":1,\"724\":1},\"1\":{\"207\":1,\"401\":1,\"420\":2,\"724\":2,\"726\":1}}],[\"bertattention计算流程图\",{\"1\":{\"726\":1}}],[\"bertattention\",{\"0\":{\"400\":1,\"723\":1,\"726\":1},\"1\":{\"400\":1,\"718\":1,\"726\":2}}],[\"bertlmpredictionhead结构图\",{\"1\":{\"729\":1}}],[\"bertlmpredictionhead\",{\"0\":{\"729\":1},\"1\":{\"403\":2,\"729\":2,\"730\":1}}],[\"bertlmheadmodel自回归语言建模实现\",{\"1\":{\"187\":1}}],[\"bertlmheadmodel\",{\"1\":{\"187\":1,\"192\":1,\"403\":2,\"420\":2}}],[\"bertlayer模型结构图\",{\"1\":{\"718\":1}}],[\"bertlayernorm\",{\"1\":{\"716\":1,\"718\":1,\"725\":1,\"728\":1}}],[\"bertlayer\",{\"0\":{\"399\":1,\"718\":1},\"1\":{\"398\":1,\"399\":1,\"420\":3,\"718\":2,\"719\":1}}],[\"bertpretrainingheads结构图\",{\"1\":{\"730\":1}}],[\"bertpretrainingheads\",{\"0\":{\"730\":1},\"1\":{\"730\":2,\"731\":1}}],[\"bertpretrainedmodel\",{\"1\":{\"397\":1,\"403\":1,\"420\":1,\"721\":1,\"722\":1,\"731\":1,\"734\":1,\"736\":1,\"737\":1}}],[\"bertpredictionheadtransform结构图\",{\"1\":{\"728\":1}}],[\"bertpredictionheadtransform\",{\"0\":{\"728\":1},\"1\":{\"403\":2,\"728\":2,\"729\":1}}],[\"bertpooler模型结构图\",{\"1\":{\"720\":1}}],[\"bertpooler\",{\"0\":{\"720\":1},\"1\":{\"397\":1,\"720\":2,\"721\":1}}],[\"bert属于\",{\"1\":{\"390\":1}}],[\"bertmodel模型结构图\",{\"1\":{\"721\":1}}],[\"bertmodel\",{\"0\":{\"397\":1,\"721\":1},\"1\":{\"190\":2,\"191\":1,\"192\":2,\"397\":1,\"403\":1,\"420\":2,\"721\":2,\"722\":1,\"724\":1,\"731\":1,\"734\":1,\"736\":1,\"737\":1}}],[\"bertconfig\",{\"1\":{\"187\":1,\"205\":2}}],[\"bertbase\",{\"1\":{\"176\":1,\"197\":1}}],[\"bert\",{\"0\":{\"250\":2,\"396\":1,\"498\":1,\"689\":1,\"690\":1,\"711\":1},\"1\":{\"171\":2,\"192\":5,\"205\":3,\"206\":3,\"207\":2,\"208\":3,\"226\":2,\"228\":1,\"231\":1,\"234\":1,\"237\":1,\"250\":4,\"252\":3,\"264\":1,\"268\":1,\"269\":1,\"280\":1,\"346\":1,\"354\":2,\"355\":1,\"369\":2,\"371\":1,\"376\":1,\"403\":3,\"417\":2,\"419\":1,\"420\":2,\"594\":1,\"640\":1,\"650\":1,\"671\":1,\"676\":1,\"678\":3,\"679\":7,\"680\":4,\"682\":1,\"690\":4,\"691\":2,\"693\":1,\"694\":4,\"697\":1,\"699\":4,\"700\":1,\"712\":16,\"713\":1,\"722\":2,\"731\":2,\"733\":5,\"734\":2,\"735\":9,\"736\":2,\"737\":7,\"822\":1}}],[\"before\",{\"1\":{\"193\":2,\"364\":1,\"381\":2,\"449\":1,\"454\":2,\"558\":1}}],[\"bear\",{\"1\":{\"202\":1}}],[\"beam数量\",{\"1\":{\"421\":1}}],[\"beams=num\",{\"1\":{\"188\":1,\"421\":1}}],[\"beams=3\",{\"1\":{\"188\":1,\"421\":1}}],[\"beams\",{\"1\":{\"188\":3,\"421\":4}}],[\"beam\",{\"1\":{\"178\":3,\"188\":4,\"421\":1}}],[\"beat\",{\"1\":{\"53\":1}}],[\"best\",{\"1\":{\"106\":6,\"107\":1,\"595\":18,\"597\":8,\"700\":7}}],[\"betwen\",{\"1\":{\"918\":1}}],[\"between\",{\"1\":{\"52\":2,\"724\":1}}],[\"beta=beta\",{\"1\":{\"590\":1,\"592\":1}}],[\"beta=1\",{\"1\":{\"213\":1}}],[\"beta得分\",{\"1\":{\"590\":1}}],[\"beta\",{\"1\":{\"213\":5,\"522\":7,\"590\":4,\"963\":2}}],[\"betas=\",{\"1\":{\"104\":1,\"918\":2}}],[\"bed\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"behaviors\",{\"1\":{\"11\":1}}],[\"behavior\",{\"1\":{\"11\":1}}],[\"bytenet和convs2s等网络模型\",{\"1\":{\"740\":1}}],[\"bytepairs\",{\"1\":{\"597\":1}}],[\"bytepairtokenizer\",{\"1\":{\"595\":2,\"596\":2,\"597\":5}}],[\"bytes\",{\"1\":{\"595\":2,\"596\":20,\"597\":35}}],[\"byte\",{\"1\":{\"540\":1,\"594\":1,\"595\":4,\"596\":7,\"597\":35,\"640\":1,\"683\":1}}],[\"byol等框架接近\",{\"1\":{\"289\":1}}],[\"byol等\",{\"1\":{\"285\":1}}],[\"byol\",{\"1\":{\"246\":1,\"264\":1,\"282\":5,\"286\":1}}],[\"by\",{\"1\":{\"7\":1,\"92\":1,\"361\":1,\"597\":1,\"619\":1,\"624\":1,\"720\":1,\"811\":2,\"893\":1,\"894\":1,\"898\":1,\"900\":1}}],[\"fmap\",{\"1\":{\"892\":8}}],[\"f放缩因子\",{\"1\":{\"775\":1}}],[\"f1放缩因子\",{\"1\":{\"775\":2}}],[\"f1达89\",{\"1\":{\"684\":1,\"685\":1}}],[\"f1从87\",{\"1\":{\"684\":1}}],[\"f1\",{\"0\":{\"567\":1},\"1\":{\"567\":4,\"641\":2,\"648\":1,\"683\":1,\"775\":2}}],[\"fw\",{\"1\":{\"503\":1}}],[\"fh\",{\"1\":{\"503\":1}}],[\"f2放缩因子\",{\"1\":{\"775\":2}}],[\"f2\",{\"1\":{\"353\":1,\"775\":1}}],[\"f2d\",{\"1\":{\"64\":1}}],[\"f3放缩因子\",{\"1\":{\"775\":1}}],[\"f30kcaptionkarpathydatamodule\",{\"1\":{\"382\":1}}],[\"f30k\",{\"1\":{\"382\":1}}],[\"f3\",{\"1\":{\"353\":2,\"775\":1}}],[\"f3d\",{\"1\":{\"64\":1}}],[\"fft的原理\",{\"1\":{\"602\":1}}],[\"ffmpeg\",{\"1\":{\"557\":1}}],[\"ff\",{\"1\":{\"274\":5,\"892\":6}}],[\"ffn\",{\"1\":{\"98\":1,\"171\":2,\"372\":5,\"376\":1,\"380\":4,\"385\":11,\"661\":1}}],[\"fn放缩因子\",{\"1\":{\"775\":1}}],[\"fn负责对返回的一个batch\",{\"1\":{\"715\":1}}],[\"fname=fname\",{\"1\":{\"582\":1}}],[\"fname\",{\"1\":{\"582\":2}}],[\"fn=collate\",{\"1\":{\"715\":1}}],[\"fn=val\",{\"1\":{\"425\":1}}],[\"fn=train\",{\"1\":{\"425\":1}}],[\"fn=self\",{\"1\":{\"382\":6}}],[\"fn叫做\",{\"1\":{\"353\":1}}],[\"fn\",{\"1\":{\"213\":3,\"255\":1,\"256\":1,\"353\":1,\"403\":3,\"424\":2,\"425\":2,\"561\":3,\"562\":1,\"590\":8,\"592\":2,\"715\":2,\"718\":3,\"728\":3,\"899\":2,\"964\":2}}],[\"fns=\",{\"1\":{\"187\":1,\"190\":1,\"192\":1}}],[\"fnn\",{\"1\":{\"60\":1}}],[\"fstn\",{\"1\":{\"154\":2}}],[\"f^2\",{\"1\":{\"150\":1}}],[\"fxia22\",{\"1\":{\"147\":1}}],[\"flush=true\",{\"1\":{\"660\":1,\"663\":1}}],[\"flashattention\",{\"1\":{\"823\":1}}],[\"flash\",{\"1\":{\"823\":6}}],[\"flat\",{\"1\":{\"918\":2,\"935\":2}}],[\"flat迭代器等\",{\"1\":{\"808\":1}}],[\"flattened\",{\"1\":{\"213\":5,\"963\":3}}],[\"flatten\",{\"1\":{\"106\":2,\"265\":2,\"266\":1,\"380\":2,\"426\":1,\"892\":1,\"893\":1,\"899\":2}}],[\"flag\",{\"1\":{\"382\":3,\"408\":1,\"597\":1}}],[\"flan\",{\"1\":{\"339\":1,\"346\":1,\"655\":1}}],[\"flip\",{\"1\":{\"293\":4}}],[\"flickr30k\",{\"1\":{\"185\":1,\"220\":1,\"268\":1,\"309\":2,\"318\":2}}],[\"floor\",{\"1\":{\"462\":1,\"501\":1}}],[\"flowerclassify\",{\"1\":{\"410\":1,\"412\":1}}],[\"flower\",{\"1\":{\"410\":4,\"412\":2,\"424\":8}}],[\"florence\",{\"1\":{\"269\":1}}],[\"floattensor\",{\"1\":{\"663\":5,\"918\":2}}],[\"float32\",{\"1\":{\"152\":1,\"293\":1}}],[\"float64\",{\"1\":{\"107\":4,\"815\":1}}],[\"float\",{\"1\":{\"82\":2,\"83\":2,\"94\":2,\"100\":3,\"107\":1,\"121\":1,\"190\":1,\"208\":1,\"265\":1,\"361\":2,\"380\":3,\"385\":4,\"386\":2,\"408\":2,\"424\":1,\"462\":1,\"513\":1,\"582\":1,\"586\":1,\"587\":1,\"588\":1,\"589\":3,\"700\":2,\"710\":1,\"809\":1,\"893\":1,\"896\":1,\"926\":2,\"938\":1,\"939\":1,\"964\":1}}],[\"flexible\",{\"1\":{\"23\":1}}],[\"fct\",{\"1\":{\"208\":2,\"403\":2,\"420\":2,\"722\":4,\"731\":3,\"734\":3,\"736\":3,\"737\":2}}],[\"fc3\",{\"1\":{\"138\":2,\"141\":2,\"152\":2,\"155\":2,\"931\":2,\"937\":2}}],[\"fc2\",{\"1\":{\"138\":2,\"141\":2,\"152\":2,\"155\":2,\"429\":2,\"931\":2,\"937\":2}}],[\"fc1\",{\"1\":{\"138\":2,\"141\":2,\"152\":2,\"155\":2,\"429\":2,\"931\":2,\"937\":2}}],[\"fc\",{\"1\":{\"60\":3,\"152\":1,\"201\":1,\"293\":2,\"361\":1,\"421\":1,\"431\":1,\"699\":5,\"931\":4,\"937\":4}}],[\"fp4\",{\"1\":{\"146\":2}}],[\"fp\",{\"1\":{\"144\":3,\"146\":1,\"561\":3,\"562\":1,\"590\":8,\"592\":2}}],[\"fpnhead\",{\"1\":{\"123\":1}}],[\"fpn\",{\"1\":{\"123\":1}}],[\"fps是一种在点云\",{\"1\":{\"134\":1}}],[\"fps\",{\"1\":{\"121\":1,\"134\":1,\"137\":7,\"141\":1,\"143\":1}}],[\"fpr\",{\"1\":{\"106\":1,\"564\":3,\"569\":3,\"572\":2}}],[\"fp16\",{\"1\":{\"680\":1,\"721\":1}}],[\"fp1\",{\"1\":{\"59\":2,\"70\":3,\"83\":2,\"94\":1,\"146\":2}}],[\"fp2\",{\"1\":{\"59\":2,\"70\":3,\"83\":2,\"94\":1,\"146\":2}}],[\"fp3\",{\"1\":{\"59\":2,\"70\":4,\"83\":2,\"94\":1,\"146\":2}}],[\"field\",{\"1\":{\"847\":3}}],[\"fig\",{\"1\":{\"935\":2,\"939\":1}}],[\"figsize=\",{\"1\":{\"816\":1,\"935\":2,\"939\":1}}],[\"figure\",{\"1\":{\"215\":1,\"502\":1,\"647\":2,\"655\":1,\"656\":1,\"657\":7,\"658\":4,\"746\":1,\"749\":1,\"751\":1,\"816\":1}}],[\"fit\",{\"1\":{\"381\":5,\"382\":6}}],[\"fill\",{\"1\":{\"187\":1,\"190\":2,\"192\":4,\"206\":1,\"207\":2,\"213\":2,\"380\":1,\"386\":2,\"420\":1,\"421\":1,\"703\":1,\"751\":1,\"893\":1,\"898\":1,\"918\":2,\"926\":1}}],[\"filtered\",{\"1\":{\"895\":2,\"898\":2}}],[\"filterwarnings\",{\"1\":{\"412\":1}}],[\"filter\",{\"0\":{\"189\":1},\"1\":{\"165\":1,\"168\":1,\"173\":2,\"177\":4,\"179\":2,\"183\":1,\"185\":1,\"190\":1,\"895\":1,\"898\":1}}],[\"filtering\",{\"1\":{\"165\":1,\"173\":1,\"341\":1}}],[\"filenotfounderror\",{\"1\":{\"963\":1,\"964\":1}}],[\"filename=to\",{\"1\":{\"815\":1}}],[\"file=\",{\"1\":{\"815\":2}}],[\"file$bert\",{\"1\":{\"712\":1}}],[\"filepath\",{\"1\":{\"596\":1}}],[\"filepaths\",{\"1\":{\"595\":5,\"597\":6}}],[\"file\",{\"1\":{\"53\":7,\"82\":5,\"187\":1,\"205\":1,\"410\":5,\"412\":5,\"424\":2,\"696\":2,\"697\":3,\"815\":3}}],[\"files\",{\"1\":{\"53\":11,\"82\":8,\"410\":2,\"412\":2,\"597\":1}}],[\"first\",{\"1\":{\"137\":2,\"192\":1,\"344\":1,\"397\":2,\"713\":2,\"720\":3,\"918\":2}}],[\"finfo\",{\"1\":{\"893\":1,\"898\":1}}],[\"find\",{\"1\":{\"92\":2,\"411\":2,\"412\":2,\"597\":1}}],[\"fingers\",{\"1\":{\"52\":2,\"55\":2}}],[\"finally\",{\"1\":{\"444\":1,\"807\":1}}],[\"final\",{\"1\":{\"52\":1,\"143\":1,\"751\":1}}],[\"finetune\",{\"0\":{\"830\":1},\"1\":{\"407\":1,\"510\":1,\"830\":1}}],[\"finetuning\",{\"1\":{\"383\":1,\"607\":1}}],[\"fine\",{\"0\":{\"34\":1,\"240\":1,\"599\":1,\"694\":1},\"1\":{\"237\":1,\"305\":1,\"339\":1,\"340\":1,\"599\":1,\"602\":3,\"607\":1,\"611\":2,\"625\":1,\"647\":1,\"656\":1,\"673\":1,\"694\":1}}],[\"f\",{\"1\":{\"52\":8,\"54\":6,\"57\":4,\"58\":3,\"59\":7,\"60\":21,\"65\":2,\"70\":5,\"83\":68,\"92\":6,\"94\":2,\"97\":2,\"98\":1,\"102\":1,\"106\":1,\"107\":8,\"137\":1,\"138\":3,\"141\":4,\"145\":1,\"146\":2,\"150\":2,\"152\":5,\"154\":2,\"155\":3,\"156\":4,\"190\":11,\"191\":2,\"192\":11,\"206\":8,\"207\":3,\"208\":2,\"213\":8,\"255\":2,\"256\":3,\"257\":2,\"258\":1,\"260\":2,\"261\":3,\"274\":2,\"293\":4,\"380\":2,\"384\":8,\"385\":21,\"386\":7,\"407\":4,\"410\":7,\"411\":4,\"412\":11,\"417\":2,\"418\":2,\"419\":3,\"444\":3,\"448\":2,\"449\":1,\"453\":2,\"454\":2,\"455\":1,\"459\":1,\"460\":8,\"461\":2,\"477\":4,\"491\":5,\"542\":2,\"582\":2,\"586\":1,\"587\":3,\"589\":2,\"590\":1,\"595\":3,\"597\":6,\"660\":3,\"663\":2,\"696\":8,\"697\":7,\"700\":3,\"743\":1,\"771\":3,\"784\":4,\"787\":4,\"792\":1,\"795\":1,\"801\":5,\"803\":4,\"805\":8,\"806\":1,\"807\":9,\"808\":2,\"815\":16,\"816\":2,\"893\":4,\"895\":1,\"899\":6,\"900\":3,\"918\":2,\"926\":1,\"930\":1,\"931\":2,\"932\":2,\"934\":1,\"937\":2,\"938\":2,\"939\":1,\"963\":6,\"964\":2}}],[\"ftfy\",{\"1\":{\"633\":1}}],[\"ft\",{\"1\":{\"48\":1}}],[\"france\",{\"1\":{\"735\":2}}],[\"french\",{\"1\":{\"640\":3}}],[\"freeze\",{\"1\":{\"293\":1}}],[\"free\",{\"0\":{\"894\":1},\"1\":{\"285\":1,\"894\":1}}],[\"frequencies\",{\"1\":{\"597\":5}}],[\"frequency\",{\"1\":{\"597\":10}}],[\"freqs\",{\"1\":{\"595\":25,\"596\":2,\"597\":30}}],[\"freq\",{\"1\":{\"204\":1,\"265\":1,\"595\":7,\"597\":5}}],[\"frozen\",{\"1\":{\"273\":2,\"417\":1}}],[\"frobenius\",{\"1\":{\"153\":2}}],[\"from=0\",{\"1\":{\"479\":1}}],[\"from\",{\"1\":{\"12\":1,\"14\":1,\"52\":7,\"55\":2,\"71\":1,\"107\":3,\"152\":1,\"187\":1,\"190\":2,\"192\":2,\"205\":3,\"213\":1,\"227\":1,\"252\":1,\"339\":1,\"410\":2,\"411\":2,\"412\":8,\"413\":2,\"417\":1,\"424\":2,\"435\":2,\"455\":1,\"478\":1,\"479\":1,\"513\":1,\"514\":1,\"516\":2,\"542\":1,\"597\":6,\"654\":1,\"660\":3,\"663\":5,\"690\":1,\"700\":1,\"713\":2,\"724\":1,\"751\":1,\"810\":7,\"815\":4,\"816\":3,\"918\":4,\"926\":4,\"930\":2,\"935\":1,\"964\":1}}],[\"frcnn\",{\"1\":{\"46\":1}}],[\"few\",{\"1\":{\"644\":1,\"646\":1,\"647\":1,\"648\":3,\"650\":2,\"656\":2,\"657\":2}}],[\"fe\",{\"1\":{\"393\":3}}],[\"featmap\",{\"1\":{\"582\":6}}],[\"feat相似度最大的那个query\",{\"1\":{\"418\":2}}],[\"feats\",{\"1\":{\"384\":12,\"385\":42,\"386\":5,\"417\":1,\"418\":5}}],[\"feat=false\",{\"1\":{\"154\":2,\"156\":1}}],[\"feat=true\",{\"1\":{\"154\":2,\"155\":1}}],[\"feat\",{\"1\":{\"94\":8,\"100\":8,\"119\":9,\"122\":14,\"154\":10,\"155\":4,\"156\":4,\"190\":22,\"191\":4,\"192\":18,\"206\":25,\"417\":1,\"418\":8,\"918\":5}}],[\"featured\",{\"1\":{\"696\":1}}],[\"feature的方法快了4倍\",{\"1\":{\"394\":1}}],[\"feature的方法速度快了60倍\",{\"1\":{\"394\":1}}],[\"feature方法直接使用cnn提取grid的特征\",{\"1\":{\"391\":1}}],[\"feature方法通常采用faster\",{\"1\":{\"391\":1}}],[\"features=none\",{\"1\":{\"429\":2}}],[\"features=mlp\",{\"1\":{\"380\":3,\"429\":1}}],[\"features=dim\",{\"1\":{\"380\":3,\"429\":1}}],[\"features\",{\"1\":{\"59\":1,\"64\":1,\"83\":2,\"119\":2,\"143\":2,\"207\":4,\"213\":9,\"266\":2,\"362\":2,\"380\":2,\"385\":8,\"408\":10,\"410\":4,\"412\":4,\"413\":1,\"427\":3,\"428\":3,\"429\":15,\"431\":5,\"547\":1,\"713\":1}}],[\"feature\",{\"1\":{\"23\":1,\"59\":2,\"60\":1,\"64\":18,\"65\":19,\"69\":9,\"70\":10,\"83\":20,\"97\":2,\"100\":1,\"131\":2,\"137\":1,\"141\":1,\"143\":5,\"144\":1,\"145\":1,\"146\":2,\"153\":3,\"154\":5,\"155\":4,\"156\":3,\"273\":1,\"340\":1,\"361\":1,\"388\":2,\"502\":3,\"892\":1}}],[\"feedforward\",{\"1\":{\"402\":1,\"892\":2}}],[\"feedback\",{\"1\":{\"339\":1,\"602\":2,\"652\":1,\"654\":1,\"655\":1}}],[\"feed\",{\"1\":{\"7\":1,\"380\":1,\"633\":1,\"663\":2,\"741\":1,\"746\":4,\"749\":5}}],[\"func函数\",{\"1\":{\"815\":1}}],[\"funcs\",{\"1\":{\"787\":4,\"801\":4,\"803\":4,\"805\":5,\"807\":5,\"815\":5}}],[\"func\",{\"1\":{\"447\":2,\"449\":5,\"451\":3,\"452\":2,\"453\":4,\"454\":6,\"455\":4,\"457\":5,\"458\":3,\"459\":4,\"460\":2,\"461\":11,\"783\":2,\"805\":6,\"807\":3,\"815\":10}}],[\"functools\",{\"1\":{\"382\":1,\"447\":1,\"454\":3}}],[\"function及运算符相关类\",{\"1\":{\"810\":1}}],[\"function引用variable\",{\"1\":{\"806\":1}}],[\"function和variable之间原本存在强引用循环\",{\"1\":{\"806\":1}}],[\"function和variable实例存在循环引用\",{\"1\":{\"806\":1}}],[\"function实例引用输入和输出的variable实例\",{\"1\":{\"806\":1}}],[\"functional\",{\"0\":{\"477\":1},\"1\":{\"102\":1,\"190\":2,\"192\":2,\"205\":2,\"361\":1,\"362\":2,\"503\":1,\"582\":2,\"930\":1}}],[\"function\",{\"1\":{\"52\":2,\"119\":1,\"121\":1,\"150\":1,\"160\":1,\"359\":1,\"431\":1,\"447\":1,\"500\":1,\"724\":1,\"762\":2,\"766\":1,\"779\":1,\"780\":1,\"783\":1,\"799\":1,\"800\":2,\"805\":2,\"806\":1,\"807\":1,\"809\":6,\"810\":1,\"814\":1,\"815\":1,\"816\":1,\"823\":2,\"846\":1,\"847\":1,\"918\":1,\"938\":1}}],[\"functions\",{\"0\":{\"39\":1},\"1\":{\"810\":1}}],[\"fuse\",{\"1\":{\"193\":2}}],[\"fusion\",{\"0\":{\"38\":1},\"1\":{\"14\":1,\"54\":2,\"57\":1,\"58\":3,\"59\":6,\"60\":3,\"64\":1,\"65\":1,\"69\":2,\"83\":2,\"94\":1,\"207\":2,\"208\":2,\"220\":1,\"222\":1,\"368\":1,\"369\":1,\"370\":1,\"377\":1}}],[\"furthestsampling\",{\"1\":{\"121\":2}}],[\"fullattncatblock\",{\"1\":{\"98\":1}}],[\"full\",{\"1\":{\"12\":1,\"121\":1,\"190\":1,\"208\":3,\"320\":1,\"384\":1,\"385\":1,\"602\":1,\"611\":2,\"681\":1,\"710\":1,\"871\":1,\"896\":1}}],[\"fake\",{\"1\":{\"918\":4}}],[\"fairseq\",{\"1\":{\"680\":1}}],[\"far\",{\"1\":{\"321\":2}}],[\"farthest\",{\"1\":{\"121\":1,\"137\":8,\"141\":1,\"143\":1}}],[\"fallback\",{\"1\":{\"809\":1}}],[\"falcon等\",{\"1\":{\"299\":1}}],[\"false=未初始化\",{\"1\":{\"213\":1}}],[\"false\",{\"1\":{\"64\":1,\"154\":1,\"187\":3,\"190\":2,\"191\":1,\"208\":2,\"213\":5,\"255\":2,\"256\":3,\"293\":1,\"361\":2,\"382\":38,\"435\":1,\"474\":1,\"476\":1,\"488\":2,\"490\":2,\"491\":1,\"513\":1,\"518\":1,\"521\":2,\"590\":6,\"596\":1,\"597\":1,\"663\":2,\"807\":2,\"892\":8,\"895\":1,\"899\":6,\"900\":7,\"918\":1,\"963\":1,\"964\":1}}],[\"facial\",{\"1\":{\"408\":1}}],[\"face\",{\"1\":{\"382\":1}}],[\"facebook\",{\"1\":{\"354\":1}}],[\"facebookresearch\",{\"1\":{\"279\":1,\"347\":1,\"582\":1}}],[\"factor=args\",{\"1\":{\"582\":2}}],[\"factor=none\",{\"1\":{\"503\":2}}],[\"factor\",{\"1\":{\"83\":3,\"503\":3}}],[\"factors\",{\"1\":{\"12\":1}}],[\"faucet\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"fast\",{\"1\":{\"696\":2}}],[\"fastai\",{\"1\":{\"696\":1}}],[\"faster\",{\"1\":{\"269\":1,\"369\":1,\"501\":1,\"502\":1}}],[\"fast=false\",{\"1\":{\"52\":1}}],[\"fashion\",{\"1\":{\"11\":1}}],[\"fool\",{\"1\":{\"918\":1}}],[\"found\",{\"1\":{\"411\":1,\"412\":1,\"424\":1,\"967\":1}}],[\"foundation\",{\"1\":{\"20\":1,\"267\":2,\"294\":1,\"604\":1,\"664\":1,\"673\":1,\"826\":1}}],[\"folded\",{\"1\":{\"866\":1}}],[\"folders\",{\"1\":{\"28\":1,\"71\":1}}],[\"follow\",{\"1\":{\"652\":1,\"746\":1,\"749\":1}}],[\"following\",{\"1\":{\"339\":2,\"655\":1,\"713\":1}}],[\"focalloss\",{\"1\":{\"589\":2}}],[\"focal\",{\"0\":{\"589\":1},\"1\":{\"6\":1,\"11\":1,\"12\":1,\"39\":1,\"64\":1,\"79\":1,\"102\":6,\"588\":1,\"589\":23}}],[\"forgetting\",{\"1\":{\"602\":1}}],[\"forge\",{\"1\":{\"557\":1}}],[\"fortran\",{\"1\":{\"542\":1}}],[\"forcing\",{\"1\":{\"271\":1}}],[\"foreign\",{\"1\":{\"219\":2}}],[\"format=\",{\"1\":{\"582\":1}}],[\"format\",{\"1\":{\"106\":1,\"265\":1,\"424\":5,\"435\":1,\"713\":1,\"805\":1,\"815\":5}}],[\"former的生成方法\",{\"1\":{\"421\":1}}],[\"former学习\",{\"1\":{\"420\":1}}],[\"former类比为一个self\",{\"1\":{\"417\":1}}],[\"former模块做模态融合\",{\"1\":{\"415\":1}}],[\"former\",{\"1\":{\"64\":1,\"417\":4,\"420\":2,\"421\":3}}],[\"forward\",{\"0\":{\"362\":1},\"1\":{\"54\":1,\"56\":1,\"57\":1,\"58\":2,\"59\":2,\"60\":2,\"64\":1,\"65\":1,\"69\":2,\"70\":1,\"83\":7,\"94\":2,\"96\":3,\"97\":1,\"98\":3,\"99\":1,\"100\":2,\"102\":1,\"119\":2,\"120\":1,\"121\":2,\"122\":1,\"123\":1,\"137\":1,\"138\":1,\"141\":2,\"143\":1,\"145\":1,\"146\":1,\"152\":1,\"154\":1,\"155\":1,\"156\":1,\"187\":1,\"190\":1,\"191\":1,\"192\":1,\"206\":1,\"207\":1,\"208\":1,\"213\":3,\"255\":1,\"256\":2,\"258\":1,\"266\":4,\"274\":1,\"293\":2,\"357\":1,\"362\":1,\"380\":5,\"383\":2,\"385\":1,\"386\":1,\"397\":2,\"398\":1,\"399\":1,\"400\":2,\"401\":1,\"403\":3,\"417\":1,\"419\":1,\"420\":5,\"426\":1,\"427\":3,\"428\":3,\"429\":2,\"430\":1,\"431\":3,\"582\":1,\"586\":1,\"587\":1,\"588\":1,\"589\":1,\"590\":1,\"592\":1,\"633\":1,\"663\":5,\"699\":1,\"703\":1,\"709\":1,\"710\":1,\"716\":1,\"718\":3,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"724\":2,\"725\":1,\"726\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"734\":1,\"736\":1,\"737\":1,\"741\":1,\"742\":1,\"743\":1,\"745\":1,\"746\":5,\"747\":1,\"749\":6,\"750\":1,\"751\":1,\"762\":3,\"766\":1,\"779\":1,\"794\":1,\"800\":3,\"805\":1,\"807\":1,\"809\":5,\"893\":1,\"894\":1,\"895\":1,\"899\":2,\"900\":1,\"918\":2,\"926\":3,\"931\":1,\"937\":1,\"963\":4,\"964\":3}}],[\"fork\",{\"1\":{\"53\":1}}],[\"for\",{\"0\":{\"385\":1},\"1\":{\"6\":2,\"12\":1,\"14\":2,\"28\":1,\"53\":3,\"67\":1,\"82\":2,\"83\":3,\"92\":3,\"97\":1,\"104\":2,\"105\":2,\"106\":4,\"107\":1,\"119\":3,\"121\":3,\"122\":2,\"123\":2,\"129\":2,\"137\":3,\"141\":4,\"145\":2,\"152\":1,\"164\":2,\"179\":1,\"187\":2,\"188\":1,\"190\":6,\"192\":6,\"204\":1,\"207\":7,\"213\":4,\"219\":2,\"255\":1,\"263\":3,\"264\":8,\"265\":2,\"266\":1,\"273\":1,\"274\":3,\"293\":12,\"294\":1,\"347\":2,\"359\":3,\"361\":1,\"362\":1,\"363\":1,\"364\":1,\"366\":2,\"380\":2,\"381\":2,\"382\":7,\"383\":5,\"384\":1,\"385\":4,\"386\":6,\"398\":2,\"401\":5,\"408\":1,\"410\":6,\"411\":2,\"412\":8,\"419\":2,\"420\":8,\"424\":9,\"431\":2,\"435\":1,\"444\":4,\"482\":2,\"518\":1,\"582\":2,\"589\":1,\"595\":15,\"596\":5,\"597\":19,\"604\":1,\"605\":1,\"647\":1,\"663\":2,\"696\":4,\"697\":8,\"698\":7,\"699\":2,\"700\":4,\"719\":2,\"724\":5,\"729\":1,\"746\":1,\"747\":1,\"749\":1,\"750\":1,\"751\":1,\"800\":3,\"801\":3,\"803\":2,\"805\":6,\"806\":2,\"807\":8,\"809\":2,\"815\":5,\"816\":2,\"837\":1,\"892\":3,\"895\":2,\"898\":2,\"899\":2,\"918\":2,\"926\":4,\"934\":2,\"935\":2,\"938\":2,\"939\":1,\"963\":2,\"964\":7}}],[\"t=1\",{\"1\":{\"889\":1}}],[\"t指定输出格式\",{\"1\":{\"815\":1}}],[\"tying\",{\"1\":{\"699\":1}}],[\"typical\",{\"1\":{\"872\":1}}],[\"typing\",{\"1\":{\"597\":1}}],[\"typeerror\",{\"1\":{\"792\":1,\"805\":1,\"808\":1}}],[\"type=bert\",{\"1\":{\"712\":1}}],[\"type=none\",{\"1\":{\"380\":1}}],[\"type=int\",{\"1\":{\"293\":1,\"918\":7}}],[\"type=float\",{\"1\":{\"293\":2,\"918\":3}}],[\"type=args\",{\"1\":{\"265\":1}}],[\"type=\",{\"1\":{\"213\":3,\"384\":1,\"385\":4}}],[\"type==\",{\"1\":{\"53\":1,\"82\":1}}],[\"type\",{\"1\":{\"52\":2,\"53\":8,\"82\":9,\"92\":1,\"208\":3,\"213\":15,\"264\":2,\"265\":1,\"266\":1,\"380\":4,\"384\":7,\"385\":8,\"392\":3,\"419\":1,\"713\":9,\"715\":4,\"716\":9,\"721\":3,\"722\":3,\"731\":3,\"733\":2,\"734\":3,\"735\":1,\"736\":3,\"737\":7,\"792\":1,\"805\":1,\"808\":1,\"918\":1,\"926\":6,\"964\":5}}],[\"types\",{\"1\":{\"14\":1,\"892\":3}}],[\"tgz\",{\"1\":{\"696\":2}}],[\"tgt2\",{\"1\":{\"100\":6}}],[\"tgt\",{\"1\":{\"94\":1,\"100\":21,\"742\":12,\"749\":2,\"750\":2}}],[\"t0++\",{\"1\":{\"655\":1}}],[\"t0\",{\"1\":{\"655\":1}}],[\"tl\",{\"1\":{\"640\":1,\"641\":1}}],[\"tlr\",{\"1\":{\"104\":1}}],[\"tqdm\",{\"1\":{\"595\":1,\"597\":3}}],[\"tverskyloss\",{\"1\":{\"590\":2}}],[\"tversky指数简化为dice系数\",{\"1\":{\"590\":1}}],[\"tversky\",{\"0\":{\"590\":1},\"1\":{\"590\":9,\"592\":1}}],[\"tnews\",{\"1\":{\"712\":4}}],[\"tn\",{\"1\":{\"561\":3,\"562\":1}}],[\"tpu\",{\"1\":{\"824\":1}}],[\"tp+α⋅fp+β⋅fn\",{\"1\":{\"590\":1}}],[\"tp\",{\"1\":{\"561\":3,\"562\":1,\"588\":1,\"590\":6}}],[\"tpr\",{\"1\":{\"106\":1,\"563\":2,\"569\":3,\"572\":3}}],[\"tf\",{\"1\":{\"507\":1,\"712\":2}}],[\"t2q\",{\"1\":{\"418\":2}}],[\"t2i\",{\"1\":{\"190\":13,\"192\":12,\"206\":8,\"207\":4,\"383\":1,\"385\":10,\"386\":6,\"418\":4,\"419\":5}}],[\"tinypytorch计算图转换为dot语言\",{\"1\":{\"815\":1}}],[\"tinypytorch采用define\",{\"1\":{\"811\":1}}],[\"tinypytorch的动态计算图模式使其在易用性和灵活性上表现突出\",{\"1\":{\"811\":1}}],[\"tinypytorch的核心能力总结\",{\"1\":{\"811\":1}}],[\"tinypytorch的计算图需要动态构建和销毁\",{\"1\":{\"806\":1}}],[\"tinypytorch框架能自动处理复杂表达式的微分\",{\"1\":{\"811\":1}}],[\"tinypytorch框架实现了完整的运算符重载体系\",{\"1\":{\"809\":1}}],[\"tinypytorch当前的反向传播会保留所有变量的导数\",{\"1\":{\"807\":1}}],[\"tinypytorch中的循环引用\",{\"1\":{\"806\":1}}],[\"tinypytorch尽量用最少的代码实现了现代深度学习框架的功能\",{\"1\":{\"753\":1}}],[\"tinypytorch\",{\"0\":{\"820\":1},\"1\":{\"752\":2,\"753\":1,\"798\":2,\"799\":2,\"810\":1,\"813\":2,\"814\":2,\"818\":2,\"819\":1}}],[\"tinybert\",{\"1\":{\"689\":2}}],[\"tie\",{\"1\":{\"699\":1}}],[\"title\",{\"1\":{\"411\":1,\"412\":1,\"424\":1,\"816\":1,\"935\":2}}],[\"timing\",{\"1\":{\"461\":1}}],[\"timm\",{\"0\":{\"510\":1},\"1\":{\"380\":1,\"510\":8,\"511\":3}}],[\"time\",{\"1\":{\"268\":1,\"410\":10,\"412\":10,\"461\":5,\"663\":1,\"822\":1}}],[\"t5策略\",{\"1\":{\"710\":1}}],[\"t5relativepositionbias\",{\"1\":{\"710\":1}}],[\"t5\",{\"0\":{\"710\":1},\"1\":{\"268\":1,\"339\":1,\"346\":1,\"650\":2,\"671\":1,\"710\":9,\"735\":1}}],[\"twitter\",{\"1\":{\"894\":1}}],[\"twins\",{\"1\":{\"246\":1}}],[\"twocropstransform\",{\"1\":{\"359\":3}}],[\"two\",{\"1\":{\"52\":2,\"359\":1,\"416\":2,\"422\":1,\"713\":1}}],[\"tmp\",{\"1\":{\"122\":3,\"815\":5}}],[\"t×d\",{\"1\":{\"96\":1}}],[\"tutorial\",{\"0\":{\"941\":1},\"1\":{\"941\":2}}],[\"turbo\",{\"1\":{\"823\":1}}],[\"turn\",{\"1\":{\"747\":1}}],[\"tuned\",{\"1\":{\"673\":1}}],[\"tuned模型\",{\"1\":{\"648\":3}}],[\"tuned大型模型表现\",{\"1\":{\"648\":1}}],[\"tuned方法的能力\",{\"1\":{\"647\":1}}],[\"tune\",{\"1\":{\"339\":1}}],[\"tuning完全不相同的另一条技术路线\",{\"1\":{\"606\":1}}],[\"tuning和prefix\",{\"1\":{\"606\":1}}],[\"tuning也保证了基座模型本身是没有变的\",{\"1\":{\"605\":1}}],[\"tuning是在transformer的encoder和decoder的网络中都加了一些特定的前缀\",{\"1\":{\"605\":1}}],[\"tuning是在embedding环节\",{\"1\":{\"605\":1}}],[\"tuning是发生在embedding这个环节的\",{\"1\":{\"604\":1}}],[\"tuning就是在保证函数本身不变的前提下\",{\"1\":{\"604\":1}}],[\"tuning的是类似的\",{\"1\":{\"605\":1}}],[\"tuning的灵感来源是\",{\"1\":{\"605\":1}}],[\"tuning的具体细节\",{\"1\":{\"604\":1,\"605\":1}}],[\"tuning的基本原理是在输入序列x之前\",{\"1\":{\"604\":1}}],[\"tuning的出发点\",{\"1\":{\"604\":1,\"605\":1}}],[\"tuning\",{\"0\":{\"34\":1,\"240\":1,\"346\":2,\"599\":1,\"604\":1,\"605\":1,\"694\":1},\"1\":{\"237\":1,\"305\":1,\"308\":1,\"320\":2,\"339\":3,\"340\":1,\"346\":13,\"599\":1,\"602\":3,\"604\":1,\"605\":1,\"607\":1,\"610\":7,\"611\":2,\"625\":1,\"647\":1,\"656\":1,\"694\":1}}],[\"tuple\",{\"1\":{\"64\":1,\"123\":1,\"208\":1,\"263\":1,\"266\":1,\"293\":2,\"380\":1,\"382\":1,\"424\":1,\"427\":2,\"430\":1,\"445\":2,\"482\":2,\"595\":2,\"596\":1,\"597\":6,\"663\":7,\"733\":1,\"800\":2,\"801\":1,\"803\":1,\"805\":2,\"807\":2,\"893\":1,\"899\":1}}],[\"txt文件\",{\"1\":{\"712\":1}}],[\"txt\",{\"1\":{\"64\":1,\"417\":1,\"557\":2,\"739\":2,\"815\":8}}],[\"t\",{\"0\":{\"867\":2},\"1\":{\"54\":9,\"58\":5,\"59\":3,\"60\":6,\"94\":11,\"96\":1,\"100\":11,\"104\":1,\"106\":5,\"143\":1,\"148\":1,\"150\":2,\"152\":3,\"157\":2,\"190\":6,\"191\":1,\"192\":4,\"206\":3,\"213\":3,\"274\":1,\"359\":1,\"361\":4,\"362\":1,\"364\":1,\"371\":2,\"373\":2,\"375\":1,\"380\":1,\"385\":2,\"407\":15,\"408\":2,\"410\":1,\"412\":1,\"424\":1,\"490\":1,\"491\":1,\"492\":1,\"522\":6,\"538\":1,\"540\":3,\"542\":3,\"545\":16,\"589\":2,\"596\":2,\"700\":1,\"703\":1,\"807\":3,\"815\":2,\"867\":2,\"897\":7,\"899\":5,\"900\":4,\"963\":1}}],[\"term\",{\"1\":{\"696\":1}}],[\"terms\",{\"1\":{\"293\":3,\"597\":3,\"734\":1}}],[\"telgarsky\",{\"1\":{\"500\":1}}],[\"te和mi分别是visual\",{\"1\":{\"390\":1}}],[\"te\",{\"1\":{\"390\":1}}],[\"tesla\",{\"1\":{\"236\":1}}],[\"testcase\",{\"1\":{\"794\":1,\"795\":1}}],[\"testenv\",{\"1\":{\"553\":2}}],[\"test\",{\"0\":{\"513\":1},\"1\":{\"82\":1,\"89\":1,\"104\":4,\"106\":1,\"187\":3,\"190\":4,\"381\":9,\"382\":28,\"410\":2,\"412\":1,\"513\":16,\"625\":1,\"626\":1,\"641\":1,\"696\":8,\"794\":2,\"795\":1,\"822\":1}}],[\"teacher\",{\"1\":{\"208\":2,\"213\":18,\"271\":1,\"282\":1,\"285\":1,\"293\":40}}],[\"tensordataset\",{\"1\":{\"713\":1}}],[\"tensorrt\",{\"1\":{\"545\":1}}],[\"tensor2\",{\"1\":{\"475\":1}}],[\"tensor1\",{\"1\":{\"475\":1}}],[\"tensors\",{\"1\":{\"466\":2}}],[\"tensors=\",{\"1\":{\"64\":1,\"187\":1,\"188\":1,\"190\":1,\"191\":1,\"192\":1,\"204\":1,\"410\":2,\"412\":2,\"417\":1,\"663\":1}}],[\"tensorboard\",{\"1\":{\"384\":1}}],[\"tensorflow\",{\"1\":{\"130\":1,\"147\":1,\"547\":1}}],[\"tensor\",{\"0\":{\"479\":1,\"482\":1,\"486\":1,\"487\":1,\"490\":1,\"491\":1},\"1\":{\"67\":1,\"82\":2,\"83\":2,\"100\":6,\"119\":3,\"122\":1,\"188\":1,\"208\":1,\"213\":3,\"256\":1,\"260\":1,\"264\":3,\"293\":3,\"386\":3,\"397\":2,\"400\":2,\"408\":1,\"424\":1,\"425\":2,\"430\":1,\"466\":2,\"469\":1,\"470\":1,\"471\":4,\"472\":5,\"474\":5,\"475\":1,\"477\":7,\"478\":3,\"479\":1,\"480\":11,\"481\":5,\"482\":6,\"483\":1,\"484\":1,\"485\":7,\"486\":1,\"487\":3,\"492\":3,\"494\":2,\"540\":3,\"542\":3,\"544\":13,\"545\":4,\"586\":4,\"587\":4,\"588\":4,\"589\":4,\"660\":2,\"663\":4,\"700\":2,\"710\":1,\"718\":2,\"720\":2,\"725\":2,\"726\":3,\"734\":1,\"892\":1,\"896\":1,\"898\":2,\"899\":1,\"900\":1,\"918\":5}}],[\"tennisracket\",{\"1\":{\"53\":1}}],[\"temp=0\",{\"1\":{\"293\":1}}],[\"temperature=1\",{\"1\":{\"897\":1}}],[\"temperature=temperature\",{\"1\":{\"895\":1,\"898\":1}}],[\"temperature\",{\"1\":{\"255\":3,\"256\":1,\"274\":1,\"355\":1,\"361\":1,\"362\":1,\"407\":1,\"895\":1,\"897\":4,\"898\":1,\"899\":5,\"900\":2}}],[\"temp3\",{\"1\":{\"102\":2}}],[\"temp2\",{\"1\":{\"102\":3}}],[\"temp1\",{\"1\":{\"102\":3}}],[\"temp\",{\"1\":{\"53\":2,\"82\":2,\"102\":2,\"105\":2,\"106\":5,\"190\":7,\"192\":5,\"205\":4,\"206\":4,\"256\":4,\"257\":1,\"293\":22,\"418\":2,\"697\":2,\"899\":5,\"900\":3}}],[\"textual\",{\"1\":{\"634\":1}}],[\"text和text\",{\"1\":{\"418\":1}}],[\"text最相关的视觉信息\",{\"1\":{\"417\":1}}],[\"text转化为18291个类别\",{\"1\":{\"413\":1}}],[\"texts\",{\"1\":{\"407\":1,\"408\":2,\"410\":1,\"412\":1,\"898\":4}}],[\"text→image\",{\"1\":{\"385\":2}}],[\"textonly\",{\"1\":{\"383\":2}}],[\"text=texts\",{\"1\":{\"410\":1,\"412\":1}}],[\"text=true\",{\"1\":{\"384\":2}}],[\"text=false\",{\"1\":{\"384\":1,\"385\":2,\"386\":3}}],[\"text=1\",{\"1\":{\"382\":1}}],[\"text=0\",{\"1\":{\"382\":4,\"385\":1}}],[\"text=self\",{\"1\":{\"382\":3}}],[\"textmlm\",{\"1\":{\"380\":2,\"383\":1}}],[\"textvqa\",{\"1\":{\"335\":1,\"337\":1}}],[\"textcaps\",{\"1\":{\"332\":1}}],[\"text\",{\"0\":{\"199\":1,\"201\":1,\"386\":1,\"418\":1,\"419\":1,\"420\":1},\"1\":{\"23\":1,\"53\":18,\"54\":7,\"58\":1,\"64\":3,\"67\":1,\"94\":4,\"95\":1,\"100\":2,\"104\":2,\"107\":7,\"171\":2,\"172\":4,\"187\":7,\"188\":2,\"190\":60,\"191\":17,\"192\":60,\"201\":1,\"204\":4,\"205\":21,\"206\":30,\"207\":24,\"208\":7,\"267\":2,\"272\":1,\"274\":56,\"309\":1,\"368\":2,\"370\":2,\"373\":2,\"380\":22,\"382\":24,\"383\":1,\"384\":30,\"385\":53,\"386\":50,\"390\":2,\"393\":1,\"397\":1,\"402\":4,\"407\":6,\"408\":13,\"410\":9,\"411\":9,\"412\":16,\"415\":2,\"417\":17,\"418\":11,\"419\":32,\"420\":5,\"421\":1,\"424\":1,\"477\":3,\"595\":2,\"597\":2,\"640\":2,\"660\":7,\"663\":2,\"681\":1,\"697\":8,\"713\":2,\"883\":1,\"890\":1,\"892\":35,\"893\":34,\"895\":23,\"898\":26,\"900\":48}}],[\"triviaqa零样本68\",{\"1\":{\"668\":1}}],[\"trinh\",{\"1\":{\"641\":1}}],[\"trilinear\",{\"1\":{\"503\":1}}],[\"tripod\",{\"1\":{\"408\":1}}],[\"trick\",{\"1\":{\"83\":1,\"258\":1,\"893\":1,\"946\":1,\"947\":1}}],[\"treebank\",{\"1\":{\"634\":1,\"641\":1,\"696\":1}}],[\"tree\",{\"1\":{\"209\":1,\"219\":1,\"226\":1,\"367\":1,\"378\":1,\"414\":1}}],[\"try\",{\"1\":{\"97\":1,\"107\":1,\"410\":2,\"411\":1,\"412\":3,\"444\":2,\"807\":1,\"815\":1,\"963\":1,\"964\":1}}],[\"truncate\",{\"1\":{\"595\":2,\"597\":4}}],[\"truncation=true\",{\"1\":{\"64\":1,\"187\":1,\"190\":1,\"191\":1,\"192\":1,\"204\":1,\"417\":1}}],[\"truncation\",{\"1\":{\"64\":2}}],[\"trunc\",{\"1\":{\"380\":2,\"427\":1,\"428\":2,\"431\":2}}],[\"truths\",{\"1\":{\"918\":1}}],[\"truthfulqa\",{\"1\":{\"655\":1,\"656\":1,\"657\":2,\"670\":1}}],[\"truth\",{\"0\":{\"88\":1},\"1\":{\"102\":2,\"106\":2,\"207\":1,\"355\":1,\"385\":7,\"586\":2,\"587\":1,\"588\":2,\"589\":1,\"590\":1}}],[\"truediv\",{\"1\":{\"809\":1}}],[\"true=已初始化\",{\"1\":{\"213\":1}}],[\"true\",{\"1\":{\"53\":1,\"64\":2,\"100\":1,\"106\":5,\"121\":2,\"141\":1,\"154\":1,\"187\":1,\"190\":1,\"191\":2,\"192\":1,\"205\":1,\"207\":2,\"208\":3,\"213\":4,\"258\":1,\"274\":1,\"380\":1,\"382\":1,\"424\":1,\"435\":1,\"440\":2,\"474\":3,\"476\":1,\"480\":2,\"488\":3,\"490\":2,\"491\":3,\"513\":1,\"518\":1,\"521\":2,\"545\":1,\"588\":1,\"590\":3,\"592\":1,\"596\":1,\"597\":1,\"663\":2,\"807\":1,\"816\":1,\"892\":4,\"899\":3,\"918\":1,\"926\":8,\"963\":1,\"964\":1}}],[\"trust\",{\"1\":{\"52\":2}}],[\"traditional\",{\"1\":{\"660\":5}}],[\"traversal\",{\"1\":{\"626\":1}}],[\"trange\",{\"1\":{\"595\":3,\"597\":2}}],[\"translate\",{\"1\":{\"640\":1}}],[\"translation\",{\"1\":{\"422\":1}}],[\"trans\",{\"1\":{\"153\":6,\"154\":11,\"155\":4,\"156\":4,\"545\":7}}],[\"transitionup\",{\"1\":{\"122\":3,\"123\":3}}],[\"transitionup层的主要作用是在点云处理中恢复分辨率并融合多尺度特征\",{\"1\":{\"122\":1}}],[\"transitiondown\",{\"1\":{\"121\":3,\"122\":2,\"123\":2}}],[\"transitiondown层的主要作用是在点云处理中进行层次化的特征学习和分辨率降低\",{\"1\":{\"121\":1}}],[\"transition\",{\"1\":{\"116\":2}}],[\"transposed\",{\"1\":{\"545\":1}}],[\"transpose\",{\"0\":{\"467\":1,\"545\":1},\"1\":{\"53\":2,\"58\":1,\"59\":2,\"60\":1,\"82\":1,\"83\":3,\"92\":1,\"94\":7,\"97\":2,\"100\":1,\"107\":1,\"119\":4,\"121\":2,\"145\":1,\"153\":1,\"154\":4,\"156\":1,\"207\":5,\"266\":1,\"364\":1,\"380\":4,\"401\":6,\"420\":8,\"426\":1,\"430\":4,\"467\":1,\"468\":3,\"469\":1,\"470\":1,\"490\":3,\"494\":1,\"545\":2,\"582\":2,\"663\":3,\"703\":1,\"709\":5,\"710\":2,\"724\":5,\"751\":3}}],[\"transf\",{\"1\":{\"898\":4}}],[\"transfo\",{\"1\":{\"293\":2}}],[\"transfo2\",{\"1\":{\"293\":2}}],[\"transfo1\",{\"1\":{\"293\":2}}],[\"transform=data\",{\"1\":{\"425\":2}}],[\"transform=none\",{\"1\":{\"424\":1}}],[\"transform=transforms\",{\"1\":{\"918\":1}}],[\"transform=transform\",{\"1\":{\"264\":1,\"293\":1,\"926\":1,\"933\":1,\"963\":1}}],[\"transform=feature\",{\"1\":{\"155\":1,\"156\":2}}],[\"transform=false\",{\"1\":{\"155\":1,\"156\":1}}],[\"transforms\",{\"1\":{\"264\":11,\"293\":14,\"359\":7,\"425\":10,\"582\":4,\"918\":5,\"926\":4,\"930\":1,\"933\":1,\"963\":2}}],[\"transformation\",{\"1\":{\"153\":1}}],[\"transformations\",{\"1\":{\"149\":1}}],[\"transform\",{\"1\":{\"153\":3,\"154\":5,\"155\":3,\"156\":2,\"187\":1,\"264\":14,\"265\":1,\"293\":1,\"359\":5,\"382\":13,\"403\":5,\"424\":5,\"425\":4,\"582\":2,\"728\":3,\"729\":2,\"926\":1,\"933\":1,\"963\":1}}],[\"transformed\",{\"1\":{\"122\":6}}],[\"transformer架构\",{\"1\":{\"650\":1}}],[\"transformer架构导入偏差是有帮助的\",{\"1\":{\"635\":1}}],[\"transformer比lstm能获取长距离信息\",{\"1\":{\"627\":1}}],[\"transformer跨各种各样任务的迁移性能更强\",{\"1\":{\"626\":1}}],[\"transformer中\",{\"1\":{\"522\":1}}],[\"transformer证明了使用transformer结构可以有效处理图像数据\",{\"1\":{\"436\":1}}],[\"transformer需要输入的是一维的token\",{\"1\":{\"426\":1}}],[\"transformer的核心流程实现\",{\"1\":{\"423\":1}}],[\"transformer的模型结构相比于transformer来说更简单\",{\"1\":{\"422\":1}}],[\"transformer是2021年谷歌在iclr上提出的算法\",{\"1\":{\"422\":1}}],[\"transformer结合起来用于多模态transformer\",{\"1\":{\"388\":1}}],[\"transformers\",{\"0\":{\"222\":1},\"1\":{\"225\":1,\"226\":2,\"227\":1,\"252\":1,\"279\":2,\"382\":1,\"412\":1,\"660\":1,\"663\":3}}],[\"transformer作为解码器\",{\"1\":{\"213\":1}}],[\"transformer作为编码器\",{\"1\":{\"213\":1}}],[\"transformer后的批归一化\",{\"1\":{\"120\":1}}],[\"transformer2\",{\"1\":{\"120\":2}}],[\"transformer论文中提出的向量自注意力机制\",{\"1\":{\"119\":1}}],[\"transformerdecoderlayer\",{\"1\":{\"100\":1}}],[\"transformer\",{\"0\":{\"108\":1,\"113\":1,\"115\":1,\"120\":1,\"123\":1,\"124\":1,\"233\":1,\"372\":2,\"380\":1,\"399\":1,\"534\":1},\"1\":{\"24\":1,\"94\":1,\"97\":2,\"100\":2,\"108\":3,\"109\":4,\"110\":1,\"112\":1,\"113\":1,\"115\":2,\"116\":1,\"117\":1,\"120\":5,\"123\":3,\"124\":2,\"125\":2,\"152\":1,\"157\":3,\"160\":1,\"171\":1,\"172\":2,\"195\":1,\"197\":2,\"208\":1,\"212\":2,\"214\":3,\"216\":1,\"217\":1,\"220\":4,\"221\":1,\"222\":2,\"224\":2,\"228\":3,\"231\":1,\"233\":4,\"234\":1,\"236\":3,\"237\":1,\"242\":1,\"246\":1,\"251\":1,\"252\":2,\"258\":1,\"264\":4,\"265\":3,\"266\":6,\"269\":3,\"271\":1,\"272\":3,\"274\":1,\"280\":4,\"286\":3,\"298\":1,\"304\":1,\"368\":6,\"369\":4,\"370\":2,\"371\":2,\"372\":3,\"374\":1,\"376\":9,\"377\":1,\"380\":8,\"384\":4,\"385\":15,\"387\":2,\"401\":1,\"402\":1,\"405\":3,\"407\":3,\"410\":2,\"413\":1,\"416\":1,\"422\":1,\"427\":3,\"429\":1,\"434\":10,\"435\":2,\"510\":2,\"523\":1,\"534\":1,\"611\":1,\"629\":2,\"630\":1,\"633\":3,\"640\":2,\"647\":1,\"650\":1,\"679\":1,\"690\":3,\"692\":1,\"694\":2,\"699\":3,\"706\":1,\"724\":1,\"735\":1,\"739\":5,\"741\":4,\"822\":1,\"823\":3,\"884\":2,\"885\":8,\"887\":2,\"889\":1,\"892\":8,\"893\":4,\"894\":1,\"895\":1,\"898\":3,\"900\":12}}],[\"transfer\",{\"1\":{\"14\":1,\"273\":1}}],[\"train=true\",{\"1\":{\"918\":1,\"926\":1,\"933\":1,\"963\":1}}],[\"trainer\",{\"1\":{\"381\":36,\"382\":12}}],[\"trained\",{\"1\":{\"14\":1,\"611\":1,\"712\":2}}],[\"traindir\",{\"1\":{\"359\":1}}],[\"trains=\",{\"1\":{\"187\":1,\"190\":1,\"192\":1}}],[\"training语言模型\",{\"1\":{\"625\":1}}],[\"training\",{\"0\":{\"198\":1,\"236\":1,\"374\":1},\"1\":{\"105\":1,\"129\":2,\"164\":2,\"165\":1,\"179\":1,\"213\":3,\"226\":2,\"237\":1,\"265\":2,\"283\":1,\"305\":2,\"340\":1,\"367\":2,\"376\":2,\"378\":1,\"381\":2,\"383\":2,\"384\":1,\"385\":1,\"386\":1,\"406\":1,\"424\":1,\"435\":1,\"624\":1,\"652\":1,\"663\":2,\"681\":1,\"683\":1,\"700\":1,\"823\":1,\"893\":2,\"918\":2}}],[\"train\",{\"0\":{\"204\":1,\"359\":1,\"513\":1},\"1\":{\"53\":8,\"82\":8,\"89\":1,\"92\":2,\"104\":4,\"105\":3,\"107\":5,\"187\":9,\"190\":7,\"192\":2,\"204\":1,\"213\":1,\"264\":1,\"265\":9,\"293\":4,\"359\":11,\"381\":6,\"382\":26,\"384\":2,\"385\":1,\"386\":1,\"409\":1,\"424\":7,\"425\":7,\"431\":1,\"513\":12,\"518\":8,\"520\":1,\"595\":1,\"597\":1,\"607\":1,\"696\":8,\"700\":1,\"712\":3,\"713\":1,\"715\":6,\"918\":2,\"926\":6,\"933\":3,\"934\":6,\"935\":1,\"938\":6,\"963\":5,\"964\":1}}],[\"trashcan\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"tax\",{\"1\":{\"655\":1,\"656\":1,\"657\":2,\"658\":1}}],[\"tag\",{\"1\":{\"627\":1}}],[\"tage\",{\"1\":{\"417\":1}}],[\"tabby\",{\"1\":{\"408\":1}}],[\"table\",{\"1\":{\"53\":1,\"82\":1,\"92\":1,\"106\":2,\"382\":1,\"680\":1,\"709\":1,\"710\":6}}],[\"takes\",{\"1\":{\"597\":1}}],[\"taken\",{\"1\":{\"410\":2,\"412\":1,\"724\":1}}],[\"take\",{\"1\":{\"359\":1,\"724\":1,\"742\":1,\"751\":1}}],[\"taking\",{\"1\":{\"12\":1,\"720\":1}}],[\"tao\",{\"1\":{\"355\":1}}],[\"tau\",{\"1\":{\"257\":1,\"258\":1,\"899\":1}}],[\"tau=temp\",{\"1\":{\"256\":1,\"257\":1,\"258\":1}}],[\"tanhdecay\",{\"1\":{\"510\":1}}],[\"tanh\",{\"1\":{\"213\":2,\"397\":1,\"431\":2,\"720\":1,\"814\":1,\"918\":1}}],[\"tangentconv\",{\"1\":{\"110\":1}}],[\"task\",{\"0\":{\"248\":1,\"350\":1,\"693\":1},\"1\":{\"213\":7,\"272\":1,\"383\":2,\"634\":1,\"640\":2,\"646\":1,\"701\":2,\"712\":1,\"713\":1}}],[\"tasks\",{\"1\":{\"7\":1,\"219\":2,\"294\":1,\"366\":2,\"376\":1,\"383\":9,\"386\":1,\"656\":1}}],[\"tails\",{\"1\":{\"868\":1}}],[\"tail\",{\"1\":{\"107\":1}}],[\"target=labels\",{\"1\":{\"265\":1}}],[\"target=false\",{\"1\":{\"261\":2}}],[\"target=true\",{\"1\":{\"256\":1,\"260\":1,\"261\":1}}],[\"targets=none\",{\"1\":{\"208\":1}}],[\"targets=labels\",{\"1\":{\"208\":1}}],[\"targets\",{\"1\":{\"105\":1,\"106\":11,\"187\":3,\"190\":8,\"192\":10,\"202\":1,\"206\":9,\"208\":5,\"362\":1,\"418\":3,\"586\":7,\"587\":11,\"588\":7,\"589\":7,\"590\":7,\"592\":8}}],[\"target\",{\"0\":{\"261\":1},\"1\":{\"102\":8,\"190\":2,\"192\":4,\"202\":1,\"208\":1,\"213\":14,\"261\":2,\"263\":6,\"359\":2,\"588\":1,\"589\":1,\"698\":2,\"742\":1,\"899\":1,\"926\":4,\"964\":3}}],[\"than\",{\"1\":{\"660\":6,\"899\":1}}],[\"that\",{\"1\":{\"52\":6,\"87\":1,\"660\":13}}],[\"thinking\",{\"1\":{\"823\":2}}],[\"think\",{\"1\":{\"619\":1}}],[\"this\",{\"1\":{\"12\":1,\"52\":3,\"67\":7,\"94\":1,\"107\":3,\"408\":1,\"724\":1}}],[\"th\",{\"1\":{\"582\":7}}],[\"threads\",{\"1\":{\"918\":1}}],[\"thre\",{\"1\":{\"106\":2}}],[\"thres=0\",{\"1\":{\"896\":2}}],[\"thres=filter\",{\"1\":{\"895\":1,\"898\":1}}],[\"thres\",{\"1\":{\"106\":2,\"895\":2,\"896\":1,\"898\":2}}],[\"threshold\",{\"1\":{\"106\":9,\"582\":2,\"586\":1}}],[\"thresholding\",{\"1\":{\"106\":2}}],[\"through\",{\"1\":{\"14\":1,\"255\":4,\"256\":1,\"257\":2,\"258\":4,\"747\":1,\"899\":8,\"959\":1,\"963\":2}}],[\"thought\",{\"0\":{\"33\":1,\"52\":1,\"620\":1},\"1\":{\"11\":1,\"31\":1,\"50\":1,\"620\":2,\"621\":1,\"825\":1}}],[\"theory\",{\"1\":{\"948\":1}}],[\"theorem\",{\"1\":{\"157\":1,\"500\":1,\"851\":1}}],[\"there\",{\"1\":{\"729\":1}}],[\"they\",{\"1\":{\"722\":1}}],[\"them\",{\"1\":{\"668\":1,\"670\":1}}],[\"their\",{\"1\":{\"597\":4,\"668\":1,\"670\":1}}],[\"then\",{\"1\":{\"220\":1,\"223\":1,\"237\":1}}],[\"thecvf\",{\"1\":{\"84\":1}}],[\"theta\",{\"1\":{\"69\":4,\"83\":4}}],[\"these\",{\"1\":{\"11\":1,\"734\":1}}],[\"the\",{\"0\":{\"870\":1},\"1\":{\"7\":4,\"11\":5,\"12\":1,\"14\":4,\"52\":56,\"55\":12,\"64\":1,\"83\":2,\"87\":2,\"102\":1,\"106\":2,\"107\":3,\"192\":1,\"202\":1,\"274\":1,\"321\":2,\"359\":1,\"361\":3,\"362\":1,\"363\":1,\"364\":1,\"385\":1,\"408\":1,\"411\":1,\"412\":1,\"424\":1,\"597\":2,\"604\":1,\"634\":1,\"681\":1,\"696\":1,\"713\":4,\"720\":3,\"724\":5,\"729\":3,\"734\":1,\"735\":2,\"736\":1,\"739\":1,\"747\":1,\"751\":2,\"815\":1,\"877\":1,\"895\":1,\"899\":1,\"900\":1,\"918\":3,\"921\":1}}],[\"tool\",{\"1\":{\"836\":1}}],[\"toothbrush\",{\"1\":{\"53\":1}}],[\"toxicity\",{\"0\":{\"670\":1}}],[\"to=1\",{\"1\":{\"479\":1}}],[\"totensor\",{\"1\":{\"264\":4,\"293\":1,\"359\":1,\"425\":2,\"582\":1,\"918\":1,\"926\":1,\"933\":1,\"963\":1}}],[\"total\",{\"0\":{\"850\":1},\"1\":{\"67\":2,\"102\":1,\"105\":1,\"106\":4,\"190\":1,\"213\":1,\"293\":4,\"383\":2,\"385\":2,\"410\":4,\"412\":4,\"430\":3,\"588\":2,\"700\":3,\"701\":2,\"731\":2,\"734\":2,\"892\":14,\"893\":3,\"895\":3,\"926\":3,\"963\":6,\"964\":3}}],[\"touvron\",{\"1\":{\"176\":1}}],[\"topk返回最小的k个值及其索引\",{\"1\":{\"119\":1}}],[\"topk\",{\"0\":{\"488\":1},\"1\":{\"119\":2,\"408\":1,\"488\":3,\"896\":1}}],[\"top\",{\"0\":{\"896\":1},\"1\":{\"52\":2,\"87\":1,\"188\":2,\"263\":5,\"280\":3,\"291\":1,\"408\":2,\"421\":3,\"641\":1,\"895\":3,\"896\":8,\"898\":3}}],[\"token分配一个唯一的词索引\",{\"1\":{\"893\":1}}],[\"token分类任务\",{\"0\":{\"736\":1}}],[\"token分类\",{\"1\":{\"694\":2}}],[\"token能力\",{\"1\":{\"892\":1}}],[\"token用于分类任务即可\",{\"1\":{\"722\":1}}],[\"token序列\",{\"1\":{\"660\":1}}],[\"token窗口中\",{\"1\":{\"647\":1}}],[\"token提取出来\",{\"1\":{\"431\":1}}],[\"token对齐\",{\"1\":{\"427\":1}}],[\"token开头\",{\"1\":{\"421\":1}}],[\"token的作用\",{\"1\":{\"427\":1}}],[\"token的相似度\",{\"1\":{\"418\":1}}],[\"token的原始特征\",{\"1\":{\"384\":1}}],[\"token作为最后的相似度得分\",{\"1\":{\"418\":2}}],[\"token作为input\",{\"1\":{\"417\":1}}],[\"token是有效的\",{\"1\":{\"417\":1}}],[\"token压缩\",{\"1\":{\"329\":1}}],[\"token做投影\",{\"1\":{\"190\":2}}],[\"token\",{\"0\":{\"232\":1,\"427\":1},\"1\":{\"64\":2,\"67\":5,\"70\":3,\"96\":2,\"97\":4,\"100\":10,\"119\":2,\"171\":5,\"187\":4,\"188\":12,\"190\":1,\"192\":3,\"200\":2,\"201\":1,\"205\":1,\"206\":3,\"207\":1,\"208\":14,\"210\":5,\"212\":4,\"213\":4,\"214\":13,\"215\":4,\"220\":4,\"222\":1,\"223\":5,\"230\":1,\"232\":9,\"233\":1,\"234\":7,\"235\":3,\"236\":2,\"242\":4,\"248\":1,\"249\":2,\"250\":3,\"251\":4,\"252\":2,\"255\":6,\"256\":2,\"258\":2,\"260\":3,\"262\":3,\"263\":1,\"264\":8,\"265\":7,\"266\":24,\"268\":1,\"269\":1,\"271\":1,\"272\":7,\"273\":3,\"274\":21,\"277\":2,\"278\":1,\"286\":7,\"293\":1,\"305\":1,\"315\":1,\"319\":2,\"331\":2,\"341\":1,\"371\":1,\"373\":2,\"376\":2,\"380\":9,\"384\":16,\"385\":14,\"397\":3,\"402\":1,\"403\":1,\"417\":1,\"418\":1,\"420\":16,\"421\":5,\"427\":23,\"428\":5,\"431\":5,\"582\":6,\"595\":10,\"596\":4,\"597\":10,\"629\":1,\"631\":1,\"635\":1,\"660\":7,\"661\":2,\"663\":20,\"679\":2,\"691\":3,\"692\":4,\"697\":4,\"698\":2,\"699\":4,\"706\":2,\"707\":2,\"708\":3,\"709\":5,\"710\":2,\"713\":35,\"715\":4,\"716\":9,\"720\":4,\"721\":2,\"722\":2,\"729\":1,\"731\":3,\"733\":9,\"734\":2,\"735\":8,\"736\":4,\"737\":6,\"823\":3,\"885\":19,\"887\":15,\"891\":2,\"892\":18,\"893\":26,\"894\":1,\"895\":26,\"896\":3,\"897\":3,\"898\":22,\"899\":18,\"900\":5,\"963\":1}}],[\"tokenize方法进行分词\",{\"1\":{\"596\":1}}],[\"tokenize方法完成分词功能\",{\"1\":{\"595\":1}}],[\"tokenize方法完成断句功能\",{\"1\":{\"595\":1}}],[\"tokenize\",{\"1\":{\"192\":1,\"408\":1,\"595\":2,\"596\":2,\"597\":4,\"633\":1,\"696\":1,\"697\":4}}],[\"tokenizer=self\",{\"1\":{\"382\":1}}],[\"tokenizer生成标签\",{\"1\":{\"264\":1}}],[\"tokenizers\",{\"1\":{\"209\":2}}],[\"tokenizer\",{\"1\":{\"52\":5,\"64\":4,\"187\":6,\"188\":7,\"190\":4,\"191\":3,\"192\":7,\"204\":2,\"205\":4,\"208\":3,\"212\":4,\"214\":1,\"215\":2,\"216\":2,\"217\":3,\"223\":2,\"224\":1,\"232\":4,\"235\":1,\"236\":1,\"251\":1,\"264\":9,\"269\":1,\"382\":19,\"417\":1,\"420\":2,\"421\":4,\"596\":7,\"597\":1,\"660\":5,\"663\":6,\"667\":1,\"697\":1,\"698\":17,\"700\":6,\"735\":4,\"823\":1,\"898\":5}}],[\"tokenization\",{\"1\":{\"64\":1,\"733\":1}}],[\"tokens可知这些被掩码token对应的真实词作为label\",{\"1\":{\"700\":1}}],[\"tokens后hellaswag分数提升至82\",{\"1\":{\"668\":1}}],[\"tokens训练耗时约21天\",{\"1\":{\"667\":1}}],[\"tokens训练后性能持续提升\",{\"1\":{\"666\":1}}],[\"tokens即停止\",{\"1\":{\"666\":1}}],[\"tokens形状\",{\"1\":{\"421\":1}}],[\"tokens做attention\",{\"1\":{\"420\":1}}],[\"tokens的embeddings在seq\",{\"1\":{\"419\":1}}],[\"tokens同时输入bertmodel时\",{\"1\":{\"419\":1}}],[\"tokens部分的mask列表\",{\"1\":{\"713\":1}}],[\"tokens部分的每个位置都映射到2维匹配空间\",{\"1\":{\"419\":1}}],[\"tokens部分的结果\",{\"1\":{\"419\":1}}],[\"tokens拼接得到的结果和图像嵌入进行cross\",{\"1\":{\"419\":1}}],[\"tokens列表中所有句子掩码数量一致\",{\"1\":{\"700\":1}}],[\"tokens列表\",{\"1\":{\"419\":1}}],[\"tokens=padding\",{\"1\":{\"898\":1}}],[\"tokens=50\",{\"1\":{\"663\":1}}],[\"tokens=image\",{\"1\":{\"274\":1}}],[\"tokens=none\",{\"1\":{\"274\":2}}],[\"tokens=false\",{\"1\":{\"265\":2,\"266\":1,\"713\":1}}],[\"tokens=true\",{\"1\":{\"188\":1,\"213\":4,\"421\":1,\"735\":2}}],[\"tokens\",{\"1\":{\"7\":1,\"64\":3,\"99\":2,\"213\":13,\"235\":1,\"255\":7,\"256\":5,\"260\":4,\"264\":4,\"266\":4,\"274\":34,\"380\":2,\"408\":2,\"417\":9,\"418\":3,\"419\":16,\"420\":16,\"421\":6,\"427\":1,\"428\":2,\"431\":2,\"595\":4,\"596\":6,\"597\":7,\"633\":1,\"640\":1,\"647\":1,\"649\":1,\"660\":6,\"663\":4,\"666\":1,\"667\":2,\"680\":1,\"697\":17,\"698\":13,\"699\":4,\"700\":2,\"703\":4,\"713\":6,\"724\":1,\"733\":2,\"735\":2,\"823\":1,\"892\":37,\"893\":17,\"895\":9,\"898\":19,\"899\":14,\"900\":3}}],[\"torchvision\",{\"1\":{\"918\":3,\"926\":2,\"930\":1,\"963\":1}}],[\"torch==1\",{\"1\":{\"739\":1}}],[\"torchscript\",{\"1\":{\"430\":1}}],[\"torch\",{\"0\":{\"473\":1,\"477\":1,\"480\":1,\"481\":1,\"482\":1,\"483\":1,\"484\":1,\"485\":1,\"488\":1},\"1\":{\"52\":2,\"56\":8,\"58\":1,\"59\":2,\"60\":1,\"64\":1,\"65\":4,\"67\":6,\"69\":5,\"70\":3,\"82\":2,\"83\":13,\"94\":3,\"100\":5,\"102\":21,\"104\":3,\"105\":2,\"106\":1,\"107\":4,\"119\":15,\"121\":9,\"122\":4,\"123\":1,\"137\":12,\"141\":3,\"145\":3,\"152\":6,\"153\":4,\"154\":7,\"156\":4,\"187\":2,\"188\":1,\"190\":30,\"191\":1,\"192\":26,\"205\":4,\"206\":7,\"207\":14,\"208\":9,\"213\":17,\"256\":3,\"260\":3,\"265\":8,\"266\":1,\"274\":3,\"293\":14,\"359\":2,\"361\":2,\"362\":5,\"363\":1,\"364\":1,\"380\":12,\"381\":3,\"382\":2,\"384\":4,\"385\":3,\"386\":18,\"401\":2,\"408\":3,\"410\":4,\"412\":5,\"417\":1,\"418\":3,\"419\":17,\"420\":6,\"421\":2,\"424\":5,\"425\":2,\"427\":2,\"428\":3,\"431\":5,\"435\":1,\"463\":3,\"466\":3,\"469\":1,\"471\":2,\"472\":3,\"474\":2,\"475\":3,\"476\":1,\"477\":1,\"478\":3,\"480\":8,\"481\":6,\"482\":3,\"483\":2,\"484\":2,\"485\":8,\"486\":1,\"488\":2,\"490\":3,\"491\":3,\"503\":1,\"510\":1,\"518\":1,\"520\":1,\"521\":4,\"540\":3,\"542\":7,\"544\":2,\"545\":4,\"582\":4,\"588\":1,\"589\":6,\"592\":4,\"660\":6,\"663\":16,\"699\":2,\"700\":4,\"703\":2,\"709\":12,\"710\":19,\"715\":1,\"716\":2,\"724\":2,\"729\":1,\"733\":4,\"739\":1,\"751\":2,\"892\":5,\"893\":7,\"895\":3,\"896\":2,\"898\":6,\"899\":3,\"900\":4,\"918\":10,\"926\":15,\"930\":5,\"931\":4,\"932\":1,\"934\":2,\"935\":3,\"937\":5,\"938\":3,\"939\":3,\"963\":8,\"964\":10}}],[\"to\",{\"0\":{\"622\":1},\"1\":{\"6\":2,\"7\":2,\"11\":5,\"12\":1,\"14\":3,\"52\":1,\"60\":6,\"64\":1,\"83\":8,\"95\":1,\"102\":1,\"107\":3,\"121\":4,\"137\":7,\"187\":2,\"188\":2,\"190\":3,\"191\":2,\"192\":4,\"204\":2,\"206\":2,\"207\":1,\"208\":1,\"213\":5,\"265\":1,\"266\":3,\"274\":3,\"293\":1,\"321\":4,\"340\":1,\"362\":1,\"380\":3,\"385\":3,\"386\":1,\"407\":2,\"410\":6,\"412\":5,\"415\":1,\"417\":2,\"419\":3,\"420\":1,\"421\":2,\"426\":1,\"431\":3,\"435\":1,\"454\":3,\"479\":1,\"582\":1,\"595\":14,\"596\":3,\"597\":53,\"607\":2,\"622\":2,\"640\":1,\"652\":1,\"658\":1,\"660\":16,\"663\":2,\"700\":2,\"710\":2,\"712\":3,\"713\":1,\"720\":1,\"721\":1,\"724\":4,\"740\":1,\"751\":1,\"815\":5,\"883\":1,\"890\":1,\"892\":4,\"893\":1,\"898\":1,\"899\":1,\"900\":6,\"918\":3,\"926\":3,\"934\":2,\"935\":2,\"938\":3,\"939\":2,\"963\":2,\"964\":4}}],[\"现存问题\",{\"1\":{\"884\":1,\"944\":1}}],[\"现已更新到\",{\"1\":{\"823\":1}}],[\"现实中的神经网络操作往往不仅仅接受一个输入\",{\"1\":{\"800\":1}}],[\"现在维度一致了\",{\"1\":{\"963\":1}}],[\"现在倒好\",{\"1\":{\"956\":1}}],[\"现在所有的期望都关于不依赖模型参数的分布\",{\"1\":{\"946\":1}}],[\"现在来看公式\",{\"1\":{\"946\":1}}],[\"现在就变成了两个多元高斯分布之间的\",{\"1\":{\"946\":1}}],[\"现在剩下的问题是如何优化上述积分\",{\"1\":{\"944\":1}}],[\"现在采样的是\",{\"1\":{\"931\":1}}],[\"现在\",{\"1\":{\"924\":1,\"960\":1}}],[\"现在我们重新定义抽红球为\",{\"1\":{\"860\":1}}],[\"现在我们问\",{\"1\":{\"846\":1}}],[\"现在我们按\",{\"1\":{\"545\":1}}],[\"现在假设使用\",{\"1\":{\"694\":1}}],[\"现在的大模型要解决的问题\",{\"1\":{\"600\":1}}],[\"现在看一个典型的装饰器例子\",{\"1\":{\"449\":1}}],[\"现代深度学习的延伸\",{\"1\":{\"500\":1}}],[\"现有的vlp模型的text\",{\"1\":{\"391\":1}}],[\"现有的视觉语言模型的三种结构类别\",{\"1\":{\"390\":1}}],[\"现有的视觉\",{\"1\":{\"296\":1}}],[\"现有两类主流架构各有优缺点\",{\"1\":{\"368\":1}}],[\"现有模型通常通过图文匹配\",{\"1\":{\"368\":1}}],[\"现有工作表明\",{\"1\":{\"301\":1}}],[\"现有视觉\",{\"1\":{\"220\":1}}],[\"现有重建目标可以分为三类\",{\"1\":{\"210\":1}}],[\"现有做法\",{\"1\":{\"168\":1,\"169\":1}}],[\"现有点云注意力方法多为全局注意力\",{\"1\":{\"110\":1}}],[\"现有研究通常仅联合训练少量任务\",{\"1\":{\"639\":1}}],[\"现有研究可分为三类\",{\"1\":{\"75\":1}}],[\"现有研究主要分为两类\",{\"1\":{\"73\":1}}],[\"现有研究主要从2d数据\",{\"1\":{\"31\":1}}],[\"现有研究尝试提升\",{\"1\":{\"20\":1}}],[\"现有方法的局限性\",{\"1\":{\"646\":1}}],[\"现有方法仍需要针对每个任务进行大规模监督数据微调\",{\"1\":{\"646\":1}}],[\"现有方法主要有三类\",{\"1\":{\"109\":1}}],[\"现有方法依赖两类对齐策略\",{\"1\":{\"76\":1}}],[\"现有方法依赖数据对齐\",{\"1\":{\"31\":1}}],[\"现有方法\",{\"1\":{\"30\":1}}],[\"现有\",{\"1\":{\"19\":1,\"22\":1,\"330\":1}}],[\"现状问题\",{\"1\":{\"167\":1}}],[\"现状\",{\"1\":{\"6\":1,\"7\":1,\"9\":1,\"11\":1,\"12\":1,\"14\":1}}],[\"现就读于电子科技大学\",{\"1\":{\"3\":1}}],[\"现就读于四川大学\",{\"1\":{\"2\":1}}],[\"dgx\",{\"1\":{\"680\":1}}],[\"dgcnn\",{\"1\":{\"110\":1,\"157\":2,\"160\":1}}],[\"d0\",{\"1\":{\"545\":2}}],[\"dv\",{\"1\":{\"538\":3}}],[\"dvae\",{\"0\":{\"254\":1},\"1\":{\"228\":1,\"232\":2,\"249\":2,\"251\":1,\"252\":1,\"265\":2,\"885\":10,\"886\":1,\"887\":1}}],[\"dk\",{\"1\":{\"538\":2}}],[\"d2\",{\"1\":{\"458\":3,\"545\":2}}],[\"d1\",{\"1\":{\"458\":3,\"545\":2}}],[\"d1+d2\",{\"1\":{\"145\":1}}],[\"during\",{\"1\":{\"893\":1,\"918\":1}}],[\"dump\",{\"1\":{\"595\":3,\"597\":3,\"696\":2,\"697\":1,\"712\":1}}],[\"dumps\",{\"1\":{\"424\":1}}],[\"dual\",{\"1\":{\"220\":1,\"268\":1,\"271\":1,\"368\":1,\"369\":1,\"370\":1,\"377\":1}}],[\"dset\",{\"1\":{\"382\":4}}],[\"dms\",{\"1\":{\"382\":12}}],[\"dm\",{\"1\":{\"382\":13}}],[\"dpr\",{\"1\":{\"380\":1}}],[\"ddp\",{\"1\":{\"362\":2}}],[\"d为投影后的embedding维度\",{\"1\":{\"206\":1}}],[\"dtype属性\",{\"1\":{\"808\":1}}],[\"dtype\",{\"1\":{\"119\":2,\"122\":1,\"213\":5,\"481\":2,\"486\":1,\"721\":1,\"808\":3,\"815\":1,\"893\":1,\"898\":1}}],[\"dtype=next\",{\"1\":{\"721\":1}}],[\"dtype=none\",{\"1\":{\"481\":1,\"486\":1}}],[\"dtype=np\",{\"1\":{\"263\":1}}],[\"dtype=feat\",{\"1\":{\"122\":1}}],[\"dtype=knn\",{\"1\":{\"119\":2}}],[\"dtype=bool\",{\"1\":{\"107\":1}}],[\"dtype=torch\",{\"1\":{\"52\":1,\"67\":1,\"119\":1,\"121\":2,\"137\":5,\"187\":1,\"188\":1,\"190\":4,\"191\":1,\"192\":4,\"205\":1,\"206\":1,\"207\":2,\"208\":1,\"361\":1,\"362\":1,\"417\":1,\"419\":4,\"420\":1,\"421\":1,\"716\":1,\"892\":1}}],[\"dynamiccache\",{\"1\":{\"663\":2}}],[\"dynamic\",{\"1\":{\"94\":1,\"100\":4,\"678\":1,\"681\":2,\"683\":1}}],[\"df\",{\"1\":{\"92\":7}}],[\"done\",{\"1\":{\"918\":3}}],[\"dong\",{\"1\":{\"216\":2}}],[\"doing\",{\"1\":{\"722\":1}}],[\"doc\",{\"1\":{\"454\":4,\"681\":1}}],[\"docvqa\",{\"1\":{\"332\":1,\"335\":1,\"337\":1}}],[\"dog\",{\"1\":{\"420\":1,\"640\":2,\"691\":7}}],[\"does\",{\"1\":{\"411\":1,\"412\":1,\"424\":1}}],[\"doesn\",{\"1\":{\"380\":1}}],[\"dot语言基础语法\",{\"1\":{\"815\":1}}],[\"dot\",{\"1\":{\"407\":3,\"410\":1,\"412\":1,\"524\":1,\"694\":2,\"724\":1,\"751\":1,\"815\":29}}],[\"do\",{\"1\":{\"188\":1,\"359\":1,\"384\":3,\"385\":3,\"421\":1,\"712\":3,\"751\":1}}],[\"dosovitskiy\",{\"1\":{\"171\":1,\"176\":1}}],[\"download=true\",{\"1\":{\"918\":1,\"926\":1,\"933\":1,\"963\":1}}],[\"downloaded\",{\"1\":{\"410\":1,\"412\":1}}],[\"downloading\",{\"1\":{\"410\":2,\"412\":2}}],[\"download\",{\"1\":{\"410\":3,\"412\":4,\"435\":1}}],[\"down\",{\"1\":{\"64\":1,\"68\":1,\"116\":1,\"607\":1}}],[\"door\",{\"1\":{\"53\":1,\"82\":1,\"87\":1,\"91\":1,\"92\":1}}],[\"d\",{\"1\":{\"30\":1,\"43\":1,\"64\":1,\"94\":1,\"137\":4,\"141\":4,\"145\":17,\"153\":4,\"190\":11,\"192\":16,\"206\":8,\"207\":1,\"208\":1,\"213\":21,\"215\":1,\"256\":2,\"265\":6,\"266\":7,\"274\":6,\"303\":2,\"304\":1,\"305\":1,\"306\":1,\"390\":1,\"407\":6,\"415\":2,\"421\":4,\"428\":2,\"478\":4,\"503\":1,\"542\":1,\"699\":10,\"703\":2,\"709\":33,\"710\":1,\"724\":2,\"737\":1,\"743\":3,\"751\":15,\"816\":1,\"871\":1,\"892\":2,\"899\":11,\"900\":4,\"918\":13}}],[\"dr\",{\"1\":{\"640\":1,\"641\":1}}],[\"draw\",{\"1\":{\"107\":1,\"382\":26,\"884\":1}}],[\"driven\",{\"1\":{\"72\":1}}],[\"drive\",{\"1\":{\"28\":2,\"71\":2}}],[\"dropping\",{\"1\":{\"724\":1}}],[\"dropped\",{\"1\":{\"401\":2,\"420\":2}}],[\"droppath\",{\"1\":{\"293\":1,\"315\":1,\"318\":1,\"380\":3,\"429\":2}}],[\"drop=0\",{\"1\":{\"380\":4,\"429\":1}}],[\"drop=attn\",{\"1\":{\"380\":2}}],[\"drop=drop\",{\"1\":{\"380\":5,\"429\":1}}],[\"drop2\",{\"1\":{\"138\":2,\"141\":2}}],[\"drop1\",{\"1\":{\"138\":2,\"141\":2,\"146\":2}}],[\"dropout=none\",{\"1\":{\"751\":1}}],[\"dropout=0\",{\"1\":{\"751\":1}}],[\"dropout=self\",{\"1\":{\"663\":1,\"751\":1}}],[\"dropout防止过拟合\",{\"1\":{\"420\":1}}],[\"dropout3\",{\"1\":{\"100\":1}}],[\"dropout2\",{\"1\":{\"100\":1}}],[\"dropout1\",{\"1\":{\"100\":1}}],[\"dropout\",{\"1\":{\"97\":4,\"100\":1,\"138\":2,\"140\":1,\"141\":2,\"143\":1,\"146\":1,\"155\":3,\"236\":1,\"266\":1,\"380\":16,\"400\":4,\"401\":2,\"419\":1,\"420\":1,\"428\":1,\"429\":4,\"430\":2,\"431\":1,\"521\":3,\"633\":2,\"656\":1,\"663\":2,\"716\":4,\"718\":4,\"722\":4,\"724\":4,\"725\":4,\"736\":4,\"737\":4,\"745\":6,\"746\":2,\"749\":2,\"751\":5,\"892\":8,\"893\":2,\"895\":1,\"898\":1,\"899\":1,\"946\":1}}],[\"drop\",{\"1\":{\"25\":2,\"96\":1,\"97\":5,\"98\":1,\"99\":1,\"104\":1,\"265\":1,\"266\":1,\"293\":3,\"359\":1,\"380\":30,\"428\":5,\"429\":16,\"430\":8,\"431\":9,\"582\":2,\"648\":1,\"656\":1,\"657\":1,\"658\":1}}],[\"da\",{\"1\":{\"712\":1}}],[\"days\",{\"1\":{\"658\":3}}],[\"dathathri\",{\"1\":{\"655\":1}}],[\"data方法实现\",{\"1\":{\"698\":1}}],[\"data为列表形式的情况\",{\"1\":{\"697\":1}}],[\"data文件所提供代码对原始数据格式进行解析\",{\"1\":{\"696\":1}}],[\"datacollatorforlanguagemodeling\",{\"1\":{\"382\":1}}],[\"datacollatorforwholewordmask\",{\"1\":{\"382\":1}}],[\"datamodules\",{\"1\":{\"382\":3}}],[\"datamodule\",{\"1\":{\"382\":7}}],[\"dataaugmentationdino\",{\"1\":{\"293\":3}}],[\"dataaugmentationforbeit\",{\"1\":{\"264\":2}}],[\"dataloader\",{\"0\":{\"715\":1},\"1\":{\"104\":3,\"265\":1,\"293\":3,\"359\":1,\"381\":7,\"382\":21,\"424\":1,\"425\":2,\"518\":1,\"700\":4,\"715\":3,\"918\":5,\"926\":2,\"930\":1,\"933\":1,\"963\":1}}],[\"dataframe\",{\"1\":{\"92\":1}}],[\"data\",{\"0\":{\"44\":1,\"169\":1},\"1\":{\"11\":1,\"52\":1,\"92\":9,\"107\":7,\"137\":14,\"141\":4,\"187\":2,\"190\":3,\"192\":6,\"204\":3,\"213\":7,\"220\":1,\"223\":1,\"264\":1,\"265\":5,\"293\":11,\"339\":2,\"359\":2,\"361\":2,\"363\":3,\"381\":1,\"382\":13,\"411\":9,\"412\":9,\"413\":2,\"424\":3,\"425\":6,\"431\":4,\"491\":4,\"492\":1,\"510\":1,\"518\":1,\"544\":1,\"546\":1,\"656\":1,\"697\":9,\"698\":5,\"700\":3,\"703\":1,\"712\":1,\"756\":3,\"757\":5,\"762\":1,\"766\":1,\"771\":4,\"778\":3,\"779\":1,\"780\":1,\"791\":1,\"792\":6,\"794\":1,\"800\":1,\"801\":1,\"803\":1,\"805\":8,\"807\":2,\"808\":15,\"809\":7,\"815\":1,\"816\":8,\"832\":1,\"918\":5,\"926\":9,\"930\":1,\"933\":1,\"934\":5,\"963\":2,\"964\":1}}],[\"datasets\",{\"1\":{\"192\":2,\"293\":1,\"359\":1,\"382\":1,\"918\":2,\"926\":2,\"930\":1,\"933\":1,\"963\":1}}],[\"dataset\",{\"1\":{\"6\":2,\"7\":3,\"11\":2,\"12\":3,\"14\":2,\"40\":1,\"53\":1,\"82\":1,\"92\":2,\"104\":6,\"105\":2,\"107\":1,\"185\":1,\"187\":9,\"190\":9,\"192\":2,\"264\":1,\"265\":3,\"293\":3,\"359\":2,\"382\":51,\"424\":5,\"425\":6,\"518\":1,\"656\":1,\"696\":1,\"697\":1,\"700\":3,\"713\":1,\"715\":3,\"926\":2,\"933\":2,\"934\":1,\"938\":1,\"963\":2}}],[\"daily\",{\"1\":{\"641\":1}}],[\"dandelin\",{\"1\":{\"387\":1}}],[\"dae\",{\"1\":{\"248\":1}}],[\"dalle\",{\"1\":{\"254\":2}}],[\"dall\",{\"0\":{\"883\":1,\"890\":1},\"1\":{\"213\":1,\"216\":1,\"251\":1,\"254\":1,\"256\":1,\"264\":5,\"265\":1,\"405\":3,\"883\":2,\"890\":2,\"891\":2,\"892\":1,\"893\":4,\"894\":1,\"895\":1,\"898\":2,\"899\":3}}],[\"damage\",{\"1\":{\"14\":1}}],[\"digraph\",{\"1\":{\"815\":5}}],[\"dinan\",{\"1\":{\"658\":1}}],[\"dinoloss\",{\"1\":{\"293\":4}}],[\"dinohead\",{\"1\":{\"293\":3}}],[\"dino同样可以在小batch下训练\",{\"1\":{\"292\":1}}],[\"dino最终在两台8\",{\"1\":{\"291\":1}}],[\"dino模型时\",{\"1\":{\"291\":1}}],[\"dino通过\",{\"1\":{\"290\":1}}],[\"dino训练中可能出现两种形式的崩溃\",{\"1\":{\"290\":1}}],[\"dino采用以下机制来避免\",{\"1\":{\"285\":1}}],[\"dino不使用预测器\",{\"1\":{\"285\":1}}],[\"dino的教师网络不是固定的\",{\"1\":{\"285\":1}}],[\"dino在此基础上引入\",{\"1\":{\"285\":1}}],[\"dino教师模型\",{\"1\":{\"213\":1}}],[\"dino输出维度为768\",{\"1\":{\"213\":1}}],[\"dino专用的缩放层\",{\"1\":{\"213\":1}}],[\"dinov2\",{\"1\":{\"22\":1,\"26\":1}}],[\"dino\",{\"0\":{\"279\":1},\"1\":{\"17\":1,\"212\":1,\"213\":4,\"215\":1,\"246\":1,\"264\":1,\"279\":1,\"280\":5,\"282\":2,\"283\":4,\"285\":1,\"293\":19,\"582\":2}}],[\"diag\",{\"1\":{\"419\":6}}],[\"diagonal\",{\"1\":{\"192\":3,\"206\":1,\"207\":2,\"386\":2,\"871\":1}}],[\"dir=\",{\"1\":{\"712\":3}}],[\"dir=save\",{\"1\":{\"410\":1,\"412\":1}}],[\"directional\",{\"1\":{\"419\":2}}],[\"directory\",{\"1\":{\"410\":5,\"411\":1,\"412\":6}}],[\"dirname\",{\"1\":{\"410\":1,\"412\":1,\"696\":2}}],[\"dir\",{\"1\":{\"382\":7,\"410\":16,\"411\":7,\"412\":23,\"582\":1,\"712\":4,\"815\":4}}],[\"diffusion\",{\"1\":{\"925\":1,\"955\":1,\"961\":1}}],[\"diffs\",{\"1\":{\"213\":3}}],[\"diff\",{\"1\":{\"119\":2,\"771\":1,\"795\":2}}],[\"different\",{\"1\":{\"52\":1}}],[\"divergence\",{\"0\":{\"911\":1}}],[\"diversity\",{\"1\":{\"87\":1}}],[\"divisible\",{\"1\":{\"900\":1}}],[\"division\",{\"1\":{\"462\":1}}],[\"divide\",{\"1\":{\"500\":1}}],[\"div\",{\"1\":{\"65\":2,\"83\":2,\"255\":3,\"256\":6,\"260\":2,\"261\":3,\"809\":5,\"899\":9}}],[\"dim1\",{\"1\":{\"467\":1}}],[\"dim0\",{\"1\":{\"467\":1}}],[\"dim代表的是卷积核的数量\",{\"1\":{\"426\":1}}],[\"dims\",{\"1\":{\"97\":13,\"468\":1}}],[\"dimensionality\",{\"1\":{\"918\":1}}],[\"dimensional\",{\"1\":{\"422\":1}}],[\"dimension\",{\"1\":{\"69\":1,\"70\":1,\"157\":1,\"361\":2,\"380\":1,\"610\":2,\"918\":1}}],[\"dimensions\",{\"1\":{\"64\":1,\"893\":1,\"900\":1}}],[\"dim=400\",{\"1\":{\"931\":1,\"937\":1}}],[\"dim=784\",{\"1\":{\"931\":1,\"937\":1}}],[\"dim=768\",{\"1\":{\"205\":2,\"266\":1,\"380\":2,\"426\":1,\"427\":1,\"428\":1,\"431\":1,\"435\":1}}],[\"dim=dim\",{\"1\":{\"897\":2}}],[\"dim=dict\",{\"1\":{\"104\":2,\"107\":2}}],[\"dim=3\",{\"1\":{\"542\":1}}],[\"dim=32\",{\"1\":{\"213\":1}}],[\"dim=none\",{\"1\":{\"480\":1,\"488\":1}}],[\"dim=embed\",{\"1\":{\"213\":1,\"380\":2,\"427\":1,\"428\":1,\"431\":2}}],[\"dim=512\",{\"1\":{\"213\":1}}],[\"dim=20\",{\"1\":{\"931\":1,\"937\":1}}],[\"dim=256\",{\"1\":{\"190\":1,\"192\":1}}],[\"dim=2\",{\"1\":{\"145\":2,\"420\":2,\"542\":1,\"663\":1}}],[\"dim=0\",{\"1\":{\"102\":1,\"188\":1,\"190\":11,\"192\":11,\"205\":2,\"207\":9,\"293\":1,\"361\":1,\"386\":4,\"419\":7,\"421\":1,\"424\":1,\"466\":1,\"480\":1,\"481\":2,\"482\":4,\"487\":1,\"542\":1,\"700\":1,\"964\":1}}],[\"dim=\",{\"1\":{\"56\":2,\"65\":1,\"69\":2,\"83\":1,\"102\":1,\"119\":1,\"137\":3,\"141\":1,\"145\":2,\"153\":1,\"156\":1,\"190\":4,\"191\":2,\"192\":4,\"206\":4,\"208\":3,\"213\":2,\"256\":1,\"260\":1,\"274\":1,\"293\":4,\"380\":1,\"385\":4,\"401\":1,\"408\":4,\"417\":2,\"420\":1,\"430\":1,\"488\":1,\"582\":1,\"660\":1,\"663\":2,\"703\":1,\"709\":1,\"724\":1,\"733\":3,\"734\":1,\"743\":1,\"751\":1,\"895\":2,\"897\":1,\"898\":2,\"926\":1,\"964\":1}}],[\"dim=10\",{\"1\":{\"937\":1}}],[\"dim=1\",{\"1\":{\"56\":4,\"58\":1,\"59\":1,\"64\":1,\"65\":2,\"69\":1,\"70\":2,\"83\":6,\"102\":4,\"119\":4,\"121\":1,\"122\":2,\"141\":1,\"146\":1,\"155\":1,\"190\":11,\"192\":10,\"206\":8,\"207\":2,\"213\":3,\"256\":1,\"257\":2,\"258\":1,\"266\":1,\"362\":3,\"380\":2,\"384\":2,\"386\":2,\"419\":5,\"420\":1,\"427\":1,\"428\":1,\"431\":2,\"475\":1,\"487\":1,\"488\":1,\"542\":1,\"582\":2,\"660\":1,\"699\":2,\"700\":1,\"893\":2,\"899\":1,\"937\":2,\"963\":3}}],[\"dim\",{\"1\":{\"54\":1,\"58\":9,\"59\":16,\"60\":22,\"65\":14,\"66\":4,\"67\":2,\"68\":2,\"69\":42,\"70\":14,\"83\":40,\"94\":3,\"104\":2,\"107\":2,\"190\":6,\"191\":3,\"192\":6,\"205\":12,\"206\":2,\"213\":32,\"255\":17,\"256\":1,\"266\":7,\"274\":15,\"293\":10,\"359\":1,\"361\":3,\"380\":45,\"426\":8,\"427\":6,\"428\":6,\"429\":7,\"430\":17,\"431\":9,\"466\":1,\"472\":1,\"480\":1,\"481\":3,\"482\":1,\"487\":2,\"488\":1,\"522\":11,\"663\":11,\"892\":12,\"895\":1,\"899\":15,\"900\":21,\"918\":3,\"931\":10,\"935\":1,\"937\":16,\"963\":5}}],[\"dict=return\",{\"1\":{\"208\":1,\"420\":1}}],[\"dict=none\",{\"1\":{\"208\":1,\"420\":1}}],[\"dict=true\",{\"1\":{\"187\":1,\"190\":4,\"192\":5,\"206\":2,\"417\":2,\"419\":1,\"420\":3}}],[\"dict=false\",{\"1\":{\"64\":1}}],[\"dicts\",{\"1\":{\"104\":2,\"382\":2}}],[\"dictionary\",{\"1\":{\"54\":1,\"57\":1,\"361\":1,\"597\":16}}],[\"dict\",{\"1\":{\"53\":7,\"82\":6,\"104\":2,\"105\":1,\"106\":2,\"107\":5,\"191\":2,\"207\":2,\"208\":6,\"293\":4,\"359\":1,\"382\":1,\"383\":1,\"420\":1,\"424\":2,\"435\":3,\"445\":2,\"474\":3,\"514\":2,\"595\":13,\"597\":23,\"697\":8,\"700\":2,\"963\":2,\"964\":2}}],[\"dicebceloss\",{\"1\":{\"587\":2}}],[\"diceloss\",{\"1\":{\"102\":4,\"586\":2}}],[\"dice\",{\"0\":{\"586\":1,\"587\":1},\"1\":{\"6\":1,\"7\":1,\"11\":1,\"12\":1,\"14\":1,\"24\":1,\"39\":1,\"64\":1,\"79\":1,\"102\":14,\"586\":17,\"587\":21,\"588\":2,\"590\":2,\"592\":14}}],[\"disables\",{\"1\":{\"293\":1}}],[\"discovery\",{\"1\":{\"894\":1}}],[\"discretized\",{\"1\":{\"921\":1}}],[\"discretevae\",{\"0\":{\"255\":1,\"256\":1,\"899\":1},\"1\":{\"254\":1,\"255\":3,\"256\":1,\"899\":5}}],[\"discrete\",{\"0\":{\"249\":1},\"1\":{\"235\":1,\"264\":2,\"265\":2}}],[\"discriminator\",{\"1\":{\"918\":14}}],[\"discrimination\",{\"0\":{\"350\":1},\"1\":{\"350\":1}}],[\"discussions\",{\"0\":{\"175\":1},\"1\":{\"174\":1}}],[\"dist=false\",{\"1\":{\"382\":1}}],[\"distributedsampler\",{\"1\":{\"382\":3,\"715\":1}}],[\"distributed\",{\"1\":{\"274\":1,\"382\":1}}],[\"distributions\",{\"0\":{\"856\":1,\"857\":1,\"869\":1}}],[\"distribution\",{\"0\":{\"858\":1,\"859\":1,\"867\":1,\"868\":1},\"1\":{\"257\":1,\"424\":1,\"854\":1,\"856\":2,\"857\":2,\"860\":1,\"863\":1,\"865\":2,\"866\":1,\"867\":1,\"868\":3,\"871\":1,\"877\":3}}],[\"distill\",{\"1\":{\"208\":4,\"823\":1}}],[\"distillation\",{\"0\":{\"168\":1,\"202\":1},\"1\":{\"14\":1,\"193\":2,\"194\":1,\"195\":2,\"213\":1,\"280\":1,\"282\":1,\"283\":1,\"285\":2}}],[\"dists\",{\"1\":{\"145\":6,\"213\":3}}],[\"distance=128\",{\"1\":{\"710\":1}}],[\"distance\",{\"0\":{\"576\":1,\"577\":1},\"1\":{\"137\":7,\"145\":1,\"710\":6,\"871\":1}}],[\"distances\",{\"1\":{\"119\":3,\"488\":1}}],[\"dist\",{\"1\":{\"119\":6,\"121\":7,\"122\":6,\"137\":3,\"145\":3,\"382\":4,\"386\":5,\"963\":2}}],[\"dist2\",{\"1\":{\"119\":4}}],[\"dishwasher\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"displayed\",{\"1\":{\"815\":1}}],[\"display\",{\"1\":{\"53\":2,\"82\":2,\"92\":2,\"815\":2}}],[\"dezero\",{\"1\":{\"815\":1}}],[\"dependency\",{\"1\":{\"696\":2}}],[\"depth\",{\"1\":{\"224\":1,\"236\":1,\"380\":6,\"431\":1,\"500\":1,\"892\":3,\"900\":6,\"926\":1}}],[\"depth=12\",{\"1\":{\"205\":2,\"380\":1,\"428\":1,\"431\":1,\"435\":1}}],[\"del\",{\"1\":{\"595\":1,\"597\":1,\"751\":3}}],[\"delta\",{\"1\":{\"263\":7}}],[\"deactivate\",{\"1\":{\"552\":1}}],[\"dead\",{\"1\":{\"213\":1}}],[\"detokenize\",{\"1\":{\"697\":2}}],[\"detokenizers\",{\"1\":{\"641\":1}}],[\"detection\",{\"1\":{\"589\":1}}],[\"deterministic\",{\"1\":{\"521\":2}}],[\"deterministic=true\",{\"1\":{\"521\":1}}],[\"details\",{\"0\":{\"702\":1},\"1\":{\"655\":1}}],[\"detail\",{\"0\":{\"356\":1}}],[\"detailed\",{\"1\":{\"36\":1,\"52\":2,\"342\":1}}],[\"detach\",{\"1\":{\"107\":1,\"190\":3,\"192\":2,\"206\":2,\"213\":9,\"258\":6,\"293\":2,\"362\":1,\"893\":1,\"899\":3,\"918\":1,\"959\":2,\"963\":3}}],[\"density\",{\"1\":{\"847\":1}}],[\"densenet\",{\"1\":{\"510\":1}}],[\"dense\",{\"1\":{\"7\":1,\"397\":2,\"400\":2,\"403\":2,\"589\":1,\"718\":4,\"720\":2,\"725\":2,\"728\":2,\"745\":1,\"823\":2}}],[\"dennison\",{\"1\":{\"655\":1}}],[\"den\",{\"1\":{\"216\":1}}],[\"deit\",{\"1\":{\"205\":1,\"510\":1}}],[\"dequeue\",{\"0\":{\"364\":1},\"1\":{\"190\":1,\"192\":1,\"206\":1,\"362\":2,\"364\":2}}],[\"devlin\",{\"1\":{\"171\":1,\"176\":1,\"216\":1,\"671\":1}}],[\"devices=0\",{\"1\":{\"520\":1}}],[\"devices\",{\"0\":{\"520\":1},\"1\":{\"520\":2}}],[\"device=x\",{\"1\":{\"709\":4}}],[\"device=xyz\",{\"1\":{\"121\":3}}],[\"device=input\",{\"1\":{\"716\":1}}],[\"device=inputs\",{\"1\":{\"663\":1}}],[\"device=image\",{\"1\":{\"418\":1}}],[\"device=img\",{\"1\":{\"385\":1}}],[\"device=none\",{\"1\":{\"486\":1,\"926\":1}}],[\"device=sim\",{\"1\":{\"419\":1}}],[\"device=logits\",{\"1\":{\"385\":1}}],[\"device=device\",{\"1\":{\"256\":1,\"260\":1,\"265\":1,\"274\":1,\"893\":3,\"898\":1,\"926\":1}}],[\"device=\",{\"1\":{\"213\":1,\"486\":1}}],[\"device=multi\",{\"1\":{\"67\":1}}],[\"device\",{\"1\":{\"52\":1,\"64\":3,\"67\":1,\"83\":5,\"94\":1,\"106\":1,\"121\":4,\"137\":15,\"187\":4,\"188\":2,\"190\":5,\"191\":2,\"192\":6,\"204\":3,\"206\":2,\"207\":1,\"208\":3,\"213\":2,\"256\":2,\"265\":3,\"274\":4,\"293\":2,\"385\":2,\"386\":1,\"410\":5,\"412\":5,\"417\":2,\"418\":1,\"419\":4,\"420\":1,\"421\":2,\"431\":4,\"435\":1,\"486\":1,\"520\":1,\"582\":1,\"663\":2,\"700\":2,\"709\":4,\"716\":1,\"893\":6,\"898\":2,\"899\":4,\"900\":8,\"926\":14,\"934\":4,\"935\":2,\"938\":5,\"939\":2,\"963\":2,\"964\":4}}],[\"deeplearning\",{\"1\":{\"359\":2}}],[\"deepseekr1\",{\"1\":{\"823\":1}}],[\"deepseekmoe\",{\"1\":{\"823\":1}}],[\"deepseek\",{\"1\":{\"326\":1,\"822\":1,\"823\":17}}],[\"deep\",{\"1\":{\"160\":1}}],[\"deepgcns\",{\"1\":{\"110\":1}}],[\"descent\",{\"1\":{\"816\":2}}],[\"desc=\",{\"1\":{\"595\":4,\"597\":2}}],[\"desc\",{\"1\":{\"408\":2}}],[\"describe\",{\"1\":{\"52\":4}}],[\"description\",{\"1\":{\"36\":1,\"52\":2,\"64\":3,\"342\":1,\"948\":1,\"950\":1}}],[\"descriptions\",{\"1\":{\"11\":1}}],[\"destroy\",{\"1\":{\"107\":1}}],[\"debug\",{\"1\":{\"106\":1,\"453\":2}}],[\"decorate\",{\"1\":{\"456\":2}}],[\"decorator\",{\"1\":{\"449\":3,\"451\":4,\"452\":2,\"453\":4,\"454\":5,\"457\":3,\"460\":2,\"895\":1,\"898\":1,\"899\":1}}],[\"deconvolution\",{\"1\":{\"239\":1}}],[\"decode\",{\"1\":{\"171\":1,\"188\":1,\"213\":6,\"421\":1,\"660\":2,\"663\":2,\"697\":1,\"735\":1,\"742\":2,\"895\":1,\"898\":1,\"899\":2,\"931\":2,\"935\":3,\"937\":2,\"939\":1}}],[\"decoders\",{\"1\":{\"921\":1}}],[\"decoder模型结构图\",{\"1\":{\"749\":1,\"750\":1}}],[\"decoderlayer\",{\"0\":{\"749\":1},\"1\":{\"749\":2}}],[\"decoder=true时\",{\"1\":{\"420\":1}}],[\"decoder=true\",{\"1\":{\"403\":1,\"420\":1}}],[\"decoder=is\",{\"1\":{\"208\":1,\"403\":1,\"420\":1}}],[\"decoder=false\",{\"1\":{\"208\":1,\"397\":1}}],[\"decoder\",{\"0\":{\"39\":1,\"171\":1,\"742\":1,\"748\":1,\"750\":1},\"1\":{\"54\":1,\"59\":1,\"64\":1,\"69\":1,\"70\":1,\"83\":8,\"94\":2,\"100\":3,\"122\":1,\"144\":1,\"165\":1,\"171\":2,\"172\":1,\"183\":1,\"187\":8,\"188\":5,\"192\":10,\"208\":2,\"212\":3,\"213\":15,\"214\":3,\"215\":2,\"220\":1,\"235\":1,\"254\":1,\"255\":2,\"256\":1,\"268\":5,\"271\":1,\"306\":1,\"403\":3,\"415\":1,\"420\":9,\"421\":2,\"629\":1,\"633\":1,\"729\":2,\"735\":1,\"741\":2,\"742\":4,\"749\":1,\"750\":3,\"823\":5,\"887\":1,\"899\":5,\"932\":1,\"945\":1,\"949\":2,\"959\":1,\"963\":7,\"964\":2}}],[\"decoding过程\",{\"1\":{\"94\":2,\"100\":1}}],[\"decoding\",{\"1\":{\"94\":1}}],[\"decx\",{\"1\":{\"123\":2}}],[\"dec1\",{\"1\":{\"123\":4}}],[\"dec2\",{\"1\":{\"123\":4}}],[\"dec3\",{\"1\":{\"123\":4}}],[\"dec4\",{\"1\":{\"123\":4}}],[\"dec\",{\"1\":{\"123\":6,\"255\":8,\"899\":22}}],[\"dec5\",{\"1\":{\"123\":5}}],[\"decay=args\",{\"1\":{\"359\":1}}],[\"decay=decay\",{\"1\":{\"213\":1}}],[\"decay=0\",{\"1\":{\"213\":3,\"700\":1}}],[\"decay=config\",{\"1\":{\"187\":1,\"190\":1,\"192\":1}}],[\"decay=opt\",{\"1\":{\"104\":1}}],[\"decay\",{\"1\":{\"104\":1,\"187\":1,\"190\":1,\"192\":1,\"213\":13,\"224\":1,\"293\":4,\"359\":1,\"918\":2}}],[\"definition\",{\"0\":{\"871\":1}}],[\"defined\",{\"1\":{\"749\":1}}],[\"define\",{\"1\":{\"359\":1,\"811\":2}}],[\"defaultdict\",{\"1\":{\"595\":5,\"597\":5}}],[\"default=400\",{\"1\":{\"918\":1}}],[\"default=1\",{\"1\":{\"918\":1}}],[\"default=100\",{\"1\":{\"918\":1}}],[\"default=28\",{\"1\":{\"918\":1}}],[\"default=200\",{\"1\":{\"918\":1}}],[\"default=0\",{\"1\":{\"918\":3}}],[\"default=64\",{\"1\":{\"918\":1}}],[\"default=8\",{\"1\":{\"293\":1,\"918\":1}}],[\"default=\",{\"1\":{\"293\":2}}],[\"default\",{\"1\":{\"107\":1,\"213\":2,\"361\":4,\"382\":2,\"895\":1,\"899\":1}}],[\"def\",{\"1\":{\"53\":2,\"54\":1,\"56\":1,\"57\":1,\"58\":4,\"59\":4,\"60\":4,\"64\":1,\"65\":2,\"67\":1,\"69\":4,\"70\":2,\"82\":2,\"83\":14,\"92\":3,\"94\":1,\"96\":2,\"97\":2,\"98\":2,\"99\":1,\"100\":2,\"102\":2,\"104\":1,\"107\":4,\"119\":4,\"120\":2,\"121\":3,\"122\":3,\"123\":4,\"137\":7,\"138\":2,\"141\":4,\"143\":1,\"145\":2,\"146\":2,\"152\":2,\"153\":1,\"154\":2,\"155\":2,\"156\":2,\"187\":5,\"188\":1,\"190\":4,\"191\":2,\"192\":4,\"204\":1,\"205\":1,\"206\":1,\"207\":1,\"208\":3,\"213\":12,\"255\":3,\"256\":1,\"263\":3,\"264\":3,\"265\":2,\"266\":4,\"274\":3,\"293\":8,\"359\":4,\"361\":1,\"362\":1,\"363\":1,\"364\":1,\"380\":8,\"382\":25,\"383\":3,\"384\":2,\"385\":3,\"386\":1,\"397\":3,\"398\":2,\"399\":1,\"400\":3,\"401\":1,\"403\":6,\"410\":8,\"411\":3,\"412\":11,\"417\":1,\"419\":1,\"420\":4,\"421\":1,\"424\":5,\"426\":2,\"427\":3,\"428\":3,\"429\":4,\"430\":2,\"431\":4,\"435\":1,\"444\":2,\"447\":2,\"448\":2,\"449\":3,\"451\":3,\"452\":3,\"453\":4,\"454\":6,\"455\":3,\"456\":1,\"457\":1,\"458\":1,\"459\":3,\"461\":6,\"511\":1,\"513\":1,\"582\":1,\"586\":2,\"587\":2,\"588\":2,\"589\":2,\"590\":2,\"592\":2,\"595\":9,\"596\":6,\"597\":23,\"660\":1,\"663\":6,\"696\":2,\"697\":6,\"698\":3,\"699\":2,\"703\":2,\"709\":2,\"710\":3,\"713\":3,\"715\":1,\"716\":2,\"718\":6,\"719\":2,\"720\":2,\"721\":2,\"722\":2,\"724\":3,\"725\":2,\"726\":2,\"728\":2,\"729\":2,\"730\":2,\"731\":2,\"734\":2,\"736\":2,\"737\":2,\"742\":4,\"743\":2,\"745\":2,\"746\":2,\"747\":2,\"749\":2,\"750\":2,\"751\":3,\"756\":1,\"762\":3,\"763\":1,\"766\":2,\"771\":1,\"778\":1,\"779\":2,\"780\":1,\"783\":2,\"784\":1,\"787\":1,\"790\":2,\"791\":1,\"792\":1,\"794\":2,\"795\":2,\"800\":3,\"801\":1,\"802\":1,\"803\":1,\"805\":5,\"806\":2,\"807\":5,\"808\":7,\"809\":24,\"810\":1,\"811\":3,\"815\":6,\"816\":2,\"892\":1,\"893\":2,\"894\":1,\"895\":1,\"896\":1,\"897\":1,\"898\":1,\"899\":4,\"900\":2,\"918\":5,\"926\":4,\"931\":5,\"932\":1,\"937\":5,\"963\":8,\"964\":5}}],[\"mdl\",{\"1\":{\"948\":1,\"950\":1}}],[\"mdm\",{\"1\":{\"223\":1}}],[\"mps\",{\"1\":{\"926\":2}}],[\"mvn\",{\"1\":{\"869\":1,\"870\":1}}],[\"mvp\",{\"1\":{\"836\":1}}],[\"mvcnn\",{\"1\":{\"157\":3}}],[\"mcmc\",{\"1\":{\"942\":1,\"944\":1}}],[\"mcp\",{\"1\":{\"823\":1}}],[\"mccann\",{\"1\":{\"640\":1}}],[\"mnist\",{\"1\":{\"819\":1,\"918\":3,\"925\":1,\"926\":1,\"928\":1,\"933\":1,\"936\":1,\"963\":1}}],[\"mnli\",{\"1\":{\"680\":1,\"683\":1,\"685\":2}}],[\"mkdir\",{\"1\":{\"815\":1}}],[\"mkl\",{\"1\":{\"557\":1}}],[\"mrpc\",{\"1\":{\"634\":1}}],[\"mrg策略在处理每个局部区域时\",{\"1\":{\"142\":1}}],[\"mrg通过结合来自不同分辨率的特征来实现效率和适应性的平衡\",{\"1\":{\"142\":1}}],[\"mrg为一种低成本的替代方案\",{\"1\":{\"142\":1}}],[\"mrg\",{\"1\":{\"139\":1}}],[\"mmlu\",{\"1\":{\"668\":2,\"669\":1}}],[\"mmllm\",{\"1\":{\"7\":1}}],[\"mm1\",{\"1\":{\"325\":1}}],[\"mme\",{\"1\":{\"305\":1,\"310\":2,\"311\":2}}],[\"m为下采样后的点数\",{\"1\":{\"121\":1}}],[\"myenv\",{\"1\":{\"550\":3,\"551\":1,\"553\":2,\"554\":1}}],[\"mymodel\",{\"1\":{\"511\":1}}],[\"mydecorator\",{\"1\":{\"459\":2}}],[\"mydataset\",{\"1\":{\"424\":1,\"425\":2}}],[\"myservice\",{\"1\":{\"456\":2}}],[\"myclass\",{\"1\":{\"455\":1}}],[\"my\",{\"1\":{\"107\":2,\"449\":3,\"451\":4,\"452\":2,\"454\":5,\"511\":1,\"691\":4}}],[\"m\",{\"1\":{\"98\":1,\"107\":2,\"119\":26,\"121\":11,\"122\":2,\"190\":34,\"192\":29,\"205\":8,\"206\":26,\"208\":4,\"293\":3,\"353\":1,\"359\":1,\"361\":4,\"363\":2,\"432\":1,\"488\":1,\"538\":5,\"545\":4,\"683\":1,\"685\":1,\"709\":3,\"749\":3,\"915\":1}}],[\"mtdatamodule\",{\"1\":{\"382\":2}}],[\"mt\",{\"1\":{\"65\":2,\"69\":6,\"70\":3,\"83\":14}}],[\"mle\",{\"0\":{\"903\":1},\"1\":{\"903\":2,\"904\":3}}],[\"mla\",{\"1\":{\"823\":1}}],[\"mlfoundations\",{\"1\":{\"385\":1}}],[\"mlm任务掩码策略的方法\",{\"1\":{\"697\":1}}],[\"mlm的目标是通过文本的上下文信息去预测masked的文本tokens\",{\"1\":{\"393\":1}}],[\"mlm=true\",{\"1\":{\"382\":1}}],[\"mlm\",{\"0\":{\"200\":1,\"208\":1},\"1\":{\"172\":1,\"198\":1,\"200\":1,\"202\":3,\"204\":2,\"205\":3,\"208\":12,\"220\":1,\"234\":1,\"354\":1,\"376\":1,\"382\":12,\"383\":4,\"384\":40,\"385\":9,\"393\":1,\"679\":1,\"681\":1,\"698\":1,\"699\":6,\"700\":2,\"701\":1}}],[\"mlp投影层\",{\"1\":{\"329\":1}}],[\"mlp比率12800的稳定配置\",{\"1\":{\"304\":1}}],[\"mlp+pooling\",{\"1\":{\"117\":1}}],[\"mlp映射\",{\"1\":{\"98\":1}}],[\"mlpmixerlayer\",{\"1\":{\"97\":1}}],[\"mlp₂\",{\"1\":{\"97\":1}}],[\"mlp₁\",{\"1\":{\"97\":1}}],[\"mlp\",{\"0\":{\"97\":1,\"431\":1},\"1\":{\"64\":1,\"65\":3,\"70\":2,\"97\":10,\"99\":1,\"100\":1,\"110\":2,\"112\":2,\"113\":1,\"114\":1,\"116\":2,\"117\":4,\"119\":4,\"125\":1,\"137\":8,\"141\":3,\"143\":5,\"145\":9,\"146\":1,\"150\":1,\"155\":2,\"205\":2,\"285\":1,\"305\":2,\"311\":3,\"317\":2,\"329\":1,\"359\":1,\"361\":1,\"380\":30,\"400\":2,\"428\":1,\"429\":14,\"431\":4,\"432\":1,\"663\":2}}],[\"mlp=\",{\"1\":{\"59\":3,\"70\":3,\"83\":3,\"138\":3,\"146\":8}}],[\"mllm\",{\"0\":{\"34\":1},\"1\":{\"48\":1,\"52\":1,\"322\":1,\"330\":1}}],[\"mllms\",{\"1\":{\"29\":1,\"31\":2,\"323\":1,\"327\":1}}],[\"mu\",{\"1\":{\"931\":9,\"932\":2,\"934\":2,\"937\":7,\"938\":2}}],[\"must\",{\"1\":{\"274\":1,\"383\":1,\"386\":1,\"697\":1,\"893\":1,\"895\":1,\"899\":3,\"900\":1}}],[\"mul等运算符函数\",{\"1\":{\"810\":1}}],[\"mul等\",{\"1\":{\"810\":1}}],[\"mul\",{\"1\":{\"102\":4,\"213\":1,\"293\":1,\"809\":12,\"810\":3,\"811\":3,\"815\":1}}],[\"multivariate\",{\"0\":{\"870\":1},\"1\":{\"869\":2}}],[\"multirc等任务上显著低于fine\",{\"1\":{\"648\":1}}],[\"multitask\",{\"1\":{\"637\":1,\"638\":1,\"639\":1}}],[\"multinli\",{\"1\":{\"625\":1,\"626\":1}}],[\"multinomial\",{\"0\":{\"857\":1},\"1\":{\"190\":2,\"192\":2,\"207\":2,\"386\":2,\"419\":2,\"857\":2,\"897\":1,\"926\":1,\"964\":1}}],[\"multiplication\",{\"1\":{\"849\":1}}],[\"multiply\",{\"1\":{\"430\":2,\"453\":2}}],[\"multiple\",{\"1\":{\"14\":1,\"49\":3,\"143\":1,\"737\":1}}],[\"multicropwrapper\",{\"1\":{\"293\":5}}],[\"multiwaytransformer\",{\"1\":{\"380\":7}}],[\"multiway\",{\"0\":{\"222\":1},\"1\":{\"220\":2,\"221\":1,\"222\":2,\"224\":1,\"225\":1}}],[\"multiheadedattention\",{\"1\":{\"751\":2}}],[\"multihead\",{\"1\":{\"100\":1}}],[\"multi\",{\"0\":{\"33\":1,\"52\":1,\"140\":1,\"142\":1,\"693\":1},\"1\":{\"7\":1,\"11\":1,\"14\":1,\"50\":1,\"64\":2,\"65\":2,\"67\":5,\"70\":5,\"86\":1,\"139\":2,\"141\":1,\"148\":1,\"159\":1,\"190\":1,\"208\":1,\"280\":1,\"285\":1,\"286\":1,\"291\":3,\"293\":5,\"305\":1,\"366\":2,\"429\":1,\"534\":1,\"640\":1,\"741\":1,\"823\":1}}],[\"multimodal\",{\"0\":{\"171\":1,\"251\":1,\"380\":1},\"1\":{\"7\":1,\"128\":2,\"165\":1,\"183\":1,\"220\":1,\"268\":1,\"274\":1,\"321\":2,\"342\":2,\"397\":1,\"398\":1,\"399\":1,\"403\":1,\"420\":2}}],[\"mug\",{\"1\":{\"53\":1,\"82\":1,\"87\":1,\"91\":1,\"92\":1,\"94\":1}}],[\"might\",{\"1\":{\"724\":1}}],[\"mit\",{\"1\":{\"667\":1}}],[\"misinformation\",{\"0\":{\"670\":1}}],[\"mishra\",{\"1\":{\"655\":1}}],[\"misalignment\",{\"1\":{\"502\":1}}],[\"missing\",{\"1\":{\"149\":1}}],[\"million\",{\"1\":{\"432\":1}}],[\"mi\",{\"1\":{\"390\":1}}],[\"mim\",{\"1\":{\"210\":4,\"214\":9,\"215\":2,\"216\":6,\"220\":1,\"227\":1,\"228\":2,\"229\":2,\"234\":2,\"235\":1,\"246\":1}}],[\"mid\",{\"1\":{\"119\":13}}],[\"middle\",{\"1\":{\"52\":1}}],[\"mincount\",{\"1\":{\"595\":4,\"597\":5}}],[\"min=a\",{\"1\":{\"592\":1}}],[\"min=1e\",{\"1\":{\"104\":1}}],[\"minlength=5\",{\"1\":{\"485\":1}}],[\"minlength=0\",{\"1\":{\"485\":1}}],[\"minlength\",{\"1\":{\"213\":1,\"485\":2}}],[\"min\",{\"1\":{\"119\":1,\"121\":1,\"160\":1,\"188\":3,\"190\":1,\"192\":2,\"204\":1,\"213\":2,\"263\":11,\"264\":2,\"293\":2,\"410\":1,\"412\":1,\"421\":2,\"698\":5,\"710\":1,\"734\":4}}],[\"minist\",{\"0\":{\"926\":1},\"1\":{\"926\":1}}],[\"minibatch\",{\"1\":{\"356\":1,\"407\":2}}],[\"minigpt\",{\"1\":{\"300\":1,\"326\":1,\"403\":1}}],[\"mini\",{\"1\":{\"137\":1,\"138\":3,\"201\":1,\"823\":8}}],[\"minimum\",{\"1\":{\"121\":1,\"597\":2,\"816\":1,\"948\":1,\"950\":1}}],[\"minimalistic\",{\"1\":{\"6\":1}}],[\"mining\",{\"1\":{\"83\":1,\"376\":2}}],[\"miou\",{\"1\":{\"99\":1,\"106\":11,\"117\":2,\"308\":1}}],[\"mixup\",{\"1\":{\"510\":1}}],[\"mixed\",{\"1\":{\"207\":2,\"401\":2,\"420\":2,\"724\":6}}],[\"mixer\",{\"0\":{\"97\":1},\"1\":{\"97\":11,\"99\":2,\"429\":1}}],[\"mixture\",{\"0\":{\"171\":1,\"380\":1},\"1\":{\"165\":1,\"183\":1,\"367\":2,\"368\":1,\"372\":2,\"378\":1,\"921\":1}}],[\"mix\",{\"1\":{\"97\":6,\"317\":1}}],[\"mixing阶段\",{\"1\":{\"99\":1}}],[\"mixing\",{\"0\":{\"97\":1},\"1\":{\"95\":1,\"97\":4,\"99\":3}}],[\"microsoft\",{\"1\":{\"209\":1,\"219\":1,\"226\":1,\"367\":1,\"378\":1,\"634\":1}}],[\"microwave\",{\"1\":{\"53\":1,\"82\":1,\"91\":1,\"92\":1}}],[\"microphone\",{\"1\":{\"53\":1}}],[\"measure\",{\"1\":{\"918\":1}}],[\"measures\",{\"1\":{\"918\":1}}],[\"mean=\",{\"1\":{\"359\":1}}],[\"mean=imagenet\",{\"1\":{\"264\":1}}],[\"mean=torch\",{\"1\":{\"264\":1}}],[\"means初始化量化器\",{\"1\":{\"213\":1}}],[\"means++\",{\"1\":{\"213\":1}}],[\"means\",{\"1\":{\"213\":22}}],[\"mean\",{\"1\":{\"46\":1,\"102\":2,\"106\":11,\"107\":1,\"153\":1,\"190\":2,\"192\":2,\"206\":2,\"208\":2,\"213\":4,\"264\":3,\"282\":1,\"285\":1,\"293\":3,\"385\":2,\"403\":1,\"419\":1,\"420\":1,\"522\":6,\"587\":1,\"589\":2,\"592\":1,\"700\":1,\"865\":1,\"877\":1,\"899\":1,\"900\":3}}],[\"merged\",{\"1\":{\"595\":6,\"596\":6,\"597\":13}}],[\"merge\",{\"1\":{\"595\":4,\"596\":5,\"597\":12}}],[\"merges\",{\"1\":{\"595\":4,\"597\":3}}],[\"merging\",{\"1\":{\"595\":3,\"597\":1}}],[\"message\",{\"1\":{\"342\":2}}],[\"meshgrid\",{\"1\":{\"816\":1}}],[\"mesh\",{\"1\":{\"159\":1}}],[\"med\",{\"0\":{\"171\":1},\"1\":{\"165\":2,\"167\":1,\"170\":1,\"171\":1,\"174\":1,\"183\":1,\"187\":6,\"190\":2,\"191\":2,\"192\":1,\"395\":1}}],[\"members\",{\"1\":{\"597\":1}}],[\"memory=true\",{\"1\":{\"293\":1,\"359\":1,\"382\":3}}],[\"memory\",{\"1\":{\"100\":5,\"282\":1,\"357\":4,\"545\":1,\"635\":1,\"710\":6,\"742\":2,\"749\":2,\"750\":2,\"832\":1}}],[\"mem\",{\"1\":{\"52\":1}}],[\"meta\",{\"1\":{\"646\":1,\"650\":1,\"823\":1}}],[\"metalm\",{\"1\":{\"225\":1}}],[\"metamind\",{\"1\":{\"0\":1}}],[\"method\",{\"0\":{\"170\":1,\"370\":1,\"390\":1},\"1\":{\"455\":3,\"460\":4}}],[\"methods\",{\"1\":{\"11\":1,\"409\":1,\"607\":1,\"655\":1}}],[\"metrics\",{\"1\":{\"106\":7}}],[\"metric\",{\"1\":{\"106\":1,\"204\":1,\"265\":1}}],[\"mhacot是一种类人推理方式\",{\"1\":{\"52\":1}}],[\"mhacot推理链\",{\"1\":{\"30\":1}}],[\"mhacot\",{\"1\":{\"29\":1,\"32\":1,\"52\":1}}],[\"ms\",{\"1\":{\"884\":2,\"888\":4}}],[\"msqrt\",{\"1\":{\"703\":1}}],[\"msa\",{\"1\":{\"372\":1}}],[\"mscoco\",{\"1\":{\"268\":1}}],[\"msg方法虽然有效\",{\"1\":{\"142\":1}}],[\"msg相当于并联了多个hierarchical\",{\"1\":{\"141\":1}}],[\"msg的关键优点在于它通过在训练期间的随机输入丢弃\",{\"1\":{\"141\":1}}],[\"msg通过应用不同尺度的分组层\",{\"1\":{\"140\":1}}],[\"msg\",{\"1\":{\"139\":1,\"141\":3}}],[\"msmvpam\",{\"1\":{\"28\":1}}],[\"mseloss\",{\"1\":{\"259\":1,\"722\":1}}],[\"mse\",{\"1\":{\"14\":1,\"213\":3,\"255\":3,\"259\":4,\"899\":4,\"963\":4}}],[\"marginal\",{\"1\":{\"852\":1,\"877\":1}}],[\"markdown\",{\"1\":{\"836\":1}}],[\"markersize=12\",{\"1\":{\"816\":1}}],[\"markersize=2\",{\"1\":{\"816\":1}}],[\"marker=\",{\"1\":{\"816\":1}}],[\"made\",{\"1\":{\"749\":1}}],[\"mail\",{\"1\":{\"641\":1}}],[\"main\",{\"1\":{\"52\":2,\"104\":1,\"107\":1,\"187\":1,\"190\":1,\"192\":1,\"265\":1,\"359\":1,\"414\":1,\"660\":3,\"663\":4,\"696\":3}}],[\"mahalanobis\",{\"0\":{\"577\":1},\"1\":{\"871\":1}}],[\"major\",{\"1\":{\"489\":1,\"541\":2,\"542\":3}}],[\"making\",{\"1\":{\"362\":1}}],[\"makedirs\",{\"1\":{\"696\":2,\"918\":2}}],[\"make\",{\"1\":{\"123\":12,\"382\":4,\"430\":1,\"698\":1,\"700\":1}}],[\"mass\",{\"1\":{\"846\":1}}],[\"master\",{\"1\":{\"209\":1,\"219\":1,\"226\":1,\"367\":1,\"378\":1}}],[\"mask作用图解\",{\"1\":{\"713\":2}}],[\"mask模样为\",{\"1\":{\"703\":1}}],[\"mask矩阵将注意力得分矩阵中对应位置的得分设置为一个非常小的值\",{\"1\":{\"703\":1}}],[\"mask矩阵\",{\"1\":{\"703\":1}}],[\"mask部分相关的掩码逻辑\",{\"1\":{\"430\":1}}],[\"mask和casual\",{\"1\":{\"430\":1}}],[\"mask方法中\",{\"1\":{\"420\":1}}],[\"mask标注哪些image\",{\"1\":{\"417\":1}}],[\"mask标签\",{\"1\":{\"382\":1}}],[\"masks\",{\"1\":{\"384\":14,\"385\":21,\"386\":19}}],[\"masks=none\",{\"1\":{\"384\":1,\"385\":1}}],[\"masking是将连续的子词tokens进行mask的技巧\",{\"1\":{\"393\":1}}],[\"masking技巧\",{\"1\":{\"393\":1}}],[\"maskinggenerator\",{\"1\":{\"263\":1,\"264\":2}}],[\"masking\",{\"0\":{\"263\":1},\"1\":{\"234\":1,\"242\":3,\"252\":1,\"263\":9,\"264\":1,\"382\":1,\"393\":1,\"472\":1,\"678\":1,\"681\":4,\"683\":1,\"697\":1,\"698\":1,\"741\":1,\"750\":1}}],[\"maskedconv2d\",{\"1\":{\"964\":3}}],[\"maskedcnn\",{\"1\":{\"926\":12}}],[\"maskedlmoutput\",{\"1\":{\"208\":2}}],[\"masked\",{\"0\":{\"200\":1,\"234\":1,\"384\":1,\"691\":1},\"1\":{\"187\":1,\"190\":2,\"192\":1,\"200\":1,\"208\":15,\"209\":2,\"212\":1,\"213\":2,\"220\":4,\"223\":1,\"229\":1,\"234\":2,\"235\":3,\"252\":1,\"263\":2,\"264\":2,\"265\":8,\"266\":10,\"354\":1,\"368\":1,\"370\":1,\"373\":1,\"380\":1,\"382\":1,\"383\":2,\"385\":1,\"393\":1,\"420\":1,\"681\":1,\"690\":1,\"691\":2,\"697\":8,\"698\":6,\"699\":12,\"700\":4,\"703\":1,\"731\":5,\"742\":1,\"751\":1,\"893\":1,\"898\":1,\"900\":2}}],[\"mask=extended\",{\"1\":{\"397\":1}}],[\"mask=encoder\",{\"1\":{\"208\":1,\"397\":1,\"403\":1,\"420\":1}}],[\"mask=co\",{\"1\":{\"384\":1,\"385\":4}}],[\"mask=mask\",{\"1\":{\"380\":1,\"751\":1}}],[\"mask=memory\",{\"1\":{\"100\":2}}],[\"mask=attn\",{\"1\":{\"274\":1}}],[\"mask=attention\",{\"1\":{\"208\":1,\"403\":1,\"419\":1,\"420\":2,\"663\":1,\"713\":1,\"722\":1,\"731\":1,\"733\":1,\"734\":1,\"736\":1,\"737\":1}}],[\"mask=head\",{\"1\":{\"208\":1,\"420\":1,\"722\":1,\"731\":1,\"736\":1,\"737\":1}}],[\"mask=image\",{\"1\":{\"187\":1,\"190\":2,\"192\":3,\"417\":1,\"419\":1,\"420\":1}}],[\"mask=q\",{\"1\":{\"99\":1}}],[\"mask=none\",{\"1\":{\"96\":1,\"98\":1,\"99\":1,\"107\":1,\"207\":3,\"208\":3,\"380\":2,\"397\":2,\"398\":2,\"399\":2,\"400\":2,\"401\":2,\"403\":2,\"420\":12,\"663\":2,\"718\":1,\"719\":2,\"721\":2,\"722\":2,\"724\":2,\"726\":1,\"731\":2,\"734\":2,\"736\":2,\"737\":2,\"751\":2}}],[\"mask=true\",{\"1\":{\"213\":1,\"893\":1}}],[\"mask=text\",{\"1\":{\"187\":1,\"190\":4,\"192\":5,\"206\":2,\"417\":1}}],[\"mask=tgt\",{\"1\":{\"100\":2}}],[\"mask=t\",{\"1\":{\"94\":1,\"100\":1}}],[\"mask=llm\",{\"1\":{\"64\":1}}],[\"mask\",{\"0\":{\"88\":2,\"102\":1,\"703\":1},\"1\":{\"64\":5,\"67\":13,\"83\":15,\"88\":3,\"92\":6,\"94\":4,\"96\":1,\"98\":3,\"99\":3,\"100\":12,\"102\":4,\"105\":2,\"106\":19,\"107\":9,\"122\":5,\"137\":6,\"187\":1,\"188\":3,\"190\":8,\"191\":5,\"192\":6,\"200\":4,\"202\":3,\"205\":1,\"206\":3,\"207\":11,\"208\":26,\"213\":6,\"214\":1,\"220\":1,\"223\":1,\"242\":1,\"250\":2,\"263\":23,\"264\":4,\"265\":4,\"266\":13,\"274\":9,\"354\":1,\"373\":1,\"380\":10,\"382\":1,\"384\":13,\"385\":18,\"386\":6,\"393\":1,\"397\":4,\"398\":2,\"399\":3,\"400\":2,\"401\":5,\"403\":2,\"417\":2,\"418\":2,\"419\":10,\"420\":29,\"421\":1,\"430\":2,\"501\":1,\"502\":1,\"586\":2,\"587\":3,\"588\":1,\"663\":6,\"679\":1,\"681\":1,\"691\":3,\"697\":6,\"698\":1,\"699\":5,\"703\":7,\"713\":14,\"715\":4,\"718\":1,\"719\":2,\"721\":8,\"722\":3,\"724\":3,\"726\":1,\"731\":2,\"733\":1,\"734\":1,\"736\":4,\"737\":6,\"741\":1,\"742\":11,\"746\":2,\"747\":3,\"749\":4,\"750\":4,\"751\":6,\"892\":13,\"893\":11,\"898\":3,\"900\":5,\"924\":10,\"926\":19,\"964\":10}}],[\"matyas\",{\"1\":{\"811\":2}}],[\"matyas函数求导\",{\"1\":{\"811\":1}}],[\"matplotlib\",{\"1\":{\"412\":1,\"816\":1,\"930\":1,\"964\":1}}],[\"matmul\",{\"1\":{\"401\":2,\"418\":2,\"420\":2,\"475\":3,\"703\":2,\"709\":2,\"710\":1,\"724\":2,\"751\":2,\"963\":1}}],[\"mathvista基准\",{\"1\":{\"335\":1}}],[\"math\",{\"1\":{\"213\":2,\"263\":5,\"265\":1,\"401\":1,\"420\":1,\"668\":1,\"709\":2,\"724\":1,\"751\":1,\"823\":1}}],[\"match\",{\"1\":{\"191\":3,\"380\":1}}],[\"matching\",{\"0\":{\"201\":1,\"386\":1,\"419\":1},\"1\":{\"172\":1,\"201\":1,\"368\":1,\"370\":1,\"373\":1,\"383\":1,\"386\":1,\"393\":2,\"411\":11,\"412\":11,\"419\":4,\"650\":1}}],[\"matrix转化成word\",{\"1\":{\"392\":1}}],[\"matrix=none\",{\"1\":{\"208\":1}}],[\"matrix\",{\"0\":{\"561\":1},\"1\":{\"106\":3,\"153\":1,\"208\":4,\"871\":4}}],[\"mansimov\",{\"1\":{\"884\":1}}],[\"manual\",{\"1\":{\"521\":10}}],[\"manycore\",{\"1\":{\"129\":1}}],[\"manipulable\",{\"1\":{\"12\":1}}],[\"macos安装\",{\"1\":{\"815\":1}}],[\"macos\",{\"1\":{\"739\":1}}],[\"macos系统某些包的加载和依赖关系上存在问题\",{\"1\":{\"62\":1}}],[\"machine\",{\"1\":{\"551\":1}}],[\"macc\",{\"1\":{\"117\":1}}],[\"maybe\",{\"1\":{\"64\":1}}],[\"maximum\",{\"0\":{\"903\":1},\"1\":{\"597\":3}}],[\"max=b\",{\"1\":{\"592\":1}}],[\"max=dict\",{\"1\":{\"104\":1}}],[\"maxvit\",{\"1\":{\"510\":1}}],[\"maxpool1d\",{\"1\":{\"121\":1}}],[\"maxpooling\",{\"1\":{\"121\":1}}],[\"max\",{\"1\":{\"52\":1,\"64\":2,\"107\":1,\"116\":1,\"137\":4,\"141\":1,\"150\":6,\"152\":2,\"154\":1,\"157\":7,\"160\":1,\"187\":3,\"188\":3,\"190\":3,\"191\":2,\"192\":3,\"204\":1,\"213\":2,\"263\":18,\"264\":2,\"265\":1,\"293\":1,\"323\":1,\"325\":1,\"380\":8,\"381\":1,\"382\":14,\"417\":3,\"418\":2,\"421\":2,\"431\":1,\"501\":2,\"502\":1,\"595\":3,\"596\":12,\"597\":19,\"663\":2,\"697\":1,\"698\":7,\"699\":10,\"700\":2,\"707\":2,\"709\":11,\"710\":18,\"712\":1,\"713\":6,\"715\":7,\"716\":1,\"734\":4,\"805\":1,\"807\":1,\"893\":4,\"896\":1,\"897\":1,\"898\":4}}],[\"maps\",{\"1\":{\"595\":2,\"597\":2}}],[\"mapper\",{\"1\":{\"382\":1}}],[\"mapping\",{\"1\":{\"14\":1,\"516\":1,\"597\":6}}],[\"map\",{\"1\":{\"52\":1,\"107\":1,\"213\":1,\"264\":3,\"420\":1,\"435\":1,\"447\":1,\"502\":2,\"597\":3,\"700\":1,\"715\":1,\"892\":2,\"899\":2,\"900\":1,\"963\":1,\"964\":1}}],[\"map=device\",{\"1\":{\"52\":1}}],[\"mae\",{\"1\":{\"46\":1,\"47\":1,\"99\":1,\"106\":16,\"216\":1,\"248\":1,\"250\":1,\"259\":1,\"269\":1,\"354\":2}}],[\"monte\",{\"1\":{\"944\":1}}],[\"moe\",{\"1\":{\"823\":2}}],[\"mobilenet\",{\"1\":{\"510\":1}}],[\"most\",{\"0\":{\"622\":1},\"1\":{\"411\":10,\"412\":10,\"516\":2,\"622\":2}}],[\"mome\",{\"0\":{\"372\":1,\"380\":1},\"1\":{\"368\":3,\"369\":1,\"370\":2,\"371\":1,\"372\":4,\"374\":1,\"376\":8,\"377\":3,\"380\":1}}],[\"moments\",{\"1\":{\"268\":1}}],[\"momentum=args\",{\"1\":{\"359\":1}}],[\"momentum=0\",{\"1\":{\"190\":1,\"192\":1,\"293\":1}}],[\"momentum\",{\"0\":{\"202\":1,\"351\":1,\"363\":1},\"1\":{\"17\":1,\"190\":4,\"192\":4,\"193\":2,\"194\":1,\"195\":1,\"202\":1,\"205\":2,\"206\":2,\"208\":3,\"280\":1,\"282\":1,\"293\":12,\"347\":2,\"359\":1,\"361\":1,\"362\":1,\"363\":2,\"819\":1,\"918\":2}}],[\"more\",{\"1\":{\"362\":1,\"660\":19}}],[\"morphology\",{\"1\":{\"11\":1}}],[\"moving\",{\"1\":{\"202\":1,\"213\":2}}],[\"move\",{\"1\":{\"53\":1,\"82\":1,\"86\":1,\"91\":1,\"92\":1,\"107\":2,\"364\":1}}],[\"motivation\",{\"0\":{\"389\":1}}],[\"motions\",{\"1\":{\"161\":1}}],[\"motorcycle\",{\"1\":{\"53\":1,\"408\":1}}],[\"mop\",{\"1\":{\"53\":1}}],[\"moco证明了一点\",{\"1\":{\"348\":1}}],[\"moco作为一个无监督的表征学习工作\",{\"1\":{\"348\":1}}],[\"moco是视觉领域使用对比学习一个里程碑的工作\",{\"1\":{\"348\":1}}],[\"moco\",{\"0\":{\"347\":1},\"1\":{\"17\":1,\"190\":2,\"199\":1,\"216\":1,\"246\":2,\"347\":1,\"351\":2,\"352\":4,\"353\":1,\"354\":2,\"355\":1,\"356\":1,\"359\":9,\"361\":3}}],[\"modifications\",{\"1\":{\"921\":1}}],[\"mod\",{\"1\":{\"194\":1}}],[\"mode=none\",{\"1\":{\"399\":1}}],[\"mode=mode\",{\"1\":{\"208\":1,\"397\":1,\"398\":1,\"403\":1}}],[\"mode=\",{\"1\":{\"190\":2,\"192\":2,\"206\":2,\"208\":1,\"397\":1,\"398\":1,\"403\":1,\"503\":1,\"582\":2}}],[\"mode=false\",{\"1\":{\"64\":1}}],[\"mode\",{\"1\":{\"64\":2,\"122\":2,\"191\":1,\"207\":2,\"399\":1,\"424\":2,\"503\":1,\"865\":1}}],[\"model是decoder输出的大小\",{\"1\":{\"743\":1}}],[\"modelnet40\",{\"1\":{\"157\":2}}],[\"modeling\",{\"0\":{\"200\":1,\"234\":1,\"384\":1},\"1\":{\"129\":2,\"172\":1,\"200\":1,\"208\":1,\"209\":2,\"212\":1,\"220\":4,\"223\":1,\"229\":1,\"234\":2,\"235\":2,\"252\":1,\"266\":1,\"354\":1,\"368\":1,\"370\":1,\"373\":1,\"382\":1,\"383\":2,\"384\":1,\"385\":1,\"393\":2,\"403\":2,\"640\":1,\"663\":2,\"681\":1,\"696\":1,\"898\":1}}],[\"models\",{\"1\":{\"14\":1,\"20\":1,\"129\":2,\"267\":2,\"294\":1,\"321\":2,\"359\":1,\"395\":1,\"435\":1,\"510\":1,\"606\":1,\"608\":1,\"620\":1,\"621\":1,\"622\":1,\"637\":1,\"638\":1,\"644\":1,\"647\":1,\"652\":1,\"655\":1,\"656\":1,\"663\":1,\"664\":1,\"673\":1,\"712\":1,\"837\":2}}],[\"model\",{\"0\":{\"197\":1,\"205\":1,\"360\":1,\"361\":1,\"362\":1,\"392\":1,\"511\":2,\"691\":1},\"1\":{\"7\":2,\"14\":2,\"52\":5,\"64\":3,\"66\":2,\"67\":1,\"68\":3,\"70\":1,\"94\":1,\"104\":3,\"105\":3,\"106\":8,\"107\":10,\"128\":2,\"138\":2,\"141\":2,\"146\":2,\"187\":5,\"188\":3,\"190\":6,\"192\":6,\"202\":1,\"204\":2,\"205\":1,\"213\":16,\"265\":5,\"339\":2,\"343\":1,\"359\":6,\"366\":2,\"380\":1,\"381\":4,\"382\":1,\"408\":2,\"410\":16,\"412\":16,\"413\":1,\"421\":2,\"431\":2,\"435\":6,\"474\":2,\"510\":3,\"511\":7,\"513\":2,\"582\":1,\"604\":1,\"611\":1,\"635\":1,\"656\":1,\"660\":2,\"663\":5,\"690\":1,\"691\":1,\"699\":10,\"700\":11,\"707\":1,\"709\":10,\"712\":6,\"720\":1,\"734\":1,\"743\":2,\"751\":7,\"819\":2,\"822\":2,\"826\":1,\"832\":1,\"918\":4,\"926\":8,\"934\":4,\"935\":3,\"938\":4,\"939\":2,\"963\":9,\"964\":5}}],[\"modality\",{\"0\":{\"391\":1},\"1\":{\"367\":2,\"368\":1,\"372\":1,\"377\":1,\"378\":1,\"380\":4,\"384\":1,\"385\":4,\"390\":1}}],[\"modal\",{\"0\":{\"38\":1},\"1\":{\"14\":1,\"18\":2,\"59\":1,\"60\":1,\"208\":1,\"366\":2,\"397\":1,\"418\":2}}],[\"modulelist\",{\"1\":{\"137\":2,\"141\":4,\"145\":2,\"380\":1,\"398\":1,\"699\":1,\"719\":1}}],[\"module\",{\"0\":{\"38\":1},\"1\":{\"7\":1,\"26\":1,\"54\":1,\"56\":1,\"57\":1,\"58\":2,\"59\":2,\"60\":2,\"65\":1,\"69\":3,\"70\":1,\"83\":8,\"94\":2,\"96\":1,\"97\":1,\"98\":1,\"99\":1,\"100\":2,\"102\":1,\"119\":1,\"120\":1,\"121\":1,\"122\":1,\"123\":1,\"137\":1,\"138\":1,\"141\":2,\"145\":1,\"146\":1,\"152\":1,\"154\":1,\"155\":1,\"156\":1,\"187\":1,\"190\":1,\"191\":1,\"192\":1,\"207\":1,\"213\":3,\"255\":1,\"265\":2,\"266\":2,\"274\":3,\"293\":2,\"361\":1,\"380\":5,\"383\":3,\"384\":11,\"385\":20,\"386\":11,\"397\":1,\"398\":3,\"399\":1,\"400\":2,\"401\":1,\"403\":2,\"419\":1,\"420\":5,\"426\":1,\"427\":2,\"428\":1,\"429\":2,\"430\":1,\"431\":1,\"444\":1,\"454\":1,\"474\":1,\"582\":1,\"586\":1,\"587\":1,\"588\":1,\"589\":1,\"590\":1,\"592\":1,\"663\":1,\"699\":1,\"703\":1,\"709\":1,\"710\":1,\"716\":1,\"718\":3,\"719\":3,\"720\":1,\"724\":1,\"725\":1,\"726\":1,\"728\":1,\"729\":1,\"730\":1,\"742\":1,\"743\":1,\"745\":1,\"746\":1,\"747\":1,\"749\":1,\"750\":1,\"751\":1,\"810\":1,\"899\":1,\"900\":1,\"918\":2,\"926\":1,\"931\":1,\"937\":1,\"963\":4,\"964\":1}}],[\"sg\",{\"1\":{\"959\":2}}],[\"sgd\",{\"1\":{\"319\":1,\"359\":2,\"819\":1}}],[\"sleep\",{\"1\":{\"950\":1}}],[\"slice\",{\"0\":{\"544\":1}}],[\"slight\",{\"1\":{\"52\":2}}],[\"s型智能增长曲线\",{\"1\":{\"837\":1}}],[\"srl\",{\"1\":{\"736\":1}}],[\"src\",{\"1\":{\"487\":5,\"663\":1,\"742\":16,\"749\":7,\"750\":2}}],[\"s3\",{\"1\":{\"696\":2}}],[\"s>e\",{\"1\":{\"694\":1}}],[\"s=2\",{\"1\":{\"694\":2}}],[\"s=str\",{\"1\":{\"424\":1}}],[\"sørensen\",{\"1\":{\"586\":1}}],[\"swiglu\",{\"1\":{\"667\":1,\"823\":4}}],[\"swin\",{\"1\":{\"510\":1}}],[\"swav\",{\"1\":{\"246\":1}}],[\"swapaxes\",{\"1\":{\"58\":3,\"59\":5,\"60\":5,\"83\":5}}],[\"snapshot\",{\"1\":{\"410\":1,\"412\":2}}],[\"snli\",{\"1\":{\"268\":1}}],[\"sst\",{\"1\":{\"634\":4,\"678\":1,\"680\":1}}],[\"ssl\",{\"1\":{\"359\":2}}],[\"ssg\",{\"1\":{\"138\":3}}],[\"sft\",{\"1\":{\"305\":1,\"317\":1,\"654\":1,\"656\":6,\"657\":1,\"658\":1,\"823\":1}}],[\"s的k\",{\"1\":{\"288\":1}}],[\"smooth=1\",{\"1\":{\"586\":1,\"587\":1,\"588\":1,\"589\":1,\"590\":1,\"592\":1}}],[\"smooth\",{\"0\":{\"259\":1},\"1\":{\"255\":5,\"259\":3,\"586\":4,\"587\":3,\"588\":3,\"589\":1,\"590\":3,\"592\":3,\"899\":6}}],[\"smoothing=0\",{\"1\":{\"403\":1,\"418\":2,\"420\":1,\"700\":2}}],[\"smoothing\",{\"1\":{\"208\":1,\"510\":1}}],[\"small\",{\"1\":{\"106\":1,\"710\":2}}],[\"sbucaptiondatamodule\",{\"1\":{\"382\":1}}],[\"sbu\",{\"1\":{\"176\":1,\"185\":1,\"224\":2,\"382\":1}}],[\"symb\",{\"1\":{\"697\":2}}],[\"symbol\",{\"1\":{\"660\":1}}],[\"symbols\",{\"1\":{\"595\":4,\"597\":4}}],[\"symlinks=false\",{\"1\":{\"410\":1,\"412\":1}}],[\"symmetric\",{\"1\":{\"150\":1,\"160\":1}}],[\"systematic\",{\"1\":{\"409\":1,\"607\":1}}],[\"sys\",{\"1\":{\"107\":3,\"265\":1,\"718\":1,\"728\":1}}],[\"squad和race任务上实现了sota性能\",{\"1\":{\"688\":1}}],[\"squad和race等任务上达到了新的\",{\"1\":{\"681\":1}}],[\"squad\",{\"1\":{\"656\":1,\"657\":1,\"658\":1,\"678\":2,\"680\":1,\"684\":2,\"685\":1,\"735\":1}}],[\"squadv2\",{\"1\":{\"648\":1}}],[\"squaretest\",{\"1\":{\"794\":1,\"795\":1}}],[\"square\",{\"1\":{\"137\":1,\"145\":1,\"762\":1,\"763\":2,\"766\":2,\"780\":1,\"790\":2,\"794\":2,\"795\":2,\"804\":3,\"807\":7}}],[\"squeeze\",{\"1\":{\"94\":1,\"107\":1,\"121\":1,\"418\":2,\"733\":2,\"734\":2,\"926\":2,\"935\":3,\"939\":1,\"964\":2}}],[\"sqrdists\",{\"1\":{\"137\":2}}],[\"sqrt\",{\"1\":{\"107\":1,\"119\":1,\"213\":2,\"263\":2,\"380\":1,\"401\":1,\"420\":1,\"522\":1,\"709\":1,\"724\":1,\"751\":1,\"899\":1}}],[\"soap\",{\"1\":{\"873\":1}}],[\"sonnet\",{\"1\":{\"823\":4}}],[\"sometimes\",{\"1\":{\"734\":1}}],[\"someone\",{\"1\":{\"454\":3}}],[\"southampton\",{\"1\":{\"712\":1}}],[\"source\",{\"1\":{\"321\":2,\"435\":1,\"558\":1}}],[\"source=chatgpt\",{\"1\":{\"185\":1}}],[\"solaiman\",{\"1\":{\"655\":1}}],[\"solarization\",{\"1\":{\"286\":1,\"293\":1}}],[\"sol\",{\"1\":{\"596\":2,\"597\":3}}],[\"solely\",{\"1\":{\"102\":1}}],[\"sota\",{\"1\":{\"157\":1,\"183\":1,\"268\":1,\"305\":1,\"309\":1,\"310\":1,\"342\":1,\"343\":1,\"641\":1,\"678\":1}}],[\"sorted=true\",{\"1\":{\"488\":1}}],[\"sorted\",{\"1\":{\"92\":1,\"447\":1,\"488\":1,\"595\":1,\"597\":1}}],[\"sort\",{\"1\":{\"92\":5,\"137\":1,\"145\":1,\"424\":1,\"582\":1,\"805\":1,\"807\":1,\"815\":1}}],[\"softtarget\",{\"1\":{\"510\":1}}],[\"soft\",{\"1\":{\"88\":1,\"102\":2,\"106\":10,\"190\":1,\"192\":3,\"202\":1,\"204\":4,\"206\":3,\"207\":1,\"208\":13,\"256\":4,\"257\":2,\"258\":10,\"283\":1,\"586\":1,\"587\":2,\"588\":1,\"589\":1,\"899\":2}}],[\"softmax归一化得到注意力概率\",{\"1\":{\"420\":1}}],[\"softmax的采样过程分为两步\",{\"1\":{\"257\":1}}],[\"softmax\",{\"0\":{\"257\":1},\"1\":{\"56\":2,\"65\":5,\"69\":3,\"83\":2,\"112\":1,\"117\":4,\"119\":5,\"138\":1,\"141\":1,\"143\":2,\"146\":2,\"155\":2,\"156\":2,\"160\":2,\"190\":6,\"192\":6,\"199\":1,\"201\":1,\"202\":1,\"205\":1,\"206\":4,\"207\":2,\"208\":3,\"216\":1,\"228\":1,\"232\":1,\"234\":1,\"238\":2,\"255\":1,\"256\":3,\"257\":7,\"258\":7,\"260\":2,\"273\":1,\"274\":1,\"285\":1,\"293\":3,\"355\":7,\"361\":1,\"380\":2,\"386\":2,\"401\":2,\"408\":1,\"419\":2,\"420\":1,\"430\":1,\"527\":1,\"537\":1,\"538\":1,\"582\":1,\"589\":2,\"631\":1,\"694\":1,\"703\":1,\"709\":1,\"710\":1,\"724\":1,\"733\":3,\"741\":1,\"743\":1,\"751\":1,\"886\":1,\"895\":2,\"897\":1,\"898\":2,\"899\":12,\"921\":1,\"926\":3,\"964\":1}}],[\"sandwich\",{\"1\":{\"892\":4}}],[\"sanh\",{\"1\":{\"655\":1}}],[\"say\",{\"1\":{\"451\":5,\"454\":3}}],[\"saucer\",{\"1\":{\"408\":1}}],[\"sacred\",{\"1\":{\"379\":1}}],[\"saturation=0\",{\"1\":{\"293\":1}}],[\"sam\",{\"1\":{\"327\":1}}],[\"same\",{\"1\":{\"190\":2,\"729\":1,\"751\":1}}],[\"sampling=false\",{\"1\":{\"421\":1}}],[\"sampling\",{\"0\":{\"134\":1,\"897\":1},\"1\":{\"83\":2,\"121\":1,\"133\":2,\"134\":1,\"137\":1,\"143\":1,\"178\":2,\"188\":1,\"355\":1,\"421\":2,\"897\":1}}],[\"sampler=self\",{\"1\":{\"382\":3}}],[\"sampler=sampler\",{\"1\":{\"265\":1}}],[\"sampler=train\",{\"1\":{\"359\":1,\"715\":1}}],[\"sampler\",{\"1\":{\"359\":2,\"382\":9,\"518\":1,\"715\":2}}],[\"samplers\",{\"1\":{\"187\":1,\"190\":1,\"192\":1}}],[\"sample=use\",{\"1\":{\"421\":1}}],[\"sample=true\",{\"1\":{\"188\":1}}],[\"sample=false\",{\"1\":{\"188\":1}}],[\"sampled\",{\"1\":{\"137\":6,\"141\":1,\"256\":2,\"899\":2,\"926\":2,\"964\":2}}],[\"samples=len\",{\"1\":{\"518\":1}}],[\"samples\",{\"1\":{\"106\":1,\"137\":1,\"213\":9,\"265\":4,\"417\":3,\"421\":2,\"518\":3,\"918\":2,\"926\":6}}],[\"sample\",{\"1\":{\"52\":1,\"53\":6,\"59\":6,\"70\":6,\"82\":3,\"83\":7,\"94\":11,\"100\":7,\"121\":3,\"132\":1,\"137\":14,\"141\":2,\"188\":2,\"213\":1,\"424\":1,\"431\":1,\"518\":2,\"698\":7,\"815\":3,\"895\":4,\"897\":1,\"898\":3,\"918\":3,\"935\":5,\"964\":7}}],[\"salesforce\",{\"1\":{\"164\":1,\"185\":1,\"193\":1,\"414\":1,\"696\":1}}],[\"sa4\",{\"1\":{\"146\":2}}],[\"sa3\",{\"1\":{\"138\":2,\"141\":2,\"146\":2}}],[\"sa\",{\"1\":{\"138\":1,\"144\":3,\"146\":1,\"171\":4,\"179\":2}}],[\"saved\",{\"1\":{\"697\":3,\"700\":1}}],[\"save\",{\"1\":{\"106\":2,\"410\":3,\"412\":3,\"420\":3,\"474\":1,\"595\":1,\"597\":1,\"700\":1,\"712\":1,\"918\":2,\"926\":2,\"963\":1,\"964\":1}}],[\"sa1\",{\"1\":{\"70\":1,\"83\":1,\"94\":1,\"138\":2,\"141\":2,\"146\":2}}],[\"sa2\",{\"1\":{\"70\":1,\"83\":3,\"94\":1,\"138\":2,\"141\":2,\"146\":2}}],[\"sv\",{\"1\":{\"69\":2}}],[\"sklearn\",{\"1\":{\"513\":2,\"514\":2}}],[\"skip\",{\"1\":{\"123\":2,\"143\":1,\"145\":1,\"188\":1,\"421\":1,\"735\":2}}],[\"sk\",{\"1\":{\"69\":2}}],[\"skateboard\",{\"1\":{\"53\":1}}],[\"scikit\",{\"0\":{\"512\":1},\"1\":{\"514\":1}}],[\"science\",{\"1\":{\"342\":1}}],[\"scienceqa\",{\"1\":{\"342\":2,\"343\":1}}],[\"scissors\",{\"1\":{\"53\":1,\"82\":1,\"87\":2,\"92\":1}}],[\"scao\",{\"1\":{\"671\":1}}],[\"scalar\",{\"1\":{\"589\":1}}],[\"scalinglayerforim\",{\"1\":{\"213\":1}}],[\"scalinglayerforclip\",{\"1\":{\"213\":1}}],[\"scaling\",{\"1\":{\"213\":3,\"294\":1,\"607\":1,\"646\":1,\"647\":1,\"650\":2,\"658\":1,\"822\":7}}],[\"scaleai\",{\"1\":{\"656\":1}}],[\"scaleddotproductattention\",{\"1\":{\"703\":1}}],[\"scaled\",{\"1\":{\"524\":1,\"751\":1}}],[\"scale未指定\",{\"1\":{\"430\":1}}],[\"scale=cond\",{\"1\":{\"895\":1}}],[\"scale=qk\",{\"1\":{\"380\":2,\"429\":1,\"431\":1}}],[\"scale=none\",{\"1\":{\"380\":3,\"428\":1,\"429\":1,\"430\":1,\"431\":1}}],[\"scale=\",{\"1\":{\"359\":1}}],[\"scale=local\",{\"1\":{\"293\":1}}],[\"scale=global\",{\"1\":{\"293\":2}}],[\"scale=0\",{\"1\":{\"192\":1}}],[\"scaler\",{\"1\":{\"265\":3}}],[\"scales\",{\"1\":{\"143\":1}}],[\"scale\",{\"0\":{\"140\":1},\"1\":{\"6\":1,\"7\":1,\"14\":1,\"25\":1,\"56\":2,\"65\":2,\"69\":3,\"83\":5,\"107\":2,\"138\":1,\"139\":1,\"141\":1,\"293\":18,\"380\":13,\"385\":21,\"429\":1,\"430\":3,\"431\":1,\"503\":5,\"582\":3,\"604\":1,\"607\":1,\"893\":1,\"894\":5,\"895\":3}}],[\"scatter\",{\"0\":{\"487\":1},\"1\":{\"213\":2,\"487\":1,\"896\":2}}],[\"schema\",{\"0\":{\"391\":1},\"1\":{\"641\":1,\"648\":1}}],[\"schedule\",{\"1\":{\"265\":6,\"293\":8}}],[\"scheduler=none\",{\"1\":{\"265\":1}}],[\"scheduler\",{\"1\":{\"104\":4,\"204\":1,\"293\":3,\"510\":1}}],[\"schuhmann\",{\"1\":{\"176\":1}}],[\"scores\",{\"1\":{\"207\":5,\"208\":6,\"401\":11,\"403\":7,\"420\":20,\"703\":4,\"709\":2,\"710\":7,\"724\":12,\"730\":2,\"731\":4,\"733\":4,\"736\":1,\"751\":4,\"895\":2}}],[\"score\",{\"1\":{\"70\":3,\"102\":1,\"106\":5,\"339\":1,\"384\":2,\"386\":1,\"418\":2,\"419\":2,\"578\":1,\"586\":2,\"587\":2,\"588\":4,\"592\":4,\"709\":5,\"730\":2,\"731\":4}}],[\"scene\",{\"1\":{\"69\":8,\"83\":7}}],[\"s\",{\"1\":{\"52\":8,\"55\":6,\"69\":4,\"83\":9,\"119\":5,\"122\":4,\"137\":16,\"141\":8,\"145\":11,\"150\":1,\"190\":1,\"233\":1,\"286\":1,\"288\":5,\"291\":1,\"424\":1,\"435\":1,\"461\":1,\"492\":1,\"595\":2,\"597\":2,\"619\":1,\"640\":3,\"641\":1,\"658\":3,\"660\":2,\"694\":2,\"713\":1,\"877\":1,\"918\":2}}],[\"shells\",{\"0\":{\"872\":1}}],[\"shell=true\",{\"1\":{\"815\":1}}],[\"shinyapps\",{\"1\":{\"854\":1}}],[\"shift\",{\"1\":{\"892\":4}}],[\"shift到\",{\"1\":{\"709\":1}}],[\"shifted\",{\"1\":{\"403\":2,\"420\":3}}],[\"shuffle\",{\"1\":{\"329\":1,\"362\":3,\"513\":1,\"698\":3}}],[\"shuffle=\",{\"1\":{\"359\":1}}],[\"shuffle=false\",{\"1\":{\"104\":2,\"382\":3,\"425\":1,\"513\":1}}],[\"shuffle=true\",{\"1\":{\"104\":1,\"293\":1,\"382\":3,\"425\":1,\"513\":3,\"700\":1,\"918\":1,\"926\":1,\"933\":1,\"963\":1}}],[\"show\",{\"1\":{\"411\":1,\"412\":1,\"424\":1,\"816\":1,\"935\":2,\"939\":1,\"964\":1}}],[\"shown\",{\"1\":{\"52\":2}}],[\"shot条件下进行全面评估的工作\",{\"1\":{\"650\":1}}],[\"shot模型\",{\"1\":{\"650\":1}}],[\"shot能力会随着模型规模的增加而显著增强\",{\"1\":{\"650\":1}}],[\"shot能力主要依赖于识别任务格式和输出模式\",{\"1\":{\"649\":1}}],[\"shot效果难以稳定复现\",{\"1\":{\"649\":1}}],[\"shot情境下\",{\"1\":{\"648\":1}}],[\"shot元学习能力\",{\"1\":{\"648\":1}}],[\"shot得分\",{\"1\":{\"648\":1}}],[\"shot得分达到77\",{\"1\":{\"648\":1}}],[\"shot得分达到71\",{\"1\":{\"648\":1}}],[\"shot翻译任务中\",{\"1\":{\"648\":1}}],[\"shot优势\",{\"1\":{\"648\":1}}],[\"shot设定就达到了与微调sota模型相当甚至更优的水平\",{\"1\":{\"648\":1}}],[\"shot设定下均展示了强大的任务适应能力\",{\"1\":{\"648\":1}}],[\"shot设定下稍高于随机水平\",{\"1\":{\"648\":1}}],[\"shot设定下表现优异\",{\"1\":{\"648\":1}}],[\"shot设定下\",{\"1\":{\"647\":1}}],[\"shot设定下的表现令人惊喜\",{\"1\":{\"647\":1}}],[\"shot设置下准确率达到86\",{\"1\":{\"648\":1}}],[\"shot图文生成\",{\"1\":{\"416\":1}}],[\"shot图像分类\",{\"1\":{\"408\":1}}],[\"shot性能评估\",{\"1\":{\"413\":1}}],[\"shot性能\",{\"1\":{\"413\":1}}],[\"shot迁移到下游任务\",{\"1\":{\"413\":1}}],[\"shot学习方面展现出极强的能力\",{\"1\":{\"649\":1}}],[\"shot学习的启发\",{\"1\":{\"650\":1}}],[\"shot学习的\",{\"1\":{\"649\":1}}],[\"shot学习中可用的示例数量\",{\"1\":{\"649\":1}}],[\"shot学习\",{\"1\":{\"413\":1,\"650\":1}}],[\"shot推理\",{\"1\":{\"410\":1}}],[\"shot分类时\",{\"1\":{\"409\":1}}],[\"shot分类的过程相当直接\",{\"1\":{\"408\":1}}],[\"shot\",{\"1\":{\"273\":2,\"309\":2,\"339\":1,\"415\":1,\"626\":1,\"638\":1,\"641\":1,\"644\":1,\"646\":3,\"647\":3,\"650\":2,\"656\":2,\"657\":2,\"668\":3,\"669\":1,\"883\":1,\"890\":1}}],[\"shortcut\",{\"1\":{\"120\":3}}],[\"should\",{\"1\":{\"107\":3}}],[\"shannon\",{\"0\":{\"911\":1}}],[\"shaw\",{\"0\":{\"709\":1},\"1\":{\"709\":3,\"710\":3}}],[\"shamir\",{\"1\":{\"500\":1}}],[\"sharpening\",{\"1\":{\"280\":1,\"285\":1,\"290\":1,\"293\":1}}],[\"sharedembedding\",{\"1\":{\"892\":2}}],[\"shared\",{\"1\":{\"376\":1,\"699\":4,\"892\":6}}],[\"sharegpt4v\",{\"1\":{\"332\":1}}],[\"share\",{\"1\":{\"119\":9,\"120\":3,\"123\":15,\"892\":2}}],[\"shape=\",{\"1\":{\"892\":1,\"964\":1}}],[\"shape=box\",{\"1\":{\"815\":4}}],[\"shape=box可将节点设为矩形\",{\"1\":{\"815\":1}}],[\"shape=self\",{\"1\":{\"263\":1}}],[\"shape为\",{\"1\":{\"426\":1,\"547\":1}}],[\"shape的形状从\",{\"1\":{\"137\":1}}],[\"shape\",{\"1\":{\"59\":1,\"64\":8,\"92\":6,\"106\":11,\"107\":2,\"119\":15,\"121\":3,\"122\":4,\"137\":15,\"138\":1,\"141\":2,\"143\":1,\"145\":2,\"152\":3,\"154\":3,\"156\":4,\"190\":3,\"207\":1,\"208\":4,\"213\":8,\"256\":4,\"263\":1,\"265\":1,\"266\":2,\"274\":4,\"293\":1,\"362\":1,\"364\":1,\"380\":12,\"384\":2,\"401\":2,\"417\":1,\"419\":1,\"420\":2,\"421\":1,\"426\":1,\"427\":1,\"428\":1,\"430\":1,\"431\":2,\"463\":2,\"466\":1,\"469\":2,\"470\":1,\"472\":4,\"478\":1,\"490\":1,\"513\":2,\"534\":1,\"540\":1,\"542\":2,\"544\":2,\"546\":3,\"582\":8,\"589\":1,\"663\":10,\"699\":7,\"709\":3,\"710\":9,\"724\":4,\"737\":1,\"808\":3,\"815\":1,\"892\":2,\"893\":11,\"895\":2,\"896\":1,\"898\":3,\"899\":4,\"900\":5,\"918\":5,\"926\":2,\"963\":4,\"964\":2}}],[\"shao\",{\"1\":{\"28\":1}}],[\"siri\",{\"1\":{\"827\":1}}],[\"sin\",{\"1\":{\"706\":1,\"814\":1}}],[\"sinusoidal\",{\"0\":{\"706\":1},\"1\":{\"706\":1}}],[\"single\",{\"1\":{\"7\":1,\"138\":1,\"268\":1,\"271\":1}}],[\"silhouette\",{\"1\":{\"408\":1}}],[\"sig\",{\"1\":{\"595\":1,\"596\":1,\"597\":1}}],[\"sigma\",{\"1\":{\"500\":1,\"847\":3}}],[\"sigmoid\",{\"1\":{\"39\":1,\"59\":3,\"70\":3,\"83\":3,\"94\":1,\"100\":3,\"102\":1,\"143\":1,\"586\":3,\"587\":4,\"588\":4,\"589\":3,\"590\":3,\"592\":1,\"918\":1,\"931\":1,\"937\":1,\"963\":1}}],[\"siglip\",{\"1\":{\"327\":2}}],[\"sign\",{\"1\":{\"106\":1}}],[\"side\",{\"1\":{\"64\":2}}],[\"size和较小的学习率\",{\"1\":{\"647\":1}}],[\"size的四倍\",{\"1\":{\"432\":1}}],[\"size是transformer\",{\"1\":{\"432\":1}}],[\"size就是对应通过embedding层后每个token的dim\",{\"1\":{\"432\":1}}],[\"size也是同样处理手段\",{\"1\":{\"417\":1}}],[\"size\",{\"1\":{\"53\":4,\"54\":1,\"60\":1,\"64\":6,\"65\":1,\"66\":2,\"67\":12,\"68\":3,\"69\":4,\"70\":3,\"82\":4,\"83\":13,\"94\":1,\"104\":3,\"107\":2,\"121\":5,\"152\":3,\"153\":2,\"154\":1,\"156\":2,\"187\":6,\"188\":2,\"190\":14,\"191\":3,\"192\":12,\"205\":9,\"206\":3,\"207\":2,\"208\":6,\"213\":21,\"223\":2,\"224\":1,\"236\":1,\"255\":8,\"256\":2,\"263\":5,\"264\":3,\"265\":2,\"266\":26,\"272\":1,\"282\":1,\"285\":1,\"292\":4,\"293\":8,\"353\":2,\"359\":2,\"361\":1,\"364\":4,\"380\":33,\"382\":27,\"384\":6,\"386\":9,\"397\":2,\"400\":3,\"401\":3,\"403\":8,\"410\":4,\"412\":4,\"417\":6,\"418\":8,\"419\":10,\"420\":7,\"421\":4,\"424\":1,\"425\":2,\"426\":24,\"427\":4,\"428\":2,\"430\":10,\"431\":4,\"463\":1,\"472\":3,\"477\":1,\"484\":2,\"486\":2,\"489\":2,\"490\":2,\"492\":2,\"503\":1,\"513\":4,\"540\":1,\"542\":1,\"582\":8,\"586\":4,\"587\":3,\"588\":3,\"589\":4,\"590\":5,\"592\":5,\"660\":1,\"663\":1,\"697\":3,\"699\":19,\"700\":2,\"703\":1,\"709\":1,\"710\":1,\"715\":1,\"716\":7,\"718\":5,\"720\":2,\"722\":1,\"724\":15,\"725\":3,\"728\":3,\"729\":3,\"730\":1,\"731\":1,\"733\":12,\"734\":3,\"736\":2,\"737\":13,\"745\":2,\"746\":4,\"747\":1,\"749\":4,\"750\":1,\"751\":3,\"808\":4,\"892\":11,\"893\":5,\"895\":6,\"899\":11,\"900\":11,\"918\":12,\"926\":8,\"964\":2}}],[\"size=7\",{\"1\":{\"964\":1}}],[\"size=768\",{\"1\":{\"435\":1}}],[\"size=opt\",{\"1\":{\"918\":1}}],[\"size=32\",{\"1\":{\"700\":1}}],[\"size=384\",{\"1\":{\"190\":1}}],[\"size=0\",{\"1\":{\"513\":2}}],[\"size=64\",{\"1\":{\"410\":2,\"412\":2,\"926\":1}}],[\"size=none\",{\"1\":{\"382\":1,\"428\":1,\"431\":1,\"503\":1,\"513\":2}}],[\"size=self\",{\"1\":{\"382\":11}}],[\"size=img\",{\"1\":{\"380\":1,\"427\":1,\"428\":1,\"431\":1}}],[\"size=patch\",{\"1\":{\"266\":1,\"380\":2,\"426\":1,\"427\":1,\"428\":1,\"431\":1}}],[\"size=args\",{\"1\":{\"264\":2,\"265\":2,\"293\":3,\"359\":1,\"715\":1}}],[\"size=hidden\",{\"1\":{\"255\":2}}],[\"size=4\",{\"1\":{\"255\":2,\"899\":2}}],[\"size=224\",{\"1\":{\"192\":1,\"266\":1,\"380\":2,\"426\":1,\"427\":1,\"428\":1,\"431\":1,\"435\":1}}],[\"size=57600\",{\"1\":{\"190\":1,\"192\":1}}],[\"size=config\",{\"1\":{\"187\":1,\"190\":2,\"192\":2,\"205\":2}}],[\"size=128\",{\"1\":{\"933\":1,\"963\":1}}],[\"size=16\",{\"1\":{\"205\":2,\"266\":1,\"380\":2,\"426\":1,\"427\":1,\"428\":1,\"431\":1,\"435\":1,\"712\":2}}],[\"size=1\",{\"1\":{\"152\":1,\"255\":2,\"899\":3,\"926\":1}}],[\"size=10\",{\"1\":{\"104\":1}}],[\"size=batch\",{\"1\":{\"104\":3,\"382\":1,\"425\":2}}],[\"size=\",{\"1\":{\"53\":1,\"82\":1,\"83\":3,\"187\":1,\"190\":1,\"192\":1}}],[\"site\",{\"1\":{\"557\":1}}],[\"sit\",{\"1\":{\"53\":1,\"82\":1,\"92\":1,\"107\":2}}],[\"simply\",{\"1\":{\"720\":1}}],[\"simplicity\",{\"1\":{\"364\":1}}],[\"similarities\",{\"1\":{\"410\":2,\"411\":2,\"412\":4}}],[\"similarity\",{\"1\":{\"46\":1,\"106\":3,\"407\":1,\"408\":1,\"410\":2,\"411\":1,\"412\":3,\"418\":4,\"506\":1,\"634\":1}}],[\"simmim\",{\"1\":{\"269\":1}}],[\"simvlm\",{\"1\":{\"268\":1}}],[\"simclr\",{\"1\":{\"246\":1,\"264\":1,\"355\":1,\"357\":2}}],[\"sim\",{\"1\":{\"46\":1,\"47\":1,\"99\":1,\"106\":16,\"190\":20,\"191\":2,\"192\":20,\"206\":18,\"207\":3,\"213\":5,\"274\":6,\"386\":4,\"418\":13,\"419\":4,\"900\":5}}],[\"sudo\",{\"1\":{\"815\":1}}],[\"sunflowers\",{\"1\":{\"411\":1,\"412\":1}}],[\"survey\",{\"1\":{\"409\":1,\"607\":1,\"837\":2}}],[\"surfboard\",{\"1\":{\"53\":1}}],[\"suite\",{\"1\":{\"382\":4}}],[\"suites\",{\"1\":{\"321\":2}}],[\"suitcase\",{\"1\":{\"53\":1}}],[\"successfully\",{\"1\":{\"410\":1,\"412\":1}}],[\"successful\",{\"1\":{\"87\":1}}],[\"such\",{\"1\":{\"12\":1}}],[\"subplot\",{\"1\":{\"964\":1}}],[\"subplots\",{\"1\":{\"935\":2,\"939\":1}}],[\"subprocess\",{\"1\":{\"815\":1}}],[\"sublayer是传入的参数\",{\"1\":{\"745\":1}}],[\"sublayer\",{\"1\":{\"745\":3,\"746\":3,\"749\":4}}],[\"sublayerconnection模型结构图\",{\"1\":{\"745\":1}}],[\"sublayerconnection\",{\"0\":{\"745\":1},\"1\":{\"745\":2,\"746\":1,\"749\":1}}],[\"subword\",{\"1\":{\"595\":4,\"597\":2}}],[\"subtract\",{\"1\":{\"516\":1}}],[\"subset的对齐分数\",{\"1\":{\"393\":1}}],[\"subset和visual\",{\"1\":{\"393\":1}}],[\"subset\",{\"1\":{\"121\":1}}],[\"subject\",{\"1\":{\"82\":2,\"83\":1}}],[\"sub\",{\"1\":{\"69\":8,\"82\":5,\"83\":24,\"410\":5,\"412\":5,\"544\":1,\"595\":1,\"597\":1,\"809\":9}}],[\"summation\",{\"1\":{\"100\":1,\"160\":1,\"475\":1}}],[\"sum\",{\"1\":{\"67\":1,\"94\":2,\"100\":4,\"102\":6,\"105\":1,\"106\":3,\"107\":1,\"119\":5,\"121\":1,\"122\":2,\"137\":1,\"145\":2,\"160\":1,\"190\":3,\"192\":2,\"206\":2,\"208\":1,\"213\":10,\"261\":1,\"263\":1,\"293\":1,\"362\":1,\"383\":1,\"403\":1,\"420\":1,\"424\":1,\"431\":1,\"475\":1,\"481\":1,\"582\":1,\"586\":3,\"587\":9,\"588\":2,\"590\":3,\"592\":3,\"932\":3,\"963\":2}}],[\"suptitle\",{\"1\":{\"935\":1,\"964\":1}}],[\"superglue整体表现良好\",{\"1\":{\"648\":1}}],[\"supervision\",{\"1\":{\"387\":2}}],[\"supervised\",{\"1\":{\"279\":2,\"305\":1,\"339\":1,\"413\":1,\"602\":1,\"656\":1}}],[\"super\",{\"1\":{\"53\":1,\"58\":2,\"59\":2,\"60\":2,\"65\":1,\"69\":2,\"70\":1,\"82\":1,\"83\":6,\"97\":1,\"102\":1,\"119\":1,\"120\":1,\"121\":1,\"122\":1,\"123\":1,\"137\":1,\"138\":1,\"141\":2,\"145\":1,\"146\":1,\"152\":1,\"154\":1,\"155\":1,\"156\":1,\"187\":1,\"192\":1,\"205\":1,\"213\":3,\"255\":1,\"266\":1,\"293\":2,\"361\":1,\"380\":4,\"382\":4,\"397\":1,\"400\":1,\"403\":1,\"426\":1,\"427\":1,\"428\":1,\"429\":2,\"430\":1,\"431\":1,\"586\":1,\"587\":1,\"588\":1,\"589\":1,\"590\":1,\"592\":1,\"699\":1,\"709\":1,\"710\":1,\"716\":1,\"718\":3,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"724\":1,\"725\":1,\"726\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"734\":1,\"736\":1,\"737\":1,\"742\":1,\"743\":1,\"745\":1,\"746\":1,\"747\":1,\"749\":1,\"750\":1,\"751\":1,\"899\":1,\"900\":1,\"918\":2,\"926\":3,\"931\":1,\"937\":1,\"963\":4,\"964\":3}}],[\"supplied\",{\"1\":{\"893\":1}}],[\"supplementary\",{\"1\":{\"11\":1}}],[\"suppose\",{\"1\":{\"703\":1}}],[\"supported\",{\"1\":{\"424\":2,\"792\":1,\"805\":1,\"808\":1}}],[\"support\",{\"1\":{\"7\":1,\"53\":1,\"82\":1,\"92\":1}}],[\"sphere\",{\"1\":{\"811\":2}}],[\"sphere函数求导\",{\"1\":{\"811\":1}}],[\"spherical\",{\"1\":{\"110\":1,\"871\":1}}],[\"spurious\",{\"1\":{\"646\":1}}],[\"specific\",{\"1\":{\"272\":1}}],[\"special\",{\"1\":{\"188\":1,\"421\":1,\"713\":6,\"735\":2}}],[\"speak\",{\"1\":{\"53\":1}}],[\"spidercnn\",{\"1\":{\"110\":1,\"157\":2}}],[\"spg\",{\"1\":{\"110\":1}}],[\"spoon\",{\"1\":{\"53\":1}}],[\"spout\",{\"1\":{\"48\":1,\"52\":6,\"55\":2}}],[\"splitext\",{\"1\":{\"410\":1,\"412\":1,\"424\":1,\"815\":1}}],[\"split=\",{\"1\":{\"92\":1,\"382\":7}}],[\"split\",{\"0\":{\"513\":1},\"1\":{\"52\":1,\"53\":8,\"64\":2,\"70\":4,\"82\":6,\"83\":4,\"92\":5,\"213\":4,\"382\":9,\"410\":1,\"412\":1,\"424\":1,\"425\":1,\"513\":4,\"595\":2,\"597\":2,\"663\":2,\"696\":1,\"733\":2,\"734\":1}}],[\"splat\",{\"1\":{\"14\":1}}],[\"splatting\",{\"0\":{\"13\":1},\"1\":{\"19\":1,\"21\":1,\"22\":1,\"26\":1}}],[\"span\",{\"1\":{\"733\":1,\"735\":2}}],[\"sparse\",{\"1\":{\"647\":1,\"892\":3,\"951\":1}}],[\"spacy\",{\"1\":{\"341\":1,\"633\":1}}],[\"space\",{\"1\":{\"12\":1,\"597\":1,\"918\":1,\"956\":1}}],[\"spatiallm\",{\"0\":{\"129\":1},\"1\":{\"129\":3}}],[\"spatial\",{\"1\":{\"12\":1,\"64\":5,\"69\":3,\"152\":1,\"924\":2}}],[\"spring\",{\"1\":{\"2\":1}}],[\"ste\",{\"1\":{\"963\":2}}],[\"stem\",{\"1\":{\"669\":1}}],[\"steps=100\",{\"1\":{\"712\":2}}],[\"steps=none\",{\"1\":{\"265\":1}}],[\"steps=epoch\",{\"1\":{\"265\":1}}],[\"steps\",{\"1\":{\"204\":1,\"265\":1}}],[\"steplr\",{\"1\":{\"104\":1}}],[\"step\",{\"0\":{\"65\":1,\"66\":1,\"67\":1,\"68\":1,\"69\":1,\"70\":1},\"1\":{\"64\":11,\"102\":5,\"104\":1,\"105\":1,\"187\":1,\"190\":1,\"192\":1,\"204\":1,\"208\":12,\"265\":1,\"293\":1,\"325\":1,\"359\":2,\"381\":9,\"383\":2,\"385\":12,\"420\":5,\"431\":1,\"440\":1,\"544\":1,\"589\":5,\"619\":2,\"660\":19,\"663\":3,\"698\":2,\"700\":1,\"710\":6,\"712\":4,\"918\":2,\"926\":1,\"932\":4,\"934\":1,\"938\":1,\"963\":1,\"964\":1}}],[\"stiennon\",{\"1\":{\"655\":1,\"656\":1}}],[\"sts\",{\"1\":{\"634\":1,\"685\":1}}],[\"student\",{\"0\":{\"867\":1},\"1\":{\"293\":26,\"867\":1,\"868\":1}}],[\"studies\",{\"0\":{\"242\":1,\"376\":1}}],[\"study\",{\"0\":{\"48\":1,\"180\":1,\"344\":1},\"1\":{\"179\":1}}],[\"stochastic\",{\"1\":{\"224\":1,\"236\":1,\"380\":3}}],[\"storage\",{\"1\":{\"491\":2,\"712\":1}}],[\"storagefurniture\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"stories\",{\"1\":{\"224\":1,\"625\":1,\"626\":1,\"680\":1}}],[\"stopping\",{\"1\":{\"265\":1}}],[\"stop\",{\"1\":{\"212\":1,\"440\":2,\"544\":1,\"959\":1,\"963\":1}}],[\"style=filled\",{\"1\":{\"815\":12}}],[\"style\",{\"0\":{\"250\":1},\"1\":{\"208\":1,\"626\":1}}],[\"stn\",{\"1\":{\"154\":3}}],[\"stnkd\",{\"1\":{\"150\":1,\"153\":1,\"154\":1}}],[\"stn3d\",{\"1\":{\"150\":1,\"152\":5,\"153\":1,\"154\":2}}],[\"std=0\",{\"1\":{\"380\":2,\"427\":1,\"428\":2,\"431\":2}}],[\"std=\",{\"1\":{\"359\":1}}],[\"std=imagenet\",{\"1\":{\"264\":1}}],[\"std=torch\",{\"1\":{\"264\":1}}],[\"std\",{\"1\":{\"264\":3,\"293\":1,\"522\":7,\"899\":1,\"931\":5,\"937\":3}}],[\"stderr\",{\"1\":{\"107\":2}}],[\"stdout\",{\"1\":{\"107\":1}}],[\"stratify=y\",{\"1\":{\"513\":1}}],[\"stratify=none\",{\"1\":{\"513\":1}}],[\"stratify\",{\"1\":{\"513\":3}}],[\"straight\",{\"1\":{\"255\":4,\"256\":1,\"257\":2,\"258\":4,\"899\":8,\"959\":1,\"963\":2}}],[\"string\",{\"1\":{\"696\":1}}],[\"strip\",{\"1\":{\"696\":4}}],[\"strict=false\",{\"1\":{\"435\":1}}],[\"strides=\",{\"1\":{\"546\":3}}],[\"strides\",{\"1\":{\"468\":1,\"540\":1,\"542\":4,\"544\":2,\"545\":4,\"546\":2}}],[\"stride\",{\"1\":{\"121\":8,\"123\":2,\"380\":1,\"426\":1,\"469\":1,\"489\":6,\"490\":3,\"492\":1,\"501\":1,\"542\":4,\"545\":3}}],[\"stride=patch\",{\"1\":{\"266\":1,\"380\":1,\"426\":1}}],[\"stride=2\",{\"1\":{\"255\":2,\"892\":1,\"899\":2,\"963\":4}}],[\"stride=2时\",{\"1\":{\"121\":1}}],[\"stride=stride\",{\"1\":{\"123\":5}}],[\"stride=1表示无下采样\",{\"1\":{\"121\":1}}],[\"stride=1\",{\"1\":{\"121\":1,\"123\":1,\"926\":1}}],[\"stream会引入额外的计算量\",{\"1\":{\"391\":1}}],[\"stream的交互方式\",{\"1\":{\"391\":1}}],[\"stream是不对图像和文本concate然后进行交互操作\",{\"1\":{\"391\":1}}],[\"stream是对图像和文本concate然后进行交互操作\",{\"1\":{\"391\":1}}],[\"stream\",{\"1\":{\"391\":2}}],[\"str\",{\"1\":{\"92\":2,\"187\":3,\"208\":1,\"403\":1,\"424\":3,\"474\":2,\"595\":19,\"596\":6,\"597\":44,\"718\":1,\"728\":1,\"808\":3,\"815\":2}}],[\"structural\",{\"1\":{\"87\":1}}],[\"structured\",{\"1\":{\"129\":2}}],[\"structure\",{\"0\":{\"197\":1,\"392\":1},\"1\":{\"11\":1,\"35\":1,\"52\":4,\"141\":1,\"422\":1}}],[\"staats\",{\"1\":{\"667\":1}}],[\"stanford\",{\"1\":{\"634\":1}}],[\"standing\",{\"1\":{\"341\":1,\"408\":2}}],[\"standard\",{\"1\":{\"208\":1}}],[\"stack\",{\"0\":{\"466\":1},\"1\":{\"67\":2,\"190\":3,\"192\":3,\"207\":3,\"274\":1,\"386\":3,\"408\":1,\"419\":3,\"424\":1,\"466\":3,\"667\":1,\"715\":1,\"747\":1}}],[\"status\",{\"1\":{\"894\":1}}],[\"stats\",{\"1\":{\"187\":1,\"265\":1,\"595\":4,\"597\":2}}],[\"static\",{\"1\":{\"681\":2}}],[\"staticmethod\",{\"1\":{\"119\":1,\"121\":1,\"424\":2,\"597\":2}}],[\"statistic\",{\"1\":{\"213\":4}}],[\"statistical\",{\"0\":{\"43\":1}}],[\"state=42\",{\"1\":{\"513\":1}}],[\"state=none\",{\"1\":{\"513\":1}}],[\"state=hidden\",{\"1\":{\"420\":1,\"663\":1}}],[\"state=sequence\",{\"1\":{\"397\":1}}],[\"state\",{\"1\":{\"106\":2,\"107\":1,\"190\":5,\"191\":2,\"192\":4,\"206\":4,\"207\":3,\"293\":2,\"417\":2,\"419\":1,\"435\":1,\"474\":3,\"513\":2,\"681\":1,\"700\":2,\"720\":1,\"963\":2,\"964\":2}}],[\"states=all\",{\"1\":{\"420\":1,\"663\":1}}],[\"states=false\",{\"1\":{\"420\":1}}],[\"states=outputs\",{\"1\":{\"208\":1,\"420\":1}}],[\"states=output\",{\"1\":{\"208\":1,\"420\":1}}],[\"states=encoder\",{\"1\":{\"208\":1,\"397\":1,\"403\":1,\"420\":1}}],[\"states=none\",{\"1\":{\"207\":1,\"208\":2,\"397\":1,\"398\":1,\"399\":1,\"400\":1,\"401\":1,\"403\":1,\"420\":5}}],[\"states=image\",{\"1\":{\"187\":1,\"190\":2,\"192\":3,\"417\":1,\"419\":1,\"420\":1}}],[\"states\",{\"1\":{\"64\":4,\"188\":1,\"191\":1,\"207\":9,\"208\":5,\"385\":11,\"397\":3,\"398\":6,\"399\":3,\"400\":12,\"401\":7,\"402\":1,\"403\":15,\"420\":21,\"421\":1,\"663\":43,\"718\":16,\"719\":4,\"720\":2,\"722\":2,\"724\":4,\"725\":8,\"728\":8,\"729\":6,\"731\":1,\"737\":1}}],[\"startswith\",{\"1\":{\"696\":1}}],[\"start\",{\"1\":{\"53\":4,\"82\":4,\"105\":1,\"119\":11,\"121\":4,\"192\":1,\"265\":3,\"293\":3,\"359\":1,\"380\":5,\"381\":7,\"385\":6,\"410\":5,\"412\":5,\"440\":1,\"461\":2,\"544\":1,\"733\":12,\"734\":15,\"735\":5}}],[\"stabilization\",{\"1\":{\"893\":1}}],[\"stable\",{\"1\":{\"892\":4,\"893\":2,\"898\":2}}],[\"stab\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"stage流程\",{\"1\":{\"416\":1}}],[\"stagewise\",{\"0\":{\"374\":1},\"1\":{\"376\":1}}],[\"stage\",{\"0\":{\"417\":1,\"421\":1},\"1\":{\"11\":1,\"306\":3,\"381\":2,\"382\":4,\"416\":3,\"417\":1,\"421\":1}}],[\"segments\",{\"1\":{\"699\":3}}],[\"segment\",{\"1\":{\"679\":1,\"681\":1,\"692\":1,\"698\":3,\"699\":1,\"700\":2,\"713\":1}}],[\"segmentation\",{\"0\":{\"239\":1},\"1\":{\"7\":1,\"84\":3,\"102\":1,\"106\":1,\"308\":1,\"408\":1,\"716\":1}}],[\"sense\",{\"1\":{\"668\":1}}],[\"sentiment\",{\"1\":{\"634\":1}}],[\"sent\",{\"1\":{\"595\":1,\"596\":1,\"597\":2,\"696\":1,\"697\":4}}],[\"sentences格式\",{\"1\":{\"681\":1}}],[\"sentences\",{\"1\":{\"595\":2,\"597\":2,\"681\":3,\"683\":3,\"696\":7,\"697\":4,\"698\":11}}],[\"sentencepiece\",{\"1\":{\"223\":1,\"224\":1}}],[\"sentence\",{\"0\":{\"692\":1},\"1\":{\"52\":1,\"188\":1,\"420\":1,\"595\":2,\"597\":2,\"640\":3,\"681\":1,\"683\":1,\"690\":1,\"713\":1,\"731\":5}}],[\"seem\",{\"1\":{\"724\":1}}],[\"seed\",{\"1\":{\"424\":1,\"521\":13}}],[\"seen\",{\"1\":{\"44\":1,\"47\":2,\"53\":2,\"82\":2,\"89\":1,\"663\":3,\"805\":3,\"807\":3,\"815\":3}}],[\"separation\",{\"1\":{\"500\":1}}],[\"separated\",{\"1\":{\"597\":1}}],[\"separate\",{\"1\":{\"376\":1}}],[\"sep\",{\"1\":{\"188\":3,\"371\":1,\"421\":1,\"679\":1,\"692\":7,\"694\":1,\"697\":2,\"698\":9,\"699\":1,\"713\":11,\"733\":2,\"735\":2,\"737\":2}}],[\"search输出\",{\"1\":{\"647\":1}}],[\"search扩展\",{\"1\":{\"421\":1}}],[\"search的beam数量\",{\"1\":{\"421\":1}}],[\"searchpicbytext\",{\"1\":{\"411\":1,\"412\":1}}],[\"search\",{\"1\":{\"178\":3,\"188\":2,\"421\":1}}],[\"set避免重复处理节点\",{\"1\":{\"815\":1}}],[\"setitem\",{\"1\":{\"808\":1}}],[\"setattr\",{\"1\":{\"807\":2}}],[\"setr\",{\"1\":{\"239\":1}}],[\"setup\",{\"0\":{\"236\":1},\"1\":{\"381\":1,\"382\":13,\"810\":4}}],[\"sets\",{\"1\":{\"160\":1}}],[\"set\",{\"1\":{\"92\":6,\"133\":1,\"138\":1,\"142\":1,\"143\":4,\"144\":1,\"146\":2,\"150\":1,\"157\":2,\"160\":1,\"382\":7,\"383\":2,\"697\":1,\"783\":2,\"800\":1,\"805\":7,\"807\":5,\"815\":4,\"872\":1,\"892\":1,\"898\":1,\"935\":2}}],[\"setting\",{\"0\":{\"46\":1},\"1\":{\"47\":2,\"53\":2,\"82\":2}}],[\"seq+1\",{\"1\":{\"274\":3}}],[\"seqafford\",{\"0\":{\"128\":1},\"1\":{\"128\":3}}],[\"seq\",{\"1\":{\"69\":1,\"266\":2,\"274\":13,\"384\":3,\"403\":1,\"417\":6,\"418\":5,\"419\":13,\"420\":1,\"477\":1,\"582\":2,\"660\":1,\"663\":5,\"698\":3,\"699\":6,\"703\":10,\"712\":1,\"716\":2,\"724\":2,\"730\":4,\"731\":4,\"733\":4,\"734\":4,\"736\":2,\"737\":4,\"751\":1,\"892\":40,\"893\":15,\"895\":20,\"898\":7,\"899\":2,\"900\":5}}],[\"sequences\",{\"1\":{\"534\":1,\"713\":2,\"742\":1}}],[\"sequences=1\",{\"1\":{\"188\":1}}],[\"sequence\",{\"1\":{\"67\":2,\"208\":2,\"397\":2,\"403\":2,\"420\":2,\"713\":4,\"721\":3,\"730\":2,\"731\":2,\"733\":3,\"734\":2,\"736\":4}}],[\"sequential\",{\"1\":{\"7\":2,\"58\":2,\"59\":3,\"60\":2,\"65\":1,\"66\":1,\"68\":1,\"69\":1,\"70\":1,\"83\":4,\"97\":2,\"119\":2,\"120\":1,\"122\":4,\"123\":5,\"128\":2,\"143\":1,\"213\":2,\"255\":5,\"431\":2,\"892\":1,\"899\":5,\"918\":2,\"964\":1}}],[\"selection\",{\"1\":{\"513\":2}}],[\"selected\",{\"1\":{\"121\":11}}],[\"select\",{\"1\":{\"53\":1,\"190\":2,\"192\":2}}],[\"sele\",{\"1\":{\"53\":2}}],[\"self指代右操作数b\",{\"1\":{\"809\":1}}],[\"self指代左操作数a\",{\"1\":{\"809\":1}}],[\"self和other是两个关键入参\",{\"1\":{\"809\":2}}],[\"selfattention\",{\"1\":{\"582\":1}}],[\"self\",{\"0\":{\"621\":1,\"661\":1,\"662\":1},\"1\":{\"53\":49,\"54\":8,\"56\":11,\"57\":5,\"58\":13,\"59\":29,\"60\":30,\"64\":18,\"65\":32,\"66\":7,\"67\":1,\"68\":6,\"69\":40,\"70\":20,\"82\":45,\"83\":90,\"92\":21,\"94\":10,\"96\":8,\"97\":10,\"98\":10,\"99\":5,\"100\":17,\"102\":9,\"119\":27,\"120\":27,\"121\":21,\"122\":10,\"123\":53,\"137\":17,\"138\":25,\"141\":41,\"145\":9,\"146\":27,\"152\":26,\"154\":24,\"155\":19,\"156\":24,\"171\":2,\"187\":23,\"188\":9,\"190\":59,\"191\":15,\"192\":57,\"205\":35,\"206\":17,\"207\":14,\"208\":14,\"213\":107,\"243\":1,\"255\":15,\"256\":9,\"263\":22,\"264\":11,\"266\":24,\"274\":23,\"279\":2,\"280\":1,\"282\":1,\"283\":1,\"293\":34,\"359\":5,\"361\":13,\"362\":9,\"363\":5,\"364\":6,\"376\":2,\"380\":104,\"382\":122,\"383\":20,\"384\":10,\"385\":28,\"386\":2,\"397\":11,\"398\":5,\"399\":4,\"400\":14,\"401\":14,\"402\":1,\"403\":22,\"417\":9,\"418\":4,\"419\":12,\"420\":49,\"421\":7,\"424\":12,\"426\":12,\"427\":16,\"428\":22,\"429\":25,\"430\":16,\"431\":33,\"455\":1,\"459\":4,\"491\":1,\"522\":7,\"582\":8,\"586\":5,\"587\":3,\"588\":3,\"589\":3,\"590\":5,\"592\":5,\"595\":4,\"596\":13,\"597\":45,\"621\":3,\"663\":31,\"694\":2,\"697\":34,\"699\":24,\"703\":1,\"709\":30,\"710\":14,\"713\":9,\"716\":13,\"718\":26,\"719\":5,\"720\":7,\"721\":11,\"722\":14,\"724\":28,\"725\":9,\"726\":11,\"728\":10,\"729\":9,\"730\":7,\"731\":8,\"733\":3,\"734\":8,\"736\":12,\"737\":9,\"741\":2,\"742\":16,\"743\":5,\"745\":8,\"746\":15,\"747\":7,\"749\":19,\"750\":7,\"751\":15,\"756\":2,\"762\":4,\"766\":1,\"778\":3,\"779\":5,\"780\":2,\"783\":4,\"784\":3,\"787\":2,\"791\":4,\"792\":2,\"794\":4,\"795\":2,\"800\":7,\"801\":5,\"802\":2,\"803\":5,\"805\":19,\"806\":4,\"807\":11,\"808\":19,\"809\":25,\"892\":21,\"893\":32,\"894\":4,\"895\":6,\"898\":12,\"899\":33,\"900\":22,\"918\":10,\"926\":72,\"931\":19,\"937\":21,\"963\":32,\"964\":15}}],[\"seconds\",{\"1\":{\"410\":2,\"412\":1}}],[\"second\",{\"1\":{\"264\":4,\"265\":1,\"274\":2,\"713\":1}}],[\"section\",{\"1\":{\"102\":2,\"680\":3}}],[\"sections=self\",{\"1\":{\"70\":2,\"83\":2}}],[\"sections=spatial\",{\"1\":{\"64\":1}}],[\"sec\",{\"1\":{\"32\":3,\"667\":1}}],[\"semantic\",{\"0\":{\"239\":1},\"1\":{\"14\":1,\"64\":4,\"69\":3,\"143\":1,\"308\":1,\"634\":1}}],[\"lcel\",{\"1\":{\"833\":2,\"834\":1}}],[\"l=4\",{\"1\":{\"710\":1}}],[\"lk\",{\"1\":{\"710\":15}}],[\"lq\",{\"1\":{\"710\":15}}],[\"lpe\",{\"1\":{\"707\":1}}],[\"lstm\",{\"1\":{\"671\":1}}],[\"lstm会掉5\",{\"1\":{\"635\":1}}],[\"lstm表现高方差\",{\"1\":{\"635\":1}}],[\"ltm的过程包含两个阶段\",{\"1\":{\"622\":1}}],[\"ltm的核心思想是\",{\"1\":{\"622\":1}}],[\"ltm\",{\"1\":{\"622\":1}}],[\"l为灰度图片\",{\"1\":{\"424\":1}}],[\"ln\",{\"1\":{\"417\":1,\"421\":1,\"663\":3}}],[\"lffn\",{\"1\":{\"385\":5}}],[\"l+1\",{\"1\":{\"380\":3}}],[\"luggage\",{\"1\":{\"341\":1}}],[\"lucidrains\",{\"1\":{\"254\":1,\"267\":1}}],[\"lgs\",{\"1\":{\"317\":1}}],[\"lxmert\",{\"1\":{\"269\":1}}],[\"lmd\",{\"1\":{\"709\":3}}],[\"lm中同样含有pad部分\",{\"1\":{\"700\":1}}],[\"lm也只包含被掩码的token对应的模型预测真实词\",{\"1\":{\"700\":1}}],[\"lm三个目标进行训练\",{\"1\":{\"192\":1}}],[\"lm\",{\"0\":{\"172\":1},\"1\":{\"172\":2,\"173\":1,\"187\":2,\"192\":6,\"208\":7,\"266\":4,\"268\":1,\"403\":4,\"420\":7,\"640\":1,\"691\":1,\"699\":3,\"700\":6,\"731\":5}}],[\"lmaffordance3d\",{\"0\":{\"61\":1,\"64\":1},\"1\":{\"61\":1,\"64\":1,\"551\":1}}],[\"l0\",{\"1\":{\"146\":8}}],[\"l4\",{\"1\":{\"146\":6}}],[\"l3\",{\"1\":{\"138\":4,\"141\":4,\"146\":9}}],[\"l1loss\",{\"1\":{\"259\":1}}],[\"l1\",{\"0\":{\"259\":1,\"507\":2},\"1\":{\"138\":4,\"141\":4,\"146\":9,\"255\":5,\"259\":4,\"507\":1,\"508\":1,\"899\":6}}],[\"lr是learning\",{\"1\":{\"816\":1}}],[\"lr=opt\",{\"1\":{\"918\":2}}],[\"lr=1e\",{\"1\":{\"700\":1,\"926\":1,\"934\":1,\"938\":1,\"963\":1,\"964\":1}}],[\"lr=6\",{\"1\":{\"633\":1}}],[\"lr=2\",{\"1\":{\"633\":1}}],[\"lr=config\",{\"1\":{\"187\":1,\"190\":1,\"192\":1}}],[\"lr=dict\",{\"1\":{\"104\":1}}],[\"lr\",{\"1\":{\"104\":4,\"107\":1,\"187\":1,\"190\":1,\"192\":1,\"265\":3,\"293\":7,\"359\":1,\"633\":1,\"700\":1,\"816\":6,\"918\":3}}],[\"l×d\",{\"1\":{\"96\":1,\"97\":1}}],[\"lipschitz\",{\"0\":{\"917\":1},\"1\":{\"917\":4}}],[\"library\",{\"1\":{\"810\":1}}],[\"limitations\",{\"1\":{\"658\":1}}],[\"liang\",{\"1\":{\"655\":1}}],[\"lightningdatamodule\",{\"1\":{\"381\":10,\"382\":3}}],[\"lightningmodule\",{\"1\":{\"381\":25,\"383\":1}}],[\"lightning\",{\"1\":{\"379\":1,\"380\":1,\"381\":2}}],[\"lightgroupattnblock\",{\"1\":{\"96\":1}}],[\"liu\",{\"1\":{\"338\":1}}],[\"lite\",{\"1\":{\"823\":2}}],[\"lit\",{\"1\":{\"269\":1}}],[\"lin\",{\"1\":{\"655\":1,\"751\":2}}],[\"lineids\",{\"1\":{\"596\":5}}],[\"line\",{\"1\":{\"596\":9,\"660\":1,\"696\":8,\"815\":1}}],[\"lines\",{\"1\":{\"596\":2}}],[\"line>\",{\"1\":{\"595\":1,\"596\":1,\"597\":2}}],[\"linears\",{\"1\":{\"751\":3}}],[\"linear3\",{\"1\":{\"120\":2}}],[\"linear1\",{\"1\":{\"100\":1,\"120\":2,\"122\":4}}],[\"linear2\",{\"1\":{\"100\":1,\"122\":4}}],[\"linear\",{\"1\":{\"58\":2,\"59\":4,\"60\":2,\"66\":2,\"68\":2,\"69\":5,\"70\":2,\"83\":4,\"97\":4,\"119\":18,\"120\":3,\"121\":7,\"122\":4,\"123\":2,\"138\":3,\"141\":3,\"152\":3,\"155\":3,\"190\":5,\"191\":3,\"192\":5,\"205\":7,\"213\":4,\"215\":1,\"266\":1,\"273\":1,\"320\":1,\"352\":1,\"380\":3,\"397\":1,\"400\":1,\"403\":2,\"429\":2,\"430\":2,\"431\":2,\"522\":21,\"694\":3,\"699\":3,\"709\":4,\"718\":2,\"720\":1,\"722\":1,\"724\":3,\"725\":1,\"728\":1,\"729\":1,\"730\":1,\"733\":1,\"734\":1,\"736\":1,\"737\":1,\"743\":1,\"751\":3,\"819\":1,\"892\":2,\"900\":3,\"918\":5,\"931\":5,\"937\":5}}],[\"linalg\",{\"1\":{\"410\":2,\"412\":2}}],[\"linguistic\",{\"1\":{\"294\":1,\"634\":1}}],[\"linspace\",{\"0\":{\"440\":1},\"1\":{\"106\":2,\"293\":1,\"380\":1,\"440\":2,\"816\":2}}],[\"li\",{\"1\":{\"84\":1,\"171\":1,\"172\":2}}],[\"likert\",{\"1\":{\"656\":1}}],[\"likelihood\",{\"0\":{\"903\":1},\"1\":{\"656\":1,\"852\":2,\"877\":2,\"885\":1,\"921\":1,\"932\":1}}],[\"like\",{\"0\":{\"418\":1,\"420\":1},\"1\":{\"83\":1,\"380\":1,\"384\":2,\"385\":2,\"386\":3,\"710\":1,\"716\":1,\"791\":1,\"801\":1,\"803\":1,\"805\":1,\"807\":1,\"893\":2,\"896\":1,\"931\":1,\"937\":1,\"964\":1}}],[\"lift\",{\"1\":{\"53\":1,\"82\":1,\"86\":1,\"91\":1,\"92\":1}}],[\"lid\",{\"1\":{\"52\":2,\"55\":2}}],[\"lidar\",{\"1\":{\"46\":1}}],[\"liquid\",{\"1\":{\"52\":2,\"55\":2}}],[\"list列表组装起来得到需要的dataset\",{\"1\":{\"713\":1}}],[\"listdir\",{\"1\":{\"410\":1,\"412\":1,\"424\":2}}],[\"listen\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"list\",{\"1\":{\"52\":2,\"53\":13,\"67\":1,\"82\":13,\"94\":1,\"107\":2,\"137\":2,\"141\":15,\"293\":2,\"384\":2,\"385\":6,\"424\":2,\"445\":1,\"482\":1,\"516\":1,\"553\":1,\"556\":1,\"595\":4,\"596\":7,\"597\":27,\"663\":3,\"697\":6,\"816\":6,\"899\":2}}],[\"l\",{\"1\":{\"28\":1,\"69\":5,\"94\":2,\"96\":1,\"97\":2,\"98\":1,\"99\":1,\"100\":12,\"150\":1,\"176\":3,\"177\":1,\"190\":7,\"192\":2,\"205\":1,\"206\":3,\"208\":2,\"214\":3,\"252\":1,\"341\":1,\"342\":1,\"362\":4,\"372\":1,\"380\":4,\"407\":5,\"410\":1,\"656\":1,\"709\":32,\"710\":33,\"712\":3,\"959\":1}}],[\"l2norm\",{\"1\":{\"213\":6}}],[\"l2\",{\"0\":{\"508\":2},\"1\":{\"23\":1,\"138\":4,\"141\":4,\"146\":9,\"213\":4,\"216\":1,\"362\":1,\"385\":2,\"407\":2,\"508\":2,\"633\":1,\"900\":1}}],[\"legend\",{\"1\":{\"816\":1}}],[\"legacy\",{\"1\":{\"663\":8}}],[\"legb\",{\"1\":{\"444\":1}}],[\"le\",{\"1\":{\"641\":1}}],[\"leakyrelu\",{\"1\":{\"918\":3}}],[\"least\",{\"0\":{\"622\":1},\"1\":{\"622\":2}}],[\"learn\",{\"0\":{\"512\":1},\"1\":{\"514\":1}}],[\"learned\",{\"1\":{\"407\":3,\"417\":2}}],[\"learners\",{\"1\":{\"637\":1,\"638\":1,\"644\":1}}],[\"learner\",{\"1\":{\"131\":2}}],[\"learnable\",{\"1\":{\"100\":2,\"306\":2,\"346\":1,\"428\":1,\"707\":1}}],[\"learning领域中如maml\",{\"1\":{\"650\":1}}],[\"learning的核心思想是通过设计合适的prompt\",{\"1\":{\"409\":1}}],[\"learning或prompt\",{\"1\":{\"409\":1}}],[\"learning\",{\"0\":{\"199\":1,\"246\":1,\"249\":1,\"349\":1,\"417\":1,\"418\":1,\"421\":1,\"693\":1},\"1\":{\"18\":2,\"19\":1,\"193\":2,\"195\":1,\"225\":1,\"271\":1,\"273\":1,\"339\":1,\"347\":2,\"368\":1,\"370\":1,\"373\":1,\"413\":2,\"417\":1,\"421\":1,\"602\":2,\"639\":1,\"646\":2,\"647\":3,\"650\":2,\"651\":1,\"654\":1,\"712\":1,\"918\":1}}],[\"let\",{\"1\":{\"619\":1}}],[\"leibler\",{\"1\":{\"202\":1,\"260\":1,\"945\":1}}],[\"len之前\",{\"1\":{\"892\":1}}],[\"len之后\",{\"1\":{\"892\":1}}],[\"lens\",{\"1\":{\"713\":1,\"715\":2}}],[\"len×d\",{\"1\":{\"707\":1}}],[\"len的维度拼接起来\",{\"1\":{\"420\":1}}],[\"len维度上拼接起来\",{\"1\":{\"419\":2}}],[\"len和hidden\",{\"1\":{\"417\":1}}],[\"len=input\",{\"1\":{\"713\":1}}],[\"len=4\",{\"1\":{\"709\":1}}],[\"len=40\",{\"1\":{\"380\":1}}],[\"len=512\",{\"1\":{\"707\":1}}],[\"len=self\",{\"1\":{\"382\":6}}],[\"len=config\",{\"1\":{\"380\":1}}],[\"length长度\",{\"1\":{\"713\":1}}],[\"length=128\",{\"1\":{\"712\":1}}],[\"length=10\",{\"1\":{\"188\":1,\"421\":1}}],[\"length=0\",{\"1\":{\"419\":1,\"420\":2}}],[\"length=25\",{\"1\":{\"204\":1}}],[\"length=35\",{\"1\":{\"190\":1,\"191\":1}}],[\"length=30\",{\"1\":{\"188\":1,\"192\":1,\"421\":1}}],[\"length=min\",{\"1\":{\"188\":2,\"421\":1}}],[\"length=max\",{\"1\":{\"188\":2,\"421\":1}}],[\"length=40\",{\"1\":{\"187\":1}}],[\"length=self\",{\"1\":{\"64\":1,\"417\":1}}],[\"length\",{\"1\":{\"67\":4,\"69\":1,\"187\":2,\"188\":4,\"190\":1,\"191\":1,\"192\":1,\"417\":1,\"419\":5,\"420\":3,\"421\":2,\"663\":1,\"710\":8,\"713\":11,\"715\":1,\"716\":2,\"733\":4,\"737\":4,\"948\":1,\"950\":1}}],[\"len\",{\"1\":{\"64\":1,\"94\":1,\"105\":3,\"107\":2,\"119\":3,\"121\":2,\"137\":1,\"141\":2,\"187\":1,\"188\":1,\"190\":1,\"192\":2,\"204\":1,\"266\":2,\"293\":6,\"380\":6,\"382\":12,\"383\":1,\"384\":3,\"385\":1,\"386\":5,\"403\":1,\"410\":1,\"412\":1,\"417\":7,\"418\":5,\"419\":10,\"420\":1,\"424\":8,\"477\":1,\"518\":2,\"595\":2,\"596\":3,\"597\":4,\"660\":1,\"663\":4,\"696\":2,\"697\":9,\"698\":18,\"699\":9,\"700\":2,\"703\":10,\"709\":11,\"710\":2,\"713\":13,\"715\":5,\"724\":2,\"734\":4,\"736\":2,\"751\":1,\"800\":1,\"805\":1,\"807\":1,\"808\":5,\"892\":31,\"893\":18,\"895\":21,\"898\":7,\"900\":5,\"918\":2,\"926\":1,\"934\":1,\"938\":1}}],[\"left\",{\"1\":{\"64\":1,\"263\":5,\"274\":2,\"477\":1,\"746\":1}}],[\"levels=np\",{\"1\":{\"816\":1}}],[\"level\",{\"1\":{\"6\":1,\"11\":1,\"83\":5,\"142\":1,\"234\":1,\"683\":1}}],[\"lorentz\",{\"1\":{\"868\":1}}],[\"lora微调\",{\"1\":{\"609\":1}}],[\"lora的基本思路\",{\"1\":{\"606\":1}}],[\"lora背后有一个假设\",{\"1\":{\"606\":1}}],[\"lora是跟prompt\",{\"1\":{\"606\":1}}],[\"lora\",{\"0\":{\"606\":1,\"611\":1},\"1\":{\"34\":1,\"607\":1,\"608\":1,\"610\":5,\"611\":9,\"614\":7}}],[\"lovaszsoftmax\",{\"1\":{\"591\":1}}],[\"lovasz\",{\"0\":{\"591\":1},\"1\":{\"591\":1}}],[\"lookahead\",{\"1\":{\"510\":1}}],[\"looking\",{\"1\":{\"408\":1}}],[\"loshchilov\",{\"1\":{\"176\":1}}],[\"loss中的alpha和beta\",{\"1\":{\"593\":1}}],[\"loss等能够处理不平衡情况的损失函数\",{\"1\":{\"593\":1}}],[\"loss或combo\",{\"1\":{\"593\":1}}],[\"loss的设计思想是\",{\"1\":{\"591\":1}}],[\"loss的设计灵感来自tversky指数\",{\"1\":{\"590\":1}}],[\"loss引入了一个衰减因子\",{\"1\":{\"589\":1}}],[\"loss和标准的二元交叉熵\",{\"1\":{\"587\":1}}],[\"loss是将dice\",{\"1\":{\"587\":1}}],[\"loss=lm\",{\"1\":{\"403\":1,\"420\":1}}],[\"loss=false\",{\"1\":{\"274\":1,\"893\":1,\"895\":1}}],[\"loss=masked\",{\"1\":{\"208\":1}}],[\"loss监督\",{\"1\":{\"79\":1}}],[\"loss\",{\"0\":{\"39\":1,\"102\":2,\"259\":1,\"385\":1,\"418\":1,\"419\":1,\"420\":1,\"586\":1,\"587\":1,\"588\":1,\"589\":1,\"590\":1,\"591\":1,\"592\":1,\"910\":1},\"1\":{\"6\":2,\"7\":3,\"11\":2,\"12\":2,\"14\":2,\"24\":1,\"39\":2,\"64\":8,\"79\":1,\"102\":25,\"104\":1,\"105\":8,\"106\":1,\"153\":2,\"172\":3,\"187\":5,\"190\":15,\"192\":20,\"204\":8,\"206\":7,\"207\":1,\"208\":20,\"213\":32,\"255\":8,\"256\":15,\"259\":4,\"265\":10,\"266\":1,\"268\":2,\"274\":19,\"293\":13,\"359\":3,\"373\":1,\"380\":2,\"381\":1,\"382\":1,\"383\":6,\"384\":10,\"385\":20,\"386\":10,\"403\":7,\"407\":8,\"418\":2,\"419\":1,\"420\":10,\"431\":3,\"510\":1,\"586\":10,\"587\":21,\"588\":21,\"589\":38,\"590\":5,\"592\":22,\"656\":3,\"691\":2,\"693\":1,\"700\":23,\"722\":8,\"731\":10,\"734\":10,\"736\":11,\"737\":5,\"892\":2,\"893\":28,\"899\":21,\"900\":4,\"918\":19,\"926\":7,\"932\":2,\"934\":8,\"938\":8,\"959\":1,\"963\":29,\"964\":9}}],[\"loal\",{\"1\":{\"131\":1}}],[\"loaded\",{\"1\":{\"700\":1,\"963\":3,\"964\":3}}],[\"loader\",{\"1\":{\"104\":3,\"105\":2,\"106\":2,\"187\":7,\"190\":8,\"192\":7,\"204\":3,\"265\":4,\"293\":6,\"359\":6,\"382\":12,\"425\":2,\"431\":2,\"918\":1,\"926\":3,\"933\":1,\"934\":2,\"935\":1,\"938\":2,\"963\":2,\"964\":1}}],[\"loading\",{\"1\":{\"359\":1,\"410\":1,\"412\":1}}],[\"load\",{\"1\":{\"52\":2,\"92\":2,\"107\":4,\"213\":3,\"293\":1,\"435\":2,\"474\":1,\"597\":4,\"697\":2,\"700\":2,\"963\":2,\"964\":2}}],[\"logvar\",{\"1\":{\"931\":13,\"932\":3,\"934\":2,\"937\":7,\"938\":2}}],[\"logistic\",{\"1\":{\"921\":1}}],[\"logit\",{\"1\":{\"102\":1,\"213\":1,\"385\":19,\"588\":1,\"589\":1,\"895\":1}}],[\"logits外\",{\"1\":{\"435\":1}}],[\"logits=prediction\",{\"1\":{\"208\":1,\"403\":1,\"420\":1}}],[\"logits=false\",{\"1\":{\"208\":1,\"420\":1,\"435\":1}}],[\"logits\",{\"1\":{\"83\":7,\"105\":1,\"116\":2,\"143\":1,\"208\":7,\"255\":1,\"256\":12,\"257\":7,\"258\":2,\"260\":3,\"274\":9,\"355\":1,\"362\":9,\"383\":2,\"384\":8,\"385\":17,\"386\":8,\"403\":1,\"407\":3,\"419\":2,\"420\":2,\"431\":3,\"435\":3,\"586\":2,\"587\":1,\"590\":2,\"660\":4,\"663\":5,\"699\":6,\"700\":4,\"722\":5,\"733\":12,\"734\":15,\"736\":6,\"737\":6,\"886\":1,\"887\":2,\"892\":24,\"893\":21,\"894\":5,\"895\":8,\"896\":12,\"897\":4,\"898\":18,\"899\":22,\"926\":2,\"964\":5}}],[\"log2\",{\"1\":{\"899\":1}}],[\"logspace\",{\"1\":{\"816\":1}}],[\"logging\",{\"1\":{\"712\":1}}],[\"logger\",{\"1\":{\"106\":2,\"204\":1,\"265\":1,\"381\":1}}],[\"logp\",{\"1\":{\"656\":1}}],[\"log\",{\"0\":{\"261\":1},\"1\":{\"102\":3,\"107\":2,\"138\":1,\"141\":1,\"146\":1,\"155\":2,\"156\":3,\"190\":2,\"192\":2,\"204\":1,\"206\":2,\"208\":1,\"213\":6,\"256\":8,\"260\":8,\"261\":11,\"263\":5,\"265\":3,\"293\":1,\"384\":2,\"385\":7,\"386\":2,\"453\":3,\"455\":2,\"461\":1,\"592\":4,\"656\":2,\"710\":2,\"743\":1,\"899\":8,\"909\":1,\"931\":4,\"932\":2}}],[\"loc\",{\"1\":{\"92\":1}}],[\"location=device\",{\"1\":{\"435\":1,\"700\":1,\"963\":1,\"964\":1}}],[\"location=\",{\"1\":{\"107\":1,\"213\":1}}],[\"locate\",{\"1\":{\"12\":1}}],[\"locality\",{\"1\":{\"422\":1,\"545\":1}}],[\"local\",{\"1\":{\"25\":2,\"131\":3,\"137\":2,\"293\":15,\"373\":1,\"410\":2,\"412\":2,\"444\":3,\"715\":1}}],[\"longterm\",{\"1\":{\"696\":1}}],[\"longtensor\",{\"1\":{\"421\":1,\"663\":2,\"700\":1,\"710\":1}}],[\"long\",{\"1\":{\"67\":1,\"119\":3,\"121\":2,\"122\":1,\"137\":5,\"187\":1,\"188\":1,\"190\":4,\"191\":1,\"192\":4,\"205\":1,\"206\":1,\"207\":2,\"208\":1,\"361\":1,\"362\":1,\"384\":2,\"385\":3,\"386\":1,\"417\":1,\"419\":4,\"420\":1,\"421\":1,\"696\":1,\"710\":2,\"716\":1,\"926\":2,\"939\":1,\"964\":2}}],[\"longest\",{\"1\":{\"64\":1,\"187\":1,\"204\":1}}],[\"lower\",{\"1\":{\"92\":1,\"410\":1,\"412\":1,\"597\":1,\"712\":1}}],[\"low\",{\"1\":{\"52\":1,\"484\":2,\"606\":1,\"608\":1,\"610\":2}}],[\"llama3\",{\"1\":{\"823\":4}}],[\"llama2\",{\"1\":{\"823\":3}}],[\"llama呈现出与同类模型相似的偏见\",{\"1\":{\"670\":1}}],[\"llama通过轻量级指令微调\",{\"1\":{\"669\":1}}],[\"llama的核心成果\",{\"1\":{\"668\":1}}],[\"llama的方法论核心是通过数据质量优化\",{\"1\":{\"667\":1}}],[\"llama采用纯公开数据混合\",{\"1\":{\"667\":1}}],[\"llama仅使用公开数据\",{\"1\":{\"666\":1}}],[\"llama强调推理成本优化而非单纯训练速度\",{\"1\":{\"666\":1}}],[\"llama是一系列高效的基础语言模型\",{\"1\":{\"665\":1}}],[\"llama\",{\"0\":{\"664\":1,\"673\":1},\"1\":{\"305\":1,\"306\":4,\"313\":1,\"315\":2,\"339\":1,\"341\":1,\"342\":1,\"594\":1,\"664\":2,\"665\":1,\"666\":1,\"668\":7,\"669\":1,\"673\":2,\"822\":1,\"823\":16}}],[\"llama系列模型通过高效架构设计和纯公开数据训练\",{\"1\":{\"672\":1}}],[\"llama系列\",{\"1\":{\"299\":1}}],[\"llava模型结构\",{\"1\":{\"341\":1}}],[\"llava\",{\"0\":{\"338\":1},\"1\":{\"7\":1,\"300\":1,\"305\":1,\"310\":2,\"317\":2,\"326\":1,\"334\":1,\"338\":2,\"339\":1,\"341\":1,\"342\":4,\"343\":4,\"346\":1}}],[\"llm就像是一个快思考的系统\",{\"1\":{\"619\":1}}],[\"llm是一个概率模型\",{\"1\":{\"616\":1}}],[\"llm已经这么强了\",{\"1\":{\"616\":1}}],[\"llm友好性\",{\"1\":{\"296\":1}}],[\"llms本身缺乏视觉理解能力\",{\"1\":{\"299\":1}}],[\"llms的参数规模已达千亿级\",{\"1\":{\"296\":1}}],[\"llms\",{\"1\":{\"295\":1,\"296\":1,\"299\":1,\"322\":1,\"323\":1,\"607\":1}}],[\"llm\",{\"0\":{\"824\":1},\"1\":{\"3\":1,\"64\":12,\"66\":3,\"67\":14,\"68\":4,\"69\":1,\"93\":1,\"304\":1,\"305\":4,\"317\":2,\"329\":1,\"346\":1,\"415\":1,\"421\":4,\"822\":7,\"823\":2,\"824\":9,\"825\":6,\"827\":4,\"828\":10,\"830\":2,\"832\":2,\"833\":9,\"834\":1,\"837\":1,\"838\":1}}],[\"langserve\",{\"1\":{\"834\":1}}],[\"langsmith\",{\"1\":{\"833\":1,\"834\":1}}],[\"langchian\",{\"1\":{\"832\":1}}],[\"langchain\",{\"0\":{\"831\":1},\"1\":{\"831\":6,\"832\":1,\"833\":13,\"834\":10,\"836\":3}}],[\"language生成学习\",{\"1\":{\"416\":1}}],[\"language表征学习\",{\"1\":{\"416\":1}}],[\"language\",{\"0\":{\"127\":1,\"200\":1,\"338\":1,\"384\":1,\"691\":1},\"1\":{\"7\":2,\"12\":2,\"61\":1,\"64\":1,\"67\":1,\"70\":1,\"84\":3,\"94\":1,\"106\":1,\"128\":2,\"129\":2,\"164\":4,\"165\":1,\"172\":1,\"179\":2,\"193\":2,\"195\":1,\"200\":1,\"208\":1,\"219\":4,\"220\":1,\"234\":1,\"252\":1,\"305\":2,\"338\":1,\"354\":1,\"366\":2,\"367\":2,\"368\":1,\"370\":1,\"373\":1,\"378\":1,\"380\":1,\"382\":2,\"383\":3,\"384\":1,\"385\":3,\"387\":2,\"393\":2,\"403\":2,\"406\":1,\"409\":1,\"413\":1,\"606\":1,\"607\":1,\"608\":1,\"611\":1,\"620\":1,\"621\":1,\"622\":1,\"624\":1,\"637\":1,\"638\":1,\"640\":1,\"644\":1,\"647\":1,\"652\":1,\"655\":1,\"664\":1,\"681\":1,\"690\":1,\"691\":1,\"696\":1,\"822\":2,\"837\":2}}],[\"law的未来\",{\"1\":{\"837\":1}}],[\"law\",{\"0\":{\"850\":1},\"1\":{\"822\":7,\"837\":1}}],[\"lawrence\",{\"1\":{\"655\":1}}],[\"laws\",{\"1\":{\"646\":1,\"647\":1,\"650\":2}}],[\"lambada去除重叠样本后\",{\"1\":{\"641\":1}}],[\"lambada\",{\"1\":{\"641\":1}}],[\"lambada等\",{\"1\":{\"641\":1}}],[\"lambda\",{\"1\":{\"444\":1,\"746\":1,\"749\":2,\"899\":2,\"900\":1,\"926\":2}}],[\"latency\",{\"1\":{\"611\":1}}],[\"latents\",{\"1\":{\"274\":14,\"900\":15}}],[\"latent\",{\"1\":{\"235\":2,\"274\":2,\"823\":1,\"899\":2,\"900\":14,\"918\":4,\"931\":4,\"935\":1,\"937\":4,\"963\":4,\"964\":14}}],[\"lavis\",{\"1\":{\"414\":1,\"558\":1}}],[\"launchpad\",{\"1\":{\"408\":1}}],[\"laplace\",{\"1\":{\"213\":1}}],[\"laptop\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"laion\",{\"1\":{\"176\":1,\"305\":2,\"332\":1}}],[\"last\",{\"1\":{\"121\":2,\"137\":3,\"141\":3,\"145\":3,\"190\":5,\"191\":2,\"192\":4,\"206\":4,\"207\":2,\"274\":4,\"293\":4,\"397\":1,\"417\":2,\"419\":1,\"420\":1,\"510\":1,\"582\":1,\"663\":1}}],[\"last=true\",{\"1\":{\"104\":1,\"265\":1,\"293\":1,\"359\":1}}],[\"laso\",{\"0\":{\"84\":1},\"1\":{\"19\":2,\"25\":1,\"30\":1,\"31\":1,\"46\":1,\"47\":1,\"84\":4,\"86\":1,\"92\":1,\"93\":1,\"94\":1,\"95\":1,\"100\":1,\"102\":2,\"106\":3,\"107\":2}}],[\"layer=act\",{\"1\":{\"380\":3,\"429\":1,\"431\":1}}],[\"layer=args\",{\"1\":{\"293\":1}}],[\"layer=nn\",{\"1\":{\"380\":2,\"429\":3}}],[\"layer=norm\",{\"1\":{\"380\":1,\"431\":1}}],[\"layer=none\",{\"1\":{\"380\":1,\"426\":1,\"427\":1,\"428\":3,\"431\":3}}],[\"layer=partial\",{\"1\":{\"205\":2}}],[\"layer=false\",{\"1\":{\"190\":2,\"191\":1,\"192\":2,\"403\":1}}],[\"layer=0\",{\"1\":{\"190\":1,\"192\":1}}],[\"layer=config\",{\"1\":{\"187\":1,\"190\":1,\"192\":1}}],[\"layer的任务是通过中心点找到邻居点\",{\"1\":{\"135\":1}}],[\"layers=7\",{\"1\":{\"964\":1}}],[\"layers=8\",{\"1\":{\"926\":2}}],[\"layerscale\",{\"1\":{\"380\":6}}],[\"layers组成\",{\"1\":{\"133\":1}}],[\"layers主要包括3个部分\",{\"1\":{\"132\":1}}],[\"layers\",{\"1\":{\"123\":8,\"133\":1,\"143\":2,\"239\":1,\"255\":14,\"274\":2,\"306\":1,\"385\":2,\"398\":2,\"420\":1,\"699\":3,\"700\":1,\"719\":1,\"724\":1,\"747\":3,\"750\":2,\"892\":2,\"899\":17,\"918\":4,\"926\":3,\"964\":6}}],[\"layer\",{\"0\":{\"134\":1,\"135\":1,\"136\":1},\"1\":{\"96\":1,\"99\":2,\"119\":6,\"132\":3,\"133\":6,\"144\":6,\"187\":4,\"190\":2,\"191\":2,\"192\":2,\"207\":7,\"213\":10,\"255\":1,\"293\":3,\"380\":23,\"385\":6,\"398\":5,\"400\":1,\"401\":17,\"403\":1,\"420\":36,\"426\":3,\"427\":2,\"428\":1,\"429\":4,\"431\":8,\"663\":3,\"699\":3,\"716\":1,\"718\":3,\"719\":4,\"724\":21,\"725\":1,\"728\":1,\"741\":1,\"747\":6,\"750\":6,\"819\":1}}],[\"layer指的是pointnet++中提供的pointnetfeaturepropagation特征传播类\",{\"1\":{\"94\":1}}],[\"layer指的是pointnet++中提供的pointnetsetabstractionmsg多尺度分组点集特征抽取类\",{\"1\":{\"94\":1}}],[\"layernorm也有可学习的偏置\",{\"1\":{\"522\":1}}],[\"layernorm也有偏置参数\",{\"1\":{\"522\":1}}],[\"layernorm\",{\"1\":{\"56\":2,\"60\":2,\"69\":6,\"97\":2,\"205\":2,\"274\":1,\"286\":1,\"380\":5,\"384\":1,\"385\":1,\"400\":3,\"402\":1,\"403\":3,\"419\":1,\"429\":1,\"431\":2,\"493\":2,\"522\":3,\"633\":1,\"716\":2,\"718\":2,\"725\":2,\"728\":2,\"745\":2,\"747\":1,\"750\":1,\"892\":2}}],[\"lay\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"label=\",{\"1\":{\"815\":12,\"816\":2}}],[\"label=none\",{\"1\":{\"731\":1}}],[\"label=label\",{\"1\":{\"713\":1}}],[\"labels=labels\",{\"1\":{\"420\":1}}],[\"labels=none\",{\"1\":{\"208\":2,\"274\":1,\"403\":1,\"420\":1,\"722\":1,\"731\":1,\"736\":1,\"737\":1}}],[\"labels=decoder\",{\"1\":{\"187\":1,\"192\":1}}],[\"labels\",{\"1\":{\"86\":1,\"143\":2,\"144\":1,\"190\":2,\"192\":2,\"207\":2,\"208\":15,\"265\":1,\"274\":8,\"280\":1,\"285\":1,\"362\":3,\"382\":1,\"384\":11,\"385\":10,\"386\":9,\"403\":4,\"407\":3,\"408\":1,\"419\":2,\"420\":8,\"424\":4,\"431\":3,\"516\":2,\"518\":4,\"713\":1,\"715\":2,\"722\":8,\"731\":2,\"734\":3,\"736\":11,\"737\":2,\"893\":3,\"900\":3,\"939\":2}}],[\"label\",{\"1\":{\"53\":9,\"64\":3,\"82\":10,\"88\":1,\"105\":1,\"106\":4,\"204\":6,\"206\":3,\"207\":1,\"208\":8,\"293\":1,\"403\":1,\"408\":1,\"409\":2,\"418\":2,\"420\":1,\"424\":8,\"425\":4,\"510\":1,\"516\":5,\"518\":4,\"589\":1,\"700\":2,\"713\":2,\"731\":2,\"937\":5}}],[\"largest\",{\"1\":{\"488\":1}}],[\"largest=true\",{\"1\":{\"488\":1}}],[\"largest=false\",{\"1\":{\"119\":1,\"488\":1}}],[\"large\",{\"0\":{\"338\":1},\"1\":{\"6\":1,\"7\":2,\"128\":2,\"129\":2,\"192\":1,\"272\":1,\"338\":1,\"410\":2,\"412\":1,\"413\":1,\"606\":1,\"608\":1,\"620\":1,\"622\":1,\"640\":1,\"681\":1,\"683\":1,\"710\":5,\"822\":1,\"837\":2}}],[\"omni\",{\"1\":{\"823\":1}}],[\"o指定输出文件名\",{\"1\":{\"815\":1}}],[\"old\",{\"1\":{\"807\":2}}],[\"our\",{\"1\":{\"734\":1}}],[\"out2\",{\"1\":{\"899\":1}}],[\"out1\",{\"1\":{\"899\":1}}],[\"outfile\",{\"1\":{\"595\":6,\"597\":6}}],[\"out=none\",{\"1\":{\"466\":1,\"481\":1,\"488\":1}}],[\"out=hidden\",{\"1\":{\"255\":2}}],[\"outer\",{\"1\":{\"447\":1,\"448\":4}}],[\"outside\",{\"1\":{\"341\":1,\"734\":1}}],[\"out\",{\"1\":{\"52\":3,\"59\":2,\"64\":5,\"70\":5,\"83\":3,\"97\":5,\"119\":12,\"121\":10,\"122\":10,\"137\":7,\"141\":4,\"145\":4,\"213\":6,\"255\":2,\"256\":4,\"293\":19,\"429\":5,\"466\":1,\"480\":8,\"481\":1,\"503\":2,\"504\":2,\"505\":2,\"522\":7,\"594\":1,\"595\":5,\"597\":5,\"640\":1,\"656\":2,\"657\":1,\"660\":6,\"709\":4,\"724\":1,\"893\":4,\"895\":10,\"899\":8,\"918\":3,\"926\":3,\"963\":2}}],[\"output=pooled\",{\"1\":{\"397\":1}}],[\"outputs\",{\"1\":{\"188\":3,\"208\":3,\"265\":1,\"381\":7,\"400\":2,\"403\":2,\"420\":26,\"421\":2,\"660\":2,\"663\":3,\"712\":1,\"721\":2,\"722\":7,\"726\":2,\"731\":6,\"733\":4,\"734\":8,\"736\":6,\"737\":6,\"800\":7,\"801\":1,\"803\":1,\"805\":8,\"806\":4,\"807\":9,\"815\":1}}],[\"output\",{\"1\":{\"7\":1,\"52\":1,\"60\":2,\"83\":3,\"100\":1,\"119\":2,\"144\":1,\"187\":2,\"188\":2,\"190\":10,\"191\":6,\"192\":12,\"206\":6,\"207\":7,\"208\":11,\"293\":17,\"359\":3,\"361\":1,\"362\":1,\"381\":4,\"383\":2,\"397\":11,\"399\":4,\"400\":1,\"402\":1,\"403\":2,\"417\":5,\"419\":4,\"420\":24,\"471\":1,\"477\":1,\"478\":1,\"498\":1,\"522\":1,\"529\":1,\"538\":1,\"540\":1,\"582\":1,\"586\":1,\"589\":1,\"660\":2,\"663\":20,\"694\":10,\"696\":8,\"699\":9,\"712\":3,\"718\":9,\"720\":4,\"721\":7,\"722\":4,\"724\":2,\"726\":4,\"729\":2,\"730\":4,\"731\":4,\"733\":5,\"734\":2,\"736\":4,\"737\":4,\"762\":2,\"779\":4,\"783\":1,\"787\":1,\"800\":2,\"801\":2,\"803\":2,\"805\":4,\"806\":4,\"807\":6,\"815\":5,\"892\":2,\"898\":4,\"926\":8}}],[\"oov\",{\"1\":{\"594\":1,\"683\":1}}],[\"oord\",{\"1\":{\"216\":1}}],[\"o5\",{\"1\":{\"123\":4}}],[\"o4\",{\"1\":{\"123\":5}}],[\"o3\",{\"1\":{\"123\":5,\"823\":3,\"924\":4}}],[\"o3d\",{\"1\":{\"107\":9}}],[\"o0\",{\"1\":{\"123\":4}}],[\"o2\",{\"1\":{\"122\":2,\"123\":5,\"924\":4}}],[\"o1\",{\"1\":{\"122\":2,\"123\":4,\"822\":1,\"823\":12,\"924\":4}}],[\"oa\",{\"1\":{\"117\":1}}],[\"occurred\",{\"1\":{\"597\":1}}],[\"occurrences\",{\"1\":{\"597\":1}}],[\"occlusion\",{\"1\":{\"12\":1}}],[\"ocr任务\",{\"1\":{\"336\":1}}],[\"ocr相关任务\",{\"1\":{\"335\":1}}],[\"ocr数据\",{\"1\":{\"332\":1}}],[\"ocr能力和高分辨率处理能力\",{\"1\":{\"330\":1}}],[\"ocr\",{\"1\":{\"305\":1,\"310\":1,\"322\":2,\"332\":1}}],[\"octnet\",{\"1\":{\"110\":1}}],[\"os\",{\"0\":{\"520\":1},\"1\":{\"92\":3,\"107\":3,\"187\":1,\"410\":11,\"411\":2,\"412\":14,\"424\":8,\"520\":2,\"582\":1,\"696\":4,\"697\":1,\"700\":1,\"815\":6,\"918\":3}}],[\"opus\",{\"1\":{\"823\":3}}],[\"op\",{\"1\":{\"809\":3}}],[\"option\",{\"1\":{\"107\":1}}],[\"optional\",{\"1\":{\"67\":1,\"100\":6,\"208\":1,\"474\":1,\"480\":2,\"663\":10,\"895\":1}}],[\"optimize\",{\"1\":{\"892\":3}}],[\"optimized\",{\"1\":{\"676\":1,\"682\":1}}],[\"optimizers\",{\"1\":{\"381\":1,\"918\":1}}],[\"optimizer\",{\"1\":{\"104\":3,\"105\":2,\"106\":2,\"187\":5,\"190\":5,\"192\":5,\"204\":3,\"265\":4,\"293\":4,\"359\":6,\"381\":5,\"431\":1,\"700\":2,\"819\":1,\"918\":6,\"926\":3,\"934\":3,\"938\":3,\"963\":3,\"964\":3}}],[\"optimizing\",{\"1\":{\"605\":1}}],[\"optimization算法进一步优化模型行为\",{\"1\":{\"654\":1}}],[\"optimization\",{\"1\":{\"339\":2,\"656\":1}}],[\"optim\",{\"1\":{\"104\":3,\"187\":1,\"190\":1,\"192\":1,\"265\":1,\"293\":1,\"359\":1,\"510\":1,\"918\":2,\"926\":3,\"930\":2,\"934\":1,\"938\":1,\"963\":1,\"964\":1}}],[\"opt\",{\"1\":{\"104\":2,\"105\":1,\"107\":3,\"671\":1,\"918\":15}}],[\"operations\",{\"1\":{\"478\":1}}],[\"operating\",{\"0\":{\"569\":1},\"1\":{\"106\":1}}],[\"operability\",{\"1\":{\"7\":1}}],[\"openwebtext2\",{\"1\":{\"823\":1}}],[\"openwebtext等\",{\"1\":{\"684\":1}}],[\"openwebtext\",{\"1\":{\"679\":1,\"680\":1}}],[\"openwebtext3\",{\"1\":{\"224\":1}}],[\"openbookqa\",{\"1\":{\"668\":1}}],[\"openclip\",{\"1\":{\"308\":1,\"309\":1}}],[\"opengvlab\",{\"1\":{\"294\":1}}],[\"open3d\",{\"1\":{\"107\":1}}],[\"openai首先尝试了virtex模型\",{\"1\":{\"413\":1}}],[\"openai从网络上收集了4亿条数据进行实验\",{\"1\":{\"413\":1}}],[\"openai从网络上收集了总计4亿对文本和图像\",{\"1\":{\"407\":1}}],[\"openai\",{\"1\":{\"254\":1,\"325\":1,\"339\":2,\"405\":1,\"410\":2,\"412\":1,\"614\":1,\"656\":3,\"658\":4,\"822\":1,\"823\":10,\"831\":1}}],[\"openaccess\",{\"1\":{\"84\":1}}],[\"openad\",{\"1\":{\"31\":1}}],[\"opening\",{\"1\":{\"52\":2,\"102\":1,\"411\":1,\"412\":1}}],[\"openimage\",{\"1\":{\"41\":1}}],[\"openkd\",{\"1\":{\"31\":1}}],[\"open\",{\"1\":{\"11\":1,\"20\":1,\"28\":1,\"30\":1,\"31\":1,\"52\":2,\"53\":2,\"55\":2,\"82\":2,\"86\":1,\"87\":1,\"91\":1,\"92\":3,\"107\":2,\"187\":1,\"321\":2,\"385\":2,\"410\":1,\"411\":1,\"412\":2,\"424\":2,\"444\":1,\"595\":4,\"596\":1,\"597\":7,\"658\":1,\"660\":20,\"664\":1,\"673\":1,\"696\":3,\"697\":3,\"815\":2}}],[\"oq\",{\"1\":{\"56\":1}}],[\"o\",{\"1\":{\"54\":5,\"57\":5,\"59\":3,\"98\":2,\"99\":1,\"119\":6,\"120\":6,\"121\":22,\"122\":8,\"157\":2,\"709\":2,\"815\":2,\"816\":1,\"823\":1,\"832\":1,\"924\":4}}],[\"od\",{\"1\":{\"53\":3}}],[\"ok=true\",{\"1\":{\"696\":2,\"918\":2}}],[\"okvqa\",{\"1\":{\"311\":1}}],[\"ok\",{\"1\":{\"53\":4,\"56\":20,\"57\":2}}],[\"onnx\",{\"1\":{\"545\":1}}],[\"only=true\",{\"1\":{\"382\":1,\"700\":1}}],[\"only=image\",{\"1\":{\"382\":2}}],[\"only=false\",{\"1\":{\"382\":2}}],[\"only=self\",{\"1\":{\"382\":4}}],[\"only\",{\"1\":{\"370\":2,\"372\":4,\"374\":5,\"380\":1,\"382\":10,\"385\":1,\"397\":1,\"421\":1,\"713\":1,\"729\":1,\"735\":2,\"736\":1,\"823\":5,\"887\":1}}],[\"on\",{\"0\":{\"941\":1},\"1\":{\"52\":2,\"55\":2,\"84\":3,\"87\":1,\"106\":1,\"107\":2,\"381\":16,\"408\":3,\"597\":1,\"751\":1,\"816\":1,\"941\":2}}],[\"onecycle\",{\"1\":{\"510\":1}}],[\"ones\",{\"1\":{\"67\":8,\"137\":1,\"187\":1,\"188\":1,\"190\":3,\"191\":1,\"192\":3,\"205\":1,\"206\":1,\"207\":1,\"293\":1,\"380\":3,\"386\":1,\"417\":1,\"419\":3,\"420\":1,\"421\":1,\"791\":1,\"801\":1,\"803\":1,\"805\":1,\"807\":3,\"964\":1}}],[\"one\",{\"1\":{\"52\":1,\"190\":2,\"199\":1,\"200\":1,\"201\":1,\"202\":3,\"204\":3,\"213\":2,\"256\":4,\"257\":7,\"258\":6,\"265\":3,\"271\":1,\"359\":2,\"431\":1,\"646\":1,\"647\":1,\"648\":1,\"650\":1,\"700\":2,\"857\":1,\"877\":1,\"899\":11,\"910\":1,\"938\":1,\"939\":1,\"959\":1}}],[\"once\",{\"1\":{\"6\":1,\"663\":1}}],[\"ov\",{\"1\":{\"56\":1}}],[\"overwrite\",{\"1\":{\"712\":1}}],[\"overall\",{\"1\":{\"106\":2}}],[\"over\",{\"0\":{\"588\":1},\"1\":{\"46\":1,\"106\":3,\"588\":2,\"866\":1}}],[\"ovag旨在通过额外指令\",{\"1\":{\"31\":1}}],[\"ovag\",{\"1\":{\"30\":1,\"31\":1}}],[\"or门组合\",{\"1\":{\"500\":1}}],[\"ordered\",{\"1\":{\"595\":3,\"597\":3}}],[\"ordereddict\",{\"1\":{\"431\":1}}],[\"order=\",{\"1\":{\"542\":1}}],[\"order\",{\"1\":{\"381\":1,\"447\":1,\"541\":2,\"542\":1,\"597\":1,\"918\":2}}],[\"orthogonal\",{\"1\":{\"153\":1}}],[\"original\",{\"1\":{\"143\":1,\"435\":1,\"477\":2,\"545\":1,\"724\":1,\"935\":1}}],[\"or\",{\"1\":{\"64\":1,\"70\":2,\"83\":2,\"149\":1,\"207\":1,\"263\":1,\"380\":2,\"387\":2,\"407\":2,\"429\":2,\"430\":1,\"431\":2,\"697\":1,\"712\":1,\"718\":1,\"728\":1,\"898\":1,\"899\":1}}],[\"org\",{\"1\":{\"28\":1,\"61\":1,\"71\":1,\"130\":1,\"147\":1,\"294\":1,\"338\":1,\"366\":1,\"414\":1,\"435\":1,\"899\":1}}],[\"oracle\",{\"0\":{\"2\":1},\"1\":{\"0\":1}}],[\"obj\",{\"1\":{\"53\":4,\"69\":7,\"82\":9,\"83\":27,\"460\":4,\"806\":4,\"809\":4}}],[\"objcot\",{\"1\":{\"48\":2}}],[\"objaverse\",{\"1\":{\"41\":1}}],[\"objective\",{\"1\":{\"656\":1}}],[\"objectives\",{\"0\":{\"198\":1,\"393\":1},\"1\":{\"383\":7,\"386\":1}}],[\"objects\",{\"1\":{\"11\":1,\"12\":1,\"49\":1,\"92\":3,\"107\":3}}],[\"object\",{\"0\":{\"35\":1},\"1\":{\"11\":4,\"12\":2,\"28\":1,\"30\":1,\"35\":1,\"37\":1,\"43\":2,\"44\":1,\"47\":1,\"52\":16,\"53\":9,\"54\":2,\"56\":1,\"57\":2,\"61\":1,\"71\":1,\"82\":9,\"83\":1,\"84\":3,\"91\":1,\"92\":4,\"106\":2,\"264\":1,\"293\":1,\"589\":1,\"815\":1}}],[\"observation\",{\"1\":{\"12\":1,\"877\":1}}],[\"observations\",{\"1\":{\"12\":1,\"61\":1}}],[\"observed\",{\"1\":{\"12\":1}}],[\"other指代左操作数a\",{\"1\":{\"809\":1}}],[\"other指代右操作数b\",{\"1\":{\"809\":1}}],[\"other\",{\"1\":{\"11\":1,\"809\":7,\"921\":1}}],[\"official\",{\"1\":{\"435\":1}}],[\"off\",{\"1\":{\"411\":1,\"412\":1,\"935\":3,\"939\":1,\"964\":1}}],[\"offsetted\",{\"1\":{\"893\":2}}],[\"offset\",{\"1\":{\"119\":17,\"121\":11,\"122\":6,\"544\":2,\"893\":4}}],[\"of\",{\"0\":{\"33\":1,\"52\":1,\"171\":1,\"380\":1,\"620\":1,\"850\":1},\"1\":{\"11\":6,\"12\":2,\"14\":3,\"31\":1,\"50\":1,\"52\":16,\"55\":2,\"83\":2,\"107\":3,\"137\":1,\"165\":1,\"183\":1,\"187\":2,\"188\":3,\"190\":1,\"226\":2,\"293\":1,\"341\":2,\"359\":1,\"361\":2,\"362\":3,\"363\":1,\"367\":2,\"368\":1,\"372\":2,\"378\":1,\"382\":2,\"385\":1,\"407\":4,\"408\":6,\"409\":3,\"410\":2,\"411\":1,\"412\":2,\"413\":3,\"420\":1,\"424\":1,\"502\":1,\"534\":1,\"594\":1,\"597\":23,\"604\":1,\"606\":1,\"607\":2,\"608\":1,\"620\":2,\"621\":1,\"634\":1,\"681\":1,\"690\":1,\"703\":1,\"713\":1,\"735\":2,\"736\":1,\"747\":1,\"749\":1,\"751\":1,\"825\":1,\"837\":1,\"877\":1,\"893\":1,\"899\":2,\"918\":12}}],[\"ae离图像生成只差一步了\",{\"1\":{\"956\":1}}],[\"ae不能够随机生成图片\",{\"1\":{\"956\":1}}],[\"ae的编码器编码出来的向量空间是不规整的\",{\"1\":{\"956\":1}}],[\"ae可不可以用来做图像生成呢\",{\"1\":{\"956\":1}}],[\"ae包含一个编码器\",{\"1\":{\"956\":1}}],[\"ae是一类能够把图片压缩成较短的向量的神经网络模型\",{\"1\":{\"956\":1}}],[\"ae\",{\"0\":{\"956\":1},\"1\":{\"956\":1,\"959\":2,\"963\":1}}],[\"axs\",{\"1\":{\"935\":10,\"939\":3}}],[\"axial\",{\"1\":{\"892\":2}}],[\"axialpositionalembedding\",{\"1\":{\"892\":1}}],[\"axis\",{\"1\":{\"411\":1,\"412\":1,\"935\":3,\"939\":1,\"964\":1}}],[\"axis=1\",{\"1\":{\"107\":1,\"407\":3,\"410\":3,\"412\":3,\"441\":2}}],[\"axis=0\",{\"1\":{\"107\":1,\"407\":1,\"441\":3}}],[\"a型和b型掩码\",{\"1\":{\"926\":1}}],[\"a类\",{\"1\":{\"923\":1}}],[\"a3b\",{\"1\":{\"823\":1}}],[\"a为m\",{\"1\":{\"606\":1}}],[\"a13\",{\"1\":{\"544\":2}}],[\"a12\",{\"1\":{\"544\":3}}],[\"a11\",{\"1\":{\"544\":4}}],[\"a10\",{\"1\":{\"544\":2}}],[\"a100\",{\"1\":{\"315\":1,\"316\":1}}],[\"a03\",{\"1\":{\"544\":2}}],[\"a02\",{\"1\":{\"544\":2}}],[\"a01\",{\"1\":{\"544\":2}}],[\"a00\",{\"1\":{\"544\":3}}],[\"a=2\",{\"1\":{\"516\":1}}],[\"a23\",{\"1\":{\"544\":2}}],[\"a22b\",{\"1\":{\"823\":1}}],[\"a22\",{\"1\":{\"544\":3}}],[\"a21\",{\"1\":{\"544\":3}}],[\"a20\",{\"1\":{\"544\":2}}],[\"a2\",{\"1\":{\"441\":3}}],[\"apt\",{\"1\":{\"815\":1}}],[\"ape\",{\"0\":{\"705\":1}}],[\"apache许可项目\",{\"1\":{\"667\":1}}],[\"api的调用\",{\"1\":{\"838\":1}}],[\"api记录之训练细节篇\",{\"0\":{\"519\":1},\"1\":{\"519\":1}}],[\"api记录之框架篇\",{\"0\":{\"509\":1},\"1\":{\"509\":1}}],[\"api记录之杂类篇\",{\"0\":{\"495\":1},\"1\":{\"495\":1}}],[\"api记录之pytorch篇\",{\"0\":{\"464\":1},\"1\":{\"464\":1}}],[\"api记录之python篇\",{\"0\":{\"442\":1},\"1\":{\"442\":1}}],[\"api记录之numpy篇\",{\"0\":{\"438\":1},\"1\":{\"438\":1}}],[\"api\",{\"1\":{\"339\":1,\"475\":1,\"510\":1,\"655\":1,\"656\":3,\"657\":2,\"658\":4,\"823\":1,\"831\":2,\"835\":1}}],[\"applied\",{\"1\":{\"751\":1}}],[\"apple\",{\"1\":{\"691\":5}}],[\"apply\",{\"1\":{\"213\":2,\"362\":1,\"380\":1,\"431\":1,\"724\":1,\"751\":2}}],[\"approach\",{\"1\":{\"676\":1,\"682\":1}}],[\"append\",{\"1\":{\"53\":3,\"67\":3,\"82\":3,\"83\":2,\"92\":1,\"106\":1,\"121\":1,\"122\":1,\"123\":4,\"137\":2,\"141\":5,\"145\":2,\"188\":1,\"190\":3,\"192\":3,\"207\":3,\"255\":6,\"293\":1,\"385\":2,\"386\":3,\"410\":3,\"412\":3,\"419\":3,\"424\":5,\"596\":2,\"597\":3,\"697\":6,\"698\":4,\"713\":1,\"787\":1,\"801\":1,\"803\":1,\"805\":1,\"807\":1,\"815\":2,\"816\":2,\"899\":5,\"918\":2,\"964\":4}}],[\"ai\",{\"0\":{\"840\":1},\"1\":{\"342\":1,\"602\":1,\"658\":3,\"660\":38,\"696\":2,\"823\":4,\"826\":2,\"835\":4}}],[\"ai2d科学图表任务表现接近商业模型\",{\"1\":{\"335\":1}}],[\"aiou\",{\"1\":{\"46\":1,\"47\":1,\"48\":2,\"106\":3}}],[\"amazonaws\",{\"1\":{\"696\":2}}],[\"american\",{\"1\":{\"408\":1}}],[\"amortized\",{\"1\":{\"268\":1}}],[\"among\",{\"1\":{\"149\":1}}],[\"amp\",{\"1\":{\"213\":1,\"265\":1}}],[\"a^t||\",{\"1\":{\"150\":1}}],[\"across\",{\"1\":{\"418\":2}}],[\"accu\",{\"1\":{\"431\":1}}],[\"accuracy\",{\"0\":{\"562\":1},\"1\":{\"384\":2,\"385\":8,\"386\":3,\"410\":8,\"412\":6,\"701\":2}}],[\"acc\",{\"1\":{\"384\":2,\"385\":8,\"386\":2,\"410\":2,\"412\":2}}],[\"accelerator\",{\"1\":{\"381\":1}}],[\"acceptability语言可接受性语料库\",{\"1\":{\"634\":1}}],[\"accept\",{\"1\":{\"274\":1}}],[\"account\",{\"1\":{\"12\":1}}],[\"act2fn\",{\"1\":{\"403\":1,\"718\":1,\"728\":1}}],[\"act\",{\"1\":{\"380\":4,\"403\":6,\"428\":1,\"429\":6,\"431\":5,\"718\":7,\"728\":7}}],[\"active\",{\"1\":{\"736\":8}}],[\"activate\",{\"1\":{\"551\":2,\"554\":1,\"557\":1,\"558\":2,\"712\":1,\"739\":1}}],[\"activation\",{\"1\":{\"100\":1,\"120\":1,\"397\":2,\"720\":2}}],[\"action\",{\"1\":{\"273\":1}}],[\"actually\",{\"1\":{\"724\":1}}],[\"actual\",{\"1\":{\"119\":6,\"410\":2,\"412\":2,\"488\":1}}],[\"after\",{\"1\":{\"381\":1,\"449\":1,\"545\":2,\"893\":1}}],[\"afm\",{\"0\":{\"95\":1,\"99\":1},\"1\":{\"94\":1,\"95\":3,\"98\":1,\"99\":5,\"100\":1}}],[\"aff\",{\"1\":{\"105\":1,\"106\":1,\"107\":2}}],[\"aff2idx\",{\"1\":{\"92\":2}}],[\"afford\",{\"1\":{\"92\":2}}],[\"affordq\",{\"1\":{\"92\":1,\"104\":3}}],[\"affordance=dict\",{\"1\":{\"107\":1}}],[\"affordancenet\",{\"1\":{\"41\":1,\"86\":2,\"87\":1,\"91\":1,\"93\":1}}],[\"affordances\",{\"1\":{\"12\":1,\"20\":1,\"49\":1}}],[\"affordance\",{\"0\":{\"4\":1,\"33\":1,\"36\":1,\"52\":1},\"1\":{\"4\":1,\"6\":2,\"11\":3,\"12\":2,\"14\":1,\"18\":2,\"19\":1,\"26\":1,\"28\":1,\"29\":1,\"30\":1,\"31\":2,\"37\":1,\"40\":1,\"43\":1,\"44\":1,\"47\":4,\"48\":3,\"49\":2,\"50\":1,\"52\":1,\"53\":18,\"54\":1,\"57\":1,\"61\":1,\"64\":4,\"69\":5,\"70\":12,\"71\":1,\"72\":2,\"73\":1,\"82\":15,\"83\":43,\"84\":3,\"86\":1,\"87\":1,\"91\":1,\"92\":15,\"94\":1,\"100\":6,\"104\":2,\"106\":3,\"107\":14,\"128\":2}}],[\"affcot\",{\"1\":{\"48\":2}}],[\"ability\",{\"1\":{\"918\":2}}],[\"about\",{\"1\":{\"408\":1}}],[\"ablation\",{\"0\":{\"48\":1,\"180\":1,\"242\":1,\"344\":1,\"376\":1},\"1\":{\"179\":1,\"344\":1}}],[\"abstract\",{\"0\":{\"352\":1}}],[\"abstraction\",{\"1\":{\"132\":1,\"133\":2,\"138\":1,\"142\":1,\"143\":4,\"144\":1,\"146\":2}}],[\"absolute\",{\"0\":{\"705\":1},\"1\":{\"46\":1,\"106\":4,\"419\":1}}],[\"abs\",{\"1\":{\"28\":1,\"61\":1,\"71\":1,\"83\":1,\"102\":4,\"130\":1,\"147\":1,\"294\":1,\"338\":1,\"380\":4,\"414\":1,\"435\":1,\"698\":1,\"710\":1,\"899\":1}}],[\"averaging\",{\"1\":{\"886\":1}}],[\"average=true\",{\"1\":{\"586\":1,\"587\":1,\"588\":1,\"589\":1,\"590\":1,\"592\":1}}],[\"average=none\",{\"1\":{\"208\":1}}],[\"average\",{\"1\":{\"46\":1,\"106\":1,\"150\":1,\"160\":1,\"202\":1,\"238\":1,\"502\":2,\"586\":3,\"587\":2,\"588\":2,\"589\":2,\"590\":4,\"592\":4,\"900\":1}}],[\"avg\",{\"1\":{\"213\":3,\"700\":5}}],[\"available\",{\"1\":{\"6\":1,\"7\":2,\"11\":2,\"12\":1,\"410\":1,\"412\":1,\"521\":1,\"918\":1,\"926\":2,\"934\":1,\"938\":1}}],[\"augmented\",{\"0\":{\"828\":1},\"1\":{\"828\":1,\"837\":1}}],[\"augmentations\",{\"1\":{\"293\":1}}],[\"augmentation\",{\"0\":{\"169\":1},\"1\":{\"280\":1,\"359\":2}}],[\"autograd\",{\"1\":{\"918\":1}}],[\"automatic\",{\"1\":{\"274\":1}}],[\"automodel\",{\"1\":{\"52\":1}}],[\"autoencoders\",{\"0\":{\"941\":1},\"1\":{\"941\":2,\"951\":1}}],[\"autoencoder\",{\"1\":{\"235\":1,\"956\":1}}],[\"auto\",{\"1\":{\"234\":1,\"827\":1,\"953\":1}}],[\"autocast\",{\"1\":{\"64\":1,\"213\":1,\"265\":1}}],[\"autotokenizer\",{\"1\":{\"52\":1}}],[\"autoregressive\",{\"1\":{\"7\":1}}],[\"auc\",{\"0\":{\"568\":1,\"570\":1,\"572\":1},\"1\":{\"46\":1,\"47\":1,\"99\":1,\"106\":23,\"569\":1,\"570\":6,\"571\":1,\"572\":3}}],[\"adjacent\",{\"1\":{\"698\":14}}],[\"adversarial\",{\"1\":{\"658\":2,\"918\":6}}],[\"admin\",{\"1\":{\"461\":2}}],[\"ade20k分割\",{\"1\":{\"296\":1}}],[\"ade20k\",{\"1\":{\"215\":2,\"220\":1,\"242\":1,\"308\":1}}],[\"adamp\",{\"1\":{\"510\":1}}],[\"adamw\",{\"1\":{\"176\":1,\"187\":2,\"190\":1,\"192\":1,\"224\":1,\"286\":1,\"293\":1,\"315\":1,\"316\":1,\"318\":1,\"667\":1,\"700\":1}}],[\"adam\",{\"1\":{\"46\":1,\"104\":1,\"236\":1,\"633\":1,\"679\":1,\"680\":2,\"886\":1,\"887\":1,\"918\":5,\"926\":1,\"934\":1,\"938\":1,\"963\":1,\"964\":1}}],[\"adaptation\",{\"1\":{\"606\":1,\"608\":1}}],[\"adaptiveavgpool1d\",{\"1\":{\"60\":1,\"70\":1,\"83\":1}}],[\"adaptive\",{\"0\":{\"38\":1},\"1\":{\"94\":1}}],[\"adapter\",{\"1\":{\"34\":2,\"64\":2,\"66\":1,\"68\":1,\"610\":2}}],[\"addition\",{\"1\":{\"848\":1}}],[\"additional\",{\"1\":{\"52\":2,\"59\":1,\"70\":2,\"83\":2,\"179\":1}}],[\"additivity\",{\"1\":{\"848\":1}}],[\"add等操作节点\",{\"1\":{\"815\":1}}],[\"added\",{\"1\":{\"597\":1}}],[\"add\",{\"0\":{\"487\":1},\"1\":{\"25\":2,\"107\":1,\"190\":2,\"191\":1,\"192\":2,\"213\":3,\"293\":4,\"403\":1,\"452\":2,\"487\":1,\"722\":1,\"803\":1,\"804\":1,\"805\":4,\"807\":6,\"809\":16,\"810\":4,\"811\":1,\"815\":6,\"918\":10}}],[\"agents\",{\"1\":{\"832\":1}}],[\"agent\",{\"0\":{\"840\":1},\"1\":{\"823\":1,\"827\":1,\"833\":1}}],[\"agreement\",{\"1\":{\"656\":1}}],[\"agnostic\",{\"1\":{\"646\":1}}],[\"agnostic模型优于那些为每个任务精心设计的模型\",{\"1\":{\"625\":1,\"626\":1}}],[\"aggregate=false\",{\"1\":{\"385\":1}}],[\"aggregate=true\",{\"1\":{\"385\":3}}],[\"aggregate\",{\"1\":{\"385\":1,\"418\":2}}],[\"aggregation\",{\"1\":{\"23\":1}}],[\"agi\",{\"1\":{\"296\":1,\"323\":1,\"827\":3}}],[\"agd20k\",{\"1\":{\"41\":1}}],[\"agpil\",{\"1\":{\"12\":1}}],[\"ascii=false\",{\"1\":{\"595\":3,\"597\":3,\"696\":2}}],[\"astronaut\",{\"1\":{\"408\":1}}],[\"astype\",{\"1\":{\"92\":2,\"106\":2,\"107\":4,\"152\":1}}],[\"assume\",{\"1\":{\"751\":1}}],[\"assistant\",{\"0\":{\"338\":1},\"1\":{\"338\":1,\"341\":1,\"342\":2}}],[\"assignments\",{\"1\":{\"283\":2}}],[\"asserttrue\",{\"1\":{\"795\":1}}],[\"assertequal\",{\"1\":{\"794\":2}}],[\"assert\",{\"1\":{\"119\":2,\"121\":1,\"122\":1,\"274\":2,\"364\":1,\"380\":1,\"382\":2,\"424\":1,\"697\":1,\"709\":1,\"751\":1,\"893\":2,\"895\":2,\"899\":3,\"900\":1,\"926\":1,\"964\":1}}],[\"aspect\",{\"1\":{\"263\":15}}],[\"aspect=none\",{\"1\":{\"263\":1}}],[\"aspect=0\",{\"1\":{\"263\":1}}],[\"as\",{\"1\":{\"12\":1,\"92\":2,\"102\":2,\"107\":4,\"213\":1,\"219\":2,\"266\":1,\"359\":1,\"380\":1,\"382\":1,\"410\":2,\"411\":1,\"412\":5,\"424\":2,\"429\":1,\"430\":1,\"440\":1,\"441\":1,\"444\":1,\"513\":1,\"514\":1,\"542\":1,\"595\":3,\"597\":6,\"696\":3,\"697\":3,\"709\":1,\"710\":1,\"716\":1,\"729\":1,\"757\":1,\"800\":1,\"805\":1,\"807\":1,\"809\":6,\"810\":4,\"815\":5,\"816\":3,\"827\":1,\"918\":4,\"926\":3,\"930\":4,\"964\":1}}],[\"art\",{\"1\":{\"681\":1}}],[\"arc挑战集\",{\"1\":{\"668\":1}}],[\"arc等\",{\"1\":{\"668\":1}}],[\"arch=\",{\"1\":{\"293\":1}}],[\"arch\",{\"1\":{\"293\":3,\"359\":1}}],[\"architecture\",{\"1\":{\"6\":1,\"7\":1,\"95\":1,\"97\":1}}],[\"arr=attentions\",{\"1\":{\"582\":1}}],[\"arr\",{\"1\":{\"440\":2}}],[\"arrays\",{\"1\":{\"513\":2}}],[\"array\",{\"1\":{\"92\":2,\"106\":1,\"107\":3,\"152\":1,\"410\":2,\"412\":2,\"440\":1,\"441\":4,\"513\":1,\"514\":1,\"542\":1,\"757\":2,\"766\":1,\"781\":1,\"794\":4,\"800\":1,\"803\":1,\"804\":1,\"805\":1,\"807\":4,\"808\":7,\"809\":12,\"810\":2,\"811\":6,\"815\":4,\"816\":6}}],[\"are\",{\"1\":{\"267\":2,\"321\":2,\"597\":1,\"637\":1,\"638\":1,\"644\":1,\"658\":1,\"722\":2,\"729\":1,\"734\":1}}],[\"area\",{\"1\":{\"11\":2,\"46\":1,\"102\":2,\"106\":3,\"263\":6,\"503\":1,\"589\":1}}],[\"arange\",{\"1\":{\"137\":3,\"274\":1,\"385\":1,\"407\":1,\"418\":1,\"419\":1,\"482\":2,\"513\":1,\"542\":2,\"663\":1,\"709\":5,\"710\":4,\"716\":1,\"892\":4,\"893\":2,\"898\":2,\"900\":3,\"939\":1}}],[\"argparse\",{\"1\":{\"918\":2}}],[\"argumentparser\",{\"1\":{\"918\":1}}],[\"argument\",{\"1\":{\"293\":3,\"445\":2,\"918\":10}}],[\"argmin\",{\"1\":{\"213\":1,\"958\":1,\"963\":1}}],[\"argmax\",{\"1\":{\"121\":1,\"207\":1,\"258\":1,\"410\":1,\"411\":1,\"412\":2,\"660\":1,\"663\":1,\"733\":2,\"887\":1,\"897\":4,\"899\":2}}],[\"argsort\",{\"1\":{\"582\":1}}],[\"args\",{\"1\":{\"67\":1,\"69\":2,\"70\":2,\"107\":4,\"120\":2,\"121\":3,\"122\":3,\"123\":2,\"187\":2,\"190\":1,\"192\":1,\"213\":5,\"264\":7,\"265\":6,\"293\":29,\"359\":13,\"380\":2,\"382\":7,\"427\":1,\"435\":1,\"449\":2,\"452\":3,\"453\":2,\"454\":4,\"455\":2,\"459\":2,\"460\":2,\"461\":7,\"521\":5,\"582\":8,\"597\":11,\"715\":1,\"800\":1,\"894\":4,\"918\":1,\"926\":2,\"964\":2}}],[\"ar\",{\"1\":{\"73\":1,\"309\":1}}],[\"arm模块\",{\"1\":{\"78\":1,\"80\":1}}],[\"arm\",{\"1\":{\"72\":1,\"83\":6}}],[\"around\",{\"1\":{\"52\":2,\"55\":2}}],[\"arxiv\",{\"1\":{\"28\":1,\"61\":1,\"71\":1,\"130\":1,\"147\":1,\"294\":1,\"338\":1,\"414\":1,\"435\":1,\"667\":1,\"899\":1}}],[\"anthropic\",{\"1\":{\"823\":3}}],[\"anaconda\",{\"1\":{\"557\":1}}],[\"anaconda3\",{\"1\":{\"553\":3}}],[\"analysis\",{\"0\":{\"43\":1,\"49\":1}}],[\"analogical\",{\"1\":{\"36\":1,\"52\":2}}],[\"analogous\",{\"1\":{\"11\":1}}],[\"answer\",{\"1\":{\"735\":3}}],[\"answering\",{\"1\":{\"383\":1,\"640\":1}}],[\"answers\",{\"1\":{\"11\":2}}],[\"anchor\",{\"1\":{\"353\":1}}],[\"ann\",{\"1\":{\"187\":5}}],[\"annotated\",{\"1\":{\"739\":5}}],[\"annotator\",{\"1\":{\"656\":1}}],[\"annotation\",{\"0\":{\"42\":1},\"1\":{\"187\":1}}],[\"annotations\",{\"1\":{\"11\":1}}],[\"anno\",{\"1\":{\"92\":9,\"107\":5}}],[\"any\",{\"1\":{\"122\":1}}],[\"angles\",{\"1\":{\"12\":1}}],[\"an\",{\"1\":{\"11\":1,\"97\":1,\"408\":1,\"729\":1}}],[\"and\",{\"0\":{\"37\":1,\"39\":1,\"175\":1,\"338\":1,\"364\":1,\"670\":1,\"856\":1,\"857\":1},\"1\":{\"6\":1,\"7\":1,\"11\":1,\"12\":4,\"14\":2,\"52\":6,\"61\":1,\"92\":1,\"102\":1,\"104\":2,\"119\":4,\"122\":2,\"124\":1,\"137\":8,\"164\":2,\"165\":1,\"173\":1,\"174\":1,\"179\":1,\"190\":1,\"192\":1,\"193\":2,\"206\":1,\"213\":1,\"219\":2,\"263\":1,\"274\":2,\"293\":4,\"294\":1,\"338\":1,\"359\":3,\"361\":1,\"362\":2,\"364\":2,\"366\":2,\"380\":1,\"382\":1,\"387\":2,\"408\":1,\"409\":1,\"420\":1,\"429\":1,\"435\":1,\"492\":1,\"500\":1,\"545\":1,\"597\":2,\"607\":1,\"655\":5,\"660\":9,\"663\":1,\"664\":1,\"673\":1,\"696\":2,\"698\":1,\"713\":1,\"718\":1,\"722\":1,\"724\":1,\"728\":1,\"731\":1,\"733\":1,\"734\":1,\"742\":2,\"747\":1,\"749\":1,\"751\":2,\"811\":2,\"815\":1,\"893\":3,\"899\":2,\"918\":1,\"921\":1,\"937\":1}}],[\"always\",{\"1\":{\"751\":1,\"892\":2}}],[\"already\",{\"1\":{\"713\":1}}],[\"alice\",{\"1\":{\"459\":1}}],[\"align=\",{\"1\":{\"424\":1}}],[\"aligning\",{\"1\":{\"294\":1,\"658\":1}}],[\"aligned\",{\"1\":{\"83\":4,\"407\":2}}],[\"align^t\",{\"1\":{\"83\":1}}],[\"align进行映射\",{\"1\":{\"83\":1}}],[\"align对齐方法需要\",{\"1\":{\"83\":1}}],[\"align映射为4\",{\"1\":{\"83\":1}}],[\"align技术\",{\"1\":{\"83\":4}}],[\"align获取物体\",{\"1\":{\"78\":1}}],[\"align\",{\"0\":{\"502\":1},\"1\":{\"70\":2,\"83\":12,\"193\":2,\"269\":2,\"272\":1,\"368\":1,\"369\":1,\"501\":1,\"503\":2}}],[\"alignment\",{\"1\":{\"6\":1,\"11\":1,\"14\":1,\"23\":1,\"26\":1,\"83\":2,\"340\":1,\"393\":1,\"654\":1,\"655\":2,\"656\":1,\"657\":2,\"658\":1}}],[\"alexnet\",{\"1\":{\"410\":1}}],[\"also\",{\"1\":{\"274\":1}}],[\"alt\",{\"1\":{\"272\":1}}],[\"al\",{\"0\":{\"709\":1},\"1\":{\"171\":5,\"172\":2,\"176\":4,\"214\":1,\"640\":3,\"655\":17,\"656\":2,\"658\":2,\"667\":1,\"669\":1,\"671\":10}}],[\"alpha+beta=1\",{\"1\":{\"590\":1}}],[\"alpha=beta=1\",{\"1\":{\"590\":1}}],[\"alpha=beta=0\",{\"1\":{\"590\":1}}],[\"alpha=0\",{\"1\":{\"206\":1,\"208\":1}}],[\"alpha=alpha\",{\"1\":{\"190\":1,\"589\":1,\"590\":1,\"592\":1}}],[\"alpha\",{\"1\":{\"102\":3,\"190\":9,\"192\":10,\"204\":13,\"206\":6,\"208\":4,\"213\":1,\"589\":8,\"590\":4,\"592\":7,\"893\":3}}],[\"albef\",{\"0\":{\"193\":1,\"196\":1},\"1\":{\"17\":1,\"193\":1,\"194\":2,\"195\":2,\"197\":1,\"198\":1,\"201\":1,\"202\":2,\"204\":1,\"205\":1,\"206\":1,\"207\":3,\"208\":1,\"269\":4,\"369\":2,\"373\":1,\"376\":1}}],[\"allclose\",{\"1\":{\"795\":1}}],[\"allenai\",{\"1\":{\"366\":1}}],[\"all=false\",{\"1\":{\"138\":2,\"146\":4}}],[\"all=true\",{\"1\":{\"137\":1,\"138\":1}}],[\"all流程图\",{\"1\":{\"137\":1}}],[\"all\",{\"1\":{\"6\":1,\"97\":1,\"137\":8,\"190\":26,\"192\":14,\"206\":8,\"207\":8,\"213\":4,\"219\":2,\"265\":2,\"266\":2,\"274\":1,\"364\":1,\"385\":9,\"386\":11,\"401\":1,\"410\":3,\"411\":1,\"412\":3,\"418\":2,\"419\":12,\"420\":1,\"491\":1,\"516\":2,\"521\":2,\"597\":4,\"696\":4,\"697\":2,\"713\":5,\"715\":16,\"724\":6,\"751\":3,\"964\":6}}],[\"a\",{\"1\":{\"6\":1,\"7\":2,\"14\":1,\"41\":1,\"48\":1,\"52\":4,\"73\":1,\"76\":1,\"83\":1,\"115\":1,\"150\":1,\"187\":2,\"188\":2,\"190\":3,\"192\":2,\"219\":2,\"222\":1,\"303\":1,\"308\":1,\"341\":2,\"362\":2,\"366\":2,\"376\":1,\"390\":1,\"408\":16,\"409\":3,\"410\":1,\"411\":2,\"412\":3,\"415\":2,\"441\":2,\"444\":1,\"452\":2,\"453\":2,\"542\":2,\"546\":4,\"572\":2,\"574\":1,\"592\":1,\"597\":4,\"607\":2,\"611\":3,\"630\":1,\"640\":1,\"641\":1,\"656\":1,\"660\":20,\"663\":1,\"676\":1,\"697\":6,\"698\":18,\"699\":1,\"703\":1,\"710\":1,\"712\":3,\"713\":4,\"724\":1,\"737\":1,\"747\":1,\"751\":2,\"766\":4,\"781\":3,\"804\":3,\"806\":6,\"809\":10,\"815\":1,\"822\":1,\"827\":1,\"837\":2,\"846\":2,\"886\":2,\"899\":1,\"918\":1,\"923\":7,\"924\":5,\"926\":6,\"944\":1,\"949\":2,\"964\":3}}],[\"attempt\",{\"1\":{\"263\":1}}],[\"attenion\",{\"1\":{\"745\":1}}],[\"attend\",{\"1\":{\"724\":1,\"740\":1}}],[\"atten1\",{\"1\":{\"60\":2}}],[\"atten\",{\"1\":{\"56\":8,\"57\":3,\"69\":10,\"83\":8}}],[\"attention可以用矩阵乘法一次计算所有的时刻\",{\"1\":{\"740\":1}}],[\"attention机制\",{\"1\":{\"740\":1}}],[\"attention=false\",{\"1\":{\"663\":1}}],[\"attention运算过程中维度变换的理解\",{\"0\":{\"523\":1},\"1\":{\"523\":1}}],[\"attention的heads数\",{\"1\":{\"432\":1}}],[\"attention的输入\",{\"1\":{\"421\":1}}],[\"attention计算\",{\"1\":{\"419\":1}}],[\"attention模块\",{\"1\":{\"417\":1}}],[\"attentional\",{\"1\":{\"272\":2,\"273\":1,\"635\":1}}],[\"attentions=all\",{\"1\":{\"420\":2,\"663\":2}}],[\"attentions=outputs\",{\"1\":{\"208\":1,\"420\":2}}],[\"attentions=output\",{\"1\":{\"208\":1,\"420\":3}}],[\"attentions=none\",{\"1\":{\"208\":1,\"420\":1}}],[\"attentions=false\",{\"1\":{\"207\":1,\"420\":3}}],[\"attentions\",{\"1\":{\"100\":1,\"208\":2,\"420\":9,\"582\":10,\"663\":2,\"722\":1,\"724\":2,\"731\":1,\"737\":1}}],[\"attention\",{\"0\":{\"400\":1,\"661\":1,\"662\":1},\"1\":{\"56\":1,\"57\":2,\"60\":1,\"64\":7,\"65\":9,\"67\":12,\"69\":14,\"70\":1,\"83\":1,\"99\":2,\"119\":3,\"124\":1,\"125\":1,\"157\":3,\"160\":1,\"171\":3,\"187\":3,\"188\":3,\"190\":11,\"191\":5,\"192\":15,\"197\":1,\"206\":5,\"207\":17,\"208\":14,\"243\":2,\"268\":3,\"272\":5,\"274\":2,\"286\":1,\"306\":2,\"376\":2,\"380\":7,\"384\":3,\"385\":2,\"397\":7,\"398\":5,\"399\":12,\"400\":5,\"401\":21,\"402\":4,\"403\":5,\"417\":3,\"418\":3,\"419\":10,\"420\":68,\"421\":1,\"429\":2,\"430\":2,\"472\":1,\"524\":1,\"529\":1,\"533\":1,\"534\":1,\"582\":2,\"611\":1,\"661\":1,\"663\":7,\"694\":2,\"710\":9,\"713\":6,\"715\":4,\"718\":7,\"719\":2,\"721\":8,\"722\":3,\"724\":26,\"726\":4,\"731\":2,\"733\":1,\"734\":2,\"736\":4,\"737\":6,\"741\":2,\"751\":4,\"823\":3,\"892\":6}}],[\"attn有4\",{\"1\":{\"634\":1}}],[\"attn\",{\"1\":{\"96\":1,\"98\":1,\"100\":4,\"208\":1,\"274\":14,\"380\":20,\"420\":3,\"428\":1,\"429\":4,\"430\":10,\"431\":2,\"582\":17,\"663\":18,\"703\":4,\"709\":2,\"710\":3,\"746\":4,\"749\":10,\"751\":7,\"892\":12}}],[\"atts=none\",{\"1\":{\"67\":1}}],[\"atts\",{\"1\":{\"64\":2,\"67\":12,\"187\":2,\"188\":2,\"190\":13,\"191\":2,\"192\":14,\"206\":2,\"207\":12,\"208\":2,\"417\":2,\"419\":11,\"420\":4,\"421\":2}}],[\"at\",{\"1\":{\"6\":1,\"52\":2,\"107\":1,\"143\":2,\"274\":1,\"364\":1,\"408\":1}}],[\"+max\",{\"1\":{\"710\":1}}],[\"+512\",{\"1\":{\"710\":1}}],[\"+5\",{\"1\":{\"669\":1}}],[\"+9\",{\"1\":{\"669\":1}}],[\"+1\",{\"1\":{\"596\":2,\"597\":2,\"892\":1}}],[\"+i\",{\"1\":{\"192\":1}}],[\"+qid\",{\"1\":{\"92\":1}}],[\"+0\",{\"1\":{\"83\":9}}],[\"+=\",{\"1\":{\"53\":1,\"82\":1,\"106\":3,\"120\":1,\"121\":2,\"122\":1,\"263\":2,\"293\":2,\"410\":1,\"412\":1,\"420\":1,\"431\":2,\"595\":6,\"596\":2,\"597\":5,\"660\":1,\"698\":1,\"700\":1,\"709\":2,\"710\":3,\"815\":7,\"893\":3,\"898\":1,\"900\":2,\"926\":1,\"934\":1,\"938\":1,\"963\":2,\"964\":1}}],[\"+\",{\"0\":{\"5\":1,\"8\":1,\"10\":2,\"814\":2,\"932\":1},\"1\":{\"11\":4,\"24\":1,\"52\":4,\"53\":1,\"56\":2,\"60\":3,\"64\":4,\"67\":2,\"69\":9,\"70\":7,\"79\":1,\"82\":1,\"83\":10,\"96\":1,\"97\":2,\"98\":3,\"99\":1,\"100\":4,\"102\":15,\"105\":1,\"106\":3,\"107\":1,\"110\":2,\"113\":1,\"114\":1,\"117\":1,\"119\":12,\"120\":3,\"121\":7,\"122\":6,\"123\":3,\"137\":2,\"138\":2,\"141\":6,\"143\":1,\"144\":1,\"145\":8,\"150\":1,\"152\":2,\"156\":1,\"157\":3,\"160\":1,\"161\":1,\"162\":1,\"167\":1,\"190\":20,\"192\":10,\"204\":2,\"206\":8,\"207\":5,\"208\":3,\"213\":5,\"214\":1,\"215\":1,\"224\":2,\"255\":3,\"256\":2,\"257\":1,\"258\":4,\"260\":1,\"263\":4,\"266\":2,\"268\":1,\"274\":5,\"280\":2,\"293\":15,\"305\":1,\"310\":1,\"312\":2,\"332\":1,\"339\":1,\"342\":1,\"363\":1,\"364\":2,\"376\":2,\"380\":13,\"384\":2,\"385\":7,\"386\":9,\"397\":1,\"400\":1,\"401\":2,\"402\":7,\"403\":2,\"407\":1,\"408\":1,\"410\":2,\"412\":2,\"415\":1,\"418\":1,\"419\":4,\"420\":6,\"424\":1,\"428\":2,\"429\":2,\"430\":11,\"431\":2,\"449\":1,\"450\":1,\"452\":1,\"502\":4,\"522\":17,\"544\":1,\"561\":4,\"586\":3,\"587\":8,\"588\":4,\"590\":6,\"592\":6,\"595\":4,\"596\":5,\"597\":5,\"606\":2,\"634\":2,\"650\":1,\"656\":1,\"657\":2,\"658\":1,\"661\":2,\"663\":11,\"680\":1,\"684\":1,\"697\":2,\"698\":21,\"699\":5,\"700\":6,\"706\":1,\"709\":1,\"710\":4,\"713\":21,\"716\":4,\"718\":1,\"722\":2,\"724\":3,\"725\":2,\"729\":1,\"731\":2,\"734\":2,\"735\":1,\"736\":1,\"737\":6,\"745\":4,\"746\":1,\"749\":4,\"750\":1,\"771\":1,\"775\":2,\"803\":2,\"805\":2,\"807\":1,\"808\":1,\"809\":14,\"811\":14,\"815\":16,\"816\":3,\"819\":1,\"835\":1,\"836\":2,\"877\":1,\"892\":7,\"893\":9,\"894\":1,\"895\":3,\"897\":1,\"898\":2,\"899\":8,\"900\":1,\"918\":3,\"924\":3,\"926\":3,\"931\":1,\"932\":2,\"937\":3,\"947\":1,\"959\":1,\"963\":4,\"964\":1}}],[\"gc可处理开发者未显式解决的循环引用\",{\"1\":{\"806\":1}}],[\"gc可能无法及时释放内存\",{\"1\":{\"806\":1}}],[\"gc作为兜底机制\",{\"1\":{\"806\":1}}],[\"gc与弱引用的互补关系\",{\"1\":{\"806\":1}}],[\"gc需要扫描整个对象图来检测循环引用\",{\"1\":{\"806\":1}}],[\"gc是一种后台机制\",{\"1\":{\"806\":1}}],[\"gc\",{\"1\":{\"806\":2}}],[\"gcc\",{\"1\":{\"382\":1}}],[\"gx1\",{\"1\":{\"809\":2}}],[\"gx0\",{\"1\":{\"809\":2}}],[\"gxs\",{\"1\":{\"801\":6,\"803\":5,\"805\":5,\"807\":5}}],[\"gx\",{\"1\":{\"780\":2,\"801\":2,\"803\":3,\"805\":3,\"807\":3,\"809\":2}}],[\"gys\",{\"1\":{\"800\":1,\"801\":2,\"803\":2,\"805\":2,\"806\":1,\"807\":2}}],[\"gy\",{\"1\":{\"779\":1,\"780\":2,\"809\":14}}],[\"gsm8k\",{\"1\":{\"668\":1}}],[\"gmm\",{\"1\":{\"574\":2}}],[\"gqa\",{\"1\":{\"310\":1,\"311\":1,\"823\":3}}],[\"gutenberg\",{\"1\":{\"667\":1}}],[\"gumbelnoise\",{\"1\":{\"258\":1}}],[\"gumbel\",{\"0\":{\"257\":1,\"897\":1},\"1\":{\"216\":1,\"232\":1,\"255\":1,\"256\":2,\"257\":7,\"258\":4,\"886\":1,\"887\":1,\"895\":3,\"897\":9,\"898\":3,\"899\":5}}],[\"guidance\",{\"0\":{\"894\":1},\"1\":{\"894\":1}}],[\"guid\",{\"1\":{\"713\":1}}],[\"guide\",{\"1\":{\"102\":1,\"607\":1}}],[\"guided\",{\"1\":{\"84\":3,\"106\":1}}],[\"guitar\",{\"1\":{\"53\":1}}],[\"gva\",{\"1\":{\"125\":3}}],[\"gt\",{\"0\":{\"88\":1},\"1\":{\"88\":2,\"92\":2,\"99\":5,\"102\":1,\"105\":2,\"106\":2,\"586\":1,\"587\":2,\"588\":2}}],[\"gp\",{\"1\":{\"574\":1,\"919\":1}}],[\"gpblock\",{\"1\":{\"99\":1}}],[\"gpb\",{\"1\":{\"94\":3}}],[\"gpt系列\",{\"1\":{\"671\":1}}],[\"gpt2pretrainedmodel\",{\"1\":{\"663\":1}}],[\"gpt2model\",{\"1\":{\"663\":1}}],[\"gpt2attention\",{\"1\":{\"663\":3}}],[\"gpt2block\",{\"1\":{\"663\":3}}],[\"gpt2config\",{\"1\":{\"663\":2}}],[\"gpt2\",{\"1\":{\"660\":2,\"663\":5}}],[\"gpt2tokenizer\",{\"1\":{\"660\":2,\"663\":2}}],[\"gpt2lmheadmodel\",{\"1\":{\"660\":2,\"663\":2}}],[\"gpt和bert通过不同训练目标\",{\"1\":{\"687\":1}}],[\"gpt和bert\",{\"1\":{\"353\":1}}],[\"gpt\",{\"0\":{\"343\":2,\"420\":1,\"624\":1,\"637\":1,\"644\":1},\"1\":{\"87\":3,\"268\":1,\"280\":1,\"321\":2,\"322\":2,\"325\":1,\"339\":5,\"342\":2,\"343\":6,\"594\":1,\"611\":2,\"614\":2,\"624\":1,\"637\":1,\"638\":1,\"639\":1,\"640\":10,\"641\":8,\"642\":1,\"644\":1,\"645\":1,\"646\":3,\"647\":6,\"648\":13,\"649\":7,\"650\":11,\"651\":1,\"655\":2,\"656\":2,\"657\":14,\"658\":2,\"667\":1,\"668\":2,\"671\":1,\"735\":2,\"822\":8,\"823\":21,\"825\":1,\"827\":1}}],[\"gpu上的矩阵运算都是充分优化和高度并行的\",{\"1\":{\"740\":1}}],[\"gpu上推理时\",{\"1\":{\"668\":1}}],[\"gpu上训练12天\",{\"1\":{\"407\":1}}],[\"gpu上训练18天\",{\"1\":{\"407\":1}}],[\"gpu服务器上训练vit\",{\"1\":{\"291\":1}}],[\"gpu节点\",{\"1\":{\"176\":1}}],[\"gpu\",{\"1\":{\"64\":1,\"157\":1,\"190\":3,\"223\":1,\"236\":1,\"265\":1,\"280\":1,\"286\":1,\"291\":1,\"315\":1,\"316\":1,\"353\":1,\"359\":1,\"364\":3,\"373\":1,\"376\":2,\"382\":1,\"386\":5,\"486\":3,\"492\":1,\"520\":7,\"521\":4,\"614\":1,\"667\":1,\"680\":1,\"712\":2,\"740\":1,\"824\":1}}],[\"g\",{\"1\":{\"64\":1,\"96\":1,\"97\":1,\"150\":1,\"298\":1,\"303\":1,\"304\":1,\"308\":2,\"309\":3,\"815\":6,\"918\":8,\"924\":9}}],[\"glm4\",{\"1\":{\"823\":1}}],[\"glm系列模型是\",{\"1\":{\"823\":1}}],[\"glm\",{\"1\":{\"822\":1,\"823\":5}}],[\"gleu\",{\"1\":{\"718\":2}}],[\"glove\",{\"1\":{\"650\":1}}],[\"global\",{\"1\":{\"25\":2,\"122\":4,\"152\":1,\"154\":8,\"155\":1,\"156\":1,\"157\":1,\"293\":10,\"373\":1,\"376\":1,\"381\":1,\"383\":1,\"386\":1,\"444\":2}}],[\"glue基准\",{\"1\":{\"685\":1}}],[\"glue多任务提升5\",{\"1\":{\"626\":1}}],[\"glue\",{\"1\":{\"311\":1,\"678\":1,\"680\":1}}],[\"glasses\",{\"1\":{\"53\":1}}],[\"gibbs\",{\"1\":{\"909\":1}}],[\"gibson\",{\"1\":{\"73\":1}}],[\"git\",{\"1\":{\"712\":2,\"739\":1}}],[\"github\",{\"1\":{\"28\":1,\"61\":1,\"71\":1,\"84\":1,\"108\":1,\"124\":1,\"128\":1,\"129\":1,\"130\":2,\"147\":2,\"164\":1,\"185\":1,\"193\":1,\"209\":1,\"219\":1,\"226\":1,\"267\":1,\"279\":1,\"294\":1,\"338\":1,\"347\":1,\"367\":1,\"378\":1,\"385\":1,\"387\":1,\"414\":1,\"424\":1,\"435\":2,\"582\":1,\"591\":1,\"663\":1,\"666\":1,\"667\":1,\"689\":1,\"712\":1,\"739\":1,\"752\":1,\"798\":1,\"813\":1,\"818\":1}}],[\"gi\",{\"1\":{\"393\":3}}],[\"giraffe\",{\"1\":{\"393\":1}}],[\"gid\",{\"1\":{\"293\":4}}],[\"giant\",{\"1\":{\"224\":2,\"272\":1}}],[\"given\",{\"1\":{\"597\":4}}],[\"give\",{\"1\":{\"52\":1}}],[\"gick\",{\"1\":{\"30\":1}}],[\"goldstein\",{\"1\":{\"811\":3,\"815\":3}}],[\"golang\",{\"1\":{\"2\":1}}],[\"goodfellow\",{\"1\":{\"918\":1}}],[\"good\",{\"1\":{\"696\":1}}],[\"googleapis\",{\"1\":{\"712\":1}}],[\"google\",{\"1\":{\"28\":1,\"71\":1,\"97\":1,\"325\":1,\"435\":2,\"690\":2,\"823\":3}}],[\"gotcha\",{\"1\":{\"670\":1}}],[\"garage\",{\"1\":{\"408\":1}}],[\"gap\",{\"1\":{\"321\":2}}],[\"gan＝\",{\"1\":{\"919\":1}}],[\"gan\",{\"0\":{\"901\":1,\"918\":1},\"1\":{\"249\":1,\"251\":1,\"901\":1,\"918\":3,\"925\":2}}],[\"gans\",{\"1\":{\"247\":1}}],[\"gather函数比较灵活\",{\"1\":{\"700\":1}}],[\"gathered\",{\"1\":{\"386\":12}}],[\"gather\",{\"1\":{\"190\":2,\"274\":1,\"364\":2,\"385\":1,\"386\":3,\"699\":4}}],[\"gacnet\",{\"1\":{\"110\":1}}],[\"gamma=gamma\",{\"1\":{\"589\":1}}],[\"gamma=0\",{\"1\":{\"104\":1}}],[\"gamma\",{\"1\":{\"102\":3,\"380\":10,\"522\":9,\"589\":3}}],[\"gafm\",{\"1\":{\"23\":1}}],[\"gaussianblur\",{\"1\":{\"293\":3}}],[\"gaussian\",{\"0\":{\"13\":1,\"869\":1,\"872\":1},\"1\":{\"14\":1,\"19\":1,\"21\":1,\"22\":1,\"26\":1,\"865\":1,\"869\":1,\"871\":1,\"873\":1}}],[\"gehman\",{\"1\":{\"655\":1}}],[\"gemini系列\",{\"1\":{\"335\":1}}],[\"gemini系列和qwen\",{\"1\":{\"323\":1}}],[\"gemini\",{\"1\":{\"322\":1,\"325\":1,\"822\":1,\"823\":14}}],[\"gen\",{\"1\":{\"918\":4,\"939\":2}}],[\"generic\",{\"1\":{\"294\":1,\"750\":1}}],[\"generator模型结构图\",{\"1\":{\"743\":1}}],[\"generator\",{\"0\":{\"743\":1},\"1\":{\"264\":2,\"742\":3,\"743\":2,\"918\":12}}],[\"generative\",{\"0\":{\"247\":1,\"421\":1},\"1\":{\"305\":1,\"421\":1,\"624\":1,\"635\":1,\"823\":1}}],[\"generation能力\",{\"1\":{\"415\":1}}],[\"generation\",{\"0\":{\"420\":1,\"828\":1},\"1\":{\"52\":4,\"164\":2,\"179\":1,\"420\":1,\"597\":1,\"605\":1,\"805\":11,\"807\":3,\"808\":1,\"815\":1,\"828\":1,\"837\":1,\"883\":1,\"890\":1,\"895\":1,\"918\":1,\"921\":1}}],[\"generated\",{\"1\":{\"918\":1,\"926\":2,\"935\":3,\"964\":1}}],[\"generate\",{\"1\":{\"11\":1,\"188\":3,\"421\":2,\"663\":2,\"895\":1,\"898\":2,\"918\":1}}],[\"generalizable\",{\"1\":{\"18\":2,\"19\":1}}],[\"generalization\",{\"1\":{\"14\":1}}],[\"genome\",{\"1\":{\"176\":1,\"185\":1,\"224\":1,\"388\":1}}],[\"gelu\",{\"1\":{\"97\":2,\"380\":1,\"429\":4,\"431\":2,\"633\":1,\"699\":5}}],[\"getcwd\",{\"1\":{\"410\":1,\"412\":1}}],[\"getattr\",{\"1\":{\"384\":2,\"385\":7,\"386\":2,\"807\":1}}],[\"get\",{\"1\":{\"53\":1,\"64\":1,\"82\":3,\"83\":3,\"104\":1,\"107\":3,\"138\":2,\"141\":2,\"146\":2,\"213\":4,\"263\":1,\"265\":2,\"293\":1,\"382\":2,\"384\":1,\"385\":4,\"386\":2,\"410\":10,\"411\":4,\"412\":12,\"582\":1,\"595\":5,\"596\":10,\"597\":16,\"663\":1,\"699\":1,\"703\":1,\"713\":1,\"724\":1,\"815\":4,\"893\":4,\"895\":1,\"899\":2}}],[\"getitem\",{\"1\":{\"53\":2,\"82\":1,\"92\":1,\"187\":1,\"382\":2,\"424\":1,\"808\":1}}],[\"geometries\",{\"1\":{\"107\":1}}],[\"geometric\",{\"1\":{\"11\":2,\"35\":1,\"52\":4,\"863\":1}}],[\"geometry\",{\"1\":{\"28\":1,\"29\":1,\"107\":3}}],[\"geal\",{\"0\":{\"18\":1},\"1\":{\"18\":2,\"19\":5,\"21\":1}}],[\"greet\",{\"1\":{\"449\":1,\"454\":10,\"459\":2}}],[\"greater\",{\"1\":{\"899\":1}}],[\"great通过模拟人类多步推理\",{\"1\":{\"31\":1}}],[\"great通过微调mllms并设计mhacot策略解决这一问题\",{\"1\":{\"31\":1}}],[\"great框架通过以下设计模拟这一过程\",{\"1\":{\"30\":1}}],[\"great\",{\"0\":{\"28\":1},\"1\":{\"28\":2,\"29\":1,\"32\":2,\"45\":1,\"47\":3,\"50\":2,\"53\":2,\"54\":1}}],[\"grit\",{\"1\":{\"332\":1}}],[\"grids\",{\"1\":{\"159\":1}}],[\"grid\",{\"1\":{\"148\":1,\"388\":1,\"391\":1,\"426\":3,\"816\":1}}],[\"grok\",{\"1\":{\"325\":1,\"822\":1}}],[\"groupnorm\",{\"1\":{\"493\":2}}],[\"group流程图\",{\"1\":{\"137\":1}}],[\"grouped\",{\"1\":{\"119\":10,\"124\":1,\"125\":1,\"137\":12,\"141\":13,\"823\":2}}],[\"groups=40\",{\"1\":{\"107\":1}}],[\"groups=opt\",{\"1\":{\"104\":1}}],[\"groups\",{\"1\":{\"104\":1,\"293\":4}}],[\"group\",{\"1\":{\"96\":1,\"99\":2,\"137\":23,\"138\":3,\"141\":3,\"146\":4,\"341\":1}}],[\"grouping阶段\",{\"1\":{\"99\":1}}],[\"grouping\",{\"0\":{\"96\":1,\"135\":1,\"140\":1,\"142\":1},\"1\":{\"95\":1,\"99\":3,\"132\":1,\"133\":2,\"135\":1,\"138\":4,\"139\":2,\"143\":1}}],[\"grounded\",{\"0\":{\"420\":1},\"1\":{\"171\":2,\"172\":2,\"420\":1}}],[\"ground\",{\"0\":{\"88\":1},\"1\":{\"102\":2,\"106\":2,\"207\":1,\"355\":1,\"385\":7,\"586\":2,\"587\":1,\"588\":2,\"589\":1,\"590\":1,\"918\":1}}],[\"grounding\",{\"0\":{\"4\":1},\"1\":{\"4\":1,\"6\":2,\"11\":1,\"12\":1,\"26\":1,\"28\":1,\"31\":2,\"61\":1,\"71\":1,\"72\":1,\"83\":1}}],[\"gray\",{\"1\":{\"935\":3,\"939\":1,\"964\":1}}],[\"graph函数\",{\"1\":{\"815\":1}}],[\"graph\",{\"1\":{\"815\":14}}],[\"graph=true\",{\"1\":{\"814\":1}}],[\"graphviz\",{\"1\":{\"814\":1,\"815\":3}}],[\"gram重叠率\",{\"1\":{\"641\":1}}],[\"grams\",{\"1\":{\"413\":2}}],[\"gram\",{\"1\":{\"234\":1,\"671\":1}}],[\"grad模式优化内存管理\",{\"1\":{\"812\":1}}],[\"grad函数\",{\"1\":{\"807\":1}}],[\"grad默认false\",{\"1\":{\"807\":1}}],[\"grad参数及时清除中间变量导数\",{\"1\":{\"807\":1}}],[\"grad参数\",{\"1\":{\"807\":1}}],[\"grad=false\",{\"1\":{\"380\":1,\"807\":1,\"918\":2}}],[\"grad=true\",{\"1\":{\"380\":2}}],[\"gradientcheckpointinglayer\",{\"1\":{\"663\":1}}],[\"gradients\",{\"1\":{\"293\":1,\"420\":1}}],[\"gradient\",{\"1\":{\"212\":1,\"359\":1,\"361\":1,\"362\":1,\"663\":1,\"795\":1,\"816\":2,\"918\":2,\"959\":1,\"963\":1}}],[\"grad\",{\"0\":{\"473\":1},\"1\":{\"104\":2,\"105\":1,\"187\":6,\"190\":7,\"191\":2,\"192\":7,\"204\":1,\"206\":1,\"207\":1,\"208\":1,\"213\":6,\"265\":3,\"293\":4,\"359\":1,\"361\":1,\"362\":1,\"363\":1,\"364\":1,\"381\":5,\"386\":2,\"408\":2,\"410\":2,\"412\":2,\"419\":1,\"435\":1,\"660\":1,\"663\":1,\"700\":1,\"778\":1,\"781\":7,\"784\":2,\"787\":2,\"791\":2,\"794\":1,\"795\":3,\"801\":4,\"802\":1,\"803\":7,\"805\":8,\"806\":1,\"807\":16,\"808\":1,\"809\":3,\"810\":2,\"811\":6,\"816\":6,\"892\":1,\"895\":1,\"898\":1,\"899\":1,\"918\":2,\"926\":2,\"934\":1,\"935\":2,\"938\":1,\"939\":1,\"963\":1,\"964\":4}}],[\"grasping\",{\"1\":{\"87\":2,\"102\":1,\"589\":1}}],[\"grasp\",{\"1\":{\"30\":1,\"31\":1,\"47\":1,\"52\":2,\"53\":1,\"55\":2,\"64\":1,\"82\":1,\"86\":1,\"87\":1,\"91\":1,\"92\":2,\"94\":1,\"95\":1}}],[\"granularity\",{\"1\":{\"7\":1,\"23\":1}}],[\"转化为更有信息的样本\",{\"1\":{\"950\":1}}],[\"转化为一系列\",{\"1\":{\"355\":1}}],[\"转化成功之后\",{\"1\":{\"712\":1}}],[\"转向\",{\"1\":{\"654\":1}}],[\"转折点\",{\"1\":{\"500\":2}}],[\"转为整数像素值\",{\"1\":{\"926\":1}}],[\"转为连续张量\",{\"1\":{\"470\":1}}],[\"转为\",{\"1\":{\"385\":1,\"892\":1,\"926\":1}}],[\"转为numpy数组\",{\"1\":{\"107\":1}}],[\"转成\",{\"1\":{\"384\":1}}],[\"转成嵌入向量\",{\"1\":{\"384\":1}}],[\"转换视角\",{\"0\":{\"860\":1}}],[\"转换后的图像展示x0\",{\"1\":{\"815\":1}}],[\"转换成样本\",{\"1\":{\"918\":1}}],[\"转换成y序列的权重参数组成的矩阵\",{\"1\":{\"600\":1}}],[\"转换成一个固定大小的特征图\",{\"1\":{\"501\":1}}],[\"转换成\",{\"1\":{\"501\":1}}],[\"转换\",{\"1\":{\"123\":1}}],[\"转换为图像\",{\"1\":{\"964\":1}}],[\"转换为向量\",{\"1\":{\"899\":1}}],[\"转换为一系列高维向量表示\",{\"1\":{\"741\":1}}],[\"转换为一组离散的视觉\",{\"1\":{\"212\":1}}],[\"转换为概率\",{\"1\":{\"590\":1}}],[\"转换为可优化的损失函数\",{\"1\":{\"588\":1}}],[\"转换为张量\",{\"1\":{\"582\":1}}],[\"转换为张量并保持设备一致\",{\"1\":{\"121\":1}}],[\"转换为固定大小\",{\"1\":{\"502\":1}}],[\"转换为元组形式\",{\"1\":{\"426\":1}}],[\"转换为视觉\",{\"1\":{\"380\":1}}],[\"转换为语言嵌入\",{\"1\":{\"342\":1}}],[\"转换为语言模型可用的\",{\"1\":{\"341\":1}}],[\"转换为嵌入向量\",{\"1\":{\"64\":1}}],[\"转换为\",{\"1\":{\"64\":1,\"67\":1,\"106\":1,\"145\":1,\"213\":1,\"293\":1,\"380\":1,\"385\":1,\"899\":1}}],[\"转换为灰度\",{\"1\":{\"22\":1}}],[\"转置前\",{\"1\":{\"545\":1}}],[\"转置包括逻辑转置和物理转置\",{\"1\":{\"545\":1}}],[\"转置操作\",{\"1\":{\"491\":1}}],[\"转置\",{\"0\":{\"545\":1},\"1\":{\"478\":1,\"490\":1,\"545\":2}}],[\"转置卷积等\",{\"1\":{\"899\":1}}],[\"转置卷积层\",{\"1\":{\"122\":1}}],[\"转置卷积\",{\"1\":{\"122\":1,\"899\":1}}],[\"转置为\",{\"1\":{\"107\":1}}],[\"转置后的步长是\",{\"1\":{\"545\":1}}],[\"转置后的矩阵步长是\",{\"1\":{\"545\":1}}],[\"转置后\",{\"1\":{\"58\":1,\"97\":1,\"545\":1}}],[\"转\",{\"1\":{\"3\":1}}],[\"转型\",{\"1\":{\"2\":1}}],[\"cfg\",{\"1\":{\"894\":2,\"895\":2}}],[\"c为常数指数\",{\"1\":{\"809\":1}}],[\"c为输入token的总维度\",{\"1\":{\"430\":1}}],[\"c中\",{\"1\":{\"809\":1}}],[\"c并求导\",{\"1\":{\"809\":1}}],[\"c的引用计数仍为1\",{\"1\":{\"806\":1}}],[\"cdf\",{\"1\":{\"847\":2,\"865\":1,\"949\":1}}],[\"cd\",{\"1\":{\"712\":1,\"739\":1}}],[\"c4\",{\"1\":{\"666\":1,\"667\":1}}],[\"ctrl\",{\"1\":{\"650\":1}}],[\"ctx\",{\"1\":{\"119\":1,\"121\":2}}],[\"cb\",{\"1\":{\"648\":1}}],[\"cbt测试模型对不同词类\",{\"1\":{\"641\":1}}],[\"cbt\",{\"1\":{\"641\":1}}],[\"cbow\",{\"1\":{\"407\":1}}],[\"c++\",{\"1\":{\"444\":1,\"492\":2}}],[\"c+d\",{\"1\":{\"137\":3}}],[\"c3\",{\"1\":{\"441\":1}}],[\"c2\",{\"1\":{\"441\":1}}],[\"c=in\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"c=3\",{\"1\":{\"426\":1,\"427\":1,\"428\":1,\"431\":1,\"478\":1}}],[\"c=通道数\",{\"1\":{\"380\":1}}],[\"c=6\",{\"1\":{\"123\":1}}],[\"ck\",{\"1\":{\"362\":1,\"475\":1}}],[\"ckpt=false\",{\"1\":{\"190\":1,\"192\":1}}],[\"ckpt=config\",{\"1\":{\"187\":1,\"190\":1,\"192\":1}}],[\"ckpt\",{\"1\":{\"187\":9,\"190\":6,\"191\":4,\"192\":6,\"213\":2,\"712\":1}}],[\"c或internvl\",{\"1\":{\"304\":1}}],[\"ccbench等中文基准上大幅领先\",{\"1\":{\"335\":1}}],[\"cc\",{\"1\":{\"224\":1,\"678\":1,\"679\":1,\"680\":1}}],[\"cc3m\",{\"1\":{\"185\":1,\"224\":1,\"341\":1}}],[\"cc12m\",{\"1\":{\"185\":1,\"224\":1}}],[\"ceil\",{\"1\":{\"501\":1}}],[\"centercrop\",{\"1\":{\"425\":1}}],[\"center\",{\"1\":{\"293\":20,\"424\":2}}],[\"centering\",{\"1\":{\"280\":1,\"285\":1,\"290\":1}}],[\"centroids\",{\"1\":{\"137\":4}}],[\"centroid\",{\"1\":{\"107\":2,\"137\":2}}],[\"ce\",{\"1\":{\"104\":1,\"105\":3,\"274\":5,\"385\":1,\"589\":15,\"592\":10}}],[\"celoss\",{\"1\":{\"102\":4}}],[\"chroma\",{\"1\":{\"836\":1}}],[\"christiano\",{\"1\":{\"655\":1}}],[\"chinesegluedatasets\",{\"1\":{\"712\":2}}],[\"chinese\",{\"1\":{\"712\":5}}],[\"chinchilla\",{\"1\":{\"668\":1,\"671\":1}}],[\"children\",{\"1\":{\"641\":1}}],[\"chunks\",{\"1\":{\"482\":5}}],[\"chunk\",{\"0\":{\"482\":1},\"1\":{\"293\":2,\"482\":2}}],[\"chunking\",{\"1\":{\"157\":1}}],[\"check\",{\"1\":{\"795\":1}}],[\"checkpointing\",{\"1\":{\"663\":1,\"667\":1}}],[\"checkpoint\",{\"1\":{\"106\":2,\"107\":2,\"192\":1,\"213\":1,\"700\":5,\"712\":2}}],[\"chen\",{\"1\":{\"171\":1}}],[\"choices\",{\"1\":{\"737\":9}}],[\"choice\",{\"1\":{\"87\":1,\"698\":1,\"737\":1}}],[\"chapter3\",{\"1\":{\"815\":3,\"816\":2}}],[\"chapter2\",{\"1\":{\"810\":7}}],[\"challenge中零样本即可取得88\",{\"1\":{\"648\":1}}],[\"challenge\",{\"1\":{\"641\":1}}],[\"chans=in\",{\"1\":{\"380\":1}}],[\"chans=3\",{\"1\":{\"266\":1,\"380\":2}}],[\"chans\",{\"1\":{\"266\":2,\"380\":2,\"899\":19}}],[\"chan\",{\"1\":{\"255\":8,\"899\":2}}],[\"channels=z\",{\"1\":{\"963\":1}}],[\"channels=128\",{\"1\":{\"963\":2}}],[\"channels=1\",{\"1\":{\"963\":3,\"964\":1}}],[\"channels=64\",{\"1\":{\"926\":2,\"963\":3,\"964\":1}}],[\"channels\",{\"1\":{\"64\":1,\"255\":3,\"424\":1,\"510\":1,\"893\":3,\"899\":6,\"900\":2,\"918\":3,\"926\":32,\"963\":12,\"964\":5}}],[\"channel=2\",{\"1\":{\"542\":1}}],[\"channel=256+3\",{\"1\":{\"146\":1}}],[\"channel=256\",{\"1\":{\"138\":1}}],[\"channel=320\",{\"1\":{\"146\":1}}],[\"channel=384\",{\"1\":{\"146\":1}}],[\"channel=768\",{\"1\":{\"146\":1}}],[\"channel=64+3\",{\"1\":{\"146\":1}}],[\"channel=9+3\",{\"1\":{\"146\":1}}],[\"channel=128+3\",{\"1\":{\"146\":1}}],[\"channel=128\",{\"1\":{\"138\":1,\"146\":1}}],[\"channel=in\",{\"1\":{\"138\":1}}],[\"channel=true\",{\"1\":{\"138\":1,\"141\":1}}],[\"channel=518+additional\",{\"1\":{\"59\":1,\"70\":1,\"83\":1}}],[\"channel=512+self\",{\"1\":{\"59\":1,\"70\":1,\"83\":1}}],[\"channel=832\",{\"1\":{\"59\":1,\"70\":1,\"83\":1}}],[\"channel\",{\"1\":{\"54\":2,\"58\":1,\"59\":2,\"70\":3,\"83\":3,\"97\":11,\"119\":3,\"137\":12,\"138\":7,\"141\":15,\"145\":11,\"213\":1,\"542\":1,\"924\":2}}],[\"character\",{\"1\":{\"597\":1}}],[\"characteristic\",{\"0\":{\"569\":1},\"1\":{\"106\":1}}],[\"chartqa\",{\"1\":{\"323\":1,\"332\":1,\"335\":1,\"337\":1}}],[\"charlesq34\",{\"1\":{\"130\":1,\"147\":1}}],[\"chair\",{\"1\":{\"53\":1,\"82\":1,\"91\":1,\"92\":1,\"107\":6}}],[\"chains\",{\"1\":{\"832\":1}}],[\"chain\",{\"0\":{\"33\":1,\"52\":1,\"620\":1},\"1\":{\"11\":1,\"31\":1,\"50\":1,\"620\":2,\"621\":1,\"825\":1,\"833\":2,\"836\":1}}],[\"chatglm\",{\"1\":{\"823\":2}}],[\"chatgpt\",{\"1\":{\"339\":1,\"342\":1,\"346\":1,\"610\":1,\"822\":1,\"823\":7,\"827\":1,\"831\":1}}],[\"chatbot\",{\"1\":{\"342\":1}}],[\"chat\",{\"1\":{\"52\":4,\"304\":1,\"310\":2,\"317\":2,\"334\":1,\"342\":1,\"673\":1,\"823\":4}}],[\"creator\",{\"1\":{\"783\":3,\"784\":1,\"787\":3,\"800\":1,\"801\":3,\"803\":3,\"805\":8,\"807\":4,\"808\":1,\"815\":3}}],[\"creating\",{\"1\":{\"595\":1,\"597\":1}}],[\"creates\",{\"1\":{\"713\":1}}],[\"create\",{\"0\":{\"511\":1},\"1\":{\"107\":1,\"187\":3,\"190\":4,\"191\":1,\"192\":4,\"265\":1,\"361\":2,\"510\":1,\"511\":3,\"550\":4,\"554\":1,\"595\":4,\"597\":6,\"712\":1,\"713\":1,\"739\":1,\"814\":1}}],[\"crawl这类数据在整个训练中只被读取一次左右\",{\"1\":{\"647\":1}}],[\"crawl执行了质量过滤和模糊去重\",{\"1\":{\"647\":1}}],[\"crawl\",{\"1\":{\"332\":1,\"640\":1,\"647\":1,\"823\":1}}],[\"crafting\",{\"0\":{\"87\":1}}],[\"critical\",{\"1\":{\"150\":1,\"157\":2}}],[\"criterion2\",{\"1\":{\"700\":2}}],[\"criterion1\",{\"1\":{\"700\":2}}],[\"criterion\",{\"1\":{\"104\":2,\"105\":2,\"359\":5,\"926\":2}}],[\"crowson\",{\"1\":{\"894\":1}}],[\"crows\",{\"1\":{\"655\":1,\"656\":1,\"657\":1,\"668\":1,\"670\":1}}],[\"crops\",{\"1\":{\"293\":29,\"359\":1}}],[\"crop策略\",{\"1\":{\"291\":1}}],[\"crop\",{\"1\":{\"82\":1,\"280\":1,\"285\":1,\"286\":1,\"291\":3,\"293\":14}}],[\"crossentropy\",{\"1\":{\"510\":1,\"592\":3}}],[\"crossentropyloss\",{\"1\":{\"104\":1,\"208\":3,\"265\":1,\"359\":1,\"382\":1,\"403\":1,\"420\":2,\"700\":2,\"722\":1,\"731\":1,\"734\":1,\"736\":1,\"737\":1,\"926\":1,\"964\":1}}],[\"crossattention\",{\"1\":{\"207\":1,\"399\":2,\"420\":1}}],[\"cross\",{\"0\":{\"38\":1,\"910\":1},\"1\":{\"7\":2,\"14\":1,\"18\":2,\"56\":1,\"57\":2,\"59\":1,\"60\":4,\"64\":1,\"69\":8,\"83\":3,\"99\":1,\"102\":1,\"171\":1,\"188\":1,\"190\":1,\"192\":2,\"197\":1,\"207\":5,\"208\":2,\"268\":3,\"272\":5,\"274\":6,\"306\":2,\"359\":2,\"373\":1,\"384\":1,\"385\":4,\"386\":1,\"397\":2,\"398\":1,\"399\":1,\"401\":3,\"402\":2,\"407\":2,\"418\":2,\"419\":1,\"420\":13,\"587\":5,\"589\":4,\"592\":2,\"663\":3,\"893\":5,\"900\":3,\"932\":4}}],[\"csv\",{\"1\":{\"92\":2,\"696\":6}}],[\"cs\",{\"1\":{\"64\":2,\"150\":2}}],[\"csdn\",{\"1\":{\"0\":1}}],[\"cp\",{\"1\":{\"64\":1,\"816\":1}}],[\"cpu实现的最远点采样算法\",{\"1\":{\"121\":1}}],[\"cpu\",{\"1\":{\"52\":1,\"64\":1,\"107\":2,\"213\":2,\"265\":1,\"408\":3,\"410\":3,\"412\":3,\"486\":1,\"492\":1,\"521\":3,\"582\":2,\"740\":1,\"918\":2,\"926\":1,\"934\":1,\"935\":3,\"938\":1,\"939\":1,\"964\":2}}],[\"cider\",{\"1\":{\"268\":1,\"309\":1}}],[\"ci\",{\"1\":{\"64\":1}}],[\"cnt\",{\"1\":{\"122\":6}}],[\"cnn中\",{\"1\":{\"500\":1,\"522\":1}}],[\"cnn具有两种归纳偏置\",{\"1\":{\"422\":1}}],[\"cnn二阶段检测器提取region的特征\",{\"1\":{\"391\":1}}],[\"cnn\",{\"1\":{\"97\":1,\"110\":2,\"152\":1,\"157\":4,\"159\":3,\"228\":1,\"264\":1,\"269\":1,\"369\":1,\"407\":1,\"422\":1,\"434\":9,\"436\":1,\"501\":2,\"502\":2,\"641\":1,\"921\":5,\"923\":1,\"963\":1}}],[\"cn\",{\"1\":{\"61\":1,\"309\":2,\"318\":1,\"335\":1}}],[\"cmap=\",{\"1\":{\"816\":1,\"935\":3,\"939\":1,\"964\":1}}],[\"cmafm\",{\"0\":{\"38\":1},\"1\":{\"29\":1,\"32\":1,\"38\":1,\"48\":1}}],[\"cmd\",{\"1\":{\"815\":2}}],[\"cm\",{\"1\":{\"574\":1}}],[\"cmff\",{\"1\":{\"59\":2}}],[\"cylindrical\",{\"1\":{\"56\":1}}],[\"cub\",{\"1\":{\"884\":1}}],[\"cudnn\",{\"1\":{\"521\":8}}],[\"cuda\",{\"0\":{\"520\":1},\"1\":{\"52\":1,\"152\":2,\"153\":2,\"213\":1,\"265\":1,\"293\":1,\"359\":1,\"362\":1,\"408\":2,\"410\":2,\"412\":2,\"486\":2,\"492\":1,\"520\":4,\"521\":5,\"557\":1,\"898\":2,\"918\":8,\"926\":2,\"934\":2,\"938\":2}}],[\"cumval\",{\"1\":{\"582\":2}}],[\"cumulative\",{\"1\":{\"481\":1}}],[\"cumsum\",{\"0\":{\"481\":1},\"1\":{\"293\":2,\"481\":3,\"582\":1}}],[\"cup\",{\"1\":{\"408\":1}}],[\"customized\",{\"1\":{\"264\":1,\"265\":1}}],[\"cutmix\",{\"1\":{\"510\":1}}],[\"cut\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"cur\",{\"1\":{\"895\":2}}],[\"curve\",{\"1\":{\"46\":1,\"52\":2,\"106\":3}}],[\"current\",{\"1\":{\"11\":1,\"83\":2,\"106\":3,\"383\":9,\"386\":1,\"410\":2,\"411\":1,\"412\":3}}],[\"c\",{\"1\":{\"19\":2,\"25\":2,\"30\":2,\"43\":1,\"54\":1,\"56\":2,\"59\":1,\"64\":1,\"65\":9,\"69\":16,\"70\":3,\"83\":54,\"94\":4,\"96\":2,\"97\":2,\"98\":2,\"99\":2,\"100\":12,\"119\":18,\"122\":4,\"123\":6,\"137\":14,\"141\":5,\"143\":3,\"145\":7,\"146\":1,\"213\":11,\"222\":1,\"256\":2,\"266\":1,\"274\":2,\"303\":2,\"306\":1,\"309\":2,\"359\":1,\"364\":1,\"380\":6,\"384\":1,\"390\":1,\"407\":1,\"415\":2,\"417\":1,\"426\":6,\"427\":3,\"428\":2,\"430\":3,\"431\":2,\"441\":2,\"444\":1,\"478\":6,\"482\":4,\"489\":2,\"503\":2,\"572\":2,\"574\":1,\"582\":3,\"590\":1,\"656\":1,\"657\":1,\"663\":4,\"696\":2,\"703\":1,\"710\":1,\"737\":1,\"766\":2,\"781\":1,\"804\":2,\"806\":5,\"809\":17,\"846\":2,\"888\":1,\"893\":5,\"899\":2,\"900\":3,\"936\":1,\"937\":9,\"938\":2,\"939\":2,\"944\":3,\"963\":7}}],[\"cauchy\",{\"0\":{\"868\":1},\"1\":{\"868\":2}}],[\"causallmoutputwithcrossattentions\",{\"1\":{\"403\":1,\"420\":1}}],[\"causal\",{\"1\":{\"403\":1,\"420\":2,\"663\":1,\"892\":1,\"900\":2}}],[\"case\",{\"1\":{\"694\":1,\"712\":1,\"835\":2,\"836\":3}}],[\"casual\",{\"1\":{\"430\":1}}],[\"cache的实现细节\",{\"1\":{\"663\":1}}],[\"cache结构\",{\"1\":{\"663\":1}}],[\"cache用元组的形式进行返回\",{\"1\":{\"663\":1}}],[\"cache形式\",{\"1\":{\"663\":1}}],[\"cache后\",{\"1\":{\"663\":1}}],[\"cache进行推理\",{\"1\":{\"660\":1}}],[\"cache详解\",{\"1\":{\"659\":1}}],[\"cache\",{\"0\":{\"659\":1,\"660\":1,\"661\":1,\"662\":1},\"1\":{\"420\":4,\"660\":1,\"661\":1,\"662\":1,\"663\":41,\"893\":9,\"894\":8,\"895\":4}}],[\"cache=cache\",{\"1\":{\"893\":1,\"895\":1}}],[\"cache=use\",{\"1\":{\"420\":1,\"663\":2}}],[\"cache=none\",{\"1\":{\"420\":1,\"663\":1,\"893\":1}}],[\"cache=true\",{\"1\":{\"417\":1,\"420\":2,\"663\":1}}],[\"calling\",{\"1\":{\"823\":1}}],[\"callbacks\",{\"1\":{\"381\":16,\"832\":1}}],[\"calls\",{\"1\":{\"381\":1}}],[\"call\",{\"1\":{\"263\":1,\"264\":1,\"293\":1,\"359\":1,\"449\":2,\"454\":2,\"459\":2,\"761\":1,\"762\":1,\"779\":2,\"783\":2,\"800\":1,\"805\":1,\"806\":1,\"807\":2,\"809\":2,\"823\":1}}],[\"calculate\",{\"1\":{\"213\":2}}],[\"capacity\",{\"1\":{\"949\":1}}],[\"capabilities\",{\"1\":{\"14\":1}}],[\"capital\",{\"1\":{\"735\":2}}],[\"captions\",{\"1\":{\"176\":2,\"185\":2,\"188\":3,\"224\":2,\"341\":1,\"421\":2,\"887\":1,\"888\":2}}],[\"caption\",{\"1\":{\"173\":1,\"187\":10,\"188\":2,\"190\":5,\"191\":2,\"192\":4,\"274\":7,\"341\":3,\"382\":5,\"413\":1,\"420\":1,\"885\":1}}],[\"captioners\",{\"0\":{\"272\":1},\"1\":{\"267\":2,\"270\":1,\"271\":1}}],[\"captioner模块初始化\",{\"1\":{\"187\":1}}],[\"captioner\",{\"0\":{\"186\":1},\"1\":{\"165\":1,\"168\":1,\"173\":1,\"177\":3,\"179\":1,\"183\":1,\"185\":1,\"187\":1,\"188\":1,\"268\":1,\"271\":2,\"272\":1}}],[\"captioning\",{\"1\":{\"165\":1,\"173\":1,\"268\":1,\"271\":1,\"309\":1,\"377\":1,\"420\":1}}],[\"capfilt\",{\"0\":{\"173\":1,\"177\":1,\"181\":1,\"185\":1},\"1\":{\"165\":1,\"167\":1,\"168\":1,\"170\":1,\"173\":2,\"174\":1,\"177\":1,\"179\":1,\"181\":3,\"182\":3,\"183\":1,\"185\":3,\"192\":1}}],[\"carlo\",{\"1\":{\"944\":1}}],[\"cardinality\",{\"1\":{\"102\":4}}],[\"carry\",{\"1\":{\"53\":1}}],[\"ca\",{\"1\":{\"64\":1,\"171\":2}}],[\"catastrophic\",{\"1\":{\"602\":1}}],[\"categorical\",{\"0\":{\"857\":1},\"1\":{\"857\":1}}],[\"categories\",{\"1\":{\"11\":1}}],[\"category\",{\"1\":{\"106\":2,\"410\":4,\"412\":4}}],[\"cat\",{\"1\":{\"56\":4,\"58\":1,\"59\":2,\"60\":1,\"65\":1,\"67\":3,\"69\":1,\"70\":1,\"83\":4,\"94\":1,\"119\":3,\"122\":2,\"123\":1,\"137\":2,\"141\":2,\"145\":1,\"154\":1,\"190\":9,\"192\":8,\"206\":2,\"207\":6,\"266\":1,\"274\":1,\"293\":2,\"362\":1,\"380\":3,\"384\":2,\"386\":5,\"408\":1,\"419\":6,\"420\":3,\"427\":1,\"428\":1,\"431\":1,\"660\":1,\"663\":1,\"893\":2,\"895\":2,\"898\":1,\"937\":2,\"964\":1}}],[\"cand\",{\"1\":{\"697\":2,\"698\":3}}],[\"candidates\",{\"1\":{\"410\":13,\"412\":10,\"698\":3}}],[\"cannot\",{\"1\":{\"430\":1}}],[\"cancel\",{\"1\":{\"293\":1}}],[\"can\",{\"1\":{\"52\":4,\"274\":1}}],[\"camera\",{\"1\":{\"408\":1}}],[\"cam\",{\"1\":{\"23\":1,\"24\":1,\"26\":1}}],[\"cli\",{\"1\":{\"834\":1}}],[\"clipprocessor\",{\"1\":{\"410\":1,\"412\":2}}],[\"clipmodel\",{\"1\":{\"410\":1,\"412\":2}}],[\"clip模型均能够以较高的置信度给出正确的分类结果\",{\"1\":{\"408\":1}}],[\"clip模型能够在没有特定任务训练数据的情况下\",{\"1\":{\"408\":1}}],[\"clip模型的一个显著优势是它能够进行zero\",{\"1\":{\"408\":1}}],[\"clip模型会预测出个可能的文本\",{\"1\":{\"407\":1}}],[\"clip包含两个核心模型\",{\"1\":{\"407\":1}}],[\"clip的训练数据采用的是文本\",{\"1\":{\"406\":1}}],[\"clip的英文全称为contrastive\",{\"1\":{\"406\":1}}],[\"clip原始论文链接\",{\"1\":{\"404\":1}}],[\"clip属于基于对比学习的多模态模型\",{\"1\":{\"406\":1}}],[\"clip属于\",{\"1\":{\"390\":1}}],[\"clip教师模型\",{\"1\":{\"213\":1}}],[\"clip输出维度为512\",{\"1\":{\"213\":1}}],[\"clip专用的缩放层\",{\"1\":{\"213\":1}}],[\"clip或dino\",{\"1\":{\"213\":1}}],[\"clip\",{\"0\":{\"418\":1,\"900\":1},\"1\":{\"26\":1,\"212\":1,\"213\":5,\"215\":1,\"251\":1,\"265\":1,\"268\":1,\"269\":2,\"305\":1,\"308\":1,\"327\":1,\"341\":2,\"342\":2,\"368\":2,\"369\":1,\"385\":3,\"405\":3,\"408\":1,\"410\":3,\"412\":1,\"413\":1,\"700\":1,\"889\":1,\"895\":5,\"899\":1,\"900\":1}}],[\"cleargrad\",{\"1\":{\"802\":1,\"816\":4}}],[\"clean\",{\"1\":{\"53\":1}}],[\"clm\",{\"1\":{\"403\":1}}],[\"clamp\",{\"1\":{\"592\":2,\"734\":4,\"931\":1}}],[\"clamped\",{\"1\":{\"213\":2}}],[\"cla\",{\"1\":{\"424\":9}}],[\"claude\",{\"1\":{\"325\":1,\"822\":1,\"823\":14}}],[\"classify\",{\"1\":{\"918\":1}}],[\"classifier\",{\"0\":{\"894\":1},\"1\":{\"694\":2,\"699\":3,\"712\":2,\"722\":2,\"736\":2,\"737\":2,\"894\":1}}],[\"classification\",{\"0\":{\"238\":1},\"1\":{\"144\":1,\"271\":1,\"282\":1,\"308\":1,\"309\":1,\"713\":1}}],[\"class=val\",{\"1\":{\"425\":1}}],[\"class=train\",{\"1\":{\"425\":1}}],[\"classes=10\",{\"1\":{\"938\":1,\"939\":1}}],[\"classes=1000\",{\"1\":{\"427\":1,\"428\":1,\"431\":1}}],[\"classes=classes\",{\"1\":{\"514\":1}}],[\"classes=5\",{\"1\":{\"435\":1}}],[\"classes=num\",{\"1\":{\"435\":1}}],[\"classes=dim\",{\"1\":{\"361\":2}}],[\"classes\",{\"1\":{\"92\":2,\"143\":1,\"146\":8,\"361\":1,\"427\":3,\"428\":2,\"431\":6,\"435\":2,\"514\":5}}],[\"class\",{\"0\":{\"427\":1,\"514\":1},\"1\":{\"6\":1,\"11\":1,\"53\":1,\"54\":1,\"56\":1,\"57\":1,\"58\":2,\"59\":2,\"60\":2,\"64\":1,\"65\":1,\"69\":2,\"70\":1,\"82\":1,\"83\":7,\"86\":1,\"92\":6,\"94\":1,\"96\":1,\"97\":1,\"98\":1,\"99\":1,\"100\":2,\"102\":1,\"107\":1,\"119\":2,\"120\":1,\"121\":2,\"122\":1,\"123\":1,\"137\":1,\"138\":4,\"141\":4,\"145\":1,\"146\":1,\"152\":1,\"154\":1,\"155\":1,\"156\":1,\"187\":2,\"190\":1,\"191\":1,\"192\":1,\"207\":1,\"208\":1,\"213\":3,\"255\":2,\"263\":1,\"264\":1,\"266\":2,\"274\":3,\"286\":1,\"293\":5,\"359\":1,\"361\":1,\"380\":5,\"382\":6,\"383\":1,\"392\":1,\"397\":2,\"398\":1,\"399\":1,\"400\":2,\"401\":1,\"403\":3,\"417\":1,\"419\":1,\"420\":4,\"421\":1,\"424\":27,\"426\":1,\"427\":3,\"428\":1,\"429\":2,\"430\":1,\"431\":1,\"444\":2,\"455\":1,\"456\":3,\"459\":1,\"514\":8,\"582\":1,\"586\":2,\"587\":1,\"588\":1,\"589\":3,\"590\":1,\"592\":1,\"597\":1,\"663\":3,\"694\":1,\"697\":1,\"699\":1,\"703\":1,\"709\":1,\"710\":1,\"716\":1,\"718\":3,\"719\":1,\"720\":1,\"721\":1,\"722\":1,\"724\":1,\"725\":1,\"726\":1,\"728\":1,\"729\":1,\"730\":1,\"731\":1,\"734\":1,\"736\":1,\"737\":1,\"742\":1,\"743\":1,\"745\":1,\"746\":1,\"747\":1,\"749\":1,\"750\":1,\"751\":1,\"756\":1,\"762\":2,\"766\":1,\"778\":1,\"779\":1,\"780\":1,\"783\":2,\"784\":1,\"787\":1,\"791\":1,\"792\":1,\"794\":1,\"795\":1,\"800\":1,\"801\":1,\"802\":1,\"803\":1,\"805\":3,\"806\":2,\"807\":3,\"808\":4,\"809\":7,\"815\":1,\"899\":1,\"900\":1,\"918\":2,\"926\":2,\"931\":1,\"937\":1,\"963\":4,\"964\":2}}],[\"clusters\",{\"1\":{\"213\":7}}],[\"cluster\",{\"1\":{\"213\":17}}],[\"clues\",{\"1\":{\"95\":1}}],[\"cls是一个二分类值\",{\"1\":{\"700\":1}}],[\"cls特征\",{\"1\":{\"192\":2}}],[\"cls2idx\",{\"1\":{\"92\":2}}],[\"cls\",{\"1\":{\"83\":2,\"92\":7,\"105\":3,\"123\":2,\"138\":1,\"141\":2,\"171\":2,\"192\":3,\"199\":1,\"201\":1,\"205\":1,\"206\":3,\"207\":3,\"208\":3,\"210\":1,\"214\":8,\"215\":3,\"266\":8,\"272\":1,\"274\":18,\"277\":2,\"286\":3,\"293\":1,\"371\":2,\"373\":3,\"375\":1,\"380\":7,\"382\":10,\"384\":8,\"385\":43,\"386\":5,\"397\":1,\"403\":2,\"420\":2,\"427\":22,\"428\":5,\"431\":5,\"456\":3,\"582\":5,\"679\":1,\"692\":4,\"694\":6,\"697\":2,\"698\":5,\"699\":10,\"700\":4,\"713\":9,\"720\":1,\"731\":3,\"733\":2,\"735\":2,\"737\":1}}],[\"cl=语言嵌入维度\",{\"1\":{\"64\":1}}],[\"cl\",{\"1\":{\"64\":3,\"92\":2}}],[\"cloze和race提升明显\",{\"1\":{\"634\":1}}],[\"cloze\",{\"1\":{\"626\":1}}],[\"closed\",{\"1\":{\"648\":1,\"668\":1}}],[\"close\",{\"1\":{\"625\":1}}],[\"closing\",{\"1\":{\"321\":2}}],[\"clones\",{\"1\":{\"746\":1,\"747\":1,\"749\":1,\"750\":1,\"751\":1}}],[\"clone\",{\"1\":{\"83\":1,\"190\":4,\"192\":4,\"206\":2,\"208\":2,\"213\":2,\"362\":1,\"419\":1,\"420\":1,\"663\":1,\"712\":1,\"739\":1,\"926\":1}}],[\"clock\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"cloud\",{\"1\":{\"7\":3,\"53\":1,\"70\":3,\"107\":6,\"159\":1}}],[\"cost=0\",{\"1\":{\"963\":1}}],[\"cost\",{\"1\":{\"963\":3}}],[\"cos\",{\"1\":{\"706\":1,\"814\":1}}],[\"cosineannealing\",{\"1\":{\"510\":1}}],[\"cosineannealinglr\",{\"1\":{\"104\":1}}],[\"cosine\",{\"1\":{\"213\":7,\"224\":1,\"236\":1,\"293\":3,\"407\":1,\"410\":2,\"411\":1,\"412\":3,\"506\":1,\"633\":1}}],[\"copies\",{\"1\":{\"681\":1}}],[\"copy\",{\"1\":{\"190\":1,\"192\":1,\"205\":1,\"213\":3,\"361\":1,\"663\":1,\"816\":2,\"894\":1}}],[\"coqa\",{\"1\":{\"641\":1}}],[\"coefficient\",{\"1\":{\"586\":3,\"587\":2,\"857\":1}}],[\"covariance\",{\"1\":{\"871\":4}}],[\"cov\",{\"1\":{\"574\":1}}],[\"covering\",{\"1\":{\"7\":1}}],[\"core\",{\"1\":{\"747\":1,\"810\":9,\"833\":1,\"834\":1}}],[\"corpus\",{\"1\":{\"633\":1,\"634\":2}}],[\"corners\",{\"1\":{\"503\":1}}],[\"corners=none\",{\"1\":{\"503\":1}}],[\"correlations\",{\"1\":{\"646\":1}}],[\"corresponding\",{\"1\":{\"597\":1,\"720\":1}}],[\"correct\",{\"1\":{\"410\":3,\"412\":3,\"701\":2,\"895\":1,\"899\":1}}],[\"corrupt\",{\"1\":{\"19\":2}}],[\"coffee\",{\"1\":{\"408\":1}}],[\"co\",{\"1\":{\"384\":3,\"385\":7}}],[\"cogvlm\",{\"1\":{\"326\":1}}],[\"coyo\",{\"1\":{\"305\":1,\"332\":1}}],[\"codistillation\",{\"1\":{\"283\":1}}],[\"coder\",{\"1\":{\"823\":2}}],[\"codebook机制最早是在vq\",{\"1\":{\"955\":1}}],[\"codebook\",{\"0\":{\"886\":1},\"1\":{\"210\":1,\"212\":1,\"213\":38,\"215\":1,\"235\":2,\"249\":1,\"255\":7,\"256\":4,\"257\":1,\"260\":1,\"262\":4,\"265\":2,\"893\":2,\"895\":1,\"899\":18,\"955\":1,\"956\":1,\"961\":2,\"963\":7,\"964\":1,\"965\":1}}],[\"code=true\",{\"1\":{\"52\":2}}],[\"code\",{\"0\":{\"184\":1,\"203\":1,\"358\":1,\"359\":1},\"1\":{\"6\":1,\"7\":1,\"11\":1,\"12\":1,\"14\":1,\"28\":1,\"213\":7,\"262\":4,\"359\":1,\"712\":7}}],[\"coca\",{\"0\":{\"267\":1,\"273\":1},\"1\":{\"267\":3,\"268\":7,\"269\":4,\"270\":2,\"272\":12,\"273\":6,\"274\":5,\"276\":2,\"278\":1}}],[\"cococaptionkarpathydataset\",{\"1\":{\"382\":5}}],[\"cococaptionkarpathydatamodule\",{\"1\":{\"382\":2}}],[\"coco\",{\"1\":{\"173\":2,\"176\":1,\"185\":1,\"187\":4,\"188\":1,\"190\":1,\"220\":4,\"224\":1,\"243\":1,\"309\":4,\"382\":6,\"884\":2,\"888\":4}}],[\"cookbook不采用梯度回传更新\",{\"1\":{\"213\":2}}],[\"cookbook\",{\"1\":{\"213\":3}}],[\"coordinates\",{\"1\":{\"107\":6}}],[\"counter\",{\"0\":{\"516\":1},\"1\":{\"516\":8,\"518\":1}}],[\"counts\",{\"1\":{\"480\":3,\"485\":2,\"516\":5,\"518\":2}}],[\"counts=false\",{\"1\":{\"480\":1}}],[\"counts=true\",{\"1\":{\"293\":2,\"480\":1}}],[\"count\",{\"1\":{\"121\":3,\"263\":4,\"410\":7,\"412\":7,\"520\":1,\"595\":2,\"597\":5,\"698\":3}}],[\"cola上取得45\",{\"1\":{\"634\":1}}],[\"cola\",{\"1\":{\"634\":1}}],[\"column\",{\"1\":{\"382\":1,\"541\":1,\"542\":2}}],[\"color=\",{\"1\":{\"816\":2}}],[\"color=lightblue\",{\"1\":{\"815\":3}}],[\"color=orange\",{\"1\":{\"815\":9}}],[\"colorjitter\",{\"1\":{\"264\":1,\"293\":1,\"359\":1}}],[\"colors\",{\"1\":{\"107\":6}}],[\"color\",{\"1\":{\"107\":9,\"293\":4}}],[\"collect\",{\"1\":{\"597\":1}}],[\"collections\",{\"0\":{\"515\":1},\"1\":{\"516\":3,\"597\":1}}],[\"collection\",{\"0\":{\"41\":1},\"1\":{\"656\":1}}],[\"collator\",{\"1\":{\"382\":7}}],[\"collator=self\",{\"1\":{\"382\":1}}],[\"collate可以参考\",{\"1\":{\"424\":1}}],[\"collate\",{\"1\":{\"187\":1,\"190\":1,\"192\":1,\"382\":17,\"424\":3,\"425\":4,\"715\":2}}],[\"collapse\",{\"1\":{\"212\":1,\"262\":3}}],[\"collaborative\",{\"1\":{\"28\":1,\"29\":1}}],[\"cot的特点是同类型问题的迁移思考\",{\"1\":{\"622\":1}}],[\"cot的效果并不明显\",{\"1\":{\"620\":1}}],[\"cot是llm足够大\",{\"1\":{\"620\":1}}],[\"cot及其变体通过多步推理增强mllms能力\",{\"1\":{\"31\":1}}],[\"cot\",{\"1\":{\"31\":2,\"620\":1,\"825\":1}}],[\"confidence\",{\"1\":{\"589\":1}}],[\"config函数\",{\"1\":{\"807\":1}}],[\"configure\",{\"1\":{\"381\":1,\"918\":2}}],[\"config=none\",{\"1\":{\"380\":1}}],[\"config=bert\",{\"1\":{\"205\":2}}],[\"config=decoder\",{\"1\":{\"192\":1}}],[\"config=encoder\",{\"1\":{\"192\":2}}],[\"config=\",{\"1\":{\"190\":1,\"192\":1}}],[\"config=med\",{\"1\":{\"187\":1,\"190\":2,\"191\":1}}],[\"configs\",{\"1\":{\"187\":1,\"190\":1,\"191\":1,\"192\":1}}],[\"config\",{\"1\":{\"52\":4,\"66\":2,\"68\":3,\"107\":1,\"187\":11,\"190\":13,\"191\":3,\"192\":11,\"204\":4,\"205\":13,\"208\":2,\"213\":13,\"380\":4,\"382\":22,\"383\":1,\"384\":1,\"397\":3,\"398\":4,\"400\":5,\"403\":15,\"420\":2,\"663\":3,\"712\":2,\"716\":9,\"718\":16,\"719\":3,\"720\":3,\"721\":5,\"722\":7,\"724\":9,\"725\":5,\"726\":3,\"728\":8,\"729\":5,\"730\":3,\"731\":5,\"733\":1,\"734\":6,\"736\":7,\"737\":5,\"807\":8,\"810\":3}}],[\"confusion\",{\"0\":{\"561\":1}}],[\"conquer\",{\"1\":{\"500\":1}}],[\"condaerror\",{\"1\":{\"558\":1}}],[\"conda\",{\"1\":{\"550\":4,\"551\":2,\"552\":1,\"553\":4,\"554\":3,\"555\":1,\"556\":1,\"557\":6,\"558\":4,\"712\":2,\"739\":2}}],[\"conda虚拟环境管理\",{\"0\":{\"549\":1},\"1\":{\"549\":1}}],[\"cond\",{\"1\":{\"444\":1,\"811\":1,\"893\":3,\"894\":10,\"895\":3}}],[\"conditioning\",{\"1\":{\"895\":1}}],[\"conditional\",{\"1\":{\"895\":1,\"921\":1}}],[\"conditionally\",{\"1\":{\"849\":1}}],[\"condition\",{\"1\":{\"476\":4,\"895\":1,\"937\":1}}],[\"conditioned\",{\"1\":{\"23\":1}}],[\"conditions\",{\"1\":{\"14\":1}}],[\"connections\",{\"1\":{\"746\":1,\"749\":1}}],[\"connection\",{\"1\":{\"96\":1,\"98\":1,\"143\":1,\"145\":1,\"663\":2,\"741\":1,\"832\":1}}],[\"concrete\",{\"1\":{\"257\":1}}],[\"conclusion\",{\"0\":{\"183\":1,\"354\":1,\"377\":1,\"394\":1},\"1\":{\"179\":1}}],[\"conceptualcaptiondatamodule\",{\"1\":{\"382\":1}}],[\"conceptual\",{\"1\":{\"176\":2,\"224\":2,\"341\":1,\"887\":1,\"888\":2}}],[\"concise\",{\"1\":{\"87\":1}}],[\"concatdataset\",{\"1\":{\"382\":3}}],[\"concatenate\",{\"0\":{\"441\":1},\"1\":{\"293\":1,\"441\":4}}],[\"concat\",{\"1\":{\"64\":2,\"67\":1,\"137\":1,\"141\":3,\"190\":2,\"364\":1,\"694\":1,\"751\":1,\"937\":1}}],[\"contour\",{\"1\":{\"816\":1}}],[\"contribution\",{\"1\":{\"388\":1}}],[\"contrast\",{\"0\":{\"349\":1,\"351\":1},\"1\":{\"347\":2}}],[\"contrast=0\",{\"1\":{\"293\":1}}],[\"contrastive\",{\"0\":{\"199\":1,\"246\":1,\"272\":1,\"385\":1,\"418\":1},\"1\":{\"172\":1,\"190\":1,\"267\":2,\"268\":1,\"270\":1,\"271\":2,\"272\":1,\"274\":10,\"305\":1,\"355\":2,\"368\":1,\"370\":1,\"373\":1,\"383\":2,\"385\":1,\"413\":1,\"418\":1}}],[\"continuous\",{\"1\":{\"605\":1}}],[\"continue\",{\"1\":{\"293\":1,\"697\":2}}],[\"contig\",{\"1\":{\"545\":4}}],[\"contiguity\",{\"1\":{\"540\":1}}],[\"contiguous\",{\"0\":{\"490\":1,\"491\":1},\"1\":{\"83\":2,\"119\":9,\"121\":3,\"122\":3,\"156\":1,\"401\":1,\"403\":3,\"420\":3,\"468\":1,\"469\":2,\"470\":2,\"490\":2,\"491\":10,\"492\":1,\"493\":2,\"494\":3,\"545\":7,\"663\":1,\"709\":1,\"724\":1,\"751\":1,\"963\":2,\"964\":1}}],[\"content\",{\"1\":{\"84\":1,\"709\":2}}],[\"contextmanager\",{\"1\":{\"807\":1}}],[\"contextlib\",{\"1\":{\"807\":2}}],[\"context设定的早期尝试\",{\"1\":{\"650\":1}}],[\"contextualized\",{\"1\":{\"735\":1}}],[\"contextual\",{\"1\":{\"87\":1,\"735\":1}}],[\"context\",{\"1\":{\"69\":2,\"83\":3,\"131\":2,\"225\":1,\"401\":7,\"420\":10,\"646\":1,\"650\":1,\"651\":1,\"703\":3,\"710\":6,\"720\":1,\"724\":9,\"733\":1}}],[\"containing\",{\"1\":{\"597\":2}}],[\"contain\",{\"1\":{\"53\":1,\"82\":1,\"92\":1}}],[\"convtranspose\",{\"1\":{\"899\":1}}],[\"convtranspose2d\",{\"1\":{\"255\":1,\"899\":1,\"963\":2}}],[\"convnext\",{\"1\":{\"510\":2}}],[\"convnets\",{\"1\":{\"272\":1,\"280\":5}}],[\"convirt基于对比学习的方法\",{\"1\":{\"413\":1}}],[\"convbench评估显示\",{\"1\":{\"335\":1}}],[\"conv4\",{\"1\":{\"156\":2}}],[\"conv3\",{\"1\":{\"152\":2,\"154\":2,\"156\":2}}],[\"conv2\",{\"1\":{\"146\":2,\"152\":2,\"154\":2,\"156\":2,\"963\":4}}],[\"conv2d\",{\"1\":{\"137\":2,\"141\":3,\"255\":6,\"266\":1,\"380\":1,\"426\":1,\"522\":1,\"899\":4,\"926\":19,\"963\":2,\"964\":2}}],[\"conv\",{\"1\":{\"137\":2,\"141\":6,\"145\":2,\"522\":1}}],[\"convs\",{\"1\":{\"137\":3,\"141\":3,\"145\":3}}],[\"conv1\",{\"1\":{\"146\":2,\"152\":2,\"154\":2,\"156\":2,\"963\":4}}],[\"conv1x1\",{\"1\":{\"121\":1}}],[\"conv1d\",{\"1\":{\"58\":1,\"59\":1,\"60\":1,\"65\":2,\"69\":2,\"83\":3,\"143\":2,\"145\":3,\"146\":2,\"152\":3,\"154\":4,\"156\":5,\"157\":1,\"663\":2}}],[\"conversation\",{\"1\":{\"342\":1,\"827\":1}}],[\"convert\",{\"1\":{\"53\":1,\"82\":1,\"187\":1,\"410\":1,\"412\":1,\"712\":1}}],[\"convention\",{\"1\":{\"100\":1}}],[\"convolution\",{\"1\":{\"100\":2,\"145\":1,\"387\":2}}],[\"consecutive\",{\"0\":{\"480\":1},\"1\":{\"293\":2,\"480\":5}}],[\"construct\",{\"1\":{\"14\":1}}],[\"consistency的过程\",{\"1\":{\"621\":1}}],[\"consistency的例子\",{\"1\":{\"621\":1}}],[\"consistency的大致原理是这样\",{\"1\":{\"621\":1}}],[\"consistency技术是在cot技术的基础之上\",{\"1\":{\"621\":1}}],[\"consistency\",{\"0\":{\"621\":1},\"1\":{\"14\":2,\"18\":2,\"26\":1,\"621\":1}}],[\"comboloss\",{\"1\":{\"592\":2}}],[\"combo\",{\"0\":{\"592\":1},\"1\":{\"592\":12}}],[\"combine\",{\"1\":{\"12\":1}}],[\"combination\",{\"1\":{\"7\":1}}],[\"compile\",{\"1\":{\"595\":1,\"597\":1}}],[\"composition\",{\"1\":{\"500\":1}}],[\"compose\",{\"1\":{\"264\":4,\"293\":5,\"359\":1,\"425\":2,\"582\":1,\"918\":1,\"926\":1,\"963\":1}}],[\"compatibility\",{\"1\":{\"721\":1}}],[\"compatible\",{\"1\":{\"492\":1}}],[\"comparison\",{\"0\":{\"47\":1}}],[\"compute\",{\"0\":{\"514\":1},\"1\":{\"359\":2,\"362\":3,\"383\":7,\"384\":1,\"385\":2,\"386\":2,\"514\":4,\"751\":1}}],[\"complement\",{\"1\":{\"848\":1}}],[\"complex\",{\"1\":{\"342\":1,\"622\":1}}],[\"complete\",{\"1\":{\"7\":1}}],[\"com\",{\"1\":{\"28\":2,\"61\":1,\"71\":2,\"84\":2,\"108\":1,\"124\":1,\"128\":1,\"130\":2,\"147\":2,\"164\":1,\"185\":2,\"193\":1,\"209\":1,\"219\":1,\"226\":1,\"267\":1,\"279\":1,\"294\":1,\"338\":1,\"347\":1,\"367\":1,\"378\":1,\"385\":1,\"387\":1,\"414\":1,\"424\":2,\"435\":3,\"582\":1,\"591\":1,\"663\":1,\"689\":1,\"696\":2,\"712\":2,\"739\":1,\"752\":1,\"798\":1,\"813\":1,\"818\":1,\"894\":1,\"927\":1}}],[\"community\",{\"1\":{\"834\":1}}],[\"comment\",{\"1\":{\"657\":1}}],[\"commercial\",{\"1\":{\"321\":2}}],[\"commitment\",{\"1\":{\"213\":1,\"963\":5}}],[\"committer\",{\"1\":{\"2\":2}}],[\"commoncrawl等网络数据隐含的社会偏见难以完全过滤\",{\"1\":{\"670\":1}}],[\"commoncrawl\",{\"1\":{\"666\":1,\"667\":1}}],[\"common\",{\"1\":{\"11\":1,\"52\":2,\"83\":3,\"264\":3,\"332\":1,\"516\":2,\"640\":1,\"647\":1,\"668\":1,\"823\":1}}],[\"cvae使我们能够解决输入到输出是一对多映射的问题\",{\"1\":{\"952\":1}}],[\"cvae\",{\"0\":{\"928\":1,\"936\":1,\"937\":1},\"1\":{\"928\":1,\"936\":3,\"937\":2,\"938\":1,\"952\":1}}],[\"cv领域的信号是在一个连续而且高维的空间\",{\"1\":{\"353\":1}}],[\"cvlab\",{\"1\":{\"108\":1}}],[\"cvpr\",{\"1\":{\"84\":1}}],[\"cvpr2024\",{\"1\":{\"84\":1}}],[\"cv\",{\"1\":{\"3\":1,\"354\":1,\"406\":1,\"823\":1}}]],\"version\":2}}")).map(([e,t])=>[e,jn(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:n,options:s,id:r}})=>{const o=An[n];e==="suggest"?self.postMessage([e,r,Et(t,o,s)]):e==="search"?self.postMessage([e,r,vt(t,o,s,"max")]):self.postMessage({suggestions:[e,r,Et(t,o,s)],results:[e,r,vt(t,o,s,__SLIMSEARCH_SORT_STRATEGY__)]})};
//# sourceMappingURL=index.js.map
