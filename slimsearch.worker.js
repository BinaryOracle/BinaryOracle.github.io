/**
* @vue/shared v3.5.13
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/const Se={},ze=()=>{},Ce=Object.assign,Oe=Array.isArray,D=e=>typeof e=="function",Me=e=>typeof e=="string",Ne=e=>typeof e=="symbol";let X;const L=()=>X||(X=typeof globalThis<"u"?globalThis:typeof self<"u"?self:typeof window<"u"?window:typeof global<"u"?global:{});/**
* @vue/reactivity v3.5.13
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/new Set(Object.getOwnPropertyNames(Symbol).filter(e=>e!=="arguments"&&e!=="caller").map(e=>Symbol[e]).filter(Ne));function P(e){const t=e&&e.__v_raw;return t?P(t):e}function Te(e){return e?e.__v_isRef===!0:!1}/**
* @vue/runtime-core v3.5.13
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/const v=[];function kt(e){v.push(e)}function It(){v.pop()}let W=!1;function Et(e,...t){if(W)return;W=!0;const n=v.length?v[v.length-1].component:null,o=n&&n.appContext.config.warnHandler,s=Fe();if(o)A(o,n,11,[e+t.map(r=>{var i,c;return(c=(i=r.toString)==null?void 0:i.call(r))!=null?c:JSON.stringify(r)}).join(""),n&&n.proxy,s.map(({vnode:r})=>`at <${re(n,r.type)}>`).join(`
`),s]);else{const r=[`[Vue warn]: ${e}`,...t];s.length&&r.push(`
`,...$e(s)),console.warn(...r)}W=!1}function Fe(){let e=v[v.length-1];if(!e)return[];const t=[];for(;e;){const n=t[0];n&&n.vnode===e?n.recurseCount++:t.push({vnode:e,recurseCount:0});const o=e.component&&e.component.parent;e=o&&o.vnode}return t}function $e(e){const t=[];return e.forEach((n,o)=>{t.push(...o===0?[]:[`
`],...Ve(n))}),t}function Ve({vnode:e,recurseCount:t}){const n=t>0?`... (${t} recursive calls)`:"",o=e.component?e.component.parent==null:!1,s=` at <${re(e.component,e.type,o)}`,r=">"+n;return e.props?[s,...Re(e.props),r]:[s+r]}function Re(e){const t=[],n=Object.keys(e);return n.slice(0,3).forEach(o=>{t.push(...Z(o,e[o]))}),n.length>3&&t.push(" ..."),t}function Z(e,t,n){return Me(t)?(t=JSON.stringify(t),n?t:[`${e}=${t}`]):typeof t=="number"||typeof t=="boolean"||t==null?n?t:[`${e}=${t}`]:Te(t)?(t=Z(e,P(t.value),!0),n?t:[`${e}=Ref<`,t,">"]):D(t)?[`${e}=fn${t.name?`<${t.name}>`:""}`]:(t=P(t),n?t:[`${e}=`,t])}const vt={sp:"serverPrefetch hook",bc:"beforeCreate hook",c:"created hook",bm:"beforeMount hook",m:"mounted hook",bu:"beforeUpdate hook",u:"updated",bum:"beforeUnmount hook",um:"unmounted hook",a:"activated hook",da:"deactivated hook",ec:"errorCaptured hook",rtc:"renderTracked hook",rtg:"renderTriggered hook",0:"setup function",1:"render function",2:"watcher getter",3:"watcher callback",4:"watcher cleanup function",5:"native event handler",6:"component event handler",7:"vnode hook",8:"directive hook",9:"transition hook",10:"app errorHandler",11:"app warnHandler",12:"ref function",13:"async component loader",14:"scheduler flush",15:"component update",16:"app unmount cleanup function"};function A(e,t,n,o){try{return o?e(...o):e()}catch(s){ee(s,t,n)}}function ee(e,t,n,o=!0){const s=t?t.vnode:null,{errorHandler:r,throwUnhandledErrorInProduction:i}=t&&t.appContext.config||Se;if(t){let c=t.parent;const l=t.proxy,u=`https://vuejs.org/error-reference/#runtime-${n}`;for(;c;){const a=c.ec;if(a){for(let h=0;h<a.length;h++)if(a[h](e,l,u)===!1)return}c=c.parent}if(r){A(r,null,10,[e,l,u]);return}}je(e,n,s,o,i)}function je(e,t,n,o=!0,s=!1){if(s)throw e;console.error(e)}const b=[];let x=-1;const S=[];let k=null,z=0;const De=Promise.resolve();let q=null;const Le=100;function Pe(e){let t=x+1,n=b.length;for(;t<n;){const o=t+n>>>1,s=b[o],r=M(s);r<e||r===e&&s.flags&2?t=o+1:n=o}return t}function We(e){if(!(e.flags&1)){const t=M(e),n=b[b.length-1];!n||!(e.flags&2)&&t>=M(n)?b.push(e):b.splice(Pe(t),0,e),e.flags|=1,te()}}function te(){q||(q=De.then(ne))}function Ae(e){Oe(e)?S.push(...e):k&&e.id===-1?k.splice(z+1,0,e):e.flags&1||(S.push(e),e.flags|=1),te()}function qe(e){if(S.length){const t=[...new Set(S)].sort((n,o)=>M(n)-M(o));if(S.length=0,k){k.push(...t);return}for(k=t,z=0;z<k.length;z++){const n=k[z];n.flags&4&&(n.flags&=-2),n.flags&8||n(),n.flags&=-2}k=null,z=0}}const M=e=>e.id==null?e.flags&2?-1:1/0:e.id;function ne(e){const t=ze;try{for(x=0;x<b.length;x++){const n=b[x];n&&!(n.flags&8)&&(n.flags&4&&(n.flags&=-2),A(n,n.i,n.i?15:14),n.flags&4||(n.flags&=-2))}}finally{for(;x<b.length;x++){const n=b[x];n&&(n.flags&=-2)}x=-1,b.length=0,qe(e),q=null,(b.length||S.length)&&ne(e)}}function St(e,t){const n=e.get(t)||0;if(n>Le){const o=t.i,s=o&&se(o.type);return ee(`Maximum recursive updates exceeded${s?` in component <${s}>`:""}. This means you have a reactive effect that is mutating its own dependencies and thus recursively triggering itself. Possible sources include component template, render function, updated hook or watcher source function.`,null,10),!0}return e.set(t,n+1),!1}const H=new Map,F=new Map;function zt(e,t){return F.has(e)?!1:(F.set(e,{initialDef:$(t),instances:new Set}),!0)}function $(e){return Je(e)?e.__vccOpts:e}function Ct(e,t){const n=F.get(e);n&&(n.initialDef.render=t,[...n.instances].forEach(o=>{t&&(o.render=t,$(o.type).render=t),o.renderCache=[],o.update()}))}function Ot(e,t){const n=F.get(e);if(!n)return;t=$(t),oe(n.initialDef,t);const o=[...n.instances];for(let s=0;s<o.length;s++){const r=o[s],i=$(r.type);let c=H.get(i);c||(i!==n.initialDef&&oe(i,t),H.set(i,c=new Set)),c.add(r),r.appContext.propsCache.delete(r.type),r.appContext.emitsCache.delete(r.type),r.appContext.optionsCache.delete(r.type),r.ceReload?(c.add(r),r.ceReload(t.styles),c.delete(r)):r.parent?We(()=>{r.parent.update(),c.delete(r)}):r.appContext.reload?r.appContext.reload():typeof window<"u"?window.location.reload():console.warn("[HMR] Root or manually mounted instance modified. Full reload required."),r.root.ce&&r!==r.root&&r.root.ce._removeChildStyle(i)}Ae(()=>{H.clear()})}function oe(e,t){Ce(e,t);for(const n in e)n!=="__file"&&!(n in t)&&delete e[n]}function Mt(e){return(t,n)=>{try{return e(t,n)}catch(o){console.error(o),console.warn("[HMR] Something went wrong during Vue component hot-reload. Full reload required.")}}}L().requestIdleCallback,L().cancelIdleCallback;const Nt={};{const e=L(),t=(n,o)=>{let s;return(s=e[n])||(s=e[n]=[]),s.push(o),r=>{s.length>1?s.forEach(i=>i(r)):s[0](r)}};t("__VUE_INSTANCE_SETTERS__",n=>n),t("__VUE_SSR_SETTERS__",n=>n)}const He=/(?:^|[-_])(\w)/g,Ue=e=>e.replace(He,t=>t.toUpperCase()).replace(/[-_]/g,"");function se(e,t=!0){return D(e)?e.displayName||e.name:e.name||t&&e.__name}function re(e,t,n=!1){let o=se(t);if(!o&&t.__file){const s=t.__file.match(/([^/\\]+)\.\w+$/);s&&(o=s[1])}if(!o&&e&&e.parent){const s=r=>{for(const i in r)if(r[i]===t)return i};o=s(e.components||e.parent.type.components)||s(e.appContext.components)}return o?Ue(o):n?"App":"Anonymous"}function Je(e){return D(e)&&"__vccOpts"in e}[...new Array(6)].map((e,t)=>`[vp-content] h${t+1}`).join(",");const{entries:Ge}=Object,{fromEntries:Be}=Object,Ye="ENTRIES",ie="KEYS",ce="VALUES",y="";class U{set;_type;_path;constructor(t,n){const o=t._tree,s=Array.from(o.keys());this.set=t,this._type=n,this._path=s.length>0?[{node:o,keys:s}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:n}=C(this._path);if(C(n)===y)return{done:!1,value:this.result()};const o=t.get(C(n));return this._path.push({node:o,keys:Array.from(o.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=C(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>C(t)).filter(t=>t!==y).join("")}value(){return C(this._path).node.get(y)}result(){switch(this._type){case ce:return this.value();case ie:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const C=e=>e[e.length-1],Ke=(e,t,n)=>{const o=new Map;if(typeof t!="string")return o;const s=t.length+1,r=s+n,i=new Uint8Array(r*s).fill(n+1);for(let c=0;c<s;++c)i[c]=c;for(let c=1;c<r;++c)i[c*s]=c;return le(e,t,n,o,i,1,s,""),o},le=(e,t,n,o,s,r,i,c)=>{const l=r*i;e:for(const u of e.keys())if(u===y){const a=s[l-1];a<=n&&o.set(c,[e.get(u),a])}else{let a=r;for(let h=0;h<u.length;++h,++a){const g=u[h],m=i*a,w=m-i;let d=s[m];const f=Math.max(0,a-n-1),p=Math.min(i-1,a+n);for(let _=f;_<p;++_){const I=g!==t[_],j=s[w+_]+ +I,T=s[w+_+1]+1,E=s[m+_]+1,O=s[m+_+1]=Math.min(j,T,E);O<d&&(d=O)}if(d>n)continue e}le(e.get(u),t,n,o,s,a,i,c+u)}};let ue=class N{_tree;_prefix;_size=void 0;constructor(t=new Map,n=""){this._tree=t,this._prefix=n}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[n,o]=V(this._tree,t.slice(this._prefix.length));if(n===void 0){const[s,r]=B(o);for(const i of s.keys())if(i!==y&&i.startsWith(r)){const c=new Map;return c.set(i.slice(r.length),s.get(i)),new N(c,t)}}return new N(n,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,Qe(this._tree,t)}entries(){return new U(this,Ye)}forEach(t){for(const[n,o]of this)t(n,o,this)}fuzzyGet(t,n){return Ke(this._tree,t,n)}get(t){const n=J(this._tree,t);return n!==void 0?n.get(y):void 0}has(t){return J(this._tree,t)?.has(y)??!1}keys(){return new U(this,ie)}set(t,n){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,G(this._tree,t).set(y,n),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,n){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const o=G(this._tree,t);return o.set(y,n(o.get(y))),this}fetch(t,n){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const o=G(this._tree,t);let s=o.get(y);return s===void 0&&o.set(y,s=n()),s}values(){return new U(this,ce)}[Symbol.iterator](){return this.entries()}static from(t){const n=new N;for(const[o,s]of t)n.set(o,s);return n}static fromObject(t){return N.from(Object.entries(t))}};const V=(e,t,n=[])=>{if(t.length===0||e==null)return[e,n];for(const o of e.keys())if(o!==y&&t.startsWith(o))return n.push([e,o]),V(e.get(o),t.slice(o.length),n);return n.push([e,t]),V(void 0,"",n)},J=(e,t)=>{if(t.length===0||!e)return e;for(const n of e.keys())if(n!==y&&t.startsWith(n))return J(e.get(n),t.slice(n.length))},G=(e,t)=>{const n=t.length;e:for(let o=0;e&&o<n;){for(const r of e.keys())if(r!==y&&t[o]===r[0]){const i=Math.min(n-o,r.length);let c=1;for(;c<i&&t[o+c]===r[c];)++c;const l=e.get(r);if(c===r.length)e=l;else{const u=new Map;u.set(r.slice(c),l),e.set(t.slice(o,o+c),u),e.delete(r),e=u}o+=c;continue e}const s=new Map;return e.set(t.slice(o),s),s}return e},Qe=(e,t)=>{const[n,o]=V(e,t);if(n!==void 0){if(n.delete(y),n.size===0)ae(o);else if(n.size===1){const[s,r]=n.entries().next().value;fe(o,s,r)}}},ae=e=>{if(e.length===0)return;const[t,n]=B(e);if(t.delete(n),t.size===0)ae(e.slice(0,-1));else if(t.size===1){const[o,s]=t.entries().next().value;o!==y&&fe(e.slice(0,-1),o,s)}},fe=(e,t,n)=>{if(e.length===0)return;const[o,s]=B(e);o.set(s+t,n),o.delete(s)},B=e=>e[e.length-1],Xe=(e,t)=>{const n=e._idToShortId.get(t);if(n!=null)return e._storedFields.get(n)},Ze=/[\n\r\p{Z}\p{P}]+/u,Y="or",de="and",et="and_not",tt=(e,t)=>{e.includes(t)||e.push(t)},he=(e,t)=>{for(const n of t)e.includes(n)||e.push(n)},pe=({score:e},{score:t})=>t-e,nt=()=>new Map,R=e=>{const t=new Map;for(const n of Object.keys(e))t.set(parseInt(n,10),e[n]);return t},ge=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,me={[Y]:(e,t)=>{for(const n of t.keys()){const o=e.get(n);if(o==null)e.set(n,t.get(n));else{const{score:s,terms:r,match:i}=t.get(n);o.score=o.score+s,o.match=Object.assign(o.match,i),he(o.terms,r)}}return e},[de]:(e,t)=>{const n=new Map;for(const o of t.keys()){const s=e.get(o);if(s==null)continue;const{score:r,terms:i,match:c}=t.get(o);he(s.terms,i),n.set(o,{score:s.score+r,terms:s.terms,match:Object.assign(s.match,c)})}return n},[et]:(e,t)=>{for(const n of t.keys())e.delete(n);return e}},ot=(e,t,n,o,s,r)=>{const{k:i,b:c,d:l}=r;return Math.log(1+(n-t+.5)/(t+.5))*(l+e*(i+1)/(e+i*(1-c+c*o/s)))},st=e=>(t,n,o)=>({term:t,fuzzy:typeof e.fuzzy=="function"?e.fuzzy(t,n,o):e.fuzzy??!1,prefix:typeof e.prefix=="function"?e.prefix(t,n,o):e.prefix===!0,termBoost:typeof e.boostTerm=="function"?e.boostTerm(t,n,o):1}),_e=(e,t,n,o)=>{for(const s of Object.keys(e._fieldIds))if(e._fieldIds[s]===n){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${o}" was not present in field "${s}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},rt=(e,t,n,o)=>{if(!e._index.has(o)){_e(e,n,t,o);return}const s=e._index.fetch(o,nt),r=s.get(t),i=r?.get(n);!r||typeof i>"u"?_e(e,n,t,o):i<=1?r.size<=1?s.delete(t):r.delete(n):r.set(n,i-1),e._index.get(o).size===0&&e._index.delete(o)},it={k:1.2,b:.7,d:.5},ct={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(Ze),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{console?.[e]?.(t)},autoVacuum:!0},ye={combineWith:Y,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:it},lt={combineWith:de,prefix:(e,t,n)=>t===n.length-1},ut={batchSize:1e3,batchWait:10},we={minDirtFactor:.1,minDirtCount:20},at={...ut,...we},be=Symbol("*"),ft=(e,t)=>{const n=new Map,o={...e._options.searchOptions,...t};for(const[s,r]of e._documentIds){const i=o.boostDocument?o.boostDocument(r,"",e._storedFields.get(s)):1;n.set(s,{score:i,terms:[],match:{}})}return n},xe=(e,t=Y)=>{if(e.length===0)return new Map;const n=t.toLowerCase();if(!(n in me))throw new Error(`Invalid combination operator: ${t}`);return e.reduce(me[n])},K=(e,t,n,o,s,r,i,c,l,u=new Map)=>{if(r==null)return u;for(const a of Object.keys(i)){const h=i[a],g=e._fieldIds[a],m=r.get(g);if(m==null)continue;let w=m.size;const d=e._avgFieldLength[g];for(const f of m.keys()){if(!e._documentIds.has(f)){rt(e,g,f,n),w-=1;continue}const p=c?c(e._documentIds.get(f),n,e._storedFields.get(f)):1;if(!p)continue;const _=m.get(f),I=e._fieldLength.get(f)[g],j=ot(_,w,e._documentCount,I,d,l),T=o*s*h*p*j,E=u.get(f);if(E){E.score+=T,tt(E.terms,t);const O=ge(E.match,n);O?O.push(a):E.match[n]=[a]}else u.set(f,{score:T,terms:[t],match:{[n]:[a]}})}}return u},dt=(e,t,n)=>{const o={...e._options.searchOptions,...n},s=(o.fields??e._options.fields).reduce((d,f)=>({...d,[f]:ge(o.boost,f)||1}),{}),{boostDocument:r,weights:i,maxFuzzy:c,bm25:l}=o,{fuzzy:u,prefix:a}={...ye.weights,...i},h=e._index.get(t.term),g=K(e,t.term,t.term,1,t.termBoost,h,s,r,l);let m,w;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const d=t.fuzzy===!0?.2:t.fuzzy,f=d<1?Math.min(c,Math.round(t.term.length*d)):d;f&&(w=e._index.fuzzyGet(t.term,f))}if(m)for(const[d,f]of m){const p=d.length-t.term.length;if(!p)continue;w?.delete(d);const _=a*d.length/(d.length+.3*p);K(e,t.term,d,_,t.termBoost,f,s,r,l,g)}if(w)for(const d of w.keys()){const[f,p]=w.get(d);if(!p)continue;const _=u*d.length/(d.length+p);K(e,t.term,d,_,t.termBoost,f,s,r,l,g)}return g},ke=(e,t,n={})=>{if(t===be)return ft(e,n);if(typeof t!="string"){const a={...n,...t,queries:void 0},h=t.queries.map(g=>ke(e,g,a));return xe(h,a.combineWith)}const{tokenize:o,processTerm:s,searchOptions:r}=e._options,i={tokenize:o,processTerm:s,...r,...n},{tokenize:c,processTerm:l}=i,u=c(t).flatMap(a=>l(a)).filter(a=>!!a).map(st(i)).map(a=>dt(e,a,i));return xe(u,i.combineWith)},Ie=(e,t,n={})=>{const{searchOptions:o}=e._options,s={...o,...n},r=ke(e,t,n),i=[];for(const[c,{score:l,terms:u,match:a}]of r){const h=u.length||1,g={id:e._documentIds.get(c),score:l*h,terms:Object.keys(a),queryTerms:u,match:a};Object.assign(g,e._storedFields.get(c)),(s.filter==null||s.filter(g))&&i.push(g)}return t===be&&s.boostDocument==null||i.sort(pe),i},ht=(e,t,n={})=>{n={...e._options.autoSuggestOptions,...n};const o=new Map;for(const{score:r,terms:i}of Ie(e,t,n)){const c=i.join(" "),l=o.get(c);l!=null?(l.score+=r,l.count+=1):o.set(c,{score:r,terms:i,count:1})}const s=[];for(const[r,{score:i,terms:c,count:l}]of o)s.push({suggestion:r,terms:c,score:i/l});return s.sort(pe),s};class pt{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(!t?.fields)throw new Error('SlimSearch: option "fields" must be provided');const n=t.autoVacuum==null||t.autoVacuum===!0?at:t.autoVacuum;this._options={...ct,...t,autoVacuum:n,searchOptions:{...ye,...t.searchOptions},autoSuggestOptions:{...lt,...t.autoSuggestOptions}},this._index=new ue,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=we,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[n,o]of this._index){const s={};for(const[r,i]of o)s[r]=Object.fromEntries(i);t.push([n,s])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,version:2}}addFields(t){for(let n=0;n<t.length;n++)this._fieldIds[t[n]]=n}}const gt=e=>new pt(e),mt=({documentCount:e,nextId:t,fieldIds:n,averageFieldLength:o,dirtCount:s,version:r},i)=>{if(r!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const c=gt(i);return c._documentCount=e,c._nextId=t,c._idToShortId=new Map,c._fieldIds=n,c._avgFieldLength=o,c._dirtCount=s??0,c._index=new ue,c},_t=(e,t)=>{const{index:n,documentIds:o,fieldLength:s,storedFields:r}=e,i=mt(e,t);i._documentIds=R(o),i._fieldLength=R(s),i._storedFields=R(r);for(const[c,l]of i._documentIds)i._idToShortId.set(l,c);for(const[c,l]of n){const u=new Map;for(const a of Object.keys(l))u.set(parseInt(a,10),R(l[a]));i._index.set(c,u)}return i},Q=(e,t)=>{const n=e.toLowerCase(),o=t.toLowerCase(),s=[];let r=0,i=0;const c=(u,a=!1)=>{let h;i===0?h=u.length>20?`… ${u.slice(-20)}`:u:a?h=u.length+i>100?`${u.slice(0,100-i)}… `:u:h=u.length>20?`${u.slice(0,20)} … ${u.slice(-20)}`:u,h&&s.push(h),i+=h.length,a||(s.push(["mark",t]),i+=t.length,i>=100&&s.push(" …"))};let l=n.indexOf(o,r);if(l===-1)return null;for(;l>=0;){const u=l+o.length;if(c(e.slice(r,l)),r=u,i>100)break;l=n.indexOf(o,r)}return i<100&&c(e.slice(r),!0),s},{entries:yt}=Object,wt=(e,t)=>t.contents.reduce((n,[,o])=>n+o,0)-e.contents.reduce((n,[,o])=>n+o,0),bt=(e,t)=>Math.max(...t.contents.map(([,n])=>n))-Math.max(...e.contents.map(([,n])=>n)),Ee=(e,t,n={},o="max")=>{const s={};return Ie(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...n}).forEach(r=>{const{id:i,terms:c,score:l}=r,u=i.includes("@"),a=i.includes("#"),[h,g]=i.split(/[#@]/),m=Number(h),w=c.sort((f,p)=>f.length-p.length).filter((f,p)=>c.slice(p+1).every(_=>!_.includes(f))),{contents:d}=s[m]??={title:"",contents:[]};if(u)d.push([{type:"customField",id:m,index:g,display:w.map(f=>r.c.map(p=>Q(p,f))).flat().filter(f=>f!==null)},l]);else{const f=w.map(p=>Q(r.h,p)).filter(p=>p!==null);if(f.length&&d.push([{type:a?"heading":"title",id:m,...a&&{anchor:g},display:f},l]),"t"in r&&r.t)for(const p of r.t){const _=w.map(I=>Q(p,I)).filter(I=>I!==null);_.length&&d.push([{type:"text",id:m,...a&&{anchor:g},display:_},l])}}}),yt(s).sort(([,r],[,i])=>(o?wt:bt)(r,i)).map(([r,{title:i,contents:c}])=>{if(!i){const l=Xe(t,r);l&&(i=l.h)}return{title:i,contents:c.map(([l])=>l)}})},ve=(e,t,n={})=>{const o=ht(t,e,{fuzzy:.2,maxFuzzy:3,...n}).map(({suggestion:s})=>s);return e.includes(" ")?o:o.filter(s=>!s.includes(" "))},xt=Be(Ge(JSON.parse("{\"/\":{\"documentCount\":123,\"nextId\":123,\"documentIds\":{\"0\":\"0\",\"1\":\"1\",\"2\":\"1#binary-oracle\",\"3\":\"1#elowen\",\"4\":\"2\",\"5\":\"2#环境配置-待完善\",\"6\":\"2#模型结构\",\"7\":\"2#lmaffordance3d\",\"8\":\"2#step-2-融合多模态空间特征\",\"9\":\"2#step-3-多模态特征投影到语言语义空间\",\"10\":\"2#step-6-拼接多模态嵌入与语言嵌入\",\"11\":\"2#step-8-降维适配器\",\"12\":\"2#step-9-解码器融合所有特征以预测可操作性特征\",\"13\":\"3\",\"14\":\"4\",\"15\":\"4#背景\",\"16\":\"4#模型结构\",\"17\":\"4#层次化点集特征学习\",\"18\":\"4#sampling-layer\",\"19\":\"4#grouping-layer\",\"20\":\"4#pointnet-layer\",\"21\":\"4#代码实现\",\"22\":\"4#单尺度分组分类模型\",\"23\":\"4#非均匀密度下稳定的特征学习\",\"24\":\"4#多尺度分组-multi-scale-grouping\",\"25\":\"4#多尺度分组分类模型\",\"26\":\"4#多分辨率分组-multi-resolution-grouping\",\"27\":\"5\",\"28\":\"5#核心\",\"29\":\"5#难点\",\"30\":\"5#解决方案\",\"31\":\"5#代码-pytorch版本\",\"32\":\"5#输入标准化\",\"33\":\"5#正则化损失\",\"34\":\"5#特征提取\",\"35\":\"5#分类任务\",\"36\":\"5#分割任务\",\"37\":\"5#缺陷\",\"38\":\"5#背景知识扫盲-可选\",\"39\":\"5#点云\",\"40\":\"5#对称函数\",\"41\":\"5#刚性运动\",\"42\":\"5#正交变换\",\"43\":\"6\",\"44\":\"7\",\"45\":\"7#环境搭建\",\"46\":\"7#数据预处理\",\"47\":\"7#模型架构\",\"48\":\"7#dataloader\",\"49\":\"7#bertembeddings\",\"50\":\"7#bertencoder\",\"51\":\"7#bertlayer\",\"52\":\"7#bertencoder-1\",\"53\":\"7#bertpooler\",\"54\":\"7#bertmodel\",\"55\":\"7#bertforsequenceclassification\",\"56\":\"7#bertattention\",\"57\":\"7#bertselfattention\",\"58\":\"7#bertselfoutput\",\"59\":\"7#bertattention-1\",\"60\":\"7#预训练\",\"61\":\"7#bertpredictionheadtransform\",\"62\":\"7#bertlmpredictionhead\",\"63\":\"7#bertpretrainingheads\",\"64\":\"7#bertforpretraining\",\"65\":\"7#其他下游任务\",\"66\":\"7#问答任务\",\"67\":\"7#代码实现\",\"68\":\"7#易混淆\",\"69\":\"7#token分类任务\",\"70\":\"7#多项选择任务\",\"71\":\"8\",\"72\":\"8#环境\",\"73\":\"8#背景\",\"74\":\"8#模型架构\",\"75\":\"8#encoder-decoder-结构\",\"76\":\"8#generator\",\"77\":\"8#encoder-结构\",\"78\":\"8#sublayerconnection\",\"79\":\"8#encoderlayer\",\"80\":\"8#encoder\",\"81\":\"8#decoder-结构\",\"82\":\"8#decoderlayer\",\"83\":\"8#decoder\",\"84\":\"8#多头自注意力\",\"85\":\"9\",\"86\":\"10\",\"87\":\"11\",\"88\":\"11#引言\",\"89\":\"11#介绍\",\"90\":\"11#训练\",\"91\":\"11#推理\",\"92\":\"11#文本描述生成\",\"93\":\"11#花卉图片分类\",\"94\":\"11#文字搜索图像\",\"95\":\"11#完整代码\",\"96\":\"11#小结\",\"97\":\"12\",\"98\":\"12#背景\",\"99\":\"12#模型结构\",\"100\":\"12#stage-1-representation-learning-表征学习\",\"101\":\"12#_1、image-text-contrastive-learning-itc-loss-clip-like\",\"102\":\"12#_2、image-text-matching-itm-loss-二分类task\",\"103\":\"12#_3、image-grounded-text-generation-itg-loss-gpt-like\",\"104\":\"12#stage-2-generative-learning-生成学习\",\"105\":\"13\",\"106\":\"13#原理\",\"107\":\"13#_0-数据下载\",\"108\":\"13#_1-图片预处理\",\"109\":\"13#_2-图片切割\",\"110\":\"13#_3-添加-class-token\",\"111\":\"13#_4-添加位置编码\",\"112\":\"13#_5-encoder\",\"113\":\"13#_6-多头自注意力\",\"114\":\"13#_7-mlp-head\",\"115\":\"13#效果对比\",\"116\":\"13#注意力可视化\",\"117\":\"13#混合模型探索\",\"118\":\"13#加载预训练模型\",\"119\":\"13#总结\",\"120\":\"14\",\"121\":\"15\",\"122\":\"16\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1,7],\"1\":[1],\"2\":[2,16],\"3\":[1,5],\"4\":[2,25],\"5\":[3,4],\"6\":[1,1],\"7\":[1,223],\"8\":[3,92],\"9\":[3,25],\"10\":[3,114],\"11\":[3,20],\"12\":[3,142],\"13\":[3],\"14\":[1,19],\"15\":[1,73],\"16\":[1,15],\"17\":[1,24],\"18\":[2,23],\"19\":[2,51],\"20\":[2,15],\"21\":[1,316],\"22\":[1,122],\"23\":[1,17],\"24\":[5,35],\"25\":[1,196],\"26\":[5,40],\"27\":[1,17],\"28\":[1,33],\"29\":[1,28],\"30\":[1,121],\"31\":[3,1],\"32\":[1,192],\"33\":[1,73],\"34\":[1,112],\"35\":[1,54],\"36\":[1,94],\"37\":[1,290],\"38\":[3],\"39\":[1,65],\"40\":[1,83],\"41\":[1,24],\"42\":[1,13],\"43\":[1],\"44\":[2,2],\"45\":[1,138],\"46\":[1,165],\"47\":[1],\"48\":[1,48],\"49\":[1,66],\"50\":[1],\"51\":[1,58],\"52\":[1,30],\"53\":[1,40],\"54\":[1,49],\"55\":[1,73],\"56\":[1],\"57\":[1,104],\"58\":[1,32],\"59\":[1,23],\"60\":[1,1],\"61\":[1,41],\"62\":[1,44],\"63\":[1,28],\"64\":[1,75],\"65\":[1,1],\"66\":[1,120],\"67\":[1,95],\"68\":[1,173],\"69\":[1,75],\"70\":[1,119],\"71\":[1,2],\"72\":[1,41],\"73\":[1,23],\"74\":[1,64],\"75\":[3,30],\"76\":[1,28],\"77\":[2],\"78\":[1,33],\"79\":[1,37],\"80\":[1,37],\"81\":[2],\"82\":[1,49],\"83\":[1,39],\"84\":[1,116],\"85\":[2,1],\"86\":[1],\"87\":[1,3],\"88\":[1,31],\"89\":[1,20],\"90\":[1,146],\"91\":[1,156],\"92\":[1,57],\"93\":[1,245],\"94\":[1,92],\"95\":[1,210],\"96\":[1,132],\"97\":[1,17],\"98\":[1,66],\"99\":[1,20],\"100\":[6,144],\"101\":[10,95],\"102\":[8,183],\"103\":[10,349],\"104\":[6,194],\"105\":[1,53],\"106\":[1,4],\"107\":[2,234],\"108\":[2,130],\"109\":[2,181],\"110\":[4,160],\"111\":[2,141],\"112\":[2,153],\"113\":[2,138],\"114\":[3,220],\"115\":[1,61],\"116\":[1,13],\"117\":[1,51],\"118\":[1,158],\"119\":[1,15],\"120\":[1],\"121\":[1],\"122\":[1,3]},\"averageFieldLength\":[1.67479674796748,77.6615358892304],\"storedFields\":{\"0\":{\"h\":\"主页\",\"t\":[\"知识星球: MetaMind , 小红书: BinaryOracle , CSDN: Binary Oracle\"]},\"1\":{\"h\":\"关于我们\"},\"2\":{\"h\":\"Binary Oracle\",\"t\":[\"一名普通但十分热爱探索技术的Coder\",\"开源框架 Spring committer\",\"Golang 开源网络库 netpoll committer\",\"Javaer 转型 3D - VL 方向研究\",\"现就读于四川大学\",\"有问题需要咨询的小伙伴，可以加微信备注来意:\"]},\"3\":{\"h\":\"Elowen\",\"t\":[\"CV 转 LLM 领域\",\"现就读于电子科技大学\"]},\"4\":{\"h\":\"LMAffordance3D 模型代码解读与复现\",\"t\":[\"Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions 论文代码解读与复现\",\"论文: https://arxiv.org/abs/2504.04744 代码: https://github.com/cn-hezhu/LMAffordance3D\"]},\"5\":{\"h\":\"环境配置 (待完善)\",\"t\":[\"建议用Linux或者Windows系统进行测试，MacOS系统某些包的加载和依赖关系上存在问题，不方便进行处理。\"]},\"6\":{\"h\":\"模型结构\",\"t\":[\"模型结构图\"]},\"7\":{\"h\":\"LMAffordance3D\",\"t\":[\"class LMAffordance3D(Blip2Base): ... def forward(self, img, point, description, label, inference_mode=False): ''' img: [B, 3, H, W] -> 输入图像 (batch_size, channels, height, width) point: [B, 3, 2048] -> 点云数据 (batch_size, dimensions, num_points) description: 自然语言指令 (e.g., \\\"Grasp the bottle\\\") label: 真实标签，即每个点对应的 affordance 概率分布 (B, 2048, 1) inference_mode: 是否为推理模式（True/False） ''' # 获取输入维度信息 B, C, H, W = img.size() B, D, N = point.size() device = img.device # 获取设备信息（CPU/GPU） # Step 1: 提取图像和点云的特征 # -------------------------------------------------- # 图像编码器：ResNet18 提取 2D 特征 F2D ∈ RB×CI×H×W img_feature = self.img_encoder(img) # shape: [B, CI, H', W'] # 点云编码器：PointNet++ 提取 3D 特征 F3D ∈ RB×CP×NP point_feature = self.point_encoder(point) # shape: [B, CP, NP] # Step 2: 融合多模态空间特征 # -------------------------------------------------- # 使用 MLP 和自注意力机制融合图像与点云特征 spatial_feature = self.fusion(img_feature, point_feature) # shape: [B, NS, CS] # Step 3: 多模态特征投影到语言语义空间 # -------------------------------------------------- # 将融合后的空间特征通过适配器上采样到与语言模型匹配的维度 if self.has_qformer: ... # 如果使用 Q-Former，则进行额外处理 else: multi_embeds = self.adapter_up(spatial_feature) # shape: [B, NS, CL] image_atts = None # 默认图像注意力掩码为空 # Step 4: 对自然语言指令进行 Tokenization # -------------------------------------------------- # 设置 tokenizer 的 padding 和 truncation 方向 self.llm_tokenizer.padding_side = \\\"right\\\" self.llm_tokenizer.truncation_side = 'left' # 对语言指令进行分词，转换为 token ID 并生成 attention mask text_input_tokens = self.llm_tokenizer( description, return_tensors=\\\"pt\\\", padding=\\\"longest\\\", # 填充至最长序列长度 truncation=True, # 截断过长文本 max_length=self.max_txt_len, # 最大文本长度 ).to(device) # Step 5: 获取语言嵌入 # -------------------------------------------------- # 使用 LLM 的 embedding 层将 token ID 转换为嵌入向量 inputs_embeds = self.llm_model.get_input_embeddings()(text_input_tokens.input_ids) # shape: [B, NL, CL] （NL=token数，CL=语言嵌入维度） # Step 6: 拼接多模态嵌入与语言嵌入 # -------------------------------------------------- # 调用 concat_input 函数，将图像+点云特征插入语言嵌入中 llm_inputs, llm_attention_mask = self.concat_input( inputs_embeds, text_input_tokens.attention_mask, multi_embeds, image_atts ) # llm_inputs: [B, NL + NS, CL] # llm_attention_mask: [B, NL + NS] # Step 7: 使用 Vision-Language Model 进行联合推理 # -------------------------------------------------- # 在混合精度下运行 LLM，融合语言与视觉特征 with self.maybe_autocast(): hidden_states = self.llm_model( inputs_embeds=llm_inputs, attention_mask=llm_attention_mask, return_dict=False, # 返回 tuple 格式输出 ) # Step 8: 降维适配器 # -------------------------------------------------- # 通过适配器层将 LLM 输出映射回合适维度 hidden_states = self.adapter_down(hidden_states) # shape: [B, NS + NL, CS] # 分割出 semantic feature 和 instructional feature # 视觉语义特征 和 语言指令理解特征 semantic_feature, instructional_feature = torch.split( hidden_states, split_size_or_sections=spatial_feature.size(1), dim=1 ) # Step 9: 解码器融合所有特征以预测可操作性特征 # -------------------------------------------------- # 使用 cross-attention 融合 instruction, semantic, spatial features affordance_feature = self.affordance_decoder( spatial_feature, instructional_feature, semantic_feature ) # shape: [B, NA, CA] # Step 10: 使用分割头预测最终的 3D 可操作性热图 # -------------------------------------------------- out = self.head(spatial_feature, affordance_feature, point_feature) # 输出 shape: [B, 2048, 1]，表示每个点是否具有特定可操作性的概率 # Step 11: 推理或训练分支 # -------------------------------------------------- if inference_mode == True: return out # 仅返回预测结果 else: loss_hm = self.loss_hm(out, label) # 计算 heatmap 的损失（focal + dice） loss = loss_hm * self.w_hm # 加权总损失 return { \\\"out\\\": out, \\\"loss\\\": loss, \\\"loss_hm\\\": loss_hm }\"]},\"8\":{\"h\":\"Step 2: 融合多模态空间特征\",\"t\":[\"class Fusion(nn.Module): def __init__(self, emb_dim = 512, num_heads = 4): super().__init__() self.emb_dim = emb_dim # 对点积结果进行缩放，防止 softmax 梯度消失或爆炸。 self.div_scale = self.emb_dim ** (-0.5) self.num_heads = num_heads # 对图像和点云特征进行 非线性增强和空间对齐 ，使得它们能够在统一的语义空间中进行有效的跨模态交互。 self.mlp = nn.Sequential( nn.Conv1d(self.emb_dim, 2*self.emb_dim, 1, 1), nn.BatchNorm1d(2*self.emb_dim), nn.ReLU(), nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) self.img_attention = Self_Attention(self.emb_dim, self.num_heads) self.point_attention = Self_Attention(self.emb_dim, self.num_heads) self.joint_attention = Self_Attention(self.emb_dim, self.num_heads) def forward(self, img_feature, point_feature): ''' i_feature: [B, C, H, W] p_feature: [B, C, N_p] HW = N_i ''' B, C, H, W = img_feature.size() img_feature = img_feature.view(B, self.emb_dim, -1) #[B, C, N_i] point_feature = point_feature[-1][1] # 对图像和点云特征进行 非线性增强和空间对齐 ，使得它们能够在统一的语义空间中进行有效的跨模态交互。 p_feature = self.mlp(point_feature) i_feature = self.mlp(img_feature) # 跨模态注意力矩阵: 每个点云点与图像中每个位置之间的相似度得分 phi = torch.bmm(p_feature.permute(0, 2, 1), i_feature)*self.div_scale #[B, N_p, N_i] # 每列是一个 softmax 分布（每个图像位置对应的所有点云点）, 表示：“对于图像中的每一个位置，应该关注哪些点云点？” phi_p = F.softmax(phi,dim=1) # 每行是一个 softmax 分布（每个点云点对应的所有图像位置）, 表示：“对于点云中的每一个点，应该关注图像中的哪些位置？” phi_i = F.softmax(phi,dim=-1) # I_enhance 是图像 patch 引导下提取的点云信息增强后的图像特征 # 它不是直接包含原始图像 patch 的语义 # 而是通过“点云中相关点”的方式重构图像 patch 的语义 I_enhance = torch.bmm(p_feature, phi_p) #[B, C, N_i] # P_enhance 是每个点云局部区域关键点引导下提取的图像信息增强后的点云关键点局部区域特征 P_enhance = torch.bmm(i_feature, phi_i.permute(0,2,1)) #[B, C, N_p] # 在跨模态融合后，进一步提取各自模态内部的语义一致性与结构关系，形成更稳定的联合表示。 I = self.img_attention(I_enhance.mT) #[B, N_i, C] P = self.point_attention(P_enhance.mT) #[B, N_p, C] # 将图像patch和点云点拼接成一个统一的token序列 # 使用自注意力机制提炼两个模态之间的语义一致性 joint_patch = torch.cat((P, I), dim=1) multi_feature = self.joint_attention(joint_patch) #[B, N_p+N_i, C] return multi_feature\"]},\"9\":{\"h\":\"Step 3: 多模态特征投影到语言语义空间\",\"t\":[\" # 将融合后的 3D 和 2D 特征从原始嵌入维度 (self.emb_dim) 映射到 LLM（语言模型）所使用的隐藏状态空间维度 （self.llm_model.config.hidden_size）。 self.adapter_up = nn.Sequential( nn.Linear(self.emb_dim, self.emb_dim), nn.ReLU(), nn.Linear(self.emb_dim, self.llm_model.config.hidden_size) )\"]},\"10\":{\"h\":\"Step 6: 拼接多模态嵌入与语言嵌入\",\"t\":[\"def concat_input(self, input_embeds, input_atts, multi_embeds, image_atts=None): ''' 将语言嵌入（text embeddings）与多模态嵌入（如图像、点云等）拼接在一起， 构建 Vision-Language Model (VLM) 所需的输入格式。 Args: input_embeds: (batch_size, sequence_length, hidden_size) - 语言 token 经过 embedding 层后的结果。 input_atts: (batch_size, sequence_length) - 语言部分的 attention mask（1 表示有效，0 表示填充）。 multi_embeds: (batch_size, n, hidden_size) - 多模态嵌入（如图像或点云特征），形状为 [B, n, H]。 image_atts: (batch_size, n), optional - 多模态数据的 attention mask，默认为全 1（即所有 token 都有效）。 Returns: llm_inputs: (batch_size, total_length, hidden_size) - 拼接后的输入嵌入，供 LLM 使用。 llm_attention_mask: (batch_size, total_length) - 对应的注意力掩码。 ''' # 初始化用于存储每个样本拼接后输入和 attention mask 的列表 llm_inputs = [] llm_attention_mask = [] # 获取 batch size bs = multi_embeds.size()[0] # 对每个样本单独处理（逐个拼接） for i in range(bs): # 获取当前样本中多模态嵌入的维度信息：(n, dim) _, n, dim = multi_embeds.size() # 计算当前语言输入中有多少个有效 token（非 padding） this_input_ones = input_atts[i].sum() # 拼接嵌入向量： # 语言前半段（有效的部分）+ 多模态嵌入 + 语言后半段（padding 部分） llm_inputs.append( torch.cat([ input_embeds[i][:this_input_ones], # 有效语言部分 multi_embeds[i], # 插入的多模态嵌入 input_embeds[i][this_input_ones:] # 剩余的语言 padding 部分 ]) ) # 构建 attention mask： if image_atts is None: # 如果没有提供 image_atts，则默认多模态 token 都是有效的（mask 全为 1） llm_attention_mask.append( torch.cat([ input_atts[i][:this_input_ones], torch.ones((n), device=multi_embeds.device, dtype=torch.long), input_atts[i][this_input_ones:] ]) ) else: # 否则使用给定的 image_atts 来标记哪些多模态 token 是有效的 llm_attention_mask.append( torch.cat([ input_atts[i][:this_input_ones], image_atts[i], input_atts[i][this_input_ones:] ]) ) # 将 list 转换为 batched tensor llm_inputs = torch.stack(llm_inputs, 0) llm_attention_mask = torch.stack(llm_attention_mask, 0) # 返回拼接好的输入和 attention mask return llm_inputs, llm_attention_mask\"]},\"11\":{\"h\":\"Step 8: 降维适配器\",\"t\":[\" # 降维适配器：将 LLM 输出的隐藏状态映射回原始嵌入维度（self.emb_dim） self.adapter_down = nn.Sequential( nn.Linear(self.llm_model.config.hidden_size, self.llm_model.config.hidden_size), nn.ReLU(), nn.Linear(self.llm_model.config.hidden_size, self.emb_dim) )\"]},\"12\":{\"h\":\"Step 9: 解码器融合所有特征以预测可操作性特征\",\"t\":[\"class Affordance_Decoder(nn.Module): def __init__(self, emb_dim, proj_dim): super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim self.cross_atten = Cross_Attention(emb_dim = self.emb_dim, proj_dim = self.proj_dim) self.fusion = nn.Sequential( nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) def forward(self, query, key, value): ''' query: [B, N_p + N_i, C] -> spatial_feature (query) key: [B, N_l, C] -> instructional_feature (key) value: [B, N_l, C] -> semantic_feature (value) ''' B, _, C = query.size() # 调整 key 和 value 的形状为 [B, C, N_l] key = key.view(B, C, -1) # [B, C, N_l] value = value.view(B, C, -1) # [B, C, N_l] # 使用 cross attention 获取两个注意力加权结果 Theta_1, Theta_2 = self.cross_atten(query, key.mT, value.mT) # 将两个注意力输出拼接在一起 joint_context = torch.cat((Theta_1.mT, Theta_2.mT), dim=1) # [B, 2C, N_p + N_i] # 使用 Conv1D 融合通道信息 affordance = self.fusion(joint_context) # [B, C, N_p + N_i] # 调整输出格式为 [B, N_p + N_i, C] affordance = affordance.permute(0, 2, 1) # [B, N_p + N_i, C] return affordance\",\"class Cross_Attention(nn.Module): def __init__(self, emb_dim, proj_dim): \\\"\\\"\\\" 多模态交叉注意力模块（Cross-Attention Module）， 用于融合来自语言模型的不同语义信息，增强空间特征表达。 Args: emb_dim: 输入特征维度（embedding dimension），例如 LLM 的 hidden size（如 4096） proj_dim: 投影维度，用于降低计算复杂度，在 attention 中使用 \\\"\\\"\\\" super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim # 定义投影层，将输入映射到低维空间以进行 attention 计算 self.proj_q = nn.Linear(self.emb_dim, proj_dim) # query 投影 self.proj_sk = nn.Linear(self.emb_dim, proj_dim) # sub key 投影 self.proj_sv = nn.Linear(self.emb_dim, proj_dim) # sub value 投影 self.proj_ek = nn.Linear(self.emb_dim, proj_dim) # scene key 投影 self.proj_ev = nn.Linear(self.emb_dim, proj_dim) # scene value 投影 # 缩放因子，用于 attention 分数归一化 self.scale = self.proj_dim ** (-0.5) # 层归一化（LayerNorm），用于稳定训练过程 self.layernorm = nn.LayerNorm(self.emb_dim) def forward(self, obj, sub, scene): \\\"\\\"\\\" 执行交叉注意力机制，融合不同来源的信息： - obj: 空间特征（spatial feature），作为 query； - sub: 指令理解特征（instructional feature），作为第一个 attention 的 key 和 value； - scene: 视觉语义特征（semantic feature），作为第二个 attention 的 key 和 value； Args: obj: [B, N_p + HW, C] → spatial_feature（query 来源） sub: [B, HW, C] → instructional_feature（key/value 来源之一） scene: [B, HW, C] → semantic_feature（key/value 来源之二） Returns: I_1: 经过 attention 加权后的输出（第一分支） I_2: 经过 attention 加权后的输出（第二分支） \\\"\\\"\\\" B, seq_length, C = obj.size() # 获取 batch size 和通道维度 # 将输入分别投影到低维空间，便于后续 attention 计算 query = self.proj_q(obj) # [B, N_q, proj_dim] s_key = self.proj_sk(sub) # [B, N_i, proj_dim] s_value = self.proj_sv(sub) # [B, N_i, proj_dim] e_key = self.proj_ek(scene) # [B, N_e, proj_dim] e_value = self.proj_ev(scene) # [B, N_e, proj_dim] # 第一个 cross attention：使用 sub 的 key 和 value 增强 query atten_I1 = torch.bmm(query, s_key.mT) * self.scale # [B, N_q, N_i] atten_I1 = atten_I1.softmax(dim=-1) # softmax 归一化 I_1 = torch.bmm(atten_I1, s_value) # [B, N_q, proj_dim] # 第二个 cross attention：使用 scene 的 key 和 value 增强 query atten_I2 = torch.bmm(query, e_key.mT) * self.scale # [B, N_q, N_e] atten_I2 = atten_I2.softmax(dim=-1) I_2 = torch.bmm(atten_I2, e_value) # [B, N_q, proj_dim] # 使用残差连接 + LayerNorm 增强稳定性 I_1 = self.layernorm(obj + I_1) # [B, N_q, emb_dim] I_2 = self.layernorm(obj + I_2) # [B, N_q, emb_dim] return I_1, I_2\"]},\"13\":{\"h\":\"3D-Vision Language\"},\"14\":{\"h\":\"简析PointNet++\",\"t\":[\"简析PointNet++\",\"论文: https://arxiv.org/abs/1706.02413 TensorFlow 版本代码: https://github.com/charlesq34/pointnet2 Pytorch 版本代码: https://github.com/yanx27/Pointnet_Pointnet2_pytorch\"]},\"15\":{\"h\":\"背景\",\"t\":[\"在PointNet中，网络对每一个点做低维到高维的映射，进行特征学习，然后把所有点映射到高维的特征通过最大池化最终表示全局特征。从本质上来说，要么对一个点做操作，要么对所有点做操作，实际上没有局部的概念(loal context) 。同时缺少 local context 在平移不变性上也有局限性（世界坐标系和局部坐标系）。对点云数据做平移操作后，所有的数据都将发生变化，导致所有的特征，全局特征都不一样了。对于单个的物体还好，可以将其平移到坐标系的中心，把他的大小归一化到一个球中，但是在一个场景中有多个物体时则不好办，需要对哪个物体做归一化呢？\",\"PointNet++ 解决了两个问题：如何生成点集的划分（Partitioning），以及如何通过局部特征学习器（Local Feature Learner）抽象点集或局部特征。\",\"生成点集的划分（Partitioning）：\",\"点集划分是指如何将一个大的点云分割成更小的、更易于管理的子集。这个过程类似于在传统的卷积神经网络中如何处理图像的小区域（或“patches”），以便可以在这些区域上应用局部操作。PointNet++需要一种方法来有效地将点云分割成多个部分，这样可以在每个部分上独立地学习特征。\",\"通过局部特征学习器（Local Feature Learner）抽象点集或局部特征：\",\"一旦点云被划分成小的子集，PointNet++的下一个任务是学习这些子集（或局部区域）的特征。这需要一个“局部特征学习器”，它能够从每个子集中提取有用的信息或特征。这与在传统CNN中学习图像局部区域特征的过程相似。\",\"两个问题是相关联的，因为：\",\"点集的划分必须产生跨分区的共同结构：为了能够在不同的局部子集上共享权重（类似于在CNN中权重共享的概念），PointNet++在进行点集划分时，需要确保这些划分具有一定的一致性或共同结构。这意味着即使是不同的局部子集，也应该以一种方式被处理，使得在它们之间可以共享学习到的特征表示的权重。这样做的目的是提高模型的效率和泛化能力，因为学习到的特征和权重可以在多个局部区域中复用。\",\"上述即为PointNet++设计中的两个核心挑战：\",\"如何有效地对点云进行分区，以便可以在这些分区上独立地学习特征。\",\"如何设计一个能够从这些局部分区中学习有用特征的机制，同时确保这些分区的处理方式允许在它们之间共享模型权重。 \",\"为了模仿传统卷积网络中的权重共享机制以提高学习效率和模型的泛化能力。\",\"PointNet++选择PointNet作为局部特征学习器（它是无序点云数据特征提取的高效算法）。\",\"可以理解为：PointNet++应用PointNet递归地对输入集进行嵌套分区。\"]},\"16\":{\"h\":\"模型结构\",\"t\":[\"以二维欧几里得空间为例，网络的分割和分类模型\",\"网络的每一组set abstraction layers主要包括3个部分：\",\"Sample layer : 对输入点进行采样，在这些点中选出若干个中心点。\",\"Grouping layer : 利用上一步得到的中心点将点集划分成若干个区域。\",\"PointNet layer : 对上述得到的每个区域进行编码，变成特征向量。\"]},\"17\":{\"h\":\"层次化点集特征学习\",\"t\":[\"层次化结构由多个set abstraction layers组成，在每个层上，一组点云被处理和抽象，以产生一个更少元素的新集合。set abstraction layers 由 Sampling layer、Grouping layer 和 PointNet layer 三部分组成。\",\"Sampling layer ：采样层 从输入点中选取一组点，定义局部区域的形心。\",\"Grouping layer ：通过查找形心点周围的“邻近点”来构建局部区域点集。\",\"PointNet layer ：使用mini-PointNet将局部区域编码为特征向量。\"]},\"18\":{\"h\":\"Sampling layer\",\"t\":[\"使用farthest point sampling（FPS）选择𝑁个点（相比于随机采样，该方法能更好的覆盖整个点集，具体选择多少个中心点以及邻域内的数量由超参数确定）\",\"FPS是一种在点云、图像处理或其他数据集中用于抽样的算法。目的是从一个大的数据集中选出一组代表性强的点，这些点彼此之间的最小距离尽可能大。\",\"作者通过FPS来抽样点集中较为重要的点。（即任务是找到点云集中的局部区域的中心点）\",\"可能存在的问题：计算成本、样本分布偏差（可能导致样本在高密度区域内过度集中，低密度区域则过于稀缺）、参数依赖（依赖初始点和距离度量方式的选择）、可能无法捕捉重要的几何细节。\"]},\"19\":{\"h\":\"Grouping layer\",\"t\":[\"文中作者通过Ball query来查询形心的邻居点。\",\"具体做法：给定两个超参数（每个区域中点的数量𝐾和query的半径𝑟），对于某个形心，Ball query找到该查询点在半径为𝑟范围内点，该范围确保局部区域的尺度是固定的。\",\"与K最近邻（kNN）查询相比，Ball query通过固定区域尺度而不是固定邻居数量来定义邻域。kNN查询寻找最近的K个邻居，但这可能导致所选邻域的实际尺寸随点的密度变化而变化，这在处理非均匀采样的数据时可能不是最优的选择。相反，Ball query通过确保每个局部区域都有一个固定的尺度，提高了模型在空间上的泛化能力。在实现时，通常会设置一个上限K，以限制每个局部区域中考虑的点的数量，以保持计算的可管理性。\",\"可改进的地方：对点云密度变换较为敏感、对参数选择依赖性高（半径太小可能无法有效捕获足够的局部详细，太大则可能导致不相关的点增多，使局部特征的表示不够精确）、计算效率问题、均匀性假设（Ball query是基于欧氏距离的均匀性假设）\",\"欧式距离的均匀性假设：即在欧氏空间中，两点的距离反映了这两点的实际相似度或关联度。\",\"基于以下前提： \",\"空间均匀性：空间是均匀和各向同性的，即任何方向上的度量都是等价的，距离的度量不受空间中位置的影响。\",\"距离直观性：在屋里空间或某些特定的抽象空间中，两个点之间的直线距离被认为是相似度或连接强度的直观表示。\",\"规模一致性：假设空间中所有区域的尺度或特征分布具有一定的一致性，即空间中的任何距离值具有相似的含义。\",\"总结: Grouping layer的任务是通过中心点找到邻居点，并将它们组织称为局部区域集。\"]},\"20\":{\"h\":\"PointNet layer\",\"t\":[\"局部坐标系转换：局部区域中的点转换成相对于形心的局部坐标系。\",\"局部区域中的每个点将相对于形心所在位置进行调整，以反映其相对位置。\",\"实现方法：通过将局部区域中的每个点-形心点的坐标来实现。\",\"特征编码：将转换后的坐标以及点的附加特征（文中的𝐶所表示的其他信息）一起送入mini-PointNet来提取局部区域中的特征。\",\"输出：利用相对坐标与点特征相结合的方式可以捕获局部区域中点与点之间的关系。\"]},\"21\":{\"h\":\"代码实现\",\"t\":[\"sample_and_group 这个函数的作用是从输入点云中：\",\"采样一些关键点\",\"为每个关键点构建局部邻域（局部区域）\",\"提取这些局部区域中的点及其特征\",\"def sample_and_group(npoint, radius, nsample, xyz, points, returnfps=False): \\\"\\\"\\\" Input: npoint: 采样的关键点数量 radius: 构建局部邻域的半径 nsample: 每个邻域内采样的关键点数量 xyz: 点云坐标数据 , [B, N, 3] points: 点的特征数据（可选）, [B, N, D] Return: new_xyz: 采样得到的关键点坐标, [B, npoint, nsample, 3] new_points: 每个关键点对应的局部区域点和特征, [B, npoint, nsample, 3+D] \\\"\\\"\\\" B, N, C = xyz.shape S = npoint # 使用 最远点采样（FPS） 从原始点云中选出 npoint 个具有代表性的点。 fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint] new_xyz = index_points(xyz, fps_idx) # [B, npoint, 3] # 对于每个选中的关键点，使用 球查询（Ball Query） 找出它周围距离小于 radius 的所有邻近点。 # 最多保留 nsample 个点，如果不够就重复最近的点来填充。 idx = query_ball_point(radius, nsample, xyz, new_xyz) # 把刚才找到的邻近点的坐标提取出来。 grouped_xyz = index_points(xyz, idx) # [B, npoint, nsample, 3] # 把它们相对于关键点的位置进行归一化（平移中心到以关键点为原点的局部坐标系上）。 grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C) # [B, npoint, nsample, 3] # 如果有额外的点特征（比如颜色、法线等），也一并提取。 if points is not None: grouped_points = index_points(points, idx) # 把邻近点的坐标和特征拼接在一起，形成最终的局部区域表示。 new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D] else: new_points = grouped_xyz_norm if returnfps: return new_xyz, new_points, grouped_xyz, fps_idx else: return new_xyz, new_points\",\"farthest_point_sample 这个函数实现的是最远点采样（Farthest Point Sampling, FPS）, 这是 PointNet++ 中用于从点云中选择具有代表性的采样点的一种策略。它的核心思想是：在点云中逐步选择离已选点尽可能远的点，使得采样点在整个点云空间中分布尽可能均匀 。\",\"def farthest_point_sample(xyz, npoint): \\\"\\\"\\\" Input: xyz: pointcloud data, [B, N, 3] npoint: number of samples Return: centroids: sampled pointcloud index, [B, npoint] \\\"\\\"\\\" device = xyz.device B, N, C = xyz.shape centroids = torch.zeros(B, npoint, dtype=torch.long).to(device) # 存储每次选出的“最远点”的索引。 distance = torch.ones(B, N).to(device) * 1e10 # 每个点到当前所有已选中心点的最小距离，初始设为一个极大值（1e10）。 farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device) # 初始时随机选择一个点作为第一个中心点。 batch_indices = torch.arange(B, dtype=torch.long).to(device) # 批次索引，用于快速访问每个 batch 的点。 # 重复 npoint 次，最终得到 npoint 个分布尽可能均匀的采样点索引。 for i in range(npoint): # 将当前选中的“最远点”索引保存下来； centroids[:, i] = farthest # （batch,npoint) # 取出当前最远点的坐标，用于后续计算其他点到该点的距离; centroid = xyz[batch_indices, farthest, :].view(B, 1, 3) # # （batch, 1 , 3) # 计算当前中心点与所有点之间的欧氏距离平方。 dist = torch.sum((xyz - centroid) ** 2, -1) # （batch,npoint) # 如果某个点到新中心点的距离比之前记录的“最小距离”还小，就更新它。 mask = dist < distance # 在 distance 中找到最大的那个距离对应的点，这就是下一个“最远点”。 distance[mask] = dist[mask] # （batch,npoint) # 在 distance 中找到最大的那个距离对应的点，这就是下一个“最远点”。 # 返回：一个元组：(values, indices)，分别是最大值和它们的位置索引。 farthest = torch.max(distance, -1)[1] # 返回位置索引 return centroids\",\"index_points 这个函数实现的是根据给定的索引 idx，从输入点云 points 中提取对应的点，形成一个新的子集。\",\"def index_points(points, idx): \\\"\\\"\\\" Input: points: input points data, [B, N, C] idx: sample index data, [B, S] Return: new_points:, indexed points data, [B, S, C] \\\"\\\"\\\" device = points.device B = points.shape[0] view_shape = list(idx.shape) view_shape[1:] = [1] * (len(view_shape) - 1) # 将view_shape的形状从[B, S]变成[B, 1]，便于广播 repeat_shape = list(idx.shape) repeat_shape[0] = 1 # 从[B, S]变成[1, S] # 从点云中根据索引提取特定点 (看不懂下面两行代码的话，可以先去了解一下python中的高级索引机制)。 batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape) new_points = points[batch_indices, idx, :] # （batch,npoint,3) return new_points\",\"query_ball_point 这个函数的作用是从点云中找出每个查询点周围一定半径范围内的邻近点索引。这个操作被称为 球查询（Ball Query）。\",\"def query_ball_point(radius, nsample, xyz, new_xyz): \\\"\\\"\\\" Input: radius: local region radius nsample: max sample number in local region xyz: all points, [B, N, 3] new_xyz: query points, [B, S, 3] Return: group_idx: grouped points index, [B, S, nsample] \\\"\\\"\\\" device = xyz.device B, N, C = xyz.shape _, S, _ = new_xyz.shape # 查询点数量（比如通过 FPS 得到的质心） # 构造一个从 0 到 N-1 的索引数组，代表原始点云中每个点的“身份证号” # 然后复制这个索引数组到每个 batch 和每个查询点上，形成 [B, S, N] 的结构 group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1]) # 计算每个查询点（new_xyz）与原始点（xyz）之间的平方欧氏距离 # 输出形状为 [B, S, N]：每个查询点对所有原始点的距离 sqrdists = square_distance(new_xyz, xyz) # 把距离超过 radius^2 的点全部替换为 N（一个非法索引），表示“这些人离我太远了，我不感兴趣。” group_idx[sqrdists > radius ** 2] = N # 对每个查询点的邻近点按索引排序（因为前面有 N，所以小的才是有效点） # 然后只保留前 nsample 个点 group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample] # 如果某个查询点附近的点太少，有些位置被标记为 N（无效）。 # 我们就用该查询点最近的那个点（第一个点）去填充这些空缺。 group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample]) mask = group_idx == N group_idx[mask] = group_first[mask] return group_idx # （batch,npoint,nsample)\",\"sample_and_group流程图\",\"sample_and_group_all 函数的作用是将整个点云视为一个“大局部区域”，不进行采样，直接对所有点进行特征提取，用于 PointNet++ 中的全局特征学习。\",\"def sample_and_group_all(xyz, points): \\\"\\\"\\\" Input: xyz: input points position data, [B, N, 3], 点云坐标数据 points: input points data, [B, N, D], 点云的额外特征（如法线、颜色等） Return: new_xyz: sampled points position data, [B, 1, 3] new_points: sampled points data, [B, 1, N, 3+D] \\\"\\\"\\\" device = xyz.device B, N, C = xyz.shape # 创建一个全零点作为“质心” # 虽然这个点没有实际意义，但它是为了统一接口设计的一个占位符 new_xyz = torch.zeros(B, 1, C).to(device) # 把原始点云 reshape 成一个大的局部区域 grouped_xyz = xyz.view(B, 1, N, C) # 如果有额外特征（比如法线、颜色），也一并加入 if points is not None: # 终输出的 new_points 是 [B, 1, N, 3+D]，代表每个 batch 中只有一组“大区域”的点及其特征 new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1) else: new_points = grouped_xyz return new_xyz, new_points # 全局质心点（0 位置）, 所有点组成的局部区域\",\"sample_and_group_all流程图\",\"PointNetSetAbstraction（点集抽象层） 是 PointNet++ 中的核心模块 ， 它的作用是负责从输入的点云数据中采样关键点，构建它们的局部邻域区域，并通过一个小型 PointNet 提取这些区域的高维特征，从而实现点云的分层特征学习。\",\"class PointNetSetAbstraction(nn.Module): def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all): super(PointNetSetAbstraction, self).__init__() self.npoint = npoint # 采样的关键点数量 self.radius = radius # 构建局部邻域的半径 self.nsample = nsample # 每个邻域内采样的关键点数量 self.mlp_convs = nn.ModuleList() self.mlp_bns = nn.ModuleList() last_channel = in_channel # 输入点的特征维度 for out_channel in mlp: self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1)) self.mlp_bns.append(nn.BatchNorm2d(out_channel)) last_channel = out_channel self.group_all = group_all def forward(self, xyz, points): \\\"\\\"\\\" Input: xyz: input points position data, [B, C, N] points: input points data, [B, D, N] Return: new_xyz: sampled points position data, [B, C, S] new_points_concat: sample points feature data, [B, D', S] \\\"\\\"\\\" xyz = xyz.permute(0, 2, 1) # [B, N, C] if points is not None: points = points.permute(0, 2, 1) # 如果 group_all=True，则对整个点云做全局特征提取。 if self.group_all: new_xyz, new_points = sample_and_group_all(xyz, points) else: # 否则使用 FPS（最远点采样）选关键点，再用 Ball Query 找出每个点的局部邻近点。 # 参数: 质点数量，采样半径，采样点数量，点坐标，点额外特征 new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points) # 局部特征编码（Mini-PointNet） # new_xyz: sampled points position data, [B, npoint, C] # new_points: sampled points data, [B, npoint, nsample, C+D] # 把邻域点的数据整理成适合卷积的格式 [B, C+D, nsample, npoint] new_points = new_points.permute(0, 3, 2, 1) # 使用多个 Conv2d + BatchNorm + ReLU 层提取特征 for i, conv in enumerate(self.mlp_convs): bn = self.mlp_bns[i] new_points = F.relu(bn(conv(new_points))) # [B, out_channel , nsample, npoint] # 对每个局部区域内所有点的最大响应值进行池化，得到该区域的固定长度特征表示。 # 在 new_points 的第 2 个维度（即每个局部邻域内的点数量维度）上做最大池化（max pooling）。 # 输出形状为 [B, out_channel, npoint]，即每个查询点有一个特征向量。 new_points = torch.max(new_points, 2)[0] # [B, out_channel, npoint] new_xyz = new_xyz.permute(0, 2, 1) # [B, C, npoint] return new_xyz, new_points # 查询点的位置(质心) ， 每个查询点点局部特征。\",\"最终每个采样得到的关键点所在的局部领域，都会被压缩为一个固定长度的特征向量。这个特征向量代表了这个局部区域的高维特征，它包含了这个区域内所有点的信息。\"]},\"22\":{\"h\":\"单尺度分组分类模型\",\"t\":[\"PointNet++ 的 单尺度分组（SSG）架构 ，通过多层 Set Abstraction 提取点云的层次化特征，并最终输出分类结果。\",\"Single-Scale Grouping (SSG)\",\"代码实现如下:\",\"# pointnet2_cls_ssg.py class get_model(nn.Module): # num_class: 输出类别数 # normal_channel: 是否包含法线信息（默认有 (x,y,z,nx,ny,nz)，否则只有 (x,y,z)） def __init__(self,num_class,normal_channel=True): super(get_model, self).__init__() in_channel = 6 if normal_channel else 3 self.normal_channel = normal_channel # PointNet++ 的核心就是逐层提取局部特征。这里的三个 SA 层构成了一个 三层分层特征学习结构 ： self.sa1 = PointNetSetAbstraction(npoint=512, radius=0.2, nsample=32, in_channel=in_channel, mlp=[64, 64, 128], group_all=False) self.sa2 = PointNetSetAbstraction(npoint=128, radius=0.4, nsample=64, in_channel=128 + 3, mlp=[128, 128, 256], group_all=False) self.sa3 = PointNetSetAbstraction(npoint=None, radius=None, nsample=None, in_channel=256 + 3, mlp=[256, 512, 1024], group_all=True) self.fc1 = nn.Linear(1024, 512) self.bn1 = nn.BatchNorm1d(512) self.drop1 = nn.Dropout(0.4) self.fc2 = nn.Linear(512, 256) self.bn2 = nn.BatchNorm1d(256) self.drop2 = nn.Dropout(0.4) self.fc3 = nn.Linear(256, num_class) def forward(self, xyz): B, _, _ = xyz.shape if self.normal_channel: norm = xyz[:, 3:, :] xyz = xyz[:, :3, :] else: norm = None l1_xyz, l1_points = self.sa1(xyz, norm) l2_xyz, l2_points = self.sa2(l1_xyz, l1_points) l3_xyz, l3_points = self.sa3(l2_xyz, l2_points) x = l3_points.view(B, 1024) x = self.drop1(F.relu(self.bn1(self.fc1(x)))) x = self.drop2(F.relu(self.bn2(self.fc2(x)))) x = self.fc3(x) x = F.log_softmax(x, -1) return x, l3_points\",\"完整的单尺度分组分类流程为:\",\"原始点云数据，首次sample，grouping，mini-PointNet后，得到:\",\"512 个关键点的坐标\",\"512 个关键点对应的局部区域特征向量\",\"二次sample，grouping，mini-PointNet后，得到:\",\"128 个关键点的坐标\",\"128 个关键点对应的局部区域特征向量\",\"三次sample，grouping，mini-PointNet后，得到:\",\"1 个关键点的坐标\",\"1 个关键点对应的全局区域特征向量\",\"获取全局区域特征向量后，通过全连接层进行分类。\"]},\"23\":{\"h\":\"非均匀密度下稳定的特征学习\",\"t\":[\"由于点集在不同区域可能会有不同的采样密度，这种非均匀性为点集特征学习带来了显著挑战。在密集采样的区域中学到的特征可能无法很好地泛化到稀疏采样的区域，反之亦然。因此，为了解决这一问题，PointNet++提出了密度自适应PointNet层，包含两种适应性特征学习层：多尺度分组（Multi-Scale Grouping, MSG）和多分辨率分组（Multi-Resolution Grouping, MRG）。\"]},\"24\":{\"h\":\"多尺度分组 （Multi-Scale Grouping）\",\"t\":[\"MSG通过应用不同尺度的分组层（按照不同的搜索半径或领域大小对点集进行分组），然后通过对应的PointNets提取每个尺度上的特征来捕获多尺度模式。不同尺度的特征被串联形成多尺度特征向量。这种方法使网络能够通过在训练期间随机丢弃输入点（称为随机输入丢弃 - random input dropout）来学习优化的策略，以结合来自不同尺度的特征。这样，网络在训练时被呈现了不同稀疏度的点集，从而学会根据输入数据的变化自适应地加权不同尺度上检测到的模式。\",\"多尺度分组\",\"具体来说，在MSG中，网络对于每个选定的形心点，按照几个预定义的半径值来搜索周围的邻近点。每个半径定义了一个局部邻域的大小，因此每个质心将根据这些不同的半径值与其周围点形成多个点集群。这样，对于每个质心点，网络不是只捕获一个尺度上的局部特征，而是能够捕获多个尺度上的局部特征。\",\"每个尺度（或每组邻域大小）的点集群都将独立地送入对应的PointNet网络进行特征提取，之后这些不同尺度上提取的特征被串联起来，形成一个综合的多尺度特征表示。这种方法使得网络能够在细节丰富的区域（通过较小的邻域尺度捕获细节）和稀疏采样的区域（通过较大的邻域尺度避免过度稀疏的问题）中均能有效提取特征。\"]},\"25\":{\"h\":\"多尺度分组分类模型\",\"t\":[\"PointNetSetAbstractionMsg 这个模块实现了 PointNet++ 中的 多尺度特征提取机制 ：对于每个局部区域，使用多个不同大小的邻域球（multi-scale ball query），分别提取特征，然后将这些不同尺度的特征拼接在一起，以获得更强的局部几何感知能力。\",\"class PointNetSetAbstractionMsg(nn.Module): def __init__(self, npoint, radius_list, nsample_list, in_channel, mlp_list): super(PointNetSetAbstractionMsg, self).__init__() self.npoint = npoint # 要采样的质心点数量 self.radius_list = radius_list # 不同尺度的查询半径列表 self.nsample_list = nsample_list # 对应半径下最多取多少邻近点 self.conv_blocks = nn.ModuleList() self.bn_blocks = nn.ModuleList() # 为每个尺度构建一个独立的小型 PointNet（Conv2d + BN + ReLU） # 每个尺度可以有不同的网络深度和宽度 # 所有尺度的网络并行运行，最后拼接结果 for i in range(len(mlp_list)): convs = nn.ModuleList() bns = nn.ModuleList() last_channel = in_channel + 3 for out_channel in mlp_list[i]: convs.append(nn.Conv2d(last_channel, out_channel, 1)) bns.append(nn.BatchNorm2d(out_channel)) last_channel = out_channel self.conv_blocks.append(convs) self.bn_blocks.append(bns) def forward(self, xyz, points): \\\"\\\"\\\" Input: xyz: input points position data, [B, C, N] points: input points data, [B, D, N] Return: new_xyz: sampled points position data, [B, C, S] new_points_concat: sample points feature data, [B, D', S] \\\"\\\"\\\" xyz = xyz.permute(0, 2, 1) # [B, N, C] if points is not None: points = points.permute(0, 2, 1) B, N, C = xyz.shape S = self.npoint # 使用 FPS（最远点采样）选出 S 个关键点作为局部区域中心 new_xyz = index_points(xyz, farthest_point_sample(xyz, S)) new_points_list = [] for i, radius in enumerate(self.radius_list): K = self.nsample_list[i] # 对每个半径 radius，找出该尺度下每个质心点周围的邻近点 group_idx = query_ball_point(radius, K, xyz, new_xyz) grouped_xyz = index_points(xyz, group_idx) # 把这些点的坐标归一化到以质心为中心的局部坐标系下 grouped_xyz -= new_xyz.view(B, S, 1, C) # 如果有额外特征，也一并加入 if points is not None: grouped_points = index_points(points, group_idx) grouped_points = torch.cat([grouped_points, grouped_xyz], dim=-1) else: grouped_points = grouped_xyz # 对每个尺度的局部点集应用对应的 Conv2d + BN + ReLU grouped_points = grouped_points.permute(0, 3, 2, 1) # [B, D, K, S] for j in range(len(self.conv_blocks[i])): conv = self.conv_blocks[i][j] bn = self.bn_blocks[i][j] grouped_points = F.relu(bn(conv(grouped_points))) # 使用最大池化聚合局部信息，生成固定长度的特征向量 new_points = torch.max(grouped_points, 2)[0] # [B, D', S] # 所有尺度的特征保存到 new_points_list new_points_list.append(new_points) new_xyz = new_xyz.permute(0, 2, 1) # 把不同尺度学到的特征拼接在一起，形成最终的局部特征表示 new_points_concat = torch.cat(new_points_list, dim=1) # 最终输出就是： 一组新的关键点位置； 每个关键点的多尺度特征表示 return new_xyz, new_points_concat\",\"pointnet2_cls_msg 这个模型使用了 PointNet++ 的 多尺度分组（MSG）策略 ，通过多个局部区域球查询提取不同尺度的局部特征，逐层抽象后融合成全局特征，最后通过全连接层完成分类任务。\",\"# pointnet2_cls_msg.py class get_model(nn.Module): def __init__(self,num_class,normal_channel=True): super(get_model, self).__init__() in_channel = 3 if normal_channel else 0 self.normal_channel = normal_channel self.sa1 = PointNetSetAbstractionMsg(512, [0.1, 0.2, 0.4], [16, 32, 128], in_channel,[[32, 32, 64], [64, 64, 128], [64, 96, 128]]) self.sa2 = PointNetSetAbstractionMsg(128, [0.2, 0.4, 0.8], [32, 64, 128], 320,[[64, 64, 128], [128, 128, 256], [128, 128, 256]]) self.sa3 = PointNetSetAbstraction(None, None, None, 640 + 3, [256, 512, 1024], True) self.fc1 = nn.Linear(1024, 512) self.bn1 = nn.BatchNorm1d(512) self.drop1 = nn.Dropout(0.4) self.fc2 = nn.Linear(512, 256) self.bn2 = nn.BatchNorm1d(256) self.drop2 = nn.Dropout(0.5) self.fc3 = nn.Linear(256, num_class) def forward(self, xyz): B, _, _ = xyz.shape if self.normal_channel: norm = xyz[:, 3:, :] xyz = xyz[:, :3, :] else: norm = None l1_xyz, l1_points = self.sa1(xyz, norm) l2_xyz, l2_points = self.sa2(l1_xyz, l1_points) l3_xyz, l3_points = self.sa3(l2_xyz, l2_points) x = l3_points.view(B, 1024) x = self.drop1(F.relu(self.bn1(self.fc1(x)))) x = self.drop2(F.relu(self.bn2(self.fc2(x)))) x = self.fc3(x) x = F.log_softmax(x, -1) return x,l3_points\",\"MSG的关键优点在于它通过在训练期间的随机输入丢弃（即随机移除一部分输入点）来模拟不同的采样密度，从而训练网络在面对实际应用中可能遇到的各种采样密度时，能够自适应地选择最适合的特征尺度进行组合，以实现最佳的性能。这种方法大大增强了网络处理非均匀采样数据的能力，提高了模型的泛化性和稳健性。\",\"在训练时引入不同密度的点集情况，使网络能学习不同采样密度下局部点云特征的提取，捕获密集到稀疏采样区域内的多尺度信息 -- 通过随机丢弃来模拟不同密度的采样，使网络能够应对实际中各种密度变换的情况-提高模型的泛化性能。\",\"MSG相当于并联了多个hierarchical structure，每个结构中心点不变，但是尺度不同。通过PointNet获取每个形心多尺度信息，之后concat形成该区域提取的总特征。在训练时引入随机丢弃形心来模拟不同密度情况，提高算法鲁棒性。\"]},\"26\":{\"h\":\"多分辨率分组（Multi-Resolution Grouping）\",\"t\":[\"MSG方法虽然有效，但在计算上可能非常昂贵，尤其是在低层次上对每个质心点运行局部PointNet时。为此，MRG为一种低成本的替代方案。\",\"MRG通过结合来自不同分辨率的特征来实现效率和适应性的平衡。具体而言，MRG策略在处理每个局部区域时，不仅考虑从当前分辨率下抽象得到的特征，还考虑了从更低分辨率（即上一层级）直接提取的特征。这两种特征被concat为一个复合特征向量，为后续的处理步骤提供信息。\",\"多分辨率分组\",\"在MRG中，某一层次𝐿𝑖的区域特征是通过将来自下一级𝐿𝑖−1的子区域特征总结后的向量与直接处理该局部区域所有原始点的单个PointNet得到的特征向量进行concat得到的。当局部区域的密度较低时，由于子区域在计算第一个向量时包含的点更稀疏，因此可能比第二个向量更不可靠。在这种情况下，应该更高地加权第二个向量。相反，当局部区域的密度较高时，第一个向量提供了更细致的信息，因为它能够在更低层次上递归地检视更高分辨率。\",\"来自下一级的特征：首先，将来自下一级（更高分辨率）的特征进行汇总，形成一个特征向量。这一过程通过对每个子区域应用集合抽象层（set abstraction level）完成。\",\"直接处理的原始点特征：另一部分特征是通过在当前分辨率直接对所有原始点应用单个PointNet得到的。\"]},\"27\":{\"h\":\"简析PointNet\",\"t\":[\"简析PointNet网络模型及其背后原理\",\"论文: https://arxiv.org/abs/1612.00593 TensorFlow 版本代码: https://github.com/charlesq34/pointnet Pytorch 版本代码: https://github.com/fxia22/pointnet.pytorch\"]},\"28\":{\"h\":\"核心\",\"t\":[\"问题背景: 点云是三维几何数据的一种重要表示形式，但由于其无序性和非规则性，传统卷积神经网络难以直接处理。\",\"❌ 传统方法的缺陷 ：\",\"将点云转换为体素网格（voxel grid）或图像视图（multi-view rendering）， 这些方法会导致信息损失、计算量大、不灵活等问题。\",\"🌟 PointNet 的创新点 ：\",\"直接以点集作为输入，避免了复杂的预处理；\",\"设计了一个统一架构，适用于分类、物体分割和场景语义解析；\",\"利用对称函数（如最大池化）实现点集顺序不变性；\",\"引入 T-Net（空间变换网络）标准化输入点云和特征空间。\"]},\"29\":{\"h\":\"难点\",\"t\":[\"点云的无序性（Unordered）: 点云是点的集合，没有固定顺序；模型必须对输入点的排列顺序不敏感（permutation invariant）。\",\"点之间存在相互作用（Interaction among points）: 点与点之间有空间关系，需要捕捉局部结构。\",\"对几何变换的不变性（Invariance under transformations）: 模型输出应不受刚性变换影响（如旋转、平移）。\",\"输入点云可能缺失或包含噪声（Missing or noisy points）: 实际采集的点云常有遮挡、稀疏、异常值等问题。\"]},\"30\":{\"h\":\"解决方案\",\"t\":[\"✅ 难点 1：点云的无序性 → 使用对称函数（Symmetric Function）\",\"使用 max pooling 作为对称函数，聚合所有点的信息；\",\"所有点经过共享参数的 MLP 提取特征；\",\"最终输出与点的顺序无关；\",\"原理说明：\",\"f({x1, ..., xn}) ≈ g(h(x1), ..., h(xn)) = γ(MAX(h(x1), ..., h(xn)))\",\"其中：\",\"h(xi) 是每个点的高维特征；\",\"MAX 是 max pooling 函数；\",\"γ 是后续的全连接网络；\",\"整个函数 f 是对称的，即对点顺序不敏感。\",\"效果：\",\"实验证明 max pooling 比排序、RNN、average pooling 更有效；s\",\"PointNet 可以处理任意顺序的点集；\",\"✅ 难点 2：点之间的相互作用 → 设计局部 + 全局信息融合机制\",\"在分割任务中，将全局特征与每个点的局部特征拼接起来；\",\"这样每个点在预测标签时都能看到整个物体的上下文；\",\"效果：\",\"显著提升了分割性能；\",\"让模型既关注局部细节，又理解整体结构；\",\"✅ 难点 3：对几何变换的不变性 → 引入 T-Net（空间变换网络）\",\"引入两个空间变换网络： \",\"STN3d：对输入点云做刚性变换（3×3 矩阵）；\",\"STNkd：对特征空间做变换（64×64 矩阵）；\",\"加入正则项约束变换矩阵接近正交：\",\"L_reg = ||I - A @ A^T||_F^2\",\"效果：\",\"PointNet 对点云的旋转、平移等变换具有鲁棒性；\",\"提升了模型的泛化能力和稳定性；\",\"✅ 难点 4：输入点云可能缺失或含有异常点 → 理论分析保证模型鲁棒性\",\"理论上证明 PointNet 学到的是一个“关键点集”（critical point set），即只依赖一小部分关键点就能判断整体形状；\",\"即使丢失一些点或加入异常点，只要关键点还在，结果就不会变；\",\"定理表明：\",\"小扰动不会改变函数输出；\",\"网络输出由一个有限子集 CS 决定（大小不超过 bottleneck 维度 K）；\",\"CS 是关键点集合，NS 是最大可容忍的点云范围；\",\"实验验证：\",\"即使 50% 的点缺失，分类准确率仅下降约 3.7%；\",\"对异常点也有一定容忍能力；\",\"✅ 总结: PointNet 通过 max pooling 实现对称性，结合 T-Net 实现变换不变性，并通过局部+全局特征融合机制实现强大的点云建模能力，解决了点云处理中的四大技术难点，为后续三维深度学习奠定了基础。\"]},\"31\":{\"h\":\"代码(Pytorch版本)\",\"t\":[\"PointNet网络模型结构图\"]},\"32\":{\"h\":\"输入标准化\",\"t\":[\"在 PointNet 架构中，第一层是一个叫做 STN3d（Spatial Transformer Network for 3D points） 的模块，它的目标是：\",\"✅ 对输入的点云做刚性变换（如旋转 + 平移），使其姿态统一，提升模型鲁棒性。\",\"这是因为在实际采集过程中，点云的姿态可能各不相同（比如椅子朝向不同、扫描角度不同等），如果不加处理，会影响特征提取的一致性。\",\"STN3d 是一个小型神经网络，专门用于预测一个 3×3 的变换矩阵 ，这个矩阵表示对点云所做的变换（通常是旋转或反射）。\",\"它具有以下特点：\",\"输入是原始点云（shape: (B, 3, N)）；\",\"输出是一个变换矩阵（shape: (B, 3, 3)）；\",\"这个变换矩阵是近似正交的，保证变换是刚性的；\",\"变换矩阵会通过 torch.bmm() 应用到原始点云上（这一步不在 STN3d 类中）；\",\"目的是让点云“摆正”，便于后续处理。\",\"代码实现:\",\"class STN3d(nn.Module): def __init__(self): super(STN3d, self).__init__() # 使用 1D 卷积 处理点云数据（每个点有 3 个坐标值） # kernel_size=1 表示只在通道维度操作，不考虑空间邻域关系 # 提取每一点的特征向量（从 3 → 64 → 128 → 1024） self.conv1 = torch.nn.Conv1d(3, 64, 1) self.conv2 = torch.nn.Conv1d(64, 128, 1) self.conv3 = torch.nn.Conv1d(128, 1024, 1) # 经过全局池化后得到一个全局特征向量（1024维） # 用全连接层逐步压缩到 9 个输出 → 对应一个 3x3 的变换矩阵 self.fc1 = nn.Linear(1024, 512) self.fc2 = nn.Linear(512, 256) self.fc3 = nn.Linear(256, 9) # 所有卷积和 FC 层后面都加了 BN 和 ReLU，帮助训练稳定收敛 self.relu = nn.ReLU() self.bn1 = nn.BatchNorm1d(64) self.bn2 = nn.BatchNorm1d(128) self.bn3 = nn.BatchNorm1d(1024) self.bn4 = nn.BatchNorm1d(512) self.bn5 = nn.BatchNorm1d(256) # x: (batch,3,point_size) def forward(self, x): # 获取当前 batch 的大小（有多少组点云） batchsize = x.size()[0] # CNN 逐点，通道维度特征提取阶段 x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) # x: (batch,1024,point_size) # 全局最大池化（Global Max Pooling） # 从所有点中选出每个通道的最大响应值，作为整个点云的“抽象”表示，shape: (B, 1024) # 返回：一个元组：(values, indices)，分别是最大值和它们的位置索引。 x = torch.max(x, 2, keepdim=True)[0] # x: (batch,1024,1) x = x.view(-1, 1024) # x: (batch,1024) # 全连接层预测变换矩阵 x = F.relu(self.bn4(self.fc1(x))) x = F.relu(self.bn5(self.fc2(x))) x = self.fc3(x) # x: (batch,9) # 加上单位矩阵作为初始偏置 # 初始假设变换为恒等变换（不做任何变化） iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1) if x.is_cuda: iden = iden.cuda() # 让网络从一个小扰动开始学习，更容易训练 x = x + iden # 最终 reshape 成 3x3 矩阵返回 x = x.view(-1, 3, 3) return x\",\"标准化的意义:\",\"✅ 1. 解决点云姿态不一致问题\",\"输入点云可能来自不同角度、不同位置；\",\"T-Net 把它们“对齐”到一个标准姿态；\",\"这样 PointNet 后续的特征提取更稳定。\",\"✅ 2. 提升模型鲁棒性\",\"如果没有 T-Net，PointNet 必须自己学会对各种姿态都识别准确；\",\"加入 T-Net 后，相当于加了一个“预处理层”，让模型更容易训练和泛化。\",\"神经网络的输出在训练初期往往接近于零，如果直接作为变换矩阵，会导致非正交、不稳定。PointNet 通过“加单位矩阵”的方式，让变换矩阵从一个恒等变换开始学习，并结合正则化损失，逐步向正交矩阵靠拢，从而保证变换是刚性的、稳定的。\"]},\"33\":{\"h\":\"正则化损失\",\"t\":[\"feature_transform_regularizer 是 PointNet 中用于约束变换矩阵接近正交性的正则化损失函数。\",\"🧠 为什么需要这个正则化项？\",\"在 PointNet 中，为了提升模型对点云姿态变化的鲁棒性，引入了两个变换网络：\",\"STN3d: 对原始点云做刚性变换（如旋转、反射），使其标准化。\",\"STNkd: 对特征空间做变换，使特征分布更稳定。\",\"这两个网络输出的是变换矩阵（分别是 3×3 和 k×k 的矩阵）。但由于它们是神经网络直接预测出来的，并不能保证这些矩阵是正交矩阵（orthogonal matrix） 。\",\"❗而只有正交矩阵才能表示刚性变换（rigid transformation），即只改变物体的方向而不改变形状和大小。\",\"所以我们需要加一个正则化项来鼓励变换矩阵接近正交矩阵 , 这就是 feature_transform_regularizer 的作用！\",\"def feature_transform_regularizer(trans): d = trans.size()[1] batchsize = trans.size()[0] # 构造一个单位矩阵 I，用于后续比较； # 添加 None 是为了扩展成 (1, d, d)，便于广播到整个 batch； I = torch.eye(d)[None, :, :] if trans.is_cuda: I = I.cuda() # 计算变换矩阵与其转置相乘后与单位矩阵之间的距离（Frobenius 范数），然后取 batch 平均值作为损失项，鼓励变换矩阵接近正交矩阵。 # Frobenius 范数（矩阵所有元素平方和开方） loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2))) return loss\"]},\"34\":{\"h\":\"特征提取\",\"t\":[\"PointNet 的核心特征提取模块 PointNetfeat ，它负责从输入点云中提取出可用于分类或分割的特征。\",\"class PointNetfeat(nn.Module): def __init__(self, global_feat = True, feature_transform = False): super(PointNetfeat, self).__init__() # 输入点云变换网络（3D） self.stn = STN3d() # 使用 Conv1D 对每个点进行特征提取； # 每个卷积层后跟一个 BatchNorm 层； # 最终输出高维特征（1024维）； self.conv1 = torch.nn.Conv1d(3, 64, 1) self.conv2 = torch.nn.Conv1d(64, 128, 1) self.conv3 = torch.nn.Conv1d(128, 1024, 1) self.bn1 = nn.BatchNorm1d(64) self.bn2 = nn.BatchNorm1d(128) self.bn3 = nn.BatchNorm1d(1024) # 全局特征开关：控制是否输出全局特征 self.global_feat = global_feat # 特征变换开关：控制是否使用 STN 对特征空间进行变换 self.feature_transform = feature_transform if self.feature_transform: self.fstn = STNkd(k=64) def forward(self, x): n_pts = x.size()[2] # 使用 STN3d 预测出一个变换矩阵； trans = self.stn(x) x = x.transpose(2, 1) # 将原始点云“摆正”； x = torch.bmm(x, trans) x = x.transpose(2, 1) # 再通过第一个卷积层提取初始特征； x = F.relu(self.bn1(self.conv1(x))) if self.feature_transform: trans_feat = self.fstn(x) x = x.transpose(2,1) x = torch.bmm(x, trans_feat) x = x.transpose(2,1) else: trans_feat = None # 提取更高维的特征； # 最后一层输出 shape: (B, 1024, N) pointfeat = x x = F.relu(self.bn2(self.conv2(x))) x = self.bn3(self.conv3(x)) # 从所有点中选出每个通道的最大响应值，作为整个点云的“抽象”表示，shape: (B, 1024) # 返回：一个元组：(values, indices)，分别是最大值和它们的位置索引。 x = torch.max(x, 2, keepdim=True)[0] x = x.view(-1, 1024) # 如果是分类任务 (global_feat=True)： if self.global_feat: return x, trans, trans_feat else: # 如果是分割任务 (global_feat=False)： x = x.view(-1, 1024, 1).repeat(1, 1, n_pts) return torch.cat([x, pointfeat], 1), trans, trans_feat\",\"✅ 如果是分类任务 (global_feat=True)，则返回：\",\"x: 全局特征 (B, 1024)\",\"trans: 输入点云变换矩阵\",\"trans_feat: 特征空间变换矩阵（可选）\",\"✅ 如果是分割任务 (global_feat=False)， 则返回：\",\"把全局特征复制 N 次并与每个点的局部特征，在通道维度进行拼接\",\"将每个点的局部特征与整个点云的全局特征拼接起来，让每个点都能看到上下文信息\",\"输出 shape: (B, 1088, N) ，即 1088 = 1024+64\"]},\"35\":{\"h\":\"分类任务\",\"t\":[\"PointNet 的分类模块 PointNetCls ，它基于前面的特征提取模块 PointNetfeat 来完成点云分类任务。\",\"class PointNetCls(nn.Module): def __init__(self, k=2, feature_transform=False): super(PointNetCls, self).__init__() self.feature_transform = feature_transform # 它使用 PointNetfeat 提取全局特征（1024维）； self.feat = PointNetfeat(global_feat=True, feature_transform=feature_transform) # 然后通过全连接层（MLP）将这些特征映射到类别空间； self.fc1 = nn.Linear(1024, 512) self.fc2 = nn.Linear(512, 256) self.fc3 = nn.Linear(256, k) self.dropout = nn.Dropout(p=0.3) self.bn1 = nn.BatchNorm1d(512) self.bn2 = nn.BatchNorm1d(256) self.relu = nn.ReLU() def forward(self, x): # 它使用 PointNetfeat 提取全局特征（1024维）； x, trans, trans_feat = self.feat(x) # 然后通过全连接层（MLP）将这些特征映射到类别空间； x = F.relu(self.bn1(self.fc1(x))) x = F.relu(self.bn2(self.dropout(self.fc2(x)))) x = self.fc3(x) # 最终输出每个类别的概率分布（log_softmax）； return F.log_softmax(x, dim=1), trans, trans_feat\"]},\"36\":{\"h\":\"分割任务\",\"t\":[\"PointNet 的分割模块 PointNetDenseCls ，它基于前面的特征提取模块 PointNetfeat 来完成点云物体分割任务。\",\"class PointNetDenseCls(nn.Module): def __init__(self, k = 2, feature_transform=False): super(PointNetDenseCls, self).__init__() self.k = k self.feature_transform=feature_transform self.feat = PointNetfeat(global_feat=False, feature_transform=feature_transform) self.conv1 = torch.nn.Conv1d(1088, 512, 1) self.conv2 = torch.nn.Conv1d(512, 256, 1) self.conv3 = torch.nn.Conv1d(256, 128, 1) self.conv4 = torch.nn.Conv1d(128, self.k, 1) self.bn1 = nn.BatchNorm1d(512) self.bn2 = nn.BatchNorm1d(256) self.bn3 = nn.BatchNorm1d(128) def forward(self, x): batchsize = x.size()[0] n_pts = x.size()[2] # 点的数量 # 调用 PointNetfeat 提取特征 # 最后将每个点的局部特征与整个点云的全局特征拼接起来，让每个点都能看到上下文信息 x, trans, trans_feat = self.feat(x) # 使用多层 Conv1D 层进一步融合局部 + 全局信息 # 最终输出 shape: (B, k, N) x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) x = self.conv4(x) # shape: (B, k, N) -> (B, N, k) , 即每个点的各个类别得分 x = x.transpose(2,1).contiguous() # 使用 log_softmax 得到 log 概率分布； x = F.log_softmax(x.view(-1,self.k), dim=-1) # shape: (B*N, k) x = x.view(batchsize, n_pts, self.k) # shape: (B, N, k) return x, trans, trans_feat\",\"✅ 1. 每个点都需要全局上下文\",\"仅靠局部特征很难判断某个点属于哪个部件（比如椅子的腿 vs 座位）；\",\"加上全局特征后，相当于告诉模型：“你知道吗，这是一个椅子”；\",\"这样模型就能根据上下文更准确地做出判断；\",\"✅ 2. 全局特征不能直接用于分割\",\"全局特征只有一份（(B, 1024)），无法直接用于每个点；\",\"所以要把它复制 N 次，变成 (B, 1024, N)；\",\"再与每个点的局部特征拼接；\"]},\"37\":{\"h\":\"缺陷\",\"t\":[\"🧠 一、核心问题：忽略局部结构信息\",\"PointNet 只通过 max pooling 聚合所有点的信息，忽略了局部邻域之间的结构关系。\",\"🔍 原因分析：\",\"PointNet 对每个点独立处理（参数共享），然后使用全局最大池化（Global Max Pooling）提取特征；\",\"这种设计使得网络只关注“最显著的点”，而没有建模点与点之间的局部几何关系；\",\"导致模型无法捕捉到更细粒度的几何细节，比如边缘、曲率、表面纹理等；\",\"💡 论文中的验证：\",\"在部件分割任务中，虽然 PointNet 表现不错，但在一些复杂区域（如椅子腿和桌面连接处）容易出错；\",\"分类任务中对缺失点具有一定鲁棒性，但遇到遮挡严重或点分布不均匀时性能下降明显；\",\"📉 二、分割任务依赖拼接机制，不够精细\",\"PointNet 的分割模块通过拼接全局特征 + 局部特征实现上下文感知，但这种方式表达能力有限。\",\"🔍 原理回顾：\",\"PointNet 的分割网络将全局特征复制 N 次并与每个点的局部特征拼接；\",\"然后使用 Conv1D 进行分类；\",\"实际上是用一个固定大小的全局特征去“广播”给每个点；\",\"⚠️ 问题所在：\",\"全局特征不能很好地反映每个点的上下文；\",\"拼接方式缺乏动态调整机制；\",\"难以区分语义相近但位置不同的区域（如桌子边缘 vs 中心）；\",\"🧱 三、对局部形状变化敏感\",\"PointNet 提取的关键点集合（critical point set）可能不足以代表复杂的局部结构。\",\"🔍 实验观察：\",\"在论文中提到，PointNet 学到的是一个关键点集合，这些点大致构成物体的骨架；\",\"如果这些关键点缺失或被遮挡，即使其他点都在，也可能导致错误分类；\",\"对于非刚性变形（如人体姿态变化），PointNet 的表现不如基于图结构的模型；\",\"📈 四、分类性能略逊于多视角方法\",\"在某些标准数据集（如 ModelNet40）上，PointNet 的分类准确率略低于 MVCNN 等基于图像的方法。\",\"方法\",\"分类准确率\",\"MVCNN（多视角 CNN）\",\"90.1%\",\"VoxNet（体素 CNN）\",\"85.9%\",\"PointNet\",\"89.2%\",\"虽然 PointNet 在速度和效率上占优，但在精度上仍略逊一筹。\",\"🧩 五、难以捕捉非刚性变换下的不变性\",\"PointNet 使用 T-Net 强制学习正交变换矩阵，只能处理刚性变换（旋转、反射），无法处理非刚性形变（如弯曲、拉伸）。\",\"🔍 举例说明：\",\"如果你有一张人脸的点云，由于表情不同，面部发生形变；\",\"PointNet 很难在这种情况下保持分类的一致性；\",\"相比之下，基于图卷积或注意力机制的模型更能捕捉这种非刚性变化；\",\"🧱 六、缺乏层次化特征提取机制\",\"PointNet 是一种单尺度网络，无法像 CNN 那样逐层提取多层次的抽象特征。\",\"✅ 后续改进：\",\"PointNet++ 正是对这一缺陷的改进；\",\"它引入了局部区域搜索 + 多尺度聚合机制；\",\"从而能够更好地捕捉点云的局部结构和层次信息；\",\"📊 七、对稀疏点云敏感\",\"当输入点云非常稀疏时（如只有几十个点），PointNet 的性能会显著下降。\",\"🔍 原因分析：\",\"PointNet 的全局特征来自于 max pooling；\",\"如果点太少，max pooling 得到的特征可能无法覆盖整个物体；\",\"特别是在遮挡严重的情况下，关键点可能丢失；\",\"📐 八、结构简单，不利于高维空间建模\",\"PointNet 的结构过于简单，难以建模更高维度的空间关系。\",\"✅ 后续发展：\",\"后续的 3D 深度学习模型（如 DGCNN、SpiderCNN、PointCNN、Transformer-based 点云模型）都尝试引入更复杂的结构来提升建模能力；\",\"如：构建点之间的邻接图、使用 attention、引入多尺度采样等；\",\"🧪 九、理论上的限制：受限于瓶颈维度 K\",\"PointNet 的表达能力受 max pooling 层维度 K 的限制，即 bottleneck dimension。\",\"📌 来自论文的理论分析：\",\"Theorem 2 表明，PointNet 的输出仅由一个不超过 K 个点的子集决定（critical point set），这意味着：\",\"如果 K 不够大，PointNet 可能遗漏重要细节；\",\"如果 K 太大，又会导致计算资源浪费；\",\"🧱 十、对噪声点敏感（尤其未训练时）\",\"虽然 PointNet 对少量异常点有一定鲁棒性，但如果训练时没有加入扰动，面对大量噪声点时效果较差。\",\"🔍 实验验证：\",\"论文中做了“插入异常点”的实验；\",\"结果显示，如果训练过程中加入了噪声，模型表现良好；\",\"否则，异常点会影响分类和分割性能；\",\"📉 十一、在大规模场景理解任务中表现一般\",\"PointNet 的时间复杂度虽然是 O(N)，但在处理超大规模点云时，仍然不如分块处理或多层级聚合模型高效。\",\"✅ 后续改进方向：\",\"使用分块策略（chunking）\",\"构建点云的层次化表示\",\"引入 attention 或图结构增强局部建模能力\",\"🧩 总结表格：PointNet 的主要缺陷\",\"缺陷类型\",\"描述\",\"是否被后续模型改进\",\"忽略局部结构\",\"仅靠 max pooling 提取特征，无局部聚合机制\",\"✅ PointNet++ 改进\",\"分割精度不高\",\"拼接机制不够精细，缺乏动态上下文感知\",\"✅ Transformer-based 改进\",\"无法处理非刚性变形\",\"T-Net 只学正交变换，无法应对弯曲、拉伸等形变\",\"✅ 图卷积、attention 改进\",\"分类精度略低\",\"在 ModelNet40 上略低于 MVCNN\",\"✅ 多视角 + PointNet 混合模型改进\",\"稀疏点云下性能差\",\"少量点无法覆盖关键结构\",\"✅ PointNet++ 改进\",\"局部建模能力弱\",\"无法捕捉边缘、曲率等细节\",\"✅ DGCNN、SpiderCNN 改进\",\"对噪声点敏感\",\"未经扰动训练时，对异常点鲁棒性差\",\"✅ 加入数据增强后缓解\",\"结构单一\",\"缺乏层次化、多尺度建模能力\",\"✅ PointNet++ / Transformer 改进\",\"📈 PointNet 的优势 vs 缺陷对比\",\"维度\",\"优势\",\"缺陷\",\"输入形式\",\"支持原始点云，无需预处理\",\"无法有效利用局部结构\",\"排列不变性\",\"完全支持\",\"无法区分顺序信息（如时间序列点云）\",\"变换不变性\",\"支持刚性变换标准化\",\"无法处理非刚性形变\",\"分类性能\",\"接近 SOTA\",\"略逊于多视角 CNN\",\"分割性能\",\"表现良好\",\"缺乏精细建模\",\"效率\",\"极其高效（O(N)）\",\"无法充分利用 GPU 并行优化\",\"扩展性\",\"易于扩展为检测、检索等任务\",\"表达能力受限于 max pooling 维度\",\"✅ 一句话总结：\",\"PointNet 的最大缺陷在于它“看不清细节”，只关注全局结构，忽视局部邻域关系，这使得它在细粒度识别、非刚性变形、稀疏点云等任务中表现受限，但它也为后续模型奠定了基础。\"]},\"38\":{\"h\":\"背景知识扫盲(可选)\"},\"39\":{\"h\":\"点云\",\"t\":[\"点云: 是一种表示三维空间中物体或场景的方式，它由大量带有位置信息的点组成。\",\"每个点通常包含：\",\"坐标信息 ：x, y, z（3D 空间中的位置）。\",\"可选属性：颜色（RGB）、法向量（Normal）、强度（Intensity）、时间戳等。\",\"表示形式:\",\"点云（Point Cloud）: 原始点集合：每个点有(x, y, z)坐标; 可选颜色、法向量等属性, 简洁、轻便; 保留原始几何信息,无序性、非结构化、难以用 CNN 处理。\",\"体素网格 (voxel grids) : 将空间划分成立方体格子，每个格子表示是否有物体; 结构规整，适合 3D CNN; 计算复杂度高、稀疏性强、精度受限。\",\"多视角图像（Multi-View Images）: 从多个角度渲染点云或 3D 模型为 2D 图像; 可使用成熟的 2D CNN 方法; 丢失部分几何信息，依赖视角选择。\",\"网格（Mesh）： 由三角形面片组成的 3D 模型； 包含表面细节，适合渲染； 难以自动构建，拓扑复杂。\"]},\"40\":{\"h\":\"对称函数\",\"t\":[\"对称函数（Symmetric Function）是一种对输入顺序不敏感的函数；换句话说，无论你如何打乱输入元素的顺序，输出结果都保持不变。\",\"🧠 数学定义:\",\"设是一个函数，如果对于任意排列（permutation），都有：\",\"那么就是一个 对称函数。\",\"PointNet 处理的是点云数据，而点云是无序集合（unordered set） ，即：\",\"点云中点的顺序不影响整体形状。\",\"所以模型必须具有对点顺序的不变性（permutation invariance）。\",\"这就要求网络中的某些关键操作必须是对称函数 ，才能保证整个网络输出与输入点的顺序无关。\",\"📦 常见的对称函数:\",\"函数\",\"描述\",\"是否可微\",\"应用场景\",\"最大池化（Max Pooling）\",\"取所有点的最大值：\",\"✅ 是\",\"PointNet 中的核心操作\",\"平均池化（Average Pooling）\",\"取所有点的平均值：\",\"✅ 是\",\"特征融合、平滑处理\",\"求和（Summation）\",\"所有点相加：\",\"✅ 是\",\"构建全局特征向量\",\"乘积（Product）\",\"所有点相乘：\",\"⚠️ 对数值变化敏感\",\"不常用，但可用于特定任务\",\"最小池化（Min Pooling）\",\"取最小值：\",\"✅ 是\",\"异常检测等特殊场景\",\"Softmax + 加权和（Attention-based Sum）\",\"根据注意力机制加权求和，权重由 softmax 得出\",\"✅ 是\",\"DGCNN、Transformer 中使用\",\"统计量（如方差、标准差）\",\"计算点集的分布特性\",\"✅ 是\",\"特征增强、异常检测\",\"集合函数近似器（如 Deep Sets）\",\"使用神经网络直接学习对称函数\",\"✅ 是\",\"更复杂的对称函数建模\"]},\"41\":{\"h\":\"刚性运动\",\"t\":[\"刚性运动(rigid motions) 是指：物体在空间中移动时，其形状和大小保持不变的运动方式 。\",\"刚性运动\",\"❌ 不改变\",\"移动、旋转\",\"非刚性运动\",\"✅ 改变\",\"弯曲、拉伸、缩放（非均匀）、变形\",\"刚性运动 = 平移 + 旋转，不改变物体形状和内部结构，只改变位置和朝向。\"]},\"42\":{\"h\":\"正交变换\",\"t\":[\"正交变换的本质是：只改变物体的方向（旋转），不改变形状和大小\",\"所以：\",\"正交变换包括：旋转 + 反射。\",\"不包括：缩放、剪切、拉伸等会导致形变的操作。\"]},\"43\":{\"h\":\"大语言模型\"},\"44\":{\"h\":\"图解 Bert\",\"t\":[\"图解Bert & Bert文本分类实战\"]},\"45\":{\"h\":\"环境搭建\",\"t\":[\"按序执行以下命令完成环境搭建:\",\"git clone https://github.com/DA-southampton/Read_Bert_Code.git cd Read_Bert_Code conda create -n Read_Bert_Code python=3.9.22 conda activate Read_Bert_Code\",\"本文使用的是谷歌的中文预训练模型：chinese_L-12_H-768_A-12.zip，模型有点大，我就不上传了，如果本地不存在，就点击这里直接下载,或者直接命令行运行\",\"wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\",\"预训练模型下载下来之后，进行解压，然后将tf模型转为对应的pytorch版本即可。对应代码如下:\",\"export BERT_BASE_DIR=/Users/zhandaohong/Read_Bert_Code/chinese_L-12_H-768_A-12 python convert_tf_checkpoint_to_pytorch.py \\\\ --tf_checkpoint_path$BERT_BASE_DIR/bert_model.ckpt \\\\ --bert_config_file$BERT_BASE_DIR/bert_config.json \\\\ --pytorch_dump_path$BERT_BASE_DIR/pytorch_model.bin\",\"转化成功之后，将模型放入到仓库对应位置：\",\"Read_Bert_Code/bert_read_step_to_step/prev_trained_model/\",\"并重新命名为：\",\" bert-base-chinese\",\"其次是准备训练数据，这里我准备做一个文本分类任务，使用的是Tnews数据集，这个数据集来源是这里，分为训练，测试和开发集，我已经上传到了仓库中，具体位置在\",\"Read_Bert_Code/bert_read_step_to_step/chineseGLUEdatasets/tnews\",\"需要注意的一点是，因为我只是为了了解内部代码情况，所以准确度不是在我的考虑范围之内，所以我只是取其中的一部分数据，其中训练数据使用1k，测试数据使用1k，开发数据1k。\",\"准备就绪，使用pycharm导入项目，准备调试，我的调试文件是run_classifier.py文件，对应的参数为\",\"--model_type=bert --model_name_or_path=prev_trained_model/bert-base-chinese --task_name=\\\"tnews\\\" --do_train --do_eval --do_lower_case --data_dir=./chineseGLUEdatasets/tnews --max_seq_length=128 --per_gpu_train_batch_size=16 --per_gpu_eval_batch_size=16 --learning_rate=2e-5 --num_train_epochs=4.0 --logging_steps=100 --save_steps=100 --output_dir=./outputs/tnews_output/ --overwrite_output_dir\",\"然后启动 run_classifier.py 文件进行调试即可 , 所参考源仓库未提供requirements.txt文件，因此需要大家自行完成运行时缺失依赖包的安装。\"]},\"46\":{\"h\":\"数据预处理\",\"t\":[\"输入数据格式\",\"{ \\\"guid\\\": \\\"train-0\\\", \\\"label\\\": \\\"104\\\", // 文本分类任务: 文本对应的标签 \\\"text_a\\\": \\\"股票中的突破形态\\\", \\\"text_b\\\": null // NSP任务: 用于判断给出的两个句子是否连续 }\",\"NSP (Next Sentence Prediction)\",\"文本分词 & 借助字典映射为word id\",\"\\\"股票中的突破形态\\\" --> ['股', '票', '中', '的', '突', '破', '形', '态'] --> [5500, 4873, 704, 4638, 4960, 4788, 2501, 2578]\",\"对于字典中不存在的词 , 用 [UNK] 表示, 对应的id为 100\",\"过长截断策略\",\"添加特殊Token标记\",\"原序列添加特殊Token标记图\",\"[101, 5500, 4873, 704, 4638, 4960, 4788, 2501, 2578, 102]\",\"BertTokenizer中的特殊token id:\",\"[CLS]: 101\",\"[SEP]: 102\",\"[MASK]: 103\",\"[UNK]: 100\",\"[PAD]: 0\",\" # BertTokenizer def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None): if token_ids_1 is None: return [self.cls_token_id] + token_ids_0 + [self.sep_token_id] cls = [self.cls_token_id] sep = [self.sep_token_id] return cls + token_ids_0 + sep + token_ids_1 + sep\",\"创建句子辨识列表，用以区分不同的句子\",\"token_type_ids作用图解\",\" # BertTokenizer def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None): \\\"\\\"\\\" Creates a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence pair mask has the following format: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 | first sequence | second sequence if token_ids_1 is None, only returns the first portion of the mask (0's). \\\"\\\"\\\" sep = [self.sep_token_id] cls = [self.cls_token_id] if token_ids_1 is None: return len(cls + token_ids_0 + sep) * [0] return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\",\"创建用以区分special tokens部分的mask列表\",\"special_tokens_mask作用图解\",\" # BertTokenizer def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False): if token_ids_1 is not None: return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1] return [1] + ([0] * len(token_ids_0)) + [1]\",\"超长截断\",\" # PreTrainedTokenizer if max_length and len(encoded_inputs[\\\"input_ids\\\"]) > max_length: encoded_inputs[\\\"input_ids\\\"] = encoded_inputs[\\\"input_ids\\\"][:max_length] encoded_inputs[\\\"token_type_ids\\\"] = encoded_inputs[\\\"token_type_ids\\\"][:max_length] encoded_inputs[\\\"special_tokens_mask\\\"] = encoded_inputs[\\\"special_tokens_mask\\\"][:max_length]\",\"生成padding部分的mask列表\",\"attention_mask作用图解\",\" # 生成注意力掩码，真实token对应1，填充token对应0 attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\",\"所有序列都填充到max_length长度,不足长度用padding填充\",\"填充过程图\",\" # 记录输入长度 input_len = len(input_ids) # 计算需要填充的长度 --- 所有输入序列等长，都等于max_length padding_length = max_length - len(input_ids) # 右填充 input_ids = input_ids + ([pad_token] * padding_length) attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length) token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\",\"数据集中每一个样本最终都会解析得到一个InputFeatures\",\"InputFeatures组成图解\",\"features.append( InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=label, input_len=input_len))\",\"label 是当前文本对应的类别标签 input_len 是序列实际长度(含special tokens)\",\"数据集预处理完后，将InputFeatures List列表组装起来得到需要的DataSet\",\"dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_lens,all_labels)\"]},\"47\":{\"h\":\"模型架构\"},\"48\":{\"h\":\"DataLoader\",\"t\":[\" train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset) train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size,collate_fn=collate_fn)\",\"DataLoader 设置的回调方法cllote_fn负责对返回的一个batch，在返回前进行预处理:\",\"def collate_fn(batch): all_input_ids, all_attention_mask, all_token_type_ids, all_lens, all_labels = map(torch.stack, zip(*batch)) max_len = max(all_lens).item() # 计算当前批次中所有序列的实际最大长度 all_input_ids = all_input_ids[:, :max_len] # 按照本批次序列中最大长度进行截断: max_length --> max_len all_attention_mask = all_attention_mask[:, :max_len] all_token_type_ids = all_token_type_ids[:, :max_len] return all_input_ids, all_attention_mask, all_token_type_ids, all_labels\"]},\"49\":{\"h\":\"BertEmbeddings\",\"t\":[\"input embeddings = token embeddings + segmentation embeddings + position embeddings\",\"class BertEmbeddings(nn.Module): def __init__(self, config): super(BertEmbeddings, self).__init__() self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0) self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size) self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, input_ids, token_type_ids=None, position_ids=None): seq_length = input_ids.size(1) if position_ids is None: # 为当前批次中的每个序列样本生成一个位置序列: (1,2,3,4,5,...) , 构成一个位置序列矩阵 position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) position_ids = position_ids.unsqueeze(0).expand_as(input_ids) if token_type_ids is None: token_type_ids = torch.zeros_like(input_ids) words_embeddings = self.word_embeddings(input_ids) position_embeddings = self.position_embeddings(position_ids) # 位置编码为可学习的矩阵 token_type_embeddings = self.token_type_embeddings(token_type_ids) # 让模型自己学会区分不同的句子 embeddings = words_embeddings + position_embeddings + token_type_embeddings embeddings = self.LayerNorm(embeddings) embeddings = self.dropout(embeddings) return embeddings\",\"嵌入向量生成过程图\"]},\"50\":{\"h\":\"BertEncoder\"},\"51\":{\"h\":\"BertLayer\",\"t\":[\"BertLayer模型结构图\",\"class BertIntermediate(nn.Module): def __init__(self, config): super(BertIntermediate, self).__init__() self.dense = nn.Linear(config.hidden_size, config.intermediate_size) # (768,3072) # 激活函数 - GLEU if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)): self.intermediate_act_fn = ACT2FN[config.hidden_act] else: self.intermediate_act_fn = config.hidden_act def forward(self, hidden_states): hidden_states = self.dense(hidden_states) hidden_states = self.intermediate_act_fn(hidden_states) # 激活函数 - GLEU return hidden_states class BertOutput(nn.Module): def __init__(self, config): super(BertOutput, self).__init__() self.dense = nn.Linear(config.intermediate_size, config.hidden_size) # (3072,768) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, hidden_states, input_tensor): hidden_states = self.dense(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.LayerNorm(hidden_states + input_tensor) return hidden_states class BertLayer(nn.Module): def __init__(self, config): super(BertLayer, self).__init__() self.attention = BertAttention(config) self.intermediate = BertIntermediate(config) self.output = BertOutput(config) def forward(self, hidden_states, attention_mask=None): attention_output = self.attention(hidden_states, attention_mask) intermediate_output = self.intermediate(attention_output) layer_output = self.output(intermediate_output, attention_output) return layer_output\"]},\"52\":{\"h\":\"BertEncoder\",\"t\":[\"BertEncoder模型结构图\",\"class BertEncoder(nn.Module): def __init__(self, config): super(BertEncoder, self).__init__() self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)]) def forward(self, hidden_states, attention_mask=None, head_mask=None): for i, layer_module in enumerate(self.layer): hidden_states = layer_module(hidden_states, attention_mask, head_mask[i]) return hidden_states\"]},\"53\":{\"h\":\"BertPooler\",\"t\":[\"BertPooler模型结构图\",\"class BertPooler(nn.Module): def __init__(self, config): super(BertPooler, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) self.activation = nn.Tanh() def forward(self, hidden_states): # We \\\"pool\\\" the model by simply taking the hidden state corresponding # to the first token. first_token_tensor = hidden_states[:, 0] # CLS Token Context Embeddings pooled_output = self.dense(first_token_tensor) pooled_output = self.activation(pooled_output) return pooled_output\"]},\"54\":{\"h\":\"BertModel\",\"t\":[\"BertModel模型结构图\",\"class BertModel(BertPreTrainedModel): def __init__(self, config): super(BertModel, self).__init__(config) self.embeddings = BertEmbeddings(config) self.encoder = BertEncoder(config) self.pooler = BertPooler(config) self.init_weights() def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None): extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2) extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0 embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids) sequence_output = self.encoder(embedding_output, extended_attention_mask, # padding mask ) pooled_output = self.pooler(sequence_output) outputs = (sequence_output, pooled_output,) return outputs\"]},\"55\":{\"h\":\"BertForSequenceClassification\",\"t\":[\"BertForSequenceClassification模型结构图\",\"class BertForSequenceClassification(BertPreTrainedModel): def __init__(self, config): super(BertForSequenceClassification, self).__init__(config) self.num_labels = config.num_labels self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, self.config.num_labels) self.init_weights() def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None): outputs = self.bert(input_ids, attention_mask=attention_mask, # padding mask token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) # None ? pooled_output = outputs[1] # 对于分类任务来说，只需要去除CLS Token用于分类任务即可 pooled_output = self.dropout(pooled_output) logits = self.classifier(pooled_output) outputs = (logits,) + outputs[2:] # add hidden states and attention if they are here if labels is not None: if self.num_labels == 1: # We are doing regression loss_fct = MSELoss() loss = loss_fct(logits.view(-1), labels.view(-1)) else: loss_fct = CrossEntropyLoss() loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) outputs = (loss,) + outputs return outputs # (loss), logits, (hidden_states), (attentions)\"]},\"56\":{\"h\":\"BertAttention\"},\"57\":{\"h\":\"BertSelfAttention\",\"t\":[\"多头自注意力计算流程图\",\"class BertSelfAttention(nn.Module): def __init__(self, config): super(BertSelfAttention, self).__init__() self.output_attentions = config.output_attentions self.num_attention_heads = config.num_attention_heads self.attention_head_size = int(config.hidden_size / config.num_attention_heads) self.all_head_size = self.num_attention_heads * self.attention_head_size self.query = nn.Linear(config.hidden_size, self.all_head_size) self.key = nn.Linear(config.hidden_size, self.all_head_size) self.value = nn.Linear(config.hidden_size, self.all_head_size) self.dropout = nn.Dropout(config.attention_probs_dropout_prob) def transpose_for_scores(self, x): new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size) x = x.view(*new_x_shape) return x.permute(0, 2, 1, 3) def forward(self, hidden_states, attention_mask=None, head_mask=None): mixed_query_layer = self.query(hidden_states) mixed_key_layer = self.key(hidden_states) mixed_value_layer = self.value(hidden_states) # view 成多头格式: (batch,heads,seq_len,d_k) query_layer = self.transpose_for_scores(mixed_query_layer) key_layer = self.transpose_for_scores(mixed_key_layer) value_layer = self.transpose_for_scores(mixed_value_layer) # Take the dot product between \\\"query\\\" and \\\"key\\\" to get the raw attention scores. attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) # (batch,heads,d_k,seq_len) attention_scores = attention_scores / math.sqrt(self.attention_head_size) if attention_mask is not None: # Apply the attention mask is (precomputed for all layers in BertModel forward() function) attention_scores = attention_scores + attention_mask # Normalize the attention scores to probabilities. attention_probs = nn.Softmax(dim=-1)(attention_scores) # This is actually dropping out entire tokens to attend to, which might # seem a bit unusual, but is taken from the original Transformer paper. attention_probs = self.dropout(attention_probs) context_layer = torch.matmul(attention_probs, value_layer) context_layer = context_layer.permute(0, 2, 1, 3).contiguous() new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) context_layer = context_layer.view(*new_context_layer_shape) # 合并头结果 return context_layer\"]},\"58\":{\"h\":\"BertSelfOutput\",\"t\":[\"BertSelfOutput计算流程图\",\"class BertSelfOutput(nn.Module): def __init__(self, config): super(BertSelfOutput, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) # 残差链接 + 层归一化 def forward(self, hidden_states, input_tensor): hidden_states = self.dense(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.LayerNorm(hidden_states + input_tensor) return hidden_states\"]},\"59\":{\"h\":\"BertAttention\",\"t\":[\"BertAttention计算流程图\",\"class BertAttention(nn.Module): def __init__(self, config): super(BertAttention, self).__init__() self.self = BertSelfAttention(config) self.output = BertSelfOutput(config) def forward(self, input_tensor, attention_mask=None): self_outputs = self.self(input_tensor, attention_mask) # 多头自注意力机制 attention_output = self.output(self_outputs, input_tensor) return attention_output\"]},\"60\":{\"h\":\"预训练\",\"t\":[\"预训练与微调\"]},\"61\":{\"h\":\"BertPredictionHeadTransform\",\"t\":[\"BertPredictionHeadTransform结构图\",\"class BertPredictionHeadTransform(nn.Module): def __init__(self, config): super(BertPredictionHeadTransform, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)): self.transform_act_fn = ACT2FN[config.hidden_act] else: self.transform_act_fn = config.hidden_act self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) def forward(self, hidden_states): hidden_states = self.dense(hidden_states) hidden_states = self.transform_act_fn(hidden_states) hidden_states = self.LayerNorm(hidden_states) return hidden_states\"]},\"62\":{\"h\":\"BertLMPredictionHead\",\"t\":[\"BertLMPredictionHead结构图\",\"class BertLMPredictionHead(nn.Module): def __init__(self, config): super(BertLMPredictionHead, self).__init__() self.transform = BertPredictionHeadTransform(config) # The output weights are the same as the input embeddings, but there is # an output-only bias for each token. self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False) self.bias = nn.Parameter(torch.zeros(config.vocab_size)) def forward(self, hidden_states): hidden_states = self.transform(hidden_states) hidden_states = self.decoder(hidden_states) + self.bias return hidden_states\"]},\"63\":{\"h\":\"BertPreTrainingHeads\",\"t\":[\"BertPreTrainingHeads结构图\",\"class BertPreTrainingHeads(nn.Module): def __init__(self, config): super(BertPreTrainingHeads, self).__init__() self.predictions = BertLMPredictionHead(config) self.seq_relationship = nn.Linear(config.hidden_size, 2) def forward(self, sequence_output, pooled_output): prediction_scores = self.predictions(sequence_output) # seq_relationship_score = self.seq_relationship(pooled_output) # 两个句子是否为上下句关系 return prediction_scores, seq_relationship_score\"]},\"64\":{\"h\":\"BertForPreTraining\",\"t\":[\"BertForPreTraining结构图\",\"class BertForPreTraining(BertPreTrainedModel): def __init__(self, config): super(BertForPreTraining, self).__init__(config) self.bert = BertModel(config) self.cls = BertPreTrainingHeads(config) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, masked_lm_labels=None, next_sentence_label=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) sequence_output, pooled_output = outputs[:2] # 隐藏层输出,CLS Token Embeddings prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output) outputs = (prediction_scores, seq_relationship_score,) # 计算掩码语言损失 和 下一个句子预测损失 if masked_lm_labels is not None and next_sentence_label is not None: loss_fct = CrossEntropyLoss(ignore_index=-1) masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1)) next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1)) total_loss = masked_lm_loss + next_sentence_loss outputs = (total_loss,) + outputs return outputs # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)\"]},\"65\":{\"h\":\"其他下游任务\",\"t\":[\"Bert支持的下游任务图\"]},\"66\":{\"h\":\"问答任务\",\"t\":[\"在 BERT 的问答任务中，典型的输入是一个包含 问题（Question） 和 上下文（Context） 的文本对。例如：\",\"问题: “谁写了《哈姆雷特》？”上下文: “莎士比亚是英国文学史上最伟大的作家之一，他写了包括《哈姆雷特》、《麦克白》等著名悲剧。”\",\"输入格式（Tokenization 后的形式），在使用 BertTokenizer 编码后，输入会变成如下结构：\",\"[CLS] 问题 tokens [SEP] 上下文 tokens [SEP]\",\"BERT 的输出（Outputs），通过调用 self.bert(...)，你将得到一个包含多个元素的 tuple 输出：\",\"outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\",\"返回值形如：\",\"( sequence_output, # (batch_size, seq_length, hidden_size) pooled_output, # (batch_size, hidden_size) )\",\"主要输出项解释:\",\"✅ sequence_output: 最终每个 token 的表示\",\"形状：(batch_size, seq_length, hidden_size)\",\"是模型最后一层所有 token（包括问题和上下文）的隐藏状态。\",\"在问答任务中，我们主要使用它来预测答案的起始和结束位置。\",\"✅ pooled_output: 句子级别表示（不常用）\",\"形状：(batch_size, hidden_size)\",\"是 [CLS] token 经过一层全连接后的输出。\",\"在分类任务中更有用，在问答任务中一般不会使用这个输出。\",\"如何利用 BERT 输出做问答预测？\",\"在 BertForQuestionAnswering 中，使用了如下逻辑：\",\"logits = self.qa_outputs(sequence_output) # (batch_size, seq_length, 2) start_logits, end_logits = logits.split(1, dim=-1) # split into start and end start_logits = start_logits.squeeze(-1) # (batch_size, seq_length) end_logits = end_logits.squeeze(-1)\",\"qa_outputs 层的作用：\",\"是一个线性层：nn.Linear(config.hidden_size, 2)\",\"将每个 token 的 hidden_size 向量映射成两个分数：一个是该 token 作为答案开始的可能性，另一个是作为答案结束的可能性。\",\"输出解释：\",\"start_logits: 每个 token 是答案起点的得分（未归一化）。\",\"end_logits: 每个 token 是答案终点的得分。\",\"比如对于一个长度为 128 的序列，每个 token 都有一个对应的 start/end 分数：\",\"start_scores = torch.softmax(start_logits, dim=-1) # softmax 得到概率 end_scores = torch.softmax(end_logits, dim=-1) # 找出最可能是 start 和 end 的位置 start_index = torch.argmax(start_scores) end_index = torch.argmax(end_scores)\",\"如果 start_index <= end_index，那么可以组合这两个索引得到答案 span。\"]},\"67\":{\"h\":\"代码实现\",\"t\":[\"class BertForQuestionAnswering(BertPreTrainedModel): def __init__(self, config): super(BertForQuestionAnswering, self).__init__(config) self.num_labels = config.num_labels # 通常是 2，即 start 和 end self.bert = BertModel(config) self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, start_positions=None, end_positions=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids) sequence_output = outputs[0] # (batch,seq_len,hidden_size) ---> (batch,seq_len,2) logits = self.qa_outputs(sequence_output) start_logits, end_logits = logits.split(1, dim=-1) start_logits = start_logits.squeeze(-1) # (batch,seq_len) end_logits = end_logits.squeeze(-1) outputs = (start_logits, end_logits,) # 计算交叉熵损失 if start_positions is not None and end_positions is not None: # sometimes the start/end positions are outside our model inputs, we ignore these terms # ignored_index = seq_len ignored_index = start_logits.size(1) # clamp_ 是 PyTorch 中的一个方法，用于将张量中的值限制在指定的范围内。 # 它的语法是 tensor.clamp_(min, max) ，表示将张量中的值限制在 min 和 max 之间。 # 如果值小于 min ，则将其设置为 min ；如果值大于 max ，则将其设置为 max 。 start_positions.clamp_(0, ignored_index) end_positions.clamp_(0, ignored_index) # ignore_index: 用于指定在计算损失时忽略的标签索引。 loss_fct = CrossEntropyLoss(ignore_index=ignored_index) # 分别计算答案起始下标和结束下标预测得到的交叉熵损失 start_loss = loss_fct(start_logits, start_positions) end_loss = loss_fct(end_logits, end_positions) total_loss = (start_loss + end_loss) / 2 outputs = (total_loss,) + outputs return outputs # (loss), start_logits, end_logits\"]},\"68\":{\"h\":\"易混淆\",\"t\":[\"BERT 是一个 基于上下文编码（Contextual Encoder） 的模型，不是自回归生成器。它不会“生成”新的文本，而是对输入文本中每个 token 的角色进行分类（如判断哪个是答案的开始、结束）。所以最终的答案只能来自原始输入文本中的某一段子串。\",\"📚 详细解释\",\"✅ BERT 是一个 Encoder-only 模型\",\"BERT 只包含 Transformer 的 encoder 部分。\",\"它的作用是给定一个完整的句子（或两个句子），对每个 token 生成一个上下文相关的表示（contextualized representation）。\",\"它不具有生成能力，不能像 GPT 这样的 decoder-only 模型那样逐词生成新内容。\",\"🔍 QA 任务的本质：定位答案 span 而非生成答案\",\"在 SQuAD 这类抽取式问答任务中：\",\"答案必须是原文中的连续片段（span）。\",\"所以模型的任务是：\",\"给出问题和上下文；\",\"在上下文中找到最可能的答案起始位置和结束位置；\",\"最终答案就是上下文中这两个位置之间的字符串。\",\"BERT 做的就是这个定位任务，而不是重新生成一个新的答案。\",\"🧩 输入与输出的关系\",\"answer_tokens = input_ids[0][start_index : end_index + 1] answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\",\"这段代码的意思是：\",\"start_index 和 end_index 是模型预测出的答案的起始和结束位置。\",\"我们从原始输入的 input_ids 中取出对应的 token ID 子序列。\",\"使用 tokenizer 把这些 token ID 解码成自然语言文本。\",\"得到的就是答案。\",\"这其实就是在说：\",\"“根据你的理解，答案应该在这段文字中的第 X 到第 Y 个词之间，请把这部分原文告诉我。”\",\"🧪 举个例子\",\"假设原始上下文是：\",\"The capital of France is Paris.\",\"经过 Tokenizer 编码后可能是：\",\"[CLS] the capital of france is paris [SEP]\",\"如果模型预测 start_index=5，end_index=5，那么对应的就是单词 \\\"paris\\\"，这就是答案。\",\"⚠️ 注意事项\",\"不能超出上下文范围\",\"start/end positions 必须落在上下文部分（即 token_type_id == 1 的区域）。\",\"否则答案可能不合理（比如取到了问题部分的内容）。\",\"特殊 token 不计入答案\",\"[CLS], [SEP] 等会被 skip_special_tokens=True 自动跳过。\",\"无法处理不在原文中的答案\",\"如果正确答案没有出现在上下文中，BERT 无法“编造”出来。\",\"这是抽取式问答模型的局限性。\",\"💡 对比：生成式 vs 抽取式问答\",\"类型\",\"模型代表\",\"是否能生成新文本\",\"答案是否必须在原文中\",\"示例\",\"抽取式\",\"BERT\",\"❌\",\"✅\",\"答案是原文中的一段\",\"生成式\",\"T5 / BART / GPT\",\"✅\",\"❌\",\"答案可以是任意文本\",\"如果你希望模型能“自己写答案”，那就需要使用生成式模型。\",\"✅ 总结\",\"问题\",\"回答\",\"为什么答案来自 input_ids？\",\"因为 BERT 是编码器模型，只做抽取式问答，答案必须是原文中的一段文本。\",\"BERT 能不能自己生成答案？\",\"不能，BERT 不具备生成能力，只能对输入文本中的 token 做分类。\",\"如何获取答案？\",\"根据预测的 start/end index，从 input_ids 中提取 token，并用 tokenizer 解码成自然语言。\"]},\"69\":{\"h\":\"Token分类任务\",\"t\":[\"Token 分类任务是指对输入文本中的每个 token 进行分类，常见的应用场景包括：\",\"命名实体识别 (NER)\",\"词性标注 (POS)\",\"语义角色标注 (SRL)\",\"class BertForTokenClassification(BertPreTrainedModel): def __init__(self, config): super(BertForTokenClassification, self).__init__(config) self.num_labels = config.num_labels self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, config.num_labels) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) sequence_output = outputs[0] # (batch,seq_len,hidden_size) sequence_output = self.dropout(sequence_output) logits = self.classifier(sequence_output) # （batch,seq_len,num_labels） outputs = (logits,) if labels is not None: loss_fct = CrossEntropyLoss() # Only keep active parts of the loss if attention_mask is not None: active_loss = attention_mask.view(-1) == 1 active_logits = logits.view(-1, self.num_labels)[active_loss] active_labels = labels.view(-1)[active_loss] loss = loss_fct(active_logits, active_labels) else: loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) outputs = (loss,) + outputs return outputs # (loss), scores\"]},\"70\":{\"h\":\"多项选择任务\",\"t\":[\"多项选择任务是指给定一个问题和多个候选答案，模型需要从中选择最合适的答案。常见的应用场景包括：\",\"阅读理解任务\",\"问答系统中的候选答案选择\",\"对话系统中的候选回复选择\",\"在 多项选择题（Multiple Choice） 任务中，BERT 的输入组织形式与普通分类或问答任务略有不同。你需要为每个选项分别构造一个完整的 BERT 输入序列，并将它们组合成一个批次进行处理。\",\"✅ 假设你有一个问题 + 4 个选项：\",\"问题：谁写了《哈姆雷特》？ A. 雨果 B. 歌德 C. 莎士比亚 D. 托尔斯泰\",\"对于这样的多选问题，BERT 的输入方式是：\",\"对每一个选项，都单独构造一个 [CLS] + 问题 + [SEP] + 选项内容 + [SEP] 的输入序列。\",\"也就是说，模型会对每个选项分别编码 ，然后从中选出最合适的那个。\",\"class BertForMultipleChoice(BertPreTrainedModel): def __init__(self, config): super(BertForMultipleChoice, self).__init__(config) self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, 1) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None): # 获取选项个数 num_choices = input_ids.shape[1] # (batch_size, num_choices, seq_length) # 将选项展平，以便一起处理: (batch_size * num_choices, seq_length) input_ids = input_ids.view(-1, input_ids.size(-1)) attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) pooled_output = outputs[1] # (batch_size * num_choices, hidden_size) pooled_output = self.dropout(pooled_output) logits = self.classifier(pooled_output) # (batch_size * num_choices, 1) reshaped_logits = logits.view(-1, num_choices) # (batch_size , num_choices, 1) outputs = (reshaped_logits,) if labels is not None: loss_fct = CrossEntropyLoss() loss = loss_fct(reshaped_logits, labels) outputs = (loss,) + outputs return outputs # (loss), reshaped_logits, (hidden_states), (attentions)\",\"在前向传播中，会将这些输入展平，变成：\",\"input_ids.view(-1, seq_length) # (batch_size * num_choices, seq_length)\",\"这样就能让 BERT 对每个选项分别进行编码。\",\"BERT 输出后，再对每个选项做分类打分，最后重新 reshape 成 (batch_size, num_choices) 形式，用于计算交叉熵损失。\"]},\"71\":{\"h\":\"图解Transformer\",\"t\":[\"图解Transformer & 机器翻译实战\"]},\"72\":{\"h\":\"环境\",\"t\":[\"本文基于 The Annotated Transformer 所提供的代码展开进行讲解。\",\"环境搭建遵从如下步骤即可:\",\"git clone https://github.com/harvardnlp/annotated-transformer cd annotated-transformer conda create -n annotated-transformer python=3.9.22 conda activate annotated-transformer pip install -r requirements.txt\",\"MacOS 用户本地运行时，需要将 requirements.txt 文件中的 torch == 1.11.0+cu113 改为 torch==1.11.0，因为CUDA不支持MacOS。\"]},\"73\":{\"h\":\"背景\",\"t\":[\"RNN等模型的缺点是需要顺序计算，从而很难并行。因此出现了Extended Neural GPU、ByteNet和ConvS2S等网络模型。这些模型都是以CNN为基础，这比较容易并行。但是和RNN相比，它较难学习到长距离的依赖关系。\",\"本文的Transformer使用了Self-Attention机制，它在编码每一词的时候都能够注意(attend to)整个句子，从而可以解决长距离依赖的问题，同时计算Self-Attention可以用矩阵乘法一次计算所有的时刻，因此可以充分利用计算资源(CPU/GPU上的矩阵运算都是充分优化和高度并行的)。\"]},\"74\":{\"h\":\"模型架构\",\"t\":[\"Transformer 模型架构图\",\"Transformer 是一种基于自注意力机制(Self-Attention) 的神经网络架构,其由七大主要部分构成:\",\"Encoder-Decoder 结构\",\"编码器(Encoder)：将输入序列（如句子）转换为一系列高维向量表示。\",\"解码器(Decoder)：根据编码器的输出生成目标序列（如翻译后的句子）。\",\"多头自注意力机制（Multi-Head Self-Attention）\",\"自注意力机制是 Transformer 的核心，它允许模型在处理每个词时关注输入序列中的所有词。\",\"多头自注意力机制通过并行计算多个注意力头，捕捉不同子空间的信息，从而增强模型的表达能力。\",\"位置编码（Positional Encoding）\",\"由于 Transformer 不使用传统的循环或卷积结构，它通过位置编码将序列中词的位置信息注入到输入中。位置编码通常使用正弦和余弦函数生成。\",\"前馈神经网络（Feed-Forward Neural Network）\",\"在自注意力机制之后，每个位置的输出会通过一个独立的前馈神经网络进行进一步处理。\",\"残差连接与层归一化（Residual Connection & Layer Normalization）\",\"每个子层（如自注意力层和前馈层）都使用了残差连接和层归一化，以加速训练并提高模型的稳定性。\",\"掩码机制（Masking）\",\"在解码器中，使用掩码机制确保模型在生成目标序列时只能看到当前及之前的词，而不能看到未来的词。\",\"在输入序列长度不一致时，通过填充掩码（Padding Mask）屏蔽填充部分的信息。\",\"输出层\",\"解码器的最终输出通过一个线性层和 Softmax 函数生成目标序列的概率分布。\"]},\"75\":{\"h\":\"Encoder-Decoder 结构\",\"t\":[\"EncoderDecoder模型结构图\",\"class EncoderDecoder(nn.Module): def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): \\\"Take in and process masked src and target sequences.\\\" return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) def encode(self, src, src_mask): return self.encoder(self.src_embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\"]},\"76\":{\"h\":\"Generator\",\"t\":[\"Generator模型结构图\",\"class Generator(nn.Module): # 根据Decoder的隐状态输出一个词 # d_model是Decoder输出的大小，vocab是词典大小 def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) # 全连接再加上一个softmax def forward(self, x): return F.log_softmax(self.proj(x), dim=-1)\"]},\"77\":{\"h\":\"Encoder 结构\"},\"78\":{\"h\":\"SublayerConnection\",\"t\":[\"SublayerConnection模型结构图\",\"class SublayerConnection(nn.Module): \\\"\\\"\\\" LayerNorm + sublayer(Self-Attenion/Dense) + dropout + 残差连接 为了简单，把LayerNorm放到了前面，这和原始论文稍有不同，原始论文LayerNorm在最后。 \\\"\\\"\\\" def __init__(self, size, dropout): super(SublayerConnection, self).__init__() self.norm = LayerNorm(size) self.dropout = nn.Dropout(dropout) def forward(self, x, sublayer): \\\"sublayer是传入的参数，参考DecoderLayer，它可以当成函数调用，这个函数的有一个输入参数\\\" return x + self.dropout(sublayer(self.norm(x)))\"]},\"79\":{\"h\":\"EncoderLayer\",\"t\":[\"EncoderLayer模型结构图\",\"# 编码器层 = 自注意力子层 + 前馈层 class EncoderLayer(nn.Module): def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward # 自注意力子层 和 前馈层 各需要一个 self.sublayer = clones(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): \\\"Follow Figure 1 (left) for connections.\\\" x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) return self.sublayer[1](x, self.feed_forward)\"]},\"80\":{\"h\":\"Encoder\",\"t\":[\"Encoder模型结构图\",\"class Encoder(nn.Module): \\\"Core encoder is a stack of N layers\\\" def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): \\\"Pass the input (and mask) through each layer in turn.\\\" for layer in self.layers: x = layer(x, mask) return self.norm(x)\"]},\"81\":{\"h\":\"Decoder 结构\"},\"82\":{\"h\":\"DecoderLayer\",\"t\":[\"Decoder模型结构图\",\"# 解码器层 = 自注意力子层 + 源注意力子层 + 前馈层 class DecoderLayer(nn.Module): \\\"Decoder is made of self-attn, src-attn, and feed forward (defined below)\\\" def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward # 自注意力子层 + 源注意力子层 + 前馈层 各需要一个 self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): \\\"Follow Figure 1 (right) for connections.\\\" m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward)\"]},\"83\":{\"h\":\"Decoder\",\"t\":[\"Decoder模型结构图\",\"# 解码器 = N个解码器层 + 层归一化 class Decoder(nn.Module): \\\"Generic N layer decoder with masking.\\\" def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): # 输入,编码器隐藏层输出,源掩码,目标掩码 for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x)\"]},\"84\":{\"h\":\"多头自注意力\",\"t\":[\"多头自注意力计算流程图\",\"class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): \\\"Take in model size and number of heads.\\\" super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # We assume d_v always equals d_k self.d_k = d_model // h # 每个头64维 self.h = h # 8个头 self.linears = clones(nn.Linear(d_model, d_model), 4) # W_q,W_k,W_v,W_projection self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): \\\"Implements Figure 2\\\" if mask is not None: # Same mask applied to all h heads. mask = mask.unsqueeze(1) nbatches = query.size(0) # 1) Do all the linear projections in batch from d_model => h x d_k query, key, value = [ lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) # (batches,heads,seq_len,d_k) for lin, x in zip(self.linears, (query, key, value)) ] # 2) Apply attention on all the projected vectors in batch. x, self.attn = attention( query, key, value, mask=mask, dropout=self.dropout ) # 3) \\\"Concat\\\" using a view and apply a final linear. x = ( x.transpose(1, 2) .contiguous() .view(nbatches, -1, self.h * self.d_k) ) del query del key del value return self.linears[-1](x)\",\"def attention(query, key, value, mask=None, dropout=None): \\\"Compute 'Scaled Dot Product Attention'\\\" d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) # 广播: (1,1,1,10) ---> (1,8,10,10) p_attn = scores.softmax(dim=-1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn\"]},\"85\":{\"h\":\"简析 LLaMA\",\"t\":[\"简析LLaMA\"]},\"86\":{\"h\":\"多模态\"},\"87\":{\"h\":\"庖丁解牛CLIP\",\"t\":[\"多模态模型CLIP原理与图片分类，文字搜索图像实战演练\",\"CLIP原始论文链接\"]},\"88\":{\"h\":\"引言\",\"t\":[\"2021 年可谓是视觉 Transformer（Vision Transformer）大放异彩的一年。自谷歌提出 ViT 之后，众多基于视觉 Transformer 的研究如潮水般涌来，广泛应用于各类计算机视觉任务。与此同时，OpenAI 在 2021 年 1 月发布的 DALL-E 和 CLIP，同样给计算机视觉领域带来了巨大影响。这两个模型都属于融合图像与文本的多模态模型，其中 DALL-E 是基于文本输入来生成图像的模型，而 CLIP 则是以文本作为监督信号，训练出具有可迁移能力的视觉模型。和 ViT 类似，DALL-E 和 CLIP 的出现也掀起了新一轮的研究热潮。\"]},\"89\":{\"h\":\"介绍\",\"t\":[\"CLIP的英文全称为Contrastive Language-Image Pre-training，它代表着一种基于对比文本-图像对的预训练方法，同时也指运用该方法构建的模型。CLIP属于基于对比学习的多模态模型。与计算机视觉（CV）领域中的一些对比学习方法，像MoCo和SimCLR有所不同，CLIP的训练数据采用的是文本-图像对，也就是一张图像搭配与之对应的文本描述。在训练过程中，借助对比学习机制，期望模型能够学习到文本和图像之间的匹配关系。\"]},\"90\":{\"h\":\"训练\",\"t\":[\"CLIP包含两个核心模型，分别是文本编码器（Text Encoder）和图像编码器（Image Encoder）。其中，文本编码器的作用是提取文本的特征，在实现时可采用自然语言处理（NLP）领域常用的文本Transformer模型；而图像编码器则用于提取图像的特征，在实际应用中可以选用常见的卷积神经网络（CNN）模型，也可以采用视觉Transformer模型。\",\"这里对提取的文本特征和图像特征进行对比学习。对于一个包含个文本-图像对的训练batch，将个文本特征和个图像特征两两组合，CLIP模型会预测出个可能的文本-图像对的相似度，这里的相似度直接计算文本特征和图像特征的余弦相似性（cosine similarity），即上图所示的矩阵。这里共有个正样本，即真正属于一对的文本和图像（矩阵中的对角线元素），而剩余的个文本-图像对为负样本，那么CLIP的训练目标就是最大个正样本的相似度，同时最小化个负样本的相似度，对应的伪代码实现如下所示：\",\"# image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter # 分别提取图像特征和文本特征 I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # 对两个特征进行线性投射，得到相同维度的特征，并进行l2归一化 I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # 计算缩放的余弦相似度：[n, n] logits = np.dot(I_e, T_e.T) * np.exp(t) # 对称的对比学习损失：等价于N个类别的cross_entropy_loss labels = np.arange(n) # 对角线元素的labels loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2\",\"为了训练CLIP模型，OpenAI从网络上收集了总计4亿对文本和图像，这些数据在论文中被称为WebImageText。若以文本单词数量来衡量，其规模与GPT-2训练时使用的WebText数据集相似。然而，从数据对的数量来看，它比谷歌的JFT-300M数据集还要多出1亿对，因此这是一个非常庞大的数据集。\",\"尽管CLIP是一个多模态模型，但其主要目的是训练可迁移的视觉模型。在论文中，文本编码器（Text Encoder）选择了一个包含6300万参数的Transformer模型，而图像编码器（Image Encoder）则采用了两种不同的架构：\",\"一种是常用的CNN架构ResNet。\",\"另一种是基于 Transformer 的ViT。\",\"ResNet包含五种不同尺寸的模型：ResNet50、ResNet101、RN50x4、RN50x16和RNx64（后三种模型是按照EfficientNet的缩放规则对ResNet分别放大4倍、16倍和64倍得到的），而ViT则选择了三种不同尺寸的模型：ViT-B/32、ViT-B/16和ViT-L/14。\",\"所有模型均训练了32个周期，使用AdamW优化器，并且在训练过程中采用了一个相对较大的批次大小：32768。由于数据量巨大，最大的ResNet模型RN50x64需要在592个V100 GPU上训练18天，而最大的ViT模型ViT-L/14则需要在256个V100 GPU上训练12天，这表明训练CLIP模型需要消耗大量的资源。对于ViT-L/14模型，还在336的分辨率下额外进行了一个周期的微调（finetune）以增强性能，论文发现这个模型的效果最佳，并将其标记为ViT-L/14@336，论文中进行对比实验的CLIP模型也采用了这一配置。\"]},\"91\":{\"h\":\"推理\",\"t\":[\"我们已经探讨了CLIP模型的运作机制，它由两个部分组成：一个视觉模型和一个文本模型。那么，如何将这个预训练的视觉模型应用到新的任务中呢？CLIP模型的一个显著优势是它能够进行zero-shot图像分类，这意味着它能够在没有任何特定任务训练数据的情况下，直接对图像进行分类。这不仅展示了CLIP的强大功能，也是其一大亮点。实现zero-shot分类的过程相当直接，可以概括为以下两个主要步骤：\",\"构建描述文本并提取特征：首先，根据任务的分类需求，为每个类别创建一个描述性的文本，例如“A photo of {label}”。这些文本随后被输入到文本编码器（Text Encoder）中，以生成相应的文本特征。如果有个类别，那么就会得到个文本特征。\",\"图像特征提取与分类：接下来，将待分类的图像输入到图像编码器（Image Encoder）中，以获取图像特征。然后，这些图像特征会与之前得到的个文本特征进行余弦相似度计算（这一过程与训练时相同）。最终，选择与图像特征相似度最高的文本所对应的类别，作为图像的分类预测结果。此外，这些相似度值可以被视为logits，通过softmax函数转换后，可以得到每个类别的预测概率。\",\"通过这种方式，CLIP模型能够在没有特定任务训练数据的情况下，直接对图像进行分类，这展示了其在图像分类任务中的灵活性和强大能力。\",\" 显然，我们通过利用CLIP模型的多模态能力，为特定任务动态构建了一个分类器。在这个过程中，文本编码器（Text Encoder）生成的文本特征相当于分类器的权重，而图像编码器（Image Encoder）提取的图像特征则是分类器的输入数据。以下是一个官方给出的CLIP模型的示例 ，该示例中的任务涉及8个类别:\",\"我们首先创建了各类别的文本描述，然后提取了相应的文本特征；\",\"然后我们读取要预测的图像，输入Image Encoder提取图像特征，并计算与文本特征的余弦相似度。\",\"# 1. 提取文本特征 texts = [ \\\"a page of text about segmentation\\\", \\\"a facial photo of a tabby cat\\\", \\\"a portrait of an astronaut with the American flag\\\", \\\"a rocket standing on a launchpad\\\", \\\"a red motorcycle standing in a garage\\\", \\\"a person looking at a camera on a tripod\\\", \\\"a black-and-white silhouette of a horse\\\", \\\"a cup of coffee on a saucer\\\" ] text_tokens = clip.tokenize([\\\"This is \\\" + desc for desc in texts]).cuda() with torch.no_grad(): text_features = model.encode_text(text_tokens).float() # 2. 提取图像特征 image_input = torch.tensor(np.stack(images)).cuda() with torch.no_grad(): image_features = model.encode_image(image_input).float() # 3. 计算余弦相似度 image_features /= image_features.norm(dim=-1, keepdim=True) text_features /= text_features.norm(dim=-1, keepdim=True) similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\",\"相似度如下所示，可以看到对于要预测的8个图像，按照最大相似度，其均能匹配到正确的文本标签：\",\"进一步地，我们也可以对得到的余弦相似度计算softmax，得到每个预测类别的概率值：\",\"text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1) top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)\",\"得到的预测概率如下所示，可以看到8个图像，CLIP模型均能够以较高的置信度给出正确的分类结果：\"]},\"92\":{\"h\":\"文本描述生成\",\"t\":[\"在使用CLIP模型进行zero-shot分类时，除了模型本身的应用，文本描述的生成也是一个关键环节。在之前的例子中，我们使用了“A photo of {label}”这样的格式来生成文本描述，但实际上，我们还有其他的选择。例如，我们可以直接使用类别标签作为文本描述。这种方法实际上与NLP领域的一个研究方向——prompt learning或prompt engineering——紧密相关。关于这一领域的详细综述，可以参考论文《Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing》。\",\"简单来说，prompt learning的核心思想是通过设计合适的prompt（提示），使得预训练模型能够直接应用于下游任务。这与传统的预训练加微调的方法有所不同。论文指出，如果我们直接使用类别标签作为文本描述，由于这些文本往往只是一个单词，缺乏具体的上下文，并且与CLIP模型的训练数据不完全一致，因此在效果上可能不如使用“A photo of {label}”这种格式（在ImageNet数据集上可以提升1.3%的效果）。\",\"此外，论文还实验了使用80个不同的prompt进行集成，结果发现在ImageNet数据集上能够带来3.5%的性能提升。具体的实验结果可以参考CLIP公开的notebook。\"]},\"93\":{\"h\":\"花卉图片分类\",\"t\":[\"本节我们将基于CLIP预训练模型实现Zero-Shot推理，训练使用到的数据集和AlexNet保持一致，因此这里就不再给出数据集下载链接了。\",\"图片分类实战 – 分别基于LeNet，AlexNet，VGG进行实现\",\"# 预训练模型名称 model_name = \\\"openai/clip-vit-large-patch14\\\" # 定义当前目录 current_dir = os.getcwd() model_dir = os.path.join(current_dir, model_name.replace(\\\"/\\\", \\\"_\\\")) # 检查当前目录是否有预训练权重文件，如果没有则下载 def download_pretrained_weights_if_needed(model_name, save_dir): if not os.path.exists(save_dir): try: print(f\\\"Downloading {model_name} to {save_dir}...\\\") snapshot_download(repo_id=model_name, local_dir=save_dir, local_dir_use_symlinks=False) print(f\\\"{model_name} downloaded successfully.\\\") except Exception as e: print(f\\\"Error downloading {model_name}: {e}\\\") download_pretrained_weights_if_needed(model_name, model_dir) # 加载模型和处理器 model = CLIPModel.from_pretrained(model_dir) processor = CLIPProcessor.from_pretrained(model_dir) device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\") model = model.to(device)\",\"在 openai/clip-vit-large-patch14 这个 CLIP 预训练模型中，图像编码器采用了 Vision Transformer（ViT）架构，具体使用的是 ViT-L/14 版本，文本编码器使用的是基于 Transformer 的架构。\",\"# 函数：生成文本嵌入 def text_embedding(texts): inputs = processor(text=texts, return_tensors=\\\"pt\\\", padding=True).to(device) with torch.no_grad(): embeddings = model.get_text_features(**inputs) return embeddings.cpu().numpy()\",\"这个函数的作用是将输入的文本转化为对应的嵌入表示（embedding）。它通过处理器对输入文本进行处理，使其符合模型的输入要求，然后利用模型获取文本特征，最后将结果转换为 numpy 数组格式返回，方便后续的计算和比较。\",\"def get_image_embeddings(image_paths): images = [] for image_path in image_paths: try: image = Image.open(image_path).convert(\\\"RGB\\\") images.append(image) except Exception as e: print(f\\\"Error loading image {e}\\\") if not images: return None inputs = processor(images=images, return_tensors=\\\"pt\\\").to(device) with torch.no_grad(): image_features = model.get_image_features(**inputs) return image_features.cpu().numpy()\",\"该函数作用是针对给定的图片路径，读取图片并将其转换为合适的格式后，通过模型获取图片的特征嵌入。如果在读取图片过程中出现错误，会进行相应的错误提示并返回 None。\",\"def cosine_similarity(vec1, vec2): vec1 = np.array(vec1) vec2 = np.array(vec2) return np.dot(vec1, vec2.T) / (np.linalg.norm(vec1, axis=1, keepdims=True) * np.linalg.norm(vec2, axis=1))\",\"在图文检索中，我们常常需要衡量文本嵌入和图片嵌入之间的相似度，这里采用了余弦相似度的计算方法。它将输入的向量转换为 numpy 数组后，按照余弦相似度的数学公式来计算两者的相似度数值。\",\"首先，我们需要根据上面给出的花卉数据集下载链接，将数据下载到当前项目目录下:\",\"其次，我们从flower_photos目录下读取出所有图片的路径:\",\"# 递归遍历目录获取所有图片路径 def get_all_image_paths(directory): image_paths = [] for root, _, files in os.walk(directory): for file in files: file_extension = os.path.splitext(file)[1].lower() if file_extension in ['.png', '.jpg', '.jpeg']: image_paths.append(os.path.join(root, file)) return image_paths image_paths = get_all_image_paths(\\\"./flower_photos\\\")\",\"同时将flower_photos下的子目录名作为我们的候选待匹配分类文本列表，并改造为a photo of 子目录名的格式，然后计算每个分类文本对应的文本嵌入向量:\",\"# 获取候选分类名列表 def get_candidates(directory): candidates = [] for sub_dir in os.listdir(directory): sub_dir_path = os.path.join(directory, sub_dir) if os.path.isdir(sub_dir_path): candidates.append(f\\\"a photo of {sub_dir}\\\") return candidates candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates)\",\"最后:\",\"分批次从图像列表中取出一批图像，获取其对应的图像嵌入向量列表\",\"计算这批图像嵌入向量中每一个图像嵌入向量和我们所有分类文本嵌入向量的余弦相似度\",\"针对每个图像嵌入向量取出和其相似度最高的那个文本嵌入向量的索引\",\"判断预测是否正确，统计正确率\",\"# 测试图片分类正确率 def accuracy(image_paths, candidates, text_embeddings, batch_size=64): correct_count = 0 total_count = len(image_paths) num_batches = (total_count + batch_size - 1) // batch_size # 分批次预测 for i in range(num_batches): start_idx = i * batch_size end_idx = min(start_idx + batch_size, total_count) # 取出当前批次的图像列表，并获得该批次图像列表对应的图像嵌入向量列表 batch_image_paths = image_paths[start_idx:end_idx] image_embeddings = get_image_embeddings(batch_image_paths) if image_embeddings is not None: # 计算图像嵌入向量列表中每个图像对应的嵌入向量和传入的分类文本嵌入向量的余弦相似度 similarities = cosine_similarity(image_embeddings, text_embeddings) # 针对每个图像嵌入向量获取和其相似度最高的分类文本嵌入向量索引下标 predicted_indices = np.argmax(similarities, axis=1) for j, predicted_index in enumerate(predicted_indices): # 针对每张图像，根据上述计算得到的和其相似度最高的分类文本索引，从候选分类文本集合中取出其分类名词 predicted_category = candidates[predicted_index].split(\\\" \\\")[-1] # 用当前图片外层目录的名字作为其分类名词 actual_category = os.path.basename(os.path.dirname(batch_image_paths[j])) # 比较两个分类名词是否相等 if predicted_category == actual_category: correct_count += 1 accuracy = correct_count / total_count return accuracy # 图片分类 def flowerClassify(): image_paths = get_all_image_paths(\\\"./flower_photos\\\") candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates) start_time = time.time() acc = accuracy(image_paths, candidates, text_embeddings, batch_size=64) end_time = time.time() print(f\\\"Time taken to test accuracy: {end_time - start_time:.2f} seconds\\\") print(f\\\"Accuracy: {acc * 100:.2f}%\\\")\",\"Time taken to test accuracy: 396.62 seconds Accuracy: 95.48%\"]},\"94\":{\"h\":\"文字搜索图像\",\"t\":[\"上面的花卉分类案例相当于使用图像去匹配最合适的文本描述，而这里我们将会反转这个逻辑，用文本描述去匹配最合适的图片内容。\",\"为了实现文字搜索图像的功能，我们只需要在计算出相似度得分矩阵后，以每个文本描述为一行，取出该行中得分最大的那一列，即为与当前文本描述相似度最高的那副图片，具体代码实现如下：\",\"# 找到与文本最匹配的图片 def find_most_matching_image(text, image_paths): text_emb = text_embedding([text]) image_embeddings = get_image_embeddings(image_paths) if image_embeddings is None: return None similarities = cosine_similarity(text_emb, image_embeddings) most_matching_index = np.argmax(similarities) return image_paths[most_matching_index]\",\"下面来实际展示一下效果，首先我们用data目录充当我们的图片库来源:\",\" 遍历data目录，拿到所有图片路径:\",\"# 遍历 data 目录获取所有图片路径 def get_images_from_data_dir(): data_dir = os.path.join(current_dir, 'data') if not os.path.exists(data_dir): print(f\\\"Data directory {data_dir} does not exist.\\\") return [] return get_all_image_paths(data_dir)\",\"这里以搜索向日葵花为例，我们首先获取图片库中所有图片，然后计算出和当前文本描述相似度最高的那副图片，并将图片展示出来:\",\"# 根据文字搜索图片 def searchPicByText(): image_paths = get_images_from_data_dir() query_text = \\\"a photo of a sunflowers\\\" most_matching_image = find_most_matching_image(query_text, image_paths) if most_matching_image: print(f\\\"The most matching image for '{query_text}' is: {most_matching_image}\\\") try: img = Image.open(most_matching_image) plt.imshow(img) plt.axis('off') plt.title(f\\\"Most matching image for '{query_text}'\\\") plt.show() except Exception as e: print(f\\\"Error opening image: {e}\\\") else: print(\\\"No matching image found.\\\")\",\"图片库中的图片： 运行上述代码，搜索出来的图片:\"]},\"95\":{\"h\":\"完整代码\",\"t\":[\"import time from matplotlib import pyplot as plt from transformers import CLIPProcessor, CLIPModel import torch from PIL import Image import numpy as np import warnings import os from huggingface_hub import snapshot_download warnings.filterwarnings(\\\"ignore\\\") # 模型名称 model_name = \\\"openai/clip-vit-large-patch14\\\" # 定义当前目录 current_dir = os.getcwd() model_dir = os.path.join(current_dir, model_name.replace(\\\"/\\\", \\\"_\\\")) # 检查当前目录是否有预训练权重文件，如果没有则下载 def download_pretrained_weights_if_needed(model_name, save_dir): if not os.path.exists(save_dir): try: print(f\\\"Downloading {model_name} to {save_dir}...\\\") snapshot_download(repo_id=model_name, local_dir=save_dir, local_dir_use_symlinks=False) print(f\\\"{model_name} downloaded successfully.\\\") except Exception as e: print(f\\\"Error downloading {model_name}: {e}\\\") download_pretrained_weights_if_needed(model_name, model_dir) # 加载模型和处理器 model = CLIPModel.from_pretrained(model_dir) processor = CLIPProcessor.from_pretrained(model_dir) device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\") model = model.to(device) # 函数：生成文本嵌入 def text_embedding(texts): inputs = processor(text=texts, return_tensors=\\\"pt\\\", padding=True).to(device) with torch.no_grad(): embeddings = model.get_text_features(**inputs) return embeddings.cpu().numpy() def get_image_embeddings(image_paths): images = [] for image_path in image_paths: try: image = Image.open(image_path).convert(\\\"RGB\\\") images.append(image) except Exception as e: print(f\\\"Error loading image {e}\\\") if not images: return None inputs = processor(images=images, return_tensors=\\\"pt\\\").to(device) with torch.no_grad(): image_features = model.get_image_features(**inputs) return image_features.cpu().numpy() def cosine_similarity(vec1, vec2): vec1 = np.array(vec1) vec2 = np.array(vec2) return np.dot(vec1, vec2.T) / (np.linalg.norm(vec1, axis=1, keepdims=True) * np.linalg.norm(vec2, axis=1)) # 递归遍历目录获取所有图片路径 def get_all_image_paths(directory): image_paths = [] for root, _, files in os.walk(directory): for file in files: file_extension = os.path.splitext(file)[1].lower() if file_extension in ['.png', '.jpg', '.jpeg']: image_paths.append(os.path.join(root, file)) return image_paths # 获取候选分类名列表 def get_candidates(directory): candidates = [] for sub_dir in os.listdir(directory): sub_dir_path = os.path.join(directory, sub_dir) if os.path.isdir(sub_dir_path): candidates.append(f\\\"a photo of {sub_dir}\\\") return candidates # 测试图片分类正确率 def accuracy(image_paths, candidates, text_embeddings, batch_size=64): correct_count = 0 total_count = len(image_paths) num_batches = (total_count + batch_size - 1) // batch_size for i in range(num_batches): start_idx = i * batch_size end_idx = min(start_idx + batch_size, total_count) batch_image_paths = image_paths[start_idx:end_idx] image_embeddings = get_image_embeddings(batch_image_paths) if image_embeddings is not None: similarities = cosine_similarity(image_embeddings, text_embeddings) predicted_indices = np.argmax(similarities, axis=1) for j, predicted_index in enumerate(predicted_indices): predicted_category = candidates[predicted_index].split(\\\" \\\")[-1] actual_category = os.path.basename(os.path.dirname(batch_image_paths[j])) if predicted_category == actual_category: correct_count += 1 accuracy = correct_count / total_count return accuracy # 图片分类 def flowerClassify(): image_paths = get_all_image_paths(\\\"./flower_photos\\\") candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates) start_time = time.time() acc = accuracy(image_paths, candidates, text_embeddings, batch_size=64) end_time = time.time() print(f\\\"Time taken to test accuracy: {end_time - start_time:.2f} seconds\\\") print(f\\\"Accuracy: {acc * 100:.2f}%\\\") ##################################################################################################3 # 遍历 data 目录获取所有图片路径 def get_images_from_data_dir(): data_dir = os.path.join(current_dir, 'data') if not os.path.exists(data_dir): print(f\\\"Data directory {data_dir} does not exist.\\\") return [] return get_all_image_paths(data_dir) # 找到与文本最匹配的图片 def find_most_matching_image(text, image_paths): text_emb = text_embedding([text]) image_embeddings = get_image_embeddings(image_paths) if image_embeddings is None: return None similarities = cosine_similarity(text_emb, image_embeddings) most_matching_index = np.argmax(similarities) return image_paths[most_matching_index] # 根据文字搜索图片 def searchPicByText(): image_paths = get_images_from_data_dir() query_text = \\\"a photo of a sunflowers\\\" most_matching_image = find_most_matching_image(query_text, image_paths) if most_matching_image: print(f\\\"The most matching image for '{query_text}' is: {most_matching_image}\\\") try: img = Image.open(most_matching_image) plt.imshow(img) plt.axis('off') plt.title(f\\\"Most matching image for '{query_text}'\\\") plt.show() except Exception as e: print(f\\\"Error opening image: {e}\\\") else: print(\\\"No matching image found.\\\")\"]},\"96\":{\"h\":\"小结\",\"t\":[\"在计算机视觉领域，常见的迁移学习方法是首先在大规模数据集（如ImageNet）上进行预训练，然后在具体的下游任务上进行微调。这种预训练通常是基于有监督学习的，需要大量的数据标注，因此成本较高。近年来，出现了一些基于自监督的方法，包括基于对比学习的方法（如MoCo和SimCLR）和基于图像掩码的方法（如MAE和BeiT）。自监督方法的优势在于不再需要标注数据。然而，无论是有监督还是自监督方法，在迁移到下游任务时，都需要进行有监督微调，无法实现zero-shot学习。对于有监督模型，由于它们在预训练数据集上采用固定类别数的分类器，因此在新的数据集上需要定义新的分类器来重新训练。对于自监督模型，代理任务通常是辅助进行表征学习，在迁移到其他数据集时也需要加上新的分类器进行有监督训练。\",\"然而，在NLP领域，基于自回归或语言掩码的预训练方法已经相对成熟，预训练模型很容易直接zero-shot迁移到下游任务，例如OpenAI的GPT-3。这种差异一方面是由于文本和图像属于两个完全不同的模态，另一个原因是NLP模型可以利用从互联网上收集的大量文本。因此，问题来了：能否基于互联网上的大量文本来预训练视觉模型？\",\"实际上，之前已经有一些研究工作探讨了使用文本作为监督信号来训练视觉模型。例如，2016年的工作《Learning Visual Features from Large Weakly Supervised Data》将这个问题转化为一个多标签分类任务，预测图像对应的文本的词袋模型；2017年的工作《Learning Visual N-Grams from Web Data》进一步扩展了这个方法来预测n-grams。最近的一些工作采用新的模型架构和预训练方法来从文本学习视觉特征，例如VirTex基于transformer的语言模型，ICMLM基于语言掩码的方法，ConVIRT基于对比学习的方法。总体来看，这方面的工作并不多，主要是因为这些方法难以实现较高的性能，例如2017年的那篇工作只在ImageNet上实现了11.5%的zero-shot性能，这远远低于ImageNet上的SOTA。此外，还有另一个方向，即基于文本弱监督来提升性能，例如谷歌的BiT和ViT基于JFT-300M数据集来预训练模型在ImageNet上取得SOTA。JFT-300M数据集是谷歌从互联网上收集的，通过一些自动化的手段将web text转化为18291个类别，但存在一定的噪声。尽管谷歌基于JFT-300M数据集取得了较好的结果，但这些模型仍然采用固定类别的softmax分类器进行预训练，这大大限制了它们的迁移能力和扩展性。\",\"谷歌采用的弱监督方法与以往方法的一个主要区别在于规模，或者说在于计算能力和数据集的规模。JFT-300M数据集的规模达到了上亿级别，谷歌利用强大的计算能力进行了预训练。相比之下，VirTex、ICMLM和ConVIRT仅在10万级别的数据上训练了几天。为了弥补数据规模上的差距，OpenAI从网络上收集了4亿条数据进行实验。然而，新的问题出现了：应该采用什么样的方法来进行训练。\",\"OpenAI首先尝试了VirTex模型，该模型联合训练一个CNN和文本transformer来预测图像的文本描述（image caption），但发现这种方法的训练效率（根据ImageNet数据集上的zero-shot性能评估）还不如直接预测词袋模型（bag of words），两者的训练效率相差3倍。如果进一步采用ConVIRT，即基于对比学习的方法，训练效率可以提高4倍。出现这种差异的原因不难理解，因为训练数据集中的文本-图像对是从互联网收集的，存在一定的噪声，即文本和图像可能不完全匹配。在这种情况下，适当降低训练目标反而可能取得更好的效果。\",\"从任务难度来看，排序为：Transformer Language Model > Bag of Words Prediction > Bag of Words Contrastive (CLIP)。由于训练数据量和模型计算量较大，训练效率成为一个至关重要的因素。因此，作者最终选择了对比学习方法来进行训练。\"]},\"97\":{\"h\":\"庖丁解牛BLIP2\",\"t\":[\"庖丁解牛BLIP2\",\"论文: https://arxiv.org/abs/2301.12597 代码: https://github.com/salesforce/LAVIS/tree/main/projects/blip2\"]},\"98\":{\"h\":\"背景\",\"t\":[\"多模态模型在过往发展的过程中，曾有一段时期一直在追求更大的网络架构（image encoder 和 text encoder/decoder）和 数据集，从而导致更大的训练代价。例如CLIP，400M数据，需要数百个GPU训练数十天，如何降低模型训练成本，同时具有很好的性能？\",\"这就是BLIP-2的起因，回顾下之前的多模态网络设计，三个模块（图像分支、文本分支、融合模块）:\",\"多模态网络设计\",\"(a) 早期的图文多模态：图像分支依赖目标检测器，模态融合比较弱，如VSE++。\",\"(b) 重点训练图像和文本特征提取，模态融合比较轻量，如CLIP。\",\"(c) 图像特征提取和模态融合都很重。\",\"(d) 侧重模态融合，特征提取网络相对轻量，如ViLT。\",\"模块\",\"(a)\",\"(b)\",\"(c)\",\"(d)\",\"理想情况\",\"视觉分支\",\"重\",\"重\",\"重\",\"轻\",\"重\",\"文本分支\",\"轻\",\"重\",\"轻\",\"轻\",\"重\",\"融合模块\",\"轻\",\"轻\",\"重\",\"重\",\"轻\",\"性能\",\"一般\",\"好\",\"好\",\"一般\",\"好\",\"训练代价\",\"中\",\"非常高\",\"非常高\",\"高\",\"中\",\"BLIP-2 基于 BLIP 架构，利用已有的ViT 和 LLM（均冻结）+ 一个的轻量Q-Former模块做模态融合，大幅降低训练成本。具有很强的zero-shot image-to-text generation能力，同时因LLM而具有了视觉推理能力。\"]},\"99\":{\"h\":\"模型结构\",\"t\":[\"BLIP-2 框架按照 Two-Stage 策略预训练轻量级查询 Transformer 以弥合模态差距。\",\"Stage 1: 不同模态数据的提取与融合。\",\"Stage 2: 把数据转换成LLM能识别的格式。\",\"Two-Stage流程\",\"从冻结的Image Encoder引到Vision-Language表征学习。\",\"从冻结的LLM引到Vision-Language生成学习，实现Zero Shot图文生成。\"]},\"100\":{\"h\":\"Stage 1: Representation Learning （表征学习）\",\"t\":[\"tage 1: Representation Learning （表征学习）\",\"Q-Former 由两个transformer模块组成，输入包含三部分：\",\"冻结参数的Image Encoder提取的图像embeddings\",\"Learned Queries\",\"Queries是一组可学习的embeddings，是第一个transformer模块的input，可认为是模型参数一部分\",\"推理时，Queries被用来从image encoder输出的embeddings里提取与input text最相关的视觉信息\",\"Input Text\",\"Stage 1 使用 图像-文本对 进行预训练，目标是训练好 Q-Former，以便 Queries 可以学习到如何更好地结合文本提取图片信息。\",\"对于Q-Former，一种比较好理解的方式：把Q-Former类比为一个Self-attention模块\",\"Q：learned queries\",\"K：input text\",\"V：image embeddings from Image Encoder\",\"Blip2Qformer核心代码实现如下:\",\"利用 query tokens 从 image embeddings 中提取与 text 最相关的视觉信息\",\"将输入的 input text 进行编码 , 然后使用第一个CLS Token 作为 input text representation\",\"class Blip2Qformer(Blip2Base): ... def forward(self, samples): image = samples[\\\"image\\\"] # (B,C,H,W) text = samples[\\\"text_input\\\"] # (B,seq_len) # frozen vit 将图片编码成 (B, seq_len, hidden_size) image_embeds = self.ln_vision(self.visual_encoder(image)) # 构建padding mask标注哪些image token是有效的 (B,seq_len) image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to( image.device ) # 初始化query tokens (B,seq_len,hidden_size) query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1) # query tokens 从 image embeddings 中提取与 text 最相关的视觉信息 # query_output (B,seq_len,hidden_size) query_output = self.Qformer.bert( query_embeds=query_tokens, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, use_cache=True, return_dict=True, ) image_feats = F.normalize( self.vision_proj(query_output.last_hidden_state), dim=-1 ) # 将input text 进行编码，维度为 (B,seq_len,hidden_size) text_tokens = self.tokenizer( text, padding=\\\"max_length\\\", truncation=True, max_length=self.max_txt_len, return_tensors=\\\"pt\\\", ).to(image.device) text_output = self.Qformer.bert( text_tokens.input_ids, attention_mask=text_tokens.attention_mask, # padding mask return_dict=True, ) # 取第一个cls token作为input text representation，维度为 (B,hidden_size) text_feat = F.normalize( self.text_proj(text_output.last_hidden_state[:, 0, :]), dim=-1 ) ...\",\"以上代码注释中统一用B代替image_batch和text_batch，以及seq_len和hidden_size也是同样处理手段，大家注意区分。\",\"为了训练好Q-Former，第一阶段设计了三个训练目标，分别如下:\"]},\"101\":{\"h\":\"1、Image-Text Contrastive Learning (ITC Loss, CLIP-like)\",\"t\":[\"目的: Image representation 与 Text representation，以最大化互信息\",\"自注意力掩码策略: Uni-modal Self-attention Mask（单模态自注意力）\",\"Queries 和 Text 仅能和自己的 tokens 做 attention（Query和Query、Text和Text）\",\"Uni-modal Self-attention Mask\",\"image_feats 中每个 image_feat 与 text_feat 计算一个 similarity score ，选择最大值作为这个图文对的相似度 :\",\"similarity score\",\"如何计算loss的: “in-batch negatives”，该方法正是CLIP在VLP领域发扬光大的。以下引用CLIP论文图做说明：\",\"in-batch negatives\",\"###============== Image-text Contrastive ===================### # 计算每个query token 和 text_feat 的相似度 , 得到相似度矩阵 (B,B,seq_len) # image_feats (B,seq_len,hidden_size) 变为 (B,1,seq_len,hidden_size) # text_feat (B,hidden_size) 变为 (B,hidden_size,1) sim_q2t = torch.matmul( image_feats.unsqueeze(1), text_feat.unsqueeze(-1) ).squeeze() # image-text similarity: aggregate across all query tokens # 保留和text_feat相似度最大的那个query token作为最后的相似度得分 , 维度为 (B,B) sim_i2t, _ = sim_q2t.max(-1) sim_i2t = sim_i2t / self.temp # 反过来计算text_feat 和 每个query token的相似度 , 得到相似度矩阵 (B,B,seq_len) # image_feats 维度变为 (B,hidden_size,seq_len) # text_feat (B,hidden_size) 变为 (B,1,1,hidden_size) sim_t2q = torch.matmul( text_feat.unsqueeze(1).unsqueeze(1), image_feats.permute(0, 2, 1) ).squeeze() # text-image similarity: aggregate across all query tokens # 保留和text_feat相似度最大的那个query token作为最后的相似度得分 , 维度为 (B,B) sim_t2i, _ = sim_t2q.max(-1) sim_t2i = sim_t2i / self.temp # 生成比标签 targets = torch.arange(image.size(0), device=image.device) # 计算 图文对比 Loss loss_itc = ( # sim_i2t 形状是 (B, B)，每一行表示一张图像和所有文本之间的相似度。 F.cross_entropy(sim_i2t, targets, label_smoothing=0.1) + F.cross_entropy(sim_t2i, targets, label_smoothing=0.1) ) / 2\"]},\"102\":{\"h\":\"2、Image-Text Matching (ITM Loss，二分类task)\",\"t\":[\"目的：通过学习image-text pair是否match，以细粒度对齐 Image representation 与 Text representation\",\"自注意力掩码策略: Bi-directional Self-attention Mask（双向自注意力）\",\"Queries 和Text都能和所有的tokens 做attention\",\"Bi-directional Self-attention Mask\",\"每个output query embedding送到二分类器中，得到一个logit；所有logits的平均作为最终的matching score:\",\"matching score\",\" ###============== Image-text Matching ===================### text_input_ids_world = text_tokens.input_ids text_attention_mask_world = text_tokens.attention_mask image_embeds_world = image_embeds with torch.no_grad(): # bs (batch size) ， diag_indices = [0,1,2,...,bs-1] diag_indices = torch.arange(bs, device=sim_t2i.device) # 把相似度矩阵对角线元素置为负无穷大，以避免模型将匹配图文对挑选为负样本 # (0,0) , (1,1) ... (bs-1,bs-1) 位置处设置为 -10000 sim_t2i[diag_indices, diag_indices] = -10000 sim_i2t[diag_indices, diag_indices] = -10000 weights_t2i = F.softmax(sim_t2i, dim=1) weights_i2t = F.softmax(sim_i2t, dim=1) # 为每个文本选择一个负样本图像 image_embeds_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_t2i[b], 1).item() image_embeds_neg.append(image_embeds_world[neg_idx]) image_embeds_neg = torch.stack(image_embeds_neg, dim=0) # 为每个图像选择一个负样本文本 text_ids_neg = [] text_atts_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_i2t[b], 1).item() text_ids_neg.append(text_input_ids_world[neg_idx]) text_atts_neg.append(text_attention_mask_world[neg_idx]) text_ids_neg = torch.stack(text_ids_neg, dim=0) text_atts_neg = torch.stack(text_atts_neg, dim=0) # 构建输入文本列表: [正样本batch，负样本batch1，负样本batch2] ，维度为 (3*bs,seq_len) text_ids_all = torch.cat( [text_tokens.input_ids, text_tokens.input_ids, text_ids_neg], dim=0 ) text_atts_all = torch.cat( [text_tokens.attention_mask, text_tokens.attention_mask, text_atts_neg], dim=0, ) # 构建query tokens列表: [正样本batch，负样本batch1，负样本batch2] ，维度为 (3*bs,seq_len,hidden_size) query_tokens_itm = self.query_tokens.expand(text_ids_all.shape[0], -1, -1) query_atts_itm = torch.ones(query_tokens_itm.size()[:-1], dtype=torch.long).to( image.device ) # 构建query和text的padding mask ，维度为 (3*bs,seq_len) attention_mask_all = torch.cat([query_atts_itm, text_atts_all], dim=1) # 构建输入图像列表: [正样本batch，负样本batch1，负样本batch2] ，维度为 (3*bs,seq_len,hidden_size) image_embeds_all = torch.cat( [image_embeds, image_embeds_neg, image_embeds], dim=0 ) image_atts_all = torch.ones(image_embeds_all.size()[:-1], dtype=torch.long).to( image.device ) # 1. 将输入文本转换为嵌入列表后和query tokens 在seq_len维度上拼接起来，维度为 (3*bs,text_seq_len + query_tokens_seq_len,hidden_size) # 2. 将文本和query tokens拼接得到的结果和图像嵌入进行cross attention计算，编码后得到输出的结果 output_itm = self.Qformer.bert( text_ids_all, query_embeds=query_tokens_itm, attention_mask=attention_mask_all, encoder_hidden_states=image_embeds_all, encoder_attention_mask=image_atts_all, return_dict=True, ) # 取 (3*bs,text_seq_len + query_tokens_seq_len,hidden_size) 中 query tokens部分的结果，维度为 (3*bs,query_tokens_seq_len,hidden_size) vl_embeddings = output_itm.last_hidden_state[:, : query_tokens_itm.size(1), :] # 把query tokens部分的每个位置都映射到2维匹配空间，维度为 (3*bs,query_tokens_seq_len,2) vl_output = self.itm_head(vl_embeddings) # 取每个位置的平均作为最终的匹配得分，维度为 (3*bs,2) logits = vl_output.mean(dim=1) # 构建匹配标签: [正样本batch=1，负样本batch1=0，负样本batch2=0] ，维度为 (3*bs) itm_labels = torch.cat( [torch.ones(bs, dtype=torch.long), torch.zeros(2 * bs, dtype=torch.long)], dim=0, ).to(image.device) # 计算交叉熵损失 loss_itm = F.cross_entropy(logits, itm_labels)\",\"当文本和query tokens同时输入BertModel时，BertEmbeddings会将text embeddings和query tokens的embeddings在seq_len维度上拼接起来。\",\"class BertEmbeddings(nn.Module): ... def forward( self, input_ids=None, position_ids=None, query_embeds=None, past_key_values_length=0, ): # 计算序列长度 if input_ids is not None: seq_length = input_ids.size()[1] else: seq_length = 0 # 如果未提供位置id，则自动生成 if position_ids is None: position_ids = self.position_ids[ :, past_key_values_length : seq_length + past_key_values_length ].clone() # 词嵌入与位置嵌入相加，若有query_embeds则拼接 if input_ids is not None: embeddings = self.word_embeddings(input_ids) if self.position_embedding_type == \\\"absolute\\\": position_embeddings = self.position_embeddings(position_ids) embeddings = embeddings + position_embeddings if query_embeds is not None: embeddings = torch.cat((query_embeds, embeddings), dim=1) else: embeddings = query_embeds embeddings = self.LayerNorm(embeddings) embeddings = self.dropout(embeddings) return embeddings\",\"下图展示了 Image-Text Matching 的完整计算流程，关于BertModel的代码解析部分，将会在下文进行详细讲解:\",\"Image-Text Matching\"]},\"103\":{\"h\":\"3、Image-Grounded Text Generation (ITG Loss, GPT-like)\",\"t\":[\"目的：让Q-Former学习“图生文”的能力，即给定Input Image，生成Text\",\"自注意力掩码策略：Multimodal Causal Self-attention Mask（多模态因果自监督）\",\"Queies 可以和所有自己的tokens做attention\",\"Text 可以和所有的query tokens 及 当前token之前的text tokens做attention\",\"Multimodal Causal Self-attention Mask\",\"视觉编码阶段:\",\"图像通过视觉编码器（如 ViT）编码为图像特征 image_embeds。Query tokens 通过 cross-attention 吸收图像特征，再通过 self-attention 生成压缩的视觉表示。缓存 query tokens 的 self-attention 的 past_key_values（而非 cross-attention 的 key/value）。\",\"QFormer 会使用 past_key_values 缓存和复用 EncoderLayer 中 self-attention 的 key/value :\",\"BertSelfAttention: 自注意力和交叉注意力流程统一化，每次计算后返回本次可能需要缓存的key & value\",\"class BertSelfAttention(nn.Module): ... def forward( self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False, ): # 判断是否为交叉注意力 is_cross_attention = encoder_hidden_states is not None # 交叉注意力则key和value都来自图像,key来自query tokens if is_cross_attention: key_layer = self.transpose_for_scores(self.key(encoder_hidden_states)) value_layer = self.transpose_for_scores(self.value(encoder_hidden_states)) attention_mask = encoder_attention_mask # 如果有缓存的key,value传入, 此时先用text embedding计算出key和value # 再和缓存的key,value在seq_len的维度拼接起来 elif past_key_value is not None: key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) key_layer = torch.cat([past_key_value[0], key_layer], dim=2) # (Batch,Heads,Seq_len,Hidden_size) value_layer = torch.cat([past_key_value[1], value_layer], dim=2) else: # 自注意力 key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) # 交叉注意力: 传入图像，则q来自query tokens # 自注意力: q来自query tokens 或者 text embedding mixed_query_layer = self.query(hidden_states) query_layer = self.transpose_for_scores(mixed_query_layer) # * 缓存key和value past_key_value = (key_layer, value_layer) # 计算注意力分数 attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) attention_scores = attention_scores / math.sqrt(self.attention_head_size) if attention_mask is not None: # 应用注意力掩码 attention_scores = attention_scores + attention_mask # softmax归一化得到注意力概率 attention_probs = nn.Softmax(dim=-1)(attention_scores) if is_cross_attention and self.save_attention: self.save_attention_map(attention_probs) attention_probs.register_hook(self.save_attn_gradients) # dropout防止过拟合 attention_probs_dropped = self.dropout(attention_probs) # 计算上下文表示 context_layer = torch.matmul(attention_probs_dropped, value_layer) context_layer = context_layer.permute(0, 2, 1, 3).contiguous() new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) context_layer = context_layer.view(*new_context_layer_shape) outputs = ( (context_layer, attention_probs) if output_attentions else (context_layer,) ) # outputs 列表最后一个记录了缓存的key和value outputs = outputs + (past_key_value,) return outputs\",\"BertLayer: 负责组织自注意力和交叉注意力的运算流程\",\"class BertLayer(nn.Module): ... def forward( self, hidden_states, # query tokens attention_mask=None, # query token padding mask head_mask=None, encoder_hidden_states=None, # image tokens encoder_attention_mask=None, # image padding mask past_key_value=None, output_attentions=False, query_length=0, ): self_attn_past_key_value = ( past_key_value[:2] if past_key_value is not None else None ) # 自注意力运算 self_attention_outputs = self.attention( hidden_states, attention_mask, head_mask, output_attentions=output_attentions, past_key_value=self_attn_past_key_value, # 缓存的key和value ) attention_output = self_attention_outputs[0] outputs = self_attention_outputs[1:-1] present_key_value = self_attention_outputs[-1] # 交叉注意力运算 if query_length > 0: query_attention_output = attention_output[:, :query_length, :] if self.has_cross_attention: cross_attention_outputs = self.crossattention( query_attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions=output_attentions, ) query_attention_output = cross_attention_outputs[0] outputs = ( outputs + cross_attention_outputs[1:-1] ) ... outputs = (layer_output,) + outputs outputs = outputs + (present_key_value,) # outputs 列表最后一个记录了缓存的key和value return outputs\",\"BertEncoder: 负责组织多个 BertLayer 叠加的运算流程\",\"class BertEncoder(nn.Module): ... def forward( self, hidden_states, # query tokens attention_mask=None, # query tokens padding mask head_mask=None, encoder_hidden_states=None, # images encoder_attention_mask=None, # images padding mask past_key_values=None, use_cache=None, output_attentions=False, output_hidden_states=False, return_dict=True, query_length=0, ): ... for i in range(self.config.num_hidden_layers): layer_module = self.layer[i] ... # 如果有缓存，则计算当前层BertLayer时，会从缓存中取出对应层先前缓存的key&value past_key_value = past_key_values[i] if past_key_values is not None else None layer_outputs = layer_module( hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, query_length, ) hidden_states = layer_outputs[0] # 每一层BertLayer产生的key&value都会进行缓存 if use_cache: next_decoder_cache += (layer_outputs[-1],) ... return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=next_decoder_cache, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, )\",\"Image-Grounded Text Generation 学习目标\",\" ... query_output = self.Qformer.bert( query_embeds=query_tokens, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, use_cache=True, # 缓存key&value return_dict=True, ) ... ##================= Image Captioning ========================## # 这一部分的目标是：根据图像特征，使用 Q-Former 解码器生成文本描述（caption） # Step 1: 准备 decoder 的输入 token IDs decoder_input_ids = text_tokens.input_ids.clone() # 将第一个 token 替换为 BOS（Begin Of Sentence）标记，表示“开始生成句子” decoder_input_ids[:, 0] = self.tokenizer.bos_token_id # Step 2: 构造训练目标 labels # 将 padding token 替换为 -100，这是 CrossEntropyLoss 默认忽略的标签值 labels = decoder_input_ids.masked_fill( decoder_input_ids == self.tokenizer.pad_token_id, -100 ) # Step 3: 构建 attention_mask（包含 query tokens 和 文本 token 的 mask） # query_atts 是 query tokens 的 attention mask，全为 1（因为都是有效 token） query_atts = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(image.device) # 将 query token 的 mask 和文本 token 的 mask 拼接在一起 attention_mask = torch.cat([query_atts, text_tokens.attention_mask], dim=1) # Step 4: 调用 Q-Former 解码器进行文本生成 lm_output = self.Qformer( decoder_input_ids, # 输入 token ID 序列（如 [BOS], dog, is...） attention_mask=attention_mask, # 指明哪些位置是有效的（非 padding） past_key_values=query_output.past_key_values, # 编码器输出的 key/value，包含图像信息 return_dict=True, # 返回字典格式结果 labels=labels, # 训练目标，用于计算 loss ) # Step 5: 提取语言模型损失 loss_lm = lm_output.loss # 使用交叉熵损失衡量生成与真实之间的差异\",\"文本生成阶段:\",\"将缓存的 past_key_values 作为文本解码器的初始状态。\",\"文本 token 在自回归生成时，通过 self-attention 复用缓存的视觉信息。\",\"BertLMHeadModel: 自回归语言建模任务（如文本生成）\",\"class BertLMHeadModel(BertPreTrainedModel): ... def forward( self, input_ids=None, attention_mask=None, position_ids=None, head_mask=None, query_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, past_key_values=None, use_cache=True, output_attentions=None, output_hidden_states=None, return_dict=None, return_logits=False, is_decoder=True, reduction=\\\"mean\\\", ): ... # 调用 BertModel 进行文本编码 (结合缓存的attention key&value) outputs = self.bert( input_ids, attention_mask=attention_mask, position_ids=position_ids, head_mask=head_mask, query_embeds=query_embeds, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, is_decoder=is_decoder, ) sequence_output = outputs[0] ... # self.cls 是一个分类头（BertOnlyMLMHead），它将每个 token 的向量映射到词汇表空间（logits） prediction_scores = self.cls(sequence_output) ... lm_loss = None if labels is not None: # 因为我们要预测下一个 token，所以把 logits 和 labels 错位对齐： # shifted_prediction_scores: 所有 token 的预测（除了最后一个） shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous() # labels: 所有 token 的真实值（从第二个开始） labels = labels[:, 1:].contiguous() loss_fct = CrossEntropyLoss(reduction=reduction, label_smoothing=0.1) lm_loss = loss_fct( shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1), ) if reduction == \\\"none\\\": lm_loss = lm_loss.view(prediction_scores.size(0), -1).sum(1) ... return CausalLMOutputWithCrossAttentions( loss=lm_loss, logits=prediction_scores, past_key_values=outputs.past_key_values, hidden_states=outputs.hidden_states, attentions=outputs.attentions, cross_attentions=outputs.cross_attentions, )\",\"BertModel 的 forward 方法中，当is_decoder=True时，会在get_extended_attention_mask方法中，构建一个下三角矩阵作为因果掩码矩阵。\"]},\"104\":{\"h\":\"Stage 2: Generative Learning（生成学习）\",\"t\":[\"Stage 2 是为了把 Q-Former 和冻结参数的 LLM 连接起来，以利用 LLM 的文本生成能力。\",\"支持两种LLM（decoder only、encoder-decoder based）:\",\"Generative Learning\",\"首先输入图片，直接输入冻结参数的 Image Encoder，得到图像的表征。\",\"然后图像的表征和 Queries 一起送入 Q-Former，得到 Queries 的输出 ，使用全连接 (FC) 层将 线性投影到与 LLM 的text embedding相同维度。\",\"后将投影后的 添加到 input text embeddings前面，Queries 的输出蕴含了视觉信息，送入LLM时，充当了soft visual prompts 。\",\"由于 Q-Former 已经过预训练以提取语言信息视觉表示，因此它有效地充当信息瓶颈，将最有用的信息提供给 LLM，同时删除不相关的视觉信息。这减少了LLM学习视觉语言对齐的负担，从而缓解了灾难性的遗忘问题。\",\"Blip2Qformer 的generate方法负责完成图像描述生成（图文到文本）:\",\"class Blip2Qformer(Blip2Base): ... def generate( self, samples, # 输入样本，包含图像和可选文本 use_nucleus_sampling=False, # 是否使用核采样（top-p采样） num_beams=3, # beam search的beam数量 max_length=30, # 生成文本的最大长度 min_length=10, # 生成文本的最小长度 top_p=0.9, # 核采样的概率阈值 repetition_penalty=1.0, # 重复惩罚系数 ): # 1. 图像编码阶段 image = samples[\\\"image\\\"] # 通过视觉编码器（如ViT）提取图像特征 (B, 257, D) image_embeds = self.ln_vision(self.visual_encoder(image)) # 2. 处理beam search扩展 if not use_nucleus_sampling: # 如果是beam search，需要复制图像特征以匹配beam数量 # (B, 257, D) -> (B*num_beams, 257, D) image_embeds = image_embeds.repeat_interleave(num_beams, dim=0) else: # 核采样时不扩展beam num_beams = 1 # 创建图像注意力掩码（全1，表示所有图像token有效） image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to( image.device ) # 3. 准备生成参数 model_kwargs = { \\\"encoder_hidden_states\\\": image_embeds, # 图像特征作为cross-attention的输入 \\\"encoder_attention_mask\\\": image_atts, # 图像注意力掩码 } # 4. 初始化文本输入（以BOS token开头） # 形状: (batch_size, 1)，初始为[BOS] input_ids = ( torch.LongTensor(image.size(0), 1) .fill_(self.tokenizer.bos_token_id) .to(image.device) ) # 5. 扩展可学习的query tokens # query_tokens形状: (batch_size, num_query_tokens, D) query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1) # 6. 调用Q-Former的生成方法 outputs = self.Qformer.generate( input_ids=input_ids, # 初始文本token [BOS] query_embeds=query_tokens, # 可学习query tokens max_length=max_length, # 最大生成长度 min_length=min_length, # 最小生成长度 num_beams=num_beams, # beam数量 do_sample=use_nucleus_sampling, # 是否采样 top_p=top_p, # 核采样参数 eos_token_id=self.tokenizer.sep_token_id, # 结束符 pad_token_id=self.tokenizer.pad_token_id, # 填充符 **model_kwargs # 图像特征和掩码 ) # 7. 解码生成的token id为文本 captions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True) return captions\"]},\"105\":{\"h\":\"庖丁解牛VIT\",\"t\":[\"多模态模型VIT原理与图片分类实战演练\",\"Vision Transformer是2021年谷歌在ICLR上提出的算法，它首次将NLP领域火热的Transformer模型架构移植到了CV领域，打破了这两个领域壁垒，并取得不错的成效。\",\"Vision Transformer的模型结构相比于Transformer来说更简单，在Transformer模型中，主要包含Encoder和Decoder结构，而ViT(Vision Transformer)仅借鉴了Encoder结构。\",\"ViT原论文中最核心的结论是: 当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果。\",\"归纳偏置:\",\"归纳偏置能够帮助学习算法缩小搜索范围，快速找到合适的模型。\",\"例如，在图像分类任务中，如果没有任何归纳偏置，学习算法需要在所有可能的函数空间中搜索最优模型，这几乎是不可能完成的任务。而通过引入特定的归纳偏置，如局部性和平移不变性（CNN 所具备的），可以将搜索范围限制在满足这些性质的模型子空间内，大大提高学习效率。\",\"但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些，因为Transformer和CNN相比缺少归纳偏置（inductive bias），即一种先验知识，提前做好的假设。CNN具有两种归纳偏置，一种是局部性（locality/two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征；一种是平移不变形（translation equivariance）。当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型。\"]},\"106\":{\"h\":\"原理\",\"t\":[\"本文将通过一个花卉分类的实战案例结合ViT原论文，来帮助大家梳理清楚Vision Transformer的核心流程实现。\"]},\"107\":{\"h\":\"0. 数据下载\",\"t\":[\"实验采用的是花蕊数据集，共5个类别，约4000多个样本。\",\"数据集下载：https://pan.baidu.com/s/137mO-7PY1jDq1Wp0NNyT3A?pwd=qvmq\",\"数据集加载代码:\",\"def read_split_data(root: str, val_rate: float = 0.2): random.seed(0) # 保证随机结果可复现 assert os.path.exists(root), \\\"dataset root: {} does not exist.\\\".format(root) # 遍历文件夹，一个文件夹对应一个类别 flower_class = [cla for cla in os.listdir(root) if os.path.isdir(os.path.join(root, cla))] # 排序，保证顺序一致 flower_class.sort() # 生成类别名称以及对应的数字索引 class_indices = dict((k, v) for v, k in enumerate(flower_class)) json_str = json.dumps(dict((val, key) for key, val in class_indices.items()), indent=4) with open('class_indices.json', 'w') as json_file: json_file.write(json_str) train_images_path = [] # 存储训练集的所有图片路径 train_images_label = [] # 存储训练集图片对应索引信息 val_images_path = [] # 存储验证集的所有图片路径 val_images_label = [] # 存储验证集图片对应索引信息 every_class_num = [] # 存储每个类别的样本总数 supported = [\\\".jpg\\\", \\\".JPG\\\", \\\".png\\\", \\\".PNG\\\"] # 支持的文件后缀类型 # 遍历每个文件夹下的文件 for cla in flower_class: cla_path = os.path.join(root, cla) # 遍历获取supported支持的所有文件路径 images = [os.path.join(root, cla, i) for i in os.listdir(cla_path) if os.path.splitext(i)[-1] in supported] # 获取该类别对应的索引 image_class = class_indices[cla] # 记录该类别的样本数量 every_class_num.append(len(images)) # 按比例随机采样验证样本 val_path = random.sample(images, k=int(len(images) * val_rate)) for img_path in images: if img_path in val_path: # 如果该路径在采样的验证集样本中则存入验证集 val_images_path.append(img_path) val_images_label.append(image_class) else: # 否则存入训练集 train_images_path.append(img_path) train_images_label.append(image_class) print(\\\"{} images were found in the dataset.\\\".format(sum(every_class_num))) print(\\\"{} images for training.\\\".format(len(train_images_path))) print(\\\"{} images for validation.\\\".format(len(val_images_path))) plot_image = True if plot_image: # 绘制每种类别个数柱状图 plt.bar(range(len(flower_class)), every_class_num, align='center') # 将横坐标0,1,2,3,4替换为相应的类别名称 plt.xticks(range(len(flower_class)), flower_class) # 在柱状图上添加数值标签 for i, v in enumerate(every_class_num): plt.text(x=i, y=v + 5, s=str(v), ha='center') # 设置x坐标 plt.xlabel('image class') # 设置y坐标 plt.ylabel('number of images') # 设置柱状图的标题 plt.title('flower class distribution') plt.show() return train_images_path, train_images_label, val_images_path, val_images_label\",\"自定义一个MyDataSet类来封装我们加载得到的数据集:\",\"from torch.utils.data import Dataset from PIL import Image import torch class MyDataSet(Dataset): \\\"\\\"\\\"自定义数据集\\\"\\\"\\\" def __init__(self, images_path: list, images_class: list, transform=None): \\\"\\\"\\\" 初始化自定义数据集类 :param images_path: 包含所有图像文件路径的列表 :param images_class: 包含所有图像对应类别的列表，与 images_path 中的图像一一对应 :param transform: 图像预处理的转换操作，默认为 None \\\"\\\"\\\" self.images_path = images_path self.images_class = images_class self.transform = transform def __len__(self): \\\"\\\"\\\" 返回数据集中图像的数量 :return: 数据集中图像的数量 \\\"\\\"\\\" return len(self.images_path) def __getitem__(self, item): \\\"\\\"\\\" 根据索引获取数据集中的图像和对应的标签 :param item: 图像的索引 :return: 经过预处理的图像和对应的标签 \\\"\\\"\\\" # 打开指定索引的图像文件 img = Image.open(self.images_path[item]) # RGB为彩色图片，L为灰度图片 # 检查图像是否为 RGB 模式，如果不是则抛出异常 if img.mode != 'RGB': raise ValueError(\\\"image: {} isn't RGB mode.\\\".format(self.images_path[item])) # 获取对应图像的标签 label = self.images_class[item] # 如果定义了图像预处理转换操作，则对图像进行处理 if self.transform is not None: img = self.transform(img) return img, label @staticmethod def collate_fn(batch): \\\"\\\"\\\" 自定义的批量数据处理函数，用于将一个批次的数据组合成一个张量 :param batch: 一个批次的数据，包含图像和对应的标签 :return: 组合后的图像张量和标签张量 \\\"\\\"\\\" # 官方实现的default_collate可以参考 # https://github.com/pytorch/pytorch/blob/67b7e751e6b5931a9f45274653f4f653a4e6cdf6/torch/utils/data/_utils/collate.py # 将一个批次的数据拆分为图像和标签两个元组 images, labels = tuple(zip(*batch)) # 将图像元组堆叠成一个四维张量，维度为 (batch_size, channels, height, width) images = torch.stack(images, dim=0) # 将标签元组转换为一个一维张量 labels = torch.as_tensor(labels) return images, labels\",\"两点注意:\",\"当使用 DataLoader 从数据集（Dataset）中加载数据时，它会将多个样本收集起来形成一个批次，但默认的组合方式可能不满足所有需求，这时就可以自定义 collate_fn 函数。\",\"@staticmethod 是 Python 中的一个装饰器，用于将一个方法定义为静态方法。静态方法是类中的一种特殊方法，它与类的实例和类本身都没有直接关联，可以直接通过类名调用，不需要创建类的实例。\"]},\"108\":{\"h\":\"1. 图片预处理\",\"t\":[\"预处理这个步骤在论文里并没有详细说明，但是对于ViT这个结构而言，输入的图片尺寸并不是自定义的，ViT-B/16为例，输入的图片尺寸必须为224x224。\",\"在 ViT - B/16 中，“B” 代表的是模型的基础（Base）版本 ，“16” 表示每个图像块的大小是 16x16 像素；ViT 通常在大规模数据集（如 ImageNet）上进行预训练，而预训练过程中使用的输入图像尺寸通常固定为 224x224。在预训练时，模型的参数是根据这个特定尺寸的输入数据进行优化和学习的。当我们在其他任务中使用预训练好的模型时，为了充分利用预训练的权重，也需要保持输入图像尺寸与预训练时一致，这样可以保证模型的特征提取能力和性能。\",\"因此，首先需要对输入图片进行尺寸变化，具体方式可以是直接缩放(Resize)，也可以进行随机裁剪(RandomResizedCrop)。\",\"对数据集和验证集划分之后，这里对训练集的处理方式是随机切成224x224像素的图片，然后进行水平翻转，再进行归一化和标准化处理；对验证集的处理方式是先Resize成256x256的图片，再从中心位置裁剪成224x224，再进行归一化和标准化处理。\",\"# 定义一个字典 data_transform，用于存储训练集和验证集的图像预处理转换操作 data_transform = { # 训练集的预处理转换操作 \\\"train\\\": transforms.Compose([ # 随机裁剪输入图像，将裁剪后的图像调整为 224x224 大小 # 这是一种数据增强的方式，通过随机裁剪可以增加训练数据的多样性，提高模型的泛化能力 transforms.RandomResizedCrop(224), # 以 0.5 的概率随机水平翻转图像 # 同样是数据增强的手段，增加了图像的多样性，有助于模型学习到不同方向的特征 transforms.RandomHorizontalFlip(), # 将 PIL 图像或 NumPy 数组转换为 PyTorch 的张量（Tensor） # 同时会将像素值从 [0, 255] 范围缩放到 [0, 1] 范围 transforms.ToTensor(), # 对图像进行归一化处理 # 第一个参数 [0.5, 0.5, 0.5] 是图像每个通道的均值，第二个参数 [0.5, 0.5, 0.5] 是图像每个通道的标准差 # 归一化有助于模型更快地收敛，提高训练的稳定性 transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) ]), # 验证集的预处理转换操作 \\\"val\\\": transforms.Compose([ # 将图像的短边缩放为 256 像素，长边按比例缩放 # 这一步是为了保证图像的整体比例不变，后续再进行裁剪操作 transforms.Resize(256), # 从图像的中心位置裁剪出 224x224 大小的图像 # 验证集不需要进行数据增强，只需要将图像调整到合适的大小 transforms.CenterCrop(224), # 将 PIL 图像或 NumPy 数组转换为 PyTorch 的张量（Tensor） # 同样会将像素值从 [0, 255] 范围缩放到 [0, 1] 范围 transforms.ToTensor(), # 对图像进行归一化处理，参数与训练集的归一化参数相同 # 保证训练集和验证集的数据处理方式一致 transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) ]) }\",\"下面我们将用于图片变换的transforms流水线和上面自定义的MyDataSet类都封装到DataLoader去。\",\"train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(data_path) # 实例化训练数据集 train_dataset = MyDataSet(images_path=train_images_path, images_class=train_images_label, transform=data_transform[\\\"train\\\"]) # 实例化验证数据集 val_dataset = MyDataSet(images_path=val_images_path, images_class=val_images_label, transform=data_transform[\\\"val\\\"]) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn) val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=val_dataset.collate_fn)\"]},\"109\":{\"h\":\"2. 图片切割\",\"t\":[\"Transformer需要输入的是一维的Token，对于二维的图像，一种朴素的想法就是把一个个像素点拉平，这样就成了一个一维序列。但是这样造成的一个后果是计算量太庞大，比如一张224x224的图片，变成1维度之后就成了50176，相当于直接输入一篇五万字的文章，模型难以计算。\",\"那么，一个改进的想法就是把一张图片分成nxn个Patch，每一个Patch作为一个Token，这样计算量就大大减小了。\",\"以ViT-B/16为例，将输入图片(224x224)按照16x16大小的Patch进行划分，划分后可以得到共个Patch。每个Patch是三通道的小图片，shape为(16, 16, 3)，将其展平就变成了一个长度为768的向量。\",\"每一个向量作为一个单独的输入，那样我们总共有196个向量，在代码中，可以变成一个[196,768]的矩阵，进行并行输入。\",\"这一步的操作在论文中是直接采用切割的处理办法，但是实际的代码实现中，采用了一种更巧妙的解决思路，就是利用一个卷积核大小为16x16，步距为16，卷积核个数为768的卷积层来进行实现。\",\"再来回顾我们的卷积层计算公式：\",\"输入为[224,244,3]，经过卷积层变成[14,14,768]，再映射为[196,768]。\",\"这样，就完成了从图片到Token之间的转换，我们通过自定义一个PatchEmbed类完成上述工作。\",\"class PatchEmbed(nn.Module): \\\"\\\"\\\" 2D Image to Patch Embedding 该类的作用是将二维图像分割成多个图像块（patch），并将这些图像块嵌入到一个低维向量空间中 \\\"\\\"\\\" def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None): \\\"\\\"\\\" 初始化 PatchEmbed 类 :param img_size: 输入图像的尺寸，默认为 224。如果传入一个整数，则表示图像是正方形，边长为该整数； :param patch_size: 每个图像块的尺寸，默认为 16。同样，如果传入一个整数，则表示图像块是正方形，边长为该整数； :param in_c: 输入图像的通道数，默认为 3（对应 RGB 图像） :param embed_dim: 嵌入维度，即每个图像块经过卷积操作后得到的特征向量的维度，默认为 768 :param norm_layer: 归一化层，默认为 None。如果传入一个归一化层类，则使用该层进行归一化；否则使用 nn.Identity() 表示不进行归一化 \\\"\\\"\\\" super().__init__() # 将 img_size 和 patch_size 转换为元组形式，如果传入的是整数，则将其转换为 (整数, 整数) 的形式 img_size = (img_size, img_size) patch_size = (patch_size, patch_size) self.img_size = img_size self.patch_size = patch_size # 计算网格大小，即图像在水平和垂直方向上分别可以划分的图像块数量 self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) # 计算图像块的总数，即网格大小的乘积 self.num_patches = self.grid_size[0] * self.grid_size[1] # 定义一个二维卷积层，用于将输入图像分割成多个图像块并进行嵌入 # in_c 是输入通道数，embed_dim 是输出通道数（也就是卷积核的数量） # kernel_size 是卷积核的大小，这里设置为图像块的大小 # stride 是卷积核的步长，这里设置为图像块的大小，确保卷积操作不会重叠 self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size) # 如果传入了归一化层类，则使用该层进行归一化；否则使用 nn.Identity() 表示不进行归一化 self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() def forward(self, x): \\\"\\\"\\\" 前向传播函数 :param x: 输入的图像张量，形状为 [B, C, H, W]，其中 B 是批量大小，C 是通道数，H 是图像高度，W 是图像宽度 :return: 经过处理后的图像块嵌入张量，形状为 [B, num_patches, embed_dim] \\\"\\\"\\\" # 获取输入图像张量的形状 B, C, H, W = x.shape # 注意下面的embed_dim代表的是卷积核的数量，也就是经过卷积后拼接得到的特征图(输出通道)数量 # H`和 W`代表输出特征图的宽和高 # 首先使用卷积层对输入图像进行处理，得到形状为 [B, embed_dim, H', W'] 的特征图 # 然后将特征图的最后两维展平为一维，得到形状为 [B, embed_dim, num_patches] 的张量 # 最后交换第 1 维和第 2 维，得到形状为 [B, num_patches, embed_dim] 的张量 # 这里的 num_patches 是图像块的总数 x = self.proj(x).flatten(2).transpose(1, 2) # 对处理后的张量进行归一化操作 x = self.norm(x) return x\",\"用一个简化版的例子说明上述过程:\",\"核心要点: 将卷积后的通道维数作为embedding的维度，卷积后剩余的长和宽相乘作为时间维度，由此把图片转换为序列的embedding形式。\"]},\"110\":{\"h\":\"3. 添加[class]token\",\"t\":[\"在上面的结构图中可以看到，输入Encoder的最左侧部分添加了一个0*这个Token，这个就是额外添加的一个[class]token，单独用来处理类别信息，经过Encoder之后，需要单独将这个Token再提取出来，输入到MLP Head之中再输出分类结果。\",\"这也是为什么结构图中MLP Head的位置是和这个[class]token对齐。\",\"这里简单介绍一下CLS TOKEN的作用:\",\"[CLS] Token 的作用是通过训练过程中损失值的降低，模型学会如何通过注意力机制将图像的有效信息汇聚到 [CLS] Token 中，从而完成图像分类任务。\",\"初始化： \",\"[CLS] Token 是一个随机初始化的向量，初始时没有任何语义信息。\",\"位置编码被添加到 patch 嵌入中，以保留图像的空间信息。\",\"前向传播： \",\"输入图像被分割成 patches，并通过线性变换映射到嵌入空间。\",\"[CLS] Token 被添加到 patch 嵌入序列的开头。\",\"通过多层 Transformer Encoder，模型计算每个 patch 嵌入（包括 [CLS] Token）与其他 patch 嵌入的关系。\",\"注意力汇聚： \",\"在每一层 Transformer 中，[CLS] Token 通过自注意力机制与其他 patch 嵌入交互。\",\"模型学会将图像中与分类任务相关的信息汇聚到 [CLS] Token 中。\",\"损失计算与反向传播： \",\"[CLS] Token 的输出向量被输入到分类头中，用于预测图像的类别。\",\"通过计算损失（如交叉熵损失），模型更新参数，使得 [CLS] Token 能够更好地聚合图像信息。\",\"收敛： \",\"随着训练的进行，损失值逐渐降低，模型学会如何通过注意力机制将图像的有效信息汇聚到 [CLS] Token 中。\",\"最终，[CLS] Token 的输出向量能够很好地表示图像的全局特征，用于分类任务。\",\"[CLS] Token 能起作用的原因在于：\",\"注意力机制的特性： \",\"自注意力机制能够捕捉图像中任意两个 patches 之间的关系。\",\"[CLS] Token 通过与其他 patches 的交互，能够动态地聚合图像信息。\",\"训练目标的引导： \",\"训练过程中，损失函数直接作用于 [CLS] Token 的输出。\",\"模型被强制学会将图像的有效信息汇聚到 [CLS] Token 中，以最小化损失。\",\"全局特征表示： \",\"[CLS] Token 位于序列的开头，能够通过多层 Transformer 逐步聚合全局信息。\",\"最终，[CLS] Token 的输出向量能够很好地表示图像的全局特征。\",\"class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, embed_layer=None): \\\"\\\"\\\" Args: img_size (int, tuple): 输入图像的尺寸 patch_size (int, tuple): 图像块的尺寸 in_c (int): 输入图像的通道数 num_classes (int): 分类任务的类别数 embed_dim (int): 嵌入维度 embed_layer (nn.Module): 图像块嵌入层 \\\"\\\"\\\" super(VisionTransformer, self).__init__() # 保存分类任务的类别数 self.num_classes = num_classes # 保存嵌入维度，同时作为特征数量，以保持与其他模型的一致性 self.num_features = self.embed_dim = embed_dim # 只有一个特殊标记（分类标记） self.num_tokens = 1 # 创建图像块嵌入层 self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # 计算图像块的数量 num_patches = self.patch_embed.num_patches # 创建可学习的分类标记 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 使用截断正态分布初始化分类标记 nn.init.trunc_normal_(self.cls_token, std=0.02) ... def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] # 将输入图像进行图像块嵌入 x = self.patch_embed(x) # [B, 196, 768] # [1, 1, 768] -> [B, 1, 768] # 扩展分类标记以匹配输入批次大小 cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将分类标记和图像块嵌入拼接 x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] ... # 返回分类标记对应的特征,x[:,0]对应维度为[B,1,768] return x[:,0]; def forward(self, x): # 提取输入图像的特征 x = self.forward_features(x) # 通过分类头 -- [B,1,768] x = self.head(x) return x\"]},\"111\":{\"h\":\"4. 添加位置编码\",\"t\":[\"在Transformer中，位置编码的作用是为了记忆输入的语序信息。ViT中，同样需要位置编码来记录各图像块之间的位置信息。\",\"这里主要有两种位置编码思路，一种思路是在转换之前(14,14)的图像块矩阵添加二维(2-D)位置编码，另一种思路是在转换后(196+1)这个维度上添加一维(1-D)位置编码。\",\"论文作者也对其做了实验，实验结果如下表所示：\",\" 可以看到，添加一维位置编码和二维位置编码并没有太大的差异。作者随后也对一维位置编码的结果进行了可视化，结果如下图所示：\",\" 上图中是每一个Patch中各位置的位置编码相似性度量，越接近黄色的位置代表越靠近位置编码的中心位置，可以看到，即使是一维位置编码，同样可以比较好地记录二维信息。\",\"class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, representation_size=None, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., embed_layer=None, norm_layer=None, act_layer=None): super(VisionTransformer, self).__init__() # 保存分类任务的类别数 self.num_classes = num_classes # 保存嵌入维度，同时作为特征数量，以保持与其他模型的一致性 self.num_features = self.embed_dim = embed_dim # 只有一个特殊标记（分类标记） self.num_tokens = 1 # 创建图像块嵌入层 self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # 计算图像块的数量 num_patches = self.patch_embed.num_patches # 创建可学习的分类标记 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 创建可学习的位置嵌入 self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim)) # 创建丢弃层，用于位置嵌入后的随机丢弃 self.pos_drop = nn.Dropout(p=drop_ratio) ... # 权重初始化 # 使用截断正态分布初始化位置嵌入 nn.init.trunc_normal_(self.pos_embed, std=0.02) # 使用截断正态分布初始化分类标记 nn.init.trunc_normal_(self.cls_token, std=0.02) ... def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] # 将输入图像进行图像块嵌入 x = self.patch_embed(x) # [B, 196, 768] # [1, 1, 768] -> [B, 1, 768] # 扩展分类标记以匹配输入批次大小 cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将分类标记和图像块嵌入拼接 x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] # 加上位置嵌入并进行随机丢弃 x = self.pos_drop(x + self.pos_embed) ... # 返回分类标记对应的特征 return x[:, 0] def forward(self, x): # 提取输入图像的特征 x = self.forward_features(x) # 通过分类头 x = self.head(x) return x\",\"上面代码实现中使用的是可学习位置嵌入，具体解释如下:\",\"可学习位置嵌入（learnable positional embedding）是指位置嵌入的参数是可以在模型训练过程中通过反向传播算法进行更新的。具体来说，在模型初始化时，位置嵌入会被初始化为一组特定的值（通常是随机初始化或者初始化为零），然后在训练过程中，这些值会根据模型的损失函数不断调整，以使得模型能够学习到最适合当前任务的位置表示。\"]},\"112\":{\"h\":\"5. Encoder\",\"t\":[\"ViT虽然采用的是Transformer Encoder的结构，但是和Transformer原始的Encoder还是有所区别，我将两者的结构进行对比，如下图所示，左侧为Transformer原始的Encoder结构。\",\" 可以看到，大致上两者结构是相同的，主要区别在于Norm层的顺序，原始Transformer的Norm层在多头注意力和前馈网络之后，而ViT将其放到前面，这里的原因，论文里没有做解释。\",\"关于Norm层，ViT仍是采用Transformer中用到Layer Normalization，计算公式如下：\",\"Norm层之后同样是多头注意力层(Multi-Head Attention)，和Transformer中的一样。\",\"后面的MLP是个单独的结构，就是两个线性层+GELU激活函数+Dropout的结构 ：\",\" MLP Block 中第一个线性层把输入特征投影到一个更高维度的空间后，不同特征之间能够进行更多样的组合。这有助于模型发现输入数据中更复杂的模式和关系。第二个线性层再把高维特征映射回原来的维度，这样就可以提取出对最终任务有帮助的特征组合。\",\"单一的线性层只能进行线性变换，其表达能力是有限的。在两个线性层之间通常会插入一个非线性激活函数（如 GELU），这样就能让 MLP 学习到输入数据的非线性特征。第一个线性层将输入特征映射到更高维度的空间，在这个高维空间里，数据的分布更加稀疏，也就为非线性激活函数提供了更多可以学习的特征组合，从而增强了模型的表达能力。\",\"class Block(nn.Module): def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm): super(Block, self).__init__() # 第一个归一化层，对输入进行归一化处理 self.norm1 = norm_layer(dim) # 多头自注意力层 self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio) # DropPath 层，用于随机深度，当 drop_path_ratio 大于 0 时使用，否则使用恒等映射 self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity() # 第二个归一化层，对经过注意力层的输出进行归一化处理 self.norm2 = norm_layer(dim) # 计算 MLP 的隐藏维度 mlp_hidden_dim = int(dim * mlp_ratio) # 创建 MLP 层 self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio) def forward(self, x): # 残差连接：输入加上经过归一化和注意力层处理后的输出 x = x + self.drop_path(self.attn(self.norm1(x))) # 残差连接：输入加上经过归一化和 MLP 层处理后的输出 x = x + self.drop_path(self.mlp(self.norm2(x))) return x\",\"class Mlp(nn.Module): \\\"\\\"\\\" MLP as used in Vision Transformer, MLP-Mixer and related networks \\\"\\\"\\\" def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.): super().__init__() # 如果未指定 out_features，则默认为 in_features out_features = out_features or in_features # 如果未指定 hidden_features，则默认为 in_features hidden_features = hidden_features or in_features # 第一个全连接层，将输入特征映射到隐藏特征空间 self.fc1 = nn.Linear(in_features, hidden_features) # 激活函数层，默认使用 GELU 激活函数 self.act = act_layer() # 第二个全连接层，将隐藏特征映射到输出特征空间 self.fc2 = nn.Linear(hidden_features, out_features) # Dropout 层，用于防止过拟合 self.drop = nn.Dropout(drop) def forward(self, x): # 通过第一个全连接层 x = self.fc1(x) # 通过激活函数层 x = self.act(x) # 应用 Dropout x = self.drop(x) # 通过第二个全连接层 x = self.fc2(x) # 再次应用 Dropout x = self.drop(x) return x\",\"一个block之后维度依然和输入相同，都是197 x 768 ，因此可以堆叠多个block。\"]},\"113\":{\"h\":\"6. 多头自注意力\",\"t\":[\"ViT中的多头自注意力模块实现逻辑和Transformer基本一致，主要的区别就是去掉了Paddding_Mask和Casual_Mask部分相关的掩码逻辑。\",\"下面所给出的代码实现，注意是通过一个线性层来同时计算qkv三个矩阵，这样可以提升计算效率。\",\"class Attention(nn.Module): def __init__(self, dim, # 嵌入层维度 num_heads=8, # 注意力头的数量，默认为8 qkv_bias=False, # 是否在生成Q、K、V时使用偏置，默认为False qk_scale=None, # 缩放因子，用于调整注意力分数，若为None则使用默认值 attn_drop_ratio=0., # 注意力矩阵的丢弃率，默认为0 proj_drop_ratio=0.): # 投影层的丢弃率，默认为0 super(Attention, self).__init__() self.num_heads = num_heads # 保存注意力头的数量 head_dim = dim // num_heads # 计算每个注意力头的维度 self.scale = qk_scale or head_dim ** -0.5 # 确定缩放因子，若qk_scale未指定，则使用默认的缩放因子 # 定义一个线性层，将输入的维度dim映射到dim * 3，用于同时生成查询（Q）、键（K）和值（V） self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) # 定义注意力矩阵的丢弃层，防止过拟合 self.attn_drop = nn.Dropout(attn_drop_ratio) # 定义投影层，将多头注意力的输出进行线性变换 self.proj = nn.Linear(dim, dim) # 定义投影层的丢弃层，防止过拟合 self.proj_drop = nn.Dropout(proj_drop_ratio) # 没有padding_mask, casual_mask def forward(self, x): # [batch_size, num_patches + 1, total_embed_dim] # 获取输入张量x的形状，B为批量大小，N为序列长度（包含分类token），C为输入token的总维度 B, N, C = x.shape # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim] # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head] # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head] # 通过qkv线性层将输入x映射到dim * 3的维度，然后调整形状并重新排列维度 # 下面的3是因为我们用一次矩阵运算得到了拼接在一起的Q,K,V矩阵，这里需要将其分离开来 qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) # [batch_size, num_heads, num_patches + 1, embed_dim_per_head] # 从qkv张量中分离出查询（Q）、键（K）和值（V） # 注意: Q,K,V计算来源相同,因此是自注意力 q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use tensor as tuple) # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1] # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1] # 将Q和K的转置相乘，得到注意力分数矩阵，再乘以缩放因子scale attn = (q @ k.transpose(-2, -1)) * self.scale # 对注意力分数矩阵应用softmax函数，得到注意力权重矩阵 attn = attn.softmax(dim=-1) # 对注意力权重矩阵应用丢弃层，防止过拟合 attn = self.attn_drop(attn) # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head] # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head] # reshape: -> [batch_size, num_patches + 1, total_embed_dim] # 将注意力权重矩阵与V相乘，得到每个注意力头的输出 # 对输出进行维度交换和形状调整，将多个注意力头的输出合并为一个张量 x = (attn @ v).transpose(1, 2).reshape(B, N, C) # 通过投影层对合并后的张量进行线性变换 x = self.proj(x) # 对投影后的结果应用丢弃层，防止过拟合 x = self.proj_drop(x) return x\",\"关于多头注意力机制流程不太清楚的，可以看这篇文章。\"]},\"114\":{\"h\":\"7. MLP Head\",\"t\":[\"在Transformer Encoder输出结果之后，需要再将第一个添加的Class Token提取出来，然后输入到MLP Head进行分类。在论文中，作者先是在ImageNet21K上进行预训练，MLP Head结构由Linear+tanh激活函数+Linear组成，但是迁移到其它数据集训练时，只需要用一个一个Linear即可。\",\"class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, representation_size=None, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., embed_layer=None, norm_layer=None, act_layer=None): super(VisionTransformer, self).__init__() # 保存分类任务的类别数 self.num_classes = num_classes # 保存嵌入维度，同时作为特征数量，以保持与其他模型的一致性 self.num_features = self.embed_dim = embed_dim # 只有一个特殊标记（分类标记） self.num_tokens = 1 # 如果没有提供归一化层，则使用默认的 LayerNorm，epsilon 为 1e-6 norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6) # 如果没有提供激活函数层，则使用 GELU 激活函数 act_layer = act_layer or nn.GELU # 创建图像块嵌入层 self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # 计算图像块的数量 num_patches = self.patch_embed.num_patches # 创建可学习的分类标记 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 创建可学习的位置嵌入 self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim)) # 创建丢弃层，用于位置嵌入后的随机丢弃 self.pos_drop = nn.Dropout(p=drop_ratio) # 创建Encoder Block块序列 self.blocks = nn.Sequential(*[ Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i], norm_layer=norm_layer, act_layer=act_layer) for i in range(depth) ]) # 创建归一化层 self.norm = norm_layer(embed_dim) ############################# MLP Head ############################################ # 更新特征数量为表示层的维度 self.num_features = representation_size # 创建预输出层，包含一个线性层和一个 Tanh 激活函数 self.pre_logits = nn.Sequential(OrderedDict([ (\\\"fc\\\", nn.Linear(embed_dim, representation_size)), (\\\"act\\\", nn.Tanh()) ])) # 分类头 # 如果类别数大于 0，则创建线性分类头，否则为恒等映射 self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity() ########################################################################### # 权重初始化 # 使用截断正态分布初始化位置嵌入 nn.init.trunc_normal_(self.pos_embed, std=0.02) # 使用截断正态分布初始化分类标记 nn.init.trunc_normal_(self.cls_token, std=0.02) # 应用自定义的权重初始化函数 self.apply(_init_vit_weights) def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] # 将输入图像进行图像块嵌入 x = self.patch_embed(x) # [B, 196, 768] # [1, 1, 768] -> [B, 1, 768] # 扩展分类标记以匹配输入批次大小 cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将分类标记和图像块嵌入拼接 x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] # 加上位置嵌入并进行随机丢弃 x = self.pos_drop(x + self.pos_embed) # 通过Encoder Block块序列 x = self.blocks(x) # 进行归一化 x = self.norm(x) # 返回分类标记对应的特征 -- 先交给预输出层进行处理 return self.pre_logits(x[:, 0]) def forward(self, x): # 提取输入图像的特征 x = self.forward_features(x) # 通过分类头 -- 映射到分类空间中去 x = self.head(x) return x\",\"self.pre_logits 模块可以看作是一个特征预处理模块，它位于最终分类头之前。通过将特征映射到特定的维度并进行非线性变换，该模块可以为后续的分类任务提供更具区分性和稳定性的特征表示，从而提高模型的分类性能。\",\"输出结果之后，再和真实标签做交叉熵损失，这样就可以完成ViT的训练过程。\",\"def train_one_epoch(model, optimizer, data_loader, device, epoch): ... # 遍历数据加载器中的每个批次数据 for step, data in enumerate(data_loader): # 解包数据，得到图像和对应的标签 images, labels = data # 累加当前批次的样本数到总样本数中 sample_num += images.shape[0] # 将图像数据移动到指定设备上，并通过模型进行前向传播，得到预测结果 pred = model(images.to(device)) # 从预测结果中找出每个样本预测概率最大的类别索引 pred_classes = torch.max(pred, dim=1)[1] # 计算预测正确的样本数，并累加到累计正确样本数中 accu_num += torch.eq(pred_classes, labels.to(device)).sum() # 计算预测结果与真实标签之间的交叉熵损失 loss = loss_function(pred, labels.to(device)) # 进行反向传播，计算梯度 loss.backward() ...\"]},\"115\":{\"h\":\"效果对比\",\"t\":[\"在论文中，作者将ViT和之前图像分类领域比较强的ResNet模型进行了对比测试，结果如下：\",\" 可以看到，右图中，作者使用了谷歌制作的JFT-300M数据集，当数据量小于30M时，ViT的效果表现不如ResNet，但是当数据量逐渐增大时，ViT才会慢慢超越ResNet。由此可见ViT工作的局限性，它必须要在超大数据集上进行预训练，然后再拿到其它数据集上做迁移学习，才会有好的效果。\",\"关于ViT模型的不同版本，论文里也做了说明： 其中的Layers就是Transformer Encoder中重复堆叠Encoder Block的次数，Hidden Size就是对应通过Embedding层后每个token的dim（向量的长度），MLP size是Transformer Encoder中MLP Block第一个全连接的节点个数（是Hidden Size的四倍），Heads代表Transformer中Multi-Head Attention的heads数。\",\"在深度学习领域，当提到模型参数量时，“M” 通常是 “million” 的缩写，代表 “百万”。所以参数量为 86M 就意味着模型大约有 86×1000000 = 8600000（八百六十万）个可训练参数。\",\"与之类似的还有 “B”，它是 “billion” 的缩写，代表 “十亿”。例如参数量为 1.2B 就表示模型大约有 1.2×1000000000 = 1200000000（十二亿）个可训练参数。\"]},\"116\":{\"h\":\"注意力可视化\",\"t\":[\"ViT这篇论文长达二十多页，里面包含了非常丰富的成果，其中包括注意力可视化。由于作者是首次将Transformer应用到图像领域，里面包含了注意力机制，那么作者就想把注意力得到的结果(也就是Q-K矩阵乘积)换源到图像上，得到结果如下图所示：\",\"可以看到，模型自动学习到了如果注意画面中的分类主体。\"]},\"117\":{\"h\":\"混合模型探索\",\"t\":[\"在论文的最后，作者又探索了一种混合模型(Hybrid)，就是将传统CNN和Transformer进行结合。\",\"下表中对比了ViT、ResNet和混合模型在不同图像分类数据集上的测试结果，可以看到当Epochs增大时，ResNet和混合模型的效果均不如ViT模型。\",\"混合模型的常见结合方式:\",\"CNN 作为特征提取器，Transformer 作为编码器 \",\"先用 CNN 对输入数据进行初步的特征提取，利用 CNN 的局部特征提取能力快速捕捉图像的底层特征。例如，在图像分类任务中，可以使用预训练的 ResNet 等 CNN 模型提取图像的特征图。\",\"然后将 CNN 提取的特征图转换为序列形式，输入到 Transformer 中进行进一步的处理。Transformer 可以利用其自注意力机制捕捉特征之间的长距离依赖关系，对特征进行更深入的建模。\",\"交错堆叠 CNN 和 Transformer 模块 \",\"在模型架构中，将 CNN 层和 Transformer 层交错堆叠。例如，先经过一层或多层 CNN 进行局部特征提取，然后再经过一层 Transformer 捕捉全局信息，如此反复。这样可以在模型的不同阶段交替利用 CNN 和 Transformer 的优势。\",\"在 Transformer 中引入卷积操作 \",\"在 Transformer 的架构中融入卷积操作，例如在多头自注意力机制或前馈网络中引入卷积层。这样可以为 Transformer 赋予局部特征提取的能力，同时保留其捕捉长距离依赖的优势。\"]},\"118\":{\"h\":\"加载预训练模型\",\"t\":[\"上面已经给出了数据集加载以及ViT模型核心代码实现了，下面我们将进入训练流程；首先说明，本次训练是基于预训练好的ViT-B/16这个模型进行微调，整体结构图如下：\",\"具体为vit_base_patch16_224_in21k这个模型:\",\"vit：代表 Vision Transformer。\",\"base：表示模型的规模。\",\"patch16：意味着在处理图像时，会将输入图像分割成大小为 16×16 像素的图像块（patches）。\",\"224：指的是输入图像的尺寸为 224×224 像素。在预训练和使用该模型时，需要将输入图像调整为这个固定的尺寸。\",\"in21k：该模型是在 ImageNet - 21k 数据集上进行预训练的。ImageNet - 21k 是一个大规模的图像数据集，包含大约 21000 个类别和 1.4 亿张图像。在如此大规模的数据集上进行预训练，模型能够学习到丰富的图像特征和模式，具有较强的泛化能力。\",\"def vit_base_patch16_224_in21k(num_classes: int = 21843, has_logits: bool = True): \\\"\\\"\\\" ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929). ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer. weights ported from official Google JAX impl: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch16_224_in21k-e5005f0a.pth \\\"\\\"\\\" model = VisionTransformer(img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, representation_size=768 if has_logits else None, num_classes=num_classes) return model # 加载预训练好的vit_base_patch16_224_in21k模型权重文件 model = vit_base_patch16_224_in21k(num_classes=5, has_logits=False).to(device) weights_dict = torch.load(args.weights, map_location=device) model.load_state_dict(weights_dict, strict=False)\",\"加载该模型后，训练了10个epoch，验证集上准确率达到了98.5%。整体模型还是比较大的，预训练权重大小为393MB，但是训练速度还是挺快的，因为在代码中有个冻结权重的操作，主干部分全部冻结，仅训练分类头。\",\"for name, para in model.named_parameters(): # 除head, pre_logits外，其他权重全部冻结 if \\\"head\\\" not in name and \\\"pre_logits\\\" not in name: para.requires_grad_(False) else: print(\\\"training {}\\\".format(name))\",\"训练与评估流程的代码为模版代码，考虑篇幅原因，这里不再贴出，大家可以自行拉取项目完整代码进行学习:\",\"https://pan.baidu.com/s/1rkdjdlR37O7gSr9j1mhjBg?pwd=vket\"]},\"119\":{\"h\":\"总结\",\"t\":[\"Vision Transformer证明了使用Transformer结构可以有效处理图像数据，并且取得了与卷积神经网络（CNN）相媲美的效果。\",\"统一多模态的可能性：使用Transformer架构为未来的多模态统一提供了可能性。\",\"图像到文本的桥梁：架起了图像空间到文本空间的桥梁。\",\"ViT核心：如何将二维图像转换为一维时间序列？通过将图像切成小片（Patches），并按行优先排序来实现。\"]},\"120\":{\"h\":\"开源项目\"},\"121\":{\"h\":\"杂谈\"},\"122\":{\"h\":\"\",\"t\":[\"404 Not Found\"]}},\"dirtCount\":0,\"index\":[[\"杂谈\",{\"0\":{\"121\":1}}],[\"架起了图像空间到文本空间的桥梁\",{\"1\":{\"119\":1}}],[\"架构中\",{\"1\":{\"32\":1}}],[\"架构\",{\"1\":{\"22\":1,\"93\":1,\"98\":1}}],[\"统一多模态的可能性\",{\"1\":{\"119\":1}}],[\"统计正确率\",{\"1\":{\"93\":1}}],[\"统计量\",{\"1\":{\"40\":1}}],[\"考虑篇幅原因\",{\"1\":{\"118\":1}}],[\"除head\",{\"1\":{\"118\":1}}],[\"除了最后一个\",{\"1\":{\"103\":1}}],[\"除了模型本身的应用\",{\"1\":{\"92\":1}}],[\"亿张图像\",{\"1\":{\"118\":1}}],[\"意味着在处理图像时\",{\"1\":{\"118\":1}}],[\"赋予局部特征提取的能力\",{\"1\":{\"117\":1}}],[\"交错堆叠\",{\"1\":{\"117\":1}}],[\"交叉注意力运算\",{\"1\":{\"103\":1}}],[\"交叉注意力\",{\"1\":{\"103\":1}}],[\"交叉注意力则key和value都来自图像\",{\"1\":{\"103\":1}}],[\"先经过一层或多层\",{\"1\":{\"117\":1}}],[\"先用\",{\"1\":{\"117\":1}}],[\"先交给预输出层进行处理\",{\"1\":{\"114\":1}}],[\"混合模型的常见结合方式\",{\"1\":{\"117\":1}}],[\"混合模型探索\",{\"0\":{\"117\":1}}],[\"混合模型改进\",{\"1\":{\"37\":1}}],[\"换源到图像上\",{\"1\":{\"116\":1}}],[\"换句话说\",{\"1\":{\"40\":1}}],[\"里面包含了注意力机制\",{\"1\":{\"116\":1}}],[\"里面包含了非常丰富的成果\",{\"1\":{\"116\":1}}],[\"百万\",{\"1\":{\"115\":1}}],[\"向量的长度\",{\"1\":{\"115\":1}}],[\"向量映射成两个分数\",{\"1\":{\"66\":1}}],[\"才会有好的效果\",{\"1\":{\"115\":1}}],[\"才能保证整个网络输出与输入点的顺序无关\",{\"1\":{\"40\":1}}],[\"右图中\",{\"1\":{\"115\":1}}],[\"右填充\",{\"1\":{\"46\":1}}],[\"累加当前批次的样本数到总样本数中\",{\"1\":{\"114\":1}}],[\"没有padding\",{\"1\":{\"113\":1}}],[\"没有固定顺序\",{\"1\":{\"29\":1}}],[\"键\",{\"1\":{\"113\":2}}],[\"确定缩放因子\",{\"1\":{\"113\":1}}],[\"确保卷积操作不会重叠\",{\"1\":{\"109\":1}}],[\"时使用\",{\"1\":{\"112\":1}}],[\"时间戳等\",{\"1\":{\"39\":1}}],[\"左侧为transformer原始的encoder结构\",{\"1\":{\"112\":1}}],[\"权重初始化\",{\"1\":{\"111\":1,\"114\":1}}],[\"权重由\",{\"1\":{\"40\":1}}],[\"越接近黄色的位置代表越靠近位置编码的中心位置\",{\"1\":{\"111\":1}}],[\"位于序列的开头\",{\"1\":{\"110\":1}}],[\"位置嵌入会被初始化为一组特定的值\",{\"1\":{\"111\":1}}],[\"位置处设置为\",{\"1\":{\"102\":1}}],[\"位置编码的作用是为了记忆输入的语序信息\",{\"1\":{\"111\":1}}],[\"位置编码被添加到\",{\"1\":{\"110\":1}}],[\"位置编码通常使用正弦和余弦函数生成\",{\"1\":{\"74\":1}}],[\"位置编码\",{\"1\":{\"74\":1,\"111\":2}}],[\"位置编码为可学习的矩阵\",{\"1\":{\"49\":1}}],[\"位置\",{\"1\":{\"21\":1}}],[\"损失函数直接作用于\",{\"1\":{\"110\":1}}],[\"损失值逐渐降低\",{\"1\":{\"110\":1}}],[\"损失计算与反向传播\",{\"1\":{\"110\":1}}],[\"随着训练的进行\",{\"1\":{\"110\":1}}],[\"随机裁剪输入图像\",{\"1\":{\"108\":1}}],[\"收敛\",{\"1\":{\"110\":1}}],[\"被添加到\",{\"1\":{\"110\":1}}],[\"维\",{\"1\":{\"109\":1}}],[\"维和第\",{\"1\":{\"109\":1}}],[\"维度变为\",{\"1\":{\"101\":1}}],[\"维度为\",{\"1\":{\"100\":2,\"101\":2,\"102\":9,\"107\":1}}],[\"维度\",{\"1\":{\"30\":1,\"37\":2}}],[\"注意\",{\"1\":{\"113\":1}}],[\"注意是通过一个线性层来同时计算qkv三个矩阵\",{\"1\":{\"113\":1}}],[\"注意力可视化\",{\"0\":{\"116\":1}}],[\"注意力矩阵的丢弃率\",{\"1\":{\"113\":1}}],[\"注意力头的数量\",{\"1\":{\"113\":1}}],[\"注意力机制的特性\",{\"1\":{\"110\":1}}],[\"注意力汇聚\",{\"1\":{\"110\":1}}],[\"注意下面的embed\",{\"1\":{\"109\":1}}],[\"注意事项\",{\"1\":{\"68\":1}}],[\"前向传播\",{\"1\":{\"110\":1}}],[\"前向传播函数\",{\"1\":{\"109\":1}}],[\"前馈层\",{\"1\":{\"79\":2,\"82\":2}}],[\"前馈神经网络\",{\"1\":{\"74\":1}}],[\"整体模型还是比较大的\",{\"1\":{\"118\":1}}],[\"整体结构图如下\",{\"1\":{\"118\":1}}],[\"整数\",{\"1\":{\"109\":2}}],[\"整个句子\",{\"1\":{\"73\":1}}],[\"整个函数\",{\"1\":{\"30\":1}}],[\"嵌入层维度\",{\"1\":{\"113\":1}}],[\"嵌入交互\",{\"1\":{\"110\":1}}],[\"嵌入的关系\",{\"1\":{\"110\":1}}],[\"嵌入\",{\"1\":{\"110\":1}}],[\"嵌入序列的开头\",{\"1\":{\"110\":1}}],[\"嵌入中\",{\"1\":{\"110\":1}}],[\"嵌入维度\",{\"1\":{\"109\":1,\"110\":1}}],[\"嵌入向量生成过程图\",{\"1\":{\"49\":1}}],[\"边长为该整数\",{\"1\":{\"109\":2}}],[\"步距为16\",{\"1\":{\"109\":1}}],[\"采用了一种更巧妙的解决思路\",{\"1\":{\"109\":1}}],[\"采样点数量\",{\"1\":{\"21\":1}}],[\"采样半径\",{\"1\":{\"21\":1}}],[\"采样得到的关键点坐标\",{\"1\":{\"21\":1}}],[\"采样的关键点数量\",{\"1\":{\"21\":2}}],[\"采样一些关键点\",{\"1\":{\"21\":1}}],[\"采样层\",{\"1\":{\"17\":1}}],[\"划分后可以得到共个patch\",{\"1\":{\"109\":1}}],[\"验证集上准确率达到了98\",{\"1\":{\"118\":1}}],[\"验证集不需要进行数据增强\",{\"1\":{\"108\":1}}],[\"验证集的预处理转换操作\",{\"1\":{\"108\":1}}],[\"长边按比例缩放\",{\"1\":{\"108\":1}}],[\"范围\",{\"1\":{\"108\":2}}],[\"范围缩放到\",{\"1\":{\"108\":2}}],[\"范数\",{\"1\":{\"33\":2}}],[\"增加了图像的多样性\",{\"1\":{\"108\":1}}],[\"增强稳定性\",{\"1\":{\"12\":1}}],[\"增强\",{\"1\":{\"12\":2}}],[\"增强空间特征表达\",{\"1\":{\"12\":1}}],[\"像素的图像块\",{\"1\":{\"118\":1}}],[\"像素\",{\"1\":{\"108\":2,\"118\":1}}],[\"像moco和simclr有所不同\",{\"1\":{\"89\":1}}],[\"静态方法是类中的一种特殊方法\",{\"1\":{\"107\":1}}],[\"官方实现的default\",{\"1\":{\"107\":1}}],[\"组合后的图像张量和标签张量\",{\"1\":{\"107\":1}}],[\"打开指定索引的图像文件\",{\"1\":{\"107\":1}}],[\"打破了这两个领域壁垒\",{\"1\":{\"105\":1}}],[\"绘制每种类别个数柱状图\",{\"1\":{\"107\":1}}],[\"记录该类别的样本数量\",{\"1\":{\"107\":1}}],[\"记录输入长度\",{\"1\":{\"46\":1}}],[\"约4000多个样本\",{\"1\":{\"107\":1}}],[\"共5个类别\",{\"1\":{\"107\":1}}],[\"快速找到合适的模型\",{\"1\":{\"105\":1}}],[\"归纳偏置能够帮助学习算法缩小搜索范围\",{\"1\":{\"105\":1}}],[\"归纳偏置\",{\"1\":{\"105\":1}}],[\"归一化层\",{\"1\":{\"109\":1}}],[\"归一化有助于模型更快地收敛\",{\"1\":{\"108\":1}}],[\"归一化\",{\"1\":{\"12\":1}}],[\"扩展分类标记以匹配输入批次大小\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"扩展可学习的query\",{\"1\":{\"104\":1}}],[\"扩展性\",{\"1\":{\"37\":1}}],[\"核采样参数\",{\"1\":{\"104\":1}}],[\"核采样时不扩展beam\",{\"1\":{\"104\":1}}],[\"核采样的概率阈值\",{\"1\":{\"104\":1}}],[\"核心要点\",{\"1\":{\"109\":1}}],[\"核心问题\",{\"1\":{\"37\":1}}],[\"核心\",{\"0\":{\"28\":1}}],[\"已经过预训练以提取语言信息视觉表示\",{\"1\":{\"104\":1}}],[\"充当了soft\",{\"1\":{\"104\":1}}],[\"送入llm时\",{\"1\":{\"104\":1}}],[\"线性投影到与\",{\"1\":{\"104\":1}}],[\"连接起来\",{\"1\":{\"104\":1}}],[\"错位对齐\",{\"1\":{\"103\":1}}],[\"复用缓存的视觉信息\",{\"1\":{\"103\":1}}],[\"指的是输入图像的尺寸为\",{\"1\":{\"118\":1}}],[\"指明哪些位置是有效的\",{\"1\":{\"103\":1}}],[\"指令理解特征\",{\"1\":{\"12\":1}}],[\"序列\",{\"1\":{\"103\":1}}],[\"标记\",{\"1\":{\"103\":1}}],[\"标准差\",{\"1\":{\"40\":1}}],[\"标准化的意义\",{\"1\":{\"32\":1}}],[\"标准化输入点云和特征空间\",{\"1\":{\"28\":1}}],[\"替换为\",{\"1\":{\"103\":2}}],[\"学习到输入数据的非线性特征\",{\"1\":{\"112\":1}}],[\"学习算法需要在所有可能的函数空间中搜索最优模型\",{\"1\":{\"105\":1}}],[\"学习目标\",{\"1\":{\"103\":1}}],[\"学到的是一个关键点集合\",{\"1\":{\"37\":1}}],[\"学到的是一个\",{\"1\":{\"30\":1}}],[\"叠加的运算流程\",{\"1\":{\"103\":1}}],[\"负责组织多个\",{\"1\":{\"103\":1}}],[\"负责组织自注意力和交叉注意力的运算流程\",{\"1\":{\"103\":1}}],[\"负样本batch2=0\",{\"1\":{\"102\":1}}],[\"负样本batch2\",{\"1\":{\"102\":3}}],[\"负样本batch1=0\",{\"1\":{\"102\":1}}],[\"负样本batch1\",{\"1\":{\"102\":3}}],[\"列表最后一个记录了缓存的key和value\",{\"1\":{\"103\":2}}],[\"传入图像\",{\"1\":{\"103\":1}}],[\"传统方法的缺陷\",{\"1\":{\"28\":1}}],[\"传统卷积神经网络难以直接处理\",{\"1\":{\"28\":1}}],[\"此时先用text\",{\"1\":{\"103\":1}}],[\"此外\",{\"1\":{\"91\":1,\"92\":1,\"96\":1}}],[\"判断是否为交叉注意力\",{\"1\":{\"103\":1}}],[\"判断预测是否正确\",{\"1\":{\"93\":1}}],[\"缓存key\",{\"1\":{\"103\":1}}],[\"缓存key和value\",{\"1\":{\"103\":1}}],[\"缓存的key和value\",{\"1\":{\"103\":1}}],[\"缓存和复用\",{\"1\":{\"103\":1}}],[\"缓存\",{\"1\":{\"103\":1}}],[\"吸收图像特征\",{\"1\":{\"103\":1}}],[\"及\",{\"1\":{\"103\":1}}],[\"若qk\",{\"1\":{\"113\":1}}],[\"若为none则使用默认值\",{\"1\":{\"113\":1}}],[\"若有query\",{\"1\":{\"102\":1}}],[\"若以文本单词数量来衡量\",{\"1\":{\"90\":1}}],[\"词嵌入与位置嵌入相加\",{\"1\":{\"102\":1}}],[\"词性标注\",{\"1\":{\"69\":1}}],[\"双向自注意力\",{\"1\":{\"102\":1}}],[\"单一的线性层只能进行线性变换\",{\"1\":{\"112\":1}}],[\"单独用来处理类别信息\",{\"1\":{\"110\":1}}],[\"单模态自注意力\",{\"1\":{\"101\":1}}],[\"单尺度分组\",{\"1\":{\"22\":1}}],[\"单尺度分组分类模型\",{\"0\":{\"22\":1}}],[\"冻结参数的image\",{\"1\":{\"100\":1}}],[\"框架按照\",{\"1\":{\"99\":1}}],[\"具有较强的泛化能力\",{\"1\":{\"118\":1}}],[\"具有很强的zero\",{\"1\":{\"98\":1}}],[\"具体为vit\",{\"1\":{\"118\":1}}],[\"具体解释如下\",{\"1\":{\"111\":1}}],[\"具体方式可以是直接缩放\",{\"1\":{\"108\":1}}],[\"具体代码实现如下\",{\"1\":{\"94\":1}}],[\"具体使用的是\",{\"1\":{\"93\":1}}],[\"具体的实验结果可以参考clip公开的notebook\",{\"1\":{\"92\":1}}],[\"具体位置在\",{\"1\":{\"45\":1}}],[\"具体而言\",{\"1\":{\"26\":1}}],[\"具体来说\",{\"1\":{\"24\":1,\"111\":1}}],[\"具体做法\",{\"1\":{\"19\":1}}],[\"具体选择多少个中心点以及邻域内的数量由超参数确定\",{\"1\":{\"18\":1}}],[\"均冻结\",{\"1\":{\"98\":1}}],[\"均匀性假设\",{\"1\":{\"19\":1}}],[\"高\",{\"1\":{\"98\":1}}],[\"好\",{\"1\":{\"98\":3}}],[\"性能\",{\"1\":{\"98\":1}}],[\"轻\",{\"1\":{\"98\":7}}],[\"轻便\",{\"1\":{\"39\":1}}],[\"视觉编码阶段\",{\"1\":{\"103\":1}}],[\"视觉分支\",{\"1\":{\"98\":1}}],[\"视觉语义特征\",{\"1\":{\"7\":1,\"12\":1}}],[\"理想情况\",{\"1\":{\"98\":1}}],[\"理论上的限制\",{\"1\":{\"37\":1}}],[\"理论上证明\",{\"1\":{\"30\":1}}],[\"理论分析保证模型鲁棒性\",{\"1\":{\"30\":1}}],[\"侧重模态融合\",{\"1\":{\"98\":1}}],[\"重\",{\"1\":{\"98\":8}}],[\"重点训练图像和文本特征提取\",{\"1\":{\"98\":1}}],[\"重复惩罚系数\",{\"1\":{\"104\":1}}],[\"重复\",{\"1\":{\"21\":1}}],[\"模式\",{\"1\":{\"107\":1}}],[\"模块可以看作是一个特征预处理模块\",{\"1\":{\"114\":1}}],[\"模块\",{\"1\":{\"98\":1,\"117\":1}}],[\"模态融合比较轻量\",{\"1\":{\"98\":1}}],[\"模态融合比较弱\",{\"1\":{\"98\":1}}],[\"模型能够学习到丰富的图像特征和模式\",{\"1\":{\"118\":1}}],[\"模型提取图像的特征图\",{\"1\":{\"117\":1}}],[\"模型自动学习到了如果注意画面中的分类主体\",{\"1\":{\"116\":1}}],[\"模型被强制学会将图像的有效信息汇聚到\",{\"1\":{\"110\":1}}],[\"模型更新参数\",{\"1\":{\"110\":1}}],[\"模型学会将图像中与分类任务相关的信息汇聚到\",{\"1\":{\"110\":1}}],[\"模型学会如何通过注意力机制将图像的有效信息汇聚到\",{\"1\":{\"110\":2}}],[\"模型计算每个\",{\"1\":{\"110\":1}}],[\"模型难以计算\",{\"1\":{\"109\":1}}],[\"模型的参数是根据这个特定尺寸的输入数据进行优化和学习的\",{\"1\":{\"108\":1}}],[\"模型名称\",{\"1\":{\"95\":1}}],[\"模型会对每个选项分别编码\",{\"1\":{\"70\":1}}],[\"模型需要从中选择最合适的答案\",{\"1\":{\"70\":1}}],[\"模型代表\",{\"1\":{\"68\":1}}],[\"模型代码解读与复现\",{\"0\":{\"4\":1}}],[\"模型那样逐词生成新内容\",{\"1\":{\"68\":1}}],[\"模型架构图\",{\"1\":{\"74\":1}}],[\"模型架构\",{\"0\":{\"47\":1,\"74\":1}}],[\"模型有点大\",{\"1\":{\"45\":1}}],[\"模型\",{\"1\":{\"39\":1,\"68\":1,\"90\":1}}],[\"模型为\",{\"1\":{\"39\":1}}],[\"模型表现良好\",{\"1\":{\"37\":1}}],[\"模型输出应不受刚性变换影响\",{\"1\":{\"29\":1}}],[\"模型必须对输入点的排列顺序不敏感\",{\"1\":{\"29\":1}}],[\"模型结构图\",{\"1\":{\"6\":1}}],[\"模型结构\",{\"0\":{\"6\":1,\"16\":1,\"99\":1}}],[\"早期的图文多模态\",{\"1\":{\"98\":1}}],[\"回顾下之前的多模态网络设计\",{\"1\":{\"98\":1}}],[\"回答\",{\"1\":{\"68\":1}}],[\"曾有一段时期一直在追求更大的网络架构\",{\"1\":{\"98\":1}}],[\"庖丁解牛vit\",{\"0\":{\"105\":1}}],[\"庖丁解牛blip2\",{\"0\":{\"97\":1},\"1\":{\"97\":1}}],[\"庖丁解牛clip\",{\"0\":{\"87\":1}}],[\"排序\",{\"1\":{\"107\":1}}],[\"排序为\",{\"1\":{\"96\":1}}],[\"排列不变性\",{\"1\":{\"37\":1}}],[\"存储每个类别的样本总数\",{\"1\":{\"107\":1}}],[\"存储每次选出的\",{\"1\":{\"21\":1}}],[\"存储验证集图片对应索引信息\",{\"1\":{\"107\":1}}],[\"存储验证集的所有图片路径\",{\"1\":{\"107\":1}}],[\"存储训练集图片对应索引信息\",{\"1\":{\"107\":1}}],[\"存储训练集的所有图片路径\",{\"1\":{\"107\":1}}],[\"存在一定的噪声\",{\"1\":{\"96\":1}}],[\"新的问题出现了\",{\"1\":{\"96\":1}}],[\"新的文本\",{\"1\":{\"68\":1}}],[\"谷歌利用强大的计算能力进行了预训练\",{\"1\":{\"96\":1}}],[\"谷歌采用的弱监督方法与以往方法的一个主要区别在于规模\",{\"1\":{\"96\":1}}],[\"尽管谷歌基于jft\",{\"1\":{\"96\":1}}],[\"尽管clip是一个多模态模型\",{\"1\":{\"90\":1}}],[\"总体来看\",{\"1\":{\"96\":1}}],[\"总结表格\",{\"1\":{\"37\":1}}],[\"总结\",{\"0\":{\"119\":1},\"1\":{\"19\":1,\"30\":1,\"68\":1}}],[\"出现这种差异的原因不难理解\",{\"1\":{\"96\":1}}],[\"出现了一些基于自监督的方法\",{\"1\":{\"96\":1}}],[\"出来\",{\"1\":{\"68\":1}}],[\"近年来\",{\"1\":{\"96\":1}}],[\"搜索出来的图片\",{\"1\":{\"94\":1}}],[\"运行上述代码\",{\"1\":{\"94\":1}}],[\"遍历数据加载器中的每个批次数据\",{\"1\":{\"114\":1}}],[\"遍历获取supported支持的所有文件路径\",{\"1\":{\"107\":1}}],[\"遍历每个文件夹下的文件\",{\"1\":{\"107\":1}}],[\"遍历文件夹\",{\"1\":{\"107\":1}}],[\"遍历\",{\"1\":{\"94\":1,\"95\":1}}],[\"遍历data目录\",{\"1\":{\"94\":1}}],[\"拿到所有图片路径\",{\"1\":{\"94\":1}}],[\"下表中对比了vit\",{\"1\":{\"117\":1}}],[\"下面我们将进入训练流程\",{\"1\":{\"118\":1}}],[\"下面我们将用于图片变换的transforms流水线和上面自定义的mydataset类都封装到dataloader去\",{\"1\":{\"108\":1}}],[\"下面的3是因为我们用一次矩阵运算得到了拼接在一起的q\",{\"1\":{\"113\":1}}],[\"下面所给出的代码实现\",{\"1\":{\"113\":1}}],[\"下面来实际展示一下效果\",{\"1\":{\"94\":1}}],[\"下图展示了\",{\"1\":{\"102\":1}}],[\"下一个句子预测损失\",{\"1\":{\"64\":1}}],[\"找到与文本最匹配的图片\",{\"1\":{\"94\":1,\"95\":1}}],[\"找出最可能是\",{\"1\":{\"66\":1}}],[\"找出该尺度下每个质心点周围的邻近点\",{\"1\":{\"25\":1}}],[\"找出每个点的局部邻近点\",{\"1\":{\"21\":1}}],[\"找出它周围距离小于\",{\"1\":{\"21\":1}}],[\"针对每张图像\",{\"1\":{\"93\":1}}],[\"针对每个图像嵌入向量获取和其相似度最高的分类文本嵌入向量索引下标\",{\"1\":{\"93\":1}}],[\"针对每个图像嵌入向量取出和其相似度最高的那个文本嵌入向量的索引\",{\"1\":{\"93\":1}}],[\"子目录名的格式\",{\"1\":{\"93\":1}}],[\"子序列\",{\"1\":{\"68\":1}}],[\"递归遍历目录获取所有图片路径\",{\"1\":{\"93\":1,\"95\":1}}],[\"读取图片并将其转换为合适的格式后\",{\"1\":{\"93\":1}}],[\"版本\",{\"1\":{\"93\":1,\"108\":1}}],[\"版本代码\",{\"1\":{\"14\":2,\"27\":2}}],[\"检查图像是否为\",{\"1\":{\"107\":1}}],[\"检查当前目录是否有预训练权重文件\",{\"1\":{\"93\":1,\"95\":1}}],[\"检索等任务\",{\"1\":{\"37\":1}}],[\"本次训练是基于预训练好的vit\",{\"1\":{\"118\":1}}],[\"本节我们将基于clip预训练模型实现zero\",{\"1\":{\"93\":1}}],[\"本文将通过一个花卉分类的实战案例结合vit原论文\",{\"1\":{\"106\":1}}],[\"本文的transformer使用了self\",{\"1\":{\"73\":1}}],[\"本文基于\",{\"1\":{\"72\":1}}],[\"本文使用的是谷歌的中文预训练模型\",{\"1\":{\"45\":1}}],[\"花卉图片分类\",{\"0\":{\"93\":1}}],[\"紧密相关\",{\"1\":{\"92\":1}}],[\"显然\",{\"1\":{\"91\":1}}],[\"显著提升了分割性能\",{\"1\":{\"30\":1}}],[\"接下来\",{\"1\":{\"91\":1}}],[\"接近\",{\"1\":{\"37\":1}}],[\"推理时\",{\"1\":{\"100\":1}}],[\"推理\",{\"0\":{\"91\":1}}],[\"推理或训练分支\",{\"1\":{\"7\":1}}],[\"然而\",{\"1\":{\"90\":1,\"96\":3}}],[\"然后再经过一层\",{\"1\":{\"117\":1}}],[\"然后再拿到其它数据集上做迁移学习\",{\"1\":{\"115\":1}}],[\"然后输入到mlp\",{\"1\":{\"114\":1}}],[\"然后调整形状并重新排列维度\",{\"1\":{\"113\":1}}],[\"然后在训练过程中\",{\"1\":{\"111\":1}}],[\"然后在具体的下游任务上进行微调\",{\"1\":{\"96\":1}}],[\"然后进行水平翻转\",{\"1\":{\"108\":1}}],[\"然后图像的表征和\",{\"1\":{\"104\":1}}],[\"然后计算出和当前文本描述相似度最高的那副图片\",{\"1\":{\"94\":1}}],[\"然后计算每个分类文本对应的文本嵌入向量\",{\"1\":{\"93\":1}}],[\"然后利用模型获取文本特征\",{\"1\":{\"93\":1}}],[\"然后我们读取要预测的图像\",{\"1\":{\"91\":1}}],[\"然后提取了相应的文本特征\",{\"1\":{\"91\":1}}],[\"然后\",{\"1\":{\"91\":1}}],[\"然后从中选出最合适的那个\",{\"1\":{\"70\":1}}],[\"然后启动\",{\"1\":{\"45\":1}}],[\"然后将\",{\"1\":{\"117\":1}}],[\"然后将特征图的最后两维展平为一维\",{\"1\":{\"109\":1}}],[\"然后将tf模型转为对应的pytorch版本即可\",{\"1\":{\"45\":1}}],[\"然后将这些不同尺度的特征拼接在一起\",{\"1\":{\"25\":1}}],[\"然后使用第一个cls\",{\"1\":{\"100\":1}}],[\"然后使用\",{\"1\":{\"37\":1}}],[\"然后使用全局最大池化\",{\"1\":{\"37\":1}}],[\"然后通过全连接层\",{\"1\":{\"35\":2}}],[\"然后通过对应的pointnets提取每个尺度上的特征来捕获多尺度模式\",{\"1\":{\"24\":1}}],[\"然后取\",{\"1\":{\"33\":1}}],[\"然后只保留前\",{\"1\":{\"21\":1}}],[\"然后复制这个索引数组到每个\",{\"1\":{\"21\":1}}],[\"然后把所有点映射到高维的特征通过最大池化最终表示全局特征\",{\"1\":{\"15\":1}}],[\"训练与评估流程的代码为模版代码\",{\"1\":{\"118\":1}}],[\"训练了10个epoch\",{\"1\":{\"118\":1}}],[\"训练过程中\",{\"1\":{\"110\":1}}],[\"训练集的预处理转换操作\",{\"1\":{\"108\":1}}],[\"训练目标的引导\",{\"1\":{\"110\":1}}],[\"训练目标\",{\"1\":{\"103\":1}}],[\"训练代价\",{\"1\":{\"98\":1}}],[\"训练效率成为一个至关重要的因素\",{\"1\":{\"96\":1}}],[\"训练效率可以提高4倍\",{\"1\":{\"96\":1}}],[\"训练使用到的数据集和alexnet保持一致\",{\"1\":{\"93\":1}}],[\"训练\",{\"0\":{\"90\":1}}],[\"训练出具有可迁移能力的视觉模型\",{\"1\":{\"88\":1}}],[\"期望模型能够学习到文本和图像之间的匹配关系\",{\"1\":{\"89\":1}}],[\"借助对比学习机制\",{\"1\":{\"89\":1}}],[\"借助字典映射为word\",{\"1\":{\"46\":1}}],[\"介绍\",{\"0\":{\"89\":1}}],[\"同样可以比较好地记录二维信息\",{\"1\":{\"111\":1}}],[\"同样需要位置编码来记录各图像块之间的位置信息\",{\"1\":{\"111\":1}}],[\"同样\",{\"1\":{\"109\":1}}],[\"同样会将像素值从\",{\"1\":{\"108\":1}}],[\"同样是数据增强的手段\",{\"1\":{\"108\":1}}],[\"同样给计算机视觉领域带来了巨大影响\",{\"1\":{\"88\":1}}],[\"同时保留其捕捉长距离依赖的优势\",{\"1\":{\"117\":1}}],[\"同时作为特征数量\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"同时会将像素值从\",{\"1\":{\"108\":1}}],[\"同时删除不相关的视觉信息\",{\"1\":{\"104\":1}}],[\"同时因llm而具有了视觉推理能力\",{\"1\":{\"98\":1}}],[\"同时具有很好的性能\",{\"1\":{\"98\":1}}],[\"同时将flower\",{\"1\":{\"93\":1}}],[\"同时最小化个负样本的相似度\",{\"1\":{\"90\":1}}],[\"同时也指运用该方法构建的模型\",{\"1\":{\"89\":1}}],[\"同时计算self\",{\"1\":{\"73\":1}}],[\"同时确保这些分区的处理方式允许在它们之间共享模型权重\",{\"1\":{\"15\":1}}],[\"同时缺少\",{\"1\":{\"15\":1}}],[\"月发布的\",{\"1\":{\"88\":1}}],[\"年\",{\"1\":{\"88\":1}}],[\"年可谓是视觉\",{\"1\":{\"88\":1}}],[\"广泛应用于各类计算机视觉任务\",{\"1\":{\"88\":1}}],[\"广播\",{\"1\":{\"37\":1,\"84\":1}}],[\"众多基于视觉\",{\"1\":{\"88\":1}}],[\"目的\",{\"1\":{\"101\":1,\"102\":1,\"103\":1}}],[\"目的是让点云\",{\"1\":{\"32\":1}}],[\"目的是从一个大的数据集中选出一组代表性强的点\",{\"1\":{\"18\":1}}],[\"目标是训练好\",{\"1\":{\"100\":1}}],[\"目标掩码\",{\"1\":{\"83\":1}}],[\"目录获取所有图片路径\",{\"1\":{\"94\":1,\"95\":1}}],[\"源掩码\",{\"1\":{\"83\":1}}],[\"源注意力子层\",{\"1\":{\"82\":2}}],[\"各需要一个\",{\"1\":{\"79\":1,\"82\":1}}],[\"参考decoderlayer\",{\"1\":{\"78\":1}}],[\"参数与训练集的归一化参数相同\",{\"1\":{\"108\":1}}],[\"参数共享\",{\"1\":{\"37\":1}}],[\"参数\",{\"1\":{\"21\":1}}],[\"参数依赖\",{\"1\":{\"18\":1}}],[\"屏蔽填充部分的信息\",{\"1\":{\"74\":1}}],[\"掩码机制\",{\"1\":{\"74\":1}}],[\"残差连接\",{\"1\":{\"78\":1,\"112\":2}}],[\"残差连接与层归一化\",{\"1\":{\"74\":1}}],[\"残差链接\",{\"1\":{\"58\":1}}],[\"捕捉全局信息\",{\"1\":{\"117\":1}}],[\"捕捉不同子空间的信息\",{\"1\":{\"74\":1}}],[\"捕获密集到稀疏采样区域内的多尺度信息\",{\"1\":{\"25\":1}}],[\"机器翻译实战\",{\"1\":{\"71\":1}}],[\"托尔斯泰\",{\"1\":{\"70\":1}}],[\"莎士比亚\",{\"1\":{\"70\":1}}],[\"莎士比亚是英国文学史上最伟大的作家之一\",{\"1\":{\"66\":1}}],[\"歌德\",{\"1\":{\"70\":1}}],[\"雨果\",{\"1\":{\"70\":1}}],[\"任务中\",{\"1\":{\"70\":1}}],[\"任务的本质\",{\"1\":{\"68\":1}}],[\"阅读理解任务\",{\"1\":{\"70\":1}}],[\"语义角色标注\",{\"1\":{\"69\":1}}],[\"语言后半段\",{\"1\":{\"10\":1}}],[\"语言前半段\",{\"1\":{\"10\":1}}],[\"语言部分的\",{\"1\":{\"10\":1}}],[\"语言\",{\"1\":{\"10\":1}}],[\"语言模型\",{\"1\":{\"9\":1}}],[\"语言指令理解特征\",{\"1\":{\"7\":1}}],[\"命名实体识别\",{\"1\":{\"69\":1}}],[\"常见的迁移学习方法是首先在大规模数据集\",{\"1\":{\"96\":1}}],[\"常见的应用场景包括\",{\"1\":{\"69\":1,\"70\":1}}],[\"常见的对称函数\",{\"1\":{\"40\":1}}],[\"做attention\",{\"1\":{\"102\":1}}],[\"做\",{\"1\":{\"101\":1}}],[\"做分类\",{\"1\":{\"68\":1}}],[\"做的就是这个定位任务\",{\"1\":{\"68\":1}}],[\"能起作用的原因在于\",{\"1\":{\"110\":1}}],[\"能够通过多层\",{\"1\":{\"110\":1}}],[\"能够动态地聚合图像信息\",{\"1\":{\"110\":1}}],[\"能够更好地聚合图像信息\",{\"1\":{\"110\":1}}],[\"能够自适应地选择最适合的特征尺度进行组合\",{\"1\":{\"25\":1}}],[\"能否基于互联网上的大量文本来预训练视觉模型\",{\"1\":{\"96\":1}}],[\"能不能自己生成答案\",{\"1\":{\"68\":1}}],[\"示例\",{\"1\":{\"68\":1}}],[\"抽取式\",{\"1\":{\"68\":1}}],[\"抽取式问答\",{\"1\":{\"68\":1}}],[\"抽象\",{\"1\":{\"32\":1,\"34\":1}}],[\"抽象点集或局部特征\",{\"1\":{\"15\":2}}],[\"编码为图像特征\",{\"1\":{\"103\":1}}],[\"编码器输出的\",{\"1\":{\"103\":1}}],[\"编码器隐藏层输出\",{\"1\":{\"83\":1}}],[\"编码器层\",{\"1\":{\"79\":1}}],[\"编码器\",{\"1\":{\"74\":1}}],[\"编码后得到输出的结果\",{\"1\":{\"102\":1}}],[\"编码后可能是\",{\"1\":{\"68\":1}}],[\"编码后\",{\"1\":{\"66\":1}}],[\"编造\",{\"1\":{\"68\":1}}],[\"自定义的批量数据处理函数\",{\"1\":{\"107\":1}}],[\"自定义数据集\",{\"1\":{\"107\":1}}],[\"自定义一个mydataset类来封装我们加载得到的数据集\",{\"1\":{\"107\":1}}],[\"自回归语言建模任务\",{\"1\":{\"103\":1}}],[\"自监督方法的优势在于不再需要标注数据\",{\"1\":{\"96\":1}}],[\"自谷歌提出\",{\"1\":{\"88\":1}}],[\"自注意力机制能够捕捉图像中任意两个\",{\"1\":{\"110\":1}}],[\"自注意力机制是\",{\"1\":{\"74\":1}}],[\"自注意力运算\",{\"1\":{\"103\":1}}],[\"自注意力\",{\"1\":{\"103\":2}}],[\"自注意力和交叉注意力流程统一化\",{\"1\":{\"103\":1}}],[\"自注意力掩码策略\",{\"1\":{\"101\":1,\"102\":1,\"103\":1}}],[\"自注意力子层\",{\"1\":{\"79\":2,\"82\":2}}],[\"自己写答案\",{\"1\":{\"68\":1}}],[\"自动跳过\",{\"1\":{\"68\":1}}],[\"自然语言指令\",{\"1\":{\"7\":1}}],[\"必须落在上下文部分\",{\"1\":{\"68\":1}}],[\"必须自己学会对各种姿态都识别准确\",{\"1\":{\"32\":1}}],[\"假设你有一个问题\",{\"1\":{\"70\":1}}],[\"假设原始上下文是\",{\"1\":{\"68\":1}}],[\"假设空间中所有区域的尺度或特征分布具有一定的一致性\",{\"1\":{\"19\":1}}],[\"举个例子\",{\"1\":{\"68\":1}}],[\"举例说明\",{\"1\":{\"37\":1}}],[\"请把这部分原文告诉我\",{\"1\":{\"68\":1}}],[\"答案必须是原文中的一段文本\",{\"1\":{\"68\":1}}],[\"答案必须是原文中的连续片段\",{\"1\":{\"68\":1}}],[\"答案可以是任意文本\",{\"1\":{\"68\":1}}],[\"答案是原文中的一段\",{\"1\":{\"68\":1}}],[\"答案是否必须在原文中\",{\"1\":{\"68\":1}}],[\"答案应该在这段文字中的第\",{\"1\":{\"68\":1}}],[\"根据索引获取数据集中的图像和对应的标签\",{\"1\":{\"107\":1}}],[\"根据图像特征\",{\"1\":{\"103\":1}}],[\"根据imagenet数据集上的zero\",{\"1\":{\"96\":1}}],[\"根据文字搜索图片\",{\"1\":{\"94\":1,\"95\":1}}],[\"根据上述计算得到的和其相似度最高的分类文本索引\",{\"1\":{\"93\":1}}],[\"根据任务的分类需求\",{\"1\":{\"91\":1}}],[\"根据decoder的隐状态输出一个词\",{\"1\":{\"76\":1}}],[\"根据编码器的输出生成目标序列\",{\"1\":{\"74\":1}}],[\"根据预测的\",{\"1\":{\"68\":1}}],[\"根据你的理解\",{\"1\":{\"68\":1}}],[\"根据注意力机制加权求和\",{\"1\":{\"40\":1}}],[\"详细解释\",{\"1\":{\"68\":1}}],[\"易混淆\",{\"0\":{\"68\":1}}],[\"易于扩展为检测\",{\"1\":{\"37\":1}}],[\"未归一化\",{\"1\":{\"66\":1}}],[\"未经扰动训练时\",{\"1\":{\"37\":1}}],[\"另一种思路是在转换后\",{\"1\":{\"111\":1}}],[\"另一种是基于\",{\"1\":{\"90\":1}}],[\"另一个原因是nlp模型可以利用从互联网上收集的大量文本\",{\"1\":{\"96\":1}}],[\"另一个是作为答案结束的可能性\",{\"1\":{\"66\":1}}],[\"另一部分特征是通过在当前分辨率直接对所有原始点应用单个pointnet得到的\",{\"1\":{\"26\":1}}],[\"句子级别表示\",{\"1\":{\"66\":1}}],[\"包括\",{\"1\":{\"110\":1}}],[\"包括基于对比学习的方法\",{\"1\":{\"96\":1}}],[\"包括问题和上下文\",{\"1\":{\"66\":1}}],[\"包含大约\",{\"1\":{\"118\":1}}],[\"包含一个线性层和一个\",{\"1\":{\"114\":1}}],[\"包含分类token\",{\"1\":{\"113\":1}}],[\"包含所有图像对应类别的列表\",{\"1\":{\"107\":1}}],[\"包含所有图像文件路径的列表\",{\"1\":{\"107\":1}}],[\"包含图像和对应的标签\",{\"1\":{\"107\":1}}],[\"包含图像和可选文本\",{\"1\":{\"104\":1}}],[\"包含图像信息\",{\"1\":{\"103\":1}}],[\"包含\",{\"1\":{\"103\":1}}],[\"包含表面细节\",{\"1\":{\"39\":1}}],[\"包含两种适应性特征学习层\",{\"1\":{\"23\":1}}],[\"主干部分全部冻结\",{\"1\":{\"118\":1}}],[\"主要的区别就是去掉了paddding\",{\"1\":{\"113\":1}}],[\"主要区别在于norm层的顺序\",{\"1\":{\"112\":1}}],[\"主要包含encoder和decoder结构\",{\"1\":{\"105\":1}}],[\"主要是因为这些方法难以实现较高的性能\",{\"1\":{\"96\":1}}],[\"主要输出项解释\",{\"1\":{\"66\":1}}],[\"主页\",{\"0\":{\"0\":1}}],[\"你需要为每个选项分别构造一个完整的\",{\"1\":{\"70\":1}}],[\"你将得到一个包含多个元素的\",{\"1\":{\"66\":1}}],[\"你知道吗\",{\"1\":{\"36\":1}}],[\"等\",{\"1\":{\"117\":1}}],[\"等价于n个类别的cross\",{\"1\":{\"90\":1}}],[\"等会被\",{\"1\":{\"68\":1}}],[\"等著名悲剧\",{\"1\":{\"66\":1}}],[\"等基于图像的方法\",{\"1\":{\"37\":1}}],[\"麦克白\",{\"1\":{\"66\":1}}],[\"他写了包括\",{\"1\":{\"66\":1}}],[\"哈姆雷特\",{\"1\":{\"66\":2,\"70\":1}}],[\"谁写了\",{\"1\":{\"66\":1,\"70\":1}}],[\"典型的输入是一个包含\",{\"1\":{\"66\":1}}],[\"问答系统中的候选答案选择\",{\"1\":{\"70\":1}}],[\"问答任务\",{\"0\":{\"66\":1}}],[\"问题来了\",{\"1\":{\"96\":1}}],[\"问题\",{\"1\":{\"66\":3,\"68\":1,\"70\":2}}],[\"问题所在\",{\"1\":{\"37\":1}}],[\"问题背景\",{\"1\":{\"28\":1}}],[\"隐藏层输出\",{\"1\":{\"64\":1}}],[\"合并头结果\",{\"1\":{\"57\":1}}],[\"激活函数层\",{\"1\":{\"112\":1}}],[\"激活函数\",{\"1\":{\"51\":2,\"112\":1,\"114\":2}}],[\"含special\",{\"1\":{\"46\":1}}],[\"填充符\",{\"1\":{\"104\":1}}],[\"填充过程图\",{\"1\":{\"46\":1}}],[\"填充token对应0\",{\"1\":{\"46\":1}}],[\"填充至最长序列长度\",{\"1\":{\"7\":1}}],[\"真实token对应1\",{\"1\":{\"46\":1}}],[\"真实标签\",{\"1\":{\"7\":1}}],[\"超长截断\",{\"1\":{\"46\":1}}],[\"|\",{\"1\":{\"46\":2}}],[\"||i\",{\"1\":{\"30\":1}}],[\"创建预输出层\",{\"1\":{\"114\":1}}],[\"创建归一化层\",{\"1\":{\"114\":1}}],[\"创建encoder\",{\"1\":{\"114\":1}}],[\"创建\",{\"1\":{\"112\":1}}],[\"创建丢弃层\",{\"1\":{\"111\":1,\"114\":1}}],[\"创建可学习的位置嵌入\",{\"1\":{\"111\":1,\"114\":1}}],[\"创建可学习的分类标记\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"创建图像块嵌入层\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"创建图像注意力掩码\",{\"1\":{\"104\":1}}],[\"创建用以区分special\",{\"1\":{\"46\":1}}],[\"创建句子辨识列表\",{\"1\":{\"46\":1}}],[\"创建一个全零点作为\",{\"1\":{\"21\":1}}],[\"过长截断策略\",{\"1\":{\"46\":1}}],[\"态\",{\"1\":{\"46\":1}}],[\"破\",{\"1\":{\"46\":1}}],[\"突破transformer缺少归纳偏置的限制\",{\"1\":{\"105\":1}}],[\"突\",{\"1\":{\"46\":1}}],[\"票\",{\"1\":{\"46\":1}}],[\"股\",{\"1\":{\"46\":1}}],[\"股票中的突破形态\",{\"1\":{\"46\":2}}],[\"数量\",{\"1\":{\"109\":1}}],[\"数组转换为\",{\"1\":{\"108\":2}}],[\"数组后\",{\"1\":{\"93\":1}}],[\"数组格式返回\",{\"1\":{\"93\":1}}],[\"数据的分布更加稀疏\",{\"1\":{\"112\":1}}],[\"数据下载\",{\"0\":{\"107\":1}}],[\"数据集上进行预训练的\",{\"1\":{\"118\":1}}],[\"数据集中图像的数量\",{\"1\":{\"107\":1}}],[\"数据集中每一个样本最终都会解析得到一个inputfeatures\",{\"1\":{\"46\":1}}],[\"数据集加载代码\",{\"1\":{\"107\":1}}],[\"数据集下载\",{\"1\":{\"107\":1}}],[\"数据集\",{\"1\":{\"98\":1}}],[\"数据集预处理完后\",{\"1\":{\"46\":1}}],[\"数据预处理\",{\"0\":{\"46\":1}}],[\"数学定义\",{\"1\":{\"40\":1}}],[\"文字搜索图像\",{\"0\":{\"94\":1}}],[\"文字搜索图像实战演练\",{\"1\":{\"87\":1}}],[\"文件中的\",{\"1\":{\"72\":1}}],[\"文件进行调试即可\",{\"1\":{\"45\":1}}],[\"文本生成阶段\",{\"1\":{\"103\":1}}],[\"文本\",{\"1\":{\"103\":2}}],[\"文本对\",{\"1\":{\"100\":1}}],[\"文本对应的标签\",{\"1\":{\"46\":1}}],[\"文本描述的生成也是一个关键环节\",{\"1\":{\"92\":1}}],[\"文本描述生成\",{\"0\":{\"92\":1}}],[\"文本编码器使用的是基于\",{\"1\":{\"93\":1}}],[\"文本编码器\",{\"1\":{\"90\":1,\"91\":1}}],[\"文本编码器的作用是提取文本的特征\",{\"1\":{\"90\":1}}],[\"文本分支\",{\"1\":{\"98\":2}}],[\"文本分词\",{\"1\":{\"46\":1}}],[\"文本分类任务\",{\"1\":{\"46\":1}}],[\"文中的𝐶所表示的其他信息\",{\"1\":{\"20\":1}}],[\"文中作者通过ball\",{\"1\":{\"19\":1}}],[\"准备生成参数\",{\"1\":{\"104\":1}}],[\"准备\",{\"1\":{\"103\":1}}],[\"准备调试\",{\"1\":{\"45\":1}}],[\"准备就绪\",{\"1\":{\"45\":1}}],[\"开始生成句子\",{\"1\":{\"103\":1}}],[\"开发数据1k\",{\"1\":{\"45\":1}}],[\"开源项目\",{\"0\":{\"120\":1}}],[\"开源网络库\",{\"1\":{\"2\":1}}],[\"开源框架\",{\"1\":{\"2\":1}}],[\"测试图片分类正确率\",{\"1\":{\"93\":1,\"95\":1}}],[\"测试数据使用1k\",{\"1\":{\"45\":1}}],[\"测试和开发集\",{\"1\":{\"45\":1}}],[\"就表示模型大约有\",{\"1\":{\"115\":1}}],[\"就意味着模型大约有\",{\"1\":{\"115\":1}}],[\"就是将传统cnn和transformer进行结合\",{\"1\":{\"117\":1}}],[\"就是两个线性层+gelu激活函数+dropout的结构\",{\"1\":{\"112\":1}}],[\"就是利用一个卷积核大小为16x16\",{\"1\":{\"109\":1}}],[\"就完成了从图片到token之间的转换\",{\"1\":{\"109\":1}}],[\"就有了很多先验信息\",{\"1\":{\"105\":1}}],[\"就点击这里直接下载\",{\"1\":{\"45\":1}}],[\"就更新它\",{\"1\":{\"21\":1}}],[\"按比例随机采样验证样本\",{\"1\":{\"107\":1}}],[\"按序执行以下命令完成环境搭建\",{\"1\":{\"45\":1}}],[\"按照16x16大小的patch进行划分\",{\"1\":{\"109\":1}}],[\"按照余弦相似度的数学公式来计算两者的相似度数值\",{\"1\":{\"93\":1}}],[\"按照最大相似度\",{\"1\":{\"91\":1}}],[\"按照本批次序列中最大长度进行截断\",{\"1\":{\"48\":1}}],[\"按照几个预定义的半径值来搜索周围的邻近点\",{\"1\":{\"24\":1}}],[\"按照不同的搜索半径或领域大小对点集进行分组\",{\"1\":{\"24\":1}}],[\"环境\",{\"0\":{\"72\":1}}],[\"环境搭建遵从如下步骤即可\",{\"1\":{\"72\":1}}],[\"环境搭建\",{\"0\":{\"45\":1}}],[\"环境配置\",{\"0\":{\"5\":1}}],[\"剪切\",{\"1\":{\"42\":1}}],[\"缩放\",{\"1\":{\"41\":1,\"42\":1}}],[\"缩放因子\",{\"1\":{\"12\":1,\"113\":1}}],[\"弯曲\",{\"1\":{\"41\":1}}],[\"改为\",{\"1\":{\"72\":1}}],[\"改变\",{\"1\":{\"41\":1}}],[\"改进\",{\"1\":{\"37\":6}}],[\"移动\",{\"1\":{\"41\":1}}],[\"其他权重全部冻结\",{\"1\":{\"118\":1}}],[\"其他下游任务\",{\"0\":{\"65\":1}}],[\"其表达能力是有限的\",{\"1\":{\"112\":1}}],[\"其次\",{\"1\":{\"93\":1}}],[\"其次是准备训练数据\",{\"1\":{\"45\":1}}],[\"其均能匹配到正确的文本标签\",{\"1\":{\"91\":1}}],[\"其规模与gpt\",{\"1\":{\"90\":1}}],[\"其由七大主要部分构成\",{\"1\":{\"74\":1}}],[\"其形状和大小保持不变的运动方式\",{\"1\":{\"41\":1}}],[\"其中包括注意力可视化\",{\"1\":{\"116\":1}}],[\"其中的layers就是transformer\",{\"1\":{\"115\":1}}],[\"其中训练数据使用1k\",{\"1\":{\"45\":1}}],[\"其中\",{\"1\":{\"30\":1,\"88\":1,\"90\":1,\"109\":1}}],[\"物体在空间中移动时\",{\"1\":{\"41\":1}}],[\"物体分割和场景语义解析\",{\"1\":{\"28\":1}}],[\"刚性运动\",{\"0\":{\"41\":1},\"1\":{\"41\":3}}],[\"集合函数近似器\",{\"1\":{\"40\":1}}],[\"得出\",{\"1\":{\"40\":1}}],[\"得到结果如下图所示\",{\"1\":{\"116\":1}}],[\"得到预测结果\",{\"1\":{\"114\":1}}],[\"得到图像和对应的标签\",{\"1\":{\"114\":1}}],[\"得到图像的表征\",{\"1\":{\"104\":1}}],[\"得到每个注意力头的输出\",{\"1\":{\"113\":1}}],[\"得到每个预测类别的概率值\",{\"1\":{\"91\":1}}],[\"得到注意力权重矩阵\",{\"1\":{\"113\":1}}],[\"得到注意力分数矩阵\",{\"1\":{\"113\":1}}],[\"得到形状为\",{\"1\":{\"109\":3}}],[\"得到一个logit\",{\"1\":{\"102\":1}}],[\"得到相似度矩阵\",{\"1\":{\"101\":2}}],[\"得到相同维度的特征\",{\"1\":{\"90\":1}}],[\"得到概率\",{\"1\":{\"66\":1}}],[\"得到的预测概率如下所示\",{\"1\":{\"91\":1}}],[\"得到的就是答案\",{\"1\":{\"68\":1}}],[\"得到的特征可能无法覆盖整个物体\",{\"1\":{\"37\":1}}],[\"得到的质心\",{\"1\":{\"21\":1}}],[\"得到\",{\"1\":{\"22\":3,\"36\":1,\"104\":1}}],[\"得到该区域的固定长度特征表示\",{\"1\":{\"21\":1}}],[\"乘积\",{\"1\":{\"40\":1}}],[\"求和\",{\"1\":{\"40\":1}}],[\"取每个位置的平均作为最终的匹配得分\",{\"1\":{\"102\":1}}],[\"取\",{\"1\":{\"102\":1}}],[\"取第一个cls\",{\"1\":{\"100\":1}}],[\"取出该行中得分最大的那一列\",{\"1\":{\"94\":1}}],[\"取出当前批次的图像列表\",{\"1\":{\"93\":1}}],[\"取出当前最远点的坐标\",{\"1\":{\"21\":1}}],[\"取最小值\",{\"1\":{\"40\":1}}],[\"取所有点的平均值\",{\"1\":{\"40\":1}}],[\"取所有点的最大值\",{\"1\":{\"40\":1}}],[\"那样我们总共有196个向量\",{\"1\":{\"109\":1}}],[\"那样逐层提取多层次的抽象特征\",{\"1\":{\"37\":1}}],[\"那就需要使用生成式模型\",{\"1\":{\"68\":1}}],[\"那么作者就想把注意力得到的结果\",{\"1\":{\"116\":1}}],[\"那么就会得到个文本特征\",{\"1\":{\"91\":1}}],[\"那么就是一个\",{\"1\":{\"40\":1}}],[\"那么\",{\"1\":{\"91\":1,\"109\":1}}],[\"那么clip的训练目标就是最大个正样本的相似度\",{\"1\":{\"90\":1}}],[\"那么对应的就是单词\",{\"1\":{\"68\":1}}],[\"那么可以组合这两个索引得到答案\",{\"1\":{\"66\":1}}],[\"拓扑复杂\",{\"1\":{\"39\":1}}],[\"网格\",{\"1\":{\"39\":1}}],[\"网络输出由一个有限子集\",{\"1\":{\"30\":1}}],[\"网络不是只捕获一个尺度上的局部特征\",{\"1\":{\"24\":1}}],[\"网络对于每个选定的形心点\",{\"1\":{\"24\":1}}],[\"网络对每一个点做低维到高维的映射\",{\"1\":{\"15\":1}}],[\"网络在训练时被呈现了不同稀疏度的点集\",{\"1\":{\"24\":1}}],[\"网络的每一组set\",{\"1\":{\"16\":1}}],[\"网络的分割和分类模型\",{\"1\":{\"16\":1}}],[\"依赖视角选择\",{\"1\":{\"39\":1}}],[\"依赖初始点和距离度量方式的选择\",{\"1\":{\"18\":1}}],[\"丢失部分几何信息\",{\"1\":{\"39\":1}}],[\"精度受限\",{\"1\":{\"39\":1}}],[\"适当降低训练目标反而可能取得更好的效果\",{\"1\":{\"96\":1}}],[\"适合渲染\",{\"1\":{\"39\":1}}],[\"适合\",{\"1\":{\"39\":1}}],[\"适用于分类\",{\"1\":{\"28\":1}}],[\"处理beam\",{\"1\":{\"104\":1}}],[\"处理的是点云数据\",{\"1\":{\"40\":1}}],[\"处理\",{\"1\":{\"39\":1}}],[\"处理点云数据\",{\"1\":{\"32\":1}}],[\"保存注意力头的数量\",{\"1\":{\"113\":1}}],[\"保存嵌入维度\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"保存分类任务的类别数\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"保证训练集和验证集的数据处理方式一致\",{\"1\":{\"108\":1}}],[\"保证顺序一致\",{\"1\":{\"107\":1}}],[\"保证随机结果可复现\",{\"1\":{\"107\":1}}],[\"保证变换是刚性的\",{\"1\":{\"32\":1}}],[\"保留和text\",{\"1\":{\"101\":2}}],[\"保留原始几何信息\",{\"1\":{\"39\":1}}],[\"简单来说\",{\"1\":{\"92\":1}}],[\"简析llama\",{\"1\":{\"85\":1}}],[\"简析\",{\"0\":{\"85\":1}}],[\"简析pointnet网络模型及其背后原理\",{\"1\":{\"27\":1}}],[\"简析pointnet\",{\"0\":{\"27\":1}}],[\"简析pointnet++\",{\"0\":{\"14\":1},\"1\":{\"14\":1}}],[\"简洁\",{\"1\":{\"39\":1}}],[\"坐标\",{\"1\":{\"39\":1}}],[\"坐标信息\",{\"1\":{\"39\":1}}],[\"强度\",{\"1\":{\"39\":1}}],[\"强制学习正交变换矩阵\",{\"1\":{\"37\":1}}],[\"法向量等属性\",{\"1\":{\"39\":1}}],[\"法向量\",{\"1\":{\"39\":1}}],[\"法线等\",{\"1\":{\"21\":1}}],[\"忽视局部邻域关系\",{\"1\":{\"37\":1}}],[\"忽略局部结构\",{\"1\":{\"37\":1}}],[\"忽略局部结构信息\",{\"1\":{\"37\":1}}],[\"忽略了局部邻域之间的结构关系\",{\"1\":{\"37\":1}}],[\"看不清细节\",{\"1\":{\"37\":1}}],[\"看不懂下面两行代码的话\",{\"1\":{\"21\":1}}],[\"极其高效\",{\"1\":{\"37\":1}}],[\"效率\",{\"1\":{\"37\":1}}],[\"效果对比\",{\"0\":{\"115\":1}}],[\"效果\",{\"1\":{\"30\":3}}],[\"略逊于多视角\",{\"1\":{\"37\":1}}],[\"支持的文件后缀类型\",{\"1\":{\"107\":1}}],[\"支持两种llm\",{\"1\":{\"104\":1}}],[\"支持刚性变换标准化\",{\"1\":{\"37\":1}}],[\"支持原始点云\",{\"1\":{\"37\":1}}],[\"优势\",{\"1\":{\"37\":1}}],[\"少量点无法覆盖关键结构\",{\"1\":{\"37\":1}}],[\"图文到文本\",{\"1\":{\"104\":1}}],[\"图文对比\",{\"1\":{\"101\":1}}],[\"图生文\",{\"1\":{\"103\":1}}],[\"图片切割\",{\"0\":{\"109\":1}}],[\"图片预处理\",{\"0\":{\"108\":1}}],[\"图片库中的图片\",{\"1\":{\"94\":1}}],[\"图片分类\",{\"1\":{\"93\":1,\"95\":1}}],[\"图片分类实战\",{\"1\":{\"93\":1}}],[\"图解transformer\",{\"0\":{\"71\":1},\"1\":{\"71\":1}}],[\"图解bert\",{\"1\":{\"44\":1}}],[\"图解\",{\"0\":{\"44\":1}}],[\"图卷积\",{\"1\":{\"37\":1}}],[\"图像到文本的桥梁\",{\"1\":{\"119\":1}}],[\"图像块嵌入层\",{\"1\":{\"110\":1}}],[\"图像块的尺寸\",{\"1\":{\"110\":1}}],[\"图像或\",{\"1\":{\"108\":2}}],[\"图像的索引\",{\"1\":{\"107\":1}}],[\"图像预处理的转换操作\",{\"1\":{\"107\":1}}],[\"图像注意力掩码\",{\"1\":{\"104\":1}}],[\"图像特征和掩码\",{\"1\":{\"104\":1}}],[\"图像特征作为cross\",{\"1\":{\"104\":1}}],[\"图像特征提取和模态融合都很重\",{\"1\":{\"98\":1}}],[\"图像特征提取与分类\",{\"1\":{\"91\":1}}],[\"图像编码阶段\",{\"1\":{\"104\":1}}],[\"图像编码器采用了\",{\"1\":{\"93\":1}}],[\"图像编码器\",{\"1\":{\"7\":1}}],[\"图像通过视觉编码器\",{\"1\":{\"103\":1}}],[\"图像分支依赖目标检测器\",{\"1\":{\"98\":1}}],[\"图像分支\",{\"1\":{\"98\":1}}],[\"图像对是从互联网收集的\",{\"1\":{\"96\":1}}],[\"图像对为负样本\",{\"1\":{\"90\":1}}],[\"图像对的相似度\",{\"1\":{\"90\":1}}],[\"图像对的训练batch\",{\"1\":{\"90\":1}}],[\"图像对的预训练方法\",{\"1\":{\"89\":1}}],[\"图像对\",{\"1\":{\"89\":1}}],[\"图像\",{\"1\":{\"39\":1,\"100\":1,\"109\":1}}],[\"图像处理或其他数据集中用于抽样的算法\",{\"1\":{\"18\":1}}],[\"描述\",{\"1\":{\"37\":1,\"40\":1}}],[\"仍然不如分块处理或多层级聚合模型高效\",{\"1\":{\"37\":1}}],[\"异常检测\",{\"1\":{\"40\":1}}],[\"异常检测等特殊场景\",{\"1\":{\"40\":1}}],[\"异常点会影响分类和分割性能\",{\"1\":{\"37\":1}}],[\"异常值等问题\",{\"1\":{\"29\":1}}],[\"插入异常点\",{\"1\":{\"37\":1}}],[\"插入的多模态嵌入\",{\"1\":{\"10\":1}}],[\"面对大量噪声点时效果较差\",{\"1\":{\"37\":1}}],[\"面部发生形变\",{\"1\":{\"37\":1}}],[\"尤其未训练时\",{\"1\":{\"37\":1}}],[\"尤其是在低层次上对每个质心点运行局部pointnet时\",{\"1\":{\"26\":1}}],[\"十二亿\",{\"1\":{\"115\":1}}],[\"十亿\",{\"1\":{\"115\":1}}],[\"十一\",{\"1\":{\"37\":1}}],[\"十\",{\"1\":{\"37\":1}}],[\"又会导致计算资源浪费\",{\"1\":{\"37\":1}}],[\"又理解整体结构\",{\"1\":{\"30\":1}}],[\"太大\",{\"1\":{\"37\":1}}],[\"太大则可能导致不相关的点增多\",{\"1\":{\"19\":1}}],[\"受限于瓶颈维度\",{\"1\":{\"37\":1}}],[\"九\",{\"1\":{\"37\":1}}],[\"深度学习模型\",{\"1\":{\"37\":1}}],[\"八百六十万\",{\"1\":{\"115\":1}}],[\"八\",{\"1\":{\"37\":1}}],[\"特殊\",{\"1\":{\"68\":1}}],[\"特别是在遮挡严重的情况下\",{\"1\":{\"37\":1}}],[\"特征增强\",{\"1\":{\"40\":1}}],[\"特征融合\",{\"1\":{\"40\":1}}],[\"特征空间变换矩阵\",{\"1\":{\"34\":1}}],[\"特征变换开关\",{\"1\":{\"34\":1}}],[\"特征提取网络相对轻量\",{\"1\":{\"98\":1}}],[\"特征提取\",{\"0\":{\"34\":1}}],[\"特征编码\",{\"1\":{\"20\":1}}],[\"特征从原始嵌入维度\",{\"1\":{\"9\":1}}],[\"特征\",{\"1\":{\"7\":2}}],[\"当提到模型参数量时\",{\"1\":{\"115\":1}}],[\"当数据量小于30m时\",{\"1\":{\"115\":1}}],[\"当\",{\"1\":{\"112\":1}}],[\"当我们在其他任务中使用预训练好的模型时\",{\"1\":{\"108\":1}}],[\"当使用\",{\"1\":{\"107\":1}}],[\"当cnn具有以上两种归纳偏置\",{\"1\":{\"105\":1}}],[\"当拥有足够多的数据进行预训练的时候\",{\"1\":{\"105\":1}}],[\"当is\",{\"1\":{\"103\":1}}],[\"当前token之前的text\",{\"1\":{\"103\":1}}],[\"当文本和query\",{\"1\":{\"102\":1}}],[\"当输入点云非常稀疏时\",{\"1\":{\"37\":1}}],[\"当局部区域的密度较高时\",{\"1\":{\"26\":1}}],[\"当局部区域的密度较低时\",{\"1\":{\"26\":1}}],[\"七\",{\"1\":{\"37\":1}}],[\"正样本batch=1\",{\"1\":{\"102\":1}}],[\"正样本batch\",{\"1\":{\"102\":3}}],[\"正交变换包括\",{\"1\":{\"42\":1}}],[\"正交变换的本质是\",{\"1\":{\"42\":1}}],[\"正交变换\",{\"0\":{\"42\":1}}],[\"正是对这一缺陷的改进\",{\"1\":{\"37\":1}}],[\"正则化损失\",{\"0\":{\"33\":1}}],[\"缺乏具体的上下文\",{\"1\":{\"92\":1}}],[\"缺乏精细建模\",{\"1\":{\"37\":1}}],[\"缺乏层次化\",{\"1\":{\"37\":1}}],[\"缺乏层次化特征提取机制\",{\"1\":{\"37\":1}}],[\"缺乏动态上下文感知\",{\"1\":{\"37\":1}}],[\"缺陷对比\",{\"1\":{\"37\":1}}],[\"缺陷类型\",{\"1\":{\"37\":1}}],[\"缺陷\",{\"0\":{\"37\":1},\"1\":{\"37\":1}}],[\"六\",{\"1\":{\"37\":1}}],[\"基于\",{\"1\":{\"98\":1}}],[\"基于自回归或语言掩码的预训练方法已经相对成熟\",{\"1\":{\"96\":1}}],[\"基于上下文编码\",{\"1\":{\"68\":1}}],[\"基于图卷积或注意力机制的模型更能捕捉这种非刚性变化\",{\"1\":{\"37\":1}}],[\"基于以下前提\",{\"1\":{\"19\":1}}],[\"很难在这种情况下保持分类的一致性\",{\"1\":{\"37\":1}}],[\"拉伸等会导致形变的操作\",{\"1\":{\"42\":1}}],[\"拉伸等形变\",{\"1\":{\"37\":1}}],[\"拉伸\",{\"1\":{\"37\":1,\"41\":1}}],[\"旋转\",{\"1\":{\"37\":1,\"41\":2,\"42\":2}}],[\"五\",{\"1\":{\"37\":1}}],[\"体素网格\",{\"1\":{\"39\":1}}],[\"体素\",{\"1\":{\"37\":1}}],[\"方便后续的计算和比较\",{\"1\":{\"93\":1}}],[\"方法中\",{\"1\":{\"103\":1}}],[\"方法\",{\"1\":{\"37\":1,\"39\":1}}],[\"方向\",{\"1\":{\"7\":1}}],[\"方向研究\",{\"1\":{\"2\":1}}],[\"四\",{\"1\":{\"37\":1}}],[\"🧪\",{\"1\":{\"37\":1,\"68\":1}}],[\"🧩\",{\"1\":{\"37\":2,\"68\":1}}],[\"🧱\",{\"1\":{\"37\":3}}],[\"🧠\",{\"1\":{\"33\":1,\"37\":1,\"40\":1}}],[\"难以自动构建\",{\"1\":{\"39\":1}}],[\"难以用\",{\"1\":{\"39\":1}}],[\"难以建模更高维度的空间关系\",{\"1\":{\"37\":1}}],[\"难以捕捉非刚性变换下的不变性\",{\"1\":{\"37\":1}}],[\"难以区分语义相近但位置不同的区域\",{\"1\":{\"37\":1}}],[\"难点\",{\"0\":{\"29\":1},\"1\":{\"30\":4}}],[\"⚠️\",{\"1\":{\"37\":1,\"40\":1,\"68\":1}}],[\"给出问题和上下文\",{\"1\":{\"68\":1}}],[\"给每个点\",{\"1\":{\"37\":1}}],[\"给定两个超参数\",{\"1\":{\"19\":1}}],[\"二分类task\",{\"0\":{\"102\":1}}],[\"二\",{\"1\":{\"37\":1}}],[\"二次sample\",{\"1\":{\"22\":1}}],[\"容易出错\",{\"1\":{\"37\":1}}],[\"虽然\",{\"1\":{\"37\":3}}],[\"虽然这个点没有实际意义\",{\"1\":{\"21\":1}}],[\"📚\",{\"1\":{\"68\":1}}],[\"📦\",{\"1\":{\"40\":1}}],[\"📌\",{\"1\":{\"37\":1}}],[\"📐\",{\"1\":{\"37\":1}}],[\"📊\",{\"1\":{\"37\":1}}],[\"📈\",{\"1\":{\"37\":2}}],[\"📉\",{\"1\":{\"37\":2}}],[\"💡\",{\"1\":{\"37\":1,\"68\":1}}],[\"🔍\",{\"1\":{\"37\":6,\"68\":1}}],[\"表征学习\",{\"0\":{\"100\":1},\"1\":{\"100\":1}}],[\"表达能力受限于\",{\"1\":{\"37\":1}}],[\"表现良好\",{\"1\":{\"37\":1}}],[\"表现不错\",{\"1\":{\"37\":1}}],[\"表明\",{\"1\":{\"37\":1}}],[\"表面纹理等\",{\"1\":{\"37\":1}}],[\"表示模型的规模\",{\"1\":{\"118\":1}}],[\"表示不进行归一化\",{\"1\":{\"109\":2}}],[\"表示每个图像块的大小是\",{\"1\":{\"108\":1}}],[\"表示每个点是否具有特定可操作性的概率\",{\"1\":{\"7\":1}}],[\"表示所有图像token有效\",{\"1\":{\"104\":1}}],[\"表示将张量中的值限制在\",{\"1\":{\"67\":1}}],[\"表示形式\",{\"1\":{\"39\":1}}],[\"表示只在通道维度操作\",{\"1\":{\"32\":1}}],[\"表示填充\",{\"1\":{\"10\":1}}],[\"表示有效\",{\"1\":{\"10\":1}}],[\"表示\",{\"1\":{\"8\":2,\"21\":1,\"32\":1,\"34\":1,\"46\":1,\"103\":1}}],[\"曲率等细节\",{\"1\":{\"37\":1}}],[\"曲率\",{\"1\":{\"37\":1}}],[\"导致模型无法捕捉到更细粒度的几何细节\",{\"1\":{\"37\":1}}],[\"导致所有的特征\",{\"1\":{\"15\":1}}],[\"而预训练过程中使用的输入图像尺寸通常固定为\",{\"1\":{\"108\":1}}],[\"而通过引入特定的归纳偏置\",{\"1\":{\"105\":1}}],[\"而vit将其放到前面\",{\"1\":{\"112\":1}}],[\"而vit\",{\"1\":{\"105\":1}}],[\"而vit则选择了三种不同尺寸的模型\",{\"1\":{\"90\":1}}],[\"而非\",{\"1\":{\"103\":1}}],[\"而非生成答案\",{\"1\":{\"68\":1}}],[\"而这里我们将会反转这个逻辑\",{\"1\":{\"94\":1}}],[\"而最大的vit模型vit\",{\"1\":{\"90\":1}}],[\"而图像编码器\",{\"1\":{\"90\":1,\"91\":1}}],[\"而图像编码器则用于提取图像的特征\",{\"1\":{\"90\":1}}],[\"而剩余的个文本\",{\"1\":{\"90\":1}}],[\"而\",{\"1\":{\"88\":1}}],[\"而不能看到未来的词\",{\"1\":{\"74\":1}}],[\"而不是重新生成一个新的答案\",{\"1\":{\"68\":1}}],[\"而点云是无序集合\",{\"1\":{\"40\":1}}],[\"而没有建模点与点之间的局部几何关系\",{\"1\":{\"37\":1}}],[\"而是对输入文本中每个\",{\"1\":{\"68\":1}}],[\"而是能够捕获多个尺度上的局部特征\",{\"1\":{\"24\":1}}],[\"而是通过\",{\"1\":{\"8\":1}}],[\"只有一个特殊标记\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"只需要用一个一个linear即可\",{\"1\":{\"114\":1}}],[\"只需要将图像调整到合适的大小\",{\"1\":{\"108\":1}}],[\"只需要去除cls\",{\"1\":{\"55\":1}}],[\"只能对输入文本中的\",{\"1\":{\"68\":1}}],[\"只能处理刚性变换\",{\"1\":{\"37\":1}}],[\"只做抽取式问答\",{\"1\":{\"68\":1}}],[\"只包含\",{\"1\":{\"68\":1}}],[\"只改变物体的方向\",{\"1\":{\"42\":1}}],[\"只改变位置和朝向\",{\"1\":{\"41\":1}}],[\"只关注全局结构\",{\"1\":{\"37\":1}}],[\"只学正交变换\",{\"1\":{\"37\":1}}],[\"只通过\",{\"1\":{\"37\":1}}],[\"只要关键点还在\",{\"1\":{\"30\":1}}],[\"无论是有监督还是自监督方法\",{\"1\":{\"96\":1}}],[\"无论你如何打乱输入元素的顺序\",{\"1\":{\"40\":1}}],[\"无序性\",{\"1\":{\"39\":1}}],[\"无需预处理\",{\"1\":{\"37\":1}}],[\"无局部聚合机制\",{\"1\":{\"37\":1}}],[\"无法实现zero\",{\"1\":{\"96\":1}}],[\"无法\",{\"1\":{\"68\":1}}],[\"无法处理不在原文中的答案\",{\"1\":{\"68\":1}}],[\"无法处理非刚性变形\",{\"1\":{\"37\":1}}],[\"无法处理非刚性形变\",{\"1\":{\"37\":2}}],[\"无法充分利用\",{\"1\":{\"37\":1}}],[\"无法区分顺序信息\",{\"1\":{\"37\":1}}],[\"无法有效利用局部结构\",{\"1\":{\"37\":1}}],[\"无法捕捉边缘\",{\"1\":{\"37\":1}}],[\"无法应对弯曲\",{\"1\":{\"37\":1}}],[\"无法像\",{\"1\":{\"37\":1}}],[\"无法直接用于每个点\",{\"1\":{\"36\":1}}],[\"无效\",{\"1\":{\"21\":1}}],[\"座位\",{\"1\":{\"36\":1}}],[\"仅训练分类头\",{\"1\":{\"118\":1}}],[\"仅借鉴了encoder结构\",{\"1\":{\"105\":1}}],[\"仅能和自己的\",{\"1\":{\"101\":1}}],[\"仅靠\",{\"1\":{\"37\":1}}],[\"仅靠局部特征很难判断某个点属于哪个部件\",{\"1\":{\"36\":1}}],[\"仅返回预测结果\",{\"1\":{\"7\":1}}],[\"再和真实标签做交叉熵损失\",{\"1\":{\"114\":1}}],[\"再和缓存的key\",{\"1\":{\"103\":1}}],[\"再乘以缩放因子scale\",{\"1\":{\"113\":1}}],[\"再次应用\",{\"1\":{\"112\":1}}],[\"再映射为\",{\"1\":{\"109\":1}}],[\"再来回顾我们的卷积层计算公式\",{\"1\":{\"109\":1}}],[\"再从中心位置裁剪成224x224\",{\"1\":{\"108\":1}}],[\"再进行归一化和标准化处理\",{\"1\":{\"108\":2}}],[\"再通过\",{\"1\":{\"103\":1}}],[\"再通过第一个卷积层提取初始特征\",{\"1\":{\"34\":1}}],[\"再对每个选项做分类打分\",{\"1\":{\"70\":1}}],[\"再与每个点的局部特征拼接\",{\"1\":{\"36\":1}}],[\"再用\",{\"1\":{\"21\":1}}],[\"预处理这个步骤在论文里并没有详细说明\",{\"1\":{\"108\":1}}],[\"预处理层\",{\"1\":{\"32\":1}}],[\"预测图像对应的文本的词袋模型\",{\"1\":{\"96\":1}}],[\"预测出一个变换矩阵\",{\"1\":{\"34\":1}}],[\"预训练权重大小为393mb\",{\"1\":{\"118\":1}}],[\"预训练模型很容易直接zero\",{\"1\":{\"96\":1}}],[\"预训练模型中\",{\"1\":{\"93\":1}}],[\"预训练模型名称\",{\"1\":{\"93\":1}}],[\"预训练模型下载下来之后\",{\"1\":{\"45\":1}}],[\"预训练与微调\",{\"1\":{\"60\":1}}],[\"预训练\",{\"0\":{\"60\":1}}],[\"控制是否使用\",{\"1\":{\"34\":1}}],[\"控制是否输出全局特征\",{\"1\":{\"34\":1}}],[\"鼓励变换矩阵接近正交矩阵\",{\"1\":{\"33\":1}}],[\"平滑处理\",{\"1\":{\"40\":1}}],[\"平均池化\",{\"1\":{\"40\":1}}],[\"平均值作为损失项\",{\"1\":{\"33\":1}}],[\"平移等变换具有鲁棒性\",{\"1\":{\"30\":1}}],[\"平移\",{\"1\":{\"29\":1,\"32\":1,\"41\":1}}],[\"平移中心到以关键点为原点的局部坐标系上\",{\"1\":{\"21\":1}}],[\"添加一维位置编码和二维位置编码并没有太大的差异\",{\"1\":{\"111\":1}}],[\"添加位置编码\",{\"0\":{\"111\":1}}],[\"添加到\",{\"1\":{\"104\":1}}],[\"添加特殊token标记\",{\"1\":{\"46\":1}}],[\"添加\",{\"0\":{\"110\":1},\"1\":{\"33\":1}}],[\"❗而只有正交矩阵才能表示刚性变换\",{\"1\":{\"33\":1}}],[\"反过来计算text\",{\"1\":{\"101\":1}}],[\"反射\",{\"1\":{\"33\":1,\"37\":1,\"42\":1}}],[\"反之亦然\",{\"1\":{\"23\":1}}],[\"稳定的\",{\"1\":{\"32\":1}}],[\"会将输入图像分割成大小为\",{\"1\":{\"118\":1}}],[\"会将这些输入展平\",{\"1\":{\"70\":1}}],[\"会在get\",{\"1\":{\"103\":1}}],[\"会从缓存中取出对应层先前缓存的key\",{\"1\":{\"103\":1}}],[\"会使用\",{\"1\":{\"103\":1}}],[\"会进行相应的错误提示并返回\",{\"1\":{\"93\":1}}],[\"会导致非正交\",{\"1\":{\"32\":1}}],[\"会影响特征提取的一致性\",{\"1\":{\"32\":1}}],[\"神经网络的输出在训练初期往往接近于零\",{\"1\":{\"32\":1}}],[\"后面的mlp是个单独的结构\",{\"1\":{\"112\":1}}],[\"后将投影后的\",{\"1\":{\"104\":1}}],[\"后三种模型是按照efficientnet的缩放规则对resnet分别放大4倍\",{\"1\":{\"90\":1}}],[\"后的形式\",{\"1\":{\"66\":1}}],[\"后续再进行裁剪操作\",{\"1\":{\"108\":1}}],[\"后续的\",{\"1\":{\"37\":1}}],[\"后续的特征提取更稳定\",{\"1\":{\"32\":1}}],[\"后续发展\",{\"1\":{\"37\":1}}],[\"后续改进方向\",{\"1\":{\"37\":1}}],[\"后续改进\",{\"1\":{\"37\":1}}],[\"后\",{\"1\":{\"32\":1}}],[\"成多头格式\",{\"1\":{\"57\":1}}],[\"成\",{\"1\":{\"32\":1,\"70\":1}}],[\"成一个大的局部区域\",{\"1\":{\"21\":1}}],[\"让q\",{\"1\":{\"103\":1}}],[\"让每个点都能看到上下文信息\",{\"1\":{\"34\":1,\"36\":1}}],[\"让变换矩阵从一个恒等变换开始学习\",{\"1\":{\"32\":1}}],[\"让模型自己学会区分不同的句子\",{\"1\":{\"49\":1}}],[\"让模型更容易训练和泛化\",{\"1\":{\"32\":1}}],[\"让模型既关注局部细节\",{\"1\":{\"30\":1}}],[\"让网络从一个小扰动开始学习\",{\"1\":{\"32\":1}}],[\"帮助训练稳定收敛\",{\"1\":{\"32\":1}}],[\"用一个简化版的例子说明上述过程\",{\"1\":{\"109\":1}}],[\"用文本描述去匹配最合适的图片内容\",{\"1\":{\"94\":1}}],[\"用当前图片外层目录的名字作为其分类名词\",{\"1\":{\"93\":1}}],[\"用户本地运行时\",{\"1\":{\"72\":1}}],[\"用以区分不同的句子\",{\"1\":{\"46\":1}}],[\"用\",{\"1\":{\"46\":1}}],[\"用全连接层逐步压缩到\",{\"1\":{\"32\":1}}],[\"用于同时生成查询\",{\"1\":{\"113\":1}}],[\"用于调整注意力分数\",{\"1\":{\"113\":1}}],[\"用于防止过拟合\",{\"1\":{\"112\":1}}],[\"用于随机深度\",{\"1\":{\"112\":1}}],[\"用于位置嵌入后的随机丢弃\",{\"1\":{\"111\":1,\"114\":1}}],[\"用于分类任务\",{\"1\":{\"110\":1}}],[\"用于预测图像的类别\",{\"1\":{\"110\":1}}],[\"用于存储训练集和验证集的图像预处理转换操作\",{\"1\":{\"108\":1}}],[\"用于将输入图像分割成多个图像块并进行嵌入\",{\"1\":{\"109\":1}}],[\"用于将一个方法定义为静态方法\",{\"1\":{\"107\":1}}],[\"用于将一个批次的数据组合成一个张量\",{\"1\":{\"107\":1}}],[\"用于将张量中的值限制在指定的范围内\",{\"1\":{\"67\":1}}],[\"用于计算\",{\"1\":{\"103\":1}}],[\"用于计算交叉熵损失\",{\"1\":{\"70\":1}}],[\"用于指定在计算损失时忽略的标签索引\",{\"1\":{\"67\":1}}],[\"用于判断给出的两个句子是否连续\",{\"1\":{\"46\":1}}],[\"用于后续比较\",{\"1\":{\"33\":1}}],[\"用于后续计算其他点到该点的距离\",{\"1\":{\"21\":1}}],[\"用于快速访问每个\",{\"1\":{\"21\":1}}],[\"用于稳定训练过程\",{\"1\":{\"12\":1}}],[\"用于\",{\"1\":{\"12\":1,\"21\":1}}],[\"用于降低计算复杂度\",{\"1\":{\"12\":1}}],[\"用于融合来自语言模型的不同语义信息\",{\"1\":{\"12\":1}}],[\"卷积后剩余的长和宽相乘作为时间维度\",{\"1\":{\"109\":1}}],[\"卷积核个数为768的卷积层来进行实现\",{\"1\":{\"109\":1}}],[\"卷积\",{\"1\":{\"32\":1}}],[\"摆正\",{\"1\":{\"32\":1,\"34\":1}}],[\"类\",{\"1\":{\"109\":1}}],[\"类似\",{\"1\":{\"88\":1}}],[\"类似于在cnn中权重共享的概念\",{\"1\":{\"15\":1}}],[\"类型\",{\"1\":{\"68\":1}}],[\"类中\",{\"1\":{\"32\":1}}],[\"应用自定义的权重初始化函数\",{\"1\":{\"114\":1}}],[\"应用\",{\"1\":{\"112\":1}}],[\"应用注意力掩码\",{\"1\":{\"103\":1}}],[\"应用场景\",{\"1\":{\"40\":1}}],[\"应用到原始点云上\",{\"1\":{\"32\":1}}],[\"应该采用什么样的方法来进行训练\",{\"1\":{\"96\":1}}],[\"应该更高地加权第二个向量\",{\"1\":{\"26\":1}}],[\"应该关注图像中的哪些位置\",{\"1\":{\"8\":1}}],[\"应该关注哪些点云点\",{\"1\":{\"8\":1}}],[\"变为\",{\"1\":{\"101\":3}}],[\"变形\",{\"1\":{\"41\":1}}],[\"变换不变性\",{\"1\":{\"37\":1}}],[\"变换矩阵会通过\",{\"1\":{\"32\":1}}],[\"变成1维度之后就成了50176\",{\"1\":{\"109\":1}}],[\"变成\",{\"1\":{\"21\":2,\"36\":1,\"70\":1}}],[\"变成特征向量\",{\"1\":{\"16\":1}}],[\"专门用于预测一个\",{\"1\":{\"32\":1}}],[\"扫描角度不同等\",{\"1\":{\"32\":1}}],[\"结束符\",{\"1\":{\"104\":1}}],[\"结束\",{\"1\":{\"68\":1}}],[\"结构\",{\"0\":{\"75\":1,\"77\":1,\"81\":1},\"1\":{\"74\":1}}],[\"结构规整\",{\"1\":{\"39\":1}}],[\"结构单一\",{\"1\":{\"37\":1}}],[\"结构简单\",{\"1\":{\"37\":1}}],[\"结果如下\",{\"1\":{\"115\":1}}],[\"结果如下图所示\",{\"1\":{\"111\":1}}],[\"结果发现在imagenet数据集上能够带来3\",{\"1\":{\"92\":1}}],[\"结果显示\",{\"1\":{\"37\":1}}],[\"结果就不会变\",{\"1\":{\"30\":1}}],[\"结合缓存的attention\",{\"1\":{\"103\":1}}],[\"结合\",{\"1\":{\"30\":1}}],[\"决定\",{\"1\":{\"30\":1}}],[\"小结\",{\"0\":{\"96\":1}}],[\"小扰动不会改变函数输出\",{\"1\":{\"30\":1}}],[\"小红书\",{\"1\":{\"0\":1}}],[\"定位答案\",{\"1\":{\"68\":1}}],[\"定理表明\",{\"1\":{\"30\":1}}],[\"定义注意力矩阵的丢弃层\",{\"1\":{\"113\":1}}],[\"定义一个线性层\",{\"1\":{\"113\":1}}],[\"定义一个二维卷积层\",{\"1\":{\"109\":1}}],[\"定义一个字典\",{\"1\":{\"108\":1}}],[\"定义当前目录\",{\"1\":{\"93\":1,\"95\":1}}],[\"定义局部区域的形心\",{\"1\":{\"17\":1}}],[\"定义投影层的丢弃层\",{\"1\":{\"113\":1}}],[\"定义投影层\",{\"1\":{\"12\":1,\"113\":1}}],[\"关于vit模型的不同版本\",{\"1\":{\"115\":1}}],[\"关于多头注意力机制流程不太清楚的\",{\"1\":{\"113\":1}}],[\"关于norm层\",{\"1\":{\"112\":1}}],[\"关于bertmodel的代码解析部分\",{\"1\":{\"102\":1}}],[\"关于这一领域的详细综述\",{\"1\":{\"92\":1}}],[\"关于我们\",{\"0\":{\"1\":1}}],[\"关键点可能丢失\",{\"1\":{\"37\":1}}],[\"关键点集\",{\"1\":{\"30\":1}}],[\"加载该模型后\",{\"1\":{\"118\":1}}],[\"加载预训练好的vit\",{\"1\":{\"118\":1}}],[\"加载预训练模型\",{\"0\":{\"118\":1}}],[\"加载模型和处理器\",{\"1\":{\"93\":1,\"95\":1}}],[\"加上位置嵌入并进行随机丢弃\",{\"1\":{\"111\":1,\"114\":1}}],[\"加上全局特征后\",{\"1\":{\"36\":1}}],[\"加上单位矩阵作为初始偏置\",{\"1\":{\"32\":1}}],[\"加单位矩阵\",{\"1\":{\"32\":1}}],[\"加入数据增强后缓解\",{\"1\":{\"37\":1}}],[\"加入\",{\"1\":{\"32\":1}}],[\"加入正则项约束变换矩阵接近正交\",{\"1\":{\"30\":1}}],[\"加权和\",{\"1\":{\"40\":1}}],[\"加权后的输出\",{\"1\":{\"12\":2}}],[\"加权总损失\",{\"1\":{\"7\":1}}],[\"矩阵中的对角线元素\",{\"1\":{\"90\":1}}],[\"矩阵所有元素平方和开方\",{\"1\":{\"33\":1}}],[\"矩阵返回\",{\"1\":{\"32\":1}}],[\"矩阵\",{\"1\":{\"30\":2}}],[\"比较两个分类名词是否相等\",{\"1\":{\"93\":1}}],[\"比排序\",{\"1\":{\"30\":1}}],[\"比如一张224x224的图片\",{\"1\":{\"109\":1}}],[\"比如取到了问题部分的内容\",{\"1\":{\"68\":1}}],[\"比如对于一个长度为\",{\"1\":{\"66\":1}}],[\"比如边缘\",{\"1\":{\"37\":1}}],[\"比如椅子的腿\",{\"1\":{\"36\":1}}],[\"比如椅子朝向不同\",{\"1\":{\"32\":1}}],[\"比如法线\",{\"1\":{\"21\":1}}],[\"比如通过\",{\"1\":{\"21\":1}}],[\"比如颜色\",{\"1\":{\"21\":1}}],[\"γ\",{\"1\":{\"30\":2}}],[\"≈\",{\"1\":{\"30\":1}}],[\"原始transformer的norm层在多头注意力和前馈网络之后\",{\"1\":{\"112\":1}}],[\"原始论文layernorm在最后\",{\"1\":{\"78\":1}}],[\"原始点集合\",{\"1\":{\"39\":1}}],[\"原始点云数据\",{\"1\":{\"22\":1}}],[\"原序列添加特殊token标记图\",{\"1\":{\"46\":1}}],[\"原理\",{\"0\":{\"106\":1}}],[\"原理回顾\",{\"1\":{\"37\":1}}],[\"原理说明\",{\"1\":{\"30\":1}}],[\"原因分析\",{\"1\":{\"37\":2}}],[\"聚合所有点的信息\",{\"1\":{\"30\":1,\"37\":1}}],[\"✅\",{\"1\":{\"30\":5,\"32\":3,\"34\":2,\"36\":2,\"37\":12,\"40\":7,\"41\":1,\"66\":2,\"68\":4,\"70\":1}}],[\"稀疏性强\",{\"1\":{\"39\":1}}],[\"稀疏点云等任务中表现受限\",{\"1\":{\"37\":1}}],[\"稀疏点云下性能差\",{\"1\":{\"37\":1}}],[\"稀疏\",{\"1\":{\"29\":1}}],[\"utils\",{\"1\":{\"107\":3,\"108\":2}}],[\"using\",{\"1\":{\"84\":1}}],[\"use\",{\"1\":{\"93\":1,\"95\":1,\"100\":1,\"103\":5,\"104\":2,\"113\":1}}],[\"used\",{\"1\":{\"46\":1,\"112\":1}}],[\"users\",{\"1\":{\"45\":1}}],[\"uni\",{\"1\":{\"101\":2}}],[\"unicode\",{\"1\":{\"51\":1,\"61\":1}}],[\"unusual\",{\"1\":{\"57\":1}}],[\"unsqueeze\",{\"1\":{\"49\":1,\"54\":2,\"84\":1,\"101\":4}}],[\"unk\",{\"1\":{\"46\":2}}],[\"under\",{\"1\":{\"29\":1}}],[\"unordered\",{\"1\":{\"29\":1,\"40\":1}}],[\"up\",{\"1\":{\"7\":1,\"9\":1}}],[\"引言\",{\"0\":{\"88\":1}}],[\"引入多尺度采样等\",{\"1\":{\"37\":1}}],[\"引入了两个变换网络\",{\"1\":{\"33\":1}}],[\"引入两个空间变换网络\",{\"1\":{\"30\":1}}],[\"引入\",{\"1\":{\"28\":1,\"30\":1,\"37\":1}}],[\"引导下提取的点云信息增强后的图像特征\",{\"1\":{\"8\":1}}],[\"设是一个函数\",{\"1\":{\"40\":1}}],[\"设计局部\",{\"1\":{\"30\":1}}],[\"设计了一个统一架构\",{\"1\":{\"28\":1}}],[\"设置柱状图的标题\",{\"1\":{\"107\":1}}],[\"设置y坐标\",{\"1\":{\"107\":1}}],[\"设置x坐标\",{\"1\":{\"107\":1}}],[\"设置的回调方法cllote\",{\"1\":{\"48\":1}}],[\"设置\",{\"1\":{\"7\":1}}],[\"避免了复杂的预处理\",{\"1\":{\"28\":1}}],[\"🌟\",{\"1\":{\"28\":1}}],[\"❌\",{\"1\":{\"28\":1,\"41\":1,\"68\":2}}],[\"完整代码\",{\"0\":{\"95\":1}}],[\"完整的单尺度分组分类流程为\",{\"1\":{\"22\":1}}],[\"完全支持\",{\"1\":{\"37\":1}}],[\"完成\",{\"1\":{\"26\":1}}],[\"更新特征数量为表示层的维度\",{\"1\":{\"114\":1}}],[\"更复杂的对称函数建模\",{\"1\":{\"40\":1}}],[\"更容易训练\",{\"1\":{\"32\":1}}],[\"更有效\",{\"1\":{\"30\":1}}],[\"更高分辨率\",{\"1\":{\"26\":1}}],[\"更易于管理的子集\",{\"1\":{\"15\":1}}],[\"首先说明\",{\"1\":{\"118\":1}}],[\"首先使用卷积层对输入图像进行处理\",{\"1\":{\"109\":1}}],[\"首先需要对输入图片进行尺寸变化\",{\"1\":{\"108\":1}}],[\"首先输入图片\",{\"1\":{\"104\":1}}],[\"首先我们用data目录充当我们的图片库来源\",{\"1\":{\"94\":1}}],[\"首先\",{\"1\":{\"26\":1,\"91\":1,\"93\":1}}],[\"首次sample\",{\"1\":{\"22\":1}}],[\"某一层次𝐿𝑖的区域特征是通过将来自下一级𝐿𝑖−1的子区域特征总结后的向量与直接处理该局部区域所有原始点的单个pointnet得到的特征向量进行concat得到的\",{\"1\":{\"26\":1}}],[\"直接输入冻结参数的\",{\"1\":{\"104\":1}}],[\"直接对图像进行分类\",{\"1\":{\"91\":2}}],[\"直接对所有点进行特征提取\",{\"1\":{\"21\":1}}],[\"直接以点集作为输入\",{\"1\":{\"28\":1}}],[\"直接处理的原始点特征\",{\"1\":{\"26\":1}}],[\"直接提取的特征\",{\"1\":{\"26\":1}}],[\"还不如直接预测词袋模型\",{\"1\":{\"96\":1}}],[\"还有另一个方向\",{\"1\":{\"96\":1}}],[\"还在336的分辨率下额外进行了一个周期的微调\",{\"1\":{\"90\":1}}],[\"还考虑了从更低分辨率\",{\"1\":{\"26\":1}}],[\"还小\",{\"1\":{\"21\":1}}],[\"逐步聚合全局信息\",{\"1\":{\"110\":1}}],[\"逐步向正交矩阵靠拢\",{\"1\":{\"32\":1}}],[\"逐点\",{\"1\":{\"32\":1}}],[\"逐层抽象后融合成全局特征\",{\"1\":{\"25\":1}}],[\"逐个拼接\",{\"1\":{\"10\":1}}],[\"策略预训练轻量级查询\",{\"1\":{\"99\":1}}],[\"策略\",{\"1\":{\"25\":1}}],[\"生成类别名称以及对应的数字索引\",{\"1\":{\"107\":1}}],[\"生成文本的最小长度\",{\"1\":{\"104\":1}}],[\"生成文本的最大长度\",{\"1\":{\"104\":1}}],[\"生成文本嵌入\",{\"1\":{\"93\":1,\"95\":1}}],[\"生成学习\",{\"0\":{\"104\":1}}],[\"生成压缩的视觉表示\",{\"1\":{\"103\":1}}],[\"生成text\",{\"1\":{\"103\":1}}],[\"生成比标签\",{\"1\":{\"101\":1}}],[\"生成的文本特征相当于分类器的权重\",{\"1\":{\"91\":1}}],[\"生成式\",{\"1\":{\"68\":2}}],[\"生成一个上下文相关的表示\",{\"1\":{\"68\":1}}],[\"生成\",{\"1\":{\"68\":1}}],[\"生成注意力掩码\",{\"1\":{\"46\":1}}],[\"生成padding部分的mask列表\",{\"1\":{\"46\":1}}],[\"生成固定长度的特征向量\",{\"1\":{\"25\":1}}],[\"生成点集的划分\",{\"1\":{\"15\":1}}],[\"要采样的质心点数量\",{\"1\":{\"25\":1}}],[\"要么对所有点做操作\",{\"1\":{\"15\":1}}],[\"要么对一个点做操作\",{\"1\":{\"15\":1}}],[\"之前已经有一些研究工作探讨了使用文本作为监督信号来训练视觉模型\",{\"1\":{\"96\":1}}],[\"之间的关系\",{\"1\":{\"110\":1}}],[\"之间的平方欧氏距离\",{\"1\":{\"21\":1}}],[\"之间\",{\"1\":{\"67\":1}}],[\"之后\",{\"1\":{\"88\":1}}],[\"之后concat形成该区域提取的总特征\",{\"1\":{\"25\":1}}],[\"之后这些不同尺度上提取的特征被串联起来\",{\"1\":{\"24\":1}}],[\"称为随机输入丢弃\",{\"1\":{\"24\":1}}],[\"多项选择题\",{\"1\":{\"70\":1}}],[\"多项选择任务是指给定一个问题和多个候选答案\",{\"1\":{\"70\":1}}],[\"多项选择任务\",{\"0\":{\"70\":1}}],[\"多头自注意力层\",{\"1\":{\"112\":1}}],[\"多头自注意力\",{\"0\":{\"84\":1,\"113\":1}}],[\"多头自注意力机制通过并行计算多个注意力头\",{\"1\":{\"74\":1}}],[\"多头自注意力机制\",{\"1\":{\"59\":1,\"74\":1}}],[\"多头自注意力计算流程图\",{\"1\":{\"57\":1,\"84\":1}}],[\"多视角图像\",{\"1\":{\"39\":1}}],[\"多视角\",{\"1\":{\"37\":2}}],[\"多分辨率分组\",{\"0\":{\"26\":1},\"1\":{\"26\":1}}],[\"多尺度建模能力\",{\"1\":{\"37\":1}}],[\"多尺度聚合机制\",{\"1\":{\"37\":1}}],[\"多尺度特征提取机制\",{\"1\":{\"25\":1}}],[\"多尺度分组分类模型\",{\"0\":{\"25\":1}}],[\"多尺度分组\",{\"0\":{\"24\":1},\"1\":{\"23\":1,\"24\":1,\"25\":1}}],[\"多模态因果自监督\",{\"1\":{\"103\":1}}],[\"多模态网络设计\",{\"1\":{\"98\":1}}],[\"多模态模型vit原理与图片分类实战演练\",{\"1\":{\"105\":1}}],[\"多模态模型在过往发展的过程中\",{\"1\":{\"98\":1}}],[\"多模态模型clip原理与图片分类\",{\"1\":{\"87\":1}}],[\"多模态\",{\"0\":{\"86\":1}}],[\"多模态交叉注意力模块\",{\"1\":{\"12\":1}}],[\"多模态数据的\",{\"1\":{\"10\":1}}],[\"多模态嵌入\",{\"1\":{\"10\":2}}],[\"多模态特征投影到语言语义空间\",{\"0\":{\"9\":1},\"1\":{\"7\":1}}],[\"因此是自注意力\",{\"1\":{\"113\":1}}],[\"因此它有效地充当信息瓶颈\",{\"1\":{\"104\":1}}],[\"因此在新的数据集上需要定义新的分类器来重新训练\",{\"1\":{\"96\":1}}],[\"因此在效果上可能不如使用\",{\"1\":{\"92\":1}}],[\"因此成本较高\",{\"1\":{\"96\":1}}],[\"因此这里就不再给出数据集下载链接了\",{\"1\":{\"93\":1}}],[\"因此这是一个非常庞大的数据集\",{\"1\":{\"90\":1}}],[\"因此可以堆叠多个block\",{\"1\":{\"112\":1}}],[\"因此可以充分利用计算资源\",{\"1\":{\"73\":1}}],[\"因此可能比第二个向量更不可靠\",{\"1\":{\"26\":1}}],[\"因此出现了extended\",{\"1\":{\"73\":1}}],[\"因此需要大家自行完成运行时缺失依赖包的安装\",{\"1\":{\"45\":1}}],[\"因此每个质心将根据这些不同的半径值与其周围点形成多个点集群\",{\"1\":{\"24\":1}}],[\"因此\",{\"1\":{\"23\":1,\"96\":2,\"108\":1}}],[\"因为在代码中有个冻结权重的操作\",{\"1\":{\"118\":1}}],[\"因为transformer和cnn相比缺少归纳偏置\",{\"1\":{\"105\":1}}],[\"因为我们要预测下一个\",{\"1\":{\"103\":1}}],[\"因为我只是为了了解内部代码情况\",{\"1\":{\"45\":1}}],[\"因为都是有效\",{\"1\":{\"103\":1}}],[\"因为训练数据集中的文本\",{\"1\":{\"96\":1}}],[\"因为cuda不支持macos\",{\"1\":{\"72\":1}}],[\"因为它能够在更低层次上递归地检视更高分辨率\",{\"1\":{\"26\":1}}],[\"因为前面有\",{\"1\":{\"21\":1}}],[\"因为学习到的特征和权重可以在多个局部区域中复用\",{\"1\":{\"15\":1}}],[\"因为\",{\"1\":{\"15\":1,\"68\":1}}],[\"三个模块\",{\"1\":{\"98\":1}}],[\"三\",{\"1\":{\"37\":1}}],[\"三次sample\",{\"1\":{\"22\":1}}],[\"三层分层特征学习结构\",{\"1\":{\"22\":1}}],[\"三部分组成\",{\"1\":{\"17\":1}}],[\"否则为恒等映射\",{\"1\":{\"114\":1}}],[\"否则存入训练集\",{\"1\":{\"107\":1}}],[\"否则答案可能不合理\",{\"1\":{\"68\":1}}],[\"否则\",{\"1\":{\"37\":1}}],[\"否则只有\",{\"1\":{\"22\":1}}],[\"否则使用恒等映射\",{\"1\":{\"112\":1}}],[\"否则使用\",{\"1\":{\"21\":1,\"109\":2}}],[\"否则使用给定的\",{\"1\":{\"10\":1}}],[\"zero\",{\"1\":{\"46\":2}}],[\"zeros\",{\"1\":{\"21\":2,\"49\":1,\"62\":1,\"102\":1,\"110\":1,\"111\":2,\"114\":2}}],[\"zhandaohong\",{\"1\":{\"45\":1}}],[\"zip\",{\"1\":{\"45\":2,\"48\":1,\"84\":1,\"107\":1}}],[\"z\",{\"1\":{\"22\":2,\"39\":2}}],[\"ylabel\",{\"1\":{\"107\":1}}],[\"y=v\",{\"1\":{\"107\":1}}],[\"y\",{\"1\":{\"22\":2,\"39\":2,\"68\":1}}],[\"yanx27\",{\"1\":{\"14\":1}}],[\"xlabel\",{\"1\":{\"107\":1}}],[\"x=i\",{\"1\":{\"107\":1}}],[\"xticks\",{\"1\":{\"107\":1}}],[\"xi\",{\"1\":{\"30\":1}}],[\"xn\",{\"1\":{\"30\":3}}],[\"x1\",{\"1\":{\"30\":3}}],[\"x\",{\"1\":{\"22\":12,\"25\":10,\"32\":29,\"34\":32,\"35\":10,\"36\":20,\"39\":2,\"57\":7,\"68\":1,\"76\":2,\"78\":3,\"79\":8,\"80\":4,\"82\":12,\"83\":4,\"84\":7,\"109\":8,\"110\":14,\"111\":15,\"112\":21,\"113\":9,\"114\":19}}],[\"xyz\",{\"1\":{\"21\":60,\"22\":11,\"25\":31}}],[\"上面已经给出了数据集加载以及vit模型核心代码实现了\",{\"1\":{\"118\":1}}],[\"上面代码实现中使用的是可学习位置嵌入\",{\"1\":{\"111\":1}}],[\"上面的花卉分类案例相当于使用图像去匹配最合适的文本描述\",{\"1\":{\"94\":1}}],[\"上图中是每一个patch中各位置的位置编码相似性度量\",{\"1\":{\"111\":1}}],[\"上进行预训练\",{\"1\":{\"96\":1,\"108\":1}}],[\"上下文\",{\"1\":{\"66\":3}}],[\"上略低于\",{\"1\":{\"37\":1}}],[\"上\",{\"1\":{\"37\":1}}],[\"上做最大池化\",{\"1\":{\"21\":1}}],[\"上述即为pointnet++设计中的两个核心挑战\",{\"1\":{\"15\":1}}],[\"质点数量\",{\"1\":{\"21\":1}}],[\"质心\",{\"1\":{\"21\":2}}],[\"选择最大值作为这个图文对的相似度\",{\"1\":{\"101\":1}}],[\"选择与图像特征相似度最高的文本所对应的类别\",{\"1\":{\"91\":1}}],[\"选择了一个包含6300万参数的transformer模型\",{\"1\":{\"90\":1}}],[\"选择𝑁个点\",{\"1\":{\"18\":1}}],[\"选项内容\",{\"1\":{\"70\":1}}],[\"选出\",{\"1\":{\"25\":1}}],[\"选关键点\",{\"1\":{\"21\":1}}],[\"大家可以自行拉取项目完整代码进行学习\",{\"1\":{\"118\":1}}],[\"大家注意区分\",{\"1\":{\"100\":1}}],[\"大于\",{\"1\":{\"112\":1}}],[\"大致上两者结构是相同的\",{\"1\":{\"112\":1}}],[\"大小的图像\",{\"1\":{\"108\":1}}],[\"大小\",{\"1\":{\"108\":1}}],[\"大小不超过\",{\"1\":{\"30\":1}}],[\"大大提高学习效率\",{\"1\":{\"105\":1}}],[\"大幅降低训练成本\",{\"1\":{\"98\":1}}],[\"大放异彩的一年\",{\"1\":{\"88\":1}}],[\"大语言模型\",{\"0\":{\"43\":1}}],[\"大区域\",{\"1\":{\"21\":1}}],[\"大局部区域\",{\"1\":{\"21\":1}}],[\"终输出的\",{\"1\":{\"21\":1}}],[\"颜色\",{\"1\":{\"21\":1,\"39\":1}}],[\"颜色等\",{\"1\":{\"21\":1}}],[\"不需要创建类的实例\",{\"1\":{\"107\":1}}],[\"不使用传统的循环或卷积结构\",{\"1\":{\"74\":1}}],[\"不具备生成能力\",{\"1\":{\"68\":1}}],[\"不计入答案\",{\"1\":{\"68\":1}}],[\"不能\",{\"1\":{\"68\":1}}],[\"不能超出上下文范围\",{\"1\":{\"68\":1}}],[\"不能像\",{\"1\":{\"68\":1}}],[\"不是自回归生成器\",{\"1\":{\"68\":1}}],[\"不足长度用padding填充\",{\"1\":{\"46\":1}}],[\"不包括\",{\"1\":{\"42\":1}}],[\"不改变形状和大小\",{\"1\":{\"42\":1}}],[\"不改变物体形状和内部结构\",{\"1\":{\"41\":1}}],[\"不改变\",{\"1\":{\"41\":1}}],[\"不常用\",{\"1\":{\"40\":1,\"66\":1}}],[\"不够大\",{\"1\":{\"37\":1}}],[\"不够精细\",{\"1\":{\"37\":1}}],[\"不利于高维空间建模\",{\"1\":{\"37\":1}}],[\"不稳定\",{\"1\":{\"32\":1}}],[\"不同特征之间能够进行更多样的组合\",{\"1\":{\"112\":1}}],[\"不同模态数据的提取与融合\",{\"1\":{\"99\":1}}],[\"不同位置\",{\"1\":{\"32\":1}}],[\"不同尺度的查询半径列表\",{\"1\":{\"25\":1}}],[\"不同尺度的特征被串联形成多尺度特征向量\",{\"1\":{\"24\":1}}],[\"不做任何变化\",{\"1\":{\"32\":1}}],[\"不考虑空间邻域关系\",{\"1\":{\"32\":1}}],[\"不灵活等问题\",{\"1\":{\"28\":1}}],[\"不仅考虑从当前分辨率下抽象得到的特征\",{\"1\":{\"26\":1}}],[\"不进行采样\",{\"1\":{\"21\":1}}],[\"不方便进行处理\",{\"1\":{\"5\":1}}],[\"去填充这些空缺\",{\"1\":{\"21\":1}}],[\"我将两者的结构进行对比\",{\"1\":{\"112\":1}}],[\"我们通过自定义一个patchembed类完成上述工作\",{\"1\":{\"109\":1}}],[\"我们通过利用clip模型的多模态能力\",{\"1\":{\"91\":1}}],[\"我们首先获取图片库中所有图片\",{\"1\":{\"94\":1}}],[\"我们首先创建了各类别的文本描述\",{\"1\":{\"91\":1}}],[\"我们只需要在计算出相似度得分矩阵后\",{\"1\":{\"94\":1}}],[\"我们从flower\",{\"1\":{\"93\":1}}],[\"我们从原始输入的\",{\"1\":{\"68\":1}}],[\"我们需要根据上面给出的花卉数据集下载链接\",{\"1\":{\"93\":1}}],[\"我们常常需要衡量文本嵌入和图片嵌入之间的相似度\",{\"1\":{\"93\":1}}],[\"我们可以直接使用类别标签作为文本描述\",{\"1\":{\"92\":1}}],[\"我们还有其他的选择\",{\"1\":{\"92\":1}}],[\"我们使用了\",{\"1\":{\"92\":1}}],[\"我们也可以对得到的余弦相似度计算softmax\",{\"1\":{\"91\":1}}],[\"我们已经探讨了clip模型的运作机制\",{\"1\":{\"91\":1}}],[\"我们主要使用它来预测答案的起始和结束位置\",{\"1\":{\"66\":1}}],[\"我们就用该查询点最近的那个点\",{\"1\":{\"21\":1}}],[\"我的调试文件是run\",{\"1\":{\"45\":1}}],[\"我已经上传到了仓库中\",{\"1\":{\"45\":1}}],[\"我就不上传了\",{\"1\":{\"45\":1}}],[\"我不感兴趣\",{\"1\":{\"21\":1}}],[\"身份证号\",{\"1\":{\"21\":1}}],[\"代理任务通常是辅助进行表征学习\",{\"1\":{\"96\":1}}],[\"代表\",{\"1\":{\"115\":2,\"118\":1}}],[\"代表的是模型的基础\",{\"1\":{\"108\":1}}],[\"代表每个\",{\"1\":{\"21\":1}}],[\"代表原始点云中每个点的\",{\"1\":{\"21\":1}}],[\"代码实现如下\",{\"1\":{\"22\":1}}],[\"代码实现\",{\"0\":{\"21\":1,\"67\":1},\"1\":{\"32\":1}}],[\"代码\",{\"0\":{\"31\":1},\"1\":{\"4\":1,\"97\":1}}],[\"到第\",{\"1\":{\"68\":1}}],[\"到一个标准姿态\",{\"1\":{\"32\":1}}],[\"到\",{\"1\":{\"21\":1}}],[\"构造训练目标\",{\"1\":{\"103\":1}}],[\"构造一个单位矩阵\",{\"1\":{\"33\":1}}],[\"构造一个从\",{\"1\":{\"21\":1}}],[\"构成一个位置序列矩阵\",{\"1\":{\"49\":1}}],[\"构建一个下三角矩阵作为因果掩码矩阵\",{\"1\":{\"103\":1}}],[\"构建匹配标签\",{\"1\":{\"102\":1}}],[\"构建输入图像列表\",{\"1\":{\"102\":1}}],[\"构建输入文本列表\",{\"1\":{\"102\":1}}],[\"构建query和text的padding\",{\"1\":{\"102\":1}}],[\"构建query\",{\"1\":{\"102\":1}}],[\"构建padding\",{\"1\":{\"100\":1}}],[\"构建描述文本并提取特征\",{\"1\":{\"91\":1}}],[\"构建全局特征向量\",{\"1\":{\"40\":1}}],[\"构建点云的层次化表示\",{\"1\":{\"37\":1}}],[\"构建点之间的邻接图\",{\"1\":{\"37\":1}}],[\"构建它们的局部邻域区域\",{\"1\":{\"21\":1}}],[\"构建局部邻域的半径\",{\"1\":{\"21\":2}}],[\"构建\",{\"1\":{\"10\":2,\"103\":1}}],[\"查询点的位置\",{\"1\":{\"21\":1}}],[\"查询点数量\",{\"1\":{\"21\":1}}],[\"查询相比\",{\"1\":{\"19\":1}}],[\"便于广播到整个\",{\"1\":{\"33\":1}}],[\"便于广播\",{\"1\":{\"21\":1}}],[\"便于后续处理\",{\"1\":{\"32\":1}}],[\"便于后续\",{\"1\":{\"12\":1}}],[\"<=\",{\"1\":{\"66\":1}}],[\"<\",{\"1\":{\"21\":1}}],[\"索引保存下来\",{\"1\":{\"21\":1}}],[\"次并与每个点的局部特征拼接\",{\"1\":{\"37\":1}}],[\"次并与每个点的局部特征\",{\"1\":{\"34\":1}}],[\"次\",{\"1\":{\"21\":1,\"36\":1}}],[\"批次索引\",{\"1\":{\"21\":1}}],[\"初始时没有任何语义信息\",{\"1\":{\"110\":1}}],[\"初始时随机选择一个点作为第一个中心点\",{\"1\":{\"21\":1}}],[\"初始文本token\",{\"1\":{\"104\":1}}],[\"初始为\",{\"1\":{\"104\":1}}],[\"初始化\",{\"1\":{\"109\":1,\"110\":1}}],[\"初始化自定义数据集类\",{\"1\":{\"107\":1}}],[\"初始化文本输入\",{\"1\":{\"104\":1}}],[\"初始化query\",{\"1\":{\"100\":1}}],[\"初始化用于存储每个样本拼接后输入和\",{\"1\":{\"10\":1}}],[\"初始假设变换为恒等变换\",{\"1\":{\"32\":1}}],[\"初始设为一个极大值\",{\"1\":{\"21\":1}}],[\"中引入卷积操作\",{\"1\":{\"117\":1}}],[\"中进行进一步的处理\",{\"1\":{\"117\":1}}],[\"中第一个线性层把输入特征投影到一个更高维度的空间后\",{\"1\":{\"112\":1}}],[\"中加载数据时\",{\"1\":{\"107\":1}}],[\"中每个\",{\"1\":{\"101\":1}}],[\"中提取与\",{\"1\":{\"100\":2}}],[\"中提取\",{\"1\":{\"68\":1}}],[\"中提取对应的点\",{\"1\":{\"21\":1}}],[\"中取出对应的\",{\"1\":{\"68\":1}}],[\"中心\",{\"1\":{\"37\":1}}],[\"中\",{\"1\":{\"33\":1,\"46\":1,\"66\":1,\"91\":2,\"98\":2,\"102\":1,\"103\":1,\"108\":1,\"110\":5}}],[\"中用于约束变换矩阵接近正交性的正则化损失函数\",{\"1\":{\"33\":1}}],[\"中用于从点云中选择具有代表性的采样点的一种策略\",{\"1\":{\"21\":1}}],[\"中均能有效提取特征\",{\"1\":{\"24\":1}}],[\"中的一个装饰器\",{\"1\":{\"107\":1}}],[\"中的一个方法\",{\"1\":{\"67\":1}}],[\"中的图像一一对应\",{\"1\":{\"107\":1}}],[\"中的核心操作\",{\"1\":{\"40\":1}}],[\"中的核心模块\",{\"1\":{\"21\":1}}],[\"中的\",{\"1\":{\"25\":1}}],[\"中的全局特征学习\",{\"1\":{\"21\":1}}],[\"中只有一组\",{\"1\":{\"21\":1}}],[\"中找到最大的那个距离对应的点\",{\"1\":{\"21\":2}}],[\"中使用\",{\"1\":{\"12\":1,\"40\":1}}],[\"也就为非线性激活函数提供了更多可以学习的特征组合\",{\"1\":{\"112\":1}}],[\"也就是q\",{\"1\":{\"116\":1}}],[\"也就是经过卷积后拼接得到的特征图\",{\"1\":{\"109\":1}}],[\"也就是卷积核的数量\",{\"1\":{\"109\":1}}],[\"也就是一张图像搭配与之对应的文本描述\",{\"1\":{\"89\":1}}],[\"也就是说\",{\"1\":{\"70\":1}}],[\"也需要保持输入图像尺寸与预训练时一致\",{\"1\":{\"108\":1}}],[\"也是其一大亮点\",{\"1\":{\"91\":1}}],[\"也可以进行随机裁剪\",{\"1\":{\"108\":1}}],[\"也可以采用视觉transformer模型\",{\"1\":{\"90\":1}}],[\"也可能导致错误分类\",{\"1\":{\"37\":1}}],[\"也一并加入\",{\"1\":{\"21\":1,\"25\":1}}],[\"也一并提取\",{\"1\":{\"21\":1}}],[\"也应该以一种方式被处理\",{\"1\":{\"15\":1}}],[\"把相似度矩阵对角线元素置为负无穷大\",{\"1\":{\"102\":1}}],[\"把query\",{\"1\":{\"102\":1}}],[\"把q\",{\"1\":{\"100\":1}}],[\"把数据转换成llm能识别的格式\",{\"1\":{\"99\":1}}],[\"把layernorm放到了前面\",{\"1\":{\"78\":1}}],[\"把这些\",{\"1\":{\"68\":1}}],[\"把这些点的坐标归一化到以质心为中心的局部坐标系下\",{\"1\":{\"25\":1}}],[\"把全局特征复制\",{\"1\":{\"34\":1}}],[\"把它们\",{\"1\":{\"32\":1}}],[\"把它们相对于关键点的位置进行归一化\",{\"1\":{\"21\":1}}],[\"把不同尺度学到的特征拼接在一起\",{\"1\":{\"25\":1}}],[\"把邻域点的数据整理成适合卷积的格式\",{\"1\":{\"21\":1}}],[\"把邻近点的坐标和特征拼接在一起\",{\"1\":{\"21\":1}}],[\"把原始点云\",{\"1\":{\"21\":1}}],[\"把距离超过\",{\"1\":{\"21\":1}}],[\"把刚才找到的邻近点的坐标提取出来\",{\"1\":{\"21\":1}}],[\"把他的大小归一化到一个球中\",{\"1\":{\"15\":1}}],[\"个类别和\",{\"1\":{\"118\":1}}],[\"个可训练参数\",{\"1\":{\"115\":2}}],[\"个选项\",{\"1\":{\"70\":1}}],[\"个词之间\",{\"1\":{\"68\":1}}],[\"个输出\",{\"1\":{\"32\":1}}],[\"个坐标值\",{\"1\":{\"32\":1}}],[\"个关键点作为局部区域中心\",{\"1\":{\"25\":1}}],[\"个关键点对应的全局区域特征向量\",{\"1\":{\"22\":1}}],[\"个关键点对应的局部区域特征向量\",{\"1\":{\"22\":2}}],[\"个关键点的坐标\",{\"1\":{\"22\":3}}],[\"个维度\",{\"1\":{\"21\":1}}],[\"个分布尽可能均匀的采样点索引\",{\"1\":{\"21\":1}}],[\"个点的子集决定\",{\"1\":{\"37\":1}}],[\"个点\",{\"1\":{\"21\":2}}],[\"个具有代表性的点\",{\"1\":{\"21\":1}}],[\"球查询\",{\"1\":{\"21\":2}}],[\"最相关的视觉信息\",{\"1\":{\"100\":2}}],[\"最近的一些工作采用新的模型架构和预训练方法来从文本学习视觉特征\",{\"1\":{\"96\":1}}],[\"最小生成长度\",{\"1\":{\"104\":1}}],[\"最小池化\",{\"1\":{\"40\":1}}],[\"最小距离\",{\"1\":{\"21\":1}}],[\"最大生成长度\",{\"1\":{\"104\":1}}],[\"最大的resnet模型rn50x64需要在592个v100\",{\"1\":{\"90\":1}}],[\"最大池化\",{\"1\":{\"40\":1}}],[\"最大文本长度\",{\"1\":{\"7\":1}}],[\"最显著的点\",{\"1\":{\"37\":1}}],[\"最后交换第\",{\"1\":{\"109\":1}}],[\"最后\",{\"1\":{\"93\":1}}],[\"最后将结果转换为\",{\"1\":{\"93\":1}}],[\"最后将每个点的局部特征与整个点云的全局特征拼接起来\",{\"1\":{\"36\":1}}],[\"最后重新\",{\"1\":{\"70\":1}}],[\"最后一层输出\",{\"1\":{\"34\":1}}],[\"最后通过全连接层完成分类任务\",{\"1\":{\"25\":1}}],[\"最后拼接结果\",{\"1\":{\"25\":1}}],[\"最终答案就是上下文中这两个位置之间的字符串\",{\"1\":{\"68\":1}}],[\"最终每个\",{\"1\":{\"66\":1}}],[\"最终每个采样得到的关键点所在的局部领域\",{\"1\":{\"21\":1}}],[\"最终\",{\"1\":{\"32\":1,\"91\":1,\"110\":2}}],[\"最终输出\",{\"1\":{\"36\":1}}],[\"最终输出每个类别的概率分布\",{\"1\":{\"35\":1}}],[\"最终输出高维特征\",{\"1\":{\"34\":1}}],[\"最终输出与点的顺序无关\",{\"1\":{\"30\":1}}],[\"最终输出就是\",{\"1\":{\"25\":1}}],[\"最终得到\",{\"1\":{\"21\":1}}],[\"最远点\",{\"1\":{\"21\":4}}],[\"最远点采样\",{\"1\":{\"21\":2,\"25\":1}}],[\"最多保留\",{\"1\":{\"21\":1}}],[\"为\",{\"1\":{\"114\":1}}],[\"为特定任务动态构建了一个分类器\",{\"1\":{\"91\":1}}],[\"为什么答案来自\",{\"1\":{\"68\":1}}],[\"为什么需要这个正则化项\",{\"1\":{\"33\":1}}],[\"为当前批次中的每个序列样本生成一个位置序列\",{\"1\":{\"49\":1}}],[\"为后续三维深度学习奠定了基础\",{\"1\":{\"30\":1}}],[\"为后续的处理步骤提供信息\",{\"1\":{\"26\":1}}],[\"为此\",{\"1\":{\"26\":1}}],[\"为每个图像选择一个负样本文本\",{\"1\":{\"102\":1}}],[\"为每个文本选择一个负样本图像\",{\"1\":{\"102\":1}}],[\"为每个类别创建一个描述性的文本\",{\"1\":{\"91\":1}}],[\"为每个尺度构建一个独立的小型\",{\"1\":{\"25\":1}}],[\"为每个关键点构建局部邻域\",{\"1\":{\"21\":1}}],[\"为了充分利用预训练的权重\",{\"1\":{\"108\":1}}],[\"为了训练好q\",{\"1\":{\"100\":1}}],[\"为了训练clip模型\",{\"1\":{\"90\":1}}],[\"为了弥补数据规模上的差距\",{\"1\":{\"96\":1}}],[\"为了实现文字搜索图像的功能\",{\"1\":{\"94\":1}}],[\"为了简单\",{\"1\":{\"78\":1}}],[\"为了提升模型对点云姿态变化的鲁棒性\",{\"1\":{\"33\":1}}],[\"为了解决这一问题\",{\"1\":{\"23\":1}}],[\"为了模仿传统卷积网络中的权重共享机制以提高学习效率和模型的泛化能力\",{\"1\":{\"15\":1}}],[\"为了能够在不同的局部子集上共享权重\",{\"1\":{\"15\":1}}],[\"利用\",{\"1\":{\"100\":1,\"117\":1}}],[\"利用已有的vit\",{\"1\":{\"98\":1}}],[\"利用对称函数\",{\"1\":{\"28\":1}}],[\"利用相对坐标与点特征相结合的方式可以捕获局部区域中点与点之间的关系\",{\"1\":{\"20\":1}}],[\"利用上一步得到的中心点将点集划分成若干个区域\",{\"1\":{\"16\":1}}],[\"实例化验证数据集\",{\"1\":{\"108\":1}}],[\"实例化训练数据集\",{\"1\":{\"108\":1}}],[\"实验结果如下表所示\",{\"1\":{\"111\":1}}],[\"实验采用的是花蕊数据集\",{\"1\":{\"107\":1}}],[\"实验观察\",{\"1\":{\"37\":1}}],[\"实验验证\",{\"1\":{\"30\":1,\"37\":1}}],[\"实验证明\",{\"1\":{\"30\":1}}],[\"实际上\",{\"1\":{\"96\":1}}],[\"实际上是用一个固定大小的全局特征去\",{\"1\":{\"37\":1}}],[\"实际上没有局部的概念\",{\"1\":{\"15\":1}}],[\"实际采集的点云常有遮挡\",{\"1\":{\"29\":1}}],[\"实现zero\",{\"1\":{\"91\":1,\"99\":1}}],[\"实现变换不变性\",{\"1\":{\"30\":1}}],[\"实现对称性\",{\"1\":{\"30\":1}}],[\"实现点集顺序不变性\",{\"1\":{\"28\":1}}],[\"实现方法\",{\"1\":{\"20\":1}}],[\"局部建模能力弱\",{\"1\":{\"37\":1}}],[\"局部特征实现上下文感知\",{\"1\":{\"37\":1}}],[\"局部特征编码\",{\"1\":{\"21\":1}}],[\"局部特征学习器\",{\"1\":{\"15\":1}}],[\"局部区域\",{\"1\":{\"21\":1}}],[\"局部区域中的每个点将相对于形心所在位置进行调整\",{\"1\":{\"20\":1}}],[\"局部区域中的点转换成相对于形心的局部坐标系\",{\"1\":{\"20\":1}}],[\"局部坐标系转换\",{\"1\":{\"20\":1}}],[\"并按行优先排序来实现\",{\"1\":{\"119\":1}}],[\"并累加到累计正确样本数中\",{\"1\":{\"114\":1}}],[\"并取得不错的成效\",{\"1\":{\"105\":1}}],[\"并获得该批次图像列表对应的图像嵌入向量列表\",{\"1\":{\"93\":1}}],[\"并改造为a\",{\"1\":{\"93\":1}}],[\"并且取得了与卷积神经网络\",{\"1\":{\"119\":1}}],[\"并且与clip模型的训练数据不完全一致\",{\"1\":{\"92\":1}}],[\"并且在训练过程中采用了一个相对较大的批次大小\",{\"1\":{\"90\":1}}],[\"并计算与文本特征的余弦相似度\",{\"1\":{\"91\":1}}],[\"并将这些图像块嵌入到一个低维向量空间中\",{\"1\":{\"109\":1}}],[\"并将图片展示出来\",{\"1\":{\"94\":1}}],[\"并将其标记为vit\",{\"1\":{\"90\":1}}],[\"并将它们组合成一个批次进行处理\",{\"1\":{\"70\":1}}],[\"并将它们组织称为局部区域集\",{\"1\":{\"19\":1}}],[\"并进行l2归一化\",{\"1\":{\"90\":1}}],[\"并用\",{\"1\":{\"68\":1}}],[\"并重新命名为\",{\"1\":{\"45\":1}}],[\"并行优化\",{\"1\":{\"37\":1}}],[\"并不能保证这些矩阵是正交矩阵\",{\"1\":{\"33\":1}}],[\"并结合正则化损失\",{\"1\":{\"32\":1}}],[\"并通过模型进行前向传播\",{\"1\":{\"114\":1}}],[\"并通过线性变换映射到嵌入空间\",{\"1\":{\"110\":1}}],[\"并通过局部+全局特征融合机制实现强大的点云建模能力\",{\"1\":{\"30\":1}}],[\"并通过一个小型\",{\"1\":{\"21\":1}}],[\"并最终输出分类结果\",{\"1\":{\"22\":1}}],[\"并生成\",{\"1\":{\"7\":1}}],[\"规模一致性\",{\"1\":{\"19\":1}}],[\"距离直观性\",{\"1\":{\"19\":1}}],[\"距离的度量不受空间中位置的影响\",{\"1\":{\"19\":1}}],[\"空间中的位置\",{\"1\":{\"39\":1}}],[\"空间变换网络\",{\"1\":{\"28\":1,\"30\":1}}],[\"空间是均匀和各向同性的\",{\"1\":{\"19\":1}}],[\"空间均匀性\",{\"1\":{\"19\":1}}],[\"空间特征\",{\"1\":{\"12\":1}}],[\"两点注意\",{\"1\":{\"107\":1}}],[\"两点的距离反映了这两点的实际相似度或关联度\",{\"1\":{\"19\":1}}],[\"两者的训练效率相差3倍\",{\"1\":{\"96\":1}}],[\"两个句子是否为上下句关系\",{\"1\":{\"63\":1}}],[\"两个点之间的直线距离被认为是相似度或连接强度的直观表示\",{\"1\":{\"19\":1}}],[\"两个问题是相关联的\",{\"1\":{\"15\":1}}],[\"欧式距离的均匀性假设\",{\"1\":{\"19\":1}}],[\"半径太小可能无法有效捕获足够的局部详细\",{\"1\":{\"19\":1}}],[\"通道维度特征提取阶段\",{\"1\":{\"32\":1}}],[\"通常在大规模数据集\",{\"1\":{\"108\":1}}],[\"通常是随机初始化或者初始化为零\",{\"1\":{\"111\":1}}],[\"通常是\",{\"1\":{\"67\":1,\"115\":1}}],[\"通常是旋转或反射\",{\"1\":{\"32\":1}}],[\"通常会设置一个上限k\",{\"1\":{\"19\":1}}],[\"通过将图像切成小片\",{\"1\":{\"119\":1}}],[\"通过将特征映射到特定的维度并进行非线性变换\",{\"1\":{\"114\":1}}],[\"通过将局部区域中的每个点\",{\"1\":{\"20\":1}}],[\"通过encoder\",{\"1\":{\"114\":1}}],[\"通过投影层对合并后的张量进行线性变换\",{\"1\":{\"113\":1}}],[\"通过qkv线性层将输入x映射到dim\",{\"1\":{\"113\":1}}],[\"通过第二个全连接层\",{\"1\":{\"112\":1}}],[\"通过第一个全连接层\",{\"1\":{\"112\":1}}],[\"通过激活函数层\",{\"1\":{\"112\":1}}],[\"通过分类头\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"通过与其他\",{\"1\":{\"110\":1}}],[\"通过计算损失\",{\"1\":{\"110\":1}}],[\"通过自注意力机制与其他\",{\"1\":{\"110\":1}}],[\"通过随机裁剪可以增加训练数据的多样性\",{\"1\":{\"108\":1}}],[\"通过随机丢弃来模拟不同密度的采样\",{\"1\":{\"25\":1}}],[\"通过视觉编码器\",{\"1\":{\"104\":1}}],[\"通过学习image\",{\"1\":{\"102\":1}}],[\"通过一些自动化的手段将web\",{\"1\":{\"96\":1}}],[\"通过模型获取图片的特征嵌入\",{\"1\":{\"93\":1}}],[\"通过这种方式\",{\"1\":{\"91\":1}}],[\"通过softmax函数转换后\",{\"1\":{\"91\":1}}],[\"通过填充掩码\",{\"1\":{\"74\":1}}],[\"通过调用\",{\"1\":{\"66\":1}}],[\"通过\",{\"1\":{\"30\":1,\"32\":1,\"103\":2}}],[\"通过pointnet获取每个形心多尺度信息\",{\"1\":{\"25\":1}}],[\"通过多个局部区域球查询提取不同尺度的局部特征\",{\"1\":{\"25\":1}}],[\"通过多层\",{\"1\":{\"22\":1,\"110\":1}}],[\"通过较大的邻域尺度避免过度稀疏的问题\",{\"1\":{\"24\":1}}],[\"通过较小的邻域尺度捕获细节\",{\"1\":{\"24\":1}}],[\"通过全连接层进行分类\",{\"1\":{\"22\":1}}],[\"通过查找形心点周围的\",{\"1\":{\"17\":1}}],[\"通过局部特征学习器\",{\"1\":{\"15\":1}}],[\"通过适配器层将\",{\"1\":{\"7\":1}}],[\"提前做好的假设\",{\"1\":{\"105\":1}}],[\"提示\",{\"1\":{\"92\":1}}],[\"提升模型鲁棒性\",{\"1\":{\"32\":2}}],[\"提升了模型的泛化能力和稳定性\",{\"1\":{\"30\":1}}],[\"提高训练的稳定性\",{\"1\":{\"108\":1}}],[\"提高模型的泛化能力\",{\"1\":{\"108\":1}}],[\"提高模型的泛化性能\",{\"1\":{\"25\":1}}],[\"提高算法鲁棒性\",{\"1\":{\"25\":1}}],[\"提高了模型的泛化性和稳健性\",{\"1\":{\"25\":1}}],[\"提高了模型在空间上的泛化能力\",{\"1\":{\"19\":1}}],[\"提取输入图像的特征\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"提取语言模型损失\",{\"1\":{\"103\":1}}],[\"提取图像特征\",{\"1\":{\"91\":1,\"104\":1}}],[\"提取图像和点云的特征\",{\"1\":{\"7\":1}}],[\"提取文本特征\",{\"1\":{\"91\":1}}],[\"提取的特征图转换为序列形式\",{\"1\":{\"117\":1}}],[\"提取的图像特征则是分类器的输入数据\",{\"1\":{\"91\":1}}],[\"提取的关键点集合\",{\"1\":{\"37\":1}}],[\"提取全局特征\",{\"1\":{\"35\":2}}],[\"提取更高维的特征\",{\"1\":{\"34\":1}}],[\"提取每一点的特征向量\",{\"1\":{\"32\":1}}],[\"提取特征\",{\"1\":{\"30\":1,\"36\":1,\"37\":2}}],[\"提取点云的层次化特征\",{\"1\":{\"22\":1}}],[\"提取这些区域的高维特征\",{\"1\":{\"21\":1}}],[\"提取这些局部区域中的点及其特征\",{\"1\":{\"21\":1}}],[\"提取\",{\"1\":{\"7\":2}}],[\"相媲美的效果\",{\"1\":{\"119\":1}}],[\"相似度如下所示\",{\"1\":{\"91\":1}}],[\"相比之下\",{\"1\":{\"37\":1,\"96\":1}}],[\"相比于随机采样\",{\"1\":{\"18\":1}}],[\"相当于直接输入一篇五万字的文章\",{\"1\":{\"109\":1}}],[\"相当于告诉模型\",{\"1\":{\"36\":1}}],[\"相当于加了一个\",{\"1\":{\"32\":1}}],[\"相反\",{\"1\":{\"19\":1,\"26\":1}}],[\"但默认的组合方式可能不满足所有需求\",{\"1\":{\"107\":1}}],[\"但发现这种方法的训练效率\",{\"1\":{\"96\":1}}],[\"但存在一定的噪声\",{\"1\":{\"96\":1}}],[\"但实际上\",{\"1\":{\"92\":1}}],[\"但其主要目的是训练可迁移的视觉模型\",{\"1\":{\"90\":1}}],[\"但可用于特定任务\",{\"1\":{\"40\":1}}],[\"但它也为后续模型奠定了基础\",{\"1\":{\"37\":1}}],[\"但它是为了统一接口设计的一个占位符\",{\"1\":{\"21\":1}}],[\"但如果训练时没有加入扰动\",{\"1\":{\"37\":1}}],[\"但这些模型仍然采用固定类别的softmax分类器进行预训练\",{\"1\":{\"96\":1}}],[\"但这种方式表达能力有限\",{\"1\":{\"37\":1}}],[\"但这可能导致所选邻域的实际尺寸随点的密度变化而变化\",{\"1\":{\"19\":1}}],[\"但遇到遮挡严重或点分布不均匀时性能下降明显\",{\"1\":{\"37\":1}}],[\"但在处理超大规模点云时\",{\"1\":{\"37\":1}}],[\"但在精度上仍略逊一筹\",{\"1\":{\"37\":1}}],[\"但在一些复杂区域\",{\"1\":{\"37\":1}}],[\"但在计算上可能非常昂贵\",{\"1\":{\"26\":1}}],[\"但由于它们是神经网络直接预测出来的\",{\"1\":{\"33\":1}}],[\"但由于其无序性和非规则性\",{\"1\":{\"28\":1}}],[\"但是训练速度还是挺快的\",{\"1\":{\"118\":1}}],[\"但是当数据量逐渐增大时\",{\"1\":{\"115\":1}}],[\"但是当训练数据集不够大的时候\",{\"1\":{\"105\":1}}],[\"但是迁移到其它数据集训练时\",{\"1\":{\"114\":1}}],[\"但是和transformer原始的encoder还是有所区别\",{\"1\":{\"112\":1}}],[\"但是和rnn相比\",{\"1\":{\"73\":1}}],[\"但是实际的代码实现中\",{\"1\":{\"109\":1}}],[\"但是这样造成的一个后果是计算量太庞大\",{\"1\":{\"109\":1}}],[\"但是对于vit这个结构而言\",{\"1\":{\"108\":1}}],[\"但是尺度不同\",{\"1\":{\"25\":1}}],[\"但是在一个场景中有多个物体时则不好办\",{\"1\":{\"15\":1}}],[\"k矩阵乘积\",{\"1\":{\"116\":1}}],[\"kwargs\",{\"1\":{\"104\":2}}],[\"k=int\",{\"1\":{\"107\":1}}],[\"k=2\",{\"1\":{\"35\":1}}],[\"k=64\",{\"1\":{\"34\":1}}],[\"k×k\",{\"1\":{\"33\":1}}],[\"keepdims=true\",{\"1\":{\"93\":1,\"95\":1}}],[\"keepdim=true\",{\"1\":{\"32\":1,\"34\":1,\"91\":2}}],[\"keep\",{\"1\":{\"69\":1}}],[\"kernel\",{\"1\":{\"32\":1,\"109\":2}}],[\"key来自query\",{\"1\":{\"103\":1}}],[\"key\",{\"1\":{\"12\":19,\"57\":7,\"84\":7,\"102\":3,\"103\":44,\"107\":2}}],[\"k\",{\"1\":{\"25\":3,\"30\":1,\"35\":1,\"36\":11,\"37\":5,\"57\":2,\"84\":9,\"100\":1,\"107\":2,\"113\":7}}],[\"knn查询寻找最近的k个邻居\",{\"1\":{\"19\":1}}],[\"knn\",{\"1\":{\"19\":1}}],[\"与之类似的还有\",{\"1\":{\"115\":1}}],[\"与其他\",{\"1\":{\"110\":1}}],[\"与\",{\"1\":{\"101\":2,\"102\":1,\"107\":1}}],[\"与计算机视觉\",{\"1\":{\"89\":1}}],[\"与此同时\",{\"1\":{\"88\":1}}],[\"与原始点\",{\"1\":{\"21\":1}}],[\"与k最近邻\",{\"1\":{\"19\":1}}],[\"与多模态嵌入\",{\"1\":{\"10\":1}}],[\"该模型是在\",{\"1\":{\"118\":1}}],[\"该模型联合训练一个cnn和文本transformer来预测图像的文本描述\",{\"1\":{\"96\":1}}],[\"该模块可以为后续的分类任务提供更具区分性和稳定性的特征表示\",{\"1\":{\"114\":1}}],[\"该类的作用是将二维图像分割成多个图像块\",{\"1\":{\"109\":1}}],[\"该方法正是clip在vlp领域发扬光大的\",{\"1\":{\"101\":1}}],[\"该方法能更好的覆盖整个点集\",{\"1\":{\"18\":1}}],[\"该函数作用是针对给定的图片路径\",{\"1\":{\"93\":1}}],[\"该示例中的任务涉及8个类别\",{\"1\":{\"91\":1}}],[\"该范围确保局部区域的尺度是固定的\",{\"1\":{\"19\":1}}],[\"低密度区域则过于稀缺\",{\"1\":{\"18\":1}}],[\"样本分布偏差\",{\"1\":{\"18\":1}}],[\"作者又探索了一种混合模型\",{\"1\":{\"117\":1}}],[\"作者使用了谷歌制作的jft\",{\"1\":{\"115\":1}}],[\"作者将vit和之前图像分类领域比较强的resnet模型进行了对比测试\",{\"1\":{\"115\":1}}],[\"作者先是在imagenet21k上进行预训练\",{\"1\":{\"114\":1}}],[\"作者随后也对一维位置编码的结果进行了可视化\",{\"1\":{\"111\":1}}],[\"作者最终选择了对比学习方法来进行训练\",{\"1\":{\"96\":1}}],[\"作者通过fps来抽样点集中较为重要的点\",{\"1\":{\"18\":1}}],[\"作为编码器\",{\"1\":{\"117\":1}}],[\"作为特征提取器\",{\"1\":{\"117\":1}}],[\"作为文本解码器的初始状态\",{\"1\":{\"103\":1}}],[\"作为图像的分类预测结果\",{\"1\":{\"91\":1}}],[\"作为答案开始的可能性\",{\"1\":{\"66\":1}}],[\"作为整个点云的\",{\"1\":{\"32\":1,\"34\":1}}],[\"作为对称函数\",{\"1\":{\"30\":1}}],[\"作为第二个\",{\"1\":{\"12\":1}}],[\"作为第一个\",{\"1\":{\"12\":1}}],[\"作为\",{\"1\":{\"12\":1,\"100\":1}}],[\"邻近点\",{\"1\":{\"17\":1}}],[\"从预测结果中找出每个样本预测概率最大的类别索引\",{\"1\":{\"114\":1}}],[\"从qkv张量中分离出查询\",{\"1\":{\"113\":1}}],[\"从图像的中心位置裁剪出\",{\"1\":{\"108\":1}}],[\"从数据集\",{\"1\":{\"107\":1}}],[\"从数据对的数量来看\",{\"1\":{\"90\":1}}],[\"从第二个开始\",{\"1\":{\"103\":1}}],[\"从冻结的llm引到vision\",{\"1\":{\"99\":1}}],[\"从冻结的image\",{\"1\":{\"99\":1}}],[\"从任务难度来看\",{\"1\":{\"96\":1}}],[\"从候选分类文本集合中取出其分类名词\",{\"1\":{\"93\":1}}],[\"从多个角度渲染点云或\",{\"1\":{\"39\":1}}],[\"从所有点中选出每个通道的最大响应值\",{\"1\":{\"32\":1,\"34\":1}}],[\"从而提高模型的分类性能\",{\"1\":{\"114\":1}}],[\"从而增强了模型的表达能力\",{\"1\":{\"112\":1}}],[\"从而增强模型的表达能力\",{\"1\":{\"74\":1}}],[\"从而完成图像分类任务\",{\"1\":{\"110\":1}}],[\"从而缓解了灾难性的遗忘问题\",{\"1\":{\"104\":1}}],[\"从而导致更大的训练代价\",{\"1\":{\"98\":1}}],[\"从而可以解决长距离依赖的问题\",{\"1\":{\"73\":1}}],[\"从而很难并行\",{\"1\":{\"73\":1}}],[\"从而能够更好地捕捉点云的局部结构和层次信息\",{\"1\":{\"37\":1}}],[\"从而保证变换是刚性的\",{\"1\":{\"32\":1}}],[\"从而训练网络在面对实际应用中可能遇到的各种采样密度时\",{\"1\":{\"25\":1}}],[\"从而学会根据输入数据的变化自适应地加权不同尺度上检测到的模式\",{\"1\":{\"24\":1}}],[\"从而实现点云的分层特征学习\",{\"1\":{\"21\":1}}],[\"从点云中根据索引提取特定点\",{\"1\":{\"21\":1}}],[\"从\",{\"1\":{\"21\":1,\"32\":1,\"68\":1,\"100\":2}}],[\"从输入点云\",{\"1\":{\"21\":1}}],[\"从输入点中选取一组点\",{\"1\":{\"17\":1}}],[\"从原始点云中选出\",{\"1\":{\"21\":1}}],[\"从本质上来说\",{\"1\":{\"15\":1}}],[\"由此可见vit工作的局限性\",{\"1\":{\"115\":1}}],[\"由此把图片转换为序列的embedding形式\",{\"1\":{\"109\":1}}],[\"由两个transformer模块组成\",{\"1\":{\"100\":1}}],[\"由三角形面片组成的\",{\"1\":{\"39\":1}}],[\"由于作者是首次将transformer应用到图像领域\",{\"1\":{\"116\":1}}],[\"由于训练数据量和模型计算量较大\",{\"1\":{\"96\":1}}],[\"由于它们在预训练数据集上采用固定类别数的分类器\",{\"1\":{\"96\":1}}],[\"由于这些文本往往只是一个单词\",{\"1\":{\"92\":1}}],[\"由于数据量巨大\",{\"1\":{\"90\":1}}],[\"由于\",{\"1\":{\"74\":1,\"104\":1}}],[\"由于表情不同\",{\"1\":{\"37\":1}}],[\"由于子区域在计算第一个向量时包含的点更稀疏\",{\"1\":{\"26\":1}}],[\"由于点集在不同区域可能会有不同的采样密度\",{\"1\":{\"23\":1}}],[\"由\",{\"1\":{\"17\":1}}],[\"需要再将第一个添加的class\",{\"1\":{\"114\":1}}],[\"需要单独将这个token再提取出来\",{\"1\":{\"110\":1}}],[\"需要相对少的数据就可以学习一个比较好的模型\",{\"1\":{\"105\":1}}],[\"需要复制图像特征以匹配beam数量\",{\"1\":{\"104\":1}}],[\"需要数百个gpu训练数十天\",{\"1\":{\"98\":1}}],[\"需要大量的数据标注\",{\"1\":{\"96\":1}}],[\"需要将输入图像调整为这个固定的尺寸\",{\"1\":{\"118\":1}}],[\"需要将\",{\"1\":{\"72\":1}}],[\"需要注意的一点是\",{\"1\":{\"45\":1}}],[\"需要捕捉局部结构\",{\"1\":{\"29\":1}}],[\"需要确保这些划分具有一定的一致性或共同结构\",{\"1\":{\"15\":1}}],[\"需要对哪个物体做归一化呢\",{\"1\":{\"15\":1}}],[\"它是\",{\"1\":{\"115\":1}}],[\"它是无序点云数据特征提取的高效算法\",{\"1\":{\"15\":1}}],[\"它必须要在超大数据集上进行预训练\",{\"1\":{\"115\":1}}],[\"它位于最终分类头之前\",{\"1\":{\"114\":1}}],[\"它与类的实例和类本身都没有直接关联\",{\"1\":{\"107\":1}}],[\"它会将多个样本收集起来形成一个批次\",{\"1\":{\"107\":1}}],[\"它首次将nlp领域火热的transformer模型架构移植到了cv领域\",{\"1\":{\"105\":1}}],[\"它将每个\",{\"1\":{\"103\":1}}],[\"它将输入的向量转换为\",{\"1\":{\"93\":1}}],[\"它通过处理器对输入文本进行处理\",{\"1\":{\"93\":1}}],[\"它通过位置编码将序列中词的位置信息注入到输入中\",{\"1\":{\"74\":1}}],[\"它由两个部分组成\",{\"1\":{\"91\":1}}],[\"它由大量带有位置信息的点组成\",{\"1\":{\"39\":1}}],[\"它比谷歌的jft\",{\"1\":{\"90\":1}}],[\"它代表着一种基于对比文本\",{\"1\":{\"89\":1}}],[\"它可以当成函数调用\",{\"1\":{\"78\":1}}],[\"它允许模型在处理每个词时关注输入序列中的所有词\",{\"1\":{\"74\":1}}],[\"它在编码每一词的时候都能够注意\",{\"1\":{\"73\":1}}],[\"它较难学习到长距离的依赖关系\",{\"1\":{\"73\":1}}],[\"它不具有生成能力\",{\"1\":{\"68\":1}}],[\"它不会\",{\"1\":{\"68\":1}}],[\"它不是直接包含原始图像\",{\"1\":{\"8\":1}}],[\"它引入了局部区域搜索\",{\"1\":{\"37\":1}}],[\"它使用\",{\"1\":{\"35\":2}}],[\"它基于前面的特征提取模块\",{\"1\":{\"35\":1,\"36\":1}}],[\"它负责从输入点云中提取出可用于分类或分割的特征\",{\"1\":{\"34\":1}}],[\"它具有以下特点\",{\"1\":{\"32\":1}}],[\"它包含了这个区域内所有点的信息\",{\"1\":{\"21\":1}}],[\"它的作用是给定一个完整的句子\",{\"1\":{\"68\":1}}],[\"它的作用是负责从输入的点云数据中采样关键点\",{\"1\":{\"21\":1}}],[\"它的语法是\",{\"1\":{\"67\":1}}],[\"它的目标是\",{\"1\":{\"32\":1}}],[\"它的核心思想是\",{\"1\":{\"21\":1}}],[\"它能够从每个子集中提取有用的信息或特征\",{\"1\":{\"15\":1}}],[\"一起送入\",{\"1\":{\"104\":1}}],[\"一起送入mini\",{\"1\":{\"20\":1}}],[\"一种思路是在转换之前\",{\"1\":{\"111\":1}}],[\"一种朴素的想法就是把一个个像素点拉平\",{\"1\":{\"109\":1}}],[\"一种是平移不变形\",{\"1\":{\"105\":1}}],[\"一种是局部性\",{\"1\":{\"105\":1}}],[\"一种是常用的cnn架构resnet\",{\"1\":{\"90\":1}}],[\"一种比较好理解的方式\",{\"1\":{\"100\":1}}],[\"一般\",{\"1\":{\"98\":2}}],[\"一句话总结\",{\"1\":{\"37\":1}}],[\"一\",{\"1\":{\"37\":1}}],[\"一组新的关键点位置\",{\"1\":{\"25\":1}}],[\"一组点云被处理和抽象\",{\"1\":{\"17\":1}}],[\"一个block之后维度依然和输入相同\",{\"1\":{\"112\":1}}],[\"一个改进的想法就是把一张图片分成nxn个patch\",{\"1\":{\"109\":1}}],[\"一个批次的数据\",{\"1\":{\"107\":1}}],[\"一个文件夹对应一个类别\",{\"1\":{\"107\":1}}],[\"一个的轻量q\",{\"1\":{\"98\":1}}],[\"一个视觉模型和一个文本模型\",{\"1\":{\"91\":1}}],[\"一个是该\",{\"1\":{\"66\":1}}],[\"一个非法索引\",{\"1\":{\"21\":1}}],[\"一个元组\",{\"1\":{\"21\":1,\"32\":1,\"34\":1}}],[\"一旦点云被划分成小的子集\",{\"1\":{\"15\":1}}],[\"一名普通但十分热爱探索技术的coder\",{\"1\":{\"2\":1}}],[\"这有助于模型发现输入数据中更复杂的模式和关系\",{\"1\":{\"112\":1}}],[\"这也是为什么结构图中mlp\",{\"1\":{\"110\":1}}],[\"这时就可以自定义\",{\"1\":{\"107\":1}}],[\"这几乎是不可能完成的任务\",{\"1\":{\"105\":1}}],[\"这减少了llm学习视觉语言对齐的负担\",{\"1\":{\"104\":1}}],[\"这大大限制了它们的迁移能力和扩展性\",{\"1\":{\"96\":1}}],[\"这远远低于imagenet上的sota\",{\"1\":{\"96\":1}}],[\"这方面的工作并不多\",{\"1\":{\"96\":1}}],[\"这与传统的预训练加微调的方法有所不同\",{\"1\":{\"92\":1}}],[\"这与在传统cnn中学习图像局部区域特征的过程相似\",{\"1\":{\"15\":1}}],[\"这展示了其在图像分类任务中的灵活性和强大能力\",{\"1\":{\"91\":1}}],[\"这不仅展示了clip的强大功能\",{\"1\":{\"91\":1}}],[\"这表明训练clip模型需要消耗大量的资源\",{\"1\":{\"90\":1}}],[\"这和原始论文稍有不同\",{\"1\":{\"78\":1}}],[\"这比较容易并行\",{\"1\":{\"73\":1}}],[\"这其实就是在说\",{\"1\":{\"68\":1}}],[\"这段代码的意思是\",{\"1\":{\"68\":1}}],[\"这类抽取式问答任务中\",{\"1\":{\"68\":1}}],[\"这里不再贴出\",{\"1\":{\"118\":1}}],[\"这里需要将其分离开来\",{\"1\":{\"113\":1}}],[\"这里主要有两种位置编码思路\",{\"1\":{\"111\":1}}],[\"这里简单介绍一下cls\",{\"1\":{\"110\":1}}],[\"这里设置为图像块的大小\",{\"1\":{\"109\":2}}],[\"这里对训练集的处理方式是随机切成224x224像素的图片\",{\"1\":{\"108\":1}}],[\"这里对提取的文本特征和图像特征进行对比学习\",{\"1\":{\"90\":1}}],[\"这里以搜索向日葵花为例\",{\"1\":{\"94\":1}}],[\"这里采用了余弦相似度的计算方法\",{\"1\":{\"93\":1}}],[\"这里共有个正样本\",{\"1\":{\"90\":1}}],[\"这里的原因\",{\"1\":{\"112\":1}}],[\"这里的\",{\"1\":{\"109\":1}}],[\"这里的相似度直接计算文本特征和图像特征的余弦相似性\",{\"1\":{\"90\":1}}],[\"这里的三个\",{\"1\":{\"22\":1}}],[\"这里我准备做一个文本分类任务\",{\"1\":{\"45\":1}}],[\"这就要求网络中的某些关键操作必须是对称函数\",{\"1\":{\"40\":1}}],[\"这就是blip\",{\"1\":{\"98\":1}}],[\"这就是答案\",{\"1\":{\"68\":1}}],[\"这就是\",{\"1\":{\"33\":1}}],[\"这就是下一个\",{\"1\":{\"21\":2}}],[\"这使得它在细粒度识别\",{\"1\":{\"37\":1}}],[\"这意味着它能够在没有任何特定任务训练数据的情况下\",{\"1\":{\"91\":1}}],[\"这意味着\",{\"1\":{\"37\":1}}],[\"这意味着即使是不同的局部子集\",{\"1\":{\"15\":1}}],[\"这两个模型都属于融合图像与文本的多模态模型\",{\"1\":{\"88\":1}}],[\"这两个网络输出的是变换矩阵\",{\"1\":{\"33\":1}}],[\"这两种特征被concat为一个复合特征向量\",{\"1\":{\"26\":1}}],[\"这一步的操作在论文中是直接采用切割的处理办法\",{\"1\":{\"109\":1}}],[\"这一步是为了保证图像的整体比例不变\",{\"1\":{\"108\":1}}],[\"这一步不在\",{\"1\":{\"32\":1}}],[\"这一部分的目标是\",{\"1\":{\"103\":1}}],[\"这一过程与训练时相同\",{\"1\":{\"91\":1}}],[\"这一过程通过对每个子区域应用集合抽象层\",{\"1\":{\"26\":1}}],[\"这种差异一方面是由于文本和图像属于两个完全不同的模态\",{\"1\":{\"96\":1}}],[\"这种预训练通常是基于有监督学习的\",{\"1\":{\"96\":1}}],[\"这种格式\",{\"1\":{\"92\":1}}],[\"这种设计使得网络只关注\",{\"1\":{\"37\":1}}],[\"这种方法实际上与nlp领域的一个研究方向\",{\"1\":{\"92\":1}}],[\"这种方法大大增强了网络处理非均匀采样数据的能力\",{\"1\":{\"25\":1}}],[\"这种方法使得网络能够在细节丰富的区域\",{\"1\":{\"24\":1}}],[\"这种方法使网络能够通过在训练期间随机丢弃输入点\",{\"1\":{\"24\":1}}],[\"这种非均匀性为点集特征学习带来了显著挑战\",{\"1\":{\"23\":1}}],[\"这些值会根据模型的损失函数不断调整\",{\"1\":{\"111\":1}}],[\"这些相似度值可以被视为logits\",{\"1\":{\"91\":1}}],[\"这些图像特征会与之前得到的个文本特征进行余弦相似度计算\",{\"1\":{\"91\":1}}],[\"这些文本随后被输入到文本编码器\",{\"1\":{\"91\":1}}],[\"这些数据在论文中被称为webimagetext\",{\"1\":{\"90\":1}}],[\"这些模型都是以cnn为基础\",{\"1\":{\"73\":1}}],[\"这些点大致构成物体的骨架\",{\"1\":{\"37\":1}}],[\"这些点彼此之间的最小距离尽可能大\",{\"1\":{\"18\":1}}],[\"这些方法会导致信息损失\",{\"1\":{\"28\":1}}],[\"这些人离我太远了\",{\"1\":{\"21\":1}}],[\"这是一种数据增强的方式\",{\"1\":{\"108\":1}}],[\"这是一个椅子\",{\"1\":{\"36\":1}}],[\"这是抽取式问答模型的局限性\",{\"1\":{\"68\":1}}],[\"这是因为在实际采集过程中\",{\"1\":{\"32\":1}}],[\"这是\",{\"1\":{\"21\":1,\"103\":1}}],[\"这个维度上添加一维\",{\"1\":{\"111\":1}}],[\"这个就是额外添加的一个\",{\"1\":{\"110\":1}}],[\"这个token\",{\"1\":{\"110\":1}}],[\"这个\",{\"1\":{\"93\":1}}],[\"这个数据集来源是这里\",{\"1\":{\"45\":1}}],[\"这个变换矩阵是近似正交的\",{\"1\":{\"32\":1}}],[\"这个矩阵表示对点云所做的变换\",{\"1\":{\"32\":1}}],[\"这个模型使用了\",{\"1\":{\"25\":1}}],[\"这个模块实现了\",{\"1\":{\"25\":1}}],[\"这个特征向量代表了这个局部区域的高维特征\",{\"1\":{\"21\":1}}],[\"这个操作被称为\",{\"1\":{\"21\":1}}],[\"这个函数的作用是将输入的文本转化为对应的嵌入表示\",{\"1\":{\"93\":1}}],[\"这个函数的作用是从点云中找出每个查询点周围一定半径范围内的邻近点索引\",{\"1\":{\"21\":1}}],[\"这个函数的作用是从输入点云中\",{\"1\":{\"21\":1}}],[\"这个函数的有一个输入参数\",{\"1\":{\"78\":1}}],[\"这个函数实现的是根据给定的索引\",{\"1\":{\"21\":1}}],[\"这个函数实现的是最远点采样\",{\"1\":{\"21\":1}}],[\"这个过程类似于在传统的卷积神经网络中如何处理图像的小区域\",{\"1\":{\"15\":1}}],[\"这在处理非均匀采样的数据时可能不是最优的选择\",{\"1\":{\"19\":1}}],[\"这样计算量就大大减小了\",{\"1\":{\"109\":1}}],[\"这样就可以完成vit的训练过程\",{\"1\":{\"114\":1}}],[\"这样就可以提取出对最终任务有帮助的特征组合\",{\"1\":{\"112\":1}}],[\"这样就成了一个一维序列\",{\"1\":{\"109\":1}}],[\"这样就能让\",{\"1\":{\"70\":1,\"112\":1}}],[\"这样可以为\",{\"1\":{\"117\":1}}],[\"这样可以在模型的不同阶段交替利用\",{\"1\":{\"117\":1}}],[\"这样可以在每个部分上独立地学习特征\",{\"1\":{\"15\":1}}],[\"这样可以提升计算效率\",{\"1\":{\"113\":1}}],[\"这样可以保证模型的特征提取能力和性能\",{\"1\":{\"108\":1}}],[\"这样的格式来生成文本描述\",{\"1\":{\"92\":1}}],[\"这样的\",{\"1\":{\"68\":1}}],[\"这样模型就能根据上下文更准确地做出判断\",{\"1\":{\"36\":1}}],[\"这样每个点在预测标签时都能看到整个物体的上下文\",{\"1\":{\"30\":1}}],[\"这样\",{\"1\":{\"24\":2,\"32\":1,\"109\":1}}],[\"这样做的目的是提高模型的效率和泛化能力\",{\"1\":{\"15\":1}}],[\"这需要一个\",{\"1\":{\"15\":1}}],[\"以使得模型能够学习到最适合当前任务的位置表示\",{\"1\":{\"111\":1}}],[\"以最小化损失\",{\"1\":{\"110\":1}}],[\"以最大化互信息\",{\"1\":{\"101\":1}}],[\"以保持与其他模型的一致性\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"以保持计算的可管理性\",{\"1\":{\"19\":1}}],[\"以保留图像的空间信息\",{\"1\":{\"110\":1}}],[\"以vit\",{\"1\":{\"109\":1}}],[\"以\",{\"1\":{\"108\":1}}],[\"以bos\",{\"1\":{\"104\":1}}],[\"以利用\",{\"1\":{\"104\":1}}],[\"以避免模型将匹配图文对挑选为负样本\",{\"1\":{\"102\":1}}],[\"以细粒度对齐\",{\"1\":{\"102\":1}}],[\"以下引用clip论文图做说明\",{\"1\":{\"101\":1}}],[\"以下是一个官方给出的clip模型的示例\",{\"1\":{\"91\":1}}],[\"以及seq\",{\"1\":{\"100\":1}}],[\"以及如何通过局部特征学习器\",{\"1\":{\"15\":1}}],[\"以上代码注释中统一用b代替image\",{\"1\":{\"100\":1}}],[\"以弥合模态差距\",{\"1\":{\"99\":1}}],[\"以每个文本描述为一行\",{\"1\":{\"94\":1}}],[\"以获取图像特征\",{\"1\":{\"91\":1}}],[\"以获得更强的局部几何感知能力\",{\"1\":{\"25\":1}}],[\"以生成相应的文本特征\",{\"1\":{\"91\":1}}],[\"以增强性能\",{\"1\":{\"90\":1}}],[\"以加速训练并提高模型的稳定性\",{\"1\":{\"74\":1}}],[\"以便\",{\"1\":{\"100\":1}}],[\"以便一起处理\",{\"1\":{\"70\":1}}],[\"以便可以在这些分区上独立地学习特征\",{\"1\":{\"15\":1}}],[\"以便可以在这些区域上应用局部操作\",{\"1\":{\"15\":1}}],[\"以实现最佳的性能\",{\"1\":{\"25\":1}}],[\"以结合来自不同尺度的特征\",{\"1\":{\"24\":1}}],[\"以反映其相对位置\",{\"1\":{\"20\":1}}],[\"以限制每个局部区域中考虑的点的数量\",{\"1\":{\"19\":1}}],[\"以产生一个更少元素的新集合\",{\"1\":{\"17\":1}}],[\"以二维欧几里得空间为例\",{\"1\":{\"16\":1}}],[\"或者\",{\"1\":{\"103\":1}}],[\"或者说在于计算能力和数据集的规模\",{\"1\":{\"96\":1}}],[\"或者直接命令行运行\",{\"1\":{\"45\":1}}],[\"或两个句子\",{\"1\":{\"68\":1}}],[\"或图结构增强局部建模能力\",{\"1\":{\"37\":1}}],[\"或图像视图\",{\"1\":{\"28\":1}}],[\"或每组邻域大小\",{\"1\":{\"24\":1}}],[\"或局部区域\",{\"1\":{\"15\":1}}],[\"或\",{\"1\":{\"15\":1}}],[\"点的数量\",{\"1\":{\"36\":1}}],[\"点的特征数据\",{\"1\":{\"21\":1}}],[\"点之间的相互作用\",{\"1\":{\"30\":1}}],[\"点之间存在相互作用\",{\"1\":{\"29\":1}}],[\"点与点之间有空间关系\",{\"1\":{\"29\":1}}],[\"点额外特征\",{\"1\":{\"21\":1}}],[\"点坐标\",{\"1\":{\"21\":1}}],[\"点集抽象层\",{\"1\":{\"21\":1}}],[\"点集的划分必须产生跨分区的共同结构\",{\"1\":{\"15\":1}}],[\"点集划分是指如何将一个大的点云分割成更小的\",{\"1\":{\"15\":1}}],[\"点云中点的顺序不影响整体形状\",{\"1\":{\"40\":1}}],[\"点云中相关点\",{\"1\":{\"8\":1}}],[\"点云\",{\"0\":{\"39\":1},\"1\":{\"39\":2}}],[\"点云模型\",{\"1\":{\"37\":1}}],[\"点云是点的集合\",{\"1\":{\"29\":1}}],[\"点云是三维几何数据的一种重要表示形式\",{\"1\":{\"28\":1}}],[\"点云的姿态可能各不相同\",{\"1\":{\"32\":1}}],[\"点云的无序性\",{\"1\":{\"29\":1,\"30\":1}}],[\"点云的额外特征\",{\"1\":{\"21\":1}}],[\"点云坐标数据\",{\"1\":{\"21\":2}}],[\"点云等\",{\"1\":{\"10\":1}}],[\"点云编码器\",{\"1\":{\"7\":1}}],[\"点云数据\",{\"1\":{\"7\":1}}],[\"解包数据\",{\"1\":{\"114\":1}}],[\"解码生成的token\",{\"1\":{\"104\":1}}],[\"解码器进行文本生成\",{\"1\":{\"103\":1}}],[\"解码器生成文本描述\",{\"1\":{\"103\":1}}],[\"解码器层\",{\"1\":{\"82\":1}}],[\"解码器的最终输出通过一个线性层和\",{\"1\":{\"74\":1}}],[\"解码器\",{\"1\":{\"74\":1,\"83\":1}}],[\"解码器融合所有特征以预测可操作性特征\",{\"0\":{\"12\":1},\"1\":{\"7\":1}}],[\"解码成自然语言\",{\"1\":{\"68\":1}}],[\"解码成自然语言文本\",{\"1\":{\"68\":1}}],[\"解决点云姿态不一致问题\",{\"1\":{\"32\":1}}],[\"解决了点云处理中的四大技术难点\",{\"1\":{\"30\":1}}],[\"解决了两个问题\",{\"1\":{\"15\":1}}],[\"解决方案\",{\"0\":{\"30\":1}}],[\"全1\",{\"1\":{\"104\":1}}],[\"全连接再加上一个softmax\",{\"1\":{\"76\":1}}],[\"全连接层预测变换矩阵\",{\"1\":{\"32\":1}}],[\"全局信息\",{\"1\":{\"36\":1}}],[\"全局信息融合机制\",{\"1\":{\"30\":1}}],[\"全局特征表示\",{\"1\":{\"110\":1}}],[\"全局特征不能很好地反映每个点的上下文\",{\"1\":{\"37\":1}}],[\"全局特征不能直接用于分割\",{\"1\":{\"36\":1}}],[\"全局特征只有一份\",{\"1\":{\"36\":1}}],[\"全局特征\",{\"1\":{\"34\":1}}],[\"全局特征开关\",{\"1\":{\"34\":1}}],[\"全局特征都不一样了\",{\"1\":{\"15\":1}}],[\"全局最大池化\",{\"1\":{\"32\":1}}],[\"全局质心点\",{\"1\":{\"21\":1}}],[\"全为\",{\"1\":{\"10\":1,\"103\":1}}],[\"世界坐标系和局部坐标系\",{\"1\":{\"15\":1}}],[\"背景知识扫盲\",{\"0\":{\"38\":1}}],[\"背景\",{\"0\":{\"15\":1,\"73\":1,\"98\":1}}],[\"第二个全连接层\",{\"1\":{\"112\":1}}],[\"第二个归一化层\",{\"1\":{\"112\":1}}],[\"第二个线性层再把高维特征映射回原来的维度\",{\"1\":{\"112\":1}}],[\"第二个参数\",{\"1\":{\"108\":1}}],[\"第二个\",{\"1\":{\"12\":1}}],[\"第二分支\",{\"1\":{\"12\":1}}],[\"第一阶段设计了三个训练目标\",{\"1\":{\"100\":1}}],[\"第一层是一个叫做\",{\"1\":{\"32\":1}}],[\"第一个全连接层\",{\"1\":{\"112\":1}}],[\"第一个归一化层\",{\"1\":{\"112\":1}}],[\"第一个线性层将输入特征映射到更高维度的空间\",{\"1\":{\"112\":1}}],[\"第一个参数\",{\"1\":{\"108\":1}}],[\"第一个向量提供了更细致的信息\",{\"1\":{\"26\":1}}],[\"第一个点\",{\"1\":{\"21\":1}}],[\"第一个\",{\"1\":{\"12\":1}}],[\"第一分支\",{\"1\":{\"12\":1}}],[\"来帮助大家梳理清楚vision\",{\"1\":{\"106\":1}}],[\"来自论文的理论分析\",{\"1\":{\"37\":1}}],[\"来自下一级的特征\",{\"1\":{\"26\":1}}],[\"来完成点云物体分割任务\",{\"1\":{\"36\":1}}],[\"来完成点云分类任务\",{\"1\":{\"35\":1}}],[\"来模拟不同的采样密度\",{\"1\":{\"25\":1}}],[\"来学习优化的策略\",{\"1\":{\"24\":1}}],[\"来构建局部区域点集\",{\"1\":{\"17\":1}}],[\"来源之二\",{\"1\":{\"12\":1}}],[\"来源之一\",{\"1\":{\"12\":1}}],[\"来源\",{\"1\":{\"12\":1}}],[\"来标记哪些多模态\",{\"1\":{\"10\":1}}],[\"→\",{\"1\":{\"12\":3,\"30\":4,\"32\":4}}],[\"执行交叉注意力机制\",{\"1\":{\"12\":1}}],[\"投影层的丢弃率\",{\"1\":{\"113\":1}}],[\"投影\",{\"1\":{\"12\":5}}],[\"投影维度\",{\"1\":{\"12\":1}}],[\"例如在多头自注意力机制或前馈网络中引入卷积层\",{\"1\":{\"117\":1}}],[\"例如参数量为\",{\"1\":{\"115\":1}}],[\"例如clip\",{\"1\":{\"98\":1}}],[\"例如谷歌的bit和vit基于jft\",{\"1\":{\"96\":1}}],[\"例如2017年的那篇工作只在imagenet上实现了11\",{\"1\":{\"96\":1}}],[\"例如virtex基于transformer的语言模型\",{\"1\":{\"96\":1}}],[\"例如openai的gpt\",{\"1\":{\"96\":1}}],[\"例如\",{\"1\":{\"12\":1,\"66\":1,\"91\":1,\"92\":1,\"96\":1,\"105\":1,\"117\":2}}],[\"调整输出格式为\",{\"1\":{\"12\":1}}],[\"调整\",{\"1\":{\"12\":1}}],[\"调用q\",{\"1\":{\"104\":1}}],[\"调用\",{\"1\":{\"7\":1,\"36\":1,\"103\":2}}],[\"都是197\",{\"1\":{\"112\":1}}],[\"都是有效的\",{\"1\":{\"10\":1}}],[\"都需要进行有监督微调\",{\"1\":{\"96\":1}}],[\"都使用了残差连接和层归一化\",{\"1\":{\"74\":1}}],[\"都单独构造一个\",{\"1\":{\"70\":1}}],[\"都等于max\",{\"1\":{\"46\":1}}],[\"都有一个对应的\",{\"1\":{\"66\":1}}],[\"都有\",{\"1\":{\"40\":1}}],[\"都有效\",{\"1\":{\"10\":1}}],[\"都尝试引入更复杂的结构来提升建模能力\",{\"1\":{\"37\":1}}],[\"都会被压缩为一个固定长度的特征向量\",{\"1\":{\"21\":1}}],[\"则创建线性分类头\",{\"1\":{\"114\":1}}],[\"则使用\",{\"1\":{\"114\":1}}],[\"则使用默认的\",{\"1\":{\"114\":1}}],[\"则使用默认的缩放因子\",{\"1\":{\"113\":1}}],[\"则使用该层进行归一化\",{\"1\":{\"109\":2}}],[\"则默认为\",{\"1\":{\"112\":2}}],[\"则默认多模态\",{\"1\":{\"10\":1}}],[\"则将其转换为\",{\"1\":{\"109\":1}}],[\"则将其设置为\",{\"1\":{\"67\":2}}],[\"则表示图像块是正方形\",{\"1\":{\"109\":1}}],[\"则表示图像是正方形\",{\"1\":{\"109\":1}}],[\"则对图像进行处理\",{\"1\":{\"107\":1}}],[\"则对整个点云做全局特征提取\",{\"1\":{\"21\":1}}],[\"则计算当前层bertlayer时\",{\"1\":{\"103\":1}}],[\"则q来自query\",{\"1\":{\"103\":1}}],[\"则自动生成\",{\"1\":{\"102\":1}}],[\"则采用了两种不同的架构\",{\"1\":{\"90\":1}}],[\"则是以文本作为监督信号\",{\"1\":{\"88\":1}}],[\"则返回\",{\"1\":{\"34\":2}}],[\"则进行额外处理\",{\"1\":{\"7\":1}}],[\"剩余的语言\",{\"1\":{\"10\":1}}],[\"部分\",{\"1\":{\"10\":2,\"68\":1}}],[\"有助于模型学习到不同方向的特征\",{\"1\":{\"108\":1}}],[\"有多少组点云\",{\"1\":{\"32\":1}}],[\"有些位置被标记为\",{\"1\":{\"21\":1}}],[\"有效语言部分\",{\"1\":{\"10\":1}}],[\"有效的部分\",{\"1\":{\"10\":1}}],[\"有问题需要咨询的小伙伴\",{\"1\":{\"2\":1}}],[\"非常高\",{\"1\":{\"98\":2}}],[\"非均匀\",{\"1\":{\"41\":1}}],[\"非均匀密度下稳定的特征学习\",{\"0\":{\"23\":1}}],[\"非刚性运动\",{\"1\":{\"41\":1}}],[\"非刚性变形\",{\"1\":{\"37\":1}}],[\"非结构化\",{\"1\":{\"39\":1}}],[\"非\",{\"1\":{\"10\":1,\"103\":1}}],[\"非线性增强和空间对齐\",{\"1\":{\"8\":2}}],[\"供\",{\"1\":{\"10\":1}}],[\"即网格大小的乘积\",{\"1\":{\"109\":1}}],[\"即图像在水平和垂直方向上分别可以划分的图像块数量\",{\"1\":{\"109\":1}}],[\"即图片上相邻的区域具有相似的特征\",{\"1\":{\"105\":1}}],[\"即一种先验知识\",{\"1\":{\"105\":1}}],[\"即给定input\",{\"1\":{\"103\":1}}],[\"即文本和图像可能不完全匹配\",{\"1\":{\"96\":1}}],[\"即基于对比学习的方法\",{\"1\":{\"96\":1}}],[\"即基于文本弱监督来提升性能\",{\"1\":{\"96\":1}}],[\"即为与当前文本描述相似度最高的那副图片\",{\"1\":{\"94\":1}}],[\"即真正属于一对的文本和图像\",{\"1\":{\"90\":1}}],[\"即上图所示的矩阵\",{\"1\":{\"90\":1}}],[\"即上一层级\",{\"1\":{\"26\":1}}],[\"即\",{\"1\":{\"34\":1,\"37\":1,\"40\":1,\"67\":1,\"68\":1}}],[\"即只改变物体的方向而不改变形状和大小\",{\"1\":{\"33\":1}}],[\"即只依赖一小部分关键点就能判断整体形状\",{\"1\":{\"30\":1}}],[\"即使是一维位置编码\",{\"1\":{\"111\":1}}],[\"即使其他点都在\",{\"1\":{\"37\":1}}],[\"即使\",{\"1\":{\"30\":1}}],[\"即使丢失一些点或加入异常点\",{\"1\":{\"30\":1}}],[\"即对点顺序不敏感\",{\"1\":{\"30\":1}}],[\"即随机移除一部分输入点\",{\"1\":{\"25\":1}}],[\"即每个图像块经过卷积操作后得到的特征向量的维度\",{\"1\":{\"109\":1}}],[\"即每个点的各个类别得分\",{\"1\":{\"36\":1}}],[\"即每个点对应的\",{\"1\":{\"7\":1}}],[\"即每个查询点有一个特征向量\",{\"1\":{\"21\":1}}],[\"即每个局部邻域内的点数量维度\",{\"1\":{\"21\":1}}],[\"即空间中的任何距离值具有相似的含义\",{\"1\":{\"19\":1}}],[\"即任何方向上的度量都是等价的\",{\"1\":{\"19\":1}}],[\"即任务是找到点云集中的局部区域的中心点\",{\"1\":{\"18\":1}}],[\"即在欧氏空间中\",{\"1\":{\"19\":1}}],[\"即所有\",{\"1\":{\"10\":1}}],[\"默认使用\",{\"1\":{\"112\":1}}],[\"默认为0\",{\"1\":{\"113\":2}}],[\"默认为false\",{\"1\":{\"113\":1}}],[\"默认为8\",{\"1\":{\"113\":1}}],[\"默认为\",{\"1\":{\"107\":1,\"109\":5}}],[\"默认为全\",{\"1\":{\"10\":1}}],[\"默认忽略的标签值\",{\"1\":{\"103\":1}}],[\"默认有\",{\"1\":{\"22\":1}}],[\"默认图像注意力掩码为空\",{\"1\":{\"7\":1}}],[\"形式\",{\"1\":{\"70\":1}}],[\"形状是\",{\"1\":{\"101\":1}}],[\"形状\",{\"1\":{\"66\":2,\"104\":1}}],[\"形状为\",{\"1\":{\"10\":1,\"109\":2}}],[\"形\",{\"1\":{\"46\":1}}],[\"形成最终的局部特征表示\",{\"1\":{\"25\":1}}],[\"形成最终的局部区域表示\",{\"1\":{\"21\":1}}],[\"形成一个特征向量\",{\"1\":{\"26\":1}}],[\"形成一个综合的多尺度特征表示\",{\"1\":{\"24\":1}}],[\"形成一个新的子集\",{\"1\":{\"21\":1}}],[\"形成\",{\"1\":{\"21\":1}}],[\"形成更稳定的联合表示\",{\"1\":{\"8\":1}}],[\"形心点的坐标来实现\",{\"1\":{\"20\":1}}],[\"层交错堆叠\",{\"1\":{\"117\":1}}],[\"层和\",{\"1\":{\"117\":1}}],[\"层处理后的输出\",{\"1\":{\"112\":1}}],[\"层的作用\",{\"1\":{\"66\":1}}],[\"层维度\",{\"1\":{\"37\":1}}],[\"层进一步融合局部\",{\"1\":{\"36\":1}}],[\"层\",{\"1\":{\"34\":1,\"112\":3}}],[\"层后面都加了\",{\"1\":{\"32\":1}}],[\"层后的结果\",{\"1\":{\"10\":1}}],[\"层构成了一个\",{\"1\":{\"22\":1}}],[\"层提取特征\",{\"1\":{\"21\":1}}],[\"层次化结构由多个set\",{\"1\":{\"17\":1}}],[\"层次化点集特征学习\",{\"0\":{\"17\":1}}],[\"层归一化\",{\"1\":{\"12\":1,\"58\":1,\"83\":1}}],[\"层将\",{\"1\":{\"7\":1,\"104\":1}}],[\"经过encoder之后\",{\"1\":{\"110\":1}}],[\"经过处理后的图像块嵌入张量\",{\"1\":{\"109\":1}}],[\"经过卷积层变成\",{\"1\":{\"109\":1}}],[\"经过预处理的图像和对应的标签\",{\"1\":{\"107\":1}}],[\"经过一层全连接后的输出\",{\"1\":{\"66\":1}}],[\"经过全局池化后得到一个全局特征向量\",{\"1\":{\"32\":1}}],[\"经过\",{\"1\":{\"10\":1,\"12\":2,\"68\":1}}],[\"所具备的\",{\"1\":{\"105\":1}}],[\"所提供的代码展开进行讲解\",{\"1\":{\"72\":1}}],[\"所参考源仓库未提供requirements\",{\"1\":{\"45\":1}}],[\"所以参数量为\",{\"1\":{\"115\":1}}],[\"所以把\",{\"1\":{\"103\":1}}],[\"所以模型的任务是\",{\"1\":{\"68\":1}}],[\"所以模型必须具有对点顺序的不变性\",{\"1\":{\"40\":1}}],[\"所以最终的答案只能来自原始输入文本中的某一段子串\",{\"1\":{\"68\":1}}],[\"所以我只是取其中的一部分数据\",{\"1\":{\"45\":1}}],[\"所以我们需要加一个正则化项来鼓励变换矩阵接近正交矩阵\",{\"1\":{\"33\":1}}],[\"所以准确度不是在我的考虑范围之内\",{\"1\":{\"45\":1}}],[\"所以\",{\"1\":{\"42\":1}}],[\"所以要把它复制\",{\"1\":{\"36\":1}}],[\"所以小的才是有效点\",{\"1\":{\"21\":1}}],[\"所有\",{\"1\":{\"103\":2}}],[\"所有logits的平均作为最终的matching\",{\"1\":{\"102\":1}}],[\"所有模型均训练了32个周期\",{\"1\":{\"90\":1}}],[\"所有输入序列等长\",{\"1\":{\"46\":1}}],[\"所有序列都填充到max\",{\"1\":{\"46\":1}}],[\"所有卷积和\",{\"1\":{\"32\":1}}],[\"所有点相乘\",{\"1\":{\"40\":1}}],[\"所有点相加\",{\"1\":{\"40\":1}}],[\"所有点经过共享参数的\",{\"1\":{\"30\":1}}],[\"所有点组成的局部区域\",{\"1\":{\"21\":1}}],[\"所有尺度的特征保存到\",{\"1\":{\"25\":1}}],[\"所有尺度的网络并行运行\",{\"1\":{\"25\":1}}],[\"所有的数据都将发生变化\",{\"1\":{\"15\":1}}],[\"所需的输入格式\",{\"1\":{\"10\":1}}],[\"所使用的隐藏状态空间维度\",{\"1\":{\"9\":1}}],[\"拼接机制不够精细\",{\"1\":{\"37\":1}}],[\"拼接方式缺乏动态调整机制\",{\"1\":{\"37\":1}}],[\"拼接嵌入向量\",{\"1\":{\"10\":1}}],[\"拼接后的输入嵌入\",{\"1\":{\"10\":1}}],[\"拼接在一起\",{\"1\":{\"10\":1,\"103\":1}}],[\"拼接多模态嵌入与语言嵌入\",{\"0\":{\"10\":1},\"1\":{\"7\":1}}],[\"如此反复\",{\"1\":{\"117\":1}}],[\"如下图所示\",{\"1\":{\"112\":1}}],[\"如交叉熵损失\",{\"1\":{\"110\":1}}],[\"如局部性和平移不变性\",{\"1\":{\"105\":1}}],[\"如文本生成\",{\"1\":{\"103\":1}}],[\"如vit\",{\"1\":{\"104\":1}}],[\"如vilt\",{\"1\":{\"98\":1}}],[\"如vse++\",{\"1\":{\"98\":1}}],[\"如clip\",{\"1\":{\"98\":1}}],[\"如mae和beit\",{\"1\":{\"96\":1}}],[\"如moco和simclr\",{\"1\":{\"96\":1}}],[\"如imagenet\",{\"1\":{\"96\":1}}],[\"如自注意力层和前馈层\",{\"1\":{\"74\":1}}],[\"如翻译后的句子\",{\"1\":{\"74\":1}}],[\"如句子\",{\"1\":{\"74\":1}}],[\"如判断哪个是答案的开始\",{\"1\":{\"68\":1}}],[\"如方差\",{\"1\":{\"40\":1}}],[\"如时间序列点云\",{\"1\":{\"37\":1}}],[\"如只有几十个点\",{\"1\":{\"37\":1}}],[\"如弯曲\",{\"1\":{\"37\":1}}],[\"如人体姿态变化\",{\"1\":{\"37\":1}}],[\"如桌子边缘\",{\"1\":{\"37\":1}}],[\"如椅子腿和桌面连接处\",{\"1\":{\"37\":1}}],[\"如旋转\",{\"1\":{\"29\":1,\"32\":1,\"33\":1}}],[\"如最大池化\",{\"1\":{\"28\":1}}],[\"如法线\",{\"1\":{\"21\":1}}],[\"如何将二维图像转换为一维时间序列\",{\"1\":{\"119\":1}}],[\"如何将这个预训练的视觉模型应用到新的任务中呢\",{\"1\":{\"91\":1}}],[\"如何计算loss的\",{\"1\":{\"101\":1}}],[\"如何降低模型训练成本\",{\"1\":{\"98\":1}}],[\"如何获取答案\",{\"1\":{\"68\":1}}],[\"如何利用\",{\"1\":{\"66\":1}}],[\"如何设计一个能够从这些局部分区中学习有用特征的机制\",{\"1\":{\"15\":1}}],[\"如何有效地对点云进行分区\",{\"1\":{\"15\":1}}],[\"如何生成点集的划分\",{\"1\":{\"15\":1}}],[\"如\",{\"1\":{\"12\":1,\"37\":3,\"40\":1,\"103\":2,\"108\":1,\"112\":1}}],[\"如果类别数大于\",{\"1\":{\"114\":1}}],[\"如果未指定\",{\"1\":{\"112\":2}}],[\"如果未提供位置id\",{\"1\":{\"102\":1}}],[\"如果传入了归一化层类\",{\"1\":{\"109\":1}}],[\"如果传入的是整数\",{\"1\":{\"109\":1}}],[\"如果传入一个归一化层类\",{\"1\":{\"109\":1}}],[\"如果传入一个整数\",{\"1\":{\"109\":2}}],[\"如果定义了图像预处理转换操作\",{\"1\":{\"107\":1}}],[\"如果该路径在采样的验证集样本中则存入验证集\",{\"1\":{\"107\":1}}],[\"如果是beam\",{\"1\":{\"104\":1}}],[\"如果是分割任务\",{\"1\":{\"34\":2}}],[\"如果是分类任务\",{\"1\":{\"34\":2}}],[\"如果进一步采用convirt\",{\"1\":{\"96\":1}}],[\"如果在读取图片过程中出现错误\",{\"1\":{\"93\":1}}],[\"如果我们直接使用类别标签作为文本描述\",{\"1\":{\"92\":1}}],[\"如果有缓存\",{\"1\":{\"103\":1}}],[\"如果有缓存的key\",{\"1\":{\"103\":1}}],[\"如果有个类别\",{\"1\":{\"91\":1}}],[\"如果有额外特征\",{\"1\":{\"21\":1,\"25\":1}}],[\"如果有额外的点特征\",{\"1\":{\"21\":1}}],[\"如果你希望模型能\",{\"1\":{\"68\":1}}],[\"如果你有一张人脸的点云\",{\"1\":{\"37\":1}}],[\"如果正确答案没有出现在上下文中\",{\"1\":{\"68\":1}}],[\"如果模型预测\",{\"1\":{\"68\":1}}],[\"如果值大于\",{\"1\":{\"67\":1}}],[\"如果值小于\",{\"1\":{\"67\":1}}],[\"如果本地不存在\",{\"1\":{\"45\":1}}],[\"如果对于任意排列\",{\"1\":{\"40\":1}}],[\"如果训练过程中加入了噪声\",{\"1\":{\"37\":1}}],[\"如果点太少\",{\"1\":{\"37\":1}}],[\"如果这些关键点缺失或被遮挡\",{\"1\":{\"37\":1}}],[\"如果直接作为变换矩阵\",{\"1\":{\"32\":1}}],[\"如果没有任何归纳偏置\",{\"1\":{\"105\":1}}],[\"如果没有则下载\",{\"1\":{\"93\":1,\"95\":1}}],[\"如果没有\",{\"1\":{\"32\":1}}],[\"如果没有提供激活函数层\",{\"1\":{\"114\":1}}],[\"如果没有提供归一化层\",{\"1\":{\"114\":1}}],[\"如果没有提供\",{\"1\":{\"10\":1}}],[\"如果不是则抛出异常\",{\"1\":{\"107\":1}}],[\"如果不加处理\",{\"1\":{\"32\":1}}],[\"如果不够就重复最近的点来填充\",{\"1\":{\"21\":1}}],[\"如果\",{\"1\":{\"21\":1,\"37\":2,\"66\":1}}],[\"如果某个查询点附近的点太少\",{\"1\":{\"21\":1}}],[\"如果某个点到新中心点的距离比之前记录的\",{\"1\":{\"21\":1}}],[\"如果使用\",{\"1\":{\"7\":1}}],[\"如图像或点云特征\",{\"1\":{\"10\":1}}],[\"如图像\",{\"1\":{\"10\":1}}],[\"映射到分类空间中去\",{\"1\":{\"114\":1}}],[\"映射到\",{\"1\":{\"9\":1}}],[\"进一步扩展了这个方法来预测n\",{\"1\":{\"96\":1}}],[\"进一步地\",{\"1\":{\"91\":1}}],[\"进一步提取各自模态内部的语义一致性与结构关系\",{\"1\":{\"8\":1}}],[\"进行局部特征提取\",{\"1\":{\"117\":1}}],[\"进行反向传播\",{\"1\":{\"114\":1}}],[\"进行归一化\",{\"1\":{\"114\":1}}],[\"进行并行输入\",{\"1\":{\"109\":1}}],[\"进行文本编码\",{\"1\":{\"103\":1}}],[\"进行编码\",{\"1\":{\"100\":2}}],[\"进行预训练\",{\"1\":{\"100\":1}}],[\"进行解压\",{\"1\":{\"45\":1}}],[\"进行分类\",{\"1\":{\"37\":1,\"69\":1}}],[\"进行特征学习\",{\"1\":{\"15\":1}}],[\"进行联合推理\",{\"1\":{\"7\":1}}],[\"在如此大规模的数据集上进行预训练\",{\"1\":{\"118\":1}}],[\"在预训练和使用该模型时\",{\"1\":{\"118\":1}}],[\"在预训练时\",{\"1\":{\"108\":1}}],[\"在模型架构中\",{\"1\":{\"117\":1}}],[\"在模型初始化时\",{\"1\":{\"111\":1}}],[\"在论文的最后\",{\"1\":{\"117\":1}}],[\"在论文中\",{\"1\":{\"90\":1,\"114\":1,\"115\":1}}],[\"在论文中提到\",{\"1\":{\"37\":1}}],[\"在深度学习领域\",{\"1\":{\"115\":1}}],[\"在两个线性层之间通常会插入一个非线性激活函数\",{\"1\":{\"112\":1}}],[\"在transformer\",{\"1\":{\"114\":1}}],[\"在transformer中\",{\"1\":{\"111\":1}}],[\"在transformer模型中\",{\"1\":{\"105\":1}}],[\"在每一层\",{\"1\":{\"110\":1}}],[\"在每个层上\",{\"1\":{\"17\":1}}],[\"在上面的结构图中可以看到\",{\"1\":{\"110\":1}}],[\"在上下文中找到最可能的答案起始位置和结束位置\",{\"1\":{\"68\":1}}],[\"在代码中\",{\"1\":{\"109\":1}}],[\"在柱状图上添加数值标签\",{\"1\":{\"107\":1}}],[\"在图像分类任务中\",{\"1\":{\"105\":1,\"117\":1}}],[\"在图文检索中\",{\"1\":{\"93\":1}}],[\"在自回归生成时\",{\"1\":{\"103\":1}}],[\"在自注意力机制之后\",{\"1\":{\"74\":1}}],[\"在seq\",{\"1\":{\"102\":1}}],[\"在nlp领域\",{\"1\":{\"96\":1}}],[\"在迁移到其他数据集时也需要加上新的分类器进行有监督训练\",{\"1\":{\"96\":1}}],[\"在迁移到下游任务时\",{\"1\":{\"96\":1}}],[\"在计算机视觉领域\",{\"1\":{\"96\":1}}],[\"在imagenet数据集上可以提升1\",{\"1\":{\"92\":1}}],[\"在之前的例子中\",{\"1\":{\"92\":1}}],[\"在实际应用中可以选用常见的卷积神经网络\",{\"1\":{\"90\":1}}],[\"在实现时可采用自然语言处理\",{\"1\":{\"90\":1}}],[\"在实现时\",{\"1\":{\"19\":1}}],[\"在训练过程中\",{\"1\":{\"89\":1}}],[\"在训练时引入随机丢弃形心来模拟不同密度情况\",{\"1\":{\"25\":1}}],[\"在训练时引入不同密度的点集情况\",{\"1\":{\"25\":1}}],[\"在输入序列长度不一致时\",{\"1\":{\"74\":1}}],[\"在解码器中\",{\"1\":{\"74\":1}}],[\"在前向传播中\",{\"1\":{\"70\":1}}],[\"在分类任务中更有用\",{\"1\":{\"66\":1}}],[\"在分割任务中\",{\"1\":{\"30\":1}}],[\"在问答任务中一般不会使用这个输出\",{\"1\":{\"66\":1}}],[\"在问答任务中\",{\"1\":{\"66\":1}}],[\"在使用clip模型进行zero\",{\"1\":{\"92\":1}}],[\"在使用\",{\"1\":{\"66\":1}}],[\"在返回前进行预处理\",{\"1\":{\"48\":1}}],[\"在大规模场景理解任务中表现一般\",{\"1\":{\"37\":1}}],[\"在速度和效率上占优\",{\"1\":{\"37\":1}}],[\"在某些标准数据集\",{\"1\":{\"37\":1}}],[\"在部件分割任务中\",{\"1\":{\"37\":1}}],[\"在通道维度进行拼接\",{\"1\":{\"34\":1}}],[\"在这个高维空间里\",{\"1\":{\"112\":1}}],[\"在这个过程中\",{\"1\":{\"91\":1}}],[\"在这种情况下\",{\"1\":{\"26\":1,\"96\":1}}],[\"在这些点中选出若干个中心点\",{\"1\":{\"16\":1}}],[\"在mrg中\",{\"1\":{\"26\":1}}],[\"在msg中\",{\"1\":{\"24\":1}}],[\"在密集采样的区域中学到的特征可能无法很好地泛化到稀疏采样的区域\",{\"1\":{\"23\":1}}],[\"在点云中逐步选择离已选点尽可能远的点\",{\"1\":{\"21\":1}}],[\"在屋里空间或某些特定的抽象空间中\",{\"1\":{\"19\":1}}],[\"在平移不变性上也有局限性\",{\"1\":{\"15\":1}}],[\"在pointnet中\",{\"1\":{\"15\":1}}],[\"在\",{\"1\":{\"12\":1,\"21\":3,\"32\":1,\"33\":1,\"37\":1,\"66\":2,\"68\":1,\"70\":1,\"88\":1,\"93\":1,\"108\":1,\"117\":2}}],[\"在跨模态融合后\",{\"1\":{\"8\":1}}],[\"在混合精度下运行\",{\"1\":{\"7\":1}}],[\"是hidden\",{\"1\":{\"115\":1}}],[\"是通道数\",{\"1\":{\"109\":1}}],[\"是批量大小\",{\"1\":{\"109\":1}}],[\"是卷积核的步长\",{\"1\":{\"109\":1}}],[\"是卷积核的大小\",{\"1\":{\"109\":1}}],[\"是输出通道数\",{\"1\":{\"109\":1}}],[\"是输入通道数\",{\"1\":{\"109\":1}}],[\"是为了把\",{\"1\":{\"104\":1}}],[\"是为了扩展成\",{\"1\":{\"33\":1}}],[\"是第一个transformer模块的input\",{\"1\":{\"100\":1}}],[\"是基于文本输入来生成图像的模型\",{\"1\":{\"88\":1}}],[\"是编码器模型\",{\"1\":{\"68\":1}}],[\"是模型预测出的答案的起始和结束位置\",{\"1\":{\"68\":1}}],[\"是模型最后一层所有\",{\"1\":{\"66\":1}}],[\"是答案终点的得分\",{\"1\":{\"66\":1}}],[\"是答案起点的得分\",{\"1\":{\"66\":1}}],[\"是序列实际长度\",{\"1\":{\"46\":1}}],[\"是当前文本对应的类别标签\",{\"1\":{\"46\":1}}],[\"是指位置嵌入的参数是可以在模型训练过程中通过反向传播算法进行更新的\",{\"1\":{\"111\":1}}],[\"是指\",{\"1\":{\"41\":1}}],[\"是一个大规模的图像数据集\",{\"1\":{\"118\":1}}],[\"是一个随机初始化的向量\",{\"1\":{\"110\":1}}],[\"是一个分类头\",{\"1\":{\"103\":1}}],[\"是一个\",{\"1\":{\"68\":2}}],[\"是一个线性层\",{\"1\":{\"66\":1}}],[\"是一个小型神经网络\",{\"1\":{\"32\":1}}],[\"是一种基于自注意力机制\",{\"1\":{\"74\":1}}],[\"是一种对输入顺序不敏感的函数\",{\"1\":{\"40\":1}}],[\"是一种表示三维空间中物体或场景的方式\",{\"1\":{\"39\":1}}],[\"是一种单尺度网络\",{\"1\":{\"37\":1}}],[\"是最大可容忍的点云范围\",{\"1\":{\"30\":1}}],[\"是关键点集合\",{\"1\":{\"30\":1}}],[\"是对称的\",{\"1\":{\"30\":1}}],[\"是后续的全连接网络\",{\"1\":{\"30\":1}}],[\"是每个点的高维特征\",{\"1\":{\"30\":1}}],[\"是每个点云局部区域关键点引导下提取的图像信息增强后的点云关键点局部区域特征\",{\"1\":{\"8\":1}}],[\"是否在生成q\",{\"1\":{\"113\":1}}],[\"是否采样\",{\"1\":{\"104\":1}}],[\"是否使用核采样\",{\"1\":{\"104\":1}}],[\"是否能生成新文本\",{\"1\":{\"68\":1}}],[\"是否可微\",{\"1\":{\"40\":1}}],[\"是否被后续模型改进\",{\"1\":{\"37\":1}}],[\"是否包含法线信息\",{\"1\":{\"22\":1}}],[\"是否为推理模式\",{\"1\":{\"7\":1}}],[\"是\",{\"1\":{\"21\":2,\"30\":1,\"33\":1,\"40\":7,\"66\":1,\"67\":1,\"103\":1,\"107\":1}}],[\"是有效的\",{\"1\":{\"10\":1}}],[\"是图像块的总数\",{\"1\":{\"109\":1}}],[\"是图像宽度\",{\"1\":{\"109\":1}}],[\"是图像高度\",{\"1\":{\"109\":1}}],[\"是图像每个通道的标准差\",{\"1\":{\"108\":1}}],[\"是图像每个通道的均值\",{\"1\":{\"108\":1}}],[\"是图像\",{\"1\":{\"8\":1}}],[\"分批次预测\",{\"1\":{\"93\":1}}],[\"分批次从图像列表中取出一批图像\",{\"1\":{\"93\":1}}],[\"分数\",{\"1\":{\"66\":1}}],[\"分数归一化\",{\"1\":{\"12\":1}}],[\"分为训练\",{\"1\":{\"45\":1}}],[\"分割性能\",{\"1\":{\"37\":1}}],[\"分割精度不高\",{\"1\":{\"37\":1}}],[\"分割任务依赖拼接机制\",{\"1\":{\"37\":1}}],[\"分割任务\",{\"0\":{\"36\":1}}],[\"分割出\",{\"1\":{\"7\":1}}],[\"分类头\",{\"1\":{\"114\":1}}],[\"分类标记\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"分类性能\",{\"1\":{\"37\":1}}],[\"分类性能略逊于多视角方法\",{\"1\":{\"37\":1}}],[\"分类精度略低\",{\"1\":{\"37\":1}}],[\"分类准确率\",{\"1\":{\"37\":1}}],[\"分类准确率仅下降约\",{\"1\":{\"30\":1}}],[\"分类任务的类别数\",{\"1\":{\"110\":1}}],[\"分类任务是指对输入文本中的每个\",{\"1\":{\"69\":1}}],[\"分类任务中对缺失点具有一定鲁棒性\",{\"1\":{\"37\":1}}],[\"分类任务\",{\"0\":{\"35\":1}}],[\"分别如下\",{\"1\":{\"100\":1}}],[\"分别基于lenet\",{\"1\":{\"93\":1}}],[\"分别提取图像特征和文本特征\",{\"1\":{\"90\":1}}],[\"分别提取特征\",{\"1\":{\"25\":1}}],[\"分别计算答案起始下标和结束下标预测得到的交叉熵损失\",{\"1\":{\"67\":1}}],[\"分别是文本编码器\",{\"1\":{\"90\":1}}],[\"分别是\",{\"1\":{\"33\":1}}],[\"分别是最大值和它们的位置索引\",{\"1\":{\"21\":1,\"32\":1,\"34\":1}}],[\"分布\",{\"1\":{\"8\":2}}],[\"每一个向量作为一个单独的输入\",{\"1\":{\"109\":1}}],[\"每一个patch作为一个token\",{\"1\":{\"109\":1}}],[\"每一层bertlayer产生的key\",{\"1\":{\"103\":1}}],[\"每一行表示一张图像和所有文本之间的相似度\",{\"1\":{\"101\":1}}],[\"每次计算后返回本次可能需要缓存的key\",{\"1\":{\"103\":1}}],[\"每行是一个\",{\"1\":{\"8\":1}}],[\"每个图像块的尺寸\",{\"1\":{\"109\":1}}],[\"每个图像位置对应的所有点云点\",{\"1\":{\"8\":1}}],[\"每个patch是三通道的小图片\",{\"1\":{\"109\":1}}],[\"每个output\",{\"1\":{\"102\":1}}],[\"每个query\",{\"1\":{\"101\":1}}],[\"每个头64维\",{\"1\":{\"84\":1}}],[\"每个子层\",{\"1\":{\"74\":1}}],[\"每个位置的输出会通过一个独立的前馈神经网络进行进一步处理\",{\"1\":{\"74\":1}}],[\"每个\",{\"1\":{\"66\":3}}],[\"每个格子表示是否有物体\",{\"1\":{\"39\":1}}],[\"每个卷积层后跟一个\",{\"1\":{\"34\":1}}],[\"每个结构中心点不变\",{\"1\":{\"25\":1}}],[\"每个关键点的多尺度特征表示\",{\"1\":{\"25\":1}}],[\"每个关键点对应的局部区域点和特征\",{\"1\":{\"21\":1}}],[\"每个尺度可以有不同的网络深度和宽度\",{\"1\":{\"25\":1}}],[\"每个尺度\",{\"1\":{\"24\":1}}],[\"每个半径定义了一个局部邻域的大小\",{\"1\":{\"24\":1}}],[\"每个查询点点局部特征\",{\"1\":{\"21\":1}}],[\"每个查询点对所有原始点的距离\",{\"1\":{\"21\":1}}],[\"每个点通常包含\",{\"1\":{\"39\":1}}],[\"每个点都需要全局上下文\",{\"1\":{\"36\":1}}],[\"每个点有\",{\"1\":{\"32\":1,\"39\":1}}],[\"每个点到当前所有已选中心点的最小距离\",{\"1\":{\"21\":1}}],[\"每个点云点对应的所有图像位置\",{\"1\":{\"8\":1}}],[\"每个点云点与图像中每个位置之间的相似度得分\",{\"1\":{\"8\":1}}],[\"每个邻域内采样的关键点数量\",{\"1\":{\"21\":2}}],[\"每个区域中点的数量𝐾和query的半径𝑟\",{\"1\":{\"19\":1}}],[\"每列是一个\",{\"1\":{\"8\":1}}],[\"跨模态注意力矩阵\",{\"1\":{\"8\":1}}],[\"jx\",{\"1\":{\"118\":1}}],[\"jax\",{\"1\":{\"118\":1}}],[\"javaer\",{\"1\":{\"2\":1}}],[\"jft\",{\"1\":{\"96\":2}}],[\"jpeg\",{\"1\":{\"93\":1,\"95\":1}}],[\"jpg\",{\"1\":{\"93\":1,\"95\":1,\"107\":2}}],[\"join\",{\"1\":{\"93\":3,\"94\":1,\"95\":4,\"107\":3}}],[\"joint\",{\"1\":{\"8\":4,\"12\":2}}],[\"json\",{\"1\":{\"45\":1,\"107\":6}}],[\"j\",{\"1\":{\"25\":3,\"93\":2,\"95\":2}}],[\"使特征分布更稳定\",{\"1\":{\"33\":1}}],[\"使其符合模型的输入要求\",{\"1\":{\"93\":1}}],[\"使其标准化\",{\"1\":{\"33\":1}}],[\"使其姿态统一\",{\"1\":{\"32\":1}}],[\"使网络能够应对实际中各种密度变换的情况\",{\"1\":{\"25\":1}}],[\"使网络能学习不同采样密度下局部点云特征的提取\",{\"1\":{\"25\":1}}],[\"使局部特征的表示不够精确\",{\"1\":{\"19\":1}}],[\"使得\",{\"1\":{\"110\":1}}],[\"使得预训练模型能够直接应用于下游任务\",{\"1\":{\"92\":1}}],[\"使得采样点在整个点云空间中分布尽可能均匀\",{\"1\":{\"21\":1}}],[\"使得在它们之间可以共享学习到的特征表示的权重\",{\"1\":{\"15\":1}}],[\"使得它们能够在统一的语义空间中进行有效的跨模态交互\",{\"1\":{\"8\":2}}],[\"使用transformer架构为未来的多模态统一提供了可能性\",{\"1\":{\"119\":1}}],[\"使用截断正态分布初始化位置嵌入\",{\"1\":{\"111\":1,\"114\":1}}],[\"使用截断正态分布初始化分类标记\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"使用全连接\",{\"1\":{\"104\":1}}],[\"使用交叉熵损失衡量生成与真实之间的差异\",{\"1\":{\"103\":1}}],[\"使用adamw优化器\",{\"1\":{\"90\":1}}],[\"使用掩码机制确保模型在生成目标序列时只能看到当前及之前的词\",{\"1\":{\"74\":1}}],[\"使用了如下逻辑\",{\"1\":{\"66\":1}}],[\"使用pycharm导入项目\",{\"1\":{\"45\":1}}],[\"使用的是tnews数据集\",{\"1\":{\"45\":1}}],[\"使用神经网络直接学习对称函数\",{\"1\":{\"40\":1}}],[\"使用分块策略\",{\"1\":{\"37\":1}}],[\"使用分割头预测最终的\",{\"1\":{\"7\":1}}],[\"使用多层\",{\"1\":{\"36\":1}}],[\"使用多个不同大小的邻域球\",{\"1\":{\"25\":1}}],[\"使用多个\",{\"1\":{\"21\":1}}],[\"使用对称函数\",{\"1\":{\"30\":1}}],[\"使用最大池化聚合局部信息\",{\"1\":{\"25\":1}}],[\"使用farthest\",{\"1\":{\"18\":1}}],[\"使用mini\",{\"1\":{\"17\":1}}],[\"使用残差连接\",{\"1\":{\"12\":1}}],[\"使用自注意力机制提炼两个模态之间的语义一致性\",{\"1\":{\"8\":1}}],[\"使用\",{\"1\":{\"7\":4,\"10\":1,\"12\":4,\"21\":2,\"25\":1,\"30\":1,\"32\":1,\"34\":2,\"36\":1,\"37\":2,\"68\":1,\"100\":1,\"103\":1}}],[\"02\",{\"1\":{\"110\":1,\"111\":2,\"114\":2}}],[\"02413\",{\"1\":{\"14\":1}}],[\"0+cu113\",{\"1\":{\"72\":1}}],[\"03\",{\"1\":{\"45\":1}}],[\"00593\",{\"1\":{\"27\":1}}],[\"0\",{\"0\":{\"107\":1},\"1\":{\"8\":3,\"10\":4,\"12\":2,\"21\":12,\"22\":2,\"25\":14,\"32\":8,\"33\":1,\"34\":1,\"36\":1,\"45\":1,\"46\":29,\"49\":1,\"51\":1,\"53\":1,\"54\":2,\"57\":2,\"61\":1,\"67\":3,\"68\":1,\"69\":1,\"72\":1,\"79\":1,\"82\":1,\"84\":3,\"91\":1,\"93\":1,\"95\":1,\"100\":2,\"101\":2,\"102\":5,\"103\":9,\"104\":3,\"107\":2,\"108\":23,\"109\":3,\"110\":3,\"111\":3,\"112\":2,\"113\":3,\"114\":6}}],[\"04744\",{\"1\":{\"4\":1}}],[\"梯度消失或爆炸\",{\"1\":{\"8\":1}}],[\"防止过拟合\",{\"1\":{\"113\":4}}],[\"防止\",{\"1\":{\"8\":1}}],[\"计算梯度\",{\"1\":{\"114\":1}}],[\"计算预测结果与真实标签之间的交叉熵损失\",{\"1\":{\"114\":1}}],[\"计算预测正确的样本数\",{\"1\":{\"114\":1}}],[\"计算公式如下\",{\"1\":{\"112\":1}}],[\"计算图像块的数量\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"计算图像块的总数\",{\"1\":{\"109\":1}}],[\"计算图像嵌入向量列表中每个图像对应的嵌入向量和传入的分类文本嵌入向量的余弦相似度\",{\"1\":{\"93\":1}}],[\"计算网格大小\",{\"1\":{\"109\":1}}],[\"计算上下文表示\",{\"1\":{\"103\":1}}],[\"计算注意力分数\",{\"1\":{\"103\":1}}],[\"计算序列长度\",{\"1\":{\"102\":1}}],[\"计算每个注意力头的维度\",{\"1\":{\"113\":1}}],[\"计算每个query\",{\"1\":{\"101\":1}}],[\"计算每个查询点\",{\"1\":{\"21\":1}}],[\"计算一个\",{\"1\":{\"101\":1}}],[\"计算这批图像嵌入向量中每一个图像嵌入向量和我们所有分类文本嵌入向量的余弦相似度\",{\"1\":{\"93\":1}}],[\"计算余弦相似度\",{\"1\":{\"91\":1}}],[\"计算缩放的余弦相似度\",{\"1\":{\"90\":1}}],[\"计算交叉熵损失\",{\"1\":{\"67\":1,\"102\":1}}],[\"计算掩码语言损失\",{\"1\":{\"64\":1}}],[\"计算需要填充的长度\",{\"1\":{\"46\":1}}],[\"计算点集的分布特性\",{\"1\":{\"40\":1}}],[\"计算复杂度高\",{\"1\":{\"39\":1}}],[\"计算变换矩阵与其转置相乘后与单位矩阵之间的距离\",{\"1\":{\"33\":1}}],[\"计算量大\",{\"1\":{\"28\":1}}],[\"计算当前批次中所有序列的实际最大长度\",{\"1\":{\"48\":1}}],[\"计算当前中心点与所有点之间的欧氏距离平方\",{\"1\":{\"21\":1}}],[\"计算当前语言输入中有多少个有效\",{\"1\":{\"10\":1}}],[\"计算效率问题\",{\"1\":{\"19\":1}}],[\"计算成本\",{\"1\":{\"18\":1}}],[\"计算\",{\"1\":{\"7\":1,\"12\":2,\"101\":1,\"112\":1}}],[\"可学习位置嵌入\",{\"1\":{\"111\":1}}],[\"可学习query\",{\"1\":{\"104\":1}}],[\"可认为是模型参数一部分\",{\"1\":{\"100\":1}}],[\"可使用成熟的\",{\"1\":{\"39\":1}}],[\"可选颜色\",{\"1\":{\"39\":1}}],[\"可选属性\",{\"1\":{\"39\":1}}],[\"可选\",{\"0\":{\"38\":1},\"1\":{\"21\":1,\"34\":1}}],[\"可改进的地方\",{\"1\":{\"19\":1}}],[\"可能遗漏重要细节\",{\"1\":{\"37\":1}}],[\"可能不足以代表复杂的局部结构\",{\"1\":{\"37\":1}}],[\"可能无法捕捉重要的几何细节\",{\"1\":{\"18\":1}}],[\"可能导致样本在高密度区域内过度集中\",{\"1\":{\"18\":1}}],[\"可能存在的问题\",{\"1\":{\"18\":1}}],[\"可以利用其自注意力机制捕捉特征之间的长距离依赖关系\",{\"1\":{\"117\":1}}],[\"可以使用预训练的\",{\"1\":{\"117\":1}}],[\"可以看这篇文章\",{\"1\":{\"113\":1}}],[\"可以看到当epochs增大时\",{\"1\":{\"117\":1}}],[\"可以看到\",{\"1\":{\"111\":2,\"112\":1,\"115\":1,\"116\":1}}],[\"可以看到8个图像\",{\"1\":{\"91\":1}}],[\"可以看到对于要预测的8个图像\",{\"1\":{\"91\":1}}],[\"可以变成一个\",{\"1\":{\"109\":1}}],[\"可以直接通过类名调用\",{\"1\":{\"107\":1}}],[\"可以将搜索范围限制在满足这些性质的模型子空间内\",{\"1\":{\"105\":1}}],[\"可以将其平移到坐标系的中心\",{\"1\":{\"15\":1}}],[\"可以在下游任务中获得较好的迁移效果\",{\"1\":{\"105\":1}}],[\"可以和所有的query\",{\"1\":{\"103\":1}}],[\"可以和所有自己的tokens做attention\",{\"1\":{\"103\":1}}],[\"可以学习到如何更好地结合文本提取图片信息\",{\"1\":{\"100\":1}}],[\"可以参考论文\",{\"1\":{\"92\":1}}],[\"可以得到每个类别的预测概率\",{\"1\":{\"91\":1}}],[\"可以概括为以下两个主要步骤\",{\"1\":{\"91\":1}}],[\"可以处理任意顺序的点集\",{\"1\":{\"30\":1}}],[\"可以先去了解一下python中的高级索引机制\",{\"1\":{\"21\":1}}],[\"可以理解为\",{\"1\":{\"15\":1}}],[\"可以加微信备注来意\",{\"1\":{\"2\":1}}],[\"可操作性热图\",{\"1\":{\"7\":1}}],[\"95\",{\"1\":{\"93\":1}}],[\"90\",{\"1\":{\"37\":1}}],[\"96\",{\"1\":{\"25\":1}}],[\"9\",{\"0\":{\"12\":1},\"1\":{\"7\":1,\"32\":4,\"37\":1,\"45\":1,\"72\":1,\"104\":1}}],[\"输入到\",{\"1\":{\"117\":1}}],[\"输入到mlp\",{\"1\":{\"110\":1}}],[\"输入加上经过归一化和\",{\"1\":{\"112\":1}}],[\"输入加上经过归一化和注意力层处理后的输出\",{\"1\":{\"112\":1}}],[\"输入encoder的最左侧部分添加了一个0\",{\"1\":{\"110\":1}}],[\"输入的图像张量\",{\"1\":{\"109\":1}}],[\"输入的图片尺寸必须为224x224\",{\"1\":{\"108\":1}}],[\"输入的图片尺寸并不是自定义的\",{\"1\":{\"108\":1}}],[\"输入为\",{\"1\":{\"109\":1}}],[\"输入样本\",{\"1\":{\"104\":1}}],[\"输入包含三部分\",{\"1\":{\"100\":1}}],[\"输入image\",{\"1\":{\"91\":1}}],[\"输入\",{\"1\":{\"83\":1,\"103\":1}}],[\"输入序列\",{\"1\":{\"70\":1}}],[\"输入与输出的关系\",{\"1\":{\"68\":1}}],[\"输入会变成如下结构\",{\"1\":{\"66\":1}}],[\"输入格式\",{\"1\":{\"66\":1}}],[\"输入数据格式\",{\"1\":{\"46\":1}}],[\"输入形式\",{\"1\":{\"37\":1}}],[\"输入是原始点云\",{\"1\":{\"32\":1}}],[\"输入标准化\",{\"0\":{\"32\":1}}],[\"输入点云变换矩阵\",{\"1\":{\"34\":1}}],[\"输入点云变换网络\",{\"1\":{\"34\":1}}],[\"输入点云可能来自不同角度\",{\"1\":{\"32\":1}}],[\"输入点云可能缺失或含有异常点\",{\"1\":{\"30\":1}}],[\"输入点云可能缺失或包含噪声\",{\"1\":{\"29\":1}}],[\"输入点的特征维度\",{\"1\":{\"21\":1}}],[\"输入特征维度\",{\"1\":{\"12\":1}}],[\"输入图像被分割成\",{\"1\":{\"110\":1}}],[\"输入图像的通道数\",{\"1\":{\"109\":1,\"110\":1}}],[\"输入图像的尺寸\",{\"1\":{\"109\":1,\"110\":1}}],[\"输入图像\",{\"1\":{\"7\":1}}],[\"输出结果之后\",{\"1\":{\"114\":1}}],[\"输出结果都保持不变\",{\"1\":{\"40\":1}}],[\"输出通道\",{\"1\":{\"109\":1}}],[\"输出层\",{\"1\":{\"74\":1}}],[\"输出后\",{\"1\":{\"70\":1}}],[\"输出解释\",{\"1\":{\"66\":1}}],[\"输出做问答预测\",{\"1\":{\"66\":1}}],[\"输出是一个变换矩阵\",{\"1\":{\"32\":1}}],[\"输出类别数\",{\"1\":{\"22\":1}}],[\"输出形状为\",{\"1\":{\"21\":2}}],[\"输出的隐藏状态映射回原始嵌入维度\",{\"1\":{\"11\":1}}],[\"输出\",{\"1\":{\"7\":1,\"20\":1,\"34\":1,\"66\":1}}],[\"输出映射回合适维度\",{\"1\":{\"7\":1}}],[\"降维适配器\",{\"0\":{\"11\":1},\"1\":{\"7\":1,\"11\":1}}],[\"8600000\",{\"1\":{\"115\":1}}],[\"86×1000000\",{\"1\":{\"115\":1}}],[\"86m\",{\"1\":{\"115\":1}}],[\"8个头\",{\"1\":{\"84\":1}}],[\"89\",{\"1\":{\"37\":1}}],[\"85\",{\"1\":{\"37\":1}}],[\"8\",{\"0\":{\"11\":1},\"1\":{\"7\":1,\"25\":1,\"84\":1}}],[\"格式输出\",{\"1\":{\"7\":1}}],[\"返回分类标记对应的特征\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"返回数据集中图像的数量\",{\"1\":{\"107\":1}}],[\"返回字典格式结果\",{\"1\":{\"103\":1}}],[\"返回值形如\",{\"1\":{\"66\":1}}],[\"返回位置索引\",{\"1\":{\"21\":1}}],[\"返回拼接好的输入和\",{\"1\":{\"10\":1}}],[\"返回\",{\"1\":{\"7\":1,\"21\":1,\"32\":1,\"34\":1}}],[\"融合模块\",{\"1\":{\"98\":2}}],[\"融合不同来源的信息\",{\"1\":{\"12\":1}}],[\"融合通道信息\",{\"1\":{\"12\":1}}],[\"融合\",{\"1\":{\"7\":1}}],[\"融合语言与视觉特征\",{\"1\":{\"7\":1}}],[\"融合多模态空间特征\",{\"0\":{\"8\":1},\"1\":{\"7\":1}}],[\"7py1jdq1wp0nnyt3a\",{\"1\":{\"107\":1}}],[\"704\",{\"1\":{\"46\":2}}],[\"768\",{\"1\":{\"45\":3,\"51\":2,\"109\":4,\"110\":6,\"111\":4,\"112\":1,\"114\":4}}],[\"7\",{\"0\":{\"114\":1},\"1\":{\"7\":1,\"30\":1,\"104\":1}}],[\"+=\",{\"1\":{\"93\":1,\"95\":1,\"103\":1,\"114\":2}}],[\"+\",{\"1\":{\"7\":4,\"10\":2,\"12\":9,\"21\":2,\"22\":2,\"25\":6,\"30\":1,\"32\":2,\"36\":1,\"37\":3,\"40\":1,\"41\":1,\"42\":1,\"46\":21,\"49\":4,\"51\":1,\"55\":2,\"57\":3,\"58\":2,\"62\":1,\"64\":2,\"67\":2,\"68\":1,\"69\":1,\"70\":6,\"78\":4,\"79\":1,\"82\":4,\"83\":1,\"90\":1,\"91\":1,\"93\":2,\"95\":2,\"98\":1,\"101\":1,\"102\":4,\"103\":6,\"107\":1,\"111\":2,\"112\":2,\"113\":11,\"114\":2}}],[\"将多个注意力头的输出合并为一个张量\",{\"1\":{\"113\":1}}],[\"将多头注意力的输出进行线性变换\",{\"1\":{\"113\":1}}],[\"将注意力权重矩阵与v相乘\",{\"1\":{\"113\":1}}],[\"将q和k的转置相乘\",{\"1\":{\"113\":1}}],[\"将隐藏特征映射到输出特征空间\",{\"1\":{\"112\":1}}],[\"将分类标记和图像块嵌入拼接\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"将卷积后的通道维数作为embedding的维度\",{\"1\":{\"109\":1}}],[\"将其展平就变成了一个长度为768的向量\",{\"1\":{\"109\":1}}],[\"将裁剪后的图像调整为\",{\"1\":{\"108\":1}}],[\"将标签元组转换为一个一维张量\",{\"1\":{\"107\":1}}],[\"将一个批次的数据拆分为图像和标签两个元组\",{\"1\":{\"107\":1}}],[\"将横坐标0\",{\"1\":{\"107\":1}}],[\"将最有用的信息提供给\",{\"1\":{\"104\":1}}],[\"将缓存的\",{\"1\":{\"103\":1}}],[\"将第一个\",{\"1\":{\"103\":1}}],[\"将会在下文进行详细讲解\",{\"1\":{\"102\":1}}],[\"将文本和query\",{\"1\":{\"102\":1}}],[\"将input\",{\"1\":{\"100\":1}}],[\"将inputfeatures\",{\"1\":{\"46\":1}}],[\"将图片编码成\",{\"1\":{\"100\":1}}],[\"将图像数据移动到指定设备上\",{\"1\":{\"114\":1}}],[\"将图像的短边缩放为\",{\"1\":{\"108\":1}}],[\"将图像元组堆叠成一个四维张量\",{\"1\":{\"107\":1}}],[\"将图像patch和点云点拼接成一个统一的token序列\",{\"1\":{\"8\":1}}],[\"将图像+点云特征插入语言嵌入中\",{\"1\":{\"7\":1}}],[\"将这个问题转化为一个多标签分类任务\",{\"1\":{\"96\":1}}],[\"将这些特征映射到类别空间\",{\"1\":{\"35\":2}}],[\"将数据下载到当前项目目录下\",{\"1\":{\"93\":1}}],[\"将待分类的图像输入到图像编码器\",{\"1\":{\"91\":1}}],[\"将个文本特征和个图像特征两两组合\",{\"1\":{\"90\":1}}],[\"将选项展平\",{\"1\":{\"70\":1}}],[\"将每个\",{\"1\":{\"66\":1}}],[\"将每个点的局部特征与整个点云的全局特征拼接起来\",{\"1\":{\"34\":1}}],[\"将模型放入到仓库对应位置\",{\"1\":{\"45\":1}}],[\"将空间划分成立方体格子\",{\"1\":{\"39\":1}}],[\"将原始点云\",{\"1\":{\"34\":1}}],[\"将全局特征与每个点的局部特征拼接起来\",{\"1\":{\"30\":1}}],[\"将点云转换为体素网格\",{\"1\":{\"28\":1}}],[\"将来自下一级\",{\"1\":{\"26\":1}}],[\"将view\",{\"1\":{\"21\":1}}],[\"将当前选中的\",{\"1\":{\"21\":1}}],[\"将转换后的坐标以及点的附加特征\",{\"1\":{\"20\":1}}],[\"将输入特征映射到隐藏特征空间\",{\"1\":{\"112\":1}}],[\"将输入图像进行图像块嵌入\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"将输入图片\",{\"1\":{\"109\":1}}],[\"将输入文本转换为嵌入列表后和query\",{\"1\":{\"102\":1}}],[\"将输入的维度dim映射到dim\",{\"1\":{\"113\":1}}],[\"将输入的\",{\"1\":{\"100\":1}}],[\"将输入序列\",{\"1\":{\"74\":1}}],[\"将输入分别投影到低维空间\",{\"1\":{\"12\":1}}],[\"将输入映射到低维空间以进行\",{\"1\":{\"12\":1}}],[\"将两个注意力输出拼接在一起\",{\"1\":{\"12\":1}}],[\"将\",{\"1\":{\"10\":1,\"11\":1,\"103\":2,\"108\":2,\"109\":1,\"117\":1}}],[\"将语言嵌入\",{\"1\":{\"10\":1}}],[\"将融合后的\",{\"1\":{\"9\":1}}],[\"将融合后的空间特征通过适配器上采样到与语言模型匹配的维度\",{\"1\":{\"7\":1}}],[\"函数生成目标序列的概率分布\",{\"1\":{\"74\":1}}],[\"函数的作用是将整个点云视为一个\",{\"1\":{\"21\":1}}],[\"函数\",{\"1\":{\"7\":1,\"30\":1,\"40\":1,\"93\":1,\"95\":1,\"107\":1}}],[\"67b7e751e6b5931a9f45274653f4f653a4e6cdf6\",{\"1\":{\"107\":1}}],[\"62\",{\"1\":{\"93\":1}}],[\"64×64\",{\"1\":{\"30\":1}}],[\"640\",{\"1\":{\"25\":1}}],[\"64\",{\"1\":{\"22\":2,\"25\":7,\"32\":4,\"34\":3}}],[\"6\",{\"0\":{\"10\":1,\"113\":1},\"1\":{\"7\":1,\"22\":1,\"104\":1,\"114\":2}}],[\"5500\",{\"1\":{\"46\":2}}],[\"50\",{\"1\":{\"30\":1}}],[\"512\",{\"1\":{\"8\":1,\"22\":6,\"25\":5,\"32\":3,\"35\":3,\"36\":3}}],[\"5\",{\"0\":{\"112\":1},\"1\":{\"7\":1,\"8\":1,\"12\":1,\"25\":1,\"45\":1,\"49\":1,\"91\":1,\"92\":1,\"96\":1,\"103\":1,\"104\":1,\"107\":1,\"108\":19,\"113\":1,\"118\":1}}],[\"截断过长文本\",{\"1\":{\"7\":1}}],[\"对特征进行更深入的建模\",{\"1\":{\"117\":1}}],[\"对特征空间进行变换\",{\"1\":{\"34\":1}}],[\"对特征空间做变换\",{\"1\":{\"30\":1,\"33\":1}}],[\"对投影后的结果应用丢弃层\",{\"1\":{\"113\":1}}],[\"对输出进行维度交换和形状调整\",{\"1\":{\"113\":1}}],[\"对输入数据进行初步的特征提取\",{\"1\":{\"117\":1}}],[\"对输入进行归一化处理\",{\"1\":{\"112\":1}}],[\"对输入的点云做刚性变换\",{\"1\":{\"32\":1}}],[\"对输入点云做刚性变换\",{\"1\":{\"30\":1}}],[\"对输入点进行采样\",{\"1\":{\"16\":1}}],[\"对注意力权重矩阵应用丢弃层\",{\"1\":{\"113\":1}}],[\"对注意力分数矩阵应用softmax函数\",{\"1\":{\"113\":1}}],[\"对经过注意力层的输出进行归一化处理\",{\"1\":{\"112\":1}}],[\"对处理后的张量进行归一化操作\",{\"1\":{\"109\":1}}],[\"对图像进行归一化处理\",{\"1\":{\"108\":2}}],[\"对图像和点云特征进行\",{\"1\":{\"8\":2}}],[\"对验证集的处理方式是先resize成256x256的图片\",{\"1\":{\"108\":1}}],[\"对数据集和验证集划分之后\",{\"1\":{\"108\":1}}],[\"对数值变化敏感\",{\"1\":{\"40\":1}}],[\"对角线元素的labels\",{\"1\":{\"90\":1}}],[\"对称的对比学习损失\",{\"1\":{\"90\":1}}],[\"对称函数\",{\"0\":{\"40\":1},\"1\":{\"40\":2}}],[\"对两个特征进行线性投射\",{\"1\":{\"90\":1}}],[\"对每一个选项\",{\"1\":{\"70\":1}}],[\"对每个选项分别进行编码\",{\"1\":{\"70\":1}}],[\"对每个\",{\"1\":{\"68\":1}}],[\"对每个点独立处理\",{\"1\":{\"37\":1}}],[\"对每个点进行特征提取\",{\"1\":{\"34\":1}}],[\"对每个尺度的局部点集应用对应的\",{\"1\":{\"25\":1}}],[\"对每个半径\",{\"1\":{\"25\":1}}],[\"对每个局部区域内所有点的最大响应值进行池化\",{\"1\":{\"21\":1}}],[\"对每个查询点的邻近点按索引排序\",{\"1\":{\"21\":1}}],[\"对每个样本单独处理\",{\"1\":{\"10\":1}}],[\"对话系统中的候选回复选择\",{\"1\":{\"70\":1}}],[\"对比\",{\"1\":{\"68\":1}}],[\"对异常点鲁棒性差\",{\"1\":{\"37\":1}}],[\"对异常点也有一定容忍能力\",{\"1\":{\"30\":1}}],[\"对少量异常点有一定鲁棒性\",{\"1\":{\"37\":1}}],[\"对噪声点敏感\",{\"1\":{\"37\":2}}],[\"对稀疏点云敏感\",{\"1\":{\"37\":1}}],[\"对局部形状变化敏感\",{\"1\":{\"37\":1}}],[\"对原始点云做刚性变换\",{\"1\":{\"33\":1}}],[\"对齐\",{\"1\":{\"32\":1}}],[\"对几何变换的不变性\",{\"1\":{\"29\":1,\"30\":1}}],[\"对应维度为\",{\"1\":{\"110\":1}}],[\"对应\",{\"1\":{\"109\":1}}],[\"对应的伪代码实现如下所示\",{\"1\":{\"90\":1}}],[\"对应的id为\",{\"1\":{\"46\":1}}],[\"对应的参数为\",{\"1\":{\"45\":1}}],[\"对应的注意力掩码\",{\"1\":{\"10\":1}}],[\"对应代码如下\",{\"1\":{\"45\":1}}],[\"对应一个\",{\"1\":{\"32\":1}}],[\"对应半径下最多取多少邻近点\",{\"1\":{\"25\":1}}],[\"对参数选择依赖性高\",{\"1\":{\"19\":1}}],[\"对上述得到的每个区域进行编码\",{\"1\":{\"16\":1}}],[\"对点云的旋转\",{\"1\":{\"30\":1}}],[\"对点云密度变换较为敏感\",{\"1\":{\"19\":1}}],[\"对点云数据做平移操作后\",{\"1\":{\"15\":1}}],[\"对点积结果进行缩放\",{\"1\":{\"8\":1}}],[\"对于二维的图像\",{\"1\":{\"109\":1}}],[\"对于q\",{\"1\":{\"100\":1}}],[\"对于自监督模型\",{\"1\":{\"96\":1}}],[\"对于有监督模型\",{\"1\":{\"96\":1}}],[\"对于vit\",{\"1\":{\"90\":1}}],[\"对于一个包含个文本\",{\"1\":{\"90\":1}}],[\"对于这样的多选问题\",{\"1\":{\"70\":1}}],[\"对于分类任务来说\",{\"1\":{\"55\":1}}],[\"对于字典中不存在的词\",{\"1\":{\"46\":1}}],[\"对于非刚性变形\",{\"1\":{\"37\":1}}],[\"对于每个局部区域\",{\"1\":{\"25\":1}}],[\"对于每个质心点\",{\"1\":{\"24\":1}}],[\"对于每个选中的关键点\",{\"1\":{\"21\":1}}],[\"对于某个形心\",{\"1\":{\"19\":1}}],[\"对于单个的物体还好\",{\"1\":{\"15\":1}}],[\"对于点云中的每一个点\",{\"1\":{\"8\":1}}],[\"对于图像中的每一个位置\",{\"1\":{\"8\":1}}],[\"对语言指令进行分词\",{\"1\":{\"7\":1}}],[\"对自然语言指令进行\",{\"1\":{\"7\":1}}],[\"和值\",{\"1\":{\"113\":2}}],[\"和transformer中的一样\",{\"1\":{\"112\":1}}],[\"和text都能和所有的tokens\",{\"1\":{\"102\":1}}],[\"和冻结参数的\",{\"1\":{\"104\":1}}],[\"和文本\",{\"1\":{\"103\":1}}],[\"和基于图像掩码的方法\",{\"1\":{\"96\":1}}],[\"和图像编码器\",{\"1\":{\"90\":1}}],[\"和稀疏采样的区域\",{\"1\":{\"24\":1}}],[\"和多分辨率分组\",{\"1\":{\"23\":1}}],[\"和每个查询点上\",{\"1\":{\"21\":1}}],[\"和通道维度\",{\"1\":{\"12\":1}}],[\"和\",{\"1\":{\"7\":3,\"9\":1,\"12\":5,\"17\":1,\"32\":1,\"33\":1,\"64\":1,\"66\":2,\"67\":2,\"68\":1,\"79\":1,\"88\":3,\"98\":3,\"101\":3,\"103\":2,\"109\":1,\"117\":2}}],[\"和自注意力机制融合图像与点云特征\",{\"1\":{\"7\":1}}],[\"pwd=vket\",{\"1\":{\"118\":1}}],[\"pwd=qvmq\",{\"1\":{\"107\":1}}],[\"plot\",{\"1\":{\"107\":2}}],[\"plt\",{\"1\":{\"94\":4,\"95\":5,\"107\":7}}],[\"penalty=1\",{\"1\":{\"104\":1}}],[\"person\",{\"1\":{\"91\":1}}],[\"per\",{\"1\":{\"45\":2,\"113\":6}}],[\"permutation\",{\"1\":{\"29\":1,\"40\":2}}],[\"permute\",{\"1\":{\"8\":2,\"12\":1,\"21\":4,\"25\":4,\"57\":2,\"101\":1,\"103\":1,\"113\":2}}],[\"p采样\",{\"1\":{\"104\":1}}],[\"pil\",{\"1\":{\"95\":1,\"107\":1,\"108\":2}}],[\"pip\",{\"1\":{\"72\":1}}],[\"png\",{\"1\":{\"93\":1,\"95\":1,\"107\":2}}],[\"photos下的子目录名作为我们的候选待匹配分类文本列表\",{\"1\":{\"93\":1}}],[\"photos\",{\"1\":{\"93\":4,\"95\":2}}],[\"photos目录下读取出所有图片的路径\",{\"1\":{\"93\":1}}],[\"photo\",{\"1\":{\"91\":2,\"92\":2,\"93\":2,\"94\":1,\"95\":2}}],[\"phi\",{\"1\":{\"8\":7}}],[\"p=drop\",{\"1\":{\"111\":1,\"114\":1}}],[\"p=dropout\",{\"1\":{\"84\":1}}],[\"p=top\",{\"1\":{\"104\":1}}],[\"p=0\",{\"1\":{\"35\":1,\"104\":1}}],[\"print\",{\"1\":{\"93\":6,\"94\":4,\"95\":10,\"107\":3,\"118\":1}}],[\"pred\",{\"1\":{\"114\":5}}],[\"predicted\",{\"1\":{\"93\":6,\"95\":6}}],[\"predict\",{\"1\":{\"92\":1}}],[\"predictions\",{\"1\":{\"63\":2}}],[\"prediction\",{\"1\":{\"46\":1,\"63\":2,\"64\":4,\"96\":1,\"103\":6}}],[\"present\",{\"1\":{\"103\":2}}],[\"pretrained\",{\"1\":{\"93\":4,\"95\":4}}],[\"pretrainedtokenizer\",{\"1\":{\"46\":1}}],[\"pre\",{\"1\":{\"89\":1,\"92\":1,\"114\":3,\"118\":2}}],[\"precomputed\",{\"1\":{\"57\":1}}],[\"prev\",{\"1\":{\"45\":1}}],[\"prompts\",{\"1\":{\"104\":1}}],[\"prompting\",{\"1\":{\"92\":1}}],[\"prompt\",{\"1\":{\"92\":3}}],[\"processor\",{\"1\":{\"93\":3,\"95\":3}}],[\"processing\",{\"1\":{\"92\":1}}],[\"process\",{\"1\":{\"75\":1}}],[\"probabilities\",{\"1\":{\"57\":1}}],[\"probs\",{\"1\":{\"57\":5,\"91\":3,\"103\":7}}],[\"prob\",{\"1\":{\"49\":1,\"51\":1,\"55\":1,\"57\":1,\"58\":1,\"69\":1,\"70\":1}}],[\"product\",{\"1\":{\"40\":1,\"57\":1,\"84\":1}}],[\"projects\",{\"1\":{\"97\":1}}],[\"projected\",{\"1\":{\"84\":1}}],[\"projections\",{\"1\":{\"84\":1}}],[\"projection\",{\"1\":{\"84\":1}}],[\"proj\",{\"1\":{\"12\":32,\"76\":2,\"90\":2,\"100\":2,\"109\":2,\"112\":1,\"113\":6}}],[\"pyplot\",{\"1\":{\"95\":1}}],[\"py文件\",{\"1\":{\"45\":1}}],[\"python\",{\"1\":{\"45\":1,\"107\":1}}],[\"python=3\",{\"1\":{\"45\":1,\"72\":1}}],[\"pytorch版本\",{\"0\":{\"31\":1}}],[\"pytorch\",{\"1\":{\"14\":2,\"27\":2,\"45\":3,\"67\":1,\"107\":2,\"108\":2,\"118\":1}}],[\"py\",{\"1\":{\"22\":1,\"25\":1,\"45\":2,\"107\":1}}],[\"ported\",{\"1\":{\"118\":1}}],[\"portrait\",{\"1\":{\"91\":1}}],[\"portion\",{\"1\":{\"46\":1}}],[\"pos\",{\"1\":{\"69\":1,\"111\":5,\"114\":5}}],[\"positional\",{\"1\":{\"74\":1,\"111\":1}}],[\"positions\",{\"1\":{\"67\":7,\"68\":1}}],[\"positions=none\",{\"1\":{\"67\":2}}],[\"position\",{\"1\":{\"21\":5,\"25\":2,\"49\":12,\"54\":2,\"55\":2,\"64\":2,\"67\":2,\"69\":2,\"70\":6,\"102\":9,\"103\":2}}],[\"pooler\",{\"1\":{\"54\":2}}],[\"pooled\",{\"1\":{\"53\":4,\"54\":2,\"55\":4,\"63\":2,\"64\":2,\"66\":2,\"70\":4}}],[\"pool\",{\"1\":{\"53\":1}}],[\"pooling\",{\"1\":{\"21\":1,\"30\":5,\"32\":1,\"37\":7,\"40\":3}}],[\"pointcnn\",{\"1\":{\"37\":1}}],[\"pointcloud\",{\"1\":{\"21\":2}}],[\"pointfeat\",{\"1\":{\"34\":2}}],[\"pointnetdensecls\",{\"1\":{\"36\":3}}],[\"pointnetcls\",{\"1\":{\"35\":3}}],[\"pointnetfeat\",{\"1\":{\"34\":3,\"35\":4,\"36\":3}}],[\"pointnet网络模型结构图\",{\"1\":{\"31\":1}}],[\"pointnet后\",{\"1\":{\"22\":3}}],[\"pointnetsetabstractionmsg\",{\"1\":{\"25\":5}}],[\"pointnetsetabstraction\",{\"1\":{\"21\":3,\"22\":3,\"25\":1}}],[\"pointnet来提取局部区域中的特征\",{\"1\":{\"20\":1}}],[\"pointnet将局部区域编码为特征向量\",{\"1\":{\"17\":1}}],[\"pointnet\",{\"0\":{\"20\":1},\"1\":{\"14\":1,\"16\":1,\"17\":2,\"21\":2,\"25\":1,\"27\":2,\"28\":1,\"30\":4,\"32\":4,\"33\":2,\"34\":1,\"35\":1,\"36\":1,\"37\":26,\"40\":2}}],[\"pointnet2\",{\"1\":{\"14\":2,\"22\":1,\"25\":2}}],[\"pointnet++提出了密度自适应pointnet层\",{\"1\":{\"23\":1}}],[\"pointnet++应用pointnet递归地对输入集进行嵌套分区\",{\"1\":{\"15\":1}}],[\"pointnet++选择pointnet作为局部特征学习器\",{\"1\":{\"15\":1}}],[\"pointnet++在进行点集划分时\",{\"1\":{\"15\":1}}],[\"pointnet++的下一个任务是学习这些子集\",{\"1\":{\"15\":1}}],[\"pointnet++需要一种方法来有效地将点云分割成多个部分\",{\"1\":{\"15\":1}}],[\"pointnet++\",{\"1\":{\"7\":1,\"15\":1,\"21\":3,\"22\":2,\"25\":2,\"37\":4}}],[\"points\",{\"1\":{\"7\":1,\"21\":68,\"22\":7,\"25\":39,\"29\":2,\"32\":1}}],[\"point\",{\"1\":{\"7\":8,\"8\":6,\"18\":1,\"21\":7,\"25\":2,\"30\":1,\"32\":2,\"37\":2,\"39\":1}}],[\"p+n\",{\"1\":{\"8\":1}}],[\"pan\",{\"1\":{\"107\":1,\"118\":1}}],[\"past\",{\"1\":{\"102\":3,\"103\":27}}],[\"pass\",{\"1\":{\"80\":1}}],[\"passed\",{\"1\":{\"46\":1}}],[\"page\",{\"1\":{\"91\":1}}],[\"paper\",{\"1\":{\"57\":1,\"118\":1}}],[\"para\",{\"1\":{\"118\":2}}],[\"param\",{\"1\":{\"107\":5,\"109\":6}}],[\"parameter\",{\"1\":{\"62\":1,\"90\":1,\"110\":1,\"111\":2,\"114\":2}}],[\"parameters\",{\"1\":{\"54\":1,\"118\":1}}],[\"partial\",{\"1\":{\"114\":1}}],[\"partitioning\",{\"1\":{\"15\":2}}],[\"parts\",{\"1\":{\"69\":1}}],[\"paris\",{\"1\":{\"68\":3}}],[\"pair是否match\",{\"1\":{\"102\":1}}],[\"pair\",{\"1\":{\"46\":2}}],[\"pad\",{\"1\":{\"46\":3,\"103\":1,\"104\":2}}],[\"padding=true\",{\"1\":{\"93\":1,\"95\":1}}],[\"padding=\",{\"1\":{\"7\":1,\"100\":1}}],[\"padding\",{\"1\":{\"7\":2,\"10\":3,\"46\":6,\"49\":1,\"54\":1,\"55\":1,\"74\":1,\"100\":1,\"103\":6}}],[\"path=val\",{\"1\":{\"108\":1}}],[\"path=train\",{\"1\":{\"108\":1}}],[\"path=prev\",{\"1\":{\"45\":1}}],[\"paths\",{\"1\":{\"93\":17,\"94\":6,\"95\":21}}],[\"path\",{\"1\":{\"93\":12,\"94\":2,\"95\":14,\"107\":30,\"108\":5,\"111\":1,\"112\":7,\"114\":2}}],[\"path$bert\",{\"1\":{\"45\":2}}],[\"patch16\",{\"1\":{\"118\":6}}],[\"patch14\",{\"1\":{\"93\":2,\"95\":1}}],[\"patchembed\",{\"1\":{\"109\":2}}],[\"patches\",{\"1\":{\"15\":1,\"109\":5,\"110\":6,\"111\":4,\"113\":11,\"114\":4,\"118\":1,\"119\":1}}],[\"patch\",{\"1\":{\"8\":5,\"109\":12,\"110\":11,\"111\":5,\"114\":5,\"118\":1}}],[\"p\",{\"1\":{\"8\":15,\"12\":6,\"84\":5,\"104\":1}}],[\"pth\",{\"1\":{\"118\":1}}],[\"pts\",{\"1\":{\"34\":2,\"36\":2}}],[\"pt\",{\"1\":{\"7\":1,\"93\":2,\"95\":2,\"100\":1}}],[\"的局部特征提取能力快速捕捉图像的底层特征\",{\"1\":{\"117\":1}}],[\"的缩写\",{\"1\":{\"115\":2}}],[\"的隐藏维度\",{\"1\":{\"112\":1}}],[\"的隐藏状态\",{\"1\":{\"66\":1}}],[\"的图像块矩阵添加二维\",{\"1\":{\"111\":1}}],[\"的交互\",{\"1\":{\"110\":1}}],[\"的形式\",{\"1\":{\"109\":1}}],[\"的形状为\",{\"1\":{\"12\":1}}],[\"的张量\",{\"1\":{\"108\":2,\"109\":2}}],[\"的概率随机水平翻转图像\",{\"1\":{\"108\":1}}],[\"的generate方法负责完成图像描述生成\",{\"1\":{\"104\":1}}],[\"的text\",{\"1\":{\"104\":1}}],[\"的文本生成能力\",{\"1\":{\"104\":1}}],[\"的文本对\",{\"1\":{\"66\":1}}],[\"的真实值\",{\"1\":{\"103\":1}}],[\"的预测\",{\"1\":{\"103\":1}}],[\"的向量映射到词汇表空间\",{\"1\":{\"103\":1}}],[\"的能力\",{\"1\":{\"103\":1}}],[\"的完整计算流程\",{\"1\":{\"102\":1}}],[\"的相似度\",{\"1\":{\"101\":1}}],[\"的zero\",{\"1\":{\"96\":1}}],[\"的架构中融入卷积操作\",{\"1\":{\"117\":1}}],[\"的架构\",{\"1\":{\"93\":1}}],[\"的性能提升\",{\"1\":{\"92\":1}}],[\"的性能会显著下降\",{\"1\":{\"37\":1}}],[\"的效果\",{\"1\":{\"92\":1}}],[\"的vit\",{\"1\":{\"90\":1}}],[\"的出现也掀起了新一轮的研究热潮\",{\"1\":{\"88\":1}}],[\"的研究如潮水般涌来\",{\"1\":{\"88\":1}}],[\"的神经网络架构\",{\"1\":{\"74\":1}}],[\"的输入\",{\"1\":{\"103\":1}}],[\"的输入序列\",{\"1\":{\"70\":1}}],[\"的输入方式是\",{\"1\":{\"70\":1}}],[\"的输入组织形式与普通分类或问答任务略有不同\",{\"1\":{\"70\":1}}],[\"的输出向量能够很好地表示图像的全局特征\",{\"1\":{\"110\":2}}],[\"的输出向量被输入到分类头中\",{\"1\":{\"110\":1}}],[\"的输出蕴含了视觉信息\",{\"1\":{\"104\":1}}],[\"的输出\",{\"1\":{\"66\":1,\"104\":1,\"110\":1}}],[\"的输出仅由一个不超过\",{\"1\":{\"37\":1}}],[\"的区域\",{\"1\":{\"68\":1}}],[\"的角色进行分类\",{\"1\":{\"68\":1}}],[\"的模型\",{\"1\":{\"68\":1}}],[\"的模块\",{\"1\":{\"32\":1}}],[\"的位置\",{\"1\":{\"66\":1}}],[\"的序列\",{\"1\":{\"66\":1}}],[\"的问答任务中\",{\"1\":{\"66\":1}}],[\"的最大缺陷在于它\",{\"1\":{\"37\":1}}],[\"的优势\",{\"1\":{\"37\":1,\"117\":1}}],[\"的主要缺陷\",{\"1\":{\"37\":1}}],[\"的时间复杂度虽然是\",{\"1\":{\"37\":1}}],[\"的实验\",{\"1\":{\"37\":1}}],[\"的限制\",{\"1\":{\"37\":1}}],[\"的表示\",{\"1\":{\"66\":1}}],[\"的表达能力受\",{\"1\":{\"37\":1}}],[\"的表现不如基于图结构的模型\",{\"1\":{\"37\":1}}],[\"的全局特征来自于\",{\"1\":{\"37\":1}}],[\"的分类准确率略低于\",{\"1\":{\"37\":1}}],[\"的分类模块\",{\"1\":{\"35\":1}}],[\"的分割网络将全局特征复制\",{\"1\":{\"37\":1}}],[\"的分割模块通过拼接全局特征\",{\"1\":{\"37\":1}}],[\"的分割模块\",{\"1\":{\"36\":1}}],[\"的核心\",{\"1\":{\"74\":1}}],[\"的核心特征提取模块\",{\"1\":{\"34\":1}}],[\"的核心就是逐层提取局部特征\",{\"1\":{\"22\":1}}],[\"的作用是通过训练过程中损失值的降低\",{\"1\":{\"110\":1}}],[\"的作用\",{\"1\":{\"33\":1}}],[\"的矩阵\",{\"1\":{\"33\":1,\"109\":1}}],[\"的方式\",{\"1\":{\"32\":1}}],[\"的方式重构图像\",{\"1\":{\"8\":1}}],[\"的大小\",{\"1\":{\"32\":1}}],[\"的变换矩阵\",{\"1\":{\"32\":2}}],[\"的创新点\",{\"1\":{\"28\":1}}],[\"的第\",{\"1\":{\"21\":1}}],[\"的结构过于简单\",{\"1\":{\"37\":1}}],[\"的结构\",{\"1\":{\"21\":1}}],[\"的点缺失\",{\"1\":{\"30\":1}}],[\"的点集群都将独立地送入对应的pointnet网络进行特征提取\",{\"1\":{\"24\":1}}],[\"的点及其特征\",{\"1\":{\"21\":1}}],[\"的点全部替换为\",{\"1\":{\"21\":1}}],[\"的点\",{\"1\":{\"21\":1}}],[\"的索引数组\",{\"1\":{\"21\":1}}],[\"的索引\",{\"1\":{\"21\":1}}],[\"的所有邻近点\",{\"1\":{\"21\":1}}],[\"的特征图\",{\"1\":{\"109\":1}}],[\"的特征进行汇总\",{\"1\":{\"26\":1}}],[\"的特征\",{\"1\":{\"15\":1}}],[\"的列表\",{\"1\":{\"10\":1}}],[\"的语义\",{\"1\":{\"8\":2}}],[\"的损失\",{\"1\":{\"7\":1}}],[\"的\",{\"1\":{\"7\":2,\"12\":5,\"22\":1,\"25\":1,\"46\":1,\"66\":1,\"68\":1,\"103\":9}}],[\"4替换为相应的类别名称\",{\"1\":{\"107\":1}}],[\"404\",{\"1\":{\"122\":1}}],[\"400m数据\",{\"1\":{\"98\":1}}],[\"4096\",{\"1\":{\"12\":1}}],[\"48\",{\"1\":{\"93\":1}}],[\"4873\",{\"1\":{\"46\":2}}],[\"4788\",{\"1\":{\"46\":2}}],[\"4960\",{\"1\":{\"46\":2}}],[\"4638\",{\"1\":{\"46\":2}}],[\"4\",{\"0\":{\"111\":1},\"1\":{\"7\":1,\"8\":1,\"22\":3,\"25\":3,\"30\":1,\"49\":1,\"70\":1,\"84\":1,\"103\":1,\"104\":1,\"113\":1,\"118\":1}}],[\"qk\",{\"1\":{\"111\":1,\"112\":2,\"113\":2,\"114\":2}}],[\"qkv\",{\"1\":{\"111\":1,\"112\":2,\"113\":8,\"114\":2}}],[\"q来自query\",{\"1\":{\"103\":1}}],[\"q2t\",{\"1\":{\"101\":2}}],[\"qa\",{\"1\":{\"66\":2,\"67\":2,\"68\":1}}],[\"queies\",{\"1\":{\"103\":1}}],[\"queries被用来从image\",{\"1\":{\"100\":1}}],[\"queries是一组可学习的embeddings\",{\"1\":{\"100\":1}}],[\"queries\",{\"1\":{\"100\":3,\"101\":1,\"102\":1,\"104\":3}}],[\"query和query\",{\"1\":{\"101\":1}}],[\"query是基于欧氏距离的均匀性假设\",{\"1\":{\"19\":1}}],[\"query通过确保每个局部区域都有一个固定的尺度\",{\"1\":{\"19\":1}}],[\"query通过固定区域尺度而不是固定邻居数量来定义邻域\",{\"1\":{\"19\":1}}],[\"query找到该查询点在半径为𝑟范围内点\",{\"1\":{\"19\":1}}],[\"query来查询形心的邻居点\",{\"1\":{\"19\":1}}],[\"query\",{\"1\":{\"12\":13,\"21\":7,\"25\":2,\"57\":7,\"84\":9,\"94\":4,\"95\":4,\"100\":8,\"101\":2,\"102\":17,\"103\":30,\"104\":5}}],[\"question\",{\"1\":{\"66\":1}}],[\"q\",{\"1\":{\"7\":1,\"12\":9,\"84\":1,\"100\":3,\"103\":2,\"104\":3,\"113\":5}}],[\"qformer\",{\"1\":{\"7\":1,\"100\":2,\"102\":1,\"103\":3,\"104\":1}}],[\"rwightman\",{\"1\":{\"118\":1}}],[\"root\",{\"1\":{\"93\":2,\"95\":2,\"107\":8}}],[\"rocket\",{\"1\":{\"91\":1}}],[\"rn50x16和rnx64\",{\"1\":{\"90\":1}}],[\"rn50x4\",{\"1\":{\"90\":1}}],[\"rnn等模型的缺点是需要顺序计算\",{\"1\":{\"73\":1}}],[\"rnn\",{\"1\":{\"30\":1}}],[\"r\",{\"1\":{\"72\":1}}],[\"run\",{\"1\":{\"45\":1}}],[\"rgb为彩色图片\",{\"1\":{\"107\":1}}],[\"rgb\",{\"1\":{\"39\":1,\"93\":1,\"95\":1,\"107\":3,\"109\":1}}],[\"rigid\",{\"1\":{\"33\":1,\"41\":1}}],[\"right\",{\"1\":{\"7\":1,\"82\":1}}],[\"ratio\",{\"1\":{\"111\":1,\"112\":7,\"113\":2,\"114\":4}}],[\"ratio=dpr\",{\"1\":{\"114\":1}}],[\"ratio=drop\",{\"1\":{\"112\":1,\"114\":1}}],[\"ratio=mlp\",{\"1\":{\"114\":1}}],[\"ratio=attn\",{\"1\":{\"112\":1,\"114\":1}}],[\"ratio=0\",{\"1\":{\"111\":3,\"112\":3,\"113\":2,\"114\":3}}],[\"ratio=4\",{\"1\":{\"111\":1,\"112\":1,\"114\":1}}],[\"rate\",{\"1\":{\"107\":2}}],[\"rate=2e\",{\"1\":{\"45\":1}}],[\"raise\",{\"1\":{\"107\":1}}],[\"raw\",{\"1\":{\"57\":1}}],[\"rank\",{\"1\":{\"48\":1}}],[\"randomhorizontalflip\",{\"1\":{\"108\":1}}],[\"randomresizedcrop\",{\"1\":{\"108\":2}}],[\"randomsampler\",{\"1\":{\"48\":1}}],[\"random\",{\"1\":{\"24\":1,\"107\":2}}],[\"randint\",{\"1\":{\"21\":1}}],[\"range\",{\"1\":{\"10\":1,\"21\":1,\"25\":2,\"52\":1,\"93\":1,\"95\":1,\"102\":2,\"103\":1,\"107\":2,\"114\":1}}],[\"radius=none\",{\"1\":{\"22\":1}}],[\"radius=0\",{\"1\":{\"22\":2}}],[\"radius^2\",{\"1\":{\"21\":1}}],[\"radius\",{\"1\":{\"21\":12,\"25\":7}}],[\"requires\",{\"1\":{\"118\":1}}],[\"requirements\",{\"1\":{\"72\":2}}],[\"reduction\",{\"1\":{\"103\":1}}],[\"reduction=reduction\",{\"1\":{\"103\":1}}],[\"reduction=\",{\"1\":{\"103\":1}}],[\"red\",{\"1\":{\"91\":1}}],[\"repetition\",{\"1\":{\"104\":1}}],[\"repeat\",{\"1\":{\"21\":6,\"32\":1,\"34\":1,\"104\":1}}],[\"repo\",{\"1\":{\"93\":1,\"95\":1}}],[\"replace\",{\"1\":{\"93\":1,\"95\":1}}],[\"representation\",{\"0\":{\"100\":1},\"1\":{\"68\":1,\"100\":3,\"101\":2,\"102\":2,\"111\":1,\"114\":3,\"118\":1}}],[\"releases\",{\"1\":{\"118\":1}}],[\"related\",{\"1\":{\"112\":1}}],[\"relationship\",{\"1\":{\"63\":4,\"64\":4}}],[\"relu\",{\"1\":{\"8\":2,\"9\":1,\"11\":1,\"12\":1,\"21\":2,\"22\":2,\"25\":5,\"32\":8,\"34\":2,\"35\":4,\"36\":3}}],[\"read\",{\"1\":{\"45\":9,\"107\":1,\"108\":1}}],[\"register\",{\"1\":{\"103\":1}}],[\"region\",{\"1\":{\"21\":2}}],[\"regression\",{\"1\":{\"55\":1}}],[\"regularizer\",{\"1\":{\"33\":3}}],[\"reg\",{\"1\":{\"30\":1}}],[\"rendering\",{\"1\":{\"28\":1}}],[\"research\",{\"1\":{\"118\":1}}],[\"resize\",{\"1\":{\"108\":2}}],[\"residual\",{\"1\":{\"74\":1}}],[\"resnet和混合模型的效果均不如vit模型\",{\"1\":{\"117\":1}}],[\"resnet和混合模型在不同图像分类数据集上的测试结果\",{\"1\":{\"117\":1}}],[\"resnet101\",{\"1\":{\"90\":1}}],[\"resnet18\",{\"1\":{\"7\":1}}],[\"resnet50\",{\"1\":{\"90\":1}}],[\"resnet包含五种不同尺寸的模型\",{\"1\":{\"90\":1}}],[\"resnet\",{\"1\":{\"90\":1,\"117\":1}}],[\"resolution\",{\"0\":{\"26\":1},\"1\":{\"23\":1}}],[\"reshaped\",{\"1\":{\"70\":4}}],[\"reshape\",{\"1\":{\"21\":1,\"32\":1,\"70\":1,\"113\":4}}],[\"returnfps\",{\"1\":{\"21\":1}}],[\"returnfps=false\",{\"1\":{\"21\":1}}],[\"returns\",{\"1\":{\"10\":1,\"12\":1,\"46\":1}}],[\"return\",{\"1\":{\"7\":4,\"8\":1,\"10\":1,\"12\":2,\"21\":13,\"22\":1,\"25\":3,\"32\":1,\"33\":1,\"34\":2,\"35\":1,\"36\":1,\"46\":6,\"48\":1,\"49\":1,\"51\":3,\"52\":1,\"53\":1,\"54\":1,\"55\":1,\"57\":2,\"58\":1,\"59\":1,\"61\":1,\"62\":1,\"63\":1,\"64\":1,\"67\":1,\"69\":1,\"70\":1,\"75\":3,\"76\":1,\"78\":1,\"79\":1,\"80\":1,\"82\":1,\"83\":1,\"84\":2,\"93\":9,\"94\":4,\"95\":13,\"100\":3,\"102\":2,\"103\":10,\"104\":1,\"107\":7,\"109\":2,\"110\":2,\"111\":2,\"112\":2,\"113\":1,\"114\":2,\"118\":1}}],[\"rb×cp×np\",{\"1\":{\"7\":1}}],[\"rb×ci×h×w\",{\"1\":{\"7\":1}}],[\"∈\",{\"1\":{\"7\":2}}],[\"获取输入张量x的形状\",{\"1\":{\"113\":1}}],[\"获取输入图像张量的形状\",{\"1\":{\"109\":1}}],[\"获取输入维度信息\",{\"1\":{\"7\":1}}],[\"获取对应图像的标签\",{\"1\":{\"107\":1}}],[\"获取该类别对应的索引\",{\"1\":{\"107\":1}}],[\"获取其对应的图像嵌入向量列表\",{\"1\":{\"93\":1}}],[\"获取候选分类名列表\",{\"1\":{\"93\":1,\"95\":1}}],[\"获取选项个数\",{\"1\":{\"70\":1}}],[\"获取当前\",{\"1\":{\"32\":1}}],[\"获取当前样本中多模态嵌入的维度信息\",{\"1\":{\"10\":1}}],[\"获取全局区域特征向量后\",{\"1\":{\"22\":1}}],[\"获取两个注意力加权结果\",{\"1\":{\"12\":1}}],[\"获取\",{\"1\":{\"10\":1,\"12\":1}}],[\"获取语言嵌入\",{\"1\":{\"7\":1}}],[\"获取设备信息\",{\"1\":{\"7\":1}}],[\"=>\",{\"1\":{\"84\":1}}],[\"=================\",{\"1\":{\"103\":1}}],[\"========================\",{\"1\":{\"103\":1}}],[\"===================\",{\"1\":{\"101\":1,\"102\":1}}],[\"==============\",{\"1\":{\"101\":1,\"102\":1}}],[\"==\",{\"1\":{\"7\":1,\"21\":1,\"48\":1,\"51\":1,\"55\":1,\"61\":1,\"68\":1,\"69\":1,\"72\":1,\"84\":2,\"93\":1,\"95\":1,\"102\":1,\"103\":2}}],[\"=\",{\"1\":{\"7\":20,\"8\":24,\"9\":1,\"10\":7,\"11\":1,\"12\":36,\"21\":63,\"22\":24,\"25\":53,\"30\":2,\"32\":25,\"33\":5,\"34\":30,\"35\":13,\"36\":20,\"41\":1,\"46\":14,\"48\":7,\"49\":16,\"51\":17,\"52\":2,\"53\":5,\"54\":10,\"55\":14,\"57\":25,\"58\":6,\"59\":4,\"61\":7,\"62\":5,\"63\":4,\"64\":11,\"66\":9,\"67\":17,\"68\":2,\"69\":16,\"70\":17,\"75\":5,\"76\":1,\"78\":2,\"79\":6,\"80\":3,\"82\":9,\"83\":4,\"84\":15,\"90\":9,\"91\":10,\"93\":40,\"94\":9,\"95\":46,\"100\":10,\"101\":8,\"102\":40,\"103\":56,\"104\":10,\"107\":25,\"108\":6,\"109\":11,\"110\":12,\"111\":15,\"112\":19,\"113\":16,\"114\":28,\"115\":2,\"118\":5}}],[\"flatten\",{\"1\":{\"109\":1}}],[\"flag\",{\"1\":{\"91\":1}}],[\"flowerclassify\",{\"1\":{\"93\":1,\"95\":1}}],[\"flower\",{\"1\":{\"93\":4,\"95\":2,\"107\":8}}],[\"float\",{\"1\":{\"91\":2,\"107\":1}}],[\"float32\",{\"1\":{\"32\":1}}],[\"feed\",{\"1\":{\"74\":1,\"79\":4,\"82\":5}}],[\"feat相似度最大的那个query\",{\"1\":{\"101\":2}}],[\"feats\",{\"1\":{\"100\":1,\"101\":5}}],[\"feat=false\",{\"1\":{\"34\":2,\"36\":1}}],[\"feat=true\",{\"1\":{\"34\":2,\"35\":1}}],[\"feat\",{\"1\":{\"34\":10,\"35\":4,\"36\":4,\"100\":1,\"101\":8}}],[\"features=none\",{\"1\":{\"112\":2}}],[\"features=mlp\",{\"1\":{\"112\":1}}],[\"features=dim\",{\"1\":{\"112\":1}}],[\"features\",{\"1\":{\"7\":1,\"46\":1,\"91\":10,\"93\":4,\"95\":4,\"96\":1,\"110\":3,\"111\":3,\"112\":15,\"114\":5}}],[\"feature\",{\"1\":{\"7\":18,\"8\":19,\"12\":9,\"15\":2,\"21\":1,\"25\":1,\"33\":3,\"34\":5,\"35\":4,\"36\":3}}],[\"france\",{\"1\":{\"68\":2}}],[\"frozen\",{\"1\":{\"100\":1}}],[\"frobenius\",{\"1\":{\"33\":2}}],[\"from\",{\"1\":{\"32\":1,\"46\":2,\"57\":1,\"84\":1,\"93\":2,\"94\":2,\"95\":8,\"96\":2,\"100\":1,\"107\":2,\"118\":2}}],[\"fp16\",{\"1\":{\"54\":1}}],[\"fps是一种在点云\",{\"1\":{\"18\":1}}],[\"fps\",{\"1\":{\"18\":1,\"21\":7,\"25\":1}}],[\"fn=val\",{\"1\":{\"108\":1}}],[\"fn=train\",{\"1\":{\"108\":1}}],[\"fn=collate\",{\"1\":{\"48\":1}}],[\"fn负责对返回的一个batch\",{\"1\":{\"48\":1}}],[\"fn\",{\"1\":{\"48\":2,\"51\":3,\"61\":3,\"107\":2,\"108\":2}}],[\"find\",{\"1\":{\"94\":2,\"95\":2}}],[\"finetune\",{\"1\":{\"90\":1}}],[\"final\",{\"1\":{\"84\":1}}],[\"filterwarnings\",{\"1\":{\"95\":1}}],[\"file\",{\"1\":{\"93\":5,\"95\":5,\"107\":2}}],[\"files\",{\"1\":{\"93\":2,\"95\":2}}],[\"file$bert\",{\"1\":{\"45\":1}}],[\"fill\",{\"1\":{\"84\":1,\"103\":1,\"104\":1}}],[\"figure\",{\"1\":{\"79\":1,\"82\":1,\"84\":1}}],[\"first\",{\"1\":{\"21\":2,\"46\":2,\"53\":3}}],[\"fstn\",{\"1\":{\"34\":2}}],[\"f^2\",{\"1\":{\"30\":1}}],[\"function\",{\"1\":{\"30\":1,\"40\":1,\"57\":1,\"114\":1}}],[\"fusion\",{\"1\":{\"7\":1,\"8\":1,\"12\":2}}],[\"fxia22\",{\"1\":{\"27\":1}}],[\"fct\",{\"1\":{\"55\":4,\"64\":3,\"67\":3,\"69\":3,\"70\":2,\"103\":2}}],[\"fc\",{\"1\":{\"32\":1,\"104\":1,\"114\":1}}],[\"fc3\",{\"1\":{\"22\":2,\"25\":2,\"32\":2,\"35\":2}}],[\"fc2\",{\"1\":{\"22\":2,\"25\":2,\"32\":2,\"35\":2,\"112\":2}}],[\"fc1\",{\"1\":{\"22\":2,\"25\":2,\"32\":2,\"35\":2,\"112\":2}}],[\"facial\",{\"1\":{\"91\":1}}],[\"farthest\",{\"1\":{\"21\":8,\"25\":1}}],[\"false\",{\"1\":{\"7\":1,\"34\":1,\"118\":1}}],[\"f\",{\"1\":{\"8\":2,\"21\":1,\"22\":3,\"25\":4,\"30\":2,\"32\":5,\"34\":2,\"35\":3,\"36\":4,\"76\":1,\"90\":4,\"93\":7,\"94\":4,\"95\":11,\"100\":2,\"101\":2,\"102\":3}}],[\"found\",{\"1\":{\"94\":1,\"95\":1,\"107\":1,\"122\":1}}],[\"follow\",{\"1\":{\"79\":1,\"82\":1}}],[\"following\",{\"1\":{\"46\":1}}],[\"focal\",{\"1\":{\"7\":1}}],[\"format\",{\"1\":{\"46\":1,\"107\":5,\"118\":1}}],[\"former的生成方法\",{\"1\":{\"104\":1}}],[\"former学习\",{\"1\":{\"103\":1}}],[\"former类比为一个self\",{\"1\":{\"100\":1}}],[\"former模块做模态融合\",{\"1\":{\"98\":1}}],[\"former\",{\"1\":{\"7\":1,\"100\":4,\"103\":2,\"104\":3}}],[\"for\",{\"1\":{\"10\":1,\"21\":3,\"25\":4,\"32\":1,\"52\":2,\"57\":5,\"62\":1,\"79\":1,\"80\":1,\"82\":1,\"83\":1,\"84\":1,\"91\":1,\"93\":6,\"94\":2,\"95\":8,\"102\":2,\"103\":8,\"107\":9,\"114\":2,\"118\":1}}],[\"forward\",{\"1\":{\"7\":1,\"8\":1,\"12\":2,\"21\":1,\"22\":1,\"25\":2,\"32\":1,\"34\":1,\"35\":1,\"36\":1,\"49\":1,\"51\":3,\"52\":1,\"53\":1,\"54\":1,\"55\":1,\"57\":2,\"58\":1,\"59\":1,\"61\":1,\"62\":1,\"63\":1,\"64\":1,\"67\":1,\"69\":1,\"70\":1,\"74\":1,\"75\":1,\"76\":1,\"78\":1,\"79\":5,\"80\":1,\"82\":6,\"83\":1,\"84\":1,\"100\":1,\"102\":1,\"103\":5,\"109\":1,\"110\":3,\"111\":3,\"112\":2,\"113\":1,\"114\":3}}],[\"f3d\",{\"1\":{\"7\":1}}],[\"f2d\",{\"1\":{\"7\":1}}],[\"t2i\",{\"1\":{\"101\":4,\"102\":5}}],[\"t2q\",{\"1\":{\"101\":2}}],[\"title\",{\"1\":{\"94\":1,\"95\":1,\"107\":1}}],[\"time\",{\"1\":{\"93\":10,\"95\":10}}],[\"turn\",{\"1\":{\"80\":1}}],[\"tuple\",{\"1\":{\"7\":1,\"66\":1,\"107\":1,\"110\":2,\"113\":1}}],[\"tgt\",{\"1\":{\"75\":12,\"82\":2,\"83\":2}}],[\"t5\",{\"1\":{\"68\":1}}],[\"tage\",{\"1\":{\"100\":1}}],[\"tabby\",{\"1\":{\"91\":1}}],[\"targets\",{\"1\":{\"101\":3}}],[\"target\",{\"1\":{\"75\":1}}],[\"taken\",{\"1\":{\"57\":1,\"93\":2,\"95\":1}}],[\"take\",{\"1\":{\"57\":1,\"75\":1,\"84\":1}}],[\"taking\",{\"1\":{\"53\":1}}],[\"tanh\",{\"1\":{\"53\":1,\"114\":2}}],[\"task\",{\"1\":{\"45\":1,\"46\":1}}],[\"two\",{\"1\":{\"46\":1,\"99\":2,\"105\":1}}],[\"type\",{\"1\":{\"46\":9,\"48\":4,\"49\":9,\"54\":3,\"55\":3,\"64\":3,\"66\":2,\"67\":3,\"68\":1,\"69\":3,\"70\":7,\"102\":1}}],[\"type=bert\",{\"1\":{\"45\":1}}],[\"tnews\",{\"1\":{\"45\":4}}],[\"tf\",{\"1\":{\"45\":2}}],[\"tree\",{\"1\":{\"97\":1}}],[\"try\",{\"1\":{\"93\":2,\"94\":1,\"95\":3}}],[\"tripod\",{\"1\":{\"91\":1}}],[\"training\",{\"1\":{\"89\":1,\"107\":1,\"118\":1}}],[\"train\",{\"1\":{\"45\":3,\"46\":1,\"48\":6,\"92\":1,\"107\":7,\"108\":7,\"114\":1}}],[\"trained\",{\"1\":{\"45\":2}}],[\"translation\",{\"1\":{\"105\":1}}],[\"transpose\",{\"1\":{\"33\":1,\"34\":4,\"36\":1,\"57\":5,\"84\":3,\"103\":8,\"109\":1,\"113\":4}}],[\"trans\",{\"1\":{\"33\":6,\"34\":11,\"35\":4,\"36\":4}}],[\"transforms\",{\"1\":{\"108\":10}}],[\"transform=data\",{\"1\":{\"108\":2}}],[\"transform=none\",{\"1\":{\"107\":1}}],[\"transform=feature\",{\"1\":{\"35\":1,\"36\":2}}],[\"transform=false\",{\"1\":{\"35\":1,\"36\":1}}],[\"transformation\",{\"1\":{\"33\":1}}],[\"transformations\",{\"1\":{\"29\":1}}],[\"transform\",{\"1\":{\"33\":3,\"34\":5,\"35\":3,\"36\":2,\"61\":3,\"62\":2,\"107\":5,\"108\":4}}],[\"transformer证明了使用transformer结构可以有效处理图像数据\",{\"1\":{\"119\":1}}],[\"transformer需要输入的是一维的token\",{\"1\":{\"109\":1}}],[\"transformer的核心流程实现\",{\"1\":{\"106\":1}}],[\"transformer的模型结构相比于transformer来说更简单\",{\"1\":{\"105\":1}}],[\"transformer是2021年谷歌在iclr上提出的算法\",{\"1\":{\"105\":1}}],[\"transformers\",{\"1\":{\"95\":1}}],[\"transformer\",{\"1\":{\"32\":1,\"37\":3,\"40\":1,\"57\":1,\"68\":1,\"72\":5,\"74\":4,\"88\":3,\"90\":3,\"93\":2,\"96\":1,\"99\":1,\"105\":1,\"110\":3,\"112\":1,\"117\":10,\"118\":2}}],[\"trunc\",{\"1\":{\"110\":1,\"111\":2,\"114\":2}}],[\"truncation=true\",{\"1\":{\"7\":1,\"100\":1}}],[\"truncation\",{\"1\":{\"7\":2}}],[\"true\",{\"1\":{\"7\":2,\"25\":1,\"34\":1,\"107\":1,\"118\":1}}],[\"t\",{\"1\":{\"28\":1,\"30\":2,\"32\":3,\"37\":2,\"90\":15,\"91\":2,\"93\":1,\"95\":1,\"107\":1}}],[\"through\",{\"1\":{\"80\":1}}],[\"this\",{\"1\":{\"10\":7,\"57\":1,\"91\":1}}],[\"these\",{\"1\":{\"67\":1}}],[\"there\",{\"1\":{\"62\":1}}],[\"they\",{\"1\":{\"55\":1}}],[\"theorem\",{\"1\":{\"37\":1}}],[\"theta\",{\"1\":{\"12\":4}}],[\"the\",{\"1\":{\"7\":1,\"46\":4,\"53\":3,\"57\":5,\"62\":3,\"67\":1,\"68\":2,\"69\":1,\"72\":1,\"80\":1,\"84\":2,\"91\":1,\"94\":1,\"95\":1,\"107\":1}}],[\"totensor\",{\"1\":{\"108\":2}}],[\"total\",{\"1\":{\"10\":2,\"64\":2,\"67\":2,\"93\":4,\"95\":4,\"113\":3}}],[\"topk\",{\"1\":{\"91\":1}}],[\"top\",{\"1\":{\"91\":2,\"104\":3}}],[\"torchscript\",{\"1\":{\"113\":1}}],[\"torch==1\",{\"1\":{\"72\":1}}],[\"torch\",{\"1\":{\"7\":1,\"8\":4,\"10\":6,\"12\":5,\"21\":12,\"25\":3,\"32\":6,\"33\":4,\"34\":7,\"36\":4,\"48\":1,\"49\":2,\"57\":2,\"62\":1,\"66\":4,\"72\":1,\"84\":2,\"91\":3,\"93\":4,\"95\":5,\"100\":1,\"101\":3,\"102\":17,\"103\":6,\"104\":2,\"107\":5,\"108\":2,\"110\":2,\"111\":3,\"114\":5,\"118\":1}}],[\"to\",{\"1\":{\"7\":1,\"21\":7,\"45\":3,\"46\":1,\"53\":1,\"54\":1,\"57\":4,\"73\":1,\"84\":1,\"90\":2,\"93\":6,\"95\":5,\"98\":1,\"100\":2,\"102\":3,\"103\":1,\"104\":2,\"109\":1,\"114\":3,\"118\":1}}],[\"token提取出来\",{\"1\":{\"114\":1}}],[\"token的作用\",{\"1\":{\"110\":1}}],[\"token的相似度\",{\"1\":{\"101\":1}}],[\"token对齐\",{\"1\":{\"110\":1}}],[\"token开头\",{\"1\":{\"104\":1}}],[\"token作为最后的相似度得分\",{\"1\":{\"101\":2}}],[\"token作为input\",{\"1\":{\"100\":1}}],[\"token是有效的\",{\"1\":{\"100\":1}}],[\"token分类任务\",{\"0\":{\"69\":1}}],[\"token用于分类任务即可\",{\"1\":{\"55\":1}}],[\"tokens形状\",{\"1\":{\"104\":1}}],[\"tokens做attention\",{\"1\":{\"103\":1}}],[\"tokens的embeddings在seq\",{\"1\":{\"102\":1}}],[\"tokens同时输入bertmodel时\",{\"1\":{\"102\":1}}],[\"tokens部分的每个位置都映射到2维匹配空间\",{\"1\":{\"102\":1}}],[\"tokens部分的结果\",{\"1\":{\"102\":1}}],[\"tokens部分的mask列表\",{\"1\":{\"46\":1}}],[\"tokens拼接得到的结果和图像嵌入进行cross\",{\"1\":{\"102\":1}}],[\"tokens列表\",{\"1\":{\"102\":1}}],[\"tokens=true\",{\"1\":{\"68\":2,\"104\":1}}],[\"tokens=false\",{\"1\":{\"46\":1}}],[\"tokens\",{\"1\":{\"7\":3,\"46\":6,\"57\":1,\"66\":2,\"68\":2,\"91\":2,\"100\":9,\"101\":3,\"102\":16,\"103\":16,\"104\":6,\"110\":1,\"111\":2,\"114\":2}}],[\"token\",{\"0\":{\"110\":1},\"1\":{\"7\":2,\"10\":5,\"46\":35,\"48\":4,\"49\":9,\"53\":4,\"54\":2,\"55\":2,\"62\":1,\"64\":3,\"66\":9,\"67\":2,\"68\":8,\"69\":4,\"70\":6,\"100\":1,\"101\":1,\"103\":16,\"104\":5,\"110\":23,\"111\":5,\"114\":5}}],[\"tokenize\",{\"1\":{\"91\":1}}],[\"tokenizer\",{\"1\":{\"7\":4,\"68\":4,\"100\":1,\"103\":2,\"104\":4}}],[\"tokenization\",{\"1\":{\"7\":1,\"66\":1}}],[\"txt文件\",{\"1\":{\"45\":1}}],[\"txt\",{\"1\":{\"7\":1,\"72\":2,\"100\":1}}],[\"temp\",{\"1\":{\"101\":2}}],[\"temperature\",{\"1\":{\"90\":1}}],[\"test\",{\"1\":{\"93\":2,\"95\":1}}],[\"terms\",{\"1\":{\"67\":1}}],[\"tensordataset\",{\"1\":{\"46\":1}}],[\"tensorflow\",{\"1\":{\"14\":1,\"27\":1}}],[\"tensor\",{\"1\":{\"10\":1,\"51\":2,\"53\":2,\"58\":2,\"59\":3,\"67\":1,\"91\":1,\"107\":1,\"108\":2,\"113\":1}}],[\"tensors=\",{\"1\":{\"7\":1,\"93\":2,\"95\":2,\"100\":1}}],[\"text和text\",{\"1\":{\"101\":1}}],[\"text最相关的视觉信息\",{\"1\":{\"100\":1}}],[\"text转化为18291个类别\",{\"1\":{\"96\":1}}],[\"text=texts\",{\"1\":{\"93\":1,\"95\":1}}],[\"texts\",{\"1\":{\"90\":1,\"91\":2,\"93\":1,\"95\":1}}],[\"text\",{\"0\":{\"101\":1,\"102\":1,\"103\":1},\"1\":{\"7\":3,\"10\":1,\"46\":2,\"90\":6,\"91\":13,\"93\":9,\"94\":9,\"95\":16,\"98\":2,\"100\":17,\"101\":11,\"102\":32,\"103\":5,\"104\":1,\"107\":1}}],[\"1rkdjdlr37o7gsr9j1mhjbg\",{\"1\":{\"118\":1}}],[\"197\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"196+1\",{\"1\":{\"111\":1}}],[\"196\",{\"1\":{\"109\":2,\"110\":1,\"111\":1,\"114\":1}}],[\"137mo\",{\"1\":{\"107\":1}}],[\"14模型\",{\"1\":{\"90\":1}}],[\"14则需要在256个v100\",{\"1\":{\"90\":1}}],[\"14\",{\"1\":{\"90\":2,\"93\":1,\"109\":2,\"111\":2}}],[\"1e\",{\"1\":{\"114\":1}}],[\"1e9\",{\"1\":{\"84\":1}}],[\"1e10\",{\"1\":{\"21\":2}}],[\"1=none\",{\"1\":{\"46\":3}}],[\"1200000000\",{\"1\":{\"115\":1}}],[\"12597\",{\"1\":{\"97\":1}}],[\"12\",{\"1\":{\"45\":6}}],[\"128\",{\"1\":{\"22\":5,\"25\":10,\"32\":4,\"34\":3,\"36\":3,\"66\":1}}],[\"1d\",{\"1\":{\"32\":1}}],[\"16×16\",{\"1\":{\"118\":1}}],[\"16这个模型进行微调\",{\"1\":{\"118\":1}}],[\"16x16\",{\"1\":{\"108\":1}}],[\"16为例\",{\"1\":{\"108\":1,\"109\":1}}],[\"16和vit\",{\"1\":{\"90\":1}}],[\"16倍和64倍得到的\",{\"1\":{\"90\":1}}],[\"1612\",{\"1\":{\"27\":1}}],[\"16\",{\"1\":{\"25\":1,\"108\":2,\"109\":3,\"118\":1}}],[\"1706\",{\"1\":{\"14\":1}}],[\"11929\",{\"1\":{\"118\":1}}],[\"11\",{\"1\":{\"7\":1,\"45\":1,\"72\":2}}],[\"103\",{\"1\":{\"46\":1}}],[\"102\",{\"1\":{\"46\":2}}],[\"1024+64\",{\"1\":{\"34\":1}}],[\"1024维\",{\"1\":{\"32\":1,\"34\":1,\"35\":2}}],[\"1024\",{\"1\":{\"22\":3,\"25\":3,\"32\":9,\"34\":7,\"35\":1,\"36\":2}}],[\"101\",{\"1\":{\"46\":2}}],[\"10000\",{\"1\":{\"54\":1,\"102\":3}}],[\"100\",{\"1\":{\"46\":2,\"91\":1,\"93\":1,\"95\":1,\"103\":2}}],[\"104\",{\"1\":{\"46\":1}}],[\"1088\",{\"1\":{\"34\":2,\"36\":1}}],[\"10\",{\"1\":{\"7\":1,\"84\":3}}],[\"1\",{\"0\":{\"100\":1,\"101\":1,\"108\":1},\"1\":{\"7\":4,\"8\":10,\"10\":3,\"12\":14,\"21\":34,\"22\":3,\"25\":9,\"30\":1,\"32\":12,\"33\":4,\"34\":13,\"36\":8,\"37\":1,\"46\":26,\"48\":1,\"49\":2,\"54\":2,\"55\":6,\"57\":5,\"64\":5,\"66\":6,\"67\":5,\"68\":2,\"69\":6,\"70\":15,\"72\":1,\"76\":1,\"79\":2,\"82\":2,\"84\":15,\"88\":1,\"91\":5,\"93\":4,\"95\":4,\"99\":1,\"100\":7,\"101\":13,\"102\":15,\"103\":20,\"104\":7,\"107\":2,\"108\":2,\"109\":5,\"110\":10,\"111\":10,\"113\":16,\"114\":10,\"115\":2,\"118\":2}}],[\"概率分布\",{\"1\":{\"7\":1,\"36\":1}}],[\"e5005f0a\",{\"1\":{\"118\":1}}],[\"eq\",{\"1\":{\"114\":1}}],[\"equivariance\",{\"1\":{\"105\":1}}],[\"equals\",{\"1\":{\"84\":1}}],[\"eos\",{\"1\":{\"104\":1}}],[\"error\",{\"1\":{\"93\":2,\"94\":1,\"95\":3}}],[\"each\",{\"1\":{\"62\":1,\"80\":1}}],[\"exist\",{\"1\":{\"94\":1,\"95\":1,\"107\":1}}],[\"exists\",{\"1\":{\"93\":1,\"94\":1,\"95\":2,\"107\":1}}],[\"extension\",{\"1\":{\"93\":2,\"95\":2}}],[\"extended\",{\"1\":{\"54\":6,\"103\":1}}],[\"exception\",{\"1\":{\"93\":2,\"94\":1,\"95\":3}}],[\"except\",{\"1\":{\"93\":2,\"94\":1,\"95\":3}}],[\"exp\",{\"1\":{\"90\":1}}],[\"expand\",{\"1\":{\"49\":1,\"100\":1,\"102\":1,\"104\":1,\"110\":1,\"111\":1,\"114\":1}}],[\"export\",{\"1\":{\"45\":1}}],[\"epoch\",{\"1\":{\"114\":2}}],[\"epochs=4\",{\"1\":{\"45\":1}}],[\"eps=1e\",{\"1\":{\"114\":1}}],[\"eps=config\",{\"1\":{\"49\":1,\"51\":1,\"58\":1,\"61\":1}}],[\"epsilon\",{\"1\":{\"114\":1}}],[\"eps\",{\"1\":{\"49\":1,\"51\":1,\"58\":1,\"61\":1}}],[\"eye\",{\"1\":{\"33\":1}}],[\"every\",{\"1\":{\"107\":5}}],[\"eval\",{\"1\":{\"45\":2}}],[\"ev\",{\"1\":{\"12\":2}}],[\"ek\",{\"1\":{\"12\":2}}],[\"engineering\",{\"1\":{\"92\":1}}],[\"entropy\",{\"1\":{\"90\":3,\"101\":2,\"102\":1}}],[\"entire\",{\"1\":{\"57\":1}}],[\"encoding\",{\"1\":{\"74\":1}}],[\"encode\",{\"1\":{\"75\":2,\"91\":2}}],[\"encoded\",{\"1\":{\"46\":7}}],[\"encoder中mlp\",{\"1\":{\"115\":1}}],[\"encoder中重复堆叠encoder\",{\"1\":{\"115\":1}}],[\"encoder输出结果之后\",{\"1\":{\"114\":1}}],[\"encoder输出的embeddings里提取与input\",{\"1\":{\"100\":1}}],[\"encoder的结构\",{\"1\":{\"112\":1}}],[\"encoder提取的图像embeddings\",{\"1\":{\"100\":1}}],[\"encoder提取图像特征\",{\"1\":{\"91\":1}}],[\"encoder引到vision\",{\"1\":{\"99\":1}}],[\"encoder模型结构图\",{\"1\":{\"80\":1}}],[\"encoderlayer模型结构图\",{\"1\":{\"79\":1}}],[\"encoderlayer\",{\"0\":{\"79\":1},\"1\":{\"79\":2,\"103\":1}}],[\"encoderdecoder\",{\"1\":{\"75\":2}}],[\"encoderdecoder模型结构图\",{\"1\":{\"75\":1}}],[\"encoder\",{\"0\":{\"75\":1,\"77\":1,\"80\":1,\"112\":1},\"1\":{\"7\":2,\"54\":2,\"68\":3,\"74\":2,\"75\":4,\"80\":3,\"90\":8,\"91\":4,\"98\":2,\"100\":4,\"102\":2,\"103\":20,\"104\":5,\"110\":1}}],[\"end\",{\"1\":{\"66\":12,\"67\":14,\"68\":5,\"93\":4,\"95\":4}}],[\"enumerate\",{\"1\":{\"21\":1,\"25\":1,\"52\":1,\"93\":1,\"95\":1,\"107\":2,\"114\":1}}],[\"enhance\",{\"1\":{\"8\":6}}],[\"emb\",{\"1\":{\"8\":14,\"9\":4,\"11\":2,\"12\":20,\"94\":2,\"95\":2}}],[\"embed\",{\"1\":{\"75\":8,\"90\":2,\"109\":9,\"110\":13,\"111\":15,\"113\":9,\"114\":17,\"118\":1}}],[\"embedding相同维度\",{\"1\":{\"104\":1}}],[\"embedding计算出key和value\",{\"1\":{\"103\":1}}],[\"embedding送到二分类器中\",{\"1\":{\"102\":1}}],[\"embeddings前面\",{\"1\":{\"104\":1}}],[\"embeddings和query\",{\"1\":{\"102\":1}}],[\"embeddings\",{\"1\":{\"7\":1,\"10\":1,\"49\":23,\"53\":1,\"54\":2,\"62\":1,\"64\":1,\"93\":12,\"94\":4,\"95\":15,\"100\":3,\"102\":17}}],[\"embedding\",{\"1\":{\"7\":1,\"10\":1,\"12\":1,\"49\":3,\"54\":2,\"93\":4,\"94\":1,\"95\":3,\"102\":1,\"103\":1,\"109\":1,\"111\":1}}],[\"embeds则拼接\",{\"1\":{\"102\":1}}],[\"embeds=none\",{\"1\":{\"102\":1,\"103\":1}}],[\"embeds=query\",{\"1\":{\"100\":1,\"102\":1,\"103\":2,\"104\":1}}],[\"embeds=llm\",{\"1\":{\"7\":1}}],[\"embeds\",{\"1\":{\"7\":4,\"10\":10,\"100\":4,\"102\":16,\"103\":3,\"104\":6}}],[\"elif\",{\"1\":{\"103\":1}}],[\"else\",{\"1\":{\"7\":2,\"10\":1,\"21\":4,\"22\":2,\"25\":3,\"34\":2,\"46\":2,\"48\":1,\"51\":1,\"55\":1,\"61\":1,\"69\":1,\"70\":3,\"93\":1,\"94\":1,\"95\":2,\"102\":2,\"103\":4,\"104\":1,\"107\":1,\"109\":1,\"112\":1,\"114\":1,\"118\":2}}],[\"elowen\",{\"0\":{\"3\":1}}],[\"e\",{\"1\":{\"7\":1,\"12\":7,\"88\":3,\"90\":6,\"93\":4,\"94\":2,\"95\":6}}],[\"n为序列长度\",{\"1\":{\"113\":1}}],[\"nbatches\",{\"1\":{\"84\":3}}],[\"n个解码器层\",{\"1\":{\"83\":1}}],[\"nucleus\",{\"1\":{\"104\":3}}],[\"null\",{\"1\":{\"46\":1}}],[\"numpy\",{\"1\":{\"32\":1,\"91\":2,\"93\":4,\"95\":3,\"108\":2}}],[\"number\",{\"1\":{\"21\":2,\"84\":1,\"107\":1}}],[\"num\",{\"1\":{\"7\":1,\"8\":6,\"22\":3,\"25\":2,\"45\":1,\"52\":1,\"55\":5,\"57\":5,\"67\":3,\"69\":6,\"70\":9,\"93\":2,\"95\":2,\"103\":1,\"104\":6,\"107\":5,\"109\":5,\"110\":9,\"111\":11,\"112\":2,\"113\":24,\"114\":18,\"118\":4}}],[\"nz\",{\"1\":{\"22\":1}}],[\"ny\",{\"1\":{\"22\":1}}],[\"nx\",{\"1\":{\"22\":1}}],[\"no\",{\"1\":{\"91\":2,\"93\":2,\"94\":1,\"95\":3,\"102\":1}}],[\"noisy\",{\"1\":{\"29\":1}}],[\"not\",{\"1\":{\"21\":3,\"25\":2,\"46\":1,\"55\":1,\"57\":1,\"64\":2,\"67\":2,\"69\":2,\"70\":4,\"84\":3,\"93\":3,\"94\":2,\"95\":5,\"102\":3,\"103\":6,\"104\":1,\"107\":2,\"118\":2,\"122\":1}}],[\"norm2\",{\"1\":{\"112\":2}}],[\"norm1\",{\"1\":{\"112\":2}}],[\"norm层之后同样是多头注意力层\",{\"1\":{\"112\":1}}],[\"normalization\",{\"1\":{\"74\":1,\"112\":1}}],[\"normalize\",{\"1\":{\"57\":1,\"90\":2,\"100\":2,\"108\":2}}],[\"normal\",{\"1\":{\"22\":6,\"25\":5,\"39\":1,\"110\":1,\"111\":2,\"114\":2}}],[\"norm\",{\"1\":{\"21\":3,\"22\":3,\"25\":3,\"33\":1,\"49\":1,\"51\":1,\"58\":1,\"61\":1,\"78\":2,\"80\":2,\"83\":2,\"91\":2,\"93\":2,\"95\":2,\"109\":6,\"111\":1,\"112\":3,\"114\":7}}],[\"none\",{\"1\":{\"7\":1,\"10\":1,\"21\":3,\"22\":1,\"25\":6,\"33\":2,\"34\":1,\"46\":4,\"49\":2,\"55\":2,\"57\":1,\"64\":2,\"67\":2,\"69\":2,\"70\":7,\"84\":4,\"93\":3,\"94\":2,\"95\":4,\"102\":4,\"103\":10,\"107\":2,\"109\":1,\"118\":1}}],[\"neighborhood\",{\"1\":{\"105\":1}}],[\"neg\",{\"1\":{\"102\":20}}],[\"negatives\",{\"1\":{\"101\":2}}],[\"needed\",{\"1\":{\"93\":2,\"95\":2}}],[\"neural\",{\"1\":{\"73\":1,\"74\":1}}],[\"ner\",{\"1\":{\"69\":1}}],[\"next\",{\"1\":{\"46\":1,\"64\":5,\"103\":1}}],[\"networks\",{\"1\":{\"112\":1}}],[\"network\",{\"1\":{\"32\":1,\"74\":1}}],[\"net\",{\"1\":{\"28\":1,\"30\":2,\"32\":3,\"37\":2}}],[\"netpoll\",{\"1\":{\"2\":1}}],[\"new\",{\"1\":{\"21\":46,\"25\":16,\"57\":4,\"103\":2}}],[\"nn\",{\"1\":{\"8\":8,\"9\":4,\"11\":4,\"12\":12,\"21\":5,\"22\":8,\"25\":15,\"32\":13,\"34\":7,\"35\":8,\"36\":8,\"49\":5,\"51\":6,\"52\":2,\"53\":3,\"55\":2,\"57\":6,\"58\":3,\"59\":1,\"61\":2,\"62\":3,\"63\":2,\"66\":1,\"67\":1,\"69\":2,\"70\":2,\"75\":1,\"76\":2,\"78\":2,\"79\":1,\"80\":1,\"82\":1,\"83\":1,\"84\":3,\"102\":1,\"103\":4,\"109\":5,\"110\":4,\"111\":6,\"112\":6,\"113\":5,\"114\":14}}],[\"natural\",{\"1\":{\"92\":1}}],[\"named\",{\"1\":{\"118\":1}}],[\"name=\",{\"1\":{\"45\":1}}],[\"name\",{\"1\":{\"45\":1,\"93\":8,\"95\":8,\"118\":4}}],[\"na\",{\"1\":{\"7\":1}}],[\"nlp\",{\"1\":{\"90\":1}}],[\"nl=token数\",{\"1\":{\"7\":1}}],[\"nl\",{\"1\":{\"7\":4}}],[\"nsp\",{\"1\":{\"46\":1}}],[\"nsp任务\",{\"1\":{\"46\":1}}],[\"nsample=none\",{\"1\":{\"22\":1}}],[\"nsample=64\",{\"1\":{\"22\":1}}],[\"nsample=32\",{\"1\":{\"22\":1}}],[\"nsample\",{\"1\":{\"21\":23,\"25\":4}}],[\"ns\",{\"1\":{\"7\":5,\"30\":1}}],[\"npoint=none\",{\"1\":{\"22\":1}}],[\"npoint=128\",{\"1\":{\"22\":1}}],[\"npoint=512\",{\"1\":{\"22\":1}}],[\"npoint\",{\"1\":{\"21\":35,\"25\":4}}],[\"np\",{\"1\":{\"7\":1,\"32\":2,\"90\":5,\"91\":1,\"93\":6,\"94\":1,\"95\":8}}],[\"n\",{\"1\":{\"7\":1,\"8\":10,\"10\":6,\"12\":29,\"21\":30,\"25\":4,\"32\":1,\"34\":5,\"36\":9,\"37\":3,\"45\":1,\"72\":1,\"80\":3,\"83\":3,\"90\":7,\"96\":1,\"113\":3}}],[\"dtype\",{\"1\":{\"54\":1}}],[\"dtype=next\",{\"1\":{\"54\":1}}],[\"dtype=torch\",{\"1\":{\"10\":1,\"21\":5,\"49\":1,\"100\":1,\"102\":4,\"103\":1,\"104\":1}}],[\"dog\",{\"1\":{\"103\":1}}],[\"does\",{\"1\":{\"94\":1,\"95\":1,\"107\":1}}],[\"dot\",{\"1\":{\"57\":1,\"84\":1,\"90\":3,\"93\":1,\"95\":1}}],[\"doing\",{\"1\":{\"55\":1}}],[\"do\",{\"1\":{\"45\":3,\"84\":1,\"104\":1}}],[\"downloaded\",{\"1\":{\"93\":1,\"95\":1}}],[\"downloading\",{\"1\":{\"93\":2,\"95\":2}}],[\"download\",{\"1\":{\"93\":3,\"95\":4,\"118\":1}}],[\"down\",{\"1\":{\"7\":1,\"11\":1}}],[\"dumps\",{\"1\":{\"107\":1}}],[\"dump\",{\"1\":{\"45\":1}}],[\"dall\",{\"1\":{\"88\":3}}],[\"da\",{\"1\":{\"45\":1}}],[\"dataloader\",{\"0\":{\"48\":1},\"1\":{\"48\":3,\"107\":1,\"108\":2}}],[\"dataset\",{\"1\":{\"46\":1,\"48\":3,\"107\":5,\"108\":6}}],[\"data\",{\"1\":{\"21\":14,\"25\":4,\"45\":1,\"94\":9,\"95\":9,\"96\":2,\"107\":3,\"108\":6,\"114\":4}}],[\"dgcnn\",{\"1\":{\"37\":2,\"40\":1}}],[\"drop=0\",{\"1\":{\"112\":1}}],[\"drop=drop\",{\"1\":{\"112\":1}}],[\"drop\",{\"1\":{\"111\":5,\"112\":16,\"113\":8,\"114\":9}}],[\"droppath\",{\"1\":{\"112\":2}}],[\"dropped\",{\"1\":{\"103\":2}}],[\"dropping\",{\"1\":{\"57\":1}}],[\"drop2\",{\"1\":{\"22\":2,\"25\":2}}],[\"dropout防止过拟合\",{\"1\":{\"103\":1}}],[\"dropout=none\",{\"1\":{\"84\":1}}],[\"dropout=self\",{\"1\":{\"84\":1}}],[\"dropout=0\",{\"1\":{\"84\":1}}],[\"dropout\",{\"1\":{\"22\":2,\"24\":1,\"25\":2,\"35\":3,\"49\":4,\"51\":4,\"55\":4,\"57\":4,\"58\":4,\"69\":4,\"70\":4,\"78\":6,\"79\":2,\"82\":2,\"84\":5,\"102\":1,\"103\":1,\"111\":1,\"112\":4,\"113\":2,\"114\":1}}],[\"drop1\",{\"1\":{\"22\":2,\"25\":2}}],[\"diag\",{\"1\":{\"102\":6}}],[\"directional\",{\"1\":{\"102\":2}}],[\"directory\",{\"1\":{\"93\":5,\"94\":1,\"95\":6}}],[\"dirname\",{\"1\":{\"93\":1,\"95\":1}}],[\"dir\",{\"1\":{\"45\":4,\"93\":16,\"94\":7,\"95\":23}}],[\"dir=save\",{\"1\":{\"93\":1,\"95\":1}}],[\"dir=\",{\"1\":{\"45\":3}}],[\"distribution\",{\"1\":{\"107\":1}}],[\"distributedsampler\",{\"1\":{\"48\":1}}],[\"dist\",{\"1\":{\"21\":3}}],[\"distance\",{\"1\":{\"21\":7}}],[\"div\",{\"1\":{\"8\":2}}],[\"dict\",{\"1\":{\"103\":1,\"107\":2,\"118\":3}}],[\"dict=return\",{\"1\":{\"103\":1}}],[\"dict=none\",{\"1\":{\"103\":1}}],[\"dict=true\",{\"1\":{\"100\":2,\"102\":1,\"103\":3}}],[\"dict=false\",{\"1\":{\"7\":1}}],[\"dice\",{\"1\":{\"7\":1}}],[\"dim代表的是卷积核的数量\",{\"1\":{\"109\":1}}],[\"dimensional\",{\"1\":{\"105\":1}}],[\"dimension\",{\"1\":{\"12\":1,\"37\":1}}],[\"dimensions\",{\"1\":{\"7\":1}}],[\"dim=embed\",{\"1\":{\"110\":1,\"111\":1,\"114\":2}}],[\"dim=768\",{\"1\":{\"109\":1,\"110\":1,\"111\":1,\"114\":1,\"118\":1}}],[\"dim=2\",{\"1\":{\"103\":2}}],[\"dim=0\",{\"1\":{\"102\":7,\"104\":1,\"107\":1}}],[\"dim=\",{\"1\":{\"8\":1,\"12\":2,\"21\":3,\"25\":1,\"33\":1,\"36\":1,\"57\":1,\"66\":3,\"67\":1,\"76\":1,\"84\":1,\"91\":4,\"100\":2,\"103\":1,\"113\":1}}],[\"dim=1\",{\"1\":{\"7\":1,\"8\":2,\"12\":1,\"25\":1,\"35\":1,\"102\":5,\"103\":1,\"110\":1,\"111\":1,\"114\":2}}],[\"dim\",{\"1\":{\"8\":14,\"9\":4,\"10\":2,\"11\":2,\"12\":42,\"109\":8,\"110\":6,\"111\":6,\"112\":7,\"113\":17,\"114\":9}}],[\"d\",{\"1\":{\"7\":1,\"21\":4,\"25\":4,\"33\":4,\"57\":2,\"70\":1,\"76\":3,\"84\":15,\"90\":6,\"98\":2,\"104\":4,\"111\":2}}],[\"depth\",{\"1\":{\"114\":1}}],[\"depth=12\",{\"1\":{\"111\":1,\"114\":1,\"118\":1}}],[\"desc\",{\"1\":{\"91\":2}}],[\"description\",{\"1\":{\"7\":3}}],[\"del\",{\"1\":{\"84\":3}}],[\"decode\",{\"1\":{\"68\":1,\"75\":2,\"104\":1}}],[\"decoder=is\",{\"1\":{\"103\":1}}],[\"decoder=true时\",{\"1\":{\"103\":1}}],[\"decoder=true\",{\"1\":{\"103\":1}}],[\"decoder模型结构图\",{\"1\":{\"82\":1,\"83\":1}}],[\"decoderlayer\",{\"0\":{\"82\":1},\"1\":{\"82\":2}}],[\"decoder\",{\"0\":{\"75\":1,\"81\":1,\"83\":1},\"1\":{\"7\":1,\"12\":1,\"62\":2,\"68\":1,\"74\":2,\"75\":4,\"82\":1,\"83\":3,\"98\":1,\"103\":9,\"104\":2}}],[\"dense\",{\"1\":{\"51\":4,\"53\":2,\"58\":2,\"61\":2,\"78\":1}}],[\"deep\",{\"1\":{\"40\":1}}],[\"device=sim\",{\"1\":{\"102\":1}}],[\"device=image\",{\"1\":{\"101\":1}}],[\"device=input\",{\"1\":{\"49\":1}}],[\"device=multi\",{\"1\":{\"10\":1}}],[\"device\",{\"1\":{\"7\":3,\"10\":1,\"21\":15,\"49\":1,\"93\":5,\"95\":5,\"100\":2,\"101\":1,\"102\":4,\"103\":1,\"104\":2,\"114\":4,\"118\":1}}],[\"defined\",{\"1\":{\"82\":1}}],[\"def\",{\"1\":{\"7\":1,\"8\":2,\"10\":1,\"12\":4,\"21\":7,\"22\":2,\"25\":4,\"32\":2,\"33\":1,\"34\":2,\"35\":2,\"36\":2,\"46\":3,\"48\":1,\"49\":2,\"51\":6,\"52\":2,\"53\":2,\"54\":2,\"55\":2,\"57\":3,\"58\":2,\"59\":2,\"61\":2,\"62\":2,\"63\":2,\"64\":2,\"67\":2,\"69\":2,\"70\":2,\"75\":4,\"76\":2,\"78\":2,\"79\":2,\"80\":2,\"82\":2,\"83\":2,\"84\":3,\"93\":8,\"94\":3,\"95\":11,\"100\":1,\"102\":1,\"103\":4,\"104\":1,\"107\":5,\"109\":2,\"110\":3,\"111\":3,\"112\":4,\"113\":2,\"114\":4,\"118\":1}}],[\"21843\",{\"1\":{\"118\":1}}],[\"21000\",{\"1\":{\"118\":1}}],[\"21k\",{\"1\":{\"118\":3}}],[\"2×1000000000\",{\"1\":{\"115\":1}}],[\"2b\",{\"1\":{\"115\":1}}],[\"244\",{\"1\":{\"109\":1}}],[\"2的起因\",{\"1\":{\"98\":1}}],[\"2301\",{\"1\":{\"97\":1}}],[\"2f\",{\"1\":{\"93\":2,\"95\":2}}],[\"2训练时使用的webtext数据集相似\",{\"1\":{\"90\":1}}],[\"2010\",{\"1\":{\"118\":1}}],[\"2017年的工作\",{\"1\":{\"96\":1}}],[\"2016年的工作\",{\"1\":{\"96\":1}}],[\"2018\",{\"1\":{\"45\":1}}],[\"2021\",{\"1\":{\"88\":2}}],[\"2048\",{\"1\":{\"7\":3}}],[\"224×224\",{\"1\":{\"118\":1}}],[\"224\",{\"1\":{\"108\":2,\"109\":2,\"118\":6}}],[\"224x224\",{\"1\":{\"108\":3,\"109\":1,\"118\":1}}],[\"22\",{\"1\":{\"45\":1,\"72\":1}}],[\"255\",{\"1\":{\"108\":2}}],[\"257\",{\"1\":{\"104\":3}}],[\"2578\",{\"1\":{\"46\":2}}],[\"2501\",{\"1\":{\"46\":2}}],[\"2504\",{\"1\":{\"4\":1}}],[\"256\",{\"1\":{\"22\":5,\"25\":6,\"32\":3,\"35\":3,\"36\":3,\"108\":2}}],[\"2c\",{\"1\":{\"12\":1}}],[\"2\",{\"0\":{\"8\":1,\"102\":1,\"104\":1,\"109\":1},\"1\":{\"7\":1,\"8\":5,\"12\":9,\"21\":8,\"22\":1,\"25\":7,\"30\":1,\"32\":2,\"33\":2,\"34\":6,\"36\":4,\"37\":2,\"49\":1,\"51\":1,\"54\":1,\"55\":1,\"57\":4,\"61\":1,\"63\":1,\"64\":2,\"66\":2,\"67\":3,\"79\":1,\"82\":1,\"84\":5,\"90\":1,\"91\":1,\"98\":1,\"99\":2,\"101\":2,\"102\":5,\"103\":5,\"104\":2,\"107\":2,\"109\":3,\"111\":1,\"113\":4}}],[\"2d\",{\"1\":{\"7\":1,\"9\":1,\"39\":2,\"109\":1}}],[\">\",{\"1\":{\"7\":2,\"12\":3,\"21\":1,\"36\":1,\"46\":3,\"48\":1,\"67\":1,\"84\":1,\"96\":2,\"103\":1,\"104\":1,\"110\":2,\"111\":2,\"112\":1,\"113\":8,\"114\":3}}],[\"w`代表输出特征图的宽和高\",{\"1\":{\"109\":1}}],[\"write\",{\"1\":{\"107\":1}}],[\"world\",{\"1\":{\"102\":6}}],[\"words\",{\"1\":{\"49\":2,\"96\":3}}],[\"word\",{\"1\":{\"49\":2,\"102\":1}}],[\"warnings\",{\"1\":{\"95\":2}}],[\"walk\",{\"1\":{\"93\":1,\"95\":1}}],[\"white\",{\"1\":{\"91\":1}}],[\"which\",{\"1\":{\"57\":1}}],[\"were\",{\"1\":{\"107\":1}}],[\"web\",{\"1\":{\"96\":1}}],[\"weakly\",{\"1\":{\"96\":1}}],[\"weights\",{\"1\":{\"54\":1,\"55\":1,\"62\":1,\"93\":2,\"95\":2,\"102\":4,\"114\":1,\"118\":5}}],[\"we\",{\"1\":{\"53\":1,\"55\":1,\"67\":1,\"84\":1}}],[\"wget\",{\"1\":{\"45\":1}}],[\"width\",{\"1\":{\"7\":1,\"107\":1}}],[\"with\",{\"1\":{\"4\":1,\"7\":1,\"46\":3,\"83\":1,\"91\":3,\"93\":2,\"95\":2,\"102\":1,\"107\":1}}],[\"w\",{\"1\":{\"7\":4,\"8\":2,\"84\":4,\"90\":5,\"100\":1,\"107\":1,\"109\":4,\"110\":1,\"111\":1,\"114\":1}}],[\"3的维度\",{\"1\":{\"113\":1}}],[\"396\",{\"1\":{\"93\":1}}],[\"336\",{\"1\":{\"90\":1}}],[\"300m数据集\",{\"1\":{\"115\":1}}],[\"300m数据集的规模达到了上亿级别\",{\"1\":{\"96\":1}}],[\"300m数据集取得了较好的结果\",{\"1\":{\"96\":1}}],[\"300m数据集是谷歌从互联网上收集的\",{\"1\":{\"96\":1}}],[\"300m数据集来预训练模型在imagenet上取得sota\",{\"1\":{\"96\":1}}],[\"300m数据集还要多出1亿对\",{\"1\":{\"90\":1}}],[\"3072\",{\"1\":{\"51\":2}}],[\"3x3\",{\"1\":{\"32\":2}}],[\"3×3\",{\"1\":{\"30\":1,\"32\":1,\"33\":1}}],[\"32768\",{\"1\":{\"90\":1}}],[\"320\",{\"1\":{\"25\":1}}],[\"32\",{\"1\":{\"25\":4,\"90\":1}}],[\"3+d\",{\"1\":{\"21\":3}}],[\"3\",{\"0\":{\"9\":1,\"103\":1,\"110\":1},\"1\":{\"7\":3,\"21\":14,\"22\":5,\"25\":6,\"30\":2,\"32\":9,\"34\":1,\"35\":1,\"49\":1,\"57\":2,\"82\":1,\"84\":1,\"91\":1,\"92\":1,\"95\":1,\"96\":1,\"102\":10,\"103\":2,\"104\":1,\"107\":1,\"109\":3,\"113\":7}}],[\"3d\",{\"0\":{\"13\":1},\"1\":{\"2\":1,\"4\":1,\"7\":2,\"9\":1,\"32\":1,\"34\":1,\"37\":1,\"39\":4}}],[\"itg\",{\"0\":{\"103\":1}}],[\"itm\",{\"0\":{\"102\":1},\"1\":{\"102\":12}}],[\"itc\",{\"0\":{\"101\":1},\"1\":{\"101\":1}}],[\"items\",{\"1\":{\"107\":1}}],[\"item\",{\"1\":{\"48\":1,\"102\":2,\"107\":5}}],[\"icmlm和convirt仅在10万级别的数据上训练了几天\",{\"1\":{\"96\":1}}],[\"icmlm基于语言掩码的方法\",{\"1\":{\"96\":1}}],[\"ignored\",{\"1\":{\"67\":4}}],[\"ignore\",{\"1\":{\"64\":1,\"67\":3,\"95\":1}}],[\"i2t\",{\"1\":{\"101\":5,\"102\":4}}],[\"i2\",{\"1\":{\"12\":4}}],[\"i1\",{\"1\":{\"12\":4}}],[\"isn\",{\"1\":{\"107\":1}}],[\"isdir\",{\"1\":{\"93\":1,\"95\":1,\"107\":1}}],[\"isinstance\",{\"1\":{\"51\":2,\"61\":2}}],[\"is\",{\"1\":{\"10\":1,\"21\":3,\"25\":2,\"32\":1,\"33\":1,\"46\":4,\"49\":2,\"55\":1,\"57\":4,\"62\":1,\"64\":2,\"67\":2,\"68\":2,\"69\":2,\"70\":4,\"80\":1,\"82\":1,\"84\":3,\"91\":1,\"93\":2,\"94\":2,\"95\":4,\"102\":4,\"103\":12,\"107\":1}}],[\"i\",{\"1\":{\"8\":17,\"10\":10,\"12\":18,\"21\":4,\"25\":7,\"33\":5,\"52\":2,\"90\":12,\"93\":2,\"95\":2,\"103\":3,\"107\":4,\"114\":2}}],[\"id为文本\",{\"1\":{\"104\":1}}],[\"id=self\",{\"1\":{\"104\":2}}],[\"id=model\",{\"1\":{\"93\":1,\"95\":1}}],[\"identity\",{\"1\":{\"109\":3,\"112\":1,\"114\":1}}],[\"iden\",{\"1\":{\"32\":4}}],[\"idx=0\",{\"1\":{\"49\":1}}],[\"idx\",{\"1\":{\"21\":21,\"25\":3,\"93\":5,\"95\":5,\"102\":5}}],[\"ids=position\",{\"1\":{\"54\":1,\"55\":1,\"64\":1,\"67\":1,\"69\":1,\"70\":1,\"103\":1}}],[\"ids=none\",{\"1\":{\"49\":2,\"54\":2,\"55\":2,\"64\":2,\"67\":2,\"69\":2,\"70\":2,\"102\":2,\"103\":2}}],[\"ids=token\",{\"1\":{\"46\":1,\"54\":1,\"55\":1,\"64\":1,\"66\":1,\"67\":1,\"69\":1,\"70\":1}}],[\"ids=input\",{\"1\":{\"46\":1,\"104\":1}}],[\"ids作用图解\",{\"1\":{\"46\":1}}],[\"ids\",{\"1\":{\"7\":1,\"46\":36,\"48\":8,\"49\":14,\"54\":4,\"55\":4,\"64\":4,\"66\":2,\"67\":4,\"68\":4,\"69\":4,\"70\":17,\"100\":1,\"102\":21,\"103\":9,\"104\":2}}],[\"id\",{\"1\":{\"7\":2,\"46\":9,\"68\":3,\"103\":3,\"104\":3}}],[\"impl\",{\"1\":{\"118\":1}}],[\"implements\",{\"1\":{\"84\":1}}],[\"import\",{\"1\":{\"95\":9,\"107\":3}}],[\"imshow\",{\"1\":{\"94\":1,\"95\":1}}],[\"imagenet\",{\"1\":{\"108\":1,\"118\":3}}],[\"images=images\",{\"1\":{\"93\":1,\"95\":1}}],[\"images\",{\"1\":{\"39\":1,\"90\":1,\"91\":1,\"93\":3,\"94\":2,\"95\":5,\"103\":2,\"107\":40,\"108\":12,\"114\":3}}],[\"image\",{\"0\":{\"101\":1,\"102\":1,\"103\":1},\"1\":{\"7\":2,\"10\":6,\"89\":1,\"90\":5,\"91\":10,\"93\":31,\"94\":21,\"95\":51,\"96\":1,\"98\":2,\"100\":14,\"101\":11,\"102\":20,\"103\":7,\"104\":15,\"107\":9,\"109\":1,\"118\":1}}],[\"img\",{\"1\":{\"7\":8,\"8\":7,\"94\":2,\"95\":2,\"107\":9,\"109\":10,\"110\":3,\"111\":2,\"114\":2,\"118\":1}}],[\"if\",{\"1\":{\"7\":2,\"10\":1,\"21\":5,\"22\":2,\"25\":4,\"32\":1,\"33\":1,\"34\":3,\"46\":7,\"48\":1,\"49\":2,\"51\":1,\"55\":3,\"57\":1,\"61\":1,\"64\":1,\"67\":1,\"69\":2,\"70\":4,\"84\":3,\"93\":9,\"94\":3,\"95\":12,\"102\":5,\"103\":11,\"104\":1,\"107\":6,\"109\":1,\"112\":1,\"114\":1,\"118\":2}}],[\"in21k模型权重文件\",{\"1\":{\"118\":1}}],[\"in21k\",{\"1\":{\"118\":4}}],[\"in21k这个模型\",{\"1\":{\"118\":1}}],[\"install\",{\"1\":{\"72\":1}}],[\"instruction\",{\"1\":{\"7\":1}}],[\"instructional\",{\"1\":{\"7\":3,\"12\":3}}],[\"instructions\",{\"1\":{\"4\":1}}],[\"into\",{\"1\":{\"66\":1}}],[\"int\",{\"1\":{\"57\":1,\"110\":5,\"112\":1,\"118\":1}}],[\"interleave\",{\"1\":{\"104\":1}}],[\"intermediate\",{\"1\":{\"51\":9}}],[\"interaction\",{\"1\":{\"29\":1}}],[\"interactions\",{\"1\":{\"4\":1}}],[\"intensity\",{\"1\":{\"39\":1}}],[\"info\",{\"1\":{\"51\":1,\"61\":1}}],[\"inference\",{\"1\":{\"7\":3}}],[\"invariance\",{\"1\":{\"29\":1,\"40\":1}}],[\"invariant\",{\"1\":{\"29\":1}}],[\"indent=4\",{\"1\":{\"107\":1}}],[\"index=5\",{\"1\":{\"68\":2}}],[\"index=ignored\",{\"1\":{\"67\":1}}],[\"index=\",{\"1\":{\"64\":1}}],[\"indexed\",{\"1\":{\"21\":1}}],[\"index\",{\"1\":{\"21\":8,\"25\":3,\"66\":4,\"67\":6,\"68\":5,\"93\":2,\"94\":2,\"95\":4}}],[\"inductive\",{\"1\":{\"105\":1}}],[\"indices\",{\"1\":{\"21\":5,\"32\":1,\"34\":1,\"93\":2,\"95\":2,\"102\":6,\"107\":4}}],[\"in\",{\"1\":{\"10\":1,\"21\":6,\"22\":4,\"25\":8,\"46\":1,\"52\":2,\"57\":1,\"75\":1,\"80\":2,\"83\":1,\"84\":4,\"91\":2,\"92\":1,\"93\":7,\"95\":7,\"101\":2,\"102\":2,\"103\":1,\"107\":10,\"109\":4,\"110\":3,\"111\":2,\"112\":8,\"114\":4,\"118\":3}}],[\"init\",{\"1\":{\"8\":2,\"12\":4,\"21\":2,\"22\":2,\"25\":4,\"32\":2,\"34\":2,\"35\":2,\"36\":2,\"49\":2,\"51\":6,\"52\":2,\"53\":2,\"54\":3,\"55\":3,\"57\":2,\"58\":2,\"59\":2,\"61\":2,\"62\":2,\"63\":2,\"64\":2,\"67\":2,\"69\":2,\"70\":2,\"75\":2,\"76\":2,\"78\":2,\"79\":2,\"80\":2,\"82\":2,\"83\":2,\"84\":2,\"107\":1,\"109\":2,\"110\":3,\"111\":4,\"112\":4,\"113\":2,\"114\":5}}],[\"inputfeatures\",{\"1\":{\"46\":1}}],[\"inputfeatures组成图解\",{\"1\":{\"46\":1}}],[\"inputs\",{\"1\":{\"7\":6,\"10\":6,\"46\":8,\"67\":1,\"93\":4,\"95\":4}}],[\"input\",{\"1\":{\"7\":7,\"10\":19,\"21\":11,\"24\":1,\"25\":3,\"46\":13,\"48\":4,\"49\":6,\"51\":2,\"54\":2,\"55\":2,\"58\":2,\"59\":3,\"62\":1,\"64\":2,\"66\":1,\"67\":2,\"68\":4,\"69\":2,\"70\":7,\"80\":1,\"91\":2,\"100\":6,\"102\":10,\"103\":8,\"104\":3}}],[\"s=str\",{\"1\":{\"107\":1}}],[\"smoothing=0\",{\"1\":{\"101\":2,\"103\":1}}],[\"snapshot\",{\"1\":{\"93\":1,\"95\":2}}],[\"shuffle=false\",{\"1\":{\"108\":1}}],[\"shuffle=true\",{\"1\":{\"108\":1}}],[\"shifted\",{\"1\":{\"103\":3}}],[\"show\",{\"1\":{\"94\":1,\"95\":1,\"107\":1}}],[\"shot图文生成\",{\"1\":{\"99\":1}}],[\"shot图像分类\",{\"1\":{\"91\":1}}],[\"shot\",{\"1\":{\"98\":1}}],[\"shot性能评估\",{\"1\":{\"96\":1}}],[\"shot性能\",{\"1\":{\"96\":1}}],[\"shot迁移到下游任务\",{\"1\":{\"96\":1}}],[\"shot学习\",{\"1\":{\"96\":1}}],[\"shot推理\",{\"1\":{\"93\":1}}],[\"shot分类时\",{\"1\":{\"92\":1}}],[\"shot分类的过程相当直接\",{\"1\":{\"91\":1}}],[\"shape为\",{\"1\":{\"109\":1}}],[\"shape的形状从\",{\"1\":{\"21\":1}}],[\"shape\",{\"1\":{\"7\":8,\"21\":15,\"22\":1,\"25\":2,\"32\":3,\"34\":3,\"36\":4,\"57\":4,\"70\":1,\"100\":1,\"102\":1,\"103\":2,\"104\":1,\"109\":1,\"110\":1,\"111\":1,\"113\":1,\"114\":2}}],[\"src\",{\"1\":{\"75\":16,\"82\":7,\"83\":2}}],[\"srl\",{\"1\":{\"69\":1}}],[\"symlinks=false\",{\"1\":{\"93\":1,\"95\":1}}],[\"symmetric\",{\"1\":{\"30\":1,\"40\":1}}],[\"systematic\",{\"1\":{\"92\":1}}],[\"sys\",{\"1\":{\"51\":1,\"61\":1}}],[\"salesforce\",{\"1\":{\"97\":1}}],[\"saucer\",{\"1\":{\"91\":1}}],[\"same\",{\"1\":{\"62\":1,\"84\":1}}],[\"sampling=false\",{\"1\":{\"104\":1}}],[\"sampling\",{\"0\":{\"18\":1},\"1\":{\"17\":2,\"18\":1,\"21\":1,\"104\":2}}],[\"sample=use\",{\"1\":{\"104\":1}}],[\"sampler=train\",{\"1\":{\"48\":1}}],[\"sampler\",{\"1\":{\"48\":2}}],[\"sampled\",{\"1\":{\"21\":6,\"25\":1}}],[\"samples\",{\"1\":{\"21\":1,\"100\":3,\"104\":2}}],[\"sample\",{\"1\":{\"16\":1,\"21\":14,\"25\":2,\"107\":1,\"114\":1}}],[\"save\",{\"1\":{\"45\":1,\"93\":3,\"95\":3,\"103\":3}}],[\"sa3\",{\"1\":{\"22\":2,\"25\":2}}],[\"sa2\",{\"1\":{\"22\":2,\"25\":2}}],[\"sa1\",{\"1\":{\"22\":2,\"25\":2}}],[\"sa\",{\"1\":{\"22\":1}}],[\"ssg\",{\"1\":{\"22\":3}}],[\"source\",{\"1\":{\"118\":1}}],[\"southampton\",{\"1\":{\"45\":1}}],[\"sometimes\",{\"1\":{\"67\":1}}],[\"sota\",{\"1\":{\"37\":1}}],[\"sort\",{\"1\":{\"21\":1,\"107\":1}}],[\"softmax归一化得到注意力概率\",{\"1\":{\"103\":1}}],[\"softmax\",{\"1\":{\"8\":5,\"12\":3,\"22\":1,\"25\":1,\"35\":2,\"36\":2,\"40\":2,\"57\":1,\"66\":3,\"74\":1,\"76\":1,\"84\":1,\"91\":1,\"102\":2,\"103\":1,\"113\":1}}],[\"squad\",{\"1\":{\"68\":1}}],[\"square\",{\"1\":{\"21\":1}}],[\"squeeze\",{\"1\":{\"66\":2,\"67\":2,\"101\":2}}],[\"sqrt\",{\"1\":{\"57\":1,\"84\":1,\"103\":1}}],[\"sqrdists\",{\"1\":{\"21\":2}}],[\"s\",{\"1\":{\"12\":4,\"21\":16,\"25\":8,\"30\":1,\"46\":1,\"107\":1,\"118\":1}}],[\"score\",{\"1\":{\"63\":2,\"64\":4,\"101\":2,\"102\":2}}],[\"scores\",{\"1\":{\"57\":12,\"63\":2,\"64\":4,\"66\":4,\"69\":1,\"84\":4,\"103\":20}}],[\"scene\",{\"1\":{\"12\":8}}],[\"scale未指定\",{\"1\":{\"113\":1}}],[\"scale=qk\",{\"1\":{\"112\":1,\"114\":1}}],[\"scale=none\",{\"1\":{\"111\":1,\"112\":1,\"113\":1,\"114\":1}}],[\"scaled\",{\"1\":{\"84\":1}}],[\"scale\",{\"0\":{\"24\":1},\"1\":{\"8\":2,\"12\":3,\"22\":1,\"23\":1,\"25\":1,\"112\":1,\"113\":3,\"114\":1}}],[\"sv\",{\"1\":{\"12\":2}}],[\"skip\",{\"1\":{\"68\":2,\"104\":1}}],[\"sk\",{\"1\":{\"12\":2}}],[\"supported\",{\"1\":{\"107\":2}}],[\"supervised\",{\"1\":{\"96\":1}}],[\"super\",{\"1\":{\"8\":1,\"12\":2,\"21\":1,\"22\":1,\"25\":2,\"32\":1,\"34\":1,\"35\":1,\"36\":1,\"49\":1,\"51\":3,\"52\":1,\"53\":1,\"54\":1,\"55\":1,\"57\":1,\"58\":1,\"59\":1,\"61\":1,\"62\":1,\"63\":1,\"64\":1,\"67\":1,\"69\":1,\"70\":1,\"75\":1,\"76\":1,\"78\":1,\"79\":1,\"80\":1,\"82\":1,\"83\":1,\"84\":1,\"109\":1,\"110\":1,\"111\":1,\"112\":2,\"113\":1,\"114\":1}}],[\"sunflowers\",{\"1\":{\"94\":1,\"95\":1}}],[\"successfully\",{\"1\":{\"93\":1,\"95\":1}}],[\"survey\",{\"1\":{\"92\":1}}],[\"sublayer是传入的参数\",{\"1\":{\"78\":1}}],[\"sublayer\",{\"1\":{\"78\":3,\"79\":3,\"82\":4}}],[\"sublayerconnection模型结构图\",{\"1\":{\"78\":1}}],[\"sublayerconnection\",{\"0\":{\"78\":1},\"1\":{\"78\":2,\"79\":1,\"82\":1}}],[\"sub\",{\"1\":{\"12\":8,\"93\":5,\"95\":5}}],[\"summation\",{\"1\":{\"40\":1}}],[\"sum\",{\"1\":{\"10\":1,\"21\":1,\"40\":1,\"103\":1,\"107\":1,\"114\":1}}],[\"seed\",{\"1\":{\"107\":1}}],[\"seem\",{\"1\":{\"57\":1}}],[\"search\",{\"1\":{\"104\":1}}],[\"search扩展\",{\"1\":{\"104\":1}}],[\"search的beam数量\",{\"1\":{\"104\":1}}],[\"searchpicbytext\",{\"1\":{\"94\":1,\"95\":1}}],[\"segmentation\",{\"1\":{\"49\":1,\"91\":1}}],[\"segment\",{\"1\":{\"46\":1}}],[\"seconds\",{\"1\":{\"93\":2,\"95\":1}}],[\"second\",{\"1\":{\"46\":1}}],[\"sections=spatial\",{\"1\":{\"7\":1}}],[\"sep\",{\"1\":{\"46\":11,\"66\":2,\"68\":2,\"70\":2,\"104\":1}}],[\"sentence\",{\"1\":{\"46\":1,\"64\":5,\"103\":1}}],[\"sets\",{\"1\":{\"40\":1}}],[\"set\",{\"1\":{\"17\":1,\"22\":1,\"26\":1,\"30\":1,\"37\":2,\"40\":1}}],[\"seq\",{\"1\":{\"12\":1,\"45\":1,\"49\":2,\"57\":2,\"63\":4,\"64\":4,\"66\":4,\"67\":4,\"69\":2,\"70\":4,\"84\":1,\"100\":6,\"101\":5,\"102\":13,\"103\":1}}],[\"sequences\",{\"1\":{\"46\":2,\"75\":1}}],[\"sequence\",{\"1\":{\"10\":2,\"46\":4,\"54\":3,\"63\":2,\"64\":2,\"66\":3,\"67\":2,\"69\":4,\"103\":2}}],[\"sequential\",{\"1\":{\"8\":1,\"9\":1,\"11\":1,\"12\":1,\"114\":2}}],[\"semantic\",{\"1\":{\"7\":4,\"12\":3}}],[\"self\",{\"1\":{\"7\":18,\"8\":32,\"9\":7,\"10\":1,\"11\":6,\"12\":40,\"21\":17,\"22\":25,\"25\":41,\"32\":26,\"34\":24,\"35\":19,\"36\":24,\"46\":9,\"49\":13,\"51\":26,\"52\":5,\"53\":7,\"54\":11,\"55\":14,\"57\":28,\"58\":9,\"59\":11,\"61\":10,\"62\":9,\"63\":7,\"64\":8,\"66\":3,\"67\":8,\"69\":12,\"70\":9,\"74\":2,\"75\":16,\"76\":5,\"78\":8,\"79\":15,\"80\":7,\"82\":19,\"83\":7,\"84\":15,\"100\":9,\"101\":4,\"102\":12,\"103\":49,\"104\":7,\"107\":12,\"109\":12,\"110\":16,\"111\":22,\"112\":25,\"113\":16,\"114\":33}}],[\"std=0\",{\"1\":{\"110\":1,\"111\":2,\"114\":2}}],[\"strict=false\",{\"1\":{\"118\":1}}],[\"stride=patch\",{\"1\":{\"109\":1}}],[\"stride\",{\"1\":{\"109\":1}}],[\"str\",{\"1\":{\"51\":1,\"61\":1,\"107\":3}}],[\"structure\",{\"1\":{\"25\":1,\"105\":1}}],[\"storage\",{\"1\":{\"45\":1}}],[\"stn\",{\"1\":{\"34\":3}}],[\"stnkd\",{\"1\":{\"30\":1,\"33\":1,\"34\":1}}],[\"stn3d\",{\"1\":{\"30\":1,\"32\":5,\"33\":1,\"34\":2}}],[\"staticmethod\",{\"1\":{\"107\":2}}],[\"state=hidden\",{\"1\":{\"103\":1}}],[\"state\",{\"1\":{\"53\":1,\"100\":2,\"102\":1,\"118\":1}}],[\"states=outputs\",{\"1\":{\"103\":1}}],[\"states=output\",{\"1\":{\"103\":1}}],[\"states=encoder\",{\"1\":{\"103\":1}}],[\"states=all\",{\"1\":{\"103\":1}}],[\"states=false\",{\"1\":{\"103\":1}}],[\"states=none\",{\"1\":{\"103\":5}}],[\"states=image\",{\"1\":{\"100\":1,\"102\":1,\"103\":1}}],[\"states\",{\"1\":{\"7\":4,\"51\":16,\"52\":4,\"53\":2,\"55\":2,\"57\":4,\"58\":8,\"61\":8,\"62\":6,\"64\":1,\"70\":1,\"103\":21,\"104\":1}}],[\"stage流程\",{\"1\":{\"99\":1}}],[\"stage\",{\"0\":{\"100\":1,\"104\":1},\"1\":{\"99\":3,\"100\":1,\"104\":1}}],[\"standing\",{\"1\":{\"91\":2}}],[\"start\",{\"1\":{\"66\":12,\"67\":15,\"68\":5,\"93\":5,\"95\":5}}],[\"stack\",{\"1\":{\"10\":2,\"48\":1,\"80\":1,\"91\":1,\"102\":3,\"107\":1}}],[\"steps=100\",{\"1\":{\"45\":2}}],[\"step\",{\"0\":{\"8\":1,\"9\":1,\"10\":1,\"11\":1,\"12\":1},\"1\":{\"7\":11,\"45\":4,\"103\":5,\"114\":1}}],[\"silhouette\",{\"1\":{\"91\":1}}],[\"sim\",{\"1\":{\"101\":13,\"102\":4}}],[\"similarities\",{\"1\":{\"93\":2,\"94\":2,\"95\":4}}],[\"similarity\",{\"1\":{\"90\":1,\"91\":1,\"93\":2,\"94\":1,\"95\":3,\"101\":4}}],[\"simply\",{\"1\":{\"53\":1}}],[\"single\",{\"1\":{\"22\":1}}],[\"side\",{\"1\":{\"7\":2}}],[\"size的四倍\",{\"1\":{\"115\":1}}],[\"size是transformer\",{\"1\":{\"115\":1}}],[\"size就是对应通过embedding层后每个token的dim\",{\"1\":{\"115\":1}}],[\"size也是同样处理手段\",{\"1\":{\"100\":1}}],[\"size=768\",{\"1\":{\"118\":1}}],[\"size=none\",{\"1\":{\"111\":1,\"114\":1}}],[\"size=img\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"size=patch\",{\"1\":{\"109\":1,\"110\":1,\"111\":1,\"114\":1}}],[\"size=224\",{\"1\":{\"109\":1,\"110\":1,\"111\":1,\"114\":1,\"118\":1}}],[\"size=batch\",{\"1\":{\"108\":2}}],[\"size=64\",{\"1\":{\"93\":2,\"95\":2}}],[\"size=args\",{\"1\":{\"48\":1}}],[\"size=16\",{\"1\":{\"45\":2,\"109\":1,\"110\":1,\"111\":1,\"114\":1,\"118\":1}}],[\"size=1\",{\"1\":{\"32\":1}}],[\"size\",{\"1\":{\"7\":6,\"8\":1,\"9\":2,\"10\":12,\"11\":3,\"12\":4,\"32\":3,\"33\":2,\"34\":1,\"36\":2,\"48\":1,\"49\":7,\"51\":5,\"53\":2,\"55\":1,\"57\":15,\"58\":3,\"61\":3,\"62\":3,\"63\":1,\"64\":1,\"66\":12,\"67\":3,\"69\":2,\"70\":13,\"78\":2,\"79\":4,\"80\":1,\"82\":4,\"83\":1,\"84\":3,\"93\":4,\"95\":4,\"100\":6,\"101\":8,\"102\":10,\"103\":7,\"104\":4,\"107\":1,\"108\":2,\"109\":24,\"110\":4,\"111\":2,\"113\":10,\"114\":4}}],[\"span\",{\"1\":{\"66\":1,\"68\":2}}],[\"spatial\",{\"1\":{\"7\":5,\"12\":3,\"32\":1}}],[\"special\",{\"1\":{\"46\":6,\"68\":2,\"104\":1}}],[\"spidercnn\",{\"1\":{\"37\":2}}],[\"splitext\",{\"1\":{\"93\":1,\"95\":1,\"107\":1}}],[\"split\",{\"1\":{\"7\":2,\"66\":2,\"67\":1,\"93\":1,\"95\":1,\"107\":1,\"108\":1}}],[\"spring\",{\"1\":{\"2\":1}}],[\"b为批量大小\",{\"1\":{\"113\":1}}],[\"bool\",{\"1\":{\"118\":1}}],[\"bos\",{\"1\":{\"103\":3,\"104\":3}}],[\"bottleneck\",{\"1\":{\"30\":1,\"37\":1}}],[\"bottle\",{\"1\":{\"7\":1}}],[\"but\",{\"1\":{\"57\":1,\"62\":1}}],[\"build\",{\"1\":{\"46\":1}}],[\"billion\",{\"1\":{\"115\":1}}],[\"bi\",{\"1\":{\"102\":2}}],[\"bias=qkv\",{\"1\":{\"112\":1,\"113\":1,\"114\":1}}],[\"bias=true\",{\"1\":{\"111\":1,\"114\":1}}],[\"bias=false\",{\"1\":{\"62\":1,\"112\":1,\"113\":1}}],[\"bias\",{\"1\":{\"62\":3,\"105\":1,\"112\":1,\"113\":1,\"114\":1}}],[\"bit\",{\"1\":{\"57\":1}}],[\"bin\",{\"1\":{\"45\":1}}],[\"binary\",{\"0\":{\"2\":1},\"1\":{\"0\":1}}],[\"binaryoracle\",{\"1\":{\"0\":1}}],[\"bytenet和convs2s等网络模型\",{\"1\":{\"73\":1}}],[\"by\",{\"1\":{\"53\":1}}],[\"beam数量\",{\"1\":{\"104\":1}}],[\"beams=num\",{\"1\":{\"104\":1}}],[\"beams=3\",{\"1\":{\"104\":1}}],[\"beams\",{\"1\":{\"104\":4}}],[\"beam\",{\"1\":{\"104\":1}}],[\"begin\",{\"1\":{\"103\":1}}],[\"below\",{\"1\":{\"82\":1}}],[\"between\",{\"1\":{\"57\":1}}],[\"be\",{\"1\":{\"46\":1}}],[\"bertonlymlmhead\",{\"1\":{\"103\":1}}],[\"bertoutput\",{\"1\":{\"51\":3}}],[\"bert支持的下游任务图\",{\"1\":{\"65\":1}}],[\"bertformultiplechoice\",{\"1\":{\"70\":2}}],[\"bertfortokenclassification\",{\"1\":{\"69\":2}}],[\"bertforquestionanswering\",{\"1\":{\"66\":1,\"67\":2}}],[\"bertforpretraining结构图\",{\"1\":{\"64\":1}}],[\"bertforpretraining\",{\"0\":{\"64\":1},\"1\":{\"64\":2}}],[\"bertforsequenceclassification模型结构图\",{\"1\":{\"55\":1}}],[\"bertforsequenceclassification\",{\"0\":{\"55\":1},\"1\":{\"55\":2}}],[\"bertlmheadmodel\",{\"1\":{\"103\":2}}],[\"bertlmpredictionhead结构图\",{\"1\":{\"62\":1}}],[\"bertlmpredictionhead\",{\"0\":{\"62\":1},\"1\":{\"62\":2,\"63\":1}}],[\"bertlayer模型结构图\",{\"1\":{\"51\":1}}],[\"bertlayer\",{\"0\":{\"51\":1},\"1\":{\"51\":2,\"52\":1,\"103\":3}}],[\"bertlayernorm\",{\"1\":{\"49\":1,\"51\":1,\"58\":1,\"61\":1}}],[\"bertselfoutput计算流程图\",{\"1\":{\"58\":1}}],[\"bertselfoutput\",{\"0\":{\"58\":1},\"1\":{\"58\":2,\"59\":1}}],[\"bertselfattention\",{\"0\":{\"57\":1},\"1\":{\"57\":2,\"59\":1,\"103\":2}}],[\"bertpretrainingheads结构图\",{\"1\":{\"63\":1}}],[\"bertpretrainingheads\",{\"0\":{\"63\":1},\"1\":{\"63\":2,\"64\":1}}],[\"bertpretrainedmodel\",{\"1\":{\"54\":1,\"55\":1,\"64\":1,\"67\":1,\"69\":1,\"70\":1,\"103\":1}}],[\"bertpredictionheadtransform结构图\",{\"1\":{\"61\":1}}],[\"bertpredictionheadtransform\",{\"0\":{\"61\":1},\"1\":{\"61\":2,\"62\":1}}],[\"bertpooler模型结构图\",{\"1\":{\"53\":1}}],[\"bertpooler\",{\"0\":{\"53\":1},\"1\":{\"53\":2,\"54\":1}}],[\"bertmodel模型结构图\",{\"1\":{\"54\":1}}],[\"bertmodel\",{\"0\":{\"54\":1},\"1\":{\"54\":2,\"55\":1,\"57\":1,\"64\":1,\"67\":1,\"69\":1,\"70\":1,\"103\":2}}],[\"bertattention计算流程图\",{\"1\":{\"59\":1}}],[\"bertattention\",{\"0\":{\"56\":1,\"59\":1},\"1\":{\"51\":1,\"59\":2}}],[\"bertintermediate\",{\"1\":{\"51\":3}}],[\"bertencoder模型结构图\",{\"1\":{\"52\":1}}],[\"bertencoder\",{\"0\":{\"50\":1,\"52\":1},\"1\":{\"52\":2,\"54\":1,\"103\":2}}],[\"bertembeddings会将text\",{\"1\":{\"102\":1}}],[\"bertembeddings\",{\"0\":{\"49\":1},\"1\":{\"49\":2,\"54\":1,\"102\":1}}],[\"berttokenizer\",{\"1\":{\"46\":3,\"66\":1}}],[\"berttokenizer中的特殊token\",{\"1\":{\"46\":1}}],[\"bert文本分类实战\",{\"1\":{\"44\":1}}],[\"bert\",{\"0\":{\"44\":1},\"1\":{\"45\":16,\"46\":1,\"55\":2,\"64\":2,\"66\":5,\"67\":2,\"68\":9,\"69\":2,\"70\":7,\"100\":2,\"102\":1,\"103\":2}}],[\"block第一个全连接的节点个数\",{\"1\":{\"115\":1}}],[\"block的次数\",{\"1\":{\"115\":1}}],[\"block块序列\",{\"1\":{\"114\":2}}],[\"block\",{\"1\":{\"112\":3,\"114\":1}}],[\"blocks\",{\"1\":{\"25\":7,\"114\":2}}],[\"blob\",{\"1\":{\"107\":1}}],[\"blip\",{\"1\":{\"98\":2,\"99\":1}}],[\"blip2qformer\",{\"1\":{\"100\":1,\"104\":2}}],[\"blip2qformer核心代码实现如下\",{\"1\":{\"100\":1}}],[\"blip2\",{\"1\":{\"97\":1}}],[\"blip2base\",{\"1\":{\"7\":1,\"100\":1,\"104\":1}}],[\"black\",{\"1\":{\"91\":1}}],[\"bn5\",{\"1\":{\"32\":2}}],[\"bn4\",{\"1\":{\"32\":2}}],[\"bn3\",{\"1\":{\"32\":2,\"34\":2,\"36\":2}}],[\"bn2\",{\"1\":{\"22\":2,\"25\":2,\"32\":2,\"34\":2,\"35\":2,\"36\":2}}],[\"bn1\",{\"1\":{\"22\":2,\"25\":2,\"32\":2,\"34\":2,\"35\":2,\"36\":2}}],[\"bn\",{\"1\":{\"21\":2,\"25\":7,\"32\":1}}],[\"bns\",{\"1\":{\"21\":3,\"25\":3}}],[\"backward\",{\"1\":{\"114\":1}}],[\"bar\",{\"1\":{\"107\":1}}],[\"bart\",{\"1\":{\"68\":1}}],[\"baidu\",{\"1\":{\"107\":1,\"118\":1}}],[\"bag\",{\"1\":{\"96\":3}}],[\"basemodeloutputwithpastandcrossattentions\",{\"1\":{\"103\":1}}],[\"basename\",{\"1\":{\"93\":1,\"95\":1}}],[\"base\",{\"1\":{\"45\":6,\"108\":1,\"118\":7}}],[\"based\",{\"1\":{\"37\":2,\"40\":1,\"104\":1}}],[\"ball\",{\"1\":{\"19\":4,\"21\":6,\"25\":2}}],[\"batch和text\",{\"1\":{\"100\":1}}],[\"batches\",{\"1\":{\"84\":1,\"93\":2,\"95\":2}}],[\"batched\",{\"1\":{\"10\":1}}],[\"batchsize\",{\"1\":{\"32\":2,\"33\":1,\"36\":2}}],[\"batchnorm\",{\"1\":{\"21\":1,\"34\":1}}],[\"batchnorm2d\",{\"1\":{\"21\":1,\"25\":1}}],[\"batchnorm1d\",{\"1\":{\"8\":2,\"12\":1,\"22\":2,\"25\":2,\"32\":5,\"34\":3,\"35\":2,\"36\":3}}],[\"batch\",{\"1\":{\"7\":2,\"10\":7,\"12\":1,\"21\":13,\"32\":6,\"33\":2,\"45\":2,\"48\":4,\"57\":2,\"66\":6,\"67\":3,\"69\":2,\"70\":7,\"84\":2,\"93\":9,\"95\":9,\"100\":1,\"101\":2,\"102\":1,\"103\":1,\"104\":3,\"107\":4,\"108\":2,\"113\":10}}],[\"bs\",{\"1\":{\"10\":2,\"102\":19}}],[\"bmm\",{\"1\":{\"8\":3,\"12\":4,\"32\":1,\"33\":1,\"34\":2}}],[\"b\",{\"1\":{\"7\":15,\"8\":11,\"10\":1,\"12\":28,\"21\":56,\"22\":2,\"25\":11,\"32\":3,\"34\":4,\"36\":7,\"46\":1,\"70\":1,\"90\":2,\"98\":2,\"100\":8,\"101\":17,\"102\":4,\"104\":3,\"108\":3,\"109\":8,\"110\":7,\"111\":5,\"113\":3,\"114\":5,\"115\":1,\"118\":2}}],[\"mydataset\",{\"1\":{\"107\":1,\"108\":2}}],[\"m\",{\"1\":{\"82\":3,\"115\":1}}],[\"mseloss\",{\"1\":{\"55\":1}}],[\"msg方法虽然有效\",{\"1\":{\"26\":1}}],[\"msg相当于并联了多个hierarchical\",{\"1\":{\"25\":1}}],[\"msg的关键优点在于它通过在训练期间的随机输入丢弃\",{\"1\":{\"25\":1}}],[\"msg通过应用不同尺度的分组层\",{\"1\":{\"24\":1}}],[\"msg\",{\"1\":{\"23\":1,\"25\":3}}],[\"most\",{\"1\":{\"94\":10,\"95\":10}}],[\"motorcycle\",{\"1\":{\"91\":1}}],[\"motions\",{\"1\":{\"41\":1}}],[\"modal\",{\"1\":{\"101\":2}}],[\"modulelist\",{\"1\":{\"21\":2,\"25\":4,\"52\":1}}],[\"module\",{\"1\":{\"8\":1,\"12\":3,\"21\":1,\"22\":1,\"25\":2,\"32\":1,\"34\":1,\"35\":1,\"36\":1,\"49\":1,\"51\":3,\"52\":3,\"53\":1,\"57\":1,\"58\":1,\"59\":1,\"61\":1,\"62\":1,\"63\":1,\"75\":1,\"76\":1,\"78\":1,\"79\":1,\"80\":1,\"82\":1,\"83\":1,\"84\":1,\"102\":1,\"103\":5,\"109\":1,\"110\":2,\"111\":1,\"112\":2,\"113\":1,\"114\":1}}],[\"model是decoder输出的大小\",{\"1\":{\"76\":1}}],[\"models\",{\"1\":{\"45\":1,\"118\":1}}],[\"modelnet40\",{\"1\":{\"37\":2}}],[\"model\",{\"1\":{\"7\":3,\"9\":2,\"10\":1,\"11\":3,\"22\":2,\"25\":2,\"45\":6,\"53\":1,\"67\":1,\"76\":2,\"84\":7,\"91\":2,\"93\":16,\"95\":16,\"96\":1,\"104\":2,\"114\":2,\"118\":6}}],[\"mode\",{\"1\":{\"7\":2,\"107\":2}}],[\"mode=false\",{\"1\":{\"7\":1}}],[\"mvcnn\",{\"1\":{\"37\":3}}],[\"methods\",{\"1\":{\"92\":1}}],[\"metamind\",{\"1\":{\"0\":1}}],[\"memory\",{\"1\":{\"75\":2,\"82\":2,\"83\":2}}],[\"mesh\",{\"1\":{\"39\":1}}],[\"mean\",{\"1\":{\"33\":1,\"102\":1,\"103\":1}}],[\"million\",{\"1\":{\"115\":1}}],[\"mixer\",{\"1\":{\"112\":1}}],[\"mixed\",{\"1\":{\"57\":6,\"103\":2}}],[\"might\",{\"1\":{\"57\":1}}],[\"min\",{\"1\":{\"40\":1,\"67\":4,\"93\":1,\"95\":1,\"104\":2}}],[\"minibatch\",{\"1\":{\"90\":2}}],[\"mini\",{\"1\":{\"21\":1,\"22\":3}}],[\"missing\",{\"1\":{\"29\":1}}],[\"mrg策略在处理每个局部区域时\",{\"1\":{\"26\":1}}],[\"mrg通过结合来自不同分辨率的特征来实现效率和适应性的平衡\",{\"1\":{\"26\":1}}],[\"mrg为一种低成本的替代方案\",{\"1\":{\"26\":1}}],[\"mrg\",{\"1\":{\"23\":1}}],[\"mt\",{\"1\":{\"8\":2,\"12\":6}}],[\"make\",{\"1\":{\"113\":1}}],[\"main\",{\"1\":{\"97\":1}}],[\"made\",{\"1\":{\"82\":1}}],[\"macos\",{\"1\":{\"72\":1}}],[\"macos系统某些包的加载和依赖关系上存在问题\",{\"1\":{\"5\":1}}],[\"matplotlib\",{\"1\":{\"95\":1}}],[\"matching\",{\"0\":{\"102\":1},\"1\":{\"94\":11,\"95\":11,\"102\":4}}],[\"math\",{\"1\":{\"57\":1,\"84\":1,\"103\":1}}],[\"matmul\",{\"1\":{\"57\":2,\"84\":2,\"101\":2,\"103\":2}}],[\"matrix\",{\"1\":{\"33\":1}}],[\"map\",{\"1\":{\"48\":1,\"103\":1,\"118\":1}}],[\"maybe\",{\"1\":{\"7\":1}}],[\"max\",{\"1\":{\"7\":2,\"21\":4,\"25\":1,\"30\":6,\"32\":2,\"34\":1,\"37\":7,\"40\":1,\"45\":1,\"46\":6,\"48\":7,\"49\":1,\"67\":4,\"100\":3,\"101\":2,\"104\":2,\"114\":1}}],[\"mask部分相关的掩码逻辑\",{\"1\":{\"113\":1}}],[\"mask和casual\",{\"1\":{\"113\":1}}],[\"mask方法中\",{\"1\":{\"103\":1}}],[\"mask标注哪些image\",{\"1\":{\"100\":1}}],[\"masking\",{\"1\":{\"74\":1,\"83\":1}}],[\"masked\",{\"1\":{\"64\":5,\"75\":1,\"84\":1,\"103\":1}}],[\"mask=encoder\",{\"1\":{\"103\":1}}],[\"mask=text\",{\"1\":{\"100\":1}}],[\"mask=image\",{\"1\":{\"100\":1,\"102\":1,\"103\":1}}],[\"mask=mask\",{\"1\":{\"84\":1}}],[\"mask=head\",{\"1\":{\"55\":1,\"64\":1,\"69\":1,\"70\":1,\"103\":1}}],[\"mask=none\",{\"1\":{\"51\":1,\"52\":2,\"54\":2,\"55\":2,\"57\":2,\"59\":1,\"64\":2,\"67\":2,\"69\":2,\"70\":2,\"84\":2,\"103\":12}}],[\"mask=attention\",{\"1\":{\"46\":1,\"55\":1,\"64\":1,\"66\":1,\"67\":1,\"69\":1,\"70\":1,\"102\":1,\"103\":2}}],[\"mask=llm\",{\"1\":{\"7\":1}}],[\"mask作用图解\",{\"1\":{\"46\":2}}],[\"mask\",{\"1\":{\"7\":5,\"10\":13,\"21\":6,\"46\":14,\"48\":4,\"51\":1,\"52\":2,\"54\":8,\"55\":3,\"57\":3,\"59\":1,\"64\":2,\"66\":1,\"67\":1,\"69\":4,\"70\":6,\"74\":1,\"75\":11,\"79\":2,\"80\":3,\"82\":4,\"83\":4,\"84\":6,\"100\":2,\"101\":2,\"102\":10,\"103\":29,\"104\":1,\"113\":2}}],[\"multiply\",{\"1\":{\"113\":2}}],[\"multiple\",{\"1\":{\"70\":1}}],[\"multimodal\",{\"1\":{\"103\":2}}],[\"multinomial\",{\"1\":{\"102\":2}}],[\"multiheadedattention\",{\"1\":{\"84\":2}}],[\"multi\",{\"0\":{\"24\":1,\"26\":1},\"1\":{\"7\":2,\"8\":2,\"10\":5,\"23\":2,\"25\":1,\"28\":1,\"39\":1,\"74\":1,\"112\":1}}],[\"mlp=\",{\"1\":{\"22\":3}}],[\"mlp\",{\"0\":{\"114\":1},\"1\":{\"7\":1,\"8\":3,\"21\":8,\"25\":3,\"30\":1,\"35\":2,\"111\":1,\"112\":14,\"114\":4,\"115\":1}}],[\"建议用linux或者windows系统进行测试\",{\"1\":{\"5\":1}}],[\"待完善\",{\"0\":{\"5\":1}}],[\"hybrid\",{\"1\":{\"117\":1}}],[\"h`和\",{\"1\":{\"109\":1}}],[\"hook\",{\"1\":{\"103\":1}}],[\"horse\",{\"1\":{\"91\":1}}],[\"hub\",{\"1\":{\"95\":1}}],[\"huggingface\",{\"1\":{\"95\":1}}],[\"happy\",{\"1\":{\"113\":1}}],[\"ha=\",{\"1\":{\"107\":1}}],[\"harvardnlp\",{\"1\":{\"72\":1}}],[\"has\",{\"1\":{\"7\":1,\"46\":2,\"103\":1,\"118\":3}}],[\"hw\",{\"1\":{\"8\":1,\"12\":3}}],[\"hm\",{\"1\":{\"7\":6}}],[\"hidden\",{\"1\":{\"7\":4,\"9\":2,\"10\":3,\"11\":3,\"12\":1,\"49\":5,\"51\":24,\"52\":5,\"53\":5,\"55\":4,\"57\":8,\"58\":12,\"61\":15,\"62\":7,\"63\":1,\"64\":1,\"66\":6,\"67\":2,\"69\":3,\"70\":4,\"100\":8,\"101\":7,\"102\":7,\"103\":34,\"104\":1,\"112\":9,\"115\":1}}],[\"here\",{\"1\":{\"55\":1}}],[\"heatmap\",{\"1\":{\"7\":1}}],[\"head结构由linear+tanh激活函数+linear组成\",{\"1\":{\"114\":1}}],[\"head进行分类\",{\"1\":{\"114\":1}}],[\"head的位置是和这个\",{\"1\":{\"110\":1}}],[\"head之中再输出分类结果\",{\"1\":{\"110\":1}}],[\"heads代表transformer中multi\",{\"1\":{\"115\":1}}],[\"heads=8\",{\"1\":{\"113\":1}}],[\"heads=num\",{\"1\":{\"112\":1,\"114\":1}}],[\"heads=12\",{\"1\":{\"111\":1,\"114\":1,\"118\":1}}],[\"heads\",{\"1\":{\"8\":6,\"57\":7,\"84\":3,\"103\":1,\"112\":2,\"113\":12,\"114\":1}}],[\"head\",{\"0\":{\"114\":1},\"1\":{\"7\":1,\"52\":2,\"54\":1,\"55\":2,\"57\":10,\"64\":2,\"67\":1,\"69\":2,\"70\":2,\"74\":1,\"102\":1,\"103\":10,\"110\":1,\"111\":1,\"112\":1,\"113\":8,\"114\":3,\"115\":1,\"118\":1}}],[\"height\",{\"1\":{\"7\":1,\"107\":1}}],[\"hezhu\",{\"1\":{\"4\":1}}],[\"h\",{\"1\":{\"7\":3,\"8\":2,\"10\":1,\"30\":5,\"45\":3,\"84\":9,\"90\":1,\"100\":1,\"109\":4,\"110\":1,\"111\":1,\"114\":1}}],[\"https\",{\"1\":{\"4\":2,\"14\":3,\"27\":3,\"45\":2,\"72\":1,\"97\":2,\"107\":2,\"118\":4}}],[\"论文里也做了说明\",{\"1\":{\"115\":1}}],[\"论文里没有做解释\",{\"1\":{\"112\":1}}],[\"论文作者也对其做了实验\",{\"1\":{\"111\":1}}],[\"论文还实验了使用80个不同的prompt进行集成\",{\"1\":{\"92\":1}}],[\"论文指出\",{\"1\":{\"92\":1}}],[\"论文发现这个模型的效果最佳\",{\"1\":{\"90\":1}}],[\"论文中进行对比实验的clip模型也采用了这一配置\",{\"1\":{\"90\":1}}],[\"论文中做了\",{\"1\":{\"37\":1}}],[\"论文中的验证\",{\"1\":{\"37\":1}}],[\"论文\",{\"1\":{\"4\":1,\"14\":1,\"27\":1,\"97\":1}}],[\"论文代码解读与复现\",{\"1\":{\"4\":1}}],[\"aggregate\",{\"1\":{\"101\":2}}],[\"axis\",{\"1\":{\"94\":1,\"95\":1}}],[\"axis=0\",{\"1\":{\"90\":1}}],[\"axis=1\",{\"1\":{\"90\":3,\"93\":3,\"95\":3}}],[\"across\",{\"1\":{\"101\":2}}],[\"accu\",{\"1\":{\"114\":1}}],[\"accuracy\",{\"1\":{\"93\":8,\"95\":6}}],[\"acc\",{\"1\":{\"93\":2,\"95\":2}}],[\"actual\",{\"1\":{\"93\":2,\"95\":2}}],[\"actually\",{\"1\":{\"57\":1}}],[\"active\",{\"1\":{\"69\":8}}],[\"activation\",{\"1\":{\"53\":2}}],[\"activate\",{\"1\":{\"45\":1,\"72\":1}}],[\"act2fn\",{\"1\":{\"51\":1,\"61\":1}}],[\"act\",{\"1\":{\"51\":7,\"61\":7,\"111\":1,\"112\":6,\"114\":5}}],[\"available\",{\"1\":{\"93\":1,\"95\":1}}],[\"average\",{\"1\":{\"30\":1,\"40\":1}}],[\"at\",{\"1\":{\"91\":1}}],[\"attn\",{\"1\":{\"79\":4,\"82\":10,\"84\":7,\"103\":3,\"111\":1,\"112\":4,\"113\":10,\"114\":2}}],[\"attenion\",{\"1\":{\"78\":1}}],[\"attend\",{\"1\":{\"57\":1,\"73\":1}}],[\"atten\",{\"1\":{\"12\":10}}],[\"attention的heads数\",{\"1\":{\"115\":1}}],[\"attention的输入\",{\"1\":{\"104\":1}}],[\"attention计算\",{\"1\":{\"102\":1}}],[\"attention模块\",{\"1\":{\"100\":1}}],[\"attention可以用矩阵乘法一次计算所有的时刻\",{\"1\":{\"73\":1}}],[\"attention机制\",{\"1\":{\"73\":1}}],[\"attentions=none\",{\"1\":{\"103\":1}}],[\"attentions=all\",{\"1\":{\"103\":2}}],[\"attentions=outputs\",{\"1\":{\"103\":2}}],[\"attentions=output\",{\"1\":{\"103\":3}}],[\"attentions=false\",{\"1\":{\"103\":3}}],[\"attentions\",{\"1\":{\"55\":1,\"57\":2,\"64\":1,\"70\":1,\"103\":9}}],[\"attention\",{\"1\":{\"7\":7,\"8\":9,\"10\":12,\"12\":14,\"37\":3,\"40\":1,\"46\":6,\"48\":4,\"51\":7,\"52\":2,\"54\":8,\"55\":3,\"57\":26,\"59\":4,\"64\":2,\"66\":1,\"67\":2,\"69\":4,\"70\":6,\"74\":2,\"84\":4,\"100\":3,\"101\":3,\"102\":10,\"103\":68,\"104\":1,\"112\":2,\"113\":2}}],[\"atts=none\",{\"1\":{\"10\":1}}],[\"atts\",{\"1\":{\"7\":2,\"10\":12,\"100\":2,\"102\":11,\"103\":4,\"104\":2}}],[\"american\",{\"1\":{\"91\":1}}],[\"among\",{\"1\":{\"29\":1}}],[\"about\",{\"1\":{\"91\":1}}],[\"absolute\",{\"1\":{\"102\":1}}],[\"abstraction\",{\"1\":{\"16\":1,\"17\":2,\"22\":1,\"26\":1}}],[\"abs\",{\"1\":{\"4\":1,\"14\":1,\"27\":1,\"97\":1,\"118\":1}}],[\"annotated\",{\"1\":{\"72\":5}}],[\"answer\",{\"1\":{\"68\":3}}],[\"an\",{\"1\":{\"62\":1,\"91\":1}}],[\"and\",{\"1\":{\"4\":1,\"21\":8,\"46\":1,\"51\":1,\"55\":1,\"57\":1,\"61\":1,\"64\":1,\"66\":1,\"67\":1,\"75\":2,\"80\":1,\"82\":1,\"84\":2,\"91\":1,\"92\":1,\"103\":1,\"112\":1,\"118\":1}}],[\"applied\",{\"1\":{\"84\":1}}],[\"apply\",{\"1\":{\"57\":1,\"84\":2,\"114\":1}}],[\"append\",{\"1\":{\"10\":3,\"21\":2,\"25\":5,\"46\":1,\"93\":3,\"95\":3,\"102\":3,\"107\":5}}],[\"add\",{\"1\":{\"55\":1}}],[\"adapter\",{\"1\":{\"7\":2,\"9\":1,\"11\":1}}],[\"astronaut\",{\"1\":{\"91\":1}}],[\"astype\",{\"1\":{\"32\":1}}],[\"assume\",{\"1\":{\"84\":1}}],[\"assert\",{\"1\":{\"84\":1,\"107\":1}}],[\"as\",{\"1\":{\"49\":1,\"62\":1,\"93\":2,\"94\":1,\"95\":5,\"107\":2,\"112\":1,\"113\":1}}],[\"align=\",{\"1\":{\"107\":1}}],[\"aligned\",{\"1\":{\"90\":2}}],[\"alexnet\",{\"1\":{\"93\":1}}],[\"always\",{\"1\":{\"84\":1}}],[\"already\",{\"1\":{\"46\":1}}],[\"all=false\",{\"1\":{\"22\":2}}],[\"all=true\",{\"1\":{\"21\":1,\"22\":1}}],[\"all流程图\",{\"1\":{\"21\":1}}],[\"all\",{\"1\":{\"21\":8,\"46\":5,\"48\":16,\"57\":6,\"84\":3,\"93\":3,\"94\":1,\"95\":3,\"101\":2,\"102\":12,\"103\":1}}],[\"a^t||\",{\"1\":{\"30\":1}}],[\"a\",{\"1\":{\"30\":1,\"45\":3,\"46\":4,\"57\":1,\"70\":1,\"80\":1,\"84\":2,\"91\":16,\"92\":3,\"93\":1,\"94\":2,\"95\":3,\"98\":2}}],[\"argmax\",{\"1\":{\"66\":2,\"93\":1,\"94\":1,\"95\":2}}],[\"args\",{\"1\":{\"10\":1,\"12\":2,\"48\":1,\"110\":1,\"118\":1}}],[\"are\",{\"1\":{\"55\":2,\"62\":1,\"67\":1}}],[\"array\",{\"1\":{\"32\":1,\"93\":2,\"95\":2}}],[\"arange\",{\"1\":{\"21\":3,\"49\":1,\"90\":1,\"101\":1,\"102\":1}}],[\"arxiv\",{\"1\":{\"4\":1,\"14\":1,\"27\":1,\"97\":1,\"118\":1}}],[\"autocast\",{\"1\":{\"7\":1}}],[\"affordance\",{\"1\":{\"4\":1,\"7\":4,\"12\":5}}],[\"v0\",{\"1\":{\"118\":1}}],[\"v计算来源相同\",{\"1\":{\"113\":1}}],[\"v矩阵\",{\"1\":{\"113\":1}}],[\"v时使用偏置\",{\"1\":{\"113\":1}}],[\"vgg进行实现\",{\"1\":{\"93\":1}}],[\"vec2\",{\"1\":{\"93\":5,\"95\":5}}],[\"vec1\",{\"1\":{\"93\":5,\"95\":5}}],[\"vectors\",{\"1\":{\"84\":1}}],[\"version\",{\"1\":{\"51\":1,\"61\":1}}],[\"v\",{\"1\":{\"84\":2,\"100\":1,\"107\":4,\"113\":4}}],[\"vocab是词典大小\",{\"1\":{\"76\":1}}],[\"vocab\",{\"1\":{\"49\":2,\"62\":2,\"64\":1,\"76\":2,\"103\":1}}],[\"voxnet\",{\"1\":{\"37\":1}}],[\"voxel\",{\"1\":{\"28\":1,\"39\":1}}],[\"vs\",{\"1\":{\"36\":1,\"37\":2,\"68\":1}}],[\"validation\",{\"1\":{\"107\":1}}],[\"val\",{\"1\":{\"107\":13,\"108\":7}}],[\"valueerror\",{\"1\":{\"107\":1}}],[\"value都会进行缓存\",{\"1\":{\"103\":1}}],[\"value=self\",{\"1\":{\"103\":1}}],[\"value=none\",{\"1\":{\"103\":2}}],[\"value在seq\",{\"1\":{\"103\":1}}],[\"value传入\",{\"1\":{\"103\":1}}],[\"values=outputs\",{\"1\":{\"103\":1}}],[\"values=past\",{\"1\":{\"103\":1}}],[\"values=query\",{\"1\":{\"103\":1}}],[\"values=next\",{\"1\":{\"103\":1}}],[\"values=none\",{\"1\":{\"103\":2}}],[\"values\",{\"1\":{\"21\":1,\"32\":1,\"34\":1,\"102\":3,\"103\":8}}],[\"value\",{\"1\":{\"12\":19,\"57\":6,\"84\":7,\"103\":30}}],[\"variable\",{\"1\":{\"32\":1}}],[\"virtex\",{\"1\":{\"96\":1}}],[\"vit核心\",{\"1\":{\"119\":1}}],[\"vitjx\",{\"1\":{\"118\":1}}],[\"vit这篇论文长达二十多页\",{\"1\":{\"116\":1}}],[\"vit才会慢慢超越resnet\",{\"1\":{\"115\":1}}],[\"vit的效果表现不如resnet\",{\"1\":{\"115\":1}}],[\"vit的表现通常比同等大小的resnets要差一些\",{\"1\":{\"105\":1}}],[\"vit的表现就会超过cnn\",{\"1\":{\"105\":1}}],[\"vit仍是采用transformer中用到layer\",{\"1\":{\"112\":1}}],[\"vit虽然采用的是transformer\",{\"1\":{\"112\":1}}],[\"vit中的多头自注意力模块实现逻辑和transformer基本一致\",{\"1\":{\"113\":1}}],[\"vit中\",{\"1\":{\"111\":1}}],[\"vit原论文中最核心的结论是\",{\"1\":{\"105\":1}}],[\"vit\",{\"1\":{\"88\":2,\"90\":2,\"93\":4,\"95\":1,\"100\":1,\"103\":1,\"108\":3,\"114\":1,\"118\":6}}],[\"view\",{\"1\":{\"8\":1,\"12\":2,\"21\":11,\"22\":1,\"25\":2,\"28\":1,\"32\":3,\"34\":2,\"36\":2,\"39\":1,\"55\":4,\"57\":3,\"64\":4,\"69\":5,\"70\":6,\"84\":3,\"103\":4}}],[\"visiontransformer\",{\"1\":{\"110\":2,\"111\":2,\"114\":2,\"118\":1}}],[\"vision\",{\"0\":{\"13\":1},\"1\":{\"7\":1,\"10\":1,\"88\":1,\"90\":1,\"93\":1,\"100\":2,\"104\":1,\"105\":3,\"112\":1,\"118\":2,\"119\":1}}],[\"visual\",{\"1\":{\"4\":1,\"96\":2,\"100\":1,\"104\":2}}],[\"vlm\",{\"1\":{\"10\":1}}],[\"vl\",{\"1\":{\"2\":1,\"102\":4}}],[\"os\",{\"1\":{\"93\":11,\"94\":2,\"95\":14,\"107\":8}}],[\"optimizer\",{\"1\":{\"114\":1}}],[\"optional\",{\"1\":{\"10\":1}}],[\"opening\",{\"1\":{\"94\":1,\"95\":1}}],[\"open\",{\"1\":{\"93\":1,\"94\":1,\"95\":2,\"107\":2}}],[\"openai首先尝试了virtex模型\",{\"1\":{\"96\":1}}],[\"openai从网络上收集了4亿条数据进行实验\",{\"1\":{\"96\":1}}],[\"openai从网络上收集了总计4亿对文本和图像\",{\"1\":{\"90\":1}}],[\"openai\",{\"1\":{\"88\":1,\"93\":2,\"95\":1}}],[\"our\",{\"1\":{\"67\":1}}],[\"outside\",{\"1\":{\"67\":1}}],[\"outputs\",{\"1\":{\"45\":1,\"54\":2,\"55\":7,\"59\":2,\"64\":6,\"66\":4,\"67\":8,\"69\":6,\"70\":6,\"103\":26,\"104\":2}}],[\"output\",{\"1\":{\"45\":3,\"51\":9,\"53\":4,\"54\":7,\"55\":4,\"57\":2,\"59\":4,\"62\":2,\"63\":4,\"64\":4,\"66\":5,\"67\":2,\"69\":4,\"70\":4,\"100\":5,\"102\":4,\"103\":24}}],[\"out\",{\"1\":{\"7\":5,\"21\":7,\"25\":4,\"57\":1,\"112\":5}}],[\"one\",{\"1\":{\"114\":1}}],[\"ones\",{\"1\":{\"10\":8,\"21\":1,\"100\":1,\"102\":3,\"103\":1,\"104\":1}}],[\"on\",{\"1\":{\"84\":1,\"91\":3}}],[\"only\",{\"1\":{\"46\":1,\"62\":1,\"68\":2,\"69\":1,\"104\":1}}],[\"overwrite\",{\"1\":{\"45\":1}}],[\"o\",{\"1\":{\"37\":2}}],[\"official\",{\"1\":{\"118\":1}}],[\"off\",{\"1\":{\"94\":1,\"95\":1}}],[\"of\",{\"1\":{\"21\":1,\"46\":1,\"68\":2,\"69\":1,\"80\":1,\"82\":1,\"84\":1,\"90\":4,\"91\":6,\"92\":3,\"93\":2,\"94\":1,\"95\":2,\"96\":3,\"103\":1,\"107\":1}}],[\"ordereddict\",{\"1\":{\"114\":1}}],[\"original\",{\"1\":{\"57\":1,\"118\":1}}],[\"orthogonal\",{\"1\":{\"33\":1}}],[\"or\",{\"1\":{\"7\":1,\"29\":1,\"45\":1,\"51\":1,\"61\":1,\"90\":2,\"112\":2,\"113\":1,\"114\":2}}],[\"org\",{\"1\":{\"4\":1,\"14\":1,\"27\":1,\"97\":1,\"118\":1}}],[\"oracle\",{\"0\":{\"2\":1},\"1\":{\"0\":1}}],[\"obj\",{\"1\":{\"12\":7}}],[\"object\",{\"1\":{\"4\":1}}],[\"observations\",{\"1\":{\"4\":1}}],[\"garage\",{\"1\":{\"91\":1}}],[\"gelu\",{\"1\":{\"112\":4,\"114\":2}}],[\"generate\",{\"1\":{\"104\":2}}],[\"generative\",{\"0\":{\"104\":1},\"1\":{\"104\":1}}],[\"generation\",{\"0\":{\"103\":1},\"1\":{\"103\":1}}],[\"generation能力\",{\"1\":{\"98\":1}}],[\"generator模型结构图\",{\"1\":{\"76\":1}}],[\"generator\",{\"0\":{\"76\":1},\"1\":{\"75\":3,\"76\":2}}],[\"generic\",{\"1\":{\"83\":1}}],[\"getitem\",{\"1\":{\"107\":1}}],[\"getcwd\",{\"1\":{\"93\":1,\"95\":1}}],[\"get\",{\"1\":{\"7\":1,\"22\":2,\"25\":2,\"46\":1,\"57\":1,\"93\":10,\"94\":4,\"95\":12}}],[\"gpt\",{\"0\":{\"103\":1},\"1\":{\"68\":2}}],[\"gpu上训练12天\",{\"1\":{\"90\":1}}],[\"gpu上训练18天\",{\"1\":{\"90\":1}}],[\"gpu上的矩阵运算都是充分优化和高度并行的\",{\"1\":{\"73\":1}}],[\"gpu\",{\"1\":{\"7\":1,\"37\":1,\"45\":2,\"73\":1}}],[\"gleu\",{\"1\":{\"51\":2}}],[\"global\",{\"1\":{\"32\":1,\"34\":8,\"35\":1,\"36\":1,\"37\":1}}],[\"guid\",{\"1\":{\"46\":1}}],[\"google\",{\"1\":{\"118\":2}}],[\"googleapis\",{\"1\":{\"45\":1}}],[\"golang\",{\"1\":{\"2\":1}}],[\"git\",{\"1\":{\"45\":2,\"72\":1}}],[\"github\",{\"1\":{\"4\":1,\"14\":2,\"27\":2,\"45\":1,\"72\":1,\"97\":1,\"107\":1,\"118\":2}}],[\"grams\",{\"1\":{\"96\":2}}],[\"gradients\",{\"1\":{\"103\":1}}],[\"grad\",{\"1\":{\"91\":2,\"93\":2,\"95\":2,\"102\":1,\"118\":1}}],[\"grasp\",{\"1\":{\"7\":1}}],[\"grids\",{\"1\":{\"39\":1}}],[\"grid\",{\"1\":{\"28\":1,\"109\":3}}],[\"grounded\",{\"0\":{\"103\":1},\"1\":{\"103\":1}}],[\"grounding\",{\"1\":{\"4\":1}}],[\"group流程图\",{\"1\":{\"21\":1}}],[\"grouped\",{\"1\":{\"21\":12,\"25\":13}}],[\"group\",{\"1\":{\"21\":23,\"22\":3,\"25\":3}}],[\"grouping\",{\"0\":{\"19\":1,\"24\":1,\"26\":1},\"1\":{\"16\":1,\"17\":2,\"19\":1,\"22\":4,\"23\":2}}],[\"g\",{\"1\":{\"7\":1,\"30\":1}}],[\"l为灰度图片\",{\"1\":{\"107\":1}}],[\"ln\",{\"1\":{\"100\":1,\"104\":1}}],[\"llama\",{\"0\":{\"85\":1}}],[\"llm\",{\"1\":{\"3\":1,\"7\":12,\"9\":3,\"10\":14,\"11\":4,\"12\":1,\"98\":1,\"104\":4}}],[\"lm\",{\"1\":{\"64\":5,\"103\":7}}],[\"lmaffordance3d\",{\"0\":{\"4\":1,\"7\":1},\"1\":{\"4\":1,\"7\":1}}],[\"l3\",{\"1\":{\"22\":4,\"25\":4}}],[\"l2\",{\"1\":{\"22\":4,\"25\":4,\"90\":2}}],[\"l1\",{\"1\":{\"22\":4,\"25\":4}}],[\"l\",{\"1\":{\"12\":5,\"30\":1,\"45\":3,\"90\":5,\"93\":1}}],[\"linalg\",{\"1\":{\"93\":2,\"95\":2}}],[\"lin\",{\"1\":{\"84\":2}}],[\"linears\",{\"1\":{\"84\":3}}],[\"linear\",{\"1\":{\"9\":2,\"11\":2,\"12\":5,\"22\":3,\"25\":3,\"32\":3,\"35\":3,\"51\":2,\"53\":1,\"55\":1,\"57\":3,\"58\":1,\"61\":1,\"62\":1,\"63\":1,\"66\":1,\"67\":1,\"69\":1,\"70\":1,\"76\":1,\"84\":3,\"112\":2,\"113\":2,\"114\":2}}],[\"like\",{\"0\":{\"101\":1,\"103\":1},\"1\":{\"49\":1}}],[\"listdir\",{\"1\":{\"93\":1,\"95\":1,\"107\":2}}],[\"list列表组装起来得到需要的dataset\",{\"1\":{\"46\":1}}],[\"list\",{\"1\":{\"10\":1,\"21\":2,\"25\":15,\"107\":2}}],[\"location=device\",{\"1\":{\"118\":1}}],[\"locality\",{\"1\":{\"105\":1}}],[\"local\",{\"1\":{\"15\":3,\"21\":2,\"48\":1,\"93\":2,\"95\":2}}],[\"load\",{\"1\":{\"118\":2}}],[\"loader\",{\"1\":{\"108\":2,\"114\":2}}],[\"loading\",{\"1\":{\"93\":1,\"95\":1}}],[\"loal\",{\"1\":{\"15\":1}}],[\"looking\",{\"1\":{\"91\":1}}],[\"lower\",{\"1\":{\"45\":1,\"93\":1,\"95\":1}}],[\"logits外\",{\"1\":{\"118\":1}}],[\"logits=prediction\",{\"1\":{\"103\":1}}],[\"logits=false\",{\"1\":{\"103\":1,\"118\":1}}],[\"logits\",{\"1\":{\"55\":5,\"66\":12,\"67\":15,\"69\":6,\"70\":6,\"90\":3,\"102\":2,\"103\":2,\"114\":3,\"118\":3}}],[\"logging\",{\"1\":{\"45\":1}}],[\"log\",{\"1\":{\"22\":1,\"25\":1,\"35\":2,\"36\":3,\"76\":1}}],[\"longtensor\",{\"1\":{\"104\":1}}],[\"long\",{\"1\":{\"10\":1,\"21\":5,\"49\":1,\"100\":1,\"102\":4,\"103\":1,\"104\":1}}],[\"longest\",{\"1\":{\"7\":1}}],[\"loss=lm\",{\"1\":{\"103\":1}}],[\"loss\",{\"0\":{\"101\":1,\"102\":1,\"103\":1},\"1\":{\"7\":8,\"33\":2,\"55\":8,\"64\":10,\"67\":10,\"69\":11,\"70\":5,\"90\":8,\"101\":2,\"102\":1,\"103\":10,\"114\":3}}],[\"learnable\",{\"1\":{\"111\":1}}],[\"learned\",{\"1\":{\"90\":3,\"100\":2}}],[\"learner\",{\"1\":{\"15\":2}}],[\"learning的核心思想是通过设计合适的prompt\",{\"1\":{\"92\":1}}],[\"learning或prompt\",{\"1\":{\"92\":1}}],[\"learning\",{\"0\":{\"100\":1,\"101\":1,\"104\":1},\"1\":{\"45\":1,\"96\":2,\"100\":1,\"104\":1}}],[\"level\",{\"1\":{\"26\":1}}],[\"len的维度拼接起来\",{\"1\":{\"103\":1}}],[\"len维度上拼接起来\",{\"1\":{\"102\":2}}],[\"len和hidden\",{\"1\":{\"100\":1}}],[\"lens\",{\"1\":{\"46\":1,\"48\":2}}],[\"len=input\",{\"1\":{\"46\":1}}],[\"length长度\",{\"1\":{\"46\":1}}],[\"length=min\",{\"1\":{\"104\":1}}],[\"length=max\",{\"1\":{\"104\":1}}],[\"length=10\",{\"1\":{\"104\":1}}],[\"length=128\",{\"1\":{\"45\":1}}],[\"length=30\",{\"1\":{\"104\":1}}],[\"length=0\",{\"1\":{\"102\":1,\"103\":2}}],[\"length=self\",{\"1\":{\"7\":1,\"100\":1}}],[\"length\",{\"1\":{\"10\":4,\"12\":1,\"46\":11,\"48\":1,\"49\":2,\"66\":4,\"70\":4,\"100\":1,\"102\":5,\"103\":3,\"104\":2}}],[\"len\",{\"1\":{\"7\":1,\"21\":1,\"25\":2,\"46\":13,\"48\":5,\"57\":2,\"67\":4,\"69\":2,\"84\":1,\"93\":1,\"95\":1,\"100\":7,\"101\":5,\"102\":10,\"103\":1,\"107\":8}}],[\"left\",{\"1\":{\"7\":1,\"79\":1}}],[\"lavis\",{\"1\":{\"97\":1}}],[\"large\",{\"1\":{\"93\":2,\"95\":1,\"96\":1}}],[\"launchpad\",{\"1\":{\"91\":1}}],[\"lambda\",{\"1\":{\"79\":1,\"82\":2}}],[\"last\",{\"1\":{\"21\":3,\"25\":3,\"100\":2,\"102\":1,\"103\":1}}],[\"layer=act\",{\"1\":{\"112\":1,\"114\":1}}],[\"layer=norm\",{\"1\":{\"114\":1}}],[\"layer=none\",{\"1\":{\"109\":1,\"110\":1,\"111\":3,\"114\":3}}],[\"layer=nn\",{\"1\":{\"112\":3}}],[\"layer的任务是通过中心点找到邻居点\",{\"1\":{\"19\":1}}],[\"layers\",{\"1\":{\"17\":1,\"52\":1,\"57\":1,\"80\":3,\"83\":2,\"103\":1}}],[\"layers组成\",{\"1\":{\"17\":1}}],[\"layers主要包括3个部分\",{\"1\":{\"16\":1}}],[\"layer\",{\"0\":{\"18\":1,\"19\":1,\"20\":1},\"1\":{\"16\":3,\"17\":6,\"49\":1,\"51\":3,\"52\":4,\"57\":21,\"58\":1,\"61\":1,\"74\":1,\"80\":6,\"83\":6,\"103\":36,\"109\":3,\"110\":2,\"111\":1,\"112\":4,\"114\":8}}],[\"layernorm\",{\"1\":{\"12\":6,\"49\":2,\"51\":2,\"58\":2,\"61\":2,\"78\":2,\"80\":1,\"83\":1,\"102\":1,\"112\":1,\"114\":2}}],[\"label=none\",{\"1\":{\"64\":1}}],[\"label=label\",{\"1\":{\"46\":1}}],[\"labels=labels\",{\"1\":{\"103\":1}}],[\"labels=none\",{\"1\":{\"55\":1,\"64\":1,\"69\":1,\"70\":1,\"103\":1}}],[\"labels\",{\"1\":{\"46\":1,\"48\":2,\"55\":8,\"64\":2,\"67\":3,\"69\":11,\"70\":2,\"90\":3,\"91\":1,\"102\":2,\"103\":8,\"107\":4,\"114\":3}}],[\"label\",{\"1\":{\"7\":3,\"46\":2,\"64\":2,\"91\":1,\"92\":2,\"101\":2,\"103\":1,\"107\":8,\"108\":4}}],[\"language生成学习\",{\"1\":{\"99\":1}}],[\"language表征学习\",{\"1\":{\"99\":1}}],[\"language\",{\"0\":{\"13\":1},\"1\":{\"4\":1,\"7\":1,\"10\":1,\"89\":1,\"92\":1,\"96\":1}}],[\"现就读于电子科技大学\",{\"1\":{\"3\":1}}],[\"现就读于四川大学\",{\"1\":{\"2\":1}}],[\"领域常用的文本transformer模型\",{\"1\":{\"90\":1}}],[\"领域中的一些对比学习方法\",{\"1\":{\"89\":1}}],[\"领域\",{\"1\":{\"3\":1}}],[\"转化成功之后\",{\"1\":{\"45\":1}}],[\"转换为元组形式\",{\"1\":{\"109\":1}}],[\"转换为一系列高维向量表示\",{\"1\":{\"74\":1}}],[\"转换为嵌入向量\",{\"1\":{\"7\":1}}],[\"转换为\",{\"1\":{\"7\":1,\"10\":1}}],[\"转\",{\"1\":{\"3\":1}}],[\"转型\",{\"1\":{\"2\":1}}],[\"c为输入token的总维度\",{\"1\":{\"113\":1}}],[\"c=in\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"c=3\",{\"1\":{\"109\":1,\"110\":1,\"111\":1,\"114\":1}}],[\"centercrop\",{\"1\":{\"108\":1}}],[\"center\",{\"1\":{\"107\":2}}],[\"centroid\",{\"1\":{\"21\":2}}],[\"centroids\",{\"1\":{\"21\":4}}],[\"current\",{\"1\":{\"93\":2,\"94\":1,\"95\":3}}],[\"cup\",{\"1\":{\"91\":1}}],[\"cuda\",{\"1\":{\"32\":2,\"33\":2,\"91\":2,\"93\":2,\"95\":2}}],[\"cbow\",{\"1\":{\"90\":1}}],[\"ckpt\",{\"1\":{\"45\":1}}],[\"cd\",{\"1\":{\"45\":1,\"72\":1}}],[\"choices\",{\"1\":{\"70\":9}}],[\"choice\",{\"1\":{\"70\":1}}],[\"checkpoint\",{\"1\":{\"45\":2}}],[\"chinesegluedatasets\",{\"1\":{\"45\":2}}],[\"chinese\",{\"1\":{\"45\":5}}],[\"chunking\",{\"1\":{\"37\":1}}],[\"channel=256\",{\"1\":{\"22\":1}}],[\"channel=128\",{\"1\":{\"22\":1}}],[\"channel=in\",{\"1\":{\"22\":1}}],[\"channel=true\",{\"1\":{\"22\":1,\"25\":1}}],[\"channel\",{\"1\":{\"21\":12,\"22\":7,\"25\":15}}],[\"channels\",{\"1\":{\"7\":1,\"107\":1}}],[\"charlesq34\",{\"1\":{\"14\":1,\"27\":1}}],[\"creates\",{\"1\":{\"46\":1}}],[\"create\",{\"1\":{\"45\":1,\"46\":1,\"72\":1}}],[\"critical\",{\"1\":{\"30\":1,\"37\":2}}],[\"crossattention\",{\"1\":{\"103\":1}}],[\"crossentropyloss\",{\"1\":{\"55\":1,\"64\":1,\"67\":1,\"69\":1,\"70\":1,\"103\":2}}],[\"cross\",{\"1\":{\"7\":1,\"12\":8,\"90\":2,\"101\":2,\"102\":1,\"103\":13}}],[\"c+d\",{\"1\":{\"21\":3}}],[\"cannot\",{\"1\":{\"113\":1}}],[\"candidates\",{\"1\":{\"93\":13,\"95\":10}}],[\"casual\",{\"1\":{\"113\":1}}],[\"case\",{\"1\":{\"45\":1}}],[\"cache\",{\"1\":{\"103\":4}}],[\"cache=use\",{\"1\":{\"103\":1}}],[\"cache=none\",{\"1\":{\"103\":1}}],[\"cache=true\",{\"1\":{\"100\":1,\"103\":2}}],[\"causallmoutputwithcrossattentions\",{\"1\":{\"103\":1}}],[\"causal\",{\"1\":{\"103\":2}}],[\"captions\",{\"1\":{\"104\":2}}],[\"captioning\",{\"1\":{\"103\":1}}],[\"caption\",{\"1\":{\"96\":1,\"103\":1}}],[\"capital\",{\"1\":{\"68\":2}}],[\"camera\",{\"1\":{\"91\":1}}],[\"category\",{\"1\":{\"93\":4,\"95\":4}}],[\"cat\",{\"1\":{\"8\":1,\"10\":3,\"12\":1,\"21\":2,\"25\":2,\"34\":1,\"91\":1,\"102\":6,\"103\":3,\"110\":1,\"111\":1,\"114\":1}}],[\"ca\",{\"1\":{\"7\":1}}],[\"count\",{\"1\":{\"93\":7,\"95\":7}}],[\"coffee\",{\"1\":{\"91\":1}}],[\"cosine\",{\"1\":{\"90\":1,\"93\":2,\"94\":1,\"95\":3}}],[\"correct\",{\"1\":{\"93\":3,\"95\":3}}],[\"corresponding\",{\"1\":{\"53\":1}}],[\"core\",{\"1\":{\"80\":1}}],[\"collate可以参考\",{\"1\":{\"107\":1}}],[\"collate\",{\"1\":{\"48\":2,\"107\":3,\"108\":4}}],[\"code\",{\"1\":{\"45\":7}}],[\"connections\",{\"1\":{\"79\":1,\"82\":1}}],[\"connection\",{\"1\":{\"74\":1}}],[\"conda\",{\"1\":{\"45\":2,\"72\":2}}],[\"contrastive\",{\"0\":{\"101\":1},\"1\":{\"96\":1,\"101\":1}}],[\"contiguous\",{\"1\":{\"36\":1,\"57\":1,\"84\":1,\"103\":3}}],[\"contextualized\",{\"1\":{\"68\":1}}],[\"contextual\",{\"1\":{\"68\":1}}],[\"context\",{\"1\":{\"12\":2,\"15\":2,\"53\":1,\"57\":9,\"66\":1,\"103\":10}}],[\"convirt基于对比学习的方法\",{\"1\":{\"96\":1}}],[\"convert\",{\"1\":{\"45\":1,\"93\":1,\"95\":1}}],[\"conv4\",{\"1\":{\"36\":2}}],[\"conv3\",{\"1\":{\"32\":2,\"34\":2,\"36\":2}}],[\"conv2\",{\"1\":{\"32\":2,\"34\":2,\"36\":2}}],[\"conv2d\",{\"1\":{\"21\":2,\"25\":3,\"109\":1}}],[\"conv1\",{\"1\":{\"32\":2,\"34\":2,\"36\":2}}],[\"conv1d\",{\"1\":{\"8\":2,\"12\":2,\"32\":3,\"34\":4,\"36\":5,\"37\":1}}],[\"conv\",{\"1\":{\"21\":2,\"25\":6}}],[\"convs\",{\"1\":{\"21\":3,\"25\":3}}],[\"config\",{\"1\":{\"9\":2,\"11\":3,\"45\":2,\"49\":9,\"51\":16,\"52\":3,\"53\":3,\"54\":5,\"55\":7,\"57\":9,\"58\":5,\"59\":3,\"61\":8,\"62\":5,\"63\":3,\"64\":5,\"66\":1,\"67\":6,\"69\":7,\"70\":5,\"103\":2}}],[\"concat\",{\"1\":{\"7\":2,\"10\":1,\"21\":1,\"25\":3,\"84\":1}}],[\"compose\",{\"1\":{\"108\":2}}],[\"compute\",{\"1\":{\"84\":1}}],[\"compatibility\",{\"1\":{\"54\":1}}],[\"com\",{\"1\":{\"4\":1,\"14\":2,\"27\":2,\"45\":2,\"72\":1,\"97\":1,\"107\":2,\"118\":3}}],[\"committer\",{\"1\":{\"2\":2}}],[\"clipprocessor\",{\"1\":{\"93\":1,\"95\":2}}],[\"clipmodel\",{\"1\":{\"93\":1,\"95\":2}}],[\"clip模型均能够以较高的置信度给出正确的分类结果\",{\"1\":{\"91\":1}}],[\"clip模型能够在没有特定任务训练数据的情况下\",{\"1\":{\"91\":1}}],[\"clip模型的一个显著优势是它能够进行zero\",{\"1\":{\"91\":1}}],[\"clip模型会预测出个可能的文本\",{\"1\":{\"90\":1}}],[\"clip包含两个核心模型\",{\"1\":{\"90\":1}}],[\"clip的训练数据采用的是文本\",{\"1\":{\"89\":1}}],[\"clip的英文全称为contrastive\",{\"1\":{\"89\":1}}],[\"clip属于基于对比学习的多模态模型\",{\"1\":{\"89\":1}}],[\"clip\",{\"0\":{\"101\":1},\"1\":{\"88\":3,\"91\":1,\"93\":3,\"95\":1,\"96\":1}}],[\"clip原始论文链接\",{\"1\":{\"87\":1}}],[\"cla\",{\"1\":{\"107\":9}}],[\"clamp\",{\"1\":{\"67\":4}}],[\"classes=5\",{\"1\":{\"118\":1}}],[\"classes=num\",{\"1\":{\"118\":1}}],[\"classes=1000\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"classes\",{\"1\":{\"110\":3,\"111\":2,\"114\":6,\"118\":2}}],[\"class=val\",{\"1\":{\"108\":1}}],[\"class=train\",{\"1\":{\"108\":1}}],[\"classification\",{\"1\":{\"46\":1}}],[\"classifier\",{\"1\":{\"45\":2,\"55\":2,\"69\":2,\"70\":2}}],[\"class\",{\"0\":{\"110\":1},\"1\":{\"7\":1,\"8\":1,\"12\":2,\"21\":1,\"22\":4,\"25\":4,\"32\":1,\"34\":1,\"35\":1,\"36\":1,\"49\":1,\"51\":3,\"52\":1,\"53\":1,\"54\":1,\"55\":1,\"57\":1,\"58\":1,\"59\":1,\"61\":1,\"62\":1,\"63\":1,\"64\":1,\"67\":1,\"69\":1,\"70\":1,\"75\":1,\"76\":1,\"78\":1,\"79\":1,\"80\":1,\"82\":1,\"83\":1,\"84\":1,\"100\":1,\"102\":1,\"103\":4,\"104\":1,\"107\":27,\"109\":1,\"110\":3,\"111\":1,\"112\":2,\"113\":1,\"114\":1}}],[\"clones\",{\"1\":{\"79\":1,\"80\":1,\"82\":1,\"83\":1,\"84\":1}}],[\"clone\",{\"1\":{\"45\":1,\"72\":1,\"102\":1,\"103\":1}}],[\"cloud\",{\"1\":{\"39\":1}}],[\"cls\",{\"1\":{\"22\":1,\"25\":2,\"46\":9,\"53\":1,\"64\":3,\"66\":2,\"68\":2,\"70\":1,\"103\":2,\"110\":22,\"111\":5,\"114\":5}}],[\"cl=语言嵌入维度\",{\"1\":{\"7\":1}}],[\"cl\",{\"1\":{\"7\":3}}],[\"cs\",{\"1\":{\"7\":2,\"30\":2}}],[\"csdn\",{\"1\":{\"0\":1}}],[\"cp\",{\"1\":{\"7\":1}}],[\"cpu\",{\"1\":{\"7\":1,\"73\":1,\"91\":3,\"93\":3,\"95\":3}}],[\"ci\",{\"1\":{\"7\":1}}],[\"c\",{\"1\":{\"7\":1,\"8\":9,\"12\":16,\"21\":14,\"25\":5,\"70\":1,\"90\":1,\"98\":2,\"100\":1,\"109\":6,\"110\":3,\"111\":2,\"113\":3,\"114\":2}}],[\"cnn具有两种归纳偏置\",{\"1\":{\"105\":1}}],[\"cnn\",{\"1\":{\"32\":1,\"37\":4,\"39\":3,\"90\":1,\"105\":1,\"117\":9,\"119\":1}}],[\"cn\",{\"1\":{\"4\":1}}],[\"cv\",{\"1\":{\"3\":1,\"89\":1}}],[\"知识星球\",{\"1\":{\"0\":1}}]],\"version\":2}}")).map(([e,t])=>[e,_t(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:n,options:o,id:s}})=>{const r=xt[n];e==="suggest"?self.postMessage([e,s,ve(t,r,o)]):e==="search"?self.postMessage([e,s,Ee(t,r,o,"max")]):self.postMessage({suggestions:[e,s,ve(t,r,o)],results:[e,s,Ee(t,r,o,__SLIMSEARCH_SORT_STRATEGY__)]})};
//# sourceMappingURL=index.js.map
