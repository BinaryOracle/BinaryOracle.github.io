/**
* @vue/shared v3.5.13
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/const Se={},ze=()=>{},Ce=Object.assign,Oe=Array.isArray,D=e=>typeof e=="function",Me=e=>typeof e=="string",Ne=e=>typeof e=="symbol";let X;const L=()=>X||(X=typeof globalThis<"u"?globalThis:typeof self<"u"?self:typeof window<"u"?window:typeof global<"u"?global:{});/**
* @vue/reactivity v3.5.13
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/new Set(Object.getOwnPropertyNames(Symbol).filter(e=>e!=="arguments"&&e!=="caller").map(e=>Symbol[e]).filter(Ne));function P(e){const t=e&&e.__v_raw;return t?P(t):e}function Te(e){return e?e.__v_isRef===!0:!1}/**
* @vue/runtime-core v3.5.13
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/const v=[];function kt(e){v.push(e)}function It(){v.pop()}let W=!1;function Et(e,...t){if(W)return;W=!0;const n=v.length?v[v.length-1].component:null,o=n&&n.appContext.config.warnHandler,s=Fe();if(o)A(o,n,11,[e+t.map(r=>{var i,c;return(c=(i=r.toString)==null?void 0:i.call(r))!=null?c:JSON.stringify(r)}).join(""),n&&n.proxy,s.map(({vnode:r})=>`at <${re(n,r.type)}>`).join(`
`),s]);else{const r=[`[Vue warn]: ${e}`,...t];s.length&&r.push(`
`,...$e(s)),console.warn(...r)}W=!1}function Fe(){let e=v[v.length-1];if(!e)return[];const t=[];for(;e;){const n=t[0];n&&n.vnode===e?n.recurseCount++:t.push({vnode:e,recurseCount:0});const o=e.component&&e.component.parent;e=o&&o.vnode}return t}function $e(e){const t=[];return e.forEach((n,o)=>{t.push(...o===0?[]:[`
`],...Ve(n))}),t}function Ve({vnode:e,recurseCount:t}){const n=t>0?`... (${t} recursive calls)`:"",o=e.component?e.component.parent==null:!1,s=` at <${re(e.component,e.type,o)}`,r=">"+n;return e.props?[s,...Re(e.props),r]:[s+r]}function Re(e){const t=[],n=Object.keys(e);return n.slice(0,3).forEach(o=>{t.push(...Z(o,e[o]))}),n.length>3&&t.push(" ..."),t}function Z(e,t,n){return Me(t)?(t=JSON.stringify(t),n?t:[`${e}=${t}`]):typeof t=="number"||typeof t=="boolean"||t==null?n?t:[`${e}=${t}`]:Te(t)?(t=Z(e,P(t.value),!0),n?t:[`${e}=Ref<`,t,">"]):D(t)?[`${e}=fn${t.name?`<${t.name}>`:""}`]:(t=P(t),n?t:[`${e}=`,t])}const vt={sp:"serverPrefetch hook",bc:"beforeCreate hook",c:"created hook",bm:"beforeMount hook",m:"mounted hook",bu:"beforeUpdate hook",u:"updated",bum:"beforeUnmount hook",um:"unmounted hook",a:"activated hook",da:"deactivated hook",ec:"errorCaptured hook",rtc:"renderTracked hook",rtg:"renderTriggered hook",0:"setup function",1:"render function",2:"watcher getter",3:"watcher callback",4:"watcher cleanup function",5:"native event handler",6:"component event handler",7:"vnode hook",8:"directive hook",9:"transition hook",10:"app errorHandler",11:"app warnHandler",12:"ref function",13:"async component loader",14:"scheduler flush",15:"component update",16:"app unmount cleanup function"};function A(e,t,n,o){try{return o?e(...o):e()}catch(s){ee(s,t,n)}}function ee(e,t,n,o=!0){const s=t?t.vnode:null,{errorHandler:r,throwUnhandledErrorInProduction:i}=t&&t.appContext.config||Se;if(t){let c=t.parent;const l=t.proxy,u=`https://vuejs.org/error-reference/#runtime-${n}`;for(;c;){const a=c.ec;if(a){for(let h=0;h<a.length;h++)if(a[h](e,l,u)===!1)return}c=c.parent}if(r){A(r,null,10,[e,l,u]);return}}je(e,n,s,o,i)}function je(e,t,n,o=!0,s=!1){if(s)throw e;console.error(e)}const b=[];let x=-1;const S=[];let k=null,z=0;const De=Promise.resolve();let q=null;const Le=100;function Pe(e){let t=x+1,n=b.length;for(;t<n;){const o=t+n>>>1,s=b[o],r=M(s);r<e||r===e&&s.flags&2?t=o+1:n=o}return t}function We(e){if(!(e.flags&1)){const t=M(e),n=b[b.length-1];!n||!(e.flags&2)&&t>=M(n)?b.push(e):b.splice(Pe(t),0,e),e.flags|=1,te()}}function te(){q||(q=De.then(ne))}function Ae(e){Oe(e)?S.push(...e):k&&e.id===-1?k.splice(z+1,0,e):e.flags&1||(S.push(e),e.flags|=1),te()}function qe(e){if(S.length){const t=[...new Set(S)].sort((n,o)=>M(n)-M(o));if(S.length=0,k){k.push(...t);return}for(k=t,z=0;z<k.length;z++){const n=k[z];n.flags&4&&(n.flags&=-2),n.flags&8||n(),n.flags&=-2}k=null,z=0}}const M=e=>e.id==null?e.flags&2?-1:1/0:e.id;function ne(e){const t=ze;try{for(x=0;x<b.length;x++){const n=b[x];n&&!(n.flags&8)&&(n.flags&4&&(n.flags&=-2),A(n,n.i,n.i?15:14),n.flags&4||(n.flags&=-2))}}finally{for(;x<b.length;x++){const n=b[x];n&&(n.flags&=-2)}x=-1,b.length=0,qe(e),q=null,(b.length||S.length)&&ne(e)}}function St(e,t){const n=e.get(t)||0;if(n>Le){const o=t.i,s=o&&se(o.type);return ee(`Maximum recursive updates exceeded${s?` in component <${s}>`:""}. This means you have a reactive effect that is mutating its own dependencies and thus recursively triggering itself. Possible sources include component template, render function, updated hook or watcher source function.`,null,10),!0}return e.set(t,n+1),!1}const H=new Map,F=new Map;function zt(e,t){return F.has(e)?!1:(F.set(e,{initialDef:$(t),instances:new Set}),!0)}function $(e){return Je(e)?e.__vccOpts:e}function Ct(e,t){const n=F.get(e);n&&(n.initialDef.render=t,[...n.instances].forEach(o=>{t&&(o.render=t,$(o.type).render=t),o.renderCache=[],o.update()}))}function Ot(e,t){const n=F.get(e);if(!n)return;t=$(t),oe(n.initialDef,t);const o=[...n.instances];for(let s=0;s<o.length;s++){const r=o[s],i=$(r.type);let c=H.get(i);c||(i!==n.initialDef&&oe(i,t),H.set(i,c=new Set)),c.add(r),r.appContext.propsCache.delete(r.type),r.appContext.emitsCache.delete(r.type),r.appContext.optionsCache.delete(r.type),r.ceReload?(c.add(r),r.ceReload(t.styles),c.delete(r)):r.parent?We(()=>{r.parent.update(),c.delete(r)}):r.appContext.reload?r.appContext.reload():typeof window<"u"?window.location.reload():console.warn("[HMR] Root or manually mounted instance modified. Full reload required."),r.root.ce&&r!==r.root&&r.root.ce._removeChildStyle(i)}Ae(()=>{H.clear()})}function oe(e,t){Ce(e,t);for(const n in e)n!=="__file"&&!(n in t)&&delete e[n]}function Mt(e){return(t,n)=>{try{return e(t,n)}catch(o){console.error(o),console.warn("[HMR] Something went wrong during Vue component hot-reload. Full reload required.")}}}L().requestIdleCallback,L().cancelIdleCallback;const Nt={};{const e=L(),t=(n,o)=>{let s;return(s=e[n])||(s=e[n]=[]),s.push(o),r=>{s.length>1?s.forEach(i=>i(r)):s[0](r)}};t("__VUE_INSTANCE_SETTERS__",n=>n),t("__VUE_SSR_SETTERS__",n=>n)}const He=/(?:^|[-_])(\w)/g,Ue=e=>e.replace(He,t=>t.toUpperCase()).replace(/[-_]/g,"");function se(e,t=!0){return D(e)?e.displayName||e.name:e.name||t&&e.__name}function re(e,t,n=!1){let o=se(t);if(!o&&t.__file){const s=t.__file.match(/([^/\\]+)\.\w+$/);s&&(o=s[1])}if(!o&&e&&e.parent){const s=r=>{for(const i in r)if(r[i]===t)return i};o=s(e.components||e.parent.type.components)||s(e.appContext.components)}return o?Ue(o):n?"App":"Anonymous"}function Je(e){return D(e)&&"__vccOpts"in e}[...new Array(6)].map((e,t)=>`[vp-content] h${t+1}`).join(",");const{entries:Ge}=Object,{fromEntries:Be}=Object,Ye="ENTRIES",ie="KEYS",ce="VALUES",y="";class U{set;_type;_path;constructor(t,n){const o=t._tree,s=Array.from(o.keys());this.set=t,this._type=n,this._path=s.length>0?[{node:o,keys:s}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:n}=C(this._path);if(C(n)===y)return{done:!1,value:this.result()};const o=t.get(C(n));return this._path.push({node:o,keys:Array.from(o.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=C(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>C(t)).filter(t=>t!==y).join("")}value(){return C(this._path).node.get(y)}result(){switch(this._type){case ce:return this.value();case ie:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const C=e=>e[e.length-1],Ke=(e,t,n)=>{const o=new Map;if(typeof t!="string")return o;const s=t.length+1,r=s+n,i=new Uint8Array(r*s).fill(n+1);for(let c=0;c<s;++c)i[c]=c;for(let c=1;c<r;++c)i[c*s]=c;return le(e,t,n,o,i,1,s,""),o},le=(e,t,n,o,s,r,i,c)=>{const l=r*i;e:for(const u of e.keys())if(u===y){const a=s[l-1];a<=n&&o.set(c,[e.get(u),a])}else{let a=r;for(let h=0;h<u.length;++h,++a){const g=u[h],m=i*a,w=m-i;let d=s[m];const f=Math.max(0,a-n-1),p=Math.min(i-1,a+n);for(let _=f;_<p;++_){const I=g!==t[_],j=s[w+_]+ +I,T=s[w+_+1]+1,E=s[m+_]+1,O=s[m+_+1]=Math.min(j,T,E);O<d&&(d=O)}if(d>n)continue e}le(e.get(u),t,n,o,s,a,i,c+u)}};let ue=class N{_tree;_prefix;_size=void 0;constructor(t=new Map,n=""){this._tree=t,this._prefix=n}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[n,o]=V(this._tree,t.slice(this._prefix.length));if(n===void 0){const[s,r]=B(o);for(const i of s.keys())if(i!==y&&i.startsWith(r)){const c=new Map;return c.set(i.slice(r.length),s.get(i)),new N(c,t)}}return new N(n,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,Qe(this._tree,t)}entries(){return new U(this,Ye)}forEach(t){for(const[n,o]of this)t(n,o,this)}fuzzyGet(t,n){return Ke(this._tree,t,n)}get(t){const n=J(this._tree,t);return n!==void 0?n.get(y):void 0}has(t){return J(this._tree,t)?.has(y)??!1}keys(){return new U(this,ie)}set(t,n){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,G(this._tree,t).set(y,n),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,n){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const o=G(this._tree,t);return o.set(y,n(o.get(y))),this}fetch(t,n){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const o=G(this._tree,t);let s=o.get(y);return s===void 0&&o.set(y,s=n()),s}values(){return new U(this,ce)}[Symbol.iterator](){return this.entries()}static from(t){const n=new N;for(const[o,s]of t)n.set(o,s);return n}static fromObject(t){return N.from(Object.entries(t))}};const V=(e,t,n=[])=>{if(t.length===0||e==null)return[e,n];for(const o of e.keys())if(o!==y&&t.startsWith(o))return n.push([e,o]),V(e.get(o),t.slice(o.length),n);return n.push([e,t]),V(void 0,"",n)},J=(e,t)=>{if(t.length===0||!e)return e;for(const n of e.keys())if(n!==y&&t.startsWith(n))return J(e.get(n),t.slice(n.length))},G=(e,t)=>{const n=t.length;e:for(let o=0;e&&o<n;){for(const r of e.keys())if(r!==y&&t[o]===r[0]){const i=Math.min(n-o,r.length);let c=1;for(;c<i&&t[o+c]===r[c];)++c;const l=e.get(r);if(c===r.length)e=l;else{const u=new Map;u.set(r.slice(c),l),e.set(t.slice(o,o+c),u),e.delete(r),e=u}o+=c;continue e}const s=new Map;return e.set(t.slice(o),s),s}return e},Qe=(e,t)=>{const[n,o]=V(e,t);if(n!==void 0){if(n.delete(y),n.size===0)ae(o);else if(n.size===1){const[s,r]=n.entries().next().value;fe(o,s,r)}}},ae=e=>{if(e.length===0)return;const[t,n]=B(e);if(t.delete(n),t.size===0)ae(e.slice(0,-1));else if(t.size===1){const[o,s]=t.entries().next().value;o!==y&&fe(e.slice(0,-1),o,s)}},fe=(e,t,n)=>{if(e.length===0)return;const[o,s]=B(e);o.set(s+t,n),o.delete(s)},B=e=>e[e.length-1],Xe=(e,t)=>{const n=e._idToShortId.get(t);if(n!=null)return e._storedFields.get(n)},Ze=/[\n\r\p{Z}\p{P}]+/u,Y="or",de="and",et="and_not",tt=(e,t)=>{e.includes(t)||e.push(t)},he=(e,t)=>{for(const n of t)e.includes(n)||e.push(n)},pe=({score:e},{score:t})=>t-e,nt=()=>new Map,R=e=>{const t=new Map;for(const n of Object.keys(e))t.set(parseInt(n,10),e[n]);return t},ge=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,me={[Y]:(e,t)=>{for(const n of t.keys()){const o=e.get(n);if(o==null)e.set(n,t.get(n));else{const{score:s,terms:r,match:i}=t.get(n);o.score=o.score+s,o.match=Object.assign(o.match,i),he(o.terms,r)}}return e},[de]:(e,t)=>{const n=new Map;for(const o of t.keys()){const s=e.get(o);if(s==null)continue;const{score:r,terms:i,match:c}=t.get(o);he(s.terms,i),n.set(o,{score:s.score+r,terms:s.terms,match:Object.assign(s.match,c)})}return n},[et]:(e,t)=>{for(const n of t.keys())e.delete(n);return e}},ot=(e,t,n,o,s,r)=>{const{k:i,b:c,d:l}=r;return Math.log(1+(n-t+.5)/(t+.5))*(l+e*(i+1)/(e+i*(1-c+c*o/s)))},st=e=>(t,n,o)=>({term:t,fuzzy:typeof e.fuzzy=="function"?e.fuzzy(t,n,o):e.fuzzy??!1,prefix:typeof e.prefix=="function"?e.prefix(t,n,o):e.prefix===!0,termBoost:typeof e.boostTerm=="function"?e.boostTerm(t,n,o):1}),_e=(e,t,n,o)=>{for(const s of Object.keys(e._fieldIds))if(e._fieldIds[s]===n){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${o}" was not present in field "${s}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},rt=(e,t,n,o)=>{if(!e._index.has(o)){_e(e,n,t,o);return}const s=e._index.fetch(o,nt),r=s.get(t),i=r?.get(n);!r||typeof i>"u"?_e(e,n,t,o):i<=1?r.size<=1?s.delete(t):r.delete(n):r.set(n,i-1),e._index.get(o).size===0&&e._index.delete(o)},it={k:1.2,b:.7,d:.5},ct={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(Ze),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{console?.[e]?.(t)},autoVacuum:!0},ye={combineWith:Y,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:it},lt={combineWith:de,prefix:(e,t,n)=>t===n.length-1},ut={batchSize:1e3,batchWait:10},we={minDirtFactor:.1,minDirtCount:20},at={...ut,...we},be=Symbol("*"),ft=(e,t)=>{const n=new Map,o={...e._options.searchOptions,...t};for(const[s,r]of e._documentIds){const i=o.boostDocument?o.boostDocument(r,"",e._storedFields.get(s)):1;n.set(s,{score:i,terms:[],match:{}})}return n},xe=(e,t=Y)=>{if(e.length===0)return new Map;const n=t.toLowerCase();if(!(n in me))throw new Error(`Invalid combination operator: ${t}`);return e.reduce(me[n])},K=(e,t,n,o,s,r,i,c,l,u=new Map)=>{if(r==null)return u;for(const a of Object.keys(i)){const h=i[a],g=e._fieldIds[a],m=r.get(g);if(m==null)continue;let w=m.size;const d=e._avgFieldLength[g];for(const f of m.keys()){if(!e._documentIds.has(f)){rt(e,g,f,n),w-=1;continue}const p=c?c(e._documentIds.get(f),n,e._storedFields.get(f)):1;if(!p)continue;const _=m.get(f),I=e._fieldLength.get(f)[g],j=ot(_,w,e._documentCount,I,d,l),T=o*s*h*p*j,E=u.get(f);if(E){E.score+=T,tt(E.terms,t);const O=ge(E.match,n);O?O.push(a):E.match[n]=[a]}else u.set(f,{score:T,terms:[t],match:{[n]:[a]}})}}return u},dt=(e,t,n)=>{const o={...e._options.searchOptions,...n},s=(o.fields??e._options.fields).reduce((d,f)=>({...d,[f]:ge(o.boost,f)||1}),{}),{boostDocument:r,weights:i,maxFuzzy:c,bm25:l}=o,{fuzzy:u,prefix:a}={...ye.weights,...i},h=e._index.get(t.term),g=K(e,t.term,t.term,1,t.termBoost,h,s,r,l);let m,w;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const d=t.fuzzy===!0?.2:t.fuzzy,f=d<1?Math.min(c,Math.round(t.term.length*d)):d;f&&(w=e._index.fuzzyGet(t.term,f))}if(m)for(const[d,f]of m){const p=d.length-t.term.length;if(!p)continue;w?.delete(d);const _=a*d.length/(d.length+.3*p);K(e,t.term,d,_,t.termBoost,f,s,r,l,g)}if(w)for(const d of w.keys()){const[f,p]=w.get(d);if(!p)continue;const _=u*d.length/(d.length+p);K(e,t.term,d,_,t.termBoost,f,s,r,l,g)}return g},ke=(e,t,n={})=>{if(t===be)return ft(e,n);if(typeof t!="string"){const a={...n,...t,queries:void 0},h=t.queries.map(g=>ke(e,g,a));return xe(h,a.combineWith)}const{tokenize:o,processTerm:s,searchOptions:r}=e._options,i={tokenize:o,processTerm:s,...r,...n},{tokenize:c,processTerm:l}=i,u=c(t).flatMap(a=>l(a)).filter(a=>!!a).map(st(i)).map(a=>dt(e,a,i));return xe(u,i.combineWith)},Ie=(e,t,n={})=>{const{searchOptions:o}=e._options,s={...o,...n},r=ke(e,t,n),i=[];for(const[c,{score:l,terms:u,match:a}]of r){const h=u.length||1,g={id:e._documentIds.get(c),score:l*h,terms:Object.keys(a),queryTerms:u,match:a};Object.assign(g,e._storedFields.get(c)),(s.filter==null||s.filter(g))&&i.push(g)}return t===be&&s.boostDocument==null||i.sort(pe),i},ht=(e,t,n={})=>{n={...e._options.autoSuggestOptions,...n};const o=new Map;for(const{score:r,terms:i}of Ie(e,t,n)){const c=i.join(" "),l=o.get(c);l!=null?(l.score+=r,l.count+=1):o.set(c,{score:r,terms:i,count:1})}const s=[];for(const[r,{score:i,terms:c,count:l}]of o)s.push({suggestion:r,terms:c,score:i/l});return s.sort(pe),s};class pt{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(!t?.fields)throw new Error('SlimSearch: option "fields" must be provided');const n=t.autoVacuum==null||t.autoVacuum===!0?at:t.autoVacuum;this._options={...ct,...t,autoVacuum:n,searchOptions:{...ye,...t.searchOptions},autoSuggestOptions:{...lt,...t.autoSuggestOptions}},this._index=new ue,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=we,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[n,o]of this._index){const s={};for(const[r,i]of o)s[r]=Object.fromEntries(i);t.push([n,s])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,version:2}}addFields(t){for(let n=0;n<t.length;n++)this._fieldIds[t[n]]=n}}const gt=e=>new pt(e),mt=({documentCount:e,nextId:t,fieldIds:n,averageFieldLength:o,dirtCount:s,version:r},i)=>{if(r!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const c=gt(i);return c._documentCount=e,c._nextId=t,c._idToShortId=new Map,c._fieldIds=n,c._avgFieldLength=o,c._dirtCount=s??0,c._index=new ue,c},_t=(e,t)=>{const{index:n,documentIds:o,fieldLength:s,storedFields:r}=e,i=mt(e,t);i._documentIds=R(o),i._fieldLength=R(s),i._storedFields=R(r);for(const[c,l]of i._documentIds)i._idToShortId.set(l,c);for(const[c,l]of n){const u=new Map;for(const a of Object.keys(l))u.set(parseInt(a,10),R(l[a]));i._index.set(c,u)}return i},Q=(e,t)=>{const n=e.toLowerCase(),o=t.toLowerCase(),s=[];let r=0,i=0;const c=(u,a=!1)=>{let h;i===0?h=u.length>20?`… ${u.slice(-20)}`:u:a?h=u.length+i>100?`${u.slice(0,100-i)}… `:u:h=u.length>20?`${u.slice(0,20)} … ${u.slice(-20)}`:u,h&&s.push(h),i+=h.length,a||(s.push(["mark",t]),i+=t.length,i>=100&&s.push(" …"))};let l=n.indexOf(o,r);if(l===-1)return null;for(;l>=0;){const u=l+o.length;if(c(e.slice(r,l)),r=u,i>100)break;l=n.indexOf(o,r)}return i<100&&c(e.slice(r),!0),s},{entries:yt}=Object,wt=(e,t)=>t.contents.reduce((n,[,o])=>n+o,0)-e.contents.reduce((n,[,o])=>n+o,0),bt=(e,t)=>Math.max(...t.contents.map(([,n])=>n))-Math.max(...e.contents.map(([,n])=>n)),Ee=(e,t,n={},o="max")=>{const s={};return Ie(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...n}).forEach(r=>{const{id:i,terms:c,score:l}=r,u=i.includes("@"),a=i.includes("#"),[h,g]=i.split(/[#@]/),m=Number(h),w=c.sort((f,p)=>f.length-p.length).filter((f,p)=>c.slice(p+1).every(_=>!_.includes(f))),{contents:d}=s[m]??={title:"",contents:[]};if(u)d.push([{type:"customField",id:m,index:g,display:w.map(f=>r.c.map(p=>Q(p,f))).flat().filter(f=>f!==null)},l]);else{const f=w.map(p=>Q(r.h,p)).filter(p=>p!==null);if(f.length&&d.push([{type:a?"heading":"title",id:m,...a&&{anchor:g},display:f},l]),"t"in r&&r.t)for(const p of r.t){const _=w.map(I=>Q(p,I)).filter(I=>I!==null);_.length&&d.push([{type:"text",id:m,...a&&{anchor:g},display:_},l])}}}),yt(s).sort(([,r],[,i])=>(o?wt:bt)(r,i)).map(([r,{title:i,contents:c}])=>{if(!i){const l=Xe(t,r);l&&(i=l.h)}return{title:i,contents:c.map(([l])=>l)}})},ve=(e,t,n={})=>{const o=ht(t,e,{fuzzy:.2,maxFuzzy:3,...n}).map(({suggestion:s})=>s);return e.includes(" ")?o:o.filter(s=>!s.includes(" "))},xt=Be(Ge(JSON.parse("{\"/\":{\"documentCount\":36,\"nextId\":36,\"documentIds\":{\"0\":\"0\",\"1\":\"1\",\"2\":\"1#binary-oracle\",\"3\":\"1#elowen\",\"4\":\"2\",\"5\":\"3\",\"6\":\"4\",\"7\":\"4#分词处理\",\"8\":\"5\",\"9\":\"5#环境\",\"10\":\"5#背景\",\"11\":\"5#模型架构\",\"12\":\"5#encoder-decoder-结构\",\"13\":\"5#generator\",\"14\":\"5#encoder-结构\",\"15\":\"5#sublayerconnection\",\"16\":\"5#encoderlayer\",\"17\":\"5#encoder\",\"18\":\"5#decoder-结构\",\"19\":\"5#decoderlayer\",\"20\":\"5#decoder\",\"21\":\"5#多头自注意力\",\"22\":\"6\",\"23\":\"7\",\"24\":\"7#引言\",\"25\":\"7#介绍\",\"26\":\"7#训练\",\"27\":\"7#推理\",\"28\":\"7#文本描述生成\",\"29\":\"7#花卉图片分类\",\"30\":\"7#文字搜索图像\",\"31\":\"7#完整代码\",\"32\":\"7#小结\",\"33\":\"8\",\"34\":\"9\",\"35\":\"10\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1,7],\"1\":[1],\"2\":[2,12],\"3\":[1,5],\"4\":[3],\"5\":[1],\"6\":[2,2],\"7\":[1,124],\"8\":[1,2],\"9\":[1,41],\"10\":[1,23],\"11\":[1,64],\"12\":[3,30],\"13\":[1,28],\"14\":[2],\"15\":[1,33],\"16\":[1,37],\"17\":[1,37],\"18\":[2],\"19\":[1,49],\"20\":[1,39],\"21\":[1,116],\"22\":[1],\"23\":[1,3],\"24\":[1,31],\"25\":[1,20],\"26\":[1,146],\"27\":[1,156],\"28\":[1,57],\"29\":[1,245],\"30\":[1,92],\"31\":[1,210],\"32\":[1,132],\"33\":[1],\"34\":[1],\"35\":[1,3]},\"averageFieldLength\":[1.222222222222222,54.57466659439513],\"storedFields\":{\"0\":{\"h\":\"主页\",\"t\":[\"知识星球: MetaMind , 小红书: BinaryOracle , CSDN: Binary Oracle\"]},\"1\":{\"h\":\"关于我们\"},\"2\":{\"h\":\"Binary Oracle\",\"t\":[\"一名普通但十分热爱探索技术的Coder\",\"开源框架 Spring committer\",\"Golang 开源网络库 netpoll committer\",\"Javaer 转型 3D - VL 方向研究\"]},\"3\":{\"h\":\"Elowen\",\"t\":[\"一名深度学习小白\",\"CV 转 LLM 领域\"]},\"4\":{\"h\":\"3D-Vision Language\"},\"5\":{\"h\":\"大语言模型\"},\"6\":{\"h\":\"图解 Bert\",\"t\":[\"图解Bert & Bert文本分类实战\"]},\"7\":{\"h\":\"分词处理\",\"t\":[\"特殊token id:\",\"[CLS]: 101\",\"[SEP]: 102\",\"[MASK]: 103\",\"[UNK]: 100\",\"[PAD]: 0\",\"class BertTokenizer(PreTrainedTokenizer): ... def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None): \\\"\\\"\\\" Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and adding special tokens. A BERT sequence has the following format: single sequence: [CLS] X [SEP] pair of sequences: [CLS] A [SEP] B [SEP] \\\"\\\"\\\" if token_ids_1 is None: return [self.cls_token_id] + token_ids_0 + [self.sep_token_id] cls = [self.cls_token_id] sep = [self.sep_token_id] return cls + token_ids_0 + sep + token_ids_1 + sep def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None): \\\"\\\"\\\" Creates a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence pair mask has the following format: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 | first sequence | second sequence if token_ids_1 is None, only returns the first portion of the mask (0's). \\\"\\\"\\\" sep = [self.sep_token_id] cls = [self.cls_token_id] if token_ids_1 is None: return len(cls + token_ids_0 + sep) * [0] return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1] def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False): \\\"\\\"\\\" Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods. Args: token_ids_0: list of ids (must not contain special tokens) token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids for sequence pairs already_has_special_tokens: (default False) Set to True if the token list is already formated with special tokens for the model Returns: A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token. \\\"\\\"\\\" if already_has_special_tokens: if token_ids_1 is not None: raise ValueError(\\\"You should not supply a second sequence if the provided sequence of \\\" \\\"ids is already formated with special tokens for the model.\\\") return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0)) if token_ids_1 is not None: return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1] return [1] + ([0] * len(token_ids_0)) + [1]\"]},\"8\":{\"h\":\"图解Transformer\",\"t\":[\"图解Transformer & 机器翻译实战\"]},\"9\":{\"h\":\"环境\",\"t\":[\"本文基于 The Annotated Transformer 所提供的代码展开进行讲解。\",\"环境搭建遵从如下步骤即可:\",\"git clone https://github.com/harvardnlp/annotated-transformer cd annotated-transformer conda create -n annotated-transformer python=3.9.22 conda activate annotated-transformer pip install -r requirements.txt\",\"MacOS 用户本地运行时，需要将 requirements.txt 文件中的 torch == 1.11.0+cu113 改为 torch==1.11.0，因为CUDA不支持MacOS。\"]},\"10\":{\"h\":\"背景\",\"t\":[\"RNN等模型的缺点是需要顺序计算，从而很难并行。因此出现了Extended Neural GPU、ByteNet和ConvS2S等网络模型。这些模型都是以CNN为基础，这比较容易并行。但是和RNN相比，它较难学习到长距离的依赖关系。\",\"本文的Transformer使用了Self-Attention机制，它在编码每一词的时候都能够注意(attend to)整个句子，从而可以解决长距离依赖的问题，同时计算Self-Attention可以用矩阵乘法一次计算所有的时刻，因此可以充分利用计算资源(CPU/GPU上的矩阵运算都是充分优化和高度并行的)。\"]},\"11\":{\"h\":\"模型架构\",\"t\":[\"Transformer 模型架构图\",\"Transformer 是一种基于自注意力机制(Self-Attention) 的神经网络架构,其由七大主要部分构成:\",\"Encoder-Decoder 结构\",\"编码器(Encoder)：将输入序列（如句子）转换为一系列高维向量表示。\",\"解码器(Decoder)：根据编码器的输出生成目标序列（如翻译后的句子）。\",\"多头自注意力机制（Multi-Head Self-Attention）\",\"自注意力机制是 Transformer 的核心，它允许模型在处理每个词时关注输入序列中的所有词。\",\"多头自注意力机制通过并行计算多个注意力头，捕捉不同子空间的信息，从而增强模型的表达能力。\",\"位置编码（Positional Encoding）\",\"由于 Transformer 不使用传统的循环或卷积结构，它通过位置编码将序列中词的位置信息注入到输入中。位置编码通常使用正弦和余弦函数生成。\",\"前馈神经网络（Feed-Forward Neural Network）\",\"在自注意力机制之后，每个位置的输出会通过一个独立的前馈神经网络进行进一步处理。\",\"残差连接与层归一化（Residual Connection & Layer Normalization）\",\"每个子层（如自注意力层和前馈层）都使用了残差连接和层归一化，以加速训练并提高模型的稳定性。\",\"掩码机制（Masking）\",\"在解码器中，使用掩码机制确保模型在生成目标序列时只能看到当前及之前的词，而不能看到未来的词。\",\"在输入序列长度不一致时，通过填充掩码（Padding Mask）屏蔽填充部分的信息。\",\"输出层\",\"解码器的最终输出通过一个线性层和 Softmax 函数生成目标序列的概率分布。\"]},\"12\":{\"h\":\"Encoder-Decoder 结构\",\"t\":[\"EncoderDecoder模型结构图\",\"class EncoderDecoder(nn.Module): def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): \\\"Take in and process masked src and target sequences.\\\" return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) def encode(self, src, src_mask): return self.encoder(self.src_embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\"]},\"13\":{\"h\":\"Generator\",\"t\":[\"Generator模型结构图\",\"class Generator(nn.Module): # 根据Decoder的隐状态输出一个词 # d_model是Decoder输出的大小，vocab是词典大小 def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) # 全连接再加上一个softmax def forward(self, x): return F.log_softmax(self.proj(x), dim=-1)\"]},\"14\":{\"h\":\"Encoder 结构\"},\"15\":{\"h\":\"SublayerConnection\",\"t\":[\"SublayerConnection模型结构图\",\"class SublayerConnection(nn.Module): \\\"\\\"\\\" LayerNorm + sublayer(Self-Attenion/Dense) + dropout + 残差连接 为了简单，把LayerNorm放到了前面，这和原始论文稍有不同，原始论文LayerNorm在最后。 \\\"\\\"\\\" def __init__(self, size, dropout): super(SublayerConnection, self).__init__() self.norm = LayerNorm(size) self.dropout = nn.Dropout(dropout) def forward(self, x, sublayer): \\\"sublayer是传入的参数，参考DecoderLayer，它可以当成函数调用，这个函数的有一个输入参数\\\" return x + self.dropout(sublayer(self.norm(x)))\"]},\"16\":{\"h\":\"EncoderLayer\",\"t\":[\"EncoderLayer模型结构图\",\"# 编码器层 = 自注意力子层 + 前馈层 class EncoderLayer(nn.Module): def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward # 自注意力子层 和 前馈层 各需要一个 self.sublayer = clones(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): \\\"Follow Figure 1 (left) for connections.\\\" x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) return self.sublayer[1](x, self.feed_forward)\"]},\"17\":{\"h\":\"Encoder\",\"t\":[\"Encoder模型结构图\",\"class Encoder(nn.Module): \\\"Core encoder is a stack of N layers\\\" def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): \\\"Pass the input (and mask) through each layer in turn.\\\" for layer in self.layers: x = layer(x, mask) return self.norm(x)\"]},\"18\":{\"h\":\"Decoder 结构\"},\"19\":{\"h\":\"DecoderLayer\",\"t\":[\"Decoder模型结构图\",\"# 解码器层 = 自注意力子层 + 源注意力子层 + 前馈层 class DecoderLayer(nn.Module): \\\"Decoder is made of self-attn, src-attn, and feed forward (defined below)\\\" def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward # 自注意力子层 + 源注意力子层 + 前馈层 各需要一个 self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): \\\"Follow Figure 1 (right) for connections.\\\" m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward)\"]},\"20\":{\"h\":\"Decoder\",\"t\":[\"Decoder模型结构图\",\"# 解码器 = N个解码器层 + 层归一化 class Decoder(nn.Module): \\\"Generic N layer decoder with masking.\\\" def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): # 输入,编码器隐藏层输出,源掩码,目标掩码 for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x)\"]},\"21\":{\"h\":\"多头自注意力\",\"t\":[\"多头自注意力计算流程图\",\"class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): \\\"Take in model size and number of heads.\\\" super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # We assume d_v always equals d_k self.d_k = d_model // h # 每个头64维 self.h = h # 8个头 self.linears = clones(nn.Linear(d_model, d_model), 4) # W_q,W_k,W_v,W_projection self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): \\\"Implements Figure 2\\\" if mask is not None: # Same mask applied to all h heads. mask = mask.unsqueeze(1) nbatches = query.size(0) # 1) Do all the linear projections in batch from d_model => h x d_k query, key, value = [ lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) # (batches,heads,seq_len,d_k) for lin, x in zip(self.linears, (query, key, value)) ] # 2) Apply attention on all the projected vectors in batch. x, self.attn = attention( query, key, value, mask=mask, dropout=self.dropout ) # 3) \\\"Concat\\\" using a view and apply a final linear. x = ( x.transpose(1, 2) .contiguous() .view(nbatches, -1, self.h * self.d_k) ) del query del key del value return self.linears[-1](x)\",\"def attention(query, key, value, mask=None, dropout=None): \\\"Compute 'Scaled Dot Product Attention'\\\" d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) # 广播: (1,1,1,10) ---> (1,8,10,10) p_attn = scores.softmax(dim=-1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn\"]},\"22\":{\"h\":\"多模态\"},\"23\":{\"h\":\"庖丁解牛CLIP\",\"t\":[\"多模态模型CLIP原理与图片分类，文字搜索图像实战演练\",\"CLIP原始论文链接\"]},\"24\":{\"h\":\"引言\",\"t\":[\"2021 年可谓是视觉 Transformer（Vision Transformer）大放异彩的一年。自谷歌提出 ViT 之后，众多基于视觉 Transformer 的研究如潮水般涌来，广泛应用于各类计算机视觉任务。与此同时，OpenAI 在 2021 年 1 月发布的 DALL-E 和 CLIP，同样给计算机视觉领域带来了巨大影响。这两个模型都属于融合图像与文本的多模态模型，其中 DALL-E 是基于文本输入来生成图像的模型，而 CLIP 则是以文本作为监督信号，训练出具有可迁移能力的视觉模型。和 ViT 类似，DALL-E 和 CLIP 的出现也掀起了新一轮的研究热潮。\"]},\"25\":{\"h\":\"介绍\",\"t\":[\"CLIP的英文全称为Contrastive Language-Image Pre-training，它代表着一种基于对比文本-图像对的预训练方法，同时也指运用该方法构建的模型。CLIP属于基于对比学习的多模态模型。与计算机视觉（CV）领域中的一些对比学习方法，像MoCo和SimCLR有所不同，CLIP的训练数据采用的是文本-图像对，也就是一张图像搭配与之对应的文本描述。在训练过程中，借助对比学习机制，期望模型能够学习到文本和图像之间的匹配关系。\"]},\"26\":{\"h\":\"训练\",\"t\":[\"CLIP包含两个核心模型，分别是文本编码器（Text Encoder）和图像编码器（Image Encoder）。其中，文本编码器的作用是提取文本的特征，在实现时可采用自然语言处理（NLP）领域常用的文本Transformer模型；而图像编码器则用于提取图像的特征，在实际应用中可以选用常见的卷积神经网络（CNN）模型，也可以采用视觉Transformer模型。\",\"这里对提取的文本特征和图像特征进行对比学习。对于一个包含个文本-图像对的训练batch，将个文本特征和个图像特征两两组合，CLIP模型会预测出个可能的文本-图像对的相似度，这里的相似度直接计算文本特征和图像特征的余弦相似性（cosine similarity），即上图所示的矩阵。这里共有个正样本，即真正属于一对的文本和图像（矩阵中的对角线元素），而剩余的个文本-图像对为负样本，那么CLIP的训练目标就是最大个正样本的相似度，同时最小化个负样本的相似度，对应的伪代码实现如下所示：\",\"# image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter # 分别提取图像特征和文本特征 I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # 对两个特征进行线性投射，得到相同维度的特征，并进行l2归一化 I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # 计算缩放的余弦相似度：[n, n] logits = np.dot(I_e, T_e.T) * np.exp(t) # 对称的对比学习损失：等价于N个类别的cross_entropy_loss labels = np.arange(n) # 对角线元素的labels loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2\",\"为了训练CLIP模型，OpenAI从网络上收集了总计4亿对文本和图像，这些数据在论文中被称为WebImageText。若以文本单词数量来衡量，其规模与GPT-2训练时使用的WebText数据集相似。然而，从数据对的数量来看，它比谷歌的JFT-300M数据集还要多出1亿对，因此这是一个非常庞大的数据集。\",\"尽管CLIP是一个多模态模型，但其主要目的是训练可迁移的视觉模型。在论文中，文本编码器（Text Encoder）选择了一个包含6300万参数的Transformer模型，而图像编码器（Image Encoder）则采用了两种不同的架构：\",\"一种是常用的CNN架构ResNet。\",\"另一种是基于 Transformer 的ViT。\",\"ResNet包含五种不同尺寸的模型：ResNet50、ResNet101、RN50x4、RN50x16和RNx64（后三种模型是按照EfficientNet的缩放规则对ResNet分别放大4倍、16倍和64倍得到的），而ViT则选择了三种不同尺寸的模型：ViT-B/32、ViT-B/16和ViT-L/14。\",\"所有模型均训练了32个周期，使用AdamW优化器，并且在训练过程中采用了一个相对较大的批次大小：32768。由于数据量巨大，最大的ResNet模型RN50x64需要在592个V100 GPU上训练18天，而最大的ViT模型ViT-L/14则需要在256个V100 GPU上训练12天，这表明训练CLIP模型需要消耗大量的资源。对于ViT-L/14模型，还在336的分辨率下额外进行了一个周期的微调（finetune）以增强性能，论文发现这个模型的效果最佳，并将其标记为ViT-L/14@336，论文中进行对比实验的CLIP模型也采用了这一配置。\"]},\"27\":{\"h\":\"推理\",\"t\":[\"我们已经探讨了CLIP模型的运作机制，它由两个部分组成：一个视觉模型和一个文本模型。那么，如何将这个预训练的视觉模型应用到新的任务中呢？CLIP模型的一个显著优势是它能够进行zero-shot图像分类，这意味着它能够在没有任何特定任务训练数据的情况下，直接对图像进行分类。这不仅展示了CLIP的强大功能，也是其一大亮点。实现zero-shot分类的过程相当直接，可以概括为以下两个主要步骤：\",\"构建描述文本并提取特征：首先，根据任务的分类需求，为每个类别创建一个描述性的文本，例如“A photo of {label}”。这些文本随后被输入到文本编码器（Text Encoder）中，以生成相应的文本特征。如果有个类别，那么就会得到个文本特征。\",\"图像特征提取与分类：接下来，将待分类的图像输入到图像编码器（Image Encoder）中，以获取图像特征。然后，这些图像特征会与之前得到的个文本特征进行余弦相似度计算（这一过程与训练时相同）。最终，选择与图像特征相似度最高的文本所对应的类别，作为图像的分类预测结果。此外，这些相似度值可以被视为logits，通过softmax函数转换后，可以得到每个类别的预测概率。\",\"通过这种方式，CLIP模型能够在没有特定任务训练数据的情况下，直接对图像进行分类，这展示了其在图像分类任务中的灵活性和强大能力。\",\" 显然，我们通过利用CLIP模型的多模态能力，为特定任务动态构建了一个分类器。在这个过程中，文本编码器（Text Encoder）生成的文本特征相当于分类器的权重，而图像编码器（Image Encoder）提取的图像特征则是分类器的输入数据。以下是一个官方给出的CLIP模型的示例 ，该示例中的任务涉及8个类别:\",\"我们首先创建了各类别的文本描述，然后提取了相应的文本特征；\",\"然后我们读取要预测的图像，输入Image Encoder提取图像特征，并计算与文本特征的余弦相似度。\",\"# 1. 提取文本特征 texts = [ \\\"a page of text about segmentation\\\", \\\"a facial photo of a tabby cat\\\", \\\"a portrait of an astronaut with the American flag\\\", \\\"a rocket standing on a launchpad\\\", \\\"a red motorcycle standing in a garage\\\", \\\"a person looking at a camera on a tripod\\\", \\\"a black-and-white silhouette of a horse\\\", \\\"a cup of coffee on a saucer\\\" ] text_tokens = clip.tokenize([\\\"This is \\\" + desc for desc in texts]).cuda() with torch.no_grad(): text_features = model.encode_text(text_tokens).float() # 2. 提取图像特征 image_input = torch.tensor(np.stack(images)).cuda() with torch.no_grad(): image_features = model.encode_image(image_input).float() # 3. 计算余弦相似度 image_features /= image_features.norm(dim=-1, keepdim=True) text_features /= text_features.norm(dim=-1, keepdim=True) similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\",\"相似度如下所示，可以看到对于要预测的8个图像，按照最大相似度，其均能匹配到正确的文本标签：\",\"进一步地，我们也可以对得到的余弦相似度计算softmax，得到每个预测类别的概率值：\",\"text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1) top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)\",\"得到的预测概率如下所示，可以看到8个图像，CLIP模型均能够以较高的置信度给出正确的分类结果：\"]},\"28\":{\"h\":\"文本描述生成\",\"t\":[\"在使用CLIP模型进行zero-shot分类时，除了模型本身的应用，文本描述的生成也是一个关键环节。在之前的例子中，我们使用了“A photo of {label}”这样的格式来生成文本描述，但实际上，我们还有其他的选择。例如，我们可以直接使用类别标签作为文本描述。这种方法实际上与NLP领域的一个研究方向——prompt learning或prompt engineering——紧密相关。关于这一领域的详细综述，可以参考论文《Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing》。\",\"简单来说，prompt learning的核心思想是通过设计合适的prompt（提示），使得预训练模型能够直接应用于下游任务。这与传统的预训练加微调的方法有所不同。论文指出，如果我们直接使用类别标签作为文本描述，由于这些文本往往只是一个单词，缺乏具体的上下文，并且与CLIP模型的训练数据不完全一致，因此在效果上可能不如使用“A photo of {label}”这种格式（在ImageNet数据集上可以提升1.3%的效果）。\",\"此外，论文还实验了使用80个不同的prompt进行集成，结果发现在ImageNet数据集上能够带来3.5%的性能提升。具体的实验结果可以参考CLIP公开的notebook。\"]},\"29\":{\"h\":\"花卉图片分类\",\"t\":[\"本节我们将基于CLIP预训练模型实现Zero-Shot推理，训练使用到的数据集和AlexNet保持一致，因此这里就不再给出数据集下载链接了。\",\"图片分类实战 – 分别基于LeNet，AlexNet，VGG进行实现\",\"# 预训练模型名称 model_name = \\\"openai/clip-vit-large-patch14\\\" # 定义当前目录 current_dir = os.getcwd() model_dir = os.path.join(current_dir, model_name.replace(\\\"/\\\", \\\"_\\\")) # 检查当前目录是否有预训练权重文件，如果没有则下载 def download_pretrained_weights_if_needed(model_name, save_dir): if not os.path.exists(save_dir): try: print(f\\\"Downloading {model_name} to {save_dir}...\\\") snapshot_download(repo_id=model_name, local_dir=save_dir, local_dir_use_symlinks=False) print(f\\\"{model_name} downloaded successfully.\\\") except Exception as e: print(f\\\"Error downloading {model_name}: {e}\\\") download_pretrained_weights_if_needed(model_name, model_dir) # 加载模型和处理器 model = CLIPModel.from_pretrained(model_dir) processor = CLIPProcessor.from_pretrained(model_dir) device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\") model = model.to(device)\",\"在 openai/clip-vit-large-patch14 这个 CLIP 预训练模型中，图像编码器采用了 Vision Transformer（ViT）架构，具体使用的是 ViT-L/14 版本，文本编码器使用的是基于 Transformer 的架构。\",\"# 函数：生成文本嵌入 def text_embedding(texts): inputs = processor(text=texts, return_tensors=\\\"pt\\\", padding=True).to(device) with torch.no_grad(): embeddings = model.get_text_features(**inputs) return embeddings.cpu().numpy()\",\"这个函数的作用是将输入的文本转化为对应的嵌入表示（embedding）。它通过处理器对输入文本进行处理，使其符合模型的输入要求，然后利用模型获取文本特征，最后将结果转换为 numpy 数组格式返回，方便后续的计算和比较。\",\"def get_image_embeddings(image_paths): images = [] for image_path in image_paths: try: image = Image.open(image_path).convert(\\\"RGB\\\") images.append(image) except Exception as e: print(f\\\"Error loading image {e}\\\") if not images: return None inputs = processor(images=images, return_tensors=\\\"pt\\\").to(device) with torch.no_grad(): image_features = model.get_image_features(**inputs) return image_features.cpu().numpy()\",\"该函数作用是针对给定的图片路径，读取图片并将其转换为合适的格式后，通过模型获取图片的特征嵌入。如果在读取图片过程中出现错误，会进行相应的错误提示并返回 None。\",\"def cosine_similarity(vec1, vec2): vec1 = np.array(vec1) vec2 = np.array(vec2) return np.dot(vec1, vec2.T) / (np.linalg.norm(vec1, axis=1, keepdims=True) * np.linalg.norm(vec2, axis=1))\",\"在图文检索中，我们常常需要衡量文本嵌入和图片嵌入之间的相似度，这里采用了余弦相似度的计算方法。它将输入的向量转换为 numpy 数组后，按照余弦相似度的数学公式来计算两者的相似度数值。\",\"首先，我们需要根据上面给出的花卉数据集下载链接，将数据下载到当前项目目录下:\",\"其次，我们从flower_photos目录下读取出所有图片的路径:\",\"# 递归遍历目录获取所有图片路径 def get_all_image_paths(directory): image_paths = [] for root, _, files in os.walk(directory): for file in files: file_extension = os.path.splitext(file)[1].lower() if file_extension in ['.png', '.jpg', '.jpeg']: image_paths.append(os.path.join(root, file)) return image_paths image_paths = get_all_image_paths(\\\"./flower_photos\\\")\",\"同时将flower_photos下的子目录名作为我们的候选待匹配分类文本列表，并改造为a photo of 子目录名的格式，然后计算每个分类文本对应的文本嵌入向量:\",\"# 获取候选分类名列表 def get_candidates(directory): candidates = [] for sub_dir in os.listdir(directory): sub_dir_path = os.path.join(directory, sub_dir) if os.path.isdir(sub_dir_path): candidates.append(f\\\"a photo of {sub_dir}\\\") return candidates candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates)\",\"最后:\",\"分批次从图像列表中取出一批图像，获取其对应的图像嵌入向量列表\",\"计算这批图像嵌入向量中每一个图像嵌入向量和我们所有分类文本嵌入向量的余弦相似度\",\"针对每个图像嵌入向量取出和其相似度最高的那个文本嵌入向量的索引\",\"判断预测是否正确，统计正确率\",\"# 测试图片分类正确率 def accuracy(image_paths, candidates, text_embeddings, batch_size=64): correct_count = 0 total_count = len(image_paths) num_batches = (total_count + batch_size - 1) // batch_size # 分批次预测 for i in range(num_batches): start_idx = i * batch_size end_idx = min(start_idx + batch_size, total_count) # 取出当前批次的图像列表，并获得该批次图像列表对应的图像嵌入向量列表 batch_image_paths = image_paths[start_idx:end_idx] image_embeddings = get_image_embeddings(batch_image_paths) if image_embeddings is not None: # 计算图像嵌入向量列表中每个图像对应的嵌入向量和传入的分类文本嵌入向量的余弦相似度 similarities = cosine_similarity(image_embeddings, text_embeddings) # 针对每个图像嵌入向量获取和其相似度最高的分类文本嵌入向量索引下标 predicted_indices = np.argmax(similarities, axis=1) for j, predicted_index in enumerate(predicted_indices): # 针对每张图像，根据上述计算得到的和其相似度最高的分类文本索引，从候选分类文本集合中取出其分类名词 predicted_category = candidates[predicted_index].split(\\\" \\\")[-1] # 用当前图片外层目录的名字作为其分类名词 actual_category = os.path.basename(os.path.dirname(batch_image_paths[j])) # 比较两个分类名词是否相等 if predicted_category == actual_category: correct_count += 1 accuracy = correct_count / total_count return accuracy # 图片分类 def flowerClassify(): image_paths = get_all_image_paths(\\\"./flower_photos\\\") candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates) start_time = time.time() acc = accuracy(image_paths, candidates, text_embeddings, batch_size=64) end_time = time.time() print(f\\\"Time taken to test accuracy: {end_time - start_time:.2f} seconds\\\") print(f\\\"Accuracy: {acc * 100:.2f}%\\\")\",\"Time taken to test accuracy: 396.62 seconds Accuracy: 95.48%\"]},\"30\":{\"h\":\"文字搜索图像\",\"t\":[\"上面的花卉分类案例相当于使用图像去匹配最合适的文本描述，而这里我们将会反转这个逻辑，用文本描述去匹配最合适的图片内容。\",\"为了实现文字搜索图像的功能，我们只需要在计算出相似度得分矩阵后，以每个文本描述为一行，取出该行中得分最大的那一列，即为与当前文本描述相似度最高的那副图片，具体代码实现如下：\",\"# 找到与文本最匹配的图片 def find_most_matching_image(text, image_paths): text_emb = text_embedding([text]) image_embeddings = get_image_embeddings(image_paths) if image_embeddings is None: return None similarities = cosine_similarity(text_emb, image_embeddings) most_matching_index = np.argmax(similarities) return image_paths[most_matching_index]\",\"下面来实际展示一下效果，首先我们用data目录充当我们的图片库来源:\",\" 遍历data目录，拿到所有图片路径:\",\"# 遍历 data 目录获取所有图片路径 def get_images_from_data_dir(): data_dir = os.path.join(current_dir, 'data') if not os.path.exists(data_dir): print(f\\\"Data directory {data_dir} does not exist.\\\") return [] return get_all_image_paths(data_dir)\",\"这里以搜索向日葵花为例，我们首先获取图片库中所有图片，然后计算出和当前文本描述相似度最高的那副图片，并将图片展示出来:\",\"# 根据文字搜索图片 def searchPicByText(): image_paths = get_images_from_data_dir() query_text = \\\"a photo of a sunflowers\\\" most_matching_image = find_most_matching_image(query_text, image_paths) if most_matching_image: print(f\\\"The most matching image for '{query_text}' is: {most_matching_image}\\\") try: img = Image.open(most_matching_image) plt.imshow(img) plt.axis('off') plt.title(f\\\"Most matching image for '{query_text}'\\\") plt.show() except Exception as e: print(f\\\"Error opening image: {e}\\\") else: print(\\\"No matching image found.\\\")\",\"图片库中的图片： 运行上述代码，搜索出来的图片:\"]},\"31\":{\"h\":\"完整代码\",\"t\":[\"import time from matplotlib import pyplot as plt from transformers import CLIPProcessor, CLIPModel import torch from PIL import Image import numpy as np import warnings import os from huggingface_hub import snapshot_download warnings.filterwarnings(\\\"ignore\\\") # 模型名称 model_name = \\\"openai/clip-vit-large-patch14\\\" # 定义当前目录 current_dir = os.getcwd() model_dir = os.path.join(current_dir, model_name.replace(\\\"/\\\", \\\"_\\\")) # 检查当前目录是否有预训练权重文件，如果没有则下载 def download_pretrained_weights_if_needed(model_name, save_dir): if not os.path.exists(save_dir): try: print(f\\\"Downloading {model_name} to {save_dir}...\\\") snapshot_download(repo_id=model_name, local_dir=save_dir, local_dir_use_symlinks=False) print(f\\\"{model_name} downloaded successfully.\\\") except Exception as e: print(f\\\"Error downloading {model_name}: {e}\\\") download_pretrained_weights_if_needed(model_name, model_dir) # 加载模型和处理器 model = CLIPModel.from_pretrained(model_dir) processor = CLIPProcessor.from_pretrained(model_dir) device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\") model = model.to(device) # 函数：生成文本嵌入 def text_embedding(texts): inputs = processor(text=texts, return_tensors=\\\"pt\\\", padding=True).to(device) with torch.no_grad(): embeddings = model.get_text_features(**inputs) return embeddings.cpu().numpy() def get_image_embeddings(image_paths): images = [] for image_path in image_paths: try: image = Image.open(image_path).convert(\\\"RGB\\\") images.append(image) except Exception as e: print(f\\\"Error loading image {e}\\\") if not images: return None inputs = processor(images=images, return_tensors=\\\"pt\\\").to(device) with torch.no_grad(): image_features = model.get_image_features(**inputs) return image_features.cpu().numpy() def cosine_similarity(vec1, vec2): vec1 = np.array(vec1) vec2 = np.array(vec2) return np.dot(vec1, vec2.T) / (np.linalg.norm(vec1, axis=1, keepdims=True) * np.linalg.norm(vec2, axis=1)) # 递归遍历目录获取所有图片路径 def get_all_image_paths(directory): image_paths = [] for root, _, files in os.walk(directory): for file in files: file_extension = os.path.splitext(file)[1].lower() if file_extension in ['.png', '.jpg', '.jpeg']: image_paths.append(os.path.join(root, file)) return image_paths # 获取候选分类名列表 def get_candidates(directory): candidates = [] for sub_dir in os.listdir(directory): sub_dir_path = os.path.join(directory, sub_dir) if os.path.isdir(sub_dir_path): candidates.append(f\\\"a photo of {sub_dir}\\\") return candidates # 测试图片分类正确率 def accuracy(image_paths, candidates, text_embeddings, batch_size=64): correct_count = 0 total_count = len(image_paths) num_batches = (total_count + batch_size - 1) // batch_size for i in range(num_batches): start_idx = i * batch_size end_idx = min(start_idx + batch_size, total_count) batch_image_paths = image_paths[start_idx:end_idx] image_embeddings = get_image_embeddings(batch_image_paths) if image_embeddings is not None: similarities = cosine_similarity(image_embeddings, text_embeddings) predicted_indices = np.argmax(similarities, axis=1) for j, predicted_index in enumerate(predicted_indices): predicted_category = candidates[predicted_index].split(\\\" \\\")[-1] actual_category = os.path.basename(os.path.dirname(batch_image_paths[j])) if predicted_category == actual_category: correct_count += 1 accuracy = correct_count / total_count return accuracy # 图片分类 def flowerClassify(): image_paths = get_all_image_paths(\\\"./flower_photos\\\") candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates) start_time = time.time() acc = accuracy(image_paths, candidates, text_embeddings, batch_size=64) end_time = time.time() print(f\\\"Time taken to test accuracy: {end_time - start_time:.2f} seconds\\\") print(f\\\"Accuracy: {acc * 100:.2f}%\\\") ##################################################################################################3 # 遍历 data 目录获取所有图片路径 def get_images_from_data_dir(): data_dir = os.path.join(current_dir, 'data') if not os.path.exists(data_dir): print(f\\\"Data directory {data_dir} does not exist.\\\") return [] return get_all_image_paths(data_dir) # 找到与文本最匹配的图片 def find_most_matching_image(text, image_paths): text_emb = text_embedding([text]) image_embeddings = get_image_embeddings(image_paths) if image_embeddings is None: return None similarities = cosine_similarity(text_emb, image_embeddings) most_matching_index = np.argmax(similarities) return image_paths[most_matching_index] # 根据文字搜索图片 def searchPicByText(): image_paths = get_images_from_data_dir() query_text = \\\"a photo of a sunflowers\\\" most_matching_image = find_most_matching_image(query_text, image_paths) if most_matching_image: print(f\\\"The most matching image for '{query_text}' is: {most_matching_image}\\\") try: img = Image.open(most_matching_image) plt.imshow(img) plt.axis('off') plt.title(f\\\"Most matching image for '{query_text}'\\\") plt.show() except Exception as e: print(f\\\"Error opening image: {e}\\\") else: print(\\\"No matching image found.\\\")\"]},\"32\":{\"h\":\"小结\",\"t\":[\"在计算机视觉领域，常见的迁移学习方法是首先在大规模数据集（如ImageNet）上进行预训练，然后在具体的下游任务上进行微调。这种预训练通常是基于有监督学习的，需要大量的数据标注，因此成本较高。近年来，出现了一些基于自监督的方法，包括基于对比学习的方法（如MoCo和SimCLR）和基于图像掩码的方法（如MAE和BeiT）。自监督方法的优势在于不再需要标注数据。然而，无论是有监督还是自监督方法，在迁移到下游任务时，都需要进行有监督微调，无法实现zero-shot学习。对于有监督模型，由于它们在预训练数据集上采用固定类别数的分类器，因此在新的数据集上需要定义新的分类器来重新训练。对于自监督模型，代理任务通常是辅助进行表征学习，在迁移到其他数据集时也需要加上新的分类器进行有监督训练。\",\"然而，在NLP领域，基于自回归或语言掩码的预训练方法已经相对成熟，预训练模型很容易直接zero-shot迁移到下游任务，例如OpenAI的GPT-3。这种差异一方面是由于文本和图像属于两个完全不同的模态，另一个原因是NLP模型可以利用从互联网上收集的大量文本。因此，问题来了：能否基于互联网上的大量文本来预训练视觉模型？\",\"实际上，之前已经有一些研究工作探讨了使用文本作为监督信号来训练视觉模型。例如，2016年的工作《Learning Visual Features from Large Weakly Supervised Data》将这个问题转化为一个多标签分类任务，预测图像对应的文本的词袋模型；2017年的工作《Learning Visual N-Grams from Web Data》进一步扩展了这个方法来预测n-grams。最近的一些工作采用新的模型架构和预训练方法来从文本学习视觉特征，例如VirTex基于transformer的语言模型，ICMLM基于语言掩码的方法，ConVIRT基于对比学习的方法。总体来看，这方面的工作并不多，主要是因为这些方法难以实现较高的性能，例如2017年的那篇工作只在ImageNet上实现了11.5%的zero-shot性能，这远远低于ImageNet上的SOTA。此外，还有另一个方向，即基于文本弱监督来提升性能，例如谷歌的BiT和ViT基于JFT-300M数据集来预训练模型在ImageNet上取得SOTA。JFT-300M数据集是谷歌从互联网上收集的，通过一些自动化的手段将web text转化为18291个类别，但存在一定的噪声。尽管谷歌基于JFT-300M数据集取得了较好的结果，但这些模型仍然采用固定类别的softmax分类器进行预训练，这大大限制了它们的迁移能力和扩展性。\",\"谷歌采用的弱监督方法与以往方法的一个主要区别在于规模，或者说在于计算能力和数据集的规模。JFT-300M数据集的规模达到了上亿级别，谷歌利用强大的计算能力进行了预训练。相比之下，VirTex、ICMLM和ConVIRT仅在10万级别的数据上训练了几天。为了弥补数据规模上的差距，OpenAI从网络上收集了4亿条数据进行实验。然而，新的问题出现了：应该采用什么样的方法来进行训练。\",\"OpenAI首先尝试了VirTex模型，该模型联合训练一个CNN和文本transformer来预测图像的文本描述（image caption），但发现这种方法的训练效率（根据ImageNet数据集上的zero-shot性能评估）还不如直接预测词袋模型（bag of words），两者的训练效率相差3倍。如果进一步采用ConVIRT，即基于对比学习的方法，训练效率可以提高4倍。出现这种差异的原因不难理解，因为训练数据集中的文本-图像对是从互联网收集的，存在一定的噪声，即文本和图像可能不完全匹配。在这种情况下，适当降低训练目标反而可能取得更好的效果。\",\"从任务难度来看，排序为：Transformer Language Model > Bag of Words Prediction > Bag of Words Contrastive (CLIP)。由于训练数据量和模型计算量较大，训练效率成为一个至关重要的因素。因此，作者最终选择了对比学习方法来进行训练。\"]},\"33\":{\"h\":\"开源项目\"},\"34\":{\"h\":\"杂谈\"},\"35\":{\"h\":\"\",\"t\":[\"404 Not Found\"]}},\"dirtCount\":0,\"index\":[[\"杂谈\",{\"0\":{\"34\":1}}],[\"作者最终选择了对比学习方法来进行训练\",{\"1\":{\"32\":1}}],[\"作为图像的分类预测结果\",{\"1\":{\"27\":1}}],[\"排序为\",{\"1\":{\"32\":1}}],[\"适当降低训练目标反而可能取得更好的效果\",{\"1\":{\"32\":1}}],[\"存在一定的噪声\",{\"1\":{\"32\":1}}],[\"出现这种差异的原因不难理解\",{\"1\":{\"32\":1}}],[\"出现了一些基于自监督的方法\",{\"1\":{\"32\":1}}],[\"两者的训练效率相差3倍\",{\"1\":{\"32\":1}}],[\"应该采用什么样的方法来进行训练\",{\"1\":{\"32\":1}}],[\"新的问题出现了\",{\"1\":{\"32\":1}}],[\"相比之下\",{\"1\":{\"32\":1}}],[\"相似度如下所示\",{\"1\":{\"27\":1}}],[\"谷歌利用强大的计算能力进行了预训练\",{\"1\":{\"32\":1}}],[\"谷歌采用的弱监督方法与以往方法的一个主要区别在于规模\",{\"1\":{\"32\":1}}],[\"或者说在于计算能力和数据集的规模\",{\"1\":{\"32\":1}}],[\"尽管谷歌基于jft\",{\"1\":{\"32\":1}}],[\"尽管clip是一个多模态模型\",{\"1\":{\"26\":1}}],[\"还不如直接预测词袋模型\",{\"1\":{\"32\":1}}],[\"还有另一个方向\",{\"1\":{\"32\":1}}],[\"还在336的分辨率下额外进行了一个周期的微调\",{\"1\":{\"26\":1}}],[\"主要是因为这些方法难以实现较高的性能\",{\"1\":{\"32\":1}}],[\"主页\",{\"0\":{\"0\":1}}],[\"总体来看\",{\"1\":{\"32\":1}}],[\"进一步扩展了这个方法来预测n\",{\"1\":{\"32\":1}}],[\"进一步地\",{\"1\":{\"27\":1}}],[\"预测图像对应的文本的词袋模型\",{\"1\":{\"32\":1}}],[\"预训练模型很容易直接zero\",{\"1\":{\"32\":1}}],[\"预训练模型中\",{\"1\":{\"29\":1}}],[\"预训练模型名称\",{\"1\":{\"29\":1}}],[\"之前已经有一些研究工作探讨了使用文本作为监督信号来训练视觉模型\",{\"1\":{\"32\":1}}],[\"之后\",{\"1\":{\"24\":1}}],[\"实际上\",{\"1\":{\"32\":1}}],[\"实现zero\",{\"1\":{\"27\":1}}],[\"能否基于互联网上的大量文本来预训练视觉模型\",{\"1\":{\"32\":1}}],[\"问题来了\",{\"1\":{\"32\":1}}],[\"另一个原因是nlp模型可以利用从互联网上收集的大量文本\",{\"1\":{\"32\":1}}],[\"另一种是基于\",{\"1\":{\"26\":1}}],[\"基于自回归或语言掩码的预训练方法已经相对成熟\",{\"1\":{\"32\":1}}],[\"代理任务通常是辅助进行表征学习\",{\"1\":{\"32\":1}}],[\"无法实现zero\",{\"1\":{\"32\":1}}],[\"无论是有监督还是自监督方法\",{\"1\":{\"32\":1}}],[\"都需要进行有监督微调\",{\"1\":{\"32\":1}}],[\"都使用了残差连接和层归一化\",{\"1\":{\"11\":1}}],[\"包括基于对比学习的方法\",{\"1\":{\"32\":1}}],[\"近年来\",{\"1\":{\"32\":1}}],[\"需要大量的数据标注\",{\"1\":{\"32\":1}}],[\"需要将\",{\"1\":{\"9\":1}}],[\"上进行预训练\",{\"1\":{\"32\":1}}],[\"上面的花卉分类案例相当于使用图像去匹配最合适的文本描述\",{\"1\":{\"30\":1}}],[\"常见的迁移学习方法是首先在大规模数据集\",{\"1\":{\"32\":1}}],[\"小结\",{\"0\":{\"32\":1}}],[\"小红书\",{\"1\":{\"0\":1}}],[\"完整代码\",{\"0\":{\"31\":1}}],[\"搜索出来的图片\",{\"1\":{\"30\":1}}],[\"运行上述代码\",{\"1\":{\"30\":1}}],[\"目录获取所有图片路径\",{\"1\":{\"30\":1,\"31\":1}}],[\"目标掩码\",{\"1\":{\"20\":1}}],[\"遍历\",{\"1\":{\"30\":1,\"31\":1}}],[\"遍历data目录\",{\"1\":{\"30\":1}}],[\"拿到所有图片路径\",{\"1\":{\"30\":1}}],[\"下面来实际展示一下效果\",{\"1\":{\"30\":1}}],[\"找到与文本最匹配的图片\",{\"1\":{\"30\":1,\"31\":1}}],[\"取出该行中得分最大的那一列\",{\"1\":{\"30\":1}}],[\"取出当前批次的图像列表\",{\"1\":{\"29\":1}}],[\"62\",{\"1\":{\"29\":1}}],[\"比较两个分类名词是否相等\",{\"1\":{\"29\":1}}],[\"用文本描述去匹配最合适的图片内容\",{\"1\":{\"30\":1}}],[\"用当前图片外层目录的名字作为其分类名词\",{\"1\":{\"29\":1}}],[\"用户本地运行时\",{\"1\":{\"9\":1}}],[\"针对每张图像\",{\"1\":{\"29\":1}}],[\"针对每个图像嵌入向量获取和其相似度最高的分类文本嵌入向量索引下标\",{\"1\":{\"29\":1}}],[\"针对每个图像嵌入向量取出和其相似度最高的那个文本嵌入向量的索引\",{\"1\":{\"29\":1}}],[\"测试图片分类正确率\",{\"1\":{\"29\":1,\"31\":1}}],[\"统计正确率\",{\"1\":{\"29\":1}}],[\"判断预测是否正确\",{\"1\":{\"29\":1}}],[\"获取其对应的图像嵌入向量列表\",{\"1\":{\"29\":1}}],[\"获取候选分类名列表\",{\"1\":{\"29\":1,\"31\":1}}],[\"子目录名的格式\",{\"1\":{\"29\":1}}],[\"递归遍历目录获取所有图片路径\",{\"1\":{\"29\":1,\"31\":1}}],[\"按照余弦相似度的数学公式来计算两者的相似度数值\",{\"1\":{\"29\":1}}],[\"按照最大相似度\",{\"1\":{\"27\":1}}],[\"数组后\",{\"1\":{\"29\":1}}],[\"数组格式返回\",{\"1\":{\"29\":1}}],[\"会进行相应的错误提示并返回\",{\"1\":{\"29\":1}}],[\"读取图片并将其转换为合适的格式后\",{\"1\":{\"29\":1}}],[\"该模型联合训练一个cnn和文本transformer来预测图像的文本描述\",{\"1\":{\"32\":1}}],[\"该函数作用是针对给定的图片路径\",{\"1\":{\"29\":1}}],[\"该示例中的任务涉及8个类别\",{\"1\":{\"27\":1}}],[\"方便后续的计算和比较\",{\"1\":{\"29\":1}}],[\"方向研究\",{\"1\":{\"2\":1}}],[\"生成文本嵌入\",{\"1\":{\"29\":1,\"31\":1}}],[\"生成的文本特征相当于分类器的权重\",{\"1\":{\"27\":1}}],[\"函数\",{\"1\":{\"29\":1,\"31\":1}}],[\"函数生成目标序列的概率分布\",{\"1\":{\"11\":1}}],[\"版本\",{\"1\":{\"29\":1}}],[\"具体代码实现如下\",{\"1\":{\"30\":1}}],[\"具体使用的是\",{\"1\":{\"29\":1}}],[\"具体的实验结果可以参考clip公开的notebook\",{\"1\":{\"28\":1}}],[\"架构\",{\"1\":{\"29\":1}}],[\"加载模型和处理器\",{\"1\":{\"29\":1,\"31\":1}}],[\"检查当前目录是否有预训练权重文件\",{\"1\":{\"29\":1,\"31\":1}}],[\"jft\",{\"1\":{\"32\":2}}],[\"j\",{\"1\":{\"29\":2,\"31\":2}}],[\"jpeg\",{\"1\":{\"29\":1,\"31\":1}}],[\"jpg\",{\"1\":{\"29\":1,\"31\":1}}],[\"join\",{\"1\":{\"29\":3,\"30\":1,\"31\":4}}],[\"javaer\",{\"1\":{\"2\":1}}],[\"定义当前目录\",{\"1\":{\"29\":1,\"31\":1}}],[\"本节我们将基于clip预训练模型实现zero\",{\"1\":{\"29\":1}}],[\"本文的transformer使用了self\",{\"1\":{\"10\":1}}],[\"本文基于\",{\"1\":{\"9\":1}}],[\"花卉图片分类\",{\"0\":{\"29\":1}}],[\"结果发现在imagenet数据集上能够带来3\",{\"1\":{\"28\":1}}],[\"结构\",{\"0\":{\"12\":1,\"14\":1,\"18\":1},\"1\":{\"11\":1}}],[\"缺乏具体的上下文\",{\"1\":{\"28\":1}}],[\"使其符合模型的输入要求\",{\"1\":{\"29\":1}}],[\"使得预训练模型能够直接应用于下游任务\",{\"1\":{\"28\":1}}],[\"使用adamw优化器\",{\"1\":{\"26\":1}}],[\"使用掩码机制确保模型在生成目标序列时只能看到当前及之前的词\",{\"1\":{\"11\":1}}],[\"提示\",{\"1\":{\"28\":1}}],[\"提取图像特征\",{\"1\":{\"27\":1}}],[\"提取文本特征\",{\"1\":{\"27\":1}}],[\"提取的图像特征则是分类器的输入数据\",{\"1\":{\"27\":1}}],[\"简单来说\",{\"1\":{\"28\":1}}],[\"关于这一领域的详细综述\",{\"1\":{\"28\":1}}],[\"关于我们\",{\"0\":{\"1\":1}}],[\"紧密相关\",{\"1\":{\"28\":1}}],[\"除了模型本身的应用\",{\"1\":{\"28\":1}}],[\"5\",{\"1\":{\"27\":1,\"28\":1,\"32\":1}}],[\"得到的预测概率如下所示\",{\"1\":{\"27\":1}}],[\"得到每个预测类别的概率值\",{\"1\":{\"27\":1}}],[\"得到相同维度的特征\",{\"1\":{\"26\":1}}],[\"计算图像嵌入向量列表中每个图像对应的嵌入向量和传入的分类文本嵌入向量的余弦相似度\",{\"1\":{\"29\":1}}],[\"计算这批图像嵌入向量中每一个图像嵌入向量和我们所有分类文本嵌入向量的余弦相似度\",{\"1\":{\"29\":1}}],[\"计算余弦相似度\",{\"1\":{\"27\":1}}],[\"计算缩放的余弦相似度\",{\"1\":{\"26\":1}}],[\"我们首先获取图片库中所有图片\",{\"1\":{\"30\":1}}],[\"我们首先创建了各类别的文本描述\",{\"1\":{\"27\":1}}],[\"我们只需要在计算出相似度得分矩阵后\",{\"1\":{\"30\":1}}],[\"我们从flower\",{\"1\":{\"29\":1}}],[\"我们需要根据上面给出的花卉数据集下载链接\",{\"1\":{\"29\":1}}],[\"我们常常需要衡量文本嵌入和图片嵌入之间的相似度\",{\"1\":{\"29\":1}}],[\"我们可以直接使用类别标签作为文本描述\",{\"1\":{\"28\":1}}],[\"我们还有其他的选择\",{\"1\":{\"28\":1}}],[\"我们使用了\",{\"1\":{\"28\":1}}],[\"我们也可以对得到的余弦相似度计算softmax\",{\"1\":{\"27\":1}}],[\"我们通过利用clip模型的多模态能力\",{\"1\":{\"27\":1}}],[\"我们已经探讨了clip模型的运作机制\",{\"1\":{\"27\":1}}],[\"显然\",{\"1\":{\"27\":1}}],[\"可以参考论文\",{\"1\":{\"28\":1}}],[\"可以看到8个图像\",{\"1\":{\"27\":1}}],[\"可以看到对于要预测的8个图像\",{\"1\":{\"27\":1}}],[\"可以得到每个类别的预测概率\",{\"1\":{\"27\":1}}],[\"可以概括为以下两个主要步骤\",{\"1\":{\"27\":1}}],[\"通过一些自动化的手段将web\",{\"1\":{\"32\":1}}],[\"通过模型获取图片的特征嵌入\",{\"1\":{\"29\":1}}],[\"通过这种方式\",{\"1\":{\"27\":1}}],[\"通过softmax函数转换后\",{\"1\":{\"27\":1}}],[\"通过填充掩码\",{\"1\":{\"11\":1}}],[\"此外\",{\"1\":{\"27\":1,\"28\":1,\"32\":1}}],[\"选择与图像特征相似度最高的文本所对应的类别\",{\"1\":{\"27\":1}}],[\"选择了一个包含6300万参数的transformer模型\",{\"1\":{\"26\":1}}],[\"最近的一些工作采用新的模型架构和预训练方法来从文本学习视觉特征\",{\"1\":{\"32\":1}}],[\"最后\",{\"1\":{\"29\":1}}],[\"最后将结果转换为\",{\"1\":{\"29\":1}}],[\"最终\",{\"1\":{\"27\":1}}],[\"最大的resnet模型rn50x64需要在592个v100\",{\"1\":{\"26\":1}}],[\"然后在具体的下游任务上进行微调\",{\"1\":{\"32\":1}}],[\"然后计算出和当前文本描述相似度最高的那副图片\",{\"1\":{\"30\":1}}],[\"然后计算每个分类文本对应的文本嵌入向量\",{\"1\":{\"29\":1}}],[\"然后利用模型获取文本特征\",{\"1\":{\"29\":1}}],[\"然后我们读取要预测的图像\",{\"1\":{\"27\":1}}],[\"然后提取了相应的文本特征\",{\"1\":{\"27\":1}}],[\"然后\",{\"1\":{\"27\":1}}],[\"然而\",{\"1\":{\"26\":1,\"32\":3}}],[\"接下来\",{\"1\":{\"27\":1}}],[\"中\",{\"1\":{\"27\":2}}],[\"例如谷歌的bit和vit基于jft\",{\"1\":{\"32\":1}}],[\"例如2017年的那篇工作只在imagenet上实现了11\",{\"1\":{\"32\":1}}],[\"例如virtex基于transformer的语言模型\",{\"1\":{\"32\":1}}],[\"例如openai的gpt\",{\"1\":{\"32\":1}}],[\"例如\",{\"1\":{\"27\":1,\"28\":1,\"32\":1}}],[\"为特定任务动态构建了一个分类器\",{\"1\":{\"27\":1}}],[\"为每个类别创建一个描述性的文本\",{\"1\":{\"27\":1}}],[\"为了弥补数据规模上的差距\",{\"1\":{\"32\":1}}],[\"为了实现文字搜索图像的功能\",{\"1\":{\"30\":1}}],[\"为了训练clip模型\",{\"1\":{\"26\":1}}],[\"为了简单\",{\"1\":{\"15\":1}}],[\"首先我们用data目录充当我们的图片库来源\",{\"1\":{\"30\":1}}],[\"首先\",{\"1\":{\"27\":1,\"29\":1}}],[\"构建描述文本并提取特征\",{\"1\":{\"27\":1}}],[\"直接对图像进行分类\",{\"1\":{\"27\":2}}],[\"那么就会得到个文本特征\",{\"1\":{\"27\":1}}],[\"那么\",{\"1\":{\"27\":1}}],[\"那么clip的训练目标就是最大个正样本的相似度\",{\"1\":{\"26\":1}}],[\"推理\",{\"0\":{\"27\":1}}],[\"论文还实验了使用80个不同的prompt进行集成\",{\"1\":{\"28\":1}}],[\"论文指出\",{\"1\":{\"28\":1}}],[\"论文中进行对比实验的clip模型也采用了这一配置\",{\"1\":{\"26\":1}}],[\"论文发现这个模型的效果最佳\",{\"1\":{\"26\":1}}],[\"以每个文本描述为一行\",{\"1\":{\"30\":1}}],[\"以下是一个官方给出的clip模型的示例\",{\"1\":{\"27\":1}}],[\"以获取图像特征\",{\"1\":{\"27\":1}}],[\"以生成相应的文本特征\",{\"1\":{\"27\":1}}],[\"以增强性能\",{\"1\":{\"26\":1}}],[\"以加速训练并提高模型的稳定性\",{\"1\":{\"11\":1}}],[\"并将图片展示出来\",{\"1\":{\"30\":1}}],[\"并将其标记为vit\",{\"1\":{\"26\":1}}],[\"并获得该批次图像列表对应的图像嵌入向量列表\",{\"1\":{\"29\":1}}],[\"并改造为a\",{\"1\":{\"29\":1}}],[\"并且与clip模型的训练数据不完全一致\",{\"1\":{\"28\":1}}],[\"并且在训练过程中采用了一个相对较大的批次大小\",{\"1\":{\"26\":1}}],[\"并计算与文本特征的余弦相似度\",{\"1\":{\"27\":1}}],[\"并进行l2归一化\",{\"1\":{\"26\":1}}],[\"所有模型均训练了32个周期\",{\"1\":{\"26\":1}}],[\"所提供的代码展开进行讲解\",{\"1\":{\"9\":1}}],[\"后三种模型是按照efficientnet的缩放规则对resnet分别放大4倍\",{\"1\":{\"26\":1}}],[\"一个视觉模型和一个文本模型\",{\"1\":{\"27\":1}}],[\"一种是常用的cnn架构resnet\",{\"1\":{\"26\":1}}],[\"一名深度学习小白\",{\"1\":{\"3\":1}}],[\"一名普通但十分热爱探索技术的coder\",{\"1\":{\"2\":1}}],[\"则采用了两种不同的架构\",{\"1\":{\"26\":1}}],[\"则是以文本作为监督信号\",{\"1\":{\"24\":1}}],[\"但发现这种方法的训练效率\",{\"1\":{\"32\":1}}],[\"但这些模型仍然采用固定类别的softmax分类器进行预训练\",{\"1\":{\"32\":1}}],[\"但存在一定的噪声\",{\"1\":{\"32\":1}}],[\"但实际上\",{\"1\":{\"28\":1}}],[\"但其主要目的是训练可迁移的视觉模型\",{\"1\":{\"26\":1}}],[\"但是和rnn相比\",{\"1\":{\"10\":1}}],[\"从任务难度来看\",{\"1\":{\"32\":1}}],[\"从候选分类文本集合中取出其分类名词\",{\"1\":{\"29\":1}}],[\"从数据对的数量来看\",{\"1\":{\"26\":1}}],[\"从而增强模型的表达能力\",{\"1\":{\"11\":1}}],[\"从而可以解决长距离依赖的问题\",{\"1\":{\"10\":1}}],[\"从而很难并行\",{\"1\":{\"10\":1}}],[\"若以文本单词数量来衡量\",{\"1\":{\"26\":1}}],[\"等价于n个类别的cross\",{\"1\":{\"26\":1}}],[\"对于自监督模型\",{\"1\":{\"32\":1}}],[\"对于有监督模型\",{\"1\":{\"32\":1}}],[\"对于vit\",{\"1\":{\"26\":1}}],[\"对于一个包含个文本\",{\"1\":{\"26\":1}}],[\"对角线元素的labels\",{\"1\":{\"26\":1}}],[\"对称的对比学习损失\",{\"1\":{\"26\":1}}],[\"对两个特征进行线性投射\",{\"1\":{\"26\":1}}],[\"对应的伪代码实现如下所示\",{\"1\":{\"26\":1}}],[\"矩阵中的对角线元素\",{\"1\":{\"26\":1}}],[\"即文本和图像可能不完全匹配\",{\"1\":{\"32\":1}}],[\"即基于对比学习的方法\",{\"1\":{\"32\":1}}],[\"即基于文本弱监督来提升性能\",{\"1\":{\"32\":1}}],[\"即为与当前文本描述相似度最高的那副图片\",{\"1\":{\"30\":1}}],[\"即真正属于一对的文本和图像\",{\"1\":{\"26\":1}}],[\"即上图所示的矩阵\",{\"1\":{\"26\":1}}],[\"将这个问题转化为一个多标签分类任务\",{\"1\":{\"32\":1}}],[\"将数据下载到当前项目目录下\",{\"1\":{\"29\":1}}],[\"将待分类的图像输入到图像编码器\",{\"1\":{\"27\":1}}],[\"将个文本特征和个图像特征两两组合\",{\"1\":{\"26\":1}}],[\"将输入序列\",{\"1\":{\"11\":1}}],[\"也是其一大亮点\",{\"1\":{\"27\":1}}],[\"也可以采用视觉transformer模型\",{\"1\":{\"26\":1}}],[\"也就是一张图像搭配与之对应的文本描述\",{\"1\":{\"25\":1}}],[\"模型名称\",{\"1\":{\"31\":1}}],[\"模型\",{\"1\":{\"26\":1}}],[\"模型架构图\",{\"1\":{\"11\":1}}],[\"模型架构\",{\"0\":{\"11\":1}}],[\"分批次预测\",{\"1\":{\"29\":1}}],[\"分批次从图像列表中取出一批图像\",{\"1\":{\"29\":1}}],[\"分别基于lenet\",{\"1\":{\"29\":1}}],[\"分别提取图像特征和文本特征\",{\"1\":{\"26\":1}}],[\"分别是文本编码器\",{\"1\":{\"26\":1}}],[\"分词处理\",{\"0\":{\"7\":1}}],[\"训练效率成为一个至关重要的因素\",{\"1\":{\"32\":1}}],[\"训练效率可以提高4倍\",{\"1\":{\"32\":1}}],[\"训练使用到的数据集和alexnet保持一致\",{\"1\":{\"29\":1}}],[\"训练\",{\"0\":{\"26\":1}}],[\"训练出具有可迁移能力的视觉模型\",{\"1\":{\"24\":1}}],[\"期望模型能够学习到文本和图像之间的匹配关系\",{\"1\":{\"25\":1}}],[\"借助对比学习机制\",{\"1\":{\"25\":1}}],[\"像moco和simclr有所不同\",{\"1\":{\"25\":1}}],[\"与计算机视觉\",{\"1\":{\"25\":1}}],[\"与此同时\",{\"1\":{\"24\":1}}],[\"图片库中的图片\",{\"1\":{\"30\":1}}],[\"图片分类\",{\"1\":{\"29\":1,\"31\":1}}],[\"图片分类实战\",{\"1\":{\"29\":1}}],[\"图像编码器采用了\",{\"1\":{\"29\":1}}],[\"图像特征提取与分类\",{\"1\":{\"27\":1}}],[\"图像对是从互联网收集的\",{\"1\":{\"32\":1}}],[\"图像对为负样本\",{\"1\":{\"26\":1}}],[\"图像对的相似度\",{\"1\":{\"26\":1}}],[\"图像对的训练batch\",{\"1\":{\"26\":1}}],[\"图像对的预训练方法\",{\"1\":{\"25\":1}}],[\"图像对\",{\"1\":{\"25\":1}}],[\"图解transformer\",{\"0\":{\"8\":1},\"1\":{\"8\":1}}],[\"图解bert\",{\"1\":{\"6\":1}}],[\"图解\",{\"0\":{\"6\":1}}],[\"介绍\",{\"0\":{\"25\":1}}],[\"类似\",{\"1\":{\"24\":1}}],[\"而这里我们将会反转这个逻辑\",{\"1\":{\"30\":1}}],[\"而最大的vit模型vit\",{\"1\":{\"26\":1}}],[\"而vit则选择了三种不同尺寸的模型\",{\"1\":{\"26\":1}}],[\"而图像编码器\",{\"1\":{\"26\":1,\"27\":1}}],[\"而图像编码器则用于提取图像的特征\",{\"1\":{\"26\":1}}],[\"而剩余的个文本\",{\"1\":{\"26\":1}}],[\"而\",{\"1\":{\"24\":1}}],[\"而不能看到未来的词\",{\"1\":{\"11\":1}}],[\"是基于文本输入来生成图像的模型\",{\"1\":{\"24\":1}}],[\"是一种基于自注意力机制\",{\"1\":{\"11\":1}}],[\"其次\",{\"1\":{\"29\":1}}],[\"其均能匹配到正确的文本标签\",{\"1\":{\"27\":1}}],[\"其规模与gpt\",{\"1\":{\"26\":1}}],[\"其中\",{\"1\":{\"24\":1,\"26\":1}}],[\"其由七大主要部分构成\",{\"1\":{\"11\":1}}],[\"同时将flower\",{\"1\":{\"29\":1}}],[\"同时最小化个负样本的相似度\",{\"1\":{\"26\":1}}],[\"同时也指运用该方法构建的模型\",{\"1\":{\"25\":1}}],[\"同时计算self\",{\"1\":{\"10\":1}}],[\"同样给计算机视觉领域带来了巨大影响\",{\"1\":{\"24\":1}}],[\"月发布的\",{\"1\":{\"24\":1}}],[\"年\",{\"1\":{\"24\":1}}],[\"年可谓是视觉\",{\"1\":{\"24\":1}}],[\"广泛应用于各类计算机视觉任务\",{\"1\":{\"24\":1}}],[\"广播\",{\"1\":{\"21\":1}}],[\"众多基于视觉\",{\"1\":{\"24\":1}}],[\"自监督方法的优势在于不再需要标注数据\",{\"1\":{\"32\":1}}],[\"自谷歌提出\",{\"1\":{\"24\":1}}],[\"自注意力子层\",{\"1\":{\"16\":2,\"19\":2}}],[\"自注意力机制是\",{\"1\":{\"11\":1}}],[\"大放异彩的一年\",{\"1\":{\"24\":1}}],[\"大语言模型\",{\"0\":{\"5\":1}}],[\"引言\",{\"0\":{\"24\":1}}],[\"文字搜索图像\",{\"0\":{\"30\":1}}],[\"文字搜索图像实战演练\",{\"1\":{\"23\":1}}],[\"文本描述的生成也是一个关键环节\",{\"1\":{\"28\":1}}],[\"文本描述生成\",{\"0\":{\"28\":1}}],[\"文本编码器使用的是基于\",{\"1\":{\"29\":1}}],[\"文本编码器\",{\"1\":{\"26\":1,\"27\":1}}],[\"文本编码器的作用是提取文本的特征\",{\"1\":{\"26\":1}}],[\"文件中的\",{\"1\":{\"9\":1}}],[\"庖丁解牛clip\",{\"0\":{\"23\":1}}],[\"多模态模型clip原理与图片分类\",{\"1\":{\"23\":1}}],[\"多模态\",{\"0\":{\"22\":1}}],[\"多头自注意力计算流程图\",{\"1\":{\"21\":1}}],[\"多头自注意力\",{\"0\":{\"21\":1}}],[\"多头自注意力机制通过并行计算多个注意力头\",{\"1\":{\"11\":1}}],[\"多头自注意力机制\",{\"1\":{\"11\":1}}],[\"8\",{\"1\":{\"21\":1}}],[\"8个头\",{\"1\":{\"21\":1}}],[\">\",{\"1\":{\"21\":1,\"32\":2}}],[\"zip\",{\"1\":{\"21\":1}}],[\"query\",{\"1\":{\"21\":9,\"30\":4,\"31\":4}}],[\"q\",{\"1\":{\"21\":1}}],[\"404\",{\"1\":{\"35\":1}}],[\"48\",{\"1\":{\"29\":1}}],[\"4\",{\"1\":{\"21\":1}}],[\"keepdims=true\",{\"1\":{\"29\":1,\"31\":1}}],[\"keepdim=true\",{\"1\":{\"27\":2}}],[\"key\",{\"1\":{\"21\":7}}],[\"k\",{\"1\":{\"21\":9}}],[\"源掩码\",{\"1\":{\"20\":1}}],[\"源注意力子层\",{\"1\":{\"19\":2}}],[\"输入image\",{\"1\":{\"27\":1}}],[\"输入\",{\"1\":{\"20\":1}}],[\"输出层\",{\"1\":{\"11\":1}}],[\"层归一化\",{\"1\":{\"20\":1}}],[\"300m数据集的规模达到了上亿级别\",{\"1\":{\"32\":1}}],[\"300m数据集取得了较好的结果\",{\"1\":{\"32\":1}}],[\"300m数据集是谷歌从互联网上收集的\",{\"1\":{\"32\":1}}],[\"300m数据集来预训练模型在imagenet上取得sota\",{\"1\":{\"32\":1}}],[\"300m数据集还要多出1亿对\",{\"1\":{\"26\":1}}],[\"396\",{\"1\":{\"29\":1}}],[\"336\",{\"1\":{\"26\":1}}],[\"32768\",{\"1\":{\"26\":1}}],[\"32\",{\"1\":{\"26\":1}}],[\"3\",{\"1\":{\"19\":1,\"21\":1,\"27\":1,\"28\":1,\"31\":1,\"32\":1}}],[\"3d\",{\"0\":{\"4\":1},\"1\":{\"2\":1}}],[\"2017年的工作\",{\"1\":{\"32\":1}}],[\"2016年的工作\",{\"1\":{\"32\":1}}],[\"2021\",{\"1\":{\"24\":2}}],[\"2f\",{\"1\":{\"29\":2,\"31\":2}}],[\"2训练时使用的webtext数据集相似\",{\"1\":{\"26\":1}}],[\"2\",{\"1\":{\"16\":1,\"19\":1,\"21\":5,\"26\":1,\"27\":1}}],[\"22\",{\"1\":{\"9\":1}}],[\"各需要一个\",{\"1\":{\"16\":1,\"19\":1}}],[\"和基于图像掩码的方法\",{\"1\":{\"32\":1}}],[\"和图像编码器\",{\"1\":{\"26\":1}}],[\"和\",{\"1\":{\"16\":1,\"24\":3}}],[\"前馈层\",{\"1\":{\"16\":2,\"19\":2}}],[\"前馈神经网络\",{\"1\":{\"11\":1}}],[\"参考decoderlayer\",{\"1\":{\"15\":1}}],[\"原始论文layernorm在最后\",{\"1\":{\"15\":1}}],[\"把layernorm放到了前面\",{\"1\":{\"15\":1}}],[\"残差连接\",{\"1\":{\"15\":1}}],[\"残差连接与层归一化\",{\"1\":{\"11\":1}}],[\"全连接再加上一个softmax\",{\"1\":{\"13\":1}}],[\"data\",{\"1\":{\"30\":9,\"31\":9,\"32\":2}}],[\"dall\",{\"1\":{\"24\":3}}],[\"dirname\",{\"1\":{\"29\":1,\"31\":1}}],[\"directory\",{\"1\":{\"29\":5,\"30\":1,\"31\":6}}],[\"dir=save\",{\"1\":{\"29\":1,\"31\":1}}],[\"dir\",{\"1\":{\"29\":16,\"30\":7,\"31\":23}}],[\"dim=\",{\"1\":{\"13\":1,\"21\":1,\"27\":4}}],[\"does\",{\"1\":{\"30\":1,\"31\":1}}],[\"downloaded\",{\"1\":{\"29\":1,\"31\":1}}],[\"downloading\",{\"1\":{\"29\":2,\"31\":2}}],[\"download\",{\"1\":{\"29\":3,\"31\":4}}],[\"dot\",{\"1\":{\"21\":1,\"26\":3,\"29\":1,\"31\":1}}],[\"do\",{\"1\":{\"21\":1}}],[\"dropout=none\",{\"1\":{\"21\":1}}],[\"dropout=self\",{\"1\":{\"21\":1}}],[\"dropout=0\",{\"1\":{\"21\":1}}],[\"dropout\",{\"1\":{\"15\":6,\"16\":2,\"19\":2,\"21\":5}}],[\"d\",{\"1\":{\"13\":3,\"21\":15,\"26\":6}}],[\"device\",{\"1\":{\"29\":5,\"31\":5}}],[\"desc\",{\"1\":{\"27\":2}}],[\"del\",{\"1\":{\"21\":3}}],[\"dense\",{\"1\":{\"15\":1}}],[\"decode\",{\"1\":{\"12\":2}}],[\"decoder模型结构图\",{\"1\":{\"19\":1,\"20\":1}}],[\"decoderlayer\",{\"0\":{\"19\":1},\"1\":{\"19\":2}}],[\"decoder\",{\"0\":{\"12\":1,\"18\":1,\"20\":1},\"1\":{\"11\":2,\"12\":4,\"19\":1,\"20\":3}}],[\"defined\",{\"1\":{\"19\":1}}],[\"default\",{\"1\":{\"7\":1}}],[\"def\",{\"1\":{\"7\":3,\"12\":4,\"13\":2,\"15\":2,\"16\":2,\"17\":2,\"19\":2,\"20\":2,\"21\":3,\"29\":8,\"30\":3,\"31\":11}}],[\"根据imagenet数据集上的zero\",{\"1\":{\"32\":1}}],[\"根据文字搜索图片\",{\"1\":{\"30\":1,\"31\":1}}],[\"根据上述计算得到的和其相似度最高的分类文本索引\",{\"1\":{\"29\":1}}],[\"根据任务的分类需求\",{\"1\":{\"27\":1}}],[\"根据decoder的隐状态输出一个词\",{\"1\":{\"13\":1}}],[\"根据编码器的输出生成目标序列\",{\"1\":{\"11\":1}}],[\"屏蔽填充部分的信息\",{\"1\":{\"11\":1}}],[\"在这种情况下\",{\"1\":{\"32\":1}}],[\"在这个过程中\",{\"1\":{\"27\":1}}],[\"在nlp领域\",{\"1\":{\"32\":1}}],[\"在迁移到其他数据集时也需要加上新的分类器进行有监督训练\",{\"1\":{\"32\":1}}],[\"在迁移到下游任务时\",{\"1\":{\"32\":1}}],[\"在计算机视觉领域\",{\"1\":{\"32\":1}}],[\"在图文检索中\",{\"1\":{\"29\":1}}],[\"在imagenet数据集上可以提升1\",{\"1\":{\"28\":1}}],[\"在之前的例子中\",{\"1\":{\"28\":1}}],[\"在使用clip模型进行zero\",{\"1\":{\"28\":1}}],[\"在论文中\",{\"1\":{\"26\":1}}],[\"在实际应用中可以选用常见的卷积神经网络\",{\"1\":{\"26\":1}}],[\"在实现时可采用自然语言处理\",{\"1\":{\"26\":1}}],[\"在训练过程中\",{\"1\":{\"25\":1}}],[\"在\",{\"1\":{\"24\":1,\"29\":1}}],[\"在输入序列长度不一致时\",{\"1\":{\"11\":1}}],[\"在解码器中\",{\"1\":{\"11\":1}}],[\"在自注意力机制之后\",{\"1\":{\"11\":1}}],[\"掩码机制\",{\"1\":{\"11\":1}}],[\"每个头64维\",{\"1\":{\"21\":1}}],[\"每个子层\",{\"1\":{\"11\":1}}],[\"每个位置的输出会通过一个独立的前馈神经网络进行进一步处理\",{\"1\":{\"11\":1}}],[\"不使用传统的循环或卷积结构\",{\"1\":{\"11\":1}}],[\"由于训练数据量和模型计算量较大\",{\"1\":{\"32\":1}}],[\"由于它们在预训练数据集上采用固定类别数的分类器\",{\"1\":{\"32\":1}}],[\"由于这些文本往往只是一个单词\",{\"1\":{\"28\":1}}],[\"由于数据量巨大\",{\"1\":{\"26\":1}}],[\"由于\",{\"1\":{\"11\":1}}],[\"位置编码通常使用正弦和余弦函数生成\",{\"1\":{\"11\":1}}],[\"位置编码\",{\"1\":{\"11\":1}}],[\"捕捉不同子空间的信息\",{\"1\":{\"11\":1}}],[\"的zero\",{\"1\":{\"32\":1}}],[\"的架构\",{\"1\":{\"29\":1}}],[\"的性能提升\",{\"1\":{\"28\":1}}],[\"的效果\",{\"1\":{\"28\":1}}],[\"的vit\",{\"1\":{\"26\":1}}],[\"的出现也掀起了新一轮的研究热潮\",{\"1\":{\"24\":1}}],[\"的研究如潮水般涌来\",{\"1\":{\"24\":1}}],[\"的核心\",{\"1\":{\"11\":1}}],[\"的神经网络架构\",{\"1\":{\"11\":1}}],[\"如mae和beit\",{\"1\":{\"32\":1}}],[\"如moco和simclr\",{\"1\":{\"32\":1}}],[\"如imagenet\",{\"1\":{\"32\":1}}],[\"如果进一步采用convirt\",{\"1\":{\"32\":1}}],[\"如果在读取图片过程中出现错误\",{\"1\":{\"29\":1}}],[\"如果没有则下载\",{\"1\":{\"29\":1,\"31\":1}}],[\"如果我们直接使用类别标签作为文本描述\",{\"1\":{\"28\":1}}],[\"如果有个类别\",{\"1\":{\"27\":1}}],[\"如何将这个预训练的视觉模型应用到新的任务中呢\",{\"1\":{\"27\":1}}],[\"如自注意力层和前馈层\",{\"1\":{\"11\":1}}],[\"如翻译后的句子\",{\"1\":{\"11\":1}}],[\"如句子\",{\"1\":{\"11\":1}}],[\"解码器层\",{\"1\":{\"19\":1}}],[\"解码器的最终输出通过一个线性层和\",{\"1\":{\"11\":1}}],[\"解码器\",{\"1\":{\"11\":1,\"20\":1}}],[\"编码器隐藏层输出\",{\"1\":{\"20\":1}}],[\"编码器层\",{\"1\":{\"16\":1}}],[\"编码器\",{\"1\":{\"11\":1}}],[\"emb\",{\"1\":{\"30\":2,\"31\":2}}],[\"embeddings\",{\"1\":{\"29\":12,\"30\":4,\"31\":15}}],[\"embedding\",{\"1\":{\"29\":4,\"30\":1,\"31\":3}}],[\"embed\",{\"1\":{\"12\":8,\"26\":2}}],[\"error\",{\"1\":{\"29\":2,\"30\":1,\"31\":3}}],[\"exist\",{\"1\":{\"30\":1,\"31\":1}}],[\"exists\",{\"1\":{\"29\":1,\"30\":1,\"31\":2}}],[\"extension\",{\"1\":{\"29\":2,\"31\":2}}],[\"exception\",{\"1\":{\"29\":2,\"30\":1,\"31\":3}}],[\"except\",{\"1\":{\"29\":2,\"30\":1,\"31\":3}}],[\"exp\",{\"1\":{\"26\":1}}],[\"enumerate\",{\"1\":{\"29\":1,\"31\":1}}],[\"end\",{\"1\":{\"29\":4,\"31\":4}}],[\"engineering\",{\"1\":{\"28\":1}}],[\"entropy\",{\"1\":{\"26\":3}}],[\"encode\",{\"1\":{\"12\":2,\"27\":2}}],[\"encoder提取图像特征\",{\"1\":{\"27\":1}}],[\"encoder模型结构图\",{\"1\":{\"17\":1}}],[\"encoderlayer模型结构图\",{\"1\":{\"16\":1}}],[\"encoderlayer\",{\"0\":{\"16\":1},\"1\":{\"16\":2}}],[\"encoderdecoder\",{\"1\":{\"12\":2}}],[\"encoderdecoder模型结构图\",{\"1\":{\"12\":1}}],[\"encoder\",{\"0\":{\"12\":1,\"14\":1,\"17\":1},\"1\":{\"11\":2,\"12\":4,\"17\":3,\"26\":8,\"27\":4}}],[\"encoding\",{\"1\":{\"11\":1}}],[\"e\",{\"1\":{\"24\":3,\"26\":6,\"29\":4,\"30\":2,\"31\":6}}],[\"equals\",{\"1\":{\"21\":1}}],[\"each\",{\"1\":{\"17\":1}}],[\"else\",{\"1\":{\"7\":1,\"29\":1,\"30\":1,\"31\":2}}],[\"elowen\",{\"0\":{\"3\":1}}],[\"整个句子\",{\"1\":{\"10\":1}}],[\"它将输入的向量转换为\",{\"1\":{\"29\":1}}],[\"它通过处理器对输入文本进行处理\",{\"1\":{\"29\":1}}],[\"它通过位置编码将序列中词的位置信息注入到输入中\",{\"1\":{\"11\":1}}],[\"它由两个部分组成\",{\"1\":{\"27\":1}}],[\"它比谷歌的jft\",{\"1\":{\"26\":1}}],[\"它代表着一种基于对比文本\",{\"1\":{\"25\":1}}],[\"它可以当成函数调用\",{\"1\":{\"15\":1}}],[\"它允许模型在处理每个词时关注输入序列中的所有词\",{\"1\":{\"11\":1}}],[\"它在编码每一词的时候都能够注意\",{\"1\":{\"10\":1}}],[\"它较难学习到长距离的依赖关系\",{\"1\":{\"10\":1}}],[\"这大大限制了它们的迁移能力和扩展性\",{\"1\":{\"32\":1}}],[\"这远远低于imagenet上的sota\",{\"1\":{\"32\":1}}],[\"这方面的工作并不多\",{\"1\":{\"32\":1}}],[\"这个函数的作用是将输入的文本转化为对应的嵌入表示\",{\"1\":{\"29\":1}}],[\"这个函数的有一个输入参数\",{\"1\":{\"15\":1}}],[\"这个\",{\"1\":{\"29\":1}}],[\"这种差异一方面是由于文本和图像属于两个完全不同的模态\",{\"1\":{\"32\":1}}],[\"这种预训练通常是基于有监督学习的\",{\"1\":{\"32\":1}}],[\"这种格式\",{\"1\":{\"28\":1}}],[\"这种方法实际上与nlp领域的一个研究方向\",{\"1\":{\"28\":1}}],[\"这与传统的预训练加微调的方法有所不同\",{\"1\":{\"28\":1}}],[\"这样的格式来生成文本描述\",{\"1\":{\"28\":1}}],[\"这展示了其在图像分类任务中的灵活性和强大能力\",{\"1\":{\"27\":1}}],[\"这一过程与训练时相同\",{\"1\":{\"27\":1}}],[\"这不仅展示了clip的强大功能\",{\"1\":{\"27\":1}}],[\"这意味着它能够在没有任何特定任务训练数据的情况下\",{\"1\":{\"27\":1}}],[\"这表明训练clip模型需要消耗大量的资源\",{\"1\":{\"26\":1}}],[\"这些相似度值可以被视为logits\",{\"1\":{\"27\":1}}],[\"这些图像特征会与之前得到的个文本特征进行余弦相似度计算\",{\"1\":{\"27\":1}}],[\"这些文本随后被输入到文本编码器\",{\"1\":{\"27\":1}}],[\"这些数据在论文中被称为webimagetext\",{\"1\":{\"26\":1}}],[\"这些模型都是以cnn为基础\",{\"1\":{\"10\":1}}],[\"这里以搜索向日葵花为例\",{\"1\":{\"30\":1}}],[\"这里采用了余弦相似度的计算方法\",{\"1\":{\"29\":1}}],[\"这里共有个正样本\",{\"1\":{\"26\":1}}],[\"这里的相似度直接计算文本特征和图像特征的余弦相似性\",{\"1\":{\"26\":1}}],[\"这里对提取的文本特征和图像特征进行对比学习\",{\"1\":{\"26\":1}}],[\"这两个模型都属于融合图像与文本的多模态模型\",{\"1\":{\"24\":1}}],[\"这和原始论文稍有不同\",{\"1\":{\"15\":1}}],[\"这比较容易并行\",{\"1\":{\"10\":1}}],[\"因为训练数据集中的文本\",{\"1\":{\"32\":1}}],[\"因为cuda不支持macos\",{\"1\":{\"9\":1}}],[\"因此\",{\"1\":{\"32\":2}}],[\"因此在新的数据集上需要定义新的分类器来重新训练\",{\"1\":{\"32\":1}}],[\"因此在效果上可能不如使用\",{\"1\":{\"28\":1}}],[\"因此成本较高\",{\"1\":{\"32\":1}}],[\"因此这里就不再给出数据集下载链接了\",{\"1\":{\"29\":1}}],[\"因此这是一个非常庞大的数据集\",{\"1\":{\"26\":1}}],[\"因此可以充分利用计算资源\",{\"1\":{\"10\":1}}],[\"因此出现了extended\",{\"1\":{\"10\":1}}],[\"背景\",{\"0\":{\"10\":1}}],[\"改为\",{\"1\":{\"9\":1}}],[\"95\",{\"1\":{\"29\":1}}],[\"9\",{\"1\":{\"9\":1}}],[\"hub\",{\"1\":{\"31\":1}}],[\"huggingface\",{\"1\":{\"31\":1}}],[\"horse\",{\"1\":{\"27\":1}}],[\"h\",{\"1\":{\"21\":9,\"26\":1}}],[\"heads\",{\"1\":{\"21\":3}}],[\"head\",{\"1\":{\"11\":1}}],[\"harvardnlp\",{\"1\":{\"9\":1}}],[\"has\",{\"1\":{\"7\":6}}],[\"https\",{\"1\":{\"9\":1}}],[\"环境搭建遵从如下步骤即可\",{\"1\":{\"9\":1}}],[\"环境\",{\"0\":{\"9\":1}}],[\"机器翻译实战\",{\"1\":{\"8\":1}}],[\"you\",{\"1\":{\"7\":1}}],[\"root\",{\"1\":{\"29\":2,\"31\":2}}],[\"rocket\",{\"1\":{\"27\":1}}],[\"rgb\",{\"1\":{\"29\":1,\"31\":1}}],[\"rn50x16和rnx64\",{\"1\":{\"26\":1}}],[\"rn50x4\",{\"1\":{\"26\":1}}],[\"rnn等模型的缺点是需要顺序计算\",{\"1\":{\"10\":1}}],[\"right\",{\"1\":{\"19\":1}}],[\"repo\",{\"1\":{\"29\":1,\"31\":1}}],[\"replace\",{\"1\":{\"29\":1,\"31\":1}}],[\"red\",{\"1\":{\"27\":1}}],[\"resnet101\",{\"1\":{\"26\":1}}],[\"resnet50\",{\"1\":{\"26\":1}}],[\"resnet包含五种不同尺寸的模型\",{\"1\":{\"26\":1}}],[\"resnet\",{\"1\":{\"26\":1}}],[\"residual\",{\"1\":{\"11\":1}}],[\"requirements\",{\"1\":{\"9\":2}}],[\"retrieves\",{\"1\":{\"7\":1}}],[\"returns\",{\"1\":{\"7\":2}}],[\"return\",{\"1\":{\"7\":7,\"12\":3,\"13\":1,\"15\":1,\"16\":1,\"17\":1,\"19\":1,\"20\":1,\"21\":2,\"29\":9,\"30\":4,\"31\":13}}],[\"r\",{\"1\":{\"9\":1}}],[\"raise\",{\"1\":{\"7\":1}}],[\"range\",{\"1\":{\"7\":1,\"29\":1,\"31\":1}}],[\"``encode\",{\"1\":{\"7\":1}}],[\"``prepare\",{\"1\":{\"7\":1}}],[\"words\",{\"1\":{\"32\":3}}],[\"warnings\",{\"1\":{\"31\":2}}],[\"walk\",{\"1\":{\"29\":1,\"31\":1}}],[\"white\",{\"1\":{\"27\":1}}],[\"when\",{\"1\":{\"7\":2}}],[\"w\",{\"1\":{\"21\":4,\"26\":5}}],[\"web\",{\"1\":{\"32\":1}}],[\"weakly\",{\"1\":{\"32\":1}}],[\"weights\",{\"1\":{\"29\":2,\"31\":2}}],[\"we\",{\"1\":{\"21\":1}}],[\"with\",{\"1\":{\"7\":3,\"20\":1,\"27\":3,\"29\":2,\"31\":2}}],[\"grams\",{\"1\":{\"32\":2}}],[\"grad\",{\"1\":{\"27\":2,\"29\":2,\"31\":2}}],[\"garage\",{\"1\":{\"27\":1}}],[\"generic\",{\"1\":{\"20\":1}}],[\"generator模型结构图\",{\"1\":{\"13\":1}}],[\"generator\",{\"0\":{\"13\":1},\"1\":{\"12\":3,\"13\":2}}],[\"getcwd\",{\"1\":{\"29\":1,\"31\":1}}],[\"get\",{\"1\":{\"7\":1,\"29\":10,\"30\":4,\"31\":12}}],[\"gpu上训练12天\",{\"1\":{\"26\":1}}],[\"gpu上训练18天\",{\"1\":{\"26\":1}}],[\"gpu上的矩阵运算都是充分优化和高度并行的\",{\"1\":{\"10\":1}}],[\"gpu\",{\"1\":{\"10\":1}}],[\"github\",{\"1\":{\"9\":1}}],[\"git\",{\"1\":{\"9\":1}}],[\"golang\",{\"1\":{\"2\":1}}],[\"|\",{\"1\":{\"7\":2}}],[\"unsqueeze\",{\"1\":{\"21\":1}}],[\"unk\",{\"1\":{\"7\":1}}],[\"use\",{\"1\":{\"29\":1,\"31\":1}}],[\"used\",{\"1\":{\"7\":1}}],[\"using\",{\"1\":{\"7\":1,\"21\":1}}],[\"=>\",{\"1\":{\"21\":1}}],[\"==\",{\"1\":{\"9\":1,\"21\":2,\"29\":1,\"31\":1}}],[\"=\",{\"1\":{\"7\":4,\"12\":5,\"13\":1,\"15\":2,\"16\":6,\"17\":3,\"19\":9,\"20\":4,\"21\":15,\"26\":9,\"27\":10,\"29\":40,\"30\":9,\"31\":46}}],[\"+=\",{\"1\":{\"29\":1,\"31\":1}}],[\"+\",{\"1\":{\"7\":18,\"15\":4,\"16\":1,\"19\":4,\"20\":1,\"26\":1,\"27\":1,\"29\":2,\"31\":2}}],[\"name\",{\"1\":{\"29\":8,\"31\":8}}],[\"natural\",{\"1\":{\"28\":1}}],[\"num\",{\"1\":{\"29\":2,\"31\":2}}],[\"numpy\",{\"1\":{\"27\":2,\"29\":4,\"31\":3}}],[\"number\",{\"1\":{\"21\":1}}],[\"np\",{\"1\":{\"26\":5,\"27\":1,\"29\":6,\"30\":1,\"31\":8}}],[\"nlp\",{\"1\":{\"26\":1}}],[\"nbatches\",{\"1\":{\"21\":3}}],[\"n个解码器层\",{\"1\":{\"20\":1}}],[\"nn\",{\"1\":{\"12\":1,\"13\":2,\"15\":2,\"16\":1,\"17\":1,\"19\":1,\"20\":1,\"21\":3}}],[\"n\",{\"1\":{\"9\":1,\"17\":3,\"20\":3,\"26\":7,\"32\":1}}],[\"needed\",{\"1\":{\"29\":2,\"31\":2}}],[\"network\",{\"1\":{\"11\":1}}],[\"netpoll\",{\"1\":{\"2\":1}}],[\"neural\",{\"1\":{\"10\":1,\"11\":1}}],[\"necessary\",{\"1\":{\"7\":1}}],[\"normalize\",{\"1\":{\"26\":2}}],[\"normalization\",{\"1\":{\"11\":1}}],[\"norm\",{\"1\":{\"15\":2,\"17\":2,\"20\":2,\"27\":2,\"29\":2,\"31\":2}}],[\"not\",{\"1\":{\"7\":5,\"21\":3,\"29\":3,\"30\":2,\"31\":5,\"35\":1}}],[\"no\",{\"1\":{\"7\":1,\"27\":2,\"29\":2,\"30\":1,\"31\":3}}],[\"none\",{\"1\":{\"7\":5,\"21\":4,\"29\":3,\"30\":2,\"31\":4}}],[\"x\",{\"1\":{\"7\":3,\"13\":2,\"15\":3,\"16\":8,\"17\":4,\"19\":12,\"20\":4,\"21\":7}}],[\"title\",{\"1\":{\"30\":1,\"31\":1}}],[\"time\",{\"1\":{\"29\":10,\"31\":10}}],[\"test\",{\"1\":{\"29\":2,\"31\":1}}],[\"tensors=\",{\"1\":{\"29\":2,\"31\":2}}],[\"tensor\",{\"1\":{\"27\":1}}],[\"temperature\",{\"1\":{\"26\":1}}],[\"text转化为18291个类别\",{\"1\":{\"32\":1}}],[\"text=texts\",{\"1\":{\"29\":1,\"31\":1}}],[\"texts\",{\"1\":{\"26\":1,\"27\":2,\"29\":1,\"31\":1}}],[\"text\",{\"1\":{\"26\":6,\"27\":13,\"29\":9,\"30\":9,\"31\":16}}],[\"t\",{\"1\":{\"26\":15,\"27\":2,\"29\":1,\"31\":1}}],[\"turn\",{\"1\":{\"17\":1}}],[\"tabby\",{\"1\":{\"27\":1}}],[\"target\",{\"1\":{\"12\":1}}],[\"taken\",{\"1\":{\"29\":2,\"31\":1}}],[\"take\",{\"1\":{\"12\":1,\"21\":1}}],[\"task\",{\"1\":{\"7\":1}}],[\"tasks\",{\"1\":{\"7\":1}}],[\"tgt\",{\"1\":{\"12\":12,\"19\":2,\"20\":2}}],[\"txt\",{\"1\":{\"9\":2}}],[\"try\",{\"1\":{\"29\":2,\"30\":1,\"31\":3}}],[\"tripod\",{\"1\":{\"27\":1}}],[\"train\",{\"1\":{\"28\":1}}],[\"training\",{\"1\":{\"25\":1}}],[\"transpose\",{\"1\":{\"21\":3}}],[\"transformers\",{\"1\":{\"31\":1}}],[\"transformer\",{\"1\":{\"9\":5,\"11\":4,\"24\":3,\"26\":3,\"29\":2,\"32\":1}}],[\"true\",{\"1\":{\"7\":1}}],[\"through\",{\"1\":{\"17\":1}}],[\"this\",{\"1\":{\"7\":1,\"27\":1}}],[\"that\",{\"1\":{\"7\":1}}],[\"the\",{\"1\":{\"7\":11,\"9\":1,\"17\":1,\"21\":2,\"27\":1,\"30\":1,\"31\":1}}],[\"total\",{\"1\":{\"29\":4,\"31\":4}}],[\"topk\",{\"1\":{\"27\":1}}],[\"top\",{\"1\":{\"27\":2}}],[\"torch==1\",{\"1\":{\"9\":1}}],[\"torch\",{\"1\":{\"9\":1,\"21\":2,\"27\":3,\"29\":4,\"31\":5}}],[\"to\",{\"1\":{\"7\":2,\"10\":1,\"21\":1,\"26\":2,\"29\":6,\"31\":5}}],[\"tokenize\",{\"1\":{\"27\":1}}],[\"tokenizer\",{\"1\":{\"7\":1}}],[\"token\",{\"1\":{\"7\":36}}],[\"tokens=false\",{\"1\":{\"7\":1}}],[\"tokens\",{\"1\":{\"7\":11,\"27\":2}}],[\"two\",{\"1\":{\"7\":1}}],[\"type\",{\"1\":{\"7\":1}}],[\"flowerclassify\",{\"1\":{\"29\":1,\"31\":1}}],[\"flower\",{\"1\":{\"29\":4,\"31\":2}}],[\"float\",{\"1\":{\"27\":2}}],[\"flag\",{\"1\":{\"27\":1}}],[\"facial\",{\"1\":{\"27\":1}}],[\"false\",{\"1\":{\"7\":1}}],[\"filterwarnings\",{\"1\":{\"31\":1}}],[\"file\",{\"1\":{\"29\":5,\"31\":5}}],[\"files\",{\"1\":{\"29\":2,\"31\":2}}],[\"fill\",{\"1\":{\"21\":1}}],[\"find\",{\"1\":{\"30\":2,\"31\":2}}],[\"finetune\",{\"1\":{\"26\":1}}],[\"final\",{\"1\":{\"21\":1}}],[\"figure\",{\"1\":{\"16\":1,\"19\":1,\"21\":1}}],[\"first\",{\"1\":{\"7\":2}}],[\"f\",{\"1\":{\"13\":1,\"26\":4,\"29\":7,\"30\":4,\"31\":11}}],[\"features\",{\"1\":{\"27\":10,\"29\":4,\"31\":4,\"32\":1}}],[\"feed\",{\"1\":{\"11\":1,\"16\":4,\"19\":5}}],[\"fetching\",{\"1\":{\"7\":1}}],[\"found\",{\"1\":{\"30\":1,\"31\":1,\"35\":1}}],[\"follow\",{\"1\":{\"16\":1,\"19\":1}}],[\"following\",{\"1\":{\"7\":2}}],[\"forward\",{\"1\":{\"11\":1,\"12\":1,\"13\":1,\"15\":1,\"16\":5,\"17\":1,\"19\":6,\"20\":1,\"21\":1}}],[\"formated\",{\"1\":{\"7\":2}}],[\"format\",{\"1\":{\"7\":2}}],[\"for\",{\"1\":{\"7\":7,\"16\":1,\"17\":1,\"19\":1,\"20\":1,\"21\":1,\"27\":1,\"29\":6,\"30\":2,\"31\":8}}],[\"from\",{\"1\":{\"7\":4,\"21\":1,\"29\":2,\"30\":2,\"31\":8,\"32\":2}}],[\"os\",{\"1\":{\"29\":11,\"30\":2,\"31\":14}}],[\"opening\",{\"1\":{\"30\":1,\"31\":1}}],[\"open\",{\"1\":{\"29\":1,\"30\":1,\"31\":2}}],[\"openai首先尝试了virtex模型\",{\"1\":{\"32\":1}}],[\"openai从网络上收集了4亿条数据进行实验\",{\"1\":{\"32\":1}}],[\"openai从网络上收集了总计4亿对文本和图像\",{\"1\":{\"26\":1}}],[\"openai\",{\"1\":{\"24\":1,\"29\":2,\"31\":1}}],[\"optional\",{\"1\":{\"7\":1}}],[\"on\",{\"1\":{\"21\":1,\"27\":3}}],[\"only\",{\"1\":{\"7\":1}}],[\"off\",{\"1\":{\"30\":1,\"31\":1}}],[\"of\",{\"1\":{\"7\":7,\"17\":1,\"19\":1,\"21\":1,\"26\":4,\"27\":6,\"28\":3,\"29\":2,\"30\":1,\"31\":2,\"32\":3}}],[\"or\",{\"1\":{\"7\":2,\"26\":2}}],[\"oracle\",{\"0\":{\"2\":1},\"1\":{\"0\":1}}],[\"axis\",{\"1\":{\"30\":1,\"31\":1}}],[\"axis=0\",{\"1\":{\"26\":1}}],[\"axis=1\",{\"1\":{\"26\":3,\"29\":3,\"31\":3}}],[\"acc\",{\"1\":{\"29\":2,\"31\":2}}],[\"accuracy\",{\"1\":{\"29\":8,\"31\":6}}],[\"actual\",{\"1\":{\"29\":2,\"31\":2}}],[\"activate\",{\"1\":{\"9\":1}}],[\"append\",{\"1\":{\"29\":3,\"31\":3}}],[\"apply\",{\"1\":{\"21\":2}}],[\"applied\",{\"1\":{\"21\":1}}],[\"available\",{\"1\":{\"29\":1,\"31\":1}}],[\"at\",{\"1\":{\"27\":1}}],[\"attn\",{\"1\":{\"16\":4,\"19\":10,\"21\":7}}],[\"attenion\",{\"1\":{\"15\":1}}],[\"attention\",{\"1\":{\"11\":2,\"21\":4}}],[\"attention可以用矩阵乘法一次计算所有的时刻\",{\"1\":{\"10\":1}}],[\"attention机制\",{\"1\":{\"10\":1}}],[\"attend\",{\"1\":{\"10\":1}}],[\"american\",{\"1\":{\"27\":1}}],[\"as\",{\"1\":{\"29\":2,\"30\":1,\"31\":5}}],[\"astronaut\",{\"1\":{\"27\":1}}],[\"assume\",{\"1\":{\"21\":1}}],[\"assert\",{\"1\":{\"21\":1}}],[\"about\",{\"1\":{\"27\":1}}],[\"argmax\",{\"1\":{\"29\":1,\"30\":1,\"31\":2}}],[\"args\",{\"1\":{\"7\":1}}],[\"array\",{\"1\":{\"29\":2,\"31\":2}}],[\"arange\",{\"1\":{\"26\":1}}],[\"alexnet\",{\"1\":{\"29\":1}}],[\"aligned\",{\"1\":{\"26\":2}}],[\"all\",{\"1\":{\"21\":3,\"29\":3,\"30\":1,\"31\":3}}],[\"always\",{\"1\":{\"21\":1}}],[\"already\",{\"1\":{\"7\":5}}],[\"an\",{\"1\":{\"27\":1}}],[\"annotated\",{\"1\":{\"9\":5}}],[\"and\",{\"1\":{\"7\":1,\"12\":2,\"17\":1,\"19\":1,\"21\":2,\"27\":1,\"28\":1}}],[\"added\",{\"1\":{\"7\":1}}],[\"adding\",{\"1\":{\"7\":2}}],[\"a\",{\"1\":{\"7\":12,\"17\":1,\"21\":2,\"27\":16,\"28\":3,\"29\":1,\"30\":2,\"31\":3}}],[\"14模型\",{\"1\":{\"26\":1}}],[\"14则需要在256个v100\",{\"1\":{\"26\":1}}],[\"14\",{\"1\":{\"26\":2,\"29\":1}}],[\"16和vit\",{\"1\":{\"26\":1}}],[\"16倍和64倍得到的\",{\"1\":{\"26\":1}}],[\"1e9\",{\"1\":{\"21\":1}}],[\"11\",{\"1\":{\"9\":2}}],[\"1\",{\"1\":{\"7\":29,\"9\":1,\"13\":1,\"16\":2,\"19\":2,\"21\":15,\"24\":1,\"27\":5,\"29\":4,\"31\":4}}],[\"1=none\",{\"1\":{\"7\":3}}],[\"10\",{\"1\":{\"21\":3}}],[\"100\",{\"1\":{\"7\":1,\"27\":1,\"29\":1,\"31\":1}}],[\"103\",{\"1\":{\"7\":1}}],[\"102\",{\"1\":{\"7\":1}}],[\"101\",{\"1\":{\"7\":1}}],[\"icmlm和convirt仅在10万级别的数据上训练了几天\",{\"1\":{\"32\":1}}],[\"icmlm基于语言掩码的方法\",{\"1\":{\"32\":1}}],[\"ignore\",{\"1\":{\"31\":1}}],[\"i\",{\"1\":{\"26\":12,\"29\":2,\"31\":2}}],[\"import\",{\"1\":{\"31\":9}}],[\"implements\",{\"1\":{\"21\":1}}],[\"imshow\",{\"1\":{\"30\":1,\"31\":1}}],[\"img\",{\"1\":{\"30\":2,\"31\":2}}],[\"images=images\",{\"1\":{\"29\":1,\"31\":1}}],[\"images\",{\"1\":{\"26\":1,\"27\":1,\"29\":3,\"30\":2,\"31\":5}}],[\"image\",{\"1\":{\"25\":1,\"26\":5,\"27\":10,\"29\":31,\"30\":21,\"31\":51,\"32\":1}}],[\"index\",{\"1\":{\"29\":2,\"30\":2,\"31\":4}}],[\"indices\",{\"1\":{\"29\":2,\"31\":2}}],[\"input\",{\"1\":{\"17\":1,\"27\":2}}],[\"inputs\",{\"1\":{\"7\":2,\"29\":4,\"31\":4}}],[\"init\",{\"1\":{\"12\":2,\"13\":2,\"15\":2,\"16\":2,\"17\":2,\"19\":2,\"20\":2,\"21\":2}}],[\"install\",{\"1\":{\"9\":1}}],[\"integers\",{\"1\":{\"7\":1}}],[\"in\",{\"1\":{\"7\":3,\"12\":1,\"17\":2,\"20\":1,\"21\":4,\"27\":2,\"28\":1,\"29\":7,\"31\":7}}],[\"isdir\",{\"1\":{\"29\":1,\"31\":1}}],[\"is\",{\"1\":{\"7\":8,\"17\":1,\"19\":1,\"21\":3,\"27\":1,\"29\":2,\"30\":2,\"31\":4}}],[\"if\",{\"1\":{\"7\":9,\"21\":3,\"29\":9,\"30\":3,\"31\":12}}],[\"idx\",{\"1\":{\"29\":5,\"31\":5}}],[\"id=model\",{\"1\":{\"29\":1,\"31\":1}}],[\"ids\",{\"1\":{\"7\":29}}],[\"id\",{\"1\":{\"7\":9}}],[\"pil\",{\"1\":{\"31\":1}}],[\"pip\",{\"1\":{\"9\":1}}],[\"pyplot\",{\"1\":{\"31\":1}}],[\"python=3\",{\"1\":{\"9\":1}}],[\"plt\",{\"1\":{\"30\":4,\"31\":5}}],[\"plus``\",{\"1\":{\"7\":1}}],[\"png\",{\"1\":{\"29\":1,\"31\":1}}],[\"pt\",{\"1\":{\"29\":2,\"31\":2}}],[\"person\",{\"1\":{\"27\":1}}],[\"photos下的子目录名作为我们的候选待匹配分类文本列表\",{\"1\":{\"29\":1}}],[\"photos\",{\"1\":{\"29\":4,\"31\":2}}],[\"photos目录下读取出所有图片的路径\",{\"1\":{\"29\":1}}],[\"photo\",{\"1\":{\"27\":2,\"28\":2,\"29\":2,\"30\":1,\"31\":2}}],[\"p\",{\"1\":{\"21\":5}}],[\"p=dropout\",{\"1\":{\"21\":1}}],[\"portrait\",{\"1\":{\"27\":1}}],[\"portion\",{\"1\":{\"7\":1}}],[\"positional\",{\"1\":{\"11\":1}}],[\"print\",{\"1\":{\"29\":6,\"30\":4,\"31\":10}}],[\"pretrained\",{\"1\":{\"29\":4,\"31\":4}}],[\"pretrainedtokenizer\",{\"1\":{\"7\":1}}],[\"prediction\",{\"1\":{\"32\":1}}],[\"predicted\",{\"1\":{\"29\":6,\"31\":6}}],[\"predict\",{\"1\":{\"28\":1}}],[\"pre\",{\"1\":{\"25\":1,\"28\":1}}],[\"prompting\",{\"1\":{\"28\":1}}],[\"prompt\",{\"1\":{\"28\":3}}],[\"probs\",{\"1\":{\"27\":3}}],[\"product\",{\"1\":{\"21\":1}}],[\"projected\",{\"1\":{\"21\":1}}],[\"projections\",{\"1\":{\"21\":1}}],[\"projection\",{\"1\":{\"21\":1}}],[\"proj\",{\"1\":{\"13\":2,\"26\":2}}],[\"processor\",{\"1\":{\"29\":3,\"31\":3}}],[\"processing\",{\"1\":{\"28\":1}}],[\"process\",{\"1\":{\"12\":1}}],[\"provided\",{\"1\":{\"7\":1}}],[\"paths\",{\"1\":{\"29\":17,\"30\":6,\"31\":21}}],[\"path\",{\"1\":{\"29\":12,\"30\":2,\"31\":14}}],[\"patch14\",{\"1\":{\"29\":2,\"31\":1}}],[\"page\",{\"1\":{\"27\":1}}],[\"parameter\",{\"1\":{\"26\":1}}],[\"pass\",{\"1\":{\"17\":1}}],[\"passed\",{\"1\":{\"7\":1}}],[\"pairs\",{\"1\":{\"7\":1}}],[\"pair\",{\"1\":{\"7\":4}}],[\"padding=true\",{\"1\":{\"29\":1,\"31\":1}}],[\"padding\",{\"1\":{\"11\":1}}],[\"pad\",{\"1\":{\"7\":1}}],[\"0+cu113\",{\"1\":{\"9\":1}}],[\"0\",{\"1\":{\"7\":31,\"9\":1,\"16\":1,\"19\":1,\"21\":3,\"27\":1,\"29\":1,\"31\":1}}],[\"min\",{\"1\":{\"29\":1,\"31\":1}}],[\"minibatch\",{\"1\":{\"26\":2}}],[\"most\",{\"1\":{\"30\":10,\"31\":10}}],[\"motorcycle\",{\"1\":{\"27\":1}}],[\"module\",{\"1\":{\"12\":1,\"13\":1,\"15\":1,\"16\":1,\"17\":1,\"19\":1,\"20\":1,\"21\":1}}],[\"model是decoder输出的大小\",{\"1\":{\"13\":1}}],[\"model``\",{\"1\":{\"7\":1}}],[\"model\",{\"1\":{\"7\":3,\"13\":2,\"21\":7,\"27\":2,\"29\":16,\"31\":16,\"32\":1}}],[\"m\",{\"1\":{\"19\":3}}],[\"memory\",{\"1\":{\"12\":2,\"19\":2,\"20\":2}}],[\"methods\",{\"1\":{\"7\":1,\"28\":1}}],[\"method\",{\"1\":{\"7\":1}}],[\"metamind\",{\"1\":{\"0\":1}}],[\"multiheadedattention\",{\"1\":{\"21\":2}}],[\"multi\",{\"1\":{\"11\":1}}],[\"must\",{\"1\":{\"7\":2}}],[\"matplotlib\",{\"1\":{\"31\":1}}],[\"matching\",{\"1\":{\"30\":11,\"31\":11}}],[\"math\",{\"1\":{\"21\":1}}],[\"matmul\",{\"1\":{\"21\":2}}],[\"made\",{\"1\":{\"19\":1}}],[\"macos\",{\"1\":{\"9\":1}}],[\"map\",{\"1\":{\"7\":1}}],[\"mask=mask\",{\"1\":{\"21\":1}}],[\"mask=none\",{\"1\":{\"21\":2}}],[\"masked\",{\"1\":{\"12\":1,\"21\":1}}],[\"masking\",{\"1\":{\"11\":1,\"20\":1}}],[\"mask\",{\"1\":{\"7\":5,\"11\":1,\"12\":11,\"16\":2,\"17\":3,\"19\":4,\"20\":4,\"21\":6}}],[\"symlinks=false\",{\"1\":{\"29\":1,\"31\":1}}],[\"systematic\",{\"1\":{\"28\":1}}],[\"snapshot\",{\"1\":{\"29\":1,\"31\":2}}],[\"save\",{\"1\":{\"29\":3,\"31\":3}}],[\"saucer\",{\"1\":{\"27\":1}}],[\"same\",{\"1\":{\"21\":1}}],[\"start\",{\"1\":{\"29\":5,\"31\":5}}],[\"standing\",{\"1\":{\"27\":2}}],[\"stack\",{\"1\":{\"17\":1,\"27\":1}}],[\"show\",{\"1\":{\"30\":1,\"31\":1}}],[\"shot性能评估\",{\"1\":{\"32\":1}}],[\"shot性能\",{\"1\":{\"32\":1}}],[\"shot迁移到下游任务\",{\"1\":{\"32\":1}}],[\"shot学习\",{\"1\":{\"32\":1}}],[\"shot推理\",{\"1\":{\"29\":1}}],[\"shot分类时\",{\"1\":{\"28\":1}}],[\"shot分类的过程相当直接\",{\"1\":{\"27\":1}}],[\"shot图像分类\",{\"1\":{\"27\":1}}],[\"should\",{\"1\":{\"7\":1}}],[\"sqrt\",{\"1\":{\"21\":1}}],[\"scores\",{\"1\":{\"21\":4}}],[\"scaled\",{\"1\":{\"21\":1}}],[\"similarities\",{\"1\":{\"29\":2,\"30\":2,\"31\":4}}],[\"similarity\",{\"1\":{\"26\":1,\"27\":1,\"29\":2,\"30\":1,\"31\":3}}],[\"silhouette\",{\"1\":{\"27\":1}}],[\"size=64\",{\"1\":{\"29\":2,\"31\":2}}],[\"size\",{\"1\":{\"15\":2,\"16\":4,\"17\":1,\"19\":4,\"20\":1,\"21\":3,\"29\":4,\"31\":4}}],[\"single\",{\"1\":{\"7\":1}}],[\"sunflowers\",{\"1\":{\"30\":1,\"31\":1}}],[\"sub\",{\"1\":{\"29\":5,\"31\":5}}],[\"sublayer是传入的参数\",{\"1\":{\"15\":1}}],[\"sublayer\",{\"1\":{\"15\":3,\"16\":3,\"19\":4}}],[\"sublayerconnection模型结构图\",{\"1\":{\"15\":1}}],[\"sublayerconnection\",{\"0\":{\"15\":1},\"1\":{\"15\":2,\"16\":1,\"19\":1}}],[\"successfully\",{\"1\":{\"29\":1,\"31\":1}}],[\"survey\",{\"1\":{\"28\":1}}],[\"supervised\",{\"1\":{\"32\":1}}],[\"super\",{\"1\":{\"12\":1,\"13\":1,\"15\":1,\"16\":1,\"17\":1,\"19\":1,\"20\":1,\"21\":1}}],[\"supply\",{\"1\":{\"7\":1}}],[\"src\",{\"1\":{\"12\":16,\"19\":7,\"20\":2}}],[\"softmax\",{\"1\":{\"11\":1,\"13\":1,\"21\":1,\"27\":1}}],[\"s\",{\"1\":{\"7\":1}}],[\"searchpicbytext\",{\"1\":{\"30\":1,\"31\":1}}],[\"segmentation\",{\"1\":{\"27\":1}}],[\"seq\",{\"1\":{\"21\":1}}],[\"sequences\",{\"1\":{\"7\":3,\"12\":1}}],[\"sequence\",{\"1\":{\"7\":15}}],[\"set\",{\"1\":{\"7\":1}}],[\"seconds\",{\"1\":{\"29\":2,\"31\":1}}],[\"second\",{\"1\":{\"7\":2}}],[\"self\",{\"1\":{\"7\":11,\"11\":2,\"12\":16,\"13\":5,\"15\":8,\"16\":15,\"17\":7,\"19\":19,\"20\":7,\"21\":15}}],[\"sep\",{\"1\":{\"7\":15}}],[\"split\",{\"1\":{\"29\":1,\"31\":1}}],[\"splitext\",{\"1\":{\"29\":1,\"31\":1}}],[\"special\",{\"1\":{\"7\":13}}],[\"spring\",{\"1\":{\"2\":1}}],[\"特殊token\",{\"1\":{\"7\":1}}],[\"bag\",{\"1\":{\"32\":3}}],[\"basename\",{\"1\":{\"29\":1,\"31\":1}}],[\"batches\",{\"1\":{\"21\":1,\"29\":2,\"31\":2}}],[\"batch\",{\"1\":{\"21\":2,\"29\":9,\"31\":9}}],[\"black\",{\"1\":{\"27\":1}}],[\"below\",{\"1\":{\"19\":1}}],[\"be\",{\"1\":{\"7\":1}}],[\"berttokenizer\",{\"1\":{\"7\":1}}],[\"bert文本分类实战\",{\"1\":{\"6\":1}}],[\"bert\",{\"0\":{\"6\":1},\"1\":{\"7\":2}}],[\"b\",{\"1\":{\"7\":1,\"26\":2}}],[\"bytenet和convs2s等网络模型\",{\"1\":{\"10\":1}}],[\"by\",{\"1\":{\"7\":1}}],[\"build\",{\"1\":{\"7\":2}}],[\"binary\",{\"0\":{\"2\":1},\"1\":{\"0\":1}}],[\"binaryoracle\",{\"1\":{\"0\":1}}],[\"lower\",{\"1\":{\"29\":1,\"31\":1}}],[\"loading\",{\"1\":{\"29\":1,\"31\":1}}],[\"local\",{\"1\":{\"29\":2,\"31\":2}}],[\"looking\",{\"1\":{\"27\":1}}],[\"loss\",{\"1\":{\"26\":8}}],[\"logits\",{\"1\":{\"26\":3}}],[\"log\",{\"1\":{\"13\":1}}],[\"l2\",{\"1\":{\"26\":2}}],[\"l\",{\"1\":{\"26\":5,\"29\":1}}],[\"learning\",{\"1\":{\"32\":2}}],[\"learning的核心思想是通过设计合适的prompt\",{\"1\":{\"28\":1}}],[\"learning或prompt\",{\"1\":{\"28\":1}}],[\"learned\",{\"1\":{\"26\":3}}],[\"left\",{\"1\":{\"16\":1}}],[\"len\",{\"1\":{\"7\":6,\"21\":1,\"29\":1,\"31\":1}}],[\"linalg\",{\"1\":{\"29\":2,\"31\":2}}],[\"lin\",{\"1\":{\"21\":2}}],[\"linears\",{\"1\":{\"21\":3}}],[\"linear\",{\"1\":{\"13\":1,\"21\":3}}],[\"listdir\",{\"1\":{\"29\":1,\"31\":1}}],[\"list\",{\"1\":{\"7\":6}}],[\"large\",{\"1\":{\"29\":2,\"31\":1,\"32\":1}}],[\"launchpad\",{\"1\":{\"27\":1}}],[\"label\",{\"1\":{\"27\":1,\"28\":2}}],[\"labels\",{\"1\":{\"26\":3,\"27\":1}}],[\"layers\",{\"1\":{\"17\":3,\"20\":2}}],[\"layernorm\",{\"1\":{\"15\":2,\"17\":1,\"20\":1}}],[\"layer\",{\"1\":{\"11\":1,\"17\":6,\"20\":6}}],[\"lambda\",{\"1\":{\"7\":1,\"16\":1,\"19\":2}}],[\"language\",{\"0\":{\"4\":1},\"1\":{\"25\":1,\"28\":1,\"32\":1}}],[\"llm\",{\"1\":{\"3\":1}}],[\"vec2\",{\"1\":{\"29\":5,\"31\":5}}],[\"vec1\",{\"1\":{\"29\":5,\"31\":5}}],[\"vectors\",{\"1\":{\"21\":1}}],[\"vgg进行实现\",{\"1\":{\"29\":1}}],[\"virtex\",{\"1\":{\"32\":1}}],[\"visual\",{\"1\":{\"32\":2}}],[\"vision\",{\"0\":{\"4\":1},\"1\":{\"24\":1,\"26\":1,\"29\":1}}],[\"vit\",{\"1\":{\"24\":2,\"26\":2,\"29\":4,\"31\":1}}],[\"view\",{\"1\":{\"21\":3}}],[\"value\",{\"1\":{\"21\":7}}],[\"valueerror\",{\"1\":{\"7\":1}}],[\"v\",{\"1\":{\"21\":2}}],[\"vocab\",{\"1\":{\"13\":2}}],[\"vocab是词典大小\",{\"1\":{\"13\":1}}],[\"vl\",{\"1\":{\"2\":1}}],[\"领域常用的文本transformer模型\",{\"1\":{\"26\":1}}],[\"领域中的一些对比学习方法\",{\"1\":{\"25\":1}}],[\"领域\",{\"1\":{\"3\":1}}],[\"转换为一系列高维向量表示\",{\"1\":{\"11\":1}}],[\"转\",{\"1\":{\"3\":1}}],[\"转型\",{\"1\":{\"2\":1}}],[\"开源项目\",{\"0\":{\"33\":1}}],[\"开源网络库\",{\"1\":{\"2\":1}}],[\"开源框架\",{\"1\":{\"2\":1}}],[\"current\",{\"1\":{\"29\":2,\"30\":1,\"31\":3}}],[\"cuda\",{\"1\":{\"27\":2,\"29\":2,\"31\":2}}],[\"cup\",{\"1\":{\"27\":1}}],[\"caption\",{\"1\":{\"32\":1}}],[\"candidates\",{\"1\":{\"29\":13,\"31\":10}}],[\"camera\",{\"1\":{\"27\":1}}],[\"category\",{\"1\":{\"29\":4,\"31\":4}}],[\"cat\",{\"1\":{\"27\":1}}],[\"called\",{\"1\":{\"7\":1}}],[\"cross\",{\"1\":{\"26\":2}}],[\"creates\",{\"1\":{\"7\":1}}],[\"create\",{\"1\":{\"7\":1,\"9\":1}}],[\"c\",{\"1\":{\"26\":1}}],[\"cbow\",{\"1\":{\"26\":1}}],[\"cnn\",{\"1\":{\"26\":1}}],[\"cpu\",{\"1\":{\"10\":1,\"27\":3,\"29\":3,\"31\":3}}],[\"cd\",{\"1\":{\"9\":1}}],[\"count\",{\"1\":{\"29\":7,\"31\":7}}],[\"correct\",{\"1\":{\"29\":3,\"31\":3}}],[\"core\",{\"1\":{\"17\":1}}],[\"coffee\",{\"1\":{\"27\":1}}],[\"cosine\",{\"1\":{\"26\":1,\"29\":2,\"30\":1,\"31\":3}}],[\"compute\",{\"1\":{\"21\":1}}],[\"com\",{\"1\":{\"9\":1}}],[\"committer\",{\"1\":{\"2\":2}}],[\"convirt基于对比学习的方法\",{\"1\":{\"32\":1}}],[\"convert\",{\"1\":{\"29\":1,\"31\":1}}],[\"contrastive\",{\"1\":{\"32\":1}}],[\"contiguous\",{\"1\":{\"21\":1}}],[\"contain\",{\"1\":{\"7\":2}}],[\"concat\",{\"1\":{\"21\":1}}],[\"concatenating\",{\"1\":{\"7\":1}}],[\"connections\",{\"1\":{\"16\":1,\"19\":1}}],[\"connection\",{\"1\":{\"11\":1}}],[\"conda\",{\"1\":{\"9\":2}}],[\"clipprocessor\",{\"1\":{\"29\":1,\"31\":2}}],[\"clipmodel\",{\"1\":{\"29\":1,\"31\":2}}],[\"clip模型均能够以较高的置信度给出正确的分类结果\",{\"1\":{\"27\":1}}],[\"clip模型能够在没有特定任务训练数据的情况下\",{\"1\":{\"27\":1}}],[\"clip模型的一个显著优势是它能够进行zero\",{\"1\":{\"27\":1}}],[\"clip模型会预测出个可能的文本\",{\"1\":{\"26\":1}}],[\"clip包含两个核心模型\",{\"1\":{\"26\":1}}],[\"clip的训练数据采用的是文本\",{\"1\":{\"25\":1}}],[\"clip的英文全称为contrastive\",{\"1\":{\"25\":1}}],[\"clip属于基于对比学习的多模态模型\",{\"1\":{\"25\":1}}],[\"clip\",{\"1\":{\"24\":3,\"27\":1,\"29\":3,\"31\":1,\"32\":1}}],[\"clip原始论文链接\",{\"1\":{\"23\":1}}],[\"clones\",{\"1\":{\"16\":1,\"17\":1,\"19\":1,\"20\":1,\"21\":1}}],[\"clone\",{\"1\":{\"9\":1}}],[\"classification\",{\"1\":{\"7\":2}}],[\"class\",{\"1\":{\"7\":1,\"12\":1,\"13\":1,\"15\":1,\"16\":1,\"17\":1,\"19\":1,\"20\":1,\"21\":1}}],[\"cls\",{\"1\":{\"7\":12}}],[\"cv\",{\"1\":{\"3\":1,\"25\":1}}],[\"csdn\",{\"1\":{\"0\":1}}],[\"知识星球\",{\"1\":{\"0\":1}}]],\"version\":2}}")).map(([e,t])=>[e,_t(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:n,options:o,id:s}})=>{const r=xt[n];e==="suggest"?self.postMessage([e,s,ve(t,r,o)]):e==="search"?self.postMessage([e,s,Ee(t,r,o,"max")]):self.postMessage({suggestions:[e,s,ve(t,r,o)],results:[e,s,Ee(t,r,o,__SLIMSEARCH_SORT_STRATEGY__)]})};
//# sourceMappingURL=index.js.map
