/**
* @vue/shared v3.5.13
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/const Se={},ze=()=>{},Ce=Object.assign,Oe=Array.isArray,D=e=>typeof e=="function",Me=e=>typeof e=="string",Ne=e=>typeof e=="symbol";let X;const L=()=>X||(X=typeof globalThis<"u"?globalThis:typeof self<"u"?self:typeof window<"u"?window:typeof global<"u"?global:{});/**
* @vue/reactivity v3.5.13
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/new Set(Object.getOwnPropertyNames(Symbol).filter(e=>e!=="arguments"&&e!=="caller").map(e=>Symbol[e]).filter(Ne));function P(e){const t=e&&e.__v_raw;return t?P(t):e}function Te(e){return e?e.__v_isRef===!0:!1}/**
* @vue/runtime-core v3.5.13
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/const v=[];function kt(e){v.push(e)}function It(){v.pop()}let W=!1;function Et(e,...t){if(W)return;W=!0;const n=v.length?v[v.length-1].component:null,o=n&&n.appContext.config.warnHandler,s=Fe();if(o)A(o,n,11,[e+t.map(r=>{var i,c;return(c=(i=r.toString)==null?void 0:i.call(r))!=null?c:JSON.stringify(r)}).join(""),n&&n.proxy,s.map(({vnode:r})=>`at <${re(n,r.type)}>`).join(`
`),s]);else{const r=[`[Vue warn]: ${e}`,...t];s.length&&r.push(`
`,...$e(s)),console.warn(...r)}W=!1}function Fe(){let e=v[v.length-1];if(!e)return[];const t=[];for(;e;){const n=t[0];n&&n.vnode===e?n.recurseCount++:t.push({vnode:e,recurseCount:0});const o=e.component&&e.component.parent;e=o&&o.vnode}return t}function $e(e){const t=[];return e.forEach((n,o)=>{t.push(...o===0?[]:[`
`],...Ve(n))}),t}function Ve({vnode:e,recurseCount:t}){const n=t>0?`... (${t} recursive calls)`:"",o=e.component?e.component.parent==null:!1,s=` at <${re(e.component,e.type,o)}`,r=">"+n;return e.props?[s,...Re(e.props),r]:[s+r]}function Re(e){const t=[],n=Object.keys(e);return n.slice(0,3).forEach(o=>{t.push(...Z(o,e[o]))}),n.length>3&&t.push(" ..."),t}function Z(e,t,n){return Me(t)?(t=JSON.stringify(t),n?t:[`${e}=${t}`]):typeof t=="number"||typeof t=="boolean"||t==null?n?t:[`${e}=${t}`]:Te(t)?(t=Z(e,P(t.value),!0),n?t:[`${e}=Ref<`,t,">"]):D(t)?[`${e}=fn${t.name?`<${t.name}>`:""}`]:(t=P(t),n?t:[`${e}=`,t])}const vt={sp:"serverPrefetch hook",bc:"beforeCreate hook",c:"created hook",bm:"beforeMount hook",m:"mounted hook",bu:"beforeUpdate hook",u:"updated",bum:"beforeUnmount hook",um:"unmounted hook",a:"activated hook",da:"deactivated hook",ec:"errorCaptured hook",rtc:"renderTracked hook",rtg:"renderTriggered hook",0:"setup function",1:"render function",2:"watcher getter",3:"watcher callback",4:"watcher cleanup function",5:"native event handler",6:"component event handler",7:"vnode hook",8:"directive hook",9:"transition hook",10:"app errorHandler",11:"app warnHandler",12:"ref function",13:"async component loader",14:"scheduler flush",15:"component update",16:"app unmount cleanup function"};function A(e,t,n,o){try{return o?e(...o):e()}catch(s){ee(s,t,n)}}function ee(e,t,n,o=!0){const s=t?t.vnode:null,{errorHandler:r,throwUnhandledErrorInProduction:i}=t&&t.appContext.config||Se;if(t){let c=t.parent;const l=t.proxy,u=`https://vuejs.org/error-reference/#runtime-${n}`;for(;c;){const a=c.ec;if(a){for(let h=0;h<a.length;h++)if(a[h](e,l,u)===!1)return}c=c.parent}if(r){A(r,null,10,[e,l,u]);return}}je(e,n,s,o,i)}function je(e,t,n,o=!0,s=!1){if(s)throw e;console.error(e)}const b=[];let x=-1;const S=[];let k=null,z=0;const De=Promise.resolve();let q=null;const Le=100;function Pe(e){let t=x+1,n=b.length;for(;t<n;){const o=t+n>>>1,s=b[o],r=M(s);r<e||r===e&&s.flags&2?t=o+1:n=o}return t}function We(e){if(!(e.flags&1)){const t=M(e),n=b[b.length-1];!n||!(e.flags&2)&&t>=M(n)?b.push(e):b.splice(Pe(t),0,e),e.flags|=1,te()}}function te(){q||(q=De.then(ne))}function Ae(e){Oe(e)?S.push(...e):k&&e.id===-1?k.splice(z+1,0,e):e.flags&1||(S.push(e),e.flags|=1),te()}function qe(e){if(S.length){const t=[...new Set(S)].sort((n,o)=>M(n)-M(o));if(S.length=0,k){k.push(...t);return}for(k=t,z=0;z<k.length;z++){const n=k[z];n.flags&4&&(n.flags&=-2),n.flags&8||n(),n.flags&=-2}k=null,z=0}}const M=e=>e.id==null?e.flags&2?-1:1/0:e.id;function ne(e){const t=ze;try{for(x=0;x<b.length;x++){const n=b[x];n&&!(n.flags&8)&&(n.flags&4&&(n.flags&=-2),A(n,n.i,n.i?15:14),n.flags&4||(n.flags&=-2))}}finally{for(;x<b.length;x++){const n=b[x];n&&(n.flags&=-2)}x=-1,b.length=0,qe(e),q=null,(b.length||S.length)&&ne(e)}}function St(e,t){const n=e.get(t)||0;if(n>Le){const o=t.i,s=o&&se(o.type);return ee(`Maximum recursive updates exceeded${s?` in component <${s}>`:""}. This means you have a reactive effect that is mutating its own dependencies and thus recursively triggering itself. Possible sources include component template, render function, updated hook or watcher source function.`,null,10),!0}return e.set(t,n+1),!1}const H=new Map,F=new Map;function zt(e,t){return F.has(e)?!1:(F.set(e,{initialDef:$(t),instances:new Set}),!0)}function $(e){return Je(e)?e.__vccOpts:e}function Ct(e,t){const n=F.get(e);n&&(n.initialDef.render=t,[...n.instances].forEach(o=>{t&&(o.render=t,$(o.type).render=t),o.renderCache=[],o.update()}))}function Ot(e,t){const n=F.get(e);if(!n)return;t=$(t),oe(n.initialDef,t);const o=[...n.instances];for(let s=0;s<o.length;s++){const r=o[s],i=$(r.type);let c=H.get(i);c||(i!==n.initialDef&&oe(i,t),H.set(i,c=new Set)),c.add(r),r.appContext.propsCache.delete(r.type),r.appContext.emitsCache.delete(r.type),r.appContext.optionsCache.delete(r.type),r.ceReload?(c.add(r),r.ceReload(t.styles),c.delete(r)):r.parent?We(()=>{r.parent.update(),c.delete(r)}):r.appContext.reload?r.appContext.reload():typeof window<"u"?window.location.reload():console.warn("[HMR] Root or manually mounted instance modified. Full reload required."),r.root.ce&&r!==r.root&&r.root.ce._removeChildStyle(i)}Ae(()=>{H.clear()})}function oe(e,t){Ce(e,t);for(const n in e)n!=="__file"&&!(n in t)&&delete e[n]}function Mt(e){return(t,n)=>{try{return e(t,n)}catch(o){console.error(o),console.warn("[HMR] Something went wrong during Vue component hot-reload. Full reload required.")}}}L().requestIdleCallback,L().cancelIdleCallback;const Nt={};{const e=L(),t=(n,o)=>{let s;return(s=e[n])||(s=e[n]=[]),s.push(o),r=>{s.length>1?s.forEach(i=>i(r)):s[0](r)}};t("__VUE_INSTANCE_SETTERS__",n=>n),t("__VUE_SSR_SETTERS__",n=>n)}const He=/(?:^|[-_])(\w)/g,Ue=e=>e.replace(He,t=>t.toUpperCase()).replace(/[-_]/g,"");function se(e,t=!0){return D(e)?e.displayName||e.name:e.name||t&&e.__name}function re(e,t,n=!1){let o=se(t);if(!o&&t.__file){const s=t.__file.match(/([^/\\]+)\.\w+$/);s&&(o=s[1])}if(!o&&e&&e.parent){const s=r=>{for(const i in r)if(r[i]===t)return i};o=s(e.components||e.parent.type.components)||s(e.appContext.components)}return o?Ue(o):n?"App":"Anonymous"}function Je(e){return D(e)&&"__vccOpts"in e}[...new Array(6)].map((e,t)=>`[vp-content] h${t+1}`).join(",");const{entries:Ge}=Object,{fromEntries:Be}=Object,Ye="ENTRIES",ie="KEYS",ce="VALUES",y="";class U{set;_type;_path;constructor(t,n){const o=t._tree,s=Array.from(o.keys());this.set=t,this._type=n,this._path=s.length>0?[{node:o,keys:s}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:n}=C(this._path);if(C(n)===y)return{done:!1,value:this.result()};const o=t.get(C(n));return this._path.push({node:o,keys:Array.from(o.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=C(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>C(t)).filter(t=>t!==y).join("")}value(){return C(this._path).node.get(y)}result(){switch(this._type){case ce:return this.value();case ie:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const C=e=>e[e.length-1],Ke=(e,t,n)=>{const o=new Map;if(typeof t!="string")return o;const s=t.length+1,r=s+n,i=new Uint8Array(r*s).fill(n+1);for(let c=0;c<s;++c)i[c]=c;for(let c=1;c<r;++c)i[c*s]=c;return le(e,t,n,o,i,1,s,""),o},le=(e,t,n,o,s,r,i,c)=>{const l=r*i;e:for(const u of e.keys())if(u===y){const a=s[l-1];a<=n&&o.set(c,[e.get(u),a])}else{let a=r;for(let h=0;h<u.length;++h,++a){const g=u[h],m=i*a,w=m-i;let d=s[m];const f=Math.max(0,a-n-1),p=Math.min(i-1,a+n);for(let _=f;_<p;++_){const I=g!==t[_],j=s[w+_]+ +I,T=s[w+_+1]+1,E=s[m+_]+1,O=s[m+_+1]=Math.min(j,T,E);O<d&&(d=O)}if(d>n)continue e}le(e.get(u),t,n,o,s,a,i,c+u)}};let ue=class N{_tree;_prefix;_size=void 0;constructor(t=new Map,n=""){this._tree=t,this._prefix=n}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[n,o]=V(this._tree,t.slice(this._prefix.length));if(n===void 0){const[s,r]=B(o);for(const i of s.keys())if(i!==y&&i.startsWith(r)){const c=new Map;return c.set(i.slice(r.length),s.get(i)),new N(c,t)}}return new N(n,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,Qe(this._tree,t)}entries(){return new U(this,Ye)}forEach(t){for(const[n,o]of this)t(n,o,this)}fuzzyGet(t,n){return Ke(this._tree,t,n)}get(t){const n=J(this._tree,t);return n!==void 0?n.get(y):void 0}has(t){return J(this._tree,t)?.has(y)??!1}keys(){return new U(this,ie)}set(t,n){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,G(this._tree,t).set(y,n),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,n){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const o=G(this._tree,t);return o.set(y,n(o.get(y))),this}fetch(t,n){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const o=G(this._tree,t);let s=o.get(y);return s===void 0&&o.set(y,s=n()),s}values(){return new U(this,ce)}[Symbol.iterator](){return this.entries()}static from(t){const n=new N;for(const[o,s]of t)n.set(o,s);return n}static fromObject(t){return N.from(Object.entries(t))}};const V=(e,t,n=[])=>{if(t.length===0||e==null)return[e,n];for(const o of e.keys())if(o!==y&&t.startsWith(o))return n.push([e,o]),V(e.get(o),t.slice(o.length),n);return n.push([e,t]),V(void 0,"",n)},J=(e,t)=>{if(t.length===0||!e)return e;for(const n of e.keys())if(n!==y&&t.startsWith(n))return J(e.get(n),t.slice(n.length))},G=(e,t)=>{const n=t.length;e:for(let o=0;e&&o<n;){for(const r of e.keys())if(r!==y&&t[o]===r[0]){const i=Math.min(n-o,r.length);let c=1;for(;c<i&&t[o+c]===r[c];)++c;const l=e.get(r);if(c===r.length)e=l;else{const u=new Map;u.set(r.slice(c),l),e.set(t.slice(o,o+c),u),e.delete(r),e=u}o+=c;continue e}const s=new Map;return e.set(t.slice(o),s),s}return e},Qe=(e,t)=>{const[n,o]=V(e,t);if(n!==void 0){if(n.delete(y),n.size===0)ae(o);else if(n.size===1){const[s,r]=n.entries().next().value;fe(o,s,r)}}},ae=e=>{if(e.length===0)return;const[t,n]=B(e);if(t.delete(n),t.size===0)ae(e.slice(0,-1));else if(t.size===1){const[o,s]=t.entries().next().value;o!==y&&fe(e.slice(0,-1),o,s)}},fe=(e,t,n)=>{if(e.length===0)return;const[o,s]=B(e);o.set(s+t,n),o.delete(s)},B=e=>e[e.length-1],Xe=(e,t)=>{const n=e._idToShortId.get(t);if(n!=null)return e._storedFields.get(n)},Ze=/[\n\r\p{Z}\p{P}]+/u,Y="or",de="and",et="and_not",tt=(e,t)=>{e.includes(t)||e.push(t)},he=(e,t)=>{for(const n of t)e.includes(n)||e.push(n)},pe=({score:e},{score:t})=>t-e,nt=()=>new Map,R=e=>{const t=new Map;for(const n of Object.keys(e))t.set(parseInt(n,10),e[n]);return t},ge=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,me={[Y]:(e,t)=>{for(const n of t.keys()){const o=e.get(n);if(o==null)e.set(n,t.get(n));else{const{score:s,terms:r,match:i}=t.get(n);o.score=o.score+s,o.match=Object.assign(o.match,i),he(o.terms,r)}}return e},[de]:(e,t)=>{const n=new Map;for(const o of t.keys()){const s=e.get(o);if(s==null)continue;const{score:r,terms:i,match:c}=t.get(o);he(s.terms,i),n.set(o,{score:s.score+r,terms:s.terms,match:Object.assign(s.match,c)})}return n},[et]:(e,t)=>{for(const n of t.keys())e.delete(n);return e}},ot=(e,t,n,o,s,r)=>{const{k:i,b:c,d:l}=r;return Math.log(1+(n-t+.5)/(t+.5))*(l+e*(i+1)/(e+i*(1-c+c*o/s)))},st=e=>(t,n,o)=>({term:t,fuzzy:typeof e.fuzzy=="function"?e.fuzzy(t,n,o):e.fuzzy??!1,prefix:typeof e.prefix=="function"?e.prefix(t,n,o):e.prefix===!0,termBoost:typeof e.boostTerm=="function"?e.boostTerm(t,n,o):1}),_e=(e,t,n,o)=>{for(const s of Object.keys(e._fieldIds))if(e._fieldIds[s]===n){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${o}" was not present in field "${s}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},rt=(e,t,n,o)=>{if(!e._index.has(o)){_e(e,n,t,o);return}const s=e._index.fetch(o,nt),r=s.get(t),i=r?.get(n);!r||typeof i>"u"?_e(e,n,t,o):i<=1?r.size<=1?s.delete(t):r.delete(n):r.set(n,i-1),e._index.get(o).size===0&&e._index.delete(o)},it={k:1.2,b:.7,d:.5},ct={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(Ze),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{console?.[e]?.(t)},autoVacuum:!0},ye={combineWith:Y,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:it},lt={combineWith:de,prefix:(e,t,n)=>t===n.length-1},ut={batchSize:1e3,batchWait:10},we={minDirtFactor:.1,minDirtCount:20},at={...ut,...we},be=Symbol("*"),ft=(e,t)=>{const n=new Map,o={...e._options.searchOptions,...t};for(const[s,r]of e._documentIds){const i=o.boostDocument?o.boostDocument(r,"",e._storedFields.get(s)):1;n.set(s,{score:i,terms:[],match:{}})}return n},xe=(e,t=Y)=>{if(e.length===0)return new Map;const n=t.toLowerCase();if(!(n in me))throw new Error(`Invalid combination operator: ${t}`);return e.reduce(me[n])},K=(e,t,n,o,s,r,i,c,l,u=new Map)=>{if(r==null)return u;for(const a of Object.keys(i)){const h=i[a],g=e._fieldIds[a],m=r.get(g);if(m==null)continue;let w=m.size;const d=e._avgFieldLength[g];for(const f of m.keys()){if(!e._documentIds.has(f)){rt(e,g,f,n),w-=1;continue}const p=c?c(e._documentIds.get(f),n,e._storedFields.get(f)):1;if(!p)continue;const _=m.get(f),I=e._fieldLength.get(f)[g],j=ot(_,w,e._documentCount,I,d,l),T=o*s*h*p*j,E=u.get(f);if(E){E.score+=T,tt(E.terms,t);const O=ge(E.match,n);O?O.push(a):E.match[n]=[a]}else u.set(f,{score:T,terms:[t],match:{[n]:[a]}})}}return u},dt=(e,t,n)=>{const o={...e._options.searchOptions,...n},s=(o.fields??e._options.fields).reduce((d,f)=>({...d,[f]:ge(o.boost,f)||1}),{}),{boostDocument:r,weights:i,maxFuzzy:c,bm25:l}=o,{fuzzy:u,prefix:a}={...ye.weights,...i},h=e._index.get(t.term),g=K(e,t.term,t.term,1,t.termBoost,h,s,r,l);let m,w;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const d=t.fuzzy===!0?.2:t.fuzzy,f=d<1?Math.min(c,Math.round(t.term.length*d)):d;f&&(w=e._index.fuzzyGet(t.term,f))}if(m)for(const[d,f]of m){const p=d.length-t.term.length;if(!p)continue;w?.delete(d);const _=a*d.length/(d.length+.3*p);K(e,t.term,d,_,t.termBoost,f,s,r,l,g)}if(w)for(const d of w.keys()){const[f,p]=w.get(d);if(!p)continue;const _=u*d.length/(d.length+p);K(e,t.term,d,_,t.termBoost,f,s,r,l,g)}return g},ke=(e,t,n={})=>{if(t===be)return ft(e,n);if(typeof t!="string"){const a={...n,...t,queries:void 0},h=t.queries.map(g=>ke(e,g,a));return xe(h,a.combineWith)}const{tokenize:o,processTerm:s,searchOptions:r}=e._options,i={tokenize:o,processTerm:s,...r,...n},{tokenize:c,processTerm:l}=i,u=c(t).flatMap(a=>l(a)).filter(a=>!!a).map(st(i)).map(a=>dt(e,a,i));return xe(u,i.combineWith)},Ie=(e,t,n={})=>{const{searchOptions:o}=e._options,s={...o,...n},r=ke(e,t,n),i=[];for(const[c,{score:l,terms:u,match:a}]of r){const h=u.length||1,g={id:e._documentIds.get(c),score:l*h,terms:Object.keys(a),queryTerms:u,match:a};Object.assign(g,e._storedFields.get(c)),(s.filter==null||s.filter(g))&&i.push(g)}return t===be&&s.boostDocument==null||i.sort(pe),i},ht=(e,t,n={})=>{n={...e._options.autoSuggestOptions,...n};const o=new Map;for(const{score:r,terms:i}of Ie(e,t,n)){const c=i.join(" "),l=o.get(c);l!=null?(l.score+=r,l.count+=1):o.set(c,{score:r,terms:i,count:1})}const s=[];for(const[r,{score:i,terms:c,count:l}]of o)s.push({suggestion:r,terms:c,score:i/l});return s.sort(pe),s};class pt{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(!t?.fields)throw new Error('SlimSearch: option "fields" must be provided');const n=t.autoVacuum==null||t.autoVacuum===!0?at:t.autoVacuum;this._options={...ct,...t,autoVacuum:n,searchOptions:{...ye,...t.searchOptions},autoSuggestOptions:{...lt,...t.autoSuggestOptions}},this._index=new ue,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=we,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[n,o]of this._index){const s={};for(const[r,i]of o)s[r]=Object.fromEntries(i);t.push([n,s])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,version:2}}addFields(t){for(let n=0;n<t.length;n++)this._fieldIds[t[n]]=n}}const gt=e=>new pt(e),mt=({documentCount:e,nextId:t,fieldIds:n,averageFieldLength:o,dirtCount:s,version:r},i)=>{if(r!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const c=gt(i);return c._documentCount=e,c._nextId=t,c._idToShortId=new Map,c._fieldIds=n,c._avgFieldLength=o,c._dirtCount=s??0,c._index=new ue,c},_t=(e,t)=>{const{index:n,documentIds:o,fieldLength:s,storedFields:r}=e,i=mt(e,t);i._documentIds=R(o),i._fieldLength=R(s),i._storedFields=R(r);for(const[c,l]of i._documentIds)i._idToShortId.set(l,c);for(const[c,l]of n){const u=new Map;for(const a of Object.keys(l))u.set(parseInt(a,10),R(l[a]));i._index.set(c,u)}return i},Q=(e,t)=>{const n=e.toLowerCase(),o=t.toLowerCase(),s=[];let r=0,i=0;const c=(u,a=!1)=>{let h;i===0?h=u.length>20?`… ${u.slice(-20)}`:u:a?h=u.length+i>100?`${u.slice(0,100-i)}… `:u:h=u.length>20?`${u.slice(0,20)} … ${u.slice(-20)}`:u,h&&s.push(h),i+=h.length,a||(s.push(["mark",t]),i+=t.length,i>=100&&s.push(" …"))};let l=n.indexOf(o,r);if(l===-1)return null;for(;l>=0;){const u=l+o.length;if(c(e.slice(r,l)),r=u,i>100)break;l=n.indexOf(o,r)}return i<100&&c(e.slice(r),!0),s},{entries:yt}=Object,wt=(e,t)=>t.contents.reduce((n,[,o])=>n+o,0)-e.contents.reduce((n,[,o])=>n+o,0),bt=(e,t)=>Math.max(...t.contents.map(([,n])=>n))-Math.max(...e.contents.map(([,n])=>n)),Ee=(e,t,n={},o="max")=>{const s={};return Ie(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...n}).forEach(r=>{const{id:i,terms:c,score:l}=r,u=i.includes("@"),a=i.includes("#"),[h,g]=i.split(/[#@]/),m=Number(h),w=c.sort((f,p)=>f.length-p.length).filter((f,p)=>c.slice(p+1).every(_=>!_.includes(f))),{contents:d}=s[m]??={title:"",contents:[]};if(u)d.push([{type:"customField",id:m,index:g,display:w.map(f=>r.c.map(p=>Q(p,f))).flat().filter(f=>f!==null)},l]);else{const f=w.map(p=>Q(r.h,p)).filter(p=>p!==null);if(f.length&&d.push([{type:a?"heading":"title",id:m,...a&&{anchor:g},display:f},l]),"t"in r&&r.t)for(const p of r.t){const _=w.map(I=>Q(p,I)).filter(I=>I!==null);_.length&&d.push([{type:"text",id:m,...a&&{anchor:g},display:_},l])}}}),yt(s).sort(([,r],[,i])=>(o?wt:bt)(r,i)).map(([r,{title:i,contents:c}])=>{if(!i){const l=Xe(t,r);l&&(i=l.h)}return{title:i,contents:c.map(([l])=>l)}})},ve=(e,t,n={})=>{const o=ht(t,e,{fuzzy:.2,maxFuzzy:3,...n}).map(({suggestion:s})=>s);return e.includes(" ")?o:o.filter(s=>!s.includes(" "))},xt=Be(Ge(JSON.parse("{\"/\":{\"documentCount\":296,\"nextId\":296,\"documentIds\":{\"0\":\"0\",\"1\":\"1\",\"2\":\"1#binary-oracle\",\"3\":\"1#elowen\",\"4\":\"2\",\"5\":\"2#背景\",\"6\":\"2#数据集\",\"7\":\"3\",\"8\":\"3#环境配置-待完善\",\"9\":\"3#模型结构\",\"10\":\"3#lmaffordance3d\",\"11\":\"3#step-2-融合多模态空间特征\",\"12\":\"3#step-3-多模态特征投影到语言语义空间\",\"13\":\"3#step-6-拼接多模态嵌入与语言嵌入\",\"14\":\"3#step-8-降维适配器\",\"15\":\"3#step-9-解码器融合所有特征以预测可操作性特征\",\"16\":\"3#step-10-使用分割头预测最终的-3d-可操作性热图\",\"17\":\"4\",\"18\":\"4#数据集\",\"19\":\"4#_1-基础数据来源\",\"20\":\"4#_2-构建问题-question-crafting\",\"21\":\"4#_3-标注-gt-mask-ground-truth-mask\",\"22\":\"4#_4-数据集组织方式\",\"23\":\"4#_5-数据增强与配对策略\",\"24\":\"4#_6-数据集统计信息-来自论文图3\",\"25\":\"4#_7-代码实现\",\"26\":\"4#_8-总结\",\"27\":\"4#模型实现\",\"28\":\"4#afm-自适应融合模块\",\"29\":\"4#_1️⃣-grouping-文本引导的点特征分组\",\"30\":\"4#_2️⃣-mixing-mlp-mixer-进行组内和通道间的信息混合\",\"31\":\"4#_3️⃣-ungrouping-将融合特征映射回点空间\",\"32\":\"4#_4️⃣-afm-自适应融合模块\",\"33\":\"4#rpo-参考点解码器\",\"34\":\"4#损失函数\",\"35\":\"4#hm-loss-hybrid-mask-loss\",\"36\":\"4#训练\",\"37\":\"4#准备\",\"38\":\"4#训练-1\",\"39\":\"4#评估\",\"40\":\"4#复现\",\"41\":\"5\",\"42\":\"6\",\"43\":\"6#背景\",\"44\":\"6#模型结构\",\"45\":\"6#层次化点集特征学习\",\"46\":\"6#sampling-layer\",\"47\":\"6#grouping-layer\",\"48\":\"6#pointnet-layer\",\"49\":\"6#代码实现\",\"50\":\"6#单尺度分组分类模型\",\"51\":\"6#非均匀密度下稳定的特征学习\",\"52\":\"6#多尺度分组-multi-scale-grouping\",\"53\":\"6#多尺度分组分类模型\",\"54\":\"6#多分辨率分组-multi-resolution-grouping\",\"55\":\"6#点云语义分割\",\"56\":\"6#代码实现-1\",\"57\":\"6#特征传播层\",\"58\":\"6#点云语义分割模型\",\"59\":\"7\",\"60\":\"7#核心\",\"61\":\"7#难点\",\"62\":\"7#解决方案\",\"63\":\"7#代码-pytorch版本\",\"64\":\"7#输入标准化\",\"65\":\"7#正则化损失\",\"66\":\"7#特征提取\",\"67\":\"7#分类任务\",\"68\":\"7#分割任务\",\"69\":\"7#缺陷\",\"70\":\"7#背景知识扫盲-可选\",\"71\":\"7#点云\",\"72\":\"7#对称函数\",\"73\":\"7#刚性运动\",\"74\":\"7#正交变换\",\"75\":\"8\",\"76\":\"9\",\"77\":\"10\",\"78\":\"10#背景\",\"79\":\"10#方法\",\"80\":\"10#预训练\",\"81\":\"10#微调\",\"82\":\"10#联合-gpt-4-的推理机制-ensemble-with-gpt-4\",\"83\":\"10#ablation-study-消融实验\",\"84\":\"10#补充\",\"85\":\"10#辨析-instruction-tuning-和-prompt-tuning\",\"86\":\"11\",\"87\":\"12\",\"88\":\"12#引言\",\"89\":\"12#介绍\",\"90\":\"12#训练\",\"91\":\"12#推理\",\"92\":\"12#文本描述生成\",\"93\":\"12#花卉图片分类\",\"94\":\"12#文字搜索图像\",\"95\":\"12#完整代码\",\"96\":\"12#小结\",\"97\":\"13\",\"98\":\"13#背景\",\"99\":\"13#模型结构\",\"100\":\"13#stage-1-representation-learning-表征学习\",\"101\":\"13#_1、image-text-contrastive-learning-itc-loss-clip-like\",\"102\":\"13#_2、image-text-matching-itm-loss-二分类task\",\"103\":\"13#_3、image-grounded-text-generation-itg-loss-gpt-like\",\"104\":\"13#stage-2-generative-learning-生成学习\",\"105\":\"14\",\"106\":\"14#原理\",\"107\":\"14#_0-数据下载\",\"108\":\"14#_1-图片预处理\",\"109\":\"14#_2-图片切割\",\"110\":\"14#_3-添加-class-token\",\"111\":\"14#_4-添加位置编码\",\"112\":\"14#_5-encoder\",\"113\":\"14#_6-多头自注意力\",\"114\":\"14#_7-mlp-head\",\"115\":\"14#效果对比\",\"116\":\"14#注意力可视化\",\"117\":\"14#混合模型探索\",\"118\":\"14#加载预训练模型\",\"119\":\"14#总结\",\"120\":\"15\",\"121\":\"16\",\"122\":\"17\",\"123\":\"17#一、注意力机制的基本流程\",\"124\":\"17#二、q、k、v-的初始维度对结果的影响\",\"125\":\"17#_1-q-×-k-t-的维度\",\"126\":\"17#_2-softmax-操作\",\"127\":\"17#_3-与-v-相乘\",\"128\":\"17#三、总结-输入维度-→-输出维度\",\"129\":\"17#四、如何理解这个过程\",\"130\":\"17#✅-1-信息融合机制\",\"131\":\"17#✅-2-维度设计的灵活性\",\"132\":\"17#✅-3-可类比为-软检索-系统\",\"133\":\"17#五、例子说明-以-transformer-为例\",\"134\":\"17#六、常见疑问解答\",\"135\":\"17#❓q-为什么和可以不同\",\"136\":\"17#❓q-为什么要除以\",\"137\":\"17#七、可视化示意\",\"138\":\"18\",\"139\":\"19\",\"140\":\"19#一、创建新环境\",\"141\":\"19#二、激活-切换-环境\",\"142\":\"19#三、退出当前环境\",\"143\":\"19#四、查看所有已创建的环境\",\"144\":\"19#五、删除已创建的环境\",\"145\":\"19#六、查看当前激活的环境\",\"146\":\"19#七、查看当前环境已安装的包\",\"147\":\"19#八、在当前环境下安装包\",\"148\":\"19#九、常见错误\",\"149\":\"20\",\"150\":\"20#二元分类场景\",\"151\":\"20#混淆矩阵-confusion-matrix\",\"152\":\"20#准确率-accuracy\",\"153\":\"20#召回率-recall-真正例率\",\"154\":\"20#误报概率-假正例率\",\"155\":\"20#精确率\",\"156\":\"20#指标的选择和权衡\",\"157\":\"20#f1-得分\",\"158\":\"20#roc-曲线和-auc\",\"159\":\"20#roc-receiver-operating-characteristic\",\"160\":\"20#auc-曲线下面积\",\"161\":\"20#精确率与召回率曲线\",\"162\":\"20#用于选择模型和阈值的-auc-和-roc\",\"163\":\"21\",\"164\":\"21#语义分割\",\"165\":\"21#损失函数\",\"166\":\"21#dice-loss\",\"167\":\"21#bce-dice-loss\",\"168\":\"21#jaccard-intersection-over-union-iou-loss\",\"169\":\"21#focal-loss\",\"170\":\"21#tversky-loss\",\"171\":\"21#lovasz-hinge-loss\",\"172\":\"21#combo-loss\",\"173\":\"21#如何选择\",\"174\":\"22\",\"175\":\"22#预训练过程\",\"176\":\"22#分词过程\",\"177\":\"23\",\"178\":\"23#什么是大模型\",\"179\":\"23#为什么要对大模型进行微调\",\"180\":\"23#如何对大模型进行微调\",\"181\":\"23#常用的peft方案\",\"182\":\"23#prompt-tuning\",\"183\":\"23#prefix-tuning\",\"184\":\"23#lora\",\"185\":\"23#qlora\",\"186\":\"24\",\"187\":\"24#符合认知的大模型微调流程\",\"188\":\"24#大模型微调大致发展历史\",\"189\":\"24#lora-微调\",\"190\":\"24#矩阵a和b为什么不能同时为零\",\"191\":\"24#秩的选择\",\"192\":\"24#注意\",\"193\":\"25\",\"194\":\"25#什么是prompt-engineering\",\"195\":\"25#如何写好prompt\",\"196\":\"25#要明确-要具体\",\"197\":\"25#给llm更多的时间去思考\",\"198\":\"25#思维链技术-chain-of-thought\",\"199\":\"25#自一致性技术-self-consistency\",\"200\":\"25#从易至难技术-least-to-most\",\"201\":\"26\",\"202\":\"27\",\"203\":\"27#摘要\",\"204\":\"27#简介\",\"205\":\"27#相关工作\",\"206\":\"27#框架\",\"207\":\"27#无监督预训练\",\"208\":\"27#有监督微调\",\"209\":\"27#特定任务输入转换\",\"210\":\"27#实验\",\"211\":\"27#设置\",\"212\":\"27#监督微调\",\"213\":\"27#分析\",\"214\":\"27#结论\",\"215\":\"28\",\"216\":\"29\",\"217\":\"29#bert-是什么\",\"218\":\"29#masked-language-model\",\"219\":\"29#next-sentence-prediction\",\"220\":\"29#multi-task-learning\",\"221\":\"29#fine-tuning\",\"222\":\"29#从-零-开始的预训练\",\"223\":\"29#数据清洗\",\"224\":\"29#分词器实现\",\"225\":\"29#batch数据准备\",\"226\":\"29#模型\",\"227\":\"29#训练\",\"228\":\"29#效果\",\"229\":\"29#details\",\"230\":\"29#padding-mask-如何生成并起作用的\",\"231\":\"30\",\"232\":\"30#环境搭建\",\"233\":\"30#数据预处理\",\"234\":\"30#模型架构\",\"235\":\"30#dataloader\",\"236\":\"30#bertembeddings\",\"237\":\"30#bertencoder\",\"238\":\"30#bertlayer\",\"239\":\"30#bertencoder-1\",\"240\":\"30#bertpooler\",\"241\":\"30#bertmodel\",\"242\":\"30#bertforsequenceclassification\",\"243\":\"30#bertattention\",\"244\":\"30#bertselfattention\",\"245\":\"30#bertselfoutput\",\"246\":\"30#bertattention-1\",\"247\":\"30#预训练\",\"248\":\"30#bertpredictionheadtransform\",\"249\":\"30#bertlmpredictionhead\",\"250\":\"30#bertpretrainingheads\",\"251\":\"30#bertforpretraining\",\"252\":\"30#其他下游任务\",\"253\":\"30#问答任务\",\"254\":\"30#代码实现\",\"255\":\"30#易混淆\",\"256\":\"30#token分类任务\",\"257\":\"30#多项选择任务\",\"258\":\"31\",\"259\":\"31#环境\",\"260\":\"31#背景\",\"261\":\"31#模型架构\",\"262\":\"31#encoder-decoder-结构\",\"263\":\"31#generator\",\"264\":\"31#encoder-结构\",\"265\":\"31#sublayerconnection\",\"266\":\"31#encoderlayer\",\"267\":\"31#encoder\",\"268\":\"31#decoder-结构\",\"269\":\"31#decoderlayer\",\"270\":\"31#decoder\",\"271\":\"31#多头自注意力\",\"272\":\"32\",\"273\":\"33\",\"274\":\"34\",\"275\":\"35\",\"276\":\"36\",\"277\":\"36#大语言模型\",\"278\":\"36#常见的llm\",\"279\":\"36#llm-的特点与能力\",\"280\":\"36#涌现能力-emergent-abilities\",\"281\":\"36#作为基座模型支持多元应用的能力\",\"282\":\"36#支持对话作为统一入口的能力\",\"283\":\"36#检索增强生成-rag-retrieval-augmented-generation\",\"284\":\"36#工作流程\",\"285\":\"36#rag-vs-finetune\",\"286\":\"36#langchain\",\"287\":\"36#核心组件\",\"288\":\"36#版本迭代\",\"289\":\"36#生态圈\",\"290\":\"36#大模型开发\",\"291\":\"36#基本流程\",\"292\":\"36#参考\",\"293\":\"37\",\"294\":\"38\",\"295\":\"39\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1,7],\"1\":[1],\"2\":[2,16],\"3\":[1,5],\"4\":[2,35],\"5\":[1],\"6\":[1],\"7\":[2,29],\"8\":[3,4],\"9\":[1,1],\"10\":[1,223],\"11\":[3,92],\"12\":[3,25],\"13\":[3,114],\"14\":[3,20],\"15\":[3,142],\"16\":[5,153],\"17\":[2,31],\"18\":[1],\"19\":[2,25],\"20\":[5,72],\"21\":[7,26],\"22\":[2,54],\"23\":[2,14],\"24\":[4,39],\"25\":[2,211],\"26\":[2,20],\"27\":[1,154],\"28\":[2,40],\"29\":[3,78],\"30\":[5,118],\"31\":[3,61],\"32\":[3,96],\"33\":[2,224],\"34\":[1],\"35\":[5,174],\"36\":[1,6],\"37\":[1,105],\"38\":[1,61],\"39\":[1,338],\"40\":[1,310],\"41\":[3],\"42\":[1,19],\"43\":[1,73],\"44\":[1,15],\"45\":[1,24],\"46\":[2,23],\"47\":[2,51],\"48\":[2,15],\"49\":[1,316],\"50\":[1,122],\"51\":[1,17],\"52\":[5,35],\"53\":[1,196],\"54\":[5,40],\"55\":[1,151],\"56\":[1,42],\"57\":[1,222],\"58\":[1,140],\"59\":[1,17],\"60\":[1,33],\"61\":[1,28],\"62\":[1,121],\"63\":[3,1],\"64\":[1,192],\"65\":[1,73],\"66\":[1,112],\"67\":[1,54],\"68\":[1,94],\"69\":[1,290],\"70\":[3],\"71\":[1,65],\"72\":[1,83],\"73\":[1,24],\"74\":[1,13],\"75\":[1],\"76\":[6,27],\"77\":[9,21],\"78\":[1,130],\"79\":[1,21],\"80\":[1,126],\"81\":[1,127],\"82\":[7,40],\"83\":[4,28],\"84\":[1],\"85\":[5,107],\"86\":[1],\"87\":[1,3],\"88\":[1,31],\"89\":[1,20],\"90\":[1,146],\"91\":[1,156],\"92\":[1,57],\"93\":[1,245],\"94\":[1,92],\"95\":[1,210],\"96\":[1,132],\"97\":[1,17],\"98\":[1,66],\"99\":[1,20],\"100\":[6,144],\"101\":[10,95],\"102\":[8,183],\"103\":[10,349],\"104\":[6,194],\"105\":[1,53],\"106\":[1,4],\"107\":[2,234],\"108\":[2,130],\"109\":[2,181],\"110\":[4,160],\"111\":[2,141],\"112\":[2,153],\"113\":[2,138],\"114\":[3,220],\"115\":[1,61],\"116\":[1,13],\"117\":[1,51],\"118\":[1,158],\"119\":[1,15],\"120\":[1],\"121\":[1],\"122\":[1,16],\"123\":[2,19],\"124\":[5],\"125\":[2,11],\"126\":[2,6],\"127\":[2,13],\"128\":[5,24],\"129\":[3,2],\"130\":[3,12],\"131\":[3,9],\"132\":[3,10],\"133\":[6,18],\"134\":[2],\"135\":[5,5],\"136\":[3,8],\"137\":[2,14],\"138\":[1],\"139\":[1,1],\"140\":[2,25],\"141\":[4,13],\"142\":[2,6],\"143\":[2,20],\"144\":[2,12],\"145\":[2,3],\"146\":[2,3],\"147\":[2,63],\"148\":[2,13],\"149\":[1,1],\"150\":[1],\"151\":[4,37],\"152\":[3,35],\"153\":[3,25],\"154\":[2,23],\"155\":[1,29],\"156\":[1,30],\"157\":[2,20],\"158\":[3,6],\"159\":[5,29],\"160\":[3,34],\"161\":[1,20],\"162\":[4,49],\"163\":[1,1],\"164\":[1,22],\"165\":[1],\"166\":[2,144],\"167\":[3,150],\"168\":[6,159],\"169\":[2,321],\"170\":[2,158],\"171\":[3,14],\"172\":[2,190],\"173\":[2,22],\"174\":[1,29],\"175\":[1,175],\"176\":[1,94],\"177\":[4,4],\"178\":[2,40],\"179\":[2,47],\"180\":[2,67],\"181\":[1,4],\"182\":[2,31],\"183\":[2,31],\"184\":[1,54],\"185\":[1,66],\"186\":[1,10],\"187\":[1,59],\"188\":[1,63],\"189\":[2,92],\"190\":[2,34],\"191\":[1,15],\"192\":[1,62],\"193\":[3,3],\"194\":[3,33],\"195\":[2],\"196\":[2,21],\"197\":[1,44],\"198\":[4,51],\"199\":[3,43],\"200\":[4,46],\"201\":[1],\"202\":[3,14],\"203\":[1,35],\"204\":[1,83],\"205\":[1,16],\"206\":[1,4],\"207\":[1,24],\"208\":[1,30],\"209\":[1,52],\"210\":[1],\"211\":[1,113],\"212\":[1,100],\"213\":[1,66],\"214\":[1,14],\"215\":[1],\"216\":[4,9],\"217\":[3,50],\"218\":[3,64],\"219\":[3,42],\"220\":[3,7],\"221\":[2,129],\"222\":[3,6],\"223\":[1,150],\"224\":[1,181],\"225\":[1,116],\"226\":[1,176],\"227\":[1,177],\"228\":[1,23],\"229\":[1,2],\"230\":[4,77],\"231\":[2,2],\"232\":[1,138],\"233\":[1,165],\"234\":[1],\"235\":[1,48],\"236\":[1,66],\"237\":[1],\"238\":[1,58],\"239\":[1,30],\"240\":[1,40],\"241\":[1,49],\"242\":[1,73],\"243\":[1],\"244\":[1,104],\"245\":[1,32],\"246\":[1,23],\"247\":[1,1],\"248\":[1,41],\"249\":[1,44],\"250\":[1,28],\"251\":[1,75],\"252\":[1,1],\"253\":[1,120],\"254\":[1,95],\"255\":[1,173],\"256\":[1,75],\"257\":[1,119],\"258\":[1,2],\"259\":[1,41],\"260\":[1,23],\"261\":[1,64],\"262\":[3,30],\"263\":[1,28],\"264\":[2],\"265\":[1,33],\"266\":[1,37],\"267\":[1,37],\"268\":[2],\"269\":[1,49],\"270\":[1,39],\"271\":[1,116],\"272\":[2,3],\"273\":[2],\"274\":[2],\"275\":[2],\"276\":[2,3],\"277\":[1,133],\"278\":[1,594],\"279\":[2,59],\"280\":[1,55],\"281\":[1,32],\"282\":[1,66],\"283\":[6,72],\"284\":[1,19],\"285\":[3,51],\"286\":[1,50],\"287\":[1,45],\"288\":[1,100],\"289\":[1,52],\"290\":[1,78],\"291\":[1,114],\"292\":[1,28],\"293\":[2,4],\"294\":[1],\"295\":[1,3]},\"averageFieldLength\":[1.9560810810810834,70.53256667845336],\"storedFields\":{\"0\":{\"h\":\"主页\",\"t\":[\"知识星球: MetaMind , 小红书: BinaryOracle , CSDN: Binary Oracle\"]},\"1\":{\"h\":\"关于我们\"},\"2\":{\"h\":\"Binary Oracle\",\"t\":[\"一名普通但十分热爱探索技术的Coder\",\"开源框架 Spring committer\",\"Golang 开源网络库 netpoll committer\",\"Javaer 转型 3D - VL 方向研究\",\"现就读于四川大学\",\"有问题需要咨询的小伙伴，可以加微信备注来意:\"]},\"3\":{\"h\":\"Elowen\",\"t\":[\"CV 转 LLM 领域\",\"现就读于电子科技大学\"]},\"4\":{\"h\":\"GREAT 模型代码解读\",\"t\":[\"GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding 论文代码解读\",\"论文: https://arxiv.org/abs/2411.19626 代码: https://github.com/yawen-shao/GREAT_code 数据集: https://drive.google.com/drive/folders/1n_L_mSmVpAM-1ASoW2T2MltYkaiA_X9X\"]},\"5\":{\"h\":\"背景\"},\"6\":{\"h\":\"数据集\"},\"7\":{\"h\":\"LMAffordance3D 模型代码解读与复现\",\"t\":[\"Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions 论文代码解读与复现\",\"论文: https://arxiv.org/abs/2504.04744 代码: https://github.com/cn-hezhu/LMAffordance3D\",\"由于论文数据集还未开源，加之原本在Github上开源的代码后续被下架，导致本论文复现流程暂时终止。\"]},\"8\":{\"h\":\"环境配置 (待完善)\",\"t\":[\"建议用Linux或者Windows系统进行测试，MacOS系统某些包的加载和依赖关系上存在问题，不方便进行处理。\"]},\"9\":{\"h\":\"模型结构\",\"t\":[\"模型结构图\"]},\"10\":{\"h\":\"LMAffordance3D\",\"t\":[\"class LMAffordance3D(Blip2Base): ... def forward(self, img, point, description, label, inference_mode=False): ''' img: [B, 3, H, W] -> 输入图像 (batch_size, channels, height, width) point: [B, 3, 2048] -> 点云数据 (batch_size, dimensions, num_points) description: 自然语言指令 (e.g., \\\"Grasp the bottle\\\") label: 真实标签，即每个点对应的 affordance 概率分布 (B, 2048, 1) inference_mode: 是否为推理模式（True/False） ''' # 获取输入维度信息 B, C, H, W = img.size() B, D, N = point.size() device = img.device # 获取设备信息（CPU/GPU） # Step 1: 提取图像和点云的特征 # -------------------------------------------------- # 图像编码器：ResNet18 提取 2D 特征 F2D ∈ RB×CI×H×W img_feature = self.img_encoder(img) # shape: [B, CI, H', W'] # 点云编码器：PointNet++ 提取 3D 特征 F3D ∈ RB×CP×NP point_feature = self.point_encoder(point) # shape: [B, CP, NP] # Step 2: 融合多模态空间特征 # -------------------------------------------------- # 使用 MLP 和自注意力机制融合图像与点云特征 spatial_feature = self.fusion(img_feature, point_feature) # shape: [B, NS, CS] # Step 3: 多模态特征投影到语言语义空间 # -------------------------------------------------- # 将融合后的空间特征通过适配器上采样到与语言模型匹配的维度 if self.has_qformer: ... # 如果使用 Q-Former，则进行额外处理 else: multi_embeds = self.adapter_up(spatial_feature) # shape: [B, NS, CL] image_atts = None # 默认图像注意力掩码为空 # Step 4: 对自然语言指令进行 Tokenization # -------------------------------------------------- # 设置 tokenizer 的 padding 和 truncation 方向 self.llm_tokenizer.padding_side = \\\"right\\\" self.llm_tokenizer.truncation_side = 'left' # 对语言指令进行分词，转换为 token ID 并生成 attention mask text_input_tokens = self.llm_tokenizer( description, return_tensors=\\\"pt\\\", padding=\\\"longest\\\", # 填充至最长序列长度 truncation=True, # 截断过长文本 max_length=self.max_txt_len, # 最大文本长度 ).to(device) # Step 5: 获取语言嵌入 # -------------------------------------------------- # 使用 LLM 的 embedding 层将 token ID 转换为嵌入向量 inputs_embeds = self.llm_model.get_input_embeddings()(text_input_tokens.input_ids) # shape: [B, NL, CL] （NL=token数，CL=语言嵌入维度） # Step 6: 拼接多模态嵌入与语言嵌入 # -------------------------------------------------- # 调用 concat_input 函数，将图像+点云特征插入语言嵌入中 llm_inputs, llm_attention_mask = self.concat_input( inputs_embeds, text_input_tokens.attention_mask, multi_embeds, image_atts ) # llm_inputs: [B, NL + NS, CL] # llm_attention_mask: [B, NL + NS] # Step 7: 使用 Vision-Language Model 进行联合推理 # -------------------------------------------------- # 在混合精度下运行 LLM，融合语言与视觉特征 with self.maybe_autocast(): hidden_states = self.llm_model( inputs_embeds=llm_inputs, attention_mask=llm_attention_mask, return_dict=False, # 返回 tuple 格式输出 ) # Step 8: 降维适配器 # -------------------------------------------------- # 通过适配器层将 LLM 输出映射回合适维度 hidden_states = self.adapter_down(hidden_states) # shape: [B, NS + NL, CS] # 分割出 semantic feature 和 instructional feature # 视觉语义特征 和 语言指令理解特征 semantic_feature, instructional_feature = torch.split( hidden_states, split_size_or_sections=spatial_feature.size(1), dim=1 ) # Step 9: 解码器融合所有特征以预测可操作性特征 # -------------------------------------------------- # 使用 cross-attention 融合 instruction, semantic, spatial features affordance_feature = self.affordance_decoder( spatial_feature, instructional_feature, semantic_feature ) # shape: [B, NA, CA] # Step 10: 使用分割头预测最终的 3D 可操作性热图 # -------------------------------------------------- out = self.head(spatial_feature, affordance_feature, point_feature) # 输出 shape: [B, 2048, 1]，表示每个点是否具有特定可操作性的概率 # Step 11: 推理或训练分支 # -------------------------------------------------- if inference_mode == True: return out # 仅返回预测结果 else: loss_hm = self.loss_hm(out, label) # 计算 heatmap 的损失（focal + dice） loss = loss_hm * self.w_hm # 加权总损失 return { \\\"out\\\": out, \\\"loss\\\": loss, \\\"loss_hm\\\": loss_hm }\"]},\"11\":{\"h\":\"Step 2: 融合多模态空间特征\",\"t\":[\"class Fusion(nn.Module): def __init__(self, emb_dim = 512, num_heads = 4): super().__init__() self.emb_dim = emb_dim # 对点积结果进行缩放，防止 softmax 梯度消失或爆炸。 self.div_scale = self.emb_dim ** (-0.5) self.num_heads = num_heads # 对图像和点云特征进行 非线性增强和空间对齐 ，使得它们能够在统一的语义空间中进行有效的跨模态交互。 self.mlp = nn.Sequential( nn.Conv1d(self.emb_dim, 2*self.emb_dim, 1, 1), nn.BatchNorm1d(2*self.emb_dim), nn.ReLU(), nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) self.img_attention = Self_Attention(self.emb_dim, self.num_heads) self.point_attention = Self_Attention(self.emb_dim, self.num_heads) self.joint_attention = Self_Attention(self.emb_dim, self.num_heads) def forward(self, img_feature, point_feature): ''' i_feature: [B, C, H, W] p_feature: [B, C, N_p] HW = N_i ''' B, C, H, W = img_feature.size() img_feature = img_feature.view(B, self.emb_dim, -1) #[B, C, N_i] point_feature = point_feature[-1][1] # 对图像和点云特征进行 非线性增强和空间对齐 ，使得它们能够在统一的语义空间中进行有效的跨模态交互。 p_feature = self.mlp(point_feature) i_feature = self.mlp(img_feature) # 跨模态注意力矩阵: 每个点云点与图像中每个位置之间的相似度得分 phi = torch.bmm(p_feature.permute(0, 2, 1), i_feature)*self.div_scale #[B, N_p, N_i] # 每列是一个 softmax 分布（每个图像位置对应的所有点云点）, 表示：“对于图像中的每一个位置，应该关注哪些点云点？” phi_p = F.softmax(phi,dim=1) # 每行是一个 softmax 分布（每个点云点对应的所有图像位置）, 表示：“对于点云中的每一个点，应该关注图像中的哪些位置？” phi_i = F.softmax(phi,dim=-1) # I_enhance 是图像 patch 引导下提取的点云信息增强后的图像特征 # 它不是直接包含原始图像 patch 的语义 # 而是通过“点云中相关点”的方式重构图像 patch 的语义 I_enhance = torch.bmm(p_feature, phi_p) #[B, C, N_i] # P_enhance 是每个点云局部区域关键点引导下提取的图像信息增强后的点云关键点局部区域特征 P_enhance = torch.bmm(i_feature, phi_i.permute(0,2,1)) #[B, C, N_p] # 在跨模态融合后，进一步提取各自模态内部的语义一致性与结构关系，形成更稳定的联合表示。 I = self.img_attention(I_enhance.mT) #[B, N_i, C] P = self.point_attention(P_enhance.mT) #[B, N_p, C] # 将图像patch和点云点拼接成一个统一的token序列 # 使用自注意力机制提炼两个模态之间的语义一致性 joint_patch = torch.cat((P, I), dim=1) multi_feature = self.joint_attention(joint_patch) #[B, N_p+N_i, C] return multi_feature\"]},\"12\":{\"h\":\"Step 3: 多模态特征投影到语言语义空间\",\"t\":[\" # 将融合后的 3D 和 2D 特征从原始嵌入维度 (self.emb_dim) 映射到 LLM（语言模型）所使用的隐藏状态空间维度 （self.llm_model.config.hidden_size）。 self.adapter_up = nn.Sequential( nn.Linear(self.emb_dim, self.emb_dim), nn.ReLU(), nn.Linear(self.emb_dim, self.llm_model.config.hidden_size) )\"]},\"13\":{\"h\":\"Step 6: 拼接多模态嵌入与语言嵌入\",\"t\":[\"def concat_input(self, input_embeds, input_atts, multi_embeds, image_atts=None): ''' 将语言嵌入（text embeddings）与多模态嵌入（如图像、点云等）拼接在一起， 构建 Vision-Language Model (VLM) 所需的输入格式。 Args: input_embeds: (batch_size, sequence_length, hidden_size) - 语言 token 经过 embedding 层后的结果。 input_atts: (batch_size, sequence_length) - 语言部分的 attention mask（1 表示有效，0 表示填充）。 multi_embeds: (batch_size, n, hidden_size) - 多模态嵌入（如图像或点云特征），形状为 [B, n, H]。 image_atts: (batch_size, n), optional - 多模态数据的 attention mask，默认为全 1（即所有 token 都有效）。 Returns: llm_inputs: (batch_size, total_length, hidden_size) - 拼接后的输入嵌入，供 LLM 使用。 llm_attention_mask: (batch_size, total_length) - 对应的注意力掩码。 ''' # 初始化用于存储每个样本拼接后输入和 attention mask 的列表 llm_inputs = [] llm_attention_mask = [] # 获取 batch size bs = multi_embeds.size()[0] # 对每个样本单独处理（逐个拼接） for i in range(bs): # 获取当前样本中多模态嵌入的维度信息：(n, dim) _, n, dim = multi_embeds.size() # 计算当前语言输入中有多少个有效 token（非 padding） this_input_ones = input_atts[i].sum() # 拼接嵌入向量： # 语言前半段（有效的部分）+ 多模态嵌入 + 语言后半段（padding 部分） llm_inputs.append( torch.cat([ input_embeds[i][:this_input_ones], # 有效语言部分 multi_embeds[i], # 插入的多模态嵌入 input_embeds[i][this_input_ones:] # 剩余的语言 padding 部分 ]) ) # 构建 attention mask： if image_atts is None: # 如果没有提供 image_atts，则默认多模态 token 都是有效的（mask 全为 1） llm_attention_mask.append( torch.cat([ input_atts[i][:this_input_ones], torch.ones((n), device=multi_embeds.device, dtype=torch.long), input_atts[i][this_input_ones:] ]) ) else: # 否则使用给定的 image_atts 来标记哪些多模态 token 是有效的 llm_attention_mask.append( torch.cat([ input_atts[i][:this_input_ones], image_atts[i], input_atts[i][this_input_ones:] ]) ) # 将 list 转换为 batched tensor llm_inputs = torch.stack(llm_inputs, 0) llm_attention_mask = torch.stack(llm_attention_mask, 0) # 返回拼接好的输入和 attention mask return llm_inputs, llm_attention_mask\"]},\"14\":{\"h\":\"Step 8: 降维适配器\",\"t\":[\" # 降维适配器：将 LLM 输出的隐藏状态映射回原始嵌入维度（self.emb_dim） self.adapter_down = nn.Sequential( nn.Linear(self.llm_model.config.hidden_size, self.llm_model.config.hidden_size), nn.ReLU(), nn.Linear(self.llm_model.config.hidden_size, self.emb_dim) )\"]},\"15\":{\"h\":\"Step 9: 解码器融合所有特征以预测可操作性特征\",\"t\":[\"class Affordance_Decoder(nn.Module): def __init__(self, emb_dim, proj_dim): super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim self.cross_atten = Cross_Attention(emb_dim = self.emb_dim, proj_dim = self.proj_dim) self.fusion = nn.Sequential( nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) def forward(self, query, key, value): ''' query: [B, N_p + N_i, C] -> spatial_feature (query) key: [B, N_l, C] -> instructional_feature (key) value: [B, N_l, C] -> semantic_feature (value) ''' B, _, C = query.size() # 调整 key 和 value 的形状为 [B, C, N_l] key = key.view(B, C, -1) # [B, C, N_l] value = value.view(B, C, -1) # [B, C, N_l] # 使用 cross attention 获取两个注意力加权结果 Theta_1, Theta_2 = self.cross_atten(query, key.mT, value.mT) # 将两个注意力输出拼接在一起 joint_context = torch.cat((Theta_1.mT, Theta_2.mT), dim=1) # [B, 2C, N_p + N_i] # 使用 Conv1D 融合通道信息 affordance = self.fusion(joint_context) # [B, C, N_p + N_i] # 调整输出格式为 [B, N_p + N_i, C] affordance = affordance.permute(0, 2, 1) # [B, N_p + N_i, C] return affordance\",\"class Cross_Attention(nn.Module): def __init__(self, emb_dim, proj_dim): \\\"\\\"\\\" 多模态交叉注意力模块（Cross-Attention Module）， 用于融合来自语言模型的不同语义信息，增强空间特征表达。 Args: emb_dim: 输入特征维度（embedding dimension），例如 LLM 的 hidden size（如 4096） proj_dim: 投影维度，用于降低计算复杂度，在 attention 中使用 \\\"\\\"\\\" super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim # 定义投影层，将输入映射到低维空间以进行 attention 计算 self.proj_q = nn.Linear(self.emb_dim, proj_dim) # query 投影 self.proj_sk = nn.Linear(self.emb_dim, proj_dim) # sub key 投影 self.proj_sv = nn.Linear(self.emb_dim, proj_dim) # sub value 投影 self.proj_ek = nn.Linear(self.emb_dim, proj_dim) # scene key 投影 self.proj_ev = nn.Linear(self.emb_dim, proj_dim) # scene value 投影 # 缩放因子，用于 attention 分数归一化 self.scale = self.proj_dim ** (-0.5) # 层归一化（LayerNorm），用于稳定训练过程 self.layernorm = nn.LayerNorm(self.emb_dim) def forward(self, obj, sub, scene): \\\"\\\"\\\" 执行交叉注意力机制，融合不同来源的信息： - obj: 空间特征（spatial feature），作为 query； - sub: 指令理解特征（instructional feature），作为第一个 attention 的 key 和 value； - scene: 视觉语义特征（semantic feature），作为第二个 attention 的 key 和 value； Args: obj: [B, N_p + HW, C] → spatial_feature（query 来源） sub: [B, HW, C] → instructional_feature（key/value 来源之一） scene: [B, HW, C] → semantic_feature（key/value 来源之二） Returns: I_1: 经过 attention 加权后的输出（第一分支） I_2: 经过 attention 加权后的输出（第二分支） \\\"\\\"\\\" B, seq_length, C = obj.size() # 获取 batch size 和通道维度 # 将输入分别投影到低维空间，便于后续 attention 计算 query = self.proj_q(obj) # [B, N_q, proj_dim] s_key = self.proj_sk(sub) # [B, N_i, proj_dim] s_value = self.proj_sv(sub) # [B, N_i, proj_dim] e_key = self.proj_ek(scene) # [B, N_e, proj_dim] e_value = self.proj_ev(scene) # [B, N_e, proj_dim] # 第一个 cross attention：使用 sub 的 key 和 value 增强 query atten_I1 = torch.bmm(query, s_key.mT) * self.scale # [B, N_q, N_i] atten_I1 = atten_I1.softmax(dim=-1) # softmax 归一化 I_1 = torch.bmm(atten_I1, s_value) # [B, N_q, proj_dim] # 第二个 cross attention：使用 scene 的 key 和 value 增强 query atten_I2 = torch.bmm(query, e_key.mT) * self.scale # [B, N_q, N_e] atten_I2 = atten_I2.softmax(dim=-1) I_2 = torch.bmm(atten_I2, e_value) # [B, N_q, proj_dim] # 使用残差连接 + LayerNorm 增强稳定性 I_1 = self.layernorm(obj + I_1) # [B, N_q, emb_dim] I_2 = self.layernorm(obj + I_2) # [B, N_q, emb_dim] return I_1, I_2\"]},\"16\":{\"h\":\"Step 10: 使用分割头预测最终的 3D 可操作性热图\",\"t\":[\"class Head(nn.Module): def __init__(self, additional_channel, emb_dim, N_p, N_raw): \\\"\\\"\\\" Head 模块用于最终的 3D 可操作性（affordance）预测。 它接收来自编码器和解码器的特征，并通过多尺度上采样与融合， 输出每个点云点的 affordance 热图（heatmap），表示该点是否具有可操作性。 Args: additional_channel: 额外通道数，例如法向量、颜色等信息 emb_dim: 特征维度（embedding dimension） N_p: point cloud token 数量（如 64） N_raw: 原始点云数量（如 2048） Notes: - 使用 PointNetFeaturePropagation 进行逐级上采样； - 结合全局池化增强语义表达； - 最终使用 MLP + Sigmoid 输出每个点的 affordance score； \\\"\\\"\\\" super().__init__() self.emb_dim = emb_dim self.N_p = N_p # point cloud token 数量 self.N_raw = N_raw # 原始点云数量（如 2048） # 多尺度上采样模块：PointNetFeaturePropagation # fp3: 输入为 [512 + emb_dim]，输出为 512 维度 self.fp3 = PointNetFeaturePropagation(in_channel=512+self.emb_dim, mlp=[768, 512]) self.fp2 = PointNetFeaturePropagation(in_channel=832, mlp=[768, 512]) self.fp1 = PointNetFeaturePropagation(in_channel=518+additional_channel, mlp=[512, 512]) # 全局平均池化层，压缩时间/空间维度 self.pool = nn.AdaptiveAvgPool1d(1) # 最终输出头：MLP + BatchNorm + ReLU + Sigmoid self.out_head = nn.Sequential( nn.Linear(self.emb_dim, self.emb_dim // 8), nn.BatchNorm1d(self.N_raw), # 对点数维度做 BN nn.ReLU(), nn.Linear(self.emb_dim // 8, 1), # 输出每个点的 affordance score nn.Sigmoid() # 输出范围 [0,1]，表示概率 ) def forward(self, multi_feature, affordance_feature, encoder_p): \\\"\\\"\\\" 执行 Head 模块的前向传播，生成最终的 3D affordance heatmap。 Args: multi_feature: [B, N_p + N_i, C] → 来自 Vision-Language Model 的拼接特征 affordance_feature: [B, N_p + N_i, C] → 来自 decoder 的可操作性特征 encoder_p: [p0, p1, p2, p3] → 编码器不同层级的点云特征 Returns: out: [B, N_raw, 1] → 每个点的 affordance score（概率值） \\\"\\\"\\\" B, N, C = multi_feature.size() # 解包编码器输出的不同层级特征 p_0, p_1, p_2, p_3 = encoder_p # 从 multi_feature 和 affordance_feature 中提取 point cloud token 部分 P_align, _ = torch.split(multi_feature, split_size_or_sections=self.N_p, dim=1) F_pa, _ = torch.split(affordance_feature, split_size_or_sections=self.N_p, dim=1) # 上采样过程：fp3 -> fp2 -> fp1 up_sample = self.fp3(p_2[0], p_3[0], p_2[1], P_align.mT) # [B, emb_dim, npoint_sa2] up_sample = self.fp2(p_1[0], p_2[0], p_1[1], up_sample) # [B, emb_dim, npoint_sa1] up_sample = self.fp1(p_0[0], p_1[0], torch.cat([p_0[0], p_0[1]], 1), up_sample) # [B, emb_dim, N_raw] # 对 F_pa 做全局池化，得到一个全局语义向量 F_pa_pool = self.pool(F_pa.mT) # [B, emb_dim, 1] # 将全局语义向量扩展回原始点云数量，实现 feature-wise attention affordance = up_sample * F_pa_pool.expand(-1, -1, self.N_raw) # [B, emb_dim, N_raw] # 输出 head：将特征映射到 0~1 的概率值，表示每个点是否具有可操作性 out = self.out_head(affordance.mT) # [B, N_raw, 1] return out\"]},\"17\":{\"h\":\"LASO 模型代码解读与复现\",\"t\":[\"LASO: Language-guided Affordance Segmentation on 3D Object 论文代码解读与复现\",\"论文: https://openaccess.thecvf.com/content/CVPR2024/papers/Li_LASO_Language-guided_Affordance_Segmentation_on_3D_Object_CVPR_2024_paper.pdf 代码: https://github.com/yl3800/LASO\",\"这篇论文提出了一项新的任务和一个配套的数据集，旨在推动 语言引导下的 3D对象功能区域分割（Language-guided Affordance Segmentation on 3D Object, 简称 LASO）。\"]},\"18\":{\"h\":\"数据集\"},\"19\":{\"h\":\"1. 基础数据来源\",\"t\":[\"数据集基于 3D-AffordanceNet 提供的点云和功能区域标注构建：\",\"每个物体都以点云形式表示；\",\"点云中的每个点被标注为支持一个或多个功能类型（multi-class affordance labels），例如 grasp、open、lift、move 等；\",\"这些功能标注是人工标注的，具有语义意义；\",\"为什么使用 3D-AffordanceNet？\",\"因为它提供了高质量的点云和功能标注，能够很好地支持 LASO 的目标：根据自然语言问题找出与之相关的功能区域。\"]},\"20\":{\"h\":\"2. 构建问题（Question Crafting）\",\"t\":[\"选取物体-功能组合： \",\"从 3D-AffordanceNet 中选取了 58 种物体-功能组合（如 mug-grasp、door-open 等）；\",\"手工设计问题： \",\"对每种组合手工编写 5 个代表性问题；\",\"使用 GPT-4 扩展生成更多问题： \",\"使用 GPT-4 为每个组合额外生成 10 个问题；\",\"总共得到 870 个专家设计的问题（58 × 15 = 870）；\",\"Affordance-Question数据可视化\",\"在扩展过程中，GPT-4 生成的问题遵循以下三个关键原则，以确保问题多样性和语义丰富性：\",\"原则\",\"描述\",\"Contextual Enrichment（上下文丰富化）\",\"添加更多上下文细节，使问题更具体地连接目标对象的功能；例：将 “Grasping scissors: top choice?” 改为 “Identify the key points on the scissors that ensure successful grasping.”\",\"Concise Phrasing（简洁表达）\",\"提炼问题本质，使其简短但仍有意义；\",\"Structural Diversity（结构多样性）\",\"使用不同句式结构（疑问句、陈述句等），防止模型偏向特定句式或长度；\"]},\"21\":{\"h\":\"3. 标注 GT Mask（Ground Truth Mask）\",\"t\":[\"对于每个问题，结合其对应的功能类型和原始点云标注信息，构造出对应的二值掩码 gt_mask：\",\"每个点是否属于当前问题描述的功能区域；\",\"gt_mask 是 (N,) 形状的一维数组，其中 N 是点数；\",\"数值可以是 0/1（binary mask），也可以是软标签（soft label），表示点属于该功能区域的概率；\",\"软标签通常用于边界模糊区域，反映点与功能核心区域的距离远近；\",\"💡 注意：这些功能标签仅用于构造问题和定位正确功能区域，在训练和测试中不作为显式监督信号。\"]},\"22\":{\"h\":\"4. 数据集组织方式\",\"t\":[\"数据总量：\",\"总样本数：19,751 个点云-问题配对；\",\"物体类别数：23 类；\",\"功能类型数：17 类；\",\"问题总数：870 个专家设计的问题；\",\"每个物体类别可有多个形状实例；\",\"一个问题可以作用于多个物体类别（泛化能力）；\",\"数据集设置（两种模式）：\",\"🔹 Seen（见过）\",\"训练和测试阶段共享相似的物体类别和功能类型的分布；\",\"目的是评估模型在熟悉场景下的表现；\",\"🔹 Unseen（未见）\",\"某些功能类型在特定物体类别下会从训练集中省略，但在测试集中保留；\",\"目的是测试模型对新组合的泛化能力；\",\"例如：模型在训练期间学会了抓取包和杯子，但测试时要求“抓取耳机”——这是训练中未曾遇到过的功能-物体组合；\",\"数据划分方式：\",\"分区\",\"物体类别数\",\"问题数\",\"样本数\",\"Train\",\"6883\",\"638\",\"16,120\",\"Val\",\"516\",\"58\",\"1,215\",\"Test\",\"1035\",\"174\",\"2,416\"]},\"23\":{\"h\":\"5. 数据增强与配对策略\",\"t\":[\"训练阶段：\",\"每次迭代中，每个形状实例随机匹配一个与其功能类型一致的问题；\",\"随机配对使模型暴露于各种语义上下文中，提升泛化能力；\",\"推理阶段（验证 & 测试）：\",\"问题配对是固定的；\",\"所有问题专属于评估阶段，不在训练中透露；\",\"确保推理一致性，保持评估完整性；\"]},\"24\":{\"h\":\"6. 数据集统计信息（来自论文图3）\",\"t\":[\"维度\",\"内容\",\"功能类型\",\"17 类，如 grasp、open、lift、move 等\",\"物体类别\",\"23 类，如 mug、microwave、chair、door 等\",\"物体-功能组合\",\"58 种唯一组合（object-affordance pairs）\",\"问题总数\",\"870 个定制化问题\",\"点云-问题配对\",\"19,751 对\",\"点云来源\",\"来自 3D-AffordanceNet，每个点云约 2048 个点\"]},\"25\":{\"h\":\"7. 代码实现\",\"t\":[\"数据集初始化的核心代码实现如下:\",\"class AffordQ(Dataset): def __init__(self, split='train', **kwargs ): # 数据集存放目录 data_root='LASO_dataset' # 数据集类型: 训练集，评估集，测试集 self.split = split # 所支持的23种物体类型和17种功能类型 classes = [\\\"Bag\\\", \\\"Bed\\\", \\\"Bowl\\\",\\\"Clock\\\", \\\"Dishwasher\\\", \\\"Display\\\", \\\"Door\\\", \\\"Earphone\\\", \\\"Faucet\\\", \\\"Hat\\\", \\\"StorageFurniture\\\", \\\"Keyboard\\\", \\\"Knife\\\", \\\"Laptop\\\", \\\"Microwave\\\", \\\"Mug\\\", \\\"Refrigerator\\\", \\\"Chair\\\", \\\"Scissors\\\", \\\"Table\\\", \\\"TrashCan\\\", \\\"Vase\\\", \\\"Bottle\\\"] afford_cl = ['lay','sit','support','grasp','lift','contain','open','wrap_grasp','pour', 'move','display','push','pull','listen','wear','press','cut','stab'] # 建立物体类型和功能类型的索引映射关系，神经网络模型只认识数字 self.cls2idx = {cls.lower():np.array(i).astype(np.int64) for i, cls in enumerate(classes)} self.aff2idx = {cls:np.array(i).astype(np.int64) for i, cls in enumerate(afford_cl)} # 加载标注数据 with open(os.path.join(data_root, f'anno_{split}.pkl'), 'rb') as f: self.anno = pickle.load(f) # 加载点云数据 with open(os.path.join(data_root, f'objects_{split}.pkl'), 'rb') as f: self.objects = pickle.load(f) # 加载58种物体-功能组合的标注数据 (数据组织形式，参考上文的 Affordance-Question数据可视化图) self.question_df = pd.read_csv(os.path.join(data_root, 'Affordance-Question.csv')) # sort anno by object class and affordance type -- 遍历标注数据列表 self.sort_anno ={} for item in sorted(self.anno, key=lambda x: x['class']): # 获取当前样本的物体类别和物体信息值: 点云ID, 功能区域掩码, 功能类别 key = item['class'] value = {'shape_id': item['shape_id'], 'mask': item['mask'], 'affordance': item['affordance']} # 每种物体可以对应多种形状实例和功能类别 if key not in self.sort_anno: # 如果当前物体类别不在排序后的字典中，直接添加 self.sort_anno[key] = [value] else: # 如果当前物体类别在排序后的字典中，将当前样本的物体信息值追加到对应列表中 self.sort_anno[key].append(value)\",\"加载的标注数据中每个样本的组织形式如下:\",\"shape_id ：点云ID\",\"class ：物体类别（如bed）\",\"affordance ：功能类别（如lay）\",\"mask ：功能区域掩码（点级别标注）\",\"标注数据组织形式\",\"点云数据组织形式\",\"每种物体可以对应多种形状实例和功能类别\",\"获取样本的代码实现:\",\" def __getitem__(self, index): # 根据样本索引取出样本数据 data = self.anno[index] # 获取当前样本对应的点云ID shape_id = data['shape_id'] # 获取当前样本对应的物体类别 cls = data['class'] # 获取当前样本对应的功能类型 affordance = data['affordance'] # 获取当前样本对应的功能区域掩码 gt_mask = data['mask'] # 取出当前样本对应的点云数据 ，（2048,3) point_set = self.objects[str(shape_id)] # 对点云数据进行归一化处理，消除尺度差异 point_set,_,_ = pc_normalize(point_set) # 对点云数据进行转置操作 ，（3,2048) point_set = point_set.transpose() # 获取当前样本对应的问题文本(训练: 随机选； 验证&测试: 固定返回问题0) question = self.find_rephrase(self.question_df, cls, affordance) # 获取当前功能类型对应的索引值 affordance = self.aff2idx[affordance] # 返回: 点云数据， 物体类别索引， 功能区域掩码， 问题文本， 功能类型索引 return point_set, self.cls2idx[cls], gt_mask, question, affordance def find_rephrase(self, df, object_name, affordance): # 如果当前是训练模式，则从问题1～15中随机选择一个问题，否则固定返回问题0 qid = str(np.random.randint(1, 15)) if self.split == 'train' else '0' qid = 'Question'+qid # 从 DataFrame df 中筛选出同时满足 物体名称匹配 和 功能属性匹配 的行，并仅保留 qid 指定的列，也就是取出上面随机选择的问题文本 result = df.loc[(df['Object'] == object_name) & (df['Affordance'] == affordance), [qid]] # 问题文本不为空，则返回该问题文本 if not result.empty: # return result.index[0], result.iloc[0]['Rephrase'] return result.iloc[0][qid] else: raise NotImplementedError\"]},\"26\":{\"h\":\"8. 总结\",\"t\":[\"LASO 数据集基于 3D-AffordanceNet 的点云和功能标注，结合人工+GPT-4 生成的多样化问题，构造出 19,751 个点云-问题配对，旨在实现语言引导下的 3D 功能区域分割，推动 3D 视觉与大语言模型（LLM）的深度融合。\"]},\"27\":{\"h\":\"模型实现\",\"t\":[\"论文提出了一个全新的模型：PointRefer，用于解决一个新颖的任务 —— 语言引导的 3D 对象功能区域分割（LASO）。\",\"模型目标： 给定一个 3D 点云对象和一个自然语言问题（例如：“Where would you grasp this mug?”），PointRefer 的目标是预测出与该问题相关的点云区域，即生成一个二值掩码，表示哪些点属于目标功能区域。\",\"PointRefer 包括以下核心模块：\",\"3D 骨干网络（3D Backbone）\",\"使用 PointNet++ 编码点云特征；\",\"多阶段编码-解码结构提取多尺度点特征；\",\"自适应融合模块（Adaptive Fusion Module, AFM）\",\"在不同解码层注入语言信息；\",\"实现语言引导下的跨模态融合；\",\"增强点特征的语义判别能力；\",\"参考点解码器（Referred Point Decoder, RPD）\",\"引入一组可学习的“问题条件化查询”（affordance queries）；\",\"利用 Transformer 解码器将这些查询与点云特征进行交互；\",\"生成动态卷积核（dynamic kernels）；\",\"最终通过卷积操作生成分割掩码；\",\"PointRefer模型结构图\",\"PointRefer 前向传播过程如下:\",\"class PointRefer(nn.Module): # 传入question文本 和 point点云数据 def forward(self, text, xyz): ''' text: [B, L, 768] xyz: [B, 3, 2048] -- (b,c,n) ''' B, C, N = xyz.size() # 1. Encoding 过程 # 1.1 Language Encoding 使用RoBert编码文本 t_feat, t_mask = self.forward_text(list(text), xyz.device) # [batch, q_len, d_model] # 1.2 BackBone Encoding 使用PointNet++编码点云 F_p_wise = self.point_encoder(xyz) \\\"\\\"\\\" Decoding \\\"\\\"\\\" # 1.3 PointNet++ 逐级做点集抽象得到的每层的点集坐标和点集特征集合 p_0, p_1, p_2, p_3 = F_p_wise # 2.Backbone Decoding过程 # 2.1 点集集合中每个点的特征和文本特征信息进行融合,传入的点集特征集合经过转置处理后的维度为: (b, n, c) p_3[1] = self.gpb(t_feat, p_3[1].transpose(-2, -1)).transpose(-2, -1) # 2.2 PointNet++ 特征传播阶段: 上采样过程中，上一层点集中的点特征重建过程中，充分吸收了高级区域抽象特征和文本特征 up_sample = self.fp3(p_2[0], p_3[0], p_2[1], p_3[1]) #[B, emb_dim, npoint_sa2] up_sample = self.gpb(t_feat, up_sample.transpose(-2, -1)).transpose(-2, -1) up_sample = self.fp2(p_1[0], p_2[0], p_1[1], up_sample) #[B, emb_dim, npoint_sa1] up_sample = self.gpb(t_feat, up_sample.transpose(-2, -1)).transpose(-2, -1) # 2.3 特征传播阶段结束: 一步步重建回原始点数量 128->256->512->1024->2048 up_sample = self.fp1(p_0[0], p_1[0], torch.cat([p_0[0], p_0[1]],1), up_sample) #[B, emb_dim, N] # 3. Referred Point Decoding过程 t_feat = self.decoder(t_feat, up_sample.transpose(-2, -1), tgt_key_padding_mask=t_mask, query_pos=self.pos1d) # b,l,c t_feat *= t_mask.unsqueeze(-1).float() _3daffordance = torch.einsum('blc,bcn->bln', t_feat, up_sample) _3daffordance = _3daffordance.sum(1)/(t_mask.float().sum(1).unsqueeze(-1)) _3daffordance = torch.sigmoid(_3daffordance) return _3daffordance.squeeze(-1)\",\"论文中所给的模型架构图中的Encoder layer指的是PointNet++中提供的PointNetSetAbstractionMsg多尺度分组点集特征抽取类\",\"论文中所给的模型架构图中的Decoder layer指的是PointNet++中提供的PointNetFeaturePropagation特征传播类\"]},\"28\":{\"h\":\"AFM 自适应融合模块\",\"t\":[\"在 LASO 任务中，模型需要根据自然语言问题（如 “Where to grasp?”）识别点云中的功能区域。由于目标功能区域的尺度、形状多样，传统方法难以适应不同情况。为此，作者设计了 AFM 模块，以增强 PointNet++ 解码过程中点特征的语言引导能力。\",\"AFM 的目标是：在不同解码阶段注入语言线索（text clues），将文本语义信息与点云特征进行跨模态融合，逐步以自上而下的方式细化点特征图，从而提升模型对多尺度、多形状的功能区域的感知能力。\",\"AFM 遵循一个 瓶颈式架构（bottleneck architecture），包含三个关键步骤：\",\"Grouping（分组）\",\"Mixing（混合）\",\"Ungrouping（解组）\",\"这三个步骤构成了一个完整的跨模态融合流程。\"]},\"29\":{\"h\":\"1️⃣ Grouping：文本引导的点特征分组\",\"t\":[\"输入：\",\"X ∈ R^{L×d}：问题编码后的文本特征（由 RoBERTa 编码得到）\",\"P ∈ R^{T×d}：某一层解码器输出的点特征，其中 T 表示该层点数\",\"处理过程：\",\"使用一个轻量级的交叉注意力模块，将文本特征作为查询（query），点特征作为键（key）和值（value），输出分组标记 G：\",\"其中：\",\" 是一个线性变换；\",\"注意力机制使得每个文本 token 对应一组相关的点特征；\",\"分组操作实现了“语言引导的点特征筛选”。\",\"重点是如何理解这里的分组: 每个文本Token询问所有点Key后，知道了哪些点跟自身的相关度更大，因此加权融合的时候，侧重于给这些点的特征分配更大的融合权重。\",\"这部分代码实现如下:\",\"# group_layer 的实现 class LightGroupAttnBlock(nn.Module): # query 是RoBerta编码后的文本特征 , (b,l,c) # key和value都是点云特征 , (b,n,c) def forward(self, query, key, value, q_mask=None): def _inner_forward(query, key, value): q = self.norm_query(query) k = q if self.key_is_query else self.norm_key(key) v = k if self.value_is_key else self.norm_value(value) # 让每个语言 token 去关注点云中最相关的区域，并在此基础上强化自身的语义表达。 # 加上原始 X 是一种残差连接（Residual Connection），可以确保语言语义不会丢失。 x = self.attn(q, k, v, q_mask) + self.drop(q) return x return _inner_forward(query, key, value)\"]},\"30\":{\"h\":\"2️⃣ Mixing：MLP-Mixer 进行组内和通道间的信息混合\",\"t\":[\"MLP-Mixer 是一种 基于 MLP（多层感知机）的视觉模型架构 ，由 Google Research 在 2021 年提出。它不使用任何注意力机制，而是通过 空间混合（mixing）和通道混合（mixing）操作 来实现全局信息建模。\",\"MLP-Mixer: An all-MLP Architecture for Vision\",\"MLP-Mixer 的核心思想是：用 MLP 替代 Transformer 中的自注意力机制 ，从而减少计算复杂度并保持性能。\",\"Token-mixing MLP\",\"对所有点/patch 的相同通道进行混合；\",\"相当于跨空间位置的信息交换；\",\"类似于 CNN 中的空间卷积；\",\"Channel-mixing MLP\",\"对每个 token 的所有通道进行处理；\",\"提取更高级的特征表示；\",\"类似于传统的全连接层或 1x1 卷积；\",\"这两个操作交替进行，形成一个类似于 Transformer 的堆叠结构，但完全不使用注意力机制。\",\"输入：\",\"G ∈ R^{L×d}：分组后的文本引导特征\",\"处理过程：\",\"使用 MLP-Mixer 来更新分组特征，生成融合特征 F：\",\"其中：\",\"MLP₁ 负责组内信息混合（token 内部）；\",\"MLP₂ 负责通道间信息混合（feature channel）；\",\"两个 MLP 交替作用，实现跨模态信息的充分交互；\",\"最终输出融合特征 F；\",\"这部分代码实现如下:\",\"# mixer 的实现 class MLPMixerLayer(nn.Module): def __init__(self, num_patches, embed_dims, patch_expansion, channel_expansion, drop_out, **kwargs): super().__init__() patch_mix_dims = int(patch_expansion * embed_dims) # 16 channel_mix_dims = int(channel_expansion * embed_dims) # 128 self.patch_mixer = nn.Sequential( nn.Linear(num_patches, patch_mix_dims, bias=False), # try here nn.GELU(), nn.Dropout(drop_out), nn.Linear(patch_mix_dims, num_patches, bias=False), nn.Dropout(drop_out) ) self.channel_mixer = nn.Sequential( nn.Linear(embed_dims, channel_mix_dims), nn.GELU(), nn.Dropout(drop_out), nn.Linear(channel_mix_dims, embed_dims), nn.Dropout(drop_out) ) self.norm1 = nn.LayerNorm(embed_dims) self.norm2 = nn.LayerNorm(embed_dims) # x 分组后的文本引导特征 : (b,l,c) def forward(self, x): # x 转置后: (b,c,l) , patch_mixer 负责组内信息混合（token 内部） x = x + self.patch_mixer(self.norm1(x).transpose(1,2)).transpose(1,2) # channel_mixer 负责通道间信息混合（feature channel) x = x + self.channel_mixer(self.norm2(x)) return x\"]},\"31\":{\"h\":\"3️⃣ Ungrouping：将融合特征映射回点空间\",\"t\":[\"输入：\",\"原始点特征 P；\",\"融合后的文本特征 F；\",\"处理过程：\",\"使用另一个注意力模块，将融合特征重新分配给每个点：\",\"其中：\",\"W₂ 是线性变换；\",\"注意力机制让每个点从融合特征中提取相关信息；\",\"输出 P_m 是语言增强后的点特征；\",\"最后加上残差连接形成最终输出 P_o：\",\"这个 P_o 就是经过 AFM 增强的点特征图，用于后续分割掩码预测。\",\"class FullAttnCatBlock(nn.Module): # query 为点云: (b,n,c) , key和value为融合后的文本特征: (b,l,c) def forward(self, query, key, value, key_padding_mask=None): def _inner_forward(query, key, value, key_padding_mask): q = self.norm_query(query) k = q if self.key_is_query else self.norm_key(key) v = k if self.value_is_key else self.norm_value(value) # 使用另一个注意力模块，将融合特征重新分配给每个点 x = self.attn(q, k, v, key_padding_mask) + self.drop(query) # MLP映射 + Residual Connection x = self.ffn(self.norm2(x)) + x return x return _inner_forward(query, key, value, key_padding_mask)\"]},\"32\":{\"h\":\"4️⃣ AFM 自适应融合模块\",\"t\":[\"有了以上 Grouping - Mixing - Ungrouping 三个关键步骤的实现，下面只需要把以上的三个步骤按流程组织起来即可得到AFM模块的完整实现了:\",\"class GPBlock(nn.Module): # q: 文本特征 (b, l, c) ， x: 点集特征集合 (b, n, c) def forward(self, q, x, q_mask=None): # Grouping阶段 gt = self.group_layer(query=q, key=x, value=x) if q_mask is not None: gt *= q_mask.unsqueeze(-1) # Mixing阶段 gt = self.mixer(gt) + self.drop(gt) # Ungrouping阶段 ungroup_tokens = self.un_group_layer(query=x, key=gt, value=gt, key_padding_mask=q_mask) return ungroup_tokens\",\"AFM 的网络结构可视化理解\",\"文本特征 X ──┐ ↓ Grouping (Cross-Attention) ↓ Mixing (MLP-Mixer) ↓ Ungrouping (Attention) ↓ 输出增强后的点特征 P_o\",\"Grouping：用语言引导点特征分组；\",\"Mixing：在分组内进行信息交换；\",\"Ungrouping：再将融合信息返回点空间；\",\"这种设计使得语言信息能有效地指导点特征的学习过程，论文中也进行了大量消融实验来验证 AFM 的有效性：\",\"模型变体\",\"mIoU\",\"AUC\",\"SIM\",\"MAE\",\"基线（不加 AFM）\",\"17.7\",\"82.1\",\"0.558\",\"0.110\",\"加入 AFM 后\",\"20.8\",\"87.3\",\"0.629\",\"0.093\",\"结果表明：加入 AFM 显著提升了所有指标，说明其确实有效增强了语言-视觉的跨模态交互能力。\"]},\"33\":{\"h\":\"RPO 参考点解码器\",\"t\":[\"Referred Point Decoder（RPD）是 LASO 任务中用于生成功能区域掩码的核心模块。\",\"它的主要目标是：\",\"利用一组问题条件化的 affordance queries 通过 Transformer 解码器与点云特征交互 ，生成一组动态卷积核（dynamic kernels），最终通过这些 kernel 对 AFM 增强后的点特征进行卷积，得到分割掩码。\",\"class TransformerDecoderLayer(nn.Module): # tgt: text feature (b,l,c), memory: up_sample (b,n,c) def forward(self, tgt, memory, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None, pos: Optional[Tensor] = None, query_pos: Optional[Tensor] = None): # 1. Affordance Query = 问题嵌入（Question Embedding）X + 可学习的位置编码（Learnable Position Embeddings） # 这里tgt就是Roberta编码得到的文本特征嵌入向量 # 使用 X 作为初始输入，确保每个 query 都带有原始语言上下文； # 如果只用 learnable embeddings，模型将完全依赖随机初始化的参数去“猜”语言含义，效率极低； q = k = self.with_pos_embed(tgt, query_pos) # 2. 自注意力机制: 让每个 query 不仅理解自己的语义，还能感知其他 query 的信息，从而形成更完整的语言上下文理解。 tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask) tgt = tgt + self.dropout1(tgt2) tgt = self.norm1(tgt) # (b,l,c) # 3. 跨模态注意力机制: 每个 affordance query 都会基于其语言语义，从点云中找出最相关的功能区域，从而为后续的动态卷积和掩码预测提供基础。 tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos), key=self.with_pos_embed(memory, pos), value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask, output_attentions = True) tgt = tgt + self.dropout2(tgt2) tgt = self.norm2(tgt) # (b,l,c) # 4. MLP: 每个query通道维度做特征融合 tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt)))) tgt = tgt + self.dropout3(tgt2) tgt = self.norm3(tgt) # (b,l,c) return tgt class PointRefer(nn.Module): def forward(self, text, xyz): ... # 3. Referred Point Decoding过程 # 3.1 利用一组问题条件化的 affordance queries 通过 Transformer 解码器与点云特征交互 ，生成一组动态卷积核（dynamic kernels）(b,l,c) t_feat = self.decoder(t_feat, up_sample.transpose(-2, -1), tgt_key_padding_mask=t_mask, query_pos=self.pos1d) # 3.2 对无效 token（padding）做掩码操作，防止其影响后续计算。 (b,l,c) t_feat *= t_mask.unsqueeze(-1).float() # 3.3 执行 动态卷积（Dynamic Convolution） 操作，用增强后的语言查询去“扫描”点云特征图 （b,l,n) _3daffordance = torch.einsum('blc,bcn->bln', t_feat, up_sample) # 3.4 对affordance query的响应图进行平均池化，融合所有 affordance query 的得分结果。 (b,n) _3daffordance = _3daffordance.sum(1)/(t_mask.float().sum(1).unsqueeze(-1)) # 3.5 将响应值映射到 [0, 1] 区间，表示每个点属于目标功能区域的概率。 (b,n) _3daffordance = torch.sigmoid(_3daffordance) return _3daffordance # (b,n)\",\"PyTorch 的 einsum 函数，它是一个非常强大且灵活的张量操作函数，支持通过爱因斯坦求和约定（Einstein Summation Convention） 来表达各种线性代数运算。\",\"下面详细介绍一下动态卷机核卷积的过程:\",\"t_feat: 语言查询特征 , 形状：(B, L, C) , 这是 经过 Referred Point Decoder (RPD) 处理后的 affordance queries，表示每个 token 对应的“动态卷积核”。\",\"up_sample: 上采样后的点云特征 , 形状：(B, C, N)。\",\"而下面这行代码实现的是一个 动态卷积（Dynamic Convolution） 操作：\",\"_3daffordance = torch.einsum('blc,bcn->bln', t_feat, up_sample)\",\"它的本质是： 用一组由语言引导的动态卷积核 t_feat 去卷积点云特征 up_sample，得到每个 token 对每个点的关注响应。\",\"详细解释 einsum 表达式:\",\"torch.einsum('blc,bcn->bln', t_feat, up_sample)\",\"维度\",\"含义\",\"b\",\"batch 维度，保持不变\",\"l\",\"token 维度，保留下来\",\"c\",\"特征通道维度，进行内积操作（求和）\",\"n\",\"点云维度，保留下来\",\"所以这个表达式的含义是：\",\"也就是说，对于每一个 batch 中的数据：\",\"每个 token（l）都与所有点（n）交互；\",\"每个 token 实际上是一个动态生成的卷积核（C × 1 × 1），作用于点云特征图（C × N）；\",\"最终输出形状为 (B, L, N)，表示： \",\"每个 token 对每个点的关注程度；\",\"输出张量\",\"形状\",\"含义\",\"_3daffordance\",\"(B, L, N)\",\"每个 token 对每个点的响应值（得分）\",\"然后在后续会进行如下处理：\",\"_3daffordance = _3daffordance.sum(1) / (t_mask.float().sum(1).unsqueeze(-1)) _3daffordance = torch.sigmoid(_3daffordance)\",\"即：\",\"在 token 维度求和（或平均池化），融合多个 token 的关注信息；\",\"使用 sigmoid 得到最终的掩码，形状 (B, N)；\",\"每个点的值 ∈ [0, 1]，表示其属于目标功能区域的概率；\"]},\"34\":{\"h\":\"损失函数\"},\"35\":{\"h\":\"HM_Loss（Hybrid Mask Loss）\",\"t\":[\"在 LASO 数据集中，模型需要根据自然语言问题识别点云中最相关的功能区域（如 grasping area, opening area 等），而 HM_Loss 是 PointRefer 模型的监督信号，它结合了：\",\"Focal Loss ：用于缓解类别不平衡问题；\",\"Dice Loss ：用于衡量预测掩码与真实标签之间的空间重合度；\",\"最终 loss = CELoss + DiceLoss，让模型同时关注逐点分类精度和整体区域匹配。\",\"import torch import torch.nn as nn import torch.nn.functional as F class HM_Loss(nn.Module): def __init__(self): \\\"\\\"\\\" Hybrid Mask Loss 实现： - BCE-Focal Loss（加权交叉熵） - Dice Loss（衡量预测掩码与 GT 的重合度） 公式来自论文 Section 4.2，用于语言引导下的功能区域分割。 \\\"\\\"\\\" super(HM_Loss, self).__init__() # 设置 Focal Loss 参数 self.gamma = 2 # 聚焦参数，放大难分类样本影响 self.alpha = 0.25 # 平衡因子，强调正类（前景点）loss def forward(self, pred, target): \\\"\\\"\\\" 输入： pred: 模型输出的原始 logit 或经过 sigmoid 的概率值； 形状为 [B, N] target: ground truth 掩码（soft mask），形状也为 [B, N] 返回： total_loss: CELoss + DiceLoss 的加权和 \\\"\\\"\\\" # Step 1: 构建 Focal Loss 权重项 # temp1：负类 loss（背景点） # temp2：正类 loss（目标功能区域） # 1e-6 的加入是为了让 log 计算保持稳定，尤其是在预测值接近极端值（0 或 1）时 temp1 = -(1 - self.alpha) * torch.mul( pred ** self.gamma, torch.mul(1 - target, torch.log(1 - pred + 1e-6)) ) temp2 = -self.alpha * torch.mul( (1 - pred) ** self.gamma, torch.mul(target, torch.log(pred + 1e-6)) ) # 将两个方向的 loss 合并，并取 batch 和点维度的平均 temp = temp1 + temp2 CELoss = torch.sum(torch.mean(temp, dim=(0, 1))) # Step 2: 计算正类 Dice Loss（预测与 Ground Truth 的交集 / 并集） intersection_positive = torch.sum(pred * target, dim=1) cardinality_positive = torch.sum(torch.abs(pred) + torch.abs(target), dim=1) dice_positive = (intersection_positive + 1e-6) / (cardinality_positive + 1e-6) # Step 3: 计算负类 Dice Loss（非目标区域匹配度） intersection_negative = torch.sum((1 - pred) * (1 - target), dim=1) cardinality_negative = torch.sum(2 - torch.abs(pred) - torch.abs(target), dim=1) dice_negative = (intersection_negative + 1e-6) / (cardinality_negative + 1e-6) # Step 4: 构建 Dice Loss，形式为 1 - Dice Score # 使用了一个偏置项 1.5（可能是经验设定） temp3 = torch.mean(1.5 - dice_positive - dice_negative, dim=0) DICELoss = torch.sum(temp3) # Step 5: 总损失 = 分类误差 + 区域匹配误差 return CELoss + 1.0 * DICELoss\",\"在论文 Section 4.2 中提到：\",\"“We solely employ Dice loss and Binary Cross-Entropy (BCE) loss to guide the segmentation mask prediction.”\",\"虽然这里用的是 Focal Loss + Dice Loss 的组合形式，但它本质上是 BCE + Dice 的改进版，具有以下优势：\",\"Focal Loss: 抑制 easy examples，放大 hard examples，防止忽略小区域\",\"Dice Loss: 关注整体掩码匹配度，提升边界识别能力\",\"两者结合可以：\",\"缓解类别极度不平衡问题；\",\"提高模型对语言指令下功能区域的理解能力；\",\"更好地应对 LASO 中的语言引导 + soft mask 场景；\"]},\"36\":{\"h\":\"训练\",\"t\":[\"模型的训练过程大体分为了 准备，训练，评估 三个流程；\"]},\"37\":{\"h\":\"准备\",\"t\":[\"准备阶段主要完成数据集加载，模型初始化，损失函数定义，优化器设置，学习率调度器初始化等；\",\"def main(opt, dict): # 1. 加载训练集，验证集，测试集 train_dataset = AffordQ('train') train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8 ,shuffle=True, drop_last=True) val_dataset = AffordQ('val') val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=8, shuffle=False) test_dataset = AffordQ('test') test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=8, shuffle=False) # 2. 初始化模型 model = get_PointRefer(emb_dim=dict['emb_dim'], proj_dim=dict['proj_dim'], num_heads=dict['num_heads'], N_raw=dict['N_raw'], num_affordance = dict['num_affordance'], n_groups=opt.n_groups) # 3. 初始化损失函数，优化器，学习率调度器 criterion_hm = HM_Loss() criterion_ce = nn.CrossEntropyLoss() param_dicts = [ {\\\"params\\\": [p for n, p in model.named_parameters() if \\\"text_encoder\\\" not in n and p.requires_grad]}, {\\\"params\\\": [p for n, p in model.named_parameters() if \\\"text_encoder\\\" in n and p.requires_grad], \\\"lr\\\": opt.tlr}] optimizer = torch.optim.Adam(params = param_dicts, lr=dict['lr'], betas=(0.9, 0.999), eps=1e-8, weight_decay=opt.decay_rate) # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=dict['Epoch'], eta_min=1e-6) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\"]},\"38\":{\"h\":\"训练\",\"t\":[\"训练阶段则是模型的核心迭代过程，包括前向传播，损失计算，反向传播，参数更新等:\",\" ''' Training ''' for epoch in range(start_epoch+1, dict['Epoch']): num_batches = len(train_loader) loss_sum = 0 total_point = 0 model = model.train() for i,(point, cls, gt_mask, question, aff_label) in enumerate(train_loader): optimizer.zero_grad() # 4. 前向传播过程 _3d = model(question, point) # 5. 计算损失 loss_hm = criterion_hm(_3d, gt_mask) # loss_ce = criterion_ce(logits, cls) # 6. 反向传播 temp_loss = loss_hm # + opt.loss_cls*loss_ce temp_loss.backward() optimizer.step() results = torch.zeros((len(val_dataset), 2048, 1)) targets = torch.zeros((len(val_dataset), 2048, 1))\"]},\"39\":{\"h\":\"评估\",\"t\":[\"评估阶段则是在验证集或测试集上评估模型的性能，计算指标包括 MAE，SIM，AUC，mIoU。\",\"在 LASO（Language-guided Affordance Segmentation on 3D Object）任务 中，作者使用了四个核心评估指标来衡量模型对语言引导下功能区域的识别能力：\",\"指标\",\"名称\",\"英文全称\",\"MAE\",\"平均绝对误差\",\"Mean Absolute Error\",\"SIM\",\"相似性得分\",\"Similarity Score\",\"AUC\",\"曲线下面积\",\"Area Under the Curve\",\"mIoU\",\"平均交并比\",\"mean Intersection over Union\",\"MAE（Mean Absolute Error）是预测值与真实值之间的平均绝对误差，用于衡量模型输出的 soft mask 与 ground truth 掩码之间的逐点偏差。\",\"其中：\",\"：点云中点的数量；\",\"：模型预测该点属于功能区域的概率；\",\"：ground truth 标签（可以是 soft mask 或 binary mask）；\",\"特点与作用：\",\"特性\",\"描述\",\"✔️ 支持 soft mask 输入\",\"不依赖 thresholding，适用于连续响应值\",\"✔️ 衡量整体分布一致性\",\"反映模型是否准确学习语言引导下的响应强度\",\"⚠️ 对边界模糊区域不敏感\",\"IoU 等指标更关注重合度\",\"SIM（Similarity）是一种基于直方图交集的相似性指标，用于衡量两个概率分布之间的匹配程度。它常用于图像检索、图像分割等任务。\",\"即：对每个点取预测值和真实值中的较小者，然后求和。也可以归一化为：\",\"特点与作用：\",\"特性\",\"描述\",\"✔️ 不需要 thresholding\",\"支持 soft mask 输入\",\"✔️ 强调分布匹配\",\"不仅看交集，还看响应强度分布\",\"✔️ 对边界模糊区域友好\",\"不像 IoU 那样依赖 hard threshold\",\"⚠️ 不直接优化最终目标\",\"不能作为 loss 使用，更适合评估\",\"AUC（Area Under ROC Curve）是 Receiver Operating Characteristic (ROC) 曲线下的面积，衡量模型对二分类问题的判别能力。\",\"AUC 的计算流程如下：\",\"对不同阈值计算 TPR 和 FPR；\",\"绘制 ROC 曲线；\",\"计算曲线下面积（AUC）；\",\"特点与作用：\",\"特性\",\"描述\",\"✔️ 不依赖特定阈值\",\"考察所有可能的 threshold 下的表现\",\"✔️ 关注排序能力\",\"判断模型是否能正确区分前景和背景\",\"✔️ 适用于 binary 分类\",\"需要先将 soft mask 转换为 binary\",\"⚠️ 对 small region 敏感度有限\",\"需结合 mIoU 使用\",\"mIoU（mean Intersection over Union）是图像/点云分割中最常用的指标之一，衡量预测区域与真实标签之间的空间重合度。\",\"公式如下：\",\"其中：\",\"：预测的 binary mask；\",\"：真实的 binary mask；\",\"通常我们会使用多个 threshold（如 np.linspace(0, 1, 20)），然后取平均得到 aiou（average IoU）。\",\"特点与作用：\",\"特性\",\"描述\",\"✔️ 直接评价分割精度\",\"最贴近实际应用需求\",\"✔️ 对边界敏感\",\"能反映边缘响应质量\",\"✔️ 易受 threshold 影响\",\"多阈值评估更稳定\",\"⚠️ 不支持 soft mask 直接输入\",\"需先 threshold 成 binary mask\",\"四个指标对比总结:\",\"指标\",\"是否支持 soft mask\",\"是否依赖 threshold\",\"是否关注分布相似性\",\"是否关注空间重合度\",\"输出范围\",\"MAE\",\"✅ 是\",\"❌ 否\",\"✅ 是\",\"❌ 否\",\"[0, ∞)\",\"SIM\",\"✅ 是\",\"❌ 否\",\"✅ 是\",\"❌ 否\",\"[0, 1]\",\"AUC\",\"✅ 是（排序）\",\"✅ 是（binary）\",\"❌ 否\",\"❌ 否\",\"[0, 1]\",\"mIoU\",\"❌ 否（需先 threshold）\",\"✅ 是\",\"❌ 否\",\"✅ 是\",\"[0, 1]\",\"结合论文理解这些指标的意义，来自论文 Table 3 的结果：\",\"方法\",\"mIoU\",\"AUC\",\"SIM\",\"MAE\",\"PointRefer（完整方法）\",\"20.8%\",\"87.3%\",\"0.629\",\"0.093\",\"这些指标共同构成了 LASO 任务的评估体系，分别从以下角度衡量模型表现：\",\"角度\",\"对应指标\",\"1. 分布一致性\",\"SIM\",\"2. 分类判别能力\",\"AUC\",\"3. 逐点误差\",\"MAE\",\"4. 区域重合度\",\"mIoU\",\"这意味着：\",\"PointRefer 不仅理解语言指令；\",\"还能生成与 GT 掩码高度匹配的功能区域；\",\"并且在 unseen object 上也具有泛化能力；\",\"在 LASO 这种类别不平衡、soft mask、语言引导的 3D 功能区域识别任务中，四个指标协同工作：\",\"MAE 衡量逐点误差；\",\"SIM 衡量分布相似性；\",\"AUC 衡量分类器排序能力；\",\"mIoU 衡量空间重合度；\",\"它们共同帮助我们判断模型是否真正理解语言引导下的功能区域语义。\",\"验证集上进行评估的核心代码实现如下:\",\" num = 0 for i, (point, _, label, question, aff_label) in enumerate(val_loader): # 1. 前向传播，得到预测的 soft mask `_3d` ∈ [B, N] _3d = model(question, point) # 2. 计算 MAE（Mean Absolute Error），衡量逐点误差 mae, point_nums = evaluating(_3d, label) total_point += point_nums total_MAE += mae.item() pred_num = _3d.shape[0] # 当前 batch 的样本数 # 3. 收集所有样本的预测结果，便于后续统一评估 results[num : num + pred_num, :, :] = _3d.unsqueeze(-1) # shape: [B, N, 1] targets[num : num + pred_num, :, :] = label.unsqueeze(-1) # shape: [B, N, 1] num += pred_num # 更新索引 # 4. 计算平均 MAE（Mean Absolute Error） mean_mae = total_MAE / total_point # 5. 计算 SIM（Similarity Metric）—— 直方图交集，衡量分布相似性 SIM_matrix = np.zeros(targets.shape[0]) for i in range(targets.shape[0]): SIM_matrix[i] = SIM(results[i], targets[i]) # SIM 函数定义见 utils.eval sim = np.mean(SIM_matrix) # 6. 初始化 AUC 和 IOU 存储数组 AUC = np.zeros((targets.shape[0], targets.shape[2])) # shape: [num_samples, 1] IOU = np.zeros((targets.shape[0], targets.shape[2])) IOU_thres = np.linspace(0, 1, 20) # 多阈值下的 IoU 计算 # 7. 将 GT 标签二值化（soft mask → binary mask） targets_binary = (targets >= 0.5).astype(int) for i in range(AUC.shape[0]): t_true = targets_binary[i].flatten() # 真实标签 p_score = results[i].flatten() # 模型输出的概率值 if np.sum(t_true) == 0: # 8. 如果当前样本没有正类（即无功能区域），标记为 nan AUC[i] = np.nan IOU[i] = np.nan else: # 9. 计算 AUC（Area Under the Curve），衡量分类器整体判别能力 auc = roc_auc_score(t_true, p_score) AUC[i] = auc # 10. 使用多个阈值计算 mIoU（mean Intersection over Union） temp_iou = [] for thre in IOU_thres: p_mask = (p_score >= thre).astype(int) # 用不同 threshold 生成 binary mask intersect = np.sum(p_mask & t_true) # 交集 union = np.sum(p_mask | t_true) # 并集 temp_iou.append(intersect / union) # IoU = intersect / union temp_iou = np.array(temp_iou) aiou = np.mean(temp_iou) # 对所有 threshold 下的 IoU 取均值 IOU[i] = aiou # 10. 最终取所有样本的 AUC 和 mIoU 均值作为最终评估指标 AUC = np.nanmean(AUC) IOU = np.nanmean(IOU) # 11. 打印当前性能指标 logger.debug(f'AUC:{AUC} | IOU:{IOU} | SIM:{sim} | MAE:{mean_mae}') current_IOU = IOU # 12. 如果当前 mIoU 超过历史最佳，则保存 best model if current_IOU > best_IOU: best_IOU = current_IOU best_model_path = save_path + '/best_model-{}.pt'.format(sign) checkpoint = { 'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'Epoch': epoch } torch.save(checkpoint, best_model_path)\",\"测试集最终评估:\",\"category_metrics, affordance_metrics, overall_metrics = evaluate(model, test_loader, device, 3) print_metrics_in_table(category_metrics, affordance_metrics, overall_metrics, logger)\"]},\"40\":{\"h\":\"复现\",\"t\":[\"设置后台运行，同时将运行时输出写入日志:\",\"nohup python -u train.py > train.log 2>&1 &\",\"在 Python 命令中， -u 是 unbuffered 的缩写。Python 存在缓存机制， sys.stdout （标准输出）是有缓存的，只有遇到换行符或者输出内容积累到一定大小时，才会将内容显示到屏幕上；而 sys.stderr （标准错误）是无缓存的，程序向 sys.stderr 输出一个字符，就会立即在屏幕上显示一个字符 1 2 3 。\",\"当在 Python 命令后加上 -u 参数（例如 python -u xx.py ），会强制标准输出也像标准错误一样，不通过缓存直接将内容打印到屏幕上 1 2 3 。这在使用 nohup 后台运行 Python 脚本时非常有用，可以避免因为输出缓存导致日志卡在某一行不输出的问题。\",\"持续追踪日志最新输出:\",\"tail -f train.log\",\"杀死训练进程:\",\"pkill -f \\\"python train.py\\\"\",\"用训练好的模型权重，进行推理 (以下代码是我自己写的一个测试代码):\",\"import os import pickle import torch import numpy as np import open3d as o3d from utils.util import read_yaml from model.PointRefer import get_PointRefer def pc_normalize(pc): \\\"\\\"\\\" 点云数据归一化处理 Args: pc: 输入点云数据，形状为 [N, 3] 的 numpy 数组 Returns: 归一化后的点云数据 \\\"\\\"\\\" centroid = np.mean(pc, axis=0) # 计算点云质心 pc = pc - centroid # 中心化 m = np.max(np.sqrt(np.sum(pc**2, axis=1))) # 计算最大半径 pc = pc / m # 归一化到单位球内 return pc def predict_affordance_mask(points, text, model_path): \\\"\\\"\\\" 预测点云的功能区域掩码 Args: points: 输入点云数据 [N, 3] text: 描述功能的文本提示 model_path: 预训练模型路径 Returns: 功能区域预测结果 [N] \\\"\\\"\\\" # 加载模型配置 dict = read_yaml(\\\"config/default.yaml\\\") dict['bs'] = 16 # 设置batch size dict['lr'] = 1e-4 # 设置学习率 dict['Epoch'] = 50 # 设置训练轮数 # 初始化PointRefer模型 model = get_PointRefer( emb_dim=dict['emb_dim'], proj_dim=dict['proj_dim'], num_heads=dict['num_heads'], N_raw=dict['N_raw'], num_affordance=dict['num_affordance'], n_groups=40 ) # 加载预训练权重 checkpoint = torch.load(model_path, map_location='cpu') model.load_state_dict(checkpoint['model']) model.eval() # 设置为评估模式 # 点云预处理 Point = pc_normalize(points) # 归一化 Point = Point.transpose() # 转置为 [3, N] Points = torch.from_numpy(Point).unsqueeze(0).float() # 增加batch维度 [1, 3, N] text_list = [text] # 文本输入格式处理 # 模型推理 pred = model(text_list, Points) pred = torch.squeeze(pred) # 去除batch维度 [N] affordance_pred = pred.cpu().detach().numpy() # 转为numpy数组 return affordance_pred def visualize_affordance(points_coordinates, affordance_pred): \\\"\\\"\\\" 可视化功能区域预测结果（渐变颜色） Args: points_coordinates: 点云坐标 [N, 3] affordance_pred: 预测结果 [N] \\\"\\\"\\\" pred_point = o3d.geometry.PointCloud() pred_point.points = o3d.utility.Vector3dVector(points_coordinates) # 颜色映射：根据预测值从灰色(背景)到红色(功能区域)渐变 color = np.zeros((2048, 3)) reference_color = np.array([255, 0, 0]) # 红色 back_color = np.array([190, 190, 190]) # 灰色 for i, aff_pred in enumerate(affordance_pred): scale_i = aff_pred color[i] = (reference_color - back_color) * scale_i + back_color pred_point.colors = o3d.utility.Vector3dVector(color.astype(np.float64) / 255.0) o3d.visualization.draw_geometries([pred_point], window_name='Predicted Affordance', width=600, height=600) def visualize_point_cloud(points, pred_mask=None): \\\"\\\"\\\" 基础点云可视化（二值化显示） Args: points: 点云数据 [N, 3] 或 [N, 4] pred_mask: 可选，预测掩码 [N] \\\"\\\"\\\" pcd = o3d.geometry.PointCloud() # 数据预处理 if points.shape[1] == 4: # 如果包含强度值 coordinates = points[:, :3].astype(np.float64) if pred_mask is None: pred_mask = points[:, 3] > 0.5 # 使用第4列作为默认掩码 else: coordinates = points.astype(np.float64) if pred_mask is None: pred_mask = np.zeros(len(points), dtype=bool) # 设置颜色：红色=功能区域，蓝色=背景 colors = np.zeros((len(points), 3)) colors[pred_mask] = [1, 0, 0] # 红色 colors[~pred_mask] = [0, 0, 1] # 蓝色 pcd.points = o3d.utility.Vector3dVector(coordinates) pcd.colors = o3d.utility.Vector3dVector(colors.astype(np.float64)) # 创建可视化窗口 vis = o3d.visualization.Visualizer() vis.create_window(window_name='LASO Prediction Visualization') vis.add_geometry(pcd) # 设置渲染参数 opt = vis.get_render_option() opt.point_size = 2.5 # 点大小 opt.background_color = np.array([1, 1, 1]) # 白色背景 vis.run() # 运行可视化 vis.destroy_window() if __name__ == '__main__': # 数据路径设置 data_root = 'LASO_dataset' # 加载标注数据 with open(os.path.join(data_root, f'anno_val.pkl'), 'rb') as f: anno = pickle.load(f) # 加载点云数据 with open(os.path.join(data_root, f'objects_val.pkl'), 'rb') as f: objects = pickle.load(f) # 示例数据（第500个样本） print(\\\"当前物体类型: \\\", anno[500][\\\"class\\\"]) print(\\\"当前物体待预测的功能区域: \\\", anno[500][\\\"affordance\\\"]) point_cloud_data = objects[anno[500][\\\"shape_id\\\"]] # visualize_point_cloud(point_cloud_data) -- 可视化当前物体点云，再决定要使用什么文本查询 # 示例文本查询 text = \\\"If I want to move this chair, which part of the chair should I hold with my hands?\\\" # 执行预测和可视化 affordance_pred = predict_affordance_mask( point_cloud_data, text, \\\"runs/train/PointRefer/best_model-try_at_6.15_23.53.29.pt\\\" ) visualize_affordance(point_cloud_data, affordance_pred)\",\"预测结果可视化:\",\"If I want to move this chair, which part of the chair should I hold with my hands?\",\"If I want to sit on this chair, which part of the chair should I sit on?\",\"本文使用的是训练了9个epoch后的模型权重进行的推理演示，后续训练完50个epoch后，会进行推理能力结果更新。\"]},\"41\":{\"h\":\"3D-Vision Language\"},\"42\":{\"h\":\"简析PointNet++\",\"t\":[\"简析PointNet++\",\"论文: https://arxiv.org/abs/1706.02413 TensorFlow 版本代码: https://github.com/charlesq34/pointnet2 Pytorch 版本代码: https://github.com/yanx27/Pointnet_Pointnet2_pytorch\"]},\"43\":{\"h\":\"背景\",\"t\":[\"在PointNet中，网络对每一个点做低维到高维的映射，进行特征学习，然后把所有点映射到高维的特征通过最大池化最终表示全局特征。从本质上来说，要么对一个点做操作，要么对所有点做操作，实际上没有局部的概念(loal context) 。同时缺少 local context 在平移不变性上也有局限性（世界坐标系和局部坐标系）。对点云数据做平移操作后，所有的数据都将发生变化，导致所有的特征，全局特征都不一样了。对于单个的物体还好，可以将其平移到坐标系的中心，把他的大小归一化到一个球中，但是在一个场景中有多个物体时则不好办，需要对哪个物体做归一化呢？\",\"PointNet++ 解决了两个问题：如何生成点集的划分（Partitioning），以及如何通过局部特征学习器（Local Feature Learner）抽象点集或局部特征。\",\"生成点集的划分（Partitioning）：\",\"点集划分是指如何将一个大的点云分割成更小的、更易于管理的子集。这个过程类似于在传统的卷积神经网络中如何处理图像的小区域（或“patches”），以便可以在这些区域上应用局部操作。PointNet++需要一种方法来有效地将点云分割成多个部分，这样可以在每个部分上独立地学习特征。\",\"通过局部特征学习器（Local Feature Learner）抽象点集或局部特征：\",\"一旦点云被划分成小的子集，PointNet++的下一个任务是学习这些子集（或局部区域）的特征。这需要一个“局部特征学习器”，它能够从每个子集中提取有用的信息或特征。这与在传统CNN中学习图像局部区域特征的过程相似。\",\"两个问题是相关联的，因为：\",\"点集的划分必须产生跨分区的共同结构：为了能够在不同的局部子集上共享权重（类似于在CNN中权重共享的概念），PointNet++在进行点集划分时，需要确保这些划分具有一定的一致性或共同结构。这意味着即使是不同的局部子集，也应该以一种方式被处理，使得在它们之间可以共享学习到的特征表示的权重。这样做的目的是提高模型的效率和泛化能力，因为学习到的特征和权重可以在多个局部区域中复用。\",\"上述即为PointNet++设计中的两个核心挑战：\",\"如何有效地对点云进行分区，以便可以在这些分区上独立地学习特征。\",\"如何设计一个能够从这些局部分区中学习有用特征的机制，同时确保这些分区的处理方式允许在它们之间共享模型权重。 \",\"为了模仿传统卷积网络中的权重共享机制以提高学习效率和模型的泛化能力。\",\"PointNet++选择PointNet作为局部特征学习器（它是无序点云数据特征提取的高效算法）。\",\"可以理解为：PointNet++应用PointNet递归地对输入集进行嵌套分区。\"]},\"44\":{\"h\":\"模型结构\",\"t\":[\"以二维欧几里得空间为例，网络的分割和分类模型\",\"网络的每一组set abstraction layers主要包括3个部分：\",\"Sample layer : 对输入点进行采样，在这些点中选出若干个中心点。\",\"Grouping layer : 利用上一步得到的中心点将点集划分成若干个区域。\",\"PointNet layer : 对上述得到的每个区域进行编码，变成特征向量。\"]},\"45\":{\"h\":\"层次化点集特征学习\",\"t\":[\"层次化结构由多个set abstraction layers组成，在每个层上，一组点云被处理和抽象，以产生一个更少元素的新集合。set abstraction layers 由 Sampling layer、Grouping layer 和 PointNet layer 三部分组成。\",\"Sampling layer ：采样层 从输入点中选取一组点，定义局部区域的形心。\",\"Grouping layer ：通过查找形心点周围的“邻近点”来构建局部区域点集。\",\"PointNet layer ：使用mini-PointNet将局部区域编码为特征向量。\"]},\"46\":{\"h\":\"Sampling layer\",\"t\":[\"使用farthest point sampling（FPS）选择𝑁个点（相比于随机采样，该方法能更好的覆盖整个点集，具体选择多少个中心点以及邻域内的数量由超参数确定）\",\"FPS是一种在点云、图像处理或其他数据集中用于抽样的算法。目的是从一个大的数据集中选出一组代表性强的点，这些点彼此之间的最小距离尽可能大。\",\"作者通过FPS来抽样点集中较为重要的点。（即任务是找到点云集中的局部区域的中心点）\",\"可能存在的问题：计算成本、样本分布偏差（可能导致样本在高密度区域内过度集中，低密度区域则过于稀缺）、参数依赖（依赖初始点和距离度量方式的选择）、可能无法捕捉重要的几何细节。\"]},\"47\":{\"h\":\"Grouping layer\",\"t\":[\"文中作者通过Ball query来查询形心的邻居点。\",\"具体做法：给定两个超参数（每个区域中点的数量𝐾和query的半径𝑟），对于某个形心，Ball query找到该查询点在半径为𝑟范围内点，该范围确保局部区域的尺度是固定的。\",\"与K最近邻（kNN）查询相比，Ball query通过固定区域尺度而不是固定邻居数量来定义邻域。kNN查询寻找最近的K个邻居，但这可能导致所选邻域的实际尺寸随点的密度变化而变化，这在处理非均匀采样的数据时可能不是最优的选择。相反，Ball query通过确保每个局部区域都有一个固定的尺度，提高了模型在空间上的泛化能力。在实现时，通常会设置一个上限K，以限制每个局部区域中考虑的点的数量，以保持计算的可管理性。\",\"可改进的地方：对点云密度变换较为敏感、对参数选择依赖性高（半径太小可能无法有效捕获足够的局部详细，太大则可能导致不相关的点增多，使局部特征的表示不够精确）、计算效率问题、均匀性假设（Ball query是基于欧氏距离的均匀性假设）\",\"欧式距离的均匀性假设：即在欧氏空间中，两点的距离反映了这两点的实际相似度或关联度。\",\"基于以下前提： \",\"空间均匀性：空间是均匀和各向同性的，即任何方向上的度量都是等价的，距离的度量不受空间中位置的影响。\",\"距离直观性：在屋里空间或某些特定的抽象空间中，两个点之间的直线距离被认为是相似度或连接强度的直观表示。\",\"规模一致性：假设空间中所有区域的尺度或特征分布具有一定的一致性，即空间中的任何距离值具有相似的含义。\",\"总结: Grouping layer的任务是通过中心点找到邻居点，并将它们组织称为局部区域集。\"]},\"48\":{\"h\":\"PointNet layer\",\"t\":[\"局部坐标系转换：局部区域中的点转换成相对于形心的局部坐标系。\",\"局部区域中的每个点将相对于形心所在位置进行调整，以反映其相对位置。\",\"实现方法：通过将局部区域中的每个点-形心点的坐标来实现。\",\"特征编码：将转换后的坐标以及点的附加特征（文中的𝐶所表示的其他信息）一起送入mini-PointNet来提取局部区域中的特征。\",\"输出：利用相对坐标与点特征相结合的方式可以捕获局部区域中点与点之间的关系。\"]},\"49\":{\"h\":\"代码实现\",\"t\":[\"sample_and_group 这个函数的作用是从输入点云中：\",\"采样一些关键点\",\"为每个关键点构建局部邻域（局部区域）\",\"提取这些局部区域中的点及其特征\",\"def sample_and_group(npoint, radius, nsample, xyz, points, returnfps=False): \\\"\\\"\\\" Input: npoint: 采样的关键点数量 radius: 构建局部邻域的半径 nsample: 每个邻域内采样的关键点数量 xyz: 点云坐标数据 , [B, N, 3] points: 点的特征数据（可选）, [B, N, D] Return: new_xyz: 采样得到的关键点坐标, [B, npoint, nsample, 3] new_points: 每个关键点对应的局部区域点和特征, [B, npoint, nsample, 3+D] \\\"\\\"\\\" B, N, C = xyz.shape S = npoint # 使用 最远点采样（FPS） 从原始点云中选出 npoint 个具有代表性的点。 fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint] new_xyz = index_points(xyz, fps_idx) # [B, npoint, 3] # 对于每个选中的关键点，使用 球查询（Ball Query） 找出它周围距离小于 radius 的所有邻近点。 # 最多保留 nsample 个点，如果不够就重复最近的点来填充。 idx = query_ball_point(radius, nsample, xyz, new_xyz) # 把刚才找到的邻近点的坐标提取出来。 grouped_xyz = index_points(xyz, idx) # [B, npoint, nsample, 3] # 把它们相对于关键点的位置进行归一化（平移中心到以关键点为原点的局部坐标系上）。 grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C) # [B, npoint, nsample, 3] # 如果有额外的点特征（比如颜色、法线等），也一并提取。 if points is not None: grouped_points = index_points(points, idx) # 把邻近点的坐标和特征拼接在一起，形成最终的局部区域表示。 new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D] else: new_points = grouped_xyz_norm if returnfps: return new_xyz, new_points, grouped_xyz, fps_idx else: return new_xyz, new_points\",\"farthest_point_sample 这个函数实现的是最远点采样（Farthest Point Sampling, FPS）, 这是 PointNet++ 中用于从点云中选择具有代表性的采样点的一种策略。它的核心思想是：在点云中逐步选择离已选点尽可能远的点，使得采样点在整个点云空间中分布尽可能均匀 。\",\"def farthest_point_sample(xyz, npoint): \\\"\\\"\\\" Input: xyz: pointcloud data, [B, N, 3] npoint: number of samples Return: centroids: sampled pointcloud index, [B, npoint] \\\"\\\"\\\" device = xyz.device B, N, C = xyz.shape centroids = torch.zeros(B, npoint, dtype=torch.long).to(device) # 存储每次选出的“最远点”的索引。 distance = torch.ones(B, N).to(device) * 1e10 # 每个点到当前所有已选中心点的最小距离，初始设为一个极大值（1e10）。 farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device) # 初始时随机选择一个点作为第一个中心点。 batch_indices = torch.arange(B, dtype=torch.long).to(device) # 批次索引，用于快速访问每个 batch 的点。 # 重复 npoint 次，最终得到 npoint 个分布尽可能均匀的采样点索引。 for i in range(npoint): # 将当前选中的“最远点”索引保存下来； centroids[:, i] = farthest # （batch,npoint) # 取出当前最远点的坐标，用于后续计算其他点到该点的距离; centroid = xyz[batch_indices, farthest, :].view(B, 1, 3) # # （batch, 1 , 3) # 计算当前中心点与所有点之间的欧氏距离平方。 dist = torch.sum((xyz - centroid) ** 2, -1) # （batch,npoint) # 如果某个点到新中心点的距离比之前记录的“最小距离”还小，就更新它。 mask = dist < distance # 在 distance 中找到最大的那个距离对应的点，这就是下一个“最远点”。 distance[mask] = dist[mask] # （batch,npoint) # 在 distance 中找到最大的那个距离对应的点，这就是下一个“最远点”。 # 返回：一个元组：(values, indices)，分别是最大值和它们的位置索引。 farthest = torch.max(distance, -1)[1] # 返回位置索引 return centroids\",\"index_points 这个函数实现的是根据给定的索引 idx，从输入点云 points 中提取对应的点，形成一个新的子集。\",\"def index_points(points, idx): \\\"\\\"\\\" Input: points: input points data, [B, N, C] idx: sample index data, [B, S] Return: new_points:, indexed points data, [B, S, C] \\\"\\\"\\\" device = points.device B = points.shape[0] view_shape = list(idx.shape) view_shape[1:] = [1] * (len(view_shape) - 1) # 将view_shape的形状从[B, S]变成[B, 1]，便于广播 repeat_shape = list(idx.shape) repeat_shape[0] = 1 # 从[B, S]变成[1, S] # 从点云中根据索引提取特定点 (看不懂下面两行代码的话，可以先去了解一下python中的高级索引机制)。 batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape) new_points = points[batch_indices, idx, :] # （batch,npoint,3) return new_points\",\"query_ball_point 这个函数的作用是从点云中找出每个查询点周围一定半径范围内的邻近点索引。这个操作被称为 球查询（Ball Query）。\",\"def query_ball_point(radius, nsample, xyz, new_xyz): \\\"\\\"\\\" Input: radius: local region radius nsample: max sample number in local region xyz: all points, [B, N, 3] new_xyz: query points, [B, S, 3] Return: group_idx: grouped points index, [B, S, nsample] \\\"\\\"\\\" device = xyz.device B, N, C = xyz.shape _, S, _ = new_xyz.shape # 查询点数量（比如通过 FPS 得到的质心） # 构造一个从 0 到 N-1 的索引数组，代表原始点云中每个点的“身份证号” # 然后复制这个索引数组到每个 batch 和每个查询点上，形成 [B, S, N] 的结构 group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1]) # 计算每个查询点（new_xyz）与原始点（xyz）之间的平方欧氏距离 # 输出形状为 [B, S, N]：每个查询点对所有原始点的距离 sqrdists = square_distance(new_xyz, xyz) # 把距离超过 radius^2 的点全部替换为 N（一个非法索引），表示“这些人离我太远了，我不感兴趣。” group_idx[sqrdists > radius ** 2] = N # 对每个查询点的邻近点按索引排序（因为前面有 N，所以小的才是有效点） # 然后只保留前 nsample 个点 group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample] # 如果某个查询点附近的点太少，有些位置被标记为 N（无效）。 # 我们就用该查询点最近的那个点（第一个点）去填充这些空缺。 group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample]) mask = group_idx == N group_idx[mask] = group_first[mask] return group_idx # （batch,npoint,nsample)\",\"sample_and_group流程图\",\"sample_and_group_all 函数的作用是将整个点云视为一个“大局部区域”，不进行采样，直接对所有点进行特征提取，用于 PointNet++ 中的全局特征学习。\",\"def sample_and_group_all(xyz, points): \\\"\\\"\\\" Input: xyz: input points position data, [B, N, 3], 点云坐标数据 points: input points data, [B, N, D], 点云的额外特征（如法线、颜色等） Return: new_xyz: sampled points position data, [B, 1, 3] new_points: sampled points data, [B, 1, N, 3+D] \\\"\\\"\\\" device = xyz.device B, N, C = xyz.shape # 创建一个全零点作为“质心” # 虽然这个点没有实际意义，但它是为了统一接口设计的一个占位符 new_xyz = torch.zeros(B, 1, C).to(device) # 把原始点云 reshape 成一个大的局部区域 grouped_xyz = xyz.view(B, 1, N, C) # 如果有额外特征（比如法线、颜色），也一并加入 if points is not None: # 终输出的 new_points 是 [B, 1, N, 3+D]，代表每个 batch 中只有一组“大区域”的点及其特征 new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1) else: new_points = grouped_xyz return new_xyz, new_points # 全局质心点（0 位置）, 所有点组成的局部区域\",\"sample_and_group_all流程图\",\"PointNetSetAbstraction（点集抽象层） 是 PointNet++ 中的核心模块 ， 它的作用是负责从输入的点云数据中采样关键点，构建它们的局部邻域区域，并通过一个小型 PointNet 提取这些区域的高维特征，从而实现点云的分层特征学习。\",\"class PointNetSetAbstraction(nn.Module): def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all): super(PointNetSetAbstraction, self).__init__() self.npoint = npoint # 采样的关键点数量 self.radius = radius # 构建局部邻域的半径 self.nsample = nsample # 每个邻域内采样的关键点数量 self.mlp_convs = nn.ModuleList() self.mlp_bns = nn.ModuleList() last_channel = in_channel # 输入点的特征维度 for out_channel in mlp: self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1)) self.mlp_bns.append(nn.BatchNorm2d(out_channel)) last_channel = out_channel self.group_all = group_all def forward(self, xyz, points): \\\"\\\"\\\" Input: xyz: input points position data, [B, C, N] points: input points data, [B, D, N] Return: new_xyz: sampled points position data, [B, C, S] new_points_concat: sample points feature data, [B, D', S] \\\"\\\"\\\" xyz = xyz.permute(0, 2, 1) # [B, N, C] if points is not None: points = points.permute(0, 2, 1) # 如果 group_all=True，则对整个点云做全局特征提取。 if self.group_all: new_xyz, new_points = sample_and_group_all(xyz, points) else: # 否则使用 FPS（最远点采样）选关键点，再用 Ball Query 找出每个点的局部邻近点。 # 参数: 质点数量，采样半径，采样点数量，点坐标，点额外特征 new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points) # 局部特征编码（Mini-PointNet） # new_xyz: sampled points position data, [B, npoint, C] # new_points: sampled points data, [B, npoint, nsample, C+D] # 把邻域点的数据整理成适合卷积的格式 [B, C+D, nsample, npoint] new_points = new_points.permute(0, 3, 2, 1) # 使用多个 Conv2d + BatchNorm + ReLU 层提取特征 for i, conv in enumerate(self.mlp_convs): bn = self.mlp_bns[i] new_points = F.relu(bn(conv(new_points))) # [B, out_channel , nsample, npoint] # 对每个局部区域内所有点的最大响应值进行池化，得到该区域的固定长度特征表示。 # 在 new_points 的第 2 个维度（即每个局部邻域内的点数量维度）上做最大池化（max pooling）。 # 输出形状为 [B, out_channel, npoint]，即每个查询点有一个特征向量。 new_points = torch.max(new_points, 2)[0] # [B, out_channel, npoint] new_xyz = new_xyz.permute(0, 2, 1) # [B, C, npoint] return new_xyz, new_points # 查询点的位置(质心) ， 每个查询点点局部特征。\",\"最终每个采样得到的关键点所在的局部领域，都会被压缩为一个固定长度的特征向量。这个特征向量代表了这个局部区域的高维特征，它包含了这个区域内所有点的信息。\"]},\"50\":{\"h\":\"单尺度分组分类模型\",\"t\":[\"PointNet++ 的 单尺度分组（SSG）架构 ，通过多层 Set Abstraction 提取点云的层次化特征，并最终输出分类结果。\",\"Single-Scale Grouping (SSG)\",\"代码实现如下:\",\"# pointnet2_cls_ssg.py class get_model(nn.Module): # num_class: 输出类别数 # normal_channel: 是否包含法线信息（默认有 (x,y,z,nx,ny,nz)，否则只有 (x,y,z)） def __init__(self,num_class,normal_channel=True): super(get_model, self).__init__() in_channel = 6 if normal_channel else 3 self.normal_channel = normal_channel # PointNet++ 的核心就是逐层提取局部特征。这里的三个 SA 层构成了一个 三层分层特征学习结构 ： self.sa1 = PointNetSetAbstraction(npoint=512, radius=0.2, nsample=32, in_channel=in_channel, mlp=[64, 64, 128], group_all=False) self.sa2 = PointNetSetAbstraction(npoint=128, radius=0.4, nsample=64, in_channel=128 + 3, mlp=[128, 128, 256], group_all=False) self.sa3 = PointNetSetAbstraction(npoint=None, radius=None, nsample=None, in_channel=256 + 3, mlp=[256, 512, 1024], group_all=True) self.fc1 = nn.Linear(1024, 512) self.bn1 = nn.BatchNorm1d(512) self.drop1 = nn.Dropout(0.4) self.fc2 = nn.Linear(512, 256) self.bn2 = nn.BatchNorm1d(256) self.drop2 = nn.Dropout(0.4) self.fc3 = nn.Linear(256, num_class) def forward(self, xyz): B, _, _ = xyz.shape if self.normal_channel: norm = xyz[:, 3:, :] xyz = xyz[:, :3, :] else: norm = None l1_xyz, l1_points = self.sa1(xyz, norm) l2_xyz, l2_points = self.sa2(l1_xyz, l1_points) l3_xyz, l3_points = self.sa3(l2_xyz, l2_points) x = l3_points.view(B, 1024) x = self.drop1(F.relu(self.bn1(self.fc1(x)))) x = self.drop2(F.relu(self.bn2(self.fc2(x)))) x = self.fc3(x) x = F.log_softmax(x, -1) return x, l3_points\",\"完整的单尺度分组分类流程为:\",\"原始点云数据，首次sample，grouping，mini-PointNet后，得到:\",\"512 个关键点的坐标\",\"512 个关键点对应的局部区域特征向量\",\"二次sample，grouping，mini-PointNet后，得到:\",\"128 个关键点的坐标\",\"128 个关键点对应的局部区域特征向量\",\"三次sample，grouping，mini-PointNet后，得到:\",\"1 个关键点的坐标\",\"1 个关键点对应的全局区域特征向量\",\"获取全局区域特征向量后，通过全连接层进行分类。\"]},\"51\":{\"h\":\"非均匀密度下稳定的特征学习\",\"t\":[\"由于点集在不同区域可能会有不同的采样密度，这种非均匀性为点集特征学习带来了显著挑战。在密集采样的区域中学到的特征可能无法很好地泛化到稀疏采样的区域，反之亦然。因此，为了解决这一问题，PointNet++提出了密度自适应PointNet层，包含两种适应性特征学习层：多尺度分组（Multi-Scale Grouping, MSG）和多分辨率分组（Multi-Resolution Grouping, MRG）。\"]},\"52\":{\"h\":\"多尺度分组 （Multi-Scale Grouping）\",\"t\":[\"MSG通过应用不同尺度的分组层（按照不同的搜索半径或领域大小对点集进行分组），然后通过对应的PointNets提取每个尺度上的特征来捕获多尺度模式。不同尺度的特征被串联形成多尺度特征向量。这种方法使网络能够通过在训练期间随机丢弃输入点（称为随机输入丢弃 - random input dropout）来学习优化的策略，以结合来自不同尺度的特征。这样，网络在训练时被呈现了不同稀疏度的点集，从而学会根据输入数据的变化自适应地加权不同尺度上检测到的模式。\",\"多尺度分组\",\"具体来说，在MSG中，网络对于每个选定的形心点，按照几个预定义的半径值来搜索周围的邻近点。每个半径定义了一个局部邻域的大小，因此每个质心将根据这些不同的半径值与其周围点形成多个点集群。这样，对于每个质心点，网络不是只捕获一个尺度上的局部特征，而是能够捕获多个尺度上的局部特征。\",\"每个尺度（或每组邻域大小）的点集群都将独立地送入对应的PointNet网络进行特征提取，之后这些不同尺度上提取的特征被串联起来，形成一个综合的多尺度特征表示。这种方法使得网络能够在细节丰富的区域（通过较小的邻域尺度捕获细节）和稀疏采样的区域（通过较大的邻域尺度避免过度稀疏的问题）中均能有效提取特征。\"]},\"53\":{\"h\":\"多尺度分组分类模型\",\"t\":[\"PointNetSetAbstractionMsg 这个模块实现了 PointNet++ 中的 多尺度特征提取机制 ：对于每个局部区域，使用多个不同大小的邻域球（multi-scale ball query），分别提取特征，然后将这些不同尺度的特征拼接在一起，以获得更强的局部几何感知能力。\",\"class PointNetSetAbstractionMsg(nn.Module): def __init__(self, npoint, radius_list, nsample_list, in_channel, mlp_list): super(PointNetSetAbstractionMsg, self).__init__() self.npoint = npoint # 要采样的质心点数量 self.radius_list = radius_list # 不同尺度的查询半径列表 self.nsample_list = nsample_list # 对应半径下最多取多少邻近点 self.conv_blocks = nn.ModuleList() self.bn_blocks = nn.ModuleList() # 为每个尺度构建一个独立的小型 PointNet（Conv2d + BN + ReLU） # 每个尺度可以有不同的网络深度和宽度 # 所有尺度的网络并行运行，最后拼接结果 for i in range(len(mlp_list)): convs = nn.ModuleList() bns = nn.ModuleList() last_channel = in_channel + 3 for out_channel in mlp_list[i]: convs.append(nn.Conv2d(last_channel, out_channel, 1)) bns.append(nn.BatchNorm2d(out_channel)) last_channel = out_channel self.conv_blocks.append(convs) self.bn_blocks.append(bns) def forward(self, xyz, points): \\\"\\\"\\\" Input: xyz: input points position data, [B, C, N] points: input points data, [B, D, N] Return: new_xyz: sampled points position data, [B, C, S] new_points_concat: sample points feature data, [B, D', S] \\\"\\\"\\\" xyz = xyz.permute(0, 2, 1) # [B, N, C] if points is not None: points = points.permute(0, 2, 1) B, N, C = xyz.shape S = self.npoint # 使用 FPS（最远点采样）选出 S 个关键点作为局部区域中心 new_xyz = index_points(xyz, farthest_point_sample(xyz, S)) new_points_list = [] for i, radius in enumerate(self.radius_list): K = self.nsample_list[i] # 对每个半径 radius，找出该尺度下每个质心点周围的邻近点 group_idx = query_ball_point(radius, K, xyz, new_xyz) grouped_xyz = index_points(xyz, group_idx) # 把这些点的坐标归一化到以质心为中心的局部坐标系下 grouped_xyz -= new_xyz.view(B, S, 1, C) # 如果有额外特征，也一并加入 if points is not None: grouped_points = index_points(points, group_idx) grouped_points = torch.cat([grouped_points, grouped_xyz], dim=-1) else: grouped_points = grouped_xyz # 对每个尺度的局部点集应用对应的 Conv2d + BN + ReLU grouped_points = grouped_points.permute(0, 3, 2, 1) # [B, D, K, S] for j in range(len(self.conv_blocks[i])): conv = self.conv_blocks[i][j] bn = self.bn_blocks[i][j] grouped_points = F.relu(bn(conv(grouped_points))) # 使用最大池化聚合局部信息，生成固定长度的特征向量 new_points = torch.max(grouped_points, 2)[0] # [B, D', S] # 所有尺度的特征保存到 new_points_list new_points_list.append(new_points) new_xyz = new_xyz.permute(0, 2, 1) # 把不同尺度学到的特征拼接在一起，形成最终的局部特征表示 new_points_concat = torch.cat(new_points_list, dim=1) # 最终输出就是： 一组新的关键点位置； 每个关键点的多尺度特征表示 return new_xyz, new_points_concat\",\"pointnet2_cls_msg 这个模型使用了 PointNet++ 的 多尺度分组（MSG）策略 ，通过多个局部区域球查询提取不同尺度的局部特征，逐层抽象后融合成全局特征，最后通过全连接层完成分类任务。\",\"# pointnet2_cls_msg.py class get_model(nn.Module): def __init__(self,num_class,normal_channel=True): super(get_model, self).__init__() in_channel = 3 if normal_channel else 0 self.normal_channel = normal_channel self.sa1 = PointNetSetAbstractionMsg(512, [0.1, 0.2, 0.4], [16, 32, 128], in_channel,[[32, 32, 64], [64, 64, 128], [64, 96, 128]]) self.sa2 = PointNetSetAbstractionMsg(128, [0.2, 0.4, 0.8], [32, 64, 128], 320,[[64, 64, 128], [128, 128, 256], [128, 128, 256]]) self.sa3 = PointNetSetAbstraction(None, None, None, 640 + 3, [256, 512, 1024], True) self.fc1 = nn.Linear(1024, 512) self.bn1 = nn.BatchNorm1d(512) self.drop1 = nn.Dropout(0.4) self.fc2 = nn.Linear(512, 256) self.bn2 = nn.BatchNorm1d(256) self.drop2 = nn.Dropout(0.5) self.fc3 = nn.Linear(256, num_class) def forward(self, xyz): B, _, _ = xyz.shape if self.normal_channel: norm = xyz[:, 3:, :] xyz = xyz[:, :3, :] else: norm = None l1_xyz, l1_points = self.sa1(xyz, norm) l2_xyz, l2_points = self.sa2(l1_xyz, l1_points) l3_xyz, l3_points = self.sa3(l2_xyz, l2_points) x = l3_points.view(B, 1024) x = self.drop1(F.relu(self.bn1(self.fc1(x)))) x = self.drop2(F.relu(self.bn2(self.fc2(x)))) x = self.fc3(x) x = F.log_softmax(x, -1) return x,l3_points\",\"MSG的关键优点在于它通过在训练期间的随机输入丢弃（即随机移除一部分输入点）来模拟不同的采样密度，从而训练网络在面对实际应用中可能遇到的各种采样密度时，能够自适应地选择最适合的特征尺度进行组合，以实现最佳的性能。这种方法大大增强了网络处理非均匀采样数据的能力，提高了模型的泛化性和稳健性。\",\"在训练时引入不同密度的点集情况，使网络能学习不同采样密度下局部点云特征的提取，捕获密集到稀疏采样区域内的多尺度信息 -- 通过随机丢弃来模拟不同密度的采样，使网络能够应对实际中各种密度变换的情况-提高模型的泛化性能。\",\"MSG相当于并联了多个hierarchical structure，每个结构中心点不变，但是尺度不同。通过PointNet获取每个形心多尺度信息，之后concat形成该区域提取的总特征。在训练时引入随机丢弃形心来模拟不同密度情况，提高算法鲁棒性。\"]},\"54\":{\"h\":\"多分辨率分组（Multi-Resolution Grouping）\",\"t\":[\"MSG方法虽然有效，但在计算上可能非常昂贵，尤其是在低层次上对每个质心点运行局部PointNet时。为此，MRG为一种低成本的替代方案。\",\"MRG通过结合来自不同分辨率的特征来实现效率和适应性的平衡。具体而言，MRG策略在处理每个局部区域时，不仅考虑从当前分辨率下抽象得到的特征，还考虑了从更低分辨率（即上一层级）直接提取的特征。这两种特征被concat为一个复合特征向量，为后续的处理步骤提供信息。\",\"多分辨率分组\",\"在MRG中，某一层次𝐿𝑖的区域特征是通过将来自下一级𝐿𝑖−1的子区域特征总结后的向量与直接处理该局部区域所有原始点的单个PointNet得到的特征向量进行concat得到的。当局部区域的密度较低时，由于子区域在计算第一个向量时包含的点更稀疏，因此可能比第二个向量更不可靠。在这种情况下，应该更高地加权第二个向量。相反，当局部区域的密度较高时，第一个向量提供了更细致的信息，因为它能够在更低层次上递归地检视更高分辨率。\",\"来自下一级的特征：首先，将来自下一级（更高分辨率）的特征进行汇总，形成一个特征向量。这一过程通过对每个子区域应用集合抽象层（set abstraction level）完成。\",\"直接处理的原始点特征：另一部分特征是通过在当前分辨率直接对所有原始点应用单个PointNet得到的。\"]},\"55\":{\"h\":\"点云语义分割\",\"t\":[\"PointNet++ 完成点云分割任务的过程是一个典型的“编码-解码”结构，结合了层级特征提取和多尺度融合机制。\",\"目标: 给定一个点云，模型需要为每个点预测一个类别标签（如桌子、椅子、墙壁等）。\",\"输入：xyz: [B, N, 3]\",\"输出：labels: [B, N, C]，其中 C 是类别数\",\"PointNet++ 分割的整体结构 :\",\"Input Points (xyz): [ B, N, 3 ] ↓ Set Abstraction Layers（编码器） ↓ Feature Vectors at Multiple Scales ↓ Feature Propagation Layers（解码器） ↓ Recovered Features at Original Resolution ↓ MLP + Softmax → Per-point Semantic Labels\",\"第一步：Set Abstraction（集合抽象）—— 编码器: 对点云进行下采样，并在每个局部区域提取特征。\",\"核心操作包括：\",\"FPS（Farthest Point Sampling）：从点云中选出有代表性的点作为中心点。\",\"Ball Query：为每个中心点找到其邻域内的点。\",\"Grouping：将邻域点组合成局部点云组。\",\"PointNet 操作：使用 T-Net 对局部点云进行变换，然后通过 MLP 提取特征。\",\"Pooling：对局部点云组做最大池化或平均池化，得到该区域的特征。\",\"多个 Set Abstraction 层堆叠，逐步减少点的数量，增加特征维度，形成多尺度特征表示。\",\"第二步：Feature Propagation（特征传播）—— 解码器: 从最稀疏的点开始，逐层将特征插值回原始点数量。\",\"特征插值方式：\",\"使用 反距离加权插值（IDW），即根据最近的几个邻近点的距离进行加权平均。\",\"可选地拼接 skip connection 中的原始特征（来自 Set Abstraction 前的某一层）。\",\"输入输出示例：\",\"def forward(xyz1, xyz2, points1, points2): # xyz1: 原始点坐标（多） # xyz2: 下采样点坐标（少） # points1: 原始点特征（可为空） # points2: 下采样点特征 return interpolated_points # 插值得到的密集特征，形状与 xyz1 一致\",\"多个 Feature Propagation 层堆叠，逐渐恢复点数，最终回到原始点数量。\",\"第三步：Head 预测头 —— 分类每个点: 对每个点的特征做一个简单的分类器，输出类别概率。\",\"实现方式：\",\"将最后一层 Feature Propagation 输出的特征送入一个小型 MLP。\",\"最后一层使用 Softmax（对于多分类）或 Sigmoid（对于多标签）激活函数。\",\"例如：\",\"mlp = nn.Sequential( nn.Conv1d(128, 128, 1), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.5), nn.Conv1d(128, num_classes, 1) ) logits = mlp(final_features) # shape: [B, C, N]\"]},\"56\":{\"h\":\"代码实现\",\"t\":[\"PointNet++ 的整体结构是一个典型的 编码器-解码器（Encoder-Decoder）架构 ：\",\"Set Abstraction 层 ：不断对点云进行下采样 + 提取局部特征（编码过程）\",\"Feature Propagation 层 ：从最稀疏的点开始，逐层恢复到原始点数（解码过程）\",\"[Input Points] ↓ SA Layer 1 → [Points: 1024 → 512] ↓ SA Layer 2 → [Points: 512 → 128] ↓ SA Layer 3 → [Points: 128 → 32] ↓ FP Layer 3 ← [Points: 32 → 128] ↓ FP Layer 2 ← [Points: 128 → 512] ↓ FP Layer 1 ← [Points: 512 → 1024] ↓ [Per-point Classification Head] ↓ [Output: per-point labels]\"]},\"57\":{\"h\":\"特征传播层\",\"t\":[\"PointNetFeaturePropagation 是 PointNet++ 中用于点云“特征传播”（Feature Propagation）的核心模块，主要作用是：\",\"将稀疏点集的特征插值回原始点集的位置上。\",\"换句话说：\",\"输入：少量点的坐标 + 特征（如经过下采样后的点）\",\"输出：在原始点数量下的每个点都拥有一个合理的特征向量\",\"这一步相当于图像任务中的 上采样（upsample）或转置卷积（transpose convolution） ，但在点云这种非结构化数据中，不能直接使用这些操作。\",\"class PointNetFeaturePropagation(nn.Module): def __init__(self, in_channel, mlp): \\\"\\\"\\\" 初始化函数，构建用于特征传播（上采样）的MLP层 参数： in_channel: 输入特征的通道数（维度） mlp: 一个列表，表示每一层MLP的输出通道数，例如 [64, 128] \\\"\\\"\\\" super(PointNetFeaturePropagation, self).__init__() # 用于保存卷积层和批归一化层 self.mlp_convs = nn.ModuleList() self.mlp_bns = nn.ModuleList() last_channel = in_channel # 当前输入通道数初始化为in_channel # 构建MLP层：每个层是一个Conv1d + BatchNorm1d + ReLU for out_channel in mlp: self.mlp_convs.append(nn.Conv1d(last_channel, out_channel, 1)) self.mlp_bns.append(nn.BatchNorm1d(out_channel)) last_channel = out_channel # 更新下一层的输入通道数 def forward(self, xyz1, xyz2, points1, points2): \\\"\\\"\\\" 前向传播函数：将稀疏点集points2插值到密集点集xyz1的位置上 参数： xyz1: 原始点坐标数据，形状 [B, C, N] （如 1024 个点） xyz2: 下采样后的点坐标数据，形状 [B, C, S] （如 256 个点） points1: 原始点对应的特征数据，形状 [B, D, N] （可为 None） points2: 下采样点对应的特征数据，形状 [B, D, S] 返回： new_points: 插值并融合后的特征，形状 [B, D', N] \\\"\\\"\\\" # 将坐标和特征从 [B, C, N] 转换为 [B, N, C] 格式，便于后续计算 xyz1 = xyz1.permute(0, 2, 1) # [B, N, C] xyz2 = xyz2.permute(0, 2, 1) # [B, S, C] points2 = points2.permute(0, 2, 1) # [B, S, D] B, N, C = xyz1.shape # 原始点数量 N _, S, _ = xyz2.shape # 下采样点数量 S # 如果只有1个下采样点，直接复制其特征到所有原始点 if S == 1: interpolated_points = points2.repeat(1, N, 1) # [B, N, D] else: # 计算原始点与下采样点之间的距离矩阵（欧氏距离平方） dists = square_distance(xyz1, xyz2) # [B, N, S] # 对每个原始点，找到最近的3个邻近点 dists, idx = dists.sort(dim=-1) dists = dists[:, :, :3] # 取最小的三个距离 [B, N, 3] idx = idx[:, :, :3] # 取对应的索引 [B, N, 3] # 使用反距离加权（IDW）计算权重: # 1.将距离转换为“权重”，距离越近，权重越大 dist_recip = 1.0 / (dists + 1e-8) # 避免除以零 # 2.对每个点的3个权重求和，得到归一化因子 norm = torch.sum(dist_recip, dim=2, keepdim=True) # 归一化因子 # 3.归一化权重，使得每个点的权重之和为1 weight = dist_recip / norm # 加权平均系数 [B, N, 3] # 为每个原始点，找到它最近的 3 个邻近点，根据距离分配权重，然后对它们的特征做加权平均，从而插值得到该点的特征。 # index_points: [B, S, D] -> [B, N, 3, D] # weight.view(B, N, 3, 1): 扩展维度后相乘 interpolated_points = torch.sum( # 1. 从下采样点中取出每个原始点对应的最近邻点的特征。 # points2: [B, S, D] —— 下采样点的特征（S 个点，每个点有 D 维特征） # idx: [B, N, 3] —— 每个原始点对应的 3 个最近邻点索引 # [B, N, 3, D] —— 每个原始点都有了它最近的 3 个邻近点的特征 index_points(points2, idx) # 将之前计算好的权重扩展维度，以便和特征相乘。 # weight: [B, N, 3] —— 每个点的三个邻近点的权重 # [B, N, 3, 1] —— 扩展后便于广播乘法 * weight.view(B, N, 3, 1), dim=2 ) # [B, N, D] # 如果原始点有特征，则拼接起来（skip connection） if points1 is not None: points1 = points1.permute(0, 2, 1) # [B, N, D] new_points = torch.cat([points1, interpolated_points], dim=-1) # [B, N, D1+D2] else: new_points = interpolated_points # [B, N, D] # 恢复张量格式为 [B, D, N]，以适配后面的卷积操作 new_points = new_points.permute(0, 2, 1) # [B, D', N] # 经过MLP进一步提取和融合特征 for i, conv in enumerate(self.mlp_convs): bn = self.mlp_bns[i] new_points = F.relu(bn(conv(new_points))) # Conv1d + BN + ReLU return new_points # 最终输出特征 [B, D', N]\",\"流程四步走：\",\"1️⃣ 找到邻居 “我这个点最近的3个熟人是谁？”\",\"计算每个原始点和下采样点之间的距离；\",\"找出最近的3个邻近点。\",\"2️⃣ 分配权重 “谁离我越近，说话越有分量。”\",\"根据距离反比加权（IDW），给这3个邻近点分配权重；\",\"权重归一化，确保它们加起来是1。\",\"3️⃣ 加权平均插值 “综合最近几个熟人的意见，猜出我的特征。”\",\"提取邻近点的特征；\",\"按照权重做加权平均；\",\"得到每个原始点的插值特征。\",\"4️⃣ 融合+增强 “如果我本来就有特征，那就一起用；再用MLP提提神。”\",\"如果原始点有自己的特征（points1），就拼接起来；\",\"经过几层 Conv1d + BN + ReLU，进一步提取和融合特征；\",\"输出最终的插值后特征。\",\"📦 输出结果\",\"new_points: 每个原始点都有了一个新的特征向量 [B, D', N]\"]},\"58\":{\"h\":\"点云语义分割模型\",\"t\":[\"下面给出的是一个基于PointNet++的点云语义分割模型定义 ，其主要功能是：\",\"对输入点云中的每个点进行分类（如桌子、椅子、地板等），输出每个点的类别概率。\",\"网络结构特点：\",\"使用 Set Abstraction（SA）层 进行多尺度特征提取和下采样；\",\"使用 Feature Propagation（FP）层 进行特征插值和上采样；\",\"最后通过两个卷积层输出每个点的分类结果；\",\"输出为 [B, N, num_classes]，即每个点都有一个类别预测。\",\"class get_model(nn.Module): def __init__(self, num_classes): \\\"\\\"\\\" 初始化 PointNet++ 分割网络 参数： num_classes: 分类类别数 \\\"\\\"\\\" super(get_model, self).__init__() # Set Abstraction 层（编码器部分） # 每层逐步下采样，并提取更高级别的局部特征 self.sa1 = PointNetSetAbstraction(npoint=1024, radius=0.1, nsample=32, in_channel=9+3, mlp=[32, 32, 64], group_all=False) self.sa2 = PointNetSetAbstraction(npoint=256, radius=0.2, nsample=32, in_channel=64+3, mlp=[64, 64, 128], group_all=False) self.sa3 = PointNetSetAbstraction(npoint=64, radius=0.4, nsample=32, in_channel=128+3, mlp=[128, 128, 256], group_all=False) self.sa4 = PointNetSetAbstraction(npoint=16, radius=0.8, nsample=32, in_channel=256+3, mlp=[256, 256, 512], group_all=False) # Feature Propagation 层（解码器部分） # 从稀疏点恢复到原始点密度，逐层融合上下文信息 self.fp4 = PointNetFeaturePropagation(in_channel=768, mlp=[256, 256]) self.fp3 = PointNetFeaturePropagation(in_channel=384, mlp=[256, 256]) self.fp2 = PointNetFeaturePropagation(in_channel=320, mlp=[256, 128]) self.fp1 = PointNetFeaturePropagation(in_channel=128, mlp=[128, 128, 128]) # 最终分类头 self.conv1 = nn.Conv1d(128, 128, 1) self.bn1 = nn.BatchNorm1d(128) self.drop1 = nn.Dropout(0.5) self.conv2 = nn.Conv1d(128, num_classes, 1) def forward(self, xyz): \\\"\\\"\\\" 前向传播函数 输入： xyz: 点云数据，形状 [B, C, N] 返回： x: 每个点的分类结果，形状 [B, N, num_classes] l4_points: 最后一层抽象特征，用于其他任务 \\\"\\\"\\\" # l0 表示最原始的点云 l0_points = xyz l0_xyz = xyz[:, :3, :] # 只取 xyz 坐标，不带法向量或其他属性 # 编码器：层层下采样并提取特征 l1_xyz, l1_points = self.sa1(l0_xyz, l0_points) # 1024 points l2_xyz, l2_points = self.sa2(l1_xyz, l1_points) # 256 points l3_xyz, l3_points = self.sa3(l2_xyz, l2_points) # 64 points l4_xyz, l4_points = self.sa4(l3_xyz, l3_points) # 16 points # 解码器：层层插值并融合特征 l3_points = self.fp4(l3_xyz, l4_xyz, l3_points, l4_points) # 64 → 64 l2_points = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points) # 256 → 256 l1_points = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points) # 1024 → 1024 l0_points = self.fp1(l0_xyz, l1_xyz, None, l1_points) # 4096 → 4096 # MLP 头部处理：进一步增强特征 x = self.drop1(F.relu(self.bn1(self.conv1(l0_points)), inplace=True)) # [B, 128, N] x = self.conv2(x) # [B, num_classes, N] # Softmax 分类 x = F.log_softmax(x, dim=1) # [B, num_classes, N] # 调整维度，返回 [B, N, num_classes] x = x.permute(0, 2, 1) return x, l4_points # 返回每个点的分类结果和抽象特征\"]},\"59\":{\"h\":\"简析PointNet\",\"t\":[\"简析PointNet网络模型及其背后原理\",\"论文: https://arxiv.org/abs/1612.00593 TensorFlow 版本代码: https://github.com/charlesq34/pointnet Pytorch 版本代码: https://github.com/fxia22/pointnet.pytorch\"]},\"60\":{\"h\":\"核心\",\"t\":[\"问题背景: 点云是三维几何数据的一种重要表示形式，但由于其无序性和非规则性，传统卷积神经网络难以直接处理。\",\"❌ 传统方法的缺陷 ：\",\"将点云转换为体素网格（voxel grid）或图像视图（multi-view rendering）， 这些方法会导致信息损失、计算量大、不灵活等问题。\",\"🌟 PointNet 的创新点 ：\",\"直接以点集作为输入，避免了复杂的预处理；\",\"设计了一个统一架构，适用于分类、物体分割和场景语义解析；\",\"利用对称函数（如最大池化）实现点集顺序不变性；\",\"引入 T-Net（空间变换网络）标准化输入点云和特征空间。\"]},\"61\":{\"h\":\"难点\",\"t\":[\"点云的无序性（Unordered）: 点云是点的集合，没有固定顺序；模型必须对输入点的排列顺序不敏感（permutation invariant）。\",\"点之间存在相互作用（Interaction among points）: 点与点之间有空间关系，需要捕捉局部结构。\",\"对几何变换的不变性（Invariance under transformations）: 模型输出应不受刚性变换影响（如旋转、平移）。\",\"输入点云可能缺失或包含噪声（Missing or noisy points）: 实际采集的点云常有遮挡、稀疏、异常值等问题。\"]},\"62\":{\"h\":\"解决方案\",\"t\":[\"✅ 难点 1：点云的无序性 → 使用对称函数（Symmetric Function）\",\"使用 max pooling 作为对称函数，聚合所有点的信息；\",\"所有点经过共享参数的 MLP 提取特征；\",\"最终输出与点的顺序无关；\",\"原理说明：\",\"f({x1, ..., xn}) ≈ g(h(x1), ..., h(xn)) = γ(MAX(h(x1), ..., h(xn)))\",\"其中：\",\"h(xi) 是每个点的高维特征；\",\"MAX 是 max pooling 函数；\",\"γ 是后续的全连接网络；\",\"整个函数 f 是对称的，即对点顺序不敏感。\",\"效果：\",\"实验证明 max pooling 比排序、RNN、average pooling 更有效；s\",\"PointNet 可以处理任意顺序的点集；\",\"✅ 难点 2：点之间的相互作用 → 设计局部 + 全局信息融合机制\",\"在分割任务中，将全局特征与每个点的局部特征拼接起来；\",\"这样每个点在预测标签时都能看到整个物体的上下文；\",\"效果：\",\"显著提升了分割性能；\",\"让模型既关注局部细节，又理解整体结构；\",\"✅ 难点 3：对几何变换的不变性 → 引入 T-Net（空间变换网络）\",\"引入两个空间变换网络： \",\"STN3d：对输入点云做刚性变换（3×3 矩阵）；\",\"STNkd：对特征空间做变换（64×64 矩阵）；\",\"加入正则项约束变换矩阵接近正交：\",\"L_reg = ||I - A @ A^T||_F^2\",\"效果：\",\"PointNet 对点云的旋转、平移等变换具有鲁棒性；\",\"提升了模型的泛化能力和稳定性；\",\"✅ 难点 4：输入点云可能缺失或含有异常点 → 理论分析保证模型鲁棒性\",\"理论上证明 PointNet 学到的是一个“关键点集”（critical point set），即只依赖一小部分关键点就能判断整体形状；\",\"即使丢失一些点或加入异常点，只要关键点还在，结果就不会变；\",\"定理表明：\",\"小扰动不会改变函数输出；\",\"网络输出由一个有限子集 CS 决定（大小不超过 bottleneck 维度 K）；\",\"CS 是关键点集合，NS 是最大可容忍的点云范围；\",\"实验验证：\",\"即使 50% 的点缺失，分类准确率仅下降约 3.7%；\",\"对异常点也有一定容忍能力；\",\"✅ 总结: PointNet 通过 max pooling 实现对称性，结合 T-Net 实现变换不变性，并通过局部+全局特征融合机制实现强大的点云建模能力，解决了点云处理中的四大技术难点，为后续三维深度学习奠定了基础。\"]},\"63\":{\"h\":\"代码(Pytorch版本)\",\"t\":[\"PointNet网络模型结构图\"]},\"64\":{\"h\":\"输入标准化\",\"t\":[\"在 PointNet 架构中，第一层是一个叫做 STN3d（Spatial Transformer Network for 3D points） 的模块，它的目标是：\",\"✅ 对输入的点云做刚性变换（如旋转 + 平移），使其姿态统一，提升模型鲁棒性。\",\"这是因为在实际采集过程中，点云的姿态可能各不相同（比如椅子朝向不同、扫描角度不同等），如果不加处理，会影响特征提取的一致性。\",\"STN3d 是一个小型神经网络，专门用于预测一个 3×3 的变换矩阵 ，这个矩阵表示对点云所做的变换（通常是旋转或反射）。\",\"它具有以下特点：\",\"输入是原始点云（shape: (B, 3, N)）；\",\"输出是一个变换矩阵（shape: (B, 3, 3)）；\",\"这个变换矩阵是近似正交的，保证变换是刚性的；\",\"变换矩阵会通过 torch.bmm() 应用到原始点云上（这一步不在 STN3d 类中）；\",\"目的是让点云“摆正”，便于后续处理。\",\"代码实现:\",\"class STN3d(nn.Module): def __init__(self): super(STN3d, self).__init__() # 使用 1D 卷积 处理点云数据（每个点有 3 个坐标值） # kernel_size=1 表示只在通道维度操作，不考虑空间邻域关系 # 提取每一点的特征向量（从 3 → 64 → 128 → 1024） self.conv1 = torch.nn.Conv1d(3, 64, 1) self.conv2 = torch.nn.Conv1d(64, 128, 1) self.conv3 = torch.nn.Conv1d(128, 1024, 1) # 经过全局池化后得到一个全局特征向量（1024维） # 用全连接层逐步压缩到 9 个输出 → 对应一个 3x3 的变换矩阵 self.fc1 = nn.Linear(1024, 512) self.fc2 = nn.Linear(512, 256) self.fc3 = nn.Linear(256, 9) # 所有卷积和 FC 层后面都加了 BN 和 ReLU，帮助训练稳定收敛 self.relu = nn.ReLU() self.bn1 = nn.BatchNorm1d(64) self.bn2 = nn.BatchNorm1d(128) self.bn3 = nn.BatchNorm1d(1024) self.bn4 = nn.BatchNorm1d(512) self.bn5 = nn.BatchNorm1d(256) # x: (batch,3,point_size) def forward(self, x): # 获取当前 batch 的大小（有多少组点云） batchsize = x.size()[0] # CNN 逐点，通道维度特征提取阶段 x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) # x: (batch,1024,point_size) # 全局最大池化（Global Max Pooling） # 从所有点中选出每个通道的最大响应值，作为整个点云的“抽象”表示，shape: (B, 1024) # 返回：一个元组：(values, indices)，分别是最大值和它们的位置索引。 x = torch.max(x, 2, keepdim=True)[0] # x: (batch,1024,1) x = x.view(-1, 1024) # x: (batch,1024) # 全连接层预测变换矩阵 x = F.relu(self.bn4(self.fc1(x))) x = F.relu(self.bn5(self.fc2(x))) x = self.fc3(x) # x: (batch,9) # 加上单位矩阵作为初始偏置 # 初始假设变换为恒等变换（不做任何变化） iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1) if x.is_cuda: iden = iden.cuda() # 让网络从一个小扰动开始学习，更容易训练 x = x + iden # 最终 reshape 成 3x3 矩阵返回 x = x.view(-1, 3, 3) return x\",\"标准化的意义:\",\"✅ 1. 解决点云姿态不一致问题\",\"输入点云可能来自不同角度、不同位置；\",\"T-Net 把它们“对齐”到一个标准姿态；\",\"这样 PointNet 后续的特征提取更稳定。\",\"✅ 2. 提升模型鲁棒性\",\"如果没有 T-Net，PointNet 必须自己学会对各种姿态都识别准确；\",\"加入 T-Net 后，相当于加了一个“预处理层”，让模型更容易训练和泛化。\",\"神经网络的输出在训练初期往往接近于零，如果直接作为变换矩阵，会导致非正交、不稳定。PointNet 通过“加单位矩阵”的方式，让变换矩阵从一个恒等变换开始学习，并结合正则化损失，逐步向正交矩阵靠拢，从而保证变换是刚性的、稳定的。\"]},\"65\":{\"h\":\"正则化损失\",\"t\":[\"feature_transform_regularizer 是 PointNet 中用于约束变换矩阵接近正交性的正则化损失函数。\",\"🧠 为什么需要这个正则化项？\",\"在 PointNet 中，为了提升模型对点云姿态变化的鲁棒性，引入了两个变换网络：\",\"STN3d: 对原始点云做刚性变换（如旋转、反射），使其标准化。\",\"STNkd: 对特征空间做变换，使特征分布更稳定。\",\"这两个网络输出的是变换矩阵（分别是 3×3 和 k×k 的矩阵）。但由于它们是神经网络直接预测出来的，并不能保证这些矩阵是正交矩阵（orthogonal matrix） 。\",\"❗而只有正交矩阵才能表示刚性变换（rigid transformation），即只改变物体的方向而不改变形状和大小。\",\"所以我们需要加一个正则化项来鼓励变换矩阵接近正交矩阵 , 这就是 feature_transform_regularizer 的作用！\",\"def feature_transform_regularizer(trans): d = trans.size()[1] batchsize = trans.size()[0] # 构造一个单位矩阵 I，用于后续比较； # 添加 None 是为了扩展成 (1, d, d)，便于广播到整个 batch； I = torch.eye(d)[None, :, :] if trans.is_cuda: I = I.cuda() # 计算变换矩阵与其转置相乘后与单位矩阵之间的距离（Frobenius 范数），然后取 batch 平均值作为损失项，鼓励变换矩阵接近正交矩阵。 # Frobenius 范数（矩阵所有元素平方和开方） loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2))) return loss\"]},\"66\":{\"h\":\"特征提取\",\"t\":[\"PointNet 的核心特征提取模块 PointNetfeat ，它负责从输入点云中提取出可用于分类或分割的特征。\",\"class PointNetfeat(nn.Module): def __init__(self, global_feat = True, feature_transform = False): super(PointNetfeat, self).__init__() # 输入点云变换网络（3D） self.stn = STN3d() # 使用 Conv1D 对每个点进行特征提取； # 每个卷积层后跟一个 BatchNorm 层； # 最终输出高维特征（1024维）； self.conv1 = torch.nn.Conv1d(3, 64, 1) self.conv2 = torch.nn.Conv1d(64, 128, 1) self.conv3 = torch.nn.Conv1d(128, 1024, 1) self.bn1 = nn.BatchNorm1d(64) self.bn2 = nn.BatchNorm1d(128) self.bn3 = nn.BatchNorm1d(1024) # 全局特征开关：控制是否输出全局特征 self.global_feat = global_feat # 特征变换开关：控制是否使用 STN 对特征空间进行变换 self.feature_transform = feature_transform if self.feature_transform: self.fstn = STNkd(k=64) def forward(self, x): n_pts = x.size()[2] # 使用 STN3d 预测出一个变换矩阵； trans = self.stn(x) x = x.transpose(2, 1) # 将原始点云“摆正”； x = torch.bmm(x, trans) x = x.transpose(2, 1) # 再通过第一个卷积层提取初始特征； x = F.relu(self.bn1(self.conv1(x))) if self.feature_transform: trans_feat = self.fstn(x) x = x.transpose(2,1) x = torch.bmm(x, trans_feat) x = x.transpose(2,1) else: trans_feat = None # 提取更高维的特征； # 最后一层输出 shape: (B, 1024, N) pointfeat = x x = F.relu(self.bn2(self.conv2(x))) x = self.bn3(self.conv3(x)) # 从所有点中选出每个通道的最大响应值，作为整个点云的“抽象”表示，shape: (B, 1024) # 返回：一个元组：(values, indices)，分别是最大值和它们的位置索引。 x = torch.max(x, 2, keepdim=True)[0] x = x.view(-1, 1024) # 如果是分类任务 (global_feat=True)： if self.global_feat: return x, trans, trans_feat else: # 如果是分割任务 (global_feat=False)： x = x.view(-1, 1024, 1).repeat(1, 1, n_pts) return torch.cat([x, pointfeat], 1), trans, trans_feat\",\"✅ 如果是分类任务 (global_feat=True)，则返回：\",\"x: 全局特征 (B, 1024)\",\"trans: 输入点云变换矩阵\",\"trans_feat: 特征空间变换矩阵（可选）\",\"✅ 如果是分割任务 (global_feat=False)， 则返回：\",\"把全局特征复制 N 次并与每个点的局部特征，在通道维度进行拼接\",\"将每个点的局部特征与整个点云的全局特征拼接起来，让每个点都能看到上下文信息\",\"输出 shape: (B, 1088, N) ，即 1088 = 1024+64\"]},\"67\":{\"h\":\"分类任务\",\"t\":[\"PointNet 的分类模块 PointNetCls ，它基于前面的特征提取模块 PointNetfeat 来完成点云分类任务。\",\"class PointNetCls(nn.Module): def __init__(self, k=2, feature_transform=False): super(PointNetCls, self).__init__() self.feature_transform = feature_transform # 它使用 PointNetfeat 提取全局特征（1024维）； self.feat = PointNetfeat(global_feat=True, feature_transform=feature_transform) # 然后通过全连接层（MLP）将这些特征映射到类别空间； self.fc1 = nn.Linear(1024, 512) self.fc2 = nn.Linear(512, 256) self.fc3 = nn.Linear(256, k) self.dropout = nn.Dropout(p=0.3) self.bn1 = nn.BatchNorm1d(512) self.bn2 = nn.BatchNorm1d(256) self.relu = nn.ReLU() def forward(self, x): # 它使用 PointNetfeat 提取全局特征（1024维）； x, trans, trans_feat = self.feat(x) # 然后通过全连接层（MLP）将这些特征映射到类别空间； x = F.relu(self.bn1(self.fc1(x))) x = F.relu(self.bn2(self.dropout(self.fc2(x)))) x = self.fc3(x) # 最终输出每个类别的概率分布（log_softmax）； return F.log_softmax(x, dim=1), trans, trans_feat\"]},\"68\":{\"h\":\"分割任务\",\"t\":[\"PointNet 的分割模块 PointNetDenseCls ，它基于前面的特征提取模块 PointNetfeat 来完成点云物体分割任务。\",\"class PointNetDenseCls(nn.Module): def __init__(self, k = 2, feature_transform=False): super(PointNetDenseCls, self).__init__() self.k = k self.feature_transform=feature_transform self.feat = PointNetfeat(global_feat=False, feature_transform=feature_transform) self.conv1 = torch.nn.Conv1d(1088, 512, 1) self.conv2 = torch.nn.Conv1d(512, 256, 1) self.conv3 = torch.nn.Conv1d(256, 128, 1) self.conv4 = torch.nn.Conv1d(128, self.k, 1) self.bn1 = nn.BatchNorm1d(512) self.bn2 = nn.BatchNorm1d(256) self.bn3 = nn.BatchNorm1d(128) def forward(self, x): batchsize = x.size()[0] n_pts = x.size()[2] # 点的数量 # 调用 PointNetfeat 提取特征 # 最后将每个点的局部特征与整个点云的全局特征拼接起来，让每个点都能看到上下文信息 x, trans, trans_feat = self.feat(x) # 使用多层 Conv1D 层进一步融合局部 + 全局信息 # 最终输出 shape: (B, k, N) x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) x = self.conv4(x) # shape: (B, k, N) -> (B, N, k) , 即每个点的各个类别得分 x = x.transpose(2,1).contiguous() # 使用 log_softmax 得到 log 概率分布； x = F.log_softmax(x.view(-1,self.k), dim=-1) # shape: (B*N, k) x = x.view(batchsize, n_pts, self.k) # shape: (B, N, k) return x, trans, trans_feat\",\"✅ 1. 每个点都需要全局上下文\",\"仅靠局部特征很难判断某个点属于哪个部件（比如椅子的腿 vs 座位）；\",\"加上全局特征后，相当于告诉模型：“你知道吗，这是一个椅子”；\",\"这样模型就能根据上下文更准确地做出判断；\",\"✅ 2. 全局特征不能直接用于分割\",\"全局特征只有一份（(B, 1024)），无法直接用于每个点；\",\"所以要把它复制 N 次，变成 (B, 1024, N)；\",\"再与每个点的局部特征拼接；\"]},\"69\":{\"h\":\"缺陷\",\"t\":[\"🧠 一、核心问题：忽略局部结构信息\",\"PointNet 只通过 max pooling 聚合所有点的信息，忽略了局部邻域之间的结构关系。\",\"🔍 原因分析：\",\"PointNet 对每个点独立处理（参数共享），然后使用全局最大池化（Global Max Pooling）提取特征；\",\"这种设计使得网络只关注“最显著的点”，而没有建模点与点之间的局部几何关系；\",\"导致模型无法捕捉到更细粒度的几何细节，比如边缘、曲率、表面纹理等；\",\"💡 论文中的验证：\",\"在部件分割任务中，虽然 PointNet 表现不错，但在一些复杂区域（如椅子腿和桌面连接处）容易出错；\",\"分类任务中对缺失点具有一定鲁棒性，但遇到遮挡严重或点分布不均匀时性能下降明显；\",\"📉 二、分割任务依赖拼接机制，不够精细\",\"PointNet 的分割模块通过拼接全局特征 + 局部特征实现上下文感知，但这种方式表达能力有限。\",\"🔍 原理回顾：\",\"PointNet 的分割网络将全局特征复制 N 次并与每个点的局部特征拼接；\",\"然后使用 Conv1D 进行分类；\",\"实际上是用一个固定大小的全局特征去“广播”给每个点；\",\"⚠️ 问题所在：\",\"全局特征不能很好地反映每个点的上下文；\",\"拼接方式缺乏动态调整机制；\",\"难以区分语义相近但位置不同的区域（如桌子边缘 vs 中心）；\",\"🧱 三、对局部形状变化敏感\",\"PointNet 提取的关键点集合（critical point set）可能不足以代表复杂的局部结构。\",\"🔍 实验观察：\",\"在论文中提到，PointNet 学到的是一个关键点集合，这些点大致构成物体的骨架；\",\"如果这些关键点缺失或被遮挡，即使其他点都在，也可能导致错误分类；\",\"对于非刚性变形（如人体姿态变化），PointNet 的表现不如基于图结构的模型；\",\"📈 四、分类性能略逊于多视角方法\",\"在某些标准数据集（如 ModelNet40）上，PointNet 的分类准确率略低于 MVCNN 等基于图像的方法。\",\"方法\",\"分类准确率\",\"MVCNN（多视角 CNN）\",\"90.1%\",\"VoxNet（体素 CNN）\",\"85.9%\",\"PointNet\",\"89.2%\",\"虽然 PointNet 在速度和效率上占优，但在精度上仍略逊一筹。\",\"🧩 五、难以捕捉非刚性变换下的不变性\",\"PointNet 使用 T-Net 强制学习正交变换矩阵，只能处理刚性变换（旋转、反射），无法处理非刚性形变（如弯曲、拉伸）。\",\"🔍 举例说明：\",\"如果你有一张人脸的点云，由于表情不同，面部发生形变；\",\"PointNet 很难在这种情况下保持分类的一致性；\",\"相比之下，基于图卷积或注意力机制的模型更能捕捉这种非刚性变化；\",\"🧱 六、缺乏层次化特征提取机制\",\"PointNet 是一种单尺度网络，无法像 CNN 那样逐层提取多层次的抽象特征。\",\"✅ 后续改进：\",\"PointNet++ 正是对这一缺陷的改进；\",\"它引入了局部区域搜索 + 多尺度聚合机制；\",\"从而能够更好地捕捉点云的局部结构和层次信息；\",\"📊 七、对稀疏点云敏感\",\"当输入点云非常稀疏时（如只有几十个点），PointNet 的性能会显著下降。\",\"🔍 原因分析：\",\"PointNet 的全局特征来自于 max pooling；\",\"如果点太少，max pooling 得到的特征可能无法覆盖整个物体；\",\"特别是在遮挡严重的情况下，关键点可能丢失；\",\"📐 八、结构简单，不利于高维空间建模\",\"PointNet 的结构过于简单，难以建模更高维度的空间关系。\",\"✅ 后续发展：\",\"后续的 3D 深度学习模型（如 DGCNN、SpiderCNN、PointCNN、Transformer-based 点云模型）都尝试引入更复杂的结构来提升建模能力；\",\"如：构建点之间的邻接图、使用 attention、引入多尺度采样等；\",\"🧪 九、理论上的限制：受限于瓶颈维度 K\",\"PointNet 的表达能力受 max pooling 层维度 K 的限制，即 bottleneck dimension。\",\"📌 来自论文的理论分析：\",\"Theorem 2 表明，PointNet 的输出仅由一个不超过 K 个点的子集决定（critical point set），这意味着：\",\"如果 K 不够大，PointNet 可能遗漏重要细节；\",\"如果 K 太大，又会导致计算资源浪费；\",\"🧱 十、对噪声点敏感（尤其未训练时）\",\"虽然 PointNet 对少量异常点有一定鲁棒性，但如果训练时没有加入扰动，面对大量噪声点时效果较差。\",\"🔍 实验验证：\",\"论文中做了“插入异常点”的实验；\",\"结果显示，如果训练过程中加入了噪声，模型表现良好；\",\"否则，异常点会影响分类和分割性能；\",\"📉 十一、在大规模场景理解任务中表现一般\",\"PointNet 的时间复杂度虽然是 O(N)，但在处理超大规模点云时，仍然不如分块处理或多层级聚合模型高效。\",\"✅ 后续改进方向：\",\"使用分块策略（chunking）\",\"构建点云的层次化表示\",\"引入 attention 或图结构增强局部建模能力\",\"🧩 总结表格：PointNet 的主要缺陷\",\"缺陷类型\",\"描述\",\"是否被后续模型改进\",\"忽略局部结构\",\"仅靠 max pooling 提取特征，无局部聚合机制\",\"✅ PointNet++ 改进\",\"分割精度不高\",\"拼接机制不够精细，缺乏动态上下文感知\",\"✅ Transformer-based 改进\",\"无法处理非刚性变形\",\"T-Net 只学正交变换，无法应对弯曲、拉伸等形变\",\"✅ 图卷积、attention 改进\",\"分类精度略低\",\"在 ModelNet40 上略低于 MVCNN\",\"✅ 多视角 + PointNet 混合模型改进\",\"稀疏点云下性能差\",\"少量点无法覆盖关键结构\",\"✅ PointNet++ 改进\",\"局部建模能力弱\",\"无法捕捉边缘、曲率等细节\",\"✅ DGCNN、SpiderCNN 改进\",\"对噪声点敏感\",\"未经扰动训练时，对异常点鲁棒性差\",\"✅ 加入数据增强后缓解\",\"结构单一\",\"缺乏层次化、多尺度建模能力\",\"✅ PointNet++ / Transformer 改进\",\"📈 PointNet 的优势 vs 缺陷对比\",\"维度\",\"优势\",\"缺陷\",\"输入形式\",\"支持原始点云，无需预处理\",\"无法有效利用局部结构\",\"排列不变性\",\"完全支持\",\"无法区分顺序信息（如时间序列点云）\",\"变换不变性\",\"支持刚性变换标准化\",\"无法处理非刚性形变\",\"分类性能\",\"接近 SOTA\",\"略逊于多视角 CNN\",\"分割性能\",\"表现良好\",\"缺乏精细建模\",\"效率\",\"极其高效（O(N)）\",\"无法充分利用 GPU 并行优化\",\"扩展性\",\"易于扩展为检测、检索等任务\",\"表达能力受限于 max pooling 维度\",\"✅ 一句话总结：\",\"PointNet 的最大缺陷在于它“看不清细节”，只关注全局结构，忽视局部邻域关系，这使得它在细粒度识别、非刚性变形、稀疏点云等任务中表现受限，但它也为后续模型奠定了基础。\"]},\"70\":{\"h\":\"背景知识扫盲(可选)\"},\"71\":{\"h\":\"点云\",\"t\":[\"点云: 是一种表示三维空间中物体或场景的方式，它由大量带有位置信息的点组成。\",\"每个点通常包含：\",\"坐标信息 ：x, y, z（3D 空间中的位置）。\",\"可选属性：颜色（RGB）、法向量（Normal）、强度（Intensity）、时间戳等。\",\"表示形式:\",\"点云（Point Cloud）: 原始点集合：每个点有(x, y, z)坐标; 可选颜色、法向量等属性, 简洁、轻便; 保留原始几何信息,无序性、非结构化、难以用 CNN 处理。\",\"体素网格 (voxel grids) : 将空间划分成立方体格子，每个格子表示是否有物体; 结构规整，适合 3D CNN; 计算复杂度高、稀疏性强、精度受限。\",\"多视角图像（Multi-View Images）: 从多个角度渲染点云或 3D 模型为 2D 图像; 可使用成熟的 2D CNN 方法; 丢失部分几何信息，依赖视角选择。\",\"网格（Mesh）： 由三角形面片组成的 3D 模型； 包含表面细节，适合渲染； 难以自动构建，拓扑复杂。\"]},\"72\":{\"h\":\"对称函数\",\"t\":[\"对称函数（Symmetric Function）是一种对输入顺序不敏感的函数；换句话说，无论你如何打乱输入元素的顺序，输出结果都保持不变。\",\"🧠 数学定义:\",\"设是一个函数，如果对于任意排列（permutation），都有：\",\"那么就是一个 对称函数。\",\"PointNet 处理的是点云数据，而点云是无序集合（unordered set） ，即：\",\"点云中点的顺序不影响整体形状。\",\"所以模型必须具有对点顺序的不变性（permutation invariance）。\",\"这就要求网络中的某些关键操作必须是对称函数 ，才能保证整个网络输出与输入点的顺序无关。\",\"📦 常见的对称函数:\",\"函数\",\"描述\",\"是否可微\",\"应用场景\",\"最大池化（Max Pooling）\",\"取所有点的最大值：\",\"✅ 是\",\"PointNet 中的核心操作\",\"平均池化（Average Pooling）\",\"取所有点的平均值：\",\"✅ 是\",\"特征融合、平滑处理\",\"求和（Summation）\",\"所有点相加：\",\"✅ 是\",\"构建全局特征向量\",\"乘积（Product）\",\"所有点相乘：\",\"⚠️ 对数值变化敏感\",\"不常用，但可用于特定任务\",\"最小池化（Min Pooling）\",\"取最小值：\",\"✅ 是\",\"异常检测等特殊场景\",\"Softmax + 加权和（Attention-based Sum）\",\"根据注意力机制加权求和，权重由 softmax 得出\",\"✅ 是\",\"DGCNN、Transformer 中使用\",\"统计量（如方差、标准差）\",\"计算点集的分布特性\",\"✅ 是\",\"特征增强、异常检测\",\"集合函数近似器（如 Deep Sets）\",\"使用神经网络直接学习对称函数\",\"✅ 是\",\"更复杂的对称函数建模\"]},\"73\":{\"h\":\"刚性运动\",\"t\":[\"刚性运动(rigid motions) 是指：物体在空间中移动时，其形状和大小保持不变的运动方式 。\",\"刚性运动\",\"❌ 不改变\",\"移动、旋转\",\"非刚性运动\",\"✅ 改变\",\"弯曲、拉伸、缩放（非均匀）、变形\",\"刚性运动 = 平移 + 旋转，不改变物体形状和内部结构，只改变位置和朝向。\"]},\"74\":{\"h\":\"正交变换\",\"t\":[\"正交变换的本质是：只改变物体的方向（旋转），不改变形状和大小\",\"所以：\",\"正交变换包括：旋转 + 反射。\",\"不包括：缩放、剪切、拉伸等会导致形变的操作。\"]},\"75\":{\"h\":\"大语言模型\"},\"76\":{\"h\":\"书生·万象多模态大模型（InternVL 1.0）\",\"t\":[\"InternVL 1.0: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks 论文简析\",\"论文链接: https://arxiv.org/abs/2312.14238 代码链接: https://github.com/OpenGVLab/InternVL\"]},\"77\":{\"h\":\"LLaVA 1.0(Large Language and Vision Assistant)\",\"t\":[\"LLaVA 1.0 : Large Language and Vision Assistant 论文简析\",\"论文链接: https://arxiv.org/abs/2304.08485 代码链接: https://github.com/haotian-liu/LLaVA\"]},\"78\":{\"h\":\"背景\",\"t\":[\"此前，大型语言模型（如 GPT-3、LLaMA）通过机器生成的指令数据进行调优，显著提升了零样本和少样本泛化能力（如 InstructGPT、FLAN-T5 等）。\",\"InstructGPT 是由 OpenAI 提出的一种通过 人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF） 来实现 指令调优（Instruction Tuning） 的方法。 其目标是让预训练语言模型（如 GPT-3）更好地理解和执行用户给出的自然语言指令，从而提升其在各种任务上的泛化能力，尤其是零样本（zero-shot）或多任务场景下的表现。 InstructGPT 的核心思想是：通过结合人工标注数据和强化学习，引导语言模型更好地遵循用户指令，并在多种任务上表现良好。 它不是单纯地“记住”训练数据中的例子，而是学会根据用户指令理解任务意图并生成合适的结果。 InstructGPT 的 instruction tuning 实现主要包括以下三个关键阶段： 步骤1：收集指令-响应对（Instruction-Following Data）\",\"OpenAI 收集了大量的人类编写的 指令（instruction） 和对应的 期望输出（response）。\",\"这些指令可以是开放式的（如“写一个关于猫的故事”），也可以是特定任务（如“翻译成中文”、“总结文章”）。\",\"数据来源包括：\",\"用户提交给 GPT-3 的 API 请求；\",\"内部标注人员手动构造的示例。\",\"目标：构建一个多样化的指令-响应数据集，用于训练或评估模型。\",\"步骤2：训练监督模型（Supervised Policy）\",\"使用标注好的指令-响应数据对模型进行微调（fine-tune）。\",\"输入是一个指令，输出是模型应该生成的响应。\",\"模型结构与原始 GPT-3 相同，只是参数经过调整以更好响应指令。\",\"步骤3：基于人类反馈的强化学习（RLHF）, 这是 InstructGPT 最具创新性的部分。具体分为三步:\",\"收集人类偏好数据\",\"对于同一个指令，让模型生成多个不同的回答；\",\"让人类标注者对这些回答进行排序，选出他们认为最好的答案。\",\"训练奖励模型（Reward Model）\",\"使用上述人类偏好数据，训练一个奖励模型（Reward Model），该模型的输入是一对（指令 + 回答），输出是对这个回答的评分（score）。\",\"奖励模型的目标是模拟人类的偏好判断。\",\"使用强化学习优化策略（Policy Optimization）\",\"使用 PPO（Proximal Policy Optimization） 等强化学习算法，以奖励模型为“环境”，进一步微调模型。\",\"在训练过程中，模型尝试生成尽可能高奖励的回答，从而更贴近人类期望。\",\"《 Visual Instruction Tuning 》 这篇论文首次尝试使用仅支持文本输入的 GPT-4 / ChatGPT 来生成图文结合的指令响应对（instruction-following data） ，并用这些数据训练一个端到端的视觉语言模型 LLaVA。\",\"论文核心创新点: 这是第一个系统性地将 NLP 中的指令调优思想引入多模态领域的研究。\"]},\"79\":{\"h\":\"方法\",\"t\":[\"作者将模型训练分为两个阶段 ：\",\"预训练阶段（Feature Alignment Pre-training）: 让视觉编码器提取的图像特征与语言模型的词嵌入空间对齐 , 也就是说：让模型理解图像和文本之间的语义关系, 这是后续指令调优的基础。\",\"微调阶段（End-to-End Fine-tuning）：在预训练的基础上，进一步训练模型理解和执行更复杂的视觉指令任务。\",\"多轮对话能力；\",\"复杂推理能力；\",\"科学问答等实际应用任务。\"]},\"80\":{\"h\":\"预训练\",\"t\":[\"预训练是 LLaVA 模型训练的第一阶段，目标让视觉编码器输出的图像特征与语言模型的词向量空间对齐 ，使得后续指令调优时，模型可以更好地理解和生成图文结合的内容。\",\"作者使用的是大规模图文对数据集 CC3M（Conceptual Captions 3M） ，包含约 300 万条图文对。\",\"为了提升数据质量，进行了以下筛选： 名词短语过滤（Noun Phrase Filtering）\",\"使用 Spacy 提取每条 caption 中的名词短语；\",\"统计每个名词短语出现的频率；\",\"去除频率小于 3 的短语（避免罕见组合）；\",\"对于频率大于 100 的短语，只保留最多 100 条描述（防止过拟合）；\",\"最终得到约 595,000 条高质量图文对 。\",\"数据构建方式: 为了模拟用户提问和模型回答的形式，将这些图文对转换为如下格式：\",\"Human: [指令] [图像描述] Assistant: [详细描述]\",\"其中：\",\"[指令] ：如“请描述这张图片。”、“图中有什么？” [图像描述] ：来自 caption 或 bounding box 的文本化表示； [详细描述] ：期望的回答，通常是图像内容的全面视觉描述。\",\"Caption: 图像的文字描述，从多个角度描述图像内容 , 如: \\\"A group of people standing outside of a black vehicle with various luggage.\\\" Bounding Box: 标注图像中的物体及其位置 , 如: person:[0.681, 0.242, 0.774, 0.694], backpack:[0.384, 0.696, 0.485, 0.914] .\",\"模型结构:\",\"视觉编码器 ：CLIP ViT-L/14（预训练好的）\",\"语言模型 ：Vicuna（基于 LLaMA 的指令调优版本）\",\"投影层 ：一个简单的线性层，连接视觉特征和语言嵌入空间\",\"LLaVA模型结构\",\"训练流程:\",\"输入图像 : 使用 CLIP 视觉编码器提取图像特征 。\",\"投影层 : 将 转换为语言模型可用的 token 序列 。\",\"训练目标: 使用交叉熵损失函数，最小化语言模型输出与真实答案之间的差异 。\",\"仅更新投影矩阵 ，保持视觉编码器和语言模型参数冻结。这个阶段相当于在语言模型的词空间中“训练出一个能看懂图的视觉分词器”。\",\"通过这个阶段训练后，模型已经具备基本的视觉理解能力，即：\",\"可以根据图像描述生成合理的文字解释；\",\"实现了图像与语言之间的初步语义对齐；\",\"为下一阶段的端到端微调提供了良好的初始化。\",\"虽然还不能执行复杂的推理任务，但已经可以处理基本的图文问答任务。\"]},\"81\":{\"h\":\"微调\",\"t\":[\"微调过程 是 LLaVA 模型训练的第二阶段，目标是让模型在预训练的基础上进一步掌握多模态指令理解与复杂推理能力 ，具体包括：\",\"支持多轮视觉对话（Multimodal Chat）\",\"理解并回答科学类问题（如 ScienceQA 数据集）\",\"执行复杂的视觉推理任务\",\"具备跨模态交互能力（图像 + 文本）\",\"这是实现“通用视觉助手”的关键一步。\",\"微调阶段使用的是作者自己构建的高质量多模态指令数据集：\",\"名称：LLaVA-Instruct-158K\",\"包含约 158,000 条图文对\",\"分为三种响应类型：\",\"对话型（Conversation） ：58,000 条\",\"详细描述型（Detailed Description） ：23,000 条\",\"复杂推理型（Complex Reasoning） ：77,000 条\",\"这些数据由 GPT-4 / ChatGPT 自动生成，涵盖多种任务类型，具有高度多样性和挑战性。\",\"微调阶段的数据组织方式如下：\",\"Xsystem-message <STOP> Human: X1instruct <STOP> Assistant: X1a <STOP> Human: X2instruct <STOP> Assistant: X2a <STOP> ...\",\"其中：\",\"Xsystem-message：系统提示语（如：“你是一个视觉助手”）； Xinstruct：用户提问或指令； Xa：期望的回答； <STOP>：分隔符，表示输入结束，开始输出回答。\",\"模型结构:\",\"视觉编码器 ：CLIP ViT-L/14（保持冻结）\",\"语言模型 ：Vicuna（基于 LLaMA 的指令调优版本）\",\"投影层 ：连接图像特征和语言嵌入空间的线性层\",\"训练流程:\",\"输入图像 : 使用 CLIP 提取图像特征 \",\"投影层 : 使用可训练的投影矩阵 将图像特征 转换为语言嵌入 \",\"训练目标: 最小化语言模型输出与真实答案之间的交叉熵损失。\",\"微调时保持视觉编码器参数不变，只更新投影层 和语言模型 Vicuna 的参数。\",\"论文中重点测试了以下两个应用场景：\",\"多模态聊天机器人（Multimodal Chatbot）： 使用 LLaVA-Instruct-158K 数据集进行训练；\",\"其中：\",\"对话型问答为多轮对话；\",\"其他两类为单轮对话；\",\"数据均匀采样，训练出一个能自然理解图像内容、并进行视觉对话的 AI 助手。\",\"科学问答（Science QA）：在 ScienceQA 数据集上进行迁移学习；\",\"每个问题包含文本或图像上下文；\",\"助手需要生成推理过程，并从多个选项中选择正确答案；\",\"在这个任务上，LLaVA 达到了 90.92% 准确率 ；\",\"当与 GPT-4 联合推理时，准确率达到 92.53% ，刷新该数据集 SOTA。\"]},\"82\":{\"h\":\"联合 GPT-4 的推理机制（Ensemble with GPT-4）\",\"t\":[\"作者还提出了一种创新方法，将 LLaVA 与 GPT-4 联合使用：\",\"方法一：GPT-4 补充\",\"当 GPT-4 无法回答时，使用 LLaVA 的预测结果；\",\"效果：准确率提升不大（仅 0.05%），说明 LLaVA 已经接近其上限。\",\"方法二：GPT-4 判断者（Judge）\",\"当 LLaVA 和 GPT-4 输出不一致时，再次用 GPT-4 做判断；\",\"效果：显著提升表现，最终准确率达到 92.53% ，刷新 ScienceQA 数据集的 SOTA。\",\"这是首次尝试将大语言模型用于模型集成（model ensemble）的研究。\"]},\"83\":{\"h\":\"ablation study（消融实验）\",\"t\":[\"论文中还进行了多项 ablation 实验，以分析不同训练策略的影响：\",\"训练策略\",\"准确率变化\",\"不做预训练\",\"-5.11%\",\"仅使用最后一层视觉特征\",\"-0.96%\",\"先生成答案再推理\",\"-1.15%\",\"使用较小的 7B 模型\",\"-1.08%\",\"这些实验表明：\",\"预训练阶段非常关键；\",\"使用倒数第二层视觉特征更有利于细节理解；\",\"推理优先（Reasoning First）有助于加快收敛；\",\"模型规模对性能有显著影响。\"]},\"84\":{\"h\":\"补充\"},\"85\":{\"h\":\"辨析 instruction tuning 和 prompt tuning\",\"t\":[\"Instruction Tuning（指令调优） 和 Prompt Tuning（提示调优） 是两种用于提升预训练语言模型（LLM）或视觉语言模型性能的技术，但它们的目标、方法和应用场景有显著区别。以下是两者的主要区别：\",\"定义与核心思想\",\"类别\",\"Instruction Tuning（指令调优）\",\"Prompt Tuning（提示调优）\",\"定义\",\"通过大量“指令-响应”对微调模型，使其更好地理解和执行用户给出的自然语言指令。\",\"在输入中添加可学习的前缀（prefix）或前缀/后缀（prompt），引导模型生成特定任务的结果，而不需要改变整个模型参数。\",\"核心思想\",\"模型要理解并遵循人类语言中的任务描述（如“总结一下这篇文章”）。\",\"模型通过在输入前后插入一些可训练的提示词来“唤醒”其已有的知识，完成特定任务。\",\"训练方式\",\"类别\",\"Instruction Tuning\",\"Prompt Tuning\",\"是否修改模型结构\",\"否（通常保留原始结构）\",\"否\",\"是否更新全部参数\",\"是（微调整个模型参数）\",\"否（仅更新插入的 prompt 参数，其余参数冻结）\",\"数据需求\",\"需要大量人工或机器生成的“指令-响应”对\",\"不需要额外标注数据，直接使用原始任务描述\",\"训练目标\",\"提升模型在各种任务上的泛化能力，尤其是零样本/少样本任务迁移\",\"让固定模型适应新任务，利用已有知识进行推理\",\"应用场景举例\",\"类别\",\"示例场景\",\"Instruction Tuning\",\"ChatGPT、InstructGPT、FLAN-T5、LLaVA（视觉+语言）等，能根据用户指令回答问题、写故事、编程、推理等。\",\"Prompt Tuning\",\"使用 [PROMPT] 前缀让 BERT 回答 QA 问题、分类任务；在图像识别中加入 learnable prefix 来适配不同类别。\",\"优缺点对比:\",\"对比维度\",\"Instruction Tuning\",\"Prompt Tuning\",\"优点\",\"- 更强的任务泛化能力- 更贴近真实用户交互- 可用于多模态任务\",\"- 参数效率高（只训练少量 prompt）- 可复用已有大模型权重\",\"缺点\",\"- 数据依赖性强（需要大量高质量指令数据）- 微调成本高（需训练整个模型）\",\"- 表达能力受限于 prompt 的设计- 泛化性不如 instruction tuning\",\"总结一句话： Instruction Tuning 是教会模型“听懂人话”，按指令做事；Prompt Tuning 是引导模型“激活已有知识”，通过提示词让它自己做任务。\"]},\"86\":{\"h\":\"多模态\"},\"87\":{\"h\":\"庖丁解牛CLIP\",\"t\":[\"多模态模型CLIP原理与图片分类，文字搜索图像实战演练\",\"CLIP原始论文链接\"]},\"88\":{\"h\":\"引言\",\"t\":[\"2021 年可谓是视觉 Transformer（Vision Transformer）大放异彩的一年。自谷歌提出 ViT 之后，众多基于视觉 Transformer 的研究如潮水般涌来，广泛应用于各类计算机视觉任务。与此同时，OpenAI 在 2021 年 1 月发布的 DALL-E 和 CLIP，同样给计算机视觉领域带来了巨大影响。这两个模型都属于融合图像与文本的多模态模型，其中 DALL-E 是基于文本输入来生成图像的模型，而 CLIP 则是以文本作为监督信号，训练出具有可迁移能力的视觉模型。和 ViT 类似，DALL-E 和 CLIP 的出现也掀起了新一轮的研究热潮。\"]},\"89\":{\"h\":\"介绍\",\"t\":[\"CLIP的英文全称为Contrastive Language-Image Pre-training，它代表着一种基于对比文本-图像对的预训练方法，同时也指运用该方法构建的模型。CLIP属于基于对比学习的多模态模型。与计算机视觉（CV）领域中的一些对比学习方法，像MoCo和SimCLR有所不同，CLIP的训练数据采用的是文本-图像对，也就是一张图像搭配与之对应的文本描述。在训练过程中，借助对比学习机制，期望模型能够学习到文本和图像之间的匹配关系。\"]},\"90\":{\"h\":\"训练\",\"t\":[\"CLIP包含两个核心模型，分别是文本编码器（Text Encoder）和图像编码器（Image Encoder）。其中，文本编码器的作用是提取文本的特征，在实现时可采用自然语言处理（NLP）领域常用的文本Transformer模型；而图像编码器则用于提取图像的特征，在实际应用中可以选用常见的卷积神经网络（CNN）模型，也可以采用视觉Transformer模型。\",\"这里对提取的文本特征和图像特征进行对比学习。对于一个包含个文本-图像对的训练batch，将个文本特征和个图像特征两两组合，CLIP模型会预测出个可能的文本-图像对的相似度，这里的相似度直接计算文本特征和图像特征的余弦相似性（cosine similarity），即上图所示的矩阵。这里共有个正样本，即真正属于一对的文本和图像（矩阵中的对角线元素），而剩余的个文本-图像对为负样本，那么CLIP的训练目标就是最大个正样本的相似度，同时最小化个负样本的相似度，对应的伪代码实现如下所示：\",\"# image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter # 分别提取图像特征和文本特征 I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # 对两个特征进行线性投射，得到相同维度的特征，并进行l2归一化 I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # 计算缩放的余弦相似度：[n, n] logits = np.dot(I_e, T_e.T) * np.exp(t) # 对称的对比学习损失：等价于N个类别的cross_entropy_loss labels = np.arange(n) # 对角线元素的labels loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2\",\"为了训练CLIP模型，OpenAI从网络上收集了总计4亿对文本和图像，这些数据在论文中被称为WebImageText。若以文本单词数量来衡量，其规模与GPT-2训练时使用的WebText数据集相似。然而，从数据对的数量来看，它比谷歌的JFT-300M数据集还要多出1亿对，因此这是一个非常庞大的数据集。\",\"尽管CLIP是一个多模态模型，但其主要目的是训练可迁移的视觉模型。在论文中，文本编码器（Text Encoder）选择了一个包含6300万参数的Transformer模型，而图像编码器（Image Encoder）则采用了两种不同的架构：\",\"一种是常用的CNN架构ResNet。\",\"另一种是基于 Transformer 的ViT。\",\"ResNet包含五种不同尺寸的模型：ResNet50、ResNet101、RN50x4、RN50x16和RNx64（后三种模型是按照EfficientNet的缩放规则对ResNet分别放大4倍、16倍和64倍得到的），而ViT则选择了三种不同尺寸的模型：ViT-B/32、ViT-B/16和ViT-L/14。\",\"所有模型均训练了32个周期，使用AdamW优化器，并且在训练过程中采用了一个相对较大的批次大小：32768。由于数据量巨大，最大的ResNet模型RN50x64需要在592个V100 GPU上训练18天，而最大的ViT模型ViT-L/14则需要在256个V100 GPU上训练12天，这表明训练CLIP模型需要消耗大量的资源。对于ViT-L/14模型，还在336的分辨率下额外进行了一个周期的微调（finetune）以增强性能，论文发现这个模型的效果最佳，并将其标记为ViT-L/14@336，论文中进行对比实验的CLIP模型也采用了这一配置。\"]},\"91\":{\"h\":\"推理\",\"t\":[\"我们已经探讨了CLIP模型的运作机制，它由两个部分组成：一个视觉模型和一个文本模型。那么，如何将这个预训练的视觉模型应用到新的任务中呢？CLIP模型的一个显著优势是它能够进行zero-shot图像分类，这意味着它能够在没有任何特定任务训练数据的情况下，直接对图像进行分类。这不仅展示了CLIP的强大功能，也是其一大亮点。实现zero-shot分类的过程相当直接，可以概括为以下两个主要步骤：\",\"构建描述文本并提取特征：首先，根据任务的分类需求，为每个类别创建一个描述性的文本，例如“A photo of {label}”。这些文本随后被输入到文本编码器（Text Encoder）中，以生成相应的文本特征。如果有个类别，那么就会得到个文本特征。\",\"图像特征提取与分类：接下来，将待分类的图像输入到图像编码器（Image Encoder）中，以获取图像特征。然后，这些图像特征会与之前得到的个文本特征进行余弦相似度计算（这一过程与训练时相同）。最终，选择与图像特征相似度最高的文本所对应的类别，作为图像的分类预测结果。此外，这些相似度值可以被视为logits，通过softmax函数转换后，可以得到每个类别的预测概率。\",\"通过这种方式，CLIP模型能够在没有特定任务训练数据的情况下，直接对图像进行分类，这展示了其在图像分类任务中的灵活性和强大能力。\",\" 显然，我们通过利用CLIP模型的多模态能力，为特定任务动态构建了一个分类器。在这个过程中，文本编码器（Text Encoder）生成的文本特征相当于分类器的权重，而图像编码器（Image Encoder）提取的图像特征则是分类器的输入数据。以下是一个官方给出的CLIP模型的示例 ，该示例中的任务涉及8个类别:\",\"我们首先创建了各类别的文本描述，然后提取了相应的文本特征；\",\"然后我们读取要预测的图像，输入Image Encoder提取图像特征，并计算与文本特征的余弦相似度。\",\"# 1. 提取文本特征 texts = [ \\\"a page of text about segmentation\\\", \\\"a facial photo of a tabby cat\\\", \\\"a portrait of an astronaut with the American flag\\\", \\\"a rocket standing on a launchpad\\\", \\\"a red motorcycle standing in a garage\\\", \\\"a person looking at a camera on a tripod\\\", \\\"a black-and-white silhouette of a horse\\\", \\\"a cup of coffee on a saucer\\\" ] text_tokens = clip.tokenize([\\\"This is \\\" + desc for desc in texts]).cuda() with torch.no_grad(): text_features = model.encode_text(text_tokens).float() # 2. 提取图像特征 image_input = torch.tensor(np.stack(images)).cuda() with torch.no_grad(): image_features = model.encode_image(image_input).float() # 3. 计算余弦相似度 image_features /= image_features.norm(dim=-1, keepdim=True) text_features /= text_features.norm(dim=-1, keepdim=True) similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\",\"相似度如下所示，可以看到对于要预测的8个图像，按照最大相似度，其均能匹配到正确的文本标签：\",\"进一步地，我们也可以对得到的余弦相似度计算softmax，得到每个预测类别的概率值：\",\"text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1) top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)\",\"得到的预测概率如下所示，可以看到8个图像，CLIP模型均能够以较高的置信度给出正确的分类结果：\"]},\"92\":{\"h\":\"文本描述生成\",\"t\":[\"在使用CLIP模型进行zero-shot分类时，除了模型本身的应用，文本描述的生成也是一个关键环节。在之前的例子中，我们使用了“A photo of {label}”这样的格式来生成文本描述，但实际上，我们还有其他的选择。例如，我们可以直接使用类别标签作为文本描述。这种方法实际上与NLP领域的一个研究方向——prompt learning或prompt engineering——紧密相关。关于这一领域的详细综述，可以参考论文《Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing》。\",\"简单来说，prompt learning的核心思想是通过设计合适的prompt（提示），使得预训练模型能够直接应用于下游任务。这与传统的预训练加微调的方法有所不同。论文指出，如果我们直接使用类别标签作为文本描述，由于这些文本往往只是一个单词，缺乏具体的上下文，并且与CLIP模型的训练数据不完全一致，因此在效果上可能不如使用“A photo of {label}”这种格式（在ImageNet数据集上可以提升1.3%的效果）。\",\"此外，论文还实验了使用80个不同的prompt进行集成，结果发现在ImageNet数据集上能够带来3.5%的性能提升。具体的实验结果可以参考CLIP公开的notebook。\"]},\"93\":{\"h\":\"花卉图片分类\",\"t\":[\"本节我们将基于CLIP预训练模型实现Zero-Shot推理，训练使用到的数据集和AlexNet保持一致，因此这里就不再给出数据集下载链接了。\",\"图片分类实战 – 分别基于LeNet，AlexNet，VGG进行实现\",\"# 预训练模型名称 model_name = \\\"openai/clip-vit-large-patch14\\\" # 定义当前目录 current_dir = os.getcwd() model_dir = os.path.join(current_dir, model_name.replace(\\\"/\\\", \\\"_\\\")) # 检查当前目录是否有预训练权重文件，如果没有则下载 def download_pretrained_weights_if_needed(model_name, save_dir): if not os.path.exists(save_dir): try: print(f\\\"Downloading {model_name} to {save_dir}...\\\") snapshot_download(repo_id=model_name, local_dir=save_dir, local_dir_use_symlinks=False) print(f\\\"{model_name} downloaded successfully.\\\") except Exception as e: print(f\\\"Error downloading {model_name}: {e}\\\") download_pretrained_weights_if_needed(model_name, model_dir) # 加载模型和处理器 model = CLIPModel.from_pretrained(model_dir) processor = CLIPProcessor.from_pretrained(model_dir) device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\") model = model.to(device)\",\"在 openai/clip-vit-large-patch14 这个 CLIP 预训练模型中，图像编码器采用了 Vision Transformer（ViT）架构，具体使用的是 ViT-L/14 版本，文本编码器使用的是基于 Transformer 的架构。\",\"# 函数：生成文本嵌入 def text_embedding(texts): inputs = processor(text=texts, return_tensors=\\\"pt\\\", padding=True).to(device) with torch.no_grad(): embeddings = model.get_text_features(**inputs) return embeddings.cpu().numpy()\",\"这个函数的作用是将输入的文本转化为对应的嵌入表示（embedding）。它通过处理器对输入文本进行处理，使其符合模型的输入要求，然后利用模型获取文本特征，最后将结果转换为 numpy 数组格式返回，方便后续的计算和比较。\",\"def get_image_embeddings(image_paths): images = [] for image_path in image_paths: try: image = Image.open(image_path).convert(\\\"RGB\\\") images.append(image) except Exception as e: print(f\\\"Error loading image {e}\\\") if not images: return None inputs = processor(images=images, return_tensors=\\\"pt\\\").to(device) with torch.no_grad(): image_features = model.get_image_features(**inputs) return image_features.cpu().numpy()\",\"该函数作用是针对给定的图片路径，读取图片并将其转换为合适的格式后，通过模型获取图片的特征嵌入。如果在读取图片过程中出现错误，会进行相应的错误提示并返回 None。\",\"def cosine_similarity(vec1, vec2): vec1 = np.array(vec1) vec2 = np.array(vec2) return np.dot(vec1, vec2.T) / (np.linalg.norm(vec1, axis=1, keepdims=True) * np.linalg.norm(vec2, axis=1))\",\"在图文检索中，我们常常需要衡量文本嵌入和图片嵌入之间的相似度，这里采用了余弦相似度的计算方法。它将输入的向量转换为 numpy 数组后，按照余弦相似度的数学公式来计算两者的相似度数值。\",\"首先，我们需要根据上面给出的花卉数据集下载链接，将数据下载到当前项目目录下:\",\"其次，我们从flower_photos目录下读取出所有图片的路径:\",\"# 递归遍历目录获取所有图片路径 def get_all_image_paths(directory): image_paths = [] for root, _, files in os.walk(directory): for file in files: file_extension = os.path.splitext(file)[1].lower() if file_extension in ['.png', '.jpg', '.jpeg']: image_paths.append(os.path.join(root, file)) return image_paths image_paths = get_all_image_paths(\\\"./flower_photos\\\")\",\"同时将flower_photos下的子目录名作为我们的候选待匹配分类文本列表，并改造为a photo of 子目录名的格式，然后计算每个分类文本对应的文本嵌入向量:\",\"# 获取候选分类名列表 def get_candidates(directory): candidates = [] for sub_dir in os.listdir(directory): sub_dir_path = os.path.join(directory, sub_dir) if os.path.isdir(sub_dir_path): candidates.append(f\\\"a photo of {sub_dir}\\\") return candidates candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates)\",\"最后:\",\"分批次从图像列表中取出一批图像，获取其对应的图像嵌入向量列表\",\"计算这批图像嵌入向量中每一个图像嵌入向量和我们所有分类文本嵌入向量的余弦相似度\",\"针对每个图像嵌入向量取出和其相似度最高的那个文本嵌入向量的索引\",\"判断预测是否正确，统计正确率\",\"# 测试图片分类正确率 def accuracy(image_paths, candidates, text_embeddings, batch_size=64): correct_count = 0 total_count = len(image_paths) num_batches = (total_count + batch_size - 1) // batch_size # 分批次预测 for i in range(num_batches): start_idx = i * batch_size end_idx = min(start_idx + batch_size, total_count) # 取出当前批次的图像列表，并获得该批次图像列表对应的图像嵌入向量列表 batch_image_paths = image_paths[start_idx:end_idx] image_embeddings = get_image_embeddings(batch_image_paths) if image_embeddings is not None: # 计算图像嵌入向量列表中每个图像对应的嵌入向量和传入的分类文本嵌入向量的余弦相似度 similarities = cosine_similarity(image_embeddings, text_embeddings) # 针对每个图像嵌入向量获取和其相似度最高的分类文本嵌入向量索引下标 predicted_indices = np.argmax(similarities, axis=1) for j, predicted_index in enumerate(predicted_indices): # 针对每张图像，根据上述计算得到的和其相似度最高的分类文本索引，从候选分类文本集合中取出其分类名词 predicted_category = candidates[predicted_index].split(\\\" \\\")[-1] # 用当前图片外层目录的名字作为其分类名词 actual_category = os.path.basename(os.path.dirname(batch_image_paths[j])) # 比较两个分类名词是否相等 if predicted_category == actual_category: correct_count += 1 accuracy = correct_count / total_count return accuracy # 图片分类 def flowerClassify(): image_paths = get_all_image_paths(\\\"./flower_photos\\\") candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates) start_time = time.time() acc = accuracy(image_paths, candidates, text_embeddings, batch_size=64) end_time = time.time() print(f\\\"Time taken to test accuracy: {end_time - start_time:.2f} seconds\\\") print(f\\\"Accuracy: {acc * 100:.2f}%\\\")\",\"Time taken to test accuracy: 396.62 seconds Accuracy: 95.48%\"]},\"94\":{\"h\":\"文字搜索图像\",\"t\":[\"上面的花卉分类案例相当于使用图像去匹配最合适的文本描述，而这里我们将会反转这个逻辑，用文本描述去匹配最合适的图片内容。\",\"为了实现文字搜索图像的功能，我们只需要在计算出相似度得分矩阵后，以每个文本描述为一行，取出该行中得分最大的那一列，即为与当前文本描述相似度最高的那副图片，具体代码实现如下：\",\"# 找到与文本最匹配的图片 def find_most_matching_image(text, image_paths): text_emb = text_embedding([text]) image_embeddings = get_image_embeddings(image_paths) if image_embeddings is None: return None similarities = cosine_similarity(text_emb, image_embeddings) most_matching_index = np.argmax(similarities) return image_paths[most_matching_index]\",\"下面来实际展示一下效果，首先我们用data目录充当我们的图片库来源:\",\" 遍历data目录，拿到所有图片路径:\",\"# 遍历 data 目录获取所有图片路径 def get_images_from_data_dir(): data_dir = os.path.join(current_dir, 'data') if not os.path.exists(data_dir): print(f\\\"Data directory {data_dir} does not exist.\\\") return [] return get_all_image_paths(data_dir)\",\"这里以搜索向日葵花为例，我们首先获取图片库中所有图片，然后计算出和当前文本描述相似度最高的那副图片，并将图片展示出来:\",\"# 根据文字搜索图片 def searchPicByText(): image_paths = get_images_from_data_dir() query_text = \\\"a photo of a sunflowers\\\" most_matching_image = find_most_matching_image(query_text, image_paths) if most_matching_image: print(f\\\"The most matching image for '{query_text}' is: {most_matching_image}\\\") try: img = Image.open(most_matching_image) plt.imshow(img) plt.axis('off') plt.title(f\\\"Most matching image for '{query_text}'\\\") plt.show() except Exception as e: print(f\\\"Error opening image: {e}\\\") else: print(\\\"No matching image found.\\\")\",\"图片库中的图片： 运行上述代码，搜索出来的图片:\"]},\"95\":{\"h\":\"完整代码\",\"t\":[\"import time from matplotlib import pyplot as plt from transformers import CLIPProcessor, CLIPModel import torch from PIL import Image import numpy as np import warnings import os from huggingface_hub import snapshot_download warnings.filterwarnings(\\\"ignore\\\") # 模型名称 model_name = \\\"openai/clip-vit-large-patch14\\\" # 定义当前目录 current_dir = os.getcwd() model_dir = os.path.join(current_dir, model_name.replace(\\\"/\\\", \\\"_\\\")) # 检查当前目录是否有预训练权重文件，如果没有则下载 def download_pretrained_weights_if_needed(model_name, save_dir): if not os.path.exists(save_dir): try: print(f\\\"Downloading {model_name} to {save_dir}...\\\") snapshot_download(repo_id=model_name, local_dir=save_dir, local_dir_use_symlinks=False) print(f\\\"{model_name} downloaded successfully.\\\") except Exception as e: print(f\\\"Error downloading {model_name}: {e}\\\") download_pretrained_weights_if_needed(model_name, model_dir) # 加载模型和处理器 model = CLIPModel.from_pretrained(model_dir) processor = CLIPProcessor.from_pretrained(model_dir) device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\") model = model.to(device) # 函数：生成文本嵌入 def text_embedding(texts): inputs = processor(text=texts, return_tensors=\\\"pt\\\", padding=True).to(device) with torch.no_grad(): embeddings = model.get_text_features(**inputs) return embeddings.cpu().numpy() def get_image_embeddings(image_paths): images = [] for image_path in image_paths: try: image = Image.open(image_path).convert(\\\"RGB\\\") images.append(image) except Exception as e: print(f\\\"Error loading image {e}\\\") if not images: return None inputs = processor(images=images, return_tensors=\\\"pt\\\").to(device) with torch.no_grad(): image_features = model.get_image_features(**inputs) return image_features.cpu().numpy() def cosine_similarity(vec1, vec2): vec1 = np.array(vec1) vec2 = np.array(vec2) return np.dot(vec1, vec2.T) / (np.linalg.norm(vec1, axis=1, keepdims=True) * np.linalg.norm(vec2, axis=1)) # 递归遍历目录获取所有图片路径 def get_all_image_paths(directory): image_paths = [] for root, _, files in os.walk(directory): for file in files: file_extension = os.path.splitext(file)[1].lower() if file_extension in ['.png', '.jpg', '.jpeg']: image_paths.append(os.path.join(root, file)) return image_paths # 获取候选分类名列表 def get_candidates(directory): candidates = [] for sub_dir in os.listdir(directory): sub_dir_path = os.path.join(directory, sub_dir) if os.path.isdir(sub_dir_path): candidates.append(f\\\"a photo of {sub_dir}\\\") return candidates # 测试图片分类正确率 def accuracy(image_paths, candidates, text_embeddings, batch_size=64): correct_count = 0 total_count = len(image_paths) num_batches = (total_count + batch_size - 1) // batch_size for i in range(num_batches): start_idx = i * batch_size end_idx = min(start_idx + batch_size, total_count) batch_image_paths = image_paths[start_idx:end_idx] image_embeddings = get_image_embeddings(batch_image_paths) if image_embeddings is not None: similarities = cosine_similarity(image_embeddings, text_embeddings) predicted_indices = np.argmax(similarities, axis=1) for j, predicted_index in enumerate(predicted_indices): predicted_category = candidates[predicted_index].split(\\\" \\\")[-1] actual_category = os.path.basename(os.path.dirname(batch_image_paths[j])) if predicted_category == actual_category: correct_count += 1 accuracy = correct_count / total_count return accuracy # 图片分类 def flowerClassify(): image_paths = get_all_image_paths(\\\"./flower_photos\\\") candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates) start_time = time.time() acc = accuracy(image_paths, candidates, text_embeddings, batch_size=64) end_time = time.time() print(f\\\"Time taken to test accuracy: {end_time - start_time:.2f} seconds\\\") print(f\\\"Accuracy: {acc * 100:.2f}%\\\") ##################################################################################################3 # 遍历 data 目录获取所有图片路径 def get_images_from_data_dir(): data_dir = os.path.join(current_dir, 'data') if not os.path.exists(data_dir): print(f\\\"Data directory {data_dir} does not exist.\\\") return [] return get_all_image_paths(data_dir) # 找到与文本最匹配的图片 def find_most_matching_image(text, image_paths): text_emb = text_embedding([text]) image_embeddings = get_image_embeddings(image_paths) if image_embeddings is None: return None similarities = cosine_similarity(text_emb, image_embeddings) most_matching_index = np.argmax(similarities) return image_paths[most_matching_index] # 根据文字搜索图片 def searchPicByText(): image_paths = get_images_from_data_dir() query_text = \\\"a photo of a sunflowers\\\" most_matching_image = find_most_matching_image(query_text, image_paths) if most_matching_image: print(f\\\"The most matching image for '{query_text}' is: {most_matching_image}\\\") try: img = Image.open(most_matching_image) plt.imshow(img) plt.axis('off') plt.title(f\\\"Most matching image for '{query_text}'\\\") plt.show() except Exception as e: print(f\\\"Error opening image: {e}\\\") else: print(\\\"No matching image found.\\\")\"]},\"96\":{\"h\":\"小结\",\"t\":[\"在计算机视觉领域，常见的迁移学习方法是首先在大规模数据集（如ImageNet）上进行预训练，然后在具体的下游任务上进行微调。这种预训练通常是基于有监督学习的，需要大量的数据标注，因此成本较高。近年来，出现了一些基于自监督的方法，包括基于对比学习的方法（如MoCo和SimCLR）和基于图像掩码的方法（如MAE和BeiT）。自监督方法的优势在于不再需要标注数据。然而，无论是有监督还是自监督方法，在迁移到下游任务时，都需要进行有监督微调，无法实现zero-shot学习。对于有监督模型，由于它们在预训练数据集上采用固定类别数的分类器，因此在新的数据集上需要定义新的分类器来重新训练。对于自监督模型，代理任务通常是辅助进行表征学习，在迁移到其他数据集时也需要加上新的分类器进行有监督训练。\",\"然而，在NLP领域，基于自回归或语言掩码的预训练方法已经相对成熟，预训练模型很容易直接zero-shot迁移到下游任务，例如OpenAI的GPT-3。这种差异一方面是由于文本和图像属于两个完全不同的模态，另一个原因是NLP模型可以利用从互联网上收集的大量文本。因此，问题来了：能否基于互联网上的大量文本来预训练视觉模型？\",\"实际上，之前已经有一些研究工作探讨了使用文本作为监督信号来训练视觉模型。例如，2016年的工作《Learning Visual Features from Large Weakly Supervised Data》将这个问题转化为一个多标签分类任务，预测图像对应的文本的词袋模型；2017年的工作《Learning Visual N-Grams from Web Data》进一步扩展了这个方法来预测n-grams。最近的一些工作采用新的模型架构和预训练方法来从文本学习视觉特征，例如VirTex基于transformer的语言模型，ICMLM基于语言掩码的方法，ConVIRT基于对比学习的方法。总体来看，这方面的工作并不多，主要是因为这些方法难以实现较高的性能，例如2017年的那篇工作只在ImageNet上实现了11.5%的zero-shot性能，这远远低于ImageNet上的SOTA。此外，还有另一个方向，即基于文本弱监督来提升性能，例如谷歌的BiT和ViT基于JFT-300M数据集来预训练模型在ImageNet上取得SOTA。JFT-300M数据集是谷歌从互联网上收集的，通过一些自动化的手段将web text转化为18291个类别，但存在一定的噪声。尽管谷歌基于JFT-300M数据集取得了较好的结果，但这些模型仍然采用固定类别的softmax分类器进行预训练，这大大限制了它们的迁移能力和扩展性。\",\"谷歌采用的弱监督方法与以往方法的一个主要区别在于规模，或者说在于计算能力和数据集的规模。JFT-300M数据集的规模达到了上亿级别，谷歌利用强大的计算能力进行了预训练。相比之下，VirTex、ICMLM和ConVIRT仅在10万级别的数据上训练了几天。为了弥补数据规模上的差距，OpenAI从网络上收集了4亿条数据进行实验。然而，新的问题出现了：应该采用什么样的方法来进行训练。\",\"OpenAI首先尝试了VirTex模型，该模型联合训练一个CNN和文本transformer来预测图像的文本描述（image caption），但发现这种方法的训练效率（根据ImageNet数据集上的zero-shot性能评估）还不如直接预测词袋模型（bag of words），两者的训练效率相差3倍。如果进一步采用ConVIRT，即基于对比学习的方法，训练效率可以提高4倍。出现这种差异的原因不难理解，因为训练数据集中的文本-图像对是从互联网收集的，存在一定的噪声，即文本和图像可能不完全匹配。在这种情况下，适当降低训练目标反而可能取得更好的效果。\",\"从任务难度来看，排序为：Transformer Language Model > Bag of Words Prediction > Bag of Words Contrastive (CLIP)。由于训练数据量和模型计算量较大，训练效率成为一个至关重要的因素。因此，作者最终选择了对比学习方法来进行训练。\"]},\"97\":{\"h\":\"庖丁解牛BLIP2\",\"t\":[\"庖丁解牛BLIP2\",\"论文: https://arxiv.org/abs/2301.12597 代码: https://github.com/salesforce/LAVIS/tree/main/projects/blip2\"]},\"98\":{\"h\":\"背景\",\"t\":[\"多模态模型在过往发展的过程中，曾有一段时期一直在追求更大的网络架构（image encoder 和 text encoder/decoder）和 数据集，从而导致更大的训练代价。例如CLIP，400M数据，需要数百个GPU训练数十天，如何降低模型训练成本，同时具有很好的性能？\",\"这就是BLIP-2的起因，回顾下之前的多模态网络设计，三个模块（图像分支、文本分支、融合模块）:\",\"多模态网络设计\",\"(a) 早期的图文多模态：图像分支依赖目标检测器，模态融合比较弱，如VSE++。\",\"(b) 重点训练图像和文本特征提取，模态融合比较轻量，如CLIP。\",\"(c) 图像特征提取和模态融合都很重。\",\"(d) 侧重模态融合，特征提取网络相对轻量，如ViLT。\",\"模块\",\"(a)\",\"(b)\",\"(c)\",\"(d)\",\"理想情况\",\"视觉分支\",\"重\",\"重\",\"重\",\"轻\",\"重\",\"文本分支\",\"轻\",\"重\",\"轻\",\"轻\",\"重\",\"融合模块\",\"轻\",\"轻\",\"重\",\"重\",\"轻\",\"性能\",\"一般\",\"好\",\"好\",\"一般\",\"好\",\"训练代价\",\"中\",\"非常高\",\"非常高\",\"高\",\"中\",\"BLIP-2 基于 BLIP 架构，利用已有的ViT 和 LLM（均冻结）+ 一个的轻量Q-Former模块做模态融合，大幅降低训练成本。具有很强的zero-shot image-to-text generation能力，同时因LLM而具有了视觉推理能力。\"]},\"99\":{\"h\":\"模型结构\",\"t\":[\"BLIP-2 框架按照 Two-Stage 策略预训练轻量级查询 Transformer 以弥合模态差距。\",\"Stage 1: 不同模态数据的提取与融合。\",\"Stage 2: 把数据转换成LLM能识别的格式。\",\"Two-Stage流程\",\"从冻结的Image Encoder引到Vision-Language表征学习。\",\"从冻结的LLM引到Vision-Language生成学习，实现Zero Shot图文生成。\"]},\"100\":{\"h\":\"Stage 1: Representation Learning （表征学习）\",\"t\":[\"tage 1: Representation Learning （表征学习）\",\"Q-Former 由两个transformer模块组成，输入包含三部分：\",\"冻结参数的Image Encoder提取的图像embeddings\",\"Learned Queries\",\"Queries是一组可学习的embeddings，是第一个transformer模块的input，可认为是模型参数一部分\",\"推理时，Queries被用来从image encoder输出的embeddings里提取与input text最相关的视觉信息\",\"Input Text\",\"Stage 1 使用 图像-文本对 进行预训练，目标是训练好 Q-Former，以便 Queries 可以学习到如何更好地结合文本提取图片信息。\",\"对于Q-Former，一种比较好理解的方式：把Q-Former类比为一个Self-attention模块\",\"Q：learned queries\",\"K：input text\",\"V：image embeddings from Image Encoder\",\"Blip2Qformer核心代码实现如下:\",\"利用 query tokens 从 image embeddings 中提取与 text 最相关的视觉信息\",\"将输入的 input text 进行编码 , 然后使用第一个CLS Token 作为 input text representation\",\"class Blip2Qformer(Blip2Base): ... def forward(self, samples): image = samples[\\\"image\\\"] # (B,C,H,W) text = samples[\\\"text_input\\\"] # (B,seq_len) # frozen vit 将图片编码成 (B, seq_len, hidden_size) image_embeds = self.ln_vision(self.visual_encoder(image)) # 构建padding mask标注哪些image token是有效的 (B,seq_len) image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to( image.device ) # 初始化query tokens (B,seq_len,hidden_size) query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1) # query tokens 从 image embeddings 中提取与 text 最相关的视觉信息 # query_output (B,seq_len,hidden_size) query_output = self.Qformer.bert( query_embeds=query_tokens, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, use_cache=True, return_dict=True, ) image_feats = F.normalize( self.vision_proj(query_output.last_hidden_state), dim=-1 ) # 将input text 进行编码，维度为 (B,seq_len,hidden_size) text_tokens = self.tokenizer( text, padding=\\\"max_length\\\", truncation=True, max_length=self.max_txt_len, return_tensors=\\\"pt\\\", ).to(image.device) text_output = self.Qformer.bert( text_tokens.input_ids, attention_mask=text_tokens.attention_mask, # padding mask return_dict=True, ) # 取第一个cls token作为input text representation，维度为 (B,hidden_size) text_feat = F.normalize( self.text_proj(text_output.last_hidden_state[:, 0, :]), dim=-1 ) ...\",\"以上代码注释中统一用B代替image_batch和text_batch，以及seq_len和hidden_size也是同样处理手段，大家注意区分。\",\"为了训练好Q-Former，第一阶段设计了三个训练目标，分别如下:\"]},\"101\":{\"h\":\"1、Image-Text Contrastive Learning (ITC Loss, CLIP-like)\",\"t\":[\"目的: Image representation 与 Text representation，以最大化互信息\",\"自注意力掩码策略: Uni-modal Self-attention Mask（单模态自注意力）\",\"Queries 和 Text 仅能和自己的 tokens 做 attention（Query和Query、Text和Text）\",\"Uni-modal Self-attention Mask\",\"image_feats 中每个 image_feat 与 text_feat 计算一个 similarity score ，选择最大值作为这个图文对的相似度 :\",\"similarity score\",\"如何计算loss的: “in-batch negatives”，该方法正是CLIP在VLP领域发扬光大的。以下引用CLIP论文图做说明：\",\"in-batch negatives\",\"###============== Image-text Contrastive ===================### # 计算每个query token 和 text_feat 的相似度 , 得到相似度矩阵 (B,B,seq_len) # image_feats (B,seq_len,hidden_size) 变为 (B,1,seq_len,hidden_size) # text_feat (B,hidden_size) 变为 (B,hidden_size,1) sim_q2t = torch.matmul( image_feats.unsqueeze(1), text_feat.unsqueeze(-1) ).squeeze() # image-text similarity: aggregate across all query tokens # 保留和text_feat相似度最大的那个query token作为最后的相似度得分 , 维度为 (B,B) sim_i2t, _ = sim_q2t.max(-1) sim_i2t = sim_i2t / self.temp # 反过来计算text_feat 和 每个query token的相似度 , 得到相似度矩阵 (B,B,seq_len) # image_feats 维度变为 (B,hidden_size,seq_len) # text_feat (B,hidden_size) 变为 (B,1,1,hidden_size) sim_t2q = torch.matmul( text_feat.unsqueeze(1).unsqueeze(1), image_feats.permute(0, 2, 1) ).squeeze() # text-image similarity: aggregate across all query tokens # 保留和text_feat相似度最大的那个query token作为最后的相似度得分 , 维度为 (B,B) sim_t2i, _ = sim_t2q.max(-1) sim_t2i = sim_t2i / self.temp # 生成比标签 targets = torch.arange(image.size(0), device=image.device) # 计算 图文对比 Loss loss_itc = ( # sim_i2t 形状是 (B, B)，每一行表示一张图像和所有文本之间的相似度。 F.cross_entropy(sim_i2t, targets, label_smoothing=0.1) + F.cross_entropy(sim_t2i, targets, label_smoothing=0.1) ) / 2\"]},\"102\":{\"h\":\"2、Image-Text Matching (ITM Loss，二分类task)\",\"t\":[\"目的：通过学习image-text pair是否match，以细粒度对齐 Image representation 与 Text representation\",\"自注意力掩码策略: Bi-directional Self-attention Mask（双向自注意力）\",\"Queries 和Text都能和所有的tokens 做attention\",\"Bi-directional Self-attention Mask\",\"每个output query embedding送到二分类器中，得到一个logit；所有logits的平均作为最终的matching score:\",\"matching score\",\" ###============== Image-text Matching ===================### text_input_ids_world = text_tokens.input_ids text_attention_mask_world = text_tokens.attention_mask image_embeds_world = image_embeds with torch.no_grad(): # bs (batch size) ， diag_indices = [0,1,2,...,bs-1] diag_indices = torch.arange(bs, device=sim_t2i.device) # 把相似度矩阵对角线元素置为负无穷大，以避免模型将匹配图文对挑选为负样本 # (0,0) , (1,1) ... (bs-1,bs-1) 位置处设置为 -10000 sim_t2i[diag_indices, diag_indices] = -10000 sim_i2t[diag_indices, diag_indices] = -10000 weights_t2i = F.softmax(sim_t2i, dim=1) weights_i2t = F.softmax(sim_i2t, dim=1) # 为每个文本选择一个负样本图像 image_embeds_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_t2i[b], 1).item() image_embeds_neg.append(image_embeds_world[neg_idx]) image_embeds_neg = torch.stack(image_embeds_neg, dim=0) # 为每个图像选择一个负样本文本 text_ids_neg = [] text_atts_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_i2t[b], 1).item() text_ids_neg.append(text_input_ids_world[neg_idx]) text_atts_neg.append(text_attention_mask_world[neg_idx]) text_ids_neg = torch.stack(text_ids_neg, dim=0) text_atts_neg = torch.stack(text_atts_neg, dim=0) # 构建输入文本列表: [正样本batch，负样本batch1，负样本batch2] ，维度为 (3*bs,seq_len) text_ids_all = torch.cat( [text_tokens.input_ids, text_tokens.input_ids, text_ids_neg], dim=0 ) text_atts_all = torch.cat( [text_tokens.attention_mask, text_tokens.attention_mask, text_atts_neg], dim=0, ) # 构建query tokens列表: [正样本batch，负样本batch1，负样本batch2] ，维度为 (3*bs,seq_len,hidden_size) query_tokens_itm = self.query_tokens.expand(text_ids_all.shape[0], -1, -1) query_atts_itm = torch.ones(query_tokens_itm.size()[:-1], dtype=torch.long).to( image.device ) # 构建query和text的padding mask ，维度为 (3*bs,seq_len) attention_mask_all = torch.cat([query_atts_itm, text_atts_all], dim=1) # 构建输入图像列表: [正样本batch，负样本batch1，负样本batch2] ，维度为 (3*bs,seq_len,hidden_size) image_embeds_all = torch.cat( [image_embeds, image_embeds_neg, image_embeds], dim=0 ) image_atts_all = torch.ones(image_embeds_all.size()[:-1], dtype=torch.long).to( image.device ) # 1. 将输入文本转换为嵌入列表后和query tokens 在seq_len维度上拼接起来，维度为 (3*bs,text_seq_len + query_tokens_seq_len,hidden_size) # 2. 将文本和query tokens拼接得到的结果和图像嵌入进行cross attention计算，编码后得到输出的结果 output_itm = self.Qformer.bert( text_ids_all, query_embeds=query_tokens_itm, attention_mask=attention_mask_all, encoder_hidden_states=image_embeds_all, encoder_attention_mask=image_atts_all, return_dict=True, ) # 取 (3*bs,text_seq_len + query_tokens_seq_len,hidden_size) 中 query tokens部分的结果，维度为 (3*bs,query_tokens_seq_len,hidden_size) vl_embeddings = output_itm.last_hidden_state[:, : query_tokens_itm.size(1), :] # 把query tokens部分的每个位置都映射到2维匹配空间，维度为 (3*bs,query_tokens_seq_len,2) vl_output = self.itm_head(vl_embeddings) # 取每个位置的平均作为最终的匹配得分，维度为 (3*bs,2) logits = vl_output.mean(dim=1) # 构建匹配标签: [正样本batch=1，负样本batch1=0，负样本batch2=0] ，维度为 (3*bs) itm_labels = torch.cat( [torch.ones(bs, dtype=torch.long), torch.zeros(2 * bs, dtype=torch.long)], dim=0, ).to(image.device) # 计算交叉熵损失 loss_itm = F.cross_entropy(logits, itm_labels)\",\"当文本和query tokens同时输入BertModel时，BertEmbeddings会将text embeddings和query tokens的embeddings在seq_len维度上拼接起来。\",\"class BertEmbeddings(nn.Module): ... def forward( self, input_ids=None, position_ids=None, query_embeds=None, past_key_values_length=0, ): # 计算序列长度 if input_ids is not None: seq_length = input_ids.size()[1] else: seq_length = 0 # 如果未提供位置id，则自动生成 if position_ids is None: position_ids = self.position_ids[ :, past_key_values_length : seq_length + past_key_values_length ].clone() # 词嵌入与位置嵌入相加，若有query_embeds则拼接 if input_ids is not None: embeddings = self.word_embeddings(input_ids) if self.position_embedding_type == \\\"absolute\\\": position_embeddings = self.position_embeddings(position_ids) embeddings = embeddings + position_embeddings if query_embeds is not None: embeddings = torch.cat((query_embeds, embeddings), dim=1) else: embeddings = query_embeds embeddings = self.LayerNorm(embeddings) embeddings = self.dropout(embeddings) return embeddings\",\"下图展示了 Image-Text Matching 的完整计算流程，关于BertModel的代码解析部分，将会在下文进行详细讲解:\",\"Image-Text Matching\"]},\"103\":{\"h\":\"3、Image-Grounded Text Generation (ITG Loss, GPT-like)\",\"t\":[\"目的：让Q-Former学习“图生文”的能力，即给定Input Image，生成Text\",\"自注意力掩码策略：Multimodal Causal Self-attention Mask（多模态因果自监督）\",\"Queies 可以和所有自己的tokens做attention\",\"Text 可以和所有的query tokens 及 当前token之前的text tokens做attention\",\"Multimodal Causal Self-attention Mask\",\"视觉编码阶段:\",\"图像通过视觉编码器（如 ViT）编码为图像特征 image_embeds。Query tokens 通过 cross-attention 吸收图像特征，再通过 self-attention 生成压缩的视觉表示。缓存 query tokens 的 self-attention 的 past_key_values（而非 cross-attention 的 key/value）。\",\"QFormer 会使用 past_key_values 缓存和复用 EncoderLayer 中 self-attention 的 key/value :\",\"BertSelfAttention: 自注意力和交叉注意力流程统一化，每次计算后返回本次可能需要缓存的key & value\",\"class BertSelfAttention(nn.Module): ... def forward( self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False, ): # 判断是否为交叉注意力 is_cross_attention = encoder_hidden_states is not None # 交叉注意力则key和value都来自图像,key来自query tokens if is_cross_attention: key_layer = self.transpose_for_scores(self.key(encoder_hidden_states)) value_layer = self.transpose_for_scores(self.value(encoder_hidden_states)) attention_mask = encoder_attention_mask # 如果有缓存的key,value传入, 此时先用text embedding计算出key和value # 再和缓存的key,value在seq_len的维度拼接起来 elif past_key_value is not None: key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) key_layer = torch.cat([past_key_value[0], key_layer], dim=2) # (Batch,Heads,Seq_len,Hidden_size) value_layer = torch.cat([past_key_value[1], value_layer], dim=2) else: # 自注意力 key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) # 交叉注意力: 传入图像，则q来自query tokens # 自注意力: q来自query tokens 或者 text embedding mixed_query_layer = self.query(hidden_states) query_layer = self.transpose_for_scores(mixed_query_layer) # * 缓存key和value past_key_value = (key_layer, value_layer) # 计算注意力分数 attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) attention_scores = attention_scores / math.sqrt(self.attention_head_size) if attention_mask is not None: # 应用注意力掩码 attention_scores = attention_scores + attention_mask # softmax归一化得到注意力概率 attention_probs = nn.Softmax(dim=-1)(attention_scores) if is_cross_attention and self.save_attention: self.save_attention_map(attention_probs) attention_probs.register_hook(self.save_attn_gradients) # dropout防止过拟合 attention_probs_dropped = self.dropout(attention_probs) # 计算上下文表示 context_layer = torch.matmul(attention_probs_dropped, value_layer) context_layer = context_layer.permute(0, 2, 1, 3).contiguous() new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) context_layer = context_layer.view(*new_context_layer_shape) outputs = ( (context_layer, attention_probs) if output_attentions else (context_layer,) ) # outputs 列表最后一个记录了缓存的key和value outputs = outputs + (past_key_value,) return outputs\",\"BertLayer: 负责组织自注意力和交叉注意力的运算流程\",\"class BertLayer(nn.Module): ... def forward( self, hidden_states, # query tokens attention_mask=None, # query token padding mask head_mask=None, encoder_hidden_states=None, # image tokens encoder_attention_mask=None, # image padding mask past_key_value=None, output_attentions=False, query_length=0, ): self_attn_past_key_value = ( past_key_value[:2] if past_key_value is not None else None ) # 自注意力运算 self_attention_outputs = self.attention( hidden_states, attention_mask, head_mask, output_attentions=output_attentions, past_key_value=self_attn_past_key_value, # 缓存的key和value ) attention_output = self_attention_outputs[0] outputs = self_attention_outputs[1:-1] present_key_value = self_attention_outputs[-1] # 交叉注意力运算 if query_length > 0: query_attention_output = attention_output[:, :query_length, :] if self.has_cross_attention: cross_attention_outputs = self.crossattention( query_attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions=output_attentions, ) query_attention_output = cross_attention_outputs[0] outputs = ( outputs + cross_attention_outputs[1:-1] ) ... outputs = (layer_output,) + outputs outputs = outputs + (present_key_value,) # outputs 列表最后一个记录了缓存的key和value return outputs\",\"BertEncoder: 负责组织多个 BertLayer 叠加的运算流程\",\"class BertEncoder(nn.Module): ... def forward( self, hidden_states, # query tokens attention_mask=None, # query tokens padding mask head_mask=None, encoder_hidden_states=None, # images encoder_attention_mask=None, # images padding mask past_key_values=None, use_cache=None, output_attentions=False, output_hidden_states=False, return_dict=True, query_length=0, ): ... for i in range(self.config.num_hidden_layers): layer_module = self.layer[i] ... # 如果有缓存，则计算当前层BertLayer时，会从缓存中取出对应层先前缓存的key&value past_key_value = past_key_values[i] if past_key_values is not None else None layer_outputs = layer_module( hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, query_length, ) hidden_states = layer_outputs[0] # 每一层BertLayer产生的key&value都会进行缓存 if use_cache: next_decoder_cache += (layer_outputs[-1],) ... return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=next_decoder_cache, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, )\",\"Image-Grounded Text Generation 学习目标\",\" ... query_output = self.Qformer.bert( query_embeds=query_tokens, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, use_cache=True, # 缓存key&value return_dict=True, ) ... ##================= Image Captioning ========================## # 这一部分的目标是：根据图像特征，使用 Q-Former 解码器生成文本描述（caption） # Step 1: 准备 decoder 的输入 token IDs decoder_input_ids = text_tokens.input_ids.clone() # 将第一个 token 替换为 BOS（Begin Of Sentence）标记，表示“开始生成句子” decoder_input_ids[:, 0] = self.tokenizer.bos_token_id # Step 2: 构造训练目标 labels # 将 padding token 替换为 -100，这是 CrossEntropyLoss 默认忽略的标签值 labels = decoder_input_ids.masked_fill( decoder_input_ids == self.tokenizer.pad_token_id, -100 ) # Step 3: 构建 attention_mask（包含 query tokens 和 文本 token 的 mask） # query_atts 是 query tokens 的 attention mask，全为 1（因为都是有效 token） query_atts = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(image.device) # 将 query token 的 mask 和文本 token 的 mask 拼接在一起 attention_mask = torch.cat([query_atts, text_tokens.attention_mask], dim=1) # Step 4: 调用 Q-Former 解码器进行文本生成 lm_output = self.Qformer( decoder_input_ids, # 输入 token ID 序列（如 [BOS], dog, is...） attention_mask=attention_mask, # 指明哪些位置是有效的（非 padding） past_key_values=query_output.past_key_values, # 编码器输出的 key/value，包含图像信息 return_dict=True, # 返回字典格式结果 labels=labels, # 训练目标，用于计算 loss ) # Step 5: 提取语言模型损失 loss_lm = lm_output.loss # 使用交叉熵损失衡量生成与真实之间的差异\",\"文本生成阶段:\",\"将缓存的 past_key_values 作为文本解码器的初始状态。\",\"文本 token 在自回归生成时，通过 self-attention 复用缓存的视觉信息。\",\"BertLMHeadModel: 自回归语言建模任务（如文本生成）\",\"class BertLMHeadModel(BertPreTrainedModel): ... def forward( self, input_ids=None, attention_mask=None, position_ids=None, head_mask=None, query_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, past_key_values=None, use_cache=True, output_attentions=None, output_hidden_states=None, return_dict=None, return_logits=False, is_decoder=True, reduction=\\\"mean\\\", ): ... # 调用 BertModel 进行文本编码 (结合缓存的attention key&value) outputs = self.bert( input_ids, attention_mask=attention_mask, position_ids=position_ids, head_mask=head_mask, query_embeds=query_embeds, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, is_decoder=is_decoder, ) sequence_output = outputs[0] ... # self.cls 是一个分类头（BertOnlyMLMHead），它将每个 token 的向量映射到词汇表空间（logits） prediction_scores = self.cls(sequence_output) ... lm_loss = None if labels is not None: # 因为我们要预测下一个 token，所以把 logits 和 labels 错位对齐： # shifted_prediction_scores: 所有 token 的预测（除了最后一个） shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous() # labels: 所有 token 的真实值（从第二个开始） labels = labels[:, 1:].contiguous() loss_fct = CrossEntropyLoss(reduction=reduction, label_smoothing=0.1) lm_loss = loss_fct( shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1), ) if reduction == \\\"none\\\": lm_loss = lm_loss.view(prediction_scores.size(0), -1).sum(1) ... return CausalLMOutputWithCrossAttentions( loss=lm_loss, logits=prediction_scores, past_key_values=outputs.past_key_values, hidden_states=outputs.hidden_states, attentions=outputs.attentions, cross_attentions=outputs.cross_attentions, )\",\"BertModel 的 forward 方法中，当is_decoder=True时，会在get_extended_attention_mask方法中，构建一个下三角矩阵作为因果掩码矩阵。\"]},\"104\":{\"h\":\"Stage 2: Generative Learning（生成学习）\",\"t\":[\"Stage 2 是为了把 Q-Former 和冻结参数的 LLM 连接起来，以利用 LLM 的文本生成能力。\",\"支持两种LLM（decoder only、encoder-decoder based）:\",\"Generative Learning\",\"首先输入图片，直接输入冻结参数的 Image Encoder，得到图像的表征。\",\"然后图像的表征和 Queries 一起送入 Q-Former，得到 Queries 的输出 ，使用全连接 (FC) 层将 线性投影到与 LLM 的text embedding相同维度。\",\"后将投影后的 添加到 input text embeddings前面，Queries 的输出蕴含了视觉信息，送入LLM时，充当了soft visual prompts 。\",\"由于 Q-Former 已经过预训练以提取语言信息视觉表示，因此它有效地充当信息瓶颈，将最有用的信息提供给 LLM，同时删除不相关的视觉信息。这减少了LLM学习视觉语言对齐的负担，从而缓解了灾难性的遗忘问题。\",\"Blip2Qformer 的generate方法负责完成图像描述生成（图文到文本）:\",\"class Blip2Qformer(Blip2Base): ... def generate( self, samples, # 输入样本，包含图像和可选文本 use_nucleus_sampling=False, # 是否使用核采样（top-p采样） num_beams=3, # beam search的beam数量 max_length=30, # 生成文本的最大长度 min_length=10, # 生成文本的最小长度 top_p=0.9, # 核采样的概率阈值 repetition_penalty=1.0, # 重复惩罚系数 ): # 1. 图像编码阶段 image = samples[\\\"image\\\"] # 通过视觉编码器（如ViT）提取图像特征 (B, 257, D) image_embeds = self.ln_vision(self.visual_encoder(image)) # 2. 处理beam search扩展 if not use_nucleus_sampling: # 如果是beam search，需要复制图像特征以匹配beam数量 # (B, 257, D) -> (B*num_beams, 257, D) image_embeds = image_embeds.repeat_interleave(num_beams, dim=0) else: # 核采样时不扩展beam num_beams = 1 # 创建图像注意力掩码（全1，表示所有图像token有效） image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to( image.device ) # 3. 准备生成参数 model_kwargs = { \\\"encoder_hidden_states\\\": image_embeds, # 图像特征作为cross-attention的输入 \\\"encoder_attention_mask\\\": image_atts, # 图像注意力掩码 } # 4. 初始化文本输入（以BOS token开头） # 形状: (batch_size, 1)，初始为[BOS] input_ids = ( torch.LongTensor(image.size(0), 1) .fill_(self.tokenizer.bos_token_id) .to(image.device) ) # 5. 扩展可学习的query tokens # query_tokens形状: (batch_size, num_query_tokens, D) query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1) # 6. 调用Q-Former的生成方法 outputs = self.Qformer.generate( input_ids=input_ids, # 初始文本token [BOS] query_embeds=query_tokens, # 可学习query tokens max_length=max_length, # 最大生成长度 min_length=min_length, # 最小生成长度 num_beams=num_beams, # beam数量 do_sample=use_nucleus_sampling, # 是否采样 top_p=top_p, # 核采样参数 eos_token_id=self.tokenizer.sep_token_id, # 结束符 pad_token_id=self.tokenizer.pad_token_id, # 填充符 **model_kwargs # 图像特征和掩码 ) # 7. 解码生成的token id为文本 captions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True) return captions\"]},\"105\":{\"h\":\"庖丁解牛VIT\",\"t\":[\"多模态模型VIT原理与图片分类实战演练\",\"Vision Transformer是2021年谷歌在ICLR上提出的算法，它首次将NLP领域火热的Transformer模型架构移植到了CV领域，打破了这两个领域壁垒，并取得不错的成效。\",\"Vision Transformer的模型结构相比于Transformer来说更简单，在Transformer模型中，主要包含Encoder和Decoder结构，而ViT(Vision Transformer)仅借鉴了Encoder结构。\",\"ViT原论文中最核心的结论是: 当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果。\",\"归纳偏置:\",\"归纳偏置能够帮助学习算法缩小搜索范围，快速找到合适的模型。\",\"例如，在图像分类任务中，如果没有任何归纳偏置，学习算法需要在所有可能的函数空间中搜索最优模型，这几乎是不可能完成的任务。而通过引入特定的归纳偏置，如局部性和平移不变性（CNN 所具备的），可以将搜索范围限制在满足这些性质的模型子空间内，大大提高学习效率。\",\"但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些，因为Transformer和CNN相比缺少归纳偏置（inductive bias），即一种先验知识，提前做好的假设。CNN具有两种归纳偏置，一种是局部性（locality/two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征；一种是平移不变形（translation equivariance）。当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型。\"]},\"106\":{\"h\":\"原理\",\"t\":[\"本文将通过一个花卉分类的实战案例结合ViT原论文，来帮助大家梳理清楚Vision Transformer的核心流程实现。\"]},\"107\":{\"h\":\"0. 数据下载\",\"t\":[\"实验采用的是花蕊数据集，共5个类别，约4000多个样本。\",\"数据集下载：https://pan.baidu.com/s/137mO-7PY1jDq1Wp0NNyT3A?pwd=qvmq\",\"数据集加载代码:\",\"def read_split_data(root: str, val_rate: float = 0.2): random.seed(0) # 保证随机结果可复现 assert os.path.exists(root), \\\"dataset root: {} does not exist.\\\".format(root) # 遍历文件夹，一个文件夹对应一个类别 flower_class = [cla for cla in os.listdir(root) if os.path.isdir(os.path.join(root, cla))] # 排序，保证顺序一致 flower_class.sort() # 生成类别名称以及对应的数字索引 class_indices = dict((k, v) for v, k in enumerate(flower_class)) json_str = json.dumps(dict((val, key) for key, val in class_indices.items()), indent=4) with open('class_indices.json', 'w') as json_file: json_file.write(json_str) train_images_path = [] # 存储训练集的所有图片路径 train_images_label = [] # 存储训练集图片对应索引信息 val_images_path = [] # 存储验证集的所有图片路径 val_images_label = [] # 存储验证集图片对应索引信息 every_class_num = [] # 存储每个类别的样本总数 supported = [\\\".jpg\\\", \\\".JPG\\\", \\\".png\\\", \\\".PNG\\\"] # 支持的文件后缀类型 # 遍历每个文件夹下的文件 for cla in flower_class: cla_path = os.path.join(root, cla) # 遍历获取supported支持的所有文件路径 images = [os.path.join(root, cla, i) for i in os.listdir(cla_path) if os.path.splitext(i)[-1] in supported] # 获取该类别对应的索引 image_class = class_indices[cla] # 记录该类别的样本数量 every_class_num.append(len(images)) # 按比例随机采样验证样本 val_path = random.sample(images, k=int(len(images) * val_rate)) for img_path in images: if img_path in val_path: # 如果该路径在采样的验证集样本中则存入验证集 val_images_path.append(img_path) val_images_label.append(image_class) else: # 否则存入训练集 train_images_path.append(img_path) train_images_label.append(image_class) print(\\\"{} images were found in the dataset.\\\".format(sum(every_class_num))) print(\\\"{} images for training.\\\".format(len(train_images_path))) print(\\\"{} images for validation.\\\".format(len(val_images_path))) plot_image = True if plot_image: # 绘制每种类别个数柱状图 plt.bar(range(len(flower_class)), every_class_num, align='center') # 将横坐标0,1,2,3,4替换为相应的类别名称 plt.xticks(range(len(flower_class)), flower_class) # 在柱状图上添加数值标签 for i, v in enumerate(every_class_num): plt.text(x=i, y=v + 5, s=str(v), ha='center') # 设置x坐标 plt.xlabel('image class') # 设置y坐标 plt.ylabel('number of images') # 设置柱状图的标题 plt.title('flower class distribution') plt.show() return train_images_path, train_images_label, val_images_path, val_images_label\",\"自定义一个MyDataSet类来封装我们加载得到的数据集:\",\"from torch.utils.data import Dataset from PIL import Image import torch class MyDataSet(Dataset): \\\"\\\"\\\"自定义数据集\\\"\\\"\\\" def __init__(self, images_path: list, images_class: list, transform=None): \\\"\\\"\\\" 初始化自定义数据集类 :param images_path: 包含所有图像文件路径的列表 :param images_class: 包含所有图像对应类别的列表，与 images_path 中的图像一一对应 :param transform: 图像预处理的转换操作，默认为 None \\\"\\\"\\\" self.images_path = images_path self.images_class = images_class self.transform = transform def __len__(self): \\\"\\\"\\\" 返回数据集中图像的数量 :return: 数据集中图像的数量 \\\"\\\"\\\" return len(self.images_path) def __getitem__(self, item): \\\"\\\"\\\" 根据索引获取数据集中的图像和对应的标签 :param item: 图像的索引 :return: 经过预处理的图像和对应的标签 \\\"\\\"\\\" # 打开指定索引的图像文件 img = Image.open(self.images_path[item]) # RGB为彩色图片，L为灰度图片 # 检查图像是否为 RGB 模式，如果不是则抛出异常 if img.mode != 'RGB': raise ValueError(\\\"image: {} isn't RGB mode.\\\".format(self.images_path[item])) # 获取对应图像的标签 label = self.images_class[item] # 如果定义了图像预处理转换操作，则对图像进行处理 if self.transform is not None: img = self.transform(img) return img, label @staticmethod def collate_fn(batch): \\\"\\\"\\\" 自定义的批量数据处理函数，用于将一个批次的数据组合成一个张量 :param batch: 一个批次的数据，包含图像和对应的标签 :return: 组合后的图像张量和标签张量 \\\"\\\"\\\" # 官方实现的default_collate可以参考 # https://github.com/pytorch/pytorch/blob/67b7e751e6b5931a9f45274653f4f653a4e6cdf6/torch/utils/data/_utils/collate.py # 将一个批次的数据拆分为图像和标签两个元组 images, labels = tuple(zip(*batch)) # 将图像元组堆叠成一个四维张量，维度为 (batch_size, channels, height, width) images = torch.stack(images, dim=0) # 将标签元组转换为一个一维张量 labels = torch.as_tensor(labels) return images, labels\",\"两点注意:\",\"当使用 DataLoader 从数据集（Dataset）中加载数据时，它会将多个样本收集起来形成一个批次，但默认的组合方式可能不满足所有需求，这时就可以自定义 collate_fn 函数。\",\"@staticmethod 是 Python 中的一个装饰器，用于将一个方法定义为静态方法。静态方法是类中的一种特殊方法，它与类的实例和类本身都没有直接关联，可以直接通过类名调用，不需要创建类的实例。\"]},\"108\":{\"h\":\"1. 图片预处理\",\"t\":[\"预处理这个步骤在论文里并没有详细说明，但是对于ViT这个结构而言，输入的图片尺寸并不是自定义的，ViT-B/16为例，输入的图片尺寸必须为224x224。\",\"在 ViT - B/16 中，“B” 代表的是模型的基础（Base）版本 ，“16” 表示每个图像块的大小是 16x16 像素；ViT 通常在大规模数据集（如 ImageNet）上进行预训练，而预训练过程中使用的输入图像尺寸通常固定为 224x224。在预训练时，模型的参数是根据这个特定尺寸的输入数据进行优化和学习的。当我们在其他任务中使用预训练好的模型时，为了充分利用预训练的权重，也需要保持输入图像尺寸与预训练时一致，这样可以保证模型的特征提取能力和性能。\",\"因此，首先需要对输入图片进行尺寸变化，具体方式可以是直接缩放(Resize)，也可以进行随机裁剪(RandomResizedCrop)。\",\"对数据集和验证集划分之后，这里对训练集的处理方式是随机切成224x224像素的图片，然后进行水平翻转，再进行归一化和标准化处理；对验证集的处理方式是先Resize成256x256的图片，再从中心位置裁剪成224x224，再进行归一化和标准化处理。\",\"# 定义一个字典 data_transform，用于存储训练集和验证集的图像预处理转换操作 data_transform = { # 训练集的预处理转换操作 \\\"train\\\": transforms.Compose([ # 随机裁剪输入图像，将裁剪后的图像调整为 224x224 大小 # 这是一种数据增强的方式，通过随机裁剪可以增加训练数据的多样性，提高模型的泛化能力 transforms.RandomResizedCrop(224), # 以 0.5 的概率随机水平翻转图像 # 同样是数据增强的手段，增加了图像的多样性，有助于模型学习到不同方向的特征 transforms.RandomHorizontalFlip(), # 将 PIL 图像或 NumPy 数组转换为 PyTorch 的张量（Tensor） # 同时会将像素值从 [0, 255] 范围缩放到 [0, 1] 范围 transforms.ToTensor(), # 对图像进行归一化处理 # 第一个参数 [0.5, 0.5, 0.5] 是图像每个通道的均值，第二个参数 [0.5, 0.5, 0.5] 是图像每个通道的标准差 # 归一化有助于模型更快地收敛，提高训练的稳定性 transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) ]), # 验证集的预处理转换操作 \\\"val\\\": transforms.Compose([ # 将图像的短边缩放为 256 像素，长边按比例缩放 # 这一步是为了保证图像的整体比例不变，后续再进行裁剪操作 transforms.Resize(256), # 从图像的中心位置裁剪出 224x224 大小的图像 # 验证集不需要进行数据增强，只需要将图像调整到合适的大小 transforms.CenterCrop(224), # 将 PIL 图像或 NumPy 数组转换为 PyTorch 的张量（Tensor） # 同样会将像素值从 [0, 255] 范围缩放到 [0, 1] 范围 transforms.ToTensor(), # 对图像进行归一化处理，参数与训练集的归一化参数相同 # 保证训练集和验证集的数据处理方式一致 transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) ]) }\",\"下面我们将用于图片变换的transforms流水线和上面自定义的MyDataSet类都封装到DataLoader去。\",\"train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(data_path) # 实例化训练数据集 train_dataset = MyDataSet(images_path=train_images_path, images_class=train_images_label, transform=data_transform[\\\"train\\\"]) # 实例化验证数据集 val_dataset = MyDataSet(images_path=val_images_path, images_class=val_images_label, transform=data_transform[\\\"val\\\"]) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn) val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=val_dataset.collate_fn)\"]},\"109\":{\"h\":\"2. 图片切割\",\"t\":[\"Transformer需要输入的是一维的Token，对于二维的图像，一种朴素的想法就是把一个个像素点拉平，这样就成了一个一维序列。但是这样造成的一个后果是计算量太庞大，比如一张224x224的图片，变成1维度之后就成了50176，相当于直接输入一篇五万字的文章，模型难以计算。\",\"那么，一个改进的想法就是把一张图片分成nxn个Patch，每一个Patch作为一个Token，这样计算量就大大减小了。\",\"以ViT-B/16为例，将输入图片(224x224)按照16x16大小的Patch进行划分，划分后可以得到共个Patch。每个Patch是三通道的小图片，shape为(16, 16, 3)，将其展平就变成了一个长度为768的向量。\",\"每一个向量作为一个单独的输入，那样我们总共有196个向量，在代码中，可以变成一个[196,768]的矩阵，进行并行输入。\",\"这一步的操作在论文中是直接采用切割的处理办法，但是实际的代码实现中，采用了一种更巧妙的解决思路，就是利用一个卷积核大小为16x16，步距为16，卷积核个数为768的卷积层来进行实现。\",\"再来回顾我们的卷积层计算公式：\",\"输入为[224,244,3]，经过卷积层变成[14,14,768]，再映射为[196,768]。\",\"这样，就完成了从图片到Token之间的转换，我们通过自定义一个PatchEmbed类完成上述工作。\",\"class PatchEmbed(nn.Module): \\\"\\\"\\\" 2D Image to Patch Embedding 该类的作用是将二维图像分割成多个图像块（patch），并将这些图像块嵌入到一个低维向量空间中 \\\"\\\"\\\" def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None): \\\"\\\"\\\" 初始化 PatchEmbed 类 :param img_size: 输入图像的尺寸，默认为 224。如果传入一个整数，则表示图像是正方形，边长为该整数； :param patch_size: 每个图像块的尺寸，默认为 16。同样，如果传入一个整数，则表示图像块是正方形，边长为该整数； :param in_c: 输入图像的通道数，默认为 3（对应 RGB 图像） :param embed_dim: 嵌入维度，即每个图像块经过卷积操作后得到的特征向量的维度，默认为 768 :param norm_layer: 归一化层，默认为 None。如果传入一个归一化层类，则使用该层进行归一化；否则使用 nn.Identity() 表示不进行归一化 \\\"\\\"\\\" super().__init__() # 将 img_size 和 patch_size 转换为元组形式，如果传入的是整数，则将其转换为 (整数, 整数) 的形式 img_size = (img_size, img_size) patch_size = (patch_size, patch_size) self.img_size = img_size self.patch_size = patch_size # 计算网格大小，即图像在水平和垂直方向上分别可以划分的图像块数量 self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) # 计算图像块的总数，即网格大小的乘积 self.num_patches = self.grid_size[0] * self.grid_size[1] # 定义一个二维卷积层，用于将输入图像分割成多个图像块并进行嵌入 # in_c 是输入通道数，embed_dim 是输出通道数（也就是卷积核的数量） # kernel_size 是卷积核的大小，这里设置为图像块的大小 # stride 是卷积核的步长，这里设置为图像块的大小，确保卷积操作不会重叠 self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size) # 如果传入了归一化层类，则使用该层进行归一化；否则使用 nn.Identity() 表示不进行归一化 self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() def forward(self, x): \\\"\\\"\\\" 前向传播函数 :param x: 输入的图像张量，形状为 [B, C, H, W]，其中 B 是批量大小，C 是通道数，H 是图像高度，W 是图像宽度 :return: 经过处理后的图像块嵌入张量，形状为 [B, num_patches, embed_dim] \\\"\\\"\\\" # 获取输入图像张量的形状 B, C, H, W = x.shape # 注意下面的embed_dim代表的是卷积核的数量，也就是经过卷积后拼接得到的特征图(输出通道)数量 # H`和 W`代表输出特征图的宽和高 # 首先使用卷积层对输入图像进行处理，得到形状为 [B, embed_dim, H', W'] 的特征图 # 然后将特征图的最后两维展平为一维，得到形状为 [B, embed_dim, num_patches] 的张量 # 最后交换第 1 维和第 2 维，得到形状为 [B, num_patches, embed_dim] 的张量 # 这里的 num_patches 是图像块的总数 x = self.proj(x).flatten(2).transpose(1, 2) # 对处理后的张量进行归一化操作 x = self.norm(x) return x\",\"用一个简化版的例子说明上述过程:\",\"核心要点: 将卷积后的通道维数作为embedding的维度，卷积后剩余的长和宽相乘作为时间维度，由此把图片转换为序列的embedding形式。\"]},\"110\":{\"h\":\"3. 添加[class]token\",\"t\":[\"在上面的结构图中可以看到，输入Encoder的最左侧部分添加了一个0*这个Token，这个就是额外添加的一个[class]token，单独用来处理类别信息，经过Encoder之后，需要单独将这个Token再提取出来，输入到MLP Head之中再输出分类结果。\",\"这也是为什么结构图中MLP Head的位置是和这个[class]token对齐。\",\"这里简单介绍一下CLS TOKEN的作用:\",\"[CLS] Token 的作用是通过训练过程中损失值的降低，模型学会如何通过注意力机制将图像的有效信息汇聚到 [CLS] Token 中，从而完成图像分类任务。\",\"初始化： \",\"[CLS] Token 是一个随机初始化的向量，初始时没有任何语义信息。\",\"位置编码被添加到 patch 嵌入中，以保留图像的空间信息。\",\"前向传播： \",\"输入图像被分割成 patches，并通过线性变换映射到嵌入空间。\",\"[CLS] Token 被添加到 patch 嵌入序列的开头。\",\"通过多层 Transformer Encoder，模型计算每个 patch 嵌入（包括 [CLS] Token）与其他 patch 嵌入的关系。\",\"注意力汇聚： \",\"在每一层 Transformer 中，[CLS] Token 通过自注意力机制与其他 patch 嵌入交互。\",\"模型学会将图像中与分类任务相关的信息汇聚到 [CLS] Token 中。\",\"损失计算与反向传播： \",\"[CLS] Token 的输出向量被输入到分类头中，用于预测图像的类别。\",\"通过计算损失（如交叉熵损失），模型更新参数，使得 [CLS] Token 能够更好地聚合图像信息。\",\"收敛： \",\"随着训练的进行，损失值逐渐降低，模型学会如何通过注意力机制将图像的有效信息汇聚到 [CLS] Token 中。\",\"最终，[CLS] Token 的输出向量能够很好地表示图像的全局特征，用于分类任务。\",\"[CLS] Token 能起作用的原因在于：\",\"注意力机制的特性： \",\"自注意力机制能够捕捉图像中任意两个 patches 之间的关系。\",\"[CLS] Token 通过与其他 patches 的交互，能够动态地聚合图像信息。\",\"训练目标的引导： \",\"训练过程中，损失函数直接作用于 [CLS] Token 的输出。\",\"模型被强制学会将图像的有效信息汇聚到 [CLS] Token 中，以最小化损失。\",\"全局特征表示： \",\"[CLS] Token 位于序列的开头，能够通过多层 Transformer 逐步聚合全局信息。\",\"最终，[CLS] Token 的输出向量能够很好地表示图像的全局特征。\",\"class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, embed_layer=None): \\\"\\\"\\\" Args: img_size (int, tuple): 输入图像的尺寸 patch_size (int, tuple): 图像块的尺寸 in_c (int): 输入图像的通道数 num_classes (int): 分类任务的类别数 embed_dim (int): 嵌入维度 embed_layer (nn.Module): 图像块嵌入层 \\\"\\\"\\\" super(VisionTransformer, self).__init__() # 保存分类任务的类别数 self.num_classes = num_classes # 保存嵌入维度，同时作为特征数量，以保持与其他模型的一致性 self.num_features = self.embed_dim = embed_dim # 只有一个特殊标记（分类标记） self.num_tokens = 1 # 创建图像块嵌入层 self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # 计算图像块的数量 num_patches = self.patch_embed.num_patches # 创建可学习的分类标记 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 使用截断正态分布初始化分类标记 nn.init.trunc_normal_(self.cls_token, std=0.02) ... def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] # 将输入图像进行图像块嵌入 x = self.patch_embed(x) # [B, 196, 768] # [1, 1, 768] -> [B, 1, 768] # 扩展分类标记以匹配输入批次大小 cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将分类标记和图像块嵌入拼接 x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] ... # 返回分类标记对应的特征,x[:,0]对应维度为[B,1,768] return x[:,0]; def forward(self, x): # 提取输入图像的特征 x = self.forward_features(x) # 通过分类头 -- [B,1,768] x = self.head(x) return x\"]},\"111\":{\"h\":\"4. 添加位置编码\",\"t\":[\"在Transformer中，位置编码的作用是为了记忆输入的语序信息。ViT中，同样需要位置编码来记录各图像块之间的位置信息。\",\"这里主要有两种位置编码思路，一种思路是在转换之前(14,14)的图像块矩阵添加二维(2-D)位置编码，另一种思路是在转换后(196+1)这个维度上添加一维(1-D)位置编码。\",\"论文作者也对其做了实验，实验结果如下表所示：\",\" 可以看到，添加一维位置编码和二维位置编码并没有太大的差异。作者随后也对一维位置编码的结果进行了可视化，结果如下图所示：\",\" 上图中是每一个Patch中各位置的位置编码相似性度量，越接近黄色的位置代表越靠近位置编码的中心位置，可以看到，即使是一维位置编码，同样可以比较好地记录二维信息。\",\"class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, representation_size=None, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., embed_layer=None, norm_layer=None, act_layer=None): super(VisionTransformer, self).__init__() # 保存分类任务的类别数 self.num_classes = num_classes # 保存嵌入维度，同时作为特征数量，以保持与其他模型的一致性 self.num_features = self.embed_dim = embed_dim # 只有一个特殊标记（分类标记） self.num_tokens = 1 # 创建图像块嵌入层 self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # 计算图像块的数量 num_patches = self.patch_embed.num_patches # 创建可学习的分类标记 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 创建可学习的位置嵌入 self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim)) # 创建丢弃层，用于位置嵌入后的随机丢弃 self.pos_drop = nn.Dropout(p=drop_ratio) ... # 权重初始化 # 使用截断正态分布初始化位置嵌入 nn.init.trunc_normal_(self.pos_embed, std=0.02) # 使用截断正态分布初始化分类标记 nn.init.trunc_normal_(self.cls_token, std=0.02) ... def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] # 将输入图像进行图像块嵌入 x = self.patch_embed(x) # [B, 196, 768] # [1, 1, 768] -> [B, 1, 768] # 扩展分类标记以匹配输入批次大小 cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将分类标记和图像块嵌入拼接 x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] # 加上位置嵌入并进行随机丢弃 x = self.pos_drop(x + self.pos_embed) ... # 返回分类标记对应的特征 return x[:, 0] def forward(self, x): # 提取输入图像的特征 x = self.forward_features(x) # 通过分类头 x = self.head(x) return x\",\"上面代码实现中使用的是可学习位置嵌入，具体解释如下:\",\"可学习位置嵌入（learnable positional embedding）是指位置嵌入的参数是可以在模型训练过程中通过反向传播算法进行更新的。具体来说，在模型初始化时，位置嵌入会被初始化为一组特定的值（通常是随机初始化或者初始化为零），然后在训练过程中，这些值会根据模型的损失函数不断调整，以使得模型能够学习到最适合当前任务的位置表示。\"]},\"112\":{\"h\":\"5. Encoder\",\"t\":[\"ViT虽然采用的是Transformer Encoder的结构，但是和Transformer原始的Encoder还是有所区别，我将两者的结构进行对比，如下图所示，左侧为Transformer原始的Encoder结构。\",\" 可以看到，大致上两者结构是相同的，主要区别在于Norm层的顺序，原始Transformer的Norm层在多头注意力和前馈网络之后，而ViT将其放到前面，这里的原因，论文里没有做解释。\",\"关于Norm层，ViT仍是采用Transformer中用到Layer Normalization，计算公式如下：\",\"Norm层之后同样是多头注意力层(Multi-Head Attention)，和Transformer中的一样。\",\"后面的MLP是个单独的结构，就是两个线性层+GELU激活函数+Dropout的结构 ：\",\" MLP Block 中第一个线性层把输入特征投影到一个更高维度的空间后，不同特征之间能够进行更多样的组合。这有助于模型发现输入数据中更复杂的模式和关系。第二个线性层再把高维特征映射回原来的维度，这样就可以提取出对最终任务有帮助的特征组合。\",\"单一的线性层只能进行线性变换，其表达能力是有限的。在两个线性层之间通常会插入一个非线性激活函数（如 GELU），这样就能让 MLP 学习到输入数据的非线性特征。第一个线性层将输入特征映射到更高维度的空间，在这个高维空间里，数据的分布更加稀疏，也就为非线性激活函数提供了更多可以学习的特征组合，从而增强了模型的表达能力。\",\"class Block(nn.Module): def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm): super(Block, self).__init__() # 第一个归一化层，对输入进行归一化处理 self.norm1 = norm_layer(dim) # 多头自注意力层 self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio) # DropPath 层，用于随机深度，当 drop_path_ratio 大于 0 时使用，否则使用恒等映射 self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity() # 第二个归一化层，对经过注意力层的输出进行归一化处理 self.norm2 = norm_layer(dim) # 计算 MLP 的隐藏维度 mlp_hidden_dim = int(dim * mlp_ratio) # 创建 MLP 层 self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio) def forward(self, x): # 残差连接：输入加上经过归一化和注意力层处理后的输出 x = x + self.drop_path(self.attn(self.norm1(x))) # 残差连接：输入加上经过归一化和 MLP 层处理后的输出 x = x + self.drop_path(self.mlp(self.norm2(x))) return x\",\"class Mlp(nn.Module): \\\"\\\"\\\" MLP as used in Vision Transformer, MLP-Mixer and related networks \\\"\\\"\\\" def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.): super().__init__() # 如果未指定 out_features，则默认为 in_features out_features = out_features or in_features # 如果未指定 hidden_features，则默认为 in_features hidden_features = hidden_features or in_features # 第一个全连接层，将输入特征映射到隐藏特征空间 self.fc1 = nn.Linear(in_features, hidden_features) # 激活函数层，默认使用 GELU 激活函数 self.act = act_layer() # 第二个全连接层，将隐藏特征映射到输出特征空间 self.fc2 = nn.Linear(hidden_features, out_features) # Dropout 层，用于防止过拟合 self.drop = nn.Dropout(drop) def forward(self, x): # 通过第一个全连接层 x = self.fc1(x) # 通过激活函数层 x = self.act(x) # 应用 Dropout x = self.drop(x) # 通过第二个全连接层 x = self.fc2(x) # 再次应用 Dropout x = self.drop(x) return x\",\"一个block之后维度依然和输入相同，都是197 x 768 ，因此可以堆叠多个block。\"]},\"113\":{\"h\":\"6. 多头自注意力\",\"t\":[\"ViT中的多头自注意力模块实现逻辑和Transformer基本一致，主要的区别就是去掉了Paddding_Mask和Casual_Mask部分相关的掩码逻辑。\",\"下面所给出的代码实现，注意是通过一个线性层来同时计算qkv三个矩阵，这样可以提升计算效率。\",\"class Attention(nn.Module): def __init__(self, dim, # 嵌入层维度 num_heads=8, # 注意力头的数量，默认为8 qkv_bias=False, # 是否在生成Q、K、V时使用偏置，默认为False qk_scale=None, # 缩放因子，用于调整注意力分数，若为None则使用默认值 attn_drop_ratio=0., # 注意力矩阵的丢弃率，默认为0 proj_drop_ratio=0.): # 投影层的丢弃率，默认为0 super(Attention, self).__init__() self.num_heads = num_heads # 保存注意力头的数量 head_dim = dim // num_heads # 计算每个注意力头的维度 self.scale = qk_scale or head_dim ** -0.5 # 确定缩放因子，若qk_scale未指定，则使用默认的缩放因子 # 定义一个线性层，将输入的维度dim映射到dim * 3，用于同时生成查询（Q）、键（K）和值（V） self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) # 定义注意力矩阵的丢弃层，防止过拟合 self.attn_drop = nn.Dropout(attn_drop_ratio) # 定义投影层，将多头注意力的输出进行线性变换 self.proj = nn.Linear(dim, dim) # 定义投影层的丢弃层，防止过拟合 self.proj_drop = nn.Dropout(proj_drop_ratio) # 没有padding_mask, casual_mask def forward(self, x): # [batch_size, num_patches + 1, total_embed_dim] # 获取输入张量x的形状，B为批量大小，N为序列长度（包含分类token），C为输入token的总维度 B, N, C = x.shape # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim] # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head] # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head] # 通过qkv线性层将输入x映射到dim * 3的维度，然后调整形状并重新排列维度 # 下面的3是因为我们用一次矩阵运算得到了拼接在一起的Q,K,V矩阵，这里需要将其分离开来 qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) # [batch_size, num_heads, num_patches + 1, embed_dim_per_head] # 从qkv张量中分离出查询（Q）、键（K）和值（V） # 注意: Q,K,V计算来源相同,因此是自注意力 q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use tensor as tuple) # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1] # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1] # 将Q和K的转置相乘，得到注意力分数矩阵，再乘以缩放因子scale attn = (q @ k.transpose(-2, -1)) * self.scale # 对注意力分数矩阵应用softmax函数，得到注意力权重矩阵 attn = attn.softmax(dim=-1) # 对注意力权重矩阵应用丢弃层，防止过拟合 attn = self.attn_drop(attn) # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head] # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head] # reshape: -> [batch_size, num_patches + 1, total_embed_dim] # 将注意力权重矩阵与V相乘，得到每个注意力头的输出 # 对输出进行维度交换和形状调整，将多个注意力头的输出合并为一个张量 x = (attn @ v).transpose(1, 2).reshape(B, N, C) # 通过投影层对合并后的张量进行线性变换 x = self.proj(x) # 对投影后的结果应用丢弃层，防止过拟合 x = self.proj_drop(x) return x\",\"关于多头注意力机制流程不太清楚的，可以看这篇文章。\"]},\"114\":{\"h\":\"7. MLP Head\",\"t\":[\"在Transformer Encoder输出结果之后，需要再将第一个添加的Class Token提取出来，然后输入到MLP Head进行分类。在论文中，作者先是在ImageNet21K上进行预训练，MLP Head结构由Linear+tanh激活函数+Linear组成，但是迁移到其它数据集训练时，只需要用一个一个Linear即可。\",\"class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, representation_size=None, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., embed_layer=None, norm_layer=None, act_layer=None): super(VisionTransformer, self).__init__() # 保存分类任务的类别数 self.num_classes = num_classes # 保存嵌入维度，同时作为特征数量，以保持与其他模型的一致性 self.num_features = self.embed_dim = embed_dim # 只有一个特殊标记（分类标记） self.num_tokens = 1 # 如果没有提供归一化层，则使用默认的 LayerNorm，epsilon 为 1e-6 norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6) # 如果没有提供激活函数层，则使用 GELU 激活函数 act_layer = act_layer or nn.GELU # 创建图像块嵌入层 self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # 计算图像块的数量 num_patches = self.patch_embed.num_patches # 创建可学习的分类标记 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 创建可学习的位置嵌入 self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim)) # 创建丢弃层，用于位置嵌入后的随机丢弃 self.pos_drop = nn.Dropout(p=drop_ratio) # 创建Encoder Block块序列 self.blocks = nn.Sequential(*[ Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i], norm_layer=norm_layer, act_layer=act_layer) for i in range(depth) ]) # 创建归一化层 self.norm = norm_layer(embed_dim) ############################# MLP Head ############################################ # 更新特征数量为表示层的维度 self.num_features = representation_size # 创建预输出层，包含一个线性层和一个 Tanh 激活函数 self.pre_logits = nn.Sequential(OrderedDict([ (\\\"fc\\\", nn.Linear(embed_dim, representation_size)), (\\\"act\\\", nn.Tanh()) ])) # 分类头 # 如果类别数大于 0，则创建线性分类头，否则为恒等映射 self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity() ########################################################################### # 权重初始化 # 使用截断正态分布初始化位置嵌入 nn.init.trunc_normal_(self.pos_embed, std=0.02) # 使用截断正态分布初始化分类标记 nn.init.trunc_normal_(self.cls_token, std=0.02) # 应用自定义的权重初始化函数 self.apply(_init_vit_weights) def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] # 将输入图像进行图像块嵌入 x = self.patch_embed(x) # [B, 196, 768] # [1, 1, 768] -> [B, 1, 768] # 扩展分类标记以匹配输入批次大小 cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将分类标记和图像块嵌入拼接 x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] # 加上位置嵌入并进行随机丢弃 x = self.pos_drop(x + self.pos_embed) # 通过Encoder Block块序列 x = self.blocks(x) # 进行归一化 x = self.norm(x) # 返回分类标记对应的特征 -- 先交给预输出层进行处理 return self.pre_logits(x[:, 0]) def forward(self, x): # 提取输入图像的特征 x = self.forward_features(x) # 通过分类头 -- 映射到分类空间中去 x = self.head(x) return x\",\"self.pre_logits 模块可以看作是一个特征预处理模块，它位于最终分类头之前。通过将特征映射到特定的维度并进行非线性变换，该模块可以为后续的分类任务提供更具区分性和稳定性的特征表示，从而提高模型的分类性能。\",\"输出结果之后，再和真实标签做交叉熵损失，这样就可以完成ViT的训练过程。\",\"def train_one_epoch(model, optimizer, data_loader, device, epoch): ... # 遍历数据加载器中的每个批次数据 for step, data in enumerate(data_loader): # 解包数据，得到图像和对应的标签 images, labels = data # 累加当前批次的样本数到总样本数中 sample_num += images.shape[0] # 将图像数据移动到指定设备上，并通过模型进行前向传播，得到预测结果 pred = model(images.to(device)) # 从预测结果中找出每个样本预测概率最大的类别索引 pred_classes = torch.max(pred, dim=1)[1] # 计算预测正确的样本数，并累加到累计正确样本数中 accu_num += torch.eq(pred_classes, labels.to(device)).sum() # 计算预测结果与真实标签之间的交叉熵损失 loss = loss_function(pred, labels.to(device)) # 进行反向传播，计算梯度 loss.backward() ...\"]},\"115\":{\"h\":\"效果对比\",\"t\":[\"在论文中，作者将ViT和之前图像分类领域比较强的ResNet模型进行了对比测试，结果如下：\",\" 可以看到，右图中，作者使用了谷歌制作的JFT-300M数据集，当数据量小于30M时，ViT的效果表现不如ResNet，但是当数据量逐渐增大时，ViT才会慢慢超越ResNet。由此可见ViT工作的局限性，它必须要在超大数据集上进行预训练，然后再拿到其它数据集上做迁移学习，才会有好的效果。\",\"关于ViT模型的不同版本，论文里也做了说明： 其中的Layers就是Transformer Encoder中重复堆叠Encoder Block的次数，Hidden Size就是对应通过Embedding层后每个token的dim（向量的长度），MLP size是Transformer Encoder中MLP Block第一个全连接的节点个数（是Hidden Size的四倍），Heads代表Transformer中Multi-Head Attention的heads数。\",\"在深度学习领域，当提到模型参数量时，“M” 通常是 “million” 的缩写，代表 “百万”。所以参数量为 86M 就意味着模型大约有 86×1000000 = 8600000（八百六十万）个可训练参数。\",\"与之类似的还有 “B”，它是 “billion” 的缩写，代表 “十亿”。例如参数量为 1.2B 就表示模型大约有 1.2×1000000000 = 1200000000（十二亿）个可训练参数。\"]},\"116\":{\"h\":\"注意力可视化\",\"t\":[\"ViT这篇论文长达二十多页，里面包含了非常丰富的成果，其中包括注意力可视化。由于作者是首次将Transformer应用到图像领域，里面包含了注意力机制，那么作者就想把注意力得到的结果(也就是Q-K矩阵乘积)换源到图像上，得到结果如下图所示：\",\"可以看到，模型自动学习到了如果注意画面中的分类主体。\"]},\"117\":{\"h\":\"混合模型探索\",\"t\":[\"在论文的最后，作者又探索了一种混合模型(Hybrid)，就是将传统CNN和Transformer进行结合。\",\"下表中对比了ViT、ResNet和混合模型在不同图像分类数据集上的测试结果，可以看到当Epochs增大时，ResNet和混合模型的效果均不如ViT模型。\",\"混合模型的常见结合方式:\",\"CNN 作为特征提取器，Transformer 作为编码器 \",\"先用 CNN 对输入数据进行初步的特征提取，利用 CNN 的局部特征提取能力快速捕捉图像的底层特征。例如，在图像分类任务中，可以使用预训练的 ResNet 等 CNN 模型提取图像的特征图。\",\"然后将 CNN 提取的特征图转换为序列形式，输入到 Transformer 中进行进一步的处理。Transformer 可以利用其自注意力机制捕捉特征之间的长距离依赖关系，对特征进行更深入的建模。\",\"交错堆叠 CNN 和 Transformer 模块 \",\"在模型架构中，将 CNN 层和 Transformer 层交错堆叠。例如，先经过一层或多层 CNN 进行局部特征提取，然后再经过一层 Transformer 捕捉全局信息，如此反复。这样可以在模型的不同阶段交替利用 CNN 和 Transformer 的优势。\",\"在 Transformer 中引入卷积操作 \",\"在 Transformer 的架构中融入卷积操作，例如在多头自注意力机制或前馈网络中引入卷积层。这样可以为 Transformer 赋予局部特征提取的能力，同时保留其捕捉长距离依赖的优势。\"]},\"118\":{\"h\":\"加载预训练模型\",\"t\":[\"上面已经给出了数据集加载以及ViT模型核心代码实现了，下面我们将进入训练流程；首先说明，本次训练是基于预训练好的ViT-B/16这个模型进行微调，整体结构图如下：\",\"具体为vit_base_patch16_224_in21k这个模型:\",\"vit：代表 Vision Transformer。\",\"base：表示模型的规模。\",\"patch16：意味着在处理图像时，会将输入图像分割成大小为 16×16 像素的图像块（patches）。\",\"224：指的是输入图像的尺寸为 224×224 像素。在预训练和使用该模型时，需要将输入图像调整为这个固定的尺寸。\",\"in21k：该模型是在 ImageNet - 21k 数据集上进行预训练的。ImageNet - 21k 是一个大规模的图像数据集，包含大约 21000 个类别和 1.4 亿张图像。在如此大规模的数据集上进行预训练，模型能够学习到丰富的图像特征和模式，具有较强的泛化能力。\",\"def vit_base_patch16_224_in21k(num_classes: int = 21843, has_logits: bool = True): \\\"\\\"\\\" ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929). ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer. weights ported from official Google JAX impl: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch16_224_in21k-e5005f0a.pth \\\"\\\"\\\" model = VisionTransformer(img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, representation_size=768 if has_logits else None, num_classes=num_classes) return model # 加载预训练好的vit_base_patch16_224_in21k模型权重文件 model = vit_base_patch16_224_in21k(num_classes=5, has_logits=False).to(device) weights_dict = torch.load(args.weights, map_location=device) model.load_state_dict(weights_dict, strict=False)\",\"加载该模型后，训练了10个epoch，验证集上准确率达到了98.5%。整体模型还是比较大的，预训练权重大小为393MB，但是训练速度还是挺快的，因为在代码中有个冻结权重的操作，主干部分全部冻结，仅训练分类头。\",\"for name, para in model.named_parameters(): # 除head, pre_logits外，其他权重全部冻结 if \\\"head\\\" not in name and \\\"pre_logits\\\" not in name: para.requires_grad_(False) else: print(\\\"training {}\\\".format(name))\",\"训练与评估流程的代码为模版代码，考虑篇幅原因，这里不再贴出，大家可以自行拉取项目完整代码进行学习:\",\"https://pan.baidu.com/s/1rkdjdlR37O7gSr9j1mhjBg?pwd=vket\"]},\"119\":{\"h\":\"总结\",\"t\":[\"Vision Transformer证明了使用Transformer结构可以有效处理图像数据，并且取得了与卷积神经网络（CNN）相媲美的效果。\",\"统一多模态的可能性：使用Transformer架构为未来的多模态统一提供了可能性。\",\"图像到文本的桥梁：架起了图像空间到文本空间的桥梁。\",\"ViT核心：如何将二维图像转换为一维时间序列？通过将图像切成小片（Patches），并按行优先排序来实现。\"]},\"120\":{\"h\":\"开源课程笔记\"},\"121\":{\"h\":\"开源项目\"},\"122\":{\"h\":\"Attention运算过程中维度变换的理解\",\"t\":[\"Attention运算过程中维度变换的理解\",\"在注意力机制（特别是 Transformer 中的 自注意力机制）中，Q（Query）、K（Key）、V（Value） 的维度对最终注意力输出的结果维度有直接影响。我们来一步步分析这个过程：\"]},\"123\":{\"h\":\"一、注意力机制的基本流程\",\"t\":[\"在标准的 缩放点积注意力（Scaled Dot-Product Attention） 中，计算公式如下：\",\"其中：\",\"：query的数量（如句子长度）\",\"：key/value的数量（也通常是句子长度）\",\"：每个 query 和 key 的维度\",\"：每个 value 的维度\"]},\"124\":{\"h\":\"二、Q、K、V 的初始维度对结果的影响\"},\"125\":{\"h\":\"1.\",\"t\":[\"这是注意力权重矩阵的来源。\",\"所以\",\"👉 这个矩阵表示的是每个 query 对应所有 key 的相似度（即注意力得分），共个值。\"]},\"126\":{\"h\":\"2.\",\"t\":[\"对每一行做 softmax，得到归一化的注意力权重：\",\"输入：\",\"输出：仍是\"]},\"127\":{\"h\":\"3.\",\"t\":[\"注意力权重：\",\"Value 矩阵：\",\"结果：\",\"👉 最终输出的维度是，也就是和输入的 query 数量一致，但每个输出向量的维度由 value 的维度决定。\"]},\"128\":{\"h\":\"三、总结：输入维度 → 输出维度\",\"t\":[\"输入\",\"维度\",\"含义\",\"Query (Q)\",\"查询向量，n 是序列长度\",\"Key (K)\",\"键向量，用于匹配查询\",\"Value (V)\",\"值向量，实际携带信息\",\"输出\",\"维度\",\"含义\",\"Attention Output\",\"每个 query 聚合了所有 value 的加权信息\"]},\"129\":{\"h\":\"四、如何理解这个过程？\",\"t\":[\"我们可以从以下角度理解：\"]},\"130\":{\"h\":\"✅ 1.\",\"t\":[\"每个 Query 都是在寻找最相关的 Key。\",\"根据相关性（注意力权重），从对应的 Value 中提取信息。\",\"最终每个 Query 得到一个融合了上下文信息的向量。\"]},\"131\":{\"h\":\"✅ 2.\",\"t\":[\"控制了相似度计算的维度，影响模型容量和梯度稳定性。\",\"决定了输出的信息维度，可以独立于设计。\",\"这种分离的设计让模型更灵活，比如多头注意力中可以分别控制每个 head 的表达能力。\"]},\"132\":{\"h\":\"✅ 3.\",\"t\":[\"类似数据库查询： \",\"Query 是你输入的问题；\",\"Key 是数据库中的索引；\",\"Value 是数据库中的内容；\",\"Attention 就是根据问题找到相关内容并返回。\"]},\"133\":{\"h\":\"五、例子说明（以 Transformer 为例）\",\"t\":[\"假设我们在 Transformer 中：\",\"输入是一个 batch of sequences，shape 为\",\"我们通过线性变换得到： \",\"那么最终输出为：\",\"如果使用多头注意力（Multi-head Attention），我们会拼接多个这样的头，最后再经过一个线性层映射回原始维度。\"]},\"134\":{\"h\":\"六、常见疑问解答\"},\"135\":{\"h\":\"❓Q: 为什么 和 可以不同？\",\"t\":[\"因为它们的作用不同：\",\"是用于计算相似度的维度；\",\"是用于信息表达的维度；\",\"两者解耦可以让模型更灵活地分配资源。\"]},\"136\":{\"h\":\"❓Q: 为什么要除以 ？\",\"t\":[\"防止内积过大导致 softmax 梯度消失。 当较大时，QK^T 的数值会很大，除以可以缓解这个问题。\"]},\"137\":{\"h\":\"七、可视化示意\",\"t\":[\"Q: [n x dk] K: [m x dk] V: [m x dv] ↓ ↓ ↓ Q @ K.T → [n x m] ↓ ↓ ↓ softmax → [n x m] V → [m x dv] ↓__________________________↓ ↓ Output → [n x dv]\"]},\"138\":{\"h\":\"杂谈\"},\"139\":{\"h\":\"conda虚拟环境管理\",\"t\":[\"conda虚拟环境管理\"]},\"140\":{\"h\":\"一、创建新环境\",\"t\":[\"基本语法：\",\"conda create --name <环境名> [包名]\",\"可使用 -name（或 n）来命名环境。\",\"示例1：创建一个空环境（只包含 Python）\",\"conda create --name myenv\",\"示例2：创建环境时指定 Python 版本\",\"conda create --name myenv python=3.9\",\"示例3：创建环境并安装一些常用包\",\"conda create --name myenv python=3.8 numpy pandas\"]},\"141\":{\"h\":\"二、激活（切换）环境\",\"t\":[\"激活环境的命令：\",\"conda activate <环境名>\",\"示例：\",\"conda activate lmaffordance3d\",\"激活后，你的终端提示符通常会显示当前环境的名字，例如：\",\"(myenv) user@machine:~$\"]},\"142\":{\"h\":\"三、退出当前环境\",\"t\":[\"要退出当前激活的环境，返回 base 环境：\",\"conda deactivate\"]},\"143\":{\"h\":\"四、查看所有已创建的环境\",\"t\":[\"你可以使用以下命令查看你所有的 conda 环境：\",\"conda env list # 或者 conda info --envs\",\"输出示例：\",\"# conda environments: # base * /home/user/anaconda3 myenv /home/user/anaconda3/envs/myenv testenv /home/user/anaconda3/envs/testenv\",\"注：带星号 * 的表示当前激活的环境。\"]},\"144\":{\"h\":\"五、删除已创建的环境\",\"t\":[\"如果你想删除某个环境，可以使用：\",\"conda env remove -n myenv\",\"如需进一步帮助，可使用：\",\"conda create --help conda activate --help\"]},\"145\":{\"h\":\"六、查看当前激活的环境\",\"t\":[\"查看当前conda激活的环境:\",\"conda info\"]},\"146\":{\"h\":\"七、查看当前环境已安装的包\",\"t\":[\"查看当前环境已安装的包：\",\"conda list\"]},\"147\":{\"h\":\"八、在当前环境下安装包\",\"t\":[\"根据 requirements.txt 安装所需要的依赖包:\",\"conda activate 你的环境名 # 先激活你的conda环境 pip install -r requirements.txt\",\"重要说明：\",\"在激活的 Conda 环境中使用 pip install，包会安装到该环境的 site-packages 中，不会影响其他环境或系统 Python\",\"如果未激活任何环境时使用 pip install，包可能会安装到基础环境或系统 Python 中\",\"建议总是先激活 Conda 环境再使用 pip，以避免安装到错误的位置\",\"可以使用 which pip 或 where pip (Windows) 确认你使用的是 Conda 环境中的 pip\",\"pip install 安装失败的包，尝试使用conda install命令安装即可，再不行尝试源码编译安装(例如某些包在arm64系统上没有预先编译好的版本)。\",\"特性\",\"pip\",\"conda\",\"默认仓库\",\"PyPI（Python Package Index）\",\"Anaconda 官方仓库 / conda-forge\",\"包类型\",\"仅 Python 包（纯 Python 或源码）\",\"预编译的二进制包（含非 Python 依赖）\",\"非 Python 依赖\",\"不管理（如 FFmpeg、HDF5）\",\"自动安装（如 CUDA、MKL）\"]},\"148\":{\"h\":\"九、常见错误\",\"t\":[\"CondaError: Run 'conda init' before 'conda activate’\",\"conda init 如果是 bash： source ~/.bashrc 如果是 zsh： bash conda activate lavis\"]},\"149\":{\"h\":\"常用评估指标\",\"t\":[\"常用评估指标\"]},\"150\":{\"h\":\"二元分类场景\"},\"151\":{\"h\":\"混淆矩阵 (confusion_matrix)\",\"t\":[\"二元分类器的每个输出有四种可能的结果，如果我们将标准答案作为列，将模型的预测作为行，则会得到以下表格（称为混淆矩阵）：\",\"实际正例\",\"实际负例\",\"预测为正例\",\"真正例 (TP)：垃圾邮件被正确分类为垃圾邮件。\",\"假正例 (FP)：非垃圾邮件被误分类为垃圾邮件。\",\"预测为负例\",\"假负例 (FN)：垃圾邮件被误分类为非垃圾邮件。\",\"真负例 (TN)：非垃圾邮件被正确分类为非垃圾邮件。\",\"请注意，每行的总和表示所有预测正例 (TP + FP) 和所有预测负例 (FN + TN)，无论其有效性如何。与此同时，每个列中的总和会显示所有真实正例 (TP + FN) 和所有真实负例 (FP + TN)，而不会考虑模型分类。\",\"如果实际正例的总数与实际负例的总数不接近，则表示数据集不平衡。不平衡数据集的一个示例可能是一组数以千计的云彩照片，其中您感兴趣的罕见云彩类型（例如卷云）只出现了几次。\"]},\"152\":{\"h\":\"准确率 (accuracy)\",\"t\":[\"准确性是指所有分类（无论是正类还是负类）正确分类的比例。其数学定义为：\",\"在垃圾邮件分类示例中，准确率衡量的是所有电子邮件正确分类所占的比例。\",\"完美的模型没有假正例和假负例，因此准确率为 1.0，即 100%。\",\"由于准确率包含混淆矩阵中的所有四种结果（TP、FP、TN、FN），因此在类别数量相近且平衡的数据集的情况下，准确率可以作为衡量模型质量的粗略指标。\",\"不过，如果数据集不平衡，或者一种错误（假负例或假正例）的代价高于另一种错误（大多数实际应用中都是如此），则最好改为针对其他指标进行优化。\",\"对于严重不均衡的数据集（其中一个类别出现的频率非常低，例如 1%），如果模型 100% 都预测为负类别，则其准确性得分为 99%，尽管该模型毫无用处。\"]},\"153\":{\"h\":\"召回率 (recall) / 真正例率\",\"t\":[\"真正例率 (TPR)，即所有实际正例被正确分类为正例的比例，也称为召回率。\",\"在数学上，召回率的定义为：\",\"假负例是指被误分类为负例的实际正例，因此会出现在分母中。在垃圾邮件分类示例中，召回率衡量的是被正确分类为垃圾邮件的垃圾邮件电子邮件的比例。\",\"假设一个完美的模型不会出现假负例，因此其召回率 (TPR) 为 1.0，也就是说，检测率为 100%。\",\"在实际正例数量非常少的不平衡数据集中，召回率比准确率更有意义，因为它衡量的是模型正确识别所有正例实例的能力。对于疾病预测等应用，正确识别阳性病例至关重要。假负例通常比假正例的后果更严重。\"]},\"154\":{\"h\":\"误报概率 / 假正例率\",\"t\":[\"假正例率 (FPR) 是指被错误地归类为正例的所有实际负例所占的比例，也称为误报概率。其数学定义为：\",\"假正例是被错误分类的实际负例，因此会出现在分母中。在垃圾邮件分类示例中，FPR 用于衡量被错误分类为垃圾邮件的合法电子邮件的比例，或模型的误报率。\",\"完美的模型不会产生假正例，因此其假正例率为 0.0，也就是说，假正例率为 0%。\",\"在实际负例数量非常少（例如总共 1-2 个示例）的不平衡数据集中，FPR 作为一个指标就没有那么有意义和实用。\"]},\"155\":{\"h\":\"精确率\",\"t\":[\"精确率是指模型所有正类别分类中实际为正类别的分类所占的比例。在数学上，其定义为：\",\"在垃圾邮件分类示例中，精确率衡量的是被归类为垃圾邮件且实际上是垃圾邮件的电子邮件所占的比例。\",\"假设有一个完美的模型，则其假正例数为零，因此精确率为 1.0。\",\"在实际正例数量非常少（例如总共 1-2 个示例）的不平衡数据集中，精确率作为指标的意义和实用性较低。\",\"随着假正例的减少，精确率会提高；随着假负例的减少，召回率会提高。提高分类阈值往往会减少假正例的数量并增加假负例的数量，而降低阈值则会产生相反的效果。因此，精确率和召回率通常呈现反向关系，提高其中一个会降低另一个。\",\"分类阈值: 模型输出的概率值大于某个值时，模型才会将该样本分类为正类。\"]},\"156\":{\"h\":\"指标的选择和权衡\",\"t\":[\"在评估模型和选择阈值时，您选择优先考虑的指标取决于特定问题的成本、收益和风险。在垃圾邮件分类示例中，通常最好优先考虑召回率（抓取所有垃圾邮件）或准确率（尝试确保被标记为垃圾邮件的电子邮件实际上是垃圾邮件），或者在达到某个最低准确性水平的情况下，兼顾这两者。\",\"指标\",\"指南\",\"准确率\",\"作为平衡数据集的模型训练进度/收敛情况的粗略指标。对于模型效果，请仅与其他指标搭配使用。避免使用不平衡的数据集。考虑使用其他指标。\",\"召回率（真正例率）\",\"当假负例的代价高于假正例时使用，有病的人不能诊断为健康。\",\"假正例率\",\"当假正例的代价高于假负例时使用，误报很可怕。\",\"精确率\",\"当正例预测的准确性非常重要时，请使用此方法。\"]},\"157\":{\"h\":\"F1 得分\",\"t\":[\"F1 得分是精确率和召回率的调和平均数（一种平均值）。\",\"在数学上，它可按下式计算：\",\"此指标可平衡精确率和召回率的重要性，对于类别不平衡的数据集，优先于准确率。当精确率和召回率均为 1.0 的满分时，F1 得分也会为 1.0 的满分。更广泛地说，当精确率和召回率的值接近时，F1 得分也会接近它们的值。当精确率和召回率相差很大时，F1 将与较差的指标相似。\"]},\"158\":{\"h\":\"ROC 曲线和 AUC\",\"t\":[\"上一部分介绍了一系列模型指标，所有这些指标都是基于单个分类阈值值计算得出的。但是，如果您想评估模型在所有可能阈值下的质量，则需要使用不同的工具。\"]},\"159\":{\"h\":\"ROC (Receiver Operating Characteristic)\",\"t\":[\"ROC 曲线直观地显示了所有阈值下的模型性能。名称的长版本“接收器操作特性”源自二战雷达检测。\",\"绘制 ROC 曲线的方法是：计算每个可能的阈值（在实践中，是按选定的间隔）的真正例率 (TPR) 和假正例率 (FPR)，然后将 TPR 与 FPR 绘制到图表中。\",\"完美的模型在某个阈值下的 TPR 为 1.0，FPR 为 0.0，如果忽略所有其他阈值，则可以用 (0, 1) 点表示，也可以用以下方式表示：\",\"图 1. 假设的理想模型的 ROC 和 AUC\"]},\"160\":{\"h\":\"AUC （曲线下面积）\",\"t\":[\"ROC 曲线下面积 (AUC) 表示，如果给定随机选择的正例和负例，模型将正例排在负例之上的概率。\",\"上面的完美模型包含边长为 1 的正方形，其曲线下面积 (AUC) 为 1.0。这意味着，模型将随机选择的正例正确排在随机选择的负例之上的概率为 100%。\",\"更具体地说，AUC 为 1.0 的垃圾邮件分类器始终会为随机垃圾邮件分配比随机合规电子邮件更高的垃圾邮件概率。每封电子邮件的实际分类取决于您选择的阈值。\",\"对于二元分类器，如果模型的效果与随机猜测或抛硬币的效果完全一样，则其 ROC 曲线为从 (0,0) 到 (1,1) 的对角线。AUC 为 0.5，表示正确对随机正例和负例进行排名的概率为 50%。\",\"在垃圾邮件分类器示例中，AUC 为 0.5 的垃圾邮件分类器仅在 50% 的情况下会将随机垃圾邮件的垃圾邮件概率设为高于随机合法邮件的垃圾邮件概率。\",\"图 2. 完全随机猜测的 ROC 和 AUC\"]},\"161\":{\"h\":\"精确率与召回率曲线\",\"t\":[\"如果数据集在类别之间大致平衡，AUC 和 ROC 非常适合比较模型。当数据集不均衡时，准确率-召回率曲线 (PRC) 和这些曲线下的面积可以更好地直观比较模型性能。精确率/召回率曲线的创建方法是，在 y 轴上绘制精确率，在 x 轴上绘制所有阈值下的召回率。\",\"图 3. 精确率与召回率曲线\"]},\"162\":{\"h\":\"用于选择模型和阈值的 AUC 和 ROC\",\"t\":[\"AUC 是比较两个不同模型性能的有效衡量指标，前提是数据集大致平衡。曲线下面积较大的模型通常是更好的模型。\",\"图 4. 两个假设模型的 ROC 和 AUC。右侧曲线的 AUC 较高，表示该模型优于左侧曲线对应的模型。\",\"ROC 曲线上最接近 (0,1) 的点表示给定模型效果最佳的阈值范围。我们选择的阈值取决于哪个指标对特定用例而言最重要。请考虑下图中的点 A、B 和 C，每个点都代表一个阈值：\",\"图 5. 三个标记的点，表示阈值。\",\"如果假正例（误报）的代价很高，则可能有必要选择 FPR 较低的阈值（例如 A 点），即使 TPR 会降低也是如此。反之，如果假正例成本较低，而假负例（漏掉的真正例）成本较高，则点 C 的阈值（可最大限度地提高 TPR）可能更为合适。如果费用大致相当，点 B 在 TPR 和 FPR 之间可能提供最佳平衡。\"]},\"163\":{\"h\":\"语义分割中常用的损失函数\",\"t\":[\"语义分割中常用的损失函数\"]},\"164\":{\"h\":\"语义分割\",\"t\":[\"语义分割是计算机视觉领域中的一项任务，旨在将图像中的每个像素分类为不同的语义类别。与对象检测任务不同，语义分割不仅需要识别图像中的物体，还需要对每个像素进行分类，从而实现对图像的细粒度理解和分析。\",\"语义分割可以被看作是像素级别的图像分割，其目标是为图像中的每个像素分配一个特定的语义类别标签。每个像素都被视为图像的基本单位，因此语义分割可以提供更详细和准确的图像分析结果。\",\"语义分割 vs 分类 :\",\"在语义分割任务中，由于需要对每个像素进行分类，因此需要使用像素级别的损失函数。\",\"语义分割任务中，图像中各个类别的像素数量通常不均衡，例如背景像素可能占据了大部分。\",\"语义分割任务需要对图像中的每个像素进行分类，同时保持空间连续性。\"]},\"165\":{\"h\":\"损失函数\"},\"166\":{\"h\":\"Dice Loss\",\"t\":[\"Dice Loss 是一种常用于语义分割任务的损失函数，尤其在目标区域较小、类别不平衡（class imbalance）的情况下表现优异。它来源于 Dice 系数（Dice Coefficient） ，又称为 Sørensen-Dice 系数 ，是衡量两个样本集合之间重叠程度的一种指标。\",\"Dice 系数衡量的是预测掩码与真实标签之间的相似性，公式如下：\",\"其中：\",\" ：模型预测出的功能区域（如经过 sigmoid 后的概率值）；\",\" ：Ground Truth 掩码（二值化或软标签）；\",\" ：预测为正类且实际也为正类的部分（交集）；\",\" ：预测和真实中所有正类区域之和；\",\"⚠️ 注意：Dice 系数范围是 [0, 1]，越大越好。\",\"Dice Loss 为了将其作为损失函数使用，我们通常取其补集：\",\"有时也会加入一个平滑项 ϵ 防止除以零：\",\"Dice Loss 的优势:\",\"优势\",\"描述\",\"对类别不平衡不敏感,更关注“有没有覆盖正确区域”，而不是“有多少点被正确分类”\",\"不像 BCE Loss 那样对负样本过多敏感\",\"直接优化 IoU 的替代指标\",\"Dice 和 IoU 表现类似，但更易梯度下降\",\"支持 soft mask 输入\",\"可处理连续概率值，不需要先 threshold\",\"更关注整体区域匹配\",\"而不是逐点分类\",\"代码实现:\",\"class DiceLoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数，支持加权和平均损失。 参数: weight (Tensor): 各类别的权重（可选） size_average (bool): 是否对 batch 中的样本取平均 loss \\\"\\\"\\\" super(DiceLoss, self).__init__() # 该参数未在当前代码中使用，但保留接口以备后续扩展 self.weight = weight # 控制是否对 batch 内 loss 取均值或求和 self.size_average = size_average def forward(self, inputs, targets, smooth=1): \\\"\\\"\\\" 前向传播函数，计算 Dice Loss。 参数: inputs (Tensor): 模型输出的预测值（logits 或 raw output），形状为 [B, N] targets (Tensor): 真实标签（ground truth mask），形状为 [B, N] smooth (float): 平滑项，防止除零错误，默认为 1 返回: dice_loss (Tensor): 计算得到的 Dice Loss \\\"\\\"\\\" # 如果你的模型最后没有 sigmoid 层，则需要在这里激活，否则应注释掉这行 inputs = F.sigmoid(inputs) # 将 logits 映射到 [0,1] 区间 # 将输入展平成一维张量，便于后续计算 # inputs: [B*N] # targets: [B*N] inputs = inputs.view(-1) targets = targets.view(-1) # 计算交集：预测与 GT 的重合部分 intersection = (inputs * targets).sum() # 计算 Dice Coefficient，加入 smooth 防止除以零 dice_score = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth) # 返回 Dice Loss，用 1 - Dice Coefficient # 值越小表示匹配越好 return 1 - dice_score\"]},\"167\":{\"h\":\"BCE-Dice Loss\",\"t\":[\"BCE-Dice Loss是将Dice Loss和标准的二元交叉熵（Binary Cross-Entropy, BCE）损失结合在一起的一种损失函数，通常用于分割模型中。它结合了两种 loss 的优点：\",\"BCE Loss ：关注每个点的分类误差；\",\"Dice Loss ：关注整体区域匹配度；\",\"Binary Cross Entropy Loss（BCE Loss）\",\"公式（逐点）：\",\"其中：\",\"：真实标签（binary 或 soft mask）；\",\"：模型输出的概率值；\",\"特点：\",\"对每个点单独计算分类误差；\",\"强调预测与 GT 的一致性；\",\"在类别平衡时效果好，但在前景远少于背景时容易偏向负样本；\",\"Dice Loss\",\"公式（简化版）：\",\"其中：\",\"：预测概率；\",\"：真实标签；\",\"：平滑项，防止除以零；\",\"特点：\",\"不依赖绝对数量，而是关注预测和 GT 的交并比；\",\"更适合前景极少的小区域识别；\",\"能缓解类别不平衡问题；\",\"为什么要把它们结合起来？\",\"模型\",\"缺陷\",\"补充方式\",\"BCE Loss\",\"对前景响应弱，易受类别不平衡影响\",\"加入 Dice Loss 增强区域匹配\",\"Dice Loss\",\"对单个点的分类精度不够敏感\",\"加入 BCE Loss 提高逐点判别能力\",\"组合后的优势：\",\"优势\",\"描述\",\"✔️ 抗类别不平衡能力强\",\"Dice Loss 起主导作用\",\"✔️ 对细节更敏感\",\"BCE Loss 提升边缘识别精度\",\"✔️ 支持 soft mask 输入\",\"可处理连续值掩码\",\"✔️ 更稳定地收敛\",\"两者互补，避免训练震荡\",\"代码实现:\",\"class DiceBCELoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数，构建一个组合损失函数 Dice + BCE。 参数: weight (Tensor): 可选参数，用于类别加权； size_average (bool): 是否对 batch 内样本取平均 loss（已弃用）； \\\"\\\"\\\" super(DiceBCELoss, self).__init__() # 这里暂时未使用 weight 和 size_average，保留接口以备扩展 def forward(self, inputs, targets, smooth=1): \\\"\\\"\\\" 前向传播函数，计算预测输出与真实标签之间的 Dice Loss 与 BCE Loss 的加权和。 参数: inputs (Tensor): 模型输出的 logits 或 raw 分数，形状为 [B, N] targets (Tensor): 真实掩码（ground truth mask），形状为 [B, N] smooth (float): 平滑项，防止除零错误，默认为 1 返回: Dice_BCE (Tensor): Dice + BCE 组合损失值 \\\"\\\"\\\" # 如果模型最后没有 sigmoid 层，这里需要激活 # 如果已经包含 sigmoid，则应注释掉这一行 inputs = F.sigmoid(inputs) # 将输入映射到概率空间 [0, 1] # 将输入和目标展平成一维张量，便于后续计算 # inputs: [B*N] # targets: [B*N] inputs = inputs.view(-1) targets = targets.view(-1) # 计算交集：预测值和真实值都为 1 的区域 intersection = (inputs * targets).sum() # 计算 Dice Loss： # Dice Coefficient = (2 * intersection) / (inputs_sum + targets_sum) # Dice Loss = 1 - Dice Coefficient inputs_sum = inputs.sum() targets_sum = targets.sum() dice_score = (2. * intersection + smooth) / (inputs_sum + targets_sum + smooth) dice_loss = 1 - dice_score # 计算 Binary Cross Entropy Loss（BCE） # 注意：F.binary_cross_entropy 默认要求 inputs 已经经过 sigmoid BCE = F.binary_cross_entropy(inputs, targets, reduction='mean') # 组合损失：BCE + Dice Loss Dice_BCE = BCE + dice_loss return Dice_BCE\"]},\"168\":{\"h\":\"Jaccard/Intersection over Union (IoU) Loss\",\"t\":[\"Jaccard Loss，也称为Intersection over Union (IoU) Loss，是一种常用的损失函数，用于语义分割任务中评估模型的分割结果与真实分割标签之间的相似性。它基于Jaccard指数（Jaccard Index），也称为 交并比（Intersection over Union, IoU）指标，用于度量两个集合之间的重叠程度。\",\"Jaccard Index（IoU）\",\"其中：\",\"：模型输出的概率值或二值化结果；\",\"：ground truth 掩码；\",\"分子是预测和 GT 的交集；\",\"分母是两者的并集；\",\"⚠️ IoU 值 ∈ [0, 1]，越大越好。\",\"Jaccard Loss（IoU Loss）\",\"为了将 IoU 转换为可优化的损失函数，我们取其补集：\",\"这样，损失越小表示预测越接近真实标签。\",\"为了避免除以零，通常加入平滑项 ：\",\"Jaccard Loss 有以下几个优点：\",\"特性\",\"描述\",\"✔️ 对类别不平衡不敏感\",\"不像 BCE Loss 那样偏向背景点\",\"✔️ 关注整体区域匹配\",\"强调预测与 GT 的空间一致性\",\"✔️ 更适合评估边界模糊区域\",\"如功能区域边缘不确定性较高\",\"与其他 Loss 的对比\",\"损失函数\",\"是否支持 soft mask\",\"是否对类别不平衡敏感\",\"是否直接优化 IoU\",\"输出范围\",\"BCE Loss\",\"❌ 否（需二值化）\",\"✅ 是\",\"❌ 否\",\"[0, ∞)\",\"Focal Loss\",\"✅ 是（加权）\",\"✅ 是（缓解）\",\"❌ 否\",\"[0, ∞)\",\"Dice Loss\",\"✅ 是\",\"✅ 是\",\"近似于 IoU\",\"[0, 1]\",\"Jaccard (IoU) Loss\",\"✅ 是\",\"✅ 是\",\"✅ 是\",\"[0, 1]\",\"虽然 Dice Loss 在实际训练中更稳定，但 Jaccard Loss 更贴近最终评估指标（IoU），适合在推理阶段作为验证标准。\",\"代码实现:\",\"class IoULoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数，构建一个基于 IoU（交并比）的损失函数。 参数: weight (Tensor): 可选参数，用于类别加权（未使用） size_average (bool): 是否对 batch 内样本取平均 loss（已弃用） \\\"\\\"\\\" super(IoULoss, self).__init__() # weight 和 size_average 在此实现中未使用，保留接口以备后续扩展 def forward(self, inputs, targets, smooth=1): \\\"\\\"\\\" 前向传播函数，计算预测输出与真实标签之间的 IoU Loss。 参数: inputs (Tensor): 模型输出的原始 logit 或经过 sigmoid 的概率值； 形状为 [B, N] targets (Tensor): ground truth 掩码，形状为 [B, N] smooth (float): 平滑项，防止除零错误，默认为 1 返回: iou_loss (Tensor): 计算得到的 IoU Loss \\\"\\\"\\\" # 如果模型最后没有 sigmoid 层，则在这里激活 # 如果已经包含 sigmoid，则应注释掉这一行 inputs = torch.sigmoid(inputs) # 将输入映射到 [0,1] 区间 # 将输入和目标展平成一维张量便于计算 # inputs: [B*N] # targets: [B*N] inputs = inputs.view(-1) targets = targets.view(-1) # 计算交集（Intersection），等价于 TP（True Positive） intersection = (inputs * targets).sum() # 计算并集：Union = input + target - intersection total = (inputs + targets).sum() union = total - intersection # 计算 IoU Score，加入平滑项防止除以零 iou_score = (intersection + smooth) / (union + smooth) # IoU Loss = 1 - IoU score，这样越接近 1，loss 越小 iou_loss = 1. - iou_score return iou_loss\"]},\"169\":{\"h\":\"Focal Loss\",\"t\":[\"Focal Loss 是一种针对类别不平衡（Class Imbalance）问题的损失函数改进方案，由何恺明团队在2017年论文《Focal Loss for Dense Object Detection》中提出，主要用于解决目标检测任务中前景-背景类别极端不平衡的问题（如1:1000）。其核心思想是通过调整难易样本的权重，使模型更关注难分类的样本。\",\"Focal Loss 基于交叉熵损失进行扩展，将样本的权重进行动态调整。与交叉熵损失函数相比，Focal Loss引入了一个衰减因子，其中 pt 是预测的概率值。这个衰减因子能够使得易分类的样本（ pt较高 ）的权重降低，从而减少对分类正确样本的贡献。\",\"核心思想:\",\"(1) 类别不平衡的问题\",\"在分类任务中（尤其是目标检测），负样本（背景）往往远多于正样本（目标），导致：\",\"模型被大量简单负样本主导，难以学习有效特征。\",\"简单样本的梯度贡献淹没难样本的梯度。\",\"(2) Focal Loss 的改进\",\"降低易分类样本的权重：对模型已经分类正确的样本（高置信度）减少损失贡献。\",\"聚焦难分类样本：对分类错误的样本（低置信度）保持高损失权重。\",\"Focal Loss 基于标准交叉熵损失（Cross-Entropy Loss）改进而来。\",\"(1) 标准交叉熵损失（CE Loss）\",\"其中：\",\"p 是模型预测的概率（经过sigmoid/softmax）。\",\"y 是真实标签（0或1）。\",\"(2) Focal Loss 定义\",\"：类别平衡权重（通常），用于平衡正负样本数量差异。\",\"：调节因子（通常），控制难易样本的权重衰减程度。\",\"γ 参数用于抑制容易分类的样本，而 α 参数用于平衡正负类别的权重。两者解决的是不同维度的问题：\",\"α：防止前景点（功能区域）被背景淹没，解决数据集中“类别数量不平衡”的问题（数据集级别）；\",\"γ：防止模型只关注简单样本，忽略难分类样本，解决模型训练时“简单样本主导梯度”的问题（样本级别）；\",\"综上，先通过 α 平衡类别数量，再通过 γ 抑制简单样本，两者协同提升模型性能。\",\"关键参数的作用:\",\"参数\",\"作用\",\"典型值\",\"控制难易样本权重：• ：退化为CE Loss• ：显著抑制简单样本\",\"0.5 ~ 5\",\"平衡正负样本数量：• ：正样本较少时增加权重\",\"0.25 ~ 0.75\",\"难样本vs易样:\",\"易分类样本（如 p=0.9 ）： 接近0，损失被大幅降低。\",\"难分类样本（如 p=0.1 ）： 接近1，损失几乎不受影响。\",\"假设两个正样本：\",\"易样本：（模型已自信分类）\",\"标准 CE Loss：\",\"Focal Loss（）：损失权重降低 100 倍！\",\"难样本：（模型分类错误）\",\"标准 CE Loss：\",\"Focal Loss（）：损失权重仅降低 20%。\",\"应用场景：\",\"目标检测（如RetinaNet）： 解决前景（目标）与背景的极端不平衡问题。\",\"医学图像分割： 病灶区域像素远少于正常组织。\",\"任何类别不平衡的分类任务： 如欺诈检测、罕见疾病诊断等。\",\"优缺点:\",\"优点\",\"缺点\",\"显著提升难样本的分类性能\",\"需调参（）\",\"抑制简单样本的梯度主导\",\"对噪声标签敏感\",\"兼容大多数分类模型\",\"计算量略高于CE Loss\",\"Focal Loss 通过 动态调整样本权重，使模型聚焦难分类样本。\",\"参数选择：\",\"：一般从2开始调优（值越大，简单样本抑制越强）。\",\"：根据正负样本比例调整（如正样本少则增大 ）。\",\"适用场景：类别不平衡越严重，Focal Loss 效果越显著。\",\"代码实现:\",\"# 设置全局参数（可调） ALPHA = 0.8 # 控制正样本（目标点）与负样本（非目标点）之间的损失权重； # 若前景点稀疏（如 grasping area），建议设为较高值（如 0.25~0.75）； GAMMA = 2 # 聚焦参数，用于抑制易分类样本，放大难分类样本的影响； class FocalLoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数，构建一个基于 BCE 的改进版 Focal Loss。 参数: weight (Tensor): 可选参数，用于类别加权（未使用）； size_average (bool): 是否对 batch 内样本取平均 loss（已弃用）； \\\"\\\"\\\" super(FocalLoss, self).__init__() # 当前实现未使用 weight 和 size_average，保留接口以备扩展 def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1): \\\"\\\"\\\" 前向传播函数，计算预测输出与真实标签之间的 Focal Loss。 参数: inputs (Tensor): 模型输出的原始 logit 或经过 sigmoid 的概率值； 形状为 [B, N]（batch_size × 点数） targets (Tensor): ground truth 掩码，形状为 [B, N] alpha (float): 平衡因子，控制正类（功能区域）和负类（非功能区域）之间的损失权重； 前景点少 → alpha 高（如 0.75），防止被背景淹没； gamma (float): 聚焦参数，抑制 easy examples，放大 hard examples； smooth (float): 平滑项，防止除零错误，默认为 1 返回: focal_loss (Tensor): 计算得到的 Focal Loss 值 \\\"\\\"\\\" # 如果模型最后没有 sigmoid 层，则在这里激活 inputs = torch.sigmoid(inputs) # 将输入展平便于后续计算 # inputs: [B*N], 表示每个点属于功能区域的概率； # targets: [B*N], 表示每个点是否属于目标功能区域（soft/hard label）； inputs = inputs.view(-1) targets = targets.view(-1) # Step 1: 计算 Binary Cross Entropy Loss（BCE） # 这里使用 'mean' reduction，表示对 batch 内取平均 ce_loss = F.binary_cross_entropy(inputs, targets, reduction='mean') # Step 2: 计算 pt = exp(-ce_loss)，即 e^{-ce_loss} pt = torch.exp(-ce_loss) # shape: scalar # Step 3: 按类别分配 alpha alpha = torch.where(targets == 1, alpha, 1 - alpha) # Step 4: 构建 Focal Weight： # focal_weight = α * (1 - pt)^γ # 目的是：让难分类样本获得更大的 loss 权重，从而引导模型学习更多语义信息 focal_weight = alpha * (1 - pt) ** gamma # Step 5: 最终 Focal Loss = focal_weight × ce_loss focal_loss = focal_weight * ce_loss return focal_loss\",\"关于计算 p_t（模型对真实类别的预测概率）代码解析:\",\"pt = torch.exp(-ce_loss) # p_t = softmax(output)[target_class]\",\"ce_loss = F.cross_entropy(...) → 这是交叉熵损失；\",\"-ce_loss → 负号；\",\"torch.exp(-ce_loss) → 求 exp（自然指数）；\",\"但实际上这行代码的意图是计算 ，即模型对真实类别的预测概率（confidence）, 这里采用的方法是一种“技巧性近似”。对于一个样本，交叉熵损失为：\",\"所以：\",\"pt = torch.exp(-ce_loss)\",\"这个表达式其实是通过 CE loss 反推出来的 ，因为：\"]},\"170\":{\"h\":\"Tversky Loss\",\"t\":[\"Tversky Loss的设计灵感来自Tversky指数（Tversky index），它是一种用于度量集合之间相似性的指标，同时也是 Dice Loss 的一种泛化形式，通过引入两个可调节参数来增强模型对假阳性（False Positives）和假阴性（False Negatives）的敏感度控制。\",\"Tversky Loss 的核心是 Tversky 系数：\",\"然后损失就是：\",\"其中：\",\"TP ：真阳性（True Positive）= 预测为正类，且真实也为正类的样本数\",\"FP ：假阳性（False Positive）= 预测为正类，但真实是负类的样本数\",\"FN ：假阴性（False Negative）= 预测为负类，但真实是正类的样本数\",\"α 和 β 是两个可调节的超参数\",\"α 越大，FP 的影响就越大 → 模型更不喜欢“误报”\",\"β 越大，FN 的影响就越大 → 模型更不喜欢“漏报”\",\"如果你设置 α>β ，说明你更讨厌“误检”\",\"如果你设置 β>α ，说明你更讨厌“漏检”\",\"分母中的 TP+α⋅FP+β⋅FN 构成了一个“加权惩罚项”\",\"例如：\",\"α=0.3, β=0.7 → 更重视召回率（Recall）\",\"α=0.7, β=0.3 → 更重视精确率（Precision）\",\"当 alpha=beta=0.5 时，Tversky指数简化为Dice系数，该系数也等于F1得分。\",\"当 alpha=beta=1 时，公式转化为Tanimoto系数，而当 alpha+beta=1 时，得到一组F-beta得分。\",\"# 设置默认参数：当 alpha = beta = 0.5 时，等价于 Dice Loss ALPHA = 0.5 # 控制假阳性（FP）的权重 BETA = 0.5 # 控制假阴性（FN）的权重 class TverskyLoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数 参数： weight: 可选，类别权重（用于处理类别不平衡） size_average: 如果为 True，则返回所有样本损失的平均值 \\\"\\\"\\\" super(TverskyLoss, self).__init__() # 本类中不直接使用 weight 和 size_average，但保留它们作为接口兼容 self.weight = weight self.size_average = size_average def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA): \\\"\\\"\\\" 前向传播计算损失值 参数： inputs: 模型输出的预测结果（logits），形状如 (N, H, W) 或 (N, C, H, W) targets: 真实标签（ground truth），形状与 inputs 相同 smooth: 平滑系数，防止除以零 alpha: FP 的惩罚权重 beta: FN 的惩罚权重 返回： loss: 计算得到的 Tversky Loss \\\"\\\"\\\" # 如果模型最后一层没有 Sigmoid 激活函数，请取消下面这行注释 # 对输出应用 Sigmoid 函数，将 logits 转换为概率 [0,1] inputs = F.sigmoid(inputs) # 将输入和目标张量展平为一维，便于后续计算 TP、FP、FN inputs = inputs.view(-1) targets = targets.view(-1) # 真阳性（True Positive）：预测为正且实际也为正的像素数量 TP = (inputs * targets).sum() # 假阳性（False Positive）：预测为正但实际为负的像素数量 FP = ((1 - targets) * inputs).sum() # 假阴性（False Negative）：预测为负但实际为正的像素数量 FN = (targets * (1 - inputs)).sum() # 计算 Tversky 系数（相似度指标） # 分母中：TP + α·FP + β·FN Tversky = (TP + smooth) / (TP + alpha * FP + beta * FN + smooth) # 最终损失是 1 - Tversky，这样在训练中最小化损失就等于最大化重叠度 return 1 - Tversky\"]},\"171\":{\"h\":\"Lovasz Hinge Loss\",\"t\":[\"Lovasz Hinge Loss的设计思想是，在计算IoU得分之前，根据预测误差对预测结果进行排序，然后累积计算每个误差对IoU得分的影响。然后，将该梯度向量与初始误差向量相乘，以最大程度地惩罚降低IoU得分的预测结果。\",\"https://github.com/bermanmaxim/LovaszSoftmax\"]},\"172\":{\"h\":\"Combo Loss\",\"t\":[\"Combo Loss 是一种结合了多个损失函数优点的混合损失函数，特别适用于图像分割任务。它将 Dice Loss 和 交叉熵损失（CrossEntropy Loss） 相结合，并引入一个可调节的权重参数，使得模型在训练过程中可以更灵活地平衡这两部分损失。\",\"核心思想：\",\"Combo Loss = α × CrossEntropy + (1 - α) × Dice Loss\",\"或者更广义地：\",\"Combo Loss = α × 分类误差（CE）+ β × 区域重叠误差（Dice）\",\"其中 α + β = 1，α 控制分类误差的重要性，β 控制区域匹配误差的重要性。\",\"数学定义:\",\"假设我们有预测概率图 ，真实标签 ，那么：\",\"交叉熵损失（Binary Cross Entropy）：\",\"Dice Loss：\",\"Combo Loss 定义为：\",\"其中：\",\"：控制两个损失之间的权重比例\",\"若 ：仅使用交叉熵损失\",\"若 ：仅使用 Dice Loss\",\"为什么使用 Combo Loss:\",\"优势\",\"描述\",\"✔️ 兼顾像素级精度和区域重叠度\",\"CE 关注每个像素的分类准确性，Dice 关注整体区域匹配程度\",\"✔️ 对类别不平衡问题鲁棒\",\"在前景像素远少于背景像素时表现良好（如医学图像）\",\"✔️ 更稳定的训练过程\",\"避免单一损失可能带来的训练不稳定性\",\"✔️ 可调性强\",\"通过调整 α 参数，适应不同任务需求\",\"对比其他损失函数：\",\"损失函数\",\"是否关注像素分类？\",\"是否关注区域匹配？\",\"是否可调？\",\"是否适合类别不平衡？\",\"CrossEntropy Loss\",\"✅\",\"❌\",\"❌\",\"❌\",\"Dice Loss\",\"❌\",\"✅\",\"❌\",\"✅\",\"Tversky Loss\",\"❌\",\"✅ ✅\",\"✅\",\"✅ ✅\",\"Combo Loss\",\"✅ ✅\",\"✅\",\"✅\",\"✅ ✅\",\"代码实现:\",\"# 超参数设置说明： ALPHA = 0.5 # 控制交叉熵中正负样本的权重 # 如果 ALPHA < 0.5：对假阳性（FP）惩罚更重（更关注精确率） # 如果 ALPHA > 0.5：对假阴性（FN）惩罚更重（更关注召回率） CE_RATIO = 0.5 # 控制交叉熵损失和 Dice 损失之间的权重分配 # CE_RATIO 越大，交叉熵在总损失中的占比越高 class ComboLoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数 参数： weight: 可选，类别权重（用于处理类别不平衡） size_average: 如果为 True，则返回所有样本损失的平均值 \\\"\\\"\\\" super(ComboLoss, self).__init__() # 这里不直接使用 weight 和 size_average，但保留作为接口兼容 self.weight = weight self.size_average = size_average def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, eps=1e-9): \\\"\\\"\\\" 前向传播计算 Combo Loss 参数： inputs: 模型输出的概率值（经过 Sigmoid），形状如 (N, H, W) targets: 真实标签，形状与 inputs 相同，值为 0 或 1 smooth: 平滑系数，防止除以零 alpha: 控制 FP/FN 的惩罚比例（用于交叉熵部分） eps: 防止 log(0) 出现的小常数 返回： combo_loss: 计算得到的 Combo Loss \\\"\\\"\\\" # 将输入和目标张量展平为一维，便于后续计算 inputs = inputs.view(-1) targets = targets.view(-1) # 计算 Dice Loss 所需的交集 intersection = (inputs * targets).sum() # Dice Score（区域匹配度） dice_score = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth) # 加入数值稳定性处理，防止 log(0) 出现 NaN # torch.clamp(x, min=a, max=b) 是 PyTorch 中的一个函数，用于将张量 x 中的每个元素限制在 [a, b] 区间内： # 这里把所有 inputs 中的值限制在区间 [eps, 1.0 - eps] 内，防止出现 0 或 1 的极端值。 inputs = torch.clamp(inputs, eps, 1.0 - eps) # 加权交叉熵损失（Weighted Cross Entropy） # 根据 ALPHA 参数调整正类和负类的权重 weighted_ce = - (ALPHA * targets * torch.log(inputs)) - ((1 - ALPHA) * (1 - targets) * torch.log(1 - inputs)) # 对损失求均值 weighted_ce = weighted_ce.mean() # Combo Loss 是交叉熵和 Dice Loss 的加权组合 # 注意：这里使用的是负的 Dice Score（因为要最小化损失） combo_loss = (CE_RATIO * weighted_ce) - ((1 - CE_RATIO) * dice_score) return combo_loss\",\"上面代码实现中使用的是加权交叉熵损失:\"]},\"173\":{\"h\":\"如何选择?\",\"t\":[\"任务需求：根据特定的分割任务的需求和特点，选择适合的损失函数。例如，对于类别不平衡的数据集，可以考虑使用Tversky Loss或Combo Loss等能够处理不平衡情况的损失函数。\",\"实验评估：在实验中，使用不同的损失函数进行训练，并评估它们在验证集或测试集上的性能。比较它们在IoU、准确率、召回率等指标上的表现，选择性能最佳的损失函数。\",\"超参数调整：一些损失函数具有额外的超参数，如Tversky Loss中的alpha和beta，可以通过调整这些超参数来进一步优化损失函数的性能。\"]},\"174\":{\"h\":\"通俗易懂解读BPE分词算法实现\",\"t\":[\"通俗易懂解读BPE分词算法实现\",\"BPE（Byte Pair Encoding，字节对编码）是一种基于频率统计的子词分词算法 ，广泛用于现代自然语言处理任务中，特别是在像 BERT、GPT 和 LLaMA 这样的大模型中。它的核心思想是通过不断合并最常见的字符对来构建一个高效的词汇表。\",\"BPE 的核心思想:\",\"从字符级别开始，逐步合并高频的字符对。\",\"最终生成一个既能表示常见单词，又能拆解未知词的子词词汇表 。\",\"可以有效控制词汇表大小，同时避免“未登录词”问题（OOV, Out-of-Vocabulary）。\"]},\"175\":{\"h\":\"预训练过程\",\"t\":[\"BPE 算法预训练工作流程:\",\"训练语料为: Hello World , Hey Wow\",\"1. 读取训练语料，同时完成断句分词任务\",\"# filepaths: 训练语料所在的文件列表 def create_vocab(filepaths: List[str]) -> Dict[str, int]: # 获取所有单词和每个单词的出现次数词典 vocab = defaultdict(int) for path in tqdm(filepaths, desc='Creating vocabulary'): text = open(path, 'r', encoding='utf-8-sig').read() # 利用NLTK库提供的sent_tokenize方法完成断句功能，即将原文本按照空格，句号等标点符号结合语义进行断句。 sentences = sent_tokenize(text) # 遍历句子列表 for sentence in sentences: # 利用NLTK库提供的wordpunct_tokenize方法完成分词功能 tokens = wordpunct_tokenize(sentence) # 记录每个词的出现次数 for token in tokens: vocab[token] += 1 # vocab: 记录每个词的出现次数的词典 return vocab\",\"2. 过滤掉vocab中的低频词\",\"def truncate_vocab(vocab: Dict[str, int], mincount: int) -> None: tokens = list(vocab.keys()) for token in tokens: if vocab[token] < mincount: del(vocab[token])\",\"示例中设置为了1，不会过滤掉任何词。\",\"3. 数据预处理\",\"将训练语料中的每个单词按字符拆分，并在结尾加上特殊标记 </w> 表示单词结束。\",\"def prepare_bpe_vocab(vocab: Dict[str, int]) -> Dict[str, int]: bpe_vocab = {} # 遍历vocab中所有词 for token in vocab: # 每个词的每个字符后都加上空格，同时末尾加上 </w> 表示单词结束 ntoken = ' '.join(list(token)) + ' </w>' bpe_vocab[ntoken] = vocab[token] return bpe_vocab\",\"4. 经历N次迭代，合并前N个最频繁的字符对\",\" # 一共合并merges个高频字符对后,才结束词汇表的构建 for i in trange(merges, desc='Merging'): # 1. 获取每个相邻字符对的出现次数 pairs = get_stats(vocab) # 2. 获取当前最高频的字符对 best = max(pairs, key=pairs.get) # 3. 合并当前最高频的字符对 vocab = merge_vocab(best, vocab)\",\"4.1 获取每个相邻字符对的出现次数\",\"def get_stats(vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]: pairs = defaultdict(int) for word, freq in vocab.items(): # 对经过预处理的vocab中的每个词按空格进行切分 symbols = word.split() # 统计每个相邻字符对的出现次数 for i in range(len(symbols)-1): pairs[symbols[i],symbols[i+1]] += freq return pairs\",\"首轮统计展示\",\"4.2 获取当前最高频的字符对\",\"4.3 合并当前最高频的字符对\",\"def merge_vocab(pair: Tuple[str, str], v_in: Dict[str, int]) -> Dict[str, int]: # 1. 将传入的最高频字符对中的两个字符用空格拼接起来，如: \\\"H e\\\" bigram = re.escape(' '.join(pair)) v_out = {} # 2. 正则匹配含有“H e”的所有单词，并且“H”和“e”必须为两个独立的词，而不能为\\\"HH e\\\"或者\\\"H ee\\\"形式 p = re.compile(r'(?<!\\\\S)' + bigram + r'(?!\\\\S)') # 3. 遍历vocab中所有词 for word in v_in: # 3.1 用正则匹配并替换匹配上的 \\\"H e\\\" 为 “He” w_out = p.sub(''.join(pair), word) v_out[w_out] = v_in[word] # 4. 返回合并最高频字符对后的vocab return v_out\",\"5.根据N轮迭代合并后的Vocab来构建最终的频次表(每个子词的出现次数)\",\"def count_byte_freqs(vocab: Dict[str, int]) -> Dict[str, int]: freqs = defaultdict(int) for word in vocab: # 1. 按空格切分 bytes_ = word.split(' ') # 2. 每个子词出现次数加1 for byte in bytes_: freqs[byte] += 1 # 3. 添加一些特殊词 for token in ['<line/>', '</line>', '<pad>', '<unk>']: freqs[token] += 1 return freqs\",\"6.根据频次表构建最终的词汇表\",\"def create_vocab_maps(freqs: Dict[str, int]) -> (Dict[str, int], Dict[int, str]): # 1. 按照 词频从高到低 的顺序排序 ordered_freqs = sorted(freqs.items(), key=lambda x: x[1], reverse=True) vocab_to_idx, idx_to_vocab = {}, {} for i in range(len(ordered_freqs)): # 2. 构建词汇表 word, freq = ordered_freqs[i] vocab_to_idx[word] = i idx_to_vocab[i] = word return vocab_to_idx, idx_to_vocab\",\"BPE 算法预训练过程完整代码如下\",\" def train_bpe(filepaths: List[str], mincount: int, merges: int) -> 'BytePairtokenizer': vocab = create_vocab(filepaths) truncate_vocab(vocab, mincount) vocab = prepare_bpe_vocab(vocab) for i in trange(merges, desc='Merging'): pairs = get_stats(vocab) best = max(pairs, key=pairs.get) vocab = merge_vocab(best, vocab) freqs = count_byte_freqs(vocab) vocab_to_idx, idx_to_vocab = create_vocab_maps(freqs) return BytePairTokenizer(freqs, vocab_to_idx, idx_to_vocab)\"]},\"176\":{\"h\":\"分词过程\",\"t\":[\"1.对输入的文本进行断句加分词\",\" # 使用NLTK库提供的sent_tokenize方法进行分词 lines = sent_tokenize(open(filepath, encoding='utf-8-sig').read()) tokens = [] # 遍历所有句子 for line in lines: if len(line) > 1: tokens += get_line_ids(line, tokenizer)\",\"def get_line_ids(line: str, tokenizer: BytePairTokenizer) -> List[int]: # 对每个句子进行分词 tokens = wordpunct_tokenize(line) # 将每个词从str转换为list列表形式，同时列表末尾追加</w> tokens = [list(t) + ['</w>'] for t in tokens] ...\",\"2. 对当前句子中每个词进行子词合并加词ID映射，最后得到当前句子对应的Token列表\",\"def get_line_ids(line: str, tokenizer: BytePairTokenizer) -> List[int]: ... lineids = [] for token in tokens: # 2.1 对每个词进行子词合并，直到无法合并为止 token = tokenizer.merge_bytes(token) # 2.2 将当前词列表中每个子词映射为字典中对于的词ID ids = tokenizer.get_byte_ids(token) lineids += ids sol_id = tokenizer.get_byte_id('<line/>') eol_id = tokenizer.get_byte_id('</line>') lineids = [sol_id] + lineids + [eol_id] return lineids\",\"2.1 对每个词进行子词合并，直到无法合并为止\",\" # 对当前词的子词进行合并，直到无法合并为止 def merge_bytes(self, bytes_: List[str]) -> List[str]: bytes_, merged = self.merge_max_pair(bytes_) while merged: bytes_, merged = self.merge_max_pair(bytes_) return bytes_ def merge_max_pair(self, bytes_: List[str]) -> (List[str], bool): # 1. 取出出现次数最多的字符对 max_pair = self.get_max_pair_idxs(bytes_) merged = True if max_pair is not None else False if merged: # 2. 合并该字符对 bytes_ = bytes_[:max_pair[0]] + \\\\ [''.join(bytes_[max_pair[0]:max_pair[1]+1])] + \\\\ bytes_[max_pair[1]+1:] return bytes_, merged def get_max_pair_idxs(self, bytes_) -> Tuple[int, int]: pairs = {} # 1. 遍历所有相邻字符对的组合 for i in range(1, len(bytes_)): pair = ''.join(bytes_[i-1:i+1]) # 2. 判断没饿过字符对是否存在于频次表中，如果存在记录出现次数 if pair in self.freqs: pairs[(i-1, i)] = self.freqs[pair] # 3. 取出出现次数最多的字符对 return None if len(pairs) == 0 else max(pairs, key=pairs.get)\",\"2.2 将当前词列表中每个子词映射为字典中对于的词ID\",\" def get_byte_ids(self, bytes_): ids = [] for byte in bytes_: if byte in self.vocab_to_idx: ids.append(self.vocab_to_idx[byte]) else: ids.append(self.vocab_to_idx[self.unk]) return ids\"]},\"177\":{\"h\":\"大模型微调(Fine Tuning)知识扫盲\",\"t\":[\"大模型微调(Fine Tuning)知识扫盲\"]},\"178\":{\"h\":\"什么是大模型 ？\",\"t\":[\"开始之前，为了方便大家理解，我们先对大模型做一个直观的抽象。\",\"本质上，现在的大模型要解决的问题，就是一个序列数据转换的问题：\",\"输入序列 X = [x1, x2, ..., xm]\",\"输出序列Y = [y1, y2, …, yn]\",\"X和Y之间的关系是：Y = WX。\",\"我们所说的“大模型”这个词：“大”是指用于训练模型的参数非常多，多达千亿、万亿；而“模型”指的就是上述公式中的矩阵W。\",\"在这里，矩阵W就是通过机器学习，得出的用来将X序列，转换成Y序列的权重参数组成的矩阵。\",\"需要特别说明：这里为了方便理解，做了大量的简化。在实际的模型中，会有多个用于不同目的的权重参数矩阵，也还有一些其它参数。\"]},\"179\":{\"h\":\"为什么要对大模型进行微调 ？\",\"t\":[\"通常，要对大模型进行微调，有以下一些原因：\",\"因为大模型的参数量非常大，训练成本非常高，每家公司都去从头训练一个自己的大模型，这个事情的性价比非常低；\",\"Prompt Engineering的方式是一种相对来说容易上手的使用大模型的方式，但是它的缺点也非常明显。因为通常大模型的实现原理，都会对输入序列的长度有限制，Prompt Engineering 的方式会把Prompt搞得很长。\",\"越长的Prompt，大模型的推理成本越高，因为推理成本是跟Prompt长度的平方正向相关的。\",\"另外，Prompt太长会因超过限制而被截断，进而导致大模型的输出质量打折口，这也是一个非常严重的问题。\",\"对于个人使用者而言，如果是解决自己日常生活、工作中的一些问题，直接用Prompt Engineering的方式，通常问题不大。\",\"但对于对外提供服务的企业来说，要想在自己的服务中接入大模型的能力，推理成本是不得不要考虑的一个因素，微调相对来说就是一个更优的方案。\",\"Prompt Engineering的效果达不到要求，企业又有比较好的自有数据，能够通过自有数据，更好的提升大模型在特定领域的能力。这时候微调就非常适用。\",\"要在个性化的服务中使用大模型的能力，这时候针对每个用户的数据，训练一个轻量级的微调模型，就是一个不错的方案。\",\"数据安全的问题。如果数据是不能传递给第三方大模型服务的，那么搭建自己的大模型就非常必要。通常这些开源的大模型都是需要用自有数据进行微调，才能够满足业务的需求，这时候也需要对大模型进行微调。\"]},\"180\":{\"h\":\"如何对大模型进行微调 ？\",\"t\":[\"从参数规模的角度，大模型的微调分成两条技术路线：\",\"一条是对全量的参数，进行全量的训练，这条路径叫全量微调FFT(Full Fine Tuning)。\",\"一条是只对部分的参数进行训练，这条路径叫PEFT(Parameter-Efficient Fine Tuning)。\",\"FFT的原理，就是用特定的数据，对大模型进行训练，将W变成，相比W ，最大的优点就是上述特定数据领域的表现会好很多。\",\"但FFT也会带来一些问题，影响比较大的问题，主要有以下两个：\",\"一个是训练的成本会比较高，因为微调的参数量跟预训练的是一样的多的；\",\"一个是叫灾难性遗忘(Catastrophic Forgetting)，用特定训练数据去微调可能会把这个领域的表现变好，但也可能会把原来表现好的别的领域的能力变差。\",\"PEFT主要想解决的问题，就是FFT存在的上述两个问题，PEFT也是目前比较主流的微调方案。\",\"从训练数据的来源、以及训练的方法的角度，大模型的微调有以下几条技术路线：\",\"监督式微调SFT(Supervised Fine Tuning) : 用人工标注的数据，用传统机器学习中监督学习的方法，对大模型进行微调；\",\"基于人类反馈的强化学习微调RLHF(Reinforcement Learning with Human Feedback) : 把人类的反馈，通过强化学习的方式，引入到对大模型的微调中去，让大模型生成的结果，更加符合人类的一些期望；\",\"基于AI反馈的强化学习微调RLAIF(Reinforcement Learning with AI Feedback) : 原理大致跟RLHF类似，但是反馈的来源是AI。这里是想解决反馈系统的效率问题，因为收集人类反馈，相对来说成本会比较高、效率比较低。\",\"不同的分类角度，只是侧重点不一样，对同一个大模型的微调，也不局限于某一个方案，可以多个方案一起。\",\"微调的最终目的，是能够在可控成本的前提下，尽可能地提升大模型在特定领域的能力。\"]},\"181\":{\"h\":\"常用的PEFT方案\",\"t\":[\"从成本和效果的角度综合考虑，PEFT是目前业界比较流行的微调方案。接下来介绍几种比较流行的PEFT微调方案。\"]},\"182\":{\"h\":\"Prompt Tuning\",\"t\":[\"Prompt Tuning的出发点，是基座模型(Foundation Model)的参数不变，为每个特定任务，训练一个少量参数的小模型，在具体执行特定任务的时候按需调用。\",\"Prompt Tuning的基本原理是在输入序列X之前，增加一些特定长度的特殊Token，以增大生成期望序列的概率。\",\"具体来说，就是将变成，。\",\"Prompt Tuning是发生在Embedding这个环节的。如果将大模型比做一个函数：，那么Prompt Tuning就是在保证函数本身不变的前提下，在X前面加上了一些特定的内容，而这些内容可以影响X生成期望中Y的概率。\",\"Prompt Tuning的具体细节,可以参见：The Power of Scale for Parameter-Efficient Prompt Tuning。\"]},\"183\":{\"h\":\"Prefix Tuning\",\"t\":[\"Prefix Tuning的灵感来源是，基于Prompt Engineering的实践表明，在不改变大模型的前提下，在Prompt上下文中添加适当的条件，可以引导大模型有更加出色的表现。\",\"Prefix Tuning的出发点，跟Prompt Tuning的是类似的，只不过它们的具体实现上有一些差异。\",\"Prompt Tuning是在Embedding环节，往输入序列X前面加特定的Token。而Prefix Tuning是在Transformer的Encoder和Decoder的网络中都加了一些特定的前缀。\",\"具体来说，就是将Y=WX中的W，变成。\",\"Prefix Tuning也保证了基座模型本身是没有变的，只是在推理的过程中，按需要在W前面拼接一些参数。\",\"Prefix Tuning的具体细节,可以参见：Prefix-Tuning: Optimizing Continuous Prompts for Generation。\"]},\"184\":{\"h\":\"LoRA\",\"t\":[\"LoRA是跟Prompt Tuning和Prefix Tuning完全不相同的另一条技术路线。\",\"LoRA背后有一个假设：我们现在看到的这些大语言模型，它们都是被过度参数化的。而过度参数化的大模型背后，都有一个低维的本质模型。\",\"通俗讲人话：大模型参数很多，但并不是所有的参数都是发挥同样作用的；大模型中有其中一部分参数，是非常重要的，是影响大模型生成结果的关键参数，这部分关键参数就是上面提到的低维的本质模型。\",\"LoRA的基本思路，包括以下几步：\",\"首先, 要适配特定的下游任务，要训练一个特定的模型，将Y=WX变成Y=(W+∆W)X，这里面∆W主是我们要微调得到的结果；\",\"其次，将∆W进行低维分解∆W=AB (∆W为m * n维，A为m * r维，B为r * n维，r就是上述假设中的低维)；\",\"接下来，用特定的训练数据，训练出A和B即可得到∆W，在推理的过程中直接将∆W加到W上去，再没有额外的成本。\",\"另外，如果要用LoRA适配不同的场景，切换也非常方便，做简单的矩阵加法即可：(W + ∆W) - ∆W + ∆W'。\",\"关于LoRA的具体细节,可以参见LoRA: Low-Rank Adaptation of Large Language Models。\"]},\"185\":{\"h\":\"QLoRA\",\"t\":[\"LoRA 效果已经非常好了，可以媲美全量微调的效果了，那为什么还要有个QLoRA呢？\",\"这里先简单介绍一下，量化（Quantization）。\",\"量化，是一种在保证模型效果基本不降低的前提下，通过降低参数的精度，来减少模型对于计算资源的需求的方法。\",\"量化的核心目标是降成本，降训练成本，特别是降后期的推理成本。\",\"QLoRA就是量化版的LoRA，它是在LoRA的基础上，进行了进一步的量化，将原本用16bit表示的参数，降为用4bit来表示，可以在保证模型效果的同时，极大地降低成本。\",\"论文中举的例子，65B的LLaMA的微调要780GB的GPU内存；而用了QLoRA之后，只需要48GB。效果相当惊人！\",\"关于QLoRA的具体细节,可以参见：QLoRA: Efficient Finetuning of Quantized LLMs。\",\"PEFT 的微调方法，还有很多种，限于篇幅原因，不再这里一一介绍。感兴趣的朋友，可以阅读这篇论文：Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning。\",\"相关阅读资料:\",\"近代自然语言处理技术发展的“第四范式”\",\"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\"]},\"186\":{\"h\":\"通俗易懂讲解LoRA微调\",\"t\":[\"通俗易懂讲解LoRA微调\",\"论文链接: LoRA: Low-Rank Adaptation of Large Language Models\"]},\"187\":{\"h\":\"符合认知的大模型微调流程\",\"t\":[\"符合我们直接观念所想的大模型微调流程为:\",\"准备与下游任务相关的数据集\",\"选择合适的预训练好的大模型\",\"在特定任务相关的数据集上执行有监督全量参数微调，将预训练模型的参数 调整为适合下游任务的 \",\"其中第三步通过反向传播全量更新模型参数的过程如下：\",\"图1: 反向传播更新模型参数过程\",\"上述的全量微调流程问题在于大模型的参数量往往特别大，也就是 占据了特别大的内存资源和计算资源，有没有办法能够减少 所占的内存资源和计算资源呢？\",\"图2: 低秩分解\",\"我们可以利用矩阵分解技术，将原始的 矩阵从 参数量降低到 级别的参数量，如下图所示:\",\"图3: 待微调的参数量下降到原来的9%\",\"这样一来，我们微调大模型的流程就变为了(前两步不变)：\",\"初始化低秩矩阵：对于需要微调的密集层，初始化两个低秩矩阵 和 ，其维度分别为 和 ，其中 是低秩的秩，远小于原始矩阵的维度。\",\"冻结预训练模型参数：在微调过程中，保持预训练模型的原始参数 不变，只对低秩矩阵 和 进行训练。\",\"执行微调训练：在准备好的数据集上，通过反向传播算法更新低秩矩阵 和 的参数，使得模型在下游任务上的表现逐渐优化。\",\"合并参数（可选）：在微调完成后，如果需要，可以将低秩矩阵 和 的更新量与原始参数 合并，得到最终适用于下游任务的模型参数 。\",\"这在LoRA这篇论文中也被称为低秩分解自适应技术。\",\"图4: 常规微调 VS LoRA微调\"]},\"188\":{\"h\":\"大模型微调大致发展历史\",\"t\":[\"大公司或者研究机构，都是有足够资源的来开发大模型，但是对于一般的小公司或者个人来说，要想开发自己的大模型几乎不可能，要知道像 ChatGPT 这样的大模型，一次训练的成本就在上千亿美元。\",\"那么那些小公司或者个人，又怎么能够利用这些开源的大模型，在自己的数据上继续训练，从而应用于自己的业务场景？有没有低成本的方法微调大模型？\",\"答案是有的。目前主流的方法包括2019年 Houlsby N 等人提出的 Adapter Tuning，2021年微软提出的 LORA，斯坦福提出的 Prefix-Tuning，谷歌提出的 Prompt Tuning，2022年清华提出的 P-tuning v2。\",\"这些方法都有各自的特点，从个人使用情况来说，LORA 的效果会好于其它几种方法。其它方法都有各自的一些问题：\",\"Adapter Tuning 增加了模型层数，引入了额外的推理延迟\",\"Prefix-Tuning 难于训练，且预留给 Prompt 的序列挤占了下游任务的输入序列空间，影响模型性能\",\"P-tuning v2 很容易导致旧知识遗忘，微调之后的模型，在之前的问题上表现明显变差\",\"基于上述背景，LORA 得益于前人的一些关于内在维度（intrinsic dimension）的发现：\",\"模型是过参数化的，它们有更小的内在维度，模型主要依赖于这个低的内在维度（low intrinsic dimension）去做任务适配。\",\"假设模型在任务适配过程中权重的改变量是低秩（low rank）的，由此提出低秩自适应（LoRA）方法。\",\"LoRA 允许我们通过优化适应过程中密集层变化的秩分解矩阵，来间接训练神经网络中的一些密集层，同时保持预先训练的权重不变。\"]},\"189\":{\"h\":\"LoRA 微调\",\"t\":[\"图5: LoRA 微调流程\",\"LoRA 的思想很简单:\",\"在 LoRA 论文中，在原始预训练语言模型（Pre - trained Language Model，简称 PLM）旁添加一条旁路，进行一次降维再升维的操作，以此来模拟所谓的内在秩（intrinsic rank）。\",\"训练的时候固定 PLM 的参数，只训练降维矩阵 A 与升维矩阵 B 。而模型的输入输出维度不变，输出时将 BA 与 PLM 的参数叠加。\",\"用随机高斯分布初始化 A ，用 0 矩阵初始化 B ，保证训练的开始此旁路矩阵依然是 0 矩阵。\",\"图6: 随机高斯分布初始化 A ，用 0 矩阵初始化 B\",\"假设要在下游任务微调一个预训练语言模型（如 GPT-3），则需要更新预训练模型参数，公式表示如下：\",\"其中， 是预训练模型初始化的参数， 就是需要更新的参数。如果是全参数微调，则它的参数量 （如果是 GPT-3，则 ）。从这可以看出要全参数微调大语言模型，代价是非常高的。\",\"而对于 LoRA 来说，只需要微调 。\",\"具体来看，假设预训练的矩阵为 ，它的更新可表示为：\",\"其中秩 。\",\"在 LoRA 的训练过程中， 是固定不变的，只有 和 是训练参数。\",\"在前向过程中， 与 都会乘以相同的输入 ，最后相加：\",\"LoRA 的这种思想有点类似于残差连接，同时使用这个旁路的更新来模拟 Full Fine-Tuning 的过程。并且，Full Fine-Tuning 可以被看作是 LoRA 的特例（当 等于 时）。\",\"在推理过程中，LoRA 也几乎未引入额外的 Inference Latency，只需要计算 即可。\",\"LoRA 与 Transformer 的结合也很简单，仅在 QKV Attention 的计算中增加一个旁路。\"]},\"190\":{\"h\":\"矩阵A和B为什么不能同时为零？\",\"t\":[\"在前面我们介绍了，用随机高斯分布初始化 ，用 0 矩阵初始化 。矩阵 为什么不也用 0 初始化？\",\"这主要是因为如果矩阵 也用 0 初始化，那么矩阵 的梯度就始终为 0，无法更新参数，导致 。这里简单推理一下。\",\"对于 ，设 ，则：\",\"因此：\",\"如果矩阵 也用 0 初始化，那么上面的梯度就变成了 0，所以矩阵 不能用 0 初始化。\",\"同样，我们看一下矩阵 初始化为 0 的影响。\",\"由于矩阵 的参数会发生更新，而 矩阵又不是 0 矩阵，因此后面 ，所以矩阵 可以用 0 初始化。\"]},\"191\":{\"h\":\"秩的选择\",\"t\":[\"论文实验结果显示，对于一般的任务， r=1,2,4,8 就足够了。而一些领域差距比较大的任务可能需要更大的 r 。\",\"同时，增加 r 值变大并不能提升微调的效果，这可能是因为参数量增加需要更多的语料。\",\"图7: 秩的选择\"]},\"192\":{\"h\":\"注意\",\"t\":[\"q进行 LoRA 高效的模型微调，重点是保持参数尺寸最小化。\",\"使用 PEFT 库来实现 LoRA，避免复杂的编码需求。\",\"将 LoRA 适应扩展到所有线性层，增强整体模型的能力。\",\"保持偏置和层归一化可训练，因为它们对模型的适应性至关重要，并且不需要低秩适应。\",\"应用量化低秩适应（Quantized LoRA，简称 QLoRA）以节省 GPU 显存并训练模型，从而能够训练更大的模型。\",\"量化是一种在深度学习领域用于减少模型内存占用和计算量的技术。在模型训练和推理过程中，神经网络的权重矩阵通常以高精度的浮点数（如 32 位浮点数）形式存储和计算，这会占用大量的内存资源并消耗较多的计算资源。量化通过将这些高精度的浮点数转换为低精度的整数（如 4 位或 8 位整数）来实现数据的压缩。\",\"在 LoRA 微调的场景中，QLoRA 就是利用量化技术的一个变体。它将权重矩阵量化为 4 位或 8 位整数，在不损失太多性能的情况下减少了模型的大小，使得模型可以在资源有限的设备上进行训练和部署，同时还能适应更多的参数。\",\"基于大模型的内在低秩特性，增加旁路矩阵来模拟全参数微调，LoRA 通过简单有效的方案来达成轻量微调的目的。它的应用自不必提，可以将现在的各种大模型通过轻量微调变成各个不同领域的专业模型。\",\"此外，考虑 OpenAI 对 GPT 模型的认知，GPT 的本质是对训练数据的有效压缩，从而发现数据内部的逻辑与联系，LoRA 的思想与之有相通之处，原模型虽大，但起核心作用的参数是低秩的，通过增加旁路，达到四两拨千斤的效果。\"]},\"193\":{\"h\":\"Prompt Engineering 知识扫盲\",\"t\":[\"Prompt Engineering 知识扫盲\"]},\"194\":{\"h\":\"什么是Prompt Engineering?\",\"t\":[\"Prompt (提示词) 是人类发给各种人工智能模型、用以完成特定任务的指令。\",\"Prompt Engineering (提示词工程) 是指我们为了让LLM能够更好地完成我们给它的任务，我们对Prompt进行优化、调整的过程。\",\"可能会有人这么问，LLM已经这么强了，直接丢给它个指令，让他去执行就好了，为什么还需要Prompt Engineering呢？\",\"确实像OpenAI的GPT4这样的LLM已经非常强了，很多简单的任务，我们直接用自然语言丢给他就去执行就好了。但是，对于一些复杂的问题，Prompt写得好不好，直接影响着大模型给出答案的正确与否。\",\"本质上，LLM是一个概率模型，它只是在给定的信息的前提下，给出概率最大的结果，它并不保证结果的合理性和正确性。\",\"要让LLM给出的结果尽可能地合理、正确，这是我们使用LLM的人的职责。\",\"这就是我们要去学习Prompt Engineering的原因。\"]},\"195\":{\"h\":\"如何写好Prompt?\"},\"196\":{\"h\":\"要明确,要具体\",\"t\":[\"我们发给LLM的批令，越明确、越具体，对于LLM越友好。\",\"举个例子，我们让LLM对一段文字进行总结：\",\"Prompt 2相比Prompt 1，对输出有了更加明确具体的要求，这样LLM输出的内容也会更加贴合我们的需求。另外，我们还用了'###'作为分隔符，进一步帮LLM明确要求。\",\"我们在给LLM发指令的时候，第一个关键点，就是我们要把给LLM做的任务尽可能细化，把要求尽可能明确、具体地描述出来。\"]},\"197\":{\"h\":\"给LLM更多的时间去思考\",\"t\":[\"《思考快与慢》这本书里介绍了我们人类大脑的“系统1”和“ 系统2”。\",\"系统1是快思考系统，反应很快，但可能会出错。\",\"系统2是慢思考系统，需要更长的反应时间，进行思考、推理，但结果会更加靠谱。\",\"默认情况下，LLM就像是一个快思考的系统，他利用自己已掌握的知识，快速给出答案，但并不能保证结果的正确性。\",\"为了让LLM给出的答案更加靠谱，我们需要通过Prompt Engineering 的方式，把LLM的慢思考调动起来。\",\"这就是“给LLM更多的时间去思考”背后的大致逻辑。\",\"给LLM更多的时间去思考，一个简单的技巧是在你的Prompt后面，加上这样一句话“Let’s think step by step”。这句话会引导LLM，会去分步骤思考，效果会比不加这句话要好。\",\"另一个技巧，在Prompt中加入一些例子，让LLM照着例子进行推理、思考。这一块的技巧性很强，我们在接下来的部分，介绍几种具体的技巧。\"]},\"198\":{\"h\":\"思维链技术：Chain-of-Thought\",\"t\":[\"这是《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》这篇论文里讲的一个Prompt Engineering的技巧。\",\"CoT(Chain-of-Thought) 的核心思想是，在Prompt中加入一些示例，来引导LLM展现出更好的推理能力。\",\"这里的关键是在Prompt中加入的示例，在这些示例中，我们会用自然语言描述一系列的推理过程，并最终引导出示例问题的正确结果。\",\"这个过程有点像，我们教小孩做应用题，我们先给小孩子分析讲解一些示例。然后再把新的问题让小孩子来解决。小孩子根据从示例中学习到的推理、分析能力，最终解出了新的问题。\",\"下面我们来看论文中给的CoT的例子：\",\"左侧是常规的Prompt，右侧是CoT Prompt\",\"蓝色标记出的部分是提供给LLM的示例。绿色标记出的部分是LLM输出的推理过程。\",\"在使用CoT这种Prompt Engineering技巧的时候，有几个注意点：\",\"CoT是LLM足够大（参数足够多，通常是在1000亿参数）时才涌现出来的能力。因此，在一些不够大的LLM上，CoT的效果并不明显。\",\"通常，在Prompt中加入的示例不是1条，而是多条。具体要考虑解决的问题类型，以及Prompt的长度（因为LLM的Prompt长度通常都是有长度限制的）。\"]},\"199\":{\"h\":\"自一致性技术：Self-Consistency\",\"t\":[\"这是《Self-Consistency Improves Chain of Thought Reasoning in Language Models》 这篇论文里讲的另一个Prompt Engineering的技巧。\",\"Self-Consistency技术是在CoT技术的基础之上，进行的进一步优化，目的是为了让LLM的推理能力能够更进一步提升。\",\"Self-Consistency的大致原理是这样：\",\"利用CoT Prompting技巧，写好Prompt；\",\"不要让LLM只生成最合适的唯一一个结果，而是利用LLM结果的多样性，生成多种不同推理路径所得的结果的集合；\",\"从结果集合中投票选择，选出投票最多的结果，做为最终的答案。\",\"这里有像我们人类解决问题的过程，如果我们用多种不同的方法去求解，大多数方法求解出来结果都一样的答案，那很有可能就是我们最终的答案。\",\"下面我们来看论文中给的Self-Consistency的例子：\",\"在上面的例子中，虚线之上是标准的CoT的过程，它得到的结果是错的。虚线之下是Self-Consistency的过程，得到的三个答案中，有1个是错的，有2个是正确的。最终答案是大多数投票的结果，是正确的。\"]},\"200\":{\"h\":\"从易至难技术：Least-to-Most\",\"t\":[\"这是《Least-to-Most Prompting Enables Complex Reasoning in Large Language Models》 这篇论文中介绍的方法。\",\"CoT的特点是同类型问题的迁移思考，因此，如果给的例子是比较简单的问题，而给的问题却是难度大很多的问题，这时候CoT的效果就不尽如人意。\",\"LtM(Least-to-Most)主是为了解决CoT这种从易到难的迁移能力不足而诞生的。\",\"LtM的核心思想是：教LLM把复杂问题，拆解成一系列的简单问题，通过解决这一系列的简单问题，来最终得到复杂问题的结果。\",\"LtM的过程包含两个阶段：\",\"分解阶段：把复杂问题分解成一系列的简单子问题。这个阶段的Prompt中要包含分解问题的示例，要和分解的问题；\",\"解决子问题阶段：这个阶段的Prompt中包含三部分内容：一是完整的LtM的例子；二是已解决的子问题及其答案列表；三是接下来要解答的子问题。\",\"这里也非常像我们人类学习解决复杂问题的过程，我们通过把复杂问题拆解成一个个的简单问题，通过把一个个的简单问题解决掉，最终把复杂问题也解决了。\",\"下面我们来看看论文中LtM的例子：\",\"从上图中，我们可以对LtM Prompting有一个直观的认知，通过引导LLM解决子问题，一步步引导LLM得出复杂问题的结果。\"]},\"201\":{\"h\":\"应用层\"},\"202\":{\"h\":\"GPT-1 论文\",\"t\":[\"GPT-1 论文\",\"论文: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\"]},\"203\":{\"h\":\"摘要\",\"t\":[\"自然语言理解包含了广泛的多样性任务，比如文本蕴涵，问答，语义相似度评估，文本分离。然而大规模的未标注的文本语料是丰富，而特定任务学习的标注数据有非常少，使得要充分做区分地训练模型非常有挑战性。作者证明通过在丰富的无标签文本语料库生成预训练generative pre-training语言模型，然后在每项具体任务上判别性微调discriminative fine-tuning，可以实现巨大的收益。对比之前的方法，作者在微调阶段使用任务感知的输入转换来实现有效的迁移，仅仅需要小小修改模型架构。通用的任务未知task-agnostic模型优于那些为每个任务精心设计的模型，在12个研究任务中9个提升到SOTA。如，作者在常识推理(Stories Close Test)上提升8.9%， 在问答上提升5.7%(RACE)，文本蕴含提升1.5%(MultiNLI)。\"]},\"204\":{\"h\":\"简介\",\"t\":[\"在NLP中，有效地从无标记的原始文本中学习的能力能减轻对监督学习的依赖。大部分深度学习方法需要大量人工标注的数据，这限制了它们在许多缺乏标记数据领域的适用性。在这种情况下，模型能从无标记数据中充分利用语义信息，为收集更多的标注数据提供了更多一个有价值的替代方案，标注数据昂贵又耗时。进一步来说，即便是那些有大量标注数据的场景，无监督学习得到的好的表示也能提供显著的提升。最有说服力的证据就是到目前为止大量使用预训练的词嵌入来提升一系列NLP任务表现。\",\"无论到什么程度，从无标注文本中充分利用词级别以外的信息是有挑战性的，有两个主要原因。\",\"不清楚在学习文本表示时，什么样的优化目标是最高效的迁移。近期研究考虑过各种各样的目标，如语言模型，机翻，语句连贯性，每种方法在不同任务上都优于其它方法。\",\"对于将这些学习到的表征迁移到目标任务的最有效方法，目前还没有达成共识。已有的技术涉及对模型架构进行特定任务的修改、使用复杂的学习方案以及添加辅助学习目标的组合。这些不确定性使得开发有效的语言处理半监督学习方法变得困难。\",\"在本文中，作者用无监督的预训练和监督的微调组合来探索关于语言理解任务半监督方法。目标是学习一个全局表示，迁移它来稍微适应一系列广泛的任务。作者假设采用一个大型无标记文本语料库和几个人工标记训练样本的数据集(目标任务)。该设置不需要这些目标任务和无标记语料库是一个领域的。并采用两段式训练流程。首先，在无标记数据上使用语言模型目标来学习神经网络初始化的参数。接着，使用对应特定任务的监督目标来调整这些参数。(预训练+微调)\",\"对于作者的模型架构，使用的是Transformer，它被证明在许多任务上有很强的表现，如机翻，文本生成，句法解析。该模型在文本上处理长期依赖提供了更结构化的内存，相比其他替代方案如RNN，Transformer跨各种各样任务的迁移性能更强。在迁移阶段，作者利用源于遍历式(traversal-style)方法的特定任务的输入改写，其将结构化文本输入处理为单一的连续字符序列。如作者在实验中证明的，这些改写使得在预训练模型架构上用最小的修改就会有效。\",\"作者在四种类型的语言理解任务评估作者的方法——自然语言推断NLI，问答，语义相识度，和文本分类。作者通用的任务未知task-agnostic模型优于那些为每个任务精心设计的模型，在12个研究任务中9个提升到SOTA。\",\"常识推理提升8.9%(Stories Cloze Test)\",\"问答提升5.6%(RACE)\",\"文本蕴含提升5.5%(MultiNLI)\",\"GLUE多任务提升5.5%.\",\"也分析了在四种不同设置下预训练模型的零次(zero-shot)表现，证明其确实为下游任务获取到了有用的语言知识。\"]},\"205\":{\"h\":\"相关工作\",\"t\":[\"NLP半监督学习: 预训练对于获取不同级别信息的需要，如从词级别信息到更高的(段落级别或者句子级别的)词嵌入。\",\"无监督预训练: 无监督预训练+监督微调方式，Transformer比LSTM能获取长距离信息。\",\"辅助训练目标: 添加一个无监督训练目标是半监督学习的一种替代形式。如POS tag，语义组块chuking, NER， 以及语言模型来提升标记的语义角色。\"]},\"206\":{\"h\":\"框架\",\"t\":[\"作者训练流程有两个阶段:\",\"在大规模文本语料上学习高容量的语言模型\",\"微调阶段，用标记的数据对特定任务微调模型\"]},\"207\":{\"h\":\"无监督预训练\",\"t\":[\"给定一个无监督学习的语料tokens ，使用标准的语言模型目标并最大化其似然：\",\"这里 是上下文窗口大小，条件概率 是参数 的神经网络模型。这些参数会以随机梯度下降训练。\",\"在作者的实验中，语言模型使用多层的 Transformer decoder（Transformer 的变种 ）。该模型在上下文 token 上使用多头自注意力操作，接一个逐位置的前馈层来生成目标字符的分布输出（比原本少了一个多头自注意力 ）：\",\"公式如下：\",\"这里 是上下文字符向量， 是层数， 是字符嵌入矩阵， 是位置嵌入矩阵。\"]},\"208\":{\"h\":\"有监督微调\",\"t\":[\"在训练公式 （ 1 ） 中的目标函数模型后，作者在监督学习目标任务上调整参数。假设有标记数据集 ，每个实例有输入字符的序列构成 ，对应着标签 。输入通过作者的预训练模型会得到最好的 transformer block 的激活状态 ，将其喂进一个参数为 的添加的线性输出层来预测 有：\",\"给出最大化的目标函数为：\",\"作者还发现加入语言模型作为辅助目标来微调有助于学习：(a) 提升监督模型的泛化能力；(b) 加速收敛。这跟之前的工作一样，观测发现用辅助目标能提升性能。尤其是，作者用以下优化（加权 ）目标：\",\"总之，作者额外要微调的参数只有 ，以及分割字符嵌入矩阵。\"]},\"209\":{\"h\":\"特定任务输入转换\",\"t\":[\"文本分类：直接微调模型\",\"问答或文本蕴含：输入是结构化的，如有序句子对，三元组（文档，问题和答案）\",\"因为作者的预训练模型是用连续的文本序列训练的，需要做些修改以便用在这些任务上。之前的工作提出了在迁移表征顶部学习特定任务的架构。这种方法重新引入了大量特定任务的定制化输入，并且不会对额外的架构组件使用迁移学习。相反，作者使用遍历式方法，就是将结构化输入转换为有序序列以便作者预训练能处理。这些输入转换使作者避免跨任务架构的大改。作者在下面部分和可视化插图 1 提供了这些输入的简洁描述。所有的转换包括添加随机初初始化的开始和结束标记 。\",\"文本蕴含：拼接前提文本 和假设 为 token 序列，用 $ 符来分隔两者。\",\"相似度：对于相似任务，两个比较的句子没有内在顺序。为了反映这点，作者修改输入序列来包含 2 种可能的顺序（用分隔符分隔），并独立地处理 2 个序列表示 ，逐元素相加然后送入线性输出层。\",\"问答和常识推理：对于这些任务，给定文档 ，一个问题 和一个可能答案集 。将文档和问题跟每个可能答案拼接起来，再在其中添加一个分隔符得到 。每个这些序列用作者的模型独立处理后通过一个 softmax 层归一化来生成所有可能答案的分布。\"]},\"210\":{\"h\":\"实验\"},\"211\":{\"h\":\"设置\",\"t\":[\"无监督预训练：BOOKS CORPUS 数据集预训练模型。长文本能让生成模型学习到长依赖信息的条件概率。ELMO 方法处理 1B Word benchmarks，在句子级别打乱顺序以破坏长距离结构信息，达到非常低的 18.4 困惑度。\",\"模型的具体配置:\",\"Transformer 架构：12 层有自注意力头（768 维隐藏层，12 个注意力头）transformer decoder 结构。\",\"逐位置前馈神经网络（position-wise feed-forward networks）：3072 维内部隐藏层。\",\"Adam 优化器方案：最大 lr=2.5e-4。开始 2000 次从 0 线性上升更新，再使用 cosine 方案退火到 0。\",\"采样与训练：从 512 连续 tokens 中随机采样得到 64 小批次样本，训练 100 轮。\",\"层归一化：改进版的 layerNorm，以 权重初始化。\",\"词汇表与正则化：40,000 合并的 BPE 词汇表，残差，嵌入和注意力层以 0.1 的 Dropout 来正则化。\",\"改进版 L2 正则：所有无偏差或增益权重设置为 。\",\"激活函数：GELU 作为激活函数。\",\"位置嵌入：使用学习的位置嵌入，而不是原始 Transformer 的正余弦曲线。\",\"数据清洗与分词：使用 ftfy 清洗原始 BooksCorpus，去掉字符和空格，再使用 spaCy tokenize。\",\"微调的细节:\",\"除非指定，使用无监督预训练超参数设置。分类层使用 0.1 的 Dropout。大部分任务，lr=6.25e-5，批大小为 32。在大部分任务中基本上 3 轮训练就足够了。lr 用以训练步数的 0.2% 预热衰减方案。 设置为 0.5。\"]},\"212\":{\"h\":\"监督微调\",\"t\":[\"微调任务和数据集如下：\",\"NLI 就是识别文本蕴含。涉及读取一对句子，判断它们之间的关系，是蕴含，矛盾或中立。因为存在各种变化现象，如词汇蕴含，共指，词汇和句法歧义，所以还是很有挑战性的。\",\"下表2是作者模型和之前SOTA模型NLI的结果比较：\",\"RTE数据集比较小，只有2490样本，只达到了56.0%准确率。\",\"问答和常识推理 结果如下表3，RACE数据集由初高中考试题构成。在Story Cloze和RACE提升明显。证明模型具有有效处理上下文长距离的信息的能力。\",\"语义相似度 语义相似度(或释义发现)任务涉及预测两个句子在语义上是否相等。挑战在于识别语句是否是概念改写，理解反面，处理句法歧义。使用的数据集：\",\"MRPC Microsoft Research Paraphrase corpus 是一些句子对，有的是同义的，有的是不同义的。\",\"QQP Quora Question Pairs 美国知识问答网站 Quora 上的问题答案数据集\",\"STS-B Semantic Textual similarity benchmark 语义文本相似度数据集，样本为文本对，评判两个文本语义信息的相似度，分数为1-5。\",\"在STS-B上有1个点的绝对提升，比Single-task BiLSTM + ELMo + Attn有4.2%的绝对提升。\",\"分类 两个不同分类任务的评估结果，也在上表4中。CoLA——Corpus of Linguistic Acceptability语言可接受性语料库，纽约大学发布的有关语法的数据集，该任务主要是对一个给定句子，判定其是否语法正确，因此CoLA属于单个句子的文本二分类任务。\",\"SST-2——The Stanford Sentiment Treebank, 主要针对电影评论来做情感分类，因此SST属于单个句子的文本分类任务（其中SST-2是二分类，SST-5是五分类，SST-5的情感极性区分的更细致)。\",\"CoLA上取得45.4，SST-2取得91.3的准确率，整体得分72.8。\",\"总体而言，在12个数据集上的9个取得SOTA结果，比许多情况下的ensemble模型要好。而且能适应不同大小数据集。\"]},\"213\":{\"h\":\"分析\",\"t\":[\"层数的迁移学习影响: 从预训练到微调迁移学习过程中，如下表2，在MultiNLI和RACE上的性能随着层数的变化而变化。作者观察标准结果，在MultiNLI上转移embedding能提升结果，每一层Transformer层能带来9%额外的提升。这表明预训练模型中的每一层都包含了解决目标问题有用的功能。\",\"零样本表现 最好要弄清楚为什么预训练模型会有效？一种假设是，与LSTMs相比，潜在生成式模型（underlying generative model）在应用到很多任务时可以提高语言建模的能力并且transformer更具结构化的注意力记忆（attentional memory）有助于迁移。在零样本上，LSTM表现高方差，表明在迁移中，Transformer架构导入偏差是有帮助的。\",\"对于CoLA（语言可接受性），样本的得分是用生成模型分配的tokens的平均对数概率，在阈值下进行预测的。\",\"对于SST-2(情感分析)，给每个实例样本加一个 very token,来限制语言模型的输出分布只有 positive和 nagative, 就是猜测被分配到高的概率值的token作为预测值。\",\"对于RACE(问答)，在文档和问题给定条件下，将生成模型分配的平均对数概率高的token作为答案。\",\"对于DPRD(威诺格拉德模式), 用两个可能的替换说法来代替定义的代词，在这之后用生成模型分配的剩下序列的平均token对数概率高的作为结果。\",\"消融研究 不同的消融研究如下表5.\",\"首先，作者在微调时用辅助的LM目标来检查作者模型的性能。在NLI和QQP任务上辅助LM目标有帮助。总之，就是大数据集有效，小数据集没有。\",\"其次，分析比较2048单元的单层LSTM和Transformer，二者都加同样的辅助LM，LSTM会掉5.6平均分数。\",\"最后，直接在监督学习任务上训练，不要预训练，这会导致在跨任务上性能降低14.8%.\"]},\"214\":{\"h\":\"结论\",\"t\":[\"作者介绍了一种框架，用任务未知的生成式预训练模型和判别式微调在自然语言理解上取得了很强的效果。通过在长篇连续文本的多样化语料库上预训练，作者模型获得了重要的世界知识和处理长距离依赖的能力，然后能成功迁移学习解决判别式任务，如问答，语义相似度评估，蕴含确定和文本分类，在12个的9个数据集取得了SOTA结果。\",\"使用无监督预训练来提升在判别式任务上的表现是机器学习研究的长期目标。作者的工作表明，实现显著的性能提升确实是可能的，并给出了提示Transformer类模型和长距离依赖的文本数据集最好用这种方法来训练。\"]},\"215\":{\"h\":\"模型层\"},\"216\":{\"h\":\"从\\\"零\\\"实现 Bert\",\"t\":[\"利用Pytorch从\\\"零\\\"实现Bert\",\"TinyBert 源码链接: https://github.com/BinaryOracle/TinyBert\"]},\"217\":{\"h\":\"Bert 是什么 ？\",\"t\":[\"BERT 全称为 Bidirectional Encoder Representation from Transformer，是 Google 以无监督的方式利用大量无标注文本「炼成」的语言模型，其架构为 Transformer 中的 Encoder（BERT = Encoder of Transformer）。\",\"以往为了解决不同的 NLP 任务，我们会为该任务设计一个最合适的神经网络架构并做训练，不同的 NLP 任务通常需要不同的模型，而设计这些模型并测试其 performance 是非常耗成本的（人力，时间，计算资源）。如果有一个能直接处理各式 NLP 任务的通用架构该有多好？\",\"随着时代演进，不少人很自然地有了这样子的想法，而 BERT 就是其中一个将此概念付诸实践的例子，Google 在预训练 BERT 时让它同时进行两个任务：\",\"漏字填空，即完型填空 (Masked Language Model)\",\"判断第 2 个句子在原始本文中是否跟第 1 个句子相接（Next Sentence Prediction）\"]},\"218\":{\"h\":\"Masked Language Model\",\"t\":[\"在 BERT 中，Masked LM（Masked Language Model）构建了语言模型，简单来说，就是随机遮盖或替换一句话里面的任意字或词，然后让模型通过上下文预测那一个被遮盖或替换的部分，之后做 Loss 的时候也只计算被遮盖部分的 Loss，这其实是一个很容易理解的任务，实际操作如下：\",\"随机把一句话中 15% 的 token（字或词）替换成以下内容：\",\"这些 token 有 80% 的几率被替换成 [MASK]，例如 my dog is hairy→my dog is [MASK]\",\"有 10% 的几率被替换成任意一个其它的 token，例如 my dog is hairy→my dog is apple\",\"有 10% 的几率原封不动，例如 my dog is hairy→my dog is hairy\",\"之后让模型预测和还原被遮盖掉或替换掉的部分，计算损失的时候，只计算在第 1 步里被随机遮盖或替换的部分，其余部分不做损失，其余部分无论输出什么东西，都无所谓。\",\"这样做的好处是，BERT 并不知道 [MASK] 替换的是哪一个词，而且任何一个词都有可能是被替换掉的，比如它看到的 apple 可能是被替换的词。这样强迫模型在编码当前时刻词的时候不能太依赖当前的词，而要考虑它的上下文，甚至根据上下文进行 \\\"纠错\\\"。比如上面的例子中，模型在编码 apple 时，根据上下文 my dog is，应该把 apple 编码成 hairy 的语义而不是 apple 的语义。\"]},\"219\":{\"h\":\"Next Sentence Prediction\",\"t\":[\"我们首先拿到属于上下文的一对句子，也就是两个句子，之后我们要在这两个句子中加一些特殊的 token：[CLS]上一句话[SEP]下一句话[SEP]。也就是在句子开头加一个 [CLS]，在两句话之间和句末加 [SEP]，具体地如下图所示：\",\"可以看到，上图中的两句话明显是连续的。如果现在有这么一句话 [CLS]我的狗很可爱[SEP]企鹅不擅长飞行[SEP]，可见这两句话就不是连续的。\",\"Token Embedding 就是正常的词向量，即 PyTorch 中的 nn.Embedding()\",\"Segment Embedding 的作用是用 embedding 的信息让模型分开上下句，我们给上句的 token 全 0，下句的 token 全 1，让模型得以判断上下句的起止位置，例如:\",\"[CLS]我的狗很可爱[SEP]企鹅不擅长飞行[SEP] 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\",\"Position Embedding 和 Transformer 中的不一样，不是三角函数，而是学习出来的。\"]},\"220\":{\"h\":\"Multi-Task Learning\",\"t\":[\"BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。\"]},\"221\":{\"h\":\"Fine-Tuning\",\"t\":[\"BERT 的 Fine-Tuning 共分为 4 中类型: 文本分类，Token分类，推理任务，问答任务。\",\"如果现在的任务是 文本分类，首先在输入句子的开头加一个代表分类的符号 [CLS]，然后将该位置的 output，丢给 Linear Classifier，让其 predict 一个 class 即可。整个过程中 Linear Classifier 的参数是需要从头开始学习的，而 BERT 中的参数微调就可以了\",\"为什么要用第一个位置，即 [CLS] 位置的 output，个人理解是因为 BERT 内部是 Transformer，而 Transformer 内部又是 Self-Attention，所以 [CLS] 的 output 里面肯定含有整句话的完整信息，这是毋庸置疑的。但是 Self-Attention 向量中，自己和自己的值其实是占大头的，现在假设使用 的 output 做分类，那么这个 output 中实际上会更加看重 ，而 又是一个有实际意义的字或词，这样难免会影响到最终的结果。但是 [CLS] 是没有任何实际意义的，只是一个占位符而已，所以就算 [CLS] 的 output 中自己的值占大头也无所谓。当然你也可以将所有词的 output 进行 concat，作为最终的 output。\",\"如果现在的任务是 Token分类，将句子中各个字对应位置的 output 分别送入不同的 Linear，预测出该字的标签。其实这本质上还是个分类问题，只不过是对每个字都要预测一个类别。\",\"如果现在的任务是 NLI（自然语言推理）。即给定一个前提，然后给出一个假设，模型要判断出这个假设是 正确、错误还是不知道。这本质上是一个三分类的问题，和 Case 1 差不多，对 [CLS] 的 output 进行预测即可\",\"如果现在的任务是 问答任务，举例来说，如上图，将一篇文章，和一个问题（这里的例子比较简单，答案一定会出现在文章中）送入模型中，模型会输出两个数 s,e，这两个数表示，这个问题的答案，落在文章的第 s 个词到第 e 个词。具体流程我们可以看下面这幅图:\",\"首先将问题和文章通过 [SEP] 分隔，送入 BERT 之后，得到上图中黄色的输出。此时我们还要训练两个 vector，即上图中橙色和黄色的向量。首先将橙色和所有的黄色向量进行 dot product，然后通过 softmax，看哪一个输出的值最大，例如上图中 对应的输出概率最大，那我们就认为 s=2。\",\"同样地，我们用蓝色的向量和所有黄色向量进行 dot product，最终预测得 的概率最大，因此 e=3。最终，答案就是 s=2,e=3。\",\"你可能会觉得这里面有个问题，假设最终的输出 s>e 怎么办，那不就矛盾了吗？其实在某些训练集里，有的问题就是没有答案的，因此此时的预测搞不好是对的，就是没有答案。\"]},\"222\":{\"h\":\"从 “零” 开始的预训练\",\"t\":[\"从本节开始，我们将从\\\"零\\\"开始，体验Bert的预训练过程是如何实现的；\"]},\"223\":{\"h\":\"数据清洗\",\"t\":[\"首先我们需要准备一个小型语料库，确保在单台机器上，仅使用CPU就能完成整个训练过程，这里采用的是 wikitext-2 和 wikitext-103 两个开源数据集:\",\"WikiText 英语词库数据（The WikiText Long Term Dependency Language Modeling Dataset）是一个包含1亿个词汇的英文词库数据，这些词汇是从Wikipedia的优质文章和标杆文章中提取得到，包括WikiText-2和WikiText-103两个版本，相比于著名的 Penn Treebank (PTB) 词库中的词汇数量，前者是其2倍，后者是其110倍。每个词汇还同时保留产生该词汇的原始文章，这尤其适合当需要长时依赖(longterm dependency)自然语言建模的场景。\",\"Wikitext-103是超过 1 亿个语句的数据合集，全部从维基百科的 Good 与 Featured 文章中提炼出来。广泛用于语言建模，当中包括 fastai 库和 ULMFiT 算法中经常用到的预训练模型。\",\"WikiText2是Wikitext-103 的子集，主要用于测试小型数据集的语言模型训练效果。\",\"WIKITEXT-2\",\"WIKITEXT-103\",\"下载地址\",\"https://s3.amazonaws.com/fast-ai-nlp/wikitext-103.tgz\",\"https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz\",\"WikiText-2 和 WikiText-103 是两个广泛用于语言模型训练和评估的英文维基百科语料数据集 ，由 Salesforce 提出并开源。它们在 NLP 领域（特别是语言建模、预训练任务）中非常经典。\",\"将数据集压缩包下载到dataset目录下，并解压到当前目录下，然后使用prepare_data文件所提供代码对原始数据格式进行解析，得到对应的JSON格式文件:\",\"相关核心代码实现如下:\",\"def process_csv(file_path): \\\"\\\"\\\"处理CSV文件,返回处理后的句子列表\\\"\\\"\\\" all_sentences = [] with open(file_path, 'r', encoding='utf-8') as f: reader = csv.reader(f) for row in reader: # 使用NLTK库，将一整段文本按“句子”切分成一个句子列表。 # 处理每行文本：去除前后空格，过滤无效行 paragraph = [line.strip() for line in sent_tokenize(row[0]) if line.strip() and not line.strip().startswith('=') and not all(c in string.punctuation for c in line.strip())] # 过滤掉句子数少于2的行 paragraph = [line for line in paragraph if len(line.split('. ')) >= 2] # 确保句子数为偶数 if len(paragraph) % 2 != 0: paragraph = paragraph[:-1] all_sentences.extend(paragraph) return all_sentences def main(): # 处理两个CSV文件 test_sentences = process_csv('wikitext-2/test.csv') train_sentences = process_csv('wikitext-2/train.csv') # 写入JSON文件 train_output_path = 'wikitext-2/train.json' os.makedirs(os.path.dirname(train_output_path), exist_ok=True) with open(train_output_path, 'w', encoding='utf-8') as f: json.dump(train_sentences, f, indent=4, ensure_ascii=False) print(f\\\"成功生成JSON文件: {train_output_path}\\\") test_output_path = 'wikitext-2/test.json' os.makedirs(os.path.dirname(test_output_path), exist_ok=True) with open(test_output_path, 'w', encoding='utf-8') as f: json.dump(test_sentences, f, indent=4, ensure_ascii=False) print(f\\\"成功生成JSON文件: {test_output_path}\\\") if __name__ == \\\"__main__\\\": main()\"]},\"224\":{\"h\":\"分词器实现\",\"t\":[\"分词器的实现较为简单，首先是其初始化方法中需要完成：字典初始化，数据预加载(可挪到其他地方实现)。\",\"class Tokenizer: def __init__(self, vocab_file = None): vocab_data = None if vocab_file is not None: with open(vocab_file, 'r') as f: vocab_data = json.load(f) # 定义字典保存路径 dict_path = 'dataset/vocab_dict.json' # 尝试加载已保存的字典 if os.path.exists(dict_path): with open(dict_path, 'r', encoding='utf-8') as f: saved_dict = json.load(f) self.word2idx = saved_dict['word2idx'] self.idx2word = {int(k): v for k, v in saved_dict['idx2word'].items()} self.vocab_size = len(self.word2idx) else: # 首先加入特殊标记：PAD, CLS, SEP, MASK , UNK , 这些是 BERT 模型中常用的特殊 token self.word2idx = {f'{name}': idx for idx, name in enumerate(['PAD', 'CLS', 'SEP', 'MASK' , 'UNK'])} # 处理vocab_data为列表形式的情况 if isinstance(vocab_data, list): # 将所有文本合并成一个字符串 all_text = ' '.join(vocab_data) # 临时替换特殊标记 ，然后对句子进行分词 temp_text = all_text.replace('<unk>', 'UNK') sentences = word_tokenize(temp_text) # 获取所有单词并去重 word_list = list(set(sentences)) # 给每个普通词分配索引，从4开始（前面是特殊token）, 当前已经有的词数（4个特殊词） hold_place = len(self.word2idx) for idx, word in enumerate(word_list): if word == 'UNK': continue self.word2idx[word] = idx + hold_place else: raise ValueError(\\\"vocab_data must be a list\\\") # 创建反向映射：索引 → 单词 self.idx2word = {idx: word for word, idx in self.word2idx.items()} # 总词汇量 self.vocab_size = len(self.word2idx) # 确保映射是一一对应的 assert len(self.word2idx) == len(self.idx2word) # 保存字典到文件 with open(dict_path, 'w') as f: json.dump({ 'word2idx': self.word2idx, 'idx2word': self.idx2word }, f, indent=4) # 对列表数据进行解析 self.max_len = 103 if isinstance(vocab_data, list): self.word_ids = [] # 两两配对遍历 for i in range(0, len(vocab_data), 2): sent_a = vocab_data[i] sent_b = vocab_data[i+1] tokens_a = self.encode(sent_a) tokens_b = self.encode(sent_b) # 如果任一句子长度超过50，跳过这对 if len(tokens_a) > 50 or len(tokens_b) > 50: continue # 否则保存这两个句子的 token ID 列表 self.word_ids.append(tokens_a) self.word_ids.append(tokens_b)\",\"字典的构建过程太过粗糙，导致最终构建得到的字典过大并且还有很多噪声，从而模型训练学习到每个词的含义需要更大量的数据集且最终效果也不会很好，可考虑换成 HuggingFace 的 BertTokenizer / WordPieceTokenizer 实现。\",\"上面优化方向很多，比如: 去除含有低频词的句对，因为低频词出现次数极少，模型很难学到它们的语义表示。\",\"对外提供的编码和解码两个方法实现如下:\",\" def encode(self, text): return self.tokenize(text) def decode(self, tokens): return self.detokenize(tokens) def tokenize(self, text): sentences = word_tokenize(text) tokens = [] for word in sentences: if word in self.word2idx: tokens.append(self.word2idx[word]) else: # 如果遇到不存在于字典中的word，则使用UNK替换 tokens.append(self.word2idx['UNK']) return tokens def detokenize(self, tokens): return ' '.join([self.idx2word[token] for token in tokens])\",\"实际实现过程中，出于方便，还将一个工具方法整合到了分词器的实现之中，它是用于执行Bert MLM任务掩码策略的方法:\",\" # 执行Bert的掩码策略: 掩码候选位置，输入序列，掩码符号 def masking_procedure(self,cand_pos, input_ids, masked_symb): masked_pos = [] masked_tokens = [] # 对于所有掩码候选位置执行掩码策略： 80% 概率替换为[MASK]，10% 概率替换为随机词，10% 概率保持不变 for pos in cand_pos: masked_pos.append(pos) # 记录被掩码的位置 masked_tokens.append(input_ids[pos]) # 记录被掩码的原token if random.random() < p_mask: # 80% 概率替换为[MASK] input_ids[pos] = masked_symb elif random.random() > (p_mask + p_replace): # 10% 概率替换为随机词 rand_word_idx = random.randint(4, self.vocab_size - 1) input_ids[pos] = rand_word_idx else: # 10% 概率保持不变 pass return masked_pos, masked_tokens\"]},\"225\":{\"h\":\"Batch数据准备\",\"t\":[\"有了分词器后，我们需要读取并构建Batch数据，用于我们的预训练任务，该过程由make_data方法实现，具体步骤为:\",\"收集相同数量的相邻句对和非相邻句对。\",\"对每个句对构建用于NSP任务的样本，形式为: [CLS] + A + [SEP] + B + [SEP]。\",\"对每个句对构建用于MLM任务的样本，首先将[CLS] + A + [SEP] + B + [SEP]句子中20%的词执行掩码策略，而针对这20%需要被掩码的词之上，再按照80%用MASK掩码替换，10%用随机词替换，10%保持原样不动的形式进行处理；最后返回两个列表: 20%执行掩码的词的位置列表，20%执行掩码的词的原Token列表。\",\"将所有输入序列填充到等长max_len。\",\"返回构建得到的单个样本列表: [被掩码后的输入序列, 句子分隔列表 , 20%执行掩码的词的位置列表, 20%执行掩码的词的原Token列表, 是否为连贯的上下句]。\",\"所有样本列表构成Batch数据返回。\",\"def make_data(tokenizer): sentences = tokenizer.word_ids batch_data = [] len_sentences = len(sentences) # Step 1: 收集相邻句对 adjacent_pairs = [] for i in range(len_sentences - 1): a, b = i, i + 1 if len(sentences[a]) <= 50 and len(sentences[b]) <= 50: adjacent_pairs.append((a, b)) # Step 2: 随机生成等量的非相邻句对 non_adjacent_pairs = [] valid_indices = [i for i in range(len_sentences) if len(sentences[i]) <= 50] for a in valid_indices: candidates = [b for b in valid_indices if abs(a - b) > 1] if candidates: b = random.choice(candidates) non_adjacent_pairs.append((a, b)) # 打乱顺序 random.shuffle(adjacent_pairs) random.shuffle(non_adjacent_pairs) # 保证数量一致 min_count = min(len(adjacent_pairs), len(non_adjacent_pairs)) adjacent_pairs = adjacent_pairs[:min_count] non_adjacent_pairs = non_adjacent_pairs[:min_count] # 构建样本 for a, b in adjacent_pairs: sample = prepare_sample(tokenizer, a, b, is_next=True) batch_data.append(sample) for a, b in non_adjacent_pairs: sample = prepare_sample(tokenizer, a, b, is_next=False) batch_data.append(sample) return batch_data def prepare_sample(tokenizer, tokens_a_idx, tokens_b_idx, is_next): sentences = tokenizer.word_ids tokens_a = sentences[tokens_a_idx] tokens_b = sentences[tokens_b_idx] # 拼接 [CLS] + A + [SEP] + B + [SEP] input_ids = [tokenizer.word2idx['CLS']] + tokens_a + [tokenizer.word2idx['SEP']] + tokens_b + [ tokenizer.word2idx['SEP']] segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (1 + len(tokens_b)) # MLM 准备 n_pred = min(max_pred, max(1, int(len(input_ids) * 0.2))) cand_pos = [ i for i, token in enumerate(input_ids) if token not in {tokenizer.word2idx['CLS'], tokenizer.word2idx['SEP'], tokenizer.word2idx['PAD'], tokenizer.word2idx['UNK']} ] random.shuffle(cand_pos) masked_pos, masked_tokens = tokenizer.masking_procedure(cand_pos[:n_pred], input_ids, tokenizer.word2idx['MASK']) # Padding def pad(seq, target_len, pad_value=tokenizer.word2idx['PAD']): seq += [pad_value] * (target_len - len(seq)) pad(input_ids, tokenizer.max_len) pad(segment_ids, tokenizer.max_len) if max_pred > n_pred: pad(masked_pos, max_pred) pad(masked_tokens, max_pred) return [input_ids, segment_ids, masked_tokens, masked_pos, is_next]\"]},\"226\":{\"h\":\"模型\",\"t\":[\"本文中的 Bert 模型整体实现也比较简单，其中关于BertEncoders编码并输出结果的整个过程如下图所示:\",\"NSP 任务会利用 CLS Token 作为整个输入序列的全局信息聚合表示，再经过非线性变换后，进行二分类任务，判断下一个句子是否是当前句子的后续句子，具体过程如下图所示:\",\"MLM 任务会利用 masked_pos 从BertEncoders编码输出结果中提取出被掩码的位置对应的嵌入向量，经过相同的非线性变换后，将这些掩码Token对应的嵌入向量映射到词向量空间中去，得到模型预测的这些掩码Token对应的真实词，具体过程如下图所示:\",\"核心代码实现如下:\",\"class BERT(nn.Module): def __init__(self, n_layers, vocab_size, max_len): \\\"\\\"\\\" 初始化一个简化版的 BERT 模型，支持 MLM（掩码语言建模） 和 NSP（下一句预测） 两个任务。 参数： n_layers: Transformer 编码器层数 vocab_size: 词表大小 max_len: 最大序列长度 \\\"\\\"\\\" super(BERT, self).__init__() # 1. 词嵌入 + 位置嵌入 + 句子嵌入 self.embedding = Embeddings(vocab_size, max_len) # 2. 多个 Transformer 编码器层堆叠 self.encoders = nn.ModuleList([ EncoderLayer() for _ in range(n_layers) ]) # 3. Pooler 层：用于提取 [CLS] token 的表示，用于 NSP 任务 self.pooler = Pooler() # 4. 下一句预测（NSP）分类器 self.next_cls = nn.Linear(d_model, 2) # 输出维度为 2，表示是否是连续句子 self.gelu = gelu # GELU 激活函数 # 5. 权重共享：Pooler 层与 FC 层共享权重 shared_weight = self.pooler.fc.weight # 获取 pooler 中的全连接层权重 self.fc = nn.Linear(d_model, d_model) # 创建新的线性层 self.fc.weight = shared_weight # 共享权重（weight tying） # 6. 权重共享：MLM 分类器共享词嵌入矩阵 shared_weight = self.embedding.word_emb.weight # 获取词嵌入层权重 self.word_classifier = nn.Linear(d_model, vocab_size, bias=False) self.word_classifier.weight = shared_weight # 权重共享（tie weights） def forward(self, tokens, segments, masked_pos): \\\"\\\"\\\" 前向传播逻辑 输入： tokens: [batch_size, seq_len]，token 的索引（已添加 [CLS], [SEP], [MASK] 等） segments: [batch_size, seq_len]，segment_id，区分句子 A 和 B masked_pos: [batch_size, max_pred]，记录被掩码的位置 输出： logits_cls: [batch_size, 2]，NSP 分类结果 logits_lm: [batch_size, max_pred, vocab_size]，MLM 预测结果 \\\"\\\"\\\" # 1. 词嵌入 + 位置嵌入 + 句子嵌入 output = self.embedding(tokens, segments) # shape: [batch_size, seq_len, d_model] # 2. 构造 padding mask（忽略填充部分） enc_self_pad_mask = get_pad_mask(tokens) # shape: [batch_size, seq_len, seq_len] # 3. 依次通过每个编码器层（Transformer Layer） for layer in self.encoders: output = layer(output, enc_self_pad_mask) # output shape: [batch_size, seq_len, d_model] # 4. NSP 任务：使用 [CLS] 标记进行下一句预测 hidden_pool = self.pooler(output[:, 0]) # 提取 [CLS] 位置的隐藏状态并池化 logits_cls = self.next_cls(hidden_pool) # 分类输出：[batch_size, 2] # 5. MLM 任务：恢复被掩码的词 # masked_pos: [batch_size, max_pred] # 扩展 masked_pos 到三维，便于从 output 中 gather 出被掩码位置的表示 masked_pos = masked_pos.unsqueeze(-1).expand(-1, -1, d_model) # shape: [batch, max_pred, d_model] # 使用 torch.gather 从 output 中取出被掩码位置的 token 表示 h_masked = torch.gather(output, dim=1, index=masked_pos) # shape: [batch_size, max_pred, d_model] # 通过全连接层 + GELU 激活函数 h_masked = self.gather(output, dim=1, index=masked_pos) # 再次提取被掩码位置的表示 h_masked = self.gelu(self.fc(h_masked)) # shape: [batch_size, max_pred, d_model] # 6. MLM 分类器：预测被掩码的词 logits_lm = self.word_classifier(h_masked) # shape: [batch_size, max_pred, vocab_size] # 返回两个任务的结果 return logits_cls, logits_lm\",\"完整的代码实现部分，大家参考仓库源码即可，本文不再全部Copy展示。\"]},\"227\":{\"h\":\"训练\",\"t\":[\"训练过程就比较常规了，有一点不同就是Bert预训练阶段的学习目标是: MLM Loss + NSP Loss ，具体核心代码实现如下:\",\"tokenizer = Tokenizer(\\\"dataset/wikitext-2/train.json\\\") batch_data = make_data(tokenizer) batch_tensor = [torch.LongTensor(ele) for ele in zip(*batch_data)] dataset = BERTDataset(*batch_tensor) dataloader = DataLoader(dataset, batch_size=32, shuffle=True) model = BERT(n_layers,tokenizer.vocab_size,tokenizer.max_len) lr = 1e-4 epochs = 100 # 优化器与学习率调度器 optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01) # 损失函数 + 标签平滑 criterion1 = nn.CrossEntropyLoss(label_smoothing=0.1) criterion2 = nn.CrossEntropyLoss(label_smoothing=0.1,ignore_index=0) # 加载检查点 checkpoint_path = 'best_model.pth' if os.path.exists(checkpoint_path): model.load_state_dict(torch.load(checkpoint_path, weights_only=True, map_location=device)) print('Loaded checkpoint from', checkpoint_path) model.to(device) best_loss = float('inf') # training total_batches = len(dataloader) for epoch in range(epochs): avg_loss = 0 for batch_idx, one_batch in enumerate(dataloader): input_ids, segment_ids, masked_tokens, masked_pos, is_next = [ele.to(device) for ele in one_batch] logits_cls, logits_lm = model(input_ids, segment_ids, masked_pos) # NSP 任务损失 loss_cls = criterion1(logits_cls, is_next) # MLM 任务损失 loss_lm = criterion2(logits_lm.view(-1, tokenizer.vocab_size), masked_tokens.view(-1)) loss_lm = (loss_lm.float()).mean() # 总损失 loss = loss_cls + loss_lm avg_loss += loss.item() if (epoch + 1) % 1 == 0: print(f'Epoch:{epoch + 1} Batch:{batch_idx + 1}/{total_batches} \\\\t loss: {loss:.6f}') loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) optimizer.step() avg_loss /= total_batches # 保存最优模型 if avg_loss < best_loss: best_loss = avg_loss torch.save(model.state_dict(), f'best_model.pth') print(f'Saved best model with loss: {best_loss:.6f}') # 效果评估 evaluate_model()\",\"由于模型输出的logits_cls是一个二分类值，因此我们只需要根据is_next取出索引0或者1下标对应的值即可知道我们是否预测正确，并且使用预测结果计算NSP任务损失值。\",\"对于MLM任务损失计算来说，我们只会计算被随机遮盖或替换的部分，其余部分不做损失，因此模型返回的logits_lm也只包含被掩码的Token对应的模型预测真实词，同时通过masked_tokens可知这些被掩码Token对应的真实词作为Label，从而计算交叉熵损失就很简单了。\",\"这里需要注意一点，对于MLM任务损失计算来说，我们需要在其对应的CrossEntropyLoss中指定ignore_index=0，即忽略掉PAD部分的损失计算；\",\"这里PAD部分指的是对于不同的句子，它们都是按照其序列长度的20%比例进行的掩码，而对于较短的句子，其掩码数量可能会偏少，因此为了确保masked_tokens列表中所有句子掩码数量一致，需要对掩码数量不足max_pred的进行PAD填充。\",\"模型返回的logits_lm中同样含有PAD部分，但是我们在计算损失时指定了ignore_index=0，即忽略掉PAD部分的损失计算，因此不会影响最终的损失值计算。\",\"gather函数比较灵活，它可以在指定维度上，根据索引矩阵，从源张量中提取特定位置的元素，构造一个新的张量。\",\"对于每一个输出位置 (i,j)，如果 dim=1（列方向），那么它从 input [index[i][j]][j] 中取值。\",\"对于每一个输出位置 (i,j)，如果 dim=0（列方向），那么它从 input [i][index[i][j]] 中取值。\"]},\"228\":{\"h\":\"效果\",\"t\":[\"本文所展示的Bert预训练属于教学级别的，最终的训练效果也是一般，仅供参考和学习:\",\"MLM Task: Correct / Total = 3167 / 9027 | Accuracy = 0.3508 (预测正确的掩码词数量/总掩码的词数量)\",\"NSP Task: Correct / Total = 504 / 960 | Accuracy = 0.5250 (预测正确的句对数量/总句对数量)\"]},\"229\":{\"h\":\"Details\",\"t\":[\"本节将会对Bert模型实现的部分细节进行说明。\"]},\"230\":{\"h\":\"Padding Mask 如何生成并起作用的 ？\",\"t\":[\"首先模型会根据传入的Tokens列表生成一个Pad Mask矩阵，该 矩阵维度 和 Q@K.T 后得到的注意力得分矩阵维度相同\",\"def get_pad_mask(tokens, pad_idx=0): ''' suppose index of [PAD] is zero in word2idx tokens: [batch, seq_len] ''' batch, seq_len = tokens.size() pad_mask = tokens.data.eq(pad_idx).unsqueeze(1) # （batch,1,seq_len) pad_mask = pad_mask.expand(batch, seq_len, seq_len) # （batch,seq_len,seq_len) return pad_mask\",\"假设输入的Token序列为: [A,B,C,PAD,PAD,PAD] , 则生成的Pad Mask模样为:\",\"在注意力得分矩阵计算完毕后，我们会使用Pad Mask矩阵将注意力得分矩阵中对应位置的得分设置为一个非常小的值，这样在后续的Softmax计算中，这些位置的概率就会接近0，从而在注意力机制中就不会考虑到这些PAD部分的Token了。\",\"class ScaledDotProductAttention(nn.Module): def forward(self, Q, K, V, attn_mask): scores = torch.matmul(Q, K.transpose(-1, -2) / msqrt(d_k)) # scores: [batch, n_heads, seq_len, seq_len] scores.masked_fill_(attn_mask, -1e9) attn = nn.Softmax(dim=-1)(scores) # context: [batch, n_heads, seq_len, d_v] context = torch.matmul(attn, V) return context\",\"横着看是计算某个词与全局序列中其他词的相关度，后续需要利用该相关度完成当前词的全局上下文信息融合，我们只需要确保对于某个词的上下文融合不被PAD词参与即可，而无需考虑PAD词的全局上下文信息是否需要进行计算。\"]},\"231\":{\"h\":\"图解 Bert\",\"t\":[\"图解Bert & Bert文本分类实战\"]},\"232\":{\"h\":\"环境搭建\",\"t\":[\"按序执行以下命令完成环境搭建:\",\"git clone https://github.com/DA-southampton/Read_Bert_Code.git cd Read_Bert_Code conda create -n Read_Bert_Code python=3.9.22 conda activate Read_Bert_Code\",\"本文使用的是谷歌的中文预训练模型：chinese_L-12_H-768_A-12.zip，模型有点大，我就不上传了，如果本地不存在，就点击这里直接下载,或者直接命令行运行\",\"wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\",\"预训练模型下载下来之后，进行解压，然后将tf模型转为对应的pytorch版本即可。对应代码如下:\",\"export BERT_BASE_DIR=/Users/zhandaohong/Read_Bert_Code/chinese_L-12_H-768_A-12 python convert_tf_checkpoint_to_pytorch.py \\\\ --tf_checkpoint_path$BERT_BASE_DIR/bert_model.ckpt \\\\ --bert_config_file$BERT_BASE_DIR/bert_config.json \\\\ --pytorch_dump_path$BERT_BASE_DIR/pytorch_model.bin\",\"转化成功之后，将模型放入到仓库对应位置：\",\"Read_Bert_Code/bert_read_step_to_step/prev_trained_model/\",\"并重新命名为：\",\" bert-base-chinese\",\"其次是准备训练数据，这里我准备做一个文本分类任务，使用的是Tnews数据集，这个数据集来源是这里，分为训练，测试和开发集，我已经上传到了仓库中，具体位置在\",\"Read_Bert_Code/bert_read_step_to_step/chineseGLUEdatasets/tnews\",\"需要注意的一点是，因为我只是为了了解内部代码情况，所以准确度不是在我的考虑范围之内，所以我只是取其中的一部分数据，其中训练数据使用1k，测试数据使用1k，开发数据1k。\",\"准备就绪，使用pycharm导入项目，准备调试，我的调试文件是run_classifier.py文件，对应的参数为\",\"--model_type=bert --model_name_or_path=prev_trained_model/bert-base-chinese --task_name=\\\"tnews\\\" --do_train --do_eval --do_lower_case --data_dir=./chineseGLUEdatasets/tnews --max_seq_length=128 --per_gpu_train_batch_size=16 --per_gpu_eval_batch_size=16 --learning_rate=2e-5 --num_train_epochs=4.0 --logging_steps=100 --save_steps=100 --output_dir=./outputs/tnews_output/ --overwrite_output_dir\",\"然后启动 run_classifier.py 文件进行调试即可 , 所参考源仓库未提供requirements.txt文件，因此需要大家自行完成运行时缺失依赖包的安装。\"]},\"233\":{\"h\":\"数据预处理\",\"t\":[\"输入数据格式\",\"{ \\\"guid\\\": \\\"train-0\\\", \\\"label\\\": \\\"104\\\", // 文本分类任务: 文本对应的标签 \\\"text_a\\\": \\\"股票中的突破形态\\\", \\\"text_b\\\": null // NSP任务: 用于判断给出的两个句子是否连续 }\",\"NSP (Next Sentence Prediction)\",\"文本分词 & 借助字典映射为word id\",\"\\\"股票中的突破形态\\\" --> ['股', '票', '中', '的', '突', '破', '形', '态'] --> [5500, 4873, 704, 4638, 4960, 4788, 2501, 2578]\",\"对于字典中不存在的词 , 用 [UNK] 表示, 对应的id为 100\",\"过长截断策略\",\"添加特殊Token标记\",\"原序列添加特殊Token标记图\",\"[101, 5500, 4873, 704, 4638, 4960, 4788, 2501, 2578, 102]\",\"BertTokenizer中的特殊token id:\",\"[CLS]: 101\",\"[SEP]: 102\",\"[MASK]: 103\",\"[UNK]: 100\",\"[PAD]: 0\",\" # BertTokenizer def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None): if token_ids_1 is None: return [self.cls_token_id] + token_ids_0 + [self.sep_token_id] cls = [self.cls_token_id] sep = [self.sep_token_id] return cls + token_ids_0 + sep + token_ids_1 + sep\",\"创建句子辨识列表，用以区分不同的句子\",\"token_type_ids作用图解\",\" # BertTokenizer def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None): \\\"\\\"\\\" Creates a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence pair mask has the following format: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 | first sequence | second sequence if token_ids_1 is None, only returns the first portion of the mask (0's). \\\"\\\"\\\" sep = [self.sep_token_id] cls = [self.cls_token_id] if token_ids_1 is None: return len(cls + token_ids_0 + sep) * [0] return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\",\"创建用以区分special tokens部分的mask列表\",\"special_tokens_mask作用图解\",\" # BertTokenizer def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False): if token_ids_1 is not None: return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1] return [1] + ([0] * len(token_ids_0)) + [1]\",\"超长截断\",\" # PreTrainedTokenizer if max_length and len(encoded_inputs[\\\"input_ids\\\"]) > max_length: encoded_inputs[\\\"input_ids\\\"] = encoded_inputs[\\\"input_ids\\\"][:max_length] encoded_inputs[\\\"token_type_ids\\\"] = encoded_inputs[\\\"token_type_ids\\\"][:max_length] encoded_inputs[\\\"special_tokens_mask\\\"] = encoded_inputs[\\\"special_tokens_mask\\\"][:max_length]\",\"生成padding部分的mask列表\",\"attention_mask作用图解\",\" # 生成注意力掩码，真实token对应1，填充token对应0 attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\",\"所有序列都填充到max_length长度,不足长度用padding填充\",\"填充过程图\",\" # 记录输入长度 input_len = len(input_ids) # 计算需要填充的长度 --- 所有输入序列等长，都等于max_length padding_length = max_length - len(input_ids) # 右填充 input_ids = input_ids + ([pad_token] * padding_length) attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length) token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\",\"数据集中每一个样本最终都会解析得到一个InputFeatures\",\"InputFeatures组成图解\",\"features.append( InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=label, input_len=input_len))\",\"label 是当前文本对应的类别标签 input_len 是序列实际长度(含special tokens)\",\"数据集预处理完后，将InputFeatures List列表组装起来得到需要的DataSet\",\"dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_lens,all_labels)\"]},\"234\":{\"h\":\"模型架构\"},\"235\":{\"h\":\"DataLoader\",\"t\":[\" train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset) train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size,collate_fn=collate_fn)\",\"DataLoader 设置的回调方法cllote_fn负责对返回的一个batch，在返回前进行预处理:\",\"def collate_fn(batch): all_input_ids, all_attention_mask, all_token_type_ids, all_lens, all_labels = map(torch.stack, zip(*batch)) max_len = max(all_lens).item() # 计算当前批次中所有序列的实际最大长度 all_input_ids = all_input_ids[:, :max_len] # 按照本批次序列中最大长度进行截断: max_length --> max_len all_attention_mask = all_attention_mask[:, :max_len] all_token_type_ids = all_token_type_ids[:, :max_len] return all_input_ids, all_attention_mask, all_token_type_ids, all_labels\"]},\"236\":{\"h\":\"BertEmbeddings\",\"t\":[\"input embeddings = token embeddings + segmentation embeddings + position embeddings\",\"class BertEmbeddings(nn.Module): def __init__(self, config): super(BertEmbeddings, self).__init__() self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0) self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size) self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, input_ids, token_type_ids=None, position_ids=None): seq_length = input_ids.size(1) if position_ids is None: # 为当前批次中的每个序列样本生成一个位置序列: (1,2,3,4,5,...) , 构成一个位置序列矩阵 position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) position_ids = position_ids.unsqueeze(0).expand_as(input_ids) if token_type_ids is None: token_type_ids = torch.zeros_like(input_ids) words_embeddings = self.word_embeddings(input_ids) position_embeddings = self.position_embeddings(position_ids) # 位置编码为可学习的矩阵 token_type_embeddings = self.token_type_embeddings(token_type_ids) # 让模型自己学会区分不同的句子 embeddings = words_embeddings + position_embeddings + token_type_embeddings embeddings = self.LayerNorm(embeddings) embeddings = self.dropout(embeddings) return embeddings\",\"嵌入向量生成过程图\"]},\"237\":{\"h\":\"BertEncoder\"},\"238\":{\"h\":\"BertLayer\",\"t\":[\"BertLayer模型结构图\",\"class BertIntermediate(nn.Module): def __init__(self, config): super(BertIntermediate, self).__init__() self.dense = nn.Linear(config.hidden_size, config.intermediate_size) # (768,3072) # 激活函数 - GLEU if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)): self.intermediate_act_fn = ACT2FN[config.hidden_act] else: self.intermediate_act_fn = config.hidden_act def forward(self, hidden_states): hidden_states = self.dense(hidden_states) hidden_states = self.intermediate_act_fn(hidden_states) # 激活函数 - GLEU return hidden_states class BertOutput(nn.Module): def __init__(self, config): super(BertOutput, self).__init__() self.dense = nn.Linear(config.intermediate_size, config.hidden_size) # (3072,768) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, hidden_states, input_tensor): hidden_states = self.dense(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.LayerNorm(hidden_states + input_tensor) return hidden_states class BertLayer(nn.Module): def __init__(self, config): super(BertLayer, self).__init__() self.attention = BertAttention(config) self.intermediate = BertIntermediate(config) self.output = BertOutput(config) def forward(self, hidden_states, attention_mask=None): attention_output = self.attention(hidden_states, attention_mask) intermediate_output = self.intermediate(attention_output) layer_output = self.output(intermediate_output, attention_output) return layer_output\"]},\"239\":{\"h\":\"BertEncoder\",\"t\":[\"BertEncoder模型结构图\",\"class BertEncoder(nn.Module): def __init__(self, config): super(BertEncoder, self).__init__() self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)]) def forward(self, hidden_states, attention_mask=None, head_mask=None): for i, layer_module in enumerate(self.layer): hidden_states = layer_module(hidden_states, attention_mask, head_mask[i]) return hidden_states\"]},\"240\":{\"h\":\"BertPooler\",\"t\":[\"BertPooler模型结构图\",\"class BertPooler(nn.Module): def __init__(self, config): super(BertPooler, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) self.activation = nn.Tanh() def forward(self, hidden_states): # We \\\"pool\\\" the model by simply taking the hidden state corresponding # to the first token. first_token_tensor = hidden_states[:, 0] # CLS Token Context Embeddings pooled_output = self.dense(first_token_tensor) pooled_output = self.activation(pooled_output) return pooled_output\"]},\"241\":{\"h\":\"BertModel\",\"t\":[\"BertModel模型结构图\",\"class BertModel(BertPreTrainedModel): def __init__(self, config): super(BertModel, self).__init__(config) self.embeddings = BertEmbeddings(config) self.encoder = BertEncoder(config) self.pooler = BertPooler(config) self.init_weights() def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None): extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2) extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0 embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids) sequence_output = self.encoder(embedding_output, extended_attention_mask, # padding mask ) pooled_output = self.pooler(sequence_output) outputs = (sequence_output, pooled_output,) return outputs\"]},\"242\":{\"h\":\"BertForSequenceClassification\",\"t\":[\"BertForSequenceClassification模型结构图\",\"class BertForSequenceClassification(BertPreTrainedModel): def __init__(self, config): super(BertForSequenceClassification, self).__init__(config) self.num_labels = config.num_labels self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, self.config.num_labels) self.init_weights() def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None): outputs = self.bert(input_ids, attention_mask=attention_mask, # padding mask token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) # None ? pooled_output = outputs[1] # 对于分类任务来说，只需要去除CLS Token用于分类任务即可 pooled_output = self.dropout(pooled_output) logits = self.classifier(pooled_output) outputs = (logits,) + outputs[2:] # add hidden states and attention if they are here if labels is not None: if self.num_labels == 1: # We are doing regression loss_fct = MSELoss() loss = loss_fct(logits.view(-1), labels.view(-1)) else: loss_fct = CrossEntropyLoss() loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) outputs = (loss,) + outputs return outputs # (loss), logits, (hidden_states), (attentions)\"]},\"243\":{\"h\":\"BertAttention\"},\"244\":{\"h\":\"BertSelfAttention\",\"t\":[\"多头自注意力计算流程图\",\"class BertSelfAttention(nn.Module): def __init__(self, config): super(BertSelfAttention, self).__init__() self.output_attentions = config.output_attentions self.num_attention_heads = config.num_attention_heads self.attention_head_size = int(config.hidden_size / config.num_attention_heads) self.all_head_size = self.num_attention_heads * self.attention_head_size self.query = nn.Linear(config.hidden_size, self.all_head_size) self.key = nn.Linear(config.hidden_size, self.all_head_size) self.value = nn.Linear(config.hidden_size, self.all_head_size) self.dropout = nn.Dropout(config.attention_probs_dropout_prob) def transpose_for_scores(self, x): new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size) x = x.view(*new_x_shape) return x.permute(0, 2, 1, 3) def forward(self, hidden_states, attention_mask=None, head_mask=None): mixed_query_layer = self.query(hidden_states) mixed_key_layer = self.key(hidden_states) mixed_value_layer = self.value(hidden_states) # view 成多头格式: (batch,heads,seq_len,d_k) query_layer = self.transpose_for_scores(mixed_query_layer) key_layer = self.transpose_for_scores(mixed_key_layer) value_layer = self.transpose_for_scores(mixed_value_layer) # Take the dot product between \\\"query\\\" and \\\"key\\\" to get the raw attention scores. attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) # (batch,heads,d_k,seq_len) attention_scores = attention_scores / math.sqrt(self.attention_head_size) if attention_mask is not None: # Apply the attention mask is (precomputed for all layers in BertModel forward() function) attention_scores = attention_scores + attention_mask # Normalize the attention scores to probabilities. attention_probs = nn.Softmax(dim=-1)(attention_scores) # This is actually dropping out entire tokens to attend to, which might # seem a bit unusual, but is taken from the original Transformer paper. attention_probs = self.dropout(attention_probs) context_layer = torch.matmul(attention_probs, value_layer) context_layer = context_layer.permute(0, 2, 1, 3).contiguous() new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) context_layer = context_layer.view(*new_context_layer_shape) # 合并头结果 return context_layer\"]},\"245\":{\"h\":\"BertSelfOutput\",\"t\":[\"BertSelfOutput计算流程图\",\"class BertSelfOutput(nn.Module): def __init__(self, config): super(BertSelfOutput, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) # 残差链接 + 层归一化 def forward(self, hidden_states, input_tensor): hidden_states = self.dense(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.LayerNorm(hidden_states + input_tensor) return hidden_states\"]},\"246\":{\"h\":\"BertAttention\",\"t\":[\"BertAttention计算流程图\",\"class BertAttention(nn.Module): def __init__(self, config): super(BertAttention, self).__init__() self.self = BertSelfAttention(config) self.output = BertSelfOutput(config) def forward(self, input_tensor, attention_mask=None): self_outputs = self.self(input_tensor, attention_mask) # 多头自注意力机制 attention_output = self.output(self_outputs, input_tensor) return attention_output\"]},\"247\":{\"h\":\"预训练\",\"t\":[\"预训练与微调\"]},\"248\":{\"h\":\"BertPredictionHeadTransform\",\"t\":[\"BertPredictionHeadTransform结构图\",\"class BertPredictionHeadTransform(nn.Module): def __init__(self, config): super(BertPredictionHeadTransform, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)): self.transform_act_fn = ACT2FN[config.hidden_act] else: self.transform_act_fn = config.hidden_act self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) def forward(self, hidden_states): hidden_states = self.dense(hidden_states) hidden_states = self.transform_act_fn(hidden_states) hidden_states = self.LayerNorm(hidden_states) return hidden_states\"]},\"249\":{\"h\":\"BertLMPredictionHead\",\"t\":[\"BertLMPredictionHead结构图\",\"class BertLMPredictionHead(nn.Module): def __init__(self, config): super(BertLMPredictionHead, self).__init__() self.transform = BertPredictionHeadTransform(config) # The output weights are the same as the input embeddings, but there is # an output-only bias for each token. self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False) self.bias = nn.Parameter(torch.zeros(config.vocab_size)) def forward(self, hidden_states): hidden_states = self.transform(hidden_states) hidden_states = self.decoder(hidden_states) + self.bias return hidden_states\"]},\"250\":{\"h\":\"BertPreTrainingHeads\",\"t\":[\"BertPreTrainingHeads结构图\",\"class BertPreTrainingHeads(nn.Module): def __init__(self, config): super(BertPreTrainingHeads, self).__init__() self.predictions = BertLMPredictionHead(config) self.seq_relationship = nn.Linear(config.hidden_size, 2) def forward(self, sequence_output, pooled_output): prediction_scores = self.predictions(sequence_output) # seq_relationship_score = self.seq_relationship(pooled_output) # 两个句子是否为上下句关系 return prediction_scores, seq_relationship_score\"]},\"251\":{\"h\":\"BertForPreTraining\",\"t\":[\"BertForPreTraining结构图\",\"class BertForPreTraining(BertPreTrainedModel): def __init__(self, config): super(BertForPreTraining, self).__init__(config) self.bert = BertModel(config) self.cls = BertPreTrainingHeads(config) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, masked_lm_labels=None, next_sentence_label=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) sequence_output, pooled_output = outputs[:2] # 隐藏层输出,CLS Token Embeddings prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output) outputs = (prediction_scores, seq_relationship_score,) # 计算掩码语言损失 和 下一个句子预测损失 if masked_lm_labels is not None and next_sentence_label is not None: loss_fct = CrossEntropyLoss(ignore_index=-1) masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1)) next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1)) total_loss = masked_lm_loss + next_sentence_loss outputs = (total_loss,) + outputs return outputs # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)\"]},\"252\":{\"h\":\"其他下游任务\",\"t\":[\"Bert支持的下游任务图\"]},\"253\":{\"h\":\"问答任务\",\"t\":[\"在 BERT 的问答任务中，典型的输入是一个包含 问题（Question） 和 上下文（Context） 的文本对。例如：\",\"问题: “谁写了《哈姆雷特》？”上下文: “莎士比亚是英国文学史上最伟大的作家之一，他写了包括《哈姆雷特》、《麦克白》等著名悲剧。”\",\"输入格式（Tokenization 后的形式），在使用 BertTokenizer 编码后，输入会变成如下结构：\",\"[CLS] 问题 tokens [SEP] 上下文 tokens [SEP]\",\"BERT 的输出（Outputs），通过调用 self.bert(...)，你将得到一个包含多个元素的 tuple 输出：\",\"outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\",\"返回值形如：\",\"( sequence_output, # (batch_size, seq_length, hidden_size) pooled_output, # (batch_size, hidden_size) )\",\"主要输出项解释:\",\"✅ sequence_output: 最终每个 token 的表示\",\"形状：(batch_size, seq_length, hidden_size)\",\"是模型最后一层所有 token（包括问题和上下文）的隐藏状态。\",\"在问答任务中，我们主要使用它来预测答案的起始和结束位置。\",\"✅ pooled_output: 句子级别表示（不常用）\",\"形状：(batch_size, hidden_size)\",\"是 [CLS] token 经过一层全连接后的输出。\",\"在分类任务中更有用，在问答任务中一般不会使用这个输出。\",\"如何利用 BERT 输出做问答预测？\",\"在 BertForQuestionAnswering 中，使用了如下逻辑：\",\"logits = self.qa_outputs(sequence_output) # (batch_size, seq_length, 2) start_logits, end_logits = logits.split(1, dim=-1) # split into start and end start_logits = start_logits.squeeze(-1) # (batch_size, seq_length) end_logits = end_logits.squeeze(-1)\",\"qa_outputs 层的作用：\",\"是一个线性层：nn.Linear(config.hidden_size, 2)\",\"将每个 token 的 hidden_size 向量映射成两个分数：一个是该 token 作为答案开始的可能性，另一个是作为答案结束的可能性。\",\"输出解释：\",\"start_logits: 每个 token 是答案起点的得分（未归一化）。\",\"end_logits: 每个 token 是答案终点的得分。\",\"比如对于一个长度为 128 的序列，每个 token 都有一个对应的 start/end 分数：\",\"start_scores = torch.softmax(start_logits, dim=-1) # softmax 得到概率 end_scores = torch.softmax(end_logits, dim=-1) # 找出最可能是 start 和 end 的位置 start_index = torch.argmax(start_scores) end_index = torch.argmax(end_scores)\",\"如果 start_index <= end_index，那么可以组合这两个索引得到答案 span。\"]},\"254\":{\"h\":\"代码实现\",\"t\":[\"class BertForQuestionAnswering(BertPreTrainedModel): def __init__(self, config): super(BertForQuestionAnswering, self).__init__(config) self.num_labels = config.num_labels # 通常是 2，即 start 和 end self.bert = BertModel(config) self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, start_positions=None, end_positions=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids) sequence_output = outputs[0] # (batch,seq_len,hidden_size) ---> (batch,seq_len,2) logits = self.qa_outputs(sequence_output) start_logits, end_logits = logits.split(1, dim=-1) start_logits = start_logits.squeeze(-1) # (batch,seq_len) end_logits = end_logits.squeeze(-1) outputs = (start_logits, end_logits,) # 计算交叉熵损失 if start_positions is not None and end_positions is not None: # sometimes the start/end positions are outside our model inputs, we ignore these terms # ignored_index = seq_len ignored_index = start_logits.size(1) # clamp_ 是 PyTorch 中的一个方法，用于将张量中的值限制在指定的范围内。 # 它的语法是 tensor.clamp_(min, max) ，表示将张量中的值限制在 min 和 max 之间。 # 如果值小于 min ，则将其设置为 min ；如果值大于 max ，则将其设置为 max 。 start_positions.clamp_(0, ignored_index) end_positions.clamp_(0, ignored_index) # ignore_index: 用于指定在计算损失时忽略的标签索引。 loss_fct = CrossEntropyLoss(ignore_index=ignored_index) # 分别计算答案起始下标和结束下标预测得到的交叉熵损失 start_loss = loss_fct(start_logits, start_positions) end_loss = loss_fct(end_logits, end_positions) total_loss = (start_loss + end_loss) / 2 outputs = (total_loss,) + outputs return outputs # (loss), start_logits, end_logits\"]},\"255\":{\"h\":\"易混淆\",\"t\":[\"BERT 是一个 基于上下文编码（Contextual Encoder） 的模型，不是自回归生成器。它不会“生成”新的文本，而是对输入文本中每个 token 的角色进行分类（如判断哪个是答案的开始、结束）。所以最终的答案只能来自原始输入文本中的某一段子串。\",\"📚 详细解释\",\"✅ BERT 是一个 Encoder-only 模型\",\"BERT 只包含 Transformer 的 encoder 部分。\",\"它的作用是给定一个完整的句子（或两个句子），对每个 token 生成一个上下文相关的表示（contextualized representation）。\",\"它不具有生成能力，不能像 GPT 这样的 decoder-only 模型那样逐词生成新内容。\",\"🔍 QA 任务的本质：定位答案 span 而非生成答案\",\"在 SQuAD 这类抽取式问答任务中：\",\"答案必须是原文中的连续片段（span）。\",\"所以模型的任务是：\",\"给出问题和上下文；\",\"在上下文中找到最可能的答案起始位置和结束位置；\",\"最终答案就是上下文中这两个位置之间的字符串。\",\"BERT 做的就是这个定位任务，而不是重新生成一个新的答案。\",\"🧩 输入与输出的关系\",\"answer_tokens = input_ids[0][start_index : end_index + 1] answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\",\"这段代码的意思是：\",\"start_index 和 end_index 是模型预测出的答案的起始和结束位置。\",\"我们从原始输入的 input_ids 中取出对应的 token ID 子序列。\",\"使用 tokenizer 把这些 token ID 解码成自然语言文本。\",\"得到的就是答案。\",\"这其实就是在说：\",\"“根据你的理解，答案应该在这段文字中的第 X 到第 Y 个词之间，请把这部分原文告诉我。”\",\"🧪 举个例子\",\"假设原始上下文是：\",\"The capital of France is Paris.\",\"经过 Tokenizer 编码后可能是：\",\"[CLS] the capital of france is paris [SEP]\",\"如果模型预测 start_index=5，end_index=5，那么对应的就是单词 \\\"paris\\\"，这就是答案。\",\"⚠️ 注意事项\",\"不能超出上下文范围\",\"start/end positions 必须落在上下文部分（即 token_type_id == 1 的区域）。\",\"否则答案可能不合理（比如取到了问题部分的内容）。\",\"特殊 token 不计入答案\",\"[CLS], [SEP] 等会被 skip_special_tokens=True 自动跳过。\",\"无法处理不在原文中的答案\",\"如果正确答案没有出现在上下文中，BERT 无法“编造”出来。\",\"这是抽取式问答模型的局限性。\",\"💡 对比：生成式 vs 抽取式问答\",\"类型\",\"模型代表\",\"是否能生成新文本\",\"答案是否必须在原文中\",\"示例\",\"抽取式\",\"BERT\",\"❌\",\"✅\",\"答案是原文中的一段\",\"生成式\",\"T5 / BART / GPT\",\"✅\",\"❌\",\"答案可以是任意文本\",\"如果你希望模型能“自己写答案”，那就需要使用生成式模型。\",\"✅ 总结\",\"问题\",\"回答\",\"为什么答案来自 input_ids？\",\"因为 BERT 是编码器模型，只做抽取式问答，答案必须是原文中的一段文本。\",\"BERT 能不能自己生成答案？\",\"不能，BERT 不具备生成能力，只能对输入文本中的 token 做分类。\",\"如何获取答案？\",\"根据预测的 start/end index，从 input_ids 中提取 token，并用 tokenizer 解码成自然语言。\"]},\"256\":{\"h\":\"Token分类任务\",\"t\":[\"Token 分类任务是指对输入文本中的每个 token 进行分类，常见的应用场景包括：\",\"命名实体识别 (NER)\",\"词性标注 (POS)\",\"语义角色标注 (SRL)\",\"class BertForTokenClassification(BertPreTrainedModel): def __init__(self, config): super(BertForTokenClassification, self).__init__(config) self.num_labels = config.num_labels self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, config.num_labels) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) sequence_output = outputs[0] # (batch,seq_len,hidden_size) sequence_output = self.dropout(sequence_output) logits = self.classifier(sequence_output) # （batch,seq_len,num_labels） outputs = (logits,) if labels is not None: loss_fct = CrossEntropyLoss() # Only keep active parts of the loss if attention_mask is not None: active_loss = attention_mask.view(-1) == 1 active_logits = logits.view(-1, self.num_labels)[active_loss] active_labels = labels.view(-1)[active_loss] loss = loss_fct(active_logits, active_labels) else: loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) outputs = (loss,) + outputs return outputs # (loss), scores\"]},\"257\":{\"h\":\"多项选择任务\",\"t\":[\"多项选择任务是指给定一个问题和多个候选答案，模型需要从中选择最合适的答案。常见的应用场景包括：\",\"阅读理解任务\",\"问答系统中的候选答案选择\",\"对话系统中的候选回复选择\",\"在 多项选择题（Multiple Choice） 任务中，BERT 的输入组织形式与普通分类或问答任务略有不同。你需要为每个选项分别构造一个完整的 BERT 输入序列，并将它们组合成一个批次进行处理。\",\"✅ 假设你有一个问题 + 4 个选项：\",\"问题：谁写了《哈姆雷特》？ A. 雨果 B. 歌德 C. 莎士比亚 D. 托尔斯泰\",\"对于这样的多选问题，BERT 的输入方式是：\",\"对每一个选项，都单独构造一个 [CLS] + 问题 + [SEP] + 选项内容 + [SEP] 的输入序列。\",\"也就是说，模型会对每个选项分别编码 ，然后从中选出最合适的那个。\",\"class BertForMultipleChoice(BertPreTrainedModel): def __init__(self, config): super(BertForMultipleChoice, self).__init__(config) self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, 1) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None): # 获取选项个数 num_choices = input_ids.shape[1] # (batch_size, num_choices, seq_length) # 将选项展平，以便一起处理: (batch_size * num_choices, seq_length) input_ids = input_ids.view(-1, input_ids.size(-1)) attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) pooled_output = outputs[1] # (batch_size * num_choices, hidden_size) pooled_output = self.dropout(pooled_output) logits = self.classifier(pooled_output) # (batch_size * num_choices, 1) reshaped_logits = logits.view(-1, num_choices) # (batch_size , num_choices, 1) outputs = (reshaped_logits,) if labels is not None: loss_fct = CrossEntropyLoss() loss = loss_fct(reshaped_logits, labels) outputs = (loss,) + outputs return outputs # (loss), reshaped_logits, (hidden_states), (attentions)\",\"在前向传播中，会将这些输入展平，变成：\",\"input_ids.view(-1, seq_length) # (batch_size * num_choices, seq_length)\",\"这样就能让 BERT 对每个选项分别进行编码。\",\"BERT 输出后，再对每个选项做分类打分，最后重新 reshape 成 (batch_size, num_choices) 形式，用于计算交叉熵损失。\"]},\"258\":{\"h\":\"图解Transformer\",\"t\":[\"图解Transformer & 机器翻译实战\"]},\"259\":{\"h\":\"环境\",\"t\":[\"本文基于 The Annotated Transformer 所提供的代码展开进行讲解。\",\"环境搭建遵从如下步骤即可:\",\"git clone https://github.com/harvardnlp/annotated-transformer cd annotated-transformer conda create -n annotated-transformer python=3.9.22 conda activate annotated-transformer pip install -r requirements.txt\",\"MacOS 用户本地运行时，需要将 requirements.txt 文件中的 torch == 1.11.0+cu113 改为 torch==1.11.0，因为CUDA不支持MacOS。\"]},\"260\":{\"h\":\"背景\",\"t\":[\"RNN等模型的缺点是需要顺序计算，从而很难并行。因此出现了Extended Neural GPU、ByteNet和ConvS2S等网络模型。这些模型都是以CNN为基础，这比较容易并行。但是和RNN相比，它较难学习到长距离的依赖关系。\",\"本文的Transformer使用了Self-Attention机制，它在编码每一词的时候都能够注意(attend to)整个句子，从而可以解决长距离依赖的问题，同时计算Self-Attention可以用矩阵乘法一次计算所有的时刻，因此可以充分利用计算资源(CPU/GPU上的矩阵运算都是充分优化和高度并行的)。\"]},\"261\":{\"h\":\"模型架构\",\"t\":[\"Transformer 模型架构图\",\"Transformer 是一种基于自注意力机制(Self-Attention) 的神经网络架构,其由七大主要部分构成:\",\"Encoder-Decoder 结构\",\"编码器(Encoder)：将输入序列（如句子）转换为一系列高维向量表示。\",\"解码器(Decoder)：根据编码器的输出生成目标序列（如翻译后的句子）。\",\"多头自注意力机制（Multi-Head Self-Attention）\",\"自注意力机制是 Transformer 的核心，它允许模型在处理每个词时关注输入序列中的所有词。\",\"多头自注意力机制通过并行计算多个注意力头，捕捉不同子空间的信息，从而增强模型的表达能力。\",\"位置编码（Positional Encoding）\",\"由于 Transformer 不使用传统的循环或卷积结构，它通过位置编码将序列中词的位置信息注入到输入中。位置编码通常使用正弦和余弦函数生成。\",\"前馈神经网络（Feed-Forward Neural Network）\",\"在自注意力机制之后，每个位置的输出会通过一个独立的前馈神经网络进行进一步处理。\",\"残差连接与层归一化（Residual Connection & Layer Normalization）\",\"每个子层（如自注意力层和前馈层）都使用了残差连接和层归一化，以加速训练并提高模型的稳定性。\",\"掩码机制（Masking）\",\"在解码器中，使用掩码机制确保模型在生成目标序列时只能看到当前及之前的词，而不能看到未来的词。\",\"在输入序列长度不一致时，通过填充掩码（Padding Mask）屏蔽填充部分的信息。\",\"输出层\",\"解码器的最终输出通过一个线性层和 Softmax 函数生成目标序列的概率分布。\"]},\"262\":{\"h\":\"Encoder-Decoder 结构\",\"t\":[\"EncoderDecoder模型结构图\",\"class EncoderDecoder(nn.Module): def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): \\\"Take in and process masked src and target sequences.\\\" return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) def encode(self, src, src_mask): return self.encoder(self.src_embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\"]},\"263\":{\"h\":\"Generator\",\"t\":[\"Generator模型结构图\",\"class Generator(nn.Module): # 根据Decoder的隐状态输出一个词 # d_model是Decoder输出的大小，vocab是词典大小 def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) # 全连接再加上一个softmax def forward(self, x): return F.log_softmax(self.proj(x), dim=-1)\"]},\"264\":{\"h\":\"Encoder 结构\"},\"265\":{\"h\":\"SublayerConnection\",\"t\":[\"SublayerConnection模型结构图\",\"class SublayerConnection(nn.Module): \\\"\\\"\\\" LayerNorm + sublayer(Self-Attenion/Dense) + dropout + 残差连接 为了简单，把LayerNorm放到了前面，这和原始论文稍有不同，原始论文LayerNorm在最后。 \\\"\\\"\\\" def __init__(self, size, dropout): super(SublayerConnection, self).__init__() self.norm = LayerNorm(size) self.dropout = nn.Dropout(dropout) def forward(self, x, sublayer): \\\"sublayer是传入的参数，参考DecoderLayer，它可以当成函数调用，这个函数的有一个输入参数\\\" return x + self.dropout(sublayer(self.norm(x)))\"]},\"266\":{\"h\":\"EncoderLayer\",\"t\":[\"EncoderLayer模型结构图\",\"# 编码器层 = 自注意力子层 + 前馈层 class EncoderLayer(nn.Module): def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward # 自注意力子层 和 前馈层 各需要一个 self.sublayer = clones(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): \\\"Follow Figure 1 (left) for connections.\\\" x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) return self.sublayer[1](x, self.feed_forward)\"]},\"267\":{\"h\":\"Encoder\",\"t\":[\"Encoder模型结构图\",\"class Encoder(nn.Module): \\\"Core encoder is a stack of N layers\\\" def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): \\\"Pass the input (and mask) through each layer in turn.\\\" for layer in self.layers: x = layer(x, mask) return self.norm(x)\"]},\"268\":{\"h\":\"Decoder 结构\"},\"269\":{\"h\":\"DecoderLayer\",\"t\":[\"Decoder模型结构图\",\"# 解码器层 = 自注意力子层 + 源注意力子层 + 前馈层 class DecoderLayer(nn.Module): \\\"Decoder is made of self-attn, src-attn, and feed forward (defined below)\\\" def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward # 自注意力子层 + 源注意力子层 + 前馈层 各需要一个 self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): \\\"Follow Figure 1 (right) for connections.\\\" m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward)\"]},\"270\":{\"h\":\"Decoder\",\"t\":[\"Decoder模型结构图\",\"# 解码器 = N个解码器层 + 层归一化 class Decoder(nn.Module): \\\"Generic N layer decoder with masking.\\\" def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): # 输入,编码器隐藏层输出,源掩码,目标掩码 for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x)\"]},\"271\":{\"h\":\"多头自注意力\",\"t\":[\"多头自注意力计算流程图\",\"class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): \\\"Take in model size and number of heads.\\\" super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # We assume d_v always equals d_k self.d_k = d_model // h # 每个头64维 self.h = h # 8个头 self.linears = clones(nn.Linear(d_model, d_model), 4) # W_q,W_k,W_v,W_projection self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): \\\"Implements Figure 2\\\" if mask is not None: # Same mask applied to all h heads. mask = mask.unsqueeze(1) nbatches = query.size(0) # 1) Do all the linear projections in batch from d_model => h x d_k query, key, value = [ lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) # (batches,heads,seq_len,d_k) for lin, x in zip(self.linears, (query, key, value)) ] # 2) Apply attention on all the projected vectors in batch. x, self.attn = attention( query, key, value, mask=mask, dropout=self.dropout ) # 3) \\\"Concat\\\" using a view and apply a final linear. x = ( x.transpose(1, 2) .contiguous() .view(nbatches, -1, self.h * self.d_k) ) del query del key del value return self.linears[-1](x)\",\"def attention(query, key, value, mask=None, dropout=None): \\\"Compute 'Scaled Dot Product Attention'\\\" d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) # 广播: (1,1,1,10) ---> (1,8,10,10) p_attn = scores.softmax(dim=-1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn\"]},\"272\":{\"h\":\"简析 LLaMA\",\"t\":[\"简析LLaMA\",\"论文链接:\"]},\"273\":{\"h\":\"CS224n-自然语言处理\"},\"274\":{\"h\":\"CS231n-计算机视觉\"},\"275\":{\"h\":\"吴恩达-大语言模型应用开发课程\"},\"276\":{\"h\":\"1.前置知识\",\"t\":[\"智慧化知识库系统: 大语言模型应用开发基础知识速览。\"]},\"277\":{\"h\":\"大语言模型\",\"t\":[\"大语言模型（LLM，Large Language Model），也称大型语言模型，是一种旨在理解和生成人类语言的人工智能模型。\",\"LLM 通常指包含数百亿（或更多）参数的语言模型，它们在海量的文本数据上进行训练，从而获得对语言深层次的理解。目前，国外的知名 LLM 有 GPT、LLaMA、Gemini、Claude 和 Grok 等，国内的有 DeepSeek、通义千问、豆包、Kimi、文心一言、GLM 等。\",\"为了探索性能的极限，许多研究人员开始训练越来越庞大的语言模型，例如拥有 175B (1750 亿)参数的 GPT-3 和 540B（5400 亿）参数的 PaLM 。尽管这些大型语言模型与小型语言模型（例如 3.3 亿参数的 BERT 和 15 亿参数的 GPT-2）使用相似的架构和预训练任务，但它们展现出截然不同的能力，尤其在解决复杂任务时表现出了惊人的潜力，这被称为“涌现能力”。以 GPT-3 和 GPT-2 为例，GPT-3 可以通过学习上下文来解决少样本任务，而 GPT-2 在这方面表现较差。因此，科研界给这些庞大的语言模型起了个名字，称之为“大语言模型（LLM）”。LLM 的一个杰出应用就是 ChatGPT ，它是 GPT 系列 LLM 用于与人类对话式应用的大胆尝试，展现出了非常流畅和自然的表现。 语言建模的研究可以追溯到 20 世纪 90 年代，当时的研究主要集中在采用统计学习方法来预测词汇，通过分析前面的词汇来预测下一个词汇。但在理解复杂语言规则方面存在一定局限性。\",\"随后，研究人员不断尝试改进，2003 年深度学习先驱 Bengio 在他的经典论文 《A Neural Probabilistic Language Model》中，首次将深度学习的思想融入到语言模型中。强大的神经网络模型，相当于为计算机提供了强大的\\\"大脑\\\"来理解语言，让模型可以更好地捕捉和理解语言中的复杂关系。\",\"2018 年左右，Transformer 架构的神经网络模型开始崭露头角。通过大量文本数据训练这些模型，使它们能够通过阅读大量文本来深入理解语言规则和模式，就像让计算机阅读 整个互联网一样，对语言有了更深刻的理解，极大地提升了模型在各种自然语言处理任务上的表现。\",\"与此同时，研究人员发现，随着语言模型规模的扩大（增加模型大小或使用更多数据），模型展现出了一些惊人的能力，在各种任务中的表现均显著提升（Scaling Law）。这一发现标志着大型语言模型（LLM）时代的开启。\",\"通常大模型由三个阶段构成：预训练、后训练和在线推理。在 2024 年 9 月之前，大模型领域仅存在预训练阶段的 Scaling Law。然而，随着 OpenAI o1 的推出，后训练和在线推理阶段也各自拥有了 Scaling Law，即后训练阶段的强化学习 Scaling Law（RL Scaling Law）和在线推理阶段的 Inference Scaling Law（Test Time Scaling Law）。 随着各阶段计算量的增加，大模型的性能不断增长。\"]},\"278\":{\"h\":\"常见的LLM\",\"t\":[\"大语言模型的发展历程虽然只有短短不到五年的时间，但是发展速度相当惊人，截止 2024 年 6 月，国内外有超过百种大模型相继发布。下图按照时间线给出了 2019 年至 2024 年 6 月比较有影响力并且模型参数量超过 100 亿的大语言模型：\",\"接下来我们主要介绍几个国内外常见的大模型（包括开源和闭源）。\",\"OpenAI\",\"OpenAI 公司在 2018 年 提出的 GPT（Generative Pre-Training） 模型是典型的 生成式预训练语言模型 之一。 GPT 模型的基本原则是通过语言建模将世界知识压缩到仅解码器 (decoder-only) 的 Transformer 模型中，这样它就可以恢复(或记忆)世界知识的语义，并充当通用任务求解器。它能够成功的两个关键点：\",\"训练能够准确预测下一个单词的 decoder-only 的 Transformer 语言模型\",\"扩展语言模型的大小\",\"OpenAI 在 LLM 上的研究大致可以分为以下几个阶段：\",\"目前，GPT 系列已形成 知识型 与 推理型 两大技术分支。\",\"2022 年 11 月，OpenAI 发布了基于 GPT 模型（GPT-3.5 和 GPT-4）的会话应用 ChatGPT。由于与人类交流的出色能力，ChatGPT 自发布以来就引发了人工智能社区的兴奋。ChatGPT 本质上是一个 LLM 应用，是基于基座模型开发出来的，与基座模型有本质的区别。ChatGPT 上线后用户增长迅速，5 天注册人数突破 100 万，两个月后月活用户破亿，成为当时史上用户增长最快的消费级应用程序。\",\"随着不断迭代，ChatGPT 逐渐丰富了其功能：\",\"插件系统：允许开发者创建工具扩展 ChatGPT 的能力，实现网页浏览、数据分析和第三方服务调用\",\"实时语音和视频对话：用户可与 AI 进行自然的语音和视频交流，支持手势识别和情感表达\",\"多模态能力：能够分析和理解用户提供的图片、音频和视频，实现全面的多模态交互\",\"自定义指令与记忆功能：记住用户之前的交互习惯和偏好，提供个性化体验\",\"GPT 构建器平台：允许用户无需编程创建专用的 AI 助手，支持自定义知识库和行为模式\",\"数据分析与可视化：直接处理和分析上传的数据文件，生成图表和可视化报告\",\"知识型与推理型双模式：可在 GPT-4.5 (知识型) 和 o1/o3 (推理型) 之间切换，满足不同场景需求\",\"思维链展示：在推理型模型中可选择性展示思考过程，帮助用户理解推理步骤\",\"2023 年 3 月 发布的 GPT-4 引入了多模态能力，相比 GPT-3.5 的 1750 亿参数，GPT-4 规模显著扩大（推测约 1.8 万亿参数），在解决复杂任务和评估任务上展现出较大的性能提升。\",\"2024 年 5 月 发布的 GPT-4o（\\\"o\\\"代表\\\"omni\\\"全能）具备对文本、语音、图像三种模态的深度理解能力，主要特点包括：\",\"多模态融合：无缝理解和生成多种形式内容\",\"实时对话：响应速度比 GPT-4 快约 2 倍\",\"情感表达：在语音互动中传递更丰富的情感变化\",\"成本效益：API 定价降低约 50%\",\"2024 年 7 月 发布的 GPT-4o mini 是一款面向消费级应用的轻量级模型，价格更加亲民，适合日常对话和基础任务场景。\",\"2025 年 2 月 发布的 GPT-4.5 在知识广度、推理深度和创意表达方面有显著提升，特别强化了对客观事实的准确性，尤其是情商方面异常优秀。上下文长度扩展至 512K。是 OpenAI 的最后一个非思维链模型。\",\"主流知识型模型对比:\",\"模型名称\",\"上下文长度\",\"特点\",\"知识截止日期\",\"GPT-4\",\"16k\",\"经济，专门对话\",\"2021 年 9 月\",\"GPT-4o\",\"128k\",\"多模态，速度快\",\"2023 年 10 月\",\"GPT-4.5\",\"128k\",\"最强知识型，精准度高\",\"2023 年 10 月\",\"GPT-4o mini\",\"128k\",\"轻量知识型，性价比高\",\"2023 年 10 月\",\"2024 年 9 月 发布的 o1-mini、o1-preview 是专为复杂推理设计的模型，在回答前会先生成一段思维链（不公开），优先考虑精确性和推理步骤的正确性。\",\"超强推理能力：在数学、编程和逻辑推理等任务中表现卓越。\",\"解题过程可靠：注重解题中间步骤的正确性。\",\"问题分解能力：将复杂问题分解为可管理的子问题。\",\"自纠错机制：识别错误并主动纠正。\",\"2024 年 12 月 发布的 o1 比 o1-preview 可以在更快的时间内响应，思考的时间更短。\",\"2025 年 1 月 发布的 o3-mini 可以显示部分思维链，与 o1 相比，可以保持效果的情况下，响应速度更快。\",\"模型名称\",\"上下文长度\",\"特点\",\"知识截止日期\",\"o1\",\"128k\",\"强推理能力，慢\",\"2023 年 10 月\",\"o1 mini\",\"200k\",\"轻量推理，中速\",\"2023 年 10 月\",\"o3 mini\",\"200k\",\"超轻量推理，最快\",\"2023 年 10 月\",\"OpenAI 的模型战略形成了“知识型”和“推理型”两条互补产品线：\",\"知识型模型 专注于广泛知识覆盖和流畅对话体验。\",\"推理型模型 专注于精确推理和复杂问题求解，让用户可根据具体需求选择最适合的模型类型。\",\"Claude\",\"Claude 系列模型是由 OpenAI 离职人员创建的 Anthropic 公司开发的闭源语言大模型。\",\"最早的 Claude 于 2023 年 3 月 15 日发布。\",\"2024 年 3 月 4 日，更新至 Claude-3，包括 Claude 3 Haiku、Claude 3 Sonnet 和 Claude 3 Opus，它们的能力依次递增，旨在满足不同用户和应用场景的需求。\",\"2024 年 10 月，Anthropic 发布了 Claude 3.5 Sonnet，这是一款在推理和通用任务上有显著提升的模型。\",\"2025 年 5 月，Anthropic 又进一步发布了 Claude 4.0，包括了 Claude 4 Sonnet 和 Claude 4 Opus，均是混合推理模型，支持标准模式与推理思考模式，编码能力异常强大。支持多工具并行调用与精准指令解析，本地文件访问时内存管理升级，可规避捷径行为，强化复杂任务处理能力。\",\"模型名称\",\"上下文长度\",\"特点\",\"Claude 3.5 Haiku\",\"200k\",\"速度最快\",\"Claude 4 Sonnet\",\"200k\",\"最强性能，领先推理力\",\"Claude 4 Opus\",\"200k\",\"性能强大，费用最高\",\"Gemini\",\"Gemini 系列语言大模型由 Google 开发。\",\"2022 年 4 月，发布了初始版本（PaLM 后更名为 Gemini）。\",\"2025 年 2 月，Google 发布了 Gemini 2.0 系列模型，在性能和效率上有显著提升。包括 Gemini 2.0 Pro、Gemini 2.0 Flash、Gemini 2.0 Flash-Lite 是 Gemini 2.0 系列的三个版本，分别适用于不同的场景。同样，推出了其推理模型 Gemini 2.0 Flash Thinking。\",\"2025 年 3 月，Google 发布了 Gemini 2.5 Pro，性能有了进一步提升，推理能力和代码能力提升非常显著。\",\"模型名称\",\"上下文长度\",\"特点\",\"Gemini 2.5 Pro\",\"2M\",\"性能最强\",\"Gemini 2.0 Flash\",\"1M\",\"低延迟，性能强\",\"Gemini 2.0 Flash-Lite\",\"1M\",\"性价比最高\",\"Gemini 2.0 Flash Thinking\",\"1M\",\"思维链展示\",\"文心一言\",\"文心一言是基于百度文心大模型的知识增强语言大模型，于 2023 年 3 月 在国内率先开启邀测。文心一言的基础模型文心大模型于 2019 年发布 1.0 版，现已更新到 4.0 版本。更进一步划分，文心大模型包括 NLP 大模型、CV 大模型、跨模态大模型、生物计算大模型、行业大模型。文心一言的中文能力相对来说非常不错。 文心一言网页版分为 免费版 和 专业版：\",\"免费版 使用文心 3.5 版本，已经能够满足个人用户或小型企业的大部分需求。\",\"专业版 使用文心 4.0 版本，定价为 59.9 元/月，连续包月优惠价为 49.9 元/月。\",\"星火大模型\",\"讯飞星火认知大模型是科大讯飞发布的语言大模型，支持多种自然语言处理任务。\",\"2023 年 5 月，首次发布。\",\"2024年 10 月，讯飞星火发布模型 星火 4.0 Turbo。\",\"2025 年 1 月，讯飞发布了推理思考模型 讯飞星火 X1 和 星火语音同传模型。\",\"LLaMA\",\"LLaMA 系列模型是 Meta 开源的一组参数规模从 8B 到 405B 的基础语言模型。\",\"2023 年 2 月，发布 LLaMA。\",\"2023 年 7 月，发布了 LLaMA2 模型。\",\"2024 年 4 月，发布了 LLaMA3 模型。\",\"2024 年 7 月，发布了 LLaMA 3.1 模型。\",\"2024 年 12 月，发布了 LLaMA 3.3 模型（只开源了 70B 的指令模型）。\",\"它们都是在数万亿个字符上训练的，展示了如何仅使用公开可用的数据集来训练最先进的模型，而不需要依赖专有或不可访问的数据集。这些数据集包括 Common Crawl、Wikipedia、OpenWebText2、RealNews、Books 等。LLaMA 模型使用了大规模的数据过滤和清洗技术，以提高数据质量和多样性，减少噪声和偏见。LLaMA 模型还使用了高效的数据并行和流水线并行技术，以加速模型的训练和扩展其中 405B 参数模型是首个公开的千亿级开源模型，性能对标 GPT-4o 等商业闭源模型。 与 GPT 系列相同，LLaMA 模型也采用了 decoder-only 架构，同时结合了一些前人工作的改进。LLaMA 系列基本上是后续大模型的标杆：\",\"Pre-normalization（正则化）：为了提高训练稳定性，LLaMA 对每个 Transformer 子层的输入进行了 RMSNorm 归一化，这种归一化方法可以避免梯度爆炸和消失的问题，提高模型的收敛速度和性能。\",\"SwiGLU（激活函数）：将 ReLU 非线性替换为 SwiGLU 激活函数，增加网络的表达能力和非线性，同时减少参数量和计算量。\",\"旋转位置编码（RoPE, Rotary Position Embedding）：模型的输入不再使用位置编码，而是在网络的每一层添加了位置编码，RoPE 位置编码可以有效地捕捉输入序列中的相对位置信息，并且具有更好的泛化能力。\",\"分组查询注意力（GQA, Grouped-Query Attention）：通过将查询（query）分组并在组内共享键（key）和值（value），减少了计算量，同时保持了模型性能，提高了大型模型的推理效率（LLaMA2 只有 70B 采用）。\",\"LLaMA 3.1 于 2024 年 7 月 发布，提高了模型的性能和效率：\",\"更多的训练数据量：LLaMA3.1 在 15 万亿个 token 的数据上进行预训练，采用了更科学的数据配比。LLaMA3.1 接触到更多的文本信息，从而提高了其理解和生成文本的能力。\",\"更长的上下文长度：LLaMA 3.1 将上下文长度大幅提升至 128K token，支持处理极长的文档和对话历史，改善了对长文本的理解和生成能力，适用于更复杂的应用场景。\",\"分组查询注意力（GQA, Grouped-Query Attention）：通过将查询（query）分组并在组内共享键（key）和值（value），减少了计算量，同时保持了模型性能，提高了大型模型的推理效率（LLaMA2 只有 70B 采用）。\",\"更大的词表：LLaMA3.1 采用了 128K 的 tokenizer，是前两代 32K 的 4 倍，这使得其语义编码能力得到了极大的增强，从而显著提升了模型的性能。\",\"精细的指令遵循：通过改进的对齐技术，LLaMA 3.1 在遵循复杂指令、理解微妙提示方面表现更出色，使模型行为更可预测和可控。\",\"完善的工具使用：增强了 Function Calling 能力，使模型能够更准确地识别何时以及如何调用外部工具，提高了与外部系统集成的能力。 LLaMA 3.1 发布了 8B、70B 和 405B 三个规模的模型，分别提供基础版（Base）和指令微调版（Instruction），进一步扩展了 LLaMA 系列在开源社区的影响力和应用前景。\",\"DeepSeek\",\"DeepSeek 是由深度求索 (DeepSeek) 团队开发的开源大语言模型系列。首个版本于 2023 年 11 月 发布。DeepSeek 采用 decoder-only 架构，融合了 FlashAttention-2、RoPE 位置编码、SwiGLU 等先进技术，在多语言理解和代码生成等方面表现出色。\",\"2023 年 11 月 12 日：发布 DeepSeek 系列基础模型，包括 7B 和 67B 两种规模的 Base 和 Chat 版本。模型在 1.2 万亿 token 上进行训练，同时发布了 DeepSeek-Coder 专用代码生成模型。\",\"2024 年 3 月 15 日：发布 DeepSeek-V2 系列，提升了多语言能力、长文本理解和推理能力，同时发布了 DeepSeek-MoE 混合专家模型。\",\"2024 年 5 月 31 日：发布 DeepSeek-V2.5，性能得到进一步提升，上下文长度扩展至 128K tokens，并改进了工具调用和多模态能力。\",\"2024 年 10 月：发布 DeepSeek-V3，在推理能力、多语言理解和创意生成方面有显著提升，支持更复杂的系统提示词控制，并进一步提升了代码质量和多轮对话一致性。\",\"2025 年 2 月：\",\"DeepSeek-R1 推理型大模型：专注于复杂问题求解和精确推理能力，在数学、逻辑推理和结构化知识方面展现出卓越性能，类似于 OpenAI 的 o1 系列。并且是首个开源的推理型大模型，在多项基准测试中超越了 o1 系列。\",\"DeepSeek-R1-Zero：直接在大规模强化学习 (RL) 训练的模型，无需 SFT，在推理方面就十分出色。\",\"同时开源了用 Llama 和 Qwen 从 DeepSeek-R1 中蒸馏出的六个 dense 模型。其中 DeepSeek-R1-Distill-Qwen-32B 在各种基准测试中均优于 OpenAI-o1-mini。\",\"deepseek 目前采用的主要改进如下：\",\"多头潜在注意力 (MLA, Multi-head Latent Attention)：通过将键值 (KV) 缓存显著压缩为潜在向量来保证高效推理的同时不降低效果。\",\"DeepSeekMoE：通过稀疏计算以经济的成本训练强大的模型。\",\"一系列推理加速技术 借助 DeepSeekR1 的卓越能力，DeepSeek 成为了现象级爆火应用。7 天完成了 1 亿用户的增长，打破了 ChatGPT 的 2 个月的最快记录，成为史上增长最快的 AI 应用。\",\"通义千问\",\"通义千问是由阿里巴巴基于“通义”大模型研发，于 2023 年 4 月 正式发布。\",\"2023 年 9 月：阿里云开源了 Qwen（通义千问）系列工作。\",\"2024 年 6 月 6 日：正式开源了 Qwen2。\",\"2025 年 4 月 29 日：发布了全新升级的 Qwen3 系列模型。\",\"Qwen 系列均采用 decoder-only 架构，并结合 SwiGLU 激活、RoPE、GQA 等技术。中文能力相对来说是非常不错的开源模型。 目前，已经开源了 7 种模型大小：\",\"Dense 模型：0.6B、1.7B、4B、8B、14B、32B；\",\"MoE 模型：30B-A3B、235B-A22B。\",\"上下文长度：\",\"8B 以下模型的上下文长度为 32k；\",\"8B 以上模型的上下文长度为 128k。\",\"Qwen3 进一步增强了模型性能，改进了推理能力和指令遵循能力，同时保持了低资源部署的高效性，使其在长文本理解和复杂任务处理方面具有更强的优势。支持思考模式和非思考模式之间无缝切换，覆盖 119 种语言和方言。强化了模型的代码能力、Agent 能力，以及对 MCP 的支持。 同时还开源了代码模型和数学模型：\",\"Qwen2.5-Coder：1.5B、7B，以及即将推出的 32B。\",\"Qwen2.5-Math：1.5B、7B，以及 72B。\",\"在推理大模型方面：\",\"2024 年 11 月：发布并开源了 QwQ-32B-Preview 模型，仅用 32B 参数便在部分达到了 o1-mini 的推理水平。\",\"2025 年 3 月：发布并开源了 QwQ-32B，其性能可与具备 671B 参数（37B 激活参数）的 DeepSeek-R1 媲美。\",\"ChatGLM\",\"GLM系列模型是 清华大学和智谱 AI 等合作研发的语言大模型。\",\"2023 年 3 月，发布了 ChatGLM。\",\"2024 年 1 月，发布了 GLM4，并于 2024 年 6 月 正式开源。\",\"GLM-4-9B-Chat 支持多轮对话的同时，还具备网页浏览、代码执行、自定义工具调用（Function Call）和长文本推理（支持最大 128K 上下文）等功能。 开源了 对话模型 GLM-4-9B-Chat、基础模型 GLM-4-9B、长文本对话模型 GLM-4-9B-Chat-1M（支持 1M 上下文长度）、多模态模型 GLM-4V-9B 等全面对标 OpenAI。\"]},\"279\":{\"h\":\"LLM 的特点与能力\",\"t\":[\"大语言模型具有多种显著特点，这些特点使它们在自然语言处理和其他领域中引起了广泛的兴趣和研究。以下是大语言模型的一些主要特点：\",\"巨大的规模： LLM 通常具有巨大的参数规模，可以达到数十亿甚至数千亿个参数。这使得它们能够捕捉更多的语言知识和复杂的语法结构。\",\"预训练和微调： LLM 采用了预训练和微调的学习方法。首先在大规模文本数据上进行预训练（无标签数据），学习通用的语言表示和知识。然后通过微调（有标签数据）适应特定任务，从而在各种NLP 任务中表现出色。\",\"上下文感知： LLM 在处理文本时具有强大的上下文感知能力，能够理解和生成依赖于前文的文本内容。这使得它们在对话、文章生成和情境理解方面表现出色。\",\"多语言支持： LLM 可以用于多种语言，不仅限于英语。它们的多语言能力使得跨文化和跨语言的应用变得更加容易。\",\"多模态支持： 一些 LLM 已经扩展到支持多模态数据，包括文本、图像和声音。使得它们可以理解和生成不同媒体类型的内容，实现更多样化的应用。\",\"伦理和风险问题： 尽管 LLM 具有出色的能力，但它们也引发了伦理和风险问题，包括生成有害内容、隐私问题、认知偏差等。因此，研究和应用 LLM 需要谨慎。\",\"高计算资源需求： LLM 参数规模庞大，需要大量的计算资源进行训练和推理。通常需要使用高性能的 GPU 或 TPU 集群来实现。 大语言模型是一种具有强大语言处理能力的技术，已经在多个领域展示了潜力。它们为自然语言理解和生成任务提供了强大的工具，同时也引发了对其伦理和风险问题的关注。这些特点使 LLM 成为了当今计算机科学和人工智能领域的重要研究和应用方向。\"]},\"280\":{\"h\":\"\",\"t\":[\"区分大语言模型（LLM）与以前的预训练语言模型（PLM）最显著的特征之一是它们的 涌现能力。涌现能力是一种令人惊讶的能力，它在小型模型中不明显，但在大型模型中特别突出。类似物理学中的相变现象，涌现能力就像是模型性能随着规模增大而迅速提升，超过了随机水平，也就是我们常说的 量变引起质变。 涌现能力可以与某些复杂任务有关，但我们更关注的是其通用能力。接下来，我们简要介绍三个 LLM 典型的涌现能力：\",\"上下文学习：上下文学习能力是由 GPT-3 首次引入的。这种能力允许语言模型在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而无需额外的训练或参数更新。\",\"指令遵循：通过使用自然语言描述的多任务数据进行微调，也就是所谓的 指令微调。LLM 被证明在使用指令形式化描述的未见过的任务上表现良好。这意味着 LLM 能够根据任务指令执行任务，而无需事先见过具体示例，展示了其强大的泛化能力。\",\"逐步推理：小型语言模型通常难以解决涉及多个推理步骤的复杂任务，例如数学问题。然而，LLM 通过采用 思维链（CoT, Chain of Thought） 推理策略，利用包含中间推理步骤的提示机制来解决这些任务，从而得出最终答案。据推测，这种能力可能是通过对代码的训练获得的。 这些涌现能力让 LLM 在处理各种任务时表现出色，使它们成为了解决复杂问题和应用于多领域的强大工具。\"]},\"281\":{\"h\":\"\",\"t\":[\"在 2021 年，斯坦福大学等多所高校的研究人员提出了基座模型（foundation model）的概念，清晰了预训练模型的作用。这是一种全新的 AI 技术范式，借助于海量无标注数据的训练，获得可以适用于大量下游任务的大模型（单模态或者多模态）。这样，多个应用可以只依赖于一个或少数几个大模型进行统一建设。 大语言模型是这个新模式的典型例子，使用统一的大模型可以极大地提高研发效率。相比于每次开发单个模型的方式，这是一项本质上的进步。大型模型不仅可以缩短每个具体应用的开发周期，减少所需人力投入，也可以基于大模型的推理、常识和写作能力，获得更好的应用效果。因此，大模型可以成为 AI 应用开发的大一统基座模型，这是一个一举多得、全新的范式，值得大力推广。\"]},\"282\":{\"h\":\"\",\"t\":[\"让大语言模型真正火爆的契机，是基于对话聊天的 ChatGPT。业界很早就发现了用户对于对话交互的特殊偏好，陆奇在微软期间，就于 2016 年推进过“对话即平台（conversation as a platform）” 的战略。此外，苹果 Siri 、亚马逊 Echo 等基于语音对话的产品也非常受欢迎，反映出互联网用户对于聊天和对话这种交互模式的偏好。虽然之前的聊天机器人存在各种问题，但大型语言模型的出现再次让聊天机器人这种交互模式可以重新涌现。用户愈发期待像钢铁侠中“贾维斯”一样的人工智能，无所不能、无所不知。这引发我们对于 智能体（Agent） 类型应用前景的思考，Auto-GPT、微软 Jarvis 等项目已经出现并受到关注，相信未来会涌现出很多类似的以对话形态让助手完成各种具体工作的项目。\",\"LLM 已经在许多领域产生了深远的影响。在自然语言处理领域，它可以帮助计算机更好地理解和生成文本，包括写文章、回答问题、翻译语言等。在信息检索领域，它可以改进搜索引擎，让我们更轻松地找到所需的信息。在计算机视觉领域，研究人员还在努力让计算机理解图像和文字，以改善多媒体交互。 最重要的是，LLM 的出现让人们重新思考了 通用人工智能（AGI） 的可能性。AGI 是一种像人类一样思考和学习的人工智能。LLM 被认为是 AGI 的一种早期形式，这引发了对未来人工智能发展的许多思考和计划。\",\"总之，LLM 是一种令人兴奋的技术，它让计算机更好地理解和使用语言，正在改变着我们与技术互动的方式，同时也引发了对未来人工智能的无限探索。\"]},\"283\":{\"h\":\"检索增强生成（RAG, Retrieval-Augmented Generation）\",\"t\":[\"大型语言模型（LLM）相较于传统的语言模型具有更强大的能力，然而在某些情况下，它们仍可能无法提供准确的答案。为了解决大型语言模型在生成文本时面临的一系列挑战，提高模型的性能和输出质量，研究人员提出了一种新的模型架构：检索增强生成（RAG, Retrieval-Augmented Generation）。该架构巧妙地整合了从庞大知识库中检索到的相关信息，并以此为基础，指导大型语言模型生成更为精准的答案，从而显著提升了回答的准确性与深度。 目前 LLM 面临的主要问题有：\",\"信息偏差/幻觉： LLM 有时会产生与客观事实不符的信息，导致用户接收到的信息不准确。RAG 通过检索数据源，辅助模型生成过程，确保输出内容的精确性和可信度，减少信息偏差。\",\"知识更新滞后性： LLM 基于静态的数据集训练，这可能导致模型的知识更新滞后，无法及时反映最新的信息动态。RAG 通过实时检索最新数据，保持内容的时效性，确保信息的持续更新和准确性。\",\"内容不可追溯： LLM 生成的内容往往缺乏明确的信息来源，影响内容的可信度。RAG 将生成内容与检索到的原始资料建立链接，增强了内容的可追溯性，从而提升了用户对生成内容的信任度。\",\"领域专业知识能力欠缺： LLM 在处理特定领域的专业知识时，效果可能不太理想，这可能会影响到其在相关领域的回答质量。RAG 通过检索特定领域的相关文档，为模型提供丰富的上下文信息，从而提升了在专业领域内的问题回答质量和深度。\",\"推理能力限制： 面对复杂问题时，LLM 可能缺乏必要的推理能力，这影响了其对问题的理解和回答。RAG 结合检索到的信息和模型的生成能力，通过提供额外的背景知识和数据支持，增强了模型的推理和理解能力。\",\"应用场景适应性受限： LLM 需在多样化的应用场景中保持高效和准确，但单一模型可能难以全面适应所有场景。RAG 使得 LLM 能够通过检索对应应用场景数据的方式，灵活适应问答系统、推荐系统等多种应用场景。\",\"长文本处理能力较弱： LLM 在理解和生成长篇内容时受限于有限的上下文窗口，且必须按顺序处理内容，输入越长，速度越慢。RAG 通过检索和整合长文本信息，强化了模型对长上下文的理解和生成，有效突破了输入长度的限制，同时降低了调用成本，并提升了整体的处理效率。\"]},\"284\":{\"h\":\"工作流程\",\"t\":[\"RAG 是一个完整的系统，其工作流程可以简单地分为数据处理、检索、增强和生成四个阶段：\",\"数据处理阶段: 对原始数据进行清洗和处理; 将处理后的数据转化为检索模型可以使用的格式; 将处理后的数据存储在对应的数据库中。\",\"检索阶段: 将用户的问题输入到检索系统中，从数据库中检索相关信息。\",\"增强阶段: 对检索到的信息进行处理和增强，以便生成模型可以更好地理解和使用。\",\"生成阶段: 将增强后的信息输入到生成模型中，生成模型根据这些信息生成答案。\"]},\"285\":{\"h\":\"RAG VS Finetune\",\"t\":[\"在提升大语言模型效果中，RAG 和 微调（Finetune）是两种主流的方法。\",\"微调: 通过在特定数据集上进一步训练大语言模型，来提升模型在特定任务上的表现。\",\"RAG 和 微调的对比可以参考下表 :\",\"特征比较\",\"RAG\",\"微调\",\"知识更新\",\"直接更新检索知识库，无需重新训练。信息更新成本低，适合动态变化的数据。\",\"通常需要重新训练来保持知识和数据的更新。更新成本高，适合相对稳定的数据。\",\"数据处理\",\"对数据的处理和操作要求极低。\",\"依赖于构建高质量的数据集，有限的数据集可能无法显著提高性能。\",\"模型定制\",\"侧重于信息检索和融合外部知识，但可能无法充分定制模型行为或写作风格。\",\"可以根据特定风格或术语调整 LLM 行为、写作风格或特定领域知识。\",\"可解释性\",\"可以追溯到具体的数据来源，有较好的可解释性和可追踪性。\",\"黑盒子，可解释性相对较低。\",\"特征比较\",\"RAG\",\"微调\",\"计算资源\",\"需要额外的资源来支持检索机制和数据库的维护。\",\"依赖高质量的训练数\",\"推理延迟\",\"增加了检索步骤的耗时\",\"单纯 LLM 生成的耗时\",\"降低幻觉\",\"通过检索到的真实信息生成回答，降低了产生幻觉的概率。\",\"模型学习特定领域的数据有助于减少幻觉，但面对未见过的输入时仍可能出现幻觉。\",\"伦理隐私\",\"检索和使用外部数据可能引发伦理和隐私方面的问题。\",\"训练数据中的敏感信息需要妥善处理，以防泄露。\"]},\"286\":{\"h\":\"LangChain\",\"t\":[\"ChatGPT 的巨大成功激发了越来越多的开发者兴趣，他们希望利用 OpenAI 提供的 API 或者私有化模型，来开发基于大型语言模型的应用程序。尽管大型语言模型的调用相对简单，但要创建完整的应用程序，仍然需要大量的定制开发工作，包括 API 集成、互动逻辑、数据存储等等。\",\"为了解决这个问题，从 2022 年开始，许多机构和个人相继推出了多个开源项目，旨在帮助开发者们快速构建基于大型语言模型的端到端应用程序或工作流程。其中一个备受关注的项目就是 LangChain 框架。\",\"LangChain 框架是一个开源工具，充分利用了大型语言模型的强大能力，以便开发各种下游应用。它的目标是为各种大型语言模型应用提供通用接口，从而简化应用程序的开发流程。具体来说，LangChain 框架可以实现数据感知和环境互动，也就是说，它能够让语言模型与其他数据来源连接，并且允许语言模型与其所处的环境进行互动。\",\"利用 LangChain 框架，我们可以轻松地构建如下所示的 RAG 应用。在下图中，每个椭圆形代表了 LangChain 的一个模块，例如数据收集模块或预处理模块。每个矩形代表了一个数据状态，例如原始数据或预处理后的数据。箭头表示数据流的方向，从一个模块流向另一个模块。在每一步中，LangChain 都可以提供对应的解决方案，帮助我们处理各种任务。\"]},\"287\":{\"h\":\"核心组件\",\"t\":[\"LangChian 作为一个大语言模型开发框架，可以将 LLM 模型（对话模型、embedding 模型等）、向量数据库、交互层 Prompt、外部知识、外部代理工具整合到一起，进而可以自由构建 LLM 应用。 LangChain 主要由以下 6 个核心组件组成:\",\"模型输入/输出（Model I/O）：与语言模型交互的接口\",\"数据连接（Data connection）：与特定应用程序的数据进行交互的接口\",\"链（Chains）：将组件组合实现端到端应用。比如后续我们会将搭建检索问答链来完成检索问答。\",\"记忆（Memory）：用于链的多次运行之间持久化应用程序状态；\",\"代理（Agents）：扩展模型的推理能力。用于复杂的应用的调用序列；\",\"回调（Callbacks）：扩展模型的推理能力。用于复杂的应用的调用序列；\",\"在开发过程中，我们可以根据自身需求灵活地进行组合。\"]},\"288\":{\"h\":\"版本迭代\",\"t\":[\"在 LLM 技术领域的迅猛发展浪潮中，LangChain 作为一个不断进化的创新平台，持续推动着技术边界的拓展。2024 年 9 月 16 日，LangChain 正式发布了其稳定版本 v0.3，这一里程碑式的更新，为开发者带来了全面而强大的功能支持。其涵盖了模型的输入与输出处理、数据连接、链式操作、记忆机制、代理服务以及回调处理等关键组件，为 LLM 应用的开发和部署提供了坚实的基础。 同时，LangChain 的持续优化和功能迭代，未来将带来更多创新特性和性能提升。\",\"兼容性与支持：LangChain 兼顾了对 Python 和 JavaScript 的支持，同时保持了向后兼容性，确保开发者能够在升级过程中无缝过渡，享受到更加安全稳定的开发体验。\",\"架构改进：通过将核心组件 langchain-core 与合作伙伴包进行有效分离，LangChain 的架构设计变得更加条理清晰和稳固，为未来的系统化扩展和安全性提升奠定了坚实基础。\",\"可观察性：LangChain 通过与 LangSmith 的深度集成，提供了业界领先的调试和观测功能。这使得开发者能够对 LLM 应用中的每一步操作及其输入输出有一个清晰的认识，极大地简化了调试和问题排查的流程。\",\"广泛的集成：LangChain 拥有近 700 个集成，覆盖了从 LLM 到向量存储、工具和智能体（Agent）等多个技术领域，极大地降低了在各种技术栈上构建 LLM 应用的复杂度。\",\"可组合性：借助 LangChain 表达式语言（LCEL），开发者可以轻松地构建和定制 chain，充分利用数据编排框架的优势，包括批量处理、并行化操作和备选方案等高级功能。\",\"流式处理：LangChain 对流式处理进行了深度优化，确保所有利用 LCEL 创建的 chain 均能支持流式处理，包括中间步骤的数据流传输，从而为用户提供更加流畅的体验。\",\"输出解析：LangChain 提供了一系列强大的输出解析工具，确保 LLM 能够以结构化的格式返回信息，这对于 LLM 执行具体行动计划至关重要。\",\"检索能力：LangChain 引入了先进的检索技术，适用于生产环境，包括文本分割、检索机制和索引管道等，使得开发者能够轻松地将私有数据与 LLM 的能力相结合。\",\"工具使用与智能体：LangChain 提供了丰富的智能体和工具集合，并提供了定义工具的简便方法，支持智能体工作负载，包括让 LLM 调用函数或工具，以及如何高效地进行多次调用和推理，极大地提升了开发效率和应用性能。\"]},\"289\":{\"h\":\"生态圈\",\"t\":[\"LangChain Community: 专注于第三方集成，极大地丰富了 LangChain 的生态系统，使得开发者可以更容易地构建复杂和强大的应用程序，同时也促进了社区的合作和共享。\",\"LangChain Core: LangChain 框架的核心库、核心组件，提供了基础抽象和 LangChain 表达式语言（LCEL），提供基础架构和工具，用于构建、运行和与 LLM 交互的应用程序，为 LangChain 应用程序的开发提供了坚实的基础。我们后续会用到的处理文档、格式化 prompt、输出解析等都来自这个库。\",\"LangChain CLI: 命令行工具，使开发者能够通过终端与 LangChain 框架交互，执行项目初始化、测试、部署等任务。提高开发效率，让开发者能够通过简单的命令来管理整个应用程序的生命周期。\",\"LangServe: 部署服务，用于将 LangChain 应用程序部署到云端，提供可扩展、高可用的托管解决方案，并带有监控和日志功能。简化部署流程，让开发者可以专注于应用程序的开发，而不必担心底层的基础设施和运维工作。\",\"LangSmith: 开发者平台，专注于 LangChain 应用程序的开发、调试和测试，提供可视化界面和性能分析工具，旨在帮助开发者提高应用程序的质量，确保它们在部署前达到预期的性能和稳定性标准。\"]},\"290\":{\"h\":\"大模型开发\",\"t\":[\"我们将开发以大语言模型为功能核心、通过大语言模型的强大理解能力和生成能力、结合特殊的数据或业务逻辑来提供独特功能的应用称为大模型开发。开发大模型相关应用，其技术核心点虽然在大语言模型上，但一般通过调用 API 或开源模型来实现核心的理解与生成，通过 Prompt Enginnering 来实现大语言模型的控制，因此，虽然大模型是深度学习领域的集大成之作，大模型开发却更多是一个工程问题。\",\"在大模型开发中，我们一般不会去大幅度改动模型，而是将大模型作为一个调用工具，通过 Prompt Engineering、数据工程、业务逻辑分解等手段来充分发挥大模型能力，适配应用任务，而不会将精力聚焦在优化模型本身上。因此，作为大模型开发的初学者，我们并不需要深研大模型内部原理，而更需要掌握使用大模型的实践技巧。\",\"# 大语言模型 ## Prompt Engineering ## 数据工程 ## 业务逻辑分解 ## 验证迭代优化\",\"同时，以调用、发挥大模型为核心的大模型开发与传统的 AI 开发在整体思路上有着较大的不同。大语言模型的两个核心能力：指令遵循与文本生成提供了复杂业务逻辑的简单平替方案。\",\"传统的 AI 开发：首先需要将非常复杂的业务逻辑依次拆解，对于每一个子业务构造训练数据与验证数据，对于每一个子业务训练优化模型，最后形成完整的模型链路来解决整个业务逻辑。\",\"大模型开发：用 Prompt Engineering 来替代子模型的训练调优，通过 Prompt 链路组合来实现业务逻辑，用一个通用大模型 + 若干业务 Prompt 来解决任务，从而将传统的模型训练调优转变成了更简单、轻松、低成本的 Prompt 设计调优。\",\"同时，在评估思路上，大模型开发与传统 AI 开发也有质的差异。\",\"传统 AI 开发：需要首先构造训练集、测试集、验证集，通过在训练集上训练模型、在测试集上调优模型、在验证集上最终验证模型效果来实现性能的评估。\",\"大模型开发：流程更为灵活和敏捷。从实际业务需求出发构造小批量验证集，设计合理 Prompt 来满足验证集效果。然后，将不断从业务逻辑中收集当下 Prompt 的 Bad Case，并将 Bad Case 加入到验证集中，针对性优化 Prompt，最后实现较好的泛化效果。\"]},\"291\":{\"h\":\"基本流程\",\"t\":[\"结合上述分析，我们一般可以将大模型开发分解为以下几个流程：\",\"确定目标: 在进行开发前，我们首先需要确定开发的目标，即要开发的应用的应用场景、目标人群、核心价值。对于个体开发者或小型开发团队而言，一般应先设定最小化目标，从构建一个 MVP（最小可行性产品）开始，逐步进行完善和优化。\",\"设计功能: 在确定开发目标后，需要设计本应用所要提供的功能，以及每一个功能的大体实现逻辑。虽然我们通过使用大模型来简化了业务逻辑的拆解，但是越清晰、深入的业务逻辑理解往往也能带来更好的 Prompt 效果。同样，对于个体开发者或小型开发团队来说，首先要确定应用的核心功能，然后延展设计核心功能的上下游功能；\",\"搭建整体架构: 目前，绝大部分大模型应用都是采用的特定数据库 + Prompt + 通用大模型的架构。我们需要针对我们所设计的功能，搭建项目的整体架构，实现从用户输入到应用输出的全流程贯通。一般来说，我们推荐基于 LangChain 框架进行开发。LangChain 提供了 Chain、Tool 等架构的实现，我们可以基于 LangChain 进行个性化定制，实现从用户输入到数据库再到大模型最后输出的整体架构连接。\",\"搭建数据库: 个性化大模型应用需要有个性化数据库进行支撑。由于大模型应用需要进行向量语义检索，一般使用诸如 Chroma 的向量数据库。在该步骤中，我们需要收集数据并进行预处理，再向量化存储到数据库中。数据预处理一般包括从多种格式向纯文本的转化，例如 PDF、MarkDown、HTML、音视频等，以及对错误数据、异常数据、脏数据进行清洗。完成预处理后，需要进行切片、向量化构建出个性化数据库。\",\"Prompt Engineering: 优质的 Prompt 对大模型能力具有极大影响，我们需要逐步迭代构建优质的 Prompt Engineering 来提升应用性能。在该步中，我们首先应该明确 Prompt 设计的一般原则及技巧，构建出一个来源于实际业务的小型验证集，基于小型验证集设计满足基本要求、具备基本能力的 Prompt。\",\"验证迭代: 验证迭代在大模型开发中是极其重要的一步，一般指通过不断发现 Bad Case 并针对性改进 Prompt Engineering 来提升系统效果、应对边界情况。在完成上一步的初始化 Prompt 设计后，我们应该进行实际业务测试，探讨边界情况，找到 Bad Case，并针对性分析 Prompt 存在的问题，从而不断迭代优化，直到达到一个较为稳定、可以基本实现目标的 Prompt 版本。\",\"前后端搭建: 完成 Prompt Engineering 及其迭代优化之后，我们就完成了应用的核心功能，可以充分发挥大语言模型的强大能力。接下来我们需要搭建前后端，设计产品页面，让我们的应用能够上线成为产品。\",\"体验优化: 在完成前后端搭建之后，应用就可以上线体验了。接下来就需要进行长期的用户体验跟踪，记录 Bad Case 与用户负反馈，再针对性进行优化即可。\"]},\"292\":{\"h\":\"参考\",\"t\":[\"LLM 部分:\",\"A Survey of Large Language Models\",\"周枫：当我们谈论大模型时，应该关注哪些新能力？\",\"S型智能增长曲线：从Deepseek R1看Scaling Law的未来\",\"一文详尽之Scaling Law！\",\"QwQ: 思忖未知之界\",\"QwQ-32B: 领略强化学习之力\",\"RAG 部分:\",\"Retrieval-Augmented Generation for Large Language Models: A Survey\",\"面向大语言模型的检索增强生成技术：综述\"]},\"293\":{\"h\":\"2.大模型API使用\",\"t\":[\"智慧化知识库系统: LLM API的调用。\"]},\"294\":{\"h\":\"智慧化知识库系统\"},\"295\":{\"h\":\"\",\"t\":[\"404 Not Found\"]}},\"dirtCount\":0,\"index\":[[\"周枫\",{\"1\":{\"292\":1}}],[\"探讨边界情况\",{\"1\":{\"291\":1}}],[\"脏数据进行清洗\",{\"1\":{\"291\":1}}],[\"音视频等\",{\"1\":{\"291\":1}}],[\"音频和视频\",{\"1\":{\"278\":1}}],[\"搭建数据库\",{\"1\":{\"291\":1}}],[\"搭建项目的整体架构\",{\"1\":{\"291\":1}}],[\"搭建整体架构\",{\"1\":{\"291\":1}}],[\"绝大部分大模型应用都是采用的特定数据库\",{\"1\":{\"291\":1}}],[\"深入的业务逻辑理解往往也能带来更好的\",{\"1\":{\"291\":1}}],[\"深度学习模型\",{\"1\":{\"69\":1}}],[\"针对性优化\",{\"1\":{\"290\":1}}],[\"针对每张图像\",{\"1\":{\"93\":1}}],[\"针对每个图像嵌入向量获取和其相似度最高的分类文本嵌入向量索引下标\",{\"1\":{\"93\":1}}],[\"针对每个图像嵌入向量取出和其相似度最高的那个文本嵌入向量的索引\",{\"1\":{\"93\":1}}],[\"发挥大模型为核心的大模型开发与传统的\",{\"1\":{\"290\":1}}],[\"发布并开源了\",{\"1\":{\"278\":2}}],[\"发布\",{\"1\":{\"278\":7}}],[\"发布了全新升级的\",{\"1\":{\"278\":1}}],[\"发布了初始版本\",{\"1\":{\"278\":1}}],[\"发布了\",{\"1\":{\"278\":10}}],[\"发布了基于\",{\"1\":{\"278\":1}}],[\"发布的\",{\"1\":{\"278\":7}}],[\"业务逻辑分解\",{\"1\":{\"290\":1}}],[\"业务逻辑分解等手段来充分发挥大模型能力\",{\"1\":{\"290\":1}}],[\"业界很早就发现了用户对于对话交互的特殊偏好\",{\"1\":{\"282\":1}}],[\"部署服务\",{\"1\":{\"289\":1}}],[\"部署等任务\",{\"1\":{\"289\":1}}],[\"部分\",{\"1\":{\"13\":2,\"16\":1,\"255\":1,\"292\":2}}],[\"流程更为灵活和敏捷\",{\"1\":{\"290\":1}}],[\"流程四步走\",{\"1\":{\"57\":1}}],[\"流式处理\",{\"1\":{\"288\":1}}],[\"工具使用与智能体\",{\"1\":{\"288\":1}}],[\"工具和智能体\",{\"1\":{\"288\":1}}],[\"工作流程\",{\"0\":{\"284\":1}}],[\"工作中的一些问题\",{\"1\":{\"179\":1}}],[\"拥有近\",{\"1\":{\"288\":1}}],[\"享受到更加安全稳定的开发体验\",{\"1\":{\"288\":1}}],[\"持续推动着技术边界的拓展\",{\"1\":{\"288\":1}}],[\"持续追踪日志最新输出\",{\"1\":{\"40\":1}}],[\"链路组合来实现业务逻辑\",{\"1\":{\"290\":1}}],[\"链式操作\",{\"1\":{\"288\":1}}],[\"链\",{\"1\":{\"287\":1}}],[\"外部代理工具整合到一起\",{\"1\":{\"287\":1}}],[\"外部知识\",{\"1\":{\"287\":1}}],[\"箭头表示数据流的方向\",{\"1\":{\"286\":1}}],[\"许多机构和个人相继推出了多个开源项目\",{\"1\":{\"286\":1}}],[\"许多研究人员开始训练越来越庞大的语言模型\",{\"1\":{\"277\":1}}],[\"互动逻辑\",{\"1\":{\"286\":1}}],[\"伦理隐私\",{\"1\":{\"285\":1}}],[\"伦理和风险问题\",{\"1\":{\"279\":1}}],[\"黑盒子\",{\"1\":{\"285\":1}}],[\"行为\",{\"1\":{\"285\":1}}],[\"行业大模型\",{\"1\":{\"278\":1}}],[\"信息更新成本低\",{\"1\":{\"285\":1}}],[\"信息偏差\",{\"1\":{\"283\":1}}],[\"灵活适应问答系统\",{\"1\":{\"283\":1}}],[\"辅助模型生成过程\",{\"1\":{\"283\":1}}],[\"辅助训练目标\",{\"1\":{\"205\":1}}],[\"幻觉\",{\"1\":{\"283\":1}}],[\"翻译语言等\",{\"1\":{\"282\":1}}],[\"翻译成中文\",{\"1\":{\"78\":1}}],[\"微软\",{\"1\":{\"282\":1}}],[\"微调任务和数据集如下\",{\"1\":{\"212\":1}}],[\"微调的对比可以参考下表\",{\"1\":{\"285\":1}}],[\"微调的细节\",{\"1\":{\"211\":1}}],[\"微调的场景中\",{\"1\":{\"192\":1}}],[\"微调的最终目的\",{\"1\":{\"180\":1}}],[\"微调流程\",{\"1\":{\"189\":1}}],[\"微调之后的模型\",{\"1\":{\"188\":1}}],[\"微调相对来说就是一个更优的方案\",{\"1\":{\"179\":1}}],[\"微调成本高\",{\"1\":{\"85\":1}}],[\"微调整个模型参数\",{\"1\":{\"85\":1}}],[\"微调时保持视觉编码器参数不变\",{\"1\":{\"81\":1}}],[\"微调过程\",{\"1\":{\"81\":1}}],[\"微调\",{\"0\":{\"81\":1,\"189\":1},\"1\":{\"285\":4}}],[\"微调阶段的数据组织方式如下\",{\"1\":{\"81\":1}}],[\"微调阶段使用的是作者自己构建的高质量多模态指令数据集\",{\"1\":{\"81\":1}}],[\"微调阶段\",{\"1\":{\"79\":1,\"206\":1}}],[\"智能体\",{\"1\":{\"282\":1}}],[\"智慧化知识库系统\",{\"0\":{\"294\":1},\"1\":{\"276\":1,\"293\":1}}],[\"贾维斯\",{\"1\":{\"282\":1}}],[\"亚马逊\",{\"1\":{\"282\":1}}],[\"苹果\",{\"1\":{\"282\":1}}],[\"陆奇在微软期间\",{\"1\":{\"282\":1}}],[\"获得更好的应用效果\",{\"1\":{\"281\":1}}],[\"获得可以适用于大量下游任务的大模型\",{\"1\":{\"281\":1}}],[\"获取选项个数\",{\"1\":{\"257\":1}}],[\"获取词嵌入层权重\",{\"1\":{\"226\":1}}],[\"获取所有单词并去重\",{\"1\":{\"224\":1}}],[\"获取所有单词和每个单词的出现次数词典\",{\"1\":{\"175\":1}}],[\"获取每个相邻字符对的出现次数\",{\"1\":{\"175\":2}}],[\"获取输入张量x的形状\",{\"1\":{\"113\":1}}],[\"获取输入图像张量的形状\",{\"1\":{\"109\":1}}],[\"获取输入维度信息\",{\"1\":{\"10\":1}}],[\"获取对应图像的标签\",{\"1\":{\"107\":1}}],[\"获取该类别对应的索引\",{\"1\":{\"107\":1}}],[\"获取其对应的图像嵌入向量列表\",{\"1\":{\"93\":1}}],[\"获取候选分类名列表\",{\"1\":{\"93\":1,\"95\":1}}],[\"获取全局区域特征向量后\",{\"1\":{\"50\":1}}],[\"获取当前最高频的字符对\",{\"1\":{\"175\":2}}],[\"获取当前\",{\"1\":{\"64\":1}}],[\"获取当前功能类型对应的索引值\",{\"1\":{\"25\":1}}],[\"获取当前样本对应的问题文本\",{\"1\":{\"25\":1}}],[\"获取当前样本对应的功能区域掩码\",{\"1\":{\"25\":1}}],[\"获取当前样本对应的功能类型\",{\"1\":{\"25\":1}}],[\"获取当前样本对应的物体类别\",{\"1\":{\"25\":1}}],[\"获取当前样本对应的点云id\",{\"1\":{\"25\":1}}],[\"获取当前样本的物体类别和物体信息值\",{\"1\":{\"25\":1}}],[\"获取当前样本中多模态嵌入的维度信息\",{\"1\":{\"13\":1}}],[\"获取样本的代码实现\",{\"1\":{\"25\":1}}],[\"获取两个注意力加权结果\",{\"1\":{\"15\":1}}],[\"获取\",{\"1\":{\"13\":1,\"15\":1,\"226\":1}}],[\"获取语言嵌入\",{\"1\":{\"10\":1}}],[\"获取设备信息\",{\"1\":{\"10\":1}}],[\"技术领域的迅猛发展浪潮中\",{\"1\":{\"288\":1}}],[\"技术范式\",{\"1\":{\"281\":1}}],[\"技巧性近似\",{\"1\":{\"169\":1}}],[\"斯坦福大学等多所高校的研究人员提出了基座模型\",{\"1\":{\"281\":1}}],[\"斯坦福提出的\",{\"1\":{\"188\":1}}],[\"据推测\",{\"1\":{\"280\":1}}],[\"量变引起质变\",{\"1\":{\"280\":1}}],[\"量化通过将这些高精度的浮点数转换为低精度的整数\",{\"1\":{\"192\":1}}],[\"量化是一种在深度学习领域用于减少模型内存占用和计算量的技术\",{\"1\":{\"192\":1}}],[\"量化的核心目标是降成本\",{\"1\":{\"185\":1}}],[\"量化\",{\"1\":{\"185\":2}}],[\"集成\",{\"1\":{\"286\":1}}],[\"集群来实现\",{\"1\":{\"279\":1}}],[\"集合函数近似器\",{\"1\":{\"72\":1}}],[\"集合抽象\",{\"1\":{\"55\":1}}],[\"研究和应用\",{\"1\":{\"279\":1}}],[\"研究人员提出了一种新的模型架构\",{\"1\":{\"283\":1}}],[\"研究人员还在努力让计算机理解图像和文字\",{\"1\":{\"282\":1}}],[\"研究人员发现\",{\"1\":{\"277\":1}}],[\"研究人员不断尝试改进\",{\"1\":{\"277\":1}}],[\"认知偏差等\",{\"1\":{\"279\":1}}],[\"隐私问题\",{\"1\":{\"279\":1}}],[\"隐藏层输出\",{\"1\":{\"251\":1}}],[\"巨大的规模\",{\"1\":{\"279\":1}}],[\"清晰了预训练模型的作用\",{\"1\":{\"281\":1}}],[\"清华大学和智谱\",{\"1\":{\"278\":1}}],[\"清洗原始\",{\"1\":{\"211\":1}}],[\"媲美\",{\"1\":{\"278\":1}}],[\"覆盖了从\",{\"1\":{\"288\":1}}],[\"覆盖\",{\"1\":{\"278\":1}}],[\"阿里云开源了\",{\"1\":{\"278\":1}}],[\"天完成了\",{\"1\":{\"278\":1}}],[\"天注册人数突破\",{\"1\":{\"278\":1}}],[\"逻辑推理和结构化知识方面展现出卓越性能\",{\"1\":{\"278\":1}}],[\"团队开发的开源大语言模型系列\",{\"1\":{\"278\":1}}],[\"减少信息偏差\",{\"1\":{\"283\":1}}],[\"减少所需人力投入\",{\"1\":{\"281\":1}}],[\"减少了计算量\",{\"1\":{\"278\":2}}],[\"减少噪声和偏见\",{\"1\":{\"278\":1}}],[\"减少损失贡献\",{\"1\":{\"169\":1}}],[\"展示了其强大的泛化能力\",{\"1\":{\"280\":1}}],[\"展示了如何仅使用公开可用的数据集来训练最先进的模型\",{\"1\":{\"278\":1}}],[\"展现出了非常流畅和自然的表现\",{\"1\":{\"277\":1}}],[\"讯飞发布了推理思考模型\",{\"1\":{\"278\":1}}],[\"讯飞星火\",{\"1\":{\"278\":1}}],[\"讯飞星火发布模型\",{\"1\":{\"278\":1}}],[\"讯飞星火认知大模型是科大讯飞发布的语言大模型\",{\"1\":{\"278\":1}}],[\"星火语音同传模型\",{\"1\":{\"278\":1}}],[\"星火\",{\"1\":{\"278\":1}}],[\"星火大模型\",{\"1\":{\"278\":1}}],[\"元\",{\"1\":{\"278\":2}}],[\"免费版\",{\"1\":{\"278\":2}}],[\"生态圈\",{\"0\":{\"289\":1}}],[\"生物计算大模型\",{\"1\":{\"278\":1}}],[\"生成模型根据这些信息生成答案\",{\"1\":{\"284\":1}}],[\"生成阶段\",{\"1\":{\"284\":1}}],[\"生成图表和可视化报告\",{\"1\":{\"278\":1}}],[\"生成式预训练语言模型\",{\"1\":{\"278\":1}}],[\"生成式\",{\"1\":{\"255\":2}}],[\"生成一个上下文相关的表示\",{\"1\":{\"255\":1}}],[\"生成一组动态卷积核\",{\"1\":{\"33\":2}}],[\"生成注意力掩码\",{\"1\":{\"233\":1}}],[\"生成padding部分的mask列表\",{\"1\":{\"233\":1}}],[\"生成多种不同推理路径所得的结果的集合\",{\"1\":{\"199\":1}}],[\"生成类别名称以及对应的数字索引\",{\"1\":{\"107\":1}}],[\"生成文本的最小长度\",{\"1\":{\"104\":1}}],[\"生成文本的最大长度\",{\"1\":{\"104\":1}}],[\"生成文本嵌入\",{\"1\":{\"93\":1,\"95\":1}}],[\"生成学习\",{\"0\":{\"104\":1}}],[\"生成压缩的视觉表示\",{\"1\":{\"103\":1}}],[\"生成text\",{\"1\":{\"103\":1}}],[\"生成比标签\",{\"1\":{\"101\":1}}],[\"生成固定长度的特征向量\",{\"1\":{\"53\":1}}],[\"生成点集的划分\",{\"1\":{\"43\":1}}],[\"生成\",{\"1\":{\"39\":1,\"255\":1}}],[\"生成融合特征\",{\"1\":{\"30\":1}}],[\"生成动态卷积核\",{\"1\":{\"27\":1}}],[\"生成的耗时\",{\"1\":{\"285\":1}}],[\"生成的内容往往缺乏明确的信息来源\",{\"1\":{\"283\":1}}],[\"生成的文本特征相当于分类器的权重\",{\"1\":{\"91\":1}}],[\"生成的多样化问题\",{\"1\":{\"26\":1}}],[\"生成的问题遵循以下三个关键原则\",{\"1\":{\"20\":1}}],[\"生成最终的\",{\"1\":{\"16\":1}}],[\"跨模态大模型\",{\"1\":{\"278\":1}}],[\"跨模态注意力机制\",{\"1\":{\"33\":1}}],[\"跨模态注意力矩阵\",{\"1\":{\"11\":1}}],[\"版\",{\"1\":{\"278\":1}}],[\"版本迭代\",{\"0\":{\"288\":1}}],[\"版本\",{\"1\":{\"93\":1,\"108\":1,\"140\":1,\"278\":4,\"291\":1}}],[\"版本代码\",{\"1\":{\"42\":2,\"59\":2}}],[\"费用最高\",{\"1\":{\"278\":1}}],[\"领略强化学习之力\",{\"1\":{\"292\":1}}],[\"领先推理力\",{\"1\":{\"278\":1}}],[\"领域专业知识能力欠缺\",{\"1\":{\"283\":1}}],[\"领域常用的文本transformer模型\",{\"1\":{\"90\":1}}],[\"领域中的一些对比学习方法\",{\"1\":{\"89\":1}}],[\"领域\",{\"1\":{\"3\":1,\"223\":1}}],[\"速度越慢\",{\"1\":{\"283\":1}}],[\"速度最快\",{\"1\":{\"278\":1}}],[\"速度快\",{\"1\":{\"278\":1}}],[\"日\",{\"1\":{\"278\":6,\"288\":1}}],[\"日发布\",{\"1\":{\"278\":1}}],[\"于\",{\"1\":{\"278\":4}}],[\"离职人员创建的\",{\"1\":{\"278\":1}}],[\"专用代码生成模型\",{\"1\":{\"278\":1}}],[\"专业版\",{\"1\":{\"278\":2}}],[\"专注于\",{\"1\":{\"289\":1}}],[\"专注于第三方集成\",{\"1\":{\"289\":1}}],[\"专注于复杂问题求解和精确推理能力\",{\"1\":{\"278\":1}}],[\"专注于精确推理和复杂问题求解\",{\"1\":{\"278\":1}}],[\"专注于广泛知识覆盖和流畅对话体验\",{\"1\":{\"278\":1}}],[\"专门对话\",{\"1\":{\"278\":1}}],[\"专门用于预测一个\",{\"1\":{\"64\":1}}],[\"慢\",{\"1\":{\"278\":1}}],[\"识别错误并主动纠正\",{\"1\":{\"278\":1}}],[\"识别点云中的功能区域\",{\"1\":{\"28\":1}}],[\"性价比最高\",{\"1\":{\"278\":1}}],[\"性价比高\",{\"1\":{\"278\":1}}],[\"性能得到进一步提升\",{\"1\":{\"278\":1}}],[\"性能对标\",{\"1\":{\"278\":1}}],[\"性能强\",{\"1\":{\"278\":1}}],[\"性能强大\",{\"1\":{\"278\":1}}],[\"性能最强\",{\"1\":{\"278\":1}}],[\"性能有了进一步提升\",{\"1\":{\"278\":1}}],[\"性能\",{\"1\":{\"98\":1}}],[\"价格更加亲民\",{\"1\":{\"278\":1}}],[\"情感表达\",{\"1\":{\"278\":1}}],[\"情感分析\",{\"1\":{\"213\":1}}],[\"快约\",{\"1\":{\"278\":1}}],[\"快速给出答案\",{\"1\":{\"197\":1}}],[\"快速找到合适的模型\",{\"1\":{\"105\":1}}],[\"规模显著扩大\",{\"1\":{\"278\":1}}],[\"规模一致性\",{\"1\":{\"47\":1}}],[\"帮助我们处理各种任务\",{\"1\":{\"286\":1}}],[\"帮助用户理解推理步骤\",{\"1\":{\"278\":1}}],[\"帮助训练稳定收敛\",{\"1\":{\"64\":1}}],[\"满足不同场景需求\",{\"1\":{\"278\":1}}],[\"允许用户无需编程创建专用的\",{\"1\":{\"278\":1}}],[\"允许开发者创建工具扩展\",{\"1\":{\"278\":1}}],[\"允许我们通过优化适应过程中密集层变化的秩分解矩阵\",{\"1\":{\"188\":1}}],[\"公司开发的闭源语言大模型\",{\"1\":{\"278\":1}}],[\"公司在\",{\"1\":{\"278\":1}}],[\"公式表示如下\",{\"1\":{\"189\":1}}],[\"公式转化为tanimoto系数\",{\"1\":{\"170\":1}}],[\"公式\",{\"1\":{\"167\":2}}],[\"公式如下\",{\"1\":{\"39\":1,\"166\":1,\"207\":1}}],[\"公式来自论文\",{\"1\":{\"35\":1}}],[\"截止\",{\"1\":{\"278\":1}}],[\"截断过长文本\",{\"1\":{\"10\":1}}],[\"月比较有影响力并且模型参数量超过\",{\"1\":{\"278\":1}}],[\"月\",{\"1\":{\"278\":50,\"288\":1}}],[\"月之前\",{\"1\":{\"277\":1}}],[\"月发布的\",{\"1\":{\"88\":1}}],[\"世界知识的语义\",{\"1\":{\"278\":1}}],[\"世界坐标系和局部坐标系\",{\"1\":{\"43\":1}}],[\"世纪\",{\"1\":{\"277\":1}}],[\"称之为\",{\"1\":{\"277\":1}}],[\"称为混淆矩阵\",{\"1\":{\"151\":1}}],[\"称为随机输入丢弃\",{\"1\":{\"52\":1}}],[\"科研界给这些庞大的语言模型起了个名字\",{\"1\":{\"277\":1}}],[\"科学问答\",{\"1\":{\"81\":1}}],[\"科学问答等实际应用任务\",{\"1\":{\"79\":1}}],[\"涌现能力可以与某些复杂任务有关\",{\"1\":{\"280\":1}}],[\"涌现能力就像是模型性能随着规模增大而迅速提升\",{\"1\":{\"280\":1}}],[\"涌现能力是一种令人惊讶的能力\",{\"1\":{\"280\":1}}],[\"涌现能力\",{\"1\":{\"277\":1,\"280\":1}}],[\"豆包\",{\"1\":{\"277\":1}}],[\"国内外有超过百种大模型相继发布\",{\"1\":{\"278\":1}}],[\"国内的有\",{\"1\":{\"277\":1}}],[\"国外的知名\",{\"1\":{\"277\":1}}],[\"吴恩达\",{\"0\":{\"275\":1}}],[\"各需要一个\",{\"1\":{\"266\":1,\"269\":1}}],[\"各类别的权重\",{\"1\":{\"166\":1}}],[\"屏蔽填充部分的信息\",{\"1\":{\"261\":1}}],[\"机器翻译实战\",{\"1\":{\"258\":1}}],[\"机翻\",{\"1\":{\"204\":1}}],[\"托尔斯泰\",{\"1\":{\"257\":1}}],[\"莎士比亚\",{\"1\":{\"257\":1}}],[\"莎士比亚是英国文学史上最伟大的作家之一\",{\"1\":{\"253\":1}}],[\"歌德\",{\"1\":{\"257\":1}}],[\"雨果\",{\"1\":{\"257\":1}}],[\"阅读理解任务\",{\"1\":{\"257\":1}}],[\"命名实体识别\",{\"1\":{\"256\":1}}],[\"命令行工具\",{\"1\":{\"289\":1}}],[\"命令后加上\",{\"1\":{\"40\":1}}],[\"命令中\",{\"1\":{\"40\":1}}],[\"抽取式\",{\"1\":{\"255\":1}}],[\"抽取式问答\",{\"1\":{\"255\":1}}],[\"抽象\",{\"1\":{\"64\":1,\"66\":1}}],[\"抽象点集或局部特征\",{\"1\":{\"43\":2}}],[\"子层的输入进行了\",{\"1\":{\"278\":1}}],[\"子序列\",{\"1\":{\"255\":1}}],[\"子目录名的格式\",{\"1\":{\"93\":1}}],[\"新的文本\",{\"1\":{\"255\":1}}],[\"新的问题出现了\",{\"1\":{\"96\":1}}],[\"麦克白\",{\"1\":{\"253\":1}}],[\"他们希望利用\",{\"1\":{\"286\":1}}],[\"他写了包括\",{\"1\":{\"253\":1}}],[\"他利用自己已掌握的知识\",{\"1\":{\"197\":1}}],[\"哈姆雷特\",{\"1\":{\"253\":2,\"257\":1}}],[\"谁写了\",{\"1\":{\"253\":1,\"257\":1}}],[\"谁离我越近\",{\"1\":{\"57\":1}}],[\"典型的涌现能力\",{\"1\":{\"280\":1}}],[\"典型的输入是一个包含\",{\"1\":{\"253\":1}}],[\"典型值\",{\"1\":{\"169\":1}}],[\"态\",{\"1\":{\"233\":1}}],[\"破\",{\"1\":{\"233\":1}}],[\"突\",{\"1\":{\"233\":1}}],[\"突破transformer缺少归纳偏置的限制\",{\"1\":{\"105\":1}}],[\"票\",{\"1\":{\"233\":1}}],[\"股\",{\"1\":{\"233\":1}}],[\"股票中的突破形态\",{\"1\":{\"233\":2}}],[\"借助于海量无标注数据的训练\",{\"1\":{\"281\":1}}],[\"借助\",{\"1\":{\"278\":1,\"288\":1}}],[\"借助字典映射为word\",{\"1\":{\"233\":1}}],[\"借助对比学习机制\",{\"1\":{\"89\":1}}],[\"横着看是计算某个词与全局序列中其他词的相关度\",{\"1\":{\"230\":1}}],[\"列方向\",{\"1\":{\"227\":2}}],[\"列表\",{\"1\":{\"224\":1}}],[\"列表最后一个记录了缓存的key和value\",{\"1\":{\"103\":2}}],[\"恢复被掩码的词\",{\"1\":{\"226\":1}}],[\"恢复张量格式为\",{\"1\":{\"57\":1}}],[\"依次通过每个编码器层\",{\"1\":{\"226\":1}}],[\"依赖高质量的训练数\",{\"1\":{\"285\":1}}],[\"依赖于构建高质量的数据集\",{\"1\":{\"285\":1}}],[\"依赖\",{\"1\":{\"147\":2}}],[\"依赖视角选择\",{\"1\":{\"71\":1}}],[\"依赖初始点和距离度量方式的选择\",{\"1\":{\"46\":1}}],[\"出来\",{\"1\":{\"255\":1}}],[\"出被掩码位置的表示\",{\"1\":{\"226\":1}}],[\"出于方便\",{\"1\":{\"224\":1}}],[\"出现\",{\"1\":{\"172\":1}}],[\"出现的小常数\",{\"1\":{\"172\":1}}],[\"出现这种差异的原因不难理解\",{\"1\":{\"96\":1}}],[\"出现了一些基于自监督的方法\",{\"1\":{\"96\":1}}],[\"跳过这对\",{\"1\":{\"224\":1}}],[\"索引\",{\"1\":{\"224\":1}}],[\"索引保存下来\",{\"1\":{\"49\":1}}],[\"临时替换特殊标记\",{\"1\":{\"224\":1}}],[\"切分成一个句子列表\",{\"1\":{\"223\":1}}],[\"切换也非常方便\",{\"1\":{\"184\":1}}],[\"切换\",{\"0\":{\"141\":1}}],[\"算法中经常用到的预训练模型\",{\"1\":{\"223\":1}}],[\"算法预训练过程完整代码如下\",{\"1\":{\"175\":1}}],[\"算法预训练工作流程\",{\"1\":{\"175\":1}}],[\"库和\",{\"1\":{\"223\":1}}],[\"库来实现\",{\"1\":{\"192\":1}}],[\"亿用户的增长\",{\"1\":{\"278\":1}}],[\"亿参数\",{\"1\":{\"278\":1}}],[\"亿参数的\",{\"1\":{\"277\":2}}],[\"亿的大语言模型\",{\"1\":{\"278\":1}}],[\"亿\",{\"1\":{\"277\":2}}],[\"亿个语句的数据合集\",{\"1\":{\"223\":1}}],[\"亿张图像\",{\"1\":{\"118\":1}}],[\"英语词库数据\",{\"1\":{\"223\":1}}],[\"英文全称\",{\"1\":{\"39\":1}}],[\"体验优化\",{\"1\":{\"291\":1}}],[\"体验bert的预训练过程是如何实现的\",{\"1\":{\"222\":1}}],[\"体素网格\",{\"1\":{\"71\":1}}],[\"体素\",{\"1\":{\"69\":1}}],[\"怎么办\",{\"1\":{\"221\":1}}],[\"看哪一个输出的值最大\",{\"1\":{\"221\":1}}],[\"看不清细节\",{\"1\":{\"69\":1}}],[\"看不懂下面两行代码的话\",{\"1\":{\"49\":1}}],[\"落在文章的第\",{\"1\":{\"221\":1}}],[\"送入\",{\"1\":{\"221\":1}}],[\"送入模型中\",{\"1\":{\"221\":1}}],[\"送入llm时\",{\"1\":{\"104\":1}}],[\"答案必须是原文中的一段文本\",{\"1\":{\"255\":1}}],[\"答案必须是原文中的连续片段\",{\"1\":{\"255\":1}}],[\"答案可以是任意文本\",{\"1\":{\"255\":1}}],[\"答案是原文中的一段\",{\"1\":{\"255\":1}}],[\"答案是否必须在原文中\",{\"1\":{\"255\":1}}],[\"答案是有的\",{\"1\":{\"188\":1}}],[\"答案应该在这段文字中的第\",{\"1\":{\"255\":1}}],[\"答案就是\",{\"1\":{\"221\":1}}],[\"答案一定会出现在文章中\",{\"1\":{\"221\":1}}],[\"差不多\",{\"1\":{\"221\":1}}],[\"错误还是不知道\",{\"1\":{\"221\":1}}],[\"错位对齐\",{\"1\":{\"103\":1}}],[\"向量化构建出个性化数据库\",{\"1\":{\"291\":1}}],[\"向量数据库\",{\"1\":{\"287\":1}}],[\"向量映射成两个分数\",{\"1\":{\"253\":1}}],[\"向量中\",{\"1\":{\"221\":1}}],[\"向量的长度\",{\"1\":{\"115\":1}}],[\"里面肯定含有整句话的完整信息\",{\"1\":{\"221\":1}}],[\"里面包含了注意力机制\",{\"1\":{\"116\":1}}],[\"里面包含了非常丰富的成果\",{\"1\":{\"116\":1}}],[\"丢给\",{\"1\":{\"221\":1}}],[\"丢失部分几何信息\",{\"1\":{\"71\":1}}],[\"企鹅不擅长飞行\",{\"1\":{\"219\":2}}],[\"企业又有比较好的自有数据\",{\"1\":{\"179\":1}}],[\"纠错\",{\"1\":{\"218\":1}}],[\"甚至根据上下文进行\",{\"1\":{\"218\":1}}],[\"字典的构建过程太过粗糙\",{\"1\":{\"224\":1}}],[\"字典初始化\",{\"1\":{\"224\":1}}],[\"字或词\",{\"1\":{\"218\":1}}],[\"字节对编码\",{\"1\":{\"174\":1}}],[\"人力\",{\"1\":{\"217\":1}}],[\"人类反馈强化学习\",{\"1\":{\"78\":1}}],[\"炼成\",{\"1\":{\"217\":1}}],[\"源掩码\",{\"1\":{\"270\":1}}],[\"源注意力子层\",{\"1\":{\"269\":2}}],[\"源码链接\",{\"1\":{\"216\":1}}],[\"源自二战雷达检测\",{\"1\":{\"159\":1}}],[\"零\",{\"0\":{\"216\":1,\"222\":1},\"1\":{\"216\":1,\"222\":1}}],[\"零样本表现\",{\"1\":{\"213\":1}}],[\"蕴含确定和文本分类\",{\"1\":{\"214\":1}}],[\"威诺格拉德模式\",{\"1\":{\"213\":1}}],[\"潜在生成式模型\",{\"1\":{\"213\":1}}],[\"额外的提升\",{\"1\":{\"213\":1}}],[\"额外通道数\",{\"1\":{\"16\":1}}],[\"判定其是否语法正确\",{\"1\":{\"212\":1}}],[\"判断下一个句子是否是当前句子的后续句子\",{\"1\":{\"226\":1}}],[\"判断第\",{\"1\":{\"217\":1}}],[\"判断它们之间的关系\",{\"1\":{\"212\":1}}],[\"判断没饿过字符对是否存在于频次表中\",{\"1\":{\"176\":1}}],[\"判断是否为交叉注意力\",{\"1\":{\"103\":1}}],[\"判断预测是否正确\",{\"1\":{\"93\":1}}],[\"判断者\",{\"1\":{\"82\":1}}],[\"判断模型是否能正确区分前景和背景\",{\"1\":{\"39\":1}}],[\"纽约大学发布的有关语法的数据集\",{\"1\":{\"212\":1}}],[\"评判两个文本语义信息的相似度\",{\"1\":{\"212\":1}}],[\"评估阶段则是在验证集或测试集上评估模型的性能\",{\"1\":{\"39\":1}}],[\"评估\",{\"0\":{\"39\":1},\"1\":{\"36\":1}}],[\"评估集\",{\"1\":{\"25\":1}}],[\"美国知识问答网站\",{\"1\":{\"212\":1}}],[\"挑战在于识别语句是否是概念改写\",{\"1\":{\"212\":1}}],[\"证明模型具有有效处理上下文长距离的信息的能力\",{\"1\":{\"212\":1}}],[\"证明其确实为下游任务获取到了有用的语言知识\",{\"1\":{\"204\":1}}],[\"矛盾或中立\",{\"1\":{\"212\":1}}],[\"涉及读取一对句子\",{\"1\":{\"212\":1}}],[\"监督微调\",{\"0\":{\"212\":1}}],[\"监督式微调sft\",{\"1\":{\"180\":1}}],[\"批大小为\",{\"1\":{\"211\":1}}],[\"批次索引\",{\"1\":{\"49\":1}}],[\"残差链接\",{\"1\":{\"245\":1}}],[\"残差\",{\"1\":{\"211\":1}}],[\"残差连接与层归一化\",{\"1\":{\"261\":1}}],[\"残差连接\",{\"1\":{\"112\":2,\"265\":1}}],[\"轮训练就足够了\",{\"1\":{\"211\":1}}],[\"轮\",{\"1\":{\"211\":1}}],[\"连续包月优惠价为\",{\"1\":{\"278\":1}}],[\"连续\",{\"1\":{\"211\":1}}],[\"连接起来\",{\"1\":{\"104\":1}}],[\"连接图像特征和语言嵌入空间的线性层\",{\"1\":{\"81\":1}}],[\"连接视觉特征和语言嵌入空间\",{\"1\":{\"80\":1}}],[\"线性上升更新\",{\"1\":{\"211\":1}}],[\"线性投影到与\",{\"1\":{\"104\":1}}],[\"困惑度\",{\"1\":{\"211\":1}}],[\"长文本处理能力较弱\",{\"1\":{\"283\":1}}],[\"长文本对话模型\",{\"1\":{\"278\":1}}],[\"长文本理解和推理能力\",{\"1\":{\"278\":1}}],[\"长文本能让生成模型学习到长依赖信息的条件概率\",{\"1\":{\"211\":1}}],[\"长边按比例缩放\",{\"1\":{\"108\":1}}],[\"符来分隔两者\",{\"1\":{\"209\":1}}],[\"符合我们直接观念所想的大模型微调流程为\",{\"1\":{\"187\":1}}],[\"符合认知的大模型微调流程\",{\"0\":{\"187\":1}}],[\"$\",{\"1\":{\"209\":1}}],[\"观测发现用辅助目标能提升性能\",{\"1\":{\"208\":1}}],[\"框架进行开发\",{\"1\":{\"291\":1}}],[\"框架交互\",{\"1\":{\"289\":1}}],[\"框架的核心库\",{\"1\":{\"289\":1}}],[\"框架可以实现数据感知和环境互动\",{\"1\":{\"286\":1}}],[\"框架是一个开源工具\",{\"1\":{\"286\":1}}],[\"框架\",{\"0\":{\"206\":1},\"1\":{\"286\":2}}],[\"框架按照\",{\"1\":{\"99\":1}}],[\"段落级别或者句子级别的\",{\"1\":{\"205\":1}}],[\"句子级别表示\",{\"1\":{\"253\":1}}],[\"句子嵌入\",{\"1\":{\"226\":2}}],[\"句子分隔列表\",{\"1\":{\"225\":1}}],[\"句子中20\",{\"1\":{\"225\":1}}],[\"句子\",{\"1\":{\"223\":1}}],[\"句法解析\",{\"1\":{\"204\":1}}],[\"句号等标点符号结合语义进行断句\",{\"1\":{\"175\":1}}],[\"迁移它来稍微适应一系列广泛的任务\",{\"1\":{\"204\":1}}],[\"什么样的优化目标是最高效的迁移\",{\"1\":{\"204\":1}}],[\"什么是prompt\",{\"0\":{\"194\":1}}],[\"什么是大模型\",{\"0\":{\"178\":1}}],[\"问答系统中的候选答案选择\",{\"1\":{\"257\":1}}],[\"问答任务\",{\"0\":{\"253\":1},\"1\":{\"221\":2}}],[\"问答和常识推理\",{\"1\":{\"209\":1,\"212\":1}}],[\"问答或文本蕴含\",{\"1\":{\"209\":1}}],[\"问答提升5\",{\"1\":{\"204\":1}}],[\"问答\",{\"1\":{\"203\":1,\"204\":1,\"213\":1}}],[\"问题分解能力\",{\"1\":{\"278\":1}}],[\"问题和答案\",{\"1\":{\"209\":1}}],[\"问题的损失函数改进方案\",{\"1\":{\"169\":1}}],[\"问题来了\",{\"1\":{\"96\":1}}],[\"问题\",{\"1\":{\"85\":1,\"174\":1,\"253\":3,\"255\":1,\"257\":2}}],[\"问题所在\",{\"1\":{\"69\":1}}],[\"问题背景\",{\"1\":{\"60\":1}}],[\"问题嵌入\",{\"1\":{\"33\":1}}],[\"问题编码后的文本特征\",{\"1\":{\"29\":1}}],[\"问题条件化查询\",{\"1\":{\"27\":1}}],[\"问题文本不为空\",{\"1\":{\"25\":1}}],[\"问题文本\",{\"1\":{\"25\":1}}],[\"问题数\",{\"1\":{\"22\":1}}],[\"问题总数\",{\"1\":{\"22\":1,\"24\":1}}],[\"问题配对是固定的\",{\"1\":{\"23\":1}}],[\"问题配对\",{\"1\":{\"22\":1,\"24\":1,\"26\":1}}],[\"摘要\",{\"0\":{\"203\":1}}],[\"拆解成一系列的简单问题\",{\"1\":{\"200\":1}}],[\"教llm把复杂问题\",{\"1\":{\"200\":1}}],[\"虚线之下是self\",{\"1\":{\"199\":1}}],[\"虚线之上是标准的cot的过程\",{\"1\":{\"199\":1}}],[\"绿色标记出的部分是llm输出的推理过程\",{\"1\":{\"198\":1}}],[\"左侧是常规的prompt\",{\"1\":{\"198\":1}}],[\"左侧为transformer原始的encoder结构\",{\"1\":{\"112\":1}}],[\"思忖未知之界\",{\"1\":{\"292\":1}}],[\"思维链\",{\"1\":{\"280\":1}}],[\"思维链展示\",{\"1\":{\"278\":2}}],[\"思维链技术\",{\"0\":{\"198\":1}}],[\"思考的时间更短\",{\"1\":{\"278\":1}}],[\"思考\",{\"1\":{\"197\":1}}],[\"思考快与慢\",{\"1\":{\"197\":1}}],[\"背后的大致逻辑\",{\"1\":{\"197\":1}}],[\"背景类别极端不平衡的问题\",{\"1\":{\"169\":1}}],[\"背景知识扫盲\",{\"0\":{\"70\":1}}],[\"背景点\",{\"1\":{\"35\":1}}],[\"背景\",{\"0\":{\"5\":1,\"43\":1,\"78\":1,\"98\":1,\"260\":1},\"1\":{\"40\":1,\"169\":1}}],[\"举例来说\",{\"1\":{\"221\":1}}],[\"举例说明\",{\"1\":{\"69\":1}}],[\"举个例子\",{\"1\":{\"196\":1,\"255\":1}}],[\"达到非常低的\",{\"1\":{\"211\":1}}],[\"达到四两拨千斤的效果\",{\"1\":{\"192\":1}}],[\"达到了\",{\"1\":{\"81\":1}}],[\"秩的选择\",{\"0\":{\"191\":1},\"1\":{\"191\":1}}],[\"旁添加一条旁路\",{\"1\":{\"189\":1}}],[\"很多简单的任务\",{\"1\":{\"194\":1}}],[\"很容易导致旧知识遗忘\",{\"1\":{\"188\":1}}],[\"很难在这种情况下保持分类的一致性\",{\"1\":{\"69\":1}}],[\"且必须按顺序处理内容\",{\"1\":{\"283\":1}}],[\"且预留给\",{\"1\":{\"188\":1}}],[\"且真实也为正类的样本数\",{\"1\":{\"170\":1}}],[\"冻结预训练模型参数\",{\"1\":{\"187\":1}}],[\"冻结参数的image\",{\"1\":{\"100\":1}}],[\"远小于原始矩阵的维度\",{\"1\":{\"187\":1}}],[\"待微调的参数量下降到原来的9\",{\"1\":{\"187\":1}}],[\"待完善\",{\"0\":{\"8\":1}}],[\"级别的参数量\",{\"1\":{\"187\":1}}],[\"占据了特别大的内存资源和计算资源\",{\"1\":{\"187\":1}}],[\"感兴趣的朋友\",{\"1\":{\"185\":1}}],[\"限于篇幅原因\",{\"1\":{\"185\":1}}],[\"极大地丰富了\",{\"1\":{\"289\":1}}],[\"极大地提升了开发效率和应用性能\",{\"1\":{\"288\":1}}],[\"极大地提升了模型在各种自然语言处理任务上的表现\",{\"1\":{\"277\":1}}],[\"极大地降低了在各种技术栈上构建\",{\"1\":{\"288\":1}}],[\"极大地降低成本\",{\"1\":{\"185\":1}}],[\"极大地简化了调试和问题排查的流程\",{\"1\":{\"288\":1}}],[\"极其高效\",{\"1\":{\"69\":1}}],[\"∆w\",{\"1\":{\"184\":3}}],[\"∆w为m\",{\"1\":{\"184\":1}}],[\"往输入序列x前面加特定的token\",{\"1\":{\"183\":1}}],[\"往往远多于正样本\",{\"1\":{\"169\":1}}],[\"跟prompt\",{\"1\":{\"183\":1}}],[\"尽可能地提升大模型在特定领域的能力\",{\"1\":{\"180\":1}}],[\"尽管大型语言模型的调用相对简单\",{\"1\":{\"286\":1}}],[\"尽管\",{\"1\":{\"279\":1}}],[\"尽管这些大型语言模型与小型语言模型\",{\"1\":{\"277\":1}}],[\"尽管该模型毫无用处\",{\"1\":{\"152\":1}}],[\"尽管谷歌基于jft\",{\"1\":{\"96\":1}}],[\"尽管clip是一个多模态模型\",{\"1\":{\"90\":1}}],[\"另外\",{\"1\":{\"179\":1,\"184\":1,\"196\":1}}],[\"另一个是作为答案结束的可能性\",{\"1\":{\"253\":1}}],[\"另一个技巧\",{\"1\":{\"197\":1}}],[\"另一个原因是nlp模型可以利用从互联网上收集的大量文本\",{\"1\":{\"96\":1}}],[\"另一种思路是在转换后\",{\"1\":{\"111\":1}}],[\"另一种是基于\",{\"1\":{\"90\":1}}],[\"另一部分特征是通过在当前分辨率直接对所有原始点应用单个pointnet得到的\",{\"1\":{\"54\":1}}],[\"现已更新到\",{\"1\":{\"278\":1}}],[\"现在假设使用\",{\"1\":{\"221\":1}}],[\"现在的大模型要解决的问题\",{\"1\":{\"178\":1}}],[\"现就读于电子科技大学\",{\"1\":{\"3\":1}}],[\"现就读于四川大学\",{\"1\":{\"2\":1}}],[\"词性标注\",{\"1\":{\"256\":1}}],[\"词表大小\",{\"1\":{\"226\":1}}],[\"词库中的词汇数量\",{\"1\":{\"223\":1}}],[\"词汇和句法歧义\",{\"1\":{\"212\":1}}],[\"词汇表\",{\"1\":{\"211\":1}}],[\"词汇表与正则化\",{\"1\":{\"211\":1}}],[\"词嵌入\",{\"1\":{\"205\":1,\"226\":2}}],[\"词嵌入与位置嵌入相加\",{\"1\":{\"102\":1}}],[\"词频从高到低\",{\"1\":{\"175\":1}}],[\"必须落在上下文部分\",{\"1\":{\"255\":1}}],[\"必须为两个独立的词\",{\"1\":{\"175\":1}}],[\"必须自己学会对各种姿态都识别准确\",{\"1\":{\"64\":1}}],[\"经济\",{\"1\":{\"278\":1}}],[\"经历n次迭代\",{\"1\":{\"175\":1}}],[\"经过一层全连接后的输出\",{\"1\":{\"253\":1}}],[\"经过相同的非线性变换后\",{\"1\":{\"226\":1}}],[\"经过sigmoid\",{\"1\":{\"169\":1}}],[\"经过encoder之后\",{\"1\":{\"110\":1}}],[\"经过处理后的图像块嵌入张量\",{\"1\":{\"109\":1}}],[\"经过卷积层变成\",{\"1\":{\"109\":1}}],[\"经过预处理的图像和对应的标签\",{\"1\":{\"107\":1}}],[\"经过全局池化后得到一个全局特征向量\",{\"1\":{\"64\":1}}],[\"经过几层\",{\"1\":{\"57\":1}}],[\"经过mlp进一步提取和融合特征\",{\"1\":{\"57\":1}}],[\"经过\",{\"1\":{\"13\":1,\"15\":2,\"33\":1,\"172\":1,\"255\":1}}],[\"过长截断策略\",{\"1\":{\"233\":1}}],[\"过滤掉句子数少于2的行\",{\"1\":{\"223\":1}}],[\"过滤掉vocab中的低频词\",{\"1\":{\"175\":1}}],[\"过滤无效行\",{\"1\":{\"223\":1}}],[\"过程\",{\"1\":{\"27\":1}}],[\"读取训练语料\",{\"1\":{\"175\":1}}],[\"读取图片并将其转换为合适的格式后\",{\"1\":{\"93\":1}}],[\"惩罚更重\",{\"1\":{\"172\":2}}],[\"超过了随机水平\",{\"1\":{\"280\":1}}],[\"超过历史最佳\",{\"1\":{\"39\":1}}],[\"超轻量推理\",{\"1\":{\"278\":1}}],[\"超强推理能力\",{\"1\":{\"278\":1}}],[\"超长截断\",{\"1\":{\"233\":1}}],[\"超参数调整\",{\"1\":{\"173\":1}}],[\"超参数设置说明\",{\"1\":{\"172\":1}}],[\"误检\",{\"1\":{\"170\":1}}],[\"误报\",{\"1\":{\"162\":1,\"170\":1}}],[\"误报很可怕\",{\"1\":{\"156\":1}}],[\"误报概率\",{\"0\":{\"154\":1}}],[\"漏字填空\",{\"1\":{\"217\":1}}],[\"漏检\",{\"1\":{\"170\":1}}],[\"漏报\",{\"1\":{\"170\":1}}],[\"漏掉的真正例\",{\"1\":{\"162\":1}}],[\"β=0\",{\"1\":{\"170\":2}}],[\"β>α\",{\"1\":{\"170\":1}}],[\"β\",{\"1\":{\"170\":3,\"172\":3}}],[\"求\",{\"1\":{\"169\":1}}],[\"求和\",{\"1\":{\"33\":1,\"72\":1}}],[\"^γ\",{\"1\":{\"169\":1}}],[\"动态调整样本权重\",{\"1\":{\"169\":1}}],[\"动态卷积核\",{\"1\":{\"33\":1}}],[\"动态卷积\",{\"1\":{\"33\":2}}],[\"兼容性与支持\",{\"1\":{\"288\":1}}],[\"兼容大多数分类模型\",{\"1\":{\"169\":1}}],[\"兼顾了对\",{\"1\":{\"288\":1}}],[\"兼顾像素级精度和区域重叠度\",{\"1\":{\"172\":1}}],[\"兼顾这两者\",{\"1\":{\"156\":1}}],[\"罕见疾病诊断等\",{\"1\":{\"169\":1}}],[\"任何类别不平衡的分类任务\",{\"1\":{\"169\":1}}],[\"任务损失\",{\"1\":{\"227\":2}}],[\"任务会利用\",{\"1\":{\"226\":2}}],[\"任务的本质\",{\"1\":{\"255\":1}}],[\"任务的通用架构该有多好\",{\"1\":{\"217\":1}}],[\"任务的评估体系\",{\"1\":{\"39\":1}}],[\"任务通常需要不同的模型\",{\"1\":{\"217\":1}}],[\"任务涉及预测两个句子在语义上是否相等\",{\"1\":{\"212\":1}}],[\"任务需求\",{\"1\":{\"173\":1}}],[\"任务\",{\"1\":{\"39\":1,\"217\":1,\"226\":3}}],[\"任务中表现出色\",{\"1\":{\"279\":1}}],[\"任务中用于生成功能区域掩码的核心模块\",{\"1\":{\"33\":1}}],[\"任务中\",{\"1\":{\"28\":1,\"257\":1}}],[\"病灶区域像素远少于正常组织\",{\"1\":{\"169\":1}}],[\"医学图像分割\",{\"1\":{\"169\":1}}],[\"倍\",{\"1\":{\"169\":1,\"278\":2}}],[\"退化为ce\",{\"1\":{\"169\":1}}],[\"退出当前环境\",{\"0\":{\"142\":1}}],[\"综述\",{\"1\":{\"292\":1}}],[\"综上\",{\"1\":{\"169\":1}}],[\"综合最近几个熟人的意见\",{\"1\":{\"57\":1}}],[\"被认为是\",{\"1\":{\"282\":1}}],[\"被证明在使用指令形式化描述的未见过的任务上表现良好\",{\"1\":{\"280\":1}}],[\"被掩码后的输入序列\",{\"1\":{\"225\":1}}],[\"被背景淹没\",{\"1\":{\"169\":1}}],[\"被添加到\",{\"1\":{\"110\":1}}],[\"α=0\",{\"1\":{\"170\":2}}],[\"α>β\",{\"1\":{\"170\":1}}],[\"α\",{\"1\":{\"169\":4,\"170\":3,\"172\":6}}],[\"低成本的\",{\"1\":{\"290\":1}}],[\"低延迟\",{\"1\":{\"278\":1}}],[\"低秩分解\",{\"1\":{\"187\":1}}],[\"低置信度\",{\"1\":{\"169\":1}}],[\"低密度区域则过于稀缺\",{\"1\":{\"46\":1}}],[\"降低了产生幻觉的概率\",{\"1\":{\"285\":1}}],[\"降低幻觉\",{\"1\":{\"285\":1}}],[\"降低易分类样本的权重\",{\"1\":{\"169\":1}}],[\"降为用4bit来表示\",{\"1\":{\"185\":1}}],[\"降训练成本\",{\"1\":{\"185\":1}}],[\"降维适配器\",{\"0\":{\"14\":1},\"1\":{\"10\":1,\"14\":1}}],[\"近期研究考虑过各种各样的目标\",{\"1\":{\"204\":1}}],[\"近代自然语言处理技术发展的\",{\"1\":{\"185\":1}}],[\"近似于\",{\"1\":{\"168\":1}}],[\"近年来\",{\"1\":{\"96\":1}}],[\"组合损失\",{\"1\":{\"167\":1}}],[\"组合损失值\",{\"1\":{\"167\":1}}],[\"组合后的优势\",{\"1\":{\"167\":1}}],[\"组合后的图像张量和标签张量\",{\"1\":{\"107\":1}}],[\"已添加\",{\"1\":{\"226\":1}}],[\"已有的技术涉及对模型架构进行特定任务的修改\",{\"1\":{\"204\":1}}],[\"已弃用\",{\"1\":{\"167\":1,\"168\":1,\"169\":1}}],[\"已经在许多领域产生了深远的影响\",{\"1\":{\"282\":1}}],[\"已经在多个领域展示了潜力\",{\"1\":{\"279\":1}}],[\"已经扩展到支持多模态数据\",{\"1\":{\"279\":1}}],[\"已经开源了\",{\"1\":{\"278\":1}}],[\"已经能够满足个人用户或小型企业的大部分需求\",{\"1\":{\"278\":1}}],[\"已经经过\",{\"1\":{\"167\":1}}],[\"已经过预训练以提取语言信息视觉表示\",{\"1\":{\"104\":1}}],[\"已经接近其上限\",{\"1\":{\"82\":1}}],[\"起主导作用\",{\"1\":{\"167\":1}}],[\"抗类别不平衡能力强\",{\"1\":{\"167\":1}}],[\"值得大力推广\",{\"1\":{\"281\":1}}],[\"值变大并不能提升微调的效果\",{\"1\":{\"191\":1}}],[\"值为\",{\"1\":{\"172\":1}}],[\"值越大\",{\"1\":{\"169\":1}}],[\"值越小表示匹配越好\",{\"1\":{\"166\":1}}],[\"值\",{\"1\":{\"168\":1,\"169\":1}}],[\"值向量\",{\"1\":{\"128\":1}}],[\"ϵ\",{\"1\":{\"166\":1}}],[\"越具体\",{\"1\":{\"196\":1}}],[\"越明确\",{\"1\":{\"196\":1}}],[\"越长的prompt\",{\"1\":{\"179\":1}}],[\"越大\",{\"1\":{\"170\":2,\"172\":1}}],[\"越大越好\",{\"1\":{\"166\":1,\"168\":1}}],[\"越小\",{\"1\":{\"168\":1}}],[\"越接近黄色的位置代表越靠近位置编码的中心位置\",{\"1\":{\"111\":1}}],[\"系列均采用\",{\"1\":{\"278\":1}}],[\"系列工作\",{\"1\":{\"278\":1}}],[\"系列基础模型\",{\"1\":{\"278\":1}}],[\"系列基本上是后续大模型的标杆\",{\"1\":{\"278\":1}}],[\"系列在开源社区的影响力和应用前景\",{\"1\":{\"278\":1}}],[\"系列相同\",{\"1\":{\"278\":1}}],[\"系列的三个版本\",{\"1\":{\"278\":1}}],[\"系列模型是\",{\"1\":{\"278\":1}}],[\"系列模型是由\",{\"1\":{\"278\":1}}],[\"系列模型\",{\"1\":{\"278\":2}}],[\"系列语言大模型由\",{\"1\":{\"278\":1}}],[\"系列已形成\",{\"1\":{\"278\":1}}],[\"系列\",{\"1\":{\"277\":1,\"278\":3}}],[\"系统2是慢思考系统\",{\"1\":{\"197\":1}}],[\"系统2\",{\"1\":{\"197\":1}}],[\"系统1是快思考系统\",{\"1\":{\"197\":1}}],[\"系统1\",{\"1\":{\"197\":1}}],[\"系统提示语\",{\"1\":{\"81\":1}}],[\"系数范围是\",{\"1\":{\"166\":1}}],[\"系数衡量的是预测掩码与真实标签之间的相似性\",{\"1\":{\"166\":1}}],[\"系数\",{\"1\":{\"166\":2,\"170\":2}}],[\"语音\",{\"1\":{\"278\":1}}],[\"语句连贯性\",{\"1\":{\"204\":1}}],[\"语义角色标注\",{\"1\":{\"256\":1}}],[\"语义文本相似度数据集\",{\"1\":{\"212\":1}}],[\"语义组块chuking\",{\"1\":{\"205\":1}}],[\"语义相似度\",{\"1\":{\"212\":2}}],[\"语义相似度评估\",{\"1\":{\"203\":1,\"214\":1}}],[\"语义相识度\",{\"1\":{\"204\":1}}],[\"语义分割任务需要对图像中的每个像素进行分类\",{\"1\":{\"164\":1}}],[\"语义分割任务中\",{\"1\":{\"164\":1}}],[\"语义分割可以被看作是像素级别的图像分割\",{\"1\":{\"164\":1}}],[\"语义分割不仅需要识别图像中的物体\",{\"1\":{\"164\":1}}],[\"语义分割是计算机视觉领域中的一项任务\",{\"1\":{\"164\":1}}],[\"语义分割\",{\"0\":{\"164\":1},\"1\":{\"164\":1}}],[\"语义分割中常用的损失函数\",{\"0\":{\"163\":1},\"1\":{\"163\":1}}],[\"语言建模的研究可以追溯到\",{\"1\":{\"277\":1}}],[\"语言可接受性\",{\"1\":{\"213\":1}}],[\"语言查询特征\",{\"1\":{\"33\":1}}],[\"语言含义\",{\"1\":{\"33\":1}}],[\"语言引导的点特征筛选\",{\"1\":{\"29\":1}}],[\"语言引导的\",{\"1\":{\"27\":1,\"39\":1}}],[\"语言引导下的\",{\"1\":{\"17\":1}}],[\"语言后半段\",{\"1\":{\"13\":1}}],[\"语言前半段\",{\"1\":{\"13\":1}}],[\"语言部分的\",{\"1\":{\"13\":1}}],[\"语言\",{\"1\":{\"13\":1}}],[\"语言模型使用多层的\",{\"1\":{\"207\":1}}],[\"语言模型\",{\"1\":{\"12\":1,\"80\":1,\"81\":1,\"278\":1}}],[\"语言指令理解特征\",{\"1\":{\"10\":1}}],[\"较低的阈值\",{\"1\":{\"162\":1}}],[\"较高\",{\"1\":{\"162\":1}}],[\"右填充\",{\"1\":{\"233\":1}}],[\"右侧是cot\",{\"1\":{\"198\":1}}],[\"右侧曲线的\",{\"1\":{\"162\":1}}],[\"右图中\",{\"1\":{\"115\":1}}],[\"轴上绘制所有阈值下的召回率\",{\"1\":{\"161\":1}}],[\"轴上绘制精确率\",{\"1\":{\"161\":1}}],[\"尝试加载已保存的字典\",{\"1\":{\"224\":1}}],[\"尝试确保被标记为垃圾邮件的电子邮件实际上是垃圾邮件\",{\"1\":{\"156\":1}}],[\"尝试使用conda\",{\"1\":{\"147\":1}}],[\"抓取所有垃圾邮件\",{\"1\":{\"156\":1}}],[\"抓取耳机\",{\"1\":{\"22\":1}}],[\"您选择优先考虑的指标取决于特定问题的成本\",{\"1\":{\"156\":1}}],[\"精细的指令遵循\",{\"1\":{\"278\":1}}],[\"精准度高\",{\"1\":{\"278\":1}}],[\"精确率与召回率曲线\",{\"0\":{\"161\":1},\"1\":{\"161\":1}}],[\"精确率和召回率通常呈现反向关系\",{\"1\":{\"155\":1}}],[\"精确率会提高\",{\"1\":{\"155\":1}}],[\"精确率作为指标的意义和实用性较低\",{\"1\":{\"155\":1}}],[\"精确率衡量的是被归类为垃圾邮件且实际上是垃圾邮件的电子邮件所占的比例\",{\"1\":{\"155\":1}}],[\"精确率是指模型所有正类别分类中实际为正类别的分类所占的比例\",{\"1\":{\"155\":1}}],[\"精确率\",{\"0\":{\"155\":1},\"1\":{\"156\":1,\"161\":1}}],[\"精度受限\",{\"1\":{\"71\":1}}],[\"召回率等指标上的表现\",{\"1\":{\"173\":1}}],[\"召回率曲线的创建方法是\",{\"1\":{\"161\":1}}],[\"召回率曲线\",{\"1\":{\"161\":1}}],[\"召回率会提高\",{\"1\":{\"155\":1}}],[\"召回率比准确率更有意义\",{\"1\":{\"153\":1}}],[\"召回率衡量的是被正确分类为垃圾邮件的垃圾邮件电子邮件的比例\",{\"1\":{\"153\":1}}],[\"召回率的定义为\",{\"1\":{\"153\":1}}],[\"召回率\",{\"0\":{\"153\":1},\"1\":{\"156\":1}}],[\"垃圾邮件被误分类为非垃圾邮件\",{\"1\":{\"151\":1}}],[\"垃圾邮件被正确分类为垃圾邮件\",{\"1\":{\"151\":1}}],[\"假阴性\",{\"1\":{\"170\":2}}],[\"假阳性\",{\"1\":{\"170\":2}}],[\"假负例通常比假正例的后果更严重\",{\"1\":{\"153\":1}}],[\"假负例是指被误分类为负例的实际正例\",{\"1\":{\"153\":1}}],[\"假负例或假正例\",{\"1\":{\"152\":1}}],[\"假负例\",{\"1\":{\"151\":1}}],[\"假正例是被错误分类的实际负例\",{\"1\":{\"154\":1}}],[\"假正例率为\",{\"1\":{\"154\":1}}],[\"假正例率\",{\"0\":{\"154\":1},\"1\":{\"154\":1,\"156\":1}}],[\"假正例\",{\"1\":{\"151\":1}}],[\"假设你有一个问题\",{\"1\":{\"257\":1}}],[\"假设原始上下文是\",{\"1\":{\"255\":1}}],[\"假设输入的token序列为\",{\"1\":{\"230\":1}}],[\"假设最终的输出\",{\"1\":{\"221\":1}}],[\"假设有标记数据集\",{\"1\":{\"208\":1}}],[\"假设有一个完美的模型\",{\"1\":{\"155\":1}}],[\"假设预训练的矩阵为\",{\"1\":{\"189\":1}}],[\"假设要在下游任务微调一个预训练语言模型\",{\"1\":{\"189\":1}}],[\"假设模型在任务适配过程中权重的改变量是低秩\",{\"1\":{\"188\":1}}],[\"假设我们有预测概率图\",{\"1\":{\"172\":1}}],[\"假设我们在\",{\"1\":{\"133\":1}}],[\"假设两个正样本\",{\"1\":{\"169\":1}}],[\"假设的理想模型的\",{\"1\":{\"159\":1}}],[\"假设一个完美的模型不会出现假负例\",{\"1\":{\"153\":1}}],[\"假设空间中所有区域的尺度或特征分布具有一定的一致性\",{\"1\":{\"47\":1}}],[\"真阳性\",{\"1\":{\"170\":2}}],[\"真负例\",{\"1\":{\"151\":1}}],[\"真正例率\",{\"0\":{\"153\":1},\"1\":{\"153\":1,\"156\":1}}],[\"真正例\",{\"1\":{\"151\":1}}],[\"真实token对应1\",{\"1\":{\"233\":1}}],[\"真实掩码\",{\"1\":{\"167\":1}}],[\"真实的\",{\"1\":{\"39\":1}}],[\"真实标签\",{\"1\":{\"10\":1,\"39\":1,\"166\":1,\"167\":2,\"170\":1,\"172\":2}}],[\"混淆矩阵\",{\"0\":{\"151\":1}}],[\"混合专家模型\",{\"1\":{\"278\":1}}],[\"混合模型的常见结合方式\",{\"1\":{\"117\":1}}],[\"混合模型探索\",{\"0\":{\"117\":1}}],[\"混合模型改进\",{\"1\":{\"69\":1}}],[\"混合\",{\"1\":{\"28\":1}}],[\"常识和写作能力\",{\"1\":{\"281\":1}}],[\"常识推理提升8\",{\"1\":{\"204\":1}}],[\"常规微调\",{\"1\":{\"187\":1}}],[\"常用的peft方案\",{\"0\":{\"181\":1}}],[\"常用评估指标\",{\"0\":{\"149\":1},\"1\":{\"149\":1}}],[\"常见错误\",{\"0\":{\"148\":1}}],[\"常见疑问解答\",{\"0\":{\"134\":1}}],[\"常见的llm\",{\"0\":{\"278\":1}}],[\"常见的应用场景包括\",{\"1\":{\"256\":1,\"257\":1}}],[\"常见的迁移学习方法是首先在大规模数据集\",{\"1\":{\"96\":1}}],[\"常见的对称函数\",{\"1\":{\"72\":1}}],[\"含special\",{\"1\":{\"233\":1}}],[\"含非\",{\"1\":{\"147\":1}}],[\"含义\",{\"1\":{\"33\":2,\"128\":2}}],[\"纯\",{\"1\":{\"147\":1}}],[\"官方仓库\",{\"1\":{\"147\":1}}],[\"官方实现的default\",{\"1\":{\"107\":1}}],[\"安装失败的包\",{\"1\":{\"147\":1}}],[\"安装所需要的依赖包\",{\"1\":{\"147\":1}}],[\"删除已创建的环境\",{\"0\":{\"144\":1}}],[\"带星号\",{\"1\":{\"143\":1}}],[\"注重解题中间步骤的正确性\",{\"1\":{\"278\":1}}],[\"注\",{\"1\":{\"143\":1}}],[\"注意事项\",{\"1\":{\"255\":1}}],[\"注意是通过一个线性层来同时计算qkv三个矩阵\",{\"1\":{\"113\":1}}],[\"注意力权重\",{\"1\":{\"127\":1,\"130\":1}}],[\"注意力可视化\",{\"0\":{\"116\":1}}],[\"注意力矩阵的丢弃率\",{\"1\":{\"113\":1}}],[\"注意力头的数量\",{\"1\":{\"113\":1}}],[\"注意力汇聚\",{\"1\":{\"110\":1}}],[\"注意力机制的基本流程\",{\"0\":{\"123\":1}}],[\"注意力机制的特性\",{\"1\":{\"110\":1}}],[\"注意力机制让每个点从融合特征中提取相关信息\",{\"1\":{\"31\":1}}],[\"注意力机制使得每个文本\",{\"1\":{\"29\":1}}],[\"注意下面的embed\",{\"1\":{\"109\":1}}],[\"注意\",{\"0\":{\"192\":1},\"1\":{\"21\":1,\"113\":1,\"166\":1,\"167\":1,\"172\":1}}],[\"查看当前环境已安装的包\",{\"0\":{\"146\":1},\"1\":{\"146\":1}}],[\"查看当前conda激活的环境\",{\"1\":{\"145\":1}}],[\"查看当前激活的环境\",{\"0\":{\"145\":1}}],[\"查看所有已创建的环境\",{\"0\":{\"143\":1}}],[\"查询向量\",{\"1\":{\"128\":1}}],[\"查询点的位置\",{\"1\":{\"49\":1}}],[\"查询点数量\",{\"1\":{\"49\":1}}],[\"查询相比\",{\"1\":{\"47\":1}}],[\"~\",{\"1\":{\"148\":1,\"169\":2}}],[\"~$\",{\"1\":{\"141\":1}}],[\"~pred\",{\"1\":{\"40\":1}}],[\"杂谈\",{\"0\":{\"138\":1}}],[\"梯度消失\",{\"1\":{\"136\":1}}],[\"梯度消失或爆炸\",{\"1\":{\"11\":1}}],[\"❓q\",{\"0\":{\"135\":1,\"136\":1}}],[\"控制\",{\"1\":{\"172\":1}}],[\"控制交叉熵损失和\",{\"1\":{\"172\":1}}],[\"控制交叉熵中正负样本的权重\",{\"1\":{\"172\":1}}],[\"控制两个损失之间的权重比例\",{\"1\":{\"172\":1}}],[\"控制区域匹配误差的重要性\",{\"1\":{\"172\":1}}],[\"控制分类误差的重要性\",{\"1\":{\"172\":1}}],[\"控制假阴性\",{\"1\":{\"170\":1}}],[\"控制假阳性\",{\"1\":{\"170\":1}}],[\"控制正类\",{\"1\":{\"169\":1}}],[\"控制正样本\",{\"1\":{\"169\":1}}],[\"控制难易样本权重\",{\"1\":{\"169\":1}}],[\"控制难易样本的权重衰减程度\",{\"1\":{\"169\":1}}],[\"控制了相似度计算的维度\",{\"1\":{\"131\":1}}],[\"控制是否对\",{\"1\":{\"166\":1}}],[\"控制是否使用\",{\"1\":{\"66\":1}}],[\"控制是否输出全局特征\",{\"1\":{\"66\":1}}],[\"仍然需要大量的定制开发工作\",{\"1\":{\"286\":1}}],[\"仍然不如分块处理或多层级聚合模型高效\",{\"1\":{\"69\":1}}],[\"仍是\",{\"1\":{\"126\":1}}],[\"共享权重\",{\"1\":{\"226\":1}}],[\"共分为\",{\"1\":{\"221\":1}}],[\"共指\",{\"1\":{\"212\":1}}],[\"共个值\",{\"1\":{\"125\":1}}],[\"共5个类别\",{\"1\":{\"107\":1}}],[\"架起了图像空间到文本空间的桥梁\",{\"1\":{\"119\":1}}],[\"架构改进\",{\"1\":{\"288\":1}}],[\"架构的神经网络模型开始崭露头角\",{\"1\":{\"277\":1}}],[\"架构中\",{\"1\":{\"64\":1}}],[\"架构\",{\"1\":{\"50\":1,\"56\":1,\"93\":1,\"98\":1,\"211\":1,\"278\":3}}],[\"统一多模态的可能性\",{\"1\":{\"119\":1}}],[\"统计每个相邻字符对的出现次数\",{\"1\":{\"175\":1}}],[\"统计每个名词短语出现的频率\",{\"1\":{\"80\":1}}],[\"统计正确率\",{\"1\":{\"93\":1}}],[\"统计量\",{\"1\":{\"72\":1}}],[\"考虑\",{\"1\":{\"192\":1}}],[\"考虑使用其他指标\",{\"1\":{\"156\":1}}],[\"考虑篇幅原因\",{\"1\":{\"118\":1}}],[\"考察所有可能的\",{\"1\":{\"39\":1}}],[\"除非指定\",{\"1\":{\"211\":1}}],[\"除以可以缓解这个问题\",{\"1\":{\"136\":1}}],[\"除head\",{\"1\":{\"118\":1}}],[\"除了最后一个\",{\"1\":{\"103\":1}}],[\"除了模型本身的应用\",{\"1\":{\"92\":1}}],[\"意味着在处理图像时\",{\"1\":{\"118\":1}}],[\"赋予局部特征提取的能力\",{\"1\":{\"117\":1}}],[\"捕捉不同子空间的信息\",{\"1\":{\"261\":1}}],[\"捕捉全局信息\",{\"1\":{\"117\":1}}],[\"捕获密集到稀疏采样区域内的多尺度信息\",{\"1\":{\"53\":1}}],[\"换源到图像上\",{\"1\":{\"116\":1}}],[\"换句话说\",{\"1\":{\"57\":1,\"72\":1}}],[\"百万\",{\"1\":{\"115\":1}}],[\"累加当前批次的样本数到总样本数中\",{\"1\":{\"114\":1}}],[\"先通过\",{\"1\":{\"169\":1}}],[\"先激活你的conda环境\",{\"1\":{\"147\":1}}],[\"先经过一层或多层\",{\"1\":{\"117\":1}}],[\"先用\",{\"1\":{\"117\":1}}],[\"先交给预输出层进行处理\",{\"1\":{\"114\":1}}],[\"先生成答案再推理\",{\"1\":{\"83\":1}}],[\"没有padding\",{\"1\":{\"113\":1}}],[\"没有固定顺序\",{\"1\":{\"61\":1}}],[\"键向量\",{\"1\":{\"128\":1}}],[\"键\",{\"1\":{\"113\":2}}],[\"确定目标\",{\"1\":{\"291\":1}}],[\"确定缩放因子\",{\"1\":{\"113\":1}}],[\"确实像openai的gpt4这样的llm已经非常强了\",{\"1\":{\"194\":1}}],[\"确认你使用的是\",{\"1\":{\"147\":1}}],[\"确保它们在部署前达到预期的性能和稳定性标准\",{\"1\":{\"289\":1}}],[\"确保它们加起来是1\",{\"1\":{\"57\":1}}],[\"确保\",{\"1\":{\"288\":1}}],[\"确保所有利用\",{\"1\":{\"288\":1}}],[\"确保开发者能够在升级过程中无缝过渡\",{\"1\":{\"288\":1}}],[\"确保信息的持续更新和准确性\",{\"1\":{\"283\":1}}],[\"确保输出内容的精确性和可信度\",{\"1\":{\"283\":1}}],[\"确保映射是一一对应的\",{\"1\":{\"224\":1}}],[\"确保句子数为偶数\",{\"1\":{\"223\":1}}],[\"确保在单台机器上\",{\"1\":{\"223\":1}}],[\"确保卷积操作不会重叠\",{\"1\":{\"109\":1}}],[\"确保每个\",{\"1\":{\"33\":1}}],[\"确保推理一致性\",{\"1\":{\"23\":1}}],[\"位整数\",{\"1\":{\"192\":2}}],[\"位或\",{\"1\":{\"192\":2}}],[\"位浮点数\",{\"1\":{\"192\":1}}],[\"位于序列的开头\",{\"1\":{\"110\":1}}],[\"位置的隐藏状态并池化\",{\"1\":{\"226\":1}}],[\"位置的\",{\"1\":{\"221\":1}}],[\"位置嵌入\",{\"1\":{\"211\":1,\"226\":2}}],[\"位置嵌入会被初始化为一组特定的值\",{\"1\":{\"111\":1}}],[\"位置编码可以有效地捕捉输入序列中的相对位置信息\",{\"1\":{\"278\":1}}],[\"位置编码通常使用正弦和余弦函数生成\",{\"1\":{\"261\":1}}],[\"位置编码为可学习的矩阵\",{\"1\":{\"236\":1}}],[\"位置编码\",{\"1\":{\"111\":2,\"261\":1,\"278\":1}}],[\"位置编码的作用是为了记忆输入的语序信息\",{\"1\":{\"111\":1}}],[\"位置编码被添加到\",{\"1\":{\"110\":1}}],[\"位置处设置为\",{\"1\":{\"102\":1}}],[\"位置\",{\"1\":{\"49\":1}}],[\"随后\",{\"1\":{\"277\":1}}],[\"随着不断迭代\",{\"1\":{\"278\":1}}],[\"随着各阶段计算量的增加\",{\"1\":{\"277\":1}}],[\"随着\",{\"1\":{\"277\":1}}],[\"随着语言模型规模的扩大\",{\"1\":{\"277\":1}}],[\"随着时代演进\",{\"1\":{\"217\":1}}],[\"随着假负例的减少\",{\"1\":{\"155\":1}}],[\"随着假正例的减少\",{\"1\":{\"155\":1}}],[\"随着训练的进行\",{\"1\":{\"110\":1}}],[\"随机生成等量的非相邻句对\",{\"1\":{\"225\":1}}],[\"随机把一句话中\",{\"1\":{\"218\":1}}],[\"随机高斯分布初始化\",{\"1\":{\"189\":1}}],[\"随机裁剪输入图像\",{\"1\":{\"108\":1}}],[\"随机选\",{\"1\":{\"25\":1}}],[\"随机配对使模型暴露于各种语义上下文中\",{\"1\":{\"23\":1}}],[\"收益和风险\",{\"1\":{\"156\":1}}],[\"收敛情况的粗略指标\",{\"1\":{\"156\":1}}],[\"收敛\",{\"1\":{\"110\":1}}],[\"收集相邻句对\",{\"1\":{\"225\":1}}],[\"收集相同数量的相邻句对和非相邻句对\",{\"1\":{\"225\":1}}],[\"收集人类偏好数据\",{\"1\":{\"78\":1}}],[\"收集了大量的人类编写的\",{\"1\":{\"78\":1}}],[\"收集指令\",{\"1\":{\"78\":1}}],[\"收集所有样本的预测结果\",{\"1\":{\"39\":1}}],[\"嵌入向量生成过程图\",{\"1\":{\"236\":1}}],[\"嵌入和注意力层以\",{\"1\":{\"211\":1}}],[\"嵌入层维度\",{\"1\":{\"113\":1}}],[\"嵌入交互\",{\"1\":{\"110\":1}}],[\"嵌入的关系\",{\"1\":{\"110\":1}}],[\"嵌入\",{\"1\":{\"110\":1}}],[\"嵌入序列的开头\",{\"1\":{\"110\":1}}],[\"嵌入中\",{\"1\":{\"110\":1}}],[\"嵌入维度\",{\"1\":{\"109\":1,\"110\":1}}],[\"整个互联网一样\",{\"1\":{\"277\":1}}],[\"整个句子\",{\"1\":{\"260\":1}}],[\"整个过程中\",{\"1\":{\"221\":1}}],[\"整个函数\",{\"1\":{\"62\":1}}],[\"整体得分72\",{\"1\":{\"212\":1}}],[\"整体模型还是比较大的\",{\"1\":{\"118\":1}}],[\"整体结构图如下\",{\"1\":{\"118\":1}}],[\"整数\",{\"1\":{\"109\":2}}],[\"边长为该整数\",{\"1\":{\"109\":2}}],[\"步里被随机遮盖或替换的部分\",{\"1\":{\"218\":1}}],[\"步距为16\",{\"1\":{\"109\":1}}],[\"步骤3\",{\"1\":{\"78\":1}}],[\"步骤2\",{\"1\":{\"78\":1}}],[\"步骤1\",{\"1\":{\"78\":1}}],[\"采用了预训练和微调的学习方法\",{\"1\":{\"279\":1}}],[\"采用了\",{\"1\":{\"278\":1}}],[\"采用了更科学的数据配比\",{\"1\":{\"278\":1}}],[\"采用了一种更巧妙的解决思路\",{\"1\":{\"109\":1}}],[\"采用\",{\"1\":{\"278\":3}}],[\"采样与训练\",{\"1\":{\"211\":1}}],[\"采样点数量\",{\"1\":{\"49\":1}}],[\"采样半径\",{\"1\":{\"49\":1}}],[\"采样得到的关键点坐标\",{\"1\":{\"49\":1}}],[\"采样的关键点数量\",{\"1\":{\"49\":2}}],[\"采样一些关键点\",{\"1\":{\"49\":1}}],[\"采样层\",{\"1\":{\"45\":1}}],[\"划分后可以得到共个patch\",{\"1\":{\"109\":1}}],[\"范围\",{\"1\":{\"108\":2}}],[\"范围缩放到\",{\"1\":{\"108\":2}}],[\"范数\",{\"1\":{\"65\":2}}],[\"像素的图像块\",{\"1\":{\"118\":1}}],[\"像素\",{\"1\":{\"108\":2,\"118\":1}}],[\"像moco和simclr有所不同\",{\"1\":{\"89\":1}}],[\"静态方法是类中的一种特殊方法\",{\"1\":{\"107\":1}}],[\"记忆机制\",{\"1\":{\"288\":1}}],[\"记忆\",{\"1\":{\"287\":1}}],[\"记录\",{\"1\":{\"291\":1}}],[\"记录输入长度\",{\"1\":{\"233\":1}}],[\"记录被掩码的原token\",{\"1\":{\"224\":1}}],[\"记录被掩码的位置\",{\"1\":{\"224\":1,\"226\":1}}],[\"记录每个词的出现次数的词典\",{\"1\":{\"175\":1}}],[\"记录每个词的出现次数\",{\"1\":{\"175\":1}}],[\"记录该类别的样本数量\",{\"1\":{\"107\":1}}],[\"记住用户之前的交互习惯和偏好\",{\"1\":{\"278\":1}}],[\"记住\",{\"1\":{\"78\":1}}],[\"约4000多个样本\",{\"1\":{\"107\":1}}],[\"归纳偏置能够帮助学习算法缩小搜索范围\",{\"1\":{\"105\":1}}],[\"归纳偏置\",{\"1\":{\"105\":1}}],[\"归一化层\",{\"1\":{\"109\":1}}],[\"归一化有助于模型更快地收敛\",{\"1\":{\"108\":1}}],[\"归一化权重\",{\"1\":{\"57\":1}}],[\"归一化因子\",{\"1\":{\"57\":1}}],[\"归一化到单位球内\",{\"1\":{\"40\":1}}],[\"归一化后的点云数据\",{\"1\":{\"40\":1}}],[\"归一化\",{\"1\":{\"15\":1,\"40\":1,\"278\":1}}],[\"打破了\",{\"1\":{\"278\":1}}],[\"打破了这两个领域壁垒\",{\"1\":{\"105\":1}}],[\"打乱顺序\",{\"1\":{\"225\":1}}],[\"打开指定索引的图像文件\",{\"1\":{\"107\":1}}],[\"打印当前性能指标\",{\"1\":{\"39\":1}}],[\"填充过程图\",{\"1\":{\"233\":1}}],[\"填充token对应0\",{\"1\":{\"233\":1}}],[\"填充符\",{\"1\":{\"104\":1}}],[\"填充至最长序列长度\",{\"1\":{\"10\":1}}],[\"核采样参数\",{\"1\":{\"104\":1}}],[\"核采样时不扩展beam\",{\"1\":{\"104\":1}}],[\"核采样的概率阈值\",{\"1\":{\"104\":1}}],[\"核心价值\",{\"1\":{\"291\":1}}],[\"核心组件\",{\"0\":{\"287\":1},\"1\":{\"289\":1}}],[\"核心代码实现如下\",{\"1\":{\"226\":1}}],[\"核心要点\",{\"1\":{\"109\":1}}],[\"核心思想\",{\"1\":{\"85\":1,\"169\":1,\"172\":1}}],[\"核心问题\",{\"1\":{\"69\":1}}],[\"核心\",{\"0\":{\"60\":1}}],[\"核心操作包括\",{\"1\":{\"55\":1}}],[\"充分利用数据编排框架的优势\",{\"1\":{\"288\":1}}],[\"充分利用了大型语言模型的强大能力\",{\"1\":{\"286\":1}}],[\"充分吸收了高级区域抽象特征和文本特征\",{\"1\":{\"27\":1}}],[\"充当了soft\",{\"1\":{\"104\":1}}],[\"替换的是哪一个词\",{\"1\":{\"218\":1}}],[\"替换成以下内容\",{\"1\":{\"218\":1}}],[\"替换为\",{\"1\":{\"103\":2}}],[\"替代\",{\"1\":{\"30\":1}}],[\"叠加的运算流程\",{\"1\":{\"103\":1}}],[\"缓解\",{\"1\":{\"168\":1}}],[\"缓解类别极度不平衡问题\",{\"1\":{\"35\":1}}],[\"缓存显著压缩为潜在向量来保证高效推理的同时不降低效果\",{\"1\":{\"278\":1}}],[\"缓存key\",{\"1\":{\"103\":1}}],[\"缓存key和value\",{\"1\":{\"103\":1}}],[\"缓存的key和value\",{\"1\":{\"103\":1}}],[\"缓存和复用\",{\"1\":{\"103\":1}}],[\"缓存\",{\"1\":{\"103\":1}}],[\"吸收图像特征\",{\"1\":{\"103\":1}}],[\"及其迭代优化之后\",{\"1\":{\"291\":1}}],[\"及\",{\"1\":{\"103\":1}}],[\"若干业务\",{\"1\":{\"290\":1}}],[\"若\",{\"1\":{\"172\":2}}],[\"若前景点稀疏\",{\"1\":{\"169\":1}}],[\"若qk\",{\"1\":{\"113\":1}}],[\"若为none则使用默认值\",{\"1\":{\"113\":1}}],[\"若有query\",{\"1\":{\"102\":1}}],[\"若以文本单词数量来衡量\",{\"1\":{\"90\":1}}],[\"双向自注意力\",{\"1\":{\"102\":1}}],[\"单纯\",{\"1\":{\"285\":1}}],[\"单模态或者多模态\",{\"1\":{\"281\":1}}],[\"单模态自注意力\",{\"1\":{\"101\":1}}],[\"单词\",{\"1\":{\"224\":1}}],[\"单一的线性层只能进行线性变换\",{\"1\":{\"112\":1}}],[\"单独用来处理类别信息\",{\"1\":{\"110\":1}}],[\"单尺度分组\",{\"1\":{\"50\":1}}],[\"单尺度分组分类模型\",{\"0\":{\"50\":1}}],[\"高可用的托管解决方案\",{\"1\":{\"289\":1}}],[\"高计算资源需求\",{\"1\":{\"279\":1}}],[\"高效的模型微调\",{\"1\":{\"192\":1}}],[\"高置信度\",{\"1\":{\"169\":1}}],[\"高\",{\"1\":{\"98\":1,\"169\":1}}],[\"好\",{\"1\":{\"98\":3}}],[\"轻松\",{\"1\":{\"290\":1}}],[\"轻量推理\",{\"1\":{\"278\":1}}],[\"轻量知识型\",{\"1\":{\"278\":1}}],[\"轻\",{\"1\":{\"98\":7}}],[\"轻便\",{\"1\":{\"71\":1}}],[\"侧重于信息检索和融合外部知识\",{\"1\":{\"285\":1}}],[\"侧重于给这些点的特征分配更大的融合权重\",{\"1\":{\"29\":1}}],[\"侧重模态融合\",{\"1\":{\"98\":1}}],[\"早期的图文多模态\",{\"1\":{\"98\":1}}],[\"回调\",{\"1\":{\"287\":1}}],[\"回顾下之前的多模态网络设计\",{\"1\":{\"98\":1}}],[\"回答问题\",{\"1\":{\"282\":1}}],[\"回答\",{\"1\":{\"78\":1,\"85\":1,\"255\":1}}],[\"曾有一段时期一直在追求更大的网络架构\",{\"1\":{\"98\":1}}],[\"庖丁解牛vit\",{\"0\":{\"105\":1}}],[\"庖丁解牛blip2\",{\"0\":{\"97\":1},\"1\":{\"97\":1}}],[\"庖丁解牛clip\",{\"0\":{\"87\":1}}],[\"谷歌提出的\",{\"1\":{\"188\":1}}],[\"谷歌利用强大的计算能力进行了预训练\",{\"1\":{\"96\":1}}],[\"谷歌采用的弱监督方法与以往方法的一个主要区别在于规模\",{\"1\":{\"96\":1}}],[\"搜索出来的图片\",{\"1\":{\"94\":1}}],[\"运行和与\",{\"1\":{\"289\":1}}],[\"运行上述代码\",{\"1\":{\"94\":1}}],[\"运行可视化\",{\"1\":{\"40\":1}}],[\"拿到所有图片路径\",{\"1\":{\"94\":1}}],[\"遍历所有相邻字符对的组合\",{\"1\":{\"176\":1}}],[\"遍历所有句子\",{\"1\":{\"176\":1}}],[\"遍历vocab中所有词\",{\"1\":{\"175\":2}}],[\"遍历句子列表\",{\"1\":{\"175\":1}}],[\"遍历数据加载器中的每个批次数据\",{\"1\":{\"114\":1}}],[\"遍历获取supported支持的所有文件路径\",{\"1\":{\"107\":1}}],[\"遍历每个文件夹下的文件\",{\"1\":{\"107\":1}}],[\"遍历文件夹\",{\"1\":{\"107\":1}}],[\"遍历\",{\"1\":{\"94\":1,\"95\":1}}],[\"遍历data目录\",{\"1\":{\"94\":1}}],[\"遍历标注数据列表\",{\"1\":{\"25\":1}}],[\"递归遍历目录获取所有图片路径\",{\"1\":{\"93\":1,\"95\":1}}],[\"检索机制和索引管道等\",{\"1\":{\"288\":1}}],[\"检索能力\",{\"1\":{\"288\":1}}],[\"检索和使用外部数据可能引发伦理和隐私方面的问题\",{\"1\":{\"285\":1}}],[\"检索阶段\",{\"1\":{\"284\":1}}],[\"检索\",{\"1\":{\"284\":1}}],[\"检索增强生成\",{\"0\":{\"283\":1},\"1\":{\"283\":1}}],[\"检索等任务\",{\"1\":{\"69\":1}}],[\"检测率为\",{\"1\":{\"153\":1}}],[\"检查图像是否为\",{\"1\":{\"107\":1}}],[\"检查当前目录是否有预训练权重文件\",{\"1\":{\"93\":1,\"95\":1}}],[\"本地文件访问时内存管理升级\",{\"1\":{\"278\":1}}],[\"本节将会对bert模型实现的部分细节进行说明\",{\"1\":{\"229\":1}}],[\"本节我们将基于clip预训练模型实现zero\",{\"1\":{\"93\":1}}],[\"本质上是一个\",{\"1\":{\"278\":1}}],[\"本质上\",{\"1\":{\"178\":1,\"194\":1}}],[\"本类中不直接使用\",{\"1\":{\"170\":1}}],[\"本次训练是基于预训练好的vit\",{\"1\":{\"118\":1}}],[\"本文的transformer使用了self\",{\"1\":{\"260\":1}}],[\"本文基于\",{\"1\":{\"259\":1}}],[\"本文使用的是谷歌的中文预训练模型\",{\"1\":{\"232\":1}}],[\"本文使用的是训练了9个epoch后的模型权重进行的推理演示\",{\"1\":{\"40\":1}}],[\"本文所展示的bert预训练属于教学级别的\",{\"1\":{\"228\":1}}],[\"本文不再全部copy展示\",{\"1\":{\"226\":1}}],[\"本文中的\",{\"1\":{\"226\":1}}],[\"本文将通过一个花卉分类的实战案例结合vit原论文\",{\"1\":{\"106\":1}}],[\"花卉图片分类\",{\"0\":{\"93\":1}}],[\"紧密相关\",{\"1\":{\"92\":1}}],[\"显存并训练模型\",{\"1\":{\"192\":1}}],[\"显著抑制简单样本\",{\"1\":{\"169\":1}}],[\"显著提升难样本的分类性能\",{\"1\":{\"169\":1}}],[\"显著提升表现\",{\"1\":{\"82\":1}}],[\"显著提升了零样本和少样本泛化能力\",{\"1\":{\"78\":1}}],[\"显著提升了分割性能\",{\"1\":{\"62\":1}}],[\"显著提升了所有指标\",{\"1\":{\"32\":1}}],[\"显然\",{\"1\":{\"91\":1}}],[\"此时我们还要训练两个\",{\"1\":{\"221\":1}}],[\"此时先用text\",{\"1\":{\"103\":1}}],[\"此指标可平衡精确率和召回率的重要性\",{\"1\":{\"157\":1}}],[\"此外\",{\"1\":{\"91\":1,\"92\":1,\"96\":1,\"192\":1,\"282\":1}}],[\"此前\",{\"1\":{\"78\":1}}],[\"接触到更多的文本信息\",{\"1\":{\"278\":1}}],[\"接一个逐位置的前馈层来生成目标字符的分布输出\",{\"1\":{\"207\":1}}],[\"接着\",{\"1\":{\"204\":1}}],[\"接收器操作特性\",{\"1\":{\"159\":1}}],[\"接下来就需要进行长期的用户体验跟踪\",{\"1\":{\"291\":1}}],[\"接下来我们需要搭建前后端\",{\"1\":{\"291\":1}}],[\"接下来我们主要介绍几个国内外常见的大模型\",{\"1\":{\"278\":1}}],[\"接下来介绍几种比较流行的peft微调方案\",{\"1\":{\"181\":1}}],[\"接下来\",{\"1\":{\"91\":1,\"184\":1,\"280\":1}}],[\"接近1\",{\"1\":{\"169\":1}}],[\"接近0\",{\"1\":{\"169\":1}}],[\"接近\",{\"1\":{\"69\":1}}],[\"然而在某些情况下\",{\"1\":{\"283\":1}}],[\"然而大规模的未标注的文本语料是丰富\",{\"1\":{\"203\":1}}],[\"然而\",{\"1\":{\"90\":1,\"96\":3,\"277\":1,\"280\":1}}],[\"然后延展设计核心功能的上下游功能\",{\"1\":{\"291\":1}}],[\"然后从中选出最合适的那个\",{\"1\":{\"257\":1}}],[\"然后启动\",{\"1\":{\"232\":1}}],[\"然后对句子进行分词\",{\"1\":{\"224\":1}}],[\"然后对它们的特征做加权平均\",{\"1\":{\"57\":1}}],[\"然后给出一个假设\",{\"1\":{\"221\":1}}],[\"然后让模型通过上下文预测那一个被遮盖或替换的部分\",{\"1\":{\"218\":1}}],[\"然后能成功迁移学习解决判别式任务\",{\"1\":{\"214\":1}}],[\"然后累积计算每个误差对iou得分的影响\",{\"1\":{\"171\":1}}],[\"然后损失就是\",{\"1\":{\"170\":1}}],[\"然后再把新的问题让小孩子来解决\",{\"1\":{\"198\":1}}],[\"然后再经过一层\",{\"1\":{\"117\":1}}],[\"然后再拿到其它数据集上做迁移学习\",{\"1\":{\"115\":1}}],[\"然后输入到mlp\",{\"1\":{\"114\":1}}],[\"然后调整形状并重新排列维度\",{\"1\":{\"113\":1}}],[\"然后将tf模型转为对应的pytorch版本即可\",{\"1\":{\"232\":1}}],[\"然后将该位置的\",{\"1\":{\"221\":1}}],[\"然后将所有的\",{\"1\":{\"220\":1}}],[\"然后将\",{\"1\":{\"117\":1,\"159\":1}}],[\"然后将特征图的最后两维展平为一维\",{\"1\":{\"109\":1}}],[\"然后将这些不同尺度的特征拼接在一起\",{\"1\":{\"53\":1}}],[\"然后进行水平翻转\",{\"1\":{\"108\":1}}],[\"然后图像的表征和\",{\"1\":{\"104\":1}}],[\"然后在每项具体任务上判别性微调discriminative\",{\"1\":{\"203\":1}}],[\"然后在训练过程中\",{\"1\":{\"111\":1}}],[\"然后在具体的下游任务上进行微调\",{\"1\":{\"96\":1}}],[\"然后在后续会进行如下处理\",{\"1\":{\"33\":1}}],[\"然后计算出和当前文本描述相似度最高的那副图片\",{\"1\":{\"94\":1}}],[\"然后计算每个分类文本对应的文本嵌入向量\",{\"1\":{\"93\":1}}],[\"然后利用模型获取文本特征\",{\"1\":{\"93\":1}}],[\"然后我们读取要预测的图像\",{\"1\":{\"91\":1}}],[\"然后提取了相应的文本特征\",{\"1\":{\"91\":1}}],[\"然后\",{\"1\":{\"91\":1,\"171\":1,\"290\":1}}],[\"然后使用prepare\",{\"1\":{\"223\":1}}],[\"然后使用第一个cls\",{\"1\":{\"100\":1}}],[\"然后使用\",{\"1\":{\"69\":1}}],[\"然后使用全局最大池化\",{\"1\":{\"69\":1}}],[\"然后取\",{\"1\":{\"65\":1}}],[\"然后取平均得到\",{\"1\":{\"39\":1}}],[\"然后通过微调\",{\"1\":{\"279\":1}}],[\"然后通过全连接层\",{\"1\":{\"67\":2}}],[\"然后通过\",{\"1\":{\"55\":1,\"221\":1}}],[\"然后通过对应的pointnets提取每个尺度上的特征来捕获多尺度模式\",{\"1\":{\"52\":1}}],[\"然后只保留前\",{\"1\":{\"49\":1}}],[\"然后复制这个索引数组到每个\",{\"1\":{\"49\":1}}],[\"然后把所有点映射到高维的特征通过最大池化最终表示全局特征\",{\"1\":{\"43\":1}}],[\"然后求和\",{\"1\":{\"39\":1}}],[\"介绍几种具体的技巧\",{\"1\":{\"197\":1}}],[\"介绍\",{\"0\":{\"89\":1}}],[\"同样地\",{\"1\":{\"221\":1}}],[\"同样可以比较好地记录二维信息\",{\"1\":{\"111\":1}}],[\"同样需要位置编码来记录各图像块之间的位置信息\",{\"1\":{\"111\":1}}],[\"同样\",{\"1\":{\"109\":1,\"190\":1,\"278\":1,\"291\":1}}],[\"同样会将像素值从\",{\"1\":{\"108\":1}}],[\"同样是数据增强的手段\",{\"1\":{\"108\":1}}],[\"同样给计算机视觉领域带来了巨大影响\",{\"1\":{\"88\":1}}],[\"同时降低了调用成本\",{\"1\":{\"283\":1}}],[\"同时还开源了代码模型和数学模型\",{\"1\":{\"278\":1}}],[\"同时还能适应更多的参数\",{\"1\":{\"192\":1}}],[\"同时开源了用\",{\"1\":{\"278\":1}}],[\"同时发布了\",{\"1\":{\"278\":2}}],[\"同时减少参数量和计算量\",{\"1\":{\"278\":1}}],[\"同时结合了一些前人工作的改进\",{\"1\":{\"278\":1}}],[\"同时计算self\",{\"1\":{\"260\":1}}],[\"同时通过masked\",{\"1\":{\"227\":1}}],[\"同时进行\",{\"1\":{\"220\":1}}],[\"同时\",{\"1\":{\"191\":1,\"288\":1,\"290\":2}}],[\"同时使用这个旁路的更新来模拟\",{\"1\":{\"189\":1}}],[\"同时列表末尾追加<\",{\"1\":{\"176\":1}}],[\"同时末尾加上\",{\"1\":{\"175\":1}}],[\"同时完成断句分词任务\",{\"1\":{\"175\":1}}],[\"同时避免\",{\"1\":{\"174\":1}}],[\"同时也促进了社区的合作和共享\",{\"1\":{\"289\":1}}],[\"同时也引发了对未来人工智能的无限探索\",{\"1\":{\"282\":1}}],[\"同时也引发了对其伦理和风险问题的关注\",{\"1\":{\"279\":1}}],[\"同时也是\",{\"1\":{\"170\":1}}],[\"同时也指运用该方法构建的模型\",{\"1\":{\"89\":1}}],[\"同时保持了向后兼容性\",{\"1\":{\"288\":1}}],[\"同时保持了低资源部署的高效性\",{\"1\":{\"278\":1}}],[\"同时保持了模型性能\",{\"1\":{\"278\":2}}],[\"同时保持预先训练的权重不变\",{\"1\":{\"188\":1}}],[\"同时保持空间连续性\",{\"1\":{\"164\":1}}],[\"同时保留其捕捉长距离依赖的优势\",{\"1\":{\"117\":1}}],[\"同时作为特征数量\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"同时会将像素值从\",{\"1\":{\"108\":1}}],[\"同时删除不相关的视觉信息\",{\"1\":{\"104\":1}}],[\"同时因llm而具有了视觉推理能力\",{\"1\":{\"98\":1}}],[\"同时具有很好的性能\",{\"1\":{\"98\":1}}],[\"同时将flower\",{\"1\":{\"93\":1}}],[\"同时将运行时输出写入日志\",{\"1\":{\"40\":1}}],[\"同时最小化个负样本的相似度\",{\"1\":{\"90\":1}}],[\"同时确保这些分区的处理方式允许在它们之间共享模型权重\",{\"1\":{\"43\":1}}],[\"同时缺少\",{\"1\":{\"43\":1}}],[\"广泛的集成\",{\"1\":{\"288\":1}}],[\"广泛用于语言建模\",{\"1\":{\"223\":1}}],[\"广泛用于现代自然语言处理任务中\",{\"1\":{\"174\":1}}],[\"广泛应用于各类计算机视觉任务\",{\"1\":{\"88\":1}}],[\"广播\",{\"1\":{\"69\":1,\"271\":1}}],[\"众多基于视觉\",{\"1\":{\"88\":1}}],[\"年开始\",{\"1\":{\"286\":1}}],[\"年推进过\",{\"1\":{\"282\":1}}],[\"年发布\",{\"1\":{\"278\":1}}],[\"年至\",{\"1\":{\"278\":1}}],[\"年左右\",{\"1\":{\"277\":1}}],[\"年深度学习先驱\",{\"1\":{\"277\":1}}],[\"年代\",{\"1\":{\"277\":1}}],[\"年\",{\"1\":{\"88\":1,\"277\":1,\"278\":49,\"281\":1,\"288\":1}}],[\"年可谓是视觉\",{\"1\":{\"88\":1}}],[\"年提出\",{\"1\":{\"30\":1}}],[\"激活参数\",{\"1\":{\"278\":1}}],[\"激活后\",{\"1\":{\"141\":1}}],[\"激活环境的命令\",{\"1\":{\"141\":1}}],[\"激活\",{\"0\":{\"141\":1},\"1\":{\"278\":1}}],[\"激活已有知识\",{\"1\":{\"85\":1}}],[\"激活函数层\",{\"1\":{\"112\":1}}],[\"激活函数\",{\"1\":{\"55\":1,\"112\":1,\"114\":2,\"170\":1,\"211\":1,\"226\":2,\"238\":2,\"278\":2}}],[\"按序执行以下命令完成环境搭建\",{\"1\":{\"232\":1}}],[\"按需要在w前面拼接一些参数\",{\"1\":{\"183\":1}}],[\"按空格切分\",{\"1\":{\"175\":1}}],[\"按类别分配\",{\"1\":{\"169\":1}}],[\"按比例随机采样验证样本\",{\"1\":{\"107\":1}}],[\"按指令做事\",{\"1\":{\"85\":1}}],[\"按照本批次序列中最大长度进行截断\",{\"1\":{\"235\":1}}],[\"按照\",{\"1\":{\"175\":1}}],[\"按照16x16大小的patch进行划分\",{\"1\":{\"109\":1}}],[\"按照余弦相似度的数学公式来计算两者的相似度数值\",{\"1\":{\"93\":1}}],[\"按照最大相似度\",{\"1\":{\"91\":1}}],[\"按照权重做加权平均\",{\"1\":{\"57\":1}}],[\"按照几个预定义的半径值来搜索周围的邻近点\",{\"1\":{\"52\":1}}],[\"按照不同的搜索半径或领域大小对点集进行分组\",{\"1\":{\"52\":1}}],[\"听懂人话\",{\"1\":{\"85\":1}}],[\"泛化性不如\",{\"1\":{\"85\":1}}],[\"泛化能力\",{\"1\":{\"22\":1}}],[\"编造\",{\"1\":{\"255\":1}}],[\"编程和逻辑推理等任务中表现卓越\",{\"1\":{\"278\":1}}],[\"编程\",{\"1\":{\"85\":1}}],[\"编码能力异常强大\",{\"1\":{\"278\":1}}],[\"编码后可能是\",{\"1\":{\"255\":1}}],[\"编码后\",{\"1\":{\"253\":1}}],[\"编码后得到输出的结果\",{\"1\":{\"102\":1}}],[\"编码成\",{\"1\":{\"218\":1}}],[\"编码为图像特征\",{\"1\":{\"103\":1}}],[\"编码过程\",{\"1\":{\"56\":1}}],[\"编码器隐藏层输出\",{\"1\":{\"270\":1}}],[\"编码器层\",{\"1\":{\"266\":1}}],[\"编码器层堆叠\",{\"1\":{\"226\":1}}],[\"编码器层数\",{\"1\":{\"226\":1}}],[\"编码器输出的\",{\"1\":{\"103\":1}}],[\"编码器部分\",{\"1\":{\"58\":1}}],[\"编码器\",{\"1\":{\"55\":2,\"56\":1,\"58\":1,\"261\":1}}],[\"编码器不同层级的点云特征\",{\"1\":{\"16\":1}}],[\"编码\",{\"1\":{\"55\":1}}],[\"编码得到\",{\"1\":{\"29\":1}}],[\"编码点云特征\",{\"1\":{\"27\":1}}],[\"写作风格或特定领域知识\",{\"1\":{\"285\":1}}],[\"写入json文件\",{\"1\":{\"223\":1}}],[\"写好prompt\",{\"1\":{\"199\":1}}],[\"写故事\",{\"1\":{\"85\":1}}],[\"写一个关于猫的故事\",{\"1\":{\"78\":1}}],[\"唤醒\",{\"1\":{\"85\":1}}],[\"辨析\",{\"0\":{\"85\":1}}],[\"消融研究\",{\"1\":{\"213\":1}}],[\"消融实验\",{\"0\":{\"83\":1}}],[\"消除尺度差异\",{\"1\":{\"25\":1}}],[\"刷新\",{\"1\":{\"82\":1}}],[\"刷新该数据集\",{\"1\":{\"81\":1}}],[\"补充方式\",{\"1\":{\"167\":1}}],[\"补充\",{\"0\":{\"84\":1},\"1\":{\"82\":1}}],[\"联合使用\",{\"1\":{\"82\":1}}],[\"联合\",{\"0\":{\"82\":1}}],[\"联合推理时\",{\"1\":{\"81\":1}}],[\"准确性是指所有分类\",{\"1\":{\"152\":1}}],[\"准确率可以作为衡量模型质量的粗略指标\",{\"1\":{\"152\":1}}],[\"准确率衡量的是所有电子邮件正确分类所占的比例\",{\"1\":{\"152\":1}}],[\"准确率变化\",{\"1\":{\"83\":1}}],[\"准确率提升不大\",{\"1\":{\"82\":1}}],[\"准确率达到\",{\"1\":{\"81\":1}}],[\"准确率\",{\"0\":{\"152\":1},\"1\":{\"81\":1,\"156\":1,\"161\":1,\"173\":1,\"212\":1}}],[\"准备调试\",{\"1\":{\"232\":1}}],[\"准备就绪\",{\"1\":{\"232\":1}}],[\"准备与下游任务相关的数据集\",{\"1\":{\"187\":1}}],[\"准备生成参数\",{\"1\":{\"104\":1}}],[\"准备阶段主要完成数据集加载\",{\"1\":{\"37\":1}}],[\"准备\",{\"0\":{\"37\":1},\"1\":{\"36\":1,\"103\":1,\"225\":1}}],[\"助手需要生成推理过程\",{\"1\":{\"81\":1}}],[\"助手\",{\"1\":{\"81\":1,\"278\":1}}],[\"开发也有质的差异\",{\"1\":{\"290\":1}}],[\"开发在整体思路上有着较大的不同\",{\"1\":{\"290\":1}}],[\"开发大模型相关应用\",{\"1\":{\"290\":1}}],[\"开发者平台\",{\"1\":{\"289\":1}}],[\"开发者可以轻松地构建和定制\",{\"1\":{\"288\":1}}],[\"开发\",{\"1\":{\"278\":1,\"290\":2}}],[\"开发数据1k\",{\"1\":{\"232\":1}}],[\"开始的预训练\",{\"0\":{\"222\":1}}],[\"开始\",{\"1\":{\"211\":1,\"222\":1,\"291\":1}}],[\"开始之前\",{\"1\":{\"178\":1}}],[\"开始生成句子\",{\"1\":{\"103\":1}}],[\"开始输出回答\",{\"1\":{\"81\":1}}],[\"开源了\",{\"1\":{\"278\":1}}],[\"开源的一组参数规模从\",{\"1\":{\"278\":1}}],[\"开源项目\",{\"0\":{\"121\":1}}],[\"开源课程笔记\",{\"0\":{\"120\":1}}],[\"开源网络库\",{\"1\":{\"2\":1}}],[\"开源框架\",{\"1\":{\"2\":1}}],[\"你需要为每个选项分别构造一个完整的\",{\"1\":{\"257\":1}}],[\"你将得到一个包含多个元素的\",{\"1\":{\"253\":1}}],[\"你可能会觉得这里面有个问题\",{\"1\":{\"221\":1}}],[\"你可以使用以下命令查看你所有的\",{\"1\":{\"143\":1}}],[\"你的环境名\",{\"1\":{\"147\":1}}],[\"你的终端提示符通常会显示当前环境的名字\",{\"1\":{\"141\":1}}],[\"你是一个视觉助手\",{\"1\":{\"81\":1}}],[\"你知道吗\",{\"1\":{\"68\":1}}],[\"涵盖多种任务类型\",{\"1\":{\"81\":1}}],[\"理解微妙提示方面表现更出色\",{\"1\":{\"278\":1}}],[\"理解反面\",{\"1\":{\"212\":1}}],[\"理解并回答科学类问题\",{\"1\":{\"81\":1}}],[\"理想情况\",{\"1\":{\"98\":1}}],[\"理论上的限制\",{\"1\":{\"69\":1}}],[\"理论上证明\",{\"1\":{\"62\":1}}],[\"理论分析保证模型鲁棒性\",{\"1\":{\"62\":1}}],[\"序列\",{\"1\":{\"80\":1,\"103\":1,\"209\":1}}],[\"期望模型能够学习到文本和图像之间的匹配关系\",{\"1\":{\"89\":1}}],[\"期望的回答\",{\"1\":{\"80\":1,\"81\":1}}],[\"期望输出\",{\"1\":{\"78\":1}}],[\"请把这部分原文告诉我\",{\"1\":{\"255\":1}}],[\"请取消下面这行注释\",{\"1\":{\"170\":1}}],[\"请考虑下图中的点\",{\"1\":{\"162\":1}}],[\"请使用此方法\",{\"1\":{\"156\":1}}],[\"请仅与其他指标搭配使用\",{\"1\":{\"156\":1}}],[\"请注意\",{\"1\":{\"151\":1}}],[\"请描述这张图片\",{\"1\":{\"80\":1}}],[\"请求\",{\"1\":{\"78\":1}}],[\"详细描述型\",{\"1\":{\"81\":1}}],[\"详细描述\",{\"1\":{\"80\":2}}],[\"详细解释\",{\"1\":{\"33\":1,\"255\":1}}],[\"条件概率\",{\"1\":{\"207\":1}}],[\"条\",{\"1\":{\"81\":3}}],[\"条图文对\",{\"1\":{\"81\":1}}],[\"条高质量图文对\",{\"1\":{\"80\":1}}],[\"条描述\",{\"1\":{\"80\":1}}],[\"名词短语过滤\",{\"1\":{\"80\":1}}],[\"名称的长版本\",{\"1\":{\"159\":1}}],[\"名称\",{\"1\":{\"39\":1,\"81\":1}}],[\"万\",{\"1\":{\"278\":1}}],[\"万亿个\",{\"1\":{\"278\":1}}],[\"万亿参数\",{\"1\":{\"278\":1}}],[\"万亿\",{\"1\":{\"178\":1,\"278\":1}}],[\"万条图文对\",{\"1\":{\"80\":1}}],[\"万象多模态大模型\",{\"0\":{\"76\":1}}],[\"复用缓存的视觉信息\",{\"1\":{\"103\":1}}],[\"复杂推理型\",{\"1\":{\"81\":1}}],[\"复杂推理能力\",{\"1\":{\"79\":1}}],[\"复现\",{\"0\":{\"40\":1}}],[\"环境搭建遵从如下步骤即可\",{\"1\":{\"259\":1}}],[\"环境搭建\",{\"0\":{\"232\":1}}],[\"环境中的\",{\"1\":{\"147\":1}}],[\"环境中使用\",{\"1\":{\"147\":1}}],[\"环境再使用\",{\"1\":{\"147\":1}}],[\"环境\",{\"0\":{\"141\":1,\"259\":1},\"1\":{\"78\":1,\"142\":1,\"143\":1}}],[\"环境配置\",{\"0\":{\"8\":1}}],[\"奖励模型的目标是模拟人类的偏好判断\",{\"1\":{\"78\":1}}],[\"响应速度更快\",{\"1\":{\"278\":1}}],[\"响应速度比\",{\"1\":{\"278\":1}}],[\"响应\",{\"1\":{\"85\":2}}],[\"响应数据对模型进行微调\",{\"1\":{\"78\":1}}],[\"响应数据集\",{\"1\":{\"78\":1}}],[\"响应对\",{\"1\":{\"78\":1}}],[\"书生\",{\"0\":{\"76\":1}}],[\"剪切\",{\"1\":{\"74\":1}}],[\"缩放点积注意力\",{\"1\":{\"123\":1}}],[\"缩放\",{\"1\":{\"73\":1,\"74\":1}}],[\"缩放因子\",{\"1\":{\"15\":1,\"113\":1}}],[\"弯曲\",{\"1\":{\"73\":1}}],[\"移动\",{\"1\":{\"73\":1}}],[\"刚性运动\",{\"0\":{\"73\":1},\"1\":{\"73\":3}}],[\"乘积\",{\"1\":{\"72\":1}}],[\"才能够满足业务的需求\",{\"1\":{\"179\":1}}],[\"才能保证整个网络输出与输入点的顺序无关\",{\"1\":{\"72\":1}}],[\"才结束词汇表的构建\",{\"1\":{\"175\":1}}],[\"才会有好的效果\",{\"1\":{\"115\":1}}],[\"才会将内容显示到屏幕上\",{\"1\":{\"40\":1}}],[\"拓扑复杂\",{\"1\":{\"71\":1}}],[\"网格\",{\"1\":{\"71\":1}}],[\"网络输出由一个有限子集\",{\"1\":{\"62\":1}}],[\"网络结构特点\",{\"1\":{\"58\":1}}],[\"网络不是只捕获一个尺度上的局部特征\",{\"1\":{\"52\":1}}],[\"网络对于每个选定的形心点\",{\"1\":{\"52\":1}}],[\"网络对每一个点做低维到高维的映射\",{\"1\":{\"43\":1}}],[\"网络在训练时被呈现了不同稀疏度的点集\",{\"1\":{\"52\":1}}],[\"网络的每一组set\",{\"1\":{\"44\":1}}],[\"网络的分割和分类模型\",{\"1\":{\"44\":1}}],[\"适配应用任务\",{\"1\":{\"290\":1}}],[\"适应特定任务\",{\"1\":{\"279\":1}}],[\"适应扩展到所有线性层\",{\"1\":{\"192\":1}}],[\"适应不同任务需求\",{\"1\":{\"172\":1}}],[\"适用场景\",{\"1\":{\"169\":1}}],[\"适用于生产环境\",{\"1\":{\"288\":1}}],[\"适用于更复杂的应用场景\",{\"1\":{\"278\":1}}],[\"适用于分类\",{\"1\":{\"60\":1}}],[\"适用于\",{\"1\":{\"39\":1}}],[\"适用于连续响应值\",{\"1\":{\"39\":1}}],[\"适当降低训练目标反而可能取得更好的效果\",{\"1\":{\"96\":1}}],[\"适合相对稳定的数据\",{\"1\":{\"285\":1}}],[\"适合动态变化的数据\",{\"1\":{\"285\":1}}],[\"适合日常对话和基础任务场景\",{\"1\":{\"278\":1}}],[\"适合在推理阶段作为验证标准\",{\"1\":{\"168\":1}}],[\"适合渲染\",{\"1\":{\"71\":1}}],[\"适合\",{\"1\":{\"71\":1}}],[\"法向量等属性\",{\"1\":{\"71\":1}}],[\"法向量\",{\"1\":{\"71\":1}}],[\"法线等\",{\"1\":{\"49\":1}}],[\"忽视局部邻域关系\",{\"1\":{\"69\":1}}],[\"忽略填充部分\",{\"1\":{\"226\":1}}],[\"忽略难分类样本\",{\"1\":{\"169\":1}}],[\"忽略局部结构\",{\"1\":{\"69\":1}}],[\"忽略局部结构信息\",{\"1\":{\"69\":1}}],[\"忽略了局部邻域之间的结构关系\",{\"1\":{\"69\":1}}],[\"易混淆\",{\"0\":{\"255\":1}}],[\"易样本\",{\"1\":{\"169\":1}}],[\"易分类样本\",{\"1\":{\"169\":1}}],[\"易于扩展为检测\",{\"1\":{\"69\":1}}],[\"易受类别不平衡影响\",{\"1\":{\"167\":1}}],[\"易受\",{\"1\":{\"39\":1}}],[\"略逊于多视角\",{\"1\":{\"69\":1}}],[\"排列不变性\",{\"1\":{\"69\":1}}],[\"排序为\",{\"1\":{\"96\":1}}],[\"排序\",{\"1\":{\"39\":1,\"107\":1}}],[\"优质的\",{\"1\":{\"291\":1}}],[\"优先考虑精确性和推理步骤的正确性\",{\"1\":{\"278\":1}}],[\"优先于准确率\",{\"1\":{\"157\":1}}],[\"优缺点\",{\"1\":{\"169\":1}}],[\"优缺点对比\",{\"1\":{\"85\":1}}],[\"优点\",{\"1\":{\"85\":1,\"169\":1}}],[\"优势\",{\"1\":{\"69\":1,\"166\":1,\"167\":1,\"172\":1}}],[\"优化器与学习率调度器\",{\"1\":{\"227\":1}}],[\"优化器方案\",{\"1\":{\"211\":1}}],[\"优化器\",{\"1\":{\"37\":1}}],[\"优化器设置\",{\"1\":{\"37\":1}}],[\"未来将带来更多创新特性和性能提升\",{\"1\":{\"288\":1}}],[\"未归一化\",{\"1\":{\"253\":1}}],[\"未登录词\",{\"1\":{\"174\":1}}],[\"未使用\",{\"1\":{\"168\":1,\"169\":1}}],[\"未经扰动训练时\",{\"1\":{\"69\":1}}],[\"未见\",{\"1\":{\"22\":1}}],[\"图解transformer\",{\"0\":{\"258\":1},\"1\":{\"258\":1}}],[\"图解bert\",{\"1\":{\"231\":1}}],[\"图解\",{\"0\":{\"231\":1}}],[\"图7\",{\"1\":{\"191\":1}}],[\"图6\",{\"1\":{\"189\":1}}],[\"图5\",{\"1\":{\"189\":1}}],[\"图4\",{\"1\":{\"187\":1}}],[\"图3\",{\"1\":{\"187\":1}}],[\"图2\",{\"1\":{\"187\":1}}],[\"图1\",{\"1\":{\"187\":1}}],[\"图\",{\"1\":{\"159\":1,\"160\":1,\"161\":1,\"162\":2}}],[\"图文到文本\",{\"1\":{\"104\":1}}],[\"图文对比\",{\"1\":{\"101\":1}}],[\"图生文\",{\"1\":{\"103\":1}}],[\"图片切割\",{\"0\":{\"109\":1}}],[\"图片预处理\",{\"0\":{\"108\":1}}],[\"图片库中的图片\",{\"1\":{\"94\":1}}],[\"图片分类\",{\"1\":{\"93\":1,\"95\":1}}],[\"图片分类实战\",{\"1\":{\"93\":1}}],[\"图中有什么\",{\"1\":{\"80\":1}}],[\"图卷积\",{\"1\":{\"69\":1}}],[\"图像和声音\",{\"1\":{\"279\":1}}],[\"图像三种模态的深度理解能力\",{\"1\":{\"278\":1}}],[\"图像中各个类别的像素数量通常不均衡\",{\"1\":{\"164\":1}}],[\"图像到文本的桥梁\",{\"1\":{\"119\":1}}],[\"图像块嵌入层\",{\"1\":{\"110\":1}}],[\"图像块的尺寸\",{\"1\":{\"110\":1}}],[\"图像或\",{\"1\":{\"108\":2}}],[\"图像的索引\",{\"1\":{\"107\":1}}],[\"图像的文字描述\",{\"1\":{\"80\":1}}],[\"图像预处理的转换操作\",{\"1\":{\"107\":1}}],[\"图像注意力掩码\",{\"1\":{\"104\":1}}],[\"图像特征和掩码\",{\"1\":{\"104\":1}}],[\"图像特征作为cross\",{\"1\":{\"104\":1}}],[\"图像特征提取和模态融合都很重\",{\"1\":{\"98\":1}}],[\"图像特征提取与分类\",{\"1\":{\"91\":1}}],[\"图像编码阶段\",{\"1\":{\"104\":1}}],[\"图像编码器采用了\",{\"1\":{\"93\":1}}],[\"图像编码器\",{\"1\":{\"10\":1}}],[\"图像通过视觉编码器\",{\"1\":{\"103\":1}}],[\"图像分支依赖目标检测器\",{\"1\":{\"98\":1}}],[\"图像分支\",{\"1\":{\"98\":1}}],[\"图像分割等任务\",{\"1\":{\"39\":1}}],[\"图像对是从互联网收集的\",{\"1\":{\"96\":1}}],[\"图像对为负样本\",{\"1\":{\"90\":1}}],[\"图像对的相似度\",{\"1\":{\"90\":1}}],[\"图像对的训练batch\",{\"1\":{\"90\":1}}],[\"图像对的预训练方法\",{\"1\":{\"89\":1}}],[\"图像对\",{\"1\":{\"89\":1}}],[\"图像描述\",{\"1\":{\"80\":2}}],[\"图像\",{\"1\":{\"71\":1,\"81\":1,\"100\":1,\"109\":1}}],[\"图像处理或其他数据集中用于抽样的算法\",{\"1\":{\"46\":1}}],[\"改善了对长文本的理解和生成能力\",{\"1\":{\"278\":1}}],[\"改变\",{\"1\":{\"73\":1}}],[\"改进了推理能力和指令遵循能力\",{\"1\":{\"278\":1}}],[\"改进版\",{\"1\":{\"211\":1}}],[\"改进版的\",{\"1\":{\"211\":1}}],[\"改进而来\",{\"1\":{\"169\":1}}],[\"改进\",{\"1\":{\"69\":6}}],[\"改为\",{\"1\":{\"20\":1,\"259\":1}}],[\"异常数据\",{\"1\":{\"291\":1}}],[\"异常检测\",{\"1\":{\"72\":1}}],[\"异常检测等特殊场景\",{\"1\":{\"72\":1}}],[\"异常点会影响分类和分割性能\",{\"1\":{\"69\":1}}],[\"异常值等问题\",{\"1\":{\"61\":1}}],[\"面向大语言模型的检索增强生成技术\",{\"1\":{\"292\":1}}],[\"面对复杂问题时\",{\"1\":{\"283\":1}}],[\"面对大量噪声点时效果较差\",{\"1\":{\"69\":1}}],[\"面临的主要问题有\",{\"1\":{\"283\":1}}],[\"面部发生形变\",{\"1\":{\"69\":1}}],[\"尤其在解决复杂任务时表现出了惊人的潜力\",{\"1\":{\"277\":1}}],[\"尤其在目标区域较小\",{\"1\":{\"166\":1}}],[\"尤其是情商方面异常优秀\",{\"1\":{\"278\":1}}],[\"尤其是\",{\"1\":{\"208\":1}}],[\"尤其是目标检测\",{\"1\":{\"169\":1}}],[\"尤其是零样本\",{\"1\":{\"78\":1,\"85\":1}}],[\"尤其是在低层次上对每个质心点运行局部pointnet时\",{\"1\":{\"54\":1}}],[\"尤其是在预测值接近极端值\",{\"1\":{\"35\":1}}],[\"尤其未训练时\",{\"1\":{\"69\":1}}],[\"十二亿\",{\"1\":{\"115\":1}}],[\"十亿\",{\"1\":{\"115\":1}}],[\"十一\",{\"1\":{\"69\":1}}],[\"十\",{\"1\":{\"69\":1}}],[\"又进一步发布了\",{\"1\":{\"278\":1}}],[\"又是一个有实际意义的字或词\",{\"1\":{\"221\":1}}],[\"又怎么能够利用这些开源的大模型\",{\"1\":{\"188\":1}}],[\"又能拆解未知词的子词词汇表\",{\"1\":{\"174\":1}}],[\"又称为\",{\"1\":{\"166\":1}}],[\"又会导致计算资源浪费\",{\"1\":{\"69\":1}}],[\"又理解整体结构\",{\"1\":{\"62\":1}}],[\"太大\",{\"1\":{\"69\":1}}],[\"太大则可能导致不相关的点增多\",{\"1\":{\"47\":1}}],[\"受限于瓶颈维度\",{\"1\":{\"69\":1}}],[\"九\",{\"0\":{\"148\":1},\"1\":{\"69\":1}}],[\"八百六十万\",{\"1\":{\"115\":1}}],[\"八\",{\"0\":{\"147\":1},\"1\":{\"69\":1}}],[\"七\",{\"0\":{\"137\":1,\"146\":1},\"1\":{\"69\":1}}],[\"缺点\",{\"1\":{\"85\":1,\"169\":1}}],[\"缺乏具体的上下文\",{\"1\":{\"92\":1}}],[\"缺乏精细建模\",{\"1\":{\"69\":1}}],[\"缺乏层次化\",{\"1\":{\"69\":1}}],[\"缺乏层次化特征提取机制\",{\"1\":{\"69\":1}}],[\"缺乏动态上下文感知\",{\"1\":{\"69\":1}}],[\"缺陷对比\",{\"1\":{\"69\":1}}],[\"缺陷类型\",{\"1\":{\"69\":1}}],[\"缺陷\",{\"0\":{\"69\":1},\"1\":{\"69\":1,\"167\":1}}],[\"六\",{\"0\":{\"134\":1,\"145\":1},\"1\":{\"69\":1}}],[\"拉伸等会导致形变的操作\",{\"1\":{\"74\":1}}],[\"拉伸等形变\",{\"1\":{\"69\":1}}],[\"拉伸\",{\"1\":{\"69\":1,\"73\":1}}],[\"旋转位置编码\",{\"1\":{\"278\":1}}],[\"旋转\",{\"1\":{\"69\":1,\"73\":2,\"74\":2}}],[\"强化了模型对长上下文的理解和生成\",{\"1\":{\"283\":1}}],[\"强化了模型的代码能力\",{\"1\":{\"278\":1}}],[\"强化复杂任务处理能力\",{\"1\":{\"278\":1}}],[\"强推理能力\",{\"1\":{\"278\":1}}],[\"强大的神经网络模型\",{\"1\":{\"277\":1}}],[\"强度\",{\"1\":{\"71\":1}}],[\"强制学习正交变换矩阵\",{\"1\":{\"69\":1}}],[\"强调预测与\",{\"1\":{\"167\":1,\"168\":1}}],[\"强调分布匹配\",{\"1\":{\"39\":1}}],[\"强调正类\",{\"1\":{\"35\":1}}],[\"五\",{\"0\":{\"133\":1,\"144\":1},\"1\":{\"69\":1}}],[\"四\",{\"0\":{\"129\":1,\"143\":1},\"1\":{\"69\":1}}],[\"四个指标协同工作\",{\"1\":{\"39\":1}}],[\"四个指标对比总结\",{\"1\":{\"39\":1}}],[\"🧪\",{\"1\":{\"69\":1,\"255\":1}}],[\"🧩\",{\"1\":{\"69\":2,\"255\":1}}],[\"🧱\",{\"1\":{\"69\":3}}],[\"🧠\",{\"1\":{\"65\":1,\"69\":1,\"72\":1}}],[\"难于训练\",{\"1\":{\"188\":1}}],[\"难样本\",{\"1\":{\"169\":1}}],[\"难样本vs易样\",{\"1\":{\"169\":1}}],[\"难分类样本\",{\"1\":{\"169\":1}}],[\"难以学习有效特征\",{\"1\":{\"169\":1}}],[\"难以自动构建\",{\"1\":{\"71\":1}}],[\"难以用\",{\"1\":{\"71\":1}}],[\"难以建模更高维度的空间关系\",{\"1\":{\"69\":1}}],[\"难以捕捉非刚性变换下的不变性\",{\"1\":{\"69\":1}}],[\"难以区分语义相近但位置不同的区域\",{\"1\":{\"69\":1}}],[\"难点\",{\"0\":{\"61\":1},\"1\":{\"62\":4}}],[\"容易出错\",{\"1\":{\"69\":1}}],[\"虽然我们通过使用大模型来简化了业务逻辑的拆解\",{\"1\":{\"291\":1}}],[\"虽然大模型是深度学习领域的集大成之作\",{\"1\":{\"290\":1}}],[\"虽然之前的聊天机器人存在各种问题\",{\"1\":{\"282\":1}}],[\"虽然还不能执行复杂的推理任务\",{\"1\":{\"80\":1}}],[\"虽然\",{\"1\":{\"69\":3,\"168\":1}}],[\"虽然这个点没有实际意义\",{\"1\":{\"49\":1}}],[\"虽然这里用的是\",{\"1\":{\"35\":1}}],[\"曲率等细节\",{\"1\":{\"69\":1}}],[\"曲率\",{\"1\":{\"69\":1}}],[\"曲线上最接近\",{\"1\":{\"162\":1}}],[\"曲线为从\",{\"1\":{\"160\":1}}],[\"曲线的方法是\",{\"1\":{\"159\":1}}],[\"曲线直观地显示了所有阈值下的模型性能\",{\"1\":{\"159\":1}}],[\"曲线和\",{\"0\":{\"158\":1}}],[\"曲线\",{\"1\":{\"39\":1}}],[\"曲线下的面积\",{\"1\":{\"39\":1}}],[\"曲线下面积较大的模型通常是更好的模型\",{\"1\":{\"162\":1}}],[\"曲线下面积\",{\"0\":{\"160\":1},\"1\":{\"39\":1,\"160\":1}}],[\"无所不知\",{\"1\":{\"282\":1}}],[\"无所不能\",{\"1\":{\"282\":1}}],[\"无标签数据\",{\"1\":{\"279\":1}}],[\"无需重新训练\",{\"1\":{\"285\":1}}],[\"无需\",{\"1\":{\"278\":1}}],[\"无需预处理\",{\"1\":{\"69\":1}}],[\"无缝理解和生成多种形式内容\",{\"1\":{\"278\":1}}],[\"无监督预训练+监督微调方式\",{\"1\":{\"205\":1}}],[\"无监督预训练\",{\"0\":{\"207\":1},\"1\":{\"205\":1,\"211\":1}}],[\"无监督学习得到的好的表示也能提供显著的提升\",{\"1\":{\"204\":1}}],[\"无论到什么程度\",{\"1\":{\"204\":1}}],[\"无论是正类还是负类\",{\"1\":{\"152\":1}}],[\"无论是有监督还是自监督方法\",{\"1\":{\"96\":1}}],[\"无论其有效性如何\",{\"1\":{\"151\":1}}],[\"无论你如何打乱输入元素的顺序\",{\"1\":{\"72\":1}}],[\"无序性\",{\"1\":{\"71\":1}}],[\"无局部聚合机制\",{\"1\":{\"69\":1}}],[\"无法及时反映最新的信息动态\",{\"1\":{\"283\":1}}],[\"无法\",{\"1\":{\"255\":1}}],[\"无法处理不在原文中的答案\",{\"1\":{\"255\":1}}],[\"无法处理非刚性变形\",{\"1\":{\"69\":1}}],[\"无法处理非刚性形变\",{\"1\":{\"69\":2}}],[\"无法更新参数\",{\"1\":{\"190\":1}}],[\"无法实现zero\",{\"1\":{\"96\":1}}],[\"无法回答时\",{\"1\":{\"82\":1}}],[\"无法充分利用\",{\"1\":{\"69\":1}}],[\"无法区分顺序信息\",{\"1\":{\"69\":1}}],[\"无法有效利用局部结构\",{\"1\":{\"69\":1}}],[\"无法捕捉边缘\",{\"1\":{\"69\":1}}],[\"无法应对弯曲\",{\"1\":{\"69\":1}}],[\"无法像\",{\"1\":{\"69\":1}}],[\"无法直接用于每个点\",{\"1\":{\"68\":1}}],[\"无效\",{\"1\":{\"49\":1}}],[\"座位\",{\"1\":{\"68\":1}}],[\"仅用\",{\"1\":{\"278\":1}}],[\"仅供参考和学习\",{\"1\":{\"228\":1}}],[\"仅仅需要小小修改模型架构\",{\"1\":{\"203\":1}}],[\"仅在\",{\"1\":{\"189\":1}}],[\"仅使用cpu就能完成整个训练过程\",{\"1\":{\"223\":1}}],[\"仅使用\",{\"1\":{\"172\":1}}],[\"仅使用交叉熵损失\",{\"1\":{\"172\":1}}],[\"仅使用最后一层视觉特征\",{\"1\":{\"83\":1}}],[\"仅训练分类头\",{\"1\":{\"118\":1}}],[\"仅借鉴了encoder结构\",{\"1\":{\"105\":1}}],[\"仅能和自己的\",{\"1\":{\"101\":1}}],[\"仅更新插入的\",{\"1\":{\"85\":1}}],[\"仅更新投影矩阵\",{\"1\":{\"80\":1}}],[\"仅\",{\"1\":{\"82\":1,\"147\":1}}],[\"仅靠\",{\"1\":{\"69\":1}}],[\"仅靠局部特征很难判断某个点属于哪个部件\",{\"1\":{\"68\":1}}],[\"仅返回预测结果\",{\"1\":{\"10\":1}}],[\"鼓励变换矩阵接近正交矩阵\",{\"1\":{\"65\":1}}],[\"添加特殊token标记\",{\"1\":{\"233\":1}}],[\"添加一个无监督训练目标是半监督学习的一种替代形式\",{\"1\":{\"205\":1}}],[\"添加一些特殊词\",{\"1\":{\"175\":1}}],[\"添加一维位置编码和二维位置编码并没有太大的差异\",{\"1\":{\"111\":1}}],[\"添加位置编码\",{\"0\":{\"111\":1}}],[\"添加到\",{\"1\":{\"104\":1}}],[\"添加\",{\"0\":{\"110\":1},\"1\":{\"65\":1}}],[\"添加更多上下文细节\",{\"1\":{\"20\":1}}],[\"❗而只有正交矩阵才能表示刚性变换\",{\"1\":{\"65\":1}}],[\"正在改变着我们与技术互动的方式\",{\"1\":{\"282\":1}}],[\"正式开源\",{\"1\":{\"278\":1}}],[\"正式开源了\",{\"1\":{\"278\":1}}],[\"正式发布了其稳定版本\",{\"1\":{\"288\":1}}],[\"正式发布\",{\"1\":{\"278\":1}}],[\"正则化\",{\"1\":{\"278\":1}}],[\"正则化损失\",{\"0\":{\"65\":1}}],[\"正则\",{\"1\":{\"211\":1}}],[\"正则匹配含有\",{\"1\":{\"175\":1}}],[\"正样本较少时增加权重\",{\"1\":{\"169\":1}}],[\"正样本batch=1\",{\"1\":{\"102\":1}}],[\"正样本batch\",{\"1\":{\"102\":3}}],[\"正确\",{\"1\":{\"194\":1,\"221\":1}}],[\"正确识别阳性病例至关重要\",{\"1\":{\"153\":1}}],[\"正确分类的比例\",{\"1\":{\"152\":1}}],[\"正交变换包括\",{\"1\":{\"74\":1}}],[\"正交变换的本质是\",{\"1\":{\"74\":1}}],[\"正交变换\",{\"0\":{\"74\":1}}],[\"正是对这一缺陷的改进\",{\"1\":{\"69\":1}}],[\"正类\",{\"1\":{\"35\":1}}],[\"稳定的\",{\"1\":{\"64\":1}}],[\"神经网络的权重矩阵通常以高精度的浮点数\",{\"1\":{\"192\":1}}],[\"神经网络的输出在训练初期往往接近于零\",{\"1\":{\"64\":1}}],[\"神经网络模型只认识数字\",{\"1\":{\"25\":1}}],[\"摆正\",{\"1\":{\"64\":1,\"66\":1}}],[\"应对边界情况\",{\"1\":{\"291\":1}}],[\"应用就可以上线体验了\",{\"1\":{\"291\":1}}],[\"应用程序的开发\",{\"1\":{\"289\":1}}],[\"应用程序的开发提供了坚实的基础\",{\"1\":{\"289\":1}}],[\"应用程序部署到云端\",{\"1\":{\"289\":1}}],[\"应用的复杂度\",{\"1\":{\"288\":1}}],[\"应用的开发和部署提供了坚实的基础\",{\"1\":{\"288\":1}}],[\"应用中的每一步操作及其输入输出有一个清晰的认识\",{\"1\":{\"288\":1}}],[\"应用开发的大一统基座模型\",{\"1\":{\"281\":1}}],[\"应用层\",{\"0\":{\"201\":1}}],[\"应用量化低秩适应\",{\"1\":{\"192\":1}}],[\"应用自定义的权重初始化函数\",{\"1\":{\"114\":1}}],[\"应用\",{\"1\":{\"112\":1,\"278\":2,\"286\":1,\"287\":1}}],[\"应用注意力掩码\",{\"1\":{\"103\":1}}],[\"应用场景适应性受限\",{\"1\":{\"283\":1}}],[\"应用场景举例\",{\"1\":{\"85\":1}}],[\"应用场景\",{\"1\":{\"72\":1,\"169\":1}}],[\"应用到原始点云上\",{\"1\":{\"64\":1}}],[\"应该把\",{\"1\":{\"218\":1}}],[\"应该采用什么样的方法来进行训练\",{\"1\":{\"96\":1}}],[\"应该更高地加权第二个向量\",{\"1\":{\"54\":1}}],[\"应该关注哪些新能力\",{\"1\":{\"292\":1}}],[\"应该关注哪些点云点\",{\"1\":{\"11\":1}}],[\"应该关注图像中的哪些位置\",{\"1\":{\"11\":1}}],[\"变为\",{\"1\":{\"101\":3}}],[\"变形\",{\"1\":{\"73\":1}}],[\"变换不变性\",{\"1\":{\"69\":1}}],[\"变换矩阵会通过\",{\"1\":{\"64\":1}}],[\"变成1维度之后就成了50176\",{\"1\":{\"109\":1}}],[\"变成\",{\"1\":{\"49\":2,\"68\":1,\"183\":1,\"257\":1}}],[\"变成特征向量\",{\"1\":{\"44\":1}}],[\"决定了输出的信息维度\",{\"1\":{\"131\":1}}],[\"决定\",{\"1\":{\"62\":1}}],[\"小型语言模型通常难以解决涉及多个推理步骤的复杂任务\",{\"1\":{\"280\":1}}],[\"小数据集没有\",{\"1\":{\"213\":1}}],[\"小批次样本\",{\"1\":{\"211\":1}}],[\"小孩子根据从示例中学习到的推理\",{\"1\":{\"198\":1}}],[\"小结\",{\"0\":{\"96\":1}}],[\"小扰动不会改变函数输出\",{\"1\":{\"62\":1}}],[\"小红书\",{\"1\":{\"0\":1}}],[\"定价为\",{\"1\":{\"278\":1}}],[\"定价降低约\",{\"1\":{\"278\":1}}],[\"定位答案\",{\"1\":{\"255\":1}}],[\"定理表明\",{\"1\":{\"62\":1}}],[\"定义字典保存路径\",{\"1\":{\"224\":1}}],[\"定义为\",{\"1\":{\"172\":1}}],[\"定义注意力矩阵的丢弃层\",{\"1\":{\"113\":1}}],[\"定义一个线性层\",{\"1\":{\"113\":1}}],[\"定义一个二维卷积层\",{\"1\":{\"109\":1}}],[\"定义一个字典\",{\"1\":{\"108\":1}}],[\"定义当前目录\",{\"1\":{\"93\":1,\"95\":1}}],[\"定义\",{\"1\":{\"85\":1,\"169\":1}}],[\"定义与核心思想\",{\"1\":{\"85\":1}}],[\"定义局部区域的形心\",{\"1\":{\"45\":1}}],[\"定义投影层的丢弃层\",{\"1\":{\"113\":1}}],[\"定义投影层\",{\"1\":{\"15\":1,\"113\":1}}],[\"学习通用的语言表示和知识\",{\"1\":{\"279\":1}}],[\"学习到输入数据的非线性特征\",{\"1\":{\"112\":1}}],[\"学习算法需要在所有可能的函数空间中搜索最优模型\",{\"1\":{\"105\":1}}],[\"学习目标\",{\"1\":{\"103\":1}}],[\"学习率调度器\",{\"1\":{\"37\":1}}],[\"学习率调度器初始化等\",{\"1\":{\"37\":1}}],[\"学到的是一个关键点集合\",{\"1\":{\"69\":1}}],[\"学到的是一个\",{\"1\":{\"62\":1}}],[\"矩阵维度\",{\"1\":{\"230\":1}}],[\"矩阵又不是\",{\"1\":{\"190\":1}}],[\"矩阵a和b为什么不能同时为零\",{\"0\":{\"190\":1}}],[\"矩阵初始化\",{\"1\":{\"189\":2,\"190\":1}}],[\"矩阵从\",{\"1\":{\"187\":1}}],[\"矩阵w就是通过机器学习\",{\"1\":{\"178\":1}}],[\"矩阵中的对角线元素\",{\"1\":{\"90\":1}}],[\"矩阵所有元素平方和开方\",{\"1\":{\"65\":1}}],[\"矩阵返回\",{\"1\":{\"64\":1}}],[\"矩阵\",{\"1\":{\"62\":2,\"127\":1,\"189\":1,\"190\":2}}],[\"比\",{\"1\":{\"278\":1}}],[\"比例进行的掩码\",{\"1\":{\"227\":1}}],[\"比许多情况下的ensemble模型要好\",{\"1\":{\"212\":1}}],[\"比single\",{\"1\":{\"212\":1}}],[\"比原本少了一个多头自注意力\",{\"1\":{\"207\":1}}],[\"比较它们在iou\",{\"1\":{\"173\":1}}],[\"比较两个分类名词是否相等\",{\"1\":{\"93\":1}}],[\"比排序\",{\"1\":{\"62\":1}}],[\"比如后续我们会将搭建检索问答链来完成检索问答\",{\"1\":{\"287\":1}}],[\"比如取到了问题部分的内容\",{\"1\":{\"255\":1}}],[\"比如对于一个长度为\",{\"1\":{\"253\":1}}],[\"比如\",{\"1\":{\"224\":1}}],[\"比如上面的例子中\",{\"1\":{\"218\":1}}],[\"比如它看到的\",{\"1\":{\"218\":1}}],[\"比如文本蕴涵\",{\"1\":{\"203\":1}}],[\"比如多头注意力中可以分别控制每个\",{\"1\":{\"131\":1}}],[\"比如一张224x224的图片\",{\"1\":{\"109\":1}}],[\"比如边缘\",{\"1\":{\"69\":1}}],[\"比如椅子的腿\",{\"1\":{\"68\":1}}],[\"比如椅子朝向不同\",{\"1\":{\"64\":1}}],[\"比如法线\",{\"1\":{\"49\":1}}],[\"比如通过\",{\"1\":{\"49\":1}}],[\"比如颜色\",{\"1\":{\"49\":1}}],[\"效率比较低\",{\"1\":{\"180\":1}}],[\"效率\",{\"1\":{\"69\":1}}],[\"效率极低\",{\"1\":{\"33\":1}}],[\"效果可能不太理想\",{\"1\":{\"283\":1}}],[\"效果评估\",{\"1\":{\"227\":1}}],[\"效果会比不加这句话要好\",{\"1\":{\"197\":1}}],[\"效果相当惊人\",{\"1\":{\"185\":1}}],[\"效果已经非常好了\",{\"1\":{\"185\":1}}],[\"效果越显著\",{\"1\":{\"169\":1}}],[\"效果对比\",{\"0\":{\"115\":1}}],[\"效果\",{\"0\":{\"228\":1},\"1\":{\"62\":3,\"82\":2,\"291\":1}}],[\"γ\",{\"1\":{\"62\":2,\"169\":3}}],[\"≈\",{\"1\":{\"62\":1}}],[\"聚焦难分类样本\",{\"1\":{\"169\":1}}],[\"聚焦参数\",{\"1\":{\"35\":1,\"169\":2}}],[\"聚合了所有\",{\"1\":{\"128\":1}}],[\"聚合所有点的信息\",{\"1\":{\"62\":1,\"69\":1}}],[\"稀疏性强\",{\"1\":{\"71\":1}}],[\"稀疏点云等任务中表现受限\",{\"1\":{\"69\":1}}],[\"稀疏点云下性能差\",{\"1\":{\"69\":1}}],[\"稀疏\",{\"1\":{\"61\":1}}],[\"设\",{\"1\":{\"190\":1}}],[\"设是一个函数\",{\"1\":{\"72\":1}}],[\"设计产品页面\",{\"1\":{\"291\":1}}],[\"设计后\",{\"1\":{\"291\":1}}],[\"设计的一般原则及技巧\",{\"1\":{\"291\":1}}],[\"设计功能\",{\"1\":{\"291\":1}}],[\"设计合理\",{\"1\":{\"290\":1}}],[\"设计调优\",{\"1\":{\"290\":1}}],[\"设计局部\",{\"1\":{\"62\":1}}],[\"设计了一个统一架构\",{\"1\":{\"60\":1}}],[\"设置的回调方法cllote\",{\"1\":{\"235\":1}}],[\"设置为\",{\"1\":{\"211\":1}}],[\"设置为评估模式\",{\"1\":{\"40\":1}}],[\"设置默认参数\",{\"1\":{\"170\":1}}],[\"设置全局参数\",{\"1\":{\"169\":1}}],[\"设置柱状图的标题\",{\"1\":{\"107\":1}}],[\"设置y坐标\",{\"1\":{\"107\":1}}],[\"设置x坐标\",{\"1\":{\"107\":1}}],[\"设置渲染参数\",{\"1\":{\"40\":1}}],[\"设置颜色\",{\"1\":{\"40\":1}}],[\"设置训练轮数\",{\"1\":{\"40\":1}}],[\"设置学习率\",{\"1\":{\"40\":1}}],[\"设置batch\",{\"1\":{\"40\":1}}],[\"设置后台运行\",{\"1\":{\"40\":1}}],[\"设置\",{\"0\":{\"211\":1},\"1\":{\"10\":1,\"35\":1}}],[\"避免复杂的编码需求\",{\"1\":{\"192\":1}}],[\"避免单一损失可能带来的训练不稳定性\",{\"1\":{\"172\":1}}],[\"避免训练震荡\",{\"1\":{\"167\":1}}],[\"避免使用不平衡的数据集\",{\"1\":{\"156\":1}}],[\"避免罕见组合\",{\"1\":{\"80\":1}}],[\"避免了复杂的预处理\",{\"1\":{\"60\":1}}],[\"避免除以零\",{\"1\":{\"57\":1}}],[\"🌟\",{\"1\":{\"60\":1}}],[\"头部处理\",{\"1\":{\"58\":1}}],[\"坐标信息\",{\"1\":{\"71\":1}}],[\"坐标\",{\"1\":{\"58\":1,\"71\":1}}],[\"只开源了\",{\"1\":{\"278\":1}}],[\"只能对输入文本中的\",{\"1\":{\"255\":1}}],[\"只能处理刚性变换\",{\"1\":{\"69\":1}}],[\"只做抽取式问答\",{\"1\":{\"255\":1}}],[\"只不过是对每个字都要预测一个类别\",{\"1\":{\"221\":1}}],[\"只不过它们的具体实现上有一些差异\",{\"1\":{\"183\":1}}],[\"只计算在第\",{\"1\":{\"218\":1}}],[\"只达到了56\",{\"1\":{\"212\":1}}],[\"只训练降维矩阵\",{\"1\":{\"189\":1}}],[\"只训练少量\",{\"1\":{\"85\":1}}],[\"只对低秩矩阵\",{\"1\":{\"187\":1}}],[\"只是一个占位符而已\",{\"1\":{\"221\":1}}],[\"只是在推理的过程中\",{\"1\":{\"183\":1}}],[\"只是侧重点不一样\",{\"1\":{\"180\":1}}],[\"只是参数经过调整以更好响应指令\",{\"1\":{\"78\":1}}],[\"只出现了几次\",{\"1\":{\"151\":1}}],[\"只包含\",{\"1\":{\"140\":1,\"255\":1}}],[\"只需要去除cls\",{\"1\":{\"242\":1}}],[\"只需要计算\",{\"1\":{\"189\":1}}],[\"只需要微调\",{\"1\":{\"189\":1}}],[\"只需要48gb\",{\"1\":{\"185\":1}}],[\"只需要用一个一个linear即可\",{\"1\":{\"114\":1}}],[\"只需要将图像调整到合适的大小\",{\"1\":{\"108\":1}}],[\"只有2490样本\",{\"1\":{\"212\":1}}],[\"只有\",{\"1\":{\"189\":1,\"278\":2}}],[\"只有一个特殊标记\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"只有遇到换行符或者输出内容积累到一定大小时\",{\"1\":{\"40\":1}}],[\"只更新投影层\",{\"1\":{\"81\":1}}],[\"只保留最多\",{\"1\":{\"80\":1}}],[\"只改变物体的方向\",{\"1\":{\"74\":1}}],[\"只改变位置和朝向\",{\"1\":{\"73\":1}}],[\"只关注全局结构\",{\"1\":{\"69\":1}}],[\"只学正交变换\",{\"1\":{\"69\":1}}],[\"只通过\",{\"1\":{\"69\":1}}],[\"只要关键点还在\",{\"1\":{\"62\":1}}],[\"只取\",{\"1\":{\"58\":1}}],[\"地板等\",{\"1\":{\"58\":1}}],[\"其技术核心点虽然在大语言模型上\",{\"1\":{\"290\":1}}],[\"其涵盖了模型的输入与输出处理\",{\"1\":{\"288\":1}}],[\"其工作流程可以简单地分为数据处理\",{\"1\":{\"284\":1}}],[\"其性能可与具备\",{\"1\":{\"278\":1}}],[\"其由七大主要部分构成\",{\"1\":{\"261\":1}}],[\"其掩码数量可能会偏少\",{\"1\":{\"227\":1}}],[\"其实在某些训练集里\",{\"1\":{\"221\":1}}],[\"其实这本质上还是个分类问题\",{\"1\":{\"221\":1}}],[\"其余部分无论输出什么东西\",{\"1\":{\"218\":1}}],[\"其余部分不做损失\",{\"1\":{\"218\":1,\"227\":1}}],[\"其余参数冻结\",{\"1\":{\"85\":1}}],[\"其架构为\",{\"1\":{\"217\":1}}],[\"其将结构化文本输入处理为单一的连续字符序列\",{\"1\":{\"204\":1}}],[\"其它方法都有各自的一些问题\",{\"1\":{\"188\":1}}],[\"其维度分别为\",{\"1\":{\"187\":1}}],[\"其核心思想是通过调整难易样本的权重\",{\"1\":{\"169\":1}}],[\"其目标是为图像中的每个像素分配一个特定的语义类别标签\",{\"1\":{\"164\":1}}],[\"其目标是让预训练语言模型\",{\"1\":{\"78\":1}}],[\"其曲线下面积\",{\"1\":{\"160\":1}}],[\"其定义为\",{\"1\":{\"155\":1}}],[\"其数学定义为\",{\"1\":{\"152\":1,\"154\":1}}],[\"其他下游任务\",{\"0\":{\"252\":1}}],[\"其他权重全部冻结\",{\"1\":{\"118\":1}}],[\"其他两类为单轮对话\",{\"1\":{\"81\":1}}],[\"其表达能力是有限的\",{\"1\":{\"112\":1}}],[\"其次是准备训练数据\",{\"1\":{\"232\":1}}],[\"其次\",{\"1\":{\"93\":1,\"184\":1,\"213\":1}}],[\"其均能匹配到正确的文本标签\",{\"1\":{\"91\":1}}],[\"其规模与gpt\",{\"1\":{\"90\":1}}],[\"其已有的知识\",{\"1\":{\"85\":1}}],[\"其形状和大小保持不变的运动方式\",{\"1\":{\"73\":1}}],[\"其主要功能是\",{\"1\":{\"58\":1}}],[\"其中一个备受关注的项目就是\",{\"1\":{\"286\":1}}],[\"其中一个类别出现的频率非常低\",{\"1\":{\"152\":1}}],[\"其中训练数据使用1k\",{\"1\":{\"232\":1}}],[\"其中关于bertencoders编码并输出结果的整个过程如下图所示\",{\"1\":{\"226\":1}}],[\"其中sst\",{\"1\":{\"212\":1}}],[\"其中秩\",{\"1\":{\"189\":1}}],[\"其中第三步通过反向传播全量更新模型参数的过程如下\",{\"1\":{\"187\":1}}],[\"其中您感兴趣的罕见云彩类型\",{\"1\":{\"151\":1}}],[\"其中包括注意力可视化\",{\"1\":{\"116\":1}}],[\"其中的layers就是transformer\",{\"1\":{\"115\":1}}],[\"其中\",{\"1\":{\"21\":1,\"29\":2,\"30\":1,\"31\":1,\"39\":2,\"55\":1,\"62\":1,\"80\":1,\"81\":2,\"88\":1,\"90\":1,\"109\":1,\"123\":1,\"166\":1,\"167\":2,\"168\":1,\"169\":2,\"170\":1,\"172\":2,\"187\":1,\"189\":1,\"278\":1}}],[\"那就需要使用生成式模型\",{\"1\":{\"255\":1}}],[\"那就一起用\",{\"1\":{\"57\":1}}],[\"那不就矛盾了吗\",{\"1\":{\"221\":1}}],[\"那我们就认为\",{\"1\":{\"221\":1}}],[\"那很有可能就是我们最终的答案\",{\"1\":{\"199\":1}}],[\"那为什么还要有个qlora呢\",{\"1\":{\"185\":1}}],[\"那么对应的就是单词\",{\"1\":{\"255\":1}}],[\"那么可以组合这两个索引得到答案\",{\"1\":{\"253\":1}}],[\"那么它从\",{\"1\":{\"227\":2}}],[\"那么这个\",{\"1\":{\"221\":1}}],[\"那么上面的梯度就变成了\",{\"1\":{\"190\":1}}],[\"那么矩阵\",{\"1\":{\"190\":1}}],[\"那么那些小公司或者个人\",{\"1\":{\"188\":1}}],[\"那么prompt\",{\"1\":{\"182\":1}}],[\"那么搭建自己的大模型就非常必要\",{\"1\":{\"179\":1}}],[\"那么最终输出为\",{\"1\":{\"133\":1}}],[\"那么作者就想把注意力得到的结果\",{\"1\":{\"116\":1}}],[\"那么就会得到个文本特征\",{\"1\":{\"91\":1}}],[\"那么就是一个\",{\"1\":{\"72\":1}}],[\"那么\",{\"1\":{\"91\":1,\"109\":1,\"172\":1}}],[\"那么clip的训练目标就是最大个正样本的相似度\",{\"1\":{\"90\":1}}],[\"那样偏向背景点\",{\"1\":{\"168\":1}}],[\"那样对负样本过多敏感\",{\"1\":{\"166\":1}}],[\"那样我们总共有196个向量\",{\"1\":{\"109\":1}}],[\"那样逐层提取多层次的抽象特征\",{\"1\":{\"69\":1}}],[\"那样依赖\",{\"1\":{\"39\":1}}],[\"给每个普通词分配索引\",{\"1\":{\"224\":1}}],[\"给每个实例样本加一个\",{\"1\":{\"213\":1}}],[\"给每个点\",{\"1\":{\"69\":1}}],[\"给出问题和上下文\",{\"1\":{\"255\":1}}],[\"给出最大化的目标函数为\",{\"1\":{\"208\":1}}],[\"给出概率最大的结果\",{\"1\":{\"194\":1}}],[\"给llm更多的时间去思考\",{\"0\":{\"197\":1},\"1\":{\"197\":2}}],[\"给这3个邻近点分配权重\",{\"1\":{\"57\":1}}],[\"给定文档\",{\"1\":{\"209\":1}}],[\"给定两个超参数\",{\"1\":{\"47\":1}}],[\"给定一个无监督学习的语料tokens\",{\"1\":{\"207\":1}}],[\"给定一个点云\",{\"1\":{\"55\":1}}],[\"给定一个\",{\"1\":{\"27\":1}}],[\"说明你更讨厌\",{\"1\":{\"170\":2}}],[\"说明\",{\"1\":{\"82\":1}}],[\"说明其确实有效增强了语言\",{\"1\":{\"32\":1}}],[\"说话越有分量\",{\"1\":{\"57\":1}}],[\"维内部隐藏层\",{\"1\":{\"211\":1}}],[\"维隐藏层\",{\"1\":{\"211\":1}}],[\"维\",{\"1\":{\"109\":1}}],[\"维和第\",{\"1\":{\"109\":1}}],[\"维特征\",{\"1\":{\"57\":1}}],[\"维度变为\",{\"1\":{\"101\":1}}],[\"维度为\",{\"1\":{\"100\":2,\"101\":2,\"102\":9,\"107\":1}}],[\"维度求和\",{\"1\":{\"33\":1}}],[\"维度\",{\"1\":{\"16\":1,\"24\":1,\"33\":3,\"57\":1,\"62\":1,\"69\":2,\"128\":2}}],[\"扩展模型的推理能力\",{\"1\":{\"287\":2}}],[\"扩展语言模型的大小\",{\"1\":{\"278\":1}}],[\"扩展\",{\"1\":{\"226\":1}}],[\"扩展分类标记以匹配输入批次大小\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"扩展可学习的query\",{\"1\":{\"104\":1}}],[\"扩展性\",{\"1\":{\"69\":1}}],[\"扩展后便于广播乘法\",{\"1\":{\"57\":1}}],[\"扩展维度后相乘\",{\"1\":{\"57\":1}}],[\"扩展生成更多问题\",{\"1\":{\"20\":1}}],[\"权重共享\",{\"1\":{\"226\":3}}],[\"权重初始化\",{\"1\":{\"111\":1,\"114\":1,\"211\":1}}],[\"权重由\",{\"1\":{\"72\":1}}],[\"权重归一化\",{\"1\":{\"57\":1}}],[\"权重越大\",{\"1\":{\"57\":1}}],[\"权重\",{\"1\":{\"57\":1,\"169\":1}}],[\"权重项\",{\"1\":{\"35\":1}}],[\"找到\",{\"1\":{\"291\":1}}],[\"找到与文本最匹配的图片\",{\"1\":{\"94\":1,\"95\":1}}],[\"找到邻居\",{\"1\":{\"57\":1}}],[\"找到它最近的\",{\"1\":{\"57\":1}}],[\"找到最近的3个邻近点\",{\"1\":{\"57\":1}}],[\"找出最可能是\",{\"1\":{\"253\":1}}],[\"找出最近的3个邻近点\",{\"1\":{\"57\":1}}],[\"找出该尺度下每个质心点周围的邻近点\",{\"1\":{\"53\":1}}],[\"找出每个点的局部邻近点\",{\"1\":{\"49\":1}}],[\"找出它周围距离小于\",{\"1\":{\"49\":1}}],[\"欧氏距离平方\",{\"1\":{\"57\":1}}],[\"欧式距离的均匀性假设\",{\"1\":{\"47\":1}}],[\"格式化\",{\"1\":{\"289\":1}}],[\"格式\",{\"1\":{\"57\":1}}],[\"格式输出\",{\"1\":{\"10\":1}}],[\"主流知识型模型对比\",{\"1\":{\"278\":1}}],[\"主是为了解决cot这种从易到难的迁移能力不足而诞生的\",{\"1\":{\"200\":1}}],[\"主干部分全部冻结\",{\"1\":{\"118\":1}}],[\"主要由以下\",{\"1\":{\"287\":1}}],[\"主要特点包括\",{\"1\":{\"278\":1}}],[\"主要输出项解释\",{\"1\":{\"253\":1}}],[\"主要用于测试小型数据集的语言模型训练效果\",{\"1\":{\"223\":1}}],[\"主要用于解决目标检测任务中前景\",{\"1\":{\"169\":1}}],[\"主要针对电影评论来做情感分类\",{\"1\":{\"212\":1}}],[\"主要有以下两个\",{\"1\":{\"180\":1}}],[\"主要的区别就是去掉了paddding\",{\"1\":{\"113\":1}}],[\"主要区别在于norm层的顺序\",{\"1\":{\"112\":1}}],[\"主要包含encoder和decoder结构\",{\"1\":{\"105\":1}}],[\"主要是因为这些方法难以实现较高的性能\",{\"1\":{\"96\":1}}],[\"主要作用是\",{\"1\":{\"57\":1}}],[\"主页\",{\"0\":{\"0\":1}}],[\"←\",{\"1\":{\"56\":3}}],[\"插件系统\",{\"1\":{\"278\":1}}],[\"插入异常点\",{\"1\":{\"69\":1}}],[\"插入的多模态嵌入\",{\"1\":{\"13\":1}}],[\"插值并融合后的特征\",{\"1\":{\"57\":1}}],[\"插值得到的密集特征\",{\"1\":{\"55\":1}}],[\"少样本任务迁移\",{\"1\":{\"85\":1}}],[\"少量点无法覆盖关键结构\",{\"1\":{\"69\":1}}],[\"少量点的坐标\",{\"1\":{\"57\":1}}],[\"少\",{\"1\":{\"55\":1}}],[\"墙壁等\",{\"1\":{\"55\":1}}],[\"椅子\",{\"1\":{\"55\":1,\"58\":1}}],[\"完善的工具使用\",{\"1\":{\"278\":1}}],[\"完全随机猜测的\",{\"1\":{\"160\":1}}],[\"完全支持\",{\"1\":{\"69\":1}}],[\"完美的模型在某个阈值下的\",{\"1\":{\"159\":1}}],[\"完美的模型不会产生假正例\",{\"1\":{\"154\":1}}],[\"完美的模型没有假正例和假负例\",{\"1\":{\"152\":1}}],[\"完成预处理后\",{\"1\":{\"291\":1}}],[\"完成特定任务\",{\"1\":{\"85\":1}}],[\"完成点云分割任务的过程是一个典型的\",{\"1\":{\"55\":1}}],[\"完成\",{\"1\":{\"54\":1,\"291\":1}}],[\"完整的代码实现部分\",{\"1\":{\"226\":1}}],[\"完整的单尺度分组分类流程为\",{\"1\":{\"50\":1}}],[\"完整代码\",{\"0\":{\"95\":1}}],[\"完整方法\",{\"1\":{\"39\":1}}],[\"首个版本于\",{\"1\":{\"278\":1}}],[\"首次引入的\",{\"1\":{\"280\":1}}],[\"首次发布\",{\"1\":{\"278\":1}}],[\"首次将深度学习的思想融入到语言模型中\",{\"1\":{\"277\":1}}],[\"首次sample\",{\"1\":{\"50\":1}}],[\"首轮统计展示\",{\"1\":{\"175\":1}}],[\"首先要确定应用的核心功能\",{\"1\":{\"291\":1}}],[\"首先需要将非常复杂的业务逻辑依次拆解\",{\"1\":{\"290\":1}}],[\"首先需要对输入图片进行尺寸变化\",{\"1\":{\"108\":1}}],[\"首先在大规模文本数据上进行预训练\",{\"1\":{\"279\":1}}],[\"首先在输入句子的开头加一个代表分类的符号\",{\"1\":{\"221\":1}}],[\"首先模型会根据传入的tokens列表生成一个pad\",{\"1\":{\"230\":1}}],[\"首先加入特殊标记\",{\"1\":{\"224\":1}}],[\"首先是其初始化方法中需要完成\",{\"1\":{\"224\":1}}],[\"首先我们需要准备一个小型语料库\",{\"1\":{\"223\":1}}],[\"首先我们用data目录充当我们的图片库来源\",{\"1\":{\"94\":1}}],[\"首先将\",{\"1\":{\"225\":1}}],[\"首先将橙色和所有的黄色向量进行\",{\"1\":{\"221\":1}}],[\"首先将问题和文章通过\",{\"1\":{\"221\":1}}],[\"首先说明\",{\"1\":{\"118\":1}}],[\"首先使用卷积层对输入图像进行处理\",{\"1\":{\"109\":1}}],[\"首先输入图片\",{\"1\":{\"104\":1}}],[\"首先\",{\"1\":{\"54\":1,\"91\":1,\"93\":1,\"184\":1,\"204\":1,\"213\":1}}],[\"策略预训练轻量级查询\",{\"1\":{\"99\":1}}],[\"策略\",{\"1\":{\"53\":1}}],[\"要和分解的问题\",{\"1\":{\"200\":1}}],[\"要具体\",{\"0\":{\"196\":1}}],[\"要明确\",{\"0\":{\"196\":1}}],[\"要让llm给出的结果尽可能地合理\",{\"1\":{\"194\":1}}],[\"要知道像\",{\"1\":{\"188\":1}}],[\"要想开发自己的大模型几乎不可能\",{\"1\":{\"188\":1}}],[\"要想在自己的服务中接入大模型的能力\",{\"1\":{\"179\":1}}],[\"要训练一个特定的模型\",{\"1\":{\"184\":1}}],[\"要适配特定的下游任务\",{\"1\":{\"184\":1}}],[\"要在个性化的服务中使用大模型的能力\",{\"1\":{\"179\":1}}],[\"要对大模型进行微调\",{\"1\":{\"179\":1}}],[\"要退出当前激活的环境\",{\"1\":{\"142\":1}}],[\"要采样的质心点数量\",{\"1\":{\"53\":1}}],[\"要么对所有点做操作\",{\"1\":{\"43\":1}}],[\"要么对一个点做操作\",{\"1\":{\"43\":1}}],[\"之一\",{\"1\":{\"278\":1}}],[\"之前的工作提出了在迁移表征顶部学习特定任务的架构\",{\"1\":{\"209\":1}}],[\"之前已经有一些研究工作探讨了使用文本作为监督信号来训练视觉模型\",{\"1\":{\"96\":1}}],[\"之间切换\",{\"1\":{\"278\":1}}],[\"之间\",{\"1\":{\"254\":1}}],[\"之间可能提供最佳平衡\",{\"1\":{\"162\":1}}],[\"之间的损失权重\",{\"1\":{\"169\":2}}],[\"之间的关系\",{\"1\":{\"110\":1}}],[\"之间的平方欧氏距离\",{\"1\":{\"49\":1}}],[\"之后我们要在这两个句子中加一些特殊的\",{\"1\":{\"219\":1}}],[\"之后让模型预测和还原被遮盖掉或替换掉的部分\",{\"1\":{\"218\":1}}],[\"之后做\",{\"1\":{\"218\":1}}],[\"之后\",{\"1\":{\"88\":1,\"221\":1}}],[\"之后concat形成该区域提取的总特征\",{\"1\":{\"53\":1}}],[\"之后这些不同尺度上提取的特征被串联起来\",{\"1\":{\"52\":1}}],[\"二者都加同样的辅助lm\",{\"1\":{\"213\":1}}],[\"二是已解决的子问题及其答案列表\",{\"1\":{\"200\":1}}],[\"二值化或软标签\",{\"1\":{\"166\":1}}],[\"二值化显示\",{\"1\":{\"40\":1}}],[\"二元分类器的每个输出有四种可能的结果\",{\"1\":{\"151\":1}}],[\"二元分类场景\",{\"0\":{\"150\":1}}],[\"二分类task\",{\"0\":{\"102\":1}}],[\"二\",{\"0\":{\"124\":1,\"141\":1},\"1\":{\"69\":1}}],[\"二次sample\",{\"1\":{\"50\":1}}],[\"zhandaohong\",{\"1\":{\"232\":1}}],[\"zsh\",{\"1\":{\"148\":1}}],[\"zip\",{\"1\":{\"107\":1,\"227\":1,\"232\":2,\"235\":1,\"271\":1}}],[\"z\",{\"1\":{\"50\":2,\"71\":2}}],[\"zeros\",{\"1\":{\"38\":2,\"39\":3,\"40\":3,\"49\":2,\"102\":1,\"110\":1,\"111\":2,\"114\":2,\"236\":1,\"249\":1}}],[\"zero\",{\"1\":{\"38\":1,\"78\":1,\"204\":1,\"230\":1,\"233\":2,\"278\":1}}],[\"质点数量\",{\"1\":{\"49\":1}}],[\"质心\",{\"1\":{\"49\":2}}],[\"大型模型不仅可以缩短每个具体应用的开发周期\",{\"1\":{\"281\":1}}],[\"大型语言模型\",{\"1\":{\"78\":1,\"283\":1}}],[\"大脑\",{\"1\":{\"277\":1}}],[\"大部分任务\",{\"1\":{\"211\":1}}],[\"大部分深度学习方法需要大量人工标注的数据\",{\"1\":{\"204\":1}}],[\"大多数方法求解出来结果都一样的答案\",{\"1\":{\"199\":1}}],[\"大多数实际应用中都是如此\",{\"1\":{\"152\":1}}],[\"大公司或者研究机构\",{\"1\":{\"188\":1}}],[\"大\",{\"1\":{\"178\":1}}],[\"大模型api使用\",{\"0\":{\"293\":1}}],[\"大模型开发与传统\",{\"1\":{\"290\":1}}],[\"大模型开发却更多是一个工程问题\",{\"1\":{\"290\":1}}],[\"大模型开发\",{\"0\":{\"290\":1},\"1\":{\"290\":2}}],[\"大模型可以成为\",{\"1\":{\"281\":1}}],[\"大模型研发\",{\"1\":{\"278\":1}}],[\"大模型领域仅存在预训练阶段的\",{\"1\":{\"277\":1}}],[\"大模型中有其中一部分参数\",{\"1\":{\"184\":1}}],[\"大模型参数很多\",{\"1\":{\"184\":1}}],[\"大模型的性能不断增长\",{\"1\":{\"277\":1}}],[\"大模型的微调有以下几条技术路线\",{\"1\":{\"180\":1}}],[\"大模型的微调分成两条技术路线\",{\"1\":{\"180\":1}}],[\"大模型的推理成本越高\",{\"1\":{\"179\":1}}],[\"大模型\",{\"1\":{\"178\":1,\"278\":2}}],[\"大模型微调大致发展历史\",{\"0\":{\"188\":1}}],[\"大模型微调\",{\"0\":{\"177\":1},\"1\":{\"177\":1}}],[\"大家参考仓库源码即可\",{\"1\":{\"226\":1}}],[\"大家可以自行拉取项目完整代码进行学习\",{\"1\":{\"118\":1}}],[\"大家注意区分\",{\"1\":{\"100\":1}}],[\"大于\",{\"1\":{\"112\":1}}],[\"大致上两者结构是相同的\",{\"1\":{\"112\":1}}],[\"大小的图像\",{\"1\":{\"108\":1}}],[\"大小\",{\"1\":{\"108\":1}}],[\"大小不超过\",{\"1\":{\"62\":1}}],[\"大大提高学习效率\",{\"1\":{\"105\":1}}],[\"大幅降低训练成本\",{\"1\":{\"98\":1}}],[\"大放异彩的一年\",{\"1\":{\"88\":1}}],[\"大语言模型的两个核心能力\",{\"1\":{\"290\":1}}],[\"大语言模型的发展历程虽然只有短短不到五年的时间\",{\"1\":{\"278\":1}}],[\"大语言模型是这个新模式的典型例子\",{\"1\":{\"281\":1}}],[\"大语言模型是一种具有强大语言处理能力的技术\",{\"1\":{\"279\":1}}],[\"大语言模型具有多种显著特点\",{\"1\":{\"279\":1}}],[\"大语言模型应用开发基础知识速览\",{\"1\":{\"276\":1}}],[\"大语言模型应用开发课程\",{\"0\":{\"275\":1}}],[\"大语言模型\",{\"0\":{\"75\":1,\"277\":1},\"1\":{\"277\":2,\"290\":1}}],[\"大区域\",{\"1\":{\"49\":1}}],[\"大局部区域\",{\"1\":{\"49\":1}}],[\"终输出的\",{\"1\":{\"49\":1}}],[\"创建的\",{\"1\":{\"288\":1}}],[\"创建用以区分special\",{\"1\":{\"233\":1}}],[\"创建句子辨识列表\",{\"1\":{\"233\":1}}],[\"创建新的线性层\",{\"1\":{\"226\":1}}],[\"创建新环境\",{\"0\":{\"140\":1}}],[\"创建反向映射\",{\"1\":{\"224\":1}}],[\"创建环境并安装一些常用包\",{\"1\":{\"140\":1}}],[\"创建环境时指定\",{\"1\":{\"140\":1}}],[\"创建一个空环境\",{\"1\":{\"140\":1}}],[\"创建一个全零点作为\",{\"1\":{\"49\":1}}],[\"创建预输出层\",{\"1\":{\"114\":1}}],[\"创建归一化层\",{\"1\":{\"114\":1}}],[\"创建encoder\",{\"1\":{\"114\":1}}],[\"创建\",{\"1\":{\"112\":1}}],[\"创建丢弃层\",{\"1\":{\"111\":1,\"114\":1}}],[\"创建可学习的位置嵌入\",{\"1\":{\"111\":1,\"114\":1}}],[\"创建可学习的分类标记\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"创建可视化窗口\",{\"1\":{\"40\":1}}],[\"创建图像块嵌入层\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"创建图像注意力掩码\",{\"1\":{\"104\":1}}],[\"我的调试文件是run\",{\"1\":{\"232\":1}}],[\"我的狗很可爱\",{\"1\":{\"219\":2}}],[\"我已经上传到了仓库中\",{\"1\":{\"232\":1}}],[\"我就不上传了\",{\"1\":{\"232\":1}}],[\"我将两者的结构进行对比\",{\"1\":{\"112\":1}}],[\"我们就完成了应用的核心功能\",{\"1\":{\"291\":1}}],[\"我们就用该查询点最近的那个点\",{\"1\":{\"49\":1}}],[\"我们应该进行实际业务测试\",{\"1\":{\"291\":1}}],[\"我们推荐基于\",{\"1\":{\"291\":1}}],[\"我们一般可以将大模型开发分解为以下几个流程\",{\"1\":{\"291\":1}}],[\"我们一般不会去大幅度改动模型\",{\"1\":{\"290\":1}}],[\"我们并不需要深研大模型内部原理\",{\"1\":{\"290\":1}}],[\"我们将开发以大语言模型为功能核心\",{\"1\":{\"290\":1}}],[\"我们将从\",{\"1\":{\"222\":1}}],[\"我们后续会用到的处理文档\",{\"1\":{\"289\":1}}],[\"我们简要介绍三个\",{\"1\":{\"280\":1}}],[\"我们从原始输入的\",{\"1\":{\"255\":1}}],[\"我们从flower\",{\"1\":{\"93\":1}}],[\"我们主要使用它来预测答案的起始和结束位置\",{\"1\":{\"253\":1}}],[\"我们只需要确保对于某个词的上下文融合不被pad词参与即可\",{\"1\":{\"230\":1}}],[\"我们只需要在计算出相似度得分矩阵后\",{\"1\":{\"94\":1}}],[\"我们只会计算被随机遮盖或替换的部分\",{\"1\":{\"227\":1}}],[\"我们用蓝色的向量和所有黄色向量进行\",{\"1\":{\"221\":1}}],[\"我们给上句的\",{\"1\":{\"219\":1}}],[\"我们先给小孩子分析讲解一些示例\",{\"1\":{\"198\":1}}],[\"我们先对大模型做一个直观的抽象\",{\"1\":{\"178\":1}}],[\"我们教小孩做应用题\",{\"1\":{\"198\":1}}],[\"我们会使用pad\",{\"1\":{\"230\":1}}],[\"我们会为该任务设计一个最合适的神经网络架构并做训练\",{\"1\":{\"217\":1}}],[\"我们会用自然语言描述一系列的推理过程\",{\"1\":{\"198\":1}}],[\"我们会拼接多个这样的头\",{\"1\":{\"133\":1}}],[\"我们在接下来的部分\",{\"1\":{\"197\":1}}],[\"我们在给llm发指令的时候\",{\"1\":{\"196\":1}}],[\"我们需要逐步迭代构建优质的\",{\"1\":{\"291\":1}}],[\"我们需要收集数据并进行预处理\",{\"1\":{\"291\":1}}],[\"我们需要针对我们所设计的功能\",{\"1\":{\"291\":1}}],[\"我们需要在其对应的crossentropyloss中指定ignore\",{\"1\":{\"227\":1}}],[\"我们需要读取并构建batch数据\",{\"1\":{\"225\":1}}],[\"我们需要通过prompt\",{\"1\":{\"197\":1}}],[\"我们需要根据上面给出的花卉数据集下载链接\",{\"1\":{\"93\":1}}],[\"我们还用了\",{\"1\":{\"196\":1}}],[\"我们还有其他的选择\",{\"1\":{\"92\":1}}],[\"我们让llm对一段文字进行总结\",{\"1\":{\"196\":1}}],[\"我们发给llm的批令\",{\"1\":{\"196\":1}}],[\"我们直接用自然语言丢给他就去执行就好了\",{\"1\":{\"194\":1}}],[\"我们对prompt进行优化\",{\"1\":{\"194\":1}}],[\"我们看一下矩阵\",{\"1\":{\"190\":1}}],[\"我们微调大模型的流程就变为了\",{\"1\":{\"187\":1}}],[\"我们现在看到的这些大语言模型\",{\"1\":{\"184\":1}}],[\"我们所说的\",{\"1\":{\"178\":1}}],[\"我们取其补集\",{\"1\":{\"168\":1}}],[\"我们通常取其补集\",{\"1\":{\"166\":1}}],[\"我们通过把复杂问题拆解成一个个的简单问题\",{\"1\":{\"200\":1}}],[\"我们通过线性变换得到\",{\"1\":{\"133\":1}}],[\"我们通过自定义一个patchembed类完成上述工作\",{\"1\":{\"109\":1}}],[\"我们通过利用clip模型的多模态能力\",{\"1\":{\"91\":1}}],[\"我们选择的阈值取决于哪个指标对特定用例而言最重要\",{\"1\":{\"162\":1}}],[\"我们可以基于\",{\"1\":{\"291\":1}}],[\"我们可以根据自身需求灵活地进行组合\",{\"1\":{\"287\":1}}],[\"我们可以轻松地构建如下所示的\",{\"1\":{\"286\":1}}],[\"我们可以对ltm\",{\"1\":{\"200\":1}}],[\"我们可以利用矩阵分解技术\",{\"1\":{\"187\":1}}],[\"我们可以从以下角度理解\",{\"1\":{\"129\":1}}],[\"我们可以直接使用类别标签作为文本描述\",{\"1\":{\"92\":1}}],[\"我们来一步步分析这个过程\",{\"1\":{\"122\":1}}],[\"我们首先应该明确\",{\"1\":{\"291\":1}}],[\"我们首先需要确定开发的目标\",{\"1\":{\"291\":1}}],[\"我们首先拿到属于上下文的一对句子\",{\"1\":{\"219\":1}}],[\"我们首先获取图片库中所有图片\",{\"1\":{\"94\":1}}],[\"我们首先创建了各类别的文本描述\",{\"1\":{\"91\":1}}],[\"我们常常需要衡量文本嵌入和图片嵌入之间的相似度\",{\"1\":{\"93\":1}}],[\"我们使用了\",{\"1\":{\"92\":1}}],[\"我们也可以对得到的余弦相似度计算softmax\",{\"1\":{\"91\":1}}],[\"我们已经探讨了clip模型的运作机制\",{\"1\":{\"91\":1}}],[\"我这个点最近的3个熟人是谁\",{\"1\":{\"57\":1}}],[\"我不感兴趣\",{\"1\":{\"49\":1}}],[\"身份证号\",{\"1\":{\"49\":1}}],[\"代理服务以及回调处理等关键组件\",{\"1\":{\"288\":1}}],[\"代理\",{\"1\":{\"287\":1}}],[\"代理任务通常是辅助进行表征学习\",{\"1\":{\"96\":1}}],[\"代价是非常高的\",{\"1\":{\"189\":1}}],[\"代表\",{\"1\":{\"115\":2,\"118\":1,\"278\":1}}],[\"代表的是模型的基础\",{\"1\":{\"108\":1}}],[\"代表每个\",{\"1\":{\"49\":1}}],[\"代表原始点云中每个点的\",{\"1\":{\"49\":1}}],[\"代码执行\",{\"1\":{\"278\":1}}],[\"代码解析\",{\"1\":{\"169\":1}}],[\"代码链接\",{\"1\":{\"76\":1,\"77\":1}}],[\"代码实现如下\",{\"1\":{\"50\":1}}],[\"代码实现\",{\"0\":{\"25\":1,\"49\":1,\"56\":1,\"254\":1},\"1\":{\"64\":1,\"166\":1,\"167\":1,\"168\":1,\"169\":1,\"172\":1}}],[\"代码\",{\"0\":{\"63\":1},\"1\":{\"4\":1,\"7\":1,\"17\":1,\"97\":1}}],[\"到向量存储\",{\"1\":{\"288\":1}}],[\"到第\",{\"1\":{\"255\":1}}],[\"到三维\",{\"1\":{\"226\":1}}],[\"到一个标准姿态\",{\"1\":{\"64\":1}}],[\"到\",{\"1\":{\"49\":1,\"160\":1,\"278\":1}}],[\"到红色\",{\"1\":{\"40\":1}}],[\"便于从\",{\"1\":{\"226\":1}}],[\"便于广播到整个\",{\"1\":{\"65\":1}}],[\"便于广播\",{\"1\":{\"49\":1}}],[\"便于后续处理\",{\"1\":{\"64\":1}}],[\"便于后续计算\",{\"1\":{\"57\":1,\"166\":1,\"167\":1,\"170\":1,\"172\":1}}],[\"便于后续统一评估\",{\"1\":{\"39\":1}}],[\"便于后续\",{\"1\":{\"15\":1}}],[\"<=\",{\"1\":{\"225\":3,\"253\":1}}],[\"<unk>\",{\"1\":{\"175\":1,\"224\":1}}],[\"<pad>\",{\"1\":{\"175\":1}}],[\"<line\",{\"1\":{\"175\":1,\"176\":1}}],[\"<环境名>\",{\"1\":{\"140\":1,\"141\":1}}],[\"<stop>\",{\"1\":{\"81\":6}}],[\"<\",{\"1\":{\"49\":1,\"172\":1,\"175\":6,\"176\":2,\"224\":1,\"227\":1}}],[\"次从\",{\"1\":{\"211\":1}}],[\"次并与每个点的局部特征拼接\",{\"1\":{\"69\":1}}],[\"次并与每个点的局部特征\",{\"1\":{\"66\":1}}],[\"次\",{\"1\":{\"49\":1,\"68\":1}}],[\"重要说明\",{\"1\":{\"147\":1}}],[\"重\",{\"1\":{\"98\":8}}],[\"重点是保持参数尺寸最小化\",{\"1\":{\"192\":1}}],[\"重点是如何理解这里的分组\",{\"1\":{\"29\":1}}],[\"重点训练图像和文本特征提取\",{\"1\":{\"98\":1}}],[\"重复惩罚系数\",{\"1\":{\"104\":1}}],[\"重复\",{\"1\":{\"49\":1}}],[\"初始时没有任何语义信息\",{\"1\":{\"110\":1}}],[\"初始时随机选择一个点作为第一个中心点\",{\"1\":{\"49\":1}}],[\"初始文本token\",{\"1\":{\"104\":1}}],[\"初始为\",{\"1\":{\"104\":1}}],[\"初始假设变换为恒等变换\",{\"1\":{\"64\":1}}],[\"初始设为一个极大值\",{\"1\":{\"49\":1}}],[\"初始化一个简化版的\",{\"1\":{\"226\":1}}],[\"初始化为\",{\"1\":{\"190\":1}}],[\"初始化两个低秩矩阵\",{\"1\":{\"187\":1}}],[\"初始化低秩矩阵\",{\"1\":{\"187\":1}}],[\"初始化自定义数据集类\",{\"1\":{\"107\":1}}],[\"初始化文本输入\",{\"1\":{\"104\":1}}],[\"初始化query\",{\"1\":{\"100\":1}}],[\"初始化函数\",{\"1\":{\"57\":1,\"166\":1,\"167\":1,\"168\":1,\"169\":1,\"170\":1,\"172\":1}}],[\"初始化pointrefer模型\",{\"1\":{\"40\":1}}],[\"初始化\",{\"1\":{\"39\":1,\"58\":1,\"109\":1,\"110\":1,\"190\":5}}],[\"初始化损失函数\",{\"1\":{\"37\":1}}],[\"初始化模型\",{\"1\":{\"37\":1}}],[\"初始化用于存储每个样本拼接后输入和\",{\"1\":{\"13\":1}}],[\"把layernorm放到了前面\",{\"1\":{\"265\":1}}],[\"把llm的慢思考调动起来\",{\"1\":{\"197\":1}}],[\"把这些\",{\"1\":{\"255\":1}}],[\"把这些点的坐标归一化到以质心为中心的局部坐标系下\",{\"1\":{\"53\":1}}],[\"把复杂问题分解成一系列的简单子问题\",{\"1\":{\"200\":1}}],[\"把要求尽可能明确\",{\"1\":{\"196\":1}}],[\"把人类的反馈\",{\"1\":{\"180\":1}}],[\"把相似度矩阵对角线元素置为负无穷大\",{\"1\":{\"102\":1}}],[\"把query\",{\"1\":{\"102\":1}}],[\"把q\",{\"1\":{\"100\":1}}],[\"把数据转换成llm能识别的格式\",{\"1\":{\"99\":1}}],[\"把全局特征复制\",{\"1\":{\"66\":1}}],[\"把它们\",{\"1\":{\"64\":1}}],[\"把它们相对于关键点的位置进行归一化\",{\"1\":{\"49\":1}}],[\"把不同尺度学到的特征拼接在一起\",{\"1\":{\"53\":1}}],[\"把邻域点的数据整理成适合卷积的格式\",{\"1\":{\"49\":1}}],[\"把邻近点的坐标和特征拼接在一起\",{\"1\":{\"49\":1}}],[\"把原始点云\",{\"1\":{\"49\":1}}],[\"把距离超过\",{\"1\":{\"49\":1}}],[\"把刚才找到的邻近点的坐标提取出来\",{\"1\":{\"49\":1}}],[\"把他的大小归一化到一个球中\",{\"1\":{\"43\":1}}],[\"球查询\",{\"1\":{\"49\":2}}],[\"局部建模能力弱\",{\"1\":{\"69\":1}}],[\"局部特征实现上下文感知\",{\"1\":{\"69\":1}}],[\"局部特征编码\",{\"1\":{\"49\":1}}],[\"局部特征学习器\",{\"1\":{\"43\":1}}],[\"局部区域\",{\"1\":{\"49\":1}}],[\"局部区域中的每个点将相对于形心所在位置进行调整\",{\"1\":{\"48\":1}}],[\"局部区域中的点转换成相对于形心的局部坐标系\",{\"1\":{\"48\":1}}],[\"局部坐标系转换\",{\"1\":{\"48\":1}}],[\"距离越近\",{\"1\":{\"57\":1}}],[\"距离直观性\",{\"1\":{\"47\":1}}],[\"距离的度量不受空间中位置的影响\",{\"1\":{\"47\":1}}],[\"均能支持流式处理\",{\"1\":{\"288\":1}}],[\"均是混合推理模型\",{\"1\":{\"278\":1}}],[\"均冻结\",{\"1\":{\"98\":1}}],[\"均匀性假设\",{\"1\":{\"47\":1}}],[\"均值作为最终评估指标\",{\"1\":{\"39\":1}}],[\"半径太小可能无法有效捕获足够的局部详细\",{\"1\":{\"47\":1}}],[\"该架构巧妙地整合了从庞大知识库中检索到的相关信息\",{\"1\":{\"283\":1}}],[\"该\",{\"1\":{\"230\":1}}],[\"该过程由make\",{\"1\":{\"225\":1}}],[\"该任务主要是对一个给定句子\",{\"1\":{\"212\":1}}],[\"该设置不需要这些目标任务和无标记语料库是一个领域的\",{\"1\":{\"204\":1}}],[\"该系数也等于f1得分\",{\"1\":{\"170\":1}}],[\"该参数未在当前代码中使用\",{\"1\":{\"166\":1}}],[\"该模块可以为后续的分类任务提供更具区分性和稳定性的特征表示\",{\"1\":{\"114\":1}}],[\"该模型在上下文\",{\"1\":{\"207\":1}}],[\"该模型在文本上处理长期依赖提供了更结构化的内存\",{\"1\":{\"204\":1}}],[\"该模型是在\",{\"1\":{\"118\":1}}],[\"该模型联合训练一个cnn和文本transformer来预测图像的文本描述\",{\"1\":{\"96\":1}}],[\"该模型的输入是一对\",{\"1\":{\"78\":1}}],[\"该类的作用是将二维图像分割成多个图像块\",{\"1\":{\"109\":1}}],[\"该方法正是clip在vlp领域发扬光大的\",{\"1\":{\"101\":1}}],[\"该方法能更好的覆盖整个点集\",{\"1\":{\"46\":1}}],[\"该函数作用是针对给定的图片路径\",{\"1\":{\"93\":1}}],[\"该示例中的任务涉及8个类别\",{\"1\":{\"91\":1}}],[\"该范围确保局部区域的尺度是固定的\",{\"1\":{\"47\":1}}],[\"文章生成和情境理解方面表现出色\",{\"1\":{\"279\":1}}],[\"文章中提炼出来\",{\"1\":{\"223\":1}}],[\"文心大模型包括\",{\"1\":{\"278\":1}}],[\"文心一言网页版分为\",{\"1\":{\"278\":1}}],[\"文心一言的中文能力相对来说非常不错\",{\"1\":{\"278\":1}}],[\"文心一言的基础模型文心大模型于\",{\"1\":{\"278\":1}}],[\"文心一言是基于百度文心大模型的知识增强语言大模型\",{\"1\":{\"278\":1}}],[\"文心一言\",{\"1\":{\"277\":1,\"278\":1}}],[\"文件中的\",{\"1\":{\"259\":1}}],[\"文件进行调试即可\",{\"1\":{\"232\":1}}],[\"文档\",{\"1\":{\"209\":1}}],[\"文字搜索图像\",{\"0\":{\"94\":1}}],[\"文字搜索图像实战演练\",{\"1\":{\"87\":1}}],[\"文中的𝐶所表示的其他信息\",{\"1\":{\"48\":1}}],[\"文中作者通过ball\",{\"1\":{\"47\":1}}],[\"文本蕴含\",{\"1\":{\"209\":1}}],[\"文本蕴含提升5\",{\"1\":{\"204\":1}}],[\"文本蕴含提升1\",{\"1\":{\"203\":1}}],[\"文本生成\",{\"1\":{\"204\":1}}],[\"文本生成阶段\",{\"1\":{\"103\":1}}],[\"文本分词\",{\"1\":{\"233\":1}}],[\"文本分类任务\",{\"1\":{\"233\":1}}],[\"文本分类\",{\"1\":{\"209\":1,\"221\":2}}],[\"文本分离\",{\"1\":{\"203\":1}}],[\"文本分支\",{\"1\":{\"98\":2}}],[\"文本对应的标签\",{\"1\":{\"233\":1}}],[\"文本对\",{\"1\":{\"100\":1}}],[\"文本描述的生成也是一个关键环节\",{\"1\":{\"92\":1}}],[\"文本描述生成\",{\"0\":{\"92\":1}}],[\"文本编码器使用的是基于\",{\"1\":{\"93\":1}}],[\"文本编码器\",{\"1\":{\"90\":1,\"91\":1}}],[\"文本编码器的作用是提取文本的特征\",{\"1\":{\"90\":1}}],[\"文本\",{\"1\":{\"81\":1,\"103\":2}}],[\"文本输入格式处理\",{\"1\":{\"40\":1}}],[\"文本特征\",{\"1\":{\"32\":2}}],[\"文本引导的点特征分组\",{\"0\":{\"29\":1}}],[\"样本的得分是用生成模型分配的tokens的平均对数概率\",{\"1\":{\"213\":1}}],[\"样本为文本对\",{\"1\":{\"212\":1}}],[\"样本级别\",{\"1\":{\"169\":1}}],[\"样本分布偏差\",{\"1\":{\"46\":1}}],[\"样本数\",{\"1\":{\"22\":1}}],[\"具备基本能力的\",{\"1\":{\"291\":1}}],[\"具备对文本\",{\"1\":{\"278\":1}}],[\"具备跨模态交互能力\",{\"1\":{\"81\":1}}],[\"具体位置在\",{\"1\":{\"232\":1}}],[\"具体核心代码实现如下\",{\"1\":{\"227\":1}}],[\"具体过程如下图所示\",{\"1\":{\"226\":2}}],[\"具体步骤为\",{\"1\":{\"225\":1}}],[\"具体流程我们可以看下面这幅图\",{\"1\":{\"221\":1}}],[\"具体地如下图所示\",{\"1\":{\"219\":1}}],[\"具体地描述出来\",{\"1\":{\"196\":1}}],[\"具体要考虑解决的问题类型\",{\"1\":{\"198\":1}}],[\"具体来看\",{\"1\":{\"189\":1}}],[\"具体来说\",{\"1\":{\"52\":1,\"111\":1,\"182\":1,\"183\":1,\"286\":1}}],[\"具体为vit\",{\"1\":{\"118\":1}}],[\"具体解释如下\",{\"1\":{\"111\":1}}],[\"具体方式可以是直接缩放\",{\"1\":{\"108\":1}}],[\"具体代码实现如下\",{\"1\":{\"94\":1}}],[\"具体使用的是\",{\"1\":{\"93\":1}}],[\"具体的实验结果可以参考clip公开的notebook\",{\"1\":{\"92\":1}}],[\"具体包括\",{\"1\":{\"81\":1}}],[\"具体分为三步\",{\"1\":{\"78\":1}}],[\"具体而言\",{\"1\":{\"54\":1}}],[\"具体做法\",{\"1\":{\"47\":1}}],[\"具体选择多少个中心点以及邻域内的数量由超参数确定\",{\"1\":{\"46\":1}}],[\"具有出色的能力\",{\"1\":{\"279\":1}}],[\"具有较强的泛化能力\",{\"1\":{\"118\":1}}],[\"具有很强的zero\",{\"1\":{\"98\":1}}],[\"具有高度多样性和挑战性\",{\"1\":{\"81\":1}}],[\"具有以下优势\",{\"1\":{\"35\":1}}],[\"具有语义意义\",{\"1\":{\"19\":1}}],[\"选项内容\",{\"1\":{\"257\":1}}],[\"选择合适的预训练好的大模型\",{\"1\":{\"187\":1}}],[\"选择性能最佳的损失函数\",{\"1\":{\"173\":1}}],[\"选择适合的损失函数\",{\"1\":{\"173\":1}}],[\"选择最大值作为这个图文对的相似度\",{\"1\":{\"101\":1}}],[\"选择与图像特征相似度最高的文本所对应的类别\",{\"1\":{\"91\":1}}],[\"选择了一个包含6300万参数的transformer模型\",{\"1\":{\"90\":1}}],[\"选择𝑁个点\",{\"1\":{\"46\":1}}],[\"选出投票最多的结果\",{\"1\":{\"199\":1}}],[\"选出他们认为最好的答案\",{\"1\":{\"78\":1}}],[\"选出\",{\"1\":{\"53\":1}}],[\"选关键点\",{\"1\":{\"49\":1}}],[\"选取物体\",{\"1\":{\"20\":1}}],[\"邻近点\",{\"1\":{\"45\":1}}],[\"三元组\",{\"1\":{\"209\":1}}],[\"三是接下来要解答的子问题\",{\"1\":{\"200\":1}}],[\"三\",{\"0\":{\"128\":1,\"142\":1},\"1\":{\"69\":1}}],[\"三次sample\",{\"1\":{\"50\":1}}],[\"三层分层特征学习结构\",{\"1\":{\"50\":1}}],[\"三部分组成\",{\"1\":{\"45\":1}}],[\"三个规模的模型\",{\"1\":{\"278\":1}}],[\"三个标记的点\",{\"1\":{\"162\":1}}],[\"三个模块\",{\"1\":{\"98\":1}}],[\"三个流程\",{\"1\":{\"36\":1}}],[\"三个关键步骤的实现\",{\"1\":{\"32\":1}}],[\"导致用户接收到的信息不准确\",{\"1\":{\"283\":1}}],[\"导致最终构建得到的字典过大并且还有很多噪声\",{\"1\":{\"224\":1}}],[\"导致\",{\"1\":{\"169\":1,\"190\":1}}],[\"导致模型无法捕捉到更细粒度的几何细节\",{\"1\":{\"69\":1}}],[\"导致所有的特征\",{\"1\":{\"43\":1}}],[\"导致本论文复现流程暂时终止\",{\"1\":{\"7\":1}}],[\"会将这些输入展平\",{\"1\":{\"257\":1}}],[\"会将输入图像分割成大小为\",{\"1\":{\"118\":1}}],[\"会去分步骤思考\",{\"1\":{\"197\":1}}],[\"会有多个用于不同目的的权重参数矩阵\",{\"1\":{\"178\":1}}],[\"会降低也是如此\",{\"1\":{\"162\":1}}],[\"会在get\",{\"1\":{\"103\":1}}],[\"会从缓存中取出对应层先前缓存的key\",{\"1\":{\"103\":1}}],[\"会使用\",{\"1\":{\"103\":1}}],[\"会进行相应的错误提示并返回\",{\"1\":{\"93\":1}}],[\"会进行推理能力结果更新\",{\"1\":{\"40\":1}}],[\"会导致非正交\",{\"1\":{\"64\":1}}],[\"会影响特征提取的一致性\",{\"1\":{\"64\":1}}],[\"会强制标准输出也像标准错误一样\",{\"1\":{\"40\":1}}],[\"示例中设置为了1\",{\"1\":{\"175\":1}}],[\"示例\",{\"1\":{\"141\":1,\"255\":1}}],[\"示例3\",{\"1\":{\"140\":1}}],[\"示例2\",{\"1\":{\"140\":1}}],[\"示例1\",{\"1\":{\"140\":1}}],[\"示例场景\",{\"1\":{\"85\":1}}],[\"示例文本查询\",{\"1\":{\"40\":1}}],[\"示例数据\",{\"1\":{\"40\":1}}],[\"再针对性进行优化即可\",{\"1\":{\"291\":1}}],[\"再向量化存储到数据库中\",{\"1\":{\"291\":1}}],[\"再对每个选项做分类打分\",{\"1\":{\"257\":1}}],[\"再经过非线性变换后\",{\"1\":{\"226\":1}}],[\"再按照80\",{\"1\":{\"225\":1}}],[\"再使用\",{\"1\":{\"211\":2}}],[\"再在其中添加一个分隔符得到\",{\"1\":{\"209\":1}}],[\"再没有额外的成本\",{\"1\":{\"184\":1}}],[\"再不行尝试源码编译安装\",{\"1\":{\"147\":1}}],[\"再和真实标签做交叉熵损失\",{\"1\":{\"114\":1}}],[\"再和缓存的key\",{\"1\":{\"103\":1}}],[\"再乘以缩放因子scale\",{\"1\":{\"113\":1}}],[\"再次提取被掩码位置的表示\",{\"1\":{\"226\":1}}],[\"再次应用\",{\"1\":{\"112\":1}}],[\"再次用\",{\"1\":{\"82\":1}}],[\"再映射为\",{\"1\":{\"109\":1}}],[\"再来回顾我们的卷积层计算公式\",{\"1\":{\"109\":1}}],[\"再从中心位置裁剪成224x224\",{\"1\":{\"108\":1}}],[\"再进行归一化和标准化处理\",{\"1\":{\"108\":2}}],[\"再通过\",{\"1\":{\"103\":1,\"169\":1}}],[\"再通过第一个卷积层提取初始特征\",{\"1\":{\"66\":1}}],[\"再与每个点的局部特征拼接\",{\"1\":{\"68\":1}}],[\"再用mlp提提神\",{\"1\":{\"57\":1}}],[\"再用\",{\"1\":{\"49\":1}}],[\"再决定要使用什么文本查询\",{\"1\":{\"40\":1}}],[\"再将融合信息返回点空间\",{\"1\":{\"32\":1}}],[\"白色背景\",{\"1\":{\"40\":1}}],[\"蓝色标记出的部分是提供给llm的示例\",{\"1\":{\"198\":1}}],[\"蓝色\",{\"1\":{\"40\":1}}],[\"蓝色=背景\",{\"1\":{\"40\":1}}],[\"灰色\",{\"1\":{\"40\":1}}],[\"红色=功能区域\",{\"1\":{\"40\":1}}],[\"红色\",{\"1\":{\"40\":2}}],[\"渐变\",{\"1\":{\"40\":1}}],[\"渐变颜色\",{\"1\":{\"40\":1}}],[\"颜色\",{\"1\":{\"49\":1,\"71\":1}}],[\"颜色等\",{\"1\":{\"49\":1}}],[\"颜色等信息\",{\"1\":{\"16\":1}}],[\"颜色映射\",{\"1\":{\"40\":1}}],[\"增加网络的表达能力和非线性\",{\"1\":{\"278\":1}}],[\"增加模型大小或使用更多数据\",{\"1\":{\"277\":1}}],[\"增加旁路矩阵来模拟全参数微调\",{\"1\":{\"192\":1}}],[\"增加\",{\"1\":{\"191\":1}}],[\"增加了检索步骤的耗时\",{\"1\":{\"285\":1}}],[\"增加了模型层数\",{\"1\":{\"188\":1}}],[\"增加了图像的多样性\",{\"1\":{\"108\":1}}],[\"增加一些特定长度的特殊token\",{\"1\":{\"182\":1}}],[\"增加特征维度\",{\"1\":{\"55\":1}}],[\"增加batch维度\",{\"1\":{\"40\":1}}],[\"增强阶段\",{\"1\":{\"284\":1}}],[\"增强和生成四个阶段\",{\"1\":{\"284\":1}}],[\"增强了模型的推理和理解能力\",{\"1\":{\"283\":1}}],[\"增强了内容的可追溯性\",{\"1\":{\"283\":1}}],[\"增强了\",{\"1\":{\"278\":1}}],[\"增强整体模型的能力\",{\"1\":{\"192\":1}}],[\"增强区域匹配\",{\"1\":{\"167\":1}}],[\"增强后的点特征进行卷积\",{\"1\":{\"33\":1}}],[\"增强的点特征图\",{\"1\":{\"31\":1}}],[\"增强点特征的语义判别能力\",{\"1\":{\"27\":1}}],[\"增强稳定性\",{\"1\":{\"15\":1}}],[\"增强\",{\"1\":{\"15\":2}}],[\"增强空间特征表达\",{\"1\":{\"15\":1}}],[\"预热衰减方案\",{\"1\":{\"211\":1}}],[\"预编译的二进制包\",{\"1\":{\"147\":1}}],[\"预处理这个步骤在论文里并没有详细说明\",{\"1\":{\"108\":1}}],[\"预处理层\",{\"1\":{\"64\":1}}],[\"预训练和微调\",{\"1\":{\"279\":1}}],[\"预训练与微调\",{\"1\":{\"247\":1}}],[\"预训练任务\",{\"1\":{\"223\":1}}],[\"预训练对于获取不同级别信息的需要\",{\"1\":{\"205\":1}}],[\"预训练+微调\",{\"1\":{\"204\":1}}],[\"预训练过程\",{\"0\":{\"175\":1}}],[\"预训练权重大小为393mb\",{\"1\":{\"118\":1}}],[\"预训练模型下载下来之后\",{\"1\":{\"232\":1}}],[\"预训练模型很容易直接zero\",{\"1\":{\"96\":1}}],[\"预训练模型中\",{\"1\":{\"93\":1}}],[\"预训练模型名称\",{\"1\":{\"93\":1}}],[\"预训练模型路径\",{\"1\":{\"40\":1}}],[\"预训练好的\",{\"1\":{\"80\":1}}],[\"预训练是\",{\"1\":{\"80\":1}}],[\"预训练\",{\"0\":{\"80\":1,\"247\":1},\"1\":{\"277\":1}}],[\"预训练阶段实际上是将上述两个任务结合起来\",{\"1\":{\"220\":1}}],[\"预训练阶段非常关键\",{\"1\":{\"83\":1}}],[\"预训练阶段\",{\"1\":{\"79\":1}}],[\"预测正确的句对数量\",{\"1\":{\"228\":1}}],[\"预测正确的掩码词数量\",{\"1\":{\"228\":1}}],[\"预测被掩码的词\",{\"1\":{\"226\":1}}],[\"预测出该字的标签\",{\"1\":{\"221\":1}}],[\"预测出一个变换矩阵\",{\"1\":{\"66\":1}}],[\"预测值和真实值都为\",{\"1\":{\"167\":1}}],[\"预测概率\",{\"1\":{\"167\":1}}],[\"预测和真实中所有正类区域之和\",{\"1\":{\"166\":1}}],[\"预测为负但实际为正的像素数量\",{\"1\":{\"170\":1}}],[\"预测为负类\",{\"1\":{\"170\":1}}],[\"预测为负例\",{\"1\":{\"151\":1}}],[\"预测为正但实际为负的像素数量\",{\"1\":{\"170\":1}}],[\"预测为正且实际也为正的像素数量\",{\"1\":{\"170\":1}}],[\"预测为正类\",{\"1\":{\"170\":2}}],[\"预测为正类且实际也为正类的部分\",{\"1\":{\"166\":1}}],[\"预测为正例\",{\"1\":{\"151\":1}}],[\"预测图像对应的文本的词袋模型\",{\"1\":{\"96\":1}}],[\"预测头\",{\"1\":{\"55\":1}}],[\"预测掩码\",{\"1\":{\"40\":1}}],[\"预测结果可视化\",{\"1\":{\"40\":1}}],[\"预测结果\",{\"1\":{\"40\":1,\"226\":1}}],[\"预测点云的功能区域掩码\",{\"1\":{\"40\":1}}],[\"预测的\",{\"1\":{\"39\":1}}],[\"预测与\",{\"1\":{\"35\":1,\"166\":1}}],[\"预测\",{\"1\":{\"16\":1}}],[\"杀死训练进程\",{\"1\":{\"40\":1}}],[\"脚本时非常有用\",{\"1\":{\"40\":1}}],[\"当我们谈论大模型时\",{\"1\":{\"292\":1}}],[\"当我们在其他任务中使用预训练好的模型时\",{\"1\":{\"108\":1}}],[\"当时的研究主要集中在采用统计学习方法来预测词汇\",{\"1\":{\"277\":1}}],[\"当中包括\",{\"1\":{\"223\":1}}],[\"当然你也可以将所有词的\",{\"1\":{\"221\":1}}],[\"当数据集不均衡时\",{\"1\":{\"161\":1}}],[\"当数据量小于30m时\",{\"1\":{\"115\":1}}],[\"当精确率和召回率相差很大时\",{\"1\":{\"157\":1}}],[\"当精确率和召回率的值接近时\",{\"1\":{\"157\":1}}],[\"当精确率和召回率均为\",{\"1\":{\"157\":1}}],[\"当正例预测的准确性非常重要时\",{\"1\":{\"156\":1}}],[\"当假正例的代价高于假负例时使用\",{\"1\":{\"156\":1}}],[\"当假负例的代价高于假正例时使用\",{\"1\":{\"156\":1}}],[\"当较大时\",{\"1\":{\"136\":1}}],[\"当提到模型参数量时\",{\"1\":{\"115\":1}}],[\"当使用\",{\"1\":{\"107\":1}}],[\"当cnn具有以上两种归纳偏置\",{\"1\":{\"105\":1}}],[\"当拥有足够多的数据进行预训练的时候\",{\"1\":{\"105\":1}}],[\"当is\",{\"1\":{\"103\":1}}],[\"当文本和query\",{\"1\":{\"102\":1}}],[\"当\",{\"1\":{\"82\":2,\"112\":1,\"170\":3,\"189\":1}}],[\"当与\",{\"1\":{\"81\":1}}],[\"当输入点云非常稀疏时\",{\"1\":{\"69\":1}}],[\"当局部区域的密度较高时\",{\"1\":{\"54\":1}}],[\"当局部区域的密度较低时\",{\"1\":{\"54\":1}}],[\"当在\",{\"1\":{\"40\":1}}],[\"当前已经有的词数\",{\"1\":{\"224\":1}}],[\"当前实现未使用\",{\"1\":{\"169\":1}}],[\"当前token之前的text\",{\"1\":{\"103\":1}}],[\"当前输入通道数初始化为in\",{\"1\":{\"57\":1}}],[\"当前物体待预测的功能区域\",{\"1\":{\"40\":1}}],[\"当前物体类型\",{\"1\":{\"40\":1}}],[\"当前\",{\"1\":{\"39\":1}}],[\"就于\",{\"1\":{\"282\":1}}],[\"就像让计算机阅读\",{\"1\":{\"277\":1}}],[\"就点击这里直接下载\",{\"1\":{\"232\":1}}],[\"就足够了\",{\"1\":{\"191\":1}}],[\"就表示模型大约有\",{\"1\":{\"115\":1}}],[\"就意味着模型大约有\",{\"1\":{\"115\":1}}],[\"就完成了从图片到token之间的转换\",{\"1\":{\"109\":1}}],[\"就是没有答案\",{\"1\":{\"221\":1}}],[\"就是正常的词向量\",{\"1\":{\"219\":1}}],[\"就是随机遮盖或替换一句话里面的任意字或词\",{\"1\":{\"218\":1}}],[\"就是其中一个将此概念付诸实践的例子\",{\"1\":{\"217\":1}}],[\"就是大数据集有效\",{\"1\":{\"213\":1}}],[\"就是猜测被分配到高的概率值的token作为预测值\",{\"1\":{\"213\":1}}],[\"就是识别文本蕴含\",{\"1\":{\"212\":1}}],[\"就是我们要把给llm做的任务尽可能细化\",{\"1\":{\"196\":1}}],[\"就是利用量化技术的一个变体\",{\"1\":{\"192\":1}}],[\"就是利用一个卷积核大小为16x16\",{\"1\":{\"109\":1}}],[\"就是需要更新的参数\",{\"1\":{\"189\":1}}],[\"就是将结构化输入转换为有序序列以便作者预训练能处理\",{\"1\":{\"209\":1}}],[\"就是将y=wx中的w\",{\"1\":{\"183\":1}}],[\"就是将变成\",{\"1\":{\"182\":1}}],[\"就是将传统cnn和transformer进行结合\",{\"1\":{\"117\":1}}],[\"就是fft存在的上述两个问题\",{\"1\":{\"180\":1}}],[\"就是用特定的数据\",{\"1\":{\"180\":1}}],[\"就是一个不错的方案\",{\"1\":{\"179\":1}}],[\"就是一个序列数据转换的问题\",{\"1\":{\"178\":1}}],[\"就是根据问题找到相关内容并返回\",{\"1\":{\"132\":1}}],[\"就是两个线性层+gelu激活函数+dropout的结构\",{\"1\":{\"112\":1}}],[\"就是经过\",{\"1\":{\"31\":1}}],[\"就有了很多先验信息\",{\"1\":{\"105\":1}}],[\"就拼接起来\",{\"1\":{\"57\":1}}],[\"就更新它\",{\"1\":{\"49\":1}}],[\"就会立即在屏幕上显示一个字符\",{\"1\":{\"40\":1}}],[\"程序向\",{\"1\":{\"40\":1}}],[\"存在的问题\",{\"1\":{\"291\":1}}],[\"存在一定的噪声\",{\"1\":{\"96\":1}}],[\"存在缓存机制\",{\"1\":{\"40\":1}}],[\"存储每个类别的样本总数\",{\"1\":{\"107\":1}}],[\"存储每次选出的\",{\"1\":{\"49\":1}}],[\"存储验证集图片对应索引信息\",{\"1\":{\"107\":1}}],[\"存储验证集的所有图片路径\",{\"1\":{\"107\":1}}],[\"存储训练集图片对应索引信息\",{\"1\":{\"107\":1}}],[\"存储训练集的所有图片路径\",{\"1\":{\"107\":1}}],[\"存储数组\",{\"1\":{\"39\":1}}],[\"取每个位置的平均作为最终的匹配得分\",{\"1\":{\"102\":1}}],[\"取\",{\"1\":{\"102\":1}}],[\"取第一个cls\",{\"1\":{\"100\":1}}],[\"取出出现次数最多的字符对\",{\"1\":{\"176\":2}}],[\"取出该行中得分最大的那一列\",{\"1\":{\"94\":1}}],[\"取出当前批次的图像列表\",{\"1\":{\"93\":1}}],[\"取出当前最远点的坐标\",{\"1\":{\"49\":1}}],[\"取出当前样本对应的点云数据\",{\"1\":{\"25\":1}}],[\"取最小值\",{\"1\":{\"72\":1}}],[\"取最小的三个距离\",{\"1\":{\"57\":1}}],[\"取所有点的平均值\",{\"1\":{\"72\":1}}],[\"取所有点的最大值\",{\"1\":{\"72\":1}}],[\"取对应的索引\",{\"1\":{\"57\":1}}],[\"取均值或求和\",{\"1\":{\"166\":1}}],[\"取均值\",{\"1\":{\"39\":1}}],[\"||i\",{\"1\":{\"62\":1}}],[\"|\",{\"1\":{\"39\":4,\"228\":2,\"233\":2}}],[\"直到达到一个较为稳定\",{\"1\":{\"291\":1}}],[\"直到无法合并为止\",{\"1\":{\"176\":3}}],[\"直方图交集\",{\"1\":{\"39\":1}}],[\"直接更新检索知识库\",{\"1\":{\"285\":1}}],[\"直接在大规模强化学习\",{\"1\":{\"278\":1}}],[\"直接在监督学习任务上训练\",{\"1\":{\"213\":1}}],[\"直接处理和分析上传的数据文件\",{\"1\":{\"278\":1}}],[\"直接处理的原始点特征\",{\"1\":{\"54\":1}}],[\"直接微调模型\",{\"1\":{\"209\":1}}],[\"直接影响着大模型给出答案的正确与否\",{\"1\":{\"194\":1}}],[\"直接丢给它个指令\",{\"1\":{\"194\":1}}],[\"直接用prompt\",{\"1\":{\"179\":1}}],[\"直接优化\",{\"1\":{\"166\":1}}],[\"直接对图像进行分类\",{\"1\":{\"91\":2}}],[\"直接对所有点进行特征提取\",{\"1\":{\"49\":1}}],[\"直接使用原始任务描述\",{\"1\":{\"85\":1}}],[\"直接以点集作为输入\",{\"1\":{\"60\":1}}],[\"直接复制其特征到所有原始点\",{\"1\":{\"57\":1}}],[\"直接提取的特征\",{\"1\":{\"54\":1}}],[\"直接输入冻结参数的\",{\"1\":{\"104\":1}}],[\"直接输入\",{\"1\":{\"39\":1}}],[\"直接评价分割精度\",{\"1\":{\"39\":1}}],[\"直接添加\",{\"1\":{\"25\":1}}],[\"`\",{\"1\":{\"39\":1}}],[\"角度\",{\"1\":{\"39\":1}}],[\"方案退火到\",{\"1\":{\"211\":1}}],[\"方便后续的计算和比较\",{\"1\":{\"93\":1}}],[\"方法处理\",{\"1\":{\"211\":1}}],[\"方法的特定任务的输入改写\",{\"1\":{\"204\":1}}],[\"方法中\",{\"1\":{\"103\":1}}],[\"方法和应用场景有显著区别\",{\"1\":{\"85\":1}}],[\"方法二\",{\"1\":{\"82\":1}}],[\"方法一\",{\"1\":{\"82\":1}}],[\"方法\",{\"0\":{\"79\":1},\"1\":{\"39\":1,\"69\":1,\"71\":1,\"188\":1}}],[\"方向\",{\"1\":{\"10\":1}}],[\"方向研究\",{\"1\":{\"2\":1}}],[\"∞\",{\"1\":{\"39\":1,\"168\":2}}],[\"否\",{\"1\":{\"39\":8,\"85\":3,\"168\":3}}],[\"否则答案可能不合理\",{\"1\":{\"255\":1}}],[\"否则保存这两个句子的\",{\"1\":{\"224\":1}}],[\"否则应注释掉这行\",{\"1\":{\"166\":1}}],[\"否则为恒等映射\",{\"1\":{\"114\":1}}],[\"否则存入训练集\",{\"1\":{\"107\":1}}],[\"否则\",{\"1\":{\"69\":1}}],[\"否则只有\",{\"1\":{\"50\":1}}],[\"否则使用恒等映射\",{\"1\":{\"112\":1}}],[\"否则使用\",{\"1\":{\"49\":1,\"109\":2}}],[\"否则使用给定的\",{\"1\":{\"13\":1}}],[\"否则固定返回问题0\",{\"1\":{\"25\":1}}],[\"❌\",{\"1\":{\"39\":8,\"60\":1,\"73\":1,\"168\":3,\"172\":6,\"255\":2}}],[\"✅\",{\"0\":{\"130\":1,\"131\":1,\"132\":1},\"1\":{\"39\":8,\"62\":5,\"64\":3,\"66\":2,\"68\":2,\"69\":12,\"72\":7,\"73\":1,\"168\":8,\"172\":14,\"253\":2,\"255\":4,\"257\":1}}],[\"成为了当今计算机科学和人工智能领域的重要研究和应用方向\",{\"1\":{\"279\":1}}],[\"成为了现象级爆火应用\",{\"1\":{\"278\":1}}],[\"成为史上增长最快的\",{\"1\":{\"278\":1}}],[\"成为当时史上用户增长最快的消费级应用程序\",{\"1\":{\"278\":1}}],[\"成本效益\",{\"1\":{\"278\":1}}],[\"成本较高\",{\"1\":{\"162\":1}}],[\"成多头格式\",{\"1\":{\"244\":1}}],[\"成功生成json文件\",{\"1\":{\"223\":2}}],[\"成一个大的局部区域\",{\"1\":{\"49\":1}}],[\"成\",{\"1\":{\"39\":1,\"64\":1,\"257\":1}}],[\"影响内容的可信度\",{\"1\":{\"283\":1}}],[\"影响模型性能\",{\"1\":{\"188\":1}}],[\"影响模型容量和梯度稳定性\",{\"1\":{\"131\":1}}],[\"影响比较大的问题\",{\"1\":{\"180\":1}}],[\"影响\",{\"1\":{\"39\":1}}],[\"能力\",{\"1\":{\"278\":2}}],[\"能不能自己生成答案\",{\"1\":{\"255\":1}}],[\"能缓解类别不平衡问题\",{\"1\":{\"167\":1}}],[\"能起作用的原因在于\",{\"1\":{\"110\":1}}],[\"能否基于互联网上的大量文本来预训练视觉模型\",{\"1\":{\"96\":1}}],[\"能根据用户指令回答问题\",{\"1\":{\"85\":1}}],[\"能够以结构化的格式返回信息\",{\"1\":{\"288\":1}}],[\"能够根据任务指令执行任务\",{\"1\":{\"280\":1}}],[\"能够理解和生成依赖于前文的文本内容\",{\"1\":{\"279\":1}}],[\"能够分析和理解用户提供的图片\",{\"1\":{\"278\":1}}],[\"能够通过检索对应应用场景数据的方式\",{\"1\":{\"283\":1}}],[\"能够通过自有数据\",{\"1\":{\"179\":1}}],[\"能够通过多层\",{\"1\":{\"110\":1}}],[\"能够动态地聚合图像信息\",{\"1\":{\"110\":1}}],[\"能够更好地聚合图像信息\",{\"1\":{\"110\":1}}],[\"能够自适应地选择最适合的特征尺度进行组合\",{\"1\":{\"53\":1}}],[\"能够很好地支持\",{\"1\":{\"19\":1}}],[\"能反映边缘响应质量\",{\"1\":{\"39\":1}}],[\"通义\",{\"1\":{\"278\":1}}],[\"通义千问是由阿里巴巴基于\",{\"1\":{\"278\":1}}],[\"通义千问\",{\"1\":{\"277\":1,\"278\":2}}],[\"通用大模型的架构\",{\"1\":{\"291\":1}}],[\"通用人工智能\",{\"1\":{\"282\":1}}],[\"通用的任务未知task\",{\"1\":{\"203\":1}}],[\"通用视觉助手\",{\"1\":{\"81\":1}}],[\"通俗易懂讲解lora微调\",{\"0\":{\"186\":1},\"1\":{\"186\":1}}],[\"通俗易懂解读bpe分词算法实现\",{\"0\":{\"174\":1},\"1\":{\"174\":1}}],[\"通俗讲人话\",{\"1\":{\"184\":1}}],[\"通道维度特征提取阶段\",{\"1\":{\"64\":1}}],[\"通常需要重新训练来保持知识和数据的更新\",{\"1\":{\"285\":1}}],[\"通常需要使用高性能的\",{\"1\":{\"279\":1}}],[\"通常具有巨大的参数规模\",{\"1\":{\"279\":1}}],[\"通常大模型由三个阶段构成\",{\"1\":{\"277\":1}}],[\"通常指包含数百亿\",{\"1\":{\"277\":1}}],[\"通常这些开源的大模型都是需要用自有数据进行微调\",{\"1\":{\"179\":1}}],[\"通常问题不大\",{\"1\":{\"179\":1}}],[\"通常\",{\"1\":{\"169\":2,\"179\":1,\"198\":1}}],[\"通常加入平滑项\",{\"1\":{\"168\":1}}],[\"通常用于分割模型中\",{\"1\":{\"167\":1}}],[\"通常最好优先考虑召回率\",{\"1\":{\"156\":1}}],[\"通常在大规模数据集\",{\"1\":{\"108\":1}}],[\"通常保留原始结构\",{\"1\":{\"85\":1}}],[\"通常是在1000亿参数\",{\"1\":{\"198\":1}}],[\"通常是\",{\"1\":{\"115\":1,\"254\":1}}],[\"通常是随机初始化或者初始化为零\",{\"1\":{\"111\":1}}],[\"通常是图像内容的全面视觉描述\",{\"1\":{\"80\":1}}],[\"通常是旋转或反射\",{\"1\":{\"64\":1}}],[\"通常会设置一个上限k\",{\"1\":{\"47\":1}}],[\"通常我们会使用多个\",{\"1\":{\"39\":1}}],[\"通过大语言模型的强大理解能力和生成能力\",{\"1\":{\"290\":1}}],[\"通过大量文本数据训练这些模型\",{\"1\":{\"277\":1}}],[\"通过大量\",{\"1\":{\"85\":1}}],[\"通过与\",{\"1\":{\"288\":1}}],[\"通过与其他\",{\"1\":{\"110\":1}}],[\"通过在训练集上训练模型\",{\"1\":{\"290\":1}}],[\"通过在特定数据集上进一步训练大语言模型\",{\"1\":{\"285\":1}}],[\"通过在长篇连续文本的多样化语料库上预训练\",{\"1\":{\"214\":1}}],[\"通过提供额外的背景知识和数据支持\",{\"1\":{\"283\":1}}],[\"通过提示词让它自己做任务\",{\"1\":{\"85\":1}}],[\"通过检索到的真实信息生成回答\",{\"1\":{\"285\":1}}],[\"通过检索和整合长文本信息\",{\"1\":{\"283\":1}}],[\"通过检索特定领域的相关文档\",{\"1\":{\"283\":1}}],[\"通过检索数据源\",{\"1\":{\"283\":1}}],[\"通过实时检索最新数据\",{\"1\":{\"283\":1}}],[\"通过采用\",{\"1\":{\"280\":1}}],[\"通过使用自然语言描述的多任务数据进行微调\",{\"1\":{\"280\":1}}],[\"通过理解上下文并生成相应输出的方式来执行任务\",{\"1\":{\"280\":1}}],[\"通过稀疏计算以经济的成本训练强大的模型\",{\"1\":{\"278\":1}}],[\"通过改进的对齐技术\",{\"1\":{\"278\":1}}],[\"通过分析前面的词汇来预测下一个词汇\",{\"1\":{\"277\":1}}],[\"通过分类头\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"通过填充掩码\",{\"1\":{\"261\":1}}],[\"通过调用\",{\"1\":{\"253\":1}}],[\"通过调整\",{\"1\":{\"172\":1}}],[\"通过全连接层\",{\"1\":{\"226\":1}}],[\"通过全连接层进行分类\",{\"1\":{\"50\":1}}],[\"通过引导llm解决子问题\",{\"1\":{\"200\":1}}],[\"通过引入两个可调节参数来增强模型对假阳性\",{\"1\":{\"170\":1}}],[\"通过把一个个的简单问题解决掉\",{\"1\":{\"200\":1}}],[\"通过解决这一系列的简单问题\",{\"1\":{\"200\":1}}],[\"通过增加旁路\",{\"1\":{\"192\":1}}],[\"通过简单有效的方案来达成轻量微调的目的\",{\"1\":{\"192\":1}}],[\"通过反向传播算法更新低秩矩阵\",{\"1\":{\"187\":1}}],[\"通过降低参数的精度\",{\"1\":{\"185\":1}}],[\"通过强化学习的方式\",{\"1\":{\"180\":1}}],[\"通过将核心组件\",{\"1\":{\"288\":1}}],[\"通过将键值\",{\"1\":{\"278\":1}}],[\"通过将查询\",{\"1\":{\"278\":2}}],[\"通过将图像切成小片\",{\"1\":{\"119\":1}}],[\"通过将特征映射到特定的维度并进行非线性变换\",{\"1\":{\"114\":1}}],[\"通过将局部区域中的每个点\",{\"1\":{\"48\":1}}],[\"通过encoder\",{\"1\":{\"114\":1}}],[\"通过投影层对合并后的张量进行线性变换\",{\"1\":{\"113\":1}}],[\"通过qkv线性层将输入x映射到dim\",{\"1\":{\"113\":1}}],[\"通过第二个全连接层\",{\"1\":{\"112\":1}}],[\"通过第一个全连接层\",{\"1\":{\"112\":1}}],[\"通过激活函数层\",{\"1\":{\"112\":1}}],[\"通过计算损失\",{\"1\":{\"110\":1}}],[\"通过自注意力机制与其他\",{\"1\":{\"110\":1}}],[\"通过随机裁剪可以增加训练数据的多样性\",{\"1\":{\"108\":1}}],[\"通过随机丢弃来模拟不同密度的采样\",{\"1\":{\"53\":1}}],[\"通过视觉编码器\",{\"1\":{\"104\":1}}],[\"通过学习image\",{\"1\":{\"102\":1}}],[\"通过一些自动化的手段将web\",{\"1\":{\"96\":1}}],[\"通过模型获取图片的特征嵌入\",{\"1\":{\"93\":1}}],[\"通过这种方式\",{\"1\":{\"91\":1}}],[\"通过这个阶段训练后\",{\"1\":{\"80\":1}}],[\"通过softmax函数转换后\",{\"1\":{\"91\":1}}],[\"通过结合人工标注数据和强化学习\",{\"1\":{\"78\":1}}],[\"通过机器生成的指令数据进行调优\",{\"1\":{\"78\":1}}],[\"通过pointnet获取每个形心多尺度信息\",{\"1\":{\"53\":1}}],[\"通过多个局部区域球查询提取不同尺度的局部特征\",{\"1\":{\"53\":1}}],[\"通过多层\",{\"1\":{\"50\":1,\"110\":1}}],[\"通过较大的邻域尺度避免过度稀疏的问题\",{\"1\":{\"52\":1}}],[\"通过较小的邻域尺度捕获细节\",{\"1\":{\"52\":1}}],[\"通过查找形心点周围的\",{\"1\":{\"45\":1}}],[\"通过局部特征学习器\",{\"1\":{\"43\":1}}],[\"通过\",{\"1\":{\"33\":2,\"62\":1,\"64\":1,\"103\":2,\"169\":1,\"290\":3}}],[\"通过适配器层将\",{\"1\":{\"10\":1}}],[\"需在多样化的应用场景中保持高效和准确\",{\"1\":{\"283\":1}}],[\"需调参\",{\"1\":{\"169\":1}}],[\"需二值化\",{\"1\":{\"168\":1}}],[\"需训练整个模型\",{\"1\":{\"85\":1}}],[\"需要进行切片\",{\"1\":{\"291\":1}}],[\"需要设计本应用所要提供的功能\",{\"1\":{\"291\":1}}],[\"需要首先构造训练集\",{\"1\":{\"290\":1}}],[\"需要额外的资源来支持检索机制和数据库的维护\",{\"1\":{\"285\":1}}],[\"需要谨慎\",{\"1\":{\"279\":1}}],[\"需要将\",{\"1\":{\"259\":1}}],[\"需要将输入图像调整为这个固定的尺寸\",{\"1\":{\"118\":1}}],[\"需要注意的一点是\",{\"1\":{\"232\":1}}],[\"需要对掩码数量不足max\",{\"1\":{\"227\":1}}],[\"需要对哪个物体做归一化呢\",{\"1\":{\"43\":1}}],[\"需要被掩码的词之上\",{\"1\":{\"225\":1}}],[\"需要做些修改以便用在这些任务上\",{\"1\":{\"209\":1}}],[\"需要更长的反应时间\",{\"1\":{\"197\":1}}],[\"需要特别说明\",{\"1\":{\"178\":1}}],[\"需要再将第一个添加的class\",{\"1\":{\"114\":1}}],[\"需要单独将这个token再提取出来\",{\"1\":{\"110\":1}}],[\"需要相对少的数据就可以学习一个比较好的模型\",{\"1\":{\"105\":1}}],[\"需要复制图像特征以匹配beam数量\",{\"1\":{\"104\":1}}],[\"需要数百个gpu训练数十天\",{\"1\":{\"98\":1}}],[\"需要大量的计算资源进行训练和推理\",{\"1\":{\"279\":1}}],[\"需要大量的数据标注\",{\"1\":{\"96\":1}}],[\"需要大量高质量指令数据\",{\"1\":{\"85\":1}}],[\"需要大量人工或机器生成的\",{\"1\":{\"85\":1}}],[\"需要捕捉局部结构\",{\"1\":{\"61\":1}}],[\"需要确保这些划分具有一定的一致性或共同结构\",{\"1\":{\"43\":1}}],[\"需要先将\",{\"1\":{\"39\":1}}],[\"需先\",{\"1\":{\"39\":2}}],[\"需结合\",{\"1\":{\"39\":1}}],[\"敏感度有限\",{\"1\":{\"39\":1}}],[\"下图按照时间线给出了\",{\"1\":{\"278\":1}}],[\"下图展示了\",{\"1\":{\"102\":1}}],[\"下一个句子预测损失\",{\"1\":{\"251\":1}}],[\"下一句预测\",{\"1\":{\"226\":2}}],[\"下一句话\",{\"1\":{\"219\":1}}],[\"下载地址\",{\"1\":{\"223\":1}}],[\"下句的\",{\"1\":{\"219\":1}}],[\"下表2是作者模型和之前sota模型nli的结果比较\",{\"1\":{\"212\":1}}],[\"下表中对比了vit\",{\"1\":{\"117\":1}}],[\"下采样后的点坐标数据\",{\"1\":{\"57\":1}}],[\"下采样点的特征\",{\"1\":{\"57\":1}}],[\"下采样点数量\",{\"1\":{\"57\":1}}],[\"下采样点对应的特征数据\",{\"1\":{\"57\":1}}],[\"下采样点特征\",{\"1\":{\"55\":1}}],[\"下采样点坐标\",{\"1\":{\"55\":1}}],[\"下的\",{\"1\":{\"39\":1}}],[\"下的表现\",{\"1\":{\"39\":1}}],[\"下面我们来看看论文中ltm的例子\",{\"1\":{\"200\":1}}],[\"下面我们来看论文中给的self\",{\"1\":{\"199\":1}}],[\"下面我们来看论文中给的cot的例子\",{\"1\":{\"198\":1}}],[\"下面我们将进入训练流程\",{\"1\":{\"118\":1}}],[\"下面我们将用于图片变换的transforms流水线和上面自定义的mydataset类都封装到dataloader去\",{\"1\":{\"108\":1}}],[\"下面的3是因为我们用一次矩阵运算得到了拼接在一起的q\",{\"1\":{\"113\":1}}],[\"下面所给出的代码实现\",{\"1\":{\"113\":1}}],[\"下面来实际展示一下效果\",{\"1\":{\"94\":1}}],[\"下面给出的是一个基于pointnet++的点云语义分割模型定义\",{\"1\":{\"58\":1}}],[\"下面详细介绍一下动态卷机核卷积的过程\",{\"1\":{\"33\":1}}],[\"下面只需要把以上的三个步骤按流程组织起来即可得到afm模块的完整实现了\",{\"1\":{\"32\":1}}],[\"绘制到图表中\",{\"1\":{\"159\":1}}],[\"绘制每种类别个数柱状图\",{\"1\":{\"107\":1}}],[\"绘制\",{\"1\":{\"39\":1,\"159\":1}}],[\"更大的词表\",{\"1\":{\"278\":1}}],[\"更长的上下文长度\",{\"1\":{\"278\":1}}],[\"更多的训练数据量\",{\"1\":{\"278\":1}}],[\"更进一步划分\",{\"1\":{\"278\":1}}],[\"更加符合人类的一些期望\",{\"1\":{\"180\":1}}],[\"更好的提升大模型在特定领域的能力\",{\"1\":{\"179\":1}}],[\"更好地理解和执行用户给出的自然语言指令\",{\"1\":{\"78\":1}}],[\"更好地应对\",{\"1\":{\"35\":1}}],[\"更稳定的训练过程\",{\"1\":{\"172\":1}}],[\"更稳定地收敛\",{\"1\":{\"167\":1}}],[\"更重视精确率\",{\"1\":{\"170\":1}}],[\"更重视召回率\",{\"1\":{\"170\":1}}],[\"更贴近最终评估指标\",{\"1\":{\"168\":1}}],[\"更贴近真实用户交互\",{\"1\":{\"85\":1}}],[\"更适合前景极少的小区域识别\",{\"1\":{\"167\":1}}],[\"更适合评估边界模糊区域\",{\"1\":{\"168\":1}}],[\"更适合评估\",{\"1\":{\"39\":1}}],[\"更关注召回率\",{\"1\":{\"172\":1}}],[\"更关注精确率\",{\"1\":{\"172\":1}}],[\"更关注整体区域匹配\",{\"1\":{\"166\":1}}],[\"更关注\",{\"1\":{\"166\":1}}],[\"更具体地说\",{\"1\":{\"160\":1}}],[\"更广泛地说\",{\"1\":{\"157\":1}}],[\"更强的任务泛化能力\",{\"1\":{\"85\":1}}],[\"更复杂的对称函数建模\",{\"1\":{\"72\":1}}],[\"更容易训练\",{\"1\":{\"64\":1}}],[\"更有效\",{\"1\":{\"62\":1}}],[\"更新成本高\",{\"1\":{\"285\":1}}],[\"更新至\",{\"1\":{\"278\":1}}],[\"更新特征数量为表示层的维度\",{\"1\":{\"114\":1}}],[\"更新下一层的输入通道数\",{\"1\":{\"57\":1}}],[\"更新索引\",{\"1\":{\"39\":1}}],[\"更高分辨率\",{\"1\":{\"54\":1}}],[\"更易于管理的子集\",{\"1\":{\"43\":1}}],[\"还具备网页浏览\",{\"1\":{\"278\":1}}],[\"还将一个工具方法整合到了分词器的实现之中\",{\"1\":{\"224\":1}}],[\"还有很多种\",{\"1\":{\"185\":1}}],[\"还有另一个方向\",{\"1\":{\"96\":1}}],[\"还需要对每个像素进行分类\",{\"1\":{\"164\":1}}],[\"还不如直接预测词袋模型\",{\"1\":{\"96\":1}}],[\"还在336的分辨率下额外进行了一个周期的微调\",{\"1\":{\"90\":1}}],[\"还考虑了从更低分辨率\",{\"1\":{\"54\":1}}],[\"还小\",{\"1\":{\"49\":1}}],[\"还能生成与\",{\"1\":{\"39\":1}}],[\"还能感知其他\",{\"1\":{\"33\":1}}],[\"还看响应强度分布\",{\"1\":{\"39\":1}}],[\"⚠️\",{\"1\":{\"39\":4,\"69\":1,\"72\":1,\"166\":1,\"168\":1,\"255\":1}}],[\"衡量空间重合度\",{\"1\":{\"39\":1}}],[\"衡量分类器整体判别能力\",{\"1\":{\"39\":1}}],[\"衡量分类器排序能力\",{\"1\":{\"39\":1}}],[\"衡量分布相似性\",{\"1\":{\"39\":2}}],[\"衡量逐点误差\",{\"1\":{\"39\":2}}],[\"衡量预测区域与真实标签之间的空间重合度\",{\"1\":{\"39\":1}}],[\"衡量预测掩码与\",{\"1\":{\"35\":1}}],[\"衡量模型对二分类问题的判别能力\",{\"1\":{\"39\":1}}],[\"衡量整体分布一致性\",{\"1\":{\"39\":1}}],[\"支持智能体工作负载\",{\"1\":{\"288\":1}}],[\"支持最大\",{\"1\":{\"278\":1}}],[\"支持思考模式和非思考模式之间无缝切换\",{\"1\":{\"278\":1}}],[\"支持更复杂的系统提示词控制\",{\"1\":{\"278\":1}}],[\"支持处理极长的文档和对话历史\",{\"1\":{\"278\":1}}],[\"支持多轮对话的同时\",{\"1\":{\"278\":1}}],[\"支持多轮视觉对话\",{\"1\":{\"81\":1}}],[\"支持多种自然语言处理任务\",{\"1\":{\"278\":1}}],[\"支持多工具并行调用与精准指令解析\",{\"1\":{\"278\":1}}],[\"支持标准模式与推理思考模式\",{\"1\":{\"278\":1}}],[\"支持自定义知识库和行为模式\",{\"1\":{\"278\":1}}],[\"支持手势识别和情感表达\",{\"1\":{\"278\":1}}],[\"支持加权和平均损失\",{\"1\":{\"166\":1}}],[\"支持的文件后缀类型\",{\"1\":{\"107\":1}}],[\"支持两种llm\",{\"1\":{\"104\":1}}],[\"支持刚性变换标准化\",{\"1\":{\"69\":1}}],[\"支持原始点云\",{\"1\":{\"69\":1}}],[\"支持\",{\"1\":{\"39\":2,\"166\":1,\"167\":1,\"226\":1,\"278\":1}}],[\"支持通过爱因斯坦求和约定\",{\"1\":{\"33\":1}}],[\"✔️\",{\"1\":{\"39\":11,\"167\":4,\"168\":3,\"172\":4}}],[\"特殊\",{\"1\":{\"255\":1}}],[\"特定任务输入转换\",{\"0\":{\"209\":1}}],[\"特别强化了对客观事实的准确性\",{\"1\":{\"278\":1}}],[\"特别适用于图像分割任务\",{\"1\":{\"172\":1}}],[\"特别是语言建模\",{\"1\":{\"223\":1}}],[\"特别是降后期的推理成本\",{\"1\":{\"185\":1}}],[\"特别是在像\",{\"1\":{\"174\":1}}],[\"特别是在遮挡严重的情况下\",{\"1\":{\"69\":1}}],[\"特别是\",{\"1\":{\"122\":1}}],[\"特点\",{\"1\":{\"167\":2,\"278\":4}}],[\"特点与作用\",{\"1\":{\"39\":4}}],[\"特性\",{\"1\":{\"39\":4,\"147\":1,\"168\":1}}],[\"特征比较\",{\"1\":{\"285\":2}}],[\"特征增强\",{\"1\":{\"72\":1}}],[\"特征融合\",{\"1\":{\"72\":1}}],[\"特征空间变换矩阵\",{\"1\":{\"66\":1}}],[\"特征变换开关\",{\"1\":{\"66\":1}}],[\"特征提取网络相对轻量\",{\"1\":{\"98\":1}}],[\"特征提取\",{\"0\":{\"66\":1}}],[\"特征插值方式\",{\"1\":{\"55\":1}}],[\"特征传播层\",{\"0\":{\"57\":1}}],[\"特征传播\",{\"1\":{\"55\":1,\"57\":1}}],[\"特征传播阶段结束\",{\"1\":{\"27\":1}}],[\"特征传播阶段\",{\"1\":{\"27\":1}}],[\"特征编码\",{\"1\":{\"48\":1}}],[\"特征通道维度\",{\"1\":{\"33\":1}}],[\"特征维度\",{\"1\":{\"16\":1}}],[\"特征从原始嵌入维度\",{\"1\":{\"12\":1}}],[\"特征\",{\"1\":{\"10\":2,\"57\":1}}],[\"标记进行下一句预测\",{\"1\":{\"226\":1}}],[\"标记\",{\"1\":{\"103\":1}}],[\"标记为\",{\"1\":{\"39\":1}}],[\"标准\",{\"1\":{\"169\":2}}],[\"标准交叉熵损失\",{\"1\":{\"169\":1}}],[\"标准差\",{\"1\":{\"72\":1}}],[\"标准化的意义\",{\"1\":{\"64\":1}}],[\"标准化输入点云和特征空间\",{\"1\":{\"60\":1}}],[\"标准错误\",{\"1\":{\"40\":1}}],[\"标准输出\",{\"1\":{\"40\":1}}],[\"标签平滑\",{\"1\":{\"227\":1}}],[\"标签二值化\",{\"1\":{\"39\":1}}],[\"标签\",{\"1\":{\"39\":1}}],[\"标注数据昂贵又耗时\",{\"1\":{\"204\":1}}],[\"标注数据组织形式\",{\"1\":{\"25\":1}}],[\"标注图像中的物体及其位置\",{\"1\":{\"80\":1}}],[\"标注\",{\"0\":{\"21\":1}}],[\"与用户负反馈\",{\"1\":{\"291\":1}}],[\"与合作伙伴包进行有效分离\",{\"1\":{\"288\":1}}],[\"与特定应用程序的数据进行交互的接口\",{\"1\":{\"287\":1}}],[\"与语言模型交互的接口\",{\"1\":{\"287\":1}}],[\"与以前的预训练语言模型\",{\"1\":{\"280\":1}}],[\"与基座模型有本质的区别\",{\"1\":{\"278\":1}}],[\"与lstms相比\",{\"1\":{\"213\":1}}],[\"与升维矩阵\",{\"1\":{\"189\":1}}],[\"与负样本\",{\"1\":{\"169\":1}}],[\"与背景的极端不平衡问题\",{\"1\":{\"169\":1}}],[\"与交叉熵损失函数相比\",{\"1\":{\"169\":1}}],[\"与对象检测任务不同\",{\"1\":{\"164\":1}}],[\"与之类似的还有\",{\"1\":{\"115\":1}}],[\"与其他\",{\"1\":{\"110\":1,\"168\":1}}],[\"与计算机视觉\",{\"1\":{\"89\":1}}],[\"与此同时\",{\"1\":{\"88\":1,\"151\":1,\"277\":1}}],[\"与原始点\",{\"1\":{\"49\":1}}],[\"与k最近邻\",{\"1\":{\"47\":1}}],[\"与\",{\"1\":{\"39\":1,\"82\":1,\"101\":2,\"102\":1,\"107\":1,\"159\":1,\"167\":1,\"189\":3,\"223\":1,\"278\":3}}],[\"与多模态嵌入\",{\"1\":{\"13\":1}}],[\"相较于传统的语言模型具有更强大的能力\",{\"1\":{\"283\":1}}],[\"相信未来会涌现出很多类似的以对话形态让助手完成各种具体工作的项目\",{\"1\":{\"282\":1}}],[\"相加\",{\"1\":{\"220\":1}}],[\"相关核心代码实现如下\",{\"1\":{\"223\":1}}],[\"相关工作\",{\"0\":{\"205\":1}}],[\"相关阅读资料\",{\"1\":{\"185\":1}}],[\"相对来说成本会比较高\",{\"1\":{\"180\":1}}],[\"相结合\",{\"1\":{\"172\":1}}],[\"相媲美的效果\",{\"1\":{\"119\":1}}],[\"相似度\",{\"1\":{\"209\":1}}],[\"相似度指标\",{\"1\":{\"170\":1}}],[\"相似度如下所示\",{\"1\":{\"91\":1}}],[\"相似性得分\",{\"1\":{\"39\":1}}],[\"相同\",{\"1\":{\"78\":1,\"170\":1,\"172\":1}}],[\"相比\",{\"1\":{\"278\":2}}],[\"相比于每次开发单个模型的方式\",{\"1\":{\"281\":1}}],[\"相比于著名的\",{\"1\":{\"223\":1}}],[\"相比于随机采样\",{\"1\":{\"46\":1}}],[\"相比其他替代方案如rnn\",{\"1\":{\"204\":1}}],[\"相比w\",{\"1\":{\"180\":1}}],[\"相比之下\",{\"1\":{\"69\":1,\"96\":1}}],[\"相当于为计算机提供了强大的\",{\"1\":{\"277\":1}}],[\"相当于直接输入一篇五万字的文章\",{\"1\":{\"109\":1}}],[\"相当于告诉模型\",{\"1\":{\"68\":1}}],[\"相当于加了一个\",{\"1\":{\"64\":1}}],[\"相当于跨空间位置的信息交换\",{\"1\":{\"30\":1}}],[\"相反\",{\"1\":{\"47\":1,\"54\":1,\"209\":1}}],[\"平衡正负样本数量\",{\"1\":{\"169\":1}}],[\"平衡类别数量\",{\"1\":{\"169\":1}}],[\"平衡因子\",{\"1\":{\"35\":1,\"169\":1}}],[\"平滑系数\",{\"1\":{\"170\":1,\"172\":1}}],[\"平滑项\",{\"1\":{\"166\":1,\"167\":2,\"168\":1,\"169\":1}}],[\"平滑处理\",{\"1\":{\"72\":1}}],[\"平移等变换具有鲁棒性\",{\"1\":{\"62\":1}}],[\"平移\",{\"1\":{\"61\":1,\"64\":1,\"73\":1}}],[\"平移中心到以关键点为原点的局部坐标系上\",{\"1\":{\"49\":1}}],[\"平均池化\",{\"1\":{\"72\":1}}],[\"平均值作为损失项\",{\"1\":{\"65\":1}}],[\"平均交并比\",{\"1\":{\"39\":1}}],[\"平均绝对误差\",{\"1\":{\"39\":1}}],[\"反应很快\",{\"1\":{\"197\":1}}],[\"反推出来的\",{\"1\":{\"169\":1}}],[\"反之\",{\"1\":{\"162\":1}}],[\"反之亦然\",{\"1\":{\"51\":1}}],[\"反过来计算text\",{\"1\":{\"101\":1}}],[\"反射\",{\"1\":{\"65\":1,\"69\":1,\"74\":1}}],[\"反距离加权插值\",{\"1\":{\"55\":1}}],[\"反映出互联网用户对于聊天和对话这种交互模式的偏好\",{\"1\":{\"282\":1}}],[\"反映模型是否准确学习语言引导下的响应强度\",{\"1\":{\"39\":1}}],[\"反映点与功能核心区域的距离远近\",{\"1\":{\"21\":1}}],[\"反向传播更新模型参数过程\",{\"1\":{\"187\":1}}],[\"反向传播\",{\"1\":{\"38\":2}}],[\"损失之间的权重分配\",{\"1\":{\"172\":1}}],[\"损失权重仅降低\",{\"1\":{\"169\":1}}],[\"损失权重降低\",{\"1\":{\"169\":1}}],[\"损失几乎不受影响\",{\"1\":{\"169\":1}}],[\"损失被大幅降低\",{\"1\":{\"169\":1}}],[\"损失越小表示预测越接近真实标签\",{\"1\":{\"168\":1}}],[\"损失结合在一起的一种损失函数\",{\"1\":{\"167\":1}}],[\"损失值逐渐降低\",{\"1\":{\"110\":1}}],[\"损失计算与反向传播\",{\"1\":{\"110\":1}}],[\"损失计算\",{\"1\":{\"38\":1}}],[\"损失函数直接作用于\",{\"1\":{\"110\":1}}],[\"损失函数定义\",{\"1\":{\"37\":1}}],[\"损失函数\",{\"0\":{\"34\":1,\"165\":1},\"1\":{\"168\":1,\"172\":1,\"227\":1}}],[\"场景\",{\"1\":{\"35\":1}}],[\"关键参数的作用\",{\"1\":{\"169\":1}}],[\"关键点可能丢失\",{\"1\":{\"69\":1}}],[\"关键点集\",{\"1\":{\"62\":1}}],[\"关于qlora的具体细节\",{\"1\":{\"185\":1}}],[\"关于lora的具体细节\",{\"1\":{\"184\":1}}],[\"关于计算\",{\"1\":{\"169\":1}}],[\"关于vit模型的不同版本\",{\"1\":{\"115\":1}}],[\"关于多头注意力机制流程不太清楚的\",{\"1\":{\"113\":1}}],[\"关于norm层\",{\"1\":{\"112\":1}}],[\"关于bertmodel的代码解析部分\",{\"1\":{\"102\":1}}],[\"关于这一领域的详细综述\",{\"1\":{\"92\":1}}],[\"关于我们\",{\"0\":{\"1\":1}}],[\"关注每个像素的分类准确性\",{\"1\":{\"172\":1}}],[\"关注每个点的分类误差\",{\"1\":{\"167\":1}}],[\"关注整体区域匹配程度\",{\"1\":{\"172\":1}}],[\"关注整体区域匹配\",{\"1\":{\"168\":1}}],[\"关注整体区域匹配度\",{\"1\":{\"167\":1}}],[\"关注整体掩码匹配度\",{\"1\":{\"35\":1}}],[\"关注排序能力\",{\"1\":{\"39\":1}}],[\"放大难分类样本的影响\",{\"1\":{\"169\":1}}],[\"放大难分类样本影响\",{\"1\":{\"35\":1}}],[\"放大\",{\"1\":{\"35\":1,\"169\":1}}],[\"抑制简单样本的梯度主导\",{\"1\":{\"169\":1}}],[\"抑制简单样本\",{\"1\":{\"169\":1}}],[\"抑制\",{\"1\":{\"35\":1,\"169\":1}}],[\"区分大语言模型\",{\"1\":{\"280\":1}}],[\"区分句子\",{\"1\":{\"226\":1}}],[\"区域匹配度\",{\"1\":{\"172\":1}}],[\"区域匹配误差\",{\"1\":{\"35\":1}}],[\"区域重叠误差\",{\"1\":{\"172\":1}}],[\"区域重合度\",{\"1\":{\"39\":1}}],[\"区间内\",{\"1\":{\"172\":1}}],[\"区间\",{\"1\":{\"33\":1,\"166\":1,\"168\":1}}],[\"合并头结果\",{\"1\":{\"244\":1}}],[\"合并的\",{\"1\":{\"211\":1}}],[\"合并参数\",{\"1\":{\"187\":1}}],[\"合并该字符对\",{\"1\":{\"176\":1}}],[\"合并当前最高频的字符对\",{\"1\":{\"175\":2}}],[\"合并前n个最频繁的字符对\",{\"1\":{\"175\":1}}],[\"合并\",{\"1\":{\"35\":1,\"187\":1}}],[\"时代的开启\",{\"1\":{\"277\":1}}],[\"时让它同时进行两个任务\",{\"1\":{\"217\":1}}],[\"时间\",{\"1\":{\"217\":1}}],[\"时间戳等\",{\"1\":{\"71\":1}}],[\"时才涌现出来的能力\",{\"1\":{\"198\":1}}],[\"时使用\",{\"1\":{\"112\":1}}],[\"时\",{\"1\":{\"35\":1,\"170\":4,\"189\":1,\"218\":1}}],[\"目前采用的主要改进如下\",{\"1\":{\"278\":1}}],[\"目前\",{\"1\":{\"277\":1,\"278\":2,\"283\":1,\"291\":1}}],[\"目前还没有达成共识\",{\"1\":{\"204\":1}}],[\"目前主流的方法包括2019年\",{\"1\":{\"188\":1}}],[\"目的\",{\"1\":{\"101\":1,\"102\":1,\"103\":1}}],[\"目的是为了让llm的推理能力能够更进一步提升\",{\"1\":{\"199\":1}}],[\"目的是\",{\"1\":{\"169\":1}}],[\"目的是让点云\",{\"1\":{\"64\":1}}],[\"目的是从一个大的数据集中选出一组代表性强的点\",{\"1\":{\"46\":1}}],[\"目的是测试模型对新组合的泛化能力\",{\"1\":{\"22\":1}}],[\"目的是评估模型在熟悉场景下的表现\",{\"1\":{\"22\":1}}],[\"目录获取所有图片路径\",{\"1\":{\"94\":1,\"95\":1}}],[\"目标人群\",{\"1\":{\"291\":1}}],[\"目标掩码\",{\"1\":{\"270\":1}}],[\"目标任务\",{\"1\":{\"204\":1}}],[\"目标点\",{\"1\":{\"169\":1}}],[\"目标检测\",{\"1\":{\"169\":1}}],[\"目标是学习一个全局表示\",{\"1\":{\"204\":1}}],[\"目标是训练好\",{\"1\":{\"100\":1}}],[\"目标是让模型在预训练的基础上进一步掌握多模态指令理解与复杂推理能力\",{\"1\":{\"81\":1}}],[\"目标让视觉编码器输出的图像特征与语言模型的词向量空间对齐\",{\"1\":{\"80\":1}}],[\"目标\",{\"1\":{\"55\":1,\"78\":1,\"169\":2,\"208\":1}}],[\"目标功能区域\",{\"1\":{\"35\":1}}],[\"负号\",{\"1\":{\"169\":1}}],[\"负样本\",{\"1\":{\"169\":1}}],[\"负样本batch2=0\",{\"1\":{\"102\":1}}],[\"负样本batch2\",{\"1\":{\"102\":3}}],[\"负样本batch1=0\",{\"1\":{\"102\":1}}],[\"负样本batch1\",{\"1\":{\"102\":3}}],[\"负类\",{\"1\":{\"35\":1}}],[\"负责组织多个\",{\"1\":{\"103\":1}}],[\"负责组织自注意力和交叉注意力的运算流程\",{\"1\":{\"103\":1}}],[\"负责组内信息混合\",{\"1\":{\"30\":2}}],[\"负责通道间信息混合\",{\"1\":{\"30\":2}}],[\"掩码机制\",{\"1\":{\"261\":1}}],[\"掩码语言建模\",{\"1\":{\"226\":1}}],[\"掩码符号\",{\"1\":{\"224\":1}}],[\"掩码候选位置\",{\"1\":{\"224\":1}}],[\"掩码高度匹配的功能区域\",{\"1\":{\"39\":1}}],[\"掩码之间的逐点偏差\",{\"1\":{\"39\":1}}],[\"掩码\",{\"1\":{\"35\":1,\"166\":1,\"168\":2,\"169\":1}}],[\"或开源模型来实现核心的理解与生成\",{\"1\":{\"290\":1}}],[\"或记忆\",{\"1\":{\"278\":1}}],[\"或更多\",{\"1\":{\"277\":1}}],[\"或两个句子\",{\"1\":{\"255\":1}}],[\"或释义发现\",{\"1\":{\"212\":1}}],[\"或准确率\",{\"1\":{\"156\":1}}],[\"或模型的误报率\",{\"1\":{\"154\":1}}],[\"或源码\",{\"1\":{\"147\":1}}],[\"或者私有化模型\",{\"1\":{\"286\":1}}],[\"或者直接命令行运行\",{\"1\":{\"232\":1}}],[\"或者更广义地\",{\"1\":{\"172\":1}}],[\"或者在达到某个最低准确性水平的情况下\",{\"1\":{\"156\":1}}],[\"或者一种错误\",{\"1\":{\"152\":1}}],[\"或者\",{\"1\":{\"103\":1,\"143\":1,\"175\":1}}],[\"或者说在于计算能力和数据集的规模\",{\"1\":{\"96\":1}}],[\"或前缀\",{\"1\":{\"85\":1}}],[\"或视觉语言模型性能的技术\",{\"1\":{\"85\":1}}],[\"或多任务场景下的表现\",{\"1\":{\"78\":1}}],[\"或图结构增强局部建模能力\",{\"1\":{\"69\":1}}],[\"或图像视图\",{\"1\":{\"60\":1}}],[\"或转置卷积\",{\"1\":{\"57\":1}}],[\"或每组邻域大小\",{\"1\":{\"52\":1}}],[\"或局部区域\",{\"1\":{\"43\":1}}],[\"或\",{\"1\":{\"35\":1,\"39\":1,\"40\":1,\"43\":1,\"55\":1,\"80\":1,\"140\":1,\"147\":1,\"166\":1,\"167\":2,\"170\":1,\"172\":2,\"279\":1}}],[\"或经过\",{\"1\":{\"35\":1,\"168\":1,\"169\":1}}],[\"或平均池化\",{\"1\":{\"33\":1}}],[\"前后端搭建\",{\"1\":{\"291\":1}}],[\"前置知识\",{\"0\":{\"276\":1}}],[\"前馈层\",{\"1\":{\"266\":2,\"269\":2}}],[\"前馈神经网络\",{\"1\":{\"261\":1}}],[\"前面是特殊token\",{\"1\":{\"224\":1}}],[\"前者是其2倍\",{\"1\":{\"223\":1}}],[\"前两步不变\",{\"1\":{\"187\":1}}],[\"前提是数据集大致平衡\",{\"1\":{\"162\":1}}],[\"前缀让\",{\"1\":{\"85\":1}}],[\"前的某一层\",{\"1\":{\"55\":1}}],[\"前向传播逻辑\",{\"1\":{\"226\":1}}],[\"前向传播计算\",{\"1\":{\"172\":1}}],[\"前向传播计算损失值\",{\"1\":{\"170\":1}}],[\"前向传播函数\",{\"1\":{\"57\":1,\"58\":1,\"109\":1,\"166\":1,\"167\":1,\"168\":1,\"169\":1}}],[\"前向传播\",{\"1\":{\"39\":1,\"110\":1}}],[\"前向传播过程\",{\"1\":{\"38\":1}}],[\"前向传播过程如下\",{\"1\":{\"27\":1}}],[\"前景点少\",{\"1\":{\"169\":1}}],[\"前景点\",{\"1\":{\"35\":1}}],[\"参数规模庞大\",{\"1\":{\"279\":1}}],[\"参数便在部分达到了\",{\"1\":{\"278\":1}}],[\"参数模型是首个公开的千亿级开源模型\",{\"1\":{\"278\":1}}],[\"参数的\",{\"1\":{\"277\":2}}],[\"参数的语言模型\",{\"1\":{\"277\":1}}],[\"参数足够多\",{\"1\":{\"198\":1}}],[\"参数量降低到\",{\"1\":{\"187\":1}}],[\"参数调整正类和负类的权重\",{\"1\":{\"172\":1}}],[\"参数选择\",{\"1\":{\"169\":1}}],[\"参数用于平衡正负类别的权重\",{\"1\":{\"169\":1}}],[\"参数用于抑制容易分类的样本\",{\"1\":{\"169\":1}}],[\"参数与训练集的归一化参数相同\",{\"1\":{\"108\":1}}],[\"参数效率高\",{\"1\":{\"85\":1}}],[\"参数共享\",{\"1\":{\"69\":1}}],[\"参数依赖\",{\"1\":{\"46\":1}}],[\"参数更新等\",{\"1\":{\"38\":1}}],[\"参数\",{\"1\":{\"35\":1,\"40\":1,\"49\":1,\"57\":2,\"58\":1,\"85\":1,\"166\":2,\"167\":2,\"168\":2,\"169\":3,\"170\":2,\"172\":3,\"226\":1,\"278\":1}}],[\"参考\",{\"0\":{\"292\":1}}],[\"参考decoderlayer\",{\"1\":{\"265\":1}}],[\"参考点解码器\",{\"0\":{\"33\":1},\"1\":{\"27\":1}}],[\"参考上文的\",{\"1\":{\"25\":1}}],[\"让我们的应用能够上线成为产品\",{\"1\":{\"291\":1}}],[\"让我们更轻松地找到所需的信息\",{\"1\":{\"282\":1}}],[\"让开发者可以专注于应用程序的开发\",{\"1\":{\"289\":1}}],[\"让开发者能够通过简单的命令来管理整个应用程序的生命周期\",{\"1\":{\"289\":1}}],[\"让大语言模型真正火爆的契机\",{\"1\":{\"282\":1}}],[\"让大模型生成的结果\",{\"1\":{\"180\":1}}],[\"让用户可根据具体需求选择最适合的模型类型\",{\"1\":{\"278\":1}}],[\"让其\",{\"1\":{\"221\":1}}],[\"让llm照着例子进行推理\",{\"1\":{\"197\":1}}],[\"让他去执行就好了\",{\"1\":{\"194\":1}}],[\"让难分类样本获得更大的\",{\"1\":{\"169\":1}}],[\"让q\",{\"1\":{\"103\":1}}],[\"让固定模型适应新任务\",{\"1\":{\"85\":1}}],[\"让视觉编码器提取的图像特征与语言模型的词嵌入空间对齐\",{\"1\":{\"79\":1}}],[\"让人类标注者对这些回答进行排序\",{\"1\":{\"78\":1}}],[\"让变换矩阵从一个恒等变换开始学习\",{\"1\":{\"64\":1}}],[\"让网络从一个小扰动开始学习\",{\"1\":{\"64\":1}}],[\"让模型可以更好地捕捉和理解语言中的复杂关系\",{\"1\":{\"277\":1}}],[\"让模型自己学会区分不同的句子\",{\"1\":{\"236\":1}}],[\"让模型得以判断上下句的起止位置\",{\"1\":{\"219\":1}}],[\"让模型理解图像和文本之间的语义关系\",{\"1\":{\"79\":1}}],[\"让模型生成多个不同的回答\",{\"1\":{\"78\":1}}],[\"让模型更容易训练和泛化\",{\"1\":{\"64\":1}}],[\"让模型既关注局部细节\",{\"1\":{\"62\":1}}],[\"让模型同时关注逐点分类精度和整体区域匹配\",{\"1\":{\"35\":1}}],[\"让每个点都能看到上下文信息\",{\"1\":{\"66\":1,\"68\":1}}],[\"让每个\",{\"1\":{\"33\":1}}],[\"让每个语言\",{\"1\":{\"29\":1}}],[\"得益于前人的一些关于内在维度\",{\"1\":{\"188\":1}}],[\"得出的用来将x序列\",{\"1\":{\"178\":1}}],[\"得出\",{\"1\":{\"72\":1}}],[\"得分也会接近它们的值\",{\"1\":{\"157\":1}}],[\"得分也会为\",{\"1\":{\"157\":1}}],[\"得分是精确率和召回率的调和平均数\",{\"1\":{\"157\":1}}],[\"得分\",{\"0\":{\"157\":1},\"1\":{\"33\":1}}],[\"得到概率\",{\"1\":{\"253\":1}}],[\"得到模型预测的这些掩码token对应的真实词\",{\"1\":{\"226\":1}}],[\"得到对应的json格式文件\",{\"1\":{\"223\":1}}],[\"得到上图中黄色的输出\",{\"1\":{\"221\":1}}],[\"得到最终适用于下游任务的模型参数\",{\"1\":{\"187\":1}}],[\"得到最终的掩码\",{\"1\":{\"33\":1}}],[\"得到一组f\",{\"1\":{\"170\":1}}],[\"得到一个融合了上下文信息的向量\",{\"1\":{\"130\":1}}],[\"得到一个logit\",{\"1\":{\"102\":1}}],[\"得到一个全局语义向量\",{\"1\":{\"16\":1}}],[\"得到归一化的注意力权重\",{\"1\":{\"126\":1}}],[\"得到归一化因子\",{\"1\":{\"57\":1}}],[\"得到结果如下图所示\",{\"1\":{\"116\":1}}],[\"得到预测结果\",{\"1\":{\"114\":1}}],[\"得到预测的\",{\"1\":{\"39\":1}}],[\"得到图像和对应的标签\",{\"1\":{\"114\":1}}],[\"得到图像的表征\",{\"1\":{\"104\":1}}],[\"得到注意力权重矩阵\",{\"1\":{\"113\":1}}],[\"得到注意力分数矩阵\",{\"1\":{\"113\":1}}],[\"得到形状为\",{\"1\":{\"109\":3}}],[\"得到相似度矩阵\",{\"1\":{\"101\":2}}],[\"得到相同维度的特征\",{\"1\":{\"90\":1}}],[\"得到的就是答案\",{\"1\":{\"255\":1}}],[\"得到的三个答案中\",{\"1\":{\"199\":1}}],[\"得到的预测概率如下所示\",{\"1\":{\"91\":1}}],[\"得到的特征可能无法覆盖整个物体\",{\"1\":{\"69\":1}}],[\"得到的质心\",{\"1\":{\"49\":1}}],[\"得到该区域的特征\",{\"1\":{\"55\":1}}],[\"得到该区域的固定长度特征表示\",{\"1\":{\"49\":1}}],[\"得到\",{\"1\":{\"50\":3,\"68\":1,\"104\":1}}],[\"得到每个注意力头的输出\",{\"1\":{\"113\":1}}],[\"得到每个预测类别的概率值\",{\"1\":{\"91\":1}}],[\"得到每个原始点的插值特征\",{\"1\":{\"57\":1}}],[\"得到每个\",{\"1\":{\"33\":1}}],[\"得到分割掩码\",{\"1\":{\"33\":1}}],[\"实时对话\",{\"1\":{\"278\":1}}],[\"实时语音和视频对话\",{\"1\":{\"278\":1}}],[\"实例化验证数据集\",{\"1\":{\"108\":1}}],[\"实例化训练数据集\",{\"1\":{\"108\":1}}],[\"实验评估\",{\"1\":{\"173\":1}}],[\"实验结果如下表所示\",{\"1\":{\"111\":1}}],[\"实验采用的是花蕊数据集\",{\"1\":{\"107\":1}}],[\"实验\",{\"0\":{\"210\":1},\"1\":{\"83\":1}}],[\"实验观察\",{\"1\":{\"69\":1}}],[\"实验验证\",{\"1\":{\"62\":1,\"69\":1}}],[\"实验证明\",{\"1\":{\"62\":1}}],[\"实际实现过程中\",{\"1\":{\"224\":1}}],[\"实际操作如下\",{\"1\":{\"218\":1}}],[\"实际负例\",{\"1\":{\"151\":1}}],[\"实际正例\",{\"1\":{\"151\":1}}],[\"实际携带信息\",{\"1\":{\"128\":1}}],[\"实际采集的点云常有遮挡\",{\"1\":{\"61\":1}}],[\"实际上\",{\"1\":{\"96\":1}}],[\"实际上是用一个固定大小的全局特征去\",{\"1\":{\"69\":1}}],[\"实际上是一个动态生成的卷积核\",{\"1\":{\"33\":1}}],[\"实际上没有局部的概念\",{\"1\":{\"43\":1}}],[\"实现从用户输入到数据库再到大模型最后输出的整体架构连接\",{\"1\":{\"291\":1}}],[\"实现从用户输入到应用输出的全流程贯通\",{\"1\":{\"291\":1}}],[\"实现更多样化的应用\",{\"1\":{\"279\":1}}],[\"实现全面的多模态交互\",{\"1\":{\"278\":1}}],[\"实现网页浏览\",{\"1\":{\"278\":1}}],[\"实现bert\",{\"1\":{\"216\":1}}],[\"实现显著的性能提升确实是可能的\",{\"1\":{\"214\":1}}],[\"实现zero\",{\"1\":{\"91\":1,\"99\":1}}],[\"实现了图像与语言之间的初步语义对齐\",{\"1\":{\"80\":1}}],[\"实现主要包括以下三个关键阶段\",{\"1\":{\"78\":1}}],[\"实现变换不变性\",{\"1\":{\"62\":1}}],[\"实现对称性\",{\"1\":{\"62\":1}}],[\"实现点集顺序不变性\",{\"1\":{\"60\":1}}],[\"实现方式\",{\"1\":{\"55\":1}}],[\"实现方法\",{\"1\":{\"48\":1}}],[\"实现跨模态信息的充分交互\",{\"1\":{\"30\":1}}],[\"实现语言引导下的跨模态融合\",{\"1\":{\"27\":1}}],[\"实现\",{\"0\":{\"216\":1},\"1\":{\"16\":1,\"35\":1,\"224\":1}}],[\"交叉熵在总损失中的占比越高\",{\"1\":{\"172\":1}}],[\"交叉熵损失\",{\"1\":{\"172\":2}}],[\"交叉熵损失为\",{\"1\":{\"169\":1}}],[\"交叉注意力运算\",{\"1\":{\"103\":1}}],[\"交叉注意力\",{\"1\":{\"103\":1}}],[\"交叉注意力则key和value都来自图像\",{\"1\":{\"103\":1}}],[\"交并比\",{\"1\":{\"168\":2}}],[\"交错堆叠\",{\"1\":{\"117\":1}}],[\"交集\",{\"1\":{\"39\":1,\"166\":1}}],[\"交互的应用程序\",{\"1\":{\"289\":1}}],[\"交互层\",{\"1\":{\"287\":1}}],[\"交互\",{\"1\":{\"33\":1}}],[\"交替作用\",{\"1\":{\"30\":1}}],[\"保存最优模型\",{\"1\":{\"227\":1}}],[\"保存字典到文件\",{\"1\":{\"224\":1}}],[\"保存注意力头的数量\",{\"1\":{\"113\":1}}],[\"保存嵌入维度\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"保存分类任务的类别数\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"保证数量一致\",{\"1\":{\"225\":1}}],[\"保证训练的开始此旁路矩阵依然是\",{\"1\":{\"189\":1}}],[\"保证训练集和验证集的数据处理方式一致\",{\"1\":{\"108\":1}}],[\"保证顺序一致\",{\"1\":{\"107\":1}}],[\"保证随机结果可复现\",{\"1\":{\"107\":1}}],[\"保证变换是刚性的\",{\"1\":{\"64\":1}}],[\"保留接口以备后续扩展\",{\"1\":{\"168\":1}}],[\"保留接口以备扩展\",{\"1\":{\"167\":1,\"169\":1}}],[\"保留和text\",{\"1\":{\"101\":2}}],[\"保留原始几何信息\",{\"1\":{\"71\":1}}],[\"保留下来\",{\"1\":{\"33\":2}}],[\"保持内容的时效性\",{\"1\":{\"283\":1}}],[\"保持原样不动的形式进行处理\",{\"1\":{\"225\":1}}],[\"保持偏置和层归一化可训练\",{\"1\":{\"192\":1}}],[\"保持预训练模型的原始参数\",{\"1\":{\"187\":1}}],[\"保持高损失权重\",{\"1\":{\"169\":1}}],[\"保持冻结\",{\"1\":{\"81\":1}}],[\"保持视觉编码器和语言模型参数冻结\",{\"1\":{\"80\":1}}],[\"保持不变\",{\"1\":{\"33\":1}}],[\"保持评估完整性\",{\"1\":{\"23\":1}}],[\"表征学习\",{\"0\":{\"100\":1},\"1\":{\"100\":1}}],[\"表达能力受限于\",{\"1\":{\"69\":1,\"85\":1}}],[\"表达式语言\",{\"1\":{\"288\":1,\"289\":1}}],[\"表达式\",{\"1\":{\"33\":1}}],[\"表现\",{\"1\":{\"204\":1}}],[\"表现类似\",{\"1\":{\"166\":1}}],[\"表现良好\",{\"1\":{\"69\":1}}],[\"表现不错\",{\"1\":{\"69\":1}}],[\"表明在迁移中\",{\"1\":{\"213\":1}}],[\"表明\",{\"1\":{\"69\":1}}],[\"表面纹理等\",{\"1\":{\"69\":1}}],[\"表示将张量中的值限制在\",{\"1\":{\"254\":1}}],[\"表示是否是连续句子\",{\"1\":{\"226\":1}}],[\"表示单词结束\",{\"1\":{\"175\":2}}],[\"表示对\",{\"1\":{\"169\":1}}],[\"表示阈值\",{\"1\":{\"162\":1}}],[\"表示正确对随机正例和负例进行排名的概率为\",{\"1\":{\"160\":1}}],[\"表示模型的规模\",{\"1\":{\"118\":1}}],[\"表示不进行归一化\",{\"1\":{\"109\":2}}],[\"表示所有图像token有效\",{\"1\":{\"104\":1}}],[\"表示输入结束\",{\"1\":{\"81\":1}}],[\"表示形式\",{\"1\":{\"71\":1}}],[\"表示只在通道维度操作\",{\"1\":{\"64\":1}}],[\"表示最原始的点云\",{\"1\":{\"58\":1}}],[\"表示每一层mlp的输出通道数\",{\"1\":{\"57\":1}}],[\"表示每个图像块的大小是\",{\"1\":{\"108\":1}}],[\"表示每个\",{\"1\":{\"33\":1}}],[\"表示每个点是否属于目标功能区域\",{\"1\":{\"169\":1}}],[\"表示每个点是否具有可操作性\",{\"1\":{\"16\":1}}],[\"表示每个点是否具有特定可操作性的概率\",{\"1\":{\"10\":1}}],[\"表示每个点属于功能区域的概率\",{\"1\":{\"169\":1}}],[\"表示每个点属于目标功能区域的概率\",{\"1\":{\"33\":1}}],[\"表示其属于目标功能区域的概率\",{\"1\":{\"33\":1}}],[\"表示该模型优于左侧曲线对应的模型\",{\"1\":{\"162\":1}}],[\"表示该层点数\",{\"1\":{\"29\":1}}],[\"表示该点是否具有可操作性\",{\"1\":{\"16\":1}}],[\"表示哪些点属于目标功能区域\",{\"1\":{\"27\":1}}],[\"表示点属于该功能区域的概率\",{\"1\":{\"21\":1}}],[\"表示概率\",{\"1\":{\"16\":1}}],[\"表示填充\",{\"1\":{\"13\":1}}],[\"表示有效\",{\"1\":{\"13\":1}}],[\"表示\",{\"1\":{\"11\":2,\"33\":1,\"49\":1,\"64\":1,\"66\":1,\"103\":1,\"160\":1,\"226\":1,\"233\":1}}],[\"去掉字符和空格\",{\"1\":{\"211\":1}}],[\"去做任务适配\",{\"1\":{\"188\":1}}],[\"去除含有低频词的句对\",{\"1\":{\"224\":1}}],[\"去除前后空格\",{\"1\":{\"223\":1}}],[\"去除频率小于\",{\"1\":{\"80\":1}}],[\"去除batch维度\",{\"1\":{\"40\":1}}],[\"去填充这些空缺\",{\"1\":{\"49\":1}}],[\"去卷积点云特征\",{\"1\":{\"33\":1}}],[\"去关注点云中最相关的区域\",{\"1\":{\"29\":1}}],[\"而更需要掌握使用大模型的实践技巧\",{\"1\":{\"290\":1}}],[\"而无需事先见过具体示例\",{\"1\":{\"280\":1}}],[\"而无需额外的训练或参数更新\",{\"1\":{\"280\":1}}],[\"而无需考虑pad词的全局上下文信息是否需要进行计算\",{\"1\":{\"230\":1}}],[\"而针对这20\",{\"1\":{\"225\":1}}],[\"而要考虑它的上下文\",{\"1\":{\"218\":1}}],[\"而且任何一个词都有可能是被替换掉的\",{\"1\":{\"218\":1}}],[\"而且能适应不同大小数据集\",{\"1\":{\"212\":1}}],[\"而设计这些模型并测试其\",{\"1\":{\"217\":1}}],[\"而特定任务学习的标注数据有非常少\",{\"1\":{\"203\":1}}],[\"而给的问题却是难度大很多的问题\",{\"1\":{\"200\":1}}],[\"而一些领域差距比较大的任务可能需要更大的\",{\"1\":{\"191\":1}}],[\"而对于较短的句子\",{\"1\":{\"227\":1}}],[\"而对于\",{\"1\":{\"189\":1}}],[\"而模型的输入输出维度不变\",{\"1\":{\"189\":1}}],[\"而用了qlora之后\",{\"1\":{\"185\":1}}],[\"而过度参数化的大模型背后\",{\"1\":{\"184\":1}}],[\"而prefix\",{\"1\":{\"183\":1}}],[\"而这些内容可以影响x生成期望中y的概率\",{\"1\":{\"182\":1}}],[\"而这里我们将会反转这个逻辑\",{\"1\":{\"94\":1}}],[\"而当\",{\"1\":{\"170\":1}}],[\"而假负例\",{\"1\":{\"162\":1}}],[\"而降低阈值则会产生相反的效果\",{\"1\":{\"155\":1}}],[\"而不会将精力聚焦在优化模型本身上\",{\"1\":{\"290\":1}}],[\"而不会考虑模型分类\",{\"1\":{\"151\":1}}],[\"而不必担心底层的基础设施和运维工作\",{\"1\":{\"289\":1}}],[\"而不需要依赖专有或不可访问的数据集\",{\"1\":{\"278\":1}}],[\"而不需要改变整个模型参数\",{\"1\":{\"85\":1}}],[\"而不能看到未来的词\",{\"1\":{\"261\":1}}],[\"而不能为\",{\"1\":{\"175\":1}}],[\"而不是重新生成一个新的答案\",{\"1\":{\"255\":1}}],[\"而不是原始\",{\"1\":{\"211\":1}}],[\"而不是逐点分类\",{\"1\":{\"166\":1}}],[\"而不是\",{\"1\":{\"166\":1}}],[\"而预训练过程中使用的输入图像尺寸通常固定为\",{\"1\":{\"108\":1}}],[\"而通过引入特定的归纳偏置\",{\"1\":{\"105\":1}}],[\"而vit将其放到前面\",{\"1\":{\"112\":1}}],[\"而vit\",{\"1\":{\"105\":1}}],[\"而vit则选择了三种不同尺寸的模型\",{\"1\":{\"90\":1}}],[\"而非生成答案\",{\"1\":{\"255\":1}}],[\"而非\",{\"1\":{\"103\":1}}],[\"而最大的vit模型vit\",{\"1\":{\"90\":1}}],[\"而图像编码器\",{\"1\":{\"90\":1,\"91\":1}}],[\"而图像编码器则用于提取图像的特征\",{\"1\":{\"90\":1}}],[\"而剩余的个文本\",{\"1\":{\"90\":1}}],[\"而点云是无序集合\",{\"1\":{\"72\":1}}],[\"而没有建模点与点之间的局部几何关系\",{\"1\":{\"69\":1}}],[\"而是将大模型作为一个调用工具\",{\"1\":{\"290\":1}}],[\"而是在网络的每一层添加了位置编码\",{\"1\":{\"278\":1}}],[\"而是对输入文本中每个\",{\"1\":{\"255\":1}}],[\"而是学习出来的\",{\"1\":{\"219\":1}}],[\"而是学会根据用户指令理解任务意图并生成合适的结果\",{\"1\":{\"78\":1}}],[\"而是利用llm结果的多样性\",{\"1\":{\"199\":1}}],[\"而是多条\",{\"1\":{\"198\":1}}],[\"而是关注预测和\",{\"1\":{\"167\":1}}],[\"而是能够捕获多个尺度上的局部特征\",{\"1\":{\"52\":1}}],[\"而是通过\",{\"1\":{\"11\":1,\"30\":1}}],[\"而\",{\"1\":{\"35\":1,\"40\":1,\"88\":1,\"169\":1,\"178\":1,\"190\":1,\"217\":1,\"221\":3,\"277\":1}}],[\"而下面这行代码实现的是一个\",{\"1\":{\"33\":1}}],[\"处理vocab\",{\"1\":{\"224\":1}}],[\"处理两个csv文件\",{\"1\":{\"223\":1}}],[\"处理每行文本\",{\"1\":{\"223\":1}}],[\"处理csv文件\",{\"1\":{\"223\":1}}],[\"处理句法歧义\",{\"1\":{\"212\":1}}],[\"处理beam\",{\"1\":{\"104\":1}}],[\"处理的是点云数据\",{\"1\":{\"72\":1}}],[\"处理\",{\"1\":{\"71\":1}}],[\"处理点云数据\",{\"1\":{\"64\":1}}],[\"处理后的\",{\"1\":{\"33\":1}}],[\"处理过程\",{\"1\":{\"29\":1,\"30\":1,\"31\":1}}],[\"扫描角度不同等\",{\"1\":{\"64\":1}}],[\"扫描\",{\"1\":{\"33\":1}}],[\"做的就是这个定位任务\",{\"1\":{\"255\":1}}],[\"做分类\",{\"1\":{\"221\":1,\"255\":1}}],[\"做为最终的答案\",{\"1\":{\"199\":1}}],[\"做简单的矩阵加法即可\",{\"1\":{\"184\":1}}],[\"做了大量的简化\",{\"1\":{\"178\":1}}],[\"做attention\",{\"1\":{\"102\":1}}],[\"做\",{\"1\":{\"101\":1}}],[\"做判断\",{\"1\":{\"82\":1}}],[\"做掩码操作\",{\"1\":{\"33\":1}}],[\"做全局池化\",{\"1\":{\"16\":1}}],[\"猜出我的特征\",{\"1\":{\"57\":1}}],[\"猜\",{\"1\":{\"33\":1}}],[\"后更名为\",{\"1\":{\"278\":1}}],[\"后训练和在线推理阶段也各自拥有了\",{\"1\":{\"277\":1}}],[\"后训练和在线推理\",{\"1\":{\"277\":1}}],[\"后的形式\",{\"1\":{\"253\":1}}],[\"后的概率值\",{\"1\":{\"166\":1}}],[\"后得到的注意力得分矩阵维度相同\",{\"1\":{\"230\":1}}],[\"后者是其110倍\",{\"1\":{\"223\":1}}],[\"后面的mlp是个单独的结构\",{\"1\":{\"112\":1}}],[\"后将投影后的\",{\"1\":{\"104\":1}}],[\"后三种模型是按照efficientnet的缩放规则对resnet分别放大4倍\",{\"1\":{\"90\":1}}],[\"后缀\",{\"1\":{\"85\":1}}],[\"后续需要利用该相关度完成当前词的全局上下文信息融合\",{\"1\":{\"230\":1}}],[\"后续再进行裁剪操作\",{\"1\":{\"108\":1}}],[\"后续的\",{\"1\":{\"69\":1}}],[\"后续的特征提取更稳定\",{\"1\":{\"64\":1}}],[\"后续发展\",{\"1\":{\"69\":1}}],[\"后续改进方向\",{\"1\":{\"69\":1}}],[\"后续改进\",{\"1\":{\"69\":1}}],[\"后续训练完50个epoch后\",{\"1\":{\"40\":1}}],[\"后台运行\",{\"1\":{\"40\":1}}],[\"后\",{\"1\":{\"32\":1,\"64\":1}}],[\"↓\",{\"1\":{\"32\":4,\"55\":5,\"56\":8,\"137\":9}}],[\"──┐\",{\"1\":{\"32\":1}}],[\"两种规模的\",{\"1\":{\"278\":1}}],[\"两种模式\",{\"1\":{\"22\":1}}],[\"两条互补产品线\",{\"1\":{\"278\":1}}],[\"两大技术分支\",{\"1\":{\"278\":1}}],[\"两两配对遍历\",{\"1\":{\"224\":1}}],[\"两点注意\",{\"1\":{\"107\":1}}],[\"两点的距离反映了这两点的实际相似度或关联度\",{\"1\":{\"47\":1}}],[\"两者协同提升模型性能\",{\"1\":{\"169\":1}}],[\"两者解决的是不同维度的问题\",{\"1\":{\"169\":1}}],[\"两者解耦可以让模型更灵活地分配资源\",{\"1\":{\"135\":1}}],[\"两者互补\",{\"1\":{\"167\":1}}],[\"两者的训练效率相差3倍\",{\"1\":{\"96\":1}}],[\"两者结合可以\",{\"1\":{\"35\":1}}],[\"两个月后月活用户破亿\",{\"1\":{\"278\":1}}],[\"两个句子是否为上下句关系\",{\"1\":{\"250\":1}}],[\"两个任务\",{\"1\":{\"226\":1}}],[\"两个开源数据集\",{\"1\":{\"223\":1}}],[\"两个不同分类任务的评估结果\",{\"1\":{\"212\":1}}],[\"两个比较的句子没有内在顺序\",{\"1\":{\"209\":1}}],[\"两个假设模型的\",{\"1\":{\"162\":1}}],[\"两个点之间的直线距离被认为是相似度或连接强度的直观表示\",{\"1\":{\"47\":1}}],[\"两个问题是相关联的\",{\"1\":{\"43\":1}}],[\"两个\",{\"1\":{\"30\":1}}],[\"内取平均\",{\"1\":{\"169\":1}}],[\"内样本取平均\",{\"1\":{\"167\":1,\"168\":1,\"169\":1}}],[\"内\",{\"1\":{\"166\":1,\"172\":1}}],[\"内部又是\",{\"1\":{\"221\":1}}],[\"内部是\",{\"1\":{\"221\":1}}],[\"内部标注人员手动构造的示例\",{\"1\":{\"78\":1}}],[\"内部\",{\"1\":{\"30\":2}}],[\"内容不可追溯\",{\"1\":{\"283\":1}}],[\"内容\",{\"1\":{\"24\":1}}],[\"卷积后剩余的长和宽相乘作为时间维度\",{\"1\":{\"109\":1}}],[\"卷积核个数为768的卷积层来进行实现\",{\"1\":{\"109\":1}}],[\"卷积\",{\"1\":{\"30\":1,\"64\":1}}],[\"用户愈发期待像钢铁侠中\",{\"1\":{\"282\":1}}],[\"用户可与\",{\"1\":{\"278\":1}}],[\"用户本地运行时\",{\"1\":{\"259\":1}}],[\"用户提问或指令\",{\"1\":{\"81\":1}}],[\"用户提交给\",{\"1\":{\"78\":1}}],[\"用随机词替换\",{\"1\":{\"225\":1}}],[\"用随机高斯分布初始化\",{\"1\":{\"189\":1,\"190\":1}}],[\"用mask掩码替换\",{\"1\":{\"225\":1}}],[\"用任务未知的生成式预训练模型和判别式微调在自然语言理解上取得了很强的效果\",{\"1\":{\"214\":1}}],[\"用两个可能的替换说法来代替定义的代词\",{\"1\":{\"213\":1}}],[\"用以区分不同的句子\",{\"1\":{\"233\":1}}],[\"用以训练步数的\",{\"1\":{\"211\":1}}],[\"用以完成特定任务的指令\",{\"1\":{\"194\":1}}],[\"用分隔符分隔\",{\"1\":{\"209\":1}}],[\"用标记的数据对特定任务微调模型\",{\"1\":{\"206\":1}}],[\"用特定的训练数据\",{\"1\":{\"184\":1}}],[\"用特定训练数据去微调可能会把这个领域的表现变好\",{\"1\":{\"180\":1}}],[\"用传统机器学习中监督学习的方法\",{\"1\":{\"180\":1}}],[\"用人工标注的数据\",{\"1\":{\"180\":1}}],[\"用正则匹配并替换匹配上的\",{\"1\":{\"175\":1}}],[\"用一个通用大模型\",{\"1\":{\"290\":1}}],[\"用一个简化版的例子说明上述过程\",{\"1\":{\"109\":1}}],[\"用一组由语言引导的动态卷积核\",{\"1\":{\"33\":1}}],[\"用文本描述去匹配最合适的图片内容\",{\"1\":{\"94\":1}}],[\"用当前图片外层目录的名字作为其分类名词\",{\"1\":{\"93\":1}}],[\"用全连接层逐步压缩到\",{\"1\":{\"64\":1}}],[\"用训练好的模型权重\",{\"1\":{\"40\":1}}],[\"用不同\",{\"1\":{\"39\":1}}],[\"用增强后的语言查询去\",{\"1\":{\"33\":1}}],[\"用语言引导点特征分组\",{\"1\":{\"32\":1}}],[\"用\",{\"1\":{\"30\":1,\"166\":1,\"189\":2,\"190\":1,\"209\":1,\"233\":1,\"290\":1}}],[\"用于构建\",{\"1\":{\"289\":1}}],[\"用于复杂的应用的调用序列\",{\"1\":{\"287\":2}}],[\"用于链的多次运行之间持久化应用程序状态\",{\"1\":{\"287\":1}}],[\"用于与人类对话式应用的大胆尝试\",{\"1\":{\"277\":1}}],[\"用于指定在计算损失时忽略的标签索引\",{\"1\":{\"254\":1}}],[\"用于判断给出的两个句子是否连续\",{\"1\":{\"233\":1}}],[\"用于提取\",{\"1\":{\"226\":1}}],[\"用于我们的预训练任务\",{\"1\":{\"225\":1}}],[\"用于交叉熵部分\",{\"1\":{\"172\":1}}],[\"用于处理类别不平衡\",{\"1\":{\"170\":1,\"172\":1}}],[\"用于抑制易分类样本\",{\"1\":{\"169\":1}}],[\"用于平衡正负样本数量差异\",{\"1\":{\"169\":1}}],[\"用于度量两个集合之间的重叠程度\",{\"1\":{\"168\":1}}],[\"用于语义分割任务中评估模型的分割结果与真实分割标签之间的相似性\",{\"1\":{\"168\":1}}],[\"用于语言引导下的功能区域分割\",{\"1\":{\"35\":1}}],[\"用于类别加权\",{\"1\":{\"167\":1,\"168\":1,\"169\":1}}],[\"用于选择模型和阈值的\",{\"0\":{\"162\":1}}],[\"用于匹配查询\",{\"1\":{\"128\":1}}],[\"用于同时生成查询\",{\"1\":{\"113\":1}}],[\"用于调整注意力分数\",{\"1\":{\"113\":1}}],[\"用于防止过拟合\",{\"1\":{\"112\":1}}],[\"用于随机深度\",{\"1\":{\"112\":1}}],[\"用于位置嵌入后的随机丢弃\",{\"1\":{\"111\":1,\"114\":1}}],[\"用于分类任务\",{\"1\":{\"110\":1}}],[\"用于预测图像的类别\",{\"1\":{\"110\":1}}],[\"用于将\",{\"1\":{\"289\":1}}],[\"用于将张量中的值限制在指定的范围内\",{\"1\":{\"254\":1}}],[\"用于将张量\",{\"1\":{\"172\":1}}],[\"用于将输入图像分割成多个图像块并进行嵌入\",{\"1\":{\"109\":1}}],[\"用于将一个方法定义为静态方法\",{\"1\":{\"107\":1}}],[\"用于将一个批次的数据组合成一个张量\",{\"1\":{\"107\":1}}],[\"用于存储训练集和验证集的图像预处理转换操作\",{\"1\":{\"108\":1}}],[\"用于计算交叉熵损失\",{\"1\":{\"257\":1}}],[\"用于计算\",{\"1\":{\"103\":1}}],[\"用于训练或评估模型\",{\"1\":{\"78\":1}}],[\"用于其他任务\",{\"1\":{\"58\":1}}],[\"用于保存卷积层和批归一化层\",{\"1\":{\"57\":1}}],[\"用于后续比较\",{\"1\":{\"65\":1}}],[\"用于后续计算其他点到该点的距离\",{\"1\":{\"49\":1}}],[\"用于后续分割掩码预测\",{\"1\":{\"31\":1}}],[\"用于快速访问每个\",{\"1\":{\"49\":1}}],[\"用于衡量被错误分类为垃圾邮件的合法电子邮件的比例\",{\"1\":{\"154\":1}}],[\"用于衡量两个概率分布之间的匹配程度\",{\"1\":{\"39\":1}}],[\"用于衡量模型输出的\",{\"1\":{\"39\":1}}],[\"用于衡量预测掩码与真实标签之间的空间重合度\",{\"1\":{\"35\":1}}],[\"用于缓解类别不平衡问题\",{\"1\":{\"35\":1}}],[\"用于解决一个新颖的任务\",{\"1\":{\"27\":1}}],[\"用于稳定训练过程\",{\"1\":{\"15\":1}}],[\"用于\",{\"1\":{\"15\":1,\"49\":1,\"226\":1}}],[\"用于降低计算复杂度\",{\"1\":{\"15\":1}}],[\"用于融合来自语言模型的不同语义信息\",{\"1\":{\"15\":1}}],[\"操作\",{\"1\":{\"30\":1,\"33\":2,\"55\":1}}],[\"基本流程\",{\"0\":{\"291\":1}}],[\"基本语法\",{\"1\":{\"140\":1}}],[\"基础模型\",{\"1\":{\"278\":1}}],[\"基础点云可视化\",{\"1\":{\"40\":1}}],[\"基础数据来源\",{\"0\":{\"19\":1}}],[\"基线\",{\"1\":{\"32\":1}}],[\"基于小型验证集设计满足基本要求\",{\"1\":{\"291\":1}}],[\"基于静态的数据集训练\",{\"1\":{\"283\":1}}],[\"基于上下文编码\",{\"1\":{\"255\":1}}],[\"基于上述背景\",{\"1\":{\"188\":1}}],[\"基于大模型的内在低秩特性\",{\"1\":{\"192\":1}}],[\"基于prompt\",{\"1\":{\"183\":1}}],[\"基于ai反馈的强化学习微调rlaif\",{\"1\":{\"180\":1}}],[\"基于标准交叉熵损失\",{\"1\":{\"169\":1}}],[\"基于交叉熵损失进行扩展\",{\"1\":{\"169\":1}}],[\"基于自回归或语言掩码的预训练方法已经相对成熟\",{\"1\":{\"96\":1}}],[\"基于人类反馈的强化学习微调rlhf\",{\"1\":{\"180\":1}}],[\"基于人类反馈的强化学习\",{\"1\":{\"78\":1}}],[\"基于图卷积或注意力机制的模型更能捕捉这种非刚性变化\",{\"1\":{\"69\":1}}],[\"基于以下前提\",{\"1\":{\"47\":1}}],[\"基于\",{\"1\":{\"30\":1,\"80\":1,\"81\":1,\"98\":1}}],[\"因此出现了extended\",{\"1\":{\"260\":1}}],[\"因此需要大家自行完成运行时缺失依赖包的安装\",{\"1\":{\"232\":1}}],[\"因此需要使用像素级别的损失函数\",{\"1\":{\"164\":1}}],[\"因此不会影响最终的损失值计算\",{\"1\":{\"227\":1}}],[\"因此为了确保masked\",{\"1\":{\"227\":1}}],[\"因此模型返回的logits\",{\"1\":{\"227\":1}}],[\"因此我们只需要根据is\",{\"1\":{\"227\":1}}],[\"因此此时的预测搞不好是对的\",{\"1\":{\"221\":1}}],[\"因此sst属于单个句子的文本分类任务\",{\"1\":{\"212\":1}}],[\"因此cola属于单个句子的文本二分类任务\",{\"1\":{\"212\":1}}],[\"因此后面\",{\"1\":{\"190\":1}}],[\"因此语义分割可以提供更详细和准确的图像分析结果\",{\"1\":{\"164\":1}}],[\"因此精确率为\",{\"1\":{\"155\":1}}],[\"因此其假正例率为\",{\"1\":{\"154\":1}}],[\"因此其召回率\",{\"1\":{\"153\":1}}],[\"因此会出现在分母中\",{\"1\":{\"153\":1,\"154\":1}}],[\"因此准确率为\",{\"1\":{\"152\":1}}],[\"因此是自注意力\",{\"1\":{\"113\":1}}],[\"因此可以充分利用计算资源\",{\"1\":{\"260\":1}}],[\"因此可以堆叠多个block\",{\"1\":{\"112\":1}}],[\"因此可能比第二个向量更不可靠\",{\"1\":{\"54\":1}}],[\"因此它有效地充当信息瓶颈\",{\"1\":{\"104\":1}}],[\"因此在类别数量相近且平衡的数据集的情况下\",{\"1\":{\"152\":1}}],[\"因此在新的数据集上需要定义新的分类器来重新训练\",{\"1\":{\"96\":1}}],[\"因此在效果上可能不如使用\",{\"1\":{\"92\":1}}],[\"因此成本较高\",{\"1\":{\"96\":1}}],[\"因此这里就不再给出数据集下载链接了\",{\"1\":{\"93\":1}}],[\"因此这是一个非常庞大的数据集\",{\"1\":{\"90\":1}}],[\"因此每个质心将根据这些不同的半径值与其周围点形成多个点集群\",{\"1\":{\"52\":1}}],[\"因此\",{\"1\":{\"51\":1,\"96\":2,\"108\":1,\"155\":1,\"190\":1,\"198\":1,\"200\":1,\"221\":1,\"277\":1,\"279\":1,\"281\":1,\"290\":2}}],[\"因此加权融合的时候\",{\"1\":{\"29\":1}}],[\"因为cuda不支持macos\",{\"1\":{\"259\":1}}],[\"因为我只是为了了解内部代码情况\",{\"1\":{\"232\":1}}],[\"因为我们要预测下一个\",{\"1\":{\"103\":1}}],[\"因为低频词出现次数极少\",{\"1\":{\"224\":1}}],[\"因为存在各种变化现象\",{\"1\":{\"212\":1}}],[\"因为作者的预训练模型是用连续的文本序列训练的\",{\"1\":{\"209\":1}}],[\"因为llm的prompt长度通常都是有长度限制的\",{\"1\":{\"198\":1}}],[\"因为收集人类反馈\",{\"1\":{\"180\":1}}],[\"因为微调的参数量跟预训练的是一样的多的\",{\"1\":{\"180\":1}}],[\"因为推理成本是跟prompt长度的平方正向相关的\",{\"1\":{\"179\":1}}],[\"因为通常大模型的实现原理\",{\"1\":{\"179\":1}}],[\"因为大模型的参数量非常大\",{\"1\":{\"179\":1}}],[\"因为要最小化损失\",{\"1\":{\"172\":1}}],[\"因为在代码中有个冻结权重的操作\",{\"1\":{\"118\":1}}],[\"因为transformer和cnn相比缺少归纳偏置\",{\"1\":{\"105\":1}}],[\"因为都是有效\",{\"1\":{\"103\":1}}],[\"因为训练数据集中的文本\",{\"1\":{\"96\":1}}],[\"因为它们对模型的适应性至关重要\",{\"1\":{\"192\":1}}],[\"因为它们的作用不同\",{\"1\":{\"135\":1}}],[\"因为它衡量的是模型正确识别所有正例实例的能力\",{\"1\":{\"153\":1}}],[\"因为它能够在更低层次上递归地检视更高分辨率\",{\"1\":{\"54\":1}}],[\"因为它提供了高质量的点云和功能标注\",{\"1\":{\"19\":1}}],[\"因为前面有\",{\"1\":{\"49\":1}}],[\"因为学习到的特征和权重可以在多个局部区域中复用\",{\"1\":{\"43\":1}}],[\"因为\",{\"1\":{\"43\":1,\"169\":1,\"255\":1}}],[\"知识更新\",{\"1\":{\"285\":1}}],[\"知识更新滞后性\",{\"1\":{\"283\":1}}],[\"知识截止日期\",{\"1\":{\"278\":2}}],[\"知识型模型\",{\"1\":{\"278\":1}}],[\"知识型与推理型双模式\",{\"1\":{\"278\":1}}],[\"知识型\",{\"1\":{\"278\":3}}],[\"知识扫盲\",{\"0\":{\"177\":1,\"193\":1},\"1\":{\"177\":1,\"193\":1}}],[\"知识星球\",{\"1\":{\"0\":1}}],[\"知道了哪些点跟自身的相关度更大\",{\"1\":{\"29\":1}}],[\"某一层次𝐿𝑖的区域特征是通过将来自下一级𝐿𝑖−1的子区域特征总结后的向量与直接处理该局部区域所有原始点的单个pointnet得到的特征向量进行concat得到的\",{\"1\":{\"54\":1}}],[\"某一层解码器输出的点特征\",{\"1\":{\"29\":1}}],[\"某些功能类型在特定物体类别下会从训练集中省略\",{\"1\":{\"22\":1}}],[\"由何恺明团队在2017年论文\",{\"1\":{\"169\":1}}],[\"由此提出低秩自适应\",{\"1\":{\"188\":1}}],[\"由此可见vit工作的局限性\",{\"1\":{\"115\":1}}],[\"由此把图片转换为序列的embedding形式\",{\"1\":{\"109\":1}}],[\"由两个transformer模块组成\",{\"1\":{\"100\":1}}],[\"由三角形面片组成的\",{\"1\":{\"71\":1}}],[\"由\",{\"1\":{\"29\":1,\"30\":1,\"45\":1,\"223\":1}}],[\"由于大模型应用需要进行向量语义检索\",{\"1\":{\"291\":1}}],[\"由于与人类交流的出色能力\",{\"1\":{\"278\":1}}],[\"由于模型输出的logits\",{\"1\":{\"227\":1}}],[\"由于矩阵\",{\"1\":{\"190\":1}}],[\"由于需要对每个像素进行分类\",{\"1\":{\"164\":1}}],[\"由于准确率包含混淆矩阵中的所有四种结果\",{\"1\":{\"152\":1}}],[\"由于作者是首次将transformer应用到图像领域\",{\"1\":{\"116\":1}}],[\"由于\",{\"1\":{\"104\":1,\"261\":1}}],[\"由于训练数据量和模型计算量较大\",{\"1\":{\"96\":1}}],[\"由于它们在预训练数据集上采用固定类别数的分类器\",{\"1\":{\"96\":1}}],[\"由于这些文本往往只是一个单词\",{\"1\":{\"92\":1}}],[\"由于数据量巨大\",{\"1\":{\"90\":1}}],[\"由于表情不同\",{\"1\":{\"69\":1}}],[\"由于子区域在计算第一个向量时包含的点更稀疏\",{\"1\":{\"54\":1}}],[\"由于点集在不同区域可能会有不同的采样密度\",{\"1\":{\"51\":1}}],[\"由于目标功能区域的尺度\",{\"1\":{\"28\":1}}],[\"由于论文数据集还未开源\",{\"1\":{\"7\":1}}],[\"包\",{\"1\":{\"147\":1}}],[\"包类型\",{\"1\":{\"147\":1}}],[\"包可能会安装到基础环境或系统\",{\"1\":{\"147\":1}}],[\"包会安装到该环境的\",{\"1\":{\"147\":1}}],[\"包名\",{\"1\":{\"140\":1}}],[\"包含大约\",{\"1\":{\"118\":1}}],[\"包含一个线性层和一个\",{\"1\":{\"114\":1}}],[\"包含分类token\",{\"1\":{\"113\":1}}],[\"包含所有图像对应类别的列表\",{\"1\":{\"107\":1}}],[\"包含所有图像文件路径的列表\",{\"1\":{\"107\":1}}],[\"包含图像和对应的标签\",{\"1\":{\"107\":1}}],[\"包含图像和可选文本\",{\"1\":{\"104\":1}}],[\"包含图像信息\",{\"1\":{\"103\":1}}],[\"包含\",{\"1\":{\"103\":1}}],[\"包含约\",{\"1\":{\"80\":1,\"81\":1}}],[\"包含表面细节\",{\"1\":{\"71\":1}}],[\"包含两种适应性特征学习层\",{\"1\":{\"51\":1}}],[\"包含三个关键步骤\",{\"1\":{\"28\":1}}],[\"包括让\",{\"1\":{\"288\":1}}],[\"包括中间步骤的数据流传输\",{\"1\":{\"288\":1}}],[\"包括批量处理\",{\"1\":{\"288\":1}}],[\"包括写文章\",{\"1\":{\"282\":1}}],[\"包括生成有害内容\",{\"1\":{\"279\":1}}],[\"包括文本分割\",{\"1\":{\"288\":1}}],[\"包括文本\",{\"1\":{\"279\":1}}],[\"包括了\",{\"1\":{\"278\":1}}],[\"包括开源和闭源\",{\"1\":{\"278\":1}}],[\"包括问题和上下文\",{\"1\":{\"253\":1}}],[\"包括wikitext\",{\"1\":{\"223\":1}}],[\"包括以下几步\",{\"1\":{\"184\":1}}],[\"包括以下核心模块\",{\"1\":{\"27\":1}}],[\"包括\",{\"1\":{\"110\":1,\"278\":3,\"286\":1}}],[\"包括基于对比学习的方法\",{\"1\":{\"96\":1}}],[\"包括前向传播\",{\"1\":{\"38\":1}}],[\"瓶颈式架构\",{\"1\":{\"28\":1}}],[\"遵循一个\",{\"1\":{\"28\":1}}],[\"以调用\",{\"1\":{\"290\":1}}],[\"以防泄露\",{\"1\":{\"285\":1}}],[\"以改善多媒体交互\",{\"1\":{\"282\":1}}],[\"以上模型的上下文长度为\",{\"1\":{\"278\":1}}],[\"以上代码注释中统一用b代替image\",{\"1\":{\"100\":1}}],[\"以加速模型的训练和扩展其中\",{\"1\":{\"278\":1}}],[\"以加速训练并提高模型的稳定性\",{\"1\":{\"261\":1}}],[\"以提高数据质量和多样性\",{\"1\":{\"278\":1}}],[\"以往为了解决不同的\",{\"1\":{\"217\":1}}],[\"以无监督的方式利用大量无标注文本\",{\"1\":{\"217\":1}}],[\"以节省\",{\"1\":{\"192\":1}}],[\"以此来模拟所谓的内在秩\",{\"1\":{\"189\":1}}],[\"以增大生成期望序列的概率\",{\"1\":{\"182\":1}}],[\"以增强性能\",{\"1\":{\"90\":1}}],[\"以增强\",{\"1\":{\"28\":1}}],[\"以避免安装到错误的位置\",{\"1\":{\"147\":1}}],[\"以避免模型将匹配图文对挑选为负样本\",{\"1\":{\"102\":1}}],[\"以使得模型能够学习到最适合当前任务的位置表示\",{\"1\":{\"111\":1}}],[\"以最大程度地惩罚降低iou得分的预测结果\",{\"1\":{\"171\":1}}],[\"以最大化互信息\",{\"1\":{\"101\":1}}],[\"以最小化损失\",{\"1\":{\"110\":1}}],[\"以保持与其他模型的一致性\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"以保持计算的可管理性\",{\"1\":{\"47\":1}}],[\"以保留图像的空间信息\",{\"1\":{\"110\":1}}],[\"以vit\",{\"1\":{\"109\":1}}],[\"以\",{\"0\":{\"133\":1},\"1\":{\"108\":1,\"211\":1,\"277\":1}}],[\"以bos\",{\"1\":{\"104\":1}}],[\"以利用\",{\"1\":{\"104\":1}}],[\"以细粒度对齐\",{\"1\":{\"102\":1}}],[\"以及每一个功能的大体实现逻辑\",{\"1\":{\"291\":1}}],[\"以及如何高效地进行多次调用和推理\",{\"1\":{\"288\":1}}],[\"以及如何通过局部特征学习器\",{\"1\":{\"43\":1}}],[\"以及\",{\"1\":{\"278\":1}}],[\"以及即将推出的\",{\"1\":{\"278\":1}}],[\"以及对错误数据\",{\"1\":{\"291\":1}}],[\"以及对\",{\"1\":{\"278\":1}}],[\"以及分割字符嵌入矩阵\",{\"1\":{\"208\":1}}],[\"以及语言模型来提升标记的语义角色\",{\"1\":{\"205\":1}}],[\"以及prompt的长度\",{\"1\":{\"198\":1}}],[\"以及训练的方法的角度\",{\"1\":{\"180\":1}}],[\"以及seq\",{\"1\":{\"100\":1}}],[\"以弥合模态差距\",{\"1\":{\"99\":1}}],[\"以每个文本描述为一行\",{\"1\":{\"94\":1}}],[\"以获取图像特征\",{\"1\":{\"91\":1}}],[\"以获得更强的局部几何感知能力\",{\"1\":{\"53\":1}}],[\"以生成相应的文本特征\",{\"1\":{\"91\":1}}],[\"以下模型的上下文长度为\",{\"1\":{\"278\":1}}],[\"以下引用clip论文图做说明\",{\"1\":{\"101\":1}}],[\"以下是大语言模型的一些主要特点\",{\"1\":{\"279\":1}}],[\"以下是一个官方给出的clip模型的示例\",{\"1\":{\"91\":1}}],[\"以下是两者的主要区别\",{\"1\":{\"85\":1}}],[\"以下代码是我自己写的一个测试代码\",{\"1\":{\"40\":1}}],[\"以分析不同训练策略的影响\",{\"1\":{\"83\":1}}],[\"以奖励模型为\",{\"1\":{\"78\":1}}],[\"以适配后面的卷积操作\",{\"1\":{\"57\":1}}],[\"以便开发各种下游应用\",{\"1\":{\"286\":1}}],[\"以便生成模型可以更好地理解和使用\",{\"1\":{\"284\":1}}],[\"以便一起处理\",{\"1\":{\"257\":1}}],[\"以便\",{\"1\":{\"100\":1}}],[\"以便和特征相乘\",{\"1\":{\"57\":1}}],[\"以便可以在这些分区上独立地学习特征\",{\"1\":{\"43\":1}}],[\"以便可以在这些区域上应用局部操作\",{\"1\":{\"43\":1}}],[\"以实现最佳的性能\",{\"1\":{\"53\":1}}],[\"以结合来自不同尺度的特征\",{\"1\":{\"52\":1}}],[\"以反映其相对位置\",{\"1\":{\"48\":1}}],[\"以限制每个局部区域中考虑的点的数量\",{\"1\":{\"47\":1}}],[\"以产生一个更少元素的新集合\",{\"1\":{\"45\":1}}],[\"以二维欧几里得空间为例\",{\"1\":{\"44\":1}}],[\"以确保问题多样性和语义丰富性\",{\"1\":{\"20\":1}}],[\"作用\",{\"1\":{\"169\":1}}],[\"作用于点云特征图\",{\"1\":{\"33\":1}}],[\"作者的工作表明\",{\"1\":{\"214\":1}}],[\"作者模型获得了重要的世界知识和处理长距离依赖的能力\",{\"1\":{\"214\":1}}],[\"作者介绍了一种框架\",{\"1\":{\"214\":1}}],[\"作者观察标准结果\",{\"1\":{\"213\":1}}],[\"作者修改输入序列来包含\",{\"1\":{\"209\":1}}],[\"作者额外要微调的参数只有\",{\"1\":{\"208\":1}}],[\"作者用以下优化\",{\"1\":{\"208\":1}}],[\"作者用无监督的预训练和监督的微调组合来探索关于语言理解任务半监督方法\",{\"1\":{\"204\":1}}],[\"作者还发现加入语言模型作为辅助目标来微调有助于学习\",{\"1\":{\"208\":1}}],[\"作者还提出了一种创新方法\",{\"1\":{\"82\":1}}],[\"作者训练流程有两个阶段\",{\"1\":{\"206\":1}}],[\"作者通用的任务未知task\",{\"1\":{\"204\":1}}],[\"作者通过fps来抽样点集中较为重要的点\",{\"1\":{\"46\":1}}],[\"作者利用源于遍历式\",{\"1\":{\"204\":1}}],[\"作者假设采用一个大型无标记文本语料库和几个人工标记训练样本的数据集\",{\"1\":{\"204\":1}}],[\"作者在微调时用辅助的lm目标来检查作者模型的性能\",{\"1\":{\"213\":1}}],[\"作者在微调阶段使用任务感知的输入转换来实现有效的迁移\",{\"1\":{\"203\":1}}],[\"作者在下面部分和可视化插图\",{\"1\":{\"209\":1}}],[\"作者在监督学习目标任务上调整参数\",{\"1\":{\"208\":1}}],[\"作者在四种类型的语言理解任务评估作者的方法\",{\"1\":{\"204\":1}}],[\"作者在常识推理\",{\"1\":{\"203\":1}}],[\"作者证明通过在丰富的无标签文本语料库生成预训练generative\",{\"1\":{\"203\":1}}],[\"作者又探索了一种混合模型\",{\"1\":{\"117\":1}}],[\"作者将vit和之前图像分类领域比较强的resnet模型进行了对比测试\",{\"1\":{\"115\":1}}],[\"作者将模型训练分为两个阶段\",{\"1\":{\"79\":1}}],[\"作者先是在imagenet21k上进行预训练\",{\"1\":{\"114\":1}}],[\"作者随后也对一维位置编码的结果进行了可视化\",{\"1\":{\"111\":1}}],[\"作者最终选择了对比学习方法来进行训练\",{\"1\":{\"96\":1}}],[\"作者使用遍历式方法\",{\"1\":{\"209\":1}}],[\"作者使用了谷歌制作的jft\",{\"1\":{\"115\":1}}],[\"作者使用了四个核心评估指标来衡量模型对语言引导下功能区域的识别能力\",{\"1\":{\"39\":1}}],[\"作者使用的是大规模图文对数据集\",{\"1\":{\"80\":1}}],[\"作者设计了\",{\"1\":{\"28\":1}}],[\"作为大模型开发的初学者\",{\"1\":{\"290\":1}}],[\"作为一个不断进化的创新平台\",{\"1\":{\"288\":1}}],[\"作为一个大语言模型开发框架\",{\"1\":{\"287\":1}}],[\"作为一个指标就没有那么有意义和实用\",{\"1\":{\"154\":1}}],[\"作为答案开始的可能性\",{\"1\":{\"253\":1}}],[\"作为整个输入序列的全局信息聚合表示\",{\"1\":{\"226\":1}}],[\"作为整个点云的\",{\"1\":{\"64\":1,\"66\":1}}],[\"作为最终的\",{\"1\":{\"221\":1}}],[\"作为激活函数\",{\"1\":{\"211\":1}}],[\"作为分隔符\",{\"1\":{\"196\":1}}],[\"作为平衡数据集的模型训练进度\",{\"1\":{\"156\":1}}],[\"作为编码器\",{\"1\":{\"117\":1}}],[\"作为特征提取器\",{\"1\":{\"117\":1}}],[\"作为文本解码器的初始状态\",{\"1\":{\"103\":1}}],[\"作为图像的分类预测结果\",{\"1\":{\"91\":1}}],[\"作为对称函数\",{\"1\":{\"62\":1}}],[\"作为初始输入\",{\"1\":{\"33\":1}}],[\"作为第二个\",{\"1\":{\"15\":1}}],[\"作为第一个\",{\"1\":{\"15\":1}}],[\"作为\",{\"1\":{\"15\":1,\"100\":1}}],[\"传统\",{\"1\":{\"290\":1}}],[\"传统的\",{\"1\":{\"290\":1}}],[\"传统方法的缺陷\",{\"1\":{\"60\":1}}],[\"传统方法难以适应不同情况\",{\"1\":{\"28\":1}}],[\"传统卷积神经网络难以直接处理\",{\"1\":{\"60\":1}}],[\"传入图像\",{\"1\":{\"103\":1}}],[\"传入的点集特征集合经过转置处理后的维度为\",{\"1\":{\"27\":1}}],[\"传入question文本\",{\"1\":{\"27\":1}}],[\"逐渐丰富了其功能\",{\"1\":{\"278\":1}}],[\"逐渐恢复点数\",{\"1\":{\"55\":1}}],[\"逐位置前馈神经网络\",{\"1\":{\"211\":1}}],[\"逐元素相加然后送入线性输出层\",{\"1\":{\"209\":1}}],[\"逐点\",{\"1\":{\"64\":1,\"167\":1}}],[\"逐点误差\",{\"1\":{\"39\":1}}],[\"逐层融合上下文信息\",{\"1\":{\"58\":1}}],[\"逐层恢复到原始点数\",{\"1\":{\"56\":1}}],[\"逐层将特征插值回原始点数量\",{\"1\":{\"55\":1}}],[\"逐层抽象后融合成全局特征\",{\"1\":{\"53\":1}}],[\"逐步进行完善和优化\",{\"1\":{\"291\":1}}],[\"逐步推理\",{\"1\":{\"280\":1}}],[\"逐步合并高频的字符对\",{\"1\":{\"174\":1}}],[\"逐步聚合全局信息\",{\"1\":{\"110\":1}}],[\"逐步向正交矩阵靠拢\",{\"1\":{\"64\":1}}],[\"逐步减少点的数量\",{\"1\":{\"55\":1}}],[\"逐步以自上而下的方式细化点特征图\",{\"1\":{\"28\":1}}],[\"逐级做点集抽象得到的每层的点集坐标和点集特征集合\",{\"1\":{\"27\":1}}],[\"逐个拼接\",{\"1\":{\"13\":1}}],[\"利用包含中间推理步骤的提示机制来解决这些任务\",{\"1\":{\"280\":1}}],[\"利用pytorch从\",{\"1\":{\"216\":1}}],[\"利用cot\",{\"1\":{\"199\":1}}],[\"利用nltk库提供的wordpunct\",{\"1\":{\"175\":1}}],[\"利用nltk库提供的sent\",{\"1\":{\"175\":1}}],[\"利用已有的vit\",{\"1\":{\"98\":1}}],[\"利用已有知识进行推理\",{\"1\":{\"85\":1}}],[\"利用对称函数\",{\"1\":{\"60\":1}}],[\"利用相对坐标与点特征相结合的方式可以捕获局部区域中点与点之间的关系\",{\"1\":{\"48\":1}}],[\"利用上一步得到的中心点将点集划分成若干个区域\",{\"1\":{\"44\":1}}],[\"利用一组问题条件化的\",{\"1\":{\"33\":2}}],[\"利用\",{\"1\":{\"27\":1,\"100\":1,\"117\":1,\"286\":1}}],[\"引言\",{\"0\":{\"88\":1}}],[\"引导模型生成特定任务的结果\",{\"1\":{\"85\":1}}],[\"引导语言模型更好地遵循用户指令\",{\"1\":{\"78\":1}}],[\"引导下提取的点云信息增强后的图像特征\",{\"1\":{\"11\":1}}],[\"引入了先进的检索技术\",{\"1\":{\"288\":1}}],[\"引入了多模态能力\",{\"1\":{\"278\":1}}],[\"引入了额外的推理延迟\",{\"1\":{\"188\":1}}],[\"引入了两个变换网络\",{\"1\":{\"65\":1}}],[\"引入到对大模型的微调中去\",{\"1\":{\"180\":1}}],[\"引入多尺度采样等\",{\"1\":{\"69\":1}}],[\"引入两个空间变换网络\",{\"1\":{\"62\":1}}],[\"引入\",{\"1\":{\"60\":1,\"62\":1,\"69\":1}}],[\"引入一组可学习的\",{\"1\":{\"27\":1}}],[\"自纠错机制\",{\"1\":{\"278\":1}}],[\"自发布以来就引发了人工智能社区的兴奋\",{\"1\":{\"278\":1}}],[\"自己写答案\",{\"1\":{\"255\":1}}],[\"自己和自己的值其实是占大头的\",{\"1\":{\"221\":1}}],[\"自一致性技术\",{\"0\":{\"199\":1}}],[\"自然语言处理\",{\"0\":{\"273\":1}}],[\"自然语言建模的场景\",{\"1\":{\"223\":1}}],[\"自然语言推理\",{\"1\":{\"221\":1}}],[\"自然语言推断nli\",{\"1\":{\"204\":1}}],[\"自然语言理解包含了广泛的多样性任务\",{\"1\":{\"203\":1}}],[\"自然语言指令\",{\"1\":{\"10\":1}}],[\"自然指数\",{\"1\":{\"169\":1}}],[\"自动跳过\",{\"1\":{\"255\":1}}],[\"自动安装\",{\"1\":{\"147\":1}}],[\"自动生成\",{\"1\":{\"81\":1}}],[\"自定义工具调用\",{\"1\":{\"278\":1}}],[\"自定义指令与记忆功能\",{\"1\":{\"278\":1}}],[\"自定义的批量数据处理函数\",{\"1\":{\"107\":1}}],[\"自定义数据集\",{\"1\":{\"107\":1}}],[\"自定义一个mydataset类来封装我们加载得到的数据集\",{\"1\":{\"107\":1}}],[\"自回归语言建模任务\",{\"1\":{\"103\":1}}],[\"自注意力子层\",{\"1\":{\"266\":2,\"269\":2}}],[\"自注意力运算\",{\"1\":{\"103\":1}}],[\"自注意力\",{\"1\":{\"103\":2}}],[\"自注意力和交叉注意力流程统一化\",{\"1\":{\"103\":1}}],[\"自注意力掩码策略\",{\"1\":{\"101\":1,\"102\":1,\"103\":1}}],[\"自注意力机制是\",{\"1\":{\"261\":1}}],[\"自注意力机制能够捕捉图像中任意两个\",{\"1\":{\"110\":1}}],[\"自注意力机制\",{\"1\":{\"33\":1,\"122\":1}}],[\"自监督方法的优势在于不再需要标注数据\",{\"1\":{\"96\":1}}],[\"自谷歌提出\",{\"1\":{\"88\":1}}],[\"自适应融合模块\",{\"0\":{\"28\":1,\"32\":1},\"1\":{\"27\":1}}],[\"骨干网络\",{\"1\":{\"27\":1}}],[\"视觉编码阶段\",{\"1\":{\"103\":1}}],[\"视觉编码器提取图像特征\",{\"1\":{\"80\":1}}],[\"视觉编码器\",{\"1\":{\"80\":1,\"81\":1}}],[\"视觉分支\",{\"1\":{\"98\":1}}],[\"视觉+语言\",{\"1\":{\"85\":1}}],[\"视觉的跨模态交互能力\",{\"1\":{\"32\":1}}],[\"视觉与大语言模型\",{\"1\":{\"26\":1}}],[\"视觉语义特征\",{\"1\":{\"10\":1,\"15\":1}}],[\"推荐系统等多种应用场景\",{\"1\":{\"283\":1}}],[\"推出了其推理模型\",{\"1\":{\"278\":1}}],[\"推测约\",{\"1\":{\"278\":1}}],[\"推动\",{\"1\":{\"26\":1}}],[\"推理延迟\",{\"1\":{\"285\":1}}],[\"推理能力限制\",{\"1\":{\"283\":1}}],[\"推理能力和代码能力提升非常显著\",{\"1\":{\"278\":1}}],[\"推理策略\",{\"1\":{\"280\":1}}],[\"推理深度和创意表达方面有显著提升\",{\"1\":{\"278\":1}}],[\"推理型大模型\",{\"1\":{\"278\":1}}],[\"推理型模型\",{\"1\":{\"278\":1}}],[\"推理型\",{\"1\":{\"278\":3}}],[\"推理任务\",{\"1\":{\"221\":1}}],[\"推理成本是不得不要考虑的一个因素\",{\"1\":{\"179\":1}}],[\"推理时\",{\"1\":{\"100\":1}}],[\"推理\",{\"0\":{\"91\":1},\"1\":{\"197\":1}}],[\"推理等\",{\"1\":{\"85\":1}}],[\"推理优先\",{\"1\":{\"83\":1}}],[\"推理阶段\",{\"1\":{\"23\":1}}],[\"推理或训练分支\",{\"1\":{\"10\":1}}],[\"旨在帮助开发者提高应用程序的质量\",{\"1\":{\"289\":1}}],[\"旨在帮助开发者们快速构建基于大型语言模型的端到端应用程序或工作流程\",{\"1\":{\"286\":1}}],[\"旨在满足不同用户和应用场景的需求\",{\"1\":{\"278\":1}}],[\"旨在将图像中的每个像素分类为不同的语义类别\",{\"1\":{\"164\":1}}],[\"旨在实现语言引导下的\",{\"1\":{\"26\":1}}],[\"旨在推动\",{\"1\":{\"17\":1}}],[\"也称大型语言模型\",{\"1\":{\"277\":1}}],[\"也称为\",{\"1\":{\"168\":1}}],[\"也称为intersection\",{\"1\":{\"168\":1}}],[\"也称为误报概率\",{\"1\":{\"154\":1}}],[\"也称为召回率\",{\"1\":{\"153\":1}}],[\"也在上表4中\",{\"1\":{\"212\":1}}],[\"也分析了在四种不同设置下预训练模型的零次\",{\"1\":{\"204\":1}}],[\"也用\",{\"1\":{\"190\":2}}],[\"也几乎未引入额外的\",{\"1\":{\"189\":1}}],[\"也不局限于某一个方案\",{\"1\":{\"180\":1}}],[\"也还有一些其它参数\",{\"1\":{\"178\":1}}],[\"也通常是句子长度\",{\"1\":{\"123\":1}}],[\"也就为非线性激活函数提供了更多可以学习的特征组合\",{\"1\":{\"112\":1}}],[\"也就是所谓的\",{\"1\":{\"280\":1}}],[\"也就是我们常说的\",{\"1\":{\"280\":1}}],[\"也就是在句子开头加一个\",{\"1\":{\"219\":1}}],[\"也就是两个句子\",{\"1\":{\"219\":1}}],[\"也就是\",{\"1\":{\"187\":1}}],[\"也就是和输入的\",{\"1\":{\"127\":1}}],[\"也就是q\",{\"1\":{\"116\":1}}],[\"也就是经过卷积后拼接得到的特征图\",{\"1\":{\"109\":1}}],[\"也就是卷积核的数量\",{\"1\":{\"109\":1}}],[\"也就是一张图像搭配与之对应的文本描述\",{\"1\":{\"89\":1}}],[\"也就是说\",{\"1\":{\"33\":1,\"79\":1,\"153\":1,\"154\":1,\"257\":1,\"286\":1}}],[\"也就是取出上面随机选择的问题文本\",{\"1\":{\"25\":1}}],[\"也需要保持输入图像尺寸与预训练时一致\",{\"1\":{\"108\":1}}],[\"也是其一大亮点\",{\"1\":{\"91\":1}}],[\"也可能导致错误分类\",{\"1\":{\"69\":1}}],[\"也可以基于大模型的推理\",{\"1\":{\"281\":1}}],[\"也可以用以下方式表示\",{\"1\":{\"159\":1}}],[\"也可以进行随机裁剪\",{\"1\":{\"108\":1}}],[\"也可以采用视觉transformer模型\",{\"1\":{\"90\":1}}],[\"也可以是特定任务\",{\"1\":{\"78\":1}}],[\"也可以是软标签\",{\"1\":{\"21\":1}}],[\"也可以归一化为\",{\"1\":{\"39\":1}}],[\"也一并加入\",{\"1\":{\"49\":1,\"53\":1}}],[\"也一并提取\",{\"1\":{\"49\":1}}],[\"也应该以一种方式被处理\",{\"1\":{\"43\":1}}],[\"指导大型语言模型生成更为精准的答案\",{\"1\":{\"283\":1}}],[\"指的就是上述公式中的矩阵w\",{\"1\":{\"178\":1}}],[\"指的是输入图像的尺寸为\",{\"1\":{\"118\":1}}],[\"指南\",{\"1\":{\"156\":1}}],[\"指明哪些位置是有效的\",{\"1\":{\"103\":1}}],[\"指令微调\",{\"1\":{\"280\":1}}],[\"指令遵循与文本生成提供了复杂业务逻辑的简单平替方案\",{\"1\":{\"290\":1}}],[\"指令遵循\",{\"1\":{\"280\":1}}],[\"指令\",{\"1\":{\"78\":2,\"80\":2,\"85\":2}}],[\"指令调优\",{\"1\":{\"78\":1,\"85\":2}}],[\"指令理解特征\",{\"1\":{\"15\":1}}],[\"指标的选择和权衡\",{\"0\":{\"156\":1}}],[\"指标\",{\"1\":{\"39\":2,\"156\":1,\"168\":1}}],[\"指定的列\",{\"1\":{\"25\":1}}],[\"固定返回问题0\",{\"1\":{\"25\":1}}],[\"根据decoder的隐状态输出一个词\",{\"1\":{\"263\":1}}],[\"根据编码器的输出生成目标序列\",{\"1\":{\"261\":1}}],[\"根据你的理解\",{\"1\":{\"255\":1}}],[\"根据索引矩阵\",{\"1\":{\"227\":1}}],[\"根据索引获取数据集中的图像和对应的标签\",{\"1\":{\"107\":1}}],[\"根据上下文\",{\"1\":{\"218\":1}}],[\"根据上述计算得到的和其相似度最高的分类文本索引\",{\"1\":{\"93\":1}}],[\"根据频次表构建最终的词汇表\",{\"1\":{\"175\":1}}],[\"根据n轮迭代合并后的vocab来构建最终的频次表\",{\"1\":{\"175\":1}}],[\"根据特定的分割任务的需求和特点\",{\"1\":{\"173\":1}}],[\"根据预测的\",{\"1\":{\"255\":1}}],[\"根据预测误差对预测结果进行排序\",{\"1\":{\"171\":1}}],[\"根据预测值从灰色\",{\"1\":{\"40\":1}}],[\"根据正负样本比例调整\",{\"1\":{\"169\":1}}],[\"根据\",{\"1\":{\"147\":1,\"172\":1}}],[\"根据相关性\",{\"1\":{\"130\":1}}],[\"根据图像特征\",{\"1\":{\"103\":1}}],[\"根据imagenet数据集上的zero\",{\"1\":{\"96\":1}}],[\"根据文字搜索图片\",{\"1\":{\"94\":1,\"95\":1}}],[\"根据任务的分类需求\",{\"1\":{\"91\":1}}],[\"根据注意力机制加权求和\",{\"1\":{\"72\":1}}],[\"根据距离反比加权\",{\"1\":{\"57\":1}}],[\"根据距离分配权重\",{\"1\":{\"57\":1}}],[\"根据样本索引取出样本数据\",{\"1\":{\"25\":1}}],[\"根据自然语言问题找出与之相关的功能区域\",{\"1\":{\"19\":1}}],[\"点数\",{\"1\":{\"169\":1}}],[\"点\",{\"1\":{\"162\":2}}],[\"点表示\",{\"1\":{\"159\":1}}],[\"点的数量\",{\"1\":{\"68\":1}}],[\"点的特征数据\",{\"1\":{\"49\":1}}],[\"点之间的相互作用\",{\"1\":{\"62\":1}}],[\"点之间存在相互作用\",{\"1\":{\"61\":1}}],[\"点与点之间有空间关系\",{\"1\":{\"61\":1}}],[\"点额外特征\",{\"1\":{\"49\":1}}],[\"点坐标\",{\"1\":{\"49\":1}}],[\"点大小\",{\"1\":{\"40\":1}}],[\"点集抽象层\",{\"1\":{\"49\":1}}],[\"点集的划分必须产生跨分区的共同结构\",{\"1\":{\"43\":1}}],[\"点集划分是指如何将一个大的点云分割成更小的\",{\"1\":{\"43\":1}}],[\"点集特征集合\",{\"1\":{\"32\":1}}],[\"点集集合中每个点的特征和文本特征信息进行融合\",{\"1\":{\"27\":1}}],[\"点特征作为键\",{\"1\":{\"29\":1}}],[\"点级别标注\",{\"1\":{\"25\":1}}],[\"点云模型\",{\"1\":{\"69\":1}}],[\"点云是点的集合\",{\"1\":{\"61\":1}}],[\"点云是三维几何数据的一种重要表示形式\",{\"1\":{\"60\":1}}],[\"点云的姿态可能各不相同\",{\"1\":{\"64\":1}}],[\"点云的无序性\",{\"1\":{\"61\":1,\"62\":1}}],[\"点云的额外特征\",{\"1\":{\"49\":1}}],[\"点云语义分割模型\",{\"0\":{\"58\":1}}],[\"点云语义分割\",{\"0\":{\"55\":1}}],[\"点云坐标数据\",{\"1\":{\"49\":2}}],[\"点云坐标\",{\"1\":{\"40\":1}}],[\"点云预处理\",{\"1\":{\"40\":1}}],[\"点云分割中最常用的指标之一\",{\"1\":{\"39\":1}}],[\"点云维度\",{\"1\":{\"33\":1}}],[\"点云特征图\",{\"1\":{\"33\":1}}],[\"点云对象和一个自然语言问题\",{\"1\":{\"27\":1}}],[\"点云id\",{\"1\":{\"25\":2}}],[\"点云来源\",{\"1\":{\"24\":1}}],[\"点云\",{\"0\":{\"71\":1},\"1\":{\"24\":1,\"71\":2}}],[\"点云中点的顺序不影响整体形状\",{\"1\":{\"72\":1}}],[\"点云中点的数量\",{\"1\":{\"39\":1}}],[\"点云中的每个点被标注为支持一个或多个功能类型\",{\"1\":{\"19\":1}}],[\"点云中相关点\",{\"1\":{\"11\":1}}],[\"点云等\",{\"1\":{\"13\":1}}],[\"点云编码器\",{\"1\":{\"10\":1}}],[\"点云数据归一化处理\",{\"1\":{\"40\":1}}],[\"点云数据组织形式\",{\"1\":{\"25\":1}}],[\"点云数据\",{\"1\":{\"10\":1,\"25\":1,\"40\":1,\"58\":1}}],[\"x和y之间的关系是\",{\"1\":{\"178\":1}}],[\"xm\",{\"1\":{\"178\":1}}],[\"xlabel\",{\"1\":{\"107\":1}}],[\"x=i\",{\"1\":{\"107\":1}}],[\"xticks\",{\"1\":{\"107\":1}}],[\"xa\",{\"1\":{\"81\":1}}],[\"x2\",{\"1\":{\"178\":1}}],[\"x2a\",{\"1\":{\"81\":1}}],[\"x2instruct\",{\"1\":{\"81\":1}}],[\"xsystem\",{\"1\":{\"81\":2}}],[\"xinstruct\",{\"1\":{\"81\":1}}],[\"xi\",{\"1\":{\"62\":1}}],[\"xn\",{\"1\":{\"62\":3}}],[\"x1a\",{\"1\":{\"81\":1}}],[\"x1instruct\",{\"1\":{\"81\":1}}],[\"x1\",{\"1\":{\"62\":3,\"178\":1,\"278\":1}}],[\"xx\",{\"1\":{\"40\":1}}],[\"xyz2\",{\"1\":{\"55\":2,\"57\":6}}],[\"xyz1\",{\"1\":{\"55\":3,\"57\":6}}],[\"xyz\",{\"1\":{\"27\":5,\"33\":1,\"49\":60,\"50\":11,\"53\":31,\"55\":2,\"58\":22}}],[\"x\",{\"1\":{\"25\":2,\"29\":4,\"30\":10,\"31\":5,\"32\":3,\"33\":2,\"50\":12,\"53\":10,\"58\":9,\"64\":29,\"66\":32,\"67\":10,\"68\":20,\"71\":2,\"109\":8,\"110\":14,\"111\":15,\"112\":21,\"113\":9,\"114\":19,\"137\":7,\"161\":1,\"172\":2,\"175\":2,\"178\":1,\"184\":1,\"244\":7,\"255\":1,\"263\":2,\"265\":3,\"266\":8,\"267\":4,\"269\":12,\"270\":4,\"271\":7}}],[\"x9x\",{\"1\":{\"4\":1}}],[\"建议设为较高值\",{\"1\":{\"169\":1}}],[\"建议总是先激活\",{\"1\":{\"147\":1}}],[\"建议用linux或者windows系统进行测试\",{\"1\":{\"8\":1}}],[\"建立物体类型和功能类型的索引映射关系\",{\"1\":{\"25\":1}}],[\"kv\",{\"1\":{\"278\":1}}],[\"kimi\",{\"1\":{\"277\":1}}],[\"k矩阵乘积\",{\"1\":{\"116\":1}}],[\"k=int\",{\"1\":{\"107\":1}}],[\"k=2\",{\"1\":{\"67\":1}}],[\"k=64\",{\"1\":{\"66\":1}}],[\"k×k\",{\"1\":{\"65\":1}}],[\"knn查询寻找最近的k个邻居\",{\"1\":{\"47\":1}}],[\"knn\",{\"1\":{\"47\":1}}],[\"knife\",{\"1\":{\"25\":1}}],[\"k\",{\"0\":{\"124\":1},\"1\":{\"29\":3,\"31\":3,\"33\":2,\"53\":3,\"62\":1,\"67\":1,\"68\":11,\"69\":5,\"100\":1,\"107\":2,\"113\":7,\"122\":1,\"128\":1,\"137\":2,\"224\":2,\"230\":4,\"244\":2,\"271\":9}}],[\"keep\",{\"1\":{\"256\":1}}],[\"keepdims=true\",{\"1\":{\"93\":1,\"95\":1}}],[\"keepdim=true\",{\"1\":{\"57\":1,\"64\":1,\"66\":1,\"91\":2}}],[\"kernel\",{\"1\":{\"33\":1,\"64\":1,\"109\":2}}],[\"kernels\",{\"1\":{\"27\":1,\"33\":2}}],[\"keys\",{\"1\":{\"175\":1}}],[\"key来自query\",{\"1\":{\"103\":1}}],[\"key=pairs\",{\"1\":{\"175\":2,\"176\":1}}],[\"key=self\",{\"1\":{\"33\":1}}],[\"key=gt\",{\"1\":{\"32\":1}}],[\"key=x\",{\"1\":{\"32\":1}}],[\"key=lambda\",{\"1\":{\"25\":1,\"175\":1}}],[\"key和value为融合后的文本特征\",{\"1\":{\"31\":1}}],[\"key和value都是点云特征\",{\"1\":{\"29\":1}}],[\"keyboard\",{\"1\":{\"25\":1}}],[\"key\",{\"1\":{\"15\":19,\"20\":1,\"25\":4,\"27\":1,\"29\":8,\"31\":11,\"32\":1,\"33\":7,\"102\":3,\"103\":44,\"107\":2,\"122\":1,\"123\":2,\"125\":1,\"128\":1,\"130\":1,\"132\":1,\"244\":7,\"271\":7,\"278\":2}}],[\"kwargs\",{\"1\":{\"25\":1,\"30\":1,\"104\":2}}],[\"种语言和方言\",{\"1\":{\"278\":1}}],[\"种模型大小\",{\"1\":{\"278\":1}}],[\"种可能的顺序\",{\"1\":{\"209\":1}}],[\"种唯一组合\",{\"1\":{\"24\":1}}],[\"种物体\",{\"1\":{\"20\":1}}],[\"不公开\",{\"1\":{\"278\":1}}],[\"不使用传统的循环或卷积结构\",{\"1\":{\"261\":1}}],[\"不具备生成能力\",{\"1\":{\"255\":1}}],[\"不计入答案\",{\"1\":{\"255\":1}}],[\"不是自回归生成器\",{\"1\":{\"255\":1}}],[\"不是三角函数\",{\"1\":{\"219\":1}}],[\"不足长度用padding填充\",{\"1\":{\"233\":1}}],[\"不少人很自然地有了这样子的想法\",{\"1\":{\"217\":1}}],[\"不要预训练\",{\"1\":{\"213\":1}}],[\"不要让llm只生成最合适的唯一一个结果\",{\"1\":{\"199\":1}}],[\"不清楚在学习文本表示时\",{\"1\":{\"204\":1}}],[\"不变\",{\"1\":{\"187\":1}}],[\"不再这里一一介绍\",{\"1\":{\"185\":1}}],[\"不会过滤掉任何词\",{\"1\":{\"175\":1}}],[\"不会影响其他环境或系统\",{\"1\":{\"147\":1}}],[\"不过\",{\"1\":{\"152\":1}}],[\"不平衡数据集的一个示例可能是一组数以千计的云彩照片\",{\"1\":{\"151\":1}}],[\"不管理\",{\"1\":{\"147\":1}}],[\"不做预训练\",{\"1\":{\"83\":1}}],[\"不做任何变化\",{\"1\":{\"64\":1}}],[\"不包括\",{\"1\":{\"74\":1}}],[\"不改变形状和大小\",{\"1\":{\"74\":1}}],[\"不改变物体形状和内部结构\",{\"1\":{\"73\":1}}],[\"不改变\",{\"1\":{\"73\":1}}],[\"不常用\",{\"1\":{\"72\":1,\"253\":1}}],[\"不够大\",{\"1\":{\"69\":1}}],[\"不够精细\",{\"1\":{\"69\":1}}],[\"不利于高维空间建模\",{\"1\":{\"69\":1}}],[\"不稳定\",{\"1\":{\"64\":1}}],[\"不同的\",{\"1\":{\"217\":1}}],[\"不同的消融研究如下表5\",{\"1\":{\"213\":1}}],[\"不同的分类角度\",{\"1\":{\"180\":1}}],[\"不同特征之间能够进行更多样的组合\",{\"1\":{\"112\":1}}],[\"不同模态数据的提取与融合\",{\"1\":{\"99\":1}}],[\"不同位置\",{\"1\":{\"64\":1}}],[\"不同尺度的查询半径列表\",{\"1\":{\"53\":1}}],[\"不同尺度的特征被串联形成多尺度特征向量\",{\"1\":{\"52\":1}}],[\"不考虑空间邻域关系\",{\"1\":{\"64\":1}}],[\"不灵活等问题\",{\"1\":{\"60\":1}}],[\"不带法向量或其他属性\",{\"1\":{\"58\":1}}],[\"不能\",{\"1\":{\"255\":1}}],[\"不能超出上下文范围\",{\"1\":{\"255\":1}}],[\"不能像\",{\"1\":{\"255\":1}}],[\"不能用\",{\"1\":{\"190\":1}}],[\"不能直接使用这些操作\",{\"1\":{\"57\":1}}],[\"不能作为\",{\"1\":{\"39\":1}}],[\"不断对点云进行下采样\",{\"1\":{\"56\":1}}],[\"不进行采样\",{\"1\":{\"49\":1}}],[\"不通过缓存直接将内容打印到屏幕上\",{\"1\":{\"40\":1}}],[\"不支持\",{\"1\":{\"39\":1}}],[\"不直接优化最终目标\",{\"1\":{\"39\":1}}],[\"不像\",{\"1\":{\"39\":1,\"166\":1,\"168\":1}}],[\"不仅限于英语\",{\"1\":{\"279\":1}}],[\"不仅考虑从当前分辨率下抽象得到的特征\",{\"1\":{\"54\":1}}],[\"不仅理解语言指令\",{\"1\":{\"39\":1}}],[\"不仅理解自己的语义\",{\"1\":{\"33\":1}}],[\"不仅看交集\",{\"1\":{\"39\":1}}],[\"不需要先\",{\"1\":{\"166\":1}}],[\"不需要创建类的实例\",{\"1\":{\"107\":1}}],[\"不需要额外标注数据\",{\"1\":{\"85\":1}}],[\"不需要\",{\"1\":{\"39\":1}}],[\"不依赖绝对数量\",{\"1\":{\"167\":1}}],[\"不依赖特定阈值\",{\"1\":{\"39\":1}}],[\"不依赖\",{\"1\":{\"39\":1}}],[\"不加\",{\"1\":{\"32\":1}}],[\"不在训练中透露\",{\"1\":{\"23\":1}}],[\"不方便进行处理\",{\"1\":{\"8\":1}}],[\"测试数据使用1k\",{\"1\":{\"232\":1}}],[\"测试和开发集\",{\"1\":{\"232\":1}}],[\"测试图片分类正确率\",{\"1\":{\"93\":1,\"95\":1}}],[\"测试集最终评估\",{\"1\":{\"39\":1}}],[\"测试集\",{\"1\":{\"25\":1,\"37\":1,\"290\":1}}],[\"测试\",{\"1\":{\"23\":1,\"25\":1,\"289\":1}}],[\"验证迭代在大模型开发中是极其重要的一步\",{\"1\":{\"291\":1}}],[\"验证迭代\",{\"1\":{\"291\":1}}],[\"验证迭代优化\",{\"1\":{\"290\":1}}],[\"验证集上准确率达到了98\",{\"1\":{\"118\":1}}],[\"验证集上进行评估的核心代码实现如下\",{\"1\":{\"39\":1}}],[\"验证集不需要进行数据增强\",{\"1\":{\"108\":1}}],[\"验证集的预处理转换操作\",{\"1\":{\"108\":1}}],[\"验证集\",{\"1\":{\"37\":1,\"290\":1}}],[\"验证\",{\"1\":{\"23\":1,\"25\":1}}],[\"训练数据中的敏感信息需要妥善处理\",{\"1\":{\"285\":1}}],[\"训练数据中的例子\",{\"1\":{\"78\":1}}],[\"训练的模型\",{\"1\":{\"278\":1}}],[\"训练的时候固定\",{\"1\":{\"189\":1}}],[\"训练能够准确预测下一个单词的\",{\"1\":{\"278\":1}}],[\"训练过程就比较常规了\",{\"1\":{\"227\":1}}],[\"训练过程中\",{\"1\":{\"110\":1}}],[\"训练一个少量参数的小模型\",{\"1\":{\"182\":1}}],[\"训练一个轻量级的微调模型\",{\"1\":{\"179\":1}}],[\"训练一个奖励模型\",{\"1\":{\"78\":1}}],[\"训练成本非常高\",{\"1\":{\"179\":1}}],[\"训练语料所在的文件列表\",{\"1\":{\"175\":1}}],[\"训练语料为\",{\"1\":{\"175\":1}}],[\"训练与评估流程的代码为模版代码\",{\"1\":{\"118\":1}}],[\"训练了10个epoch\",{\"1\":{\"118\":1}}],[\"训练代价\",{\"1\":{\"98\":1}}],[\"训练效率成为一个至关重要的因素\",{\"1\":{\"96\":1}}],[\"训练效率可以提高4倍\",{\"1\":{\"96\":1}}],[\"训练使用到的数据集和alexnet保持一致\",{\"1\":{\"93\":1}}],[\"训练出a和b即可得到∆w\",{\"1\":{\"184\":1}}],[\"训练出具有可迁移能力的视觉模型\",{\"1\":{\"88\":1}}],[\"训练出一个能自然理解图像内容\",{\"1\":{\"81\":1}}],[\"训练出一个能看懂图的视觉分词器\",{\"1\":{\"80\":1}}],[\"训练方式\",{\"1\":{\"85\":1}}],[\"训练策略\",{\"1\":{\"83\":1}}],[\"训练目标的引导\",{\"1\":{\"110\":1}}],[\"训练目标\",{\"1\":{\"80\":1,\"81\":1,\"85\":1,\"103\":1}}],[\"训练流程\",{\"1\":{\"80\":1,\"81\":1}}],[\"训练奖励模型\",{\"1\":{\"78\":1}}],[\"训练监督模型\",{\"1\":{\"78\":1}}],[\"训练\",{\"0\":{\"36\":1,\"38\":1,\"90\":1,\"227\":1},\"1\":{\"25\":1,\"36\":1,\"211\":1}}],[\"训练集的预处理转换操作\",{\"1\":{\"108\":1}}],[\"训练集\",{\"1\":{\"25\":1}}],[\"训练阶段则是模型的核心迭代过程\",{\"1\":{\"38\":1}}],[\"训练阶段\",{\"1\":{\"23\":1}}],[\"训练和测试阶段共享相似的物体类别和功能类型的分布\",{\"1\":{\"22\":1}}],[\"物体在空间中移动时\",{\"1\":{\"73\":1}}],[\"物体分割和场景语义解析\",{\"1\":{\"60\":1}}],[\"物体名称匹配\",{\"1\":{\"25\":1}}],[\"物体\",{\"1\":{\"24\":1}}],[\"物体类别索引\",{\"1\":{\"25\":1}}],[\"物体类别\",{\"1\":{\"24\":1,\"25\":1}}],[\"物体类别数\",{\"1\":{\"22\":2}}],[\"物体组合\",{\"1\":{\"22\":1}}],[\"但一般通过调用\",{\"1\":{\"290\":1}}],[\"但要创建完整的应用程序\",{\"1\":{\"286\":1}}],[\"但面对未见过的输入时仍可能出现幻觉\",{\"1\":{\"285\":1}}],[\"但单一模型可能难以全面适应所有场景\",{\"1\":{\"283\":1}}],[\"但大型语言模型的出现再次让聊天机器人这种交互模式可以重新涌现\",{\"1\":{\"282\":1}}],[\"但我们更关注的是其通用能力\",{\"1\":{\"280\":1}}],[\"但并不能保证结果的正确性\",{\"1\":{\"197\":1}}],[\"但并不是所有的参数都是发挥同样作用的\",{\"1\":{\"184\":1}}],[\"但结果会更加靠谱\",{\"1\":{\"197\":1}}],[\"但可能无法充分定制模型行为或写作风格\",{\"1\":{\"285\":1}}],[\"但可能会出错\",{\"1\":{\"197\":1}}],[\"但可用于特定任务\",{\"1\":{\"72\":1}}],[\"但起核心作用的参数是低秩的\",{\"1\":{\"192\":1}}],[\"但也可能会把原来表现好的别的领域的能力变差\",{\"1\":{\"180\":1}}],[\"但fft也会带来一些问题\",{\"1\":{\"180\":1}}],[\"但对于对外提供服务的企业来说\",{\"1\":{\"179\":1}}],[\"但保留作为接口兼容\",{\"1\":{\"172\":1}}],[\"但保留它们作为接口兼容\",{\"1\":{\"170\":1}}],[\"但保留接口以备后续扩展\",{\"1\":{\"166\":1}}],[\"但真实是正类的样本数\",{\"1\":{\"170\":1}}],[\"但真实是负类的样本数\",{\"1\":{\"170\":1}}],[\"但\",{\"1\":{\"168\":1}}],[\"但更易梯度下降\",{\"1\":{\"166\":1}}],[\"但每个输出向量的维度由\",{\"1\":{\"127\":1}}],[\"但默认的组合方式可能不满足所有需求\",{\"1\":{\"107\":1}}],[\"但发现这种方法的训练效率\",{\"1\":{\"96\":1}}],[\"但存在一定的噪声\",{\"1\":{\"96\":1}}],[\"但实际上这行代码的意图是计算\",{\"1\":{\"169\":1}}],[\"但实际上\",{\"1\":{\"92\":1}}],[\"但其主要目的是训练可迁移的视觉模型\",{\"1\":{\"90\":1}}],[\"但已经可以处理基本的图文问答任务\",{\"1\":{\"80\":1}}],[\"但如果训练时没有加入扰动\",{\"1\":{\"69\":1}}],[\"但这些模型仍然采用固定类别的softmax分类器进行预训练\",{\"1\":{\"96\":1}}],[\"但这种方式表达能力有限\",{\"1\":{\"69\":1}}],[\"但这可能导致所选邻域的实际尺寸随点的密度变化而变化\",{\"1\":{\"47\":1}}],[\"但遇到遮挡严重或点分布不均匀时性能下降明显\",{\"1\":{\"69\":1}}],[\"但由于它们是神经网络直接预测出来的\",{\"1\":{\"65\":1}}],[\"但由于其无序性和非规则性\",{\"1\":{\"60\":1}}],[\"但在大型模型中特别突出\",{\"1\":{\"280\":1}}],[\"但在理解复杂语言规则方面存在一定局限性\",{\"1\":{\"277\":1}}],[\"但在前景远少于背景时容易偏向负样本\",{\"1\":{\"167\":1}}],[\"但在处理超大规模点云时\",{\"1\":{\"69\":1}}],[\"但在精度上仍略逊一筹\",{\"1\":{\"69\":1}}],[\"但在一些复杂区域\",{\"1\":{\"69\":1}}],[\"但在点云这种非结构化数据中\",{\"1\":{\"57\":1}}],[\"但在计算上可能非常昂贵\",{\"1\":{\"54\":1}}],[\"但在测试集中保留\",{\"1\":{\"22\":1}}],[\"但是越清晰\",{\"1\":{\"291\":1}}],[\"但是发展速度相当惊人\",{\"1\":{\"278\":1}}],[\"但是和rnn相比\",{\"1\":{\"260\":1}}],[\"但是和transformer原始的encoder还是有所区别\",{\"1\":{\"112\":1}}],[\"但是我们在计算损失时指定了ignore\",{\"1\":{\"227\":1}}],[\"但是对于一般的小公司或者个人来说\",{\"1\":{\"188\":1}}],[\"但是对于vit这个结构而言\",{\"1\":{\"108\":1}}],[\"但是反馈的来源是ai\",{\"1\":{\"180\":1}}],[\"但是它的缺点也非常明显\",{\"1\":{\"179\":1}}],[\"但是\",{\"1\":{\"158\":1,\"194\":1,\"221\":2}}],[\"但是训练速度还是挺快的\",{\"1\":{\"118\":1}}],[\"但是当数据量逐渐增大时\",{\"1\":{\"115\":1}}],[\"但是当训练数据集不够大的时候\",{\"1\":{\"105\":1}}],[\"但是迁移到其它数据集训练时\",{\"1\":{\"114\":1}}],[\"但是实际的代码实现中\",{\"1\":{\"109\":1}}],[\"但是这样造成的一个后果是计算量太庞大\",{\"1\":{\"109\":1}}],[\"但是尺度不同\",{\"1\":{\"53\":1}}],[\"但是在一个场景中有多个物体时则不好办\",{\"1\":{\"43\":1}}],[\"但它们也引发了伦理和风险问题\",{\"1\":{\"279\":1}}],[\"但它们展现出截然不同的能力\",{\"1\":{\"277\":1}}],[\"但它们的目标\",{\"1\":{\"85\":1}}],[\"但它也为后续模型奠定了基础\",{\"1\":{\"69\":1}}],[\"但它是为了统一接口设计的一个占位符\",{\"1\":{\"49\":1}}],[\"但它本质上是\",{\"1\":{\"35\":1}}],[\"但完全不使用注意力机制\",{\"1\":{\"30\":1}}],[\"但测试时要求\",{\"1\":{\"22\":1}}],[\"using\",{\"1\":{\"271\":1}}],[\"users\",{\"1\":{\"232\":1}}],[\"user\",{\"1\":{\"141\":1,\"143\":3}}],[\"used\",{\"1\":{\"112\":1,\"233\":1}}],[\"use\",{\"1\":{\"93\":1,\"95\":1,\"100\":1,\"103\":5,\"104\":2,\"113\":1}}],[\"ulmfit\",{\"1\":{\"223\":1}}],[\"utf\",{\"1\":{\"175\":1,\"176\":1,\"223\":3,\"224\":1}}],[\"utility\",{\"1\":{\"40\":4}}],[\"util\",{\"1\":{\"40\":1}}],[\"utils\",{\"1\":{\"39\":1,\"40\":1,\"107\":3,\"108\":2,\"227\":1}}],[\"u\",{\"1\":{\"40\":4}}],[\"unusual\",{\"1\":{\"244\":1}}],[\"unk\",{\"1\":{\"176\":1,\"224\":5,\"225\":1,\"233\":2}}],[\"unicode\",{\"1\":{\"238\":1,\"248\":1}}],[\"uni\",{\"1\":{\"101\":2}}],[\"union\",{\"0\":{\"168\":1},\"1\":{\"39\":6,\"168\":5}}],[\"unordered\",{\"1\":{\"61\":1,\"72\":1}}],[\"unbuffered\",{\"1\":{\"40\":1}}],[\"underlying\",{\"1\":{\"213\":1}}],[\"understanding\",{\"1\":{\"202\":1}}],[\"under\",{\"1\":{\"39\":3,\"61\":1}}],[\"un\",{\"1\":{\"32\":1}}],[\"ungroup\",{\"1\":{\"32\":2}}],[\"ungrouping阶段\",{\"1\":{\"32\":1}}],[\"ungrouping\",{\"0\":{\"31\":1},\"1\":{\"28\":1,\"32\":3}}],[\"unsupervised\",{\"1\":{\"202\":1}}],[\"unsqueeze\",{\"1\":{\"27\":2,\"32\":1,\"33\":3,\"39\":2,\"40\":1,\"101\":4,\"226\":1,\"230\":1,\"236\":1,\"241\":2,\"271\":1}}],[\"unseen\",{\"1\":{\"22\":1,\"39\":1}}],[\"upsample\",{\"1\":{\"57\":1}}],[\"up\",{\"1\":{\"10\":1,\"12\":1,\"16\":6,\"27\":11,\"33\":7,\"76\":1,\"185\":1}}],[\"见过\",{\"1\":{\"22\":1}}],[\"📚\",{\"1\":{\"255\":1}}],[\"👉\",{\"1\":{\"125\":1,\"127\":1}}],[\"📌\",{\"1\":{\"69\":1}}],[\"📐\",{\"1\":{\"69\":1}}],[\"📊\",{\"1\":{\"69\":1}}],[\"📈\",{\"1\":{\"69\":2}}],[\"📉\",{\"1\":{\"69\":2}}],[\"🔍\",{\"1\":{\"69\":6,\"255\":1}}],[\"📦\",{\"1\":{\"57\":1,\"72\":1}}],[\"🔹\",{\"1\":{\"22\":2}}],[\"💡\",{\"1\":{\"21\":1,\"69\":1,\"255\":1}}],[\"一文详尽之scaling\",{\"1\":{\"292\":1}}],[\"一样的人工智能\",{\"1\":{\"282\":1}}],[\"一些\",{\"1\":{\"279\":1}}],[\"一些损失函数具有额外的超参数\",{\"1\":{\"173\":1}}],[\"一系列推理加速技术\",{\"1\":{\"278\":1}}],[\"一步步引导llm得出复杂问题的结果\",{\"1\":{\"200\":1}}],[\"一步步重建回原始点数量\",{\"1\":{\"27\":1}}],[\"一是完整的ltm的例子\",{\"1\":{\"200\":1}}],[\"一次训练的成本就在上千亿美元\",{\"1\":{\"188\":1}}],[\"一条是只对部分的参数进行训练\",{\"1\":{\"180\":1}}],[\"一条是对全量的参数\",{\"1\":{\"180\":1}}],[\"一共合并merges个高频字符对后\",{\"1\":{\"175\":1}}],[\"一起送入\",{\"1\":{\"104\":1}}],[\"一起送入mini\",{\"1\":{\"48\":1}}],[\"一种假设是\",{\"1\":{\"213\":1}}],[\"一种平均值\",{\"1\":{\"157\":1}}],[\"一种思路是在转换之前\",{\"1\":{\"111\":1}}],[\"一种朴素的想法就是把一个个像素点拉平\",{\"1\":{\"109\":1}}],[\"一种是平移不变形\",{\"1\":{\"105\":1}}],[\"一种是局部性\",{\"1\":{\"105\":1}}],[\"一种是常用的cnn架构resnet\",{\"1\":{\"90\":1}}],[\"一种比较好理解的方式\",{\"1\":{\"100\":1}}],[\"一般指通过不断发现\",{\"1\":{\"291\":1}}],[\"一般使用诸如\",{\"1\":{\"291\":1}}],[\"一般来说\",{\"1\":{\"291\":1}}],[\"一般应先设定最小化目标\",{\"1\":{\"291\":1}}],[\"一般从2开始调优\",{\"1\":{\"169\":1}}],[\"一般\",{\"1\":{\"98\":2}}],[\"一句话总结\",{\"1\":{\"69\":1}}],[\"一\",{\"0\":{\"123\":1,\"140\":1},\"1\":{\"69\":1}}],[\"一致\",{\"1\":{\"55\":1}}],[\"一组新的关键点位置\",{\"1\":{\"53\":1}}],[\"一组点云被处理和抽象\",{\"1\":{\"45\":1}}],[\"一个\",{\"1\":{\"221\":1}}],[\"一个问题\",{\"1\":{\"209\":1}}],[\"一个问题可以作用于多个物体类别\",{\"1\":{\"22\":1}}],[\"一个简单的技巧是在你的prompt后面\",{\"1\":{\"197\":1}}],[\"一个简单的线性层\",{\"1\":{\"80\":1}}],[\"一个是该\",{\"1\":{\"253\":1}}],[\"一个是叫灾难性遗忘\",{\"1\":{\"180\":1}}],[\"一个是训练的成本会比较高\",{\"1\":{\"180\":1}}],[\"一个block之后维度依然和输入相同\",{\"1\":{\"112\":1}}],[\"一个改进的想法就是把一张图片分成nxn个patch\",{\"1\":{\"109\":1}}],[\"一个批次的数据\",{\"1\":{\"107\":1}}],[\"一个文件夹对应一个类别\",{\"1\":{\"107\":1}}],[\"一个的轻量q\",{\"1\":{\"98\":1}}],[\"一个视觉模型和一个文本模型\",{\"1\":{\"91\":1}}],[\"一个列表\",{\"1\":{\"57\":1}}],[\"一个非法索引\",{\"1\":{\"49\":1}}],[\"一个元组\",{\"1\":{\"49\":1,\"64\":1,\"66\":1}}],[\"一旦点云被划分成小的子集\",{\"1\":{\"43\":1}}],[\"一名普通但十分热爱探索技术的coder\",{\"1\":{\"2\":1}}],[\"功能区域\",{\"1\":{\"40\":1,\"169\":2}}],[\"功能区域预测结果\",{\"1\":{\"40\":1}}],[\"功能区域识别任务中\",{\"1\":{\"39\":1}}],[\"功能区域分割\",{\"1\":{\"26\":1}}],[\"功能区域掩码\",{\"1\":{\"25\":3}}],[\"功能属性匹配\",{\"1\":{\"25\":1}}],[\"功能类别\",{\"1\":{\"25\":2}}],[\"功能类型索引\",{\"1\":{\"25\":1}}],[\"功能类型\",{\"1\":{\"24\":1}}],[\"功能类型数\",{\"1\":{\"22\":1}}],[\"功能组合的标注数据\",{\"1\":{\"25\":1}}],[\"功能组合\",{\"1\":{\"20\":2,\"24\":1}}],[\"类型应用前景的思考\",{\"1\":{\"282\":1}}],[\"类型\",{\"1\":{\"255\":1}}],[\"类似物理学中的相变现象\",{\"1\":{\"280\":1}}],[\"类似数据库查询\",{\"1\":{\"132\":1}}],[\"类似\",{\"1\":{\"88\":1}}],[\"类似于在cnn中权重共享的概念\",{\"1\":{\"43\":1}}],[\"类似于传统的全连接层或\",{\"1\":{\"30\":1}}],[\"类似于\",{\"1\":{\"30\":1,\"278\":1}}],[\"类别权重\",{\"1\":{\"170\":1,\"172\":1}}],[\"类别数量不平衡\",{\"1\":{\"169\":1}}],[\"类别平衡权重\",{\"1\":{\"169\":1}}],[\"类别不平衡越严重\",{\"1\":{\"169\":1}}],[\"类别不平衡的问题\",{\"1\":{\"169\":1}}],[\"类别不平衡\",{\"1\":{\"166\":1}}],[\"类别\",{\"1\":{\"85\":3}}],[\"类中\",{\"1\":{\"64\":1}}],[\"类\",{\"1\":{\"22\":2,\"24\":2,\"109\":1}}],[\"总句对数量\",{\"1\":{\"228\":1}}],[\"总掩码的词数量\",{\"1\":{\"228\":1}}],[\"总词汇量\",{\"1\":{\"224\":1}}],[\"总体而言\",{\"1\":{\"212\":1}}],[\"总体来看\",{\"1\":{\"96\":1}}],[\"总之\",{\"1\":{\"208\":1,\"213\":1,\"282\":1}}],[\"总损失\",{\"1\":{\"35\":1,\"227\":1}}],[\"总结一句话\",{\"1\":{\"85\":1}}],[\"总结一下这篇文章\",{\"1\":{\"85\":1}}],[\"总结文章\",{\"1\":{\"78\":1}}],[\"总结表格\",{\"1\":{\"69\":1}}],[\"总结\",{\"0\":{\"26\":1,\"119\":1,\"128\":1},\"1\":{\"47\":1,\"62\":1,\"255\":1}}],[\"总样本数\",{\"1\":{\"22\":1}}],[\"总共得到\",{\"1\":{\"20\":1}}],[\"软标签通常用于边界模糊区域\",{\"1\":{\"21\":1}}],[\"构成一个位置序列矩阵\",{\"1\":{\"236\":1}}],[\"构成了一个\",{\"1\":{\"170\":1}}],[\"构造\",{\"1\":{\"226\":1}}],[\"构造训练目标\",{\"1\":{\"103\":1}}],[\"构造一个新的张量\",{\"1\":{\"227\":1}}],[\"构造一个单位矩阵\",{\"1\":{\"65\":1}}],[\"构造一个从\",{\"1\":{\"49\":1}}],[\"构造出\",{\"1\":{\"26\":1}}],[\"构造出对应的二值掩码\",{\"1\":{\"21\":1}}],[\"构建出一个来源于实际业务的小型验证集\",{\"1\":{\"291\":1}}],[\"构建器平台\",{\"1\":{\"278\":1}}],[\"构建样本\",{\"1\":{\"225\":1}}],[\"构建了语言模型\",{\"1\":{\"218\":1}}],[\"构建词汇表\",{\"1\":{\"175\":1}}],[\"构建一个基于\",{\"1\":{\"168\":1,\"169\":1}}],[\"构建一个组合损失函数\",{\"1\":{\"167\":1}}],[\"构建一个下三角矩阵作为因果掩码矩阵\",{\"1\":{\"103\":1}}],[\"构建一个多样化的指令\",{\"1\":{\"78\":1}}],[\"构建匹配标签\",{\"1\":{\"102\":1}}],[\"构建输入图像列表\",{\"1\":{\"102\":1}}],[\"构建输入文本列表\",{\"1\":{\"102\":1}}],[\"构建query和text的padding\",{\"1\":{\"102\":1}}],[\"构建query\",{\"1\":{\"102\":1}}],[\"构建padding\",{\"1\":{\"100\":1}}],[\"构建描述文本并提取特征\",{\"1\":{\"91\":1}}],[\"构建全局特征向量\",{\"1\":{\"72\":1}}],[\"构建点云的层次化表示\",{\"1\":{\"69\":1}}],[\"构建点之间的邻接图\",{\"1\":{\"69\":1}}],[\"构建mlp层\",{\"1\":{\"57\":1}}],[\"构建用于特征传播\",{\"1\":{\"57\":1}}],[\"构建它们的局部邻域区域\",{\"1\":{\"49\":1}}],[\"构建局部邻域的半径\",{\"1\":{\"49\":2}}],[\"构建问题\",{\"0\":{\"20\":1}}],[\"构建\",{\"1\":{\"13\":2,\"35\":2,\"103\":1,\"169\":1}}],[\"陈述句等\",{\"1\":{\"20\":1}}],[\"疑问句\",{\"1\":{\"20\":1}}],[\"结束\",{\"1\":{\"255\":1}}],[\"结束符\",{\"1\":{\"104\":1}}],[\"结论\",{\"0\":{\"214\":1}}],[\"结果\",{\"1\":{\"127\":1}}],[\"结果如下表3\",{\"1\":{\"212\":1}}],[\"结果如下\",{\"1\":{\"115\":1}}],[\"结果如下图所示\",{\"1\":{\"111\":1}}],[\"结果发现在imagenet数据集上能够带来3\",{\"1\":{\"92\":1}}],[\"结果显示\",{\"1\":{\"69\":1}}],[\"结果就不会变\",{\"1\":{\"62\":1}}],[\"结果表明\",{\"1\":{\"32\":1}}],[\"结构规整\",{\"1\":{\"71\":1}}],[\"结构单一\",{\"1\":{\"69\":1}}],[\"结构简单\",{\"1\":{\"69\":1}}],[\"结构\",{\"0\":{\"262\":1,\"264\":1,\"268\":1},\"1\":{\"55\":1,\"211\":1,\"261\":1}}],[\"结构多样性\",{\"1\":{\"20\":1}}],[\"结合上述分析\",{\"1\":{\"291\":1}}],[\"结合特殊的数据或业务逻辑来提供独特功能的应用称为大模型开发\",{\"1\":{\"290\":1}}],[\"结合检索到的信息和模型的生成能力\",{\"1\":{\"283\":1}}],[\"结合缓存的attention\",{\"1\":{\"103\":1}}],[\"结合\",{\"1\":{\"62\":1}}],[\"结合了层级特征提取和多尺度融合机制\",{\"1\":{\"55\":1}}],[\"结合论文理解这些指标的意义\",{\"1\":{\"39\":1}}],[\"结合人工+gpt\",{\"1\":{\"26\":1}}],[\"结合其对应的功能类型和原始点云标注信息\",{\"1\":{\"21\":1}}],[\"结合全局池化增强语义表达\",{\"1\":{\"16\":1}}],[\"简化部署流程\",{\"1\":{\"289\":1}}],[\"简化版\",{\"1\":{\"167\":1}}],[\"简析llama\",{\"1\":{\"272\":1}}],[\"简析\",{\"0\":{\"272\":1}}],[\"简析pointnet网络模型及其背后原理\",{\"1\":{\"59\":1}}],[\"简析pointnet\",{\"0\":{\"59\":1}}],[\"简析pointnet++\",{\"0\":{\"42\":1},\"1\":{\"42\":1}}],[\"简介\",{\"0\":{\"204\":1}}],[\"简单样本抑制越强\",{\"1\":{\"169\":1}}],[\"简单样本主导梯度\",{\"1\":{\"169\":1}}],[\"简单样本的梯度贡献淹没难样本的梯度\",{\"1\":{\"169\":1}}],[\"简单来说\",{\"1\":{\"92\":1,\"218\":1}}],[\"简洁\",{\"1\":{\"71\":1}}],[\"简洁表达\",{\"1\":{\"20\":1}}],[\"简称\",{\"1\":{\"17\":1,\"189\":1,\"192\":1}}],[\"例子说明\",{\"0\":{\"133\":1}}],[\"例\",{\"1\":{\"20\":1}}],[\"例如原始数据或预处理后的数据\",{\"1\":{\"286\":1}}],[\"例如数据收集模块或预处理模块\",{\"1\":{\"286\":1}}],[\"例如数学问题\",{\"1\":{\"280\":1}}],[\"例如拥有\",{\"1\":{\"277\":1}}],[\"例如上图中\",{\"1\":{\"221\":1}}],[\"例如背景像素可能占据了大部分\",{\"1\":{\"164\":1}}],[\"例如总共\",{\"1\":{\"154\":1,\"155\":1}}],[\"例如卷云\",{\"1\":{\"151\":1}}],[\"例如某些包在arm64系统上没有预先编译好的版本\",{\"1\":{\"147\":1}}],[\"例如在多头自注意力机制或前馈网络中引入卷积层\",{\"1\":{\"117\":1}}],[\"例如参数量为\",{\"1\":{\"115\":1}}],[\"例如clip\",{\"1\":{\"98\":1}}],[\"例如谷歌的bit和vit基于jft\",{\"1\":{\"96\":1}}],[\"例如2017年的那篇工作只在imagenet上实现了11\",{\"1\":{\"96\":1}}],[\"例如virtex基于transformer的语言模型\",{\"1\":{\"96\":1}}],[\"例如openai的gpt\",{\"1\":{\"96\":1}}],[\"例如法向量\",{\"1\":{\"16\":1}}],[\"例如\",{\"1\":{\"15\":1,\"19\":1,\"22\":1,\"27\":1,\"40\":1,\"55\":1,\"57\":1,\"91\":1,\"92\":1,\"96\":1,\"105\":1,\"117\":2,\"141\":1,\"152\":1,\"162\":1,\"170\":1,\"173\":1,\"218\":3,\"219\":1,\"253\":1,\"277\":1,\"291\":1}}],[\"上进行训练\",{\"1\":{\"278\":1}}],[\"上进行预训练\",{\"1\":{\"96\":1,\"108\":1}}],[\"上线后用户增长迅速\",{\"1\":{\"278\":1}}],[\"上的研究大致可以分为以下几个阶段\",{\"1\":{\"278\":1}}],[\"上的问题答案数据集\",{\"1\":{\"212\":1}}],[\"上下文学习能力是由\",{\"1\":{\"280\":1}}],[\"上下文学习\",{\"1\":{\"280\":1}}],[\"上下文感知\",{\"1\":{\"279\":1}}],[\"上下文长度\",{\"1\":{\"278\":6}}],[\"上下文长度扩展至\",{\"1\":{\"278\":2}}],[\"上下文\",{\"1\":{\"253\":3,\"278\":1}}],[\"上下文丰富化\",{\"1\":{\"20\":1}}],[\"上图中的两句话明显是连续的\",{\"1\":{\"219\":1}}],[\"上图中是每一个patch中各位置的位置编码相似性度量\",{\"1\":{\"111\":1}}],[\"上使用多头自注意力操作\",{\"1\":{\"207\":1}}],[\"上提升8\",{\"1\":{\"203\":1}}],[\"上述的全量微调流程问题在于大模型的参数量往往特别大\",{\"1\":{\"187\":1}}],[\"上述即为pointnet++设计中的两个核心挑战\",{\"1\":{\"43\":1}}],[\"上一句话\",{\"1\":{\"219\":1}}],[\"上一部分介绍了一系列模型指标\",{\"1\":{\"158\":1}}],[\"上一层点集中的点特征重建过程中\",{\"1\":{\"27\":1}}],[\"上面优化方向很多\",{\"1\":{\"224\":1}}],[\"上面代码实现中使用的是加权交叉熵损失\",{\"1\":{\"172\":1}}],[\"上面代码实现中使用的是可学习位置嵌入\",{\"1\":{\"111\":1}}],[\"上面的完美模型包含边长为\",{\"1\":{\"160\":1}}],[\"上面的花卉分类案例相当于使用图像去匹配最合适的文本描述\",{\"1\":{\"94\":1}}],[\"上面已经给出了数据集加载以及vit模型核心代码实现了\",{\"1\":{\"118\":1}}],[\"上略低于\",{\"1\":{\"69\":1}}],[\"上\",{\"1\":{\"69\":1}}],[\"上做最大池化\",{\"1\":{\"49\":1}}],[\"上也具有泛化能力\",{\"1\":{\"39\":1}}],[\"上采样\",{\"1\":{\"57\":2}}],[\"上采样后的点云特征\",{\"1\":{\"33\":1}}],[\"上采样过程中\",{\"1\":{\"27\":1}}],[\"上采样过程\",{\"1\":{\"16\":1}}],[\"描述功能的文本提示\",{\"1\":{\"40\":1}}],[\"描述\",{\"1\":{\"20\":1,\"39\":4,\"69\":1,\"72\":1,\"166\":1,\"167\":1,\"168\":1,\"172\":1}}],[\"原序列添加特殊token标记图\",{\"1\":{\"233\":1}}],[\"原模型虽大\",{\"1\":{\"192\":1}}],[\"原始论文layernorm在最后\",{\"1\":{\"265\":1}}],[\"原始transformer的norm层在多头注意力和前馈网络之后\",{\"1\":{\"112\":1}}],[\"原始点集合\",{\"1\":{\"71\":1}}],[\"原始点数量\",{\"1\":{\"57\":1}}],[\"原始点对应的特征数据\",{\"1\":{\"57\":1}}],[\"原始点坐标数据\",{\"1\":{\"57\":1}}],[\"原始点坐标\",{\"1\":{\"55\":1}}],[\"原始点云数据\",{\"1\":{\"50\":1}}],[\"原始点云数量\",{\"1\":{\"16\":2}}],[\"原始点特征\",{\"1\":{\"31\":1,\"55\":1}}],[\"原理大致跟rlhf类似\",{\"1\":{\"180\":1}}],[\"原理\",{\"0\":{\"106\":1}}],[\"原理回顾\",{\"1\":{\"69\":1}}],[\"原理说明\",{\"1\":{\"62\":1}}],[\"原因分析\",{\"1\":{\"69\":2}}],[\"原则\",{\"1\":{\"20\":1}}],[\"×\",{\"1\":{\"20\":1,\"33\":3,\"169\":2,\"172\":4}}],[\"个性化大模型应用需要有个性化数据库进行支撑\",{\"1\":{\"291\":1}}],[\"个集成\",{\"1\":{\"288\":1}}],[\"个核心组件组成\",{\"1\":{\"287\":1}}],[\"个月的最快记录\",{\"1\":{\"278\":1}}],[\"个选项\",{\"1\":{\"257\":1}}],[\"个词之间\",{\"1\":{\"255\":1}}],[\"个词\",{\"1\":{\"221\":1}}],[\"个词到第\",{\"1\":{\"221\":1}}],[\"个人理解是因为\",{\"1\":{\"221\":1}}],[\"个句子相接\",{\"1\":{\"217\":1}}],[\"个句子在原始本文中是否跟第\",{\"1\":{\"217\":1}}],[\"个注意力头\",{\"1\":{\"211\":1}}],[\"个序列表示\",{\"1\":{\"209\":1}}],[\"个示例\",{\"1\":{\"154\":1,\"155\":1}}],[\"个类别和\",{\"1\":{\"118\":1}}],[\"个可训练参数\",{\"1\":{\"115\":2}}],[\"个输出\",{\"1\":{\"64\":1}}],[\"个坐标值\",{\"1\":{\"64\":1}}],[\"个最近邻点索引\",{\"1\":{\"57\":1}}],[\"个邻近点的特征\",{\"1\":{\"57\":1}}],[\"个邻近点\",{\"1\":{\"57\":1}}],[\"个关键点作为局部区域中心\",{\"1\":{\"53\":1}}],[\"个关键点对应的全局区域特征向量\",{\"1\":{\"50\":1}}],[\"个关键点对应的局部区域特征向量\",{\"1\":{\"50\":2}}],[\"个关键点的坐标\",{\"1\":{\"50\":3}}],[\"个维度\",{\"1\":{\"49\":1}}],[\"个分布尽可能均匀的采样点索引\",{\"1\":{\"49\":1}}],[\"个具有代表性的点\",{\"1\":{\"49\":1}}],[\"个点的子集决定\",{\"1\":{\"69\":1}}],[\"个点\",{\"1\":{\"24\":1,\"49\":2,\"57\":3}}],[\"个点云\",{\"1\":{\"22\":1,\"26\":1}}],[\"个定制化问题\",{\"1\":{\"24\":1}}],[\"个专家设计的问题\",{\"1\":{\"20\":1,\"22\":1}}],[\"个问题\",{\"1\":{\"20\":1}}],[\"个代表性问题\",{\"1\":{\"20\":1}}],[\"为未来的系统化扩展和安全性提升奠定了坚实基础\",{\"1\":{\"288\":1}}],[\"为开发者带来了全面而强大的功能支持\",{\"1\":{\"288\":1}}],[\"为模型提供丰富的上下文信息\",{\"1\":{\"283\":1}}],[\"为当前批次中的每个序列样本生成一个位置序列\",{\"1\":{\"236\":1}}],[\"为收集更多的标注数据提供了更多一个有价值的替代方案\",{\"1\":{\"204\":1}}],[\"为例\",{\"0\":{\"133\":1},\"1\":{\"277\":1}}],[\"为\",{\"1\":{\"114\":1,\"133\":1,\"153\":1,\"159\":2,\"160\":4,\"175\":1,\"209\":1,\"288\":1,\"289\":1}}],[\"为特定任务动态构建了一个分类器\",{\"1\":{\"91\":1}}],[\"为下一阶段的端到端微调提供了良好的初始化\",{\"1\":{\"80\":1}}],[\"为什么答案来自\",{\"1\":{\"255\":1}}],[\"为什么还需要prompt\",{\"1\":{\"194\":1}}],[\"为什么不也用\",{\"1\":{\"190\":1}}],[\"为什么要用第一个位置\",{\"1\":{\"221\":1}}],[\"为什么要对大模型进行微调\",{\"0\":{\"179\":1}}],[\"为什么要把它们结合起来\",{\"1\":{\"167\":1}}],[\"为什么要除以\",{\"0\":{\"136\":1}}],[\"为什么\",{\"0\":{\"135\":1}}],[\"为什么需要这个正则化项\",{\"1\":{\"65\":1}}],[\"为什么使用\",{\"1\":{\"19\":1,\"172\":1}}],[\"为后续三维深度学习奠定了基础\",{\"1\":{\"62\":1}}],[\"为后续的处理步骤提供信息\",{\"1\":{\"54\":1}}],[\"为每个特定任务\",{\"1\":{\"182\":1}}],[\"为每个图像选择一个负样本文本\",{\"1\":{\"102\":1}}],[\"为每个文本选择一个负样本图像\",{\"1\":{\"102\":1}}],[\"为每个类别创建一个描述性的文本\",{\"1\":{\"91\":1}}],[\"为每个原始点\",{\"1\":{\"57\":1}}],[\"为每个中心点找到其邻域内的点\",{\"1\":{\"55\":1}}],[\"为每个尺度构建一个独立的小型\",{\"1\":{\"53\":1}}],[\"为每个关键点构建局部邻域\",{\"1\":{\"49\":1}}],[\"为每个组合额外生成\",{\"1\":{\"20\":1}}],[\"为了解决这个问题\",{\"1\":{\"286\":1}}],[\"为了解决这一问题\",{\"1\":{\"51\":1}}],[\"为了解决大型语言模型在生成文本时面临的一系列挑战\",{\"1\":{\"283\":1}}],[\"为了提高训练稳定性\",{\"1\":{\"278\":1}}],[\"为了提升数据质量\",{\"1\":{\"80\":1}}],[\"为了提升模型对点云姿态变化的鲁棒性\",{\"1\":{\"65\":1}}],[\"为了探索性能的极限\",{\"1\":{\"277\":1}}],[\"为了简单\",{\"1\":{\"265\":1}}],[\"为了反映这点\",{\"1\":{\"209\":1}}],[\"为了让llm给出的答案更加靠谱\",{\"1\":{\"197\":1}}],[\"为了方便大家理解\",{\"1\":{\"178\":1}}],[\"为了避免除以零\",{\"1\":{\"168\":1}}],[\"为了将\",{\"1\":{\"168\":1}}],[\"为了将其作为损失函数使用\",{\"1\":{\"166\":1}}],[\"为了充分利用预训练的权重\",{\"1\":{\"108\":1}}],[\"为了训练好q\",{\"1\":{\"100\":1}}],[\"为了训练clip模型\",{\"1\":{\"90\":1}}],[\"为了弥补数据规模上的差距\",{\"1\":{\"96\":1}}],[\"为了实现文字搜索图像的功能\",{\"1\":{\"94\":1}}],[\"为了模拟用户提问和模型回答的形式\",{\"1\":{\"80\":1}}],[\"为了模仿传统卷积网络中的权重共享机制以提高学习效率和模型的泛化能力\",{\"1\":{\"43\":1}}],[\"为了能够在不同的局部子集上共享权重\",{\"1\":{\"43\":1}}],[\"为点云\",{\"1\":{\"31\":1}}],[\"为此\",{\"1\":{\"28\":1,\"54\":1}}],[\"手工设计问题\",{\"1\":{\"20\":1}}],[\"这对于\",{\"1\":{\"288\":1}}],[\"这影响了其对问题的理解和回答\",{\"1\":{\"283\":1}}],[\"这可能会影响到其在相关领域的回答质量\",{\"1\":{\"283\":1}}],[\"这可能导致模型的知识更新滞后\",{\"1\":{\"283\":1}}],[\"这可能是因为参数量增加需要更多的语料\",{\"1\":{\"191\":1}}],[\"这引发了对未来人工智能发展的许多思考和计划\",{\"1\":{\"282\":1}}],[\"这引发我们对于\",{\"1\":{\"282\":1}}],[\"这使得开发者能够对\",{\"1\":{\"288\":1}}],[\"这使得它们在对话\",{\"1\":{\"279\":1}}],[\"这使得它们能够捕捉更多的语言知识和复杂的语法结构\",{\"1\":{\"279\":1}}],[\"这使得它在细粒度识别\",{\"1\":{\"69\":1}}],[\"这使得其语义编码能力得到了极大的增强\",{\"1\":{\"278\":1}}],[\"这被称为\",{\"1\":{\"277\":1}}],[\"这和原始论文稍有不同\",{\"1\":{\"265\":1}}],[\"这比较容易并行\",{\"1\":{\"260\":1}}],[\"这其实就是在说\",{\"1\":{\"255\":1}}],[\"这其实是一个很容易理解的任务\",{\"1\":{\"218\":1}}],[\"这段代码的意思是\",{\"1\":{\"255\":1}}],[\"这类抽取式问答任务中\",{\"1\":{\"255\":1}}],[\"这尤其适合当需要长时依赖\",{\"1\":{\"223\":1}}],[\"这本质上是一个三分类的问题\",{\"1\":{\"221\":1}}],[\"这本书里介绍了我们人类大脑的\",{\"1\":{\"197\":1}}],[\"这会导致在跨任务上性能降低14\",{\"1\":{\"213\":1}}],[\"这会占用大量的内存资源并消耗较多的计算资源\",{\"1\":{\"192\":1}}],[\"这表明预训练模型中的每一层都包含了解决目标问题有用的功能\",{\"1\":{\"213\":1}}],[\"这表明训练clip模型需要消耗大量的资源\",{\"1\":{\"90\":1}}],[\"这跟之前的工作一样\",{\"1\":{\"208\":1}}],[\"这限制了它们在许多缺乏标记数据领域的适用性\",{\"1\":{\"204\":1}}],[\"这句话会引导llm\",{\"1\":{\"197\":1}}],[\"这主要是因为如果矩阵\",{\"1\":{\"190\":1}}],[\"这部分关键参数就是上面提到的低维的本质模型\",{\"1\":{\"184\":1}}],[\"这部分代码实现如下\",{\"1\":{\"29\":1,\"30\":1}}],[\"这条路径叫peft\",{\"1\":{\"180\":1}}],[\"这条路径叫全量微调fft\",{\"1\":{\"180\":1}}],[\"这时候cot的效果就不尽如人意\",{\"1\":{\"200\":1}}],[\"这时候也需要对大模型进行微调\",{\"1\":{\"179\":1}}],[\"这时候针对每个用户的数据\",{\"1\":{\"179\":1}}],[\"这时候微调就非常适用\",{\"1\":{\"179\":1}}],[\"这时就可以自定义\",{\"1\":{\"107\":1}}],[\"这也是一个非常严重的问题\",{\"1\":{\"179\":1}}],[\"这也是为什么结构图中mlp\",{\"1\":{\"110\":1}}],[\"这有助于模型发现输入数据中更复杂的模式和关系\",{\"1\":{\"112\":1}}],[\"这几乎是不可能完成的任务\",{\"1\":{\"105\":1}}],[\"这减少了llm学习视觉语言对齐的负担\",{\"1\":{\"104\":1}}],[\"这大大限制了它们的迁移能力和扩展性\",{\"1\":{\"96\":1}}],[\"这远远低于imagenet上的sota\",{\"1\":{\"96\":1}}],[\"这方面的工作并不多\",{\"1\":{\"96\":1}}],[\"这与传统的预训练加微调的方法有所不同\",{\"1\":{\"92\":1}}],[\"这与在传统cnn中学习图像局部区域特征的过程相似\",{\"1\":{\"43\":1}}],[\"这展示了其在图像分类任务中的灵活性和强大能力\",{\"1\":{\"91\":1}}],[\"这不仅展示了clip的强大功能\",{\"1\":{\"91\":1}}],[\"这篇论文中介绍的方法\",{\"1\":{\"200\":1}}],[\"这篇论文里讲的另一个prompt\",{\"1\":{\"199\":1}}],[\"这篇论文里讲的一个prompt\",{\"1\":{\"198\":1}}],[\"这篇论文首次尝试使用仅支持文本输入的\",{\"1\":{\"78\":1}}],[\"这篇论文提出了一项新的任务和一个配套的数据集\",{\"1\":{\"17\":1}}],[\"这就要求网络中的某些关键操作必须是对称函数\",{\"1\":{\"72\":1}}],[\"这就是答案\",{\"1\":{\"255\":1}}],[\"这就是我们要去学习prompt\",{\"1\":{\"194\":1}}],[\"这就是blip\",{\"1\":{\"98\":1}}],[\"这就是\",{\"1\":{\"65\":1,\"197\":1}}],[\"这就是下一个\",{\"1\":{\"49\":2}}],[\"这一里程碑式的更新\",{\"1\":{\"288\":1}}],[\"这一发现标志着大型语言模型\",{\"1\":{\"277\":1}}],[\"这一块的技巧性很强\",{\"1\":{\"197\":1}}],[\"这一部分的目标是\",{\"1\":{\"103\":1}}],[\"这一过程与训练时相同\",{\"1\":{\"91\":1}}],[\"这一过程通过对每个子区域应用集合抽象层\",{\"1\":{\"54\":1}}],[\"这一步的操作在论文中是直接采用切割的处理办法\",{\"1\":{\"109\":1}}],[\"这一步是为了保证图像的整体比例不变\",{\"1\":{\"108\":1}}],[\"这一步不在\",{\"1\":{\"64\":1}}],[\"这一步相当于图像任务中的\",{\"1\":{\"57\":1}}],[\"这两个数表示\",{\"1\":{\"221\":1}}],[\"这两个模型都属于融合图像与文本的多模态模型\",{\"1\":{\"88\":1}}],[\"这两个网络输出的是变换矩阵\",{\"1\":{\"65\":1}}],[\"这两个操作交替进行\",{\"1\":{\"30\":1}}],[\"这两种特征被concat为一个复合特征向量\",{\"1\":{\"54\":1}}],[\"这里我准备做一个文本分类任务\",{\"1\":{\"232\":1}}],[\"这里pad部分指的是对于不同的句子\",{\"1\":{\"227\":1}}],[\"这里\",{\"1\":{\"207\":2}}],[\"这里也非常像我们人类学习解决复杂问题的过程\",{\"1\":{\"200\":1}}],[\"这里有像我们人类解决问题的过程\",{\"1\":{\"199\":1}}],[\"这里简单推理一下\",{\"1\":{\"190\":1}}],[\"这里简单介绍一下cls\",{\"1\":{\"110\":1}}],[\"这里先简单介绍一下\",{\"1\":{\"185\":1}}],[\"这里面∆w主是我们要微调得到的结果\",{\"1\":{\"184\":1}}],[\"这里是想解决反馈系统的效率问题\",{\"1\":{\"180\":1}}],[\"这里为了方便理解\",{\"1\":{\"178\":1}}],[\"这里把所有\",{\"1\":{\"172\":1}}],[\"这里不直接使用\",{\"1\":{\"172\":1}}],[\"这里不再贴出\",{\"1\":{\"118\":1}}],[\"这里采用的是\",{\"1\":{\"223\":1}}],[\"这里采用的方法是一种\",{\"1\":{\"169\":1}}],[\"这里采用了余弦相似度的计算方法\",{\"1\":{\"93\":1}}],[\"这里使用的是负的\",{\"1\":{\"172\":1}}],[\"这里使用\",{\"1\":{\"169\":1}}],[\"这里需要注意一点\",{\"1\":{\"227\":1}}],[\"这里需要激活\",{\"1\":{\"167\":1}}],[\"这里需要将其分离开来\",{\"1\":{\"113\":1}}],[\"这里暂时未使用\",{\"1\":{\"167\":1}}],[\"这里主要有两种位置编码思路\",{\"1\":{\"111\":1}}],[\"这里设置为图像块的大小\",{\"1\":{\"109\":2}}],[\"这里对训练集的处理方式是随机切成224x224像素的图片\",{\"1\":{\"108\":1}}],[\"这里对提取的文本特征和图像特征进行对比学习\",{\"1\":{\"90\":1}}],[\"这里以搜索向日葵花为例\",{\"1\":{\"94\":1}}],[\"这里共有个正样本\",{\"1\":{\"90\":1}}],[\"这里的例子比较简单\",{\"1\":{\"221\":1}}],[\"这里的关键是在prompt中加入的示例\",{\"1\":{\"198\":1}}],[\"这里的原因\",{\"1\":{\"112\":1}}],[\"这里的\",{\"1\":{\"109\":1}}],[\"这里的相似度直接计算文本特征和图像特征的余弦相似性\",{\"1\":{\"90\":1}}],[\"这里的三个\",{\"1\":{\"50\":1}}],[\"这里tgt就是roberta编码得到的文本特征嵌入向量\",{\"1\":{\"33\":1}}],[\"这在lora这篇论文中也被称为低秩分解自适应技术\",{\"1\":{\"187\":1}}],[\"这在处理非均匀采样的数据时可能不是最优的选择\",{\"1\":{\"47\":1}}],[\"这在使用\",{\"1\":{\"40\":1}}],[\"这样它就可以恢复\",{\"1\":{\"278\":1}}],[\"这样在后续的softmax计算中\",{\"1\":{\"230\":1}}],[\"这样在训练中最小化损失就等于最大化重叠度\",{\"1\":{\"170\":1}}],[\"这样难免会影响到最终的结果\",{\"1\":{\"221\":1}}],[\"这样强迫模型在编码当前时刻词的时候不能太依赖当前的词\",{\"1\":{\"218\":1}}],[\"这样做的好处是\",{\"1\":{\"218\":1}}],[\"这样做的目的是提高模型的效率和泛化能力\",{\"1\":{\"43\":1}}],[\"这样llm输出的内容也会更加贴合我们的需求\",{\"1\":{\"196\":1}}],[\"这样一来\",{\"1\":{\"187\":1}}],[\"这样的\",{\"1\":{\"255\":1}}],[\"这样的大模型\",{\"1\":{\"188\":1}}],[\"这样的大模型中\",{\"1\":{\"174\":1}}],[\"这样的格式来生成文本描述\",{\"1\":{\"92\":1}}],[\"这样越接近\",{\"1\":{\"168\":1}}],[\"这样就可以完成vit的训练过程\",{\"1\":{\"114\":1}}],[\"这样就可以提取出对最终任务有帮助的特征组合\",{\"1\":{\"112\":1}}],[\"这样就能让\",{\"1\":{\"112\":1,\"257\":1}}],[\"这样就成了一个一维序列\",{\"1\":{\"109\":1}}],[\"这样计算量就大大减小了\",{\"1\":{\"109\":1}}],[\"这样可以为\",{\"1\":{\"117\":1}}],[\"这样可以在模型的不同阶段交替利用\",{\"1\":{\"117\":1}}],[\"这样可以在每个部分上独立地学习特征\",{\"1\":{\"43\":1}}],[\"这样可以提升计算效率\",{\"1\":{\"113\":1}}],[\"这样可以保证模型的特征提取能力和性能\",{\"1\":{\"108\":1}}],[\"这样模型就能根据上下文更准确地做出判断\",{\"1\":{\"68\":1}}],[\"这样每个点在预测标签时都能看到整个物体的上下文\",{\"1\":{\"62\":1}}],[\"这样\",{\"1\":{\"52\":2,\"64\":1,\"109\":1,\"168\":1,\"281\":1}}],[\"这需要一个\",{\"1\":{\"43\":1}}],[\"这种能力可能是通过对代码的训练获得的\",{\"1\":{\"280\":1}}],[\"这种能力允许语言模型在提供自然语言指令或多个任务示例的情况下\",{\"1\":{\"280\":1}}],[\"这种归一化方法可以避免梯度爆炸和消失的问题\",{\"1\":{\"278\":1}}],[\"这种分离的设计让模型更灵活\",{\"1\":{\"131\":1}}],[\"这种差异一方面是由于文本和图像属于两个完全不同的模态\",{\"1\":{\"96\":1}}],[\"这种预训练通常是基于有监督学习的\",{\"1\":{\"96\":1}}],[\"这种格式\",{\"1\":{\"92\":1}}],[\"这种设计使得网络只关注\",{\"1\":{\"69\":1}}],[\"这种设计使得语言信息能有效地指导点特征的学习过程\",{\"1\":{\"32\":1}}],[\"这种方法重新引入了大量特定任务的定制化输入\",{\"1\":{\"209\":1}}],[\"这种方法实际上与nlp领域的一个研究方向\",{\"1\":{\"92\":1}}],[\"这种方法大大增强了网络处理非均匀采样数据的能力\",{\"1\":{\"53\":1}}],[\"这种方法使得网络能够在细节丰富的区域\",{\"1\":{\"52\":1}}],[\"这种方法使网络能够通过在训练期间随机丢弃输入点\",{\"1\":{\"52\":1}}],[\"这种非均匀性为点集特征学习带来了显著挑战\",{\"1\":{\"51\":1}}],[\"这种类别不平衡\",{\"1\":{\"39\":1}}],[\"这意味着它能够在没有任何特定任务训练数据的情况下\",{\"1\":{\"91\":1}}],[\"这意味着即使是不同的局部子集\",{\"1\":{\"43\":1}}],[\"这意味着\",{\"1\":{\"39\":1,\"69\":1,\"160\":1,\"280\":1}}],[\"这些涌现能力让\",{\"1\":{\"280\":1}}],[\"这些特点使\",{\"1\":{\"279\":1}}],[\"这些特点使它们在自然语言处理和其他领域中引起了广泛的兴趣和研究\",{\"1\":{\"279\":1}}],[\"这些模型都是以cnn为基础\",{\"1\":{\"260\":1}}],[\"这些位置的概率就会接近0\",{\"1\":{\"230\":1}}],[\"这些是\",{\"1\":{\"224\":1}}],[\"这些词汇是从wikipedia的优质文章和标杆文章中提取得到\",{\"1\":{\"223\":1}}],[\"这些\",{\"1\":{\"218\":1}}],[\"这些输入转换使作者避免跨任务架构的大改\",{\"1\":{\"209\":1}}],[\"这些参数会以随机梯度下降训练\",{\"1\":{\"207\":1}}],[\"这些改写使得在预训练模型架构上用最小的修改就会有效\",{\"1\":{\"204\":1}}],[\"这些不确定性使得开发有效的语言处理半监督学习方法变得困难\",{\"1\":{\"204\":1}}],[\"这些方法都有各自的特点\",{\"1\":{\"188\":1}}],[\"这些方法会导致信息损失\",{\"1\":{\"60\":1}}],[\"这些值会根据模型的损失函数不断调整\",{\"1\":{\"111\":1}}],[\"这些相似度值可以被视为logits\",{\"1\":{\"91\":1}}],[\"这些图像特征会与之前得到的个文本特征进行余弦相似度计算\",{\"1\":{\"91\":1}}],[\"这些文本随后被输入到文本编码器\",{\"1\":{\"91\":1}}],[\"这些数据集包括\",{\"1\":{\"278\":1}}],[\"这些数据在论文中被称为webimagetext\",{\"1\":{\"90\":1}}],[\"这些数据由\",{\"1\":{\"81\":1}}],[\"这些实验表明\",{\"1\":{\"83\":1}}],[\"这些指令可以是开放式的\",{\"1\":{\"78\":1}}],[\"这些指标共同构成了\",{\"1\":{\"39\":1}}],[\"这些点大致构成物体的骨架\",{\"1\":{\"69\":1}}],[\"这些点彼此之间的最小距离尽可能大\",{\"1\":{\"46\":1}}],[\"这些人离我太远了\",{\"1\":{\"49\":1}}],[\"这些功能标签仅用于构造问题和定位正确功能区域\",{\"1\":{\"21\":1}}],[\"这些功能标注是人工标注的\",{\"1\":{\"19\":1}}],[\"这是抽取式问答模型的局限性\",{\"1\":{\"255\":1}}],[\"这是毋庸置疑的\",{\"1\":{\"221\":1}}],[\"这是我们使用llm的人的职责\",{\"1\":{\"194\":1}}],[\"这是交叉熵损失\",{\"1\":{\"169\":1}}],[\"这是注意力权重矩阵的来源\",{\"1\":{\"125\":1}}],[\"这是一个一举多得\",{\"1\":{\"281\":1}}],[\"这是一个椅子\",{\"1\":{\"68\":1}}],[\"这是一项本质上的进步\",{\"1\":{\"281\":1}}],[\"这是一种全新的\",{\"1\":{\"281\":1}}],[\"这是一种数据增强的方式\",{\"1\":{\"108\":1}}],[\"这是一款在推理和通用任务上有显著提升的模型\",{\"1\":{\"278\":1}}],[\"这是首次尝试将大语言模型用于模型集成\",{\"1\":{\"82\":1}}],[\"这是实现\",{\"1\":{\"81\":1}}],[\"这是后续指令调优的基础\",{\"1\":{\"79\":1}}],[\"这是第一个系统性地将\",{\"1\":{\"78\":1}}],[\"这是因为在实际采集过程中\",{\"1\":{\"64\":1}}],[\"这是\",{\"1\":{\"33\":1,\"49\":1,\"78\":1,\"103\":1,\"198\":1,\"199\":1,\"200\":1}}],[\"这是训练中未曾遇到过的功能\",{\"1\":{\"22\":1}}],[\"这个数据集来源是这里\",{\"1\":{\"232\":1}}],[\"这个问题的答案\",{\"1\":{\"221\":1}}],[\"这个阶段的prompt中包含三部分内容\",{\"1\":{\"200\":1}}],[\"这个阶段的prompt中要包含分解问题的示例\",{\"1\":{\"200\":1}}],[\"这个阶段相当于在语言模型的词空间中\",{\"1\":{\"80\":1}}],[\"这个过程有点像\",{\"1\":{\"198\":1}}],[\"这个过程类似于在传统的卷积神经网络中如何处理图像的小区域\",{\"1\":{\"43\":1}}],[\"这个事情的性价比非常低\",{\"1\":{\"179\":1}}],[\"这个词\",{\"1\":{\"178\":1}}],[\"这个表达式其实是通过\",{\"1\":{\"169\":1}}],[\"这个衰减因子能够使得易分类的样本\",{\"1\":{\"169\":1}}],[\"这个矩阵表示的是每个\",{\"1\":{\"125\":1}}],[\"这个矩阵表示对点云所做的变换\",{\"1\":{\"64\":1}}],[\"这个维度上添加一维\",{\"1\":{\"111\":1}}],[\"这个就是额外添加的一个\",{\"1\":{\"110\":1}}],[\"这个token\",{\"1\":{\"110\":1}}],[\"这个变换矩阵是近似正交的\",{\"1\":{\"64\":1}}],[\"这个模型使用了\",{\"1\":{\"53\":1}}],[\"这个模块实现了\",{\"1\":{\"53\":1}}],[\"这个特征向量代表了这个局部区域的高维特征\",{\"1\":{\"49\":1}}],[\"这个操作被称为\",{\"1\":{\"49\":1}}],[\"这个函数的有一个输入参数\",{\"1\":{\"265\":1}}],[\"这个函数的作用是将输入的文本转化为对应的嵌入表示\",{\"1\":{\"93\":1}}],[\"这个函数的作用是从点云中找出每个查询点周围一定半径范围内的邻近点索引\",{\"1\":{\"49\":1}}],[\"这个函数的作用是从输入点云中\",{\"1\":{\"49\":1}}],[\"这个函数实现的是根据给定的索引\",{\"1\":{\"49\":1}}],[\"这个函数实现的是最远点采样\",{\"1\":{\"49\":1}}],[\"这个\",{\"1\":{\"31\":1,\"93\":1}}],[\"这三个步骤构成了一个完整的跨模态融合流程\",{\"1\":{\"28\":1}}],[\"等架构的实现\",{\"1\":{\"291\":1}}],[\"等多个技术领域\",{\"1\":{\"288\":1}}],[\"等项目已经出现并受到关注\",{\"1\":{\"282\":1}}],[\"等基于语音对话的产品也非常受欢迎\",{\"1\":{\"282\":1}}],[\"等基于图像的方法\",{\"1\":{\"69\":1}}],[\"等全面对标\",{\"1\":{\"278\":1}}],[\"等功能\",{\"1\":{\"278\":1}}],[\"等合作研发的语言大模型\",{\"1\":{\"278\":1}}],[\"等技术\",{\"1\":{\"278\":1}}],[\"等先进技术\",{\"1\":{\"278\":1}}],[\"等商业闭源模型\",{\"1\":{\"278\":1}}],[\"等会被\",{\"1\":{\"255\":1}}],[\"等著名悲剧\",{\"1\":{\"253\":1}}],[\"等于\",{\"1\":{\"189\":1}}],[\"等人提出的\",{\"1\":{\"188\":1}}],[\"等价于\",{\"1\":{\"168\":1,\"170\":1}}],[\"等价于n个类别的cross\",{\"1\":{\"90\":1}}],[\"等强化学习算法\",{\"1\":{\"78\":1}}],[\"等指标更关注重合度\",{\"1\":{\"39\":1}}],[\"等\",{\"1\":{\"19\":1,\"20\":1,\"24\":2,\"35\":1,\"78\":1,\"85\":1,\"117\":1,\"226\":1,\"277\":2,\"278\":1}}],[\"提出的\",{\"1\":{\"278\":1}}],[\"提出的一种通过\",{\"1\":{\"78\":1}}],[\"提出并开源\",{\"1\":{\"223\":1}}],[\"提供可视化界面和性能分析工具\",{\"1\":{\"289\":1}}],[\"提供可扩展\",{\"1\":{\"289\":1}}],[\"提供基础架构和工具\",{\"1\":{\"289\":1}}],[\"提供了\",{\"1\":{\"291\":1}}],[\"提供了基础抽象和\",{\"1\":{\"289\":1}}],[\"提供了丰富的智能体和工具集合\",{\"1\":{\"288\":1}}],[\"提供了一系列强大的输出解析工具\",{\"1\":{\"288\":1}}],[\"提供了业界领先的调试和观测功能\",{\"1\":{\"288\":1}}],[\"提供了这些输入的简洁描述\",{\"1\":{\"209\":1}}],[\"提供的\",{\"1\":{\"286\":1}}],[\"提供的点云和功能区域标注构建\",{\"1\":{\"19\":1}}],[\"提供个性化体验\",{\"1\":{\"278\":1}}],[\"提前做好的假设\",{\"1\":{\"105\":1}}],[\"提示词工程\",{\"1\":{\"194\":1}}],[\"提示词\",{\"1\":{\"194\":1}}],[\"提示\",{\"1\":{\"92\":1}}],[\"提示调优\",{\"1\":{\"85\":2}}],[\"提高开发效率\",{\"1\":{\"289\":1}}],[\"提高了与外部系统集成的能力\",{\"1\":{\"278\":1}}],[\"提高了大型模型的推理效率\",{\"1\":{\"278\":2}}],[\"提高了模型的性能和效率\",{\"1\":{\"278\":1}}],[\"提高了模型的泛化性和稳健性\",{\"1\":{\"53\":1}}],[\"提高了模型在空间上的泛化能力\",{\"1\":{\"47\":1}}],[\"提高逐点判别能力\",{\"1\":{\"167\":1}}],[\"提高其中一个会降低另一个\",{\"1\":{\"155\":1}}],[\"提高分类阈值往往会减少假正例的数量并增加假负例的数量\",{\"1\":{\"155\":1}}],[\"提高训练的稳定性\",{\"1\":{\"108\":1}}],[\"提高算法鲁棒性\",{\"1\":{\"53\":1}}],[\"提高模型的性能和输出质量\",{\"1\":{\"283\":1}}],[\"提高模型的收敛速度和性能\",{\"1\":{\"278\":1}}],[\"提高模型的泛化能力\",{\"1\":{\"108\":1}}],[\"提高模型的泛化性能\",{\"1\":{\"53\":1}}],[\"提高模型对语言指令下功能区域的理解能力\",{\"1\":{\"35\":1}}],[\"提升了多语言能力\",{\"1\":{\"278\":1}}],[\"提升了模型的泛化能力和稳定性\",{\"1\":{\"62\":1}}],[\"提升监督模型的泛化能力\",{\"1\":{\"208\":1}}],[\"提升边缘识别精度\",{\"1\":{\"167\":1}}],[\"提升边界识别能力\",{\"1\":{\"35\":1}}],[\"提升模型在各种任务上的泛化能力\",{\"1\":{\"85\":1}}],[\"提升模型鲁棒性\",{\"1\":{\"64\":2}}],[\"提升泛化能力\",{\"1\":{\"23\":1}}],[\"提炼问题本质\",{\"1\":{\"20\":1}}],[\"提取输入图像的特征\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"提取语言模型损失\",{\"1\":{\"103\":1}}],[\"提取文本特征\",{\"1\":{\"91\":1}}],[\"提取的特征图转换为序列形式\",{\"1\":{\"117\":1}}],[\"提取的图像特征则是分类器的输入数据\",{\"1\":{\"91\":1}}],[\"提取的关键点集合\",{\"1\":{\"69\":1}}],[\"提取图像特征\",{\"1\":{\"81\":1,\"91\":1,\"104\":1}}],[\"提取图像和点云的特征\",{\"1\":{\"10\":1}}],[\"提取每条\",{\"1\":{\"80\":1}}],[\"提取每一点的特征向量\",{\"1\":{\"64\":1}}],[\"提取全局特征\",{\"1\":{\"67\":2}}],[\"提取更高维的特征\",{\"1\":{\"66\":1}}],[\"提取更高级的特征表示\",{\"1\":{\"30\":1}}],[\"提取邻近点的特征\",{\"1\":{\"57\":1}}],[\"提取局部特征\",{\"1\":{\"56\":1}}],[\"提取特征\",{\"1\":{\"55\":1,\"62\":1,\"68\":1,\"69\":2}}],[\"提取点云的层次化特征\",{\"1\":{\"50\":1}}],[\"提取这些区域的高维特征\",{\"1\":{\"49\":1}}],[\"提取这些局部区域中的点及其特征\",{\"1\":{\"49\":1}}],[\"提取\",{\"1\":{\"10\":2,\"226\":1}}],[\"yn\",{\"1\":{\"178\":1}}],[\"y2\",{\"1\":{\"178\":1}}],[\"y1\",{\"1\":{\"178\":1}}],[\"ylabel\",{\"1\":{\"107\":1}}],[\"yl3800\",{\"1\":{\"17\":1}}],[\"y=v\",{\"1\":{\"107\":1}}],[\"y\",{\"1\":{\"50\":2,\"71\":2,\"161\":1,\"169\":1,\"178\":1,\"255\":1}}],[\"yanx27\",{\"1\":{\"42\":1}}],[\"yaml\",{\"1\":{\"40\":3}}],[\"yawen\",{\"1\":{\"4\":1}}],[\"you\",{\"1\":{\"27\":1}}],[\"中文能力相对来说是非常不错的开源模型\",{\"1\":{\"278\":1}}],[\"中蒸馏出的六个\",{\"1\":{\"278\":1}}],[\"中速\",{\"1\":{\"278\":1}}],[\"中取出对应的\",{\"1\":{\"255\":1}}],[\"中取出被掩码位置的\",{\"1\":{\"226\":1}}],[\"中取值\",{\"1\":{\"227\":2}}],[\"中非常经典\",{\"1\":{\"223\":1}}],[\"中自己的值占大头也无所谓\",{\"1\":{\"221\":1}}],[\"中实际上会更加看重\",{\"1\":{\"221\":1}}],[\"中类型\",{\"1\":{\"221\":1}}],[\"中随机采样得到\",{\"1\":{\"211\":1}}],[\"中引入卷积操作\",{\"1\":{\"117\":1}}],[\"中进行进一步的处理\",{\"1\":{\"117\":1}}],[\"中第一个线性层把输入特征投影到一个更高维度的空间后\",{\"1\":{\"112\":1}}],[\"中加载数据时\",{\"1\":{\"107\":1}}],[\"中每个\",{\"1\":{\"101\":1}}],[\"中心\",{\"1\":{\"69\":1}}],[\"中心化\",{\"1\":{\"40\":1}}],[\"中用于约束变换矩阵接近正交性的正则化损失函数\",{\"1\":{\"65\":1}}],[\"中用于点云\",{\"1\":{\"57\":1}}],[\"中用于从点云中选择具有代表性的采样点的一种策略\",{\"1\":{\"49\":1}}],[\"中均能有效提取特征\",{\"1\":{\"52\":1}}],[\"中只有一组\",{\"1\":{\"49\":1}}],[\"中找到最大的那个距离对应的点\",{\"1\":{\"49\":2}}],[\"中\",{\"1\":{\"39\":1,\"65\":1,\"91\":2,\"98\":2,\"102\":1,\"103\":1,\"108\":1,\"110\":5,\"122\":1,\"123\":1,\"133\":1,\"147\":2,\"218\":1,\"226\":1,\"233\":1,\"253\":1,\"277\":1}}],[\"中提出\",{\"1\":{\"169\":1}}],[\"中提到\",{\"1\":{\"35\":1}}],[\"中提取信息\",{\"1\":{\"130\":1}}],[\"中提取与\",{\"1\":{\"100\":2}}],[\"中提取对应的点\",{\"1\":{\"49\":1}}],[\"中提取\",{\"1\":{\"16\":1,\"255\":1}}],[\"中的全连接层权重\",{\"1\":{\"226\":1}}],[\"中的全局特征学习\",{\"1\":{\"49\":1}}],[\"中的参数微调就可以了\",{\"1\":{\"221\":1}}],[\"中的不一样\",{\"1\":{\"219\":1}}],[\"中的目标函数模型后\",{\"1\":{\"208\":1}}],[\"中的值限制在区间\",{\"1\":{\"172\":1}}],[\"中的每个元素限制在\",{\"1\":{\"172\":1}}],[\"中的一个方法\",{\"1\":{\"254\":1}}],[\"中的一个函数\",{\"1\":{\"172\":1}}],[\"中的一个装饰器\",{\"1\":{\"107\":1}}],[\"中的样本取平均\",{\"1\":{\"166\":1}}],[\"中的图像一一对应\",{\"1\":{\"107\":1}}],[\"中的名词短语\",{\"1\":{\"80\":1}}],[\"中的指令调优思想引入多模态领域的研究\",{\"1\":{\"78\":1}}],[\"中的核心操作\",{\"1\":{\"72\":1}}],[\"中的核心模块\",{\"1\":{\"49\":1}}],[\"中的原始特征\",{\"1\":{\"55\":1}}],[\"中的\",{\"1\":{\"53\":1,\"122\":1,\"217\":1,\"219\":1}}],[\"中的语言引导\",{\"1\":{\"35\":1}}],[\"中的数据\",{\"1\":{\"33\":1}}],[\"中的空间卷积\",{\"1\":{\"30\":1}}],[\"中的自注意力机制\",{\"1\":{\"30\":1}}],[\"中筛选出同时满足\",{\"1\":{\"25\":1}}],[\"中选取了\",{\"1\":{\"20\":1}}],[\"中使用\",{\"1\":{\"15\":1,\"72\":1}}],[\"从deepseek\",{\"1\":{\"292\":1}}],[\"从构建一个\",{\"1\":{\"291\":1}}],[\"从实际业务需求出发构造小批量验证集\",{\"1\":{\"290\":1}}],[\"从一个模块流向另一个模块\",{\"1\":{\"286\":1}}],[\"从源张量中提取特定位置的元素\",{\"1\":{\"227\":1}}],[\"从bertencoders编码输出结果中提取出被掩码的位置对应的嵌入向量\",{\"1\":{\"226\":1}}],[\"从4开始\",{\"1\":{\"224\":1}}],[\"从本节开始\",{\"1\":{\"222\":1}}],[\"从本质上来说\",{\"1\":{\"43\":1}}],[\"从预训练到微调迁移学习过程中\",{\"1\":{\"213\":1}}],[\"从预测结果中找出每个样本预测概率最大的类别索引\",{\"1\":{\"114\":1}}],[\"从无标注文本中充分利用词级别以外的信息是有挑战性的\",{\"1\":{\"204\":1}}],[\"从上图中\",{\"1\":{\"200\":1}}],[\"从易至难技术\",{\"0\":{\"200\":1}}],[\"从结果集合中投票选择\",{\"1\":{\"199\":1}}],[\"从这可以看出要全参数微调大语言模型\",{\"1\":{\"189\":1}}],[\"从个人使用情况来说\",{\"1\":{\"188\":1}}],[\"从成本和效果的角度综合考虑\",{\"1\":{\"181\":1}}],[\"从训练数据的来源\",{\"1\":{\"180\":1}}],[\"从参数规模的角度\",{\"1\":{\"180\":1}}],[\"从字符级别开始\",{\"1\":{\"174\":1}}],[\"从对应的\",{\"1\":{\"130\":1}}],[\"从qkv张量中分离出查询\",{\"1\":{\"113\":1}}],[\"从图像的中心位置裁剪出\",{\"1\":{\"108\":1}}],[\"从数据库中检索相关信息\",{\"1\":{\"284\":1}}],[\"从数据集\",{\"1\":{\"107\":1}}],[\"从数据对的数量来看\",{\"1\":{\"90\":1}}],[\"从第二个开始\",{\"1\":{\"103\":1}}],[\"从冻结的llm引到vision\",{\"1\":{\"99\":1}}],[\"从冻结的image\",{\"1\":{\"99\":1}}],[\"从任务难度来看\",{\"1\":{\"96\":1}}],[\"从候选分类文本集合中取出其分类名词\",{\"1\":{\"93\":1}}],[\"从多个角度描述图像内容\",{\"1\":{\"80\":1}}],[\"从多个角度渲染点云或\",{\"1\":{\"71\":1}}],[\"从所有点中选出每个通道的最大响应值\",{\"1\":{\"64\":1,\"66\":1}}],[\"从稀疏点恢复到原始点密度\",{\"1\":{\"58\":1}}],[\"从下采样点中取出每个原始点对应的最近邻点的特征\",{\"1\":{\"57\":1}}],[\"从最稀疏的点开始\",{\"1\":{\"55\":1,\"56\":1}}],[\"从点云中选出有代表性的点作为中心点\",{\"1\":{\"55\":1}}],[\"从点云中根据索引提取特定点\",{\"1\":{\"49\":1}}],[\"从点云中找出最相关的功能区域\",{\"1\":{\"33\":1}}],[\"从输入点云\",{\"1\":{\"49\":1}}],[\"从输入点中选取一组点\",{\"1\":{\"45\":1}}],[\"从原始点云中选出\",{\"1\":{\"49\":1}}],[\"从而不断迭代优化\",{\"1\":{\"291\":1}}],[\"从而将传统的模型训练调优转变成了更简单\",{\"1\":{\"290\":1}}],[\"从而为用户提供更加流畅的体验\",{\"1\":{\"288\":1}}],[\"从而为后续的动态卷积和掩码预测提供基础\",{\"1\":{\"33\":1}}],[\"从而简化应用程序的开发流程\",{\"1\":{\"286\":1}}],[\"从而显著提升了回答的准确性与深度\",{\"1\":{\"283\":1}}],[\"从而显著提升了模型的性能\",{\"1\":{\"278\":1}}],[\"从而得出最终答案\",{\"1\":{\"280\":1}}],[\"从而在各种nlp\",{\"1\":{\"279\":1}}],[\"从而在注意力机制中就不会考虑到这些pad部分的token了\",{\"1\":{\"230\":1}}],[\"从而获得对语言深层次的理解\",{\"1\":{\"277\":1}}],[\"从而增强模型的表达能力\",{\"1\":{\"261\":1}}],[\"从而增强了模型的表达能力\",{\"1\":{\"112\":1}}],[\"从而可以解决长距离依赖的问题\",{\"1\":{\"260\":1}}],[\"从而很难并行\",{\"1\":{\"260\":1}}],[\"从而计算交叉熵损失就很简单了\",{\"1\":{\"227\":1}}],[\"从而模型训练学习到每个词的含义需要更大量的数据集且最终效果也不会很好\",{\"1\":{\"224\":1}}],[\"从而发现数据内部的逻辑与联系\",{\"1\":{\"192\":1}}],[\"从而能够训练更大的模型\",{\"1\":{\"192\":1}}],[\"从而能够更好地捕捉点云的局部结构和层次信息\",{\"1\":{\"69\":1}}],[\"从而应用于自己的业务场景\",{\"1\":{\"188\":1}}],[\"从而引导模型学习更多语义信息\",{\"1\":{\"169\":1}}],[\"从而减少对分类正确样本的贡献\",{\"1\":{\"169\":1}}],[\"从而减少计算复杂度并保持性能\",{\"1\":{\"30\":1}}],[\"从而实现对图像的细粒度理解和分析\",{\"1\":{\"164\":1}}],[\"从而实现点云的分层特征学习\",{\"1\":{\"49\":1}}],[\"从而提高了其理解和生成文本的能力\",{\"1\":{\"278\":1}}],[\"从而提高模型的分类性能\",{\"1\":{\"114\":1}}],[\"从而提升了在专业领域内的问题回答质量和深度\",{\"1\":{\"283\":1}}],[\"从而提升了用户对生成内容的信任度\",{\"1\":{\"283\":1}}],[\"从而提升其在各种任务上的泛化能力\",{\"1\":{\"78\":1}}],[\"从而提升模型对多尺度\",{\"1\":{\"28\":1}}],[\"从而完成图像分类任务\",{\"1\":{\"110\":1}}],[\"从而缓解了灾难性的遗忘问题\",{\"1\":{\"104\":1}}],[\"从而导致更大的训练代价\",{\"1\":{\"98\":1}}],[\"从而更贴近人类期望\",{\"1\":{\"78\":1}}],[\"从而保证变换是刚性的\",{\"1\":{\"64\":1}}],[\"从而插值得到该点的特征\",{\"1\":{\"57\":1}}],[\"从而训练网络在面对实际应用中可能遇到的各种采样密度时\",{\"1\":{\"53\":1}}],[\"从而学会根据输入数据的变化自适应地加权不同尺度上检测到的模式\",{\"1\":{\"52\":1}}],[\"从而形成更完整的语言上下文理解\",{\"1\":{\"33\":1}}],[\"从\",{\"0\":{\"216\":1,\"222\":1},\"1\":{\"16\":1,\"20\":1,\"25\":1,\"49\":1,\"64\":1,\"100\":2,\"211\":1,\"226\":1,\"255\":1,\"278\":1,\"286\":1}}],[\"解题过程可靠\",{\"1\":{\"278\":1}}],[\"解包数据\",{\"1\":{\"114\":1}}],[\"解包编码器输出的不同层级特征\",{\"1\":{\"16\":1}}],[\"解决子问题阶段\",{\"1\":{\"200\":1}}],[\"解决前景\",{\"1\":{\"169\":1}}],[\"解决模型训练时\",{\"1\":{\"169\":1}}],[\"解决数据集中\",{\"1\":{\"169\":1}}],[\"解决点云姿态不一致问题\",{\"1\":{\"64\":1}}],[\"解决了点云处理中的四大技术难点\",{\"1\":{\"62\":1}}],[\"解决了两个问题\",{\"1\":{\"43\":1}}],[\"解决方案\",{\"0\":{\"62\":1}}],[\"解组\",{\"1\":{\"28\":1}}],[\"解码成自然语言\",{\"1\":{\"255\":1}}],[\"解码成自然语言文本\",{\"1\":{\"255\":1}}],[\"解码生成的token\",{\"1\":{\"104\":1}}],[\"解码过程\",{\"1\":{\"56\":1}}],[\"解码过程中点特征的语言引导能力\",{\"1\":{\"28\":1}}],[\"解码\",{\"1\":{\"55\":1}}],[\"解码器层\",{\"1\":{\"269\":1}}],[\"解码器的最终输出通过一个线性层和\",{\"1\":{\"261\":1}}],[\"解码器进行文本生成\",{\"1\":{\"103\":1}}],[\"解码器生成文本描述\",{\"1\":{\"103\":1}}],[\"解码器部分\",{\"1\":{\"58\":1}}],[\"解码器\",{\"1\":{\"55\":2,\"56\":1,\"58\":1,\"261\":1,\"270\":1}}],[\"解码器与点云特征交互\",{\"1\":{\"33\":2}}],[\"解码器将这些查询与点云特征进行交互\",{\"1\":{\"27\":1}}],[\"解码器融合所有特征以预测可操作性特征\",{\"0\":{\"15\":1},\"1\":{\"10\":1}}],[\"解码结构提取多尺度点特征\",{\"1\":{\"27\":1}}],[\"概率保持不变\",{\"1\":{\"224\":2}}],[\"概率替换为随机词\",{\"1\":{\"224\":2}}],[\"概率替换为\",{\"1\":{\"224\":2}}],[\"概率值\",{\"1\":{\"16\":1}}],[\"概率分布\",{\"1\":{\"10\":1,\"68\":1}}],[\"执行项目初始化\",{\"1\":{\"289\":1}}],[\"执行具体行动计划至关重要\",{\"1\":{\"288\":1}}],[\"执行掩码的词的原token列表\",{\"1\":{\"225\":2}}],[\"执行掩码的词的位置列表\",{\"1\":{\"225\":2}}],[\"执行bert的掩码策略\",{\"1\":{\"224\":1}}],[\"执行微调训练\",{\"1\":{\"187\":1}}],[\"执行复杂的视觉推理任务\",{\"1\":{\"81\":1}}],[\"执行预测和可视化\",{\"1\":{\"40\":1}}],[\"执行\",{\"1\":{\"16\":1,\"33\":1}}],[\"执行交叉注意力机制\",{\"1\":{\"15\":1}}],[\"空间中的位置\",{\"1\":{\"71\":1}}],[\"空间变换网络\",{\"1\":{\"60\":1,\"62\":1}}],[\"空间是均匀和各向同性的\",{\"1\":{\"47\":1}}],[\"空间均匀性\",{\"1\":{\"47\":1}}],[\"空间混合\",{\"1\":{\"30\":1}}],[\"空间维度\",{\"1\":{\"16\":1}}],[\"空间特征\",{\"1\":{\"15\":1}}],[\"压缩时间\",{\"1\":{\"16\":1}}],[\"全新的范式\",{\"1\":{\"281\":1}}],[\"全能\",{\"1\":{\"278\":1}}],[\"全连接再加上一个softmax\",{\"1\":{\"263\":1}}],[\"全连接层预测变换矩阵\",{\"1\":{\"64\":1}}],[\"全部从维基百科的\",{\"1\":{\"223\":1}}],[\"全\",{\"1\":{\"219\":2}}],[\"全称为\",{\"1\":{\"217\":1}}],[\"全1\",{\"1\":{\"104\":1}}],[\"全局信息\",{\"1\":{\"68\":1}}],[\"全局信息融合机制\",{\"1\":{\"62\":1}}],[\"全局特征表示\",{\"1\":{\"110\":1}}],[\"全局特征不能很好地反映每个点的上下文\",{\"1\":{\"69\":1}}],[\"全局特征不能直接用于分割\",{\"1\":{\"68\":1}}],[\"全局特征只有一份\",{\"1\":{\"68\":1}}],[\"全局特征\",{\"1\":{\"66\":1}}],[\"全局特征开关\",{\"1\":{\"66\":1}}],[\"全局特征都不一样了\",{\"1\":{\"43\":1}}],[\"全局最大池化\",{\"1\":{\"64\":1}}],[\"全局质心点\",{\"1\":{\"49\":1}}],[\"全局平均池化层\",{\"1\":{\"16\":1}}],[\"全为\",{\"1\":{\"13\":1,\"103\":1}}],[\"多语言支持\",{\"1\":{\"279\":1}}],[\"多语言理解和创意生成方面有显著提升\",{\"1\":{\"278\":1}}],[\"多头潜在注意力\",{\"1\":{\"278\":1}}],[\"多头自注意力机制通过并行计算多个注意力头\",{\"1\":{\"261\":1}}],[\"多头自注意力机制\",{\"1\":{\"246\":1,\"261\":1}}],[\"多头自注意力计算流程图\",{\"1\":{\"244\":1,\"271\":1}}],[\"多头自注意力\",{\"0\":{\"113\":1,\"271\":1}}],[\"多头自注意力层\",{\"1\":{\"112\":1}}],[\"多项选择题\",{\"1\":{\"257\":1}}],[\"多项选择任务是指给定一个问题和多个候选答案\",{\"1\":{\"257\":1}}],[\"多项选择任务\",{\"0\":{\"257\":1}}],[\"多达千亿\",{\"1\":{\"178\":1}}],[\"多轮对话能力\",{\"1\":{\"79\":1}}],[\"多视角图像\",{\"1\":{\"71\":1}}],[\"多视角\",{\"1\":{\"69\":2}}],[\"多\",{\"1\":{\"55\":1}}],[\"多个应用可以只依赖于一个或少数几个大模型进行统一建设\",{\"1\":{\"281\":1}}],[\"多个\",{\"1\":{\"55\":2,\"226\":1}}],[\"多分辨率分组\",{\"0\":{\"54\":1},\"1\":{\"54\":1}}],[\"多尺度建模能力\",{\"1\":{\"69\":1}}],[\"多尺度聚合机制\",{\"1\":{\"69\":1}}],[\"多尺度特征提取机制\",{\"1\":{\"53\":1}}],[\"多尺度分组分类模型\",{\"0\":{\"53\":1}}],[\"多尺度分组\",{\"0\":{\"52\":1},\"1\":{\"51\":1,\"52\":1,\"53\":1}}],[\"多尺度上采样模块\",{\"1\":{\"16\":1}}],[\"多阈值下的\",{\"1\":{\"39\":1}}],[\"多阈值评估更稳定\",{\"1\":{\"39\":1}}],[\"多层感知机\",{\"1\":{\"30\":1}}],[\"多形状的功能区域的感知能力\",{\"1\":{\"28\":1}}],[\"多阶段编码\",{\"1\":{\"27\":1}}],[\"多模态支持\",{\"1\":{\"279\":1}}],[\"多模态融合\",{\"1\":{\"278\":1}}],[\"多模态能力\",{\"1\":{\"278\":1}}],[\"多模态因果自监督\",{\"1\":{\"103\":1}}],[\"多模态网络设计\",{\"1\":{\"98\":1}}],[\"多模态模型\",{\"1\":{\"278\":1}}],[\"多模态模型vit原理与图片分类实战演练\",{\"1\":{\"105\":1}}],[\"多模态模型在过往发展的过程中\",{\"1\":{\"98\":1}}],[\"多模态模型clip原理与图片分类\",{\"1\":{\"87\":1}}],[\"多模态\",{\"0\":{\"86\":1},\"1\":{\"278\":1}}],[\"多模态聊天机器人\",{\"1\":{\"81\":1}}],[\"多模态交叉注意力模块\",{\"1\":{\"15\":1}}],[\"多模态数据的\",{\"1\":{\"13\":1}}],[\"多模态嵌入\",{\"1\":{\"13\":2}}],[\"多模态特征投影到语言语义空间\",{\"0\":{\"12\":1},\"1\":{\"10\":1}}],[\"最重要的是\",{\"1\":{\"282\":1}}],[\"最显著的特征之一是它们的\",{\"1\":{\"280\":1}}],[\"最显著的点\",{\"1\":{\"69\":1}}],[\"最强性能\",{\"1\":{\"278\":1}}],[\"最强知识型\",{\"1\":{\"278\":1}}],[\"最早的\",{\"1\":{\"278\":1}}],[\"最快\",{\"1\":{\"278\":1}}],[\"最好要弄清楚为什么预训练模型会有效\",{\"1\":{\"213\":1}}],[\"最有说服力的证据就是到目前为止大量使用预训练的词嵌入来提升一系列nlp任务表现\",{\"1\":{\"204\":1}}],[\"最相关的视觉信息\",{\"1\":{\"100\":2}}],[\"最近的一些工作采用新的模型架构和预训练方法来从文本学习视觉特征\",{\"1\":{\"96\":1}}],[\"最具创新性的部分\",{\"1\":{\"78\":1}}],[\"最小可行性产品\",{\"1\":{\"291\":1}}],[\"最小生成长度\",{\"1\":{\"104\":1}}],[\"最小化语言模型输出与真实答案之间的交叉熵损失\",{\"1\":{\"81\":1}}],[\"最小化语言模型输出与真实答案之间的差异\",{\"1\":{\"80\":1}}],[\"最小池化\",{\"1\":{\"72\":1}}],[\"最小距离\",{\"1\":{\"49\":1}}],[\"最大序列长度\",{\"1\":{\"226\":1}}],[\"最大\",{\"1\":{\"211\":1}}],[\"最大的优点就是上述特定数据领域的表现会好很多\",{\"1\":{\"180\":1}}],[\"最大的resnet模型rn50x64需要在592个v100\",{\"1\":{\"90\":1}}],[\"最大生成长度\",{\"1\":{\"104\":1}}],[\"最大池化\",{\"1\":{\"72\":1}}],[\"最大文本长度\",{\"1\":{\"10\":1}}],[\"最后实现较好的泛化效果\",{\"1\":{\"290\":1}}],[\"最后形成完整的模型链路来解决整个业务逻辑\",{\"1\":{\"290\":1}}],[\"最后重新\",{\"1\":{\"257\":1}}],[\"最后返回两个列表\",{\"1\":{\"225\":1}}],[\"最后相加\",{\"1\":{\"189\":1}}],[\"最后得到当前句子对应的token列表\",{\"1\":{\"176\":1}}],[\"最后再经过一个线性层映射回原始维度\",{\"1\":{\"133\":1}}],[\"最后交换第\",{\"1\":{\"109\":1}}],[\"最后\",{\"1\":{\"93\":1,\"213\":1}}],[\"最后将结果转换为\",{\"1\":{\"93\":1}}],[\"最后将每个点的局部特征与整个点云的全局特征拼接起来\",{\"1\":{\"68\":1}}],[\"最后一层输出\",{\"1\":{\"66\":1}}],[\"最后一层抽象特征\",{\"1\":{\"58\":1}}],[\"最后一层使用\",{\"1\":{\"55\":1}}],[\"最后通过两个卷积层输出每个点的分类结果\",{\"1\":{\"58\":1}}],[\"最后通过全连接层完成分类任务\",{\"1\":{\"53\":1}}],[\"最后拼接结果\",{\"1\":{\"53\":1}}],[\"最后加上残差连接形成最终输出\",{\"1\":{\"31\":1}}],[\"最远点\",{\"1\":{\"49\":4}}],[\"最远点采样\",{\"1\":{\"49\":2,\"53\":1}}],[\"最多保留\",{\"1\":{\"49\":1}}],[\"最贴近实际应用需求\",{\"1\":{\"39\":1}}],[\"最终答案就是上下文中这两个位置之间的字符串\",{\"1\":{\"255\":1}}],[\"最终答案是大多数投票的结果\",{\"1\":{\"199\":1}}],[\"最终的训练效果也是一般\",{\"1\":{\"228\":1}}],[\"最终预测得\",{\"1\":{\"221\":1}}],[\"最终把复杂问题也解决了\",{\"1\":{\"200\":1}}],[\"最终解出了新的问题\",{\"1\":{\"198\":1}}],[\"最终生成一个既能表示常见单词\",{\"1\":{\"174\":1}}],[\"最终损失是\",{\"1\":{\"170\":1}}],[\"最终每个\",{\"1\":{\"130\":1,\"253\":1}}],[\"最终每个采样得到的关键点所在的局部领域\",{\"1\":{\"49\":1}}],[\"最终准确率达到\",{\"1\":{\"82\":1}}],[\"最终分类头\",{\"1\":{\"58\":1}}],[\"最终回到原始点数量\",{\"1\":{\"55\":1}}],[\"最终得到约\",{\"1\":{\"80\":1}}],[\"最终得到\",{\"1\":{\"49\":1}}],[\"最终取所有样本的\",{\"1\":{\"39\":1}}],[\"最终\",{\"1\":{\"35\":1,\"64\":1,\"91\":1,\"110\":2,\"169\":1,\"221\":1}}],[\"最终通过这些\",{\"1\":{\"33\":1}}],[\"最终通过卷积操作生成分割掩码\",{\"1\":{\"27\":1}}],[\"最终输出的维度是\",{\"1\":{\"127\":1}}],[\"最终输出\",{\"1\":{\"68\":1}}],[\"最终输出每个类别的概率分布\",{\"1\":{\"67\":1}}],[\"最终输出高维特征\",{\"1\":{\"66\":1}}],[\"最终输出与点的顺序无关\",{\"1\":{\"62\":1}}],[\"最终输出特征\",{\"1\":{\"57\":1}}],[\"最终输出就是\",{\"1\":{\"53\":1}}],[\"最终输出形状为\",{\"1\":{\"33\":1}}],[\"最终输出融合特征\",{\"1\":{\"30\":1}}],[\"最终输出头\",{\"1\":{\"16\":1}}],[\"最终使用\",{\"1\":{\"16\":1}}],[\"数学定义\",{\"1\":{\"72\":1,\"172\":1}}],[\"数组转换为\",{\"1\":{\"108\":2}}],[\"数组后\",{\"1\":{\"93\":1}}],[\"数组格式返回\",{\"1\":{\"93\":1}}],[\"数组\",{\"1\":{\"40\":1}}],[\"数据工程\",{\"1\":{\"290\":2}}],[\"数据连接\",{\"1\":{\"287\":1,\"288\":1}}],[\"数据存储等等\",{\"1\":{\"286\":1}}],[\"数据处理\",{\"1\":{\"285\":1}}],[\"数据处理阶段\",{\"1\":{\"284\":1}}],[\"数据分析与可视化\",{\"1\":{\"278\":1}}],[\"数据分析和第三方服务调用\",{\"1\":{\"278\":1}}],[\"数据预加载\",{\"1\":{\"224\":1}}],[\"数据预处理一般包括从多种格式向纯文本的转化\",{\"1\":{\"291\":1}}],[\"数据预处理\",{\"0\":{\"233\":1},\"1\":{\"40\":1,\"175\":1}}],[\"数据清洗\",{\"0\":{\"223\":1}}],[\"数据清洗与分词\",{\"1\":{\"211\":1}}],[\"数据安全的问题\",{\"1\":{\"179\":1}}],[\"数据的分布更加稀疏\",{\"1\":{\"112\":1}}],[\"数据下载\",{\"0\":{\"107\":1}}],[\"数据依赖性强\",{\"1\":{\"85\":1}}],[\"数据需求\",{\"1\":{\"85\":1}}],[\"数据均匀采样\",{\"1\":{\"81\":1}}],[\"数据构建方式\",{\"1\":{\"80\":1}}],[\"数据来源包括\",{\"1\":{\"78\":1}}],[\"数据路径设置\",{\"1\":{\"40\":1}}],[\"数据组织形式\",{\"1\":{\"25\":1}}],[\"数据增强与配对策略\",{\"0\":{\"23\":1}}],[\"数据划分方式\",{\"1\":{\"22\":1}}],[\"数据总量\",{\"1\":{\"22\":1}}],[\"数据集预处理完后\",{\"1\":{\"233\":1}}],[\"数据集预训练模型\",{\"1\":{\"211\":1}}],[\"数据集级别\",{\"1\":{\"169\":1}}],[\"数据集上进行预训练的\",{\"1\":{\"118\":1}}],[\"数据集上进行迁移学习\",{\"1\":{\"81\":1}}],[\"数据集加载代码\",{\"1\":{\"107\":1}}],[\"数据集下载\",{\"1\":{\"107\":1}}],[\"数据集的\",{\"1\":{\"82\":1}}],[\"数据集进行训练\",{\"1\":{\"81\":1}}],[\"数据集中每一个样本最终都会解析得到一个inputfeatures\",{\"1\":{\"233\":1}}],[\"数据集中图像的数量\",{\"1\":{\"107\":1}}],[\"数据集中\",{\"1\":{\"35\":1}}],[\"数据集类型\",{\"1\":{\"25\":1}}],[\"数据集存放目录\",{\"1\":{\"25\":1}}],[\"数据集初始化的核心代码实现如下\",{\"1\":{\"25\":1}}],[\"数据集统计信息\",{\"0\":{\"24\":1}}],[\"数据集设置\",{\"1\":{\"22\":1}}],[\"数据集组织方式\",{\"0\":{\"22\":1}}],[\"数据集基于\",{\"1\":{\"19\":1,\"26\":1}}],[\"数据集\",{\"0\":{\"6\":1,\"18\":1},\"1\":{\"4\":1,\"81\":1,\"98\":1}}],[\"数值可以是\",{\"1\":{\"21\":1}}],[\"数量一致\",{\"1\":{\"127\":1}}],[\"数量\",{\"1\":{\"16\":2,\"109\":1}}],[\"热图\",{\"1\":{\"16\":1}}],[\"并针对性分析\",{\"1\":{\"291\":1}}],[\"并针对性改进\",{\"1\":{\"291\":1}}],[\"并带有监控和日志功能\",{\"1\":{\"289\":1}}],[\"并行化操作和备选方案等高级功能\",{\"1\":{\"288\":1}}],[\"并行优化\",{\"1\":{\"69\":1}}],[\"并提供了定义工具的简便方法\",{\"1\":{\"288\":1}}],[\"并提升了整体的处理效率\",{\"1\":{\"283\":1}}],[\"并提取更高级别的局部特征\",{\"1\":{\"58\":1}}],[\"并以此为基础\",{\"1\":{\"283\":1}}],[\"并于\",{\"1\":{\"278\":1}}],[\"并结合\",{\"1\":{\"278\":1}}],[\"并结合正则化损失\",{\"1\":{\"64\":1}}],[\"并进一步提升了代码质量和多轮对话一致性\",{\"1\":{\"278\":1}}],[\"并进行l2归一化\",{\"1\":{\"90\":1}}],[\"并进行视觉对话的\",{\"1\":{\"81\":1}}],[\"并改进了工具调用和多模态能力\",{\"1\":{\"278\":1}}],[\"并改造为a\",{\"1\":{\"93\":1}}],[\"并充当通用任务求解器\",{\"1\":{\"278\":1}}],[\"并用\",{\"1\":{\"255\":1}}],[\"并用这些数据训练一个端到端的视觉语言模型\",{\"1\":{\"78\":1}}],[\"并重新命名为\",{\"1\":{\"232\":1}}],[\"并解压到当前目录下\",{\"1\":{\"223\":1}}],[\"并不知道\",{\"1\":{\"218\":1}}],[\"并不能保证这些矩阵是正交矩阵\",{\"1\":{\"65\":1}}],[\"并给出了提示transformer类模型和长距离依赖的文本数据集最好用这种方法来训练\",{\"1\":{\"214\":1}}],[\"并独立地处理\",{\"1\":{\"209\":1}}],[\"并采用两段式训练流程\",{\"1\":{\"204\":1}}],[\"并最终引导出示例问题的正确结果\",{\"1\":{\"198\":1}}],[\"并最终输出分类结果\",{\"1\":{\"50\":1}}],[\"并评估它们在验证集或测试集上的性能\",{\"1\":{\"173\":1}}],[\"并引入一个可调节的权重参数\",{\"1\":{\"172\":1}}],[\"并按行优先排序来实现\",{\"1\":{\"119\":1}}],[\"并累加到累计正确样本数中\",{\"1\":{\"114\":1}}],[\"并获得该批次图像列表对应的图像嵌入向量列表\",{\"1\":{\"93\":1}}],[\"并且允许语言模型与其所处的环境进行互动\",{\"1\":{\"286\":1}}],[\"并且是首个开源的推理型大模型\",{\"1\":{\"278\":1}}],[\"并且具有更好的泛化能力\",{\"1\":{\"278\":1}}],[\"并且使用预测结果计算nsp任务损失值\",{\"1\":{\"227\":1}}],[\"并且不会对额外的架构组件使用迁移学习\",{\"1\":{\"209\":1}}],[\"并且不需要低秩适应\",{\"1\":{\"192\":1}}],[\"并且\",{\"1\":{\"175\":1,\"189\":1}}],[\"并且取得了与卷积神经网络\",{\"1\":{\"119\":1}}],[\"并且与clip模型的训练数据不完全一致\",{\"1\":{\"92\":1}}],[\"并且在训练过程中采用了一个相对较大的批次大小\",{\"1\":{\"90\":1}}],[\"并且在\",{\"1\":{\"39\":1}}],[\"并计算与文本特征的余弦相似度\",{\"1\":{\"91\":1}}],[\"并将\",{\"1\":{\"290\":1}}],[\"并将它们组合成一个批次进行处理\",{\"1\":{\"257\":1}}],[\"并将它们组织称为局部区域集\",{\"1\":{\"47\":1}}],[\"并将这些图像块嵌入到一个低维向量空间中\",{\"1\":{\"109\":1}}],[\"并将图片展示出来\",{\"1\":{\"94\":1}}],[\"并将其标记为vit\",{\"1\":{\"90\":1}}],[\"并从多个选项中选择正确答案\",{\"1\":{\"81\":1}}],[\"并在结尾加上特殊标记\",{\"1\":{\"175\":1}}],[\"并在多种任务上表现良好\",{\"1\":{\"78\":1}}],[\"并在每个局部区域提取特征\",{\"1\":{\"55\":1}}],[\"并在此基础上强化自身的语义表达\",{\"1\":{\"29\":1}}],[\"并通过模型进行前向传播\",{\"1\":{\"114\":1}}],[\"并通过线性变换映射到嵌入空间\",{\"1\":{\"110\":1}}],[\"并通过局部+全局特征融合机制实现强大的点云建模能力\",{\"1\":{\"62\":1}}],[\"并通过一个小型\",{\"1\":{\"49\":1}}],[\"并通过多尺度上采样与融合\",{\"1\":{\"16\":1}}],[\"并集\",{\"1\":{\"35\":1,\"39\":1}}],[\"并取得不错的成效\",{\"1\":{\"105\":1}}],[\"并取\",{\"1\":{\"35\":1}}],[\"并仅保留\",{\"1\":{\"25\":1}}],[\"并生成\",{\"1\":{\"10\":1}}],[\"它让计算机更好地理解和使用语言\",{\"1\":{\"282\":1}}],[\"它在小型模型中不明显\",{\"1\":{\"280\":1}}],[\"它在编码每一词的时候都能够注意\",{\"1\":{\"260\":1}}],[\"它能够让语言模型与其他数据来源连接\",{\"1\":{\"286\":1}}],[\"它能够成功的两个关键点\",{\"1\":{\"278\":1}}],[\"它能够从每个子集中提取有用的信息或特征\",{\"1\":{\"43\":1}}],[\"它通过位置编码将序列中词的位置信息注入到输入中\",{\"1\":{\"261\":1}}],[\"它通过处理器对输入文本进行处理\",{\"1\":{\"93\":1}}],[\"它允许模型在处理每个词时关注输入序列中的所有词\",{\"1\":{\"261\":1}}],[\"它较难学习到长距离的依赖关系\",{\"1\":{\"260\":1}}],[\"它可以改进搜索引擎\",{\"1\":{\"282\":1}}],[\"它可以帮助计算机更好地理解和生成文本\",{\"1\":{\"282\":1}}],[\"它可以当成函数调用\",{\"1\":{\"265\":1}}],[\"它可以在指定维度上\",{\"1\":{\"227\":1}}],[\"它可按下式计算\",{\"1\":{\"157\":1}}],[\"它被证明在许多任务上有很强的表现\",{\"1\":{\"204\":1}}],[\"它得到的结果是错的\",{\"1\":{\"199\":1}}],[\"它并不保证结果的合理性和正确性\",{\"1\":{\"194\":1}}],[\"它只是在给定的信息的前提下\",{\"1\":{\"194\":1}}],[\"它们仍可能无法提供准确的答案\",{\"1\":{\"283\":1}}],[\"它们为自然语言理解和生成任务提供了强大的工具\",{\"1\":{\"279\":1}}],[\"它们的多语言能力使得跨文化和跨语言的应用变得更加容易\",{\"1\":{\"279\":1}}],[\"它们的能力依次递增\",{\"1\":{\"278\":1}}],[\"它们都是在数万亿个字符上训练的\",{\"1\":{\"278\":1}}],[\"它们都是按照其序列长度的20\",{\"1\":{\"227\":1}}],[\"它们都是被过度参数化的\",{\"1\":{\"184\":1}}],[\"它们在海量的文本数据上进行训练\",{\"1\":{\"277\":1}}],[\"它们在\",{\"1\":{\"223\":1}}],[\"它们有更小的内在维度\",{\"1\":{\"188\":1}}],[\"它们共同帮助我们判断模型是否真正理解语言引导下的功能区域语义\",{\"1\":{\"39\":1}}],[\"它基于jaccard指数\",{\"1\":{\"168\":1}}],[\"它基于前面的特征提取模块\",{\"1\":{\"67\":1,\"68\":1}}],[\"它来源于\",{\"1\":{\"166\":1}}],[\"它必须要在超大数据集上进行预训练\",{\"1\":{\"115\":1}}],[\"它位于最终分类头之前\",{\"1\":{\"114\":1}}],[\"它与类的实例和类本身都没有直接关联\",{\"1\":{\"107\":1}}],[\"它会将多个样本收集起来形成一个批次\",{\"1\":{\"107\":1}}],[\"它首次将nlp领域火热的transformer模型架构移植到了cv领域\",{\"1\":{\"105\":1}}],[\"它将权重矩阵量化为\",{\"1\":{\"192\":1}}],[\"它将\",{\"1\":{\"172\":1}}],[\"它将每个\",{\"1\":{\"103\":1}}],[\"它将输入的向量转换为\",{\"1\":{\"93\":1}}],[\"它由两个部分组成\",{\"1\":{\"91\":1}}],[\"它由大量带有位置信息的点组成\",{\"1\":{\"71\":1}}],[\"它比谷歌的jft\",{\"1\":{\"90\":1}}],[\"它代表着一种基于对比文本\",{\"1\":{\"89\":1}}],[\"它引入了局部区域搜索\",{\"1\":{\"69\":1}}],[\"它使用\",{\"1\":{\"67\":2}}],[\"它负责从输入点云中提取出可用于分类或分割的特征\",{\"1\":{\"66\":1}}],[\"它具有以下特点\",{\"1\":{\"64\":1}}],[\"它包含了这个区域内所有点的信息\",{\"1\":{\"49\":1}}],[\"它是用于执行bert\",{\"1\":{\"224\":1}}],[\"它是在lora的基础上\",{\"1\":{\"185\":1}}],[\"它是一种用于度量集合之间相似性的指标\",{\"1\":{\"170\":1}}],[\"它是一个非常强大且灵活的张量操作函数\",{\"1\":{\"33\":1}}],[\"它是\",{\"1\":{\"115\":1,\"277\":1}}],[\"它是无序点云数据特征提取的高效算法\",{\"1\":{\"43\":1}}],[\"它常用于图像检索\",{\"1\":{\"39\":1}}],[\"它结合了两种\",{\"1\":{\"167\":1}}],[\"它结合了\",{\"1\":{\"35\":1}}],[\"它的作用是给定一个完整的句子\",{\"1\":{\"255\":1}}],[\"它的作用是负责从输入的点云数据中采样关键点\",{\"1\":{\"49\":1}}],[\"它的语法是\",{\"1\":{\"254\":1}}],[\"它的应用自不必提\",{\"1\":{\"192\":1}}],[\"它的更新可表示为\",{\"1\":{\"189\":1}}],[\"它的目标是为各种大型语言模型应用提供通用接口\",{\"1\":{\"286\":1}}],[\"它的目标是\",{\"1\":{\"64\":1}}],[\"它的核心思想是通过不断合并最常见的字符对来构建一个高效的词汇表\",{\"1\":{\"174\":1}}],[\"它的核心思想是\",{\"1\":{\"49\":1}}],[\"它的本质是\",{\"1\":{\"33\":1}}],[\"它的主要目标是\",{\"1\":{\"33\":1}}],[\"它不具有生成能力\",{\"1\":{\"255\":1}}],[\"它不会\",{\"1\":{\"255\":1}}],[\"它不是单纯地\",{\"1\":{\"78\":1}}],[\"它不是直接包含原始图像\",{\"1\":{\"11\":1}}],[\"它不使用任何注意力机制\",{\"1\":{\"30\":1}}],[\"它接收来自编码器和解码器的特征\",{\"1\":{\"16\":1}}],[\"模式\",{\"1\":{\"107\":1}}],[\"模态融合比较轻量\",{\"1\":{\"98\":1}}],[\"模态融合比较弱\",{\"1\":{\"98\":1}}],[\"模块可以看作是一个特征预处理模块\",{\"1\":{\"114\":1}}],[\"模块\",{\"1\":{\"28\":1,\"98\":1,\"117\":1}}],[\"模块的前向传播\",{\"1\":{\"16\":1}}],[\"模块用于最终的\",{\"1\":{\"16\":1}}],[\"模型输入\",{\"1\":{\"287\":1}}],[\"模型输出应不受刚性变换影响\",{\"1\":{\"61\":1}}],[\"模型输出的预测结果\",{\"1\":{\"170\":1}}],[\"模型输出的预测值\",{\"1\":{\"166\":1}}],[\"模型输出的\",{\"1\":{\"167\":1}}],[\"模型输出的概率值或二值化结果\",{\"1\":{\"168\":1}}],[\"模型输出的概率值大于某个值时\",{\"1\":{\"155\":1}}],[\"模型输出的概率值\",{\"1\":{\"39\":1,\"167\":1,\"172\":1}}],[\"模型输出的原始\",{\"1\":{\"35\":1,\"168\":1,\"169\":1}}],[\"模型等\",{\"1\":{\"287\":1}}],[\"模型学习特定领域的数据有助于减少幻觉\",{\"1\":{\"285\":1}}],[\"模型学会将图像中与分类任务相关的信息汇聚到\",{\"1\":{\"110\":1}}],[\"模型学会如何通过注意力机制将图像的有效信息汇聚到\",{\"1\":{\"110\":2}}],[\"模型定制\",{\"1\":{\"285\":1}}],[\"模型也采用了\",{\"1\":{\"278\":1}}],[\"模型还使用了高效的数据并行和流水线并行技术\",{\"1\":{\"278\":1}}],[\"模型使用了大规模的数据过滤和清洗技术\",{\"1\":{\"278\":1}}],[\"模型中\",{\"1\":{\"278\":1}}],[\"模型中常用的特殊\",{\"1\":{\"224\":1}}],[\"模型是典型的\",{\"1\":{\"278\":1}}],[\"模型是过参数化的\",{\"1\":{\"188\":1}}],[\"模型展现出了一些惊人的能力\",{\"1\":{\"277\":1}}],[\"模型会对每个选项分别编码\",{\"1\":{\"257\":1}}],[\"模型会输出两个数\",{\"1\":{\"221\":1}}],[\"模型代表\",{\"1\":{\"255\":1}}],[\"模型代码解读与复现\",{\"0\":{\"7\":1,\"17\":1}}],[\"模型代码解读\",{\"0\":{\"4\":1}}],[\"模型那样逐词生成新内容\",{\"1\":{\"255\":1}}],[\"模型架构图\",{\"1\":{\"261\":1}}],[\"模型架构\",{\"0\":{\"234\":1,\"261\":1}}],[\"模型有点大\",{\"1\":{\"232\":1}}],[\"模型返回的logits\",{\"1\":{\"227\":1}}],[\"模型整体实现也比较简单\",{\"1\":{\"226\":1}}],[\"模型很难学到它们的语义表示\",{\"1\":{\"224\":1}}],[\"模型要判断出这个假设是\",{\"1\":{\"221\":1}}],[\"模型要理解并遵循人类语言中的任务描述\",{\"1\":{\"85\":1}}],[\"模型在\",{\"1\":{\"278\":1}}],[\"模型在编码\",{\"1\":{\"218\":1}}],[\"模型在训练期间学会了抓取包和杯子\",{\"1\":{\"22\":1}}],[\"模型层\",{\"0\":{\"215\":1}}],[\"模型能从无标记数据中充分利用语义信息\",{\"1\":{\"204\":1}}],[\"模型能够学习到丰富的图像特征和模式\",{\"1\":{\"118\":1}}],[\"模型主要依赖于这个低的内在维度\",{\"1\":{\"188\":1}}],[\"模型更不喜欢\",{\"1\":{\"170\":2}}],[\"模型更新参数\",{\"1\":{\"110\":1}}],[\"模型对真实类别的预测概率\",{\"1\":{\"169\":1}}],[\"模型分类错误\",{\"1\":{\"169\":1}}],[\"模型已自信分类\",{\"1\":{\"169\":1}}],[\"模型已经具备基本的视觉理解能力\",{\"1\":{\"80\":1}}],[\"模型被大量简单负样本主导\",{\"1\":{\"169\":1}}],[\"模型被强制学会将图像的有效信息汇聚到\",{\"1\":{\"110\":1}}],[\"模型预测出的功能区域\",{\"1\":{\"166\":1}}],[\"模型预测该点属于功能区域的概率\",{\"1\":{\"39\":1}}],[\"模型将随机选择的正例正确排在随机选择的负例之上的概率为\",{\"1\":{\"160\":1}}],[\"模型将正例排在负例之上的概率\",{\"1\":{\"160\":1}}],[\"模型将完全依赖随机初始化的参数去\",{\"1\":{\"33\":1}}],[\"模型才会将该样本分类为正类\",{\"1\":{\"155\":1}}],[\"模型提取图像的特征图\",{\"1\":{\"117\":1}}],[\"模型自动学习到了如果注意画面中的分类主体\",{\"1\":{\"116\":1}}],[\"模型计算每个\",{\"1\":{\"110\":1}}],[\"模型难以计算\",{\"1\":{\"109\":1}}],[\"模型名称\",{\"1\":{\"95\":1,\"278\":4}}],[\"模型通过在输入前后插入一些可训练的提示词来\",{\"1\":{\"85\":1}}],[\"模型规模对性能有显著影响\",{\"1\":{\"83\":1}}],[\"模型训练的第二阶段\",{\"1\":{\"81\":1}}],[\"模型训练的第一阶段\",{\"1\":{\"80\":1}}],[\"模型可以更好地理解和生成图文结合的内容\",{\"1\":{\"80\":1}}],[\"模型尝试生成尽可能高奖励的回答\",{\"1\":{\"78\":1}}],[\"模型\",{\"0\":{\"226\":1},\"1\":{\"71\":1,\"83\":1,\"90\":1,\"167\":1,\"178\":1,\"226\":1,\"255\":1,\"278\":9,\"287\":1}}],[\"模型为\",{\"1\":{\"71\":1}}],[\"模型表现良好\",{\"1\":{\"69\":1}}],[\"模型必须对输入点的排列顺序不敏感\",{\"1\":{\"61\":1}}],[\"模型需要从中选择最合适的答案\",{\"1\":{\"257\":1}}],[\"模型需要为每个点预测一个类别标签\",{\"1\":{\"55\":1}}],[\"模型需要根据自然语言问题识别点云中最相关的功能区域\",{\"1\":{\"35\":1}}],[\"模型需要根据自然语言问题\",{\"1\":{\"28\":1}}],[\"模型推理\",{\"1\":{\"40\":1}}],[\"模型初始化\",{\"1\":{\"37\":1}}],[\"模型的输入不再使用位置编码\",{\"1\":{\"278\":1}}],[\"模型的基本原则是通过语言建模将世界知识压缩到仅解码器\",{\"1\":{\"278\":1}}],[\"模型的具体配置\",{\"1\":{\"211\":1}}],[\"模型的认知\",{\"1\":{\"192\":1}}],[\"模型的参数是根据这个特定尺寸的输入数据进行优化和学习的\",{\"1\":{\"108\":1}}],[\"模型的训练过程大体分为了\",{\"1\":{\"36\":1}}],[\"模型的监督信号\",{\"1\":{\"35\":1}}],[\"模型变体\",{\"1\":{\"32\":1}}],[\"模型目标\",{\"1\":{\"27\":1}}],[\"模型实现\",{\"0\":{\"27\":1}}],[\"模型结构与原始\",{\"1\":{\"78\":1}}],[\"模型结构图\",{\"1\":{\"9\":1}}],[\"模型结构\",{\"0\":{\"9\":1,\"44\":1,\"99\":1},\"1\":{\"80\":1,\"81\":1}}],[\"第四范式\",{\"1\":{\"185\":1}}],[\"第三步\",{\"1\":{\"55\":1}}],[\"第500个样本\",{\"1\":{\"40\":1}}],[\"第二步\",{\"1\":{\"55\":1}}],[\"第二个全连接层\",{\"1\":{\"112\":1}}],[\"第二个归一化层\",{\"1\":{\"112\":1}}],[\"第二个线性层再把高维特征映射回原来的维度\",{\"1\":{\"112\":1}}],[\"第二个参数\",{\"1\":{\"108\":1}}],[\"第二个\",{\"1\":{\"15\":1}}],[\"第二分支\",{\"1\":{\"15\":1}}],[\"第一阶段设计了三个训练目标\",{\"1\":{\"100\":1}}],[\"第一层是一个叫做\",{\"1\":{\"64\":1}}],[\"第一步\",{\"1\":{\"55\":1}}],[\"第一个关键点\",{\"1\":{\"196\":1}}],[\"第一个全连接层\",{\"1\":{\"112\":1}}],[\"第一个归一化层\",{\"1\":{\"112\":1}}],[\"第一个线性层将输入特征映射到更高维度的空间\",{\"1\":{\"112\":1}}],[\"第一个参数\",{\"1\":{\"108\":1}}],[\"第一个向量提供了更细致的信息\",{\"1\":{\"54\":1}}],[\"第一个点\",{\"1\":{\"49\":1}}],[\"第一个\",{\"1\":{\"15\":1}}],[\"第一分支\",{\"1\":{\"15\":1}}],[\"来提升系统效果\",{\"1\":{\"291\":1}}],[\"来提升应用性能\",{\"1\":{\"291\":1}}],[\"来提升模型在特定任务上的表现\",{\"1\":{\"285\":1}}],[\"来满足验证集效果\",{\"1\":{\"290\":1}}],[\"来解决任务\",{\"1\":{\"290\":1}}],[\"来替代子模型的训练调优\",{\"1\":{\"290\":1}}],[\"来开发基于大型语言模型的应用程序\",{\"1\":{\"286\":1}}],[\"来理解语言\",{\"1\":{\"277\":1}}],[\"来限制语言模型的输出分布只有\",{\"1\":{\"213\":1}}],[\"来正则化\",{\"1\":{\"211\":1}}],[\"来最终得到复杂问题的结果\",{\"1\":{\"200\":1}}],[\"来引导llm展现出更好的推理能力\",{\"1\":{\"198\":1}}],[\"来说\",{\"1\":{\"189\":1}}],[\"来间接训练神经网络中的一些密集层\",{\"1\":{\"188\":1}}],[\"来减少模型对于计算资源的需求的方法\",{\"1\":{\"185\":1}}],[\"来命名环境\",{\"1\":{\"140\":1}}],[\"来帮助大家梳理清楚vision\",{\"1\":{\"106\":1}}],[\"来适配不同类别\",{\"1\":{\"85\":1}}],[\"来生成图文结合的指令响应对\",{\"1\":{\"78\":1}}],[\"来实现大语言模型的控制\",{\"1\":{\"290\":1}}],[\"来实现数据的压缩\",{\"1\":{\"192\":1}}],[\"来实现\",{\"1\":{\"78\":1}}],[\"来实现全局信息建模\",{\"1\":{\"30\":1}}],[\"来完成点云物体分割任务\",{\"1\":{\"68\":1}}],[\"来完成点云分类任务\",{\"1\":{\"67\":1}}],[\"来模拟不同的采样密度\",{\"1\":{\"53\":1}}],[\"来学习优化的策略\",{\"1\":{\"52\":1}}],[\"来构建局部区域点集\",{\"1\":{\"45\":1}}],[\"来表达各种线性代数运算\",{\"1\":{\"33\":1}}],[\"来更新分组特征\",{\"1\":{\"30\":1}}],[\"来自下一级的特征\",{\"1\":{\"54\":1}}],[\"来自论文的理论分析\",{\"1\":{\"69\":1}}],[\"来自论文\",{\"1\":{\"39\":1}}],[\"来自论文图3\",{\"0\":{\"24\":1}}],[\"来自\",{\"1\":{\"16\":2,\"24\":1,\"55\":1,\"80\":1}}],[\"来源之二\",{\"1\":{\"15\":1}}],[\"来源之一\",{\"1\":{\"15\":1}}],[\"来源\",{\"1\":{\"15\":1}}],[\"来标记哪些多模态\",{\"1\":{\"13\":1}}],[\"→\",{\"0\":{\"128\":1},\"1\":{\"15\":3,\"16\":4,\"39\":1,\"55\":1,\"56\":9,\"58\":4,\"62\":4,\"64\":4,\"137\":4,\"169\":4,\"170\":4,\"224\":1}}],[\"投影层的丢弃率\",{\"1\":{\"113\":1}}],[\"投影层\",{\"1\":{\"80\":2,\"81\":2}}],[\"投影\",{\"1\":{\"15\":5}}],[\"投影维度\",{\"1\":{\"15\":1}}],[\"调试和测试\",{\"1\":{\"289\":1}}],[\"调节因子\",{\"1\":{\"169\":1}}],[\"调整的过程\",{\"1\":{\"194\":1}}],[\"调整为适合下游任务的\",{\"1\":{\"187\":1}}],[\"调整维度\",{\"1\":{\"58\":1}}],[\"调整输出格式为\",{\"1\":{\"15\":1}}],[\"调整\",{\"1\":{\"15\":1}}],[\"调用函数或工具\",{\"1\":{\"288\":1}}],[\"调用q\",{\"1\":{\"104\":1}}],[\"调用\",{\"1\":{\"10\":1,\"68\":1,\"103\":2}}],[\"都可以提供对应的解决方案\",{\"1\":{\"286\":1}}],[\"都使用了残差连接和层归一化\",{\"1\":{\"261\":1}}],[\"都单独构造一个\",{\"1\":{\"257\":1}}],[\"都等于max\",{\"1\":{\"233\":1}}],[\"都无所谓\",{\"1\":{\"218\":1}}],[\"都预测为负类别\",{\"1\":{\"152\":1}}],[\"都是有足够资源的来开发大模型\",{\"1\":{\"188\":1}}],[\"都是有效的\",{\"1\":{\"13\":1}}],[\"都是在寻找最相关的\",{\"1\":{\"130\":1}}],[\"都是197\",{\"1\":{\"112\":1}}],[\"都需要进行有监督微调\",{\"1\":{\"96\":1}}],[\"都有一个对应的\",{\"1\":{\"253\":1}}],[\"都有一个低维的本质模型\",{\"1\":{\"184\":1}}],[\"都有\",{\"1\":{\"72\":1}}],[\"都有效\",{\"1\":{\"13\":1}}],[\"都尝试引入更复杂的结构来提升建模能力\",{\"1\":{\"69\":1}}],[\"都会乘以相同的输入\",{\"1\":{\"189\":1}}],[\"都会对输入序列的长度有限制\",{\"1\":{\"179\":1}}],[\"都会被压缩为一个固定长度的特征向量\",{\"1\":{\"49\":1}}],[\"都会基于其语言语义\",{\"1\":{\"33\":1}}],[\"都与所有点\",{\"1\":{\"33\":1}}],[\"都带有原始语言上下文\",{\"1\":{\"33\":1}}],[\"则将其设置为\",{\"1\":{\"254\":2}}],[\"则将其转换为\",{\"1\":{\"109\":1}}],[\"则生成的pad\",{\"1\":{\"230\":1}}],[\"则\",{\"1\":{\"189\":1,\"190\":1}}],[\"则它的参数量\",{\"1\":{\"189\":1}}],[\"则在这里激活\",{\"1\":{\"168\":1,\"169\":1}}],[\"则应注释掉这一行\",{\"1\":{\"167\":1,\"168\":1}}],[\"则需要更新预训练模型参数\",{\"1\":{\"189\":1}}],[\"则需要在这里激活\",{\"1\":{\"166\":1}}],[\"则需要使用不同的工具\",{\"1\":{\"158\":1}}],[\"则点\",{\"1\":{\"162\":1}}],[\"则可能有必要选择\",{\"1\":{\"162\":1}}],[\"则可以用\",{\"1\":{\"159\":1}}],[\"则其\",{\"1\":{\"160\":1}}],[\"则其假正例数为零\",{\"1\":{\"155\":1}}],[\"则其准确性得分为\",{\"1\":{\"152\":1}}],[\"则最好改为针对其他指标进行优化\",{\"1\":{\"152\":1}}],[\"则表示数据集不平衡\",{\"1\":{\"151\":1}}],[\"则表示图像块是正方形\",{\"1\":{\"109\":1}}],[\"则表示图像是正方形\",{\"1\":{\"109\":1}}],[\"则会得到以下表格\",{\"1\":{\"151\":1}}],[\"则创建线性分类头\",{\"1\":{\"114\":1}}],[\"则使用unk替换\",{\"1\":{\"224\":1}}],[\"则使用\",{\"1\":{\"114\":1}}],[\"则使用默认的\",{\"1\":{\"114\":1}}],[\"则使用默认的缩放因子\",{\"1\":{\"113\":1}}],[\"则使用该层进行归一化\",{\"1\":{\"109\":2}}],[\"则默认为\",{\"1\":{\"112\":2}}],[\"则默认多模态\",{\"1\":{\"13\":1}}],[\"则对图像进行处理\",{\"1\":{\"107\":1}}],[\"则对整个点云做全局特征提取\",{\"1\":{\"49\":1}}],[\"则计算当前层bertlayer时\",{\"1\":{\"103\":1}}],[\"则q来自query\",{\"1\":{\"103\":1}}],[\"则自动生成\",{\"1\":{\"102\":1}}],[\"则采用了两种不同的架构\",{\"1\":{\"90\":1}}],[\"则是以文本作为监督信号\",{\"1\":{\"88\":1}}],[\"则返回所有样本损失的平均值\",{\"1\":{\"170\":1,\"172\":1}}],[\"则返回\",{\"1\":{\"66\":2}}],[\"则返回该问题文本\",{\"1\":{\"25\":1}}],[\"则拼接起来\",{\"1\":{\"57\":1}}],[\"则保存\",{\"1\":{\"39\":1}}],[\"则从问题1～15中随机选择一个问题\",{\"1\":{\"25\":1}}],[\"则进行额外处理\",{\"1\":{\"10\":1}}],[\"剩余的语言\",{\"1\":{\"13\":1}}],[\"有较好的可解释性和可追踪性\",{\"1\":{\"285\":1}}],[\"有限的数据集可能无法显著提高性能\",{\"1\":{\"285\":1}}],[\"有时会产生与客观事实不符的信息\",{\"1\":{\"283\":1}}],[\"有时也会加入一个平滑项\",{\"1\":{\"166\":1}}],[\"有标签数据\",{\"1\":{\"279\":1}}],[\"有一点不同就是bert预训练阶段的学习目标是\",{\"1\":{\"227\":1}}],[\"有了分词器后\",{\"1\":{\"225\":1}}],[\"有了以上\",{\"1\":{\"32\":1}}],[\"有的问题就是没有答案的\",{\"1\":{\"221\":1}}],[\"有的是不同义的\",{\"1\":{\"212\":1}}],[\"有的是同义的\",{\"1\":{\"212\":1}}],[\"有\",{\"1\":{\"208\":1,\"218\":3,\"277\":1}}],[\"有监督微调\",{\"0\":{\"208\":1}}],[\"有两个主要原因\",{\"1\":{\"204\":1}}],[\"有2个是正确的\",{\"1\":{\"199\":1}}],[\"有1个是错的\",{\"1\":{\"199\":1}}],[\"有几个注意点\",{\"1\":{\"198\":1}}],[\"有没有低成本的方法微调大模型\",{\"1\":{\"188\":1}}],[\"有没有办法能够减少\",{\"1\":{\"187\":1}}],[\"有没有覆盖正确区域\",{\"1\":{\"166\":1}}],[\"有以下一些原因\",{\"1\":{\"179\":1}}],[\"有以下几个优点\",{\"1\":{\"168\":1}}],[\"有多少点被正确分类\",{\"1\":{\"166\":1}}],[\"有多少组点云\",{\"1\":{\"64\":1}}],[\"有病的人不能诊断为健康\",{\"1\":{\"156\":1}}],[\"有助于迁移\",{\"1\":{\"213\":1}}],[\"有助于模型学习到不同方向的特征\",{\"1\":{\"108\":1}}],[\"有助于加快收敛\",{\"1\":{\"83\":1}}],[\"有些位置被标记为\",{\"1\":{\"49\":1}}],[\"有效突破了输入长度的限制\",{\"1\":{\"283\":1}}],[\"有效地从无标记的原始文本中学习的能力能减轻对监督学习的依赖\",{\"1\":{\"204\":1}}],[\"有效语言部分\",{\"1\":{\"13\":1}}],[\"有效的部分\",{\"1\":{\"13\":1}}],[\"有问题需要咨询的小伙伴\",{\"1\":{\"2\":1}}],[\"非线性替换为\",{\"1\":{\"278\":1}}],[\"非线性增强和空间对齐\",{\"1\":{\"11\":2}}],[\"非功能区域\",{\"1\":{\"169\":1}}],[\"非目标点\",{\"1\":{\"169\":1}}],[\"非目标区域匹配度\",{\"1\":{\"35\":1}}],[\"非常适合比较模型\",{\"1\":{\"161\":1}}],[\"非常高\",{\"1\":{\"98\":2}}],[\"非垃圾邮件被正确分类为非垃圾邮件\",{\"1\":{\"151\":1}}],[\"非垃圾邮件被误分类为垃圾邮件\",{\"1\":{\"151\":1}}],[\"非均匀\",{\"1\":{\"73\":1}}],[\"非均匀密度下稳定的特征学习\",{\"0\":{\"51\":1}}],[\"非刚性运动\",{\"1\":{\"73\":1}}],[\"非刚性变形\",{\"1\":{\"69\":1}}],[\"非结构化\",{\"1\":{\"71\":1}}],[\"非\",{\"1\":{\"13\":1,\"103\":1,\"147\":1}}],[\"供\",{\"1\":{\"13\":1}}],[\"即要开发的应用的应用场景\",{\"1\":{\"291\":1}}],[\"即后训练阶段的强化学习\",{\"1\":{\"277\":1}}],[\"即忽略掉pad部分的损失计算\",{\"1\":{\"227\":2}}],[\"即给定一个前提\",{\"1\":{\"221\":1}}],[\"即给定input\",{\"1\":{\"103\":1}}],[\"即完型填空\",{\"1\":{\"217\":1}}],[\"即便是那些有大量标注数据的场景\",{\"1\":{\"204\":1}}],[\"即可\",{\"1\":{\"189\":1,\"221\":1}}],[\"即将原文本按照空格\",{\"1\":{\"175\":1}}],[\"即模型对真实类别的预测概率\",{\"1\":{\"169\":1}}],[\"即注意力得分\",{\"1\":{\"125\":1}}],[\"即网格大小的乘积\",{\"1\":{\"109\":1}}],[\"即图像在水平和垂直方向上分别可以划分的图像块数量\",{\"1\":{\"109\":1}}],[\"即图片上相邻的区域具有相似的特征\",{\"1\":{\"105\":1}}],[\"即一种先验知识\",{\"1\":{\"105\":1}}],[\"即文本和图像可能不完全匹配\",{\"1\":{\"96\":1}}],[\"即基于对比学习的方法\",{\"1\":{\"96\":1}}],[\"即基于文本弱监督来提升性能\",{\"1\":{\"96\":1}}],[\"即为与当前文本描述相似度最高的那副图片\",{\"1\":{\"94\":1}}],[\"即真正属于一对的文本和图像\",{\"1\":{\"90\":1}}],[\"即上图中橙色和黄色的向量\",{\"1\":{\"221\":1}}],[\"即上图所示的矩阵\",{\"1\":{\"90\":1}}],[\"即上一层级\",{\"1\":{\"54\":1}}],[\"即只改变物体的方向而不改变形状和大小\",{\"1\":{\"65\":1}}],[\"即只依赖一小部分关键点就能判断整体形状\",{\"1\":{\"62\":1}}],[\"即使是一维位置编码\",{\"1\":{\"111\":1}}],[\"即使其他点都在\",{\"1\":{\"69\":1}}],[\"即使\",{\"1\":{\"62\":1,\"162\":1}}],[\"即使丢失一些点或加入异常点\",{\"1\":{\"62\":1}}],[\"即对点顺序不敏感\",{\"1\":{\"62\":1}}],[\"即根据最近的几个邻近点的距离进行加权平均\",{\"1\":{\"55\":1}}],[\"即随机移除一部分输入点\",{\"1\":{\"53\":1}}],[\"即每个图像块经过卷积操作后得到的特征向量的维度\",{\"1\":{\"109\":1}}],[\"即每个点的各个类别得分\",{\"1\":{\"68\":1}}],[\"即每个点都有一个类别预测\",{\"1\":{\"58\":1}}],[\"即每个点对应的\",{\"1\":{\"10\":1}}],[\"即每个查询点有一个特征向量\",{\"1\":{\"49\":1}}],[\"即每个局部邻域内的点数量维度\",{\"1\":{\"49\":1}}],[\"即空间中的任何距离值具有相似的含义\",{\"1\":{\"47\":1}}],[\"即任何方向上的度量都是等价的\",{\"1\":{\"47\":1}}],[\"即任务是找到点云集中的局部区域的中心点\",{\"1\":{\"46\":1}}],[\"即在欧氏空间中\",{\"1\":{\"47\":1}}],[\"即无功能区域\",{\"1\":{\"39\":1}}],[\"即\",{\"1\":{\"33\":1,\"39\":1,\"66\":1,\"69\":1,\"72\":1,\"80\":1,\"152\":1,\"169\":1,\"219\":1,\"221\":1,\"254\":1,\"255\":1}}],[\"即生成一个二值掩码\",{\"1\":{\"27\":1}}],[\"即所有实际正例被正确分类为正例的比例\",{\"1\":{\"153\":1}}],[\"即所有\",{\"1\":{\"13\":1}}],[\"默认情况下\",{\"1\":{\"197\":1}}],[\"默认要求\",{\"1\":{\"167\":1}}],[\"默认仓库\",{\"1\":{\"147\":1}}],[\"默认使用\",{\"1\":{\"112\":1}}],[\"默认为0\",{\"1\":{\"113\":2}}],[\"默认为false\",{\"1\":{\"113\":1}}],[\"默认为8\",{\"1\":{\"113\":1}}],[\"默认为\",{\"1\":{\"107\":1,\"109\":5,\"166\":1,\"167\":1,\"168\":1,\"169\":1}}],[\"默认为全\",{\"1\":{\"13\":1}}],[\"默认忽略的标签值\",{\"1\":{\"103\":1}}],[\"默认有\",{\"1\":{\"50\":1}}],[\"默认图像注意力掩码为空\",{\"1\":{\"10\":1}}],[\"形\",{\"1\":{\"233\":1}}],[\"形式存储和计算\",{\"1\":{\"192\":1}}],[\"形式\",{\"1\":{\"175\":1,\"257\":1}}],[\"形式为\",{\"1\":{\"35\":1,\"225\":1}}],[\"形心点的坐标来实现\",{\"1\":{\"48\":1}}],[\"形成多尺度特征表示\",{\"1\":{\"55\":1}}],[\"形成最终的局部特征表示\",{\"1\":{\"53\":1}}],[\"形成最终的局部区域表示\",{\"1\":{\"49\":1}}],[\"形成\",{\"1\":{\"49\":1}}],[\"形成一个特征向量\",{\"1\":{\"54\":1}}],[\"形成一个综合的多尺度特征表示\",{\"1\":{\"52\":1}}],[\"形成一个新的子集\",{\"1\":{\"49\":1}}],[\"形成一个类似于\",{\"1\":{\"30\":1}}],[\"形成更稳定的联合表示\",{\"1\":{\"11\":1}}],[\"形状如\",{\"1\":{\"170\":1,\"172\":1}}],[\"形状是\",{\"1\":{\"101\":1}}],[\"形状与\",{\"1\":{\"55\":1,\"170\":1,\"172\":1}}],[\"形状也为\",{\"1\":{\"35\":1}}],[\"形状\",{\"1\":{\"33\":4,\"57\":5,\"58\":2,\"104\":1,\"253\":2}}],[\"形状多样\",{\"1\":{\"28\":1}}],[\"形状的一维数组\",{\"1\":{\"21\":1}}],[\"形状为\",{\"1\":{\"13\":1,\"35\":1,\"40\":1,\"109\":2,\"166\":2,\"167\":2,\"168\":2,\"169\":2}}],[\"层的作用\",{\"1\":{\"253\":1}}],[\"层共享权重\",{\"1\":{\"226\":1}}],[\"层与\",{\"1\":{\"226\":1}}],[\"层数的迁移学习影响\",{\"1\":{\"213\":1}}],[\"层有自注意力头\",{\"1\":{\"211\":1}}],[\"层交错堆叠\",{\"1\":{\"117\":1}}],[\"层和\",{\"1\":{\"117\":1}}],[\"层处理后的输出\",{\"1\":{\"112\":1}}],[\"层维度\",{\"1\":{\"69\":1}}],[\"层进一步融合局部\",{\"1\":{\"68\":1}}],[\"层后面都加了\",{\"1\":{\"64\":1}}],[\"层后的结果\",{\"1\":{\"13\":1}}],[\"层层插值并融合特征\",{\"1\":{\"58\":1}}],[\"层层下采样并提取特征\",{\"1\":{\"58\":1}}],[\"层\",{\"1\":{\"56\":2,\"58\":4,\"66\":1,\"112\":3,\"166\":1,\"167\":1,\"168\":1,\"169\":1,\"226\":1}}],[\"层堆叠\",{\"1\":{\"55\":2}}],[\"层构成了一个\",{\"1\":{\"50\":1}}],[\"层提取特征\",{\"1\":{\"49\":1}}],[\"层次化结构由多个set\",{\"1\":{\"45\":1}}],[\"层次化点集特征学习\",{\"0\":{\"45\":1}}],[\"层归一化来生成所有可能答案的分布\",{\"1\":{\"209\":1}}],[\"层归一化\",{\"1\":{\"15\":1,\"211\":1,\"245\":1,\"270\":1}}],[\"层将\",{\"1\":{\"10\":1,\"104\":1}}],[\"所提供的代码展开进行讲解\",{\"1\":{\"259\":1}}],[\"所参考源仓库未提供requirements\",{\"1\":{\"232\":1}}],[\"所占的内存资源和计算资源呢\",{\"1\":{\"187\":1}}],[\"所需的交集\",{\"1\":{\"172\":1}}],[\"所需的输入格式\",{\"1\":{\"13\":1}}],[\"所具备的\",{\"1\":{\"105\":1}}],[\"所以模型的任务是\",{\"1\":{\"255\":1}}],[\"所以模型必须具有对点顺序的不变性\",{\"1\":{\"72\":1}}],[\"所以最终的答案只能来自原始输入文本中的某一段子串\",{\"1\":{\"255\":1}}],[\"所以我只是取其中的一部分数据\",{\"1\":{\"232\":1}}],[\"所以我们需要加一个正则化项来鼓励变换矩阵接近正交矩阵\",{\"1\":{\"65\":1}}],[\"所以准确度不是在我的考虑范围之内\",{\"1\":{\"232\":1}}],[\"所以就算\",{\"1\":{\"221\":1}}],[\"所以还是很有挑战性的\",{\"1\":{\"212\":1}}],[\"所以矩阵\",{\"1\":{\"190\":2}}],[\"所以参数量为\",{\"1\":{\"115\":1}}],[\"所以把\",{\"1\":{\"103\":1}}],[\"所以\",{\"1\":{\"74\":1,\"125\":1,\"169\":1,\"221\":1}}],[\"所以要把它复制\",{\"1\":{\"68\":1}}],[\"所以小的才是有效点\",{\"1\":{\"49\":1}}],[\"所以这个表达式的含义是\",{\"1\":{\"33\":1}}],[\"所有输入序列等长\",{\"1\":{\"233\":1}}],[\"所有序列都填充到max\",{\"1\":{\"233\":1}}],[\"所有样本列表构成batch数据返回\",{\"1\":{\"225\":1}}],[\"所有无偏差或增益权重设置为\",{\"1\":{\"211\":1}}],[\"所有的转换包括添加随机初初始化的开始和结束标记\",{\"1\":{\"209\":1}}],[\"所有的数据都将发生变化\",{\"1\":{\"43\":1}}],[\"所有这些指标都是基于单个分类阈值值计算得出的\",{\"1\":{\"158\":1}}],[\"所有\",{\"1\":{\"103\":2}}],[\"所有logits的平均作为最终的matching\",{\"1\":{\"102\":1}}],[\"所有模型均训练了32个周期\",{\"1\":{\"90\":1}}],[\"所有卷积和\",{\"1\":{\"64\":1}}],[\"所有点相乘\",{\"1\":{\"72\":1}}],[\"所有点相加\",{\"1\":{\"72\":1}}],[\"所有点经过共享参数的\",{\"1\":{\"62\":1}}],[\"所有点组成的局部区域\",{\"1\":{\"49\":1}}],[\"所有尺度的特征保存到\",{\"1\":{\"53\":1}}],[\"所有尺度的网络并行运行\",{\"1\":{\"53\":1}}],[\"所有问题专属于评估阶段\",{\"1\":{\"23\":1}}],[\"所支持的23种物体类型和17种功能类型\",{\"1\":{\"25\":1}}],[\"所使用的隐藏状态空间维度\",{\"1\":{\"12\":1}}],[\"拼接\",{\"1\":{\"225\":1}}],[\"拼接前提文本\",{\"1\":{\"209\":1}}],[\"拼接机制不够精细\",{\"1\":{\"69\":1}}],[\"拼接方式缺乏动态调整机制\",{\"1\":{\"69\":1}}],[\"拼接嵌入向量\",{\"1\":{\"13\":1}}],[\"拼接后的输入嵌入\",{\"1\":{\"13\":1}}],[\"拼接在一起\",{\"1\":{\"13\":1,\"103\":1}}],[\"拼接多模态嵌入与语言嵌入\",{\"0\":{\"13\":1},\"1\":{\"10\":1}}],[\"如自注意力层和前馈层\",{\"1\":{\"261\":1}}],[\"如翻译后的句子\",{\"1\":{\"261\":1}}],[\"如句子\",{\"1\":{\"261\":1}}],[\"如句子长度\",{\"1\":{\"123\":1}}],[\"如判断哪个是答案的开始\",{\"1\":{\"255\":1}}],[\"如上图\",{\"1\":{\"221\":1}}],[\"如问答\",{\"1\":{\"214\":1}}],[\"如下表2\",{\"1\":{\"213\":1}}],[\"如下图所示\",{\"1\":{\"112\":1,\"187\":1}}],[\"如词汇蕴含\",{\"1\":{\"212\":1}}],[\"如有序句子对\",{\"1\":{\"209\":1}}],[\"如pos\",{\"1\":{\"205\":1}}],[\"如从词级别信息到更高的\",{\"1\":{\"205\":1}}],[\"如作者在实验中证明的\",{\"1\":{\"204\":1}}],[\"如机翻\",{\"1\":{\"204\":1}}],[\"如语言模型\",{\"1\":{\"204\":1}}],[\"如tversky\",{\"1\":{\"173\":1}}],[\"如医学图像\",{\"1\":{\"172\":1}}],[\"如正样本少则增大\",{\"1\":{\"169\":1}}],[\"如欺诈检测\",{\"1\":{\"169\":1}}],[\"如retinanet\",{\"1\":{\"169\":1}}],[\"如1\",{\"1\":{\"169\":1}}],[\"如功能区域边缘不确定性较高\",{\"1\":{\"168\":1}}],[\"如经过\",{\"1\":{\"166\":1}}],[\"如经过下采样后的点\",{\"1\":{\"57\":1}}],[\"如需进一步帮助\",{\"1\":{\"144\":1}}],[\"如此反复\",{\"1\":{\"117\":1}}],[\"如交叉熵损失\",{\"1\":{\"110\":1}}],[\"如局部性和平移不变性\",{\"1\":{\"105\":1}}],[\"如文本生成\",{\"1\":{\"103\":1}}],[\"如vit\",{\"1\":{\"104\":1}}],[\"如vilt\",{\"1\":{\"98\":1}}],[\"如vse++\",{\"1\":{\"98\":1}}],[\"如clip\",{\"1\":{\"98\":1}}],[\"如mae和beit\",{\"1\":{\"96\":1}}],[\"如moco和simclr\",{\"1\":{\"96\":1}}],[\"如imagenet\",{\"1\":{\"96\":1}}],[\"如方差\",{\"1\":{\"72\":1}}],[\"如时间序列点云\",{\"1\":{\"69\":1}}],[\"如只有几十个点\",{\"1\":{\"69\":1}}],[\"如弯曲\",{\"1\":{\"69\":1}}],[\"如人体姿态变化\",{\"1\":{\"69\":1}}],[\"如椅子腿和桌面连接处\",{\"1\":{\"69\":1}}],[\"如旋转\",{\"1\":{\"61\":1,\"64\":1,\"65\":1}}],[\"如最大池化\",{\"1\":{\"60\":1}}],[\"如桌子边缘\",{\"1\":{\"69\":1}}],[\"如桌子\",{\"1\":{\"55\":1,\"58\":1}}],[\"如法线\",{\"1\":{\"49\":1}}],[\"如何获取答案\",{\"1\":{\"255\":1}}],[\"如何利用\",{\"1\":{\"253\":1}}],[\"如何生成并起作用的\",{\"0\":{\"230\":1}}],[\"如何生成点集的划分\",{\"1\":{\"43\":1}}],[\"如何写好prompt\",{\"0\":{\"195\":1}}],[\"如何对大模型进行微调\",{\"0\":{\"180\":1}}],[\"如何选择\",{\"0\":{\"173\":1}}],[\"如何理解这个过程\",{\"0\":{\"129\":1}}],[\"如何将二维图像转换为一维时间序列\",{\"1\":{\"119\":1}}],[\"如何将这个预训练的视觉模型应用到新的任务中呢\",{\"1\":{\"91\":1}}],[\"如何计算loss的\",{\"1\":{\"101\":1}}],[\"如何降低模型训练成本\",{\"1\":{\"98\":1}}],[\"如何设计一个能够从这些局部分区中学习有用特征的机制\",{\"1\":{\"43\":1}}],[\"如何有效地对点云进行分区\",{\"1\":{\"43\":1}}],[\"如lay\",{\"1\":{\"25\":1}}],[\"如bed\",{\"1\":{\"25\":1}}],[\"如\",{\"1\":{\"15\":1,\"16\":3,\"20\":1,\"24\":2,\"28\":1,\"35\":1,\"39\":1,\"57\":2,\"69\":3,\"72\":1,\"78\":5,\"80\":3,\"81\":2,\"85\":1,\"103\":2,\"108\":1,\"112\":1,\"147\":2,\"169\":5,\"175\":1,\"189\":1,\"192\":2,\"203\":1}}],[\"如果正确答案没有出现在上下文中\",{\"1\":{\"255\":1}}],[\"如果值大于\",{\"1\":{\"254\":1}}],[\"如果值小于\",{\"1\":{\"254\":1}}],[\"如果本地不存在\",{\"1\":{\"232\":1}}],[\"如果遇到不存在于字典中的word\",{\"1\":{\"224\":1}}],[\"如果任一句子长度超过50\",{\"1\":{\"224\":1}}],[\"如果现在的任务是\",{\"1\":{\"221\":4}}],[\"如果现在有这么一句话\",{\"1\":{\"219\":1}}],[\"如果给的例子是比较简单的问题\",{\"1\":{\"200\":1}}],[\"如果给定随机选择的正例和负例\",{\"1\":{\"160\":1}}],[\"如果矩阵\",{\"1\":{\"190\":1}}],[\"如果需要\",{\"1\":{\"187\":1}}],[\"如果要用lora适配不同的场景\",{\"1\":{\"184\":1}}],[\"如果将大模型比做一个函数\",{\"1\":{\"182\":1}}],[\"如果数据是不能传递给第三方大模型服务的\",{\"1\":{\"179\":1}}],[\"如果数据集在类别之间大致平衡\",{\"1\":{\"161\":1}}],[\"如果数据集不平衡\",{\"1\":{\"152\":1}}],[\"如果存在记录出现次数\",{\"1\":{\"176\":1}}],[\"如果为\",{\"1\":{\"170\":1,\"172\":1}}],[\"如果已经包含\",{\"1\":{\"167\":1,\"168\":1}}],[\"如果费用大致相当\",{\"1\":{\"162\":1}}],[\"如果假正例成本较低\",{\"1\":{\"162\":1}}],[\"如果假正例\",{\"1\":{\"162\":1}}],[\"如果忽略所有其他阈值\",{\"1\":{\"159\":1}}],[\"如果您想评估模型在所有可能阈值下的质量\",{\"1\":{\"158\":1}}],[\"如果模型预测\",{\"1\":{\"255\":1}}],[\"如果模型最后一层没有\",{\"1\":{\"170\":1}}],[\"如果模型最后没有\",{\"1\":{\"167\":1,\"168\":1,\"169\":1}}],[\"如果模型的效果与随机猜测或抛硬币的效果完全一样\",{\"1\":{\"160\":1}}],[\"如果模型\",{\"1\":{\"152\":1}}],[\"如果实际正例的总数与实际负例的总数不接近\",{\"1\":{\"151\":1}}],[\"如果你希望模型能\",{\"1\":{\"255\":1}}],[\"如果你设置\",{\"1\":{\"170\":2}}],[\"如果你的模型最后没有\",{\"1\":{\"166\":1}}],[\"如果你想删除某个环境\",{\"1\":{\"144\":1}}],[\"如果你有一张人脸的点云\",{\"1\":{\"69\":1}}],[\"如果类别数大于\",{\"1\":{\"114\":1}}],[\"如果未激活任何环境时使用\",{\"1\":{\"147\":1}}],[\"如果未指定\",{\"1\":{\"112\":2}}],[\"如果未提供位置id\",{\"1\":{\"102\":1}}],[\"如果传入了归一化层类\",{\"1\":{\"109\":1}}],[\"如果传入的是整数\",{\"1\":{\"109\":1}}],[\"如果传入一个归一化层类\",{\"1\":{\"109\":1}}],[\"如果传入一个整数\",{\"1\":{\"109\":2}}],[\"如果定义了图像预处理转换操作\",{\"1\":{\"107\":1}}],[\"如果该路径在采样的验证集样本中则存入验证集\",{\"1\":{\"107\":1}}],[\"如果是全参数微调\",{\"1\":{\"189\":1}}],[\"如果是解决自己日常生活\",{\"1\":{\"179\":1}}],[\"如果是\",{\"1\":{\"148\":2,\"189\":1}}],[\"如果是beam\",{\"1\":{\"104\":1}}],[\"如果是分割任务\",{\"1\":{\"66\":2}}],[\"如果是分类任务\",{\"1\":{\"66\":2}}],[\"如果进一步采用convirt\",{\"1\":{\"96\":1}}],[\"如果在读取图片过程中出现错误\",{\"1\":{\"93\":1}}],[\"如果我们用多种不同的方法去求解\",{\"1\":{\"199\":1}}],[\"如果我们将标准答案作为列\",{\"1\":{\"151\":1}}],[\"如果我们直接使用类别标签作为文本描述\",{\"1\":{\"92\":1}}],[\"如果我本来就有特征\",{\"1\":{\"57\":1}}],[\"如果有一个能直接处理各式\",{\"1\":{\"217\":1}}],[\"如果有缓存\",{\"1\":{\"103\":1}}],[\"如果有缓存的key\",{\"1\":{\"103\":1}}],[\"如果有个类别\",{\"1\":{\"91\":1}}],[\"如果有额外特征\",{\"1\":{\"49\":1,\"53\":1}}],[\"如果有额外的点特征\",{\"1\":{\"49\":1}}],[\"如果对于任意排列\",{\"1\":{\"72\":1}}],[\"如果训练过程中加入了噪声\",{\"1\":{\"69\":1}}],[\"如果点太少\",{\"1\":{\"69\":1}}],[\"如果这些关键点缺失或被遮挡\",{\"1\":{\"69\":1}}],[\"如果直接作为变换矩阵\",{\"1\":{\"64\":1}}],[\"如果没有任何归纳偏置\",{\"1\":{\"105\":1}}],[\"如果没有则下载\",{\"1\":{\"93\":1,\"95\":1}}],[\"如果没有\",{\"1\":{\"64\":1}}],[\"如果没有提供激活函数层\",{\"1\":{\"114\":1}}],[\"如果没有提供归一化层\",{\"1\":{\"114\":1}}],[\"如果没有提供\",{\"1\":{\"13\":1}}],[\"如果不是则抛出异常\",{\"1\":{\"107\":1}}],[\"如果不加处理\",{\"1\":{\"64\":1}}],[\"如果不够就重复最近的点来填充\",{\"1\":{\"49\":1}}],[\"如果原始点有自己的特征\",{\"1\":{\"57\":1}}],[\"如果原始点有特征\",{\"1\":{\"57\":1}}],[\"如果只有1个下采样点\",{\"1\":{\"57\":1}}],[\"如果只用\",{\"1\":{\"33\":1}}],[\"如果\",{\"1\":{\"49\":1,\"69\":2,\"172\":2,\"227\":2,\"253\":1}}],[\"如果某个查询点附近的点太少\",{\"1\":{\"49\":1}}],[\"如果某个点到新中心点的距离比之前记录的\",{\"1\":{\"49\":1}}],[\"如果包含强度值\",{\"1\":{\"40\":1}}],[\"如果当前\",{\"1\":{\"39\":1}}],[\"如果当前样本没有正类\",{\"1\":{\"39\":1}}],[\"如果当前是训练模式\",{\"1\":{\"25\":1}}],[\"如果当前物体类别在排序后的字典中\",{\"1\":{\"25\":1}}],[\"如果当前物体类别不在排序后的字典中\",{\"1\":{\"25\":1}}],[\"如果使用多头注意力\",{\"1\":{\"133\":1}}],[\"如果使用\",{\"1\":{\"10\":1}}],[\"如图像或点云特征\",{\"1\":{\"13\":1}}],[\"如图像\",{\"1\":{\"13\":1}}],[\"映射到分类空间中去\",{\"1\":{\"114\":1}}],[\"映射到\",{\"1\":{\"12\":1,\"166\":1}}],[\"进而可以自由构建\",{\"1\":{\"287\":1}}],[\"进而导致大模型的输出质量打折口\",{\"1\":{\"179\":1}}],[\"进一步增强了模型性能\",{\"1\":{\"278\":1}}],[\"进一步增强特征\",{\"1\":{\"58\":1}}],[\"进一步扩展了\",{\"1\":{\"278\":1}}],[\"进一步扩展了这个方法来预测n\",{\"1\":{\"96\":1}}],[\"进一步来说\",{\"1\":{\"204\":1}}],[\"进一步帮llm明确要求\",{\"1\":{\"196\":1}}],[\"进一步地\",{\"1\":{\"91\":1}}],[\"进一步训练模型理解和执行更复杂的视觉指令任务\",{\"1\":{\"79\":1}}],[\"进一步微调模型\",{\"1\":{\"78\":1}}],[\"进一步提取和融合特征\",{\"1\":{\"57\":1}}],[\"进一步提取各自模态内部的语义一致性与结构关系\",{\"1\":{\"11\":1}}],[\"进行个性化定制\",{\"1\":{\"291\":1}}],[\"进行自然的语音和视频交流\",{\"1\":{\"278\":1}}],[\"进行解压\",{\"1\":{\"232\":1}}],[\"进行二分类任务\",{\"1\":{\"226\":1}}],[\"进行预测即可\",{\"1\":{\"221\":1}}],[\"进行预训练\",{\"1\":{\"100\":1}}],[\"进行\",{\"1\":{\"221\":1}}],[\"进行的进一步优化\",{\"1\":{\"199\":1}}],[\"进行思考\",{\"1\":{\"197\":1}}],[\"进行一次降维再升维的操作\",{\"1\":{\"189\":1}}],[\"进行训练\",{\"1\":{\"187\":1}}],[\"进行了进一步的量化\",{\"1\":{\"185\":1}}],[\"进行了以下筛选\",{\"1\":{\"80\":1}}],[\"进行全量的训练\",{\"1\":{\"180\":1}}],[\"进行局部特征提取\",{\"1\":{\"117\":1}}],[\"进行反向传播\",{\"1\":{\"114\":1}}],[\"进行归一化\",{\"1\":{\"114\":1}}],[\"进行并行输入\",{\"1\":{\"109\":1}}],[\"进行文本编码\",{\"1\":{\"103\":1}}],[\"进行编码\",{\"1\":{\"100\":2}}],[\"进行分类\",{\"1\":{\"69\":1,\"256\":1}}],[\"进行特征插值和上采样\",{\"1\":{\"58\":1}}],[\"进行特征学习\",{\"1\":{\"43\":1}}],[\"进行多尺度特征提取和下采样\",{\"1\":{\"58\":1}}],[\"进行推理\",{\"1\":{\"40\":1}}],[\"进行内积操作\",{\"1\":{\"33\":1}}],[\"进行组内和通道间的信息混合\",{\"0\":{\"30\":1}}],[\"进行逐级上采样\",{\"1\":{\"16\":1}}],[\"进行联合推理\",{\"1\":{\"10\":1}}],[\"在完成前后端搭建之后\",{\"1\":{\"291\":1}}],[\"在完成上一步的初始化\",{\"1\":{\"291\":1}}],[\"在该步中\",{\"1\":{\"291\":1}}],[\"在该步骤中\",{\"1\":{\"291\":1}}],[\"在确定开发目标后\",{\"1\":{\"291\":1}}],[\"在进行开发前\",{\"1\":{\"291\":1}}],[\"在验证集上最终验证模型效果来实现性能的评估\",{\"1\":{\"290\":1}}],[\"在测试集上调优模型\",{\"1\":{\"290\":1}}],[\"在评估思路上\",{\"1\":{\"290\":1}}],[\"在评估模型和选择阈值时\",{\"1\":{\"156\":1}}],[\"在开发过程中\",{\"1\":{\"287\":1}}],[\"在下图中\",{\"1\":{\"286\":1}}],[\"在提升大语言模型效果中\",{\"1\":{\"285\":1}}],[\"在理解和生成长篇内容时受限于有限的上下文窗口\",{\"1\":{\"283\":1}}],[\"在信息检索领域\",{\"1\":{\"282\":1}}],[\"在处理特定领域的专业知识时\",{\"1\":{\"283\":1}}],[\"在处理各种任务时表现出色\",{\"1\":{\"280\":1}}],[\"在处理文本时具有强大的上下文感知能力\",{\"1\":{\"279\":1}}],[\"在各种基准测试中均优于\",{\"1\":{\"278\":1}}],[\"在各种任务中的表现均显著提升\",{\"1\":{\"277\":1}}],[\"在多项基准测试中超越了\",{\"1\":{\"278\":1}}],[\"在多语言理解和代码生成等方面表现出色\",{\"1\":{\"278\":1}}],[\"在遵循复杂指令\",{\"1\":{\"278\":1}}],[\"在国内率先开启邀测\",{\"1\":{\"278\":1}}],[\"在性能和效率上有显著提升\",{\"1\":{\"278\":1}}],[\"在数学\",{\"1\":{\"278\":2}}],[\"在数学上\",{\"1\":{\"153\":1,\"155\":1,\"157\":1}}],[\"在回答前会先生成一段思维链\",{\"1\":{\"278\":1}}],[\"在知识广度\",{\"1\":{\"278\":1}}],[\"在语音互动中传递更丰富的情感变化\",{\"1\":{\"278\":1}}],[\"在语义分割任务中\",{\"1\":{\"164\":1}}],[\"在解决复杂任务和评估任务上展现出较大的性能提升\",{\"1\":{\"278\":1}}],[\"在解码器中\",{\"1\":{\"261\":1}}],[\"在他的经典论文\",{\"1\":{\"277\":1}}],[\"在输入序列长度不一致时\",{\"1\":{\"261\":1}}],[\"在输入中添加可学习的前缀\",{\"1\":{\"85\":1}}],[\"在上下文中找到最可能的答案起始位置和结束位置\",{\"1\":{\"255\":1}}],[\"在上面的例子中\",{\"1\":{\"199\":1}}],[\"在上面的结构图中可以看到\",{\"1\":{\"110\":1}}],[\"在问答任务中一般不会使用这个输出\",{\"1\":{\"253\":1}}],[\"在问答任务中\",{\"1\":{\"253\":1}}],[\"在问答上提升5\",{\"1\":{\"203\":1}}],[\"在使用\",{\"1\":{\"253\":1}}],[\"在使用cot这种prompt\",{\"1\":{\"198\":1}}],[\"在使用clip模型进行zero\",{\"1\":{\"92\":1}}],[\"在返回前进行预处理\",{\"1\":{\"235\":1}}],[\"在注意力得分矩阵计算完毕后\",{\"1\":{\"230\":1}}],[\"在注意力机制\",{\"1\":{\"122\":1}}],[\"在两句话之间和句末加\",{\"1\":{\"219\":1}}],[\"在两个线性层之间通常会插入一个非线性激活函数\",{\"1\":{\"112\":1}}],[\"在nli和qqp任务上辅助lm目标有帮助\",{\"1\":{\"213\":1}}],[\"在nlp中\",{\"1\":{\"204\":1}}],[\"在nlp领域\",{\"1\":{\"96\":1}}],[\"在文档和问题给定条件下\",{\"1\":{\"213\":1}}],[\"在阈值下进行预测的\",{\"1\":{\"213\":1}}],[\"在零样本上\",{\"1\":{\"213\":1}}],[\"在应用到很多任务时可以提高语言建模的能力并且transformer更具结构化的注意力记忆\",{\"1\":{\"213\":1}}],[\"在12个的9个数据集取得了sota结果\",{\"1\":{\"214\":1}}],[\"在12个数据集上的9个取得sota结果\",{\"1\":{\"212\":1}}],[\"在12个研究任务中9个提升到sota\",{\"1\":{\"203\":1,\"204\":1}}],[\"在sts\",{\"1\":{\"212\":1}}],[\"在story\",{\"1\":{\"212\":1}}],[\"在seq\",{\"1\":{\"102\":1}}],[\"在大模型开发中\",{\"1\":{\"290\":1}}],[\"在大部分任务中基本上\",{\"1\":{\"211\":1}}],[\"在大规模文本语料上学习高容量的语言模型\",{\"1\":{\"206\":1}}],[\"在大规模场景理解任务中表现一般\",{\"1\":{\"69\":1}}],[\"在句子级别打乱顺序以破坏长距离结构信息\",{\"1\":{\"211\":1}}],[\"在作者的实验中\",{\"1\":{\"207\":1}}],[\"在迁移阶段\",{\"1\":{\"204\":1}}],[\"在迁移到其他数据集时也需要加上新的分类器进行有监督训练\",{\"1\":{\"96\":1}}],[\"在迁移到下游任务时\",{\"1\":{\"96\":1}}],[\"在无标记数据上使用语言模型目标来学习神经网络初始化的参数\",{\"1\":{\"204\":1}}],[\"在本文中\",{\"1\":{\"204\":1}}],[\"在一些不够大的llm上\",{\"1\":{\"198\":1}}],[\"在推理大模型方面\",{\"1\":{\"278\":1}}],[\"在推理方面就十分出色\",{\"1\":{\"278\":1}}],[\"在推理能力\",{\"1\":{\"278\":1}}],[\"在推理型模型中可选择性展示思考过程\",{\"1\":{\"278\":1}}],[\"在推理过程中\",{\"1\":{\"189\":1}}],[\"在推理的过程中直接将∆w加到w上去\",{\"1\":{\"184\":1}}],[\"在前向传播中\",{\"1\":{\"257\":1}}],[\"在前向过程中\",{\"1\":{\"189\":1}}],[\"在前面我们介绍了\",{\"1\":{\"190\":1}}],[\"在前景像素远少于背景像素时表现良好\",{\"1\":{\"172\":1}}],[\"在原始预训练语言模型\",{\"1\":{\"189\":1}}],[\"在原始点数量下的每个点都拥有一个合理的特征向量\",{\"1\":{\"57\":1}}],[\"在之前的问题上表现明显变差\",{\"1\":{\"188\":1}}],[\"在之前的例子中\",{\"1\":{\"92\":1}}],[\"在自然语言处理领域\",{\"1\":{\"282\":1}}],[\"在自注意力机制之后\",{\"1\":{\"261\":1}}],[\"在自己的数据上继续训练\",{\"1\":{\"188\":1}}],[\"在自回归生成时\",{\"1\":{\"103\":1}}],[\"在微调完成后\",{\"1\":{\"187\":1}}],[\"在微调过程中\",{\"1\":{\"187\":1}}],[\"在准备好的数据集上\",{\"1\":{\"187\":1}}],[\"在特定任务相关的数据集上执行有监督全量参数微调\",{\"1\":{\"187\":1}}],[\"在prompt中加入的示例不是1条\",{\"1\":{\"198\":1}}],[\"在prompt中加入一些示例\",{\"1\":{\"198\":1}}],[\"在prompt中加入一些例子\",{\"1\":{\"197\":1}}],[\"在prompt上下文中添加适当的条件\",{\"1\":{\"183\":1}}],[\"在pointnet中\",{\"1\":{\"43\":1}}],[\"在不损失太多性能的情况下减少了模型的大小\",{\"1\":{\"192\":1}}],[\"在不改变大模型的前提下\",{\"1\":{\"183\":1}}],[\"在不同解码阶段注入语言线索\",{\"1\":{\"28\":1}}],[\"在不同解码层注入语言信息\",{\"1\":{\"27\":1}}],[\"在x前面加上了一些特定的内容\",{\"1\":{\"182\":1}}],[\"在具体执行特定任务的时候按需调用\",{\"1\":{\"182\":1}}],[\"在计算iou得分之前\",{\"1\":{\"171\":1}}],[\"在计算机视觉领域\",{\"1\":{\"96\":1,\"282\":1}}],[\"在此实现中未使用\",{\"1\":{\"168\":1}}],[\"在类别平衡时效果好\",{\"1\":{\"167\":1}}],[\"在垃圾邮件分类器示例中\",{\"1\":{\"160\":1}}],[\"在垃圾邮件分类示例中\",{\"1\":{\"152\":1,\"153\":1,\"154\":1,\"155\":1,\"156\":1}}],[\"在激活的\",{\"1\":{\"147\":1}}],[\"在当前环境下安装包\",{\"0\":{\"147\":1}}],[\"在标准的\",{\"1\":{\"123\":1}}],[\"在如此大规模的数据集上进行预训练\",{\"1\":{\"118\":1}}],[\"在模型训练和推理过程中\",{\"1\":{\"192\":1}}],[\"在模型架构中\",{\"1\":{\"117\":1}}],[\"在模型初始化时\",{\"1\":{\"111\":1}}],[\"在深度学习领域\",{\"1\":{\"115\":1}}],[\"在transformer\",{\"1\":{\"114\":1}}],[\"在transformer中\",{\"1\":{\"111\":1}}],[\"在transformer模型中\",{\"1\":{\"105\":1}}],[\"在每一步中\",{\"1\":{\"286\":1}}],[\"在每一层\",{\"1\":{\"110\":1}}],[\"在每个层上\",{\"1\":{\"45\":1}}],[\"在代码中\",{\"1\":{\"109\":1}}],[\"在预训练\",{\"1\":{\"217\":1}}],[\"在预训练和使用该模型时\",{\"1\":{\"118\":1}}],[\"在预训练时\",{\"1\":{\"108\":1}}],[\"在预训练的基础上\",{\"1\":{\"79\":1}}],[\"在柱状图上添加数值标签\",{\"1\":{\"107\":1}}],[\"在图像分类任务中\",{\"1\":{\"105\":1,\"117\":1}}],[\"在图像识别中加入\",{\"1\":{\"85\":1}}],[\"在图文检索中\",{\"1\":{\"93\":1}}],[\"在imagenet数据集上可以提升1\",{\"1\":{\"92\":1}}],[\"在实验中\",{\"1\":{\"173\":1}}],[\"在实践中\",{\"1\":{\"159\":1}}],[\"在实际的模型中\",{\"1\":{\"178\":1}}],[\"在实际训练中更稳定\",{\"1\":{\"168\":1}}],[\"在实际正例数量非常少\",{\"1\":{\"155\":1}}],[\"在实际正例数量非常少的不平衡数据集中\",{\"1\":{\"153\":1}}],[\"在实际负例数量非常少\",{\"1\":{\"154\":1}}],[\"在实际应用中可以选用常见的卷积神经网络\",{\"1\":{\"90\":1}}],[\"在实现时可采用自然语言处理\",{\"1\":{\"90\":1}}],[\"在实现时\",{\"1\":{\"47\":1}}],[\"在速度和效率上占优\",{\"1\":{\"69\":1}}],[\"在某些标准数据集\",{\"1\":{\"69\":1}}],[\"在部件分割任务中\",{\"1\":{\"69\":1}}],[\"在通道维度进行拼接\",{\"1\":{\"66\":1}}],[\"在分类任务中更有用\",{\"1\":{\"253\":1}}],[\"在分类任务中\",{\"1\":{\"169\":1}}],[\"在分割任务中\",{\"1\":{\"62\":1}}],[\"在分组内进行信息交换\",{\"1\":{\"32\":1}}],[\"在这方面表现较差\",{\"1\":{\"277\":1}}],[\"在这之后用生成模型分配的剩下序列的平均token对数概率高的作为结果\",{\"1\":{\"213\":1}}],[\"在这些示例中\",{\"1\":{\"198\":1}}],[\"在这些点中选出若干个中心点\",{\"1\":{\"44\":1}}],[\"在这里\",{\"1\":{\"178\":1}}],[\"在这个高维空间里\",{\"1\":{\"112\":1}}],[\"在这个过程中\",{\"1\":{\"91\":1}}],[\"在这个任务上\",{\"1\":{\"81\":1}}],[\"在这种情况下\",{\"1\":{\"54\":1,\"96\":1,\"204\":1}}],[\"在multinli上转移embedding能提升结果\",{\"1\":{\"213\":1}}],[\"在multinli和race上的性能随着层数的变化而变化\",{\"1\":{\"213\":1}}],[\"在mrg中\",{\"1\":{\"54\":1}}],[\"在msg中\",{\"1\":{\"52\":1}}],[\"在训练公式\",{\"1\":{\"208\":1}}],[\"在训练过程中\",{\"1\":{\"78\":1,\"89\":1}}],[\"在训练时引入随机丢弃形心来模拟不同密度情况\",{\"1\":{\"53\":1}}],[\"在训练时引入不同密度的点集情况\",{\"1\":{\"53\":1}}],[\"在训练和测试中不作为显式监督信号\",{\"1\":{\"21\":1}}],[\"在密集采样的区域中学到的特征可能无法很好地泛化到稀疏采样的区域\",{\"1\":{\"51\":1}}],[\"在点云中逐步选择离已选点尽可能远的点\",{\"1\":{\"49\":1}}],[\"在屋里空间或某些特定的抽象空间中\",{\"1\":{\"47\":1}}],[\"在平移不变性上也有局限性\",{\"1\":{\"43\":1}}],[\"在论文的最后\",{\"1\":{\"117\":1}}],[\"在论文中\",{\"1\":{\"90\":1,\"114\":1,\"115\":1}}],[\"在论文中提到\",{\"1\":{\"69\":1}}],[\"在论文\",{\"1\":{\"35\":1}}],[\"在扩展过程中\",{\"1\":{\"20\":1}}],[\"在\",{\"1\":{\"15\":1,\"28\":1,\"30\":1,\"33\":1,\"35\":1,\"39\":2,\"40\":1,\"49\":3,\"64\":1,\"65\":1,\"69\":1,\"81\":1,\"88\":1,\"93\":1,\"108\":1,\"117\":2,\"161\":2,\"162\":1,\"189\":2,\"192\":1,\"218\":1,\"253\":2,\"255\":1,\"257\":1,\"277\":1,\"278\":2,\"281\":1,\"288\":1}}],[\"在跨模态融合后\",{\"1\":{\"11\":1}}],[\"在混合精度下运行\",{\"1\":{\"10\":1}}],[\"是前两代\",{\"1\":{\"278\":1}}],[\"是专为复杂推理设计的模型\",{\"1\":{\"278\":1}}],[\"是编码器模型\",{\"1\":{\"255\":1}}],[\"是答案终点的得分\",{\"1\":{\"253\":1}}],[\"是答案起点的得分\",{\"1\":{\"253\":1}}],[\"是模型预测出的答案的起始和结束位置\",{\"1\":{\"255\":1}}],[\"是模型预测的概率\",{\"1\":{\"169\":1}}],[\"是模型最后一层所有\",{\"1\":{\"253\":1}}],[\"是序列实际长度\",{\"1\":{\"233\":1}}],[\"是序列长度\",{\"1\":{\"128\":1}}],[\"是当前文本对应的类别标签\",{\"1\":{\"233\":1}}],[\"是没有任何实际意义的\",{\"1\":{\"221\":1}}],[\"是非常耗成本的\",{\"1\":{\"217\":1}}],[\"是非常重要的\",{\"1\":{\"184\":1}}],[\"是什么\",{\"0\":{\"217\":1}}],[\"是蕴含\",{\"1\":{\"212\":1}}],[\"是位置嵌入矩阵\",{\"1\":{\"207\":1}}],[\"是字符嵌入矩阵\",{\"1\":{\"207\":1}}],[\"是层数\",{\"1\":{\"207\":1}}],[\"是上下文字符向量\",{\"1\":{\"207\":1}}],[\"是上下文窗口大小\",{\"1\":{\"207\":1}}],[\"是参数\",{\"1\":{\"207\":1}}],[\"是正确的\",{\"1\":{\"199\":1}}],[\"是人类发给各种人工智能模型\",{\"1\":{\"194\":1}}],[\"是训练参数\",{\"1\":{\"189\":1}}],[\"是固定不变的\",{\"1\":{\"189\":1}}],[\"是预训练模型初始化的参数\",{\"1\":{\"189\":1}}],[\"是预测的概率值\",{\"1\":{\"169\":1}}],[\"是预测值与真实值之间的平均绝对误差\",{\"1\":{\"39\":1}}],[\"是低秩的秩\",{\"1\":{\"187\":1}}],[\"是影响大模型生成结果的关键参数\",{\"1\":{\"184\":1}}],[\"是基于对话聊天的\",{\"1\":{\"282\":1}}],[\"是基于基座模型开发出来的\",{\"1\":{\"278\":1}}],[\"是基于文本输入来生成图像的模型\",{\"1\":{\"88\":1}}],[\"是基座模型\",{\"1\":{\"182\":1}}],[\"是能够在可控成本的前提下\",{\"1\":{\"180\":1}}],[\"是交叉熵和\",{\"1\":{\"172\":1}}],[\"是两种主流的方法\",{\"1\":{\"285\":1}}],[\"是两种用于提升预训练语言模型\",{\"1\":{\"85\":1}}],[\"是两个广泛用于语言模型训练和评估的英文维基百科语料数据集\",{\"1\":{\"223\":1}}],[\"是两个可调节的超参数\",{\"1\":{\"170\":1}}],[\"是真实标签\",{\"1\":{\"169\":1}}],[\"是衡量两个样本集合之间重叠程度的一种指标\",{\"1\":{\"166\":1}}],[\"是比较两个不同模型性能的有效衡量指标\",{\"1\":{\"162\":1}}],[\"是按选定的间隔\",{\"1\":{\"159\":1}}],[\"是用于信息表达的维度\",{\"1\":{\"135\":1}}],[\"是用于计算相似度的维度\",{\"1\":{\"135\":1}}],[\"是数据库中的内容\",{\"1\":{\"132\":1}}],[\"是数据库中的索引\",{\"1\":{\"132\":1}}],[\"是你输入的问题\",{\"1\":{\"132\":1}}],[\"是hidden\",{\"1\":{\"115\":1}}],[\"是通道数\",{\"1\":{\"109\":1}}],[\"是批量大小\",{\"1\":{\"109\":1}}],[\"是卷积核的步长\",{\"1\":{\"109\":1}}],[\"是卷积核的大小\",{\"1\":{\"109\":1}}],[\"是输出通道数\",{\"1\":{\"109\":1}}],[\"是输入通道数\",{\"1\":{\"109\":1}}],[\"是为了把\",{\"1\":{\"104\":1}}],[\"是为了扩展成\",{\"1\":{\"65\":1}}],[\"是第一个transformer模块的input\",{\"1\":{\"100\":1}}],[\"是引导模型\",{\"1\":{\"85\":1}}],[\"是教会模型\",{\"1\":{\"85\":1}}],[\"是由深度求索\",{\"1\":{\"278\":1}}],[\"是由\",{\"1\":{\"78\":1}}],[\"是指我们为了让llm能够更好地完成我们给它的任务\",{\"1\":{\"194\":1}}],[\"是指用于训练模型的参数非常多\",{\"1\":{\"178\":1}}],[\"是指被错误地归类为正例的所有实际负例所占的比例\",{\"1\":{\"154\":1}}],[\"是指位置嵌入的参数是可以在模型训练过程中通过反向传播算法进行更新的\",{\"1\":{\"111\":1}}],[\"是指\",{\"1\":{\"73\":1}}],[\"是最大可容忍的点云范围\",{\"1\":{\"62\":1}}],[\"是关键点集合\",{\"1\":{\"62\":1}}],[\"是对称的\",{\"1\":{\"62\":1}}],[\"是后续的全连接网络\",{\"1\":{\"62\":1}}],[\"是每个点的高维特征\",{\"1\":{\"62\":1}}],[\"是每个点云局部区域关键点引导下提取的图像信息增强后的点云关键点局部区域特征\",{\"1\":{\"11\":1}}],[\"是类别数\",{\"1\":{\"55\":1}}],[\"是无缓存的\",{\"1\":{\"40\":1}}],[\"是有缓存的\",{\"1\":{\"40\":1}}],[\"是有效的\",{\"1\":{\"13\":1}}],[\"是否能生成新文本\",{\"1\":{\"255\":1}}],[\"是否为连贯的上下句\",{\"1\":{\"225\":1}}],[\"是否为推理模式\",{\"1\":{\"10\":1}}],[\"是否适合类别不平衡\",{\"1\":{\"172\":1}}],[\"是否可调\",{\"1\":{\"172\":1}}],[\"是否可微\",{\"1\":{\"72\":1}}],[\"是否直接优化\",{\"1\":{\"168\":1}}],[\"是否对类别不平衡敏感\",{\"1\":{\"168\":1}}],[\"是否对\",{\"1\":{\"166\":1,\"167\":1,\"168\":1,\"169\":1}}],[\"是否在生成q\",{\"1\":{\"113\":1}}],[\"是否采样\",{\"1\":{\"104\":1}}],[\"是否使用核采样\",{\"1\":{\"104\":1}}],[\"是否更新全部参数\",{\"1\":{\"85\":1}}],[\"是否修改模型结构\",{\"1\":{\"85\":1}}],[\"是否被后续模型改进\",{\"1\":{\"69\":1}}],[\"是否包含法线信息\",{\"1\":{\"50\":1}}],[\"是否关注区域匹配\",{\"1\":{\"172\":1}}],[\"是否关注像素分类\",{\"1\":{\"172\":1}}],[\"是否关注空间重合度\",{\"1\":{\"39\":1}}],[\"是否关注分布相似性\",{\"1\":{\"39\":1}}],[\"是否依赖\",{\"1\":{\"39\":1}}],[\"是否支持\",{\"1\":{\"39\":1,\"168\":1}}],[\"是语言增强后的点特征\",{\"1\":{\"31\":1}}],[\"是线性变换\",{\"1\":{\"31\":1}}],[\"是一款面向消费级应用的轻量级模型\",{\"1\":{\"278\":1}}],[\"是一些句子对\",{\"1\":{\"212\":1}}],[\"是一个完整的系统\",{\"1\":{\"284\":1}}],[\"是一个\",{\"1\":{\"255\":2}}],[\"是一个线性层\",{\"1\":{\"253\":1}}],[\"是一个线性变换\",{\"1\":{\"29\":1}}],[\"是一个包含1亿个词汇的英文词库数据\",{\"1\":{\"223\":1}}],[\"是一个大规模的图像数据集\",{\"1\":{\"118\":1}}],[\"是一个随机初始化的向量\",{\"1\":{\"110\":1}}],[\"是一个分类头\",{\"1\":{\"103\":1}}],[\"是一个小型神经网络\",{\"1\":{\"64\":1}}],[\"是一种令人兴奋的技术\",{\"1\":{\"282\":1}}],[\"是一种像人类一样思考和学习的人工智能\",{\"1\":{\"282\":1}}],[\"是一种旨在理解和生成人类语言的人工智能模型\",{\"1\":{\"277\":1}}],[\"是一种在保证模型效果基本不降低的前提下\",{\"1\":{\"185\":1}}],[\"是一种基于自注意力机制\",{\"1\":{\"261\":1}}],[\"是一种基于频率统计的子词分词算法\",{\"1\":{\"174\":1}}],[\"是一种基于直方图交集的相似性指标\",{\"1\":{\"39\":1}}],[\"是一种结合了多个损失函数优点的混合损失函数\",{\"1\":{\"172\":1}}],[\"是一种针对类别不平衡\",{\"1\":{\"169\":1}}],[\"是一种常用的损失函数\",{\"1\":{\"168\":1}}],[\"是一种常用于语义分割任务的损失函数\",{\"1\":{\"166\":1}}],[\"是一种对输入顺序不敏感的函数\",{\"1\":{\"72\":1}}],[\"是一种表示三维空间中物体或场景的方式\",{\"1\":{\"71\":1}}],[\"是一种单尺度网络\",{\"1\":{\"69\":1}}],[\"是一种\",{\"1\":{\"30\":1}}],[\"是一种残差连接\",{\"1\":{\"29\":1}}],[\"是roberta编码后的文本特征\",{\"1\":{\"29\":1}}],[\"是点数\",{\"1\":{\"21\":1}}],[\"是\",{\"1\":{\"21\":1,\"33\":1,\"35\":1,\"39\":9,\"40\":1,\"49\":2,\"57\":1,\"62\":1,\"65\":1,\"72\":7,\"81\":1,\"85\":1,\"103\":1,\"107\":1,\"168\":8,\"172\":1,\"217\":1,\"253\":1,\"254\":1,\"278\":2}}],[\"是图像块的总数\",{\"1\":{\"109\":1}}],[\"是图像宽度\",{\"1\":{\"109\":1}}],[\"是图像高度\",{\"1\":{\"109\":1}}],[\"是图像每个通道的标准差\",{\"1\":{\"108\":1}}],[\"是图像每个通道的均值\",{\"1\":{\"108\":1}}],[\"是图像\",{\"1\":{\"11\":1,\"39\":1}}],[\"分为训练\",{\"1\":{\"232\":1}}],[\"分为三种响应类型\",{\"1\":{\"81\":1}}],[\"分词器的实现较为简单\",{\"1\":{\"224\":1}}],[\"分词器实现\",{\"0\":{\"224\":1}}],[\"分词过程\",{\"0\":{\"176\":1}}],[\"分隔\",{\"1\":{\"221\":1}}],[\"分隔符\",{\"1\":{\"81\":1}}],[\"分析比较2048单元的单层lstm和transformer\",{\"1\":{\"213\":1}}],[\"分析\",{\"0\":{\"213\":1}}],[\"分析能力\",{\"1\":{\"198\":1}}],[\"分解阶段\",{\"1\":{\"200\":1}}],[\"分母中\",{\"1\":{\"170\":1}}],[\"分母中的\",{\"1\":{\"170\":1}}],[\"分母是两者的并集\",{\"1\":{\"168\":1}}],[\"分子是预测和\",{\"1\":{\"168\":1}}],[\"分数为1\",{\"1\":{\"212\":1}}],[\"分数\",{\"1\":{\"167\":1,\"253\":1}}],[\"分数归一化\",{\"1\":{\"15\":1}}],[\"分批次预测\",{\"1\":{\"93\":1}}],[\"分批次从图像列表中取出一批图像\",{\"1\":{\"93\":1}}],[\"分配权重\",{\"1\":{\"57\":1}}],[\"分割性能\",{\"1\":{\"69\":1}}],[\"分割精度不高\",{\"1\":{\"69\":1}}],[\"分割任务依赖拼接机制\",{\"1\":{\"69\":1}}],[\"分割任务\",{\"0\":{\"68\":1}}],[\"分割网络\",{\"1\":{\"58\":1}}],[\"分割的整体结构\",{\"1\":{\"55\":1}}],[\"分割出\",{\"1\":{\"10\":1}}],[\"分别提供基础版\",{\"1\":{\"278\":1}}],[\"分别提取图像特征和文本特征\",{\"1\":{\"90\":1}}],[\"分别提取特征\",{\"1\":{\"53\":1}}],[\"分别适用于不同的场景\",{\"1\":{\"278\":1}}],[\"分别计算答案起始下标和结束下标预测得到的交叉熵损失\",{\"1\":{\"254\":1}}],[\"分别送入不同的\",{\"1\":{\"221\":1}}],[\"分别如下\",{\"1\":{\"100\":1}}],[\"分别基于lenet\",{\"1\":{\"93\":1}}],[\"分别是文本编码器\",{\"1\":{\"90\":1}}],[\"分别是\",{\"1\":{\"65\":1}}],[\"分别是最大值和它们的位置索引\",{\"1\":{\"49\":1,\"64\":1,\"66\":1}}],[\"分别从以下角度衡量模型表现\",{\"1\":{\"39\":1}}],[\"分类输出\",{\"1\":{\"226\":1}}],[\"分类结果\",{\"1\":{\"226\":1}}],[\"分类器共享词嵌入矩阵\",{\"1\":{\"226\":1}}],[\"分类器\",{\"1\":{\"226\":2}}],[\"分类层使用\",{\"1\":{\"211\":1}}],[\"分类阈值\",{\"1\":{\"155\":1}}],[\"分类头\",{\"1\":{\"114\":1}}],[\"分类标记\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"分类性能\",{\"1\":{\"69\":1}}],[\"分类性能略逊于多视角方法\",{\"1\":{\"69\":1}}],[\"分类精度略低\",{\"1\":{\"69\":1}}],[\"分类准确率\",{\"1\":{\"69\":1}}],[\"分类准确率仅下降约\",{\"1\":{\"62\":1}}],[\"分类任务是指对输入文本中的每个\",{\"1\":{\"256\":1}}],[\"分类任务的类别数\",{\"1\":{\"110\":1}}],[\"分类任务中对缺失点具有一定鲁棒性\",{\"1\":{\"69\":1}}],[\"分类任务\",{\"0\":{\"67\":1},\"1\":{\"85\":1}}],[\"分类类别数\",{\"1\":{\"58\":1}}],[\"分类每个点\",{\"1\":{\"55\":1}}],[\"分类判别能力\",{\"1\":{\"39\":1}}],[\"分类\",{\"1\":{\"39\":1,\"58\":1,\"164\":1,\"212\":1}}],[\"分类误差\",{\"1\":{\"35\":1,\"172\":1}}],[\"分组并在组内共享键\",{\"1\":{\"278\":2}}],[\"分组查询注意力\",{\"1\":{\"278\":2}}],[\"分组后的文本引导特征\",{\"1\":{\"30\":2}}],[\"分组操作实现了\",{\"1\":{\"29\":1}}],[\"分组\",{\"1\":{\"28\":1}}],[\"分区\",{\"1\":{\"22\":1}}],[\"分布一致性\",{\"1\":{\"39\":1}}],[\"分布\",{\"1\":{\"11\":2}}],[\"每种方法在不同任务上都优于其它方法\",{\"1\":{\"204\":1}}],[\"每种物体可以对应多种形状实例和功能类别\",{\"1\":{\"25\":2}}],[\"每家公司都去从头训练一个自己的大模型\",{\"1\":{\"179\":1}}],[\"每封电子邮件的实际分类取决于您选择的阈值\",{\"1\":{\"160\":1}}],[\"每行的总和表示所有预测正例\",{\"1\":{\"151\":1}}],[\"每行是一个\",{\"1\":{\"11\":1}}],[\"每一层transformer层能带来9\",{\"1\":{\"213\":1}}],[\"每一层bertlayer产生的key\",{\"1\":{\"103\":1}}],[\"每一个向量作为一个单独的输入\",{\"1\":{\"109\":1}}],[\"每一个patch作为一个token\",{\"1\":{\"109\":1}}],[\"每一行表示一张图像和所有文本之间的相似度\",{\"1\":{\"101\":1}}],[\"每次计算后返回本次可能需要缓存的key\",{\"1\":{\"103\":1}}],[\"每次迭代中\",{\"1\":{\"23\":1}}],[\"每层逐步下采样\",{\"1\":{\"58\":1}}],[\"每个矩形代表了一个数据状态\",{\"1\":{\"286\":1}}],[\"每个椭圆形代表了\",{\"1\":{\"286\":1}}],[\"每个头64维\",{\"1\":{\"271\":1}}],[\"每个子层\",{\"1\":{\"261\":1}}],[\"每个子词出现次数加1\",{\"1\":{\"175\":1}}],[\"每个子词的出现次数\",{\"1\":{\"175\":1}}],[\"每个位置的输出会通过一个独立的前馈神经网络进行进一步处理\",{\"1\":{\"261\":1}}],[\"每个词汇还同时保留产生该词汇的原始文章\",{\"1\":{\"223\":1}}],[\"每个词的每个字符后都加上空格\",{\"1\":{\"175\":1}}],[\"每个这些序列用作者的模型独立处理后通过一个\",{\"1\":{\"209\":1}}],[\"每个实例有输入字符的序列构成\",{\"1\":{\"208\":1}}],[\"每个像素都被视为图像的基本单位\",{\"1\":{\"164\":1}}],[\"每个列中的总和会显示所有真实正例\",{\"1\":{\"151\":1}}],[\"每个图像块的尺寸\",{\"1\":{\"109\":1}}],[\"每个图像位置对应的所有点云点\",{\"1\":{\"11\":1}}],[\"每个patch是三通道的小图片\",{\"1\":{\"109\":1}}],[\"每个output\",{\"1\":{\"102\":1}}],[\"每个query\",{\"1\":{\"101\":1}}],[\"每个query通道维度做特征融合\",{\"1\":{\"33\":1}}],[\"每个问题包含文本或图像上下文\",{\"1\":{\"81\":1}}],[\"每个格子表示是否有物体\",{\"1\":{\"71\":1}}],[\"每个卷积层后跟一个\",{\"1\":{\"66\":1}}],[\"每个原始点都有了一个新的特征向量\",{\"1\":{\"57\":1}}],[\"每个原始点都有了它最近的\",{\"1\":{\"57\":1}}],[\"每个原始点对应的\",{\"1\":{\"57\":1}}],[\"每个层是一个conv1d\",{\"1\":{\"57\":1}}],[\"每个结构中心点不变\",{\"1\":{\"53\":1}}],[\"每个关键点的多尺度特征表示\",{\"1\":{\"53\":1}}],[\"每个关键点对应的局部区域点和特征\",{\"1\":{\"49\":1}}],[\"每个尺度可以有不同的网络深度和宽度\",{\"1\":{\"53\":1}}],[\"每个尺度\",{\"1\":{\"52\":1}}],[\"每个半径定义了一个局部邻域的大小\",{\"1\":{\"52\":1}}],[\"每个查询点点局部特征\",{\"1\":{\"49\":1}}],[\"每个查询点对所有原始点的距离\",{\"1\":{\"49\":1}}],[\"每个邻域内采样的关键点数量\",{\"1\":{\"49\":2}}],[\"每个区域中点的数量𝐾和query的半径𝑟\",{\"1\":{\"47\":1}}],[\"每个\",{\"1\":{\"33\":5,\"123\":2,\"128\":1,\"130\":1,\"253\":3}}],[\"每个文本token询问所有点key后\",{\"1\":{\"29\":1}}],[\"每个形状实例随机匹配一个与其功能类型一致的问题\",{\"1\":{\"23\":1}}],[\"每个物体类别可有多个形状实例\",{\"1\":{\"22\":1}}],[\"每个物体都以点云形式表示\",{\"1\":{\"19\":1}}],[\"每个点都代表一个阈值\",{\"1\":{\"162\":1}}],[\"每个点都需要全局上下文\",{\"1\":{\"68\":1}}],[\"每个点通常包含\",{\"1\":{\"71\":1}}],[\"每个点有\",{\"1\":{\"57\":1,\"64\":1,\"71\":1}}],[\"每个点到当前所有已选中心点的最小距离\",{\"1\":{\"49\":1}}],[\"每个点云约\",{\"1\":{\"24\":1}}],[\"每个点云点对应的所有图像位置\",{\"1\":{\"11\":1}}],[\"每个点云点与图像中每个位置之间的相似度得分\",{\"1\":{\"11\":1}}],[\"每个点是否属于当前问题描述的功能区域\",{\"1\":{\"21\":1}}],[\"每个点的分类结果\",{\"1\":{\"58\":1}}],[\"每个点的三个邻近点的权重\",{\"1\":{\"57\":1}}],[\"每个点的值\",{\"1\":{\"33\":1}}],[\"每个点的\",{\"1\":{\"16\":1}}],[\"每列是一个\",{\"1\":{\"11\":1}}],[\"jx\",{\"1\":{\"118\":1}}],[\"javascript\",{\"1\":{\"288\":1}}],[\"javaer\",{\"1\":{\"2\":1}}],[\"jarvis\",{\"1\":{\"282\":1}}],[\"jaccard\",{\"0\":{\"168\":1},\"1\":{\"168\":7}}],[\"jax\",{\"1\":{\"118\":1}}],[\"json\",{\"1\":{\"107\":6,\"223\":4,\"224\":4,\"227\":1,\"232\":1}}],[\"jft\",{\"1\":{\"96\":2}}],[\"jpeg\",{\"1\":{\"93\":1,\"95\":1}}],[\"jpg\",{\"1\":{\"93\":1,\"95\":1,\"107\":2}}],[\"judge\",{\"1\":{\"82\":1}}],[\"j\",{\"1\":{\"53\":3,\"93\":2,\"95\":2,\"227\":5}}],[\"join\",{\"1\":{\"25\":3,\"40\":2,\"93\":3,\"94\":1,\"95\":4,\"107\":3,\"175\":3,\"176\":2,\"224\":2}}],[\"joint\",{\"1\":{\"11\":4,\"15\":2}}],[\"使开发者能够通过终端与\",{\"1\":{\"289\":1}}],[\"使它们成为了解决复杂问题和应用于多领域的强大工具\",{\"1\":{\"280\":1}}],[\"使它们能够通过阅读大量文本来深入理解语言规则和模式\",{\"1\":{\"277\":1}}],[\"使模型能够更准确地识别何时以及如何调用外部工具\",{\"1\":{\"278\":1}}],[\"使模型行为更可预测和可控\",{\"1\":{\"278\":1}}],[\"使模型聚焦难分类样本\",{\"1\":{\"169\":1}}],[\"使模型更关注难分类的样本\",{\"1\":{\"169\":1}}],[\"使特征分布更稳定\",{\"1\":{\"65\":1}}],[\"使其在长文本理解和复杂任务处理方面具有更强的优势\",{\"1\":{\"278\":1}}],[\"使其符合模型的输入要求\",{\"1\":{\"93\":1}}],[\"使其更好地理解和执行用户给出的自然语言指令\",{\"1\":{\"85\":1}}],[\"使其标准化\",{\"1\":{\"65\":1}}],[\"使其姿态统一\",{\"1\":{\"64\":1}}],[\"使其简短但仍有意义\",{\"1\":{\"20\":1}}],[\"使网络能够应对实际中各种密度变换的情况\",{\"1\":{\"53\":1}}],[\"使网络能学习不同采样密度下局部点云特征的提取\",{\"1\":{\"53\":1}}],[\"使局部特征的表示不够精确\",{\"1\":{\"47\":1}}],[\"使得开发者可以更容易地构建复杂和强大的应用程序\",{\"1\":{\"289\":1}}],[\"使得开发者能够轻松地将私有数据与\",{\"1\":{\"288\":1}}],[\"使得它们可以理解和生成不同媒体类型的内容\",{\"1\":{\"279\":1}}],[\"使得它们能够在统一的语义空间中进行有效的跨模态交互\",{\"1\":{\"11\":2}}],[\"使得要充分做区分地训练模型非常有挑战性\",{\"1\":{\"203\":1}}],[\"使得模型可以在资源有限的设备上进行训练和部署\",{\"1\":{\"192\":1}}],[\"使得模型在下游任务上的表现逐渐优化\",{\"1\":{\"187\":1}}],[\"使得模型在训练过程中可以更灵活地平衡这两部分损失\",{\"1\":{\"172\":1}}],[\"使得\",{\"1\":{\"110\":1,\"283\":1}}],[\"使得预训练模型能够直接应用于下游任务\",{\"1\":{\"92\":1}}],[\"使得后续指令调优时\",{\"1\":{\"80\":1}}],[\"使得每个点的权重之和为1\",{\"1\":{\"57\":1}}],[\"使得采样点在整个点云空间中分布尽可能均匀\",{\"1\":{\"49\":1}}],[\"使得在它们之间可以共享学习到的特征表示的权重\",{\"1\":{\"43\":1}}],[\"使问题更具体地连接目标对象的功能\",{\"1\":{\"20\":1}}],[\"使用统一的大模型可以极大地提高研发效率\",{\"1\":{\"281\":1}}],[\"使用文心\",{\"1\":{\"278\":2}}],[\"使用相似的架构和预训练任务\",{\"1\":{\"277\":1}}],[\"使用掩码机制确保模型在生成目标序列时只能看到当前及之前的词\",{\"1\":{\"261\":1}}],[\"使用了如下逻辑\",{\"1\":{\"253\":1}}],[\"使用了一个偏置项\",{\"1\":{\"35\":1}}],[\"使用pycharm导入项目\",{\"1\":{\"232\":1}}],[\"使用pointnet++编码点云\",{\"1\":{\"27\":1}}],[\"使用nltk库\",{\"1\":{\"223\":1}}],[\"使用nltk库提供的sent\",{\"1\":{\"176\":1}}],[\"使用无监督预训练来提升在判别式任务上的表现是机器学习研究的长期目标\",{\"1\":{\"214\":1}}],[\"使用无监督预训练超参数设置\",{\"1\":{\"211\":1}}],[\"使用的是tnews数据集\",{\"1\":{\"232\":1}}],[\"使用的是transformer\",{\"1\":{\"204\":1}}],[\"使用的数据集\",{\"1\":{\"212\":1}}],[\"使用学习的位置嵌入\",{\"1\":{\"211\":1}}],[\"使用标准的语言模型目标并最大化其似然\",{\"1\":{\"207\":1}}],[\"使用标注好的指令\",{\"1\":{\"78\":1}}],[\"使用对应特定任务的监督目标来调整这些参数\",{\"1\":{\"204\":1}}],[\"使用对称函数\",{\"1\":{\"62\":1}}],[\"使用复杂的学习方案以及添加辅助学习目标的组合\",{\"1\":{\"204\":1}}],[\"使用不同的损失函数进行训练\",{\"1\":{\"173\":1}}],[\"使用不同句式结构\",{\"1\":{\"20\":1}}],[\"使用transformer架构为未来的多模态统一提供了可能性\",{\"1\":{\"119\":1}}],[\"使用截断正态分布初始化位置嵌入\",{\"1\":{\"111\":1,\"114\":1}}],[\"使用截断正态分布初始化分类标记\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"使用全连接\",{\"1\":{\"104\":1}}],[\"使用交叉熵损失衡量生成与真实之间的差异\",{\"1\":{\"103\":1}}],[\"使用交叉熵损失函数\",{\"1\":{\"80\":1}}],[\"使用adamw优化器\",{\"1\":{\"90\":1}}],[\"使用倒数第二层视觉特征更有利于细节理解\",{\"1\":{\"83\":1}}],[\"使用较小的\",{\"1\":{\"83\":1}}],[\"使用可训练的投影矩阵\",{\"1\":{\"81\":1}}],[\"使用强化学习优化策略\",{\"1\":{\"78\":1}}],[\"使用上述人类偏好数据\",{\"1\":{\"78\":1}}],[\"使用神经网络直接学习对称函数\",{\"1\":{\"72\":1}}],[\"使用分块策略\",{\"1\":{\"69\":1}}],[\"使用分割头预测最终的\",{\"0\":{\"16\":1},\"1\":{\"10\":1}}],[\"使用多层\",{\"1\":{\"68\":1}}],[\"使用多个不同大小的邻域球\",{\"1\":{\"53\":1}}],[\"使用多个\",{\"1\":{\"49\":1}}],[\"使用多个阈值计算\",{\"1\":{\"39\":1}}],[\"使用反距离加权\",{\"1\":{\"57\":1}}],[\"使用最大池化聚合局部信息\",{\"1\":{\"53\":1}}],[\"使用farthest\",{\"1\":{\"46\":1}}],[\"使用mini\",{\"1\":{\"45\":1}}],[\"使用第4列作为默认掩码\",{\"1\":{\"40\":1}}],[\"使用另一个注意力模块\",{\"1\":{\"31\":2}}],[\"使用一个轻量级的交叉注意力模块\",{\"1\":{\"29\":1}}],[\"使用robert编码文本\",{\"1\":{\"27\":1}}],[\"使用残差连接\",{\"1\":{\"15\":1}}],[\"使用自注意力机制提炼两个模态之间的语义一致性\",{\"1\":{\"11\":1}}],[\"使用\",{\"1\":{\"10\":4,\"13\":1,\"15\":4,\"16\":1,\"20\":2,\"27\":1,\"30\":1,\"33\":2,\"39\":2,\"49\":2,\"53\":1,\"55\":2,\"58\":2,\"62\":1,\"64\":1,\"66\":2,\"68\":1,\"69\":2,\"78\":1,\"80\":2,\"81\":2,\"82\":1,\"85\":1,\"100\":1,\"103\":1,\"192\":1,\"211\":1,\"226\":2,\"255\":1}}],[\"0+cu113\",{\"1\":{\"259\":1}}],[\"03\",{\"1\":{\"232\":1}}],[\"01\",{\"1\":{\"227\":1}}],[\"0或1\",{\"1\":{\"169\":1}}],[\"02\",{\"1\":{\"110\":1,\"111\":2,\"114\":2}}],[\"02413\",{\"1\":{\"42\":1}}],[\"08\",{\"1\":{\"83\":1}}],[\"08485\",{\"1\":{\"77\":1}}],[\"05\",{\"1\":{\"82\":1}}],[\"000\",{\"1\":{\"80\":1,\"81\":4,\"211\":1}}],[\"00593\",{\"1\":{\"59\":1}}],[\"093\",{\"1\":{\"32\":1,\"39\":1}}],[\"0~1\",{\"1\":{\"16\":1}}],[\"0\",{\"0\":{\"76\":1,\"77\":1,\"107\":1},\"1\":{\"11\":3,\"13\":4,\"15\":2,\"16\":12,\"21\":1,\"25\":4,\"27\":11,\"32\":4,\"33\":2,\"35\":4,\"37\":2,\"38\":2,\"39\":17,\"40\":9,\"49\":12,\"50\":2,\"53\":14,\"55\":1,\"57\":6,\"58\":2,\"64\":8,\"65\":1,\"66\":1,\"68\":1,\"76\":1,\"77\":1,\"80\":8,\"82\":1,\"83\":1,\"91\":1,\"93\":1,\"95\":1,\"100\":2,\"101\":2,\"102\":5,\"103\":9,\"104\":3,\"107\":2,\"108\":23,\"109\":3,\"110\":3,\"111\":3,\"112\":2,\"113\":3,\"114\":6,\"152\":1,\"153\":1,\"154\":3,\"155\":1,\"157\":2,\"159\":4,\"160\":6,\"162\":1,\"166\":2,\"167\":1,\"168\":6,\"169\":6,\"170\":4,\"172\":10,\"176\":3,\"189\":3,\"190\":10,\"211\":6,\"212\":1,\"219\":9,\"223\":2,\"224\":1,\"225\":2,\"226\":1,\"227\":3,\"228\":2,\"232\":1,\"233\":29,\"236\":1,\"238\":1,\"240\":1,\"241\":2,\"244\":2,\"248\":1,\"254\":3,\"255\":1,\"256\":1,\"259\":1,\"266\":1,\"269\":1,\"271\":3,\"278\":15}}],[\"04744\",{\"1\":{\"7\":1}}],[\"防止出现\",{\"1\":{\"172\":1}}],[\"防止被背景淹没\",{\"1\":{\"169\":1}}],[\"防止模型只关注简单样本\",{\"1\":{\"169\":1}}],[\"防止模型偏向特定句式或长度\",{\"1\":{\"20\":1}}],[\"防止前景点\",{\"1\":{\"169\":1}}],[\"防止除零错误\",{\"1\":{\"166\":1,\"167\":1,\"168\":1,\"169\":1}}],[\"防止除以零\",{\"1\":{\"166\":2,\"167\":1,\"170\":1,\"172\":1}}],[\"防止内积过大导致\",{\"1\":{\"136\":1}}],[\"防止过拟合\",{\"1\":{\"80\":1,\"113\":4}}],[\"防止忽略小区域\",{\"1\":{\"35\":1}}],[\"防止其影响后续计算\",{\"1\":{\"33\":1}}],[\"防止\",{\"1\":{\"11\":1,\"172\":2}}],[\"加速收敛\",{\"1\":{\"208\":1}}],[\"加单位矩阵\",{\"1\":{\"64\":1}}],[\"加上这样一句话\",{\"1\":{\"197\":1}}],[\"加上位置嵌入并进行随机丢弃\",{\"1\":{\"111\":1,\"114\":1}}],[\"加上全局特征后\",{\"1\":{\"68\":1}}],[\"加上单位矩阵作为初始偏置\",{\"1\":{\"64\":1}}],[\"加上原始\",{\"1\":{\"29\":1}}],[\"加入到验证集中\",{\"1\":{\"290\":1}}],[\"加入数值稳定性处理\",{\"1\":{\"172\":1}}],[\"加入数据增强后缓解\",{\"1\":{\"69\":1}}],[\"加入平滑项防止除以零\",{\"1\":{\"168\":1}}],[\"加入正则项约束变换矩阵接近正交\",{\"1\":{\"62\":1}}],[\"加入\",{\"1\":{\"32\":2,\"64\":1,\"166\":1,\"167\":2}}],[\"加载检查点\",{\"1\":{\"227\":1}}],[\"加载该模型后\",{\"1\":{\"118\":1}}],[\"加载预训练好的vit\",{\"1\":{\"118\":1}}],[\"加载预训练模型\",{\"0\":{\"118\":1}}],[\"加载预训练权重\",{\"1\":{\"40\":1}}],[\"加载模型和处理器\",{\"1\":{\"93\":1,\"95\":1}}],[\"加载模型配置\",{\"1\":{\"40\":1}}],[\"加载训练集\",{\"1\":{\"37\":1}}],[\"加载的标注数据中每个样本的组织形式如下\",{\"1\":{\"25\":1}}],[\"加载58种物体\",{\"1\":{\"25\":1}}],[\"加载点云数据\",{\"1\":{\"25\":1,\"40\":1}}],[\"加载标注数据\",{\"1\":{\"25\":1,\"40\":1}}],[\"加权惩罚项\",{\"1\":{\"170\":1}}],[\"加权\",{\"1\":{\"168\":1,\"208\":1}}],[\"加权和\",{\"1\":{\"72\":1}}],[\"加权平均插值\",{\"1\":{\"57\":1}}],[\"加权平均系数\",{\"1\":{\"57\":1}}],[\"加权交叉熵损失\",{\"1\":{\"172\":1}}],[\"加权交叉熵\",{\"1\":{\"35\":1}}],[\"加权后的输出\",{\"1\":{\"15\":2}}],[\"加权总损失\",{\"1\":{\"10\":1}}],[\"加之原本在github上开源的代码后续被下架\",{\"1\":{\"7\":1}}],[\"计算机视觉\",{\"0\":{\"274\":1}}],[\"计算掩码语言损失\",{\"1\":{\"251\":1}}],[\"计算需要填充的长度\",{\"1\":{\"233\":1}}],[\"计算资源\",{\"1\":{\"217\":1,\"285\":1}}],[\"计算量略高于ce\",{\"1\":{\"169\":1}}],[\"计算量大\",{\"1\":{\"60\":1}}],[\"计算并集\",{\"1\":{\"168\":1}}],[\"计算交集\",{\"1\":{\"166\":1,\"167\":1,\"168\":1}}],[\"计算交叉熵损失\",{\"1\":{\"102\":1,\"254\":1}}],[\"计算得到的\",{\"1\":{\"166\":1,\"168\":1,\"169\":1,\"170\":1,\"172\":1}}],[\"计算梯度\",{\"1\":{\"114\":1}}],[\"计算预测输出与真实标签之间的\",{\"1\":{\"167\":1,\"168\":1,\"169\":1}}],[\"计算预测结果与真实标签之间的交叉熵损失\",{\"1\":{\"114\":1}}],[\"计算预测正确的样本数\",{\"1\":{\"114\":1}}],[\"计算公式如下\",{\"1\":{\"112\":1,\"123\":1}}],[\"计算图像块的数量\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"计算图像块的总数\",{\"1\":{\"109\":1}}],[\"计算图像嵌入向量列表中每个图像对应的嵌入向量和传入的分类文本嵌入向量的余弦相似度\",{\"1\":{\"93\":1}}],[\"计算网格大小\",{\"1\":{\"109\":1}}],[\"计算上下文表示\",{\"1\":{\"103\":1}}],[\"计算注意力分数\",{\"1\":{\"103\":1}}],[\"计算序列长度\",{\"1\":{\"102\":1}}],[\"计算一个\",{\"1\":{\"101\":1}}],[\"计算这批图像嵌入向量中每一个图像嵌入向量和我们所有分类文本嵌入向量的余弦相似度\",{\"1\":{\"93\":1}}],[\"计算余弦相似度\",{\"1\":{\"91\":1}}],[\"计算缩放的余弦相似度\",{\"1\":{\"90\":1}}],[\"计算点集的分布特性\",{\"1\":{\"72\":1}}],[\"计算点云质心\",{\"1\":{\"40\":1}}],[\"计算复杂度高\",{\"1\":{\"71\":1}}],[\"计算变换矩阵与其转置相乘后与单位矩阵之间的距离\",{\"1\":{\"65\":1}}],[\"计算每个可能的阈值\",{\"1\":{\"159\":1}}],[\"计算每个注意力头的维度\",{\"1\":{\"113\":1}}],[\"计算每个query\",{\"1\":{\"101\":1}}],[\"计算每个原始点和下采样点之间的距离\",{\"1\":{\"57\":1}}],[\"计算每个查询点\",{\"1\":{\"49\":1}}],[\"计算权重\",{\"1\":{\"57\":1}}],[\"计算原始点与下采样点之间的距离矩阵\",{\"1\":{\"57\":1}}],[\"计算当前批次中所有序列的实际最大长度\",{\"1\":{\"235\":1}}],[\"计算当前中心点与所有点之间的欧氏距离平方\",{\"1\":{\"49\":1}}],[\"计算当前语言输入中有多少个有效\",{\"1\":{\"13\":1}}],[\"计算效率问题\",{\"1\":{\"47\":1}}],[\"计算成本\",{\"1\":{\"46\":1}}],[\"计算最大半径\",{\"1\":{\"40\":1}}],[\"计算平均\",{\"1\":{\"39\":1}}],[\"计算曲线下面积\",{\"1\":{\"39\":1}}],[\"计算指标包括\",{\"1\":{\"39\":1}}],[\"计算损失的时候\",{\"1\":{\"218\":1}}],[\"计算损失\",{\"1\":{\"38\":1}}],[\"计算负类\",{\"1\":{\"35\":1}}],[\"计算正类\",{\"1\":{\"35\":1}}],[\"计算保持稳定\",{\"1\":{\"35\":1}}],[\"计算\",{\"1\":{\"10\":1,\"15\":2,\"39\":4,\"101\":1,\"112\":1,\"166\":2,\"167\":2,\"168\":1,\"169\":2,\"170\":1,\"172\":1}}],[\"可组合性\",{\"1\":{\"288\":1}}],[\"可观察性\",{\"1\":{\"288\":1}}],[\"可解释性相对较低\",{\"1\":{\"285\":1}}],[\"可解释性\",{\"1\":{\"285\":1}}],[\"可规避捷径行为\",{\"1\":{\"278\":1}}],[\"可在\",{\"1\":{\"278\":1}}],[\"可考虑换成\",{\"1\":{\"224\":1}}],[\"可挪到其他地方实现\",{\"1\":{\"224\":1}}],[\"可见这两句话就不是连续的\",{\"1\":{\"219\":1}}],[\"可调性强\",{\"1\":{\"172\":1}}],[\"可调\",{\"1\":{\"169\":1}}],[\"可处理连续值掩码\",{\"1\":{\"167\":1}}],[\"可处理连续概率值\",{\"1\":{\"166\":1}}],[\"可最大限度地提高\",{\"1\":{\"162\":1}}],[\"可使用\",{\"1\":{\"140\":1,\"144\":1}}],[\"可使用成熟的\",{\"1\":{\"71\":1}}],[\"可学习位置嵌入\",{\"1\":{\"111\":1}}],[\"可学习query\",{\"1\":{\"104\":1}}],[\"可学习的位置编码\",{\"1\":{\"33\":1}}],[\"可认为是模型参数一部分\",{\"1\":{\"100\":1}}],[\"可复用已有大模型权重\",{\"1\":{\"85\":1}}],[\"可用于多模态任务\",{\"1\":{\"85\":1}}],[\"可为\",{\"1\":{\"57\":1}}],[\"可为空\",{\"1\":{\"55\":1}}],[\"可改进的地方\",{\"1\":{\"47\":1}}],[\"可能缺乏必要的推理能力\",{\"1\":{\"283\":1}}],[\"可能是被替换的词\",{\"1\":{\"218\":1}}],[\"可能是经验设定\",{\"1\":{\"35\":1}}],[\"可能会有人这么问\",{\"1\":{\"194\":1}}],[\"可能更为合适\",{\"1\":{\"162\":1}}],[\"可能遗漏重要细节\",{\"1\":{\"69\":1}}],[\"可能不足以代表复杂的局部结构\",{\"1\":{\"69\":1}}],[\"可能无法捕捉重要的几何细节\",{\"1\":{\"46\":1}}],[\"可能导致样本在高密度区域内过度集中\",{\"1\":{\"46\":1}}],[\"可能存在的问题\",{\"1\":{\"46\":1}}],[\"可视化示意\",{\"0\":{\"137\":1}}],[\"可视化当前物体点云\",{\"1\":{\"40\":1}}],[\"可视化功能区域预测结果\",{\"1\":{\"40\":1}}],[\"可选参数\",{\"1\":{\"167\":1,\"168\":1,\"169\":1}}],[\"可选颜色\",{\"1\":{\"71\":1}}],[\"可选属性\",{\"1\":{\"71\":1}}],[\"可选地拼接\",{\"1\":{\"55\":1}}],[\"可选\",{\"0\":{\"70\":1},\"1\":{\"40\":1,\"49\":1,\"66\":1,\"166\":1,\"170\":1,\"172\":1,\"187\":1}}],[\"可以充分发挥大语言模型的强大能力\",{\"1\":{\"291\":1}}],[\"可以基本实现目标的\",{\"1\":{\"291\":1}}],[\"可以追溯到具体的数据来源\",{\"1\":{\"285\":1}}],[\"可以根据特定风格或术语调整\",{\"1\":{\"285\":1}}],[\"可以根据图像描述生成合理的文字解释\",{\"1\":{\"80\":1}}],[\"可以达到数十亿甚至数千亿个参数\",{\"1\":{\"279\":1}}],[\"可以保持效果的情况下\",{\"1\":{\"278\":1}}],[\"可以显示部分思维链\",{\"1\":{\"278\":1}}],[\"可以通过学习上下文来解决少样本任务\",{\"1\":{\"277\":1}}],[\"可以通过调整这些超参数来进一步优化损失函数的性能\",{\"1\":{\"173\":1}}],[\"可以实现巨大的收益\",{\"1\":{\"203\":1}}],[\"可以用于多种语言\",{\"1\":{\"279\":1}}],[\"可以用\",{\"1\":{\"190\":1}}],[\"可以被看作是\",{\"1\":{\"189\":1}}],[\"可以阅读这篇论文\",{\"1\":{\"185\":1}}],[\"可以在更快的时间内响应\",{\"1\":{\"278\":1}}],[\"可以在保证模型效果的同时\",{\"1\":{\"185\":1}}],[\"可以在下游任务中获得较好的迁移效果\",{\"1\":{\"105\":1}}],[\"可以媲美全量微调的效果了\",{\"1\":{\"185\":1}}],[\"可以引导大模型有更加出色的表现\",{\"1\":{\"183\":1}}],[\"可以参见lora\",{\"1\":{\"184\":1}}],[\"可以参见\",{\"1\":{\"182\":1,\"183\":1,\"185\":1}}],[\"可以参考论文\",{\"1\":{\"92\":1}}],[\"可以多个方案一起\",{\"1\":{\"180\":1}}],[\"可以有效控制词汇表大小\",{\"1\":{\"174\":1}}],[\"可以考虑使用tversky\",{\"1\":{\"173\":1}}],[\"可以使用\",{\"1\":{\"144\":1,\"147\":1}}],[\"可以使用预训练的\",{\"1\":{\"117\":1}}],[\"可以不同\",{\"0\":{\"135\":1}}],[\"可以独立于设计\",{\"1\":{\"131\":1}}],[\"可以利用其自注意力机制捕捉特征之间的长距离依赖关系\",{\"1\":{\"117\":1}}],[\"可以看这篇文章\",{\"1\":{\"113\":1}}],[\"可以看到当epochs增大时\",{\"1\":{\"117\":1}}],[\"可以看到\",{\"1\":{\"111\":2,\"112\":1,\"115\":1,\"116\":1,\"219\":1}}],[\"可以看到8个图像\",{\"1\":{\"91\":1}}],[\"可以看到对于要预测的8个图像\",{\"1\":{\"91\":1}}],[\"可以变成一个\",{\"1\":{\"109\":1}}],[\"可以直接通过类名调用\",{\"1\":{\"107\":1}}],[\"可以将\",{\"1\":{\"287\":1}}],[\"可以将现在的各种大模型通过轻量微调变成各个不同领域的专业模型\",{\"1\":{\"192\":1}}],[\"可以将低秩矩阵\",{\"1\":{\"187\":1}}],[\"可以将搜索范围限制在满足这些性质的模型子空间内\",{\"1\":{\"105\":1}}],[\"可以将其平移到坐标系的中心\",{\"1\":{\"43\":1}}],[\"可以和所有的query\",{\"1\":{\"103\":1}}],[\"可以和所有自己的tokens做attention\",{\"1\":{\"103\":1}}],[\"可以学习到如何更好地结合文本提取图片信息\",{\"1\":{\"100\":1}}],[\"可以得到每个类别的预测概率\",{\"1\":{\"91\":1}}],[\"可以概括为以下两个主要步骤\",{\"1\":{\"91\":1}}],[\"可以处理任意顺序的点集\",{\"1\":{\"62\":1}}],[\"可以先去了解一下python中的高级索引机制\",{\"1\":{\"49\":1}}],[\"可以理解为\",{\"1\":{\"43\":1}}],[\"可以避免因为输出缓存导致日志卡在某一行不输出的问题\",{\"1\":{\"40\":1}}],[\"可以是\",{\"1\":{\"39\":1}}],[\"可以确保语言语义不会丢失\",{\"1\":{\"29\":1}}],[\"可以加微信备注来意\",{\"1\":{\"2\":1}}],[\"可操作性\",{\"1\":{\"16\":1}}],[\"可操作性热图\",{\"0\":{\"16\":1},\"1\":{\"10\":1}}],[\"9b\",{\"1\":{\"278\":5}}],[\"99\",{\"1\":{\"152\":1}}],[\"999\",{\"1\":{\"37\":1}}],[\"95\",{\"1\":{\"93\":1}}],[\"92\",{\"1\":{\"81\":2,\"82\":1}}],[\"914\",{\"1\":{\"80\":1}}],[\"9027\",{\"1\":{\"228\":1}}],[\"90\",{\"1\":{\"69\":1,\"81\":1,\"277\":1}}],[\"960\",{\"1\":{\"228\":1}}],[\"96\",{\"1\":{\"53\":1,\"83\":1}}],[\"9\",{\"0\":{\"15\":1},\"1\":{\"10\":1,\"37\":1,\"39\":1,\"64\":4,\"69\":1,\"104\":1,\"140\":1,\"169\":1,\"172\":1,\"203\":1,\"204\":1,\"232\":1,\"259\":1,\"277\":1,\"278\":5,\"288\":1}}],[\"输入越长\",{\"1\":{\"283\":1}}],[\"输入与输出的关系\",{\"1\":{\"255\":1}}],[\"输入会变成如下结构\",{\"1\":{\"253\":1}}],[\"输入格式\",{\"1\":{\"253\":1}}],[\"输入数据格式\",{\"1\":{\"233\":1}}],[\"输入通过作者的预训练模型会得到最好的\",{\"1\":{\"208\":1}}],[\"输入序列\",{\"1\":{\"178\":1,\"224\":1,\"257\":1}}],[\"输入维度\",{\"0\":{\"128\":1}}],[\"输入到\",{\"1\":{\"117\":1}}],[\"输入到mlp\",{\"1\":{\"110\":1}}],[\"输入加上经过归一化和\",{\"1\":{\"112\":1}}],[\"输入加上经过归一化和注意力层处理后的输出\",{\"1\":{\"112\":1}}],[\"输入encoder的最左侧部分添加了一个0\",{\"1\":{\"110\":1}}],[\"输入的图像张量\",{\"1\":{\"109\":1}}],[\"输入的图片尺寸必须为224x224\",{\"1\":{\"108\":1}}],[\"输入的图片尺寸并不是自定义的\",{\"1\":{\"108\":1}}],[\"输入样本\",{\"1\":{\"104\":1}}],[\"输入包含三部分\",{\"1\":{\"100\":1}}],[\"输入image\",{\"1\":{\"91\":1}}],[\"输入是结构化的\",{\"1\":{\"209\":1}}],[\"输入是一个\",{\"1\":{\"133\":1}}],[\"输入是一个指令\",{\"1\":{\"78\":1}}],[\"输入是原始点云\",{\"1\":{\"64\":1}}],[\"输入形式\",{\"1\":{\"69\":1}}],[\"输入标准化\",{\"0\":{\"64\":1}}],[\"输入特征的通道数\",{\"1\":{\"57\":1}}],[\"输入特征维度\",{\"1\":{\"15\":1}}],[\"输入输出示例\",{\"1\":{\"55\":1}}],[\"输入点云变换矩阵\",{\"1\":{\"66\":1}}],[\"输入点云变换网络\",{\"1\":{\"66\":1}}],[\"输入点云可能来自不同角度\",{\"1\":{\"64\":1}}],[\"输入点云可能缺失或含有异常点\",{\"1\":{\"62\":1}}],[\"输入点云可能缺失或包含噪声\",{\"1\":{\"61\":1}}],[\"输入点云数据\",{\"1\":{\"40\":2}}],[\"输入点的特征维度\",{\"1\":{\"49\":1}}],[\"输入\",{\"1\":{\"29\":1,\"30\":1,\"31\":1,\"35\":1,\"39\":2,\"55\":1,\"57\":1,\"58\":1,\"103\":1,\"126\":1,\"128\":1,\"166\":1,\"167\":1,\"226\":1,\"270\":1}}],[\"输入为\",{\"1\":{\"16\":1,\"109\":1}}],[\"输入图像被分割成\",{\"1\":{\"110\":1}}],[\"输入图像的通道数\",{\"1\":{\"109\":1,\"110\":1}}],[\"输入图像的尺寸\",{\"1\":{\"109\":1,\"110\":1}}],[\"输入图像\",{\"1\":{\"10\":1,\"80\":1,\"81\":1}}],[\"输出解析等都来自这个库\",{\"1\":{\"289\":1}}],[\"输出解析\",{\"1\":{\"288\":1}}],[\"输出解释\",{\"1\":{\"253\":1}}],[\"输出层\",{\"1\":{\"261\":1}}],[\"输出后\",{\"1\":{\"257\":1}}],[\"输出做问答预测\",{\"1\":{\"253\":1}}],[\"输出时将\",{\"1\":{\"189\":1}}],[\"输出序列y\",{\"1\":{\"178\":1}}],[\"输出示例\",{\"1\":{\"143\":1}}],[\"输出维度为\",{\"1\":{\"226\":1}}],[\"输出维度\",{\"0\":{\"128\":1}}],[\"输出通道\",{\"1\":{\"109\":1}}],[\"输出不一致时\",{\"1\":{\"82\":1}}],[\"输出是对这个回答的评分\",{\"1\":{\"78\":1}}],[\"输出是模型应该生成的响应\",{\"1\":{\"78\":1}}],[\"输出是一个变换矩阵\",{\"1\":{\"64\":1}}],[\"输出结果之后\",{\"1\":{\"114\":1}}],[\"输出结果都保持不变\",{\"1\":{\"72\":1}}],[\"输出结果\",{\"1\":{\"57\":1}}],[\"输出最终的插值后特征\",{\"1\":{\"57\":1}}],[\"输出的特征送入一个小型\",{\"1\":{\"55\":1}}],[\"输出的隐藏状态映射回原始嵌入维度\",{\"1\":{\"14\":1}}],[\"输出类别概率\",{\"1\":{\"55\":1}}],[\"输出类别数\",{\"1\":{\"50\":1}}],[\"输出形状为\",{\"1\":{\"49\":2}}],[\"输出一个字符\",{\"1\":{\"40\":1}}],[\"输出张量\",{\"1\":{\"33\":1}}],[\"输出增强后的点特征\",{\"1\":{\"32\":1}}],[\"输出分组标记\",{\"1\":{\"29\":1}}],[\"输出范围\",{\"1\":{\"16\":1,\"39\":1,\"168\":1}}],[\"输出为\",{\"1\":{\"16\":1,\"58\":1}}],[\"输出每个点的类别概率\",{\"1\":{\"58\":1}}],[\"输出每个点的\",{\"1\":{\"16\":2}}],[\"输出每个点云点的\",{\"1\":{\"16\":1}}],[\"输出\",{\"1\":{\"10\":1,\"16\":1,\"31\":1,\"48\":1,\"55\":1,\"57\":1,\"66\":1,\"126\":1,\"128\":1,\"226\":1,\"253\":1,\"287\":1}}],[\"输出映射回合适维度\",{\"1\":{\"10\":1}}],[\"8b\",{\"1\":{\"278\":5}}],[\"8个头\",{\"1\":{\"271\":1}}],[\"80\",{\"1\":{\"218\":1,\"224\":2}}],[\"8600000\",{\"1\":{\"115\":1}}],[\"86×1000000\",{\"1\":{\"115\":1}}],[\"86m\",{\"1\":{\"115\":1}}],[\"89\",{\"1\":{\"69\":1}}],[\"85\",{\"1\":{\"69\":1}}],[\"87\",{\"1\":{\"32\":1,\"39\":1}}],[\"870\",{\"1\":{\"20\":2,\"22\":1,\"24\":1}}],[\"82\",{\"1\":{\"32\":1}}],[\"8\",{\"0\":{\"14\":1,\"26\":1},\"1\":{\"10\":1,\"16\":2,\"32\":1,\"37\":1,\"39\":2,\"53\":1,\"57\":1,\"58\":1,\"140\":1,\"169\":1,\"175\":1,\"176\":1,\"191\":1,\"192\":2,\"212\":1,\"213\":1,\"223\":3,\"224\":1,\"271\":1,\"278\":1}}],[\"返回值形如\",{\"1\":{\"253\":1}}],[\"返回两个任务的结果\",{\"1\":{\"226\":1}}],[\"返回构建得到的单个样本列表\",{\"1\":{\"225\":1}}],[\"返回处理后的句子列表\",{\"1\":{\"223\":1}}],[\"返回合并最高频字符对后的vocab\",{\"1\":{\"175\":1}}],[\"返回分类标记对应的特征\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"返回数据集中图像的数量\",{\"1\":{\"107\":1}}],[\"返回字典格式结果\",{\"1\":{\"103\":1}}],[\"返回每个点的分类结果和抽象特征\",{\"1\":{\"58\":1}}],[\"返回位置索引\",{\"1\":{\"49\":1}}],[\"返回拼接好的输入和\",{\"1\":{\"13\":1}}],[\"返回\",{\"1\":{\"10\":1,\"25\":1,\"35\":1,\"49\":1,\"57\":1,\"58\":2,\"64\":1,\"66\":1,\"142\":1,\"166\":2,\"167\":1,\"168\":1,\"169\":1,\"170\":1,\"172\":1}}],[\"融合了\",{\"1\":{\"278\":1}}],[\"融合模块\",{\"1\":{\"98\":2}}],[\"融合+增强\",{\"1\":{\"57\":1}}],[\"融合多个\",{\"1\":{\"33\":1}}],[\"融合多模态空间特征\",{\"0\":{\"11\":1},\"1\":{\"10\":1}}],[\"融合所有\",{\"1\":{\"33\":1}}],[\"融合后的文本特征\",{\"1\":{\"31\":1}}],[\"融合不同来源的信息\",{\"1\":{\"15\":1}}],[\"融合通道信息\",{\"1\":{\"15\":1}}],[\"融合\",{\"1\":{\"10\":1}}],[\"融合语言与视觉特征\",{\"1\":{\"10\":1}}],[\"72b\",{\"1\":{\"278\":1}}],[\"700\",{\"1\":{\"288\":1}}],[\"70b\",{\"1\":{\"278\":4}}],[\"704\",{\"1\":{\"233\":2}}],[\"75\",{\"1\":{\"169\":3}}],[\"751\",{\"1\":{\"22\":1,\"24\":1,\"26\":1}}],[\"7py1jdq1wp0nnyt3a\",{\"1\":{\"107\":1}}],[\"7b\",{\"1\":{\"83\":1,\"278\":4}}],[\"77\",{\"1\":{\"81\":1}}],[\"774\",{\"1\":{\"80\":1}}],[\"768\",{\"1\":{\"16\":2,\"27\":1,\"109\":4,\"110\":6,\"111\":4,\"112\":1,\"114\":4,\"211\":1,\"232\":3,\"238\":2}}],[\"7\",{\"0\":{\"25\":1,\"114\":1},\"1\":{\"10\":1,\"32\":1,\"39\":1,\"62\":1,\"104\":1,\"170\":2,\"203\":1,\"278\":6}}],[\"+1\",{\"1\":{\"176\":2}}],[\"+=\",{\"1\":{\"39\":3,\"93\":1,\"95\":1,\"103\":1,\"114\":2,\"175\":4,\"176\":2,\"225\":1,\"227\":1}}],[\"+qid\",{\"1\":{\"25\":1}}],[\"+\",{\"1\":{\"10\":4,\"13\":2,\"15\":9,\"16\":7,\"29\":1,\"30\":2,\"31\":3,\"32\":1,\"33\":4,\"35\":15,\"38\":1,\"39\":3,\"40\":1,\"49\":2,\"50\":2,\"53\":6,\"55\":1,\"56\":1,\"57\":8,\"62\":1,\"64\":2,\"68\":1,\"69\":3,\"72\":1,\"73\":1,\"74\":1,\"78\":1,\"81\":1,\"90\":1,\"91\":1,\"93\":2,\"95\":2,\"98\":1,\"101\":1,\"102\":4,\"103\":6,\"107\":1,\"111\":2,\"112\":2,\"113\":11,\"114\":2,\"151\":4,\"166\":3,\"167\":8,\"168\":4,\"170\":6,\"172\":6,\"175\":3,\"176\":5,\"184\":2,\"212\":2,\"224\":2,\"225\":21,\"226\":5,\"227\":6,\"233\":21,\"236\":4,\"238\":1,\"242\":2,\"244\":3,\"245\":2,\"249\":1,\"251\":2,\"254\":2,\"255\":1,\"256\":1,\"257\":6,\"265\":4,\"266\":1,\"269\":4,\"270\":1,\"290\":1,\"291\":2}}],[\"将不断从业务逻辑中收集当下\",{\"1\":{\"290\":1}}],[\"将组件组合实现端到端应用\",{\"1\":{\"287\":1}}],[\"将增强后的信息输入到生成模型中\",{\"1\":{\"284\":1}}],[\"将用户的问题输入到检索系统中\",{\"1\":{\"284\":1}}],[\"将处理后的数据存储在对应的数据库中\",{\"1\":{\"284\":1}}],[\"将处理后的数据转化为检索模型可以使用的格式\",{\"1\":{\"284\":1}}],[\"将生成内容与检索到的原始资料建立链接\",{\"1\":{\"283\":1}}],[\"将生成模型分配的平均对数概率高的token作为答案\",{\"1\":{\"213\":1}}],[\"将上下文长度大幅提升至\",{\"1\":{\"278\":1}}],[\"将复杂问题分解为可管理的子问题\",{\"1\":{\"278\":1}}],[\"将选项展平\",{\"1\":{\"257\":1}}],[\"将模型放入到仓库对应位置\",{\"1\":{\"232\":1}}],[\"将模型的预测作为行\",{\"1\":{\"151\":1}}],[\"将所有输入序列填充到等长max\",{\"1\":{\"225\":1}}],[\"将所有文本合并成一个字符串\",{\"1\":{\"224\":1}}],[\"将数据集压缩包下载到dataset目录下\",{\"1\":{\"223\":1}}],[\"将数据下载到当前项目目录下\",{\"1\":{\"93\":1}}],[\"将一整段文本按\",{\"1\":{\"223\":1}}],[\"将一篇文章\",{\"1\":{\"221\":1}}],[\"将一个批次的数据拆分为图像和标签两个元组\",{\"1\":{\"107\":1}}],[\"将句子中各个字对应位置的\",{\"1\":{\"221\":1}}],[\"将文档和问题跟每个可能答案拼接起来\",{\"1\":{\"209\":1}}],[\"将文本和query\",{\"1\":{\"102\":1}}],[\"将文本特征作为查询\",{\"1\":{\"29\":1}}],[\"将文本语义信息与点云特征进行跨模态融合\",{\"1\":{\"28\":1}}],[\"将其喂进一个参数为\",{\"1\":{\"208\":1}}],[\"将其展平就变成了一个长度为768的向量\",{\"1\":{\"109\":1}}],[\"将预训练模型的参数\",{\"1\":{\"187\":1}}],[\"将原始的\",{\"1\":{\"187\":1}}],[\"将原始点云\",{\"1\":{\"66\":1}}],[\"将原本用16bit表示的参数\",{\"1\":{\"185\":1}}],[\"将∆w进行低维分解∆w=ab\",{\"1\":{\"184\":1}}],[\"将y=wx变成y=\",{\"1\":{\"184\":1}}],[\"将w变成\",{\"1\":{\"180\":1}}],[\"将每个\",{\"1\":{\"253\":1}}],[\"将每个词从str转换为list列表形式\",{\"1\":{\"176\":1}}],[\"将每个点的局部特征与整个点云的全局特征拼接起来\",{\"1\":{\"66\":1}}],[\"将传入的最高频字符对中的两个字符用空格拼接起来\",{\"1\":{\"175\":1}}],[\"将训练语料中的每个单词按字符拆分\",{\"1\":{\"175\":1}}],[\"将该梯度向量与初始误差向量相乘\",{\"1\":{\"171\":1}}],[\"将样本的权重进行动态调整\",{\"1\":{\"169\":1}}],[\"将与较差的指标相似\",{\"1\":{\"157\":1}}],[\"将多个注意力头的输出合并为一个张量\",{\"1\":{\"113\":1}}],[\"将多头注意力的输出进行线性变换\",{\"1\":{\"113\":1}}],[\"将注意力权重矩阵与v相乘\",{\"1\":{\"113\":1}}],[\"将q和k的转置相乘\",{\"1\":{\"113\":1}}],[\"将隐藏特征映射到输出特征空间\",{\"1\":{\"112\":1}}],[\"将分类标记和图像块嵌入拼接\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"将卷积后的通道维数作为embedding的维度\",{\"1\":{\"109\":1}}],[\"将裁剪后的图像调整为\",{\"1\":{\"108\":1}}],[\"将标签元组转换为一个一维张量\",{\"1\":{\"107\":1}}],[\"将横坐标0\",{\"1\":{\"107\":1}}],[\"将最有用的信息提供给\",{\"1\":{\"104\":1}}],[\"将最后一层\",{\"1\":{\"55\":1}}],[\"将缓存的\",{\"1\":{\"103\":1}}],[\"将第一个\",{\"1\":{\"103\":1}}],[\"将会在下文进行详细讲解\",{\"1\":{\"102\":1}}],[\"将inputfeatures\",{\"1\":{\"233\":1}}],[\"将input\",{\"1\":{\"100\":1}}],[\"将图片编码成\",{\"1\":{\"100\":1}}],[\"将图像数据移动到指定设备上\",{\"1\":{\"114\":1}}],[\"将图像的短边缩放为\",{\"1\":{\"108\":1}}],[\"将图像元组堆叠成一个四维张量\",{\"1\":{\"107\":1}}],[\"将图像特征\",{\"1\":{\"81\":1}}],[\"将图像patch和点云点拼接成一个统一的token序列\",{\"1\":{\"11\":1}}],[\"将图像+点云特征插入语言嵌入中\",{\"1\":{\"10\":1}}],[\"将这个问题转化为一个多标签分类任务\",{\"1\":{\"96\":1}}],[\"将这些掩码token对应的嵌入向量映射到词向量空间中去\",{\"1\":{\"226\":1}}],[\"将这些图文对转换为如下格式\",{\"1\":{\"80\":1}}],[\"将这些特征映射到类别空间\",{\"1\":{\"67\":2}}],[\"将待分类的图像输入到图像编码器\",{\"1\":{\"91\":1}}],[\"将个文本特征和个图像特征两两组合\",{\"1\":{\"90\":1}}],[\"将空间划分成立方体格子\",{\"1\":{\"71\":1}}],[\"将全局特征与每个点的局部特征拼接起来\",{\"1\":{\"62\":1}}],[\"将全局语义向量扩展回原始点云数量\",{\"1\":{\"16\":1}}],[\"将点云转换为体素网格\",{\"1\":{\"60\":1}}],[\"将之前计算好的权重扩展维度\",{\"1\":{\"57\":1}}],[\"将距离转换为\",{\"1\":{\"57\":1}}],[\"将坐标和特征从\",{\"1\":{\"57\":1}}],[\"将稀疏点集points2插值到密集点集xyz1的位置上\",{\"1\":{\"57\":1}}],[\"将稀疏点集的特征插值回原始点集的位置上\",{\"1\":{\"57\":1}}],[\"将邻域点组合成局部点云组\",{\"1\":{\"55\":1}}],[\"将来自下一级\",{\"1\":{\"54\":1}}],[\"将view\",{\"1\":{\"49\":1}}],[\"将当前词列表中每个子词映射为字典中对于的词id\",{\"1\":{\"176\":2}}],[\"将当前选中的\",{\"1\":{\"49\":1}}],[\"将当前样本的物体信息值追加到对应列表中\",{\"1\":{\"25\":1}}],[\"将转换后的坐标以及点的附加特征\",{\"1\":{\"48\":1}}],[\"将两个方向的\",{\"1\":{\"35\":1}}],[\"将两个注意力输出拼接在一起\",{\"1\":{\"15\":1}}],[\"将响应值映射到\",{\"1\":{\"33\":1}}],[\"将融合特征重新分配给每个点\",{\"1\":{\"31\":2}}],[\"将融合特征映射回点空间\",{\"0\":{\"31\":1}}],[\"将融合后的\",{\"1\":{\"12\":1}}],[\"将融合后的空间特征通过适配器上采样到与语言模型匹配的维度\",{\"1\":{\"10\":1}}],[\"将特征映射到\",{\"1\":{\"16\":1}}],[\"将输入序列\",{\"1\":{\"261\":1}}],[\"将输入和目标张量展平为一维\",{\"1\":{\"170\":1,\"172\":1}}],[\"将输入和目标展平成一维张量便于计算\",{\"1\":{\"168\":1}}],[\"将输入和目标展平成一维张量\",{\"1\":{\"167\":1}}],[\"将输入展平便于后续计算\",{\"1\":{\"169\":1}}],[\"将输入展平成一维张量\",{\"1\":{\"166\":1}}],[\"将输入映射到\",{\"1\":{\"168\":1}}],[\"将输入映射到概率空间\",{\"1\":{\"167\":1}}],[\"将输入映射到低维空间以进行\",{\"1\":{\"15\":1}}],[\"将输入特征映射到隐藏特征空间\",{\"1\":{\"112\":1}}],[\"将输入图像进行图像块嵌入\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"将输入图片\",{\"1\":{\"109\":1}}],[\"将输入文本转换为嵌入列表后和query\",{\"1\":{\"102\":1}}],[\"将输入的维度dim映射到dim\",{\"1\":{\"113\":1}}],[\"将输入的\",{\"1\":{\"100\":1}}],[\"将输入分别投影到低维空间\",{\"1\":{\"15\":1}}],[\"将\",{\"1\":{\"13\":1,\"14\":1,\"20\":1,\"39\":1,\"80\":1,\"82\":1,\"103\":2,\"108\":2,\"109\":1,\"117\":1,\"166\":1,\"170\":1,\"192\":1,\"278\":1}}],[\"将语言嵌入\",{\"1\":{\"13\":1}}],[\"函数生成目标序列的概率分布\",{\"1\":{\"261\":1}}],[\"函数的作用是将整个点云视为一个\",{\"1\":{\"49\":1}}],[\"函数定义见\",{\"1\":{\"39\":1}}],[\"函数\",{\"1\":{\"10\":1,\"33\":1,\"62\":1,\"72\":1,\"93\":1,\"95\":1,\"107\":1,\"170\":1}}],[\"671b\",{\"1\":{\"278\":1}}],[\"67b\",{\"1\":{\"278\":1}}],[\"67b7e751e6b5931a9f45274653f4f653a4e6cdf6\",{\"1\":{\"107\":1}}],[\"6b\",{\"1\":{\"278\":1}}],[\"6f\",{\"1\":{\"227\":2}}],[\"6平均分数\",{\"1\":{\"213\":1}}],[\"65b的llama的微调要780gb的gpu内存\",{\"1\":{\"185\":1}}],[\"62\",{\"1\":{\"93\":1}}],[\"629\",{\"1\":{\"32\":1,\"39\":1}}],[\"696\",{\"1\":{\"80\":1}}],[\"694\",{\"1\":{\"80\":1}}],[\"681\",{\"1\":{\"80\":1}}],[\"6883\",{\"1\":{\"22\":1}}],[\"638\",{\"1\":{\"22\":1}}],[\"64×64\",{\"1\":{\"62\":1}}],[\"640\",{\"1\":{\"53\":1}}],[\"64\",{\"1\":{\"16\":1,\"50\":2,\"53\":7,\"57\":1,\"58\":6,\"64\":4,\"66\":3,\"211\":1}}],[\"6\",{\"0\":{\"13\":1,\"24\":1,\"113\":1},\"1\":{\"10\":1,\"35\":7,\"37\":1,\"38\":1,\"39\":1,\"40\":1,\"50\":1,\"104\":1,\"114\":2,\"175\":1,\"204\":1,\"226\":2,\"278\":5,\"287\":1}}],[\"5b\",{\"1\":{\"278\":2}}],[\"59\",{\"1\":{\"278\":1}}],[\"595\",{\"1\":{\"80\":1}}],[\"5400\",{\"1\":{\"277\":1}}],[\"540b\",{\"1\":{\"277\":1}}],[\"5500\",{\"1\":{\"233\":2}}],[\"558\",{\"1\":{\"32\":1}}],[\"5250\",{\"1\":{\"228\":1}}],[\"5的情感极性区分的更细致\",{\"1\":{\"212\":1}}],[\"5是五分类\",{\"1\":{\"212\":1}}],[\"5e\",{\"1\":{\"211\":1}}],[\"53\",{\"1\":{\"40\":1,\"81\":1,\"82\":1}}],[\"504\",{\"1\":{\"228\":1}}],[\"500\",{\"1\":{\"40\":3}}],[\"50\",{\"1\":{\"40\":1,\"62\":1,\"160\":2,\"224\":2,\"225\":3,\"278\":1}}],[\"516\",{\"1\":{\"22\":1}}],[\"512k\",{\"1\":{\"278\":1}}],[\"512\",{\"1\":{\"11\":1,\"16\":6,\"50\":6,\"53\":5,\"56\":4,\"58\":1,\"64\":3,\"67\":3,\"68\":3,\"211\":1}}],[\"58\",{\"1\":{\"20\":2,\"22\":1,\"24\":1,\"81\":1}}],[\"5\",{\"0\":{\"23\":1,\"112\":1},\"1\":{\"10\":1,\"11\":1,\"15\":1,\"20\":1,\"33\":1,\"35\":3,\"37\":1,\"38\":1,\"39\":2,\"40\":2,\"53\":1,\"55\":1,\"58\":1,\"83\":1,\"91\":1,\"92\":1,\"96\":1,\"103\":1,\"104\":1,\"107\":1,\"108\":19,\"113\":1,\"118\":1,\"160\":2,\"162\":1,\"169\":3,\"170\":4,\"172\":4,\"175\":1,\"203\":1,\"204\":2,\"211\":2,\"212\":1,\"226\":2,\"232\":1,\"236\":1,\"278\":18}}],[\"对大模型能力具有极大影响\",{\"1\":{\"291\":1}}],[\"对大模型进行微调\",{\"1\":{\"180\":1}}],[\"对大模型进行训练\",{\"1\":{\"180\":1}}],[\"对流式处理进行了深度优化\",{\"1\":{\"288\":1}}],[\"对检索到的信息进行处理和增强\",{\"1\":{\"284\":1}}],[\"对原始数据进行清洗和处理\",{\"1\":{\"284\":1}}],[\"对原始点云做刚性变换\",{\"1\":{\"65\":1}}],[\"对语言有了更深刻的理解\",{\"1\":{\"277\":1}}],[\"对语言指令进行分词\",{\"1\":{\"10\":1}}],[\"对话即平台\",{\"1\":{\"282\":1}}],[\"对话模型\",{\"1\":{\"278\":1,\"287\":1}}],[\"对话系统中的候选回复选择\",{\"1\":{\"257\":1}}],[\"对话型问答为多轮对话\",{\"1\":{\"81\":1}}],[\"对话型\",{\"1\":{\"81\":1}}],[\"对外提供的编码和解码两个方法实现如下\",{\"1\":{\"224\":1}}],[\"对列表数据进行解析\",{\"1\":{\"224\":1}}],[\"对同一个大模型的微调\",{\"1\":{\"180\":1}}],[\"对当前词的子词进行合并\",{\"1\":{\"176\":1}}],[\"对当前句子中每个词进行子词合并加词id映射\",{\"1\":{\"176\":1}}],[\"对经过预处理的vocab中的每个词按空格进行切分\",{\"1\":{\"175\":1}}],[\"对经过注意力层的输出进行归一化处理\",{\"1\":{\"112\":1}}],[\"对损失求均值\",{\"1\":{\"172\":1}}],[\"对假阴性\",{\"1\":{\"172\":1}}],[\"对假阳性\",{\"1\":{\"172\":1}}],[\"对比\",{\"1\":{\"255\":1}}],[\"对比之前的方法\",{\"1\":{\"203\":1}}],[\"对比其他损失函数\",{\"1\":{\"172\":1}}],[\"对比维度\",{\"1\":{\"85\":1}}],[\"对类别不平衡问题鲁棒\",{\"1\":{\"172\":1}}],[\"对类别不平衡不敏感\",{\"1\":{\"166\":1,\"168\":1}}],[\"对噪声标签敏感\",{\"1\":{\"169\":1}}],[\"对噪声点敏感\",{\"1\":{\"69\":2}}],[\"对分类错误的样本\",{\"1\":{\"169\":1}}],[\"对模型已经分类正确的样本\",{\"1\":{\"169\":1}}],[\"对细节更敏感\",{\"1\":{\"167\":1}}],[\"对单个点的分类精度不够敏感\",{\"1\":{\"167\":1}}],[\"对前景响应弱\",{\"1\":{\"167\":1}}],[\"对特征进行更深入的建模\",{\"1\":{\"117\":1}}],[\"对特征空间进行变换\",{\"1\":{\"66\":1}}],[\"对特征空间做变换\",{\"1\":{\"62\":1,\"65\":1}}],[\"对投影后的结果应用丢弃层\",{\"1\":{\"113\":1}}],[\"对输出有了更加明确具体的要求\",{\"1\":{\"196\":1}}],[\"对输出应用\",{\"1\":{\"170\":1}}],[\"对输出进行维度交换和形状调整\",{\"1\":{\"113\":1}}],[\"对输入的文本进行断句加分词\",{\"1\":{\"176\":1}}],[\"对输入的点云做刚性变换\",{\"1\":{\"64\":1}}],[\"对输入数据进行初步的特征提取\",{\"1\":{\"117\":1}}],[\"对输入进行归一化处理\",{\"1\":{\"112\":1}}],[\"对输入点云做刚性变换\",{\"1\":{\"62\":1}}],[\"对输入点云中的每个点进行分类\",{\"1\":{\"58\":1}}],[\"对输入点进行采样\",{\"1\":{\"44\":1}}],[\"对注意力权重矩阵应用丢弃层\",{\"1\":{\"113\":1}}],[\"对注意力分数矩阵应用softmax函数\",{\"1\":{\"113\":1}}],[\"对处理后的张量进行归一化操作\",{\"1\":{\"109\":1}}],[\"对图像进行归一化处理\",{\"1\":{\"108\":2}}],[\"对图像和点云特征进行\",{\"1\":{\"11\":2}}],[\"对验证集的处理方式是先resize成256x256的图片\",{\"1\":{\"108\":1}}],[\"对数据的处理和操作要求极低\",{\"1\":{\"285\":1}}],[\"对数据集和验证集划分之后\",{\"1\":{\"108\":1}}],[\"对数值变化敏感\",{\"1\":{\"72\":1}}],[\"对角线元素的labels\",{\"1\":{\"90\":1}}],[\"对称的对比学习损失\",{\"1\":{\"90\":1}}],[\"对称函数\",{\"0\":{\"72\":1},\"1\":{\"72\":2}}],[\"对两个特征进行线性投射\",{\"1\":{\"90\":1}}],[\"对微调模型\",{\"1\":{\"85\":1}}],[\"对异常点鲁棒性差\",{\"1\":{\"69\":1}}],[\"对异常点也有一定容忍能力\",{\"1\":{\"62\":1}}],[\"对少量异常点有一定鲁棒性\",{\"1\":{\"69\":1}}],[\"对稀疏点云敏感\",{\"1\":{\"69\":1}}],[\"对局部形状变化敏感\",{\"1\":{\"69\":1}}],[\"对局部点云组做最大池化或平均池化\",{\"1\":{\"55\":1}}],[\"对局部点云进行变换\",{\"1\":{\"55\":1}}],[\"对齐\",{\"1\":{\"64\":1}}],[\"对几何变换的不变性\",{\"1\":{\"61\":1,\"62\":1}}],[\"对参数选择依赖性高\",{\"1\":{\"47\":1}}],[\"对上述得到的每个区域进行编码\",{\"1\":{\"44\":1}}],[\"对所有\",{\"1\":{\"39\":1}}],[\"对所有点\",{\"1\":{\"30\":1}}],[\"对边界敏感\",{\"1\":{\"39\":1}}],[\"对边界模糊区域友好\",{\"1\":{\"39\":1}}],[\"对边界模糊区域不敏感\",{\"1\":{\"39\":1}}],[\"对不同阈值计算\",{\"1\":{\"39\":1}}],[\"对affordance\",{\"1\":{\"33\":1}}],[\"对无效\",{\"1\":{\"33\":1}}],[\"对应代码如下\",{\"1\":{\"232\":1}}],[\"对应着标签\",{\"1\":{\"208\":1}}],[\"对应所有\",{\"1\":{\"125\":1}}],[\"对应维度为\",{\"1\":{\"110\":1}}],[\"对应\",{\"1\":{\"109\":1}}],[\"对应一个\",{\"1\":{\"64\":1}}],[\"对应一组相关的点特征\",{\"1\":{\"29\":1}}],[\"对应半径下最多取多少邻近点\",{\"1\":{\"53\":1}}],[\"对应指标\",{\"1\":{\"39\":1}}],[\"对应的id为\",{\"1\":{\"233\":1}}],[\"对应的参数为\",{\"1\":{\"232\":1}}],[\"对应的输出概率最大\",{\"1\":{\"221\":1}}],[\"对应的伪代码实现如下所示\",{\"1\":{\"90\":1}}],[\"对应的\",{\"1\":{\"33\":1}}],[\"对应的注意力掩码\",{\"1\":{\"13\":1}}],[\"对象功能区域分割\",{\"1\":{\"27\":1}}],[\"对每一个选项\",{\"1\":{\"257\":1}}],[\"对每一行做\",{\"1\":{\"126\":1}}],[\"对每个选项分别进行编码\",{\"1\":{\"257\":1}}],[\"对每个句对构建用于mlm任务的样本\",{\"1\":{\"225\":1}}],[\"对每个句对构建用于nsp任务的样本\",{\"1\":{\"225\":1}}],[\"对每个句子进行分词\",{\"1\":{\"176\":1}}],[\"对每个词进行子词合并\",{\"1\":{\"176\":2}}],[\"对每个原始点\",{\"1\":{\"57\":1}}],[\"对每个尺度的局部点集应用对应的\",{\"1\":{\"53\":1}}],[\"对每个半径\",{\"1\":{\"53\":1}}],[\"对每个局部区域内所有点的最大响应值进行池化\",{\"1\":{\"49\":1}}],[\"对每个查询点的邻近点按索引排序\",{\"1\":{\"49\":1}}],[\"对每个点单独计算分类误差\",{\"1\":{\"167\":1}}],[\"对每个点独立处理\",{\"1\":{\"69\":1}}],[\"对每个点进行特征提取\",{\"1\":{\"66\":1}}],[\"对每个点取预测值和真实值中的较小者\",{\"1\":{\"39\":1}}],[\"对每个点的3个权重求和\",{\"1\":{\"57\":1}}],[\"对每个点的特征做一个简单的分类器\",{\"1\":{\"55\":1}}],[\"对每个点的响应值\",{\"1\":{\"33\":1}}],[\"对每个点的关注程度\",{\"1\":{\"33\":1}}],[\"对每个点的关注响应\",{\"1\":{\"33\":1}}],[\"对每个\",{\"1\":{\"30\":1,\"255\":1,\"278\":1}}],[\"对每个样本单独处理\",{\"1\":{\"13\":1}}],[\"对每种组合手工编写\",{\"1\":{\"20\":1}}],[\"对\",{\"1\":{\"16\":1,\"24\":1,\"33\":1,\"39\":1,\"85\":1,\"192\":1,\"221\":1}}],[\"对点云的旋转\",{\"1\":{\"62\":1}}],[\"对点云进行下采样\",{\"1\":{\"55\":1}}],[\"对点云密度变换较为敏感\",{\"1\":{\"47\":1}}],[\"对点云数据做平移操作后\",{\"1\":{\"43\":1}}],[\"对点云数据进行转置操作\",{\"1\":{\"25\":1}}],[\"对点云数据进行归一化处理\",{\"1\":{\"25\":1}}],[\"对点数维度做\",{\"1\":{\"16\":1}}],[\"对点积结果进行缩放\",{\"1\":{\"11\":1}}],[\"对于个体开发者或小型开发团队来说\",{\"1\":{\"291\":1}}],[\"对于个体开发者或小型开发团队而言\",{\"1\":{\"291\":1}}],[\"对于个人使用者而言\",{\"1\":{\"179\":1}}],[\"对于这样的多选问题\",{\"1\":{\"257\":1}}],[\"对于这些任务\",{\"1\":{\"209\":1}}],[\"对于分类任务来说\",{\"1\":{\"242\":1}}],[\"对于字典中不存在的词\",{\"1\":{\"233\":1}}],[\"对于mlm任务损失计算来说\",{\"1\":{\"227\":2}}],[\"对于所有掩码候选位置执行掩码策略\",{\"1\":{\"224\":1}}],[\"对于dprd\",{\"1\":{\"213\":1}}],[\"对于race\",{\"1\":{\"213\":1}}],[\"对于sst\",{\"1\":{\"213\":1}}],[\"对于cola\",{\"1\":{\"213\":1}}],[\"对于相似任务\",{\"1\":{\"209\":1}}],[\"对于作者的模型架构\",{\"1\":{\"204\":1}}],[\"对于将这些学习到的表征迁移到目标任务的最有效方法\",{\"1\":{\"204\":1}}],[\"对于llm越友好\",{\"1\":{\"196\":1}}],[\"对于一些复杂的问题\",{\"1\":{\"194\":1}}],[\"对于一般的任务\",{\"1\":{\"191\":1}}],[\"对于一个样本\",{\"1\":{\"169\":1}}],[\"对于一个包含个文本\",{\"1\":{\"90\":1}}],[\"对于\",{\"1\":{\"190\":1}}],[\"对于需要微调的密集层\",{\"1\":{\"187\":1}}],[\"对于二元分类器\",{\"1\":{\"160\":1}}],[\"对于二维的图像\",{\"1\":{\"109\":1}}],[\"对于类别不平衡的数据集\",{\"1\":{\"157\":1,\"173\":1}}],[\"对于模型效果\",{\"1\":{\"156\":1}}],[\"对于疾病预测等应用\",{\"1\":{\"153\":1}}],[\"对于严重不均衡的数据集\",{\"1\":{\"152\":1}}],[\"对于q\",{\"1\":{\"100\":1}}],[\"对于自监督模型\",{\"1\":{\"96\":1}}],[\"对于有监督模型\",{\"1\":{\"96\":1}}],[\"对于vit\",{\"1\":{\"90\":1}}],[\"对于频率大于\",{\"1\":{\"80\":1}}],[\"对于同一个指令\",{\"1\":{\"78\":1}}],[\"对于非刚性变形\",{\"1\":{\"69\":1}}],[\"对于多标签\",{\"1\":{\"55\":1}}],[\"对于多分类\",{\"1\":{\"55\":1}}],[\"对于某个形心\",{\"1\":{\"47\":1}}],[\"对于单个的物体还好\",{\"1\":{\"43\":1}}],[\"对于每个局部区域\",{\"1\":{\"53\":1}}],[\"对于每个质心点\",{\"1\":{\"52\":1}}],[\"对于每个选中的关键点\",{\"1\":{\"49\":1}}],[\"对于每个问题\",{\"1\":{\"21\":1}}],[\"对于每一个子业务训练优化模型\",{\"1\":{\"290\":1}}],[\"对于每一个子业务构造训练数据与验证数据\",{\"1\":{\"290\":1}}],[\"对于每一个输出位置\",{\"1\":{\"227\":2}}],[\"对于每一个\",{\"1\":{\"33\":1}}],[\"对于点云中的每一个点\",{\"1\":{\"11\":1}}],[\"对于图像中的每一个位置\",{\"1\":{\"11\":1}}],[\"对自然语言指令进行\",{\"1\":{\"10\":1}}],[\"和长文本推理\",{\"1\":{\"278\":1}}],[\"和指令微调版\",{\"1\":{\"278\":1}}],[\"和在线推理阶段的\",{\"1\":{\"277\":1}}],[\"和一个问题\",{\"1\":{\"221\":1}}],[\"和一个可能答案集\",{\"1\":{\"209\":1}}],[\"和假设\",{\"1\":{\"209\":1}}],[\"和假阴性\",{\"1\":{\"170\":1}}],[\"和假正例率\",{\"1\":{\"159\":1}}],[\"和负类\",{\"1\":{\"169\":1}}],[\"和这些曲线下的面积可以更好地直观比较模型性能\",{\"1\":{\"161\":1}}],[\"和所有真实负例\",{\"1\":{\"151\":1}}],[\"和所有预测负例\",{\"1\":{\"151\":1}}],[\"和transformer中的一样\",{\"1\":{\"112\":1}}],[\"和text都能和所有的tokens\",{\"1\":{\"102\":1}}],[\"和冻结参数的\",{\"1\":{\"104\":1}}],[\"和文本分类\",{\"1\":{\"204\":1}}],[\"和文本\",{\"1\":{\"103\":1}}],[\"和基于图像掩码的方法\",{\"1\":{\"96\":1}}],[\"和图像编码器\",{\"1\":{\"90\":1}}],[\"和语言模型\",{\"1\":{\"81\":1}}],[\"和对应的\",{\"1\":{\"78\":1}}],[\"和稀疏采样的区域\",{\"1\":{\"52\":1}}],[\"和多分辨率分组\",{\"1\":{\"51\":1}}],[\"和每个查询点上\",{\"1\":{\"49\":1}}],[\"和点维度的平均\",{\"1\":{\"35\":1}}],[\"和通道混合\",{\"1\":{\"30\":1}}],[\"和通道维度\",{\"1\":{\"15\":1}}],[\"和值\",{\"1\":{\"29\":1,\"113\":2,\"278\":2}}],[\"和\",{\"0\":{\"85\":1,\"135\":1,\"162\":1},\"1\":{\"10\":3,\"12\":1,\"15\":5,\"16\":1,\"25\":1,\"27\":1,\"39\":3,\"45\":1,\"64\":1,\"65\":1,\"82\":1,\"85\":1,\"88\":3,\"98\":3,\"101\":3,\"103\":2,\"109\":1,\"117\":2,\"123\":1,\"159\":1,\"160\":1,\"161\":1,\"162\":3,\"166\":1,\"167\":1,\"168\":1,\"169\":1,\"170\":2,\"172\":2,\"174\":1,\"175\":1,\"187\":5,\"189\":1,\"197\":1,\"219\":1,\"221\":1,\"223\":2,\"226\":2,\"230\":1,\"251\":1,\"253\":2,\"254\":2,\"255\":1,\"266\":1,\"277\":4,\"278\":11,\"285\":2,\"288\":1}}],[\"和自注意力机制融合图像与点云特征\",{\"1\":{\"10\":1}}],[\"pwd=vket\",{\"1\":{\"118\":1}}],[\"pwd=qvmq\",{\"1\":{\"107\":1}}],[\"platform\",{\"1\":{\"282\":1}}],[\"place\",{\"1\":{\"224\":2}}],[\"plm\",{\"1\":{\"189\":3,\"280\":1}}],[\"plot\",{\"1\":{\"107\":2}}],[\"plt\",{\"1\":{\"94\":4,\"95\":5,\"107\":7}}],[\"p=dropout\",{\"1\":{\"271\":1}}],[\"p=drop\",{\"1\":{\"111\":1,\"114\":1}}],[\"p=top\",{\"1\":{\"104\":1}}],[\"p=0\",{\"1\":{\"67\":1,\"104\":1,\"169\":2}}],[\"p采样\",{\"1\":{\"104\":1}}],[\"pip\",{\"1\":{\"147\":9,\"259\":1}}],[\"pil\",{\"1\":{\"95\":1,\"107\":1,\"108\":2}}],[\"pickle\",{\"1\":{\"25\":2,\"40\":3}}],[\"png\",{\"1\":{\"93\":1,\"95\":1,\"107\":2}}],[\"penn\",{\"1\":{\"223\":1}}],[\"penalty=1\",{\"1\":{\"104\":1}}],[\"peft\",{\"1\":{\"185\":1,\"192\":1}}],[\"peft是目前业界比较流行的微调方案\",{\"1\":{\"181\":1}}],[\"peft也是目前比较主流的微调方案\",{\"1\":{\"180\":1}}],[\"peft主要想解决的问题\",{\"1\":{\"180\":1}}],[\"people\",{\"1\":{\"80\":1}}],[\"performance\",{\"1\":{\"217\":1}}],[\"person\",{\"1\":{\"80\":1,\"91\":1}}],[\"permutation\",{\"1\":{\"61\":1,\"72\":2}}],[\"permute\",{\"1\":{\"11\":2,\"15\":1,\"49\":4,\"53\":4,\"57\":5,\"58\":1,\"101\":1,\"103\":1,\"113\":2,\"244\":2}}],[\"per\",{\"1\":{\"55\":1,\"56\":2,\"113\":6,\"232\":2}}],[\"ppo\",{\"1\":{\"78\":1}}],[\"pkill\",{\"1\":{\"40\":1}}],[\"pkl\",{\"1\":{\"25\":2,\"40\":2}}],[\"py文件\",{\"1\":{\"232\":1}}],[\"pypi\",{\"1\":{\"147\":1}}],[\"pyplot\",{\"1\":{\"95\":1}}],[\"py\",{\"1\":{\"40\":3,\"50\":1,\"53\":1,\"107\":1,\"232\":2}}],[\"python=3\",{\"1\":{\"140\":2,\"232\":1,\"259\":1}}],[\"python\",{\"1\":{\"40\":7,\"107\":1,\"140\":2,\"147\":7,\"232\":1,\"288\":1}}],[\"pytorch版本\",{\"0\":{\"63\":1}}],[\"pytorch\",{\"1\":{\"33\":1,\"42\":2,\"59\":2,\"107\":2,\"108\":2,\"118\":1,\"172\":1,\"219\":1,\"232\":3,\"254\":1}}],[\"pcd\",{\"1\":{\"40\":4}}],[\"pc\",{\"1\":{\"25\":1,\"40\":11}}],[\"pd\",{\"1\":{\"25\":1}}],[\"pdf\",{\"1\":{\"17\":1,\"202\":1,\"291\":1}}],[\"prc\",{\"1\":{\"161\":1}}],[\"pro\",{\"1\":{\"278\":3}}],[\"probabilistic\",{\"1\":{\"277\":1}}],[\"probabilities\",{\"1\":{\"244\":1}}],[\"prob\",{\"1\":{\"236\":1,\"238\":1,\"242\":1,\"244\":1,\"245\":1,\"256\":1,\"257\":1}}],[\"probs\",{\"1\":{\"91\":3,\"103\":7,\"244\":5}}],[\"procedure\",{\"1\":{\"224\":1,\"225\":1}}],[\"process\",{\"1\":{\"223\":3,\"262\":1}}],[\"processor\",{\"1\":{\"93\":3,\"95\":3}}],[\"processing\",{\"1\":{\"92\":1,\"185\":1}}],[\"prompt写得好不好\",{\"1\":{\"194\":1}}],[\"prompt太长会因超过限制而被截断\",{\"1\":{\"179\":1}}],[\"prompts\",{\"1\":{\"104\":1,\"183\":1}}],[\"prompting有一个直观的认知\",{\"1\":{\"200\":1}}],[\"prompting技巧\",{\"1\":{\"199\":1}}],[\"prompting\",{\"1\":{\"92\":1,\"185\":1,\"198\":1,\"200\":1}}],[\"prompt\",{\"0\":{\"85\":1,\"182\":1,\"193\":1},\"1\":{\"85\":11,\"92\":3,\"179\":3,\"182\":5,\"183\":1,\"185\":1,\"188\":2,\"193\":1,\"194\":2,\"196\":1,\"198\":1,\"287\":1,\"289\":1,\"290\":10,\"291\":12}}],[\"proximal\",{\"1\":{\"78\":1}}],[\"product\",{\"1\":{\"72\":1,\"123\":1,\"221\":2,\"244\":1,\"271\":1}}],[\"propagation\",{\"1\":{\"55\":4,\"56\":1,\"57\":1,\"58\":2}}],[\"projected\",{\"1\":{\"271\":1}}],[\"projections\",{\"1\":{\"271\":1}}],[\"projection\",{\"1\":{\"271\":1}}],[\"projects\",{\"1\":{\"97\":1}}],[\"proj\",{\"1\":{\"15\":32,\"37\":2,\"40\":2,\"90\":2,\"100\":2,\"109\":2,\"112\":1,\"113\":6,\"263\":2}}],[\"print\",{\"1\":{\"39\":1,\"40\":2,\"93\":6,\"94\":4,\"95\":10,\"107\":3,\"118\":1,\"223\":2,\"227\":3}}],[\"precomputed\",{\"1\":{\"244\":1}}],[\"precision\",{\"1\":{\"170\":1}}],[\"preview\",{\"1\":{\"278\":3}}],[\"prev\",{\"1\":{\"232\":1}}],[\"prepare\",{\"1\":{\"175\":2,\"225\":3}}],[\"present\",{\"1\":{\"103\":2}}],[\"press\",{\"1\":{\"25\":1}}],[\"pretrainedtokenizer\",{\"1\":{\"233\":1}}],[\"pretrained\",{\"1\":{\"93\":4,\"95\":4}}],[\"prefix\",{\"0\":{\"183\":1},\"1\":{\"85\":2,\"183\":5,\"188\":2}}],[\"pre\",{\"1\":{\"79\":1,\"89\":1,\"92\":1,\"114\":3,\"118\":2,\"185\":1,\"189\":1,\"203\":1,\"278\":2}}],[\"pred的进行pad填充\",{\"1\":{\"227\":1}}],[\"predicted\",{\"1\":{\"40\":1,\"93\":6,\"95\":6}}],[\"predict\",{\"1\":{\"40\":2,\"92\":1,\"185\":1,\"221\":1}}],[\"predictions\",{\"1\":{\"250\":2}}],[\"prediction\",{\"0\":{\"219\":1},\"1\":{\"35\":1,\"40\":1,\"96\":1,\"103\":6,\"217\":1,\"233\":1,\"250\":2,\"251\":4}}],[\"pred\",{\"1\":{\"35\":10,\"39\":4,\"40\":24,\"114\":5,\"225\":7,\"226\":7}}],[\"punctuation\",{\"1\":{\"223\":1}}],[\"pull\",{\"1\":{\"25\":1}}],[\"push\",{\"1\":{\"25\":1}}],[\"photos下的子目录名作为我们的候选待匹配分类文本列表\",{\"1\":{\"93\":1}}],[\"photos\",{\"1\":{\"93\":4,\"95\":2}}],[\"photos目录下读取出所有图片的路径\",{\"1\":{\"93\":1}}],[\"photo\",{\"1\":{\"91\":2,\"92\":2,\"93\":2,\"94\":1,\"95\":2}}],[\"phrase\",{\"1\":{\"80\":1}}],[\"phrasing\",{\"1\":{\"20\":1}}],[\"phi\",{\"1\":{\"11\":7}}],[\"p3\",{\"1\":{\"16\":1}}],[\"p2\",{\"1\":{\"16\":1}}],[\"p1\",{\"1\":{\"16\":1}}],[\"p0\",{\"1\":{\"16\":1}}],[\"power\",{\"1\":{\"182\":1}}],[\"portion\",{\"1\":{\"233\":1}}],[\"ported\",{\"1\":{\"118\":1}}],[\"portrait\",{\"1\":{\"91\":1}}],[\"policy\",{\"1\":{\"78\":3}}],[\"positive和\",{\"1\":{\"213\":1}}],[\"positives\",{\"1\":{\"170\":1}}],[\"positive\",{\"1\":{\"35\":6,\"168\":1,\"170\":4}}],[\"positions\",{\"1\":{\"254\":7,\"255\":1}}],[\"positions=none\",{\"1\":{\"254\":2}}],[\"positional\",{\"1\":{\"111\":1,\"261\":1}}],[\"position\",{\"1\":{\"33\":1,\"49\":5,\"53\":2,\"102\":9,\"103\":2,\"211\":1,\"219\":1,\"236\":12,\"241\":2,\"242\":2,\"251\":2,\"254\":2,\"256\":2,\"257\":6,\"278\":1}}],[\"pos\",{\"1\":{\"33\":8,\"111\":5,\"114\":5,\"224\":10,\"225\":6,\"226\":9,\"227\":2,\"256\":1}}],[\"pos1d\",{\"1\":{\"27\":1,\"33\":1}}],[\"pos=self\",{\"1\":{\"27\":1,\"33\":1}}],[\"pour\",{\"1\":{\"25\":1}}],[\"pooled\",{\"1\":{\"240\":4,\"241\":2,\"242\":4,\"250\":2,\"251\":2,\"253\":2,\"257\":4}}],[\"pooler\",{\"1\":{\"226\":7,\"241\":2}}],[\"pooling\",{\"1\":{\"49\":1,\"55\":1,\"62\":5,\"64\":1,\"69\":7,\"72\":3}}],[\"pool\",{\"1\":{\"16\":4,\"226\":2,\"240\":1}}],[\"pointcnn\",{\"1\":{\"69\":1}}],[\"pointcloud\",{\"1\":{\"40\":2,\"49\":2}}],[\"pointfeat\",{\"1\":{\"66\":2}}],[\"point点云数据\",{\"1\":{\"27\":1}}],[\"pointrefer模型结构图\",{\"1\":{\"27\":1}}],[\"pointrefer\",{\"1\":{\"27\":5,\"33\":1,\"35\":1,\"37\":1,\"39\":2,\"40\":4}}],[\"pointnetdensecls\",{\"1\":{\"68\":3}}],[\"pointnetcls\",{\"1\":{\"67\":3}}],[\"pointnetfeat\",{\"1\":{\"66\":3,\"67\":4,\"68\":3}}],[\"pointnetfeaturepropagation\",{\"1\":{\"16\":5,\"57\":3,\"58\":4}}],[\"pointnet网络模型结构图\",{\"1\":{\"63\":1}}],[\"pointnet后\",{\"1\":{\"50\":3}}],[\"pointnetsetabstractionmsg\",{\"1\":{\"53\":5}}],[\"pointnetsetabstraction\",{\"1\":{\"49\":3,\"50\":3,\"53\":1,\"58\":4}}],[\"pointnet来提取局部区域中的特征\",{\"1\":{\"48\":1}}],[\"pointnet将局部区域编码为特征向量\",{\"1\":{\"45\":1}}],[\"pointnet\",{\"0\":{\"48\":1},\"1\":{\"42\":1,\"44\":1,\"45\":2,\"49\":2,\"53\":1,\"55\":1,\"59\":2,\"60\":1,\"62\":4,\"64\":4,\"65\":2,\"66\":1,\"67\":1,\"68\":1,\"69\":26,\"72\":2}}],[\"pointnet2\",{\"1\":{\"42\":2,\"50\":1,\"53\":2}}],[\"pointnet++提出了密度自适应pointnet层\",{\"1\":{\"51\":1}}],[\"pointnet++应用pointnet递归地对输入集进行嵌套分区\",{\"1\":{\"43\":1}}],[\"pointnet++选择pointnet作为局部特征学习器\",{\"1\":{\"43\":1}}],[\"pointnet++在进行点集划分时\",{\"1\":{\"43\":1}}],[\"pointnet++的下一个任务是学习这些子集\",{\"1\":{\"43\":1}}],[\"pointnet++需要一种方法来有效地将点云分割成多个部分\",{\"1\":{\"43\":1}}],[\"pointnet++\",{\"1\":{\"10\":1,\"27\":3,\"28\":1,\"43\":1,\"49\":3,\"50\":2,\"53\":2,\"55\":2,\"56\":1,\"57\":1,\"58\":1,\"69\":4}}],[\"points2\",{\"1\":{\"55\":2,\"57\":7}}],[\"points1\",{\"1\":{\"55\":2,\"57\":7}}],[\"points\",{\"1\":{\"10\":1,\"20\":1,\"40\":18,\"49\":68,\"50\":7,\"53\":39,\"55\":2,\"56\":7,\"57\":15,\"58\":27,\"61\":2,\"64\":1}}],[\"point\",{\"1\":{\"10\":8,\"11\":6,\"16\":3,\"25\":6,\"27\":3,\"33\":3,\"38\":3,\"39\":6,\"40\":15,\"46\":1,\"49\":7,\"53\":2,\"55\":2,\"56\":2,\"62\":1,\"64\":2,\"69\":2,\"71\":1}}],[\"p+n\",{\"1\":{\"11\":1}}],[\"palm\",{\"1\":{\"277\":1,\"278\":1}}],[\"passed\",{\"1\":{\"233\":1}}],[\"pass\",{\"1\":{\"224\":1,\"267\":1}}],[\"past\",{\"1\":{\"102\":3,\"103\":27}}],[\"package\",{\"1\":{\"147\":1}}],[\"packages\",{\"1\":{\"147\":1}}],[\"pandas\",{\"1\":{\"140\":1}}],[\"pan\",{\"1\":{\"107\":1,\"118\":1}}],[\"pad\",{\"1\":{\"103\":1,\"104\":2,\"224\":2,\"225\":9,\"226\":3,\"230\":11,\"233\":3}}],[\"padding=true\",{\"1\":{\"93\":1,\"95\":1}}],[\"padding=\",{\"1\":{\"10\":1,\"100\":1}}],[\"padding\",{\"0\":{\"230\":1},\"1\":{\"10\":2,\"13\":3,\"27\":1,\"31\":4,\"32\":1,\"33\":8,\"100\":1,\"103\":6,\"225\":1,\"226\":1,\"233\":6,\"236\":1,\"241\":1,\"242\":1,\"261\":1}}],[\"pair\",{\"1\":{\"174\":1,\"175\":3,\"176\":14,\"233\":2}}],[\"pair是否match\",{\"1\":{\"102\":1}}],[\"pairs\",{\"1\":{\"24\":1,\"175\":7,\"176\":4,\"212\":1,\"225\":14}}],[\"page\",{\"1\":{\"91\":1}}],[\"paris\",{\"1\":{\"255\":3}}],[\"paragraph\",{\"1\":{\"223\":7}}],[\"paraphrase\",{\"1\":{\"212\":1}}],[\"para\",{\"1\":{\"118\":2}}],[\"parameter\",{\"1\":{\"90\":1,\"110\":1,\"111\":2,\"114\":2,\"180\":1,\"182\":1,\"185\":1,\"249\":1}}],[\"parameters\",{\"1\":{\"37\":2,\"118\":1,\"227\":2,\"241\":1}}],[\"params\",{\"1\":{\"37\":3}}],[\"param\",{\"1\":{\"37\":2,\"107\":5,\"109\":6}}],[\"parts\",{\"1\":{\"256\":1}}],[\"partial\",{\"1\":{\"114\":1}}],[\"partitioning\",{\"1\":{\"43\":2}}],[\"part\",{\"1\":{\"40\":3}}],[\"path$bert\",{\"1\":{\"232\":2}}],[\"path=prev\",{\"1\":{\"232\":1}}],[\"path=val\",{\"1\":{\"108\":1}}],[\"path=train\",{\"1\":{\"108\":1}}],[\"paths\",{\"1\":{\"93\":17,\"94\":6,\"95\":21}}],[\"path\",{\"1\":{\"25\":3,\"39\":3,\"40\":5,\"93\":12,\"94\":2,\"95\":14,\"107\":30,\"108\":5,\"111\":1,\"112\":7,\"114\":2,\"175\":2,\"223\":12,\"224\":5,\"227\":5}}],[\"patch16\",{\"1\":{\"118\":6}}],[\"patch14\",{\"1\":{\"93\":2,\"95\":1}}],[\"patchembed\",{\"1\":{\"109\":2}}],[\"patches\",{\"1\":{\"30\":3,\"43\":1,\"109\":5,\"110\":6,\"111\":4,\"113\":11,\"114\":4,\"118\":1,\"119\":1}}],[\"patch\",{\"1\":{\"11\":5,\"30\":9,\"109\":12,\"110\":11,\"111\":5,\"114\":5,\"118\":1}}],[\"paper\",{\"1\":{\"17\":1,\"118\":1,\"202\":1,\"244\":1}}],[\"papers\",{\"1\":{\"17\":1}}],[\"pa\",{\"1\":{\"16\":5}}],[\"p\",{\"1\":{\"11\":15,\"15\":6,\"16\":27,\"27\":19,\"29\":1,\"31\":4,\"32\":1,\"37\":6,\"39\":6,\"104\":1,\"169\":3,\"175\":2,\"188\":2,\"224\":3,\"271\":5}}],[\"ptb\",{\"1\":{\"223\":1}}],[\"pt较高\",{\"1\":{\"169\":1}}],[\"pth\",{\"1\":{\"118\":1,\"227\":2}}],[\"pts\",{\"1\":{\"66\":2,\"68\":2}}],[\"pt\",{\"1\":{\"10\":1,\"39\":1,\"40\":1,\"93\":2,\"95\":2,\"100\":1,\"169\":7}}],[\"的向量数据库\",{\"1\":{\"291\":1}}],[\"的向量映射到词汇表空间\",{\"1\":{\"103\":1}}],[\"的生态系统\",{\"1\":{\"289\":1}}],[\"的深度集成\",{\"1\":{\"288\":1}}],[\"的深度融合\",{\"1\":{\"26\":1}}],[\"的持续优化和功能迭代\",{\"1\":{\"288\":1}}],[\"的巨大成功激发了越来越多的开发者兴趣\",{\"1\":{\"286\":1}}],[\"的可能性\",{\"1\":{\"282\":1}}],[\"的可操作性特征\",{\"1\":{\"16\":1}}],[\"的出现让人们重新思考了\",{\"1\":{\"282\":1}}],[\"的出现也掀起了新一轮的研究热潮\",{\"1\":{\"88\":1}}],[\"的战略\",{\"1\":{\"282\":1}}],[\"的概念\",{\"1\":{\"281\":1}}],[\"的概率最大\",{\"1\":{\"221\":1}}],[\"的概率随机水平翻转图像\",{\"1\":{\"108\":1}}],[\"的概率值\",{\"1\":{\"16\":1,\"35\":1,\"168\":1,\"169\":1}}],[\"的支持\",{\"1\":{\"278\":1,\"288\":1}}],[\"的卓越能力\",{\"1\":{\"278\":1}}],[\"的数据上进行预训练\",{\"1\":{\"278\":1}}],[\"的数值会很大\",{\"1\":{\"136\":1}}],[\"的指令模型\",{\"1\":{\"278\":1}}],[\"的指令调优版本\",{\"1\":{\"80\":1,\"81\":1}}],[\"的基础语言模型\",{\"1\":{\"278\":1}}],[\"的最后一个非思维链模型\",{\"1\":{\"278\":1}}],[\"的最大缺陷在于它\",{\"1\":{\"69\":1}}],[\"的会话应用\",{\"1\":{\"278\":1}}],[\"的推理水平\",{\"1\":{\"278\":1}}],[\"的推理机制\",{\"0\":{\"82\":1}}],[\"的推出\",{\"1\":{\"277\":1}}],[\"的神经网络架构\",{\"1\":{\"261\":1}}],[\"的神经网络模型\",{\"1\":{\"207\":1}}],[\"的角色进行分类\",{\"1\":{\"255\":1}}],[\"的模型战略形成了\",{\"1\":{\"278\":1}}],[\"的模型\",{\"1\":{\"255\":1}}],[\"的模块\",{\"1\":{\"64\":1}}],[\"的位置\",{\"1\":{\"253\":1}}],[\"的序列\",{\"1\":{\"253\":1}}],[\"的序列挤占了下游任务的输入序列空间\",{\"1\":{\"188\":1}}],[\"的隐藏状态\",{\"1\":{\"253\":1}}],[\"的隐藏维度\",{\"1\":{\"112\":1}}],[\"的问答任务中\",{\"1\":{\"253\":1}}],[\"的问题\",{\"1\":{\"169\":2}}],[\"的词执行掩码策略\",{\"1\":{\"225\":1}}],[\"的子集\",{\"1\":{\"223\":1}}],[\"的几率原封不动\",{\"1\":{\"218\":1}}],[\"的几率被替换成任意一个其它的\",{\"1\":{\"218\":1}}],[\"的几率被替换成\",{\"1\":{\"218\":1}}],[\"的时候也只计算被遮盖部分的\",{\"1\":{\"218\":1}}],[\"的时间复杂度虽然是\",{\"1\":{\"69\":1}}],[\"的语言模型\",{\"1\":{\"217\":1}}],[\"的语义而不是\",{\"1\":{\"218\":1}}],[\"的语义\",{\"1\":{\"11\":2,\"218\":1}}],[\"的绝对提升\",{\"1\":{\"212\":1}}],[\"的正余弦曲线\",{\"1\":{\"211\":1}}],[\"的正方形\",{\"1\":{\"160\":1}}],[\"的添加的线性输出层来预测\",{\"1\":{\"208\":1}}],[\"的激活状态\",{\"1\":{\"208\":1}}],[\"的变种\",{\"1\":{\"207\":1}}],[\"的变换矩阵\",{\"1\":{\"64\":2}}],[\"的思想与之有相通之处\",{\"1\":{\"192\":1}}],[\"的思想很简单\",{\"1\":{\"189\":1}}],[\"的本质是对训练数据的有效压缩\",{\"1\":{\"192\":1}}],[\"的影响\",{\"1\":{\"190\":1}}],[\"的影响就越大\",{\"1\":{\"170\":2}}],[\"的梯度就始终为\",{\"1\":{\"190\":1}}],[\"的计算中增加一个旁路\",{\"1\":{\"189\":1}}],[\"的计算流程如下\",{\"1\":{\"39\":1}}],[\"的特点与能力\",{\"0\":{\"279\":1}}],[\"的特例\",{\"1\":{\"189\":1}}],[\"的特征图\",{\"1\":{\"109\":1}}],[\"的特征进行汇总\",{\"1\":{\"54\":1}}],[\"的特征\",{\"1\":{\"43\":1}}],[\"的过程\",{\"1\":{\"189\":1}}],[\"的这种思想有点类似于残差连接\",{\"1\":{\"189\":1}}],[\"的训练过程中\",{\"1\":{\"189\":1}}],[\"的发现\",{\"1\":{\"188\":1}}],[\"的更新量与原始参数\",{\"1\":{\"187\":1}}],[\"的微调方法\",{\"1\":{\"185\":1}}],[\"的顺序排序\",{\"1\":{\"175\":1}}],[\"的极端值\",{\"1\":{\"172\":1}}],[\"的惩罚比例\",{\"1\":{\"172\":1}}],[\"的惩罚权重\",{\"1\":{\"170\":2}}],[\"的权重\",{\"1\":{\"170\":2}}],[\"的权重降低\",{\"1\":{\"169\":1}}],[\"的敏感度控制\",{\"1\":{\"170\":1}}],[\"的一个模块\",{\"1\":{\"286\":1}}],[\"的一个杰出应用就是\",{\"1\":{\"277\":1}}],[\"的一种早期形式\",{\"1\":{\"282\":1}}],[\"的一种泛化形式\",{\"1\":{\"170\":1}}],[\"的一致性\",{\"1\":{\"167\":1}}],[\"的改进\",{\"1\":{\"169\":1}}],[\"的改进版\",{\"1\":{\"35\":1,\"169\":1}}],[\"的对比\",{\"1\":{\"168\":1}}],[\"的对角线\",{\"1\":{\"160\":1}}],[\"的空间一致性\",{\"1\":{\"168\":1}}],[\"的区域\",{\"1\":{\"167\":1,\"255\":1}}],[\"的优点\",{\"1\":{\"167\":1}}],[\"的优势\",{\"1\":{\"69\":1,\"117\":1,\"166\":1}}],[\"的重合部分\",{\"1\":{\"166\":1}}],[\"的重合度\",{\"1\":{\"35\":1}}],[\"的替代指标\",{\"1\":{\"166\":1}}],[\"的情况下表现优异\",{\"1\":{\"166\":1}}],[\"的情况下会将随机垃圾邮件的垃圾邮件概率设为高于随机合法邮件的垃圾邮件概率\",{\"1\":{\"160\":1}}],[\"的阈值\",{\"1\":{\"162\":1}}],[\"的代价很高\",{\"1\":{\"162\":1}}],[\"的代价高于另一种错误\",{\"1\":{\"152\":1}}],[\"的垃圾邮件分类器仅在\",{\"1\":{\"160\":1}}],[\"的垃圾邮件分类器始终会为随机垃圾邮件分配比随机合规电子邮件更高的垃圾邮件概率\",{\"1\":{\"160\":1}}],[\"的真正例率\",{\"1\":{\"159\":1}}],[\"的真实值\",{\"1\":{\"103\":1}}],[\"的满分\",{\"1\":{\"157\":1}}],[\"的满分时\",{\"1\":{\"157\":1}}],[\"的不平衡数据集中\",{\"1\":{\"154\":1,\"155\":1}}],[\"的初始维度对结果的影响\",{\"0\":{\"124\":1}}],[\"的维度决定\",{\"1\":{\"127\":1}}],[\"的维度\",{\"1\":{\"123\":2}}],[\"的维度对最终注意力输出的结果维度有直接影响\",{\"1\":{\"122\":1}}],[\"的局部特征提取能力快速捕捉图像的底层特征\",{\"1\":{\"117\":1}}],[\"的图像块矩阵添加二维\",{\"1\":{\"111\":1}}],[\"的交并比\",{\"1\":{\"167\":1}}],[\"的交互\",{\"1\":{\"110\":1}}],[\"的交集\",{\"1\":{\"35\":1,\"168\":1}}],[\"的形式\",{\"1\":{\"109\":1}}],[\"的形状为\",{\"1\":{\"15\":1}}],[\"的张量\",{\"1\":{\"108\":2,\"109\":2}}],[\"的generate方法负责完成图像描述生成\",{\"1\":{\"104\":1}}],[\"的text\",{\"1\":{\"104\":1}}],[\"的文本对\",{\"1\":{\"253\":1}}],[\"的文本生成能力\",{\"1\":{\"104\":1}}],[\"的文本化表示\",{\"1\":{\"80\":1}}],[\"的预测\",{\"1\":{\"103\":1}}],[\"的预测结果\",{\"1\":{\"82\":1}}],[\"的输出向量能够很好地表示图像的全局特征\",{\"1\":{\"110\":2}}],[\"的输出向量被输入到分类头中\",{\"1\":{\"110\":1}}],[\"的输出蕴含了视觉信息\",{\"1\":{\"104\":1}}],[\"的输出\",{\"1\":{\"104\":1,\"110\":1,\"253\":1}}],[\"的输出仅由一个不超过\",{\"1\":{\"69\":1}}],[\"的输入序列\",{\"1\":{\"257\":1}}],[\"的输入方式是\",{\"1\":{\"257\":1}}],[\"的输入组织形式与普通分类或问答任务略有不同\",{\"1\":{\"257\":1}}],[\"的输入\",{\"1\":{\"103\":1}}],[\"的能力相结合\",{\"1\":{\"288\":1}}],[\"的能力\",{\"1\":{\"103\":1,\"278\":1}}],[\"的完整计算流程\",{\"1\":{\"102\":1}}],[\"的相似度\",{\"1\":{\"101\":1,\"125\":1}}],[\"的相同通道进行混合\",{\"1\":{\"30\":1}}],[\"的zero\",{\"1\":{\"96\":1}}],[\"的架构设计变得更加条理清晰和稳固\",{\"1\":{\"288\":1}}],[\"的架构中融入卷积操作\",{\"1\":{\"117\":1}}],[\"的架构\",{\"1\":{\"93\":1}}],[\"的性能提升\",{\"1\":{\"92\":1}}],[\"的性能会显著下降\",{\"1\":{\"69\":1}}],[\"的效果会好于其它几种方法\",{\"1\":{\"188\":1}}],[\"的效果\",{\"1\":{\"92\":1}}],[\"的vit\",{\"1\":{\"90\":1}}],[\"的设计\",{\"1\":{\"85\":1}}],[\"的研究如潮水般涌来\",{\"1\":{\"88\":1}}],[\"的研究\",{\"1\":{\"82\":1}}],[\"的参数是需要从头开始学习的\",{\"1\":{\"221\":1}}],[\"的参数会发生更新\",{\"1\":{\"190\":1}}],[\"的参数叠加\",{\"1\":{\"189\":1}}],[\"的参数不变\",{\"1\":{\"182\":1}}],[\"的参数\",{\"1\":{\"81\":1,\"187\":1,\"189\":1}}],[\"的关键一步\",{\"1\":{\"81\":1}}],[\"的关注信息\",{\"1\":{\"33\":1}}],[\"的短语\",{\"1\":{\"80\":2}}],[\"的方法\",{\"1\":{\"78\":1}}],[\"的方式会把prompt搞得很长\",{\"1\":{\"179\":1}}],[\"的方式\",{\"1\":{\"64\":1,\"197\":1}}],[\"的方式重构图像\",{\"1\":{\"11\":1}}],[\"的主要缺陷\",{\"1\":{\"69\":1}}],[\"的实验\",{\"1\":{\"69\":1}}],[\"的实现\",{\"1\":{\"29\":1,\"30\":1}}],[\"的限制\",{\"1\":{\"69\":1}}],[\"的表示\",{\"1\":{\"226\":1,\"253\":1}}],[\"的表示当前激活的环境\",{\"1\":{\"143\":1}}],[\"的表达能力\",{\"1\":{\"131\":1}}],[\"的表达能力受\",{\"1\":{\"69\":1}}],[\"的表现不如基于图结构的模型\",{\"1\":{\"69\":1}}],[\"的全局特征来自于\",{\"1\":{\"69\":1}}],[\"的分类准确率略低于\",{\"1\":{\"69\":1}}],[\"的分类模块\",{\"1\":{\"67\":1}}],[\"的分割网络将全局特征复制\",{\"1\":{\"69\":1}}],[\"的分割模块通过拼接全局特征\",{\"1\":{\"69\":1}}],[\"的分割模块\",{\"1\":{\"68\":1}}],[\"的作用是用\",{\"1\":{\"219\":1}}],[\"的作用是通过训练过程中损失值的降低\",{\"1\":{\"110\":1}}],[\"的作用\",{\"1\":{\"65\":1}}],[\"的矩阵\",{\"1\":{\"65\":1,\"109\":1}}],[\"的大小\",{\"1\":{\"64\":1}}],[\"的创新点\",{\"1\":{\"60\":1}}],[\"的mlp层\",{\"1\":{\"57\":1}}],[\"的整体结构是一个典型的\",{\"1\":{\"56\":1}}],[\"的核心\",{\"1\":{\"261\":1}}],[\"的核心思想\",{\"1\":{\"174\":1}}],[\"的核心思想是\",{\"1\":{\"30\":1,\"78\":1,\"198\":1}}],[\"的核心是\",{\"1\":{\"170\":1}}],[\"的核心特征提取模块\",{\"1\":{\"66\":1}}],[\"的核心模块\",{\"1\":{\"57\":1}}],[\"的核心就是逐层提取局部特征\",{\"1\":{\"50\":1}}],[\"的第\",{\"1\":{\"49\":1}}],[\"的结合也很简单\",{\"1\":{\"189\":1}}],[\"的结构过于简单\",{\"1\":{\"69\":1}}],[\"的结构\",{\"1\":{\"49\":1}}],[\"的结果\",{\"1\":{\"39\":1}}],[\"的点表示给定模型效果最佳的阈值范围\",{\"1\":{\"162\":1}}],[\"的点缺失\",{\"1\":{\"62\":1}}],[\"的点集群都将独立地送入对应的pointnet网络进行特征提取\",{\"1\":{\"52\":1}}],[\"的点及其特征\",{\"1\":{\"49\":1}}],[\"的点全部替换为\",{\"1\":{\"49\":1}}],[\"的点\",{\"1\":{\"49\":1}}],[\"的点云和功能标注\",{\"1\":{\"26\":1}}],[\"的索引数组\",{\"1\":{\"49\":1}}],[\"的索引\",{\"1\":{\"49\":1,\"226\":1}}],[\"的所有单词\",{\"1\":{\"175\":1}}],[\"的所有邻近点\",{\"1\":{\"49\":1}}],[\"的所有通道进行处理\",{\"1\":{\"30\":1}}],[\"的缩写\",{\"1\":{\"40\":1,\"115\":2}}],[\"的样本数\",{\"1\":{\"39\":1}}],[\"的组合形式\",{\"1\":{\"35\":1}}],[\"的加权组合\",{\"1\":{\"172\":1}}],[\"的加权信息\",{\"1\":{\"128\":1}}],[\"的加权和\",{\"1\":{\"35\":1,\"167\":1}}],[\"的加入是为了让\",{\"1\":{\"35\":1}}],[\"的得分结果\",{\"1\":{\"33\":1}}],[\"的信息让模型分开上下句\",{\"1\":{\"219\":1}}],[\"的信息\",{\"1\":{\"33\":1}}],[\"的有效性\",{\"1\":{\"32\":1}}],[\"的网络结构可视化理解\",{\"1\":{\"32\":1}}],[\"的堆叠结构\",{\"1\":{\"30\":1}}],[\"的视觉模型架构\",{\"1\":{\"30\":1}}],[\"的行\",{\"1\":{\"25\":1}}],[\"的目标是\",{\"1\":{\"28\":1}}],[\"的目标是预测出与该问题相关的点云区域\",{\"1\":{\"27\":1}}],[\"的目标\",{\"1\":{\"19\":1}}],[\"的拼接特征\",{\"1\":{\"16\":1}}],[\"的列表\",{\"1\":{\"13\":1}}],[\"的损失函数\",{\"1\":{\"168\":1}}],[\"的损失\",{\"1\":{\"10\":1}}],[\"的\",{\"1\":{\"10\":2,\"15\":5,\"33\":1,\"40\":1,\"50\":1,\"53\":1,\"78\":2,\"103\":9,\"188\":1,\"211\":2,\"218\":1,\"221\":5,\"224\":1,\"233\":1,\"253\":1,\"255\":1,\"278\":8,\"290\":1}}],[\"4v\",{\"1\":{\"278\":1}}],[\"4b\",{\"1\":{\"278\":1}}],[\"49\",{\"1\":{\"278\":1}}],[\"4960\",{\"1\":{\"233\":2}}],[\"4o\",{\"1\":{\"278\":5}}],[\"4788\",{\"1\":{\"233\":2}}],[\"4638\",{\"1\":{\"233\":2}}],[\"4个特殊词\",{\"1\":{\"224\":1}}],[\"4替换为相应的类别名称\",{\"1\":{\"107\":1}}],[\"404\",{\"1\":{\"295\":1}}],[\"405b\",{\"1\":{\"278\":3}}],[\"40\",{\"1\":{\"211\":1}}],[\"400m数据\",{\"1\":{\"98\":1}}],[\"4096\",{\"1\":{\"15\":1,\"58\":2}}],[\"4873\",{\"1\":{\"233\":2}}],[\"48\",{\"1\":{\"93\":1}}],[\"485\",{\"1\":{\"80\":1}}],[\"4️⃣\",{\"0\":{\"32\":1},\"1\":{\"57\":1}}],[\"416\",{\"1\":{\"22\":1}}],[\"4\",{\"0\":{\"22\":1,\"82\":2,\"111\":1},\"1\":{\"10\":1,\"11\":1,\"20\":3,\"26\":1,\"33\":2,\"35\":3,\"38\":1,\"39\":2,\"40\":3,\"50\":3,\"53\":3,\"58\":1,\"62\":1,\"78\":1,\"81\":2,\"82\":6,\"103\":1,\"104\":1,\"113\":1,\"118\":1,\"162\":1,\"169\":1,\"175\":5,\"191\":1,\"192\":2,\"211\":2,\"212\":1,\"221\":1,\"224\":1,\"226\":2,\"227\":2,\"236\":1,\"257\":1,\"271\":1,\"278\":26}}],[\"qwq\",{\"1\":{\"278\":2,\"292\":2}}],[\"qwen3\",{\"1\":{\"278\":2}}],[\"qwen2\",{\"1\":{\"278\":3}}],[\"qwen\",{\"1\":{\"278\":4}}],[\"qqp\",{\"1\":{\"212\":1}}],[\"q进行\",{\"1\":{\"192\":1}}],[\"quora\",{\"1\":{\"212\":2}}],[\"quantized\",{\"1\":{\"185\":1,\"192\":1}}],[\"quantization\",{\"1\":{\"185\":1}}],[\"queies\",{\"1\":{\"103\":1}}],[\"queries被用来从image\",{\"1\":{\"100\":1}}],[\"queries是一组可学习的embeddings\",{\"1\":{\"100\":1}}],[\"queries\",{\"1\":{\"27\":1,\"33\":3,\"100\":3,\"101\":1,\"102\":1,\"104\":3}}],[\"query的数量\",{\"1\":{\"123\":1}}],[\"query的响应图进行平均池化\",{\"1\":{\"33\":1}}],[\"query和query\",{\"1\":{\"101\":1}}],[\"query是基于欧氏距离的均匀性假设\",{\"1\":{\"47\":1}}],[\"query通过确保每个局部区域都有一个固定的尺度\",{\"1\":{\"47\":1}}],[\"query通过固定区域尺度而不是固定邻居数量来定义邻域\",{\"1\":{\"47\":1}}],[\"query找到该查询点在半径为𝑟范围内点\",{\"1\":{\"47\":1}}],[\"query来查询形心的邻居点\",{\"1\":{\"47\":1}}],[\"query=self\",{\"1\":{\"33\":1}}],[\"query=x\",{\"1\":{\"32\":1}}],[\"query=q\",{\"1\":{\"32\":1}}],[\"query\",{\"1\":{\"15\":13,\"27\":1,\"29\":8,\"31\":8,\"33\":10,\"49\":7,\"53\":2,\"55\":1,\"94\":4,\"95\":4,\"100\":8,\"101\":2,\"102\":17,\"103\":30,\"104\":5,\"122\":1,\"123\":1,\"125\":1,\"127\":1,\"128\":2,\"130\":2,\"132\":1,\"244\":7,\"271\":9,\"278\":4}}],[\"question数据可视化图\",{\"1\":{\"25\":1}}],[\"question数据可视化\",{\"1\":{\"20\":1}}],[\"question\",{\"0\":{\"20\":1},\"1\":{\"25\":6,\"33\":1,\"38\":2,\"39\":2,\"212\":1,\"253\":1}}],[\"qlora就是量化版的lora\",{\"1\":{\"185\":1}}],[\"qlora\",{\"0\":{\"185\":1},\"1\":{\"185\":1,\"192\":2}}],[\"qk^t\",{\"1\":{\"136\":1}}],[\"qk\",{\"1\":{\"111\":1,\"112\":2,\"113\":2,\"114\":2}}],[\"qkv\",{\"1\":{\"111\":1,\"112\":2,\"113\":8,\"114\":2,\"189\":1}}],[\"q来自query\",{\"1\":{\"103\":1}}],[\"q2t\",{\"1\":{\"101\":2}}],[\"qa\",{\"1\":{\"81\":1,\"85\":1,\"253\":2,\"254\":2,\"255\":1}}],[\"qid\",{\"1\":{\"25\":5}}],[\"q\",{\"0\":{\"124\":1},\"1\":{\"10\":1,\"15\":9,\"27\":1,\"29\":6,\"31\":3,\"32\":5,\"33\":2,\"100\":3,\"103\":2,\"104\":3,\"113\":5,\"122\":1,\"128\":1,\"137\":2,\"230\":3,\"271\":1}}],[\"qformer\",{\"1\":{\"10\":1,\"100\":2,\"102\":1,\"103\":3,\"104\":1}}],[\"r1看scaling\",{\"1\":{\"292\":1}}],[\"r1\",{\"1\":{\"278\":5}}],[\"rmsnorm\",{\"1\":{\"278\":1}}],[\"rl\",{\"1\":{\"277\":1,\"278\":1}}],[\"rlhf\",{\"1\":{\"78\":2}}],[\"rte数据集比较小\",{\"1\":{\"212\":1}}],[\"r=1\",{\"1\":{\"191\":1}}],[\"r就是上述假设中的低维\",{\"1\":{\"184\":1}}],[\"r维\",{\"1\":{\"184\":1}}],[\"r\",{\"1\":{\"147\":1,\"175\":3,\"191\":2,\"223\":1,\"224\":2,\"259\":1}}],[\"rwightman\",{\"1\":{\"118\":1}}],[\"rn50x16和rnx64\",{\"1\":{\"90\":1}}],[\"rn50x4\",{\"1\":{\"90\":1}}],[\"rnn等模型的缺点是需要顺序计算\",{\"1\":{\"260\":1}}],[\"rnn\",{\"1\":{\"62\":1}}],[\"rgb为彩色图片\",{\"1\":{\"107\":1}}],[\"rgb\",{\"1\":{\"71\":1,\"93\":1,\"95\":1,\"107\":3,\"109\":1}}],[\"rigid\",{\"1\":{\"65\":1,\"73\":1}}],[\"right\",{\"1\":{\"10\":1,\"269\":1}}],[\"runs\",{\"1\":{\"40\":1}}],[\"run\",{\"1\":{\"40\":1,\"148\":1,\"232\":1}}],[\"rpo\",{\"0\":{\"33\":1}}],[\"rpd\",{\"1\":{\"27\":1,\"33\":2}}],[\"rotary\",{\"1\":{\"278\":1}}],[\"rope\",{\"1\":{\"278\":4}}],[\"row\",{\"1\":{\"223\":2}}],[\"rocket\",{\"1\":{\"91\":1}}],[\"roc\",{\"0\":{\"158\":1,\"159\":1,\"162\":1},\"1\":{\"39\":4,\"159\":3,\"160\":3,\"161\":1,\"162\":2}}],[\"roberta\",{\"1\":{\"29\":1}}],[\"root\",{\"1\":{\"25\":3,\"40\":3,\"93\":2,\"95\":2,\"107\":8}}],[\"root=\",{\"1\":{\"25\":1}}],[\"r^\",{\"1\":{\"29\":2,\"30\":1}}],[\"rb\",{\"1\":{\"25\":2,\"40\":2}}],[\"rb×cp×np\",{\"1\":{\"10\":1}}],[\"rb×ci×h×w\",{\"1\":{\"10\":1}}],[\"rag\",{\"0\":{\"283\":1,\"285\":1},\"1\":{\"283\":8,\"284\":1,\"285\":4,\"286\":1,\"292\":1}}],[\"race数据集由初高中考试题构成\",{\"1\":{\"212\":1}}],[\"race\",{\"1\":{\"203\":1,\"204\":1}}],[\"ratio\",{\"1\":{\"111\":1,\"112\":7,\"113\":2,\"114\":4,\"172\":4}}],[\"ratio=dpr\",{\"1\":{\"114\":1}}],[\"ratio=drop\",{\"1\":{\"112\":1,\"114\":1}}],[\"ratio=mlp\",{\"1\":{\"114\":1}}],[\"ratio=attn\",{\"1\":{\"112\":1,\"114\":1}}],[\"ratio=0\",{\"1\":{\"111\":3,\"112\":3,\"113\":2,\"114\":3}}],[\"ratio=4\",{\"1\":{\"111\":1,\"112\":1,\"114\":1}}],[\"rate=2e\",{\"1\":{\"232\":1}}],[\"rate\",{\"1\":{\"37\":1,\"107\":2}}],[\"radius=none\",{\"1\":{\"50\":1}}],[\"radius=0\",{\"1\":{\"50\":2,\"58\":4}}],[\"radius^2\",{\"1\":{\"49\":1}}],[\"radius\",{\"1\":{\"49\":12,\"53\":7}}],[\"raise\",{\"1\":{\"25\":1,\"107\":1,\"224\":1}}],[\"rank\",{\"1\":{\"184\":1,\"186\":1,\"188\":1,\"189\":1,\"235\":1}}],[\"rand\",{\"1\":{\"224\":2}}],[\"randint\",{\"1\":{\"25\":1,\"49\":1,\"224\":1}}],[\"randomsampler\",{\"1\":{\"235\":1}}],[\"randomhorizontalflip\",{\"1\":{\"108\":1}}],[\"randomresizedcrop\",{\"1\":{\"108\":2}}],[\"random\",{\"1\":{\"25\":1,\"52\":1,\"107\":2,\"224\":5,\"225\":4}}],[\"range\",{\"1\":{\"13\":1,\"38\":1,\"39\":2,\"49\":1,\"53\":2,\"93\":1,\"95\":1,\"102\":2,\"103\":1,\"107\":2,\"114\":1,\"175\":2,\"176\":1,\"224\":1,\"225\":2,\"226\":1,\"227\":1,\"239\":1}}],[\"raw=dict\",{\"1\":{\"37\":1,\"40\":1}}],[\"raw\",{\"1\":{\"16\":10,\"37\":1,\"40\":1,\"166\":1,\"167\":1,\"244\":1}}],[\"retrieval\",{\"0\":{\"283\":1},\"1\":{\"283\":1,\"292\":1}}],[\"returnfps\",{\"1\":{\"49\":1}}],[\"returnfps=false\",{\"1\":{\"49\":1}}],[\"returns\",{\"1\":{\"13\":1,\"15\":1,\"16\":1,\"40\":2,\"233\":1}}],[\"return\",{\"1\":{\"10\":4,\"11\":1,\"13\":1,\"15\":2,\"16\":1,\"25\":3,\"27\":1,\"29\":2,\"30\":1,\"31\":2,\"32\":1,\"33\":2,\"35\":1,\"40\":2,\"49\":13,\"50\":1,\"53\":3,\"55\":1,\"57\":1,\"58\":1,\"64\":1,\"65\":1,\"66\":2,\"67\":1,\"68\":1,\"93\":9,\"94\":4,\"95\":13,\"100\":3,\"102\":2,\"103\":10,\"104\":1,\"107\":7,\"109\":2,\"110\":2,\"111\":2,\"112\":2,\"113\":1,\"114\":2,\"118\":1,\"166\":1,\"167\":1,\"168\":1,\"169\":1,\"170\":1,\"172\":1,\"175\":7,\"176\":5,\"223\":1,\"224\":5,\"225\":2,\"226\":1,\"230\":2,\"233\":6,\"235\":1,\"236\":1,\"238\":3,\"239\":1,\"240\":1,\"241\":1,\"242\":1,\"244\":2,\"245\":1,\"246\":1,\"248\":1,\"249\":1,\"250\":1,\"251\":1,\"254\":1,\"256\":1,\"257\":1,\"262\":3,\"263\":1,\"265\":1,\"266\":1,\"267\":1,\"269\":1,\"270\":1,\"271\":2}}],[\"reverse=true\",{\"1\":{\"175\":1}}],[\"re\",{\"1\":{\"175\":2}}],[\"requirements\",{\"1\":{\"147\":2,\"259\":2}}],[\"requires\",{\"1\":{\"37\":2,\"118\":1}}],[\"remove\",{\"1\":{\"144\":1}}],[\"relationship\",{\"1\":{\"250\":4,\"251\":4}}],[\"related\",{\"1\":{\"112\":1}}],[\"releases\",{\"1\":{\"118\":1}}],[\"relu\",{\"1\":{\"11\":2,\"12\":1,\"14\":1,\"15\":1,\"16\":2,\"49\":2,\"50\":2,\"53\":5,\"55\":1,\"57\":4,\"58\":1,\"64\":8,\"66\":2,\"67\":4,\"68\":3,\"278\":1}}],[\"reduction\",{\"1\":{\"103\":1,\"169\":1}}],[\"reduction=reduction\",{\"1\":{\"103\":1}}],[\"reduction=\",{\"1\":{\"103\":1,\"167\":1,\"169\":1}}],[\"red\",{\"1\":{\"91\":1}}],[\"realnews\",{\"1\":{\"278\":1}}],[\"reasoning\",{\"1\":{\"81\":1,\"83\":1,\"198\":1,\"199\":1,\"200\":1}}],[\"reader\",{\"1\":{\"223\":3}}],[\"read\",{\"1\":{\"25\":1,\"40\":2,\"107\":1,\"108\":1,\"175\":1,\"176\":1,\"232\":9}}],[\"reward\",{\"1\":{\"78\":2}}],[\"reinforcement\",{\"1\":{\"78\":1,\"180\":2}}],[\"regression\",{\"1\":{\"242\":1}}],[\"register\",{\"1\":{\"103\":1}}],[\"region\",{\"1\":{\"39\":1,\"49\":2}}],[\"regularizer\",{\"1\":{\"65\":3}}],[\"reg\",{\"1\":{\"62\":1}}],[\"recall\",{\"0\":{\"153\":1},\"1\":{\"170\":1}}],[\"recip\",{\"1\":{\"57\":3}}],[\"recovered\",{\"1\":{\"55\":1}}],[\"receiver\",{\"0\":{\"159\":1},\"1\":{\"39\":1}}],[\"repetition\",{\"1\":{\"104\":1}}],[\"repeat\",{\"1\":{\"49\":6,\"57\":1,\"64\":1,\"66\":1,\"104\":1}}],[\"representation\",{\"0\":{\"100\":1},\"1\":{\"100\":3,\"101\":2,\"102\":2,\"111\":1,\"114\":3,\"118\":1,\"217\":1,\"255\":1}}],[\"repo\",{\"1\":{\"93\":1,\"95\":1}}],[\"replace\",{\"1\":{\"93\":1,\"95\":1,\"224\":2}}],[\"rephrase\",{\"1\":{\"25\":3}}],[\"rendering\",{\"1\":{\"60\":1}}],[\"render\",{\"1\":{\"40\":1}}],[\"reference\",{\"1\":{\"40\":2}}],[\"referred\",{\"1\":{\"27\":2,\"33\":3}}],[\"refrigerator\",{\"1\":{\"25\":1}}],[\"resize\",{\"1\":{\"108\":2}}],[\"residual\",{\"1\":{\"29\":1,\"31\":1,\"261\":1}}],[\"resnet和混合模型的效果均不如vit模型\",{\"1\":{\"117\":1}}],[\"resnet和混合模型在不同图像分类数据集上的测试结果\",{\"1\":{\"117\":1}}],[\"resnet101\",{\"1\":{\"90\":1}}],[\"resnet18\",{\"1\":{\"10\":1}}],[\"resnet50\",{\"1\":{\"90\":1}}],[\"resnet包含五种不同尺寸的模型\",{\"1\":{\"90\":1}}],[\"resnet\",{\"1\":{\"90\":1,\"117\":1}}],[\"response\",{\"1\":{\"78\":1}}],[\"resolution\",{\"0\":{\"54\":1},\"1\":{\"51\":1,\"55\":1}}],[\"reshaped\",{\"1\":{\"257\":4}}],[\"reshape\",{\"1\":{\"49\":1,\"64\":1,\"113\":4,\"257\":1}}],[\"research\",{\"1\":{\"30\":1,\"118\":1,\"202\":1,\"212\":1}}],[\"results\",{\"1\":{\"38\":1,\"39\":3}}],[\"result\",{\"1\":{\"25\":5}}],[\"∈\",{\"1\":{\"10\":2,\"29\":2,\"30\":1,\"33\":1,\"39\":1,\"168\":1}}],[\"=>\",{\"1\":{\"271\":1}}],[\"=================\",{\"1\":{\"103\":1}}],[\"========================\",{\"1\":{\"103\":1}}],[\"===================\",{\"1\":{\"101\":1,\"102\":1}}],[\"==============\",{\"1\":{\"101\":1,\"102\":1}}],[\"==\",{\"1\":{\"10\":1,\"25\":3,\"39\":1,\"40\":2,\"49\":1,\"57\":1,\"93\":1,\"95\":1,\"102\":1,\"103\":2,\"169\":1,\"176\":1,\"223\":1,\"224\":2,\"227\":1,\"235\":1,\"238\":1,\"242\":1,\"248\":1,\"255\":1,\"256\":1,\"259\":1,\"271\":2}}],[\"=\",{\"1\":{\"10\":20,\"11\":24,\"12\":1,\"13\":7,\"14\":1,\"15\":36,\"16\":18,\"20\":1,\"25\":25,\"27\":15,\"29\":4,\"30\":8,\"31\":5,\"32\":4,\"33\":27,\"35\":16,\"37\":15,\"38\":10,\"39\":35,\"40\":45,\"49\":63,\"50\":24,\"53\":53,\"55\":2,\"57\":24,\"58\":26,\"62\":2,\"64\":25,\"65\":5,\"66\":30,\"67\":13,\"68\":20,\"73\":1,\"90\":9,\"91\":10,\"93\":40,\"94\":9,\"95\":46,\"100\":10,\"101\":8,\"102\":40,\"103\":56,\"104\":10,\"107\":25,\"108\":6,\"109\":11,\"110\":12,\"111\":15,\"112\":19,\"113\":16,\"114\":28,\"115\":2,\"118\":5,\"166\":7,\"167\":12,\"168\":10,\"169\":17,\"170\":16,\"172\":15,\"175\":32,\"176\":19,\"178\":3,\"217\":1,\"223\":11,\"224\":30,\"225\":22,\"226\":21,\"227\":23,\"228\":4,\"230\":6,\"233\":14,\"235\":7,\"236\":16,\"238\":17,\"239\":2,\"240\":5,\"241\":10,\"242\":14,\"244\":25,\"245\":6,\"246\":4,\"248\":7,\"249\":5,\"250\":4,\"251\":11,\"253\":9,\"254\":17,\"255\":2,\"256\":16,\"257\":17,\"262\":5,\"263\":1,\"265\":2,\"266\":6,\"267\":3,\"269\":9,\"270\":4,\"271\":15}}],[\"ftfy\",{\"1\":{\"211\":1}}],[\"france\",{\"1\":{\"255\":2}}],[\"freqs\",{\"1\":{\"175\":14,\"176\":2}}],[\"freq\",{\"1\":{\"175\":3}}],[\"frozen\",{\"1\":{\"100\":1}}],[\"frobenius\",{\"1\":{\"65\":2}}],[\"from\",{\"1\":{\"40\":3,\"64\":1,\"78\":1,\"93\":2,\"94\":2,\"95\":8,\"96\":2,\"100\":1,\"107\":2,\"118\":2,\"217\":1,\"227\":1,\"233\":2,\"244\":1,\"271\":1}}],[\"f1\",{\"0\":{\"157\":1},\"1\":{\"157\":4}}],[\"fft的原理\",{\"1\":{\"180\":1}}],[\"ffmpeg\",{\"1\":{\"147\":1}}],[\"ffn\",{\"1\":{\"31\":1}}],[\"fn负责对返回的一个batch\",{\"1\":{\"235\":1}}],[\"fn=collate\",{\"1\":{\"235\":1}}],[\"fn=val\",{\"1\":{\"108\":1}}],[\"fn=train\",{\"1\":{\"108\":1}}],[\"fn\",{\"1\":{\"107\":2,\"108\":2,\"151\":3,\"152\":1,\"170\":8,\"172\":2,\"235\":2,\"238\":3,\"248\":3}}],[\"feed\",{\"1\":{\"211\":1,\"261\":1,\"266\":4,\"269\":5}}],[\"feedback\",{\"1\":{\"78\":1,\"180\":2}}],[\"feat相似度最大的那个query\",{\"1\":{\"101\":2}}],[\"feats\",{\"1\":{\"100\":1,\"101\":5}}],[\"feat=false\",{\"1\":{\"66\":2,\"68\":1}}],[\"feat=true\",{\"1\":{\"66\":2,\"67\":1}}],[\"feat\",{\"1\":{\"27\":8,\"33\":8,\"66\":10,\"67\":4,\"68\":4,\"100\":1,\"101\":8}}],[\"featured\",{\"1\":{\"223\":1}}],[\"features=none\",{\"1\":{\"112\":2}}],[\"features=mlp\",{\"1\":{\"112\":1}}],[\"features=dim\",{\"1\":{\"112\":1}}],[\"features\",{\"1\":{\"10\":1,\"55\":2,\"91\":10,\"93\":4,\"95\":4,\"96\":1,\"110\":3,\"111\":3,\"112\":15,\"114\":5,\"233\":1}}],[\"feature\",{\"1\":{\"10\":18,\"11\":19,\"15\":9,\"16\":10,\"30\":2,\"33\":1,\"43\":2,\"49\":1,\"53\":1,\"55\":5,\"56\":1,\"57\":1,\"58\":2,\"65\":3,\"66\":5,\"67\":4,\"68\":3,\"79\":1}}],[\"fstn\",{\"1\":{\"66\":2}}],[\"f^2\",{\"1\":{\"62\":1}}],[\"fxia22\",{\"1\":{\"59\":1}}],[\"fct\",{\"1\":{\"103\":2,\"242\":4,\"251\":3,\"254\":3,\"256\":3,\"257\":2}}],[\"fc\",{\"1\":{\"64\":1,\"104\":1,\"114\":1,\"226\":5}}],[\"fc3\",{\"1\":{\"50\":2,\"53\":2,\"64\":2,\"67\":2}}],[\"fc2\",{\"1\":{\"50\":2,\"53\":2,\"64\":2,\"67\":2,\"112\":2}}],[\"fc1\",{\"1\":{\"50\":2,\"53\":2,\"64\":2,\"67\":2,\"112\":2}}],[\"figure\",{\"1\":{\"266\":1,\"269\":1,\"271\":1}}],[\"fill\",{\"1\":{\"103\":1,\"104\":1,\"230\":1,\"271\":1}}],[\"filterwarnings\",{\"1\":{\"95\":1}}],[\"filtering\",{\"1\":{\"80\":1}}],[\"file$bert\",{\"1\":{\"232\":1}}],[\"filepath\",{\"1\":{\"176\":1}}],[\"filepaths\",{\"1\":{\"175\":5}}],[\"file\",{\"1\":{\"93\":5,\"95\":5,\"107\":2,\"223\":2,\"224\":3}}],[\"files\",{\"1\":{\"93\":2,\"95\":2}}],[\"finetuning\",{\"1\":{\"185\":1}}],[\"finetune\",{\"0\":{\"285\":1},\"1\":{\"90\":1,\"285\":1}}],[\"fine\",{\"0\":{\"177\":1,\"221\":1},\"1\":{\"78\":1,\"79\":1,\"177\":1,\"180\":3,\"185\":1,\"189\":2,\"203\":1,\"221\":1}}],[\"final\",{\"1\":{\"55\":1,\"271\":1}}],[\"find\",{\"1\":{\"25\":2,\"94\":2,\"95\":2}}],[\"first\",{\"1\":{\"49\":2,\"83\":1,\"233\":2,\"240\":3}}],[\"flowerclassify\",{\"1\":{\"93\":1,\"95\":1}}],[\"flower\",{\"1\":{\"93\":4,\"95\":2,\"107\":8}}],[\"float32\",{\"1\":{\"64\":1}}],[\"float64\",{\"1\":{\"40\":4}}],[\"float\",{\"1\":{\"27\":2,\"33\":3,\"40\":1,\"91\":2,\"107\":1,\"166\":1,\"167\":1,\"168\":1,\"169\":3,\"227\":2}}],[\"flashattention\",{\"1\":{\"278\":1}}],[\"flash\",{\"1\":{\"278\":6}}],[\"flag\",{\"1\":{\"91\":1}}],[\"flan\",{\"1\":{\"78\":1,\"85\":1}}],[\"flatten\",{\"1\":{\"39\":2,\"109\":1}}],[\"full\",{\"1\":{\"180\":1,\"189\":2}}],[\"fullattncatblock\",{\"1\":{\"31\":1}}],[\"function\",{\"1\":{\"62\":1,\"72\":1,\"114\":1,\"244\":1,\"278\":2}}],[\"functional\",{\"1\":{\"35\":1}}],[\"fusion\",{\"1\":{\"10\":1,\"11\":1,\"15\":2,\"27\":1}}],[\"fast\",{\"1\":{\"223\":2}}],[\"fastai\",{\"1\":{\"223\":1}}],[\"facial\",{\"1\":{\"91\":1}}],[\"farthest\",{\"1\":{\"49\":8,\"53\":1,\"55\":1}}],[\"faucet\",{\"1\":{\"25\":1}}],[\"false\",{\"1\":{\"10\":1,\"66\":1,\"118\":1,\"170\":6,\"176\":1}}],[\"fp4\",{\"1\":{\"58\":2}}],[\"fp\",{\"1\":{\"56\":3,\"58\":1,\"151\":3,\"152\":1,\"170\":8,\"172\":2}}],[\"fps是一种在点云\",{\"1\":{\"46\":1}}],[\"fps\",{\"1\":{\"46\":1,\"49\":7,\"53\":1,\"55\":1}}],[\"fpr\",{\"1\":{\"39\":1,\"154\":3,\"159\":3,\"162\":2}}],[\"fp16\",{\"1\":{\"241\":1}}],[\"fp1\",{\"1\":{\"16\":3,\"27\":1,\"58\":2}}],[\"fp2\",{\"1\":{\"16\":3,\"27\":1,\"58\":2}}],[\"fp3\",{\"1\":{\"16\":4,\"27\":1,\"58\":2}}],[\"f\",{\"1\":{\"11\":2,\"16\":5,\"25\":6,\"27\":2,\"30\":2,\"31\":1,\"35\":1,\"39\":1,\"40\":8,\"49\":1,\"50\":3,\"53\":4,\"57\":1,\"58\":2,\"62\":2,\"64\":5,\"66\":2,\"67\":3,\"68\":4,\"90\":4,\"93\":7,\"94\":4,\"95\":11,\"100\":2,\"101\":2,\"102\":3,\"166\":1,\"167\":3,\"169\":2,\"170\":1,\"223\":8,\"224\":7,\"227\":3,\"263\":1}}],[\"f3d\",{\"1\":{\"10\":1}}],[\"f2d\",{\"1\":{\"10\":1}}],[\"found\",{\"1\":{\"94\":1,\"95\":1,\"107\":1,\"295\":1}}],[\"foundation\",{\"1\":{\"76\":1,\"182\":1,\"281\":1}}],[\"follow\",{\"1\":{\"266\":1,\"269\":1}}],[\"following\",{\"1\":{\"78\":2,\"233\":1}}],[\"folders\",{\"1\":{\"4\":1}}],[\"focalloss\",{\"1\":{\"169\":2}}],[\"focal\",{\"0\":{\"169\":1},\"1\":{\"10\":1,\"35\":6,\"168\":1,\"169\":23}}],[\"forgetting\",{\"1\":{\"180\":1}}],[\"forge\",{\"1\":{\"147\":1}}],[\"format\",{\"1\":{\"39\":1,\"107\":5,\"118\":1,\"233\":1}}],[\"former的生成方法\",{\"1\":{\"104\":1}}],[\"former学习\",{\"1\":{\"103\":1}}],[\"former类比为一个self\",{\"1\":{\"100\":1}}],[\"former模块做模态融合\",{\"1\":{\"98\":1}}],[\"former\",{\"1\":{\"10\":1,\"100\":4,\"103\":2,\"104\":3}}],[\"forward\",{\"1\":{\"10\":1,\"11\":1,\"15\":2,\"16\":1,\"27\":2,\"29\":3,\"30\":1,\"31\":3,\"32\":1,\"33\":2,\"35\":1,\"49\":1,\"50\":1,\"53\":2,\"55\":1,\"57\":1,\"58\":1,\"64\":1,\"66\":1,\"67\":1,\"68\":1,\"100\":1,\"102\":1,\"103\":5,\"109\":1,\"110\":3,\"111\":3,\"112\":2,\"113\":1,\"114\":3,\"166\":1,\"167\":1,\"168\":1,\"169\":1,\"170\":1,\"172\":1,\"211\":1,\"226\":1,\"230\":1,\"236\":1,\"238\":3,\"239\":1,\"240\":1,\"241\":1,\"242\":1,\"244\":2,\"245\":1,\"246\":1,\"248\":1,\"249\":1,\"250\":1,\"251\":1,\"254\":1,\"256\":1,\"257\":1,\"261\":1,\"262\":1,\"263\":1,\"265\":1,\"266\":5,\"267\":1,\"269\":6,\"270\":1,\"271\":1}}],[\"for\",{\"1\":{\"4\":1,\"13\":1,\"25\":3,\"30\":1,\"37\":2,\"38\":2,\"39\":4,\"40\":1,\"49\":3,\"53\":4,\"57\":2,\"64\":1,\"76\":1,\"91\":1,\"93\":6,\"94\":2,\"95\":8,\"102\":2,\"103\":8,\"107\":9,\"114\":2,\"118\":1,\"169\":1,\"175\":14,\"176\":5,\"182\":1,\"183\":1,\"223\":4,\"224\":8,\"225\":7,\"226\":2,\"227\":4,\"239\":2,\"244\":5,\"249\":1,\"266\":1,\"267\":1,\"269\":1,\"270\":1,\"271\":1,\"292\":1}}],[\"tf\",{\"1\":{\"232\":2}}],[\"tying\",{\"1\":{\"226\":1}}],[\"type=bert\",{\"1\":{\"232\":1}}],[\"type\",{\"1\":{\"25\":1,\"102\":1,\"233\":9,\"235\":4,\"236\":9,\"241\":3,\"242\":3,\"251\":3,\"253\":2,\"254\":3,\"255\":1,\"256\":3,\"257\":7}}],[\"tgz\",{\"1\":{\"223\":2}}],[\"tgt2\",{\"1\":{\"33\":6}}],[\"tgt\",{\"1\":{\"27\":1,\"33\":21,\"262\":12,\"269\":2,\"270\":2}}],[\"tqdm\",{\"1\":{\"175\":1}}],[\"tverskyloss\",{\"1\":{\"170\":2}}],[\"tversky指数简化为dice系数\",{\"1\":{\"170\":1}}],[\"tversky\",{\"0\":{\"170\":1},\"1\":{\"170\":9,\"172\":1}}],[\"tnews\",{\"1\":{\"232\":4}}],[\"tn\",{\"1\":{\"151\":3,\"152\":1}}],[\"tpu\",{\"1\":{\"279\":1}}],[\"tp+α⋅fp+β⋅fn\",{\"1\":{\"170\":1}}],[\"tp\",{\"1\":{\"151\":3,\"152\":1,\"168\":1,\"170\":6}}],[\"tpr\",{\"1\":{\"39\":1,\"153\":2,\"159\":3,\"162\":3}}],[\"t2i\",{\"1\":{\"101\":4,\"102\":5}}],[\"t2q\",{\"1\":{\"101\":2}}],[\"two\",{\"1\":{\"99\":2,\"105\":1,\"233\":1}}],[\"tie\",{\"1\":{\"226\":1}}],[\"tinybert\",{\"1\":{\"216\":2}}],[\"title\",{\"1\":{\"94\":1,\"95\":1,\"107\":1}}],[\"time\",{\"1\":{\"93\":10,\"95\":10,\"277\":1}}],[\"turbo\",{\"1\":{\"278\":1}}],[\"turn\",{\"1\":{\"267\":1}}],[\"tune\",{\"1\":{\"78\":1}}],[\"tuning完全不相同的另一条技术路线\",{\"1\":{\"184\":1}}],[\"tuning和prefix\",{\"1\":{\"184\":1}}],[\"tuning也保证了基座模型本身是没有变的\",{\"1\":{\"183\":1}}],[\"tuning是在transformer的encoder和decoder的网络中都加了一些特定的前缀\",{\"1\":{\"183\":1}}],[\"tuning是在embedding环节\",{\"1\":{\"183\":1}}],[\"tuning是发生在embedding这个环节的\",{\"1\":{\"182\":1}}],[\"tuning就是在保证函数本身不变的前提下\",{\"1\":{\"182\":1}}],[\"tuning的是类似的\",{\"1\":{\"183\":1}}],[\"tuning的灵感来源是\",{\"1\":{\"183\":1}}],[\"tuning的具体细节\",{\"1\":{\"182\":1,\"183\":1}}],[\"tuning的基本原理是在输入序列x之前\",{\"1\":{\"182\":1}}],[\"tuning的出发点\",{\"1\":{\"182\":1,\"183\":1}}],[\"tuning\",{\"0\":{\"85\":2,\"177\":1,\"182\":1,\"183\":1,\"221\":1},\"1\":{\"78\":3,\"79\":1,\"85\":13,\"177\":1,\"180\":3,\"182\":1,\"183\":1,\"185\":1,\"188\":7,\"189\":2,\"203\":1,\"221\":1}}],[\"tuple\",{\"1\":{\"10\":1,\"107\":1,\"110\":2,\"113\":1,\"175\":2,\"176\":1,\"253\":1}}],[\"t5\",{\"1\":{\"78\":1,\"85\":1,\"255\":1}}],[\"tlr\",{\"1\":{\"37\":1}}],[\"take\",{\"1\":{\"244\":1,\"262\":1,\"271\":1}}],[\"taken\",{\"1\":{\"93\":2,\"95\":1,\"244\":1}}],[\"taking\",{\"1\":{\"240\":1}}],[\"task\",{\"0\":{\"220\":1},\"1\":{\"212\":1,\"228\":2,\"232\":1,\"233\":1}}],[\"tasks\",{\"1\":{\"76\":1}}],[\"tag\",{\"1\":{\"205\":1}}],[\"tage\",{\"1\":{\"100\":1}}],[\"tanh\",{\"1\":{\"114\":2,\"240\":1}}],[\"tabby\",{\"1\":{\"91\":1}}],[\"table\",{\"1\":{\"25\":1,\"39\":2}}],[\"tail\",{\"1\":{\"40\":1}}],[\"targets\",{\"1\":{\"38\":1,\"39\":11,\"101\":3,\"166\":7,\"167\":11,\"168\":7,\"169\":7,\"170\":7,\"172\":8}}],[\"target\",{\"1\":{\"35\":8,\"168\":1,\"169\":1,\"225\":2,\"262\":1}}],[\"t×d\",{\"1\":{\"29\":1}}],[\"t\",{\"1\":{\"27\":11,\"29\":1,\"33\":11,\"37\":1,\"39\":5,\"55\":1,\"60\":1,\"62\":2,\"64\":3,\"69\":2,\"90\":15,\"91\":2,\"93\":1,\"95\":1,\"107\":1,\"137\":1,\"169\":2,\"176\":2,\"227\":1,\"230\":1}}],[\"treebank\",{\"1\":{\"212\":1,\"223\":1}}],[\"tree\",{\"1\":{\"97\":1}}],[\"tripod\",{\"1\":{\"91\":1}}],[\"try\",{\"1\":{\"30\":1,\"40\":1,\"93\":2,\"94\":1,\"95\":3}}],[\"traversal\",{\"1\":{\"204\":1}}],[\"trange\",{\"1\":{\"175\":2}}],[\"translation\",{\"1\":{\"105\":1}}],[\"trans\",{\"1\":{\"65\":6,\"66\":11,\"67\":4,\"68\":4}}],[\"transforms\",{\"1\":{\"108\":10}}],[\"transform=data\",{\"1\":{\"108\":2}}],[\"transform=none\",{\"1\":{\"107\":1}}],[\"transform=feature\",{\"1\":{\"67\":1,\"68\":2}}],[\"transform=false\",{\"1\":{\"67\":1,\"68\":1}}],[\"transformation\",{\"1\":{\"65\":1}}],[\"transformations\",{\"1\":{\"61\":1}}],[\"transform\",{\"1\":{\"65\":3,\"66\":5,\"67\":3,\"68\":2,\"107\":5,\"108\":4,\"248\":3,\"249\":2}}],[\"transformer架构导入偏差是有帮助的\",{\"1\":{\"213\":1}}],[\"transformer比lstm能获取长距离信息\",{\"1\":{\"205\":1}}],[\"transformer跨各种各样任务的迁移性能更强\",{\"1\":{\"204\":1}}],[\"transformer证明了使用transformer结构可以有效处理图像数据\",{\"1\":{\"119\":1}}],[\"transformer需要输入的是一维的token\",{\"1\":{\"109\":1}}],[\"transformer的核心流程实现\",{\"1\":{\"106\":1}}],[\"transformer的模型结构相比于transformer来说更简单\",{\"1\":{\"105\":1}}],[\"transformer是2021年谷歌在iclr上提出的算法\",{\"1\":{\"105\":1}}],[\"transformers\",{\"1\":{\"95\":1}}],[\"transformerdecoderlayer\",{\"1\":{\"33\":1}}],[\"transformer\",{\"0\":{\"133\":1},\"1\":{\"27\":1,\"30\":2,\"33\":2,\"64\":1,\"69\":3,\"72\":1,\"88\":3,\"90\":3,\"93\":2,\"96\":1,\"99\":1,\"105\":1,\"110\":3,\"112\":1,\"117\":10,\"118\":2,\"122\":1,\"133\":1,\"189\":1,\"207\":2,\"208\":1,\"211\":3,\"217\":3,\"219\":1,\"221\":2,\"226\":3,\"244\":1,\"255\":1,\"259\":5,\"261\":4,\"277\":1,\"278\":3}}],[\"transpose\",{\"1\":{\"25\":1,\"27\":7,\"30\":2,\"33\":1,\"40\":1,\"57\":1,\"65\":1,\"66\":4,\"68\":1,\"103\":8,\"109\":1,\"113\":4,\"230\":1,\"244\":5,\"271\":3}}],[\"trashcan\",{\"1\":{\"25\":1}}],[\"trained\",{\"1\":{\"189\":1,\"232\":2}}],[\"training语言模型\",{\"1\":{\"203\":1}}],[\"training\",{\"1\":{\"38\":1,\"79\":1,\"89\":1,\"107\":1,\"118\":1,\"227\":1,\"278\":1}}],[\"train\",{\"1\":{\"22\":1,\"25\":2,\"37\":4,\"38\":3,\"40\":5,\"92\":1,\"107\":7,\"108\":7,\"114\":1,\"175\":1,\"185\":1,\"223\":8,\"227\":1,\"232\":3,\"233\":1,\"235\":6}}],[\"truncate\",{\"1\":{\"175\":2}}],[\"truncation=true\",{\"1\":{\"10\":1,\"100\":1}}],[\"truncation\",{\"1\":{\"10\":2}}],[\"trunc\",{\"1\":{\"110\":1,\"111\":2,\"114\":2}}],[\"truth\",{\"0\":{\"21\":1},\"1\":{\"35\":2,\"39\":2,\"166\":2,\"167\":1,\"168\":2,\"169\":1,\"170\":1}}],[\"true\",{\"1\":{\"10\":2,\"33\":1,\"39\":5,\"53\":1,\"66\":1,\"107\":1,\"118\":1,\"168\":1,\"170\":3,\"172\":1,\"176\":1}}],[\"through\",{\"1\":{\"267\":1}}],[\"thre\",{\"1\":{\"39\":2}}],[\"thres\",{\"1\":{\"39\":2}}],[\"threshold\",{\"1\":{\"39\":9,\"166\":1}}],[\"thresholding\",{\"1\":{\"39\":2}}],[\"thought\",{\"0\":{\"198\":1},\"1\":{\"198\":2,\"199\":1,\"280\":1}}],[\"thinking\",{\"1\":{\"278\":2}}],[\"think\",{\"1\":{\"197\":1}}],[\"this\",{\"1\":{\"13\":7,\"27\":1,\"40\":3,\"91\":1,\"244\":1}}],[\"that\",{\"1\":{\"20\":1}}],[\"these\",{\"1\":{\"254\":1}}],[\"there\",{\"1\":{\"249\":1}}],[\"they\",{\"1\":{\"242\":1}}],[\"theorem\",{\"1\":{\"69\":1}}],[\"thecvf\",{\"1\":{\"17\":1}}],[\"theta\",{\"1\":{\"15\":4}}],[\"the\",{\"1\":{\"10\":1,\"20\":2,\"35\":1,\"39\":2,\"40\":3,\"91\":1,\"94\":1,\"95\":1,\"107\":1,\"182\":1,\"212\":1,\"223\":1,\"233\":4,\"240\":3,\"244\":5,\"249\":3,\"254\":1,\"255\":2,\"256\":1,\"259\":1,\"267\":1,\"271\":2}}],[\"tool\",{\"1\":{\"291\":1}}],[\"totensor\",{\"1\":{\"108\":2}}],[\"total\",{\"1\":{\"13\":2,\"35\":1,\"38\":1,\"39\":4,\"93\":4,\"95\":4,\"113\":3,\"168\":2,\"227\":3,\"228\":2,\"251\":2,\"254\":2}}],[\"topk\",{\"1\":{\"91\":1}}],[\"top\",{\"1\":{\"20\":1,\"91\":2,\"104\":3}}],[\"torch==1\",{\"1\":{\"259\":1}}],[\"torchscript\",{\"1\":{\"113\":1}}],[\"torch\",{\"1\":{\"10\":1,\"11\":4,\"13\":6,\"15\":5,\"16\":3,\"27\":3,\"33\":5,\"35\":21,\"37\":3,\"38\":2,\"39\":1,\"40\":4,\"49\":12,\"53\":3,\"57\":3,\"64\":6,\"65\":4,\"66\":7,\"68\":4,\"91\":3,\"93\":4,\"95\":5,\"100\":1,\"101\":3,\"102\":17,\"103\":6,\"104\":2,\"107\":5,\"108\":2,\"110\":2,\"111\":3,\"114\":5,\"118\":1,\"168\":1,\"169\":6,\"172\":4,\"226\":2,\"227\":4,\"230\":2,\"235\":1,\"236\":2,\"244\":2,\"249\":1,\"253\":4,\"259\":1,\"271\":2}}],[\"to\",{\"0\":{\"200\":1},\"1\":{\"10\":1,\"28\":1,\"35\":1,\"40\":3,\"49\":7,\"79\":1,\"90\":2,\"93\":6,\"95\":5,\"98\":1,\"100\":2,\"102\":3,\"103\":1,\"104\":2,\"109\":1,\"114\":3,\"118\":1,\"175\":10,\"176\":3,\"185\":2,\"200\":2,\"227\":2,\"232\":3,\"233\":1,\"240\":1,\"241\":1,\"244\":4,\"260\":1,\"271\":1}}],[\"token用于分类任务即可\",{\"1\":{\"242\":1}}],[\"token分类任务\",{\"0\":{\"256\":1}}],[\"token分类\",{\"1\":{\"221\":2}}],[\"token提取出来\",{\"1\":{\"114\":1}}],[\"token的作用\",{\"1\":{\"110\":1}}],[\"token的相似度\",{\"1\":{\"101\":1}}],[\"token对齐\",{\"1\":{\"110\":1}}],[\"token开头\",{\"1\":{\"104\":1}}],[\"token作为最后的相似度得分\",{\"1\":{\"101\":2}}],[\"token作为input\",{\"1\":{\"100\":1}}],[\"token是有效的\",{\"1\":{\"100\":1}}],[\"tokens=false\",{\"1\":{\"233\":1}}],[\"tokens=true\",{\"1\":{\"104\":1,\"255\":2}}],[\"tokens可知这些被掩码token对应的真实词作为label\",{\"1\":{\"227\":1}}],[\"tokens形状\",{\"1\":{\"104\":1}}],[\"tokens做attention\",{\"1\":{\"103\":1}}],[\"tokens的embeddings在seq\",{\"1\":{\"102\":1}}],[\"tokens同时输入bertmodel时\",{\"1\":{\"102\":1}}],[\"tokens部分的mask列表\",{\"1\":{\"233\":1}}],[\"tokens部分的每个位置都映射到2维匹配空间\",{\"1\":{\"102\":1}}],[\"tokens部分的结果\",{\"1\":{\"102\":1}}],[\"tokens拼接得到的结果和图像嵌入进行cross\",{\"1\":{\"102\":1}}],[\"tokens列表中所有句子掩码数量一致\",{\"1\":{\"227\":1}}],[\"tokens列表\",{\"1\":{\"102\":1}}],[\"tokens\",{\"1\":{\"10\":3,\"32\":2,\"91\":2,\"100\":9,\"101\":3,\"102\":16,\"103\":16,\"104\":6,\"110\":1,\"111\":2,\"114\":2,\"175\":4,\"176\":6,\"211\":1,\"224\":17,\"225\":13,\"226\":4,\"227\":2,\"230\":4,\"233\":6,\"244\":1,\"253\":2,\"255\":2,\"278\":1}}],[\"token\",{\"0\":{\"110\":1},\"1\":{\"10\":2,\"13\":5,\"16\":3,\"29\":2,\"30\":4,\"33\":10,\"80\":1,\"100\":1,\"101\":1,\"103\":16,\"104\":5,\"110\":23,\"111\":5,\"114\":5,\"175\":10,\"176\":4,\"207\":1,\"209\":1,\"213\":1,\"218\":3,\"219\":4,\"224\":4,\"225\":2,\"226\":4,\"233\":35,\"235\":4,\"236\":9,\"240\":4,\"241\":2,\"242\":2,\"249\":1,\"251\":3,\"253\":9,\"254\":2,\"255\":8,\"256\":4,\"257\":6,\"278\":3}}],[\"tokenize方法进行分词\",{\"1\":{\"176\":1}}],[\"tokenize方法完成分词功能\",{\"1\":{\"175\":1}}],[\"tokenize方法完成断句功能\",{\"1\":{\"175\":1}}],[\"tokenize\",{\"1\":{\"91\":1,\"175\":2,\"176\":2,\"211\":1,\"223\":1,\"224\":4}}],[\"tokenizer\",{\"1\":{\"10\":4,\"100\":1,\"103\":2,\"104\":4,\"176\":7,\"224\":1,\"225\":17,\"227\":6,\"255\":4,\"278\":1}}],[\"tokenization\",{\"1\":{\"10\":1,\"253\":1}}],[\"txt文件\",{\"1\":{\"232\":1}}],[\"txt\",{\"1\":{\"10\":1,\"100\":1,\"147\":2,\"259\":2}}],[\"terms\",{\"1\":{\"254\":1}}],[\"term\",{\"1\":{\"223\":1}}],[\"temperature\",{\"1\":{\"90\":1}}],[\"temp3\",{\"1\":{\"35\":2}}],[\"temp\",{\"1\":{\"35\":2,\"38\":2,\"39\":5,\"101\":2,\"224\":2}}],[\"temp2\",{\"1\":{\"35\":3}}],[\"temp1\",{\"1\":{\"35\":3}}],[\"testenv\",{\"1\":{\"143\":2}}],[\"test\",{\"1\":{\"22\":1,\"37\":4,\"39\":1,\"93\":2,\"95\":1,\"203\":1,\"204\":1,\"223\":8,\"277\":1}}],[\"tensordataset\",{\"1\":{\"233\":1}}],[\"tensorflow\",{\"1\":{\"42\":1,\"59\":1}}],[\"tensor\",{\"1\":{\"13\":1,\"33\":6,\"91\":1,\"107\":1,\"108\":2,\"113\":1,\"166\":4,\"167\":4,\"168\":4,\"169\":4,\"227\":2,\"238\":2,\"240\":2,\"245\":2,\"246\":3,\"254\":1}}],[\"tensors=\",{\"1\":{\"10\":1,\"93\":2,\"95\":2,\"100\":1}}],[\"textual\",{\"1\":{\"212\":1}}],[\"text和text\",{\"1\":{\"101\":1}}],[\"text最相关的视觉信息\",{\"1\":{\"100\":1}}],[\"text转化为18291个类别\",{\"1\":{\"96\":1}}],[\"text=texts\",{\"1\":{\"93\":1,\"95\":1}}],[\"texts\",{\"1\":{\"90\":1,\"91\":2,\"93\":1,\"95\":1}}],[\"text\",{\"0\":{\"101\":1,\"102\":1,\"103\":1},\"1\":{\"10\":3,\"13\":1,\"27\":4,\"28\":1,\"33\":2,\"37\":2,\"40\":7,\"90\":6,\"91\":13,\"93\":9,\"94\":9,\"95\":16,\"98\":2,\"100\":17,\"101\":11,\"102\":32,\"103\":5,\"104\":1,\"107\":1,\"175\":2,\"224\":8,\"233\":2}}],[\"echo\",{\"1\":{\"282\":1}}],[\"e=3\",{\"1\":{\"221\":2}}],[\"efficient\",{\"1\":{\"180\":1,\"182\":1,\"185\":2}}],[\"eol\",{\"1\":{\"176\":2}}],[\"eos\",{\"1\":{\"104\":1}}],[\"ee\",{\"1\":{\"175\":1}}],[\"escape\",{\"1\":{\"175\":1}}],[\"e^\",{\"1\":{\"169\":1}}],[\"e5005f0a\",{\"1\":{\"118\":1}}],[\"equals\",{\"1\":{\"271\":1}}],[\"equivariance\",{\"1\":{\"105\":1}}],[\"eq\",{\"1\":{\"114\":1,\"230\":1}}],[\"eye\",{\"1\":{\"65\":1}}],[\"error\",{\"1\":{\"39\":4,\"93\":2,\"94\":1,\"95\":3}}],[\"eta\",{\"1\":{\"37\":1}}],[\"eps=config\",{\"1\":{\"236\":1,\"238\":1,\"245\":1,\"248\":1}}],[\"eps=1e\",{\"1\":{\"37\":1,\"114\":1,\"172\":1}}],[\"eps\",{\"1\":{\"172\":5,\"236\":1,\"238\":1,\"245\":1,\"248\":1}}],[\"epsilon\",{\"1\":{\"114\":1}}],[\"epochs=4\",{\"1\":{\"232\":1}}],[\"epochs\",{\"1\":{\"227\":2}}],[\"epoch+1\",{\"1\":{\"38\":1}}],[\"epoch\",{\"1\":{\"37\":1,\"38\":2,\"39\":2,\"40\":1,\"114\":2,\"227\":4}}],[\"extend\",{\"1\":{\"223\":1}}],[\"extended\",{\"1\":{\"103\":1,\"241\":6}}],[\"extension\",{\"1\":{\"93\":2,\"95\":2}}],[\"exist\",{\"1\":{\"94\":1,\"95\":1,\"107\":1,\"223\":2}}],[\"exists\",{\"1\":{\"93\":1,\"94\":1,\"95\":2,\"107\":1,\"224\":1,\"227\":1}}],[\"exception\",{\"1\":{\"93\":2,\"94\":1,\"95\":3}}],[\"except\",{\"1\":{\"93\":2,\"94\":1,\"95\":3}}],[\"export\",{\"1\":{\"232\":1}}],[\"exp\",{\"1\":{\"90\":1,\"169\":6}}],[\"expansion\",{\"1\":{\"30\":4}}],[\"expand\",{\"1\":{\"16\":1,\"100\":1,\"102\":1,\"104\":1,\"110\":1,\"111\":1,\"114\":1,\"226\":1,\"230\":1,\"236\":1}}],[\"examples\",{\"1\":{\"35\":2,\"169\":2}}],[\"each\",{\"1\":{\"249\":1,\"267\":1}}],[\"easy\",{\"1\":{\"35\":1,\"169\":1}}],[\"earphone\",{\"1\":{\"25\":1}}],[\"einstein\",{\"1\":{\"33\":1}}],[\"einsum\",{\"1\":{\"27\":1,\"33\":5}}],[\"employ\",{\"1\":{\"35\":1}}],[\"empty\",{\"1\":{\"25\":1}}],[\"emb\",{\"1\":{\"11\":14,\"12\":4,\"14\":2,\"15\":20,\"16\":14,\"27\":3,\"37\":2,\"40\":2,\"94\":2,\"95\":2,\"226\":1}}],[\"embed\",{\"1\":{\"30\":7,\"33\":3,\"90\":2,\"109\":9,\"110\":13,\"111\":15,\"113\":9,\"114\":17,\"118\":1,\"262\":8}}],[\"embedding相同维度\",{\"1\":{\"104\":1}}],[\"embedding计算出key和value\",{\"1\":{\"103\":1}}],[\"embedding送到二分类器中\",{\"1\":{\"102\":1}}],[\"embeddings前面\",{\"1\":{\"104\":1}}],[\"embeddings和query\",{\"1\":{\"102\":1}}],[\"embeddings\",{\"1\":{\"10\":1,\"13\":1,\"33\":2,\"93\":12,\"94\":4,\"95\":15,\"100\":3,\"102\":17,\"226\":1,\"236\":23,\"240\":1,\"241\":2,\"249\":1,\"251\":1}}],[\"embedding\",{\"1\":{\"10\":1,\"13\":1,\"15\":1,\"16\":1,\"33\":1,\"93\":4,\"94\":1,\"95\":3,\"102\":1,\"103\":1,\"109\":1,\"111\":1,\"219\":5,\"226\":3,\"236\":3,\"241\":2,\"278\":1,\"287\":1}}],[\"embeds则拼接\",{\"1\":{\"102\":1}}],[\"embeds=none\",{\"1\":{\"102\":1,\"103\":1}}],[\"embeds=query\",{\"1\":{\"100\":1,\"102\":1,\"103\":2,\"104\":1}}],[\"embeds=llm\",{\"1\":{\"10\":1}}],[\"embeds\",{\"1\":{\"10\":4,\"13\":10,\"100\":4,\"102\":16,\"103\":3,\"104\":6}}],[\"every\",{\"1\":{\"107\":5}}],[\"evaluate\",{\"1\":{\"39\":1,\"227\":1}}],[\"evaluating\",{\"1\":{\"39\":1}}],[\"eval\",{\"1\":{\"39\":1,\"40\":1,\"232\":2}}],[\"ev\",{\"1\":{\"15\":2}}],[\"ek\",{\"1\":{\"15\":2}}],[\"enginnering\",{\"1\":{\"290\":1}}],[\"engineering技巧的时候\",{\"1\":{\"198\":1}}],[\"engineering呢\",{\"1\":{\"194\":1}}],[\"engineering的技巧\",{\"1\":{\"198\":1,\"199\":1}}],[\"engineering的原因\",{\"1\":{\"194\":1}}],[\"engineering的实践表明\",{\"1\":{\"183\":1}}],[\"engineering的效果达不到要求\",{\"1\":{\"179\":1}}],[\"engineering的方式\",{\"1\":{\"179\":1}}],[\"engineering的方式是一种相对来说容易上手的使用大模型的方式\",{\"1\":{\"179\":1}}],[\"engineering\",{\"0\":{\"193\":1,\"194\":1},\"1\":{\"92\":1,\"179\":1,\"193\":1,\"194\":1,\"197\":1,\"290\":3,\"291\":4}}],[\"entire\",{\"1\":{\"244\":1}}],[\"entropy\",{\"1\":{\"35\":1,\"90\":3,\"101\":2,\"102\":1,\"167\":5,\"169\":4,\"172\":2}}],[\"enc\",{\"1\":{\"226\":2}}],[\"encoded\",{\"1\":{\"233\":7}}],[\"encode\",{\"1\":{\"91\":2,\"224\":3,\"262\":2}}],[\"encoder模型结构图\",{\"1\":{\"267\":1}}],[\"encoderdecoder\",{\"1\":{\"262\":2}}],[\"encoderdecoder模型结构图\",{\"1\":{\"262\":1}}],[\"encoders\",{\"1\":{\"226\":2}}],[\"encoder中mlp\",{\"1\":{\"115\":1}}],[\"encoder中重复堆叠encoder\",{\"1\":{\"115\":1}}],[\"encoder输出结果之后\",{\"1\":{\"114\":1}}],[\"encoder输出的embeddings里提取与input\",{\"1\":{\"100\":1}}],[\"encoder的结构\",{\"1\":{\"112\":1}}],[\"encoderlayer模型结构图\",{\"1\":{\"266\":1}}],[\"encoderlayer\",{\"0\":{\"266\":1},\"1\":{\"103\":1,\"226\":1,\"266\":2}}],[\"encoder提取的图像embeddings\",{\"1\":{\"100\":1}}],[\"encoder提取图像特征\",{\"1\":{\"91\":1}}],[\"encoder引到vision\",{\"1\":{\"99\":1}}],[\"encoder\",{\"0\":{\"112\":1,\"262\":1,\"264\":1,\"267\":1},\"1\":{\"10\":2,\"16\":3,\"27\":1,\"37\":2,\"56\":1,\"90\":8,\"91\":4,\"98\":2,\"100\":4,\"102\":2,\"103\":20,\"104\":5,\"110\":1,\"217\":3,\"241\":2,\"255\":3,\"261\":2,\"262\":4,\"267\":3}}],[\"encoding=\",{\"1\":{\"175\":1,\"176\":1,\"223\":3,\"224\":1}}],[\"encoding\",{\"1\":{\"27\":3,\"174\":1,\"261\":1}}],[\"enables\",{\"1\":{\"200\":1}}],[\"environments\",{\"1\":{\"143\":1}}],[\"envs\",{\"1\":{\"143\":3}}],[\"env\",{\"1\":{\"143\":1,\"144\":1}}],[\"ensemble\",{\"0\":{\"82\":1},\"1\":{\"82\":1}}],[\"ensure\",{\"1\":{\"20\":1,\"223\":2}}],[\"end\",{\"1\":{\"79\":2,\"93\":4,\"95\":4,\"253\":12,\"254\":14,\"255\":5}}],[\"enumerate\",{\"1\":{\"25\":2,\"38\":1,\"39\":1,\"40\":1,\"49\":1,\"53\":1,\"57\":1,\"93\":1,\"95\":1,\"107\":2,\"114\":1,\"224\":2,\"225\":1,\"227\":1,\"239\":1}}],[\"enrichment\",{\"1\":{\"20\":1}}],[\"enhance\",{\"1\":{\"11\":6}}],[\"ele\",{\"1\":{\"227\":4}}],[\"elmo\",{\"1\":{\"211\":1,\"212\":1}}],[\"elicits\",{\"1\":{\"198\":1}}],[\"elif\",{\"1\":{\"103\":1,\"224\":1}}],[\"else\",{\"1\":{\"10\":2,\"13\":1,\"25\":3,\"29\":2,\"31\":2,\"39\":1,\"40\":1,\"49\":4,\"50\":2,\"53\":3,\"57\":2,\"66\":2,\"93\":1,\"94\":1,\"95\":2,\"102\":2,\"103\":4,\"104\":1,\"107\":1,\"109\":1,\"112\":1,\"114\":1,\"118\":2,\"176\":3,\"224\":4,\"233\":2,\"235\":1,\"238\":1,\"242\":1,\"248\":1,\"256\":1,\"257\":3}}],[\"elowen\",{\"0\":{\"3\":1}}],[\"e\",{\"1\":{\"10\":1,\"15\":7,\"88\":3,\"90\":6,\"93\":4,\"94\":2,\"95\":6,\"175\":5,\"221\":2}}],[\"nbatches\",{\"1\":{\"271\":3}}],[\"n个解码器层\",{\"1\":{\"270\":1}}],[\"n维\",{\"1\":{\"184\":2}}],[\"ntoken\",{\"1\":{\"175\":2}}],[\"n为序列长度\",{\"1\":{\"113\":1}}],[\"null\",{\"1\":{\"233\":1}}],[\"nucleus\",{\"1\":{\"104\":3}}],[\"number\",{\"1\":{\"49\":2,\"107\":1,\"271\":1}}],[\"numpy\",{\"1\":{\"40\":4,\"64\":1,\"91\":2,\"93\":4,\"95\":3,\"108\":2,\"140\":1}}],[\"nums\",{\"1\":{\"39\":2}}],[\"num\",{\"1\":{\"10\":1,\"11\":6,\"30\":3,\"37\":7,\"38\":1,\"39\":11,\"40\":4,\"50\":3,\"53\":2,\"55\":1,\"58\":8,\"93\":2,\"95\":2,\"103\":1,\"104\":6,\"107\":5,\"109\":5,\"110\":9,\"111\":11,\"112\":2,\"113\":24,\"114\":18,\"118\":4,\"232\":1,\"239\":1,\"242\":5,\"244\":5,\"254\":3,\"256\":6,\"257\":9}}],[\"nz\",{\"1\":{\"50\":1}}],[\"ny\",{\"1\":{\"50\":1}}],[\"nx\",{\"1\":{\"50\":1}}],[\"neural\",{\"1\":{\"260\":1,\"261\":1,\"277\":1}}],[\"ner\",{\"1\":{\"205\":1,\"256\":1}}],[\"neighborhood\",{\"1\":{\"105\":1}}],[\"next取出索引0或者1下标对应的值即可知道我们是否预测正确\",{\"1\":{\"227\":1}}],[\"next=false\",{\"1\":{\"225\":1}}],[\"next=true\",{\"1\":{\"225\":1}}],[\"next\",{\"0\":{\"219\":1},\"1\":{\"103\":1,\"217\":1,\"225\":2,\"226\":2,\"227\":2,\"233\":1,\"251\":5}}],[\"neg\",{\"1\":{\"102\":20}}],[\"negatives\",{\"1\":{\"101\":2,\"170\":1}}],[\"negative\",{\"1\":{\"35\":6,\"170\":2}}],[\"needed\",{\"1\":{\"93\":2,\"95\":2}}],[\"networks\",{\"1\":{\"112\":1,\"211\":1}}],[\"network\",{\"1\":{\"64\":1,\"261\":1}}],[\"net\",{\"1\":{\"55\":1,\"60\":1,\"62\":2,\"64\":3,\"69\":2}}],[\"netpoll\",{\"1\":{\"2\":1}}],[\"new\",{\"1\":{\"49\":46,\"53\":16,\"57\":9,\"103\":2,\"244\":4}}],[\"non\",{\"1\":{\"225\":7}}],[\"none\",{\"1\":{\"10\":1,\"13\":1,\"32\":1,\"33\":6,\"40\":2,\"49\":3,\"50\":1,\"53\":6,\"57\":2,\"58\":1,\"65\":2,\"66\":1,\"93\":3,\"94\":2,\"95\":4,\"102\":4,\"103\":10,\"107\":2,\"109\":1,\"118\":1,\"175\":1,\"176\":2,\"224\":3,\"233\":4,\"236\":2,\"242\":2,\"244\":1,\"251\":2,\"254\":2,\"256\":2,\"257\":7,\"271\":4}}],[\"no\",{\"1\":{\"91\":2,\"93\":2,\"94\":1,\"95\":3,\"102\":1}}],[\"noun\",{\"1\":{\"80\":1}}],[\"noisy\",{\"1\":{\"61\":1}}],[\"nohup\",{\"1\":{\"40\":2}}],[\"norm=1\",{\"1\":{\"227\":1}}],[\"norm层之后同样是多头注意力层\",{\"1\":{\"112\":1}}],[\"normalization\",{\"1\":{\"112\":1,\"261\":1,\"278\":1}}],[\"normalize\",{\"1\":{\"25\":1,\"40\":2,\"90\":2,\"100\":2,\"108\":2,\"244\":1}}],[\"normal\",{\"1\":{\"50\":6,\"53\":5,\"71\":1,\"110\":1,\"111\":2,\"114\":2}}],[\"norm3\",{\"1\":{\"33\":1}}],[\"norm2\",{\"1\":{\"30\":2,\"31\":1,\"33\":1,\"112\":2}}],[\"norm1\",{\"1\":{\"30\":2,\"33\":1,\"112\":2}}],[\"norm\",{\"1\":{\"29\":3,\"31\":3,\"49\":3,\"50\":3,\"53\":3,\"57\":2,\"65\":1,\"91\":2,\"93\":2,\"95\":2,\"109\":6,\"111\":1,\"112\":3,\"114\":7,\"227\":1,\"236\":1,\"238\":1,\"245\":1,\"248\":1,\"265\":2,\"267\":2,\"270\":2}}],[\"notimplementederror\",{\"1\":{\"25\":1}}],[\"not\",{\"1\":{\"25\":2,\"32\":1,\"37\":1,\"49\":3,\"53\":2,\"57\":1,\"93\":3,\"94\":2,\"95\":5,\"102\":3,\"103\":6,\"104\":1,\"107\":2,\"118\":2,\"176\":1,\"223\":2,\"224\":1,\"225\":1,\"233\":1,\"242\":1,\"244\":1,\"251\":2,\"254\":2,\"256\":2,\"257\":4,\"271\":3,\"295\":1}}],[\"notes\",{\"1\":{\"16\":1}}],[\"nn\",{\"1\":{\"11\":8,\"12\":4,\"14\":4,\"15\":12,\"16\":8,\"27\":1,\"29\":1,\"30\":15,\"31\":1,\"32\":1,\"33\":2,\"35\":4,\"37\":1,\"49\":5,\"50\":8,\"53\":15,\"55\":6,\"57\":5,\"58\":5,\"64\":13,\"66\":7,\"67\":8,\"68\":8,\"102\":1,\"103\":4,\"109\":5,\"110\":4,\"111\":6,\"112\":6,\"113\":5,\"114\":14,\"166\":1,\"167\":1,\"168\":1,\"169\":1,\"170\":1,\"172\":1,\"219\":1,\"226\":5,\"227\":3,\"230\":2,\"236\":5,\"238\":6,\"239\":2,\"240\":3,\"242\":2,\"244\":6,\"245\":3,\"246\":1,\"248\":2,\"249\":3,\"250\":2,\"253\":1,\"254\":1,\"256\":2,\"257\":2,\"262\":1,\"263\":2,\"265\":2,\"266\":1,\"267\":1,\"269\":1,\"270\":1,\"271\":3}}],[\"nagative\",{\"1\":{\"213\":1}}],[\"natural\",{\"1\":{\"92\":1,\"185\":1}}],[\"nanmean\",{\"1\":{\"39\":2}}],[\"nan\",{\"1\":{\"39\":3,\"172\":1}}],[\"name=\",{\"1\":{\"40\":2,\"232\":1}}],[\"named\",{\"1\":{\"37\":2,\"118\":1}}],[\"name\",{\"1\":{\"25\":2,\"40\":1,\"93\":8,\"95\":8,\"118\":4,\"140\":5,\"223\":1,\"224\":2,\"232\":1}}],[\"na\",{\"1\":{\"10\":1}}],[\"nli\",{\"1\":{\"212\":1,\"221\":1}}],[\"nlp半监督学习\",{\"1\":{\"205\":1}}],[\"nlp\",{\"1\":{\"78\":1,\"90\":1,\"217\":3,\"223\":3,\"278\":1}}],[\"nl=token数\",{\"1\":{\"10\":1}}],[\"nl\",{\"1\":{\"10\":4}}],[\"nsp任务\",{\"1\":{\"233\":1}}],[\"nsp\",{\"1\":{\"226\":6,\"227\":2,\"228\":1,\"233\":1}}],[\"nsample=none\",{\"1\":{\"50\":1}}],[\"nsample=64\",{\"1\":{\"50\":1}}],[\"nsample=32\",{\"1\":{\"50\":1,\"58\":4}}],[\"nsample\",{\"1\":{\"49\":23,\"53\":4}}],[\"ns\",{\"1\":{\"10\":5,\"62\":1}}],[\"npoint=64\",{\"1\":{\"58\":1}}],[\"npoint=256\",{\"1\":{\"58\":1}}],[\"npoint=16\",{\"1\":{\"58\":1}}],[\"npoint=1024\",{\"1\":{\"58\":1}}],[\"npoint=128\",{\"1\":{\"50\":1}}],[\"npoint=none\",{\"1\":{\"50\":1}}],[\"npoint=512\",{\"1\":{\"50\":1}}],[\"npoint\",{\"1\":{\"16\":2,\"27\":2,\"49\":35,\"53\":4}}],[\"np\",{\"1\":{\"10\":1,\"25\":5,\"39\":15,\"40\":15,\"64\":2,\"90\":5,\"91\":1,\"93\":6,\"94\":1,\"95\":8}}],[\"n\",{\"1\":{\"10\":1,\"11\":10,\"13\":6,\"15\":29,\"16\":21,\"21\":2,\"27\":4,\"29\":1,\"31\":1,\"32\":1,\"33\":12,\"35\":2,\"37\":8,\"39\":3,\"40\":14,\"49\":30,\"53\":4,\"55\":4,\"57\":29,\"58\":7,\"64\":1,\"66\":5,\"68\":9,\"69\":3,\"90\":7,\"96\":1,\"113\":3,\"128\":1,\"137\":4,\"140\":1,\"144\":1,\"166\":4,\"167\":4,\"168\":4,\"169\":4,\"170\":2,\"172\":1,\"188\":1,\"225\":3,\"226\":3,\"227\":1,\"230\":2,\"232\":1,\"259\":1,\"267\":3,\"270\":3}}],[\">=\",{\"1\":{\"39\":2,\"223\":1}}],[\">bln\",{\"1\":{\"27\":1,\"33\":3}}],[\">2048\",{\"1\":{\"27\":1}}],[\">256\",{\"1\":{\"27\":1}}],[\">1024\",{\"1\":{\"27\":1}}],[\">512\",{\"1\":{\"27\":1}}],[\">\",{\"1\":{\"10\":2,\"15\":3,\"16\":2,\"39\":1,\"40\":2,\"49\":1,\"57\":1,\"68\":1,\"96\":2,\"103\":1,\"104\":1,\"110\":2,\"111\":2,\"112\":1,\"113\":8,\"114\":3,\"172\":1,\"175\":9,\"176\":7,\"224\":3,\"225\":2,\"233\":3,\"235\":1,\"254\":1,\"271\":1}}],[\"wget\",{\"1\":{\"232\":1}}],[\"w+∆w\",{\"1\":{\"184\":1}}],[\"wx\",{\"1\":{\"178\":1}}],[\"w>\",{\"1\":{\"175\":3,\"176\":2}}],[\"w`代表输出特征图的宽和高\",{\"1\":{\"109\":1}}],[\"write\",{\"1\":{\"107\":1}}],[\"wrap\",{\"1\":{\"25\":1}}],[\"warnings\",{\"1\":{\"95\":2}}],[\"walk\",{\"1\":{\"93\":1,\"95\":1}}],[\"want\",{\"1\":{\"40\":3}}],[\"while\",{\"1\":{\"176\":1}}],[\"white\",{\"1\":{\"91\":1}}],[\"which\",{\"1\":{\"40\":3,\"147\":1,\"244\":1}}],[\"where\",{\"1\":{\"27\":1,\"28\":1,\"147\":1,\"169\":1}}],[\"wow\",{\"1\":{\"175\":1}}],[\"wordpiecetokenizer\",{\"1\":{\"224\":1}}],[\"wordpunct\",{\"1\":{\"175\":1,\"176\":1}}],[\"word2idx\",{\"1\":{\"224\":14,\"225\":9,\"230\":1}}],[\"word\",{\"1\":{\"102\":1,\"175\":10,\"211\":1,\"224\":17,\"225\":2,\"226\":4,\"236\":2}}],[\"words\",{\"1\":{\"96\":3,\"236\":2}}],[\"world\",{\"1\":{\"102\":6,\"175\":1}}],[\"workers=8\",{\"1\":{\"37\":3}}],[\"would\",{\"1\":{\"27\":1}}],[\"were\",{\"1\":{\"107\":1}}],[\"web\",{\"1\":{\"96\":1}}],[\"weakly\",{\"1\":{\"96\":1}}],[\"wear\",{\"1\":{\"25\":1}}],[\"weighted\",{\"1\":{\"172\":5}}],[\"weight=none\",{\"1\":{\"166\":1,\"167\":1,\"168\":1,\"169\":1,\"170\":1,\"172\":1}}],[\"weights\",{\"1\":{\"93\":2,\"95\":2,\"102\":4,\"114\":1,\"118\":5,\"226\":1,\"227\":1,\"241\":1,\"242\":1,\"249\":1}}],[\"weight\",{\"1\":{\"37\":1,\"57\":4,\"166\":3,\"167\":2,\"168\":2,\"169\":7,\"170\":4,\"172\":4,\"226\":9,\"227\":1}}],[\"we\",{\"1\":{\"35\":1,\"240\":1,\"242\":1,\"254\":1,\"271\":1}}],[\"w₂\",{\"1\":{\"31\":1}}],[\"wikipedia\",{\"1\":{\"278\":1}}],[\"wikitext2是wikitext\",{\"1\":{\"223\":1}}],[\"wikitext\",{\"1\":{\"223\":15,\"227\":1}}],[\"windows\",{\"1\":{\"147\":1}}],[\"window\",{\"1\":{\"40\":4}}],[\"wise\",{\"1\":{\"16\":1,\"27\":2,\"211\":1}}],[\"width=600\",{\"1\":{\"40\":1}}],[\"width\",{\"1\":{\"10\":1,\"107\":1}}],[\"with\",{\"0\":{\"82\":1},\"1\":{\"7\":1,\"10\":1,\"25\":2,\"33\":3,\"40\":4,\"80\":1,\"91\":3,\"93\":2,\"95\":2,\"102\":1,\"107\":1,\"180\":2,\"223\":3,\"224\":3,\"227\":1,\"233\":3,\"270\":1}}],[\"w\",{\"1\":{\"10\":4,\"11\":2,\"90\":5,\"100\":1,\"107\":1,\"109\":4,\"110\":1,\"111\":1,\"114\":1,\"170\":2,\"172\":1,\"175\":2,\"184\":1,\"223\":2,\"224\":1,\"271\":4}}],[\"37b\",{\"1\":{\"278\":1}}],[\"31\",{\"1\":{\"278\":1}}],[\"3167\",{\"1\":{\"228\":1}}],[\"3508\",{\"1\":{\"228\":1}}],[\"3的准确率\",{\"1\":{\"212\":1}}],[\"3的维度\",{\"1\":{\"113\":1}}],[\"30b\",{\"1\":{\"278\":1}}],[\"3072\",{\"1\":{\"211\":1,\"238\":2}}],[\"300m数据集\",{\"1\":{\"115\":1}}],[\"300m数据集的规模达到了上亿级别\",{\"1\":{\"96\":1}}],[\"300m数据集取得了较好的结果\",{\"1\":{\"96\":1}}],[\"300m数据集是谷歌从互联网上收集的\",{\"1\":{\"96\":1}}],[\"300m数据集来预训练模型在imagenet上取得sota\",{\"1\":{\"96\":1}}],[\"300m数据集还要多出1亿对\",{\"1\":{\"90\":1}}],[\"300\",{\"1\":{\"80\":1}}],[\"396\",{\"1\":{\"93\":1}}],[\"336\",{\"1\":{\"90\":1}}],[\"384\",{\"1\":{\"80\":1}}],[\"3m\",{\"1\":{\"80\":1}}],[\"3x3\",{\"1\":{\"64\":2}}],[\"3×3\",{\"1\":{\"62\":1,\"64\":1,\"65\":1}}],[\"32b\",{\"1\":{\"278\":6,\"292\":1}}],[\"32k\",{\"1\":{\"278\":2}}],[\"32768\",{\"1\":{\"90\":1}}],[\"320\",{\"1\":{\"53\":1}}],[\"32\",{\"1\":{\"53\":4,\"56\":2,\"58\":2,\"90\":1,\"192\":1,\"211\":1}}],[\"3+d\",{\"1\":{\"49\":3}}],[\"3️⃣\",{\"0\":{\"31\":1},\"1\":{\"57\":1}}],[\"3\",{\"0\":{\"12\":1,\"21\":1,\"103\":1,\"110\":1,\"127\":1,\"132\":1},\"1\":{\"10\":3,\"16\":2,\"25\":2,\"27\":9,\"32\":1,\"33\":8,\"35\":1,\"37\":1,\"39\":5,\"40\":12,\"49\":14,\"50\":5,\"53\":6,\"55\":2,\"56\":2,\"57\":16,\"58\":1,\"62\":2,\"64\":9,\"66\":1,\"67\":1,\"78\":4,\"80\":1,\"91\":1,\"92\":1,\"95\":1,\"96\":1,\"102\":10,\"103\":2,\"104\":1,\"107\":1,\"109\":3,\"113\":7,\"161\":1,\"169\":1,\"170\":2,\"175\":6,\"176\":1,\"189\":2,\"211\":1,\"226\":2,\"236\":1,\"244\":2,\"269\":1,\"271\":1,\"277\":5,\"278\":24,\"280\":1,\"288\":1}}],[\"3d`\",{\"1\":{\"39\":1}}],[\"3daffordance\",{\"1\":{\"27\":6,\"33\":12}}],[\"3d对象功能区域分割\",{\"1\":{\"17\":1}}],[\"3d\",{\"0\":{\"16\":1,\"41\":1},\"1\":{\"2\":1,\"4\":1,\"7\":1,\"10\":2,\"12\":1,\"16\":2,\"17\":3,\"19\":2,\"20\":1,\"24\":1,\"26\":3,\"27\":4,\"38\":2,\"39\":6,\"64\":1,\"66\":1,\"69\":1,\"71\":4}}],[\"i+1\",{\"1\":{\"175\":1,\"176\":1,\"224\":1}}],[\"itg\",{\"0\":{\"103\":1}}],[\"itm\",{\"0\":{\"102\":1},\"1\":{\"102\":12}}],[\"itc\",{\"0\":{\"101\":1},\"1\":{\"101\":1}}],[\"items\",{\"1\":{\"107\":1,\"175\":2,\"224\":2}}],[\"item\",{\"1\":{\"25\":5,\"39\":1,\"102\":2,\"107\":5,\"227\":1,\"235\":1}}],[\"icmlm和convirt仅在10万级别的数据上训练了几天\",{\"1\":{\"96\":1}}],[\"icmlm基于语言掩码的方法\",{\"1\":{\"96\":1}}],[\"ignored\",{\"1\":{\"254\":4}}],[\"ignore\",{\"1\":{\"95\":1,\"227\":1,\"251\":1,\"254\":3}}],[\"iouloss\",{\"1\":{\"168\":2}}],[\"iou\",{\"0\":{\"168\":1},\"1\":{\"39\":27,\"166\":2,\"168\":21}}],[\"iloc\",{\"1\":{\"25\":2}}],[\"i2t\",{\"1\":{\"101\":5,\"102\":4}}],[\"i2\",{\"1\":{\"15\":4}}],[\"i1\",{\"1\":{\"15\":4}}],[\"isinstance\",{\"1\":{\"224\":2,\"238\":2,\"248\":2}}],[\"isn\",{\"1\":{\"107\":1}}],[\"isdir\",{\"1\":{\"93\":1,\"95\":1,\"107\":1}}],[\"is\",{\"1\":{\"13\":1,\"29\":2,\"31\":2,\"32\":1,\"40\":2,\"49\":3,\"53\":2,\"57\":1,\"64\":1,\"65\":1,\"91\":1,\"93\":2,\"94\":2,\"95\":4,\"102\":4,\"103\":12,\"107\":1,\"176\":1,\"218\":7,\"224\":1,\"225\":4,\"227\":2,\"230\":1,\"233\":4,\"236\":2,\"242\":1,\"244\":4,\"249\":1,\"251\":2,\"254\":2,\"255\":2,\"256\":2,\"257\":4,\"267\":1,\"269\":1,\"271\":3}}],[\"i\",{\"1\":{\"11\":17,\"13\":10,\"15\":18,\"16\":2,\"25\":4,\"38\":1,\"39\":12,\"40\":10,\"49\":4,\"53\":7,\"57\":2,\"65\":5,\"90\":12,\"93\":2,\"95\":2,\"103\":3,\"107\":4,\"114\":2,\"175\":8,\"176\":4,\"224\":2,\"225\":8,\"227\":5,\"239\":2,\"287\":1}}],[\"id为文本\",{\"1\":{\"104\":1}}],[\"id=self\",{\"1\":{\"104\":2}}],[\"id=model\",{\"1\":{\"93\":1,\"95\":1}}],[\"identity\",{\"1\":{\"109\":3,\"112\":1,\"114\":1}}],[\"identify\",{\"1\":{\"20\":1}}],[\"iden\",{\"1\":{\"64\":4}}],[\"idw\",{\"1\":{\"55\":1,\"57\":2}}],[\"idx=0\",{\"1\":{\"230\":1,\"236\":1}}],[\"idx2word\",{\"1\":{\"224\":7}}],[\"idxs\",{\"1\":{\"176\":2}}],[\"idx\",{\"1\":{\"49\":21,\"53\":3,\"57\":5,\"93\":5,\"95\":5,\"102\":5,\"175\":10,\"176\":3,\"224\":8,\"225\":4,\"227\":2,\"230\":1}}],[\"ids作用图解\",{\"1\":{\"233\":1}}],[\"ids=token\",{\"1\":{\"233\":1,\"241\":1,\"242\":1,\"251\":1,\"253\":1,\"254\":1,\"256\":1,\"257\":1}}],[\"ids=input\",{\"1\":{\"104\":1,\"233\":1}}],[\"ids=position\",{\"1\":{\"103\":1,\"241\":1,\"242\":1,\"251\":1,\"254\":1,\"256\":1,\"257\":1}}],[\"ids=none\",{\"1\":{\"102\":2,\"103\":2,\"236\":2,\"241\":2,\"242\":2,\"251\":2,\"254\":2,\"256\":2,\"257\":2}}],[\"ids\",{\"1\":{\"10\":1,\"100\":1,\"102\":21,\"103\":9,\"104\":2,\"176\":11,\"224\":7,\"225\":11,\"227\":4,\"233\":36,\"235\":8,\"236\":14,\"241\":4,\"242\":4,\"251\":4,\"253\":2,\"254\":4,\"255\":4,\"256\":4,\"257\":17}}],[\"id\",{\"1\":{\"10\":2,\"25\":6,\"40\":1,\"103\":3,\"104\":3,\"176\":6,\"224\":1,\"226\":1,\"233\":9,\"255\":3}}],[\"imbalance\",{\"1\":{\"166\":1,\"169\":1}}],[\"improves\",{\"1\":{\"199\":1}}],[\"implements\",{\"1\":{\"271\":1}}],[\"impl\",{\"1\":{\"118\":1}}],[\"import\",{\"1\":{\"35\":3,\"40\":7,\"95\":9,\"107\":3}}],[\"imshow\",{\"1\":{\"94\":1,\"95\":1}}],[\"imagenet\",{\"1\":{\"108\":1,\"118\":3}}],[\"images=images\",{\"1\":{\"93\":1,\"95\":1}}],[\"images\",{\"1\":{\"71\":1,\"90\":1,\"91\":1,\"93\":3,\"94\":2,\"95\":5,\"103\":2,\"107\":40,\"108\":12,\"114\":3}}],[\"image\",{\"0\":{\"101\":1,\"102\":1,\"103\":1},\"1\":{\"10\":2,\"13\":6,\"89\":1,\"90\":5,\"91\":10,\"93\":31,\"94\":21,\"95\":51,\"96\":1,\"98\":2,\"100\":14,\"101\":11,\"102\":20,\"103\":7,\"104\":15,\"107\":9,\"109\":1,\"118\":1}}],[\"img\",{\"1\":{\"10\":8,\"11\":7,\"94\":2,\"95\":2,\"107\":9,\"109\":10,\"110\":3,\"111\":2,\"114\":2,\"118\":1}}],[\"if\",{\"1\":{\"10\":2,\"13\":1,\"25\":3,\"29\":2,\"31\":2,\"32\":1,\"37\":2,\"39\":2,\"40\":7,\"49\":5,\"50\":2,\"53\":4,\"57\":2,\"64\":1,\"65\":1,\"66\":3,\"93\":9,\"94\":3,\"95\":12,\"102\":5,\"103\":11,\"104\":1,\"107\":6,\"109\":1,\"112\":1,\"114\":1,\"118\":2,\"175\":1,\"176\":6,\"223\":4,\"224\":8,\"225\":6,\"227\":3,\"233\":7,\"235\":1,\"236\":2,\"238\":1,\"242\":3,\"244\":1,\"248\":1,\"251\":1,\"254\":1,\"256\":2,\"257\":4,\"271\":3}}],[\"install命令安装即可\",{\"1\":{\"147\":1}}],[\"install\",{\"1\":{\"147\":4,\"259\":1}}],[\"instruct\",{\"1\":{\"81\":2}}],[\"instructgpt\",{\"1\":{\"78\":5,\"85\":1}}],[\"instruction\",{\"0\":{\"85\":1},\"1\":{\"10\":1,\"78\":6,\"85\":7,\"278\":1}}],[\"instructional\",{\"1\":{\"10\":3,\"15\":3}}],[\"instructions\",{\"1\":{\"7\":1}}],[\"inf\",{\"1\":{\"227\":1}}],[\"info\",{\"1\":{\"143\":1,\"145\":1,\"238\":1,\"248\":1}}],[\"inference\",{\"1\":{\"4\":1,\"10\":3,\"189\":1,\"277\":1}}],[\"in21k模型权重文件\",{\"1\":{\"118\":1}}],[\"in21k\",{\"1\":{\"118\":4}}],[\"in21k这个模型\",{\"1\":{\"118\":1}}],[\"invariance\",{\"1\":{\"61\":1,\"72\":1}}],[\"invariant\",{\"1\":{\"61\":1}}],[\"inplace=true\",{\"1\":{\"58\":1}}],[\"inputfeatures\",{\"1\":{\"233\":1}}],[\"inputfeatures组成图解\",{\"1\":{\"233\":1}}],[\"inputs\",{\"1\":{\"10\":6,\"13\":6,\"93\":4,\"95\":4,\"166\":9,\"167\":14,\"168\":9,\"169\":8,\"170\":10,\"172\":12,\"233\":8,\"254\":1}}],[\"input\",{\"1\":{\"10\":7,\"13\":19,\"49\":11,\"52\":1,\"53\":3,\"55\":1,\"56\":1,\"91\":2,\"100\":6,\"102\":10,\"103\":8,\"104\":3,\"168\":1,\"224\":4,\"225\":6,\"227\":4,\"233\":13,\"235\":4,\"236\":6,\"238\":2,\"241\":2,\"242\":2,\"245\":2,\"246\":3,\"249\":1,\"251\":2,\"253\":1,\"254\":2,\"255\":4,\"256\":2,\"257\":7,\"267\":1}}],[\"indent=4\",{\"1\":{\"107\":1,\"223\":2,\"224\":1}}],[\"index=5\",{\"1\":{\"255\":2}}],[\"index=ignored\",{\"1\":{\"254\":1}}],[\"index=\",{\"1\":{\"251\":1}}],[\"index=0\",{\"1\":{\"227\":3}}],[\"index=masked\",{\"1\":{\"226\":2}}],[\"indexed\",{\"1\":{\"49\":1}}],[\"index\",{\"1\":{\"25\":3,\"49\":8,\"53\":3,\"57\":2,\"93\":2,\"94\":2,\"95\":4,\"147\":1,\"168\":2,\"170\":1,\"227\":2,\"230\":1,\"253\":4,\"254\":6,\"255\":5}}],[\"inductive\",{\"1\":{\"105\":1}}],[\"indices\",{\"1\":{\"49\":5,\"64\":1,\"66\":1,\"93\":2,\"95\":2,\"102\":6,\"107\":4,\"225\":3}}],[\"inner\",{\"1\":{\"29\":2,\"31\":2}}],[\"into\",{\"1\":{\"253\":1}}],[\"intrinsic\",{\"1\":{\"188\":2,\"189\":1}}],[\"int\",{\"1\":{\"30\":2,\"39\":2,\"110\":5,\"112\":1,\"118\":1,\"175\":19,\"176\":4,\"224\":1,\"225\":1,\"244\":1}}],[\"int64\",{\"1\":{\"25\":2}}],[\"intensity\",{\"1\":{\"71\":1}}],[\"intention\",{\"1\":{\"4\":1}}],[\"intermediate\",{\"1\":{\"238\":9}}],[\"interleave\",{\"1\":{\"104\":1}}],[\"internvl\",{\"0\":{\"76\":1},\"1\":{\"76\":2}}],[\"interaction\",{\"1\":{\"61\":1}}],[\"interactions\",{\"1\":{\"7\":1}}],[\"interpolated\",{\"1\":{\"55\":1,\"57\":4}}],[\"intersect\",{\"1\":{\"39\":3}}],[\"intersection\",{\"0\":{\"168\":1},\"1\":{\"35\":4,\"39\":3,\"166\":2,\"167\":3,\"168\":6,\"172\":2}}],[\"in\",{\"1\":{\"13\":1,\"16\":3,\"25\":4,\"37\":4,\"38\":2,\"39\":5,\"40\":1,\"49\":6,\"50\":4,\"53\":8,\"57\":5,\"58\":8,\"91\":2,\"92\":1,\"93\":7,\"95\":7,\"101\":2,\"102\":2,\"103\":1,\"107\":10,\"109\":4,\"110\":3,\"111\":2,\"112\":8,\"114\":4,\"118\":3,\"175\":17,\"176\":7,\"185\":1,\"198\":1,\"199\":1,\"200\":1,\"223\":5,\"224\":9,\"225\":8,\"226\":2,\"227\":4,\"230\":1,\"233\":1,\"239\":2,\"244\":1,\"262\":1,\"267\":2,\"270\":1,\"271\":4}}],[\"init\",{\"1\":{\"11\":2,\"15\":4,\"16\":2,\"25\":1,\"30\":2,\"35\":2,\"49\":2,\"50\":2,\"53\":4,\"57\":2,\"58\":2,\"64\":2,\"66\":2,\"67\":2,\"68\":2,\"107\":1,\"109\":2,\"110\":3,\"111\":4,\"112\":4,\"113\":2,\"114\":5,\"148\":2,\"166\":2,\"167\":2,\"168\":2,\"169\":2,\"170\":2,\"172\":2,\"224\":1,\"226\":2,\"236\":2,\"238\":6,\"239\":2,\"240\":2,\"241\":3,\"242\":3,\"244\":2,\"245\":2,\"246\":2,\"248\":2,\"249\":2,\"250\":2,\"251\":2,\"254\":2,\"256\":2,\"257\":2,\"262\":2,\"263\":2,\"265\":2,\"266\":2,\"267\":2,\"269\":2,\"270\":2,\"271\":2}}],[\"dtype\",{\"1\":{\"241\":1}}],[\"dtype=next\",{\"1\":{\"241\":1}}],[\"dtype=bool\",{\"1\":{\"40\":1}}],[\"dtype=torch\",{\"1\":{\"13\":1,\"49\":5,\"100\":1,\"102\":4,\"103\":1,\"104\":1,\"236\":1}}],[\"dump\",{\"1\":{\"223\":2,\"224\":1,\"232\":1}}],[\"dumps\",{\"1\":{\"107\":1}}],[\"dv\",{\"1\":{\"137\":3}}],[\"dk\",{\"1\":{\"137\":2}}],[\"da\",{\"1\":{\"232\":1}}],[\"dall\",{\"1\":{\"88\":3}}],[\"data方法实现\",{\"1\":{\"225\":1}}],[\"data为列表形式的情况\",{\"1\":{\"224\":1}}],[\"data文件所提供代码对原始数据格式进行解析\",{\"1\":{\"223\":1}}],[\"dataloader\",{\"0\":{\"235\":1},\"1\":{\"37\":3,\"107\":1,\"108\":2,\"227\":4,\"235\":3}}],[\"dataframe\",{\"1\":{\"25\":1}}],[\"data\",{\"1\":{\"25\":9,\"40\":7,\"49\":14,\"53\":4,\"78\":2,\"94\":9,\"95\":9,\"96\":2,\"107\":3,\"108\":6,\"114\":4,\"224\":9,\"225\":5,\"227\":3,\"230\":1,\"232\":1,\"287\":1}}],[\"dataset\",{\"1\":{\"25\":2,\"37\":6,\"38\":2,\"40\":1,\"107\":5,\"108\":6,\"223\":1,\"224\":1,\"227\":3,\"233\":1,\"235\":3}}],[\"dgcnn\",{\"1\":{\"69\":2,\"72\":1}}],[\"d1+d2\",{\"1\":{\"57\":1}}],[\"draw\",{\"1\":{\"40\":1}}],[\"drop=0\",{\"1\":{\"112\":1}}],[\"drop=drop\",{\"1\":{\"112\":1}}],[\"dropping\",{\"1\":{\"244\":1}}],[\"droppath\",{\"1\":{\"112\":2}}],[\"dropped\",{\"1\":{\"103\":2}}],[\"drop2\",{\"1\":{\"50\":2,\"53\":2}}],[\"drop1\",{\"1\":{\"50\":2,\"53\":2,\"58\":2}}],[\"dropout=none\",{\"1\":{\"271\":1}}],[\"dropout=self\",{\"1\":{\"271\":1}}],[\"dropout=0\",{\"1\":{\"271\":1}}],[\"dropout防止过拟合\",{\"1\":{\"103\":1}}],[\"dropout3\",{\"1\":{\"33\":1}}],[\"dropout2\",{\"1\":{\"33\":1}}],[\"dropout1\",{\"1\":{\"33\":1}}],[\"dropout\",{\"1\":{\"30\":4,\"33\":1,\"50\":2,\"52\":1,\"53\":2,\"55\":1,\"58\":1,\"67\":3,\"102\":1,\"103\":1,\"111\":1,\"112\":4,\"113\":2,\"114\":1,\"211\":2,\"236\":4,\"238\":4,\"242\":4,\"244\":4,\"245\":4,\"256\":4,\"257\":4,\"265\":6,\"266\":2,\"269\":2,\"271\":5}}],[\"drop\",{\"1\":{\"29\":1,\"30\":5,\"31\":1,\"32\":1,\"37\":1,\"111\":5,\"112\":16,\"113\":8,\"114\":9}}],[\"drive\",{\"1\":{\"4\":2}}],[\"dynamic\",{\"1\":{\"27\":1,\"33\":4}}],[\"df\",{\"1\":{\"25\":7}}],[\"doing\",{\"1\":{\"242\":1}}],[\"do\",{\"1\":{\"104\":1,\"232\":3,\"271\":1}}],[\"dog\",{\"1\":{\"103\":1,\"218\":7}}],[\"does\",{\"1\":{\"94\":1,\"95\":1,\"107\":1}}],[\"dot\",{\"1\":{\"90\":3,\"93\":1,\"95\":1,\"123\":1,\"221\":2,\"244\":1,\"271\":1}}],[\"door\",{\"1\":{\"20\":1,\"24\":1,\"25\":1}}],[\"downloaded\",{\"1\":{\"93\":1,\"95\":1}}],[\"downloading\",{\"1\":{\"93\":2,\"95\":2}}],[\"download\",{\"1\":{\"93\":3,\"95\":4,\"118\":1}}],[\"down\",{\"1\":{\"10\":1,\"14\":1,\"185\":1}}],[\"diag\",{\"1\":{\"102\":6}}],[\"dir=\",{\"1\":{\"232\":3}}],[\"dir=save\",{\"1\":{\"93\":1,\"95\":1}}],[\"directional\",{\"1\":{\"102\":2}}],[\"directory\",{\"1\":{\"93\":5,\"94\":1,\"95\":6}}],[\"dirname\",{\"1\":{\"93\":1,\"95\":1,\"223\":2}}],[\"dir\",{\"1\":{\"93\":16,\"94\":7,\"95\":23,\"232\":4}}],[\"distill\",{\"1\":{\"278\":1}}],[\"distributedsampler\",{\"1\":{\"235\":1}}],[\"distribution\",{\"1\":{\"107\":1}}],[\"dists\",{\"1\":{\"57\":6}}],[\"dist\",{\"1\":{\"49\":3,\"57\":3}}],[\"distance\",{\"1\":{\"49\":7,\"57\":1}}],[\"display\",{\"1\":{\"25\":2}}],[\"dishwasher\",{\"1\":{\"25\":1}}],[\"diversity\",{\"1\":{\"20\":1}}],[\"div\",{\"1\":{\"11\":2}}],[\"dict=return\",{\"1\":{\"103\":1}}],[\"dict=none\",{\"1\":{\"103\":1}}],[\"dict=true\",{\"1\":{\"100\":2,\"102\":1,\"103\":3}}],[\"dict=false\",{\"1\":{\"10\":1}}],[\"dicts\",{\"1\":{\"37\":2}}],[\"dict\",{\"1\":{\"37\":2,\"38\":1,\"39\":2,\"40\":5,\"103\":1,\"107\":2,\"118\":3,\"175\":13,\"224\":8,\"227\":2}}],[\"dicebceloss\",{\"1\":{\"167\":2}}],[\"diceloss\",{\"1\":{\"35\":4,\"166\":2}}],[\"dice\",{\"0\":{\"166\":1,\"167\":1},\"1\":{\"10\":1,\"35\":14,\"166\":17,\"167\":21,\"168\":2,\"170\":2,\"172\":14}}],[\"dim代表的是卷积核的数量\",{\"1\":{\"109\":1}}],[\"dims\",{\"1\":{\"30\":13}}],[\"dimensional\",{\"1\":{\"105\":1}}],[\"dimension\",{\"1\":{\"15\":1,\"16\":1,\"69\":1,\"188\":2}}],[\"dimensions\",{\"1\":{\"10\":1}}],[\"dim=embed\",{\"1\":{\"110\":1,\"111\":1,\"114\":2}}],[\"dim=768\",{\"1\":{\"109\":1,\"110\":1,\"111\":1,\"114\":1,\"118\":1}}],[\"dim=2\",{\"1\":{\"57\":2,\"103\":2}}],[\"dim=dict\",{\"1\":{\"37\":2,\"40\":2}}],[\"dim=0\",{\"1\":{\"35\":1,\"102\":7,\"104\":1,\"107\":1,\"227\":1}}],[\"dim=\",{\"1\":{\"11\":1,\"15\":2,\"35\":1,\"49\":3,\"53\":1,\"57\":2,\"65\":1,\"68\":1,\"91\":4,\"100\":2,\"103\":1,\"113\":1,\"230\":1,\"244\":1,\"253\":3,\"254\":1,\"263\":1,\"271\":1}}],[\"dim=1\",{\"1\":{\"10\":1,\"11\":2,\"15\":1,\"16\":2,\"35\":4,\"53\":1,\"58\":1,\"67\":1,\"102\":5,\"103\":1,\"110\":1,\"111\":1,\"114\":2,\"226\":2,\"227\":1}}],[\"dim\",{\"1\":{\"11\":14,\"12\":4,\"13\":2,\"14\":2,\"15\":42,\"16\":14,\"27\":3,\"37\":2,\"40\":2,\"109\":8,\"110\":6,\"111\":6,\"112\":7,\"113\":17,\"114\":9}}],[\"d\",{\"1\":{\"10\":1,\"27\":1,\"49\":4,\"53\":4,\"57\":17,\"65\":4,\"90\":6,\"98\":2,\"104\":4,\"111\":2,\"226\":10,\"230\":2,\"244\":2,\"257\":1,\"263\":3,\"271\":15}}],[\"dependency\",{\"1\":{\"223\":2}}],[\"depth\",{\"1\":{\"114\":1}}],[\"depth=12\",{\"1\":{\"111\":1,\"114\":1,\"118\":1}}],[\"del\",{\"1\":{\"175\":1,\"271\":3}}],[\"detokenize\",{\"1\":{\"224\":2}}],[\"detection\",{\"1\":{\"169\":1}}],[\"details\",{\"0\":{\"229\":1}}],[\"detailed\",{\"1\":{\"81\":1}}],[\"detach\",{\"1\":{\"40\":1}}],[\"dense\",{\"1\":{\"169\":1,\"238\":4,\"240\":2,\"245\":2,\"248\":2,\"265\":1,\"278\":2}}],[\"deactivate\",{\"1\":{\"142\":1}}],[\"deepseekr1\",{\"1\":{\"278\":1}}],[\"deepseekmoe\",{\"1\":{\"278\":1}}],[\"deepseek\",{\"1\":{\"277\":1,\"278\":17}}],[\"deep\",{\"1\":{\"72\":1}}],[\"desc=\",{\"1\":{\"175\":3}}],[\"desc\",{\"1\":{\"91\":2}}],[\"description\",{\"1\":{\"10\":3,\"81\":1}}],[\"destroy\",{\"1\":{\"40\":1}}],[\"debug\",{\"1\":{\"39\":1}}],[\"decay=0\",{\"1\":{\"227\":1}}],[\"decay=opt\",{\"1\":{\"37\":1}}],[\"decay\",{\"1\":{\"37\":1}}],[\"decode\",{\"1\":{\"104\":1,\"224\":1,\"255\":1,\"262\":2}}],[\"decoder模型结构图\",{\"1\":{\"269\":1,\"270\":1}}],[\"decoderlayer\",{\"0\":{\"269\":1},\"1\":{\"269\":2}}],[\"decoder=is\",{\"1\":{\"103\":1}}],[\"decoder=true时\",{\"1\":{\"103\":1}}],[\"decoder=true\",{\"1\":{\"103\":1}}],[\"decoder\",{\"0\":{\"262\":1,\"268\":1,\"270\":1},\"1\":{\"10\":1,\"15\":1,\"16\":1,\"27\":2,\"33\":3,\"56\":1,\"98\":1,\"103\":9,\"104\":2,\"207\":1,\"211\":1,\"249\":2,\"255\":1,\"261\":2,\"262\":4,\"269\":1,\"270\":3,\"278\":5}}],[\"decoding过程\",{\"1\":{\"27\":2,\"33\":1}}],[\"decoding\",{\"1\":{\"27\":1}}],[\"device=input\",{\"1\":{\"236\":1}}],[\"device=image\",{\"1\":{\"101\":1}}],[\"device=sim\",{\"1\":{\"102\":1}}],[\"device=multi\",{\"1\":{\"13\":1}}],[\"device\",{\"1\":{\"10\":3,\"13\":1,\"27\":1,\"39\":1,\"49\":15,\"93\":5,\"95\":5,\"100\":2,\"101\":1,\"102\":4,\"103\":1,\"104\":2,\"114\":4,\"118\":1,\"227\":2,\"236\":1}}],[\"defined\",{\"1\":{\"269\":1}}],[\"defaultdict\",{\"1\":{\"175\":3}}],[\"default\",{\"1\":{\"40\":1}}],[\"def\",{\"1\":{\"10\":1,\"11\":2,\"13\":1,\"15\":4,\"16\":2,\"25\":3,\"27\":1,\"29\":2,\"30\":2,\"31\":2,\"32\":1,\"33\":2,\"35\":2,\"37\":1,\"40\":4,\"49\":7,\"50\":2,\"53\":4,\"55\":1,\"57\":2,\"58\":2,\"64\":2,\"65\":1,\"66\":2,\"67\":2,\"68\":2,\"93\":8,\"94\":3,\"95\":11,\"100\":1,\"102\":1,\"103\":4,\"104\":1,\"107\":5,\"109\":2,\"110\":3,\"111\":3,\"112\":4,\"113\":2,\"114\":4,\"118\":1,\"166\":2,\"167\":2,\"168\":2,\"169\":2,\"170\":2,\"172\":2,\"175\":8,\"176\":6,\"223\":2,\"224\":6,\"225\":3,\"226\":2,\"230\":2,\"233\":3,\"235\":1,\"236\":2,\"238\":6,\"239\":2,\"240\":2,\"241\":2,\"242\":2,\"244\":3,\"245\":2,\"246\":2,\"248\":2,\"249\":2,\"250\":2,\"251\":2,\"254\":2,\"256\":2,\"257\":2,\"262\":4,\"263\":2,\"265\":2,\"266\":2,\"267\":2,\"269\":2,\"270\":2,\"271\":3}}],[\"but\",{\"1\":{\"244\":1,\"249\":1}}],[\"build\",{\"1\":{\"233\":1}}],[\"b上有1个点的绝对提升\",{\"1\":{\"212\":1}}],[\"b为r\",{\"1\":{\"184\":1}}],[\"b为批量大小\",{\"1\":{\"113\":1}}],[\"bpe\",{\"1\":{\"174\":2,\"175\":8,\"211\":1}}],[\"bengio\",{\"1\":{\"277\":1}}],[\"benchmark\",{\"1\":{\"212\":1}}],[\"benchmarks\",{\"1\":{\"211\":1}}],[\"below\",{\"1\":{\"269\":1}}],[\"between\",{\"1\":{\"244\":1}}],[\"beta=beta\",{\"1\":{\"170\":1,\"172\":1}}],[\"beta\",{\"1\":{\"170\":4}}],[\"beta得分\",{\"1\":{\"170\":1}}],[\"betas=\",{\"1\":{\"37\":1}}],[\"be\",{\"1\":{\"224\":1,\"233\":1}}],[\"bermanmaxim\",{\"1\":{\"171\":1}}],[\"bert支持的下游任务图\",{\"1\":{\"252\":1}}],[\"bertformultiplechoice\",{\"1\":{\"257\":2}}],[\"bertfortokenclassification\",{\"1\":{\"256\":2}}],[\"bertforquestionanswering\",{\"1\":{\"253\":1,\"254\":2}}],[\"bertforpretraining结构图\",{\"1\":{\"251\":1}}],[\"bertforpretraining\",{\"0\":{\"251\":1},\"1\":{\"251\":2}}],[\"bertforsequenceclassification模型结构图\",{\"1\":{\"242\":1}}],[\"bertforsequenceclassification\",{\"0\":{\"242\":1},\"1\":{\"242\":2}}],[\"bertselfoutput计算流程图\",{\"1\":{\"245\":1}}],[\"bertselfoutput\",{\"0\":{\"245\":1},\"1\":{\"245\":2,\"246\":1}}],[\"bertselfattention\",{\"0\":{\"244\":1},\"1\":{\"103\":2,\"244\":2,\"246\":1}}],[\"bertpretrainingheads结构图\",{\"1\":{\"250\":1}}],[\"bertpretrainingheads\",{\"0\":{\"250\":1},\"1\":{\"250\":2,\"251\":1}}],[\"bertpretrainedmodel\",{\"1\":{\"103\":1,\"241\":1,\"242\":1,\"251\":1,\"254\":1,\"256\":1,\"257\":1}}],[\"bertpredictionheadtransform结构图\",{\"1\":{\"248\":1}}],[\"bertpredictionheadtransform\",{\"0\":{\"248\":1},\"1\":{\"248\":2,\"249\":1}}],[\"bertpooler模型结构图\",{\"1\":{\"240\":1}}],[\"bertpooler\",{\"0\":{\"240\":1},\"1\":{\"240\":2,\"241\":1}}],[\"bertattention计算流程图\",{\"1\":{\"246\":1}}],[\"bertattention\",{\"0\":{\"243\":1,\"246\":1},\"1\":{\"238\":1,\"246\":2}}],[\"bertoutput\",{\"1\":{\"238\":3}}],[\"bertonlymlmhead\",{\"1\":{\"103\":1}}],[\"bertintermediate\",{\"1\":{\"238\":3}}],[\"bert文本分类实战\",{\"1\":{\"231\":1}}],[\"bertdataset\",{\"1\":{\"227\":1}}],[\"berttokenizer中的特殊token\",{\"1\":{\"233\":1}}],[\"berttokenizer\",{\"1\":{\"224\":1,\"233\":3,\"253\":1}}],[\"bertmodel模型结构图\",{\"1\":{\"241\":1}}],[\"bertmodel\",{\"0\":{\"241\":1},\"1\":{\"103\":2,\"241\":2,\"242\":1,\"244\":1,\"251\":1,\"254\":1,\"256\":1,\"257\":1}}],[\"bertlmpredictionhead结构图\",{\"1\":{\"249\":1}}],[\"bertlmpredictionhead\",{\"0\":{\"249\":1},\"1\":{\"249\":2,\"250\":1}}],[\"bertlmheadmodel\",{\"1\":{\"103\":2}}],[\"bertlayer模型结构图\",{\"1\":{\"238\":1}}],[\"bertlayernorm\",{\"1\":{\"236\":1,\"238\":1,\"245\":1,\"248\":1}}],[\"bertlayer\",{\"0\":{\"238\":1},\"1\":{\"103\":3,\"238\":2,\"239\":1}}],[\"bertencoder模型结构图\",{\"1\":{\"239\":1}}],[\"bertencoder\",{\"0\":{\"237\":1,\"239\":1},\"1\":{\"103\":2,\"239\":2,\"241\":1}}],[\"bertembeddings\",{\"0\":{\"236\":1},\"1\":{\"102\":1,\"236\":2,\"241\":1}}],[\"bertembeddings会将text\",{\"1\":{\"102\":1}}],[\"bert\",{\"0\":{\"216\":1,\"217\":1,\"231\":1},\"1\":{\"85\":1,\"100\":2,\"102\":1,\"103\":2,\"174\":1,\"217\":4,\"218\":2,\"220\":1,\"221\":4,\"224\":1,\"226\":4,\"227\":1,\"232\":16,\"233\":1,\"242\":2,\"251\":2,\"253\":5,\"254\":2,\"255\":9,\"256\":2,\"257\":7,\"277\":1}}],[\"before\",{\"1\":{\"148\":1}}],[\"beam数量\",{\"1\":{\"104\":1}}],[\"beams=num\",{\"1\":{\"104\":1}}],[\"beams=3\",{\"1\":{\"104\":1}}],[\"beams\",{\"1\":{\"104\":4}}],[\"beam\",{\"1\":{\"104\":1}}],[\"begin\",{\"1\":{\"103\":1}}],[\"best\",{\"1\":{\"39\":6,\"40\":1,\"175\":4,\"227\":7}}],[\"bed\",{\"1\":{\"25\":1}}],[\"bce\",{\"0\":{\"167\":1},\"1\":{\"35\":3,\"166\":1,\"167\":17,\"168\":2,\"169\":2}}],[\"bcn\",{\"1\":{\"27\":1,\"33\":3}}],[\"bit\",{\"1\":{\"244\":1}}],[\"bin\",{\"1\":{\"232\":1}}],[\"binary\",{\"0\":{\"2\":1},\"1\":{\"0\":1,\"21\":1,\"35\":1,\"39\":11,\"167\":6,\"169\":2,\"172\":1}}],[\"binaryoracle\",{\"1\":{\"0\":1,\"216\":1}}],[\"bidirectional\",{\"1\":{\"217\":1}}],[\"bilstm\",{\"1\":{\"212\":1}}],[\"billion\",{\"1\":{\"115\":1}}],[\"bigram\",{\"1\":{\"175\":2}}],[\"bias=qkv\",{\"1\":{\"112\":1,\"113\":1,\"114\":1}}],[\"bias=true\",{\"1\":{\"111\":1,\"114\":1}}],[\"bias=false\",{\"1\":{\"30\":2,\"112\":1,\"113\":1,\"226\":1,\"249\":1}}],[\"bias\",{\"1\":{\"105\":1,\"112\":1,\"113\":1,\"114\":1,\"249\":3}}],[\"bi\",{\"1\":{\"102\":2}}],[\"block第一个全连接的节点个数\",{\"1\":{\"115\":1}}],[\"block的次数\",{\"1\":{\"115\":1}}],[\"block块序列\",{\"1\":{\"114\":2}}],[\"block\",{\"1\":{\"112\":3,\"114\":1,\"208\":1}}],[\"blocks\",{\"1\":{\"53\":7,\"114\":2}}],[\"blob\",{\"1\":{\"107\":1}}],[\"blip\",{\"1\":{\"98\":2,\"99\":1}}],[\"blip2qformer\",{\"1\":{\"100\":1,\"104\":2}}],[\"blip2qformer核心代码实现如下\",{\"1\":{\"100\":1}}],[\"blip2\",{\"1\":{\"97\":1}}],[\"blip2base\",{\"1\":{\"10\":1,\"100\":1,\"104\":1}}],[\"black\",{\"1\":{\"80\":1,\"91\":1}}],[\"blc\",{\"1\":{\"27\":1,\"33\":3}}],[\"bytenet和convs2s等网络模型\",{\"1\":{\"260\":1}}],[\"bytepairtokenizer\",{\"1\":{\"175\":2,\"176\":2}}],[\"bytes\",{\"1\":{\"175\":2,\"176\":20}}],[\"byte\",{\"1\":{\"174\":1,\"175\":4,\"176\":7}}],[\"by\",{\"1\":{\"25\":1,\"197\":1,\"240\":1}}],[\"bookscorpus\",{\"1\":{\"211\":1}}],[\"books\",{\"1\":{\"211\":1,\"278\":1}}],[\"bool\",{\"1\":{\"118\":1,\"166\":1,\"167\":1,\"168\":1,\"169\":1,\"176\":1}}],[\"bos\",{\"1\":{\"103\":3,\"104\":3}}],[\"box\",{\"1\":{\"80\":2}}],[\"bounding\",{\"1\":{\"80\":2}}],[\"bowl\",{\"1\":{\"25\":1}}],[\"bottleneck\",{\"1\":{\"28\":1,\"62\":1,\"69\":1}}],[\"bottle\",{\"1\":{\"10\":1,\"25\":1}}],[\"bad\",{\"1\":{\"290\":2,\"291\":3}}],[\"ba\",{\"1\":{\"189\":1}}],[\"bashrc\",{\"1\":{\"148\":1}}],[\"bash\",{\"1\":{\"148\":2}}],[\"base\",{\"1\":{\"108\":1,\"118\":7,\"142\":1,\"143\":1,\"232\":6,\"278\":2}}],[\"basemodeloutputwithpastandcrossattentions\",{\"1\":{\"103\":1}}],[\"basename\",{\"1\":{\"93\":1,\"95\":1}}],[\"based\",{\"1\":{\"69\":2,\"72\":1,\"104\":1}}],[\"bart\",{\"1\":{\"255\":1}}],[\"bar\",{\"1\":{\"107\":1}}],[\"baidu\",{\"1\":{\"107\":1,\"118\":1}}],[\"ball\",{\"1\":{\"47\":4,\"49\":6,\"53\":2,\"55\":1}}],[\"backpack\",{\"1\":{\"80\":1}}],[\"background\",{\"1\":{\"40\":1}}],[\"back\",{\"1\":{\"40\":3}}],[\"backward\",{\"1\":{\"38\":1,\"114\":1,\"227\":1}}],[\"backbone\",{\"1\":{\"27\":3}}],[\"bag\",{\"1\":{\"25\":1,\"96\":3}}],[\"batch数据准备\",{\"0\":{\"225\":1}}],[\"batch和text\",{\"1\":{\"100\":1}}],[\"batchsize\",{\"1\":{\"64\":2,\"65\":1,\"68\":2}}],[\"batches\",{\"1\":{\"38\":1,\"93\":2,\"95\":2,\"227\":3,\"271\":1}}],[\"batched\",{\"1\":{\"13\":1}}],[\"batchnorm2d\",{\"1\":{\"49\":1,\"53\":1}}],[\"batchnorm\",{\"1\":{\"16\":1,\"49\":1,\"66\":1}}],[\"batchnorm1d\",{\"1\":{\"11\":2,\"15\":1,\"16\":1,\"50\":2,\"53\":2,\"55\":1,\"57\":2,\"58\":1,\"64\":5,\"66\":3,\"67\":2,\"68\":3}}],[\"batch\",{\"1\":{\"10\":2,\"13\":7,\"15\":1,\"27\":1,\"33\":2,\"35\":1,\"37\":3,\"39\":1,\"49\":13,\"64\":6,\"65\":2,\"93\":9,\"95\":9,\"100\":1,\"101\":2,\"102\":1,\"103\":1,\"104\":3,\"107\":4,\"108\":2,\"113\":10,\"133\":1,\"166\":2,\"167\":1,\"168\":1,\"169\":3,\"225\":4,\"226\":14,\"227\":10,\"230\":7,\"232\":2,\"235\":4,\"244\":2,\"253\":6,\"254\":3,\"256\":2,\"257\":7,\"271\":2}}],[\"bn5\",{\"1\":{\"64\":2}}],[\"bn4\",{\"1\":{\"64\":2}}],[\"bn3\",{\"1\":{\"64\":2,\"66\":2,\"68\":2}}],[\"bn2\",{\"1\":{\"50\":2,\"53\":2,\"64\":2,\"66\":2,\"67\":2,\"68\":2}}],[\"bn1\",{\"1\":{\"50\":2,\"53\":2,\"58\":2,\"64\":2,\"66\":2,\"67\":2,\"68\":2}}],[\"bns\",{\"1\":{\"49\":3,\"53\":3,\"57\":3}}],[\"bn\",{\"1\":{\"16\":1,\"49\":2,\"53\":7,\"57\":4,\"64\":1}}],[\"bs\",{\"1\":{\"13\":2,\"40\":1,\"102\":19}}],[\"bmm\",{\"1\":{\"11\":3,\"15\":4,\"64\":1,\"65\":1,\"66\":2}}],[\"b\",{\"1\":{\"10\":15,\"11\":11,\"13\":1,\"15\":28,\"16\":10,\"27\":9,\"29\":2,\"30\":2,\"31\":2,\"32\":2,\"33\":17,\"35\":2,\"39\":3,\"49\":56,\"50\":2,\"53\":11,\"55\":4,\"57\":33,\"58\":7,\"64\":3,\"66\":4,\"68\":7,\"90\":2,\"98\":2,\"100\":8,\"101\":17,\"102\":4,\"104\":3,\"108\":3,\"109\":8,\"110\":7,\"111\":5,\"113\":3,\"114\":5,\"115\":1,\"118\":2,\"162\":2,\"166\":4,\"167\":4,\"168\":4,\"169\":4,\"172\":1,\"189\":3,\"208\":1,\"212\":1,\"224\":5,\"225\":20,\"226\":1,\"230\":1,\"233\":1,\"257\":1}}],[\"html\",{\"1\":{\"291\":1}}],[\"https\",{\"1\":{\"4\":3,\"7\":2,\"17\":2,\"42\":3,\"59\":3,\"76\":2,\"77\":2,\"97\":2,\"107\":2,\"118\":4,\"171\":1,\"202\":1,\"216\":1,\"223\":2,\"232\":2,\"259\":1}}],[\"hh\",{\"1\":{\"175\":1}}],[\"hinge\",{\"0\":{\"171\":1},\"1\":{\"171\":1}}],[\"hidden\",{\"1\":{\"10\":4,\"12\":2,\"13\":3,\"14\":3,\"15\":1,\"100\":8,\"101\":7,\"102\":7,\"103\":34,\"104\":1,\"112\":9,\"115\":1,\"226\":2,\"236\":5,\"238\":24,\"239\":5,\"240\":5,\"242\":4,\"244\":8,\"245\":12,\"248\":15,\"249\":7,\"250\":1,\"251\":1,\"253\":6,\"254\":2,\"256\":3,\"257\":4}}],[\"hdf5\",{\"1\":{\"147\":1}}],[\"h`和\",{\"1\":{\"109\":1}}],[\"hub\",{\"1\":{\"95\":1}}],[\"huggingface\",{\"1\":{\"95\":1,\"224\":1}}],[\"human\",{\"1\":{\"78\":1,\"80\":1,\"81\":2,\"180\":1}}],[\"houlsby\",{\"1\":{\"188\":1}}],[\"home\",{\"1\":{\"143\":3}}],[\"hook\",{\"1\":{\"103\":1}}],[\"horse\",{\"1\":{\"91\":1}}],[\"hold\",{\"1\":{\"40\":2,\"224\":2}}],[\"hybrid\",{\"0\":{\"35\":1},\"1\":{\"35\":1,\"117\":1}}],[\"haiku\",{\"1\":{\"278\":2}}],[\"hairy\",{\"1\":{\"218\":2}}],[\"hairy→my\",{\"1\":{\"218\":3}}],[\"harvardnlp\",{\"1\":{\"259\":1}}],[\"hard\",{\"1\":{\"35\":1,\"39\":1,\"169\":2}}],[\"happy\",{\"1\":{\"113\":1}}],[\"ha=\",{\"1\":{\"107\":1}}],[\"haotian\",{\"1\":{\"77\":1}}],[\"hands\",{\"1\":{\"40\":2}}],[\"hat\",{\"1\":{\"25\":1}}],[\"has\",{\"1\":{\"10\":1,\"103\":1,\"118\":3,\"233\":2}}],[\"hw\",{\"1\":{\"11\":1,\"15\":3}}],[\"hm\",{\"0\":{\"35\":1},\"1\":{\"10\":6,\"35\":3,\"37\":2,\"38\":3}}],[\"he\",{\"1\":{\"175\":1}}],[\"hey\",{\"1\":{\"175\":1}}],[\"hello\",{\"1\":{\"175\":1}}],[\"help\",{\"1\":{\"144\":2}}],[\"here\",{\"1\":{\"30\":1,\"242\":1}}],[\"heatmap\",{\"1\":{\"10\":1,\"16\":2}}],[\"head结构由linear+tanh激活函数+linear组成\",{\"1\":{\"114\":1}}],[\"head进行分类\",{\"1\":{\"114\":1}}],[\"head的位置是和这个\",{\"1\":{\"110\":1}}],[\"head之中再输出分类结果\",{\"1\":{\"110\":1}}],[\"heads代表transformer中multi\",{\"1\":{\"115\":1}}],[\"heads=8\",{\"1\":{\"113\":1}}],[\"heads=num\",{\"1\":{\"112\":1,\"114\":1}}],[\"heads=12\",{\"1\":{\"111\":1,\"114\":1,\"118\":1}}],[\"heads=dict\",{\"1\":{\"37\":1,\"40\":1}}],[\"heads\",{\"1\":{\"11\":6,\"37\":1,\"40\":1,\"103\":1,\"112\":2,\"113\":12,\"114\":1,\"230\":2,\"244\":7,\"271\":3}}],[\"head\",{\"0\":{\"114\":1},\"1\":{\"10\":1,\"16\":6,\"55\":1,\"56\":1,\"102\":1,\"103\":10,\"110\":1,\"111\":1,\"112\":1,\"113\":8,\"114\":3,\"115\":1,\"118\":1,\"131\":1,\"133\":1,\"239\":2,\"241\":1,\"242\":2,\"244\":10,\"251\":2,\"254\":1,\"256\":2,\"257\":2,\"261\":1,\"278\":1}}],[\"height=600\",{\"1\":{\"40\":1}}],[\"height\",{\"1\":{\"10\":1,\"107\":1}}],[\"hezhu\",{\"1\":{\"7\":1}}],[\"h\",{\"1\":{\"10\":3,\"11\":2,\"13\":1,\"62\":5,\"90\":1,\"100\":1,\"109\":4,\"110\":1,\"111\":1,\"114\":1,\"170\":2,\"172\":1,\"175\":5,\"226\":5,\"232\":3,\"271\":9}}],[\"2m\",{\"1\":{\"278\":1}}],[\"22\",{\"1\":{\"232\":1,\"259\":1}}],[\"224×224\",{\"1\":{\"118\":1}}],[\"224\",{\"1\":{\"108\":2,\"109\":2,\"118\":6}}],[\"224x224\",{\"1\":{\"108\":3,\"109\":1,\"118\":1}}],[\"2和wikitext\",{\"1\":{\"223\":1}}],[\"2取得91\",{\"1\":{\"212\":1}}],[\"2是二分类\",{\"1\":{\"212\":1}}],[\"2相比prompt\",{\"1\":{\"196\":1}}],[\"21843\",{\"1\":{\"118\":1}}],[\"21000\",{\"1\":{\"118\":1}}],[\"21k\",{\"1\":{\"118\":3}}],[\"215\",{\"1\":{\"22\":1}}],[\"2×1000000000\",{\"1\":{\"115\":1}}],[\"2b\",{\"1\":{\"115\":1}}],[\"2的起因\",{\"1\":{\"98\":1}}],[\"2f\",{\"1\":{\"93\":2,\"95\":2}}],[\"2训练时使用的webtext数据集相似\",{\"1\":{\"90\":1}}],[\"244\",{\"1\":{\"109\":1}}],[\"242\",{\"1\":{\"80\":1}}],[\"2411\",{\"1\":{\"4\":1}}],[\"29\",{\"1\":{\"40\":1,\"278\":1}}],[\"2>\",{\"1\":{\"40\":1}}],[\"2501\",{\"1\":{\"233\":2}}],[\"2504\",{\"1\":{\"7\":1}}],[\"25e\",{\"1\":{\"211\":1}}],[\"25~0\",{\"1\":{\"169\":1}}],[\"2578\",{\"1\":{\"233\":2}}],[\"257\",{\"1\":{\"104\":3}}],[\"256\",{\"1\":{\"50\":5,\"53\":6,\"57\":1,\"58\":11,\"64\":3,\"67\":3,\"68\":3,\"108\":2}}],[\"255\",{\"1\":{\"40\":2,\"108\":2}}],[\"25\",{\"1\":{\"35\":1,\"169\":1}}],[\"2️⃣\",{\"0\":{\"30\":1},\"1\":{\"57\":1}}],[\"235b\",{\"1\":{\"278\":1}}],[\"2301\",{\"1\":{\"97\":1}}],[\"2304\",{\"1\":{\"77\":1}}],[\"2312\",{\"1\":{\"76\":1}}],[\"23\",{\"1\":{\"22\":1,\"24\":1,\"40\":1,\"81\":1}}],[\"200k\",{\"1\":{\"278\":5}}],[\"2003\",{\"1\":{\"277\":1}}],[\"2000\",{\"1\":{\"211\":1}}],[\"2016\",{\"1\":{\"282\":1}}],[\"2016年的工作\",{\"1\":{\"96\":1}}],[\"2019\",{\"1\":{\"278\":2}}],[\"2018\",{\"1\":{\"232\":1,\"277\":1,\"278\":1}}],[\"2010\",{\"1\":{\"118\":1}}],[\"2017年的工作\",{\"1\":{\"96\":1}}],[\"20\",{\"1\":{\"32\":1,\"39\":3,\"169\":1,\"225\":4,\"277\":1}}],[\"2025\",{\"1\":{\"278\":9}}],[\"2023\",{\"1\":{\"278\":17}}],[\"2022\",{\"1\":{\"278\":2,\"286\":1}}],[\"2022年清华提出的\",{\"1\":{\"188\":1}}],[\"2021年微软提出的\",{\"1\":{\"188\":1}}],[\"2021\",{\"1\":{\"30\":1,\"88\":2,\"278\":1,\"281\":1}}],[\"2024年\",{\"1\":{\"278\":1}}],[\"2024\",{\"1\":{\"17\":1,\"277\":1,\"278\":19,\"288\":1}}],[\"2048\",{\"1\":{\"10\":3,\"16\":2,\"24\":1,\"25\":2,\"27\":1,\"38\":2,\"40\":1}}],[\"2c\",{\"1\":{\"15\":1}}],[\"2\",{\"0\":{\"11\":1,\"20\":1,\"102\":1,\"104\":1,\"109\":1,\"126\":1,\"131\":1,\"293\":1},\"1\":{\"10\":1,\"11\":5,\"15\":9,\"16\":4,\"22\":1,\"27\":17,\"30\":2,\"33\":3,\"35\":5,\"37\":1,\"39\":4,\"40\":4,\"49\":8,\"50\":1,\"53\":7,\"56\":2,\"57\":6,\"58\":2,\"62\":1,\"64\":2,\"65\":2,\"66\":6,\"68\":4,\"69\":2,\"90\":1,\"91\":1,\"98\":1,\"99\":2,\"101\":2,\"102\":5,\"103\":5,\"104\":2,\"107\":2,\"109\":3,\"111\":1,\"113\":4,\"154\":1,\"155\":1,\"160\":1,\"166\":1,\"167\":2,\"169\":4,\"172\":1,\"175\":6,\"176\":9,\"191\":1,\"209\":2,\"211\":1,\"212\":2,\"213\":1,\"217\":1,\"223\":10,\"224\":1,\"225\":2,\"226\":6,\"227\":1,\"230\":1,\"236\":1,\"238\":1,\"241\":1,\"242\":1,\"244\":4,\"248\":1,\"250\":1,\"251\":2,\"253\":2,\"254\":3,\"266\":1,\"269\":1,\"271\":5,\"277\":3,\"278\":19}}],[\"2d\",{\"1\":{\"10\":1,\"12\":1,\"71\":2,\"109\":1}}],[\"mvp\",{\"1\":{\"291\":1}}],[\"mvcnn\",{\"1\":{\"69\":3}}],[\"mcp\",{\"1\":{\"278\":1}}],[\"mla\",{\"1\":{\"278\":1}}],[\"mlm\",{\"1\":{\"225\":1,\"226\":6,\"227\":2,\"228\":1}}],[\"mlm任务掩码策略的方法\",{\"1\":{\"224\":1}}],[\"mlp映射\",{\"1\":{\"31\":1}}],[\"mlpmixerlayer\",{\"1\":{\"30\":1}}],[\"mlp₂\",{\"1\":{\"30\":1}}],[\"mlp₁\",{\"1\":{\"30\":1}}],[\"mlp=\",{\"1\":{\"16\":3,\"50\":3,\"58\":8}}],[\"mlp\",{\"0\":{\"30\":1,\"114\":1},\"1\":{\"10\":1,\"11\":3,\"16\":2,\"30\":10,\"32\":1,\"33\":1,\"49\":8,\"53\":3,\"55\":5,\"57\":9,\"58\":1,\"62\":1,\"67\":2,\"111\":1,\"112\":14,\"114\":4,\"115\":1}}],[\"mrpc\",{\"1\":{\"212\":1}}],[\"mrg策略在处理每个局部区域时\",{\"1\":{\"54\":1}}],[\"mrg通过结合来自不同分辨率的特征来实现效率和适应性的平衡\",{\"1\":{\"54\":1}}],[\"mrg为一种低成本的替代方案\",{\"1\":{\"54\":1}}],[\"mrg\",{\"1\":{\"51\":1}}],[\"mkl\",{\"1\":{\"147\":1}}],[\"mseloss\",{\"1\":{\"242\":1}}],[\"msqrt\",{\"1\":{\"230\":1}}],[\"msg方法虽然有效\",{\"1\":{\"54\":1}}],[\"msg相当于并联了多个hierarchical\",{\"1\":{\"53\":1}}],[\"msg的关键优点在于它通过在训练期间的随机输入丢弃\",{\"1\":{\"53\":1}}],[\"msg通过应用不同尺度的分组层\",{\"1\":{\"52\":1}}],[\"msg\",{\"1\":{\"51\":1,\"53\":3}}],[\"msmvpam\",{\"1\":{\"4\":1}}],[\"myenv\",{\"1\":{\"140\":3,\"141\":1,\"143\":2,\"144\":1}}],[\"mydataset\",{\"1\":{\"107\":1,\"108\":2}}],[\"my\",{\"1\":{\"40\":2,\"218\":4}}],[\"merged\",{\"1\":{\"176\":6}}],[\"merge\",{\"1\":{\"175\":3,\"176\":5}}],[\"merges\",{\"1\":{\"175\":3}}],[\"merging\",{\"1\":{\"175\":2}}],[\"message\",{\"1\":{\"81\":2}}],[\"mesh\",{\"1\":{\"71\":1}}],[\"meta\",{\"1\":{\"278\":1}}],[\"metamind\",{\"1\":{\"0\":1}}],[\"methods\",{\"1\":{\"92\":1,\"185\":1}}],[\"metrics\",{\"1\":{\"39\":7}}],[\"metric\",{\"1\":{\"39\":1}}],[\"mean\",{\"1\":{\"35\":2,\"39\":11,\"40\":1,\"65\":1,\"102\":1,\"103\":1,\"167\":1,\"169\":2,\"172\":1,\"227\":1}}],[\"memory\",{\"1\":{\"33\":5,\"213\":1,\"262\":2,\"269\":2,\"270\":2,\"287\":1}}],[\"m\",{\"1\":{\"31\":1,\"40\":2,\"115\":1,\"137\":5,\"269\":3}}],[\"might\",{\"1\":{\"244\":1}}],[\"microsoft\",{\"1\":{\"212\":1}}],[\"microwave\",{\"1\":{\"24\":1,\"25\":1}}],[\"million\",{\"1\":{\"115\":1}}],[\"missing\",{\"1\":{\"61\":1}}],[\"mincount\",{\"1\":{\"175\":4}}],[\"min=a\",{\"1\":{\"172\":1}}],[\"min=1e\",{\"1\":{\"37\":1}}],[\"min\",{\"1\":{\"72\":1,\"93\":1,\"95\":1,\"104\":2,\"225\":5,\"254\":4}}],[\"minibatch\",{\"1\":{\"90\":2}}],[\"mini\",{\"1\":{\"49\":1,\"50\":3,\"278\":8}}],[\"miou\",{\"1\":{\"32\":1,\"39\":11}}],[\"mixed\",{\"1\":{\"103\":2,\"244\":6}}],[\"mixer\",{\"0\":{\"30\":1},\"1\":{\"30\":11,\"32\":2,\"112\":1}}],[\"mix\",{\"1\":{\"30\":6}}],[\"mixing阶段\",{\"1\":{\"32\":1}}],[\"mixing\",{\"0\":{\"30\":1},\"1\":{\"28\":1,\"30\":4,\"32\":3}}],[\"must\",{\"1\":{\"224\":1}}],[\"mul\",{\"1\":{\"35\":4}}],[\"multinli\",{\"1\":{\"203\":1,\"204\":1}}],[\"multinomial\",{\"1\":{\"102\":2}}],[\"multiply\",{\"1\":{\"113\":2}}],[\"multiple\",{\"1\":{\"55\":1,\"257\":1}}],[\"multimodal\",{\"1\":{\"81\":2,\"103\":2}}],[\"multiheadedattention\",{\"1\":{\"271\":2}}],[\"multihead\",{\"1\":{\"33\":1}}],[\"multi\",{\"0\":{\"52\":1,\"54\":1,\"220\":1},\"1\":{\"10\":2,\"11\":2,\"13\":5,\"16\":5,\"19\":1,\"51\":2,\"53\":1,\"60\":1,\"71\":1,\"112\":1,\"133\":1,\"261\":1,\"278\":1}}],[\"mug\",{\"1\":{\"20\":1,\"24\":1,\"25\":1,\"27\":1}}],[\"moe\",{\"1\":{\"278\":2}}],[\"most\",{\"0\":{\"200\":1},\"1\":{\"94\":10,\"95\":10,\"200\":2}}],[\"motorcycle\",{\"1\":{\"91\":1}}],[\"motions\",{\"1\":{\"73\":1}}],[\"move\",{\"1\":{\"19\":1,\"24\":1,\"25\":1,\"40\":2}}],[\"modal\",{\"1\":{\"101\":2}}],[\"modulelist\",{\"1\":{\"49\":2,\"53\":4,\"57\":2,\"226\":1,\"239\":1}}],[\"module\",{\"1\":{\"11\":1,\"15\":3,\"16\":1,\"27\":2,\"29\":1,\"30\":1,\"31\":1,\"32\":1,\"33\":2,\"35\":1,\"49\":1,\"50\":1,\"53\":2,\"57\":1,\"58\":1,\"64\":1,\"66\":1,\"67\":1,\"68\":1,\"102\":1,\"103\":5,\"109\":1,\"110\":2,\"111\":1,\"112\":2,\"113\":1,\"114\":1,\"166\":1,\"167\":1,\"168\":1,\"169\":1,\"170\":1,\"172\":1,\"226\":1,\"230\":1,\"236\":1,\"238\":3,\"239\":3,\"240\":1,\"244\":1,\"245\":1,\"246\":1,\"248\":1,\"249\":1,\"250\":1,\"262\":1,\"263\":1,\"265\":1,\"266\":1,\"267\":1,\"269\":1,\"270\":1,\"271\":1}}],[\"model是decoder输出的大小\",{\"1\":{\"263\":1}}],[\"modeling\",{\"1\":{\"223\":1}}],[\"models\",{\"1\":{\"76\":1,\"118\":1,\"184\":1,\"186\":1,\"198\":1,\"199\":1,\"200\":1,\"232\":1,\"292\":2}}],[\"modelnet40\",{\"1\":{\"69\":2}}],[\"model\",{\"0\":{\"218\":1},\"1\":{\"10\":3,\"12\":2,\"13\":1,\"14\":3,\"16\":1,\"27\":1,\"37\":3,\"38\":3,\"39\":8,\"40\":10,\"50\":2,\"53\":2,\"58\":2,\"78\":2,\"82\":1,\"91\":2,\"93\":16,\"95\":16,\"96\":1,\"104\":2,\"114\":2,\"118\":6,\"182\":1,\"189\":1,\"213\":1,\"217\":1,\"218\":1,\"226\":10,\"227\":11,\"232\":6,\"240\":1,\"254\":1,\"263\":2,\"271\":7,\"277\":2,\"281\":1,\"287\":1}}],[\"mode\",{\"1\":{\"10\":2,\"107\":2}}],[\"mode=false\",{\"1\":{\"10\":1}}],[\"mt\",{\"1\":{\"11\":2,\"15\":6,\"16\":3}}],[\"markdown\",{\"1\":{\"291\":1}}],[\"made\",{\"1\":{\"269\":1}}],[\"macos\",{\"1\":{\"259\":1}}],[\"macos系统某些包的加载和依赖关系上存在问题\",{\"1\":{\"8\":1}}],[\"machine\",{\"1\":{\"141\":1}}],[\"makedirs\",{\"1\":{\"223\":2}}],[\"make\",{\"1\":{\"113\":1,\"225\":1,\"227\":1}}],[\"math\",{\"1\":{\"103\":1,\"244\":1,\"271\":1,\"278\":1}}],[\"matmul\",{\"1\":{\"101\":2,\"103\":2,\"230\":2,\"244\":2,\"271\":2}}],[\"matplotlib\",{\"1\":{\"95\":1}}],[\"matching\",{\"0\":{\"102\":1},\"1\":{\"94\":11,\"95\":11,\"102\":4}}],[\"matrix\",{\"0\":{\"151\":1},\"1\":{\"39\":3,\"65\":1}}],[\"maps\",{\"1\":{\"175\":2}}],[\"map\",{\"1\":{\"40\":1,\"103\":1,\"118\":1,\"227\":1,\"235\":1}}],[\"main\",{\"1\":{\"37\":1,\"40\":1,\"97\":1,\"223\":3}}],[\"mae\",{\"1\":{\"32\":1,\"39\":16}}],[\"maybe\",{\"1\":{\"10\":1}}],[\"max=b\",{\"1\":{\"172\":1}}],[\"max=dict\",{\"1\":{\"37\":1}}],[\"max\",{\"1\":{\"10\":2,\"40\":1,\"49\":4,\"53\":1,\"62\":6,\"64\":2,\"66\":1,\"69\":7,\"72\":1,\"100\":3,\"101\":2,\"104\":2,\"114\":1,\"175\":2,\"176\":12,\"224\":1,\"225\":7,\"226\":10,\"227\":2,\"232\":1,\"233\":6,\"235\":7,\"236\":1,\"254\":4}}],[\"mask作用图解\",{\"1\":{\"233\":2}}],[\"mask模样为\",{\"1\":{\"230\":1}}],[\"mask矩阵将注意力得分矩阵中对应位置的得分设置为一个非常小的值\",{\"1\":{\"230\":1}}],[\"mask矩阵\",{\"1\":{\"230\":1}}],[\"masking\",{\"1\":{\"224\":1,\"225\":1,\"261\":1,\"270\":1}}],[\"mask部分相关的掩码逻辑\",{\"1\":{\"113\":1}}],[\"mask和casual\",{\"1\":{\"113\":1}}],[\"mask方法中\",{\"1\":{\"103\":1}}],[\"masked\",{\"0\":{\"218\":1},\"1\":{\"103\":1,\"217\":1,\"218\":2,\"224\":8,\"225\":6,\"226\":12,\"227\":4,\"230\":1,\"251\":5,\"262\":1,\"271\":1}}],[\"mask标注哪些image\",{\"1\":{\"100\":1}}],[\"mask=mask\",{\"1\":{\"271\":1}}],[\"mask=memory\",{\"1\":{\"33\":2}}],[\"mask=encoder\",{\"1\":{\"103\":1}}],[\"mask=head\",{\"1\":{\"103\":1,\"242\":1,\"251\":1,\"256\":1,\"257\":1}}],[\"mask=attention\",{\"1\":{\"102\":1,\"103\":2,\"233\":1,\"242\":1,\"251\":1,\"253\":1,\"254\":1,\"256\":1,\"257\":1}}],[\"mask=image\",{\"1\":{\"100\":1,\"102\":1,\"103\":1}}],[\"mask=q\",{\"1\":{\"32\":1}}],[\"mask=none\",{\"1\":{\"29\":1,\"31\":1,\"32\":1,\"40\":1,\"103\":12,\"238\":1,\"239\":2,\"241\":2,\"242\":2,\"244\":2,\"246\":1,\"251\":2,\"254\":2,\"256\":2,\"257\":2,\"271\":2}}],[\"mask=text\",{\"1\":{\"100\":1}}],[\"mask=tgt\",{\"1\":{\"33\":2}}],[\"mask=t\",{\"1\":{\"27\":1,\"33\":1}}],[\"mask=llm\",{\"1\":{\"10\":1}}],[\"mask\",{\"0\":{\"21\":2,\"35\":1,\"230\":1},\"1\":{\"10\":5,\"13\":13,\"21\":3,\"25\":6,\"27\":4,\"29\":1,\"31\":3,\"32\":3,\"33\":12,\"35\":4,\"38\":2,\"39\":19,\"40\":9,\"49\":6,\"100\":2,\"101\":2,\"102\":10,\"103\":29,\"104\":1,\"113\":2,\"166\":2,\"167\":3,\"168\":1,\"218\":3,\"224\":6,\"225\":1,\"226\":5,\"230\":7,\"233\":14,\"235\":4,\"238\":1,\"239\":2,\"241\":8,\"242\":3,\"244\":3,\"246\":1,\"251\":2,\"253\":1,\"254\":1,\"256\":4,\"257\":6,\"261\":1,\"262\":11,\"266\":2,\"267\":3,\"269\":4,\"270\":4,\"271\":6}}],[\"lcel\",{\"1\":{\"288\":2,\"289\":1}}],[\"lstm会掉5\",{\"1\":{\"213\":1}}],[\"lstm表现高方差\",{\"1\":{\"213\":1}}],[\"ltm的过程包含两个阶段\",{\"1\":{\"200\":1}}],[\"ltm的核心思想是\",{\"1\":{\"200\":1}}],[\"ltm\",{\"1\":{\"200\":1}}],[\"l为灰度图片\",{\"1\":{\"107\":1}}],[\"lm中同样含有pad部分\",{\"1\":{\"227\":1}}],[\"lm也只包含被掩码的token对应的模型预测真实词\",{\"1\":{\"227\":1}}],[\"lm\",{\"1\":{\"103\":7,\"218\":1,\"226\":3,\"227\":6,\"251\":5}}],[\"lmaffordance3d\",{\"0\":{\"7\":1,\"10\":1},\"1\":{\"7\":1,\"10\":1,\"141\":1}}],[\"ln\",{\"1\":{\"100\":1,\"104\":1}}],[\"luggage\",{\"1\":{\"80\":1}}],[\"llama3\",{\"1\":{\"278\":4}}],[\"llama2\",{\"1\":{\"278\":3}}],[\"llama\",{\"0\":{\"272\":1},\"1\":{\"78\":1,\"80\":1,\"81\":1,\"174\":1,\"277\":1,\"278\":16}}],[\"llava模型结构\",{\"1\":{\"80\":1}}],[\"llava\",{\"0\":{\"77\":1},\"1\":{\"77\":2,\"78\":1,\"80\":1,\"81\":4,\"82\":4,\"85\":1}}],[\"llm就像是一个快思考的系统\",{\"1\":{\"197\":1}}],[\"llm是一个概率模型\",{\"1\":{\"194\":1}}],[\"llm已经这么强了\",{\"1\":{\"194\":1}}],[\"llms\",{\"1\":{\"185\":1}}],[\"llm\",{\"0\":{\"279\":1},\"1\":{\"3\":1,\"10\":12,\"12\":3,\"13\":14,\"14\":4,\"15\":1,\"26\":1,\"85\":1,\"98\":1,\"104\":4,\"277\":7,\"278\":2,\"279\":9,\"280\":6,\"282\":4,\"283\":10,\"285\":2,\"287\":2,\"288\":9,\"289\":1,\"292\":1,\"293\":1}}],[\"l0\",{\"1\":{\"58\":8}}],[\"l4\",{\"1\":{\"58\":6}}],[\"l3\",{\"1\":{\"50\":4,\"53\":4,\"58\":9}}],[\"l2\",{\"1\":{\"50\":4,\"53\":4,\"58\":9,\"90\":2,\"211\":1}}],[\"l1\",{\"1\":{\"50\":4,\"53\":4,\"58\":9}}],[\"lr=1e\",{\"1\":{\"227\":1}}],[\"lr=6\",{\"1\":{\"211\":1}}],[\"lr=2\",{\"1\":{\"211\":1}}],[\"lr=dict\",{\"1\":{\"37\":1}}],[\"lr\",{\"1\":{\"37\":4,\"40\":1,\"211\":1,\"227\":1}}],[\"l×d\",{\"1\":{\"29\":1,\"30\":1}}],[\"lite\",{\"1\":{\"278\":2}}],[\"like\",{\"0\":{\"101\":1,\"103\":1},\"1\":{\"236\":1}}],[\"liu\",{\"1\":{\"77\":1}}],[\"lin\",{\"1\":{\"271\":2}}],[\"lineids\",{\"1\":{\"176\":5}}],[\"line\",{\"1\":{\"176\":9,\"223\":8}}],[\"lines\",{\"1\":{\"176\":2}}],[\"line>\",{\"1\":{\"175\":1,\"176\":1}}],[\"linears\",{\"1\":{\"271\":3}}],[\"linear1\",{\"1\":{\"33\":1}}],[\"linear2\",{\"1\":{\"33\":1}}],[\"linear\",{\"1\":{\"12\":2,\"14\":2,\"15\":5,\"16\":2,\"30\":4,\"50\":3,\"53\":3,\"64\":3,\"67\":3,\"112\":2,\"113\":2,\"114\":2,\"221\":3,\"226\":3,\"238\":2,\"240\":1,\"242\":1,\"244\":3,\"245\":1,\"248\":1,\"249\":1,\"250\":1,\"253\":1,\"254\":1,\"256\":1,\"257\":1,\"263\":1,\"271\":3}}],[\"linalg\",{\"1\":{\"93\":2,\"95\":2}}],[\"linguistic\",{\"1\":{\"76\":1,\"212\":1}}],[\"linspace\",{\"1\":{\"39\":2}}],[\"lightgroupattnblock\",{\"1\":{\"29\":1}}],[\"lift\",{\"1\":{\"19\":1,\"24\":1,\"25\":1}}],[\"li\",{\"1\":{\"17\":1}}],[\"list列表组装起来得到需要的dataset\",{\"1\":{\"233\":1}}],[\"listdir\",{\"1\":{\"93\":1,\"95\":1,\"107\":2}}],[\"listen\",{\"1\":{\"25\":1}}],[\"list\",{\"1\":{\"13\":1,\"27\":1,\"40\":2,\"49\":2,\"53\":15,\"107\":2,\"143\":1,\"146\":1,\"175\":4,\"176\":7,\"224\":6}}],[\"low\",{\"1\":{\"184\":1,\"186\":1,\"188\":2}}],[\"lower\",{\"1\":{\"25\":1,\"93\":1,\"95\":1,\"232\":1}}],[\"lora微调\",{\"1\":{\"187\":1}}],[\"lora的基本思路\",{\"1\":{\"184\":1}}],[\"lora背后有一个假设\",{\"1\":{\"184\":1}}],[\"lora是跟prompt\",{\"1\":{\"184\":1}}],[\"lora\",{\"0\":{\"184\":1,\"189\":1},\"1\":{\"185\":1,\"186\":1,\"188\":5,\"189\":9,\"192\":7}}],[\"lovaszsoftmax\",{\"1\":{\"171\":1}}],[\"lovasz\",{\"0\":{\"171\":1},\"1\":{\"171\":1}}],[\"looking\",{\"1\":{\"91\":1}}],[\"loal\",{\"1\":{\"43\":1}}],[\"loaded\",{\"1\":{\"227\":1}}],[\"loader\",{\"1\":{\"37\":3,\"38\":2,\"39\":2,\"108\":2,\"114\":2}}],[\"loading\",{\"1\":{\"93\":1,\"95\":1}}],[\"load\",{\"1\":{\"25\":2,\"40\":4,\"118\":2,\"224\":2,\"227\":2}}],[\"logging\",{\"1\":{\"232\":1}}],[\"logger\",{\"1\":{\"39\":2}}],[\"log\",{\"1\":{\"35\":3,\"40\":2,\"50\":1,\"53\":1,\"58\":1,\"67\":2,\"68\":3,\"172\":4,\"263\":1}}],[\"logits外\",{\"1\":{\"118\":1}}],[\"logits=prediction\",{\"1\":{\"103\":1}}],[\"logits=false\",{\"1\":{\"103\":1,\"118\":1}}],[\"logits\",{\"1\":{\"38\":1,\"55\":1,\"90\":3,\"102\":2,\"103\":2,\"114\":3,\"118\":3,\"166\":2,\"167\":1,\"170\":2,\"226\":6,\"227\":4,\"242\":5,\"253\":12,\"254\":15,\"256\":6,\"257\":6}}],[\"logit\",{\"1\":{\"35\":1,\"168\":1,\"169\":1}}],[\"locality\",{\"1\":{\"105\":1}}],[\"local\",{\"1\":{\"43\":3,\"49\":2,\"93\":2,\"95\":2,\"235\":1}}],[\"location=device\",{\"1\":{\"118\":1,\"227\":1}}],[\"location=\",{\"1\":{\"40\":1}}],[\"loc\",{\"1\":{\"25\":1}}],[\"longterm\",{\"1\":{\"223\":1}}],[\"longtensor\",{\"1\":{\"104\":1,\"227\":1}}],[\"long\",{\"1\":{\"13\":1,\"49\":5,\"100\":1,\"102\":4,\"103\":1,\"104\":1,\"223\":1,\"236\":1}}],[\"longest\",{\"1\":{\"10\":1}}],[\"loss中的alpha和beta\",{\"1\":{\"173\":1}}],[\"loss等能够处理不平衡情况的损失函数\",{\"1\":{\"173\":1}}],[\"loss或combo\",{\"1\":{\"173\":1}}],[\"loss的设计思想是\",{\"1\":{\"171\":1}}],[\"loss的设计灵感来自tversky指数\",{\"1\":{\"170\":1}}],[\"loss引入了一个衰减因子\",{\"1\":{\"169\":1}}],[\"loss和标准的二元交叉熵\",{\"1\":{\"167\":1}}],[\"loss是将dice\",{\"1\":{\"167\":1}}],[\"loss=lm\",{\"1\":{\"103\":1}}],[\"loss\",{\"0\":{\"35\":2,\"101\":1,\"102\":1,\"103\":1,\"166\":1,\"167\":1,\"168\":1,\"169\":1,\"170\":1,\"171\":1,\"172\":1},\"1\":{\"10\":8,\"35\":25,\"37\":1,\"38\":8,\"39\":1,\"65\":2,\"90\":8,\"101\":2,\"102\":1,\"103\":10,\"114\":3,\"166\":10,\"167\":21,\"168\":21,\"169\":38,\"170\":5,\"172\":22,\"218\":2,\"220\":1,\"227\":23,\"242\":8,\"251\":10,\"254\":10,\"256\":11,\"257\":5}}],[\"least\",{\"0\":{\"200\":1},\"1\":{\"200\":2}}],[\"learned\",{\"1\":{\"90\":3,\"100\":2}}],[\"learner\",{\"1\":{\"43\":2}}],[\"learning的核心思想是通过设计合适的prompt\",{\"1\":{\"92\":1}}],[\"learning或prompt\",{\"1\":{\"92\":1}}],[\"learning\",{\"0\":{\"100\":1,\"101\":1,\"104\":1,\"220\":1},\"1\":{\"78\":1,\"96\":2,\"100\":1,\"104\":1,\"180\":2,\"232\":1}}],[\"learnable\",{\"1\":{\"33\":2,\"85\":1,\"111\":1}}],[\"let\",{\"1\":{\"197\":1}}],[\"level\",{\"1\":{\"54\":1}}],[\"lens\",{\"1\":{\"233\":1,\"235\":2}}],[\"len=input\",{\"1\":{\"233\":1}}],[\"len的维度拼接起来\",{\"1\":{\"103\":1}}],[\"len维度上拼接起来\",{\"1\":{\"102\":2}}],[\"len和hidden\",{\"1\":{\"100\":1}}],[\"length长度\",{\"1\":{\"233\":1}}],[\"length=128\",{\"1\":{\"232\":1}}],[\"length=10\",{\"1\":{\"104\":1}}],[\"length=min\",{\"1\":{\"104\":1}}],[\"length=max\",{\"1\":{\"104\":1}}],[\"length=30\",{\"1\":{\"104\":1}}],[\"length=0\",{\"1\":{\"102\":1,\"103\":2}}],[\"length=self\",{\"1\":{\"10\":1,\"100\":1}}],[\"length\",{\"1\":{\"13\":4,\"15\":1,\"100\":1,\"102\":5,\"103\":3,\"104\":2,\"233\":11,\"235\":1,\"236\":2,\"253\":4,\"257\":4}}],[\"len\",{\"1\":{\"10\":1,\"27\":1,\"38\":3,\"40\":2,\"49\":1,\"53\":2,\"93\":1,\"95\":1,\"100\":7,\"101\":5,\"102\":10,\"103\":1,\"107\":8,\"175\":2,\"176\":3,\"223\":2,\"224\":9,\"225\":18,\"226\":9,\"227\":2,\"230\":10,\"233\":13,\"235\":5,\"244\":2,\"254\":4,\"256\":2,\"271\":1}}],[\"left\",{\"1\":{\"10\":1,\"266\":1}}],[\"langserve\",{\"1\":{\"289\":1}}],[\"langsmith\",{\"1\":{\"288\":1,\"289\":1}}],[\"langchian\",{\"1\":{\"287\":1}}],[\"langchain\",{\"0\":{\"286\":1},\"1\":{\"286\":6,\"287\":1,\"288\":13,\"289\":10,\"291\":3}}],[\"language生成学习\",{\"1\":{\"99\":1}}],[\"language表征学习\",{\"1\":{\"99\":1}}],[\"language\",{\"0\":{\"41\":1,\"77\":1,\"218\":1},\"1\":{\"7\":1,\"10\":1,\"13\":1,\"16\":1,\"17\":3,\"27\":1,\"39\":1,\"77\":1,\"89\":1,\"92\":1,\"96\":1,\"184\":1,\"185\":1,\"186\":1,\"189\":1,\"198\":1,\"199\":1,\"200\":1,\"202\":2,\"217\":1,\"218\":1,\"223\":1,\"277\":2,\"292\":2}}],[\"latent\",{\"1\":{\"278\":1}}],[\"latency\",{\"1\":{\"189\":1}}],[\"law的未来\",{\"1\":{\"292\":1}}],[\"law\",{\"1\":{\"277\":7,\"292\":1}}],[\"lambda\",{\"1\":{\"266\":1,\"269\":2}}],[\"lavis\",{\"1\":{\"97\":1,\"148\":1}}],[\"launchpad\",{\"1\":{\"91\":1}}],[\"large\",{\"0\":{\"77\":1},\"1\":{\"77\":1,\"93\":2,\"95\":1,\"96\":1,\"184\":1,\"186\":1,\"198\":1,\"200\":1,\"277\":1,\"292\":2}}],[\"last\",{\"1\":{\"49\":3,\"53\":3,\"57\":3,\"100\":2,\"102\":1,\"103\":1}}],[\"last=true\",{\"1\":{\"37\":1}}],[\"laso\",{\"0\":{\"17\":1},\"1\":{\"17\":4,\"19\":1,\"25\":1,\"26\":1,\"27\":1,\"28\":1,\"33\":1,\"35\":2,\"39\":3,\"40\":2}}],[\"layer=act\",{\"1\":{\"112\":1,\"114\":1}}],[\"layer=norm\",{\"1\":{\"114\":1}}],[\"layer=none\",{\"1\":{\"109\":1,\"110\":1,\"111\":3,\"114\":3}}],[\"layer=nn\",{\"1\":{\"112\":3}}],[\"layer的任务是通过中心点找到邻居点\",{\"1\":{\"47\":1}}],[\"layers\",{\"1\":{\"45\":1,\"55\":2,\"103\":1,\"226\":3,\"227\":1,\"239\":1,\"244\":1,\"267\":3,\"270\":2}}],[\"layers组成\",{\"1\":{\"45\":1}}],[\"layers主要包括3个部分\",{\"1\":{\"44\":1}}],[\"layer\",{\"0\":{\"46\":1,\"47\":1,\"48\":1},\"1\":{\"29\":1,\"32\":2,\"44\":3,\"45\":6,\"56\":6,\"103\":36,\"109\":3,\"110\":2,\"111\":1,\"112\":4,\"114\":8,\"226\":3,\"236\":1,\"238\":3,\"239\":4,\"244\":21,\"245\":1,\"248\":1,\"261\":1,\"267\":6,\"270\":6}}],[\"layer指的是pointnet++中提供的pointnetfeaturepropagation特征传播类\",{\"1\":{\"27\":1}}],[\"layer指的是pointnet++中提供的pointnetsetabstractionmsg多尺度分组点集特征抽取类\",{\"1\":{\"27\":1}}],[\"layernorm\",{\"1\":{\"15\":6,\"30\":2,\"102\":1,\"112\":1,\"114\":2,\"211\":1,\"236\":2,\"238\":2,\"245\":2,\"248\":2,\"265\":2,\"267\":1,\"270\":1}}],[\"lay\",{\"1\":{\"25\":1}}],[\"laptop\",{\"1\":{\"25\":1}}],[\"label=none\",{\"1\":{\"251\":1}}],[\"label=label\",{\"1\":{\"233\":1}}],[\"labels=none\",{\"1\":{\"103\":1,\"242\":1,\"251\":1,\"256\":1,\"257\":1}}],[\"labels=labels\",{\"1\":{\"103\":1}}],[\"labels\",{\"1\":{\"19\":1,\"55\":2,\"56\":1,\"90\":3,\"91\":1,\"102\":2,\"103\":8,\"107\":4,\"114\":3,\"233\":1,\"235\":2,\"242\":8,\"251\":2,\"254\":3,\"256\":11,\"257\":2}}],[\"label\",{\"1\":{\"10\":3,\"21\":1,\"38\":1,\"39\":4,\"91\":1,\"92\":2,\"101\":2,\"103\":1,\"107\":8,\"108\":4,\"169\":1,\"227\":2,\"233\":2,\"251\":2}}],[\"l\",{\"1\":{\"4\":1,\"15\":5,\"27\":2,\"29\":1,\"30\":2,\"31\":1,\"32\":1,\"33\":12,\"62\":1,\"80\":1,\"81\":1,\"90\":5,\"93\":1,\"232\":3}}],[\"1m\",{\"1\":{\"278\":5}}],[\"1=none\",{\"1\":{\"233\":3}}],[\"18\",{\"1\":{\"211\":1}}],[\"1b\",{\"1\":{\"211\":1}}],[\"1rkdjdlr37o7gsr9j1mhjbg\",{\"1\":{\"118\":1}}],[\"137mo\",{\"1\":{\"107\":1}}],[\"14b\",{\"1\":{\"278\":1}}],[\"14模型\",{\"1\":{\"90\":1}}],[\"14则需要在256个v100\",{\"1\":{\"90\":1}}],[\"14\",{\"1\":{\"80\":1,\"81\":1,\"90\":2,\"93\":1,\"109\":2,\"111\":2}}],[\"14238\",{\"1\":{\"76\":1}}],[\"1d\",{\"1\":{\"64\":1}}],[\"1e9\",{\"1\":{\"230\":1,\"271\":1}}],[\"1e10\",{\"1\":{\"49\":2}}],[\"1e\",{\"1\":{\"35\":7,\"40\":1,\"57\":1,\"114\":1,\"227\":1}}],[\"1x1\",{\"1\":{\"30\":1}}],[\"1️⃣\",{\"0\":{\"29\":1},\"1\":{\"57\":1}}],[\"12597\",{\"1\":{\"97\":1}}],[\"12\",{\"1\":{\"39\":1,\"211\":2,\"232\":6,\"278\":3}}],[\"128k\",{\"1\":{\"278\":9}}],[\"128\",{\"1\":{\"27\":1,\"30\":1,\"50\":5,\"53\":10,\"55\":4,\"56\":4,\"57\":1,\"58\":12,\"64\":4,\"66\":3,\"68\":3,\"253\":1}}],[\"1200000000\",{\"1\":{\"115\":1}}],[\"120\",{\"1\":{\"22\":1}}],[\"16k\",{\"1\":{\"278\":1}}],[\"16×16\",{\"1\":{\"118\":1}}],[\"16这个模型进行微调\",{\"1\":{\"118\":1}}],[\"16x16\",{\"1\":{\"108\":1}}],[\"16为例\",{\"1\":{\"108\":1,\"109\":1}}],[\"16和vit\",{\"1\":{\"90\":1}}],[\"16倍和64倍得到的\",{\"1\":{\"90\":1}}],[\"1612\",{\"1\":{\"59\":1}}],[\"16\",{\"1\":{\"22\":1,\"30\":1,\"40\":1,\"53\":1,\"58\":1,\"108\":2,\"109\":3,\"118\":1,\"288\":1}}],[\"1750\",{\"1\":{\"277\":1,\"278\":1}}],[\"175b\",{\"1\":{\"277\":1}}],[\"1706\",{\"1\":{\"42\":1}}],[\"174\",{\"1\":{\"22\":1}}],[\"17\",{\"1\":{\"22\":1,\"24\":1,\"32\":1}}],[\"197\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"196+1\",{\"1\":{\"111\":1}}],[\"196\",{\"1\":{\"109\":2,\"110\":1,\"111\":1,\"114\":1}}],[\"19626\",{\"1\":{\"4\":1}}],[\"190\",{\"1\":{\"40\":3}}],[\"19\",{\"1\":{\"22\":1,\"24\":1,\"26\":1}}],[\"158\",{\"1\":{\"81\":1}}],[\"158k\",{\"1\":{\"81\":2}}],[\"15\",{\"1\":{\"20\":1,\"25\":1,\"40\":1,\"83\":1,\"218\":1,\"277\":1,\"278\":3}}],[\"119\",{\"1\":{\"278\":1}}],[\"11929\",{\"1\":{\"118\":1}}],[\"110\",{\"1\":{\"32\":1}}],[\"11\",{\"1\":{\"10\":1,\"39\":1,\"83\":1,\"232\":1,\"259\":2,\"278\":4}}],[\"102\",{\"1\":{\"233\":2}}],[\"1024+64\",{\"1\":{\"66\":1}}],[\"1024维\",{\"1\":{\"64\":1,\"66\":1,\"67\":2}}],[\"1024\",{\"1\":{\"50\":3,\"53\":3,\"56\":2,\"57\":1,\"58\":3,\"64\":9,\"66\":7,\"67\":1,\"68\":2}}],[\"101\",{\"1\":{\"233\":2}}],[\"104\",{\"1\":{\"233\":1}}],[\"103是超过\",{\"1\":{\"223\":1}}],[\"103两个版本\",{\"1\":{\"223\":1}}],[\"103\",{\"1\":{\"223\":5,\"224\":1,\"233\":1}}],[\"1035\",{\"1\":{\"22\":1}}],[\"1000\",{\"1\":{\"169\":1}}],[\"10000\",{\"1\":{\"102\":3,\"241\":1}}],[\"100\",{\"1\":{\"80\":2,\"91\":1,\"93\":1,\"95\":1,\"103\":2,\"152\":2,\"153\":1,\"160\":1,\"169\":1,\"211\":1,\"227\":1,\"233\":2,\"278\":2}}],[\"1088\",{\"1\":{\"66\":2,\"68\":1}}],[\"10\",{\"0\":{\"16\":1},\"1\":{\"10\":1,\"20\":1,\"39\":2,\"218\":2,\"224\":4,\"225\":2,\"271\":3,\"278\":9}}],[\"1\",{\"0\":{\"19\":1,\"76\":1,\"77\":1,\"100\":1,\"101\":1,\"108\":1,\"125\":1,\"130\":1,\"202\":1,\"276\":1},\"1\":{\"10\":4,\"11\":10,\"13\":3,\"15\":14,\"16\":16,\"21\":1,\"22\":1,\"25\":1,\"27\":29,\"30\":2,\"32\":2,\"33\":14,\"35\":13,\"37\":1,\"38\":2,\"39\":12,\"40\":10,\"49\":34,\"50\":3,\"53\":9,\"55\":2,\"56\":2,\"57\":17,\"58\":4,\"62\":1,\"64\":12,\"65\":4,\"66\":13,\"68\":8,\"69\":1,\"76\":1,\"77\":1,\"83\":2,\"88\":1,\"91\":5,\"93\":4,\"95\":4,\"99\":1,\"100\":7,\"101\":13,\"102\":15,\"103\":20,\"104\":7,\"107\":2,\"108\":2,\"109\":5,\"110\":10,\"111\":10,\"113\":16,\"114\":10,\"115\":2,\"118\":2,\"152\":2,\"153\":1,\"154\":1,\"155\":2,\"157\":2,\"159\":3,\"160\":5,\"162\":1,\"166\":7,\"167\":7,\"168\":10,\"169\":11,\"170\":7,\"172\":12,\"175\":12,\"176\":11,\"196\":1,\"202\":1,\"208\":1,\"209\":1,\"211\":2,\"217\":1,\"218\":1,\"219\":9,\"221\":1,\"223\":2,\"224\":1,\"225\":9,\"226\":5,\"227\":8,\"230\":4,\"233\":26,\"235\":1,\"236\":2,\"241\":2,\"242\":6,\"244\":5,\"251\":5,\"253\":6,\"254\":5,\"255\":2,\"256\":6,\"257\":15,\"259\":1,\"263\":1,\"266\":2,\"269\":2,\"271\":15,\"278\":18}}],[\"1asow2t2mltykaia\",{\"1\":{\"4\":1}}],[\"1n\",{\"1\":{\"4\":1}}],[\"s型智能增长曲线\",{\"1\":{\"292\":1}}],[\"sft\",{\"1\":{\"278\":1}}],[\"swiglu\",{\"1\":{\"278\":4}}],[\"src\",{\"1\":{\"262\":16,\"269\":7,\"270\":2}}],[\"srl\",{\"1\":{\"256\":1}}],[\"s3\",{\"1\":{\"223\":2}}],[\"s>e\",{\"1\":{\"221\":1}}],[\"s=2\",{\"1\":{\"221\":2}}],[\"s=str\",{\"1\":{\"107\":1}}],[\"sst\",{\"1\":{\"212\":4}}],[\"ssg\",{\"1\":{\"50\":3}}],[\"sørensen\",{\"1\":{\"166\":1}}],[\"smooth\",{\"1\":{\"166\":4,\"167\":3,\"168\":3,\"169\":1,\"170\":3,\"172\":3}}],[\"smooth=1\",{\"1\":{\"166\":1,\"167\":1,\"168\":1,\"169\":1,\"170\":1,\"172\":1}}],[\"smoothing=0\",{\"1\":{\"101\":2,\"103\":1,\"227\":2}}],[\"small\",{\"1\":{\"39\":1}}],[\"snapshot\",{\"1\":{\"93\":1,\"95\":2}}],[\"symb\",{\"1\":{\"224\":2}}],[\"symbols\",{\"1\":{\"175\":4}}],[\"symlinks=false\",{\"1\":{\"93\":1,\"95\":1}}],[\"symmetric\",{\"1\":{\"62\":1,\"72\":1}}],[\"systematic\",{\"1\":{\"92\":1,\"185\":1}}],[\"sys\",{\"1\":{\"40\":3,\"238\":1,\"248\":1}}],[\"squad\",{\"1\":{\"255\":1}}],[\"square\",{\"1\":{\"49\":1,\"57\":1}}],[\"squeeze\",{\"1\":{\"27\":1,\"40\":1,\"101\":2,\"253\":2,\"254\":2}}],[\"sqrdists\",{\"1\":{\"49\":2}}],[\"sqrt\",{\"1\":{\"40\":1,\"103\":1,\"244\":1,\"271\":1}}],[\"shuffle\",{\"1\":{\"225\":3}}],[\"shuffle=false\",{\"1\":{\"37\":2,\"108\":1}}],[\"shuffle=true\",{\"1\":{\"37\":1,\"108\":1,\"227\":1}}],[\"shifted\",{\"1\":{\"103\":3}}],[\"show\",{\"1\":{\"94\":1,\"95\":1,\"107\":1}}],[\"shot图文生成\",{\"1\":{\"99\":1}}],[\"shot图像分类\",{\"1\":{\"91\":1}}],[\"shot性能评估\",{\"1\":{\"96\":1}}],[\"shot性能\",{\"1\":{\"96\":1}}],[\"shot迁移到下游任务\",{\"1\":{\"96\":1}}],[\"shot学习\",{\"1\":{\"96\":1}}],[\"shot推理\",{\"1\":{\"93\":1}}],[\"shot分类时\",{\"1\":{\"92\":1}}],[\"shot分类的过程相当直接\",{\"1\":{\"91\":1}}],[\"shot\",{\"1\":{\"78\":1,\"98\":1,\"204\":1}}],[\"should\",{\"1\":{\"40\":3}}],[\"shared\",{\"1\":{\"226\":4}}],[\"shape为\",{\"1\":{\"109\":1}}],[\"shape的形状从\",{\"1\":{\"49\":1}}],[\"shape\",{\"1\":{\"10\":8,\"25\":6,\"39\":11,\"40\":2,\"49\":15,\"50\":1,\"53\":2,\"55\":1,\"57\":2,\"64\":3,\"66\":3,\"68\":4,\"100\":1,\"102\":1,\"103\":2,\"104\":1,\"109\":1,\"110\":1,\"111\":1,\"113\":1,\"114\":2,\"133\":1,\"169\":1,\"226\":7,\"244\":4,\"257\":1}}],[\"shao\",{\"1\":{\"4\":1}}],[\"sonnet\",{\"1\":{\"278\":4}}],[\"sometimes\",{\"1\":{\"254\":1}}],[\"southampton\",{\"1\":{\"232\":1}}],[\"source\",{\"1\":{\"118\":1,\"148\":1}}],[\"sol\",{\"1\":{\"176\":2}}],[\"solely\",{\"1\":{\"35\":1}}],[\"sota\",{\"1\":{\"69\":1,\"81\":1,\"82\":1}}],[\"sorted\",{\"1\":{\"25\":1,\"175\":1}}],[\"sort\",{\"1\":{\"25\":5,\"49\":1,\"57\":1,\"107\":1}}],[\"soft\",{\"1\":{\"21\":1,\"35\":2,\"39\":10,\"166\":1,\"167\":2,\"168\":1,\"169\":1}}],[\"softmax归一化得到注意力概率\",{\"1\":{\"103\":1}}],[\"softmax\",{\"1\":{\"11\":5,\"15\":3,\"50\":1,\"53\":1,\"55\":2,\"58\":2,\"67\":2,\"68\":2,\"72\":2,\"91\":1,\"102\":2,\"103\":1,\"113\":1,\"126\":1,\"136\":1,\"137\":1,\"169\":2,\"209\":1,\"221\":1,\"230\":1,\"244\":1,\"253\":3,\"261\":1,\"263\":1,\"271\":1}}],[\"same\",{\"1\":{\"249\":1,\"271\":1}}],[\"sampling=false\",{\"1\":{\"104\":1}}],[\"sampling\",{\"0\":{\"46\":1},\"1\":{\"45\":2,\"46\":1,\"49\":1,\"55\":1,\"104\":2}}],[\"sampler=train\",{\"1\":{\"235\":1}}],[\"sampler\",{\"1\":{\"235\":2}}],[\"sample=use\",{\"1\":{\"104\":1}}],[\"sampled\",{\"1\":{\"49\":6,\"53\":1}}],[\"samples\",{\"1\":{\"39\":1,\"49\":1,\"100\":3,\"104\":2}}],[\"sample\",{\"1\":{\"16\":6,\"27\":11,\"33\":7,\"44\":1,\"49\":14,\"53\":2,\"107\":1,\"114\":1,\"225\":7}}],[\"salesforce\",{\"1\":{\"97\":1,\"223\":1}}],[\"saucer\",{\"1\":{\"91\":1}}],[\"sa4\",{\"1\":{\"58\":2}}],[\"sa3\",{\"1\":{\"50\":2,\"53\":2,\"58\":2}}],[\"sa\",{\"1\":{\"50\":1,\"56\":3,\"58\":1}}],[\"saved\",{\"1\":{\"224\":3,\"227\":1}}],[\"save\",{\"1\":{\"39\":2,\"93\":3,\"95\":3,\"103\":3,\"227\":1,\"232\":1}}],[\"sa1\",{\"1\":{\"16\":1,\"27\":1,\"50\":2,\"53\":2,\"58\":2}}],[\"sa2\",{\"1\":{\"16\":1,\"27\":1,\"50\":2,\"53\":2,\"58\":2}}],[\"s\",{\"1\":{\"15\":4,\"49\":16,\"53\":8,\"57\":11,\"62\":1,\"107\":1,\"118\":1,\"175\":2,\"197\":1,\"221\":2,\"233\":1}}],[\"science\",{\"1\":{\"81\":1}}],[\"scienceqa\",{\"1\":{\"81\":2,\"82\":1}}],[\"scissors\",{\"1\":{\"20\":2,\"25\":1}}],[\"scalar\",{\"1\":{\"169\":1}}],[\"scaling\",{\"1\":{\"76\":1,\"185\":1,\"277\":7}}],[\"scaleddotproductattention\",{\"1\":{\"230\":1}}],[\"scaled\",{\"1\":{\"123\":1,\"271\":1}}],[\"scale未指定\",{\"1\":{\"113\":1}}],[\"scale=qk\",{\"1\":{\"112\":1,\"114\":1}}],[\"scale=none\",{\"1\":{\"111\":1,\"112\":1,\"113\":1,\"114\":1}}],[\"scales\",{\"1\":{\"55\":1}}],[\"scale\",{\"0\":{\"52\":1},\"1\":{\"11\":2,\"15\":3,\"40\":2,\"50\":1,\"51\":1,\"53\":1,\"112\":1,\"113\":3,\"114\":1,\"182\":1,\"185\":1}}],[\"scheduler\",{\"1\":{\"37\":4}}],[\"scores\",{\"1\":{\"103\":20,\"230\":4,\"244\":12,\"250\":2,\"251\":4,\"253\":4,\"256\":1,\"271\":4}}],[\"score\",{\"1\":{\"16\":3,\"35\":1,\"39\":5,\"78\":1,\"101\":2,\"102\":2,\"166\":2,\"167\":2,\"168\":4,\"172\":4,\"250\":2,\"251\":4}}],[\"scene\",{\"1\":{\"15\":8}}],[\"sv\",{\"1\":{\"15\":2}}],[\"skip\",{\"1\":{\"55\":1,\"57\":1,\"104\":1,\"255\":2}}],[\"sk\",{\"1\":{\"15\":2}}],[\"sunflowers\",{\"1\":{\"94\":1,\"95\":1}}],[\"survey\",{\"1\":{\"92\":1,\"185\":1,\"292\":2}}],[\"suppose\",{\"1\":{\"230\":1}}],[\"supported\",{\"1\":{\"107\":2}}],[\"support\",{\"1\":{\"25\":1}}],[\"supervised\",{\"1\":{\"78\":1,\"96\":1,\"180\":1}}],[\"super\",{\"1\":{\"11\":1,\"15\":2,\"16\":1,\"30\":1,\"35\":1,\"49\":1,\"50\":1,\"53\":2,\"57\":1,\"58\":1,\"64\":1,\"66\":1,\"67\":1,\"68\":1,\"109\":1,\"110\":1,\"111\":1,\"112\":2,\"113\":1,\"114\":1,\"166\":1,\"167\":1,\"168\":1,\"169\":1,\"170\":1,\"172\":1,\"226\":1,\"236\":1,\"238\":3,\"239\":1,\"240\":1,\"241\":1,\"242\":1,\"244\":1,\"245\":1,\"246\":1,\"248\":1,\"249\":1,\"250\":1,\"251\":1,\"254\":1,\"256\":1,\"257\":1,\"262\":1,\"263\":1,\"265\":1,\"266\":1,\"267\":1,\"269\":1,\"270\":1,\"271\":1}}],[\"successfully\",{\"1\":{\"93\":1,\"95\":1}}],[\"successful\",{\"1\":{\"20\":1}}],[\"sublayer是传入的参数\",{\"1\":{\"265\":1}}],[\"sublayer\",{\"1\":{\"265\":3,\"266\":3,\"269\":4}}],[\"sublayerconnection模型结构图\",{\"1\":{\"265\":1}}],[\"sublayerconnection\",{\"0\":{\"265\":1},\"1\":{\"265\":2,\"266\":1,\"269\":1}}],[\"sub\",{\"1\":{\"15\":8,\"93\":5,\"95\":5,\"175\":1}}],[\"summation\",{\"1\":{\"33\":1,\"72\":1}}],[\"sum\",{\"1\":{\"13\":1,\"27\":2,\"33\":4,\"35\":6,\"38\":1,\"39\":3,\"40\":1,\"49\":1,\"57\":2,\"72\":1,\"103\":1,\"107\":1,\"114\":1,\"166\":3,\"167\":9,\"168\":2,\"170\":3,\"172\":3}}],[\"segments\",{\"1\":{\"226\":3}}],[\"segment\",{\"1\":{\"219\":1,\"225\":3,\"226\":1,\"227\":2,\"233\":1}}],[\"segmentation\",{\"1\":{\"17\":3,\"35\":1,\"39\":1,\"91\":1,\"236\":1}}],[\"sentiment\",{\"1\":{\"212\":1}}],[\"sent\",{\"1\":{\"175\":1,\"176\":1,\"223\":1,\"224\":4}}],[\"sentences\",{\"1\":{\"175\":2,\"223\":7,\"224\":4,\"225\":11}}],[\"sentence\",{\"0\":{\"219\":1},\"1\":{\"103\":1,\"175\":2,\"217\":1,\"233\":1,\"251\":5}}],[\"seem\",{\"1\":{\"244\":1}}],[\"seed\",{\"1\":{\"107\":1}}],[\"seen\",{\"1\":{\"22\":1}}],[\"sep\",{\"1\":{\"104\":1,\"219\":7,\"221\":1,\"224\":2,\"225\":9,\"226\":1,\"233\":11,\"253\":2,\"255\":2,\"257\":2}}],[\"search\",{\"1\":{\"104\":1}}],[\"search扩展\",{\"1\":{\"104\":1}}],[\"search的beam数量\",{\"1\":{\"104\":1}}],[\"searchpicbytext\",{\"1\":{\"94\":1,\"95\":1}}],[\"second\",{\"1\":{\"233\":1}}],[\"seconds\",{\"1\":{\"93\":2,\"95\":1}}],[\"section\",{\"1\":{\"35\":2}}],[\"sections=self\",{\"1\":{\"16\":2}}],[\"sections=spatial\",{\"1\":{\"10\":1}}],[\"sets\",{\"1\":{\"72\":1}}],[\"set\",{\"1\":{\"25\":6,\"45\":1,\"50\":1,\"54\":1,\"55\":4,\"56\":1,\"58\":2,\"62\":1,\"69\":2,\"72\":1,\"224\":1}}],[\"seq\",{\"1\":{\"15\":1,\"100\":6,\"101\":5,\"102\":13,\"103\":1,\"225\":3,\"226\":6,\"230\":10,\"232\":1,\"236\":2,\"244\":2,\"250\":4,\"251\":4,\"253\":4,\"254\":4,\"256\":2,\"257\":4,\"271\":1}}],[\"sequences\",{\"1\":{\"133\":1,\"233\":2,\"262\":1}}],[\"sequence\",{\"1\":{\"13\":2,\"103\":2,\"233\":4,\"241\":3,\"250\":2,\"251\":2,\"253\":3,\"254\":2,\"256\":4}}],[\"sequential\",{\"1\":{\"11\":1,\"12\":1,\"14\":1,\"15\":1,\"16\":1,\"30\":2,\"55\":1,\"114\":2}}],[\"semantic\",{\"1\":{\"10\":4,\"15\":3,\"55\":1,\"212\":1}}],[\"self\",{\"0\":{\"199\":1},\"1\":{\"10\":18,\"11\":32,\"12\":7,\"13\":1,\"14\":6,\"15\":40,\"16\":20,\"25\":21,\"27\":10,\"29\":8,\"30\":10,\"31\":10,\"32\":5,\"33\":17,\"35\":9,\"49\":17,\"50\":25,\"53\":41,\"57\":9,\"58\":27,\"64\":26,\"66\":24,\"67\":19,\"68\":24,\"100\":9,\"101\":4,\"102\":12,\"103\":49,\"104\":7,\"107\":12,\"109\":12,\"110\":16,\"111\":22,\"112\":25,\"113\":16,\"114\":33,\"166\":5,\"167\":3,\"168\":3,\"169\":3,\"170\":5,\"172\":5,\"176\":13,\"199\":3,\"221\":2,\"224\":34,\"226\":24,\"230\":1,\"233\":9,\"236\":13,\"238\":26,\"239\":5,\"240\":7,\"241\":11,\"242\":14,\"244\":28,\"245\":9,\"246\":11,\"248\":10,\"249\":9,\"250\":7,\"251\":8,\"253\":3,\"254\":8,\"256\":12,\"257\":9,\"261\":2,\"262\":16,\"263\":5,\"265\":8,\"266\":15,\"267\":7,\"269\":19,\"270\":7,\"271\":15}}],[\"sts\",{\"1\":{\"212\":1}}],[\"style\",{\"1\":{\"204\":1}}],[\"storage\",{\"1\":{\"232\":1}}],[\"storagefurniture\",{\"1\":{\"25\":1}}],[\"stories\",{\"1\":{\"203\":1,\"204\":1}}],[\"study\",{\"0\":{\"83\":1}}],[\"stn\",{\"1\":{\"66\":3}}],[\"stnkd\",{\"1\":{\"62\":1,\"65\":1,\"66\":1}}],[\"stn3d\",{\"1\":{\"62\":1,\"64\":5,\"65\":1,\"66\":2}}],[\"std=0\",{\"1\":{\"110\":1,\"111\":2,\"114\":2}}],[\"stderr\",{\"1\":{\"40\":2}}],[\"stdout\",{\"1\":{\"40\":1}}],[\"string\",{\"1\":{\"223\":1}}],[\"strip\",{\"1\":{\"223\":4}}],[\"strict=false\",{\"1\":{\"118\":1}}],[\"stride=patch\",{\"1\":{\"109\":1}}],[\"stride\",{\"1\":{\"109\":1}}],[\"structure\",{\"1\":{\"53\":1,\"105\":1}}],[\"structural\",{\"1\":{\"20\":1}}],[\"str\",{\"1\":{\"25\":2,\"107\":3,\"175\":18,\"176\":6,\"238\":1,\"248\":1}}],[\"stanford\",{\"1\":{\"212\":1}}],[\"standing\",{\"1\":{\"80\":1,\"91\":2}}],[\"stats\",{\"1\":{\"175\":3}}],[\"staticmethod\",{\"1\":{\"107\":2}}],[\"state=hidden\",{\"1\":{\"103\":1}}],[\"state\",{\"1\":{\"39\":2,\"40\":1,\"100\":2,\"102\":1,\"118\":1,\"227\":2,\"240\":1}}],[\"states=outputs\",{\"1\":{\"103\":1}}],[\"states=output\",{\"1\":{\"103\":1}}],[\"states=encoder\",{\"1\":{\"103\":1}}],[\"states=all\",{\"1\":{\"103\":1}}],[\"states=false\",{\"1\":{\"103\":1}}],[\"states=none\",{\"1\":{\"103\":5}}],[\"states=image\",{\"1\":{\"100\":1,\"102\":1,\"103\":1}}],[\"states\",{\"1\":{\"10\":4,\"103\":21,\"104\":1,\"238\":16,\"239\":4,\"240\":2,\"242\":2,\"244\":4,\"245\":8,\"248\":8,\"249\":6,\"251\":1,\"257\":1}}],[\"stage流程\",{\"1\":{\"99\":1}}],[\"stage\",{\"0\":{\"100\":1,\"104\":1},\"1\":{\"99\":3,\"100\":1,\"104\":1}}],[\"startswith\",{\"1\":{\"223\":1}}],[\"start\",{\"1\":{\"38\":1,\"93\":5,\"95\":5,\"253\":12,\"254\":15,\"255\":5}}],[\"stab\",{\"1\":{\"25\":1}}],[\"stack\",{\"1\":{\"13\":2,\"91\":1,\"102\":3,\"107\":1,\"235\":1,\"267\":1}}],[\"steps=100\",{\"1\":{\"232\":2}}],[\"steplr\",{\"1\":{\"37\":1}}],[\"step\",{\"0\":{\"11\":1,\"12\":1,\"13\":1,\"14\":1,\"15\":1,\"16\":1},\"1\":{\"10\":11,\"35\":5,\"37\":1,\"38\":1,\"103\":5,\"114\":1,\"169\":5,\"197\":2,\"225\":2,\"227\":1,\"232\":4}}],[\"siri\",{\"1\":{\"282\":1}}],[\"silhouette\",{\"1\":{\"91\":1}}],[\"single\",{\"1\":{\"50\":1}}],[\"sig\",{\"1\":{\"175\":1,\"176\":1}}],[\"sign\",{\"1\":{\"39\":1}}],[\"sigmoid\",{\"1\":{\"16\":3,\"27\":1,\"33\":3,\"35\":1,\"55\":1,\"166\":3,\"167\":4,\"168\":4,\"169\":3,\"170\":3,\"172\":1}}],[\"simply\",{\"1\":{\"240\":1}}],[\"similarities\",{\"1\":{\"93\":2,\"94\":2,\"95\":4}}],[\"similarity\",{\"1\":{\"39\":3,\"90\":1,\"91\":1,\"93\":2,\"94\":1,\"95\":3,\"101\":4,\"212\":1}}],[\"sim\",{\"1\":{\"32\":1,\"39\":16,\"101\":13,\"102\":4}}],[\"site\",{\"1\":{\"147\":1}}],[\"sit\",{\"1\":{\"25\":1,\"40\":2}}],[\"side\",{\"1\":{\"10\":2}}],[\"size的四倍\",{\"1\":{\"115\":1}}],[\"size是transformer\",{\"1\":{\"115\":1}}],[\"size就是对应通过embedding层后每个token的dim\",{\"1\":{\"115\":1}}],[\"size也是同样处理手段\",{\"1\":{\"100\":1}}],[\"size=args\",{\"1\":{\"235\":1}}],[\"size=32\",{\"1\":{\"227\":1}}],[\"size=768\",{\"1\":{\"118\":1}}],[\"size=none\",{\"1\":{\"111\":1,\"114\":1}}],[\"size=img\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"size=patch\",{\"1\":{\"109\":1,\"110\":1,\"111\":1,\"114\":1}}],[\"size=224\",{\"1\":{\"109\":1,\"110\":1,\"111\":1,\"114\":1,\"118\":1}}],[\"size=64\",{\"1\":{\"93\":2,\"95\":2}}],[\"size=16\",{\"1\":{\"109\":1,\"110\":1,\"111\":1,\"114\":1,\"118\":1,\"232\":2}}],[\"size=1\",{\"1\":{\"64\":1}}],[\"size=10\",{\"1\":{\"37\":1}}],[\"size=batch\",{\"1\":{\"37\":3,\"108\":2}}],[\"size\",{\"1\":{\"10\":6,\"11\":1,\"12\":2,\"13\":12,\"14\":3,\"15\":4,\"16\":3,\"27\":1,\"37\":3,\"40\":2,\"64\":3,\"65\":2,\"66\":1,\"68\":2,\"93\":4,\"95\":4,\"100\":6,\"101\":8,\"102\":10,\"103\":7,\"104\":4,\"107\":1,\"108\":2,\"109\":24,\"110\":4,\"111\":2,\"113\":10,\"114\":4,\"166\":4,\"167\":3,\"168\":3,\"169\":4,\"170\":5,\"172\":5,\"224\":3,\"226\":19,\"227\":2,\"230\":1,\"235\":1,\"236\":7,\"238\":5,\"240\":2,\"242\":1,\"244\":15,\"245\":3,\"248\":3,\"249\":3,\"250\":1,\"251\":1,\"253\":12,\"254\":3,\"256\":2,\"257\":13,\"265\":2,\"266\":4,\"267\":1,\"269\":4,\"270\":1,\"271\":3}}],[\"special\",{\"1\":{\"104\":1,\"233\":6,\"255\":2}}],[\"span\",{\"1\":{\"253\":1,\"255\":2}}],[\"spacy\",{\"1\":{\"80\":1,\"211\":1}}],[\"spatial\",{\"1\":{\"10\":5,\"15\":3,\"64\":1}}],[\"spidercnn\",{\"1\":{\"69\":2}}],[\"splitext\",{\"1\":{\"93\":1,\"95\":1,\"107\":1}}],[\"split=\",{\"1\":{\"25\":1}}],[\"split\",{\"1\":{\"10\":2,\"16\":4,\"25\":5,\"93\":1,\"95\":1,\"107\":1,\"108\":1,\"175\":2,\"223\":1,\"253\":2,\"254\":1}}],[\"spring\",{\"1\":{\"2\":1}}],[\"a22b\",{\"1\":{\"278\":1}}],[\"a3b\",{\"1\":{\"278\":1}}],[\"agi\",{\"1\":{\"282\":3}}],[\"agents\",{\"1\":{\"287\":1}}],[\"agent\",{\"1\":{\"278\":1,\"282\":1,\"288\":1}}],[\"agnostic模型优于那些为每个任务精心设计的模型\",{\"1\":{\"203\":1,\"204\":1}}],[\"aggregate\",{\"1\":{\"101\":2}}],[\"a为m\",{\"1\":{\"184\":1}}],[\"axis\",{\"1\":{\"94\":1,\"95\":1}}],[\"axis=1\",{\"1\":{\"40\":1,\"90\":3,\"93\":3,\"95\":3}}],[\"axis=0\",{\"1\":{\"40\":1,\"90\":1}}],[\"across\",{\"1\":{\"101\":2}}],[\"acceptability语言可接受性语料库\",{\"1\":{\"212\":1}}],[\"accu\",{\"1\":{\"114\":1}}],[\"accuracy\",{\"0\":{\"152\":1},\"1\":{\"93\":8,\"95\":6,\"228\":2}}],[\"acc\",{\"1\":{\"93\":2,\"95\":2}}],[\"active\",{\"1\":{\"256\":8}}],[\"activate\",{\"1\":{\"141\":2,\"144\":1,\"147\":1,\"148\":2,\"232\":1,\"259\":1}}],[\"activation\",{\"1\":{\"33\":1,\"240\":2}}],[\"act2fn\",{\"1\":{\"238\":1,\"248\":1}}],[\"act\",{\"1\":{\"111\":1,\"112\":6,\"114\":5,\"238\":7,\"248\":7}}],[\"actually\",{\"1\":{\"244\":1}}],[\"actual\",{\"1\":{\"93\":2,\"95\":2}}],[\"avg\",{\"1\":{\"227\":5}}],[\"available\",{\"1\":{\"93\":1,\"95\":1}}],[\"average=true\",{\"1\":{\"166\":1,\"167\":1,\"168\":1,\"169\":1,\"170\":1,\"172\":1}}],[\"average\",{\"1\":{\"39\":1,\"62\":1,\"72\":1,\"166\":3,\"167\":2,\"168\":2,\"169\":2,\"170\":4,\"172\":4}}],[\"amazonaws\",{\"1\":{\"223\":2}}],[\"american\",{\"1\":{\"91\":1}}],[\"among\",{\"1\":{\"61\":1}}],[\"about\",{\"1\":{\"91\":1}}],[\"ablation\",{\"0\":{\"83\":1},\"1\":{\"83\":1}}],[\"abstraction\",{\"1\":{\"44\":1,\"45\":2,\"50\":1,\"54\":1,\"55\":4,\"56\":1,\"58\":2}}],[\"absolute\",{\"1\":{\"39\":4,\"102\":1}}],[\"abs\",{\"1\":{\"4\":1,\"7\":1,\"35\":4,\"42\":1,\"59\":1,\"76\":1,\"77\":1,\"97\":1,\"118\":1,\"225\":1}}],[\"ai\",{\"1\":{\"81\":1,\"180\":1,\"223\":2,\"278\":4,\"281\":2,\"290\":4}}],[\"aiou\",{\"1\":{\"39\":3}}],[\"applied\",{\"1\":{\"271\":1}}],[\"apple\",{\"1\":{\"218\":5}}],[\"apply\",{\"1\":{\"114\":1,\"244\":1,\"271\":2}}],[\"append\",{\"1\":{\"13\":3,\"25\":1,\"39\":1,\"49\":2,\"53\":5,\"57\":2,\"93\":3,\"95\":3,\"102\":3,\"107\":5,\"176\":2,\"224\":6,\"225\":4,\"233\":1}}],[\"api的调用\",{\"1\":{\"293\":1}}],[\"api\",{\"1\":{\"78\":1,\"278\":1,\"286\":2,\"290\":1}}],[\"a^t||\",{\"1\":{\"62\":1}}],[\"a\",{\"1\":{\"62\":1,\"80\":2,\"91\":16,\"92\":3,\"93\":1,\"94\":2,\"95\":3,\"98\":2,\"162\":2,\"172\":1,\"185\":2,\"189\":3,\"208\":1,\"224\":6,\"225\":18,\"226\":1,\"230\":1,\"232\":3,\"233\":4,\"244\":1,\"257\":1,\"267\":1,\"271\":2,\"277\":1,\"282\":1,\"292\":2}}],[\"at\",{\"1\":{\"40\":1,\"55\":2,\"91\":1}}],[\"attn有4\",{\"1\":{\"212\":1}}],[\"attn\",{\"1\":{\"29\":1,\"31\":1,\"33\":4,\"103\":3,\"111\":1,\"112\":4,\"113\":10,\"114\":2,\"230\":4,\"266\":4,\"269\":10,\"271\":7}}],[\"attenion\",{\"1\":{\"265\":1}}],[\"attend\",{\"1\":{\"244\":1,\"260\":1}}],[\"atten\",{\"1\":{\"15\":10}}],[\"attention可以用矩阵乘法一次计算所有的时刻\",{\"1\":{\"260\":1}}],[\"attention机制\",{\"1\":{\"260\":1}}],[\"attentional\",{\"1\":{\"213\":1}}],[\"attention运算过程中维度变换的理解\",{\"0\":{\"122\":1},\"1\":{\"122\":1}}],[\"attention的heads数\",{\"1\":{\"115\":1}}],[\"attention的输入\",{\"1\":{\"104\":1}}],[\"attention计算\",{\"1\":{\"102\":1}}],[\"attention模块\",{\"1\":{\"100\":1}}],[\"attentions=none\",{\"1\":{\"103\":1}}],[\"attentions=all\",{\"1\":{\"103\":2}}],[\"attentions=outputs\",{\"1\":{\"103\":2}}],[\"attentions=output\",{\"1\":{\"103\":3}}],[\"attentions=false\",{\"1\":{\"103\":3}}],[\"attentions\",{\"1\":{\"33\":1,\"103\":9,\"242\":1,\"244\":2,\"251\":1,\"257\":1}}],[\"attention\",{\"1\":{\"10\":7,\"11\":9,\"13\":12,\"15\":14,\"16\":1,\"32\":2,\"69\":3,\"72\":1,\"100\":3,\"101\":3,\"102\":10,\"103\":68,\"104\":1,\"112\":2,\"113\":2,\"123\":1,\"128\":1,\"132\":1,\"133\":1,\"189\":1,\"221\":2,\"233\":6,\"235\":4,\"238\":7,\"239\":2,\"241\":8,\"242\":3,\"244\":26,\"246\":4,\"251\":2,\"253\":1,\"254\":2,\"256\":4,\"257\":6,\"261\":2,\"271\":4,\"278\":3}}],[\"atts=none\",{\"1\":{\"13\":1}}],[\"atts\",{\"1\":{\"10\":2,\"13\":12,\"100\":2,\"102\":11,\"103\":4,\"104\":2}}],[\"augmented\",{\"0\":{\"283\":1},\"1\":{\"283\":1,\"292\":1}}],[\"auto\",{\"1\":{\"282\":1}}],[\"autocast\",{\"1\":{\"10\":1}}],[\"auc\",{\"0\":{\"158\":1,\"160\":1,\"162\":1},\"1\":{\"32\":1,\"39\":23,\"159\":1,\"160\":6,\"161\":1,\"162\":3}}],[\"always\",{\"1\":{\"271\":1}}],[\"already\",{\"1\":{\"233\":1}}],[\"alexnet\",{\"1\":{\"93\":1}}],[\"alpha+beta=1\",{\"1\":{\"170\":1}}],[\"alpha=beta=1\",{\"1\":{\"170\":1}}],[\"alpha=beta=0\",{\"1\":{\"170\":1}}],[\"alpha=alpha\",{\"1\":{\"169\":1,\"170\":1,\"172\":1}}],[\"alpha\",{\"1\":{\"35\":3,\"169\":8,\"170\":4,\"172\":7}}],[\"all=false\",{\"1\":{\"50\":2,\"58\":4}}],[\"all=true\",{\"1\":{\"49\":1,\"50\":1}}],[\"all流程图\",{\"1\":{\"49\":1}}],[\"all\",{\"1\":{\"30\":1,\"49\":8,\"93\":3,\"94\":1,\"95\":3,\"101\":2,\"102\":12,\"103\":1,\"223\":4,\"224\":2,\"233\":5,\"235\":16,\"244\":6,\"271\":3}}],[\"align=\",{\"1\":{\"107\":1}}],[\"aligned\",{\"1\":{\"90\":2}}],[\"alignment\",{\"1\":{\"79\":1}}],[\"aligning\",{\"1\":{\"76\":1}}],[\"align\",{\"1\":{\"16\":2}}],[\"afm\",{\"0\":{\"28\":1,\"32\":1},\"1\":{\"27\":1,\"28\":3,\"31\":1,\"32\":5,\"33\":1}}],[\"aff\",{\"1\":{\"38\":1,\"39\":1,\"40\":2}}],[\"aff2idx\",{\"1\":{\"25\":2}}],[\"afford\",{\"1\":{\"25\":2}}],[\"affordq\",{\"1\":{\"25\":1,\"37\":3}}],[\"affordance=dict\",{\"1\":{\"40\":1}}],[\"affordancenet\",{\"1\":{\"19\":2,\"20\":1,\"24\":1,\"26\":1}}],[\"affordance\",{\"1\":{\"4\":1,\"7\":1,\"10\":4,\"15\":5,\"16\":12,\"17\":3,\"19\":1,\"20\":1,\"24\":1,\"25\":15,\"27\":1,\"33\":6,\"37\":2,\"39\":3,\"40\":14}}],[\"ascii=false\",{\"1\":{\"223\":2}}],[\"assume\",{\"1\":{\"271\":1}}],[\"assert\",{\"1\":{\"107\":1,\"224\":1,\"271\":1}}],[\"assistant\",{\"0\":{\"77\":1},\"1\":{\"77\":1,\"80\":1,\"81\":2}}],[\"astronaut\",{\"1\":{\"91\":1}}],[\"astype\",{\"1\":{\"25\":2,\"39\":2,\"40\":4,\"64\":1}}],[\"as\",{\"1\":{\"25\":2,\"35\":2,\"40\":4,\"93\":2,\"94\":1,\"95\":5,\"107\":2,\"112\":1,\"113\":1,\"223\":3,\"224\":3,\"236\":1,\"249\":1,\"282\":1}}],[\"anthropic\",{\"1\":{\"278\":3}}],[\"answer\",{\"1\":{\"255\":3}}],[\"anaconda\",{\"1\":{\"147\":1}}],[\"anaconda3\",{\"1\":{\"143\":3}}],[\"an\",{\"1\":{\"30\":1,\"91\":1,\"249\":1}}],[\"annotated\",{\"1\":{\"259\":5}}],[\"anno\",{\"1\":{\"25\":9,\"40\":5}}],[\"and\",{\"0\":{\"77\":1},\"1\":{\"7\":1,\"25\":1,\"35\":1,\"37\":2,\"49\":8,\"76\":1,\"77\":1,\"91\":1,\"92\":1,\"103\":1,\"112\":1,\"118\":1,\"185\":1,\"223\":2,\"225\":1,\"233\":1,\"238\":1,\"242\":1,\"244\":1,\"248\":1,\"251\":1,\"253\":1,\"254\":1,\"262\":2,\"267\":1,\"269\":1,\"271\":2}}],[\"adjacent\",{\"1\":{\"225\":14}}],[\"add\",{\"1\":{\"40\":1,\"242\":1}}],[\"additional\",{\"1\":{\"16\":2}}],[\"adamw\",{\"1\":{\"227\":1}}],[\"adam\",{\"1\":{\"37\":1,\"211\":1}}],[\"adaptation\",{\"1\":{\"184\":1,\"186\":1}}],[\"adaptive\",{\"1\":{\"27\":1}}],[\"adaptiveavgpool1d\",{\"1\":{\"16\":1}}],[\"adapter\",{\"1\":{\"10\":2,\"12\":1,\"14\":1,\"188\":2}}],[\"are\",{\"1\":{\"242\":2,\"249\":1,\"254\":1}}],[\"area\",{\"1\":{\"35\":2,\"39\":3,\"169\":1}}],[\"argmax\",{\"1\":{\"93\":1,\"94\":1,\"95\":2,\"253\":2}}],[\"args\",{\"1\":{\"13\":1,\"15\":2,\"16\":2,\"40\":4,\"110\":1,\"118\":1,\"235\":1}}],[\"arange\",{\"1\":{\"49\":3,\"90\":1,\"101\":1,\"102\":1,\"236\":1}}],[\"architecture\",{\"1\":{\"28\":1,\"30\":1}}],[\"array\",{\"1\":{\"25\":2,\"39\":1,\"40\":3,\"64\":1,\"93\":2,\"95\":2}}],[\"arxiv\",{\"1\":{\"4\":1,\"7\":1,\"42\":1,\"59\":1,\"76\":1,\"77\":1,\"97\":1,\"118\":1}}],[\"论文实验结果显示\",{\"1\":{\"191\":1}}],[\"论文里也做了说明\",{\"1\":{\"115\":1}}],[\"论文里没有做解释\",{\"1\":{\"112\":1}}],[\"论文作者也对其做了实验\",{\"1\":{\"111\":1}}],[\"论文还实验了使用80个不同的prompt进行集成\",{\"1\":{\"92\":1}}],[\"论文指出\",{\"1\":{\"92\":1}}],[\"论文发现这个模型的效果最佳\",{\"1\":{\"90\":1}}],[\"论文核心创新点\",{\"1\":{\"78\":1}}],[\"论文链接\",{\"1\":{\"76\":1,\"77\":1,\"186\":1,\"272\":1}}],[\"论文简析\",{\"1\":{\"76\":1,\"77\":1}}],[\"论文中\",{\"1\":{\"189\":1}}],[\"论文中举的例子\",{\"1\":{\"185\":1}}],[\"论文中进行对比实验的clip模型也采用了这一配置\",{\"1\":{\"90\":1}}],[\"论文中还进行了多项\",{\"1\":{\"83\":1}}],[\"论文中重点测试了以下两个应用场景\",{\"1\":{\"81\":1}}],[\"论文中做了\",{\"1\":{\"69\":1}}],[\"论文中的验证\",{\"1\":{\"69\":1}}],[\"论文中也进行了大量消融实验来验证\",{\"1\":{\"32\":1}}],[\"论文中所给的模型架构图中的decoder\",{\"1\":{\"27\":1}}],[\"论文中所给的模型架构图中的encoder\",{\"1\":{\"27\":1}}],[\"论文提出了一个全新的模型\",{\"1\":{\"27\":1}}],[\"论文\",{\"0\":{\"202\":1},\"1\":{\"4\":1,\"7\":1,\"17\":1,\"42\":1,\"59\":1,\"97\":1,\"202\":2}}],[\"论文代码解读与复现\",{\"1\":{\"7\":1,\"17\":1}}],[\"论文代码解读\",{\"1\":{\"4\":1}}],[\"v3\",{\"1\":{\"278\":1}}],[\"v2\",{\"1\":{\"188\":2,\"278\":2}}],[\"v0\",{\"1\":{\"118\":1,\"288\":1}}],[\"v计算来源相同\",{\"1\":{\"113\":1}}],[\"v矩阵\",{\"1\":{\"113\":1}}],[\"v时使用偏置\",{\"1\":{\"113\":1}}],[\"vgg进行实现\",{\"1\":{\"93\":1}}],[\"version\",{\"1\":{\"238\":1,\"248\":1}}],[\"very\",{\"1\":{\"213\":1}}],[\"vec2\",{\"1\":{\"93\":5,\"95\":5}}],[\"vec1\",{\"1\":{\"93\":5,\"95\":5}}],[\"vector\",{\"1\":{\"221\":1}}],[\"vectors\",{\"1\":{\"55\":1,\"271\":1}}],[\"vector3dvector\",{\"1\":{\"40\":4}}],[\"vehicle\",{\"1\":{\"80\":1}}],[\"vs\",{\"0\":{\"285\":1},\"1\":{\"68\":1,\"69\":2,\"164\":1,\"187\":1,\"255\":1}}],[\"vocab是词典大小\",{\"1\":{\"263\":1}}],[\"vocab\",{\"1\":{\"103\":1,\"175\":50,\"176\":3,\"224\":16,\"226\":6,\"227\":2,\"236\":2,\"249\":2,\"251\":1,\"263\":2}}],[\"vocabulary\",{\"1\":{\"4\":1,\"174\":1,\"175\":1}}],[\"voxnet\",{\"1\":{\"69\":1}}],[\"voxel\",{\"1\":{\"60\":1,\"71\":1}}],[\"v\",{\"0\":{\"124\":1},\"1\":{\"29\":2,\"31\":2,\"100\":1,\"107\":4,\"113\":4,\"122\":1,\"128\":1,\"137\":2,\"175\":6,\"224\":2,\"230\":3,\"271\":2}}],[\"various\",{\"1\":{\"80\":1}}],[\"variable\",{\"1\":{\"64\":1}}],[\"vase\",{\"1\":{\"25\":1}}],[\"valid\",{\"1\":{\"225\":3}}],[\"validation\",{\"1\":{\"107\":1}}],[\"val\",{\"1\":{\"22\":1,\"37\":4,\"38\":2,\"39\":1,\"40\":2,\"107\":13,\"108\":7}}],[\"value的数量\",{\"1\":{\"123\":1}}],[\"valueerror\",{\"1\":{\"107\":1,\"224\":1}}],[\"value都会进行缓存\",{\"1\":{\"103\":1}}],[\"value在seq\",{\"1\":{\"103\":1}}],[\"value传入\",{\"1\":{\"103\":1}}],[\"values=outputs\",{\"1\":{\"103\":1}}],[\"values=past\",{\"1\":{\"103\":1}}],[\"values=query\",{\"1\":{\"103\":1}}],[\"values=next\",{\"1\":{\"103\":1}}],[\"values=none\",{\"1\":{\"103\":2}}],[\"values\",{\"1\":{\"49\":1,\"64\":1,\"66\":1,\"102\":3,\"103\":8}}],[\"value=tokenizer\",{\"1\":{\"225\":1}}],[\"value=tgt\",{\"1\":{\"33\":1}}],[\"value=self\",{\"1\":{\"103\":1}}],[\"value=none\",{\"1\":{\"103\":2}}],[\"value=memory\",{\"1\":{\"33\":1}}],[\"value=gt\",{\"1\":{\"32\":1}}],[\"value=x\",{\"1\":{\"32\":1}}],[\"value\",{\"1\":{\"15\":19,\"25\":3,\"29\":7,\"31\":6,\"103\":30,\"122\":1,\"123\":1,\"127\":2,\"128\":2,\"130\":1,\"132\":1,\"225\":1,\"244\":6,\"271\":7,\"278\":2}}],[\"virtex\",{\"1\":{\"96\":1}}],[\"vicuna\",{\"1\":{\"80\":1,\"81\":2}}],[\"vit核心\",{\"1\":{\"119\":1}}],[\"vitjx\",{\"1\":{\"118\":1}}],[\"vit这篇论文长达二十多页\",{\"1\":{\"116\":1}}],[\"vit才会慢慢超越resnet\",{\"1\":{\"115\":1}}],[\"vit的效果表现不如resnet\",{\"1\":{\"115\":1}}],[\"vit的表现通常比同等大小的resnets要差一些\",{\"1\":{\"105\":1}}],[\"vit的表现就会超过cnn\",{\"1\":{\"105\":1}}],[\"vit仍是采用transformer中用到layer\",{\"1\":{\"112\":1}}],[\"vit虽然采用的是transformer\",{\"1\":{\"112\":1}}],[\"vit中的多头自注意力模块实现逻辑和transformer基本一致\",{\"1\":{\"113\":1}}],[\"vit中\",{\"1\":{\"111\":1}}],[\"vit原论文中最核心的结论是\",{\"1\":{\"105\":1}}],[\"vit\",{\"1\":{\"80\":1,\"81\":1,\"88\":2,\"90\":2,\"93\":4,\"95\":1,\"100\":1,\"103\":1,\"108\":3,\"114\":1,\"118\":6}}],[\"view\",{\"1\":{\"11\":1,\"15\":2,\"49\":11,\"50\":1,\"53\":2,\"57\":2,\"60\":1,\"64\":3,\"66\":2,\"68\":2,\"71\":1,\"103\":4,\"166\":2,\"167\":2,\"168\":2,\"169\":2,\"170\":2,\"172\":2,\"227\":2,\"242\":4,\"244\":3,\"251\":4,\"256\":5,\"257\":6,\"271\":3}}],[\"vis\",{\"1\":{\"40\":6}}],[\"visiontransformer\",{\"1\":{\"110\":2,\"111\":2,\"114\":2,\"118\":1}}],[\"vision\",{\"0\":{\"41\":1,\"77\":1},\"1\":{\"10\":1,\"13\":1,\"16\":1,\"30\":1,\"76\":1,\"77\":1,\"88\":1,\"90\":1,\"93\":1,\"100\":2,\"104\":1,\"105\":3,\"112\":1,\"118\":2,\"119\":1}}],[\"visualization\",{\"1\":{\"40\":3}}],[\"visualizer\",{\"1\":{\"40\":1}}],[\"visualize\",{\"1\":{\"40\":4}}],[\"visual\",{\"1\":{\"7\":1,\"76\":1,\"78\":1,\"96\":2,\"100\":1,\"104\":2}}],[\"vlm\",{\"1\":{\"13\":1}}],[\"vl\",{\"1\":{\"2\":1,\"102\":4}}],[\"omni\",{\"1\":{\"278\":1}}],[\"o3\",{\"1\":{\"278\":3}}],[\"o3d\",{\"1\":{\"40\":9}}],[\"o1\",{\"1\":{\"277\":1,\"278\":12}}],[\"our\",{\"1\":{\"254\":1}}],[\"outside\",{\"1\":{\"80\":1,\"254\":1}}],[\"outputs\",{\"1\":{\"103\":26,\"104\":2,\"232\":1,\"241\":2,\"242\":7,\"246\":2,\"251\":6,\"253\":4,\"254\":8,\"256\":6,\"257\":6}}],[\"output\",{\"1\":{\"33\":1,\"56\":1,\"100\":5,\"102\":4,\"103\":24,\"128\":1,\"137\":1,\"166\":1,\"169\":1,\"221\":10,\"223\":8,\"226\":9,\"232\":3,\"238\":9,\"240\":4,\"241\":7,\"242\":4,\"244\":2,\"246\":4,\"249\":2,\"250\":4,\"251\":4,\"253\":5,\"254\":2,\"256\":4,\"257\":4}}],[\"out\",{\"1\":{\"10\":5,\"16\":5,\"30\":5,\"49\":7,\"53\":4,\"57\":4,\"112\":5,\"174\":1,\"175\":5,\"244\":1}}],[\"ok=true\",{\"1\":{\"223\":2}}],[\"oov\",{\"1\":{\"174\":1}}],[\"official\",{\"1\":{\"118\":1}}],[\"off\",{\"1\":{\"94\":1,\"95\":1}}],[\"of\",{\"0\":{\"198\":1},\"1\":{\"40\":3,\"49\":1,\"80\":2,\"90\":4,\"91\":6,\"92\":3,\"93\":2,\"94\":1,\"95\":2,\"96\":3,\"103\":1,\"107\":1,\"133\":1,\"174\":1,\"182\":1,\"184\":1,\"185\":2,\"186\":1,\"198\":2,\"199\":1,\"212\":1,\"217\":1,\"230\":1,\"233\":1,\"255\":2,\"256\":1,\"267\":1,\"269\":1,\"271\":1,\"280\":1,\"292\":1}}],[\"overwrite\",{\"1\":{\"232\":1}}],[\"overall\",{\"1\":{\"39\":2}}],[\"over\",{\"0\":{\"168\":1},\"1\":{\"39\":3,\"168\":2}}],[\"o\",{\"1\":{\"31\":2,\"32\":1,\"69\":2,\"278\":1,\"287\":1}}],[\"os\",{\"1\":{\"25\":3,\"40\":3,\"93\":11,\"94\":2,\"95\":14,\"107\":8,\"223\":4,\"224\":1,\"227\":1}}],[\"one\",{\"1\":{\"114\":1,\"227\":2}}],[\"ones\",{\"1\":{\"13\":8,\"49\":1,\"100\":1,\"102\":3,\"103\":1,\"104\":1}}],[\"only=true\",{\"1\":{\"227\":1}}],[\"only\",{\"1\":{\"104\":1,\"233\":1,\"249\":1,\"255\":2,\"256\":1,\"278\":5}}],[\"on\",{\"1\":{\"17\":3,\"20\":1,\"39\":1,\"40\":2,\"91\":3,\"271\":1}}],[\"opus\",{\"1\":{\"278\":3}}],[\"operating\",{\"0\":{\"159\":1},\"1\":{\"39\":1}}],[\"openwebtext2\",{\"1\":{\"278\":1}}],[\"openai首先尝试了virtex模型\",{\"1\":{\"96\":1}}],[\"openai从网络上收集了4亿条数据进行实验\",{\"1\":{\"96\":1}}],[\"openai从网络上收集了总计4亿对文本和图像\",{\"1\":{\"90\":1}}],[\"openai\",{\"1\":{\"78\":2,\"88\":1,\"93\":2,\"95\":1,\"192\":1,\"202\":1,\"277\":1,\"278\":10,\"286\":1}}],[\"openaccess\",{\"1\":{\"17\":1}}],[\"opengvlab\",{\"1\":{\"76\":1}}],[\"open3d\",{\"1\":{\"40\":1}}],[\"opening\",{\"1\":{\"35\":1,\"94\":1,\"95\":1}}],[\"open\",{\"1\":{\"4\":1,\"19\":1,\"20\":1,\"24\":1,\"25\":3,\"40\":2,\"93\":1,\"94\":1,\"95\":2,\"107\":2,\"175\":1,\"176\":1,\"223\":3,\"224\":3}}],[\"option\",{\"1\":{\"40\":1}}],[\"optional\",{\"1\":{\"13\":1,\"33\":6}}],[\"optimizing\",{\"1\":{\"183\":1}}],[\"optimization\",{\"1\":{\"78\":2}}],[\"optimizer\",{\"1\":{\"37\":3,\"38\":2,\"39\":2,\"114\":1,\"227\":2}}],[\"optim\",{\"1\":{\"37\":3}}],[\"opt\",{\"1\":{\"37\":2,\"38\":1,\"40\":3}}],[\"obj\",{\"1\":{\"15\":7}}],[\"objects\",{\"1\":{\"25\":3,\"40\":3}}],[\"object\",{\"1\":{\"4\":1,\"7\":1,\"17\":3,\"24\":1,\"25\":4,\"39\":2,\"169\":1}}],[\"observations\",{\"1\":{\"7\":1}}],[\"ordered\",{\"1\":{\"175\":3}}],[\"ordereddict\",{\"1\":{\"114\":1}}],[\"orthogonal\",{\"1\":{\"65\":1}}],[\"original\",{\"1\":{\"55\":1,\"118\":1,\"244\":1}}],[\"or\",{\"1\":{\"10\":1,\"16\":2,\"61\":1,\"90\":2,\"112\":2,\"113\":1,\"114\":2,\"224\":1,\"232\":1,\"238\":1,\"248\":1}}],[\"org\",{\"1\":{\"4\":1,\"7\":1,\"42\":1,\"59\":1,\"76\":1,\"77\":1,\"97\":1,\"118\":1}}],[\"oracle\",{\"0\":{\"2\":1},\"1\":{\"0\":1}}],[\"gqa\",{\"1\":{\"278\":3}}],[\"guid\",{\"1\":{\"233\":1}}],[\"guide\",{\"1\":{\"35\":1,\"185\":1}}],[\"guided\",{\"1\":{\"17\":3,\"39\":1}}],[\"git\",{\"1\":{\"232\":2,\"259\":1}}],[\"github\",{\"1\":{\"4\":1,\"7\":1,\"17\":1,\"42\":2,\"59\":2,\"76\":1,\"77\":1,\"97\":1,\"107\":1,\"118\":2,\"171\":1,\"216\":1,\"232\":1,\"259\":1}}],[\"glm4\",{\"1\":{\"278\":1}}],[\"glm系列模型是\",{\"1\":{\"278\":1}}],[\"glm\",{\"1\":{\"277\":1,\"278\":5}}],[\"gleu\",{\"1\":{\"238\":2}}],[\"glue多任务提升5\",{\"1\":{\"204\":1}}],[\"global\",{\"1\":{\"64\":1,\"66\":8,\"67\":1,\"68\":1,\"69\":1}}],[\"gather函数比较灵活\",{\"1\":{\"227\":1}}],[\"gather\",{\"1\":{\"226\":4}}],[\"garage\",{\"1\":{\"91\":1}}],[\"gamma=gamma\",{\"1\":{\"169\":1}}],[\"gamma=0\",{\"1\":{\"37\":1}}],[\"gamma\",{\"1\":{\"35\":3,\"169\":3}}],[\"gt\",{\"0\":{\"21\":1},\"1\":{\"21\":2,\"25\":2,\"32\":5,\"35\":1,\"38\":2,\"39\":2,\"166\":1,\"167\":2,\"168\":2}}],[\"gpblock\",{\"1\":{\"32\":1}}],[\"gpb\",{\"1\":{\"27\":3}}],[\"gpt\",{\"0\":{\"82\":2,\"103\":1,\"202\":1},\"1\":{\"20\":3,\"78\":5,\"81\":2,\"82\":6,\"174\":1,\"189\":2,\"192\":2,\"202\":1,\"255\":2,\"277\":8,\"278\":21,\"280\":1,\"282\":1}}],[\"gpu上的矩阵运算都是充分优化和高度并行的\",{\"1\":{\"260\":1}}],[\"gpu上训练12天\",{\"1\":{\"90\":1}}],[\"gpu上训练18天\",{\"1\":{\"90\":1}}],[\"gpu\",{\"1\":{\"10\":1,\"69\":1,\"192\":1,\"232\":2,\"260\":1,\"279\":1}}],[\"gemini\",{\"1\":{\"277\":1,\"278\":14}}],[\"generator模型结构图\",{\"1\":{\"263\":1}}],[\"generator\",{\"0\":{\"263\":1},\"1\":{\"262\":3,\"263\":2}}],[\"generate\",{\"1\":{\"104\":2}}],[\"generative\",{\"0\":{\"104\":1},\"1\":{\"104\":1,\"213\":1,\"278\":1}}],[\"generation\",{\"0\":{\"103\":1,\"283\":1},\"1\":{\"103\":1,\"183\":1,\"283\":1,\"292\":1}}],[\"generation能力\",{\"1\":{\"98\":1}}],[\"generic\",{\"1\":{\"76\":1,\"270\":1}}],[\"geometries\",{\"1\":{\"40\":1}}],[\"geometry\",{\"1\":{\"4\":1,\"40\":3}}],[\"gelu\",{\"1\":{\"30\":2,\"112\":4,\"114\":2,\"211\":1,\"226\":5}}],[\"getcwd\",{\"1\":{\"93\":1,\"95\":1}}],[\"getitem\",{\"1\":{\"25\":1,\"107\":1}}],[\"get\",{\"1\":{\"10\":1,\"37\":1,\"40\":3,\"50\":2,\"53\":2,\"58\":2,\"93\":10,\"94\":4,\"95\":12,\"175\":5,\"176\":10,\"226\":1,\"230\":1,\"233\":1,\"244\":1}}],[\"g\",{\"1\":{\"10\":1,\"29\":1,\"30\":1,\"62\":1}}],[\"good\",{\"1\":{\"223\":1}}],[\"googleapis\",{\"1\":{\"232\":1}}],[\"google\",{\"1\":{\"4\":1,\"30\":1,\"118\":2,\"217\":2,\"278\":3}}],[\"golang\",{\"1\":{\"2\":1}}],[\"grok\",{\"1\":{\"277\":1}}],[\"group流程图\",{\"1\":{\"49\":1}}],[\"grouped\",{\"1\":{\"49\":12,\"53\":13,\"278\":2}}],[\"groups=40\",{\"1\":{\"40\":1}}],[\"groups=opt\",{\"1\":{\"37\":1}}],[\"groups\",{\"1\":{\"37\":1}}],[\"group\",{\"1\":{\"29\":1,\"32\":2,\"49\":23,\"50\":3,\"53\":3,\"58\":4,\"80\":1}}],[\"grouping阶段\",{\"1\":{\"32\":1}}],[\"grouping\",{\"0\":{\"29\":1,\"47\":1,\"52\":1,\"54\":1},\"1\":{\"28\":1,\"32\":3,\"44\":1,\"45\":2,\"47\":1,\"50\":4,\"51\":2,\"55\":1}}],[\"grounded\",{\"0\":{\"103\":1},\"1\":{\"103\":1}}],[\"ground\",{\"0\":{\"21\":1},\"1\":{\"35\":2,\"39\":2,\"166\":2,\"167\":1,\"168\":2,\"169\":1,\"170\":1}}],[\"grounding\",{\"1\":{\"4\":1,\"7\":1}}],[\"grids\",{\"1\":{\"71\":1}}],[\"grid\",{\"1\":{\"60\":1,\"109\":3}}],[\"grams\",{\"1\":{\"96\":2}}],[\"gradients\",{\"1\":{\"103\":1}}],[\"grad\",{\"1\":{\"37\":2,\"38\":1,\"91\":2,\"93\":2,\"95\":2,\"102\":1,\"118\":1,\"227\":1}}],[\"grasping\",{\"1\":{\"20\":2,\"35\":1,\"169\":1}}],[\"grasp\",{\"1\":{\"10\":1,\"19\":1,\"20\":1,\"24\":1,\"25\":2,\"27\":1,\"28\":1}}],[\"great\",{\"0\":{\"4\":1},\"1\":{\"4\":2}}],[\"转化成功之后\",{\"1\":{\"232\":1}}],[\"转换成y序列的权重参数组成的矩阵\",{\"1\":{\"178\":1}}],[\"转换为一系列高维向量表示\",{\"1\":{\"261\":1}}],[\"转换为概率\",{\"1\":{\"170\":1}}],[\"转换为可优化的损失函数\",{\"1\":{\"168\":1}}],[\"转换为元组形式\",{\"1\":{\"109\":1}}],[\"转换为语言嵌入\",{\"1\":{\"81\":1}}],[\"转换为语言模型可用的\",{\"1\":{\"80\":1}}],[\"转换为嵌入向量\",{\"1\":{\"10\":1}}],[\"转换为\",{\"1\":{\"10\":1,\"13\":1,\"39\":1,\"57\":1}}],[\"转为numpy数组\",{\"1\":{\"40\":1}}],[\"转置为\",{\"1\":{\"40\":1}}],[\"转置后\",{\"1\":{\"30\":1}}],[\"转\",{\"1\":{\"3\":1}}],[\"转型\",{\"1\":{\"2\":1}}],[\"ckpt\",{\"1\":{\"232\":1}}],[\"cd\",{\"1\":{\"232\":1,\"259\":1}}],[\"cdn\",{\"1\":{\"202\":1}}],[\"c为输入token的总维度\",{\"1\":{\"113\":1}}],[\"c=in\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"c=3\",{\"1\":{\"109\":1,\"110\":1,\"111\":1,\"114\":1}}],[\"cbow\",{\"1\":{\"90\":1}}],[\"cc3m\",{\"1\":{\"80\":1}}],[\"c+d\",{\"1\":{\"49\":3}}],[\"cup\",{\"1\":{\"91\":1}}],[\"cuda\",{\"1\":{\"64\":2,\"65\":2,\"91\":2,\"93\":2,\"95\":2,\"147\":1}}],[\"current\",{\"1\":{\"39\":3,\"93\":2,\"94\":1,\"95\":3}}],[\"curve\",{\"1\":{\"39\":3}}],[\"cut\",{\"1\":{\"25\":1}}],[\"centercrop\",{\"1\":{\"108\":1}}],[\"center\",{\"1\":{\"107\":2}}],[\"centroids\",{\"1\":{\"49\":4}}],[\"centroid\",{\"1\":{\"40\":2,\"49\":2}}],[\"ce\",{\"1\":{\"37\":1,\"38\":3,\"169\":15,\"172\":10}}],[\"celoss\",{\"1\":{\"35\":4}}],[\"chroma\",{\"1\":{\"291\":1}}],[\"chinesegluedatasets\",{\"1\":{\"232\":2}}],[\"chinese\",{\"1\":{\"232\":5}}],[\"chunking\",{\"1\":{\"69\":1}}],[\"checkpoint\",{\"1\":{\"39\":2,\"40\":2,\"227\":5,\"232\":2}}],[\"chains\",{\"1\":{\"287\":1}}],[\"chain\",{\"0\":{\"198\":1},\"1\":{\"198\":2,\"199\":1,\"280\":1,\"288\":2,\"291\":1}}],[\"chair\",{\"1\":{\"24\":1,\"25\":1,\"40\":6}}],[\"chatglm\",{\"1\":{\"278\":2}}],[\"chatgpt\",{\"1\":{\"78\":1,\"81\":1,\"85\":1,\"188\":1,\"277\":1,\"278\":7,\"282\":1,\"286\":1}}],[\"chatbot\",{\"1\":{\"81\":1}}],[\"chat\",{\"1\":{\"81\":1,\"278\":4}}],[\"charlesq34\",{\"1\":{\"42\":1,\"59\":1}}],[\"characteristic\",{\"0\":{\"159\":1},\"1\":{\"39\":1}}],[\"channel=320\",{\"1\":{\"58\":1}}],[\"channel=384\",{\"1\":{\"58\":1}}],[\"channel=768\",{\"1\":{\"58\":1}}],[\"channel=64+3\",{\"1\":{\"58\":1}}],[\"channel=9+3\",{\"1\":{\"58\":1}}],[\"channel=256+3\",{\"1\":{\"58\":1}}],[\"channel=256\",{\"1\":{\"50\":1}}],[\"channel=128+3\",{\"1\":{\"58\":1}}],[\"channel=128\",{\"1\":{\"50\":1,\"58\":1}}],[\"channel=in\",{\"1\":{\"50\":1}}],[\"channel=true\",{\"1\":{\"50\":1,\"53\":1}}],[\"channel=518+additional\",{\"1\":{\"16\":1}}],[\"channel=512+self\",{\"1\":{\"16\":1}}],[\"channel=832\",{\"1\":{\"16\":1}}],[\"channel\",{\"1\":{\"16\":3,\"30\":11,\"49\":12,\"50\":7,\"53\":15,\"57\":11}}],[\"channels\",{\"1\":{\"10\":1,\"107\":1}}],[\"choices\",{\"1\":{\"257\":9}}],[\"choice\",{\"1\":{\"20\":1,\"225\":1,\"257\":1}}],[\"crawl\",{\"1\":{\"278\":1}}],[\"crafting\",{\"0\":{\"20\":1}}],[\"creating\",{\"1\":{\"175\":1}}],[\"creates\",{\"1\":{\"233\":1}}],[\"create\",{\"1\":{\"40\":1,\"140\":4,\"144\":1,\"175\":4,\"232\":1,\"233\":1,\"259\":1}}],[\"critical\",{\"1\":{\"62\":1,\"69\":2}}],[\"criterion2\",{\"1\":{\"227\":2}}],[\"criterion1\",{\"1\":{\"227\":2}}],[\"criterion\",{\"1\":{\"37\":2,\"38\":2}}],[\"crossentropy\",{\"1\":{\"172\":3}}],[\"crossentropyloss\",{\"1\":{\"37\":1,\"103\":2,\"227\":2,\"242\":1,\"251\":1,\"254\":1,\"256\":1,\"257\":1}}],[\"crossattention\",{\"1\":{\"103\":1}}],[\"cross\",{\"1\":{\"10\":1,\"15\":8,\"32\":1,\"35\":1,\"90\":2,\"101\":2,\"102\":1,\"103\":13,\"167\":5,\"169\":4,\"172\":2}}],[\"callbacks\",{\"1\":{\"287\":1}}],[\"call\",{\"1\":{\"278\":1}}],[\"calling\",{\"1\":{\"278\":1}}],[\"capital\",{\"1\":{\"255\":2}}],[\"captioning\",{\"1\":{\"103\":1}}],[\"caption\",{\"1\":{\"80\":3,\"96\":1,\"103\":1}}],[\"captions\",{\"1\":{\"80\":1,\"104\":2}}],[\"case\",{\"1\":{\"221\":1,\"232\":1,\"290\":2,\"291\":3}}],[\"casual\",{\"1\":{\"113\":1}}],[\"cand\",{\"1\":{\"224\":2,\"225\":3}}],[\"candidates\",{\"1\":{\"93\":13,\"95\":10,\"225\":3}}],[\"cannot\",{\"1\":{\"113\":1}}],[\"cache\",{\"1\":{\"103\":4}}],[\"cache=use\",{\"1\":{\"103\":1}}],[\"cache=none\",{\"1\":{\"103\":1}}],[\"cache=true\",{\"1\":{\"100\":1,\"103\":2}}],[\"causallmoutputwithcrossattentions\",{\"1\":{\"103\":1}}],[\"causal\",{\"1\":{\"103\":2}}],[\"camera\",{\"1\":{\"91\":1}}],[\"cardinality\",{\"1\":{\"35\":4}}],[\"catastrophic\",{\"1\":{\"180\":1}}],[\"category\",{\"1\":{\"39\":2,\"93\":4,\"95\":4}}],[\"cat\",{\"1\":{\"11\":1,\"13\":3,\"15\":1,\"16\":1,\"27\":1,\"49\":2,\"53\":2,\"57\":1,\"66\":1,\"91\":1,\"102\":6,\"103\":3,\"110\":1,\"111\":1,\"114\":1}}],[\"ca\",{\"1\":{\"10\":1}}],[\"cli\",{\"1\":{\"289\":1}}],[\"clipprocessor\",{\"1\":{\"93\":1,\"95\":2}}],[\"clipmodel\",{\"1\":{\"93\":1,\"95\":2}}],[\"clip模型均能够以较高的置信度给出正确的分类结果\",{\"1\":{\"91\":1}}],[\"clip模型能够在没有特定任务训练数据的情况下\",{\"1\":{\"91\":1}}],[\"clip模型的一个显著优势是它能够进行zero\",{\"1\":{\"91\":1}}],[\"clip模型会预测出个可能的文本\",{\"1\":{\"90\":1}}],[\"clip包含两个核心模型\",{\"1\":{\"90\":1}}],[\"clip的训练数据采用的是文本\",{\"1\":{\"89\":1}}],[\"clip的英文全称为contrastive\",{\"1\":{\"89\":1}}],[\"clip属于基于对比学习的多模态模型\",{\"1\":{\"89\":1}}],[\"clip原始论文链接\",{\"1\":{\"87\":1}}],[\"clip\",{\"0\":{\"101\":1},\"1\":{\"80\":2,\"81\":2,\"88\":3,\"91\":1,\"93\":3,\"95\":1,\"96\":1,\"227\":1}}],[\"claude\",{\"1\":{\"277\":1,\"278\":14}}],[\"clamp\",{\"1\":{\"172\":2,\"254\":4}}],[\"cla\",{\"1\":{\"107\":9}}],[\"classifier\",{\"1\":{\"221\":2,\"226\":3,\"232\":2,\"242\":2,\"256\":2,\"257\":2}}],[\"classification\",{\"1\":{\"56\":1,\"233\":1}}],[\"class=val\",{\"1\":{\"108\":1}}],[\"class=train\",{\"1\":{\"108\":1}}],[\"classes=5\",{\"1\":{\"118\":1}}],[\"classes=num\",{\"1\":{\"118\":1}}],[\"classes=1000\",{\"1\":{\"110\":1,\"111\":1,\"114\":1}}],[\"classes\",{\"1\":{\"25\":2,\"55\":1,\"58\":8,\"110\":3,\"111\":2,\"114\":6,\"118\":2}}],[\"class\",{\"0\":{\"110\":1},\"1\":{\"10\":1,\"11\":1,\"15\":2,\"16\":1,\"19\":1,\"25\":6,\"27\":1,\"29\":1,\"30\":1,\"31\":1,\"32\":1,\"33\":2,\"35\":1,\"40\":1,\"49\":1,\"50\":4,\"53\":4,\"57\":1,\"58\":1,\"64\":1,\"66\":1,\"67\":1,\"68\":1,\"100\":1,\"102\":1,\"103\":4,\"104\":1,\"107\":27,\"109\":1,\"110\":3,\"111\":1,\"112\":2,\"113\":1,\"114\":1,\"166\":2,\"167\":1,\"168\":1,\"169\":3,\"170\":1,\"172\":1,\"221\":1,\"224\":1,\"226\":1,\"230\":1,\"236\":1,\"238\":3,\"239\":1,\"240\":1,\"241\":1,\"242\":1,\"244\":1,\"245\":1,\"246\":1,\"248\":1,\"249\":1,\"250\":1,\"251\":1,\"254\":1,\"256\":1,\"257\":1,\"262\":1,\"263\":1,\"265\":1,\"266\":1,\"267\":1,\"269\":1,\"270\":1,\"271\":1}}],[\"clues\",{\"1\":{\"28\":1}}],[\"cls是一个二分类值\",{\"1\":{\"227\":1}}],[\"cls\",{\"1\":{\"25\":7,\"38\":3,\"50\":1,\"53\":2,\"103\":2,\"110\":22,\"111\":5,\"114\":5,\"219\":4,\"221\":6,\"224\":2,\"225\":5,\"226\":10,\"227\":4,\"233\":9,\"240\":1,\"251\":3,\"253\":2,\"255\":2,\"257\":1}}],[\"cls2idx\",{\"1\":{\"25\":2}}],[\"cloze和race提升明显\",{\"1\":{\"212\":1}}],[\"cloze\",{\"1\":{\"204\":1}}],[\"close\",{\"1\":{\"203\":1}}],[\"clones\",{\"1\":{\"266\":1,\"267\":1,\"269\":1,\"270\":1,\"271\":1}}],[\"clone\",{\"1\":{\"102\":1,\"103\":1,\"232\":1,\"259\":1}}],[\"clock\",{\"1\":{\"25\":1}}],[\"cloud\",{\"1\":{\"16\":3,\"40\":6,\"71\":1}}],[\"cl=语言嵌入维度\",{\"1\":{\"10\":1}}],[\"cl\",{\"1\":{\"10\":3,\"25\":2}}],[\"cs231n\",{\"0\":{\"274\":1}}],[\"cs224n\",{\"0\":{\"273\":1}}],[\"csv\",{\"1\":{\"25\":2,\"223\":6}}],[\"cs\",{\"1\":{\"10\":2,\"62\":2}}],[\"csdn\",{\"1\":{\"0\":1}}],[\"cp\",{\"1\":{\"10\":1}}],[\"cpu\",{\"1\":{\"10\":1,\"40\":2,\"91\":3,\"93\":3,\"95\":3,\"260\":1}}],[\"ci\",{\"1\":{\"10\":1}}],[\"c\",{\"1\":{\"10\":1,\"11\":9,\"15\":16,\"16\":3,\"27\":4,\"29\":2,\"30\":2,\"31\":2,\"32\":2,\"33\":12,\"49\":14,\"53\":5,\"55\":3,\"57\":7,\"58\":1,\"90\":1,\"98\":2,\"100\":1,\"109\":6,\"110\":3,\"111\":2,\"113\":3,\"114\":2,\"162\":2,\"170\":1,\"223\":2,\"230\":1,\"257\":1}}],[\"cnn具有两种归纳偏置\",{\"1\":{\"105\":1}}],[\"cnn\",{\"1\":{\"30\":1,\"64\":1,\"69\":4,\"71\":3,\"90\":1,\"105\":1,\"117\":9,\"119\":1}}],[\"cn\",{\"1\":{\"7\":1}}],[\"core\",{\"1\":{\"267\":1,\"288\":1,\"289\":1}}],[\"corresponding\",{\"1\":{\"240\":1}}],[\"correct\",{\"1\":{\"93\":3,\"95\":3,\"228\":2}}],[\"corpus\",{\"1\":{\"211\":1,\"212\":2}}],[\"covers\",{\"1\":{\"202\":1}}],[\"cot的特点是同类型问题的迁移思考\",{\"1\":{\"200\":1}}],[\"cot的效果并不明显\",{\"1\":{\"198\":1}}],[\"cot是llm足够大\",{\"1\":{\"198\":1}}],[\"cot\",{\"1\":{\"198\":1,\"280\":1}}],[\"coefficient\",{\"1\":{\"166\":3,\"167\":2}}],[\"count\",{\"1\":{\"93\":7,\"95\":7,\"175\":2,\"225\":3}}],[\"coffee\",{\"1\":{\"91\":1}}],[\"cosine\",{\"1\":{\"90\":1,\"93\":2,\"94\":1,\"95\":3,\"211\":1}}],[\"cosineannealinglr\",{\"1\":{\"37\":1}}],[\"cola上取得45\",{\"1\":{\"212\":1}}],[\"cola\",{\"1\":{\"212\":1}}],[\"collate可以参考\",{\"1\":{\"107\":1}}],[\"collate\",{\"1\":{\"107\":3,\"108\":4,\"235\":2}}],[\"collaborative\",{\"1\":{\"4\":1}}],[\"colors\",{\"1\":{\"40\":6}}],[\"color\",{\"1\":{\"40\":9}}],[\"coordinates\",{\"1\":{\"40\":6}}],[\"consistency的过程\",{\"1\":{\"199\":1}}],[\"consistency的例子\",{\"1\":{\"199\":1}}],[\"consistency的大致原理是这样\",{\"1\":{\"199\":1}}],[\"consistency技术是在cot技术的基础之上\",{\"1\":{\"199\":1}}],[\"consistency\",{\"0\":{\"199\":1},\"1\":{\"199\":1}}],[\"confidence\",{\"1\":{\"169\":1}}],[\"config\",{\"1\":{\"12\":2,\"14\":3,\"40\":1,\"103\":2,\"232\":2,\"236\":9,\"238\":16,\"239\":3,\"240\":3,\"241\":5,\"242\":7,\"244\":9,\"245\":5,\"246\":3,\"248\":8,\"249\":5,\"250\":3,\"251\":5,\"253\":1,\"254\":6,\"256\":7,\"257\":5}}],[\"confusion\",{\"0\":{\"151\":1}}],[\"condaerror\",{\"1\":{\"148\":1}}],[\"conda\",{\"1\":{\"140\":4,\"141\":2,\"142\":1,\"143\":4,\"144\":3,\"145\":1,\"146\":1,\"147\":6,\"148\":4,\"232\":2,\"259\":2}}],[\"conda虚拟环境管理\",{\"0\":{\"139\":1},\"1\":{\"139\":1}}],[\"convirt基于对比学习的方法\",{\"1\":{\"96\":1}}],[\"convert\",{\"1\":{\"93\":1,\"95\":1,\"232\":1}}],[\"conversation\",{\"1\":{\"81\":1,\"282\":1}}],[\"convention\",{\"1\":{\"33\":1}}],[\"conv4\",{\"1\":{\"68\":2}}],[\"conv3\",{\"1\":{\"64\":2,\"66\":2,\"68\":2}}],[\"conv2\",{\"1\":{\"58\":2,\"64\":2,\"66\":2,\"68\":2}}],[\"conv2d\",{\"1\":{\"49\":2,\"53\":3,\"109\":1}}],[\"conv1\",{\"1\":{\"58\":2,\"64\":2,\"66\":2,\"68\":2}}],[\"conv1d\",{\"1\":{\"11\":2,\"15\":2,\"55\":2,\"57\":3,\"58\":2,\"64\":3,\"66\":4,\"68\":5,\"69\":1}}],[\"conv\",{\"1\":{\"49\":2,\"53\":6,\"57\":2}}],[\"convs\",{\"1\":{\"49\":3,\"53\":3,\"57\":3}}],[\"convolution\",{\"1\":{\"33\":2,\"57\":1}}],[\"connections\",{\"1\":{\"266\":1,\"269\":1}}],[\"connection\",{\"1\":{\"29\":1,\"31\":1,\"55\":1,\"57\":1,\"261\":1,\"287\":1}}],[\"continue\",{\"1\":{\"224\":2}}],[\"continuous\",{\"1\":{\"183\":1}}],[\"contiguous\",{\"1\":{\"68\":1,\"103\":3,\"244\":1,\"271\":1}}],[\"contrastive\",{\"0\":{\"101\":1},\"1\":{\"96\":1,\"101\":1}}],[\"contain\",{\"1\":{\"25\":1}}],[\"content\",{\"1\":{\"17\":1}}],[\"contextualized\",{\"1\":{\"255\":1}}],[\"contextual\",{\"1\":{\"20\":1,\"255\":1}}],[\"context\",{\"1\":{\"15\":2,\"43\":2,\"103\":10,\"230\":3,\"240\":1,\"244\":9,\"253\":1}}],[\"conceptual\",{\"1\":{\"80\":1}}],[\"concise\",{\"1\":{\"20\":1}}],[\"concat\",{\"1\":{\"10\":2,\"13\":1,\"49\":1,\"53\":3,\"221\":1,\"271\":1}}],[\"coder\",{\"1\":{\"278\":2}}],[\"code\",{\"1\":{\"4\":1,\"232\":7}}],[\"community\",{\"1\":{\"289\":1}}],[\"common\",{\"1\":{\"278\":1}}],[\"committer\",{\"1\":{\"2\":2}}],[\"comboloss\",{\"1\":{\"172\":2}}],[\"combo\",{\"0\":{\"172\":1},\"1\":{\"172\":12}}],[\"compute\",{\"1\":{\"271\":1}}],[\"compatibility\",{\"1\":{\"241\":1}}],[\"compile\",{\"1\":{\"175\":1}}],[\"compose\",{\"1\":{\"108\":2}}],[\"complex\",{\"1\":{\"81\":1,\"200\":1}}],[\"com\",{\"1\":{\"4\":2,\"7\":1,\"17\":2,\"42\":2,\"59\":2,\"76\":1,\"77\":1,\"97\":1,\"107\":2,\"118\":3,\"171\":1,\"202\":1,\"216\":1,\"223\":2,\"232\":2,\"259\":1}}],[\"cvpr\",{\"1\":{\"17\":1}}],[\"cvpr2024\",{\"1\":{\"17\":1}}],[\"cv\",{\"1\":{\"3\":1,\"89\":1,\"278\":1}}]],\"version\":2}}")).map(([e,t])=>[e,_t(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:n,options:o,id:s}})=>{const r=xt[n];e==="suggest"?self.postMessage([e,s,ve(t,r,o)]):e==="search"?self.postMessage([e,s,Ee(t,r,o,"max")]):self.postMessage({suggestions:[e,s,ve(t,r,o)],results:[e,s,Ee(t,r,o,__SLIMSEARCH_SORT_STRATEGY__)]})};
//# sourceMappingURL=index.js.map
