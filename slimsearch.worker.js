/**
* @vue/shared v3.5.17
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**//*! #__NO_SIDE_EFFECTS__ */function xt(e){const t=Object.create(null);for(const n of e.split(","))t[n]=1;return n=>n in t}const Ot={},St=()=>{},ze=Object.assign,It=Object.prototype.hasOwnProperty,se=(e,t)=>It.call(e,t),z=Array.isArray,Y=e=>Ve(e)==="[object Map]",oe=e=>typeof e=="function",Ce=e=>typeof e=="string",L=e=>typeof e=="symbol",B=e=>e!==null&&typeof e=="object",Nt=Object.prototype.toString,Ve=e=>Nt.call(e),ke=e=>Ve(e).slice(8,-1),ie=e=>Ce(e)&&e!=="NaN"&&e[0]!=="-"&&""+parseInt(e,10)===e,Mt=e=>{const t=Object.create(null);return n=>t[n]||(t[n]=e(n))},Ln=Mt(e=>e.charAt(0).toUpperCase()+e.slice(1)),W=(e,t)=>!Object.is(e,t);let $e;const ce=()=>$e||($e=typeof globalThis<"u"?globalThis:typeof self<"u"?self:typeof window<"u"?window:typeof global<"u"?global:{});/**
* @vue/reactivity v3.5.17
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/function Rt(e,...t){console.warn(`[Vue warn] ${e}`,...t)}let Tt,je=0,ae;function ue(){je++}function le(){if(--je>0)return;let e;for(;ae;){let t=ae;for(ae=void 0;t;){const n=t.next;if(t.next=void 0,t.flags&=-9,t.flags&1)try{t.trigger()}catch(s){e||(e=s)}t=n}}if(e)throw e}let G=!0;const De=[];function fe(){De.push(G),G=!1}function de(){const e=De.pop();G=e===void 0?!0:e}class Fe{constructor(t){this.computed=t,this.version=0,this.activeLink=void 0,this.subs=void 0,this.map=void 0,this.key=void 0,this.sc=0,this.__v_skip=!0}track(t){}trigger(t){this.version++,this.notify(t)}notify(t){ue();try{for(let n=this.subs;n;n=n.prevSub)n.sub.notify()&&n.sub.dep.notify()}finally{le()}}}const he=new WeakMap,C=Symbol(""),pe=Symbol(""),H=Symbol("");function E(e,t,n){if(G&&Tt){let s=he.get(e);s||he.set(e,s=new Map);let r=s.get(n);r||(s.set(n,r=new Fe),r.map=s,r.key=n),r.track()}}function I(e,t,n,s,r,o){const i=he.get(e);if(!i)return;const c=a=>{a&&a.trigger()};if(ue(),t==="clear")i.forEach(c);else{const a=z(e),u=a&&ie(n);if(a&&n==="length"){const l=Number(s);i.forEach((f,d)=>{(d==="length"||d===H||!L(d)&&d>=l)&&c(f)})}else switch((n!==void 0||i.has(void 0))&&c(i.get(n)),u&&c(i.get(H)),t){case"add":a?u&&c(i.get("length")):(c(i.get(C)),Y(e)&&c(i.get(pe)));break;case"delete":a||(c(i.get(C)),Y(e)&&c(i.get(pe)));break;case"set":Y(e)&&c(i.get(C));break}}le()}function $(e){const t=p(e);return t===e?t:(E(t,"iterate",H),N(e)?t:t.map(v))}function ge(e){return E(e=p(e),"iterate",H),e}const zt={__proto__:null,[Symbol.iterator](){return _e(this,Symbol.iterator,v)},concat(...e){return $(this).concat(...e.map(t=>z(t)?$(t):t))},entries(){return _e(this,"entries",e=>(e[1]=v(e[1]),e))},every(e,t){return O(this,"every",e,t,void 0,arguments)},filter(e,t){return O(this,"filter",e,t,n=>n.map(v),arguments)},find(e,t){return O(this,"find",e,t,v,arguments)},findIndex(e,t){return O(this,"findIndex",e,t,void 0,arguments)},findLast(e,t){return O(this,"findLast",e,t,v,arguments)},findLastIndex(e,t){return O(this,"findLastIndex",e,t,void 0,arguments)},forEach(e,t){return O(this,"forEach",e,t,void 0,arguments)},includes(...e){return me(this,"includes",e)},indexOf(...e){return me(this,"indexOf",e)},join(e){return $(this).join(e)},lastIndexOf(...e){return me(this,"lastIndexOf",e)},map(e,t){return O(this,"map",e,t,void 0,arguments)},pop(){return K(this,"pop")},push(...e){return K(this,"push",e)},reduce(e,...t){return Pe(this,"reduce",e,t)},reduceRight(e,...t){return Pe(this,"reduceRight",e,t)},shift(){return K(this,"shift")},some(e,t){return O(this,"some",e,t,void 0,arguments)},splice(...e){return K(this,"splice",e)},toReversed(){return $(this).toReversed()},toSorted(e){return $(this).toSorted(e)},toSpliced(...e){return $(this).toSpliced(...e)},unshift(...e){return K(this,"unshift",e)},values(){return _e(this,"values",v)}};function _e(e,t,n){const s=ge(e),r=s[t]();return s!==e&&!N(e)&&(r._next=r.next,r.next=()=>{const o=r._next();return o.value&&(o.value=n(o.value)),o}),r}const Ct=Array.prototype;function O(e,t,n,s,r,o){const i=ge(e),c=i!==e&&!N(e),a=i[t];if(a!==Ct[t]){const f=a.apply(e,o);return c?v(f):f}let u=n;i!==e&&(c?u=function(f,d){return n.call(this,v(f),d,e)}:n.length>2&&(u=function(f,d){return n.call(this,f,d,e)}));const l=a.call(i,u,s);return c&&r?r(l):l}function Pe(e,t,n,s){const r=ge(e);let o=n;return r!==e&&(N(e)?n.length>3&&(o=function(i,c,a){return n.call(this,i,c,a,e)}):o=function(i,c,a){return n.call(this,i,v(c),a,e)}),r[t](o,...s)}function me(e,t,n){const s=p(e);E(s,"iterate",H);const r=s[t](...n);return(r===-1||r===!1)&&Jt(n[0])?(n[0]=p(n[0]),s[t](...n)):r}function K(e,t,n=[]){fe(),ue();const s=p(e)[t].apply(e,n);return le(),de(),s}const Vt=xt("__proto__,__v_isRef,__isVue"),Ae=new Set(Object.getOwnPropertyNames(Symbol).filter(e=>e!=="arguments"&&e!=="caller").map(e=>Symbol[e]).filter(L));function kt(e){L(e)||(e=String(e));const t=p(this);return E(t,"has",e),t.hasOwnProperty(e)}class Le{constructor(t=!1,n=!1){this._isReadonly=t,this._isShallow=n}get(t,n,s){if(n==="__v_skip")return t.__v_skip;const r=this._isReadonly,o=this._isShallow;if(n==="__v_isReactive")return!r;if(n==="__v_isReadonly")return r;if(n==="__v_isShallow")return o;if(n==="__v_raw")return s===(r?o?Kt:Ke:o?Ht:He).get(t)||Object.getPrototypeOf(t)===Object.getPrototypeOf(s)?t:void 0;const i=z(t);if(!r){let a;if(i&&(a=zt[n]))return a;if(n==="hasOwnProperty")return kt}const c=Reflect.get(t,n,V(t)?t:s);return(L(n)?Ae.has(n):Vt(n))||(r||E(t,"get",n),o)?c:V(c)?i&&ie(n)?c:c.value:B(c)?r?Ue(c):qe(c):c}}class $t extends Le{constructor(t=!1){super(!1,t)}set(t,n,s,r){let o=t[n];if(!this._isShallow){const a=j(o);if(!N(s)&&!j(s)&&(o=p(o),s=p(s)),!z(t)&&V(o)&&!V(s))return a?!1:(o.value=s,!0)}const i=z(t)&&ie(n)?Number(n)<t.length:se(t,n),c=Reflect.set(t,n,s,V(t)?t:r);return t===p(r)&&(i?W(s,o)&&I(t,"set",n,s,o):I(t,"add",n,s)),c}deleteProperty(t,n){const s=se(t,n),r=t[n],o=Reflect.deleteProperty(t,n);return o&&s&&I(t,"delete",n,void 0,r),o}has(t,n){const s=Reflect.has(t,n);return(!L(n)||!Ae.has(n))&&E(t,"has",n),s}ownKeys(t){return E(t,"iterate",z(t)?"length":C),Reflect.ownKeys(t)}}class jt extends Le{constructor(t=!1){super(!0,t)}set(t,n){return!0}deleteProperty(t,n){return!0}}const Dt=new $t,Ft=new jt,ye=e=>e,Q=e=>Reflect.getPrototypeOf(e);function Pt(e,t,n){return function(...s){const r=this.__v_raw,o=p(r),i=Y(o),c=e==="entries"||e===Symbol.iterator&&i,a=e==="keys"&&i,u=r[e](...s),l=n?ye:t?we:v;return!t&&E(o,"iterate",a?pe:C),{next(){const{value:f,done:d}=u.next();return d?{value:f,done:d}:{value:c?[l(f[0]),l(f[1])]:l(f),done:d}},[Symbol.iterator](){return this}}}}function X(e){return function(...t){return e==="delete"?!1:e==="clear"?void 0:this}}function At(e,t){const n={get(r){const o=this.__v_raw,i=p(o),c=p(r);e||(W(r,c)&&E(i,"get",r),E(i,"get",c));const{has:a}=Q(i),u=t?ye:e?we:v;if(a.call(i,r))return u(o.get(r));if(a.call(i,c))return u(o.get(c));o!==i&&o.get(r)},get size(){const r=this.__v_raw;return!e&&E(p(r),"iterate",C),Reflect.get(r,"size",r)},has(r){const o=this.__v_raw,i=p(o),c=p(r);return e||(W(r,c)&&E(i,"has",r),E(i,"has",c)),r===c?o.has(r):o.has(r)||o.has(c)},forEach(r,o){const i=this,c=i.__v_raw,a=p(c),u=t?ye:e?we:v;return!e&&E(a,"iterate",C),c.forEach((l,f)=>r.call(o,u(l),u(f),i))}};return ze(n,e?{add:X("add"),set:X("set"),delete:X("delete"),clear:X("clear")}:{add(r){!t&&!N(r)&&!j(r)&&(r=p(r));const o=p(this);return Q(o).has.call(o,r)||(o.add(r),I(o,"add",r,r)),this},set(r,o){!t&&!N(o)&&!j(o)&&(o=p(o));const i=p(this),{has:c,get:a}=Q(i);let u=c.call(i,r);u||(r=p(r),u=c.call(i,r));const l=a.call(i,r);return i.set(r,o),u?W(o,l)&&I(i,"set",r,o,l):I(i,"add",r,o),this},delete(r){const o=p(this),{has:i,get:c}=Q(o);let a=i.call(o,r);a||(r=p(r),a=i.call(o,r));const u=c?c.call(o,r):void 0,l=o.delete(r);return a&&I(o,"delete",r,void 0,u),l},clear(){const r=p(this),o=r.size!==0,i=void 0,c=r.clear();return o&&I(r,"clear",void 0,void 0,i),c}}),["keys","values","entries",Symbol.iterator].forEach(r=>{n[r]=Pt(r,e,t)}),n}function We(e,t){const n=At(e,t);return(s,r,o)=>r==="__v_isReactive"?!e:r==="__v_isReadonly"?e:r==="__v_raw"?s:Reflect.get(se(n,r)&&r in s?n:s,r,o)}const Lt={get:We(!1,!1)},Wt={get:We(!0,!1)};function Wn(e,t,n){const s=p(n);if(s!==n&&t.call(e,s)){const r=ke(e);Rt(`Reactive ${r} contains both the raw and reactive versions of the same object${r==="Map"?" as keys":""}, which can lead to inconsistencies. Avoid differentiating between the raw and reactive versions of an object and only use the reactive version if possible.`)}}const He=new WeakMap,Ht=new WeakMap,Ke=new WeakMap,Kt=new WeakMap;function qt(e){switch(e){case"Object":case"Array":return 1;case"Map":case"Set":case"WeakMap":case"WeakSet":return 2;default:return 0}}function Ut(e){return e.__v_skip||!Object.isExtensible(e)?0:qt(ke(e))}function qe(e){return j(e)?e:Je(e,!1,Dt,Lt,He)}function Ue(e){return Je(e,!0,Ft,Wt,Ke)}function Je(e,t,n,s,r){if(!B(e)||e.__v_raw&&!(t&&e.__v_isReactive))return e;const o=Ut(e);if(o===0)return e;const i=r.get(e);if(i)return i;const c=new Proxy(e,o===2?s:n);return r.set(e,c),c}function j(e){return!!(e&&e.__v_isReadonly)}function N(e){return!!(e&&e.__v_isShallow)}function Jt(e){return e?!!e.__v_raw:!1}function p(e){const t=e&&e.__v_raw;return t?p(t):e}const v=e=>B(e)?qe(e):e,we=e=>B(e)?Ue(e):e;function V(e){return e?e.__v_isRef===!0:!1}function Yt(e){return Bt(e,!1)}function Bt(e,t){return V(e)?e:new Gt(e,t)}class Gt{constructor(t,n){this.dep=new Fe,this.__v_isRef=!0,this.__v_isShallow=!1,this._rawValue=n?t:p(t),this._value=n?t:v(t),this.__v_isShallow=n}get value(){return this.dep.track(),this._value}set value(t){const n=this._rawValue,s=this.__v_isShallow||N(t)||j(t);t=s?t:p(t),W(t,n)&&(this._rawValue=t,this._value=s?t:v(t),this.dep.trigger())}}/**
* @vue/runtime-core v3.5.17
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/const k=[];function Hn(e){k.push(e)}function Kn(){k.pop()}let be=!1;function qn(e,...t){if(be)return;be=!0,fe();const n=k.length?k[k.length-1].component:null,s=n&&n.appContext.config.warnHandler,r=Qt();if(s)ve(s,n,11,[e+t.map(o=>{var i,c;return(c=(i=o.toString)==null?void 0:i.call(o))!=null?c:JSON.stringify(o)}).join(""),n&&n.proxy,r.map(({vnode:o})=>`at <${et(n,o.type)}>`).join(`
`),r]);else{const o=[`[Vue warn]: ${e}`,...t];r.length&&o.push(`
`,...Xt(r)),console.warn(...o)}de(),be=!1}function Qt(){let e=k[k.length-1];if(!e)return[];const t=[];for(;e;){const n=t[0];n&&n.vnode===e?n.recurseCount++:t.push({vnode:e,recurseCount:0});const s=e.component&&e.component.parent;e=s&&s.vnode}return t}function Xt(e){const t=[];return e.forEach((n,s)=>{t.push(...s===0?[]:[`
`],...Zt(n))}),t}function Zt({vnode:e,recurseCount:t}){const n=t>0?`... (${t} recursive calls)`:"",s=e.component?e.component.parent==null:!1,r=` at <${et(e.component,e.type,s)}`,o=">"+n;return e.props?[r,...en(e.props),o]:[r+o]}function en(e){const t=[],n=Object.keys(e);return n.slice(0,3).forEach(s=>{t.push(...Ye(s,e[s]))}),n.length>3&&t.push(" ..."),t}function Ye(e,t,n){return Ce(t)?(t=JSON.stringify(t),n?t:[`${e}=${t}`]):typeof t=="number"||typeof t=="boolean"||t==null?n?t:[`${e}=${t}`]:V(t)?(t=Ye(e,p(t.value),!0),n?t:[`${e}=Ref<`,t,">"]):oe(t)?[`${e}=fn${t.name?`<${t.name}>`:""}`]:(t=p(t),n?t:[`${e}=`,t])}const Un={sp:"serverPrefetch hook",bc:"beforeCreate hook",c:"created hook",bm:"beforeMount hook",m:"mounted hook",bu:"beforeUpdate hook",u:"updated",bum:"beforeUnmount hook",um:"unmounted hook",a:"activated hook",da:"deactivated hook",ec:"errorCaptured hook",rtc:"renderTracked hook",rtg:"renderTriggered hook",0:"setup function",1:"render function",2:"watcher getter",3:"watcher callback",4:"watcher cleanup function",5:"native event handler",6:"component event handler",7:"vnode hook",8:"directive hook",9:"transition hook",10:"app errorHandler",11:"app warnHandler",12:"ref function",13:"async component loader",14:"scheduler flush",15:"component update",16:"app unmount cleanup function"};function ve(e,t,n,s){try{return s?e(...s):e()}catch(r){Be(r,t,n)}}function Be(e,t,n,s=!0){const r=t?t.vnode:null,{errorHandler:o,throwUnhandledErrorInProduction:i}=t&&t.appContext.config||Ot;if(t){let c=t.parent;const a=t.proxy,u=`https://vuejs.org/error-reference/#runtime-${n}`;for(;c;){const l=c.ec;if(l){for(let f=0;f<l.length;f++)if(l[f](e,a,u)===!1)return}c=c.parent}if(o){fe(),ve(o,null,10,[e,a,u]),de();return}}tn(e,n,r,s,i)}function tn(e,t,n,s=!0,r=!1){if(r)throw e;console.error(e)}const x=[];let S=-1;const D=[];let M=null,F=0;const nn=Promise.resolve();let Ee=null;const rn=100;function sn(e){let t=S+1,n=x.length;for(;t<n;){const s=t+n>>>1,r=x[s],o=q(r);o<e||o===e&&r.flags&2?t=s+1:n=s}return t}function on(e){if(!(e.flags&1)){const t=q(e),n=x[x.length-1];!n||!(e.flags&2)&&t>=q(n)?x.push(e):x.splice(sn(t),0,e),e.flags|=1,Ge()}}function Ge(){Ee||(Ee=nn.then(Qe))}function cn(e){z(e)?D.push(...e):M&&e.id===-1?M.splice(F+1,0,e):e.flags&1||(D.push(e),e.flags|=1),Ge()}function an(e){if(D.length){const t=[...new Set(D)].sort((n,s)=>q(n)-q(s));if(D.length=0,M){M.push(...t);return}for(M=t,F=0;F<M.length;F++){const n=M[F];n.flags&4&&(n.flags&=-2),n.flags&8||n(),n.flags&=-2}M=null,F=0}}const q=e=>e.id==null?e.flags&2?-1:1/0:e.id;function Qe(e){const t=St;try{for(S=0;S<x.length;S++){const n=x[S];n&&!(n.flags&8)&&(n.flags&4&&(n.flags&=-2),ve(n,n.i,n.i?15:14),n.flags&4||(n.flags&=-2))}}finally{for(;S<x.length;S++){const n=x[S];n&&(n.flags&=-2)}S=-1,x.length=0,an(e),Ee=null,(x.length||D.length)&&Qe(e)}}function Jn(e,t){const n=e.get(t)||0;if(n>rn){const s=t.i,r=s&&Ze(s.type);return Be(`Maximum recursive updates exceeded${r?` in component <${r}>`:""}. This means you have a reactive effect that is mutating its own dependencies and thus recursively triggering itself. Possible sources include component template, render function, updated hook or watcher source function.`,null,10),!0}return e.set(t,n+1),!1}const xe=new Map,Z=new Map;function Yn(e,t){return Z.has(e)?!1:(Z.set(e,{initialDef:ee(t),instances:new Set}),!0)}function ee(e){return fn(e)?e.__vccOpts:e}function Bn(e,t){const n=Z.get(e);n&&(n.initialDef.render=t,[...n.instances].forEach(s=>{t&&(s.render=t,ee(s.type).render=t),s.renderCache=[],s.update()}))}function Gn(e,t){const n=Z.get(e);if(!n)return;t=ee(t),Xe(n.initialDef,t);const s=[...n.instances];for(let r=0;r<s.length;r++){const o=s[r],i=ee(o.type);let c=xe.get(i);c||(i!==n.initialDef&&Xe(i,t),xe.set(i,c=new Set)),c.add(o),o.appContext.propsCache.delete(o.type),o.appContext.emitsCache.delete(o.type),o.appContext.optionsCache.delete(o.type),o.ceReload?(c.add(o),o.ceReload(t.styles),c.delete(o)):o.parent?on(()=>{o.parent.update(),c.delete(o)}):o.appContext.reload?o.appContext.reload():typeof window<"u"?window.location.reload():console.warn("[HMR] Root or manually mounted instance modified. Full reload required."),o.root.ce&&o!==o.root&&o.root.ce._removeChildStyle(i)}cn(()=>{xe.clear()})}function Xe(e,t){ze(e,t);for(const n in e)n!=="__file"&&!(n in t)&&delete e[n]}function Qn(e){return(t,n)=>{try{return e(t,n)}catch(s){console.error(s),console.warn("[HMR] Something went wrong during Vue component hot-reload. Full reload required.")}}}ce().requestIdleCallback,ce().cancelIdleCallback;const Xn={};{const e=ce(),t=(n,s)=>{let r;return(r=e[n])||(r=e[n]=[]),r.push(s),o=>{r.length>1?r.forEach(i=>i(o)):r[0](o)}};t("__VUE_INSTANCE_SETTERS__",n=>n),t("__VUE_SSR_SETTERS__",n=>n)}const un=/(?:^|[-_])(\w)/g,ln=e=>e.replace(un,t=>t.toUpperCase()).replace(/[-_]/g,"");function Ze(e,t=!0){return oe(e)?e.displayName||e.name:e.name||t&&e.__name}function et(e,t,n=!1){let s=Ze(t);if(!s&&t.__file){const r=t.__file.match(/([^/\\]+)\.\w+$/);r&&(s=r[1])}if(!s&&e&&e.parent){const r=o=>{for(const i in o)if(o[i]===t)return i};s=r(e.components||e.parent.type.components)||r(e.appContext.components)}return s?ln(s):n?"App":"Anonymous"}function fn(e){return oe(e)&&"__vccOpts"in e}const tt=()=>document.documentElement.getAttribute("data-theme")==="dark";[...new Array(6)].map((e,t)=>`[vp-content] h${t+1}`).join(",");const{entries:dn}=Object,{fromEntries:hn}=Object,nt=Yt(!1);typeof document<"u"&&(nt.value=tt(),new MutationObserver(()=>{nt.value=tt()}).observe(document.documentElement,{attributeFilter:["data-theme"],attributes:!0}));const pn="ENTRIES",rt="KEYS",st="VALUES",w="";class Oe{set;_type;_path;constructor(t,n){const s=t._tree,r=Array.from(s.keys());this.set=t,this._type=n,this._path=r.length>0?[{node:s,keys:r}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:n}=P(this._path);if(P(n)===w)return{done:!1,value:this.result()};const s=t.get(P(n));return this._path.push({node:s,keys:Array.from(s.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=P(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>P(t)).filter(t=>t!==w).join("")}value(){return P(this._path).node.get(w)}result(){switch(this._type){case st:return this.value();case rt:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const P=e=>e[e.length-1],gn=(e,t,n)=>{const s=new Map;if(typeof t!="string")return s;const r=t.length+1,o=r+n,i=new Uint8Array(o*r).fill(n+1);for(let c=0;c<r;++c)i[c]=c;for(let c=1;c<o;++c)i[c*r]=c;return ot(e,t,n,s,i,1,r,""),s},ot=(e,t,n,s,r,o,i,c)=>{const a=o*i;e:for(const u of e.keys())if(u===w){const l=r[a-1];l<=n&&s.set(c,[e.get(u),l])}else{let l=o;for(let f=0;f<u.length;++f,++l){const d=u[f],m=i*l,b=m-i;let g=r[m];const h=Math.max(0,l-n-1),_=Math.min(i-1,l+n);for(let y=h;y<_;++y){const R=d!==t[y],re=r[b+y]+ +R,J=r[b+y+1]+1,T=r[m+y]+1,A=r[m+y+1]=Math.min(re,J,T);A<g&&(g=A)}if(g>n)continue e}ot(e.get(u),t,n,s,r,l,i,c+u)}};let it=class U{_tree;_prefix;_size=void 0;constructor(t=new Map,n=""){this._tree=t,this._prefix=n}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[n,s]=te(this._tree,t.slice(this._prefix.length));if(n===void 0){const[r,o]=Ne(s);for(const i of r.keys())if(i!==w&&i.startsWith(o)){const c=new Map;return c.set(i.slice(o.length),r.get(i)),new U(c,t)}}return new U(n,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,_n(this._tree,t)}entries(){return new Oe(this,pn)}forEach(t){for(const[n,s]of this)t(n,s,this)}fuzzyGet(t,n){return gn(this._tree,t,n)}get(t){const n=Se(this._tree,t);return n!==void 0?n.get(w):void 0}has(t){return Se(this._tree,t)?.has(w)??!1}keys(){return new Oe(this,rt)}set(t,n){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,Ie(this._tree,t).set(w,n),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,n){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const s=Ie(this._tree,t);return s.set(w,n(s.get(w))),this}fetch(t,n){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const s=Ie(this._tree,t);let r=s.get(w);return r===void 0&&s.set(w,r=n()),r}values(){return new Oe(this,st)}[Symbol.iterator](){return this.entries()}static from(t){const n=new U;for(const[s,r]of t)n.set(s,r);return n}static fromObject(t){return U.from(Object.entries(t))}};const te=(e,t,n=[])=>{if(t.length===0||e==null)return[e,n];for(const s of e.keys())if(s!==w&&t.startsWith(s))return n.push([e,s]),te(e.get(s),t.slice(s.length),n);return n.push([e,t]),te(void 0,"",n)},Se=(e,t)=>{if(t.length===0||!e)return e;for(const n of e.keys())if(n!==w&&t.startsWith(n))return Se(e.get(n),t.slice(n.length))},Ie=(e,t)=>{const n=t.length;e:for(let s=0;e&&s<n;){for(const o of e.keys())if(o!==w&&t[s]===o[0]){const i=Math.min(n-s,o.length);let c=1;for(;c<i&&t[s+c]===o[c];)++c;const a=e.get(o);if(c===o.length)e=a;else{const u=new Map;u.set(o.slice(c),a),e.set(t.slice(s,s+c),u),e.delete(o),e=u}s+=c;continue e}const r=new Map;return e.set(t.slice(s),r),r}return e},_n=(e,t)=>{const[n,s]=te(e,t);if(n!==void 0){if(n.delete(w),n.size===0)ct(s);else if(n.size===1){const[r,o]=n.entries().next().value;at(s,r,o)}}},ct=e=>{if(e.length===0)return;const[t,n]=Ne(e);if(t.delete(n),t.size===0)ct(e.slice(0,-1));else if(t.size===1){const[s,r]=t.entries().next().value;s!==w&&at(e.slice(0,-1),s,r)}},at=(e,t,n)=>{if(e.length===0)return;const[s,r]=Ne(e);s.set(r+t,n),s.delete(r)},Ne=e=>e[e.length-1],mn=(e,t)=>{const n=e._idToShortId.get(t);if(n!=null)return e._storedFields.get(n)},yn=/[\n\r\p{Z}\p{P}]+/u,Me="or",ut="and",wn="and_not",bn=(e,t)=>{e.includes(t)||e.push(t)},lt=(e,t)=>{for(const n of t)e.includes(n)||e.push(n)},ft=({score:e},{score:t})=>t-e,vn=()=>new Map,ne=e=>{const t=new Map;for(const n of Object.keys(e))t.set(parseInt(n,10),e[n]);return t},dt=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,ht={[Me]:(e,t)=>{for(const n of t.keys()){const s=e.get(n);if(s==null)e.set(n,t.get(n));else{const{score:r,terms:o,match:i}=t.get(n);s.score=s.score+r,s.match=Object.assign(s.match,i),lt(s.terms,o)}}return e},[ut]:(e,t)=>{const n=new Map;for(const s of t.keys()){const r=e.get(s);if(r==null)continue;const{score:o,terms:i,match:c}=t.get(s);lt(r.terms,i),n.set(s,{score:r.score+o,terms:r.terms,match:Object.assign(r.match,c)})}return n},[wn]:(e,t)=>{for(const n of t.keys())e.delete(n);return e}},En=(e,t,n,s,r,o)=>{const{k:i,b:c,d:a}=o;return Math.log(1+(n-t+.5)/(t+.5))*(a+e*(i+1)/(e+i*(1-c+c*s/r)))},xn=e=>(t,n,s)=>({term:t,fuzzy:typeof e.fuzzy=="function"?e.fuzzy(t,n,s):e.fuzzy??!1,prefix:typeof e.prefix=="function"?e.prefix(t,n,s):e.prefix===!0,termBoost:typeof e.boostTerm=="function"?e.boostTerm(t,n,s):1}),pt=(e,t,n,s)=>{for(const r of Object.keys(e._fieldIds))if(e._fieldIds[r]===n){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${s}" was not present in field "${r}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},On=(e,t,n,s)=>{if(!e._index.has(s)){pt(e,n,t,s);return}const r=e._index.fetch(s,vn),o=r.get(t),i=o?.get(n);!o||typeof i>"u"?pt(e,n,t,s):i<=1?o.size<=1?r.delete(t):o.delete(n):o.set(n,i-1),e._index.get(s).size===0&&e._index.delete(s)},Sn={k:1.2,b:.7,d:.5},In={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(yn),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{console?.[e]?.(t)},autoVacuum:!0},gt={combineWith:Me,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:Sn},Nn={combineWith:ut,prefix:(e,t,n)=>t===n.length-1},Mn={batchSize:1e3,batchWait:10},_t={minDirtFactor:.1,minDirtCount:20},Rn={...Mn,..._t},mt=Symbol("*"),Tn=(e,t)=>{const n=new Map,s={...e._options.searchOptions,...t};for(const[r,o]of e._documentIds){const i=s.boostDocument?s.boostDocument(o,"",e._storedFields.get(r)):1;n.set(r,{score:i,terms:[],match:{}})}return n},yt=(e,t=Me)=>{if(e.length===0)return new Map;const n=t.toLowerCase();if(!(n in ht))throw new Error(`Invalid combination operator: ${t}`);return e.reduce(ht[n])},Re=(e,t,n,s,r,o,i,c,a,u=new Map)=>{if(o==null)return u;for(const l of Object.keys(i)){const f=i[l],d=e._fieldIds[l],m=o.get(d);if(m==null)continue;let b=m.size;const g=e._avgFieldLength[d];for(const h of m.keys()){if(!e._documentIds.has(h)){On(e,d,h,n),b-=1;continue}const _=c?c(e._documentIds.get(h),n,e._storedFields.get(h)):1;if(!_)continue;const y=m.get(h),R=e._fieldLength.get(h)[d],re=En(y,b,e._documentCount,R,g,a),J=s*r*f*_*re,T=u.get(h);if(T){T.score+=J,bn(T.terms,t);const A=dt(T.match,n);A?A.push(l):T.match[n]=[l]}else u.set(h,{score:J,terms:[t],match:{[n]:[l]}})}}return u},zn=(e,t,n)=>{const s={...e._options.searchOptions,...n},r=(s.fields??e._options.fields).reduce((g,h)=>({...g,[h]:dt(s.boost,h)||1}),{}),{boostDocument:o,weights:i,maxFuzzy:c,bm25:a}=s,{fuzzy:u,prefix:l}={...gt.weights,...i},f=e._index.get(t.term),d=Re(e,t.term,t.term,1,t.termBoost,f,r,o,a);let m,b;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const g=t.fuzzy===!0?.2:t.fuzzy,h=g<1?Math.min(c,Math.round(t.term.length*g)):g;h&&(b=e._index.fuzzyGet(t.term,h))}if(m)for(const[g,h]of m){const _=g.length-t.term.length;if(!_)continue;b?.delete(g);const y=l*g.length/(g.length+.3*_);Re(e,t.term,g,y,t.termBoost,h,r,o,a,d)}if(b)for(const g of b.keys()){const[h,_]=b.get(g);if(!_)continue;const y=u*g.length/(g.length+_);Re(e,t.term,g,y,t.termBoost,h,r,o,a,d)}return d},wt=(e,t,n={})=>{if(t===mt)return Tn(e,n);if(typeof t!="string"){const l={...n,...t,queries:void 0},f=t.queries.map(d=>wt(e,d,l));return yt(f,l.combineWith)}const{tokenize:s,processTerm:r,searchOptions:o}=e._options,i={tokenize:s,processTerm:r,...o,...n},{tokenize:c,processTerm:a}=i,u=c(t).flatMap(l=>a(l)).filter(l=>!!l).map(xn(i)).map(l=>zn(e,l,i));return yt(u,i.combineWith)},bt=(e,t,n={})=>{const{searchOptions:s}=e._options,r={...s,...n},o=wt(e,t,n),i=[];for(const[c,{score:a,terms:u,match:l}]of o){const f=u.length||1,d={id:e._documentIds.get(c),score:a*f,terms:Object.keys(l),queryTerms:u,match:l};Object.assign(d,e._storedFields.get(c)),(r.filter==null||r.filter(d))&&i.push(d)}return t===mt&&r.boostDocument==null||i.sort(ft),i},Cn=(e,t,n={})=>{n={...e._options.autoSuggestOptions,...n};const s=new Map;for(const{score:o,terms:i}of bt(e,t,n)){const c=i.join(" "),a=s.get(c);a!=null?(a.score+=o,a.count+=1):s.set(c,{score:o,terms:i,count:1})}const r=[];for(const[o,{score:i,terms:c,count:a}]of s)r.push({suggestion:o,terms:c,score:i/a});return r.sort(ft),r};class Vn{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(!t?.fields)throw new Error('SlimSearch: option "fields" must be provided');const n=t.autoVacuum==null||t.autoVacuum===!0?Rn:t.autoVacuum;this._options={...In,...t,autoVacuum:n,searchOptions:{...gt,...t.searchOptions},autoSuggestOptions:{...Nn,...t.autoSuggestOptions}},this._index=new it,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=_t,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[n,s]of this._index){const r={};for(const[o,i]of s)r[o]=Object.fromEntries(i);t.push([n,r])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,version:2}}addFields(t){for(let n=0;n<t.length;n++)this._fieldIds[t[n]]=n}}const kn=e=>new Vn(e),$n=({documentCount:e,nextId:t,fieldIds:n,averageFieldLength:s,dirtCount:r,version:o},i)=>{if(o!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const c=kn(i);return c._documentCount=e,c._nextId=t,c._idToShortId=new Map,c._fieldIds=n,c._avgFieldLength=s,c._dirtCount=r??0,c._index=new it,c},jn=(e,t)=>{const{index:n,documentIds:s,fieldLength:r,storedFields:o}=e,i=$n(e,t);i._documentIds=ne(s),i._fieldLength=ne(r),i._storedFields=ne(o);for(const[c,a]of i._documentIds)i._idToShortId.set(a,c);for(const[c,a]of n){const u=new Map;for(const l of Object.keys(a))u.set(parseInt(l,10),ne(a[l]));i._index.set(c,u)}return i},Te=(e,t)=>{const n=e.toLowerCase(),s=t.toLowerCase(),r=[];let o=0,i=0;const c=(u,l=!1)=>{let f;i===0?f=u.length>20?`… ${u.slice(-20)}`:u:l?f=u.length+i>100?`${u.slice(0,100-i)}… `:u:f=u.length>20?`${u.slice(0,20)} … ${u.slice(-20)}`:u,f&&r.push(f),i+=f.length,l||(r.push(["mark",t]),i+=t.length,i>=100&&r.push(" …"))};let a=n.indexOf(s,o);if(a===-1)return null;for(;a>=0;){const u=a+s.length;if(c(e.slice(o,a)),o=u,i>100)break;a=n.indexOf(s,o)}return i<100&&c(e.slice(o),!0),r},{entries:Dn}=Object,Fn=(e,t)=>t.contents.reduce((n,[,s])=>n+s,0)-e.contents.reduce((n,[,s])=>n+s,0),Pn=(e,t)=>Math.max(...t.contents.map(([,n])=>n))-Math.max(...e.contents.map(([,n])=>n)),vt=(e,t,n={},s="max")=>{const r={};return bt(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...n}).forEach(o=>{const{id:i,terms:c,score:a}=o,u=i.includes("@"),l=i.includes("#"),[f,d]=i.split(/[#@]/),m=Number(f),b=c.sort((h,_)=>h.length-_.length).filter((h,_)=>c.slice(_+1).every(y=>!y.includes(h))),{contents:g}=r[m]??={title:"",contents:[]};if(u)g.push([{type:"customField",id:m,index:d,display:b.map(h=>o.c.map(_=>Te(_,h))).flat().filter(h=>h!==null)},a]);else{const h=b.map(_=>Te(o.h,_)).filter(_=>_!==null);if(h.length&&g.push([{type:l?"heading":"title",id:m,...l&&{anchor:d},display:h},a]),"t"in o&&o.t)for(const _ of o.t){const y=b.map(R=>Te(_,R)).filter(R=>R!==null);y.length&&g.push([{type:"text",id:m,...l&&{anchor:d},display:y},a])}}}),Dn(r).sort(([,o],[,i])=>(s?Fn:Pn)(o,i)).map(([o,{title:i,contents:c}])=>{if(!i){const a=mn(t,o);a&&(i=a.h)}return{title:i,contents:c.map(([a])=>a)}})},Et=(e,t,n={})=>{const s=Cn(t,e,{fuzzy:.2,maxFuzzy:3,...n}).map(({suggestion:r})=>r);return e.includes(" ")?s:s.filter(r=>!r.includes(" "))},An=hn(dn(JSON.parse("{\"/\":{\"documentCount\":692,\"nextId\":692,\"documentIds\":{\"0\":\"0\",\"1\":\"1\",\"2\":\"1#binary-oracle\",\"3\":\"1#elowen\",\"4\":\"2\",\"5\":\"2#摘要\",\"6\":\"2#简介\",\"7\":\"2#相关工作\",\"8\":\"2#方法\",\"9\":\"2#_3-2-multi-head-affordance-chain-of-thought\",\"10\":\"2#fine-tuning-mllm\",\"11\":\"2#object-head-reasoning-几何推理\",\"12\":\"2#affordance-head-reasoning-类比推理\",\"13\":\"2#knowledge-encoding-and-integration\",\"14\":\"2#_3-3-cross-modal-adaptive-fusion-module-cmafm\",\"15\":\"2#_3-4-decoder-and-loss-functions\",\"16\":\"2#数据集\",\"17\":\"2#数据收集-collection\",\"18\":\"2#标注策略-annotation\",\"19\":\"2#统计分析-statistical-analysis\",\"20\":\"2#数据划分-data-partitions\",\"21\":\"2#实验\",\"22\":\"2#_5-1-benchmark-setting\",\"23\":\"2#_5-2-comparison-results\",\"24\":\"2#_5-3-ablation-study\",\"25\":\"2#_5-4-performance-analysis\",\"26\":\"2#结论\",\"27\":\"2#代码\",\"28\":\"2#multi-head-affordance-chain-of-thought\",\"29\":\"2#数据集-1\",\"30\":\"2#模型\",\"31\":\"2#文本编码\",\"32\":\"2#改良的交叉注意力\",\"33\":\"2#几何结构信息与交互信息的融合\",\"34\":\"2#交互信息与图像特征的融合\",\"35\":\"2#解码阶段\",\"36\":\"2#点云特征与几何结构特征的融合\",\"37\":\"3\",\"38\":\"3#环境配置-待完善\",\"39\":\"3#模型结构\",\"40\":\"3#lmaffordance3d\",\"41\":\"3#step-2-融合多模态空间特征\",\"42\":\"3#step-3-多模态特征投影到语言语义空间\",\"43\":\"3#step-6-拼接多模态嵌入与语言嵌入\",\"44\":\"3#step-8-降维适配器\",\"45\":\"3#step-9-解码器融合所有特征以预测可操作性特征\",\"46\":\"3#step-10-使用分割头预测最终的-3d-可操作性热图\",\"47\":\"4\",\"48\":\"4#摘要\",\"49\":\"4#简介\",\"50\":\"4#相关工作\",\"51\":\"4#_1-功能学习-affordance-learning\",\"52\":\"4#_2-图像-点云跨模态学习\",\"53\":\"4#方法\",\"54\":\"4#_1-整体框架-iag网络\",\"55\":\"4#_2-损失函数\",\"56\":\"4#_3-关键创新\",\"57\":\"4#代码\",\"58\":\"4#数据集\",\"59\":\"4#模型\",\"60\":\"5\",\"61\":\"5#数据集\",\"62\":\"5#_1-基础数据来源\",\"63\":\"5#_2-构建问题-question-crafting\",\"64\":\"5#_3-标注-gt-mask-ground-truth-mask\",\"65\":\"5#_4-数据集组织方式\",\"66\":\"5#_5-数据增强与配对策略\",\"67\":\"5#_6-数据集统计信息-来自论文图3\",\"68\":\"5#_7-代码实现\",\"69\":\"5#_8-总结\",\"70\":\"5#模型实现\",\"71\":\"5#afm-自适应融合模块\",\"72\":\"5#_1️⃣-grouping-文本引导的点特征分组\",\"73\":\"5#_2️⃣-mixing-mlp-mixer-进行组内和通道间的信息混合\",\"74\":\"5#_3️⃣-ungrouping-将融合特征映射回点空间\",\"75\":\"5#_4️⃣-afm-自适应融合模块\",\"76\":\"5#rpo-参考点解码器\",\"77\":\"5#损失函数\",\"78\":\"5#hm-loss-hybrid-mask-loss\",\"79\":\"5#训练\",\"80\":\"5#准备\",\"81\":\"5#训练-1\",\"82\":\"5#评估\",\"83\":\"5#复现\",\"84\":\"6\",\"85\":\"7\",\"86\":\"7#背景\",\"87\":\"7#模型结构\",\"88\":\"7#层次化点集特征学习\",\"89\":\"7#sampling-layer\",\"90\":\"7#grouping-layer\",\"91\":\"7#pointnet-layer\",\"92\":\"7#代码实现\",\"93\":\"7#单尺度分组分类模型\",\"94\":\"7#非均匀密度下稳定的特征学习\",\"95\":\"7#多尺度分组-multi-scale-grouping\",\"96\":\"7#多尺度分组分类模型\",\"97\":\"7#多分辨率分组-multi-resolution-grouping\",\"98\":\"7#点云语义分割\",\"99\":\"7#代码实现-1\",\"100\":\"7#特征传播层\",\"101\":\"7#点云语义分割模型\",\"102\":\"8\",\"103\":\"8#核心\",\"104\":\"8#难点\",\"105\":\"8#解决方案\",\"106\":\"8#代码-pytorch版本\",\"107\":\"8#输入标准化\",\"108\":\"8#正则化损失\",\"109\":\"8#特征提取\",\"110\":\"8#分类任务\",\"111\":\"8#分割任务\",\"112\":\"8#缺陷\",\"113\":\"8#背景知识扫盲-可选\",\"114\":\"8#点云\",\"115\":\"8#对称函数\",\"116\":\"8#刚性运动\",\"117\":\"8#正交变换\",\"118\":\"9\",\"119\":\"10\",\"120\":\"10#introduction\",\"121\":\"10#related-work\",\"122\":\"10#视觉-语言预训练-vlp\",\"123\":\"10#知识蒸馏-knowledge-distillation\",\"124\":\"10#数据增强-data-augmentation\",\"125\":\"10#method\",\"126\":\"10#模型架构-med-multimodal-mixture-of-encoder-decoder\",\"127\":\"10#预训练目标-itc、itm、lm\",\"128\":\"10#capfilt-图文数据的自举式清洗机制\",\"129\":\"10#小结\",\"130\":\"10#experiments-and-discussions\",\"131\":\"10#预训练细节\",\"132\":\"10#capfilt-效果验证\",\"133\":\"10#合成文本的多样性对性能的影响\",\"134\":\"10#编码器-解码器参数共享与解耦\",\"135\":\"10#ablation-study\",\"136\":\"10#capfilt-的性能提升并非源于更长的训练时间\",\"137\":\"10#应使用-bootstrapped-数据集重新训练模型\",\"138\":\"10#conclusion\",\"139\":\"10#code-implementation\",\"140\":\"10#capfilt-模块实现\",\"141\":\"10#captioner-模块\",\"142\":\"10#微调阶段\",\"143\":\"10#生成阶段\",\"144\":\"10#filter-模块\",\"145\":\"10#微调阶段-1\",\"146\":\"10#过滤阶段\",\"147\":\"10#blip-预训练\",\"148\":\"11\",\"149\":\"11#introduction\",\"150\":\"11#related-work\",\"151\":\"11#albef\",\"152\":\"11#model-structure\",\"153\":\"11#pre-training-objectives\",\"154\":\"11#image-text-contrastive-learning\",\"155\":\"11#masked-language-modeling-mlm\",\"156\":\"11#image-text-matching-itm\",\"157\":\"11#momentum-distillation\",\"158\":\"11#code-implementation\",\"159\":\"11#train\",\"160\":\"11#model-init\",\"161\":\"11#itc\",\"162\":\"11#itm\",\"163\":\"11#mlm\",\"164\":\"12\",\"165\":\"12#摘要\",\"166\":\"12#简介\",\"167\":\"12#方法\",\"168\":\"12#图像表示\",\"169\":\"12#图像块\",\"170\":\"12#视觉标记\",\"171\":\"12#骨干网络-图像-transformer\",\"172\":\"12#beit-预训练-掩码图像建模\",\"173\":\"12#变分自编码器视角\",\"174\":\"12#预训练设置\",\"175\":\"12#下游任务微调\",\"176\":\"13\",\"177\":\"13#摘要\",\"178\":\"13#简介\",\"179\":\"14\",\"180\":\"14#摘要\",\"181\":\"14#简介\",\"182\":\"14#相关工作\",\"183\":\"14#_1-视觉基础模型-vision-foundation-models\",\"184\":\"14#_2-大语言模型-large-language-models-llms\",\"185\":\"14#_3-视觉大语言模型-vision-large-language-models-vllms\",\"186\":\"14#核心问题与本文定位\",\"187\":\"14#方法\",\"188\":\"14#_1-整体架构设计\",\"189\":\"14#_2-模型设计\",\"190\":\"14#_3-对齐策略\",\"191\":\"14#实现细节\",\"192\":\"14#实验\",\"193\":\"14#视觉感知能力验证-visual-perception-benchmarks\",\"194\":\"14#视觉-语言任务能力-vision-language-benchmarks\",\"195\":\"14#多模态对话任务-multi-modal-dialogue-benchmarks\",\"196\":\"14#消融实验-ablation-study\",\"197\":\"14#总结\",\"198\":\"14#结论\",\"199\":\"14#详细训练设置-附录内容\",\"200\":\"14#第一阶段设置-stage-1\",\"201\":\"14#第二阶段设置-stage-2\",\"202\":\"14#第三阶段设置-stage-3\",\"203\":\"14#检索任务微调设置\",\"204\":\"14#imagenet-线性探测设置\",\"205\":\"14#ade20k-语义分割设置\",\"206\":\"15\",\"207\":\"15#摘要\",\"208\":\"15#简介\",\"209\":\"15#相关工作\",\"210\":\"15#_1-商业专有多模态大模型-proprietary-commercial-mllms\",\"211\":\"15#_2-开源多模态大模型-open-source-mllms\",\"212\":\"15#_3-视觉基础模型-vision-foundation-models-vfms\",\"213\":\"15#方法\",\"214\":\"15#_1-整体架构\",\"215\":\"15#_2-强视觉编码器\",\"216\":\"15#_3-动态高分辨率策略\",\"217\":\"15#_4-高质量双语数据集\",\"218\":\"15#实验\",\"219\":\"15#_1-实现细节\",\"220\":\"15#_2-基准测试结果\",\"221\":\"15#_3-关键消融研究\",\"222\":\"15#结论\",\"223\":\"16\",\"224\":\"16#背景\",\"225\":\"16#方法\",\"226\":\"16#预训练\",\"227\":\"16#微调\",\"228\":\"16#联合-gpt-4-的推理机制-ensemble-with-gpt-4\",\"229\":\"16#ablation-study-消融实验\",\"230\":\"16#补充\",\"231\":\"16#辨析-instruction-tuning-和-prompt-tuning\",\"232\":\"17\",\"233\":\"17#introduction\",\"234\":\"17#what-is-contrast-learning\",\"235\":\"17#instance-discrimination-task\",\"236\":\"17#momentum-contrast\",\"237\":\"17#abstract\",\"238\":\"17#introduction-1\",\"239\":\"17#conclusion\",\"240\":\"17#related-work\",\"241\":\"17#detail\",\"242\":\"17#preview-work\",\"243\":\"17#code-implementation\",\"244\":\"17#train-code\",\"245\":\"17#model-implementation\",\"246\":\"17#model-init\",\"247\":\"17#model-forward\",\"248\":\"17#momentum-update\",\"249\":\"17#dequeue-and-enqueue\",\"250\":\"18\",\"251\":\"19\",\"252\":\"20\",\"253\":\"20#introduction\",\"254\":\"20#motivation\",\"255\":\"20#method\",\"256\":\"20#modality-interaction-schema\",\"257\":\"20#model-structure\",\"258\":\"20#pretraining-objectives\",\"259\":\"20#conclusion\",\"260\":\"21\",\"261\":\"21#多模态-bert-前向传播流程\",\"262\":\"21#_1-整体流程总览-bertmodel\",\"263\":\"21#_2-编码器-bertencoder\",\"264\":\"21#_3-transformer-层-bertlayer\",\"265\":\"21#_4-attention-模块-bertattention\",\"266\":\"21#_5-核心计算-bertselfattention\",\"267\":\"21#_6-小结\",\"268\":\"21#自回归语言建模\",\"269\":\"22\",\"270\":\"22#引言\",\"271\":\"22#介绍\",\"272\":\"22#训练\",\"273\":\"22#推理\",\"274\":\"22#文本描述生成\",\"275\":\"22#花卉图片分类\",\"276\":\"22#文字搜索图像\",\"277\":\"22#完整代码\",\"278\":\"22#小结\",\"279\":\"23\",\"280\":\"23#背景\",\"281\":\"23#模型结构\",\"282\":\"23#stage-1-representation-learning-表征学习\",\"283\":\"23#_1、image-text-contrastive-learning-itc-loss-clip-like\",\"284\":\"23#_2、image-text-matching-itm-loss-二分类task\",\"285\":\"23#_3、image-grounded-text-generation-itg-loss-gpt-like\",\"286\":\"23#stage-2-generative-learning-生成学习\",\"287\":\"24\",\"288\":\"24#原理\",\"289\":\"24#_0-数据下载\",\"290\":\"24#_1-图片预处理\",\"291\":\"24#_2-图片切割\",\"292\":\"24#_3-添加-class-token\",\"293\":\"24#_4-添加位置编码\",\"294\":\"24#_5-encoder\",\"295\":\"24#_6-多头自注意力\",\"296\":\"24#_7-mlp-head\",\"297\":\"24#效果对比\",\"298\":\"24#注意力可视化\",\"299\":\"24#混合模型探索\",\"300\":\"24#加载预训练模型\",\"301\":\"24#总结\",\"302\":\"25\",\"303\":\"26\",\"304\":\"27\",\"305\":\"27#一、注意力机制的基本流程\",\"306\":\"27#二、q、k、v-的初始维度对结果的影响\",\"307\":\"27#_1-q-×-k-t-的维度\",\"308\":\"27#_2-softmax-操作\",\"309\":\"27#_3-与-v-相乘\",\"310\":\"27#三、总结-输入维度-→-输出维度\",\"311\":\"27#四、如何理解这个过程\",\"312\":\"27#✅-1-信息融合机制\",\"313\":\"27#✅-2-维度设计的灵活性\",\"314\":\"27#✅-3-可类比为-软检索-系统\",\"315\":\"27#五、例子说明-以-transformer-为例\",\"316\":\"27#六、常见疑问解答\",\"317\":\"27#❓q-为什么和可以不同\",\"318\":\"27#❓q-为什么要除以\",\"319\":\"27#七、可视化示意\",\"320\":\"28\",\"321\":\"28#引言\",\"322\":\"28#连续性\",\"323\":\"28#步长\",\"324\":\"28#张量变换操作\",\"325\":\"28#切片-slice\",\"326\":\"28#转置-transpose\",\"327\":\"28#广播-broadcast\",\"328\":\"28#维度问题\",\"329\":\"29\",\"330\":\"30\",\"331\":\"30#一、创建新环境\",\"332\":\"30#二、激活-切换-环境\",\"333\":\"30#三、退出当前环境\",\"334\":\"30#四、查看所有已创建的环境\",\"335\":\"30#五、删除已创建的环境\",\"336\":\"30#六、查看当前激活的环境\",\"337\":\"30#七、查看当前环境已安装的包\",\"338\":\"30#八、在当前环境下安装包\",\"339\":\"30#九、常见错误\",\"340\":\"31\",\"341\":\"31#二元分类场景\",\"342\":\"31#混淆矩阵-confusion-matrix\",\"343\":\"31#准确率-accuracy\",\"344\":\"31#召回率-recall-真正例率\",\"345\":\"31#误报概率-假正例率\",\"346\":\"31#精确率\",\"347\":\"31#指标的选择和权衡\",\"348\":\"31#f1-得分\",\"349\":\"31#roc-曲线和-auc\",\"350\":\"31#roc-receiver-operating-characteristic\",\"351\":\"31#auc-曲线下面积\",\"352\":\"31#精确率与召回率曲线\",\"353\":\"31#用于选择模型和阈值的-auc-和-roc\",\"354\":\"32\",\"355\":\"32#协方差矩阵\",\"356\":\"32#马氏距离\",\"357\":\"32#欧几里得距离-euclidean-distance\",\"358\":\"32#马氏距离-mahalanobis-distance\",\"359\":\"32#尺度差异性\",\"360\":\"32#总结\",\"361\":\"33\",\"362\":\"33#python\",\"363\":\"33#位置参数与关键字参数\",\"364\":\"33#闭包与高阶导数\",\"365\":\"33#什么是高阶函数\",\"366\":\"33#什么是闭包\",\"367\":\"33#装饰器的实现用到了什么\",\"368\":\"33#装饰器\",\"369\":\"33#最基本的函数装饰器\",\"370\":\"33#带参数的函数装饰器\",\"371\":\"33#带参数的装饰器-装饰器工厂\",\"372\":\"33#使用-functools-wraps-保留原函数元信息\",\"373\":\"33#装饰类方法-普通方法-类方法-静态方法\",\"374\":\"33#装饰整个类\",\"375\":\"33#装饰器的底层原理与执行过程\",\"376\":\"33#多个装饰器叠加时的执行顺序-从内到外\",\"377\":\"33#类装饰器\",\"378\":\"33#总结\",\"379\":\"33#典型应用场景举例\",\"380\":\"33#pytorch\",\"381\":\"33#stack\",\"382\":\"33#transpose\",\"383\":\"33#permute\",\"384\":\"33#view\",\"385\":\"33#reshape\",\"386\":\"33#repeat\",\"387\":\"33#expand\",\"388\":\"33#torch-no-grad\",\"389\":\"33#register-buffer\",\"390\":\"33#einsum\",\"391\":\"33#模型\",\"392\":\"33#resnet18\",\"393\":\"33#bert\",\"394\":\"33#公式-定理\",\"395\":\"33#通用近似定理\",\"396\":\"33#roi-pooling\",\"397\":\"33#roi-align\",\"398\":\"34\",\"399\":\"34#语义分割\",\"400\":\"34#损失函数\",\"401\":\"34#dice-loss\",\"402\":\"34#bce-dice-loss\",\"403\":\"34#jaccard-intersection-over-union-iou-loss\",\"404\":\"34#focal-loss\",\"405\":\"34#tversky-loss\",\"406\":\"34#lovasz-hinge-loss\",\"407\":\"34#combo-loss\",\"408\":\"34#如何选择\",\"409\":\"35\",\"410\":\"35#预训练过程\",\"411\":\"35#分词过程\",\"412\":\"35#附录\",\"413\":\"36\",\"414\":\"36#什么是大模型\",\"415\":\"36#为什么要对大模型进行微调\",\"416\":\"36#如何对大模型进行微调\",\"417\":\"36#常用的peft方案\",\"418\":\"36#prompt-tuning\",\"419\":\"36#prefix-tuning\",\"420\":\"36#lora\",\"421\":\"36#qlora\",\"422\":\"37\",\"423\":\"37#符合认知的大模型微调流程\",\"424\":\"37#大模型微调大致发展历史\",\"425\":\"37#lora-微调\",\"426\":\"37#矩阵a和b为什么不能同时为零\",\"427\":\"37#秩的选择\",\"428\":\"37#注意\",\"429\":\"38\",\"430\":\"38#什么是prompt-engineering\",\"431\":\"38#如何写好prompt\",\"432\":\"38#要明确-要具体\",\"433\":\"38#给llm更多的时间去思考\",\"434\":\"38#思维链技术-chain-of-thought\",\"435\":\"38#自一致性技术-self-consistency\",\"436\":\"38#从易至难技术-least-to-most\",\"437\":\"39\",\"438\":\"40\",\"439\":\"40#摘要\",\"440\":\"40#简介\",\"441\":\"40#相关工作\",\"442\":\"40#框架\",\"443\":\"40#无监督预训练\",\"444\":\"40#有监督微调\",\"445\":\"40#特定任务输入转换\",\"446\":\"40#实验\",\"447\":\"40#设置\",\"448\":\"40#监督微调\",\"449\":\"40#分析\",\"450\":\"40#结论\",\"451\":\"41\",\"452\":\"41#摘要\",\"453\":\"41#简介\",\"454\":\"41#方法\",\"455\":\"41#实验\",\"456\":\"41#讨论\",\"457\":\"41#总结\",\"458\":\"42\",\"459\":\"42#摘要\",\"460\":\"42#简介\",\"461\":\"42#方法\",\"462\":\"42#结果\",\"463\":\"42#局限性\",\"464\":\"42#相关工作\",\"465\":\"42#结论\",\"466\":\"43\",\"467\":\"43#摘要\",\"468\":\"43#简介\",\"469\":\"43#相关工作\",\"470\":\"43#方法\",\"471\":\"43#结果\",\"472\":\"43#讨论\",\"473\":\"44\",\"474\":\"44#why-we-need-kv-cache\",\"475\":\"44#self-attention-without-cache\",\"476\":\"44#self-attention-with-cache\",\"477\":\"44#huggingface-官方代码实现\",\"478\":\"45\",\"479\":\"45#摘要\",\"480\":\"45#简介\",\"481\":\"45#方法\",\"482\":\"45#结果\",\"483\":\"45#指令微调\",\"484\":\"45#bias-toxicity-and-misinformation\",\"485\":\"45#相关工作\",\"486\":\"45#总结\",\"487\":\"46\",\"488\":\"46#摘要\",\"489\":\"47\",\"490\":\"48\",\"491\":\"48#摘要\",\"492\":\"48#引言\",\"493\":\"48#背景\",\"494\":\"48#实验步骤\",\"495\":\"48#训练步骤分析\",\"496\":\"48#roberta核心改进总结\",\"497\":\"48#_1-训练策略优化\",\"498\":\"48#_2-数据规模与训练时长\",\"499\":\"48#_3-性能表现-关键结果\",\"500\":\"48#_4-结论与启示\",\"501\":\"48#相关工作\",\"502\":\"48#总结\",\"503\":\"49\",\"504\":\"49#bert-是什么\",\"505\":\"49#masked-language-model\",\"506\":\"49#next-sentence-prediction\",\"507\":\"49#multi-task-learning\",\"508\":\"49#fine-tuning\",\"509\":\"49#从-零-开始的预训练\",\"510\":\"49#数据清洗\",\"511\":\"49#分词器实现\",\"512\":\"49#batch数据准备\",\"513\":\"49#模型\",\"514\":\"49#训练\",\"515\":\"49#效果\",\"516\":\"49#details\",\"517\":\"49#padding-mask-如何生成并起作用的\",\"518\":\"50\",\"519\":\"50#环境搭建\",\"520\":\"50#数据预处理\",\"521\":\"50#模型架构\",\"522\":\"50#dataloader\",\"523\":\"50#bertembeddings\",\"524\":\"50#bertencoder\",\"525\":\"50#bertlayer\",\"526\":\"50#bertencoder-1\",\"527\":\"50#bertpooler\",\"528\":\"50#bertmodel\",\"529\":\"50#bertforsequenceclassification\",\"530\":\"50#bertattention\",\"531\":\"50#bertselfattention\",\"532\":\"50#bertselfoutput\",\"533\":\"50#bertattention-1\",\"534\":\"50#预训练\",\"535\":\"50#bertpredictionheadtransform\",\"536\":\"50#bertlmpredictionhead\",\"537\":\"50#bertpretrainingheads\",\"538\":\"50#bertforpretraining\",\"539\":\"50#其他下游任务\",\"540\":\"50#问答任务\",\"541\":\"50#代码实现\",\"542\":\"50#易混淆\",\"543\":\"50#token分类任务\",\"544\":\"50#多项选择任务\",\"545\":\"51\",\"546\":\"51#环境\",\"547\":\"51#背景\",\"548\":\"51#模型架构\",\"549\":\"51#encoder-decoder-结构\",\"550\":\"51#generator\",\"551\":\"51#encoder-结构\",\"552\":\"51#sublayerconnection\",\"553\":\"51#encoderlayer\",\"554\":\"51#encoder\",\"555\":\"51#decoder-结构\",\"556\":\"51#decoderlayer\",\"557\":\"51#decoder\",\"558\":\"51#多头自注意力\",\"559\":\"52\",\"560\":\"53\",\"561\":\"54\",\"562\":\"55\",\"563\":\"56\",\"564\":\"56#概率空间\",\"565\":\"56#离散随机变量\",\"566\":\"56#连续随机变量\",\"567\":\"56#概率公理\",\"568\":\"56#条件概率\",\"569\":\"56#全概率公式-law-of-total-probability\",\"570\":\"56#贝叶斯法则\",\"571\":\"56#离散随机变量形式\",\"572\":\"56#连续随机变量形式\",\"573\":\"56#一些常见的概率分布\",\"574\":\"56#离散分布\",\"575\":\"56#伯努利分布与二项分布-bernoulli-and-binomial-distributions\",\"576\":\"56#分类分布与多项分布-categorical-and-multinomial-distributions\",\"577\":\"56#泊松分布-poisson-distribution\",\"578\":\"56#负二项分布-negative-binomial-distribution\",\"579\":\"56#转换视角-定义失败为红球-成功为蓝球\",\"580\":\"56#特殊情况说明\",\"581\":\"56#数学期望与方差\",\"582\":\"56#负二项分布的意义与优势\",\"583\":\"56#定义在实数上的连续分布\",\"584\":\"56#高斯分布-正态分布\",\"585\":\"56#半正态分布-half-normal\",\"586\":\"56#学生-t-分布-student-t-distribution\",\"587\":\"56#柯西分布-cauchy-distribution\",\"588\":\"56#高斯联合分布-gaussian-joint-distributions\",\"589\":\"56#多元正态分布-the-multivariate-normal\",\"590\":\"56#定义-definition\",\"591\":\"56#高斯壳-gaussian-shells\",\"592\":\"56#直观解释如下\",\"593\":\"56#数学解释-为什么高斯样本集中在壳层上\",\"594\":\"56#图像空间的含义-例如灰度图像\",\"595\":\"57\",\"596\":\"57#bayes-rule\",\"597\":\"57#inverse-problems\",\"598\":\"58\",\"599\":\"58#计数法则\",\"600\":\"58#排列-考虑元素之间的顺序\",\"601\":\"58#组合-不考虑元素之间的顺序\",\"602\":\"59\",\"603\":\"60\",\"604\":\"60#引言-揭开深度学习框架的神秘面纱\",\"605\":\"60#步骤1-作为-箱子-的变量\",\"606\":\"60#变量的基本概念\",\"607\":\"60#代码实现\",\"608\":\"60#使用示例\",\"609\":\"60#关键要点\",\"610\":\"60#步骤2-创建变量的函数\",\"611\":\"60#函数与计算图\",\"612\":\"60#函数类的设计\",\"613\":\"60#代码实现-1\",\"614\":\"60#辅助函数\",\"615\":\"60#步骤3-函数的连续调用\",\"616\":\"60#复合函数的计算\",\"617\":\"60#代码示例\",\"618\":\"60#计算图的意义\",\"619\":\"60#步骤4-数值微分\",\"620\":\"60#导数的定义\",\"621\":\"60#数值微分的实现\",\"622\":\"60#代码实现-2\",\"623\":\"60#数值微分的问题\",\"624\":\"60#步骤5-反向传播的理论知识\",\"625\":\"60#链式法则\",\"626\":\"60#反向传播的方向\",\"627\":\"60#计算图的反向传播\",\"628\":\"60#步骤6-手动进行反向传播\",\"629\":\"60#扩展variable类\",\"630\":\"60#扩展function类\",\"631\":\"60#具体函数的反向传播\",\"632\":\"60#反向传播的执行\",\"633\":\"60#步骤7-反向传播的自动化\",\"634\":\"60#建立变量与函数的连接\",\"635\":\"60#自动反向传播的实现\",\"636\":\"60#步骤8-从递归到循环\",\"637\":\"60#递归实现的问题\",\"638\":\"60#循环实现反向传播\",\"639\":\"60#循环实现的优势\",\"640\":\"60#步骤9-让函数更易用\",\"641\":\"60#函数的python化\",\"642\":\"60#自动设置梯度\",\"643\":\"60#数据类型检查\",\"644\":\"60#步骤10-测试\",\"645\":\"60#单元测试\",\"646\":\"60#梯度检验\",\"647\":\"60#测试的重要性\",\"648\":\"60#第一阶段总结\",\"649\":\"61\",\"650\":\"61#引言-从自动微分迈向通用框架\",\"651\":\"61#步骤11-多输入与多输出\",\"652\":\"61#步骤12-backward-的多输入实现\",\"653\":\"61#步骤13-重置导数\",\"654\":\"61#步骤14-共享变量与梯度累加\",\"655\":\"61#步骤15-梯度重复累加的问题\",\"656\":\"61#步骤16-辈分-机制\",\"657\":\"61#步骤17-循环引用与内存释放\",\"658\":\"61#步骤18-优化内存消耗\",\"659\":\"61#步骤19-variable-功能增强\",\"660\":\"61#步骤20–22-运算符重载\",\"661\":\"61#步骤23-项目模块化结构\",\"662\":\"61#步骤24-复杂函数的求导\",\"663\":\"61#第二阶段总结\",\"664\":\"62\",\"665\":\"62#引言-从自动微分走向-可视化-高阶导数-灵活控制\",\"666\":\"62#步骤25-可视化计算图\",\"667\":\"62#步骤26-寻找函数最优解\",\"668\":\"62#步骤27-高阶导数\",\"669\":\"63\",\"670\":\"63#引言-从自动微分迈向可训练的神经网络模型\",\"671\":\"64\",\"672\":\"65\",\"673\":\"65#大语言模型\",\"674\":\"65#常见的llm\",\"675\":\"65#llm-的特点与能力\",\"676\":\"65#涌现能力-emergent-abilities\",\"677\":\"65#作为基座模型支持多元应用的能力\",\"678\":\"65#支持对话作为统一入口的能力\",\"679\":\"65#检索增强生成-rag-retrieval-augmented-generation\",\"680\":\"65#工作流程\",\"681\":\"65#rag-vs-finetune\",\"682\":\"65#langchain\",\"683\":\"65#核心组件\",\"684\":\"65#版本迭代\",\"685\":\"65#生态圈\",\"686\":\"65#大模型开发\",\"687\":\"65#基本流程\",\"688\":\"65#参考\",\"689\":\"66\",\"690\":\"67\",\"691\":\"68\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1,7],\"1\":[1],\"2\":[2,16],\"3\":[1,5],\"4\":[2,35],\"5\":[1,25],\"6\":[1,83],\"7\":[1,71],\"8\":[1,48],\"9\":[8],\"10\":[3,22],\"11\":[5,16],\"12\":[5,19],\"13\":[4,15],\"14\":[8,27],\"15\":[6,24],\"16\":[1,16],\"17\":[3,40],\"18\":[3,12],\"19\":[4,18],\"20\":[4,15],\"21\":[1,8],\"22\":[4,59],\"23\":[4,45],\"24\":[4,48],\"25\":[4,22],\"26\":[1,49],\"27\":[1],\"28\":[6,202],\"29\":[1,242],\"30\":[1,73],\"31\":[1,34],\"32\":[1,69],\"33\":[1,40],\"34\":[1,54],\"35\":[1,100],\"36\":[1,75],\"37\":[2,29],\"38\":[3,4],\"39\":[1,1],\"40\":[1,223],\"41\":[3,92],\"42\":[3,25],\"43\":[3,114],\"44\":[3,20],\"45\":[3,142],\"46\":[5,153],\"47\":[2,27],\"48\":[1,29],\"49\":[1,73],\"50\":[1],\"51\":[1,31],\"52\":[1,19],\"53\":[1],\"54\":[1,35],\"55\":[1,15],\"56\":[1,14],\"57\":[1],\"58\":[1,152],\"59\":[1,259],\"60\":[2,31],\"61\":[1],\"62\":[2,25],\"63\":[5,72],\"64\":[7,26],\"65\":[2,54],\"66\":[2,14],\"67\":[4,39],\"68\":[2,211],\"69\":[2,20],\"70\":[1,154],\"71\":[2,40],\"72\":[3,78],\"73\":[5,118],\"74\":[3,61],\"75\":[3,96],\"76\":[2,224],\"77\":[1],\"78\":[5,174],\"79\":[1,6],\"80\":[1,105],\"81\":[1,61],\"82\":[1,338],\"83\":[1,310],\"84\":[3],\"85\":[1,19],\"86\":[1,73],\"87\":[1,15],\"88\":[1,24],\"89\":[2,23],\"90\":[2,51],\"91\":[2,15],\"92\":[1,316],\"93\":[1,122],\"94\":[1,17],\"95\":[5,35],\"96\":[1,196],\"97\":[5,40],\"98\":[1,151],\"99\":[1,42],\"100\":[1,222],\"101\":[1,140],\"102\":[1,17],\"103\":[1,33],\"104\":[1,28],\"105\":[1,121],\"106\":[3,1],\"107\":[1,192],\"108\":[1,73],\"109\":[1,112],\"110\":[1,54],\"111\":[1,94],\"112\":[1,290],\"113\":[3],\"114\":[1,65],\"115\":[1,83],\"116\":[1,24],\"117\":[1,13],\"118\":[1],\"119\":[2,19],\"120\":[1,65],\"121\":[2],\"122\":[4,26],\"123\":[4,19],\"124\":[4,11],\"125\":[1,11],\"126\":[8,87],\"127\":[5,55],\"128\":[2,43],\"129\":[1,22],\"130\":[3],\"131\":[1,69],\"132\":[2,32],\"133\":[1,25],\"134\":[2,61],\"135\":[2],\"136\":[2,23],\"137\":[3,17],\"138\":[1,42],\"139\":[2],\"140\":[2,43],\"141\":[2],\"142\":[1,248],\"143\":[1,149],\"144\":[2],\"145\":[1,353],\"146\":[1,92],\"147\":[2,363],\"148\":[2,19],\"149\":[1,46],\"150\":[2,36],\"151\":[1],\"152\":[2,27],\"153\":[3,12],\"154\":[4,35],\"155\":[5,27],\"156\":[5,39],\"157\":[2,109],\"158\":[2],\"159\":[1,110],\"160\":[2,147],\"161\":[1,173],\"162\":[1,191],\"163\":[1,307],\"164\":[2,8],\"165\":[1,27],\"166\":[1,69],\"167\":[1,17],\"168\":[1,10],\"169\":[1,29],\"170\":[1,36],\"171\":[3,12],\"172\":[3,17],\"173\":[1,6],\"174\":[1,16],\"175\":[1,14],\"176\":[7,8],\"177\":[1,11],\"178\":[1,69],\"179\":[6,27],\"180\":[1,21],\"181\":[1,62],\"182\":[1],\"183\":[1,27],\"184\":[1,15],\"185\":[1,21],\"186\":[1,6],\"187\":[1],\"188\":[1,80],\"189\":[1,72],\"190\":[1,131],\"191\":[1,75],\"192\":[1],\"193\":[1,53],\"194\":[1,69],\"195\":[1,41],\"196\":[1,45],\"197\":[1,21],\"198\":[1,25],\"199\":[3],\"200\":[1,60],\"201\":[1,37],\"202\":[1,33],\"203\":[1,23],\"204\":[1,23],\"205\":[1,11],\"206\":[6,19],\"207\":[1,52],\"208\":[1,83],\"209\":[1],\"210\":[1,39],\"211\":[1,27],\"212\":[1,41],\"213\":[1],\"214\":[1,41],\"215\":[1,58],\"216\":[1,25],\"217\":[1,51],\"218\":[1],\"219\":[1,34],\"220\":[1,37],\"221\":[1,22],\"222\":[1,20],\"223\":[9,21],\"224\":[1,130],\"225\":[1,21],\"226\":[1,126],\"227\":[1,127],\"228\":[7,40],\"229\":[4,28],\"230\":[1],\"231\":[5,107],\"232\":[2,15],\"233\":[1,13],\"234\":[5,40],\"235\":[3,50],\"236\":[2,24],\"237\":[1,45],\"238\":[1,153],\"239\":[1,45],\"240\":[2,179],\"241\":[1,62],\"242\":[2,74],\"243\":[2],\"244\":[2,151],\"245\":[2],\"246\":[2,98],\"247\":[2,109],\"248\":[2,30],\"249\":[3,54],\"250\":[1],\"251\":[3,6],\"252\":[2,18],\"253\":[1,56],\"254\":[1,7],\"255\":[1,47],\"256\":[3,35],\"257\":[2,31],\"258\":[2,50],\"259\":[1,21],\"260\":[1,7],\"261\":[3,5],\"262\":[4,85],\"263\":[3,43],\"264\":[4,42],\"265\":[4,39],\"266\":[3,71],\"267\":[2,46],\"268\":[1,143],\"269\":[1,3],\"270\":[1,31],\"271\":[1,20],\"272\":[1,146],\"273\":[1,156],\"274\":[1,57],\"275\":[1,245],\"276\":[1,92],\"277\":[1,210],\"278\":[1,132],\"279\":[1,17],\"280\":[1,66],\"281\":[1,20],\"282\":[6,144],\"283\":[10,95],\"284\":[8,183],\"285\":[10,349],\"286\":[6,194],\"287\":[1,53],\"288\":[1,4],\"289\":[2,234],\"290\":[2,130],\"291\":[2,181],\"292\":[4,160],\"293\":[2,141],\"294\":[2,153],\"295\":[2,138],\"296\":[3,220],\"297\":[1,61],\"298\":[1,13],\"299\":[1,51],\"300\":[1,158],\"301\":[1,15],\"302\":[1],\"303\":[1],\"304\":[1,16],\"305\":[2,19],\"306\":[5],\"307\":[2,11],\"308\":[2,6],\"309\":[2,13],\"310\":[5,24],\"311\":[3,2],\"312\":[3,12],\"313\":[3,9],\"314\":[3,10],\"315\":[6,18],\"316\":[2],\"317\":[5,5],\"318\":[3,8],\"319\":[2,14],\"320\":[1,1],\"321\":[1,52],\"322\":[1,30],\"323\":[1,147],\"324\":[1],\"325\":[3,114],\"326\":[3,120],\"327\":[3,83],\"328\":[1,28],\"329\":[1],\"330\":[1,1],\"331\":[2,25],\"332\":[4,13],\"333\":[2,6],\"334\":[2,20],\"335\":[2,12],\"336\":[2,3],\"337\":[2,3],\"338\":[2,63],\"339\":[2,13],\"340\":[1,1],\"341\":[1],\"342\":[4,37],\"343\":[3,35],\"344\":[3,25],\"345\":[2,23],\"346\":[1,29],\"347\":[1,30],\"348\":[2,20],\"349\":[3,6],\"350\":[5,29],\"351\":[3,34],\"352\":[1,20],\"353\":[4,49],\"354\":[1,1],\"355\":[1,121],\"356\":[1],\"357\":[4,13],\"358\":[4,12],\"359\":[1,70],\"360\":[1,7],\"361\":[1,1],\"362\":[1],\"363\":[1,19],\"364\":[1],\"365\":[2,25],\"366\":[2,37],\"367\":[2,40],\"368\":[1,22],\"369\":[1,23],\"370\":[1,24],\"371\":[3,27],\"372\":[2,64],\"373\":[5,20],\"374\":[1,12],\"375\":[1,17],\"376\":[3,12],\"377\":[1,23],\"378\":[1,16],\"379\":[1,35],\"380\":[1],\"381\":[1,31],\"382\":[1,16],\"383\":[1,26],\"384\":[1,37],\"385\":[1,42],\"386\":[1,26],\"387\":[1,77],\"388\":[4,8],\"389\":[2,64],\"390\":[1,45],\"391\":[1],\"392\":[1,16],\"393\":[1,4],\"394\":[2],\"395\":[1,266],\"396\":[2,94],\"397\":[2,128],\"398\":[1,1],\"399\":[1,22],\"400\":[1],\"401\":[2,144],\"402\":[3,150],\"403\":[6,159],\"404\":[2,321],\"405\":[2,158],\"406\":[3,14],\"407\":[2,190],\"408\":[2,22],\"409\":[1,29],\"410\":[1,200],\"411\":[1,98],\"412\":[1,214],\"413\":[4,4],\"414\":[2,40],\"415\":[2,47],\"416\":[2,67],\"417\":[1,4],\"418\":[2,31],\"419\":[2,31],\"420\":[1,54],\"421\":[1,66],\"422\":[1,10],\"423\":[1,59],\"424\":[1,63],\"425\":[2,92],\"426\":[2,34],\"427\":[1,15],\"428\":[1,62],\"429\":[3,3],\"430\":[3,33],\"431\":[2],\"432\":[2,21],\"433\":[1,44],\"434\":[4,51],\"435\":[3,43],\"436\":[4,46],\"437\":[1],\"438\":[3,10],\"439\":[1,35],\"440\":[1,83],\"441\":[1,16],\"442\":[1,4],\"443\":[1,24],\"444\":[1,30],\"445\":[1,52],\"446\":[1],\"447\":[1,113],\"448\":[1,100],\"449\":[1,66],\"450\":[1,14],\"451\":[3,10],\"452\":[1,29],\"453\":[1,64],\"454\":[1,213],\"455\":[1,235],\"456\":[1,35],\"457\":[1,26],\"458\":[3,10],\"459\":[1,18],\"460\":[1,100],\"461\":[1,128],\"462\":[1,150],\"463\":[1,104],\"464\":[1,135],\"465\":[1,18],\"466\":[2,12],\"467\":[1,26],\"468\":[1,76],\"469\":[1,166],\"470\":[1,287],\"471\":[1,202],\"472\":[1,230],\"473\":[3,2],\"474\":[6,136],\"475\":[4,16],\"476\":[4,5],\"477\":[2,333],\"478\":[2,10],\"479\":[1,16],\"480\":[1,67],\"481\":[1,114],\"482\":[1,166],\"483\":[1,68],\"484\":[4,69],\"485\":[1,47],\"486\":[1,5],\"487\":[2,12],\"488\":[1],\"489\":[1],\"490\":[2,9],\"491\":[1,20],\"492\":[1,62],\"493\":[1,85],\"494\":[1,101],\"495\":[1,148],\"496\":[1,11],\"497\":[1,62],\"498\":[1,31],\"499\":[1,37],\"500\":[1,18],\"501\":[1,19],\"502\":[1,15],\"503\":[4,9],\"504\":[3,50],\"505\":[3,64],\"506\":[3,42],\"507\":[3,7],\"508\":[2,129],\"509\":[3,6],\"510\":[1,150],\"511\":[1,181],\"512\":[1,116],\"513\":[1,176],\"514\":[1,177],\"515\":[1,23],\"516\":[1,2],\"517\":[4,77],\"518\":[2,2],\"519\":[1,138],\"520\":[1,165],\"521\":[1],\"522\":[1,48],\"523\":[1,66],\"524\":[1],\"525\":[1,58],\"526\":[1,30],\"527\":[1,40],\"528\":[1,49],\"529\":[1,73],\"530\":[1],\"531\":[1,104],\"532\":[1,32],\"533\":[1,23],\"534\":[1,1],\"535\":[1,41],\"536\":[1,44],\"537\":[1,28],\"538\":[1,75],\"539\":[1,1],\"540\":[1,120],\"541\":[1,95],\"542\":[1,173],\"543\":[1,75],\"544\":[1,119],\"545\":[1,2],\"546\":[1,41],\"547\":[1,23],\"548\":[1,64],\"549\":[3,30],\"550\":[1,28],\"551\":[2],\"552\":[1,33],\"553\":[1,37],\"554\":[1,37],\"555\":[2],\"556\":[1,49],\"557\":[1,39],\"558\":[1,116],\"559\":[2],\"560\":[2],\"561\":[2],\"562\":[1],\"563\":[1,5],\"564\":[1,13],\"565\":[1,117],\"566\":[1,166],\"567\":[1,50],\"568\":[1,37],\"569\":[6,51],\"570\":[1,12],\"571\":[1,16],\"572\":[1,9],\"573\":[1,12],\"574\":[1,4],\"575\":[6,20],\"576\":[6,37],\"577\":[4,13],\"578\":[5,18],\"579\":[3,40],\"580\":[1,6],\"581\":[1,4],\"582\":[1,16],\"583\":[1,6],\"584\":[3,33],\"585\":[4,17],\"586\":[6,32],\"587\":[4,33],\"588\":[5,18],\"589\":[5,5],\"590\":[3,60],\"591\":[4,23],\"592\":[2,27],\"593\":[3,18],\"594\":[3,23],\"595\":[1,5],\"596\":[2,121],\"597\":[2,46],\"598\":[1,1],\"599\":[1,18],\"600\":[3,41],\"601\":[3,9],\"602\":[1],\"603\":[4,11],\"604\":[2,11],\"605\":[4],\"606\":[1,8],\"607\":[1,7],\"608\":[1,14],\"609\":[1,4],\"610\":[2],\"611\":[1,6],\"612\":[1,5],\"613\":[1,19],\"614\":[1,8],\"615\":[2],\"616\":[1,6],\"617\":[1,27],\"618\":[1,4],\"619\":[2],\"620\":[1,3],\"621\":[1,5],\"622\":[1,19],\"623\":[1,6],\"624\":[2],\"625\":[1,6],\"626\":[1,33],\"627\":[1,5],\"628\":[2],\"629\":[1,11],\"630\":[1,41],\"631\":[1,15],\"632\":[1,17],\"633\":[2],\"634\":[1,19],\"635\":[1,25],\"636\":[2],\"637\":[1,3],\"638\":[1,27],\"639\":[1,3],\"640\":[2],\"641\":[1,10],\"642\":[1,17],\"643\":[1,23],\"644\":[2],\"645\":[1,28],\"646\":[1,32],\"647\":[1,3],\"648\":[1,17],\"649\":[4,11],\"650\":[2,34],\"651\":[2,64],\"652\":[3,50],\"653\":[2,15],\"654\":[2,65],\"655\":[2,38],\"656\":[3,105],\"657\":[2,137],\"658\":[2,159],\"659\":[3,92],\"660\":[3,278],\"661\":[2,67],\"662\":[2,113],\"663\":[1,25],\"664\":[4,11],\"665\":[7,50],\"666\":[2,236],\"667\":[2,190],\"668\":[2],\"669\":[4,11],\"670\":[2,44],\"671\":[1],\"672\":[2,3],\"673\":[1,133],\"674\":[1,594],\"675\":[2,59],\"676\":[1,55],\"677\":[1,32],\"678\":[1,66],\"679\":[6,72],\"680\":[1,19],\"681\":[3,51],\"682\":[1,50],\"683\":[1,45],\"684\":[1,100],\"685\":[1,52],\"686\":[1,78],\"687\":[1,114],\"688\":[1,28],\"689\":[2,4],\"690\":[1],\"691\":[1,3]},\"averageFieldLength\":[1.9031791907514444,61.02063772891073],\"storedFields\":{\"0\":{\"h\":\"主页\",\"t\":[\"知识星球: MetaMind , 小红书: BinaryOracle , CSDN: Binary Oracle\"]},\"1\":{\"h\":\"关于我们\"},\"2\":{\"h\":\"Binary Oracle\",\"t\":[\"一名普通但十分热爱探索技术的Coder\",\"开源框架 Spring committer\",\"Golang 开源网络库 netpoll committer\",\"Javaer 转型 3D - VL 方向研究\",\"现就读于四川大学\",\"有问题需要咨询的小伙伴，可以加微信备注来意:\"]},\"3\":{\"h\":\"Elowen\",\"t\":[\"CV 转 LLM 领域\",\"现就读于电子科技大学\"]},\"4\":{\"h\":\"GREAT 论文解读\",\"t\":[\"GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding 论文解读\",\"论文: https://arxiv.org/abs/2411.19626 代码: https://github.com/yawen-shao/GREAT_code 数据集: https://drive.google.com/drive/folders/1n_L_mSmVpAM-1ASoW2T2MltYkaiA_X9X\"]},\"5\":{\"h\":\"摘要\",\"t\":[\"GREAT（Geometry-Intention Collaborative Inference）是一种新颖的框架，旨在通过挖掘物体的不变几何属性和潜在交互意图，以开放词汇的方式定位3D物体的功能区域（affordance）。该框架结合了多模态大语言模型（MLLMs）的推理能力，设计了多头部功能链式思维（MHACoT）策略，逐步分析交互图像中的几何属性和交互意图，并通过跨模态自适应融合模块（CMAFM）将这些知识与点云和图像特征结合，实现精准的3D功能定位。此外，研究还提出了目前最大的3D功能数据集PIADv2，包含15K交互图像和38K标注的3D物体实例。实验证明了GREAT在开放词汇场景下的有效性和优越性。\"]},\"6\":{\"h\":\"简介\",\"t\":[\"Open-Vocabulary 3D对象功能定位（OVAG）旨在通过任意指令定位物体上支持特定交互的“动作可能性”区域，对机器人感知与操作至关重要。现有方法（如IAGNet、LASO）通过结合描述交互的图像或语言与3D几何结构引入外部先验，但存在以下局限性（如图1(b)所示）：\",\"语义空间受限：依赖预定义类别，难以泛化到未见过的功能（如将“pour”错误分类为“grasp”）。\",\"几何与意图利用不足：未充分挖掘物体间共享的几何不变性（如手柄的抓握属性）和同一物体的多交互意图关联。\",\"人类认知启发:\",\"研究表明（Gick & Holyoak, 1980），人类通过多步推理和类比思维解决复杂任务。例如，观察倒水场景时（图1(c)），人类会：\",\"识别交互部件（壶嘴）\",\"提取几何属性（倾斜曲面）\",\"推理潜在意图（倒水/注水）\",\"方法创新:\",\"GREAT框架通过以下设计模拟这一过程（图1(d)）：\",\"MHACoT推理链：基于微调的MLLM（如InternVL）分步推理：\",\"Object-Head：定位交互部件并分析几何结构（如“为什么壶嘴适合倒水”）\",\"Affordance-Head：描述实际交互（如“握柄倒水”）并联想潜在意图（如“注水/清洗”）\",\"跨模态融合：通过CMAFM模块将几何属性（）与交互意图（）注入点云（）和图像特征（），最终解码为3D功能热图 。\",\"数据集贡献:\",\"扩展构建了PIADv2（对比见表1）：\",\"规模：15K交互图像（×3）和38K 3D实例（×5）\",\"多样性：43类物体、24类功能，覆盖多对多关联（图3(c)）\"]},\"7\":{\"h\":\"相关工作\",\"t\":[\"1. Affordance Grounding\",\"现有研究主要从2D数据（如图像、视频）和自然语言理解出发，定位“动作可能性”区域。例如，部分工作通过语言理解在2D数据中定位功能区域（3, 21），但机器人操作需要3D信息，2D方法难以直接迁移。随着3D数据集（如5, 6）的出现，部分研究开始映射语义功能到3D结构，但受限于预定义类别，无法处理开放词汇场景。\",\"2. Open-Vocabulary 3D Affordance Grounding (OVAG)\",\"OVAG旨在通过额外指令（如文本或图像）引入交互先验，提升泛化能力。例如：\",\"IAGNet 利用2D交互语义指导3D功能定位；\",\"LASO 通过文本条件查询分割功能区域；\",\"OpenAD 和 OpenKD 利用CLIP编码器实现文本-点云关联。\",\"这些方法仍受限于训练语义空间，而GREAT通过几何-意图协同推理（CoT）解决此问题（如表2所示）。\",\"3. Chain-of-Thought (CoT) 与多模态大模型 (MLLMs)\",\"CoT及其变体通过多步推理增强MLLMs能力。例如：\",\"视觉任务中，MLLMs（如InternVL）结合CoT在目标检测、机器人操作等任务中表现优异；\",\"但动态功能特性使得MLLMs难以直接从交互图像推理3D功能，GREAT通过微调MLLMs并设计MHACoT策略解决这一问题。\",\"关键问题（如图1所示）：\",\"现有方法依赖数据对齐，泛化性不足（如将“pour”误分类为“grasp”）；\",\"GREAT通过模拟人类多步推理（几何属性提取+意图类比）实现开放词汇功能定位。\"]},\"8\":{\"h\":\"方法\",\"t\":[\"GREAT 的输入为 ，其中 是点云，包含物体的坐标 和其对应的 3D 可供性标注 ， 为图像。目标是优化模型 ，输出 3D 物体可供性 ，即：\",\"如图2所示，首先使用 ResNet [9] 和 PointNet++ [43] 提取特征，分别得到 和 ，随后将 reshape 为 （其中 ）。接着通过多头可供性链式思维（MHACoT）策略对交互图像进行推理，挖掘不变几何属性与潜在交互意图。\",\"然后，使用 Roberta [28] 编码推理结果，通过交叉注意力机制计算对象几何特征 和可供性意图特征 （见 Sec. 3.2）。GREAT 利用跨模态自适应融合模块（CMAFM）将这些知识注入点云特征并与图像特征融合，得到融合特征 （见 Sec. 3.3）。最后将这两个特征送入解码器以获得可供性输出 ，并通过复合损失优化整个流程（见 Sec. 3.4）。\"]},\"9\":{\"h\":\"3.2 Multi-Head Affordance Chain-of-Thought\"},\"10\":{\"h\":\"Fine-Tuning MLLM\",\"t\":[\"为了获得对物体可供性更深入的理解，我们对 InternVL [4] 使用可学习的 Adapter [10] 进行微调，仅更新 Adapter 模块（10 个 epoch，学习率 4e-5，LoRA rank 为 16），其余参数保持冻结，以保持原始模型识别能力的同时增强其推理能力。\"]},\"11\":{\"h\":\"Object-Head Reasoning（几何推理）\",\"t\":[\"该部分包含：\",\"物体交互感知（Object Interaction Perception）：识别图像中物体与人发生交互的部分。Prompt 示例为：“指出图像中物体与人交互的部分。”\",\"几何结构推理（Geometric Structure Reasoning）：进一步从几何结构角度推理为什么该部位适合交互。Prompt 示例为：“从几何结构解释该部位可以交互的原因。”\"]},\"12\":{\"h\":\"Affordance-Head Reasoning（类比推理）\",\"t\":[\"该部分包含：\",\"交互细节描述（Interaction Detailed Description）：描述图像中人与物体之间的完整交互过程，生成细粒度表示。Prompt 示例为：“描述图像中人与物体的交互方式。”\",\"交互类比推理（Interactive Analogical Reasoning）：模拟人类对交互方式的联想，挖掘其他可能交互意图，增强类比能力。Prompt 示例为：“列举两个该物体常见的其他交互方式。”\"]},\"13\":{\"h\":\"Knowledge Encoding and Integration\",\"t\":[\"从 Object-Head 得到的几何属性描述与 Affordance-Head 推理的交互描述被 Roberta 编码为两个特征：\",\"：物体几何知识特征\",\"：可供性意图知识特征\",\"通过交叉注意力层 与自注意力层 对齐二者，公式如下：\"]},\"14\":{\"h\":\"3.3 Cross-Modal Adaptive Fusion Module (CMAFM)\",\"t\":[\"为了将几何属性与点云特征更好地对齐融合，CMAFM 将 融合至 PointNet++ 最深层特征，并与图像特征联合用于预测。\",\"具体地，对点云特征 和知识特征 进行线性映射形成 Query、Key、Value：\",\"跨注意力融合公式为：\",\"最终点云融合特征表示为：\",\"其中 为全连接层， 表示池化后扩展为 ， 为 卷积，输出 上采样至原始点数后记为：\",\"图像特征 与意图特征 融合表示为：\"]},\"15\":{\"h\":\"3.4 Decoder and Loss Functions\",\"t\":[\"最终将融合后的图像特征 和点云特征 拼接后送入解码器输出可供性预测：\",\"其中 为 sigmoid 激活， 为输出头，， 是最终的 3D 可供性预测。\",\"损失函数由 focal loss [26] 与 dice loss [37] 组成：\",\"这种设计无需依赖具体的可供性分类标签，而是通过监督点级热图，将 3D 可供性与交互图像直接联系起来。\"]},\"16\":{\"h\":\"数据集\",\"t\":[\"为支撑开放词汇 3D 物体可供性定位任务，本文构建了 PIADv2（Point Image Affordance Dataset v2），由成对的 2D 交互图像与 3D 点云对象组成，是当前规模最大的同类数据集。\"]},\"17\":{\"h\":\"数据收集（Collection）\",\"t\":[\"点云部分主要来自以下开源数据源：\",\"3DIR [57]\",\"3D-AffordanceNet [6]\",\"Objaverse [5]\",\"图像部分主要来源于：\",\"AGD20k [32]\",\"OpenImage [18]\",\"其他开源许可网站\",\"总体数据统计：\",\"图像数：15,213\",\"点云数：38,889\",\"覆盖类别：\",\"物体类别：43类\",\"可供性类别：24类\",\"该数据集大大超越了前作 PIAD [56]，其图像数量是前者的三倍，点云数量是前者的五倍。\",\"如图3(a) 所示，红色区域为点云的可供性标注。图3(b) 展示了各类别的分布情况，显示出数据集对交互多样性和类别多样性的全面覆盖。\"]},\"18\":{\"h\":\"标注策略（Annotation）\",\"t\":[\"对于点云实例：\",\"每个点云实例按可供性类别标注\",\"每个样本为一个 的矩阵，含：\",\"2048 个点\",\"每个点包括 坐标与热力图形式的可供性值\",\"对于图像：\",\"图像按可供性类别进行分类，以支持训练阶段的匹配与推理\"]},\"19\":{\"h\":\"统计分析（Statistical Analysis）\",\"t\":[\"图像与点云之间不需要一一对应，二者分别从不同实例中采样，以增强泛化能力\",\"多对多关系分析：如图3(c) 所示，affordance 与 object 类别间存在明显的多对多关系，挑战模型对可供性的泛化能力\",\"类平衡分析：图3(d) 展示了各 object 类别下图像与点云的数量比例，体现出数据集在样本分布上的全面性和均衡性\"]},\"20\":{\"h\":\"数据划分（Data Partitions）\",\"t\":[\"PIADv2 提供三种标准划分方式（前两种与 PIAD [56] 保持一致）：\",\"Seen：\",\"训练集与测试集中的物体与可供性类别相同\",\"Unseen Object：\",\"测试集中包含训练集中未出现的物体类别，但可供性类别相同\",\"Unseen Affordance：\",\"测试集中的可供性类别未在训练集中出现，同时包含部分新物体类别\"]},\"21\":{\"h\":\"实验\",\"t\":[\"为验证所提方法 GREAT 的有效性与泛化能力，作者在提出的 PIADv2 数据集上开展了系统性的实验评估，包括与多个先进方法的对比以及消融实验和可视化分析。\"]},\"22\":{\"h\":\"5.1 Benchmark Setting\",\"t\":[\"评估指标：\",\"实验采用以下评估指标评估 3D 可供性预测质量（参考 [25, 56]）：\",\"AUC（Area Under Curve）[29]\",\"aIOU（average Intersection over Union）[45]\",\"SIM（Similarity）[47]\",\"MAE（Mean Absolute Error）[52]\",\"对比方法：\",\"IAG (2023)：2D-引导的3D可供性方法\",\"LASO (2024)：基于语言引导的3D可供性分割\",\"FRCNN [54]：LiDAR-图像融合两阶段3D检测框架\",\"XMF [1]：图像-点云的跨模态点云形状补全方法\",\"Baseline：直接拼接图像与点云特征作为输入\",\"实现细节：\",\"3D backbone：PointNet++ [43]\",\"2D backbone：ResNet18 [9]\",\"优化器：Adam\",\"学习率：1e-4\",\"批大小：16\",\"总训练轮次：65\"]},\"23\":{\"h\":\"5.2 Comparison Results\",\"t\":[\"如表2所示，GREAT 在所有划分（Seen、Unseen Object、Unseen Affordance）下均显著优于现有方法，达成最新最优性能。\",\"量化分析：\",\"在 Unseen Affordance 这一最具挑战性的设置下，GREAT 依旧表现出色：\",\"AUC：69.81（高出 LASO 约 9%）\",\"aIOU：12.05（高出 IAG 约 34%）\",\"SIM：0.290（大幅超越所有基线）\",\"MAE：0.127（最小）\",\"可视化分析（如图4所示）：\",\"Seen setting：各方法差别不大\",\"Unseen setting：\",\"其他方法倾向于错误地预测为训练集中频繁出现的 affordance（如 grasp）\",\"GREAT 能正确捕捉如 \\\"pour\\\" 这类 unseen affordance，定位精度显著更高\"]},\"24\":{\"h\":\"5.3 Ablation Study\",\"t\":[\"表3 展示了对关键模块的消融实验结果：\",\"消融项分析：\",\"✗ AffCoT（无意图推理）：\",\"unseen affordance 的 aIOU 下降了 1.12，表明交互意图推理对泛化至新 affordance 极为重要\",\"✗ ObjCoT（无几何推理）：\",\"模型对物体关键交互区域的识别能力下降\",\"✗ CMAFM（无跨模态融合）：\",\"几何信息无法有效注入点云，导致各项指标大幅下降（aIOU 从 38.03 降到 29.48）\",\"✗ FT（无 MLLM 微调）：\",\"推理能力受限，泛化性下降明显\",\"可视化支持（见图5）：\",\"(a)：若缺失 AffCoT，模型无法进行类比推理，预测倾向训练集中已有的 affordance\",\"(b)：缺失 ObjCoT 时，模型无法精确聚焦于关键交互部位（如 kettle 的 spout）\"]},\"25\":{\"h\":\"5.4 Performance Analysis\",\"t\":[\"为进一步评估模型的理解与泛化能力，作者设计了多个分析实验。\",\"多个物体场景（Multiple Objects）：\",\"在同一张交互图像中存在多个物体时，模型能准确对每个对象生成独立的 affordance 区域（见图6）\",\"多种可供性（Multiple Affordances）：\",\"同一物体在不同交互图像中被推理出不同的 3D affordance 区域，体现出模型对语义的灵活解析能力（见图7）\",\"多实例鲁棒性（Multiple Instances）：\",\"在几何形状变化显著的同类物体中，模型依然能稳定预测合理的交互区域（见图8），说明其具备良好的泛化能力与鲁棒性\"]},\"26\":{\"h\":\"结论\",\"t\":[\"我们提出了一种开放词汇形式的 3D 物体可供性定位方法，该方法从交互图像中进行推理，能够突破预定义样本空间的限制，并推广至未见场景。为实现这一目标，我们设计了一个新颖的框架 —— 通过多头可供性链式思维（Multi-Head Affordance Chain-of-Thought）推理，挖掘物体的不变几何属性，并对潜在交互方式进行类比推理，同时结合跨模态特征对齐，实现对 3D 可供性区域的精准定位。\",\"此外，我们引入了目前最大规模的 3D 可供性数据集 PIADv2，涵盖 1.5 万张交互图像与超过 3.8 万个标注完整的 3D 物体。大量实验验证了我们提出的 GREAT 框架在多项评估指标上具有显著优势，能够在开放场景下支持可供性理解，有望提升机器人在未知环境中的自主交互能力。我们相信该研究将为视觉可供性理解领域带来新的启发并推动其发展。\",\"局限性与未来工作： GREAT 的主要局限在于其多步推理机制带来了较高的计算复杂度，在大规模或实时应用中可能成为瓶颈。未来，我们计划构建专用于推理的数据集，并利用这些数据集对多模态模型进行知识蒸馏，使其专注于特定领域，从而在实际应用中实现更快、更高效的性能。\"]},\"27\":{\"h\":\"代码\"},\"28\":{\"h\":\"Multi-Head Affordance Chain-of-Thought\",\"t\":[\"MHACoT是一种类人推理方式，分多个步骤，模拟人观察交互图像时的思维链条：\",\"识别交互部位（Object Interaction Perception）\",\"解析几何属性（Geometric Structure Reasoning）\",\"详细描述交互（Interaction Detailed Description）\",\"类比额外交互（Interactive Analogical Reasoning）\",\"每个子步骤都由一个 prompt 引导 MLLM（如 InternVL）做回答，从而获得：\",\"对象的交互区域\",\"Object Interaction Perception Prompt 1: Point out which part of the object in the image interacts with the person.\",\"🔹目标：定位交互发生的对象区域（如“水壶的壶嘴”）\",\"对应的几何属性\",\"Geometric Structure Reasoning Prompt 2: Explain why this part can interact from the geometric structure of the object.\",\"🔹目标：推理几何形态支持该交互（如“壶嘴上开口狭窄、带曲线”）\",\"当前交互行为\",\"Interaction Detailed Description Prompt 3: Describe the interaction between object and the person.\",\"🔹目标：细致地识别交互动作及其参与部位（如“用手握住壶把倒水”）\",\"潜在交互意图\",\"Interactive Analogical Reasoning Prompt 4: List two interactions that describe additional common interactions that the object can interact with people.\",\"🔹目标：推理除了当前交互以外，该物体常见的其他交互（如“开壶盖、抓握中部”）\",\"核心代码实现如下:\",\"# 1. 加载预训练多模态大模型 model = AutoModel.from_pretrained( path, torch_dtype=torch.bfloat16, #load_in_8bit=True, low_cpu_mem_usage=True, trust_remote_code=True, device_map=device_map).eval() tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False) # 2. 加载图像数据 image_path = 'PATH/Data/Kettle/Internet/pour/kettle_pour_1.jpg' pixel_values = load_image(image_path, max_num=12).to(torch.bfloat16).cuda() object = image_path.split('/')[-4] # 图像所属的物体名 # 3. 定位交互部位 question1 = f'Point out which part of the {object} in the image interacts with the person. If this part is different from the part of the {object} shown in the image that performs the main function, point out the part of the {object} that performs the main function shown in the image.' response1, history = model.chat(tokenizer, pixel_values, question1, generation_config, history=None, return_history=True) print(f'{response1}') # 4. 推理几何结构 question2 = f'Explain why this part can interact from the geometric structure of the {object}. Just give the final result in one sentence.' response2, history = model.chat(tokenizer, pixel_values, question2, generation_config, history=history, return_history=True) print(f'{response2}') # 5. 详细交互行为 question3 = f'Describe the interaction between {object} and the person in the image, including the interaction type, the interaction part of the {object}, and the interaction part of the person.' response3, history= model.chat(tokenizer, pixel_values, question3, generation_config, history=history, return_history=True) print(f'{response3}') # 6. 推测其他交互 question4 = f'List two interactions that describe additional common interactions that the {object} can interact with people, including the interaction type, the interaction part of the {object}, and the interaction part of the person.' response4, history= model.chat(tokenizer, pixel_values, question4, generation_config, history=history, return_history=True) print(f'{response4}') ''' Sample output 1. the spout of kettle. 2. a narrow opening, a slight curve and the spout's position at the top of the kettle. 3. pour the liquid from the spout of the kettle using people’s hand 4. grasp the kettle using person's hand around middle body, open the kettle using people's fingers on the lid object knowledge: the spout of kettle: a narrow opening, a slight curve and the spout's position at the top of the kettle. affordance/human knowledge: pour the liquid from the spout of the kettle using people’s hand, grasp the kettle using person's hand around handle, open the kettle using people's fingers on the lid ''\",\"其中:\",\"几何结构知识 = Prompt 1 + Prompt 2 的回答 = 交互部位 + 该部位的几何属性推理\",\"交互知识 = Prompt 3 + Prompt 4 的回答 = 当前交互 + 类比/补充的交互方式\",\"MHACoT 这个过程发生在数据集准备阶段。\"]},\"29\":{\"h\":\"数据集\",\"t\":[\"先了解一下GREAT项目对应的数据集目录结构:\",\"数据集的初始化:\",\"class PIAD(Dataset): def __init__(self, run_type, setting_type, point_path, img_path, text_hk_path, text_ok_path, pair=2, img_size=(224, 224)): super().__init__() self.run_type = run_type # 当前是训练/测试/验证环境 self.p_path = point_path # 点云索引文件路径 self.i_path = img_path # 图片索引文件路径 self.text_hk_path = text_hk_path # 物体几何结构文本数据文件路径 self.text_ok_path = text_ok_path # 人类交互文本数据文件路径 self.pair_num = pair # 控制每个 图像样本 对应多少个 3D点云样本 self.affordance_label_list = ['grasp', 'contain', 'lift', 'open', 'lay', 'sit', 'support', 'wrapgrasp', 'pour', 'move', 'display', 'push', 'listen', 'wear', 'press', 'cut', 'stab', 'carry', 'ride', 'clean', 'play', 'beat', 'speak', 'pull'] # 24 ... ''' Seen ''' # 43 if setting_type == 'Seen': number_dict = {'Bag': 0, 'Microphone': 0, 'Toothbrush': 0, 'TrashCan': 0, 'Bicycle': 0, 'Guitar': 0, 'Glasses': 0, 'Hat': 0, 'Microwave':0, 'Backpack': 0, 'Door':0, 'Scissors': 0, 'Bowl': 0, 'Baseballbat': 0, 'Mop': 0, 'Dishwasher': 0, 'Bed': 0, 'Keyboard': 0, 'Clock': 0, 'Vase': 0, 'Knife': 0, 'Suitcase': 0, 'Hammer': 0, 'Refrigerator': 0, 'Chair': 0, 'Umbrella': 0, 'Bucket': 0, 'Display': 0, 'Earphone': 0, 'Motorcycle': 0, 'StorageFurniture': 0, 'Fork': 0, 'Broom': 0, 'Skateboard': 0, 'Tennisracket': 0, 'Laptop': 0, 'Table':0, 'Bottle': 0, 'Faucet': 0, 'Kettle': 0, 'Surfboard': 0, 'Mug': 0, 'Spoon': 0 } # 读取所有图片路径，所有人类交互文本数据，所有物体几何结构文本数据 self.img_files = self.read_file(self.i_path) self.text_human_files = self.read_file(self.text_hk_path) self.text_object_files = self.read_file(self.text_ok_path) self.img_size = img_size if self.run_type == 'train': # 读取所有点云路径，同时记录每类物体对应的样本总量，比如: 椅子对应的点云一共1000个 self.point_files, self.number_dict = self.read_file(self.p_path, number_dict) self.object_list = list(number_dict.keys()) # 注意: Dict 按照key的插入顺序返回的 self.object_train_split = {} start_index = 0 # 记录每个物体对应的点云索引下标区间 for obj_ in self.object_list: temp_split = [start_index, start_index + self.number_dict[obj_]] self.object_train_split[obj_] = temp_split start_index += self.number_dict[obj_] else: self.point_files = self.read_file(self.p_path)\",\"为什么我们需要pair_num参数?\",\"问题背景：GREAT 需要将 2D 交互图像（Image）与 3D 点云（Point Cloud）的特征进行对齐，但同一物体的不同实例可能有几何差异（例如不同形状的椅子）。\",\"解决方案：通过为每张图像配对多个点云（pair_num > 1），模型能够学习从 多样化的几何变体 中提取共性的几何属性（如“可抓握”的共享结构特征），而不仅仅依赖单一实例。\",\"代码体现：在 getitem 中，训练时会对每个图像随机采样 pair_num 个同类别点云（见 point_sample_idx 的生成逻辑）\",\"GREAT 项目的数据组织中，将每个样本属于的物体类型，待预测功能区域类型全部隐含在了样本对应的文件路径中:\",\"获取数据:\",\" def __getitem__(self, index): # 1. 获取图片，人类交互文本，物体几何结构文本 img_path = self.img_files[index] text_hd = self.text_human_files[index] text_od = self.text_object_files[index] # 2.1 评估时需要标准的单一样本对比 if (self.run_type=='val'): point_path = self.point_files[index] else: # 2.2 从图片路径中截取得到物体名，交互行为名，点云索引下标区间 object_name = img_path.split('/')[-4] affordance_name = img_path.split('/')[-2] range_ = self.object_train_split[object_name] # 从索引区间中随机采样pair_num个点云样本 point_sample_idx = random.sample(range(range_[0],range_[1]), self.pair_num) # 3. 加载点云样本，同时判断是否与当前图片交互行为一致，不一致则重新随机选 for i ,idx in enumerate(point_sample_idx): while True: point_path = self.point_files[idx] sele_affordance = point_path.split('/')[-2] if sele_affordance == affordance_name: point_sample_idx[i] = idx break else: idx = random.randint(range_[0],range_[1]-1) # re-select idx Img = Image.open(img_path).convert('RGB') if(self.run_type == 'train'): Img = Img.resize(self.img_size) Img = img_normalize_train(Img) # 4. 加载列表中所有点云样本 Points_List = [] affordance_label_List = [] affordance_index_List = [] for id_x in point_sample_idx: point_path = self.point_files[id_x] # 加载点云数据和功能区域掩码(功能区域热力图) Points, affordance_label = self.extract_point_file(point_path) # （2048，3） Points,_,_ = pc_normalize(Points) Points = Points.transpose() # (3,2048) affordance_index = self.get_affordance_label(img_path) # 当前点云待预测的交互行为/功能区域类型 Points_List.append(Points) # 点云 affordance_label_List.append(affordance_label) # 功能区域热力图 affordance_index_List.append(affordance_index) # 待预测功能区域类型 else: Img = Img.resize(self.img_size) Img = img_normalize_train(Img) Point, affordance_label = self.extract_point_file(point_path) Point,_,_ = pc_normalize(Point) Point = Point.transpose() if(self.run_type == 'train'): # 图片 ， 交互信息文本，物体几何结构文本，点云样本列表，功能区域热力图列表，待预测功能区域类型列表 return Img, text_hd, text_od, Points_List, affordance_label_List, affordance_index_List else: return Img, text_hd, text_od, Point, affordance_label, img_path, point_path\"]},\"30\":{\"h\":\"模型\",\"t\":[\"class GREAT(nn.Module): ... def forward(self, img, xyz, text_human, text_object): ''' img: [B, 3, H, W] xyz: [B, 3, 2048] ''' B, C, N = xyz.size() # 1. 用Resnet18对图像进行编码，返回的高维隐向量维度为 (batch,512,7,7) -- （batch,channel,h,w) F_I = self.img_encoder(img) # 维度展平(batch,channel,h*w) F_i = F_I.view(B, self.emb_dim, -1) # 2， PointNet++ 对点云进行编码 F_p_wise = self.point_encoder(xyz) # 3. Roberta 对交互文本和几何结构文本进行编码 T_h= self.text_encoder(text_human) # (batch,3,512) T_o = self.text_encoder2(text_object) # (batch,1,512) # 4. 交互文本和几何结构文本的信息通过改良的交叉注意力机制进行交互融合 T_h_, T_o_ =self.affordance_dictionary_fusion(T_h, T_o) # 维度同上，均保持不变 # 5. 交互文本信息与图像信息进行融合 I_h = self.img_text_fusion(F_i,T_h_) # (batch,512,49) # 6. 几何结构文本信息与点云信息进行融合，然后进入pointnet++的特征传播阶段(插值阶段)，最后再与I_h进行交互融合 _3daffordance = self.decoder(T_o_, I_h.permute(0,2,1), F_p_wise) # T_o_(batch,1,512)，I_h.permute(batch,49,512)，点云特征 return _3daffordance\"]},\"31\":{\"h\":\"文本编码\",\"t\":[\"使用 RoBerta 对交互文本和几何结构文本进行编码这块，需要注意在对交互文本进行编码时，会按照 \\\",\\\" 将文本切分为多个句子，对每个句子独立进行编码:\",\"原始交互文本: pour the liquid from the spout of the kettle using people’s hand, grasp the kettle using person's hand around handle, open the kettle using people's fingers on the lid 切分后: pour the liquid from the spout of the kettle using people’s hand grasp the kettle using person's hand around handle open the kettle using people's fingers on the lid\",\"这样做的原因是因为交互文本由当前图片反映的交互行为和模型额外补充的当前物体存在的其他交互行为构成，他们之间的关系是独立的。而几何结构文本则是单一连贯的几何描述，无需切分，直接对整句进行编码。\"]},\"32\":{\"h\":\"改良的交叉注意力\",\"t\":[\"人类通过同时分析物体的 功能意图（如\\\"倒水\\\"）和 几何属性（如\\\"壶嘴的形状\\\"）来推断交互可能性。交叉注意力模拟了这种双向推理过程，通过建立意图与几何的显式关联，实现类似人类的类比推理能力。\",\"class Cross_Attention(nn.Module): ... def forward(self, hk, ok): ''' hk : human knowledge [B,N_hk,C] ok : object knowledge [B,N_ok,C] ''' # 用意图文本（如\\\"pour\\\"）筛选相关的几何特征（强化\\\"壶嘴\\\"结构，弱化\\\"把手\\\"） hk_q = self.proj_hq(hk) ok_key = self.proj_ok(ok) ok_value = self.proj_ov(ok) ok_key_ = torch.cat((hk_q,ok_key),dim=1) # 强化人类意图在物体语义推理中的引导作用 ok_value_ = torch.cat((hk_q,ok_value),dim=1) atten_I1 = torch.bmm(hk_q, ok_key_.permute(0, 2, 1))*self.scale atten_I1 = atten_I1.softmax(dim=-1) I_1 = torch.bmm(atten_I1, ok_value_) I_1 = self.layernorm(hk + I_1) # 用几何结构（如\\\"cylindrical handle\\\"）修正意图理解（排除与几何矛盾的意图） ok_q = self.proj_oq(ok) hk_key = self.proj_hk(hk) hk_value = self.proj_hv(hk) hk_key_ = torch.cat((ok_q,hk_key),dim=1) # 利用物体结构辅助推断更多人类交互意图 hk_value_ = torch.cat((ok_q,hk_value),dim=1) atten_I2 = torch.bmm(ok_q, hk_key_.permute(0, 2, 1))*self.scale atten_I2 = atten_I2.softmax(dim=-1) I_2 = torch.bmm(atten_I2, hk_value_) I_2 = self.layernorm(ok + I_2) return I_1, I_2\"]},\"33\":{\"h\":\"几何结构信息与交互信息的融合\",\"t\":[\"class affordance_dictionary_fusion(nn.Module): ... def forward(self,f_hk,f_ok): # 第一阶段：语义对齐（cross attention）➜ 把 Human 与 Object 信息“连接”起来 H, O = self.cross_atten(f_hk, f_ok) # 第二阶段：结构融合（self attention）➜ 在 Human 内部或 Object 内部 “整理、总结、泛化” H_= self.h_atten(H) O_= self.o_atten(O) return H_, O_\"]},\"34\":{\"h\":\"交互信息与图像特征的融合\",\"t\":[\"class img_text_fusion(nn.Module): def __init__(self, emb_dim = 512, proj_dim = 512): class SwapAxes(nn.Module): def __init__(self): super().__init__() def forward(self, x): return x.transpose(1, 2) super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim self.fusion = nn.Sequential( nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) self.reshape = nn.Sequential( nn.Linear(3, 3 * 8), # (batch,512,24) SwapAxes(), # (batch,24,512) nn.BatchNorm1d(3 * 8), nn.ReLU(), SwapAxes(), # (batch,512,24) nn.Linear(3 * 8, 49), # （batch,512,49) ) # F_i (batch,512,49) --> (batch,channel,H*W) def forward(self,F_i,T_h_): # T_h_(batch,3,512) ---> 转置后 (batch,512,3) --> reshape后 (batch,512,49) T_h_ = self.reshape(T_h_.permute(0,2,1)) # 拼接后: (batch,1024,49) I_ = torch.cat((F_i, T_h_),dim=1) # 通道维度上进行特征融合，同时降维: (batch.512,49) I_ = self.fusion(I_) return I_\"]},\"35\":{\"h\":\"解码阶段\",\"t\":[\"class Decoder(nn.Module): def __init__(self, additional_channel, emb_dim, proj_dim): class SwapAxes(nn.Module): def __init__(self): super().__init__() def forward(self, x): return x.transpose(1, 2) super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim #upsample self.fp3 = PointNetFeaturePropagation(in_channel=512+self.emb_dim, mlp=[768, 512]) self.fp2 = PointNetFeaturePropagation(in_channel=832, mlp=[768, 512]) self.fp1 = PointNetFeaturePropagation(in_channel=518+additional_channel, mlp=[512, 512]) self.cmff = Cross_Modal_Feature_Fusion(emb_dim, proj_dim) self.out_head = nn.Sequential( nn.Linear(self.emb_dim, self.emb_dim // 8), SwapAxes(), nn.BatchNorm1d(self.emb_dim // 8), nn.ReLU(), SwapAxes(), nn.Linear(self.emb_dim // 8, 1), ) self.reshape = nn.Sequential( nn.Linear(49, 49 * 8), SwapAxes(), nn.BatchNorm1d(49 * 8), nn.ReLU(), SwapAxes(), nn.Linear(49 * 8, 2048), ) self.sigmoid = nn.Sigmoid() self.fusion = nn.Sequential( nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) def forward(self, T_o, I_h, encoder_p): ''' T_o --->object knowledge embedding (batch,1,512) I_h ---> [B, N_i, C] (batch,49,512) encoder_p ---> [Hierarchy feature] ''' B, _, _ = I_h.shape # p_i[1]: (1,3,2048) , （1，320，512) , (1,512,128) , (1,512,64) --> (batch,features,points) # p_i[0] 为坐标 p_0, p_1, p_2, p_3 = encoder_p # 逐层点云特征列表 # 1. 传入数据维度: (1,1,512) , (1,64,512) , 点云特征和几何结构特征做特征融合 p_3[1] = self.cmff(T_o, p_3[1].transpose(-2, -1)) # (1,512,64) # 2. 进入PointNet++经典的特征传播阶段 up_sample = self.fp3(p_2[0], p_3[0], p_2[1], p_3[1]) # (1,512,128) up_sample = self.fp2(p_1[0], p_2[0], p_1[1], up_sample) # (1,512,512) up_sample = self.fp1(p_0[0], p_1[0], torch.cat([p_0[0], p_0[1]],1), up_sample) # (1,512,2048) # 3. I_h reshape后 (1,512,2048) F_I = self.reshape(I_h.permute(0,2,1)) # 4. 图像交互信息与点云特征做融合: 拼接后，通道维度上进行特征融合，同时降维: (1,512,2048) F_j = torch.cat((F_I, up_sample),dim=1) F_j_fusion = self.fusion(F_j) # 5. F_j_fusion.permute后(1,2048,512) --> (1,2048,1) _3daffordance = self.out_head(F_j_fusion.permute(0, 2, 1)) _3daffordance = self.sigmoid(_3daffordance) # 生成功能区域掩码 return _3daffordance\"]},\"36\":{\"h\":\"点云特征与几何结构特征的融合\",\"t\":[\"class Cross_Modal_Feature_Fusion(nn.Module): def __init__(self, emb_dim, proj_dim): class SwapAxes(nn.Module): def __init__(self): super().__init__() def forward(self, x): return x.transpose(1, 2) super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim self.cross_atten1 = Cross_Attention(emb_dim = self.emb_dim, proj_dim = self.proj_dim) # 假设输入数据维度为 (1,64,512) : 先降维，进行信息压缩 self.fc = nn.Sequential( nn.Linear(self.emb_dim, self.emb_dim//2), # (1,64,256) SwapAxes(), # (1,256,64) nn.BatchNorm1d(self.emb_dim // 2), nn.ReLU(), SwapAxes(), # (1,64,256) nn.Linear(self.emb_dim//2, self.emb_dim), # (1,64,512) SwapAxes(), # (1,512,64) nn.BatchNorm1d(self.emb_dim), SwapAxes(), # (1,64,512) ) self.norm1 = nn.LayerNorm(self.emb_dim) self.norm2 = nn.LayerNorm(self.emb_dim) self.pool = nn.AdaptiveAvgPool1d(1) self.fusion = nn.Sequential( nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) # (1,1,512) , (1,64,512) def forward(self,f_t,f_p): _, N_P, _ = f_p.size() # 1. 应用改良的交叉注意力机制 f_to, f_po = self.cross_atten1(f_t, f_p) # 2. 注意力后，加上经典的: x + FNN f_to = f_to + self.fc(f_to) f_po = f_po + self.fc(f_po) # 3. f_to.permute维度(1,512,1) --> pool后(1,512,1) f_t_p = self.pool(f_to.permute(0,2,1)) # 4. 维度扩展到64 --> (1,512,64) f_t_r = f_t_p.repeat(1, 1, N_P) # 5. f_po.permute维度(1,512,64) --> 拼接后(1,1024,64) joint = torch.cat((f_po.permute(0,2,1), f_t_r), dim = 1) # 6. 通道维度作信息融合(1,512,64) output = self.fusion(joint) return output\"]},\"37\":{\"h\":\"LMAffordance3D 模型代码解读与复现\",\"t\":[\"Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions 论文代码解读与复现\",\"论文: https://arxiv.org/abs/2504.04744 代码: https://github.com/cn-hezhu/LMAffordance3D\",\"由于论文数据集还未开源，加之原本在Github上开源的代码后续被下架，导致本论文复现流程暂时终止。\"]},\"38\":{\"h\":\"环境配置 (待完善)\",\"t\":[\"建议用Linux或者Windows系统进行测试，MacOS系统某些包的加载和依赖关系上存在问题，不方便进行处理。\"]},\"39\":{\"h\":\"模型结构\",\"t\":[\"模型结构图\"]},\"40\":{\"h\":\"LMAffordance3D\",\"t\":[\"class LMAffordance3D(Blip2Base): ... def forward(self, img, point, description, label, inference_mode=False): ''' img: [B, 3, H, W] -> 输入图像 (batch_size, channels, height, width) point: [B, 3, 2048] -> 点云数据 (batch_size, dimensions, num_points) description: 自然语言指令 (e.g., \\\"Grasp the bottle\\\") label: 真实标签，即每个点对应的 affordance 概率分布 (B, 2048, 1) inference_mode: 是否为推理模式（True/False） ''' # 获取输入维度信息 B, C, H, W = img.size() B, D, N = point.size() device = img.device # 获取设备信息（CPU/GPU） # Step 1: 提取图像和点云的特征 # -------------------------------------------------- # 图像编码器：ResNet18 提取 2D 特征 F2D ∈ RB×CI×H×W img_feature = self.img_encoder(img) # shape: [B, CI, H', W'] # 点云编码器：PointNet++ 提取 3D 特征 F3D ∈ RB×CP×NP point_feature = self.point_encoder(point) # shape: [B, CP, NP] # Step 2: 融合多模态空间特征 # -------------------------------------------------- # 使用 MLP 和自注意力机制融合图像与点云特征 spatial_feature = self.fusion(img_feature, point_feature) # shape: [B, NS, CS] # Step 3: 多模态特征投影到语言语义空间 # -------------------------------------------------- # 将融合后的空间特征通过适配器上采样到与语言模型匹配的维度 if self.has_qformer: ... # 如果使用 Q-Former，则进行额外处理 else: multi_embeds = self.adapter_up(spatial_feature) # shape: [B, NS, CL] image_atts = None # 默认图像注意力掩码为空 # Step 4: 对自然语言指令进行 Tokenization # -------------------------------------------------- # 设置 tokenizer 的 padding 和 truncation 方向 self.llm_tokenizer.padding_side = \\\"right\\\" self.llm_tokenizer.truncation_side = 'left' # 对语言指令进行分词，转换为 token ID 并生成 attention mask text_input_tokens = self.llm_tokenizer( description, return_tensors=\\\"pt\\\", padding=\\\"longest\\\", # 填充至最长序列长度 truncation=True, # 截断过长文本 max_length=self.max_txt_len, # 最大文本长度 ).to(device) # Step 5: 获取语言嵌入 # -------------------------------------------------- # 使用 LLM 的 embedding 层将 token ID 转换为嵌入向量 inputs_embeds = self.llm_model.get_input_embeddings()(text_input_tokens.input_ids) # shape: [B, NL, CL] （NL=token数，CL=语言嵌入维度） # Step 6: 拼接多模态嵌入与语言嵌入 # -------------------------------------------------- # 调用 concat_input 函数，将图像+点云特征插入语言嵌入中 llm_inputs, llm_attention_mask = self.concat_input( inputs_embeds, text_input_tokens.attention_mask, multi_embeds, image_atts ) # llm_inputs: [B, NL + NS, CL] # llm_attention_mask: [B, NL + NS] # Step 7: 使用 Vision-Language Model 进行联合推理 # -------------------------------------------------- # 在混合精度下运行 LLM，融合语言与视觉特征 with self.maybe_autocast(): hidden_states = self.llm_model( inputs_embeds=llm_inputs, attention_mask=llm_attention_mask, return_dict=False, # 返回 tuple 格式输出 ) # Step 8: 降维适配器 # -------------------------------------------------- # 通过适配器层将 LLM 输出映射回合适维度 hidden_states = self.adapter_down(hidden_states) # shape: [B, NS + NL, CS] # 分割出 semantic feature 和 instructional feature # 视觉语义特征 和 语言指令理解特征 semantic_feature, instructional_feature = torch.split( hidden_states, split_size_or_sections=spatial_feature.size(1), dim=1 ) # Step 9: 解码器融合所有特征以预测可操作性特征 # -------------------------------------------------- # 使用 cross-attention 融合 instruction, semantic, spatial features affordance_feature = self.affordance_decoder( spatial_feature, instructional_feature, semantic_feature ) # shape: [B, NA, CA] # Step 10: 使用分割头预测最终的 3D 可操作性热图 # -------------------------------------------------- out = self.head(spatial_feature, affordance_feature, point_feature) # 输出 shape: [B, 2048, 1]，表示每个点是否具有特定可操作性的概率 # Step 11: 推理或训练分支 # -------------------------------------------------- if inference_mode == True: return out # 仅返回预测结果 else: loss_hm = self.loss_hm(out, label) # 计算 heatmap 的损失（focal + dice） loss = loss_hm * self.w_hm # 加权总损失 return { \\\"out\\\": out, \\\"loss\\\": loss, \\\"loss_hm\\\": loss_hm }\"]},\"41\":{\"h\":\"Step 2: 融合多模态空间特征\",\"t\":[\"class Fusion(nn.Module): def __init__(self, emb_dim = 512, num_heads = 4): super().__init__() self.emb_dim = emb_dim # 对点积结果进行缩放，防止 softmax 梯度消失或爆炸。 self.div_scale = self.emb_dim ** (-0.5) self.num_heads = num_heads # 对图像和点云特征进行 非线性增强和空间对齐 ，使得它们能够在统一的语义空间中进行有效的跨模态交互。 self.mlp = nn.Sequential( nn.Conv1d(self.emb_dim, 2*self.emb_dim, 1, 1), nn.BatchNorm1d(2*self.emb_dim), nn.ReLU(), nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) self.img_attention = Self_Attention(self.emb_dim, self.num_heads) self.point_attention = Self_Attention(self.emb_dim, self.num_heads) self.joint_attention = Self_Attention(self.emb_dim, self.num_heads) def forward(self, img_feature, point_feature): ''' i_feature: [B, C, H, W] p_feature: [B, C, N_p] HW = N_i ''' B, C, H, W = img_feature.size() img_feature = img_feature.view(B, self.emb_dim, -1) #[B, C, N_i] point_feature = point_feature[-1][1] # 对图像和点云特征进行 非线性增强和空间对齐 ，使得它们能够在统一的语义空间中进行有效的跨模态交互。 p_feature = self.mlp(point_feature) i_feature = self.mlp(img_feature) # 跨模态注意力矩阵: 每个点云点与图像中每个位置之间的相似度得分 phi = torch.bmm(p_feature.permute(0, 2, 1), i_feature)*self.div_scale #[B, N_p, N_i] # 每列是一个 softmax 分布（每个图像位置对应的所有点云点）, 表示：“对于图像中的每一个位置，应该关注哪些点云点？” phi_p = F.softmax(phi,dim=1) # 每行是一个 softmax 分布（每个点云点对应的所有图像位置）, 表示：“对于点云中的每一个点，应该关注图像中的哪些位置？” phi_i = F.softmax(phi,dim=-1) # I_enhance 是图像 patch 引导下提取的点云信息增强后的图像特征 # 它不是直接包含原始图像 patch 的语义 # 而是通过“点云中相关点”的方式重构图像 patch 的语义 I_enhance = torch.bmm(p_feature, phi_p) #[B, C, N_i] # P_enhance 是每个点云局部区域关键点引导下提取的图像信息增强后的点云关键点局部区域特征 P_enhance = torch.bmm(i_feature, phi_i.permute(0,2,1)) #[B, C, N_p] # 在跨模态融合后，进一步提取各自模态内部的语义一致性与结构关系，形成更稳定的联合表示。 I = self.img_attention(I_enhance.mT) #[B, N_i, C] P = self.point_attention(P_enhance.mT) #[B, N_p, C] # 将图像patch和点云点拼接成一个统一的token序列 # 使用自注意力机制提炼两个模态之间的语义一致性 joint_patch = torch.cat((P, I), dim=1) multi_feature = self.joint_attention(joint_patch) #[B, N_p+N_i, C] return multi_feature\"]},\"42\":{\"h\":\"Step 3: 多模态特征投影到语言语义空间\",\"t\":[\" # 将融合后的 3D 和 2D 特征从原始嵌入维度 (self.emb_dim) 映射到 LLM（语言模型）所使用的隐藏状态空间维度 （self.llm_model.config.hidden_size）。 self.adapter_up = nn.Sequential( nn.Linear(self.emb_dim, self.emb_dim), nn.ReLU(), nn.Linear(self.emb_dim, self.llm_model.config.hidden_size) )\"]},\"43\":{\"h\":\"Step 6: 拼接多模态嵌入与语言嵌入\",\"t\":[\"def concat_input(self, input_embeds, input_atts, multi_embeds, image_atts=None): ''' 将语言嵌入（text embeddings）与多模态嵌入（如图像、点云等）拼接在一起， 构建 Vision-Language Model (VLM) 所需的输入格式。 Args: input_embeds: (batch_size, sequence_length, hidden_size) - 语言 token 经过 embedding 层后的结果。 input_atts: (batch_size, sequence_length) - 语言部分的 attention mask（1 表示有效，0 表示填充）。 multi_embeds: (batch_size, n, hidden_size) - 多模态嵌入（如图像或点云特征），形状为 [B, n, H]。 image_atts: (batch_size, n), optional - 多模态数据的 attention mask，默认为全 1（即所有 token 都有效）。 Returns: llm_inputs: (batch_size, total_length, hidden_size) - 拼接后的输入嵌入，供 LLM 使用。 llm_attention_mask: (batch_size, total_length) - 对应的注意力掩码。 ''' # 初始化用于存储每个样本拼接后输入和 attention mask 的列表 llm_inputs = [] llm_attention_mask = [] # 获取 batch size bs = multi_embeds.size()[0] # 对每个样本单独处理（逐个拼接） for i in range(bs): # 获取当前样本中多模态嵌入的维度信息：(n, dim) _, n, dim = multi_embeds.size() # 计算当前语言输入中有多少个有效 token（非 padding） this_input_ones = input_atts[i].sum() # 拼接嵌入向量： # 语言前半段（有效的部分）+ 多模态嵌入 + 语言后半段（padding 部分） llm_inputs.append( torch.cat([ input_embeds[i][:this_input_ones], # 有效语言部分 multi_embeds[i], # 插入的多模态嵌入 input_embeds[i][this_input_ones:] # 剩余的语言 padding 部分 ]) ) # 构建 attention mask： if image_atts is None: # 如果没有提供 image_atts，则默认多模态 token 都是有效的（mask 全为 1） llm_attention_mask.append( torch.cat([ input_atts[i][:this_input_ones], torch.ones((n), device=multi_embeds.device, dtype=torch.long), input_atts[i][this_input_ones:] ]) ) else: # 否则使用给定的 image_atts 来标记哪些多模态 token 是有效的 llm_attention_mask.append( torch.cat([ input_atts[i][:this_input_ones], image_atts[i], input_atts[i][this_input_ones:] ]) ) # 将 list 转换为 batched tensor llm_inputs = torch.stack(llm_inputs, 0) llm_attention_mask = torch.stack(llm_attention_mask, 0) # 返回拼接好的输入和 attention mask return llm_inputs, llm_attention_mask\"]},\"44\":{\"h\":\"Step 8: 降维适配器\",\"t\":[\" # 降维适配器：将 LLM 输出的隐藏状态映射回原始嵌入维度（self.emb_dim） self.adapter_down = nn.Sequential( nn.Linear(self.llm_model.config.hidden_size, self.llm_model.config.hidden_size), nn.ReLU(), nn.Linear(self.llm_model.config.hidden_size, self.emb_dim) )\"]},\"45\":{\"h\":\"Step 9: 解码器融合所有特征以预测可操作性特征\",\"t\":[\"class Affordance_Decoder(nn.Module): def __init__(self, emb_dim, proj_dim): super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim self.cross_atten = Cross_Attention(emb_dim = self.emb_dim, proj_dim = self.proj_dim) self.fusion = nn.Sequential( nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) def forward(self, query, key, value): ''' query: [B, N_p + N_i, C] -> spatial_feature (query) key: [B, N_l, C] -> instructional_feature (key) value: [B, N_l, C] -> semantic_feature (value) ''' B, _, C = query.size() # 调整 key 和 value 的形状为 [B, C, N_l] key = key.view(B, C, -1) # [B, C, N_l] value = value.view(B, C, -1) # [B, C, N_l] # 使用 cross attention 获取两个注意力加权结果 Theta_1, Theta_2 = self.cross_atten(query, key.mT, value.mT) # 将两个注意力输出拼接在一起 joint_context = torch.cat((Theta_1.mT, Theta_2.mT), dim=1) # [B, 2C, N_p + N_i] # 使用 Conv1D 融合通道信息 affordance = self.fusion(joint_context) # [B, C, N_p + N_i] # 调整输出格式为 [B, N_p + N_i, C] affordance = affordance.permute(0, 2, 1) # [B, N_p + N_i, C] return affordance\",\"class Cross_Attention(nn.Module): def __init__(self, emb_dim, proj_dim): \\\"\\\"\\\" 多模态交叉注意力模块（Cross-Attention Module）， 用于融合来自语言模型的不同语义信息，增强空间特征表达。 Args: emb_dim: 输入特征维度（embedding dimension），例如 LLM 的 hidden size（如 4096） proj_dim: 投影维度，用于降低计算复杂度，在 attention 中使用 \\\"\\\"\\\" super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim # 定义投影层，将输入映射到低维空间以进行 attention 计算 self.proj_q = nn.Linear(self.emb_dim, proj_dim) # query 投影 self.proj_sk = nn.Linear(self.emb_dim, proj_dim) # sub key 投影 self.proj_sv = nn.Linear(self.emb_dim, proj_dim) # sub value 投影 self.proj_ek = nn.Linear(self.emb_dim, proj_dim) # scene key 投影 self.proj_ev = nn.Linear(self.emb_dim, proj_dim) # scene value 投影 # 缩放因子，用于 attention 分数归一化 self.scale = self.proj_dim ** (-0.5) # 层归一化（LayerNorm），用于稳定训练过程 self.layernorm = nn.LayerNorm(self.emb_dim) def forward(self, obj, sub, scene): \\\"\\\"\\\" 执行交叉注意力机制，融合不同来源的信息： - obj: 空间特征（spatial feature），作为 query； - sub: 指令理解特征（instructional feature），作为第一个 attention 的 key 和 value； - scene: 视觉语义特征（semantic feature），作为第二个 attention 的 key 和 value； Args: obj: [B, N_p + HW, C] → spatial_feature（query 来源） sub: [B, HW, C] → instructional_feature（key/value 来源之一） scene: [B, HW, C] → semantic_feature（key/value 来源之二） Returns: I_1: 经过 attention 加权后的输出（第一分支） I_2: 经过 attention 加权后的输出（第二分支） \\\"\\\"\\\" B, seq_length, C = obj.size() # 获取 batch size 和通道维度 # 将输入分别投影到低维空间，便于后续 attention 计算 query = self.proj_q(obj) # [B, N_q, proj_dim] s_key = self.proj_sk(sub) # [B, N_i, proj_dim] s_value = self.proj_sv(sub) # [B, N_i, proj_dim] e_key = self.proj_ek(scene) # [B, N_e, proj_dim] e_value = self.proj_ev(scene) # [B, N_e, proj_dim] # 第一个 cross attention：使用 sub 的 key 和 value 增强 query atten_I1 = torch.bmm(query, s_key.mT) * self.scale # [B, N_q, N_i] atten_I1 = atten_I1.softmax(dim=-1) # softmax 归一化 I_1 = torch.bmm(atten_I1, s_value) # [B, N_q, proj_dim] # 第二个 cross attention：使用 scene 的 key 和 value 增强 query atten_I2 = torch.bmm(query, e_key.mT) * self.scale # [B, N_q, N_e] atten_I2 = atten_I2.softmax(dim=-1) I_2 = torch.bmm(atten_I2, e_value) # [B, N_q, proj_dim] # 使用残差连接 + LayerNorm 增强稳定性 I_1 = self.layernorm(obj + I_1) # [B, N_q, emb_dim] I_2 = self.layernorm(obj + I_2) # [B, N_q, emb_dim] return I_1, I_2\"]},\"46\":{\"h\":\"Step 10: 使用分割头预测最终的 3D 可操作性热图\",\"t\":[\"class Head(nn.Module): def __init__(self, additional_channel, emb_dim, N_p, N_raw): \\\"\\\"\\\" Head 模块用于最终的 3D 可操作性（affordance）预测。 它接收来自编码器和解码器的特征，并通过多尺度上采样与融合， 输出每个点云点的 affordance 热图（heatmap），表示该点是否具有可操作性。 Args: additional_channel: 额外通道数，例如法向量、颜色等信息 emb_dim: 特征维度（embedding dimension） N_p: point cloud token 数量（如 64） N_raw: 原始点云数量（如 2048） Notes: - 使用 PointNetFeaturePropagation 进行逐级上采样； - 结合全局池化增强语义表达； - 最终使用 MLP + Sigmoid 输出每个点的 affordance score； \\\"\\\"\\\" super().__init__() self.emb_dim = emb_dim self.N_p = N_p # point cloud token 数量 self.N_raw = N_raw # 原始点云数量（如 2048） # 多尺度上采样模块：PointNetFeaturePropagation # fp3: 输入为 [512 + emb_dim]，输出为 512 维度 self.fp3 = PointNetFeaturePropagation(in_channel=512+self.emb_dim, mlp=[768, 512]) self.fp2 = PointNetFeaturePropagation(in_channel=832, mlp=[768, 512]) self.fp1 = PointNetFeaturePropagation(in_channel=518+additional_channel, mlp=[512, 512]) # 全局平均池化层，压缩时间/空间维度 self.pool = nn.AdaptiveAvgPool1d(1) # 最终输出头：MLP + BatchNorm + ReLU + Sigmoid self.out_head = nn.Sequential( nn.Linear(self.emb_dim, self.emb_dim // 8), nn.BatchNorm1d(self.N_raw), # 对点数维度做 BN nn.ReLU(), nn.Linear(self.emb_dim // 8, 1), # 输出每个点的 affordance score nn.Sigmoid() # 输出范围 [0,1]，表示概率 ) def forward(self, multi_feature, affordance_feature, encoder_p): \\\"\\\"\\\" 执行 Head 模块的前向传播，生成最终的 3D affordance heatmap。 Args: multi_feature: [B, N_p + N_i, C] → 来自 Vision-Language Model 的拼接特征 affordance_feature: [B, N_p + N_i, C] → 来自 decoder 的可操作性特征 encoder_p: [p0, p1, p2, p3] → 编码器不同层级的点云特征 Returns: out: [B, N_raw, 1] → 每个点的 affordance score（概率值） \\\"\\\"\\\" B, N, C = multi_feature.size() # 解包编码器输出的不同层级特征 p_0, p_1, p_2, p_3 = encoder_p # 从 multi_feature 和 affordance_feature 中提取 point cloud token 部分 P_align, _ = torch.split(multi_feature, split_size_or_sections=self.N_p, dim=1) F_pa, _ = torch.split(affordance_feature, split_size_or_sections=self.N_p, dim=1) # 上采样过程：fp3 -> fp2 -> fp1 up_sample = self.fp3(p_2[0], p_3[0], p_2[1], P_align.mT) # [B, emb_dim, npoint_sa2] up_sample = self.fp2(p_1[0], p_2[0], p_1[1], up_sample) # [B, emb_dim, npoint_sa1] up_sample = self.fp1(p_0[0], p_1[0], torch.cat([p_0[0], p_0[1]], 1), up_sample) # [B, emb_dim, N_raw] # 对 F_pa 做全局池化，得到一个全局语义向量 F_pa_pool = self.pool(F_pa.mT) # [B, emb_dim, 1] # 将全局语义向量扩展回原始点云数量，实现 feature-wise attention affordance = up_sample * F_pa_pool.expand(-1, -1, self.N_raw) # [B, emb_dim, N_raw] # 输出 head：将特征映射到 0~1 的概率值，表示每个点是否具有可操作性 out = self.out_head(affordance.mT) # [B, N_raw, 1] return out\"]},\"47\":{\"h\":\"IAGNet 论文解读\",\"t\":[\"Grounding 3D Object Affordance from 2D Interactions in Images 论文解读\",\"论文: https://arxiv.org/abs/2303.10437 代码: https://github.com/yyvhang/IAGNet 数据集: https://drive.google.com/drive/folders/1F242TsdXjRZkKQotiBsiN2u6rJAGRZ2W\"]},\"48\":{\"h\":\"摘要\",\"t\":[\"这篇论文提出了一种新颖的任务设定：通过2D图像中的交互信息来预测3D物体的功能区域（affordance），旨在为具身智能体建立感知与操作之间的联系。作者设计了一个名为IAG（Interaction-driven 3D Affordance Grounding Network）的框架，通过联合区域对齐模块（JRA）解决不同来源物体区域的对齐问题，并通过功能揭示模块（ARM）建模交互上下文以明确功能区域。此外，作者还构建了一个包含图像-点云配对数据的数据集PIAD，用于支持该任务。实验结果表明，该方法在PIAD数据集上表现优异，验证了任务设定的可行性和方法的有效性。这一研究为功能学习领域提供了新的视角，并有望应用于机器人操作、增强现实等领域。\"]},\"49\":{\"h\":\"简介\",\"t\":[\"Gibson（2014）提出的“功能可供性”（affordance）概念，即物体支持的交互可能性，是连接具身智能体感知与操作的关键。现有研究主要分为两类：\",\"几何结构映射方法（如11、22）通过标注物体交互区域建立几何结构与功能的固定关联，但泛化性受限，且对多功能的相似结构易产生混淆（如图2(b)中椅子的“坐”与“移动”功能）。\",\"强化学习方法（如54）通过智能体在虚拟环境中主动交互学习功能，但搜索空间大、耗时严重。\",\"本文创新点：\",\"任务设定：首次提出通过2D交互图像预测3D物体功能区域（如图1），模拟人类通过观察学习物体功能的能力。\",\"核心挑战： \",\"对齐模糊性：2D演示与3D物体来自不同实例，需跨源对齐区域（图2(a)展示同类物体的结构相似性可辅助对齐）。\",\"功能模糊性：同一物体区域可能支持多功能（如“杯子”既可“握持”也可“盛放”），需通过交互上下文建模解决（图2(b)）。\",\"解决方案：\",\"IAG框架：包含JRA模块（通过密集跨模态相似性 对齐区域）和ARM模块（通过交叉注意力建模物体-主体/场景交互以揭示功能）。\",\"PIAD数据集：包含7,012个点云和5,162张图像，覆盖23类物体和17种功能，支持“可见”与“未见”场景的评估（图4）。\",\"意义：该方法摆脱了对几何标注或固定场景的依赖，为机器人操作、AR/VR等应用提供了更通用的功能理解范式。\"]},\"50\":{\"h\":\"相关工作\"},\"51\":{\"h\":\"\",\"t\":[\"现有研究可分为三类（如表1所示）：\",\"2D功能检测：\",\"早期工作（如12、69）从图像/视频中分割功能区域，但无法定位具体交互部位。\",\"语言辅助方法（如36）结合文本描述提升语义理解。\",\"3D功能定位：\",\"基于几何映射的方法（如11）直接关联结构与功能，泛化性差。\",\"强化学习方法（如54）通过智能体主动交互学习，但效率低。\",\"机器人操作应用：\",\"针对铰接物体（如48）设计功能热图，指导抓取和运动规划。\",\"本文区别：首次通过非配对的2D-3D数据学习功能，摆脱几何标注和固定场景限制。\"]},\"52\":{\"h\":\"\",\"t\":[\"现有方法依赖两类对齐策略：\",\"空间先验对齐：\",\"基于相机参数（如68、90）将点云投影到图像平面，需严格的空间对应。\",\"特征空间对齐：\",\"无相机参数方法（如1、6）直接建模跨模态特征相似性。\",\"本文创新：利用功能-结构的隐式关联（如图2(a)），在无空间先验下实现跨源特征对齐。\"]},\"53\":{\"h\":\"方法\"},\"54\":{\"h\":\"\",\"t\":[\"如图3所示，IAG网络输入为四元组 ，其中：\",\" 为点云坐标\",\" 为RGB图像\",\" 为图像中主体和物体的边界框\",\" 为功能类别标签\",\"处理流程：\",\"特征提取：\",\"图像分支：ResNet提取特征 \",\"点云分支：PointNet++提取特征 \",\"区域定位：\",\"通过ROI-Align获取物体/主体/场景特征 （）\",\"联合区域对齐（JRA模块）：\",\"计算密集跨模态相似性矩阵：\",\"通过自注意力建模模态内结构关系：\",\"联合注意力生成对齐特征 \",\"功能揭示（ARM模块）：\",\"交叉注意力建模交互上下文：\",\"融合生成功能表征 \",\"解码输出：\",\"功能类别预测 ：对 和 池化后拼接\",\"3D功能热图 ：通过特征传播层上采样：\"]},\"55\":{\"h\":\"\",\"t\":[\"总损失包含三项：\",\"功能分类损失：交叉熵损失监督 \",\"特征分布对齐损失：KL散度约束 与 分布：\",\"热图回归损失：Focal Loss + Dice Loss监督 \",\"最终损失为加权和：\"]},\"56\":{\"h\":\"\",\"t\":[\"JRA模块：通过跨模态相似性（）和联合注意力（）实现无先验对齐\",\"ARM模块：通过双路交叉注意力分别建模物体-主体（）和物体-场景（）交互\",\"互优化机制： 使功能表征与对齐特征相互增强（如图15所示）\"]},\"57\":{\"h\":\"代码\"},\"58\":{\"h\":\"数据集\",\"t\":[\"数据集目录下的组织方式:\",\"数据集初始化\",\"class PIAD(Dataset): def __init__(self, run_type, setting_type, point_path, img_path, box_path, pair=2, img_size=(224, 224)): super().__init__() self.run_type = run_type # train/val/test self.p_path = point_path self.i_path = img_path self.b_path = box_path # 记录物体边界框 self.pair_num = pair self.affordance_label_list = ['grasp', 'contain', 'lift', 'open', 'lay', 'sit', 'support', 'wrapgrasp', 'pour', 'move', 'display', 'push', 'listen', 'wear', 'press', 'cut', 'stab'] ... ''' Seen ''' if setting_type == 'Seen': number_dict = {'Earphone': 0, 'Bag': 0, 'Chair': 0, 'Refrigerator': 0, 'Knife': 0, 'Dishwasher': 0, 'Keyboard': 0, 'Scissors': 0, 'Table': 0, 'StorageFurniture': 0, 'Bottle': 0, 'Bowl': 0, 'Microwave': 0, 'Display': 0, 'TrashCan': 0, 'Hat': 0, 'Clock': 0, 'Door': 0, 'Mug': 0, 'Faucet': 0, 'Vase': 0, 'Laptop': 0, 'Bed': 0} # 读取出所有图片路径，存储了物体边界框文件路径 self.img_files = self.read_file(self.i_path) self.box_files = self.read_file(self.b_path) self.img_size = img_size if self.run_type == 'train': # 读取出所有点云文件路径,同时记录每类物体共对应多少不同的点云 self.point_files, self.number_dict = self.read_file(self.p_path, number_dict) self.object_list = list(number_dict.keys()) self.object_train_split = {} start_index = 0 # 记录每类物体对应的点云文件下标索引区间 for obj_ in self.object_list: temp_split = [start_index, start_index + self.number_dict[obj_]] self.object_train_split[obj_] = temp_split start_index += self.number_dict[obj_] else: self.point_files = self.read_file(self.p_path)\",\"获取数据\",\" def __getitem__(self, index): # 1. 获取图片，Box框文件路径 img_path = self.img_files[index] box_path = self.box_files[index] if (self.run_type=='val'): point_path = self.point_files[index] else: # 2. 从文件路径中提取物体名 object_name = img_path.split('_')[-3] # 3. 一张图片对应多张同物体但形状不同的点云图片 range_ = self.object_train_split[object_name] point_sample_idx = random.sample(range(range_[0],range_[1]), self.pair_num) Img = Image.open(img_path).convert('RGB') if(self.run_type == 'train'): # 4. 随机裁剪图片，同时获取裁剪后的物体框(交互主体框，目标物体框) Img, subject, object = self.get_crop(box_path, Img, self.run_type) # 5. 对图片进行缩放，同时等比例对物体框做同样的缩放 sub_box, obj_box = self.get_resize_box(Img, self.img_size, subject, object) sub_box, obj_box = torch.tensor(sub_box).float(), torch.tensor(obj_box).float() Img = Img.resize(self.img_size) Img = img_normalize_train(Img) Points_List = [] affordance_label_List = [] affordance_index_List = [] # 6. 加载点云 for id_x in point_sample_idx: point_path = self.point_files[id_x] Points, affordance_label = self.extract_point_file(point_path) Points,_,_ = pc_normalize(Points) Points = Points.transpose() affordance_label, affordance_index = self.get_affordance_label(img_path, affordance_label) Points_List.append(Points) affordance_label_List.append(affordance_label) affordance_index_List.append(affordance_index) else: ... if(self.run_type == 'train'): # 7. 图片，点云列表，点云功能区域掩码列表，点云功能区域索引列表，交互主体框，目标物体框 return Img, Points_List, affordance_label_List, affordance_index_List, sub_box, obj_box else: return Img, Point, affordance_label, img_path, point_path, sub_box, obj_box\"]},\"59\":{\"h\":\"模型\",\"t\":[\"class IAG(nn.Module): ... def forward(self, img, xyz, sub_box, obj_box): ''' img: [B, 3, H, W] xyz: [B, 3, 2048] sub_box: bounding box of the interactive subject obj_box: bounding box of the interactive object ''' B, C, N = xyz.size() ... # 1. ResNet18 编码图像 (batch,512,7,7) F_I = self.img_encoder(img) # 2. 利用ROI Align技术，得到目标物体区域特征，交互主体区域特征，背景区域特征 ROI_box = self.get_roi_box(B).to(device) F_i, F_s, F_e = self.get_mask_feature(img, F_I, sub_box, obj_box, device) # 背景区域特征图经过ROI Align映射为4*4大小的特征图 # ROI_box 大小为 7*7 , 正好为resnet18最后生成的特征图的分辨率, 因为背景区域大小等于特征图大小 F_e = roi_align(F_e, ROI_box, output_size=(4,4)) # F_i (batch,512,4,4) , F_s (batch,512,4,4) , F_e (batch,512,4,4) # 3. PointNet编码点云 # (B,3,2048) , (B,320,512) , (B,512,128) ， (B,512,64) F_p_wise = self.point_encoder(xyz) # 4. F_j = self.JRA(F_i, F_p_wise[-1][1]) # 5. affordance = self.ARM(F_j, F_s, F_e) _3daffordance, logits, to_KL = self.decoder(F_j, affordance, F_p_wise) return _3daffordance, logits, to_KL\",\"关于利用ROI Align技术，得到目标物体区域特征，交互主体区域特征，背景区域特征过程的实现细节如下:\",\" def get_mask_feature(self, raw_img, img_feature, sub_box, obj_box, device): raw_size = raw_img.size(2) current_size = img_feature.size(2) B = img_feature.size(0) # 1. 计算经过下采样得到的特征图相比于原始图片的缩小比例 scale_factor = current_size / raw_size # 2. 将交互主体框和目标物体框等比例缩小 sub_box[:, :] = sub_box[:, :] * scale_factor obj_box[:, :] = obj_box[:, :] * scale_factor # 3. 根据目标物体框，将掩码图像中目标物体所在区域激活，得到目标物体区域掩码 obj_mask = torch.zeros_like(img_feature) obj_roi_box = [] for i in range(B): obj_mask[i,:, int(obj_box[i][1]+0.5):int(obj_box[i][3]+0.5), int(obj_box[i][0]+0.5):int(obj_box[i][2]+0.5)] = 1 roi_obj = [obj_box[i][0], obj_box[i][1], obj_box[i][2]+0.5, obj_box[i][3]] # 对交互主体框位置进行精细调整(just a trick) roi_obj.insert(0, i) # 插入批次索引 -- ROI Align对齐方法需要 obj_roi_box.append(roi_obj) obj_roi_box = torch.tensor(obj_roi_box).float().to(device) sub_roi_box = [] # 4. 根据交互主体框，在目标物体区域掩码之上，激活交互主体所在区域 Scene_mask = obj_mask.clone() for i in range(B): Scene_mask[i,:, int(sub_box[i][1]+0.5):int(sub_box[i][3]+0.5), int(sub_box[i][0]+0.5):int(sub_box[i][2]+0.5)] = 1 roi_sub = [sub_box[i][0], sub_box[i][1], sub_box[i][2], sub_box[i][3]] roi_sub.insert(0,i) sub_roi_box.append(roi_sub) # 5. 借助取反激活图片背景区域 Scene_mask = torch.abs(Scene_mask - 1) # 6. 拿到图片背景区域特征图 Scene_mask_feature = img_feature * Scene_mask sub_roi_box = torch.tensor(sub_roi_box).float().to(device) # 7. 利用ROI Align技术，将目标物体区域框在特征图中框出的区域，映射为4*4大小的特征图 obj_feature = roi_align(img_feature, obj_roi_box, output_size=(4,4), sampling_ratio=4) # 8. 利用ROI Align技术，将交互主体区域框在特征图中框出的区域，映射为4*4大小的特征图 sub_feature = roi_align(img_feature, sub_roi_box, output_size=(4,4), sampling_ratio=4) # 9. 返回目标物体区域特征图，交互主体区域特征图，背景区域特征图(未经ROI Align进行映射) return obj_feature, sub_feature, Scene_mask_feature\",\"to_common 是一个跨模态特征投影模块，其核心目标是将来自不同模态（图像和点云）的特征映射到一个统一的公共特征空间，从而消除模态间的分布差异，为后续的跨模态交互提供基础。\",\"关键功能：\",\"非线性变换：通过MLP（多层感知机）对输入特征进行非线性映射。\",\"特征融合准备：将异构特征（图像网格特征 vs 点云无序特征）转换为同构表示，便于计算相似度。\",\"筛选关键信息：通过瓶颈结构（先升维后降维）过滤噪声，保留跨模态共享的显著特征。\",\"class Joint_Region_Alignment(nn.Module): def __init__(self, emb_dim = 512, num_heads = 4): super().__init__() class SwapAxes(nn.Module): def __init__(self): super().__init__() def forward(self, x): return x.transpose(1, 2) self.emb_dim = emb_dim self.div_scale = self.emb_dim ** (-0.5) self.num_heads = num_heads self.to_common = nn.Sequential( nn.Conv1d(self.emb_dim, 2*self.emb_dim, 1, 1), nn.BatchNorm1d(2*self.emb_dim), nn.ReLU(), nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) self.i_atten = Inherent_relation(self.emb_dim, self.num_heads) self.p_atten = Inherent_relation(self.emb_dim, self.num_heads) self.joint_atten = Inherent_relation(self.emb_dim, self.num_heads) def forward(self, F_i, F_p): ''' i_feature: [B, C, H, W] p_feature: [B, C, N_p] HW = N_i ''' B,_,N_p = F_p.size() # (B,512,64) # 1. 物体区域特征图展平: (B,512,4,4) --> (B,512,4*4) F_i = F_i.view(B, self.emb_dim, -1) #[B, C, N_i] # 2. 通过共享MLP迫使图像和点云特征在相同空间分布，消除模态差异 I = self.to_common(F_i) # (B,512,16) P = self.to_common(F_p) # (B,512,64) # 3. 计算相似度矩阵: (B,64,512) * (B,512,16) = (B,64,16) phi = torch.bmm(P.permute(0, 2, 1), I)*self.div_scale #[B, N_p, N_i] phi_p = F.softmax(phi,dim=1) # 计算特征图中每个点和点云每个点特征的相似度 phi_i = F.softmax(phi,dim=-1) # 计算点云中每个点和特征图中每个点特征的相似度 # 4. 特征增强(按照相似度完成信息融合 + 自注意力完成内部信息建模) I_enhance = torch.bmm(P, phi_p) # (B,512,64) * (B,64,16) = （B,512,16） [B, C, N_i] P_enhance = torch.bmm(I, phi_i.permute(0,2,1)) # (B,512,16) * (B,16,64) = （B,512,64） [B, C, N_p] I_ = self.i_atten(I_enhance.mT) #[B, N_i, C] P_ = self.p_atten(P_enhance.mT) #[B, N_p, C] # I_ (B,16,512) , P_ (B,64,512) # 5. 联合建模: 拼接 (B,80,512) + 自注意力 joint_patch = torch.cat((P_, I_), dim=1) F_j = self.joint_atten(joint_patch) #[B, N_p+N_i, C] return F_j\",\"class Affordance_Revealed_Module(nn.Module): def __init__(self, emb_dim, proj_dim): class SwapAxes(nn.Module): def __init__(self): super().__init__() def forward(self, x): return x.transpose(1, 2) super().__init__() self.emb_dim = emb_dim self.proj_dim = proj_dim self.cross_atten = Cross_Attention(emb_dim = self.emb_dim, proj_dim = self.proj_dim) self.fusion = nn.Sequential( nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1), nn.BatchNorm1d(self.emb_dim), nn.ReLU() ) def forward(self, F_j, F_s, F_e): ''' F_j: [B, N_p + N_i, C] (B,80,512) 物体区域特征和点云特征的联合建模 F_s: [B, H, W, C] (B,512,4,4) 交互区域特征 F_e: [B, H, W, C] (B,512,4,4) 背景特征 ''' B,_,C = F_j.size() # 拉平: (B,512,4,4) --> (B,512,4*4) F_s = F_s.view(B, C, -1) #[B, N_i, C] F_e = F_e.view(B, C, -1) #[B, N_i, C] # 利用联合建模特征作为query，从交互区域特征和背景特征中提取相关信息分别单独加到自己身上 Theta_1, Theta_2 = self.cross_atten(F_j, F_s.mT, F_e.mT) #[B, C, N_p + N_i] # 通道维度完成拼接后，利用1x1卷积完成通道维度上的信息融合 joint_context = torch.cat((Theta_1.mT, Theta_2.mT), dim=1) #[B, 2C, N_p + N_i] affordance = self.fusion(joint_context) #[B, C, N_p + N_i] affordance = affordance.permute(0, 2, 1) #[B, N_p + N_i, C] return affordance # （B,80,512)\",\"class Decoder(nn.Module): def __init__(self, additional_channel, emb_dim, N_p, N_raw, num_affordance): class SwapAxes(nn.Module): def __init__(self): super().__init__() def forward(self, x): return x.transpose(1, 2) super().__init__() self.emb_dim = emb_dim self.N_p = N_p self.N = N_raw self.num_affordance = num_affordance #upsample self.fp3 = PointNetFeaturePropagation(in_channel=512+self.emb_dim, mlp=[768, 512]) self.fp2 = PointNetFeaturePropagation(in_channel=832, mlp=[768, 512]) self.fp1 = PointNetFeaturePropagation(in_channel=518+additional_channel, mlp=[512, 512]) self.pool = nn.AdaptiveAvgPool1d(1) self.out_head = nn.Sequential( nn.Linear(self.emb_dim, self.emb_dim // 8), SwapAxes(), nn.BatchNorm1d(self.emb_dim // 8), nn.ReLU(), SwapAxes(), nn.Linear(self.emb_dim // 8, 1), ) self.cls_head = nn.Sequential( nn.Linear(2*self.emb_dim, self.emb_dim // 2), nn.BatchNorm1d(self.emb_dim // 2), nn.ReLU(), nn.Linear(self.emb_dim // 2, self.num_affordance), nn.BatchNorm1d(self.num_affordance) ) self.sigmoid = nn.Sigmoid() def forward(self, F_j, affordance, encoder_p): ''' obj ---> [F_j] affordance ---> [B, N_p + N_i, C] encoder_p ---> [Hierarchy feature] ''' B,_,_ = F_j.size() p_0, p_1, p_2, p_3 = encoder_p P_align, I_align = torch.split(F_j, split_size_or_sections=self.N_p, dim=1) #[B, N_p, C] --- [B, N_i, C] F_pa, F_ia = torch.split(affordance, split_size_or_sections = self.N_p, dim=1) #[B, N_p, C] --- [B, N_i, C] up_sample = self.fp3(p_2[0], p_3[0], p_2[1], P_align.mT) #[B, emb_dim, npoint_sa2] up_sample = self.fp2(p_1[0], p_2[0], p_1[1], up_sample) #[B, emb_dim, npoint_sa1] up_sample = self.fp1(p_0[0], p_1[0], torch.cat([p_0[0], p_0[1]],1), up_sample) #[B, emb_dim, N] F_pa_pool = self.pool(F_pa.mT) #[B, emb_dim, 1] F_ia_pool = self.pool(F_ia.mT) #[B, emb_dim, 1] logits = torch.cat((F_pa_pool, F_ia_pool), dim=1) #[B, 2*emb_dim, 1] logits = self.cls_head(logits.view(B,-1)) _3daffordance = up_sample * F_pa_pool.expand(-1,-1,self.N) #[B, emb_dim, 2048] _3daffordance = self.out_head(_3daffordance.mT) #[B, 2048, 1] _3daffordance = self.sigmoid(_3daffordance) return _3daffordance, logits, [F_ia.mT.contiguous(), I_align.mT.contiguous()]\"]},\"60\":{\"h\":\"LASO 模型代码解读与复现\",\"t\":[\"LASO: Language-guided Affordance Segmentation on 3D Object 论文代码解读与复现\",\"论文: https://openaccess.thecvf.com/content/CVPR2024/papers/Li_LASO_Language-guided_Affordance_Segmentation_on_3D_Object_CVPR_2024_paper.pdf 代码: https://github.com/yl3800/LASO\",\"这篇论文提出了一项新的任务和一个配套的数据集，旨在推动 语言引导下的 3D对象功能区域分割（Language-guided Affordance Segmentation on 3D Object, 简称 LASO）。\"]},\"61\":{\"h\":\"数据集\"},\"62\":{\"h\":\"1. 基础数据来源\",\"t\":[\"数据集基于 3D-AffordanceNet 提供的点云和功能区域标注构建：\",\"每个物体都以点云形式表示；\",\"点云中的每个点被标注为支持一个或多个功能类型（multi-class affordance labels），例如 grasp、open、lift、move 等；\",\"这些功能标注是人工标注的，具有语义意义；\",\"为什么使用 3D-AffordanceNet？\",\"因为它提供了高质量的点云和功能标注，能够很好地支持 LASO 的目标：根据自然语言问题找出与之相关的功能区域。\"]},\"63\":{\"h\":\"2. 构建问题（Question Crafting）\",\"t\":[\"选取物体-功能组合： \",\"从 3D-AffordanceNet 中选取了 58 种物体-功能组合（如 mug-grasp、door-open 等）；\",\"手工设计问题： \",\"对每种组合手工编写 5 个代表性问题；\",\"使用 GPT-4 扩展生成更多问题： \",\"使用 GPT-4 为每个组合额外生成 10 个问题；\",\"总共得到 870 个专家设计的问题（58 × 15 = 870）；\",\"Affordance-Question数据可视化\",\"在扩展过程中，GPT-4 生成的问题遵循以下三个关键原则，以确保问题多样性和语义丰富性：\",\"原则\",\"描述\",\"Contextual Enrichment（上下文丰富化）\",\"添加更多上下文细节，使问题更具体地连接目标对象的功能；例：将 “Grasping scissors: top choice?” 改为 “Identify the key points on the scissors that ensure successful grasping.”\",\"Concise Phrasing（简洁表达）\",\"提炼问题本质，使其简短但仍有意义；\",\"Structural Diversity（结构多样性）\",\"使用不同句式结构（疑问句、陈述句等），防止模型偏向特定句式或长度；\"]},\"64\":{\"h\":\"3. 标注 GT Mask（Ground Truth Mask）\",\"t\":[\"对于每个问题，结合其对应的功能类型和原始点云标注信息，构造出对应的二值掩码 gt_mask：\",\"每个点是否属于当前问题描述的功能区域；\",\"gt_mask 是 (N,) 形状的一维数组，其中 N 是点数；\",\"数值可以是 0/1（binary mask），也可以是软标签（soft label），表示点属于该功能区域的概率；\",\"软标签通常用于边界模糊区域，反映点与功能核心区域的距离远近；\",\"💡 注意：这些功能标签仅用于构造问题和定位正确功能区域，在训练和测试中不作为显式监督信号。\"]},\"65\":{\"h\":\"4. 数据集组织方式\",\"t\":[\"数据总量：\",\"总样本数：19,751 个点云-问题配对；\",\"物体类别数：23 类；\",\"功能类型数：17 类；\",\"问题总数：870 个专家设计的问题；\",\"每个物体类别可有多个形状实例；\",\"一个问题可以作用于多个物体类别（泛化能力）；\",\"数据集设置（两种模式）：\",\"🔹 Seen（见过）\",\"训练和测试阶段共享相似的物体类别和功能类型的分布；\",\"目的是评估模型在熟悉场景下的表现；\",\"🔹 Unseen（未见）\",\"某些功能类型在特定物体类别下会从训练集中省略，但在测试集中保留；\",\"目的是测试模型对新组合的泛化能力；\",\"例如：模型在训练期间学会了抓取包和杯子，但测试时要求“抓取耳机”——这是训练中未曾遇到过的功能-物体组合；\",\"数据划分方式：\",\"分区\",\"物体类别数\",\"问题数\",\"样本数\",\"Train\",\"6883\",\"638\",\"16,120\",\"Val\",\"516\",\"58\",\"1,215\",\"Test\",\"1035\",\"174\",\"2,416\"]},\"66\":{\"h\":\"5. 数据增强与配对策略\",\"t\":[\"训练阶段：\",\"每次迭代中，每个形状实例随机匹配一个与其功能类型一致的问题；\",\"随机配对使模型暴露于各种语义上下文中，提升泛化能力；\",\"推理阶段（验证 & 测试）：\",\"问题配对是固定的；\",\"所有问题专属于评估阶段，不在训练中透露；\",\"确保推理一致性，保持评估完整性；\"]},\"67\":{\"h\":\"6. 数据集统计信息（来自论文图3）\",\"t\":[\"维度\",\"内容\",\"功能类型\",\"17 类，如 grasp、open、lift、move 等\",\"物体类别\",\"23 类，如 mug、microwave、chair、door 等\",\"物体-功能组合\",\"58 种唯一组合（object-affordance pairs）\",\"问题总数\",\"870 个定制化问题\",\"点云-问题配对\",\"19,751 对\",\"点云来源\",\"来自 3D-AffordanceNet，每个点云约 2048 个点\"]},\"68\":{\"h\":\"7. 代码实现\",\"t\":[\"数据集初始化的核心代码实现如下:\",\"class AffordQ(Dataset): def __init__(self, split='train', **kwargs ): # 数据集存放目录 data_root='LASO_dataset' # 数据集类型: 训练集，评估集，测试集 self.split = split # 所支持的23种物体类型和17种功能类型 classes = [\\\"Bag\\\", \\\"Bed\\\", \\\"Bowl\\\",\\\"Clock\\\", \\\"Dishwasher\\\", \\\"Display\\\", \\\"Door\\\", \\\"Earphone\\\", \\\"Faucet\\\", \\\"Hat\\\", \\\"StorageFurniture\\\", \\\"Keyboard\\\", \\\"Knife\\\", \\\"Laptop\\\", \\\"Microwave\\\", \\\"Mug\\\", \\\"Refrigerator\\\", \\\"Chair\\\", \\\"Scissors\\\", \\\"Table\\\", \\\"TrashCan\\\", \\\"Vase\\\", \\\"Bottle\\\"] afford_cl = ['lay','sit','support','grasp','lift','contain','open','wrap_grasp','pour', 'move','display','push','pull','listen','wear','press','cut','stab'] # 建立物体类型和功能类型的索引映射关系，神经网络模型只认识数字 self.cls2idx = {cls.lower():np.array(i).astype(np.int64) for i, cls in enumerate(classes)} self.aff2idx = {cls:np.array(i).astype(np.int64) for i, cls in enumerate(afford_cl)} # 加载标注数据 with open(os.path.join(data_root, f'anno_{split}.pkl'), 'rb') as f: self.anno = pickle.load(f) # 加载点云数据 with open(os.path.join(data_root, f'objects_{split}.pkl'), 'rb') as f: self.objects = pickle.load(f) # 加载58种物体-功能组合的标注数据 (数据组织形式，参考上文的 Affordance-Question数据可视化图) self.question_df = pd.read_csv(os.path.join(data_root, 'Affordance-Question.csv')) # sort anno by object class and affordance type -- 遍历标注数据列表 self.sort_anno ={} for item in sorted(self.anno, key=lambda x: x['class']): # 获取当前样本的物体类别和物体信息值: 点云ID, 功能区域掩码, 功能类别 key = item['class'] value = {'shape_id': item['shape_id'], 'mask': item['mask'], 'affordance': item['affordance']} # 每种物体可以对应多种形状实例和功能类别 if key not in self.sort_anno: # 如果当前物体类别不在排序后的字典中，直接添加 self.sort_anno[key] = [value] else: # 如果当前物体类别在排序后的字典中，将当前样本的物体信息值追加到对应列表中 self.sort_anno[key].append(value)\",\"加载的标注数据中每个样本的组织形式如下:\",\"shape_id ：点云ID\",\"class ：物体类别（如bed）\",\"affordance ：功能类别（如lay）\",\"mask ：功能区域掩码（点级别标注）\",\"标注数据组织形式\",\"点云数据组织形式\",\"每种物体可以对应多种形状实例和功能类别\",\"获取样本的代码实现:\",\" def __getitem__(self, index): # 根据样本索引取出样本数据 data = self.anno[index] # 获取当前样本对应的点云ID shape_id = data['shape_id'] # 获取当前样本对应的物体类别 cls = data['class'] # 获取当前样本对应的功能类型 affordance = data['affordance'] # 获取当前样本对应的功能区域掩码 gt_mask = data['mask'] # 取出当前样本对应的点云数据 ，（2048,3) point_set = self.objects[str(shape_id)] # 对点云数据进行归一化处理，消除尺度差异 point_set,_,_ = pc_normalize(point_set) # 对点云数据进行转置操作 ，（3,2048) point_set = point_set.transpose() # 获取当前样本对应的问题文本(训练: 随机选； 验证&测试: 固定返回问题0) question = self.find_rephrase(self.question_df, cls, affordance) # 获取当前功能类型对应的索引值 affordance = self.aff2idx[affordance] # 返回: 点云数据， 物体类别索引， 功能区域掩码， 问题文本， 功能类型索引 return point_set, self.cls2idx[cls], gt_mask, question, affordance def find_rephrase(self, df, object_name, affordance): # 如果当前是训练模式，则从问题1～15中随机选择一个问题，否则固定返回问题0 qid = str(np.random.randint(1, 15)) if self.split == 'train' else '0' qid = 'Question'+qid # 从 DataFrame df 中筛选出同时满足 物体名称匹配 和 功能属性匹配 的行，并仅保留 qid 指定的列，也就是取出上面随机选择的问题文本 result = df.loc[(df['Object'] == object_name) & (df['Affordance'] == affordance), [qid]] # 问题文本不为空，则返回该问题文本 if not result.empty: # return result.index[0], result.iloc[0]['Rephrase'] return result.iloc[0][qid] else: raise NotImplementedError\"]},\"69\":{\"h\":\"8. 总结\",\"t\":[\"LASO 数据集基于 3D-AffordanceNet 的点云和功能标注，结合人工+GPT-4 生成的多样化问题，构造出 19,751 个点云-问题配对，旨在实现语言引导下的 3D 功能区域分割，推动 3D 视觉与大语言模型（LLM）的深度融合。\"]},\"70\":{\"h\":\"模型实现\",\"t\":[\"论文提出了一个全新的模型：PointRefer，用于解决一个新颖的任务 —— 语言引导的 3D 对象功能区域分割（LASO）。\",\"模型目标： 给定一个 3D 点云对象和一个自然语言问题（例如：“Where would you grasp this mug?”），PointRefer 的目标是预测出与该问题相关的点云区域，即生成一个二值掩码，表示哪些点属于目标功能区域。\",\"PointRefer 包括以下核心模块：\",\"3D 骨干网络（3D Backbone）\",\"使用 PointNet++ 编码点云特征；\",\"多阶段编码-解码结构提取多尺度点特征；\",\"自适应融合模块（Adaptive Fusion Module, AFM）\",\"在不同解码层注入语言信息；\",\"实现语言引导下的跨模态融合；\",\"增强点特征的语义判别能力；\",\"参考点解码器（Referred Point Decoder, RPD）\",\"引入一组可学习的“问题条件化查询”（affordance queries）；\",\"利用 Transformer 解码器将这些查询与点云特征进行交互；\",\"生成动态卷积核（dynamic kernels）；\",\"最终通过卷积操作生成分割掩码；\",\"PointRefer模型结构图\",\"PointRefer 前向传播过程如下:\",\"class PointRefer(nn.Module): # 传入question文本 和 point点云数据 def forward(self, text, xyz): ''' text: [B, L, 768] xyz: [B, 3, 2048] -- (b,c,n) ''' B, C, N = xyz.size() # 1. Encoding 过程 # 1.1 Language Encoding 使用RoBert编码文本 t_feat, t_mask = self.forward_text(list(text), xyz.device) # [batch, q_len, d_model] # 1.2 BackBone Encoding 使用PointNet++编码点云 F_p_wise = self.point_encoder(xyz) \\\"\\\"\\\" Decoding \\\"\\\"\\\" # 1.3 PointNet++ 逐级做点集抽象得到的每层的点集坐标和点集特征集合 p_0, p_1, p_2, p_3 = F_p_wise # 2.Backbone Decoding过程 # 2.1 点集集合中每个点的特征和文本特征信息进行融合,传入的点集特征集合经过转置处理后的维度为: (b, n, c) p_3[1] = self.gpb(t_feat, p_3[1].transpose(-2, -1)).transpose(-2, -1) # 2.2 PointNet++ 特征传播阶段: 上采样过程中，上一层点集中的点特征重建过程中，充分吸收了高级区域抽象特征和文本特征 up_sample = self.fp3(p_2[0], p_3[0], p_2[1], p_3[1]) #[B, emb_dim, npoint_sa2] up_sample = self.gpb(t_feat, up_sample.transpose(-2, -1)).transpose(-2, -1) up_sample = self.fp2(p_1[0], p_2[0], p_1[1], up_sample) #[B, emb_dim, npoint_sa1] up_sample = self.gpb(t_feat, up_sample.transpose(-2, -1)).transpose(-2, -1) # 2.3 特征传播阶段结束: 一步步重建回原始点数量 128->256->512->1024->2048 up_sample = self.fp1(p_0[0], p_1[0], torch.cat([p_0[0], p_0[1]],1), up_sample) #[B, emb_dim, N] # 3. Referred Point Decoding过程 t_feat = self.decoder(t_feat, up_sample.transpose(-2, -1), tgt_key_padding_mask=t_mask, query_pos=self.pos1d) # b,l,c t_feat *= t_mask.unsqueeze(-1).float() _3daffordance = torch.einsum('blc,bcn->bln', t_feat, up_sample) _3daffordance = _3daffordance.sum(1)/(t_mask.float().sum(1).unsqueeze(-1)) _3daffordance = torch.sigmoid(_3daffordance) return _3daffordance.squeeze(-1)\",\"论文中所给的模型架构图中的Encoder layer指的是PointNet++中提供的PointNetSetAbstractionMsg多尺度分组点集特征抽取类\",\"论文中所给的模型架构图中的Decoder layer指的是PointNet++中提供的PointNetFeaturePropagation特征传播类\"]},\"71\":{\"h\":\"AFM 自适应融合模块\",\"t\":[\"在 LASO 任务中，模型需要根据自然语言问题（如 “Where to grasp?”）识别点云中的功能区域。由于目标功能区域的尺度、形状多样，传统方法难以适应不同情况。为此，作者设计了 AFM 模块，以增强 PointNet++ 解码过程中点特征的语言引导能力。\",\"AFM 的目标是：在不同解码阶段注入语言线索（text clues），将文本语义信息与点云特征进行跨模态融合，逐步以自上而下的方式细化点特征图，从而提升模型对多尺度、多形状的功能区域的感知能力。\",\"AFM 遵循一个 瓶颈式架构（bottleneck architecture），包含三个关键步骤：\",\"Grouping（分组）\",\"Mixing（混合）\",\"Ungrouping（解组）\",\"这三个步骤构成了一个完整的跨模态融合流程。\"]},\"72\":{\"h\":\"1️⃣ Grouping：文本引导的点特征分组\",\"t\":[\"输入：\",\"X ∈ R^{L×d}：问题编码后的文本特征（由 RoBERTa 编码得到）\",\"P ∈ R^{T×d}：某一层解码器输出的点特征，其中 T 表示该层点数\",\"处理过程：\",\"使用一个轻量级的交叉注意力模块，将文本特征作为查询（query），点特征作为键（key）和值（value），输出分组标记 G：\",\"其中：\",\" 是一个线性变换；\",\"注意力机制使得每个文本 token 对应一组相关的点特征；\",\"分组操作实现了“语言引导的点特征筛选”。\",\"重点是如何理解这里的分组: 每个文本Token询问所有点Key后，知道了哪些点跟自身的相关度更大，因此加权融合的时候，侧重于给这些点的特征分配更大的融合权重。\",\"这部分代码实现如下:\",\"# group_layer 的实现 class LightGroupAttnBlock(nn.Module): # query 是RoBerta编码后的文本特征 , (b,l,c) # key和value都是点云特征 , (b,n,c) def forward(self, query, key, value, q_mask=None): def _inner_forward(query, key, value): q = self.norm_query(query) k = q if self.key_is_query else self.norm_key(key) v = k if self.value_is_key else self.norm_value(value) # 让每个语言 token 去关注点云中最相关的区域，并在此基础上强化自身的语义表达。 # 加上原始 X 是一种残差连接（Residual Connection），可以确保语言语义不会丢失。 x = self.attn(q, k, v, q_mask) + self.drop(q) return x return _inner_forward(query, key, value)\"]},\"73\":{\"h\":\"2️⃣ Mixing：MLP-Mixer 进行组内和通道间的信息混合\",\"t\":[\"MLP-Mixer 是一种 基于 MLP（多层感知机）的视觉模型架构 ，由 Google Research 在 2021 年提出。它不使用任何注意力机制，而是通过 空间混合（mixing）和通道混合（mixing）操作 来实现全局信息建模。\",\"MLP-Mixer: An all-MLP Architecture for Vision\",\"MLP-Mixer 的核心思想是：用 MLP 替代 Transformer 中的自注意力机制 ，从而减少计算复杂度并保持性能。\",\"Token-mixing MLP\",\"对所有点/patch 的相同通道进行混合；\",\"相当于跨空间位置的信息交换；\",\"类似于 CNN 中的空间卷积；\",\"Channel-mixing MLP\",\"对每个 token 的所有通道进行处理；\",\"提取更高级的特征表示；\",\"类似于传统的全连接层或 1x1 卷积；\",\"这两个操作交替进行，形成一个类似于 Transformer 的堆叠结构，但完全不使用注意力机制。\",\"输入：\",\"G ∈ R^{L×d}：分组后的文本引导特征\",\"处理过程：\",\"使用 MLP-Mixer 来更新分组特征，生成融合特征 F：\",\"其中：\",\"MLP₁ 负责组内信息混合（token 内部）；\",\"MLP₂ 负责通道间信息混合（feature channel）；\",\"两个 MLP 交替作用，实现跨模态信息的充分交互；\",\"最终输出融合特征 F；\",\"这部分代码实现如下:\",\"# mixer 的实现 class MLPMixerLayer(nn.Module): def __init__(self, num_patches, embed_dims, patch_expansion, channel_expansion, drop_out, **kwargs): super().__init__() patch_mix_dims = int(patch_expansion * embed_dims) # 16 channel_mix_dims = int(channel_expansion * embed_dims) # 128 self.patch_mixer = nn.Sequential( nn.Linear(num_patches, patch_mix_dims, bias=False), # try here nn.GELU(), nn.Dropout(drop_out), nn.Linear(patch_mix_dims, num_patches, bias=False), nn.Dropout(drop_out) ) self.channel_mixer = nn.Sequential( nn.Linear(embed_dims, channel_mix_dims), nn.GELU(), nn.Dropout(drop_out), nn.Linear(channel_mix_dims, embed_dims), nn.Dropout(drop_out) ) self.norm1 = nn.LayerNorm(embed_dims) self.norm2 = nn.LayerNorm(embed_dims) # x 分组后的文本引导特征 : (b,l,c) def forward(self, x): # x 转置后: (b,c,l) , patch_mixer 负责组内信息混合（token 内部） x = x + self.patch_mixer(self.norm1(x).transpose(1,2)).transpose(1,2) # channel_mixer 负责通道间信息混合（feature channel) x = x + self.channel_mixer(self.norm2(x)) return x\"]},\"74\":{\"h\":\"3️⃣ Ungrouping：将融合特征映射回点空间\",\"t\":[\"输入：\",\"原始点特征 P；\",\"融合后的文本特征 F；\",\"处理过程：\",\"使用另一个注意力模块，将融合特征重新分配给每个点：\",\"其中：\",\"W₂ 是线性变换；\",\"注意力机制让每个点从融合特征中提取相关信息；\",\"输出 P_m 是语言增强后的点特征；\",\"最后加上残差连接形成最终输出 P_o：\",\"这个 P_o 就是经过 AFM 增强的点特征图，用于后续分割掩码预测。\",\"class FullAttnCatBlock(nn.Module): # query 为点云: (b,n,c) , key和value为融合后的文本特征: (b,l,c) def forward(self, query, key, value, key_padding_mask=None): def _inner_forward(query, key, value, key_padding_mask): q = self.norm_query(query) k = q if self.key_is_query else self.norm_key(key) v = k if self.value_is_key else self.norm_value(value) # 使用另一个注意力模块，将融合特征重新分配给每个点 x = self.attn(q, k, v, key_padding_mask) + self.drop(query) # MLP映射 + Residual Connection x = self.ffn(self.norm2(x)) + x return x return _inner_forward(query, key, value, key_padding_mask)\"]},\"75\":{\"h\":\"4️⃣ AFM 自适应融合模块\",\"t\":[\"有了以上 Grouping - Mixing - Ungrouping 三个关键步骤的实现，下面只需要把以上的三个步骤按流程组织起来即可得到AFM模块的完整实现了:\",\"class GPBlock(nn.Module): # q: 文本特征 (b, l, c) ， x: 点集特征集合 (b, n, c) def forward(self, q, x, q_mask=None): # Grouping阶段 gt = self.group_layer(query=q, key=x, value=x) if q_mask is not None: gt *= q_mask.unsqueeze(-1) # Mixing阶段 gt = self.mixer(gt) + self.drop(gt) # Ungrouping阶段 ungroup_tokens = self.un_group_layer(query=x, key=gt, value=gt, key_padding_mask=q_mask) return ungroup_tokens\",\"AFM 的网络结构可视化理解\",\"文本特征 X ──┐ ↓ Grouping (Cross-Attention) ↓ Mixing (MLP-Mixer) ↓ Ungrouping (Attention) ↓ 输出增强后的点特征 P_o\",\"Grouping：用语言引导点特征分组；\",\"Mixing：在分组内进行信息交换；\",\"Ungrouping：再将融合信息返回点空间；\",\"这种设计使得语言信息能有效地指导点特征的学习过程，论文中也进行了大量消融实验来验证 AFM 的有效性：\",\"模型变体\",\"mIoU\",\"AUC\",\"SIM\",\"MAE\",\"基线（不加 AFM）\",\"17.7\",\"82.1\",\"0.558\",\"0.110\",\"加入 AFM 后\",\"20.8\",\"87.3\",\"0.629\",\"0.093\",\"结果表明：加入 AFM 显著提升了所有指标，说明其确实有效增强了语言-视觉的跨模态交互能力。\"]},\"76\":{\"h\":\"RPO 参考点解码器\",\"t\":[\"Referred Point Decoder（RPD）是 LASO 任务中用于生成功能区域掩码的核心模块。\",\"它的主要目标是：\",\"利用一组问题条件化的 affordance queries 通过 Transformer 解码器与点云特征交互 ，生成一组动态卷积核（dynamic kernels），最终通过这些 kernel 对 AFM 增强后的点特征进行卷积，得到分割掩码。\",\"class TransformerDecoderLayer(nn.Module): # tgt: text feature (b,l,c), memory: up_sample (b,n,c) def forward(self, tgt, memory, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None, pos: Optional[Tensor] = None, query_pos: Optional[Tensor] = None): # 1. Affordance Query = 问题嵌入（Question Embedding）X + 可学习的位置编码（Learnable Position Embeddings） # 这里tgt就是Roberta编码得到的文本特征嵌入向量 # 使用 X 作为初始输入，确保每个 query 都带有原始语言上下文； # 如果只用 learnable embeddings，模型将完全依赖随机初始化的参数去“猜”语言含义，效率极低； q = k = self.with_pos_embed(tgt, query_pos) # 2. 自注意力机制: 让每个 query 不仅理解自己的语义，还能感知其他 query 的信息，从而形成更完整的语言上下文理解。 tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask) tgt = tgt + self.dropout1(tgt2) tgt = self.norm1(tgt) # (b,l,c) # 3. 跨模态注意力机制: 每个 affordance query 都会基于其语言语义，从点云中找出最相关的功能区域，从而为后续的动态卷积和掩码预测提供基础。 tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos), key=self.with_pos_embed(memory, pos), value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask, output_attentions = True) tgt = tgt + self.dropout2(tgt2) tgt = self.norm2(tgt) # (b,l,c) # 4. MLP: 每个query通道维度做特征融合 tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt)))) tgt = tgt + self.dropout3(tgt2) tgt = self.norm3(tgt) # (b,l,c) return tgt class PointRefer(nn.Module): def forward(self, text, xyz): ... # 3. Referred Point Decoding过程 # 3.1 利用一组问题条件化的 affordance queries 通过 Transformer 解码器与点云特征交互 ，生成一组动态卷积核（dynamic kernels）(b,l,c) t_feat = self.decoder(t_feat, up_sample.transpose(-2, -1), tgt_key_padding_mask=t_mask, query_pos=self.pos1d) # 3.2 对无效 token（padding）做掩码操作，防止其影响后续计算。 (b,l,c) t_feat *= t_mask.unsqueeze(-1).float() # 3.3 执行 动态卷积（Dynamic Convolution） 操作，用增强后的语言查询去“扫描”点云特征图 （b,l,n) _3daffordance = torch.einsum('blc,bcn->bln', t_feat, up_sample) # 3.4 对affordance query的响应图进行平均池化，融合所有 affordance query 的得分结果。 (b,n) _3daffordance = _3daffordance.sum(1)/(t_mask.float().sum(1).unsqueeze(-1)) # 3.5 将响应值映射到 [0, 1] 区间，表示每个点属于目标功能区域的概率。 (b,n) _3daffordance = torch.sigmoid(_3daffordance) return _3daffordance # (b,n)\",\"PyTorch 的 einsum 函数，它是一个非常强大且灵活的张量操作函数，支持通过爱因斯坦求和约定（Einstein Summation Convention） 来表达各种线性代数运算。\",\"下面详细介绍一下动态卷机核卷积的过程:\",\"t_feat: 语言查询特征 , 形状：(B, L, C) , 这是 经过 Referred Point Decoder (RPD) 处理后的 affordance queries，表示每个 token 对应的“动态卷积核”。\",\"up_sample: 上采样后的点云特征 , 形状：(B, C, N)。\",\"而下面这行代码实现的是一个 动态卷积（Dynamic Convolution） 操作：\",\"_3daffordance = torch.einsum('blc,bcn->bln', t_feat, up_sample)\",\"它的本质是： 用一组由语言引导的动态卷积核 t_feat 去卷积点云特征 up_sample，得到每个 token 对每个点的关注响应。\",\"详细解释 einsum 表达式:\",\"torch.einsum('blc,bcn->bln', t_feat, up_sample)\",\"维度\",\"含义\",\"b\",\"batch 维度，保持不变\",\"l\",\"token 维度，保留下来\",\"c\",\"特征通道维度，进行内积操作（求和）\",\"n\",\"点云维度，保留下来\",\"所以这个表达式的含义是：\",\"也就是说，对于每一个 batch 中的数据：\",\"每个 token（l）都与所有点（n）交互；\",\"每个 token 实际上是一个动态生成的卷积核（C × 1 × 1），作用于点云特征图（C × N）；\",\"最终输出形状为 (B, L, N)，表示： \",\"每个 token 对每个点的关注程度；\",\"输出张量\",\"形状\",\"含义\",\"_3daffordance\",\"(B, L, N)\",\"每个 token 对每个点的响应值（得分）\",\"然后在后续会进行如下处理：\",\"_3daffordance = _3daffordance.sum(1) / (t_mask.float().sum(1).unsqueeze(-1)) _3daffordance = torch.sigmoid(_3daffordance)\",\"即：\",\"在 token 维度求和（或平均池化），融合多个 token 的关注信息；\",\"使用 sigmoid 得到最终的掩码，形状 (B, N)；\",\"每个点的值 ∈ [0, 1]，表示其属于目标功能区域的概率；\"]},\"77\":{\"h\":\"损失函数\"},\"78\":{\"h\":\"HM_Loss（Hybrid Mask Loss）\",\"t\":[\"在 LASO 数据集中，模型需要根据自然语言问题识别点云中最相关的功能区域（如 grasping area, opening area 等），而 HM_Loss 是 PointRefer 模型的监督信号，它结合了：\",\"Focal Loss ：用于缓解类别不平衡问题；\",\"Dice Loss ：用于衡量预测掩码与真实标签之间的空间重合度；\",\"最终 loss = CELoss + DiceLoss，让模型同时关注逐点分类精度和整体区域匹配。\",\"import torch import torch.nn as nn import torch.nn.functional as F class HM_Loss(nn.Module): def __init__(self): \\\"\\\"\\\" Hybrid Mask Loss 实现： - BCE-Focal Loss（加权交叉熵） - Dice Loss（衡量预测掩码与 GT 的重合度） 公式来自论文 Section 4.2，用于语言引导下的功能区域分割。 \\\"\\\"\\\" super(HM_Loss, self).__init__() # 设置 Focal Loss 参数 self.gamma = 2 # 聚焦参数，放大难分类样本影响 self.alpha = 0.25 # 平衡因子，强调正类（前景点）loss def forward(self, pred, target): \\\"\\\"\\\" 输入： pred: 模型输出的原始 logit 或经过 sigmoid 的概率值； 形状为 [B, N] target: ground truth 掩码（soft mask），形状也为 [B, N] 返回： total_loss: CELoss + DiceLoss 的加权和 \\\"\\\"\\\" # Step 1: 构建 Focal Loss 权重项 # temp1：负类 loss（背景点） # temp2：正类 loss（目标功能区域） # 1e-6 的加入是为了让 log 计算保持稳定，尤其是在预测值接近极端值（0 或 1）时 temp1 = -(1 - self.alpha) * torch.mul( pred ** self.gamma, torch.mul(1 - target, torch.log(1 - pred + 1e-6)) ) temp2 = -self.alpha * torch.mul( (1 - pred) ** self.gamma, torch.mul(target, torch.log(pred + 1e-6)) ) # 将两个方向的 loss 合并，并取 batch 和点维度的平均 temp = temp1 + temp2 CELoss = torch.sum(torch.mean(temp, dim=(0, 1))) # Step 2: 计算正类 Dice Loss（预测与 Ground Truth 的交集 / 并集） intersection_positive = torch.sum(pred * target, dim=1) cardinality_positive = torch.sum(torch.abs(pred) + torch.abs(target), dim=1) dice_positive = (intersection_positive + 1e-6) / (cardinality_positive + 1e-6) # Step 3: 计算负类 Dice Loss（非目标区域匹配度） intersection_negative = torch.sum((1 - pred) * (1 - target), dim=1) cardinality_negative = torch.sum(2 - torch.abs(pred) - torch.abs(target), dim=1) dice_negative = (intersection_negative + 1e-6) / (cardinality_negative + 1e-6) # Step 4: 构建 Dice Loss，形式为 1 - Dice Score # 使用了一个偏置项 1.5（可能是经验设定） temp3 = torch.mean(1.5 - dice_positive - dice_negative, dim=0) DICELoss = torch.sum(temp3) # Step 5: 总损失 = 分类误差 + 区域匹配误差 return CELoss + 1.0 * DICELoss\",\"在论文 Section 4.2 中提到：\",\"“We solely employ Dice loss and Binary Cross-Entropy (BCE) loss to guide the segmentation mask prediction.”\",\"虽然这里用的是 Focal Loss + Dice Loss 的组合形式，但它本质上是 BCE + Dice 的改进版，具有以下优势：\",\"Focal Loss: 抑制 easy examples，放大 hard examples，防止忽略小区域\",\"Dice Loss: 关注整体掩码匹配度，提升边界识别能力\",\"两者结合可以：\",\"缓解类别极度不平衡问题；\",\"提高模型对语言指令下功能区域的理解能力；\",\"更好地应对 LASO 中的语言引导 + soft mask 场景；\"]},\"79\":{\"h\":\"训练\",\"t\":[\"模型的训练过程大体分为了 准备，训练，评估 三个流程；\"]},\"80\":{\"h\":\"准备\",\"t\":[\"准备阶段主要完成数据集加载，模型初始化，损失函数定义，优化器设置，学习率调度器初始化等；\",\"def main(opt, dict): # 1. 加载训练集，验证集，测试集 train_dataset = AffordQ('train') train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8 ,shuffle=True, drop_last=True) val_dataset = AffordQ('val') val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=8, shuffle=False) test_dataset = AffordQ('test') test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=8, shuffle=False) # 2. 初始化模型 model = get_PointRefer(emb_dim=dict['emb_dim'], proj_dim=dict['proj_dim'], num_heads=dict['num_heads'], N_raw=dict['N_raw'], num_affordance = dict['num_affordance'], n_groups=opt.n_groups) # 3. 初始化损失函数，优化器，学习率调度器 criterion_hm = HM_Loss() criterion_ce = nn.CrossEntropyLoss() param_dicts = [ {\\\"params\\\": [p for n, p in model.named_parameters() if \\\"text_encoder\\\" not in n and p.requires_grad]}, {\\\"params\\\": [p for n, p in model.named_parameters() if \\\"text_encoder\\\" in n and p.requires_grad], \\\"lr\\\": opt.tlr}] optimizer = torch.optim.Adam(params = param_dicts, lr=dict['lr'], betas=(0.9, 0.999), eps=1e-8, weight_decay=opt.decay_rate) # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=dict['Epoch'], eta_min=1e-6) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\"]},\"81\":{\"h\":\"训练\",\"t\":[\"训练阶段则是模型的核心迭代过程，包括前向传播，损失计算，反向传播，参数更新等:\",\" ''' Training ''' for epoch in range(start_epoch+1, dict['Epoch']): num_batches = len(train_loader) loss_sum = 0 total_point = 0 model = model.train() for i,(point, cls, gt_mask, question, aff_label) in enumerate(train_loader): optimizer.zero_grad() # 4. 前向传播过程 _3d = model(question, point) # 5. 计算损失 loss_hm = criterion_hm(_3d, gt_mask) # loss_ce = criterion_ce(logits, cls) # 6. 反向传播 temp_loss = loss_hm # + opt.loss_cls*loss_ce temp_loss.backward() optimizer.step() results = torch.zeros((len(val_dataset), 2048, 1)) targets = torch.zeros((len(val_dataset), 2048, 1))\"]},\"82\":{\"h\":\"评估\",\"t\":[\"评估阶段则是在验证集或测试集上评估模型的性能，计算指标包括 MAE，SIM，AUC，mIoU。\",\"在 LASO（Language-guided Affordance Segmentation on 3D Object）任务 中，作者使用了四个核心评估指标来衡量模型对语言引导下功能区域的识别能力：\",\"指标\",\"名称\",\"英文全称\",\"MAE\",\"平均绝对误差\",\"Mean Absolute Error\",\"SIM\",\"相似性得分\",\"Similarity Score\",\"AUC\",\"曲线下面积\",\"Area Under the Curve\",\"mIoU\",\"平均交并比\",\"mean Intersection over Union\",\"MAE（Mean Absolute Error）是预测值与真实值之间的平均绝对误差，用于衡量模型输出的 soft mask 与 ground truth 掩码之间的逐点偏差。\",\"其中：\",\"：点云中点的数量；\",\"：模型预测该点属于功能区域的概率；\",\"：ground truth 标签（可以是 soft mask 或 binary mask）；\",\"特点与作用：\",\"特性\",\"描述\",\"✔️ 支持 soft mask 输入\",\"不依赖 thresholding，适用于连续响应值\",\"✔️ 衡量整体分布一致性\",\"反映模型是否准确学习语言引导下的响应强度\",\"⚠️ 对边界模糊区域不敏感\",\"IoU 等指标更关注重合度\",\"SIM（Similarity）是一种基于直方图交集的相似性指标，用于衡量两个概率分布之间的匹配程度。它常用于图像检索、图像分割等任务。\",\"即：对每个点取预测值和真实值中的较小者，然后求和。也可以归一化为：\",\"特点与作用：\",\"特性\",\"描述\",\"✔️ 不需要 thresholding\",\"支持 soft mask 输入\",\"✔️ 强调分布匹配\",\"不仅看交集，还看响应强度分布\",\"✔️ 对边界模糊区域友好\",\"不像 IoU 那样依赖 hard threshold\",\"⚠️ 不直接优化最终目标\",\"不能作为 loss 使用，更适合评估\",\"AUC（Area Under ROC Curve）是 Receiver Operating Characteristic (ROC) 曲线下的面积，衡量模型对二分类问题的判别能力。\",\"AUC 的计算流程如下：\",\"对不同阈值计算 TPR 和 FPR；\",\"绘制 ROC 曲线；\",\"计算曲线下面积（AUC）；\",\"特点与作用：\",\"特性\",\"描述\",\"✔️ 不依赖特定阈值\",\"考察所有可能的 threshold 下的表现\",\"✔️ 关注排序能力\",\"判断模型是否能正确区分前景和背景\",\"✔️ 适用于 binary 分类\",\"需要先将 soft mask 转换为 binary\",\"⚠️ 对 small region 敏感度有限\",\"需结合 mIoU 使用\",\"mIoU（mean Intersection over Union）是图像/点云分割中最常用的指标之一，衡量预测区域与真实标签之间的空间重合度。\",\"公式如下：\",\"其中：\",\"：预测的 binary mask；\",\"：真实的 binary mask；\",\"通常我们会使用多个 threshold（如 np.linspace(0, 1, 20)），然后取平均得到 aiou（average IoU）。\",\"特点与作用：\",\"特性\",\"描述\",\"✔️ 直接评价分割精度\",\"最贴近实际应用需求\",\"✔️ 对边界敏感\",\"能反映边缘响应质量\",\"✔️ 易受 threshold 影响\",\"多阈值评估更稳定\",\"⚠️ 不支持 soft mask 直接输入\",\"需先 threshold 成 binary mask\",\"四个指标对比总结:\",\"指标\",\"是否支持 soft mask\",\"是否依赖 threshold\",\"是否关注分布相似性\",\"是否关注空间重合度\",\"输出范围\",\"MAE\",\"✅ 是\",\"❌ 否\",\"✅ 是\",\"❌ 否\",\"[0, ∞)\",\"SIM\",\"✅ 是\",\"❌ 否\",\"✅ 是\",\"❌ 否\",\"[0, 1]\",\"AUC\",\"✅ 是（排序）\",\"✅ 是（binary）\",\"❌ 否\",\"❌ 否\",\"[0, 1]\",\"mIoU\",\"❌ 否（需先 threshold）\",\"✅ 是\",\"❌ 否\",\"✅ 是\",\"[0, 1]\",\"结合论文理解这些指标的意义，来自论文 Table 3 的结果：\",\"方法\",\"mIoU\",\"AUC\",\"SIM\",\"MAE\",\"PointRefer（完整方法）\",\"20.8%\",\"87.3%\",\"0.629\",\"0.093\",\"这些指标共同构成了 LASO 任务的评估体系，分别从以下角度衡量模型表现：\",\"角度\",\"对应指标\",\"1. 分布一致性\",\"SIM\",\"2. 分类判别能力\",\"AUC\",\"3. 逐点误差\",\"MAE\",\"4. 区域重合度\",\"mIoU\",\"这意味着：\",\"PointRefer 不仅理解语言指令；\",\"还能生成与 GT 掩码高度匹配的功能区域；\",\"并且在 unseen object 上也具有泛化能力；\",\"在 LASO 这种类别不平衡、soft mask、语言引导的 3D 功能区域识别任务中，四个指标协同工作：\",\"MAE 衡量逐点误差；\",\"SIM 衡量分布相似性；\",\"AUC 衡量分类器排序能力；\",\"mIoU 衡量空间重合度；\",\"它们共同帮助我们判断模型是否真正理解语言引导下的功能区域语义。\",\"验证集上进行评估的核心代码实现如下:\",\" num = 0 for i, (point, _, label, question, aff_label) in enumerate(val_loader): # 1. 前向传播，得到预测的 soft mask `_3d` ∈ [B, N] _3d = model(question, point) # 2. 计算 MAE（Mean Absolute Error），衡量逐点误差 mae, point_nums = evaluating(_3d, label) total_point += point_nums total_MAE += mae.item() pred_num = _3d.shape[0] # 当前 batch 的样本数 # 3. 收集所有样本的预测结果，便于后续统一评估 results[num : num + pred_num, :, :] = _3d.unsqueeze(-1) # shape: [B, N, 1] targets[num : num + pred_num, :, :] = label.unsqueeze(-1) # shape: [B, N, 1] num += pred_num # 更新索引 # 4. 计算平均 MAE（Mean Absolute Error） mean_mae = total_MAE / total_point # 5. 计算 SIM（Similarity Metric）—— 直方图交集，衡量分布相似性 SIM_matrix = np.zeros(targets.shape[0]) for i in range(targets.shape[0]): SIM_matrix[i] = SIM(results[i], targets[i]) # SIM 函数定义见 utils.eval sim = np.mean(SIM_matrix) # 6. 初始化 AUC 和 IOU 存储数组 AUC = np.zeros((targets.shape[0], targets.shape[2])) # shape: [num_samples, 1] IOU = np.zeros((targets.shape[0], targets.shape[2])) IOU_thres = np.linspace(0, 1, 20) # 多阈值下的 IoU 计算 # 7. 将 GT 标签二值化（soft mask → binary mask） targets_binary = (targets >= 0.5).astype(int) for i in range(AUC.shape[0]): t_true = targets_binary[i].flatten() # 真实标签 p_score = results[i].flatten() # 模型输出的概率值 if np.sum(t_true) == 0: # 8. 如果当前样本没有正类（即无功能区域），标记为 nan AUC[i] = np.nan IOU[i] = np.nan else: # 9. 计算 AUC（Area Under the Curve），衡量分类器整体判别能力 auc = roc_auc_score(t_true, p_score) AUC[i] = auc # 10. 使用多个阈值计算 mIoU（mean Intersection over Union） temp_iou = [] for thre in IOU_thres: p_mask = (p_score >= thre).astype(int) # 用不同 threshold 生成 binary mask intersect = np.sum(p_mask & t_true) # 交集 union = np.sum(p_mask | t_true) # 并集 temp_iou.append(intersect / union) # IoU = intersect / union temp_iou = np.array(temp_iou) aiou = np.mean(temp_iou) # 对所有 threshold 下的 IoU 取均值 IOU[i] = aiou # 10. 最终取所有样本的 AUC 和 mIoU 均值作为最终评估指标 AUC = np.nanmean(AUC) IOU = np.nanmean(IOU) # 11. 打印当前性能指标 logger.debug(f'AUC:{AUC} | IOU:{IOU} | SIM:{sim} | MAE:{mean_mae}') current_IOU = IOU # 12. 如果当前 mIoU 超过历史最佳，则保存 best model if current_IOU > best_IOU: best_IOU = current_IOU best_model_path = save_path + '/best_model-{}.pt'.format(sign) checkpoint = { 'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'Epoch': epoch } torch.save(checkpoint, best_model_path)\",\"测试集最终评估:\",\"category_metrics, affordance_metrics, overall_metrics = evaluate(model, test_loader, device, 3) print_metrics_in_table(category_metrics, affordance_metrics, overall_metrics, logger)\"]},\"83\":{\"h\":\"复现\",\"t\":[\"设置后台运行，同时将运行时输出写入日志:\",\"nohup python -u train.py > train.log 2>&1 &\",\"在 Python 命令中， -u 是 unbuffered 的缩写。Python 存在缓存机制， sys.stdout （标准输出）是有缓存的，只有遇到换行符或者输出内容积累到一定大小时，才会将内容显示到屏幕上；而 sys.stderr （标准错误）是无缓存的，程序向 sys.stderr 输出一个字符，就会立即在屏幕上显示一个字符 1 2 3 。\",\"当在 Python 命令后加上 -u 参数（例如 python -u xx.py ），会强制标准输出也像标准错误一样，不通过缓存直接将内容打印到屏幕上 1 2 3 。这在使用 nohup 后台运行 Python 脚本时非常有用，可以避免因为输出缓存导致日志卡在某一行不输出的问题。\",\"持续追踪日志最新输出:\",\"tail -f train.log\",\"杀死训练进程:\",\"pkill -f \\\"python train.py\\\"\",\"用训练好的模型权重，进行推理 (以下代码是我自己写的一个测试代码):\",\"import os import pickle import torch import numpy as np import open3d as o3d from utils.util import read_yaml from model.PointRefer import get_PointRefer def pc_normalize(pc): \\\"\\\"\\\" 点云数据归一化处理 Args: pc: 输入点云数据，形状为 [N, 3] 的 numpy 数组 Returns: 归一化后的点云数据 \\\"\\\"\\\" centroid = np.mean(pc, axis=0) # 计算点云质心 pc = pc - centroid # 中心化 m = np.max(np.sqrt(np.sum(pc**2, axis=1))) # 计算最大半径 pc = pc / m # 归一化到单位球内 return pc def predict_affordance_mask(points, text, model_path): \\\"\\\"\\\" 预测点云的功能区域掩码 Args: points: 输入点云数据 [N, 3] text: 描述功能的文本提示 model_path: 预训练模型路径 Returns: 功能区域预测结果 [N] \\\"\\\"\\\" # 加载模型配置 dict = read_yaml(\\\"config/default.yaml\\\") dict['bs'] = 16 # 设置batch size dict['lr'] = 1e-4 # 设置学习率 dict['Epoch'] = 50 # 设置训练轮数 # 初始化PointRefer模型 model = get_PointRefer( emb_dim=dict['emb_dim'], proj_dim=dict['proj_dim'], num_heads=dict['num_heads'], N_raw=dict['N_raw'], num_affordance=dict['num_affordance'], n_groups=40 ) # 加载预训练权重 checkpoint = torch.load(model_path, map_location='cpu') model.load_state_dict(checkpoint['model']) model.eval() # 设置为评估模式 # 点云预处理 Point = pc_normalize(points) # 归一化 Point = Point.transpose() # 转置为 [3, N] Points = torch.from_numpy(Point).unsqueeze(0).float() # 增加batch维度 [1, 3, N] text_list = [text] # 文本输入格式处理 # 模型推理 pred = model(text_list, Points) pred = torch.squeeze(pred) # 去除batch维度 [N] affordance_pred = pred.cpu().detach().numpy() # 转为numpy数组 return affordance_pred def visualize_affordance(points_coordinates, affordance_pred): \\\"\\\"\\\" 可视化功能区域预测结果（渐变颜色） Args: points_coordinates: 点云坐标 [N, 3] affordance_pred: 预测结果 [N] \\\"\\\"\\\" pred_point = o3d.geometry.PointCloud() pred_point.points = o3d.utility.Vector3dVector(points_coordinates) # 颜色映射：根据预测值从灰色(背景)到红色(功能区域)渐变 color = np.zeros((2048, 3)) reference_color = np.array([255, 0, 0]) # 红色 back_color = np.array([190, 190, 190]) # 灰色 for i, aff_pred in enumerate(affordance_pred): scale_i = aff_pred color[i] = (reference_color - back_color) * scale_i + back_color pred_point.colors = o3d.utility.Vector3dVector(color.astype(np.float64) / 255.0) o3d.visualization.draw_geometries([pred_point], window_name='Predicted Affordance', width=600, height=600) def visualize_point_cloud(points, pred_mask=None): \\\"\\\"\\\" 基础点云可视化（二值化显示） Args: points: 点云数据 [N, 3] 或 [N, 4] pred_mask: 可选，预测掩码 [N] \\\"\\\"\\\" pcd = o3d.geometry.PointCloud() # 数据预处理 if points.shape[1] == 4: # 如果包含强度值 coordinates = points[:, :3].astype(np.float64) if pred_mask is None: pred_mask = points[:, 3] > 0.5 # 使用第4列作为默认掩码 else: coordinates = points.astype(np.float64) if pred_mask is None: pred_mask = np.zeros(len(points), dtype=bool) # 设置颜色：红色=功能区域，蓝色=背景 colors = np.zeros((len(points), 3)) colors[pred_mask] = [1, 0, 0] # 红色 colors[~pred_mask] = [0, 0, 1] # 蓝色 pcd.points = o3d.utility.Vector3dVector(coordinates) pcd.colors = o3d.utility.Vector3dVector(colors.astype(np.float64)) # 创建可视化窗口 vis = o3d.visualization.Visualizer() vis.create_window(window_name='LASO Prediction Visualization') vis.add_geometry(pcd) # 设置渲染参数 opt = vis.get_render_option() opt.point_size = 2.5 # 点大小 opt.background_color = np.array([1, 1, 1]) # 白色背景 vis.run() # 运行可视化 vis.destroy_window() if __name__ == '__main__': # 数据路径设置 data_root = 'LASO_dataset' # 加载标注数据 with open(os.path.join(data_root, f'anno_val.pkl'), 'rb') as f: anno = pickle.load(f) # 加载点云数据 with open(os.path.join(data_root, f'objects_val.pkl'), 'rb') as f: objects = pickle.load(f) # 示例数据（第500个样本） print(\\\"当前物体类型: \\\", anno[500][\\\"class\\\"]) print(\\\"当前物体待预测的功能区域: \\\", anno[500][\\\"affordance\\\"]) point_cloud_data = objects[anno[500][\\\"shape_id\\\"]] # visualize_point_cloud(point_cloud_data) -- 可视化当前物体点云，再决定要使用什么文本查询 # 示例文本查询 text = \\\"If I want to move this chair, which part of the chair should I hold with my hands?\\\" # 执行预测和可视化 affordance_pred = predict_affordance_mask( point_cloud_data, text, \\\"runs/train/PointRefer/best_model-try_at_6.15_23.53.29.pt\\\" ) visualize_affordance(point_cloud_data, affordance_pred)\",\"预测结果可视化:\",\"If I want to move this chair, which part of the chair should I hold with my hands?\",\"If I want to sit on this chair, which part of the chair should I sit on?\",\"本文使用的是训练了9个epoch后的模型权重进行的推理演示，后续训练完50个epoch后，会进行推理能力结果更新。\"]},\"84\":{\"h\":\"3D-Vision Language\"},\"85\":{\"h\":\"简析PointNet++\",\"t\":[\"简析PointNet++\",\"论文: https://arxiv.org/abs/1706.02413 TensorFlow 版本代码: https://github.com/charlesq34/pointnet2 Pytorch 版本代码: https://github.com/yanx27/Pointnet_Pointnet2_pytorch\"]},\"86\":{\"h\":\"背景\",\"t\":[\"在PointNet中，网络对每一个点做低维到高维的映射，进行特征学习，然后把所有点映射到高维的特征通过最大池化最终表示全局特征。从本质上来说，要么对一个点做操作，要么对所有点做操作，实际上没有局部的概念(loal context) 。同时缺少 local context 在平移不变性上也有局限性（世界坐标系和局部坐标系）。对点云数据做平移操作后，所有的数据都将发生变化，导致所有的特征，全局特征都不一样了。对于单个的物体还好，可以将其平移到坐标系的中心，把他的大小归一化到一个球中，但是在一个场景中有多个物体时则不好办，需要对哪个物体做归一化呢？\",\"PointNet++ 解决了两个问题：如何生成点集的划分（Partitioning），以及如何通过局部特征学习器（Local Feature Learner）抽象点集或局部特征。\",\"生成点集的划分（Partitioning）：\",\"点集划分是指如何将一个大的点云分割成更小的、更易于管理的子集。这个过程类似于在传统的卷积神经网络中如何处理图像的小区域（或“patches”），以便可以在这些区域上应用局部操作。PointNet++需要一种方法来有效地将点云分割成多个部分，这样可以在每个部分上独立地学习特征。\",\"通过局部特征学习器（Local Feature Learner）抽象点集或局部特征：\",\"一旦点云被划分成小的子集，PointNet++的下一个任务是学习这些子集（或局部区域）的特征。这需要一个“局部特征学习器”，它能够从每个子集中提取有用的信息或特征。这与在传统CNN中学习图像局部区域特征的过程相似。\",\"两个问题是相关联的，因为：\",\"点集的划分必须产生跨分区的共同结构：为了能够在不同的局部子集上共享权重（类似于在CNN中权重共享的概念），PointNet++在进行点集划分时，需要确保这些划分具有一定的一致性或共同结构。这意味着即使是不同的局部子集，也应该以一种方式被处理，使得在它们之间可以共享学习到的特征表示的权重。这样做的目的是提高模型的效率和泛化能力，因为学习到的特征和权重可以在多个局部区域中复用。\",\"上述即为PointNet++设计中的两个核心挑战：\",\"如何有效地对点云进行分区，以便可以在这些分区上独立地学习特征。\",\"如何设计一个能够从这些局部分区中学习有用特征的机制，同时确保这些分区的处理方式允许在它们之间共享模型权重。 \",\"为了模仿传统卷积网络中的权重共享机制以提高学习效率和模型的泛化能力。\",\"PointNet++选择PointNet作为局部特征学习器（它是无序点云数据特征提取的高效算法）。\",\"可以理解为：PointNet++应用PointNet递归地对输入集进行嵌套分区。\"]},\"87\":{\"h\":\"模型结构\",\"t\":[\"以二维欧几里得空间为例，网络的分割和分类模型\",\"网络的每一组set abstraction layers主要包括3个部分：\",\"Sample layer : 对输入点进行采样，在这些点中选出若干个中心点。\",\"Grouping layer : 利用上一步得到的中心点将点集划分成若干个区域。\",\"PointNet layer : 对上述得到的每个区域进行编码，变成特征向量。\"]},\"88\":{\"h\":\"层次化点集特征学习\",\"t\":[\"层次化结构由多个set abstraction layers组成，在每个层上，一组点云被处理和抽象，以产生一个更少元素的新集合。set abstraction layers 由 Sampling layer、Grouping layer 和 PointNet layer 三部分组成。\",\"Sampling layer ：采样层 从输入点中选取一组点，定义局部区域的形心。\",\"Grouping layer ：通过查找形心点周围的“邻近点”来构建局部区域点集。\",\"PointNet layer ：使用mini-PointNet将局部区域编码为特征向量。\"]},\"89\":{\"h\":\"Sampling layer\",\"t\":[\"使用farthest point sampling（FPS）选择𝑁个点（相比于随机采样，该方法能更好的覆盖整个点集，具体选择多少个中心点以及邻域内的数量由超参数确定）\",\"FPS是一种在点云、图像处理或其他数据集中用于抽样的算法。目的是从一个大的数据集中选出一组代表性强的点，这些点彼此之间的最小距离尽可能大。\",\"作者通过FPS来抽样点集中较为重要的点。（即任务是找到点云集中的局部区域的中心点）\",\"可能存在的问题：计算成本、样本分布偏差（可能导致样本在高密度区域内过度集中，低密度区域则过于稀缺）、参数依赖（依赖初始点和距离度量方式的选择）、可能无法捕捉重要的几何细节。\"]},\"90\":{\"h\":\"Grouping layer\",\"t\":[\"文中作者通过Ball query来查询形心的邻居点。\",\"具体做法：给定两个超参数（每个区域中点的数量𝐾和query的半径𝑟），对于某个形心，Ball query找到该查询点在半径为𝑟范围内点，该范围确保局部区域的尺度是固定的。\",\"与K最近邻（kNN）查询相比，Ball query通过固定区域尺度而不是固定邻居数量来定义邻域。kNN查询寻找最近的K个邻居，但这可能导致所选邻域的实际尺寸随点的密度变化而变化，这在处理非均匀采样的数据时可能不是最优的选择。相反，Ball query通过确保每个局部区域都有一个固定的尺度，提高了模型在空间上的泛化能力。在实现时，通常会设置一个上限K，以限制每个局部区域中考虑的点的数量，以保持计算的可管理性。\",\"可改进的地方：对点云密度变换较为敏感、对参数选择依赖性高（半径太小可能无法有效捕获足够的局部详细，太大则可能导致不相关的点增多，使局部特征的表示不够精确）、计算效率问题、均匀性假设（Ball query是基于欧氏距离的均匀性假设）\",\"欧式距离的均匀性假设：即在欧氏空间中，两点的距离反映了这两点的实际相似度或关联度。\",\"基于以下前提： \",\"空间均匀性：空间是均匀和各向同性的，即任何方向上的度量都是等价的，距离的度量不受空间中位置的影响。\",\"距离直观性：在屋里空间或某些特定的抽象空间中，两个点之间的直线距离被认为是相似度或连接强度的直观表示。\",\"规模一致性：假设空间中所有区域的尺度或特征分布具有一定的一致性，即空间中的任何距离值具有相似的含义。\",\"总结: Grouping layer的任务是通过中心点找到邻居点，并将它们组织称为局部区域集。\"]},\"91\":{\"h\":\"PointNet layer\",\"t\":[\"局部坐标系转换：局部区域中的点转换成相对于形心的局部坐标系。\",\"局部区域中的每个点将相对于形心所在位置进行调整，以反映其相对位置。\",\"实现方法：通过将局部区域中的每个点-形心点的坐标来实现。\",\"特征编码：将转换后的坐标以及点的附加特征（文中的𝐶所表示的其他信息）一起送入mini-PointNet来提取局部区域中的特征。\",\"输出：利用相对坐标与点特征相结合的方式可以捕获局部区域中点与点之间的关系。\"]},\"92\":{\"h\":\"代码实现\",\"t\":[\"sample_and_group 这个函数的作用是从输入点云中：\",\"采样一些关键点\",\"为每个关键点构建局部邻域（局部区域）\",\"提取这些局部区域中的点及其特征\",\"def sample_and_group(npoint, radius, nsample, xyz, points, returnfps=False): \\\"\\\"\\\" Input: npoint: 采样的关键点数量 radius: 构建局部邻域的半径 nsample: 每个邻域内采样的关键点数量 xyz: 点云坐标数据 , [B, N, 3] points: 点的特征数据（可选）, [B, N, D] Return: new_xyz: 采样得到的关键点坐标, [B, npoint, nsample, 3] new_points: 每个关键点对应的局部区域点和特征, [B, npoint, nsample, 3+D] \\\"\\\"\\\" B, N, C = xyz.shape S = npoint # 使用 最远点采样（FPS） 从原始点云中选出 npoint 个具有代表性的点。 fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint] new_xyz = index_points(xyz, fps_idx) # [B, npoint, 3] # 对于每个选中的关键点，使用 球查询（Ball Query） 找出它周围距离小于 radius 的所有邻近点。 # 最多保留 nsample 个点，如果不够就重复最近的点来填充。 idx = query_ball_point(radius, nsample, xyz, new_xyz) # 把刚才找到的邻近点的坐标提取出来。 grouped_xyz = index_points(xyz, idx) # [B, npoint, nsample, 3] # 把它们相对于关键点的位置进行归一化（平移中心到以关键点为原点的局部坐标系上）。 grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C) # [B, npoint, nsample, 3] # 如果有额外的点特征（比如颜色、法线等），也一并提取。 if points is not None: grouped_points = index_points(points, idx) # 把邻近点的坐标和特征拼接在一起，形成最终的局部区域表示。 new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D] else: new_points = grouped_xyz_norm if returnfps: return new_xyz, new_points, grouped_xyz, fps_idx else: return new_xyz, new_points\",\"farthest_point_sample 这个函数实现的是最远点采样（Farthest Point Sampling, FPS）, 这是 PointNet++ 中用于从点云中选择具有代表性的采样点的一种策略。它的核心思想是：在点云中逐步选择离已选点尽可能远的点，使得采样点在整个点云空间中分布尽可能均匀 。\",\"def farthest_point_sample(xyz, npoint): \\\"\\\"\\\" Input: xyz: pointcloud data, [B, N, 3] npoint: number of samples Return: centroids: sampled pointcloud index, [B, npoint] \\\"\\\"\\\" device = xyz.device B, N, C = xyz.shape centroids = torch.zeros(B, npoint, dtype=torch.long).to(device) # 存储每次选出的“最远点”的索引。 distance = torch.ones(B, N).to(device) * 1e10 # 每个点到当前所有已选中心点的最小距离，初始设为一个极大值（1e10）。 farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device) # 初始时随机选择一个点作为第一个中心点。 batch_indices = torch.arange(B, dtype=torch.long).to(device) # 批次索引，用于快速访问每个 batch 的点。 # 重复 npoint 次，最终得到 npoint 个分布尽可能均匀的采样点索引。 for i in range(npoint): # 将当前选中的“最远点”索引保存下来； centroids[:, i] = farthest # （batch,npoint) # 取出当前最远点的坐标，用于后续计算其他点到该点的距离; centroid = xyz[batch_indices, farthest, :].view(B, 1, 3) # # （batch, 1 , 3) # 计算当前中心点与所有点之间的欧氏距离平方。 dist = torch.sum((xyz - centroid) ** 2, -1) # （batch,npoint) # 如果某个点到新中心点的距离比之前记录的“最小距离”还小，就更新它。 mask = dist < distance # 在 distance 中找到最大的那个距离对应的点，这就是下一个“最远点”。 distance[mask] = dist[mask] # （batch,npoint) # 在 distance 中找到最大的那个距离对应的点，这就是下一个“最远点”。 # 返回：一个元组：(values, indices)，分别是最大值和它们的位置索引。 farthest = torch.max(distance, -1)[1] # 返回位置索引 return centroids\",\"index_points 这个函数实现的是根据给定的索引 idx，从输入点云 points 中提取对应的点，形成一个新的子集。\",\"def index_points(points, idx): \\\"\\\"\\\" Input: points: input points data, [B, N, C] idx: sample index data, [B, S] Return: new_points:, indexed points data, [B, S, C] \\\"\\\"\\\" device = points.device B = points.shape[0] view_shape = list(idx.shape) view_shape[1:] = [1] * (len(view_shape) - 1) # 将view_shape的形状从[B, S]变成[B, 1]，便于广播 repeat_shape = list(idx.shape) repeat_shape[0] = 1 # 从[B, S]变成[1, S] # 从点云中根据索引提取特定点 (看不懂下面两行代码的话，可以先去了解一下python中的高级索引机制)。 batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape) new_points = points[batch_indices, idx, :] # （batch,npoint,3) return new_points\",\"query_ball_point 这个函数的作用是从点云中找出每个查询点周围一定半径范围内的邻近点索引。这个操作被称为 球查询（Ball Query）。\",\"def query_ball_point(radius, nsample, xyz, new_xyz): \\\"\\\"\\\" Input: radius: local region radius nsample: max sample number in local region xyz: all points, [B, N, 3] new_xyz: query points, [B, S, 3] Return: group_idx: grouped points index, [B, S, nsample] \\\"\\\"\\\" device = xyz.device B, N, C = xyz.shape _, S, _ = new_xyz.shape # 查询点数量（比如通过 FPS 得到的质心） # 构造一个从 0 到 N-1 的索引数组，代表原始点云中每个点的“身份证号” # 然后复制这个索引数组到每个 batch 和每个查询点上，形成 [B, S, N] 的结构 group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1]) # 计算每个查询点（new_xyz）与原始点（xyz）之间的平方欧氏距离 # 输出形状为 [B, S, N]：每个查询点对所有原始点的距离 sqrdists = square_distance(new_xyz, xyz) # 把距离超过 radius^2 的点全部替换为 N（一个非法索引），表示“这些人离我太远了，我不感兴趣。” group_idx[sqrdists > radius ** 2] = N # 对每个查询点的邻近点按索引排序（因为前面有 N，所以小的才是有效点） # 然后只保留前 nsample 个点 group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample] # 如果某个查询点附近的点太少，有些位置被标记为 N（无效）。 # 我们就用该查询点最近的那个点（第一个点）去填充这些空缺。 group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample]) mask = group_idx == N group_idx[mask] = group_first[mask] return group_idx # （batch,npoint,nsample)\",\"sample_and_group流程图\",\"sample_and_group_all 函数的作用是将整个点云视为一个“大局部区域”，不进行采样，直接对所有点进行特征提取，用于 PointNet++ 中的全局特征学习。\",\"def sample_and_group_all(xyz, points): \\\"\\\"\\\" Input: xyz: input points position data, [B, N, 3], 点云坐标数据 points: input points data, [B, N, D], 点云的额外特征（如法线、颜色等） Return: new_xyz: sampled points position data, [B, 1, 3] new_points: sampled points data, [B, 1, N, 3+D] \\\"\\\"\\\" device = xyz.device B, N, C = xyz.shape # 创建一个全零点作为“质心” # 虽然这个点没有实际意义，但它是为了统一接口设计的一个占位符 new_xyz = torch.zeros(B, 1, C).to(device) # 把原始点云 reshape 成一个大的局部区域 grouped_xyz = xyz.view(B, 1, N, C) # 如果有额外特征（比如法线、颜色），也一并加入 if points is not None: # 终输出的 new_points 是 [B, 1, N, 3+D]，代表每个 batch 中只有一组“大区域”的点及其特征 new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1) else: new_points = grouped_xyz return new_xyz, new_points # 全局质心点（0 位置）, 所有点组成的局部区域\",\"sample_and_group_all流程图\",\"PointNetSetAbstraction（点集抽象层） 是 PointNet++ 中的核心模块 ， 它的作用是负责从输入的点云数据中采样关键点，构建它们的局部邻域区域，并通过一个小型 PointNet 提取这些区域的高维特征，从而实现点云的分层特征学习。\",\"class PointNetSetAbstraction(nn.Module): def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all): super(PointNetSetAbstraction, self).__init__() self.npoint = npoint # 采样的关键点数量 self.radius = radius # 构建局部邻域的半径 self.nsample = nsample # 每个邻域内采样的关键点数量 self.mlp_convs = nn.ModuleList() self.mlp_bns = nn.ModuleList() last_channel = in_channel # 输入点的特征维度 for out_channel in mlp: self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1)) self.mlp_bns.append(nn.BatchNorm2d(out_channel)) last_channel = out_channel self.group_all = group_all def forward(self, xyz, points): \\\"\\\"\\\" Input: xyz: input points position data, [B, C, N] points: input points data, [B, D, N] Return: new_xyz: sampled points position data, [B, C, S] new_points_concat: sample points feature data, [B, D', S] \\\"\\\"\\\" xyz = xyz.permute(0, 2, 1) # [B, N, C] if points is not None: points = points.permute(0, 2, 1) # 如果 group_all=True，则对整个点云做全局特征提取。 if self.group_all: new_xyz, new_points = sample_and_group_all(xyz, points) else: # 否则使用 FPS（最远点采样）选关键点，再用 Ball Query 找出每个点的局部邻近点。 # 参数: 质点数量，采样半径，采样点数量，点坐标，点额外特征 new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points) # 局部特征编码（Mini-PointNet） # new_xyz: sampled points position data, [B, npoint, C] # new_points: sampled points data, [B, npoint, nsample, C+D] # 把邻域点的数据整理成适合卷积的格式 [B, C+D, nsample, npoint] new_points = new_points.permute(0, 3, 2, 1) # 使用多个 Conv2d + BatchNorm + ReLU 层提取特征 for i, conv in enumerate(self.mlp_convs): bn = self.mlp_bns[i] new_points = F.relu(bn(conv(new_points))) # [B, out_channel , nsample, npoint] # 对每个局部区域内所有点的最大响应值进行池化，得到该区域的固定长度特征表示。 # 在 new_points 的第 2 个维度（即每个局部邻域内的点数量维度）上做最大池化（max pooling）。 # 输出形状为 [B, out_channel, npoint]，即每个查询点有一个特征向量。 new_points = torch.max(new_points, 2)[0] # [B, out_channel, npoint] new_xyz = new_xyz.permute(0, 2, 1) # [B, C, npoint] return new_xyz, new_points # 查询点的位置(质心) ， 每个查询点点局部特征。\",\"最终每个采样得到的关键点所在的局部领域，都会被压缩为一个固定长度的特征向量。这个特征向量代表了这个局部区域的高维特征，它包含了这个区域内所有点的信息。\"]},\"93\":{\"h\":\"单尺度分组分类模型\",\"t\":[\"PointNet++ 的 单尺度分组（SSG）架构 ，通过多层 Set Abstraction 提取点云的层次化特征，并最终输出分类结果。\",\"Single-Scale Grouping (SSG)\",\"代码实现如下:\",\"# pointnet2_cls_ssg.py class get_model(nn.Module): # num_class: 输出类别数 # normal_channel: 是否包含法线信息（默认有 (x,y,z,nx,ny,nz)，否则只有 (x,y,z)） def __init__(self,num_class,normal_channel=True): super(get_model, self).__init__() in_channel = 6 if normal_channel else 3 self.normal_channel = normal_channel # PointNet++ 的核心就是逐层提取局部特征。这里的三个 SA 层构成了一个 三层分层特征学习结构 ： self.sa1 = PointNetSetAbstraction(npoint=512, radius=0.2, nsample=32, in_channel=in_channel, mlp=[64, 64, 128], group_all=False) self.sa2 = PointNetSetAbstraction(npoint=128, radius=0.4, nsample=64, in_channel=128 + 3, mlp=[128, 128, 256], group_all=False) self.sa3 = PointNetSetAbstraction(npoint=None, radius=None, nsample=None, in_channel=256 + 3, mlp=[256, 512, 1024], group_all=True) self.fc1 = nn.Linear(1024, 512) self.bn1 = nn.BatchNorm1d(512) self.drop1 = nn.Dropout(0.4) self.fc2 = nn.Linear(512, 256) self.bn2 = nn.BatchNorm1d(256) self.drop2 = nn.Dropout(0.4) self.fc3 = nn.Linear(256, num_class) def forward(self, xyz): B, _, _ = xyz.shape if self.normal_channel: norm = xyz[:, 3:, :] xyz = xyz[:, :3, :] else: norm = None l1_xyz, l1_points = self.sa1(xyz, norm) l2_xyz, l2_points = self.sa2(l1_xyz, l1_points) l3_xyz, l3_points = self.sa3(l2_xyz, l2_points) x = l3_points.view(B, 1024) x = self.drop1(F.relu(self.bn1(self.fc1(x)))) x = self.drop2(F.relu(self.bn2(self.fc2(x)))) x = self.fc3(x) x = F.log_softmax(x, -1) return x, l3_points\",\"完整的单尺度分组分类流程为:\",\"原始点云数据，首次sample，grouping，mini-PointNet后，得到:\",\"512 个关键点的坐标\",\"512 个关键点对应的局部区域特征向量\",\"二次sample，grouping，mini-PointNet后，得到:\",\"128 个关键点的坐标\",\"128 个关键点对应的局部区域特征向量\",\"三次sample，grouping，mini-PointNet后，得到:\",\"1 个关键点的坐标\",\"1 个关键点对应的全局区域特征向量\",\"获取全局区域特征向量后，通过全连接层进行分类。\"]},\"94\":{\"h\":\"非均匀密度下稳定的特征学习\",\"t\":[\"由于点集在不同区域可能会有不同的采样密度，这种非均匀性为点集特征学习带来了显著挑战。在密集采样的区域中学到的特征可能无法很好地泛化到稀疏采样的区域，反之亦然。因此，为了解决这一问题，PointNet++提出了密度自适应PointNet层，包含两种适应性特征学习层：多尺度分组（Multi-Scale Grouping, MSG）和多分辨率分组（Multi-Resolution Grouping, MRG）。\"]},\"95\":{\"h\":\"多尺度分组 （Multi-Scale Grouping）\",\"t\":[\"MSG通过应用不同尺度的分组层（按照不同的搜索半径或领域大小对点集进行分组），然后通过对应的PointNets提取每个尺度上的特征来捕获多尺度模式。不同尺度的特征被串联形成多尺度特征向量。这种方法使网络能够通过在训练期间随机丢弃输入点（称为随机输入丢弃 - random input dropout）来学习优化的策略，以结合来自不同尺度的特征。这样，网络在训练时被呈现了不同稀疏度的点集，从而学会根据输入数据的变化自适应地加权不同尺度上检测到的模式。\",\"多尺度分组\",\"具体来说，在MSG中，网络对于每个选定的形心点，按照几个预定义的半径值来搜索周围的邻近点。每个半径定义了一个局部邻域的大小，因此每个质心将根据这些不同的半径值与其周围点形成多个点集群。这样，对于每个质心点，网络不是只捕获一个尺度上的局部特征，而是能够捕获多个尺度上的局部特征。\",\"每个尺度（或每组邻域大小）的点集群都将独立地送入对应的PointNet网络进行特征提取，之后这些不同尺度上提取的特征被串联起来，形成一个综合的多尺度特征表示。这种方法使得网络能够在细节丰富的区域（通过较小的邻域尺度捕获细节）和稀疏采样的区域（通过较大的邻域尺度避免过度稀疏的问题）中均能有效提取特征。\"]},\"96\":{\"h\":\"多尺度分组分类模型\",\"t\":[\"PointNetSetAbstractionMsg 这个模块实现了 PointNet++ 中的 多尺度特征提取机制 ：对于每个局部区域，使用多个不同大小的邻域球（multi-scale ball query），分别提取特征，然后将这些不同尺度的特征拼接在一起，以获得更强的局部几何感知能力。\",\"class PointNetSetAbstractionMsg(nn.Module): def __init__(self, npoint, radius_list, nsample_list, in_channel, mlp_list): super(PointNetSetAbstractionMsg, self).__init__() self.npoint = npoint # 要采样的质心点数量 self.radius_list = radius_list # 不同尺度的查询半径列表 self.nsample_list = nsample_list # 对应半径下最多取多少邻近点 self.conv_blocks = nn.ModuleList() self.bn_blocks = nn.ModuleList() # 为每个尺度构建一个独立的小型 PointNet（Conv2d + BN + ReLU） # 每个尺度可以有不同的网络深度和宽度 # 所有尺度的网络并行运行，最后拼接结果 for i in range(len(mlp_list)): convs = nn.ModuleList() bns = nn.ModuleList() last_channel = in_channel + 3 for out_channel in mlp_list[i]: convs.append(nn.Conv2d(last_channel, out_channel, 1)) bns.append(nn.BatchNorm2d(out_channel)) last_channel = out_channel self.conv_blocks.append(convs) self.bn_blocks.append(bns) def forward(self, xyz, points): \\\"\\\"\\\" Input: xyz: input points position data, [B, C, N] points: input points data, [B, D, N] Return: new_xyz: sampled points position data, [B, C, S] new_points_concat: sample points feature data, [B, D', S] \\\"\\\"\\\" xyz = xyz.permute(0, 2, 1) # [B, N, C] if points is not None: points = points.permute(0, 2, 1) B, N, C = xyz.shape S = self.npoint # 使用 FPS（最远点采样）选出 S 个关键点作为局部区域中心 new_xyz = index_points(xyz, farthest_point_sample(xyz, S)) new_points_list = [] for i, radius in enumerate(self.radius_list): K = self.nsample_list[i] # 对每个半径 radius，找出该尺度下每个质心点周围的邻近点 group_idx = query_ball_point(radius, K, xyz, new_xyz) grouped_xyz = index_points(xyz, group_idx) # 把这些点的坐标归一化到以质心为中心的局部坐标系下 grouped_xyz -= new_xyz.view(B, S, 1, C) # 如果有额外特征，也一并加入 if points is not None: grouped_points = index_points(points, group_idx) grouped_points = torch.cat([grouped_points, grouped_xyz], dim=-1) else: grouped_points = grouped_xyz # 对每个尺度的局部点集应用对应的 Conv2d + BN + ReLU grouped_points = grouped_points.permute(0, 3, 2, 1) # [B, D, K, S] for j in range(len(self.conv_blocks[i])): conv = self.conv_blocks[i][j] bn = self.bn_blocks[i][j] grouped_points = F.relu(bn(conv(grouped_points))) # 使用最大池化聚合局部信息，生成固定长度的特征向量 new_points = torch.max(grouped_points, 2)[0] # [B, D', S] # 所有尺度的特征保存到 new_points_list new_points_list.append(new_points) new_xyz = new_xyz.permute(0, 2, 1) # 把不同尺度学到的特征拼接在一起，形成最终的局部特征表示 new_points_concat = torch.cat(new_points_list, dim=1) # 最终输出就是： 一组新的关键点位置； 每个关键点的多尺度特征表示 return new_xyz, new_points_concat\",\"pointnet2_cls_msg 这个模型使用了 PointNet++ 的 多尺度分组（MSG）策略 ，通过多个局部区域球查询提取不同尺度的局部特征，逐层抽象后融合成全局特征，最后通过全连接层完成分类任务。\",\"# pointnet2_cls_msg.py class get_model(nn.Module): def __init__(self,num_class,normal_channel=True): super(get_model, self).__init__() in_channel = 3 if normal_channel else 0 self.normal_channel = normal_channel self.sa1 = PointNetSetAbstractionMsg(512, [0.1, 0.2, 0.4], [16, 32, 128], in_channel,[[32, 32, 64], [64, 64, 128], [64, 96, 128]]) self.sa2 = PointNetSetAbstractionMsg(128, [0.2, 0.4, 0.8], [32, 64, 128], 320,[[64, 64, 128], [128, 128, 256], [128, 128, 256]]) self.sa3 = PointNetSetAbstraction(None, None, None, 640 + 3, [256, 512, 1024], True) self.fc1 = nn.Linear(1024, 512) self.bn1 = nn.BatchNorm1d(512) self.drop1 = nn.Dropout(0.4) self.fc2 = nn.Linear(512, 256) self.bn2 = nn.BatchNorm1d(256) self.drop2 = nn.Dropout(0.5) self.fc3 = nn.Linear(256, num_class) def forward(self, xyz): B, _, _ = xyz.shape if self.normal_channel: norm = xyz[:, 3:, :] xyz = xyz[:, :3, :] else: norm = None l1_xyz, l1_points = self.sa1(xyz, norm) l2_xyz, l2_points = self.sa2(l1_xyz, l1_points) l3_xyz, l3_points = self.sa3(l2_xyz, l2_points) x = l3_points.view(B, 1024) x = self.drop1(F.relu(self.bn1(self.fc1(x)))) x = self.drop2(F.relu(self.bn2(self.fc2(x)))) x = self.fc3(x) x = F.log_softmax(x, -1) return x,l3_points\",\"MSG的关键优点在于它通过在训练期间的随机输入丢弃（即随机移除一部分输入点）来模拟不同的采样密度，从而训练网络在面对实际应用中可能遇到的各种采样密度时，能够自适应地选择最适合的特征尺度进行组合，以实现最佳的性能。这种方法大大增强了网络处理非均匀采样数据的能力，提高了模型的泛化性和稳健性。\",\"在训练时引入不同密度的点集情况，使网络能学习不同采样密度下局部点云特征的提取，捕获密集到稀疏采样区域内的多尺度信息 -- 通过随机丢弃来模拟不同密度的采样，使网络能够应对实际中各种密度变换的情况-提高模型的泛化性能。\",\"MSG相当于并联了多个hierarchical structure，每个结构中心点不变，但是尺度不同。通过PointNet获取每个形心多尺度信息，之后concat形成该区域提取的总特征。在训练时引入随机丢弃形心来模拟不同密度情况，提高算法鲁棒性。\"]},\"97\":{\"h\":\"多分辨率分组（Multi-Resolution Grouping）\",\"t\":[\"MSG方法虽然有效，但在计算上可能非常昂贵，尤其是在低层次上对每个质心点运行局部PointNet时。为此，MRG为一种低成本的替代方案。\",\"MRG通过结合来自不同分辨率的特征来实现效率和适应性的平衡。具体而言，MRG策略在处理每个局部区域时，不仅考虑从当前分辨率下抽象得到的特征，还考虑了从更低分辨率（即上一层级）直接提取的特征。这两种特征被concat为一个复合特征向量，为后续的处理步骤提供信息。\",\"多分辨率分组\",\"在MRG中，某一层次𝐿𝑖的区域特征是通过将来自下一级𝐿𝑖−1的子区域特征总结后的向量与直接处理该局部区域所有原始点的单个PointNet得到的特征向量进行concat得到的。当局部区域的密度较低时，由于子区域在计算第一个向量时包含的点更稀疏，因此可能比第二个向量更不可靠。在这种情况下，应该更高地加权第二个向量。相反，当局部区域的密度较高时，第一个向量提供了更细致的信息，因为它能够在更低层次上递归地检视更高分辨率。\",\"来自下一级的特征：首先，将来自下一级（更高分辨率）的特征进行汇总，形成一个特征向量。这一过程通过对每个子区域应用集合抽象层（set abstraction level）完成。\",\"直接处理的原始点特征：另一部分特征是通过在当前分辨率直接对所有原始点应用单个PointNet得到的。\"]},\"98\":{\"h\":\"点云语义分割\",\"t\":[\"PointNet++ 完成点云分割任务的过程是一个典型的“编码-解码”结构，结合了层级特征提取和多尺度融合机制。\",\"目标: 给定一个点云，模型需要为每个点预测一个类别标签（如桌子、椅子、墙壁等）。\",\"输入：xyz: [B, N, 3]\",\"输出：labels: [B, N, C]，其中 C 是类别数\",\"PointNet++ 分割的整体结构 :\",\"Input Points (xyz): [ B, N, 3 ] ↓ Set Abstraction Layers（编码器） ↓ Feature Vectors at Multiple Scales ↓ Feature Propagation Layers（解码器） ↓ Recovered Features at Original Resolution ↓ MLP + Softmax → Per-point Semantic Labels\",\"第一步：Set Abstraction（集合抽象）—— 编码器: 对点云进行下采样，并在每个局部区域提取特征。\",\"核心操作包括：\",\"FPS（Farthest Point Sampling）：从点云中选出有代表性的点作为中心点。\",\"Ball Query：为每个中心点找到其邻域内的点。\",\"Grouping：将邻域点组合成局部点云组。\",\"PointNet 操作：使用 T-Net 对局部点云进行变换，然后通过 MLP 提取特征。\",\"Pooling：对局部点云组做最大池化或平均池化，得到该区域的特征。\",\"多个 Set Abstraction 层堆叠，逐步减少点的数量，增加特征维度，形成多尺度特征表示。\",\"第二步：Feature Propagation（特征传播）—— 解码器: 从最稀疏的点开始，逐层将特征插值回原始点数量。\",\"特征插值方式：\",\"使用 反距离加权插值（IDW），即根据最近的几个邻近点的距离进行加权平均。\",\"可选地拼接 skip connection 中的原始特征（来自 Set Abstraction 前的某一层）。\",\"输入输出示例：\",\"def forward(xyz1, xyz2, points1, points2): # xyz1: 原始点坐标（多） # xyz2: 下采样点坐标（少） # points1: 原始点特征（可为空） # points2: 下采样点特征 return interpolated_points # 插值得到的密集特征，形状与 xyz1 一致\",\"多个 Feature Propagation 层堆叠，逐渐恢复点数，最终回到原始点数量。\",\"第三步：Head 预测头 —— 分类每个点: 对每个点的特征做一个简单的分类器，输出类别概率。\",\"实现方式：\",\"将最后一层 Feature Propagation 输出的特征送入一个小型 MLP。\",\"最后一层使用 Softmax（对于多分类）或 Sigmoid（对于多标签）激活函数。\",\"例如：\",\"mlp = nn.Sequential( nn.Conv1d(128, 128, 1), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.5), nn.Conv1d(128, num_classes, 1) ) logits = mlp(final_features) # shape: [B, C, N]\"]},\"99\":{\"h\":\"代码实现\",\"t\":[\"PointNet++ 的整体结构是一个典型的 编码器-解码器（Encoder-Decoder）架构 ：\",\"Set Abstraction 层 ：不断对点云进行下采样 + 提取局部特征（编码过程）\",\"Feature Propagation 层 ：从最稀疏的点开始，逐层恢复到原始点数（解码过程）\",\"[Input Points] ↓ SA Layer 1 → [Points: 1024 → 512] ↓ SA Layer 2 → [Points: 512 → 128] ↓ SA Layer 3 → [Points: 128 → 32] ↓ FP Layer 3 ← [Points: 32 → 128] ↓ FP Layer 2 ← [Points: 128 → 512] ↓ FP Layer 1 ← [Points: 512 → 1024] ↓ [Per-point Classification Head] ↓ [Output: per-point labels]\"]},\"100\":{\"h\":\"特征传播层\",\"t\":[\"PointNetFeaturePropagation 是 PointNet++ 中用于点云“特征传播”（Feature Propagation）的核心模块，主要作用是：\",\"将稀疏点集的特征插值回原始点集的位置上。\",\"换句话说：\",\"输入：少量点的坐标 + 特征（如经过下采样后的点）\",\"输出：在原始点数量下的每个点都拥有一个合理的特征向量\",\"这一步相当于图像任务中的 上采样（upsample）或转置卷积（transpose convolution） ，但在点云这种非结构化数据中，不能直接使用这些操作。\",\"class PointNetFeaturePropagation(nn.Module): def __init__(self, in_channel, mlp): \\\"\\\"\\\" 初始化函数，构建用于特征传播（上采样）的MLP层 参数： in_channel: 输入特征的通道数（维度） mlp: 一个列表，表示每一层MLP的输出通道数，例如 [64, 128] \\\"\\\"\\\" super(PointNetFeaturePropagation, self).__init__() # 用于保存卷积层和批归一化层 self.mlp_convs = nn.ModuleList() self.mlp_bns = nn.ModuleList() last_channel = in_channel # 当前输入通道数初始化为in_channel # 构建MLP层：每个层是一个Conv1d + BatchNorm1d + ReLU for out_channel in mlp: self.mlp_convs.append(nn.Conv1d(last_channel, out_channel, 1)) self.mlp_bns.append(nn.BatchNorm1d(out_channel)) last_channel = out_channel # 更新下一层的输入通道数 def forward(self, xyz1, xyz2, points1, points2): \\\"\\\"\\\" 前向传播函数：将稀疏点集points2插值到密集点集xyz1的位置上 参数： xyz1: 原始点坐标数据，形状 [B, C, N] （如 1024 个点） xyz2: 下采样后的点坐标数据，形状 [B, C, S] （如 256 个点） points1: 原始点对应的特征数据，形状 [B, D, N] （可为 None） points2: 下采样点对应的特征数据，形状 [B, D, S] 返回： new_points: 插值并融合后的特征，形状 [B, D', N] \\\"\\\"\\\" # 将坐标和特征从 [B, C, N] 转换为 [B, N, C] 格式，便于后续计算 xyz1 = xyz1.permute(0, 2, 1) # [B, N, C] xyz2 = xyz2.permute(0, 2, 1) # [B, S, C] points2 = points2.permute(0, 2, 1) # [B, S, D] B, N, C = xyz1.shape # 原始点数量 N _, S, _ = xyz2.shape # 下采样点数量 S # 如果只有1个下采样点，直接复制其特征到所有原始点 if S == 1: interpolated_points = points2.repeat(1, N, 1) # [B, N, D] else: # 计算原始点与下采样点之间的距离矩阵（欧氏距离平方） dists = square_distance(xyz1, xyz2) # [B, N, S] # 对每个原始点，找到最近的3个邻近点 dists, idx = dists.sort(dim=-1) dists = dists[:, :, :3] # 取最小的三个距离 [B, N, 3] idx = idx[:, :, :3] # 取对应的索引 [B, N, 3] # 使用反距离加权（IDW）计算权重: # 1.将距离转换为“权重”，距离越近，权重越大 dist_recip = 1.0 / (dists + 1e-8) # 避免除以零 # 2.对每个点的3个权重求和，得到归一化因子 norm = torch.sum(dist_recip, dim=2, keepdim=True) # 归一化因子 # 3.归一化权重，使得每个点的权重之和为1 weight = dist_recip / norm # 加权平均系数 [B, N, 3] # 为每个原始点，找到它最近的 3 个邻近点，根据距离分配权重，然后对它们的特征做加权平均，从而插值得到该点的特征。 # index_points: [B, S, D] -> [B, N, 3, D] # weight.view(B, N, 3, 1): 扩展维度后相乘 interpolated_points = torch.sum( # 1. 从下采样点中取出每个原始点对应的最近邻点的特征。 # points2: [B, S, D] —— 下采样点的特征（S 个点，每个点有 D 维特征） # idx: [B, N, 3] —— 每个原始点对应的 3 个最近邻点索引 # [B, N, 3, D] —— 每个原始点都有了它最近的 3 个邻近点的特征 index_points(points2, idx) # 将之前计算好的权重扩展维度，以便和特征相乘。 # weight: [B, N, 3] —— 每个点的三个邻近点的权重 # [B, N, 3, 1] —— 扩展后便于广播乘法 * weight.view(B, N, 3, 1), dim=2 ) # [B, N, D] # 如果原始点有特征，则拼接起来（skip connection） if points1 is not None: points1 = points1.permute(0, 2, 1) # [B, N, D] new_points = torch.cat([points1, interpolated_points], dim=-1) # [B, N, D1+D2] else: new_points = interpolated_points # [B, N, D] # 恢复张量格式为 [B, D, N]，以适配后面的卷积操作 new_points = new_points.permute(0, 2, 1) # [B, D', N] # 经过MLP进一步提取和融合特征 for i, conv in enumerate(self.mlp_convs): bn = self.mlp_bns[i] new_points = F.relu(bn(conv(new_points))) # Conv1d + BN + ReLU return new_points # 最终输出特征 [B, D', N]\",\"流程四步走：\",\"1️⃣ 找到邻居 “我这个点最近的3个熟人是谁？”\",\"计算每个原始点和下采样点之间的距离；\",\"找出最近的3个邻近点。\",\"2️⃣ 分配权重 “谁离我越近，说话越有分量。”\",\"根据距离反比加权（IDW），给这3个邻近点分配权重；\",\"权重归一化，确保它们加起来是1。\",\"3️⃣ 加权平均插值 “综合最近几个熟人的意见，猜出我的特征。”\",\"提取邻近点的特征；\",\"按照权重做加权平均；\",\"得到每个原始点的插值特征。\",\"4️⃣ 融合+增强 “如果我本来就有特征，那就一起用；再用MLP提提神。”\",\"如果原始点有自己的特征（points1），就拼接起来；\",\"经过几层 Conv1d + BN + ReLU，进一步提取和融合特征；\",\"输出最终的插值后特征。\",\"📦 输出结果\",\"new_points: 每个原始点都有了一个新的特征向量 [B, D', N]\"]},\"101\":{\"h\":\"点云语义分割模型\",\"t\":[\"下面给出的是一个基于PointNet++的点云语义分割模型定义 ，其主要功能是：\",\"对输入点云中的每个点进行分类（如桌子、椅子、地板等），输出每个点的类别概率。\",\"网络结构特点：\",\"使用 Set Abstraction（SA）层 进行多尺度特征提取和下采样；\",\"使用 Feature Propagation（FP）层 进行特征插值和上采样；\",\"最后通过两个卷积层输出每个点的分类结果；\",\"输出为 [B, N, num_classes]，即每个点都有一个类别预测。\",\"class get_model(nn.Module): def __init__(self, num_classes): \\\"\\\"\\\" 初始化 PointNet++ 分割网络 参数： num_classes: 分类类别数 \\\"\\\"\\\" super(get_model, self).__init__() # Set Abstraction 层（编码器部分） # 每层逐步下采样，并提取更高级别的局部特征 self.sa1 = PointNetSetAbstraction(npoint=1024, radius=0.1, nsample=32, in_channel=9+3, mlp=[32, 32, 64], group_all=False) self.sa2 = PointNetSetAbstraction(npoint=256, radius=0.2, nsample=32, in_channel=64+3, mlp=[64, 64, 128], group_all=False) self.sa3 = PointNetSetAbstraction(npoint=64, radius=0.4, nsample=32, in_channel=128+3, mlp=[128, 128, 256], group_all=False) self.sa4 = PointNetSetAbstraction(npoint=16, radius=0.8, nsample=32, in_channel=256+3, mlp=[256, 256, 512], group_all=False) # Feature Propagation 层（解码器部分） # 从稀疏点恢复到原始点密度，逐层融合上下文信息 self.fp4 = PointNetFeaturePropagation(in_channel=768, mlp=[256, 256]) self.fp3 = PointNetFeaturePropagation(in_channel=384, mlp=[256, 256]) self.fp2 = PointNetFeaturePropagation(in_channel=320, mlp=[256, 128]) self.fp1 = PointNetFeaturePropagation(in_channel=128, mlp=[128, 128, 128]) # 最终分类头 self.conv1 = nn.Conv1d(128, 128, 1) self.bn1 = nn.BatchNorm1d(128) self.drop1 = nn.Dropout(0.5) self.conv2 = nn.Conv1d(128, num_classes, 1) def forward(self, xyz): \\\"\\\"\\\" 前向传播函数 输入： xyz: 点云数据，形状 [B, C, N] 返回： x: 每个点的分类结果，形状 [B, N, num_classes] l4_points: 最后一层抽象特征，用于其他任务 \\\"\\\"\\\" # l0 表示最原始的点云 l0_points = xyz l0_xyz = xyz[:, :3, :] # 只取 xyz 坐标，不带法向量或其他属性 # 编码器：层层下采样并提取特征 l1_xyz, l1_points = self.sa1(l0_xyz, l0_points) # 1024 points l2_xyz, l2_points = self.sa2(l1_xyz, l1_points) # 256 points l3_xyz, l3_points = self.sa3(l2_xyz, l2_points) # 64 points l4_xyz, l4_points = self.sa4(l3_xyz, l3_points) # 16 points # 解码器：层层插值并融合特征 l3_points = self.fp4(l3_xyz, l4_xyz, l3_points, l4_points) # 64 → 64 l2_points = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points) # 256 → 256 l1_points = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points) # 1024 → 1024 l0_points = self.fp1(l0_xyz, l1_xyz, None, l1_points) # 4096 → 4096 # MLP 头部处理：进一步增强特征 x = self.drop1(F.relu(self.bn1(self.conv1(l0_points)), inplace=True)) # [B, 128, N] x = self.conv2(x) # [B, num_classes, N] # Softmax 分类 x = F.log_softmax(x, dim=1) # [B, num_classes, N] # 调整维度，返回 [B, N, num_classes] x = x.permute(0, 2, 1) return x, l4_points # 返回每个点的分类结果和抽象特征\"]},\"102\":{\"h\":\"简析PointNet\",\"t\":[\"简析PointNet网络模型及其背后原理\",\"论文: https://arxiv.org/abs/1612.00593 TensorFlow 版本代码: https://github.com/charlesq34/pointnet Pytorch 版本代码: https://github.com/fxia22/pointnet.pytorch\"]},\"103\":{\"h\":\"核心\",\"t\":[\"问题背景: 点云是三维几何数据的一种重要表示形式，但由于其无序性和非规则性，传统卷积神经网络难以直接处理。\",\"❌ 传统方法的缺陷 ：\",\"将点云转换为体素网格（voxel grid）或图像视图（multi-view rendering）， 这些方法会导致信息损失、计算量大、不灵活等问题。\",\"🌟 PointNet 的创新点 ：\",\"直接以点集作为输入，避免了复杂的预处理；\",\"设计了一个统一架构，适用于分类、物体分割和场景语义解析；\",\"利用对称函数（如最大池化）实现点集顺序不变性；\",\"引入 T-Net（空间变换网络）标准化输入点云和特征空间。\"]},\"104\":{\"h\":\"难点\",\"t\":[\"点云的无序性（Unordered）: 点云是点的集合，没有固定顺序；模型必须对输入点的排列顺序不敏感（permutation invariant）。\",\"点之间存在相互作用（Interaction among points）: 点与点之间有空间关系，需要捕捉局部结构。\",\"对几何变换的不变性（Invariance under transformations）: 模型输出应不受刚性变换影响（如旋转、平移）。\",\"输入点云可能缺失或包含噪声（Missing or noisy points）: 实际采集的点云常有遮挡、稀疏、异常值等问题。\"]},\"105\":{\"h\":\"解决方案\",\"t\":[\"✅ 难点 1：点云的无序性 → 使用对称函数（Symmetric Function）\",\"使用 max pooling 作为对称函数，聚合所有点的信息；\",\"所有点经过共享参数的 MLP 提取特征；\",\"最终输出与点的顺序无关；\",\"原理说明：\",\"f({x1, ..., xn}) ≈ g(h(x1), ..., h(xn)) = γ(MAX(h(x1), ..., h(xn)))\",\"其中：\",\"h(xi) 是每个点的高维特征；\",\"MAX 是 max pooling 函数；\",\"γ 是后续的全连接网络；\",\"整个函数 f 是对称的，即对点顺序不敏感。\",\"效果：\",\"实验证明 max pooling 比排序、RNN、average pooling 更有效；s\",\"PointNet 可以处理任意顺序的点集；\",\"✅ 难点 2：点之间的相互作用 → 设计局部 + 全局信息融合机制\",\"在分割任务中，将全局特征与每个点的局部特征拼接起来；\",\"这样每个点在预测标签时都能看到整个物体的上下文；\",\"效果：\",\"显著提升了分割性能；\",\"让模型既关注局部细节，又理解整体结构；\",\"✅ 难点 3：对几何变换的不变性 → 引入 T-Net（空间变换网络）\",\"引入两个空间变换网络： \",\"STN3d：对输入点云做刚性变换（3×3 矩阵）；\",\"STNkd：对特征空间做变换（64×64 矩阵）；\",\"加入正则项约束变换矩阵接近正交：\",\"L_reg = ||I - A @ A^T||_F^2\",\"效果：\",\"PointNet 对点云的旋转、平移等变换具有鲁棒性；\",\"提升了模型的泛化能力和稳定性；\",\"✅ 难点 4：输入点云可能缺失或含有异常点 → 理论分析保证模型鲁棒性\",\"理论上证明 PointNet 学到的是一个“关键点集”（critical point set），即只依赖一小部分关键点就能判断整体形状；\",\"即使丢失一些点或加入异常点，只要关键点还在，结果就不会变；\",\"定理表明：\",\"小扰动不会改变函数输出；\",\"网络输出由一个有限子集 CS 决定（大小不超过 bottleneck 维度 K）；\",\"CS 是关键点集合，NS 是最大可容忍的点云范围；\",\"实验验证：\",\"即使 50% 的点缺失，分类准确率仅下降约 3.7%；\",\"对异常点也有一定容忍能力；\",\"✅ 总结: PointNet 通过 max pooling 实现对称性，结合 T-Net 实现变换不变性，并通过局部+全局特征融合机制实现强大的点云建模能力，解决了点云处理中的四大技术难点，为后续三维深度学习奠定了基础。\"]},\"106\":{\"h\":\"代码(Pytorch版本)\",\"t\":[\"PointNet网络模型结构图\"]},\"107\":{\"h\":\"输入标准化\",\"t\":[\"在 PointNet 架构中，第一层是一个叫做 STN3d（Spatial Transformer Network for 3D points） 的模块，它的目标是：\",\"✅ 对输入的点云做刚性变换（如旋转 + 平移），使其姿态统一，提升模型鲁棒性。\",\"这是因为在实际采集过程中，点云的姿态可能各不相同（比如椅子朝向不同、扫描角度不同等），如果不加处理，会影响特征提取的一致性。\",\"STN3d 是一个小型神经网络，专门用于预测一个 3×3 的变换矩阵 ，这个矩阵表示对点云所做的变换（通常是旋转或反射）。\",\"它具有以下特点：\",\"输入是原始点云（shape: (B, 3, N)）；\",\"输出是一个变换矩阵（shape: (B, 3, 3)）；\",\"这个变换矩阵是近似正交的，保证变换是刚性的；\",\"变换矩阵会通过 torch.bmm() 应用到原始点云上（这一步不在 STN3d 类中）；\",\"目的是让点云“摆正”，便于后续处理。\",\"代码实现:\",\"class STN3d(nn.Module): def __init__(self): super(STN3d, self).__init__() # 使用 1D 卷积 处理点云数据（每个点有 3 个坐标值） # kernel_size=1 表示只在通道维度操作，不考虑空间邻域关系 # 提取每一点的特征向量（从 3 → 64 → 128 → 1024） self.conv1 = torch.nn.Conv1d(3, 64, 1) self.conv2 = torch.nn.Conv1d(64, 128, 1) self.conv3 = torch.nn.Conv1d(128, 1024, 1) # 经过全局池化后得到一个全局特征向量（1024维） # 用全连接层逐步压缩到 9 个输出 → 对应一个 3x3 的变换矩阵 self.fc1 = nn.Linear(1024, 512) self.fc2 = nn.Linear(512, 256) self.fc3 = nn.Linear(256, 9) # 所有卷积和 FC 层后面都加了 BN 和 ReLU，帮助训练稳定收敛 self.relu = nn.ReLU() self.bn1 = nn.BatchNorm1d(64) self.bn2 = nn.BatchNorm1d(128) self.bn3 = nn.BatchNorm1d(1024) self.bn4 = nn.BatchNorm1d(512) self.bn5 = nn.BatchNorm1d(256) # x: (batch,3,point_size) def forward(self, x): # 获取当前 batch 的大小（有多少组点云） batchsize = x.size()[0] # CNN 逐点，通道维度特征提取阶段 x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) # x: (batch,1024,point_size) # 全局最大池化（Global Max Pooling） # 从所有点中选出每个通道的最大响应值，作为整个点云的“抽象”表示，shape: (B, 1024) # 返回：一个元组：(values, indices)，分别是最大值和它们的位置索引。 x = torch.max(x, 2, keepdim=True)[0] # x: (batch,1024,1) x = x.view(-1, 1024) # x: (batch,1024) # 全连接层预测变换矩阵 x = F.relu(self.bn4(self.fc1(x))) x = F.relu(self.bn5(self.fc2(x))) x = self.fc3(x) # x: (batch,9) # 加上单位矩阵作为初始偏置 # 初始假设变换为恒等变换（不做任何变化） iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1) if x.is_cuda: iden = iden.cuda() # 让网络从一个小扰动开始学习，更容易训练 x = x + iden # 最终 reshape 成 3x3 矩阵返回 x = x.view(-1, 3, 3) return x\",\"标准化的意义:\",\"✅ 1. 解决点云姿态不一致问题\",\"输入点云可能来自不同角度、不同位置；\",\"T-Net 把它们“对齐”到一个标准姿态；\",\"这样 PointNet 后续的特征提取更稳定。\",\"✅ 2. 提升模型鲁棒性\",\"如果没有 T-Net，PointNet 必须自己学会对各种姿态都识别准确；\",\"加入 T-Net 后，相当于加了一个“预处理层”，让模型更容易训练和泛化。\",\"神经网络的输出在训练初期往往接近于零，如果直接作为变换矩阵，会导致非正交、不稳定。PointNet 通过“加单位矩阵”的方式，让变换矩阵从一个恒等变换开始学习，并结合正则化损失，逐步向正交矩阵靠拢，从而保证变换是刚性的、稳定的。\"]},\"108\":{\"h\":\"正则化损失\",\"t\":[\"feature_transform_regularizer 是 PointNet 中用于约束变换矩阵接近正交性的正则化损失函数。\",\"🧠 为什么需要这个正则化项？\",\"在 PointNet 中，为了提升模型对点云姿态变化的鲁棒性，引入了两个变换网络：\",\"STN3d: 对原始点云做刚性变换（如旋转、反射），使其标准化。\",\"STNkd: 对特征空间做变换，使特征分布更稳定。\",\"这两个网络输出的是变换矩阵（分别是 3×3 和 k×k 的矩阵）。但由于它们是神经网络直接预测出来的，并不能保证这些矩阵是正交矩阵（orthogonal matrix） 。\",\"❗而只有正交矩阵才能表示刚性变换（rigid transformation），即只改变物体的方向而不改变形状和大小。\",\"所以我们需要加一个正则化项来鼓励变换矩阵接近正交矩阵 , 这就是 feature_transform_regularizer 的作用！\",\"def feature_transform_regularizer(trans): d = trans.size()[1] batchsize = trans.size()[0] # 构造一个单位矩阵 I，用于后续比较； # 添加 None 是为了扩展成 (1, d, d)，便于广播到整个 batch； I = torch.eye(d)[None, :, :] if trans.is_cuda: I = I.cuda() # 计算变换矩阵与其转置相乘后与单位矩阵之间的距离（Frobenius 范数），然后取 batch 平均值作为损失项，鼓励变换矩阵接近正交矩阵。 # Frobenius 范数（矩阵所有元素平方和开方） loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2))) return loss\"]},\"109\":{\"h\":\"特征提取\",\"t\":[\"PointNet 的核心特征提取模块 PointNetfeat ，它负责从输入点云中提取出可用于分类或分割的特征。\",\"class PointNetfeat(nn.Module): def __init__(self, global_feat = True, feature_transform = False): super(PointNetfeat, self).__init__() # 输入点云变换网络（3D） self.stn = STN3d() # 使用 Conv1D 对每个点进行特征提取； # 每个卷积层后跟一个 BatchNorm 层； # 最终输出高维特征（1024维）； self.conv1 = torch.nn.Conv1d(3, 64, 1) self.conv2 = torch.nn.Conv1d(64, 128, 1) self.conv3 = torch.nn.Conv1d(128, 1024, 1) self.bn1 = nn.BatchNorm1d(64) self.bn2 = nn.BatchNorm1d(128) self.bn3 = nn.BatchNorm1d(1024) # 全局特征开关：控制是否输出全局特征 self.global_feat = global_feat # 特征变换开关：控制是否使用 STN 对特征空间进行变换 self.feature_transform = feature_transform if self.feature_transform: self.fstn = STNkd(k=64) def forward(self, x): n_pts = x.size()[2] # 使用 STN3d 预测出一个变换矩阵； trans = self.stn(x) x = x.transpose(2, 1) # 将原始点云“摆正”； x = torch.bmm(x, trans) x = x.transpose(2, 1) # 再通过第一个卷积层提取初始特征； x = F.relu(self.bn1(self.conv1(x))) if self.feature_transform: trans_feat = self.fstn(x) x = x.transpose(2,1) x = torch.bmm(x, trans_feat) x = x.transpose(2,1) else: trans_feat = None # 提取更高维的特征； # 最后一层输出 shape: (B, 1024, N) pointfeat = x x = F.relu(self.bn2(self.conv2(x))) x = self.bn3(self.conv3(x)) # 从所有点中选出每个通道的最大响应值，作为整个点云的“抽象”表示，shape: (B, 1024) # 返回：一个元组：(values, indices)，分别是最大值和它们的位置索引。 x = torch.max(x, 2, keepdim=True)[0] x = x.view(-1, 1024) # 如果是分类任务 (global_feat=True)： if self.global_feat: return x, trans, trans_feat else: # 如果是分割任务 (global_feat=False)： x = x.view(-1, 1024, 1).repeat(1, 1, n_pts) return torch.cat([x, pointfeat], 1), trans, trans_feat\",\"✅ 如果是分类任务 (global_feat=True)，则返回：\",\"x: 全局特征 (B, 1024)\",\"trans: 输入点云变换矩阵\",\"trans_feat: 特征空间变换矩阵（可选）\",\"✅ 如果是分割任务 (global_feat=False)， 则返回：\",\"把全局特征复制 N 次并与每个点的局部特征，在通道维度进行拼接\",\"将每个点的局部特征与整个点云的全局特征拼接起来，让每个点都能看到上下文信息\",\"输出 shape: (B, 1088, N) ，即 1088 = 1024+64\"]},\"110\":{\"h\":\"分类任务\",\"t\":[\"PointNet 的分类模块 PointNetCls ，它基于前面的特征提取模块 PointNetfeat 来完成点云分类任务。\",\"class PointNetCls(nn.Module): def __init__(self, k=2, feature_transform=False): super(PointNetCls, self).__init__() self.feature_transform = feature_transform # 它使用 PointNetfeat 提取全局特征（1024维）； self.feat = PointNetfeat(global_feat=True, feature_transform=feature_transform) # 然后通过全连接层（MLP）将这些特征映射到类别空间； self.fc1 = nn.Linear(1024, 512) self.fc2 = nn.Linear(512, 256) self.fc3 = nn.Linear(256, k) self.dropout = nn.Dropout(p=0.3) self.bn1 = nn.BatchNorm1d(512) self.bn2 = nn.BatchNorm1d(256) self.relu = nn.ReLU() def forward(self, x): # 它使用 PointNetfeat 提取全局特征（1024维）； x, trans, trans_feat = self.feat(x) # 然后通过全连接层（MLP）将这些特征映射到类别空间； x = F.relu(self.bn1(self.fc1(x))) x = F.relu(self.bn2(self.dropout(self.fc2(x)))) x = self.fc3(x) # 最终输出每个类别的概率分布（log_softmax）； return F.log_softmax(x, dim=1), trans, trans_feat\"]},\"111\":{\"h\":\"分割任务\",\"t\":[\"PointNet 的分割模块 PointNetDenseCls ，它基于前面的特征提取模块 PointNetfeat 来完成点云物体分割任务。\",\"class PointNetDenseCls(nn.Module): def __init__(self, k = 2, feature_transform=False): super(PointNetDenseCls, self).__init__() self.k = k self.feature_transform=feature_transform self.feat = PointNetfeat(global_feat=False, feature_transform=feature_transform) self.conv1 = torch.nn.Conv1d(1088, 512, 1) self.conv2 = torch.nn.Conv1d(512, 256, 1) self.conv3 = torch.nn.Conv1d(256, 128, 1) self.conv4 = torch.nn.Conv1d(128, self.k, 1) self.bn1 = nn.BatchNorm1d(512) self.bn2 = nn.BatchNorm1d(256) self.bn3 = nn.BatchNorm1d(128) def forward(self, x): batchsize = x.size()[0] n_pts = x.size()[2] # 点的数量 # 调用 PointNetfeat 提取特征 # 最后将每个点的局部特征与整个点云的全局特征拼接起来，让每个点都能看到上下文信息 x, trans, trans_feat = self.feat(x) # 使用多层 Conv1D 层进一步融合局部 + 全局信息 # 最终输出 shape: (B, k, N) x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) x = self.conv4(x) # shape: (B, k, N) -> (B, N, k) , 即每个点的各个类别得分 x = x.transpose(2,1).contiguous() # 使用 log_softmax 得到 log 概率分布； x = F.log_softmax(x.view(-1,self.k), dim=-1) # shape: (B*N, k) x = x.view(batchsize, n_pts, self.k) # shape: (B, N, k) return x, trans, trans_feat\",\"✅ 1. 每个点都需要全局上下文\",\"仅靠局部特征很难判断某个点属于哪个部件（比如椅子的腿 vs 座位）；\",\"加上全局特征后，相当于告诉模型：“你知道吗，这是一个椅子”；\",\"这样模型就能根据上下文更准确地做出判断；\",\"✅ 2. 全局特征不能直接用于分割\",\"全局特征只有一份（(B, 1024)），无法直接用于每个点；\",\"所以要把它复制 N 次，变成 (B, 1024, N)；\",\"再与每个点的局部特征拼接；\"]},\"112\":{\"h\":\"缺陷\",\"t\":[\"🧠 一、核心问题：忽略局部结构信息\",\"PointNet 只通过 max pooling 聚合所有点的信息，忽略了局部邻域之间的结构关系。\",\"🔍 原因分析：\",\"PointNet 对每个点独立处理（参数共享），然后使用全局最大池化（Global Max Pooling）提取特征；\",\"这种设计使得网络只关注“最显著的点”，而没有建模点与点之间的局部几何关系；\",\"导致模型无法捕捉到更细粒度的几何细节，比如边缘、曲率、表面纹理等；\",\"💡 论文中的验证：\",\"在部件分割任务中，虽然 PointNet 表现不错，但在一些复杂区域（如椅子腿和桌面连接处）容易出错；\",\"分类任务中对缺失点具有一定鲁棒性，但遇到遮挡严重或点分布不均匀时性能下降明显；\",\"📉 二、分割任务依赖拼接机制，不够精细\",\"PointNet 的分割模块通过拼接全局特征 + 局部特征实现上下文感知，但这种方式表达能力有限。\",\"🔍 原理回顾：\",\"PointNet 的分割网络将全局特征复制 N 次并与每个点的局部特征拼接；\",\"然后使用 Conv1D 进行分类；\",\"实际上是用一个固定大小的全局特征去“广播”给每个点；\",\"⚠️ 问题所在：\",\"全局特征不能很好地反映每个点的上下文；\",\"拼接方式缺乏动态调整机制；\",\"难以区分语义相近但位置不同的区域（如桌子边缘 vs 中心）；\",\"🧱 三、对局部形状变化敏感\",\"PointNet 提取的关键点集合（critical point set）可能不足以代表复杂的局部结构。\",\"🔍 实验观察：\",\"在论文中提到，PointNet 学到的是一个关键点集合，这些点大致构成物体的骨架；\",\"如果这些关键点缺失或被遮挡，即使其他点都在，也可能导致错误分类；\",\"对于非刚性变形（如人体姿态变化），PointNet 的表现不如基于图结构的模型；\",\"📈 四、分类性能略逊于多视角方法\",\"在某些标准数据集（如 ModelNet40）上，PointNet 的分类准确率略低于 MVCNN 等基于图像的方法。\",\"方法\",\"分类准确率\",\"MVCNN（多视角 CNN）\",\"90.1%\",\"VoxNet（体素 CNN）\",\"85.9%\",\"PointNet\",\"89.2%\",\"虽然 PointNet 在速度和效率上占优，但在精度上仍略逊一筹。\",\"🧩 五、难以捕捉非刚性变换下的不变性\",\"PointNet 使用 T-Net 强制学习正交变换矩阵，只能处理刚性变换（旋转、反射），无法处理非刚性形变（如弯曲、拉伸）。\",\"🔍 举例说明：\",\"如果你有一张人脸的点云，由于表情不同，面部发生形变；\",\"PointNet 很难在这种情况下保持分类的一致性；\",\"相比之下，基于图卷积或注意力机制的模型更能捕捉这种非刚性变化；\",\"🧱 六、缺乏层次化特征提取机制\",\"PointNet 是一种单尺度网络，无法像 CNN 那样逐层提取多层次的抽象特征。\",\"✅ 后续改进：\",\"PointNet++ 正是对这一缺陷的改进；\",\"它引入了局部区域搜索 + 多尺度聚合机制；\",\"从而能够更好地捕捉点云的局部结构和层次信息；\",\"📊 七、对稀疏点云敏感\",\"当输入点云非常稀疏时（如只有几十个点），PointNet 的性能会显著下降。\",\"🔍 原因分析：\",\"PointNet 的全局特征来自于 max pooling；\",\"如果点太少，max pooling 得到的特征可能无法覆盖整个物体；\",\"特别是在遮挡严重的情况下，关键点可能丢失；\",\"📐 八、结构简单，不利于高维空间建模\",\"PointNet 的结构过于简单，难以建模更高维度的空间关系。\",\"✅ 后续发展：\",\"后续的 3D 深度学习模型（如 DGCNN、SpiderCNN、PointCNN、Transformer-based 点云模型）都尝试引入更复杂的结构来提升建模能力；\",\"如：构建点之间的邻接图、使用 attention、引入多尺度采样等；\",\"🧪 九、理论上的限制：受限于瓶颈维度 K\",\"PointNet 的表达能力受 max pooling 层维度 K 的限制，即 bottleneck dimension。\",\"📌 来自论文的理论分析：\",\"Theorem 2 表明，PointNet 的输出仅由一个不超过 K 个点的子集决定（critical point set），这意味着：\",\"如果 K 不够大，PointNet 可能遗漏重要细节；\",\"如果 K 太大，又会导致计算资源浪费；\",\"🧱 十、对噪声点敏感（尤其未训练时）\",\"虽然 PointNet 对少量异常点有一定鲁棒性，但如果训练时没有加入扰动，面对大量噪声点时效果较差。\",\"🔍 实验验证：\",\"论文中做了“插入异常点”的实验；\",\"结果显示，如果训练过程中加入了噪声，模型表现良好；\",\"否则，异常点会影响分类和分割性能；\",\"📉 十一、在大规模场景理解任务中表现一般\",\"PointNet 的时间复杂度虽然是 O(N)，但在处理超大规模点云时，仍然不如分块处理或多层级聚合模型高效。\",\"✅ 后续改进方向：\",\"使用分块策略（chunking）\",\"构建点云的层次化表示\",\"引入 attention 或图结构增强局部建模能力\",\"🧩 总结表格：PointNet 的主要缺陷\",\"缺陷类型\",\"描述\",\"是否被后续模型改进\",\"忽略局部结构\",\"仅靠 max pooling 提取特征，无局部聚合机制\",\"✅ PointNet++ 改进\",\"分割精度不高\",\"拼接机制不够精细，缺乏动态上下文感知\",\"✅ Transformer-based 改进\",\"无法处理非刚性变形\",\"T-Net 只学正交变换，无法应对弯曲、拉伸等形变\",\"✅ 图卷积、attention 改进\",\"分类精度略低\",\"在 ModelNet40 上略低于 MVCNN\",\"✅ 多视角 + PointNet 混合模型改进\",\"稀疏点云下性能差\",\"少量点无法覆盖关键结构\",\"✅ PointNet++ 改进\",\"局部建模能力弱\",\"无法捕捉边缘、曲率等细节\",\"✅ DGCNN、SpiderCNN 改进\",\"对噪声点敏感\",\"未经扰动训练时，对异常点鲁棒性差\",\"✅ 加入数据增强后缓解\",\"结构单一\",\"缺乏层次化、多尺度建模能力\",\"✅ PointNet++ / Transformer 改进\",\"📈 PointNet 的优势 vs 缺陷对比\",\"维度\",\"优势\",\"缺陷\",\"输入形式\",\"支持原始点云，无需预处理\",\"无法有效利用局部结构\",\"排列不变性\",\"完全支持\",\"无法区分顺序信息（如时间序列点云）\",\"变换不变性\",\"支持刚性变换标准化\",\"无法处理非刚性形变\",\"分类性能\",\"接近 SOTA\",\"略逊于多视角 CNN\",\"分割性能\",\"表现良好\",\"缺乏精细建模\",\"效率\",\"极其高效（O(N)）\",\"无法充分利用 GPU 并行优化\",\"扩展性\",\"易于扩展为检测、检索等任务\",\"表达能力受限于 max pooling 维度\",\"✅ 一句话总结：\",\"PointNet 的最大缺陷在于它“看不清细节”，只关注全局结构，忽视局部邻域关系，这使得它在细粒度识别、非刚性变形、稀疏点云等任务中表现受限，但它也为后续模型奠定了基础。\"]},\"113\":{\"h\":\"背景知识扫盲(可选)\"},\"114\":{\"h\":\"点云\",\"t\":[\"点云: 是一种表示三维空间中物体或场景的方式，它由大量带有位置信息的点组成。\",\"每个点通常包含：\",\"坐标信息 ：x, y, z（3D 空间中的位置）。\",\"可选属性：颜色（RGB）、法向量（Normal）、强度（Intensity）、时间戳等。\",\"表示形式:\",\"点云（Point Cloud）: 原始点集合：每个点有(x, y, z)坐标; 可选颜色、法向量等属性, 简洁、轻便; 保留原始几何信息,无序性、非结构化、难以用 CNN 处理。\",\"体素网格 (voxel grids) : 将空间划分成立方体格子，每个格子表示是否有物体; 结构规整，适合 3D CNN; 计算复杂度高、稀疏性强、精度受限。\",\"多视角图像（Multi-View Images）: 从多个角度渲染点云或 3D 模型为 2D 图像; 可使用成熟的 2D CNN 方法; 丢失部分几何信息，依赖视角选择。\",\"网格（Mesh）： 由三角形面片组成的 3D 模型； 包含表面细节，适合渲染； 难以自动构建，拓扑复杂。\"]},\"115\":{\"h\":\"对称函数\",\"t\":[\"对称函数（Symmetric Function）是一种对输入顺序不敏感的函数；换句话说，无论你如何打乱输入元素的顺序，输出结果都保持不变。\",\"🧠 数学定义:\",\"设是一个函数，如果对于任意排列（permutation），都有：\",\"那么就是一个 对称函数。\",\"PointNet 处理的是点云数据，而点云是无序集合（unordered set） ，即：\",\"点云中点的顺序不影响整体形状。\",\"所以模型必须具有对点顺序的不变性（permutation invariance）。\",\"这就要求网络中的某些关键操作必须是对称函数 ，才能保证整个网络输出与输入点的顺序无关。\",\"📦 常见的对称函数:\",\"函数\",\"描述\",\"是否可微\",\"应用场景\",\"最大池化（Max Pooling）\",\"取所有点的最大值：\",\"✅ 是\",\"PointNet 中的核心操作\",\"平均池化（Average Pooling）\",\"取所有点的平均值：\",\"✅ 是\",\"特征融合、平滑处理\",\"求和（Summation）\",\"所有点相加：\",\"✅ 是\",\"构建全局特征向量\",\"乘积（Product）\",\"所有点相乘：\",\"⚠️ 对数值变化敏感\",\"不常用，但可用于特定任务\",\"最小池化（Min Pooling）\",\"取最小值：\",\"✅ 是\",\"异常检测等特殊场景\",\"Softmax + 加权和（Attention-based Sum）\",\"根据注意力机制加权求和，权重由 softmax 得出\",\"✅ 是\",\"DGCNN、Transformer 中使用\",\"统计量（如方差、标准差）\",\"计算点集的分布特性\",\"✅ 是\",\"特征增强、异常检测\",\"集合函数近似器（如 Deep Sets）\",\"使用神经网络直接学习对称函数\",\"✅ 是\",\"更复杂的对称函数建模\"]},\"116\":{\"h\":\"刚性运动\",\"t\":[\"刚性运动(rigid motions) 是指：物体在空间中移动时，其形状和大小保持不变的运动方式 。\",\"刚性运动\",\"❌ 不改变\",\"移动、旋转\",\"非刚性运动\",\"✅ 改变\",\"弯曲、拉伸、缩放（非均匀）、变形\",\"刚性运动 = 平移 + 旋转，不改变物体形状和内部结构，只改变位置和朝向。\"]},\"117\":{\"h\":\"正交变换\",\"t\":[\"正交变换的本质是：只改变物体的方向（旋转），不改变形状和大小\",\"所以：\",\"正交变换包括：旋转 + 反射。\",\"不包括：缩放、剪切、拉伸等会导致形变的操作。\"]},\"118\":{\"h\":\"大语言模型\"},\"119\":{\"h\":\"BLIP 论文\",\"t\":[\"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 论文解读\",\"论文链接: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 代码链接: https://github.com/salesforce/BLIP\"]},\"120\":{\"h\":\"Introduction\",\"t\":[\"当前视觉-语言预训练（VLP）方法虽然在多模态任务上取得进展，但普遍存在两个问题：\",\"模型限制：编码器模型不适合文本生成任务；编码器-解码器模型难以用于图文检索。\",\"数据质量差：大多使用从网络收集的嘈杂图文对作为训练数据，监督信号不理想。\",\"BLIP（Bootstrapping Language-Image Pre-training）是一个新颖的 VLP 框架，兼顾理解与生成能力。其两大创新点：\",\"MED 模型结构（Multimodal Mixture of Encoder-Decoder）：\",\"同时支持编码器、图像条件编码器、图像条件解码器三种模式。\",\"联合训练三种任务：图文对比学习、图文匹配、图像条件语言建模。\",\"实现多任务预训练与灵活迁移。\",\"CapFilt 数据自举方法（Captioning and Filtering）：\",\"使用训练好的 MED 模型构建两个模块：\",\"描述器（captioner）生成图像的合成描述；\",\"过滤器（filter）剔除原始和生成的低质量描述。\",\"在保留信息的同时提升训练数据质量。\",\"实验结果与表现:\",\"BLIP 在多个任务（图文检索、图像描述、VQA 等）上取得最先进性能。\",\"同时，在两个视频-语言任务上以零样本方式迁移也表现优异。\",\"实验证明：描述器与过滤器的组合能显著提升性能，多样化描述更有利于学习。\"]},\"121\":{\"h\":\"Related Work\"},\"122\":{\"h\":\"视觉-语言预训练（VLP）\",\"t\":[\"现状问题：\",\"主流 VLP 方法依赖从网络抓取的图文对数据，虽然规模大，但包含大量噪声文本。\",\"尽管使用简单的过滤规则，噪声仍广泛存在。\",\"编码器模型适合理解类任务但难以生成文本；编码器-解码器适合生成任务但不适用于检索。\",\"BLIP 的改进：\",\"提出 CapFilt：通过“生成 + 过滤”的方式优化数据质量。\",\"提出 MED 模型结构：在保持预训练高效的前提下，同时兼顾理解与生成任务，提升泛化能力。\"]},\"123\":{\"h\":\"知识蒸馏（Knowledge Distillation）\",\"t\":[\"现有做法：\",\"知识蒸馏让小模型（学生）学习大模型（教师）的预测结果。\",\"自蒸馏也取得了不错效果，尤其在图像分类与部分 VLP 方法中已开始尝试。\",\"BLIP 的新视角：\",\"CapFilt 可视为一种结构化的知识蒸馏方式：\",\"Captioner 模块用生成的语义丰富描述进行蒸馏；\",\"Filter 模块通过剔除噪声文本完成隐式知识过滤。\"]},\"124\":{\"h\":\"数据增强（Data Augmentation）\",\"t\":[\"现有做法：\",\"图像任务中数据增强广泛应用，但语言任务的数据增强较困难。\",\"近年来生成模型被用于文本任务的样本合成，但多用于低资源语言场景。\",\"BLIP 的贡献：\",\"展示了在大规模视觉-语言预训练中使用合成图像描述的独特优势，提升了多模态学习效果。\"]},\"125\":{\"h\":\"Method\",\"t\":[\"在本文中，作者提出了一个统一的视觉语言预训练框架 BLIP，该方法能够有效从噪声图文对中学习。方法部分主要分为三个内容：模型架构 MED、预训练目标，以及数据集自举策略 CapFilt。\"]},\"126\":{\"h\":\"模型架构：MED（Multimodal Mixture of Encoder-Decoder）\",\"t\":[\"作者在模型中采用了 视觉 Transformer（ViT）（Dosovitskiy et al., 2021）作为图像编码器，它会将图像切分为 patch，并编码为一系列嵌入表示，其中额外添加的 [CLS] token 被用作图像的全局特征。相比使用预训练目标检测器（如 Chen et al., 2020），ViT 的使用更具计算效率，也被近年来的工作所采纳（Li et al., 2021a；Kim et al., 2021）。\",\"为了训练一个既具理解能力又具生成能力的统一模型，作者提出了一个多任务模型架构：多模态混合的编码器-解码器（MED）。该架构支持以下三种功能模式：\",\"Unimodal Encoder（单模态编码器）\",\"图像和文本分别编码。\",\"文本编码器使用 BERT（Devlin et al., 2019），在文本前添加 [CLS] token 来表示整句话的语义。\",\"Image-grounded Text Encoder（图像引导的文本编码器）\",\"在 BERT 的每个 block 中添加一层 cross-attention（CA），位于 self-attention（SA）和 FFN 之间。\",\"文本末尾添加一个 [Encode] token，输出的该 token 表示图文对的多模态表示。\",\"Image-grounded Text Decoder（图像引导的文本解码器）\",\"在 encoder 的基础上将双向 SA 替换为因果 self-attention，以实现生成任务。\",\"使用 [Decode] token 标记序列开始，使用 <eos> 表示结束。\",\"为了实现多任务高效预训练，模型设计上 encoder 和 decoder 共享除了 SA 层以外的所有参数。作者认为编码和解码之间的主要差异体现在 SA 层（前者为双向，后者为因果），而嵌入层、CA 和 FFN 层则可以共享。这样的共享设计能够提升训练效率，并有利于多任务学习。\"]},\"127\":{\"h\":\"预训练目标（ITC、ITM、LM）\",\"t\":[\"BLIP 同时优化三个预训练目标，其中包括两个理解任务和一个生成任务。每对图文样本只需通过一次视觉 Transformer，但文本 Transformer 会根据不同功能分支前向三次，以计算以下三种损失：\",\"Image-Text Contrastive Loss（ITC）\",\"启用 Unimodal Encoder，用于对齐图像和文本的表示空间。\",\"目标是使正样本的图文对在特征空间中接近，同时区分负样本。\",\"方法参考 Li et al. (2021a)，采用动量编码器来生成表示，并用其产生的软标签作为训练目标，从而考虑负样本中的潜在正样本。\",\"Image-Text Matching Loss（ITM）\",\"启用 Image-grounded Text Encoder，用于学习图文之间的细粒度对齐关系。\",\"本质是一个二分类任务，判断图文是否匹配，使用一个线性分类头进行预测。\",\"采用 Li et al. (2021a) 提出的 困难负样本挖掘策略，即在 batch 中优先选择对比相似度高的负样本来增强训练信号。\",\"Language Modeling Loss（LM）\",\"启用 Image-grounded Text Decoder，训练模型根据图像生成文本描述。\",\"使用交叉熵损失，训练模型以自回归方式生成文本；计算时引入 0.1 的标签平滑。\",\"相比于传统的 MLM，LM 更能增强模型将视觉信息转化为自然语言的能力。\"]},\"128\":{\"h\":\"CapFilt：图文数据的自举式清洗机制\",\"t\":[\"由于高质量人工标注图文对（如 COCO）数量有限，而自动爬取的网页图文对（）噪声严重，作者提出了一个新的数据处理流程：CapFilt（Captioning and Filtering），用于提升图文语料的质量。\",\"CapFilt 包含两个模块，分别用于生成和过滤文本：\",\"Captioner\",\"是一个图像引导的文本解码器，使用 LM 目标在 COCO 上轻量微调。\",\"输入网页图像 ，输出合成描述 （一张图对应一条生成的 caption）。\",\"Filter\",\"是一个图像引导的文本编码器，使用 ITC 和 ITM 目标进行微调。\",\"输入图像及文本，判断是否匹配；若 ITM 头预测为不匹配，则认为是噪声文本。\",\"过滤对象既包括网页原始文本 ，也包括合成描述 。\",\"最后，作者将通过 Filter 筛选出的图文对与人工标注数据结合，组成新的训练集，用于训练下一个更强的模型。\"]},\"129\":{\"h\":\"小结\",\"t\":[\"BLIP 在模型设计上提出了一个灵活、统一的多任务架构（MED），同时结合图文对比、匹配和生成等任务目标进行联合训练。在数据方面，通过 CapFilt 机制有效从网页图文对中挖掘高质量样本，显著扩展了训练数据的规模和质量。\",\"整体来看，BLIP 是一个兼顾理解与生成、统一建模与数据增强的多模态预训练方法，具有较强的实用性与拓展性。\",\"以下是对“Experiments and Discussions”部分翻译内容的总结，遵循您的格式规范：\"]},\"130\":{\"h\":\"Experiments and Discussions\"},\"131\":{\"h\":\"预训练细节\",\"t\":[\"BLIP 模型使用 PyTorch 实现，预训练环境为两个16-GPU节点。图像编码器基于在 ImageNet 上预训练的 ViT（参考 Touvron et al., 2020；Dosovitskiy et al., 2021），文本编码器则基于 BERTbase（Devlin et al., 2019）。模型变体包括 ViT-B/16 和 ViT-L/16，其中默认使用 ViT-B。\",\"训练配置如下：\",\"训练轮数为 20，批量大小为 2880（ViT-B）/ 2400（ViT-L）；\",\"优化器使用 AdamW（Loshchilov & Hutter, 2017），权重衰减为 0.05，学习率预热后分别达到 （ViT-B）和 （ViT-L），随后线性衰减；\",\"图像预训练分辨率为 ，微调时提升为 。\",\"所使用的训练数据总共包含约 1400 万张图像，覆盖以下数据集：\",\"COCO 和 Visual Genome（人工标注）；\",\"Conceptual Captions、Conceptual 12M 和 SBU Captions（网页收集）；\",\"补充实验中还使用了 LAION（Schuhmann et al., 2021）的大规模网页数据集（1.15 亿图像），每轮只使用 1/5 数据。\"]},\"132\":{\"h\":\"CapFilt 效果验证\",\"t\":[\"CapFilt 模块通过 Captioner 生成合成文本，再由 Filter 去除噪声文本，显著提升模型性能（表1）。三种设置下的对比表明：\",\"单独使用 Captioner 或 Filter 均有性能提升；\",\"两者联合使用时效果最佳，且具备数据量和模型规模的可扩展性；\",\"更强的视觉主干（如 ViT-L）可进一步增强性能。\",\"图4 展示了网页原始文本（）与合成文本（）的对比，绿色为 Filter 接收的文本，红色为剔除的文本，验证了 Captioner 提供新描述、Filter 移除无效数据的有效 性。\"]},\"133\":{\"h\":\"合成文本的多样性对性能的影响\",\"t\":[\"表2 比较了两种文本生成方式：\",\"Beam search：确定性搜索，噪声比例较低（19%）；\",\"Nucleus sampling：随机采样，噪声比例稍高（25%），但性能全面超越 Beam search。\",\"作者推测，nucleus sampling 生成的文本更具有 新颖性与多样性，提供更多额外信息；而 beam search 更倾向于生成数据集中常见的“安全”文本，难以提升模型泛化能力。\"]},\"134\":{\"h\":\"编码器-解码器参数共享与解耦\",\"t\":[\"表3 分析了不同的参数共享策略对模型性能的影响。结论如下：\",\"最佳方案是 仅在 SA 层不共享参数，其余部分共享；\",\"如果完全不共享参数，则模型体积大、性能次优；\",\"如果共享 SA 层，性能反而下降，因其在编码（双向注意力）与解码（因果注意力）间存在功能冲突。\",\"在 CapFilt 阶段，Captioner 与 Filter 分别进行微调。表4 显示，如果二者参数共享，则会因“确认偏差”导致性能下降 —— 生成的错误文本更难被 Filter 剔除（噪声比例仅 8%，远低于解耦时的 25%）。\",\"以下是论文 BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 中第 6章 Additional Ablation Study 和第 7章 Conclusion 部分的翻译：\"]},\"135\":{\"h\":\"Ablation Study\"},\"136\":{\"h\":\"CapFilt 的性能提升并非源于更长的训练时间\",\"t\":[\"由于经过 CapFilt 处理后的数据集包含比原始数据集更多的文本，因此在相同的 epoch 数下，训练时间会更长。为了验证 CapFilt 的有效性是否真正来自其机制本身，而非训练时间变长，我们将原始数据集中的网页文本复制，使得每个 epoch 的样本数量与 bootstrapped 数据集一致。\",\"如表 12 所示，仅通过扩充原始数据进行更长时间训练 并未带来性能提升，验证了 CapFilt 的真正价值。\"]},\"137\":{\"h\":\"应使用 Bootstrapped 数据集重新训练模型\",\"t\":[\"Bootstrapped 数据集应当用于重新训练一个新模型。我们对比了两种方式：\",\"在预训练模型基础上继续使用 CapFilt 数据训练；\",\"用 CapFilt 数据从头训练一个新模型。\",\"表 13 显示，继续训练的效果不如重新训练，这与知识蒸馏领域的常见做法一致：学生模型不能由教师模型直接初始化。这也间接说明 CapFilt 机制与重新初始化更契合。\"]},\"138\":{\"h\":\"Conclusion\",\"t\":[\"BLIP 是一个新的视觉-语言预训练框架，在众多下游任务中都实现了最先进（SOTA）性能，包括理解类任务和生成类任务。\",\"BLIP 使用多模态混合的编码器-解码器模型（Multimodal Mixture of Encoder-Decoder, MED），并通过对大规模嘈杂图文数据进行 bootstrapping 构建预训练语料：注入多样的合成描述，并剔除低质量描述。\",\"发布了 bootstrapped 数据集，以促进视觉-语言研究的发展。\",\"未来可进一步探索以下方向以提升 BLIP 表现：\",\"多轮数据集 Bootstrapping；\",\"为每张图像生成 多个合成描述，进一步扩大语料规模；\",\"使用多个不同的 Captioner 和 Filter 训练多个模型，再进行集成，增强 CapFilt 的效果。\"]},\"139\":{\"h\":\"Code Implementation\"},\"140\":{\"h\":\"CapFilt 模块实现\",\"t\":[\"BLIP 使用 CapFilt 对多个大规模噪声网页图文数据集（包括 CC12M、CC3M 和 SBU Captions）进行增强，首先通过 captioner 为图像生成合成文本，再通过 filter 过滤掉与图像不匹配的原始和合成文本，最终构建出高质量的自举数据集（bootstrapped dataset），用于预训练新模型。\",\"在 CapFilt 模块微调阶段，BLIP 则基于高质量人工标注的数据集如 COCO Captions、Visual Genome 和 Flickr30K 进行训练和评估。\",\"经过 CapFilt 处理后，输出的数据集是经过图文对齐质量优化的图文对集合，有效提升了下游任务中的表现。\",\"官方代码库并没有非常清晰指明CapFilt模块的实现代码位置，但是官方仓库的ISSUE给出了明确答复: https://github.com/salesforce/BLIP/issues/86?utm_source=chatgpt.com\"]},\"141\":{\"h\":\"Captioner 模块\"},\"142\":{\"h\":\"微调阶段\",\"t\":[\"Captioner 基于 Coco 数据集进行微调:\",\"# 训练函数：执行一个 epoch 的训练流程 def train(model, data_loader, optimizer, device): for i, (image, caption, _) in data_loader: loss = model(image, caption) # 前向传播，计算语言建模损失 optimizer.zero_grad() # 清除旧梯度 loss.backward() # 反向传播 optimizer.step() # 更新模型参数 # 主流程：加载数据、初始化模型和优化器、执行多轮训练 def main(args, config): #### Dataset #### # 加载 COCO Caption 数据集的训练/验证/测试划分 train_dataset, val_dataset, test_dataset = create_dataset('caption_coco', config) # 构造对应的数据加载器 train_loader, val_loader, test_loader = create_loader( [train_dataset, val_dataset, test_dataset], samplers, batch_size=[config['batch_size']]*3, num_workers=[4, 4, 4], is_trains=[True, False, False], collate_fns=[None, None, None] ) #### Model #### # 初始化 BLIP 解码器模型（用于图像字幕生成），加载预训练视觉编码器与文本解码器 model = blip_decoder( pretrained=config['pretrained'], image_size=config['image_size'], vit=config['vit'], vit_grad_ckpt=config['vit_grad_ckpt'], vit_ckpt_layer=config['vit_ckpt_layer'], prompt=config['prompt'] ) # 初始化优化器（AdamW） optimizer = torch.optim.AdamW( params=model.parameters(), lr=config['init_lr'], weight_decay=config['weight_decay'] ) #### Train #### for epoch in range(0, config['max_epoch']): # 每个 epoch 执行一次训练 train_stats = train(model, train_loader, optimizer, epoch, device)\",\"训练阶段唯一需要注意的一点就是数据集的构造过程中，会在Coco数据集每个样本原有Caption的基础上添加一个Prompt:\",\"class coco_karpathy_train(Dataset): def __getitem__(self, index): ann = self.annotation[index] image_path = os.path.join(self.image_root,ann['image']) image = Image.open(image_path).convert('RGB') image = self.transform(image) # 在caption前添加prompt , prompt 默认为 'a picture of ' caption = self.prompt+pre_caption(ann['caption'], self.max_words) # self.img_ids[ann['image_id']]: 取出图像 ID 对应的 索引编号 return image, caption, self.img_ids[ann['image_id']]\",\"下面将给出Captioner模块基于Coco数据集，采用 next token predict 方法进行训练的代码实现：\",\"class BLIP_Decoder(nn.Module): def __init__(self, med_config = 'configs/med_config.json', image_size = 384, vit = 'base', vit_grad_ckpt = False, vit_ckpt_layer = 0, prompt = 'a picture of ', ): \\\"\\\"\\\" BLIP Captioner模块初始化，实现论文中提出的图像-文本跨模态编码器-解码器架构 Args: med_config (str): 混合编码器-解码器模型配置文件路径，对应论文3.1节中提到的多模态融合模块配置 image_size (int): 输入图像尺寸，论文4.1节实验设置中使用384x384 vit (str): 视觉Transformer模型大小，论文中采用ViT-Base作为默认视觉编码器 vit_grad_ckpt (bool): 是否使用梯度检查点优化ViT显存占用，论文附录A中提到的训练优化策略 vit_ckpt_layer (int): ViT梯度检查点层数，用于平衡训练效率与显存使用 prompt (str): 图像描述生成的引导提示词，对应论文3.2节中使用的prompt engineering技术 \\\"\\\"\\\" super().__init__() self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer) # 初始化视觉编码器，对应论文图1中的视觉Transformer self.tokenizer = init_tokenizer() # 初始化文本分词器，采用BERT分词器实现论文中的文本预处理 med_config = BertConfig.from_json_file(med_config) med_config.encoder_width = vision_width self.text_decoder = BertLMHeadModel(config=med_config) # 初始化文本解码器，实现论文3.1节中的跨模态解码器 self.prompt = prompt # 存储图像描述引导提示词，用于论文3.3节中的条件生成任务 self.prompt_length = len(self.tokenizer(self.prompt).input_ids)-1 # 计算提示词token长度，用于后续解码时区分提示与生成文本 def forward(self, image, caption): # 提取图像特征表示 image_embeds = self.visual_encoder(image) image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device) # 编码文本输入，并替换开头 token 为 [BOS] text = self.tokenizer(caption, padding='longest', truncation=True, max_length=40, return_tensors=\\\"pt\\\").to(image.device) text.input_ids[:, 0] = self.tokenizer.bos_token_id # 构建语言建模标签：屏蔽掉 padding 和 prompt 部分 decoder_targets = text.input_ids.masked_fill(text.input_ids == self.tokenizer.pad_token_id, -100) decoder_targets[:, :self.prompt_length] = -100 # 调用跨模态解码器，执行语言建模训练 decoder_output = self.text_decoder( text.input_ids, attention_mask=text.attention_mask, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, labels=decoder_targets, return_dict=True ) loss_lm = decoder_output.loss # 提取语言建模损失 return loss_lm\",\"BertLMHeadModel自回归语言建模实现\"]},\"143\":{\"h\":\"生成阶段\",\"t\":[\"当 Captioner 模块在 Coco 数据集上进行训练后，即可用于生成图像描述。\",\"def generate(self, image, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0): # 通过视觉编码器提取图像的视觉特征（image embeddings） image_embeds = self.visual_encoder(image) if not sample: # 如果使用 beam search 生成，则需要将图像特征复制 num_beams 份 # 这是为了每个 beam 都能接收相同的图像信息 image_embeds = image_embeds.repeat_interleave(num_beams, dim=0) # 构造图像 attention mask，全为 1，表示图像特征没有被 mask image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device) # 构造 encoder-decoder 所需的关键词参数（即图像特征作为 cross-attention 的条件输入） model_kwargs = { \\\"encoder_hidden_states\\\": image_embeds, \\\"encoder_attention_mask\\\": image_atts } # 构造输入的 prompt，格式为 [\\\"a picture of \\\", \\\"a picture of \\\", ...] prompt = [self.prompt] * image.size(0) # 对 prompt 进行分词并转为 tensor，作为 decoder 的输入起点（input_ids） input_ids = self.tokenizer(prompt, return_tensors=\\\"pt\\\").input_ids.to(image.device) # 强制将每个样本开头的 token 设为 [BOS]（beginning-of-sentence） input_ids[:, 0] = self.tokenizer.bos_token_id # 移除最后一个 token（保持 prompt 是 decoder 的 prefix） input_ids = input_ids[:, :-1] if sample: # ---------- 使用 nucleus sampling（核采样）生成 ---------- outputs = self.text_decoder.generate( input_ids=input_ids, max_length=max_length, min_length=min_length, do_sample=True, # 启用采样 top_p=top_p, # nucleus 采样的阈值 num_return_sequences=1, # 每张图像生成一个序列 eos_token_id=self.tokenizer.sep_token_id, # 使用 [SEP] 作为结束标记 pad_token_id=self.tokenizer.pad_token_id, # 使用 [PAD] 作为 padding repetition_penalty=1.1, # 防止重复生成 **model_kwargs # 传入图像编码信息 ) else: # ---------- 使用 beam search（束搜索）生成 ---------- outputs = self.text_decoder.generate( input_ids=input_ids, max_length=max_length, min_length=min_length, num_beams=num_beams, # beam 数量 eos_token_id=self.tokenizer.sep_token_id, pad_token_id=self.tokenizer.pad_token_id, repetition_penalty=repetition_penalty, # 重复惩罚项 **model_kwargs ) # ---------- 解码生成的 token 序列为文本 ---------- captions = [] for output in outputs: caption = self.tokenizer.decode(output, skip_special_tokens=True) # 去掉 prompt 的前缀，只保留生成部分 captions.append(caption[len(self.prompt):]) return captions\"]},\"144\":{\"h\":\"Filter 模块\"},\"145\":{\"h\":\"微调阶段\",\"t\":[\"Filter 模块同样也是基于 Coco 数据集进行微调，但采用图文检索和图文匹配作为训练目标:\",\"def train(model, data_loader, optimizer, epoch, device, config): for i,(image, caption, idx) in data_loader: # 采用Moco的动量慢更新策略进行学习 if epoch>0: alpha = config['alpha'] else: alpha = config['alpha']*min(1,i/len(data_loader)) # idx 是每个图像对应的索引编号 loss_ita, loss_itm = model(image, caption, alpha=alpha, idx=idx) loss = loss_ita + loss_itm optimizer.zero_grad() loss.backward() optimizer.step() def main(args, config): #### Dataset #### # 数据集处理方面，同样会为Coco数据集中每个样本的Caption前添加固定长度的Prompt: 'a picture of' train_dataset, val_dataset, test_dataset = create_dataset('retrieval_%s'%config['dataset'], config) train_loader, val_loader, test_loader = create_loader([train_dataset, val_dataset, test_dataset],samplers, batch_size=[config['batch_size_train']]+[config['batch_size_test']]*2, num_workers=[4,4,4], is_trains=[True, False, False], collate_fns=[None,None,None]) #### Model #### model = blip_retrieval(pretrained=config['pretrained'], image_size=config['image_size'], vit=config['vit'], vit_grad_ckpt=config['vit_grad_ckpt'], vit_ckpt_layer=config['vit_ckpt_layer'], queue_size=config['queue_size'], negative_all_rank=config['negative_all_rank']) optimizer = torch.optim.AdamW(params=model.parameters(), lr=config['init_lr'], weight_decay=config['weight_decay']) for epoch in range(0, config['max_epoch']): train(model, train_loader, optimizer, epoch, device, config)\",\"训练过程代码实现基本遵循Moco论文中所提出的动量慢更新对比学习代码实现，下面先给出 BLIP_Retrieval 模型 init 方法实现:\",\"class BLIP_Retrieval(nn.Module): def __init__(self, med_config='configs/med_config.json', image_size=384, vit='base', vit_grad_ckpt=False, vit_ckpt_layer=0, embed_dim=256, queue_size=57600, momentum=0.995, negative_all_rank=False): # 初始化视觉编码器 self.visual_encoder, vision_width = create_vit(vit, image_size, vit_grad_ckpt, vit_ckpt_layer) # 初始化文本编码器和分词器 self.tokenizer = init_tokenizer() self.text_encoder = BertModel(config=med_config, add_pooling_layer=False) # 特征投影层（用于对比学习） self.vision_proj = nn.Linear(vision_width, embed_dim) self.text_proj = nn.Linear(text_width, embed_dim) # ITM分类头（判断图文是否匹配） self.itm_head = nn.Linear(text_width, 2) # 创建动量编码器（momentum encoder） self.visual_encoder_m, vision_width = create_vit(vit, image_size) self.vision_proj_m = nn.Linear(vision_width, embed_dim) self.text_encoder_m = BertModel(config=med_config, add_pooling_layer=False) self.text_proj_m = nn.Linear(text_width, embed_dim) # 配对编码器用于同步参数 self.model_pairs = [[self.visual_encoder, self.visual_encoder_m], [self.vision_proj, self.vision_proj_m], [self.text_encoder, self.text_encoder_m], [self.text_proj, self.text_proj_m]] self.copy_params() # 初始化对比学习的负样本队列 self.register_buffer(\\\"image_queue\\\", torch.randn(embed_dim, queue_size)) self.register_buffer(\\\"text_queue\\\", torch.randn(embed_dim, queue_size)) self.register_buffer(\\\"idx_queue\\\", torch.full((1, queue_size), -100)) self.register_buffer(\\\"ptr_queue\\\", torch.zeros(1, dtype=torch.long)) self.image_queue = nn.functional.normalize(self.image_queue, dim=0) self.text_queue = nn.functional.normalize(self.text_queue, dim=0) self.queue_size = queue_size self.momentum = momentum self.temp = nn.Parameter(0.07 * torch.ones([])) # 对比学习温度参数 self.negative_all_rank = negative_all_rank\",\"前向传播过程主要是为了计算两个训练目标的损失:\",\"图文对比目标（ITC）\",\"图文匹配目标（ITM）\",\"代码整体流程比较长，我们切分为多个步骤进行解析:\",\"提取图像和文本特征\",\" def forward(self, image, caption, alpha, idx): # 图像特征提取和投影 # image: (B, 3, H, W) -> image_embeds: (B, N, D) image_embeds = self.visual_encoder(image) image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device) # (B, N) image_feat = F.normalize(self.vision_proj(image_embeds[:, 0, :]), dim=-1) # 只取CLS Token做投影: (B, D_proj) # 文本特征提取和投影 text = self.tokenizer(caption, padding='max_length', truncation=True, max_length=35, return_tensors=\\\"pt\\\").to(image.device) # text.input_ids: (B, L) text_output = self.text_encoder(text.input_ids, attention_mask=text.attention_mask, return_dict=True, mode='text') text_feat = F.normalize(self.text_proj(text_output.last_hidden_state[:, 0, :]), dim=-1) # 只取CLS Token做投影: (B, D_proj)\",\"构造图文匹配矩阵 (Target)\",\" # 构造图文匹配矩阵 idx = idx.view(-1, 1) # (B, 1) idx_all = torch.cat([idx.t(), self.idx_queue.clone().detach()], dim=1) # (1, B + Q) pos_idx = torch.eq(idx, idx_all).float() # (B, B + Q) sim_targets = pos_idx / pos_idx.sum(1, keepdim=True) # (B, B + Q)\",\"这里需要和 MoCo 论文实现进行区分，MoCo 中采用的是每个 query（图像）只对应一个 positive（key），因此正负样本是 one-hot 编码，contrastive loss 是严格的一对一；而在 BLIP 中，由于图文对来自自然语言描述，可能存在多个正样本（即同一个图像可以有多个 caption），而且动量队列可能重复包含同一个样本（multi-hot），因此这里构造 sim_targets 时不是用 one-hot，而是通过 pos_idx 判断当前图文对与队列中哪些样本是正对（idx 相等），然后用行归一化将多个正样本平均分配权重，形成 soft target 分布，从而使对比学习更加稳健。\",\"举例:\",\"# 1. 环境 idx = [[7], [13], [20]] # B=3 idx_queue = [1, 7, 5, 13, 9, 30] # Q=6 idx_all = [7, 13, 20, 1, 7, 5, 13, 9, 30] # shape: (1, 9) # 2. 构造图文匹配矩阵 pos_idx = torch.eq(idx, idx_all) # 第1行: [1, 0, 0, 0, 1, 0, 0, 0, 0] # 第2行: [0, 1, 0, 0, 0, 0, 1, 0, 0] # 第3行: [0, 0, 1, 0, 0, 0, 0, 0, 0] # 3. 进行归一化 # 第1行: [0.5, 0, 0, 0, 0.5, 0, 0, 0, 0] # 第2行: [0, 0.5, 0, 0, 0, 0, 0.5, 0, 0] # 第3行: [0, 0, 1.0, 0, 0, 0, 0, 0, 0]\",\"动量慢更新 + 软标签计算\",\" # 使用动量编码器获取特征 with torch.no_grad(): self._momentum_update() image_embeds_m = self.visual_encoder_m(image) # (B, N, D) image_feat_m = F.normalize(self.vision_proj_m(image_embeds_m[:, 0, :]), dim=-1) # (B, D_proj) image_feat_m_all = torch.cat([image_feat_m.t(), self.image_queue.clone().detach()], dim=1) # (D_proj, B + Q) text_output_m = self.text_encoder_m(text.input_ids, attention_mask=text.attention_mask,return_dict=True, mode='text') text_feat_m = F.normalize(self.text_proj_m(text_output_m.last_hidden_state[:, 0, :]), dim=-1) # (B, D_proj) text_feat_m_all = torch.cat([text_feat_m.t(), self.text_queue.clone().detach()], dim=1) # (D_proj, B + Q) sim_i2t_m = image_feat_m @ text_feat_m_all / self.temp # (B, B + Q) sim_t2i_m = text_feat_m @ image_feat_m_all / self.temp # (B, B + Q) sim_i2t_targets = alpha * F.softmax(sim_i2t_m, dim=1) + (1 - alpha) * sim_targets # (B, B + Q) sim_t2i_targets = alpha * F.softmax(sim_t2i_m, dim=1) + (1 - alpha) * sim_targets # (B, B + Q)\",\"计算ITC损失 + 更新动量队列\",\" # 当前 batch 与动量队列的相似度 sim_i2t = image_feat @ text_feat_m_all / self.temp # (B, B + Q) sim_t2i = text_feat @ image_feat_m_all / self.temp # (B, B + Q) # 对比损失（InfoNCE） loss_i2t = -torch.sum(F.log_softmax(sim_i2t, dim=1) * sim_i2t_targets, dim=1).mean() loss_t2i = -torch.sum(F.log_softmax(sim_t2i, dim=1) * sim_t2i_targets, dim=1).mean() loss_ita = (loss_i2t + loss_t2i) / 2 # 更新队列 idxs = concat_all_gather(idx) # (B*, 1) self._dequeue_and_enqueue(image_feat_m, text_feat_m, idxs)\",\"正样本编码 + 难负样本采样\",\" ### 图文匹配任务 (ITM) ### encoder_input_ids = text.input_ids.clone() # (B, L) encoder_input_ids[:, 0] = self.tokenizer.enc_token_id bs = image.size(0) # 正样本编码 output_pos = self.text_encoder(encoder_input_ids, attention_mask=text.attention_mask, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, return_dict=True) # last_hidden_state: (B, L, D) # 采样难负样本（是否跨 GPU） if self.negative_all_rank: # 跨GPU部分实现，自信看源码进行学习 else: with torch.no_grad(): mask = torch.eq(idx, idx.t()) sim_i2t = image_feat @ text_feat.t() / self.temp sim_t2i = text_feat @ image_feat.t() / self.temp weights_i2t = F.softmax(sim_i2t,dim=1) weights_i2t.masked_fill_(mask, 0) weights_t2i = F.softmax(sim_t2i,dim=1) weights_t2i.masked_fill_(mask, 0) # select a negative image (from same rank) for each text image_embeds_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_t2i[b], 1).item() image_embeds_neg.append(image_embeds[neg_idx]) image_embeds_neg = torch.stack(image_embeds_neg,dim=0) # select a negative text (from same rank) for each image text_ids_neg = [] text_atts_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_i2t[b], 1).item() text_ids_neg.append(encoder_input_ids[neg_idx]) text_atts_neg.append(text.attention_mask[neg_idx]) text_ids_neg = torch.stack(text_ids_neg, dim=0) # (B, L) text_atts_neg = torch.stack(text_atts_neg, dim=0) # (B, L)\",\"idx 是当前 GPU 上 本地批次（batch）的样本索引（shape: (B, 1)）。\",\"idxs 是通过 concat_all_gather(idx) 得到的 所有 GPU 上所有样本索引的集合（shape: (total_batch_size, 1)）。\",\"构造两组负样本\",\" # [正样本，负样本] text_ids_all = torch.cat([encoder_input_ids, text_ids_neg], dim=0) # (2B, L) text_atts_all = torch.cat([text.attention_mask, text_atts_neg], dim=0) # (2B, L) # [负样本，正样本] image_embeds_all = torch.cat([image_embeds_neg, image_embeds], dim=0) # (2B, N, D) image_atts_all = torch.cat([image_atts, image_atts], dim=0) # (2B, N) # 两组负样本编码 output_neg = self.text_encoder(text_ids_all, attention_mask=text_atts_all, encoder_hidden_states=image_embeds_all, encoder_attention_mask=image_atts_all, return_dict=True)\",\"计算ITM损失 + 返回ITC损失和ITM损失\",\" # ITM 分类损失 vl_embeddings = torch.cat([output_pos.last_hidden_state[:, 0, :], output_neg.last_hidden_state[:, 0, :]], dim=0) # (3B, D) vl_output = self.itm_head(vl_embeddings) # (3B, 2) itm_labels = torch.cat([torch.ones(bs, dtype=torch.long), torch.zeros(2 * bs, dtype=torch.long)], dim=0).to(image.device) # (3B,) loss_itm = F.cross_entropy(vl_output, itm_labels) return loss_ita, loss_itm\"]},\"146\":{\"h\":\"过滤阶段\",\"t\":[\"当Filter模块在Coco数据集上，采用ITC和ITM目标执行完微调后，便得到了模态对齐好的图像编码器Vit 和 文本编码器Bert ， 然后我们便可以直接用训练好的Vit和Bert来做图文匹配和图文相似度计算了。\",\"class BLIP_ITM(nn.Module): def __init__(self, med_config = 'configs/med_config.json', image_size = 384, vit = 'base', vit_grad_ckpt = False, vit_ckpt_layer = 0, embed_dim = 256, ): self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer) self.tokenizer = init_tokenizer() self.text_encoder = BertModel(config=med_config, add_pooling_layer=False) self.vision_proj = nn.Linear(vision_width, embed_dim) self.text_proj = nn.Linear(text_width, embed_dim) self.itm_head = nn.Linear(text_width, 2) def forward(self, image, caption, match_head='itm'): image_embeds = self.visual_encoder(image) image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device) text = self.tokenizer(caption, padding='max_length', truncation=True, max_length=35, return_tensors=\\\"pt\\\").to(image.device) if match_head=='itm': output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask, encoder_hidden_states = image_embeds, encoder_attention_mask = image_atts, return_dict = True, ) itm_output = self.itm_head(output.last_hidden_state[:,0,:]) return itm_output elif match_head=='itc': text_output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask, return_dict = True, mode = 'text') image_feat = F.normalize(self.vision_proj(image_embeds[:,0,:]),dim=-1) text_feat = F.normalize(self.text_proj(text_output.last_hidden_state[:,0,:]),dim=-1) sim = image_feat @ text_feat.t() return sim\"]},\"147\":{\"h\":\"BLIP 预训练\",\"t\":[\"BLIP 模型基于 CapFilt 模块增强后的数据集上，采用ITC，ITM，LM三个目标进行训练，以下首先给出的是 BLIP 模型的训练代码:\",\"def train(model, data_loader, optimizer, epoch, device, config): for i, (image, caption) in data_loader: optimizer.zero_grad() # ramp up alpha in the first 2 epochs alpha = config['alpha']*min(1,(epoch*len(data_loader)+i)/(2*len(data_loader))) loss_ita, loss_itm, loss_lm = model(image, caption, alpha = alpha) loss = loss_ita + loss_itm + loss_lm loss.backward() optimizer.step() def main(args, config): #### Dataset #### datasets = [create_dataset('pretrain', config, min_scale=0.2)] # 返回的caption前不添加prompt data_loader = create_loader(datasets,samplers,batch_size=[config['batch_size']], num_workers=[4], is_trains=[True], collate_fns=[None])[0] #### Model #### model = blip_pretrain(image_size=config['image_size'], vit=config['vit'], vit_grad_ckpt=config['vit_grad_ckpt'], vit_ckpt_layer=config['vit_ckpt_layer'], queue_size=config['queue_size']) optimizer = torch.optim.AdamW(params=model.parameters(), lr=config['init_lr'], weight_decay=config['weight_decay']) for epoch in range(start_epoch, config['max_epoch']): train(model, data_loader, optimizer, epoch, device, config)\",\"BLIP 预训练代码实现部分参考Moco论文实现，采用动量慢更新策略，整体流程和ALBEF模型实现一致，下面首先给出的是 BLIP 模型的 init 初始化方法:\",\"class BLIP_Pretrain(nn.Module): def __init__(self, med_config='configs/bert_config.json', # 文本编码器配置 image_size=224, # 输入图像大小 vit='base', # 使用的 ViT 模型类型（如 base、large） vit_grad_ckpt=False, # 是否使用梯度检查点（节省显存） vit_ckpt_layer=0, # 从第几层开始启用 checkpoint embed_dim=256, # 图文共享表示的嵌入维度 queue_size=57600, # 对比学习中图文特征队列长度 momentum=0.995, # 动量编码器的更新参数 ): super().__init__() # 1. 创建主视觉编码器（ViT） self.visual_encoder, vision_width = create_vit( vit, image_size, vit_grad_ckpt, vit_ckpt_layer, 0 ) # 2. 创建文本编码器（BERT） self.tokenizer = init_tokenizer() # 加载 tokenizer（默认 BERT） self.text_encoder = BertModel.from_pretrained( 'bert-base-uncased', config=encoder_config, add_pooling_layer=False ) # 3. 视觉 / 文本 特征映射到共享空间 self.vision_proj = nn.Linear(vision_width, embed_dim) # (D_v → D_e) self.text_proj = nn.Linear(text_width, embed_dim) # (D_t → D_e) # 4. 图文匹配（ITM）任务的二分类头 self.itm_head = nn.Linear(text_width, 2) # ======================= 动量编码器（Momentum Encoder） ======================= # # 用于构造 InfoNCE 的 soft target，与主模型参数不同步，而是 EMA 滑动平均更新 self.visual_encoder_m, _ = create_vit(vit, image_size) # 动量视觉编码器 self.vision_proj_m = nn.Linear(vision_width, embed_dim) self.text_encoder_m = BertModel( config=encoder_config, add_pooling_layer=False ) # 动量文本编码器 self.text_proj_m = nn.Linear(text_width, embed_dim) # 将主模型和动量模型参数组织成配对，用于拷贝和更新 self.model_pairs = [ [self.visual_encoder, self.visual_encoder_m], [self.vision_proj, self.vision_proj_m], [self.text_encoder, self.text_encoder_m], [self.text_proj, self.text_proj_m], ] self.copy_params() # 初始化时直接复制参数（后续 EMA 更新） # ======================= 特征队列初始化 ======================= # # 队列用于 InfoNCE 对比学习中的负样本缓存（增强样本多样性） self.register_buffer(\\\"image_queue\\\", torch.randn(embed_dim, queue_size)) # 图像队列：(D_e, Q) self.register_buffer(\\\"text_queue\\\", torch.randn(embed_dim, queue_size)) # 文本队列：(D_e, Q) self.register_buffer(\\\"queue_ptr\\\", torch.zeros(1, dtype=torch.long)) # 队列指针（循环更新） # 初始化队列为单位向量（便于计算归一化相似度） self.image_queue = nn.functional.normalize(self.image_queue, dim=0) self.text_queue = nn.functional.normalize(self.text_queue, dim=0) self.queue_size = queue_size self.momentum = momentum # InfoNCE 温度参数（可学习） self.temp = nn.Parameter(0.07 * torch.ones([])) # ======================= 文本解码器（用于 LM 任务） ======================= # self.text_decoder = BertLMHeadModel.from_pretrained( 'bert-base-uncased', config=decoder_config )\",\"BLIP 模型的前向传播流程和ALBEF实现基本一致，这里不过多进行展开:\",\"def forward(self, image, caption, alpha): # ===================== 1. 图像与文本特征提取 ===================== # # 图像编码：提取视觉特征 image_embeds = self.visual_encoder(image) # (B, N, D_v) image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device) # (B, N) image_feat = F.normalize(self.vision_proj(image_embeds[:, 0, :]), dim=-1) # (B, D_e)，CLS特征 → 投影 → 归一化 # 文本编码：tokenize 文本 text = self.tokenizer(caption, padding='max_length', truncation=True, max_length=30, return_tensors=\\\"pt\\\").to(image.device) text_output = self.text_encoder(text.input_ids, attention_mask=text.attention_mask, return_dict=True, mode='text') text_feat = F.normalize(self.text_proj(text_output.last_hidden_state[:, 0, :]), dim=-1) # (B, D_e)，CLS特征 → 投影 → 归一化 # ===================== 2. 计算动量编码器输出，用于生成 soft target ===================== # with torch.no_grad(): self._momentum_update() # 图像动量编码器 image_embeds_m = self.visual_encoder_m(image) # (B, N, D_v) image_feat_m = F.normalize(self.vision_proj_m(image_embeds_m[:, 0, :]), dim=-1) # (B, D_e) # 构建图像所有对比特征 = 当前batch + 队列 image_feat_all = torch.cat([image_feat_m.T, self.image_queue.clone().detach()], dim=1) # (D_e, B+Q) # 文本动量编码器 text_output_m = self.text_encoder_m(text.input_ids, attention_mask=text.attention_mask, return_dict=True, mode='text') text_feat_m = F.normalize(self.text_proj_m(text_output_m.last_hidden_state[:, 0, :]), dim=-1) # (B, D_e) # 构建文本所有对比特征 = 当前batch + 队列 text_feat_all = torch.cat([text_feat_m.T, self.text_queue.clone().detach()], dim=1) # (D_e, B+Q) # 计算图 → 文本 和 文本 → 图 相似度（soft target） sim_i2t_m = image_feat_m @ text_feat_all / self.temp # (B, B+Q) sim_t2i_m = text_feat_m @ image_feat_all / self.temp # (B, B+Q) sim_targets = torch.zeros(sim_i2t_m.size()).to(image.device) sim_targets.fill_diagonal_(1) # 构造 hard target (对角线为正例) sim_i2t_targets = alpha * F.softmax(sim_i2t_m, dim=1) + (1 - alpha) * sim_targets # 软标签 + 硬标签混合 sim_t2i_targets = alpha * F.softmax(sim_t2i_m, dim=1) + (1 - alpha) * sim_targets # ===================== 3. 计算 InfoNCE 对比学习损失 (ITC) ===================== # sim_i2t = image_feat @ text_feat_all / self.temp # (B, B+Q) sim_t2i = text_feat @ image_feat_all / self.temp # (B, B+Q) loss_i2t = -torch.sum(F.log_softmax(sim_i2t, dim=1) * sim_i2t_targets, dim=1).mean() loss_t2i = -torch.sum(F.log_softmax(sim_t2i, dim=1) * sim_t2i_targets, dim=1).mean() loss_ita = (loss_i2t + loss_t2i) / 2 # 更新负样本队列 self._dequeue_and_enqueue(image_feat_m, text_feat_m) # ===================== 4. 图文匹配 (ITM) ===================== # # 用于多模态 cross-attention 编码的输入文本（替换 CLS） encoder_input_ids = text.input_ids.clone() encoder_input_ids[:, 0] = self.tokenizer.enc_token_id bs = image.size(0) # 正样本对 output_pos = self.text_encoder(encoder_input_ids, attention_mask=text.attention_mask, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, return_dict=True) with torch.no_grad(): # 为 ITM 任务采样负样本索引（从 sim 分布中采样，避免选到自己） weights_t2i = F.softmax(sim_t2i[:, :bs], dim=1) + 1e-4 weights_t2i.fill_diagonal_(0) weights_i2t = F.softmax(sim_i2t[:, :bs], dim=1) + 1e-4 weights_i2t.fill_diagonal_(0) # select a negative image for each text image_embeds_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_t2i[b], 1).item() image_embeds_neg.append(image_embeds[neg_idx]) image_embeds_neg = torch.stack(image_embeds_neg,dim=0) # select a negative text for each image text_ids_neg = [] text_atts_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_i2t[b], 1).item() text_ids_neg.append(encoder_input_ids[neg_idx]) text_atts_neg.append(text.attention_mask[neg_idx]) text_ids_neg = torch.stack(text_ids_neg,dim=0) text_atts_neg = torch.stack(text_atts_neg,dim=0) # 合并正负样本对 text_ids_all = torch.cat([encoder_input_ids, text_ids_neg], dim=0) # (2B, L) text_atts_all = torch.cat([text.attention_mask, text_atts_neg], dim=0) # (2B, L) image_embeds_all = torch.cat([image_embeds_neg, image_embeds], dim=0) # (2B, N, D_v) image_atts_all = torch.cat([image_atts, image_atts], dim=0) # (2B, N) output_neg = self.text_encoder(text_ids_all, attention_mask=text_atts_all, encoder_hidden_states=image_embeds_all, encoder_attention_mask=image_atts_all, return_dict=True) # 提取 [CLS] 融合特征，用于二分类匹配 vl_embeddings = torch.cat([output_pos.last_hidden_state[:, 0, :], output_neg.last_hidden_state[:, 0, :]], dim=0) # (3B, D_t) vl_output = self.itm_head(vl_embeddings) # (3B, 2)，匹配or不匹配 itm_labels = torch.cat([ torch.ones(bs, dtype=torch.long), torch.zeros(2 * bs, dtype=torch.long) ], dim=0).to(image.device) loss_itm = F.cross_entropy(vl_output, itm_labels) # ===================== 5. 文本生成任务（LM） ===================== # decoder_input_ids = text.input_ids.clone() decoder_input_ids[:, 0] = self.tokenizer.bos_token_id # 用 [BOS] 替换 [CLS] decoder_targets = decoder_input_ids.masked_fill(decoder_input_ids == self.tokenizer.pad_token_id, -100) # 忽略pad位loss decoder_output = self.text_decoder(decoder_input_ids, attention_mask=text.attention_mask, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, labels=decoder_targets, return_dict=True) loss_lm = decoder_output.loss # ===================== 6. 返回三个 loss ===================== # return loss_ita, loss_itm, loss_lm\"]},\"148\":{\"h\":\"ALBEF 论文\",\"t\":[\"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation 论文简析\",\"论文链接: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation 代码链接: https://github.com/salesforce/ALBEF\"]},\"149\":{\"h\":\"Introduction\",\"t\":[\"视觉-语言大规模预训练能提升多种视觉语言任务，但视觉和文本的token未对齐，导致多模态编码器难以有效建模图文交互。当前论文提出了一个基于对比损失的“先对齐后融合”（ALBEF）策略，通过对比学习先对齐图像和文本的表示，再用跨模态注意力融合，提升表示的准确性。并且ALBEF 不依赖目标检测器，也不需要高分辨率图像；同时通过引入动量蒸馏(momentum distillation)自训练方法，能更有效应对含噪声的网络数据，提高泛化能力。\",\"传统VLP方法依赖目标检测器提取区域特征，结合文本通过多模态编码器处理，面临：\",\"图像和文本特征空间不一致，交互难度大。\",\"目标检测器开销高，注释和计算开销大。\",\"网络图文数据噪声多，容易导致预训练过拟合，泛化差。\",\"ALBEF 采用无检测器的图像编码器和文本编码器独立编码，再用多模态编码器融合。设计图文对比（ITC）损失：\",\"对齐图文特征空间，简化跨模态融合。\",\"改善单模态编码语义理解。\",\"学习共同低维空间，促进硬负样本挖掘。\",\"提出动量蒸馏（MoD），用动量模型生成伪标签，缓解噪声影响，提升预训练及下游表现。\"]},\"150\":{\"h\":\"Related Work\",\"t\":[\"Vision-Language Representation Learning:\",\"当前视觉-语言表示学习主要分为两类：一类使用多模态 Transformer 编码器建模图文交互，适用于复杂推理任务但依赖目标检测器和高分辨率图像，计算开销大；另一类则采用独立的图像与文本编码器，通过对比学习在大规模图文对中对齐表示，虽在图文检索中效果出色，但难以处理复杂语义交互。ALBEF 融合两者优势，先用对比学习对齐图文表示，再通过跨模态注意力实现深度融合，同时摒弃目标检测器，在保证高性能的同时显著降低了计算成本，兼顾了效率与泛化能力。\",\"Knowledge Distillation:\",\"传统知识蒸馏通过教师模型指导学生模型提升性能，通常依赖预训练教师。近年来的在线蒸馏则使用多个同时训练的模型进行知识迁移。ALBEF 提出的 Momentum Distillation 属于自蒸馏的一种形式，通过使用自身参数的滑动平均作为教师，生成伪标签辅助训练。该方法无需额外模型，能缓解弱标注图文数据中的噪声问题，提升表示稳定性和泛化能力。\"]},\"151\":{\"h\":\"ALBEF\"},\"152\":{\"h\":\"Model Structure\",\"t\":[\"ALBEF 模型由三个主要部分组成：图像编码器、文本编码器和多模态编码器。图像编码器采用了预训练的 ViT-B/16（12 层视觉 Transformer），将输入图像编码为一系列嵌入向量。文本编码器和多模态编码器均为 6 层 Transformer，分别初始化自 BERTbase 的前 6 层和后 6 层。文本经过编码后生成的嵌入序列，会与图像嵌入一起送入多模态编码器进行融合。融合过程在多模态编码器的每一层中通过跨模态注意力（Cross Attention）实现，实现图文信息的深层交互。\"]},\"153\":{\"h\":\"Pre-training Objectives\",\"t\":[\"我们对 ALBEF 进行预训练，包含三个目标：在单模态编码器上进行图文对比学习（ITC），以及在多模态编码器上进行掩码语言模型（MLM）和图文匹配（ITM）。我们通过在线对比难样本挖掘来改进图文匹配（ITM）。\"]},\"154\":{\"h\":\"Image-Text Contrastive Learning\",\"t\":[\"图文对比学习旨在融合之前学习更好的单模态表示。它通过学习一个相似度函数\",\"使得配对的图文具有更高的相似度得分。这里 和 是线性变换，用于将 [CLS] 表征映射到归一化的低维（256维）向量。受 MoCo 启发，我们维护两个队列来存储动量单模态编码器最近的 个图文表示。动量编码器生成的归一化特征分别记为\",\"定义相似度函数为：\",\"对于每个图像和文本，我们计算归一化的图像到文本和文本到图像的 softmax 相似度：\",\"其中 是一个可学习的温度参数。令 和 分别表示真实的 one-hot 标签，负样本概率为 0，正样本概率为 1。图文对比损失定义为交叉熵 ：\"]},\"155\":{\"h\":\"Masked Language Modeling（MLM）\",\"t\":[\"Masked Language Modeling 利用图像和上下文文本共同预测被 mask 掉的单词。我们以 15% 的概率随机将输入文本中的 token 替换为特殊标记 [MASK]。设 表示被 mask 的文本， 表示模型预测的被 mask token 的概率分布。MLM 任务的目标是最小化交叉熵损失：\",\"其中 是 one-hot 词表分布，真实标签对应的概率为 1。\"]},\"156\":{\"h\":\"Image-Text Matching（ITM）\",\"t\":[\"Image-Text Matching 用于判断图文对是否匹配。我们使用多模态编码器输出的 [CLS] token 表征作为图文对的联合表示，接一个全连接层（FC），再通过 softmax 得到预测概率 ，最终计算 ITM 的交叉熵损失：\",\"其中 是二分类 one-hot 向量，表示图文对的真实匹配状态。\",\"我们提出了一种 零计算开销的 ITM 硬负样本采样策略：\",\"若图文语义相近但细节不同，则视为 hard negative；\",\"利用图文对比损失中的相似度作为度量，在 mini-batch 中为每张图像选择一个最相似的非匹配文本作为负样本；\",\"同样地，为每个文本选择一个最相似的非匹配图像。\",\"ALBEF 预训练总目标函数：\"]},\"157\":{\"h\":\"Momentum Distillation\",\"t\":[\"视觉-语言预训练所使用的图文对大多来自网页，因此存在较大的噪声。例如：正样本图文对往往关联性较弱，文本中可能包含与图像无关的信息，图像中也可能存在未被文本描述的实体。在对比学习（ITC）中，有些“负样本”文本可能实际上与图像语义一致；而在掩码语言建模（MLM）中，也可能存在多个与被 mask 单词同样合理甚至更好的替代词。但标准 one-hot 标签的监督会一律惩罚这些“非标答案”。\",\"为解决这一问题，ALBEF 提出使用 动量模型（Momentum Model）生成伪标签（pseudo-targets）进行蒸馏监督。动量模型是对主模型参数的滑动平均版本（exponential moving average），起到“老师模型”的作用。在训练中，主模型被训练去匹配动量模型的预测，从而提升鲁棒性和泛化能力。\",\"对比学习中的动量蒸馏:\",\"设动量编码器生成的相似度为：\",\"将其代入标准的对比学习 softmax 公式中，构造软标签（soft pseudo-target）。然后定义 ITC 的动量蒸馏损失为：\",\"这里的 KL 表示 Kullback-Leibler 散度，衡量模型预测分布与动量模型生成的软标签之间的差异。\",\"掩码语言建模中的动量蒸馏:\",\"设动量模型在图像 和被 mask 的文本 上预测得到的概率分布为 ，主模型的预测为 。对应的蒸馏损失定义为：\",\"这样的设计使得 MLM 模型不再被 one-hot 标签约束，可以学习更丰富的词汇表达，捕捉与图像内容相关的多种可能性。\",\"如论文中的图 2 所示，动量模型生成的伪标签往往比真实标签更具多样性和语义丰富性。例如：\",\"原始文本：\\\"polar bear in the [MASK]\\\" 真实标签：wild 伪标签前五名：zoo, pool, water, pond, wild\",\"这种伪标签不仅能补充视觉信息中的遗漏，还能提供更灵活的语义参考。\",\"通过引入动量蒸馏，ALBEF 能够：\",\"在噪声标签数据上提高学习效果；\",\"避免因 one-hot 标签过度惩罚合理预测；\",\"在多任务（如 ITC 和 MLM）中更稳定地训练；\",\"提高预训练模型在下游任务中的表现。\",\"动量蒸馏的总体损失是对原始监督信号与伪监督信号的加权组合，平衡其指导作用：\",\"其中 控制动量蒸馏信号的强度，实验中统一设为 0.4。\"]},\"158\":{\"h\":\"Code Implementation\"},\"159\":{\"h\":\"Train\",\"t\":[\"训练代码:\",\"def train(model, data_loader, optimizer, tokenizer, epoch, warmup_steps, device, scheduler, config): for i, (image, text) in enumerate(metric_logger.log_every(data_loader, print_freq, header)): optimizer.zero_grad() image = image.to(device,non_blocking=True) text_input = tokenizer(text, padding='longest', truncation=True, max_length=25, return_tensors=\\\"pt\\\").to(device) if epoch>0: alpha = config['alpha'] else: alpha = config['alpha']*min(1,i/len(data_loader)) loss_mlm, loss_ita, loss_itm = model(image, text_input, alpha = alpha) loss = loss_mlm + loss_ita + loss_itm loss.backward() optimizer.step()\",\"为何前期要让 α 慢慢增加？\",\"训练初期模型尚不稳定，动量分支的 soft label 不可靠。直接用 soft label 可能误导主模型。因此，先以 hard label 为主，逐渐引入 soft label 的指导。\",\"前期：alpha ≈ 0 → 以 one-hot 监督为主，训练稳定\",\"中期：alpha 上升 → soft label 引入更丰富的监督\",\"后期：alpha ≈ config['alpha'] → 强化多义性和类间相似度学习，提高泛化\",\"图中展示的是 ALBEF 模型训练过程中 alpha 参数的变化趋势：\",\"第一个 epoch（前 100 步）：alpha 线性从 0 增加到设定的最大值（如 0.5）。这种方式在训练初期让模型更多依赖于 one-hot 形式的监督信号，降低动量负样本带来的扰动。\",\"第二个 epoch 及之后：alpha 恒定为最大值（例如 0.5），意味着动量分布和 one-hot label 的加权比固定，开始充分利用动量编码器提供的软标签来训练。\"]},\"160\":{\"h\":\"Model Init\",\"t\":[\"ALBEF 模型初始化:\",\"def __init__(self, text_encoder = None, tokenizer = None, config = None, temp = 0.07, init_deit = True ): super().__init__() # 初始化 tokenizer（用于文本编码） self.tokenizer = tokenizer # MLM 任务中 mask 掉的 token 比例 self.mlm_probability = config['mlm_probability'] # 图文对比学习后的共同嵌入维度 embed_dim = config['embed_dim'] # 初始化视觉编码器（ViT backbone），输出: (B, N+1, 768) self.visual_encoder = VisionTransformer( img_size=config['image_res'], patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6)) vision_width = config['vision_width'] # ViT 输出维度，通常为 768 # 加载文本编码器 BertConfig 配置 bert_config = BertConfig.from_json_file(config['bert_config']) # 加载文本编码器，输出: (B, L, hidden_size)，默认 hidden_size = 768 self.text_encoder = BertForMaskedLM.from_pretrained(text_encoder, config=bert_config) text_width = self.text_encoder.config.hidden_size # Bert 输出维度（默认 768） # 图像特征 → 共享嵌入空间（Linear projection）：(B, vision_width) → (B, embed_dim) self.vision_proj = nn.Linear(vision_width, embed_dim) # 文本特征 → 共享嵌入空间（Linear projection）：(B, text_width) → (B, embed_dim) self.text_proj = nn.Linear(text_width, embed_dim) # 学习温度系数 temp ∈ [0.001, 0.5]，用于对比学习中的 softmax 除法 self.temp = nn.Parameter(torch.ones([]) * config['temp']) # 对比学习中的队列长度（如 65536） self.queue_size = config['queue_size'] # 动量更新系数（如 0.995） self.momentum = config['momentum'] # ITM 分类头：输入为 text_encoder 最后一层的 CLS 特征 → 输出为二分类 (B, 2) self.itm_head = nn.Linear(text_width, 2) # =============== 构建动量编码器（结构与主模型相同，仅参数使用 EMA 更新）=============== self.visual_encoder_m = VisionTransformer( img_size=config['image_res'], patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6)) self.vision_proj_m = nn.Linear(vision_width, embed_dim) self.text_encoder_m = BertForMaskedLM.from_pretrained(text_encoder, config=bert_config) self.text_proj_m = nn.Linear(text_width, embed_dim) # 将主编码器和动量编码器配对，用于 EMA 参数更新 self.model_pairs = [[self.visual_encoder,self.visual_encoder_m], [self.vision_proj,self.vision_proj_m], [self.text_encoder,self.text_encoder_m], [self.text_proj,self.text_proj_m], ] # 初始化动量编码器参数 = 主模型参数 self.copy_params() # =============== 初始化负样本队列 =============== # 图像特征队列：(embed_dim, queue_size) self.register_buffer(\\\"image_queue\\\", torch.randn(embed_dim, self.queue_size)) # 文本特征队列：(embed_dim, queue_size) self.register_buffer(\\\"text_queue\\\", torch.randn(embed_dim, self.queue_size)) # 当前入队位置指针，形状: (1,) self.register_buffer(\\\"queue_ptr\\\", torch.zeros(1, dtype=torch.long)) # 初始化队列特征为单位向量，便于之后相似度计算 self.image_queue = nn.functional.normalize(self.image_queue, dim=0) self.text_queue = nn.functional.normalize(self.text_queue, dim=0)\"]},\"161\":{\"h\":\"ITC\",\"t\":[\"ALBEF 模型前向传播中的 ITC 学习目标实现过程:\",\"def forward(self, image, text, alpha=0): # 1. 使用 ViT 对图像进行编码，输出图像特征 # image_embeds: (B, N+1, embed_dim)，N 个 patch + 1 个 CLS token image_embeds = self.visual_encoder(image) # 2. 构造图像的 attention mask，全部为1，表示无 padding # image_atts: (B, N+1)，与 image_embeds 保持一致 image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device) # 3. 取出 CLS Token（图像全局语义），进行线性变换 + 归一化 # image_feat: (B, D)，D为投影后的embedding维度（如256） image_feat = F.normalize(self.vision_proj(image_embeds[:, 0, :]), dim=-1) # 4. 文本编码器（BERT）对文本进行编码 # text_output.last_hidden_state: (B, L, H)，L 为文本长度，H 为hidden size text_output = self.text_encoder.bert(text.input_ids, attention_mask=text.attention_mask, return_dict=True, mode='text') text_embeds = text_output.last_hidden_state # 5. 取出 CLS Token（文本全局语义），线性变换 + 归一化 # text_feat: (B, D) text_feat = F.normalize(self.text_proj(text_embeds[:, 0, :]), dim=-1) # ========== 以下为动量编码器分支（momentum encoder），不参与反向传播 ========== with torch.no_grad(): # 6. 更新动量编码器参数（对主编码器做 EMA） self._momentum_update() # 7. 动量图像编码器输出特征 # image_embeds_m: (B, N+1, embed_dim) # image_feat_m: (B, D) image_embeds_m = self.visual_encoder_m(image) image_feat_m = F.normalize(self.vision_proj_m(image_embeds_m[:, 0, :]), dim=-1) # 8. 拼接当前动量图像特征和图像队列（K 个历史负样本） # image_feat_m.T: (D, B) # image_queue: (D, K) # image_feat_all: (D, B + K) image_feat_all = torch.cat([image_feat_m.t(), self.image_queue.clone().detach()], dim=1) # 9. 动量文本编码器输出特征 # text_output_m.last_hidden_state: (B, L, H) # text_feat_m: (B, D) text_output_m = self.text_encoder_m.bert(text.input_ids, attention_mask=text.attention_mask, return_dict=True, mode='text') text_feat_m = F.normalize(self.text_proj_m(text_output_m.last_hidden_state[:, 0, :]), dim=-1) # 10. 拼接当前动量文本特征和文本队列 # text_feat_all: (D, B + K) text_feat_all = torch.cat([text_feat_m.t(), self.text_queue.clone().detach()], dim=1) # 11. 图像特征与所有文本特征做内积，计算相似度（B, B+K） sim_i2t_m = image_feat_m @ text_feat_all / self.temp sim_t2i_m = text_feat_m @ image_feat_all / self.temp # 12. 构造一对一的匹配目标（对角为正样本） # sim_targets: (B, B+K)，对角为1，其他为0 sim_targets = torch.zeros(sim_i2t_m.size()).to(image.device) sim_targets.fill_diagonal_(1) # 13. 构造 soft label（平滑过的对比目标） # alpha = 0 则为 hard label，alpha 越大越 soft sim_i2t_targets = alpha * F.softmax(sim_i2t_m, dim=1) + (1 - alpha) * sim_targets sim_t2i_targets = alpha * F.softmax(sim_t2i_m, dim=1) + (1 - alpha) * sim_targets # ========== 当前主分支计算相似度，用于 loss 反向传播 ========== # 14. 使用主分支特征与队列拼接结果计算图像-文本相似度（B, B+K） sim_i2t = image_feat @ text_feat_all / self.temp sim_t2i = text_feat @ image_feat_all / self.temp # 15. 计算交叉熵损失（基于 soft label 的 KL loss） loss_i2t = -torch.sum(F.log_softmax(sim_i2t, dim=1) * sim_i2t_targets, dim=1).mean() loss_t2i = -torch.sum(F.log_softmax(sim_t2i, dim=1) * sim_t2i_targets, dim=1).mean() # 16. 图文对比损失（取双向平均） loss_ita = (loss_i2t + loss_t2i) / 2 # 17. 将当前动量特征送入队列，更新队列 self._dequeue_and_enqueue(image_feat_m, text_feat_m)\",\"加入动量队列中的样本作为负样本，是为了扩大负样本池，提升训练难度、判别性和稳定性，使模型能学到更强的图文对齐表示。\"]},\"162\":{\"h\":\"ITM\",\"t\":[\"ALBEF 模型前向传播中的 ITM 学习目标实现过程:\",\"###=================================### # 正向图文对的前向传播（正样本） output_pos = self.text_encoder.bert( encoder_embeds = text_embeds, # 输入文本的嵌入表示 attention_mask = text.attention_mask, # 文本的注意力掩码 encoder_hidden_states = image_embeds, # 图像特征作为 cross-attention 的 encoder hidden state encoder_attention_mask = image_atts, # 图像 attention mask（通常为全 1） return_dict = True, # 返回结构化输出（字典） mode = 'fusion', # 模态融合模式 ) # ================================= # # 计算 ITC 相似度生成的 soft label，用于选择难负样本 with torch.no_grad(): bs = image.size(0) # batch size # 图像到文本的相似度权重（归一化) , sim_i2t维度为(B, B+K) , 这里只取前B个样本, 不考虑从动量队列拿到的负样本 weights_i2t = F.softmax(sim_i2t[:, :bs], dim=1) # 文本到图像的相似度权重（归一化） weights_t2i = F.softmax(sim_t2i[:, :bs], dim=1) # 屏蔽对角线（避免采样到自己） weights_i2t.fill_diagonal_(0) weights_t2i.fill_diagonal_(0) # ================================= # # 采样每个文本对应的负图像（hard negative） image_embeds_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_t2i[b], 1).item() # 按权重从当前行采样一个负样本索引 image_embeds_neg.append(image_embeds[neg_idx]) # 获取对应的负图像嵌入 image_embeds_neg = torch.stack(image_embeds_neg, dim=0) # [B, D] # 采样每个图像对应的负文本（hard negative） text_embeds_neg = [] text_atts_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_i2t[b], 1).item() text_embeds_neg.append(text_embeds[neg_idx]) # 获取对应的负文本嵌入 text_atts_neg.append(text.attention_mask[neg_idx]) # 同时获取对应的 attention mask text_embeds_neg = torch.stack(text_embeds_neg, dim=0) text_atts_neg = torch.stack(text_atts_neg, dim=0) # 构造新的图文对：正文本 + 负文本，负图像 + 正图像 # 共有 2N 个图文对：N 个正样本 + N 个负样本（正文负图 + 正图负文） text_embeds_all = torch.cat([text_embeds, text_embeds_neg], dim=0) text_atts_all = torch.cat([text.attention_mask, text_atts_neg], dim=0) image_embeds_all = torch.cat([image_embeds_neg, image_embeds], dim=0) image_atts_all = torch.cat([image_atts, image_atts], dim=0) # 所有图文对的前向传播（用于 ITM 分类） output_neg = self.text_encoder.bert( encoder_embeds = text_embeds_all, attention_mask = text_atts_all, encoder_hidden_states = image_embeds_all, encoder_attention_mask = image_atts_all, return_dict = True, mode = 'fusion', ) # 提取 [CLS] token 表征作为跨模态图文对表示，输入到 ITM 头 vl_embeddings = torch.cat([ output_pos.last_hidden_state[:, 0, :], # 正样本 [CLS] output_neg.last_hidden_state[:, 0, :] # 负样本 [CLS] ], dim=0) # 二分类：匹配 or 不匹配 vl_output = self.itm_head(vl_embeddings) # shape: [3N, 2] # 构造 ground-truth 标签：前 N 个为正样本（1），后 2N 个为负样本（0） itm_labels = torch.cat([ torch.ones(bs, dtype=torch.long), # N 个正样本 torch.zeros(2*bs, dtype=torch.long) # 2N 个负样本（正图负文 + 正文负图） ], dim=0).to(image.device) # 计算 ITM 的交叉熵损失 loss_itm = F.cross_entropy(vl_output, itm_labels)\",\"补充说明:\",\"ALBEF 在进行 CrossAttention 时，image features 会作为 key 和 value ，而 text features 作为 query:\",\"class BertSelfAttention(nn.Module): def forward( self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False, ): # 1. text features 固定作为 query 计算来源 mixed_query_layer = self.query(hidden_states) # 2. 传入了 image features ，则做 cross attention is_cross_attention = encoder_hidden_states is not None if is_cross_attention: key_layer = self.transpose_for_scores(self.key(encoder_hidden_states)) value_layer = self.transpose_for_scores(self.value(encoder_hidden_states)) attention_mask = encoder_attention_mask else: key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) query_layer = self.transpose_for_scores(mixed_query_layer) ...\",\"ALBEF 采用加权随机采样而非直接取相似度最大的负样本（argmax），是为了在突出“难负样本”的同时保持训练的稳定性和多样性，避免模型过拟合于极端负样本或伪负样本，从而提升泛化能力和鲁棒性。\"]},\"163\":{\"h\":\"MLM\",\"t\":[\"ALBEF 模型前向传播中的 MLM 学习目标实现过程:\",\"##================= MLM ========================## # 克隆一份 input_ids 和 labels，作为 MLM 的输入和标签副本 input_ids = text.input_ids.clone() labels = input_ids.clone() # 构造一个与 input_ids 同形状的矩阵，值为 mask 概率（例如 0.15） probability_matrix = torch.full(labels.shape, self.mlm_probability) # 对 input_ids 按照给定概率进行 [MASK] 操作，同时将对应的 labels 保留为原始 token id，其余位置设为 -100（忽略） input_ids, labels = self.mask(input_ids, self.text_encoder.config.vocab_size, image.device, targets=labels, probability_matrix = probability_matrix) # ===== 使用动量编码器对 masked 输入做前向传播，获取 soft target（Teacher 网络） ===== # 注意：这一步不计算梯度，仅用于生成 soft label with torch.no_grad(): logits_m = self.text_encoder_m( input_ids, attention_mask = text.attention_mask, # 文本 attention mask encoder_hidden_states = image_embeds_m, # 动量视觉特征 encoder_attention_mask = image_atts, # 图像 attention mask return_dict = True, return_logits = True, # 返回 logits 用于 soft label ) # ===== 主网络进行 MLM 前向传播，并引入 soft label 监督 ===== mlm_output = self.text_encoder( input_ids, attention_mask = text.attention_mask, encoder_hidden_states = image_embeds, # 主视觉特征（非动量） encoder_attention_mask = image_atts, return_dict = True, labels = labels, # 用于 standard cross-entropy 监督（hard label） soft_labels = F.softmax(logits_m, dim=-1), # soft label 来自动量编码器（Teacher） alpha = alpha # 混合比：控制 hard 和 soft loss 的权重 ) # 最终的 masked language modeling 损失 loss_mlm = mlm_output.loss\",\"mask 方法代码实现:\",\"def mask(self, input_ids, vocab_size, device, targets=None, masked_indices=None, probability_matrix=None): # Step 1: 生成掩码位置 if masked_indices is None: # 若未指定掩码位置，则按给定的概率矩阵进行伯努利采样，得到每个 token 是否被 mask masked_indices = torch.bernoulli(probability_matrix).bool() # Step 2: 屏蔽不可 mask 的位置（例如 [PAD] 和 [CLS]） masked_indices[input_ids == self.tokenizer.pad_token_id] = False masked_indices[input_ids == self.tokenizer.cls_token_id] = False # Step 3: 构造目标标签（只对被 mask 的位置计算 loss） if targets is not None: targets[~masked_indices] = -100 # 非 mask 位置的标签设为 -100，表示 loss 忽略 # Step 4: 对被 mask 的 token 进行替换（按 BERT 策略） # 80% 的 mask token 被替换为 [MASK] indices_replaced = torch.bernoulli(torch.full(input_ids.shape, 0.8)).bool() & masked_indices input_ids[indices_replaced] = self.tokenizer.mask_token_id # 10% 的 mask token 被替换为随机 token（噪声） indices_random = torch.bernoulli(torch.full(input_ids.shape, 0.5)).bool() & masked_indices & ~indices_replaced random_words = torch.randint(vocab_size, input_ids.shape, dtype=torch.long).to(device) input_ids[indices_random] = random_words[indices_random] # 剩下的 10% 保持原样（不修改 token） # Step 5: 返回掩码后的 input_ids 和（可选的）标签 targets if targets is not None: return input_ids, targets else: return input_ids\",\"text_encoder 前向传播代码实现:\",\"def forward( self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_embeds=None, # 外部传入的 encoder embeddings（不常用） encoder_hidden_states=None, # 图像编码器输出（作为 cross-attn 的 K,V） encoder_attention_mask=None, # 图像部分的 attention mask labels=None, # MLM 标签（只在 MLM 模式中提供） output_attentions=None, output_hidden_states=None, return_dict=None, is_decoder=False, mode='multi_modal', # 模式控制，支持 'fusion'（图文融合）等 soft_labels=None, # 蒸馏 soft labels，来自 momentum encoder alpha=0, # 蒸馏损失的权重 return_logits=False, # 是否仅返回 logits（用于 momentum 计算） ): # Step 1: 调用 BERT 模型（支持 encoder-decoder 模式，支持 fusion 模式） outputs = self.bert( input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, encoder_embeds=encoder_embeds, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, is_decoder=is_decoder, mode=mode, ) # Step 2: 获取 transformer 输出的 token 表征（[B, L, D]） sequence_output = outputs[0] # Step 3: 计算每个 token 的预测分布（[B, L, vocab_size]） prediction_scores = self.cls(sequence_output) # Step 4: 若只需输出 logits（如 momentum 模型前向），直接返回 if return_logits: return prediction_scores # Step 5: 计算标准 MLM 交叉熵损失（仅对 label ≠ -100 的位置有效） masked_lm_loss = None if labels is not None: loss_fct = CrossEntropyLoss() # 忽略标签为 -100 的位置 masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1)) # Step 6: 若提供 soft labels（如知识蒸馏），计算 KL-style 蒸馏损失 if soft_labels is not None: # 蒸馏损失：soft label 和当前输出的 softmax 分布之间的 KL 散度 loss_distill = -torch.sum( F.log_softmax(prediction_scores, dim=-1) * soft_labels, dim=-1 ) loss_distill = loss_distill[labels != -100].mean() # 混合两种损失：标准 MLM loss 和 蒸馏 loss masked_lm_loss = (1 - alpha) * masked_lm_loss + alpha * loss_distill # Step 7: 根据 return_dict 控制输出格式（支持 tuple 或 dict） if not return_dict: output = (prediction_scores,) + outputs[2:] return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output # 标准化输出（MaskedLMOutput 是 huggingface 定义的一个结构体） return MaskedLMOutput( loss=masked_lm_loss, logits=prediction_scores, hidden_states=outputs.hidden_states, attentions=outputs.attentions, )\",\"注意:\",\"HuggingFace 的 CrossEntropyLoss 默认会忽略标签为 -100 的位置，这是 PyTorch 官方文档中的行为规范：\",\"class CrossEntropyLoss(_WeightedLoss): def __init__( self, weight: Optional[Tensor] = None, size_average=None, ignore_index: int = -100, reduce=None, reduction: str = \\\"mean\\\", label_smoothing: float = 0.0, )\"]},\"164\":{\"h\":\"BEiT 论文\",\"t\":[\"BEiT: BERT Pre-Training of Image Transformers\",\"论文链接: BEiT: BERT Pre-Training of Image Transformers\"]},\"165\":{\"h\":\"摘要\",\"t\":[\"BEiT（Bidirectional Encoder representation from Image Transformers）是一种基于自监督学习的视觉Transformer预训练模型，其核心思想借鉴了BERT的掩码语言建模任务，提出掩码图像建模(MIM)方法。\",\"具体而言，BEiT将图像表示为两种视图——图像块（如16×16像素的局部区域）和离散视觉标记（通过图像分词器生成），在预训练阶段随机掩码部分图像块并让模型预测原始视觉标记，而非直接回归像素值。\",\"实验表明，BEiT在图像分类和语义分割等下游任务中表现优异，且能加速微调收敛。该方法避免了传统像素级重建的局限性，通过高层语义的离散标记学习更有效的视觉表示，为视觉Transformer的自监督预训练提供了新思路。\"]},\"166\":{\"h\":\"简介\",\"t\":[\"Transformer 在计算机视觉领域展现出强大潜力，但视觉 Transformer 通常比卷积神经网络（CNN）需要更多的训练数据。为解决这一问题，自监督预训练成为利用大规模无标注图像数据的关键方法。目前，对比学习和自蒸馏等方法已被探索，但 BERT 风格的掩码建模在视觉领域的应用尚未充分研究。\",\"BEiT 提出了一种基于 掩码图像建模（MIM） 的自监督预训练方法，其核心挑战在于：\",\"缺乏预定义词汇：与 NLP 不同，图像块没有现成的词汇表，无法直接使用 softmax 分类器预测所有可能的候选块。\",\"像素回归的局限性：直接预测掩码块的原始像素会导致模型过度关注短程依赖和高频细节，而非高层语义。\",\"BEiT 的解决方案是：\",\"使用 双视图表示（图 1）：图像块（输入）和视觉标记（目标）。视觉标记通过离散变分自编码器（dVAE）学习，形成离散化的语义表示。\",\"在预训练时，随机掩码约 40% 的图像块，并让模型基于上下文预测原始视觉标记，而非像素值。\",\"实验表明，BEiT 在图像分类和语义分割任务上优于从零训练的模型和其他自监督方法。此外，BEiT 无需人工标注即可通过自注意力机制学习语义区域和物体边界（如图 2 所示），证明了其自动捕获高层视觉知识的能力。\",\"BEiT 的贡献包括：\",\"提出 MIM 任务，为视觉 Transformer 提供理论解释（基于变分自编码器视角）。\",\"在多个下游任务（如分类、分割）上验证了其有效性。\",\"揭示了自监督预训练中自注意力机制对语义理解的自动学习能力。\"]},\"167\":{\"h\":\"方法\",\"t\":[\"给定输入图像 ，BEiT 将其编码为上下文相关的向量表示。如图1所示，BEiT 通过自监督学习中的掩码图像建模（Masked Image Modeling, MIM）任务进行预训练。MIM 的目标是基于编码向量恢复被掩码的图像块。对于下游任务（如图像分类和语义分割），我们在预训练的 BEiT 上添加任务层，并在特定数据集上微调参数。\"]},\"168\":{\"h\":\"图像表示\",\"t\":[\"我们的方法中，图像有两种表示视图：图像块（image patch）和视觉标记（visual token）。这两种类型分别作为预训练时的输入和输出表示。\"]},\"169\":{\"h\":\"图像块\",\"t\":[\"将 2D 图像分割为一系列块序列，使标准 Transformer 可以直接接受图像数据。形式上，我们将图像 重塑为 个块 ，其中：\",\" 是通道数\",\" 是输入图像分辨率\",\" 是每个块的分辨率\",\"图像块 被展平为向量并进行线性投影，类似于 BERT 中的词嵌入。图像块保留了原始像素，用作 BEiT 的输入特征。\",\"在我们的实验中，将每个 图像分割为 网格的图像块，每个块大小为 。\"]},\"170\":{\"h\":\"视觉标记\",\"t\":[\"与自然语言类似，我们通过\\\"图像分词器\\\"将图像表示为离散标记序列，而不是原始像素。具体来说，我们将图像 分词为 ，其中词汇表 包含离散标记索引。\",\"我们使用离散变分自编码器（dVAE）学习的图像分词器。视觉标记学习包含两个模块：\",\"分词器 ：根据视觉码本将图像像素 映射到离散标记 \",\"解码器 ：基于视觉标记 重建输入图像 \",\"重建目标可表示为：\",\"由于潜在视觉标记是离散的，模型训练不可微分，因此采用 Gumbel-softmax 松弛进行训练。此外，在 dVAE 训练期间对 施加均匀先验。\",\"我们将每个图像分词为 网格的视觉标记。注意一张图像的视觉标记数量与图像块数量相同。词汇表大小设为 。\"]},\"171\":{\"h\":\"骨干网络：图像 Transformer\",\"t\":[\"我们使用标准 Transformer 作为骨干网络。输入是图像块序列 ，经过线性投影得到块嵌入 ，其中 。输入向量表示为：\",\"其中 是位置嵌入。编码器包含 层 Transformer 块：\"]},\"172\":{\"h\":\"BEiT 预训练：掩码图像建模\",\"t\":[\"我们提出掩码图像建模（MIM）任务。给定图像 ，我们：\",\"分割为 个图像块 \",\"分词为 个视觉标记 \",\"随机掩码约 40% 图像块，位置记为 \",\"被掩码的图像表示为：\",\"预测目标为：\",\"预训练目标函数：\"]},\"173\":{\"h\":\"变分自编码器视角\",\"t\":[\"BEiT 预训练可视为变分自编码器训练，其证据下界（ELBO）为：\"]},\"174\":{\"h\":\"预训练设置\",\"t\":[\"关键参数：\",\"12 层 Transformer\",\"隐藏大小 768\",\"12 个注意力头\",\"学习率 1.5e-3\",\"批量大小 2048\",\"训练 800 轮次\"]},\"175\":{\"h\":\"下游任务微调\",\"t\":[\"图像分类：\",\"使用平均池化聚合表示\",\"softmax 分类器：\",\"语义分割：\",\"使用预训练 BEiT 作为编码器\",\"添加反卷积层作为解码器\",\"中间微调：\",\"先在 ImageNet 上微调\",\"再在目标任务上微调\"]},\"176\":{\"h\":\"Zero-Shot Text-to-Image Generation 论文\",\"t\":[\"Zero-Shot Text-to-Image Generation 论文\",\"论文链接: Zero-Shot Text-to-Image Generation\"]},\"177\":{\"h\":\"摘要\",\"t\":[\"本文提出了一种基于自回归Transformer的简单而高效的文本到图像生成方法。与传统的依赖复杂架构或辅助损失的方法不同，该方法通过将文本和图像标记建模为单一数据流，利用大规模数据和模型参数（120亿参数的Transformer训练于2.5亿图文对），在零样本条件下实现了与领域专用模型相媲美的性能。实验表明，该方法在生成质量和泛化能力上表现优异，无需针对特定数据集进行微调即可生成高保真图像。\"]},\"178\":{\"h\":\"简介\",\"t\":[\"文本生成图像（Text-to-Image Generation）领域自Mansimov等人(2015)的开创性工作以来，经历了多个阶段的发展。早期研究基于循环变分自编码器（如DRAW模型），随后Reed等人(2016b)引入生成对抗网络（GANs），显著提升了生成图像的保真度，并展示了零样本泛化能力。后续研究通过多尺度生成器（Zhang等人2017、2018）、注意力机制（Xu等人2018）和辅助损失函数进一步优化了生成质量。\",\"尽管技术进步，生成图像仍存在以下问题：\",\"物体结构扭曲\",\"空间逻辑错误\",\"前景背景融合异常\",\"本文提出了一种基于自回归Transformer的简单方法，通过两阶段训练流程实现：\",\"阶段一：训练离散变分自编码器（dVAE），将图像压缩为的8192维视觉词表（如图1所示），显著降低计算复杂度。\",\"阶段二：联合建模文本（BPE编码，最多256词）与图像标记（1024词），通过12B参数的稀疏Transformer学习联合分布。\",\"模型优化目标为证据下界（ELB）：\",\"其中（实际取6.6）以提升词表利用率。\",\"实验效果：\",\"在MS-COCO数据集上零样本评估，人类评估者90%更偏好本文模型（如图7）。\",\"模型展现出组合概念（如“手风琴组成的貘”）、文本渲染和零样本图像翻译等能力（如图2）。\",\"数据与规模的重要性：\",\"研究指出，传统小规模数据集（如MS-COCO）可能限制模型性能。本文通过250M互联网图文对训练，验证了数据与模型规模对泛化能力的提升作用。\"]},\"179\":{\"h\":\"书生·万象多模态大模型（InternVL 1.0）\",\"t\":[\"InternVL 1.0: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks 论文简析\",\"论文链接: https://arxiv.org/abs/2312.14238 代码链接: https://github.com/OpenGVLab/InternVL\"]},\"180\":{\"h\":\"摘要\",\"t\":[\"InternVL是一个大规模视觉-语言基础模型，旨在解决当前视觉与视觉-语言基础模型发展滞后于大型语言模型（LLMs）的问题。该模型通过将视觉基础模型扩展到60亿参数，并利用多源网络图像-文本数据进行渐进式对齐训练，成功实现了视觉与语言模型在参数规模和特征表示上的协调。InternVL在32个通用视觉-语言任务中表现出色，包括图像分类、语义分割、视频分类、图像/视频-文本检索以及多模态对话系统等，展现了强大的视觉能力和与LLMs的无缝集成潜力，为多模态大模型的发展提供了重要贡献。\"]},\"181\":{\"h\":\"简介\",\"t\":[\"研究背景与问题: 大型语言模型（LLMs）的快速发展推动了通用人工智能（AGI）系统的进步，但视觉和视觉-语言基础模型的发展却相对滞后。现有的视觉-语言大模型（VLLMs）通常使用轻量级的“胶水层”（如QFormer或线性投影）来对齐视觉和语言模型的特征，但这种方法存在三个主要限制：\",\"参数规模不匹配：LLMs的参数规模已达千亿级，而视觉编码器通常仅约10亿参数，限制了LLM的能力利用。\",\"表征不一致：视觉模型通常基于纯视觉数据或BERT系列模型训练，与LLMs的特征空间存在差异。\",\"低效连接：轻量级胶水层难以捕捉跨模态的复杂交互。\",\"解决方案与核心设计: 论文提出 InternVL，通过以下关键设计解决上述问题：\",\"参数平衡的视觉与语言组件：包含60亿参数的视觉编码器（InternViT-6B）和80亿参数的语言中间件（QLLaMA），后者作为强大的“胶水层”重组视觉特征。\",\"一致的表征对齐：使用多语言LLaMA初始化中间件，确保视觉编码器与LLMs的特征空间一致。\",\"渐进式图像-文本对齐策略：先在大规模噪声数据上对比学习，再在高质量数据上生成学习，逐步提升模型性能（如图1c所示）。\",\"模型优势\",\"多功能性：可作为独立视觉编码器或与语言中间件结合，支持感知、检索、生成和对话任务。\",\"强大性能：在ImageNet分类、ADE20K分割、视频检索等任务中达到SOTA（如图2所示）。\",\"LLM友好性：与LLaMA、Vicuna等LLMs无缝集成，推动多模态应用发展。\"]},\"182\":{\"h\":\"相关工作\"},\"183\":{\"h\":\"\",\"t\":[\"视觉基础模型在过去十年中经历了显著发展，从早期的AlexNet和CNN架构（如ResNet）到近年来的Vision Transformer（ViT）及其变体。ViT及其衍生模型（如ViT-G、EVA-02等）通过扩大模型规模和参数量，显著提升了视觉任务的性能。然而，当前广泛使用的视觉模型参数量仍停留在约10亿级别（如ViT-22B除外），远落后于LLMs的规模。此外，这些模型多基于纯视觉数据（如ImageNet、JFT）训练，或与BERT系列模型对齐，缺乏与LLMs的直接特征兼容性，限制了其在多模态任务中的表现。\"]},\"184\":{\"h\":\"\",\"t\":[\"LLMs（如GPT-3、LLaMA系列、Vicuna等）在自然语言处理领域取得了突破性进展，展示了强大的少样本和零样本学习能力。开源模型（如ChatGLM、Falcon等）的涌现进一步加速了多模态研究的进程。然而，LLMs本身缺乏视觉理解能力，如何将其与视觉模态结合成为关键挑战。\"]},\"185\":{\"h\":\"\",\"t\":[\"近期研究通过将视觉模型与LLMs结合，构建了多模态对话系统（如Flamingo、LLaVA、MiniGPT-4等）。这些工作主要依赖轻量级适配层（如QFormer、线性投影）连接视觉编码器和LLM，但受限于视觉模型的规模和对齐效率。部分模型（如KOSMOS-2、Qwen-VL）进一步引入了视觉定位能力，支持区域描述和问答。尽管如此，视觉基础模型的性能瓶颈仍是制约VLLMs发展的关键因素。\"]},\"186\":{\"h\":\"\",\"t\":[\"现有工作表明，视觉模型的规模不足和对齐低效是阻碍多模态能力提升的主要障碍。InternVL通过规模化视觉编码器和渐进式跨模态对齐，首次实现了视觉与语言模型在参数和特征空间的深度协同，填补了这一领域的空白。\"]},\"187\":{\"h\":\"方法\"},\"188\":{\"h\":\"\",\"t\":[\"InternVL的整体架构（如图3所示）突破了传统视觉模型（如ViT）和双塔模型（如CLIP）的局限，通过以下两个核心组件实现跨模态深度协同：\",\"InternViT-6B: 基于标准ViT架构的60亿参数视觉编码器，通过超参数搜索优化了深度（48层）、头数（25）和MLP比率（8），在模型规模（5.9B参数）与计算效率间取得平衡（详见表1）。其输出支持密集特征图（）或全局池化特征，适配分类、分割等任务。\",\"QLLaMA: 基于多语言LLaMA-7B初始化的80亿参数语言中间件，新增96个可学习查询和交叉注意力层（1B参数），作为视觉与LLMs之间的\\\"重型胶水层\\\"。相比QFormer等轻量适配器，其参数量提升42倍，能更有效地重组视觉特征为LLM兼容的序列（见图4b/d）。\",\"如图1所示，InternVL的架构设计显著区别于：\",\"(a) 纯视觉模型（如ResNet）：仅支持单模态任务，缺乏语言对齐。\",\"(b) 双塔模型（如CLIP）：独立编码图像/文本，依赖浅层相似度计算。\",\"(c) InternVL：通过QLLaMA实现动态特征交互，同时支持对比学习（如检索）和生成任务（如描述）。\",\"通过组合不同组件，InternVL可灵活切换为四种模式（图4）：\",\"纯视觉模式（图4a）：仅用InternViT-6B处理图像分类/分割。\",\"对比模式-InternVL-C（图4b）：视觉编码器+注意力池化，用于零样本分类/检索。\",\"对比模式-InternVL-G（图4b）：联合QLLaMA二次编码视觉特征，提升检索精度。\",\"对话模式（图4c/d）：连接LLM（如Vicuna），支持多模态问答。\"]},\"189\":{\"h\":\"\",\"t\":[\"大规模视觉编码器（InternViT-6B）\",\"InternViT-6B是一个基于Vision Transformer（ViT）的视觉编码器，参数量达到60亿，旨在与大型语言模型（LLM）的规模相匹配。\",\"通过超参数搜索（如模型深度、头维度和MLP比例），作者确定了在性能和效率之间取得平衡的最佳配置（表1）。实验发现，模型深度对速度的影响在GPU计算饱和后可以忽略，而参数数量相同时，不同配置对性能影响较小。最终选择了深度48、宽度3200、MLP比率12800的稳定配置。\",\"该编码器支持密集预测任务（如语义分割）和图像分类任务，并能生成全局或局部视觉特征（图4a/b）。\",\"语言中间件（QLLaMA）\",\"QLLaMA是一个80亿参数的语言中间件，基于多语言LLaMA-7B初始化，新增了96个可学习查询和交叉注意力层（1亿参数），用于对齐视觉与语言特征（图3）。\",\"相比传统轻量级“胶水层”（如QFormer或线性投影），QLLaMA的优势包括：\",\"通过预训练权重实现视觉特征到LLM表示的对齐；\",\"参数量是QFormer的42倍，即使冻结LLM解码器也能在多模态对话任务中表现优异；\",\"支持对比学习任务（如零样本图像分类和检索）。\",\"灵活的组合方式（“瑞士军刀”模型）\",\"InternVL通过组合视觉编码器和语言中间件，支持多种任务模式（图4）：\",\"视觉感知任务：直接使用InternViT-6B提取特征。\",\"对比任务（如检索）：通过注意力池化生成全局特征（InternVL-C或InternVL-G）。\",\"生成任务（如图像描述）：QLLaMA利用其大规模参数重组视觉表示并生成文本。\",\"多模态对话：连接LLM解码器（InternVL-Chat），支持两种配置（图4c/d）。\"]},\"190\":{\"h\":\"\",\"t\":[\"1. 视觉-语言对比训练（Vision-Language Contrastive Training）\",\"目标：初步对齐视觉编码器（InternViT-6B）和文本编码器（LLaMA-7B）。\",\"数据：使用大规模但噪声较多的公开网络图像-文本对（如 LAION-en、LAION-multi、COYO 等，共 4.98B 样本，表 2）。\",\"方法：\",\"采用 CLIP 风格的对比学习，最小化图像-文本对的对称交叉熵损失。\",\"初始阶段在较低分辨率（196×196）训练，并应用 50% 图像 token 掩码 以提高效率，后期切换至 224×224 分辨率。\",\"效果：使模型在零样本分类、图像-文本检索等对比任务上表现优异，并为后续阶段提供稳健的视觉表示。\",\"2. 视觉-语言生成训练（Vision-Language Generative Training）\",\"目标：增强模型生成能力，进一步对齐视觉与语言特征。\",\"数据：筛选高质量图像-文本数据（1.03B，表 2），去除低质量描述（如重复文本、无意义内容）。\",\"方法：\",\"冻结 InternViT-6B 和 QLLaMA 的预训练权重，仅训练新增的 可学习查询和交叉注意力层。\",\"结合 三种损失函数：\",\"ITC（图像-文本对比损失）\",\"ITM（图像-文本匹配损失）\",\"ITG（基于图像的文本生成损失）\",\"效果：使 QLLaMA 能够有效重组视觉特征，并生成连贯的文本描述（如表 10 的零样本图像描述结果）。\",\"3. 监督微调（Supervised Fine-tuning, SFT）\",\"目标：优化多模态对话能力，连接 LLM 解码器（如 Vicuna、InternLM）。\",\"数据：收集约 400 万高质量指令数据（表 3），涵盖图像描述、VQA、OCR、视觉定位等任务。\",\"方法：\",\"两种配置（图 4c/d）：\",\"仅使用 InternViT-6B，通过 MLP 层连接 LLM（类似 LLaVA）。\",\"使用完整 InternVL（InternViT + QLLaMA），利用其对齐的特征空间提升性能。\",\"由于 QLLaMA 与 LLM 特征空间一致，即使冻结 LLM 解码器，仅微调 MLP 层也能取得良好效果。\",\"效果：在 MME、POPE 等多模态对话基准上达到 SOTA（表 9）。\",\"这一渐进式策略确保模型 从粗粒度对齐过渡到细粒度优化，充分利用不同质量的数据，最终实现强大的多模态理解和生成能力。\"]},\"191\":{\"h\":\"实现细节\",\"t\":[\"第一阶段（Stage 1）\",\"在该阶段，图像编码器 InternViT-6B 是随机初始化的 7，而文本编码器 LLaMA-7B 则使用来自文献 32的预训练权重进行初始化。此阶段中，所有参数都是可训练的。\",\"第二阶段（Stage 2）\",\"在该阶段，InternViT-6B 和 QLLaMA 继承了第一阶段中学习到的权重，而 QLLaMA 中新加入的可学习查询（learnable queries）和跨注意力层（cross-attention layers）是随机初始化的。由于第一阶段中已获得了强大的表示能力，我们在该阶段冻结 InternViT-6B 和 QLLaMA，仅训练新引入的参数。\",\"基座是 LLaMA-7B：QLLaMA 继承了经过第一阶段对比训练后得到的 LLaMA-7B 权重；\",\"新增模块：\",\"96 个 learnable query 向量：用于从视觉特征中提取信息；\",\"Cross-Attention 层：插入到了 LLaMA 的每一层 decoder block 中（这是主流做法，如 BLIP-2 也是如此），使得语言模型具备视觉融合能力；\",\"参数量：新加入模块约为 10 亿参数，占 QLLaMA 总体 8B 的一部分；\",\"第三阶段（Stage 3）\",\"此阶段有两种不同的配置方式：\",\"一种是单独使用 InternViT-6B，如图 4(c) 所示；\",\"另一种是同时使用完整的 InternVL 模型，如图 4(d) 所示。\"]},\"192\":{\"h\":\"实验\"},\"193\":{\"h\":\"\",\"t\":[\"图像分类（Image Classification）：\",\"InternViT-6B 在 ImageNet-1K 及其多个变种（如 IN-A、IN-R、IN-V2 等）上进行线性探测评估。结果显示，其在冻结骨干网络的前提下，取得了领先的零样本分类准确率，平均精度达到 82.5%，超过了如 OpenCLIP-G、EVA-01-CLIP-g 等主流模型。\",\"语义分割（Semantic Segmentation）：\",\"在 ADE20K 上进行语义分割测试，在不同微调策略下（线性探测、Head Tuning、全量微调），InternViT-6B 都展现出更强的像素级感知能力。例如，在全参数微调下，mIoU 达到 58.9%，显著优于 ViT-22B（55.3%）。\"]},\"194\":{\"h\":\"\",\"t\":[\"零样本图像分类（Zero-Shot Image Classification）：\",\"在多语言版本的 ImageNet 上（EN, ZH, JP, AR, IT），InternVL-C 的表现优于 OpenCLIP-XLM-R 和其他多语言模型，展示了良好的语言泛化能力。\",\"零样本图像-文本检索（Image-Text Retrieval）：\",\"InternVL-C 和 InternVL-G 在英中双语的 Flickr30K / COCO / Flickr30K-CN / COCO-CN 上均取得 SoTA 表现，InternVL-G 的 Recall@1 在 COCO 图像→文本检索任务中达到 85.0%，在多语言图像→文本检索任务 XTD 中，Recall@10 平均可达 96.6%，显著超越现有方法。\",\"零样本图像字幕生成（Image Captioning）：\",\"InternVL-G 在不使用指令微调的前提下，仅通过 QLLaMA 即可生成高质量图像描述。例如在 COCO 测试集上，zero-shot CIDEr 得分达到 128.2，超越如 BLIP-2、Qwen-VL 等多模态生成模型。\"]},\"195\":{\"h\":\"\",\"t\":[\"InternVL-Chat 在多模态对话基准（如 MME、POPE）上超越了多个 SoTA 模型。比如，在 MME 综合指标上，InternVL-Chat（13B + QLLaMA）达到 1586.4 分，优于 LLaVA-1.5 和 InstructBLIP 等方法。\",\"此外，InternVL 的多模态对话能力还体现在：\",\"VQA 子任务上：GQA 得分达 59.5（优于 LLaVA-13B 的 63.3）；\",\"图像字幕、OCR、视觉推理任务中均表现稳定，兼具理解和生成能力。\"]},\"196\":{\"h\":\"\",\"t\":[\"视觉主干设计选择（InternViT-6B）：\",\"作者在不同模型深度、宽度、MLP 比例等超参数组合上进行对比试验，最终选择了参数约为 5.9B 的 variant 3 作为 InternViT-6B 版本，在计算成本和准确率之间取得了良好平衡。\",\"QLLaMA 的重要性验证：\",\"通过最小化配置（仅训练 MLP 层）进行对比，发现使用 QLLaMA 作为 glue 层明显优于传统 MLP 层或 QFormer，在对话任务（如 MME、OKVQA、GQA）上均有显著提升。例如 MME 得分从 1022.3（无 QLLaMA）提高至 1317.2（使用 QLLaMA 和 Vicuna-13B）。\"]},\"197\":{\"h\":\"总结\",\"t\":[\"InternVL 的实验结果充分证明了其设计策略的有效性：\",\"大型视觉编码器（InternViT-6B）具备极强的感知能力；\",\"QLLaMA 显著提升了视觉-语言对齐与生成能力；\",\"多阶段训练策略（对比 + 生成 + 指令微调）保障了模型的通用性与灵活性；\",\"在图像分类、文本检索、VQA、多模态对话等任务上全面领先于现有开源模型，是当前最具代表性的通用多模态基础模型之一。\"]},\"198\":{\"h\":\"结论\",\"t\":[\"通过将视觉基础模型扩展到 60 亿参数规模（InternViT-6B），并与一个由 LLaMA 初始化的语言中间件（QLLaMA）进行渐进式对齐，InternVL 构建了一个强大且通用的视觉-语言基础模型。借助海量图文数据和多阶段训练策略（对比、生成、微调），InternVL 实现了在图像分类、图文检索、图像描述、VQA、多模态对话等任务上的领先性能，成功弥合了视觉模型与大型语言模型之间的能力与表示鸿沟，推动了多模态大模型的发展。\"]},\"199\":{\"h\":\"详细训练设置(附录内容)\"},\"200\":{\"h\":\"\",\"t\":[\"如表20所示，在此阶段：\",\"图像编码器（InternViT-6B） 采用 BEiT 的初始化方法随机初始化，文本编码器（LLaMA-7B） 则加载多语言 LLaMA-7B 的预训练权重。所有参数均参与训练。\",\"优化器使用 AdamW，超参数为 β1=0.9、β2=0.95，权重衰减 0.1，学习率采用余弦退火策略（图像编码器初始 1e-3，文本编码器 1e-4）。\",\"采用 DropPath 率 0.2，总批量大小 164K，在 640 张 A100 GPU 上训练 175K 步，处理约 287 亿样本。\",\"为提升效率，初期在 196×196 分辨率 下训练并 掩码 50% 图像 token，后期切换至 224×224 分辨率 并取消掩码（最后 5 亿样本）。\"]},\"201\":{\"h\":\"\",\"t\":[\"InternViT-6B 和 QLLaMA 继承第一阶段权重，新增的可学习查询和交叉注意力层 随机初始化。\",\"冻结主干网络，仅训练新增参数，输入分辨率保持 224×224。\",\"优化器使用 AdamW（β1=0.9、β2=0.98，权重衰减 0.05），总批量大小 20K，在 160 张 A100 GPU 上训练 80K 步（含 2K 步热身），峰值学习率 5e-5。\"]},\"202\":{\"h\":\"\",\"t\":[\"提供两种配置：\",\"InternVL-Chat（不含 QLLaMA）\",\"类似 LLaVA-1.5，先以 LGS-558K 数据集 训练 MLP 层，再用 LLaVA-Mix-665K 微调 LLM，各训练 1 轮。\",\"InternVL-Chat（含 QLLaMA）\",\"分两步：先用自定义 SFT 数据训练 MLP 层，再微调 LLM。因数据量扩大，批量大小增至 512。\"]},\"203\":{\"h\":\"\",\"t\":[\"所有参数可训练，分别在 Flickr30K 和 Flickr30K-CN 上微调。\",\"分辨率 364×364，采用 分层学习率衰减（0.9） 和 DropPath 率 0.3。\",\"使用 AdamW，批量大小 1024，训练 10 轮。\"]},\"204\":{\"h\":\"\",\"t\":[\"遵循常规做法：\",\"用 BatchNorm 归一化特征，拼接平均池化的 patch token 和类别 token。\",\"线性分类头使用 SGD 训练 10 轮（批量 1024，峰值学习率 0.2，1 轮热身，无权重衰减），数据增强包括随机裁剪和翻转。\"]},\"205\":{\"h\":\"\",\"t\":[\"表23列出了三种配置的超参数：\",\"线性探测（Linear Probing）\",\"头部调优（Head Tuning）\",\"全参数调优（Full-parameter Tuning）\"]},\"206\":{\"h\":\"书生·万象多模态大模型（InternVL 1.5）\",\"t\":[\"How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites 论文简析\",\"论文链接: How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites\"]},\"207\":{\"h\":\"摘要\",\"t\":[\"InternVL 1.5 是一个开源的多模态大语言模型（MLLM），旨在缩小开源模型与商业多模态模型（如 GPT-4V）之间的性能差距。其核心改进包括以下三点：\",\"强大的视觉编码器：通过持续学习策略优化大规模视觉基础模型 InternViT-6B，提升其视觉理解能力，并使其能够适配不同的语言模型（LLMs）。\",\"动态高分辨率处理：根据输入图像的长宽比和分辨率，将其动态分割为 1 到 40 个 448×448 像素的图块，最高支持 4K 分辨率输入，同时保留全局缩略图以捕捉上下文信息。\",\"高质量双语数据集：精心构建了一个涵盖常见场景和文档图像的双语数据集（中英文问答对），显著提升了模型在 OCR 和中文相关任务中的表现。\",\"实验结果表明，InternVL 1.5 在 18 个多模态基准测试中表现优异，其中 8 项达到领先水平，尤其在 OCR 相关任务中表现突出。其性能与商业模型（如 GPT-4V、Gemini 系列等）相当，部分任务甚至超越商业模型。这一成果为开源多模态模型的发展提供了重要支持。\"]},\"208\":{\"h\":\"简介\",\"t\":[\"研究背景与问题: 大型语言模型（LLMs）在推动通用人工智能（AGI）方面发挥了重要作用，而多模态大型语言模型（MLLMs）进一步扩展了文本与视觉信息的交互能力。然而，开源模型与商业专有模型（如GPT-4V、Gemini系列和Qwen-VL-Max）之间仍存在显著差距，主要体现在三个方面：\",\"参数规模：商业模型通常具有超过1000亿参数，而开源模型的视觉基础模型（VFM）通常仅3亿参数，搭配70亿或130亿参数的LLMs。\",\"图像分辨率：商业模型支持动态分辨率以保留原始宽高比，而开源模型多采用固定分辨率（如336×336或448×448），限制了细节理解能力。\",\"多语言能力：商业模型通过多语言数据集训练，而开源模型主要依赖英语数据，其他语言任务表现较差（如OCR和中文场景理解）。\",\"解决方案与创新: 论文提出InternVL 1.5，通过以下改进缩小差距：\",\"强大的视觉编码器：基于InternViT-6B的持续学习策略，增强视觉理解能力并适配不同LLMs。\",\"动态高分辨率处理：将图像分割为1至40个448×448像素的区块（支持4K分辨率），并添加缩略图以保留全局上下文（见图4）。\",\"高质量双语数据集：涵盖常见场景和文档图像，通过中英文问答对标注，显著提升OCR和中文任务性能（见表1）。\",\"模型优势\",\"灵活分辨率：类似GPT-4V的“低/高”模式，用户可根据任务需求选择分辨率（如低分辨率用于场景描述，高分辨率用于文档分析）。\",\"双语能力：在中文任务中表现优于GPT-4V（见图1）。\",\"强视觉表征：InternViT-6B的大参数规模使其视觉表征能力媲美200亿参数的LLMs，实现多模态能力的协同提升（见图2）。\",\"性能验证: InternVL 1.5在18个多模态基准测试中表现优异，在8个任务中达到SOTA，尤其在OCR相关任务（如TextVQA、ChartQA）中超越商业模型（见表2）。研究团队开源模型权重，以促进MLLM社区发展。\"]},\"209\":{\"h\":\"相关工作\"},\"210\":{\"h\":\"\",\"t\":[\"商业模型在多模态领域占据领先地位，主要代表包括：\",\"GPT-4V（OpenAI）：扩展GPT-4的视觉能力，支持文本和图像输入。\",\"Gemini 系列（Google）：从1.0到1.5版本，支持文本、图像和音频，并扩展至100万tokens的上下文窗口。\",\"Qwen-VL-Plus/Max（阿里）：在无需OCR工具的情况下展现强大的多模态能力。\",\"Claude-3V、HPT Pro、MM1、Step-1V、Grok-1.5V 等新兴模型进一步推动多模态技术的发展。\",\"这些模型的优势在于大规模参数、动态分辨率支持和多语言优化，但通常不开源，限制了研究社区的应用和优化。\"]},\"211\":{\"h\":\"\",\"t\":[\"开源模型在视觉-语言任务中取得显著进展，代表性工作包括：\",\"LLaVA 系列、MiniGPT-4、Qwen-VL、CogVLM 等，主要采用固定分辨率（如336×336或448×448），导致在非常规宽高比或文档理解任务上表现受限。\",\"高分辨率优化方法：\",\"双分支视觉编码器（如LLaVA-HR、DeepSeek-VL），结合低分辨率和高分辨率特征。\",\"分块策略（如UReader），将高分辨率图像分割为多个低分辨率区块处理。\",\"尽管这些方法有所改进，但开源模型在文档、图表和场景文本理解方面仍显著落后于商业模型。\"]},\"212\":{\"h\":\"\",\"t\":[\"VFMs 是 MLLMs 的核心组件，当前研究重点包括：\",\"CLIP-ViT 和 SigLIP 是主流选择，但它们在非互联网图像（如文档）上的表现较差。\",\"混合特征方法（如结合CLIP和DINOv2）提升视觉表征能力。\",\"双编码器设计（如DeepSeek-VL 使用 SigLIP 和 SAM-B）优化不同分辨率输入。\",\"本文提出的 InternViT-6B 通过持续学习策略增强视觉理解能力，并适配不同LLMs，提升模型的泛化性。\",\"商业模型在规模和性能上领先，但开源模型通过高分辨率优化、数据增强和更强的视觉编码器（如InternViT-6B）逐步缩小差距。InternVL 1.5 的创新点在于动态分辨率、双语数据集和持续学习的视觉编码器，使其在OCR和中文任务上表现优异。\"]},\"213\":{\"h\":\"方法\"},\"214\":{\"h\":\"\",\"t\":[\"InternVL 1.5 采用经典的 \\\"ViT-MLP-LLM\\\" 架构（见图3），主要包含以下组件：\",\"视觉编码器：基于 InternViT-6B（45层），通过持续学习优化，支持高分辨率输入。\",\"语言模型：采用 InternLM2-20B（聊天版本），提供强大的语言理解能力。\",\"动态分辨率策略：训练时根据输入图像的宽高比和分辨率，将图像分割为 1~12个448×448区块，测试时可扩展至 40区块（4K分辨率），并引入缩略图保留全局信息。\",\"Token压缩：使用 Pixel Shuffle 操作将视觉Token数量减少至1/4（如448×448图像对应256个Token），提升计算效率。\",\"MLP投影层: 随机初始化\"]},\"215\":{\"h\":\"\",\"t\":[\"现有 MLLM 通常采用对比学习预训练的 ViT 模型作为视觉基础模型。然而，这些 ViT 模型通常在固定低分辨率（如224×224）的互联网爬取图像-文本对上训练，因此在处理高分辨率图像或非互联网来源图像（如文档图像）时性能会下降。\",\"InternViT-6B-448px-V1.2：\",\"为解决这一问题，我们在InternVL 1.2版本中对InternViT-6B模型进行了持续预训练。首先，我们发现倒数第四层的特征在多模态任务中表现最佳，因此直接移除了最后三层的权重，将模型层数从48层缩减至45层。\",\"随后，我们将分辨率从224提升至448，并将其与Nous-Hermes-2-Yi-34B结合。为赋予模型高分辨率处理和OCR能力，我们在训练中同时激活视觉编码器和MLP投影层，使用了混合的图像描述和OCR专用数据集。\",\"InternViT-6B-448px-V1.5：\",\"InternVL 1.5的开发基于InternViT-6B-448px-V1.2的强健基础进一步预训练。在此版本中，训练图像的分辨率从固定的448×448扩展为动态的448×448，其中基础图块大小为448×448，图块数量为1至12个。此外，我们还提升了预训练数据集的规模、质量和多样性，最终使1.5版本模型具备了强大的鲁棒性、OCR能力和高分辨率处理能力。\",\"语言模型从Nous-Hermes-2-Yi-34B更换为InternLM2-20B，但InternViT仍展现出与新语言模型的优秀兼容性和可移植性。这表明，InternViT-6B在MLLM预训练阶段学习到的视觉特征具有广泛适用性，并不依赖于特定的语言模型\"]},\"216\":{\"h\":\"\",\"t\":[\"受UReader启发，我们采用动态高分辨率训练策略，有效适应输入图像的不同分辨率和宽高比。该方法通过灵活分割图像图块，在保留细节信息的同时兼容多样化的图像分辨率。主要步骤如下：\",\"动态宽高比匹配：从35种预设宽高比中选择最接近输入图像的配置，避免过度拉伸。\",\"分块与缩略图：\",\"图像调整至目标分辨率（如800×1300 → 896×1344）后分割为448×448区块。\",\"额外添加448×448缩略图以保留全局上下文。\",\"训练与测试灵活性：训练时最多12区块（3,328 Token），测试时支持40区块（10,496 Token）。\"]},\"217\":{\"h\":\"\",\"t\":[\"预训练数据（53.9% 图像描述 + 32% OCR数据）：\",\"涵盖 Laion-EN/ZH、COYO、GRIT 等通用数据集，以及 Wukong-OCR、Common Crawl PDFs 等大规模OCR数据。\",\"使用PaddleOCR生成中英文文本标注，增强模型文字识别能力。\",\"微调数据：\",\"包括 TextCaps、ShareGPT4V（双语描述）、DocVQA、ChartQA 等任务专用数据。\",\"通过翻译管道（图5）将英文数据转为中文，提升多语言支持。\",\"InternVL 1.5 通过 强视觉编码器、动态分辨率策略 和 双语数据集，显著提升了开源模型在OCR、中文任务和高分辨率场景下的性能，缩小了与商业模型的差距。其模块化设计（如InternViT-6B的兼容性）为后续研究提供了灵活的基础。\"]},\"218\":{\"h\":\"实验\"},\"219\":{\"h\":\"\",\"t\":[\"InternVL 1.5 基于 InternViT-6B（视觉编码器）和 InternLM2-20B-Chat（聊天版语言模型）构建，采用 动态高分辨率策略：\",\"训练阶段：图像分割为 1~12个448×448图块\",\"测试阶段：支持零样本扩展至 40图块（4K分辨率）\",\"两阶段训练：\",\"（1）预训练视觉编码器+MLP投影器\",\"（2）全模型微调（260亿参数）\",\"技术配置：上下文长度4096，响应格式与 LLaVA 1.5 一致，评估工具 VLMEvalKit\"]},\"220\":{\"h\":\"\",\"t\":[\"（1）OCR相关任务\",\"文档理解（DocVQA）、图表解析（ChartQA）、场景文本（TextVQA）等任务表现优异\",\"关键优势：在ChartQA和OCRBench上超越所有商业模型（如GPT-4V、Gemini系列）\",\"（2）通用多模态任务\",\"中文能力突出：在MMBench-CN、CCBench等中文基准上大幅领先\",\"幻觉控制：HallusionBench分数最高\",\"科学理解：AI2D科学图表任务表现接近商业模型\",\"（3）数学推理\",\"MathVista基准：超越GPT-4V，展示强大的数学-视觉联合推理能力\",\"（4）多轮对话\",\"ConvBench评估显示：在开源模型中领先，但较GPT-4V仍有差距\"]},\"221\":{\"h\":\"\",\"t\":[\"（1）视觉编码器规模的影响\",\"实验证明：大语言模型（如34B参数）需搭配大规模视觉编码器（如6B参数），才能更好处理复杂多模态任务\",\"（2）动态分辨率的作用\",\"OCR任务（如DocVQA）：高分辨率（更多图块）显著提升性能\",\"通用任务（如MMMU）：分辨率过高可能略微降低效果\",\"灵活性：模型可自适应调整分辨率，平衡效率与精度\"]},\"222\":{\"h\":\"结论\",\"t\":[\"InternVL 1.5作为开源多模态大语言模型，通过持续优化的视觉编码器InternViT-6B、创新的动态高分辨率处理策略（支持4K输入）和高质量双语数据集，在18个多模态基准测试中展现出媲美商业模型的性能，尤其在OCR相关任务（TextVQA/ChartQA/DocVQA）和中文理解方面表现突出，部分能力甚至超越GPT-4V，其开源的模型权重和研究方法为多模态AI发展提供了重要基准，未来将持续优化对话和推理能力，推动技术民主化进程。\"]},\"223\":{\"h\":\"LLaVA 1.0(Large Language and Vision Assistant)\",\"t\":[\"LLaVA 1.0 : Large Language and Vision Assistant 论文简析\",\"论文链接: https://arxiv.org/abs/2304.08485 代码链接: https://github.com/haotian-liu/LLaVA\"]},\"224\":{\"h\":\"背景\",\"t\":[\"此前，大型语言模型（如 GPT-3、LLaMA）通过机器生成的指令数据进行调优，显著提升了零样本和少样本泛化能力（如 InstructGPT、FLAN-T5 等）。\",\"InstructGPT 是由 OpenAI 提出的一种通过 人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF） 来实现 指令调优（Instruction Tuning） 的方法。 其目标是让预训练语言模型（如 GPT-3）更好地理解和执行用户给出的自然语言指令，从而提升其在各种任务上的泛化能力，尤其是零样本（zero-shot）或多任务场景下的表现。 InstructGPT 的核心思想是：通过结合人工标注数据和强化学习，引导语言模型更好地遵循用户指令，并在多种任务上表现良好。 它不是单纯地“记住”训练数据中的例子，而是学会根据用户指令理解任务意图并生成合适的结果。 InstructGPT 的 instruction tuning 实现主要包括以下三个关键阶段： 步骤1：收集指令-响应对（Instruction-Following Data）\",\"OpenAI 收集了大量的人类编写的 指令（instruction） 和对应的 期望输出（response）。\",\"这些指令可以是开放式的（如“写一个关于猫的故事”），也可以是特定任务（如“翻译成中文”、“总结文章”）。\",\"数据来源包括：\",\"用户提交给 GPT-3 的 API 请求；\",\"内部标注人员手动构造的示例。\",\"目标：构建一个多样化的指令-响应数据集，用于训练或评估模型。\",\"步骤2：训练监督模型（Supervised Policy）\",\"使用标注好的指令-响应数据对模型进行微调（fine-tune）。\",\"输入是一个指令，输出是模型应该生成的响应。\",\"模型结构与原始 GPT-3 相同，只是参数经过调整以更好响应指令。\",\"步骤3：基于人类反馈的强化学习（RLHF）, 这是 InstructGPT 最具创新性的部分。具体分为三步:\",\"收集人类偏好数据\",\"对于同一个指令，让模型生成多个不同的回答；\",\"让人类标注者对这些回答进行排序，选出他们认为最好的答案。\",\"训练奖励模型（Reward Model）\",\"使用上述人类偏好数据，训练一个奖励模型（Reward Model），该模型的输入是一对（指令 + 回答），输出是对这个回答的评分（score）。\",\"奖励模型的目标是模拟人类的偏好判断。\",\"使用强化学习优化策略（Policy Optimization）\",\"使用 PPO（Proximal Policy Optimization） 等强化学习算法，以奖励模型为“环境”，进一步微调模型。\",\"在训练过程中，模型尝试生成尽可能高奖励的回答，从而更贴近人类期望。\",\"《 Visual Instruction Tuning 》 这篇论文首次尝试使用仅支持文本输入的 GPT-4 / ChatGPT 来生成图文结合的指令响应对（instruction-following data） ，并用这些数据训练一个端到端的视觉语言模型 LLaVA。\",\"论文核心创新点: 这是第一个系统性地将 NLP 中的指令调优思想引入多模态领域的研究。\"]},\"225\":{\"h\":\"方法\",\"t\":[\"作者将模型训练分为两个阶段 ：\",\"预训练阶段（Feature Alignment Pre-training）: 让视觉编码器提取的图像特征与语言模型的词嵌入空间对齐 , 也就是说：让模型理解图像和文本之间的语义关系, 这是后续指令调优的基础。\",\"微调阶段（End-to-End Fine-tuning）：在预训练的基础上，进一步训练模型理解和执行更复杂的视觉指令任务。\",\"多轮对话能力；\",\"复杂推理能力；\",\"科学问答等实际应用任务。\"]},\"226\":{\"h\":\"预训练\",\"t\":[\"预训练是 LLaVA 模型训练的第一阶段，目标让视觉编码器输出的图像特征与语言模型的词向量空间对齐 ，使得后续指令调优时，模型可以更好地理解和生成图文结合的内容。\",\"作者使用的是大规模图文对数据集 CC3M（Conceptual Captions 3M） ，包含约 300 万条图文对。\",\"为了提升数据质量，进行了以下筛选： 名词短语过滤（Noun Phrase Filtering）\",\"使用 Spacy 提取每条 caption 中的名词短语；\",\"统计每个名词短语出现的频率；\",\"去除频率小于 3 的短语（避免罕见组合）；\",\"对于频率大于 100 的短语，只保留最多 100 条描述（防止过拟合）；\",\"最终得到约 595,000 条高质量图文对 。\",\"数据构建方式: 为了模拟用户提问和模型回答的形式，将这些图文对转换为如下格式：\",\"Human: [指令] [图像描述] Assistant: [详细描述]\",\"其中：\",\"[指令] ：如“请描述这张图片。”、“图中有什么？” [图像描述] ：来自 caption 或 bounding box 的文本化表示； [详细描述] ：期望的回答，通常是图像内容的全面视觉描述。\",\"Caption: 图像的文字描述，从多个角度描述图像内容 , 如: \\\"A group of people standing outside of a black vehicle with various luggage.\\\" Bounding Box: 标注图像中的物体及其位置 , 如: person:[0.681, 0.242, 0.774, 0.694], backpack:[0.384, 0.696, 0.485, 0.914] .\",\"模型结构:\",\"视觉编码器 ：CLIP ViT-L/14（预训练好的）\",\"语言模型 ：Vicuna（基于 LLaMA 的指令调优版本）\",\"投影层 ：一个简单的线性层，连接视觉特征和语言嵌入空间\",\"LLaVA模型结构\",\"训练流程:\",\"输入图像 : 使用 CLIP 视觉编码器提取图像特征 。\",\"投影层 : 将 转换为语言模型可用的 token 序列 。\",\"训练目标: 使用交叉熵损失函数，最小化语言模型输出与真实答案之间的差异 。\",\"仅更新投影矩阵 ，保持视觉编码器和语言模型参数冻结。这个阶段相当于在语言模型的词空间中“训练出一个能看懂图的视觉分词器”。\",\"通过这个阶段训练后，模型已经具备基本的视觉理解能力，即：\",\"可以根据图像描述生成合理的文字解释；\",\"实现了图像与语言之间的初步语义对齐；\",\"为下一阶段的端到端微调提供了良好的初始化。\",\"虽然还不能执行复杂的推理任务，但已经可以处理基本的图文问答任务。\"]},\"227\":{\"h\":\"微调\",\"t\":[\"微调过程 是 LLaVA 模型训练的第二阶段，目标是让模型在预训练的基础上进一步掌握多模态指令理解与复杂推理能力 ，具体包括：\",\"支持多轮视觉对话（Multimodal Chat）\",\"理解并回答科学类问题（如 ScienceQA 数据集）\",\"执行复杂的视觉推理任务\",\"具备跨模态交互能力（图像 + 文本）\",\"这是实现“通用视觉助手”的关键一步。\",\"微调阶段使用的是作者自己构建的高质量多模态指令数据集：\",\"名称：LLaVA-Instruct-158K\",\"包含约 158,000 条图文对\",\"分为三种响应类型：\",\"对话型（Conversation） ：58,000 条\",\"详细描述型（Detailed Description） ：23,000 条\",\"复杂推理型（Complex Reasoning） ：77,000 条\",\"这些数据由 GPT-4 / ChatGPT 自动生成，涵盖多种任务类型，具有高度多样性和挑战性。\",\"微调阶段的数据组织方式如下：\",\"Xsystem-message <STOP> Human: X1instruct <STOP> Assistant: X1a <STOP> Human: X2instruct <STOP> Assistant: X2a <STOP> ...\",\"其中：\",\"Xsystem-message：系统提示语（如：“你是一个视觉助手”）； Xinstruct：用户提问或指令； Xa：期望的回答； <STOP>：分隔符，表示输入结束，开始输出回答。\",\"模型结构:\",\"视觉编码器 ：CLIP ViT-L/14（保持冻结）\",\"语言模型 ：Vicuna（基于 LLaMA 的指令调优版本）\",\"投影层 ：连接图像特征和语言嵌入空间的线性层\",\"训练流程:\",\"输入图像 : 使用 CLIP 提取图像特征 \",\"投影层 : 使用可训练的投影矩阵 将图像特征 转换为语言嵌入 \",\"训练目标: 最小化语言模型输出与真实答案之间的交叉熵损失。\",\"微调时保持视觉编码器参数不变，只更新投影层 和语言模型 Vicuna 的参数。\",\"论文中重点测试了以下两个应用场景：\",\"多模态聊天机器人（Multimodal Chatbot）： 使用 LLaVA-Instruct-158K 数据集进行训练；\",\"其中：\",\"对话型问答为多轮对话；\",\"其他两类为单轮对话；\",\"数据均匀采样，训练出一个能自然理解图像内容、并进行视觉对话的 AI 助手。\",\"科学问答（Science QA）：在 ScienceQA 数据集上进行迁移学习；\",\"每个问题包含文本或图像上下文；\",\"助手需要生成推理过程，并从多个选项中选择正确答案；\",\"在这个任务上，LLaVA 达到了 90.92% 准确率 ；\",\"当与 GPT-4 联合推理时，准确率达到 92.53% ，刷新该数据集 SOTA。\"]},\"228\":{\"h\":\"联合 GPT-4 的推理机制（Ensemble with GPT-4）\",\"t\":[\"作者还提出了一种创新方法，将 LLaVA 与 GPT-4 联合使用：\",\"方法一：GPT-4 补充\",\"当 GPT-4 无法回答时，使用 LLaVA 的预测结果；\",\"效果：准确率提升不大（仅 0.05%），说明 LLaVA 已经接近其上限。\",\"方法二：GPT-4 判断者（Judge）\",\"当 LLaVA 和 GPT-4 输出不一致时，再次用 GPT-4 做判断；\",\"效果：显著提升表现，最终准确率达到 92.53% ，刷新 ScienceQA 数据集的 SOTA。\",\"这是首次尝试将大语言模型用于模型集成（model ensemble）的研究。\"]},\"229\":{\"h\":\"ablation study（消融实验）\",\"t\":[\"论文中还进行了多项 ablation 实验，以分析不同训练策略的影响：\",\"训练策略\",\"准确率变化\",\"不做预训练\",\"-5.11%\",\"仅使用最后一层视觉特征\",\"-0.96%\",\"先生成答案再推理\",\"-1.15%\",\"使用较小的 7B 模型\",\"-1.08%\",\"这些实验表明：\",\"预训练阶段非常关键；\",\"使用倒数第二层视觉特征更有利于细节理解；\",\"推理优先（Reasoning First）有助于加快收敛；\",\"模型规模对性能有显著影响。\"]},\"230\":{\"h\":\"补充\"},\"231\":{\"h\":\"辨析 instruction tuning 和 prompt tuning\",\"t\":[\"Instruction Tuning（指令调优） 和 Prompt Tuning（提示调优） 是两种用于提升预训练语言模型（LLM）或视觉语言模型性能的技术，但它们的目标、方法和应用场景有显著区别。以下是两者的主要区别：\",\"定义与核心思想\",\"类别\",\"Instruction Tuning（指令调优）\",\"Prompt Tuning（提示调优）\",\"定义\",\"通过大量“指令-响应”对微调模型，使其更好地理解和执行用户给出的自然语言指令。\",\"在输入中添加可学习的前缀（prefix）或前缀/后缀（prompt），引导模型生成特定任务的结果，而不需要改变整个模型参数。\",\"核心思想\",\"模型要理解并遵循人类语言中的任务描述（如“总结一下这篇文章”）。\",\"模型通过在输入前后插入一些可训练的提示词来“唤醒”其已有的知识，完成特定任务。\",\"训练方式\",\"类别\",\"Instruction Tuning\",\"Prompt Tuning\",\"是否修改模型结构\",\"否（通常保留原始结构）\",\"否\",\"是否更新全部参数\",\"是（微调整个模型参数）\",\"否（仅更新插入的 prompt 参数，其余参数冻结）\",\"数据需求\",\"需要大量人工或机器生成的“指令-响应”对\",\"不需要额外标注数据，直接使用原始任务描述\",\"训练目标\",\"提升模型在各种任务上的泛化能力，尤其是零样本/少样本任务迁移\",\"让固定模型适应新任务，利用已有知识进行推理\",\"应用场景举例\",\"类别\",\"示例场景\",\"Instruction Tuning\",\"ChatGPT、InstructGPT、FLAN-T5、LLaVA（视觉+语言）等，能根据用户指令回答问题、写故事、编程、推理等。\",\"Prompt Tuning\",\"使用 [PROMPT] 前缀让 BERT 回答 QA 问题、分类任务；在图像识别中加入 learnable prefix 来适配不同类别。\",\"优缺点对比:\",\"对比维度\",\"Instruction Tuning\",\"Prompt Tuning\",\"优点\",\"- 更强的任务泛化能力- 更贴近真实用户交互- 可用于多模态任务\",\"- 参数效率高（只训练少量 prompt）- 可复用已有大模型权重\",\"缺点\",\"- 数据依赖性强（需要大量高质量指令数据）- 微调成本高（需训练整个模型）\",\"- 表达能力受限于 prompt 的设计- 泛化性不如 instruction tuning\",\"总结一句话： Instruction Tuning 是教会模型“听懂人话”，按指令做事；Prompt Tuning 是引导模型“激活已有知识”，通过提示词让它自己做任务。\"]},\"232\":{\"h\":\"MoCo 论文\",\"t\":[\"Momentum Contrast for Unsupervised Visual Representation Learning 论文简析\",\"论文链接: Momentum Contrast for Unsupervised Visual Representation Learning 代码链接: https://github.com/facebookresearch/moco\"]},\"233\":{\"h\":\"Introduction\",\"t\":[\"对比学习从2019年开始到现在一直都比较火，Moco是视觉领域使用对比学习一个里程碑的工作。\",\"Moco作为一个无监督的表征学习工作，不仅在分类任务上逼近了有监督的基线模型，在其他任务，如检测、分割、人体关键点检测上都超越了有监督的预训练模型，也就是ImageNet上的预训练模型；\",\"Moco证明了一点，无监督学习真的可行，我们并不需要大量标注好的数据；\"]},\"234\":{\"h\":\"What is contrast learning?\",\"t\":[\"首先说对比学习想要做到什么呢？我们现在有三张图，第一张图是人高兴，第二张图片是人悲伤，第三张图片是狗。\",\"我们想得到一个结果，就是我们不需要知道前两张图片是人这个类别，不需要知道第三张图片是狗这个类别。但是我们需要知道前两张图片是一个类别，第三张图片不是一个类别。\",\"换句话说，我们现在把这三张图片输入一个模型，得到三个表征，我们需要让这三个表征在特征空间中，前两张图片的表征距离比较近，第三张图片和它们的距离比较远。\",\"一句话说，我们希望在特征空间里，同一个类别的物体处于相邻的区域，不同类别的物体处于不相邻的区域。\",\"在这个过程中，我们需要知道的是，我们并没有用到标签信息，我们不需要知道第一张和第二张图片是人，第三张是狗。\",\"但是我们用到了另外一种信息，就是第一张图片和第二张图片是同一个类别，第三张图片不是同一个类别的信息。这其实也是一种标签信息。\",\"不过这种标签信息，我们可以使用一些代理任务，巧妙构造出来，而不需要人为地去标注这种标签信息。这些代理任务，会去定义一些规则，这些规则可以去定义哪些图片是相似的，哪些图片是不相似的，从而可以提供一些监督信号给到模型去训练。这个过程其实也是自监督训练的一个过程。\"]},\"235\":{\"h\":\"instance discrimination task\",\"t\":[\"一个最经典的代理任务就是：instance discrimination，叫做个体判别。\",\"这个代理任务是指，如果我们有一个没有标注的数据集，里面有n个图片。\",\"从这个数据集中，我们随机选择一个图片 ，对这个图片做随机裁剪（或者其他的数据增广操作，我们称之为transformation），从而得到另外两张图；\",\"一个是 一个是 ，这样我们会得到两个不太一样的照片。但是由于这两张图片是从同一个图片经过某种变化得到的，语义信息不应该发生变化。所以这两张图片就可以称之为正样本，也就是同一个类别的图片。\",\"这个代理任务，同时认为，这个数据集中剩余的所有图片都是负样本。\",\"为什么叫做个体判别呢？因为它认为每个图片自成一个类别，剩余的图片都不是同一个类别。\",\"这个粒度其实是很细，你在图片分类的时候很多照片是同一个类别，其余的照片又分为了很多类别，所以个体判别这个代理任务经过模型训练，表征会很细。\",\"对于ImageNet这个数据集来说，如果是个体判别任务，不是一千个类别，而是100多万个类别。\",\"所以个体判别这个代理任务定义了什么是正样本，什么是负样本，接下来就很简单了，我们只需要经过模型，然后使用一个对比学习的函数去训练模型就可以了，比如说NCEloss。\",\"在这个过程中，其实有一个很有意思的点，就是代理任务是多样性的，是很灵活的。只要你能够得到一个判断正样本和负样本的规律，后续的损失函数之类的训练就很常规了。\",\"比如说在视频领域，同一个视频里的任意两帧是正样本，其他视频里的帧是负样本；\"]},\"236\":{\"h\":\"Momentum Contrast\",\"t\":[\"Moco 这个名字就是来源于前两个单词的前两个字母，即基于动量的对比学习。\",\"动量是一种加权移动平均:\",\" 是上一个时刻的输出， 是动量超参数， 是当前时刻的输入。\",\"说白了，就是不想让当前时刻的输出只是依赖于当前时刻的输入，还希望和之前时刻的输出有关系。动量这个超参数是 的一个参数；如果 趋近于 ，那么 的改变会非常缓慢，因为 趋近于零。\",\"Moco 就是利用这个动量的特性，去缓慢地更新编码器，从而让中间学习到的字典特征尽可能保持一致（这句话没看懂没关系，一会详细讲）。\"]},\"237\":{\"h\":\"Abstract\",\"t\":[\"Moco 把对比学习看成了是一个字典查询的过程，构建了一个动态的字典。这个动态的字典分为两个部分：第一部分是一个队列，第二部分是一个移动平均的编码器。\",\"队列里的样本不需要进行梯度回传，因此我们可以往队列里放入很多负样本，从而让字典的规模变得很大。\",\"为什么还要使用一个移动平均的编码器呢？这是为了让字典里的特征尽可能保持一致。\",\"在训练过程中发现，拥有一个规模大且特征较为一致的字典，能让无监督的对比学习取得很好的效果。\",\"从实验结果来看，在 ImageNet 数据集上，如果采用 线性评估（Linear Probing） 进行测试，Moco 可以取得和之前最优的无监督方法相近甚至更好的结果。\",\"线性评估指的是，先预训练好一个骨干模型，然后将这个骨干网络参数冻结，只训练最后的全连接层，再查看在不同数据集上的表现结果。这样做其实类似于把骨干网络当成一个特征提取器，仅从其中提取特征，这和使用 ResNet 作为特征提取器的方式差不多。\",\"Moco 一个很大的优势在于，学习到的特征在下游任务上具有很好的迁移性。我们看重无监督学习的优点，就是它可以从大量未标注的数据上学习到特征，并迁移到标注数据较少的任务上。\",\"Moco 在 7 个下游任务（如分割、检测等）上超越了之前的有监督预训练模型。\"]},\"238\":{\"h\":\"Introduction\",\"t\":[\"GPT和BERT，已经证明无监督学习在NLP任务上是行得通的。但是在CV领域，有监督预训练还是占据主导地位；\",\"之前也有很多优秀的无监督工作，但是表现都会比有监督要差，作者认为这是因为CV领域和NLP领域的原始信号空间不同。\",\"对于NLP领域来说，它们是离散的信号，也就是原始的信号空间是由单词组成，或者更细一点，是由单词词缀组成的，所以我们可以很容易地去建立一个字典，然后让模型去学习特征。那么字典中的每个key就是一个类别，我们可以根据这个类别去学习模型（比如BERT最后进行的softmax操作，不就是分类操作吗）\",\"但是对于CV领域来讲，情况完全不一样。CV领域的信号是在一个连续而且高维的空间，它并不像单词那样有很强的语义信息，也没有浓缩得那么简洁；所以CV领域并不适合去建立一个字典来学习模型；如果没有这个字典，无监督就很难去建模。因此，在CV领域，无监督学习的表现往往不如有监督学习。\",\"在之前有很多优秀的对比学习工作，都可以归纳为一种字典查询的工作。\",\"我们接着来看图：\",\"两个编码器，一个是E11，一个是E12；然后我们将图片x1经过数据增强T1得到图片X11，再经过E11这个编码器，得到图片表征f11；同理，图片x1经过数据增强T2得到图片x12，然后经过E12这个编码器，得到表征f12。\",\"我们把X11这个图片叫做 anchor（锚点），x12叫做x11的正样本。\",\"什么是负样本呢？就是数据集中剩余的所有图片都是负样本。那么负样本走哪个编码器呢？走的是E12这个编码器，因为正样本和负样本都是相对于锚点来说的，所以正样本和负样本要走同一个编码器，从而让特征的获取过程保持一致性。于是，负样本x2、x3、x4等等也经过E12得到了真正的负样本表征f2、f3、fn。\",\"我们把f11叫做 query，把f12、f2、f3、fn叫做 key。\",\"对比学习的过程就是想要在特征空间里，让正样本的key和query距离近，其余的key离query远。\",\"我们其实可以把key集合看成字典。那么对比学习的过程，就是想得到一个模型，让query在字典中与自己匹配的正样本更近。\",\"如果把对比学习的过程看成一个动态字典的过程，若想要得到比较好的效果，那么字典最好需要满足两个条件：第一个是字典足够大，第二个是在训练的时候尽量保持一致性。\",\"首先，我们在做对比学习的时候，是一个batch一个batch地去做，所以如果key这个字典足够大，那么从中抽样的可能性组合就很多，模型的泛化性就很强。如果字典很小，泛化性就不足，相当于数据量不够。\",\"其次，保持一致性是因为我们需要字典中的特征尽可能使用同一个或者相近的编码器进行表征。因为如果不这样做，模型可能就只会学习到和query使用同样编码器的那个key，导致模型泛化性不足，走了捷径。\",\"所以Moco要做的就是：在对比学习框架中，提供一个又大又一致的字典；框架图如下：\",\"大字典是怎么做到的：维护一个队列，把每次训练的 batch-size 和队列大小分离开；具体来说就是这个队列可以很大，但是我们每次更新这个队列，是一点点地更新的，也就是说当我们用一个很小的 batchsize 时，把当前 batch 中的特征加入队列，将最老的 batch-size 的特征从队列中抽离；这样我们的队列就可以设置得很大，比如几万。通过这种方式，使用一个 GPU 也可以很好地训练模型。\",\"那么一致性是如何做到的？刚才说了，每次都是使用新的编码器更新 batch 大小的队列特征，除此之外的特征都是使用之前的编码器得到的，这样不就不一致了吗？这时使用动量更新即可。我们最开始右边分支的编码器是由左边初始化而来，后续更新时对右边这个编码器的参数进行动量更新，让 m 足够大，确保右边编码器更新得非常缓慢，从公式来说，就是这个图：\",\"可以看到，右边编码器会被之前的 编码，和当前时刻的 编码影响。当 足够大，无限接近于 时，就可以认为其无限被 控制，更新会非常缓慢。\",\"Moco 只是建立中间模型的一种方式，它非常灵活，可以和很多代理任务结合，这里使用的是之前讲过的个体判别任务。\",\"无监督学习最大的一个卖点，就是我们的模型在大量无标注的数据集上进行训练之后，得到的特征可以很好地迁移到下游任务中（比如标注数据很少的任务）。\"]},\"239\":{\"h\":\"Conclusion\",\"t\":[\"Moco 论文在 ImageNet 数据集上取得了很好的结果，在 Facebook 自家的数据集上同样效果良好，但提升幅度不大。当数据集规模从 100 万增长到 10 亿时，提升依然不明显。作者认为大规模数据没有被充分利用，可能采用一个更好的代理任务会取得更好的效果。所以作者提出，除了个体判别这个任务外，有没有可能将 MoCo 和 mask encoded 任务结合起来，也就是类似 BERT 的操作，使用 MLM（Masked Language Modeling） 自监督的方式进行学习。（这不就是 MAE 模型吗）\",\"开头提到过 CV 和 NLP 的信号空间不一致，直接照搬 BERT 的方法可能行不通，具体可参考 MAE 模型。\"]},\"240\":{\"h\":\"Related Work\",\"t\":[\"一般来说，自监督学习的方法可以从两个方面来进行优化或创新：\",\"在损失函数上做文章：设计或改进对比损失函数，使得模型能够更有效地学习到有判别性的表示。\",\"在代理任务上做文章：通过设计合理的预训练任务（如图像重建、上下文预测等）引导模型学习有用的特征。\",\"💡 注解：自监督学习其实是一种特殊形式的无监督学习，通过人为构造“标签”来训练模型。\",\"损失函数:\",\"NCE（Noise Contrastive Estimation）损失函数：\",\"最初用于语言模型（如word2vec）中，它的基本思想是把一个“超级大的多分类问题”（即从大量候选中选出正确样本）转化为一系列“二分类问题”（即判断某个样本是否是正样本）。\",\"因为直接在超大类别空间上做 softmax 计算代价太高，所以 NCE 提供了一种更可行的替代方法。\",\"类似于 word2vec 中的 negative sampling 技术。\",\"InfoNCE：\",\"是 NCE 的一种改进或变体，被广泛应用于对比学习中（如MoCo、SimCLR）。\",\"它的目标是在一个由一个正样本和多个负样本构成的集合中，最大化正样本的得分，使得模型能够区分正负样本对。\",\"温度系数（temperature hyperparameter）：\",\"在 InfoNCE 的 softmax 函数中，通常会加入一个 温度超参数 ，用于调节 logits（相似度）的“尖锐程度”：\",\" 越小，softmax 越尖锐，模型对正负样本差异更加敏感；\",\" 越大，softmax 趋于平滑，训练更稳定但区分度下降。\",\"在看 InfoNCE 的损失函数的时候，首先从 softmax 看起，这个是softmax的公式:\",\"然后我们加一个 就是 交叉熵损失函数：\",\"这个公式其实可以直接用在对比学习中。\",\"什么意思呢？ 交叉熵是一个多分类问题的损失函数，一个one-hot向量，和我真实输出做一个损失，目标是为了让真正标签的输出尽可能的大。\",\"那么有一个问题，如果把这个损失函数，直接套到对比学习中去，那么是什么意义呢？\",\"比如 ImageNet 100万个图片，那么我当前图片经过数据增强之后，经过编码器1得到了 锚点特征，经过编码器2得到了正样本，也就是我的 ground-truth；\",\"那么除了我当前这个图片外，其余100万 - 1个图片经过编码器2得到的表征都是负样本，也就是会得到这样一个向量：\",\"1 0 0 0 （1个1 , 100万 - 1个0）\",\"在这个向量上做交叉熵，其实就可以用在对比学习上。\",\"但是这样做 softmax 计算量太大了，像 BERT 这种模型，也就几万个类别，处理起来没啥问题，几百万个类别就太难了。\",\"这个时候 NCE（Noise Contrastive Estimation） 就是一种很好的解决方式，将问题转化为一个二分类问题，也就是现在只有两个类别，一个是正常样本，除此之外的都是噪声样本。\",\"但是这样做的逻辑不太清晰，所以 InfoNCE 就应运而生了。\",\"与其在整个数据集上计算损失，不如抽样一部分数据来计算损失。如果选取的抽样部分过少，就没什么意义，无法模拟整个数据集，所以抽样的部分还是要大一点。此时，字典的大小就很重要，也就是字典的大小就是分母下方的类别数量；在这个过程中，InfoNCE 把 NCE 的一系列二分类问题又转为了多分类问题。\",\" 就是我们的 query 表征，也就是 锚点 那个图片的特征， 就是正样本，分母累加那里的 ，就是我们的负样本数量，分类累加了 项，因为有 个负样本和一个正样本。\",\"温度参数 （希腊字母读音为 \\\"tao\\\"），在蒸馏相关内容里其实提到过，如果 很大，那么 softmax 分布会很平滑，看不出区别，就是把所有的负样本一视同仁，导致模型学习没有轻重；如果 很小，分布会更尖锐，会让模型只关注那个困难的负样本，其实那些负样本很有可能是潜在的正样本，如果模型过度关注这个困难的负样本，会导致模型很难收敛，或者 学习 到的特征不太好泛化。\",\"去除这个温度超参数，InfoNCE 本质就是一个交叉熵损失函数，只不过类别和所有样本相比，做了个近似，进行了一次随机抽样，抽样数量就是字典大小。Moco 伪代码里的 InfoNCE 直接使用的就是交叉熵损失函数代码。\"]},\"241\":{\"h\":\"Detail\",\"t\":[\"有个细节，为什么使用队列这种数据结构存储字典呢？\",\"因为先进先出，每次一个 batch 进来，最老的那个部分 batch 数据会出去，这部分数据是过时的，从而能够保持队列中的特征尽可能的 一致性。\",\"另一个细节：\",\"第二个分支不能随着走这一支的样本使用梯度回传进行更新，为什么呢？因为如果这样做了，第二个分支的编码器就更新得太快了，这些特征，我们是放到字典中去的，就会导致特征 不一致。\",\"为什么第二个分支不直接不更新，而是采用缓慢更新呢？（我自己理解，如果第二个分支一直不变，由于正样本的定义规则是经过编码器之后所在的语义空间才为正样本。在模型训练时可能会出现问题，因为到后来第一个分支和第二个分支编码器差距越来越大，原本是正样本的，损失也会很大，导致模型很难训练。）\",\"两个贡献：\",\"一个是构建很大的字典：计算损失的时候使用多分类，能够很近似在整个数据集上做的多分类损失。\",\"一个是保证字典内特征一致性，使用动量更新。\",\"需要注意的一点：就是 InfoNCE 损失计算的是整个字典做多分类。minibatch 大小和字典大小剥离开，batch 可以设置为 256，然后进来 256 个样本，每个样本都需要做一个 锚点，走一遍对比学习的流程。\",\"动量设置为了 0.99，这个值 很大 了。字典大小是 65536。\",\"在 Moco 之前的工作中， 字典大小和字典特征一致性经常不能同时满足。\"]},\"242\":{\"h\":\"Preview Work\",\"t\":[\"端到端的框架：\",\"端到端的框架 就是两个编码器都可以通过梯度回传进行更新，因为 和 都是从同一个 batch 中来的，我们通过一次 forward 就可以拿到所有样本的特征，我们直接梯度回传就可以了。这要求 batch 大小要足够的大，那么 InfoNCE 才能起作用，做到一个近似的全部数据集的多分类。SIMCLR 就是这个端到端的框架。这样字典是高度一致的。在这种情况下，batch 大小和字典大小是等价的。SIMCLR 就是用了 8192 作为 batch 大小。\",\"另一流派，更关注 字典的大，然后牺牲一些一致性，就是 memory bank；在这个流派只有一个编码器，就是 query 的编码器，可以进行梯度回传进行更新。对于 key 这边，是没有一个单独的编码器。\",\"memory bank 就是把整个数据集的特征，都存到了一起。对于 ImageNet 来说，这里就是 128 万个特征（作者说到，每个特征 128 维度，只需要 600M 的空间，还好。）\",\"然后每次训练的时候，从 memory bank 中随机抽样字典大小就可以了。右边的编码是在线下执行。\",\"在执行的时候，比如字典大小是 3，那么抽出三个来，左边做一次梯度回传之后，我们把字典中的 3 个用新的编码器做一个编码，放回到 memory bank 中去。\",\"（首先，我认为为了保持正样本的定义，肯定得更新样本特征）\",\"因为你这个更新操作，导致字典内编码特征不一致。\"]},\"243\":{\"h\":\"Code Implementation\"},\"244\":{\"h\":\"Train Code\",\"t\":[\"MoCo 训练代码如下所示:\",\"def main_worker(args): # 1. init MoCo Model model = deeplearning.cross_image_ssl.moco.builder.MoCo( models.__dict__[args.arch], args.moco_dim, args.moco_k, args.moco_m, args.moco_t, args.mlp, ) # 2. define loss function (criterion) and optimizer criterion = nn.CrossEntropyLoss().cuda(args.gpu) optimizer = torch.optim.SGD( model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay, ) # 3. Data loading code normalize = transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] ) augmentation = [ transforms.RandomResizedCrop(224, scale=(0.2, 1.0)), transforms.RandomGrayscale(p=0.2), transforms.ColorJitter(0.4, 0.4, 0.4, 0.4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize, ] train_dataset = datasets.ImageFolder( traindir, deeplearning.cross_image_ssl.moco.loader.TwoCropsTransform( transforms.Compose(augmentation) ), ) train_loader = torch.utils.data.DataLoader( train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=args.workers, pin_memory=True, sampler=train_sampler, drop_last=True, ) for epoch in range(args.start_epoch, args.epochs): # 4. train for one epoch train(train_loader, model, criterion, optimizer, epoch, args)\",\"这里的重点是 TwoCropsTransform 这个增强方法，它的作用是对一个样本进行两次增强，得到两个样本。\",\"class TwoCropsTransform: \\\"\\\"\\\"Take two random crops of one image as the query and key.\\\"\\\"\\\" def __init__(self, base_transform) -> None: self.base_transform = base_transform def __call__(self, x): q = self.base_transform(x) k = self.base_transform(x) return [q, k]\",\"此时 train_loader 返回的每个 batch 维度为: [2, batch_size, C, H, W] , 其中 batch[0] 代表 query , batch[1] 代表 positive key。\",\"def train(train_loader, model, criterion, optimizer, epoch, args) -> None: for i, (images, _) in enumerate(train_loader): # 1. compute output output, target = model(im_q=images[0], im_k=images[1]) loss = criterion(output, target) # 2. compute gradient and do SGD step optimizer.zero_grad() loss.backward() optimizer.step()\"]},\"245\":{\"h\":\"Model Implementation\"},\"246\":{\"h\":\"Model Init\",\"t\":[\"模型初始化代码如下所示:\",\"class MoCo(nn.Module): def __init__( self, base_encoder, # ResNet 模型 dim: int = 128, K: int = 65536, # 队列大小/字典大小/负样本数量 m: float = 0.999, # 动量更新参数 T: float = 0.07, # 温度参数 mlp: bool = False, ) -> None: \\\"\\\"\\\" dim: feature dimension (default: 128) K: queue size; number of negative keys (default: 65536) m: moco momentum of updating key encoder (default: 0.999) T: softmax temperature (default: 0.07) \\\"\\\"\\\" super(MoCo, self).__init__() self.K = K self.m = m self.T = T # 1. create the encoders: num_classes is the output fc dimension self.encoder_q = base_encoder(num_classes=dim) self.encoder_k = base_encoder(num_classes=dim) # 2. key encoder参数使用query encoder进行初始化，同时key encoder不参与梯度运算 for param_q, param_k in zip( self.encoder_q.parameters(), self.encoder_k.parameters() ): param_k.data.copy_(param_q.data) # initialize param_k.requires_grad = False # not update by gradient # 3. create the queue/dictionary and pointer self.register_buffer(\\\"queue\\\", torch.randn(dim, K)) self.queue = nn.functional.normalize(self.queue, dim=0) self.register_buffer(\\\"queue_ptr\\\", torch.zeros(1, dtype=torch.long)) # 队列指针，负责完成出队入队的信息记录\"]},\"247\":{\"h\":\"Model Forward\",\"t\":[\"模型前向传播代码如下所示:\",\" def forward(self, im_q, im_k): \\\"\\\"\\\" Input: im_q: a batch of query images im_k: a batch of key images Output: logits, targets \\\"\\\"\\\" # 1. compute query features q = self.encoder_q(im_q) # queries: NxC q = nn.functional.normalize(q, dim=1) # 2. compute key features with torch.no_grad(): # no gradient to keys self._momentum_update_key_encoder() # update the key encoder # shuffle for making use of BN im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k) k = self.encoder_k(im_k) # keys: NxC k = nn.functional.normalize(k, dim=1) # undo shuffle k = self._batch_unshuffle_ddp(k, idx_unshuffle) # 3. compute logits # Einstein sum is more intuitive # positive logits: Nx1 l_pos = torch.einsum(\\\"nc,nc->n\\\", [q, k]).unsqueeze(-1) # negative logits: NxK l_neg = torch.einsum(\\\"nc,ck->nk\\\", [q, self.queue.clone().detach()]) # 4. logits: Nx(1+K) logits = torch.cat([l_pos, l_neg], dim=1) # 5. apply temperature logits /= self.T # 6. labels: positive key indicators labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda() # 7. dequeue and enqueue self._dequeue_and_enqueue(k) return logits, labels\",\"论文采用 ResNet 作为编码器，其最后的全连接层（在全局平均池化之后）输出一个固定维度的向量（128 维）。这个输出向量会通过其 L2 范数进行归一化。该向量即表示一个 query（查询向量） 或 key（键向量） 的特征表示。\"]},\"248\":{\"h\":\"Momentum Update\",\"t\":[\"采用动量更新方式对 key encoder 进行缓慢更新的代码实现如下所示:\",\" @torch.no_grad() def _momentum_update_key_encoder(self) -> None: \\\"\\\"\\\" Momentum update of the key encoder \\\"\\\"\\\" for param_q, param_k in zip( self.encoder_q.parameters(), self.encoder_k.parameters() ): param_k.data = param_k.data * self.m + param_q.data * (1.0 - self.m)\"]},\"249\":{\"h\":\"Dequeue and Enqueue\",\"t\":[\"维护队列状态的代码实现如下所示：\",\" @torch.no_grad() def _dequeue_and_enqueue(self, keys) -> None: # 1. gather keys before updating queue keys = concat_all_gather(keys) # 如果你在多 GPU 上训练，每个 GPU 都会处理一部分 batch，该方法会将所有 GPU 上的 key 向量合并成一个完整的 batch batch_size = keys.shape[0] ptr = int(self.queue_ptr) assert self.K % batch_size == 0 # for simplicity # 2. replace the keys at ptr (dequeue and enqueue) self.queue[:, ptr : ptr + batch_size] = keys.T # queue 的形状是 (C, K)，每列是一个 key 向量（transpose 存储是为了快速矩阵乘） ptr = (ptr + batch_size) % self.K # move pointer self.queue_ptr[0] = ptr\"]},\"250\":{\"h\":\"多模态\"},\"251\":{\"h\":\"VQ-VAE 论文\",\"t\":[\"Neural Discrete Representation Learning 论文简析\",\"论文链接: Neural Discrete Representation Learning\"]},\"252\":{\"h\":\"ViLT 论文\",\"t\":[\"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision 论文简析\",\"论文链接: ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision 代码链接: https://github.com/dandelin/vilt\"]},\"253\":{\"h\":\"Introduction\",\"t\":[\"视觉-语言预训练（VLP）领域中，传统的视觉特征提取主要有两种典型实现方案：\",\"Region Feature（区域特征）：通常使用预训练的目标检测器（如基于 Visual Genome 数据集训练的检测模型）来定位图像中的物体区域，并提取每个区域的特征。这种方法能够捕获较为精细的对象信息，是许多早期VLP模型的标准做法，但计算复杂且处理速度较慢。\",\"Grid Feature（网格特征）：用卷积神经网络（如 ResNet）对整张图像进行处理，将图像划分为固定大小的网格，通过卷积提取每个网格的视觉特征。这种方式避免了目标检测的步骤，提取速度相对更快，但仍依赖卷积架构，计算资源消耗仍然较大。\",\"ViLT模型提出了一种极简化的视觉嵌入方案，摒弃了传统的目标检测和卷积视觉嵌入器，采用无卷积的浅层线性投影直接将图像块（patch）嵌入，并与文本token一同输入transformer处理。这样，ViLT不仅极大降低了模型参数和计算负担，实现了比基于区域特征的模型快数十倍、比基于网格特征的模型快至少四倍的推理速度，还在多项视觉-语言任务中取得了竞争力甚至更优的性能。\",\"此外，ViLT首次引入了全词掩码和图像增强技术于视觉-语言预训练，进一步推动了模型的下游表现，展示了其轻量化设计在效率与性能上的优势。\",\"Contribution:\",\"第一个基于patch projection的多模态预训练模型，其是首个使用patch projection来做visual embedding的方法。\",\"证明了可以将BERT的方法和Vison Transformer结合起来用于多模态transformer。\",\"体现了全词掩码在预训练时以及图像增强在微调时的重要性。\"]},\"254\":{\"h\":\"Motivation\",\"t\":[\"目前参数量最小的多模态Transformer方法。ViLT使用预训练的ViT来初始化交互的transformer，这样就可以直接利用交互层来处理视觉特征，不需要额外增加一个视觉encoder（如Faster-RCNN）。\"]},\"255\":{\"h\":\"Method\",\"t\":[\"现有的视觉语言模型的三种结构类别：\",\"VE = Vision Embedding\",\"TE = Text Embedding\",\"MI = Modality Interaction\",\"上图是4种不同类型的VLP模型示意图。其中每个矩形的高表示相对计算量大小，VE、TE和MI分别是visual embedding、text embedding和modality interaction的简写。\",\"作者提出这4种类型的主要依据有两点：\",\"在参数或者计算上，两种模态是否保持平衡。\",\"在网络深层中，两种模态是否相互作用。\",\"VSE、VSE++和SCAN属于(a)类型。对图像和文本独立使用encoder，图像的更重，文本的更轻，使用简单的点积或者浅层attention层来表示两种模态特征的相似性。\",\"CLIP属于(b)类型。每个模态单独使用重的transformer encoder，使用池化后的图像特征点积计算特征相似性。\",\"ViLBERT、UNTER和Pixel-BERT属于(c)类型。这些方法使用深层transformer进行交互作用，但是由于VE仍然使用重的卷积网络进行特征抽取，导致计算量依然很大。\",\"作者提出的ViLT属于(d)类型。ViLT是首个将VE设计的如TE一样轻量的方法，该方法的主要计算量都集中在模态交互上。\"]},\"256\":{\"h\":\"Modality Interaction Schema\",\"t\":[\"模态交互部分可以分成两种方式：一种是single-stream(如BERT和UNITER)，另一种是dual-stream(如ViLBERT和LXMERT)。其中single-stream是对图像和文本concate然后进行交互操作，而dual-stream是不对图像和文本concate然后进行交互操作。ViLT延用single-stream的交互方式，因为dual-stream会引入额外的计算量。\",\"现有的VLP模型的text embedding基本上都使用类BERT结构(图1)，但是visual embedding存在着差异。在大多数情况下，visual embedding是现有VLP模型的瓶颈。visual embedding的方法总共有三大类，其中region feature方法通常采用Faster R-CNN二阶段检测器提取region的特征，grid feature方法直接使用CNN提取grid的特征，patch projection方法将输入图片切片投影提取特征。ViLT是首个使用patch projection来做visual embedding的方法。\"]},\"257\":{\"h\":\"Model Structure\",\"t\":[\"作者提出的ViLT可以认为是目前最简单的多模态Transformer方法。ViLT使用预训练的ViT来初始化交互的transformer，这样就可以直接利用交互层来处理视觉特征，不需要额外增加一个视觉encoder。\",\"文本特征输入部分，将文本看成一个词序列，通过word embedding matrix转化成word embedding，然后和position embedding进行相加，最后和modal-type embedding进行concate。\",\"图像特征输入部分，将图像切块看成一个图像块序列，通过linear projection转化成visual embedding，然后和postion embedding进行相加，最后和modal-type embedding进行concate。\",\"其中word embedding和visual embedding通过可学习的modal-type embedding标志位来区分，其中0标志位表示word embedding部分，1标志位表示visual embedding部分。\",\"wrod embedding和visual embedding分别都嵌入了一个额外的可学习 [class] embedding，方便和下游任务对接。\"]},\"258\":{\"h\":\"Pretraining Objectives\",\"t\":[\"ViLT预训练的优化目标有两个：一个是image text matching(ITM)，另一个是masked language modeling(MLM)。\",\"ImageText Matching：随机以0.5的概率将文本对应的图片替换成不同的图片，然后对文本标志位对应输出使用一个线性的ITM head将输出feature映射成一个二值logits，用来判断图像文本是否匹配。另外ViLT还设计了一个word patch alignment (WPA)来计算textual subset和visual subset的对齐分数。\",\"Masked Language Modeling：MLM的目标是通过文本的上下文信息去预测masked的文本tokens。随机以0.15的概率mask掉tokens，然后文本输出接两层MLP预测mask掉的tokens。\",\"Whole Word Masking：另外ViLT还使用了whole word masking技巧。whole word masking是将连续的子词tokens进行mask的技巧，避免了只通过单词上下文进行预测。比如将“giraffe”词tokenized成3个部分[“gi”, “##raf”, “##fe”]，可以mask成[“gi”, “[MASK]”, “##fe”]，模型会通过mask的上下文信息[“gi”，“##fe”]来预测mask的“##raf”，就会导致不利用图像信息。\"]},\"259\":{\"h\":\"Conclusion\",\"t\":[\"本文提出的方法在效率上大大提升且表现出相似的性能，相比于region feature的方法速度快了60倍，相比于grid feature的方法快了4倍，而且下游任务表现出相似甚至更好的性能。\",\"缺点：\",\"1、性能不够高，在一些数据集上的表现比不过C类方法，有可能因为对于现有的任务来说，因为数据集的bias，或者这个任务需要更多的视觉信息，因此需要更多得视觉部分，最后的效果才能好。\",\"2、虽然推理时间快，但是训练速度很慢。只是结构上简化了多模态学习，但一般人还是玩不起。\"]},\"260\":{\"h\":\"多模态常用改编Bert代码实现\",\"t\":[\"多模态论文中常用的改编版本的Bert代码实现记录\",\"本文改编Bert代码讲解基于BLIP项目展开，代码链接: BLIP/models/med.py\"]},\"261\":{\"h\":\"多模态 Bert 前向传播流程\",\"t\":[\"本节我们将对多模态Bert的前向传播基本流程进行讲解，所给代码删除了大量非核心逻辑，如需了解各类优化手段，请阅读源码进行学习。\"]},\"262\":{\"h\":\"1. 整体流程总览（BertModel）\",\"t\":[\"class BertModel(BertPreTrainedModel): def forward( self, input_ids=None, attention_mask=None, position_ids=None, encoder_hidden_states=None, # 图像模态特征 encoder_attention_mask=None, # 图像掩码 is_decoder=False, mode='multimodal', # 控制是否启用 cross-attention ): # 1. 词嵌入 + 位置编码 embedding_output = self.embeddings(input_ids=input_ids, position_ids=position_ids) # 2. 编码阶段（Text-only 或 Cross-modal） sequence_output = self.encoder( embedding_output, attention_mask=extended_attention_mask, # 可用于多头自注意力的文本 padding mask encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, # 可用于多头自注意力的图像 padding mask mode=mode, ) # 3. 池化输出（用于分类任务） pooled_output = self.pooler(sequence_output) if self.pooler is not None else None return BaseModelOutputWithPoolingAndCrossAttentions( last_hidden_state=sequence_output, pooler_output=pooled_output, )\",\"池化输出实现:\",\"class BertPooler(nn.Module): def __init__(self, config): super().__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) self.activation = nn.Tanh() def forward(self, hidden_states): # 1. 拿到能够代表整段文本或者整个多模态表示的 CLS Token first_token_tensor = hidden_states[:, 0] # 2. 非线性变换 pooled_output = self.dense(first_token_tensor) pooled_output = self.activation(pooled_output) return pooled_output\"]},\"263\":{\"h\":\"2. 编码器：BertEncoder\",\"t\":[\"class BertEncoder(nn.Module): def __init__(self, config): self.layer = nn.ModuleList([BertLayer(config, i) for i in range(config.num_hidden_layers)]) def forward( self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, mode='multimodal', ): for i in range(self.config.num_hidden_layers): layer_module = self.layer[i] hidden_states = layer_module( hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, mode=mode, ) return hidden_states\",\"多模态关键点：\",\"多模态时，每个 Layer 都有机会执行 cross-attention。\",\"encoder_hidden_states 来自视觉模型（如 ViT 的输出），将图像特征注入到文本流中。\"]},\"264\":{\"h\":\"3. Transformer 层：BertLayer\",\"t\":[\"class BertLayer(nn.Module): def forward( self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, mode=None, ): # 1. 自注意力（Self-Attention） attention_output = self.attention(hidden_states, attention_mask) # 2. 多模态交叉注意力（Cross-Attention） if mode == 'multimodal': attention_output = self.crossattention( attention_output, attention_mask, encoder_hidden_states, encoder_attention_mask, ) return attention_output\",\"多模态关键点：\",\"自注意力捕捉文本内部的依赖；\",\"跨模态注意力（CrossAttention）让文本 Query 关注图像 Key 和 Value，实现信息融合。\"]},\"265\":{\"h\":\"4. Attention 模块：BertAttention\",\"t\":[\"class BertAttention(nn.Module): def forward( self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, ): self_outputs = self.self( hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, ) # attention 后应用一个 MLP return self.output(self_outputs, hidden_states)\",\"MLP 实现:\",\"class BertSelfOutput(nn.Module): def __init__(self, config): super().__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, hidden_states, input_tensor): hidden_states = self.dense(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.LayerNorm(hidden_states + input_tensor) return hidden_states\"]},\"266\":{\"h\":\"5. 核心计算：BertSelfAttention\",\"t\":[\"class BertSelfAttention(nn.Module): def forward( self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, ): # 获取 Query mixed_query_layer = self.query(hidden_states) # 判断是否为 Cross Attention is_cross_attention = encoder_hidden_states is not None if is_cross_attention: key_layer = self.transpose_for_scores(self.key(encoder_hidden_states)) value_layer = self.transpose_for_scores(self.value(encoder_hidden_states)) attention_mask = encoder_attention_mask else: key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) query_layer = self.transpose_for_scores(mixed_query_layer) # 计算 Attention 分数（缩放点积注意力） attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) attention_scores = attention_scores / math.sqrt(self.attention_head_size) # 加 Mask if attention_mask is not None: attention_scores = attention_scores + attention_mask # Softmax 归一化为权重 attention_probs = nn.Softmax(dim=-1)(attention_scores) # Dropout（来自 Transformer 原始实现） attention_probs_dropped = self.dropout(attention_probs) # 应用注意力权重 context_layer = torch.matmul(attention_probs_dropped, value_layer) # Reshape context_layer = context_layer.permute(0, 2, 1, 3).contiguous() new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) return context_layer.view(*new_context_layer_shape)\"]},\"267\":{\"h\":\"6. 小结\",\"t\":[\"多模态交互核心(Cross Attention):\",\"项目\",\"说明\",\"Query\",\"来自文本（attention_output）\",\"Key/Value\",\"来自图像（encoder_hidden_states）\",\"作用\",\"让文本动态关注图像区域，建立 Token 与视觉 Patch 的对齐\",\"应用\",\"文本问图（VQA）、图文检索、图文生成等多模态任务\",\"总结:\",\" +--------------------------+ | Text Embeddings | +-----------+--------------+ | [Transformer Encoder] | ┌────────┴───────────┐ │ Self-Attention │ │ (Text <-> Text) │ └────────┬───────────┘ │ ┌────────▼───────────┐ │ Cross-Attention │ <--- 图像特征作为 Key / Value │ (Text <-> Image) │ └────────┬───────────┘ │ FeedForward + LayerNorm + Residual\"]},\"268\":{\"h\":\"自回归语言建模\",\"t\":[\"BertLMHeadModel 是基于 BERT 构建的 语言建模头（Language Modeling Head）模型，其主要用于 自回归语言建模（Causal Language Modeling, CLM），尤其是在 多模态生成任务中充当解码器。它通常用于像 UNITER、VLBERT、MiniGPT-4、BLIP 等多模态架构中的文本生成部分。\",\"class BertLMHeadModel(BertPreTrainedModel): def __init__(self, config): self.bert = BertModel(config, add_pooling_layer=False) self.cls = BertLMPredictionHead(config) def forward( self, input_ids=None, attention_mask=None, position_ids=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, is_decoder=True, reduction='mean', mode='multimodal', ): # 1. 调用BertModel outputs = self.bert( input_ids, attention_mask=attention_mask, position_ids=position_ids, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, is_decoder=is_decoder, mode=mode, ) # 2. 解码 sequence_output = outputs[0] prediction_scores = self.cls(sequence_output) # 3. 返回预测得分 if return_logits: return prediction_scores[:, :-1, :].contiguous() # 返回预测出来的: [x2,x3,...,xn] , 丢掉 X(n+1) # 4. 计算 next-token prediction 损失 lm_loss = None if labels is not None: # 4.1 模型预测出来的: [x2,x3,...,xn] , 丢掉 X(n+1) 和 标签: [x2,x3,...,xn] , 丢掉 X(1) shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous() labels = labels[:, 1:].contiguous() # 4.2 计算交叉熵损失 loss_fct = CrossEntropyLoss(reduction=reduction, label_smoothing=0.1) lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1)) if reduction=='none': lm_loss = lm_loss.view(prediction_scores.size(0),-1).sum(1) return CausalLMOutputWithCrossAttentions( loss=lm_loss, logits=prediction_scores, )\",\"# 对输入进行非线性变换: 投影 + 激活 + 归一化 class BertPredictionHeadTransform(nn.Module): def __init__(self, config): super().__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) # 默认采用GELU激活函数 if isinstance(config.hidden_act, str): self.transform_act_fn = ACT2FN[config.hidden_act] else: self.transform_act_fn = config.hidden_act self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) def forward(self, hidden_states): hidden_states = self.dense(hidden_states) hidden_states = self.transform_act_fn(hidden_states) hidden_states = self.LayerNorm(hidden_states) return hidden_states class BertLMPredictionHead(nn.Module): def __init__(self, config): self.transform = BertPredictionHeadTransform(config) self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False) def forward(self, hidden_states): # 1. 非线性变换 hidden_states = self.transform(hidden_states) # 2. 解码: 将(seq_len,hidden_size)中每个word映射到词空间 hidden_states = self.decoder(hidden_states) return hidden_states\"]},\"269\":{\"h\":\"庖丁解牛CLIP\",\"t\":[\"多模态模型CLIP原理与图片分类，文字搜索图像实战演练\",\"CLIP原始论文链接\"]},\"270\":{\"h\":\"引言\",\"t\":[\"2021 年可谓是视觉 Transformer（Vision Transformer）大放异彩的一年。自谷歌提出 ViT 之后，众多基于视觉 Transformer 的研究如潮水般涌来，广泛应用于各类计算机视觉任务。与此同时，OpenAI 在 2021 年 1 月发布的 DALL-E 和 CLIP，同样给计算机视觉领域带来了巨大影响。这两个模型都属于融合图像与文本的多模态模型，其中 DALL-E 是基于文本输入来生成图像的模型，而 CLIP 则是以文本作为监督信号，训练出具有可迁移能力的视觉模型。和 ViT 类似，DALL-E 和 CLIP 的出现也掀起了新一轮的研究热潮。\"]},\"271\":{\"h\":\"介绍\",\"t\":[\"CLIP的英文全称为Contrastive Language-Image Pre-training，它代表着一种基于对比文本-图像对的预训练方法，同时也指运用该方法构建的模型。CLIP属于基于对比学习的多模态模型。与计算机视觉（CV）领域中的一些对比学习方法，像MoCo和SimCLR有所不同，CLIP的训练数据采用的是文本-图像对，也就是一张图像搭配与之对应的文本描述。在训练过程中，借助对比学习机制，期望模型能够学习到文本和图像之间的匹配关系。\"]},\"272\":{\"h\":\"训练\",\"t\":[\"CLIP包含两个核心模型，分别是文本编码器（Text Encoder）和图像编码器（Image Encoder）。其中，文本编码器的作用是提取文本的特征，在实现时可采用自然语言处理（NLP）领域常用的文本Transformer模型；而图像编码器则用于提取图像的特征，在实际应用中可以选用常见的卷积神经网络（CNN）模型，也可以采用视觉Transformer模型。\",\"这里对提取的文本特征和图像特征进行对比学习。对于一个包含个文本-图像对的训练batch，将个文本特征和个图像特征两两组合，CLIP模型会预测出个可能的文本-图像对的相似度，这里的相似度直接计算文本特征和图像特征的余弦相似性（cosine similarity），即上图所示的矩阵。这里共有个正样本，即真正属于一对的文本和图像（矩阵中的对角线元素），而剩余的个文本-图像对为负样本，那么CLIP的训练目标就是最大个正样本的相似度，同时最小化个负样本的相似度，对应的伪代码实现如下所示：\",\"# image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter # 分别提取图像特征和文本特征 I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # 对两个特征进行线性投射，得到相同维度的特征，并进行l2归一化 I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # 计算缩放的余弦相似度：[n, n] logits = np.dot(I_e, T_e.T) * np.exp(t) # 对称的对比学习损失：等价于N个类别的cross_entropy_loss labels = np.arange(n) # 对角线元素的labels loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2\",\"为了训练CLIP模型，OpenAI从网络上收集了总计4亿对文本和图像，这些数据在论文中被称为WebImageText。若以文本单词数量来衡量，其规模与GPT-2训练时使用的WebText数据集相似。然而，从数据对的数量来看，它比谷歌的JFT-300M数据集还要多出1亿对，因此这是一个非常庞大的数据集。\",\"尽管CLIP是一个多模态模型，但其主要目的是训练可迁移的视觉模型。在论文中，文本编码器（Text Encoder）选择了一个包含6300万参数的Transformer模型，而图像编码器（Image Encoder）则采用了两种不同的架构：\",\"一种是常用的CNN架构ResNet。\",\"另一种是基于 Transformer 的ViT。\",\"ResNet包含五种不同尺寸的模型：ResNet50、ResNet101、RN50x4、RN50x16和RNx64（后三种模型是按照EfficientNet的缩放规则对ResNet分别放大4倍、16倍和64倍得到的），而ViT则选择了三种不同尺寸的模型：ViT-B/32、ViT-B/16和ViT-L/14。\",\"所有模型均训练了32个周期，使用AdamW优化器，并且在训练过程中采用了一个相对较大的批次大小：32768。由于数据量巨大，最大的ResNet模型RN50x64需要在592个V100 GPU上训练18天，而最大的ViT模型ViT-L/14则需要在256个V100 GPU上训练12天，这表明训练CLIP模型需要消耗大量的资源。对于ViT-L/14模型，还在336的分辨率下额外进行了一个周期的微调（finetune）以增强性能，论文发现这个模型的效果最佳，并将其标记为ViT-L/14@336，论文中进行对比实验的CLIP模型也采用了这一配置。\"]},\"273\":{\"h\":\"推理\",\"t\":[\"我们已经探讨了CLIP模型的运作机制，它由两个部分组成：一个视觉模型和一个文本模型。那么，如何将这个预训练的视觉模型应用到新的任务中呢？CLIP模型的一个显著优势是它能够进行zero-shot图像分类，这意味着它能够在没有任何特定任务训练数据的情况下，直接对图像进行分类。这不仅展示了CLIP的强大功能，也是其一大亮点。实现zero-shot分类的过程相当直接，可以概括为以下两个主要步骤：\",\"构建描述文本并提取特征：首先，根据任务的分类需求，为每个类别创建一个描述性的文本，例如“A photo of {label}”。这些文本随后被输入到文本编码器（Text Encoder）中，以生成相应的文本特征。如果有个类别，那么就会得到个文本特征。\",\"图像特征提取与分类：接下来，将待分类的图像输入到图像编码器（Image Encoder）中，以获取图像特征。然后，这些图像特征会与之前得到的个文本特征进行余弦相似度计算（这一过程与训练时相同）。最终，选择与图像特征相似度最高的文本所对应的类别，作为图像的分类预测结果。此外，这些相似度值可以被视为logits，通过softmax函数转换后，可以得到每个类别的预测概率。\",\"通过这种方式，CLIP模型能够在没有特定任务训练数据的情况下，直接对图像进行分类，这展示了其在图像分类任务中的灵活性和强大能力。\",\" 显然，我们通过利用CLIP模型的多模态能力，为特定任务动态构建了一个分类器。在这个过程中，文本编码器（Text Encoder）生成的文本特征相当于分类器的权重，而图像编码器（Image Encoder）提取的图像特征则是分类器的输入数据。以下是一个官方给出的CLIP模型的示例 ，该示例中的任务涉及8个类别:\",\"我们首先创建了各类别的文本描述，然后提取了相应的文本特征；\",\"然后我们读取要预测的图像，输入Image Encoder提取图像特征，并计算与文本特征的余弦相似度。\",\"# 1. 提取文本特征 texts = [ \\\"a page of text about segmentation\\\", \\\"a facial photo of a tabby cat\\\", \\\"a portrait of an astronaut with the American flag\\\", \\\"a rocket standing on a launchpad\\\", \\\"a red motorcycle standing in a garage\\\", \\\"a person looking at a camera on a tripod\\\", \\\"a black-and-white silhouette of a horse\\\", \\\"a cup of coffee on a saucer\\\" ] text_tokens = clip.tokenize([\\\"This is \\\" + desc for desc in texts]).cuda() with torch.no_grad(): text_features = model.encode_text(text_tokens).float() # 2. 提取图像特征 image_input = torch.tensor(np.stack(images)).cuda() with torch.no_grad(): image_features = model.encode_image(image_input).float() # 3. 计算余弦相似度 image_features /= image_features.norm(dim=-1, keepdim=True) text_features /= text_features.norm(dim=-1, keepdim=True) similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\",\"相似度如下所示，可以看到对于要预测的8个图像，按照最大相似度，其均能匹配到正确的文本标签：\",\"进一步地，我们也可以对得到的余弦相似度计算softmax，得到每个预测类别的概率值：\",\"text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1) top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)\",\"得到的预测概率如下所示，可以看到8个图像，CLIP模型均能够以较高的置信度给出正确的分类结果：\"]},\"274\":{\"h\":\"文本描述生成\",\"t\":[\"在使用CLIP模型进行zero-shot分类时，除了模型本身的应用，文本描述的生成也是一个关键环节。在之前的例子中，我们使用了“A photo of {label}”这样的格式来生成文本描述，但实际上，我们还有其他的选择。例如，我们可以直接使用类别标签作为文本描述。这种方法实际上与NLP领域的一个研究方向——prompt learning或prompt engineering——紧密相关。关于这一领域的详细综述，可以参考论文《Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing》。\",\"简单来说，prompt learning的核心思想是通过设计合适的prompt（提示），使得预训练模型能够直接应用于下游任务。这与传统的预训练加微调的方法有所不同。论文指出，如果我们直接使用类别标签作为文本描述，由于这些文本往往只是一个单词，缺乏具体的上下文，并且与CLIP模型的训练数据不完全一致，因此在效果上可能不如使用“A photo of {label}”这种格式（在ImageNet数据集上可以提升1.3%的效果）。\",\"此外，论文还实验了使用80个不同的prompt进行集成，结果发现在ImageNet数据集上能够带来3.5%的性能提升。具体的实验结果可以参考CLIP公开的notebook。\"]},\"275\":{\"h\":\"花卉图片分类\",\"t\":[\"本节我们将基于CLIP预训练模型实现Zero-Shot推理，训练使用到的数据集和AlexNet保持一致，因此这里就不再给出数据集下载链接了。\",\"图片分类实战 – 分别基于LeNet，AlexNet，VGG进行实现\",\"# 预训练模型名称 model_name = \\\"openai/clip-vit-large-patch14\\\" # 定义当前目录 current_dir = os.getcwd() model_dir = os.path.join(current_dir, model_name.replace(\\\"/\\\", \\\"_\\\")) # 检查当前目录是否有预训练权重文件，如果没有则下载 def download_pretrained_weights_if_needed(model_name, save_dir): if not os.path.exists(save_dir): try: print(f\\\"Downloading {model_name} to {save_dir}...\\\") snapshot_download(repo_id=model_name, local_dir=save_dir, local_dir_use_symlinks=False) print(f\\\"{model_name} downloaded successfully.\\\") except Exception as e: print(f\\\"Error downloading {model_name}: {e}\\\") download_pretrained_weights_if_needed(model_name, model_dir) # 加载模型和处理器 model = CLIPModel.from_pretrained(model_dir) processor = CLIPProcessor.from_pretrained(model_dir) device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\") model = model.to(device)\",\"在 openai/clip-vit-large-patch14 这个 CLIP 预训练模型中，图像编码器采用了 Vision Transformer（ViT）架构，具体使用的是 ViT-L/14 版本，文本编码器使用的是基于 Transformer 的架构。\",\"# 函数：生成文本嵌入 def text_embedding(texts): inputs = processor(text=texts, return_tensors=\\\"pt\\\", padding=True).to(device) with torch.no_grad(): embeddings = model.get_text_features(**inputs) return embeddings.cpu().numpy()\",\"这个函数的作用是将输入的文本转化为对应的嵌入表示（embedding）。它通过处理器对输入文本进行处理，使其符合模型的输入要求，然后利用模型获取文本特征，最后将结果转换为 numpy 数组格式返回，方便后续的计算和比较。\",\"def get_image_embeddings(image_paths): images = [] for image_path in image_paths: try: image = Image.open(image_path).convert(\\\"RGB\\\") images.append(image) except Exception as e: print(f\\\"Error loading image {e}\\\") if not images: return None inputs = processor(images=images, return_tensors=\\\"pt\\\").to(device) with torch.no_grad(): image_features = model.get_image_features(**inputs) return image_features.cpu().numpy()\",\"该函数作用是针对给定的图片路径，读取图片并将其转换为合适的格式后，通过模型获取图片的特征嵌入。如果在读取图片过程中出现错误，会进行相应的错误提示并返回 None。\",\"def cosine_similarity(vec1, vec2): vec1 = np.array(vec1) vec2 = np.array(vec2) return np.dot(vec1, vec2.T) / (np.linalg.norm(vec1, axis=1, keepdims=True) * np.linalg.norm(vec2, axis=1))\",\"在图文检索中，我们常常需要衡量文本嵌入和图片嵌入之间的相似度，这里采用了余弦相似度的计算方法。它将输入的向量转换为 numpy 数组后，按照余弦相似度的数学公式来计算两者的相似度数值。\",\"首先，我们需要根据上面给出的花卉数据集下载链接，将数据下载到当前项目目录下:\",\"其次，我们从flower_photos目录下读取出所有图片的路径:\",\"# 递归遍历目录获取所有图片路径 def get_all_image_paths(directory): image_paths = [] for root, _, files in os.walk(directory): for file in files: file_extension = os.path.splitext(file)[1].lower() if file_extension in ['.png', '.jpg', '.jpeg']: image_paths.append(os.path.join(root, file)) return image_paths image_paths = get_all_image_paths(\\\"./flower_photos\\\")\",\"同时将flower_photos下的子目录名作为我们的候选待匹配分类文本列表，并改造为a photo of 子目录名的格式，然后计算每个分类文本对应的文本嵌入向量:\",\"# 获取候选分类名列表 def get_candidates(directory): candidates = [] for sub_dir in os.listdir(directory): sub_dir_path = os.path.join(directory, sub_dir) if os.path.isdir(sub_dir_path): candidates.append(f\\\"a photo of {sub_dir}\\\") return candidates candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates)\",\"最后:\",\"分批次从图像列表中取出一批图像，获取其对应的图像嵌入向量列表\",\"计算这批图像嵌入向量中每一个图像嵌入向量和我们所有分类文本嵌入向量的余弦相似度\",\"针对每个图像嵌入向量取出和其相似度最高的那个文本嵌入向量的索引\",\"判断预测是否正确，统计正确率\",\"# 测试图片分类正确率 def accuracy(image_paths, candidates, text_embeddings, batch_size=64): correct_count = 0 total_count = len(image_paths) num_batches = (total_count + batch_size - 1) // batch_size # 分批次预测 for i in range(num_batches): start_idx = i * batch_size end_idx = min(start_idx + batch_size, total_count) # 取出当前批次的图像列表，并获得该批次图像列表对应的图像嵌入向量列表 batch_image_paths = image_paths[start_idx:end_idx] image_embeddings = get_image_embeddings(batch_image_paths) if image_embeddings is not None: # 计算图像嵌入向量列表中每个图像对应的嵌入向量和传入的分类文本嵌入向量的余弦相似度 similarities = cosine_similarity(image_embeddings, text_embeddings) # 针对每个图像嵌入向量获取和其相似度最高的分类文本嵌入向量索引下标 predicted_indices = np.argmax(similarities, axis=1) for j, predicted_index in enumerate(predicted_indices): # 针对每张图像，根据上述计算得到的和其相似度最高的分类文本索引，从候选分类文本集合中取出其分类名词 predicted_category = candidates[predicted_index].split(\\\" \\\")[-1] # 用当前图片外层目录的名字作为其分类名词 actual_category = os.path.basename(os.path.dirname(batch_image_paths[j])) # 比较两个分类名词是否相等 if predicted_category == actual_category: correct_count += 1 accuracy = correct_count / total_count return accuracy # 图片分类 def flowerClassify(): image_paths = get_all_image_paths(\\\"./flower_photos\\\") candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates) start_time = time.time() acc = accuracy(image_paths, candidates, text_embeddings, batch_size=64) end_time = time.time() print(f\\\"Time taken to test accuracy: {end_time - start_time:.2f} seconds\\\") print(f\\\"Accuracy: {acc * 100:.2f}%\\\")\",\"Time taken to test accuracy: 396.62 seconds Accuracy: 95.48%\"]},\"276\":{\"h\":\"文字搜索图像\",\"t\":[\"上面的花卉分类案例相当于使用图像去匹配最合适的文本描述，而这里我们将会反转这个逻辑，用文本描述去匹配最合适的图片内容。\",\"为了实现文字搜索图像的功能，我们只需要在计算出相似度得分矩阵后，以每个文本描述为一行，取出该行中得分最大的那一列，即为与当前文本描述相似度最高的那副图片，具体代码实现如下：\",\"# 找到与文本最匹配的图片 def find_most_matching_image(text, image_paths): text_emb = text_embedding([text]) image_embeddings = get_image_embeddings(image_paths) if image_embeddings is None: return None similarities = cosine_similarity(text_emb, image_embeddings) most_matching_index = np.argmax(similarities) return image_paths[most_matching_index]\",\"下面来实际展示一下效果，首先我们用data目录充当我们的图片库来源:\",\" 遍历data目录，拿到所有图片路径:\",\"# 遍历 data 目录获取所有图片路径 def get_images_from_data_dir(): data_dir = os.path.join(current_dir, 'data') if not os.path.exists(data_dir): print(f\\\"Data directory {data_dir} does not exist.\\\") return [] return get_all_image_paths(data_dir)\",\"这里以搜索向日葵花为例，我们首先获取图片库中所有图片，然后计算出和当前文本描述相似度最高的那副图片，并将图片展示出来:\",\"# 根据文字搜索图片 def searchPicByText(): image_paths = get_images_from_data_dir() query_text = \\\"a photo of a sunflowers\\\" most_matching_image = find_most_matching_image(query_text, image_paths) if most_matching_image: print(f\\\"The most matching image for '{query_text}' is: {most_matching_image}\\\") try: img = Image.open(most_matching_image) plt.imshow(img) plt.axis('off') plt.title(f\\\"Most matching image for '{query_text}'\\\") plt.show() except Exception as e: print(f\\\"Error opening image: {e}\\\") else: print(\\\"No matching image found.\\\")\",\"图片库中的图片： 运行上述代码，搜索出来的图片:\"]},\"277\":{\"h\":\"完整代码\",\"t\":[\"import time from matplotlib import pyplot as plt from transformers import CLIPProcessor, CLIPModel import torch from PIL import Image import numpy as np import warnings import os from huggingface_hub import snapshot_download warnings.filterwarnings(\\\"ignore\\\") # 模型名称 model_name = \\\"openai/clip-vit-large-patch14\\\" # 定义当前目录 current_dir = os.getcwd() model_dir = os.path.join(current_dir, model_name.replace(\\\"/\\\", \\\"_\\\")) # 检查当前目录是否有预训练权重文件，如果没有则下载 def download_pretrained_weights_if_needed(model_name, save_dir): if not os.path.exists(save_dir): try: print(f\\\"Downloading {model_name} to {save_dir}...\\\") snapshot_download(repo_id=model_name, local_dir=save_dir, local_dir_use_symlinks=False) print(f\\\"{model_name} downloaded successfully.\\\") except Exception as e: print(f\\\"Error downloading {model_name}: {e}\\\") download_pretrained_weights_if_needed(model_name, model_dir) # 加载模型和处理器 model = CLIPModel.from_pretrained(model_dir) processor = CLIPProcessor.from_pretrained(model_dir) device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\") model = model.to(device) # 函数：生成文本嵌入 def text_embedding(texts): inputs = processor(text=texts, return_tensors=\\\"pt\\\", padding=True).to(device) with torch.no_grad(): embeddings = model.get_text_features(**inputs) return embeddings.cpu().numpy() def get_image_embeddings(image_paths): images = [] for image_path in image_paths: try: image = Image.open(image_path).convert(\\\"RGB\\\") images.append(image) except Exception as e: print(f\\\"Error loading image {e}\\\") if not images: return None inputs = processor(images=images, return_tensors=\\\"pt\\\").to(device) with torch.no_grad(): image_features = model.get_image_features(**inputs) return image_features.cpu().numpy() def cosine_similarity(vec1, vec2): vec1 = np.array(vec1) vec2 = np.array(vec2) return np.dot(vec1, vec2.T) / (np.linalg.norm(vec1, axis=1, keepdims=True) * np.linalg.norm(vec2, axis=1)) # 递归遍历目录获取所有图片路径 def get_all_image_paths(directory): image_paths = [] for root, _, files in os.walk(directory): for file in files: file_extension = os.path.splitext(file)[1].lower() if file_extension in ['.png', '.jpg', '.jpeg']: image_paths.append(os.path.join(root, file)) return image_paths # 获取候选分类名列表 def get_candidates(directory): candidates = [] for sub_dir in os.listdir(directory): sub_dir_path = os.path.join(directory, sub_dir) if os.path.isdir(sub_dir_path): candidates.append(f\\\"a photo of {sub_dir}\\\") return candidates # 测试图片分类正确率 def accuracy(image_paths, candidates, text_embeddings, batch_size=64): correct_count = 0 total_count = len(image_paths) num_batches = (total_count + batch_size - 1) // batch_size for i in range(num_batches): start_idx = i * batch_size end_idx = min(start_idx + batch_size, total_count) batch_image_paths = image_paths[start_idx:end_idx] image_embeddings = get_image_embeddings(batch_image_paths) if image_embeddings is not None: similarities = cosine_similarity(image_embeddings, text_embeddings) predicted_indices = np.argmax(similarities, axis=1) for j, predicted_index in enumerate(predicted_indices): predicted_category = candidates[predicted_index].split(\\\" \\\")[-1] actual_category = os.path.basename(os.path.dirname(batch_image_paths[j])) if predicted_category == actual_category: correct_count += 1 accuracy = correct_count / total_count return accuracy # 图片分类 def flowerClassify(): image_paths = get_all_image_paths(\\\"./flower_photos\\\") candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates) start_time = time.time() acc = accuracy(image_paths, candidates, text_embeddings, batch_size=64) end_time = time.time() print(f\\\"Time taken to test accuracy: {end_time - start_time:.2f} seconds\\\") print(f\\\"Accuracy: {acc * 100:.2f}%\\\") ##################################################################################################3 # 遍历 data 目录获取所有图片路径 def get_images_from_data_dir(): data_dir = os.path.join(current_dir, 'data') if not os.path.exists(data_dir): print(f\\\"Data directory {data_dir} does not exist.\\\") return [] return get_all_image_paths(data_dir) # 找到与文本最匹配的图片 def find_most_matching_image(text, image_paths): text_emb = text_embedding([text]) image_embeddings = get_image_embeddings(image_paths) if image_embeddings is None: return None similarities = cosine_similarity(text_emb, image_embeddings) most_matching_index = np.argmax(similarities) return image_paths[most_matching_index] # 根据文字搜索图片 def searchPicByText(): image_paths = get_images_from_data_dir() query_text = \\\"a photo of a sunflowers\\\" most_matching_image = find_most_matching_image(query_text, image_paths) if most_matching_image: print(f\\\"The most matching image for '{query_text}' is: {most_matching_image}\\\") try: img = Image.open(most_matching_image) plt.imshow(img) plt.axis('off') plt.title(f\\\"Most matching image for '{query_text}'\\\") plt.show() except Exception as e: print(f\\\"Error opening image: {e}\\\") else: print(\\\"No matching image found.\\\")\"]},\"278\":{\"h\":\"小结\",\"t\":[\"在计算机视觉领域，常见的迁移学习方法是首先在大规模数据集（如ImageNet）上进行预训练，然后在具体的下游任务上进行微调。这种预训练通常是基于有监督学习的，需要大量的数据标注，因此成本较高。近年来，出现了一些基于自监督的方法，包括基于对比学习的方法（如MoCo和SimCLR）和基于图像掩码的方法（如MAE和BeiT）。自监督方法的优势在于不再需要标注数据。然而，无论是有监督还是自监督方法，在迁移到下游任务时，都需要进行有监督微调，无法实现zero-shot学习。对于有监督模型，由于它们在预训练数据集上采用固定类别数的分类器，因此在新的数据集上需要定义新的分类器来重新训练。对于自监督模型，代理任务通常是辅助进行表征学习，在迁移到其他数据集时也需要加上新的分类器进行有监督训练。\",\"然而，在NLP领域，基于自回归或语言掩码的预训练方法已经相对成熟，预训练模型很容易直接zero-shot迁移到下游任务，例如OpenAI的GPT-3。这种差异一方面是由于文本和图像属于两个完全不同的模态，另一个原因是NLP模型可以利用从互联网上收集的大量文本。因此，问题来了：能否基于互联网上的大量文本来预训练视觉模型？\",\"实际上，之前已经有一些研究工作探讨了使用文本作为监督信号来训练视觉模型。例如，2016年的工作《Learning Visual Features from Large Weakly Supervised Data》将这个问题转化为一个多标签分类任务，预测图像对应的文本的词袋模型；2017年的工作《Learning Visual N-Grams from Web Data》进一步扩展了这个方法来预测n-grams。最近的一些工作采用新的模型架构和预训练方法来从文本学习视觉特征，例如VirTex基于transformer的语言模型，ICMLM基于语言掩码的方法，ConVIRT基于对比学习的方法。总体来看，这方面的工作并不多，主要是因为这些方法难以实现较高的性能，例如2017年的那篇工作只在ImageNet上实现了11.5%的zero-shot性能，这远远低于ImageNet上的SOTA。此外，还有另一个方向，即基于文本弱监督来提升性能，例如谷歌的BiT和ViT基于JFT-300M数据集来预训练模型在ImageNet上取得SOTA。JFT-300M数据集是谷歌从互联网上收集的，通过一些自动化的手段将web text转化为18291个类别，但存在一定的噪声。尽管谷歌基于JFT-300M数据集取得了较好的结果，但这些模型仍然采用固定类别的softmax分类器进行预训练，这大大限制了它们的迁移能力和扩展性。\",\"谷歌采用的弱监督方法与以往方法的一个主要区别在于规模，或者说在于计算能力和数据集的规模。JFT-300M数据集的规模达到了上亿级别，谷歌利用强大的计算能力进行了预训练。相比之下，VirTex、ICMLM和ConVIRT仅在10万级别的数据上训练了几天。为了弥补数据规模上的差距，OpenAI从网络上收集了4亿条数据进行实验。然而，新的问题出现了：应该采用什么样的方法来进行训练。\",\"OpenAI首先尝试了VirTex模型，该模型联合训练一个CNN和文本transformer来预测图像的文本描述（image caption），但发现这种方法的训练效率（根据ImageNet数据集上的zero-shot性能评估）还不如直接预测词袋模型（bag of words），两者的训练效率相差3倍。如果进一步采用ConVIRT，即基于对比学习的方法，训练效率可以提高4倍。出现这种差异的原因不难理解，因为训练数据集中的文本-图像对是从互联网收集的，存在一定的噪声，即文本和图像可能不完全匹配。在这种情况下，适当降低训练目标反而可能取得更好的效果。\",\"从任务难度来看，排序为：Transformer Language Model > Bag of Words Prediction > Bag of Words Contrastive (CLIP)。由于训练数据量和模型计算量较大，训练效率成为一个至关重要的因素。因此，作者最终选择了对比学习方法来进行训练。\"]},\"279\":{\"h\":\"庖丁解牛BLIP2\",\"t\":[\"庖丁解牛BLIP2\",\"论文: https://arxiv.org/abs/2301.12597 代码: https://github.com/salesforce/LAVIS/tree/main/projects/blip2\"]},\"280\":{\"h\":\"背景\",\"t\":[\"多模态模型在过往发展的过程中，曾有一段时期一直在追求更大的网络架构（image encoder 和 text encoder/decoder）和 数据集，从而导致更大的训练代价。例如CLIP，400M数据，需要数百个GPU训练数十天，如何降低模型训练成本，同时具有很好的性能？\",\"这就是BLIP-2的起因，回顾下之前的多模态网络设计，三个模块（图像分支、文本分支、融合模块）:\",\"多模态网络设计\",\"(a) 早期的图文多模态：图像分支依赖目标检测器，模态融合比较弱，如VSE++。\",\"(b) 重点训练图像和文本特征提取，模态融合比较轻量，如CLIP。\",\"(c) 图像特征提取和模态融合都很重。\",\"(d) 侧重模态融合，特征提取网络相对轻量，如ViLT。\",\"模块\",\"(a)\",\"(b)\",\"(c)\",\"(d)\",\"理想情况\",\"视觉分支\",\"重\",\"重\",\"重\",\"轻\",\"重\",\"文本分支\",\"轻\",\"重\",\"轻\",\"轻\",\"重\",\"融合模块\",\"轻\",\"轻\",\"重\",\"重\",\"轻\",\"性能\",\"一般\",\"好\",\"好\",\"一般\",\"好\",\"训练代价\",\"中\",\"非常高\",\"非常高\",\"高\",\"中\",\"BLIP-2 基于 BLIP 架构，利用已有的ViT 和 LLM（均冻结）+ 一个的轻量Q-Former模块做模态融合，大幅降低训练成本。具有很强的zero-shot image-to-text generation能力，同时因LLM而具有了视觉推理能力。\"]},\"281\":{\"h\":\"模型结构\",\"t\":[\"BLIP-2 框架按照 Two-Stage 策略预训练轻量级查询 Transformer 以弥合模态差距。\",\"Stage 1: 不同模态数据的提取与融合。\",\"Stage 2: 把数据转换成LLM能识别的格式。\",\"Two-Stage流程\",\"从冻结的Image Encoder引到Vision-Language表征学习。\",\"从冻结的LLM引到Vision-Language生成学习，实现Zero Shot图文生成。\"]},\"282\":{\"h\":\"Stage 1: Representation Learning （表征学习）\",\"t\":[\"tage 1: Representation Learning （表征学习）\",\"Q-Former 由两个transformer模块组成，输入包含三部分：\",\"冻结参数的Image Encoder提取的图像embeddings\",\"Learned Queries\",\"Queries是一组可学习的embeddings，是第一个transformer模块的input，可认为是模型参数一部分\",\"推理时，Queries被用来从image encoder输出的embeddings里提取与input text最相关的视觉信息\",\"Input Text\",\"Stage 1 使用 图像-文本对 进行预训练，目标是训练好 Q-Former，以便 Queries 可以学习到如何更好地结合文本提取图片信息。\",\"对于Q-Former，一种比较好理解的方式：把Q-Former类比为一个Self-attention模块\",\"Q：learned queries\",\"K：input text\",\"V：image embeddings from Image Encoder\",\"Blip2Qformer核心代码实现如下:\",\"利用 query tokens 从 image embeddings 中提取与 text 最相关的视觉信息\",\"将输入的 input text 进行编码 , 然后使用第一个CLS Token 作为 input text representation\",\"class Blip2Qformer(Blip2Base): ... def forward(self, samples): image = samples[\\\"image\\\"] # (B,C,H,W) text = samples[\\\"text_input\\\"] # (B,seq_len) # frozen vit 将图片编码成 (B, seq_len, hidden_size) image_embeds = self.ln_vision(self.visual_encoder(image)) # 构建padding mask标注哪些image token是有效的 (B,seq_len) image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to( image.device ) # 初始化query tokens (B,seq_len,hidden_size) query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1) # query tokens 从 image embeddings 中提取与 text 最相关的视觉信息 # query_output (B,seq_len,hidden_size) query_output = self.Qformer.bert( query_embeds=query_tokens, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, use_cache=True, return_dict=True, ) image_feats = F.normalize( self.vision_proj(query_output.last_hidden_state), dim=-1 ) # 将input text 进行编码，维度为 (B,seq_len,hidden_size) text_tokens = self.tokenizer( text, padding=\\\"max_length\\\", truncation=True, max_length=self.max_txt_len, return_tensors=\\\"pt\\\", ).to(image.device) text_output = self.Qformer.bert( text_tokens.input_ids, attention_mask=text_tokens.attention_mask, # padding mask return_dict=True, ) # 取第一个cls token作为input text representation，维度为 (B,hidden_size) text_feat = F.normalize( self.text_proj(text_output.last_hidden_state[:, 0, :]), dim=-1 ) ...\",\"以上代码注释中统一用B代替image_batch和text_batch，以及seq_len和hidden_size也是同样处理手段，大家注意区分。\",\"为了训练好Q-Former，第一阶段设计了三个训练目标，分别如下:\"]},\"283\":{\"h\":\"1、Image-Text Contrastive Learning (ITC Loss, CLIP-like)\",\"t\":[\"目的: Image representation 与 Text representation，以最大化互信息\",\"自注意力掩码策略: Uni-modal Self-attention Mask（单模态自注意力）\",\"Queries 和 Text 仅能和自己的 tokens 做 attention（Query和Query、Text和Text）\",\"Uni-modal Self-attention Mask\",\"image_feats 中每个 image_feat 与 text_feat 计算一个 similarity score ，选择最大值作为这个图文对的相似度 :\",\"similarity score\",\"如何计算loss的: “in-batch negatives”，该方法正是CLIP在VLP领域发扬光大的。以下引用CLIP论文图做说明：\",\"in-batch negatives\",\"###============== Image-text Contrastive ===================### # 计算每个query token 和 text_feat 的相似度 , 得到相似度矩阵 (B,B,seq_len) # image_feats (B,seq_len,hidden_size) 变为 (B,1,seq_len,hidden_size) # text_feat (B,hidden_size) 变为 (B,hidden_size,1) sim_q2t = torch.matmul( image_feats.unsqueeze(1), text_feat.unsqueeze(-1) ).squeeze() # image-text similarity: aggregate across all query tokens # 保留和text_feat相似度最大的那个query token作为最后的相似度得分 , 维度为 (B,B) sim_i2t, _ = sim_q2t.max(-1) sim_i2t = sim_i2t / self.temp # 反过来计算text_feat 和 每个query token的相似度 , 得到相似度矩阵 (B,B,seq_len) # image_feats 维度变为 (B,hidden_size,seq_len) # text_feat (B,hidden_size) 变为 (B,1,1,hidden_size) sim_t2q = torch.matmul( text_feat.unsqueeze(1).unsqueeze(1), image_feats.permute(0, 2, 1) ).squeeze() # text-image similarity: aggregate across all query tokens # 保留和text_feat相似度最大的那个query token作为最后的相似度得分 , 维度为 (B,B) sim_t2i, _ = sim_t2q.max(-1) sim_t2i = sim_t2i / self.temp # 生成比标签 targets = torch.arange(image.size(0), device=image.device) # 计算 图文对比 Loss loss_itc = ( # sim_i2t 形状是 (B, B)，每一行表示一张图像和所有文本之间的相似度。 F.cross_entropy(sim_i2t, targets, label_smoothing=0.1) + F.cross_entropy(sim_t2i, targets, label_smoothing=0.1) ) / 2\"]},\"284\":{\"h\":\"2、Image-Text Matching (ITM Loss，二分类task)\",\"t\":[\"目的：通过学习image-text pair是否match，以细粒度对齐 Image representation 与 Text representation\",\"自注意力掩码策略: Bi-directional Self-attention Mask（双向自注意力）\",\"Queries 和Text都能和所有的tokens 做attention\",\"Bi-directional Self-attention Mask\",\"每个output query embedding送到二分类器中，得到一个logit；所有logits的平均作为最终的matching score:\",\"matching score\",\" ###============== Image-text Matching ===================### text_input_ids_world = text_tokens.input_ids text_attention_mask_world = text_tokens.attention_mask image_embeds_world = image_embeds with torch.no_grad(): # bs (batch size) ， diag_indices = [0,1,2,...,bs-1] diag_indices = torch.arange(bs, device=sim_t2i.device) # 把相似度矩阵对角线元素置为负无穷大，以避免模型将匹配图文对挑选为负样本 # (0,0) , (1,1) ... (bs-1,bs-1) 位置处设置为 -10000 sim_t2i[diag_indices, diag_indices] = -10000 sim_i2t[diag_indices, diag_indices] = -10000 weights_t2i = F.softmax(sim_t2i, dim=1) weights_i2t = F.softmax(sim_i2t, dim=1) # 为每个文本选择一个负样本图像 image_embeds_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_t2i[b], 1).item() image_embeds_neg.append(image_embeds_world[neg_idx]) image_embeds_neg = torch.stack(image_embeds_neg, dim=0) # 为每个图像选择一个负样本文本 text_ids_neg = [] text_atts_neg = [] for b in range(bs): neg_idx = torch.multinomial(weights_i2t[b], 1).item() text_ids_neg.append(text_input_ids_world[neg_idx]) text_atts_neg.append(text_attention_mask_world[neg_idx]) text_ids_neg = torch.stack(text_ids_neg, dim=0) text_atts_neg = torch.stack(text_atts_neg, dim=0) # 构建输入文本列表: [正样本batch，负样本batch1，负样本batch2] ，维度为 (3*bs,seq_len) text_ids_all = torch.cat( [text_tokens.input_ids, text_tokens.input_ids, text_ids_neg], dim=0 ) text_atts_all = torch.cat( [text_tokens.attention_mask, text_tokens.attention_mask, text_atts_neg], dim=0, ) # 构建query tokens列表: [正样本batch，负样本batch1，负样本batch2] ，维度为 (3*bs,seq_len,hidden_size) query_tokens_itm = self.query_tokens.expand(text_ids_all.shape[0], -1, -1) query_atts_itm = torch.ones(query_tokens_itm.size()[:-1], dtype=torch.long).to( image.device ) # 构建query和text的padding mask ，维度为 (3*bs,seq_len) attention_mask_all = torch.cat([query_atts_itm, text_atts_all], dim=1) # 构建输入图像列表: [正样本batch，负样本batch1，负样本batch2] ，维度为 (3*bs,seq_len,hidden_size) image_embeds_all = torch.cat( [image_embeds, image_embeds_neg, image_embeds], dim=0 ) image_atts_all = torch.ones(image_embeds_all.size()[:-1], dtype=torch.long).to( image.device ) # 1. 将输入文本转换为嵌入列表后和query tokens 在seq_len维度上拼接起来，维度为 (3*bs,text_seq_len + query_tokens_seq_len,hidden_size) # 2. 将文本和query tokens拼接得到的结果和图像嵌入进行cross attention计算，编码后得到输出的结果 output_itm = self.Qformer.bert( text_ids_all, query_embeds=query_tokens_itm, attention_mask=attention_mask_all, encoder_hidden_states=image_embeds_all, encoder_attention_mask=image_atts_all, return_dict=True, ) # 取 (3*bs,text_seq_len + query_tokens_seq_len,hidden_size) 中 query tokens部分的结果，维度为 (3*bs,query_tokens_seq_len,hidden_size) vl_embeddings = output_itm.last_hidden_state[:, : query_tokens_itm.size(1), :] # 把query tokens部分的每个位置都映射到2维匹配空间，维度为 (3*bs,query_tokens_seq_len,2) vl_output = self.itm_head(vl_embeddings) # 取每个位置的平均作为最终的匹配得分，维度为 (3*bs,2) logits = vl_output.mean(dim=1) # 构建匹配标签: [正样本batch=1，负样本batch1=0，负样本batch2=0] ，维度为 (3*bs) itm_labels = torch.cat( [torch.ones(bs, dtype=torch.long), torch.zeros(2 * bs, dtype=torch.long)], dim=0, ).to(image.device) # 计算交叉熵损失 loss_itm = F.cross_entropy(logits, itm_labels)\",\"当文本和query tokens同时输入BertModel时，BertEmbeddings会将text embeddings和query tokens的embeddings在seq_len维度上拼接起来。\",\"class BertEmbeddings(nn.Module): ... def forward( self, input_ids=None, position_ids=None, query_embeds=None, past_key_values_length=0, ): # 计算序列长度 if input_ids is not None: seq_length = input_ids.size()[1] else: seq_length = 0 # 如果未提供位置id，则自动生成 if position_ids is None: position_ids = self.position_ids[ :, past_key_values_length : seq_length + past_key_values_length ].clone() # 词嵌入与位置嵌入相加，若有query_embeds则拼接 if input_ids is not None: embeddings = self.word_embeddings(input_ids) if self.position_embedding_type == \\\"absolute\\\": position_embeddings = self.position_embeddings(position_ids) embeddings = embeddings + position_embeddings if query_embeds is not None: embeddings = torch.cat((query_embeds, embeddings), dim=1) else: embeddings = query_embeds embeddings = self.LayerNorm(embeddings) embeddings = self.dropout(embeddings) return embeddings\",\"下图展示了 Image-Text Matching 的完整计算流程，关于BertModel的代码解析部分，将会在下文进行详细讲解:\",\"Image-Text Matching\"]},\"285\":{\"h\":\"3、Image-Grounded Text Generation (ITG Loss, GPT-like)\",\"t\":[\"目的：让Q-Former学习“图生文”的能力，即给定Input Image，生成Text\",\"自注意力掩码策略：Multimodal Causal Self-attention Mask（多模态因果自监督）\",\"Queies 可以和所有自己的tokens做attention\",\"Text 可以和所有的query tokens 及 当前token之前的text tokens做attention\",\"Multimodal Causal Self-attention Mask\",\"视觉编码阶段:\",\"图像通过视觉编码器（如 ViT）编码为图像特征 image_embeds。Query tokens 通过 cross-attention 吸收图像特征，再通过 self-attention 生成压缩的视觉表示。缓存 query tokens 的 self-attention 的 past_key_values（而非 cross-attention 的 key/value）。\",\"QFormer 会使用 past_key_values 缓存和复用 EncoderLayer 中 self-attention 的 key/value :\",\"BertSelfAttention: 自注意力和交叉注意力流程统一化，每次计算后返回本次可能需要缓存的key & value\",\"class BertSelfAttention(nn.Module): ... def forward( self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False, ): # 判断是否为交叉注意力 is_cross_attention = encoder_hidden_states is not None # 交叉注意力则key和value都来自图像,key来自query tokens if is_cross_attention: key_layer = self.transpose_for_scores(self.key(encoder_hidden_states)) value_layer = self.transpose_for_scores(self.value(encoder_hidden_states)) attention_mask = encoder_attention_mask # 如果有缓存的key,value传入, 此时先用text embedding计算出key和value # 再和缓存的key,value在seq_len的维度拼接起来 elif past_key_value is not None: key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) key_layer = torch.cat([past_key_value[0], key_layer], dim=2) # (Batch,Heads,Seq_len,Hidden_size) value_layer = torch.cat([past_key_value[1], value_layer], dim=2) else: # 自注意力 key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) # 交叉注意力: 传入图像，则q来自query tokens # 自注意力: q来自query tokens 或者 text embedding mixed_query_layer = self.query(hidden_states) query_layer = self.transpose_for_scores(mixed_query_layer) # * 缓存key和value past_key_value = (key_layer, value_layer) # 计算注意力分数 attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) attention_scores = attention_scores / math.sqrt(self.attention_head_size) if attention_mask is not None: # 应用注意力掩码 attention_scores = attention_scores + attention_mask # softmax归一化得到注意力概率 attention_probs = nn.Softmax(dim=-1)(attention_scores) if is_cross_attention and self.save_attention: self.save_attention_map(attention_probs) attention_probs.register_hook(self.save_attn_gradients) # dropout防止过拟合 attention_probs_dropped = self.dropout(attention_probs) # 计算上下文表示 context_layer = torch.matmul(attention_probs_dropped, value_layer) context_layer = context_layer.permute(0, 2, 1, 3).contiguous() new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) context_layer = context_layer.view(*new_context_layer_shape) outputs = ( (context_layer, attention_probs) if output_attentions else (context_layer,) ) # outputs 列表最后一个记录了缓存的key和value outputs = outputs + (past_key_value,) return outputs\",\"BertLayer: 负责组织自注意力和交叉注意力的运算流程\",\"class BertLayer(nn.Module): ... def forward( self, hidden_states, # query tokens attention_mask=None, # query token padding mask head_mask=None, encoder_hidden_states=None, # image tokens encoder_attention_mask=None, # image padding mask past_key_value=None, output_attentions=False, query_length=0, ): self_attn_past_key_value = ( past_key_value[:2] if past_key_value is not None else None ) # 自注意力运算 self_attention_outputs = self.attention( hidden_states, attention_mask, head_mask, output_attentions=output_attentions, past_key_value=self_attn_past_key_value, # 缓存的key和value ) attention_output = self_attention_outputs[0] outputs = self_attention_outputs[1:-1] present_key_value = self_attention_outputs[-1] # 交叉注意力运算 if query_length > 0: query_attention_output = attention_output[:, :query_length, :] if self.has_cross_attention: cross_attention_outputs = self.crossattention( query_attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions=output_attentions, ) query_attention_output = cross_attention_outputs[0] outputs = ( outputs + cross_attention_outputs[1:-1] ) ... outputs = (layer_output,) + outputs outputs = outputs + (present_key_value,) # outputs 列表最后一个记录了缓存的key和value return outputs\",\"BertEncoder: 负责组织多个 BertLayer 叠加的运算流程\",\"class BertEncoder(nn.Module): ... def forward( self, hidden_states, # query tokens attention_mask=None, # query tokens padding mask head_mask=None, encoder_hidden_states=None, # images encoder_attention_mask=None, # images padding mask past_key_values=None, use_cache=None, output_attentions=False, output_hidden_states=False, return_dict=True, query_length=0, ): ... for i in range(self.config.num_hidden_layers): layer_module = self.layer[i] ... # 如果有缓存，则计算当前层BertLayer时，会从缓存中取出对应层先前缓存的key&value past_key_value = past_key_values[i] if past_key_values is not None else None layer_outputs = layer_module( hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, query_length, ) hidden_states = layer_outputs[0] # 每一层BertLayer产生的key&value都会进行缓存 if use_cache: next_decoder_cache += (layer_outputs[-1],) ... return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=next_decoder_cache, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, )\",\"Image-Grounded Text Generation 学习目标\",\" ... query_output = self.Qformer.bert( query_embeds=query_tokens, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, use_cache=True, # 缓存key&value return_dict=True, ) ... ##================= Image Captioning ========================## # 这一部分的目标是：根据图像特征，使用 Q-Former 解码器生成文本描述（caption） # Step 1: 准备 decoder 的输入 token IDs decoder_input_ids = text_tokens.input_ids.clone() # 将第一个 token 替换为 BOS（Begin Of Sentence）标记，表示“开始生成句子” decoder_input_ids[:, 0] = self.tokenizer.bos_token_id # Step 2: 构造训练目标 labels # 将 padding token 替换为 -100，这是 CrossEntropyLoss 默认忽略的标签值 labels = decoder_input_ids.masked_fill( decoder_input_ids == self.tokenizer.pad_token_id, -100 ) # Step 3: 构建 attention_mask（包含 query tokens 和 文本 token 的 mask） # query_atts 是 query tokens 的 attention mask，全为 1（因为都是有效 token） query_atts = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(image.device) # 将 query token 的 mask 和文本 token 的 mask 拼接在一起 attention_mask = torch.cat([query_atts, text_tokens.attention_mask], dim=1) # Step 4: 调用 Q-Former 解码器进行文本生成 lm_output = self.Qformer( decoder_input_ids, # 输入 token ID 序列（如 [BOS], dog, is...） attention_mask=attention_mask, # 指明哪些位置是有效的（非 padding） past_key_values=query_output.past_key_values, # 编码器输出的 key/value，包含图像信息 return_dict=True, # 返回字典格式结果 labels=labels, # 训练目标，用于计算 loss ) # Step 5: 提取语言模型损失 loss_lm = lm_output.loss # 使用交叉熵损失衡量生成与真实之间的差异\",\"文本生成阶段:\",\"将缓存的 past_key_values 作为文本解码器的初始状态。\",\"文本 token 在自回归生成时，通过 self-attention 复用缓存的视觉信息。\",\"BertLMHeadModel: 自回归语言建模任务（如文本生成）\",\"class BertLMHeadModel(BertPreTrainedModel): ... def forward( self, input_ids=None, attention_mask=None, position_ids=None, head_mask=None, query_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, past_key_values=None, use_cache=True, output_attentions=None, output_hidden_states=None, return_dict=None, return_logits=False, is_decoder=True, reduction=\\\"mean\\\", ): ... # 调用 BertModel 进行文本编码 (结合缓存的attention key&value) outputs = self.bert( input_ids, attention_mask=attention_mask, position_ids=position_ids, head_mask=head_mask, query_embeds=query_embeds, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, is_decoder=is_decoder, ) sequence_output = outputs[0] ... # self.cls 是一个分类头（BertOnlyMLMHead），它将每个 token 的向量映射到词汇表空间（logits） prediction_scores = self.cls(sequence_output) ... lm_loss = None if labels is not None: # 因为我们要预测下一个 token，所以把 logits 和 labels 错位对齐： # shifted_prediction_scores: 所有 token 的预测（除了最后一个） shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous() # labels: 所有 token 的真实值（从第二个开始） labels = labels[:, 1:].contiguous() loss_fct = CrossEntropyLoss(reduction=reduction, label_smoothing=0.1) lm_loss = loss_fct( shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1), ) if reduction == \\\"none\\\": lm_loss = lm_loss.view(prediction_scores.size(0), -1).sum(1) ... return CausalLMOutputWithCrossAttentions( loss=lm_loss, logits=prediction_scores, past_key_values=outputs.past_key_values, hidden_states=outputs.hidden_states, attentions=outputs.attentions, cross_attentions=outputs.cross_attentions, )\",\"BertModel 的 forward 方法中，当is_decoder=True时，会在get_extended_attention_mask方法中，构建一个下三角矩阵作为因果掩码矩阵。\"]},\"286\":{\"h\":\"Stage 2: Generative Learning（生成学习）\",\"t\":[\"Stage 2 是为了把 Q-Former 和冻结参数的 LLM 连接起来，以利用 LLM 的文本生成能力。\",\"支持两种LLM（decoder only、encoder-decoder based）:\",\"Generative Learning\",\"首先输入图片，直接输入冻结参数的 Image Encoder，得到图像的表征。\",\"然后图像的表征和 Queries 一起送入 Q-Former，得到 Queries 的输出 ，使用全连接 (FC) 层将 线性投影到与 LLM 的text embedding相同维度。\",\"后将投影后的 添加到 input text embeddings前面，Queries 的输出蕴含了视觉信息，送入LLM时，充当了soft visual prompts 。\",\"由于 Q-Former 已经过预训练以提取语言信息视觉表示，因此它有效地充当信息瓶颈，将最有用的信息提供给 LLM，同时删除不相关的视觉信息。这减少了LLM学习视觉语言对齐的负担，从而缓解了灾难性的遗忘问题。\",\"Blip2Qformer 的generate方法负责完成图像描述生成（图文到文本）:\",\"class Blip2Qformer(Blip2Base): ... def generate( self, samples, # 输入样本，包含图像和可选文本 use_nucleus_sampling=False, # 是否使用核采样（top-p采样） num_beams=3, # beam search的beam数量 max_length=30, # 生成文本的最大长度 min_length=10, # 生成文本的最小长度 top_p=0.9, # 核采样的概率阈值 repetition_penalty=1.0, # 重复惩罚系数 ): # 1. 图像编码阶段 image = samples[\\\"image\\\"] # 通过视觉编码器（如ViT）提取图像特征 (B, 257, D) image_embeds = self.ln_vision(self.visual_encoder(image)) # 2. 处理beam search扩展 if not use_nucleus_sampling: # 如果是beam search，需要复制图像特征以匹配beam数量 # (B, 257, D) -> (B*num_beams, 257, D) image_embeds = image_embeds.repeat_interleave(num_beams, dim=0) else: # 核采样时不扩展beam num_beams = 1 # 创建图像注意力掩码（全1，表示所有图像token有效） image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to( image.device ) # 3. 准备生成参数 model_kwargs = { \\\"encoder_hidden_states\\\": image_embeds, # 图像特征作为cross-attention的输入 \\\"encoder_attention_mask\\\": image_atts, # 图像注意力掩码 } # 4. 初始化文本输入（以BOS token开头） # 形状: (batch_size, 1)，初始为[BOS] input_ids = ( torch.LongTensor(image.size(0), 1) .fill_(self.tokenizer.bos_token_id) .to(image.device) ) # 5. 扩展可学习的query tokens # query_tokens形状: (batch_size, num_query_tokens, D) query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1) # 6. 调用Q-Former的生成方法 outputs = self.Qformer.generate( input_ids=input_ids, # 初始文本token [BOS] query_embeds=query_tokens, # 可学习query tokens max_length=max_length, # 最大生成长度 min_length=min_length, # 最小生成长度 num_beams=num_beams, # beam数量 do_sample=use_nucleus_sampling, # 是否采样 top_p=top_p, # 核采样参数 eos_token_id=self.tokenizer.sep_token_id, # 结束符 pad_token_id=self.tokenizer.pad_token_id, # 填充符 **model_kwargs # 图像特征和掩码 ) # 7. 解码生成的token id为文本 captions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True) return captions\"]},\"287\":{\"h\":\"庖丁解牛VIT\",\"t\":[\"多模态模型VIT原理与图片分类实战演练\",\"Vision Transformer是2021年谷歌在ICLR上提出的算法，它首次将NLP领域火热的Transformer模型架构移植到了CV领域，打破了这两个领域壁垒，并取得不错的成效。\",\"Vision Transformer的模型结构相比于Transformer来说更简单，在Transformer模型中，主要包含Encoder和Decoder结构，而ViT(Vision Transformer)仅借鉴了Encoder结构。\",\"ViT原论文中最核心的结论是: 当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果。\",\"归纳偏置:\",\"归纳偏置能够帮助学习算法缩小搜索范围，快速找到合适的模型。\",\"例如，在图像分类任务中，如果没有任何归纳偏置，学习算法需要在所有可能的函数空间中搜索最优模型，这几乎是不可能完成的任务。而通过引入特定的归纳偏置，如局部性和平移不变性（CNN 所具备的），可以将搜索范围限制在满足这些性质的模型子空间内，大大提高学习效率。\",\"但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些，因为Transformer和CNN相比缺少归纳偏置（inductive bias），即一种先验知识，提前做好的假设。CNN具有两种归纳偏置，一种是局部性（locality/two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征；一种是平移不变形（translation equivariance）。当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型。\"]},\"288\":{\"h\":\"原理\",\"t\":[\"本文将通过一个花卉分类的实战案例结合ViT原论文，来帮助大家梳理清楚Vision Transformer的核心流程实现。\"]},\"289\":{\"h\":\"0. 数据下载\",\"t\":[\"实验采用的是花蕊数据集，共5个类别，约4000多个样本。\",\"数据集下载：https://pan.baidu.com/s/137mO-7PY1jDq1Wp0NNyT3A?pwd=qvmq\",\"数据集加载代码:\",\"def read_split_data(root: str, val_rate: float = 0.2): random.seed(0) # 保证随机结果可复现 assert os.path.exists(root), \\\"dataset root: {} does not exist.\\\".format(root) # 遍历文件夹，一个文件夹对应一个类别 flower_class = [cla for cla in os.listdir(root) if os.path.isdir(os.path.join(root, cla))] # 排序，保证顺序一致 flower_class.sort() # 生成类别名称以及对应的数字索引 class_indices = dict((k, v) for v, k in enumerate(flower_class)) json_str = json.dumps(dict((val, key) for key, val in class_indices.items()), indent=4) with open('class_indices.json', 'w') as json_file: json_file.write(json_str) train_images_path = [] # 存储训练集的所有图片路径 train_images_label = [] # 存储训练集图片对应索引信息 val_images_path = [] # 存储验证集的所有图片路径 val_images_label = [] # 存储验证集图片对应索引信息 every_class_num = [] # 存储每个类别的样本总数 supported = [\\\".jpg\\\", \\\".JPG\\\", \\\".png\\\", \\\".PNG\\\"] # 支持的文件后缀类型 # 遍历每个文件夹下的文件 for cla in flower_class: cla_path = os.path.join(root, cla) # 遍历获取supported支持的所有文件路径 images = [os.path.join(root, cla, i) for i in os.listdir(cla_path) if os.path.splitext(i)[-1] in supported] # 获取该类别对应的索引 image_class = class_indices[cla] # 记录该类别的样本数量 every_class_num.append(len(images)) # 按比例随机采样验证样本 val_path = random.sample(images, k=int(len(images) * val_rate)) for img_path in images: if img_path in val_path: # 如果该路径在采样的验证集样本中则存入验证集 val_images_path.append(img_path) val_images_label.append(image_class) else: # 否则存入训练集 train_images_path.append(img_path) train_images_label.append(image_class) print(\\\"{} images were found in the dataset.\\\".format(sum(every_class_num))) print(\\\"{} images for training.\\\".format(len(train_images_path))) print(\\\"{} images for validation.\\\".format(len(val_images_path))) plot_image = True if plot_image: # 绘制每种类别个数柱状图 plt.bar(range(len(flower_class)), every_class_num, align='center') # 将横坐标0,1,2,3,4替换为相应的类别名称 plt.xticks(range(len(flower_class)), flower_class) # 在柱状图上添加数值标签 for i, v in enumerate(every_class_num): plt.text(x=i, y=v + 5, s=str(v), ha='center') # 设置x坐标 plt.xlabel('image class') # 设置y坐标 plt.ylabel('number of images') # 设置柱状图的标题 plt.title('flower class distribution') plt.show() return train_images_path, train_images_label, val_images_path, val_images_label\",\"自定义一个MyDataSet类来封装我们加载得到的数据集:\",\"from torch.utils.data import Dataset from PIL import Image import torch class MyDataSet(Dataset): \\\"\\\"\\\"自定义数据集\\\"\\\"\\\" def __init__(self, images_path: list, images_class: list, transform=None): \\\"\\\"\\\" 初始化自定义数据集类 :param images_path: 包含所有图像文件路径的列表 :param images_class: 包含所有图像对应类别的列表，与 images_path 中的图像一一对应 :param transform: 图像预处理的转换操作，默认为 None \\\"\\\"\\\" self.images_path = images_path self.images_class = images_class self.transform = transform def __len__(self): \\\"\\\"\\\" 返回数据集中图像的数量 :return: 数据集中图像的数量 \\\"\\\"\\\" return len(self.images_path) def __getitem__(self, item): \\\"\\\"\\\" 根据索引获取数据集中的图像和对应的标签 :param item: 图像的索引 :return: 经过预处理的图像和对应的标签 \\\"\\\"\\\" # 打开指定索引的图像文件 img = Image.open(self.images_path[item]) # RGB为彩色图片，L为灰度图片 # 检查图像是否为 RGB 模式，如果不是则抛出异常 if img.mode != 'RGB': raise ValueError(\\\"image: {} isn't RGB mode.\\\".format(self.images_path[item])) # 获取对应图像的标签 label = self.images_class[item] # 如果定义了图像预处理转换操作，则对图像进行处理 if self.transform is not None: img = self.transform(img) return img, label @staticmethod def collate_fn(batch): \\\"\\\"\\\" 自定义的批量数据处理函数，用于将一个批次的数据组合成一个张量 :param batch: 一个批次的数据，包含图像和对应的标签 :return: 组合后的图像张量和标签张量 \\\"\\\"\\\" # 官方实现的default_collate可以参考 # https://github.com/pytorch/pytorch/blob/67b7e751e6b5931a9f45274653f4f653a4e6cdf6/torch/utils/data/_utils/collate.py # 将一个批次的数据拆分为图像和标签两个元组 images, labels = tuple(zip(*batch)) # 将图像元组堆叠成一个四维张量，维度为 (batch_size, channels, height, width) images = torch.stack(images, dim=0) # 将标签元组转换为一个一维张量 labels = torch.as_tensor(labels) return images, labels\",\"两点注意:\",\"当使用 DataLoader 从数据集（Dataset）中加载数据时，它会将多个样本收集起来形成一个批次，但默认的组合方式可能不满足所有需求，这时就可以自定义 collate_fn 函数。\",\"@staticmethod 是 Python 中的一个装饰器，用于将一个方法定义为静态方法。静态方法是类中的一种特殊方法，它与类的实例和类本身都没有直接关联，可以直接通过类名调用，不需要创建类的实例。\"]},\"290\":{\"h\":\"1. 图片预处理\",\"t\":[\"预处理这个步骤在论文里并没有详细说明，但是对于ViT这个结构而言，输入的图片尺寸并不是自定义的，ViT-B/16为例，输入的图片尺寸必须为224x224。\",\"在 ViT - B/16 中，“B” 代表的是模型的基础（Base）版本 ，“16” 表示每个图像块的大小是 16x16 像素；ViT 通常在大规模数据集（如 ImageNet）上进行预训练，而预训练过程中使用的输入图像尺寸通常固定为 224x224。在预训练时，模型的参数是根据这个特定尺寸的输入数据进行优化和学习的。当我们在其他任务中使用预训练好的模型时，为了充分利用预训练的权重，也需要保持输入图像尺寸与预训练时一致，这样可以保证模型的特征提取能力和性能。\",\"因此，首先需要对输入图片进行尺寸变化，具体方式可以是直接缩放(Resize)，也可以进行随机裁剪(RandomResizedCrop)。\",\"对数据集和验证集划分之后，这里对训练集的处理方式是随机切成224x224像素的图片，然后进行水平翻转，再进行归一化和标准化处理；对验证集的处理方式是先Resize成256x256的图片，再从中心位置裁剪成224x224，再进行归一化和标准化处理。\",\"# 定义一个字典 data_transform，用于存储训练集和验证集的图像预处理转换操作 data_transform = { # 训练集的预处理转换操作 \\\"train\\\": transforms.Compose([ # 随机裁剪输入图像，将裁剪后的图像调整为 224x224 大小 # 这是一种数据增强的方式，通过随机裁剪可以增加训练数据的多样性，提高模型的泛化能力 transforms.RandomResizedCrop(224), # 以 0.5 的概率随机水平翻转图像 # 同样是数据增强的手段，增加了图像的多样性，有助于模型学习到不同方向的特征 transforms.RandomHorizontalFlip(), # 将 PIL 图像或 NumPy 数组转换为 PyTorch 的张量（Tensor） # 同时会将像素值从 [0, 255] 范围缩放到 [0, 1] 范围 transforms.ToTensor(), # 对图像进行归一化处理 # 第一个参数 [0.5, 0.5, 0.5] 是图像每个通道的均值，第二个参数 [0.5, 0.5, 0.5] 是图像每个通道的标准差 # 归一化有助于模型更快地收敛，提高训练的稳定性 transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) ]), # 验证集的预处理转换操作 \\\"val\\\": transforms.Compose([ # 将图像的短边缩放为 256 像素，长边按比例缩放 # 这一步是为了保证图像的整体比例不变，后续再进行裁剪操作 transforms.Resize(256), # 从图像的中心位置裁剪出 224x224 大小的图像 # 验证集不需要进行数据增强，只需要将图像调整到合适的大小 transforms.CenterCrop(224), # 将 PIL 图像或 NumPy 数组转换为 PyTorch 的张量（Tensor） # 同样会将像素值从 [0, 255] 范围缩放到 [0, 1] 范围 transforms.ToTensor(), # 对图像进行归一化处理，参数与训练集的归一化参数相同 # 保证训练集和验证集的数据处理方式一致 transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) ]) }\",\"下面我们将用于图片变换的transforms流水线和上面自定义的MyDataSet类都封装到DataLoader去。\",\"train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(data_path) # 实例化训练数据集 train_dataset = MyDataSet(images_path=train_images_path, images_class=train_images_label, transform=data_transform[\\\"train\\\"]) # 实例化验证数据集 val_dataset = MyDataSet(images_path=val_images_path, images_class=val_images_label, transform=data_transform[\\\"val\\\"]) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn) val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=val_dataset.collate_fn)\"]},\"291\":{\"h\":\"2. 图片切割\",\"t\":[\"Transformer需要输入的是一维的Token，对于二维的图像，一种朴素的想法就是把一个个像素点拉平，这样就成了一个一维序列。但是这样造成的一个后果是计算量太庞大，比如一张224x224的图片，变成1维度之后就成了50176，相当于直接输入一篇五万字的文章，模型难以计算。\",\"那么，一个改进的想法就是把一张图片分成nxn个Patch，每一个Patch作为一个Token，这样计算量就大大减小了。\",\"以ViT-B/16为例，将输入图片(224x224)按照16x16大小的Patch进行划分，划分后可以得到共个Patch。每个Patch是三通道的小图片，shape为(16, 16, 3)，将其展平就变成了一个长度为768的向量。\",\"每一个向量作为一个单独的输入，那样我们总共有196个向量，在代码中，可以变成一个[196,768]的矩阵，进行并行输入。\",\"这一步的操作在论文中是直接采用切割的处理办法，但是实际的代码实现中，采用了一种更巧妙的解决思路，就是利用一个卷积核大小为16x16，步距为16，卷积核个数为768的卷积层来进行实现。\",\"再来回顾我们的卷积层计算公式：\",\"输入为[224,244,3]，经过卷积层变成[14,14,768]，再映射为[196,768]。\",\"这样，就完成了从图片到Token之间的转换，我们通过自定义一个PatchEmbed类完成上述工作。\",\"class PatchEmbed(nn.Module): \\\"\\\"\\\" 2D Image to Patch Embedding 该类的作用是将二维图像分割成多个图像块（patch），并将这些图像块嵌入到一个低维向量空间中 \\\"\\\"\\\" def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None): \\\"\\\"\\\" 初始化 PatchEmbed 类 :param img_size: 输入图像的尺寸，默认为 224。如果传入一个整数，则表示图像是正方形，边长为该整数； :param patch_size: 每个图像块的尺寸，默认为 16。同样，如果传入一个整数，则表示图像块是正方形，边长为该整数； :param in_c: 输入图像的通道数，默认为 3（对应 RGB 图像） :param embed_dim: 嵌入维度，即每个图像块经过卷积操作后得到的特征向量的维度，默认为 768 :param norm_layer: 归一化层，默认为 None。如果传入一个归一化层类，则使用该层进行归一化；否则使用 nn.Identity() 表示不进行归一化 \\\"\\\"\\\" super().__init__() # 将 img_size 和 patch_size 转换为元组形式，如果传入的是整数，则将其转换为 (整数, 整数) 的形式 img_size = (img_size, img_size) patch_size = (patch_size, patch_size) self.img_size = img_size self.patch_size = patch_size # 计算网格大小，即图像在水平和垂直方向上分别可以划分的图像块数量 self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) # 计算图像块的总数，即网格大小的乘积 self.num_patches = self.grid_size[0] * self.grid_size[1] # 定义一个二维卷积层，用于将输入图像分割成多个图像块并进行嵌入 # in_c 是输入通道数，embed_dim 是输出通道数（也就是卷积核的数量） # kernel_size 是卷积核的大小，这里设置为图像块的大小 # stride 是卷积核的步长，这里设置为图像块的大小，确保卷积操作不会重叠 self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size) # 如果传入了归一化层类，则使用该层进行归一化；否则使用 nn.Identity() 表示不进行归一化 self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() def forward(self, x): \\\"\\\"\\\" 前向传播函数 :param x: 输入的图像张量，形状为 [B, C, H, W]，其中 B 是批量大小，C 是通道数，H 是图像高度，W 是图像宽度 :return: 经过处理后的图像块嵌入张量，形状为 [B, num_patches, embed_dim] \\\"\\\"\\\" # 获取输入图像张量的形状 B, C, H, W = x.shape # 注意下面的embed_dim代表的是卷积核的数量，也就是经过卷积后拼接得到的特征图(输出通道)数量 # H`和 W`代表输出特征图的宽和高 # 首先使用卷积层对输入图像进行处理，得到形状为 [B, embed_dim, H', W'] 的特征图 # 然后将特征图的最后两维展平为一维，得到形状为 [B, embed_dim, num_patches] 的张量 # 最后交换第 1 维和第 2 维，得到形状为 [B, num_patches, embed_dim] 的张量 # 这里的 num_patches 是图像块的总数 x = self.proj(x).flatten(2).transpose(1, 2) # 对处理后的张量进行归一化操作 x = self.norm(x) return x\",\"用一个简化版的例子说明上述过程:\",\"核心要点: 将卷积后的通道维数作为embedding的维度，卷积后剩余的长和宽相乘作为时间维度，由此把图片转换为序列的embedding形式。\"]},\"292\":{\"h\":\"3. 添加[class]token\",\"t\":[\"在上面的结构图中可以看到，输入Encoder的最左侧部分添加了一个0*这个Token，这个就是额外添加的一个[class]token，单独用来处理类别信息，经过Encoder之后，需要单独将这个Token再提取出来，输入到MLP Head之中再输出分类结果。\",\"这也是为什么结构图中MLP Head的位置是和这个[class]token对齐。\",\"这里简单介绍一下CLS TOKEN的作用:\",\"[CLS] Token 的作用是通过训练过程中损失值的降低，模型学会如何通过注意力机制将图像的有效信息汇聚到 [CLS] Token 中，从而完成图像分类任务。\",\"初始化： \",\"[CLS] Token 是一个随机初始化的向量，初始时没有任何语义信息。\",\"位置编码被添加到 patch 嵌入中，以保留图像的空间信息。\",\"前向传播： \",\"输入图像被分割成 patches，并通过线性变换映射到嵌入空间。\",\"[CLS] Token 被添加到 patch 嵌入序列的开头。\",\"通过多层 Transformer Encoder，模型计算每个 patch 嵌入（包括 [CLS] Token）与其他 patch 嵌入的关系。\",\"注意力汇聚： \",\"在每一层 Transformer 中，[CLS] Token 通过自注意力机制与其他 patch 嵌入交互。\",\"模型学会将图像中与分类任务相关的信息汇聚到 [CLS] Token 中。\",\"损失计算与反向传播： \",\"[CLS] Token 的输出向量被输入到分类头中，用于预测图像的类别。\",\"通过计算损失（如交叉熵损失），模型更新参数，使得 [CLS] Token 能够更好地聚合图像信息。\",\"收敛： \",\"随着训练的进行，损失值逐渐降低，模型学会如何通过注意力机制将图像的有效信息汇聚到 [CLS] Token 中。\",\"最终，[CLS] Token 的输出向量能够很好地表示图像的全局特征，用于分类任务。\",\"[CLS] Token 能起作用的原因在于：\",\"注意力机制的特性： \",\"自注意力机制能够捕捉图像中任意两个 patches 之间的关系。\",\"[CLS] Token 通过与其他 patches 的交互，能够动态地聚合图像信息。\",\"训练目标的引导： \",\"训练过程中，损失函数直接作用于 [CLS] Token 的输出。\",\"模型被强制学会将图像的有效信息汇聚到 [CLS] Token 中，以最小化损失。\",\"全局特征表示： \",\"[CLS] Token 位于序列的开头，能够通过多层 Transformer 逐步聚合全局信息。\",\"最终，[CLS] Token 的输出向量能够很好地表示图像的全局特征。\",\"class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, embed_layer=None): \\\"\\\"\\\" Args: img_size (int, tuple): 输入图像的尺寸 patch_size (int, tuple): 图像块的尺寸 in_c (int): 输入图像的通道数 num_classes (int): 分类任务的类别数 embed_dim (int): 嵌入维度 embed_layer (nn.Module): 图像块嵌入层 \\\"\\\"\\\" super(VisionTransformer, self).__init__() # 保存分类任务的类别数 self.num_classes = num_classes # 保存嵌入维度，同时作为特征数量，以保持与其他模型的一致性 self.num_features = self.embed_dim = embed_dim # 只有一个特殊标记（分类标记） self.num_tokens = 1 # 创建图像块嵌入层 self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # 计算图像块的数量 num_patches = self.patch_embed.num_patches # 创建可学习的分类标记 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 使用截断正态分布初始化分类标记 nn.init.trunc_normal_(self.cls_token, std=0.02) ... def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] # 将输入图像进行图像块嵌入 x = self.patch_embed(x) # [B, 196, 768] # [1, 1, 768] -> [B, 1, 768] # 扩展分类标记以匹配输入批次大小 cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将分类标记和图像块嵌入拼接 x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] ... # 返回分类标记对应的特征,x[:,0]对应维度为[B,1,768] return x[:,0]; def forward(self, x): # 提取输入图像的特征 x = self.forward_features(x) # 通过分类头 -- [B,1,768] x = self.head(x) return x\"]},\"293\":{\"h\":\"4. 添加位置编码\",\"t\":[\"在Transformer中，位置编码的作用是为了记忆输入的语序信息。ViT中，同样需要位置编码来记录各图像块之间的位置信息。\",\"这里主要有两种位置编码思路，一种思路是在转换之前(14,14)的图像块矩阵添加二维(2-D)位置编码，另一种思路是在转换后(196+1)这个维度上添加一维(1-D)位置编码。\",\"论文作者也对其做了实验，实验结果如下表所示：\",\" 可以看到，添加一维位置编码和二维位置编码并没有太大的差异。作者随后也对一维位置编码的结果进行了可视化，结果如下图所示：\",\" 上图中是每一个Patch中各位置的位置编码相似性度量，越接近黄色的位置代表越靠近位置编码的中心位置，可以看到，即使是一维位置编码，同样可以比较好地记录二维信息。\",\"class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, representation_size=None, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., embed_layer=None, norm_layer=None, act_layer=None): super(VisionTransformer, self).__init__() # 保存分类任务的类别数 self.num_classes = num_classes # 保存嵌入维度，同时作为特征数量，以保持与其他模型的一致性 self.num_features = self.embed_dim = embed_dim # 只有一个特殊标记（分类标记） self.num_tokens = 1 # 创建图像块嵌入层 self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # 计算图像块的数量 num_patches = self.patch_embed.num_patches # 创建可学习的分类标记 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 创建可学习的位置嵌入 self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim)) # 创建丢弃层，用于位置嵌入后的随机丢弃 self.pos_drop = nn.Dropout(p=drop_ratio) ... # 权重初始化 # 使用截断正态分布初始化位置嵌入 nn.init.trunc_normal_(self.pos_embed, std=0.02) # 使用截断正态分布初始化分类标记 nn.init.trunc_normal_(self.cls_token, std=0.02) ... def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] # 将输入图像进行图像块嵌入 x = self.patch_embed(x) # [B, 196, 768] # [1, 1, 768] -> [B, 1, 768] # 扩展分类标记以匹配输入批次大小 cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将分类标记和图像块嵌入拼接 x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] # 加上位置嵌入并进行随机丢弃 x = self.pos_drop(x + self.pos_embed) ... # 返回分类标记对应的特征 return x[:, 0] def forward(self, x): # 提取输入图像的特征 x = self.forward_features(x) # 通过分类头 x = self.head(x) return x\",\"上面代码实现中使用的是可学习位置嵌入，具体解释如下:\",\"可学习位置嵌入（learnable positional embedding）是指位置嵌入的参数是可以在模型训练过程中通过反向传播算法进行更新的。具体来说，在模型初始化时，位置嵌入会被初始化为一组特定的值（通常是随机初始化或者初始化为零），然后在训练过程中，这些值会根据模型的损失函数不断调整，以使得模型能够学习到最适合当前任务的位置表示。\"]},\"294\":{\"h\":\"5. Encoder\",\"t\":[\"ViT虽然采用的是Transformer Encoder的结构，但是和Transformer原始的Encoder还是有所区别，我将两者的结构进行对比，如下图所示，左侧为Transformer原始的Encoder结构。\",\" 可以看到，大致上两者结构是相同的，主要区别在于Norm层的顺序，原始Transformer的Norm层在多头注意力和前馈网络之后，而ViT将其放到前面，这里的原因，论文里没有做解释。\",\"关于Norm层，ViT仍是采用Transformer中用到Layer Normalization，计算公式如下：\",\"Norm层之后同样是多头注意力层(Multi-Head Attention)，和Transformer中的一样。\",\"后面的MLP是个单独的结构，就是两个线性层+GELU激活函数+Dropout的结构 ：\",\" MLP Block 中第一个线性层把输入特征投影到一个更高维度的空间后，不同特征之间能够进行更多样的组合。这有助于模型发现输入数据中更复杂的模式和关系。第二个线性层再把高维特征映射回原来的维度，这样就可以提取出对最终任务有帮助的特征组合。\",\"单一的线性层只能进行线性变换，其表达能力是有限的。在两个线性层之间通常会插入一个非线性激活函数（如 GELU），这样就能让 MLP 学习到输入数据的非线性特征。第一个线性层将输入特征映射到更高维度的空间，在这个高维空间里，数据的分布更加稀疏，也就为非线性激活函数提供了更多可以学习的特征组合，从而增强了模型的表达能力。\",\"class Block(nn.Module): def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm): super(Block, self).__init__() # 第一个归一化层，对输入进行归一化处理 self.norm1 = norm_layer(dim) # 多头自注意力层 self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio) # DropPath 层，用于随机深度，当 drop_path_ratio 大于 0 时使用，否则使用恒等映射 self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity() # 第二个归一化层，对经过注意力层的输出进行归一化处理 self.norm2 = norm_layer(dim) # 计算 MLP 的隐藏维度 mlp_hidden_dim = int(dim * mlp_ratio) # 创建 MLP 层 self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio) def forward(self, x): # 残差连接：输入加上经过归一化和注意力层处理后的输出 x = x + self.drop_path(self.attn(self.norm1(x))) # 残差连接：输入加上经过归一化和 MLP 层处理后的输出 x = x + self.drop_path(self.mlp(self.norm2(x))) return x\",\"class Mlp(nn.Module): \\\"\\\"\\\" MLP as used in Vision Transformer, MLP-Mixer and related networks \\\"\\\"\\\" def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.): super().__init__() # 如果未指定 out_features，则默认为 in_features out_features = out_features or in_features # 如果未指定 hidden_features，则默认为 in_features hidden_features = hidden_features or in_features # 第一个全连接层，将输入特征映射到隐藏特征空间 self.fc1 = nn.Linear(in_features, hidden_features) # 激活函数层，默认使用 GELU 激活函数 self.act = act_layer() # 第二个全连接层，将隐藏特征映射到输出特征空间 self.fc2 = nn.Linear(hidden_features, out_features) # Dropout 层，用于防止过拟合 self.drop = nn.Dropout(drop) def forward(self, x): # 通过第一个全连接层 x = self.fc1(x) # 通过激活函数层 x = self.act(x) # 应用 Dropout x = self.drop(x) # 通过第二个全连接层 x = self.fc2(x) # 再次应用 Dropout x = self.drop(x) return x\",\"一个block之后维度依然和输入相同，都是197 x 768 ，因此可以堆叠多个block。\"]},\"295\":{\"h\":\"6. 多头自注意力\",\"t\":[\"ViT中的多头自注意力模块实现逻辑和Transformer基本一致，主要的区别就是去掉了Paddding_Mask和Casual_Mask部分相关的掩码逻辑。\",\"下面所给出的代码实现，注意是通过一个线性层来同时计算qkv三个矩阵，这样可以提升计算效率。\",\"class Attention(nn.Module): def __init__(self, dim, # 嵌入层维度 num_heads=8, # 注意力头的数量，默认为8 qkv_bias=False, # 是否在生成Q、K、V时使用偏置，默认为False qk_scale=None, # 缩放因子，用于调整注意力分数，若为None则使用默认值 attn_drop_ratio=0., # 注意力矩阵的丢弃率，默认为0 proj_drop_ratio=0.): # 投影层的丢弃率，默认为0 super(Attention, self).__init__() self.num_heads = num_heads # 保存注意力头的数量 head_dim = dim // num_heads # 计算每个注意力头的维度 self.scale = qk_scale or head_dim ** -0.5 # 确定缩放因子，若qk_scale未指定，则使用默认的缩放因子 # 定义一个线性层，将输入的维度dim映射到dim * 3，用于同时生成查询（Q）、键（K）和值（V） self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) # 定义注意力矩阵的丢弃层，防止过拟合 self.attn_drop = nn.Dropout(attn_drop_ratio) # 定义投影层，将多头注意力的输出进行线性变换 self.proj = nn.Linear(dim, dim) # 定义投影层的丢弃层，防止过拟合 self.proj_drop = nn.Dropout(proj_drop_ratio) # 没有padding_mask, casual_mask def forward(self, x): # [batch_size, num_patches + 1, total_embed_dim] # 获取输入张量x的形状，B为批量大小，N为序列长度（包含分类token），C为输入token的总维度 B, N, C = x.shape # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim] # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head] # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head] # 通过qkv线性层将输入x映射到dim * 3的维度，然后调整形状并重新排列维度 # 下面的3是因为我们用一次矩阵运算得到了拼接在一起的Q,K,V矩阵，这里需要将其分离开来 qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) # [batch_size, num_heads, num_patches + 1, embed_dim_per_head] # 从qkv张量中分离出查询（Q）、键（K）和值（V） # 注意: Q,K,V计算来源相同,因此是自注意力 q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use tensor as tuple) # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1] # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1] # 将Q和K的转置相乘，得到注意力分数矩阵，再乘以缩放因子scale attn = (q @ k.transpose(-2, -1)) * self.scale # 对注意力分数矩阵应用softmax函数，得到注意力权重矩阵 attn = attn.softmax(dim=-1) # 对注意力权重矩阵应用丢弃层，防止过拟合 attn = self.attn_drop(attn) # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head] # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head] # reshape: -> [batch_size, num_patches + 1, total_embed_dim] # 将注意力权重矩阵与V相乘，得到每个注意力头的输出 # 对输出进行维度交换和形状调整，将多个注意力头的输出合并为一个张量 x = (attn @ v).transpose(1, 2).reshape(B, N, C) # 通过投影层对合并后的张量进行线性变换 x = self.proj(x) # 对投影后的结果应用丢弃层，防止过拟合 x = self.proj_drop(x) return x\",\"关于多头注意力机制流程不太清楚的，可以看这篇文章。\"]},\"296\":{\"h\":\"7. MLP Head\",\"t\":[\"在Transformer Encoder输出结果之后，需要再将第一个添加的Class Token提取出来，然后输入到MLP Head进行分类。在论文中，作者先是在ImageNet21K上进行预训练，MLP Head结构由Linear+tanh激活函数+Linear组成，但是迁移到其它数据集训练时，只需要用一个一个Linear即可。\",\"class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, representation_size=None, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., embed_layer=None, norm_layer=None, act_layer=None): super(VisionTransformer, self).__init__() # 保存分类任务的类别数 self.num_classes = num_classes # 保存嵌入维度，同时作为特征数量，以保持与其他模型的一致性 self.num_features = self.embed_dim = embed_dim # 只有一个特殊标记（分类标记） self.num_tokens = 1 # 如果没有提供归一化层，则使用默认的 LayerNorm，epsilon 为 1e-6 norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6) # 如果没有提供激活函数层，则使用 GELU 激活函数 act_layer = act_layer or nn.GELU # 创建图像块嵌入层 self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # 计算图像块的数量 num_patches = self.patch_embed.num_patches # 创建可学习的分类标记 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 创建可学习的位置嵌入 self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim)) # 创建丢弃层，用于位置嵌入后的随机丢弃 self.pos_drop = nn.Dropout(p=drop_ratio) # 创建Encoder Block块序列 self.blocks = nn.Sequential(*[ Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i], norm_layer=norm_layer, act_layer=act_layer) for i in range(depth) ]) # 创建归一化层 self.norm = norm_layer(embed_dim) ############################# MLP Head ############################################ # 更新特征数量为表示层的维度 self.num_features = representation_size # 创建预输出层，包含一个线性层和一个 Tanh 激活函数 self.pre_logits = nn.Sequential(OrderedDict([ (\\\"fc\\\", nn.Linear(embed_dim, representation_size)), (\\\"act\\\", nn.Tanh()) ])) # 分类头 # 如果类别数大于 0，则创建线性分类头，否则为恒等映射 self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity() ########################################################################### # 权重初始化 # 使用截断正态分布初始化位置嵌入 nn.init.trunc_normal_(self.pos_embed, std=0.02) # 使用截断正态分布初始化分类标记 nn.init.trunc_normal_(self.cls_token, std=0.02) # 应用自定义的权重初始化函数 self.apply(_init_vit_weights) def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] # 将输入图像进行图像块嵌入 x = self.patch_embed(x) # [B, 196, 768] # [1, 1, 768] -> [B, 1, 768] # 扩展分类标记以匹配输入批次大小 cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将分类标记和图像块嵌入拼接 x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] # 加上位置嵌入并进行随机丢弃 x = self.pos_drop(x + self.pos_embed) # 通过Encoder Block块序列 x = self.blocks(x) # 进行归一化 x = self.norm(x) # 返回分类标记对应的特征 -- 先交给预输出层进行处理 return self.pre_logits(x[:, 0]) def forward(self, x): # 提取输入图像的特征 x = self.forward_features(x) # 通过分类头 -- 映射到分类空间中去 x = self.head(x) return x\",\"self.pre_logits 模块可以看作是一个特征预处理模块，它位于最终分类头之前。通过将特征映射到特定的维度并进行非线性变换，该模块可以为后续的分类任务提供更具区分性和稳定性的特征表示，从而提高模型的分类性能。\",\"输出结果之后，再和真实标签做交叉熵损失，这样就可以完成ViT的训练过程。\",\"def train_one_epoch(model, optimizer, data_loader, device, epoch): ... # 遍历数据加载器中的每个批次数据 for step, data in enumerate(data_loader): # 解包数据，得到图像和对应的标签 images, labels = data # 累加当前批次的样本数到总样本数中 sample_num += images.shape[0] # 将图像数据移动到指定设备上，并通过模型进行前向传播，得到预测结果 pred = model(images.to(device)) # 从预测结果中找出每个样本预测概率最大的类别索引 pred_classes = torch.max(pred, dim=1)[1] # 计算预测正确的样本数，并累加到累计正确样本数中 accu_num += torch.eq(pred_classes, labels.to(device)).sum() # 计算预测结果与真实标签之间的交叉熵损失 loss = loss_function(pred, labels.to(device)) # 进行反向传播，计算梯度 loss.backward() ...\"]},\"297\":{\"h\":\"效果对比\",\"t\":[\"在论文中，作者将ViT和之前图像分类领域比较强的ResNet模型进行了对比测试，结果如下：\",\" 可以看到，右图中，作者使用了谷歌制作的JFT-300M数据集，当数据量小于30M时，ViT的效果表现不如ResNet，但是当数据量逐渐增大时，ViT才会慢慢超越ResNet。由此可见ViT工作的局限性，它必须要在超大数据集上进行预训练，然后再拿到其它数据集上做迁移学习，才会有好的效果。\",\"关于ViT模型的不同版本，论文里也做了说明： 其中的Layers就是Transformer Encoder中重复堆叠Encoder Block的次数，Hidden Size就是对应通过Embedding层后每个token的dim（向量的长度），MLP size是Transformer Encoder中MLP Block第一个全连接的节点个数（是Hidden Size的四倍），Heads代表Transformer中Multi-Head Attention的heads数。\",\"在深度学习领域，当提到模型参数量时，“M” 通常是 “million” 的缩写，代表 “百万”。所以参数量为 86M 就意味着模型大约有 86×1000000 = 8600000（八百六十万）个可训练参数。\",\"与之类似的还有 “B”，它是 “billion” 的缩写，代表 “十亿”。例如参数量为 1.2B 就表示模型大约有 1.2×1000000000 = 1200000000（十二亿）个可训练参数。\"]},\"298\":{\"h\":\"注意力可视化\",\"t\":[\"ViT这篇论文长达二十多页，里面包含了非常丰富的成果，其中包括注意力可视化。由于作者是首次将Transformer应用到图像领域，里面包含了注意力机制，那么作者就想把注意力得到的结果(也就是Q-K矩阵乘积)换源到图像上，得到结果如下图所示：\",\"可以看到，模型自动学习到了如果注意画面中的分类主体。\"]},\"299\":{\"h\":\"混合模型探索\",\"t\":[\"在论文的最后，作者又探索了一种混合模型(Hybrid)，就是将传统CNN和Transformer进行结合。\",\"下表中对比了ViT、ResNet和混合模型在不同图像分类数据集上的测试结果，可以看到当Epochs增大时，ResNet和混合模型的效果均不如ViT模型。\",\"混合模型的常见结合方式:\",\"CNN 作为特征提取器，Transformer 作为编码器 \",\"先用 CNN 对输入数据进行初步的特征提取，利用 CNN 的局部特征提取能力快速捕捉图像的底层特征。例如，在图像分类任务中，可以使用预训练的 ResNet 等 CNN 模型提取图像的特征图。\",\"然后将 CNN 提取的特征图转换为序列形式，输入到 Transformer 中进行进一步的处理。Transformer 可以利用其自注意力机制捕捉特征之间的长距离依赖关系，对特征进行更深入的建模。\",\"交错堆叠 CNN 和 Transformer 模块 \",\"在模型架构中，将 CNN 层和 Transformer 层交错堆叠。例如，先经过一层或多层 CNN 进行局部特征提取，然后再经过一层 Transformer 捕捉全局信息，如此反复。这样可以在模型的不同阶段交替利用 CNN 和 Transformer 的优势。\",\"在 Transformer 中引入卷积操作 \",\"在 Transformer 的架构中融入卷积操作，例如在多头自注意力机制或前馈网络中引入卷积层。这样可以为 Transformer 赋予局部特征提取的能力，同时保留其捕捉长距离依赖的优势。\"]},\"300\":{\"h\":\"加载预训练模型\",\"t\":[\"上面已经给出了数据集加载以及ViT模型核心代码实现了，下面我们将进入训练流程；首先说明，本次训练是基于预训练好的ViT-B/16这个模型进行微调，整体结构图如下：\",\"具体为vit_base_patch16_224_in21k这个模型:\",\"vit：代表 Vision Transformer。\",\"base：表示模型的规模。\",\"patch16：意味着在处理图像时，会将输入图像分割成大小为 16×16 像素的图像块（patches）。\",\"224：指的是输入图像的尺寸为 224×224 像素。在预训练和使用该模型时，需要将输入图像调整为这个固定的尺寸。\",\"in21k：该模型是在 ImageNet - 21k 数据集上进行预训练的。ImageNet - 21k 是一个大规模的图像数据集，包含大约 21000 个类别和 1.4 亿张图像。在如此大规模的数据集上进行预训练，模型能够学习到丰富的图像特征和模式，具有较强的泛化能力。\",\"def vit_base_patch16_224_in21k(num_classes: int = 21843, has_logits: bool = True): \\\"\\\"\\\" ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929). ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer. weights ported from official Google JAX impl: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch16_224_in21k-e5005f0a.pth \\\"\\\"\\\" model = VisionTransformer(img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, representation_size=768 if has_logits else None, num_classes=num_classes) return model # 加载预训练好的vit_base_patch16_224_in21k模型权重文件 model = vit_base_patch16_224_in21k(num_classes=5, has_logits=False).to(device) weights_dict = torch.load(args.weights, map_location=device) model.load_state_dict(weights_dict, strict=False)\",\"加载该模型后，训练了10个epoch，验证集上准确率达到了98.5%。整体模型还是比较大的，预训练权重大小为393MB，但是训练速度还是挺快的，因为在代码中有个冻结权重的操作，主干部分全部冻结，仅训练分类头。\",\"for name, para in model.named_parameters(): # 除head, pre_logits外，其他权重全部冻结 if \\\"head\\\" not in name and \\\"pre_logits\\\" not in name: para.requires_grad_(False) else: print(\\\"training {}\\\".format(name))\",\"训练与评估流程的代码为模版代码，考虑篇幅原因，这里不再贴出，大家可以自行拉取项目完整代码进行学习:\",\"https://pan.baidu.com/s/1rkdjdlR37O7gSr9j1mhjBg?pwd=vket\"]},\"301\":{\"h\":\"总结\",\"t\":[\"Vision Transformer证明了使用Transformer结构可以有效处理图像数据，并且取得了与卷积神经网络（CNN）相媲美的效果。\",\"统一多模态的可能性：使用Transformer架构为未来的多模态统一提供了可能性。\",\"图像到文本的桥梁：架起了图像空间到文本空间的桥梁。\",\"ViT核心：如何将二维图像转换为一维时间序列？通过将图像切成小片（Patches），并按行优先排序来实现。\"]},\"302\":{\"h\":\"课程笔记\"},\"303\":{\"h\":\"开源项目\"},\"304\":{\"h\":\"Attention运算过程中维度变换的理解\",\"t\":[\"Attention运算过程中维度变换的理解\",\"在注意力机制（特别是 Transformer 中的 自注意力机制）中，Q（Query）、K（Key）、V（Value） 的维度对最终注意力输出的结果维度有直接影响。我们来一步步分析这个过程：\"]},\"305\":{\"h\":\"一、注意力机制的基本流程\",\"t\":[\"在标准的 缩放点积注意力（Scaled Dot-Product Attention） 中，计算公式如下：\",\"其中：\",\"：query的数量（如句子长度）\",\"：key/value的数量（也通常是句子长度）\",\"：每个 query 和 key 的维度\",\"：每个 value 的维度\"]},\"306\":{\"h\":\"二、Q、K、V 的初始维度对结果的影响\"},\"307\":{\"h\":\"1.\",\"t\":[\"这是注意力权重矩阵的来源。\",\"所以\",\"👉 这个矩阵表示的是每个 query 对应所有 key 的相似度（即注意力得分），共个值。\"]},\"308\":{\"h\":\"2.\",\"t\":[\"对每一行做 softmax，得到归一化的注意力权重：\",\"输入：\",\"输出：仍是\"]},\"309\":{\"h\":\"3.\",\"t\":[\"注意力权重：\",\"Value 矩阵：\",\"结果：\",\"👉 最终输出的维度是，也就是和输入的 query 数量一致，但每个输出向量的维度由 value 的维度决定。\"]},\"310\":{\"h\":\"三、总结：输入维度 → 输出维度\",\"t\":[\"输入\",\"维度\",\"含义\",\"Query (Q)\",\"查询向量，n 是序列长度\",\"Key (K)\",\"键向量，用于匹配查询\",\"Value (V)\",\"值向量，实际携带信息\",\"输出\",\"维度\",\"含义\",\"Attention Output\",\"每个 query 聚合了所有 value 的加权信息\"]},\"311\":{\"h\":\"四、如何理解这个过程？\",\"t\":[\"我们可以从以下角度理解：\"]},\"312\":{\"h\":\"✅ 1.\",\"t\":[\"每个 Query 都是在寻找最相关的 Key。\",\"根据相关性（注意力权重），从对应的 Value 中提取信息。\",\"最终每个 Query 得到一个融合了上下文信息的向量。\"]},\"313\":{\"h\":\"✅ 2.\",\"t\":[\"控制了相似度计算的维度，影响模型容量和梯度稳定性。\",\"决定了输出的信息维度，可以独立于设计。\",\"这种分离的设计让模型更灵活，比如多头注意力中可以分别控制每个 head 的表达能力。\"]},\"314\":{\"h\":\"✅ 3.\",\"t\":[\"类似数据库查询： \",\"Query 是你输入的问题；\",\"Key 是数据库中的索引；\",\"Value 是数据库中的内容；\",\"Attention 就是根据问题找到相关内容并返回。\"]},\"315\":{\"h\":\"五、例子说明（以 Transformer 为例）\",\"t\":[\"假设我们在 Transformer 中：\",\"输入是一个 batch of sequences，shape 为\",\"我们通过线性变换得到： \",\"那么最终输出为：\",\"如果使用多头注意力（Multi-head Attention），我们会拼接多个这样的头，最后再经过一个线性层映射回原始维度。\"]},\"316\":{\"h\":\"六、常见疑问解答\"},\"317\":{\"h\":\"❓Q: 为什么 和 可以不同？\",\"t\":[\"因为它们的作用不同：\",\"是用于计算相似度的维度；\",\"是用于信息表达的维度；\",\"两者解耦可以让模型更灵活地分配资源。\"]},\"318\":{\"h\":\"❓Q: 为什么要除以 ？\",\"t\":[\"防止内积过大导致 softmax 梯度消失。 当较大时，QK^T 的数值会很大，除以可以缓解这个问题。\"]},\"319\":{\"h\":\"七、可视化示意\",\"t\":[\"Q: [n x dk] K: [m x dk] V: [m x dv] ↓ ↓ ↓ Q @ K.T → [n x m] ↓ ↓ ↓ softmax → [n x m] V → [m x dv] ↓__________________________↓ ↓ Output → [n x dv]\"]},\"320\":{\"h\":\"Pytorch张量存储与访问原理\",\"t\":[\"Pytorch张量存储与访问原理\"]},\"321\":{\"h\":\"引言\",\"t\":[\"张量（Tensor）是数学和物理学中用于表示多维数组的一个概念，在机器学习、深度学习等领域也得到了广泛应用，简单来说，张量可以被视为一种广义的矩阵，它可以拥有任意数量的维度。我们生成一个3*3 的张量,如下所示:\",\"import torch t = torch.tensor([[1,2,3],[4,5,6],[7,8,9]]) print(t, t.shape) output: tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) torch.Size([3, 3])\",\"我们知道，计算机的内存（RAM）本质上是一个线性、一维的存储空间。每一个存储单元（通常是字节，byte）都有一个唯一的地址，这些地址从 0 开始顺序排列，形成一个连续的一维空间。因此不论张量是多少维的，最终都会被映射为一维。在映射机制里，涉及两个重要概念，连续性(contiguity rule)和步长(strides)。\"]},\"322\":{\"h\":\"连续性\",\"t\":[\"连续性: 数据元素在内存空间的排列顺序，主要包括行优先和列优先两类，Pytorch 默认使用行优先方式。\",\"在行优先顺序（Row-major order）中，，内存先存储第 0 行的所有元素，接着是第 1 行的所有元素，依此类推。对于上述代码中的矩阵，其在内存中行优先布局:\",\"1,2,3,4,5,6,7,8,9\",\"在列优先顺序（Column-major order）中，内存先存储第 0 列的所有元素，接着是第 1 列的所有元素，依此类推。对于上述代码中的矩阵，其在内存中列优先布局:\",\"1,4,7,2,5,8,3,6,9\"]},\"323\":{\"h\":\"步长\",\"t\":[\"步长解决的是如何将多维张量中的 (i, j) 或 (i, j, k, …) 索引映射到内存中的地址，比如在内存中从一个索引移动到另一个索引时，沿着某个特定维度需要跨越多少个元素（不一定是字节）。\",\"行优先存储模式的步长，如下例所示:\",\"import torch x = torch.tensor([[1, 2, 3], [4, 5, 6]]) print(\\\"Tensor:\\\") print(x) print(\\\"Strides:\\\", x.stride()) # 输出: (3, 1) # 行优先（Row-major）二维布局 - 形状（Shape）：`[2, 3]`（2 行，3 列） - 典型步长（Strides）：`[3, 1]` 解释： - 第一个维度（“行”维度）的步长是 `3`，意味着如果你从第 `i` 行移动到第 `i+1` 行，你需要在内存中跳过 `3` 个元素。 - 第二个维度（“列”维度）的步长是 `1`，意味着相邻列的元素在内存中也是相邻的。\",\"列优先存储模式的步长，如下例所示:\",\"import numpy as np import torch #创建一个 NumPy 的列优先（column-major）数组 a = np.array([[1, 2, 3], [4, 5, 6]], order='F') # 'F' 表示 Fortran order（列优先） x = torch.from_numpy(a) print(x) print(\\\"Strides:\\\", x.stride()) #输出：(1，2) #列优先（Column-major）二维布局 - 形状（Shape）：`[2, 3]`（2 行，3 列） - 典型步长（Strides）：`[1, 2]` 解释： - 第一个维度（“行”维度）的步长是 `1`，意味着从第 `i` 行移动到第 `i+1` 行只需要在内存中前进一步。 - 第二个维度（“列”维度）的步长是 `2`，意味着从第 `j` 列跳转到第 `j+1` 列时，你需要跳过 `2` 个元素\",\"再来看一个多维张量的例子:\",\"import torch t = torch.arange(0, 24).reshape(1, 2, 3, 4) print(t.stride()) torch.arange(0, 24) 创建了一个一维张量，包含从 0 到 23 的 24 个数字： [ 0, 1, 2, ..., 23] 然后 .reshape(1, 2, 3, 4) 将其重塑为一个 4 D 张量，其形状是： (第0维: batch=1, 第1维: channel=2, 第2维: height=3, 第3维: width=4) 也就是说这个张量可以看作是一个 batch size 为 1、有 2 个通道、每个通道有 3 行 4 列的数据。 tensor([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]]) 执行 print(t.stride())，输出为： (24, 12, 4, 1) 这表示在内存中访问该张量时，每个维度上的“步长”分别是： dim=0 (batch) 24 要跳到下一个 batch 需要移动 24 个元素（但因为 batch=1，实际上没用） dim=1 (channel) 12 要跳到下一个通道需要移动 12 个元素 dim=2 (height/行) 4 要跳到下一行需要移动 4 个元素 dim=3 (width/列) 1 要跳到下一列只需要移动 1 个元素\"]},\"324\":{\"h\":\"张量变换操作\"},\"325\":{\"h\":\"切片(Slice)\",\"t\":[\"切片，指从一个大的数据结构中提取出一部分连续的数据子集，如下所示:\",\"import torch # 创建一个张量 tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) # 获取第二行 print(tensor[1, :]) # 输出: tensor([4, 5, 6]) # 获取第二列 print(tensor[:, 1]) # 输出: tensor([2, 5, 8]) # 获取子张量：前两行、前两列 print(tensor[:2, :2]) # 输出: tensor([[1, 2], # [4, 5]]) # 每隔一行 + 每隔一列 print(tensor[::2, ::2]) # 输出: tensor([[1, 3], # [7, 9]])\",\"切片索引语法: tensor[start:stop:step]\",\"对一个张量（或矩阵）进行“切片”时，实际上并没有复制内存中的任何数据值。相反，这是通过“零拷贝”的方式，调整以下三个要素来创建一个新的视图（view）。\",\"这三个关键要素是：\",\"起始偏移量 (Base offset / Data pointer offset): 表示新视图从原始内存块中的哪个位置开始; 它是一个指向内存中某个元素的指针偏移量。\",\"形状 (Shape): 描述这个新视图的维度大小; 例如：原张量是 3x4，切片后可能是 2x3 或 1x4。\",\"步长 (Strides): 指明在每个维度上移动一个索引单位时，需要跨越多少个内存元素; 通过调整步长，可以实现按行、列或其他任意模式访问数据。\",\"具体的，假设有一个 3×4 的张量（即形状为 [3, 4]），在内存中是行优先存储：\",\"[[A00, A01, A02, A03], [A10, A11, A12, A13], [A20, A21, A22, A23]]\",\"对应的内存布局为一维数组：\",\"[A00, A01, A02, A03, A10, A11, A12, A13, A20, A21, A22, A23]\",\"如果我们做如下切片操作（取第1行到第3行，第1列到第3列）：\",\"sub_tensor = tensor[1:, 1:3]\",\"结果是一个形状为 [2, 2] 的新视图：\",\"[[A11, A12], [A21, A22]]\",\"但此时并没有复制任何数据，而是通过以下方式创建了一个新的“视图”，数据还是指向内存中已有的数据。\",\"属性\",\"值\",\"说明\",\"数据指针偏移\",\"指向 A11 的位置\",\"即内存地址偏移 5（从 A00 开始数）\",\"形状（shape）\",\"[2, 2]\",\"表示两行两列\",\"步长（strides）\",\"[4, 1]\",\"行步长为 4（跳过一行），列步长为 1（逐列）\",\"其下标索引公式可表示为:\"]},\"326\":{\"h\":\"转置(Transpose)\",\"t\":[\"转置,可以理解为对多维张量的各个维度（轴）进行重新排列。\",\"比如，对于一个二维矩阵（形状为 [m, n]），最简单的转置就是将行和列交换，得到一个新的形状为 [n, m] 的矩阵。对于更高维的张量，”转置” 通常意味着对各个轴进行更一般的排列组合。例如，将一个形状为 [D0, D1, D2] 的张量转置为 [D2, D0, D1]。\",\"转置包括逻辑转置和物理转置。\",\"基于步长的逻辑转置（零拷贝）：我们可以通改变对内存中数据缓冲区的解读方式来实现逻辑上的转置，而不是真正地重新排列数据。\",\"比如，在一个简单的二维情况（行优先布局）中：\",\"原始的步长可能是 strides = [n, 1]，对应一个形状为 [m, n] 的数组。\",\"当进行转置时，只需将这些步长交换为 [1, m]，就可以“按转置顺序”来读取数据。\",\"这种方法不需要复制数据，但要注意的是，如果原始数据是按行优先方式存储的，这种转置后的布局在缓存访问上可能效率较低。具体示例如下所示:\",\"import torch # 创建一个张量 t = torch.tensor([[1, 2, 3], [4, 5, 6]]) print(\\\"Original tensor:\\\") print(t) print(\\\"Strides:\\\", t.stride()) # 输出: (3, 1) # 逻辑转置 t_trans = t.t() # 或者使用 t.transpose(0, 1) print(\\\"\\\\nTransposed view:\\\") print(t_trans) print(\\\"Strides after transpose:\\\", t_trans.stride()) # 输出: (1, 3)\",\"物理转置（拷贝）：实际创建一个新的数据缓冲区，并将元素按照转置后的位置写入新内存，使转置后的数据在新的布局中是连续存储的。虽然这种方式需要花费时间和内存来完成拷贝，但它能为后续许多针对转置数据的操作带来更好的内存局部性（memory locality），比如在 TensorRT、ONNX Runtime等加速库中可以更好地利用缓存从而提升性能。\",\"import torch # 创建一个张量 t = torch.tensor([[1, 2, 3], [4, 5, 6]]) # 创建一个连续的转置副本 t_trans_contig = t.t().contiguous() print(\\\"\\\\nPhysically transposed and contiguous tensor:\\\") print(t_trans_contig) print(\\\"Strides after contiguous:\\\", t_trans_contig.stride()) # 输出: (2, 1) print(\\\"Is contiguous?\\\", t_trans_contig.is_contiguous()) # 应为 True\",\"值得注意的是，逻辑转置中，转置后的矩阵步长是(1,3)，而物理转置中，转置后的步长是(2,1)。\",\"这里解释一下，前提条件是行优先存储，在逻辑转置中，转置前\",\"[[1, 2, 3], [4, 5, 6]]\",\"底层数据存储为[1,2,3,4,5,6]，转置后\",\"[[1, 4], [2, 5], [3, 6]]\",\"底层内存中的数据并没有被重新排列！还是：[1, 2, 3, 4, 5, 6]，只是访问方式变了 —— 现在我们按“列优先”的方式去读这个数组，所以正确的strides是(1,3)。\",\"而在物理转置中，由于使用了.contiguous()，内存空间得以连续布局，数据在底层存储为[1, 4, 2, 5, 3, 6]，这个新内存布局使得每一行是连续的，因此步长为 (2,1)。\",\"我们可以把 .contiguous() 理解为：“我不管你现在怎么解读这块内存，我现在要把你的数据重新整理成行优先、连续存储的方式。”\"]},\"327\":{\"h\":\"广播(BroadCast)\",\"t\":[\"广播，是在张量（或数组）运算中非常常见且强大的一种机制，它允许不同形状的张量进行逐元素（element-wise）操作，而不需要显式复制数据。\",\"我们使用了一个示例来说明这一概念：对两个张量 a 和 b 进行相加操作，其中 a.shape = [4, 3]，b.shape = [1, 3]，如下图所示\",\"该图展示了 广播（Broadcast） 在数组运算中的底层实现原理，重点在于 如何通过调整步幅（strides）实现数据虚拟扩展，而非物理复制数据。\",\"数组 a：形状为 4×3，假设为常规行优先存储（如 strides=[3,1]）。\",\"数组 b：原始形状为 1×3，数据内容 [1, 2, 3]，初始步幅 strides=[3,1]。\",\"将 b 从 1×3虚拟扩展为 4×3，使其能与 a 进行逐元素运算（如加法）。将维度大小为 1 的轴扩展至目标大小（此处将行维度从 1 扩展到 4）。\",\"广播通过 设置步幅为 0 实现虚拟扩展：\",\"调整后的 b 属性：\",\"shape = [4, 3]，扩展后的逻辑形状。\",\"data = [1, 2, 3]，物理数据未改变。\",\"strides = [0, 1]，行维度步幅 0：表示在行方向移动时，内存地址不变（复用同一行数据），列维度步幅 1：与原始步幅一致，按元素大小（假设为 1 字节）步进。\",\"扩展后的 b 在逻辑上表现为每行都是 [1, 2, 3]，但物理内存中数据不复制。根据公式\",\"对任意行 i（0 ≤ i <4），代入调整后的步幅 strides=[0, 1]，所有行均指向原始数据的第 j 个元素，实现虚拟复制\"]},\"328\":{\"h\":\"维度问题\",\"t\":[\"3 轴张量，形状：[3, 2, 5] 的多种呈现方式\",\"四阶张量的例子\",\"TensorFlow/PyTorch中使用比较多的tensor的阶为4，shape为[Batch,Height,Weight,Features]\",\"n阶张量的排列规律如下图所示\",\"可以将规律总结为：从shape列表的最右边往左遍历，最开始三个阶按照“下-右-里”的顺序排列，然后打包成一个group，再将整个group按照“下-右-里”的顺序排列，满三次后再打包成一个group，如此往复循环。\",\"本部分图片来源: 张量简介\"]},\"329\":{\"h\":\"杂谈\"},\"330\":{\"h\":\"conda虚拟环境管理\",\"t\":[\"conda虚拟环境管理\"]},\"331\":{\"h\":\"一、创建新环境\",\"t\":[\"基本语法：\",\"conda create --name <环境名> [包名]\",\"可使用 -name（或 n）来命名环境。\",\"示例1：创建一个空环境（只包含 Python）\",\"conda create --name myenv\",\"示例2：创建环境时指定 Python 版本\",\"conda create --name myenv python=3.9\",\"示例3：创建环境并安装一些常用包\",\"conda create --name myenv python=3.8 numpy pandas\"]},\"332\":{\"h\":\"二、激活（切换）环境\",\"t\":[\"激活环境的命令：\",\"conda activate <环境名>\",\"示例：\",\"conda activate lmaffordance3d\",\"激活后，你的终端提示符通常会显示当前环境的名字，例如：\",\"(myenv) user@machine:~$\"]},\"333\":{\"h\":\"三、退出当前环境\",\"t\":[\"要退出当前激活的环境，返回 base 环境：\",\"conda deactivate\"]},\"334\":{\"h\":\"四、查看所有已创建的环境\",\"t\":[\"你可以使用以下命令查看你所有的 conda 环境：\",\"conda env list # 或者 conda info --envs\",\"输出示例：\",\"# conda environments: # base * /home/user/anaconda3 myenv /home/user/anaconda3/envs/myenv testenv /home/user/anaconda3/envs/testenv\",\"注：带星号 * 的表示当前激活的环境。\"]},\"335\":{\"h\":\"五、删除已创建的环境\",\"t\":[\"如果你想删除某个环境，可以使用：\",\"conda env remove -n myenv\",\"如需进一步帮助，可使用：\",\"conda create --help conda activate --help\"]},\"336\":{\"h\":\"六、查看当前激活的环境\",\"t\":[\"查看当前conda激活的环境:\",\"conda info\"]},\"337\":{\"h\":\"七、查看当前环境已安装的包\",\"t\":[\"查看当前环境已安装的包：\",\"conda list\"]},\"338\":{\"h\":\"八、在当前环境下安装包\",\"t\":[\"根据 requirements.txt 安装所需要的依赖包:\",\"conda activate 你的环境名 # 先激活你的conda环境 pip install -r requirements.txt\",\"重要说明：\",\"在激活的 Conda 环境中使用 pip install，包会安装到该环境的 site-packages 中，不会影响其他环境或系统 Python\",\"如果未激活任何环境时使用 pip install，包可能会安装到基础环境或系统 Python 中\",\"建议总是先激活 Conda 环境再使用 pip，以避免安装到错误的位置\",\"可以使用 which pip 或 where pip (Windows) 确认你使用的是 Conda 环境中的 pip\",\"pip install 安装失败的包，尝试使用conda install命令安装即可，再不行尝试源码编译安装(例如某些包在arm64系统上没有预先编译好的版本)。\",\"特性\",\"pip\",\"conda\",\"默认仓库\",\"PyPI（Python Package Index）\",\"Anaconda 官方仓库 / conda-forge\",\"包类型\",\"仅 Python 包（纯 Python 或源码）\",\"预编译的二进制包（含非 Python 依赖）\",\"非 Python 依赖\",\"不管理（如 FFmpeg、HDF5）\",\"自动安装（如 CUDA、MKL）\"]},\"339\":{\"h\":\"九、常见错误\",\"t\":[\"CondaError: Run 'conda init' before 'conda activate’\",\"conda init 如果是 bash： source ~/.bashrc 如果是 zsh： bash conda activate lavis\"]},\"340\":{\"h\":\"常用评估指标\",\"t\":[\"常用评估指标\"]},\"341\":{\"h\":\"二元分类场景\"},\"342\":{\"h\":\"混淆矩阵 (confusion_matrix)\",\"t\":[\"二元分类器的每个输出有四种可能的结果，如果我们将标准答案作为列，将模型的预测作为行，则会得到以下表格（称为混淆矩阵）：\",\"实际正例\",\"实际负例\",\"预测为正例\",\"真正例 (TP)：垃圾邮件被正确分类为垃圾邮件。\",\"假正例 (FP)：非垃圾邮件被误分类为垃圾邮件。\",\"预测为负例\",\"假负例 (FN)：垃圾邮件被误分类为非垃圾邮件。\",\"真负例 (TN)：非垃圾邮件被正确分类为非垃圾邮件。\",\"请注意，每行的总和表示所有预测正例 (TP + FP) 和所有预测负例 (FN + TN)，无论其有效性如何。与此同时，每个列中的总和会显示所有真实正例 (TP + FN) 和所有真实负例 (FP + TN)，而不会考虑模型分类。\",\"如果实际正例的总数与实际负例的总数不接近，则表示数据集不平衡。不平衡数据集的一个示例可能是一组数以千计的云彩照片，其中您感兴趣的罕见云彩类型（例如卷云）只出现了几次。\"]},\"343\":{\"h\":\"准确率 (accuracy)\",\"t\":[\"准确性是指所有分类（无论是正类还是负类）正确分类的比例。其数学定义为：\",\"在垃圾邮件分类示例中，准确率衡量的是所有电子邮件正确分类所占的比例。\",\"完美的模型没有假正例和假负例，因此准确率为 1.0，即 100%。\",\"由于准确率包含混淆矩阵中的所有四种结果（TP、FP、TN、FN），因此在类别数量相近且平衡的数据集的情况下，准确率可以作为衡量模型质量的粗略指标。\",\"不过，如果数据集不平衡，或者一种错误（假负例或假正例）的代价高于另一种错误（大多数实际应用中都是如此），则最好改为针对其他指标进行优化。\",\"对于严重不均衡的数据集（其中一个类别出现的频率非常低，例如 1%），如果模型 100% 都预测为负类别，则其准确性得分为 99%，尽管该模型毫无用处。\"]},\"344\":{\"h\":\"召回率 (recall) / 真正例率\",\"t\":[\"真正例率 (TPR)，即所有实际正例被正确分类为正例的比例，也称为召回率。\",\"在数学上，召回率的定义为：\",\"假负例是指被误分类为负例的实际正例，因此会出现在分母中。在垃圾邮件分类示例中，召回率衡量的是被正确分类为垃圾邮件的垃圾邮件电子邮件的比例。\",\"假设一个完美的模型不会出现假负例，因此其召回率 (TPR) 为 1.0，也就是说，检测率为 100%。\",\"在实际正例数量非常少的不平衡数据集中，召回率比准确率更有意义，因为它衡量的是模型正确识别所有正例实例的能力。对于疾病预测等应用，正确识别阳性病例至关重要。假负例通常比假正例的后果更严重。\"]},\"345\":{\"h\":\"误报概率 / 假正例率\",\"t\":[\"假正例率 (FPR) 是指被错误地归类为正例的所有实际负例所占的比例，也称为误报概率。其数学定义为：\",\"假正例是被错误分类的实际负例，因此会出现在分母中。在垃圾邮件分类示例中，FPR 用于衡量被错误分类为垃圾邮件的合法电子邮件的比例，或模型的误报率。\",\"完美的模型不会产生假正例，因此其假正例率为 0.0，也就是说，假正例率为 0%。\",\"在实际负例数量非常少（例如总共 1-2 个示例）的不平衡数据集中，FPR 作为一个指标就没有那么有意义和实用。\"]},\"346\":{\"h\":\"精确率\",\"t\":[\"精确率是指模型所有正类别分类中实际为正类别的分类所占的比例。在数学上，其定义为：\",\"在垃圾邮件分类示例中，精确率衡量的是被归类为垃圾邮件且实际上是垃圾邮件的电子邮件所占的比例。\",\"假设有一个完美的模型，则其假正例数为零，因此精确率为 1.0。\",\"在实际正例数量非常少（例如总共 1-2 个示例）的不平衡数据集中，精确率作为指标的意义和实用性较低。\",\"随着假正例的减少，精确率会提高；随着假负例的减少，召回率会提高。提高分类阈值往往会减少假正例的数量并增加假负例的数量，而降低阈值则会产生相反的效果。因此，精确率和召回率通常呈现反向关系，提高其中一个会降低另一个。\",\"分类阈值: 模型输出的概率值大于某个值时，模型才会将该样本分类为正类。\"]},\"347\":{\"h\":\"指标的选择和权衡\",\"t\":[\"在评估模型和选择阈值时，您选择优先考虑的指标取决于特定问题的成本、收益和风险。在垃圾邮件分类示例中，通常最好优先考虑召回率（抓取所有垃圾邮件）或准确率（尝试确保被标记为垃圾邮件的电子邮件实际上是垃圾邮件），或者在达到某个最低准确性水平的情况下，兼顾这两者。\",\"指标\",\"指南\",\"准确率\",\"作为平衡数据集的模型训练进度/收敛情况的粗略指标。对于模型效果，请仅与其他指标搭配使用。避免使用不平衡的数据集。考虑使用其他指标。\",\"召回率（真正例率）\",\"当假负例的代价高于假正例时使用，有病的人不能诊断为健康。\",\"假正例率\",\"当假正例的代价高于假负例时使用，误报很可怕。\",\"精确率\",\"当正例预测的准确性非常重要时，请使用此方法。\"]},\"348\":{\"h\":\"F1 得分\",\"t\":[\"F1 得分是精确率和召回率的调和平均数（一种平均值）。\",\"在数学上，它可按下式计算：\",\"此指标可平衡精确率和召回率的重要性，对于类别不平衡的数据集，优先于准确率。当精确率和召回率均为 1.0 的满分时，F1 得分也会为 1.0 的满分。更广泛地说，当精确率和召回率的值接近时，F1 得分也会接近它们的值。当精确率和召回率相差很大时，F1 将与较差的指标相似。\"]},\"349\":{\"h\":\"ROC 曲线和 AUC\",\"t\":[\"上一部分介绍了一系列模型指标，所有这些指标都是基于单个分类阈值值计算得出的。但是，如果您想评估模型在所有可能阈值下的质量，则需要使用不同的工具。\"]},\"350\":{\"h\":\"ROC (Receiver Operating Characteristic)\",\"t\":[\"ROC 曲线直观地显示了所有阈值下的模型性能。名称的长版本“接收器操作特性”源自二战雷达检测。\",\"绘制 ROC 曲线的方法是：计算每个可能的阈值（在实践中，是按选定的间隔）的真正例率 (TPR) 和假正例率 (FPR)，然后将 TPR 与 FPR 绘制到图表中。\",\"完美的模型在某个阈值下的 TPR 为 1.0，FPR 为 0.0，如果忽略所有其他阈值，则可以用 (0, 1) 点表示，也可以用以下方式表示：\",\"图 1. 假设的理想模型的 ROC 和 AUC\"]},\"351\":{\"h\":\"AUC （曲线下面积）\",\"t\":[\"ROC 曲线下面积 (AUC) 表示，如果给定随机选择的正例和负例，模型将正例排在负例之上的概率。\",\"上面的完美模型包含边长为 1 的正方形，其曲线下面积 (AUC) 为 1.0。这意味着，模型将随机选择的正例正确排在随机选择的负例之上的概率为 100%。\",\"更具体地说，AUC 为 1.0 的垃圾邮件分类器始终会为随机垃圾邮件分配比随机合规电子邮件更高的垃圾邮件概率。每封电子邮件的实际分类取决于您选择的阈值。\",\"对于二元分类器，如果模型的效果与随机猜测或抛硬币的效果完全一样，则其 ROC 曲线为从 (0,0) 到 (1,1) 的对角线。AUC 为 0.5，表示正确对随机正例和负例进行排名的概率为 50%。\",\"在垃圾邮件分类器示例中，AUC 为 0.5 的垃圾邮件分类器仅在 50% 的情况下会将随机垃圾邮件的垃圾邮件概率设为高于随机合法邮件的垃圾邮件概率。\",\"图 2. 完全随机猜测的 ROC 和 AUC\"]},\"352\":{\"h\":\"精确率与召回率曲线\",\"t\":[\"如果数据集在类别之间大致平衡，AUC 和 ROC 非常适合比较模型。当数据集不均衡时，准确率-召回率曲线 (PRC) 和这些曲线下的面积可以更好地直观比较模型性能。精确率/召回率曲线的创建方法是，在 y 轴上绘制精确率，在 x 轴上绘制所有阈值下的召回率。\",\"图 3. 精确率与召回率曲线\"]},\"353\":{\"h\":\"用于选择模型和阈值的 AUC 和 ROC\",\"t\":[\"AUC 是比较两个不同模型性能的有效衡量指标，前提是数据集大致平衡。曲线下面积较大的模型通常是更好的模型。\",\"图 4. 两个假设模型的 ROC 和 AUC。右侧曲线的 AUC 较高，表示该模型优于左侧曲线对应的模型。\",\"ROC 曲线上最接近 (0,1) 的点表示给定模型效果最佳的阈值范围。我们选择的阈值取决于哪个指标对特定用例而言最重要。请考虑下图中的点 A、B 和 C，每个点都代表一个阈值：\",\"图 5. 三个标记的点，表示阈值。\",\"如果假正例（误报）的代价很高，则可能有必要选择 FPR 较低的阈值（例如 A 点），即使 TPR 会降低也是如此。反之，如果假正例成本较低，而假负例（漏掉的真正例）成本较高，则点 C 的阈值（可最大限度地提高 TPR）可能更为合适。如果费用大致相当，点 B 在 TPR 和 FPR 之间可能提供最佳平衡。\"]},\"354\":{\"h\":\"数学知识点\",\"t\":[\"数学知识点\"]},\"355\":{\"h\":\"协方差矩阵\",\"t\":[\"协方差是两个变量“是否一起变化”的度量:\",\"如果两个变量 一起变大或一起变小，协方差是正的；\",\"如果一个变量变大时另一个变小，协方差是负的；\",\"如果两个变量 无关，协方差接近 0。\",\"数学定义（以两个变量为例）:\",\"直观上，它表示：\",\"“X 和 Y 的偏离平均值的乘积”的期望。\",\"扩展到多个变量：协方差矩阵\",\"如果你有 多个变量（如 ），你就可以把它们两两之间的协方差，组成一个 矩阵，这个矩阵就叫：\",\"协方差矩阵的结构，以三个变量为例（比如身高、体重、年龄）：\",\"注意：\",\"对角线上的元素：，也就是每个变量自己的方差。\",\"非对角线上的元素：表示变量之间的相关性（协方差）。\",\"这个矩阵是 对称的（因为 ）。\",\"图像直观理解（二维协方差矩阵），如果我们画出一个二维正态分布：\",\"当两个变量 不相关，协方差 = 0 → 分布是一个 圆形。\",\"当两个变量 正相关，协方差 > 0 → 分布是一个 沿对角线方向拉长的椭圆。\",\"当两个变量 负相关，协方差 < 0 → 椭圆朝反对角线方向倾斜。\",\"在深度学习与生成模型中，协方差矩阵可以用来：\",\"场景\",\"用法\",\"多元高斯分布\",\"表达不同维度之间的“联合关系”\",\"高斯混合模型（GMM）\",\"不同类别的“形状”和“方向”由协方差控制\",\"马氏距离\",\"衡量点与均值之间的距离，但考虑变量相关性\",\"主成分分析（PCA）\",\"通过协方差矩阵找出“主要变化方向”（特征值分解）\",\"高斯过程（GP）\",\"协方差函数定义样本之间的相似性结构\",\"一个例子帮助理解，假设我们有如下样本（身高和体重）：\",\"人\",\"身高（cm）\",\"体重（kg）\",\"A\",\"170\",\"65\",\"B\",\"180\",\"75\",\"C\",\"160\",\"55\",\"先计算每一维的均值，再计算协方差矩阵（不展开计算细节）后，你会得到：\",\"这意味着：\",\"身高和体重的方差都为 100；\",\"协方差为 100，说明它们强烈正相关。\",\"总结:\",\"名词\",\"含义\",\"协方差（cov）\",\"度量两个变量是否同步变化\",\"协方差矩阵（）\",\"所有变量两两之间协方差的矩阵表示\",\"对角线\",\"每个变量自己的方差\",\"非对角线\",\"表示不同变量之间的线性相关性\",\"应用\",\"多元高斯、高斯过程、GMM、PCA、马氏距离等\"]},\"356\":{\"h\":\"马氏距离\"},\"357\":{\"h\":\"欧几里得距离（Euclidean Distance）\",\"t\":[\"公式是：\",\"直观理解：\",\"它就是我们平时量两个点之间“直线距离”的方法。\",\"它对每个维度的偏差一视同仁，不考虑各维度数据的分布特征。\",\"换句话说，哪怕某个维度的数据本来波动很大（方差大），它在这个维度上的偏差也会被直接算入距离，导致整体距离变大。\"]},\"358\":{\"h\":\"马氏距离（Mahalanobis Distance）\",\"t\":[\"公式是：\",\"其中 是数据的协方差矩阵。\",\"直观理解：\",\"它不仅考虑两个点之间的差异，还考虑数据在各个维度上的方差大小和维度间的相关性。\",\" 是协方差矩阵的逆，起到了“标准化”的作用，把数据的不同尺度和相关性都考虑进来。\"]},\"359\":{\"h\":\"尺度差异性\",\"t\":[\"欧几里得距离计算方式：\",\"它直接对每个维度的偏差做平方加总。\",\"如果某个维度的数值范围很大（如年龄：0~100），另一个维度很小（如身高标准化后波动在 ），那么前者的变化会主导整个距离。\",\"这会导致尺度大的特征“支配”了距离判断，从而导致偏差。\",\"马氏距离定义如下：\",\"其中 是样本协方差矩阵， 是它的逆矩阵。\",\"假设我们有一个样本点 ，均值是 ，方差是 ，那么协方差矩阵就是：\",\"此时马氏距离变为：\",\"👉 这就是我们熟悉的 标准差单位距离（z-score 距离）。\",\"结论：马氏距离会自动把不同特征的偏差按标准差进行“标准化”。\",\"在高维空间中：\",\" 是一个 的协方差矩阵；\",\"它包含了每个特征的方差（主对角线），和特征间的协方差（非对角元素）；\",\" 相当于一个“加权标准化器”，对不同方向的偏差做缩放和正交旋转。\",\"设两维特征的协方差矩阵是：\",\"说明：\",\"第一个维度方差是 100，第二个维度方差是 1；\",\"马氏距离中对第一个维度的偏差乘上 ，惩罚少；\",\"第二个维度的偏差乘上 1，惩罚多。\",\"这就实现了 对每个维度根据尺度差异进行惩罚调整 —— 偏差大但常见的就不判定为“远”，偏差小但罕见的要严惩。\",\"你可以把马氏距离想成是：\",\"“在考虑数据分布形状后，重新拉直空间、拉平数据”的距离度量。\",\"原始空间中，数据可能沿某个方向拉长、压扁；\",\"马氏距离通过协方差矩阵逆变换，把这些方向“拉回正态”；\",\"变换后再用欧几里得距离度量 —— 就能反映“真实统计意义上的远近”。\"]},\"360\":{\"h\":\"总结\",\"t\":[\"欧几里得距离适合所有维度的尺度和方差差不多时，或者你不在意尺度差异。\",\"马氏距离适合不同维度尺度差别大、方差差异大且可能相关的情况，更能反映真实的“统计距离”。\"]},\"361\":{\"h\":\"深度学习中常见问题记录\",\"t\":[\"深度学习中常见API记录\"]},\"362\":{\"h\":\"Python\"},\"363\":{\"h\":\"位置参数与关键字参数\",\"t\":[\"位置参数（Positional Argument）：按 位置顺序 传入函数的参数。\",\"关键字参数（Keyword Argument）：用 key=value 的形式明确指定的参数。\",\"场景\",\"位置参数 *\",\"关键字参数 **\",\"调用时\",\"解包 tuple/list\",\"解包 dict\",\"定义时\",\"收集成 tuple\",\"收集成 dict\"]},\"364\":{\"h\":\"闭包与高阶导数\"},\"365\":{\"h\":\"什么是高阶函数？\",\"t\":[\"高阶函数（Higher-Order Function）满足以下两个条件之一即可：\",\"函数接收另一个函数作为参数；\",\"函数返回一个函数。\",\"Python 中的 map、sorted、functools.partial 都是高阶函数。\",\"比如这个函数就是高阶函数：\",\"def outer(func): # 接收函数作为参数 def inner(): print(\\\"调用前\\\") func() print(\\\"调用后\\\") return inner # 返回一个函数\"]},\"366\":{\"h\":\"什么是闭包？\",\"t\":[\"闭包是一个函数，它“记住”了它定义时的 外部作用域变量，即使外部函数已经执行完毕，这些变量依然存在。\",\"例如：\",\"def outer(): x = 10 def inner(): print(x) # inner 记住了 x return inner f = outer() f() # 输出 10\",\"这里 inner 是一个闭包，因为它引用了 outer 中的变量 x，而 outer 已经返回了。\",\"正常情况下：局部变量会在函数执行完后被释放; 但如果我们在内部函数中引用了外部函数的变量，Python 会自动把这些变量“绑定”到这个内部函数上，也就是形成闭包, 变量“被引用”而不会释放。\"]},\"367\":{\"h\":\"装饰器的实现用到了什么？\",\"t\":[\"现在看一个典型的装饰器例子：\",\"def my_decorator(func): # ✅ 高阶函数（接收函数并返回函数） def wrapper(*args, **kwargs): # ✅ wrapper 是闭包（记住了 func） print(\\\"Before call\\\") result = func(*args, **kwargs) print(\\\"After call\\\") return result return wrapper @my_decorator def greet(name): print(f\\\"Hello, {name}\\\")\",\"my_decorator 是 高阶函数，因为它接收 func 并返回 wrapper。\",\"wrapper 是 闭包，因为它访问了其外部作用域的变量 func，并在被调用时依然保留这个引用。\",\"“装饰器 = 高阶函数 + 闭包” 的意思是：\",\"一个装饰器的实现，必须用高阶函数（来接收和返回函数），而在返回的内部函数中，依赖闭包机制来记住原函数的引用，从而实现对原函数行为的增强或修改。\"]},\"368\":{\"h\":\"装饰器\",\"t\":[\"装饰器是 Python 中的一种语法结构，本质是一个 函数（或类），它接收一个函数或类作为参数，对其进行加工，并返回一个新的函数或类对象。\",\"简而言之：\",\"装饰器 = 高阶函数 + 闭包\",\"装饰器主要用于在 不修改原始函数代码的前提下，动态增加其功能，这在日志记录、性能测试、权限校验等场景中非常常见。\"]},\"369\":{\"h\":\"最基本的函数装饰器\",\"t\":[\"def my_decorator(func): def wrapper(): print(\\\"调用前\\\") func() print(\\\"调用后\\\") return wrapper @my_decorator def say_hello(): print(\\\"Hello\\\") say_hello()\",\"输出：\",\"调用前 Hello 调用后\",\"说明：\",\"@my_decorator 相当于：say_hello = my_decorator(say_hello)\",\"wrapper() 是闭包，持有对 func 的引用。\",\"返回的 wrapper 函数替代了原来的 say_hello 函数。\"]},\"370\":{\"h\":\"带参数的函数装饰器\",\"t\":[\"装饰器支持原函数有参数的情况：\",\"def my_decorator(func): def wrapper(*args, **kwargs): print(\\\"开始\\\") result = func(*args, **kwargs) print(\\\"结束\\\") return result return wrapper @my_decorator def add(a, b): return a + b print(add(3, 5))\",\"使用 *args 和 **kwargs 是为了支持任意参数签名。\"]},\"371\":{\"h\":\"带参数的装饰器（装饰器工厂）\",\"t\":[\"如果你希望装饰器 本身接受参数，则需要再多一层函数嵌套：\",\"def log(prefix): def decorator(func): def wrapper(*args, **kwargs): print(f\\\"{prefix} 开始调用 {func.__name__}\\\") result = func(*args, **kwargs) print(f\\\"{prefix} 结束调用 {func.__name__}\\\") return result return wrapper return decorator @log(\\\"DEBUG\\\") def multiply(a, b): return a * b\",\"执行顺序：\",\"@log(\\\"DEBUG\\\") 先返回 decorator\",\"然后 decorator(multiply) 返回 wrapper\"]},\"372\":{\"h\":\"使用 保留原函数元信息\",\"t\":[\"装饰器会改变函数的元信息:\",\"def my_decorator(func): def wrapper(*args, **kwargs): print(\\\"Before call\\\") return func(*args, **kwargs) return wrapper @my_decorator def greet(name): \\\"\\\"\\\"Say hello to someone\\\"\\\"\\\" print(f\\\"Hello, {name}\\\") print(greet.__name__) # ⚠️ 输出 wrapper，不是 greet print(greet.__doc__) # ⚠️ 输出 None，不是函数原文档\",\"@my_decorator 返回的是 wrapper 函数，所以 greet 实际上变成了 wrapper，它的名字和文档字符串也被覆盖了，所以使用装饰器会导致原函数的 __name__、__doc__ 等属性丢失。\",\"Python 提供了 functools.wraps(func) 装饰器，作用是：\",\"把原函数的 __name__、__doc__、__module__ 等元信息“复制”到 wrapper 函数上，让被装饰函数看起来仍然像原来的函数。\",\"import functools def my_decorator(func): @functools.wraps(func) # ✅ 这一步很关键 def wrapper(*args, **kwargs): print(\\\"Before call\\\") return func(*args, **kwargs) return wrapper @my_decorator def greet(name): \\\"\\\"\\\"Say hello to someone\\\"\\\"\\\" print(f\\\"Hello, {name}\\\") print(greet.__name__) # ✅ greet print(greet.__doc__) # ✅ Say hello to someone\",\"这在调试、文档生成、类型检查、元编程、反射中都非常重要。例如：\",\"help(greet)：没有 wraps 就看不到真实文档了\",\"使用 inspect 模块查看参数、注解、类型签名会失效\",\"多个装饰器嵌套时更容易出错\"]},\"373\":{\"h\":\"装饰类方法（普通方法 / 类方法 / 静态方法）\",\"t\":[\"def log_method(func): @wraps(func) def wrapper(*args, **kwargs): print(f\\\"调用方法 {func.__name__}\\\") return func(*args, **kwargs) return wrapper class MyClass: @log_method def hello(self): print(\\\"Hello from method\\\")\"]},\"374\":{\"h\":\"装饰整个类\",\"t\":[\"def decorate_class(cls): cls.version = \\\"1.0\\\" return cls @decorate_class class MyService: pass print(MyService.version) # 1.0\"]},\"375\":{\"h\":\"装饰器的底层原理与执行过程\",\"t\":[\"本质：装饰器 = 函数替换器\",\"一个装饰器：\",\"@decorator def func(): pass\",\"等价于：\",\"func = decorator(func)\",\"即：把 func 传给 decorator 函数，并用它的返回值替换 func 本身。\"]},\"376\":{\"h\":\"多个装饰器叠加时的执行顺序（从内到外）\",\"t\":[\"@d1 @d2 def func(): pass\",\"等价于：\",\"func = d1(d2(func))\",\"即，先应用最内层的 d2，再由外层 d1 包裹起来。\"]},\"377\":{\"h\":\"类装饰器\",\"t\":[\"类装饰器通常通过实现 __call__ 方法来模拟函数行为：\",\"class MyDecorator: def __init__(self, func): self.func = func def __call__(self, *args, **kwargs): print(\\\"调用前\\\") result = self.func(*args, **kwargs) print(\\\"调用后\\\") return result @MyDecorator def greet(name): print(f\\\"Hi, {name}\\\") greet(\\\"Alice\\\")\"]},\"378\":{\"h\":\"总结\",\"t\":[\"类型\",\"例子\",\"含义\",\"最基本装饰器\",\"@func\",\"f = func(f)\",\"装饰器工厂\",\"@decorator(x)\",\"f = decorator(x)(f)\",\"对象方法装饰器\",\"@obj.method\",\"f = obj.method(f)\",\"对象方法工厂\",\"@obj.method(args)\",\"f = obj.method(args)(f)\"]},\"379\":{\"h\":\"典型应用场景举例\",\"t\":[\"日志记录：\",\"def log(func): @wraps(func) def wrapper(*args, **kwargs): print(f\\\"调用 {func.__name__} 参数: {args}, {kwargs}\\\") return func(*args, **kwargs) return wrapper\",\"权限控制：\",\"def require_admin(func): @wraps(func) def wrapper(*args, **kwargs): if not user_is_admin(): raise PermissionError(\\\"需要管理员权限\\\") return func(*args, **kwargs) return wrapper\",\"性能测试（统计函数运行时间）：\",\"import time def timing(func): @wraps(func) def wrapper(*args, **kwargs): start = time.time() result = func(*args, **kwargs) print(f\\\"{func.__name__} 耗时: {time.time() - start:.4f}s\\\") return result return wrapper\"]},\"380\":{\"h\":\"Pytorch\"},\"381\":{\"h\":\"stack\",\"t\":[\"torch.stack() 是 PyTorch 中用于将多个形状相同的张量沿一个新维度拼接的函数。\",\"torch.stack(tensors, dim=0, *, out=None)\",\"tensors：一个可迭代对象（如列表、元组），其中包含多个形状相同的 Tensor。\",\"dim：插入新维度的位置（默认是 0）。这个新维度就是拼接的那一维。\",\"out：可选输出张量，用于写入结果。\",\"例子如下:\",\"注意:\",\"所有张量必须具有完全相同的 shape。\",\"如果你想把一个 batch 中的多个样本打包成一个大 tensor，通常会用 torch.stack()。\"]},\"382\":{\"h\":\"transpose\",\"t\":[\"y = x.transpose(dim0, dim1)\",\"只交换两个指定维度，常用于 2D 或 3D 张量，如图像转置、RNN 输入调整等。\"]},\"383\":{\"h\":\"permute\",\"t\":[\"y = x.permute(dims)\",\"可以任意重新排列所有维度，是 transpose 的泛化，支持多维度同时交换。\",\"transpose() 和 permute() 返回的张量虽然是视图（view），但它们的 内存布局（strides）被改变。如果你接下来要对它们执行 .view() 或某些要求内存连续的操作，就必须先调用 .contiguous()。\",\"执行 transpose(0, 2) 后:\"]},\"384\":{\"h\":\"view\",\"t\":[\"view: 在不复制数据的前提下，返回具有新形状（shape）的张量视图（view）。\",\"new_tensor = x.view(shape)\",\".view() 只适用于连续内存的张量，某些操作（如 permute, transpose）会改变张量的 stride（内存步长），使其变得 非连续。此时必须先 .contiguous() 再 .view()：\",\"x = torch.randn(2, 3, 4) y = x.permute(0, 2, 1) # 改变维度顺序 z = y.contiguous().view(2, -1) # 否则可能报错\",\".view() 不会复制数据，是原张量的一个视图（共享内存）\"]},\"385\":{\"h\":\"reshape\",\"t\":[\"reshape: 返回具有新形状的张量。必要时会复制数据，否则返回视图。 相比 .view()，reshape() 不要求原始张量是连续的，这是它最大的优势。\",\"new_tensor = x.reshape(shape)\",\"在 PyTorch 中，reshape() 在多数情况下会返回原张量的视图（不复制数据），但当张量的内存布局不连续（例如经过了 permute()、transpose() 等操作），或新形状无法与原内存布局兼容时，reshape() 就会进行数据复制以创建新的张量。此外，如果张量来源于 expand()（广播视图），或者跨设备/特殊操作后的中间结果，也可能触发复制。因此，若希望确保内存效率，建议在 reshape 前使用 .is_contiguous() 检查，必要时用 .contiguous() 转为连续张量。\"]},\"386\":{\"h\":\"repeat\",\"t\":[\"tensor.repeat() 是 PyTorch 中用于沿指定维度重复张量内容的操作，它会复制数据，从而扩展张量的形状（不是视图）。\",\"repeated_tensor = x.repeat(repeat_1, repeat_2, ..., repeat_n)\",\"参数个数必须和 x 的维度数相同。\",\"每个 repeat_i 表示该维度上复制的次数。\",\"import torch x = torch.tensor([[1, 2], [3, 4]]) x = x.repeat(2,3) print(x) output: tensor([[1, 2, 1, 2, 1, 2], [3, 4, 3, 4, 3, 4], [1, 2, 1, 2, 1, 2], [3, 4, 3, 4, 3, 4]])\"]},\"387\":{\"h\":\"expand\",\"t\":[\"tensor.expand() 是 PyTorch 中用于扩展张量尺寸但不复制数据的一种高效方法，它通过广播（broadcasting）机制生成新的视图，节省内存。\",\"expanded_tensor = x.expand(size_1, size_2, ..., size_n)\",\"参数个数必须和 x.dim() 相同，或可以通过在前面添加维度来自动广播。\",\"某一维如果是 -1，表示保持原来的大小。\",\"x = torch.tensor([[1], [2], [3]]) # shape: [3, 1] x.expand(3, 4) # → 每行复制 4 次，但不占用额外内存 # tensor([[1, 1, 1, 1], # [2, 2, 2, 2], # [3, 3, 3, 3]])\",\"使用 -1 保留维度：\",\"x = torch.randn(3, 1, 5) # shape: [3, 1, 5] x.expand(-1, 4, -1) # shape → [3, 4, 5]\",\"核心原则：只有原始维度 = 1 的位置，才能通过 expand 变大；其他位置必须 相等。\",\"x = torch.tensor([[1, 2, 3]]) # shape: [1, 3] y = x.expand(2, 3) # ✅ 第 0 维是 1 → 可以扩展成 2 # ❌ 第 1 维是 3 → 目标仍是 3，虽然没变，但也不能写成 6！ x.expand(2, 6) # ❌ 报错！因为第 1 维是 3，不能变成 6\",\"特性\",\".expand()\",\".repeat()\",\"是否复制数据\",\"❌ 否（返回视图，节省内存）\",\"✅ 是（创建新张量，开销大）\",\"是否支持广播\",\"✅ 支持（自动按维度扩展）\",\"❌ 不支持，必须精确指定每维重复次数\",\"是否可用于改变维度\",\"❌ 否（维度必须兼容）\",\"✅ 是\",\"常用于\",\"高效广播，如 attention、masking 等\",\"实际复制，如构造重复输入\"]},\"388\":{\"h\":\"@torch.no_grad()\",\"t\":[\"在这个装饰器修饰的函数内，PyTorch 不会跟踪计算图，也不会计算梯度。\",\"这样可以减少内存使用和计算开销，因为不需要保存中间变量用于反向传播。\",\"适用于只需要前向推理且不需要更新模型参数的场景。\"]},\"389\":{\"h\":\"register_buffer\",\"t\":[\"# nn.Module 类中提供的方法 register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True)\",\"name (str)\",\"缓冲区的名称（字符串）。\",\"之后可以用 model.name 访问，比如 model.queue。\",\"tensor (torch.Tensor 或 None)\",\"要注册的张量。\",\"这个张量会成为模型的一个成员，但不会被视为可训练参数。\",\"也可以传 None，表示先占位，后面再赋值。\",\"persistent (bool，默认 True，PyTorch 1.8以后支持)\",\"如果为 True，该缓冲区会包含在 state_dict() 中，即会被保存和加载。\",\"如果为 False，缓冲区不会保存到 state_dict()，常用于临时缓存数据。\",\"register_buffer的作用和意义：\",\"它会把一个张量（tensor）作为模型的缓冲区注册，不会被当作模型的可训练参数（不会出现在model.parameters()里，也不会参与梯度计算或优化）。\",\"但是，缓冲区会被自动保存到模型的状态字典（state_dict）中，也会被加载（load）和保存（save）。\",\"常用于保存一些模型的状态信息，但这些信息不需要训练，比如：均值、方差、队列、掩码等。\"]},\"390\":{\"h\":\"einsum\",\"t\":[\"einsum 是 爱因斯坦求和约定（Einstein Summation） 的简写，是一个非常强大且直观的张量操作工具。\",\"相比 matmul、bmm、torch.matmul 这类 API，einsum 让你显式指定维度之间怎么相乘/求和/保留。\",\"torch.einsum(\\\"维度规则\\\", [tensor1, tensor2, ...])\",\"引号中是 对每个 tensor 的维度命名\",\"相同的维度字母表示要做 点积/求和\",\"没有重复的维度字母表示保留该维度\",\"einsum 表达式\",\"等价操作\",\"输出形状\",\"含义\",\"\\\"nc,nc->n\\\"\",\"(q * k).sum(dim=1)\",\"(N,)\",\"每个 query 与其正样本的点积\",\"\\\"nc,ck->nk\\\"\",\"torch.matmul(q, queue)\",\"(N, K)\",\"每个 query 与所有负样本的相似度\"]},\"391\":{\"h\":\"模型\"},\"392\":{\"h\":\"ResNet18\",\"t\":[\"ResNet18是一种深度残差网络，它由18层组成。它的结构包括一个输入层、四个残差块和一个输出层。每个残差块包含两个3x3的卷积层，每个卷积层后面都跟着一个Batch Normalization和ReLU激活函数。此外，每个残差块还包含一条跨层的连接线，将输入直接连接到输出。这种设计使得网络能够更好地处理深层特征，并且可以避免梯度消失问题。ResNet18在图像分类任务中表现出色，可以用于训练大型数据集，如ImageNet。\"]},\"393\":{\"h\":\"Bert\",\"t\":[\"pooler_output 的输出用于捕获整个句子的全局语义信息:\"]},\"394\":{\"h\":\"公式&定理\"},\"395\":{\"h\":\"通用近似定理\",\"t\":[\"以下内容来自: << 神经网络与深度学习 >> 4.3.1 通用近似定理\",\"根据通用近似定理，对于具有线性输出层和至少一个使用“挤压”性质的激活函数的隐藏层组成的前馈神经网络，只要其隐藏层神经元的数量足够，它可以以任意的精度来近似任何一个定义在实数空间中的有界闭集函数．所谓“挤压”性质的函数是指像Sigmoid函数的有界函数，但神经网络的通用近似性质也被证明对于其他类型的激活函数，比如ReLU，也都是适用的．\",\"个人对上述内容的理解\",\"通用近似定理中“隐藏层神经元的数量足够”这一条件，与多项式逼近（如泰勒展开）中 '增加阶数提高精度' 的思想有深刻的相似性，但神经网络的非线性基函数组合比传统多项式逼近更灵活。以下是具体分析：\",\"逼近方式\",\"多项式逼近（泰勒展开）\",\"神经网络逼近\",\"基函数\",\"单项式基 \",\"非线性激活后的基 \",\"组合方式\",\"线性加权和 \",\"线性加权和 \",\"逼近原理\",\"增加阶数 提高精度\",\"增加神经元数量 提高精度\",\"函数空间\",\"多项式函数空间\",\"自适应生成的非线性函数空间\",\"关键共同点：\",\"两者都通过增加基函数的数量（多项式阶数/神经元数量）来扩大逼近空间的容量，从而提升对目标函数的拟合精度。\",\"神经网络的独特优势:\",\"自适应基函数\",\"多项式逼近的基函数是固定的（如 ），而神经网络的基函数 的形状和位置（由权重 决定）可通过训练动态调整，更灵活适应目标函数。\",\"示例：拟合分段函数时，ReLU神经元可自动学习“转折点”，而多项式需极高阶数才能近似突变。\",\"维度诅咒的缓解\",\"在高维空间（）中，多项式逼近需要 项（指数增长），而神经网络通过非线性激活和分层结构，可能以 神经元实现相同精度。\",\"对非平滑函数的适应性\",\"泰勒展开要求函数无限可微，而神经网络（如使用ReLU）可逼近连续但不可微的函数（如 ）。\",\"案例：逼近区间 上的 \",\"多项式逼近: 需高阶泰勒展开 ，且高次项易导致震荡（龙格现象）。\",\"神经网络逼近: 仅需4个Tanh神经元即可高精度拟合，因基函数 能自适应频率和相位。\",\"理论限制的相似性:\",\"逼近精度与代价的权衡\",\"多项式：高阶项导致数值不稳定（如大数相减损失精度）。\",\"神经网络：神经元过多易过拟合，且训练难度增加（梯度消失/爆炸）。\",\"全局逼近 vs 局部逼近\",\"多项式：调整某一系数会影响全局拟合。\",\"神经网络：可通过局部神经元（如ReLU）实现分段逼近，更适应局部特征。\",\"现代深度学习的延伸: 深层神经网络通过函数复合（Function Composition）能够以指数级减少所需的神经元数量，核心原因在于层次化的函数构造方式比单层网络的线性组合更高效。这与多项式逼近等传统方法有本质区别，具体可以从以下几个方面理解：\",\"1. 函数复合 vs. 线性组合：数学本质对比\",\"单层网络（线性组合）：\",\"单隐藏层神经网络的输出形式为：\",\"它通过一组非线性基函数（(\\\\sigma)）的加权和逼近目标函数，类似于多项式逼近中的基函数组合。要逼近复杂函数，可能需要大量神经元（(N) 极大）。\",\"深层网络（函数复合）：\",\" 层网络的输出是多次复合的结果：\",\"每一层 都是一个非线性变换（如 ）。通过逐层抽象，深层网络可以逐步构造出更复杂的函数。\",\"关键区别：\",\"单层网络依赖基函数的数量（宽度）来增加表达能力。\",\"深层网络依赖函数的嵌套深度，通过分层组合简单函数，实现复杂功能。\",\"2. 为什么函数复合更高效？\",\"(1) 分治策略（Divide-and-Conquer）\",\"深层网络将复杂函数分解为多个简单步骤，每一层只需学习局部特征，最后组合成全局解。例如：\",\"目标函数：拟合一个“锯齿波”\",\"单层网络：需要大量神经元构造多个“转折点”。\",\"深层网络：每层学习一个转折点，通过复合实现指数级增长的分段线性区域（如 层ReLU网络可生成 个分段）。\",\"(2) 指数级表达能力\",\"理论结果：\",\"Telgarsky (2016) 证明：用深度 的ReLU网络可以构造具有 个线性区域的函数，而单层网络需要 个神经元才能达到相同效果。\",\"直观理解：每一层的非线性变换（如ReLU）相当于对输入空间进行一次“折叠”，深度叠加导致表达能力爆炸式增长。\",\"网络类型\",\"所需神经元/层数\",\"表达能力增长方式\",\"单层宽网络\",\" 神经元\",\"线性增长（基函数叠加）\",\"深层网络\",\" 层，每层 神经元\",\"指数增长（函数复合）\",\"(3) 参数复用与模块化\",\"深层网络通过共享参数（如卷积核）和模块化设计（如残差块），进一步减少冗余：\",\"示例：CNN中，同一卷积核在不同位置重复使用，避免为每个像素单独建模。\",\"3. 与多项式逼近的对比\",\"多项式逼近通过增加阶数（如泰勒展开）提升精度，但存在两大局限：\",\"全局性：调整某一系数会影响整个函数，难以局部修正。\",\"维度灾难：高维输入时，多项式项数 爆炸式增长。\",\"而神经网络的函数复合：\",\"局部性：每层聚焦不同抽象层次（如边缘→纹理→物体）。\",\"维度友好：通过分层降维（如池化）逐步压缩信息。\",\"4. 实例说明\",\"案例1：逼近“多次折叠”的函数\",\"目标函数：\",\"单层网络：需数百个神经元拟合嵌套正弦波。\",\"深层网络：3层即可，每层对应一个 操作。\",\"案例2：图像分类\",\"单层网络：需直接建模像素到类别的复杂映射，参数量极大。\",\"深层CNN：逐层提取边缘→纹理→部件→物体，参数量更少。\",\"5. 理论支持\",\"深度分离定理（Depth Separation Theorem）: 存在某些函数，用浅层网络逼近需要指数级神经元，而深层网络只需多项式数量（如 Eldan & Shamir, 2016）。\",\"电路理论类比: 深层网络类似布尔电路中的分层设计（如AND-OR门组合），比单层电路更高效。\",\"6. 深层网络的代价\",\"虽然深度减少了神经元数量，但带来了：\",\"优化难度：梯度消失/爆炸问题。\",\"过拟合风险：需正则化（如Dropout）。\",\"计算开销：并行化要求更高。\",\"总结:\",\"神经网络通过非线性激活函数生成的动态基函数组合，实现了比多项式逼近更高效的函数近似。虽然“增加神经元数量”与“提高多项式阶数”在思想上都体现了用更多自由度提升精度，但神经网络的自适应基函数和分层结构使其：\",\"对高维和非平滑函数更鲁棒\",\"避免了手工设计基函数的局限性\",\"在实践中通过梯度下降自动学习逼近策略\"]},\"396\":{\"h\":\"ROI Pooling\",\"t\":[\"在目标检测任务中，比如 Faster R-CNN，我们会从一张图片中生成多个候选区域（ROI），这些区域的大小各不相同。而神经网络的全连接层只能接受固定大小的输入，这就产生了一个问题：\",\"如何将不同尺寸的ROI特征，统一变为相同尺寸？\",\"ROI Pooling 的目标就是: 从不同大小的 ROI 区域中提取固定大小的特征（例如 7×7），同时保留最有代表性的空间信息。\",\"ROI Pooling 的操作流程可以分为三个步骤:\",\"映射 ROI 到特征图空间\",\"假设输入图像经过卷积得到一个特征图（例如从 ResNet 输出的特征图），而我们检测到一个 ROI（例如在原图上坐标为 ）。\",\"由于特征图的尺寸比原图小（通常是原图的 1/16），我们需要先将 ROI 坐标 映射到特征图上：\",\"其中 stride 是特征图相对于原图的缩放比例。\",\"将该 ROI 划分成固定数量的网格区域\",\"例如我们希望将每个 ROI 转换成 7×7 的特征图，那么就把该 ROI 分成 7 行 × 7 列的 小块（每一小块大小不同，但数目固定）。\",\"每个小块做 max pooling\",\"对每个小块区域做 最大池化（Max Pooling），取出该区域内的最大值，这样就将原本不定尺寸的 ROI 转换成一个固定大小的特征图（例如 7×7）。\",\"假设某个 ROI 映射到特征图上之后是一个大小为 14×14 的区域，我们希望输出一个 7×7 的固定大小特征图。\",\"将 14×14 区域划分为 7×7 的网格（每个网格是 2×2 大小）\",\"对每个 2×2 的小格子做最大池化 → 输出一个 7×7 特征图\",\"⚠️ ROI Pooling 有一个问题：量化误差。\",\"ROI Pooling 的划分方式中涉及到了取整（floor/ceil），这在某些场景下会导致位置偏差、信息丢失。\",\"为了更精确，Mask R-CNN 提出了更先进的方法：ROI Align，它使用双线性插值来避免量化误差，使得检测/分割性能更好。\"]},\"397\":{\"h\":\"ROI Align\",\"t\":[\"ROIAlign 是 Mask R-CNN 中为了解决 RoIPooling 引起的对齐误差问题而提出的关键组件。\",\"RoIPool（Region of Interest Pooling） 是 Faster R-CNN 中的标准组件，用于将任意大小的候选框（RoI）转换为固定大小（例如 7×7）的特征图，以便送入全连接层进行分类和回归。\",\"问题： RoIPool 在处理浮点型的 RoI 坐标时进行了两次量化（quantization）操作：\",\"RoI 边界坐标的量化（例如将 x/16 向下取整）；\",\"池化 bin 分割时的量化（每个 bin 的边界坐标再取整）。\",\"这会导致特征图上的空间对齐误差（misalignment），尤其对 像素级别任务如分割 影响显著。\",\"RoIAlign目标：消除量化误差，实现精确的像素级对齐。\",\"实现步骤如下：\",\"不进行任何量化\",\"保留浮点型的 RoI 坐标值（例如 x/16 而不是 [x/16]），也不对 bin 边界进行离散化。\",\"对每个 bin 采样多个点（如 2×2）\",\"将 RoI 分成固定数量的 bin（例如 7×7）。\",\"每个 bin 中选定若干个浮点坐标点（通常为4个采样点，中心或等距分布）。\",\"+----------+ | * * | | | | | | * * | +----------+\",\"每个 * 就是一个采样点，它们分布在 4 个角的中间位置，平均对称。\",\"使用双线性插值（Bilinear Interpolation）提取特征值\",\"由于坐标是浮点数，不对应实际的 feature map 网格点，因此使用四邻域双线性插值从特征图中获取精确的 feature 值。\",\"你要在 (3.6, 5.2) 点上取值： - 它离 (3,5) 的距离是 (1 - 0.6) * (1 - 0.2) = 0.4 * 0.8 = 0.32 - 它离 (4,5) 的距离是 0.6 * 0.8 = 0.48 - 它离 (3,6) 的距离是 0.4 * 0.2 = 0.08 - 它离 (4,6) 的距离是 0.6 * 0.2 = 0.12 于是你把这 4 个点的值按这个比例加起来，就得到了 (3.6, 5.2) 的值。 就像你在地图上两个村庄中间估算温度时，不会只看一个村，而是综合周围村子的情况加权得出。\",\"每个撒下去的小数点都用周围的4个整数点去“平均估计”（双线性插值）;\",\"对采样点的值进行聚合\",\"可以采用 max 或 average（论文推荐 average）。\",\"每个 bin 的最终输出为这些采样点值的聚合结果。\",\"图示（见论文 Figure 3）：\",\"实线为 RoI，虚线为 feature map 网格，黑点为采样点，通过插值获得值后聚合。\",\"RoIAlign 就是：\",\"先把目标区域平均切成小格子（比如 7×7）；\",\"在每个小格子里撒几个点（比如 2×2）；\",\"每个撒下去的小数点都用周围的4个整数点去“平均估计”（双线性插值）；\",\"最后把所有点的值求平均，就得到了这个格子的特征。\"]},\"398\":{\"h\":\"语义分割中常用的损失函数\",\"t\":[\"语义分割中常用的损失函数\"]},\"399\":{\"h\":\"语义分割\",\"t\":[\"语义分割是计算机视觉领域中的一项任务，旨在将图像中的每个像素分类为不同的语义类别。与对象检测任务不同，语义分割不仅需要识别图像中的物体，还需要对每个像素进行分类，从而实现对图像的细粒度理解和分析。\",\"语义分割可以被看作是像素级别的图像分割，其目标是为图像中的每个像素分配一个特定的语义类别标签。每个像素都被视为图像的基本单位，因此语义分割可以提供更详细和准确的图像分析结果。\",\"语义分割 vs 分类 :\",\"在语义分割任务中，由于需要对每个像素进行分类，因此需要使用像素级别的损失函数。\",\"语义分割任务中，图像中各个类别的像素数量通常不均衡，例如背景像素可能占据了大部分。\",\"语义分割任务需要对图像中的每个像素进行分类，同时保持空间连续性。\"]},\"400\":{\"h\":\"损失函数\"},\"401\":{\"h\":\"Dice Loss\",\"t\":[\"Dice Loss 是一种常用于语义分割任务的损失函数，尤其在目标区域较小、类别不平衡（class imbalance）的情况下表现优异。它来源于 Dice 系数（Dice Coefficient） ，又称为 Sørensen-Dice 系数 ，是衡量两个样本集合之间重叠程度的一种指标。\",\"Dice 系数衡量的是预测掩码与真实标签之间的相似性，公式如下：\",\"其中：\",\" ：模型预测出的功能区域（如经过 sigmoid 后的概率值）；\",\" ：Ground Truth 掩码（二值化或软标签）；\",\" ：预测为正类且实际也为正类的部分（交集）；\",\" ：预测和真实中所有正类区域之和；\",\"⚠️ 注意：Dice 系数范围是 [0, 1]，越大越好。\",\"Dice Loss 为了将其作为损失函数使用，我们通常取其补集：\",\"有时也会加入一个平滑项 ϵ 防止除以零：\",\"Dice Loss 的优势:\",\"优势\",\"描述\",\"对类别不平衡不敏感,更关注“有没有覆盖正确区域”，而不是“有多少点被正确分类”\",\"不像 BCE Loss 那样对负样本过多敏感\",\"直接优化 IoU 的替代指标\",\"Dice 和 IoU 表现类似，但更易梯度下降\",\"支持 soft mask 输入\",\"可处理连续概率值，不需要先 threshold\",\"更关注整体区域匹配\",\"而不是逐点分类\",\"代码实现:\",\"class DiceLoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数，支持加权和平均损失。 参数: weight (Tensor): 各类别的权重（可选） size_average (bool): 是否对 batch 中的样本取平均 loss \\\"\\\"\\\" super(DiceLoss, self).__init__() # 该参数未在当前代码中使用，但保留接口以备后续扩展 self.weight = weight # 控制是否对 batch 内 loss 取均值或求和 self.size_average = size_average def forward(self, inputs, targets, smooth=1): \\\"\\\"\\\" 前向传播函数，计算 Dice Loss。 参数: inputs (Tensor): 模型输出的预测值（logits 或 raw output），形状为 [B, N] targets (Tensor): 真实标签（ground truth mask），形状为 [B, N] smooth (float): 平滑项，防止除零错误，默认为 1 返回: dice_loss (Tensor): 计算得到的 Dice Loss \\\"\\\"\\\" # 如果你的模型最后没有 sigmoid 层，则需要在这里激活，否则应注释掉这行 inputs = F.sigmoid(inputs) # 将 logits 映射到 [0,1] 区间 # 将输入展平成一维张量，便于后续计算 # inputs: [B*N] # targets: [B*N] inputs = inputs.view(-1) targets = targets.view(-1) # 计算交集：预测与 GT 的重合部分 intersection = (inputs * targets).sum() # 计算 Dice Coefficient，加入 smooth 防止除以零 dice_score = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth) # 返回 Dice Loss，用 1 - Dice Coefficient # 值越小表示匹配越好 return 1 - dice_score\"]},\"402\":{\"h\":\"BCE-Dice Loss\",\"t\":[\"BCE-Dice Loss是将Dice Loss和标准的二元交叉熵（Binary Cross-Entropy, BCE）损失结合在一起的一种损失函数，通常用于分割模型中。它结合了两种 loss 的优点：\",\"BCE Loss ：关注每个点的分类误差；\",\"Dice Loss ：关注整体区域匹配度；\",\"Binary Cross Entropy Loss（BCE Loss）\",\"公式（逐点）：\",\"其中：\",\"：真实标签（binary 或 soft mask）；\",\"：模型输出的概率值；\",\"特点：\",\"对每个点单独计算分类误差；\",\"强调预测与 GT 的一致性；\",\"在类别平衡时效果好，但在前景远少于背景时容易偏向负样本；\",\"Dice Loss\",\"公式（简化版）：\",\"其中：\",\"：预测概率；\",\"：真实标签；\",\"：平滑项，防止除以零；\",\"特点：\",\"不依赖绝对数量，而是关注预测和 GT 的交并比；\",\"更适合前景极少的小区域识别；\",\"能缓解类别不平衡问题；\",\"为什么要把它们结合起来？\",\"模型\",\"缺陷\",\"补充方式\",\"BCE Loss\",\"对前景响应弱，易受类别不平衡影响\",\"加入 Dice Loss 增强区域匹配\",\"Dice Loss\",\"对单个点的分类精度不够敏感\",\"加入 BCE Loss 提高逐点判别能力\",\"组合后的优势：\",\"优势\",\"描述\",\"✔️ 抗类别不平衡能力强\",\"Dice Loss 起主导作用\",\"✔️ 对细节更敏感\",\"BCE Loss 提升边缘识别精度\",\"✔️ 支持 soft mask 输入\",\"可处理连续值掩码\",\"✔️ 更稳定地收敛\",\"两者互补，避免训练震荡\",\"代码实现:\",\"class DiceBCELoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数，构建一个组合损失函数 Dice + BCE。 参数: weight (Tensor): 可选参数，用于类别加权； size_average (bool): 是否对 batch 内样本取平均 loss（已弃用）； \\\"\\\"\\\" super(DiceBCELoss, self).__init__() # 这里暂时未使用 weight 和 size_average，保留接口以备扩展 def forward(self, inputs, targets, smooth=1): \\\"\\\"\\\" 前向传播函数，计算预测输出与真实标签之间的 Dice Loss 与 BCE Loss 的加权和。 参数: inputs (Tensor): 模型输出的 logits 或 raw 分数，形状为 [B, N] targets (Tensor): 真实掩码（ground truth mask），形状为 [B, N] smooth (float): 平滑项，防止除零错误，默认为 1 返回: Dice_BCE (Tensor): Dice + BCE 组合损失值 \\\"\\\"\\\" # 如果模型最后没有 sigmoid 层，这里需要激活 # 如果已经包含 sigmoid，则应注释掉这一行 inputs = F.sigmoid(inputs) # 将输入映射到概率空间 [0, 1] # 将输入和目标展平成一维张量，便于后续计算 # inputs: [B*N] # targets: [B*N] inputs = inputs.view(-1) targets = targets.view(-1) # 计算交集：预测值和真实值都为 1 的区域 intersection = (inputs * targets).sum() # 计算 Dice Loss： # Dice Coefficient = (2 * intersection) / (inputs_sum + targets_sum) # Dice Loss = 1 - Dice Coefficient inputs_sum = inputs.sum() targets_sum = targets.sum() dice_score = (2. * intersection + smooth) / (inputs_sum + targets_sum + smooth) dice_loss = 1 - dice_score # 计算 Binary Cross Entropy Loss（BCE） # 注意：F.binary_cross_entropy 默认要求 inputs 已经经过 sigmoid BCE = F.binary_cross_entropy(inputs, targets, reduction='mean') # 组合损失：BCE + Dice Loss Dice_BCE = BCE + dice_loss return Dice_BCE\"]},\"403\":{\"h\":\"Jaccard/Intersection over Union (IoU) Loss\",\"t\":[\"Jaccard Loss，也称为Intersection over Union (IoU) Loss，是一种常用的损失函数，用于语义分割任务中评估模型的分割结果与真实分割标签之间的相似性。它基于Jaccard指数（Jaccard Index），也称为 交并比（Intersection over Union, IoU）指标，用于度量两个集合之间的重叠程度。\",\"Jaccard Index（IoU）\",\"其中：\",\"：模型输出的概率值或二值化结果；\",\"：ground truth 掩码；\",\"分子是预测和 GT 的交集；\",\"分母是两者的并集；\",\"⚠️ IoU 值 ∈ [0, 1]，越大越好。\",\"Jaccard Loss（IoU Loss）\",\"为了将 IoU 转换为可优化的损失函数，我们取其补集：\",\"这样，损失越小表示预测越接近真实标签。\",\"为了避免除以零，通常加入平滑项 ：\",\"Jaccard Loss 有以下几个优点：\",\"特性\",\"描述\",\"✔️ 对类别不平衡不敏感\",\"不像 BCE Loss 那样偏向背景点\",\"✔️ 关注整体区域匹配\",\"强调预测与 GT 的空间一致性\",\"✔️ 更适合评估边界模糊区域\",\"如功能区域边缘不确定性较高\",\"与其他 Loss 的对比\",\"损失函数\",\"是否支持 soft mask\",\"是否对类别不平衡敏感\",\"是否直接优化 IoU\",\"输出范围\",\"BCE Loss\",\"❌ 否（需二值化）\",\"✅ 是\",\"❌ 否\",\"[0, ∞)\",\"Focal Loss\",\"✅ 是（加权）\",\"✅ 是（缓解）\",\"❌ 否\",\"[0, ∞)\",\"Dice Loss\",\"✅ 是\",\"✅ 是\",\"近似于 IoU\",\"[0, 1]\",\"Jaccard (IoU) Loss\",\"✅ 是\",\"✅ 是\",\"✅ 是\",\"[0, 1]\",\"虽然 Dice Loss 在实际训练中更稳定，但 Jaccard Loss 更贴近最终评估指标（IoU），适合在推理阶段作为验证标准。\",\"代码实现:\",\"class IoULoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数，构建一个基于 IoU（交并比）的损失函数。 参数: weight (Tensor): 可选参数，用于类别加权（未使用） size_average (bool): 是否对 batch 内样本取平均 loss（已弃用） \\\"\\\"\\\" super(IoULoss, self).__init__() # weight 和 size_average 在此实现中未使用，保留接口以备后续扩展 def forward(self, inputs, targets, smooth=1): \\\"\\\"\\\" 前向传播函数，计算预测输出与真实标签之间的 IoU Loss。 参数: inputs (Tensor): 模型输出的原始 logit 或经过 sigmoid 的概率值； 形状为 [B, N] targets (Tensor): ground truth 掩码，形状为 [B, N] smooth (float): 平滑项，防止除零错误，默认为 1 返回: iou_loss (Tensor): 计算得到的 IoU Loss \\\"\\\"\\\" # 如果模型最后没有 sigmoid 层，则在这里激活 # 如果已经包含 sigmoid，则应注释掉这一行 inputs = torch.sigmoid(inputs) # 将输入映射到 [0,1] 区间 # 将输入和目标展平成一维张量便于计算 # inputs: [B*N] # targets: [B*N] inputs = inputs.view(-1) targets = targets.view(-1) # 计算交集（Intersection），等价于 TP（True Positive） intersection = (inputs * targets).sum() # 计算并集：Union = input + target - intersection total = (inputs + targets).sum() union = total - intersection # 计算 IoU Score，加入平滑项防止除以零 iou_score = (intersection + smooth) / (union + smooth) # IoU Loss = 1 - IoU score，这样越接近 1，loss 越小 iou_loss = 1. - iou_score return iou_loss\"]},\"404\":{\"h\":\"Focal Loss\",\"t\":[\"Focal Loss 是一种针对类别不平衡（Class Imbalance）问题的损失函数改进方案，由何恺明团队在2017年论文《Focal Loss for Dense Object Detection》中提出，主要用于解决目标检测任务中前景-背景类别极端不平衡的问题（如1:1000）。其核心思想是通过调整难易样本的权重，使模型更关注难分类的样本。\",\"Focal Loss 基于交叉熵损失进行扩展，将样本的权重进行动态调整。与交叉熵损失函数相比，Focal Loss引入了一个衰减因子，其中 pt 是预测的概率值。这个衰减因子能够使得易分类的样本（ pt较高 ）的权重降低，从而减少对分类正确样本的贡献。\",\"核心思想:\",\"(1) 类别不平衡的问题\",\"在分类任务中（尤其是目标检测），负样本（背景）往往远多于正样本（目标），导致：\",\"模型被大量简单负样本主导，难以学习有效特征。\",\"简单样本的梯度贡献淹没难样本的梯度。\",\"(2) Focal Loss 的改进\",\"降低易分类样本的权重：对模型已经分类正确的样本（高置信度）减少损失贡献。\",\"聚焦难分类样本：对分类错误的样本（低置信度）保持高损失权重。\",\"Focal Loss 基于标准交叉熵损失（Cross-Entropy Loss）改进而来。\",\"(1) 标准交叉熵损失（CE Loss）\",\"其中：\",\"p 是模型预测的概率（经过sigmoid/softmax）。\",\"y 是真实标签（0或1）。\",\"(2) Focal Loss 定义\",\"：类别平衡权重（通常），用于平衡正负样本数量差异。\",\"：调节因子（通常），控制难易样本的权重衰减程度。\",\"γ 参数用于抑制容易分类的样本，而 α 参数用于平衡正负类别的权重。两者解决的是不同维度的问题：\",\"α：防止前景点（功能区域）被背景淹没，解决数据集中“类别数量不平衡”的问题（数据集级别）；\",\"γ：防止模型只关注简单样本，忽略难分类样本，解决模型训练时“简单样本主导梯度”的问题（样本级别）；\",\"综上，先通过 α 平衡类别数量，再通过 γ 抑制简单样本，两者协同提升模型性能。\",\"关键参数的作用:\",\"参数\",\"作用\",\"典型值\",\"控制难易样本权重：• ：退化为CE Loss• ：显著抑制简单样本\",\"0.5 ~ 5\",\"平衡正负样本数量：• ：正样本较少时增加权重\",\"0.25 ~ 0.75\",\"难样本vs易样:\",\"易分类样本（如 p=0.9 ）： 接近0，损失被大幅降低。\",\"难分类样本（如 p=0.1 ）： 接近1，损失几乎不受影响。\",\"假设两个正样本：\",\"易样本：（模型已自信分类）\",\"标准 CE Loss：\",\"Focal Loss（）：损失权重降低 100 倍！\",\"难样本：（模型分类错误）\",\"标准 CE Loss：\",\"Focal Loss（）：损失权重仅降低 20%。\",\"应用场景：\",\"目标检测（如RetinaNet）： 解决前景（目标）与背景的极端不平衡问题。\",\"医学图像分割： 病灶区域像素远少于正常组织。\",\"任何类别不平衡的分类任务： 如欺诈检测、罕见疾病诊断等。\",\"优缺点:\",\"优点\",\"缺点\",\"显著提升难样本的分类性能\",\"需调参（）\",\"抑制简单样本的梯度主导\",\"对噪声标签敏感\",\"兼容大多数分类模型\",\"计算量略高于CE Loss\",\"Focal Loss 通过 动态调整样本权重，使模型聚焦难分类样本。\",\"参数选择：\",\"：一般从2开始调优（值越大，简单样本抑制越强）。\",\"：根据正负样本比例调整（如正样本少则增大 ）。\",\"适用场景：类别不平衡越严重，Focal Loss 效果越显著。\",\"代码实现:\",\"# 设置全局参数（可调） ALPHA = 0.8 # 控制正样本（目标点）与负样本（非目标点）之间的损失权重； # 若前景点稀疏（如 grasping area），建议设为较高值（如 0.25~0.75）； GAMMA = 2 # 聚焦参数，用于抑制易分类样本，放大难分类样本的影响； class FocalLoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数，构建一个基于 BCE 的改进版 Focal Loss。 参数: weight (Tensor): 可选参数，用于类别加权（未使用）； size_average (bool): 是否对 batch 内样本取平均 loss（已弃用）； \\\"\\\"\\\" super(FocalLoss, self).__init__() # 当前实现未使用 weight 和 size_average，保留接口以备扩展 def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1): \\\"\\\"\\\" 前向传播函数，计算预测输出与真实标签之间的 Focal Loss。 参数: inputs (Tensor): 模型输出的原始 logit 或经过 sigmoid 的概率值； 形状为 [B, N]（batch_size × 点数） targets (Tensor): ground truth 掩码，形状为 [B, N] alpha (float): 平衡因子，控制正类（功能区域）和负类（非功能区域）之间的损失权重； 前景点少 → alpha 高（如 0.75），防止被背景淹没； gamma (float): 聚焦参数，抑制 easy examples，放大 hard examples； smooth (float): 平滑项，防止除零错误，默认为 1 返回: focal_loss (Tensor): 计算得到的 Focal Loss 值 \\\"\\\"\\\" # 如果模型最后没有 sigmoid 层，则在这里激活 inputs = torch.sigmoid(inputs) # 将输入展平便于后续计算 # inputs: [B*N], 表示每个点属于功能区域的概率； # targets: [B*N], 表示每个点是否属于目标功能区域（soft/hard label）； inputs = inputs.view(-1) targets = targets.view(-1) # Step 1: 计算 Binary Cross Entropy Loss（BCE） # 这里使用 'mean' reduction，表示对 batch 内取平均 ce_loss = F.binary_cross_entropy(inputs, targets, reduction='mean') # Step 2: 计算 pt = exp(-ce_loss)，即 e^{-ce_loss} pt = torch.exp(-ce_loss) # shape: scalar # Step 3: 按类别分配 alpha alpha = torch.where(targets == 1, alpha, 1 - alpha) # Step 4: 构建 Focal Weight： # focal_weight = α * (1 - pt)^γ # 目的是：让难分类样本获得更大的 loss 权重，从而引导模型学习更多语义信息 focal_weight = alpha * (1 - pt) ** gamma # Step 5: 最终 Focal Loss = focal_weight × ce_loss focal_loss = focal_weight * ce_loss return focal_loss\",\"关于计算 p_t（模型对真实类别的预测概率）代码解析:\",\"pt = torch.exp(-ce_loss) # p_t = softmax(output)[target_class]\",\"ce_loss = F.cross_entropy(...) → 这是交叉熵损失；\",\"-ce_loss → 负号；\",\"torch.exp(-ce_loss) → 求 exp（自然指数）；\",\"但实际上这行代码的意图是计算 ，即模型对真实类别的预测概率（confidence）, 这里采用的方法是一种“技巧性近似”。对于一个样本，交叉熵损失为：\",\"所以：\",\"pt = torch.exp(-ce_loss)\",\"这个表达式其实是通过 CE loss 反推出来的 ，因为：\"]},\"405\":{\"h\":\"Tversky Loss\",\"t\":[\"Tversky Loss的设计灵感来自Tversky指数（Tversky index），它是一种用于度量集合之间相似性的指标，同时也是 Dice Loss 的一种泛化形式，通过引入两个可调节参数来增强模型对假阳性（False Positives）和假阴性（False Negatives）的敏感度控制。\",\"Tversky Loss 的核心是 Tversky 系数：\",\"然后损失就是：\",\"其中：\",\"TP ：真阳性（True Positive）= 预测为正类，且真实也为正类的样本数\",\"FP ：假阳性（False Positive）= 预测为正类，但真实是负类的样本数\",\"FN ：假阴性（False Negative）= 预测为负类，但真实是正类的样本数\",\"α 和 β 是两个可调节的超参数\",\"α 越大，FP 的影响就越大 → 模型更不喜欢“误报”\",\"β 越大，FN 的影响就越大 → 模型更不喜欢“漏报”\",\"如果你设置 α>β ，说明你更讨厌“误检”\",\"如果你设置 β>α ，说明你更讨厌“漏检”\",\"分母中的 TP+α⋅FP+β⋅FN 构成了一个“加权惩罚项”\",\"例如：\",\"α=0.3, β=0.7 → 更重视召回率（Recall）\",\"α=0.7, β=0.3 → 更重视精确率（Precision）\",\"当 alpha=beta=0.5 时，Tversky指数简化为Dice系数，该系数也等于F1得分。\",\"当 alpha=beta=1 时，公式转化为Tanimoto系数，而当 alpha+beta=1 时，得到一组F-beta得分。\",\"# 设置默认参数：当 alpha = beta = 0.5 时，等价于 Dice Loss ALPHA = 0.5 # 控制假阳性（FP）的权重 BETA = 0.5 # 控制假阴性（FN）的权重 class TverskyLoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数 参数： weight: 可选，类别权重（用于处理类别不平衡） size_average: 如果为 True，则返回所有样本损失的平均值 \\\"\\\"\\\" super(TverskyLoss, self).__init__() # 本类中不直接使用 weight 和 size_average，但保留它们作为接口兼容 self.weight = weight self.size_average = size_average def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA): \\\"\\\"\\\" 前向传播计算损失值 参数： inputs: 模型输出的预测结果（logits），形状如 (N, H, W) 或 (N, C, H, W) targets: 真实标签（ground truth），形状与 inputs 相同 smooth: 平滑系数，防止除以零 alpha: FP 的惩罚权重 beta: FN 的惩罚权重 返回： loss: 计算得到的 Tversky Loss \\\"\\\"\\\" # 如果模型最后一层没有 Sigmoid 激活函数，请取消下面这行注释 # 对输出应用 Sigmoid 函数，将 logits 转换为概率 [0,1] inputs = F.sigmoid(inputs) # 将输入和目标张量展平为一维，便于后续计算 TP、FP、FN inputs = inputs.view(-1) targets = targets.view(-1) # 真阳性（True Positive）：预测为正且实际也为正的像素数量 TP = (inputs * targets).sum() # 假阳性（False Positive）：预测为正但实际为负的像素数量 FP = ((1 - targets) * inputs).sum() # 假阴性（False Negative）：预测为负但实际为正的像素数量 FN = (targets * (1 - inputs)).sum() # 计算 Tversky 系数（相似度指标） # 分母中：TP + α·FP + β·FN Tversky = (TP + smooth) / (TP + alpha * FP + beta * FN + smooth) # 最终损失是 1 - Tversky，这样在训练中最小化损失就等于最大化重叠度 return 1 - Tversky\"]},\"406\":{\"h\":\"Lovasz Hinge Loss\",\"t\":[\"Lovasz Hinge Loss的设计思想是，在计算IoU得分之前，根据预测误差对预测结果进行排序，然后累积计算每个误差对IoU得分的影响。然后，将该梯度向量与初始误差向量相乘，以最大程度地惩罚降低IoU得分的预测结果。\",\"https://github.com/bermanmaxim/LovaszSoftmax\"]},\"407\":{\"h\":\"Combo Loss\",\"t\":[\"Combo Loss 是一种结合了多个损失函数优点的混合损失函数，特别适用于图像分割任务。它将 Dice Loss 和 交叉熵损失（CrossEntropy Loss） 相结合，并引入一个可调节的权重参数，使得模型在训练过程中可以更灵活地平衡这两部分损失。\",\"核心思想：\",\"Combo Loss = α × CrossEntropy + (1 - α) × Dice Loss\",\"或者更广义地：\",\"Combo Loss = α × 分类误差（CE）+ β × 区域重叠误差（Dice）\",\"其中 α + β = 1，α 控制分类误差的重要性，β 控制区域匹配误差的重要性。\",\"数学定义:\",\"假设我们有预测概率图 ，真实标签 ，那么：\",\"交叉熵损失（Binary Cross Entropy）：\",\"Dice Loss：\",\"Combo Loss 定义为：\",\"其中：\",\"：控制两个损失之间的权重比例\",\"若 ：仅使用交叉熵损失\",\"若 ：仅使用 Dice Loss\",\"为什么使用 Combo Loss:\",\"优势\",\"描述\",\"✔️ 兼顾像素级精度和区域重叠度\",\"CE 关注每个像素的分类准确性，Dice 关注整体区域匹配程度\",\"✔️ 对类别不平衡问题鲁棒\",\"在前景像素远少于背景像素时表现良好（如医学图像）\",\"✔️ 更稳定的训练过程\",\"避免单一损失可能带来的训练不稳定性\",\"✔️ 可调性强\",\"通过调整 α 参数，适应不同任务需求\",\"对比其他损失函数：\",\"损失函数\",\"是否关注像素分类？\",\"是否关注区域匹配？\",\"是否可调？\",\"是否适合类别不平衡？\",\"CrossEntropy Loss\",\"✅\",\"❌\",\"❌\",\"❌\",\"Dice Loss\",\"❌\",\"✅\",\"❌\",\"✅\",\"Tversky Loss\",\"❌\",\"✅ ✅\",\"✅\",\"✅ ✅\",\"Combo Loss\",\"✅ ✅\",\"✅\",\"✅\",\"✅ ✅\",\"代码实现:\",\"# 超参数设置说明： ALPHA = 0.5 # 控制交叉熵中正负样本的权重 # 如果 ALPHA < 0.5：对假阳性（FP）惩罚更重（更关注精确率） # 如果 ALPHA > 0.5：对假阴性（FN）惩罚更重（更关注召回率） CE_RATIO = 0.5 # 控制交叉熵损失和 Dice 损失之间的权重分配 # CE_RATIO 越大，交叉熵在总损失中的占比越高 class ComboLoss(nn.Module): def __init__(self, weight=None, size_average=True): \\\"\\\"\\\" 初始化函数 参数： weight: 可选，类别权重（用于处理类别不平衡） size_average: 如果为 True，则返回所有样本损失的平均值 \\\"\\\"\\\" super(ComboLoss, self).__init__() # 这里不直接使用 weight 和 size_average，但保留作为接口兼容 self.weight = weight self.size_average = size_average def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, eps=1e-9): \\\"\\\"\\\" 前向传播计算 Combo Loss 参数： inputs: 模型输出的概率值（经过 Sigmoid），形状如 (N, H, W) targets: 真实标签，形状与 inputs 相同，值为 0 或 1 smooth: 平滑系数，防止除以零 alpha: 控制 FP/FN 的惩罚比例（用于交叉熵部分） eps: 防止 log(0) 出现的小常数 返回： combo_loss: 计算得到的 Combo Loss \\\"\\\"\\\" # 将输入和目标张量展平为一维，便于后续计算 inputs = inputs.view(-1) targets = targets.view(-1) # 计算 Dice Loss 所需的交集 intersection = (inputs * targets).sum() # Dice Score（区域匹配度） dice_score = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth) # 加入数值稳定性处理，防止 log(0) 出现 NaN # torch.clamp(x, min=a, max=b) 是 PyTorch 中的一个函数，用于将张量 x 中的每个元素限制在 [a, b] 区间内： # 这里把所有 inputs 中的值限制在区间 [eps, 1.0 - eps] 内，防止出现 0 或 1 的极端值。 inputs = torch.clamp(inputs, eps, 1.0 - eps) # 加权交叉熵损失（Weighted Cross Entropy） # 根据 ALPHA 参数调整正类和负类的权重 weighted_ce = - (ALPHA * targets * torch.log(inputs)) - ((1 - ALPHA) * (1 - targets) * torch.log(1 - inputs)) # 对损失求均值 weighted_ce = weighted_ce.mean() # Combo Loss 是交叉熵和 Dice Loss 的加权组合 # 注意：这里使用的是负的 Dice Score（因为要最小化损失） combo_loss = (CE_RATIO * weighted_ce) - ((1 - CE_RATIO) * dice_score) return combo_loss\",\"上面代码实现中使用的是加权交叉熵损失:\"]},\"408\":{\"h\":\"如何选择?\",\"t\":[\"任务需求：根据特定的分割任务的需求和特点，选择适合的损失函数。例如，对于类别不平衡的数据集，可以考虑使用Tversky Loss或Combo Loss等能够处理不平衡情况的损失函数。\",\"实验评估：在实验中，使用不同的损失函数进行训练，并评估它们在验证集或测试集上的性能。比较它们在IoU、准确率、召回率等指标上的表现，选择性能最佳的损失函数。\",\"超参数调整：一些损失函数具有额外的超参数，如Tversky Loss中的alpha和beta，可以通过调整这些超参数来进一步优化损失函数的性能。\"]},\"409\":{\"h\":\"通俗易懂解读BPE分词算法实现\",\"t\":[\"通俗易懂解读BPE分词算法实现\",\"BPE（Byte Pair Encoding，字节对编码）是一种基于频率统计的子词分词算法 ，广泛用于现代自然语言处理任务中，特别是在像 BERT、GPT 和 LLaMA 这样的大模型中。它的核心思想是通过不断合并最常见的字符对来构建一个高效的词汇表。\",\"BPE 的核心思想:\",\"从字符级别开始，逐步合并高频的字符对。\",\"最终生成一个既能表示常见单词，又能拆解未知词的子词词汇表 。\",\"可以有效控制词汇表大小，同时避免“未登录词”问题（OOV, Out-of-Vocabulary）。\"]},\"410\":{\"h\":\"预训练过程\",\"t\":[\"BPE 算法预训练工作流程:\",\"训练语料为: Hello World , Hey Wow\",\"1. 读取训练语料，同时完成断句分词任务\",\"# filepaths: 训练语料所在的文件列表 def create_vocab(filepaths: List[str]) -> Dict[str, int]: # 获取所有单词和每个单词的出现次数词典 vocab = defaultdict(int) for path in tqdm(filepaths, desc='Creating vocabulary'): text = open(path, 'r', encoding='utf-8-sig').read() # 利用NLTK库提供的sent_tokenize方法完成断句功能，即将原文本按照空格，句号等标点符号结合语义进行断句。 sentences = sent_tokenize(text) # 遍历句子列表 for sentence in sentences: # 利用NLTK库提供的wordpunct_tokenize方法完成分词功能 tokens = wordpunct_tokenize(sentence) # 记录每个词的出现次数 for token in tokens: vocab[token] += 1 # vocab: 记录每个词的出现次数的词典 return vocab\",\"2. 过滤掉vocab中的低频词\",\"def truncate_vocab(vocab: Dict[str, int], mincount: int) -> None: tokens = list(vocab.keys()) for token in tokens: if vocab[token] < mincount: del(vocab[token])\",\"示例中设置为了1，不会过滤掉任何词。\",\"3. 数据预处理\",\"将训练语料中的每个单词按字符拆分，并在结尾加上特殊标记 </w> 表示单词结束。\",\"def prepare_bpe_vocab(vocab: Dict[str, int]) -> Dict[str, int]: bpe_vocab = {} # 遍历vocab中所有词 for token in vocab: # 每个词的每个字符后都加上空格，同时末尾加上 </w> 表示单词结束 ntoken = ' '.join(list(token)) + ' </w>' bpe_vocab[ntoken] = vocab[token] return bpe_vocab\",\"4. 经历N次迭代，合并前N个最频繁的字符对\",\" # 一共合并merges个高频字符对后,才结束词汇表的构建 for i in trange(merges, desc='Merging'): # 1. 获取每个相邻字符对的出现次数 pairs = get_stats(vocab) # 2. 获取当前最高频的字符对 best = max(pairs, key=pairs.get) # 3. 合并当前最高频的字符对 vocab = merge_vocab(best, vocab) ######记录历史合并的最高频子词对及其频率(传统BPE算法没有这一步)###### merged_pair_freqs = defaultdict(int) # 一共合并merges个高频字符对后,才结束词汇表的构建 for _ in trange(merges, desc='Merging'): # 1. 获取每个相邻字符对的出现次数 pairs = get_stats(vocab) # 2. 获取当前最高频的字符对 best_pair = max(pairs.items(), key=lambda x: x[1]) ######记录该子词对的全局频率(传统BPE算法没有这一步)###### best_subword = ''.join(best_pair[0]) best_freq = best_pair[1] merged_pair_freqs[best_subword] += best_freq # 3. 合并当前最高频的字符对 vocab = merge_vocab(best_pair[0], vocab)\",\"4.1 获取每个相邻字符对的出现次数\",\"def get_stats(vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]: pairs = defaultdict(int) for word, freq in vocab.items(): # 对经过预处理的vocab中的每个词按空格进行切分 symbols = word.split() # 统计每个相邻字符对的出现次数 for i in range(len(symbols)-1): pairs[symbols[i],symbols[i+1]] += freq return pairs\",\"首轮统计展示\",\"4.2 获取当前最高频的字符对\",\"4.3 合并当前最高频的字符对\",\"def merge_vocab(pair: Tuple[str, str], v_in: Dict[str, int]) -> Dict[str, int]: # 1. 将传入的最高频字符对中的两个字符用空格拼接起来，如: \\\"H e\\\" bigram = re.escape(' '.join(pair)) v_out = {} # 2. 正则匹配含有“H e”的所有单词，并且“H”和“e”必须为两个独立的词，而不能为\\\"HH e\\\"或者\\\"H ee\\\"形式 p = re.compile(r'(?<!\\\\S)' + bigram + r'(?!\\\\S)') # 3. 遍历vocab中所有词 for word in v_in: # 3.1 用正则匹配并替换匹配上的 \\\"H e\\\" 为 “He” w_out = p.sub(''.join(pair), word) v_out[w_out] = v_in[word] # 4. 返回合并最高频字符对后的vocab return v_out\",\"5.根据N轮迭代合并后的Vocab来构建最终的频次表(每个子词的出现次数)\",\"def count_byte_freqs(vocab: Dict[str, int]) -> Dict[str, int]: freqs = defaultdict(int) for word in vocab: # 1. 按空格切分 bytes_ = word.split(' ') # 2. 每个子词出现次数加1 for byte in bytes_: freqs[byte] += 1 # 3. 添加一些特殊词 for token in ['<line/>', '</line>', '<pad>', '<unk>']: freqs[token] += 1 return freqs\",\"6.根据频次表构建最终的词汇表\",\"def create_vocab_maps(freqs: Dict[str, int]) -> (Dict[str, int], Dict[int, str]): # 1. 按照 词频从高到低 的顺序排序 ordered_freqs = sorted(freqs.items(), key=lambda x: x[1], reverse=True) vocab_to_idx, idx_to_vocab = {}, {} for i in range(len(ordered_freqs)): # 2. 构建词汇表 word, freq = ordered_freqs[i] vocab_to_idx[word] = i idx_to_vocab[i] = word return vocab_to_idx, idx_to_vocab\",\"7. freqs = 最终子词频率 + 历史最高频合并对的频率(传统BPE算法没有这一步)\",\" freqs.update(merged_pair_freqs)\",\"8. 通常最后会将预训练生成的频次表和词汇表写入文件保存\",\" def save(self, path: str) -> None: # 1. 频次表记录合并规则，也就是有哪些子词以及这些子词的出现次数，作为分词时的合并规则和优先选择权 with open(f'{path}/freqs.json', 'w', encoding='utf-8') as outfile: json.dump(self.freqs, outfile, indent=4, ensure_ascii=False) # 2. 常规的词汇表 with open(f'{path}/vocab_to_idx.json', 'w', encoding='utf-8') as outfile: json.dump(self.vocab_to_idx, outfile, indent=4, ensure_ascii=False) with open(f'{path}/idx_to_vocab.json', 'w', encoding='utf-8') as outfile: json.dump(self.idx_to_vocab, outfile, indent=4, ensure_ascii=False)\",\"BPE 算法预训练过程完整代码如下\",\" def train_bpe(filepaths: List[str], mincount: int, merges: int) -> 'BytePairTokenizer': vocab = create_vocab(filepaths) truncate_vocab(vocab, mincount) vocab = prepare_bpe_vocab(vocab) merged_pair_freqs = defaultdict(int) # (传统BPE算法没有这一步) for _ in trange(merges, desc='Merging'): pairs = get_stats(vocab) best_pair = max(pairs.items(), key=lambda x: x[1]) best_subword = ''.join(best_pair[0]) # (传统BPE算法没有这一步) best_freq = best_pair[1] # (传统BPE算法没有这一步) merged_pair_freqs[best_subword] += best_freq # (传统BPE算法没有这一步) vocab = merge_vocab(best_pair[0], vocab) freqs = count_byte_freqs(vocab) vocab_to_idx, idx_to_vocab = create_vocab_maps(freqs) freqs.update(merged_pair_freqs) # (传统BPE算法没有这一步) return BytePairTokenizer(freqs, vocab_to_idx, idx_to_vocab)\"]},\"411\":{\"h\":\"分词过程\",\"t\":[\"1.对输入的文本进行断句加分词\",\" # 使用NLTK库提供的sent_tokenize方法进行分词 lines = sent_tokenize(open(filepath, encoding='utf-8-sig').read()) tokens = [] # 遍历所有句子 for line in lines: if len(line) > 1: tokens += get_line_ids(line, tokenizer)\",\"def get_line_ids(line: str, tokenizer: BytePairTokenizer) -> List[int]: # 对每个句子进行分词 tokens = wordpunct_tokenize(line) # 将每个词从str转换为list列表形式，同时列表末尾追加</w> tokens = [list(t) + ['</w>'] for t in tokens] ...\",\"以输入 \\\"Hello World\\\" 为例\",\"2. 对当前句子中每个词进行子词合并加词ID映射，最后得到当前句子对应的Token列表\",\"def get_line_ids(line: str, tokenizer: BytePairTokenizer) -> List[int]: ... lineids = [] for token in tokens: # 2.1 对每个词进行子词合并，直到无法合并为止 token = tokenizer.merge_bytes(token) # 2.2 将当前词列表中每个子词映射为字典中对于的词ID ids = tokenizer.get_byte_ids(token) lineids += ids sol_id = tokenizer.get_byte_id('<line/>') eol_id = tokenizer.get_byte_id('</line>') lineids = [sol_id] + lineids + [eol_id] return lineids\",\"2.1 对每个词进行子词合并，直到无法合并为止\",\" # 对当前词的子词进行合并，直到无法合并为止 def merge_bytes(self, bytes_: List[str]) -> List[str]: bytes_, merged = self.merge_max_pair(bytes_) while merged: bytes_, merged = self.merge_max_pair(bytes_) return bytes_ def merge_max_pair(self, bytes_: List[str]) -> (List[str], bool): # 1. 取出出现次数最多的字符对 max_pair = self.get_max_pair_idxs(bytes_) merged = True if max_pair is not None else False if merged: # 2. 合并该字符对 bytes_ = bytes_[:max_pair[0]] + \\\\ [''.join(bytes_[max_pair[0]:max_pair[1]+1])] + \\\\ bytes_[max_pair[1]+1:] return bytes_, merged def get_max_pair_idxs(self, bytes_) -> Tuple[int, int]: pairs = {} # 1. 遍历所有相邻字符对的组合 for i in range(1, len(bytes_)): pair = ''.join(bytes_[i-1:i+1]) # 2. 判断每个字符对是否存在于频次表中，如果存在记录出现次数 if pair in self.freqs: pairs[(i-1, i)] = self.freqs[pair] # 3. 取出出现次数最多的字符对 return None if len(pairs) == 0 else max(pairs, key=pairs.get)\",\"2.2 将当前词列表中每个子词映射为字典中对于的词ID\",\" def get_byte_ids(self, bytes_): ids = [] for byte in bytes_: if byte in self.vocab_to_idx: ids.append(self.vocab_to_idx[byte]) else: ids.append(self.vocab_to_idx[self.unk]) return ids\"]},\"412\":{\"h\":\"附录\",\"t\":[\"BPE 分词器完整代码实现:\",\"from typing import Tuple, Dict, List from collections import defaultdict import json, re from nltk import wordpunct_tokenize, sent_tokenize from tqdm import trange, tqdm class BytePairTokenizer: def __init__(self, freqs: Dict[str, int], vocab_to_idx: Dict[str, int], idx_to_vocab: Dict[int, str]): \\\"\\\"\\\" Initialize byte pair tokenizer Args: freqs: frequency dictionary of vocabulary vocab_to_index: map of vocabulary words to indices index_to_vocab: map of vocabulary indices to words \\\"\\\"\\\" self.vocab_to_idx = vocab_to_idx self.idx_to_vocab = idx_to_vocab self.freqs = freqs self.sol = '<line/>' self.eol = '</line>' self.pad = '<pad>' self.unk = '<unk>' self.eow = '</w>' def get_sol(self) -> str: return self.sol def get_eol(self) -> str: return self.eol def get_pad(self) -> str: return self.pad def get_unk(self) -> str: return self.unk def get_eow(self) -> str: return self.eow def get_byte(self, byte_id: int) -> str: return self.idx_to_vocab[byte_id] def get_byte_id(self, byte: str) -> int: unk_id = self.vocab_to_idx[self.unk] bid = self.vocab_to_idx[byte] if byte in self.vocab_to_idx else unk_id return bid def get_byte_ids(self, bytes_): \\\"\\\"\\\" Get byte ids for each byte in provided list \\\"\\\"\\\" ids = [] for byte in bytes_: if byte in self.vocab_to_idx: ids.append(self.vocab_to_idx[byte]) else: ids.append(self.vocab_to_idx[self.unk]) return ids def get_bytes(self, byte_ids: List[int]) -> List[str]: \\\"\\\"\\\" Given a list of byte ids return corresponding bytes Args: byte_ids: list of byte ids Returns: (List[str]): list of bytes \\\"\\\"\\\" tokens = [] for byte_id in byte_ids: tokens.append(self.idx_to_vocab[byte_id]) return tokens def merge_bytes(self, bytes_: List[str]) -> List[str]: \\\"\\\"\\\" Return list of bytes with max pair merged Args: bytes_: list to merge max pair in Returns: (List[str]): list of bytes with all max pair occurrences merged \\\"\\\"\\\" bytes_, merged = self.merge_max_pair(bytes_) while merged: bytes_, merged = self.merge_max_pair(bytes_) return bytes_ def merge_max_pair(self, bytes_: List[str]) -> (List[str], bool): \\\"\\\"\\\" Takes in a list of bytes and merges the max pair if possible Args: bytes_: list of bytes to merge max pair in Returns: (bytes_): list of bytes with max pair merged (bool): flag indicating whether merge occurred \\\"\\\"\\\" max_pair = self.get_max_pair_idxs(bytes_) merged = True if max_pair is not None else False if merged: bytes_ = bytes_[:max_pair[0]] + \\\\ [''.join(bytes_[max_pair[0]:max_pair[1]+1])] + \\\\ bytes_[max_pair[1]+1:] return bytes_, merged def get_max_pair_idxs(self, bytes_) -> Tuple[int, int]: \\\"\\\"\\\" Get index of maximum byte pair in list of bytes Args: bytes_: list of bytes to find maximum pair from Returns: (Tuple[int, int]): maximum frequency byte pair \\\"\\\"\\\" pairs = {} for i in range(1, len(bytes_)): pair = ''.join(bytes_[i-1:i+1]) if pair in self.freqs: pairs[(i-1, i)] = self.freqs[pair] return None if len(pairs) == 0 else max(pairs, key=pairs.get) def save(self, path: str) -> None: with open(f'{path}/freqs.json', 'w', encoding='utf-8') as outfile: json.dump(self.freqs, outfile, indent=4, ensure_ascii=False) with open(f'{path}/vocab_to_idx.json', 'w', encoding='utf-8') as outfile: json.dump(self.vocab_to_idx, outfile, indent=4, ensure_ascii=False) with open(f'{path}/idx_to_vocab.json', 'w', encoding='utf-8') as outfile: json.dump(self.idx_to_vocab, outfile, indent=4, ensure_ascii=False) @staticmethod def load(path: str) -> 'BytePairTokenizer': with open(f'{path}/freqs.json', 'r', encoding='utf-8') as infile: freqs = json.load(infile) with open(f'{path}/vocab_to_idx.json', 'r', encoding='utf-8') as infile: vocab_to_idx = json.load(infile) with open(f'{path}/idx_to_vocab.json', 'r', encoding='utf-8') as infile: idx_to_vocab = json.load(infile) return BytePairTokenizer(freqs, vocab_to_idx, idx_to_vocab) @staticmethod def train_bpe(filepaths: List[str], mincount: int, merges: int) -> 'BytePairTokenizer': vocab = create_vocab(filepaths) truncate_vocab(vocab, mincount) vocab = prepare_bpe_vocab(vocab) merged_pair_freqs = defaultdict(int) for _ in trange(merges, desc='Merging'): pairs = get_stats(vocab) if not pairs: break best_pair = max(pairs.items(), key=lambda x: x[1]) best_subword = ''.join(best_pair[0]) best_freq = best_pair[1] merged_pair_freqs[best_subword] += best_freq vocab = merge_vocab(best_pair[0], vocab) freqs = count_byte_freqs(vocab) vocab_to_idx, idx_to_vocab = create_vocab_maps(freqs) freqs.update(merged_pair_freqs) return BytePairTokenizer(freqs, vocab_to_idx, idx_to_vocab) def create_vocab(filepaths: List[str]) -> Dict[str, int]: \\\"\\\"\\\" Create dictionary of vocabulary frequencies in given list of files Args: filepaths: list of filepaths to collect vocabulary from Returns: (Dict[str, int]): dictionary mapping vocabulary terms to their frequency \\\"\\\"\\\" vocab = defaultdict(int) for path in tqdm(filepaths, desc='Creating vocabulary'): text = open(path, 'r', encoding='utf-8-sig').read() sentences = sent_tokenize(text) for sentence in sentences: tokens = wordpunct_tokenize(sentence) for token in tokens: vocab[token] += 1 return vocab def truncate_vocab(vocab: Dict[str, int], mincount: int) -> None: \\\"\\\"\\\" Truncate vocabulary dictionary based on a minimum count Args: vocab: frequency mapping dictionary to truncate mincount: minimum count for members of dictionary (words with lower frequencies will be removed) \\\"\\\"\\\" tokens = list(vocab.keys()) for token in tokens: if vocab[token] < mincount: del(vocab[token]) def prepare_bpe_vocab(vocab: Dict[str, int]) -> Dict[str, int]: \\\"\\\"\\\" Prepare vocabulary frequency dictionary for byte-pair generation. End-of-word byte '</w>' added to words, every character separated by space Args: vocab: vocabulary frequency dictionary to prepare Returns: (Dict[str, int]): byte-pair ready vocabulary frequency dictionary \\\"\\\"\\\" bpe_vocab = {} for token in vocab: ntoken = ' '.join(list(token)) + ' </w>' bpe_vocab[ntoken] = vocab[token] return bpe_vocab def get_stats(vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]: \\\"\\\"\\\" Count all bytepairs in a dictionary containing vocabulary frequencies Args: vocab: dictionary mapping words to their frequency Returns: (Dict[Tuple[str, str], int]): dictionary containing byte pair frequencies \\\"\\\"\\\" pairs = defaultdict(int) for word, freq in vocab.items(): symbols = word.split() for i in range(len(symbols)-1): pairs[symbols[i],symbols[i+1]] += freq return pairs def merge_vocab(pair: Tuple[str, str], v_in: Dict[str, int]) -> Dict[str, int]: \\\"\\\"\\\" Merge all instances of given byte pair in vocabulary frequency dictionary Args: pair: byte pair to merge v_in: vocabulary to merge byte pair int Returns: (Dict[str, int]): resulting vocabulary with all instances of given byte pair merged \\\"\\\"\\\" bigram = re.escape(' '.join(pair)) v_out = {} p = re.compile(r'(?<!\\\\S)' + bigram + r'(?!\\\\S)') for word in v_in: w_out = p.sub(''.join(pair), word) v_out[w_out] = v_in[word] return v_out def count_byte_freqs(vocab: Dict[str, int]) -> Dict[str, int]: freqs = defaultdict(int) for word in vocab: bytes_ = word.split(' ') for byte in bytes_: freqs[byte] += 1 for token in ['<line/>', '</line>', '<pad>', '<unk>']: freqs[token] += 1 return freqs def create_vocab_maps(freqs: Dict[str, int]) -> (Dict[str, int], Dict[int, str]): \\\"\\\"\\\" Create map of vocabulary terms to indices and vice versa. Word indices are in order of their frequency in the provided vocabulary Args: freqs: dictionary mapping vocabulary terms to their frequencies Returns: (Dict[str, int]): dictionary mapping vocab to indices (Dict[int, str]): dictionary mapping indices to vocab \\\"\\\"\\\" ordered_freqs = sorted(freqs.items(), key=lambda x: x[1], reverse=True) vocab_to_idx, idx_to_vocab = {}, {} for i in range(len(ordered_freqs)): word, freq = ordered_freqs[i] vocab_to_idx[word] = i idx_to_vocab[i] = word return vocab_to_idx, idx_to_vocab\"]},\"413\":{\"h\":\"大模型微调(Fine Tuning)知识扫盲\",\"t\":[\"大模型微调(Fine Tuning)知识扫盲\"]},\"414\":{\"h\":\"什么是大模型 ？\",\"t\":[\"开始之前，为了方便大家理解，我们先对大模型做一个直观的抽象。\",\"本质上，现在的大模型要解决的问题，就是一个序列数据转换的问题：\",\"输入序列 X = [x1, x2, ..., xm]\",\"输出序列Y = [y1, y2, …, yn]\",\"X和Y之间的关系是：Y = WX。\",\"我们所说的“大模型”这个词：“大”是指用于训练模型的参数非常多，多达千亿、万亿；而“模型”指的就是上述公式中的矩阵W。\",\"在这里，矩阵W就是通过机器学习，得出的用来将X序列，转换成Y序列的权重参数组成的矩阵。\",\"需要特别说明：这里为了方便理解，做了大量的简化。在实际的模型中，会有多个用于不同目的的权重参数矩阵，也还有一些其它参数。\"]},\"415\":{\"h\":\"为什么要对大模型进行微调 ？\",\"t\":[\"通常，要对大模型进行微调，有以下一些原因：\",\"因为大模型的参数量非常大，训练成本非常高，每家公司都去从头训练一个自己的大模型，这个事情的性价比非常低；\",\"Prompt Engineering的方式是一种相对来说容易上手的使用大模型的方式，但是它的缺点也非常明显。因为通常大模型的实现原理，都会对输入序列的长度有限制，Prompt Engineering 的方式会把Prompt搞得很长。\",\"越长的Prompt，大模型的推理成本越高，因为推理成本是跟Prompt长度的平方正向相关的。\",\"另外，Prompt太长会因超过限制而被截断，进而导致大模型的输出质量打折口，这也是一个非常严重的问题。\",\"对于个人使用者而言，如果是解决自己日常生活、工作中的一些问题，直接用Prompt Engineering的方式，通常问题不大。\",\"但对于对外提供服务的企业来说，要想在自己的服务中接入大模型的能力，推理成本是不得不要考虑的一个因素，微调相对来说就是一个更优的方案。\",\"Prompt Engineering的效果达不到要求，企业又有比较好的自有数据，能够通过自有数据，更好的提升大模型在特定领域的能力。这时候微调就非常适用。\",\"要在个性化的服务中使用大模型的能力，这时候针对每个用户的数据，训练一个轻量级的微调模型，就是一个不错的方案。\",\"数据安全的问题。如果数据是不能传递给第三方大模型服务的，那么搭建自己的大模型就非常必要。通常这些开源的大模型都是需要用自有数据进行微调，才能够满足业务的需求，这时候也需要对大模型进行微调。\"]},\"416\":{\"h\":\"如何对大模型进行微调 ？\",\"t\":[\"从参数规模的角度，大模型的微调分成两条技术路线：\",\"一条是对全量的参数，进行全量的训练，这条路径叫全量微调FFT(Full Fine Tuning)。\",\"一条是只对部分的参数进行训练，这条路径叫PEFT(Parameter-Efficient Fine Tuning)。\",\"FFT的原理，就是用特定的数据，对大模型进行训练，将W变成，相比W ，最大的优点就是上述特定数据领域的表现会好很多。\",\"但FFT也会带来一些问题，影响比较大的问题，主要有以下两个：\",\"一个是训练的成本会比较高，因为微调的参数量跟预训练的是一样的多的；\",\"一个是叫灾难性遗忘(Catastrophic Forgetting)，用特定训练数据去微调可能会把这个领域的表现变好，但也可能会把原来表现好的别的领域的能力变差。\",\"PEFT主要想解决的问题，就是FFT存在的上述两个问题，PEFT也是目前比较主流的微调方案。\",\"从训练数据的来源、以及训练的方法的角度，大模型的微调有以下几条技术路线：\",\"监督式微调SFT(Supervised Fine Tuning) : 用人工标注的数据，用传统机器学习中监督学习的方法，对大模型进行微调；\",\"基于人类反馈的强化学习微调RLHF(Reinforcement Learning with Human Feedback) : 把人类的反馈，通过强化学习的方式，引入到对大模型的微调中去，让大模型生成的结果，更加符合人类的一些期望；\",\"基于AI反馈的强化学习微调RLAIF(Reinforcement Learning with AI Feedback) : 原理大致跟RLHF类似，但是反馈的来源是AI。这里是想解决反馈系统的效率问题，因为收集人类反馈，相对来说成本会比较高、效率比较低。\",\"不同的分类角度，只是侧重点不一样，对同一个大模型的微调，也不局限于某一个方案，可以多个方案一起。\",\"微调的最终目的，是能够在可控成本的前提下，尽可能地提升大模型在特定领域的能力。\"]},\"417\":{\"h\":\"常用的PEFT方案\",\"t\":[\"从成本和效果的角度综合考虑，PEFT是目前业界比较流行的微调方案。接下来介绍几种比较流行的PEFT微调方案。\"]},\"418\":{\"h\":\"Prompt Tuning\",\"t\":[\"Prompt Tuning的出发点，是基座模型(Foundation Model)的参数不变，为每个特定任务，训练一个少量参数的小模型，在具体执行特定任务的时候按需调用。\",\"Prompt Tuning的基本原理是在输入序列X之前，增加一些特定长度的特殊Token，以增大生成期望序列的概率。\",\"具体来说，就是将变成，。\",\"Prompt Tuning是发生在Embedding这个环节的。如果将大模型比做一个函数：，那么Prompt Tuning就是在保证函数本身不变的前提下，在X前面加上了一些特定的内容，而这些内容可以影响X生成期望中Y的概率。\",\"Prompt Tuning的具体细节,可以参见：The Power of Scale for Parameter-Efficient Prompt Tuning。\"]},\"419\":{\"h\":\"Prefix Tuning\",\"t\":[\"Prefix Tuning的灵感来源是，基于Prompt Engineering的实践表明，在不改变大模型的前提下，在Prompt上下文中添加适当的条件，可以引导大模型有更加出色的表现。\",\"Prefix Tuning的出发点，跟Prompt Tuning的是类似的，只不过它们的具体实现上有一些差异。\",\"Prompt Tuning是在Embedding环节，往输入序列X前面加特定的Token。而Prefix Tuning是在Transformer的Encoder和Decoder的网络中都加了一些特定的前缀。\",\"具体来说，就是将Y=WX中的W，变成。\",\"Prefix Tuning也保证了基座模型本身是没有变的，只是在推理的过程中，按需要在W前面拼接一些参数。\",\"Prefix Tuning的具体细节,可以参见：Prefix-Tuning: Optimizing Continuous Prompts for Generation。\"]},\"420\":{\"h\":\"LoRA\",\"t\":[\"LoRA是跟Prompt Tuning和Prefix Tuning完全不相同的另一条技术路线。\",\"LoRA背后有一个假设：我们现在看到的这些大语言模型，它们都是被过度参数化的。而过度参数化的大模型背后，都有一个低维的本质模型。\",\"通俗讲人话：大模型参数很多，但并不是所有的参数都是发挥同样作用的；大模型中有其中一部分参数，是非常重要的，是影响大模型生成结果的关键参数，这部分关键参数就是上面提到的低维的本质模型。\",\"LoRA的基本思路，包括以下几步：\",\"首先, 要适配特定的下游任务，要训练一个特定的模型，将Y=WX变成Y=(W+∆W)X，这里面∆W主是我们要微调得到的结果；\",\"其次，将∆W进行低维分解∆W=AB (∆W为m * n维，A为m * r维，B为r * n维，r就是上述假设中的低维)；\",\"接下来，用特定的训练数据，训练出A和B即可得到∆W，在推理的过程中直接将∆W加到W上去，再没有额外的成本。\",\"另外，如果要用LoRA适配不同的场景，切换也非常方便，做简单的矩阵加法即可：(W + ∆W) - ∆W + ∆W'。\",\"关于LoRA的具体细节,可以参见LoRA: Low-Rank Adaptation of Large Language Models。\"]},\"421\":{\"h\":\"QLoRA\",\"t\":[\"LoRA 效果已经非常好了，可以媲美全量微调的效果了，那为什么还要有个QLoRA呢？\",\"这里先简单介绍一下，量化（Quantization）。\",\"量化，是一种在保证模型效果基本不降低的前提下，通过降低参数的精度，来减少模型对于计算资源的需求的方法。\",\"量化的核心目标是降成本，降训练成本，特别是降后期的推理成本。\",\"QLoRA就是量化版的LoRA，它是在LoRA的基础上，进行了进一步的量化，将原本用16bit表示的参数，降为用4bit来表示，可以在保证模型效果的同时，极大地降低成本。\",\"论文中举的例子，65B的LLaMA的微调要780GB的GPU内存；而用了QLoRA之后，只需要48GB。效果相当惊人！\",\"关于QLoRA的具体细节,可以参见：QLoRA: Efficient Finetuning of Quantized LLMs。\",\"PEFT 的微调方法，还有很多种，限于篇幅原因，不再这里一一介绍。感兴趣的朋友，可以阅读这篇论文：Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning。\",\"相关阅读资料:\",\"近代自然语言处理技术发展的“第四范式”\",\"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\"]},\"422\":{\"h\":\"通俗易懂讲解LoRA微调\",\"t\":[\"通俗易懂讲解LoRA微调\",\"论文链接: LoRA: Low-Rank Adaptation of Large Language Models\"]},\"423\":{\"h\":\"符合认知的大模型微调流程\",\"t\":[\"符合我们直接观念所想的大模型微调流程为:\",\"准备与下游任务相关的数据集\",\"选择合适的预训练好的大模型\",\"在特定任务相关的数据集上执行有监督全量参数微调，将预训练模型的参数 调整为适合下游任务的 \",\"其中第三步通过反向传播全量更新模型参数的过程如下：\",\"图1: 反向传播更新模型参数过程\",\"上述的全量微调流程问题在于大模型的参数量往往特别大，也就是 占据了特别大的内存资源和计算资源，有没有办法能够减少 所占的内存资源和计算资源呢？\",\"图2: 低秩分解\",\"我们可以利用矩阵分解技术，将原始的 矩阵从 参数量降低到 级别的参数量，如下图所示:\",\"图3: 待微调的参数量下降到原来的9%\",\"这样一来，我们微调大模型的流程就变为了(前两步不变)：\",\"初始化低秩矩阵：对于需要微调的密集层，初始化两个低秩矩阵 和 ，其维度分别为 和 ，其中 是低秩的秩，远小于原始矩阵的维度。\",\"冻结预训练模型参数：在微调过程中，保持预训练模型的原始参数 不变，只对低秩矩阵 和 进行训练。\",\"执行微调训练：在准备好的数据集上，通过反向传播算法更新低秩矩阵 和 的参数，使得模型在下游任务上的表现逐渐优化。\",\"合并参数（可选）：在微调完成后，如果需要，可以将低秩矩阵 和 的更新量与原始参数 合并，得到最终适用于下游任务的模型参数 。\",\"这在LoRA这篇论文中也被称为低秩分解自适应技术。\",\"图4: 常规微调 VS LoRA微调\"]},\"424\":{\"h\":\"大模型微调大致发展历史\",\"t\":[\"大公司或者研究机构，都是有足够资源的来开发大模型，但是对于一般的小公司或者个人来说，要想开发自己的大模型几乎不可能，要知道像 ChatGPT 这样的大模型，一次训练的成本就在上千亿美元。\",\"那么那些小公司或者个人，又怎么能够利用这些开源的大模型，在自己的数据上继续训练，从而应用于自己的业务场景？有没有低成本的方法微调大模型？\",\"答案是有的。目前主流的方法包括2019年 Houlsby N 等人提出的 Adapter Tuning，2021年微软提出的 LORA，斯坦福提出的 Prefix-Tuning，谷歌提出的 Prompt Tuning，2022年清华提出的 P-tuning v2。\",\"这些方法都有各自的特点，从个人使用情况来说，LORA 的效果会好于其它几种方法。其它方法都有各自的一些问题：\",\"Adapter Tuning 增加了模型层数，引入了额外的推理延迟\",\"Prefix-Tuning 难于训练，且预留给 Prompt 的序列挤占了下游任务的输入序列空间，影响模型性能\",\"P-tuning v2 很容易导致旧知识遗忘，微调之后的模型，在之前的问题上表现明显变差\",\"基于上述背景，LORA 得益于前人的一些关于内在维度（intrinsic dimension）的发现：\",\"模型是过参数化的，它们有更小的内在维度，模型主要依赖于这个低的内在维度（low intrinsic dimension）去做任务适配。\",\"假设模型在任务适配过程中权重的改变量是低秩（low rank）的，由此提出低秩自适应（LoRA）方法。\",\"LoRA 允许我们通过优化适应过程中密集层变化的秩分解矩阵，来间接训练神经网络中的一些密集层，同时保持预先训练的权重不变。\"]},\"425\":{\"h\":\"LoRA 微调\",\"t\":[\"图5: LoRA 微调流程\",\"LoRA 的思想很简单:\",\"在 LoRA 论文中，在原始预训练语言模型（Pre - trained Language Model，简称 PLM）旁添加一条旁路，进行一次降维再升维的操作，以此来模拟所谓的内在秩（intrinsic rank）。\",\"训练的时候固定 PLM 的参数，只训练降维矩阵 A 与升维矩阵 B 。而模型的输入输出维度不变，输出时将 BA 与 PLM 的参数叠加。\",\"用随机高斯分布初始化 A ，用 0 矩阵初始化 B ，保证训练的开始此旁路矩阵依然是 0 矩阵。\",\"图6: 随机高斯分布初始化 A ，用 0 矩阵初始化 B\",\"假设要在下游任务微调一个预训练语言模型（如 GPT-3），则需要更新预训练模型参数，公式表示如下：\",\"其中， 是预训练模型初始化的参数， 就是需要更新的参数。如果是全参数微调，则它的参数量 （如果是 GPT-3，则 ）。从这可以看出要全参数微调大语言模型，代价是非常高的。\",\"而对于 LoRA 来说，只需要微调 。\",\"具体来看，假设预训练的矩阵为 ，它的更新可表示为：\",\"其中秩 。\",\"在 LoRA 的训练过程中， 是固定不变的，只有 和 是训练参数。\",\"在前向过程中， 与 都会乘以相同的输入 ，最后相加：\",\"LoRA 的这种思想有点类似于残差连接，同时使用这个旁路的更新来模拟 Full Fine-Tuning 的过程。并且，Full Fine-Tuning 可以被看作是 LoRA 的特例（当 等于 时）。\",\"在推理过程中，LoRA 也几乎未引入额外的 Inference Latency，只需要计算 即可。\",\"LoRA 与 Transformer 的结合也很简单，仅在 QKV Attention 的计算中增加一个旁路。\"]},\"426\":{\"h\":\"矩阵A和B为什么不能同时为零？\",\"t\":[\"在前面我们介绍了，用随机高斯分布初始化 ，用 0 矩阵初始化 。矩阵 为什么不也用 0 初始化？\",\"这主要是因为如果矩阵 也用 0 初始化，那么矩阵 的梯度就始终为 0，无法更新参数，导致 。这里简单推理一下。\",\"对于 ，设 ，则：\",\"因此：\",\"如果矩阵 也用 0 初始化，那么上面的梯度就变成了 0，所以矩阵 不能用 0 初始化。\",\"同样，我们看一下矩阵 初始化为 0 的影响。\",\"由于矩阵 的参数会发生更新，而 矩阵又不是 0 矩阵，因此后面 ，所以矩阵 可以用 0 初始化。\"]},\"427\":{\"h\":\"秩的选择\",\"t\":[\"论文实验结果显示，对于一般的任务， r=1,2,4,8 就足够了。而一些领域差距比较大的任务可能需要更大的 r 。\",\"同时，增加 r 值变大并不能提升微调的效果，这可能是因为参数量增加需要更多的语料。\",\"图7: 秩的选择\"]},\"428\":{\"h\":\"注意\",\"t\":[\"q进行 LoRA 高效的模型微调，重点是保持参数尺寸最小化。\",\"使用 PEFT 库来实现 LoRA，避免复杂的编码需求。\",\"将 LoRA 适应扩展到所有线性层，增强整体模型的能力。\",\"保持偏置和层归一化可训练，因为它们对模型的适应性至关重要，并且不需要低秩适应。\",\"应用量化低秩适应（Quantized LoRA，简称 QLoRA）以节省 GPU 显存并训练模型，从而能够训练更大的模型。\",\"量化是一种在深度学习领域用于减少模型内存占用和计算量的技术。在模型训练和推理过程中，神经网络的权重矩阵通常以高精度的浮点数（如 32 位浮点数）形式存储和计算，这会占用大量的内存资源并消耗较多的计算资源。量化通过将这些高精度的浮点数转换为低精度的整数（如 4 位或 8 位整数）来实现数据的压缩。\",\"在 LoRA 微调的场景中，QLoRA 就是利用量化技术的一个变体。它将权重矩阵量化为 4 位或 8 位整数，在不损失太多性能的情况下减少了模型的大小，使得模型可以在资源有限的设备上进行训练和部署，同时还能适应更多的参数。\",\"基于大模型的内在低秩特性，增加旁路矩阵来模拟全参数微调，LoRA 通过简单有效的方案来达成轻量微调的目的。它的应用自不必提，可以将现在的各种大模型通过轻量微调变成各个不同领域的专业模型。\",\"此外，考虑 OpenAI 对 GPT 模型的认知，GPT 的本质是对训练数据的有效压缩，从而发现数据内部的逻辑与联系，LoRA 的思想与之有相通之处，原模型虽大，但起核心作用的参数是低秩的，通过增加旁路，达到四两拨千斤的效果。\"]},\"429\":{\"h\":\"Prompt Engineering 知识扫盲\",\"t\":[\"Prompt Engineering 知识扫盲\"]},\"430\":{\"h\":\"什么是Prompt Engineering?\",\"t\":[\"Prompt (提示词) 是人类发给各种人工智能模型、用以完成特定任务的指令。\",\"Prompt Engineering (提示词工程) 是指我们为了让LLM能够更好地完成我们给它的任务，我们对Prompt进行优化、调整的过程。\",\"可能会有人这么问，LLM已经这么强了，直接丢给它个指令，让他去执行就好了，为什么还需要Prompt Engineering呢？\",\"确实像OpenAI的GPT4这样的LLM已经非常强了，很多简单的任务，我们直接用自然语言丢给他就去执行就好了。但是，对于一些复杂的问题，Prompt写得好不好，直接影响着大模型给出答案的正确与否。\",\"本质上，LLM是一个概率模型，它只是在给定的信息的前提下，给出概率最大的结果，它并不保证结果的合理性和正确性。\",\"要让LLM给出的结果尽可能地合理、正确，这是我们使用LLM的人的职责。\",\"这就是我们要去学习Prompt Engineering的原因。\"]},\"431\":{\"h\":\"如何写好Prompt?\"},\"432\":{\"h\":\"要明确,要具体\",\"t\":[\"我们发给LLM的批令，越明确、越具体，对于LLM越友好。\",\"举个例子，我们让LLM对一段文字进行总结：\",\"Prompt 2相比Prompt 1，对输出有了更加明确具体的要求，这样LLM输出的内容也会更加贴合我们的需求。另外，我们还用了'###'作为分隔符，进一步帮LLM明确要求。\",\"我们在给LLM发指令的时候，第一个关键点，就是我们要把给LLM做的任务尽可能细化，把要求尽可能明确、具体地描述出来。\"]},\"433\":{\"h\":\"给LLM更多的时间去思考\",\"t\":[\"《思考快与慢》这本书里介绍了我们人类大脑的“系统1”和“ 系统2”。\",\"系统1是快思考系统，反应很快，但可能会出错。\",\"系统2是慢思考系统，需要更长的反应时间，进行思考、推理，但结果会更加靠谱。\",\"默认情况下，LLM就像是一个快思考的系统，他利用自己已掌握的知识，快速给出答案，但并不能保证结果的正确性。\",\"为了让LLM给出的答案更加靠谱，我们需要通过Prompt Engineering 的方式，把LLM的慢思考调动起来。\",\"这就是“给LLM更多的时间去思考”背后的大致逻辑。\",\"给LLM更多的时间去思考，一个简单的技巧是在你的Prompt后面，加上这样一句话“Let’s think step by step”。这句话会引导LLM，会去分步骤思考，效果会比不加这句话要好。\",\"另一个技巧，在Prompt中加入一些例子，让LLM照着例子进行推理、思考。这一块的技巧性很强，我们在接下来的部分，介绍几种具体的技巧。\"]},\"434\":{\"h\":\"思维链技术：Chain-of-Thought\",\"t\":[\"这是《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》这篇论文里讲的一个Prompt Engineering的技巧。\",\"CoT(Chain-of-Thought) 的核心思想是，在Prompt中加入一些示例，来引导LLM展现出更好的推理能力。\",\"这里的关键是在Prompt中加入的示例，在这些示例中，我们会用自然语言描述一系列的推理过程，并最终引导出示例问题的正确结果。\",\"这个过程有点像，我们教小孩做应用题，我们先给小孩子分析讲解一些示例。然后再把新的问题让小孩子来解决。小孩子根据从示例中学习到的推理、分析能力，最终解出了新的问题。\",\"下面我们来看论文中给的CoT的例子：\",\"左侧是常规的Prompt，右侧是CoT Prompt\",\"蓝色标记出的部分是提供给LLM的示例。绿色标记出的部分是LLM输出的推理过程。\",\"在使用CoT这种Prompt Engineering技巧的时候，有几个注意点：\",\"CoT是LLM足够大（参数足够多，通常是在1000亿参数）时才涌现出来的能力。因此，在一些不够大的LLM上，CoT的效果并不明显。\",\"通常，在Prompt中加入的示例不是1条，而是多条。具体要考虑解决的问题类型，以及Prompt的长度（因为LLM的Prompt长度通常都是有长度限制的）。\"]},\"435\":{\"h\":\"自一致性技术：Self-Consistency\",\"t\":[\"这是《Self-Consistency Improves Chain of Thought Reasoning in Language Models》 这篇论文里讲的另一个Prompt Engineering的技巧。\",\"Self-Consistency技术是在CoT技术的基础之上，进行的进一步优化，目的是为了让LLM的推理能力能够更进一步提升。\",\"Self-Consistency的大致原理是这样：\",\"利用CoT Prompting技巧，写好Prompt；\",\"不要让LLM只生成最合适的唯一一个结果，而是利用LLM结果的多样性，生成多种不同推理路径所得的结果的集合；\",\"从结果集合中投票选择，选出投票最多的结果，做为最终的答案。\",\"这里有像我们人类解决问题的过程，如果我们用多种不同的方法去求解，大多数方法求解出来结果都一样的答案，那很有可能就是我们最终的答案。\",\"下面我们来看论文中给的Self-Consistency的例子：\",\"在上面的例子中，虚线之上是标准的CoT的过程，它得到的结果是错的。虚线之下是Self-Consistency的过程，得到的三个答案中，有1个是错的，有2个是正确的。最终答案是大多数投票的结果，是正确的。\"]},\"436\":{\"h\":\"从易至难技术：Least-to-Most\",\"t\":[\"这是《Least-to-Most Prompting Enables Complex Reasoning in Large Language Models》 这篇论文中介绍的方法。\",\"CoT的特点是同类型问题的迁移思考，因此，如果给的例子是比较简单的问题，而给的问题却是难度大很多的问题，这时候CoT的效果就不尽如人意。\",\"LtM(Least-to-Most)主是为了解决CoT这种从易到难的迁移能力不足而诞生的。\",\"LtM的核心思想是：教LLM把复杂问题，拆解成一系列的简单问题，通过解决这一系列的简单问题，来最终得到复杂问题的结果。\",\"LtM的过程包含两个阶段：\",\"分解阶段：把复杂问题分解成一系列的简单子问题。这个阶段的Prompt中要包含分解问题的示例，要和分解的问题；\",\"解决子问题阶段：这个阶段的Prompt中包含三部分内容：一是完整的LtM的例子；二是已解决的子问题及其答案列表；三是接下来要解答的子问题。\",\"这里也非常像我们人类学习解决复杂问题的过程，我们通过把复杂问题拆解成一个个的简单问题，通过把一个个的简单问题解决掉，最终把复杂问题也解决了。\",\"下面我们来看看论文中LtM的例子：\",\"从上图中，我们可以对LtM Prompting有一个直观的认知，通过引导LLM解决子问题，一步步引导LLM得出复杂问题的结果。\"]},\"437\":{\"h\":\"应用层\"},\"438\":{\"h\":\"GPT-1 论文\",\"t\":[\"GPT-1 论文\",\"论文: Improving Language Understanding by Generative Pre-Training\"]},\"439\":{\"h\":\"摘要\",\"t\":[\"自然语言理解包含了广泛的多样性任务，比如文本蕴涵，问答，语义相似度评估，文本分离。然而大规模的未标注的文本语料是丰富，而特定任务学习的标注数据有非常少，使得要充分做区分地训练模型非常有挑战性。作者证明通过在丰富的无标签文本语料库生成预训练generative pre-training语言模型，然后在每项具体任务上判别性微调discriminative fine-tuning，可以实现巨大的收益。对比之前的方法，作者在微调阶段使用任务感知的输入转换来实现有效的迁移，仅仅需要小小修改模型架构。通用的任务未知task-agnostic模型优于那些为每个任务精心设计的模型，在12个研究任务中9个提升到SOTA。如，作者在常识推理(Stories Close Test)上提升8.9%， 在问答上提升5.7%(RACE)，文本蕴含提升1.5%(MultiNLI)。\"]},\"440\":{\"h\":\"简介\",\"t\":[\"在NLP中，有效地从无标记的原始文本中学习的能力能减轻对监督学习的依赖。大部分深度学习方法需要大量人工标注的数据，这限制了它们在许多缺乏标记数据领域的适用性。在这种情况下，模型能从无标记数据中充分利用语义信息，为收集更多的标注数据提供了更多一个有价值的替代方案，标注数据昂贵又耗时。进一步来说，即便是那些有大量标注数据的场景，无监督学习得到的好的表示也能提供显著的提升。最有说服力的证据就是到目前为止大量使用预训练的词嵌入来提升一系列NLP任务表现。\",\"无论到什么程度，从无标注文本中充分利用词级别以外的信息是有挑战性的，有两个主要原因。\",\"不清楚在学习文本表示时，什么样的优化目标是最高效的迁移。近期研究考虑过各种各样的目标，如语言模型，机翻，语句连贯性，每种方法在不同任务上都优于其它方法。\",\"对于将这些学习到的表征迁移到目标任务的最有效方法，目前还没有达成共识。已有的技术涉及对模型架构进行特定任务的修改、使用复杂的学习方案以及添加辅助学习目标的组合。这些不确定性使得开发有效的语言处理半监督学习方法变得困难。\",\"在本文中，作者用无监督的预训练和监督的微调组合来探索关于语言理解任务半监督方法。目标是学习一个全局表示，迁移它来稍微适应一系列广泛的任务。作者假设采用一个大型无标记文本语料库和几个人工标记训练样本的数据集(目标任务)。该设置不需要这些目标任务和无标记语料库是一个领域的。并采用两段式训练流程。首先，在无标记数据上使用语言模型目标来学习神经网络初始化的参数。接着，使用对应特定任务的监督目标来调整这些参数。(预训练+微调)\",\"对于作者的模型架构，使用的是Transformer，它被证明在许多任务上有很强的表现，如机翻，文本生成，句法解析。该模型在文本上处理长期依赖提供了更结构化的内存，相比其他替代方案如RNN，Transformer跨各种各样任务的迁移性能更强。在迁移阶段，作者利用源于遍历式(traversal-style)方法的特定任务的输入改写，其将结构化文本输入处理为单一的连续字符序列。如作者在实验中证明的，这些改写使得在预训练模型架构上用最小的修改就会有效。\",\"作者在四种类型的语言理解任务评估作者的方法——自然语言推断NLI，问答，语义相识度，和文本分类。作者通用的任务未知task-agnostic模型优于那些为每个任务精心设计的模型，在12个研究任务中9个提升到SOTA。\",\"常识推理提升8.9%(Stories Cloze Test)\",\"问答提升5.6%(RACE)\",\"文本蕴含提升5.5%(MultiNLI)\",\"GLUE多任务提升5.5%.\",\"也分析了在四种不同设置下预训练模型的零次(zero-shot)表现，证明其确实为下游任务获取到了有用的语言知识。\"]},\"441\":{\"h\":\"相关工作\",\"t\":[\"NLP半监督学习: 预训练对于获取不同级别信息的需要，如从词级别信息到更高的(段落级别或者句子级别的)词嵌入。\",\"无监督预训练: 无监督预训练+监督微调方式，Transformer比LSTM能获取长距离信息。\",\"辅助训练目标: 添加一个无监督训练目标是半监督学习的一种替代形式。如POS tag，语义组块chuking, NER， 以及语言模型来提升标记的语义角色。\"]},\"442\":{\"h\":\"框架\",\"t\":[\"作者训练流程有两个阶段:\",\"在大规模文本语料上学习高容量的语言模型\",\"微调阶段，用标记的数据对特定任务微调模型\"]},\"443\":{\"h\":\"无监督预训练\",\"t\":[\"给定一个无监督学习的语料tokens ，使用标准的语言模型目标并最大化其似然：\",\"这里 是上下文窗口大小，条件概率 是参数 的神经网络模型。这些参数会以随机梯度下降训练。\",\"在作者的实验中，语言模型使用多层的 Transformer decoder（Transformer 的变种 ）。该模型在上下文 token 上使用多头自注意力操作，接一个逐位置的前馈层来生成目标字符的分布输出（比原本少了一个多头自注意力 ）：\",\"公式如下：\",\"这里 是上下文字符向量， 是层数， 是字符嵌入矩阵， 是位置嵌入矩阵。\"]},\"444\":{\"h\":\"有监督微调\",\"t\":[\"在训练公式 （ 1 ） 中的目标函数模型后，作者在监督学习目标任务上调整参数。假设有标记数据集 ，每个实例有输入字符的序列构成 ，对应着标签 。输入通过作者的预训练模型会得到最好的 transformer block 的激活状态 ，将其喂进一个参数为 的添加的线性输出层来预测 有：\",\"给出最大化的目标函数为：\",\"作者还发现加入语言模型作为辅助目标来微调有助于学习：(a) 提升监督模型的泛化能力；(b) 加速收敛。这跟之前的工作一样，观测发现用辅助目标能提升性能。尤其是，作者用以下优化（加权 ）目标：\",\"总之，作者额外要微调的参数只有 ，以及分割字符嵌入矩阵。\"]},\"445\":{\"h\":\"特定任务输入转换\",\"t\":[\"文本分类：直接微调模型\",\"问答或文本蕴含：输入是结构化的，如有序句子对，三元组（文档，问题和答案）\",\"因为作者的预训练模型是用连续的文本序列训练的，需要做些修改以便用在这些任务上。之前的工作提出了在迁移表征顶部学习特定任务的架构。这种方法重新引入了大量特定任务的定制化输入，并且不会对额外的架构组件使用迁移学习。相反，作者使用遍历式方法，就是将结构化输入转换为有序序列以便作者预训练能处理。这些输入转换使作者避免跨任务架构的大改。作者在下面部分和可视化插图 1 提供了这些输入的简洁描述。所有的转换包括添加随机初初始化的开始和结束标记 。\",\"文本蕴含：拼接前提文本 和假设 为 token 序列，用 $ 符来分隔两者。\",\"相似度：对于相似任务，两个比较的句子没有内在顺序。为了反映这点，作者修改输入序列来包含 2 种可能的顺序（用分隔符分隔），并独立地处理 2 个序列表示 ，逐元素相加然后送入线性输出层。\",\"问答和常识推理：对于这些任务，给定文档 ，一个问题 和一个可能答案集 。将文档和问题跟每个可能答案拼接起来，再在其中添加一个分隔符得到 。每个这些序列用作者的模型独立处理后通过一个 softmax 层归一化来生成所有可能答案的分布。\"]},\"446\":{\"h\":\"实验\"},\"447\":{\"h\":\"设置\",\"t\":[\"无监督预训练：BOOKS CORPUS 数据集预训练模型。长文本能让生成模型学习到长依赖信息的条件概率。ELMO 方法处理 1B Word benchmarks，在句子级别打乱顺序以破坏长距离结构信息，达到非常低的 18.4 困惑度。\",\"模型的具体配置:\",\"Transformer 架构：12 层有自注意力头（768 维隐藏层，12 个注意力头）transformer decoder 结构。\",\"逐位置前馈神经网络（position-wise feed-forward networks）：3072 维内部隐藏层。\",\"Adam 优化器方案：最大 lr=2.5e-4。开始 2000 次从 0 线性上升更新，再使用 cosine 方案退火到 0。\",\"采样与训练：从 512 连续 tokens 中随机采样得到 64 小批次样本，训练 100 轮。\",\"层归一化：改进版的 layerNorm，以 权重初始化。\",\"词汇表与正则化：40,000 合并的 BPE 词汇表，残差，嵌入和注意力层以 0.1 的 Dropout 来正则化。\",\"改进版 L2 正则：所有无偏差或增益权重设置为 。\",\"激活函数：GELU 作为激活函数。\",\"位置嵌入：使用学习的位置嵌入，而不是原始 Transformer 的正余弦曲线。\",\"数据清洗与分词：使用 ftfy 清洗原始 BooksCorpus，去掉字符和空格，再使用 spaCy tokenize。\",\"微调的细节:\",\"除非指定，使用无监督预训练超参数设置。分类层使用 0.1 的 Dropout。大部分任务，lr=6.25e-5，批大小为 32。在大部分任务中基本上 3 轮训练就足够了。lr 用以训练步数的 0.2% 预热衰减方案。 设置为 0.5。\"]},\"448\":{\"h\":\"监督微调\",\"t\":[\"微调任务和数据集如下：\",\"NLI 就是识别文本蕴含。涉及读取一对句子，判断它们之间的关系，是蕴含，矛盾或中立。因为存在各种变化现象，如词汇蕴含，共指，词汇和句法歧义，所以还是很有挑战性的。\",\"下表2是作者模型和之前SOTA模型NLI的结果比较：\",\"RTE数据集比较小，只有2490样本，只达到了56.0%准确率。\",\"问答和常识推理 结果如下表3，RACE数据集由初高中考试题构成。在Story Cloze和RACE提升明显。证明模型具有有效处理上下文长距离的信息的能力。\",\"语义相似度 语义相似度(或释义发现)任务涉及预测两个句子在语义上是否相等。挑战在于识别语句是否是概念改写，理解反面，处理句法歧义。使用的数据集：\",\"MRPC Microsoft Research Paraphrase corpus 是一些句子对，有的是同义的，有的是不同义的。\",\"QQP Quora Question Pairs 美国知识问答网站 Quora 上的问题答案数据集\",\"STS-B Semantic Textual similarity benchmark 语义文本相似度数据集，样本为文本对，评判两个文本语义信息的相似度，分数为1-5。\",\"在STS-B上有1个点的绝对提升，比Single-task BiLSTM + ELMo + Attn有4.2%的绝对提升。\",\"分类 两个不同分类任务的评估结果，也在上表4中。CoLA——Corpus of Linguistic Acceptability语言可接受性语料库，纽约大学发布的有关语法的数据集，该任务主要是对一个给定句子，判定其是否语法正确，因此CoLA属于单个句子的文本二分类任务。\",\"SST-2——The Stanford Sentiment Treebank, 主要针对电影评论来做情感分类，因此SST属于单个句子的文本分类任务（其中SST-2是二分类，SST-5是五分类，SST-5的情感极性区分的更细致)。\",\"CoLA上取得45.4，SST-2取得91.3的准确率，整体得分72.8。\",\"总体而言，在12个数据集上的9个取得SOTA结果，比许多情况下的ensemble模型要好。而且能适应不同大小数据集。\"]},\"449\":{\"h\":\"分析\",\"t\":[\"层数的迁移学习影响: 从预训练到微调迁移学习过程中，如下表2，在MultiNLI和RACE上的性能随着层数的变化而变化。作者观察标准结果，在MultiNLI上转移embedding能提升结果，每一层Transformer层能带来9%额外的提升。这表明预训练模型中的每一层都包含了解决目标问题有用的功能。\",\"零样本表现 最好要弄清楚为什么预训练模型会有效？一种假设是，与LSTMs相比，潜在生成式模型（underlying generative model）在应用到很多任务时可以提高语言建模的能力并且transformer更具结构化的注意力记忆（attentional memory）有助于迁移。在零样本上，LSTM表现高方差，表明在迁移中，Transformer架构导入偏差是有帮助的。\",\"对于CoLA（语言可接受性），样本的得分是用生成模型分配的tokens的平均对数概率，在阈值下进行预测的。\",\"对于SST-2(情感分析)，给每个实例样本加一个 very token,来限制语言模型的输出分布只有 positive和 nagative, 就是猜测被分配到高的概率值的token作为预测值。\",\"对于RACE(问答)，在文档和问题给定条件下，将生成模型分配的平均对数概率高的token作为答案。\",\"对于DPRD(威诺格拉德模式), 用两个可能的替换说法来代替定义的代词，在这之后用生成模型分配的剩下序列的平均token对数概率高的作为结果。\",\"消融研究 不同的消融研究如下表5.\",\"首先，作者在微调时用辅助的LM目标来检查作者模型的性能。在NLI和QQP任务上辅助LM目标有帮助。总之，就是大数据集有效，小数据集没有。\",\"其次，分析比较2048单元的单层LSTM和Transformer，二者都加同样的辅助LM，LSTM会掉5.6平均分数。\",\"最后，直接在监督学习任务上训练，不要预训练，这会导致在跨任务上性能降低14.8%.\"]},\"450\":{\"h\":\"结论\",\"t\":[\"作者介绍了一种框架，用任务未知的生成式预训练模型和判别式微调在自然语言理解上取得了很强的效果。通过在长篇连续文本的多样化语料库上预训练，作者模型获得了重要的世界知识和处理长距离依赖的能力，然后能成功迁移学习解决判别式任务，如问答，语义相似度评估，蕴含确定和文本分类，在12个的9个数据集取得了SOTA结果。\",\"使用无监督预训练来提升在判别式任务上的表现是机器学习研究的长期目标。作者的工作表明，实现显著的性能提升确实是可能的，并给出了提示Transformer类模型和长距离依赖的文本数据集最好用这种方法来训练。\"]},\"451\":{\"h\":\"GPT-2 论文\",\"t\":[\"GPT-2 论文\",\"论文链接: Language Models are Unsupervised Multitask Learners\"]},\"452\":{\"h\":\"摘要\",\"t\":[\"这篇论文《Language Models are Unsupervised Multitask Learners》由OpenAI团队提出，介绍了GPT-2模型，展示了大规模语言模型在无监督多任务学习中的潜力。GPT-2通过训练一个包含45百万网页链接的WebText数据集，能够在零样本（zero-shot）设置下完成多种自然语言处理任务，如问答、翻译、摘要和阅读理解等，无需任务特定的监督训练。研究发现，模型容量对任务性能至关重要，更大的模型在多个基准测试中达到了最先进水平。论文还探讨了模型泛化与记忆的关系，并指出GPT-2在生成连贯文本方面的能力，为构建通用语言处理系统提供了新思路。\"]},\"453\":{\"h\":\"简介\",\"t\":[\"当前机器学习系统的局限性\",\"当前的机器学习系统虽然在特定任务上表现出色，但依赖于大量标注数据和监督学习，导致其泛化能力有限。这些系统往往对数据分布或任务定义的微小变化非常敏感，表现为“狭窄的专家”而非通用的多任务处理者。作者指出，这种局限性部分源于单任务、单领域的数据集训练模式，限制了模型在多样化场景中的应用能力。\",\"多任务学习的挑战与机遇\",\"多任务学习（Multitask Learning）被视为提升模型通用性的潜在途径，但其在自然语言处理（NLP）领域的进展仍处于早期阶段。现有研究通常仅联合训练少量任务（如10-17个任务），而机器学习系统通常需要数百至数千个任务示例才能实现良好的泛化。作者认为，单纯依赖人工标注和设计任务目标难以满足多任务学习的规模化需求，因此需要探索更高效的学习范式。\",\"预训练与迁移学习的趋势\",\"近年来，预训练结合监督微调的方法在NLP任务中表现突出。从早期的词向量（如Word2Vec）到上下文感知的循环神经网络（如ELMo），再到基于自注意力机制的Transformer架构（如BERT、GPT），模型的迁移能力逐渐增强。然而，这些方法仍依赖监督数据。作者提出，语言模型本身可能通过无监督学习捕捉任务相关的知识，从而减少对显式监督的依赖。\",\"论文的核心假设与目标\",\"本文的核心假设是：足够大的语言模型在多样化文本训练下，能够通过预测任务的自然语言描述（如问答、翻译的文本示例）间接学习任务，而无需参数调整或架构修改。作者通过实验验证这一假设，证明GPT-2在零样本设置下能完成多种任务，部分任务性能接近或超越监督基线模型。这一发现为构建通用语言系统提供了新方向，同时揭示了模型容量与任务性能之间的紧密关联。\",\"研究意义\",\"论文强调，无监督任务学习是预训练技术成功的关键因素之一。尽管零样本性能尚不完美，但结果表明语言模型在无监督条件下已具备初步的多任务处理能力，为未来探索更通用的AI系统奠定了基础。\"]},\"454\":{\"h\":\"方法\",\"t\":[\"1. 语言建模的核心框架\",\"论文的核心方法是基于语言建模（Language Modeling, LM），即通过无监督学习估计文本序列的概率分布。给定一个符号序列 ((s_1, s_2, ..., s_n))，语言模型通过链式法则计算联合概率：\",\"这一框架允许模型不仅生成文本，还能计算任意条件概率，例如预测缺失的单词或句子。近年来，Transformer 架构（Vaswani et al., 2017）的引入显著提升了语言模型的表达能力，使其能够建模长距离依赖关系。\",\"2. 多任务学习的概率视角\",\"传统监督学习通常建模 ，而通用系统需要能够根据任务描述动态调整行为，即建模 。作者指出，McCann et al. (2018) 的MQAN（Multi-task Question Answering Network） 已经证明，任务可以通过自然语言描述（如“translate to French, English text, French text”）来指定。本文进一步假设，语言模型本身可以通过观察任务的自然语言演示（如问答对、翻译示例）来隐式学习任务，而无需显式监督。\",\"3. 训练数据集：WebText\",\"为了训练一个能够泛化到多种任务的语言模型，论文构建了一个新的数据集 WebText，其关键特点是：\",\"数据来源：从 Reddit 上爬取高赞（≥3 karma）的外链网页，确保内容经过人工筛选，质量高于 Common Crawl 等原始网络数据。\",\"规模与处理：初步版本包含约800万篇文档（40GB文本），去重并移除了 Wikipedia 数据以避免测试集污染。\",\"多样性目标：涵盖广泛的主题和写作风格，以增加模型接触不同任务（如翻译、摘要）自然演示的机会。\",\"4. 输入表示：改进的字节对编码（BPE）\",\"传统语言模型通常依赖单词或字符级输入，但存在词汇表限制或效率问题。本文采用字节级 BPE（Byte Pair Encoding），其优势包括：\",\"词汇表灵活性：基础词汇仅需256个字节，可表示任意 Unicode 字符串，避免传统 BPE 对 Unicode 编码的冗余扩展（如130,000+基词）。\",\"改进的合并策略：防止跨字符类别的合并（如“dog”与“dog!”被分开），减少词汇碎片化，同时允许空格合并以提高压缩效率。\",\"兼容性：支持对任何文本（无论预处理方式）直接计算概率，便于跨数据集评估。\",\"5. 模型架构：GPT-2 的改进\",\"GPT-2 基于 Transformer 架构，延续了 GPT-1（Radford et al., 2018）的设计，但进行了以下优化：\",\"层归一化调整：移至每个子模块的输入（类似预激活残差网络），并在最终自注意力块后增加额外层归一化。\",\"初始化优化：残差层权重按 缩放（ 为残差层数），缓解深层网络的梯度问题。\",\"扩展配置：词汇表增至50,257，上下文窗口从512扩展到1024 tokens，批大小提升至512。\",\"6. 实验设置与模型规模\",\"论文训练了 4种不同规模的模型（参数从117M到1.5B），以研究模型容量对性能的影响：\",\"最小模型（117M）与原始 GPT 相当，中等模型（345M）匹配 BERT-Large，最大模型 GPT-2（1.5B） 参数量远超 GPT-1。\",\"所有模型在 WebText 上仍表现欠拟合（held-out perplexity 持续下降），表明进一步扩大数据或模型可能提升性能。\",\"7. 任务执行的零样本机制\",\"GPT-2 的零样本能力依赖于任务提示（Task Prompting），即通过自然语言描述或示例引导模型生成目标输出。例如：\",\"翻译任务：输入“english sentence = french sentence”示例后，模型在“english sentence =”提示下生成翻译。\",\"摘要任务：在文章末尾添加“TL;DR:”触发摘要生成。\",\"问答任务：输入文档+对话历史+“A:”引导答案生成。\",\"这种方法无需微调，完全依赖语言模型对任务上下文的理解能力。\",\"总结\",\"无监督多任务学习的可行性证明：语言模型通过预测多样化文本中的任务演示（如翻译对、问答），隐式学习任务逻辑。\",\"数据与架构创新：WebText 的高质量数据、字节级 BPE 的通用性，以及 GPT-2 的规模化改进，共同支撑了零样本泛化能力。\",\"任务提示的关键作用：自然语言指令可作为隐式任务描述，激活模型的相关能力。\",\"这些设计使 GPT-2 成为首个在零样本设置下接近监督模型性能的大规模语言模型，为后续研究（如 GPT-3 的少样本学习）奠定了基础。\"]},\"455\":{\"h\":\"实验\",\"t\":[\"1. 实验设计与模型配置\",\"论文系统评估了不同规模的GPT-2模型（117M、345M、762M和1.5B参数）在多个NLP任务上的零样本（zero-shot）性能。所有模型均采用相同的架构，但层数、隐藏层维度和参数量不同（见表2）。实验的关键发现是：模型容量与任务性能呈对数线性关系，更大的模型在几乎所有任务上都显著优于小模型。作者特别指出，即使是最大的1.5B模型，在WebText验证集上仍未完全收敛（underfitting），表明进一步扩大模型和数据规模可能带来额外提升。\",\"2. 语言建模任务评估\",\"GPT-2在8个标准语言建模数据集上进行了测试，包括Penn Treebank（PTB）、WikiText-2、LAMBADA等。结果显示：\",\"跨领域泛化能力：GPT-2在7/8的数据集上刷新了零样本SOTA，特别是在小数据集（如PTB）和长程依赖任务（如LAMBADA）上提升显著。\",\"预处理的影响：由于GPT-2使用字节级BPE，可以避免传统tokenization的损失。通过可逆去token化（invertible detokenizers）处理，GPT-2的困惑度（perplexity）进一步降低了2.5-5点。\",\"例外情况：在One Billion Word Benchmark（1BW）上表现较差，作者归因于该数据集的句子级打乱破坏了长程依赖。\",\"3. 具体任务表现分析\",\"3.1 Children's Book Test（CBT）\",\"CBT测试模型对不同词类（命名实体、名词等）的预测能力。GPT-2在验证集（避免与WebText重叠）上达到：\",\"常见名词准确率93.3%（原SOTA 85.7%）\",\"命名实体准确率89.1%（原SOTA 82.3%）\",\"分析表明，模型容量增加直接缩小了与人类表现的差距（图2）。\",\"3.2 LAMBADA\",\"该任务要求预测句子的最后一个词，需要至少50个token的上下文理解。GPT-2将：\",\"困惑度从99.8降至8.6\",\"准确率从19%提升至52.66%\",\"通过添加停用词过滤器（避免生成非结尾词），准确率进一步提升至63.24%，超过此前需依赖上下文词约束的SOTA方法。\",\"3.3 Winograd Schema Challenge\",\"测试常识推理能力（共273个样例）。GPT-2以70.7%的准确率超越前SOTA（Trinh & Le, 2018）7个百分点。作者指出，尽管数据集小，但结果与人类表现（约95%）的差距已显著缩小。\",\"3.4 阅读理解（CoQA）\",\"在对话式问答数据集上，GPT-2仅通过文档+历史对话+“A:”提示生成答案，达到55 F1：\",\"匹配或超过3/4监督基线的性能\",\"但远低于人类（89 F1）和BERT SOTA\",\"错误分析显示，模型倾向于使用简单的检索启发式（如用文档中的人名回答\\\"who\\\"问题）。\",\"3.5 摘要生成（CNN/Daily Mail）\",\"通过“TL;DR:”提示+Top-k采样（k=2）生成摘要：\",\"ROUGE分数仅略高于随机选句基线\",\"但定性分析显示生成内容类似摘要, 移除提示后性能下降6.4点，证明自然语言指令可激活任务特定行为。\",\"3.6 翻译（WMT-14）\",\"尽管WebText几乎无平行语料，GPT-2通过示例提示：\",\"英→法：5 BLEU（低于词对齐基线）\",\"法→英：11.5 BLEU（超过部分无监督方法）\",\"作者推测英语语言模型的强大概率补偿了翻译知识的不足。\",\"3.7 问答（Natural Questions）\",\"在事实型问答测试中：\",\"精确匹配准确率4.1%（远低于监督系统30-50%）\",\"但对高置信度预测（top 1%），准确率达63.1%\",\"表明模型容量是限制因素（最小模型仅1.0%准确率）。\",\"4. 记忆与泛化分析\",\"4.1 数据重叠检测\",\"使用Bloom过滤器统计测试集与WebText的8-gram重叠率：\",\"平均重叠率3.2%（与常规训练-测试重叠率5.9%相比更低）\",\"极端案例：WikiText-103测试集1.6%重叠（因文章复用段落）\",\"对性能影响：LAMBADA去除重叠样本后，困惑度仅从8.6→8.7\",\"4.2 训练集与测试集性能对比\",\"图4显示，WebText训练集和测试集的困惑度同步下降，表明：\",\"即使1.5B模型仍欠拟合\",\"性能提升非源于记忆，而是真实泛化能力\",\"4.3 生成样本分析\",\"表13展示GPT-2在非分布数据（如“独角兽新闻”）上的生成能力：\",\"能生成连贯但虚构的内容\",\"证实模型并非简单记忆，而是组合学到的知识\",\"5. 实验结论\",\"规模定律：模型容量与零样本性能强相关，1.5B参数模型在多数任务上逼近或超越监督基线。\",\"任务通用性：单一语言模型可处理翻译、问答、摘要等多样化任务，仅需自然语言提示。\",\"数据质量：WebText的多样性和规模是关键，但数据重叠对结果影响有限。\",\"局限性：摘要、翻译等任务表现仍远逊于专业系统，显示无监督学习的当前边界。\",\"这些实验为后续研究（如GPT-3的少样本学习）提供了重要基准，证明无监督预训练在多任务迁移中的巨大潜力。\"]},\"456\":{\"h\":\"讨论\",\"t\":[\"无监督任务学习作为预训练技术成功的关键因素。研究表明，当语言模型在足够多样化的文本数据上训练时，能够通过预测任务的自然语言演示（如翻译对、问答示例）隐式学习任务逻辑，而无需显式监督。这一发现为理解当前预训练模型的有效性提供了新视角，并表明在极限情况下，语言模型可能直接学会执行任务。\",\"作者同时指出GPT-2的局限性：虽然在阅读理解等任务上接近监督基线，但在摘要、翻译等任务上的表现仍远未达到实用水平。这种性能差异揭示了当前方法的边界，表明模型容量和训练数据规模仍需进一步扩大。特别值得注意的是，GPT-2的完全抽象式输出（如问答时生成而非抽取答案）与传统指针网络方法形成鲜明对比，这为未来探索更灵活的文本生成方式提供了启示。\",\"未来研究方向，包括探索GPT-2的微调潜力（如在GLUE等基准上的表现），以及研究双向表示（如BERT）与单向语言模型的互补性。这些发现为后续GPT-3等更大规模模型的开发奠定了基础，推动学界重新思考语言模型在多任务学习中的角色。\"]},\"457\":{\"h\":\"总结\",\"t\":[\"本文通过GPT-2模型证明了大规模语言模型在无监督多任务学习中的强大潜力。当模型在足够大且多样化的文本数据（WebText）上训练时，仅通过语言建模目标就能在零样本设置下完成多种NLP任务，并在7/8的语言建模基准上达到SOTA水平。这一发现表明，高容量模型通过最大化文本序列的似然估计，可以自发地学习执行任务，而无需明确的监督信号。\",\"研究结果对构建通用语言系统具有重要意义：首先，它验证了单一模型架构通过规模扩展即可实现多任务处理的可能性；其次，展示了自然语言本身作为任务描述符的有效性。尽管当前零样本性能仍有限，但这一方向为减少对人工标注数据的依赖提供了新思路。\",\"最后，论文指出这仅是通向更通用AI系统的初步探索。作者开放了模型代码和小型预训练模型，鼓励后续研究继续探索更大规模语言模型的行为边界，以及如何更好地利用其隐含学习到的多任务能力。这项工作为后续GPT系列模型的发展奠定了理论基础和方法框架。\"]},\"458\":{\"h\":\"GPT-3 论文\",\"t\":[\"GPT-3 论文\",\"论文链接: Language Models are Few-Shot Learners\"]},\"459\":{\"h\":\"摘要\",\"t\":[\"这篇论文介绍了GPT-3，一个具有1750亿参数的自回归语言模型，通过大规模训练显著提升了少样本学习能力。GPT-3在多种自然语言处理任务中表现出色，包括翻译、问答和文本生成等，甚至在零样本和单样本设置下也能取得有竞争力的结果。研究还探讨了数据污染问题、模型局限性及其社会影响，如偏见和能源消耗。实验表明，模型规模的扩大带来了性能的持续提升，但某些任务仍存在挑战。论文强调了GPT-3在通用语言系统发展中的潜力及其可能带来的广泛社会影响。\"]},\"460\":{\"h\":\"简介\",\"t\":[\"1. 背景与动机\",\"近年来，自然语言处理（NLP）领域逐渐转向预训练语言模型，并采用更灵活的任务无关（task-agnostic）方法进行下游迁移学习。早期的模型（如词向量、RNN）依赖任务特定的架构，而现代模型（如Transformer）可直接微调，无需额外架构调整。然而，现有方法仍需要针对每个任务进行大规模监督数据微调，这限制了模型的广泛应用。相比之下，人类仅需少量示例或简单指令即可完成新任务，因此论文探索如何让语言模型具备类似的少样本学习能力。\",\"2. 现有方法的局限性\",\"当前基于微调的方法存在三个主要问题：\",\"数据需求高：每个任务需要数千至数十万标注样本，难以覆盖广泛的语言任务。\",\"泛化能力有限：模型容易过拟合训练数据的虚假相关性（spurious correlations），导致在分布外数据上表现不佳。\",\"与人类学习方式不匹配：人类可通过少量示例或自然语言指令快速适应新任务，而现有模型难以实现类似能力。\",\"3. 元学习与上下文学习的潜力\",\"论文提出通过元学习 (meta-learning) 提升模型的少样本学习能力，即在预训练阶段让模型隐式学习多种技能，并在推理时通过上下文（in-context learning）快速适应新任务。此前的研究（如GPT-2）已初步验证了上下文学习的可行性，但性能远低于微调方法。论文假设，模型规模的扩大可能显著提升上下文学习能力，因为更大容量的模型能吸收更多任务相关的模式。\",\"4. GPT-3 的目标与贡献\",\"论文训练了GPT-3（1750亿参数），比此前最大的非稀疏语言模型大10倍，并系统评估其在零样本（zero-shot）、单样本（one-shot）和少样本（few-shot）设置下的表现。实验覆盖了翻译、问答、常识推理等多样化任务，结果显示：\",\"在少样本设置下，GPT-3 接近或超越部分任务的微调模型性能。\",\"模型规模与少样本学习能力呈正相关，表明缩放定律（scaling laws）在此类任务中依然适用。\",\"同时，论文也分析了模型在自然语言推理（NLI）等任务上的局限性，并探讨了数据污染和社会影响等问题。\",\"5. 研究意义\",\"GPT-3 的成果表明，超大规模语言模型可以显著减少对任务特定数据的需求，推动更通用、灵活的语言系统发展。然而，其局限性（如计算成本、偏见问题）也提示了未来改进方向，如结合双向架构或多模态训练。论文最终强调，这一研究为探索语言模型的元学习机制和实际应用奠定了基础。\"]},\"461\":{\"h\":\"方法\",\"t\":[\"1. 四种任务设定方法的比较\",\"作者首先定义了语言模型执行任务的四种方式：\",\"Fine-tuning（微调）：在任务特定数据集上更新模型权重，通常需要数千到数十万个标注样本，性能最佳，但泛化能力弱，且每个任务都需新数据。\",\"Few-shot learning（少样本学习）：在推理时为模型提供10-100个任务示例作为上下文，无需参数更新，显著减少数据需求。\",\"One-shot learning（单样本学习）：提供一条示例和任务描述，有时更贴近人类学习习惯。\",\"Zero-shot learning（零样本学习）：仅提供任务描述，不给任何示例，是最具挑战也最通用的形式。\",\"如图 2.1 所示（Figure 2.1），这些方法在数据需求和任务适应能力之间形成一个光谱，GPT-3主要研究后三种方法，强调它们在无需微调的情况下就能取得良好效果，尤其是few-shot设定下的表现令人惊喜。\",\"2. 模型架构与规模设计\",\"GPT-3模型架构基本沿用GPT-2，包括预归一化、可逆tokenizer等设计，但采用稀疏注意力机制（Sparse Transformer）以提升效率。作者训练了从125M到175B参数的8个模型（见表 2.1），以研究性能与规模之间的关系。所有模型共享最大上下文窗口为2048 tokens。模型训练过程中采用混合并行策略以适应大规模参数训练。\",\"3. 数据集构成与过滤策略\",\"GPT-3的训练数据主要来自以下五个来源（见表 2.2）：\",\"Common Crawl（经过过滤，占比60%）\",\"WebText2、Books1、Books2、Wikipedia（合计40%）\",\"为保证数据质量，作者对Common Crawl执行了质量过滤和模糊去重，并引入高质量参考语料。重要的是，数据在训练中并非按体量采样，而是按质量设权重采样，高质量数据被重复使用，而Common Crawl这类数据在整个训练中只被读取一次左右。\",\"4. 训练过程与资源分配\",\"大模型使用较大的batch size和较小的学习率（详见表 2.1）。训练依赖微软提供的高带宽GPU集群，采用模型层间和矩阵级别的并行方式进行。所有模型都使用3000亿tokens进行训练，训练策略遵循了《Scaling Laws for Neural Language Models》一文的建议。\",\"如图 2.2 所示（Figure 2.2），GPT-3虽然模型更大，但实际训练所需的计算资源与较小模型相当，这得益于更高效的数据利用率。\",\"5. 评估方法与设定\",\"在few-shot设定下，模型的每个测试样本前会插入K条示例（K通常为10-100，取决于是否能容纳在2048 token窗口中）。对于没有训练集的数据集，示例从开发集提取；对于多选题，GPT-3比较不同答案的语言模型概率（归一化处理）；对于生成类任务，则使用beam search输出，并按F1、BLEU或精确匹配评估。最终结果在公开测试集或开发集上报告。\",\"总结\",\"GPT-3的研究方法基于“任务不可知”的设定，通过大规模预训练和精心设计的上下文输入，在不进行梯度更新的前提下实现任务适应。这种“以上下文为接口”的元学习方法，加上参数规模的扩展，使得GPT-3在多个任务上展现出超越以往fine-tuned方法的能力，为未来通用语言智能系统奠定了基础。\"]},\"462\":{\"h\":\"结果\",\"t\":[\"1. 模型性能随规模扩展而持续提升\",\"作者首先展示了8个不同规模的模型在训练过程中的表现，发现无论是在训练损失还是实际任务中的表现，都随着模型参数的增长而呈现平滑的幂律提升趋势（见图 3.1 和图 3.3）。这表明大模型能够更好地吸收语言知识和上下文信息。\",\"2. GPT-3在语言建模和完形填空任务中的表现\",\"GPT-3在传统语言建模任务（如PTB）中零样本设定下创下新SOTA（PPL 20.5），远优于此前结果（PPL 35.8）。在LAMBADA数据集上，few-shot设置下准确率达到86.4%，比原SOTA高出18%（见图 3.2）。此外，在StoryCloze和HellaSwag等故事完形任务中，GPT-3也表现出明显的few-shot优势。\",\"3. 在封闭式问答任务中接近甚至超越SOTA\",\"GPT-3在TriviaQA、WebQuestions 和 Natural Questions这三个问答任务中，在没有使用外部检索信息（closed-book）或微调的前提下，仅通过few-shot设定就达到了与微调SOTA模型相当甚至更优的水平。尤其在TriviaQA中，few-shot得分达到71.2%，超越了一些基于检索系统的模型（如RAG）。\",\"4. 多语言翻译能力显著提升\",\"尽管训练数据中非英语文本仅占7%，GPT-3在英法、英德、英罗等语言对的few-shot翻译任务中，已超越多项无监督NMT方法的表现（见表 3.4 和图 3.4）。尤其在翻译为英语的方向上，GPT-3展现出更强的语言建模优势。\",\"5. 常识推理与Winograd类任务\",\"GPT-3在Winograd Schema Challenge中零样本即可取得88.3%的准确率，接近人类水平，且在更具挑战性的Winogrande数据集上few-shot得分达到77.7%，逼近fine-tuned大型模型表现（见图 3.5）。但对于如WiC（语义一致性）任务，GPT-3表现较差（仅为49.4%），显示在一些语义比较任务上仍存在明显短板。\",\"6. 阅读理解与逻辑推理任务表现不一\",\"在阅读理解任务中（如CoQA、DROP、QuAC、SQuADv2），GPT-3在few-shot设定下表现优异，尤其在CoQA中few-shot得分（85.0 F1）仅比人类低几分（见图 3.7）。但在结构化或需要多步推理的任务中（如DROP、RACE、QuAC），表现则不及微调模型，显示GPT-3对复杂语义结构的掌握仍有提升空间。\",\"7. SuperGLUE整体表现良好，但有短板\",\"在SuperGLUE基准测试中，GPT-3在少样本（32个示例）设定下，在BoolQ、ReCoRD等任务上表现接近SOTA，在COPA任务中仅落后1-2分。但在如WiC、CB、MultiRC等任务上显著低于fine-tuned模型（见表 3.8 和图 3.8）。这说明GPT-3在识别细粒度语义差异上仍有明显不足。\",\"8. NLI和Adversarial推理任务仍具挑战性\",\"在自然语言推理任务（如RTE和ANLI）中，即使是GPT-3 175B也只能在few-shot设定下稍高于随机水平（约33%），表现远不如fine-tuned模型（见图 3.9）。尤其在ANLI这种对抗性构建的数据集上，GPT-3展示了推理能力的不足。\",\"9. 在合成任务和灵活性测试中展现强大泛化能力\",\"GPT-3在设计的算术、字母重排、新词使用、语法纠错等任务中，只需提供极少量的示例就能成功完成，这表明其具有一定程度的推理和快速适应能力。这些任务测试了GPT-3的few-shot元学习能力，显示其对“任务模式”的提取并非依赖微调。\",\"总结\",\"GPT-3在多数NLP任务中，在zero-, one-, few-shot设定下均展示了强大的任务适应能力，尤其在few-shot情境下，其表现多次逼近甚至超越传统fine-tuned模型。与此同时，一些任务（如对抗性推理、语义比较等）仍暴露出其推理深度与语言理解的局限，提示未来需在结构理解与逻辑泛化方面进一步改进。\"]},\"463\":{\"h\":\"局限性\",\"t\":[\"1. GPT-3 并非通用智能：能力分布不均\",\"尽管GPT-3在多个任务上取得了令人印象深刻的成绩，但作者明确指出，它并不是一个通用智能系统，其表现呈现出高度任务依赖性：在某些任务中可与SOTA模型媲美，但在其他任务（如自然语言推理、逻辑比较）中则表现平庸甚至接近随机。\",\"这种“选择性优势”意味着GPT-3更像是一个巨大的“模式匹配引擎”，而非真正理解语言和任务的系统。\",\"2. 缺乏鲁棒的系统性泛化能力\",\"GPT-3的few-shot能力主要依赖于识别任务格式和输出模式，而不是进行真正意义上的概念抽象和泛化。作者指出，目前尚不清楚模型在推理任务中是否“学会”了新知识，还是只是记住了相似的训练样本。这种 泛化机制的模糊性 是目前元学习方法的一个重要限制。\",\"3. Prompt依赖性强，输入微小变动影响大\",\"GPT-3对提示（prompt）形式和内容高度敏感。不同的措辞、问题格式甚至换行方式都可能造成性能大幅波动。\",\"这意味着few-shot效果难以稳定复现，缺乏可控性与鲁棒性，在实际部署中可能导致意外错误。\",\"4. 上下文窗口限制性能提升\",\"尽管GPT-3的上下文窗口扩大到2048 tokens，相比前代模型大幅提升，但这仍然限制了few-shot学习中可用的示例数量（尤其是在长文本任务中）。作者认为，有限上下文容量成为当前few-shot学习的“瓶颈”。\",\"5. 无法利用结构化监督信号\",\"GPT-3完全不依赖梯度更新，因此无法像微调方法那样从结构化监督中持续优化。在特定任务上（如NER、结构化问答、程序生成等），GPT-3的表现明显弱于专门微调过的模型。这表明它在需要长期优化和知识整合的任务中仍有较大局限。\",\"6. 推理与数学能力仍然有限\",\"GPT-3虽然能完成基础算术和简单逻辑题，但在 多步推理、抽象代数、数理一致性等方面 表现仍然较弱。这限制了其在金融、科研、工程等高精度领域的适用性。\",\"7. 模型不可解释性问题严重\",\"GPT-3的推理过程完全由大量参数和非线性变换组成，目前尚无有效方式解释它为何会给出某一答案。这种不可解释性限制了其在高风险领域的应用，如医疗、法律、金融决策等。\",\"总结\",\"虽然GPT-3在few-shot学习方面展现出极强的能力，但其本质仍是一个“超大规模、强记忆型的语言预测器”，而非具备深层理解与推理能力的系统。它面临的问题包括任务适应不均、prompt敏感性高、缺乏结构化监督利用能力、推理有限、以及缺乏透明性等。这些限制提示我们，在使用GPT-3及其衍生模型时，仍需谨慎评估其边界与适用性，并探索更强的系统性泛化能力和稳健性。\"]},\"464\":{\"h\":\"相关工作\",\"t\":[\"1. 从词向量到上下文表示的发展历程\",\"该部分首先回顾了自然语言处理（NLP）领域中语言表示学习的演进：\",\"早期方法（如 word2vec、GloVe）关注学习固定词向量；\",\"后续方法（如 ELMo、ULMFiT）引入上下文，支持基于上下文的词表示；\",\"Transformer 时代：BERT、GPT 系列、XLNet 等模型将预训练语言模型推向主流，支持更广泛的下游任务，通过微调在多个任务上实现了SOTA。\",\"GPT-3继承了这一发展路线，并将参数规模推至前所未有的高度，强化了“无任务特定架构”的方法论。\",\"2. 微调范式与任务适应能力的关系\",\"在GPT-3之前，大多数SOTA模型依赖于“预训练 + 微调”范式，即先在大语料上预训练，再在具体任务数据上进行监督微调（如BERT、T5）。这种方法虽然效果强大，但依赖大量任务标注数据，不利于迁移与泛化。\",\"GPT-3的核心创新之一，是系统性地探索 无梯度更新的few-shot学习（in-context learning），挑战了传统对“适应任务必须微调”的假设。\",\"3. 元学习与few-shot学习的启发\",\"作者借鉴了 元学习（meta-learning） 的理念，即模型在“外循环”中获得广泛能力，在“内循环”中快速适应新任务。GPT-3通过扩展模型容量，在预训练阶段学习泛化模式，在推理阶段用文本输入指定任务，实质是一种“隐式元学习”机制。\",\"这与Few-shot Learning领域中如MAML、Prototypical Networks、Matching Networks等方法异曲同工，但不同于它们使用结构明确的元任务，GPT-3完全通过文本学习并表达任务结构。\",\"4. 模型规模扩展趋势与“Scaling Laws”\",\"文中引用了Kaplan等人提出的“神经语言模型的规模定律（Scaling Laws）”，即验证集损失随着模型规模、数据量和计算量按幂律缩放。在这一理论指导下，GPT-3以175B参数扩展至前代模型的10倍以上。\",\"GPT-3验证了一个关键假设：few-shot能力会随着模型规模的增加而显著增强，补足了先前few-shot模型（如 GPT-2、CTRL、T5）表现不稳定的问题。\",\"5. 多任务与多语言学习的基础\",\"GPT-3并未对每个任务建立单独的模型，而是通过单一语言建模目标，实现任务统一与跨任务迁移，呼应了T5等模型的“文本到文本”框架。同时，它在某种程度上也具备一定的多语言能力，尽管其非英语性能仍有限。\",\"此外，文中提到了一些少量使用in-context设定的早期尝试（如 GPT-2 的zero-shot prompt），但GPT-3是首次系统性、大规模地在zero-, one-, few-shot条件下进行全面评估的工作。\",\"总结\",\"GPT-3站在了词向量、上下文建模、transformer架构、微调范式、元学习和模型扩展趋势等多个重要研究方向的交汇点上。它在技术上并非从零出发，而是有机融合并推升了这些已有成果，将预训练语言模型从“参数调优”时代推向了“推理即编程”的新范式。\"]},\"465\":{\"h\":\"结论\",\"t\":[\"作者指出，GPT-3 展示了强大的in-context learning（上下文学习）能力，在不进行任何梯度更新的前提下，仅通过自然语言提示和示例，即可在多种语言任务中实现从零样本到少样本的泛化，部分任务甚至达到或超越微调模型的水平。尽管仍存在局限，但结果表明：随着模型规模扩展，大规模语言模型在任务通用性与灵活性方面具有巨大潜力，为未来通用语言智能系统的发展提供了重要方向。\"]},\"466\":{\"h\":\"InstructGPT 论文\",\"t\":[\"InstructGPT 论文\",\"论文链接: Training language models to follow instructions with human feedback\"]},\"467\":{\"h\":\"摘要\",\"t\":[\"本研究指出，仅通过增加语言模型的规模，并不能显著提升其对用户意图的理解与遵循能力。为了解决这一问题，作者提出一种通过人类反馈对模型进行微调的方法，用以更好地对齐模型行为与用户意图。\",\"具体方法包括：首先利用人工演示数据对GPT-3进行监督学习微调；然后通过人类对多个模型输出的偏好进行排序，训练奖励模型，并结合强化学习进一步优化模型。最终所得的InstructGPT模型，即使参数量远小于原始GPT-3（例如1.3B对比175B），在用户偏好评估中仍表现更优。此外，InstructGPT在输出真实性、减少有害内容生成等方面也有所改进，且在公开NLP任务上的性能损失极小。研究表明，人类反馈微调是一种有效的模型对齐手段，尽管仍有提升空间。\"]},\"468\":{\"h\":\"简介\",\"t\":[\"作者指出，大型语言模型（如GPT-3）虽然具备强大的自然语言处理能力，但它们常常偏离用户意图，表现出诸如捏造事实、生成有害或无关文本、不遵循指令等问题。这是因为它们的训练目标是最大化互联网文本的下一个词预测概率，而非“安全且有用地遵循用户指令”，这造成了目标的不一致，即“对齐问题”（alignment problem）。\",\"为解决这一问题，本文提出了一种对齐语言模型与用户意图的策略：通过人类反馈进行强化学习（Reinforcement Learning from Human Feedback, RLHF）。该方法包括三个关键步骤：\",\"监督学习微调（SFT）：收集人类示范数据，微调预训练的GPT-3模型；\",\"奖励模型训练（RM）：收集人类对模型多个输出的排序偏好，训练出一个能预测人类偏好的奖励模型；\",\"强化学习微调（PPO）：使用奖励模型的反馈，采用Proximal Policy Optimization算法进一步优化模型行为。\",\"作者称这些过程使得模型输出更符合人类偏好，但强调这种对齐是相对于特定人群（即标注者和研究者）的偏好，并非广义上的“人类价值”。\",\"通过图1的结果可见，即便是只有1.3B参数的InstructGPT模型，其输出也比175B的原始GPT-3更受人类偏好，显示出这种人类反馈驱动的微调策略极具潜力。图1中显示的不同模型在人类偏好评估中的胜率清晰反映了该方法的有效性，表明训练目标的改变（从“预测下一个词”转向“优化人类偏好”）能带来质的改善。\",\"此外，作者采用了“有帮助（helpful）、诚实（honest）、无害（harmless）”三大原则来评估模型对齐效果，强调未来开发和部署语言模型时需格外关注其社会影响及安全性。\",\"总之，本文引入了一种有效的对齐方法，为语言模型行为与用户意图之间架起了桥梁，为AI安全和实用性的发展提供了关键路径。\"]},\"469\":{\"h\":\"相关工作\",\"t\":[\"一、基于人类反馈的模型对齐（Alignment via Human Feedback）\",\"InstructGPT 的核心技术基础是 “强化学习来自人类反馈（RLHF）”，旨在将模型输出行为与人类意图对齐。这一方法起初应用于强化学习场景：\",\"用人类偏好训练强化学习代理 : Christiano et al., 2017 提出了一种通过人类偏好比较训练代理的强化学习方法。\",\"在模拟环境中用人类反馈改进行为策略 : Ibarz et al., 2018将人类偏好学习应用于模仿学习。\",\"RLHF 后来被应用于语言任务，如摘要：\",\"风格延续任务中的偏好学习 : Ziegler et al., 2019\",\"文本摘要中的奖励建模与 PPO 微调 : Stiennon et al., 2020\",\"此外，该方向在对话系统（Jaques et al., 2019）、机器翻译（Bahdanau et al., 2016）、语义解析（Lawrence and Riezler, 2018）、故事生成（Zhou and Xu, 2020）等任务中也得到了广泛实践。\",\"InstructGPT 的工作属于对上述方法的泛化：将 RLHF 用于对齐语言模型在广泛任务分布下的行为。\",\"二、训练语言模型以遵循自然语言指令（Instruction Following）\",\"另一相关研究方向是使用自然语言指令训练模型以实现跨任务泛化：\",\"FLAN：使用数十个 NLP 数据集、配以自然语言任务说明进行微调。 （Wei et al., 2021）\",\"T0 / T0++：将 NLP 基准任务转换为指令格式，通过多任务微调训练语言模型。 （Sanh et al., 2021）\",\"Natural Instructions：探索指令格式变化对模型泛化能力的影响。 （Mishra et al., 2021）\",\"InstructGPT 与上述方法的不同之处在于其训练数据源真实 API 用户提交的指令，更具 任务多样性与实用性。\",\"图 1 支持这一点：即使参数量远小于 GPT-3（1.3B vs. 175B），InstructGPT 模型依然在用户指令任务中获得更高的偏好评分。\",\"三、评估语言模型的风险与危害\",\"InstructGPT 还借鉴了对语言模型潜在风险的研究，这些研究强调：\",\"语言模型会生成有害或偏见内容（Bender et al., 2021；Gehman et al., 2020）\",\"TruthfulQA 提供了一个用于测试模型生成信息真实性的基准数据集 （Lin et al., 2021）\",\"偏见评估数据集：包括 Winogender（性别偏见）和 CrowS-Pairs（社会偏见）（Rudinger et al., 2018,Nangia et al., 2020）\",\"InstructGPT 在实验部分也采用了这些基准（见论文第 4 节），并指出：在对毒性任务加入“请尊重”提示时，InstructGPT 比 GPT-3 更少生成有害内容（见 Figure 7）。\",\"四、模型行为干预与有害输出缓解策略\",\"文献中也探索了多种控制模型输出的策略，这些方法为 InstructGPT 所采用的 RLHF 方式提供了对照方案：\",\"微调小型数据集以嵌入价值观 （Solaiman and Dennison, 2021）\",\"通过触发短语过滤预训练语料，以降低毒性输出倾向 （Ngo et al., 2021）\",\"使用外部语言模型引导生成方向（如 Plug-and-Play Language Models） （Dathathri et al., 2019）\",\"用正则化或投影技术缓解嵌入空间中的偏见 （Liang et al., 2021）\",\"尽管 InstructGPT 并未直接采用这些方法，但在强化学习微调中加入 KL 约束、预训练梯度（PPO-ptx）等机制，实际上也体现了对 对齐损失(alignment tax) 的控制。\",\"以下是对 InstructGPT 论文中第 3 节 “Methods and Experimental Details” 的详细总结，内容结构遵循原文小节安排（3.1–3.6），并结合论文图表（如图 2）以增强理解。部分内容需分多段呈现以保留关键信息。\"]},\"470\":{\"h\":\"方法\",\"t\":[\"InstructGPT 的方法主要基于 Stiennon et al. (2020) 和 Ziegler et al. (2019) 提出的 三步训练框架，用以实现语言模型对人类意图的对齐。该流程可参见论文图 2 的三步训练流程：\",\"监督微调（Supervised Fine-Tuning, SFT）: 使用人类标注者示范的优质输出，微调预训练 GPT-3 模型，得到初始策略模型。\",\"奖励模型训练（Reward Model, RM）: 收集一组模型输出对（针对同一输入），由人类标注者根据偏好进行排序。将这些排序用作训练奖励模型（RM）的监督信号，使其学会预测哪一输出更受偏好。\",\"使用 PPO 强化学习（Proximal Policy Optimization）微调模型: 以奖励模型为环境反馈信号，对 SFT 模型进一步使用 PPO 算法进行强化学习优化，从而得到最终的 InstructGPT 模型。\",\"图 2（Figure 2） 明确展示了这三步流程之间的数据流和优化路径，是 InstructGPT 方法的核心概括图。\",\"数据集构建（Dataset）\",\"InstructGPT 的训练数据主要来自以下两个来源：\",\"真实用户在 OpenAI API Playground 提交的 prompt\",\"提取并去重后用于训练 SFT、RM 和 PPO 模型。为确保训练集与评估集分离，按用户 ID 进行划分。\",\"为防止泄露隐私，对训练数据进行了 PII 过滤。\",\"标注者创作的 prompt（主要用于冷启动训练）\",\"分为三类：\",\"Plain（开放任务）\",\"Few-shot（带示例的任务）\",\"User-based（模拟用户需求的任务）\",\"三类子数据集：\",\"SFT 数据集（~13k prompts）用于监督微调\",\"RM 数据集（~33k prompts）用于训练奖励模型\",\"PPO 数据集（~31k prompts）为 PPO 模型提供输入\",\"表 1 显示了 API prompt 的任务分布：约 46% 为生成类任务，QA 和聊天合计约 23%，突出了真实用户需求的多样性。\",\"任务类型（Tasks）\",\"训练任务覆盖广泛，包括但不限于：\",\"文本生成（如创作、补全）\",\"问答（开放型和封闭型）\",\"对话、重写、摘要、分类、抽取等\",\"大部分任务通过自然语言指令表达意图。少量则通过 few-shot 示例或文本上下文隐式表达。标注者在判断指令时需考虑信息准确性、避免偏见与毒性，这为 InstructGPT 模型“helpful, honest, harmless”标准提供训练信号。\",\"人类数据采集（Human Data Collection）\",\"OpenAI 雇佣了约 40 名标注者（通过 Upwork 与 ScaleAI）参与数据标注，执行以下任务：\",\"提供高质量示范（用于 SFT）\",\"对模型输出进行偏好排序（用于 RM）\",\"对最终模型进行评估\",\"为了保证标注质量，OpenAI 设计了 筛选测试 来挑选具有敏感内容识别能力的标注者。训练过程中的一些 prompt 包含争议性内容，故特别强调标注者的社会敏感性。\",\"在人类偏好标注中，inter-annotator agreement 达到 73±1.5%，说明标注者之间达成了较高的一致性。论文还进行了一组 held-out 标注者实验，显示 InstructGPT 模型能够泛化到新标注者的偏好。\",\"模型结构与训练细节（Models）\",\"所有模型都基于 GPT-3 架构 ，在三个参数规模（1.3B、6B、175B）下进行训练，训练策略如下：\",\"SFT 模型训练\",\"使用标注者示范数据，训练 16 个 epoch\",\"使用余弦学习率衰减，0.2 的残差 dropout\",\"用奖励模型得分选择最佳模型（而非验证 loss）\",\"奖励模型（RM）训练\",\"输入为 prompt 和 response，输出为标量奖励\",\"在每个 prompt 上收集 K（4–9）个响应，由标注者排序，训练时将所有配对作为一个 batch，防止过拟合\",\"使用如下 pairwise ranking loss：\",\"loss(θ) = − (1 / C(K,2)) * E[log(σ(r_θ(x, y_w) − r_θ(x, y_l)))]\",\"其中 、 分别为更受欢迎和较差的响应。\",\"PPO 和 PPO-ptx 模型训练\",\"PPO 使用 RM 作为奖励函数\",\"为缓解对奖励函数的过度优化，引入 KL 惩罚项\",\"PPO-ptx 版本进一步加入 pretraining 任务的 log-likelihood 更新项，以防对齐过程中性能退化（alignment tax）\",\"PPO-ptx 目标函数如下：\",\"Objective = E[r − β * KL + γ * logP_pretrain]\",\"评估方式（Evaluation）\",\"为了衡量模型的“对齐程度”，InstructGPT 使用了综合性评估框架：\",\"A. API prompt 分布评估\",\"使用 held-out 用户的 prompt\",\"人类评估输出的偏好、质量（Likert 1–7）、以及一系列元数据（如是否 hallucinate、是否尊重约束）\",\"图 4 展示了模型在是否遵循指令、幻觉率等多个维度的性能\",\"B. 公共 NLP 数据集评估\",\"涉及 TruthfulQA（真实性）、RealToxicityPrompts（毒性）、Winogender/CrowS-Pairs（偏见）\",\"还评估模型在 SQuAD、DROP、HellaSwag、WMT 2015 等任务上的零样本表现\",\"显示模型在强化学习过程中存在轻微性能损失，但 PPO-ptx 可有效缓解（见图 29–34，原文）\"]},\"471\":{\"h\":\"结果\",\"t\":[\"1. 在 API prompt 分布上的实验结果:\",\"InstructGPT 的核心实验基于真实用户提交的指令性 prompts，在这些任务中：\",\"人类评估者显著偏好 InstructGPT 输出\",\"Figure 1 显示：在用户任务分布中，1.3B 的 InstructGPT 模型比 175B GPT-3 更受偏好。\",\"即使是少样本提示增强的 GPT-3（few-shot GPT-3），也不及 InstructGPT。\",\"例如，175B InstructGPT 输出相较于标准 GPT-3 的偏好比为 85% ± 3%，相比 few-shot GPT-3 为 71% ± 4%。\",\"PPO-ptx 与 PPO 模型均优于 SFT 和 GPT-3\",\"Figure 3 显示，在两类提示分布（GPT-3 与 InstructGPT 用户提交）上，InstructGPT 在所有规模下均优于 GPT-3。\",\"并且该优势在训练标注者和 held-out 标注者之间都保持一致，说明偏好并非训练数据过拟合造成。\",\"更好地遵循指令，减少幻觉，更适合作为用户助手\",\"Figure 4 展示了模型输出的多维质量元数据对比：\",\"InstructGPT 更少“幻觉”（hallucination）\",\"更能遵守“指令中的显式约束”\",\"更常“尝试正确完成任务”\",\"更适合用于“客户助手场景”\",\"2. 在公开 NLP 数据集上的实验结果\",\"在 TruthfulQA 上更真实、更少编造\",\"Figure 6 显示，在 TruthfulQA 基准上，PPO 和 PPO-ptx 模型显著提升回答的真实性与信息性。\",\"例如，在加入指导性提示（instruction+QA）时，InstructGPT 倾向于不作伪答（如选择“I have no comment”），而 GPT-3 则容易自信地给出错误答案。\",\"InstructGPT 输出更少毒性内容，尤其在有“尊重”提示下\",\"使用 RealToxicityPrompts 数据集 + Perspective API 自动打分 + 人类评估。\",\"Figure 7 显示：\",\"有“请保持尊重”提示时，InstructGPT 显著比 GPT-3 更少生成有毒文本。\",\"无提示时，两者毒性差异减小。\",\"若刻意要求生成毒性内容，InstructGPT 反而更“有效”执行（更高毒性），说明其任务执行能力更强，但未具内置限制。\",\"在偏见测试中未表现出优势\",\"在 CrowS-Pairs 和 Winogender 数据集上，InstructGPT 和 GPT-3 偏见水平相当，有时更低 entropy 表示模型更“确信”其回答，但不一定更公正。\",\"使用 PPO-ptx 缓解了对齐损失（alignment tax）\",\"原始 PPO 模型在 SQuAD、DROP 等任务上表现退化。\",\"但通过在 RL 过程中混入预训练目标（PPO-ptx），可基本恢复甚至超越 GPT-3 性能（见附录图 29–34）。\",\"3. 定性分析与模型行为观察\",\"模型泛化能力强：能处理非训练分布指令\",\"InstructGPT 可：\",\"处理 非英语指令，如法语（尽管有时仍用英文回应）\",\"总结并解释 代码片段\",\"Figure 8 示例显示：\",\"GPT-3 未能回答“列表 C 的作用”问题，InstructGPT 给出较为合理的解释（虽然也不完全正确）\",\"模型仍存在简单错误与对“荒谬”指令的顺从\",\"InstructGPT 在面对带错误前提的指令时，可能不会质疑，而是“默认接受并执行”。\",\"它也倾向于过度规避风险，在回答简单问题时冗长解释或“过于中性”。\",\"Figure 9 展示：\",\"对“冥想后吃袜子有何用”这类指令，GPT-3 胡编乱造；InstructGPT 则写出听起来“认真合理”的答案，但仍在胡说。\",\"对“炮弹打南瓜”的问题，InstructGPT没能直接回答（如“炸碎”），而是列举可能性并犹豫。\",\"总结：InstructGPT 的结果证明了 RLHF 的有效性\",\"提升：输出更符合人类偏好，减少幻觉与毒性，对指令遵循度高。\",\"挑战：仍可生成有害内容，对荒谬命令未进行识别，任务复杂性上限未显现。\",\"泛化性：在代码、非英语指令等非监督数据上表现较好。\",\"控制手段：通过 PPO-ptx 控制 alignment tax，维持 NLP 性能。\"]},\"472\":{\"h\":\"讨论\",\"t\":[\"InstructGPT 是 OpenAI 迭代式对齐研究计划的一部分，目标是使现有模型更符合人类意图，同时构建适用于未来更强 AI 的通用方法。\",\"RLHF 是一种低成本高回报的对齐方法\",\"与预训练相比，使用 RLHF 对齐语言模型所需的计算成本极低：\",\"训练 GPT-3（175B）需约 3640 petaflop/s-days；\",\"而 InstructGPT 的 SFT 阶段只需 4.9 petaflop/s-days；\",\"PPO 微调也仅为 60 petaflop/s-days。\",\"与其训练更大的模型，不如在现有模型上投资对齐方法：例如，1.3B InstructGPT 的输出比 175B GPT-3 更受欢迎（见 Figure 1）。\",\"RLHF 能够泛化“指令跟随能力”\",\"模型在未明确训练的任务上也表现良好，如非英语任务、代码任务（见 Figure 8）。\",\"这意味着对齐方法不仅优化模型行为，还能提高其泛化能力，有助于构建更通用、适应性强的智能系统。\",\"可显著降低对齐带来的性能损失(alignment tax)\",\"原始 PPO 模型在一些公开 NLP 数据集上的性能下降（如 DROP、SQuAD）。\",\"但通过引入预训练梯度混合（PPO-ptx），可以在保持对齐的同时维持甚至提升性能（详见附录 Figure 29–34）。\",\"将抽象对齐技术成功应用于现实世界模型部署\",\"与以往在合成任务或小型模型上的研究不同，InstructGPT 将 RLHF 应用于真实的 API 模型中，验证了该技术在生产环境下的可行性和价值。\",\"我们到底在“对齐”谁？(Who Are We Aligning To ?)\",\"作者清晰指出当前模型对齐行为的“参考群体”是有局限的，实际对齐的是训练流程中的多重人为偏好叠加：\",\"标注者的偏好：训练数据和奖励信号均来自一组英语标注者（主要来自美国和东南亚），并非普遍“人类代表”。\",\"研究者的设计意图：OpenAI 研究团队定义了标注规则、标准与示例，标注者受其引导。\",\"API 用户行为：训练 prompt 来源于真实 API 用户提交，其任务形式和风格可能代表一类高频商业用途。\",\"用户 ≠ 社会：OpenAI API 用户为申请加入测试队列的群体，不代表所有潜在用户，更不代表所有受语言模型影响的人群。\",\"结论：当前对齐并非通用意义上的“人类价值对齐”，而是特定群体与目标下的实用性对齐。未来若需面向多元人群，可能需要模型具备多偏好条件控制能力。\",\"限制与盲点（Limitations）\",\"InstructGPT 在多个方面仍存在不足：\",\"模型行为问题：\",\"仍会生成有害、偏见或捏造内容。\",\"过度顺从错误指令：如“吃袜子”、“南瓜吸引炮弹”等（Figure 9）。\",\"复杂指令处理能力不足：多条件限制（如“用两句话总结 1930 年代法国电影”）仍表现不佳。\",\"数据收集问题：\",\"标注者人数有限（约40人），且偏好有偏，可能影响输出的一致性与代表性。\",\"多数比较数据仅有 1 位标注者进行判断，可能遗漏歧义与分歧点。\",\"语言多样性不足：训练数据主要为英文，非英语泛化能力未系统评估。\",\"作者建议未来采用更多元标注、歧义加权、以及特定群体优先原则（如针对少数群体敏感任务）。\",\"尚待探索的问题（Open Questions）\",\"作者列出多个值得进一步研究的问题：\",\"如何更有效缓解毒性与偏见？\",\"引入 adversarial 数据收集（如 Dinan et al., 2019b）；\",\"在预训练层面进行数据过滤（如 Ngo et al., 2021）；\",\"建立更强的拒绝机制以识别恶意请求。\",\"如何应对多价值体系的冲突？\",\"开发具备“偏好条件化能力”的模型（即对不同用户群体可调节输出风格/规范）；\",\"探索“社会契约式”对齐方法以处理价值多样性。\",\"如何建立更强的 reject 模型机制？\",\"当任务违反道德或逻辑前提时，模型应能自动识别并拒绝执行，而非“高质量完成”。\",\"社会影响与部署考量（Broader Impacts）\",\"正面影响：\",\"更符合用户指令、更具可控性、更少毒性，适合用于构建对话助手、总结系统、教育工具等。\",\"为“人类偏好引导型 AI”提供现实路径，降低部署风险。\",\"潜在风险：\",\"当前偏好群体有限，若未经适当调节可能导致某些群体观点被系统性排除；\",\"对齐本身可被滥用，尤其在军事、虚假宣传等敏感场景下；\",\"若拒绝机制不足，模型仍可能在对抗性攻击下暴露隐私、输出有害内容。\",\"作者强调，技术细节必须伴随规范治理与透明流程，否则对齐仅为形式上的“驯化”，而非本质的 AI 安全。\",\"总结:\",\"项目\",\"关键结论\",\"RLHF 价值\",\"成本低、泛化强、性能好，优于简单 scaling\",\"当前对齐对象\",\"并非“人类普遍价值”，而是 OpenAI 标注者 + 用户\",\"局限性\",\"模型顺从性过高、多样性不足、对抗性脆弱\",\"未来方向\",\"多群体条件对齐、拒绝模型、反毒性 adversarial 训练\",\"部署建议\",\"建议伴随伦理审查、偏好反馈机制、开放接口控制\"]},\"473\":{\"h\":\"KV-Cache 详解\",\"t\":[\"大模型加速技术之KV Cache详解\"]},\"474\":{\"h\":\"Why we need KV Cache ？\",\"t\":[\"生成式generative模型的推理过程很有特点，我们给一个输入文本，模型会输出一个回答（长度为N），其实该过程中执行了N次推理过程。即GPT类模型一次推理只输出一个token，输出token会与输入tokens 拼接在一起，然后作为下一次推理的输入，这样不断反复直到遇到终止符。\",\"如上描述是我们通常认知的GPT推理过程。代码描述如下：\",\"import torch from transformers import GPT2LMHeadModel, GPT2Tokenizer def main(): # 加载模型和 tokenizer model = GPT2LMHeadModel.from_pretrained(\\\"gpt2\\\").eval() tokenizer = GPT2Tokenizer.from_pretrained(\\\"gpt2\\\") # 初始输入 in_text = \\\"Open AI is a\\\" in_tokens = torch.tensor(tokenizer.encode(in_text)).unsqueeze(0) # [1, seq_len] token_eos = torch.tensor([198]) # line break symbol out_token = None i = 0 with torch.no_grad(): while out_token != token_eos: outputs = model(in_tokens) logits = outputs.logits out_token = torch.argmax(logits[0, -1, :], dim=-1, keepdim=True).unsqueeze(0) # [1, 1] in_tokens = torch.cat((in_tokens, out_token), dim=1) text = tokenizer.decode(in_tokens[0]) print(f'step {i} input: {text}', flush=True) i += 1 out_text = tokenizer.decode(in_tokens[0]) print(f'\\\\nInput: {in_text}') print(f'Output: {out_text}') if __name__ == \\\"__main__\\\": main()\",\"输出:\",\"step 0 input: Open AI is a new step 1 input: Open AI is a new way step 2 input: Open AI is a new way to step 3 input: Open AI is a new way to build step 4 input: Open AI is a new way to build AI step 5 input: Open AI is a new way to build AI that step 6 input: Open AI is a new way to build AI that is step 7 input: Open AI is a new way to build AI that is more step 8 input: Open AI is a new way to build AI that is more efficient step 9 input: Open AI is a new way to build AI that is more efficient and step 10 input: Open AI is a new way to build AI that is more efficient and more step 11 input: Open AI is a new way to build AI that is more efficient and more efficient step 12 input: Open AI is a new way to build AI that is more efficient and more efficient than step 13 input: Open AI is a new way to build AI that is more efficient and more efficient than traditional step 14 input: Open AI is a new way to build AI that is more efficient and more efficient than traditional AI step 15 input: Open AI is a new way to build AI that is more efficient and more efficient than traditional AI. step 16 input: Open AI is a new way to build AI that is more efficient and more efficient than traditional AI. Input: Open AI is a Output: Open AI is a new way to build AI that is more efficient and more efficient than traditional AI.\",\"在上面的推理过程中，每 step 内，输入一个 token序列，经过Embedding层将输入token序列变为一个三维张量 [b, s, h]，经过一通计算，最后经 logits 层将计算结果映射至词表空间，输出张量维度为 [b, s, vocab_size]。\",\"当前轮输出token与输入tokens拼接，并作为下一轮的输入tokens，反复多次。可以看出第 i+1 轮输入数据只比第 i 轮输入数据新增了一个 token，其他全部相同！\",\"因此第 i+1 轮推理时必然包含了第 i 轮的部分计算。KV Cache 的出发点就在这里，缓存当前轮可重复利用的计算结果，下一轮计算时直接读取缓存结果。\",\"上面所举例子并没有使用KV Cache进行推理,请注意。\"]},\"475\":{\"h\":\"Self-Attention Without Cache\",\"t\":[\"下图给出了无 Cache 情况下，类GPT式生成式模型进行推理的过程:\",\"这种方式的问题是: 每生成一个 token，就要重新计算所有之前 token 的 Q/K/V + Attention + FFN 。\"]},\"476\":{\"h\":\"Self-Attention With Cache\",\"t\":[\"下图给出了有 Cache 情况下，类GPT式生成式模型进行推理的过程:\"]},\"477\":{\"h\":\"Huggingface 官方代码实现\",\"t\":[\"本节将根据 Huggingface 官方代码实现进行 KV Cache 实现讲解 (只展示核心代码，移除了大量与本文无关的逻辑)。\",\"官方代码链接: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\",\"下面将给出使用了 KV Cache 进行推理的代码:\",\"import torch from transformers import GPT2Tokenizer, GPT2Config from modeling_gpt2 import GPT2LMHeadModel # copy from huggingface , 删除了大量无关代码 def generate_text(model, tokenizer, prompt, max_new_tokens=50, eos_token_id=198): model.eval() input_ids = tokenizer.encode(prompt, return_tensors=\\\"pt\\\") past_key_values = None output_ids = input_ids.clone() with torch.no_grad(): for step in range(max_new_tokens): outputs = model( input_ids=input_ids, past_key_values=past_key_values, use_cache=True ) logits = outputs.logits past_key_values = outputs.past_key_values next_token_logits = logits[:, -1, :] next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True) output_ids = torch.cat([output_ids, next_token], dim=-1) if next_token.item() == eos_token_id: break input_ids = next_token # 采用KV Cache后，推理过程修改的关键: 下一步只送入新 token print(f\\\"step {step}: {tokenizer.decode(output_ids[0])}\\\", flush=True) return tokenizer.decode(output_ids[0]) def main(): config = GPT2Config() tokenizer = GPT2Tokenizer.from_pretrained(\\\"gpt2\\\") model = GPT2LMHeadModel(config) prompt = \\\"Once upon a time\\\" output = generate_text(model, tokenizer, prompt) print(\\\"\\\\nFinal output:\\\") print(output) if __name__ == \\\"__main__\\\": main()\",\"KV Cache 的引入是为了加速自回归模型的推理速度，具体体现在:\",\"每轮推理时，只需要计算当前轮新增 token 的 Q/K/V，而不需要重新计算所有之前 token 的 Q/K/V。\",\"缓存当前轮计算结果，下一轮推理时直接读取缓存结果。\",\"在首轮推理的过程中，我们传入的是 promt 提示词列表，并且 KV Cache 此时为空，还未进行初始化。因此首轮推理过程需要完成 promt 提示词列表的 keys 和 values 的缓存；由于 GPT2 由多层 GPT2Block 堆叠而成，而每一层 GPT2Block 都有一个 GPT2Attention 模块， 因此 KV Cache 需要准备好每一层 GPT2Attention 模块的 keys 和 values 缓存 (分层Cache - legacy_cache)。\",\"class GPT2Model(GPT2PreTrainedModel): def forward( self, input_ids=None, past_key_values=None, cache_position=None, attention_mask=None, position_ids=None, head_mask=None, use_cache=None, ): return_legacy_cache = False if use_cache: # 1. 首轮推理，先进行 Legacy Cache 初始化 if past_key_values is None: return_legacy_cache = True past_key_values = DynamicCache() # 2. 后续推理，将模型以元组形式返回的缓存重新封装为Legacy Cache形式 elif not isinstance(past_key_values, Cache): return_legacy_cache = True past_key_values = DynamicCache.from_legacy_cache(past_key_values) # 3. 词嵌入 inputs_embeds = self.wte(input_ids) # 4. 位置编码计算 if cache_position is None: # 4.1 已经缓存的词序列长度 past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0 # 4.2 只为当前传入的词生成位置序列 cache_position = torch.arange( past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device ) if position_ids is None: position_ids = cache_position.unsqueeze(0) # 添加batch维度 # 4.3 生成位置编码 position_embeds = self.wpe(position_ids) # 5. 词嵌入 + 位置编码 hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device) # 6. 进入堆叠GPT2Block模块前向传播流程 for i, block in enumerate(self.h): hidden_states = block( hidden_states, past_key_values if not (self.gradient_checkpointing and self.training) else None, # 训练时，不启用KV Cache cache_position, causal_mask, use_cache=use_cache, ) hidden_states = self.ln_f(hidden_states) hidden_states = hidden_states.view(output_shape) # 7. 将KV Cache用元组的形式进行返回 past_key_values = past_key_values if use_cache else None if return_legacy_cache: past_key_values = past_key_values.to_legacy_cache() return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=past_key_values, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, )\",\"下图展示的是步骤7中以元组形式返回的KV Cache结构:\",\"下面将展示GPT2Block模块的实现逻辑，由于不涉及KV Cache的实现细节，所以不过多展开:\",\"class GPT2Block(GradientCheckpointingLayer): def forward( self, hidden_states: Optional[tuple[torch.FloatTensor]], past_key_value: Optional[Cache] = None, cache_position: Optional[torch.LongTensor] = None, attention_mask: Optional[torch.FloatTensor] = None, use_cache: Optional[bool] = False, ) -> Union[tuple[torch.Tensor], Optional[tuple[torch.Tensor, tuple[torch.FloatTensor, ...]]]]: # 1. 归一化 residual = hidden_states hidden_states = self.ln_1(hidden_states) # 2. 自注意力运算 attn_output, self_attn_weights = self.attn( hidden_states, past_key_value=past_key_value, cache_position=cache_position, attention_mask=attention_mask, use_cache=use_cache, ) # 3. residual connection hidden_states = attn_output + residual # 4. 归一化 + MLP + residual connection residual = hidden_states hidden_states = self.ln_2(hidden_states) feed_forward_hidden_states = self.mlp(hidden_states) hidden_states = residual + feed_forward_hidden_states return hidden_states\",\"推理时的常规流程（无 KV Cache）， 每生成一个新 token，都要：\",\"重新输入全部历史 token\",\"对所有历史 token 重新计算 key 和 value\",\"这意味着重复计算，效率低，计算开销线性增长\",\"有了 KV Cache 后的改进：\",\"第一次输入完整句子，计算并缓存其 key/value；\",\"后续每次生成新 token 时：\",\"只计算新 token 的 query、key、value；\",\"把新 token 的 key/value 插入缓存中（代码中用 past_key_value.update(...) 完成）；\",\"attention 直接使用「历史缓存 key/value + 当前新 token 的 key/value」来完成；\",\"整个注意力的 query 只有一个（当前 token），key/value 是历史缓存 + 当前 token。\",\"class GPT2Attention(nn.Module): def __init__(self, config, is_cross_attention=False, layer_idx=None): self.c_attn = Conv1D(3 * self.embed_dim, self.embed_dim) # 输入维度: (batch,seq_len,embed_dim) , 变换后的输出维度: (batch,seq_len,3*embed_dim) self.c_proj = Conv1D(self.embed_dim, self.embed_dim) def forward( self, hidden_states: Optional[tuple[torch.FloatTensor]], past_key_value: Optional[Cache] = None, cache_position: Optional[torch.LongTensor] = None, attention_mask: Optional[torch.FloatTensor] = None, ) -> tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]: # 1. 一维卷积进行线性变换和升维，然后切分成query，key，value query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) # 2. (batch,seq_len,-1,head_dim) , head_dim 是多头自注意力中每个头切分到的维度 shape_q = (*query_states.shape[:-1], -1, self.head_dim) shape_kv = (*key_states.shape[:-1], -1, self.head_dim) # 3. 维度统一: (batch,heads,seq_len,head_dim) query_states = query_states.view(shape_q).transpose(1, 2) key_states = key_states.view(shape_kv).transpose(1, 2) value_states = value_states.view(shape_kv).transpose(1, 2) # 4. KV Cache 不为空 if past_key_value is not None: # 4.1 cache_position 记录当前词对应输入词序列中的索引 cache_kwargs = {\\\"cache_position\\\": cache_position} # 4.2 将当前词的key和val进行缓存，根据所在GPTBlock层级(layer_idx说明)，和位于词序列的索引(cache_kwargs说明),插入对应层的list缓存中去，同时返回对应的key和val list key_states, value_states = past_key_value.update( key_states, value_states, self.layer_idx, cache_kwargs=cache_kwargs ) # 5. 进行经典的多头自注意力运算(不展开细聊) attn_output, attn_weights = attention_interface( self, query_states, # 当前输入词的query key_states, # cache key list + 输入词的key value_states, # cache val list + 输入词的val attention_mask, # padding mask dropout=self.attn_dropout.p if self.training else 0.0, ) attn_output = attn_output.reshape(*attn_output.shape[:-2], -1).contiguous() attn_output = self.c_proj(attn_output) attn_output = self.resid_dropout(attn_output) return attn_output, attn_weights\"]},\"478\":{\"h\":\"LLaMA-1论文\",\"t\":[\"LLaMA-1 论文\",\"论文链接: LLaMA: Open and Efficient Foundation Language Models\"]},\"479\":{\"h\":\"摘要\",\"t\":[\"LLaMA是一系列高效的基础语言模型，参数规模从7B到65B不等，其特点在于仅使用公开可用的数据集进行训练，而无需依赖专有数据。实验结果表明，LLaMA-13B在多数基准测试中优于GPT-3（175B），而LLaMA-65B则与Chinchilla-70B和PaLM-540B等顶尖模型表现相当。这些模型的发布旨在促进研究社区的开放访问和研究，部分模型甚至可以在单个GPU上运行。\"]},\"480\":{\"h\":\"简介\",\"t\":[\"模型规模与性能的重新思考\",\"论文指出传统观点认为模型参数越多性能越优（如GPT-3的175B参数），但Hoffmann等人（2022）的研究表明，在固定计算预算下，小模型+更多数据训练可能更优。例如，LLaMA-7B在1T tokens训练后性能持续提升（见图1训练损失曲线），而Hoffmann推荐的10B模型仅训练200B tokens即停止。这一发现挑战了单纯追求参数规模的范式。\",\"推理效率的核心目标\",\"LLaMA强调推理成本优化而非单纯训练速度。论文指出，虽然大模型训练更快达到目标性能，但小模型在长期训练后推理效率更高（如13B模型比GPT-3小10倍却性能更优）。这一设计理念直接反映在模型架构选择上（见表2的参数字段与学习率配置）。\",\"数据策略与开源兼容性\",\"与Chinchilla、PaLM等依赖未公开数据（如\\\"Books-2TB\\\"）不同，LLaMA仅使用公开数据（CommonCrawl 67%、C4 15%、GitHub 4.5%等，详见表1），使其完全可开源。这一策略虽限制数据量（总计1.4T tokens），但通过高效训练仍实现SOTA。\",\"性能验证与社会责任\",\"65B模型在常识推理（表3）、闭卷问答（表4-5）等任务上超越Chinchilla-70B\",\"代码生成（表8）和数学推理（表7）的竞争力\",\"同时分析模型偏见（表12-13）与毒性（表11），呼应AI伦理需求\"]},\"481\":{\"h\":\"方法\",\"t\":[\"1. 预训练数据与处理\",\"LLaMA采用纯公开数据混合，总规模1.4T tokens，主要来源包括：\",\"CommonCrawl（67%）：经CCNet流水线去重、语言识别（保留英文）和质量过滤（基于Wikipedia引用分类）。\",\"C4（15%）：补充多样性，启发式过滤低质量网页（如标点缺失）。\",\"代码与学术数据：GitHub（4.5%，MIT/Apache许可项目）、ArXiv（2.5%，移除宏定义和参考文献）、Stack Exchange（2%，按评分排序答案）。\",\"其他数据如Wikipedia（4.5%）和书籍（Gutenberg/Books3，4.5%）均经过严格去重（见表1的采样比例与磁盘大小）。\",\"Tokenizer：使用SentencePiece的BPE算法，数字拆分为独立字符，UTF-8回退到字节级处理。\",\"2. 模型架构改进\",\"基于Transformer的优化设计（对比原始架构）：\",\"预归一化（Pre-normalization）：采用RMSNorm对子层输入归一化（灵感来自GPT-3），提升训练稳定性。\",\"激活函数：替换ReLU为SwiGLU（PaLM方案），隐藏层维度设为 以平衡计算效率。\",\"位置编码：使用旋转位置嵌入（RoPE）（GPT-NeoX方案），替代绝对位置编码。\",\"详细参数配置见表2，例如65B模型维度为8192、64头注意力、80层。\",\"3. 训练优化策略\",\"优化器：AdamW（），余弦学习率调度（最终学习率为峰值10%），权重衰减0.1，梯度裁剪1.0。\",\"效率优化：\",\"内存管理：通过xformers库实现因果多头注意力的高效计算，避免存储注意力权重（参考Rabe & Staats 2021）。\",\"激活检查点（Checkpointing）：手动实现线性层反向传播，减少重计算（节省GPU内存）。\",\"并行策略：模型与序列并行（Korthikanti et al. 2022），重叠计算与GPU通信。\",\"如图1所示，65B模型在2048块A100（80GB）上训练速度达380 tokens/sec/GPU，1.4T tokens训练耗时约21天。\",\"总结\",\"LLaMA的方法论核心是通过数据质量优化（公开数据+严格过滤）、架构微调（SwiGLU/RoPE）和工程创新（内存/并行优化）实现高效训练。其设计始终围绕推理效率目标（如小模型长期训练），最终在多个基准测试中超越更大规模的闭源模型。\"]},\"482\":{\"h\":\"结果\",\"t\":[\"1. 常识推理（Common Sense Reasoning）\",\"零样本性能（表3）： LLaMA-65B在8个常识推理基准（如BoolQ、PIQA、ARC等）中全面超越Chinchilla-70B，并在多数任务上击败PaLM-540B（除BoolQ和WinoGrande）。例如：\",\"ARC挑战集：LLaMA-65B得分57.8，显著高于PaLM-540B的53.0。\",\"OpenBookQA：65B模型以60.2%准确率刷新SOTA。\",\"关键发现：LLaMA-13B性能优于GPT-3（175B），验证小模型+长训练的有效性。\",\"2. 闭卷问答（Closed-Book QA）\",\"NaturalQuestions（表4）与TriviaQA（表5）：\",\"65B模型在零样本和少样本（64-shot）设置下均达到SOTA（TriviaQA零样本68.2%，超越Chinchilla-70B的55.4%）。\",\"13B模型在单V100 GPU上推理时，性能仍优于GPT-3（如TriviaQA 64-shot 64.0% vs. GPT-3 57.2%）。\",\"训练动态：图2显示模型性能与训练token量强相关（如33B模型在1.4T tokens后HellaSwag分数提升至82.8）。\",\"3. 代码生成与数学推理\",\"代码生成（表8）: LLaMA-65B在HumanEval（pass@1 23.7%）和MBPP（37.7%）上超越未微调的PaLM-62B（15.9%/21.4%），接近PaLM-540B（26.2%/36.8%）。\",\"数学能力（表7）：\",\"GSM8k：65B模型未经数学微调即达50.9%（多数投票69.7%），优于Minerva-62B（52.4%）。\",\"MATH：65B模型（10.6%）表现接近PaLM-62B（8.8%），但远低于Minerva-540B（33.6%），凸显领域微调的重要性。\",\"4. 多任务理解（MMLU）与指令微调\",\"MMLU 5-shot（表9/16）: LLaMA-65B平均得分63.4%，落后于Chinchilla-70B（67.5%）和PaLM-540B（69.3%），主因是书籍数据量不足（仅177GB vs. 其他模型2TB）。\",\"指令微调（LLaMA-I）（表10）: 简单微调后，65B模型在MMLU上提升至68.9%，超越Flan-PaLM-62B（66.1%），证明指令适应的高效性。\",\"5. 偏见与毒性分析\",\"RealToxicityPrompts（表11）: 模型越大毒性倾向越高（65B Respectful类毒性分0.141 vs. 7B的0.081），与OPT等模型趋势一致。\",\"CrowS-Pairs（表12）: LLaMA-65B平均偏见得分66.6，优于OPT-175B（69.5），但宗教类别偏差显著（79.0）。\",\"WinoGender（表13）: 模型对非二元代词（their/them）的指代准确率（81.7%）高于性别化代词（his/him 72.1%），反映社会偏见。\",\"LLaMA的核心成果：\",\"效率突破：小模型（如13B）通过数据与训练优化达到大模型（GPT-3/Chinchilla）性能。\",\"多领域竞争力：在代码、数学等专业任务中，未微调模型即接近SOTA。\",\"可复现性：纯公开数据训练结果挑战了专有数据的必要性，但书籍/学术数据不足限制MMLU表现。\",\"责任缺陷：模型规模与毒性/偏见正相关，需后续治理（论文第5章重点讨论）。\"]},\"483\":{\"h\":\"指令微调\",\"t\":[\"方法与目标: LLaMA通过轻量级指令微调（遵循Chung et al., 2022的协议）优化LLaMA-65B，得到LLaMA-I，旨在提升任务泛化能力，无需复杂架构调整。\",\"关键性能提升（表10）\",\"MMLU 5-shot：微调后准确率从63.4%→68.9%，超越Flan-PaLM-62B（66.1%），但低于GPT-3.5（77.4%）。\",\"领域差异（表16 - 参考上文）：STEM（如Astronomy +9.2%）和人文任务（Philosophy +5.1%）提升显著。\",\"生成能力（附录D）\",\"代码生成：可输出规范代码（如HTML标签清理的正则表达式）。\",\"多轮交互：支持复杂对话（如象棋开局策略分析）。\",\"伦理响应：自动生成AI使用指南，强调责任约束。\",\"局限性与挑战\",\"数据不透明：微调数据规模/多样性未公开，可能限制泛化。\",\"逻辑缺陷：数学/推理任务仍存在幻觉（需后处理）。\",\"总结\",\"LLaMA-I证明小规模微调即可显著提升任务适应性，但透明性与可靠性仍需优化，为开源社区提供了可复现的基线（如后续Alpaca/Vicuna工作）。\"]},\"484\":{\"h\":\"Bias, Toxicity and Misinformation\",\"t\":[\"毒性生成评估（RealToxicityPrompts）\",\"使用PerspectiveAPI对100k提示生成内容进行毒性评分（0-1分）\",\"关键发现（表11）：\",\"模型规模与毒性正相关（65B毒性分0.141 vs 7B的0.081）\",\"\\\"Respectful\\\"提示仍可能触发毒性响应\",\"与Chinchiila（0.087）等模型趋势一致\",\"社会偏见分析\",\"CrowS-Pairs（表12）：\",\"平均偏见得分66.6（优于OPT-175B的69.5）\",\"宗教类别偏见最显著（79.0分）\",\"WinoGender（表13）：\",\"对非二元代词（their/them）指代准确率81.7%\",\"性别化代词（his/him）准确率低至72.1%\",\"\\\"gotcha\\\"测试显示职业性别刻板印象明显\",\"真实性缺陷（TruthfulQA）\",\"65B模型真实答案率仅57%（表14）\",\"在对抗性问题上易产生幻觉\",\"表现优于GPT-3但可靠性仍不足\",\"关键问题\",\"数据根源：CommonCrawl等网络数据隐含的社会偏见难以完全过滤\",\"规模悖论：能力提升伴随风险增加（如65B毒性最高）\",\"总结\",\"LLaMA呈现出与同类模型相似的偏见/毒性模式，凸显公开数据训练的固有挑战。需结合：\",\"1）更严格的数据清洗（如Wikipedia引用过滤）\",\"2）后处理技术（如perspectiveAPI过滤）\",\"3）社区治理框架\"]},\"485\":{\"h\":\"相关工作\",\"t\":[\"语言模型发展脉络\",\"从统计语言模型（n-gram）到神经网络（RNN/LSTM），最终演进至Transformer架构（Vaswani et al., 2017）\",\"关键里程碑：\",\"GPT系列（Radford et al., 2018, 2019, 2020）确立自回归范式\",\"BERT（Devlin et al., 2018）推动双向预训练\",\"T5（Raffel et al., 2020）统一文本到文本框架\",\"规模化研究\",\"计算律发现（Kaplan et al., 2020）揭示模型性能与规模的关系\",\"Chinchilla（Hoffmann et al., 2022）提出数据-计算最优平衡理论\",\"涌现能力研究（Wei et al., 2022）分析规模带来的质变\",\"开源模型进展\",\"OPT（Zhang et al., 2022）和BLOOM（Scao et al., 2022）推动开源大模型发展\",\"GPT-NeoX（Black et al., 2022）提供20B参数开源基线\"]},\"486\":{\"h\":\"总结\",\"t\":[\"LLaMA系列模型通过高效架构设计和纯公开数据训练，在多个基准测试中达到与更大规模专有模型相当的性能，同时保持开源可复现性，为AI研究的民主化提供了重要范例。\"]},\"487\":{\"h\":\"LLaMA-2论文\",\"t\":[\"LLaMA-2 论文\",\"论文链接: Llama 2: Open Foundation and Fine-Tuned Chat Models\"]},\"488\":{\"h\":\"摘要\"},\"489\":{\"h\":\"模型层\"},\"490\":{\"h\":\"RoBERTa 论文\",\"t\":[\"RoBERTa 论文\",\"论文链接: RoBERTa: A Robustly Optimized BERT Pretraining Approach\"]},\"491\":{\"h\":\"摘要\",\"t\":[\"RoBERTa是一项针对BERT预训练方法的优化研究，通过系统性的实验发现BERT存在训练不足的问题，并提出了一系列改进措施。这些改进包括更长的训练时间、更大的批次规模、更多的数据、移除下一句预测（NSP）目标、使用更长的序列以及动态调整掩码模式。实验结果表明，优化后的RoBERTa在多个基准测试（如GLUE、RACE和SQuAD）上达到了最先进的性能，甚至超越了后续提出的模型。研究强调了预训练中设计选择和数据规模的重要性，同时表明BERT的掩码语言模型目标在优化后仍具有竞争力。相关模型和代码已公开供进一步研究。\"]},\"492\":{\"h\":\"引言\",\"t\":[\"RoBERTa 是一项针对 BERT 预训练方法的复制研究，旨在通过系统性的实验评估不同超参数和数据规模对模型性能的影响。研究发现，BERT 的训练存在显著不足，通过优化训练策略（如延长训练时间、增大批次规模、使用更多数据等），RoBERTa 能够匹配甚至超越后续提出的多种模型（如 XLNet）。\",\"论文的主要改进包括：\",\"动态掩码（Dynamic Masking）（对比静态掩码，如表 1 显示动态掩码在 SQuAD 2.0 和 SST-2 任务上表现更优）；\",\"移除下一句预测（NSP）目标（实验表明 NSP 对性能影响有限，甚至可能损害模型表现，如表 2 对比不同输入格式）；\",\"更大批次训练（表 3 显示增大批次规模可提升模型困惑度和下游任务准确率）；\",\"更高效的字节级 BPE 编码（减少未知词影响）。\",\"此外，RoBERTa 引入了新数据集 CC-News（76GB），并验证了数据规模对预训练的关键作用。最终，RoBERTa 在 GLUE、SQuAD 和 RACE 上取得 SOTA 结果（如表 4、5、6），证明 BERT 的掩码语言模型目标在优化后仍具竞争力。\"]},\"493\":{\"h\":\"背景\",\"t\":[\"RoBERTa 基于 BERT 的架构和训练方法，但通过优化关键设计选择提升性能。BERT 采用 Transformer 结构，输入由两个文本片段（Segment）组成，并添加特殊标记（如 [CLS]、[SEP]）。其预训练任务包括：\",\"掩码语言模型（MLM）：随机选择 15% 的输入 token，其中 80% 替换为 [MASK]，10% 保持不变，10% 替换为随机 token。原始 BERT 使用静态掩码（即预处理时固定掩码模式），而 RoBERTa 改用动态掩码（每次输入时重新生成掩码），实验证明动态掩码效果更优（如表 1）。\",\"下一句预测（NSP）：判断两个片段是否连续。尽管 BERT 认为 NSP 对下游任务（如自然语言推理）有帮助，但 RoBERTa 的实验表明移除 NSP 可能提升性能（如表 2 对比不同输入格式）。\",\"优化策略：\",\"使用 Adam 优化器（, , ）。\",\"学习率采用线性预热（10,000 步）和衰减策略。\",\"原始 BERT 训练 1M 步，批次大小 256，序列长度 512。\",\"数据：BERT 使用 BookCorpus 和 Wikipedia（共 16GB），而 RoBERTa 扩展至更大规模数据（如 CC-News、OpenWebText 等，总计 160GB）。\",\"RoBERTa 通过调整这些关键因素（如动态掩码、移除 NSP、增大批次和数据规模），显著提升了 BERT 的预训练效率和下游任务表现。\"]},\"494\":{\"h\":\"实验步骤\",\"t\":[\"1. 模型实现与优化\",\"RoBERTa 基于 fairseq 工具包重新实现了 BERT，并优化了训练细节：\",\"学习率调整：相比原始 BERT 的固定学习率（1e-4），RoBERTa 针对不同设置调整峰值学习率和预热步数。\",\"Adam 优化器改进：发现 Adam 的 项对训练稳定性影响较大，调整 以提升大批次训练的稳定性（参考 Section 3.1）。\",\"序列长度：始终使用完整长度序列（512 tokens），而原始 BERT 会在训练初期使用较短序列。\",\"2. 训练硬件与效率\",\"采用 混合精度训练（FP16），在配备 8×32GB NVIDIA V100 GPU 的 DGX-1 机器上进行分布式训练，利用 Infiniband 互联提升效率。\",\"3. 数据配置\",\"RoBERTa 使用了 5 个英语语料库，总计超过 160GB 文本，包括：\",\"BookCorpus + Wikipedia（16GB，原始 BERT 数据）\",\"CC-News（76GB，新闻数据）\",\"OpenWebText（38GB，Reddit 高赞网页内容）\",\"Stories（31GB，故事类文本）\",\"通过控制数据规模（如对比 16GB vs. 160GB），RoBERTa 验证了更多数据能显著提升模型性能（参考 Section 5 和 Table 4）。\",\"4. 评估基准\",\"实验在三大基准任务上进行：\",\"GLUE：涵盖 9 项自然语言理解任务（如 MNLI、SST-2 等），采用单任务微调（非多任务学习）。\",\"SQuAD：\",\"V1.1：答案必存在于上下文中。\",\"V2.0：支持无答案问题，RoBERTa 增加了二分类器判断可答性（参考 Section 3.3）。\",\"RACE：长文本阅读理解任务，需从 4 个选项中选择正确答案，测试模型的长距离依赖能力。\"]},\"495\":{\"h\":\"训练步骤分析\",\"t\":[\"静态与动态掩码（Static vs. Dynamic Masking）\",\"原始BERT使用静态掩码，即在数据预处理阶段生成掩码模式并固定，通过复制数据来增加多样性。\",\"RoBERTa改为动态掩码，每次输入序列时生成新的掩码模式。实验表明，动态掩码性能略优于静态掩码（如表1所示），且更高效。因此，后续实验均采用动态掩码。\",\"在BERT和RoBERTa的预训练中，掩码（Masking） 是 Masked Language Modeling (MLM) 任务的核心步骤，即随机遮盖输入文本的部分单词，并让模型预测这些被遮盖的单词。\",\"1. 静态掩码（Static Masking）\",\"原始BERT的做法：\",\"在数据预处理阶段，一次性 对每个句子随机选择15%的单词进行掩码（其中80%替换为 [MASK]，10%保持不变，10%替换为随机单词）。\",\"由于BERT训练时会多次遍历数据（如40个epoch），为了避免每次训练时看到相同的掩码模式，BERT采用 数据复制 的方法：\",\"将训练数据复制 10份，每份采用不同的随机掩码模式。\",\"这样，每个句子在训练过程中会被看到 4次（40 epochs / 10 copies = 4次），但每次掩码不同。\",\"问题：\",\"数据复制增加了存储和计算开销。\",\"由于掩码模式是固定的（尽管有10种变体），模型可能过拟合这些特定的掩码模式，影响泛化能力。\",\"2. 动态掩码（Dynamic Masking）\",\"RoBERTa的改进：\",\"不再预先固定掩码模式，而是在 每次输入模型时动态生成掩码。\",\"例如，同一个句子在训练的不同批次（batch）中，可能会被掩码不同的单词。\",\"优势：\",\"减少存储开销：无需复制数据，节省内存。\",\"增加多样性：模型在训练过程中看到更多的掩码变体，提升泛化能力。\",\"更适合长训练周期：当训练步数远超过BERT的1M步时（如RoBERTa训练500K步），动态掩码能持续提供新的掩码模式，避免过拟合。\",\"输入格式与下一句预测（NSP）\",\"原始BERT使用“Segment-pair+NSP”输入格式，包含两个文档片段和NSP损失。\",\"RoBERTa对比了多种输入格式（如表2所示）：\",\"Sentence-pair+NSP：使用单句对，性能下降，可能因无法学习长距离依赖。\",\"Full-sentences：连续句子打包，去除NSP损失，性能优于原始BERT。\",\"Doc-sentences：限制输入来自同一文档，性能略优于Full-sentences，但因批次大小可变，最终选择Full-sentences格式。\",\"实验表明，去除NSP损失不仅未降低性能，反而有所提升，这与原始BERT的结论相反。\",\"大批量训练（Large Batch Training）\",\"原始BERT使用256的批次大小训练1M步。RoBERTa尝试增大批次至2K和8K，并调整学习率（如表3所示）。\",\"结果显示，大批量训练（如8K）在保持相同计算成本下，能提升掩码语言模型的困惑度和下游任务性能。因此，RoBERTa采用8K批次进行训练。\",\"文本编码（Text Encoding）\",\"原始BERT使用30K的字符级BPE词汇表。\",\"RoBERTa改用基于字节的BPE（50K词汇表），无需额外预处理。虽然早期实验显示性能略有下降，但其通用性优势使其成为最终选择。\",\"这些改进共同构成了RoBERTa的核心优化策略，显著提升了模型性能（如表4所示）。实验结果表明，BERT原始设计存在优化空间，而RoBERTa通过系统性的调整，在GLUE、SQuAD和RACE等任务上达到了新的 state-of-the-art 水平。\"]},\"496\":{\"h\":\"RoBERTa核心改进总结\",\"t\":[\"RoBERTa（Robustly Optimized BERT Approach）是对BERT预训练过程的系统性优化，通过调整训练策略、数据规模和模型设置，显著提升了性能。其主要改进包括：\"]},\"497\":{\"h\":\"\",\"t\":[\"动态掩码（Dynamic Masking）：\",\"原始BERT使用静态掩码（预处理阶段固定掩码模式），而RoBERTa改为每次输入时动态生成掩码，减少存储开销并提升泛化能力（见表1）。\",\"结果：动态掩码在SQuAD 2.0和SST-2任务上表现略优（F1 78.7 vs. 78.3）。\",\"移除NSP任务（Next Sentence Prediction）：\",\"BERT使用NSP任务（判断两个句子是否连续），但实验表明去除NSP后性能反而提升（见表2）。\",\"RoBERTa改用Full-sentences（连续句子打包，不跨文档）或Doc-sentences（单文档内句子打包），后者效果略优但计算复杂，最终选择Full-sentences。\",\"大批量训练（Large Batch Training）：\",\"BERT使用256的批次大小，RoBERTa增大至8K，并调整学习率（如1e-3）。\",\"结果：大批量训练提升MLM困惑度（3.77 vs. 3.99）和下游任务准确率（MNLI-m 84.6 vs. 84.7）（见表3）。\",\"字节级BPE（Byte-level BPE）：\",\"改用50K词汇表的字节级BPE编码，减少未登录词（OOV）问题，虽对部分任务性能略有影响，但通用性更强。\"]},\"498\":{\"h\":\"\",\"t\":[\"更大规模数据：\",\"BERT训练数据：16GB（BookCorpus + Wikipedia）。\",\"RoBERTa新增CC-News、OpenWebText等，总数据量达160GB。\",\"结果：数据量增加后，SQuAD 2.0 F1从87.3提升至87.7（见表4）。\",\"更长训练步数：\",\"BERT训练1M步，RoBERTa延长至300K~500K步（计算成本相当，因批次更大）。\",\"结果：500K步时，SQuAD 2.0 F1达89.4，超越XLNet（88.8）（见表4）。\"]},\"499\":{\"h\":\"\",\"t\":[\"GLUE基准：\",\"单任务微调：RoBERTa在9项任务中全面超越BERT和XLNet（MNLI-m 90.2 vs. 89.8）（见表5）。\",\"排行榜提交：以88.5平均分刷新SOTA，其中4项任务（MNLI、QNLI、RTE、STS-B）领先（见表5）。\",\"SQuAD 2.0：\",\"仅用SQuAD数据（无外部数据），F1达89.8，超越XLNet（89.1）（见表6）。\",\"RACE阅读理解：\",\"准确率83.2%，显著高于BERT（72.0）和XLNet（81.7）（见表7）。\"]},\"500\":{\"h\":\"\",\"t\":[\"BERT原始设计未充分优化：RoBERTa证明更长训练、更大批次、更多数据是关键。\",\"NSP任务非必要：去除后性能反而提升，与BERT结论相反。\",\"动态掩码与大批量训练：提升效率的同时改善泛化能力。\",\"开源贡献：发布模型、代码及新数据集CC-News。\",\"RoBERTa的改进表明，BERT的MLM目标本身足够强大，只需优化训练策略即可达到SOTA，无需复杂结构调整。\"]},\"501\":{\"h\":\"相关工作\",\"t\":[\"早期方法如ELMo、GPT和BERT通过不同训练目标（如语言建模、机器翻译、掩码语言建模）取得了显著进展，而后续工作通过多任务微调、实体嵌入、跨度预测和自回归预训练（如XLNet）进一步提升了性能。作者强调，这些改进通常依赖于更大模型和更多数据（如XLNet使用10倍于BERT的数据），而RoBERTa的目标是通过系统性地复现、简化和优化BERT的训练过程，为这些方法提供一个更清晰的性能基准，从而帮助社区更好地理解不同改进的相对贡献。\"]},\"502\":{\"h\":\"总结\",\"t\":[\"通过系统优化BERT的预训练策略（包括动态掩码、移除NSP任务、增大批次和训练数据、延长训练时间），RoBERTa在GLUE、SQuAD和RACE任务上实现了SOTA性能，证明了BERT原始设计的潜力尚未被充分挖掘；\",\"同时，研究揭示了模型性能提升的关键因素并非复杂结构改动，而是训练策略和数据规模的优化，相关代码、模型和CC-News数据集已开源以促进后续研究。\"]},\"503\":{\"h\":\"从\\\"零\\\"实现 Bert\",\"t\":[\"利用Pytorch从\\\"零\\\"实现Bert\",\"TinyBert 源码链接: https://github.com/BinaryOracle/TinyBert\"]},\"504\":{\"h\":\"Bert 是什么 ？\",\"t\":[\"BERT 全称为 Bidirectional Encoder Representation from Transformer，是 Google 以无监督的方式利用大量无标注文本「炼成」的语言模型，其架构为 Transformer 中的 Encoder（BERT = Encoder of Transformer）。\",\"以往为了解决不同的 NLP 任务，我们会为该任务设计一个最合适的神经网络架构并做训练，不同的 NLP 任务通常需要不同的模型，而设计这些模型并测试其 performance 是非常耗成本的（人力，时间，计算资源）。如果有一个能直接处理各式 NLP 任务的通用架构该有多好？\",\"随着时代演进，不少人很自然地有了这样子的想法，而 BERT 就是其中一个将此概念付诸实践的例子，Google 在预训练 BERT 时让它同时进行两个任务：\",\"漏字填空，即完型填空 (Masked Language Model)\",\"判断第 2 个句子在原始本文中是否跟第 1 个句子相接（Next Sentence Prediction）\"]},\"505\":{\"h\":\"Masked Language Model\",\"t\":[\"在 BERT 中，Masked LM（Masked Language Model）构建了语言模型，简单来说，就是随机遮盖或替换一句话里面的任意字或词，然后让模型通过上下文预测那一个被遮盖或替换的部分，之后做 Loss 的时候也只计算被遮盖部分的 Loss，这其实是一个很容易理解的任务，实际操作如下：\",\"随机把一句话中 15% 的 token（字或词）替换成以下内容：\",\"这些 token 有 80% 的几率被替换成 [MASK]，例如 my dog is hairy→my dog is [MASK]\",\"有 10% 的几率被替换成任意一个其它的 token，例如 my dog is hairy→my dog is apple\",\"有 10% 的几率原封不动，例如 my dog is hairy→my dog is hairy\",\"之后让模型预测和还原被遮盖掉或替换掉的部分，计算损失的时候，只计算在第 1 步里被随机遮盖或替换的部分，其余部分不做损失，其余部分无论输出什么东西，都无所谓。\",\"这样做的好处是，BERT 并不知道 [MASK] 替换的是哪一个词，而且任何一个词都有可能是被替换掉的，比如它看到的 apple 可能是被替换的词。这样强迫模型在编码当前时刻词的时候不能太依赖当前的词，而要考虑它的上下文，甚至根据上下文进行 \\\"纠错\\\"。比如上面的例子中，模型在编码 apple 时，根据上下文 my dog is，应该把 apple 编码成 hairy 的语义而不是 apple 的语义。\"]},\"506\":{\"h\":\"Next Sentence Prediction\",\"t\":[\"我们首先拿到属于上下文的一对句子，也就是两个句子，之后我们要在这两个句子中加一些特殊的 token：[CLS]上一句话[SEP]下一句话[SEP]。也就是在句子开头加一个 [CLS]，在两句话之间和句末加 [SEP]，具体地如下图所示：\",\"可以看到，上图中的两句话明显是连续的。如果现在有这么一句话 [CLS]我的狗很可爱[SEP]企鹅不擅长飞行[SEP]，可见这两句话就不是连续的。\",\"Token Embedding 就是正常的词向量，即 PyTorch 中的 nn.Embedding()\",\"Segment Embedding 的作用是用 embedding 的信息让模型分开上下句，我们给上句的 token 全 0，下句的 token 全 1，让模型得以判断上下句的起止位置，例如:\",\"[CLS]我的狗很可爱[SEP]企鹅不擅长飞行[SEP] 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\",\"Position Embedding 和 Transformer 中的不一样，不是三角函数，而是学习出来的。\"]},\"507\":{\"h\":\"Multi-Task Learning\",\"t\":[\"BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。\"]},\"508\":{\"h\":\"Fine-Tuning\",\"t\":[\"BERT 的 Fine-Tuning 共分为 4 中类型: 文本分类，Token分类，推理任务，问答任务。\",\"如果现在的任务是 文本分类，首先在输入句子的开头加一个代表分类的符号 [CLS]，然后将该位置的 output，丢给 Linear Classifier，让其 predict 一个 class 即可。整个过程中 Linear Classifier 的参数是需要从头开始学习的，而 BERT 中的参数微调就可以了\",\"为什么要用第一个位置，即 [CLS] 位置的 output，个人理解是因为 BERT 内部是 Transformer，而 Transformer 内部又是 Self-Attention，所以 [CLS] 的 output 里面肯定含有整句话的完整信息，这是毋庸置疑的。但是 Self-Attention 向量中，自己和自己的值其实是占大头的，现在假设使用 的 output 做分类，那么这个 output 中实际上会更加看重 ，而 又是一个有实际意义的字或词，这样难免会影响到最终的结果。但是 [CLS] 是没有任何实际意义的，只是一个占位符而已，所以就算 [CLS] 的 output 中自己的值占大头也无所谓。当然你也可以将所有词的 output 进行 concat，作为最终的 output。\",\"如果现在的任务是 Token分类，将句子中各个字对应位置的 output 分别送入不同的 Linear，预测出该字的标签。其实这本质上还是个分类问题，只不过是对每个字都要预测一个类别。\",\"如果现在的任务是 NLI（自然语言推理）。即给定一个前提，然后给出一个假设，模型要判断出这个假设是 正确、错误还是不知道。这本质上是一个三分类的问题，和 Case 1 差不多，对 [CLS] 的 output 进行预测即可\",\"如果现在的任务是 问答任务，举例来说，如上图，将一篇文章，和一个问题（这里的例子比较简单，答案一定会出现在文章中）送入模型中，模型会输出两个数 s,e，这两个数表示，这个问题的答案，落在文章的第 s 个词到第 e 个词。具体流程我们可以看下面这幅图:\",\"首先将问题和文章通过 [SEP] 分隔，送入 BERT 之后，得到上图中黄色的输出。此时我们还要训练两个 vector，即上图中橙色和黄色的向量。首先将橙色和所有的黄色向量进行 dot product，然后通过 softmax，看哪一个输出的值最大，例如上图中 对应的输出概率最大，那我们就认为 s=2。\",\"同样地，我们用蓝色的向量和所有黄色向量进行 dot product，最终预测得 的概率最大，因此 e=3。最终，答案就是 s=2,e=3。\",\"你可能会觉得这里面有个问题，假设最终的输出 s>e 怎么办，那不就矛盾了吗？其实在某些训练集里，有的问题就是没有答案的，因此此时的预测搞不好是对的，就是没有答案。\"]},\"509\":{\"h\":\"从 “零” 开始的预训练\",\"t\":[\"从本节开始，我们将从\\\"零\\\"开始，体验Bert的预训练过程是如何实现的；\"]},\"510\":{\"h\":\"数据清洗\",\"t\":[\"首先我们需要准备一个小型语料库，确保在单台机器上，仅使用CPU就能完成整个训练过程，这里采用的是 wikitext-2 和 wikitext-103 两个开源数据集:\",\"WikiText 英语词库数据（The WikiText Long Term Dependency Language Modeling Dataset）是一个包含1亿个词汇的英文词库数据，这些词汇是从Wikipedia的优质文章和标杆文章中提取得到，包括WikiText-2和WikiText-103两个版本，相比于著名的 Penn Treebank (PTB) 词库中的词汇数量，前者是其2倍，后者是其110倍。每个词汇还同时保留产生该词汇的原始文章，这尤其适合当需要长时依赖(longterm dependency)自然语言建模的场景。\",\"Wikitext-103是超过 1 亿个语句的数据合集，全部从维基百科的 Good 与 Featured 文章中提炼出来。广泛用于语言建模，当中包括 fastai 库和 ULMFiT 算法中经常用到的预训练模型。\",\"WikiText2是Wikitext-103 的子集，主要用于测试小型数据集的语言模型训练效果。\",\"WIKITEXT-2\",\"WIKITEXT-103\",\"下载地址\",\"https://s3.amazonaws.com/fast-ai-nlp/wikitext-103.tgz\",\"https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz\",\"WikiText-2 和 WikiText-103 是两个广泛用于语言模型训练和评估的英文维基百科语料数据集 ，由 Salesforce 提出并开源。它们在 NLP 领域（特别是语言建模、预训练任务）中非常经典。\",\"将数据集压缩包下载到dataset目录下，并解压到当前目录下，然后使用prepare_data文件所提供代码对原始数据格式进行解析，得到对应的JSON格式文件:\",\"相关核心代码实现如下:\",\"def process_csv(file_path): \\\"\\\"\\\"处理CSV文件,返回处理后的句子列表\\\"\\\"\\\" all_sentences = [] with open(file_path, 'r', encoding='utf-8') as f: reader = csv.reader(f) for row in reader: # 使用NLTK库，将一整段文本按“句子”切分成一个句子列表。 # 处理每行文本：去除前后空格，过滤无效行 paragraph = [line.strip() for line in sent_tokenize(row[0]) if line.strip() and not line.strip().startswith('=') and not all(c in string.punctuation for c in line.strip())] # 过滤掉句子数少于2的行 paragraph = [line for line in paragraph if len(line.split('. ')) >= 2] # 确保句子数为偶数 if len(paragraph) % 2 != 0: paragraph = paragraph[:-1] all_sentences.extend(paragraph) return all_sentences def main(): # 处理两个CSV文件 test_sentences = process_csv('wikitext-2/test.csv') train_sentences = process_csv('wikitext-2/train.csv') # 写入JSON文件 train_output_path = 'wikitext-2/train.json' os.makedirs(os.path.dirname(train_output_path), exist_ok=True) with open(train_output_path, 'w', encoding='utf-8') as f: json.dump(train_sentences, f, indent=4, ensure_ascii=False) print(f\\\"成功生成JSON文件: {train_output_path}\\\") test_output_path = 'wikitext-2/test.json' os.makedirs(os.path.dirname(test_output_path), exist_ok=True) with open(test_output_path, 'w', encoding='utf-8') as f: json.dump(test_sentences, f, indent=4, ensure_ascii=False) print(f\\\"成功生成JSON文件: {test_output_path}\\\") if __name__ == \\\"__main__\\\": main()\"]},\"511\":{\"h\":\"分词器实现\",\"t\":[\"分词器的实现较为简单，首先是其初始化方法中需要完成：字典初始化，数据预加载(可挪到其他地方实现)。\",\"class Tokenizer: def __init__(self, vocab_file = None): vocab_data = None if vocab_file is not None: with open(vocab_file, 'r') as f: vocab_data = json.load(f) # 定义字典保存路径 dict_path = 'dataset/vocab_dict.json' # 尝试加载已保存的字典 if os.path.exists(dict_path): with open(dict_path, 'r', encoding='utf-8') as f: saved_dict = json.load(f) self.word2idx = saved_dict['word2idx'] self.idx2word = {int(k): v for k, v in saved_dict['idx2word'].items()} self.vocab_size = len(self.word2idx) else: # 首先加入特殊标记：PAD, CLS, SEP, MASK , UNK , 这些是 BERT 模型中常用的特殊 token self.word2idx = {f'{name}': idx for idx, name in enumerate(['PAD', 'CLS', 'SEP', 'MASK' , 'UNK'])} # 处理vocab_data为列表形式的情况 if isinstance(vocab_data, list): # 将所有文本合并成一个字符串 all_text = ' '.join(vocab_data) # 临时替换特殊标记 ，然后对句子进行分词 temp_text = all_text.replace('<unk>', 'UNK') sentences = word_tokenize(temp_text) # 获取所有单词并去重 word_list = list(set(sentences)) # 给每个普通词分配索引，从4开始（前面是特殊token）, 当前已经有的词数（4个特殊词） hold_place = len(self.word2idx) for idx, word in enumerate(word_list): if word == 'UNK': continue self.word2idx[word] = idx + hold_place else: raise ValueError(\\\"vocab_data must be a list\\\") # 创建反向映射：索引 → 单词 self.idx2word = {idx: word for word, idx in self.word2idx.items()} # 总词汇量 self.vocab_size = len(self.word2idx) # 确保映射是一一对应的 assert len(self.word2idx) == len(self.idx2word) # 保存字典到文件 with open(dict_path, 'w') as f: json.dump({ 'word2idx': self.word2idx, 'idx2word': self.idx2word }, f, indent=4) # 对列表数据进行解析 self.max_len = 103 if isinstance(vocab_data, list): self.word_ids = [] # 两两配对遍历 for i in range(0, len(vocab_data), 2): sent_a = vocab_data[i] sent_b = vocab_data[i+1] tokens_a = self.encode(sent_a) tokens_b = self.encode(sent_b) # 如果任一句子长度超过50，跳过这对 if len(tokens_a) > 50 or len(tokens_b) > 50: continue # 否则保存这两个句子的 token ID 列表 self.word_ids.append(tokens_a) self.word_ids.append(tokens_b)\",\"字典的构建过程太过粗糙，导致最终构建得到的字典过大并且还有很多噪声，从而模型训练学习到每个词的含义需要更大量的数据集且最终效果也不会很好，可考虑换成 HuggingFace 的 BertTokenizer / WordPieceTokenizer 实现。\",\"上面优化方向很多，比如: 去除含有低频词的句对，因为低频词出现次数极少，模型很难学到它们的语义表示。\",\"对外提供的编码和解码两个方法实现如下:\",\" def encode(self, text): return self.tokenize(text) def decode(self, tokens): return self.detokenize(tokens) def tokenize(self, text): sentences = word_tokenize(text) tokens = [] for word in sentences: if word in self.word2idx: tokens.append(self.word2idx[word]) else: # 如果遇到不存在于字典中的word，则使用UNK替换 tokens.append(self.word2idx['UNK']) return tokens def detokenize(self, tokens): return ' '.join([self.idx2word[token] for token in tokens])\",\"实际实现过程中，出于方便，还将一个工具方法整合到了分词器的实现之中，它是用于执行Bert MLM任务掩码策略的方法:\",\" # 执行Bert的掩码策略: 掩码候选位置，输入序列，掩码符号 def masking_procedure(self,cand_pos, input_ids, masked_symb): masked_pos = [] masked_tokens = [] # 对于所有掩码候选位置执行掩码策略： 80% 概率替换为[MASK]，10% 概率替换为随机词，10% 概率保持不变 for pos in cand_pos: masked_pos.append(pos) # 记录被掩码的位置 masked_tokens.append(input_ids[pos]) # 记录被掩码的原token if random.random() < p_mask: # 80% 概率替换为[MASK] input_ids[pos] = masked_symb elif random.random() > (p_mask + p_replace): # 10% 概率替换为随机词 rand_word_idx = random.randint(4, self.vocab_size - 1) input_ids[pos] = rand_word_idx else: # 10% 概率保持不变 pass return masked_pos, masked_tokens\"]},\"512\":{\"h\":\"Batch数据准备\",\"t\":[\"有了分词器后，我们需要读取并构建Batch数据，用于我们的预训练任务，该过程由make_data方法实现，具体步骤为:\",\"收集相同数量的相邻句对和非相邻句对。\",\"对每个句对构建用于NSP任务的样本，形式为: [CLS] + A + [SEP] + B + [SEP]。\",\"对每个句对构建用于MLM任务的样本，首先将[CLS] + A + [SEP] + B + [SEP]句子中20%的词执行掩码策略，而针对这20%需要被掩码的词之上，再按照80%用MASK掩码替换，10%用随机词替换，10%保持原样不动的形式进行处理；最后返回两个列表: 20%执行掩码的词的位置列表，20%执行掩码的词的原Token列表。\",\"将所有输入序列填充到等长max_len。\",\"返回构建得到的单个样本列表: [被掩码后的输入序列, 句子分隔列表 , 20%执行掩码的词的位置列表, 20%执行掩码的词的原Token列表, 是否为连贯的上下句]。\",\"所有样本列表构成Batch数据返回。\",\"def make_data(tokenizer): sentences = tokenizer.word_ids batch_data = [] len_sentences = len(sentences) # Step 1: 收集相邻句对 adjacent_pairs = [] for i in range(len_sentences - 1): a, b = i, i + 1 if len(sentences[a]) <= 50 and len(sentences[b]) <= 50: adjacent_pairs.append((a, b)) # Step 2: 随机生成等量的非相邻句对 non_adjacent_pairs = [] valid_indices = [i for i in range(len_sentences) if len(sentences[i]) <= 50] for a in valid_indices: candidates = [b for b in valid_indices if abs(a - b) > 1] if candidates: b = random.choice(candidates) non_adjacent_pairs.append((a, b)) # 打乱顺序 random.shuffle(adjacent_pairs) random.shuffle(non_adjacent_pairs) # 保证数量一致 min_count = min(len(adjacent_pairs), len(non_adjacent_pairs)) adjacent_pairs = adjacent_pairs[:min_count] non_adjacent_pairs = non_adjacent_pairs[:min_count] # 构建样本 for a, b in adjacent_pairs: sample = prepare_sample(tokenizer, a, b, is_next=True) batch_data.append(sample) for a, b in non_adjacent_pairs: sample = prepare_sample(tokenizer, a, b, is_next=False) batch_data.append(sample) return batch_data def prepare_sample(tokenizer, tokens_a_idx, tokens_b_idx, is_next): sentences = tokenizer.word_ids tokens_a = sentences[tokens_a_idx] tokens_b = sentences[tokens_b_idx] # 拼接 [CLS] + A + [SEP] + B + [SEP] input_ids = [tokenizer.word2idx['CLS']] + tokens_a + [tokenizer.word2idx['SEP']] + tokens_b + [ tokenizer.word2idx['SEP']] segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (1 + len(tokens_b)) # MLM 准备 n_pred = min(max_pred, max(1, int(len(input_ids) * 0.2))) cand_pos = [ i for i, token in enumerate(input_ids) if token not in {tokenizer.word2idx['CLS'], tokenizer.word2idx['SEP'], tokenizer.word2idx['PAD'], tokenizer.word2idx['UNK']} ] random.shuffle(cand_pos) masked_pos, masked_tokens = tokenizer.masking_procedure(cand_pos[:n_pred], input_ids, tokenizer.word2idx['MASK']) # Padding def pad(seq, target_len, pad_value=tokenizer.word2idx['PAD']): seq += [pad_value] * (target_len - len(seq)) pad(input_ids, tokenizer.max_len) pad(segment_ids, tokenizer.max_len) if max_pred > n_pred: pad(masked_pos, max_pred) pad(masked_tokens, max_pred) return [input_ids, segment_ids, masked_tokens, masked_pos, is_next]\"]},\"513\":{\"h\":\"模型\",\"t\":[\"本文中的 Bert 模型整体实现也比较简单，其中关于BertEncoders编码并输出结果的整个过程如下图所示:\",\"NSP 任务会利用 CLS Token 作为整个输入序列的全局信息聚合表示，再经过非线性变换后，进行二分类任务，判断下一个句子是否是当前句子的后续句子，具体过程如下图所示:\",\"MLM 任务会利用 masked_pos 从BertEncoders编码输出结果中提取出被掩码的位置对应的嵌入向量，经过相同的非线性变换后，将这些掩码Token对应的嵌入向量映射到词向量空间中去，得到模型预测的这些掩码Token对应的真实词，具体过程如下图所示:\",\"核心代码实现如下:\",\"class BERT(nn.Module): def __init__(self, n_layers, vocab_size, max_len): \\\"\\\"\\\" 初始化一个简化版的 BERT 模型，支持 MLM（掩码语言建模） 和 NSP（下一句预测） 两个任务。 参数： n_layers: Transformer 编码器层数 vocab_size: 词表大小 max_len: 最大序列长度 \\\"\\\"\\\" super(BERT, self).__init__() # 1. 词嵌入 + 位置嵌入 + 句子嵌入 self.embedding = Embeddings(vocab_size, max_len) # 2. 多个 Transformer 编码器层堆叠 self.encoders = nn.ModuleList([ EncoderLayer() for _ in range(n_layers) ]) # 3. Pooler 层：用于提取 [CLS] token 的表示，用于 NSP 任务 self.pooler = Pooler() # 4. 下一句预测（NSP）分类器 self.next_cls = nn.Linear(d_model, 2) # 输出维度为 2，表示是否是连续句子 self.gelu = gelu # GELU 激活函数 # 5. 权重共享：Pooler 层与 FC 层共享权重 shared_weight = self.pooler.fc.weight # 获取 pooler 中的全连接层权重 self.fc = nn.Linear(d_model, d_model) # 创建新的线性层 self.fc.weight = shared_weight # 共享权重（weight tying） # 6. 权重共享：MLM 分类器共享词嵌入矩阵 shared_weight = self.embedding.word_emb.weight # 获取词嵌入层权重 self.word_classifier = nn.Linear(d_model, vocab_size, bias=False) self.word_classifier.weight = shared_weight # 权重共享（tie weights） def forward(self, tokens, segments, masked_pos): \\\"\\\"\\\" 前向传播逻辑 输入： tokens: [batch_size, seq_len]，token 的索引（已添加 [CLS], [SEP], [MASK] 等） segments: [batch_size, seq_len]，segment_id，区分句子 A 和 B masked_pos: [batch_size, max_pred]，记录被掩码的位置 输出： logits_cls: [batch_size, 2]，NSP 分类结果 logits_lm: [batch_size, max_pred, vocab_size]，MLM 预测结果 \\\"\\\"\\\" # 1. 词嵌入 + 位置嵌入 + 句子嵌入 output = self.embedding(tokens, segments) # shape: [batch_size, seq_len, d_model] # 2. 构造 padding mask（忽略填充部分） enc_self_pad_mask = get_pad_mask(tokens) # shape: [batch_size, seq_len, seq_len] # 3. 依次通过每个编码器层（Transformer Layer） for layer in self.encoders: output = layer(output, enc_self_pad_mask) # output shape: [batch_size, seq_len, d_model] # 4. NSP 任务：使用 [CLS] 标记进行下一句预测 hidden_pool = self.pooler(output[:, 0]) # 提取 [CLS] 位置的隐藏状态并池化 logits_cls = self.next_cls(hidden_pool) # 分类输出：[batch_size, 2] # 5. MLM 任务：恢复被掩码的词 # masked_pos: [batch_size, max_pred] # 扩展 masked_pos 到三维，便于从 output 中 gather 出被掩码位置的表示 masked_pos = masked_pos.unsqueeze(-1).expand(-1, -1, d_model) # shape: [batch, max_pred, d_model] # 使用 torch.gather 从 output 中取出被掩码位置的 token 表示 h_masked = torch.gather(output, dim=1, index=masked_pos) # shape: [batch_size, max_pred, d_model] # 通过全连接层 + GELU 激活函数 h_masked = self.gather(output, dim=1, index=masked_pos) # 再次提取被掩码位置的表示 h_masked = self.gelu(self.fc(h_masked)) # shape: [batch_size, max_pred, d_model] # 6. MLM 分类器：预测被掩码的词 logits_lm = self.word_classifier(h_masked) # shape: [batch_size, max_pred, vocab_size] # 返回两个任务的结果 return logits_cls, logits_lm\",\"完整的代码实现部分，大家参考仓库源码即可，本文不再全部Copy展示。\"]},\"514\":{\"h\":\"训练\",\"t\":[\"训练过程就比较常规了，有一点不同就是Bert预训练阶段的学习目标是: MLM Loss + NSP Loss ，具体核心代码实现如下:\",\"tokenizer = Tokenizer(\\\"dataset/wikitext-2/train.json\\\") batch_data = make_data(tokenizer) batch_tensor = [torch.LongTensor(ele) for ele in zip(*batch_data)] dataset = BERTDataset(*batch_tensor) dataloader = DataLoader(dataset, batch_size=32, shuffle=True) model = BERT(n_layers,tokenizer.vocab_size,tokenizer.max_len) lr = 1e-4 epochs = 100 # 优化器与学习率调度器 optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01) # 损失函数 + 标签平滑 criterion1 = nn.CrossEntropyLoss(label_smoothing=0.1) criterion2 = nn.CrossEntropyLoss(label_smoothing=0.1,ignore_index=0) # 加载检查点 checkpoint_path = 'best_model.pth' if os.path.exists(checkpoint_path): model.load_state_dict(torch.load(checkpoint_path, weights_only=True, map_location=device)) print('Loaded checkpoint from', checkpoint_path) model.to(device) best_loss = float('inf') # training total_batches = len(dataloader) for epoch in range(epochs): avg_loss = 0 for batch_idx, one_batch in enumerate(dataloader): input_ids, segment_ids, masked_tokens, masked_pos, is_next = [ele.to(device) for ele in one_batch] logits_cls, logits_lm = model(input_ids, segment_ids, masked_pos) # NSP 任务损失 loss_cls = criterion1(logits_cls, is_next) # MLM 任务损失 loss_lm = criterion2(logits_lm.view(-1, tokenizer.vocab_size), masked_tokens.view(-1)) loss_lm = (loss_lm.float()).mean() # 总损失 loss = loss_cls + loss_lm avg_loss += loss.item() if (epoch + 1) % 1 == 0: print(f'Epoch:{epoch + 1} Batch:{batch_idx + 1}/{total_batches} \\\\t loss: {loss:.6f}') loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) optimizer.step() avg_loss /= total_batches # 保存最优模型 if avg_loss < best_loss: best_loss = avg_loss torch.save(model.state_dict(), f'best_model.pth') print(f'Saved best model with loss: {best_loss:.6f}') # 效果评估 evaluate_model()\",\"由于模型输出的logits_cls是一个二分类值，因此我们只需要根据is_next取出索引0或者1下标对应的值即可知道我们是否预测正确，并且使用预测结果计算NSP任务损失值。\",\"对于MLM任务损失计算来说，我们只会计算被随机遮盖或替换的部分，其余部分不做损失，因此模型返回的logits_lm也只包含被掩码的Token对应的模型预测真实词，同时通过masked_tokens可知这些被掩码Token对应的真实词作为Label，从而计算交叉熵损失就很简单了。\",\"这里需要注意一点，对于MLM任务损失计算来说，我们需要在其对应的CrossEntropyLoss中指定ignore_index=0，即忽略掉PAD部分的损失计算；\",\"这里PAD部分指的是对于不同的句子，它们都是按照其序列长度的20%比例进行的掩码，而对于较短的句子，其掩码数量可能会偏少，因此为了确保masked_tokens列表中所有句子掩码数量一致，需要对掩码数量不足max_pred的进行PAD填充。\",\"模型返回的logits_lm中同样含有PAD部分，但是我们在计算损失时指定了ignore_index=0，即忽略掉PAD部分的损失计算，因此不会影响最终的损失值计算。\",\"gather函数比较灵活，它可以在指定维度上，根据索引矩阵，从源张量中提取特定位置的元素，构造一个新的张量。\",\"对于每一个输出位置 (i,j)，如果 dim=1（列方向），那么它从 input [index[i][j]][j] 中取值。\",\"对于每一个输出位置 (i,j)，如果 dim=0（列方向），那么它从 input [i][index[i][j]] 中取值。\"]},\"515\":{\"h\":\"效果\",\"t\":[\"本文所展示的Bert预训练属于教学级别的，最终的训练效果也是一般，仅供参考和学习:\",\"MLM Task: Correct / Total = 3167 / 9027 | Accuracy = 0.3508 (预测正确的掩码词数量/总掩码的词数量)\",\"NSP Task: Correct / Total = 504 / 960 | Accuracy = 0.5250 (预测正确的句对数量/总句对数量)\"]},\"516\":{\"h\":\"Details\",\"t\":[\"本节将会对Bert模型实现的部分细节进行说明。\"]},\"517\":{\"h\":\"Padding Mask 如何生成并起作用的 ？\",\"t\":[\"首先模型会根据传入的Tokens列表生成一个Pad Mask矩阵，该 矩阵维度 和 Q@K.T 后得到的注意力得分矩阵维度相同\",\"def get_pad_mask(tokens, pad_idx=0): ''' suppose index of [PAD] is zero in word2idx tokens: [batch, seq_len] ''' batch, seq_len = tokens.size() pad_mask = tokens.data.eq(pad_idx).unsqueeze(1) # （batch,1,seq_len) pad_mask = pad_mask.expand(batch, seq_len, seq_len) # （batch,seq_len,seq_len) return pad_mask\",\"假设输入的Token序列为: [A,B,C,PAD,PAD,PAD] , 则生成的Pad Mask模样为:\",\"在注意力得分矩阵计算完毕后，我们会使用Pad Mask矩阵将注意力得分矩阵中对应位置的得分设置为一个非常小的值，这样在后续的Softmax计算中，这些位置的概率就会接近0，从而在注意力机制中就不会考虑到这些PAD部分的Token了。\",\"class ScaledDotProductAttention(nn.Module): def forward(self, Q, K, V, attn_mask): scores = torch.matmul(Q, K.transpose(-1, -2) / msqrt(d_k)) # scores: [batch, n_heads, seq_len, seq_len] scores.masked_fill_(attn_mask, -1e9) attn = nn.Softmax(dim=-1)(scores) # context: [batch, n_heads, seq_len, d_v] context = torch.matmul(attn, V) return context\",\"横着看是计算某个词与全局序列中其他词的相关度，后续需要利用该相关度完成当前词的全局上下文信息融合，我们只需要确保对于某个词的上下文融合不被PAD词参与即可，而无需考虑PAD词的全局上下文信息是否需要进行计算。\"]},\"518\":{\"h\":\"图解 Bert\",\"t\":[\"图解Bert & Bert文本分类实战\"]},\"519\":{\"h\":\"环境搭建\",\"t\":[\"按序执行以下命令完成环境搭建:\",\"git clone https://github.com/DA-southampton/Read_Bert_Code.git cd Read_Bert_Code conda create -n Read_Bert_Code python=3.9.22 conda activate Read_Bert_Code\",\"本文使用的是谷歌的中文预训练模型：chinese_L-12_H-768_A-12.zip，模型有点大，我就不上传了，如果本地不存在，就点击这里直接下载,或者直接命令行运行\",\"wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\",\"预训练模型下载下来之后，进行解压，然后将tf模型转为对应的pytorch版本即可。对应代码如下:\",\"export BERT_BASE_DIR=/Users/zhandaohong/Read_Bert_Code/chinese_L-12_H-768_A-12 python convert_tf_checkpoint_to_pytorch.py \\\\ --tf_checkpoint_path$BERT_BASE_DIR/bert_model.ckpt \\\\ --bert_config_file$BERT_BASE_DIR/bert_config.json \\\\ --pytorch_dump_path$BERT_BASE_DIR/pytorch_model.bin\",\"转化成功之后，将模型放入到仓库对应位置：\",\"Read_Bert_Code/bert_read_step_to_step/prev_trained_model/\",\"并重新命名为：\",\" bert-base-chinese\",\"其次是准备训练数据，这里我准备做一个文本分类任务，使用的是Tnews数据集，这个数据集来源是这里，分为训练，测试和开发集，我已经上传到了仓库中，具体位置在\",\"Read_Bert_Code/bert_read_step_to_step/chineseGLUEdatasets/tnews\",\"需要注意的一点是，因为我只是为了了解内部代码情况，所以准确度不是在我的考虑范围之内，所以我只是取其中的一部分数据，其中训练数据使用1k，测试数据使用1k，开发数据1k。\",\"准备就绪，使用pycharm导入项目，准备调试，我的调试文件是run_classifier.py文件，对应的参数为\",\"--model_type=bert --model_name_or_path=prev_trained_model/bert-base-chinese --task_name=\\\"tnews\\\" --do_train --do_eval --do_lower_case --data_dir=./chineseGLUEdatasets/tnews --max_seq_length=128 --per_gpu_train_batch_size=16 --per_gpu_eval_batch_size=16 --learning_rate=2e-5 --num_train_epochs=4.0 --logging_steps=100 --save_steps=100 --output_dir=./outputs/tnews_output/ --overwrite_output_dir\",\"然后启动 run_classifier.py 文件进行调试即可 , 所参考源仓库未提供requirements.txt文件，因此需要大家自行完成运行时缺失依赖包的安装。\"]},\"520\":{\"h\":\"数据预处理\",\"t\":[\"输入数据格式\",\"{ \\\"guid\\\": \\\"train-0\\\", \\\"label\\\": \\\"104\\\", // 文本分类任务: 文本对应的标签 \\\"text_a\\\": \\\"股票中的突破形态\\\", \\\"text_b\\\": null // NSP任务: 用于判断给出的两个句子是否连续 }\",\"NSP (Next Sentence Prediction)\",\"文本分词 & 借助字典映射为word id\",\"\\\"股票中的突破形态\\\" --> ['股', '票', '中', '的', '突', '破', '形', '态'] --> [5500, 4873, 704, 4638, 4960, 4788, 2501, 2578]\",\"对于字典中不存在的词 , 用 [UNK] 表示, 对应的id为 100\",\"过长截断策略\",\"添加特殊Token标记\",\"原序列添加特殊Token标记图\",\"[101, 5500, 4873, 704, 4638, 4960, 4788, 2501, 2578, 102]\",\"BertTokenizer中的特殊token id:\",\"[CLS]: 101\",\"[SEP]: 102\",\"[MASK]: 103\",\"[UNK]: 100\",\"[PAD]: 0\",\" # BertTokenizer def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None): if token_ids_1 is None: return [self.cls_token_id] + token_ids_0 + [self.sep_token_id] cls = [self.cls_token_id] sep = [self.sep_token_id] return cls + token_ids_0 + sep + token_ids_1 + sep\",\"创建句子辨识列表，用以区分不同的句子\",\"token_type_ids作用图解\",\" # BertTokenizer def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None): \\\"\\\"\\\" Creates a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence pair mask has the following format: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 | first sequence | second sequence if token_ids_1 is None, only returns the first portion of the mask (0's). \\\"\\\"\\\" sep = [self.sep_token_id] cls = [self.cls_token_id] if token_ids_1 is None: return len(cls + token_ids_0 + sep) * [0] return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\",\"创建用以区分special tokens部分的mask列表\",\"special_tokens_mask作用图解\",\" # BertTokenizer def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False): if token_ids_1 is not None: return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1] return [1] + ([0] * len(token_ids_0)) + [1]\",\"超长截断\",\" # PreTrainedTokenizer if max_length and len(encoded_inputs[\\\"input_ids\\\"]) > max_length: encoded_inputs[\\\"input_ids\\\"] = encoded_inputs[\\\"input_ids\\\"][:max_length] encoded_inputs[\\\"token_type_ids\\\"] = encoded_inputs[\\\"token_type_ids\\\"][:max_length] encoded_inputs[\\\"special_tokens_mask\\\"] = encoded_inputs[\\\"special_tokens_mask\\\"][:max_length]\",\"生成padding部分的mask列表\",\"attention_mask作用图解\",\" # 生成注意力掩码，真实token对应1，填充token对应0 attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\",\"所有序列都填充到max_length长度,不足长度用padding填充\",\"填充过程图\",\" # 记录输入长度 input_len = len(input_ids) # 计算需要填充的长度 --- 所有输入序列等长，都等于max_length padding_length = max_length - len(input_ids) # 右填充 input_ids = input_ids + ([pad_token] * padding_length) attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length) token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\",\"数据集中每一个样本最终都会解析得到一个InputFeatures\",\"InputFeatures组成图解\",\"features.append( InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=label, input_len=input_len))\",\"label 是当前文本对应的类别标签 input_len 是序列实际长度(含special tokens)\",\"数据集预处理完后，将InputFeatures List列表组装起来得到需要的DataSet\",\"dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_lens,all_labels)\"]},\"521\":{\"h\":\"模型架构\"},\"522\":{\"h\":\"DataLoader\",\"t\":[\" train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset) train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size,collate_fn=collate_fn)\",\"DataLoader 设置的回调方法cllote_fn负责对返回的一个batch，在返回前进行预处理:\",\"def collate_fn(batch): all_input_ids, all_attention_mask, all_token_type_ids, all_lens, all_labels = map(torch.stack, zip(*batch)) max_len = max(all_lens).item() # 计算当前批次中所有序列的实际最大长度 all_input_ids = all_input_ids[:, :max_len] # 按照本批次序列中最大长度进行截断: max_length --> max_len all_attention_mask = all_attention_mask[:, :max_len] all_token_type_ids = all_token_type_ids[:, :max_len] return all_input_ids, all_attention_mask, all_token_type_ids, all_labels\"]},\"523\":{\"h\":\"BertEmbeddings\",\"t\":[\"input embeddings = token embeddings + segmentation embeddings + position embeddings\",\"class BertEmbeddings(nn.Module): def __init__(self, config): super(BertEmbeddings, self).__init__() self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0) self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size) self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, input_ids, token_type_ids=None, position_ids=None): seq_length = input_ids.size(1) if position_ids is None: # 为当前批次中的每个序列样本生成一个位置序列: (1,2,3,4,5,...) , 构成一个位置序列矩阵 position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) position_ids = position_ids.unsqueeze(0).expand_as(input_ids) if token_type_ids is None: token_type_ids = torch.zeros_like(input_ids) words_embeddings = self.word_embeddings(input_ids) position_embeddings = self.position_embeddings(position_ids) # 位置编码为可学习的矩阵 token_type_embeddings = self.token_type_embeddings(token_type_ids) # 让模型自己学会区分不同的句子 embeddings = words_embeddings + position_embeddings + token_type_embeddings embeddings = self.LayerNorm(embeddings) embeddings = self.dropout(embeddings) return embeddings\",\"嵌入向量生成过程图\"]},\"524\":{\"h\":\"BertEncoder\"},\"525\":{\"h\":\"BertLayer\",\"t\":[\"BertLayer模型结构图\",\"class BertIntermediate(nn.Module): def __init__(self, config): super(BertIntermediate, self).__init__() self.dense = nn.Linear(config.hidden_size, config.intermediate_size) # (768,3072) # 激活函数 - GLEU if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)): self.intermediate_act_fn = ACT2FN[config.hidden_act] else: self.intermediate_act_fn = config.hidden_act def forward(self, hidden_states): hidden_states = self.dense(hidden_states) hidden_states = self.intermediate_act_fn(hidden_states) # 激活函数 - GLEU return hidden_states class BertOutput(nn.Module): def __init__(self, config): super(BertOutput, self).__init__() self.dense = nn.Linear(config.intermediate_size, config.hidden_size) # (3072,768) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, hidden_states, input_tensor): hidden_states = self.dense(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.LayerNorm(hidden_states + input_tensor) return hidden_states class BertLayer(nn.Module): def __init__(self, config): super(BertLayer, self).__init__() self.attention = BertAttention(config) self.intermediate = BertIntermediate(config) self.output = BertOutput(config) def forward(self, hidden_states, attention_mask=None): attention_output = self.attention(hidden_states, attention_mask) intermediate_output = self.intermediate(attention_output) layer_output = self.output(intermediate_output, attention_output) return layer_output\"]},\"526\":{\"h\":\"BertEncoder\",\"t\":[\"BertEncoder模型结构图\",\"class BertEncoder(nn.Module): def __init__(self, config): super(BertEncoder, self).__init__() self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)]) def forward(self, hidden_states, attention_mask=None, head_mask=None): for i, layer_module in enumerate(self.layer): hidden_states = layer_module(hidden_states, attention_mask, head_mask[i]) return hidden_states\"]},\"527\":{\"h\":\"BertPooler\",\"t\":[\"BertPooler模型结构图\",\"class BertPooler(nn.Module): def __init__(self, config): super(BertPooler, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) self.activation = nn.Tanh() def forward(self, hidden_states): # We \\\"pool\\\" the model by simply taking the hidden state corresponding # to the first token. first_token_tensor = hidden_states[:, 0] # CLS Token Context Embeddings pooled_output = self.dense(first_token_tensor) pooled_output = self.activation(pooled_output) return pooled_output\"]},\"528\":{\"h\":\"BertModel\",\"t\":[\"BertModel模型结构图\",\"class BertModel(BertPreTrainedModel): def __init__(self, config): super(BertModel, self).__init__(config) self.embeddings = BertEmbeddings(config) self.encoder = BertEncoder(config) self.pooler = BertPooler(config) self.init_weights() def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None): extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2) extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0 embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids) sequence_output = self.encoder(embedding_output, extended_attention_mask, # padding mask ) pooled_output = self.pooler(sequence_output) outputs = (sequence_output, pooled_output,) return outputs\"]},\"529\":{\"h\":\"BertForSequenceClassification\",\"t\":[\"BertForSequenceClassification模型结构图\",\"class BertForSequenceClassification(BertPreTrainedModel): def __init__(self, config): super(BertForSequenceClassification, self).__init__(config) self.num_labels = config.num_labels self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, self.config.num_labels) self.init_weights() def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None): outputs = self.bert(input_ids, attention_mask=attention_mask, # padding mask token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) # None ? pooled_output = outputs[1] # 对于分类任务来说，只需要去除CLS Token用于分类任务即可 pooled_output = self.dropout(pooled_output) logits = self.classifier(pooled_output) outputs = (logits,) + outputs[2:] # add hidden states and attention if they are here if labels is not None: if self.num_labels == 1: # We are doing regression loss_fct = MSELoss() loss = loss_fct(logits.view(-1), labels.view(-1)) else: loss_fct = CrossEntropyLoss() loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) outputs = (loss,) + outputs return outputs # (loss), logits, (hidden_states), (attentions)\"]},\"530\":{\"h\":\"BertAttention\"},\"531\":{\"h\":\"BertSelfAttention\",\"t\":[\"多头自注意力计算流程图\",\"class BertSelfAttention(nn.Module): def __init__(self, config): super(BertSelfAttention, self).__init__() self.output_attentions = config.output_attentions self.num_attention_heads = config.num_attention_heads self.attention_head_size = int(config.hidden_size / config.num_attention_heads) self.all_head_size = self.num_attention_heads * self.attention_head_size self.query = nn.Linear(config.hidden_size, self.all_head_size) self.key = nn.Linear(config.hidden_size, self.all_head_size) self.value = nn.Linear(config.hidden_size, self.all_head_size) self.dropout = nn.Dropout(config.attention_probs_dropout_prob) def transpose_for_scores(self, x): new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size) x = x.view(*new_x_shape) return x.permute(0, 2, 1, 3) def forward(self, hidden_states, attention_mask=None, head_mask=None): mixed_query_layer = self.query(hidden_states) mixed_key_layer = self.key(hidden_states) mixed_value_layer = self.value(hidden_states) # view 成多头格式: (batch,heads,seq_len,d_k) query_layer = self.transpose_for_scores(mixed_query_layer) key_layer = self.transpose_for_scores(mixed_key_layer) value_layer = self.transpose_for_scores(mixed_value_layer) # Take the dot product between \\\"query\\\" and \\\"key\\\" to get the raw attention scores. attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) # (batch,heads,d_k,seq_len) attention_scores = attention_scores / math.sqrt(self.attention_head_size) if attention_mask is not None: # Apply the attention mask is (precomputed for all layers in BertModel forward() function) attention_scores = attention_scores + attention_mask # Normalize the attention scores to probabilities. attention_probs = nn.Softmax(dim=-1)(attention_scores) # This is actually dropping out entire tokens to attend to, which might # seem a bit unusual, but is taken from the original Transformer paper. attention_probs = self.dropout(attention_probs) context_layer = torch.matmul(attention_probs, value_layer) context_layer = context_layer.permute(0, 2, 1, 3).contiguous() new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) context_layer = context_layer.view(*new_context_layer_shape) # 合并头结果 return context_layer\"]},\"532\":{\"h\":\"BertSelfOutput\",\"t\":[\"BertSelfOutput计算流程图\",\"class BertSelfOutput(nn.Module): def __init__(self, config): super(BertSelfOutput, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) # 残差链接 + 层归一化 def forward(self, hidden_states, input_tensor): hidden_states = self.dense(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.LayerNorm(hidden_states + input_tensor) return hidden_states\"]},\"533\":{\"h\":\"BertAttention\",\"t\":[\"BertAttention计算流程图\",\"class BertAttention(nn.Module): def __init__(self, config): super(BertAttention, self).__init__() self.self = BertSelfAttention(config) self.output = BertSelfOutput(config) def forward(self, input_tensor, attention_mask=None): self_outputs = self.self(input_tensor, attention_mask) # 多头自注意力机制 attention_output = self.output(self_outputs, input_tensor) return attention_output\"]},\"534\":{\"h\":\"预训练\",\"t\":[\"预训练与微调\"]},\"535\":{\"h\":\"BertPredictionHeadTransform\",\"t\":[\"BertPredictionHeadTransform结构图\",\"class BertPredictionHeadTransform(nn.Module): def __init__(self, config): super(BertPredictionHeadTransform, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)): self.transform_act_fn = ACT2FN[config.hidden_act] else: self.transform_act_fn = config.hidden_act self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) def forward(self, hidden_states): hidden_states = self.dense(hidden_states) hidden_states = self.transform_act_fn(hidden_states) hidden_states = self.LayerNorm(hidden_states) return hidden_states\"]},\"536\":{\"h\":\"BertLMPredictionHead\",\"t\":[\"BertLMPredictionHead结构图\",\"class BertLMPredictionHead(nn.Module): def __init__(self, config): super(BertLMPredictionHead, self).__init__() self.transform = BertPredictionHeadTransform(config) # The output weights are the same as the input embeddings, but there is # an output-only bias for each token. self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False) self.bias = nn.Parameter(torch.zeros(config.vocab_size)) def forward(self, hidden_states): hidden_states = self.transform(hidden_states) hidden_states = self.decoder(hidden_states) + self.bias return hidden_states\"]},\"537\":{\"h\":\"BertPreTrainingHeads\",\"t\":[\"BertPreTrainingHeads结构图\",\"class BertPreTrainingHeads(nn.Module): def __init__(self, config): super(BertPreTrainingHeads, self).__init__() self.predictions = BertLMPredictionHead(config) self.seq_relationship = nn.Linear(config.hidden_size, 2) def forward(self, sequence_output, pooled_output): prediction_scores = self.predictions(sequence_output) # seq_relationship_score = self.seq_relationship(pooled_output) # 两个句子是否为上下句关系 return prediction_scores, seq_relationship_score\"]},\"538\":{\"h\":\"BertForPreTraining\",\"t\":[\"BertForPreTraining结构图\",\"class BertForPreTraining(BertPreTrainedModel): def __init__(self, config): super(BertForPreTraining, self).__init__(config) self.bert = BertModel(config) self.cls = BertPreTrainingHeads(config) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, masked_lm_labels=None, next_sentence_label=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) sequence_output, pooled_output = outputs[:2] # 隐藏层输出,CLS Token Embeddings prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output) outputs = (prediction_scores, seq_relationship_score,) # 计算掩码语言损失 和 下一个句子预测损失 if masked_lm_labels is not None and next_sentence_label is not None: loss_fct = CrossEntropyLoss(ignore_index=-1) masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1)) next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1)) total_loss = masked_lm_loss + next_sentence_loss outputs = (total_loss,) + outputs return outputs # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)\"]},\"539\":{\"h\":\"其他下游任务\",\"t\":[\"Bert支持的下游任务图\"]},\"540\":{\"h\":\"问答任务\",\"t\":[\"在 BERT 的问答任务中，典型的输入是一个包含 问题（Question） 和 上下文（Context） 的文本对。例如：\",\"问题: “谁写了《哈姆雷特》？”上下文: “莎士比亚是英国文学史上最伟大的作家之一，他写了包括《哈姆雷特》、《麦克白》等著名悲剧。”\",\"输入格式（Tokenization 后的形式），在使用 BertTokenizer 编码后，输入会变成如下结构：\",\"[CLS] 问题 tokens [SEP] 上下文 tokens [SEP]\",\"BERT 的输出（Outputs），通过调用 self.bert(...)，你将得到一个包含多个元素的 tuple 输出：\",\"outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\",\"返回值形如：\",\"( sequence_output, # (batch_size, seq_length, hidden_size) pooled_output, # (batch_size, hidden_size) )\",\"主要输出项解释:\",\"✅ sequence_output: 最终每个 token 的表示\",\"形状：(batch_size, seq_length, hidden_size)\",\"是模型最后一层所有 token（包括问题和上下文）的隐藏状态。\",\"在问答任务中，我们主要使用它来预测答案的起始和结束位置。\",\"✅ pooled_output: 句子级别表示（不常用）\",\"形状：(batch_size, hidden_size)\",\"是 [CLS] token 经过一层全连接后的输出。\",\"在分类任务中更有用，在问答任务中一般不会使用这个输出。\",\"如何利用 BERT 输出做问答预测？\",\"在 BertForQuestionAnswering 中，使用了如下逻辑：\",\"logits = self.qa_outputs(sequence_output) # (batch_size, seq_length, 2) start_logits, end_logits = logits.split(1, dim=-1) # split into start and end start_logits = start_logits.squeeze(-1) # (batch_size, seq_length) end_logits = end_logits.squeeze(-1)\",\"qa_outputs 层的作用：\",\"是一个线性层：nn.Linear(config.hidden_size, 2)\",\"将每个 token 的 hidden_size 向量映射成两个分数：一个是该 token 作为答案开始的可能性，另一个是作为答案结束的可能性。\",\"输出解释：\",\"start_logits: 每个 token 是答案起点的得分（未归一化）。\",\"end_logits: 每个 token 是答案终点的得分。\",\"比如对于一个长度为 128 的序列，每个 token 都有一个对应的 start/end 分数：\",\"start_scores = torch.softmax(start_logits, dim=-1) # softmax 得到概率 end_scores = torch.softmax(end_logits, dim=-1) # 找出最可能是 start 和 end 的位置 start_index = torch.argmax(start_scores) end_index = torch.argmax(end_scores)\",\"如果 start_index <= end_index，那么可以组合这两个索引得到答案 span。\"]},\"541\":{\"h\":\"代码实现\",\"t\":[\"class BertForQuestionAnswering(BertPreTrainedModel): def __init__(self, config): super(BertForQuestionAnswering, self).__init__(config) self.num_labels = config.num_labels # 通常是 2，即 start 和 end self.bert = BertModel(config) self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, start_positions=None, end_positions=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids) sequence_output = outputs[0] # (batch,seq_len,hidden_size) ---> (batch,seq_len,2) logits = self.qa_outputs(sequence_output) start_logits, end_logits = logits.split(1, dim=-1) start_logits = start_logits.squeeze(-1) # (batch,seq_len) end_logits = end_logits.squeeze(-1) outputs = (start_logits, end_logits,) # 计算交叉熵损失 if start_positions is not None and end_positions is not None: # sometimes the start/end positions are outside our model inputs, we ignore these terms # ignored_index = seq_len ignored_index = start_logits.size(1) # clamp_ 是 PyTorch 中的一个方法，用于将张量中的值限制在指定的范围内。 # 它的语法是 tensor.clamp_(min, max) ，表示将张量中的值限制在 min 和 max 之间。 # 如果值小于 min ，则将其设置为 min ；如果值大于 max ，则将其设置为 max 。 start_positions.clamp_(0, ignored_index) end_positions.clamp_(0, ignored_index) # ignore_index: 用于指定在计算损失时忽略的标签索引。 loss_fct = CrossEntropyLoss(ignore_index=ignored_index) # 分别计算答案起始下标和结束下标预测得到的交叉熵损失 start_loss = loss_fct(start_logits, start_positions) end_loss = loss_fct(end_logits, end_positions) total_loss = (start_loss + end_loss) / 2 outputs = (total_loss,) + outputs return outputs # (loss), start_logits, end_logits\"]},\"542\":{\"h\":\"易混淆\",\"t\":[\"BERT 是一个 基于上下文编码（Contextual Encoder） 的模型，不是自回归生成器。它不会“生成”新的文本，而是对输入文本中每个 token 的角色进行分类（如判断哪个是答案的开始、结束）。所以最终的答案只能来自原始输入文本中的某一段子串。\",\"📚 详细解释\",\"✅ BERT 是一个 Encoder-only 模型\",\"BERT 只包含 Transformer 的 encoder 部分。\",\"它的作用是给定一个完整的句子（或两个句子），对每个 token 生成一个上下文相关的表示（contextualized representation）。\",\"它不具有生成能力，不能像 GPT 这样的 decoder-only 模型那样逐词生成新内容。\",\"🔍 QA 任务的本质：定位答案 span 而非生成答案\",\"在 SQuAD 这类抽取式问答任务中：\",\"答案必须是原文中的连续片段（span）。\",\"所以模型的任务是：\",\"给出问题和上下文；\",\"在上下文中找到最可能的答案起始位置和结束位置；\",\"最终答案就是上下文中这两个位置之间的字符串。\",\"BERT 做的就是这个定位任务，而不是重新生成一个新的答案。\",\"🧩 输入与输出的关系\",\"answer_tokens = input_ids[0][start_index : end_index + 1] answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\",\"这段代码的意思是：\",\"start_index 和 end_index 是模型预测出的答案的起始和结束位置。\",\"我们从原始输入的 input_ids 中取出对应的 token ID 子序列。\",\"使用 tokenizer 把这些 token ID 解码成自然语言文本。\",\"得到的就是答案。\",\"这其实就是在说：\",\"“根据你的理解，答案应该在这段文字中的第 X 到第 Y 个词之间，请把这部分原文告诉我。”\",\"🧪 举个例子\",\"假设原始上下文是：\",\"The capital of France is Paris.\",\"经过 Tokenizer 编码后可能是：\",\"[CLS] the capital of france is paris [SEP]\",\"如果模型预测 start_index=5，end_index=5，那么对应的就是单词 \\\"paris\\\"，这就是答案。\",\"⚠️ 注意事项\",\"不能超出上下文范围\",\"start/end positions 必须落在上下文部分（即 token_type_id == 1 的区域）。\",\"否则答案可能不合理（比如取到了问题部分的内容）。\",\"特殊 token 不计入答案\",\"[CLS], [SEP] 等会被 skip_special_tokens=True 自动跳过。\",\"无法处理不在原文中的答案\",\"如果正确答案没有出现在上下文中，BERT 无法“编造”出来。\",\"这是抽取式问答模型的局限性。\",\"💡 对比：生成式 vs 抽取式问答\",\"类型\",\"模型代表\",\"是否能生成新文本\",\"答案是否必须在原文中\",\"示例\",\"抽取式\",\"BERT\",\"❌\",\"✅\",\"答案是原文中的一段\",\"生成式\",\"T5 / BART / GPT\",\"✅\",\"❌\",\"答案可以是任意文本\",\"如果你希望模型能“自己写答案”，那就需要使用生成式模型。\",\"✅ 总结\",\"问题\",\"回答\",\"为什么答案来自 input_ids？\",\"因为 BERT 是编码器模型，只做抽取式问答，答案必须是原文中的一段文本。\",\"BERT 能不能自己生成答案？\",\"不能，BERT 不具备生成能力，只能对输入文本中的 token 做分类。\",\"如何获取答案？\",\"根据预测的 start/end index，从 input_ids 中提取 token，并用 tokenizer 解码成自然语言。\"]},\"543\":{\"h\":\"Token分类任务\",\"t\":[\"Token 分类任务是指对输入文本中的每个 token 进行分类，常见的应用场景包括：\",\"命名实体识别 (NER)\",\"词性标注 (POS)\",\"语义角色标注 (SRL)\",\"class BertForTokenClassification(BertPreTrainedModel): def __init__(self, config): super(BertForTokenClassification, self).__init__(config) self.num_labels = config.num_labels self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, config.num_labels) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) sequence_output = outputs[0] # (batch,seq_len,hidden_size) sequence_output = self.dropout(sequence_output) logits = self.classifier(sequence_output) # （batch,seq_len,num_labels） outputs = (logits,) if labels is not None: loss_fct = CrossEntropyLoss() # Only keep active parts of the loss if attention_mask is not None: active_loss = attention_mask.view(-1) == 1 active_logits = logits.view(-1, self.num_labels)[active_loss] active_labels = labels.view(-1)[active_loss] loss = loss_fct(active_logits, active_labels) else: loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) outputs = (loss,) + outputs return outputs # (loss), scores\"]},\"544\":{\"h\":\"多项选择任务\",\"t\":[\"多项选择任务是指给定一个问题和多个候选答案，模型需要从中选择最合适的答案。常见的应用场景包括：\",\"阅读理解任务\",\"问答系统中的候选答案选择\",\"对话系统中的候选回复选择\",\"在 多项选择题（Multiple Choice） 任务中，BERT 的输入组织形式与普通分类或问答任务略有不同。你需要为每个选项分别构造一个完整的 BERT 输入序列，并将它们组合成一个批次进行处理。\",\"✅ 假设你有一个问题 + 4 个选项：\",\"问题：谁写了《哈姆雷特》？ A. 雨果 B. 歌德 C. 莎士比亚 D. 托尔斯泰\",\"对于这样的多选问题，BERT 的输入方式是：\",\"对每一个选项，都单独构造一个 [CLS] + 问题 + [SEP] + 选项内容 + [SEP] 的输入序列。\",\"也就是说，模型会对每个选项分别编码 ，然后从中选出最合适的那个。\",\"class BertForMultipleChoice(BertPreTrainedModel): def __init__(self, config): super(BertForMultipleChoice, self).__init__(config) self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, 1) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None): # 获取选项个数 num_choices = input_ids.shape[1] # (batch_size, num_choices, seq_length) # 将选项展平，以便一起处理: (batch_size * num_choices, seq_length) input_ids = input_ids.view(-1, input_ids.size(-1)) attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) pooled_output = outputs[1] # (batch_size * num_choices, hidden_size) pooled_output = self.dropout(pooled_output) logits = self.classifier(pooled_output) # (batch_size * num_choices, 1) reshaped_logits = logits.view(-1, num_choices) # (batch_size , num_choices, 1) outputs = (reshaped_logits,) if labels is not None: loss_fct = CrossEntropyLoss() loss = loss_fct(reshaped_logits, labels) outputs = (loss,) + outputs return outputs # (loss), reshaped_logits, (hidden_states), (attentions)\",\"在前向传播中，会将这些输入展平，变成：\",\"input_ids.view(-1, seq_length) # (batch_size * num_choices, seq_length)\",\"这样就能让 BERT 对每个选项分别进行编码。\",\"BERT 输出后，再对每个选项做分类打分，最后重新 reshape 成 (batch_size, num_choices) 形式，用于计算交叉熵损失。\"]},\"545\":{\"h\":\"图解Transformer\",\"t\":[\"图解Transformer & 机器翻译实战\"]},\"546\":{\"h\":\"环境\",\"t\":[\"本文基于 The Annotated Transformer 所提供的代码展开进行讲解。\",\"环境搭建遵从如下步骤即可:\",\"git clone https://github.com/harvardnlp/annotated-transformer cd annotated-transformer conda create -n annotated-transformer python=3.9.22 conda activate annotated-transformer pip install -r requirements.txt\",\"MacOS 用户本地运行时，需要将 requirements.txt 文件中的 torch == 1.11.0+cu113 改为 torch==1.11.0，因为CUDA不支持MacOS。\"]},\"547\":{\"h\":\"背景\",\"t\":[\"RNN等模型的缺点是需要顺序计算，从而很难并行。因此出现了Extended Neural GPU、ByteNet和ConvS2S等网络模型。这些模型都是以CNN为基础，这比较容易并行。但是和RNN相比，它较难学习到长距离的依赖关系。\",\"本文的Transformer使用了Self-Attention机制，它在编码每一词的时候都能够注意(attend to)整个句子，从而可以解决长距离依赖的问题，同时计算Self-Attention可以用矩阵乘法一次计算所有的时刻，因此可以充分利用计算资源(CPU/GPU上的矩阵运算都是充分优化和高度并行的)。\"]},\"548\":{\"h\":\"模型架构\",\"t\":[\"Transformer 模型架构图\",\"Transformer 是一种基于自注意力机制(Self-Attention) 的神经网络架构,其由七大主要部分构成:\",\"Encoder-Decoder 结构\",\"编码器(Encoder)：将输入序列（如句子）转换为一系列高维向量表示。\",\"解码器(Decoder)：根据编码器的输出生成目标序列（如翻译后的句子）。\",\"多头自注意力机制（Multi-Head Self-Attention）\",\"自注意力机制是 Transformer 的核心，它允许模型在处理每个词时关注输入序列中的所有词。\",\"多头自注意力机制通过并行计算多个注意力头，捕捉不同子空间的信息，从而增强模型的表达能力。\",\"位置编码（Positional Encoding）\",\"由于 Transformer 不使用传统的循环或卷积结构，它通过位置编码将序列中词的位置信息注入到输入中。位置编码通常使用正弦和余弦函数生成。\",\"前馈神经网络（Feed-Forward Neural Network）\",\"在自注意力机制之后，每个位置的输出会通过一个独立的前馈神经网络进行进一步处理。\",\"残差连接与层归一化（Residual Connection & Layer Normalization）\",\"每个子层（如自注意力层和前馈层）都使用了残差连接和层归一化，以加速训练并提高模型的稳定性。\",\"掩码机制（Masking）\",\"在解码器中，使用掩码机制确保模型在生成目标序列时只能看到当前及之前的词，而不能看到未来的词。\",\"在输入序列长度不一致时，通过填充掩码（Padding Mask）屏蔽填充部分的信息。\",\"输出层\",\"解码器的最终输出通过一个线性层和 Softmax 函数生成目标序列的概率分布。\"]},\"549\":{\"h\":\"Encoder-Decoder 结构\",\"t\":[\"EncoderDecoder模型结构图\",\"class EncoderDecoder(nn.Module): def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): \\\"Take in and process masked src and target sequences.\\\" return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) def encode(self, src, src_mask): return self.encoder(self.src_embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\"]},\"550\":{\"h\":\"Generator\",\"t\":[\"Generator模型结构图\",\"class Generator(nn.Module): # 根据Decoder的隐状态输出一个词 # d_model是Decoder输出的大小，vocab是词典大小 def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) # 全连接再加上一个softmax def forward(self, x): return F.log_softmax(self.proj(x), dim=-1)\"]},\"551\":{\"h\":\"Encoder 结构\"},\"552\":{\"h\":\"SublayerConnection\",\"t\":[\"SublayerConnection模型结构图\",\"class SublayerConnection(nn.Module): \\\"\\\"\\\" LayerNorm + sublayer(Self-Attenion/Dense) + dropout + 残差连接 为了简单，把LayerNorm放到了前面，这和原始论文稍有不同，原始论文LayerNorm在最后。 \\\"\\\"\\\" def __init__(self, size, dropout): super(SublayerConnection, self).__init__() self.norm = LayerNorm(size) self.dropout = nn.Dropout(dropout) def forward(self, x, sublayer): \\\"sublayer是传入的参数，参考DecoderLayer，它可以当成函数调用，这个函数的有一个输入参数\\\" return x + self.dropout(sublayer(self.norm(x)))\"]},\"553\":{\"h\":\"EncoderLayer\",\"t\":[\"EncoderLayer模型结构图\",\"# 编码器层 = 自注意力子层 + 前馈层 class EncoderLayer(nn.Module): def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward # 自注意力子层 和 前馈层 各需要一个 self.sublayer = clones(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): \\\"Follow Figure 1 (left) for connections.\\\" x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) return self.sublayer[1](x, self.feed_forward)\"]},\"554\":{\"h\":\"Encoder\",\"t\":[\"Encoder模型结构图\",\"class Encoder(nn.Module): \\\"Core encoder is a stack of N layers\\\" def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): \\\"Pass the input (and mask) through each layer in turn.\\\" for layer in self.layers: x = layer(x, mask) return self.norm(x)\"]},\"555\":{\"h\":\"Decoder 结构\"},\"556\":{\"h\":\"DecoderLayer\",\"t\":[\"Decoder模型结构图\",\"# 解码器层 = 自注意力子层 + 源注意力子层 + 前馈层 class DecoderLayer(nn.Module): \\\"Decoder is made of self-attn, src-attn, and feed forward (defined below)\\\" def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward # 自注意力子层 + 源注意力子层 + 前馈层 各需要一个 self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): \\\"Follow Figure 1 (right) for connections.\\\" m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward)\"]},\"557\":{\"h\":\"Decoder\",\"t\":[\"Decoder模型结构图\",\"# 解码器 = N个解码器层 + 层归一化 class Decoder(nn.Module): \\\"Generic N layer decoder with masking.\\\" def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): # 输入,编码器隐藏层输出,源掩码,目标掩码 for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x)\"]},\"558\":{\"h\":\"多头自注意力\",\"t\":[\"多头自注意力计算流程图\",\"class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): \\\"Take in model size and number of heads.\\\" super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # We assume d_v always equals d_k self.d_k = d_model // h # 每个头64维 self.h = h # 8个头 self.linears = clones(nn.Linear(d_model, d_model), 4) # W_q,W_k,W_v,W_projection self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): \\\"Implements Figure 2\\\" if mask is not None: # Same mask applied to all h heads. mask = mask.unsqueeze(1) nbatches = query.size(0) # 1) Do all the linear projections in batch from d_model => h x d_k query, key, value = [ lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) # (batches,heads,seq_len,d_k) for lin, x in zip(self.linears, (query, key, value)) ] # 2) Apply attention on all the projected vectors in batch. x, self.attn = attention( query, key, value, mask=mask, dropout=self.dropout ) # 3) \\\"Concat\\\" using a view and apply a final linear. x = ( x.transpose(1, 2) .contiguous() .view(nbatches, -1, self.h * self.d_k) ) del query del key del value return self.linears[-1](x)\",\"def attention(query, key, value, mask=None, dropout=None): \\\"Compute 'Scaled Dot Product Attention'\\\" d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) # 广播: (1,1,1,10) ---> (1,8,10,10) p_attn = scores.softmax(dim=-1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn\"]},\"559\":{\"h\":\"CS224n-自然语言处理\"},\"560\":{\"h\":\"CS231n-计算机视觉\"},\"561\":{\"h\":\"吴恩达-大语言模型应用开发课程\"},\"562\":{\"h\":\"概率论基础知识\"},\"563\":{\"h\":\"概率论基础概念\",\"t\":[\"概率论基础概念(用到多少，学多少 =_=)\"]},\"564\":{\"h\":\"概率空间\",\"t\":[\"我们将概率空间定义为三元组 ，其中：\",\" 是样本空间，表示实验中所有可能的结果组成的集合；\",\" 是事件空间，即 的所有子集的集合；\",\" 是概率度量，是一个从事件 到 区间数值的映射（即 ），满足某些一致性要求。\"]},\"565\":{\"h\":\"离散随机变量\",\"t\":[\"最简单的情况是实验的结果是可数的。例如，掷一个三面骰子，其三个面分别标记为 “A”、“B”、“C”（为了简洁，我们用3面而不是6面）。此时：\",\"样本空间为 ，表示所有可能的实验结果；\",\"事件空间为 。\",\"其中每一个事件就是事件空间中的一个元素。例如：\",\"事件 表示骰子掷出面为 A 或 B；\",\"事件 表示骰子掷出面为 C。\",\"定义事件空间后，需要指定概率度量 ，即为事件空间中的每个集合赋予一个“权重”或“大小”。例如，设：\",\"，\",\"，\",\"。\",\"则复合事件的概率可通过求和得到，例如：\",\"。\",\"为简化记号，我们可以将每个样本空间中的结果映射为一个实数，这就定义了随机变量（random variable，记作 rv）：\",\"随机变量 = 一个把“事件结果”映射为“数值”的函数，它本身不随机，随机的是它作用的输入（样本 ）。\",\"，将每个结果 映射为实数 。\",\"例如，对三面骰子设：\",\"，\",\"，\",\"。\",\"再如，掷两次公平硬币，样本空间为：\",\"。\",\"设随机变量 表示“正面出现次数”，则：\",\"，\",\"，\",\"，\",\"。\",\"我们将随机变量可能的取值集合称为其状态空间，记作 。给定某个状态 ，定义：\",\"其中 ，称为 的原像。\",\"你有一个随机变量 ，它是一个函数，从样本空间 映射到实数；给定某个输出值 ，我们关心的是：随机变量等于这个值的概率是多少，即 。 但是：随机变量是函数，它本身不“随机”，真正随机的是实验结果 。所以，要知道“”的概率是多少，其实等价于问：\",\"有多少个 会导致 ，而这些 的总概率是多少？\",\"所以，我们这么定义：\",\"：是所有让 成立的样本点集合（这就是“原像”）；\",\"然后，：就是计算这些 的总概率。\",\"实验：投两次硬币\",\"定义随机变量 ：表示正面（H）的次数\",\"现在我们问：\",\"这等价于找出：\",\"哪些 会导致 ？\",\"答案是 \",\"如果每个 的概率都是 ，那么：\",\"这里， 称为概率质量函数（pmf，probability mass function）。继续上述例子，掷两次硬币的 pmf 为：\",\"，\",\"，\",\"。\",\"pmf 可用柱状图表示，也可用参数化函数表示。我们称 为随机变量 的概率分布。在上下文明确的情况下，常省略下标 。\",\"随机变量 把世界事件映射成数字；pmf 把这些数字映射成它们发生的概率。\"]},\"566\":{\"h\":\"连续随机变量\",\"t\":[\"我们也可以考虑结果为连续值的实验。这种情况下，假设样本空间是实数集合的子集：，并定义随机变量为恒等函数 。\",\"例如，测量某事件持续时间（单位：秒），设：\",\"。\",\"由于该集合是不可数的，无法像离散情形那样枚举所有子集。因此，我们需要借助Borel σ-代数（Borel sigma-field）来定义事件空间。其定义如下：\",\"集合 是一个 σ-代数（sigma-field）当且仅当：\",\"，且 ；\",\"若 ，则其补集 ；\",\"若 ，则 与 也属于 。\",\"Borel σ-代数是由半开区间 生成的最小 σ-代数。通过这些区间的并、交和补运算，我们可以得到：\",\"当我们讨论连续型随机变量（比如测量一个时间、距离或温度）时，它的样本空间是连续的，比如：\",\"在这种连续的空间里，所有可能的“事件”不是像离散情况那样简单地枚举出来的（比如 ），而可能是“无限多种可能的区间组合”。 比如我们可能想表示这些事件：\",\"“温度在 1 到 2 度之间” → 区间 (1, 2)\",\"“时间小于 5 秒” → 区间 \",\"“温度是 3 度或 7 度” → \",\"“测量值是无理数” → 这也算是一类事件！\",\"但是问题是：我们不能对“所有”这样的集合都定义概率！ 因为某些集合太“奇怪”或太“复杂”，会导致概率的定义出现矛盾或不收敛。 所以我们需要一个规则体系来规定“我们只对哪些集合定义概率” ——> 这个规则体系就是σ-代数（sigma-field）。\",\"σ-代数是一个集合的集合（简单理解：是你允许讨论的事件的全集合），它必须满足以下三条规则（你可以把它们理解成“合理事件空间”的要求）：\",\"包含整个样本空间和空集\",\"你总得允许“什么都不发生”（空事件）\",\"也得允许“一定会发生”（整个样本空间）\",\"如果你能谈某个事件，那它的补集你也得能谈\",\"比如：“温度小于 30 度” 这个事件存在，那“温度不小于 30 度”这个事件也应该存在\",\"如果你能谈一堆事件，那它们的并集和交集也得能谈\",\"比如你能谈“温度在 (0,1)”、“温度在 (1,2)”……，那“温度在 (0,2)”这种组合你也得能谈\",\"换句话说：σ-代数就是一种封闭的事件系统，允许你用基本事件构造更复杂事件，但不会跑出系统之外。 Borel σ-代数是专门为实数空间（）设计的一种 σ-代数，用来处理实数范围内的“正常”区间事件。 它的定义是：Borel σ-代数是由所有形如 的区间生成的最小 σ-代数。 也就是说，它从一些基本的“区间事件”出发，通过反复地做并集、交集、补集操作，构造出你所需要的所有“常见事件”。 比如：\",\"：开区间\",\"：闭区间\",\"、：半开区间\",\"：单点集\",\"任意有限/可数个区间并集交集……\",\"这些都属于 Borel σ-代数。 你可以把它理解成：我们定义概率，只在这些“结构正常的区间组合”上做，不碰那些太反直觉或病态的集合。 总结: Borel σ-代数是一种你可以安全地讨论概率的“事件集合体系”，它由一些基本区间（比如 ）出发，闭包得到所有“常规可测的集合”。\",\"在持续时间的例子中，可进一步限制事件空间只包含区间 ，其中 。\",\"为定义概率度量，我们为每个 赋一个非负权重 ，称为概率密度函数（pdf，probability density function）。\",\"对于事件 ，概率由积分给出：\",\"还可以定义累积分布函数（cdf）：\",\"从中可以计算区间概率：\",\"“概率分布”一词既可以指 pdf ，也可以指 cdf ，甚至指概率度量 本身。\",\"上述定义也可推广到多维空间 ，以及函数等更复杂的样本空间。\"]},\"567\":{\"h\":\"概率公理\",\"t\":[\"与事件空间相关联的概率规律，必须遵循概率公理（Kolmogorov 公理），具体如下：\",\"非负性（Non-negativity）：\",\"对任意事件 ，有\",\"规范性（Normalization）：\",\"整个样本空间的概率为 1：\",\"可加性（Additivity）：\",\"对于任意一列两两互不相交（即互斥）的事件 ，有\",\"在有限的情况下，比如只有两个互斥事件 和 ，上述公式简化为：\",\"这个公式对应的是“事件 或 发生”的概率（前提是这两个事件是互斥的）。\",\"从这些公理可以推导出一些常用结论：\",\"补集规则（Complement Rule）：\",\"其中， 表示事件 的补集。\",\"这个结论来自于：\",\"其他可推出的结论：\",\"：可通过反证法证明\",\"：可由补集规则推出，当 时，，所以 \",\"加法规则（Addition Rule）：\",\"对于任意两个事件（不要求互斥），有：\",\"这个公式适用于任意两个事件，即使它们可能有重叠。\"]},\"568\":{\"h\":\"条件概率\",\"t\":[\"考虑两个事件 和 。如果 ，则定义事件 在 已经发生的条件下的条件概率为：\",\"根据这个定义，可以得到乘法法则（multiplication rule）：\",\"条件概率衡量的是：在事件 已经发生的前提下，事件 发生的可能性有多大。\",\"然而，如果两个事件是无关的，那么一个事件的发生不会改变另一个事件的概率。更形式化地说，若满足以下条件，则称 与 是独立事件（independent events）：\",\"若 且 ，上式等价于：\",\"，或\",\"同理，若在某个事件 已知的条件下，事件 与 满足下式：\",\"则称 和 在给定 的条件下是条件独立的（conditionally independent）。\"]},\"569\":{\"h\":\"全概率公式（Law of total probability）\",\"t\":[\"根据条件概率的定义，还可以推导出全概率公式：\",\"若集合 构成样本空间 的一个划分（partition），那么对于任意事件 ，有：\",\"全概率公式: 一个事件总体概率 = 在不同情形下它发生的概率 × 各种情形本身的概率，加起来。 假设我们有一个检测疾病的筛查工具，我们要问：\",\"一个人检测为阳性的总体概率是多少？\",\"我们知道：\",\"人群中 1% 有病（记作 ），99% 无病（记作 ）；\",\"如果有病（），检测为阳性（）的概率是 0.9（即 ）；\",\"如果没病（），误报为阳性概率是 0.05（即 ）；\",\"问：一个人检测阳性的总体概率是多少？ 我们把“人是否患病”作为划分事件：\",\"：患病，\",\"：未患病，\",\"我们要算的事件是“检测为阳性”（）：\",\"所以，虽然你可能觉得“阳性概率应该很高”，但实际上总体阳性概率只有 5.85%，因为大多数人根本没病，而没病的人也有误报。\"]},\"570\":{\"h\":\"贝叶斯法则\",\"t\":[\"根据条件概率的定义，可以推导出贝叶斯法则（Bayes’ rule），也称为贝叶斯定理（Bayes’ theorem）。对于任意两个满足 且 的事件 和 ，有：\"]},\"571\":{\"h\":\"离散随机变量形式\",\"t\":[\"对于一个具有 个可能取值的离散随机变量 ，结合全概率公式，贝叶斯法则可以写为：\",\"其中：\",\"：称为先验概率（prior probability）\",\"：称为似然（likelihood）\",\"：称为后验概率（posterior probability）\",\"：为归一化常数，也叫作边缘似然（marginal likelihood）\"]},\"572\":{\"h\":\"连续随机变量形式\",\"t\":[\"对于一个连续型随机变量 ，贝叶斯法则写作：\",\"这就是贝叶斯法则在离散和连续两种情形下的表达方式，它提供了一种根据观测数据（事件 ）来更新我们对未知变量 的信念的机制，是整个贝叶斯推断的核心。\"]},\"573\":{\"h\":\"一些常见的概率分布\",\"t\":[\"在构建各种类型的模型时，我们会用到多种概率分布。以下小节总结了其中一些常见分布。\",\"交互式可视化网站： 🔗 https://ben18785.shinyapps.io/distribution-zoo/\"]},\"574\":{\"h\":\"离散分布\",\"t\":[\"本节讨论的是定义在（非负）整数子集上的一些离散型概率分布。\"]},\"575\":{\"h\":\"伯努利分布与二项分布（Bernoulli and Binomial distributions）\",\"t\":[\"设 ，二项分布（Binomial distribution）定义为：\",\"其中， 是从 个元素中选出 个的组合数（称为二项系数，读作 “N 选 x”）。\",\"如果 ，即 ，则二项分布退化为伯努利分布（Bernoulli distribution）：\",\"其中， 是分布的均值。\"]},\"576\":{\"h\":\"分类分布与多项分布（Categorical and Multinomial distributions）\",\"t\":[\"如果变量是多值离散型的（例如 ），我们可以使用分类分布（categorical distribution）：\",\"其中 表示选择类别 的概率， 是指示函数，表示当 时取 1，否则取 0。\",\"或者，也可以将 K 类变量 表示成一个 独热编码（one-hot）向量，这时分类分布可以写为：\",\"其中 表示当前样本属于第 类，其它元素为 0。\",\"如果 表示类别 在总共 次试验中出现的次数，那么就得到了多项分布（multinomial distribution）：\",\"其中，多项系数（multinomial coefficient）定义为：\"]},\"577\":{\"h\":\"泊松分布（Poisson distribution）\",\"t\":[\"设随机变量 。若 服从参数为 的泊松分布，记作 ，则其概率质量函数（pmf）为：\",\"其中， 是该分布的均值，同时也是方差，即：\"]},\"578\":{\"h\":\"负二项分布（Negative binomial distribution）\",\"t\":[\"假设我们有一个“盒子”（或称“容器”）中有 个球，其中：\",\" 个是红球\",\" 个是蓝球\",\"我们进行有放回抽样，直到抽出 个球。设 表示这 个球中蓝球的数量。可以证明：\",\"即： 服从二项分布。\"]},\"579\":{\"h\":\"转换视角：定义失败为红球，成功为蓝球\",\"t\":[\"现在我们重新定义抽红球为“失败”、抽蓝球为“成功”。我们继续抽球，直到观察到 次失败（红球）为止。\",\"设 表示在这过程中抽到的“成功”次数（即蓝球个数）。可以证明：\",\"也就是说， 服从 负二项分布（Negative binomial distribution），其概率质量函数定义为：\",\"其中 ，表示成功的次数。\",\"组合数 这表示从 次试验中选出 次成功的位置，剩下的是失败。 这里试验顺序重要，且第 次失败必须是第 次试验的结果（最后一次失败）。\",\"二项分布关注试验次数固定，成功次数随机。\",\"负二项分布关注成功次数固定，试验次数（失败次数）随机。\"]},\"580\":{\"h\":\"特殊情况说明\",\"t\":[\"如果 是实数，我们将组合数 替换为伽马函数表达式：\",\"利用了恒等式 。\"]},\"581\":{\"h\":\"数学期望与方差\",\"t\":[\"负二项分布的两个矩（均值和方差）为：\"]},\"582\":{\"h\":\"负二项分布的意义与优势\",\"t\":[\"这种含有两个参数的分布比泊松分布更具有建模灵活性，因为它可以单独控制均值与方差。这在模拟某些“传染性事件”时非常有用，例如某些事件之间是正相关的，它们的出现会导致比独立情形更大的方差。\",\"事实上，泊松分布是负二项分布的一个特例。可以证明：\",\"另一个特例是当 时，负二项分布变为几何分布（Geometric distribution）。\"]},\"583\":{\"h\":\"定义在实数上的连续分布\",\"t\":[\"在本节中，我们讨论一些定义在实数集合 上的一元连续分布，即 且 。\"]},\"584\":{\"h\":\"高斯分布（正态分布）\",\"t\":[\"最广泛使用的一元分布是高斯分布（Gaussian distribution），也叫正态分布（normal distribution）。\",\"高斯分布的概率密度函数（pdf）定义为：\",\"其中， 是归一化常数，用于确保整个密度函数的积分为 1。\",\"参数 表示分布的均值（mean），也是该分布的众数（mode）。\",\"参数 表示分布的方差（variance）。\",\"有时我们也会讨论高斯分布的精度（precision），即方差的倒数：。\",\"精度越高意味着分布越“窄”（即方差小），集中在 附近。\",\"高斯分布的累积分布函数（cdf）定义为：\",\"如果 、（即所谓的标准正态分布），我们简写为 。\"]},\"585\":{\"h\":\"半正态分布（Half-normal）\",\"t\":[\"在某些问题中，我们希望使用定义在非负实数上的分布。一种构造这类分布的方法是设定：\",\"由此诱导出的 的分布被称为半正态分布（half-normal distribution），其概率密度函数为：\",\"这个分布可以被看作是标准正态分布 在 0 处对折（folded over）后的结果。\"]},\"586\":{\"h\":\"学生 t 分布（Student t-distribution）\",\"t\":[\"高斯分布的一个问题在于它对离群点非常敏感，因为其概率密度随着与中心的平方距离增大而指数级衰减。一种更具鲁棒性的分布是 学生 t 分布（Student t-distribution），我们简称为Student 分布。它的概率密度函数（pdf）如下：\",\"其中：\",\" 是均值，\",\" 是尺度参数（注意：不是标准差），\",\" 被称为自由度（不过一个更恰当的术语可能是“正态程度” [Kru13]，因为当 趋于较大值时，该分布会表现得像高斯分布）。\",\"归一化常数 的表达式为：\",\"这里：\",\" 是Gamma 函数，定义为：\",\" 是Beta 函数，定义为：\"]},\"587\":{\"h\":\"柯西分布（Cauchy distribution）\",\"t\":[\"当 时，Student 分布被称为 柯西分布（Cauchy distribution） 或 洛伦兹分布（Lorentz distribution）。它的概率密度函数（pdf）定义为：\",\"其中：\",\"，即归一化常数。\",\"这个分布的一个显著特点是：它的尾部非常厚重（heavy tails），以至于定义均值的积分并不收敛（即没有期望值）。\",\"半柯西分布（half Cauchy distribution） 是一种基于均值为 0 的柯西分布进行“折叠”的版本，也就是说，它的概率密度函数全部集中在正实数轴上。\",\"因此，其形式为：\",\"更多分布使用到的时候再进行补充\"]},\"588\":{\"h\":\"高斯联合分布（Gaussian joint distributions）\",\"t\":[\"对于连续型随机变量，使用最广泛的联合概率分布是多元高斯分布（multivariate Gaussian 或 multivariate normal，简称 MVN）。\",\"这种分布之所以受欢迎，一方面是因为其数学处理非常方便，另一方面在许多实际问题中，高斯分布作为近似是相当合理的。事实上，在给定均值和协方差矩的约束下，高斯分布是熵最大的分布。鉴于它的重要性，本节将详细讨论高斯分布。\"]},\"589\":{\"h\":\"多元正态分布（The multivariate normal）\",\"t\":[\"在本节中，我们将详细介绍多元高斯分布，又称多元正态分布（MVN）。\"]},\"590\":{\"h\":\"定义（Definition）\",\"t\":[\"多元高斯分布的概率密度函数定义如下：\",\"其中：\",\" 是均值向量，\",\" 是 的协方差矩阵，\",\"归一化常数 保证整个 pdf 的积分为 1。\",\"指数中的表达式（忽略系数 -0.5）是数据向量 相对于均值 的马氏距离（Mahalanobis distance）平方，定义如下：\",\"在二维空间中，多元高斯分布被称为二维高斯分布（bivariate Gaussian distribution）。其概率密度函数可表示为：\",\"协方差矩阵形式为：\",\"其中：\",\" 是相关系数，定义为：\",\"图 2.8 展示了三种不同协方差矩阵下的二维多元高斯密度图：\",\"完全协方差矩阵（Full covariance matrix）: 有 个自由参数（由于 是对称的，所以除以 2）。\",\"对角协方差矩阵（Diagonal covariance matrix）: 只有 D 个自由参数，非对角线元素为 0，表示变量之间不相关。\",\"球形协方差矩阵（Spherical covariance matrix）: 也称为各向同性协方差矩阵（isotropic covariance matrix），只有一个自由参数 ，表示所有方向的方差相同。\",\"只有一个自由参数 ，表示所有方向的方差相同。\",\"当然可以，以下是你提供内容的逐段翻译与解释，保持原意清晰、结构一致：\"]},\"591\":{\"h\":\"高斯壳（Gaussian shells）\",\"t\":[\"在高维空间中，多元高斯分布的行为可能会显得非常反直觉。我们可以提出这样一个问题：\",\"如果我们从 中采样，其中 是维度数，我们应该预期这些样本大多会落在空间的哪里？\",\"由于概率密度函数的峰值（众数）位于原点 ，我们直觉上会认为：大多数样本应该靠近原点。\",\"然而，在高维空间中，高斯分布的“典型集合（typical set）”实际上是一个很薄的壳层或环带，其：\",\"与原点的距离为：\",\"壳层的厚度为：\"]},\"592\":{\"h\":\"直观解释如下：\",\"t\":[\"虽然密度函数以 的形式衰减 —— 即离原点越远，密度越小，但与此同时：\",\"球体的体积随半径 按 的速率快速增长；\",\"因为概率质量 = 密度 × 体积；\",\"所以，两者之间会出现一种“相互抵消的平衡点” —— 也就是在某个距离范围内，虽然密度在下降，但体积增加更快；\",\"大多数样本会集中在这个区域上 —— 也就是所谓的“高斯肥皂泡现象（Gaussian soap bubble phenomenon）”。\"]},\"593\":{\"h\":\"数学解释：为什么高斯样本集中在壳层上？\",\"t\":[\"考虑某个点 到原点的平方距离：\",\"期望平方距离为：\",\"方差为：\",\"相对标准差（变异系数）为：\",\"这意味着：\",\"尽管每个样本的距离是随机的；\",\"但当维度 越大时，它们的平方距离越来越集中在 附近；\",\"从而，距离本身越来越集中在 附近。\",\"这就是为什么我们说样本会集中在距离原点约为 的薄壳层上。\"]},\"594\":{\"h\":\"图像空间的含义（例如灰度图像）\",\"t\":[\"图 2.9b 展示了一些从如下高斯分布中采样的灰度图像：\",\"其中 是一张“全灰”的图片（每个像素亮度相同）。\",\"你可能以为既然是围绕全灰图采样，那么采样出来的图像应该也接近灰色。但事实恰恰相反：\",\"在高维图像空间中，几乎不可能采样到接近灰色的图像。\",\"这是因为样本几乎全部落在离 一定距离的“典型壳层”上，而不是密度最大的中心点（即灰色图像）。这非常反直觉，但却是高维高斯的真实现象。\"]},\"595\":{\"h\":\"概率论基础模型\",\"t\":[\"概率论基础模型(用到多少，学多少 =_=)\"]},\"596\":{\"h\":\"Bayes' rule\",\"t\":[\"Question One: What's the mean of Bayesian inference ?\",\"“推理”（inference）是指“从样本数据出发，得出带有一定置信度的一般性结论的行为”。术语“贝叶斯”（Bayesian）则用来指代那些使用概率理论来表示“置信度”（即确定程度）并利用 贝叶斯公式(Bayes’ rule) 根据观察数据更新置信度的方法。\",\"贝叶斯公式本身非常简单：它是一个用于计算在给定观测数据 情况下，某个未知（或隐藏）变量 可能取值的概率分布的公式：\",\"这个公式可以由以下恒等式直接推出：\",\"而这个恒等式又来自于概率的乘法法则（product rule）。\",\"在公式 (2.51) 中，术语 表示在我们看到任何数据之前，对 的可能取值的了解；这被称为先验分布（prior distribution）。如果 有 个可能的取值，那么 就是一个包含 个元素的向量，其中的概率和为 1。\",\"术语 表示在假设 的前提下，我们对可能出现的结果 的分布，这被称为观测分布（observation distribution）。当我们将其评估于实际观测结果 上时，就得到了函数 ，这被称为似然函数（likelihood）。需要注意的是，这其实是 的函数，因为 是已知的固定值，并且它不是一个概率分布，因为它的和不一定为 1 。\",\"将先验概率 与似然函数 相乘，可以得到未归一化的联合分布。我们可以通过除以 将其变为归一化分布，这个除数被称为边际似然（marginal likelihood），因为它是通过对未知量 进行边际化（即求和）得到的：\",\"通过对每个 计算 ，我们就得到了后验分布（posterior distribution），它表示我们在看到数据 之后，对 可能取值的最新信念状态。\",\"我们可以用一句话来总结贝叶斯公式：\",\"这里使用符号 （“正比于”）表示我们省略了分母，因为它只是一个与 无关的常数。\",\"使用贝叶斯公式，根据观测数据对某一感兴趣的未知量的分布进行更新的过程，被称为贝叶斯推理（Bayesian inference）或后验推理（posterior inference），也可以简称为概率推理（probabilistic inference）。\",\"Bayes 公式人话版本: “先有预期 + 接收信息 → 更新判断”\",\"：隐藏的“真相”或假设\",\"：你观测到的信息\",\"：你在没有观察任何信息前对 H 的先验信念\",\"：如果 H 是真的，你会看到这个信息的可能性\",\"：你在看到 Y 后对 H 的新判断（后验）\"]},\"597\":{\"h\":\"Inverse problems\",\"t\":[\"概率论的核心是：在已知世界状态 的前提下，预测某个结果 的分布。而逆概率问题关注的则是：通过观察结果 ，去推断世界的状态。我们可以把这看作是对 映射关系的反向求解。\",\"举个例子，设想我们要从一张二维图像 中推断出一个三维形状 。这是视觉场景理解中的一个经典问题。不幸的是，这是一个根本上的病态问题（ill-posed problem），如图 2.8 所示：同一个观测结果 ，可能对应多个潜在的隐藏状态 。同样地，我们也可以将自然语言理解看作是一个病态问题：听者必须从说话者表达出的（通常是模糊的）语言中，去推测其真正的意图 。\",\"为了解决这类反向问题，我们可以使用贝叶斯公式来计算后验概率，它描述了在观测到 的情况下，对各种可能世界状态 的概率分布。\",\"要实现这一点，需要给出：\",\"前向模型：描述在给定 的前提下，结果 是如何产生的；\",\"先验分布：用于排除或降低某些不太可能的世界状态。\"]},\"598\":{\"h\":\"组合分析\",\"t\":[\"组合分析\"]},\"599\":{\"h\":\"计数法则\",\"t\":[\"一共有 r 个实验 ， 第一个实验有 n1 种可能结果; 对应于第一个实验的每一种实验结果，第二个实验有 n2 种可能结果； 对应于头两个实验的每一种实验结果，第三个实验有 n3 种可能结果； 等等，那么，这 r 个实验一共有 n1 * n2 * ... * nr 种可能结果。\"]},\"600\":{\"h\":\"排列(考虑元素之间的顺序)\",\"t\":[\"n 个不同的元素，按任意顺序进行排列，总的排列方式共有: n*(n-1)(n-2)...32*1 = n!\",\"可用计数法则进行理解。\",\"n 个元素，如果其中 n1 个元素彼此相同，另 n2 个彼此相同，... ，nr 个也彼此相同，那么一共有 种不同的排列方式。\",\"总排列数 = 异排列数 * 每种异排列对应的重复排列数 --> 异排列数 = 总排列数 / 每种异排列对应的重复排列数\",\"例: 用 PEPPER 的 6 个字母进行排列，考察其中任一排列方式: PEPPER ，固定P,E,R的相对顺序不变，对P,E,R单独进行重排，会产生: 3! * 2! * 1! 种重复排列，如下所示:\"]},\"601\":{\"h\":\"组合(不考虑元素之间的顺序)\",\"t\":[\"n 个不同的元素中取 r 个\",\"总排列数 = 异排列数 * 每种异排列对应的全排列数 --> 异排列数 = 总排列数 / 每种异排列对应的全排列数\"]},\"602\":{\"h\":\"生成模型学习\"},\"603\":{\"h\":\"🚀 从零构建深度学习框架（一）：计算图与自动微分的起点\",\"t\":[\"1.TinyPytorch 第一阶段: 计算图与自动微分\",\"仓库链接: https://github.com/BinaryOracle/TinyPytorch 本节代码:\"]},\"604\":{\"h\":\"引言：揭开深度学习框架的神秘面纱\",\"t\":[\"深度学习框架中蕴藏着惊人的技术和有趣的机制，而本系列旨在揭开这些技术和机制的神秘面纱，帮助读者正确理解技术，体会它们的有趣之处。为此，本系列将带领读者从零开始创建一个深度学习框架——TinyPytorch。TinyPytorch尽量用最少的代码实现了现代深度学习框架的功能。第一阶段共包含10个步骤，让我们逐步构建起TinyPytorch的基础功能。\"]},\"605\":{\"h\":\"步骤1：作为\\\"箱子\\\"的变量\"},\"606\":{\"h\":\"变量的基本概念\",\"t\":[\"变量是TinyPytorch最重要的组成部分，可将其比作存放数据的\\\"箱子\\\"。在编程中，变量的作用是存储数据，而TinyPytorch的变量实现为Variable类，其核心功能是保存和管理数据。\"]},\"607\":{\"h\":\"代码实现\",\"t\":[\"class Variable: def __init__(self, data): self.data = data\"]},\"608\":{\"h\":\"使用示例\",\"t\":[\"import numpy as np data = np.array(1.0) x = Variable(data) print(x.data) # 输出 1.0 x.data = np.array(2.0) print(x.data) # 输出: 2.0\"]},\"609\":{\"h\":\"关键要点\",\"t\":[\"Variable类封装了NumPy的多维数组（ndarray）\",\"数据存储在data属性中\",\"支持数据的修改和读取\"]},\"610\":{\"h\":\"步骤2：创建变量的函数\"},\"611\":{\"h\":\"函数与计算图\",\"t\":[\"函数定义了变量之间的对应关系，通过计算图可以直观地表示变量与函数的关系。计算图用圆框表示变量，用方框表示函数，节点和箭头展示计算流程。\"]},\"612\":{\"h\":\"函数类的设计\",\"t\":[\"设计Function类作为基类，具体函数继承该类并实现forward方法。__call__方法处理输入输出的变量封装。\"]},\"613\":{\"h\":\"代码实现\",\"t\":[\"class Function: # 设定传入的input和返回的output均为Variable类型 def __call__(self, input): x = input.data y = self.forward(x) output = Variable(y) return output def forward(self, x): raise NotImplementedError() class Square(Function): def forward(self, x): return x ** 2\"]},\"614\":{\"h\":\"辅助函数\",\"t\":[\"为了方便使用，将函数类封装为Python函数：\",\"def square(x): return Square()(x)\"]},\"615\":{\"h\":\"步骤3：函数的连续调用\"},\"616\":{\"h\":\"复合函数的计算\",\"t\":[\"多个函数可以连续调用，形成复合函数。例如，计算，可以通过连续使用平方函数和指数函数实现。\"]},\"617\":{\"h\":\"代码示例\",\"t\":[\"class Exp(Function): def forward(self, x): return np.exp(x) def exp(x): return Exp()(x) A = Square() B = Exp() C = Square() x = Variable(np.array(0.5)) a = A(x) b = B(a) y = C(b) print(y.data) # 输出: 1.648721270700128\"]},\"618\":{\"h\":\"计算图的意义\",\"t\":[\"复合函数的计算图展示了函数的组合过程，而反向传播算法可以高效地求出每个变量的导数，这为自动微分奠定了基础。\"]},\"619\":{\"h\":\"步骤4：数值微分\"},\"620\":{\"h\":\"导数的定义\",\"t\":[\"导数是变化率的表示，函数在处的导数定义为：\"]},\"621\":{\"h\":\"数值微分的实现\",\"t\":[\"使用中心差分近似计算数值微分，公式为：\",\"本部分感兴趣的可以回顾高数中微分定义部分内容，中心差分近似可以问问GPT。\"]},\"622\":{\"h\":\"代码实现\",\"t\":[\"def numerical_diff(f, x, eps=1e-4): x0 = Variable(x.data - eps) x1 = Variable(x.data + eps) y0 = f(x0) y1 = f(x1) return (y1.data - y0.data) / (2 * eps)\"]},\"623\":{\"h\":\"数值微分的问题\",\"t\":[\"结果包含误差（精度丢失）\",\"计算成本高（尤其对于多变量函数）\",\"可用于梯度检验，验证反向传播的正确性\"]},\"624\":{\"h\":\"步骤5：反向传播的理论知识\"},\"625\":{\"h\":\"链式法则\",\"t\":[\"反向传播的理论基础是链式法则，对于复合函数，若由、、组成，则：\"]},\"626\":{\"h\":\"反向传播的方向\",\"t\":[\"反向传播按从输出到输入的顺序计算导数，与正向传播方向相反。这种方式在输出为标量的情况下计算效率更高，适合机器学习中损失函数的优化。\",\"反向传播用于计算输入X对输出Y大小变化的影响，如果没有Function作用于X上，即Y=X，那么影响因子R恒为1；\",\"如果存在Function作用在X上，那么Function可能会放大或者缩小X对输出Y大小变化的影响，也就是改变X对于Y的影响因子，此时R= 1 * F放缩因子；\",\"如果存在多个Function先后作用在X上，即Y=Fn(...F2(F1(X)))，那么此时影响因子R= 1 * F1放缩因子 * F2放缩因子 * ... * Fn放缩因子；\",\"对于Y=F2(F1(x)) + F3(x) , 此时影响因子R = 1 * F1放缩因子 * F2放缩因子 + 1 * F3放缩因子；\",\"实际此处的函数放缩因子也称为函数的导数，多元场景下也称为偏导数。\"]},\"627\":{\"h\":\"计算图的反向传播\",\"t\":[\"反向传播过程中，导数从输出端向输入端传播，每个函数节点需要计算其导数，并将结果传递给前一层变量。\"]},\"628\":{\"h\":\"步骤6：手动进行反向传播\"},\"629\":{\"h\":\"扩展Variable类\",\"t\":[\"为Variable类添加grad属性，用于存储导数：\",\"class Variable: def __init__(self, data): self.data = data self.grad = None\"]},\"630\":{\"h\":\"扩展Function类\",\"t\":[\"为Function类添加反向传播方法backward，并在__call__方法中保存输入输出变量：\",\"class Function: def __call__(self, input): x = input.data y = self.forward(x) output = Variable(y) self.input = input self.output = output return output def backward(self, gy): raise NotImplementedError()\",\"反向传播的核心是依据链式法则，沿计算图反向推导各变量的导数。而链式法则的计算需要知晓每个函数在正向传播时的输入值和输出值。\",\"例如，对于函数 ，其导数 依赖于输入x的具体值（如平方函数 的导数 ，需要知道正向传播时的 值）。记录input后，反向传播时可直接获取这些值，避免重复计算或存储额外数据。\",\"再例如，对于函数 ，其导数依赖于输出值。 函数的表达式为 ，其导数 ，即需要知道正向传播时的输出值 才能计算导数。\"]},\"631\":{\"h\":\"具体函数的反向传播\",\"t\":[\"以平方函数为例：\",\"class Square(Function): def backward(self, gy): x = self.input.data gx = 2 * x * gy return gx\"]},\"632\":{\"h\":\"反向传播的执行\",\"t\":[\"按反向顺序调用各函数的backward方法，手动传递导数：\",\"y.grad = np.array(1.0) b.grad = C.backward(y.grad) a.grad = B.backward(b.grad) x.grad = A.backward(a.grad)\"]},\"633\":{\"h\":\"步骤7：反向传播的自动化\"},\"634\":{\"h\":\"建立变量与函数的连接\",\"t\":[\"在Variable类中添加creator属性，记录创建该变量的函数；在Function类的__call__方法中，设置变量的creator：\",\"class Variable: def set_creator(self, func): self.creator = func class Function: def __call__(self, input): # 其他代码... output.set_creator(self)\"]},\"635\":{\"h\":\"自动反向传播的实现\",\"t\":[\"在Variable类中添加backward方法，通过递归或循环遍历计算图，自动执行反向传播：\",\"class Variable: def backward(self): f = self.creator if f is not None: x = f.input x.grad = f.backward(self.grad) x.backward()\",\"你品，你细品 ~ 🤔 (目前实现的版本无法实现上图中计算图多分支的结构，只能实现一条竖线形状的计算图结构的反向传播)\"]},\"636\":{\"h\":\"步骤8：从递归到循环\"},\"637\":{\"h\":\"递归实现的问题\",\"t\":[\"递归实现的反向传播在计算图较深时可能导致栈溢出，且效率较低。\"]},\"638\":{\"h\":\"循环实现反向传播\",\"t\":[\"使用列表保存待处理的函数，通过循环替代递归：\",\"class Variable: def backward(self): funcs = [self.creator] while funcs: f = funcs.pop() # 弹出列表尾部元素 x, y = f.input, f.output x.grad = f.backward(y.grad) if x.creator is not None: funcs.append(x.creator) # 追加到列表尾部\",\"由于目前只支持竖线形状的计算图，因此func列表同一时刻最多只存在一个func\"]},\"639\":{\"h\":\"循环实现的优势\",\"t\":[\"避免栈溢出\",\"提高执行效率\",\"更容易处理复杂计算图\"]},\"640\":{\"h\":\"步骤9：让函数更易用\"},\"641\":{\"h\":\"函数的Python化\",\"t\":[\"将函数类封装为Python函数，便于直接调用：\",\"def square(x): return Square()(x) def exp(x): return Exp()(x)\"]},\"642\":{\"h\":\"自动设置梯度\",\"t\":[\"在Variable的backward方法中自动初始化梯度：\",\"class Variable: def backward(self): if self.grad is None: self.grad = np.ones_like(self.data) # 其他代码...\"]},\"643\":{\"h\":\"数据类型检查\",\"t\":[\"确保Variable只接受ndarray实例，提高代码健壮性：\",\"class Variable: def __init__(self, data): if data is not None: if not isinstance(data, np.ndarray): raise TypeError(f'{type(data)} is not supported') self.data = data # 其他代码...\"]},\"644\":{\"h\":\"步骤10：测试\"},\"645\":{\"h\":\"单元测试\",\"t\":[\"使用Python的unittest模块编写测试用例，验证函数的正向传播和反向传播：\",\"import unittest class SquareTest(unittest.TestCase): def test_forward(self): x = Variable(np.array(2.0)) y = square(x) self.assertEqual(y.data, np.array(4.0)) def test_backward(self): x = Variable(np.array(3.0)) y = square(x) y.backward() self.assertEqual(x.grad, np.array(6.0))\"]},\"646\":{\"h\":\"梯度检验\",\"t\":[\"将数值微分的结果与反向传播的结果进行比较，验证反向传播的正确性：\",\"def numerical_diff(f, x, eps=1e-4): # 实现如前所述 class SquareTest(unittest.TestCase): def test_gradient_check(self): x = Variable(np.random.rand(1)) y = square(x) y.backward() num_grad = numerical_diff(square, x) self.assertTrue(np.allclose(x.grad, num_grad))\"]},\"647\":{\"h\":\"测试的重要性\",\"t\":[\"确保代码功能正确性\",\"发现潜在bug\",\"支持代码重构和扩展\"]},\"648\":{\"h\":\"第一阶段总结\",\"t\":[\"通过第一阶段的10个步骤，我们从零开始构建了TinyPytorch框架的基础功能：\",\"实现了变量和函数的基本结构\",\"完成了自动微分的核心算法——反向传播\",\"实现了数值微分作为梯度检验工具\",\"优化了反向传播的实现，从递归改为循环\",\"提高了框架的易用性和健壮性\",\"建立了测试机制，确保代码质量\",\"此时的TinyPytorch已经具备了自动微分的能力，可以处理简单的计算图，并为后续的功能扩展奠定了坚实基础。接下来的阶段将进一步扩展TinyPytorch，使其支持更复杂的计算和神经网络的构建。\"]},\"649\":{\"h\":\"🧮 从零构建深度学习框架（二）：自动反向传播与计算图进阶\",\"t\":[\"1.TinyPytorch 第二阶段: 自动反向传播与框架基础能力提升\",\"仓库链接: https://github.com/BinaryOracle/TinyPytorch 本节代码:\"]},\"650\":{\"h\":\"引言：从自动微分迈向通用框架\",\"t\":[\"深度学习框架之所以强大，不仅因为其前向计算功能，更因为其背后复杂而精妙的自动微分系统。本系列文章试图揭开这些机制背后的本质，帮助读者从零搭建属于自己的深度学习引擎。\",\"第一阶段中，我们构建了变量（Variable）与函数（Function）类，实现了基础的计算图结构与反向传播流程，并通过链式法则自动推导了导数。\",\"第二阶段将从第11步延续，全面扩展 TinyPytorch 的核心能力。在第一阶段，我们完成了计算图与手动反向传播的雏形。而在本阶段，我们将继续揭开深度学习框架的核心机制：实现真正意义上的自动反向传播、多输入/输出处理、计算图遍历优化、梯度累加、配置控制等。通过这 14 个步骤，TinyPytorch 将蜕变为一个更通用、更高效、更接近真实框架的自动微分系统。\"]},\"651\":{\"h\":\"步骤11: 多输入与多输出\",\"t\":[\"现实中的神经网络操作往往不仅仅接受一个输入，也可能产生多个输出，例如加法、乘法、切片、拼接等操作。因此我们扩展 Function 类以支持 可变参数输入与输出列表。\",\"class Function: def __call__(self, *inputs): xs = [x.data for x in inputs] ys = self.forward(*xs) if not isinstance(ys, tuple): ys = (ys,) outputs = [Variable(as_array(y)) for y in ys] for output in outputs: output.set_creator(self) self.inputs = inputs self.outputs = outputs return outputs if len(outputs) > 1 else outputs[0] def forward(self, xs): raise NotImplementedError() def backward(self, gys): raise NotImplementedError()\",\"这一扩展使我们的函数定义更接近 NumPy 风格，支持多个输入与输出，提高了灵活性。\",\"为了更好地支持多输入函数，我们学习和利用了 Python 中的几个语法技巧：\",\"*args：接收任意个数的位置参数，用于函数的输入接口；\",\"*xs 解包语法：在调用如 forward(*xs) 时展开变量列表；\",\"tuple 判断：让返回值始终封装为元组，统一处理逻辑。\"]},\"652\":{\"h\":\"步骤12: backward 的多输入实现\",\"t\":[\"在 Variable.backward() 中支持多输出节点：\",\"class Variable: ... def backward(self): if self.grad is None: self.grad = np.ones_like(self.data) funcs = [self.creator] while funcs: f = funcs.pop() gys = [output.grad for output in f.outputs] gxs = f.backward(*gys) if not isinstance(gxs,tuple): gxs = (gxs,) for x, gx in zip(f.inputs, gxs): x.grad = gx if x.creator is not None: funcs.append(x.creator)\",\"这里 gxs 和 f.inputs 的每个元素都是一一对应的。准确来说, 如果有第i个元素, 那么f.input[i]的导数值对应于gxs[i]。于是代码中使用zip 函数和 for 循环来设置每一对的导数。以上就是Variable类的新 backward 方法。\"]},\"653\":{\"h\":\"步骤13: 重置导数\",\"t\":[\"当我们使用同一个变量分别进行多次计算时，我们希望每次计算都能得到正确的导数。为了实现这一点，我们需要在每次计算之前将导数重置为0。\",\"下面为 Variable 类提供一个新的方法，实现变量导数的重置。\",\"class Variable: ... def cleargrad(self): self.grad = None\"]},\"654\":{\"h\":\"步骤14: 共享变量与梯度累加\",\"t\":[\"当某个变量被多次用作输入时（例如 z = x + x），反向传播过程中它的梯度应累加。\",\"这是因为同一个变量对输出的影响路径有多条。如果我们不进行累加，而是直接覆盖梯度值，就会导致只有最后一条路径上的梯度被保留，其他路径上的梯度信息将丢失。\",\"例如：\",\"x = Variable(np.array(3.0)) y = add(x, x) # x 被用作两次输入\",\"在反向传播过程中，x 的梯度来自两个路径：一条是第一个 x 到 y，另一条是第二个 x 到 y。如果我们不对这两个梯度求和，只保留一个，那么 x 的真实导数就会被低估一半，最终影响训练结果。\",\"因此，在实现中应当如下处理：\",\"class Variable: ... def backward(self): if self.grad is None: self.grad = np.ones_like(self.data) funcs = [self.creator] while funcs: f = funcs.pop() gys = [output.grad for output in f.outputs] gxs = f.backward(*gys) if not isinstance(gxs,tuple): gxs = (gxs,) for x, gx in zip(f.inputs, gxs): if x.grad is None: x.grad = gx else: x.grad = x.grad + gx # 正确的累加方式 if x.creator is not None: funcs.append(x.creator)\",\"这样才能确保所有路径的贡献都被纳入最终的梯度值。\"]},\"655\":{\"h\":\"步骤15: 梯度重复累加的问题\",\"t\":[\"在步骤14中，我们通过变量梯度非空则进行累加的改动，解决了共享变量梯度重置的问题，但是这一改动也引发了另一个问题：梯度重复累加。 观察下图，由于目前Variable.backward()的实现逻辑总是将函数追加到待处理列表的末尾，同时又优先处理列表末尾的函数，为\\\"先进先出\\\"的实现逻辑，因此对于存在多分支的复杂计算图而言，它也总是会沿着某个分支进行DFS直到\\\"叶节点\\\"，这会导致如下图所示的共享变量a的梯度被重复累加，导致x变量梯度计算错误。\",\"例子如下:\",\"x = Variable(np.array(2.0)) a = square(x) b = square(a) c = square(a) y = add(b, c) y.backward()\",\"上面的问题本质是因为函数调用顺序错误导致的，对于共享变量，我们要先计算出其梯度后，才能继续计算其前向的梯度，其实也就是按照拓扑排序的方式去遍历计算图。\"]},\"656\":{\"h\":\"步骤16: \\\"辈分\\\"机制\",\"t\":[\"为了解决上述的问题，我们可以采用拓扑排序，但是这里为了方便理解，我们采用更加暴力的“辈分排序”机制确保函数调用顺序的正确执行。\",\"我们可以获取到哪个函数生成了哪个变量，这就构成了函数与变量的\\\"辈分\\\"关系；我们可以通过变量的辈分来设置其创建者函数的辈分，如下图所示:\",\"我们在Variable类和Function类中增加实例变量generation，用其来表示函数(或变量)属于哪一代。\",\"class Variable: def __init__ (self , data): if data is not None: if not isinstance(data , np.ndarray): raise TypeError( '{} is not supported' .format(type(data))) self.data = data self.grad = None self.creator = None self.generation = 0 def set_creator(self, func): self.creator = func self.generation = func.generation + 1 ...\",\"Variable 类将 generation 初始化为 0。之后, 当 set_creator 方法被调用时, 它将 generation 的值设置为父函数的 generation 值加1。\",\"Function 类的 generation 被设置为多个输入变量中最大的generation的值。\",\"class Function: def __call__(self, *inputs): xs = [x.data for x in inputs] ys = self.forward(*xs) if not isinstance(ys, tuple): ys = (ys,) outputs = [Variable(as_array(y)) for y in ys] self.generation = max([x.generation for x in inputs]) for output in outputs: output.set_creator(self) self.inputs = inputs self.outputs = outputs return outputs if len(outputs) > 1 else outputs[0]\",\"通过以上修改, 在进行普通计算(即正向传播)时, 变量和函数中会设置好 generation 的值，我们便可以通过\\\"辈分\\\"按序取出元素。\",\"如上图所示，此时我们可以通过辈分来确保函数B和C先于函数A取出；我们只需要如下修改Variable变量的backward方法即可完成按照辈分获取函数的逻辑:\",\"class Variable: ... def backward(self): if self.grad is None: self.grad = np.ones_like(self.data) funcs = [] seen_set = set() # 防止同一个函数被多次添加到func列表中，从而避免一个函数的backward方法被错误地多次调用 def add_func(f): if f not in seen_set: funcs.append(f) seen_set.add(f) funcs.sort(key=lambda x : x.generation) add_func(self.creator) while funcs: f = funcs.pop() # 每次取出辈分最大的函数 gys = [output.grad for output in f.outputs] gxs = f.backward(*gys) if not isinstance(gxs,tuple): gxs = (gxs,) for x, gx in zip(f.inputs, gxs): if x.grad is None: x.grad = gx else: x.grad = x.grad + gx # 正确的累加方式 if x.creator is not None: add_func(x.creator)\"]},\"657\":{\"h\":\"步骤17: 循环引用与内存释放\",\"t\":[\"Python的内存管理主要依靠两种机制：引用计数和分代垃圾回收（GC）。在深度学习框架中，合理的内存管理至关重要，尤其当处理大规模数据时，不当的内存管理可能导致内存泄漏或程序崩溃。\",\"引用计数：Python通过跟踪对象的引用次数来管理内存。当对象的引用计数为0时，会被立即回收。以下情况会增加引用计数：\",\"使用赋值运算符（如a = obj()）\",\"向函数传递参数（如f(a)）\",\"向容器对象（列表、元组等）添加元素。\",\"循环引用指对象之间相互引用，导致引用计数无法归零，从而无法被自动回收。例如：\",\"a = Obj() b = Obj() c = Obj() a.b = b b.c = c c.a = a a = b = c = None # 此时a、b、c的引用计数仍为1，无法回收\",\"这种情况下，尽管用户不再访问这些对象，但因循环引用，引用计数无法降至0，需依赖垃圾回收机制处理。\",\"虽然Python的垃圾回收（GC）机制可以处理循环引用对象，但在对内存敏感的场景下依然存在问题，主要原因如下：\",\"回收时机非即时性: GC是一种后台机制，通常在内存不足或满足特定条件时才会触发，而非实时回收循环引用对象。例如，当TinyPytorch处理大量神经网络计算时，若存在循环引用，GC可能无法及时释放内存，导致内存占用持续升高，甚至引发内存不足错误。\",\"性能开销较高: GC需要扫描整个对象图来检测循环引用，这一过程对大规模计算框架（如TinyPytorch）而言可能产生显著的性能损耗。尤其在神经网络训练中，频繁的GC操作会影响计算效率，而弱引用可通过避免循环引用直接解决问题，减少GC触发频率。\",\"TinyPytorch中的循环引用:\",\"当前TinyPytorch框架中，Function和Variable实例存在循环引用：\",\"Function实例引用输入和输出的Variable实例（self.inputs和self.outputs）。\",\"Variable实例通过creator属性引用创建它的Function实例。\",\"解决方案：弱引用\",\"弱引用的优势:\",\"避免强引用导致的内存滞留: Function和Variable之间原本存在强引用循环（Function引用Variable，Variable通过creator引用Function）。若使用强引用，即使计算图不再被用户访问，循环引用仍会导致对象无法释放。而弱引用不会增加对象的引用计数，当用户不再引用Variable时，对象可被立即回收，无需等待GC。\",\"符合框架设计需求: TinyPytorch的计算图需要动态构建和销毁，弱引用能确保计算图在使用完毕后自动释放内存。例如，当用户执行完一次前向传播和反向传播后，计算图中的中间变量（如Function和临时Variable）应被及时回收，以释放内存供后续计算使用。\",\"GC与弱引用的互补关系\",\"GC作为兜底机制：GC可处理开发者未显式解决的循环引用，但无法替代弱引用在框架设计中的针对性优化。\",\"弱引用作为主动优化：在TinyPytorch中，通过弱引用主动打破Function与Variable之间的循环引用，能更精准地控制内存释放时机，避免因GC延迟导致的内存问题。\",\"使用Python的weakref模块创建弱引用，避免增加对象的引用计数：\",\"修改Function类：\",\"import weakref class Function: def __call__(self, *inputs): # 原有代码... self.outputs = [weakref.ref(output) for output in outputs] # 将强引用改为弱引用\",\"弱引用不会增加对象的引用计数，当Variable实例不再被其他对象引用时，会被正常回收。\",\"修改Variable类的backward方法：\",\"class Variable: def backward(self): # 原有代码... gys = [output().grad for output in f.outputs] # 通过output()获取实际对象\",\"使用output()从弱引用中获取Variable实例，避免直接引用导致循环。\",\"总结:\",\"循环引用会导致Python对象无法被正常回收，需通过弱引用解决。\",\"在TinyPytorch中，将Function对Variable的引用改为弱引用，避免内存泄漏。\",\"优化后的内存管理确保框架在处理大规模计算时的稳定性和效率。\"]},\"658\":{\"h\":\"步骤18: 优化内存消耗\",\"t\":[\"优化反向传播的内存消耗: TinyPytorch当前的反向传播会保留所有变量的导数，但在实际应用中，仅终端变量的导数需要被保留，中间变量的导数往往无用。为此，我们引入释放中间变量导数的机制。\",\"修改Variable.backward方法: 添加retain_grad参数，若为False（默认），则反向传播后清除中间变量的导数。\",\" class Variable: def backward(self, retain_grad=False): if self.grad is None: self.grad = np.ones_like(self.data) funcs = [] seen_set = set() def add_func(f): if f not in seen_set: funcs.append(f) seen_set.add(f) funcs.sort(key=lambda x: x.generation) add_func(self.creator) while funcs: f = funcs.pop() gys = [output().grad for output in f.outputs] gxs = f.backward(*gys) if not isinstance(gxs, tuple): gxs = (gxs,) for x, gx in zip(f.inputs, gxs): if x.grad is None: x.grad = gx else: x.grad = x.grad + gx if x.creator is not None: add_func(x.creator) if not retain_grad: for y in f.outputs: y().grad = None # 清除中间变量的导数\",\"验证案例：\",\" x0 = Variable(np.array(1.0)) x1 = Variable(np.array(1.0)) t = add(x0, x1) y = add(x0, t) y.backward() # retain_grad默认False print(y.grad, t.grad) # 输出：None None（中间变量导数被清除） print(x0.grad, x1.grad) # 输出：2.0 1.0（终端变量导数保留）\",\"中间变量y和t的导数被立即释放，减少内存占用。\",\"禁用反向传播的模式优化: 在推理阶段（如模型预测），无需计算导数，可通过禁用反向传播模式进一步节省内存。\",\"创建配置类Config：\",\"class Config: enable_backprop = True # 控制反向传播是否启用\",\"修改Function.__call__方法： 仅当Config.enable_backprop为True时，保留反向传播所需的计算图连接：\",\"class Function: def __call__(self, *inputs): xs = [x.data for x in inputs] ys = self.forward(*xs) if not isinstance(ys, tuple): ys = (ys,) outputs = [Variable(as_array(y)) for y in ys] if Config.enable_backprop: self.generation = max([x.generation for x in inputs]) for output in outputs: output.set_creator(self) self.inputs = inputs self.outputs = [weakref.ref(output) for output in outputs] return outputs if len(outputs) > 1 else outputs[0]\",\"模式切换示例：\",\"# 启用反向传播（默认模式） x = Variable(np.ones((100, 100, 100))) y = square(square(square(x))) # 保留中间结果，内存占用高 # 禁用反向传播（推理模式） Config.enable_backprop = False x = Variable(np.ones((100, 100, 100))) y = square(square(square(x))) # 不保留中间结果，内存占用低\",\"禁用模式下，计算完成后中间变量立即释放，内存使用量大幅降低。\",\"使用with语句便捷切换模式: 为避免手动修改Config属性，可通过contextlib模块实现with语句上下文管理。\",\"实现using_config函数：\",\"import contextlib @contextlib.contextmanager def using_config(name, value): old_value = getattr(Config, name) setattr(Config, name, value) try: yield finally: setattr(Config, name, old_value)\",\"封装no_grad函数：\",\"def no_grad(): return using_config('enable_backprop', False) # 使用示例 with no_grad(): x = Variable(np.array(2.0)) y = square(x) # 禁用反向传播，不构建计算图\",\"退出with块后，模式自动恢复，避免因忘记重置配置导致的错误。\",\"优化效果总结:\",\"内存释放机制：通过retain_grad参数及时清除中间变量导数，避免内存长期占用。\",\"推理模式优化：禁用反向传播后，计算过程不保留计算图连接，适合无需梯度的场景（如模型预测）。\",\"工程实践：with no_grad()语法简洁，便于在训练和推理阶段灵活切换，提升代码可读性和鲁棒性。\"]},\"659\":{\"h\":\"步骤19: Variable 功能增强\",\"t\":[\"为了便于区分和调试，在Variable类中添加name属性，支持为变量设置自定义名称：\",\"class Variable: def __init__(self, data, name=None): if data is not None: if not isinstance(data, np.ndarray): raise TypeError(f'{type(data)} is not supported') self.data = data self.name = name # 新增名称属性 self.creator = None self.grad = None self.generation = 0\",\"使用示例：\",\"x = Variable(np.array(1.0), name='input_x') y = Variable(np.array(2.0), name='input_y')\",\"变量名称可在计算图可视化等场景中显示，提升调试效率。\",\"使Variable实例具备ndarray的行为特征，隐藏数据封装细节：\",\"添加shape、ndim、size、dtype属性:\",\"class Variable: @property def shape(self): return self.data.shape # 获取数据形状 @property def ndim(self): return self.data.ndim # 获取维度数 @property def size(self): return self.data.size # 获取元素总数 @property def dtype(self): return self.data.dtype # 获取数据类型\",\"示例验证：\",\"x = Variable(np.array([[1, 2, 3], [4, 5, 6]])) print(x.shape) # 输出：(2, 3) print(x.ndim) # 输出：2 print(x.size) # 输出：6 print(x.dtype) # 输出：int64\",\"变量实例可直接访问ndarray的核心属性，使用户无需关心data属性。\",\"实现__len__方法：\",\"class Variable: def __len__(self): return len(self.data) # 返回第1维度的元素数\",\"示例：\",\"x = Variable(np.array([1, 2, 3, 4])) print(len(x)) # 输出：4 y = Variable(np.array([[1, 2], [3, 4]])) print(len(y)) # 输出：2\",\"自定义打印格式：\",\" class Variable: def __repr__(self): if self.data is None: return 'variable(None)' data_str = str(self.data).replace('\\\\n', '\\\\n' + ' ' * 9) return f'variable({data_str})'\",\"输出效果：\",\"x = Variable(np.array([1, 2, 3])) print(x) # 输出：variable([1 2 3]) y = Variable(np.array([[1, 2], [3, 4]])) print(y) # 输出： # variable([[1 2] # [3 4]])\",\"打印时自动对齐多行数据，并标注“variable”前缀，便于识别变量类型。\",\"可继续添加ndarray的其他属性（如T转置、flat迭代器等），或实现__getitem__、__setitem__方法以支持数组索引，进一步强化变量的“透明箱子”特性。\"]},\"660\":{\"h\":\"步骤20–22: 运算符重载\",\"t\":[\"乘法运算的实现与运算符重载：\",\"在深度学习框架中，乘法运算是最基础的操作之一。为了让Variable实例支持自然的乘法表达式（如a * b），我们需要实现Mul类来处理正向传播和反向传播，并将其绑定到*运算符上。\",\"正向传播：计算两个输入变量的乘积，即y = x0 * x1。\",\"反向传播：根据导数公式，若y = x0 * x1，则，。因此，反向传播时需要将上游传来的梯度gy分别乘以x1和x0，传递给下游变量。\",\"class Mul(Function): def forward(self, x0, x1): y = x0 * x1 return y def backward(self, gy): x0, x1 = self.inputs[0].data, self.inputs[1].data return gy * x1, gy * x0 # 将梯度分别乘以x1和x0\",\"为了方便使用，我们将Mul类封装为Python函数mul，并通过Variable.__mul__和Variable.__rmul__绑定乘法运算符，使其支持左右操作数为Variable的情况。由于乘法满足交换律，__mul__和__rmul__可共用同一实现。\",\"def mul(x0, x1): x1 = as_array(x1) return Mul()(x0, x1) Variable.__mul__ = mul # 处理a * b Variable.__rmul__ = mul # 处理b * a\",\"加法运算的运算符重载:\",\"加法运算的Add类已在上文中实现，其反向传播逻辑为将上游梯度原封不动地传递给两个输入变量（因为，）。类似地，我们将Add类绑定到+运算符：\",\"def add(x0, x1): x1 = as_array(x1) return Add()(x0, x1) Variable.__add__ = add # 处理a + b Variable.__radd__ = add # 处理b + a\",\"复合运算的验证:\",\"通过组合加法和乘法运算符，我们可以验证框架是否支持复杂表达式的自动微分。例如，计算y = a * b + c并求导：\",\"a = Variable(np.array(3.0)) b = Variable(np.array(2.0)) c = Variable(np.array(1.0)) y = a * b + c # 等价于 add(mul(a, b), c) y.backward() print(y.data) # 输出：7.0（3*2+1） print(a.grad) # 输出：2.0（∂y/∂a = b） print(b.grad) # 输出：3.0（∂y/∂b = a） print(c.grad) # 输出：1.0（∂y/∂c = 1）\",\"此例中，反向传播正确计算了每个变量的梯度，证明运算符重载与自动微分机制的一致性。\",\"(可选部分) 在Python中，运算符重载需要同时考虑左右运算符（如__add__和__radd__），这是由Python的运算符调度机制决定的。当表达式中的左右操作数类型不同时，Python会根据操作数的类型选择不同的方法调用路径。Python中，运算符的调用顺序遵循以下规则：\",\"左操作数优先：当执行表达式a OP b时，Python首先尝试调用左操作数a的__OP__方法。\",\"右操作数 fallback：如果左操作数未实现__OP__方法，或返回NotImplemented，则尝试调用右操作数b的__rOP__方法。\",\"以加法a + b为例：\",\"首先调用a.__add__(b)；\",\"若a未实现__add__或返回NotImplemented，则调用b.__radd__(a)。\",\"当操作数类型不同时（如Variable与数值、ndarray混合运算），必须通过__rOP__处理右操作数为自定义类型的情况。例如：\",\"x + 3：左操作数x是Variable，调用x.__add__(3)，可正常转换3为Variable；\",\"3 + x：左操作数3是int，不具备__add__方法处理Variable，因此需调用x.__radd__(3)。\",\"在Python的运算符重载中，以def __add__(self, other)为例，self和other是两个关键入参:\",\"self：在表达式a + b中，self指代左操作数a，即调用__add__方法的实例。\",\"other：在表达式a + b中，other指代右操作数b，即调用__radd__方法的实例。\",\"在 Python 的运算符重载中，以def radd(self, other)为例，self和other是两个关键入参：\",\"在表达式a + b中，若左操作数a不支持__add__方法（或返回NotImplemented），则会调用右操作数b的__radd__方法。此时，self指代右操作数b，即调用__radd__方法的实例\",\"在__radd__方法中，other指代左操作数a，其类型可能是原生数值、ndarray或Variable。\",\"支持与ndarray及数值类型的混合运算:\",\"为了提升框架的易用性，我们需要让Variable实例能与NumPy数组（ndarray）、Python数值类型（如int、float）直接进行运算。关键在于实现类型转换工具函数as_variable，将非Variable对象转换为Variable实例：\",\"def as_variable(obj): if isinstance(obj, Variable): return obj return Variable(obj) # 将ndarray或数值转换为Variable\",\"同时，修改Function类的__call__方法，在接收输入时自动将参数转换为Variable：\",\"class Function: def __call__(self, *inputs): inputs = [as_variable(x) for x in inputs] # 统一转换为Variable xs = [x.data for x in inputs] # 后续计算逻辑...\",\"并且在所有重载运算符函数实现中，将other参数统一转换为ndarray：\",\"def add(self, other): other = as_array(other) return Add()(self, other) def as_array(x): if np.isscalar(x): return np.array(x) return x\",\"这样，当执行x + np.array(3.0)或x + 3.0时，右侧的ndarray或数值会被自动转换为Variable，确保运算正常进行。\",\"处理运算符的左右操作数差异:\",\"以乘法为例，当表达式为3.0 * x时，Python会调用x的__rmul__方法（右乘）。由于乘法满足交换律，__rmul__可复用__mul__的实现：\",\"Variable.__rmul__ = mul # 与__mul__共用逻辑，支持3.0 * x\",\"类似地，对于不满足交换律的运算符（如减法），需要分别处理左右操作数。例如，2.0 - x需要调用x的__rsub__方法，此时需交换操作数顺序并调用Sub类：\",\"def rsub(x0, x1): return Sub()(x1, x0) # 实现a - b = Sub(b, a) Variable.__rsub__ = rsub\",\"运算符优先级与类型转换:\",\"为了确保Variable实例在混合运算中优先被处理，我们为Variable类添加__array_priority__属性，设置其优先级高于ndarray（默认优先级为100）：\",\"class Variable: __array_priority__ = 200 # 高于ndarray的优先级，确保类型转换优先\",\"这使得当表达式为np.array([2.0]) + x时，x的__radd__方法会被优先调用，保证运算按预期执行。\",\"负数运算（-）的实现:\",\"负数运算y = -x的正向传播简单地对输入取反，反向传播时将上游梯度取反（因为）：\",\"class Neg(Function): def forward(self, x): return -x def backward(self, gy): return -gy # 梯度取反\",\"绑定-运算符到neg函数：\",\"def neg(x): return Neg()(x) Variable.__neg__ = neg # 支持y = -x\",\"减法运算（-）的完整实现:\",\"减法运算y = x0 - x1的反向传播中，，，因此反向传播时需将上游梯度gy分别乘以1和-1：\",\"class Sub(Function): def forward(self, x0, x1): return x0 - x1 def backward(self, gy): return gy, -gy # 梯度分别乘以1和-1\",\"由于减法不满足交换律，需分别实现__sub__（处理x0 - x1）和__rsub__（处理x1 - x0）：\",\"def sub(x0, x1): return Sub()(x0, x1) def rsub(x0, x1): return Sub()(x1, x0) # 交换操作数实现a - b Variable.__sub__ = sub # 支持x0 - x1 Variable.__rsub__ = rsub # 支持x1 - x0\",\"除法运算（/）的实现:\",\"除法运算y = x0 / x1的导数公式为，，反向传播时需按此计算梯度：\",\"class Div(Function): def forward(self, x0, x1): return x0 / x1 def backward(self, gy): x0, x1 = self.inputs[0].data, self.inputs[1].data gx0 = gy / x1 gx1 = -gy * x0 / (x1 ** 2) return gx0, gx1 # 按导数公式计算梯度\",\"同样，需处理左右操作数的除法运算：\",\"def div(x0, x1): return Div()(x0, x1) def rdiv(x0, x1): return Div()(x1, x0) # 交换操作数实现a / b Variable.__truediv__ = div # 支持x0 / x1 Variable.__rtruediv__ = rdiv # 支持x1 / x0\",\"幂运算（**）的实现:\",\"幂运算y = x ** c中，c为常数指数，其导数公式为。反向传播时需按此计算梯度：\",\"class Pow(Function): def __init__(self, c): self.c = c # 保存指数 def forward(self, x): return x ** self.c def backward(self, gy): x = self.inputs[0].data c = self.c gx = c * (x ** (c - 1)) * gy # 导数公式：c·x^(c-1)·gy return gx\",\"绑定**运算符到pow函数：\",\"def pow(x, c): return Pow(c)(x) Variable.__pow__ = pow # 支持x ** c\",\"通过步骤20-22的实现，TinyPytorch框架实现了完整的运算符重载体系，使开发者能以自然的数学表达式编写代码（如y = (x + 3) ** 2 / 2），而无需调用特定函数。这种“可微分编程”的方式不仅降低了学习成本，还确保了复杂表达式的自动微分正确性，为后续实现神经网络层和优化算法奠定了基础。\"]},\"661\":{\"h\":\"步骤23: 项目模块化结构\",\"t\":[\"在Python开发中，模块、包和库是组织代码的重要方式：\",\"模块（module）：单个Python文件，如core.py，用于封装功能。\",\"包（package）：多个模块的集合，以目录形式存在，需包含__init__.py文件。\",\"库（library）：多个包的集合，通常用于实现完整功能（如TinyPytorch框架）。\",\"为将 TinyPytorch 的代码组织为可复用的包，从本章开始为每个Chapter设计如下目录结构：\",\"__init__.py # 包初始化文件 core.py # 核心功能（简化版） functions.py # 具体函数实现 utils.py # 工具函数\",\"core.py，包括：\",\"类定义：Config、Variable、Function及运算符相关类（Add、Mul等）。\",\"函数定义：using_config、no_grad、as_array、as_variable等工具函数，以及add、mul等运算符函数。\",\"__init__.py中导入核心类并初始化运算符重载:\",\"from chapter2.core import Variable from chapter2.core import Function from chapter2.core import using_config from chapter2.core import no_grad from chapter2.core import as_array from chapter2.core import as_variable from chapter2.core import setup_variable setup_variable() # 初始化运算符重载\",\"其中，setup_variable()函数负责绑定运算符方法：\",\"def setup_variable(): Variable.__add__ = add Variable.__radd__ = add Variable.__mul__ = mul Variable.__rmul__ = mul # 其他运算符绑定...\"]},\"662\":{\"h\":\"步骤24：复杂函数的求导\",\"t\":[\"优化问题中常使用特定函数评估算法性能，这些函数被称为测试函数。通过对复杂测试函数求导，可验证TinyPytorch框架处理高阶微分的能力。本步骤选取3个经典测试函数，演示TinyPytorch的自动微分功能。\",\"Sphere函数求导\",\"函数定义：，是简单的平方和函数，用于验证基础微分逻辑。\",\"代码实现：\",\"def sphere(x, y): z = x ** 2 + y ** 2 return z x = Variable(np.array(1.0)) y = Variable(np.array(1.0)) z = sphere(x, y) z.backward() print(x.grad, y.grad) # 输出：2.0 2.0\",\"结果验证：根据导数公式 、，在 处导数为(2.0, 2.0)，与运行结果一致。\",\"Matyas函数求导\",\"函数定义：，是包含交叉项的二维函数。\",\"代码实现：\",\"def matyas(x, y): z = 0.26 * (x ** 2 + y ** 2) - 0.48 * x * y return z x = Variable(np.array(1.0)) y = Variable(np.array(1.0)) z = matyas(x, y) z.backward() print(x.grad, y.grad) # 输出：0.04 0.04（近似值）\",\"结果分析：通过运算符重载，可直接将数学表达式转译为代码。若不使用运算符，需编写繁琐的函数调用（如sub(mul(0.26, add(pow(x, 2), pow(y, 2))), mul(0.48, mul(x, y))），凸显运算符重载的可读性优势。\",\"Goldstein-Price函数求导\",\"函数定义：\",\"该函数形式复杂，包含高次项和交叉项，是验证框架能力的理想案例。\",\"代码实现：\",\"def goldstein(x, y): z = (1 + (x + y + 1)** 2 * (19 - 14*x + 3*x**2 - 14*y + 6*x*y + 3*y** 2)) * \\\\ (30 + (2*x - 3*y)** 2 * (18 - 32*x + 12*x**2 + 48*y - 36*x*y + 27*y** 2)) return z x = Variable(np.array(1.0)) y = Variable(np.array(1.0)) z = goldstein(x, y) z.backward() print(x.grad, y.grad) # 输出：-5376.0 8064.0\",\"结果验证：通过梯度检验可知结果正确。TinyPytorch框架能自动处理复杂表达式的微分，无需手动推导导数公式，体现了自动微分的优势。\",\"TinyPytorch的核心能力总结\",\"自然代码表达：支持将数学公式直接转译为Python代码，如z = (x + y + 1)**2 * ...，无需额外接口。\",\"复杂计算图处理：无论计算图结构多复杂（如多层嵌套、高次运算），均能正确构建反向传播路径。\",\"可微分编程：将普通数值计算转换为可微分计算，使深度学习框架具备自动求导能力，为优化算法和神经网络训练奠定基础。\",\"深度学习框架的计算图范式: TinyPytorch采用Define-by-Run（动态计算图） 模式，与Define-and-Run（静态计算图）的对比如下：\",\"Define-by-Run：计算与图构建同时进行，如TinyPytorch中每一步运算都会动态创建计算图链接，支持Python原生控制流（if、while），调试便捷。\",\"Define-and-Run：先定义计算图再执行，需使用领域特定语言（如TensorFlow 1.x的tf.cond），适合大规模优化但灵活性较低。\",\"TinyPytorch的动态计算图模式使其在易用性和灵活性上表现突出，尤其适合研究和快速开发场景。\"]},\"663\":{\"h\":\"第二阶段总结\",\"t\":[\"这一阶段，我们构建了如下关键功能：\",\"扩展DeZero以处理多输入多输出函数，支持用+、*等运算符自然表达计算。\",\"修改Function类，通过列表处理可变长参数，优化正向传播实现。\",\"实现多元函数反向传播，解决变量重复使用时的梯度累加问题。\",\"引入“辈分”机制，确保复杂计算图反向传播顺序正确。\",\"使用弱引用解决循环引用，通过Config类和no_grad模式优化内存管理。\",\"重载运算符，支持Variable与数值、数组混合运算，提升代码可读性。\",\"为Variable添加shape等属性，使其行为更接近NumPy数组，优化打印等交互体验。\"]},\"664\":{\"h\":\"🧠 从零构建深度学习框架（三）：动态图可视化与高阶导数构建\",\"t\":[\"1.TinyPytorch 第三阶段: 高阶导数与深度学习优化进阶\",\"仓库链接: https://github.com/BinaryOracle/TinyPytorch 本节代码:\"]},\"665\":{\"h\":\"引言：从自动微分走向“可视化 + 高阶导数 + 灵活控制”\",\"t\":[\"随着 TinyPytorch 框架核心功能的日益完善，我们开始迈入更深入也更贴近真实深度学习框架设计的阶段。在前一阶段，我们实现了自动构建计算图与反向传播的基本机制，使得模型训练具备了基础的“学习”能力。\",\"在第三阶段，我们将从第25步继续出发，围绕计算图可视化、高阶导数构建、动态图控制与框架灵活性展开一系列扩展与优化：\",\"引入 Graphviz 实现计算图的可视化渲染，帮助开发者直观理解前向与反向传播路径；\",\"实现 create_graph=True 支持高阶导数的构建；\",\"引入 sin、cos、tanh 等函数节点，扩展函数库并验证高阶导数；\",\"构建泰勒展开、牛顿法等经典函数逼近与优化示例；\",\"完善框架的模块结构，优化 Function 与 Variable 的内存管理与执行流程。\",\"通过这 10 个步骤，TinyPytorch 不仅具备了现代框架应有的可视化与控制能力，还能够处理更复杂的自动微分任务，为后续的神经网络模块与训练机制打下坚实基础。我们将看到，它不仅是“能跑起来”，而是真正朝着“易用、清晰、高效”的方向进化。\"]},\"666\":{\"h\":\"步骤25: 可视化计算图\",\"t\":[\"当前TinyPytorch已能将复杂式子转化为代码，但需直观呈现计算图全貌以辅助调试与理解。为此引入第三方工具Graphviz，其支持节点和箭头构成的数据结构可视化，可用于展示TinyPytorch计算图。\",\"macOS安装：通过Homebrew执行 brew install graphviz。\",\"Ubuntu安装：执行 sudo apt install graphviz。\",\"验证安装：运行 dot -V，若显示版本信息（如dot - graphviz version 2.40.1）则安装成功。\",\"文件转换命令：使用dot sample.dot -T png -o sample.png将DOT格式文件转换为PNG图像，其中-T指定输出格式，-o指定输出文件名。\",\"DOT语言基础语法:\",\"简单节点定义：定义包含节点x和y的有向图，节点间用换行分隔。\",\"digraph g { x y }\",\"节点属性设置：定义节点ID为1，标签为x，颜色橙色并填充；shape=box可将节点设为矩形。\",\"digraph g { 1 [label=\\\"x\\\", color=orange, style=filled] 2 [label=\\\"y\\\", color=orange, style=filled, shape=box] }\",\"节点连接：使用->表示箭头连接，如1->2表示从节点1到节点3的有向边。\",\"digraph g { 1 [label=\\\"x\\\", color=orange, style=filled] 2 [label=\\\"y\\\", color=orange, style=filled] 1 -> 2 }\",\"TinyPytorch计算图转换为DOT语言:\",\"import numpy as np from chapter3 import Variable from chapter3 import get_dot_graph x0 = Variable(np.array(1.0)) x1 = Variable(np.array(1.0)) y = x0 + x1 x0.name = 'x0' x1.name = 'x1' y.name = 'y' txt = get_dot_graph(y, verbose=False) print(txt) with open('sample.dot', 'w') as f: f.write(txt)\",\"代码将变量y的计算图转换为DOT语言字符串，并保存为文件。verbose参数控制是否显示详细信息。\",\"输出的DOT语言示例包含变量节点（橙色圆形）和函数节点（浅蓝色矩形），如：\",\"digraph g { 4847712112 [label=\\\"y\\\", color=orange, style=filled] 4847712064 [label=\\\"Add\\\", color=lightblue, style=filled, shape=box] 4775983056 -> 4847712064 4847711968 -> 4847712064 4847712064 -> 4847712112 4775983056 [label=\\\"x0\\\", color=orange, style=filled] 4847711968 [label=\\\"x1\\\", color=orange, style=filled] }\",\"转换后的图像展示x0 + x1的计算图，包含Add函数节点和变量连接。\",\"核心函数实现原理:\",\"_dot_var函数：生成变量节点的DOT描述，使用id(v)作为节点唯一ID，支持显示变量名、形状和数据类型：\",\"def _dot_var(v, verbose=False): dot_var = '{} [label=\\\"{}\\\", color=orange, style=filled]\\\\n' name = '' if v.name is None else v.name if verbose and v.data is not None: if v.name is not None: name += ': ' name += str(v.shape) + ' ' + str(v.dtype) return dot_var.format(id(v), name)\",\"示例输出：4423761088 [label=\\\"x: (2, 3) float64\\\", color=orange, style=filled]。\",\"_dot_func函数：生成函数节点的DOT描述，使用函数类名作为标签：\",\"def _dot_func(f): # for function dot_func = '{} [label=\\\"{}\\\", color=lightblue, style=filled, shape=box]\\\\n' ret = dot_func.format(id(f), f.__class__.__name__) # for edge dot_edge = '{} -> {}\\\\n' for x in f.inputs: ret += dot_edge.format(id(x), id(f)) for y in f.outputs: # y is weakref ret += dot_edge.format(id(f), id(y())) return ret\",\"示例输出：4423742632 [label=\\\"Add\\\", color=lightblue, style=filled, shape=box]。\",\"计算图遍历逻辑：与反向传播类似，从输出变量出发遍历所有节点（变量和函数），生成DOT语言字符串。通过seen_set避免重复处理节点，使用funcs.append(f)和funcs.pop()实现后序遍历。\",\"def get_dot_graph(output, verbose=True): txt = '' funcs = [] seen_set = set() def add_func(f): if f not in seen_set: funcs.append(f) # funcs.sort(key=lambda x: x.generation) seen_set.add(f) add_func(output.creator) txt += _dot_var(output, verbose) while funcs: func = funcs.pop() txt += _dot_func(func) for x in func.inputs: txt += _dot_var(x, verbose) if x.creator is not None: add_func(x.creator) return 'digraph g {\\\\n' + txt + '}'\",\"可视化工具封装:\",\"plot_dot_graph函数：自动执行DOT文件转换并显示图像，支持保存为PNG、PDF等格式：\",\"def plot_dot_graph(output, verbose=True, to_file='graph.png'): dot_graph = get_dot_graph(output, verbose) tmp_dir = os.path.join(os.path.expanduser('~'), '.dezero') if not os.path.exists(tmp_dir): os.mkdir(tmp_dir) graph_path = os.path.join(tmp_dir, 'tmp_graph.dot') with open(graph_path, 'w') as f: f.write(dot_graph) extension = os.path.splitext(to_file)[1][1:] # Extension(e.g. png, pdf) cmd = 'dot {} -T {} -o {}'.format(graph_path, extension, to_file) subprocess.run(cmd, shell=True) # Return the image as a Jupyter Image object, to be displayed in-line. try: from IPython import display return display.Image(filename=to_file) except: pass\",\"该函数自动调用系统命令转换文件，并支持在Jupyter Notebook中直接显示图像。\",\"复杂函数可视化示例: 以Goldstein-Price函数为例\",\"import numpy as np from chapter3 import plot_dot_graph, Variable def goldstein(x, y): z = (1 + (x + y + 1)**2 * (19 - 14*x + 3*x**2 - 14*y + 6*x*y + 3*y**2)) * \\\\ (30 + (2*x - 3*y)** 2 * (18 - 32*x + 12*x**2 + 48*y - 36*x*y + 27*y**2)) return z x = Variable(np.array(1.0)) y = Variable(np.array(1.0)) z = goldstein(x, y) z.backward() x.name = 'x' y.name = 'y' z.name = 'z' plot_dot_graph(z, to_file='goldstein.png')\",\"可视化结果显示复杂计算图，包含多层Pow、Mul、Add等操作节点，验证DeZero对复杂表达式的计算图构建能力。\"]},\"667\":{\"h\":\"步骤26: 寻找函数最优解\",\"t\":[\"本步骤将处理Rosenbrock函数，其式子为：\",\"该函数的形状如下图所示，若画出其“山”的等高线，会发现线的形状类似香蕉，因此Rosenbrock函数也被称为“香蕉函数”。\",\"本步骤的目标是找到使Rosenbrock函数输出值最小的和。已知Rosenbrock函数的最小值在处，接下来将使用TinyPytorch验证是否能找到该最小值。\",\"Rosenbrock函数的严格定义是，其中和是常数。上述例子是、时的Rosenbrock函数，该函数常作为优化问题的基准函数使用。\",\"首先求Rosenbrock函数在处的导数和，使用TinyPytorch实现的代码如下：\",\"import numpy as np from chapter3 import Variable def rosenbrock(x0, x1): y = 100 * (x1 - x0 ** 2) ** 2 + (x0 - 1) ** 2 return y x0 = Variable(np.array(0.0)) x1 = Variable(np.array(2.0)) y = rosenbrock(x0, x1) y.backward() print(x0.grad, x1.grad)\",\"运行结果为：\",\"-2.0 400.0\",\"这里将数值数据封装在Variable中，通过backward()方法求导。得到的导数为-2.0，导数为400.0。梯度展示了各点上函数输出值增加最快的方向，在点上，值增加最快的方向是(-2.0,400.0)，那么值减少最快的方向是(2.0,-400.0)。\",\"梯度下降法解决问题:\",\"对于形状复杂的函数，其最大值可能不在梯度指示方向，最小值也可能不在梯度反方向，但从局部看，梯度表示函数输出值最大的方向。重复向梯度方向移动一定距离，再求梯度，可逐渐接近目标位置，这就是梯度下降法。若从好的起点开始，使用梯度下降法能高效找到目标值。\",\"使用梯度下降法寻找Rosenbrock函数最小值的代码如下:\",\"x0 = Variable(np.array(0.0)) x1 = Variable(np.array(2.0)) lr = 0.001 iters = 1000 for i in range(iters): y = rosenbrock(x0, x1) x0.cleargrad() x1.cleargrad() y.backward() x0.data -= lr * x0.grad x1.data -= lr * x1.grad\",\"代码中，迭代次数设为iters（iters是iterations的缩写），与梯度相乘的值设为lr=0.001（lr是learning rate的缩写，即学习率）。\",\"由于for语句反复使用Variable实例x0和x1求导，而每次反向传播时导数会累加，所以在反向传播前需调用各变量的cleargrad方法重置导数。\",\"运行代码，从输出信息可看到(x0,x1)值的更新过程，部分结果如下：\",\"iter 992: x0 = 0.682166, x1 = 0.463833 iter 993: x0 = 0.682388, x1 = 0.464137 iter 994: x0 = 0.682609, x1 = 0.464440 iter 995: x0 = 0.682830, x1 = 0.464743 iter 996: x0 = 0.683051, x1 = 0.465046 iter 997: x0 = 0.683271, x1 = 0.465348 iter 998: x0 = 0.683492, x1 = 0.465651 iter 999: x0 = 0.683712, x1 = 0.465953\",\"将计算结果绘制在图上，如下图所示，从图中可看出逐渐接近星号所指的目的地位置，但尚未到达。\",\"增加迭代次数设为 iters =10000，结果如下图所示，此时离目的地更近，(x0,x1)的值为(0.99449622,0.98900063)。\",\"若再增加迭代次数到 iters =50000，就会抵达(1.0,1.0)。\",\"包含绘图的完整代码:\",\"import numpy as np from matplotlib import pyplot as plt from chapter3 import Variable def rosenbrock(x0, x1): y = 100 * (x1 - x0 ** 2) ** 2 + (x0 - 1) ** 2 return y x0 = Variable(np.array(0.0)) x1 = Variable(np.array(2.0)) lr = 0.001 iters = 50000 x0_list = [] x1_list = [] for i in range(iters): y = rosenbrock(x0, x1) x0.cleargrad() x1.cleargrad() y.backward() x0.data -= lr * x0.grad x1.data -= lr * x1.grad x0_list.append(x0.data.copy()) x1_list.append(x1.data.copy()) print('iter %d: x0 = %f, x1 = %f' % (i, x0.data, x1.data)) # 绘制等高线图 x = np.linspace(-2, 2, 400) y = np.linspace(-1, 3, 400) X, Y = np.meshgrid(x, y) Z = (1 - X)**2 + 100 * (Y - X**2)**2 plt.figure(figsize=(8, 6)) cp = plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 30), cmap='jet') plt.plot(x0_list, x1_list, 'o-', color='yellow', markersize=2, label='Gradient Descent Path') # 在 (1.0, 1.0) 处标记最优点 plt.plot(1.0, 1.0, marker='*', markersize=12, color='red', label='Minimum (1,1)') plt.xlabel('x0') plt.ylabel('x1') plt.title('Gradient Descent on Rosenbrock Function') plt.legend() plt.grid(True) plt.show()\",\"本步骤使用TinyPytorch实现了梯度下降法，找到了Rosenbrock函数最小值的位置，不过迭代次数较多，有5万次。实际上梯度下降法并不擅长处理Rosenbrock这种类型的函数，下一个步骤会介绍并实现另一种优化方法。\"]},\"668\":{\"h\":\"步骤27: 高阶导数\"},\"669\":{\"h\":\"🏗️ 从零构建深度学习框架（四）：计算图进阶与通用神经网络实现\",\"t\":[\"4.TinyPytorch 第四阶段: 通用网络层封装与模型训练流程构建\",\"仓库链接: https://github.com/BinaryOracle/TinyPytorch 本节代码:\"]},\"670\":{\"h\":\"引言：从自动微分迈向可训练的神经网络模型\",\"t\":[\"前三阶段的 TinyPytorch，已实现自动微分系统与基础函数操作。在第四阶段，我们将真正迈入“深度学习框架”的核心部分——从简单函数组合进化到模块化神经网络，实现可复用的层（Layer）、模型（Model）、优化器（Optimizer）等，最终完成一个能训练分类任务的通用框架。\",\"本阶段的目标是打造一个“小而全”的深度学习训练系统。我们将实现：\",\"网络层封装（如 Linear、ReLU 等）\",\"模型类 Model 与训练流程规范\",\"参数管理与清理机制\",\"SGD 优化器与 momentum 拓展\",\"批处理、数据加载器与数据集支持\",\"实际任务训练（分类任务 + MNIST 手写数字）\",\"第四阶段共 14 个步骤，从第44步到第57步，形成了一个具备如下特征的微型深度学习框架：\"]},\"671\":{\"h\":\"TinyPytorch\"},\"672\":{\"h\":\"1.前置知识\",\"t\":[\"智慧化知识库系统: 大语言模型应用开发基础知识速览。\"]},\"673\":{\"h\":\"大语言模型\",\"t\":[\"大语言模型（LLM，Large Language Model），也称大型语言模型，是一种旨在理解和生成人类语言的人工智能模型。\",\"LLM 通常指包含数百亿（或更多）参数的语言模型，它们在海量的文本数据上进行训练，从而获得对语言深层次的理解。目前，国外的知名 LLM 有 GPT、LLaMA、Gemini、Claude 和 Grok 等，国内的有 DeepSeek、通义千问、豆包、Kimi、文心一言、GLM 等。\",\"为了探索性能的极限，许多研究人员开始训练越来越庞大的语言模型，例如拥有 175B (1750 亿)参数的 GPT-3 和 540B（5400 亿）参数的 PaLM 。尽管这些大型语言模型与小型语言模型（例如 3.3 亿参数的 BERT 和 15 亿参数的 GPT-2）使用相似的架构和预训练任务，但它们展现出截然不同的能力，尤其在解决复杂任务时表现出了惊人的潜力，这被称为“涌现能力”。以 GPT-3 和 GPT-2 为例，GPT-3 可以通过学习上下文来解决少样本任务，而 GPT-2 在这方面表现较差。因此，科研界给这些庞大的语言模型起了个名字，称之为“大语言模型（LLM）”。LLM 的一个杰出应用就是 ChatGPT ，它是 GPT 系列 LLM 用于与人类对话式应用的大胆尝试，展现出了非常流畅和自然的表现。 语言建模的研究可以追溯到 20 世纪 90 年代，当时的研究主要集中在采用统计学习方法来预测词汇，通过分析前面的词汇来预测下一个词汇。但在理解复杂语言规则方面存在一定局限性。\",\"随后，研究人员不断尝试改进，2003 年深度学习先驱 Bengio 在他的经典论文 《A Neural Probabilistic Language Model》中，首次将深度学习的思想融入到语言模型中。强大的神经网络模型，相当于为计算机提供了强大的\\\"大脑\\\"来理解语言，让模型可以更好地捕捉和理解语言中的复杂关系。\",\"2018 年左右，Transformer 架构的神经网络模型开始崭露头角。通过大量文本数据训练这些模型，使它们能够通过阅读大量文本来深入理解语言规则和模式，就像让计算机阅读 整个互联网一样，对语言有了更深刻的理解，极大地提升了模型在各种自然语言处理任务上的表现。\",\"与此同时，研究人员发现，随着语言模型规模的扩大（增加模型大小或使用更多数据），模型展现出了一些惊人的能力，在各种任务中的表现均显著提升（Scaling Law）。这一发现标志着大型语言模型（LLM）时代的开启。\",\"通常大模型由三个阶段构成：预训练、后训练和在线推理。在 2024 年 9 月之前，大模型领域仅存在预训练阶段的 Scaling Law。然而，随着 OpenAI o1 的推出，后训练和在线推理阶段也各自拥有了 Scaling Law，即后训练阶段的强化学习 Scaling Law（RL Scaling Law）和在线推理阶段的 Inference Scaling Law（Test Time Scaling Law）。 随着各阶段计算量的增加，大模型的性能不断增长。\"]},\"674\":{\"h\":\"常见的LLM\",\"t\":[\"大语言模型的发展历程虽然只有短短不到五年的时间，但是发展速度相当惊人，截止 2024 年 6 月，国内外有超过百种大模型相继发布。下图按照时间线给出了 2019 年至 2024 年 6 月比较有影响力并且模型参数量超过 100 亿的大语言模型：\",\"接下来我们主要介绍几个国内外常见的大模型（包括开源和闭源）。\",\"OpenAI\",\"OpenAI 公司在 2018 年 提出的 GPT（Generative Pre-Training） 模型是典型的 生成式预训练语言模型 之一。 GPT 模型的基本原则是通过语言建模将世界知识压缩到仅解码器 (decoder-only) 的 Transformer 模型中，这样它就可以恢复(或记忆)世界知识的语义，并充当通用任务求解器。它能够成功的两个关键点：\",\"训练能够准确预测下一个单词的 decoder-only 的 Transformer 语言模型\",\"扩展语言模型的大小\",\"OpenAI 在 LLM 上的研究大致可以分为以下几个阶段：\",\"目前，GPT 系列已形成 知识型 与 推理型 两大技术分支。\",\"2022 年 11 月，OpenAI 发布了基于 GPT 模型（GPT-3.5 和 GPT-4）的会话应用 ChatGPT。由于与人类交流的出色能力，ChatGPT 自发布以来就引发了人工智能社区的兴奋。ChatGPT 本质上是一个 LLM 应用，是基于基座模型开发出来的，与基座模型有本质的区别。ChatGPT 上线后用户增长迅速，5 天注册人数突破 100 万，两个月后月活用户破亿，成为当时史上用户增长最快的消费级应用程序。\",\"随着不断迭代，ChatGPT 逐渐丰富了其功能：\",\"插件系统：允许开发者创建工具扩展 ChatGPT 的能力，实现网页浏览、数据分析和第三方服务调用\",\"实时语音和视频对话：用户可与 AI 进行自然的语音和视频交流，支持手势识别和情感表达\",\"多模态能力：能够分析和理解用户提供的图片、音频和视频，实现全面的多模态交互\",\"自定义指令与记忆功能：记住用户之前的交互习惯和偏好，提供个性化体验\",\"GPT 构建器平台：允许用户无需编程创建专用的 AI 助手，支持自定义知识库和行为模式\",\"数据分析与可视化：直接处理和分析上传的数据文件，生成图表和可视化报告\",\"知识型与推理型双模式：可在 GPT-4.5 (知识型) 和 o1/o3 (推理型) 之间切换，满足不同场景需求\",\"思维链展示：在推理型模型中可选择性展示思考过程，帮助用户理解推理步骤\",\"2023 年 3 月 发布的 GPT-4 引入了多模态能力，相比 GPT-3.5 的 1750 亿参数，GPT-4 规模显著扩大（推测约 1.8 万亿参数），在解决复杂任务和评估任务上展现出较大的性能提升。\",\"2024 年 5 月 发布的 GPT-4o（\\\"o\\\"代表\\\"omni\\\"全能）具备对文本、语音、图像三种模态的深度理解能力，主要特点包括：\",\"多模态融合：无缝理解和生成多种形式内容\",\"实时对话：响应速度比 GPT-4 快约 2 倍\",\"情感表达：在语音互动中传递更丰富的情感变化\",\"成本效益：API 定价降低约 50%\",\"2024 年 7 月 发布的 GPT-4o mini 是一款面向消费级应用的轻量级模型，价格更加亲民，适合日常对话和基础任务场景。\",\"2025 年 2 月 发布的 GPT-4.5 在知识广度、推理深度和创意表达方面有显著提升，特别强化了对客观事实的准确性，尤其是情商方面异常优秀。上下文长度扩展至 512K。是 OpenAI 的最后一个非思维链模型。\",\"主流知识型模型对比:\",\"模型名称\",\"上下文长度\",\"特点\",\"知识截止日期\",\"GPT-4\",\"16k\",\"经济，专门对话\",\"2021 年 9 月\",\"GPT-4o\",\"128k\",\"多模态，速度快\",\"2023 年 10 月\",\"GPT-4.5\",\"128k\",\"最强知识型，精准度高\",\"2023 年 10 月\",\"GPT-4o mini\",\"128k\",\"轻量知识型，性价比高\",\"2023 年 10 月\",\"2024 年 9 月 发布的 o1-mini、o1-preview 是专为复杂推理设计的模型，在回答前会先生成一段思维链（不公开），优先考虑精确性和推理步骤的正确性。\",\"超强推理能力：在数学、编程和逻辑推理等任务中表现卓越。\",\"解题过程可靠：注重解题中间步骤的正确性。\",\"问题分解能力：将复杂问题分解为可管理的子问题。\",\"自纠错机制：识别错误并主动纠正。\",\"2024 年 12 月 发布的 o1 比 o1-preview 可以在更快的时间内响应，思考的时间更短。\",\"2025 年 1 月 发布的 o3-mini 可以显示部分思维链，与 o1 相比，可以保持效果的情况下，响应速度更快。\",\"模型名称\",\"上下文长度\",\"特点\",\"知识截止日期\",\"o1\",\"128k\",\"强推理能力，慢\",\"2023 年 10 月\",\"o1 mini\",\"200k\",\"轻量推理，中速\",\"2023 年 10 月\",\"o3 mini\",\"200k\",\"超轻量推理，最快\",\"2023 年 10 月\",\"OpenAI 的模型战略形成了“知识型”和“推理型”两条互补产品线：\",\"知识型模型 专注于广泛知识覆盖和流畅对话体验。\",\"推理型模型 专注于精确推理和复杂问题求解，让用户可根据具体需求选择最适合的模型类型。\",\"Claude\",\"Claude 系列模型是由 OpenAI 离职人员创建的 Anthropic 公司开发的闭源语言大模型。\",\"最早的 Claude 于 2023 年 3 月 15 日发布。\",\"2024 年 3 月 4 日，更新至 Claude-3，包括 Claude 3 Haiku、Claude 3 Sonnet 和 Claude 3 Opus，它们的能力依次递增，旨在满足不同用户和应用场景的需求。\",\"2024 年 10 月，Anthropic 发布了 Claude 3.5 Sonnet，这是一款在推理和通用任务上有显著提升的模型。\",\"2025 年 5 月，Anthropic 又进一步发布了 Claude 4.0，包括了 Claude 4 Sonnet 和 Claude 4 Opus，均是混合推理模型，支持标准模式与推理思考模式，编码能力异常强大。支持多工具并行调用与精准指令解析，本地文件访问时内存管理升级，可规避捷径行为，强化复杂任务处理能力。\",\"模型名称\",\"上下文长度\",\"特点\",\"Claude 3.5 Haiku\",\"200k\",\"速度最快\",\"Claude 4 Sonnet\",\"200k\",\"最强性能，领先推理力\",\"Claude 4 Opus\",\"200k\",\"性能强大，费用最高\",\"Gemini\",\"Gemini 系列语言大模型由 Google 开发。\",\"2022 年 4 月，发布了初始版本（PaLM 后更名为 Gemini）。\",\"2025 年 2 月，Google 发布了 Gemini 2.0 系列模型，在性能和效率上有显著提升。包括 Gemini 2.0 Pro、Gemini 2.0 Flash、Gemini 2.0 Flash-Lite 是 Gemini 2.0 系列的三个版本，分别适用于不同的场景。同样，推出了其推理模型 Gemini 2.0 Flash Thinking。\",\"2025 年 3 月，Google 发布了 Gemini 2.5 Pro，性能有了进一步提升，推理能力和代码能力提升非常显著。\",\"模型名称\",\"上下文长度\",\"特点\",\"Gemini 2.5 Pro\",\"2M\",\"性能最强\",\"Gemini 2.0 Flash\",\"1M\",\"低延迟，性能强\",\"Gemini 2.0 Flash-Lite\",\"1M\",\"性价比最高\",\"Gemini 2.0 Flash Thinking\",\"1M\",\"思维链展示\",\"文心一言\",\"文心一言是基于百度文心大模型的知识增强语言大模型，于 2023 年 3 月 在国内率先开启邀测。文心一言的基础模型文心大模型于 2019 年发布 1.0 版，现已更新到 4.0 版本。更进一步划分，文心大模型包括 NLP 大模型、CV 大模型、跨模态大模型、生物计算大模型、行业大模型。文心一言的中文能力相对来说非常不错。 文心一言网页版分为 免费版 和 专业版：\",\"免费版 使用文心 3.5 版本，已经能够满足个人用户或小型企业的大部分需求。\",\"专业版 使用文心 4.0 版本，定价为 59.9 元/月，连续包月优惠价为 49.9 元/月。\",\"星火大模型\",\"讯飞星火认知大模型是科大讯飞发布的语言大模型，支持多种自然语言处理任务。\",\"2023 年 5 月，首次发布。\",\"2024年 10 月，讯飞星火发布模型 星火 4.0 Turbo。\",\"2025 年 1 月，讯飞发布了推理思考模型 讯飞星火 X1 和 星火语音同传模型。\",\"LLaMA\",\"LLaMA 系列模型是 Meta 开源的一组参数规模从 8B 到 405B 的基础语言模型。\",\"2023 年 2 月，发布 LLaMA。\",\"2023 年 7 月，发布了 LLaMA2 模型。\",\"2024 年 4 月，发布了 LLaMA3 模型。\",\"2024 年 7 月，发布了 LLaMA 3.1 模型。\",\"2024 年 12 月，发布了 LLaMA 3.3 模型（只开源了 70B 的指令模型）。\",\"它们都是在数万亿个字符上训练的，展示了如何仅使用公开可用的数据集来训练最先进的模型，而不需要依赖专有或不可访问的数据集。这些数据集包括 Common Crawl、Wikipedia、OpenWebText2、RealNews、Books 等。LLaMA 模型使用了大规模的数据过滤和清洗技术，以提高数据质量和多样性，减少噪声和偏见。LLaMA 模型还使用了高效的数据并行和流水线并行技术，以加速模型的训练和扩展其中 405B 参数模型是首个公开的千亿级开源模型，性能对标 GPT-4o 等商业闭源模型。 与 GPT 系列相同，LLaMA 模型也采用了 decoder-only 架构，同时结合了一些前人工作的改进。LLaMA 系列基本上是后续大模型的标杆：\",\"Pre-normalization（正则化）：为了提高训练稳定性，LLaMA 对每个 Transformer 子层的输入进行了 RMSNorm 归一化，这种归一化方法可以避免梯度爆炸和消失的问题，提高模型的收敛速度和性能。\",\"SwiGLU（激活函数）：将 ReLU 非线性替换为 SwiGLU 激活函数，增加网络的表达能力和非线性，同时减少参数量和计算量。\",\"旋转位置编码（RoPE, Rotary Position Embedding）：模型的输入不再使用位置编码，而是在网络的每一层添加了位置编码，RoPE 位置编码可以有效地捕捉输入序列中的相对位置信息，并且具有更好的泛化能力。\",\"分组查询注意力（GQA, Grouped-Query Attention）：通过将查询（query）分组并在组内共享键（key）和值（value），减少了计算量，同时保持了模型性能，提高了大型模型的推理效率（LLaMA2 只有 70B 采用）。\",\"LLaMA 3.1 于 2024 年 7 月 发布，提高了模型的性能和效率：\",\"更多的训练数据量：LLaMA3.1 在 15 万亿个 token 的数据上进行预训练，采用了更科学的数据配比。LLaMA3.1 接触到更多的文本信息，从而提高了其理解和生成文本的能力。\",\"更长的上下文长度：LLaMA 3.1 将上下文长度大幅提升至 128K token，支持处理极长的文档和对话历史，改善了对长文本的理解和生成能力，适用于更复杂的应用场景。\",\"分组查询注意力（GQA, Grouped-Query Attention）：通过将查询（query）分组并在组内共享键（key）和值（value），减少了计算量，同时保持了模型性能，提高了大型模型的推理效率（LLaMA2 只有 70B 采用）。\",\"更大的词表：LLaMA3.1 采用了 128K 的 tokenizer，是前两代 32K 的 4 倍，这使得其语义编码能力得到了极大的增强，从而显著提升了模型的性能。\",\"精细的指令遵循：通过改进的对齐技术，LLaMA 3.1 在遵循复杂指令、理解微妙提示方面表现更出色，使模型行为更可预测和可控。\",\"完善的工具使用：增强了 Function Calling 能力，使模型能够更准确地识别何时以及如何调用外部工具，提高了与外部系统集成的能力。 LLaMA 3.1 发布了 8B、70B 和 405B 三个规模的模型，分别提供基础版（Base）和指令微调版（Instruction），进一步扩展了 LLaMA 系列在开源社区的影响力和应用前景。\",\"DeepSeek\",\"DeepSeek 是由深度求索 (DeepSeek) 团队开发的开源大语言模型系列。首个版本于 2023 年 11 月 发布。DeepSeek 采用 decoder-only 架构，融合了 FlashAttention-2、RoPE 位置编码、SwiGLU 等先进技术，在多语言理解和代码生成等方面表现出色。\",\"2023 年 11 月 12 日：发布 DeepSeek 系列基础模型，包括 7B 和 67B 两种规模的 Base 和 Chat 版本。模型在 1.2 万亿 token 上进行训练，同时发布了 DeepSeek-Coder 专用代码生成模型。\",\"2024 年 3 月 15 日：发布 DeepSeek-V2 系列，提升了多语言能力、长文本理解和推理能力，同时发布了 DeepSeek-MoE 混合专家模型。\",\"2024 年 5 月 31 日：发布 DeepSeek-V2.5，性能得到进一步提升，上下文长度扩展至 128K tokens，并改进了工具调用和多模态能力。\",\"2024 年 10 月：发布 DeepSeek-V3，在推理能力、多语言理解和创意生成方面有显著提升，支持更复杂的系统提示词控制，并进一步提升了代码质量和多轮对话一致性。\",\"2025 年 2 月：\",\"DeepSeek-R1 推理型大模型：专注于复杂问题求解和精确推理能力，在数学、逻辑推理和结构化知识方面展现出卓越性能，类似于 OpenAI 的 o1 系列。并且是首个开源的推理型大模型，在多项基准测试中超越了 o1 系列。\",\"DeepSeek-R1-Zero：直接在大规模强化学习 (RL) 训练的模型，无需 SFT，在推理方面就十分出色。\",\"同时开源了用 Llama 和 Qwen 从 DeepSeek-R1 中蒸馏出的六个 dense 模型。其中 DeepSeek-R1-Distill-Qwen-32B 在各种基准测试中均优于 OpenAI-o1-mini。\",\"deepseek 目前采用的主要改进如下：\",\"多头潜在注意力 (MLA, Multi-head Latent Attention)：通过将键值 (KV) 缓存显著压缩为潜在向量来保证高效推理的同时不降低效果。\",\"DeepSeekMoE：通过稀疏计算以经济的成本训练强大的模型。\",\"一系列推理加速技术 借助 DeepSeekR1 的卓越能力，DeepSeek 成为了现象级爆火应用。7 天完成了 1 亿用户的增长，打破了 ChatGPT 的 2 个月的最快记录，成为史上增长最快的 AI 应用。\",\"通义千问\",\"通义千问是由阿里巴巴基于“通义”大模型研发，于 2023 年 4 月 正式发布。\",\"2023 年 9 月：阿里云开源了 Qwen（通义千问）系列工作。\",\"2024 年 6 月 6 日：正式开源了 Qwen2。\",\"2025 年 4 月 29 日：发布了全新升级的 Qwen3 系列模型。\",\"Qwen 系列均采用 decoder-only 架构，并结合 SwiGLU 激活、RoPE、GQA 等技术。中文能力相对来说是非常不错的开源模型。 目前，已经开源了 7 种模型大小：\",\"Dense 模型：0.6B、1.7B、4B、8B、14B、32B；\",\"MoE 模型：30B-A3B、235B-A22B。\",\"上下文长度：\",\"8B 以下模型的上下文长度为 32k；\",\"8B 以上模型的上下文长度为 128k。\",\"Qwen3 进一步增强了模型性能，改进了推理能力和指令遵循能力，同时保持了低资源部署的高效性，使其在长文本理解和复杂任务处理方面具有更强的优势。支持思考模式和非思考模式之间无缝切换，覆盖 119 种语言和方言。强化了模型的代码能力、Agent 能力，以及对 MCP 的支持。 同时还开源了代码模型和数学模型：\",\"Qwen2.5-Coder：1.5B、7B，以及即将推出的 32B。\",\"Qwen2.5-Math：1.5B、7B，以及 72B。\",\"在推理大模型方面：\",\"2024 年 11 月：发布并开源了 QwQ-32B-Preview 模型，仅用 32B 参数便在部分达到了 o1-mini 的推理水平。\",\"2025 年 3 月：发布并开源了 QwQ-32B，其性能可与具备 671B 参数（37B 激活参数）的 DeepSeek-R1 媲美。\",\"ChatGLM\",\"GLM系列模型是 清华大学和智谱 AI 等合作研发的语言大模型。\",\"2023 年 3 月，发布了 ChatGLM。\",\"2024 年 1 月，发布了 GLM4，并于 2024 年 6 月 正式开源。\",\"GLM-4-9B-Chat 支持多轮对话的同时，还具备网页浏览、代码执行、自定义工具调用（Function Call）和长文本推理（支持最大 128K 上下文）等功能。 开源了 对话模型 GLM-4-9B-Chat、基础模型 GLM-4-9B、长文本对话模型 GLM-4-9B-Chat-1M（支持 1M 上下文长度）、多模态模型 GLM-4V-9B 等全面对标 OpenAI。\"]},\"675\":{\"h\":\"LLM 的特点与能力\",\"t\":[\"大语言模型具有多种显著特点，这些特点使它们在自然语言处理和其他领域中引起了广泛的兴趣和研究。以下是大语言模型的一些主要特点：\",\"巨大的规模： LLM 通常具有巨大的参数规模，可以达到数十亿甚至数千亿个参数。这使得它们能够捕捉更多的语言知识和复杂的语法结构。\",\"预训练和微调： LLM 采用了预训练和微调的学习方法。首先在大规模文本数据上进行预训练（无标签数据），学习通用的语言表示和知识。然后通过微调（有标签数据）适应特定任务，从而在各种NLP 任务中表现出色。\",\"上下文感知： LLM 在处理文本时具有强大的上下文感知能力，能够理解和生成依赖于前文的文本内容。这使得它们在对话、文章生成和情境理解方面表现出色。\",\"多语言支持： LLM 可以用于多种语言，不仅限于英语。它们的多语言能力使得跨文化和跨语言的应用变得更加容易。\",\"多模态支持： 一些 LLM 已经扩展到支持多模态数据，包括文本、图像和声音。使得它们可以理解和生成不同媒体类型的内容，实现更多样化的应用。\",\"伦理和风险问题： 尽管 LLM 具有出色的能力，但它们也引发了伦理和风险问题，包括生成有害内容、隐私问题、认知偏差等。因此，研究和应用 LLM 需要谨慎。\",\"高计算资源需求： LLM 参数规模庞大，需要大量的计算资源进行训练和推理。通常需要使用高性能的 GPU 或 TPU 集群来实现。 大语言模型是一种具有强大语言处理能力的技术，已经在多个领域展示了潜力。它们为自然语言理解和生成任务提供了强大的工具，同时也引发了对其伦理和风险问题的关注。这些特点使 LLM 成为了当今计算机科学和人工智能领域的重要研究和应用方向。\"]},\"676\":{\"h\":\"\",\"t\":[\"区分大语言模型（LLM）与以前的预训练语言模型（PLM）最显著的特征之一是它们的 涌现能力。涌现能力是一种令人惊讶的能力，它在小型模型中不明显，但在大型模型中特别突出。类似物理学中的相变现象，涌现能力就像是模型性能随着规模增大而迅速提升，超过了随机水平，也就是我们常说的 量变引起质变。 涌现能力可以与某些复杂任务有关，但我们更关注的是其通用能力。接下来，我们简要介绍三个 LLM 典型的涌现能力：\",\"上下文学习：上下文学习能力是由 GPT-3 首次引入的。这种能力允许语言模型在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而无需额外的训练或参数更新。\",\"指令遵循：通过使用自然语言描述的多任务数据进行微调，也就是所谓的 指令微调。LLM 被证明在使用指令形式化描述的未见过的任务上表现良好。这意味着 LLM 能够根据任务指令执行任务，而无需事先见过具体示例，展示了其强大的泛化能力。\",\"逐步推理：小型语言模型通常难以解决涉及多个推理步骤的复杂任务，例如数学问题。然而，LLM 通过采用 思维链（CoT, Chain of Thought） 推理策略，利用包含中间推理步骤的提示机制来解决这些任务，从而得出最终答案。据推测，这种能力可能是通过对代码的训练获得的。 这些涌现能力让 LLM 在处理各种任务时表现出色，使它们成为了解决复杂问题和应用于多领域的强大工具。\"]},\"677\":{\"h\":\"\",\"t\":[\"在 2021 年，斯坦福大学等多所高校的研究人员提出了基座模型（foundation model）的概念，清晰了预训练模型的作用。这是一种全新的 AI 技术范式，借助于海量无标注数据的训练，获得可以适用于大量下游任务的大模型（单模态或者多模态）。这样，多个应用可以只依赖于一个或少数几个大模型进行统一建设。 大语言模型是这个新模式的典型例子，使用统一的大模型可以极大地提高研发效率。相比于每次开发单个模型的方式，这是一项本质上的进步。大型模型不仅可以缩短每个具体应用的开发周期，减少所需人力投入，也可以基于大模型的推理、常识和写作能力，获得更好的应用效果。因此，大模型可以成为 AI 应用开发的大一统基座模型，这是一个一举多得、全新的范式，值得大力推广。\"]},\"678\":{\"h\":\"\",\"t\":[\"让大语言模型真正火爆的契机，是基于对话聊天的 ChatGPT。业界很早就发现了用户对于对话交互的特殊偏好，陆奇在微软期间，就于 2016 年推进过“对话即平台（conversation as a platform）” 的战略。此外，苹果 Siri 、亚马逊 Echo 等基于语音对话的产品也非常受欢迎，反映出互联网用户对于聊天和对话这种交互模式的偏好。虽然之前的聊天机器人存在各种问题，但大型语言模型的出现再次让聊天机器人这种交互模式可以重新涌现。用户愈发期待像钢铁侠中“贾维斯”一样的人工智能，无所不能、无所不知。这引发我们对于 智能体（Agent） 类型应用前景的思考，Auto-GPT、微软 Jarvis 等项目已经出现并受到关注，相信未来会涌现出很多类似的以对话形态让助手完成各种具体工作的项目。\",\"LLM 已经在许多领域产生了深远的影响。在自然语言处理领域，它可以帮助计算机更好地理解和生成文本，包括写文章、回答问题、翻译语言等。在信息检索领域，它可以改进搜索引擎，让我们更轻松地找到所需的信息。在计算机视觉领域，研究人员还在努力让计算机理解图像和文字，以改善多媒体交互。 最重要的是，LLM 的出现让人们重新思考了 通用人工智能（AGI） 的可能性。AGI 是一种像人类一样思考和学习的人工智能。LLM 被认为是 AGI 的一种早期形式，这引发了对未来人工智能发展的许多思考和计划。\",\"总之，LLM 是一种令人兴奋的技术，它让计算机更好地理解和使用语言，正在改变着我们与技术互动的方式，同时也引发了对未来人工智能的无限探索。\"]},\"679\":{\"h\":\"检索增强生成（RAG, Retrieval-Augmented Generation）\",\"t\":[\"大型语言模型（LLM）相较于传统的语言模型具有更强大的能力，然而在某些情况下，它们仍可能无法提供准确的答案。为了解决大型语言模型在生成文本时面临的一系列挑战，提高模型的性能和输出质量，研究人员提出了一种新的模型架构：检索增强生成（RAG, Retrieval-Augmented Generation）。该架构巧妙地整合了从庞大知识库中检索到的相关信息，并以此为基础，指导大型语言模型生成更为精准的答案，从而显著提升了回答的准确性与深度。 目前 LLM 面临的主要问题有：\",\"信息偏差/幻觉： LLM 有时会产生与客观事实不符的信息，导致用户接收到的信息不准确。RAG 通过检索数据源，辅助模型生成过程，确保输出内容的精确性和可信度，减少信息偏差。\",\"知识更新滞后性： LLM 基于静态的数据集训练，这可能导致模型的知识更新滞后，无法及时反映最新的信息动态。RAG 通过实时检索最新数据，保持内容的时效性，确保信息的持续更新和准确性。\",\"内容不可追溯： LLM 生成的内容往往缺乏明确的信息来源，影响内容的可信度。RAG 将生成内容与检索到的原始资料建立链接，增强了内容的可追溯性，从而提升了用户对生成内容的信任度。\",\"领域专业知识能力欠缺： LLM 在处理特定领域的专业知识时，效果可能不太理想，这可能会影响到其在相关领域的回答质量。RAG 通过检索特定领域的相关文档，为模型提供丰富的上下文信息，从而提升了在专业领域内的问题回答质量和深度。\",\"推理能力限制： 面对复杂问题时，LLM 可能缺乏必要的推理能力，这影响了其对问题的理解和回答。RAG 结合检索到的信息和模型的生成能力，通过提供额外的背景知识和数据支持，增强了模型的推理和理解能力。\",\"应用场景适应性受限： LLM 需在多样化的应用场景中保持高效和准确，但单一模型可能难以全面适应所有场景。RAG 使得 LLM 能够通过检索对应应用场景数据的方式，灵活适应问答系统、推荐系统等多种应用场景。\",\"长文本处理能力较弱： LLM 在理解和生成长篇内容时受限于有限的上下文窗口，且必须按顺序处理内容，输入越长，速度越慢。RAG 通过检索和整合长文本信息，强化了模型对长上下文的理解和生成，有效突破了输入长度的限制，同时降低了调用成本，并提升了整体的处理效率。\"]},\"680\":{\"h\":\"工作流程\",\"t\":[\"RAG 是一个完整的系统，其工作流程可以简单地分为数据处理、检索、增强和生成四个阶段：\",\"数据处理阶段: 对原始数据进行清洗和处理; 将处理后的数据转化为检索模型可以使用的格式; 将处理后的数据存储在对应的数据库中。\",\"检索阶段: 将用户的问题输入到检索系统中，从数据库中检索相关信息。\",\"增强阶段: 对检索到的信息进行处理和增强，以便生成模型可以更好地理解和使用。\",\"生成阶段: 将增强后的信息输入到生成模型中，生成模型根据这些信息生成答案。\"]},\"681\":{\"h\":\"RAG VS Finetune\",\"t\":[\"在提升大语言模型效果中，RAG 和 微调（Finetune）是两种主流的方法。\",\"微调: 通过在特定数据集上进一步训练大语言模型，来提升模型在特定任务上的表现。\",\"RAG 和 微调的对比可以参考下表 :\",\"特征比较\",\"RAG\",\"微调\",\"知识更新\",\"直接更新检索知识库，无需重新训练。信息更新成本低，适合动态变化的数据。\",\"通常需要重新训练来保持知识和数据的更新。更新成本高，适合相对稳定的数据。\",\"数据处理\",\"对数据的处理和操作要求极低。\",\"依赖于构建高质量的数据集，有限的数据集可能无法显著提高性能。\",\"模型定制\",\"侧重于信息检索和融合外部知识，但可能无法充分定制模型行为或写作风格。\",\"可以根据特定风格或术语调整 LLM 行为、写作风格或特定领域知识。\",\"可解释性\",\"可以追溯到具体的数据来源，有较好的可解释性和可追踪性。\",\"黑盒子，可解释性相对较低。\",\"特征比较\",\"RAG\",\"微调\",\"计算资源\",\"需要额外的资源来支持检索机制和数据库的维护。\",\"依赖高质量的训练数\",\"推理延迟\",\"增加了检索步骤的耗时\",\"单纯 LLM 生成的耗时\",\"降低幻觉\",\"通过检索到的真实信息生成回答，降低了产生幻觉的概率。\",\"模型学习特定领域的数据有助于减少幻觉，但面对未见过的输入时仍可能出现幻觉。\",\"伦理隐私\",\"检索和使用外部数据可能引发伦理和隐私方面的问题。\",\"训练数据中的敏感信息需要妥善处理，以防泄露。\"]},\"682\":{\"h\":\"LangChain\",\"t\":[\"ChatGPT 的巨大成功激发了越来越多的开发者兴趣，他们希望利用 OpenAI 提供的 API 或者私有化模型，来开发基于大型语言模型的应用程序。尽管大型语言模型的调用相对简单，但要创建完整的应用程序，仍然需要大量的定制开发工作，包括 API 集成、互动逻辑、数据存储等等。\",\"为了解决这个问题，从 2022 年开始，许多机构和个人相继推出了多个开源项目，旨在帮助开发者们快速构建基于大型语言模型的端到端应用程序或工作流程。其中一个备受关注的项目就是 LangChain 框架。\",\"LangChain 框架是一个开源工具，充分利用了大型语言模型的强大能力，以便开发各种下游应用。它的目标是为各种大型语言模型应用提供通用接口，从而简化应用程序的开发流程。具体来说，LangChain 框架可以实现数据感知和环境互动，也就是说，它能够让语言模型与其他数据来源连接，并且允许语言模型与其所处的环境进行互动。\",\"利用 LangChain 框架，我们可以轻松地构建如下所示的 RAG 应用。在下图中，每个椭圆形代表了 LangChain 的一个模块，例如数据收集模块或预处理模块。每个矩形代表了一个数据状态，例如原始数据或预处理后的数据。箭头表示数据流的方向，从一个模块流向另一个模块。在每一步中，LangChain 都可以提供对应的解决方案，帮助我们处理各种任务。\"]},\"683\":{\"h\":\"核心组件\",\"t\":[\"LangChian 作为一个大语言模型开发框架，可以将 LLM 模型（对话模型、embedding 模型等）、向量数据库、交互层 Prompt、外部知识、外部代理工具整合到一起，进而可以自由构建 LLM 应用。 LangChain 主要由以下 6 个核心组件组成:\",\"模型输入/输出（Model I/O）：与语言模型交互的接口\",\"数据连接（Data connection）：与特定应用程序的数据进行交互的接口\",\"链（Chains）：将组件组合实现端到端应用。比如后续我们会将搭建检索问答链来完成检索问答。\",\"记忆（Memory）：用于链的多次运行之间持久化应用程序状态；\",\"代理（Agents）：扩展模型的推理能力。用于复杂的应用的调用序列；\",\"回调（Callbacks）：扩展模型的推理能力。用于复杂的应用的调用序列；\",\"在开发过程中，我们可以根据自身需求灵活地进行组合。\"]},\"684\":{\"h\":\"版本迭代\",\"t\":[\"在 LLM 技术领域的迅猛发展浪潮中，LangChain 作为一个不断进化的创新平台，持续推动着技术边界的拓展。2024 年 9 月 16 日，LangChain 正式发布了其稳定版本 v0.3，这一里程碑式的更新，为开发者带来了全面而强大的功能支持。其涵盖了模型的输入与输出处理、数据连接、链式操作、记忆机制、代理服务以及回调处理等关键组件，为 LLM 应用的开发和部署提供了坚实的基础。 同时，LangChain 的持续优化和功能迭代，未来将带来更多创新特性和性能提升。\",\"兼容性与支持：LangChain 兼顾了对 Python 和 JavaScript 的支持，同时保持了向后兼容性，确保开发者能够在升级过程中无缝过渡，享受到更加安全稳定的开发体验。\",\"架构改进：通过将核心组件 langchain-core 与合作伙伴包进行有效分离，LangChain 的架构设计变得更加条理清晰和稳固，为未来的系统化扩展和安全性提升奠定了坚实基础。\",\"可观察性：LangChain 通过与 LangSmith 的深度集成，提供了业界领先的调试和观测功能。这使得开发者能够对 LLM 应用中的每一步操作及其输入输出有一个清晰的认识，极大地简化了调试和问题排查的流程。\",\"广泛的集成：LangChain 拥有近 700 个集成，覆盖了从 LLM 到向量存储、工具和智能体（Agent）等多个技术领域，极大地降低了在各种技术栈上构建 LLM 应用的复杂度。\",\"可组合性：借助 LangChain 表达式语言（LCEL），开发者可以轻松地构建和定制 chain，充分利用数据编排框架的优势，包括批量处理、并行化操作和备选方案等高级功能。\",\"流式处理：LangChain 对流式处理进行了深度优化，确保所有利用 LCEL 创建的 chain 均能支持流式处理，包括中间步骤的数据流传输，从而为用户提供更加流畅的体验。\",\"输出解析：LangChain 提供了一系列强大的输出解析工具，确保 LLM 能够以结构化的格式返回信息，这对于 LLM 执行具体行动计划至关重要。\",\"检索能力：LangChain 引入了先进的检索技术，适用于生产环境，包括文本分割、检索机制和索引管道等，使得开发者能够轻松地将私有数据与 LLM 的能力相结合。\",\"工具使用与智能体：LangChain 提供了丰富的智能体和工具集合，并提供了定义工具的简便方法，支持智能体工作负载，包括让 LLM 调用函数或工具，以及如何高效地进行多次调用和推理，极大地提升了开发效率和应用性能。\"]},\"685\":{\"h\":\"生态圈\",\"t\":[\"LangChain Community: 专注于第三方集成，极大地丰富了 LangChain 的生态系统，使得开发者可以更容易地构建复杂和强大的应用程序，同时也促进了社区的合作和共享。\",\"LangChain Core: LangChain 框架的核心库、核心组件，提供了基础抽象和 LangChain 表达式语言（LCEL），提供基础架构和工具，用于构建、运行和与 LLM 交互的应用程序，为 LangChain 应用程序的开发提供了坚实的基础。我们后续会用到的处理文档、格式化 prompt、输出解析等都来自这个库。\",\"LangChain CLI: 命令行工具，使开发者能够通过终端与 LangChain 框架交互，执行项目初始化、测试、部署等任务。提高开发效率，让开发者能够通过简单的命令来管理整个应用程序的生命周期。\",\"LangServe: 部署服务，用于将 LangChain 应用程序部署到云端，提供可扩展、高可用的托管解决方案，并带有监控和日志功能。简化部署流程，让开发者可以专注于应用程序的开发，而不必担心底层的基础设施和运维工作。\",\"LangSmith: 开发者平台，专注于 LangChain 应用程序的开发、调试和测试，提供可视化界面和性能分析工具，旨在帮助开发者提高应用程序的质量，确保它们在部署前达到预期的性能和稳定性标准。\"]},\"686\":{\"h\":\"大模型开发\",\"t\":[\"我们将开发以大语言模型为功能核心、通过大语言模型的强大理解能力和生成能力、结合特殊的数据或业务逻辑来提供独特功能的应用称为大模型开发。开发大模型相关应用，其技术核心点虽然在大语言模型上，但一般通过调用 API 或开源模型来实现核心的理解与生成，通过 Prompt Enginnering 来实现大语言模型的控制，因此，虽然大模型是深度学习领域的集大成之作，大模型开发却更多是一个工程问题。\",\"在大模型开发中，我们一般不会去大幅度改动模型，而是将大模型作为一个调用工具，通过 Prompt Engineering、数据工程、业务逻辑分解等手段来充分发挥大模型能力，适配应用任务，而不会将精力聚焦在优化模型本身上。因此，作为大模型开发的初学者，我们并不需要深研大模型内部原理，而更需要掌握使用大模型的实践技巧。\",\"# 大语言模型 ## Prompt Engineering ## 数据工程 ## 业务逻辑分解 ## 验证迭代优化\",\"同时，以调用、发挥大模型为核心的大模型开发与传统的 AI 开发在整体思路上有着较大的不同。大语言模型的两个核心能力：指令遵循与文本生成提供了复杂业务逻辑的简单平替方案。\",\"传统的 AI 开发：首先需要将非常复杂的业务逻辑依次拆解，对于每一个子业务构造训练数据与验证数据，对于每一个子业务训练优化模型，最后形成完整的模型链路来解决整个业务逻辑。\",\"大模型开发：用 Prompt Engineering 来替代子模型的训练调优，通过 Prompt 链路组合来实现业务逻辑，用一个通用大模型 + 若干业务 Prompt 来解决任务，从而将传统的模型训练调优转变成了更简单、轻松、低成本的 Prompt 设计调优。\",\"同时，在评估思路上，大模型开发与传统 AI 开发也有质的差异。\",\"传统 AI 开发：需要首先构造训练集、测试集、验证集，通过在训练集上训练模型、在测试集上调优模型、在验证集上最终验证模型效果来实现性能的评估。\",\"大模型开发：流程更为灵活和敏捷。从实际业务需求出发构造小批量验证集，设计合理 Prompt 来满足验证集效果。然后，将不断从业务逻辑中收集当下 Prompt 的 Bad Case，并将 Bad Case 加入到验证集中，针对性优化 Prompt，最后实现较好的泛化效果。\"]},\"687\":{\"h\":\"基本流程\",\"t\":[\"结合上述分析，我们一般可以将大模型开发分解为以下几个流程：\",\"确定目标: 在进行开发前，我们首先需要确定开发的目标，即要开发的应用的应用场景、目标人群、核心价值。对于个体开发者或小型开发团队而言，一般应先设定最小化目标，从构建一个 MVP（最小可行性产品）开始，逐步进行完善和优化。\",\"设计功能: 在确定开发目标后，需要设计本应用所要提供的功能，以及每一个功能的大体实现逻辑。虽然我们通过使用大模型来简化了业务逻辑的拆解，但是越清晰、深入的业务逻辑理解往往也能带来更好的 Prompt 效果。同样，对于个体开发者或小型开发团队来说，首先要确定应用的核心功能，然后延展设计核心功能的上下游功能；\",\"搭建整体架构: 目前，绝大部分大模型应用都是采用的特定数据库 + Prompt + 通用大模型的架构。我们需要针对我们所设计的功能，搭建项目的整体架构，实现从用户输入到应用输出的全流程贯通。一般来说，我们推荐基于 LangChain 框架进行开发。LangChain 提供了 Chain、Tool 等架构的实现，我们可以基于 LangChain 进行个性化定制，实现从用户输入到数据库再到大模型最后输出的整体架构连接。\",\"搭建数据库: 个性化大模型应用需要有个性化数据库进行支撑。由于大模型应用需要进行向量语义检索，一般使用诸如 Chroma 的向量数据库。在该步骤中，我们需要收集数据并进行预处理，再向量化存储到数据库中。数据预处理一般包括从多种格式向纯文本的转化，例如 PDF、MarkDown、HTML、音视频等，以及对错误数据、异常数据、脏数据进行清洗。完成预处理后，需要进行切片、向量化构建出个性化数据库。\",\"Prompt Engineering: 优质的 Prompt 对大模型能力具有极大影响，我们需要逐步迭代构建优质的 Prompt Engineering 来提升应用性能。在该步中，我们首先应该明确 Prompt 设计的一般原则及技巧，构建出一个来源于实际业务的小型验证集，基于小型验证集设计满足基本要求、具备基本能力的 Prompt。\",\"验证迭代: 验证迭代在大模型开发中是极其重要的一步，一般指通过不断发现 Bad Case 并针对性改进 Prompt Engineering 来提升系统效果、应对边界情况。在完成上一步的初始化 Prompt 设计后，我们应该进行实际业务测试，探讨边界情况，找到 Bad Case，并针对性分析 Prompt 存在的问题，从而不断迭代优化，直到达到一个较为稳定、可以基本实现目标的 Prompt 版本。\",\"前后端搭建: 完成 Prompt Engineering 及其迭代优化之后，我们就完成了应用的核心功能，可以充分发挥大语言模型的强大能力。接下来我们需要搭建前后端，设计产品页面，让我们的应用能够上线成为产品。\",\"体验优化: 在完成前后端搭建之后，应用就可以上线体验了。接下来就需要进行长期的用户体验跟踪，记录 Bad Case 与用户负反馈，再针对性进行优化即可。\"]},\"688\":{\"h\":\"参考\",\"t\":[\"LLM 部分:\",\"A Survey of Large Language Models\",\"周枫：当我们谈论大模型时，应该关注哪些新能力？\",\"S型智能增长曲线：从Deepseek R1看Scaling Law的未来\",\"一文详尽之Scaling Law！\",\"QwQ: 思忖未知之界\",\"QwQ-32B: 领略强化学习之力\",\"RAG 部分:\",\"Retrieval-Augmented Generation for Large Language Models: A Survey\",\"面向大语言模型的检索增强生成技术：综述\"]},\"689\":{\"h\":\"2.大模型API使用\",\"t\":[\"智慧化知识库系统: LLM API的调用。\"]},\"690\":{\"h\":\"智慧化知识库系统\"},\"691\":{\"h\":\"\",\"t\":[\"404 Not Found\"]}},\"dirtCount\":0,\"index\":[[\"周枫\",{\"1\":{\"688\":1}}],[\"探讨边界情况\",{\"1\":{\"687\":1}}],[\"探索\",{\"1\":{\"472\":1}}],[\"探索指令格式变化对模型泛化能力的影响\",{\"1\":{\"469\":1}}],[\"脏数据进行清洗\",{\"1\":{\"687\":1}}],[\"音视频等\",{\"1\":{\"687\":1}}],[\"音频和视频\",{\"1\":{\"674\":1}}],[\"绝大部分大模型应用都是采用的特定数据库\",{\"1\":{\"687\":1}}],[\"搭建数据库\",{\"1\":{\"687\":1}}],[\"搭建项目的整体架构\",{\"1\":{\"687\":1}}],[\"搭建整体架构\",{\"1\":{\"687\":1}}],[\"搭配70亿或130亿参数的llms\",{\"1\":{\"208\":1}}],[\"业务逻辑分解\",{\"1\":{\"686\":1}}],[\"业务逻辑分解等手段来充分发挥大模型能力\",{\"1\":{\"686\":1}}],[\"业界很早就发现了用户对于对话交互的特殊偏好\",{\"1\":{\"678\":1}}],[\"流程更为灵活和敏捷\",{\"1\":{\"686\":1}}],[\"流程四步走\",{\"1\":{\"100\":1}}],[\"流式处理\",{\"1\":{\"684\":1}}],[\"拥有近\",{\"1\":{\"684\":1}}],[\"拥有一个规模大且特征较为一致的字典\",{\"1\":{\"237\":1}}],[\"享受到更加安全稳定的开发体验\",{\"1\":{\"684\":1}}],[\"链路组合来实现业务逻辑\",{\"1\":{\"686\":1}}],[\"链式操作\",{\"1\":{\"684\":1}}],[\"链式法则\",{\"0\":{\"625\":1}}],[\"链\",{\"1\":{\"683\":1}}],[\"箭头表示数据流的方向\",{\"1\":{\"682\":1}}],[\"许多机构和个人相继推出了多个开源项目\",{\"1\":{\"682\":1}}],[\"许多研究人员开始训练越来越庞大的语言模型\",{\"1\":{\"673\":1}}],[\"黑盒子\",{\"1\":{\"681\":1}}],[\"黑点为采样点\",{\"1\":{\"397\":1}}],[\"微软\",{\"1\":{\"678\":1}}],[\"微调后准确率从63\",{\"1\":{\"483\":1}}],[\"微调也仅为\",{\"1\":{\"472\":1}}],[\"微调模型\",{\"1\":{\"470\":1}}],[\"微调预训练\",{\"1\":{\"470\":1}}],[\"微调预训练的gpt\",{\"1\":{\"468\":1}}],[\"微调小型数据集以嵌入价值观\",{\"1\":{\"469\":1}}],[\"微调范式\",{\"1\":{\"464\":1}}],[\"微调范式与任务适应能力的关系\",{\"1\":{\"464\":1}}],[\"微调任务和数据集如下\",{\"1\":{\"448\":1}}],[\"微调的对比可以参考下表\",{\"1\":{\"681\":1}}],[\"微调的细节\",{\"1\":{\"447\":1}}],[\"微调的场景中\",{\"1\":{\"428\":1}}],[\"微调的最终目的\",{\"1\":{\"416\":1}}],[\"微调流程\",{\"1\":{\"425\":1}}],[\"微调之后的模型\",{\"1\":{\"424\":1}}],[\"微调相对来说就是一个更优的方案\",{\"1\":{\"415\":1}}],[\"微调成本高\",{\"1\":{\"231\":1}}],[\"微调整个模型参数\",{\"1\":{\"231\":1}}],[\"微调时保持视觉编码器参数不变\",{\"1\":{\"227\":1}}],[\"微调时提升为\",{\"1\":{\"131\":1}}],[\"微调过程\",{\"1\":{\"227\":1}}],[\"微调数据规模\",{\"1\":{\"483\":1}}],[\"微调数据\",{\"1\":{\"217\":1}}],[\"微调阶段的数据组织方式如下\",{\"1\":{\"227\":1}}],[\"微调阶段使用的是作者自己构建的高质量多模态指令数据集\",{\"1\":{\"227\":1}}],[\"微调阶段\",{\"0\":{\"142\":1,\"145\":1},\"1\":{\"225\":1,\"442\":1}}],[\"微调\",{\"0\":{\"227\":1,\"425\":1},\"1\":{\"24\":1,\"198\":1,\"202\":1,\"461\":1,\"464\":1,\"469\":1,\"681\":4}}],[\"智能体\",{\"1\":{\"678\":1}}],[\"智慧化知识库系统\",{\"0\":{\"690\":1},\"1\":{\"672\":1,\"689\":1}}],[\"贾维斯\",{\"1\":{\"678\":1}}],[\"亚马逊\",{\"1\":{\"678\":1}}],[\"苹果\",{\"1\":{\"678\":1}}],[\"陆奇在微软期间\",{\"1\":{\"678\":1}}],[\"获得更好的应用效果\",{\"1\":{\"677\":1}}],[\"获得可以适用于大量下游任务的大模型\",{\"1\":{\"677\":1}}],[\"获取元素总数\",{\"1\":{\"659\":1}}],[\"获取维度数\",{\"1\":{\"659\":1}}],[\"获取实际对象\",{\"1\":{\"657\":1}}],[\"获取选项个数\",{\"1\":{\"544\":1}}],[\"获取词嵌入层权重\",{\"1\":{\"513\":1}}],[\"获取所有单词并去重\",{\"1\":{\"511\":1}}],[\"获取所有单词和每个单词的出现次数词典\",{\"1\":{\"410\":1}}],[\"获取每个相邻字符对的出现次数\",{\"1\":{\"410\":3}}],[\"获取子张量\",{\"1\":{\"325\":1}}],[\"获取第二列\",{\"1\":{\"325\":1}}],[\"获取第二行\",{\"1\":{\"325\":1}}],[\"获取输入张量x的形状\",{\"1\":{\"295\":1}}],[\"获取输入图像张量的形状\",{\"1\":{\"291\":1}}],[\"获取输入维度信息\",{\"1\":{\"40\":1}}],[\"获取对应图像的标签\",{\"1\":{\"289\":1}}],[\"获取对应的负文本嵌入\",{\"1\":{\"162\":1}}],[\"获取对应的负图像嵌入\",{\"1\":{\"162\":1}}],[\"获取该类别对应的索引\",{\"1\":{\"289\":1}}],[\"获取其对应的图像嵌入向量列表\",{\"1\":{\"275\":1}}],[\"获取候选分类名列表\",{\"1\":{\"275\":1,\"277\":1}}],[\"获取全局区域特征向量后\",{\"1\":{\"93\":1}}],[\"获取当前最高频的字符对\",{\"1\":{\"410\":3}}],[\"获取当前\",{\"1\":{\"107\":1}}],[\"获取当前功能类型对应的索引值\",{\"1\":{\"68\":1}}],[\"获取当前样本对应的问题文本\",{\"1\":{\"68\":1}}],[\"获取当前样本对应的功能区域掩码\",{\"1\":{\"68\":1}}],[\"获取当前样本对应的功能类型\",{\"1\":{\"68\":1}}],[\"获取当前样本对应的物体类别\",{\"1\":{\"68\":1}}],[\"获取当前样本对应的点云id\",{\"1\":{\"68\":1}}],[\"获取当前样本的物体类别和物体信息值\",{\"1\":{\"68\":1}}],[\"获取当前样本中多模态嵌入的维度信息\",{\"1\":{\"43\":1}}],[\"获取样本的代码实现\",{\"1\":{\"68\":1}}],[\"获取两个注意力加权结果\",{\"1\":{\"45\":1}}],[\"获取\",{\"1\":{\"43\":1,\"45\":1,\"163\":2,\"266\":1,\"513\":1}}],[\"获取语言嵌入\",{\"1\":{\"40\":1}}],[\"获取设备信息\",{\"1\":{\"40\":1}}],[\"获取图片\",{\"1\":{\"29\":1,\"58\":1}}],[\"获取数据类型\",{\"1\":{\"659\":1}}],[\"获取数据形状\",{\"1\":{\"659\":1}}],[\"获取数据\",{\"1\":{\"29\":1,\"58\":1}}],[\"斯坦福大学等多所高校的研究人员提出了基座模型\",{\"1\":{\"677\":1}}],[\"斯坦福提出的\",{\"1\":{\"424\":1}}],[\"据推测\",{\"1\":{\"676\":1}}],[\"量变引起质变\",{\"1\":{\"676\":1}}],[\"量化通过将这些高精度的浮点数转换为低精度的整数\",{\"1\":{\"428\":1}}],[\"量化是一种在深度学习领域用于减少模型内存占用和计算量的技术\",{\"1\":{\"428\":1}}],[\"量化的核心目标是降成本\",{\"1\":{\"421\":1}}],[\"量化\",{\"1\":{\"421\":2}}],[\"量化误差\",{\"1\":{\"396\":1}}],[\"量化分析\",{\"1\":{\"23\":1}}],[\"伦理隐私\",{\"1\":{\"681\":1}}],[\"伦理和风险问题\",{\"1\":{\"675\":1}}],[\"伦理响应\",{\"1\":{\"483\":1}}],[\"巨大的规模\",{\"1\":{\"675\":1}}],[\"媲美\",{\"1\":{\"674\":1}}],[\"天完成了\",{\"1\":{\"674\":1}}],[\"天注册人数突破\",{\"1\":{\"674\":1}}],[\"团队开发的开源大语言模型系列\",{\"1\":{\"674\":1}}],[\"讯飞发布了推理思考模型\",{\"1\":{\"674\":1}}],[\"讯飞星火\",{\"1\":{\"674\":1}}],[\"讯飞星火发布模型\",{\"1\":{\"674\":1}}],[\"讯飞星火认知大模型是科大讯飞发布的语言大模型\",{\"1\":{\"674\":1}}],[\"星火语音同传模型\",{\"1\":{\"674\":1}}],[\"星火\",{\"1\":{\"674\":1}}],[\"星火大模型\",{\"1\":{\"674\":1}}],[\"免费版\",{\"1\":{\"674\":2}}],[\"生态圈\",{\"0\":{\"685\":1}}],[\"生物计算大模型\",{\"1\":{\"674\":1}}],[\"生成模型根据这些信息生成答案\",{\"1\":{\"680\":1}}],[\"生成模型学习\",{\"0\":{\"602\":1}}],[\"生成图表和可视化报告\",{\"1\":{\"674\":1}}],[\"生成图像仍存在以下问题\",{\"1\":{\"178\":1}}],[\"生成图像的合成描述\",{\"1\":{\"120\":1}}],[\"生成dot语言字符串\",{\"1\":{\"666\":1}}],[\"生成函数节点的dot描述\",{\"1\":{\"666\":1}}],[\"生成变量节点的dot描述\",{\"1\":{\"666\":1}}],[\"生成式预训练语言模型\",{\"1\":{\"674\":1}}],[\"生成式\",{\"1\":{\"542\":2}}],[\"生成式generative模型的推理过程很有特点\",{\"1\":{\"474\":1}}],[\"生成一个上下文相关的表示\",{\"1\":{\"542\":1}}],[\"生成一组动态卷积核\",{\"1\":{\"76\":2}}],[\"生成注意力掩码\",{\"1\":{\"520\":1}}],[\"生成padding部分的mask列表\",{\"1\":{\"520\":1}}],[\"生成能力\",{\"1\":{\"483\":1}}],[\"生成位置编码\",{\"1\":{\"477\":1}}],[\"生成有害或无关文本\",{\"1\":{\"468\":1}}],[\"生成样本分析\",{\"1\":{\"455\":1}}],[\"生成摘要\",{\"1\":{\"455\":1}}],[\"生成多种不同推理路径所得的结果的集合\",{\"1\":{\"435\":1}}],[\"生成类别名称以及对应的数字索引\",{\"1\":{\"289\":1}}],[\"生成文本的最小长度\",{\"1\":{\"286\":1}}],[\"生成文本的最大长度\",{\"1\":{\"286\":1}}],[\"生成文本嵌入\",{\"1\":{\"275\":1,\"277\":1}}],[\"生成学习\",{\"0\":{\"286\":1}}],[\"生成压缩的视觉表示\",{\"1\":{\"285\":1}}],[\"生成text\",{\"1\":{\"285\":1}}],[\"生成比标签\",{\"1\":{\"283\":1}}],[\"生成任务\",{\"1\":{\"189\":1}}],[\"生成和对话任务\",{\"1\":{\"181\":1}}],[\"生成掩码位置\",{\"1\":{\"163\":1}}],[\"生成伪标签\",{\"1\":{\"157\":1}}],[\"生成伪标签辅助训练\",{\"1\":{\"150\":1}}],[\"生成阶段\",{\"0\":{\"143\":1},\"1\":{\"680\":1}}],[\"生成合成文本\",{\"1\":{\"132\":1}}],[\"生成固定长度的特征向量\",{\"1\":{\"96\":1}}],[\"生成点集的划分\",{\"1\":{\"86\":1}}],[\"生成\",{\"1\":{\"82\":1,\"122\":1,\"143\":3,\"197\":1,\"198\":1,\"542\":1}}],[\"生成融合特征\",{\"1\":{\"73\":1}}],[\"生成动态卷积核\",{\"1\":{\"70\":1}}],[\"生成的耗时\",{\"1\":{\"681\":1}}],[\"生成的内容往往缺乏明确的信息来源\",{\"1\":{\"679\":1}}],[\"生成的最小\",{\"1\":{\"566\":1}}],[\"生成的文本特征相当于分类器的权重\",{\"1\":{\"273\":1}}],[\"生成的文本更具有\",{\"1\":{\"133\":1}}],[\"生成的错误文本更难被\",{\"1\":{\"134\":1}}],[\"生成的多样化问题\",{\"1\":{\"69\":1}}],[\"生成的问题遵循以下三个关键原则\",{\"1\":{\"63\":1}}],[\"生成最终的\",{\"1\":{\"46\":1}}],[\"生成功能区域掩码\",{\"1\":{\"35\":1}}],[\"生成细粒度表示\",{\"1\":{\"12\":1}}],[\"版\",{\"1\":{\"674\":1}}],[\"版本迭代\",{\"0\":{\"684\":1}}],[\"版本进一步加入\",{\"1\":{\"470\":1}}],[\"版本\",{\"1\":{\"196\":1,\"275\":1,\"290\":1,\"331\":1,\"674\":4,\"687\":1}}],[\"版本代码\",{\"1\":{\"85\":2,\"102\":2}}],[\"费用最高\",{\"1\":{\"674\":1}}],[\"速度越慢\",{\"1\":{\"679\":1}}],[\"速度最快\",{\"1\":{\"674\":1}}],[\"速度快\",{\"1\":{\"674\":1}}],[\"日\",{\"1\":{\"674\":6,\"684\":1}}],[\"日发布\",{\"1\":{\"674\":1}}],[\"日志记录\",{\"1\":{\"379\":1}}],[\"于\",{\"1\":{\"674\":4}}],[\"于是代码中使用zip\",{\"1\":{\"652\":1}}],[\"于是你把这\",{\"1\":{\"397\":1}}],[\"于是\",{\"1\":{\"238\":1}}],[\"离职人员创建的\",{\"1\":{\"674\":1}}],[\"离散分布\",{\"0\":{\"574\":1}}],[\"离散随机变量形式\",{\"0\":{\"571\":1}}],[\"离散随机变量\",{\"0\":{\"565\":1}}],[\"专用代码生成模型\",{\"1\":{\"674\":1}}],[\"专业版\",{\"1\":{\"674\":2}}],[\"专注于\",{\"1\":{\"685\":1}}],[\"专注于第三方集成\",{\"1\":{\"685\":1}}],[\"专注于复杂问题求解和精确推理能力\",{\"1\":{\"674\":1}}],[\"专注于精确推理和复杂问题求解\",{\"1\":{\"674\":1}}],[\"专注于广泛知识覆盖和流畅对话体验\",{\"1\":{\"674\":1}}],[\"专门对话\",{\"1\":{\"674\":1}}],[\"专门用于预测一个\",{\"1\":{\"107\":1}}],[\"慢\",{\"1\":{\"674\":1}}],[\"慢慢增加\",{\"1\":{\"159\":1}}],[\"价格更加亲民\",{\"1\":{\"674\":1}}],[\"价值\",{\"1\":{\"472\":1}}],[\"快约\",{\"1\":{\"674\":1}}],[\"快速适应新任务\",{\"1\":{\"460\":1}}],[\"快速给出答案\",{\"1\":{\"433\":1}}],[\"快速找到合适的模型\",{\"1\":{\"287\":1}}],[\"截止\",{\"1\":{\"674\":1}}],[\"截断过长文本\",{\"1\":{\"40\":1}}],[\"月比较有影响力并且模型参数量超过\",{\"1\":{\"674\":1}}],[\"月\",{\"1\":{\"674\":50,\"684\":1}}],[\"月之前\",{\"1\":{\"673\":1}}],[\"月发布的\",{\"1\":{\"270\":1}}],[\"世界知识的语义\",{\"1\":{\"674\":1}}],[\"世界坐标系和局部坐标系\",{\"1\":{\"86\":1}}],[\"世纪\",{\"1\":{\"673\":1}}],[\"称之为\",{\"1\":{\"673\":1}}],[\"称为二项系数\",{\"1\":{\"575\":1}}],[\"称为后验概率\",{\"1\":{\"571\":1}}],[\"称为似然\",{\"1\":{\"571\":1}}],[\"称为先验概率\",{\"1\":{\"571\":1}}],[\"称为概率密度函数\",{\"1\":{\"566\":1}}],[\"称为概率质量函数\",{\"1\":{\"565\":1}}],[\"称为\",{\"1\":{\"565\":1}}],[\"称为混淆矩阵\",{\"1\":{\"342\":1}}],[\"称为随机输入丢弃\",{\"1\":{\"95\":1}}],[\"涌现能力可以与某些复杂任务有关\",{\"1\":{\"676\":1}}],[\"涌现能力就像是模型性能随着规模增大而迅速提升\",{\"1\":{\"676\":1}}],[\"涌现能力是一种令人惊讶的能力\",{\"1\":{\"676\":1}}],[\"涌现能力\",{\"1\":{\"673\":1,\"676\":1}}],[\"涌现能力研究\",{\"1\":{\"485\":1}}],[\"豆包\",{\"1\":{\"673\":1}}],[\"国内外有超过百种大模型相继发布\",{\"1\":{\"674\":1}}],[\"国内的有\",{\"1\":{\"673\":1}}],[\"国外的知名\",{\"1\":{\"673\":1}}],[\"拓展\",{\"1\":{\"670\":1}}],[\"拓扑复杂\",{\"1\":{\"114\":1}}],[\"🏗️\",{\"0\":{\"669\":1}}],[\"🌟\",{\"1\":{\"103\":1}}],[\"迭代次数设为iters\",{\"1\":{\"667\":1}}],[\"迭代式对齐研究计划的一部分\",{\"1\":{\"472\":1}}],[\"香蕉函数\",{\"1\":{\"667\":1}}],[\"山\",{\"1\":{\"667\":1}}],[\"寻找函数最优解\",{\"0\":{\"667\":1}}],[\"浅蓝色矩形\",{\"1\":{\"666\":1}}],[\"橙色圆形\",{\"1\":{\"666\":1}}],[\"牛顿法等经典函数逼近与优化示例\",{\"1\":{\"665\":1}}],[\"围绕计算图可视化\",{\"1\":{\"665\":1}}],[\"演示tinypytorch的自动微分功能\",{\"1\":{\"662\":1}}],[\"幂运算y\",{\"1\":{\"660\":1}}],[\"幂运算\",{\"1\":{\"660\":1}}],[\"减法运算y\",{\"1\":{\"660\":1}}],[\"减法运算\",{\"1\":{\"660\":1}}],[\"减少信息偏差\",{\"1\":{\"679\":1}}],[\"减少所需人力投入\",{\"1\":{\"677\":1}}],[\"减少了计算量\",{\"1\":{\"674\":2}}],[\"减少噪声和偏见\",{\"1\":{\"674\":1}}],[\"减少内存占用\",{\"1\":{\"658\":1}}],[\"减少gc触发频率\",{\"1\":{\"657\":1}}],[\"减少未登录词\",{\"1\":{\"497\":1}}],[\"减少未知词影响\",{\"1\":{\"492\":1}}],[\"减少存储开销并提升泛化能力\",{\"1\":{\"497\":1}}],[\"减少存储开销\",{\"1\":{\"495\":1}}],[\"减少重计算\",{\"1\":{\"481\":1}}],[\"减少幻觉与毒性\",{\"1\":{\"471\":1}}],[\"减少幻觉\",{\"1\":{\"471\":1}}],[\"减少有害内容生成等方面也有所改进\",{\"1\":{\"467\":1}}],[\"减少词汇碎片化\",{\"1\":{\"454\":1}}],[\"减少损失贡献\",{\"1\":{\"404\":1}}],[\"∂c\",{\"1\":{\"660\":1}}],[\"∂b\",{\"1\":{\"660\":1}}],[\"∂a\",{\"1\":{\"660\":1}}],[\"∂y\",{\"1\":{\"660\":3}}],[\"透明箱子\",{\"1\":{\"659\":1}}],[\"封装no\",{\"1\":{\"658\":1}}],[\"禁用模式下\",{\"1\":{\"658\":1}}],[\"禁用反向传播后\",{\"1\":{\"658\":1}}],[\"禁用反向传播\",{\"1\":{\"658\":2}}],[\"禁用反向传播的模式优化\",{\"1\":{\"658\":1}}],[\"终端变量导数保留\",{\"1\":{\"658\":1}}],[\"终输出的\",{\"1\":{\"92\":1}}],[\"修改function\",{\"1\":{\"658\":1}}],[\"修改function类的\",{\"1\":{\"660\":1}}],[\"修改function类\",{\"1\":{\"657\":1,\"663\":1}}],[\"修改variable\",{\"1\":{\"658\":1}}],[\"修改variable类的backward方法\",{\"1\":{\"657\":1}}],[\"修正意图理解\",{\"1\":{\"32\":1}}],[\"弱引用不会增加对象的引用计数\",{\"1\":{\"657\":1}}],[\"弱引用作为主动优化\",{\"1\":{\"657\":1}}],[\"弱引用能确保计算图在使用完毕后自动释放内存\",{\"1\":{\"657\":1}}],[\"弱引用的优势\",{\"1\":{\"657\":1}}],[\"弱引用\",{\"1\":{\"657\":1}}],[\"弱化\",{\"1\":{\"32\":1}}],[\"频繁的gc操作会影响计算效率\",{\"1\":{\"657\":1}}],[\"频次表记录合并规则\",{\"1\":{\"410\":1}}],[\"辈分排序\",{\"1\":{\"656\":1}}],[\"辈分\",{\"0\":{\"656\":1},\"1\":{\"656\":2,\"663\":1}}],[\"叶节点\",{\"1\":{\"655\":1}}],[\"乘法运算是最基础的操作之一\",{\"1\":{\"660\":1}}],[\"乘法运算的实现与运算符重载\",{\"1\":{\"660\":1}}],[\"乘法\",{\"1\":{\"651\":1}}],[\"乘积\",{\"1\":{\"115\":1}}],[\"追加到列表尾部\",{\"1\":{\"638\":1}}],[\"弹出列表尾部元素\",{\"1\":{\"638\":1}}],[\"循环引用会导致python对象无法被正常回收\",{\"1\":{\"657\":1}}],[\"循环引用仍会导致对象无法释放\",{\"1\":{\"657\":1}}],[\"循环引用指对象之间相互引用\",{\"1\":{\"657\":1}}],[\"循环引用与内存释放\",{\"0\":{\"657\":1}}],[\"循环来设置每一对的导数\",{\"1\":{\"652\":1}}],[\"循环实现的优势\",{\"0\":{\"639\":1}}],[\"循环实现反向传播\",{\"0\":{\"638\":1}}],[\"循环更新\",{\"1\":{\"147\":1}}],[\"递归实现的反向传播在计算图较深时可能导致栈溢出\",{\"1\":{\"637\":1}}],[\"递归实现的问题\",{\"0\":{\"637\":1}}],[\"递归遍历目录获取所有图片路径\",{\"1\":{\"275\":1,\"277\":1}}],[\"导数为400\",{\"1\":{\"667\":1}}],[\"导数公式\",{\"1\":{\"660\":1}}],[\"导数从输出端向输入端传播\",{\"1\":{\"627\":1}}],[\"导数是变化率的表示\",{\"1\":{\"620\":1}}],[\"导数的定义\",{\"0\":{\"620\":1}}],[\"导致用户接收到的信息不准确\",{\"1\":{\"679\":1}}],[\"导致内存占用持续升高\",{\"1\":{\"657\":1}}],[\"导致引用计数无法归零\",{\"1\":{\"657\":1}}],[\"导致x变量梯度计算错误\",{\"1\":{\"655\":1}}],[\"导致最终构建得到的字典过大并且还有很多噪声\",{\"1\":{\"511\":1}}],[\"导致在分布外数据上表现不佳\",{\"1\":{\"460\":1}}],[\"导致在非常规宽高比或文档理解任务上表现受限\",{\"1\":{\"211\":1}}],[\"导致其泛化能力有限\",{\"1\":{\"453\":1}}],[\"导致\",{\"1\":{\"404\":1,\"426\":1}}],[\"导致整体距离变大\",{\"1\":{\"357\":1}}],[\"导致计算量依然很大\",{\"1\":{\"255\":1}}],[\"导致字典内编码特征不一致\",{\"1\":{\"242\":1}}],[\"导致模型很难训练\",{\"1\":{\"241\":1}}],[\"导致模型学习没有轻重\",{\"1\":{\"240\":1}}],[\"导致模型泛化性不足\",{\"1\":{\"238\":1}}],[\"导致模型无法捕捉到更细粒度的几何细节\",{\"1\":{\"112\":1}}],[\"导致多模态编码器难以有效建模图文交互\",{\"1\":{\"149\":1}}],[\"导致性能下降\",{\"1\":{\"134\":1}}],[\"导致所有的特征\",{\"1\":{\"86\":1}}],[\"导致本论文复现流程暂时终止\",{\"1\":{\"37\":1}}],[\"导致各项指标大幅下降\",{\"1\":{\"24\":1}}],[\"辅助模型生成过程\",{\"1\":{\"679\":1}}],[\"辅助函数\",{\"0\":{\"614\":1}}],[\"辅助训练目标\",{\"1\":{\"441\":1}}],[\"箱子\",{\"0\":{\"605\":1},\"1\":{\"606\":1}}],[\"帮助我们处理各种任务\",{\"1\":{\"682\":1}}],[\"帮助用户理解推理步骤\",{\"1\":{\"674\":1}}],[\"帮助开发者直观理解前向与反向传播路径\",{\"1\":{\"665\":1}}],[\"帮助读者从零搭建属于自己的深度学习引擎\",{\"1\":{\"650\":1}}],[\"帮助读者正确理解技术\",{\"1\":{\"604\":1}}],[\"帮助训练稳定收敛\",{\"1\":{\"107\":1}}],[\"揭开深度学习框架的神秘面纱\",{\"0\":{\"604\":1}}],[\"揭示模型性能与规模的关系\",{\"1\":{\"485\":1}}],[\"揭示了自监督预训练中自注意力机制对语义理解的自动学习能力\",{\"1\":{\"166\":1}}],[\"仓库链接\",{\"1\":{\"603\":1,\"649\":1,\"664\":1,\"669\":1}}],[\"异排列数\",{\"1\":{\"600\":2,\"601\":2}}],[\"异常数据\",{\"1\":{\"687\":1}}],[\"异常检测\",{\"1\":{\"115\":1}}],[\"异常检测等特殊场景\",{\"1\":{\"115\":1}}],[\"异常点会影响分类和分割性能\",{\"1\":{\"112\":1}}],[\"异常值等问题\",{\"1\":{\"104\":1}}],[\"计数法则\",{\"0\":{\"599\":1}}],[\"计算与图构建同时进行\",{\"1\":{\"662\":1}}],[\"计算y\",{\"1\":{\"660\":1}}],[\"计算两个输入变量的乘积\",{\"1\":{\"660\":1}}],[\"计算过程不保留计算图连接\",{\"1\":{\"658\":1}}],[\"计算完成后中间变量立即释放\",{\"1\":{\"658\":1}}],[\"计算机视觉\",{\"0\":{\"560\":1}}],[\"计算机的内存\",{\"1\":{\"321\":1}}],[\"计算掩码语言损失\",{\"1\":{\"538\":1}}],[\"计算需要填充的长度\",{\"1\":{\"520\":1}}],[\"计算资源\",{\"1\":{\"504\":1,\"681\":1}}],[\"计算资源消耗仍然较大\",{\"1\":{\"253\":1}}],[\"计算最优平衡理论\",{\"1\":{\"485\":1}}],[\"计算最大半径\",{\"1\":{\"83\":1}}],[\"计算律发现\",{\"1\":{\"485\":1}}],[\"计算并缓存其\",{\"1\":{\"477\":1}}],[\"计算并集\",{\"1\":{\"403\":1}}],[\"计算交集\",{\"1\":{\"401\":1,\"402\":1,\"403\":1}}],[\"计算交叉熵损失\",{\"1\":{\"161\":1,\"268\":1,\"284\":1,\"541\":1}}],[\"计算得到的\",{\"1\":{\"401\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1}}],[\"计算开销线性增长\",{\"1\":{\"477\":1}}],[\"计算开销\",{\"1\":{\"395\":1}}],[\"计算开销大\",{\"1\":{\"150\":1}}],[\"计算梯度\",{\"1\":{\"296\":1}}],[\"计算预测输出与真实标签之间的\",{\"1\":{\"402\":1,\"403\":1,\"404\":1}}],[\"计算预测结果与真实标签之间的交叉熵损失\",{\"1\":{\"296\":1}}],[\"计算预测正确的样本数\",{\"1\":{\"296\":1}}],[\"计算公式如下\",{\"1\":{\"294\":1,\"305\":1}}],[\"计算网格大小\",{\"1\":{\"291\":1}}],[\"计算上下文表示\",{\"1\":{\"285\":1}}],[\"计算注意力分数\",{\"1\":{\"285\":1}}],[\"计算序列长度\",{\"1\":{\"284\":1}}],[\"计算一个\",{\"1\":{\"283\":1}}],[\"计算这批图像嵌入向量中每一个图像嵌入向量和我们所有分类文本嵌入向量的余弦相似度\",{\"1\":{\"275\":1}}],[\"计算余弦相似度\",{\"1\":{\"273\":1}}],[\"计算缩放的余弦相似度\",{\"1\":{\"272\":1}}],[\"计算量略高于ce\",{\"1\":{\"404\":1}}],[\"计算量太大了\",{\"1\":{\"240\":1}}],[\"计算量大\",{\"1\":{\"103\":1}}],[\"计算代价太高\",{\"1\":{\"240\":1}}],[\"计算标准\",{\"1\":{\"163\":1}}],[\"计算来源\",{\"1\":{\"162\":1}}],[\"计算相似度\",{\"1\":{\"161\":1}}],[\"计算相似度矩阵\",{\"1\":{\"59\":1}}],[\"计算图进阶与通用神经网络实现\",{\"0\":{\"669\":1}}],[\"计算图遍历逻辑\",{\"1\":{\"666\":1}}],[\"计算图遍历优化\",{\"1\":{\"650\":1}}],[\"计算图中的中间变量\",{\"1\":{\"657\":1}}],[\"计算图的反向传播\",{\"0\":{\"627\":1}}],[\"计算图的意义\",{\"0\":{\"618\":1}}],[\"计算图用圆框表示变量\",{\"1\":{\"611\":1}}],[\"计算图与自动微分\",{\"1\":{\"603\":1}}],[\"计算图与自动微分的起点\",{\"0\":{\"603\":1}}],[\"计算图像块的数量\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"计算图像块的总数\",{\"1\":{\"291\":1}}],[\"计算图像嵌入向量列表中每个图像对应的嵌入向量和传入的分类文本嵌入向量的余弦相似度\",{\"1\":{\"275\":1}}],[\"计算图\",{\"1\":{\"147\":1}}],[\"计算动量编码器输出\",{\"1\":{\"147\":1}}],[\"计算itm损失\",{\"1\":{\"145\":1}}],[\"计算itc损失\",{\"1\":{\"145\":1}}],[\"计算提示词token长度\",{\"1\":{\"142\":1}}],[\"计算语言建模损失\",{\"1\":{\"142\":1}}],[\"计算时引入\",{\"1\":{\"127\":1}}],[\"计算点集的分布特性\",{\"1\":{\"115\":1}}],[\"计算点云质心\",{\"1\":{\"83\":1}}],[\"计算点云中每个点和特征图中每个点特征的相似度\",{\"1\":{\"59\":1}}],[\"计算复杂度高\",{\"1\":{\"114\":1}}],[\"计算变换矩阵与其转置相乘后与单位矩阵之间的距离\",{\"1\":{\"108\":1}}],[\"计算每个可能的阈值\",{\"1\":{\"350\":1}}],[\"计算每个注意力头的维度\",{\"1\":{\"295\":1}}],[\"计算每个query\",{\"1\":{\"283\":1}}],[\"计算每个\",{\"1\":{\"163\":1}}],[\"计算每个原始点和下采样点之间的距离\",{\"1\":{\"100\":1}}],[\"计算每个查询点\",{\"1\":{\"92\":1}}],[\"计算权重\",{\"1\":{\"100\":1}}],[\"计算原始点与下采样点之间的距离矩阵\",{\"1\":{\"100\":1}}],[\"计算当前批次中所有序列的实际最大长度\",{\"1\":{\"522\":1}}],[\"计算当前中心点与所有点之间的欧氏距离平方\",{\"1\":{\"92\":1}}],[\"计算当前语言输入中有多少个有效\",{\"1\":{\"43\":1}}],[\"计算效率问题\",{\"1\":{\"90\":1}}],[\"计算成本高\",{\"1\":{\"623\":1}}],[\"计算成本相当\",{\"1\":{\"498\":1}}],[\"计算成本\",{\"1\":{\"89\":1}}],[\"计算平均\",{\"1\":{\"82\":1}}],[\"计算曲线下面积\",{\"1\":{\"82\":1}}],[\"计算指标包括\",{\"1\":{\"82\":1}}],[\"计算损失的时候\",{\"1\":{\"505\":1}}],[\"计算损失的时候使用多分类\",{\"1\":{\"241\":1}}],[\"计算损失\",{\"1\":{\"81\":1}}],[\"计算负类\",{\"1\":{\"78\":1}}],[\"计算正类\",{\"1\":{\"78\":1}}],[\"计算保持稳定\",{\"1\":{\"78\":1}}],[\"计算特征图中每个点和点云每个点特征的相似度\",{\"1\":{\"59\":1}}],[\"计算经过下采样得到的特征图相比于原始图片的缩小比例\",{\"1\":{\"59\":1}}],[\"计算密集跨模态相似性矩阵\",{\"1\":{\"54\":1}}],[\"计算\",{\"1\":{\"40\":1,\"45\":2,\"82\":4,\"147\":1,\"162\":2,\"163\":2,\"266\":1,\"268\":1,\"283\":1,\"294\":1,\"401\":2,\"402\":2,\"403\":1,\"404\":2,\"405\":1,\"407\":1,\"596\":1,\"616\":1}}],[\"听者必须从说话者表达出的\",{\"1\":{\"597\":1}}],[\"听懂人话\",{\"1\":{\"231\":1}}],[\"置信度\",{\"1\":{\"596\":1}}],[\"贝叶斯公式本身非常简单\",{\"1\":{\"596\":1}}],[\"贝叶斯公式\",{\"1\":{\"596\":1}}],[\"贝叶斯\",{\"1\":{\"596\":1}}],[\"贝叶斯法则写作\",{\"1\":{\"572\":1}}],[\"贝叶斯法则可以写为\",{\"1\":{\"571\":1}}],[\"贝叶斯法则\",{\"0\":{\"570\":1}}],[\"术语\",{\"1\":{\"596\":3}}],[\"密度\",{\"1\":{\"592\":1}}],[\"密度越小\",{\"1\":{\"592\":1}}],[\"壳层的厚度为\",{\"1\":{\"591\":1}}],[\"众数\",{\"1\":{\"591\":1}}],[\"众多基于视觉\",{\"1\":{\"270\":1}}],[\"球体的体积随半径\",{\"1\":{\"592\":1}}],[\"球形协方差矩阵\",{\"1\":{\"590\":1}}],[\"球查询\",{\"1\":{\"92\":2}}],[\"鉴于它的重要性\",{\"1\":{\"588\":1}}],[\"洛伦兹分布\",{\"1\":{\"587\":1}}],[\"柯西分布\",{\"0\":{\"587\":1},\"1\":{\"587\":1}}],[\"处标记最优点\",{\"1\":{\"667\":1}}],[\"处导数为\",{\"1\":{\"662\":1}}],[\"处对折\",{\"1\":{\"585\":1}}],[\"处理x1\",{\"1\":{\"660\":1}}],[\"处理x0\",{\"1\":{\"660\":1}}],[\"处理运算符的左右操作数差异\",{\"1\":{\"660\":1}}],[\"处理右操作数为自定义类型的情况\",{\"1\":{\"660\":1}}],[\"处理b\",{\"1\":{\"660\":2}}],[\"处理beam\",{\"1\":{\"286\":1}}],[\"处理a\",{\"1\":{\"660\":2}}],[\"处理vocab\",{\"1\":{\"511\":1}}],[\"处理两个csv文件\",{\"1\":{\"510\":1}}],[\"处理每行文本\",{\"1\":{\"510\":1}}],[\"处理csv文件\",{\"1\":{\"510\":1}}],[\"处理句法歧义\",{\"1\":{\"448\":1}}],[\"处理起来没啥问题\",{\"1\":{\"240\":1}}],[\"处理约\",{\"1\":{\"200\":1}}],[\"处理后\",{\"1\":{\"140\":1}}],[\"处理后的数据集包含比原始数据集更多的文本\",{\"1\":{\"136\":1}}],[\"处理后的\",{\"1\":{\"76\":1}}],[\"处理的是点云数据\",{\"1\":{\"115\":1}}],[\"处理\",{\"1\":{\"114\":1,\"455\":1,\"471\":1}}],[\"处理点云数据\",{\"1\":{\"107\":1}}],[\"处理过程\",{\"1\":{\"72\":1,\"73\":1,\"74\":1}}],[\"处理流程\",{\"1\":{\"54\":1}}],[\"附近\",{\"1\":{\"584\":1,\"593\":2}}],[\"附录d\",{\"1\":{\"483\":1}}],[\"附录\",{\"0\":{\"412\":1}}],[\"附录内容\",{\"0\":{\"199\":1}}],[\"集成\",{\"1\":{\"682\":1}}],[\"集群来实现\",{\"1\":{\"675\":1}}],[\"集中在\",{\"1\":{\"584\":1}}],[\"集合\",{\"1\":{\"566\":1}}],[\"集合函数近似器\",{\"1\":{\"115\":1}}],[\"集合抽象\",{\"1\":{\"98\":1}}],[\"窄\",{\"1\":{\"584\":1}}],[\"事实上\",{\"1\":{\"582\":1,\"588\":1}}],[\"事件集合体系\",{\"1\":{\"566\":1}}],[\"事件结果\",{\"1\":{\"565\":1}}],[\"事件\",{\"1\":{\"565\":2,\"566\":1,\"567\":1,\"568\":2,\"572\":1}}],[\"事件空间为\",{\"1\":{\"565\":1}}],[\"试验次数\",{\"1\":{\"579\":1}}],[\"红球\",{\"1\":{\"579\":1}}],[\"红色为剔除的文本\",{\"1\":{\"132\":1}}],[\"红色=功能区域\",{\"1\":{\"83\":1}}],[\"红色\",{\"1\":{\"83\":2}}],[\"红色区域为点云的可供性标注\",{\"1\":{\"17\":1}}],[\"失败次数\",{\"1\":{\"579\":1}}],[\"失败\",{\"1\":{\"579\":1}}],[\"服从\",{\"1\":{\"579\":1}}],[\"服从二项分布\",{\"1\":{\"578\":1}}],[\"服从参数为\",{\"1\":{\"577\":1}}],[\"容器\",{\"1\":{\"578\":1}}],[\"容易导致预训练过拟合\",{\"1\":{\"149\":1}}],[\"容易出错\",{\"1\":{\"112\":1}}],[\"盒子\",{\"1\":{\"578\":1}}],[\"泊松分布是负二项分布的一个特例\",{\"1\":{\"582\":1}}],[\"泊松分布\",{\"0\":{\"577\":1}}],[\"读作\",{\"1\":{\"575\":1}}],[\"读取训练语料\",{\"1\":{\"410\":1}}],[\"读取图片并将其转换为合适的格式后\",{\"1\":{\"275\":1}}],[\"读取出所有点云文件路径\",{\"1\":{\"58\":1}}],[\"读取出所有图片路径\",{\"1\":{\"58\":1}}],[\"读取所有点云路径\",{\"1\":{\"29\":1}}],[\"读取所有图片路径\",{\"1\":{\"29\":1}}],[\"伯努利分布与二项分布\",{\"0\":{\"575\":1}}],[\"阳性概率应该很高\",{\"1\":{\"569\":1}}],[\"患病\",{\"1\":{\"569\":1}}],[\"赋一个非负权重\",{\"1\":{\"566\":1}}],[\"赋予局部特征提取的能力\",{\"1\":{\"299\":1}}],[\"半柯西分布\",{\"1\":{\"587\":1}}],[\"半正态分布\",{\"0\":{\"585\":1}}],[\"半开区间\",{\"1\":{\"566\":1}}],[\"半径太小可能无法有效捕获足够的局部详细\",{\"1\":{\"90\":1}}],[\"允许用户无需编程创建专用的\",{\"1\":{\"674\":1}}],[\"允许开发者创建工具扩展\",{\"1\":{\"674\":1}}],[\"允许你用基本事件构造更复杂事件\",{\"1\":{\"566\":1}}],[\"允许我们通过优化适应过程中密集层变化的秩分解矩阵\",{\"1\":{\"424\":1}}],[\"空事件\",{\"1\":{\"566\":1}}],[\"空间逻辑错误\",{\"1\":{\"178\":1}}],[\"空间中的位置\",{\"1\":{\"114\":1}}],[\"空间变换网络\",{\"1\":{\"103\":1,\"105\":1}}],[\"空间是均匀和各向同性的\",{\"1\":{\"90\":1}}],[\"空间均匀性\",{\"1\":{\"90\":1}}],[\"空间混合\",{\"1\":{\"73\":1}}],[\"空间先验对齐\",{\"1\":{\"52\":1}}],[\"空间维度\",{\"1\":{\"46\":1}}],[\"空间特征\",{\"1\":{\"45\":1}}],[\"奇怪\",{\"1\":{\"566\":1}}],[\"度\",{\"1\":{\"566\":3}}],[\"度或\",{\"1\":{\"566\":1}}],[\"度之间\",{\"1\":{\"566\":1}}],[\"度量两个变量是否同步变化\",{\"1\":{\"355\":1}}],[\"秒\",{\"1\":{\"566\":2}}],[\"测量值是无理数\",{\"1\":{\"566\":1}}],[\"测量某事件持续时间\",{\"1\":{\"566\":1}}],[\"测试的重要性\",{\"0\":{\"647\":1}}],[\"测试数据使用1k\",{\"1\":{\"519\":1}}],[\"测试和开发集\",{\"1\":{\"519\":1}}],[\"测试模型的长距离依赖能力\",{\"1\":{\"494\":1}}],[\"测试显示职业性别刻板印象明显\",{\"1\":{\"484\":1}}],[\"测试重叠率5\",{\"1\":{\"455\":1}}],[\"测试常识推理能力\",{\"1\":{\"455\":1}}],[\"测试图片分类正确率\",{\"1\":{\"275\":1,\"277\":1}}],[\"测试阶段\",{\"1\":{\"219\":1}}],[\"测试时支持40区块\",{\"1\":{\"216\":1}}],[\"测试时可扩展至\",{\"1\":{\"214\":1}}],[\"测试划分\",{\"1\":{\"142\":1}}],[\"测试集上\",{\"1\":{\"194\":1}}],[\"测试集最终评估\",{\"1\":{\"82\":1}}],[\"测试集\",{\"1\":{\"68\":1,\"80\":1,\"686\":1}}],[\"测试集中的可供性类别未在训练集中出现\",{\"1\":{\"20\":1}}],[\"测试集中包含训练集中未出现的物体类别\",{\"1\":{\"20\":1}}],[\"测试\",{\"0\":{\"644\":1},\"1\":{\"29\":1,\"66\":1,\"68\":1,\"685\":1}}],[\"投两次硬币\",{\"1\":{\"565\":1}}],[\"投影层的丢弃率\",{\"1\":{\"295\":1}}],[\"投影层\",{\"1\":{\"226\":2,\"227\":2}}],[\"投影\",{\"1\":{\"45\":5,\"147\":2,\"268\":1}}],[\"投影维度\",{\"1\":{\"45\":1}}],[\"掷两次硬币的\",{\"1\":{\"565\":1}}],[\"掷两次公平硬币\",{\"1\":{\"565\":1}}],[\"掷一个三面骰子\",{\"1\":{\"565\":1}}],[\"吴恩达\",{\"0\":{\"561\":1}}],[\"托尔斯泰\",{\"1\":{\"544\":1}}],[\"莎士比亚\",{\"1\":{\"544\":1}}],[\"莎士比亚是英国文学史上最伟大的作家之一\",{\"1\":{\"540\":1}}],[\"歌德\",{\"1\":{\"544\":1}}],[\"雨果\",{\"1\":{\"544\":1}}],[\"麦克白\",{\"1\":{\"540\":1}}],[\"哈姆雷特\",{\"1\":{\"540\":2,\"544\":1}}],[\"态\",{\"1\":{\"520\":1}}],[\"破\",{\"1\":{\"520\":1}}],[\"票\",{\"1\":{\"520\":1}}],[\"股\",{\"1\":{\"520\":1}}],[\"股票中的突破形态\",{\"1\":{\"520\":2}}],[\"横着看是计算某个词与全局序列中其他词的相关度\",{\"1\":{\"517\":1}}],[\"恢复被掩码的词\",{\"1\":{\"513\":1}}],[\"恢复张量格式为\",{\"1\":{\"100\":1}}],[\"出发\",{\"1\":{\"566\":2}}],[\"出来\",{\"1\":{\"542\":1}}],[\"出被掩码位置的表示\",{\"1\":{\"513\":1}}],[\"出于方便\",{\"1\":{\"511\":1}}],[\"出现\",{\"1\":{\"407\":1}}],[\"出现的小常数\",{\"1\":{\"407\":1}}],[\"出现这种差异的原因不难理解\",{\"1\":{\"278\":1}}],[\"出现了一些基于自监督的方法\",{\"1\":{\"278\":1}}],[\"跳过这对\",{\"1\":{\"511\":1}}],[\"跳过一行\",{\"1\":{\"325\":1}}],[\"临时替换特殊标记\",{\"1\":{\"511\":1}}],[\"库\",{\"1\":{\"661\":1}}],[\"库和\",{\"1\":{\"510\":1}}],[\"库来实现\",{\"1\":{\"428\":1}}],[\"怎么办\",{\"1\":{\"508\":1}}],[\"落在文章的第\",{\"1\":{\"508\":1}}],[\"落后于chinchilla\",{\"1\":{\"482\":1}}],[\"送入\",{\"1\":{\"508\":1}}],[\"送入模型中\",{\"1\":{\"508\":1}}],[\"送入llm时\",{\"1\":{\"286\":1}}],[\"差不多\",{\"1\":{\"508\":1}}],[\"企鹅不擅长飞行\",{\"1\":{\"506\":2}}],[\"企业又有比较好的自有数据\",{\"1\":{\"415\":1}}],[\"纠错\",{\"1\":{\"505\":1}}],[\"炼成\",{\"1\":{\"504\":1}}],[\"源掩码\",{\"1\":{\"557\":1}}],[\"源注意力子层\",{\"1\":{\"556\":2}}],[\"源码链接\",{\"1\":{\"503\":1}}],[\"源自二战雷达检测\",{\"1\":{\"350\":1}}],[\"延长训练时间\",{\"1\":{\"502\":1}}],[\"延续了\",{\"1\":{\"454\":1}}],[\"领略强化学习之力\",{\"1\":{\"688\":1}}],[\"领先推理力\",{\"1\":{\"674\":1}}],[\"领先\",{\"1\":{\"499\":1}}],[\"领域专业知识能力欠缺\",{\"1\":{\"679\":1}}],[\"领域差异\",{\"1\":{\"483\":1}}],[\"领域逐渐转向预训练语言模型\",{\"1\":{\"460\":1}}],[\"领域的进展仍处于早期阶段\",{\"1\":{\"453\":1}}],[\"领域常用的文本transformer模型\",{\"1\":{\"272\":1}}],[\"领域中语言表示学习的演进\",{\"1\":{\"464\":1}}],[\"领域中的一些对比学习方法\",{\"1\":{\"271\":1}}],[\"领域中\",{\"1\":{\"253\":1}}],[\"领域自mansimov等人\",{\"1\":{\"178\":1}}],[\"领域\",{\"1\":{\"3\":1,\"510\":1}}],[\"水平\",{\"1\":{\"495\":1}}],[\"水壶的壶嘴\",{\"1\":{\"28\":1}}],[\"静态计算图\",{\"1\":{\"662\":1}}],[\"静态掩码\",{\"1\":{\"495\":1}}],[\"静态与动态掩码\",{\"1\":{\"495\":1}}],[\"静态方法\",{\"0\":{\"373\":1}}],[\"静态方法是类中的一种特殊方法\",{\"1\":{\"289\":1}}],[\"答案可以是任意文本\",{\"1\":{\"542\":1}}],[\"答案是\",{\"1\":{\"565\":1}}],[\"答案是原文中的一段\",{\"1\":{\"542\":1}}],[\"答案是否必须在原文中\",{\"1\":{\"542\":1}}],[\"答案是有的\",{\"1\":{\"424\":1}}],[\"答案应该在这段文字中的第\",{\"1\":{\"542\":1}}],[\"答案必须是原文中的一段文本\",{\"1\":{\"542\":1}}],[\"答案必须是原文中的连续片段\",{\"1\":{\"542\":1}}],[\"答案必存在于上下文中\",{\"1\":{\"494\":1}}],[\"答案就是\",{\"1\":{\"508\":1}}],[\"答案一定会出现在文章中\",{\"1\":{\"508\":1}}],[\"互动逻辑\",{\"1\":{\"682\":1}}],[\"互联提升效率\",{\"1\":{\"494\":1}}],[\"互优化机制\",{\"1\":{\"56\":1}}],[\"始终使用完整长度序列\",{\"1\":{\"494\":1}}],[\"认知偏差等\",{\"1\":{\"675\":1}}],[\"认为\",{\"1\":{\"493\":1}}],[\"认真合理\",{\"1\":{\"471\":1}}],[\"甚至引发内存不足错误\",{\"1\":{\"657\":1}}],[\"甚至指概率度量\",{\"1\":{\"566\":1}}],[\"甚至根据上下文进行\",{\"1\":{\"505\":1}}],[\"甚至可能损害模型表现\",{\"1\":{\"492\":1}}],[\"甚至超越了后续提出的模型\",{\"1\":{\"491\":1}}],[\"甚至在零样本和单样本设置下也能取得有竞争力的结果\",{\"1\":{\"459\":1}}],[\"社区治理框架\",{\"1\":{\"484\":1}}],[\"社会影响与部署考量\",{\"1\":{\"472\":1}}],[\"社会契约式\",{\"1\":{\"472\":1}}],[\"社会\",{\"1\":{\"472\":1}}],[\"社会偏见分析\",{\"1\":{\"484\":1}}],[\"社会偏见\",{\"1\":{\"469\":1}}],[\"凸显运算符重载的可读性优势\",{\"1\":{\"662\":1}}],[\"凸显公开数据训练的固有挑战\",{\"1\":{\"484\":1}}],[\"凸显领域微调的重要性\",{\"1\":{\"482\":1}}],[\"宗教类别偏见最显著\",{\"1\":{\"484\":1}}],[\"责任缺陷\",{\"1\":{\"482\":1}}],[\"梯度表示函数输出值最大的方向\",{\"1\":{\"667\":1}}],[\"梯度下降法解决问题\",{\"1\":{\"667\":1}}],[\"梯度展示了各点上函数输出值增加最快的方向\",{\"1\":{\"667\":1}}],[\"梯度分别乘以1和\",{\"1\":{\"660\":1}}],[\"梯度取反\",{\"1\":{\"660\":1}}],[\"梯度重复累加\",{\"1\":{\"655\":1}}],[\"梯度重复累加的问题\",{\"0\":{\"655\":1}}],[\"梯度累加\",{\"1\":{\"650\":1}}],[\"梯度检验\",{\"0\":{\"646\":1}}],[\"梯度裁剪1\",{\"1\":{\"481\":1}}],[\"梯度消失\",{\"1\":{\"318\":1,\"395\":2}}],[\"梯度消失或爆炸\",{\"1\":{\"41\":1}}],[\"余弦学习率调度\",{\"1\":{\"481\":1}}],[\"灵感来自gpt\",{\"1\":{\"481\":1}}],[\"灵活适应问答系统\",{\"1\":{\"679\":1}}],[\"灵活控制\",{\"0\":{\"665\":1}}],[\"灵活的语言系统发展\",{\"1\":{\"460\":1}}],[\"灵活的组合方式\",{\"1\":{\"189\":1}}],[\"灵活性\",{\"1\":{\"221\":1}}],[\"灵活分辨率\",{\"1\":{\"208\":1}}],[\"呼应ai伦理需求\",{\"1\":{\"480\":1}}],[\"呼应了t5等模型的\",{\"1\":{\"464\":1}}],[\"闭区间\",{\"1\":{\"566\":1}}],[\"闭卷问答\",{\"1\":{\"480\":1,\"482\":1}}],[\"闭包得到所有\",{\"1\":{\"566\":1}}],[\"闭包\",{\"1\":{\"367\":2,\"368\":1}}],[\"闭包是一个函数\",{\"1\":{\"366\":1}}],[\"闭包与高阶导数\",{\"0\":{\"364\":1}}],[\"历史缓存\",{\"1\":{\"477\":1}}],[\"历史最高频合并对的频率\",{\"1\":{\"410\":1}}],[\"堆叠而成\",{\"1\":{\"477\":1}}],[\"删除了大量无关代码\",{\"1\":{\"477\":1}}],[\"删除已创建的环境\",{\"0\":{\"335\":1}}],[\"部署服务\",{\"1\":{\"685\":1}}],[\"部署等任务\",{\"1\":{\"685\":1}}],[\"部署建议\",{\"1\":{\"472\":1}}],[\"部分结果如下\",{\"1\":{\"667\":1}}],[\"部分内容需分多段呈现以保留关键信息\",{\"1\":{\"469\":1}}],[\"部分任务甚至达到或超越微调模型的水平\",{\"1\":{\"465\":1}}],[\"部分任务甚至超越商业模型\",{\"1\":{\"207\":1}}],[\"部分任务性能接近或超越监督基线模型\",{\"1\":{\"453\":1}}],[\"部分能力甚至超越gpt\",{\"1\":{\"222\":1}}],[\"部分模型甚至可以在单个gpu上运行\",{\"1\":{\"479\":1}}],[\"部分模型\",{\"1\":{\"185\":1}}],[\"部分的翻译\",{\"1\":{\"134\":1}}],[\"部分翻译内容的总结\",{\"1\":{\"129\":1}}],[\"部分\",{\"1\":{\"43\":2,\"46\":1,\"142\":1,\"542\":1,\"688\":2}}],[\"部分研究开始映射语义功能到3d结构\",{\"1\":{\"7\":1}}],[\"部分工作通过语言理解在2d数据中定位功能区域\",{\"1\":{\"7\":1}}],[\"拒绝模型\",{\"1\":{\"472\":1}}],[\"驯化\",{\"1\":{\"472\":1}}],[\"虚假宣传等敏感场景下\",{\"1\":{\"472\":1}}],[\"虚线之下是self\",{\"1\":{\"435\":1}}],[\"虚线之上是标准的cot的过程\",{\"1\":{\"435\":1}}],[\"虚线为\",{\"1\":{\"397\":1}}],[\"规范性\",{\"1\":{\"567\":1}}],[\"规范\",{\"1\":{\"472\":1}}],[\"规模显著扩大\",{\"1\":{\"674\":1}}],[\"规模化研究\",{\"1\":{\"485\":1}}],[\"规模悖论\",{\"1\":{\"484\":1}}],[\"规模定律\",{\"1\":{\"455\":1}}],[\"规模与处理\",{\"1\":{\"454\":1}}],[\"规模一致性\",{\"1\":{\"90\":1}}],[\"规模\",{\"1\":{\"6\":1}}],[\"尚待探索的问题\",{\"1\":{\"472\":1}}],[\"歧义加权\",{\"1\":{\"472\":1}}],[\"南瓜吸引炮弹\",{\"1\":{\"472\":1}}],[\"吃袜子\",{\"1\":{\"472\":1}}],[\"谁写了\",{\"1\":{\"540\":1,\"544\":1}}],[\"谁\",{\"1\":{\"472\":1}}],[\"谁离我越近\",{\"1\":{\"100\":1}}],[\"炸碎\",{\"1\":{\"471\":1}}],[\"炮弹打南瓜\",{\"1\":{\"471\":1}}],[\"胡编乱造\",{\"1\":{\"471\":1}}],[\"冥想后吃袜子有何用\",{\"1\":{\"471\":1}}],[\"荒谬\",{\"1\":{\"471\":1}}],[\"尊重\",{\"1\":{\"471\":1}}],[\"倾向于不作伪答\",{\"1\":{\"471\":1}}],[\"倾斜曲面\",{\"1\":{\"6\":1}}],[\"客户助手场景\",{\"1\":{\"471\":1}}],[\"±\",{\"1\":{\"471\":2}}],[\"毒性模式\",{\"1\":{\"484\":1}}],[\"毒性生成评估\",{\"1\":{\"484\":1}}],[\"毒性\",{\"1\":{\"470\":1}}],[\"公司开发的闭源语言大模型\",{\"1\":{\"674\":1}}],[\"公司在\",{\"1\":{\"674\":1}}],[\"公理\",{\"1\":{\"567\":1}}],[\"公开数据+严格过滤\",{\"1\":{\"481\":1}}],[\"公共\",{\"1\":{\"470\":1}}],[\"公式为\",{\"1\":{\"621\":1}}],[\"公式人话版本\",{\"1\":{\"596\":1}}],[\"公式表示如下\",{\"1\":{\"425\":1}}],[\"公式转化为tanimoto系数\",{\"1\":{\"405\":1}}],[\"公式\",{\"0\":{\"394\":1},\"1\":{\"402\":2}}],[\"公式是\",{\"1\":{\"357\":1,\"358\":1}}],[\"公式中\",{\"1\":{\"157\":1}}],[\"公式来自论文\",{\"1\":{\"78\":1}}],[\"公式如下\",{\"1\":{\"13\":1,\"82\":1,\"401\":1,\"443\":1}}],[\"幻觉\",{\"1\":{\"471\":1,\"679\":1}}],[\"幻觉率等多个维度的性能\",{\"1\":{\"470\":1}}],[\"幻觉控制\",{\"1\":{\"220\":1}}],[\"σ\",{\"1\":{\"470\":1,\"566\":12}}],[\"−\",{\"1\":{\"470\":3}}],[\"θ\",{\"1\":{\"470\":3}}],[\"故事类文本\",{\"1\":{\"494\":1}}],[\"故事生成\",{\"1\":{\"469\":1}}],[\"故特别强调标注者的社会敏感性\",{\"1\":{\"470\":1}}],[\"雇佣了约\",{\"1\":{\"470\":1}}],[\"突\",{\"1\":{\"520\":1}}],[\"突出了真实用户需求的多样性\",{\"1\":{\"470\":1}}],[\"突破transformer缺少归纳偏置的限制\",{\"1\":{\"287\":1}}],[\"突破了传统视觉模型\",{\"1\":{\"188\":1}}],[\"明确展示了这三步流程之间的数据流和优化路径\",{\"1\":{\"470\":1}}],[\"算法中经常用到的预训练模型\",{\"1\":{\"510\":1}}],[\"算法进行强化学习优化\",{\"1\":{\"470\":1}}],[\"算法预训练过程完整代码如下\",{\"1\":{\"410\":1}}],[\"算法预训练工作流程\",{\"1\":{\"410\":1}}],[\"节点连接\",{\"1\":{\"666\":1}}],[\"节点属性设置\",{\"1\":{\"666\":1}}],[\"节点间用换行分隔\",{\"1\":{\"666\":1}}],[\"节点和箭头展示计算流程\",{\"1\":{\"611\":1}}],[\"节\",{\"1\":{\"469\":2}}],[\"节省gpu内存\",{\"1\":{\"481\":1}}],[\"节省内存\",{\"1\":{\"387\":2,\"495\":1}}],[\"节省显存\",{\"1\":{\"147\":1}}],[\"风格\",{\"1\":{\"651\":1}}],[\"风格延续任务中的偏好学习\",{\"1\":{\"469\":1}}],[\"风格的对比学习\",{\"1\":{\"190\":1}}],[\"风格的掩码建模在视觉领域的应用尚未充分研究\",{\"1\":{\"166\":1}}],[\"诚实\",{\"1\":{\"468\":1}}],[\"奖励模型\",{\"1\":{\"470\":1}}],[\"奖励模型训练\",{\"1\":{\"468\":1,\"470\":1}}],[\"奖励模型的目标是模拟人类的偏好判断\",{\"1\":{\"224\":1}}],[\"补集规则\",{\"1\":{\"567\":1}}],[\"补集操作\",{\"1\":{\"566\":1}}],[\"补全\",{\"1\":{\"470\":1}}],[\"补足了先前few\",{\"1\":{\"464\":1}}],[\"补充多样性\",{\"1\":{\"481\":1}}],[\"补充方式\",{\"1\":{\"402\":1}}],[\"补充\",{\"0\":{\"230\":1},\"1\":{\"228\":1}}],[\"补充说明\",{\"1\":{\"162\":1}}],[\"补充实验中还使用了\",{\"1\":{\"131\":1}}],[\"补充的交互方式\",{\"1\":{\"28\":1}}],[\"外循环\",{\"1\":{\"464\":1}}],[\"外部代理工具整合到一起\",{\"1\":{\"683\":1}}],[\"外部知识\",{\"1\":{\"683\":1}}],[\"外部作用域变量\",{\"1\":{\"366\":1}}],[\"外部传入的\",{\"1\":{\"163\":1}}],[\"金融决策等\",{\"1\":{\"463\":1}}],[\"工作流程\",{\"0\":{\"680\":1}}],[\"工作中的一些问题\",{\"1\":{\"415\":1}}],[\"工具使用与智能体\",{\"1\":{\"684\":1}}],[\"工具和智能体\",{\"1\":{\"684\":1}}],[\"工具函数\",{\"1\":{\"661\":1}}],[\"工具包重新实现了\",{\"1\":{\"494\":1}}],[\"工程实践\",{\"1\":{\"658\":1}}],[\"工程等高精度领域的适用性\",{\"1\":{\"463\":1}}],[\"科研界给这些庞大的语言模型起了个名字\",{\"1\":{\"673\":1}}],[\"科研\",{\"1\":{\"463\":1}}],[\"科学问答\",{\"1\":{\"227\":1}}],[\"科学问答等实际应用任务\",{\"1\":{\"225\":1}}],[\"科学理解\",{\"1\":{\"220\":1}}],[\"程序生成等\",{\"1\":{\"463\":1}}],[\"程序向\",{\"1\":{\"83\":1}}],[\"瓶颈\",{\"1\":{\"463\":1}}],[\"瓶颈式架构\",{\"1\":{\"71\":1}}],[\"逻辑推理和结构化知识方面展现出卓越性能\",{\"1\":{\"674\":1}}],[\"逻辑缺陷\",{\"1\":{\"483\":1}}],[\"逻辑比较\",{\"1\":{\"463\":1}}],[\"逻辑转置中\",{\"1\":{\"326\":1}}],[\"逻辑转置\",{\"1\":{\"326\":1}}],[\"偏好反馈机制\",{\"1\":{\"472\":1}}],[\"偏好条件化能力\",{\"1\":{\"472\":1}}],[\"偏见正相关\",{\"1\":{\"482\":1}}],[\"偏见与毒性分析\",{\"1\":{\"482\":1}}],[\"偏见或捏造内容\",{\"1\":{\"472\":1}}],[\"偏见水平相当\",{\"1\":{\"471\":1}}],[\"偏见\",{\"1\":{\"470\":1}}],[\"偏见评估数据集\",{\"1\":{\"469\":1}}],[\"偏见问题\",{\"1\":{\"460\":1}}],[\"偏差小但罕见的要严惩\",{\"1\":{\"359\":1}}],[\"偏差大但常见的就不判定为\",{\"1\":{\"359\":1}}],[\"鼓励后续研究继续探索更大规模语言模型的行为边界\",{\"1\":{\"457\":1}}],[\"鼓励变换矩阵接近正交矩阵\",{\"1\":{\"108\":1}}],[\"讨论\",{\"0\":{\"456\":1,\"472\":1}}],[\"证实模型并非简单记忆\",{\"1\":{\"455\":1}}],[\"证明运算符重载与自动微分机制的一致性\",{\"1\":{\"660\":1}}],[\"证明指令适应的高效性\",{\"1\":{\"482\":1}}],[\"证明无监督预训练在多任务迁移中的巨大潜力\",{\"1\":{\"455\":1}}],[\"证明自然语言指令可激活任务特定行为\",{\"1\":{\"455\":1}}],[\"证明gpt\",{\"1\":{\"453\":1}}],[\"证明模型具有有效处理上下文长距离的信息的能力\",{\"1\":{\"448\":1}}],[\"证明其确实为下游任务获取到了有用的语言知识\",{\"1\":{\"440\":1}}],[\"证明\",{\"1\":{\"395\":1,\"492\":1}}],[\"证明了bert原始设计的潜力尚未被充分挖掘\",{\"1\":{\"502\":1}}],[\"证明了可以将bert的方法和vison\",{\"1\":{\"253\":1}}],[\"证明了其自动捕获高层视觉知识的能力\",{\"1\":{\"166\":1}}],[\"独热编码\",{\"1\":{\"576\":1}}],[\"独角兽新闻\",{\"1\":{\"455\":1}}],[\"独立编码图像\",{\"1\":{\"188\":1}}],[\"英语词库数据\",{\"1\":{\"510\":1}}],[\"英罗等语言对的few\",{\"1\":{\"462\":1}}],[\"英德\",{\"1\":{\"462\":1}}],[\"英→法\",{\"1\":{\"455\":1}}],[\"英文全称\",{\"1\":{\"82\":1}}],[\"阅读理解任务\",{\"1\":{\"544\":1}}],[\"阅读理解与逻辑推理任务表现不一\",{\"1\":{\"462\":1}}],[\"阅读理解\",{\"1\":{\"455\":1}}],[\"命名实体识别\",{\"1\":{\"543\":1}}],[\"命名实体准确率89\",{\"1\":{\"455\":1}}],[\"命名实体\",{\"1\":{\"455\":1}}],[\"命令行工具\",{\"1\":{\"685\":1}}],[\"命令后加上\",{\"1\":{\"83\":1}}],[\"命令中\",{\"1\":{\"83\":1}}],[\"奠定了基础\",{\"1\":{\"454\":1}}],[\"隐私问题\",{\"1\":{\"675\":1}}],[\"隐式元学习\",{\"1\":{\"464\":1}}],[\"隐式学习任务逻辑\",{\"1\":{\"454\":1,\"456\":1}}],[\"隐藏数据封装细节\",{\"1\":{\"659\":1}}],[\"隐藏的\",{\"1\":{\"596\":1}}],[\"隐藏层输出\",{\"1\":{\"538\":1}}],[\"隐藏层维度设为\",{\"1\":{\"481\":1}}],[\"隐藏层维度和参数量不同\",{\"1\":{\"455\":1}}],[\"隐藏层神经元的数量足够\",{\"1\":{\"395\":1}}],[\"隐藏大小\",{\"1\":{\"174\":1}}],[\"触发摘要生成\",{\"1\":{\"454\":1}}],[\"≥3\",{\"1\":{\"454\":1}}],[\"间接学习任务\",{\"1\":{\"453\":1}}],[\"间存在功能冲突\",{\"1\":{\"134\":1}}],[\"狭窄的专家\",{\"1\":{\"453\":1}}],[\"翻译语言等\",{\"1\":{\"678\":1}}],[\"翻译等任务上的表现仍远未达到实用水平\",{\"1\":{\"456\":1}}],[\"翻译等任务表现仍远逊于专业系统\",{\"1\":{\"455\":1}}],[\"翻译任务\",{\"1\":{\"454\":1}}],[\"翻译示例\",{\"1\":{\"454\":1}}],[\"翻译的文本示例\",{\"1\":{\"453\":1}}],[\"翻译\",{\"1\":{\"452\":1,\"455\":1}}],[\"翻译成中文\",{\"1\":{\"224\":1}}],[\"蕴含确定和文本分类\",{\"1\":{\"450\":1}}],[\"威诺格拉德模式\",{\"1\":{\"449\":1}}],[\"情感表达\",{\"1\":{\"674\":1}}],[\"情感分析\",{\"1\":{\"449\":1}}],[\"情况下\",{\"1\":{\"475\":1,\"476\":1,\"596\":1}}],[\"情况完全不一样\",{\"1\":{\"238\":1}}],[\"潜在风险\",{\"1\":{\"472\":1}}],[\"潜在生成式模型\",{\"1\":{\"449\":1}}],[\"潜在交互意图\",{\"1\":{\"28\":1}}],[\"纽约大学发布的有关语法的数据集\",{\"1\":{\"448\":1}}],[\"评判两个文本语义信息的相似度\",{\"1\":{\"448\":1}}],[\"评估基准\",{\"1\":{\"494\":1}}],[\"评估方式\",{\"1\":{\"470\":1}}],[\"评估方法与设定\",{\"1\":{\"461\":1}}],[\"评估语言模型的风险与危害\",{\"1\":{\"469\":1}}],[\"评估工具\",{\"1\":{\"219\":1}}],[\"评估阶段则是在验证集或测试集上评估模型的性能\",{\"1\":{\"82\":1}}],[\"评估\",{\"0\":{\"82\":1},\"1\":{\"79\":1}}],[\"评估集\",{\"1\":{\"68\":1}}],[\"评估时需要标准的单一样本对比\",{\"1\":{\"29\":1}}],[\"评估指标\",{\"1\":{\"22\":1}}],[\"美国知识问答网站\",{\"1\":{\"448\":1}}],[\"挑战\",{\"1\":{\"471\":1}}],[\"挑战了传统对\",{\"1\":{\"464\":1}}],[\"挑战在于识别语句是否是概念改写\",{\"1\":{\"448\":1}}],[\"挑战模型对可供性的泛化能力\",{\"1\":{\"19\":1}}],[\"矛盾或中立\",{\"1\":{\"448\":1}}],[\"涉及\",{\"1\":{\"470\":1}}],[\"涉及读取一对句子\",{\"1\":{\"448\":1}}],[\"涉及两个重要概念\",{\"1\":{\"321\":1}}],[\"残差链接\",{\"1\":{\"532\":1}}],[\"残差层权重按\",{\"1\":{\"454\":1}}],[\"残差\",{\"1\":{\"447\":1}}],[\"残差连接与层归一化\",{\"1\":{\"548\":1}}],[\"残差连接\",{\"1\":{\"294\":2,\"552\":1}}],[\"困惑度仅从8\",{\"1\":{\"455\":1}}],[\"困惑度从99\",{\"1\":{\"455\":1}}],[\"困惑度\",{\"1\":{\"447\":1}}],[\"困难负样本挖掘策略\",{\"1\":{\"127\":1}}],[\"长文本处理能力较弱\",{\"1\":{\"679\":1}}],[\"长文本对话模型\",{\"1\":{\"674\":1}}],[\"长文本理解和推理能力\",{\"1\":{\"674\":1}}],[\"长文本阅读理解任务\",{\"1\":{\"494\":1}}],[\"长文本能让生成模型学习到长依赖信息的条件概率\",{\"1\":{\"447\":1}}],[\"长度为n\",{\"1\":{\"474\":1}}],[\"长边按比例缩放\",{\"1\":{\"290\":1}}],[\"符来分隔两者\",{\"1\":{\"445\":1}}],[\"符合框架设计需求\",{\"1\":{\"657\":1}}],[\"符合我们直接观念所想的大模型微调流程为\",{\"1\":{\"423\":1}}],[\"符合认知的大模型微调流程\",{\"0\":{\"423\":1}}],[\"$\",{\"1\":{\"445\":1}}],[\"观察下图\",{\"1\":{\"655\":1}}],[\"观察倒水场景时\",{\"1\":{\"6\":1}}],[\"观测发现用辅助目标能提升性能\",{\"1\":{\"444\":1}}],[\"段落级别或者句子级别的\",{\"1\":{\"441\":1}}],[\"句子级别表示\",{\"1\":{\"540\":1}}],[\"句子嵌入\",{\"1\":{\"513\":2}}],[\"句子分隔列表\",{\"1\":{\"512\":1}}],[\"句子中20\",{\"1\":{\"512\":1}}],[\"句子\",{\"1\":{\"510\":1}}],[\"句法解析\",{\"1\":{\"440\":1}}],[\"句号等标点符号结合语义进行断句\",{\"1\":{\"410\":1}}],[\"迁移它来稍微适应一系列广泛的任务\",{\"1\":{\"440\":1}}],[\"问\",{\"1\":{\"569\":1}}],[\"问答系统中的候选答案选择\",{\"1\":{\"544\":1}}],[\"问答和文本生成等\",{\"1\":{\"459\":1}}],[\"问答和常识推理\",{\"1\":{\"445\":1,\"448\":1}}],[\"问答示例\",{\"1\":{\"456\":1}}],[\"问答任务\",{\"0\":{\"540\":1},\"1\":{\"454\":1,\"508\":2}}],[\"问答或文本蕴含\",{\"1\":{\"445\":1}}],[\"问答提升5\",{\"1\":{\"440\":1}}],[\"问答\",{\"1\":{\"439\":1,\"440\":1,\"449\":1,\"454\":1,\"455\":2,\"460\":1,\"470\":1}}],[\"问题分解能力\",{\"1\":{\"674\":1}}],[\"问题格式甚至换行方式都可能造成性能大幅波动\",{\"1\":{\"463\":1}}],[\"问题和答案\",{\"1\":{\"445\":1}}],[\"问题的损失函数改进方案\",{\"1\":{\"404\":1}}],[\"问题来了\",{\"1\":{\"278\":1}}],[\"问题\",{\"1\":{\"231\":1,\"397\":1,\"409\":1,\"455\":1,\"471\":1,\"495\":1,\"497\":1,\"540\":3,\"542\":1,\"544\":2}}],[\"问题所在\",{\"1\":{\"112\":1}}],[\"问题嵌入\",{\"1\":{\"76\":1}}],[\"问题编码后的文本特征\",{\"1\":{\"72\":1}}],[\"问题条件化查询\",{\"1\":{\"70\":1}}],[\"问题文本不为空\",{\"1\":{\"68\":1}}],[\"问题文本\",{\"1\":{\"68\":1}}],[\"问题数\",{\"1\":{\"65\":1}}],[\"问题总数\",{\"1\":{\"65\":1,\"67\":1}}],[\"问题配对是固定的\",{\"1\":{\"66\":1}}],[\"问题配对\",{\"1\":{\"65\":1,\"67\":1,\"69\":1}}],[\"问题背景\",{\"1\":{\"29\":1,\"103\":1}}],[\"拆解成一系列的简单问题\",{\"1\":{\"436\":1}}],[\"教育工具等\",{\"1\":{\"472\":1}}],[\"教llm把复杂问题\",{\"1\":{\"436\":1}}],[\"教师\",{\"1\":{\"123\":1}}],[\"绿色标记出的部分是llm输出的推理过程\",{\"1\":{\"434\":1}}],[\"绿色为\",{\"1\":{\"132\":1}}],[\"思忖未知之界\",{\"1\":{\"688\":1}}],[\"思维链\",{\"1\":{\"676\":1}}],[\"思维链展示\",{\"1\":{\"674\":2}}],[\"思维链技术\",{\"0\":{\"434\":1}}],[\"思考的时间更短\",{\"1\":{\"674\":1}}],[\"思考\",{\"1\":{\"433\":1}}],[\"思考快与慢\",{\"1\":{\"433\":1}}],[\"背后的大致逻辑\",{\"1\":{\"433\":1}}],[\"背景与动机\",{\"1\":{\"460\":1}}],[\"背景类别极端不平衡的问题\",{\"1\":{\"404\":1}}],[\"背景知识扫盲\",{\"0\":{\"113\":1}}],[\"背景\",{\"0\":{\"86\":1,\"224\":1,\"280\":1,\"493\":1,\"547\":1},\"1\":{\"83\":1,\"404\":1}}],[\"背景点\",{\"1\":{\"78\":1}}],[\"背景特征\",{\"1\":{\"59\":1}}],[\"背景区域特征图\",{\"1\":{\"59\":1}}],[\"背景区域特征图经过roi\",{\"1\":{\"59\":1}}],[\"背景区域特征过程的实现细节如下\",{\"1\":{\"59\":1}}],[\"背景区域特征\",{\"1\":{\"59\":1}}],[\"他们希望利用\",{\"1\":{\"682\":1}}],[\"他们之间的关系是独立的\",{\"1\":{\"31\":1}}],[\"他写了包括\",{\"1\":{\"540\":1}}],[\"他利用自己已掌握的知识\",{\"1\":{\"433\":1}}],[\"举个例子\",{\"1\":{\"432\":1,\"542\":1,\"597\":1}}],[\"举例来说\",{\"1\":{\"508\":1}}],[\"举例\",{\"1\":{\"145\":1}}],[\"举例说明\",{\"1\":{\"112\":1}}],[\"秩的选择\",{\"0\":{\"427\":1},\"1\":{\"427\":1}}],[\"旁添加一条旁路\",{\"1\":{\"425\":1}}],[\"级别的参数量\",{\"1\":{\"423\":1}}],[\"感兴趣的朋友\",{\"1\":{\"421\":1}}],[\"限制输入来自同一文档\",{\"1\":{\"495\":1}}],[\"限制与盲点\",{\"1\":{\"472\":1}}],[\"限制了模型在多样化场景中的应用能力\",{\"1\":{\"453\":1}}],[\"限制了研究社区的应用和优化\",{\"1\":{\"210\":1}}],[\"限制了细节理解能力\",{\"1\":{\"208\":1}}],[\"限制了其在多模态任务中的表现\",{\"1\":{\"183\":1}}],[\"限制了llm的能力利用\",{\"1\":{\"181\":1}}],[\"限于篇幅原因\",{\"1\":{\"421\":1}}],[\"∆w\",{\"1\":{\"420\":3}}],[\"∆w为m\",{\"1\":{\"420\":1}}],[\"往输入序列x前面加特定的token\",{\"1\":{\"419\":1}}],[\"往往远多于正样本\",{\"1\":{\"404\":1}}],[\"跟prompt\",{\"1\":{\"419\":1}}],[\"尽可能地提升大模型在特定领域的能力\",{\"1\":{\"416\":1}}],[\"尽管大型语言模型的调用相对简单\",{\"1\":{\"682\":1}}],[\"尽管这些大型语言模型与小型语言模型\",{\"1\":{\"673\":1}}],[\"尽管这些方法有所改进\",{\"1\":{\"211\":1}}],[\"尽管用户不再访问这些对象\",{\"1\":{\"657\":1}}],[\"尽管每个样本的距离是随机的\",{\"1\":{\"593\":1}}],[\"尽管有10种变体\",{\"1\":{\"495\":1}}],[\"尽管有时仍用英文回应\",{\"1\":{\"471\":1}}],[\"尽管\",{\"1\":{\"469\":1,\"493\":1,\"675\":1}}],[\"尽管仍有提升空间\",{\"1\":{\"467\":1}}],[\"尽管仍存在局限\",{\"1\":{\"465\":1}}],[\"尽管其非英语性能仍有限\",{\"1\":{\"464\":1}}],[\"尽管gpt\",{\"1\":{\"463\":2}}],[\"尽管训练数据中非英语文本仅占7\",{\"1\":{\"462\":1}}],[\"尽管当前零样本性能仍有限\",{\"1\":{\"457\":1}}],[\"尽管webtext几乎无平行语料\",{\"1\":{\"455\":1}}],[\"尽管数据集小\",{\"1\":{\"455\":1}}],[\"尽管零样本性能尚不完美\",{\"1\":{\"453\":1}}],[\"尽管该模型毫无用处\",{\"1\":{\"343\":1}}],[\"尽管谷歌基于jft\",{\"1\":{\"278\":1}}],[\"尽管clip是一个多模态模型\",{\"1\":{\"272\":1}}],[\"尽管如此\",{\"1\":{\"185\":1}}],[\"尽管技术进步\",{\"1\":{\"178\":1}}],[\"尽管使用简单的过滤规则\",{\"1\":{\"122\":1}}],[\"漏字填空\",{\"1\":{\"504\":1}}],[\"漏检\",{\"1\":{\"405\":1}}],[\"漏报\",{\"1\":{\"405\":1}}],[\"漏掉的真正例\",{\"1\":{\"353\":1}}],[\"技巧性近似\",{\"1\":{\"404\":1}}],[\"技术领域的迅猛发展浪潮中\",{\"1\":{\"684\":1}}],[\"技术范式\",{\"1\":{\"677\":1}}],[\"技术细节必须伴随规范治理与透明流程\",{\"1\":{\"472\":1}}],[\"技术\",{\"1\":{\"240\":1}}],[\"技术配置\",{\"1\":{\"219\":1}}],[\"求\",{\"1\":{\"404\":1}}],[\"求和\",{\"1\":{\"76\":1,\"115\":1,\"390\":2}}],[\"^γ\",{\"1\":{\"404\":1}}],[\"罕见疾病诊断等\",{\"1\":{\"404\":1}}],[\"任意有限\",{\"1\":{\"566\":1}}],[\"任何类别不平衡的分类任务\",{\"1\":{\"404\":1}}],[\"任务损失\",{\"1\":{\"514\":2}}],[\"任务会利用\",{\"1\":{\"513\":2}}],[\"任务通常需要不同的模型\",{\"1\":{\"504\":1}}],[\"任务通用性\",{\"1\":{\"455\":1}}],[\"任务上表现更优\",{\"1\":{\"492\":1}}],[\"任务复杂性上限未显现\",{\"1\":{\"471\":1}}],[\"任务类型\",{\"1\":{\"470\":1}}],[\"任务多样性与实用性\",{\"1\":{\"469\":1}}],[\"任务模式\",{\"1\":{\"462\":1}}],[\"任务不可知\",{\"1\":{\"461\":1}}],[\"任务提示的关键作用\",{\"1\":{\"454\":1}}],[\"任务执行的零样本机制\",{\"1\":{\"454\":1}}],[\"任务可以通过自然语言描述\",{\"1\":{\"454\":1}}],[\"任务涉及预测两个句子在语义上是否相等\",{\"1\":{\"448\":1}}],[\"任务需求\",{\"1\":{\"408\":1}}],[\"任务结合起来\",{\"1\":{\"239\":1}}],[\"任务进行预训练\",{\"1\":{\"167\":1}}],[\"任务采样负样本索引\",{\"1\":{\"147\":1}}],[\"任务的本质\",{\"1\":{\"542\":1}}],[\"任务的通用架构该有多好\",{\"1\":{\"504\":1}}],[\"任务的核心步骤\",{\"1\":{\"495\":1}}],[\"任务的\",{\"1\":{\"470\":1}}],[\"任务的目标是最小化交叉熵损失\",{\"1\":{\"155\":1}}],[\"任务的二分类头\",{\"1\":{\"147\":1}}],[\"任务的评估体系\",{\"1\":{\"82\":1}}],[\"任务\",{\"1\":{\"82\":1,\"147\":1,\"166\":1,\"172\":1,\"462\":1,\"504\":1,\"513\":3}}],[\"任务中表现出色\",{\"1\":{\"675\":1}}],[\"任务中用于生成功能区域掩码的核心模块\",{\"1\":{\"76\":1}}],[\"任务中\",{\"1\":{\"71\":1,\"160\":1,\"544\":1}}],[\"任务设定\",{\"1\":{\"49\":1}}],[\"病灶区域像素远少于正常组织\",{\"1\":{\"404\":1}}],[\"医学图像分割\",{\"1\":{\"404\":1}}],[\"倍\",{\"1\":{\"404\":1,\"674\":2}}],[\"退出with块后\",{\"1\":{\"658\":1}}],[\"退出当前环境\",{\"0\":{\"333\":1}}],[\"退化为ce\",{\"1\":{\"404\":1}}],[\"综述\",{\"1\":{\"688\":1}}],[\"综上\",{\"1\":{\"404\":1}}],[\"综合指标上\",{\"1\":{\"195\":1}}],[\"综合最近几个熟人的意见\",{\"1\":{\"100\":1}}],[\"已实现自动微分系统与基础函数操作\",{\"1\":{\"670\":1}}],[\"已知rosenbrock函数的最小值在处\",{\"1\":{\"667\":1}}],[\"已知的条件下\",{\"1\":{\"568\":1}}],[\"已添加\",{\"1\":{\"513\":1}}],[\"已超越多项无监督nmt方法的表现\",{\"1\":{\"462\":1}}],[\"已初步验证了上下文学习的可行性\",{\"1\":{\"460\":1}}],[\"已有的技术涉及对模型架构进行特定任务的修改\",{\"1\":{\"440\":1}}],[\"已弃用\",{\"1\":{\"402\":1,\"403\":1,\"404\":1}}],[\"已经在许多领域产生了深远的影响\",{\"1\":{\"678\":1}}],[\"已经在多个领域展示了潜力\",{\"1\":{\"675\":1}}],[\"已经扩展到支持多模态数据\",{\"1\":{\"675\":1}}],[\"已经开源了\",{\"1\":{\"674\":1}}],[\"已经能够满足个人用户或小型企业的大部分需求\",{\"1\":{\"674\":1}}],[\"已经发生的前提下\",{\"1\":{\"568\":1}}],[\"已经发生的条件下的条件概率为\",{\"1\":{\"568\":1}}],[\"已经缓存的词序列长度\",{\"1\":{\"477\":1}}],[\"已经证明\",{\"1\":{\"454\":1}}],[\"已经证明无监督学习在nlp任务上是行得通的\",{\"1\":{\"238\":1}}],[\"已经经过\",{\"1\":{\"402\":1}}],[\"已经返回了\",{\"1\":{\"366\":1}}],[\"已经过预训练以提取语言信息视觉表示\",{\"1\":{\"286\":1}}],[\"已经接近其上限\",{\"1\":{\"228\":1}}],[\"抗类别不平衡能力强\",{\"1\":{\"402\":1}}],[\"ϵ\",{\"1\":{\"401\":1}}],[\"向容器对象\",{\"1\":{\"657\":1}}],[\"向函数传递参数\",{\"1\":{\"657\":1}}],[\"向下取整\",{\"1\":{\"397\":1}}],[\"向量化构建出个性化数据库\",{\"1\":{\"687\":1}}],[\"向量数据库\",{\"1\":{\"683\":1}}],[\"向量映射成两个分数\",{\"1\":{\"540\":1}}],[\"向量中\",{\"1\":{\"508\":1}}],[\"向量的长度\",{\"1\":{\"297\":1}}],[\"向量合并成一个完整的\",{\"1\":{\"249\":1}}],[\"向量\",{\"1\":{\"154\":1,\"156\":1,\"191\":1,\"249\":1,\"576\":1}}],[\"边界进行离散化\",{\"1\":{\"397\":1}}],[\"边界坐标的量化\",{\"1\":{\"397\":1}}],[\"边长为该整数\",{\"1\":{\"291\":2}}],[\"划分成固定数量的网格区域\",{\"1\":{\"396\":1}}],[\"划分后可以得到共个patch\",{\"1\":{\"291\":1}}],[\"电路理论类比\",{\"1\":{\"395\":1}}],[\"折叠\",{\"1\":{\"395\":1,\"587\":1}}],[\"锯齿波\",{\"1\":{\"395\":1}}],[\"拟合一个\",{\"1\":{\"395\":1}}],[\"拟合分段函数时\",{\"1\":{\"395\":1}}],[\"深入的业务逻辑理解往往也能带来更好的\",{\"1\":{\"687\":1}}],[\"深度分离定理\",{\"1\":{\"395\":1}}],[\"深度叠加导致表达能力爆炸式增长\",{\"1\":{\"395\":1}}],[\"深度学习框架\",{\"1\":{\"670\":1}}],[\"深度学习框架的计算图范式\",{\"1\":{\"662\":1}}],[\"深度学习框架之所以强大\",{\"1\":{\"650\":1}}],[\"深度学习框架中蕴藏着惊人的技术和有趣的机制\",{\"1\":{\"604\":1}}],[\"深度学习中常见api记录\",{\"1\":{\"361\":1}}],[\"深度学习中常见问题记录\",{\"0\":{\"361\":1}}],[\"深度学习等领域也得到了广泛应用\",{\"1\":{\"321\":1}}],[\"深度学习模型\",{\"1\":{\"112\":1}}],[\"深层cnn\",{\"1\":{\"395\":1}}],[\"深层网络的代价\",{\"1\":{\"395\":1}}],[\"深层网络类似布尔电路中的分层设计\",{\"1\":{\"395\":1}}],[\"深层网络通过共享参数\",{\"1\":{\"395\":1}}],[\"深层网络将复杂函数分解为多个简单步骤\",{\"1\":{\"395\":1}}],[\"深层网络依赖函数的嵌套深度\",{\"1\":{\"395\":1}}],[\"深层网络可以逐步构造出更复杂的函数\",{\"1\":{\"395\":1}}],[\"深层网络\",{\"1\":{\"395\":4}}],[\"深层神经网络通过函数复合\",{\"1\":{\"395\":1}}],[\"爆炸问题\",{\"1\":{\"395\":1}}],[\"爆炸式增长\",{\"1\":{\"395\":1}}],[\"爆炸\",{\"1\":{\"395\":1}}],[\"龙格现象\",{\"1\":{\"395\":1}}],[\"案例2\",{\"1\":{\"395\":1}}],[\"案例1\",{\"1\":{\"395\":1}}],[\"案例\",{\"1\":{\"395\":1}}],[\"神经语言模型的规模定律\",{\"1\":{\"464\":1}}],[\"神经元\",{\"1\":{\"395\":2}}],[\"神经元过多易过拟合\",{\"1\":{\"395\":1}}],[\"神经元实现相同精度\",{\"1\":{\"395\":1}}],[\"神经元数量\",{\"1\":{\"395\":1}}],[\"神经网络通过非线性激活函数生成的动态基函数组合\",{\"1\":{\"395\":1}}],[\"神经网络\",{\"1\":{\"395\":2}}],[\"神经网络的权重矩阵通常以高精度的浮点数\",{\"1\":{\"428\":1}}],[\"神经网络的独特优势\",{\"1\":{\"395\":1}}],[\"神经网络的输出在训练初期往往接近于零\",{\"1\":{\"107\":1}}],[\"神经网络逼近\",{\"1\":{\"395\":2}}],[\"神经网络与深度学习\",{\"1\":{\"395\":1}}],[\"神经网络模型只认识数字\",{\"1\":{\"68\":1}}],[\"逼近fine\",{\"1\":{\"462\":1}}],[\"逼近\",{\"1\":{\"395\":1}}],[\"逼近精度与代价的权衡\",{\"1\":{\"395\":1}}],[\"逼近区间\",{\"1\":{\"395\":1}}],[\"逼近原理\",{\"1\":{\"395\":1}}],[\"逼近方式\",{\"1\":{\"395\":1}}],[\"泰勒展开要求函数无限可微\",{\"1\":{\"395\":1}}],[\"泰勒展开\",{\"1\":{\"395\":1}}],[\"挤压\",{\"1\":{\"395\":2}}],[\"爱因斯坦求和约定\",{\"1\":{\"390\":1}}],[\"访问\",{\"1\":{\"389\":1}}],[\"报错\",{\"1\":{\"387\":1}}],[\"必要时用\",{\"1\":{\"385\":1}}],[\"必要时会复制数据\",{\"1\":{\"385\":1}}],[\"必须通过\",{\"1\":{\"660\":1}}],[\"必须遵循概率公理\",{\"1\":{\"567\":1}}],[\"必须落在上下文部分\",{\"1\":{\"542\":1}}],[\"必须为两个独立的词\",{\"1\":{\"410\":1}}],[\"必须精确指定每维重复次数\",{\"1\":{\"387\":1}}],[\"必须用高阶函数\",{\"1\":{\"367\":1}}],[\"必须自己学会对各种姿态都识别准确\",{\"1\":{\"107\":1}}],[\"元\",{\"1\":{\"674\":2}}],[\"元学习和模型扩展趋势等多个重要研究方向的交汇点上\",{\"1\":{\"464\":1}}],[\"元学习\",{\"1\":{\"464\":1}}],[\"元学习与few\",{\"1\":{\"464\":1}}],[\"元学习与上下文学习的潜力\",{\"1\":{\"460\":1}}],[\"元组等\",{\"1\":{\"657\":1}}],[\"元组\",{\"1\":{\"381\":1}}],[\"元编程\",{\"1\":{\"372\":1}}],[\"耗时\",{\"1\":{\"379\":1}}],[\"耗时严重\",{\"1\":{\"49\":1}}],[\"典型的涌现能力\",{\"1\":{\"676\":1}}],[\"典型的输入是一个包含\",{\"1\":{\"540\":1}}],[\"典型壳层\",{\"1\":{\"594\":1}}],[\"典型集合\",{\"1\":{\"591\":1}}],[\"典型值\",{\"1\":{\"404\":1}}],[\"典型应用场景举例\",{\"0\":{\"379\":1}}],[\"典型步长\",{\"1\":{\"323\":2}}],[\"普通方法\",{\"0\":{\"373\":1}}],[\"装饰整个类\",{\"0\":{\"374\":1}}],[\"装饰类方法\",{\"0\":{\"373\":1}}],[\"装饰器的底层原理与执行过程\",{\"0\":{\"375\":1}}],[\"装饰器的实现用到了什么\",{\"0\":{\"367\":1}}],[\"装饰器会改变函数的元信息\",{\"1\":{\"372\":1}}],[\"装饰器工厂\",{\"0\":{\"371\":1},\"1\":{\"378\":1}}],[\"装饰器支持原函数有参数的情况\",{\"1\":{\"370\":1}}],[\"装饰器主要用于在\",{\"1\":{\"368\":1}}],[\"装饰器是\",{\"1\":{\"368\":1}}],[\"装饰器\",{\"0\":{\"368\":1},\"1\":{\"367\":1,\"368\":1,\"372\":1,\"375\":1}}],[\"持续推动着技术边界的拓展\",{\"1\":{\"684\":1}}],[\"持续下降\",{\"1\":{\"454\":1}}],[\"持续追踪日志最新输出\",{\"1\":{\"83\":1}}],[\"持有对\",{\"1\":{\"369\":1}}],[\"权限控制\",{\"1\":{\"379\":1}}],[\"权限校验等场景中非常常见\",{\"1\":{\"368\":1}}],[\"权重共享\",{\"1\":{\"513\":3}}],[\"权重初始化\",{\"1\":{\"293\":1,\"296\":1,\"447\":1}}],[\"权重衰减0\",{\"1\":{\"481\":1}}],[\"权重衰减\",{\"1\":{\"200\":1,\"201\":1}}],[\"权重衰减为\",{\"1\":{\"131\":1}}],[\"权重由\",{\"1\":{\"115\":1}}],[\"权重归一化\",{\"1\":{\"100\":1}}],[\"权重越大\",{\"1\":{\"100\":1}}],[\"权重\",{\"1\":{\"100\":1,\"191\":1,\"404\":1,\"565\":1}}],[\"权重项\",{\"1\":{\"78\":1}}],[\"绑定乘法运算符\",{\"1\":{\"660\":1}}],[\"绑定\",{\"1\":{\"366\":1,\"660\":2}}],[\"满足不同场景需求\",{\"1\":{\"674\":1}}],[\"满足下式\",{\"1\":{\"568\":1}}],[\"满足某些一致性要求\",{\"1\":{\"564\":1}}],[\"满足以下两个条件之一即可\",{\"1\":{\"365\":1}}],[\"满三次后再打包成一个group\",{\"1\":{\"328\":1}}],[\"压扁\",{\"1\":{\"359\":1}}],[\"压缩时间\",{\"1\":{\"46\":1}}],[\"惩罚项\",{\"1\":{\"470\":1}}],[\"惩罚更重\",{\"1\":{\"407\":2}}],[\"惩罚多\",{\"1\":{\"359\":1}}],[\"惩罚少\",{\"1\":{\"359\":1}}],[\"支配\",{\"1\":{\"359\":1}}],[\"支持智能体工作负载\",{\"1\":{\"684\":1}}],[\"支持最大\",{\"1\":{\"674\":1}}],[\"支持思考模式和非思考模式之间无缝切换\",{\"1\":{\"674\":1}}],[\"支持更复杂的系统提示词控制\",{\"1\":{\"674\":1}}],[\"支持更广泛的下游任务\",{\"1\":{\"464\":1}}],[\"支持处理极长的文档和对话历史\",{\"1\":{\"674\":1}}],[\"支持标准模式与推理思考模式\",{\"1\":{\"674\":1}}],[\"支持自定义知识库和行为模式\",{\"1\":{\"674\":1}}],[\"支持手势识别和情感表达\",{\"1\":{\"674\":1}}],[\"支持保存为png\",{\"1\":{\"666\":1}}],[\"支持显示变量名\",{\"1\":{\"666\":1}}],[\"支持高阶导数的构建\",{\"1\":{\"665\":1}}],[\"支持高分辨率输入\",{\"1\":{\"214\":1}}],[\"支持variable与数值\",{\"1\":{\"663\":1}}],[\"支持用+\",{\"1\":{\"663\":1}}],[\"支持python原生控制流\",{\"1\":{\"662\":1}}],[\"支持将数学公式直接转译为python代码\",{\"1\":{\"662\":1}}],[\"支持x\",{\"1\":{\"660\":1}}],[\"支持x1\",{\"1\":{\"660\":2}}],[\"支持x0\",{\"1\":{\"660\":2}}],[\"支持y\",{\"1\":{\"660\":1}}],[\"支持3\",{\"1\":{\"660\":1}}],[\"支持与ndarray及数值类型的混合运算\",{\"1\":{\"660\":1}}],[\"支持为变量设置自定义名称\",{\"1\":{\"659\":1}}],[\"支持代码重构和扩展\",{\"1\":{\"647\":1}}],[\"支持数据的修改和读取\",{\"1\":{\"609\":1}}],[\"支持无答案问题\",{\"1\":{\"494\":1}}],[\"支持复杂对话\",{\"1\":{\"483\":1}}],[\"支持这一点\",{\"1\":{\"469\":1}}],[\"支持基于上下文的词表示\",{\"1\":{\"464\":1}}],[\"支持对任何文本\",{\"1\":{\"454\":1}}],[\"支持对比学习任务\",{\"1\":{\"189\":1}}],[\"支持加权和平均损失\",{\"1\":{\"401\":1}}],[\"支持的文件后缀类型\",{\"1\":{\"289\":1}}],[\"支持两种llm\",{\"1\":{\"286\":1}}],[\"支持两种配置\",{\"1\":{\"189\":1}}],[\"支持4k输入\",{\"1\":{\"222\":1}}],[\"支持4k分辨率\",{\"1\":{\"208\":1}}],[\"支持零样本扩展至\",{\"1\":{\"219\":1}}],[\"支持文本\",{\"1\":{\"210\":1}}],[\"支持文本和图像输入\",{\"1\":{\"210\":1}}],[\"支持多轮对话的同时\",{\"1\":{\"674\":1}}],[\"支持多轮视觉对话\",{\"1\":{\"227\":1}}],[\"支持多种自然语言处理任务\",{\"1\":{\"674\":1}}],[\"支持多种任务模式\",{\"1\":{\"189\":1}}],[\"支持多工具并行调用与精准指令解析\",{\"1\":{\"674\":1}}],[\"支持多个输入与输出\",{\"1\":{\"651\":1}}],[\"支持多维度同时交换\",{\"1\":{\"383\":1}}],[\"支持多模态问答\",{\"1\":{\"188\":1}}],[\"支持区域描述和问答\",{\"1\":{\"185\":1}}],[\"支持感知\",{\"1\":{\"181\":1}}],[\"支持刚性变换标准化\",{\"1\":{\"112\":1}}],[\"支持原始点云\",{\"1\":{\"112\":1}}],[\"支持通过爱因斯坦求和约定\",{\"1\":{\"76\":1}}],[\"支持\",{\"1\":{\"49\":1,\"82\":2,\"163\":4,\"387\":1,\"401\":1,\"402\":1,\"513\":1,\"674\":1}}],[\"尺度差异性\",{\"0\":{\"359\":1}}],[\"哪些\",{\"1\":{\"565\":1}}],[\"哪些图片是不相似的\",{\"1\":{\"234\":1}}],[\"哪怕某个维度的数据本来波动很大\",{\"1\":{\"357\":1}}],[\"身高\",{\"1\":{\"355\":1}}],[\"身高和体重的方差都为\",{\"1\":{\"355\":1}}],[\"身高和体重\",{\"1\":{\"355\":1}}],[\"身份证号\",{\"1\":{\"92\":1}}],[\"马氏距离适合不同维度尺度差别大\",{\"1\":{\"360\":1}}],[\"马氏距离通过协方差矩阵逆变换\",{\"1\":{\"359\":1}}],[\"马氏距离中对第一个维度的偏差乘上\",{\"1\":{\"359\":1}}],[\"马氏距离会自动把不同特征的偏差按标准差进行\",{\"1\":{\"359\":1}}],[\"马氏距离定义如下\",{\"1\":{\"359\":1}}],[\"马氏距离等\",{\"1\":{\"355\":1}}],[\"马氏距离\",{\"0\":{\"356\":1,\"358\":1},\"1\":{\"355\":1}}],[\"椭圆朝反对角线方向倾斜\",{\"1\":{\"355\":1}}],[\"沿计算图反向推导各变量的导数\",{\"1\":{\"630\":1}}],[\"沿对角线方向拉长的椭圆\",{\"1\":{\"355\":1}}],[\"沿着某个特定维度需要跨越多少个元素\",{\"1\":{\"323\":1}}],[\"圆形\",{\"1\":{\"355\":1}}],[\"协方差为\",{\"1\":{\"355\":1}}],[\"协方差函数定义样本之间的相似性结构\",{\"1\":{\"355\":1}}],[\"协方差\",{\"1\":{\"355\":5}}],[\"协方差接近\",{\"1\":{\"355\":1}}],[\"协方差是负的\",{\"1\":{\"355\":1}}],[\"协方差是正的\",{\"1\":{\"355\":1}}],[\"协方差是两个变量\",{\"1\":{\"355\":1}}],[\"协方差矩阵形式为\",{\"1\":{\"590\":1}}],[\"协方差矩阵可以用来\",{\"1\":{\"355\":1}}],[\"协方差矩阵的结构\",{\"1\":{\"355\":1}}],[\"协方差矩阵\",{\"0\":{\"355\":1},\"1\":{\"355\":2}}],[\"较低的阈值\",{\"1\":{\"353\":1}}],[\"较高\",{\"1\":{\"353\":1}}],[\"尝试加载已保存的字典\",{\"1\":{\"511\":1}}],[\"尝试正确完成任务\",{\"1\":{\"471\":1}}],[\"尝试确保被标记为垃圾邮件的电子邮件实际上是垃圾邮件\",{\"1\":{\"347\":1}}],[\"尝试使用conda\",{\"1\":{\"338\":1}}],[\"您选择优先考虑的指标取决于特定问题的成本\",{\"1\":{\"347\":1}}],[\"误检\",{\"1\":{\"405\":1}}],[\"误报为阳性概率是\",{\"1\":{\"569\":1}}],[\"误报\",{\"1\":{\"353\":1,\"405\":1}}],[\"误报很可怕\",{\"1\":{\"347\":1}}],[\"误报概率\",{\"0\":{\"345\":1}}],[\"误分类为\",{\"1\":{\"7\":1}}],[\"召回率等指标上的表现\",{\"1\":{\"408\":1}}],[\"召回率曲线的创建方法是\",{\"1\":{\"352\":1}}],[\"召回率曲线\",{\"1\":{\"352\":1}}],[\"召回率会提高\",{\"1\":{\"346\":1}}],[\"召回率比准确率更有意义\",{\"1\":{\"344\":1}}],[\"召回率衡量的是被正确分类为垃圾邮件的垃圾邮件电子邮件的比例\",{\"1\":{\"344\":1}}],[\"召回率的定义为\",{\"1\":{\"344\":1}}],[\"召回率\",{\"0\":{\"344\":1},\"1\":{\"347\":1}}],[\"垃圾邮件被误分类为非垃圾邮件\",{\"1\":{\"342\":1}}],[\"垃圾邮件被正确分类为垃圾邮件\",{\"1\":{\"342\":1}}],[\"假阴性\",{\"1\":{\"405\":2}}],[\"假阳性\",{\"1\":{\"405\":2}}],[\"假负例通常比假正例的后果更严重\",{\"1\":{\"344\":1}}],[\"假负例是指被误分类为负例的实际正例\",{\"1\":{\"344\":1}}],[\"假负例或假正例\",{\"1\":{\"343\":1}}],[\"假负例\",{\"1\":{\"342\":1}}],[\"假正例是被错误分类的实际负例\",{\"1\":{\"345\":1}}],[\"假正例率为\",{\"1\":{\"345\":1}}],[\"假正例率\",{\"0\":{\"345\":1},\"1\":{\"345\":1,\"347\":1}}],[\"假正例\",{\"1\":{\"342\":1}}],[\"假设样本空间是实数集合的子集\",{\"1\":{\"566\":1}}],[\"假设你有一个问题\",{\"1\":{\"544\":1}}],[\"假设原始上下文是\",{\"1\":{\"542\":1}}],[\"假设最终的输出\",{\"1\":{\"508\":1}}],[\"假设有标记数据集\",{\"1\":{\"444\":1}}],[\"假设有一个完美的模型\",{\"1\":{\"346\":1}}],[\"假设有一个\",{\"1\":{\"325\":1}}],[\"假设预训练的矩阵为\",{\"1\":{\"425\":1}}],[\"假设要在下游任务微调一个预训练语言模型\",{\"1\":{\"425\":1}}],[\"假设模型在任务适配过程中权重的改变量是低秩\",{\"1\":{\"424\":1}}],[\"假设两个正样本\",{\"1\":{\"404\":1}}],[\"假设某个\",{\"1\":{\"396\":1}}],[\"假设输入的token序列为\",{\"1\":{\"517\":1}}],[\"假设输入图像经过卷积得到一个特征图\",{\"1\":{\"396\":1}}],[\"假设输入数据维度为\",{\"1\":{\"36\":1}}],[\"假设我们有一个\",{\"1\":{\"578\":1}}],[\"假设我们有一个检测疾病的筛查工具\",{\"1\":{\"569\":1}}],[\"假设我们有一个样本点\",{\"1\":{\"359\":1}}],[\"假设我们有预测概率图\",{\"1\":{\"407\":1}}],[\"假设我们有如下样本\",{\"1\":{\"355\":1}}],[\"假设我们在\",{\"1\":{\"315\":1}}],[\"假设的理想模型的\",{\"1\":{\"350\":1}}],[\"假设一个完美的模型不会出现假负例\",{\"1\":{\"344\":1}}],[\"假设为\",{\"1\":{\"327\":1}}],[\"假设为常规行优先存储\",{\"1\":{\"327\":1}}],[\"假设空间中所有区域的尺度或特征分布具有一定的一致性\",{\"1\":{\"90\":1}}],[\"真相\",{\"1\":{\"596\":1}}],[\"真正随机的是实验结果\",{\"1\":{\"565\":1}}],[\"真正例率\",{\"0\":{\"344\":1},\"1\":{\"344\":1,\"347\":1}}],[\"真正例\",{\"1\":{\"342\":1}}],[\"真阳性\",{\"1\":{\"405\":2}}],[\"真负例\",{\"1\":{\"342\":1}}],[\"真实token对应1\",{\"1\":{\"520\":1}}],[\"真实性缺陷\",{\"1\":{\"484\":1}}],[\"真实性\",{\"1\":{\"470\":1}}],[\"真实用户在\",{\"1\":{\"470\":1}}],[\"真实掩码\",{\"1\":{\"402\":1}}],[\"真实统计意义上的远近\",{\"1\":{\"359\":1}}],[\"真实的\",{\"1\":{\"82\":1}}],[\"真实标签对应的概率为\",{\"1\":{\"155\":1}}],[\"真实标签\",{\"1\":{\"40\":1,\"82\":1,\"157\":1,\"401\":1,\"402\":2,\"405\":1,\"407\":2}}],[\"混淆矩阵\",{\"0\":{\"342\":1}}],[\"混合专家模型\",{\"1\":{\"674\":1}}],[\"混合精度训练\",{\"1\":{\"494\":1}}],[\"混合模型的常见结合方式\",{\"1\":{\"299\":1}}],[\"混合模型探索\",{\"0\":{\"299\":1}}],[\"混合模型改进\",{\"1\":{\"112\":1}}],[\"混合特征方法\",{\"1\":{\"212\":1}}],[\"混合两种损失\",{\"1\":{\"163\":1}}],[\"混合比\",{\"1\":{\"163\":1}}],[\"混合编码器\",{\"1\":{\"142\":1}}],[\"混合\",{\"1\":{\"71\":1}}],[\"常识和写作能力\",{\"1\":{\"677\":1}}],[\"常识推理\",{\"1\":{\"482\":1}}],[\"常识推理与winograd类任务\",{\"1\":{\"462\":1}}],[\"常识推理等多样化任务\",{\"1\":{\"460\":1}}],[\"常识推理提升8\",{\"1\":{\"440\":1}}],[\"常省略下标\",{\"1\":{\"565\":1}}],[\"常规可测的集合\",{\"1\":{\"566\":1}}],[\"常规微调\",{\"1\":{\"423\":1}}],[\"常规的词汇表\",{\"1\":{\"410\":1}}],[\"常用的peft方案\",{\"0\":{\"417\":1}}],[\"常用于保存一些模型的状态信息\",{\"1\":{\"389\":1}}],[\"常用于临时缓存数据\",{\"1\":{\"389\":1}}],[\"常用于\",{\"1\":{\"382\":1,\"387\":1}}],[\"常用评估指标\",{\"0\":{\"340\":1},\"1\":{\"340\":1}}],[\"常见事件\",{\"1\":{\"566\":1}}],[\"常见名词准确率93\",{\"1\":{\"455\":1}}],[\"常见错误\",{\"0\":{\"339\":1}}],[\"常见疑问解答\",{\"0\":{\"316\":1}}],[\"常见的llm\",{\"0\":{\"674\":1}}],[\"常见的应用场景包括\",{\"1\":{\"543\":1,\"544\":1}}],[\"常见的迁移学习方法是首先在大规模数据集\",{\"1\":{\"278\":1}}],[\"常见的对称函数\",{\"1\":{\"115\":1}}],[\"纯公开数据训练结果挑战了专有数据的必要性\",{\"1\":{\"482\":1}}],[\"纯\",{\"1\":{\"338\":1}}],[\"纯视觉模式\",{\"1\":{\"188\":1}}],[\"纯视觉模型\",{\"1\":{\"188\":1}}],[\"安装失败的包\",{\"1\":{\"338\":1}}],[\"安装所需要的依赖包\",{\"1\":{\"338\":1}}],[\"安全且有用地遵循用户指令\",{\"1\":{\"468\":1}}],[\"安全\",{\"1\":{\"133\":1,\"472\":1}}],[\"带示例的任务\",{\"1\":{\"470\":1}}],[\"带参数的装饰器\",{\"0\":{\"371\":1}}],[\"带参数的函数装饰器\",{\"0\":{\"370\":1}}],[\"带星号\",{\"1\":{\"334\":1}}],[\"带曲线\",{\"1\":{\"28\":1}}],[\"查看当前环境已安装的包\",{\"0\":{\"337\":1},\"1\":{\"337\":1}}],[\"查看当前conda激活的环境\",{\"1\":{\"336\":1}}],[\"查看当前激活的环境\",{\"0\":{\"336\":1}}],[\"查看所有已创建的环境\",{\"0\":{\"334\":1}}],[\"查询向量\",{\"1\":{\"247\":1,\"310\":1}}],[\"查询点的位置\",{\"1\":{\"92\":1}}],[\"查询点数量\",{\"1\":{\"92\":1}}],[\"查询相比\",{\"1\":{\"90\":1}}],[\"杂谈\",{\"0\":{\"329\":1}}],[\"里\",{\"1\":{\"328\":2,\"389\":1}}],[\"里面肯定含有整句话的完整信息\",{\"1\":{\"508\":1}}],[\"里面包含了注意力机制\",{\"1\":{\"298\":1}}],[\"里面包含了非常丰富的成果\",{\"1\":{\"298\":1}}],[\"里面有n个图片\",{\"1\":{\"235\":1}}],[\"≤\",{\"1\":{\"327\":1}}],[\"字或词\",{\"1\":{\"505\":1}}],[\"字母重排\",{\"1\":{\"462\":1}}],[\"字符串\",{\"1\":{\"389\":1,\"454\":1}}],[\"字节级bpe\",{\"1\":{\"497\":1}}],[\"字节级\",{\"1\":{\"454\":1}}],[\"字节对编码\",{\"1\":{\"409\":1}}],[\"字节\",{\"1\":{\"327\":1}}],[\"字典的构建过程太过粗糙\",{\"1\":{\"511\":1}}],[\"字典的大\",{\"1\":{\"242\":1}}],[\"字典的大小就很重要\",{\"1\":{\"240\":1}}],[\"字典初始化\",{\"1\":{\"511\":1}}],[\"字典大小\",{\"1\":{\"246\":1}}],[\"字典大小和字典特征一致性经常不能同时满足\",{\"1\":{\"241\":1}}],[\"字典大小是\",{\"1\":{\"241\":1}}],[\"字典\",{\"1\":{\"162\":1}}],[\"运算符到pow函数\",{\"1\":{\"660\":1}}],[\"运算符到neg函数\",{\"1\":{\"660\":1}}],[\"运算符优先级与类型转换\",{\"1\":{\"660\":1}}],[\"运算符的调用顺序遵循以下规则\",{\"1\":{\"660\":1}}],[\"运算符上\",{\"1\":{\"660\":1}}],[\"运算符重载需要同时考虑左右运算符\",{\"1\":{\"660\":1}}],[\"运算符重载\",{\"0\":{\"660\":1}}],[\"运算中非常常见且强大的一种机制\",{\"1\":{\"327\":1}}],[\"运行和与\",{\"1\":{\"685\":1}}],[\"运行代码\",{\"1\":{\"667\":1}}],[\"运行结果为\",{\"1\":{\"667\":1}}],[\"运行\",{\"1\":{\"666\":1}}],[\"运行上述代码\",{\"1\":{\"276\":1}}],[\"运行可视化\",{\"1\":{\"83\":1}}],[\"底层内存中的数据并没有被重新排列\",{\"1\":{\"326\":1}}],[\"底层数据存储为\",{\"1\":{\"326\":1}}],[\"拷贝\",{\"1\":{\"326\":1}}],[\"物理数据未改变\",{\"1\":{\"327\":1}}],[\"物理转置\",{\"1\":{\"326\":1}}],[\"物体结构扭曲\",{\"1\":{\"178\":1}}],[\"物体在空间中移动时\",{\"1\":{\"116\":1}}],[\"物体分割和场景语义解析\",{\"1\":{\"103\":1}}],[\"物体名称匹配\",{\"1\":{\"68\":1}}],[\"物体组合\",{\"1\":{\"65\":1}}],[\"物体区域特征和点云特征的联合建模\",{\"1\":{\"59\":1}}],[\"物体区域特征图展平\",{\"1\":{\"59\":1}}],[\"物体几何结构文本\",{\"1\":{\"29\":2}}],[\"物体几何结构文本数据文件路径\",{\"1\":{\"29\":1}}],[\"物体几何知识特征\",{\"1\":{\"13\":1}}],[\"物体\",{\"1\":{\"26\":1,\"67\":1}}],[\"物体类别索引\",{\"1\":{\"68\":1}}],[\"物体类别数\",{\"1\":{\"65\":2}}],[\"物体类别\",{\"1\":{\"17\":1,\"67\":1,\"68\":1}}],[\"物体交互感知\",{\"1\":{\"11\":1}}],[\"物体可供性定位方法\",{\"1\":{\"26\":1}}],[\"物体可供性定位任务\",{\"1\":{\"16\":1}}],[\"物体可供性\",{\"1\":{\"8\":1}}],[\"轴上绘制所有阈值下的召回率\",{\"1\":{\"352\":1}}],[\"轴上绘制精确率\",{\"1\":{\"352\":1}}],[\"轴张量\",{\"1\":{\"328\":1}}],[\"轴\",{\"1\":{\"326\":1}}],[\"属于哪一代\",{\"1\":{\"656\":1}}],[\"属于自蒸馏的一种形式\",{\"1\":{\"150\":1}}],[\"属性\",{\"1\":{\"325\":1,\"327\":1,\"660\":1}}],[\"切分成一个句子列表\",{\"1\":{\"510\":1}}],[\"切分后\",{\"1\":{\"31\":1}}],[\"切换也非常方便\",{\"1\":{\"420\":1}}],[\"切换\",{\"0\":{\"332\":1}}],[\"切片后可能是\",{\"1\":{\"325\":1}}],[\"切片索引语法\",{\"1\":{\"325\":1}}],[\"切片\",{\"0\":{\"325\":1},\"1\":{\"325\":2,\"651\":1}}],[\"行为\",{\"1\":{\"681\":1}}],[\"行业大模型\",{\"1\":{\"674\":1}}],[\"行维度步幅\",{\"1\":{\"327\":1}}],[\"行步长为\",{\"1\":{\"325\":1}}],[\"行只需要在内存中前进一步\",{\"1\":{\"323\":1}}],[\"行移动到第\",{\"1\":{\"323\":2}}],[\"行\",{\"1\":{\"323\":7,\"396\":1}}],[\"行优先布局\",{\"1\":{\"326\":1}}],[\"行优先\",{\"1\":{\"323\":1}}],[\"行优先存储模式的步长\",{\"1\":{\"323\":1}}],[\"行的所有元素\",{\"1\":{\"322\":2}}],[\"连续包月优惠价为\",{\"1\":{\"674\":1}}],[\"连续随机变量形式\",{\"0\":{\"572\":1}}],[\"连续随机变量\",{\"0\":{\"566\":1}}],[\"连续句子打包\",{\"1\":{\"495\":1,\"497\":1}}],[\"连续\",{\"1\":{\"447\":1}}],[\"连续存储的方式\",{\"1\":{\"326\":1}}],[\"连续性\",{\"0\":{\"322\":1},\"1\":{\"321\":1,\"322\":1}}],[\"连接起来\",{\"1\":{\"286\":1}}],[\"连接图像特征和语言嵌入空间的线性层\",{\"1\":{\"227\":1}}],[\"连接视觉特征和语言嵌入空间\",{\"1\":{\"226\":1}}],[\"连接视觉编码器和llm\",{\"1\":{\"185\":1}}],[\"连接llm解码器\",{\"1\":{\"189\":1}}],[\"连接llm\",{\"1\":{\"188\":1}}],[\"连接\",{\"1\":{\"33\":1,\"190\":1}}],[\"❓q\",{\"0\":{\"317\":1,\"318\":1}}],[\"值得大力推广\",{\"1\":{\"677\":1}}],[\"值得注意的是\",{\"1\":{\"326\":1}}],[\"值的更新过程\",{\"1\":{\"667\":1}}],[\"值增加最快的方向是\",{\"1\":{\"667\":1}}],[\"值加1\",{\"1\":{\"656\":1}}],[\"值变大并不能提升微调的效果\",{\"1\":{\"427\":1}}],[\"值越大\",{\"1\":{\"404\":1}}],[\"值越小表示匹配越好\",{\"1\":{\"401\":1}}],[\"值\",{\"1\":{\"325\":1,\"397\":1,\"403\":1,\"404\":1,\"630\":1}}],[\"值向量\",{\"1\":{\"310\":1}}],[\"值为\",{\"1\":{\"163\":1,\"407\":1}}],[\"仍然需要大量的定制开发工作\",{\"1\":{\"682\":1}}],[\"仍然不如分块处理或多层级聚合模型高效\",{\"1\":{\"112\":1}}],[\"仍表现不佳\",{\"1\":{\"472\":1}}],[\"仍会生成有害\",{\"1\":{\"472\":1}}],[\"仍可生成有害内容\",{\"1\":{\"471\":1}}],[\"仍需谨慎评估其边界与适用性\",{\"1\":{\"463\":1}}],[\"仍暴露出其推理深度与语言理解的局限\",{\"1\":{\"462\":1}}],[\"仍是\",{\"1\":{\"308\":1}}],[\"课程笔记\",{\"0\":{\"302\":1}}],[\"架起了图像空间到文本空间的桥梁\",{\"1\":{\"301\":1}}],[\"架构改进\",{\"1\":{\"684\":1}}],[\"架构的神经网络模型开始崭露头角\",{\"1\":{\"673\":1}}],[\"架构微调\",{\"1\":{\"481\":1}}],[\"架构中\",{\"1\":{\"107\":1}}],[\"架构\",{\"1\":{\"93\":1,\"99\":1,\"214\":1,\"275\":1,\"280\":1,\"447\":1,\"454\":2,\"470\":1,\"674\":3}}],[\"考察其中任一排列方式\",{\"1\":{\"600\":1}}],[\"考察所有可能的\",{\"1\":{\"82\":1}}],[\"考虑元素之间的顺序\",{\"0\":{\"600\":1}}],[\"考虑某个点\",{\"1\":{\"593\":1}}],[\"考虑两个事件\",{\"1\":{\"568\":1}}],[\"考虑\",{\"1\":{\"428\":1}}],[\"考虑使用其他指标\",{\"1\":{\"347\":1}}],[\"考虑篇幅原因\",{\"1\":{\"300\":1}}],[\"换源到图像上\",{\"1\":{\"298\":1}}],[\"换句话说\",{\"1\":{\"100\":1,\"115\":1,\"234\":1,\"357\":1,\"566\":1}}],[\"百万\",{\"1\":{\"297\":1}}],[\"右乘\",{\"1\":{\"660\":1}}],[\"右操作数\",{\"1\":{\"660\":1}}],[\"右填充\",{\"1\":{\"520\":1}}],[\"右侧的ndarray或数值会被自动转换为variable\",{\"1\":{\"660\":1}}],[\"右侧是cot\",{\"1\":{\"434\":1}}],[\"右侧曲线的\",{\"1\":{\"353\":1}}],[\"右\",{\"1\":{\"328\":2}}],[\"右图中\",{\"1\":{\"297\":1}}],[\"右边的编码是在线下执行\",{\"1\":{\"242\":1}}],[\"右边编码器会被之前的\",{\"1\":{\"238\":1}}],[\"累加当前批次的样本数到总样本数中\",{\"1\":{\"296\":1}}],[\"没有重复的维度字母表示保留该维度\",{\"1\":{\"390\":1}}],[\"没有\",{\"1\":{\"372\":1}}],[\"没有padding\",{\"1\":{\"295\":1}}],[\"没有固定顺序\",{\"1\":{\"104\":1}}],[\"键\",{\"1\":{\"295\":2}}],[\"键向量\",{\"1\":{\"247\":1,\"310\":1}}],[\"左操作数3是int\",{\"1\":{\"660\":1}}],[\"左操作数x是variable\",{\"1\":{\"660\":1}}],[\"左操作数优先\",{\"1\":{\"660\":1}}],[\"左侧是常规的prompt\",{\"1\":{\"434\":1}}],[\"左侧为transformer原始的encoder结构\",{\"1\":{\"294\":1}}],[\"左边做一次梯度回传之后\",{\"1\":{\"242\":1}}],[\"收益和风险\",{\"1\":{\"347\":1}}],[\"收敛情况的粗略指标\",{\"1\":{\"347\":1}}],[\"收敛\",{\"1\":{\"292\":1}}],[\"收集相邻句对\",{\"1\":{\"512\":1}}],[\"收集相同数量的相邻句对和非相邻句对\",{\"1\":{\"512\":1}}],[\"收集一组模型输出对\",{\"1\":{\"470\":1}}],[\"收集人类对模型多个输出的排序偏好\",{\"1\":{\"468\":1}}],[\"收集人类示范数据\",{\"1\":{\"468\":1}}],[\"收集人类偏好数据\",{\"1\":{\"224\":1}}],[\"收集成\",{\"1\":{\"363\":2}}],[\"收集了大量的人类编写的\",{\"1\":{\"224\":1}}],[\"收集指令\",{\"1\":{\"224\":1}}],[\"收集约\",{\"1\":{\"190\":1}}],[\"收集所有样本的预测结果\",{\"1\":{\"82\":1}}],[\"范式\",{\"1\":{\"464\":1}}],[\"范围\",{\"1\":{\"290\":2}}],[\"范围缩放到\",{\"1\":{\"290\":2}}],[\"范数进行归一化\",{\"1\":{\"247\":1}}],[\"范数\",{\"1\":{\"108\":2}}],[\"组合\",{\"0\":{\"601\":1}}],[\"组合分析\",{\"0\":{\"598\":1},\"1\":{\"598\":1}}],[\"组合数\",{\"1\":{\"579\":1}}],[\"组合损失\",{\"1\":{\"402\":1}}],[\"组合损失值\",{\"1\":{\"402\":1}}],[\"组合后的优势\",{\"1\":{\"402\":1}}],[\"组合后的图像张量和标签张量\",{\"1\":{\"289\":1}}],[\"组合方式\",{\"1\":{\"395\":1}}],[\"组成一个\",{\"1\":{\"355\":1}}],[\"组成新的训练集\",{\"1\":{\"128\":1}}],[\"组成\",{\"1\":{\"15\":1,\"493\":1,\"625\":1}}],[\"归纳偏置能够帮助学习算法缩小搜索范围\",{\"1\":{\"287\":1}}],[\"归纳偏置\",{\"1\":{\"287\":1}}],[\"归一化常数\",{\"1\":{\"586\":1,\"590\":1}}],[\"归一化处理\",{\"1\":{\"461\":1}}],[\"归一化层\",{\"1\":{\"291\":1}}],[\"归一化有助于模型更快地收敛\",{\"1\":{\"290\":1}}],[\"归一化为权重\",{\"1\":{\"266\":1}}],[\"归一化特征\",{\"1\":{\"204\":1}}],[\"归一化权重\",{\"1\":{\"100\":1}}],[\"归一化因子\",{\"1\":{\"100\":1}}],[\"归一化到单位球内\",{\"1\":{\"83\":1}}],[\"归一化后的点云数据\",{\"1\":{\"83\":1}}],[\"归一化\",{\"1\":{\"45\":1,\"83\":1,\"147\":2,\"161\":2,\"162\":2,\"268\":1,\"477\":2,\"674\":1}}],[\"打破了\",{\"1\":{\"674\":1}}],[\"打破了这两个领域壁垒\",{\"1\":{\"287\":1}}],[\"打印时自动对齐多行数据\",{\"1\":{\"659\":1}}],[\"打印当前性能指标\",{\"1\":{\"82\":1}}],[\"打乱顺序\",{\"1\":{\"512\":1}}],[\"打开指定索引的图像文件\",{\"1\":{\"289\":1}}],[\"充当了soft\",{\"1\":{\"286\":1}}],[\"充分利用数据编排框架的优势\",{\"1\":{\"684\":1}}],[\"充分利用了大型语言模型的强大能力\",{\"1\":{\"682\":1}}],[\"充分利用不同质量的数据\",{\"1\":{\"190\":1}}],[\"充分吸收了高级区域抽象特征和文本特征\",{\"1\":{\"70\":1}}],[\"错误还是不知道\",{\"1\":{\"508\":1}}],[\"错误分析显示\",{\"1\":{\"455\":1}}],[\"错误分类为\",{\"1\":{\"6\":1}}],[\"错位对齐\",{\"1\":{\"285\":1}}],[\"叠加的运算流程\",{\"1\":{\"285\":1}}],[\"列方向\",{\"1\":{\"514\":2}}],[\"列表\",{\"1\":{\"471\":1,\"511\":1,\"657\":1}}],[\"列表最后一个记录了缓存的key和value\",{\"1\":{\"285\":2}}],[\"列维度步幅\",{\"1\":{\"327\":1}}],[\"列步长为\",{\"1\":{\"325\":1}}],[\"列或其他任意模式访问数据\",{\"1\":{\"325\":1}}],[\"列的\",{\"1\":{\"396\":1}}],[\"列的数据\",{\"1\":{\"323\":1}}],[\"列的所有元素\",{\"1\":{\"322\":2}}],[\"列时\",{\"1\":{\"323\":1}}],[\"列跳转到第\",{\"1\":{\"323\":1}}],[\"列优先\",{\"1\":{\"323\":2,\"326\":1}}],[\"列优先存储模式的步长\",{\"1\":{\"323\":1}}],[\"列\",{\"1\":{\"323\":5}}],[\"列举两个该物体常见的其他交互方式\",{\"1\":{\"12\":1}}],[\"缓冲区会被自动保存到模型的状态字典\",{\"1\":{\"389\":1}}],[\"缓冲区不会保存到\",{\"1\":{\"389\":1}}],[\"缓冲区的名称\",{\"1\":{\"389\":1}}],[\"缓存显著压缩为潜在向量来保证高效推理的同时不降低效果\",{\"1\":{\"674\":1}}],[\"缓存当前轮计算结果\",{\"1\":{\"477\":1}}],[\"缓存当前轮可重复利用的计算结果\",{\"1\":{\"474\":1}}],[\"缓存key\",{\"1\":{\"285\":1}}],[\"缓存key和value\",{\"1\":{\"285\":1}}],[\"缓存的key和value\",{\"1\":{\"285\":1}}],[\"缓存和复用\",{\"1\":{\"285\":1}}],[\"缓存\",{\"1\":{\"285\":1,\"477\":1}}],[\"缓解了对齐损失\",{\"1\":{\"471\":1}}],[\"缓解深层网络的梯度问题\",{\"1\":{\"454\":1}}],[\"缓解\",{\"1\":{\"403\":1}}],[\"缓解噪声影响\",{\"1\":{\"149\":1}}],[\"缓解类别极度不平衡问题\",{\"1\":{\"78\":1}}],[\"吸收图像特征\",{\"1\":{\"285\":1}}],[\"好\",{\"1\":{\"280\":3}}],[\"侧重于信息检索和融合外部知识\",{\"1\":{\"681\":1}}],[\"侧重于给这些点的特征分配更大的融合权重\",{\"1\":{\"72\":1}}],[\"侧重模态融合\",{\"1\":{\"280\":1}}],[\"回调\",{\"1\":{\"683\":1}}],[\"回收时机非即时性\",{\"1\":{\"657\":1}}],[\"回顾下之前的多模态网络设计\",{\"1\":{\"280\":1}}],[\"回答问题\",{\"1\":{\"678\":1}}],[\"回答\",{\"1\":{\"224\":1,\"231\":1,\"542\":1}}],[\"曾有一段时期一直在追求更大的网络架构\",{\"1\":{\"280\":1}}],[\"庖丁解牛vit\",{\"0\":{\"287\":1}}],[\"庖丁解牛blip2\",{\"0\":{\"279\":1},\"1\":{\"279\":1}}],[\"庖丁解牛clip\",{\"0\":{\"269\":1}}],[\"谷歌提出的\",{\"1\":{\"424\":1}}],[\"谷歌利用强大的计算能力进行了预训练\",{\"1\":{\"278\":1}}],[\"谷歌采用的弱监督方法与以往方法的一个主要区别在于规模\",{\"1\":{\"278\":1}}],[\"搜索出来的图片\",{\"1\":{\"276\":1}}],[\"遍历所有相邻字符对的组合\",{\"1\":{\"411\":1}}],[\"遍历所有句子\",{\"1\":{\"411\":1}}],[\"遍历vocab中所有词\",{\"1\":{\"410\":2}}],[\"遍历句子列表\",{\"1\":{\"410\":1}}],[\"遍历数据加载器中的每个批次数据\",{\"1\":{\"296\":1}}],[\"遍历获取supported支持的所有文件路径\",{\"1\":{\"289\":1}}],[\"遍历每个文件夹下的文件\",{\"1\":{\"289\":1}}],[\"遍历文件夹\",{\"1\":{\"289\":1}}],[\"遍历\",{\"1\":{\"276\":1,\"277\":1}}],[\"遍历data目录\",{\"1\":{\"276\":1}}],[\"遍历标注数据列表\",{\"1\":{\"68\":1}}],[\"针对性优化\",{\"1\":{\"686\":1}}],[\"针对不同设置调整峰值学习率和预热步数\",{\"1\":{\"494\":1}}],[\"针对同一输入\",{\"1\":{\"470\":1}}],[\"针对每张图像\",{\"1\":{\"275\":1}}],[\"针对每个图像嵌入向量获取和其相似度最高的分类文本嵌入向量索引下标\",{\"1\":{\"275\":1}}],[\"针对每个图像嵌入向量取出和其相似度最高的那个文本嵌入向量的索引\",{\"1\":{\"275\":1}}],[\"针对铰接物体\",{\"1\":{\"51\":1}}],[\"子层的输入进行了\",{\"1\":{\"674\":1}}],[\"子序列\",{\"1\":{\"542\":1}}],[\"子目录名的格式\",{\"1\":{\"275\":1}}],[\"子任务上\",{\"1\":{\"195\":1}}],[\"花卉图片分类\",{\"0\":{\"275\":1}}],[\"紧密相关\",{\"1\":{\"274\":1}}],[\"介绍了gpt\",{\"1\":{\"452\":1}}],[\"介绍几种具体的技巧\",{\"1\":{\"433\":1}}],[\"介绍\",{\"0\":{\"271\":1}}],[\"广泛的集成\",{\"1\":{\"684\":1}}],[\"广泛用于语言建模\",{\"1\":{\"510\":1}}],[\"广泛用于现代自然语言处理任务中\",{\"1\":{\"409\":1}}],[\"广泛应用于各类计算机视觉任务\",{\"1\":{\"270\":1}}],[\"广播视图\",{\"1\":{\"385\":1}}],[\"广播通过\",{\"1\":{\"327\":1}}],[\"广播\",{\"0\":{\"327\":1},\"1\":{\"112\":1,\"327\":2,\"558\":1}}],[\"年开始\",{\"1\":{\"682\":1}}],[\"年推进过\",{\"1\":{\"678\":1}}],[\"年发布\",{\"1\":{\"674\":1}}],[\"年至\",{\"1\":{\"674\":1}}],[\"年左右\",{\"1\":{\"673\":1}}],[\"年深度学习先驱\",{\"1\":{\"673\":1}}],[\"年代\",{\"1\":{\"673\":1}}],[\"年代法国电影\",{\"1\":{\"472\":1}}],[\"年龄\",{\"1\":{\"355\":1}}],[\"年\",{\"1\":{\"270\":1,\"673\":1,\"674\":49,\"677\":1,\"684\":1}}],[\"年可谓是视觉\",{\"1\":{\"270\":1}}],[\"年提出\",{\"1\":{\"73\":1}}],[\"丢给\",{\"1\":{\"508\":1}}],[\"丢掉\",{\"1\":{\"268\":3}}],[\"丢失部分几何信息\",{\"1\":{\"114\":1}}],[\"┌────────▼───────────┐\",{\"1\":{\"267\":1}}],[\"┌────────┴───────────┐\",{\"1\":{\"267\":1}}],[\"└────────┬───────────┘\",{\"1\":{\"267\":2}}],[\"│\",{\"1\":{\"267\":10}}],[\"拿到所有图片路径\",{\"1\":{\"276\":1}}],[\"拿到能够代表整段文本或者整个多模态表示的\",{\"1\":{\"262\":1}}],[\"拿到图片背景区域特征图\",{\"1\":{\"59\":1}}],[\"池化\",{\"1\":{\"397\":1}}],[\"池化输出实现\",{\"1\":{\"262\":1}}],[\"池化输出\",{\"1\":{\"262\":1}}],[\"池化后拼接\",{\"1\":{\"54\":1}}],[\"另\",{\"1\":{\"600\":1}}],[\"另外\",{\"1\":{\"415\":1,\"420\":1,\"432\":1}}],[\"另外vilt还使用了whole\",{\"1\":{\"258\":1}}],[\"另外vilt还设计了一个word\",{\"1\":{\"258\":1}}],[\"另一条是第二个\",{\"1\":{\"654\":1}}],[\"另一方面在许多实际问题中\",{\"1\":{\"588\":1}}],[\"另一相关研究方向是使用自然语言指令训练模型以实现跨任务泛化\",{\"1\":{\"469\":1}}],[\"另一种思路是在转换后\",{\"1\":{\"293\":1}}],[\"另一种是基于\",{\"1\":{\"272\":1}}],[\"另一种是dual\",{\"1\":{\"256\":1}}],[\"另一种是同时使用完整的\",{\"1\":{\"191\":1}}],[\"另一个特例是当\",{\"1\":{\"582\":1}}],[\"另一个是作为答案结束的可能性\",{\"1\":{\"540\":1}}],[\"另一个是masked\",{\"1\":{\"258\":1}}],[\"另一个技巧\",{\"1\":{\"433\":1}}],[\"另一个维度很小\",{\"1\":{\"359\":1}}],[\"另一个原因是nlp模型可以利用从互联网上收集的大量文本\",{\"1\":{\"278\":1}}],[\"另一个细节\",{\"1\":{\"241\":1}}],[\"另一流派\",{\"1\":{\"242\":1}}],[\"另一类则采用独立的图像与文本编码器\",{\"1\":{\"150\":1}}],[\"另一部分特征是通过在当前分辨率直接对所有原始点应用单个pointnet得到的\",{\"1\":{\"97\":1}}],[\"嵌入向量生成过程图\",{\"1\":{\"523\":1}}],[\"嵌入和注意力层以\",{\"1\":{\"447\":1}}],[\"嵌入层维度\",{\"1\":{\"295\":1}}],[\"嵌入交互\",{\"1\":{\"292\":1}}],[\"嵌入的关系\",{\"1\":{\"292\":1}}],[\"嵌入序列的开头\",{\"1\":{\"292\":1}}],[\"嵌入中\",{\"1\":{\"292\":1}}],[\"嵌入维度\",{\"1\":{\"291\":1,\"292\":1}}],[\"嵌入\",{\"1\":{\"253\":1,\"292\":1}}],[\"摒弃了传统的目标检测和卷积视觉嵌入器\",{\"1\":{\"253\":1}}],[\"肯定得更新样本特征\",{\"1\":{\"242\":1}}],[\"放回到\",{\"1\":{\"242\":1}}],[\"放大难分类样本的影响\",{\"1\":{\"404\":1}}],[\"放大难分类样本影响\",{\"1\":{\"78\":1}}],[\"放大\",{\"1\":{\"78\":1,\"404\":1}}],[\"端到端的框架\",{\"1\":{\"242\":2}}],[\"了新知识\",{\"1\":{\"463\":1}}],[\"了它定义时的\",{\"1\":{\"366\":1}}],[\"了距离判断\",{\"1\":{\"359\":1}}],[\"了\",{\"1\":{\"241\":1}}],[\"伪代码里的\",{\"1\":{\"240\":1}}],[\"伪标签前五名\",{\"1\":{\"157\":1}}],[\"抽蓝球为\",{\"1\":{\"579\":1}}],[\"抽取式\",{\"1\":{\"542\":1}}],[\"抽取式问答\",{\"1\":{\"542\":1}}],[\"抽取等\",{\"1\":{\"470\":1}}],[\"抽样数量就是字典大小\",{\"1\":{\"240\":1}}],[\"抽象代数\",{\"1\":{\"463\":1}}],[\"抽象\",{\"1\":{\"107\":1,\"109\":1}}],[\"抽象点集或局部特征\",{\"1\":{\"86\":2}}],[\"很多简单的任务\",{\"1\":{\"430\":1}}],[\"很容易导致旧知识遗忘\",{\"1\":{\"424\":1}}],[\"很小\",{\"1\":{\"240\":1}}],[\"很大\",{\"1\":{\"240\":1,\"241\":1}}],[\"很难在这种情况下保持分类的一致性\",{\"1\":{\"112\":1}}],[\"希腊字母读音为\",{\"1\":{\"240\":1}}],[\"几乎不可能采样到接近灰色的图像\",{\"1\":{\"594\":1}}],[\"几百万个类别就太难了\",{\"1\":{\"240\":1}}],[\"几何属性\",{\"1\":{\"32\":1}}],[\"几何属性提取+意图类比\",{\"1\":{\"7\":1}}],[\"几何结构映射方法\",{\"1\":{\"49\":1}}],[\"几何结构信息与交互信息的融合\",{\"0\":{\"33\":1}}],[\"几何结构文本信息与点云信息进行融合\",{\"1\":{\"30\":1}}],[\"几何结构知识\",{\"1\":{\"28\":1}}],[\"几何结构推理\",{\"1\":{\"11\":1}}],[\"几何信息无法有效注入点云\",{\"1\":{\"24\":1}}],[\"几何推理\",{\"0\":{\"11\":1}}],[\"几何与意图利用不足\",{\"1\":{\"6\":1}}],[\"像moco和simclr有所不同\",{\"1\":{\"271\":1}}],[\"像\",{\"1\":{\"240\":1}}],[\"像素级别任务如分割\",{\"1\":{\"397\":1}}],[\"像素的图像块\",{\"1\":{\"300\":1}}],[\"像素的图块\",{\"1\":{\"207\":1}}],[\"像素\",{\"1\":{\"290\":2,\"300\":1}}],[\"像素回归的局限性\",{\"1\":{\"166\":1}}],[\"什么都不发生\",{\"1\":{\"566\":1}}],[\"什么样的优化目标是最高效的迁移\",{\"1\":{\"440\":1}}],[\"什么是prompt\",{\"0\":{\"430\":1}}],[\"什么是大模型\",{\"0\":{\"414\":1}}],[\"什么是闭包\",{\"0\":{\"366\":1}}],[\"什么是高阶函数\",{\"0\":{\"365\":1}}],[\"什么是负样本呢\",{\"1\":{\"238\":1}}],[\"什么是负样本\",{\"1\":{\"235\":1}}],[\"什么意思呢\",{\"1\":{\"240\":1}}],[\"看哪一个输出的值最大\",{\"1\":{\"508\":1}}],[\"看起\",{\"1\":{\"240\":1}}],[\"看不出区别\",{\"1\":{\"240\":1}}],[\"看不清细节\",{\"1\":{\"112\":1}}],[\"看不懂下面两行代码的话\",{\"1\":{\"92\":1}}],[\"趋于较大值时\",{\"1\":{\"586\":1}}],[\"趋于平滑\",{\"1\":{\"240\":1}}],[\"趋近于零\",{\"1\":{\"236\":1}}],[\"趋近于\",{\"1\":{\"236\":1}}],[\"越具体\",{\"1\":{\"432\":1}}],[\"越明确\",{\"1\":{\"432\":1}}],[\"越长的prompt\",{\"1\":{\"415\":1}}],[\"越接近黄色的位置代表越靠近位置编码的中心位置\",{\"1\":{\"293\":1}}],[\"越大时\",{\"1\":{\"593\":1}}],[\"越大\",{\"1\":{\"240\":1,\"405\":2,\"407\":1}}],[\"越大越好\",{\"1\":{\"401\":1,\"403\":1}}],[\"越大越\",{\"1\":{\"161\":1}}],[\"越尖锐\",{\"1\":{\"240\":1}}],[\"越小\",{\"1\":{\"240\":1,\"403\":1}}],[\"尖锐程度\",{\"1\":{\"240\":1}}],[\"温度不小于\",{\"1\":{\"566\":1}}],[\"温度小于\",{\"1\":{\"566\":1}}],[\"温度是\",{\"1\":{\"566\":1}}],[\"温度在\",{\"1\":{\"566\":4}}],[\"温度超参数\",{\"1\":{\"240\":1}}],[\"温度系数\",{\"1\":{\"240\":1}}],[\"温度参数\",{\"1\":{\"147\":1,\"240\":1,\"246\":1}}],[\"足够大的语言模型在多样化文本训练下\",{\"1\":{\"453\":1}}],[\"足够大\",{\"1\":{\"238\":2}}],[\"除boolq和winogrande\",{\"1\":{\"482\":1}}],[\"除非指定\",{\"1\":{\"447\":1}}],[\"除以可以缓解这个问题\",{\"1\":{\"318\":1}}],[\"除head\",{\"1\":{\"300\":1}}],[\"除了最后一个\",{\"1\":{\"285\":1}}],[\"除了模型本身的应用\",{\"1\":{\"274\":1}}],[\"除了个体判别这个任务外\",{\"1\":{\"239\":1}}],[\"除此之外的都是噪声样本\",{\"1\":{\"240\":1}}],[\"除此之外的特征都是使用之前的编码器得到的\",{\"1\":{\"238\":1}}],[\"除法运算y\",{\"1\":{\"660\":1}}],[\"除法运算\",{\"1\":{\"660\":1}}],[\"除法\",{\"1\":{\"160\":1}}],[\"刚才说了\",{\"1\":{\"238\":1}}],[\"刚性运动\",{\"0\":{\"116\":1},\"1\":{\"116\":3}}],[\"走一遍对比学习的流程\",{\"1\":{\"241\":1}}],[\"走了捷径\",{\"1\":{\"238\":1}}],[\"走的是e12这个编码器\",{\"1\":{\"238\":1}}],[\"锚点特征\",{\"1\":{\"240\":1}}],[\"锚点\",{\"1\":{\"238\":1,\"240\":1,\"241\":1}}],[\"检测为阳性\",{\"1\":{\"569\":2}}],[\"检测率为\",{\"1\":{\"344\":1}}],[\"检测等\",{\"1\":{\"237\":1}}],[\"检查\",{\"1\":{\"385\":1}}],[\"检查图像是否为\",{\"1\":{\"289\":1}}],[\"检查当前目录是否有预训练权重文件\",{\"1\":{\"275\":1,\"277\":1}}],[\"检索机制和索引管道等\",{\"1\":{\"684\":1}}],[\"检索能力\",{\"1\":{\"684\":1}}],[\"检索和使用外部数据可能引发伦理和隐私方面的问题\",{\"1\":{\"681\":1}}],[\"检索阶段\",{\"1\":{\"680\":1}}],[\"检索增强生成\",{\"0\":{\"679\":1},\"1\":{\"679\":1}}],[\"检索\",{\"1\":{\"181\":1,\"188\":1,\"680\":1}}],[\"检索等任务\",{\"1\":{\"112\":1}}],[\"叫做个体判别\",{\"1\":{\"235\":1}}],[\"巧妙构造出来\",{\"1\":{\"234\":1}}],[\"编造\",{\"1\":{\"542\":1}}],[\"编程和逻辑推理等任务中表现卓越\",{\"1\":{\"674\":1}}],[\"编程\",{\"1\":{\"231\":1}}],[\"编码能力异常强大\",{\"1\":{\"674\":1}}],[\"编码后可能是\",{\"1\":{\"542\":1}}],[\"编码后\",{\"1\":{\"540\":1}}],[\"编码后得到输出的结果\",{\"1\":{\"284\":1}}],[\"编码成\",{\"1\":{\"505\":1}}],[\"编码的冗余扩展\",{\"1\":{\"454\":1}}],[\"编码的输入文本\",{\"1\":{\"147\":1}}],[\"编码为图像特征\",{\"1\":{\"285\":1}}],[\"编码为两个特征\",{\"1\":{\"13\":1}}],[\"编码阶段\",{\"1\":{\"262\":1}}],[\"编码影响\",{\"1\":{\"238\":1}}],[\"编码文本输入\",{\"1\":{\"142\":1}}],[\"编码过程\",{\"1\":{\"99\":1}}],[\"编码器隐藏层输出\",{\"1\":{\"557\":1}}],[\"编码器层\",{\"1\":{\"553\":1}}],[\"编码器层堆叠\",{\"1\":{\"513\":1}}],[\"编码器层数\",{\"1\":{\"513\":1}}],[\"编码器输出的\",{\"1\":{\"285\":1}}],[\"编码器包含\",{\"1\":{\"171\":1}}],[\"编码器建模图文交互\",{\"1\":{\"150\":1}}],[\"编码器模型适合理解类任务但难以生成文本\",{\"1\":{\"122\":1}}],[\"编码器模型不适合文本生成任务\",{\"1\":{\"120\":1}}],[\"编码器部分\",{\"1\":{\"101\":1}}],[\"编码器\",{\"0\":{\"134\":1,\"263\":1},\"1\":{\"98\":2,\"99\":1,\"101\":1,\"120\":1,\"122\":1,\"548\":1}}],[\"编码器不同层级的点云特征\",{\"1\":{\"46\":1}}],[\"编码\",{\"1\":{\"98\":1,\"145\":1,\"238\":1,\"492\":1}}],[\"编码得到\",{\"1\":{\"72\":1}}],[\"编码点云特征\",{\"1\":{\"70\":1}}],[\"编码图像\",{\"1\":{\"59\":1}}],[\"编码推理结果\",{\"1\":{\"8\":1}}],[\"写作风格或特定领域知识\",{\"1\":{\"681\":1}}],[\"写入json文件\",{\"1\":{\"510\":1}}],[\"写好prompt\",{\"1\":{\"435\":1}}],[\"写故事\",{\"1\":{\"231\":1}}],[\"写一个关于猫的故事\",{\"1\":{\"224\":1}}],[\"唤醒\",{\"1\":{\"231\":1}}],[\"辨析\",{\"0\":{\"231\":1}}],[\"刷新\",{\"1\":{\"228\":1}}],[\"刷新该数据集\",{\"1\":{\"227\":1}}],[\"准确来说\",{\"1\":{\"652\":1}}],[\"准确性是指所有分类\",{\"1\":{\"343\":1}}],[\"准确率83\",{\"1\":{\"499\":1}}],[\"准确率低至72\",{\"1\":{\"484\":1}}],[\"准确率刷新sota\",{\"1\":{\"482\":1}}],[\"准确率达63\",{\"1\":{\"455\":1}}],[\"准确率达到\",{\"1\":{\"227\":1}}],[\"准确率进一步提升至63\",{\"1\":{\"455\":1}}],[\"准确率从19\",{\"1\":{\"455\":1}}],[\"准确率可以作为衡量模型质量的粗略指标\",{\"1\":{\"343\":1}}],[\"准确率衡量的是所有电子邮件正确分类所占的比例\",{\"1\":{\"343\":1}}],[\"准确率变化\",{\"1\":{\"229\":1}}],[\"准确率提升不大\",{\"1\":{\"228\":1}}],[\"准确率\",{\"0\":{\"343\":1},\"1\":{\"227\":1,\"347\":1,\"352\":1,\"408\":1,\"448\":1,\"455\":1}}],[\"准备调试\",{\"1\":{\"519\":1}}],[\"准备就绪\",{\"1\":{\"519\":1}}],[\"准备与下游任务相关的数据集\",{\"1\":{\"423\":1}}],[\"准备生成参数\",{\"1\":{\"286\":1}}],[\"准备阶段主要完成数据集加载\",{\"1\":{\"80\":1}}],[\"准备\",{\"0\":{\"80\":1},\"1\":{\"79\":1,\"285\":1,\"512\":1}}],[\"助手需要生成推理过程\",{\"1\":{\"227\":1}}],[\"助手\",{\"1\":{\"227\":1,\"674\":1}}],[\"你细品\",{\"1\":{\"635\":1}}],[\"你品\",{\"1\":{\"635\":1}}],[\"你会看到这个信息的可能性\",{\"1\":{\"596\":1}}],[\"你会得到\",{\"1\":{\"355\":1}}],[\"你在看到\",{\"1\":{\"596\":1}}],[\"你在没有观察任何信息前对\",{\"1\":{\"596\":1}}],[\"你在图片分类的时候很多照片是同一个类别\",{\"1\":{\"235\":1}}],[\"你观测到的信息\",{\"1\":{\"596\":1}}],[\"你总得允许\",{\"1\":{\"566\":1}}],[\"你有一个随机变量\",{\"1\":{\"565\":1}}],[\"你将得到一个包含多个元素的\",{\"1\":{\"540\":1}}],[\"你可能以为既然是围绕全灰图采样\",{\"1\":{\"594\":1}}],[\"你可能会觉得这里面有个问题\",{\"1\":{\"508\":1}}],[\"你可以把它理解成\",{\"1\":{\"566\":1}}],[\"你可以把它们理解成\",{\"1\":{\"566\":1}}],[\"你可以把马氏距离想成是\",{\"1\":{\"359\":1}}],[\"你可以使用以下命令查看你所有的\",{\"1\":{\"334\":1}}],[\"你要在\",{\"1\":{\"397\":1}}],[\"你就可以把它们两两之间的协方差\",{\"1\":{\"355\":1}}],[\"你的环境名\",{\"1\":{\"338\":1}}],[\"你的终端提示符通常会显示当前环境的名字\",{\"1\":{\"332\":1}}],[\"你需要为每个选项分别构造一个完整的\",{\"1\":{\"544\":1}}],[\"你需要跳过\",{\"1\":{\"323\":1}}],[\"你需要在内存中跳过\",{\"1\":{\"323\":1}}],[\"你是一个视觉助手\",{\"1\":{\"227\":1}}],[\"你知道吗\",{\"1\":{\"111\":1}}],[\"理解微妙提示方面表现更出色\",{\"1\":{\"674\":1}}],[\"理解反面\",{\"1\":{\"448\":1}}],[\"理解为\",{\"1\":{\"326\":1}}],[\"理解并回答科学类问题\",{\"1\":{\"227\":1}}],[\"理想情况\",{\"1\":{\"280\":1}}],[\"理论支持\",{\"1\":{\"395\":1}}],[\"理论结果\",{\"1\":{\"395\":1}}],[\"理论限制的相似性\",{\"1\":{\"395\":1}}],[\"理论上的限制\",{\"1\":{\"112\":1}}],[\"理论上证明\",{\"1\":{\"105\":1}}],[\"理论分析保证模型鲁棒性\",{\"1\":{\"105\":1}}],[\"序列长度\",{\"1\":{\"493\":1,\"494\":1}}],[\"序列\",{\"1\":{\"226\":1,\"285\":1,\"445\":1}}],[\"序列为文本\",{\"1\":{\"143\":1}}],[\"期望平方距离为\",{\"1\":{\"593\":1}}],[\"期望模型能够学习到文本和图像之间的匹配关系\",{\"1\":{\"271\":1}}],[\"期望的回答\",{\"1\":{\"226\":1,\"227\":1}}],[\"期望输出\",{\"1\":{\"224\":1}}],[\"请把这部分原文告诉我\",{\"1\":{\"542\":1}}],[\"请保持尊重\",{\"1\":{\"471\":1}}],[\"请尊重\",{\"1\":{\"469\":1}}],[\"请取消下面这行注释\",{\"1\":{\"405\":1}}],[\"请考虑下图中的点\",{\"1\":{\"353\":1}}],[\"请使用此方法\",{\"1\":{\"347\":1}}],[\"请仅与其他指标搭配使用\",{\"1\":{\"347\":1}}],[\"请注意\",{\"1\":{\"342\":1,\"474\":1}}],[\"请阅读源码进行学习\",{\"1\":{\"261\":1}}],[\"请描述这张图片\",{\"1\":{\"226\":1}}],[\"请求\",{\"1\":{\"224\":1}}],[\"条件概率衡量的是\",{\"1\":{\"568\":1}}],[\"条件概率\",{\"0\":{\"568\":1},\"1\":{\"443\":1}}],[\"条\",{\"1\":{\"227\":3}}],[\"条图文对\",{\"1\":{\"227\":1}}],[\"条高质量图文对\",{\"1\":{\"226\":1}}],[\"条描述\",{\"1\":{\"226\":1}}],[\"名标注者\",{\"1\":{\"470\":1}}],[\"名词等\",{\"1\":{\"455\":1}}],[\"名词\",{\"1\":{\"355\":1}}],[\"名词短语过滤\",{\"1\":{\"226\":1}}],[\"名称的长版本\",{\"1\":{\"350\":1}}],[\"名称\",{\"1\":{\"82\":1,\"227\":1}}],[\"复合运算的验证\",{\"1\":{\"660\":1}}],[\"复合函数的计算图展示了函数的组合过程\",{\"1\":{\"618\":1}}],[\"复合函数的计算\",{\"0\":{\"616\":1}}],[\"复杂函数可视化示例\",{\"1\":{\"666\":1}}],[\"复杂函数的求导\",{\"0\":{\"662\":1}}],[\"复杂计算图处理\",{\"1\":{\"662\":1}}],[\"复杂\",{\"1\":{\"566\":1}}],[\"复杂指令处理能力不足\",{\"1\":{\"472\":1}}],[\"复杂推理型\",{\"1\":{\"227\":1}}],[\"复杂推理能力\",{\"1\":{\"225\":1}}],[\"复制\",{\"1\":{\"372\":1}}],[\"复用同一行数据\",{\"1\":{\"327\":1}}],[\"复用缓存的视觉信息\",{\"1\":{\"285\":1}}],[\"复现\",{\"0\":{\"83\":1}}],[\"响应速度更快\",{\"1\":{\"674\":1}}],[\"响应速度比\",{\"1\":{\"674\":1}}],[\"响应\",{\"1\":{\"231\":2}}],[\"响应数据对模型进行微调\",{\"1\":{\"224\":1}}],[\"响应数据集\",{\"1\":{\"224\":1}}],[\"响应对\",{\"1\":{\"224\":1}}],[\"响应格式与\",{\"1\":{\"219\":1}}],[\"记忆机制\",{\"1\":{\"684\":1}}],[\"记忆\",{\"1\":{\"683\":1}}],[\"记忆与泛化分析\",{\"1\":{\"455\":1}}],[\"记作\",{\"1\":{\"565\":2,\"569\":2,\"577\":1}}],[\"记住用户之前的交互习惯和偏好\",{\"1\":{\"674\":1}}],[\"记住了\",{\"1\":{\"366\":1,\"367\":1}}],[\"记住\",{\"1\":{\"224\":1,\"366\":1}}],[\"记录\",{\"1\":{\"687\":1}}],[\"记录创建该变量的函数\",{\"1\":{\"634\":1}}],[\"记录input后\",{\"1\":{\"630\":1}}],[\"记录输入长度\",{\"1\":{\"520\":1}}],[\"记录被掩码的原token\",{\"1\":{\"511\":1}}],[\"记录被掩码的位置\",{\"1\":{\"511\":1,\"513\":1}}],[\"记录当前词对应输入词序列中的索引\",{\"1\":{\"477\":1}}],[\"记录该子词对的全局频率\",{\"1\":{\"410\":1}}],[\"记录该类别的样本数量\",{\"1\":{\"289\":1}}],[\"记录历史合并的最高频子词对及其频率\",{\"1\":{\"410\":1}}],[\"记录每个词的出现次数的词典\",{\"1\":{\"410\":1}}],[\"记录每个词的出现次数\",{\"1\":{\"410\":1}}],[\"记录每个物体对应的点云索引下标区间\",{\"1\":{\"29\":1}}],[\"记录每类物体对应的点云文件下标索引区间\",{\"1\":{\"58\":1}}],[\"记录物体边界框\",{\"1\":{\"58\":1}}],[\"创新的动态高分辨率处理策略\",{\"1\":{\"222\":1}}],[\"创建的\",{\"1\":{\"684\":1}}],[\"创建配置类config\",{\"1\":{\"658\":1}}],[\"创建变量的函数\",{\"0\":{\"610\":1}}],[\"创建用以区分special\",{\"1\":{\"520\":1}}],[\"创建句子辨识列表\",{\"1\":{\"520\":1}}],[\"创建反向映射\",{\"1\":{\"511\":1}}],[\"创建新的线性层\",{\"1\":{\"513\":1}}],[\"创建新张量\",{\"1\":{\"387\":1}}],[\"创建新环境\",{\"0\":{\"331\":1}}],[\"创建环境并安装一些常用包\",{\"1\":{\"331\":1}}],[\"创建环境时指定\",{\"1\":{\"331\":1}}],[\"创建了一个一维张量\",{\"1\":{\"323\":1}}],[\"创建一个空环境\",{\"1\":{\"331\":1}}],[\"创建一个连续的转置副本\",{\"1\":{\"326\":1}}],[\"创建一个张量\",{\"1\":{\"325\":1,\"326\":2}}],[\"创建一个\",{\"1\":{\"323\":1}}],[\"创建一个全零点作为\",{\"1\":{\"92\":1}}],[\"创建预输出层\",{\"1\":{\"296\":1}}],[\"创建归一化层\",{\"1\":{\"296\":1}}],[\"创建encoder\",{\"1\":{\"296\":1}}],[\"创建\",{\"1\":{\"294\":1}}],[\"创建丢弃层\",{\"1\":{\"293\":1,\"296\":1}}],[\"创建可学习的位置嵌入\",{\"1\":{\"293\":1,\"296\":1}}],[\"创建可学习的分类标记\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"创建可视化窗口\",{\"1\":{\"83\":1}}],[\"创建图像块嵌入层\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"创建图像注意力掩码\",{\"1\":{\"286\":1}}],[\"创建文本编码器\",{\"1\":{\"147\":1}}],[\"创建主视觉编码器\",{\"1\":{\"147\":1}}],[\"创建动量编码器\",{\"1\":{\"145\":1}}],[\"聊天版语言模型\",{\"1\":{\"219\":1}}],[\"聊天版本\",{\"1\":{\"214\":1}}],[\"缩小了与商业模型的差距\",{\"1\":{\"217\":1}}],[\"缩放点积注意力\",{\"1\":{\"266\":1,\"305\":1}}],[\"缩放\",{\"1\":{\"116\":1,\"117\":1,\"454\":1}}],[\"缩放因子\",{\"1\":{\"45\":1,\"295\":1}}],[\"额外的提升\",{\"1\":{\"449\":1}}],[\"额外添加448×448缩略图以保留全局上下文\",{\"1\":{\"216\":1}}],[\"额外通道数\",{\"1\":{\"46\":1}}],[\"阿里云开源了\",{\"1\":{\"674\":1}}],[\"阿里\",{\"1\":{\"210\":1}}],[\"商业模型在规模和性能上领先\",{\"1\":{\"212\":1}}],[\"商业模型在多模态领域占据领先地位\",{\"1\":{\"210\":1}}],[\"商业模型通过多语言数据集训练\",{\"1\":{\"208\":1}}],[\"商业模型通常具有超过1000亿参数\",{\"1\":{\"208\":1}}],[\"商业模型支持动态分辨率以保留原始宽高比\",{\"1\":{\"208\":1}}],[\"系数范围是\",{\"1\":{\"401\":1}}],[\"系数衡量的是预测掩码与真实标签之间的相似性\",{\"1\":{\"401\":1}}],[\"系数\",{\"1\":{\"401\":2,\"405\":2}}],[\"系统2是慢思考系统\",{\"1\":{\"433\":1}}],[\"系统2\",{\"1\":{\"433\":1}}],[\"系统1是快思考系统\",{\"1\":{\"433\":1}}],[\"系统1\",{\"1\":{\"433\":1}}],[\"系统提示语\",{\"1\":{\"227\":1}}],[\"系统的进步\",{\"1\":{\"181\":1}}],[\"系列均采用\",{\"1\":{\"674\":1}}],[\"系列工作\",{\"1\":{\"674\":1}}],[\"系列基础模型\",{\"1\":{\"674\":1}}],[\"系列基本上是后续大模型的标杆\",{\"1\":{\"674\":1}}],[\"系列在开源社区的影响力和应用前景\",{\"1\":{\"674\":1}}],[\"系列相同\",{\"1\":{\"674\":1}}],[\"系列的三个版本\",{\"1\":{\"674\":1}}],[\"系列模型是\",{\"1\":{\"674\":1}}],[\"系列模型是由\",{\"1\":{\"674\":1}}],[\"系列模型\",{\"1\":{\"674\":2}}],[\"系列语言大模型由\",{\"1\":{\"674\":1}}],[\"系列已形成\",{\"1\":{\"674\":1}}],[\"系列\",{\"1\":{\"210\":1,\"211\":1,\"464\":1,\"673\":1,\"674\":3}}],[\"系列等\",{\"1\":{\"207\":1}}],[\"项自然语言理解任务\",{\"1\":{\"494\":1}}],[\"项对训练稳定性影响较大\",{\"1\":{\"494\":1}}],[\"项目模块化结构\",{\"0\":{\"661\":1}}],[\"项目\",{\"1\":{\"267\":1,\"472\":1}}],[\"项目的数据组织中\",{\"1\":{\"29\":1}}],[\"项\",{\"1\":{\"240\":1,\"395\":1}}],[\"项达到领先水平\",{\"1\":{\"207\":1}}],[\"精细的指令遵循\",{\"1\":{\"674\":1}}],[\"精准度高\",{\"1\":{\"674\":1}}],[\"精度丢失\",{\"1\":{\"623\":1}}],[\"精度越高意味着分布越\",{\"1\":{\"584\":1}}],[\"精度受限\",{\"1\":{\"114\":1}}],[\"精确匹配准确率4\",{\"1\":{\"455\":1}}],[\"精确率与召回率曲线\",{\"0\":{\"352\":1},\"1\":{\"352\":1}}],[\"精确率和召回率通常呈现反向关系\",{\"1\":{\"346\":1}}],[\"精确率会提高\",{\"1\":{\"346\":1}}],[\"精确率作为指标的意义和实用性较低\",{\"1\":{\"346\":1}}],[\"精确率衡量的是被归类为垃圾邮件且实际上是垃圾邮件的电子邮件所占的比例\",{\"1\":{\"346\":1}}],[\"精确率是指模型所有正类别分类中实际为正类别的分类所占的比例\",{\"1\":{\"346\":1}}],[\"精确率\",{\"0\":{\"346\":1},\"1\":{\"347\":1,\"352\":1}}],[\"精心构建了一个涵盖常见场景和文档图像的双语数据集\",{\"1\":{\"207\":1}}],[\"高可用的托管解决方案\",{\"1\":{\"685\":1}}],[\"高计算资源需求\",{\"1\":{\"675\":1}}],[\"高次运算\",{\"1\":{\"662\":1}}],[\"高于ndarray的优先级\",{\"1\":{\"660\":1}}],[\"高于性别化代词\",{\"1\":{\"482\":1}}],[\"高赞网页内容\",{\"1\":{\"494\":1}}],[\"高质量完成\",{\"1\":{\"472\":1}}],[\"高质量数据被重复使用\",{\"1\":{\"461\":1}}],[\"高质量双语数据集\",{\"1\":{\"207\":1,\"208\":1}}],[\"高容量模型通过最大化文本序列的似然估计\",{\"1\":{\"457\":1}}],[\"高效\",{\"1\":{\"665\":1}}],[\"高效的模型微调\",{\"1\":{\"428\":1}}],[\"高效广播\",{\"1\":{\"387\":1}}],[\"高置信度\",{\"1\":{\"404\":1}}],[\"高维输入时\",{\"1\":{\"395\":1}}],[\"高阶导数构建\",{\"1\":{\"665\":1}}],[\"高阶导数\",{\"0\":{\"665\":1,\"668\":1}}],[\"高阶导数与深度学习优化进阶\",{\"1\":{\"664\":1}}],[\"高阶项导致数值不稳定\",{\"1\":{\"395\":1}}],[\"高阶函数\",{\"1\":{\"365\":1,\"367\":3,\"368\":1}}],[\"高斯肥皂泡现象\",{\"1\":{\"592\":1}}],[\"高斯壳\",{\"0\":{\"591\":1}}],[\"高斯联合分布\",{\"0\":{\"588\":1}}],[\"高斯分布是熵最大的分布\",{\"1\":{\"588\":1}}],[\"高斯分布作为近似是相当合理的\",{\"1\":{\"588\":1}}],[\"高斯分布的\",{\"1\":{\"591\":1}}],[\"高斯分布的一个问题在于它对离群点非常敏感\",{\"1\":{\"586\":1}}],[\"高斯分布的累积分布函数\",{\"1\":{\"584\":1}}],[\"高斯分布的概率密度函数\",{\"1\":{\"584\":1}}],[\"高斯分布\",{\"0\":{\"584\":1}}],[\"高斯过程\",{\"1\":{\"355\":2}}],[\"高斯混合模型\",{\"1\":{\"355\":1}}],[\"高分辨率\",{\"1\":{\"221\":1}}],[\"高分辨率优化方法\",{\"1\":{\"211\":1}}],[\"高分辨率用于文档分析\",{\"1\":{\"208\":1}}],[\"高\",{\"1\":{\"208\":1,\"280\":1,\"404\":1}}],[\"高出\",{\"1\":{\"23\":2}}],[\"轮的部分计算\",{\"1\":{\"474\":1}}],[\"轮推理时必然包含了第\",{\"1\":{\"474\":1}}],[\"轮输入数据新增了一个\",{\"1\":{\"474\":1}}],[\"轮输入数据只比第\",{\"1\":{\"474\":1}}],[\"轮训练就足够了\",{\"1\":{\"447\":1}}],[\"轮热身\",{\"1\":{\"204\":1}}],[\"轮\",{\"1\":{\"202\":1,\"203\":1,\"204\":1,\"447\":1}}],[\"轮次\",{\"1\":{\"174\":1}}],[\"各种情形本身的概率\",{\"1\":{\"569\":1}}],[\"各需要一个\",{\"1\":{\"553\":1,\"556\":1}}],[\"各类别的权重\",{\"1\":{\"401\":1}}],[\"各训练\",{\"1\":{\"202\":1}}],[\"各方法差别不大\",{\"1\":{\"23\":1}}],[\"峰值学习率\",{\"1\":{\"201\":1,\"204\":1}}],[\"张量简介\",{\"1\":{\"328\":1}}],[\"张量变换操作\",{\"0\":{\"324\":1}}],[\"张量可以被视为一种广义的矩阵\",{\"1\":{\"321\":1}}],[\"张量\",{\"1\":{\"321\":1,\"323\":1,\"382\":1}}],[\"张\",{\"1\":{\"200\":1,\"201\":1}}],[\"率\",{\"1\":{\"200\":1,\"203\":1}}],[\"β=0\",{\"1\":{\"405\":2}}],[\"β>α\",{\"1\":{\"405\":1}}],[\"β\",{\"1\":{\"405\":3,\"407\":3,\"470\":1}}],[\"β2=0\",{\"1\":{\"200\":1,\"201\":1}}],[\"β1=0\",{\"1\":{\"200\":1,\"201\":1}}],[\"借助于海量无标注数据的训练\",{\"1\":{\"677\":1}}],[\"借助\",{\"1\":{\"674\":1,\"684\":1}}],[\"借助字典映射为word\",{\"1\":{\"520\":1}}],[\"借助对比学习机制\",{\"1\":{\"271\":1}}],[\"借助海量图文数据和多阶段训练策略\",{\"1\":{\"198\":1}}],[\"借助取反激活图片背景区域\",{\"1\":{\"59\":1}}],[\"发挥大模型为核心的大模型开发与传统的\",{\"1\":{\"686\":1}}],[\"发生的可能性有多大\",{\"1\":{\"568\":1}}],[\"发生\",{\"1\":{\"567\":1}}],[\"发布并开源了\",{\"1\":{\"674\":2}}],[\"发布\",{\"1\":{\"674\":7}}],[\"发布的\",{\"1\":{\"674\":7}}],[\"发布模型\",{\"1\":{\"500\":1}}],[\"发布了全新升级的\",{\"1\":{\"674\":1}}],[\"发布了初始版本\",{\"1\":{\"674\":1}}],[\"发布了基于\",{\"1\":{\"674\":1}}],[\"发布了\",{\"1\":{\"138\":1,\"674\":10}}],[\"发现潜在bug\",{\"1\":{\"647\":1}}],[\"发现\",{\"1\":{\"494\":1}}],[\"发现无论是在训练损失还是实际任务中的表现\",{\"1\":{\"462\":1}}],[\"发现使用\",{\"1\":{\"196\":1}}],[\"宽度\",{\"1\":{\"196\":1,\"395\":1}}],[\"宽度3200\",{\"1\":{\"189\":1}}],[\"兼容性与支持\",{\"1\":{\"684\":1}}],[\"兼容性\",{\"1\":{\"454\":1}}],[\"兼容大多数分类模型\",{\"1\":{\"404\":1}}],[\"兼具理解和生成能力\",{\"1\":{\"195\":1}}],[\"兼顾了对\",{\"1\":{\"684\":1}}],[\"兼顾了效率与泛化能力\",{\"1\":{\"150\":1}}],[\"兼顾像素级精度和区域重叠度\",{\"1\":{\"407\":1}}],[\"兼顾这两者\",{\"1\":{\"347\":1}}],[\"兼顾理解与生成能力\",{\"1\":{\"120\":1}}],[\"超轻量推理\",{\"1\":{\"674\":1}}],[\"超强推理能力\",{\"1\":{\"674\":1}}],[\"超长截断\",{\"1\":{\"520\":1}}],[\"超大规模\",{\"1\":{\"463\":1}}],[\"超大规模语言模型可以显著减少对任务特定数据的需求\",{\"1\":{\"460\":1}}],[\"超参数调整\",{\"1\":{\"408\":1}}],[\"超参数设置说明\",{\"1\":{\"407\":1}}],[\"超参数为\",{\"1\":{\"200\":1}}],[\"超级大的多分类问题\",{\"1\":{\"240\":1}}],[\"超越xlnet\",{\"1\":{\"498\":1,\"499\":1}}],[\"超越flan\",{\"1\":{\"482\":1,\"483\":1}}],[\"超越chinchilla\",{\"1\":{\"482\":1}}],[\"超越了一些基于检索系统的模型\",{\"1\":{\"462\":1}}],[\"超越gpt\",{\"1\":{\"220\":1}}],[\"超越如\",{\"1\":{\"194\":1}}],[\"超过了随机水平\",{\"1\":{\"676\":1}}],[\"超过了如\",{\"1\":{\"193\":1}}],[\"超过部分无监督方法\",{\"1\":{\"455\":1}}],[\"超过此前需依赖上下文词约束的sota方法\",{\"1\":{\"455\":1}}],[\"超过历史最佳\",{\"1\":{\"82\":1}}],[\"零\",{\"0\":{\"503\":1,\"509\":1},\"1\":{\"503\":1,\"509\":1}}],[\"零样本性能\",{\"1\":{\"482\":1}}],[\"零样本学习\",{\"1\":{\"461\":1}}],[\"零样本表现\",{\"1\":{\"449\":1}}],[\"零样本图像字幕生成\",{\"1\":{\"194\":1}}],[\"零样本图像\",{\"1\":{\"194\":1}}],[\"零样本图像分类\",{\"1\":{\"194\":1}}],[\"零拷贝\",{\"1\":{\"325\":1,\"326\":1}}],[\"零计算开销的\",{\"1\":{\"156\":1}}],[\"达到55\",{\"1\":{\"455\":1}}],[\"达到非常低的\",{\"1\":{\"447\":1}}],[\"达到四两拨千斤的效果\",{\"1\":{\"428\":1}}],[\"达到了\",{\"1\":{\"227\":1}}],[\"达到\",{\"1\":{\"193\":1,\"195\":1,\"470\":1}}],[\"达成最新最优性能\",{\"1\":{\"23\":1}}],[\"占比60\",{\"1\":{\"461\":1}}],[\"占据了特别大的内存资源和计算资源\",{\"1\":{\"423\":1}}],[\"占\",{\"1\":{\"191\":1}}],[\"亿用户的增长\",{\"1\":{\"674\":1}}],[\"亿的大语言模型\",{\"1\":{\"674\":1}}],[\"亿\",{\"1\":{\"673\":2}}],[\"亿个语句的数据合集\",{\"1\":{\"510\":1}}],[\"亿张图像\",{\"1\":{\"300\":1}}],[\"亿时\",{\"1\":{\"239\":1}}],[\"亿样本\",{\"1\":{\"200\":2}}],[\"亿参数的\",{\"1\":{\"673\":2}}],[\"亿参数规模\",{\"1\":{\"198\":1}}],[\"亿参数\",{\"1\":{\"191\":1,\"674\":1}}],[\"亿图像\",{\"1\":{\"131\":1}}],[\"继续上述例子\",{\"1\":{\"565\":1}}],[\"继续训练的效果不如重新训练\",{\"1\":{\"137\":1}}],[\"继承第一阶段权重\",{\"1\":{\"201\":1}}],[\"继承了经过第一阶段对比训练后得到的\",{\"1\":{\"191\":1}}],[\"继承了第一阶段中学习到的权重\",{\"1\":{\"191\":1}}],[\"此例中\",{\"1\":{\"660\":1}}],[\"此指标可平衡精确率和召回率的重要性\",{\"1\":{\"348\":1}}],[\"此处将行维度从\",{\"1\":{\"327\":1}}],[\"此时离目的地更近\",{\"1\":{\"667\":1}}],[\"此时需交换操作数顺序并调用sub类\",{\"1\":{\"660\":1}}],[\"此时a\",{\"1\":{\"657\":1}}],[\"此时我们可以通过辈分来确保函数b和c先于函数a取出\",{\"1\":{\"656\":1}}],[\"此时我们还要训练两个\",{\"1\":{\"508\":1}}],[\"此时的tinypytorch已经具备了自动微分的能力\",{\"1\":{\"648\":1}}],[\"此时影响因子r\",{\"1\":{\"626\":1}}],[\"此时r=\",{\"1\":{\"626\":1}}],[\"此时为空\",{\"1\":{\"477\":1}}],[\"此时必须先\",{\"1\":{\"384\":1}}],[\"此时马氏距离变为\",{\"1\":{\"359\":1}}],[\"此时先用text\",{\"1\":{\"285\":1}}],[\"此时\",{\"1\":{\"240\":1,\"244\":1,\"565\":1,\"660\":1}}],[\"此前的研究\",{\"1\":{\"460\":1}}],[\"此前\",{\"1\":{\"224\":1}}],[\"此阶段有两种不同的配置方式\",{\"1\":{\"191\":1}}],[\"此阶段中\",{\"1\":{\"191\":1}}],[\"此外\",{\"1\":{\"5\":1,\"26\":1,\"48\":1,\"166\":1,\"170\":1,\"183\":1,\"195\":1,\"215\":1,\"253\":1,\"273\":1,\"274\":1,\"278\":1,\"385\":1,\"392\":1,\"428\":1,\"462\":1,\"464\":1,\"467\":1,\"468\":1,\"469\":1,\"492\":1,\"678\":1}}],[\"冻结预训练模型参数\",{\"1\":{\"423\":1}}],[\"冻结参数的image\",{\"1\":{\"282\":1}}],[\"冻结主干网络\",{\"1\":{\"201\":1}}],[\"冻结\",{\"1\":{\"190\":1}}],[\"初步版本包含约800万篇文档\",{\"1\":{\"454\":1}}],[\"初步对齐视觉编码器\",{\"1\":{\"190\":1}}],[\"初期在\",{\"1\":{\"200\":1}}],[\"初始输入\",{\"1\":{\"474\":1}}],[\"初始步幅\",{\"1\":{\"327\":1}}],[\"初始时没有任何语义信息\",{\"1\":{\"292\":1}}],[\"初始时随机选择一个点作为第一个中心点\",{\"1\":{\"92\":1}}],[\"初始文本token\",{\"1\":{\"286\":1}}],[\"初始为\",{\"1\":{\"286\":1}}],[\"初始阶段在较低分辨率\",{\"1\":{\"190\":1}}],[\"初始假设变换为恒等变换\",{\"1\":{\"107\":1}}],[\"初始设为一个极大值\",{\"1\":{\"92\":1}}],[\"初始化运算符重载\",{\"1\":{\"661\":1}}],[\"初始化一个简化版的\",{\"1\":{\"513\":1}}],[\"初始化优化\",{\"1\":{\"454\":1}}],[\"初始化优化器\",{\"1\":{\"142\":1}}],[\"初始化为\",{\"1\":{\"426\":1,\"656\":1}}],[\"初始化两个低秩矩阵\",{\"1\":{\"423\":1}}],[\"初始化低秩矩阵\",{\"1\":{\"423\":1}}],[\"初始化自定义数据集类\",{\"1\":{\"289\":1}}],[\"初始化query\",{\"1\":{\"282\":1}}],[\"初始化的语言中间件\",{\"1\":{\"198\":1}}],[\"初始化队列特征为单位向量\",{\"1\":{\"160\":1}}],[\"初始化队列为单位向量\",{\"1\":{\"147\":1}}],[\"初始化负样本队列\",{\"1\":{\"160\":1}}],[\"初始化动量编码器参数\",{\"1\":{\"160\":1}}],[\"初始化时直接复制参数\",{\"1\":{\"147\":1}}],[\"初始化方法\",{\"1\":{\"147\":1}}],[\"初始化对比学习的负样本队列\",{\"1\":{\"145\":1}}],[\"初始化文本输入\",{\"1\":{\"286\":1}}],[\"初始化文本编码器和分词器\",{\"1\":{\"145\":1}}],[\"初始化文本解码器\",{\"1\":{\"142\":1}}],[\"初始化文本分词器\",{\"1\":{\"142\":1}}],[\"初始化视觉编码器\",{\"1\":{\"142\":1,\"145\":1,\"160\":1}}],[\"初始化函数\",{\"1\":{\"100\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1}}],[\"初始化pointrefer模型\",{\"1\":{\"83\":1}}],[\"初始化\",{\"1\":{\"82\":1,\"101\":1,\"142\":1,\"160\":1,\"291\":1,\"292\":1,\"426\":5,\"477\":1}}],[\"初始化损失函数\",{\"1\":{\"80\":1}}],[\"初始化模型和优化器\",{\"1\":{\"142\":1}}],[\"初始化模型\",{\"1\":{\"80\":1}}],[\"初始化用于存储每个样本拼接后输入和\",{\"1\":{\"43\":1}}],[\"瑞士军刀\",{\"1\":{\"189\":1}}],[\"新的文本\",{\"1\":{\"542\":1}}],[\"新的问题出现了\",{\"1\":{\"278\":1}}],[\"新闻数据\",{\"1\":{\"494\":1}}],[\"新词使用\",{\"1\":{\"462\":1}}],[\"新加入模块约为\",{\"1\":{\"191\":1}}],[\"新增名称属性\",{\"1\":{\"659\":1}}],[\"新增的可学习查询和交叉注意力层\",{\"1\":{\"201\":1}}],[\"新增模块\",{\"1\":{\"191\":1}}],[\"新增了96个可学习查询和交叉注意力层\",{\"1\":{\"189\":1}}],[\"新增96个可学习查询和交叉注意力层\",{\"1\":{\"188\":1}}],[\"新颖性与多样性\",{\"1\":{\"133\":1}}],[\"详解\",{\"0\":{\"473\":1}}],[\"详见附录\",{\"1\":{\"472\":1}}],[\"详见表\",{\"1\":{\"461\":1}}],[\"详见表1\",{\"1\":{\"188\":1,\"480\":1}}],[\"详细参数配置见表2\",{\"1\":{\"481\":1}}],[\"详细描述型\",{\"1\":{\"227\":1}}],[\"详细描述\",{\"1\":{\"226\":2}}],[\"详细描述交互\",{\"1\":{\"28\":1}}],[\"详细训练设置\",{\"0\":{\"199\":1}}],[\"详细解释\",{\"1\":{\"76\":1,\"542\":1}}],[\"详细交互行为\",{\"1\":{\"28\":1}}],[\"填充过程图\",{\"1\":{\"520\":1}}],[\"填充token对应0\",{\"1\":{\"520\":1}}],[\"填充符\",{\"1\":{\"286\":1}}],[\"填充至最长序列长度\",{\"1\":{\"40\":1}}],[\"填补了这一领域的空白\",{\"1\":{\"186\":1}}],[\"近似值\",{\"1\":{\"662\":1}}],[\"近似于\",{\"1\":{\"403\":1}}],[\"近期研究考虑过各种各样的目标\",{\"1\":{\"440\":1}}],[\"近期研究通过将视觉模型与llms结合\",{\"1\":{\"185\":1}}],[\"近代自然语言处理技术发展的\",{\"1\":{\"421\":1}}],[\"近年来\",{\"1\":{\"278\":1,\"453\":1,\"454\":1,\"460\":1}}],[\"近年来的在线蒸馏则使用多个同时训练的模型进行知识迁移\",{\"1\":{\"150\":1}}],[\"近年来生成模型被用于文本任务的样本合成\",{\"1\":{\"124\":1}}],[\"远优于此前结果\",{\"1\":{\"462\":1}}],[\"远低于监督系统30\",{\"1\":{\"455\":1}}],[\"远低于解耦时的\",{\"1\":{\"134\":1}}],[\"远小于原始矩阵的维度\",{\"1\":{\"423\":1}}],[\"远\",{\"1\":{\"359\":1}}],[\"远落后于llms的规模\",{\"1\":{\"183\":1}}],[\"然而在某些情况下\",{\"1\":{\"679\":1}}],[\"然而大规模的未标注的文本语料是丰富\",{\"1\":{\"439\":1}}],[\"然而\",{\"1\":{\"183\":1,\"184\":1,\"208\":1,\"215\":1,\"272\":1,\"278\":3,\"453\":1,\"460\":2,\"568\":1,\"591\":1,\"673\":1,\"676\":1}}],[\"然后延展设计核心功能的上下游功能\",{\"1\":{\"687\":1}}],[\"然后从中选出最合适的那个\",{\"1\":{\"544\":1}}],[\"然后启动\",{\"1\":{\"519\":1}}],[\"然后给出一个假设\",{\"1\":{\"508\":1}}],[\"然后让模型通过上下文预测那一个被遮盖或替换的部分\",{\"1\":{\"505\":1}}],[\"然后让模型去学习特征\",{\"1\":{\"238\":1}}],[\"然后切分成query\",{\"1\":{\"477\":1}}],[\"然后作为下一次推理的输入\",{\"1\":{\"474\":1}}],[\"然后能成功迁移学习解决判别式任务\",{\"1\":{\"450\":1}}],[\"然后累积计算每个误差对iou得分的影响\",{\"1\":{\"406\":1}}],[\"然后损失就是\",{\"1\":{\"405\":1}}],[\"然后打包成一个group\",{\"1\":{\"328\":1}}],[\"然后再把新的问题让小孩子来解决\",{\"1\":{\"434\":1}}],[\"然后再经过一层\",{\"1\":{\"299\":1}}],[\"然后再拿到其它数据集上做迁移学习\",{\"1\":{\"297\":1}}],[\"然后输入到mlp\",{\"1\":{\"296\":1}}],[\"然后调整形状并重新排列维度\",{\"1\":{\"295\":1}}],[\"然后将tf模型转为对应的pytorch版本即可\",{\"1\":{\"519\":1}}],[\"然后将该位置的\",{\"1\":{\"508\":1}}],[\"然后将所有的\",{\"1\":{\"507\":1}}],[\"然后将\",{\"1\":{\"299\":1,\"350\":1}}],[\"然后将特征图的最后两维展平为一维\",{\"1\":{\"291\":1}}],[\"然后将这个骨干网络参数冻结\",{\"1\":{\"237\":1}}],[\"然后将这些不同尺度的特征拼接在一起\",{\"1\":{\"96\":1}}],[\"然后图像的表征和\",{\"1\":{\"286\":1}}],[\"然后在每项具体任务上判别性微调discriminative\",{\"1\":{\"439\":1}}],[\"然后在训练过程中\",{\"1\":{\"293\":1}}],[\"然后在具体的下游任务上进行微调\",{\"1\":{\"278\":1}}],[\"然后在后续会进行如下处理\",{\"1\":{\"76\":1}}],[\"然后计算出和当前文本描述相似度最高的那副图片\",{\"1\":{\"276\":1}}],[\"然后计算每个分类文本对应的文本嵌入向量\",{\"1\":{\"275\":1}}],[\"然后利用模型获取文本特征\",{\"1\":{\"275\":1}}],[\"然后提取了相应的文本特征\",{\"1\":{\"273\":1}}],[\"然后文本输出接两层mlp预测mask掉的tokens\",{\"1\":{\"258\":1}}],[\"然后对句子进行分词\",{\"1\":{\"511\":1}}],[\"然后对文本标志位对应输出使用一个线性的itm\",{\"1\":{\"258\":1}}],[\"然后对它们的特征做加权平均\",{\"1\":{\"100\":1}}],[\"然后和postion\",{\"1\":{\"257\":1}}],[\"然后和position\",{\"1\":{\"257\":1}}],[\"然后每次训练的时候\",{\"1\":{\"242\":1}}],[\"然后牺牲一些一致性\",{\"1\":{\"242\":1}}],[\"然后进行水平翻转\",{\"1\":{\"290\":1}}],[\"然后进来\",{\"1\":{\"241\":1}}],[\"然后进入pointnet++的特征传播阶段\",{\"1\":{\"30\":1}}],[\"然后经过e12这个编码器\",{\"1\":{\"238\":1}}],[\"然后我们读取要预测的图像\",{\"1\":{\"273\":1}}],[\"然后我们加一个\",{\"1\":{\"240\":1}}],[\"然后我们将图片x1经过数据增强t1得到图片x11\",{\"1\":{\"238\":1}}],[\"然后我们便可以直接用训练好的vit和bert来做图文匹配和图文相似度计算了\",{\"1\":{\"146\":1}}],[\"然后定义\",{\"1\":{\"157\":1}}],[\"然后用行归一化将多个正样本平均分配权重\",{\"1\":{\"145\":1}}],[\"然后使用prepare\",{\"1\":{\"510\":1}}],[\"然后使用第一个cls\",{\"1\":{\"282\":1}}],[\"然后使用一个对比学习的函数去训练模型就可以了\",{\"1\":{\"235\":1}}],[\"然后使用\",{\"1\":{\"112\":1}}],[\"然后使用全局最大池化\",{\"1\":{\"112\":1}}],[\"然后取\",{\"1\":{\"108\":1}}],[\"然后取平均得到\",{\"1\":{\"82\":1}}],[\"然后通过微调\",{\"1\":{\"675\":1}}],[\"然后通过人类对多个模型输出的偏好进行排序\",{\"1\":{\"467\":1}}],[\"然后通过全连接层\",{\"1\":{\"110\":2}}],[\"然后通过\",{\"1\":{\"98\":1,\"508\":1}}],[\"然后通过对应的pointnets提取每个尺度上的特征来捕获多尺度模式\",{\"1\":{\"95\":1}}],[\"然后只保留前\",{\"1\":{\"92\":1}}],[\"然后复制这个索引数组到每个\",{\"1\":{\"92\":1}}],[\"然后把所有点映射到高维的特征通过最大池化最终表示全局特征\",{\"1\":{\"86\":1}}],[\"然后求和\",{\"1\":{\"82\":1}}],[\"然后\",{\"1\":{\"8\":1,\"273\":1,\"323\":1,\"371\":1,\"406\":1,\"565\":1,\"686\":1}}],[\"及\",{\"1\":{\"285\":1}}],[\"及其迭代优化之后\",{\"1\":{\"687\":1}}],[\"及其多个变种\",{\"1\":{\"193\":1}}],[\"及其变体\",{\"1\":{\"183\":1}}],[\"及之后\",{\"1\":{\"159\":1}}],[\"渐进式图像\",{\"1\":{\"181\":1}}],[\"渐变\",{\"1\":{\"83\":1}}],[\"渐变颜色\",{\"1\":{\"83\":1}}],[\"轻松\",{\"1\":{\"686\":1}}],[\"轻量推理\",{\"1\":{\"674\":1}}],[\"轻量知识型\",{\"1\":{\"674\":1}}],[\"轻量级胶水层难以捕捉跨模态的复杂交互\",{\"1\":{\"181\":1}}],[\"轻\",{\"1\":{\"280\":7}}],[\"轻便\",{\"1\":{\"114\":1}}],[\"低成本的\",{\"1\":{\"686\":1}}],[\"低延迟\",{\"1\":{\"674\":1}}],[\"低于词对齐基线\",{\"1\":{\"455\":1}}],[\"低秩分解\",{\"1\":{\"423\":1}}],[\"低置信度\",{\"1\":{\"404\":1}}],[\"低\",{\"1\":{\"208\":1}}],[\"低效连接\",{\"1\":{\"181\":1}}],[\"低密度区域则过于稀缺\",{\"1\":{\"89\":1}}],[\"胶水层\",{\"1\":{\"181\":2,\"189\":1}}],[\"展现出了非常流畅和自然的表现\",{\"1\":{\"673\":1}}],[\"展现了强大的视觉能力和与llms的无缝集成潜力\",{\"1\":{\"180\":1}}],[\"展示\",{\"1\":{\"471\":1}}],[\"展示强大的数学\",{\"1\":{\"220\":1}}],[\"展示同类物体的结构相似性可辅助对齐\",{\"1\":{\"49\":1}}],[\"展示了其强大的泛化能力\",{\"1\":{\"676\":1}}],[\"展示了其轻量化设计在效率与性能上的优势\",{\"1\":{\"253\":1}}],[\"展示了如何仅使用公开可用的数据集来训练最先进的模型\",{\"1\":{\"674\":1}}],[\"展示了一些从如下高斯分布中采样的灰度图像\",{\"1\":{\"594\":1}}],[\"展示了三种不同协方差矩阵下的二维多元高斯密度图\",{\"1\":{\"590\":1}}],[\"展示了模型输出的多维质量元数据对比\",{\"1\":{\"471\":1}}],[\"展示了模型在是否遵循指令\",{\"1\":{\"470\":1}}],[\"展示了强大的in\",{\"1\":{\"465\":1}}],[\"展示了强大的少样本和零样本学习能力\",{\"1\":{\"184\":1}}],[\"展示了自然语言本身作为任务描述符的有效性\",{\"1\":{\"457\":1}}],[\"展示了大规模语言模型在无监督多任务学习中的潜力\",{\"1\":{\"452\":1}}],[\"展示了良好的语言泛化能力\",{\"1\":{\"194\":1}}],[\"展示了网页原始文本\",{\"1\":{\"132\":1}}],[\"展示了在大规模视觉\",{\"1\":{\"124\":1}}],[\"展示了对关键模块的消融实验结果\",{\"1\":{\"24\":1}}],[\"展示了各\",{\"1\":{\"19\":1}}],[\"展示了各类别的分布情况\",{\"1\":{\"17\":1}}],[\"书生\",{\"0\":{\"179\":1,\"206\":1}}],[\"手写数字\",{\"1\":{\"670\":1}}],[\"手动传递导数\",{\"1\":{\"632\":1}}],[\"手动进行反向传播\",{\"0\":{\"628\":1}}],[\"手动实现线性层反向传播\",{\"1\":{\"481\":1}}],[\"手风琴组成的貘\",{\"1\":{\"178\":1}}],[\"手工设计问题\",{\"1\":{\"63\":1}}],[\"早期方法如elmo\",{\"1\":{\"501\":1}}],[\"早期方法\",{\"1\":{\"464\":1}}],[\"早期的模型\",{\"1\":{\"460\":1}}],[\"早期的图文多模态\",{\"1\":{\"280\":1}}],[\"早期研究基于循环变分自编码器\",{\"1\":{\"178\":1}}],[\"早期工作\",{\"1\":{\"51\":1}}],[\"经济\",{\"1\":{\"674\":1}}],[\"经ccnet流水线去重\",{\"1\":{\"481\":1}}],[\"经历n次迭代\",{\"1\":{\"410\":1}}],[\"经历了多个阶段的发展\",{\"1\":{\"178\":1}}],[\"经过一层全连接后的输出\",{\"1\":{\"540\":1}}],[\"经过一通计算\",{\"1\":{\"474\":1}}],[\"经过相同的非线性变换后\",{\"1\":{\"513\":1}}],[\"经过embedding层将输入token序列变为一个三维张量\",{\"1\":{\"474\":1}}],[\"经过encoder之后\",{\"1\":{\"292\":1}}],[\"经过过滤\",{\"1\":{\"461\":1}}],[\"经过sigmoid\",{\"1\":{\"404\":1}}],[\"经过处理后的图像块嵌入张量\",{\"1\":{\"291\":1}}],[\"经过卷积层变成\",{\"1\":{\"291\":1}}],[\"经过预处理的图像和对应的标签\",{\"1\":{\"289\":1}}],[\"经过编码器2得到了正样本\",{\"1\":{\"240\":1}}],[\"经过编码器1得到了\",{\"1\":{\"240\":1}}],[\"经过线性投影得到块嵌入\",{\"1\":{\"171\":1}}],[\"经过全局池化后得到一个全局特征向量\",{\"1\":{\"107\":1}}],[\"经过几层\",{\"1\":{\"100\":1}}],[\"经过mlp进一步提取和融合特征\",{\"1\":{\"100\":1}}],[\"经过\",{\"1\":{\"43\":1,\"45\":2,\"76\":1,\"140\":1,\"407\":1,\"542\":1}}],[\"块\",{\"1\":{\"171\":1}}],[\"词性标注\",{\"1\":{\"543\":1}}],[\"词表大小\",{\"1\":{\"513\":1}}],[\"词表分布\",{\"1\":{\"155\":1}}],[\"词库中的词汇数量\",{\"1\":{\"510\":1}}],[\"词汇和句法歧义\",{\"1\":{\"448\":1}}],[\"词汇表增至50\",{\"1\":{\"454\":1}}],[\"词汇表灵活性\",{\"1\":{\"454\":1}}],[\"词汇表\",{\"1\":{\"447\":1}}],[\"词汇表与正则化\",{\"1\":{\"447\":1}}],[\"词汇表大小设为\",{\"1\":{\"170\":1}}],[\"词频从高到低\",{\"1\":{\"410\":1}}],[\"词嵌入与位置嵌入相加\",{\"1\":{\"284\":1}}],[\"词嵌入\",{\"1\":{\"262\":1,\"441\":1,\"477\":2,\"513\":2}}],[\"词tokenized成3个部分\",{\"1\":{\"258\":1}}],[\"施加均匀先验\",{\"1\":{\"170\":1}}],[\"松弛进行训练\",{\"1\":{\"170\":1}}],[\"双线性插值\",{\"1\":{\"397\":2}}],[\"双向自注意力\",{\"1\":{\"284\":1}}],[\"双向注意力\",{\"1\":{\"134\":1}}],[\"双语数据集\",{\"1\":{\"217\":1}}],[\"双语数据集和持续学习的视觉编码器\",{\"1\":{\"212\":1}}],[\"双语描述\",{\"1\":{\"217\":1}}],[\"双语能力\",{\"1\":{\"208\":1}}],[\"双编码器设计\",{\"1\":{\"212\":1}}],[\"双分支视觉编码器\",{\"1\":{\"211\":1}}],[\"双塔模型\",{\"1\":{\"188\":1}}],[\"双视图表示\",{\"1\":{\"166\":1}}],[\"官方代码链接\",{\"1\":{\"477\":1}}],[\"官方代码实现进行\",{\"1\":{\"477\":1}}],[\"官方代码实现\",{\"0\":{\"477\":1}}],[\"官方代码库并没有非常清晰指明capfilt模块的实现代码位置\",{\"1\":{\"140\":1}}],[\"官方仓库\",{\"1\":{\"338\":1}}],[\"官方实现的default\",{\"1\":{\"289\":1}}],[\"官方文档中的行为规范\",{\"1\":{\"163\":1}}],[\"≠\",{\"1\":{\"163\":1,\"472\":1}}],[\"蒸馏损失\",{\"1\":{\"163\":2}}],[\"蒸馏损失的权重\",{\"1\":{\"163\":1}}],[\"蒸馏\",{\"1\":{\"163\":2}}],[\"剩余的图片都不是同一个类别\",{\"1\":{\"235\":1}}],[\"剩余的语言\",{\"1\":{\"43\":1}}],[\"剩下的是失败\",{\"1\":{\"579\":1}}],[\"剩下的\",{\"1\":{\"163\":1}}],[\"被认为是\",{\"1\":{\"678\":1}}],[\"被证明在使用指令形式化描述的未见过的任务上表现良好\",{\"1\":{\"676\":1}}],[\"被设置为多个输入变量中最大的generation的值\",{\"1\":{\"656\":1}}],[\"被用作两次输入\",{\"1\":{\"654\":1}}],[\"被用作图像的全局特征\",{\"1\":{\"126\":1}}],[\"被称为贝叶斯推理\",{\"1\":{\"596\":1}}],[\"被称为自由度\",{\"1\":{\"586\":1}}],[\"被掩码后的输入序列\",{\"1\":{\"512\":1}}],[\"被掩码的图像表示为\",{\"1\":{\"172\":1}}],[\"被分开\",{\"1\":{\"454\":1}}],[\"被视为提升模型通用性的潜在途径\",{\"1\":{\"453\":1}}],[\"被背景淹没\",{\"1\":{\"404\":1}}],[\"被改变\",{\"1\":{\"383\":1}}],[\"被引用\",{\"1\":{\"366\":1}}],[\"被添加到\",{\"1\":{\"292\":1}}],[\"被广泛应用于对比学习中\",{\"1\":{\"240\":1}}],[\"被展平为向量并进行线性投影\",{\"1\":{\"169\":1}}],[\"被替换为随机\",{\"1\":{\"163\":1}}],[\"被替换为\",{\"1\":{\"163\":1}}],[\"~31k\",{\"1\":{\"470\":1}}],[\"~33k\",{\"1\":{\"470\":1}}],[\"~13k\",{\"1\":{\"470\":1}}],[\"~\",{\"1\":{\"339\":1,\"404\":2,\"635\":1,\"666\":1}}],[\"~$\",{\"1\":{\"332\":1}}],[\"~indices\",{\"1\":{\"163\":1}}],[\"~masked\",{\"1\":{\"163\":1}}],[\"~pred\",{\"1\":{\"83\":1}}],[\"克隆一份\",{\"1\":{\"163\":1}}],[\"固定p\",{\"1\":{\"600\":1}}],[\"固定作为\",{\"1\":{\"162\":1}}],[\"固定返回问题0\",{\"1\":{\"68\":1}}],[\"共用逻辑\",{\"1\":{\"660\":1}}],[\"共分为\",{\"1\":{\"508\":1}}],[\"共273个样例\",{\"1\":{\"455\":1}}],[\"共同支撑了零样本泛化能力\",{\"1\":{\"454\":1}}],[\"共指\",{\"1\":{\"448\":1}}],[\"共个值\",{\"1\":{\"307\":1}}],[\"共5个类别\",{\"1\":{\"289\":1}}],[\"共\",{\"1\":{\"190\":1,\"493\":1}}],[\"共有\",{\"1\":{\"162\":1}}],[\"共享变量与梯度累加\",{\"0\":{\"654\":1}}],[\"共享权重\",{\"1\":{\"513\":1}}],[\"共享内存\",{\"1\":{\"384\":1}}],[\"共享嵌入空间\",{\"1\":{\"160\":2}}],[\"共享除了\",{\"1\":{\"126\":1}}],[\"按导数公式计算梯度\",{\"1\":{\"660\":1}}],[\"按序取出元素\",{\"1\":{\"656\":1}}],[\"按序执行以下命令完成环境搭建\",{\"1\":{\"519\":1}}],[\"按反向顺序调用各函数的backward方法\",{\"1\":{\"632\":1}}],[\"按任意顺序进行排列\",{\"1\":{\"600\":1}}],[\"按评分排序答案\",{\"1\":{\"481\":1}}],[\"按用户\",{\"1\":{\"470\":1}}],[\"按需要在w前面拼接一些参数\",{\"1\":{\"419\":1}}],[\"按空格切分\",{\"1\":{\"410\":1}}],[\"按类别分配\",{\"1\":{\"404\":1}}],[\"按元素大小\",{\"1\":{\"327\":1}}],[\"按转置顺序\",{\"1\":{\"326\":1}}],[\"按比例随机采样验证样本\",{\"1\":{\"289\":1}}],[\"按指令做事\",{\"1\":{\"231\":1}}],[\"按\",{\"1\":{\"163\":1,\"363\":1,\"592\":1}}],[\"按权重从当前行采样一个负样本索引\",{\"1\":{\"162\":1}}],[\"按照本批次序列中最大长度进行截断\",{\"1\":{\"522\":1}}],[\"按照\",{\"1\":{\"410\":1}}],[\"按照16x16大小的patch进行划分\",{\"1\":{\"291\":1}}],[\"按照余弦相似度的数学公式来计算两者的相似度数值\",{\"1\":{\"275\":1}}],[\"按照最大相似度\",{\"1\":{\"273\":1}}],[\"按照给定概率进行\",{\"1\":{\"163\":1}}],[\"按照权重做加权平均\",{\"1\":{\"100\":1}}],[\"按照几个预定义的半径值来搜索周围的邻近点\",{\"1\":{\"95\":1}}],[\"按照不同的搜索半径或领域大小对点集进行分组\",{\"1\":{\"95\":1}}],[\"按照相似度完成信息融合\",{\"1\":{\"59\":1}}],[\"按照key的插入顺序返回的\",{\"1\":{\"29\":1}}],[\"屏蔽填充部分的信息\",{\"1\":{\"548\":1}}],[\"屏蔽不可\",{\"1\":{\"163\":1}}],[\"屏蔽对角线\",{\"1\":{\"162\":1}}],[\"屏蔽掉\",{\"1\":{\"142\":1}}],[\"判定其是否语法正确\",{\"1\":{\"448\":1}}],[\"判别性和稳定性\",{\"1\":{\"161\":1}}],[\"判断\",{\"1\":{\"651\":1}}],[\"判断下一个句子是否是当前句子的后续句子\",{\"1\":{\"513\":1}}],[\"判断第\",{\"1\":{\"504\":1}}],[\"判断两个句子是否连续\",{\"1\":{\"497\":1}}],[\"判断两个片段是否连续\",{\"1\":{\"493\":1}}],[\"判断它们之间的关系\",{\"1\":{\"448\":1}}],[\"判断每个字符对是否存在于频次表中\",{\"1\":{\"411\":1}}],[\"判断预测是否正确\",{\"1\":{\"275\":1}}],[\"判断是否为交叉注意力\",{\"1\":{\"285\":1}}],[\"判断是否为\",{\"1\":{\"266\":1}}],[\"判断是否匹配\",{\"1\":{\"128\":1}}],[\"判断者\",{\"1\":{\"228\":1}}],[\"判断当前图文对与队列中哪些样本是正对\",{\"1\":{\"145\":1}}],[\"判断图文是否匹配\",{\"1\":{\"127\":1,\"145\":1}}],[\"判断模型是否能正确区分前景和背景\",{\"1\":{\"82\":1}}],[\"线性上升更新\",{\"1\":{\"447\":1}}],[\"线性增长\",{\"1\":{\"395\":1}}],[\"线性组合\",{\"1\":{\"395\":2}}],[\"线性加权和\",{\"1\":{\"395\":2}}],[\"线性评估指的是\",{\"1\":{\"237\":1}}],[\"线性评估\",{\"1\":{\"237\":1}}],[\"线性分类头使用\",{\"1\":{\"204\":1}}],[\"线性探测\",{\"1\":{\"193\":1,\"205\":1}}],[\"线性投影到与\",{\"1\":{\"286\":1}}],[\"线性投影\",{\"1\":{\"185\":1}}],[\"线性变换\",{\"1\":{\"161\":1}}],[\"线性从\",{\"1\":{\"159\":1}}],[\"配以自然语言任务说明进行微调\",{\"1\":{\"469\":1}}],[\"配置控制等\",{\"1\":{\"650\":1}}],[\"配置\",{\"1\":{\"160\":1}}],[\"配对编码器用于同步参数\",{\"1\":{\"145\":1}}],[\"掉的\",{\"1\":{\"160\":1}}],[\"掉的单词\",{\"1\":{\"155\":1}}],[\"恒定为最大值\",{\"1\":{\"159\":1}}],[\"步里被随机遮盖或替换的部分\",{\"1\":{\"505\":1}}],[\"步进\",{\"1\":{\"327\":1}}],[\"步长解决的是如何将多维张量中的\",{\"1\":{\"323\":1}}],[\"步长\",{\"0\":{\"323\":1},\"1\":{\"323\":1,\"325\":2}}],[\"步距为16\",{\"1\":{\"291\":1}}],[\"步骤9\",{\"0\":{\"640\":1}}],[\"步骤8\",{\"0\":{\"636\":1}}],[\"步骤7\",{\"0\":{\"633\":1}}],[\"步骤6\",{\"0\":{\"628\":1}}],[\"步骤5\",{\"0\":{\"624\":1}}],[\"步骤4\",{\"0\":{\"619\":1}}],[\"步骤3\",{\"0\":{\"615\":1},\"1\":{\"224\":1}}],[\"步骤27\",{\"0\":{\"668\":1}}],[\"步骤26\",{\"0\":{\"667\":1}}],[\"步骤25\",{\"0\":{\"666\":1}}],[\"步骤24\",{\"0\":{\"662\":1}}],[\"步骤23\",{\"0\":{\"661\":1}}],[\"步骤20\",{\"0\":{\"660\":1}}],[\"步骤2\",{\"0\":{\"610\":1},\"1\":{\"224\":1}}],[\"步骤19\",{\"0\":{\"659\":1}}],[\"步骤18\",{\"0\":{\"658\":1}}],[\"步骤17\",{\"0\":{\"657\":1}}],[\"步骤16\",{\"0\":{\"656\":1}}],[\"步骤15\",{\"0\":{\"655\":1}}],[\"步骤14\",{\"0\":{\"654\":1}}],[\"步骤13\",{\"0\":{\"653\":1}}],[\"步骤12\",{\"0\":{\"652\":1}}],[\"步骤11\",{\"0\":{\"651\":1}}],[\"步骤10\",{\"0\":{\"644\":1}}],[\"步骤1\",{\"0\":{\"605\":1},\"1\":{\"224\":1}}],[\"步热身\",{\"1\":{\"201\":1}}],[\"步\",{\"1\":{\"159\":1,\"200\":1,\"201\":1,\"493\":2}}],[\"监督学习微调\",{\"1\":{\"468\":1}}],[\"监督式微调sft\",{\"1\":{\"416\":1}}],[\"监督微调\",{\"0\":{\"448\":1},\"1\":{\"190\":1,\"470\":1}}],[\"监督\",{\"1\":{\"163\":2}}],[\"监督为主\",{\"1\":{\"159\":1}}],[\"监督信号不理想\",{\"1\":{\"120\":1}}],[\"α=0\",{\"1\":{\"405\":2}}],[\"α>β\",{\"1\":{\"405\":1}}],[\"α\",{\"1\":{\"159\":1,\"404\":4,\"405\":3,\"407\":6}}],[\"捕捉不同子空间的信息\",{\"1\":{\"548\":1}}],[\"捕捉全局信息\",{\"1\":{\"299\":1}}],[\"捕捉与图像内容相关的多种可能性\",{\"1\":{\"157\":1}}],[\"捕获密集到稀疏采样区域内的多尺度信息\",{\"1\":{\"96\":1}}],[\"散度\",{\"1\":{\"157\":1,\"163\":1}}],[\"老师模型\",{\"1\":{\"157\":1}}],[\"起主导作用\",{\"1\":{\"402\":1}}],[\"起始偏移量\",{\"1\":{\"325\":1}}],[\"起到了\",{\"1\":{\"358\":1}}],[\"起到\",{\"1\":{\"157\":1}}],[\"起来\",{\"1\":{\"33\":1}}],[\"硬负样本采样策略\",{\"1\":{\"156\":1}}],[\"硬标签混合\",{\"1\":{\"147\":1}}],[\"令\",{\"1\":{\"154\":1}}],[\"启发式过滤低质量网页\",{\"1\":{\"481\":1}}],[\"启发\",{\"1\":{\"154\":1}}],[\"启用反向传播\",{\"1\":{\"658\":1}}],[\"启用采样\",{\"1\":{\"143\":1}}],[\"启用\",{\"1\":{\"127\":3}}],[\"受ureader启发\",{\"1\":{\"216\":1}}],[\"受\",{\"1\":{\"154\":1}}],[\"受限于瓶颈维度\",{\"1\":{\"112\":1}}],[\"虽对部分任务性能略有影响\",{\"1\":{\"497\":1}}],[\"虽在图文检索中效果出色\",{\"1\":{\"150\":1}}],[\"虽然我们通过使用大模型来简化了业务逻辑的拆解\",{\"1\":{\"687\":1}}],[\"虽然大模型是深度学习领域的集大成之作\",{\"1\":{\"686\":1}}],[\"虽然大模型训练更快达到目标性能\",{\"1\":{\"480\":1}}],[\"虽然之前的聊天机器人存在各种问题\",{\"1\":{\"678\":1}}],[\"虽然python的垃圾回收\",{\"1\":{\"657\":1}}],[\"虽然密度在下降\",{\"1\":{\"592\":1}}],[\"虽然密度函数以\",{\"1\":{\"592\":1}}],[\"虽然你可能觉得\",{\"1\":{\"569\":1}}],[\"虽然早期实验显示性能略有下降\",{\"1\":{\"495\":1}}],[\"虽然也不完全正确\",{\"1\":{\"471\":1}}],[\"虽然具备强大的自然语言处理能力\",{\"1\":{\"468\":1}}],[\"虽然gpt\",{\"1\":{\"463\":1}}],[\"虽然在阅读理解等任务上接近监督基线\",{\"1\":{\"456\":1}}],[\"虽然深度减少了神经元数量\",{\"1\":{\"395\":1}}],[\"虽然没变\",{\"1\":{\"387\":1}}],[\"虽然推理时间快\",{\"1\":{\"259\":1}}],[\"虽然还不能执行复杂的推理任务\",{\"1\":{\"226\":1}}],[\"虽然规模大\",{\"1\":{\"122\":1}}],[\"虽然\",{\"1\":{\"112\":3,\"395\":1,\"403\":1}}],[\"虽然这种方式需要花费时间和内存来完成拷贝\",{\"1\":{\"326\":1}}],[\"虽然这个点没有实际意义\",{\"1\":{\"92\":1}}],[\"虽然这里用的是\",{\"1\":{\"78\":1}}],[\"促进硬负样本挖掘\",{\"1\":{\"149\":1}}],[\"匹配或超过3\",{\"1\":{\"455\":1}}],[\"匹配\",{\"1\":{\"162\":1,\"454\":1}}],[\"匹配or不匹配\",{\"1\":{\"147\":1}}],[\"匹配和生成等任务目标进行联合训练\",{\"1\":{\"129\":1}}],[\"队列大小\",{\"1\":{\"246\":1}}],[\"队列里的样本不需要进行梯度回传\",{\"1\":{\"237\":1}}],[\"队列\",{\"1\":{\"147\":2,\"389\":1}}],[\"队列指针\",{\"1\":{\"147\":1,\"246\":1}}],[\"队列用于\",{\"1\":{\"147\":1}}],[\"滑动平均更新\",{\"1\":{\"147\":1}}],[\"便得到了模态对齐好的图像编码器vit\",{\"1\":{\"146\":1}}],[\"便于识别变量类型\",{\"1\":{\"659\":1}}],[\"便于在训练和推理阶段灵活切换\",{\"1\":{\"658\":1}}],[\"便于直接调用\",{\"1\":{\"641\":1}}],[\"便于从\",{\"1\":{\"513\":1}}],[\"便于跨数据集评估\",{\"1\":{\"454\":1}}],[\"便于之后相似度计算\",{\"1\":{\"160\":1}}],[\"便于计算归一化相似度\",{\"1\":{\"147\":1}}],[\"便于计算相似度\",{\"1\":{\"59\":1}}],[\"便于广播到整个\",{\"1\":{\"108\":1}}],[\"便于广播\",{\"1\":{\"92\":1}}],[\"便于后续处理\",{\"1\":{\"107\":1}}],[\"便于后续计算\",{\"1\":{\"100\":1,\"401\":1,\"402\":1,\"405\":1,\"407\":1}}],[\"便于后续统一评估\",{\"1\":{\"82\":1}}],[\"便于后续\",{\"1\":{\"45\":1}}],[\"软标签\",{\"1\":{\"147\":1}}],[\"软标签计算\",{\"1\":{\"145\":1}}],[\"软标签通常用于边界模糊区域\",{\"1\":{\"64\":1}}],[\"环境搭建遵从如下步骤即可\",{\"1\":{\"546\":1}}],[\"环境搭建\",{\"0\":{\"519\":1}}],[\"环境中的\",{\"1\":{\"338\":1}}],[\"环境中使用\",{\"1\":{\"338\":1}}],[\"环境再使用\",{\"1\":{\"338\":1}}],[\"环境\",{\"0\":{\"332\":1,\"546\":1},\"1\":{\"145\":1,\"224\":1,\"333\":1,\"334\":1}}],[\"环境配置\",{\"0\":{\"38\":1}}],[\"束搜索\",{\"1\":{\"143\":1}}],[\"核采样参数\",{\"1\":{\"286\":1}}],[\"核采样时不扩展beam\",{\"1\":{\"286\":1}}],[\"核采样的概率阈值\",{\"1\":{\"286\":1}}],[\"核采样\",{\"1\":{\"143\":1}}],[\"核心价值\",{\"1\":{\"687\":1}}],[\"核心组件\",{\"0\":{\"683\":1},\"1\":{\"685\":1}}],[\"核心函数实现原理\",{\"1\":{\"666\":1}}],[\"核心功能\",{\"1\":{\"661\":1}}],[\"核心原因在于层次化的函数构造方式比单层网络的线性组合更高效\",{\"1\":{\"395\":1}}],[\"核心原则\",{\"1\":{\"387\":1}}],[\"核心要点\",{\"1\":{\"291\":1}}],[\"核心计算\",{\"0\":{\"266\":1}}],[\"核心思想\",{\"1\":{\"231\":1,\"404\":1,\"407\":1}}],[\"核心问题\",{\"1\":{\"112\":1}}],[\"核心\",{\"0\":{\"103\":1}}],[\"核心操作包括\",{\"1\":{\"98\":1}}],[\"核心挑战\",{\"1\":{\"49\":1}}],[\"核心代码实现如下\",{\"1\":{\"28\":1,\"513\":1}}],[\"份\",{\"1\":{\"143\":1}}],[\"索引\",{\"1\":{\"511\":1}}],[\"索引映射到内存中的地址\",{\"1\":{\"323\":1}}],[\"索引编号\",{\"1\":{\"142\":1}}],[\"索引保存下来\",{\"1\":{\"92\":1}}],[\"清华大学和智谱\",{\"1\":{\"674\":1}}],[\"清晰了预训练模型的作用\",{\"1\":{\"677\":1}}],[\"清晰\",{\"1\":{\"665\":1}}],[\"清除中间变量的导数\",{\"1\":{\"658\":1}}],[\"清除旧梯度\",{\"1\":{\"142\":1}}],[\"清洗原始\",{\"1\":{\"447\":1}}],[\"清洗\",{\"1\":{\"6\":1}}],[\"剔除\",{\"1\":{\"134\":1}}],[\"剔除原始和生成的低质量描述\",{\"1\":{\"120\":1}}],[\"阶段只需\",{\"1\":{\"472\":1}}],[\"阶段二\",{\"1\":{\"178\":1}}],[\"阶段一\",{\"1\":{\"178\":1}}],[\"阶段\",{\"1\":{\"134\":1}}],[\"确立自回归范式\",{\"1\":{\"485\":1}}],[\"确信\",{\"1\":{\"471\":1}}],[\"确实像openai的gpt4这样的llm已经非常强了\",{\"1\":{\"430\":1}}],[\"确认你使用的是\",{\"1\":{\"338\":1}}],[\"确认偏差\",{\"1\":{\"134\":1}}],[\"确定目标\",{\"1\":{\"687\":1}}],[\"确定缩放因子\",{\"1\":{\"295\":1}}],[\"确定性搜索\",{\"1\":{\"133\":1}}],[\"确保它们在部署前达到预期的性能和稳定性标准\",{\"1\":{\"685\":1}}],[\"确保它们加起来是1\",{\"1\":{\"100\":1}}],[\"确保\",{\"1\":{\"684\":1}}],[\"确保所有利用\",{\"1\":{\"684\":1}}],[\"确保开发者能够在升级过程中无缝过渡\",{\"1\":{\"684\":1}}],[\"确保信息的持续更新和准确性\",{\"1\":{\"679\":1}}],[\"确保输出内容的精确性和可信度\",{\"1\":{\"679\":1}}],[\"确保复杂计算图反向传播顺序正确\",{\"1\":{\"663\":1}}],[\"确保类型转换优先\",{\"1\":{\"660\":1}}],[\"确保运算正常进行\",{\"1\":{\"660\":1}}],[\"确保代码质量\",{\"1\":{\"648\":1}}],[\"确保代码功能正确性\",{\"1\":{\"647\":1}}],[\"确保variable只接受ndarray实例\",{\"1\":{\"643\":1}}],[\"确保映射是一一对应的\",{\"1\":{\"511\":1}}],[\"确保句子数为偶数\",{\"1\":{\"510\":1}}],[\"确保在单台机器上\",{\"1\":{\"510\":1}}],[\"确保内容经过人工筛选\",{\"1\":{\"454\":1}}],[\"确保卷积操作不会重叠\",{\"1\":{\"291\":1}}],[\"确保右边编码器更新得非常缓慢\",{\"1\":{\"238\":1}}],[\"确保视觉编码器与llms的特征空间一致\",{\"1\":{\"181\":1}}],[\"确保每个\",{\"1\":{\"76\":1}}],[\"确保推理一致性\",{\"1\":{\"66\":1}}],[\"合理的内存管理至关重要\",{\"1\":{\"657\":1}}],[\"合理事件空间\",{\"1\":{\"566\":1}}],[\"合计40\",{\"1\":{\"461\":1}}],[\"合成文本的多样性对性能的影响\",{\"0\":{\"133\":1}}],[\"合并头结果\",{\"1\":{\"531\":1}}],[\"合并的\",{\"1\":{\"447\":1}}],[\"合并参数\",{\"1\":{\"423\":1}}],[\"合并该字符对\",{\"1\":{\"411\":1}}],[\"合并当前最高频的字符对\",{\"1\":{\"410\":3}}],[\"合并前n个最频繁的字符对\",{\"1\":{\"410\":1}}],[\"合并正负样本对\",{\"1\":{\"147\":1}}],[\"合并\",{\"1\":{\"78\":1,\"423\":1}}],[\"性价比最高\",{\"1\":{\"674\":1}}],[\"性价比高\",{\"1\":{\"674\":1}}],[\"性别化代词\",{\"1\":{\"484\":1}}],[\"性别偏见\",{\"1\":{\"469\":1}}],[\"性质的函数是指像sigmoid函数的有界函数\",{\"1\":{\"395\":1}}],[\"性质的激活函数的隐藏层组成的前馈神经网络\",{\"1\":{\"395\":1}}],[\"性能得到进一步提升\",{\"1\":{\"674\":1}}],[\"性能对标\",{\"1\":{\"674\":1}}],[\"性能强\",{\"1\":{\"674\":1}}],[\"性能强大\",{\"1\":{\"674\":1}}],[\"性能最强\",{\"1\":{\"674\":1}}],[\"性能最佳\",{\"1\":{\"461\":1}}],[\"性能有了进一步提升\",{\"1\":{\"674\":1}}],[\"性能开销较高\",{\"1\":{\"657\":1}}],[\"性能略优于full\",{\"1\":{\"495\":1}}],[\"性能优于原始bert\",{\"1\":{\"495\":1}}],[\"性能下降\",{\"1\":{\"495\":1}}],[\"性能仍优于gpt\",{\"1\":{\"482\":1}}],[\"性能好\",{\"1\":{\"472\":1}}],[\"性能提升非源于记忆\",{\"1\":{\"455\":1}}],[\"性能测试\",{\"1\":{\"368\":1,\"379\":1}}],[\"性能不够高\",{\"1\":{\"259\":1}}],[\"性能验证与社会责任\",{\"1\":{\"480\":1}}],[\"性能验证\",{\"1\":{\"208\":1}}],[\"性能\",{\"1\":{\"138\":1,\"280\":1,\"455\":1,\"471\":2,\"482\":1}}],[\"性能反而下降\",{\"1\":{\"134\":1}}],[\"性能次优\",{\"1\":{\"134\":1}}],[\"性\",{\"1\":{\"132\":1}}],[\"移至每个子模块的输入\",{\"1\":{\"454\":1}}],[\"移除nsp任务\",{\"1\":{\"497\":1,\"502\":1}}],[\"移除\",{\"1\":{\"493\":1}}],[\"移除下一句预测\",{\"1\":{\"491\":1,\"492\":1}}],[\"移除宏定义和参考文献\",{\"1\":{\"481\":1}}],[\"移除了大量与本文无关的逻辑\",{\"1\":{\"477\":1}}],[\"移除提示后性能下降6\",{\"1\":{\"455\":1}}],[\"移除最后一个\",{\"1\":{\"143\":1}}],[\"移除无效数据的有效\",{\"1\":{\"132\":1}}],[\"移动\",{\"1\":{\"49\":1,\"116\":1}}],[\"且必须按顺序处理内容\",{\"1\":{\"679\":1}}],[\"且效率较低\",{\"1\":{\"637\":1}}],[\"且第\",{\"1\":{\"579\":1}}],[\"且\",{\"1\":{\"566\":1,\"568\":1,\"570\":1,\"583\":1}}],[\"且更高效\",{\"1\":{\"495\":1}}],[\"且偏好有偏\",{\"1\":{\"472\":1}}],[\"且在公开nlp任务上的性能损失极小\",{\"1\":{\"467\":1}}],[\"且在更具挑战性的winogrande数据集上few\",{\"1\":{\"462\":1}}],[\"且每个任务都需新数据\",{\"1\":{\"461\":1}}],[\"且预留给\",{\"1\":{\"424\":1}}],[\"且真实也为正类的样本数\",{\"1\":{\"405\":1}}],[\"且训练难度增加\",{\"1\":{\"395\":1}}],[\"且高次项易导致震荡\",{\"1\":{\"395\":1}}],[\"且能加速微调收敛\",{\"1\":{\"165\":1}}],[\"且具备数据量和模型规模的可扩展性\",{\"1\":{\"132\":1}}],[\"且对多功能的相似结构易产生混淆\",{\"1\":{\"49\":1}}],[\"人是否患病\",{\"1\":{\"569\":1}}],[\"人群中\",{\"1\":{\"569\":1}}],[\"人力\",{\"1\":{\"504\":1}}],[\"人\",{\"1\":{\"355\":1}}],[\"人体关键点检测上都超越了有监督的预训练模型\",{\"1\":{\"233\":1}}],[\"人工标注\",{\"1\":{\"131\":1}}],[\"人类普遍价值\",{\"1\":{\"472\":1}}],[\"人类偏好引导型\",{\"1\":{\"472\":1}}],[\"人类代表\",{\"1\":{\"472\":1}}],[\"人类评估\",{\"1\":{\"471\":1}}],[\"人类评估者显著偏好\",{\"1\":{\"471\":1}}],[\"人类评估者90\",{\"1\":{\"178\":1}}],[\"人类评估输出的偏好\",{\"1\":{\"470\":1}}],[\"人类数据采集\",{\"1\":{\"470\":1}}],[\"人类价值对齐\",{\"1\":{\"472\":1}}],[\"人类价值\",{\"1\":{\"468\":1}}],[\"人类反馈微调是一种有效的模型对齐手段\",{\"1\":{\"467\":1}}],[\"人类反馈强化学习\",{\"1\":{\"224\":1}}],[\"人类可通过少量示例或自然语言指令快速适应新任务\",{\"1\":{\"460\":1}}],[\"人类仅需少量示例或简单指令即可完成新任务\",{\"1\":{\"460\":1}}],[\"人类通过同时分析物体的\",{\"1\":{\"32\":1}}],[\"人类通过多步推理和类比思维解决复杂任务\",{\"1\":{\"6\":1}}],[\"人类交互文本\",{\"1\":{\"29\":1}}],[\"人类交互文本数据文件路径\",{\"1\":{\"29\":1}}],[\"人类会\",{\"1\":{\"6\":1}}],[\"人类认知启发\",{\"1\":{\"6\":1}}],[\"遵循chung\",{\"1\":{\"483\":1}}],[\"遵循常规做法\",{\"1\":{\"204\":1}}],[\"遵循您的格式规范\",{\"1\":{\"129\":1}}],[\"遵循一个\",{\"1\":{\"71\":1}}],[\"机器上进行分布式训练\",{\"1\":{\"494\":1}}],[\"机器翻译实战\",{\"1\":{\"545\":1}}],[\"机器翻译\",{\"1\":{\"469\":1,\"501\":1}}],[\"机器人操作应用\",{\"1\":{\"51\":1}}],[\"机器人操作等任务中表现优异\",{\"1\":{\"7\":1}}],[\"机翻\",{\"1\":{\"440\":1}}],[\"机制可以处理循环引用对象\",{\"1\":{\"657\":1}}],[\"机制确保函数调用顺序的正确执行\",{\"1\":{\"656\":1}}],[\"机制\",{\"0\":{\"656\":1},\"1\":{\"464\":1,\"663\":1}}],[\"机制生成新的视图\",{\"1\":{\"387\":1}}],[\"机制与重新初始化更契合\",{\"1\":{\"137\":1}}],[\"机制有效从网页图文对中挖掘高质量样本\",{\"1\":{\"129\":1}}],[\"统一转换为variable\",{\"1\":{\"660\":1}}],[\"统一处理逻辑\",{\"1\":{\"651\":1}}],[\"统一文本到文本框架\",{\"1\":{\"485\":1}}],[\"统一变为相同尺寸\",{\"1\":{\"396\":1}}],[\"统一多模态的可能性\",{\"1\":{\"301\":1}}],[\"统一建模与数据增强的多模态预训练方法\",{\"1\":{\"129\":1}}],[\"统一的多任务架构\",{\"1\":{\"129\":1}}],[\"统计每个相邻字符对的出现次数\",{\"1\":{\"410\":1}}],[\"统计每个名词短语出现的频率\",{\"1\":{\"226\":1}}],[\"统计函数运行时间\",{\"1\":{\"379\":1}}],[\"统计距离\",{\"1\":{\"360\":1}}],[\"统计正确率\",{\"1\":{\"275\":1}}],[\"统计量\",{\"1\":{\"115\":1}}],[\"统计分析\",{\"0\":{\"19\":1}}],[\"头部调优\",{\"1\":{\"205\":1}}],[\"头部处理\",{\"1\":{\"101\":1}}],[\"头维度和mlp比例\",{\"1\":{\"189\":1}}],[\"头数\",{\"1\":{\"188\":1}}],[\"头\",{\"1\":{\"162\":1}}],[\"头预测为不匹配\",{\"1\":{\"128\":1}}],[\"若干业务\",{\"1\":{\"686\":1}}],[\"若再增加迭代次数到\",{\"1\":{\"667\":1}}],[\"若从好的起点开始\",{\"1\":{\"667\":1}}],[\"若画出其\",{\"1\":{\"667\":1}}],[\"若显示版本信息\",{\"1\":{\"666\":1}}],[\"若不使用运算符\",{\"1\":{\"662\":1}}],[\"若左操作数a不支持\",{\"1\":{\"660\":1}}],[\"若a未实现\",{\"1\":{\"660\":1}}],[\"若y\",{\"1\":{\"660\":1}}],[\"若为false\",{\"1\":{\"658\":1}}],[\"若为none则使用默认值\",{\"1\":{\"295\":1}}],[\"若使用强引用\",{\"1\":{\"657\":1}}],[\"若存在循环引用\",{\"1\":{\"657\":1}}],[\"若由\",{\"1\":{\"625\":1}}],[\"若集合\",{\"1\":{\"569\":1}}],[\"若在某个事件\",{\"1\":{\"568\":1}}],[\"若满足以下条件\",{\"1\":{\"568\":1}}],[\"若拒绝机制不足\",{\"1\":{\"472\":1}}],[\"若未经适当调节可能导致某些群体观点被系统性排除\",{\"1\":{\"472\":1}}],[\"若未指定掩码位置\",{\"1\":{\"163\":1}}],[\"若刻意要求生成毒性内容\",{\"1\":{\"471\":1}}],[\"若前景点稀疏\",{\"1\":{\"404\":1}}],[\"若希望确保内存效率\",{\"1\":{\"385\":1}}],[\"若qk\",{\"1\":{\"295\":1}}],[\"若有query\",{\"1\":{\"284\":1}}],[\"若以文本单词数量来衡量\",{\"1\":{\"272\":1}}],[\"若想要得到比较好的效果\",{\"1\":{\"238\":1}}],[\"若提供\",{\"1\":{\"163\":1}}],[\"若只需输出\",{\"1\":{\"163\":1}}],[\"若图文语义相近但细节不同\",{\"1\":{\"156\":1}}],[\"若\",{\"1\":{\"128\":1,\"407\":2,\"566\":2,\"568\":1,\"577\":1}}],[\"若缺失\",{\"1\":{\"24\":1}}],[\"噪声\",{\"1\":{\"163\":1}}],[\"噪声比例仅\",{\"1\":{\"134\":1}}],[\"噪声比例稍高\",{\"1\":{\"133\":1}}],[\"噪声比例较低\",{\"1\":{\"133\":1}}],[\"噪声严重\",{\"1\":{\"128\":1}}],[\"噪声仍广泛存在\",{\"1\":{\"122\":1}}],[\"本地文件访问时内存管理升级\",{\"1\":{\"674\":1}}],[\"本地批次\",{\"1\":{\"145\":1}}],[\"本阶段的目标是打造一个\",{\"1\":{\"670\":1}}],[\"本步骤使用tinypytorch实现了梯度下降法\",{\"1\":{\"667\":1}}],[\"本步骤的目标是找到使rosenbrock函数输出值最小的和\",{\"1\":{\"667\":1}}],[\"本步骤将处理rosenbrock函数\",{\"1\":{\"667\":1}}],[\"本步骤选取3个经典测试函数\",{\"1\":{\"662\":1}}],[\"本系列文章试图揭开这些机制背后的本质\",{\"1\":{\"650\":1}}],[\"本系列将带领读者从零开始创建一个深度学习框架\",{\"1\":{\"604\":1}}],[\"本部分感兴趣的可以回顾高数中微分定义部分内容\",{\"1\":{\"621\":1}}],[\"本部分图片来源\",{\"1\":{\"328\":1}}],[\"本节代码\",{\"1\":{\"603\":1,\"649\":1,\"664\":1,\"669\":1}}],[\"本节讨论的是定义在\",{\"1\":{\"574\":1}}],[\"本节将详细讨论高斯分布\",{\"1\":{\"588\":1}}],[\"本节将会对bert模型实现的部分细节进行说明\",{\"1\":{\"516\":1}}],[\"本节将根据\",{\"1\":{\"477\":1}}],[\"本节我们将基于clip预训练模型实现zero\",{\"1\":{\"275\":1}}],[\"本节我们将对多模态bert的前向传播基本流程进行讲解\",{\"1\":{\"261\":1}}],[\"本研究指出\",{\"1\":{\"467\":1}}],[\"本类中不直接使用\",{\"1\":{\"405\":1}}],[\"本身\",{\"1\":{\"375\":1,\"566\":1}}],[\"本身接受参数\",{\"1\":{\"371\":1}}],[\"本次训练是基于预训练好的vit\",{\"1\":{\"300\":1}}],[\"本质上是一个\",{\"1\":{\"674\":1}}],[\"本质上是一个线性\",{\"1\":{\"321\":1}}],[\"本质上\",{\"1\":{\"414\":1,\"430\":1}}],[\"本质\",{\"1\":{\"375\":1}}],[\"本质是一个\",{\"1\":{\"368\":1}}],[\"本质是一个二分类任务\",{\"1\":{\"127\":1}}],[\"本质就是一个交叉熵损失函数\",{\"1\":{\"240\":1}}],[\"本文的transformer使用了self\",{\"1\":{\"547\":1}}],[\"本文的核心假设是\",{\"1\":{\"453\":1}}],[\"本文基于\",{\"1\":{\"546\":1}}],[\"本文使用的是谷歌的中文预训练模型\",{\"1\":{\"519\":1}}],[\"本文使用的是训练了9个epoch后的模型权重进行的推理演示\",{\"1\":{\"83\":1}}],[\"本文所展示的bert预训练属于教学级别的\",{\"1\":{\"515\":1}}],[\"本文不再全部copy展示\",{\"1\":{\"513\":1}}],[\"本文中的\",{\"1\":{\"513\":1}}],[\"本文引入了一种有效的对齐方法\",{\"1\":{\"468\":1}}],[\"本文通过gpt\",{\"1\":{\"457\":1}}],[\"本文通过250m互联网图文对训练\",{\"1\":{\"178\":1}}],[\"本文采用字节级\",{\"1\":{\"454\":1}}],[\"本文进一步假设\",{\"1\":{\"454\":1}}],[\"本文将通过一个花卉分类的实战案例结合vit原论文\",{\"1\":{\"288\":1}}],[\"本文改编bert代码讲解基于blip项目展开\",{\"1\":{\"260\":1}}],[\"本文提出了一种对齐语言模型与用户意图的策略\",{\"1\":{\"468\":1}}],[\"本文提出了一种基于自回归transformer的简单方法\",{\"1\":{\"178\":1}}],[\"本文提出了一种基于自回归transformer的简单而高效的文本到图像生成方法\",{\"1\":{\"177\":1}}],[\"本文提出的方法在效率上大大提升且表现出相似的性能\",{\"1\":{\"259\":1}}],[\"本文提出的\",{\"1\":{\"212\":1}}],[\"本文创新\",{\"1\":{\"52\":1}}],[\"本文创新点\",{\"1\":{\"49\":1}}],[\"本文区别\",{\"1\":{\"51\":1}}],[\"本文构建了\",{\"1\":{\"16\":1}}],[\"采用了预训练和微调的学习方法\",{\"1\":{\"675\":1}}],[\"采用了\",{\"1\":{\"674\":1}}],[\"采用了更科学的数据配比\",{\"1\":{\"674\":1}}],[\"采用了一种更巧妙的解决思路\",{\"1\":{\"291\":1}}],[\"采用单任务微调\",{\"1\":{\"494\":1}}],[\"采用rmsnorm对子层输入归一化\",{\"1\":{\"481\":1}}],[\"采用kv\",{\"1\":{\"477\":1}}],[\"采用proximal\",{\"1\":{\"468\":1}}],[\"采用模型层间和矩阵级别的并行方式进行\",{\"1\":{\"461\":1}}],[\"采用无卷积的浅层线性投影直接将图像块\",{\"1\":{\"253\":1}}],[\"采用无检测器的图像编码器和文本编码器独立编码\",{\"1\":{\"149\":1}}],[\"采用经典的\",{\"1\":{\"214\":1}}],[\"采用加权随机采样而非直接取相似度最大的负样本\",{\"1\":{\"162\":1}}],[\"采用动量更新方式对\",{\"1\":{\"248\":1}}],[\"采用动量慢更新策略\",{\"1\":{\"147\":1}}],[\"采用动量编码器来生成表示\",{\"1\":{\"127\":1}}],[\"采用itc\",{\"1\":{\"147\":1}}],[\"采用itc和itm目标执行完微调后\",{\"1\":{\"146\":1}}],[\"采用moco的动量慢更新策略进行学习\",{\"1\":{\"145\":1}}],[\"采用bert分词器实现论文中的文本预处理\",{\"1\":{\"142\":1}}],[\"采用\",{\"1\":{\"127\":1,\"142\":1,\"190\":1,\"200\":2,\"203\":1,\"214\":1,\"219\":1,\"493\":1,\"494\":1,\"674\":3}}],[\"采样与训练\",{\"1\":{\"447\":1}}],[\"采样多个点\",{\"1\":{\"397\":1}}],[\"采样每个图像对应的负文本\",{\"1\":{\"162\":1}}],[\"采样每个文本对应的负图像\",{\"1\":{\"162\":1}}],[\"采样难负样本\",{\"1\":{\"145\":1}}],[\"采样的阈值\",{\"1\":{\"143\":1}}],[\"采样的关键点数量\",{\"1\":{\"92\":2}}],[\"采样点数量\",{\"1\":{\"92\":1}}],[\"采样半径\",{\"1\":{\"92\":1}}],[\"采样得到的关键点坐标\",{\"1\":{\"92\":1}}],[\"采样一些关键点\",{\"1\":{\"92\":1}}],[\"采样层\",{\"1\":{\"88\":1}}],[\"替换的是哪一个词\",{\"1\":{\"505\":1}}],[\"替换成以下内容\",{\"1\":{\"505\":1}}],[\"替换relu为swiglu\",{\"1\":{\"481\":1}}],[\"替换为伽马函数表达式\",{\"1\":{\"580\":1}}],[\"替换为随机单词\",{\"1\":{\"495\":1}}],[\"替换为随机\",{\"1\":{\"493\":1}}],[\"替换为\",{\"1\":{\"285\":2,\"493\":1,\"495\":1}}],[\"替换为特殊标记\",{\"1\":{\"155\":1}}],[\"替换为因果\",{\"1\":{\"126\":1}}],[\"替换\",{\"1\":{\"147\":2}}],[\"替代绝对位置编码\",{\"1\":{\"481\":1}}],[\"替代\",{\"1\":{\"73\":1}}],[\"位标注者进行判断\",{\"1\":{\"472\":1}}],[\"位整数\",{\"1\":{\"428\":2}}],[\"位或\",{\"1\":{\"428\":2}}],[\"位浮点数\",{\"1\":{\"428\":1}}],[\"位于原点\",{\"1\":{\"591\":1}}],[\"位于序列的开头\",{\"1\":{\"292\":1}}],[\"位于\",{\"1\":{\"126\":1}}],[\"位置的隐藏状态并池化\",{\"1\":{\"513\":1}}],[\"位置的\",{\"1\":{\"508\":1}}],[\"位置的标签设为\",{\"1\":{\"163\":1}}],[\"位置嵌入\",{\"1\":{\"447\":1,\"513\":2}}],[\"位置嵌入会被初始化为一组特定的值\",{\"1\":{\"293\":1}}],[\"位置顺序\",{\"1\":{\"363\":1}}],[\"位置参数\",{\"1\":{\"363\":2}}],[\"位置参数与关键字参数\",{\"0\":{\"363\":1}}],[\"位置处设置为\",{\"1\":{\"284\":1}}],[\"位置编码可以有效地捕捉输入序列中的相对位置信息\",{\"1\":{\"674\":1}}],[\"位置编码通常使用正弦和余弦函数生成\",{\"1\":{\"548\":1}}],[\"位置编码为可学习的矩阵\",{\"1\":{\"523\":1}}],[\"位置编码计算\",{\"1\":{\"477\":1}}],[\"位置编码的作用是为了记忆输入的语序信息\",{\"1\":{\"293\":1}}],[\"位置编码被添加到\",{\"1\":{\"292\":1}}],[\"位置编码\",{\"1\":{\"262\":1,\"293\":2,\"477\":1,\"481\":1,\"548\":1,\"674\":1}}],[\"位置记为\",{\"1\":{\"172\":1}}],[\"位置\",{\"1\":{\"92\":1}}],[\"单纯\",{\"1\":{\"681\":1}}],[\"单纯依赖人工标注和设计任务目标难以满足多任务学习的规模化需求\",{\"1\":{\"453\":1}}],[\"单个python文件\",{\"1\":{\"661\":1}}],[\"单元测试\",{\"0\":{\"645\":1}}],[\"单点集\",{\"1\":{\"566\":1}}],[\"单位\",{\"1\":{\"566\":1}}],[\"单词\",{\"1\":{\"511\":1}}],[\"单词同样合理甚至更好的替代词\",{\"1\":{\"157\":1}}],[\"单任务微调\",{\"1\":{\"499\":1}}],[\"单文档内句子打包\",{\"1\":{\"497\":1}}],[\"单样本学习\",{\"1\":{\"461\":1}}],[\"单样本\",{\"1\":{\"460\":1}}],[\"单一语言模型可处理翻译\",{\"1\":{\"455\":1}}],[\"单一的线性层只能进行线性变换\",{\"1\":{\"294\":1}}],[\"单领域的数据集训练模式\",{\"1\":{\"453\":1}}],[\"单层宽网络\",{\"1\":{\"395\":1}}],[\"单层网络依赖基函数的数量\",{\"1\":{\"395\":1}}],[\"单层网络\",{\"1\":{\"395\":4}}],[\"单隐藏层神经网络的输出形式为\",{\"1\":{\"395\":1}}],[\"单项式基\",{\"1\":{\"395\":1}}],[\"单独用来处理类别信息\",{\"1\":{\"292\":1}}],[\"单独使用\",{\"1\":{\"132\":1}}],[\"单模态或者多模态\",{\"1\":{\"677\":1}}],[\"单模态自注意力\",{\"1\":{\"283\":1}}],[\"单模态编码器\",{\"1\":{\"126\":1}}],[\"单尺度分组\",{\"1\":{\"93\":1}}],[\"单尺度分组分类模型\",{\"0\":{\"93\":1}}],[\"框架进行开发\",{\"1\":{\"687\":1}}],[\"框架交互\",{\"1\":{\"685\":1}}],[\"框架的核心库\",{\"1\":{\"685\":1}}],[\"框架可以实现数据感知和环境互动\",{\"1\":{\"682\":1}}],[\"框架是一个开源工具\",{\"1\":{\"682\":1}}],[\"框架核心功能的日益完善\",{\"1\":{\"665\":1}}],[\"框架按照\",{\"1\":{\"281\":1}}],[\"框架图如下\",{\"1\":{\"238\":1}}],[\"框架\",{\"0\":{\"442\":1},\"1\":{\"120\":1,\"464\":1,\"682\":2}}],[\"框架在多项评估指标上具有显著优势\",{\"1\":{\"26\":1}}],[\"剪切\",{\"1\":{\"117\":1}}],[\"弯曲\",{\"1\":{\"116\":1}}],[\"才结束词汇表的构建\",{\"1\":{\"410\":2}}],[\"才会有好的效果\",{\"1\":{\"297\":1}}],[\"才会将内容显示到屏幕上\",{\"1\":{\"83\":1}}],[\"才能继续计算其前向的梯度\",{\"1\":{\"655\":1}}],[\"才能计算导数\",{\"1\":{\"630\":1}}],[\"才能够满足业务的需求\",{\"1\":{\"415\":1}}],[\"才能通过\",{\"1\":{\"387\":1}}],[\"才能起作用\",{\"1\":{\"242\":1}}],[\"才能更好处理复杂多模态任务\",{\"1\":{\"221\":1}}],[\"才能保证整个网络输出与输入点的顺序无关\",{\"1\":{\"115\":1}}],[\"网页收集\",{\"1\":{\"131\":1}}],[\"网格点\",{\"1\":{\"397\":1}}],[\"网格特征\",{\"1\":{\"253\":1}}],[\"网格的视觉标记\",{\"1\":{\"170\":1}}],[\"网格的图像块\",{\"1\":{\"169\":1}}],[\"网格\",{\"1\":{\"114\":1,\"397\":1}}],[\"网络层封装\",{\"1\":{\"670\":1}}],[\"网络类型\",{\"1\":{\"395\":1}}],[\"网络\",{\"1\":{\"163\":1}}],[\"网络图文数据噪声多\",{\"1\":{\"149\":1}}],[\"网络输出由一个有限子集\",{\"1\":{\"105\":1}}],[\"网络结构特点\",{\"1\":{\"101\":1}}],[\"网络不是只捕获一个尺度上的局部特征\",{\"1\":{\"95\":1}}],[\"网络对于每个选定的形心点\",{\"1\":{\"95\":1}}],[\"网络对每一个点做低维到高维的映射\",{\"1\":{\"86\":1}}],[\"网络在训练时被呈现了不同稀疏度的点集\",{\"1\":{\"95\":1}}],[\"网络的每一组set\",{\"1\":{\"87\":1}}],[\"网络的分割和分类模型\",{\"1\":{\"87\":1}}],[\"适配应用任务\",{\"1\":{\"686\":1}}],[\"适配分类\",{\"1\":{\"188\":1}}],[\"适应特定任务\",{\"1\":{\"675\":1}}],[\"适应性强的智能系统\",{\"1\":{\"472\":1}}],[\"适应任务必须微调\",{\"1\":{\"464\":1}}],[\"适应扩展到所有线性层\",{\"1\":{\"428\":1}}],[\"适应不同任务需求\",{\"1\":{\"407\":1}}],[\"适用场景\",{\"1\":{\"404\":1}}],[\"适用于生产环境\",{\"1\":{\"684\":1}}],[\"适用于更复杂的应用场景\",{\"1\":{\"674\":1}}],[\"适用于只需要前向推理且不需要更新模型参数的场景\",{\"1\":{\"388\":1}}],[\"适用于复杂推理任务但依赖目标检测器和高分辨率图像\",{\"1\":{\"150\":1}}],[\"适用于分类\",{\"1\":{\"103\":1}}],[\"适用于\",{\"1\":{\"82\":1}}],[\"适用于连续响应值\",{\"1\":{\"82\":1}}],[\"适当降低训练目标反而可能取得更好的效果\",{\"1\":{\"278\":1}}],[\"适合相对稳定的数据\",{\"1\":{\"681\":1}}],[\"适合动态变化的数据\",{\"1\":{\"681\":1}}],[\"适合日常对话和基础任务场景\",{\"1\":{\"674\":1}}],[\"适合大规模优化但灵活性较低\",{\"1\":{\"662\":1}}],[\"适合无需梯度的场景\",{\"1\":{\"658\":1}}],[\"适合机器学习中损失函数的优化\",{\"1\":{\"626\":1}}],[\"适合用于构建对话助手\",{\"1\":{\"472\":1}}],[\"适合在推理阶段作为验证标准\",{\"1\":{\"403\":1}}],[\"适合渲染\",{\"1\":{\"114\":1}}],[\"适合\",{\"1\":{\"114\":1}}],[\"法律\",{\"1\":{\"463\":1}}],[\"法→英\",{\"1\":{\"455\":1}}],[\"法向量等属性\",{\"1\":{\"114\":1}}],[\"法向量\",{\"1\":{\"114\":1}}],[\"法线等\",{\"1\":{\"92\":1}}],[\"忽视局部邻域关系\",{\"1\":{\"112\":1}}],[\"忽略系数\",{\"1\":{\"590\":1}}],[\"忽略填充部分\",{\"1\":{\"513\":1}}],[\"忽略难分类样本\",{\"1\":{\"404\":1}}],[\"忽略标签为\",{\"1\":{\"163\":1}}],[\"忽略\",{\"1\":{\"163\":2}}],[\"忽略pad位loss\",{\"1\":{\"147\":1}}],[\"忽略局部结构\",{\"1\":{\"112\":1}}],[\"忽略局部结构信息\",{\"1\":{\"112\":1}}],[\"忽略了局部邻域之间的结构关系\",{\"1\":{\"112\":1}}],[\"易用\",{\"1\":{\"665\":1}}],[\"易混淆\",{\"0\":{\"542\":1}}],[\"易样本\",{\"1\":{\"404\":1}}],[\"易分类样本\",{\"1\":{\"404\":1}}],[\"易于扩展为检测\",{\"1\":{\"112\":1}}],[\"易受类别不平衡影响\",{\"1\":{\"402\":1}}],[\"易受\",{\"1\":{\"82\":1}}],[\"极端案例\",{\"1\":{\"455\":1}}],[\"极大地丰富了\",{\"1\":{\"685\":1}}],[\"极大地提升了开发效率和应用性能\",{\"1\":{\"684\":1}}],[\"极大地提升了模型在各种自然语言处理任务上的表现\",{\"1\":{\"673\":1}}],[\"极大地降低了在各种技术栈上构建\",{\"1\":{\"684\":1}}],[\"极大地降低成本\",{\"1\":{\"421\":1}}],[\"极大地简化了调试和问题排查的流程\",{\"1\":{\"684\":1}}],[\"极大\",{\"1\":{\"395\":1}}],[\"极其高效\",{\"1\":{\"112\":1}}],[\"极为重要\",{\"1\":{\"24\":1}}],[\"略逊于多视角\",{\"1\":{\"112\":1}}],[\"接触到更多的文本信息\",{\"1\":{\"674\":1}}],[\"接一个逐位置的前馈层来生成目标字符的分布输出\",{\"1\":{\"443\":1}}],[\"接一个全连接层\",{\"1\":{\"156\":1}}],[\"接收任意个数的位置参数\",{\"1\":{\"651\":1}}],[\"接收信息\",{\"1\":{\"596\":1}}],[\"接收函数并返回函数\",{\"1\":{\"367\":1}}],[\"接收函数作为参数\",{\"1\":{\"365\":1}}],[\"接收器操作特性\",{\"1\":{\"350\":1}}],[\"接收的文本\",{\"1\":{\"132\":1}}],[\"接着\",{\"1\":{\"440\":1}}],[\"接着是第\",{\"1\":{\"322\":2}}],[\"接着通过多头可供性链式思维\",{\"1\":{\"8\":1}}],[\"接下来就需要进行长期的用户体验跟踪\",{\"1\":{\"687\":1}}],[\"接下来就很简单了\",{\"1\":{\"235\":1}}],[\"接下来我们需要搭建前后端\",{\"1\":{\"687\":1}}],[\"接下来我们主要介绍几个国内外常见的大模型\",{\"1\":{\"674\":1}}],[\"接下来将使用tinypytorch验证是否能找到该最小值\",{\"1\":{\"667\":1}}],[\"接下来的阶段将进一步扩展tinypytorch\",{\"1\":{\"648\":1}}],[\"接下来介绍几种比较流行的peft微调方案\",{\"1\":{\"417\":1}}],[\"接下来\",{\"1\":{\"273\":1,\"420\":1,\"676\":1}}],[\"接近palm\",{\"1\":{\"482\":1}}],[\"接近人类水平\",{\"1\":{\"462\":1}}],[\"接近或超越部分任务的微调模型性能\",{\"1\":{\"460\":1}}],[\"接近1\",{\"1\":{\"404\":1}}],[\"接近0\",{\"1\":{\"404\":1}}],[\"接近\",{\"1\":{\"112\":1}}],[\"优质的\",{\"1\":{\"687\":1}}],[\"优先考虑精确性和推理步骤的正确性\",{\"1\":{\"674\":1}}],[\"优先于准确率\",{\"1\":{\"348\":1}}],[\"优缺点\",{\"1\":{\"404\":1}}],[\"优缺点对比\",{\"1\":{\"231\":1}}],[\"优点\",{\"1\":{\"231\":1,\"404\":1}}],[\"优于opt\",{\"1\":{\"482\":1,\"484\":1}}],[\"优于minerva\",{\"1\":{\"482\":1}}],[\"优于简单\",{\"1\":{\"472\":1}}],[\"优于\",{\"1\":{\"195\":2}}],[\"优化\",{\"1\":{\"665\":1}}],[\"优化打印等交互体验\",{\"1\":{\"663\":1}}],[\"优化正向传播实现\",{\"1\":{\"663\":1}}],[\"优化问题中常使用特定函数评估算法性能\",{\"1\":{\"662\":1}}],[\"优化效果总结\",{\"1\":{\"658\":1}}],[\"优化反向传播的内存消耗\",{\"1\":{\"658\":1}}],[\"优化内存消耗\",{\"0\":{\"658\":1}}],[\"优化后的内存管理确保框架在处理大规模计算时的稳定性和效率\",{\"1\":{\"657\":1}}],[\"优化后的roberta在多个基准测试\",{\"1\":{\"491\":1}}],[\"优化了反向传播的实现\",{\"1\":{\"648\":1}}],[\"优化策略\",{\"1\":{\"493\":1}}],[\"优化llama\",{\"1\":{\"483\":1}}],[\"优化人类偏好\",{\"1\":{\"468\":1}}],[\"优化难度\",{\"1\":{\"395\":1}}],[\"优化不同分辨率输入\",{\"1\":{\"212\":1}}],[\"优化多模态对话能力\",{\"1\":{\"190\":1}}],[\"优化器与\",{\"1\":{\"670\":1}}],[\"优化器与学习率调度器\",{\"1\":{\"514\":1}}],[\"优化器改进\",{\"1\":{\"494\":1}}],[\"优化器方案\",{\"1\":{\"447\":1}}],[\"优化器使用\",{\"1\":{\"131\":1,\"200\":1,\"201\":1}}],[\"优化器设置\",{\"1\":{\"80\":1}}],[\"优化器\",{\"1\":{\"22\":1,\"80\":1,\"481\":1,\"493\":1,\"670\":1}}],[\"优势\",{\"1\":{\"112\":1,\"401\":1,\"402\":1,\"407\":1,\"495\":1}}],[\"面向大语言模型的检索增强生成技术\",{\"1\":{\"688\":1}}],[\"面对复杂问题时\",{\"1\":{\"679\":1}}],[\"面对大量噪声点时效果较差\",{\"1\":{\"112\":1}}],[\"面临的主要问题有\",{\"1\":{\"679\":1}}],[\"面临\",{\"1\":{\"149\":1}}],[\"面部发生形变\",{\"1\":{\"112\":1}}],[\"尤其适合研究和快速开发场景\",{\"1\":{\"662\":1}}],[\"尤其当处理大规模数据时\",{\"1\":{\"657\":1}}],[\"尤其对于多变量函数\",{\"1\":{\"623\":1}}],[\"尤其对\",{\"1\":{\"397\":1}}],[\"尤其是情商方面异常优秀\",{\"1\":{\"674\":1}}],[\"尤其是few\",{\"1\":{\"461\":1}}],[\"尤其是\",{\"1\":{\"444\":1}}],[\"尤其是目标检测\",{\"1\":{\"404\":1}}],[\"尤其是零样本\",{\"1\":{\"224\":1,\"231\":1}}],[\"尤其是在长文本任务中\",{\"1\":{\"463\":1}}],[\"尤其是在\",{\"1\":{\"268\":1}}],[\"尤其是在低层次上对每个质心点运行局部pointnet时\",{\"1\":{\"97\":1}}],[\"尤其是在预测值接近极端值\",{\"1\":{\"78\":1}}],[\"尤其在解决复杂任务时表现出了惊人的潜力\",{\"1\":{\"673\":1}}],[\"尤其在神经网络训练中\",{\"1\":{\"657\":1}}],[\"尤其在军事\",{\"1\":{\"472\":1}}],[\"尤其在有\",{\"1\":{\"471\":1}}],[\"尤其在few\",{\"1\":{\"462\":1}}],[\"尤其在anli这种对抗性构建的数据集上\",{\"1\":{\"462\":1}}],[\"尤其在coqa中few\",{\"1\":{\"462\":1}}],[\"尤其在翻译为英语的方向上\",{\"1\":{\"462\":1}}],[\"尤其在triviaqa中\",{\"1\":{\"462\":1}}],[\"尤其在目标区域较小\",{\"1\":{\"401\":1}}],[\"尤其在ocr相关任务\",{\"1\":{\"208\":1,\"222\":1}}],[\"尤其在\",{\"1\":{\"207\":1}}],[\"尤其在图像分类与部分\",{\"1\":{\"123\":1}}],[\"尤其未训练时\",{\"1\":{\"112\":1}}],[\"十二亿\",{\"1\":{\"297\":1}}],[\"十亿\",{\"1\":{\"297\":1}}],[\"十一\",{\"1\":{\"112\":1}}],[\"十\",{\"1\":{\"112\":1}}],[\"又进一步发布了\",{\"1\":{\"674\":1}}],[\"又称多元正态分布\",{\"1\":{\"589\":1}}],[\"又称为\",{\"1\":{\"401\":1}}],[\"又是一个有实际意义的字或词\",{\"1\":{\"508\":1}}],[\"又怎么能够利用这些开源的大模型\",{\"1\":{\"424\":1}}],[\"又能拆解未知词的子词词汇表\",{\"1\":{\"409\":1}}],[\"又会导致计算资源浪费\",{\"1\":{\"112\":1}}],[\"又理解整体结构\",{\"1\":{\"105\":1}}],[\"太大\",{\"1\":{\"112\":1}}],[\"太大则可能导致不相关的点增多\",{\"1\":{\"90\":1}}],[\"九\",{\"0\":{\"339\":1},\"1\":{\"112\":1}}],[\"八百六十万\",{\"1\":{\"297\":1}}],[\"八\",{\"0\":{\"338\":1},\"1\":{\"112\":1}}],[\"七\",{\"0\":{\"319\":1,\"337\":1},\"1\":{\"112\":1}}],[\"六\",{\"0\":{\"316\":1,\"336\":1},\"1\":{\"112\":1}}],[\"拉回正态\",{\"1\":{\"359\":1}}],[\"拉伸等会导致形变的操作\",{\"1\":{\"117\":1}}],[\"拉伸等形变\",{\"1\":{\"112\":1}}],[\"拉伸\",{\"1\":{\"112\":1,\"116\":1}}],[\"拉平数据\",{\"1\":{\"359\":1}}],[\"拉平\",{\"1\":{\"59\":1}}],[\"旋转位置编码\",{\"1\":{\"674\":1}}],[\"旋转\",{\"1\":{\"112\":1,\"116\":2,\"117\":2}}],[\"五\",{\"0\":{\"315\":1,\"335\":1},\"1\":{\"112\":1}}],[\"体验优化\",{\"1\":{\"687\":1}}],[\"体验bert的预训练过程是如何实现的\",{\"1\":{\"509\":1}}],[\"体会它们的有趣之处\",{\"1\":{\"604\":1}}],[\"体积\",{\"1\":{\"592\":1}}],[\"体重\",{\"1\":{\"355\":2}}],[\"体现了自动微分的优势\",{\"1\":{\"662\":1}}],[\"体现了全词掩码在预训练时以及图像增强在微调时的重要性\",{\"1\":{\"253\":1}}],[\"体现出模型对语义的灵活解析能力\",{\"1\":{\"25\":1}}],[\"体现出数据集在样本分布上的全面性和均衡性\",{\"1\":{\"19\":1}}],[\"体素网格\",{\"1\":{\"114\":1}}],[\"体素\",{\"1\":{\"112\":1}}],[\"四种任务设定方法的比较\",{\"1\":{\"461\":1}}],[\"四个残差块和一个输出层\",{\"1\":{\"392\":1}}],[\"四个指标协同工作\",{\"1\":{\"82\":1}}],[\"四个指标对比总结\",{\"1\":{\"82\":1}}],[\"四阶张量的例子\",{\"1\":{\"328\":1}}],[\"四\",{\"0\":{\"311\":1,\"334\":1,\"669\":1},\"1\":{\"112\":1,\"469\":1}}],[\"🧮\",{\"0\":{\"649\":1}}],[\"🤔\",{\"1\":{\"635\":1}}],[\"🧪\",{\"1\":{\"112\":1,\"542\":1}}],[\"🧩\",{\"1\":{\"112\":2,\"542\":1}}],[\"🧱\",{\"1\":{\"112\":3}}],[\"🧠\",{\"0\":{\"664\":1},\"1\":{\"108\":1,\"112\":1,\"115\":1}}],[\"曲率等细节\",{\"1\":{\"112\":1}}],[\"曲率\",{\"1\":{\"112\":1}}],[\"曲线上最接近\",{\"1\":{\"353\":1}}],[\"曲线为从\",{\"1\":{\"351\":1}}],[\"曲线的方法是\",{\"1\":{\"350\":1}}],[\"曲线直观地显示了所有阈值下的模型性能\",{\"1\":{\"350\":1}}],[\"曲线和\",{\"0\":{\"349\":1}}],[\"曲线\",{\"1\":{\"82\":1}}],[\"曲线下的面积\",{\"1\":{\"82\":1}}],[\"曲线下面积较大的模型通常是更好的模型\",{\"1\":{\"353\":1}}],[\"曲线下面积\",{\"0\":{\"351\":1},\"1\":{\"82\":1,\"351\":1}}],[\"缺点\",{\"1\":{\"231\":1,\"259\":1,\"404\":1}}],[\"缺乏结构化监督利用能力\",{\"1\":{\"463\":1}}],[\"缺乏可控性与鲁棒性\",{\"1\":{\"463\":1}}],[\"缺乏鲁棒的系统性泛化能力\",{\"1\":{\"463\":1}}],[\"缺乏具体的上下文\",{\"1\":{\"274\":1}}],[\"缺乏语言对齐\",{\"1\":{\"188\":1}}],[\"缺乏与llms的直接特征兼容性\",{\"1\":{\"183\":1}}],[\"缺乏预定义词汇\",{\"1\":{\"166\":1}}],[\"缺乏精细建模\",{\"1\":{\"112\":1}}],[\"缺乏层次化\",{\"1\":{\"112\":1}}],[\"缺乏层次化特征提取机制\",{\"1\":{\"112\":1}}],[\"缺乏动态上下文感知\",{\"1\":{\"112\":1}}],[\"缺陷对比\",{\"1\":{\"112\":1}}],[\"缺陷类型\",{\"1\":{\"112\":1}}],[\"缺陷\",{\"0\":{\"112\":1},\"1\":{\"112\":1,\"402\":1}}],[\"缺失\",{\"1\":{\"24\":1}}],[\"座位\",{\"1\":{\"111\":1}}],[\"控制反向传播是否启用\",{\"1\":{\"658\":1}}],[\"控制手段\",{\"1\":{\"471\":1}}],[\"控制交叉熵损失和\",{\"1\":{\"407\":1}}],[\"控制交叉熵中正负样本的权重\",{\"1\":{\"407\":1}}],[\"控制两个损失之间的权重比例\",{\"1\":{\"407\":1}}],[\"控制区域匹配误差的重要性\",{\"1\":{\"407\":1}}],[\"控制分类误差的重要性\",{\"1\":{\"407\":1}}],[\"控制假阴性\",{\"1\":{\"405\":1}}],[\"控制假阳性\",{\"1\":{\"405\":1}}],[\"控制正类\",{\"1\":{\"404\":1}}],[\"控制正样本\",{\"1\":{\"404\":1}}],[\"控制难易样本权重\",{\"1\":{\"404\":1}}],[\"控制难易样本的权重衰减程度\",{\"1\":{\"404\":1}}],[\"控制了相似度计算的维度\",{\"1\":{\"313\":1}}],[\"控制输出格式\",{\"1\":{\"163\":1}}],[\"控制\",{\"1\":{\"163\":1,\"238\":1,\"407\":1,\"471\":1}}],[\"控制动量蒸馏信号的强度\",{\"1\":{\"157\":1}}],[\"控制是否对\",{\"1\":{\"401\":1}}],[\"控制是否启用\",{\"1\":{\"262\":1}}],[\"控制是否使用\",{\"1\":{\"109\":1}}],[\"控制是否输出全局特征\",{\"1\":{\"109\":1}}],[\"控制每个\",{\"1\":{\"29\":1}}],[\"添加shape\",{\"1\":{\"659\":1}}],[\"添加retain\",{\"1\":{\"658\":1}}],[\"添加元素\",{\"1\":{\"657\":1}}],[\"添加特殊token标记\",{\"1\":{\"520\":1}}],[\"添加batch维度\",{\"1\":{\"477\":1}}],[\"添加一个无监督训练目标是半监督学习的一种替代形式\",{\"1\":{\"441\":1}}],[\"添加一些特殊词\",{\"1\":{\"410\":1}}],[\"添加一维位置编码和二维位置编码并没有太大的差异\",{\"1\":{\"293\":1}}],[\"添加位置编码\",{\"0\":{\"293\":1}}],[\"添加到\",{\"1\":{\"286\":1}}],[\"添加反卷积层作为解码器\",{\"1\":{\"175\":1}}],[\"添加\",{\"0\":{\"292\":1},\"1\":{\"108\":1}}],[\"添加更多上下文细节\",{\"1\":{\"63\":1}}],[\"❗而只有正交矩阵才能表示刚性变换\",{\"1\":{\"108\":1}}],[\"稳定的\",{\"1\":{\"107\":1}}],[\"摆正\",{\"1\":{\"107\":1,\"109\":1}}],[\"摆脱几何标注和固定场景限制\",{\"1\":{\"51\":1}}],[\"变异系数\",{\"1\":{\"593\":1}}],[\"变大\",{\"1\":{\"387\":1}}],[\"变量和函数\",{\"1\":{\"666\":1}}],[\"变量和函数中会设置好\",{\"1\":{\"656\":1}}],[\"变量实例可直接访问ndarray的核心属性\",{\"1\":{\"659\":1}}],[\"变量名称可在计算图可视化等场景中显示\",{\"1\":{\"659\":1}}],[\"变量的作用是存储数据\",{\"1\":{\"606\":1}}],[\"变量的基本概念\",{\"0\":{\"606\":1}}],[\"变量是tinypytorch最重要的组成部分\",{\"1\":{\"606\":1}}],[\"变量\",{\"1\":{\"366\":1,\"596\":1}}],[\"变为\",{\"1\":{\"283\":3}}],[\"变分自编码器视角\",{\"0\":{\"173\":1}}],[\"变形\",{\"1\":{\"116\":1}}],[\"变换后的输出维度\",{\"1\":{\"477\":1}}],[\"变换后再用欧几里得距离度量\",{\"1\":{\"359\":1}}],[\"变换不变性\",{\"1\":{\"112\":1}}],[\"变换矩阵会通过\",{\"1\":{\"107\":1}}],[\"变成1维度之后就成了50176\",{\"1\":{\"291\":1}}],[\"变成\",{\"1\":{\"92\":2,\"111\":1,\"419\":1,\"544\":1}}],[\"变成特征向量\",{\"1\":{\"87\":1}}],[\"决定了输出的信息维度\",{\"1\":{\"313\":1}}],[\"决定\",{\"1\":{\"105\":1,\"395\":1}}],[\"小型语言模型通常难以解决涉及多个推理步骤的复杂任务\",{\"1\":{\"676\":1}}],[\"小而全\",{\"1\":{\"670\":1}}],[\"小模型\",{\"1\":{\"482\":1}}],[\"小模型+更多数据训练可能更优\",{\"1\":{\"480\":1}}],[\"小数据集没有\",{\"1\":{\"449\":1}}],[\"小批次样本\",{\"1\":{\"447\":1}}],[\"小孩子根据从示例中学习到的推理\",{\"1\":{\"434\":1}}],[\"小块\",{\"1\":{\"396\":1}}],[\"小结\",{\"0\":{\"129\":1,\"267\":1,\"278\":1}}],[\"小扰动不会改变函数输出\",{\"1\":{\"105\":1}}],[\"小红书\",{\"1\":{\"0\":1}}],[\"学多少\",{\"1\":{\"563\":1,\"595\":1}}],[\"学术数据不足限制mmlu表现\",{\"1\":{\"482\":1}}],[\"学会\",{\"1\":{\"463\":1}}],[\"学习通用的语言表示和知识\",{\"1\":{\"675\":1}}],[\"学习到输入数据的非线性特征\",{\"1\":{\"294\":1}}],[\"学习到的特征在下游任务上具有很好的迁移性\",{\"1\":{\"237\":1}}],[\"学习算法需要在所有可能的函数空间中搜索最优模型\",{\"1\":{\"287\":1}}],[\"学习目标\",{\"1\":{\"285\":1}}],[\"学习目标实现过程\",{\"1\":{\"161\":1,\"162\":1,\"163\":1}}],[\"学习的图像分词器\",{\"1\":{\"170\":1}}],[\"学习\",{\"1\":{\"166\":1,\"240\":1,\"665\":1}}],[\"学习温度系数\",{\"1\":{\"160\":1}}],[\"学习共同低维空间\",{\"1\":{\"149\":1}}],[\"学习大模型\",{\"1\":{\"123\":1}}],[\"学习率调整\",{\"1\":{\"494\":1}}],[\"学习率调度器\",{\"1\":{\"80\":1}}],[\"学习率调度器初始化等\",{\"1\":{\"80\":1}}],[\"学习率采用线性预热\",{\"1\":{\"493\":1}}],[\"学习率采用余弦退火策略\",{\"1\":{\"200\":1}}],[\"学习率预热后分别达到\",{\"1\":{\"131\":1}}],[\"学习率\",{\"1\":{\"10\":1,\"22\":1,\"174\":1}}],[\"学生模型不能由教师模型直接初始化\",{\"1\":{\"137\":1}}],[\"学生\",{\"0\":{\"586\":1},\"1\":{\"123\":1,\"586\":1}}],[\"学到的是一个关键点集合\",{\"1\":{\"112\":1}}],[\"学到的是一个\",{\"1\":{\"105\":1}}],[\"矩阵维度\",{\"1\":{\"517\":1}}],[\"矩阵又不是\",{\"1\":{\"426\":1}}],[\"矩阵a和b为什么不能同时为零\",{\"0\":{\"426\":1}}],[\"矩阵初始化\",{\"1\":{\"425\":2,\"426\":1}}],[\"矩阵从\",{\"1\":{\"423\":1}}],[\"矩阵w就是通过机器学习\",{\"1\":{\"414\":1}}],[\"矩阵中的对角线元素\",{\"1\":{\"272\":1}}],[\"矩阵所有元素平方和开方\",{\"1\":{\"108\":1}}],[\"矩阵返回\",{\"1\":{\"107\":1}}],[\"矩阵\",{\"1\":{\"105\":2,\"309\":1,\"355\":1,\"425\":1,\"426\":2}}],[\"比\",{\"1\":{\"469\":1,\"674\":1}}],[\"比原sota高出18\",{\"1\":{\"462\":1}}],[\"比原本少了一个多头自注意力\",{\"1\":{\"443\":1}}],[\"比此前最大的非稀疏语言模型大10倍\",{\"1\":{\"460\":1}}],[\"比许多情况下的ensemble模型要好\",{\"1\":{\"448\":1}}],[\"比single\",{\"1\":{\"448\":1}}],[\"比单层电路更高效\",{\"1\":{\"395\":1}}],[\"比较它们在iou\",{\"1\":{\"408\":1}}],[\"比较两个分类名词是否相等\",{\"1\":{\"275\":1}}],[\"比较了两种文本生成方式\",{\"1\":{\"133\":1}}],[\"比基于网格特征的模型快至少四倍的推理速度\",{\"1\":{\"253\":1}}],[\"比例进行的掩码\",{\"1\":{\"514\":1}}],[\"比例等超参数组合上进行对比试验\",{\"1\":{\"196\":1}}],[\"比例\",{\"1\":{\"160\":1}}],[\"比排序\",{\"1\":{\"105\":1}}],[\"比如后续我们会将搭建检索问答链来完成检索问答\",{\"1\":{\"683\":1}}],[\"比如只有两个互斥事件\",{\"1\":{\"567\":1}}],[\"比如你能谈\",{\"1\":{\"566\":1}}],[\"比如我们可能想表示这些事件\",{\"1\":{\"566\":1}}],[\"比如测量一个时间\",{\"1\":{\"566\":1}}],[\"比如取到了问题部分的内容\",{\"1\":{\"542\":1}}],[\"比如对于一个长度为\",{\"1\":{\"540\":1}}],[\"比如上面的例子中\",{\"1\":{\"505\":1}}],[\"比如它看到的\",{\"1\":{\"505\":1}}],[\"比如文本蕴涵\",{\"1\":{\"439\":1}}],[\"比如relu\",{\"1\":{\"395\":1}}],[\"比如这个函数就是高阶函数\",{\"1\":{\"365\":1}}],[\"比如身高\",{\"1\":{\"355\":1}}],[\"比如在\",{\"1\":{\"326\":1}}],[\"比如在内存中从一个索引移动到另一个索引时\",{\"1\":{\"323\":1}}],[\"比如多头注意力中可以分别控制每个\",{\"1\":{\"313\":1}}],[\"比如一张224x224的图片\",{\"1\":{\"291\":1}}],[\"比如将\",{\"1\":{\"258\":1}}],[\"比如字典大小是\",{\"1\":{\"242\":1}}],[\"比如标注数据很少的任务\",{\"1\":{\"238\":1}}],[\"比如几万\",{\"1\":{\"238\":1}}],[\"比如bert最后进行的softmax操作\",{\"1\":{\"238\":1}}],[\"比如说在视频领域\",{\"1\":{\"235\":1}}],[\"比如说nceloss\",{\"1\":{\"235\":1}}],[\"比如边缘\",{\"1\":{\"112\":1}}],[\"比如椅子的腿\",{\"1\":{\"111\":1}}],[\"比如椅子朝向不同\",{\"1\":{\"107\":1}}],[\"比如法线\",{\"1\":{\"92\":1}}],[\"比如通过\",{\"1\":{\"92\":1}}],[\"比如颜色\",{\"1\":{\"92\":1}}],[\"比如\",{\"1\":{\"29\":1,\"195\":1,\"240\":1,\"326\":2,\"389\":2,\"396\":1,\"397\":2,\"511\":1,\"566\":5}}],[\"效率突破\",{\"1\":{\"482\":1}}],[\"效率优化\",{\"1\":{\"481\":1}}],[\"效率低\",{\"1\":{\"477\":1}}],[\"效率比较低\",{\"1\":{\"416\":1}}],[\"效率\",{\"1\":{\"112\":1}}],[\"效率极低\",{\"1\":{\"76\":1}}],[\"效果可能不太理想\",{\"1\":{\"679\":1}}],[\"效果评估\",{\"1\":{\"514\":1}}],[\"效果会比不加这句话要好\",{\"1\":{\"433\":1}}],[\"效果相当惊人\",{\"1\":{\"421\":1}}],[\"效果已经非常好了\",{\"1\":{\"421\":1}}],[\"效果越显著\",{\"1\":{\"404\":1}}],[\"效果对比\",{\"0\":{\"297\":1}}],[\"效果验证\",{\"0\":{\"132\":1}}],[\"效果\",{\"0\":{\"515\":1},\"1\":{\"105\":3,\"190\":3,\"228\":2,\"687\":1}}],[\"整个互联网一样\",{\"1\":{\"673\":1}}],[\"整个样本空间的概率为\",{\"1\":{\"567\":1}}],[\"整个样本空间\",{\"1\":{\"566\":1}}],[\"整个句子\",{\"1\":{\"547\":1}}],[\"整个过程中\",{\"1\":{\"508\":1}}],[\"整个注意力的\",{\"1\":{\"477\":1}}],[\"整个函数\",{\"1\":{\"105\":1}}],[\"整数子集上的一些离散型概率分布\",{\"1\":{\"574\":1}}],[\"整数\",{\"1\":{\"291\":2}}],[\"整体得分72\",{\"1\":{\"448\":1}}],[\"整体模型还是比较大的\",{\"1\":{\"300\":1}}],[\"整体结构图如下\",{\"1\":{\"300\":1}}],[\"整体流程总览\",{\"0\":{\"262\":1}}],[\"整体流程和albef模型实现一致\",{\"1\":{\"147\":1}}],[\"整体来看\",{\"1\":{\"129\":1}}],[\"整理\",{\"1\":{\"33\":1}}],[\"γ\",{\"1\":{\"105\":2,\"404\":3,\"470\":1}}],[\"≈\",{\"1\":{\"105\":1,\"159\":2}}],[\"聚焦难分类样本\",{\"1\":{\"404\":1}}],[\"聚焦参数\",{\"1\":{\"78\":1,\"404\":2}}],[\"聚合了所有\",{\"1\":{\"310\":1}}],[\"聚合所有点的信息\",{\"1\":{\"105\":1,\"112\":1}}],[\"稀疏性强\",{\"1\":{\"114\":1}}],[\"稀疏点云等任务中表现受限\",{\"1\":{\"112\":1}}],[\"稀疏点云下性能差\",{\"1\":{\"112\":1}}],[\"稀疏\",{\"1\":{\"104\":1}}],[\"难于训练\",{\"1\":{\"424\":1}}],[\"难样本\",{\"1\":{\"404\":1}}],[\"难样本vs易样\",{\"1\":{\"404\":1}}],[\"难分类样本\",{\"1\":{\"404\":1}}],[\"难负样本\",{\"1\":{\"162\":1}}],[\"难负样本采样\",{\"1\":{\"145\":1}}],[\"难以覆盖广泛的语言任务\",{\"1\":{\"460\":1}}],[\"难以学习有效特征\",{\"1\":{\"404\":1}}],[\"难以局部修正\",{\"1\":{\"395\":1}}],[\"难以提升模型泛化能力\",{\"1\":{\"133\":1}}],[\"难以自动构建\",{\"1\":{\"114\":1}}],[\"难以用\",{\"1\":{\"114\":1}}],[\"难以建模更高维度的空间关系\",{\"1\":{\"112\":1}}],[\"难以捕捉非刚性变换下的不变性\",{\"1\":{\"112\":1}}],[\"难以区分语义相近但位置不同的区域\",{\"1\":{\"112\":1}}],[\"难以泛化到未见过的功能\",{\"1\":{\"6\":1}}],[\"难点\",{\"0\":{\"104\":1},\"1\":{\"105\":4}}],[\"避免内存长期占用\",{\"1\":{\"658\":1}}],[\"避免内存泄漏\",{\"1\":{\"657\":1}}],[\"避免直接引用导致循环\",{\"1\":{\"657\":1}}],[\"避免增加对象的引用计数\",{\"1\":{\"657\":1}}],[\"避免强引用导致的内存滞留\",{\"1\":{\"657\":1}}],[\"避免栈溢出\",{\"1\":{\"639\":1}}],[\"避免重复计算或存储额外数据\",{\"1\":{\"630\":1}}],[\"避免过拟合\",{\"1\":{\"495\":1}}],[\"避免过度拉伸\",{\"1\":{\"216\":1}}],[\"避免存储注意力权重\",{\"1\":{\"481\":1}}],[\"避免偏见与毒性\",{\"1\":{\"470\":1}}],[\"避免生成非结尾词\",{\"1\":{\"455\":1}}],[\"避免与webtext重叠\",{\"1\":{\"455\":1}}],[\"避免传统\",{\"1\":{\"454\":1}}],[\"避免复杂的编码需求\",{\"1\":{\"428\":1}}],[\"避免单一损失可能带来的训练不稳定性\",{\"1\":{\"407\":1}}],[\"避免训练震荡\",{\"1\":{\"402\":1}}],[\"避免为每个像素单独建模\",{\"1\":{\"395\":1}}],[\"避免使用不平衡的数据集\",{\"1\":{\"347\":1}}],[\"避免了手工设计基函数的局限性\",{\"1\":{\"395\":1}}],[\"避免了只通过单词上下文进行预测\",{\"1\":{\"258\":1}}],[\"避免了复杂的预处理\",{\"1\":{\"103\":1}}],[\"避免罕见组合\",{\"1\":{\"226\":1}}],[\"避免模型过拟合于极端负样本或伪负样本\",{\"1\":{\"162\":1}}],[\"避免采样到自己\",{\"1\":{\"162\":1}}],[\"避免因忘记重置配置导致的错误\",{\"1\":{\"658\":1}}],[\"避免因gc延迟导致的内存问题\",{\"1\":{\"657\":1}}],[\"避免因\",{\"1\":{\"157\":1}}],[\"避免选到自己\",{\"1\":{\"147\":1}}],[\"避免除以零\",{\"1\":{\"100\":1}}],[\"只开源了\",{\"1\":{\"674\":1}}],[\"只能实现一条竖线形状的计算图结构的反向传播\",{\"1\":{\"635\":1}}],[\"只能对输入文本中的\",{\"1\":{\"542\":1}}],[\"只能处理刚性变换\",{\"1\":{\"112\":1}}],[\"只做抽取式问答\",{\"1\":{\"542\":1}}],[\"只计算在第\",{\"1\":{\"505\":1}}],[\"只计算新\",{\"1\":{\"477\":1}}],[\"只为当前传入的词生成位置序列\",{\"1\":{\"477\":1}}],[\"只展示核心代码\",{\"1\":{\"477\":1}}],[\"只达到了56\",{\"1\":{\"448\":1}}],[\"只不过是对每个字都要预测一个类别\",{\"1\":{\"508\":1}}],[\"只不过它们的具体实现上有一些差异\",{\"1\":{\"419\":1}}],[\"只不过类别和所有样本相比\",{\"1\":{\"240\":1}}],[\"只适用于连续内存的张量\",{\"1\":{\"384\":1}}],[\"只交换两个指定维度\",{\"1\":{\"382\":1}}],[\"只出现了几次\",{\"1\":{\"342\":1}}],[\"只包含\",{\"1\":{\"331\":1,\"542\":1}}],[\"只需优化训练策略即可达到sota\",{\"1\":{\"500\":1}}],[\"只需提供极少量的示例就能成功完成\",{\"1\":{\"462\":1}}],[\"只需将这些步长交换为\",{\"1\":{\"326\":1}}],[\"只需要去除cls\",{\"1\":{\"529\":1}}],[\"只需要计算当前轮新增\",{\"1\":{\"477\":1}}],[\"只需要计算\",{\"1\":{\"425\":1}}],[\"只需要微调\",{\"1\":{\"425\":1}}],[\"只需要48gb\",{\"1\":{\"421\":1}}],[\"只需要用一个一个linear即可\",{\"1\":{\"296\":1}}],[\"只需要将图像调整到合适的大小\",{\"1\":{\"290\":1}}],[\"只需要\",{\"1\":{\"242\":1}}],[\"只有一个自由参数\",{\"1\":{\"590\":2}}],[\"只有一个\",{\"1\":{\"477\":1}}],[\"只有一个特殊标记\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"只有2490样本\",{\"1\":{\"448\":1}}],[\"只有\",{\"1\":{\"425\":1,\"590\":1,\"674\":2}}],[\"只有原始维度\",{\"1\":{\"387\":1}}],[\"只有遇到换行符或者输出内容积累到一定大小时\",{\"1\":{\"83\":1}}],[\"只是一个占位符而已\",{\"1\":{\"508\":1}}],[\"只是在推理的过程中\",{\"1\":{\"419\":1}}],[\"只是侧重点不一样\",{\"1\":{\"416\":1}}],[\"只是访问方式变了\",{\"1\":{\"326\":1}}],[\"只是结构上简化了多模态学习\",{\"1\":{\"259\":1}}],[\"只是建立中间模型的一种方式\",{\"1\":{\"238\":1}}],[\"只是参数经过调整以更好响应指令\",{\"1\":{\"224\":1}}],[\"只训练降维矩阵\",{\"1\":{\"425\":1}}],[\"只训练最后的全连接层\",{\"1\":{\"237\":1}}],[\"只训练少量\",{\"1\":{\"231\":1}}],[\"只要其隐藏层神经元的数量足够\",{\"1\":{\"395\":1}}],[\"只要你能够得到一个判断正样本和负样本的规律\",{\"1\":{\"235\":1}}],[\"只要关键点还在\",{\"1\":{\"105\":1}}],[\"只更新投影层\",{\"1\":{\"227\":1}}],[\"只保留一个\",{\"1\":{\"654\":1}}],[\"只保留最多\",{\"1\":{\"226\":1}}],[\"只保留生成部分\",{\"1\":{\"143\":1}}],[\"只在这些\",{\"1\":{\"566\":1}}],[\"只在\",{\"1\":{\"163\":1}}],[\"只对低秩矩阵\",{\"1\":{\"423\":1}}],[\"只对被\",{\"1\":{\"163\":1}}],[\"只对应一个\",{\"1\":{\"145\":1}}],[\"只改变物体的方向\",{\"1\":{\"117\":1}}],[\"只改变位置和朝向\",{\"1\":{\"116\":1}}],[\"只关注全局结构\",{\"1\":{\"112\":1}}],[\"只学正交变换\",{\"1\":{\"112\":1}}],[\"只通过\",{\"1\":{\"112\":1}}],[\"只取cls\",{\"1\":{\"145\":2}}],[\"只取\",{\"1\":{\"101\":1}}],[\"地板等\",{\"1\":{\"101\":1}}],[\"那它们的并集和交集也得能谈\",{\"1\":{\"566\":1}}],[\"那它的补集你也得能谈\",{\"1\":{\"566\":1}}],[\"那\",{\"1\":{\"566\":2}}],[\"那就需要使用生成式模型\",{\"1\":{\"542\":1}}],[\"那就一起用\",{\"1\":{\"100\":1}}],[\"那不就矛盾了吗\",{\"1\":{\"508\":1}}],[\"那我们就认为\",{\"1\":{\"508\":1}}],[\"那很有可能就是我们最终的答案\",{\"1\":{\"435\":1}}],[\"那为什么还要有个qlora呢\",{\"1\":{\"421\":1}}],[\"那个图片的特征\",{\"1\":{\"240\":1}}],[\"那么值减少最快的方向是\",{\"1\":{\"667\":1}}],[\"那么f\",{\"1\":{\"652\":1}}],[\"那么function可能会放大或者缩小x对输出y大小变化的影响\",{\"1\":{\"626\":1}}],[\"那么此时影响因子r=\",{\"1\":{\"626\":1}}],[\"那么影响因子r恒为1\",{\"1\":{\"626\":1}}],[\"那么采样出来的图像应该也接近灰色\",{\"1\":{\"594\":1}}],[\"那么一共有\",{\"1\":{\"600\":1}}],[\"那么一个事件的发生不会改变另一个事件的概率\",{\"1\":{\"568\":1}}],[\"那么一致性是如何做到的\",{\"1\":{\"238\":1}}],[\"那么对于任意事件\",{\"1\":{\"569\":1}}],[\"那么对应的就是单词\",{\"1\":{\"542\":1}}],[\"那么对比学习的过程\",{\"1\":{\"238\":1}}],[\"那么可以组合这两个索引得到答案\",{\"1\":{\"540\":1}}],[\"那么它从\",{\"1\":{\"514\":2}}],[\"那么这个\",{\"1\":{\"508\":1}}],[\"那么上面的梯度就变成了\",{\"1\":{\"426\":1}}],[\"那么矩阵\",{\"1\":{\"426\":1}}],[\"那么那些小公司或者个人\",{\"1\":{\"424\":1}}],[\"那么prompt\",{\"1\":{\"418\":1}}],[\"那么搭建自己的大模型就非常必要\",{\"1\":{\"415\":1}}],[\"那么协方差矩阵就是\",{\"1\":{\"359\":1}}],[\"那么前者的变化会主导整个距离\",{\"1\":{\"359\":1}}],[\"那么最终输出为\",{\"1\":{\"315\":1}}],[\"那么作者就想把注意力得到的结果\",{\"1\":{\"298\":1}}],[\"那么就得到了多项分布\",{\"1\":{\"576\":1}}],[\"那么就把该\",{\"1\":{\"396\":1}}],[\"那么就会得到个文本特征\",{\"1\":{\"273\":1}}],[\"那么就是一个\",{\"1\":{\"115\":1}}],[\"那么clip的训练目标就是最大个正样本的相似度\",{\"1\":{\"272\":1}}],[\"那么抽出三个来\",{\"1\":{\"242\":1}}],[\"那么除了我当前这个图片外\",{\"1\":{\"240\":1}}],[\"那么我当前图片经过数据增强之后\",{\"1\":{\"240\":1}}],[\"那么是什么意义呢\",{\"1\":{\"240\":1}}],[\"那么有一个问题\",{\"1\":{\"240\":1}}],[\"那么从中抽样的可能性组合就很多\",{\"1\":{\"238\":1}}],[\"那么字典最好需要满足两个条件\",{\"1\":{\"238\":1}}],[\"那么字典中的每个key就是一个类别\",{\"1\":{\"238\":1}}],[\"那么负样本走哪个编码器呢\",{\"1\":{\"238\":1}}],[\"那么\",{\"1\":{\"236\":1,\"240\":1,\"242\":1,\"273\":1,\"291\":1,\"407\":1,\"565\":1,\"596\":1,\"599\":1,\"654\":1}}],[\"那样偏向背景点\",{\"1\":{\"403\":1}}],[\"那样对负样本过多敏感\",{\"1\":{\"401\":1}}],[\"那样我们总共有196个向量\",{\"1\":{\"291\":1}}],[\"那样逐层提取多层次的抽象特征\",{\"1\":{\"112\":1}}],[\"那样依赖\",{\"1\":{\"82\":1}}],[\"给每个普通词分配索引\",{\"1\":{\"511\":1}}],[\"给每个实例样本加一个\",{\"1\":{\"449\":1}}],[\"给每个点\",{\"1\":{\"112\":1}}],[\"给出问题和上下文\",{\"1\":{\"542\":1}}],[\"给出较为合理的解释\",{\"1\":{\"471\":1}}],[\"给出最大化的目标函数为\",{\"1\":{\"444\":1}}],[\"给出概率最大的结果\",{\"1\":{\"430\":1}}],[\"给llm更多的时间去思考\",{\"0\":{\"433\":1},\"1\":{\"433\":2}}],[\"给这3个邻近点分配权重\",{\"1\":{\"100\":1}}],[\"给定某个输出值\",{\"1\":{\"565\":1}}],[\"给定某个状态\",{\"1\":{\"565\":1}}],[\"给定文档\",{\"1\":{\"445\":1}}],[\"给定图像\",{\"1\":{\"172\":1}}],[\"给定输入图像\",{\"1\":{\"167\":1}}],[\"给定两个超参数\",{\"1\":{\"90\":1}}],[\"给定一个符号序列\",{\"1\":{\"454\":1}}],[\"给定一个无监督学习的语料tokens\",{\"1\":{\"443\":1}}],[\"给定一个点云\",{\"1\":{\"98\":1}}],[\"给定一个\",{\"1\":{\"70\":1}}],[\"说白了\",{\"1\":{\"236\":1}}],[\"说明偏好并非训练数据过拟合造成\",{\"1\":{\"471\":1}}],[\"说明标注者之间达成了较高的一致性\",{\"1\":{\"470\":1}}],[\"说明你更讨厌\",{\"1\":{\"405\":2}}],[\"说明它们强烈正相关\",{\"1\":{\"355\":1}}],[\"说明\",{\"1\":{\"228\":1,\"267\":1,\"325\":1,\"359\":1,\"369\":1}}],[\"说明其任务执行能力更强\",{\"1\":{\"471\":1}}],[\"说明其确实有效增强了语言\",{\"1\":{\"75\":1}}],[\"说明其具备良好的泛化能力与鲁棒性\",{\"1\":{\"25\":1}}],[\"说话越有分量\",{\"1\":{\"100\":1}}],[\"维持\",{\"1\":{\"471\":1}}],[\"维内部隐藏层\",{\"1\":{\"447\":1}}],[\"维隐藏层\",{\"1\":{\"447\":1}}],[\"维是\",{\"1\":{\"387\":3}}],[\"维和第\",{\"1\":{\"291\":1}}],[\"维护队列状态的代码实现如下所示\",{\"1\":{\"249\":1}}],[\"维护一个队列\",{\"1\":{\"238\":1}}],[\"维\",{\"1\":{\"247\":1,\"291\":1}}],[\"维特征\",{\"1\":{\"100\":1}}],[\"维度统一\",{\"1\":{\"477\":1}}],[\"维度友好\",{\"1\":{\"395\":1}}],[\"维度灾难\",{\"1\":{\"395\":1}}],[\"维度诅咒的缓解\",{\"1\":{\"395\":1}}],[\"维度规则\",{\"1\":{\"390\":1}}],[\"维度必须兼容\",{\"1\":{\"387\":1}}],[\"维度问题\",{\"0\":{\"328\":1}}],[\"维度变为\",{\"1\":{\"283\":1}}],[\"维度为\",{\"1\":{\"244\":1,\"282\":2,\"283\":2,\"284\":9,\"289\":1}}],[\"维度求和\",{\"1\":{\"76\":1}}],[\"维度\",{\"1\":{\"46\":1,\"67\":1,\"76\":3,\"100\":1,\"105\":1,\"112\":2,\"242\":1,\"310\":2,\"323\":4}}],[\"维度扩展到64\",{\"1\":{\"36\":1}}],[\"维度同上\",{\"1\":{\"30\":1}}],[\"维度展平\",{\"1\":{\"30\":1}}],[\"找到\",{\"1\":{\"687\":1}}],[\"找到了rosenbrock函数最小值的位置\",{\"1\":{\"667\":1}}],[\"找到与文本最匹配的图片\",{\"1\":{\"276\":1,\"277\":1}}],[\"找到邻居\",{\"1\":{\"100\":1}}],[\"找到它最近的\",{\"1\":{\"100\":1}}],[\"找到最近的3个邻近点\",{\"1\":{\"100\":1}}],[\"找出最可能是\",{\"1\":{\"540\":1}}],[\"找出最近的3个邻近点\",{\"1\":{\"100\":1}}],[\"找出该尺度下每个质心点周围的邻近点\",{\"1\":{\"96\":1}}],[\"找出每个点的局部邻近点\",{\"1\":{\"92\":1}}],[\"找出它周围距离小于\",{\"1\":{\"92\":1}}],[\"欧几里得距离适合所有维度的尺度和方差差不多时\",{\"1\":{\"360\":1}}],[\"欧几里得距离计算方式\",{\"1\":{\"359\":1}}],[\"欧几里得距离\",{\"0\":{\"357\":1}}],[\"欧氏距离平方\",{\"1\":{\"100\":1}}],[\"欧式距离的均匀性假设\",{\"1\":{\"90\":1}}],[\"格式化\",{\"1\":{\"685\":1}}],[\"格式为\",{\"1\":{\"143\":1}}],[\"格式\",{\"1\":{\"100\":1}}],[\"格式输出\",{\"1\":{\"40\":1}}],[\"←\",{\"1\":{\"99\":3}}],[\"少量则通过\",{\"1\":{\"470\":1}}],[\"少量点无法覆盖关键结构\",{\"1\":{\"112\":1}}],[\"少量点的坐标\",{\"1\":{\"100\":1}}],[\"少样本学习\",{\"1\":{\"461\":1}}],[\"少样本任务迁移\",{\"1\":{\"231\":1}}],[\"少\",{\"1\":{\"98\":1}}],[\"墙壁等\",{\"1\":{\"98\":1}}],[\"椅子\",{\"1\":{\"98\":1,\"101\":1}}],[\"椅子对应的点云一共1000个\",{\"1\":{\"29\":1}}],[\"完善的工具使用\",{\"1\":{\"674\":1}}],[\"完善框架的模块结构\",{\"1\":{\"665\":1}}],[\"完全协方差矩阵\",{\"1\":{\"590\":1}}],[\"完全依赖语言模型对任务上下文的理解能力\",{\"1\":{\"454\":1}}],[\"完全随机猜测的\",{\"1\":{\"351\":1}}],[\"完全支持\",{\"1\":{\"112\":1}}],[\"完美的模型在某个阈值下的\",{\"1\":{\"350\":1}}],[\"完美的模型不会产生假正例\",{\"1\":{\"345\":1}}],[\"完美的模型没有假正例和假负例\",{\"1\":{\"343\":1}}],[\"完成预处理后\",{\"1\":{\"687\":1}}],[\"完成了自动微分的核心算法\",{\"1\":{\"648\":1}}],[\"完成特定任务\",{\"1\":{\"231\":1}}],[\"完成点云分割任务的过程是一个典型的\",{\"1\":{\"98\":1}}],[\"完成\",{\"1\":{\"97\":1,\"477\":1,\"687\":1}}],[\"完整的代码实现部分\",{\"1\":{\"513\":1}}],[\"完整的单尺度分组分类流程为\",{\"1\":{\"93\":1}}],[\"完整代码\",{\"0\":{\"277\":1}}],[\"完整方法\",{\"1\":{\"82\":1}}],[\"要实现这一点\",{\"1\":{\"597\":1}}],[\"要知道\",{\"1\":{\"565\":1}}],[\"要知道像\",{\"1\":{\"424\":1}}],[\"要和分解的问题\",{\"1\":{\"436\":1}}],[\"要具体\",{\"0\":{\"432\":1}}],[\"要明确\",{\"0\":{\"432\":1}}],[\"要让llm给出的结果尽可能地合理\",{\"1\":{\"430\":1}}],[\"要想开发自己的大模型几乎不可能\",{\"1\":{\"424\":1}}],[\"要想在自己的服务中接入大模型的能力\",{\"1\":{\"415\":1}}],[\"要训练一个特定的模型\",{\"1\":{\"420\":1}}],[\"要适配特定的下游任务\",{\"1\":{\"420\":1}}],[\"要在个性化的服务中使用大模型的能力\",{\"1\":{\"415\":1}}],[\"要对大模型进行微调\",{\"1\":{\"415\":1}}],[\"要逼近复杂函数\",{\"1\":{\"395\":1}}],[\"要注册的张量\",{\"1\":{\"389\":1}}],[\"要退出当前激活的环境\",{\"1\":{\"333\":1}}],[\"要跳到下一列只需要移动\",{\"1\":{\"323\":1}}],[\"要跳到下一行需要移动\",{\"1\":{\"323\":1}}],[\"要跳到下一个通道需要移动\",{\"1\":{\"323\":1}}],[\"要跳到下一个\",{\"1\":{\"323\":1}}],[\"要采样的质心点数量\",{\"1\":{\"96\":1}}],[\"要么对所有点做操作\",{\"1\":{\"86\":1}}],[\"要么对一个点做操作\",{\"1\":{\"86\":1}}],[\"之一\",{\"1\":{\"674\":1}}],[\"之前的工作提出了在迁移表征顶部学习特定任务的架构\",{\"1\":{\"445\":1}}],[\"之前的工作中\",{\"1\":{\"241\":1}}],[\"之前已经有一些研究工作探讨了使用文本作为监督信号来训练视觉模型\",{\"1\":{\"278\":1}}],[\"之前也有很多优秀的无监督工作\",{\"1\":{\"238\":1}}],[\"之间切换\",{\"1\":{\"674\":1}}],[\"之间可能提供最佳平衡\",{\"1\":{\"353\":1}}],[\"之间仍存在显著差距\",{\"1\":{\"208\":1}}],[\"之间的损失权重\",{\"1\":{\"404\":2}}],[\"之间的关系\",{\"1\":{\"292\":1}}],[\"之间的性能差距\",{\"1\":{\"207\":1}}],[\"之间的平方欧氏距离\",{\"1\":{\"92\":1}}],[\"之间\",{\"1\":{\"126\":1,\"541\":1}}],[\"之后我们要在这两个句子中加一些特殊的\",{\"1\":{\"506\":1}}],[\"之后让模型预测和还原被遮盖掉或替换掉的部分\",{\"1\":{\"505\":1}}],[\"之后做\",{\"1\":{\"505\":1}}],[\"之后可以用\",{\"1\":{\"389\":1}}],[\"之后\",{\"1\":{\"270\":1,\"508\":1,\"596\":1,\"656\":1}}],[\"之后concat形成该区域提取的总特征\",{\"1\":{\"96\":1}}],[\"之后这些不同尺度上提取的特征被串联起来\",{\"1\":{\"95\":1}}],[\"ziegler\",{\"1\":{\"469\":1,\"470\":1}}],[\"zip\",{\"1\":{\"246\":1,\"248\":1,\"289\":1,\"514\":1,\"519\":2,\"522\":1,\"558\":1,\"652\":1,\"654\":1,\"656\":1,\"658\":1}}],[\"zsh\",{\"1\":{\"339\":1}}],[\"zhandaohong\",{\"1\":{\"519\":1}}],[\"zhang\",{\"1\":{\"485\":1}}],[\"zhang等人2017\",{\"1\":{\"178\":1}}],[\"zhou\",{\"1\":{\"469\":1}}],[\"zh\",{\"1\":{\"194\":1,\"217\":1}}],[\"zoo\",{\"1\":{\"157\":1,\"573\":1}}],[\"z\",{\"1\":{\"93\":2,\"114\":2,\"359\":1,\"384\":1,\"654\":1,\"662\":12,\"666\":7,\"667\":2}}],[\"zero\",{\"0\":{\"176\":1},\"1\":{\"81\":1,\"142\":1,\"145\":1,\"147\":1,\"159\":1,\"176\":2,\"194\":2,\"224\":1,\"244\":1,\"440\":1,\"452\":1,\"455\":1,\"460\":1,\"461\":1,\"517\":1,\"520\":2,\"674\":1}}],[\"zeros\",{\"1\":{\"59\":1,\"81\":2,\"82\":3,\"83\":3,\"92\":2,\"145\":2,\"147\":3,\"160\":1,\"161\":1,\"162\":1,\"246\":1,\"247\":1,\"284\":1,\"292\":1,\"293\":2,\"296\":2,\"523\":1,\"536\":1}}],[\"质量\",{\"1\":{\"470\":1}}],[\"质量高于\",{\"1\":{\"454\":1}}],[\"质量和多样性\",{\"1\":{\"215\":1}}],[\"质点数量\",{\"1\":{\"92\":1}}],[\"质心\",{\"1\":{\"92\":2}}],[\"我的调试文件是run\",{\"1\":{\"519\":1}}],[\"我的狗很可爱\",{\"1\":{\"506\":2}}],[\"我已经上传到了仓库中\",{\"1\":{\"519\":1}}],[\"我就不上传了\",{\"1\":{\"519\":1}}],[\"我现在要把你的数据重新整理成行优先\",{\"1\":{\"326\":1}}],[\"我不管你现在怎么解读这块内存\",{\"1\":{\"326\":1}}],[\"我不感兴趣\",{\"1\":{\"92\":1}}],[\"我将两者的结构进行对比\",{\"1\":{\"294\":1}}],[\"我认为为了保持正样本的定义\",{\"1\":{\"242\":1}}],[\"我自己理解\",{\"1\":{\"241\":1}}],[\"我这个点最近的3个熟人是谁\",{\"1\":{\"100\":1}}],[\"我们应该进行实际业务测试\",{\"1\":{\"687\":1}}],[\"我们应该预期这些样本大多会落在空间的哪里\",{\"1\":{\"591\":1}}],[\"我们推荐基于\",{\"1\":{\"687\":1}}],[\"我们一般可以将大模型开发分解为以下几个流程\",{\"1\":{\"687\":1}}],[\"我们一般不会去大幅度改动模型\",{\"1\":{\"686\":1}}],[\"我们后续会用到的处理文档\",{\"1\":{\"685\":1}}],[\"我们实现了自动构建计算图与反向传播的基本机制\",{\"1\":{\"665\":1}}],[\"我们开始迈入更深入也更贴近真实深度学习框架设计的阶段\",{\"1\":{\"665\":1}}],[\"我们构建了如下关键功能\",{\"1\":{\"663\":1}}],[\"我们构建了变量\",{\"1\":{\"650\":1}}],[\"我们为variable类添加\",{\"1\":{\"660\":1}}],[\"我们为每个\",{\"1\":{\"566\":1}}],[\"我们引入释放中间变量导数的机制\",{\"1\":{\"658\":1}}],[\"我们引入了目前最大规模的\",{\"1\":{\"26\":1}}],[\"我们便可以通过\",{\"1\":{\"656\":1}}],[\"我们采用更加暴力的\",{\"1\":{\"656\":1}}],[\"我们采用动态高分辨率训练策略\",{\"1\":{\"216\":1}}],[\"我们学习和利用了\",{\"1\":{\"651\":1}}],[\"我们完成了计算图与手动反向传播的雏形\",{\"1\":{\"650\":1}}],[\"我们就完成了应用的核心功能\",{\"1\":{\"687\":1}}],[\"我们就得到了后验分布\",{\"1\":{\"596\":1}}],[\"我们就用该查询点最近的那个点\",{\"1\":{\"92\":1}}],[\"我们直觉上会认为\",{\"1\":{\"591\":1}}],[\"我们直接用自然语言丢给他就去执行就好了\",{\"1\":{\"430\":1}}],[\"我们直接梯度回传就可以了\",{\"1\":{\"242\":1}}],[\"我们简要介绍三个\",{\"1\":{\"676\":1}}],[\"我们简称为student\",{\"1\":{\"586\":1}}],[\"我们简写为\",{\"1\":{\"584\":1}}],[\"我们讨论一些定义在实数集合\",{\"1\":{\"583\":1}}],[\"我们继续抽球\",{\"1\":{\"579\":1}}],[\"我们进行有放回抽样\",{\"1\":{\"578\":1}}],[\"我们要先计算出其梯度后\",{\"1\":{\"655\":1}}],[\"我们要算的事件是\",{\"1\":{\"569\":1}}],[\"我们要问\",{\"1\":{\"569\":1}}],[\"我们定义概率\",{\"1\":{\"566\":1}}],[\"我们不能对\",{\"1\":{\"566\":1}}],[\"我们不需要知道第一张和第二张图片是人\",{\"1\":{\"234\":1}}],[\"我们也可以将自然语言理解看作是一个病态问题\",{\"1\":{\"597\":1}}],[\"我们也可以考虑结果为连续值的实验\",{\"1\":{\"566\":1}}],[\"我们也可以对得到的余弦相似度计算softmax\",{\"1\":{\"273\":1}}],[\"我们称\",{\"1\":{\"565\":1}}],[\"我们称之为transformation\",{\"1\":{\"235\":1}}],[\"我们这么定义\",{\"1\":{\"565\":1}}],[\"我们关心的是\",{\"1\":{\"565\":1}}],[\"我们用3面而不是6面\",{\"1\":{\"565\":1}}],[\"我们用蓝色的向量和所有黄色向量进行\",{\"1\":{\"508\":1}}],[\"我们从零开始构建了tinypytorch框架的基础功能\",{\"1\":{\"648\":1}}],[\"我们从原始输入的\",{\"1\":{\"542\":1}}],[\"我们从flower\",{\"1\":{\"275\":1}}],[\"我们主要使用它来预测答案的起始和结束位置\",{\"1\":{\"540\":1}}],[\"我们只对哪些集合定义概率\",{\"1\":{\"566\":1}}],[\"我们只会计算被随机遮盖或替换的部分\",{\"1\":{\"514\":1}}],[\"我们只需要如下修改variable变量的backward方法即可完成按照辈分获取函数的逻辑\",{\"1\":{\"656\":1}}],[\"我们只需要确保对于某个词的上下文融合不被pad词参与即可\",{\"1\":{\"517\":1}}],[\"我们只需要在计算出相似度得分矩阵后\",{\"1\":{\"276\":1}}],[\"我们只需要经过模型\",{\"1\":{\"235\":1}}],[\"我们给上句的\",{\"1\":{\"506\":1}}],[\"我们给一个输入文本\",{\"1\":{\"474\":1}}],[\"我们传入的是\",{\"1\":{\"477\":1}}],[\"我们到底在\",{\"1\":{\"472\":1}}],[\"我们先给小孩子分析讲解一些示例\",{\"1\":{\"434\":1}}],[\"我们先对大模型做一个直观的抽象\",{\"1\":{\"414\":1}}],[\"我们教小孩做应用题\",{\"1\":{\"434\":1}}],[\"我们让llm对一段文字进行总结\",{\"1\":{\"432\":1}}],[\"我们发给llm的批令\",{\"1\":{\"432\":1}}],[\"我们发现倒数第四层的特征在多模态任务中表现最佳\",{\"1\":{\"215\":1}}],[\"我们看一下矩阵\",{\"1\":{\"426\":1}}],[\"我们看重无监督学习的优点\",{\"1\":{\"237\":1}}],[\"我们微调大模型的流程就变为了\",{\"1\":{\"423\":1}}],[\"我们所说的\",{\"1\":{\"414\":1}}],[\"我们取其补集\",{\"1\":{\"403\":1}}],[\"我们通常取其补集\",{\"1\":{\"401\":1}}],[\"我们通过变量梯度非空则进行累加的改动\",{\"1\":{\"655\":1}}],[\"我们通过把复杂问题拆解成一个个的简单问题\",{\"1\":{\"436\":1}}],[\"我们通过线性变换得到\",{\"1\":{\"315\":1}}],[\"我们通过自定义一个patchembed类完成上述工作\",{\"1\":{\"291\":1}}],[\"我们通过利用clip模型的多模态能力\",{\"1\":{\"273\":1}}],[\"我们通过一次\",{\"1\":{\"242\":1}}],[\"我们通过\",{\"1\":{\"170\":1}}],[\"我们通过在线对比难样本挖掘来改进图文匹配\",{\"1\":{\"153\":1}}],[\"我们希望每次计算都能得到正确的导数\",{\"1\":{\"653\":1}}],[\"我们希望使用定义在非负实数上的分布\",{\"1\":{\"585\":1}}],[\"我们希望输出一个\",{\"1\":{\"396\":1}}],[\"我们希望在特征空间里\",{\"1\":{\"234\":1}}],[\"我们会用到多种概率分布\",{\"1\":{\"573\":1}}],[\"我们会用自然语言描述一系列的推理过程\",{\"1\":{\"434\":1}}],[\"我们会使用pad\",{\"1\":{\"517\":1}}],[\"我们会为该任务设计一个最合适的神经网络架构并做训练\",{\"1\":{\"504\":1}}],[\"我们会从一张图片中生成多个候选区域\",{\"1\":{\"396\":1}}],[\"我们会拼接多个这样的头\",{\"1\":{\"315\":1}}],[\"我们选择的阈值取决于哪个指标对特定用例而言最重要\",{\"1\":{\"353\":1}}],[\"我们知道\",{\"1\":{\"321\":1,\"569\":1}}],[\"我们生成一个3\",{\"1\":{\"321\":1}}],[\"我们来一步步分析这个过程\",{\"1\":{\"304\":1}}],[\"我们首先应该明确\",{\"1\":{\"687\":1}}],[\"我们首先需要确定开发的目标\",{\"1\":{\"687\":1}}],[\"我们首先拿到属于上下文的一对句子\",{\"1\":{\"506\":1}}],[\"我们首先获取图片库中所有图片\",{\"1\":{\"276\":1}}],[\"我们首先创建了各类别的文本描述\",{\"1\":{\"273\":1}}],[\"我们常常需要衡量文本嵌入和图片嵌入之间的相似度\",{\"1\":{\"275\":1}}],[\"我们还用了\",{\"1\":{\"432\":1}}],[\"我们还有其他的选择\",{\"1\":{\"274\":1}}],[\"我们还提升了预训练数据集的规模\",{\"1\":{\"215\":1}}],[\"我们已经探讨了clip模型的运作机制\",{\"1\":{\"273\":1}}],[\"我们是放到字典中去的\",{\"1\":{\"241\":1}}],[\"我们最开始右边分支的编码器是由左边初始化而来\",{\"1\":{\"238\":1}}],[\"我们其实可以把key集合看成字典\",{\"1\":{\"238\":1}}],[\"我们把\",{\"1\":{\"569\":1}}],[\"我们把字典中的\",{\"1\":{\"242\":1}}],[\"我们把f11叫做\",{\"1\":{\"238\":1}}],[\"我们把x11这个图片叫做\",{\"1\":{\"238\":1}}],[\"我们接着来看图\",{\"1\":{\"238\":1}}],[\"我们可以基于\",{\"1\":{\"687\":1}}],[\"我们可以根据自身需求灵活地进行组合\",{\"1\":{\"683\":1}}],[\"我们可以根据这个类别去学习模型\",{\"1\":{\"238\":1}}],[\"我们可以轻松地构建如下所示的\",{\"1\":{\"682\":1}}],[\"我们可以验证框架是否支持复杂表达式的自动微分\",{\"1\":{\"660\":1}}],[\"我们可以获取到哪个函数生成了哪个变量\",{\"1\":{\"656\":1}}],[\"我们可以采用拓扑排序\",{\"1\":{\"656\":1}}],[\"我们可以用一句话来总结贝叶斯公式\",{\"1\":{\"596\":1}}],[\"我们可以通过变量的辈分来设置其创建者函数的辈分\",{\"1\":{\"656\":1}}],[\"我们可以通过除以\",{\"1\":{\"596\":1}}],[\"我们可以通改变对内存中数据缓冲区的解读方式来实现逻辑上的转置\",{\"1\":{\"326\":1}}],[\"我们可以提出这样一个问题\",{\"1\":{\"591\":1}}],[\"我们可以使用贝叶斯公式来计算后验概率\",{\"1\":{\"597\":1}}],[\"我们可以使用分类分布\",{\"1\":{\"576\":1}}],[\"我们可以使用一些代理任务\",{\"1\":{\"234\":1}}],[\"我们可以得到\",{\"1\":{\"566\":1}}],[\"我们可以将每个样本空间中的结果映射为一个实数\",{\"1\":{\"565\":1}}],[\"我们可以对ltm\",{\"1\":{\"436\":1}}],[\"我们可以利用矩阵分解技术\",{\"1\":{\"423\":1}}],[\"我们可以把这看作是对\",{\"1\":{\"597\":1}}],[\"我们可以把\",{\"1\":{\"326\":1}}],[\"我们可以从以下角度理解\",{\"1\":{\"311\":1}}],[\"我们可以直接使用类别标签作为文本描述\",{\"1\":{\"274\":1}}],[\"我们随机选择一个图片\",{\"1\":{\"235\":1}}],[\"我们并不需要深研大模型内部原理\",{\"1\":{\"686\":1}}],[\"我们并不需要大量标注好的数据\",{\"1\":{\"233\":1}}],[\"我们并没有用到标签信息\",{\"1\":{\"234\":1}}],[\"我们需要逐步迭代构建优质的\",{\"1\":{\"687\":1}}],[\"我们需要收集数据并进行预处理\",{\"1\":{\"687\":1}}],[\"我们需要针对我们所设计的功能\",{\"1\":{\"687\":1}}],[\"我们需要让variable实例能与numpy数组\",{\"1\":{\"660\":1}}],[\"我们需要让这三个表征在特征空间中\",{\"1\":{\"234\":1}}],[\"我们需要实现mul类来处理正向传播和反向传播\",{\"1\":{\"660\":1}}],[\"我们需要在每次计算之前将导数重置为0\",{\"1\":{\"653\":1}}],[\"我们需要在其对应的crossentropyloss中指定ignore\",{\"1\":{\"514\":1}}],[\"我们需要借助borel\",{\"1\":{\"566\":1}}],[\"我们需要读取并构建batch数据\",{\"1\":{\"512\":1}}],[\"我们需要通过prompt\",{\"1\":{\"433\":1}}],[\"我们需要先将\",{\"1\":{\"396\":1}}],[\"我们需要根据上面给出的花卉数据集下载链接\",{\"1\":{\"275\":1}}],[\"我们需要知道的是\",{\"1\":{\"234\":1}}],[\"我们现在看到的这些大语言模型\",{\"1\":{\"420\":1}}],[\"我们现在把这三张图片输入一个模型\",{\"1\":{\"234\":1}}],[\"我们现在有三张图\",{\"1\":{\"234\":1}}],[\"我们想得到一个结果\",{\"1\":{\"234\":1}}],[\"我们在variable类和function类中增加实例变量generation\",{\"1\":{\"656\":1}}],[\"我们在接下来的部分\",{\"1\":{\"433\":1}}],[\"我们在给llm发指令的时候\",{\"1\":{\"432\":1}}],[\"我们在做对比学习的时候\",{\"1\":{\"238\":1}}],[\"我们在训练中同时激活视觉编码器和mlp投影层\",{\"1\":{\"215\":1}}],[\"我们在internvl\",{\"1\":{\"215\":1}}],[\"我们在该阶段冻结\",{\"1\":{\"191\":1}}],[\"我们在预训练的\",{\"1\":{\"167\":1}}],[\"我们\",{\"1\":{\"172\":1}}],[\"我们提出掩码图像建模\",{\"1\":{\"172\":1}}],[\"我们提出了一种\",{\"1\":{\"156\":1}}],[\"我们提出了一种开放词汇形式的\",{\"1\":{\"26\":1}}],[\"我们使用了一个示例来说明这一概念\",{\"1\":{\"327\":1}}],[\"我们使用了\",{\"1\":{\"274\":1}}],[\"我们使用标准\",{\"1\":{\"171\":1}}],[\"我们使用离散变分自编码器\",{\"1\":{\"170\":1}}],[\"我们使用多模态编码器输出的\",{\"1\":{\"156\":1}}],[\"我们将开发以大语言模型为功能核心\",{\"1\":{\"686\":1}}],[\"我们将实现\",{\"1\":{\"670\":1}}],[\"我们将真正迈入\",{\"1\":{\"670\":1}}],[\"我们将看到\",{\"1\":{\"665\":1}}],[\"我们将add类绑定到+运算符\",{\"1\":{\"660\":1}}],[\"我们将mul类封装为python函数mul\",{\"1\":{\"660\":1}}],[\"我们将继续揭开深度学习框架的核心机制\",{\"1\":{\"650\":1}}],[\"我们将详细介绍多元高斯分布\",{\"1\":{\"589\":1}}],[\"我们将组合数\",{\"1\":{\"580\":1}}],[\"我们将随机变量可能的取值集合称为其状态空间\",{\"1\":{\"565\":1}}],[\"我们将概率空间定义为三元组\",{\"1\":{\"564\":1}}],[\"我们将从第25步继续出发\",{\"1\":{\"665\":1}}],[\"我们将从\",{\"1\":{\"509\":1}}],[\"我们将分辨率从224提升至448\",{\"1\":{\"215\":1}}],[\"我们将每个图像分词为\",{\"1\":{\"170\":1}}],[\"我们将图像\",{\"1\":{\"169\":1,\"170\":1}}],[\"我们将原始数据集中的网页文本复制\",{\"1\":{\"136\":1}}],[\"我们的方法中\",{\"1\":{\"168\":1}}],[\"我们以\",{\"1\":{\"155\":1}}],[\"我们计算归一化的图像到文本和文本到图像的\",{\"1\":{\"154\":1}}],[\"我们计划构建专用于推理的数据集\",{\"1\":{\"26\":1}}],[\"我们维护两个队列来存储动量单模态编码器最近的\",{\"1\":{\"154\":1}}],[\"我们切分为多个步骤进行解析\",{\"1\":{\"145\":1}}],[\"我们相信该研究将为视觉可供性理解领域带来新的启发并推动其发展\",{\"1\":{\"26\":1}}],[\"我们设计了一个新颖的框架\",{\"1\":{\"26\":1}}],[\"我们对可能出现的结果\",{\"1\":{\"596\":1}}],[\"我们对prompt进行优化\",{\"1\":{\"430\":1}}],[\"我们对比了两种方式\",{\"1\":{\"137\":1}}],[\"我们对\",{\"1\":{\"10\":1,\"153\":1}}],[\"代理服务以及回调处理等关键组件\",{\"1\":{\"684\":1}}],[\"代理\",{\"1\":{\"683\":1}}],[\"代理任务通常是辅助进行表征学习\",{\"1\":{\"278\":1}}],[\"代数就是一种封闭的事件系统\",{\"1\":{\"566\":1}}],[\"代数是一种你可以安全地讨论概率的\",{\"1\":{\"566\":1}}],[\"代数是一个集合的集合\",{\"1\":{\"566\":1}}],[\"代数是由所有形如\",{\"1\":{\"566\":1}}],[\"代数是由半开区间\",{\"1\":{\"566\":1}}],[\"代数是专门为实数空间\",{\"1\":{\"566\":1}}],[\"代数\",{\"1\":{\"566\":7}}],[\"代价是非常高的\",{\"1\":{\"425\":1}}],[\"代入调整后的步幅\",{\"1\":{\"327\":1}}],[\"代表的是模型的基础\",{\"1\":{\"290\":1}}],[\"代表\",{\"1\":{\"244\":2,\"297\":2,\"300\":1,\"674\":1}}],[\"代表性工作包括\",{\"1\":{\"211\":1}}],[\"代表每个\",{\"1\":{\"92\":1}}],[\"代表原始点云中每个点的\",{\"1\":{\"92\":1}}],[\"代码执行\",{\"1\":{\"674\":1}}],[\"代码中\",{\"1\":{\"667\":1}}],[\"代码中用\",{\"1\":{\"477\":1}}],[\"代码将变量y的计算图转换为dot语言字符串\",{\"1\":{\"666\":1}}],[\"代码示例\",{\"0\":{\"617\":1}}],[\"代码及新数据集cc\",{\"1\":{\"500\":1}}],[\"代码与学术数据\",{\"1\":{\"481\":1}}],[\"代码生成与数学推理\",{\"1\":{\"482\":1}}],[\"代码生成\",{\"1\":{\"480\":1,\"482\":1,\"483\":1}}],[\"代码描述如下\",{\"1\":{\"474\":1}}],[\"代码任务\",{\"1\":{\"472\":1}}],[\"代码片段\",{\"1\":{\"471\":1}}],[\"代码解析\",{\"1\":{\"404\":1}}],[\"代码整体流程比较长\",{\"1\":{\"145\":1}}],[\"代码链接\",{\"1\":{\"119\":1,\"148\":1,\"179\":1,\"223\":1,\"232\":1,\"252\":1,\"260\":1}}],[\"代码实现如下\",{\"1\":{\"93\":1}}],[\"代码实现\",{\"0\":{\"68\":1,\"92\":1,\"99\":1,\"541\":1,\"607\":1,\"613\":1,\"622\":1},\"1\":{\"107\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"407\":1,\"662\":3}}],[\"代码体现\",{\"1\":{\"29\":1}}],[\"代码\",{\"0\":{\"27\":1,\"57\":1,\"106\":1},\"1\":{\"4\":1,\"37\":1,\"47\":1,\"60\":1,\"279\":1}}],[\"到向量存储\",{\"1\":{\"684\":1}}],[\"到原点的平方距离\",{\"1\":{\"593\":1}}],[\"到第\",{\"1\":{\"542\":1}}],[\"到三维\",{\"1\":{\"513\":1}}],[\"到神经网络\",{\"1\":{\"485\":1}}],[\"到上下文感知的循环神经网络\",{\"1\":{\"453\":1}}],[\"到特征图空间\",{\"1\":{\"396\":1}}],[\"到这个内部函数上\",{\"1\":{\"366\":1}}],[\"到的特征不太好泛化\",{\"1\":{\"240\":1}}],[\"到近年来的vision\",{\"1\":{\"183\":1}}],[\"到一个标准姿态\",{\"1\":{\"107\":1}}],[\"到\",{\"1\":{\"92\":1,\"207\":1,\"323\":1,\"351\":1,\"372\":1,\"564\":1,\"566\":1,\"654\":2,\"674\":1}}],[\"到红色\",{\"1\":{\"83\":1}}],[\"<=\",{\"1\":{\"512\":3,\"540\":1}}],[\"<unk>\",{\"1\":{\"410\":1,\"412\":2,\"511\":1}}],[\"<pad>\",{\"1\":{\"410\":1,\"412\":2}}],[\"<line\",{\"1\":{\"410\":1,\"411\":1,\"412\":2}}],[\"<<\",{\"1\":{\"395\":1}}],[\"<环境名>\",{\"1\":{\"331\":1,\"332\":1}}],[\"<4\",{\"1\":{\"327\":1}}],[\"<stop>\",{\"1\":{\"227\":6}}],[\"<eos>\",{\"1\":{\"126\":1}}],[\"<\",{\"1\":{\"92\":1,\"267\":3,\"355\":1,\"407\":1,\"410\":6,\"411\":2,\"412\":7,\"511\":1,\"514\":1}}],[\"次试验的结果\",{\"1\":{\"579\":1}}],[\"次试验中选出\",{\"1\":{\"579\":1}}],[\"次试验中出现的次数\",{\"1\":{\"576\":1}}],[\"次成功的位置\",{\"1\":{\"579\":1}}],[\"次数\",{\"1\":{\"579\":1}}],[\"次失败必须是第\",{\"1\":{\"579\":1}}],[\"次失败\",{\"1\":{\"579\":1}}],[\"次从\",{\"1\":{\"447\":1}}],[\"次并与每个点的局部特征拼接\",{\"1\":{\"112\":1}}],[\"次并与每个点的局部特征\",{\"1\":{\"109\":1}}],[\"次\",{\"1\":{\"92\":1,\"111\":1,\"387\":1}}],[\"重载运算符\",{\"1\":{\"663\":1}}],[\"重置导数\",{\"0\":{\"653\":1}}],[\"重新计算\",{\"1\":{\"477\":1}}],[\"重新输入全部历史\",{\"1\":{\"477\":1}}],[\"重新拉直空间\",{\"1\":{\"359\":1}}],[\"重写\",{\"1\":{\"470\":1}}],[\"重要的是\",{\"1\":{\"461\":1}}],[\"重要说明\",{\"1\":{\"338\":1}}],[\"重叠计算与gpu通信\",{\"1\":{\"481\":1}}],[\"重叠\",{\"1\":{\"455\":1}}],[\"重\",{\"1\":{\"280\":8}}],[\"重点是保持参数尺寸最小化\",{\"1\":{\"428\":1}}],[\"重点是如何理解这里的分组\",{\"1\":{\"72\":1}}],[\"重点在于\",{\"1\":{\"327\":1}}],[\"重点训练图像和文本特征提取\",{\"1\":{\"280\":1}}],[\"重型胶水层\",{\"1\":{\"188\":1}}],[\"重组视觉特征\",{\"1\":{\"181\":1}}],[\"重建目标可表示为\",{\"1\":{\"170\":1}}],[\"重建输入图像\",{\"1\":{\"170\":1}}],[\"重塑为\",{\"1\":{\"169\":1}}],[\"重复向梯度方向移动一定距离\",{\"1\":{\"667\":1}}],[\"重复惩罚系数\",{\"1\":{\"286\":1}}],[\"重复惩罚项\",{\"1\":{\"143\":1}}],[\"重复\",{\"1\":{\"92\":1}}],[\"批处理\",{\"1\":{\"670\":1}}],[\"批次大小\",{\"1\":{\"493\":1}}],[\"批次索引\",{\"1\":{\"92\":1}}],[\"批量\",{\"1\":{\"204\":1}}],[\"批量大小增至\",{\"1\":{\"202\":1}}],[\"批量大小\",{\"1\":{\"174\":1,\"203\":1}}],[\"批量大小为\",{\"1\":{\"131\":1}}],[\"批大小提升至512\",{\"1\":{\"454\":1}}],[\"批大小为\",{\"1\":{\"447\":1}}],[\"批大小\",{\"1\":{\"22\":1}}],[\"距离本身越来越集中在\",{\"1\":{\"593\":1}}],[\"距离或温度\",{\"1\":{\"566\":1}}],[\"距离\",{\"1\":{\"359\":1}}],[\"距离越近\",{\"1\":{\"100\":1}}],[\"距离直观性\",{\"1\":{\"90\":1}}],[\"距离的度量不受空间中位置的影响\",{\"1\":{\"90\":1}}],[\"文章生成和情境理解方面表现出色\",{\"1\":{\"675\":1}}],[\"文章中提炼出来\",{\"1\":{\"510\":1}}],[\"文心大模型包括\",{\"1\":{\"674\":1}}],[\"文心一言网页版分为\",{\"1\":{\"674\":1}}],[\"文心一言的中文能力相对来说非常不错\",{\"1\":{\"674\":1}}],[\"文心一言的基础模型文心大模型于\",{\"1\":{\"674\":1}}],[\"文心一言是基于百度文心大模型的知识增强语言大模型\",{\"1\":{\"674\":1}}],[\"文心一言\",{\"1\":{\"673\":1,\"674\":1}}],[\"文件转换命令\",{\"1\":{\"666\":1}}],[\"文件中的\",{\"1\":{\"546\":1}}],[\"文件进行调试即可\",{\"1\":{\"519\":1}}],[\"文献中也探索了多种控制模型输出的策略\",{\"1\":{\"469\":1}}],[\"文档\",{\"1\":{\"445\":1}}],[\"文档生成\",{\"1\":{\"372\":1}}],[\"文档理解\",{\"1\":{\"220\":1}}],[\"文字搜索图像\",{\"0\":{\"276\":1}}],[\"文字搜索图像实战演练\",{\"1\":{\"269\":1}}],[\"文中提到了一些少量使用in\",{\"1\":{\"464\":1}}],[\"文中引用了kaplan等人提出的\",{\"1\":{\"464\":1}}],[\"文中的𝐶所表示的其他信息\",{\"1\":{\"91\":1}}],[\"文中作者通过ball\",{\"1\":{\"90\":1}}],[\"文本摘要中的奖励建模与\",{\"1\":{\"469\":1}}],[\"文本到文本\",{\"1\":{\"464\":1}}],[\"文本到图像的相似度权重\",{\"1\":{\"162\":1}}],[\"文本蕴含\",{\"1\":{\"445\":1}}],[\"文本蕴含提升5\",{\"1\":{\"440\":1}}],[\"文本蕴含提升1\",{\"1\":{\"439\":1}}],[\"文本分词\",{\"1\":{\"520\":1}}],[\"文本分类任务\",{\"1\":{\"520\":1}}],[\"文本分类\",{\"1\":{\"445\":1,\"508\":2}}],[\"文本分离\",{\"1\":{\"439\":1}}],[\"文本分支\",{\"1\":{\"280\":2}}],[\"文本描述的生成也是一个关键环节\",{\"1\":{\"274\":1}}],[\"文本描述生成\",{\"0\":{\"274\":1}}],[\"文本问图\",{\"1\":{\"267\":1}}],[\"文本的更轻\",{\"1\":{\"255\":1}}],[\"文本的注意力掩码\",{\"1\":{\"162\":1}}],[\"文本匹配损失\",{\"1\":{\"190\":1}}],[\"文本数据\",{\"1\":{\"190\":1}}],[\"文本数据进行渐进式对齐训练\",{\"1\":{\"180\":1}}],[\"文本检索\",{\"1\":{\"194\":1,\"197\":1}}],[\"文本检索等对比任务上表现优异\",{\"1\":{\"190\":1}}],[\"文本检索以及多模态对话系统等\",{\"1\":{\"180\":1}}],[\"文本对应的标签\",{\"1\":{\"520\":1}}],[\"文本对上训练\",{\"1\":{\"215\":1}}],[\"文本对比损失\",{\"1\":{\"190\":1}}],[\"文本对的对称交叉熵损失\",{\"1\":{\"190\":1}}],[\"文本对\",{\"1\":{\"190\":1,\"282\":1}}],[\"文本对齐策略\",{\"1\":{\"181\":1}}],[\"文本渲染和零样本图像翻译等能力\",{\"1\":{\"178\":1}}],[\"文本生成\",{\"1\":{\"440\":1,\"470\":1}}],[\"文本生成阶段\",{\"1\":{\"285\":1}}],[\"文本生成图像\",{\"1\":{\"178\":1}}],[\"文本生成任务\",{\"1\":{\"147\":1}}],[\"文本相似度\",{\"1\":{\"161\":1}}],[\"文本全局语义\",{\"1\":{\"161\":1}}],[\"文本可能实际上与图像语义一致\",{\"1\":{\"157\":1}}],[\"文本中可能包含与图像无关的信息\",{\"1\":{\"157\":1}}],[\"文本经过编码后生成的嵌入序列\",{\"1\":{\"152\":1}}],[\"文本动量编码器\",{\"1\":{\"147\":1}}],[\"文本解码器\",{\"1\":{\"147\":1}}],[\"文本队列\",{\"1\":{\"147\":1}}],[\"文本跨模态编码器\",{\"1\":{\"142\":1}}],[\"文本\",{\"1\":{\"133\":1,\"147\":4,\"163\":1,\"188\":1,\"227\":1,\"285\":2,\"494\":1}}],[\"文本末尾添加一个\",{\"1\":{\"126\":1}}],[\"文本输入格式处理\",{\"1\":{\"83\":1}}],[\"文本特征输入部分\",{\"1\":{\"257\":1}}],[\"文本特征队列\",{\"1\":{\"160\":1}}],[\"文本特征提取和投影\",{\"1\":{\"145\":1}}],[\"文本特征\",{\"1\":{\"75\":2,\"160\":1}}],[\"文本引导的点特征分组\",{\"0\":{\"72\":1}}],[\"文本编码器的作用是提取文本的特征\",{\"1\":{\"272\":1}}],[\"文本编码器\",{\"1\":{\"161\":1,\"200\":2,\"272\":1,\"273\":1}}],[\"文本编码器和多模态编码器均为\",{\"1\":{\"152\":1}}],[\"文本编码器和多模态编码器\",{\"1\":{\"152\":1}}],[\"文本编码器配置\",{\"1\":{\"147\":1}}],[\"文本编码器bert\",{\"1\":{\"146\":1}}],[\"文本编码器则基于\",{\"1\":{\"131\":1}}],[\"文本编码器使用的是基于\",{\"1\":{\"275\":1}}],[\"文本编码器使用\",{\"1\":{\"126\":1}}],[\"文本编码\",{\"0\":{\"31\":1},\"1\":{\"147\":1,\"495\":1}}],[\"样本空间为\",{\"1\":{\"565\":2}}],[\"样本的得分是用生成模型分配的tokens的平均对数概率\",{\"1\":{\"449\":1}}],[\"样本为文本对\",{\"1\":{\"448\":1}}],[\"样本级别\",{\"1\":{\"404\":1}}],[\"样本\",{\"1\":{\"190\":1,\"565\":1}}],[\"样本分布偏差\",{\"1\":{\"89\":1}}],[\"样本数\",{\"1\":{\"65\":1}}],[\"选\",{\"1\":{\"575\":1}}],[\"选项内容\",{\"1\":{\"544\":1}}],[\"选择性优势\",{\"1\":{\"463\":1}}],[\"选择性能最佳的损失函数\",{\"1\":{\"408\":1}}],[\"选择合适的预训练好的大模型\",{\"1\":{\"423\":1}}],[\"选择适合的损失函数\",{\"1\":{\"408\":1}}],[\"选择最大值作为这个图文对的相似度\",{\"1\":{\"283\":1}}],[\"选择与图像特征相似度最高的文本所对应的类别\",{\"1\":{\"273\":1}}],[\"选择了一个包含6300万参数的transformer模型\",{\"1\":{\"272\":1}}],[\"选择𝑁个点\",{\"1\":{\"89\":1}}],[\"选出投票最多的结果\",{\"1\":{\"435\":1}}],[\"选出他们认为最好的答案\",{\"1\":{\"224\":1}}],[\"选出\",{\"1\":{\"96\":1}}],[\"选关键点\",{\"1\":{\"92\":1}}],[\"选取物体\",{\"1\":{\"63\":1}}],[\"邻近点\",{\"1\":{\"88\":1}}],[\"三类子数据集\",{\"1\":{\"470\":1}}],[\"三步训练框架\",{\"1\":{\"470\":1}}],[\"三大原则来评估模型对齐效果\",{\"1\":{\"468\":1}}],[\"三元组\",{\"1\":{\"445\":1}}],[\"三是接下来要解答的子问题\",{\"1\":{\"436\":1}}],[\"三种损失函数\",{\"1\":{\"190\":1}}],[\"三种设置下的对比表明\",{\"1\":{\"132\":1}}],[\"三\",{\"0\":{\"310\":1,\"333\":1,\"664\":1},\"1\":{\"112\":1,\"469\":1}}],[\"三次sample\",{\"1\":{\"93\":1}}],[\"三层分层特征学习结构\",{\"1\":{\"93\":1}}],[\"三部分组成\",{\"1\":{\"88\":1}}],[\"三个规模的模型\",{\"1\":{\"674\":1}}],[\"三个标记的点\",{\"1\":{\"353\":1}}],[\"三个模块\",{\"1\":{\"280\":1}}],[\"三个流程\",{\"1\":{\"79\":1}}],[\"三个关键步骤的实现\",{\"1\":{\"75\":1}}],[\"局限性与挑战\",{\"1\":{\"483\":1}}],[\"局限性与未来工作\",{\"1\":{\"26\":1}}],[\"局限性\",{\"0\":{\"463\":1},\"1\":{\"455\":1,\"472\":1}}],[\"局部性\",{\"1\":{\"395\":1}}],[\"局部逼近\",{\"1\":{\"395\":1}}],[\"局部变量会在函数执行完后被释放\",{\"1\":{\"366\":1}}],[\"局部建模能力弱\",{\"1\":{\"112\":1}}],[\"局部特征实现上下文感知\",{\"1\":{\"112\":1}}],[\"局部特征编码\",{\"1\":{\"92\":1}}],[\"局部特征学习器\",{\"1\":{\"86\":1}}],[\"局部区域\",{\"1\":{\"92\":1}}],[\"局部区域中的每个点将相对于形心所在位置进行调整\",{\"1\":{\"91\":1}}],[\"局部区域中的点转换成相对于形心的局部坐标系\",{\"1\":{\"91\":1}}],[\"局部坐标系转换\",{\"1\":{\"91\":1}}],[\"再针对性进行优化即可\",{\"1\":{\"687\":1}}],[\"再向量化存储到数据库中\",{\"1\":{\"687\":1}}],[\"再求梯度\",{\"1\":{\"667\":1}}],[\"再例如\",{\"1\":{\"630\":1}}],[\"再如\",{\"1\":{\"565\":1}}],[\"再对每个选项做分类打分\",{\"1\":{\"544\":1}}],[\"再经过非线性变换后\",{\"1\":{\"513\":1}}],[\"再经过e11这个编码器\",{\"1\":{\"238\":1}}],[\"再按照80\",{\"1\":{\"512\":1}}],[\"再到基于自注意力机制的transformer架构\",{\"1\":{\"453\":1}}],[\"再使用\",{\"1\":{\"447\":2}}],[\"再没有额外的成本\",{\"1\":{\"420\":1}}],[\"再\",{\"1\":{\"384\":1}}],[\"再计算协方差矩阵\",{\"1\":{\"355\":1}}],[\"再不行尝试源码编译安装\",{\"1\":{\"338\":1}}],[\"再将整个group按照\",{\"1\":{\"328\":1}}],[\"再将融合信息返回点空间\",{\"1\":{\"75\":1}}],[\"再来看一个多维张量的例子\",{\"1\":{\"323\":1}}],[\"再来回顾我们的卷积层计算公式\",{\"1\":{\"291\":1}}],[\"再和真实标签做交叉熵损失\",{\"1\":{\"296\":1}}],[\"再和缓存的key\",{\"1\":{\"285\":1}}],[\"再乘以缩放因子scale\",{\"1\":{\"295\":1}}],[\"再次提取被掩码位置的表示\",{\"1\":{\"513\":1}}],[\"再次应用\",{\"1\":{\"294\":1}}],[\"再次用\",{\"1\":{\"228\":1}}],[\"再映射为\",{\"1\":{\"291\":1}}],[\"再从中心位置裁剪成224x224\",{\"1\":{\"290\":1}}],[\"再进行归一化和标准化处理\",{\"1\":{\"290\":2}}],[\"再进行集成\",{\"1\":{\"138\":1}}],[\"再查看在不同数据集上的表现结果\",{\"1\":{\"237\":1}}],[\"再微调\",{\"1\":{\"202\":1}}],[\"再在具体任务数据上进行监督微调\",{\"1\":{\"464\":1}}],[\"再在其中添加一个分隔符得到\",{\"1\":{\"445\":1}}],[\"再在高质量数据上生成学习\",{\"1\":{\"181\":1}}],[\"再在目标任务上微调\",{\"1\":{\"175\":1}}],[\"再通过跨模态注意力实现深度融合\",{\"1\":{\"150\":1}}],[\"再通过\",{\"1\":{\"140\":1,\"156\":1,\"285\":1,\"404\":1}}],[\"再通过第一个卷积层提取初始特征\",{\"1\":{\"109\":1}}],[\"再由外层\",{\"1\":{\"376\":1}}],[\"再由\",{\"1\":{\"132\":1}}],[\"再与每个点的局部特征拼接\",{\"1\":{\"111\":1}}],[\"再用多模态编码器融合\",{\"1\":{\"149\":1}}],[\"再用跨模态注意力融合\",{\"1\":{\"149\":1}}],[\"再用mlp提提神\",{\"1\":{\"100\":1}}],[\"再用\",{\"1\":{\"92\":1,\"202\":1}}],[\"再决定要使用什么文本查询\",{\"1\":{\"83\":1}}],[\"示例输出\",{\"1\":{\"666\":2}}],[\"示例验证\",{\"1\":{\"659\":1}}],[\"示例显示\",{\"1\":{\"471\":1}}],[\"示例或文本上下文隐式表达\",{\"1\":{\"470\":1}}],[\"示例从开发集提取\",{\"1\":{\"461\":1}}],[\"示例后\",{\"1\":{\"454\":1}}],[\"示例中设置为了1\",{\"1\":{\"410\":1}}],[\"示例\",{\"1\":{\"332\":1,\"395\":2,\"542\":1,\"659\":1}}],[\"示例3\",{\"1\":{\"331\":1}}],[\"示例2\",{\"1\":{\"331\":1}}],[\"示例1\",{\"1\":{\"331\":1}}],[\"示例场景\",{\"1\":{\"231\":1}}],[\"示例文本查询\",{\"1\":{\"83\":1}}],[\"示例数据\",{\"1\":{\"83\":1}}],[\"示例为\",{\"1\":{\"11\":2,\"12\":2}}],[\"白色背景\",{\"1\":{\"83\":1}}],[\"蓝色标记出的部分是提供给llm的示例\",{\"1\":{\"434\":1}}],[\"蓝色\",{\"1\":{\"83\":1}}],[\"蓝色=背景\",{\"1\":{\"83\":1}}],[\"二项分布关注试验次数固定\",{\"1\":{\"579\":1}}],[\"二项分布\",{\"1\":{\"575\":1}}],[\"二者都加同样的辅助lm\",{\"1\":{\"449\":1}}],[\"二者分别从不同实例中采样\",{\"1\":{\"19\":1}}],[\"二是已解决的子问题及其答案列表\",{\"1\":{\"436\":1}}],[\"二值化或软标签\",{\"1\":{\"401\":1}}],[\"二值化显示\",{\"1\":{\"83\":1}}],[\"二维协方差矩阵\",{\"1\":{\"355\":1}}],[\"二维布局\",{\"1\":{\"323\":2}}],[\"二元分类器的每个输出有四种可能的结果\",{\"1\":{\"342\":1}}],[\"二元分类场景\",{\"0\":{\"341\":1}}],[\"二分类task\",{\"0\":{\"284\":1}}],[\"二分类问题\",{\"1\":{\"240\":1}}],[\"二分类\",{\"1\":{\"162\":1}}],[\"二\",{\"0\":{\"306\":1,\"332\":1,\"649\":1},\"1\":{\"112\":1,\"469\":1}}],[\"二次sample\",{\"1\":{\"93\":1}}],[\"灰色\",{\"1\":{\"83\":1}}],[\"颜色橙色并填充\",{\"1\":{\"666\":1}}],[\"颜色\",{\"1\":{\"92\":1,\"114\":1}}],[\"颜色等\",{\"1\":{\"92\":1}}],[\"颜色等信息\",{\"1\":{\"46\":1}}],[\"颜色映射\",{\"1\":{\"83\":1}}],[\"增大批次和训练数据\",{\"1\":{\"502\":1}}],[\"增大批次和数据规模\",{\"1\":{\"493\":1}}],[\"增大批次规模\",{\"1\":{\"492\":1}}],[\"增加网络的表达能力和非线性\",{\"1\":{\"674\":1}}],[\"增加模型大小或使用更多数据\",{\"1\":{\"673\":1}}],[\"增加迭代次数设为\",{\"1\":{\"667\":1}}],[\"增加多样性\",{\"1\":{\"495\":1}}],[\"增加旁路矩阵来模拟全参数微调\",{\"1\":{\"428\":1}}],[\"增加\",{\"1\":{\"427\":1}}],[\"增加了检索步骤的耗时\",{\"1\":{\"681\":1}}],[\"增加了二分类器判断可答性\",{\"1\":{\"494\":1}}],[\"增加了模型层数\",{\"1\":{\"424\":1}}],[\"增加了图像的多样性\",{\"1\":{\"290\":1}}],[\"增加一些特定长度的特殊token\",{\"1\":{\"418\":1}}],[\"增加神经元数量\",{\"1\":{\"395\":2}}],[\"增加阶数\",{\"1\":{\"395\":1}}],[\"增加阶数提高精度\",{\"1\":{\"395\":1}}],[\"增加到设定的最大值\",{\"1\":{\"159\":1}}],[\"增加特征维度\",{\"1\":{\"98\":1}}],[\"增加batch维度\",{\"1\":{\"83\":1}}],[\"增强阶段\",{\"1\":{\"680\":1}}],[\"增强和生成四个阶段\",{\"1\":{\"680\":1}}],[\"增强了模型的推理和理解能力\",{\"1\":{\"679\":1}}],[\"增强了内容的可追溯性\",{\"1\":{\"679\":1}}],[\"增强了\",{\"1\":{\"674\":1}}],[\"增强整体模型的能力\",{\"1\":{\"428\":1}}],[\"增强区域匹配\",{\"1\":{\"402\":1}}],[\"增强模型文字识别能力\",{\"1\":{\"217\":1}}],[\"增强模型生成能力\",{\"1\":{\"190\":1}}],[\"增强视觉理解能力并适配不同llms\",{\"1\":{\"208\":1}}],[\"增强样本多样性\",{\"1\":{\"147\":1}}],[\"增强后的点特征进行卷积\",{\"1\":{\"76\":1}}],[\"增强的点特征图\",{\"1\":{\"74\":1}}],[\"增强点特征的语义判别能力\",{\"1\":{\"70\":1}}],[\"增强现实等领域\",{\"1\":{\"48\":1}}],[\"增强稳定性\",{\"1\":{\"45\":1}}],[\"增强\",{\"1\":{\"45\":2,\"138\":1}}],[\"增强空间特征表达\",{\"1\":{\"45\":1}}],[\"增强类比能力\",{\"1\":{\"12\":1}}],[\"预归一化\",{\"1\":{\"481\":1}}],[\"预热衰减方案\",{\"1\":{\"447\":1}}],[\"预编译的二进制包\",{\"1\":{\"338\":1}}],[\"预处理阶段固定掩码模式\",{\"1\":{\"497\":1}}],[\"预处理的影响\",{\"1\":{\"455\":1}}],[\"预处理这个步骤在论文里并没有详细说明\",{\"1\":{\"290\":1}}],[\"预处理层\",{\"1\":{\"107\":1}}],[\"预训练和微调\",{\"1\":{\"675\":1}}],[\"预训练与微调\",{\"1\":{\"534\":1}}],[\"预训练与迁移学习的趋势\",{\"1\":{\"453\":1}}],[\"预训练任务\",{\"1\":{\"510\":1}}],[\"预训练方法的复制研究\",{\"1\":{\"492\":1}}],[\"预训练梯度\",{\"1\":{\"469\":1}}],[\"预训练结合监督微调的方法在nlp任务中表现突出\",{\"1\":{\"453\":1}}],[\"预训练对于获取不同级别信息的需要\",{\"1\":{\"441\":1}}],[\"预训练+微调\",{\"1\":{\"440\":1}}],[\"预训练过程\",{\"0\":{\"410\":1}}],[\"预训练权重大小为393mb\",{\"1\":{\"300\":1}}],[\"预训练模型下载下来之后\",{\"1\":{\"519\":1}}],[\"预训练模型很容易直接zero\",{\"1\":{\"278\":1}}],[\"预训练模型中\",{\"1\":{\"275\":1}}],[\"预训练模型名称\",{\"1\":{\"275\":1}}],[\"预训练模型路径\",{\"1\":{\"83\":1}}],[\"预训练好的\",{\"1\":{\"226\":1}}],[\"预训练是\",{\"1\":{\"226\":1}}],[\"预训练阶段实际上是将上述两个任务结合起来\",{\"1\":{\"507\":1}}],[\"预训练阶段非常关键\",{\"1\":{\"229\":1}}],[\"预训练阶段\",{\"1\":{\"225\":1}}],[\"预训练视觉编码器+mlp投影器\",{\"1\":{\"219\":1}}],[\"预训练数据与处理\",{\"1\":{\"481\":1}}],[\"预训练数据\",{\"1\":{\"217\":1}}],[\"预训练设置\",{\"0\":{\"174\":1}}],[\"预训练可视为变分自编码器训练\",{\"1\":{\"173\":1}}],[\"预训练总目标函数\",{\"1\":{\"156\":1}}],[\"预训练代码实现部分参考moco论文实现\",{\"1\":{\"147\":1}}],[\"预训练\",{\"0\":{\"147\":1,\"172\":1,\"226\":1,\"534\":1},\"1\":{\"464\":1,\"673\":1}}],[\"预训练环境为两个16\",{\"1\":{\"131\":1}}],[\"预训练细节\",{\"0\":{\"131\":1}}],[\"预训练目标函数\",{\"1\":{\"172\":1}}],[\"预训练目标\",{\"0\":{\"127\":1},\"1\":{\"125\":1}}],[\"预测某个结果\",{\"1\":{\"597\":1}}],[\"预测正确的句对数量\",{\"1\":{\"515\":1}}],[\"预测正确的掩码词数量\",{\"1\":{\"515\":1}}],[\"预测被掩码的词\",{\"1\":{\"513\":1}}],[\"预测出该字的标签\",{\"1\":{\"508\":1}}],[\"预测出一个变换矩阵\",{\"1\":{\"109\":1}}],[\"预测下一个词\",{\"1\":{\"468\":1}}],[\"预测值和真实值都为\",{\"1\":{\"402\":1}}],[\"预测概率\",{\"1\":{\"402\":1}}],[\"预测和真实中所有正类区域之和\",{\"1\":{\"401\":1}}],[\"预测为负但实际为正的像素数量\",{\"1\":{\"405\":1}}],[\"预测为负类\",{\"1\":{\"405\":1}}],[\"预测为负例\",{\"1\":{\"342\":1}}],[\"预测为正但实际为负的像素数量\",{\"1\":{\"405\":1}}],[\"预测为正且实际也为正的像素数量\",{\"1\":{\"405\":1}}],[\"预测为正类\",{\"1\":{\"405\":2}}],[\"预测为正类且实际也为正类的部分\",{\"1\":{\"401\":1}}],[\"预测为正例\",{\"1\":{\"342\":1}}],[\"预测图像对应的文本的词袋模型\",{\"1\":{\"278\":1}}],[\"预测目标为\",{\"1\":{\"172\":1}}],[\"预测头\",{\"1\":{\"98\":1}}],[\"预测掩码\",{\"1\":{\"83\":1}}],[\"预测结果可视化\",{\"1\":{\"83\":1}}],[\"预测结果\",{\"1\":{\"83\":1,\"513\":1}}],[\"预测点云的功能区域掩码\",{\"1\":{\"83\":1}}],[\"预测的\",{\"1\":{\"82\":1}}],[\"预测与\",{\"1\":{\"78\":1,\"401\":1}}],[\"预测\",{\"1\":{\"46\":1}}],[\"预测倾向训练集中已有的\",{\"1\":{\"24\":1}}],[\"杀死训练进程\",{\"1\":{\"83\":1}}],[\"脚本时非常有用\",{\"1\":{\"83\":1}}],[\"会发现线的形状类似香蕉\",{\"1\":{\"667\":1}}],[\"会被正常回收\",{\"1\":{\"657\":1}}],[\"会被立即回收\",{\"1\":{\"657\":1}}],[\"会产生\",{\"1\":{\"600\":1}}],[\"会将这些输入展平\",{\"1\":{\"544\":1}}],[\"会将输入图像分割成大小为\",{\"1\":{\"300\":1}}],[\"会去分步骤思考\",{\"1\":{\"433\":1}}],[\"会去定义一些规则\",{\"1\":{\"234\":1}}],[\"会有多个用于不同目的的权重参数矩阵\",{\"1\":{\"414\":1}}],[\"会改变张量的\",{\"1\":{\"384\":1}}],[\"会自动把这些变量\",{\"1\":{\"366\":1}}],[\"会降低也是如此\",{\"1\":{\"353\":1}}],[\"会在训练初期使用较短序列\",{\"1\":{\"494\":1}}],[\"会在get\",{\"1\":{\"285\":1}}],[\"会在coco数据集每个样本原有caption的基础上添加一个prompt\",{\"1\":{\"142\":1}}],[\"会从缓存中取出对应层先前缓存的key\",{\"1\":{\"285\":1}}],[\"会使用\",{\"1\":{\"285\":1}}],[\"会进行相应的错误提示并返回\",{\"1\":{\"275\":1}}],[\"会进行推理能力结果更新\",{\"1\":{\"83\":1}}],[\"会导致概率的定义出现矛盾或不收敛\",{\"1\":{\"566\":1}}],[\"会导致\",{\"1\":{\"565\":2}}],[\"会导致模型很难收敛\",{\"1\":{\"240\":1}}],[\"会导致非正交\",{\"1\":{\"107\":1}}],[\"会让模型只关注那个困难的负样本\",{\"1\":{\"240\":1}}],[\"会作为\",{\"1\":{\"162\":1}}],[\"会与图像嵌入一起送入多模态编码器进行融合\",{\"1\":{\"152\":1}}],[\"会根据不同功能分支前向三次\",{\"1\":{\"127\":1}}],[\"会影响特征提取的一致性\",{\"1\":{\"107\":1}}],[\"会强制标准输出也像标准错误一样\",{\"1\":{\"83\":1}}],[\"会按照\",{\"1\":{\"31\":1}}],[\"当时的研究主要集中在采用统计学习方法来预测词汇\",{\"1\":{\"673\":1}}],[\"当表达式为3\",{\"1\":{\"660\":1}}],[\"当表达式中的左右操作数类型不同时\",{\"1\":{\"660\":1}}],[\"当执行x\",{\"1\":{\"660\":1}}],[\"当执行表达式a\",{\"1\":{\"660\":1}}],[\"当操作数类型不同时\",{\"1\":{\"660\":1}}],[\"当variable实例不再被其他对象引用时\",{\"1\":{\"657\":1}}],[\"当用户执行完一次前向传播和反向传播后\",{\"1\":{\"657\":1}}],[\"当用户不再引用variable时\",{\"1\":{\"657\":1}}],[\"当tinypytorch处理大量神经网络计算时\",{\"1\":{\"657\":1}}],[\"当对象的引用计数为0时\",{\"1\":{\"657\":1}}],[\"当某个变量被多次用作输入时\",{\"1\":{\"654\":1}}],[\"当然可以\",{\"1\":{\"590\":1}}],[\"当然你也可以将所有词的\",{\"1\":{\"508\":1}}],[\"当我们谈论大模型时\",{\"1\":{\"688\":1}}],[\"当我们使用同一个变量分别进行多次计算时\",{\"1\":{\"653\":1}}],[\"当我们将其评估于实际观测结果\",{\"1\":{\"596\":1}}],[\"当我们讨论连续型随机变量\",{\"1\":{\"566\":1}}],[\"当我们在其他任务中使用预训练好的模型时\",{\"1\":{\"290\":1}}],[\"当且仅当\",{\"1\":{\"566\":1}}],[\"当中包括\",{\"1\":{\"510\":1}}],[\"当训练步数远超过bert的1m步时\",{\"1\":{\"495\":1}}],[\"当任务违反道德或逻辑前提时\",{\"1\":{\"472\":1}}],[\"当模型在足够大且多样化的文本数据\",{\"1\":{\"457\":1}}],[\"当语言模型在足够多样化的文本数据上训练时\",{\"1\":{\"456\":1}}],[\"当两个变量\",{\"1\":{\"355\":3}}],[\"当精确率和召回率相差很大时\",{\"1\":{\"348\":1}}],[\"当精确率和召回率的值接近时\",{\"1\":{\"348\":1}}],[\"当精确率和召回率均为\",{\"1\":{\"348\":1}}],[\"当正例预测的准确性非常重要时\",{\"1\":{\"347\":1}}],[\"当假正例的代价高于假负例时使用\",{\"1\":{\"347\":1}}],[\"当假负例的代价高于假正例时使用\",{\"1\":{\"347\":1}}],[\"当进行转置时\",{\"1\":{\"326\":1}}],[\"当较大时\",{\"1\":{\"318\":1}}],[\"当提到模型参数量时\",{\"1\":{\"297\":1}}],[\"当数据集不均衡时\",{\"1\":{\"352\":1}}],[\"当数据集规模从\",{\"1\":{\"239\":1}}],[\"当数据量小于30m时\",{\"1\":{\"297\":1}}],[\"当使用\",{\"1\":{\"289\":1}}],[\"当cnn具有以上两种归纳偏置\",{\"1\":{\"287\":1}}],[\"当拥有足够多的数据进行预训练的时候\",{\"1\":{\"287\":1}}],[\"当is\",{\"1\":{\"285\":1}}],[\"当文本和query\",{\"1\":{\"284\":1}}],[\"当与\",{\"1\":{\"227\":1}}],[\"当filter模块在coco数据集上\",{\"1\":{\"146\":1}}],[\"当\",{\"1\":{\"143\":1,\"228\":2,\"238\":1,\"294\":1,\"405\":3,\"425\":1,\"567\":1,\"587\":1,\"656\":1}}],[\"当输入点云非常稀疏时\",{\"1\":{\"112\":1}}],[\"当局部区域的密度较高时\",{\"1\":{\"97\":1}}],[\"当局部区域的密度较低时\",{\"1\":{\"97\":1}}],[\"当在\",{\"1\":{\"83\":1}}],[\"当前tinypytorch已能将复杂式子转化为代码\",{\"1\":{\"666\":1}}],[\"当前tinypytorch框架中\",{\"1\":{\"657\":1}}],[\"当前token之前的text\",{\"1\":{\"285\":1}}],[\"当前已经有的词数\",{\"1\":{\"511\":1}}],[\"当前输入词的query\",{\"1\":{\"477\":1}}],[\"当前输入通道数初始化为in\",{\"1\":{\"100\":1}}],[\"当前新\",{\"1\":{\"477\":1}}],[\"当前轮输出token与输入tokens拼接\",{\"1\":{\"474\":1}}],[\"当前对齐对象\",{\"1\":{\"472\":1}}],[\"当前对齐并非通用意义上的\",{\"1\":{\"472\":1}}],[\"当前偏好群体有限\",{\"1\":{\"472\":1}}],[\"当前基于微调的方法存在三个主要问题\",{\"1\":{\"460\":1}}],[\"当前的机器学习系统虽然在特定任务上表现出色\",{\"1\":{\"453\":1}}],[\"当前机器学习系统的局限性\",{\"1\":{\"453\":1}}],[\"当前实现未使用\",{\"1\":{\"404\":1}}],[\"当前研究重点包括\",{\"1\":{\"212\":1}}],[\"当前广泛使用的视觉模型参数量仍停留在约10亿级别\",{\"1\":{\"183\":1}}],[\"当前主分支计算相似度\",{\"1\":{\"161\":1}}],[\"当前入队位置指针\",{\"1\":{\"160\":1}}],[\"当前论文提出了一个基于对比损失的\",{\"1\":{\"149\":1}}],[\"当前batch\",{\"1\":{\"147\":2}}],[\"当前视觉\",{\"1\":{\"120\":1,\"150\":1}}],[\"当前物体待预测的功能区域\",{\"1\":{\"83\":1}}],[\"当前物体类型\",{\"1\":{\"83\":1}}],[\"当前\",{\"1\":{\"82\":1,\"145\":1,\"477\":2}}],[\"当前点云待预测的交互行为\",{\"1\":{\"29\":1}}],[\"当前是训练\",{\"1\":{\"29\":1}}],[\"当前交互\",{\"1\":{\"28\":1}}],[\"当前交互行为\",{\"1\":{\"28\":1}}],[\"就于\",{\"1\":{\"678\":1}}],[\"就像让计算机阅读\",{\"1\":{\"673\":1}}],[\"就像你在地图上两个村庄中间估算温度时\",{\"1\":{\"397\":1}}],[\"就点击这里直接下载\",{\"1\":{\"519\":1}}],[\"就要重新计算所有之前\",{\"1\":{\"475\":1}}],[\"就足够了\",{\"1\":{\"427\":1}}],[\"就得到了函数\",{\"1\":{\"596\":1}}],[\"就得到了这个格子的特征\",{\"1\":{\"397\":1}}],[\"就得到了\",{\"1\":{\"397\":1}}],[\"就必须先调用\",{\"1\":{\"383\":1}}],[\"就看不到真实文档了\",{\"1\":{\"372\":1}}],[\"就能反映\",{\"1\":{\"359\":1}}],[\"就表示模型大约有\",{\"1\":{\"297\":1}}],[\"就意味着模型大约有\",{\"1\":{\"297\":1}}],[\"就完成了从图片到token之间的转换\",{\"1\":{\"291\":1}}],[\"就有了很多先验信息\",{\"1\":{\"287\":1}}],[\"就可以\",{\"1\":{\"326\":1}}],[\"就可以拿到所有样本的特征\",{\"1\":{\"242\":1}}],[\"就可以认为其无限被\",{\"1\":{\"238\":1}}],[\"就会抵达\",{\"1\":{\"667\":1}}],[\"就会进行数据复制以创建新的张量\",{\"1\":{\"385\":1}}],[\"就会导致只有最后一条路径上的梯度被保留\",{\"1\":{\"654\":1}}],[\"就会导致不利用图像信息\",{\"1\":{\"258\":1}}],[\"就会导致特征\",{\"1\":{\"241\":1}}],[\"就会立即在屏幕上显示一个字符\",{\"1\":{\"83\":1}}],[\"就没什么意义\",{\"1\":{\"240\":1}}],[\"就应运而生了\",{\"1\":{\"240\":1}}],[\"就是计算这些\",{\"1\":{\"565\":1}}],[\"就是没有答案\",{\"1\":{\"508\":1}}],[\"就是正常的词向量\",{\"1\":{\"506\":1}}],[\"就是正样本\",{\"1\":{\"240\":1}}],[\"就是随机遮盖或替换一句话里面的任意字或词\",{\"1\":{\"505\":1}}],[\"就是其中一个将此概念付诸实践的例子\",{\"1\":{\"504\":1}}],[\"就是大数据集有效\",{\"1\":{\"449\":1}}],[\"就是猜测被分配到高的概率值的token作为预测值\",{\"1\":{\"449\":1}}],[\"就是识别文本蕴含\",{\"1\":{\"448\":1}}],[\"就是需要更新的参数\",{\"1\":{\"425\":1}}],[\"就是将结构化输入转换为有序序列以便作者预训练能处理\",{\"1\":{\"445\":1}}],[\"就是将y=wx中的w\",{\"1\":{\"419\":1}}],[\"就是将变成\",{\"1\":{\"418\":1}}],[\"就是将传统cnn和transformer进行结合\",{\"1\":{\"299\":1}}],[\"就是fft存在的上述两个问题\",{\"1\":{\"416\":1}}],[\"就是用特定的数据\",{\"1\":{\"416\":1}}],[\"就是用了\",{\"1\":{\"242\":1}}],[\"就是一个包含\",{\"1\":{\"596\":1}}],[\"就是一个不错的方案\",{\"1\":{\"415\":1}}],[\"就是一个序列数据转换的问题\",{\"1\":{\"414\":1}}],[\"就是一个采样点\",{\"1\":{\"397\":1}}],[\"就是一种很好的解决方式\",{\"1\":{\"240\":1}}],[\"就是根据问题找到相关内容并返回\",{\"1\":{\"314\":1}}],[\"就是两个线性层+gelu激活函数+dropout的结构\",{\"1\":{\"294\":1}}],[\"就是两个编码器都可以通过梯度回传进行更新\",{\"1\":{\"242\":1}}],[\"就是利用量化技术的一个变体\",{\"1\":{\"428\":1}}],[\"就是利用一个卷积核大小为16x16\",{\"1\":{\"291\":1}}],[\"就是利用这个动量的特性\",{\"1\":{\"236\":1}}],[\"就是把整个数据集的特征\",{\"1\":{\"242\":1}}],[\"就是把所有的负样本一视同仁\",{\"1\":{\"240\":1}}],[\"就是这个端到端的框架\",{\"1\":{\"242\":1}}],[\"就是这个图\",{\"1\":{\"238\":1}}],[\"就是\",{\"1\":{\"240\":1,\"241\":1,\"242\":2,\"397\":1}}],[\"就是我们要把给llm做的任务尽可能细化\",{\"1\":{\"432\":1}}],[\"就是我们的负样本数量\",{\"1\":{\"240\":1}}],[\"就是我们的\",{\"1\":{\"240\":1}}],[\"就是我们的模型在大量无标注的数据集上进行训练之后\",{\"1\":{\"238\":1}}],[\"就是我们不需要知道前两张图片是人这个类别\",{\"1\":{\"234\":1}}],[\"就是想得到一个模型\",{\"1\":{\"238\":1}}],[\"就是数据集中剩余的所有图片都是负样本\",{\"1\":{\"238\":1}}],[\"就是它可以从大量未标注的数据上学习到特征\",{\"1\":{\"237\":1}}],[\"就是不想让当前时刻的输出只是依赖于当前时刻的输入\",{\"1\":{\"236\":1}}],[\"就是代理任务是多样性的\",{\"1\":{\"235\":1}}],[\"就是第一张图片和第二张图片是同一个类别\",{\"1\":{\"234\":1}}],[\"就是经过\",{\"1\":{\"74\":1}}],[\"就拼接起来\",{\"1\":{\"100\":1}}],[\"就更新它\",{\"1\":{\"92\":1}}],[\"存在的问题\",{\"1\":{\"687\":1}}],[\"存在某些函数\",{\"1\":{\"395\":1}}],[\"存在一定的噪声\",{\"1\":{\"278\":1}}],[\"存在缓存机制\",{\"1\":{\"83\":1}}],[\"存储每个类别的样本总数\",{\"1\":{\"289\":1}}],[\"存储每次选出的\",{\"1\":{\"92\":1}}],[\"存储验证集图片对应索引信息\",{\"1\":{\"289\":1}}],[\"存储验证集的所有图片路径\",{\"1\":{\"289\":1}}],[\"存储训练集图片对应索引信息\",{\"1\":{\"289\":1}}],[\"存储训练集的所有图片路径\",{\"1\":{\"289\":1}}],[\"存储是为了快速矩阵乘\",{\"1\":{\"249\":1}}],[\"存储图像描述引导提示词\",{\"1\":{\"142\":1}}],[\"存储数组\",{\"1\":{\"82\":1}}],[\"存储了物体边界框文件路径\",{\"1\":{\"58\":1}}],[\"均能支持流式处理\",{\"1\":{\"684\":1}}],[\"均能正确构建反向传播路径\",{\"1\":{\"662\":1}}],[\"均是混合推理模型\",{\"1\":{\"674\":1}}],[\"均经过严格去重\",{\"1\":{\"481\":1}}],[\"均值和方差\",{\"1\":{\"581\":1}}],[\"均值\",{\"1\":{\"389\":1}}],[\"均值是\",{\"1\":{\"359\":1}}],[\"均值作为最终评估指标\",{\"1\":{\"82\":1}}],[\"均冻结\",{\"1\":{\"280\":1}}],[\"均有性能提升\",{\"1\":{\"132\":1}}],[\"均匀性假设\",{\"1\":{\"90\":1}}],[\"均保持不变\",{\"1\":{\"30\":1}}],[\"取得了显著进展\",{\"1\":{\"501\":1}}],[\"取得了领先的零样本分类准确率\",{\"1\":{\"193\":1}}],[\"取决于是否能容纳在2048\",{\"1\":{\"461\":1}}],[\"取第1行到第3行\",{\"1\":{\"325\":1}}],[\"取第一个cls\",{\"1\":{\"282\":1}}],[\"取每个位置的平均作为最终的匹配得分\",{\"1\":{\"284\":1}}],[\"取\",{\"1\":{\"284\":1}}],[\"取双向平均\",{\"1\":{\"161\":1}}],[\"取出出现次数最多的字符对\",{\"1\":{\"411\":2}}],[\"取出该区域内的最大值\",{\"1\":{\"396\":1}}],[\"取出该行中得分最大的那一列\",{\"1\":{\"276\":1}}],[\"取出\",{\"1\":{\"161\":2}}],[\"取出图像\",{\"1\":{\"142\":1}}],[\"取出当前批次的图像列表\",{\"1\":{\"275\":1}}],[\"取出当前最远点的坐标\",{\"1\":{\"92\":1}}],[\"取出当前样本对应的点云数据\",{\"1\":{\"68\":1}}],[\"取最小值\",{\"1\":{\"115\":1}}],[\"取最小的三个距离\",{\"1\":{\"100\":1}}],[\"取所有点的平均值\",{\"1\":{\"115\":1}}],[\"取所有点的最大值\",{\"1\":{\"115\":1}}],[\"取对应的索引\",{\"1\":{\"100\":1}}],[\"取均值或求和\",{\"1\":{\"401\":1}}],[\"取均值\",{\"1\":{\"82\":1}}],[\"||i\",{\"1\":{\"105\":1}}],[\"|\",{\"1\":{\"82\":4,\"267\":4,\"397\":8,\"515\":2,\"520\":2}}],[\"直到达到一个较为稳定\",{\"1\":{\"687\":1}}],[\"直到观察到\",{\"1\":{\"579\":1}}],[\"直到抽出\",{\"1\":{\"578\":1}}],[\"直到无法合并为止\",{\"1\":{\"411\":3}}],[\"直线距离\",{\"1\":{\"357\":1}}],[\"直观解释如下\",{\"0\":{\"592\":1}}],[\"直观理解\",{\"1\":{\"357\":1,\"358\":1,\"395\":1}}],[\"直观上\",{\"1\":{\"355\":1}}],[\"直方图交集\",{\"1\":{\"82\":1}}],[\"直接更新检索知识库\",{\"1\":{\"681\":1}}],[\"直接在大规模强化学习\",{\"1\":{\"674\":1}}],[\"直接在监督学习任务上训练\",{\"1\":{\"449\":1}}],[\"直接处理和分析上传的数据文件\",{\"1\":{\"674\":1}}],[\"直接处理的原始点特征\",{\"1\":{\"97\":1}}],[\"直接进行运算\",{\"1\":{\"660\":1}}],[\"直接计算概率\",{\"1\":{\"454\":1}}],[\"直接微调模型\",{\"1\":{\"445\":1}}],[\"直接影响着大模型给出答案的正确与否\",{\"1\":{\"430\":1}}],[\"直接丢给它个指令\",{\"1\":{\"430\":1}}],[\"直接优化\",{\"1\":{\"401\":1}}],[\"直接套到对比学习中去\",{\"1\":{\"240\":1}}],[\"直接照搬\",{\"1\":{\"239\":1}}],[\"直接使用\",{\"1\":{\"477\":1}}],[\"直接使用的就是交叉熵损失函数代码\",{\"1\":{\"240\":1}}],[\"直接使用原始任务描述\",{\"1\":{\"231\":1}}],[\"直接使用internvit\",{\"1\":{\"189\":1}}],[\"直接预测掩码块的原始像素会导致模型过度关注短程依赖和高频细节\",{\"1\":{\"166\":1}}],[\"直接返回\",{\"1\":{\"163\":1}}],[\"直接用prompt\",{\"1\":{\"415\":1}}],[\"直接用\",{\"1\":{\"159\":1}}],[\"直接以点集作为输入\",{\"1\":{\"103\":1}}],[\"直接复制其特征到所有原始点\",{\"1\":{\"100\":1}}],[\"直接提取的特征\",{\"1\":{\"97\":1}}],[\"直接对图像进行分类\",{\"1\":{\"273\":2}}],[\"直接对所有点进行特征提取\",{\"1\":{\"92\":1}}],[\"直接对整句进行编码\",{\"1\":{\"31\":1}}],[\"直接输入冻结参数的\",{\"1\":{\"286\":1}}],[\"直接输入\",{\"1\":{\"82\":1}}],[\"直接评价分割精度\",{\"1\":{\"82\":1}}],[\"直接添加\",{\"1\":{\"68\":1}}],[\"直接建模跨模态特征相似性\",{\"1\":{\"52\":1}}],[\"直接关联结构与功能\",{\"1\":{\"51\":1}}],[\"直接拼接图像与点云特征作为输入\",{\"1\":{\"22\":1}}],[\"`j+1`\",{\"1\":{\"323\":1}}],[\"`j`\",{\"1\":{\"323\":1}}],[\"`2`\",{\"1\":{\"323\":2}}],[\"`1`\",{\"1\":{\"323\":2}}],[\"`i+1`\",{\"1\":{\"323\":2}}],[\"`i`\",{\"1\":{\"323\":2}}],[\"`3`\",{\"1\":{\"323\":2}}],[\"`\",{\"1\":{\"82\":1,\"323\":8}}],[\"角度\",{\"1\":{\"82\":1}}],[\"排列\",{\"0\":{\"600\":1}}],[\"排列不变性\",{\"1\":{\"112\":1}}],[\"排行榜提交\",{\"1\":{\"499\":1}}],[\"排序为\",{\"1\":{\"278\":1}}],[\"排序\",{\"1\":{\"82\":1,\"289\":1}}],[\"排除与几何矛盾的意图\",{\"1\":{\"32\":1}}],[\"∞\",{\"1\":{\"82\":1,\"403\":2}}],[\"否\",{\"1\":{\"82\":8,\"231\":3,\"387\":2,\"403\":3}}],[\"否则取\",{\"1\":{\"576\":1}}],[\"否则答案可能不合理\",{\"1\":{\"542\":1}}],[\"否则保存这两个句子的\",{\"1\":{\"511\":1}}],[\"否则对齐仅为形式上的\",{\"1\":{\"472\":1}}],[\"否则应注释掉这行\",{\"1\":{\"401\":1}}],[\"否则返回视图\",{\"1\":{\"385\":1}}],[\"否则可能报错\",{\"1\":{\"384\":1}}],[\"否则为恒等映射\",{\"1\":{\"296\":1}}],[\"否则存入训练集\",{\"1\":{\"289\":1}}],[\"否则\",{\"1\":{\"112\":1}}],[\"否则只有\",{\"1\":{\"93\":1}}],[\"否则使用恒等映射\",{\"1\":{\"294\":1}}],[\"否则使用\",{\"1\":{\"92\":1,\"291\":2}}],[\"否则使用给定的\",{\"1\":{\"43\":1}}],[\"否则固定返回问题0\",{\"1\":{\"68\":1}}],[\"❌\",{\"1\":{\"82\":8,\"103\":1,\"116\":1,\"387\":5,\"403\":3,\"407\":6,\"542\":2}}],[\"✅\",{\"0\":{\"312\":1,\"313\":1,\"314\":1},\"1\":{\"82\":8,\"105\":5,\"107\":3,\"109\":2,\"111\":2,\"112\":12,\"115\":7,\"116\":1,\"367\":2,\"372\":3,\"387\":4,\"403\":8,\"407\":14,\"540\":2,\"542\":4,\"544\":1}}],[\"成为了当今计算机科学和人工智能领域的重要研究和应用方向\",{\"1\":{\"675\":1}}],[\"成为了现象级爆火应用\",{\"1\":{\"674\":1}}],[\"成为史上增长最快的\",{\"1\":{\"674\":1}}],[\"成为当时史上用户增长最快的消费级应用程序\",{\"1\":{\"674\":1}}],[\"成为首个在零样本设置下接近监督模型性能的大规模语言模型\",{\"1\":{\"454\":1}}],[\"成立的样本点集合\",{\"1\":{\"565\":1}}],[\"成多头格式\",{\"1\":{\"531\":1}}],[\"成本效益\",{\"1\":{\"674\":1}}],[\"成本低\",{\"1\":{\"472\":1}}],[\"成本较高\",{\"1\":{\"353\":1}}],[\"成功次数随机\",{\"1\":{\"579\":1}}],[\"成功\",{\"1\":{\"579\":2}}],[\"成功为蓝球\",{\"0\":{\"579\":1}}],[\"成功生成json文件\",{\"1\":{\"510\":2}}],[\"成功弥合了视觉模型与大型语言模型之间的能力与表示鸿沟\",{\"1\":{\"198\":1}}],[\"成功实现了视觉与语言模型在参数规模和特征表示上的协调\",{\"1\":{\"180\":1}}],[\"成一个大的局部区域\",{\"1\":{\"92\":1}}],[\"成\",{\"1\":{\"82\":1,\"107\":1,\"544\":1}}],[\"影响内容的可信度\",{\"1\":{\"679\":1}}],[\"影响泛化能力\",{\"1\":{\"495\":1}}],[\"影响模型性能\",{\"1\":{\"424\":1}}],[\"影响模型容量和梯度稳定性\",{\"1\":{\"313\":1}}],[\"影响比较大的问题\",{\"1\":{\"416\":1}}],[\"影响显著\",{\"1\":{\"397\":1}}],[\"影响\",{\"1\":{\"82\":1}}],[\"敏感度有限\",{\"1\":{\"82\":1}}],[\"绘制等高线图\",{\"1\":{\"667\":1}}],[\"绘制到图表中\",{\"1\":{\"350\":1}}],[\"绘制每种类别个数柱状图\",{\"1\":{\"289\":1}}],[\"绘制\",{\"1\":{\"82\":1,\"350\":1}}],[\"还具备网页浏览\",{\"1\":{\"674\":1}}],[\"还确保了复杂表达式的自动微分正确性\",{\"1\":{\"660\":1}}],[\"还可以推导出全概率公式\",{\"1\":{\"569\":1}}],[\"还可以定义累积分布函数\",{\"1\":{\"566\":1}}],[\"还将一个工具方法整合到了分词器的实现之中\",{\"1\":{\"511\":1}}],[\"还未进行初始化\",{\"1\":{\"477\":1}}],[\"还评估模型在\",{\"1\":{\"470\":1}}],[\"还借鉴了对语言模型潜在风险的研究\",{\"1\":{\"469\":1}}],[\"还有很多种\",{\"1\":{\"421\":1}}],[\"还有另一个方向\",{\"1\":{\"278\":1}}],[\"还需要对每个像素进行分类\",{\"1\":{\"399\":1}}],[\"还考虑数据在各个维度上的方差大小和维度间的相关性\",{\"1\":{\"358\":1}}],[\"还考虑了从更低分辨率\",{\"1\":{\"97\":1}}],[\"还是只是记住了相似的训练样本\",{\"1\":{\"463\":1}}],[\"还是\",{\"1\":{\"326\":1}}],[\"还不如直接预测词袋模型\",{\"1\":{\"278\":1}}],[\"还在336的分辨率下额外进行了一个周期的微调\",{\"1\":{\"272\":1}}],[\"还在多项视觉\",{\"1\":{\"253\":1}}],[\"还好\",{\"1\":{\"242\":1}}],[\"还希望和之前时刻的输出有关系\",{\"1\":{\"236\":1}}],[\"还小\",{\"1\":{\"92\":1}}],[\"还能够处理更复杂的自动微分任务\",{\"1\":{\"665\":1}}],[\"还能提高其泛化能力\",{\"1\":{\"472\":1}}],[\"还能提供更灵活的语义参考\",{\"1\":{\"157\":1}}],[\"还能计算任意条件概率\",{\"1\":{\"454\":1}}],[\"还能生成与\",{\"1\":{\"82\":1}}],[\"还能感知其他\",{\"1\":{\"76\":1}}],[\"还看响应强度分布\",{\"1\":{\"82\":1}}],[\"⚠️\",{\"1\":{\"82\":4,\"112\":1,\"115\":1,\"372\":2,\"396\":1,\"401\":1,\"403\":1,\"542\":1}}],[\"衡量点与均值之间的距离\",{\"1\":{\"355\":1}}],[\"衡量模型预测分布与动量模型生成的软标签之间的差异\",{\"1\":{\"157\":1}}],[\"衡量模型对二分类问题的判别能力\",{\"1\":{\"82\":1}}],[\"衡量空间重合度\",{\"1\":{\"82\":1}}],[\"衡量分类器整体判别能力\",{\"1\":{\"82\":1}}],[\"衡量分类器排序能力\",{\"1\":{\"82\":1}}],[\"衡量分布相似性\",{\"1\":{\"82\":2}}],[\"衡量逐点误差\",{\"1\":{\"82\":2}}],[\"衡量预测区域与真实标签之间的空间重合度\",{\"1\":{\"82\":1}}],[\"衡量预测掩码与\",{\"1\":{\"78\":1}}],[\"衡量整体分布一致性\",{\"1\":{\"82\":1}}],[\"✔️\",{\"1\":{\"82\":11,\"402\":4,\"403\":3,\"407\":4}}],[\"特殊情况说明\",{\"0\":{\"580\":1}}],[\"特殊\",{\"1\":{\"542\":1}}],[\"特殊操作后的中间结果\",{\"1\":{\"385\":1}}],[\"特定任务输入转换\",{\"0\":{\"445\":1}}],[\"特别强化了对客观事实的准确性\",{\"1\":{\"674\":1}}],[\"特别值得注意的是\",{\"1\":{\"456\":1}}],[\"特别适用于图像分割任务\",{\"1\":{\"407\":1}}],[\"特别是语言建模\",{\"1\":{\"510\":1}}],[\"特别是降后期的推理成本\",{\"1\":{\"421\":1}}],[\"特别是在小数据集\",{\"1\":{\"455\":1}}],[\"特别是在像\",{\"1\":{\"409\":1}}],[\"特别是在遮挡严重的情况下\",{\"1\":{\"112\":1}}],[\"特别是\",{\"1\":{\"304\":1}}],[\"特点\",{\"1\":{\"402\":2,\"674\":4}}],[\"特点与作用\",{\"1\":{\"82\":4}}],[\"特性\",{\"1\":{\"82\":4,\"338\":1,\"387\":1,\"403\":1,\"659\":1}}],[\"特征比较\",{\"1\":{\"681\":2}}],[\"特征图\",{\"1\":{\"396\":1}}],[\"特征值分解\",{\"1\":{\"355\":1}}],[\"特征队列初始化\",{\"1\":{\"147\":1}}],[\"特征映射到共享空间\",{\"1\":{\"147\":1}}],[\"特征投影层\",{\"1\":{\"145\":1}}],[\"特征融合\",{\"1\":{\"115\":1}}],[\"特征融合准备\",{\"1\":{\"59\":1}}],[\"特征空间一致\",{\"1\":{\"190\":1}}],[\"特征空间变换矩阵\",{\"1\":{\"109\":1}}],[\"特征空间对齐\",{\"1\":{\"52\":1}}],[\"特征变换开关\",{\"1\":{\"109\":1}}],[\"特征插值方式\",{\"1\":{\"98\":1}}],[\"特征传播层\",{\"0\":{\"100\":1}}],[\"特征传播\",{\"1\":{\"98\":1,\"100\":1}}],[\"特征传播阶段结束\",{\"1\":{\"70\":1}}],[\"特征传播阶段\",{\"1\":{\"70\":1}}],[\"特征编码\",{\"1\":{\"91\":1}}],[\"特征通道维度\",{\"1\":{\"76\":1}}],[\"特征增强\",{\"1\":{\"59\":1,\"115\":1}}],[\"特征分布对齐损失\",{\"1\":{\"55\":1}}],[\"特征提取网络相对轻量\",{\"1\":{\"280\":1}}],[\"特征提取\",{\"0\":{\"109\":1},\"1\":{\"54\":1}}],[\"特征维度\",{\"1\":{\"46\":1}}],[\"特征从原始嵌入维度\",{\"1\":{\"42\":1}}],[\"特征\",{\"1\":{\"40\":2,\"100\":1,\"160\":1}}],[\"标记进行下一句预测\",{\"1\":{\"513\":1}}],[\"标记\",{\"1\":{\"285\":1}}],[\"标记序列开始\",{\"1\":{\"126\":1}}],[\"标记为\",{\"1\":{\"82\":1}}],[\"标准与示例\",{\"1\":{\"472\":1}}],[\"标准提供训练信号\",{\"1\":{\"470\":1}}],[\"标准交叉熵损失\",{\"1\":{\"404\":1}}],[\"标准\",{\"1\":{\"163\":1,\"404\":2}}],[\"标准差单位距离\",{\"1\":{\"359\":1}}],[\"标准差\",{\"1\":{\"115\":1}}],[\"标准化\",{\"1\":{\"358\":1,\"359\":1}}],[\"标准化输出\",{\"1\":{\"163\":1}}],[\"标准化输入点云和特征空间\",{\"1\":{\"103\":1}}],[\"标准化的意义\",{\"1\":{\"107\":1}}],[\"标准错误\",{\"1\":{\"83\":1}}],[\"标准输出\",{\"1\":{\"83\":1}}],[\"标签为x\",{\"1\":{\"666\":1}}],[\"标签平滑\",{\"1\":{\"514\":1}}],[\"标签过度惩罚合理预测\",{\"1\":{\"157\":1}}],[\"标签约束\",{\"1\":{\"157\":1}}],[\"标签的监督会一律惩罚这些\",{\"1\":{\"157\":1}}],[\"标签二值化\",{\"1\":{\"82\":1}}],[\"标签\",{\"1\":{\"82\":1,\"154\":1,\"162\":1,\"163\":2,\"240\":1,\"268\":1}}],[\"标注者\",{\"1\":{\"472\":1}}],[\"标注者人数有限\",{\"1\":{\"472\":1}}],[\"标注者受其引导\",{\"1\":{\"472\":1}}],[\"标注者的偏好\",{\"1\":{\"472\":1}}],[\"标注者之间都保持一致\",{\"1\":{\"471\":1}}],[\"标注者实验\",{\"1\":{\"470\":1}}],[\"标注者在判断指令时需考虑信息准确性\",{\"1\":{\"470\":1}}],[\"标注者创作的\",{\"1\":{\"470\":1}}],[\"标注数据昂贵又耗时\",{\"1\":{\"440\":1}}],[\"标注数据组织形式\",{\"1\":{\"68\":1}}],[\"标注图像中的物体及其位置\",{\"1\":{\"226\":1}}],[\"标注\",{\"0\":{\"64\":1}}],[\"标注策略\",{\"0\":{\"18\":1}}],[\"平方\",{\"1\":{\"590\":1}}],[\"平滑系数\",{\"1\":{\"405\":1,\"407\":1}}],[\"平滑项\",{\"1\":{\"401\":1,\"402\":2,\"403\":1,\"404\":1}}],[\"平滑过的对比目标\",{\"1\":{\"161\":1}}],[\"平滑处理\",{\"1\":{\"115\":1}}],[\"平衡正负样本数量\",{\"1\":{\"404\":1}}],[\"平衡类别数量\",{\"1\":{\"404\":1}}],[\"平衡效率与精度\",{\"1\":{\"221\":1}}],[\"平衡其指导作用\",{\"1\":{\"157\":1}}],[\"平衡因子\",{\"1\":{\"78\":1,\"404\":1}}],[\"平移等变换具有鲁棒性\",{\"1\":{\"105\":1}}],[\"平移\",{\"1\":{\"104\":1,\"107\":1,\"116\":1}}],[\"平移中心到以关键点为原点的局部坐标系上\",{\"1\":{\"92\":1}}],[\"平均偏见得分66\",{\"1\":{\"484\":1}}],[\"平均重叠率3\",{\"1\":{\"455\":1}}],[\"平均估计\",{\"1\":{\"397\":2}}],[\"平均对称\",{\"1\":{\"397\":1}}],[\"平均可达\",{\"1\":{\"194\":1}}],[\"平均精度达到\",{\"1\":{\"193\":1}}],[\"平均池化\",{\"1\":{\"115\":1}}],[\"平均值作为损失项\",{\"1\":{\"108\":1}}],[\"平均交并比\",{\"1\":{\"82\":1}}],[\"平均绝对误差\",{\"1\":{\"82\":1}}],[\"反而有所提升\",{\"1\":{\"495\":1}}],[\"反而更\",{\"1\":{\"471\":1}}],[\"反复多次\",{\"1\":{\"474\":1}}],[\"反毒性\",{\"1\":{\"472\":1}}],[\"反应很快\",{\"1\":{\"433\":1}}],[\"反推出来的\",{\"1\":{\"404\":1}}],[\"反之\",{\"1\":{\"353\":1}}],[\"反之亦然\",{\"1\":{\"94\":1}}],[\"反过来计算text\",{\"1\":{\"283\":1}}],[\"反射中都非常重要\",{\"1\":{\"372\":1}}],[\"反射\",{\"1\":{\"108\":1,\"112\":1,\"117\":1}}],[\"反距离加权插值\",{\"1\":{\"98\":1}}],[\"反映出互联网用户对于聊天和对话这种交互模式的偏好\",{\"1\":{\"678\":1}}],[\"反映社会偏见\",{\"1\":{\"482\":1}}],[\"反映模型是否准确学习语言引导下的响应强度\",{\"1\":{\"82\":1}}],[\"反映点与功能核心区域的距离远近\",{\"1\":{\"64\":1}}],[\"反向传播正确计算了每个变量的梯度\",{\"1\":{\"660\":1}}],[\"反向传播时需按此计算梯度\",{\"1\":{\"660\":2}}],[\"反向传播时需要将上游传来的梯度gy分别乘以x1和x0\",{\"1\":{\"660\":1}}],[\"反向传播时将上游梯度取反\",{\"1\":{\"660\":1}}],[\"反向传播时可直接获取这些值\",{\"1\":{\"630\":1}}],[\"反向传播过程中它的梯度应累加\",{\"1\":{\"654\":1}}],[\"反向传播过程中\",{\"1\":{\"627\":1}}],[\"反向传播用于计算输入x对输出y大小变化的影响\",{\"1\":{\"626\":1}}],[\"反向传播按从输出到输入的顺序计算导数\",{\"1\":{\"626\":1}}],[\"反向传播的自动化\",{\"0\":{\"633\":1}}],[\"反向传播的执行\",{\"0\":{\"632\":1}}],[\"反向传播的核心是依据链式法则\",{\"1\":{\"630\":1}}],[\"反向传播的方向\",{\"0\":{\"626\":1}}],[\"反向传播的理论基础是链式法则\",{\"1\":{\"625\":1}}],[\"反向传播的理论知识\",{\"0\":{\"624\":1}}],[\"反向传播更新模型参数过程\",{\"1\":{\"423\":1}}],[\"反向传播\",{\"1\":{\"81\":2,\"142\":1,\"161\":1,\"648\":1,\"660\":1}}],[\"损失之间的权重分配\",{\"1\":{\"407\":1}}],[\"损失权重仅降低\",{\"1\":{\"404\":1}}],[\"损失权重降低\",{\"1\":{\"404\":1}}],[\"损失几乎不受影响\",{\"1\":{\"404\":1}}],[\"损失被大幅降低\",{\"1\":{\"404\":1}}],[\"损失越小表示预测越接近真实标签\",{\"1\":{\"403\":1}}],[\"损失结合在一起的一种损失函数\",{\"1\":{\"402\":1}}],[\"损失值逐渐降低\",{\"1\":{\"292\":1}}],[\"损失也会很大\",{\"1\":{\"241\":1}}],[\"损失\",{\"1\":{\"149\":1,\"163\":1,\"268\":1}}],[\"损失计算与反向传播\",{\"1\":{\"292\":1}}],[\"损失计算的是整个字典做多分类\",{\"1\":{\"241\":1}}],[\"损失计算\",{\"1\":{\"81\":1}}],[\"损失函数直接作用于\",{\"1\":{\"292\":1}}],[\"损失函数定义\",{\"1\":{\"80\":1}}],[\"损失函数\",{\"0\":{\"77\":1,\"400\":1},\"1\":{\"240\":2,\"403\":1,\"407\":1,\"514\":1}}],[\"损失函数由\",{\"1\":{\"15\":1}}],[\"更长的上下文长度\",{\"1\":{\"674\":1}}],[\"更长训练步数\",{\"1\":{\"498\":1}}],[\"更进一步划分\",{\"1\":{\"674\":1}}],[\"更接近真实框架的自动微分系统\",{\"1\":{\"650\":1}}],[\"更因为其背后复杂而精妙的自动微分系统\",{\"1\":{\"650\":1}}],[\"更容易处理复杂计算图\",{\"1\":{\"639\":1}}],[\"更容易训练\",{\"1\":{\"107\":1}}],[\"更形式化地说\",{\"1\":{\"568\":1}}],[\"更大批次\",{\"1\":{\"500\":1}}],[\"更大批次训练\",{\"1\":{\"492\":1}}],[\"更大规模数据\",{\"1\":{\"498\":1}}],[\"更大的词表\",{\"1\":{\"674\":1}}],[\"更大的批次规模\",{\"1\":{\"491\":1}}],[\"更大的模型在几乎所有任务上都显著优于小模型\",{\"1\":{\"455\":1}}],[\"更大的模型在多个基准测试中达到了最先进水平\",{\"1\":{\"452\":1}}],[\"更多的训练数据量\",{\"1\":{\"674\":1}}],[\"更多的数据\",{\"1\":{\"491\":1}}],[\"更多分布使用到的时候再进行补充\",{\"1\":{\"587\":1}}],[\"更多数据是关键\",{\"1\":{\"500\":1}}],[\"更多图块\",{\"1\":{\"221\":1}}],[\"更严格的数据清洗\",{\"1\":{\"484\":1}}],[\"更符合用户指令\",{\"1\":{\"472\":1}}],[\"更不代表所有受语言模型影响的人群\",{\"1\":{\"472\":1}}],[\"更受欢迎\",{\"1\":{\"472\":1}}],[\"更受偏好\",{\"1\":{\"471\":1}}],[\"更常\",{\"1\":{\"471\":1}}],[\"更少毒性\",{\"1\":{\"472\":1}}],[\"更少生成有毒文本\",{\"1\":{\"471\":1}}],[\"更少生成有害内容\",{\"1\":{\"469\":1}}],[\"更少编造\",{\"1\":{\"471\":1}}],[\"更少\",{\"1\":{\"471\":1}}],[\"更具可控性\",{\"1\":{\"472\":1}}],[\"更具\",{\"1\":{\"469\":1}}],[\"更具体地说\",{\"1\":{\"351\":1}}],[\"更加符合人类的一些期望\",{\"1\":{\"416\":1}}],[\"更好的提升大模型在特定领域的能力\",{\"1\":{\"415\":1}}],[\"更好地遵循指令\",{\"1\":{\"471\":1}}],[\"更好地理解和执行用户给出的自然语言指令\",{\"1\":{\"224\":1}}],[\"更好地应对\",{\"1\":{\"78\":1}}],[\"更稳定的训练过程\",{\"1\":{\"407\":1}}],[\"更稳定地收敛\",{\"1\":{\"402\":1}}],[\"更重视精确率\",{\"1\":{\"405\":1}}],[\"更重视召回率\",{\"1\":{\"405\":1}}],[\"更贴近最终评估指标\",{\"1\":{\"403\":1}}],[\"更贴近真实用户交互\",{\"1\":{\"231\":1}}],[\"更适合长训练周期\",{\"1\":{\"495\":1}}],[\"更适合用于\",{\"1\":{\"471\":1}}],[\"更适合作为用户助手\",{\"1\":{\"471\":1}}],[\"更适合前景极少的小区域识别\",{\"1\":{\"402\":1}}],[\"更适合评估边界模糊区域\",{\"1\":{\"403\":1}}],[\"更适合评估\",{\"1\":{\"82\":1}}],[\"更适应局部特征\",{\"1\":{\"395\":1}}],[\"更灵活适应目标函数\",{\"1\":{\"395\":1}}],[\"更能遵守\",{\"1\":{\"471\":1}}],[\"更能反映真实的\",{\"1\":{\"360\":1}}],[\"更能增强模型将视觉信息转化为自然语言的能力\",{\"1\":{\"127\":1}}],[\"更广泛地说\",{\"1\":{\"348\":1}}],[\"更关注召回率\",{\"1\":{\"407\":1}}],[\"更关注精确率\",{\"1\":{\"407\":1}}],[\"更关注整体区域匹配\",{\"1\":{\"401\":1}}],[\"更关注\",{\"1\":{\"242\":1,\"401\":1}}],[\"更强的任务泛化能力\",{\"1\":{\"231\":1}}],[\"更强的视觉主干\",{\"1\":{\"132\":1}}],[\"更偏好本文模型\",{\"1\":{\"178\":1}}],[\"更倾向于生成数据集中常见的\",{\"1\":{\"133\":1}}],[\"更复杂的对称函数建模\",{\"1\":{\"115\":1}}],[\"更有效\",{\"1\":{\"105\":1}}],[\"更新成本高\",{\"1\":{\"681\":1}}],[\"更新至\",{\"1\":{\"674\":1}}],[\"更新判断\",{\"1\":{\"596\":1}}],[\"更新项\",{\"1\":{\"470\":1}}],[\"更新特征数量为表示层的维度\",{\"1\":{\"296\":1}}],[\"更新会非常缓慢\",{\"1\":{\"238\":1}}],[\"更新动量编码器参数\",{\"1\":{\"161\":1}}],[\"更新动量队列\",{\"1\":{\"145\":1}}],[\"更新负样本队列\",{\"1\":{\"147\":1}}],[\"更新\",{\"1\":{\"147\":1,\"160\":1}}],[\"更新队列\",{\"1\":{\"145\":1,\"161\":1}}],[\"更新模型参数\",{\"1\":{\"142\":1}}],[\"更新下一层的输入通道数\",{\"1\":{\"100\":1}}],[\"更新索引\",{\"1\":{\"82\":1}}],[\"更高效\",{\"1\":{\"650\":1}}],[\"更高效的字节级\",{\"1\":{\"492\":1}}],[\"更高效的性能\",{\"1\":{\"26\":1}}],[\"更高毒性\",{\"1\":{\"471\":1}}],[\"更高分辨率\",{\"1\":{\"97\":1}}],[\"更易于管理的子集\",{\"1\":{\"86\":1}}],[\"抑制简单样本的梯度主导\",{\"1\":{\"404\":1}}],[\"抑制简单样本\",{\"1\":{\"404\":1}}],[\"抑制\",{\"1\":{\"78\":1,\"404\":1}}],[\"正在改变着我们与技术互动的方式\",{\"1\":{\"678\":1}}],[\"正式开源\",{\"1\":{\"674\":1}}],[\"正式开源了\",{\"1\":{\"674\":1}}],[\"正式发布了其稳定版本\",{\"1\":{\"684\":1}}],[\"正式发布\",{\"1\":{\"674\":1}}],[\"正向传播\",{\"1\":{\"660\":1}}],[\"正向图文对的前向传播\",{\"1\":{\"162\":1}}],[\"正比于\",{\"1\":{\"596\":1}}],[\"正态程度\",{\"1\":{\"586\":1}}],[\"正态分布\",{\"0\":{\"584\":1}}],[\"正常\",{\"1\":{\"566\":1}}],[\"正常情况下\",{\"1\":{\"366\":1}}],[\"正面出现次数\",{\"1\":{\"565\":1}}],[\"正面影响\",{\"1\":{\"472\":1}}],[\"正则化\",{\"1\":{\"674\":1}}],[\"正则化损失\",{\"0\":{\"108\":1}}],[\"正则\",{\"1\":{\"447\":1}}],[\"正则匹配含有\",{\"1\":{\"410\":1}}],[\"正相关\",{\"1\":{\"355\":1}}],[\"正确的累加方式\",{\"1\":{\"654\":1,\"656\":1}}],[\"正确\",{\"1\":{\"430\":1,\"508\":1}}],[\"正确识别阳性病例至关重要\",{\"1\":{\"344\":1}}],[\"正确分类的比例\",{\"1\":{\"343\":1}}],[\"正图负文\",{\"1\":{\"162\":2}}],[\"正图像\",{\"1\":{\"162\":1}}],[\"正文负图\",{\"1\":{\"162\":2}}],[\"正文本\",{\"1\":{\"162\":1}}],[\"正样本较少时增加权重\",{\"1\":{\"404\":1}}],[\"正样本batch=1\",{\"1\":{\"284\":1}}],[\"正样本batch\",{\"1\":{\"284\":3}}],[\"正样本图文对往往关联性较弱\",{\"1\":{\"157\":1}}],[\"正样本概率为\",{\"1\":{\"154\":1}}],[\"正样本对\",{\"1\":{\"147\":1}}],[\"正样本\",{\"1\":{\"145\":2,\"162\":2}}],[\"正样本编码\",{\"1\":{\"145\":2}}],[\"正交变换包括\",{\"1\":{\"117\":1}}],[\"正交变换的本质是\",{\"1\":{\"117\":1}}],[\"正交变换\",{\"0\":{\"117\":1}}],[\"正是对这一缺陷的改进\",{\"1\":{\"112\":1}}],[\"正类\",{\"1\":{\"78\":1}}],[\"正好为resnet18最后生成的特征图的分辨率\",{\"1\":{\"59\":1}}],[\"负数运算y\",{\"1\":{\"660\":1}}],[\"负数运算\",{\"1\":{\"660\":1}}],[\"负二项分布变为几何分布\",{\"1\":{\"582\":1}}],[\"负二项分布的意义与优势\",{\"0\":{\"582\":1}}],[\"负二项分布的两个矩\",{\"1\":{\"581\":1}}],[\"负二项分布关注成功次数固定\",{\"1\":{\"579\":1}}],[\"负二项分布\",{\"0\":{\"578\":1},\"1\":{\"579\":1}}],[\"负号\",{\"1\":{\"404\":1}}],[\"负相关\",{\"1\":{\"355\":1}}],[\"负图像\",{\"1\":{\"162\":1}}],[\"负文本\",{\"1\":{\"162\":1}}],[\"负样本batch2=0\",{\"1\":{\"284\":1}}],[\"负样本batch2\",{\"1\":{\"284\":3}}],[\"负样本batch1=0\",{\"1\":{\"284\":1}}],[\"负样本batch1\",{\"1\":{\"284\":3}}],[\"负样本数量\",{\"1\":{\"246\":1}}],[\"负样本x2\",{\"1\":{\"238\":1}}],[\"负样本概率为\",{\"1\":{\"154\":1}}],[\"负样本\",{\"1\":{\"145\":2,\"157\":1,\"162\":1,\"404\":1}}],[\"负类\",{\"1\":{\"78\":1}}],[\"负责组织多个\",{\"1\":{\"285\":1}}],[\"负责组织自注意力和交叉注意力的运算流程\",{\"1\":{\"285\":1}}],[\"负责组内信息混合\",{\"1\":{\"73\":2}}],[\"负责完成出队入队的信息记录\",{\"1\":{\"246\":1}}],[\"负责通道间信息混合\",{\"1\":{\"73\":2}}],[\"掩码机制\",{\"1\":{\"548\":1}}],[\"掩码符号\",{\"1\":{\"511\":1}}],[\"掩码候选位置\",{\"1\":{\"511\":1}}],[\"掩码语言建模\",{\"1\":{\"501\":1,\"513\":1}}],[\"掩码语言建模中的动量蒸馏\",{\"1\":{\"157\":1}}],[\"掩码语言模型\",{\"1\":{\"493\":1}}],[\"掩码等\",{\"1\":{\"389\":1}}],[\"掩码图像建模\",{\"0\":{\"172\":1},\"1\":{\"166\":1}}],[\"掩码高度匹配的功能区域\",{\"1\":{\"82\":1}}],[\"掩码之间的逐点偏差\",{\"1\":{\"82\":1}}],[\"掩码\",{\"1\":{\"78\":1,\"190\":1,\"200\":1,\"401\":1,\"403\":2,\"404\":1,\"495\":1}}],[\"或开源模型来实现核心的理解与生成\",{\"1\":{\"686\":1}}],[\"或记忆\",{\"1\":{\"674\":1}}],[\"或更多\",{\"1\":{\"673\":1}}],[\"或x\",{\"1\":{\"660\":1}}],[\"或返回notimplemented\",{\"1\":{\"660\":3}}],[\"或实现\",{\"1\":{\"659\":1}}],[\"或变量\",{\"1\":{\"656\":1}}],[\"或假设\",{\"1\":{\"596\":1}}],[\"或后验推理\",{\"1\":{\"596\":1}}],[\"或隐藏\",{\"1\":{\"596\":1}}],[\"或称\",{\"1\":{\"578\":1}}],[\"或太\",{\"1\":{\"566\":1}}],[\"或两个句子\",{\"1\":{\"542\":1}}],[\"或doc\",{\"1\":{\"497\":1}}],[\"或微调的前提下\",{\"1\":{\"462\":1}}],[\"或释义发现\",{\"1\":{\"448\":1}}],[\"或可以通过在前面添加维度来自动广播\",{\"1\":{\"387\":1}}],[\"或新形状无法与原内存布局兼容时\",{\"1\":{\"385\":1}}],[\"或某些要求内存连续的操作\",{\"1\":{\"383\":1}}],[\"或类\",{\"1\":{\"368\":1}}],[\"或准确率\",{\"1\":{\"347\":1}}],[\"或模型的误报率\",{\"1\":{\"345\":1}}],[\"或源码\",{\"1\":{\"338\":1}}],[\"或数组\",{\"1\":{\"327\":1}}],[\"或矩阵\",{\"1\":{\"325\":1}}],[\"或者私有化模型\",{\"1\":{\"682\":1}}],[\"或者直接命令行运行\",{\"1\":{\"519\":1}}],[\"或者更广义地\",{\"1\":{\"407\":1}}],[\"或者更细一点\",{\"1\":{\"238\":1}}],[\"或者跨设备\",{\"1\":{\"385\":1}}],[\"或者你不在意尺度差异\",{\"1\":{\"360\":1}}],[\"或者在达到某个最低准确性水平的情况下\",{\"1\":{\"347\":1}}],[\"或者一种错误\",{\"1\":{\"343\":1}}],[\"或者使用\",{\"1\":{\"326\":1}}],[\"或者说在于计算能力和数据集的规模\",{\"1\":{\"278\":1}}],[\"或者这个任务需要更多的视觉信息\",{\"1\":{\"259\":1}}],[\"或者\",{\"1\":{\"240\":1,\"285\":1,\"334\":1,\"410\":1,\"576\":1}}],[\"或者其他的数据增广操作\",{\"1\":{\"235\":1}}],[\"或前缀\",{\"1\":{\"231\":1}}],[\"或视觉语言模型性能的技术\",{\"1\":{\"231\":1}}],[\"或多任务场景下的表现\",{\"1\":{\"224\":1}}],[\"或全局池化特征\",{\"1\":{\"188\":1}}],[\"或与bert系列模型对齐\",{\"1\":{\"183\":1}}],[\"或图结构增强局部建模能力\",{\"1\":{\"112\":1}}],[\"或图像视图\",{\"1\":{\"103\":1}}],[\"或转置卷积\",{\"1\":{\"100\":1}}],[\"或每组邻域大小\",{\"1\":{\"95\":1}}],[\"或局部区域\",{\"1\":{\"86\":1}}],[\"或\",{\"1\":{\"78\":1,\"82\":1,\"83\":1,\"86\":1,\"98\":1,\"132\":1,\"163\":1,\"226\":1,\"247\":1,\"262\":1,\"323\":1,\"325\":1,\"331\":1,\"338\":1,\"382\":1,\"389\":1,\"397\":1,\"401\":1,\"402\":2,\"405\":1,\"407\":2,\"565\":2,\"567\":1,\"568\":1,\"587\":1,\"588\":1,\"675\":1}}],[\"或经过\",{\"1\":{\"78\":1,\"403\":1,\"404\":1}}],[\"或平均池化\",{\"1\":{\"76\":1}}],[\"强推理能力\",{\"1\":{\"674\":1}}],[\"强记忆型的语言预测器\",{\"1\":{\"463\":1}}],[\"强视觉编码器\",{\"1\":{\"217\":1}}],[\"强视觉表征\",{\"1\":{\"208\":1}}],[\"强大的神经网络模型\",{\"1\":{\"673\":1}}],[\"强大的视觉编码器\",{\"1\":{\"207\":1,\"208\":1}}],[\"强大性能\",{\"1\":{\"181\":1}}],[\"强制将每个样本开头的\",{\"1\":{\"143\":1}}],[\"强制学习正交变换矩阵\",{\"1\":{\"112\":1}}],[\"强度\",{\"1\":{\"114\":1}}],[\"强调责任约束\",{\"1\":{\"483\":1}}],[\"强调未来开发和部署语言模型时需格外关注其社会影响及安全性\",{\"1\":{\"468\":1}}],[\"强调它们在无需微调的情况下就能取得良好效果\",{\"1\":{\"461\":1}}],[\"强调预测与\",{\"1\":{\"402\":1,\"403\":1}}],[\"强调分布匹配\",{\"1\":{\"82\":1}}],[\"强调正类\",{\"1\":{\"78\":1}}],[\"强化复杂任务处理能力\",{\"1\":{\"674\":1}}],[\"强化学习\",{\"1\":{\"470\":1}}],[\"强化学习来自人类反馈\",{\"1\":{\"469\":1}}],[\"强化学习微调\",{\"1\":{\"468\":1}}],[\"强化学习方法\",{\"1\":{\"49\":1,\"51\":1}}],[\"强化了模型对长上下文的理解和生成\",{\"1\":{\"679\":1}}],[\"强化了模型的代码能力\",{\"1\":{\"674\":1}}],[\"强化了\",{\"1\":{\"464\":1}}],[\"强化多义性和类间相似度学习\",{\"1\":{\"159\":1}}],[\"强化人类意图在物体语义推理中的引导作用\",{\"1\":{\"32\":1}}],[\"强化\",{\"1\":{\"32\":1}}],[\"参与数据标注\",{\"1\":{\"470\":1}}],[\"参数便在部分达到了\",{\"1\":{\"674\":1}}],[\"参数模型是首个公开的千亿级开源模型\",{\"1\":{\"674\":1}}],[\"参数的\",{\"1\":{\"673\":2}}],[\"参数的语言模型\",{\"1\":{\"673\":1}}],[\"参数的变化趋势\",{\"1\":{\"159\":1}}],[\"参数管理与清理机制\",{\"1\":{\"670\":1}}],[\"参数调优\",{\"1\":{\"464\":1}}],[\"参数调整正类和负类的权重\",{\"1\":{\"407\":1}}],[\"参数从117m到1\",{\"1\":{\"454\":1}}],[\"参数足够多\",{\"1\":{\"434\":1}}],[\"参数选择\",{\"1\":{\"404\":1}}],[\"参数用于平衡正负类别的权重\",{\"1\":{\"404\":1}}],[\"参数用于抑制容易分类的样本\",{\"1\":{\"404\":1}}],[\"参数复用与模块化\",{\"1\":{\"395\":1}}],[\"参数个数必须和\",{\"1\":{\"386\":1,\"387\":1}}],[\"参数与训练集的归一化参数相同\",{\"1\":{\"290\":1}}],[\"参数效率高\",{\"1\":{\"231\":1}}],[\"参数规模庞大\",{\"1\":{\"675\":1}}],[\"参数规模从7b到65b不等\",{\"1\":{\"479\":1}}],[\"参数规模\",{\"1\":{\"208\":1}}],[\"参数规模不匹配\",{\"1\":{\"181\":1}}],[\"参数量远超\",{\"1\":{\"454\":1}}],[\"参数量降低到\",{\"1\":{\"423\":1}}],[\"参数量更少\",{\"1\":{\"395\":1}}],[\"参数量极大\",{\"1\":{\"395\":1}}],[\"参数量\",{\"1\":{\"191\":1}}],[\"参数量是qformer的42倍\",{\"1\":{\"189\":1}}],[\"参数量达到60亿\",{\"1\":{\"189\":1}}],[\"参数平衡的视觉与语言组件\",{\"1\":{\"181\":1}}],[\"参数更新\",{\"1\":{\"160\":1}}],[\"参数更新等\",{\"1\":{\"81\":1}}],[\"参数共享\",{\"1\":{\"112\":1}}],[\"参数依赖\",{\"1\":{\"89\":1}}],[\"参数\",{\"1\":{\"78\":1,\"83\":1,\"92\":1,\"100\":2,\"101\":1,\"231\":1,\"379\":1,\"401\":2,\"402\":2,\"403\":2,\"404\":3,\"405\":2,\"407\":3,\"513\":1,\"584\":2,\"674\":1}}],[\"参考decoderlayer\",{\"1\":{\"552\":1}}],[\"参考上文\",{\"1\":{\"483\":1}}],[\"参考上文的\",{\"1\":{\"68\":1}}],[\"参考rabe\",{\"1\":{\"481\":1}}],[\"参考群体\",{\"1\":{\"472\":1}}],[\"参考点解码器\",{\"0\":{\"76\":1},\"1\":{\"70\":1}}],[\"参考\",{\"0\":{\"688\":1},\"1\":{\"22\":1,\"131\":1,\"494\":3}}],[\"让开发者可以专注于应用程序的开发\",{\"1\":{\"685\":1}}],[\"让开发者能够通过简单的命令来管理整个应用程序的生命周期\",{\"1\":{\"685\":1}}],[\"让我们的应用能够上线成为产品\",{\"1\":{\"687\":1}}],[\"让我们更轻松地找到所需的信息\",{\"1\":{\"678\":1}}],[\"让我们逐步构建起tinypytorch的基础功能\",{\"1\":{\"604\":1}}],[\"让大语言模型真正火爆的契机\",{\"1\":{\"678\":1}}],[\"让大模型生成的结果\",{\"1\":{\"416\":1}}],[\"让用户可根据具体需求选择最适合的模型类型\",{\"1\":{\"674\":1}}],[\"让返回值始终封装为元组\",{\"1\":{\"651\":1}}],[\"让函数更易用\",{\"0\":{\"640\":1}}],[\"让其\",{\"1\":{\"508\":1}}],[\"让llm照着例子进行推理\",{\"1\":{\"433\":1}}],[\"让他去执行就好了\",{\"1\":{\"430\":1}}],[\"让难分类样本获得更大的\",{\"1\":{\"404\":1}}],[\"让你显式指定维度之间怎么相乘\",{\"1\":{\"390\":1}}],[\"让被装饰函数看起来仍然像原来的函数\",{\"1\":{\"372\":1}}],[\"让q\",{\"1\":{\"285\":1}}],[\"让query在字典中与自己匹配的正样本更近\",{\"1\":{\"238\":1}}],[\"让文本动态关注图像区域\",{\"1\":{\"267\":1}}],[\"让文本\",{\"1\":{\"264\":1}}],[\"让\",{\"1\":{\"238\":1}}],[\"让正样本的key和query距离近\",{\"1\":{\"238\":1}}],[\"让固定模型适应新任务\",{\"1\":{\"231\":1}}],[\"让视觉编码器提取的图像特征与语言模型的词嵌入空间对齐\",{\"1\":{\"225\":1}}],[\"让人类标注者对这些回答进行排序\",{\"1\":{\"224\":1}}],[\"让变换矩阵从一个恒等变换开始学习\",{\"1\":{\"107\":1}}],[\"让网络从一个小扰动开始学习\",{\"1\":{\"107\":1}}],[\"让模型可以更好地捕捉和理解语言中的复杂关系\",{\"1\":{\"673\":1}}],[\"让模型自己学会区分不同的句子\",{\"1\":{\"523\":1}}],[\"让模型得以判断上下句的起止位置\",{\"1\":{\"506\":1}}],[\"让模型理解图像和文本之间的语义关系\",{\"1\":{\"225\":1}}],[\"让模型生成多个不同的回答\",{\"1\":{\"224\":1}}],[\"让模型更容易训练和泛化\",{\"1\":{\"107\":1}}],[\"让模型既关注局部细节\",{\"1\":{\"105\":1}}],[\"让模型同时关注逐点分类精度和整体区域匹配\",{\"1\":{\"78\":1}}],[\"让每个点都能看到上下文信息\",{\"1\":{\"109\":1,\"111\":1}}],[\"让每个\",{\"1\":{\"76\":1}}],[\"让每个语言\",{\"1\":{\"72\":1}}],[\"得益于前人的一些关于内在维度\",{\"1\":{\"424\":1}}],[\"得出带有一定置信度的一般性结论的行为\",{\"1\":{\"596\":1}}],[\"得出的用来将x序列\",{\"1\":{\"414\":1}}],[\"得出\",{\"1\":{\"115\":1}}],[\"得分也会接近它们的值\",{\"1\":{\"348\":1}}],[\"得分也会为\",{\"1\":{\"348\":1}}],[\"得分是精确率和召回率的调和平均数\",{\"1\":{\"348\":1}}],[\"得分从\",{\"1\":{\"196\":1}}],[\"得分达\",{\"1\":{\"195\":1}}],[\"得分达到\",{\"1\":{\"194\":1}}],[\"得分\",{\"0\":{\"348\":1},\"1\":{\"76\":1}}],[\"得到概率\",{\"1\":{\"540\":1}}],[\"得到模型预测的这些掩码token对应的真实词\",{\"1\":{\"513\":1}}],[\"得到对应的json格式文件\",{\"1\":{\"510\":1}}],[\"得到上图中黄色的输出\",{\"1\":{\"508\":1}}],[\"得到llama\",{\"1\":{\"483\":1}}],[\"得到初始策略模型\",{\"1\":{\"470\":1}}],[\"得到最终适用于下游任务的模型参数\",{\"1\":{\"423\":1}}],[\"得到最终的掩码\",{\"1\":{\"76\":1}}],[\"得到一组f\",{\"1\":{\"405\":1}}],[\"得到一个新的形状为\",{\"1\":{\"326\":1}}],[\"得到一个融合了上下文信息的向量\",{\"1\":{\"312\":1}}],[\"得到一个logit\",{\"1\":{\"284\":1}}],[\"得到一个全局语义向量\",{\"1\":{\"46\":1}}],[\"得到归一化的注意力权重\",{\"1\":{\"308\":1}}],[\"得到归一化因子\",{\"1\":{\"100\":1}}],[\"得到结果如下图所示\",{\"1\":{\"298\":1}}],[\"得到注意力权重矩阵\",{\"1\":{\"295\":1}}],[\"得到注意力分数矩阵\",{\"1\":{\"295\":1}}],[\"得到形状为\",{\"1\":{\"291\":3}}],[\"得到图像和对应的标签\",{\"1\":{\"296\":1}}],[\"得到图像的表征\",{\"1\":{\"286\":1}}],[\"得到图片表征f11\",{\"1\":{\"238\":1}}],[\"得到相似度矩阵\",{\"1\":{\"283\":2}}],[\"得到相同维度的特征\",{\"1\":{\"272\":1}}],[\"得到两个样本\",{\"1\":{\"244\":1}}],[\"得到表征f12\",{\"1\":{\"238\":1}}],[\"得到三个表征\",{\"1\":{\"234\":1}}],[\"得到预测结果\",{\"1\":{\"296\":1}}],[\"得到预测概率\",{\"1\":{\"156\":1}}],[\"得到预测的\",{\"1\":{\"82\":1}}],[\"得到该区域的特征\",{\"1\":{\"98\":1}}],[\"得到该区域的固定长度特征表示\",{\"1\":{\"92\":1}}],[\"得到\",{\"1\":{\"93\":3,\"111\":1,\"286\":1}}],[\"得到的导数为\",{\"1\":{\"667\":1}}],[\"得到的就是答案\",{\"1\":{\"542\":1}}],[\"得到的三个答案中\",{\"1\":{\"435\":1}}],[\"得到的预测概率如下所示\",{\"1\":{\"273\":1}}],[\"得到的特征可以很好地迁移到下游任务中\",{\"1\":{\"238\":1}}],[\"得到的特征可能无法覆盖整个物体\",{\"1\":{\"112\":1}}],[\"得到的\",{\"1\":{\"145\":1,\"596\":1}}],[\"得到的质心\",{\"1\":{\"92\":1}}],[\"得到的几何属性描述与\",{\"1\":{\"13\":1}}],[\"得到每个注意力头的输出\",{\"1\":{\"295\":1}}],[\"得到每个预测类别的概率值\",{\"1\":{\"273\":1}}],[\"得到每个原始点的插值特征\",{\"1\":{\"100\":1}}],[\"得到每个\",{\"1\":{\"76\":1,\"163\":1}}],[\"得到分割掩码\",{\"1\":{\"76\":1}}],[\"得到目标物体区域掩码\",{\"1\":{\"59\":1}}],[\"得到目标物体区域特征\",{\"1\":{\"59\":2}}],[\"得到融合特征\",{\"1\":{\"8\":1}}],[\"去推测其真正的意图\",{\"1\":{\"597\":1}}],[\"去推断世界的状态\",{\"1\":{\"597\":1}}],[\"去重并移除了\",{\"1\":{\"454\":1}}],[\"去做任务适配\",{\"1\":{\"424\":1}}],[\"去缓慢地更新编码器\",{\"1\":{\"236\":1}}],[\"去掉字符和空格\",{\"1\":{\"447\":1}}],[\"去掉\",{\"1\":{\"143\":1}}],[\"去除含有低频词的句对\",{\"1\":{\"511\":1}}],[\"去除前后空格\",{\"1\":{\"510\":1}}],[\"去除后性能反而提升\",{\"1\":{\"500\":1}}],[\"去除nsp损失不仅未降低性能\",{\"1\":{\"495\":1}}],[\"去除nsp损失\",{\"1\":{\"495\":1}}],[\"去除这个温度超参数\",{\"1\":{\"240\":1}}],[\"去除频率小于\",{\"1\":{\"226\":1}}],[\"去除低质量描述\",{\"1\":{\"190\":1}}],[\"去除噪声文本\",{\"1\":{\"132\":1}}],[\"去除batch维度\",{\"1\":{\"83\":1}}],[\"去填充这些空缺\",{\"1\":{\"92\":1}}],[\"去卷积点云特征\",{\"1\":{\"76\":1}}],[\"去关注点云中最相关的区域\",{\"1\":{\"72\":1}}],[\"区分大语言模型\",{\"1\":{\"676\":1}}],[\"区分句子\",{\"1\":{\"513\":1}}],[\"区间事件\",{\"1\":{\"566\":2}}],[\"区间数值的映射\",{\"1\":{\"564\":1}}],[\"区间内\",{\"1\":{\"407\":1}}],[\"区间\",{\"1\":{\"76\":1,\"401\":1,\"403\":1,\"566\":2}}],[\"区域匹配度\",{\"1\":{\"407\":1}}],[\"区域匹配误差\",{\"1\":{\"78\":1}}],[\"区域重叠误差\",{\"1\":{\"407\":1}}],[\"区域重合度\",{\"1\":{\"82\":1}}],[\"区域划分为\",{\"1\":{\"396\":1}}],[\"区域中提取固定大小的特征\",{\"1\":{\"396\":1}}],[\"区域特征\",{\"1\":{\"253\":1}}],[\"区域定位\",{\"1\":{\"54\":1}}],[\"区域\",{\"1\":{\"6\":1,\"7\":1,\"25\":2}}],[\"扫描角度不同等\",{\"1\":{\"107\":1}}],[\"扫描\",{\"1\":{\"76\":1}}],[\"动态图控制与框架灵活性展开一系列扩展与优化\",{\"1\":{\"665\":1}}],[\"动态图可视化与高阶导数构建\",{\"0\":{\"664\":1}}],[\"动态计算图\",{\"1\":{\"662\":1}}],[\"动态掩码与大批量训练\",{\"1\":{\"500\":1}}],[\"动态掩码在squad\",{\"1\":{\"497\":1}}],[\"动态掩码能持续提供新的掩码模式\",{\"1\":{\"495\":1}}],[\"动态掩码性能略优于静态掩码\",{\"1\":{\"495\":1}}],[\"动态掩码\",{\"1\":{\"492\":1,\"495\":1,\"497\":1}}],[\"动态调整样本权重\",{\"1\":{\"404\":1}}],[\"动态增加其功能\",{\"1\":{\"368\":1}}],[\"动态高分辨率策略\",{\"1\":{\"219\":1}}],[\"动态高分辨率处理\",{\"1\":{\"207\":1,\"208\":1}}],[\"动态宽高比匹配\",{\"1\":{\"216\":1}}],[\"动态分辨率的作用\",{\"1\":{\"221\":1}}],[\"动态分辨率策略\",{\"1\":{\"214\":1,\"217\":1}}],[\"动态分辨率支持和多语言优化\",{\"1\":{\"210\":1}}],[\"动态卷积核\",{\"1\":{\"76\":1}}],[\"动态卷积\",{\"1\":{\"76\":2}}],[\"动量更新参数\",{\"1\":{\"246\":1}}],[\"动量更新系数\",{\"1\":{\"160\":1}}],[\"动量设置为了\",{\"1\":{\"241\":1}}],[\"动量这个超参数是\",{\"1\":{\"236\":1}}],[\"动量是一种加权移动平均\",{\"1\":{\"236\":1}}],[\"动量视觉特征\",{\"1\":{\"163\":1}}],[\"动量视觉编码器\",{\"1\":{\"147\":1}}],[\"动量图像编码器输出特征\",{\"1\":{\"161\":1}}],[\"动量分支的\",{\"1\":{\"159\":1}}],[\"动量蒸馏的总体损失是对原始监督信号与伪监督信号的加权组合\",{\"1\":{\"157\":1}}],[\"动量模型生成的伪标签往往比真实标签更具多样性和语义丰富性\",{\"1\":{\"157\":1}}],[\"动量模型是对主模型参数的滑动平均版本\",{\"1\":{\"157\":1}}],[\"动量模型\",{\"1\":{\"157\":1}}],[\"动量文本编码器输出特征\",{\"1\":{\"161\":1}}],[\"动量文本编码器\",{\"1\":{\"147\":1}}],[\"动量编码器生成的归一化特征分别记为\",{\"1\":{\"154\":1}}],[\"动量编码器\",{\"1\":{\"147\":1}}],[\"动量编码器的更新参数\",{\"1\":{\"147\":1}}],[\"动量慢更新\",{\"1\":{\"145\":1}}],[\"动作可能性\",{\"1\":{\"6\":1,\"7\":1}}],[\"猜出我的特征\",{\"1\":{\"100\":1}}],[\"猜\",{\"1\":{\"76\":1}}],[\"显存并训练模型\",{\"1\":{\"428\":1}}],[\"显然\",{\"1\":{\"273\":1}}],[\"显示增大批次规模可提升模型困惑度和下游任务准确率\",{\"1\":{\"492\":1}}],[\"显示动态掩码在\",{\"1\":{\"492\":1}}],[\"显示模型在强化学习过程中存在轻微性能损失\",{\"1\":{\"470\":1}}],[\"显示了\",{\"1\":{\"470\":1}}],[\"显示出这种人类反馈驱动的微调策略极具潜力\",{\"1\":{\"468\":1}}],[\"显示出数据集对交互多样性和类别多样性的全面覆盖\",{\"1\":{\"17\":1}}],[\"显示其对\",{\"1\":{\"462\":1}}],[\"显示gpt\",{\"1\":{\"462\":1}}],[\"显示在一些语义比较任务上仍存在明显短板\",{\"1\":{\"462\":1}}],[\"显示无监督学习的当前边界\",{\"1\":{\"455\":1}}],[\"显示\",{\"1\":{\"134\":1,\"137\":1,\"470\":1,\"471\":4}}],[\"显著高于bert\",{\"1\":{\"499\":1}}],[\"显著高于palm\",{\"1\":{\"482\":1}}],[\"显著比\",{\"1\":{\"471\":1}}],[\"显著减少数据需求\",{\"1\":{\"461\":1}}],[\"显著抑制简单样本\",{\"1\":{\"404\":1}}],[\"显著超越现有方法\",{\"1\":{\"194\":1}}],[\"显著优于\",{\"1\":{\"193\":1}}],[\"显著降低计算复杂度\",{\"1\":{\"178\":1}}],[\"显著提升难样本的分类性能\",{\"1\":{\"404\":1}}],[\"显著提升表现\",{\"1\":{\"228\":1}}],[\"显著提升性能\",{\"1\":{\"221\":1}}],[\"显著提升ocr和中文任务性能\",{\"1\":{\"208\":1}}],[\"显著提升模型性能\",{\"1\":{\"132\":1}}],[\"显著提升了性能\",{\"1\":{\"496\":1}}],[\"显著提升了模型性能\",{\"1\":{\"495\":1}}],[\"显著提升了模型在\",{\"1\":{\"207\":1}}],[\"显著提升了\",{\"1\":{\"493\":1}}],[\"显著提升了零样本和少样本泛化能力\",{\"1\":{\"224\":1}}],[\"显著提升了开源模型在ocr\",{\"1\":{\"217\":1}}],[\"显著提升了视觉\",{\"1\":{\"197\":1}}],[\"显著提升了视觉任务的性能\",{\"1\":{\"183\":1}}],[\"显著提升了生成图像的保真度\",{\"1\":{\"178\":1}}],[\"显著提升了分割性能\",{\"1\":{\"105\":1}}],[\"显著提升了所有指标\",{\"1\":{\"75\":1}}],[\"显著扩展了训练数据的规模和质量\",{\"1\":{\"129\":1}}],[\"后更名为\",{\"1\":{\"674\":1}}],[\"后训练和在线推理阶段也各自拥有了\",{\"1\":{\"673\":1}}],[\"后训练和在线推理\",{\"1\":{\"673\":1}}],[\"后验\",{\"1\":{\"596\":1}}],[\"后对\",{\"1\":{\"596\":1}}],[\"后得到的注意力得分矩阵维度相同\",{\"1\":{\"517\":1}}],[\"后处理技术\",{\"1\":{\"484\":1}}],[\"后的结果\",{\"1\":{\"585\":1}}],[\"后的形式\",{\"1\":{\"540\":1}}],[\"后的改进\",{\"1\":{\"477\":1}}],[\"后的概率值\",{\"1\":{\"401\":1}}],[\"后来被应用于语言任务\",{\"1\":{\"469\":1}}],[\"后面再赋值\",{\"1\":{\"389\":1}}],[\"后面的mlp是个单独的结构\",{\"1\":{\"294\":1}}],[\"后将投影后的\",{\"1\":{\"286\":1}}],[\"后三种模型是按照efficientnet的缩放规则对resnet分别放大4倍\",{\"1\":{\"272\":1}}],[\"后应用一个\",{\"1\":{\"265\":1}}],[\"后缀\",{\"1\":{\"231\":1}}],[\"后分割为448×448区块\",{\"1\":{\"216\":1}}],[\"后者是其110倍\",{\"1\":{\"510\":1}}],[\"后者效果略优但计算复杂\",{\"1\":{\"497\":1}}],[\"后者作为强大的\",{\"1\":{\"181\":1}}],[\"后者为因果\",{\"1\":{\"126\":1}}],[\"后期切换至\",{\"1\":{\"190\":1,\"200\":1}}],[\"后期\",{\"1\":{\"159\":1}}],[\"后续计算逻辑\",{\"1\":{\"660\":1}}],[\"后续需要利用该相关度完成当前词的全局上下文信息融合\",{\"1\":{\"517\":1}}],[\"后续实验均采用动态掩码\",{\"1\":{\"495\":1}}],[\"后续每次生成新\",{\"1\":{\"477\":1}}],[\"后续推理\",{\"1\":{\"477\":1}}],[\"后续方法\",{\"1\":{\"464\":1}}],[\"后续再进行裁剪操作\",{\"1\":{\"290\":1}}],[\"后续更新时对右边这个编码器的参数进行动量更新\",{\"1\":{\"238\":1}}],[\"后续研究通过多尺度生成器\",{\"1\":{\"178\":1}}],[\"后续\",{\"1\":{\"147\":1}}],[\"后续的损失函数之类的训练就很常规了\",{\"1\":{\"235\":1}}],[\"后续的\",{\"1\":{\"112\":1}}],[\"后续的特征提取更稳定\",{\"1\":{\"107\":1}}],[\"后续发展\",{\"1\":{\"112\":1}}],[\"后续改进方向\",{\"1\":{\"112\":1}}],[\"后续改进\",{\"1\":{\"112\":1}}],[\"后续训练完50个epoch后\",{\"1\":{\"83\":1}}],[\"后台运行\",{\"1\":{\"83\":1}}],[\"后\",{\"1\":{\"75\":1,\"107\":1,\"162\":1,\"355\":1,\"383\":1}}],[\"↓\",{\"1\":{\"75\":4,\"98\":5,\"99\":8,\"319\":9}}],[\"──┐\",{\"1\":{\"75\":1}}],[\"两条互补产品线\",{\"1\":{\"674\":1}}],[\"两大技术分支\",{\"1\":{\"674\":1}}],[\"两两配对遍历\",{\"1\":{\"511\":1}}],[\"两点注意\",{\"1\":{\"289\":1}}],[\"两点的距离反映了这两点的实际相似度或关联度\",{\"1\":{\"90\":1}}],[\"两阶段训练\",{\"1\":{\"219\":1}}],[\"两种规模的\",{\"1\":{\"674\":1}}],[\"两种模态是否相互作用\",{\"1\":{\"255\":1}}],[\"两种模态是否保持平衡\",{\"1\":{\"255\":1}}],[\"两种模式\",{\"1\":{\"65\":1}}],[\"两种配置\",{\"1\":{\"190\":1}}],[\"两组负样本编码\",{\"1\":{\"145\":1}}],[\"两者之间会出现一种\",{\"1\":{\"592\":1}}],[\"两者毒性差异减小\",{\"1\":{\"471\":1}}],[\"两者协同提升模型性能\",{\"1\":{\"404\":1}}],[\"两者解决的是不同维度的问题\",{\"1\":{\"404\":1}}],[\"两者解耦可以让模型更灵活地分配资源\",{\"1\":{\"317\":1}}],[\"两者互补\",{\"1\":{\"402\":1}}],[\"两者都通过增加基函数的数量\",{\"1\":{\"395\":1}}],[\"两者的训练效率相差3倍\",{\"1\":{\"278\":1}}],[\"两者联合使用时效果最佳\",{\"1\":{\"132\":1}}],[\"两者结合可以\",{\"1\":{\"78\":1}}],[\"两个月后月活用户破亿\",{\"1\":{\"674\":1}}],[\"两个句子是否为上下句关系\",{\"1\":{\"537\":1}}],[\"两个任务\",{\"1\":{\"513\":1}}],[\"两个开源数据集\",{\"1\":{\"510\":1}}],[\"两个不同分类任务的评估结果\",{\"1\":{\"448\":1}}],[\"两个比较的句子没有内在顺序\",{\"1\":{\"445\":1}}],[\"两个假设模型的\",{\"1\":{\"353\":1}}],[\"两个贡献\",{\"1\":{\"241\":1}}],[\"两个编码器\",{\"1\":{\"238\":1}}],[\"两个点之间的直线距离被认为是相似度或连接强度的直观表示\",{\"1\":{\"90\":1}}],[\"两个问题是相关联的\",{\"1\":{\"86\":1}}],[\"两个\",{\"1\":{\"73\":1}}],[\"相较于传统的语言模型具有更强大的能力\",{\"1\":{\"679\":1}}],[\"相信未来会涌现出很多类似的以对话形态让助手完成各种具体工作的项目\",{\"1\":{\"678\":1}}],[\"相乘\",{\"1\":{\"596\":1}}],[\"相互抵消的平衡点\",{\"1\":{\"592\":1}}],[\"相对标准差\",{\"1\":{\"593\":1}}],[\"相对于均值\",{\"1\":{\"590\":1}}],[\"相对来说成本会比较高\",{\"1\":{\"416\":1}}],[\"相加\",{\"1\":{\"507\":1}}],[\"相结合\",{\"1\":{\"407\":1}}],[\"相媲美的效果\",{\"1\":{\"301\":1}}],[\"相同的维度字母表示要做\",{\"1\":{\"390\":1}}],[\"相同\",{\"1\":{\"224\":1,\"387\":1,\"405\":1,\"407\":1}}],[\"相当\",{\"1\":{\"207\":1,\"454\":1}}],[\"相当于为计算机提供了强大的\",{\"1\":{\"673\":1}}],[\"相当于对输入空间进行一次\",{\"1\":{\"395\":1}}],[\"相当于\",{\"1\":{\"369\":1}}],[\"相当于一个\",{\"1\":{\"359\":1}}],[\"相当于直接输入一篇五万字的文章\",{\"1\":{\"291\":1}}],[\"相当于数据量不够\",{\"1\":{\"238\":1}}],[\"相当于告诉模型\",{\"1\":{\"111\":1}}],[\"相当于加了一个\",{\"1\":{\"107\":1}}],[\"相当于跨空间位置的信息交换\",{\"1\":{\"73\":1}}],[\"相关核心代码实现如下\",{\"1\":{\"510\":1}}],[\"相关代码\",{\"1\":{\"502\":1}}],[\"相关模型和代码已公开供进一步研究\",{\"1\":{\"491\":1}}],[\"相关阅读资料\",{\"1\":{\"421\":1}}],[\"相关任务中表现突出\",{\"1\":{\"207\":1}}],[\"相关工作\",{\"0\":{\"7\":1,\"50\":1,\"182\":1,\"209\":1,\"441\":1,\"464\":1,\"469\":1,\"485\":1,\"501\":1}}],[\"相似度指标\",{\"1\":{\"405\":1}}],[\"相似度如下所示\",{\"1\":{\"273\":1}}],[\"相似度生成的\",{\"1\":{\"162\":1}}],[\"相似度\",{\"1\":{\"147\":1,\"154\":1,\"240\":1,\"445\":1}}],[\"相似性得分\",{\"1\":{\"82\":1}}],[\"相等\",{\"1\":{\"145\":1,\"387\":1}}],[\"相比原始\",{\"1\":{\"494\":1}}],[\"相比前代模型大幅提升\",{\"1\":{\"463\":1}}],[\"相比更低\",{\"1\":{\"455\":1}}],[\"相比其他替代方案如rnn\",{\"1\":{\"440\":1}}],[\"相比w\",{\"1\":{\"416\":1}}],[\"相比\",{\"1\":{\"385\":1,\"390\":1,\"471\":1,\"674\":2}}],[\"相比传统轻量级\",{\"1\":{\"189\":1}}],[\"相比qformer等轻量适配器\",{\"1\":{\"188\":1}}],[\"相比于每次开发单个模型的方式\",{\"1\":{\"677\":1}}],[\"相比于著名的\",{\"1\":{\"510\":1}}],[\"相比于grid\",{\"1\":{\"259\":1}}],[\"相比于region\",{\"1\":{\"259\":1}}],[\"相比于传统的\",{\"1\":{\"127\":1}}],[\"相比于随机采样\",{\"1\":{\"89\":1}}],[\"相比使用预训练目标检测器\",{\"1\":{\"126\":1}}],[\"相比之下\",{\"1\":{\"112\":1,\"278\":1,\"460\":1}}],[\"相反\",{\"1\":{\"90\":1,\"97\":1,\"325\":1,\"445\":1}}],[\"操作将视觉token数量减少至1\",{\"1\":{\"214\":1}}],[\"操作\",{\"1\":{\"73\":1,\"76\":2,\"98\":1,\"163\":1,\"327\":1,\"395\":1,\"397\":1}}],[\"因批次更大\",{\"1\":{\"498\":1}}],[\"因文章复用段落\",{\"1\":{\"455\":1}}],[\"因基函数\",{\"1\":{\"395\":1}}],[\"因数据量扩大\",{\"1\":{\"202\":1}}],[\"因果注意力\",{\"1\":{\"134\":1}}],[\"因其在编码\",{\"1\":{\"134\":1}}],[\"因此rosenbrock函数也被称为\",{\"1\":{\"667\":1}}],[\"因此反向传播时需将上游梯度gy分别乘以1和\",{\"1\":{\"660\":1}}],[\"因此需调用x\",{\"1\":{\"660\":1}}],[\"因此需要大家自行完成运行时缺失依赖包的安装\",{\"1\":{\"519\":1}}],[\"因此需要探索更高效的学习范式\",{\"1\":{\"453\":1}}],[\"因此需要使用像素级别的损失函数\",{\"1\":{\"399\":1}}],[\"因此需要更多得视觉部分\",{\"1\":{\"259\":1}}],[\"因此对于存在多分支的复杂计算图而言\",{\"1\":{\"655\":1}}],[\"因此func列表同一时刻最多只存在一个func\",{\"1\":{\"638\":1}}],[\"因此出现了extended\",{\"1\":{\"547\":1}}],[\"因此不会影响最终的损失值计算\",{\"1\":{\"514\":1}}],[\"因此不论张量是多少维的\",{\"1\":{\"321\":1}}],[\"因此为了确保masked\",{\"1\":{\"514\":1}}],[\"因此模型返回的logits\",{\"1\":{\"514\":1}}],[\"因此我们扩展\",{\"1\":{\"651\":1}}],[\"因此我们只需要根据is\",{\"1\":{\"514\":1}}],[\"因此我们可以往队列里放入很多负样本\",{\"1\":{\"237\":1}}],[\"因此此时的预测搞不好是对的\",{\"1\":{\"508\":1}}],[\"因此首轮推理过程需要完成\",{\"1\":{\"477\":1}}],[\"因此第\",{\"1\":{\"474\":1}}],[\"因此无法像微调方法那样从结构化监督中持续优化\",{\"1\":{\"463\":1}}],[\"因此论文探索如何让语言模型具备类似的少样本学习能力\",{\"1\":{\"460\":1}}],[\"因此sst属于单个句子的文本分类任务\",{\"1\":{\"448\":1}}],[\"因此cola属于单个句子的文本二分类任务\",{\"1\":{\"448\":1}}],[\"因此后面\",{\"1\":{\"426\":1}}],[\"因此语义分割可以提供更详细和准确的图像分析结果\",{\"1\":{\"399\":1}}],[\"因此使用四邻域双线性插值从特征图中获取精确的\",{\"1\":{\"397\":1}}],[\"因此精确率为\",{\"1\":{\"346\":1}}],[\"因此其假正例率为\",{\"1\":{\"345\":1}}],[\"因此其召回率\",{\"1\":{\"344\":1}}],[\"因此会出现在分母中\",{\"1\":{\"344\":1,\"345\":1}}],[\"因此准确率为\",{\"1\":{\"343\":1}}],[\"因此步长为\",{\"1\":{\"326\":1}}],[\"因此是自注意力\",{\"1\":{\"295\":1}}],[\"因此可以充分利用计算资源\",{\"1\":{\"547\":1}}],[\"因此可以堆叠多个block\",{\"1\":{\"294\":1}}],[\"因此可能比第二个向量更不可靠\",{\"1\":{\"97\":1}}],[\"因此它有效地充当信息瓶颈\",{\"1\":{\"286\":1}}],[\"因此成本较高\",{\"1\":{\"278\":1}}],[\"因此这里就不再给出数据集下载链接了\",{\"1\":{\"275\":1}}],[\"因此这里构造\",{\"1\":{\"145\":1}}],[\"因此这是一个非常庞大的数据集\",{\"1\":{\"272\":1}}],[\"因此直接移除了最后三层的权重\",{\"1\":{\"215\":1}}],[\"因此在类别数量相近且平衡的数据集的情况下\",{\"1\":{\"343\":1}}],[\"因此在新的数据集上需要定义新的分类器来重新训练\",{\"1\":{\"278\":1}}],[\"因此在效果上可能不如使用\",{\"1\":{\"274\":1}}],[\"因此在处理高分辨率图像或非互联网来源图像\",{\"1\":{\"215\":1}}],[\"因此在相同的\",{\"1\":{\"136\":1}}],[\"因此采用\",{\"1\":{\"170\":1}}],[\"因此存在较大的噪声\",{\"1\":{\"157\":1}}],[\"因此正负样本是\",{\"1\":{\"145\":1}}],[\"因此每个质心将根据这些不同的半径值与其周围点形成多个点集群\",{\"1\":{\"95\":1}}],[\"因此\",{\"1\":{\"94\":1,\"159\":1,\"238\":1,\"278\":2,\"290\":1,\"346\":1,\"385\":1,\"426\":1,\"434\":1,\"436\":1,\"477\":1,\"495\":2,\"508\":1,\"566\":1,\"587\":1,\"654\":1,\"660\":1,\"673\":1,\"675\":1,\"677\":1,\"686\":2}}],[\"因此加权融合的时候\",{\"1\":{\"72\":1}}],[\"因为概率质量\",{\"1\":{\"592\":1}}],[\"因为当\",{\"1\":{\"586\":1}}],[\"因为其概率密度随着与中心的平方距离增大而指数级衰减\",{\"1\":{\"586\":1}}],[\"因为大多数人根本没病\",{\"1\":{\"569\":1}}],[\"因为大模型的参数量非常大\",{\"1\":{\"415\":1}}],[\"因为某些集合太\",{\"1\":{\"566\":1}}],[\"因为cuda不支持macos\",{\"1\":{\"546\":1}}],[\"因为我只是为了了解内部代码情况\",{\"1\":{\"519\":1}}],[\"因为我们要预测下一个\",{\"1\":{\"285\":1}}],[\"因为低频词出现次数极少\",{\"1\":{\"511\":1}}],[\"因为更大容量的模型能吸收更多任务相关的模式\",{\"1\":{\"460\":1}}],[\"因为存在各种变化现象\",{\"1\":{\"448\":1}}],[\"因为作者的预训练模型是用连续的文本序列训练的\",{\"1\":{\"445\":1}}],[\"因为llm的prompt长度通常都是有长度限制的\",{\"1\":{\"434\":1}}],[\"因为收集人类反馈\",{\"1\":{\"416\":1}}],[\"因为微调的参数量跟预训练的是一样的多的\",{\"1\":{\"416\":1}}],[\"因为推理成本是跟prompt长度的平方正向相关的\",{\"1\":{\"415\":1}}],[\"因为通常大模型的实现原理\",{\"1\":{\"415\":1}}],[\"因为要最小化损失\",{\"1\":{\"407\":1}}],[\"因为不需要保存中间变量用于反向传播\",{\"1\":{\"388\":1}}],[\"因为第\",{\"1\":{\"387\":1}}],[\"因为在代码中有个冻结权重的操作\",{\"1\":{\"300\":1}}],[\"因为transformer和cnn相比缺少归纳偏置\",{\"1\":{\"287\":1}}],[\"因为都是有效\",{\"1\":{\"285\":1}}],[\"因为训练数据集中的文本\",{\"1\":{\"278\":1}}],[\"因为数据集的bias\",{\"1\":{\"259\":1}}],[\"因为dual\",{\"1\":{\"256\":1}}],[\"因为你这个更新操作\",{\"1\":{\"242\":1}}],[\"因为到后来第一个分支和第二个分支编码器差距越来越大\",{\"1\":{\"241\":1}}],[\"因为如果这样做了\",{\"1\":{\"241\":1}}],[\"因为如果不这样做\",{\"1\":{\"238\":1}}],[\"因为先进先出\",{\"1\":{\"241\":1}}],[\"因为有\",{\"1\":{\"240\":1}}],[\"因为直接在超大类别空间上做\",{\"1\":{\"240\":1}}],[\"因为正样本和负样本都是相对于锚点来说的\",{\"1\":{\"238\":1}}],[\"因为它只是一个与\",{\"1\":{\"596\":1}}],[\"因为它是通过对未知量\",{\"1\":{\"596\":1}}],[\"因为它的和不一定为\",{\"1\":{\"596\":1}}],[\"因为它可以单独控制均值与方差\",{\"1\":{\"582\":1}}],[\"因为它们对模型的适应性至关重要\",{\"1\":{\"428\":1}}],[\"因为它们的作用不同\",{\"1\":{\"317\":1}}],[\"因为它访问了其外部作用域的变量\",{\"1\":{\"367\":1}}],[\"因为它接收\",{\"1\":{\"367\":1}}],[\"因为它引用了\",{\"1\":{\"366\":1}}],[\"因为它衡量的是模型正确识别所有正例实例的能力\",{\"1\":{\"344\":1}}],[\"因为它认为每个图片自成一个类别\",{\"1\":{\"235\":1}}],[\"因为它能够在更低层次上递归地检视更高分辨率\",{\"1\":{\"97\":1}}],[\"因为它提供了高质量的点云和功能标注\",{\"1\":{\"62\":1}}],[\"因为前面有\",{\"1\":{\"92\":1}}],[\"因为学习到的特征和权重可以在多个局部区域中复用\",{\"1\":{\"86\":1}}],[\"因为\",{\"1\":{\"86\":1,\"236\":1,\"242\":1,\"355\":1,\"404\":1,\"542\":1,\"596\":1,\"660\":2}}],[\"因为背景区域大小等于特征图大小\",{\"1\":{\"59\":1}}],[\"知识更新\",{\"1\":{\"681\":1}}],[\"知识更新滞后性\",{\"1\":{\"679\":1}}],[\"知识截止日期\",{\"1\":{\"674\":2}}],[\"知识型模型\",{\"1\":{\"674\":1}}],[\"知识型与推理型双模式\",{\"1\":{\"674\":1}}],[\"知识型\",{\"1\":{\"674\":3}}],[\"知识扫盲\",{\"0\":{\"413\":1,\"429\":1},\"1\":{\"413\":1,\"429\":1}}],[\"知识蒸馏让小模型\",{\"1\":{\"123\":1}}],[\"知识蒸馏\",{\"0\":{\"123\":1}}],[\"知识星球\",{\"1\":{\"0\":1}}],[\"知道了哪些点跟自身的相关度更大\",{\"1\":{\"72\":1}}],[\"某个未知\",{\"1\":{\"596\":1}}],[\"某一维如果是\",{\"1\":{\"387\":1}}],[\"某一层次𝐿𝑖的区域特征是通过将来自下一级𝐿𝑖−1的子区域特征总结后的向量与直接处理该局部区域所有原始点的单个pointnet得到的特征向量进行concat得到的\",{\"1\":{\"97\":1}}],[\"某一层解码器输出的点特征\",{\"1\":{\"72\":1}}],[\"某些操作\",{\"1\":{\"384\":1}}],[\"某些功能类型在特定物体类别下会从训练集中省略\",{\"1\":{\"65\":1}}],[\"传递给下游变量\",{\"1\":{\"660\":1}}],[\"传染性事件\",{\"1\":{\"582\":1}}],[\"传给\",{\"1\":{\"375\":1}}],[\"传统\",{\"1\":{\"686\":1}}],[\"传统的\",{\"1\":{\"686\":1}}],[\"传统的视觉特征提取主要有两种典型实现方案\",{\"1\":{\"253\":1}}],[\"传统语言模型通常依赖单词或字符级输入\",{\"1\":{\"454\":1}}],[\"传统监督学习通常建模\",{\"1\":{\"454\":1}}],[\"传统bpe算法没有这一步\",{\"1\":{\"410\":8}}],[\"传统小规模数据集\",{\"1\":{\"178\":1}}],[\"传统知识蒸馏通过教师模型指导学生模型提升性能\",{\"1\":{\"150\":1}}],[\"传统vlp方法依赖目标检测器提取区域特征\",{\"1\":{\"149\":1}}],[\"传统方法的缺陷\",{\"1\":{\"103\":1}}],[\"传统方法难以适应不同情况\",{\"1\":{\"71\":1}}],[\"传统卷积神经网络难以直接处理\",{\"1\":{\"103\":1}}],[\"传入函数的参数\",{\"1\":{\"363\":1}}],[\"传入图像\",{\"1\":{\"285\":1}}],[\"传入图像编码信息\",{\"1\":{\"143\":1}}],[\"传入了\",{\"1\":{\"162\":1}}],[\"传入的点集特征集合经过转置处理后的维度为\",{\"1\":{\"70\":1}}],[\"传入question文本\",{\"1\":{\"70\":1}}],[\"传入数据维度\",{\"1\":{\"35\":1}}],[\"过长截断策略\",{\"1\":{\"520\":1}}],[\"过度顺从错误指令\",{\"1\":{\"472\":1}}],[\"过于中性\",{\"1\":{\"471\":1}}],[\"过拟合风险\",{\"1\":{\"395\":1}}],[\"过滤无效行\",{\"1\":{\"510\":1}}],[\"过滤掉句子数少于2的行\",{\"1\":{\"510\":1}}],[\"过滤掉vocab中的低频词\",{\"1\":{\"410\":1}}],[\"过滤掉与图像不匹配的原始和合成文本\",{\"1\":{\"140\":1}}],[\"过滤阶段\",{\"0\":{\"146\":1}}],[\"过滤对象既包括网页原始文本\",{\"1\":{\"128\":1}}],[\"过滤\",{\"1\":{\"122\":1,\"470\":1}}],[\"过滤器\",{\"1\":{\"120\":1}}],[\"过滤噪声\",{\"1\":{\"59\":1}}],[\"过程中混入预训练目标\",{\"1\":{\"471\":1}}],[\"过程\",{\"1\":{\"70\":1}}],[\"前后端搭建\",{\"1\":{\"687\":1}}],[\"前置知识\",{\"0\":{\"672\":1}}],[\"前三阶段的\",{\"1\":{\"670\":1}}],[\"前缀\",{\"1\":{\"659\":1}}],[\"前缀让\",{\"1\":{\"231\":1}}],[\"前向模型\",{\"1\":{\"597\":1}}],[\"前向传播逻辑\",{\"1\":{\"513\":1}}],[\"前向传播计算\",{\"1\":{\"407\":1}}],[\"前向传播计算损失值\",{\"1\":{\"405\":1}}],[\"前向传播流程\",{\"0\":{\"261\":1}}],[\"前向传播代码实现\",{\"1\":{\"163\":1}}],[\"前向传播函数\",{\"1\":{\"100\":1,\"101\":1,\"291\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1}}],[\"前向传播\",{\"1\":{\"82\":1,\"142\":1,\"163\":1,\"292\":1}}],[\"前向传播过程主要是为了计算两个训练目标的损失\",{\"1\":{\"145\":1}}],[\"前向传播过程\",{\"1\":{\"81\":1}}],[\"前向传播过程如下\",{\"1\":{\"70\":1}}],[\"前馈层\",{\"1\":{\"553\":2,\"556\":2}}],[\"前馈神经网络\",{\"1\":{\"548\":1}}],[\"前面是特殊token\",{\"1\":{\"511\":1}}],[\"前者是其2倍\",{\"1\":{\"510\":1}}],[\"前者为双向\",{\"1\":{\"126\":1}}],[\"前使用\",{\"1\":{\"385\":1}}],[\"前提是这两个事件是互斥的\",{\"1\":{\"567\":1}}],[\"前提是数据集大致平衡\",{\"1\":{\"353\":1}}],[\"前提条件是行优先存储\",{\"1\":{\"326\":1}}],[\"前两步不变\",{\"1\":{\"423\":1}}],[\"前两列\",{\"1\":{\"325\":1}}],[\"前两行\",{\"1\":{\"325\":1}}],[\"前两张图片的表征距离比较近\",{\"1\":{\"234\":1}}],[\"前两种与\",{\"1\":{\"20\":1}}],[\"前景背景融合异常\",{\"1\":{\"178\":1}}],[\"前景点少\",{\"1\":{\"404\":1}}],[\"前景点\",{\"1\":{\"78\":1}}],[\"前\",{\"1\":{\"159\":1,\"162\":1}}],[\"前期\",{\"1\":{\"159\":1}}],[\"前的某一层\",{\"1\":{\"98\":1}}],[\"骨干网络\",{\"0\":{\"171\":1},\"1\":{\"70\":1}}],[\"也称大型语言模型\",{\"1\":{\"673\":1}}],[\"也称为各向同性协方差矩阵\",{\"1\":{\"590\":1}}],[\"也称为贝叶斯定理\",{\"1\":{\"570\":1}}],[\"也称为\",{\"1\":{\"403\":1}}],[\"也称为intersection\",{\"1\":{\"403\":1}}],[\"也称为误报概率\",{\"1\":{\"345\":1}}],[\"也称为召回率\",{\"1\":{\"344\":1}}],[\"也叫正态分布\",{\"1\":{\"584\":1}}],[\"也叫作边缘似然\",{\"1\":{\"571\":1}}],[\"也得允许\",{\"1\":{\"566\":1}}],[\"也属于\",{\"1\":{\"566\":1}}],[\"也提示了未来改进方向\",{\"1\":{\"460\":1}}],[\"也在上表4中\",{\"1\":{\"448\":1}}],[\"也分析了在四种不同设置下预训练模型的零次\",{\"1\":{\"440\":1}}],[\"也用\",{\"1\":{\"426\":2}}],[\"也几乎未引入额外的\",{\"1\":{\"425\":1}}],[\"也还有一些其它参数\",{\"1\":{\"414\":1}}],[\"也都是适用的\",{\"1\":{\"395\":1}}],[\"也会被加载\",{\"1\":{\"389\":1}}],[\"也不及\",{\"1\":{\"471\":1}}],[\"也不局限于某一个方案\",{\"1\":{\"416\":1}}],[\"也不对\",{\"1\":{\"397\":1}}],[\"也不会参与梯度计算或优化\",{\"1\":{\"389\":1}}],[\"也不会计算梯度\",{\"1\":{\"388\":1}}],[\"也不需要高分辨率图像\",{\"1\":{\"149\":1}}],[\"也通常是句子长度\",{\"1\":{\"305\":1}}],[\"也需要保持输入图像尺寸与预训练时一致\",{\"1\":{\"290\":1}}],[\"也是该分布的众数\",{\"1\":{\"584\":1}}],[\"也是其一大亮点\",{\"1\":{\"273\":1}}],[\"也是如此\",{\"1\":{\"191\":1}}],[\"也就为非线性激活函数提供了更多可以学习的特征组合\",{\"1\":{\"294\":1}}],[\"也就几万个类别\",{\"1\":{\"240\":1}}],[\"也就是我们常说的\",{\"1\":{\"676\":1}}],[\"也就是我的\",{\"1\":{\"240\":1}}],[\"也就是改变x对于y的影响因子\",{\"1\":{\"626\":1}}],[\"也就是所谓的\",{\"1\":{\"592\":1,\"676\":1}}],[\"也就是在某个距离范围内\",{\"1\":{\"592\":1}}],[\"也就是在句子开头加一个\",{\"1\":{\"506\":1}}],[\"也就是两个句子\",{\"1\":{\"506\":1}}],[\"也就是有哪些子词以及这些子词的出现次数\",{\"1\":{\"410\":1}}],[\"也就是形成闭包\",{\"1\":{\"366\":1}}],[\"也就是每个变量自己的方差\",{\"1\":{\"355\":1}}],[\"也就是和输入的\",{\"1\":{\"309\":1}}],[\"也就是q\",{\"1\":{\"298\":1}}],[\"也就是经过卷积后拼接得到的特征图\",{\"1\":{\"291\":1}}],[\"也就是卷积核的数量\",{\"1\":{\"291\":1}}],[\"也就是一张图像搭配与之对应的文本描述\",{\"1\":{\"271\":1}}],[\"也就是\",{\"1\":{\"240\":1,\"423\":1}}],[\"也就是字典的大小就是分母下方的类别数量\",{\"1\":{\"240\":1}}],[\"也就是现在只有两个类别\",{\"1\":{\"240\":1}}],[\"也就是会得到这样一个向量\",{\"1\":{\"240\":1}}],[\"也就是类似\",{\"1\":{\"239\":1}}],[\"也就是原始的信号空间是由单词组成\",{\"1\":{\"238\":1}}],[\"也就是同一个类别的图片\",{\"1\":{\"235\":1}}],[\"也就是imagenet上的预训练模型\",{\"1\":{\"233\":1}}],[\"也就是说这个张量可以看作是一个\",{\"1\":{\"323\":1}}],[\"也就是说当我们用一个很小的\",{\"1\":{\"238\":1}}],[\"也就是说\",{\"1\":{\"76\":1,\"225\":1,\"344\":1,\"345\":1,\"544\":1,\"566\":1,\"579\":1,\"587\":1,\"682\":1}}],[\"也就是取出上面随机选择的问题文本\",{\"1\":{\"68\":1}}],[\"也没有浓缩得那么简洁\",{\"1\":{\"238\":1}}],[\"也包括合成描述\",{\"1\":{\"128\":1}}],[\"也被近年来的工作所采纳\",{\"1\":{\"126\":1}}],[\"也一并加入\",{\"1\":{\"92\":1,\"96\":1}}],[\"也一并提取\",{\"1\":{\"92\":1}}],[\"也应该以一种方式被处理\",{\"1\":{\"86\":1}}],[\"也可用参数化函数表示\",{\"1\":{\"565\":1}}],[\"也可能产生多个输出\",{\"1\":{\"651\":1}}],[\"也可能触发复制\",{\"1\":{\"385\":1}}],[\"也可能存在多个与被\",{\"1\":{\"157\":1}}],[\"也可能导致错误分类\",{\"1\":{\"112\":1}}],[\"也可以基于大模型的推理\",{\"1\":{\"677\":1}}],[\"也可以简称为概率推理\",{\"1\":{\"596\":1}}],[\"也可以将\",{\"1\":{\"576\":1}}],[\"也可以指\",{\"1\":{\"566\":1}}],[\"也可以传\",{\"1\":{\"389\":1}}],[\"也可以用以下方式表示\",{\"1\":{\"350\":1}}],[\"也可以进行随机裁剪\",{\"1\":{\"290\":1}}],[\"也可以采用视觉transformer模型\",{\"1\":{\"272\":1}}],[\"也可以很好地训练模型\",{\"1\":{\"238\":1}}],[\"也可以是特定任务\",{\"1\":{\"224\":1}}],[\"也可以是软标签\",{\"1\":{\"64\":1}}],[\"也可以归一化为\",{\"1\":{\"82\":1}}],[\"也可\",{\"1\":{\"49\":1}}],[\"点数\",{\"1\":{\"404\":1}}],[\"点上取值\",{\"1\":{\"397\":1}}],[\"点积\",{\"1\":{\"390\":1}}],[\"点\",{\"1\":{\"353\":2}}],[\"点表示\",{\"1\":{\"350\":1}}],[\"点的数量\",{\"1\":{\"111\":1}}],[\"点的特征数据\",{\"1\":{\"92\":1}}],[\"点之间的相互作用\",{\"1\":{\"105\":1}}],[\"点之间存在相互作用\",{\"1\":{\"104\":1}}],[\"点与点之间有空间关系\",{\"1\":{\"104\":1}}],[\"点额外特征\",{\"1\":{\"92\":1}}],[\"点坐标\",{\"1\":{\"92\":1}}],[\"点大小\",{\"1\":{\"83\":1}}],[\"点集抽象层\",{\"1\":{\"92\":1}}],[\"点集的划分必须产生跨分区的共同结构\",{\"1\":{\"86\":1}}],[\"点集划分是指如何将一个大的点云分割成更小的\",{\"1\":{\"86\":1}}],[\"点集特征集合\",{\"1\":{\"75\":1}}],[\"点集集合中每个点的特征和文本特征信息进行融合\",{\"1\":{\"70\":1}}],[\"点特征作为键\",{\"1\":{\"72\":1}}],[\"点级别标注\",{\"1\":{\"68\":1}}],[\"点云模型\",{\"1\":{\"112\":1}}],[\"点云是点的集合\",{\"1\":{\"104\":1}}],[\"点云是三维几何数据的一种重要表示形式\",{\"1\":{\"103\":1}}],[\"点云语义分割模型\",{\"0\":{\"101\":1}}],[\"点云语义分割\",{\"0\":{\"98\":1}}],[\"点云的姿态可能各不相同\",{\"1\":{\"107\":1}}],[\"点云的无序性\",{\"1\":{\"104\":1,\"105\":1}}],[\"点云的额外特征\",{\"1\":{\"92\":1}}],[\"点云的跨模态点云形状补全方法\",{\"1\":{\"22\":1}}],[\"点云坐标数据\",{\"1\":{\"92\":2}}],[\"点云坐标\",{\"1\":{\"83\":1}}],[\"点云预处理\",{\"1\":{\"83\":1}}],[\"点云分割中最常用的指标之一\",{\"1\":{\"82\":1}}],[\"点云分支\",{\"1\":{\"54\":1}}],[\"点云维度\",{\"1\":{\"76\":1}}],[\"点云对象和一个自然语言问题\",{\"1\":{\"70\":1}}],[\"点云对象组成\",{\"1\":{\"16\":1}}],[\"点云id\",{\"1\":{\"68\":2}}],[\"点云来源\",{\"1\":{\"67\":1}}],[\"点云中点的顺序不影响整体形状\",{\"1\":{\"115\":1}}],[\"点云中点的数量\",{\"1\":{\"82\":1}}],[\"点云中的每个点被标注为支持一个或多个功能类型\",{\"1\":{\"62\":1}}],[\"点云中相关点\",{\"1\":{\"41\":1}}],[\"点云无序特征\",{\"1\":{\"59\":1}}],[\"点云功能区域索引列表\",{\"1\":{\"58\":1}}],[\"点云功能区域掩码列表\",{\"1\":{\"58\":1}}],[\"点云列表\",{\"1\":{\"58\":1}}],[\"点云配对数据的数据集piad\",{\"1\":{\"48\":1}}],[\"点云等\",{\"1\":{\"43\":1}}],[\"点云编码器\",{\"1\":{\"40\":1}}],[\"点云特征图\",{\"1\":{\"76\":1}}],[\"点云特征与几何结构特征的融合\",{\"0\":{\"36\":1}}],[\"点云特征和几何结构特征做特征融合\",{\"1\":{\"35\":1}}],[\"点云特征\",{\"1\":{\"30\":1}}],[\"点云样本列表\",{\"1\":{\"29\":1}}],[\"点云索引下标区间\",{\"1\":{\"29\":1}}],[\"点云索引文件路径\",{\"1\":{\"29\":1}}],[\"点云\",{\"0\":{\"114\":1},\"1\":{\"29\":2,\"67\":1,\"114\":2}}],[\"点云数据归一化处理\",{\"1\":{\"83\":1}}],[\"点云数据组织形式\",{\"1\":{\"68\":1}}],[\"点云数据\",{\"1\":{\"40\":1,\"68\":1,\"83\":1,\"101\":1}}],[\"点云数量是前者的五倍\",{\"1\":{\"17\":1}}],[\"点云数\",{\"1\":{\"17\":1}}],[\"点云部分主要来自以下开源数据源\",{\"1\":{\"17\":1}}],[\"点云关联\",{\"1\":{\"7\":1}}],[\"种语言和方言\",{\"1\":{\"674\":1}}],[\"种模型大小\",{\"1\":{\"674\":1}}],[\"种重复排列\",{\"1\":{\"600\":1}}],[\"种不同的排列方式\",{\"1\":{\"600\":1}}],[\"种可能结果\",{\"1\":{\"599\":4}}],[\"种可能的顺序\",{\"1\":{\"445\":1}}],[\"种唯一组合\",{\"1\":{\"67\":1}}],[\"种物体\",{\"1\":{\"63\":1}}],[\"内循环\",{\"1\":{\"464\":1}}],[\"内取平均\",{\"1\":{\"404\":1}}],[\"内样本取平均\",{\"1\":{\"402\":1,\"403\":1,\"404\":1}}],[\"内\",{\"1\":{\"401\":1,\"407\":1,\"474\":1}}],[\"内存释放机制\",{\"1\":{\"658\":1}}],[\"内存使用量大幅降低\",{\"1\":{\"658\":1}}],[\"内存占用低\",{\"1\":{\"658\":1}}],[\"内存占用高\",{\"1\":{\"658\":1}}],[\"内存\",{\"1\":{\"481\":1}}],[\"内存管理\",{\"1\":{\"481\":1}}],[\"内存步长\",{\"1\":{\"384\":1}}],[\"内存布局\",{\"1\":{\"383\":1}}],[\"内存地址不变\",{\"1\":{\"327\":1}}],[\"内存空间得以连续布局\",{\"1\":{\"326\":1}}],[\"内存先存储第\",{\"1\":{\"322\":2}}],[\"内容不可追溯\",{\"1\":{\"679\":1}}],[\"内容结构遵循原文小节安排\",{\"1\":{\"469\":1}}],[\"内容\",{\"1\":{\"67\":1}}],[\"内部又是\",{\"1\":{\"508\":1}}],[\"内部是\",{\"1\":{\"508\":1}}],[\"内部标注人员手动构造的示例\",{\"1\":{\"224\":1}}],[\"内部\",{\"1\":{\"33\":1,\"73\":2}}],[\"内部或\",{\"1\":{\"33\":1}}],[\"抓取所有垃圾邮件\",{\"1\":{\"347\":1}}],[\"抓取耳机\",{\"1\":{\"65\":1}}],[\"抓握中部\",{\"1\":{\"28\":1}}],[\"目的\",{\"1\":{\"283\":1,\"284\":1,\"285\":1}}],[\"目的是为了让llm的推理能力能够更进一步提升\",{\"1\":{\"435\":1}}],[\"目的是\",{\"1\":{\"404\":1}}],[\"目的是让点云\",{\"1\":{\"107\":1}}],[\"目的是从一个大的数据集中选出一组代表性强的点\",{\"1\":{\"89\":1}}],[\"目的是测试模型对新组合的泛化能力\",{\"1\":{\"65\":1}}],[\"目的是评估模型在熟悉场景下的表现\",{\"1\":{\"65\":1}}],[\"目录获取所有图片路径\",{\"1\":{\"276\":1,\"277\":1}}],[\"目前采用的主要改进如下\",{\"1\":{\"674\":1}}],[\"目前实现的版本无法实现上图中计算图多分支的结构\",{\"1\":{\"635\":1}}],[\"目前尚无有效方式解释它为何会给出某一答案\",{\"1\":{\"463\":1}}],[\"目前尚不清楚模型在推理任务中是否\",{\"1\":{\"463\":1}}],[\"目前还没有达成共识\",{\"1\":{\"440\":1}}],[\"目前主流的方法包括2019年\",{\"1\":{\"424\":1}}],[\"目前参数量最小的多模态transformer方法\",{\"1\":{\"254\":1}}],[\"目前\",{\"1\":{\"166\":1,\"673\":1,\"674\":2,\"679\":1,\"687\":1}}],[\"目标人群\",{\"1\":{\"687\":1}}],[\"目标掩码\",{\"1\":{\"557\":1}}],[\"目标任务\",{\"1\":{\"440\":1}}],[\"目标点\",{\"1\":{\"404\":1}}],[\"目标检测\",{\"1\":{\"404\":1}}],[\"目标检测器开销高\",{\"1\":{\"149\":1}}],[\"目标函数如下\",{\"1\":{\"470\":1}}],[\"目标函数\",{\"1\":{\"395\":2}}],[\"目标仍是\",{\"1\":{\"387\":1}}],[\"目标让视觉编码器输出的图像特征与语言模型的词向量空间对齐\",{\"1\":{\"226\":1}}],[\"目标进行微调\",{\"1\":{\"128\":1}}],[\"目标在\",{\"1\":{\"128\":1}}],[\"目标是使现有模型更符合人类意图\",{\"1\":{\"472\":1}}],[\"目标是使正样本的图文对在特征空间中接近\",{\"1\":{\"127\":1}}],[\"目标是学习一个全局表示\",{\"1\":{\"440\":1}}],[\"目标是训练好\",{\"1\":{\"282\":1}}],[\"目标是为了让真正标签的输出尽可能的大\",{\"1\":{\"240\":1}}],[\"目标是让模型在预训练的基础上进一步掌握多模态指令理解与复杂推理能力\",{\"1\":{\"227\":1}}],[\"目标是优化模型\",{\"1\":{\"8\":1}}],[\"目标\",{\"1\":{\"98\":1,\"166\":1,\"190\":3,\"224\":1,\"404\":2,\"444\":1,\"491\":1,\"492\":1}}],[\"目标功能区域\",{\"1\":{\"78\":1}}],[\"目标物体框\",{\"1\":{\"58\":2}}],[\"🚀\",{\"0\":{\"603\":1}}],[\"🔗\",{\"1\":{\"573\":1}}],[\"📚\",{\"1\":{\"542\":1}}],[\"👉\",{\"1\":{\"307\":1,\"309\":1,\"359\":1}}],[\"📌\",{\"1\":{\"112\":1}}],[\"📐\",{\"1\":{\"112\":1}}],[\"📊\",{\"1\":{\"112\":1}}],[\"📈\",{\"1\":{\"112\":2}}],[\"📉\",{\"1\":{\"112\":2}}],[\"🔍\",{\"1\":{\"112\":6,\"542\":1}}],[\"📦\",{\"1\":{\"100\":1,\"115\":1}}],[\"🔹\",{\"1\":{\"65\":2}}],[\"🔹目标\",{\"1\":{\"28\":4}}],[\"💡\",{\"1\":{\"64\":1,\"112\":1,\"240\":1,\"542\":1}}],[\"构成样本空间\",{\"1\":{\"569\":1}}],[\"构成一个位置序列矩阵\",{\"1\":{\"523\":1}}],[\"构成了一个\",{\"1\":{\"405\":1}}],[\"构造训练目标\",{\"1\":{\"285\":1}}],[\"构造目标标签\",{\"1\":{\"163\":1}}],[\"构造新的图文对\",{\"1\":{\"162\":1}}],[\"构造一对一的匹配目标\",{\"1\":{\"161\":1}}],[\"构造一个新的张量\",{\"1\":{\"514\":1}}],[\"构造一个与\",{\"1\":{\"163\":1}}],[\"构造一个单位矩阵\",{\"1\":{\"108\":1}}],[\"构造一个从\",{\"1\":{\"92\":1}}],[\"构造软标签\",{\"1\":{\"157\":1}}],[\"构造两组负样本\",{\"1\":{\"145\":1}}],[\"构造图文匹配矩阵\",{\"1\":{\"145\":3}}],[\"构造图像的\",{\"1\":{\"161\":1}}],[\"构造图像\",{\"1\":{\"143\":1}}],[\"构造输入的\",{\"1\":{\"143\":1}}],[\"构造\",{\"1\":{\"143\":1,\"147\":1,\"161\":1,\"162\":1,\"513\":1}}],[\"构造对应的数据加载器\",{\"1\":{\"142\":1}}],[\"构造出你所需要的所有\",{\"1\":{\"566\":1}}],[\"构造出\",{\"1\":{\"69\":1}}],[\"构造出对应的二值掩码\",{\"1\":{\"64\":1}}],[\"构建出一个来源于实际业务的小型验证集\",{\"1\":{\"687\":1}}],[\"构建器平台\",{\"1\":{\"674\":1}}],[\"构建泰勒展开\",{\"1\":{\"665\":1}}],[\"构建样本\",{\"1\":{\"512\":1}}],[\"构建词汇表\",{\"1\":{\"410\":1}}],[\"构建一个基于\",{\"1\":{\"403\":1,\"404\":1}}],[\"构建一个组合损失函数\",{\"1\":{\"402\":1}}],[\"构建一个下三角矩阵作为因果掩码矩阵\",{\"1\":{\"285\":1}}],[\"构建一个多样化的指令\",{\"1\":{\"224\":1}}],[\"构建匹配标签\",{\"1\":{\"284\":1}}],[\"构建输入图像列表\",{\"1\":{\"284\":1}}],[\"构建输入文本列表\",{\"1\":{\"284\":1}}],[\"构建query和text的padding\",{\"1\":{\"284\":1}}],[\"构建query\",{\"1\":{\"284\":1}}],[\"构建padding\",{\"1\":{\"282\":1}}],[\"构建描述文本并提取特征\",{\"1\":{\"273\":1}}],[\"构建的\",{\"1\":{\"268\":1}}],[\"构建了语言模型\",{\"1\":{\"505\":1}}],[\"构建了一个动态的字典\",{\"1\":{\"237\":1}}],[\"构建了一个强大且通用的视觉\",{\"1\":{\"198\":1}}],[\"构建了多模态对话系统\",{\"1\":{\"185\":1}}],[\"构建动量编码器\",{\"1\":{\"160\":1}}],[\"构建文本所有对比特征\",{\"1\":{\"147\":1}}],[\"构建图像所有对比特征\",{\"1\":{\"147\":1}}],[\"构建语言建模标签\",{\"1\":{\"142\":1}}],[\"构建预训练语料\",{\"1\":{\"138\":1}}],[\"构建全局特征向量\",{\"1\":{\"115\":1}}],[\"构建点云的层次化表示\",{\"1\":{\"112\":1}}],[\"构建点之间的邻接图\",{\"1\":{\"112\":1}}],[\"构建mlp层\",{\"1\":{\"100\":1}}],[\"构建用于特征传播\",{\"1\":{\"100\":1}}],[\"构建它们的局部邻域区域\",{\"1\":{\"92\":1}}],[\"构建局部邻域的半径\",{\"1\":{\"92\":2}}],[\"构建问题\",{\"0\":{\"63\":1}}],[\"构建\",{\"1\":{\"43\":2,\"78\":2,\"219\":1,\"285\":1,\"404\":1}}],[\"陈述句等\",{\"1\":{\"63\":1}}],[\"疑问句\",{\"1\":{\"63\":1}}],[\"改善了对长文本的理解和生成能力\",{\"1\":{\"674\":1}}],[\"改善单模态编码语义理解\",{\"1\":{\"149\":1}}],[\"改用50k词汇表的字节级bpe编码\",{\"1\":{\"497\":1}}],[\"改用动态掩码\",{\"1\":{\"493\":1}}],[\"改变维度顺序\",{\"1\":{\"384\":1}}],[\"改变\",{\"1\":{\"116\":1}}],[\"改进了推理能力和指令遵循能力\",{\"1\":{\"674\":1}}],[\"改进的合并策略\",{\"1\":{\"454\":1}}],[\"改进的字节对编码\",{\"1\":{\"454\":1}}],[\"改进版\",{\"1\":{\"447\":1}}],[\"改进版的\",{\"1\":{\"447\":1}}],[\"改进而来\",{\"1\":{\"404\":1}}],[\"改进\",{\"1\":{\"112\":6}}],[\"改为\",{\"1\":{\"63\":1,\"546\":1}}],[\"改良的交叉注意力\",{\"0\":{\"32\":1}}],[\"例外情况\",{\"1\":{\"455\":1}}],[\"例子如下\",{\"1\":{\"381\":1,\"655\":1}}],[\"例子\",{\"1\":{\"378\":1}}],[\"例子说明\",{\"0\":{\"315\":1}}],[\"例\",{\"1\":{\"63\":1,\"600\":1}}],[\"例如原始数据或预处理后的数据\",{\"1\":{\"682\":1}}],[\"例如数据收集模块或预处理模块\",{\"1\":{\"682\":1}}],[\"例如数学问题\",{\"1\":{\"676\":1}}],[\"例如拥有\",{\"1\":{\"673\":1}}],[\"例如加法\",{\"1\":{\"651\":1}}],[\"例如灰度图像\",{\"0\":{\"594\":1}}],[\"例如某些事件之间是正相关的\",{\"1\":{\"582\":1}}],[\"例如某些包在arm64系统上没有预先编译好的版本\",{\"1\":{\"338\":1}}],[\"例如上图中\",{\"1\":{\"508\":1}}],[\"例如65b模型维度为8192\",{\"1\":{\"481\":1}}],[\"例如1\",{\"1\":{\"467\":1}}],[\"例如预测缺失的单词或句子\",{\"1\":{\"454\":1}}],[\"例如背景像素可能占据了大部分\",{\"1\":{\"399\":1}}],[\"例如将\",{\"1\":{\"397\":1}}],[\"例如我们希望将每个\",{\"1\":{\"396\":1}}],[\"例如从\",{\"1\":{\"396\":1}}],[\"例如经过了\",{\"1\":{\"385\":1}}],[\"例如总共\",{\"1\":{\"345\":1,\"346\":1}}],[\"例如卷云\",{\"1\":{\"342\":1}}],[\"例如参数量为\",{\"1\":{\"297\":1}}],[\"例如clip\",{\"1\":{\"280\":1}}],[\"例如谷歌的bit和vit基于jft\",{\"1\":{\"278\":1}}],[\"例如2017年的那篇工作只在imagenet上实现了11\",{\"1\":{\"278\":1}}],[\"例如virtex基于transformer的语言模型\",{\"1\":{\"278\":1}}],[\"例如openai的gpt\",{\"1\":{\"278\":1}}],[\"例如在原图上坐标为\",{\"1\":{\"396\":1}}],[\"例如在多头自注意力机制或前馈网络中引入卷积层\",{\"1\":{\"299\":1}}],[\"例如在\",{\"1\":{\"194\":1}}],[\"例如法向量\",{\"1\":{\"46\":1}}],[\"例如不同形状的椅子\",{\"1\":{\"29\":1}}],[\"例如\",{\"1\":{\"6\":1,\"7\":3,\"45\":1,\"62\":1,\"65\":1,\"70\":1,\"83\":1,\"98\":1,\"100\":1,\"157\":2,\"159\":1,\"163\":2,\"193\":1,\"196\":1,\"273\":1,\"274\":1,\"278\":1,\"287\":1,\"299\":2,\"325\":1,\"326\":1,\"332\":1,\"343\":1,\"353\":1,\"366\":1,\"372\":1,\"395\":1,\"396\":2,\"397\":3,\"405\":1,\"408\":1,\"454\":1,\"471\":2,\"472\":1,\"480\":1,\"482\":1,\"495\":1,\"505\":3,\"506\":1,\"540\":1,\"565\":5,\"566\":1,\"576\":1,\"616\":1,\"630\":1,\"654\":2,\"657\":3,\"660\":3,\"673\":1,\"687\":1}}],[\"上线后用户增长迅速\",{\"1\":{\"674\":1}}],[\"上时\",{\"1\":{\"596\":1}}],[\"上式等价于\",{\"1\":{\"568\":1}}],[\"上做\",{\"1\":{\"566\":1}}],[\"上做最大池化\",{\"1\":{\"92\":1}}],[\"上取得\",{\"1\":{\"492\":1}}],[\"上取得最先进性能\",{\"1\":{\"120\":1}}],[\"上超越未微调的palm\",{\"1\":{\"482\":1}}],[\"上超越了之前的有监督预训练模型\",{\"1\":{\"237\":1}}],[\"上超越了多个\",{\"1\":{\"195\":1}}],[\"上更真实\",{\"1\":{\"471\":1}}],[\"上收集\",{\"1\":{\"470\":1}}],[\"上达到了最先进的性能\",{\"1\":{\"491\":1}}],[\"上达到\",{\"1\":{\"455\":1}}],[\"上表现较差\",{\"1\":{\"455\":1}}],[\"上提升显著\",{\"1\":{\"455\":1}}],[\"上提升8\",{\"1\":{\"439\":1}}],[\"上仍表现欠拟合\",{\"1\":{\"454\":1}}],[\"上爬取高赞\",{\"1\":{\"454\":1}}],[\"上使用多头自注意力操作\",{\"1\":{\"443\":1}}],[\"上述例子是\",{\"1\":{\"667\":1}}],[\"上述公式简化为\",{\"1\":{\"567\":1}}],[\"上述定义也可推广到多维空间\",{\"1\":{\"566\":1}}],[\"上述的全量微调流程问题在于大模型的参数量往往特别大\",{\"1\":{\"423\":1}}],[\"上述即为pointnet++设计中的两个核心挑战\",{\"1\":{\"86\":1}}],[\"上一句话\",{\"1\":{\"506\":1}}],[\"上一部分介绍了一系列模型指标\",{\"1\":{\"349\":1}}],[\"上一层点集中的点特征重建过程中\",{\"1\":{\"70\":1}}],[\"上面优化方向很多\",{\"1\":{\"511\":1}}],[\"上面所举例子并没有使用kv\",{\"1\":{\"474\":1}}],[\"上面代码实现中使用的是加权交叉熵损失\",{\"1\":{\"407\":1}}],[\"上面代码实现中使用的是可学习位置嵌入\",{\"1\":{\"293\":1}}],[\"上面的问题本质是因为函数调用顺序错误导致的\",{\"1\":{\"655\":1}}],[\"上面的完美模型包含边长为\",{\"1\":{\"351\":1}}],[\"上面的花卉分类案例相当于使用图像去匹配最合适的文本描述\",{\"1\":{\"276\":1}}],[\"上面已经给出了数据集加载以及vit模型核心代码实现了\",{\"1\":{\"300\":1}}],[\"上图中的两句话明显是连续的\",{\"1\":{\"506\":1}}],[\"上图中是每一个patch中各位置的位置编码相似性度量\",{\"1\":{\"293\":1}}],[\"上图是4种不同类型的vlp模型示意图\",{\"1\":{\"255\":1}}],[\"上的研究大致可以分为以下几个阶段\",{\"1\":{\"674\":1}}],[\"上的一元连续分布\",{\"1\":{\"583\":1}}],[\"上的生成能力\",{\"1\":{\"455\":1}}],[\"上的问题答案数据集\",{\"1\":{\"448\":1}}],[\"上的\",{\"1\":{\"249\":1,\"395\":1}}],[\"上的表现较差\",{\"1\":{\"212\":1}}],[\"上下文感知\",{\"1\":{\"675\":1}}],[\"上下文长度\",{\"1\":{\"674\":6}}],[\"上下文长度扩展至\",{\"1\":{\"674\":2}}],[\"上下文长度4096\",{\"1\":{\"219\":1}}],[\"上下文\",{\"1\":{\"540\":3,\"674\":1}}],[\"上下文学习能力是由\",{\"1\":{\"676\":1}}],[\"上下文学习\",{\"1\":{\"465\":1,\"676\":1}}],[\"上下文建模\",{\"1\":{\"464\":1}}],[\"上下文窗口限制性能提升\",{\"1\":{\"463\":1}}],[\"上下文窗口从512扩展到1024\",{\"1\":{\"454\":1}}],[\"上下文预测等\",{\"1\":{\"240\":1}}],[\"上下文丰富化\",{\"1\":{\"63\":1}}],[\"上训练速度达380\",{\"1\":{\"481\":1}}],[\"上训练时\",{\"1\":{\"457\":1}}],[\"上训练\",{\"1\":{\"200\":1,\"201\":1,\"249\":1}}],[\"上均有显著提升\",{\"1\":{\"196\":1}}],[\"上均取得\",{\"1\":{\"194\":1}}],[\"上进行训练\",{\"1\":{\"674\":1}}],[\"上进行预训练\",{\"1\":{\"278\":1,\"290\":1}}],[\"上进行语义分割测试\",{\"1\":{\"193\":1}}],[\"上进行线性探测评估\",{\"1\":{\"193\":1}}],[\"上微调\",{\"1\":{\"175\":1,\"203\":1}}],[\"上添加任务层\",{\"1\":{\"167\":1}}],[\"上验证了其有效性\",{\"1\":{\"166\":1}}],[\"上升\",{\"1\":{\"159\":1}}],[\"上预测得到的概率分布为\",{\"1\":{\"157\":1}}],[\"上预训练的\",{\"1\":{\"131\":1}}],[\"上所有样本索引的集合\",{\"1\":{\"145\":1}}],[\"上轻量微调\",{\"1\":{\"128\":1}}],[\"上略低于\",{\"1\":{\"112\":1}}],[\"上\",{\"1\":{\"112\":1,\"145\":1,\"194\":1,\"471\":1,\"594\":1}}],[\"上也具有泛化能力\",{\"1\":{\"82\":1}}],[\"上采样\",{\"1\":{\"100\":2}}],[\"上采样后的点云特征\",{\"1\":{\"76\":1}}],[\"上采样过程中\",{\"1\":{\"70\":1}}],[\"上采样过程\",{\"1\":{\"46\":1}}],[\"上采样至原始点数后记为\",{\"1\":{\"14\":1}}],[\"原有代码\",{\"1\":{\"657\":2}}],[\"原像\",{\"1\":{\"565\":1}}],[\"原序列添加特殊token标记图\",{\"1\":{\"520\":1}}],[\"原文\",{\"1\":{\"470\":1}}],[\"原sota\",{\"1\":{\"455\":2}}],[\"原模型虽大\",{\"1\":{\"428\":1}}],[\"原张量是\",{\"1\":{\"325\":1}}],[\"原本是正样本的\",{\"1\":{\"241\":1}}],[\"原理大致跟rlhf类似\",{\"1\":{\"416\":1}}],[\"原理\",{\"0\":{\"288\":1}}],[\"原理回顾\",{\"1\":{\"112\":1}}],[\"原理说明\",{\"1\":{\"105\":1}}],[\"原因分析\",{\"1\":{\"112\":2}}],[\"原则\",{\"1\":{\"63\":1}}],[\"原始论文layernorm在最后\",{\"1\":{\"552\":1}}],[\"原始bert使用30k的字符级bpe词汇表\",{\"1\":{\"495\":1}}],[\"原始bert使用256的批次大小训练1m步\",{\"1\":{\"495\":1}}],[\"原始bert使用\",{\"1\":{\"495\":1}}],[\"原始bert使用静态掩码\",{\"1\":{\"495\":1,\"497\":1}}],[\"原始bert的做法\",{\"1\":{\"495\":1}}],[\"原始\",{\"1\":{\"471\":1,\"472\":1,\"493\":2,\"494\":1}}],[\"原始空间中\",{\"1\":{\"359\":1}}],[\"原始形状为\",{\"1\":{\"327\":1}}],[\"原始的步长可能是\",{\"1\":{\"326\":1}}],[\"原始transformer的norm层在多头注意力和前馈网络之后\",{\"1\":{\"294\":1}}],[\"原始实现\",{\"1\":{\"266\":1}}],[\"原始文本\",{\"1\":{\"157\":1}}],[\"原始点集合\",{\"1\":{\"114\":1}}],[\"原始点数量\",{\"1\":{\"100\":1}}],[\"原始点对应的特征数据\",{\"1\":{\"100\":1}}],[\"原始点坐标数据\",{\"1\":{\"100\":1}}],[\"原始点坐标\",{\"1\":{\"98\":1}}],[\"原始点云数据\",{\"1\":{\"93\":1}}],[\"原始点云数量\",{\"1\":{\"46\":2}}],[\"原始点特征\",{\"1\":{\"74\":1,\"98\":1}}],[\"原始交互文本\",{\"1\":{\"31\":1}}],[\"扩展模型的推理能力\",{\"1\":{\"683\":2}}],[\"扩展语言模型的大小\",{\"1\":{\"674\":1}}],[\"扩展函数库并验证高阶导数\",{\"1\":{\"665\":1}}],[\"扩展dezero以处理多输入多输出函数\",{\"1\":{\"663\":1}}],[\"扩展function类\",{\"0\":{\"630\":1}}],[\"扩展variable类\",{\"0\":{\"629\":1}}],[\"扩展\",{\"1\":{\"513\":1}}],[\"扩展至更大规模数据\",{\"1\":{\"493\":1}}],[\"扩展配置\",{\"1\":{\"454\":1}}],[\"扩展后的\",{\"1\":{\"327\":1}}],[\"扩展后的逻辑形状\",{\"1\":{\"327\":1}}],[\"扩展后便于广播乘法\",{\"1\":{\"100\":1}}],[\"扩展到多个变量\",{\"1\":{\"355\":1}}],[\"扩展到\",{\"1\":{\"327\":1}}],[\"扩展分类标记以匹配输入批次大小\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"扩展可学习的query\",{\"1\":{\"286\":1}}],[\"扩展gpt\",{\"1\":{\"210\":1}}],[\"扩展性\",{\"1\":{\"112\":1}}],[\"扩展维度后相乘\",{\"1\":{\"100\":1}}],[\"扩展生成更多问题\",{\"1\":{\"63\":1}}],[\"扩展构建了piadv2\",{\"1\":{\"6\":1}}],[\"具备基本能力的\",{\"1\":{\"687\":1}}],[\"具备对文本\",{\"1\":{\"674\":1}}],[\"具备跨模态交互能力\",{\"1\":{\"227\":1}}],[\"具备极强的感知能力\",{\"1\":{\"197\":1}}],[\"具体函数实现\",{\"1\":{\"661\":1}}],[\"具体函数的反向传播\",{\"0\":{\"631\":1}}],[\"具体函数继承该类并实现forward方法\",{\"1\":{\"612\":1}}],[\"具体如下\",{\"1\":{\"567\":1}}],[\"具体位置在\",{\"1\":{\"519\":1}}],[\"具体核心代码实现如下\",{\"1\":{\"514\":1}}],[\"具体过程如下图所示\",{\"1\":{\"513\":2}}],[\"具体步骤为\",{\"1\":{\"512\":1}}],[\"具体流程我们可以看下面这幅图\",{\"1\":{\"508\":1}}],[\"具体体现在\",{\"1\":{\"477\":1}}],[\"具体方法包括\",{\"1\":{\"467\":1}}],[\"具体方式可以是直接缩放\",{\"1\":{\"290\":1}}],[\"具体任务表现分析\",{\"1\":{\"455\":1}}],[\"具体要考虑解决的问题类型\",{\"1\":{\"434\":1}}],[\"具体来看\",{\"1\":{\"425\":1}}],[\"具体来说就是这个队列可以很大\",{\"1\":{\"238\":1}}],[\"具体来说\",{\"1\":{\"95\":1,\"170\":1,\"293\":1,\"418\":1,\"419\":1,\"682\":1}}],[\"具体可以从以下几个方面理解\",{\"1\":{\"395\":1}}],[\"具体可参考\",{\"1\":{\"239\":1}}],[\"具体示例如下所示\",{\"1\":{\"326\":1}}],[\"具体的\",{\"1\":{\"325\":1}}],[\"具体的实验结果可以参考clip公开的notebook\",{\"1\":{\"274\":1}}],[\"具体为vit\",{\"1\":{\"300\":1}}],[\"具体解释如下\",{\"1\":{\"293\":1}}],[\"具体代码实现如下\",{\"1\":{\"276\":1}}],[\"具体使用的是\",{\"1\":{\"275\":1}}],[\"具体包括\",{\"1\":{\"227\":1}}],[\"具体分为三步\",{\"1\":{\"224\":1}}],[\"具体而言\",{\"1\":{\"97\":1,\"165\":1}}],[\"具体做法\",{\"1\":{\"90\":1}}],[\"具体选择多少个中心点以及邻域内的数量由超参数确定\",{\"1\":{\"89\":1}}],[\"具体地如下图所示\",{\"1\":{\"506\":1}}],[\"具体地描述出来\",{\"1\":{\"432\":1}}],[\"具体地\",{\"1\":{\"14\":1}}],[\"具有出色的能力\",{\"1\":{\"675\":1}}],[\"具有较强的泛化能力\",{\"1\":{\"300\":1}}],[\"具有较强的实用性与拓展性\",{\"1\":{\"129\":1}}],[\"具有很强的zero\",{\"1\":{\"280\":1}}],[\"具有高度多样性和挑战性\",{\"1\":{\"227\":1}}],[\"具有以下优势\",{\"1\":{\"78\":1}}],[\"具有语义意义\",{\"1\":{\"62\":1}}],[\"等架构的实现\",{\"1\":{\"687\":1}}],[\"等多个技术领域\",{\"1\":{\"684\":1}}],[\"等多模态架构中的文本生成部分\",{\"1\":{\"268\":1}}],[\"等多模态生成模型\",{\"1\":{\"194\":1}}],[\"等多模态对话基准上达到\",{\"1\":{\"190\":1}}],[\"等项目已经出现并受到关注\",{\"1\":{\"678\":1}}],[\"等基于语音对话的产品也非常受欢迎\",{\"1\":{\"678\":1}}],[\"等基于图像的方法\",{\"1\":{\"112\":1}}],[\"等全面对标\",{\"1\":{\"674\":1}}],[\"等功能\",{\"1\":{\"674\":1}}],[\"等合作研发的语言大模型\",{\"1\":{\"674\":1}}],[\"等技术\",{\"1\":{\"674\":1}}],[\"等先进技术\",{\"1\":{\"674\":1}}],[\"等商业闭源模型\",{\"1\":{\"674\":1}}],[\"等函数节点\",{\"1\":{\"665\":1}}],[\"等运算符自然表达计算\",{\"1\":{\"663\":1}}],[\"等等\",{\"1\":{\"599\":1}}],[\"等会被\",{\"1\":{\"542\":1}}],[\"等著名悲剧\",{\"1\":{\"540\":1}}],[\"等模型趋势一致\",{\"1\":{\"484\":1}}],[\"等模型将预训练语言模型推向主流\",{\"1\":{\"464\":1}}],[\"等机制\",{\"1\":{\"469\":1}}],[\"等原始网络数据\",{\"1\":{\"454\":1}}],[\"等于\",{\"1\":{\"425\":1}}],[\"等人提出的\",{\"1\":{\"424\":1}}],[\"等价操作\",{\"1\":{\"390\":1}}],[\"等价于\",{\"1\":{\"375\":1,\"376\":1,\"403\":1,\"405\":1,\"660\":1}}],[\"等价于n个类别的cross\",{\"1\":{\"272\":1}}],[\"等操作\",{\"1\":{\"385\":1}}],[\"等元信息\",{\"1\":{\"372\":1}}],[\"等属性丢失\",{\"1\":{\"372\":1}}],[\"等强化学习算法\",{\"1\":{\"224\":1}}],[\"等任务上超越chinchilla\",{\"1\":{\"480\":1}}],[\"等任务上表现退化\",{\"1\":{\"471\":1}}],[\"等任务上的零样本表现\",{\"1\":{\"470\":1}}],[\"等任务上的局限性\",{\"1\":{\"460\":1}}],[\"等任务中也得到了广泛实践\",{\"1\":{\"469\":1}}],[\"等任务表现优异\",{\"1\":{\"220\":1}}],[\"等任务专用数据\",{\"1\":{\"217\":1}}],[\"等大规模ocr数据\",{\"1\":{\"217\":1}}],[\"等通用数据集\",{\"1\":{\"217\":1}}],[\"等新兴模型进一步推动多模态技术的发展\",{\"1\":{\"210\":1}}],[\"等方法\",{\"1\":{\"195\":1}}],[\"等主流模型\",{\"1\":{\"193\":1}}],[\"等指标更关注重合度\",{\"1\":{\"82\":1}}],[\"等\",{\"1\":{\"62\":1,\"63\":1,\"67\":2,\"78\":1,\"120\":1,\"163\":1,\"190\":1,\"193\":1,\"211\":1,\"224\":1,\"231\":1,\"299\":1,\"387\":1,\"472\":1,\"480\":1,\"493\":1,\"494\":1,\"513\":1,\"670\":2,\"673\":2,\"674\":1}}],[\"基本流程\",{\"0\":{\"687\":1}}],[\"基本语法\",{\"1\":{\"331\":1}}],[\"基准上\",{\"1\":{\"471\":1}}],[\"基准任务转换为指令格式\",{\"1\":{\"469\":1}}],[\"基函数叠加\",{\"1\":{\"395\":1}}],[\"基函数\",{\"1\":{\"395\":1}}],[\"基座是\",{\"1\":{\"191\":1}}],[\"基础模型\",{\"1\":{\"674\":1}}],[\"基础词汇仅需256个字节\",{\"1\":{\"454\":1}}],[\"基础点云可视化\",{\"1\":{\"83\":1}}],[\"基础数据来源\",{\"0\":{\"62\":1}}],[\"基线\",{\"1\":{\"75\":1}}],[\"基于小型验证集设计满足基本要求\",{\"1\":{\"687\":1}}],[\"基于静态的数据集训练\",{\"1\":{\"679\":1}}],[\"基于上下文编码\",{\"1\":{\"542\":1}}],[\"基于上述背景\",{\"1\":{\"424\":1}}],[\"基于transformer的优化设计\",{\"1\":{\"481\":1}}],[\"基于wikipedia引用分类\",{\"1\":{\"481\":1}}],[\"基于人类反馈的模型对齐\",{\"1\":{\"469\":1}}],[\"基于人类反馈的强化学习微调rlhf\",{\"1\":{\"416\":1}}],[\"基于人类反馈的强化学习\",{\"1\":{\"224\":1}}],[\"基于大模型的内在低秩特性\",{\"1\":{\"428\":1}}],[\"基于prompt\",{\"1\":{\"419\":1}}],[\"基于ai反馈的强化学习微调rlaif\",{\"1\":{\"416\":1}}],[\"基于标准交叉熵损失\",{\"1\":{\"404\":1}}],[\"基于标准vit架构的60亿参数视觉编码器\",{\"1\":{\"188\":1}}],[\"基于交叉熵损失进行扩展\",{\"1\":{\"404\":1}}],[\"基于步长的逻辑转置\",{\"1\":{\"326\":1}}],[\"基于自回归或语言掩码的预训练方法已经相对成熟\",{\"1\":{\"278\":1}}],[\"基于internvit\",{\"1\":{\"208\":1}}],[\"基于图像的文本生成损失\",{\"1\":{\"190\":1}}],[\"基于图卷积或注意力机制的模型更能捕捉这种非刚性变化\",{\"1\":{\"112\":1}}],[\"基于多语言llama\",{\"1\":{\"188\":1,\"189\":1}}],[\"基于视觉标记\",{\"1\":{\"170\":1}}],[\"基于变分自编码器视角\",{\"1\":{\"166\":1}}],[\"基于以下前提\",{\"1\":{\"90\":1}}],[\"基于\",{\"1\":{\"73\":1,\"142\":1,\"161\":1,\"214\":1,\"219\":1,\"226\":1,\"227\":1,\"280\":1,\"454\":1,\"493\":1,\"494\":1}}],[\"基于相机参数\",{\"1\":{\"52\":1}}],[\"基于几何映射的方法\",{\"1\":{\"51\":1}}],[\"基于语言引导的3d可供性分割\",{\"1\":{\"22\":1}}],[\"基于微调的mllm\",{\"1\":{\"6\":1}}],[\"简单节点定义\",{\"1\":{\"666\":1}}],[\"简单理解\",{\"1\":{\"566\":1}}],[\"简单微调后\",{\"1\":{\"482\":1}}],[\"简单样本抑制越强\",{\"1\":{\"404\":1}}],[\"简单样本主导梯度\",{\"1\":{\"404\":1}}],[\"简单样本的梯度贡献淹没难样本的梯度\",{\"1\":{\"404\":1}}],[\"简单来说\",{\"1\":{\"274\":1,\"321\":1,\"505\":1}}],[\"简化部署流程\",{\"1\":{\"685\":1}}],[\"简化和优化bert的训练过程\",{\"1\":{\"501\":1}}],[\"简化版\",{\"1\":{\"402\":1,\"661\":1}}],[\"简化跨模态融合\",{\"1\":{\"149\":1}}],[\"简而言之\",{\"1\":{\"368\":1}}],[\"简洁\",{\"1\":{\"114\":1}}],[\"简洁表达\",{\"1\":{\"63\":1}}],[\"简析pointnet网络模型及其背后原理\",{\"1\":{\"102\":1}}],[\"简析pointnet\",{\"0\":{\"102\":1}}],[\"简析pointnet++\",{\"0\":{\"85\":1},\"1\":{\"85\":1}}],[\"简称\",{\"1\":{\"60\":1,\"425\":1,\"428\":1,\"588\":1}}],[\"简介\",{\"0\":{\"6\":1,\"49\":1,\"166\":1,\"178\":1,\"181\":1,\"208\":1,\"440\":1,\"453\":1,\"460\":1,\"468\":1,\"480\":1}}],[\"自纠错机制\",{\"1\":{\"674\":1}}],[\"自发布以来就引发了人工智能社区的兴奋\",{\"1\":{\"674\":1}}],[\"自己写答案\",{\"1\":{\"542\":1}}],[\"自己和自己的值其实是占大头的\",{\"1\":{\"508\":1}}],[\"自一致性技术\",{\"0\":{\"435\":1}}],[\"自然代码表达\",{\"1\":{\"662\":1}}],[\"自然演示的机会\",{\"1\":{\"454\":1}}],[\"自然语言建模的场景\",{\"1\":{\"510\":1}}],[\"自然语言推理\",{\"1\":{\"508\":1}}],[\"自然语言推断nli\",{\"1\":{\"440\":1}}],[\"自然语言处理\",{\"0\":{\"559\":1},\"1\":{\"460\":1}}],[\"自然语言理解包含了广泛的多样性任务\",{\"1\":{\"439\":1}}],[\"自然语言指令可作为隐式任务描述\",{\"1\":{\"454\":1}}],[\"自然语言指令\",{\"1\":{\"40\":1}}],[\"自然指数\",{\"1\":{\"404\":1}}],[\"自适应基函数\",{\"1\":{\"395\":1}}],[\"自适应生成的非线性函数空间\",{\"1\":{\"395\":1}}],[\"自适应融合模块\",{\"0\":{\"71\":1,\"75\":1},\"1\":{\"70\":1}}],[\"自动执行dot文件转换并显示图像\",{\"1\":{\"666\":1}}],[\"自动执行反向传播\",{\"1\":{\"635\":1}}],[\"自动反向传播与框架基础能力提升\",{\"1\":{\"649\":1}}],[\"自动反向传播与计算图进阶\",{\"0\":{\"649\":1}}],[\"自动反向传播的实现\",{\"0\":{\"635\":1}}],[\"自动设置梯度\",{\"0\":{\"642\":1}}],[\"自动跳过\",{\"1\":{\"542\":1}}],[\"自动打分\",{\"1\":{\"471\":1}}],[\"自动按维度扩展\",{\"1\":{\"387\":1}}],[\"自动安装\",{\"1\":{\"338\":1}}],[\"自动生成ai使用指南\",{\"1\":{\"483\":1}}],[\"自动生成\",{\"1\":{\"227\":1}}],[\"自定义工具调用\",{\"1\":{\"674\":1}}],[\"自定义指令与记忆功能\",{\"1\":{\"674\":1}}],[\"自定义打印格式\",{\"1\":{\"659\":1}}],[\"自定义的批量数据处理函数\",{\"1\":{\"289\":1}}],[\"自定义数据集\",{\"1\":{\"289\":1}}],[\"自定义一个mydataset类来封装我们加载得到的数据集\",{\"1\":{\"289\":1}}],[\"自谷歌提出\",{\"1\":{\"270\":1}}],[\"自回归语言建模任务\",{\"1\":{\"285\":1}}],[\"自回归语言建模\",{\"0\":{\"268\":1},\"1\":{\"268\":1}}],[\"自监督方法的优势在于不再需要标注数据\",{\"1\":{\"278\":1}}],[\"自监督学习其实是一种特殊形式的无监督学习\",{\"1\":{\"240\":1}}],[\"自监督学习的方法可以从两个方面来进行优化或创新\",{\"1\":{\"240\":1}}],[\"自监督的方式进行学习\",{\"1\":{\"239\":1}}],[\"自监督预训练成为利用大规模无标注图像数据的关键方法\",{\"1\":{\"166\":1}}],[\"自家的数据集上同样效果良好\",{\"1\":{\"239\":1}}],[\"自训练方法\",{\"1\":{\"149\":1}}],[\"自信看源码进行学习\",{\"1\":{\"145\":1}}],[\"自蒸馏也取得了不错效果\",{\"1\":{\"123\":1}}],[\"自注意力子层\",{\"1\":{\"553\":2,\"556\":2}}],[\"自注意力运算\",{\"1\":{\"285\":1,\"477\":1}}],[\"自注意力和交叉注意力流程统一化\",{\"1\":{\"285\":1}}],[\"自注意力掩码策略\",{\"1\":{\"283\":1,\"284\":1,\"285\":1}}],[\"自注意力捕捉文本内部的依赖\",{\"1\":{\"264\":1}}],[\"自注意力机制是\",{\"1\":{\"548\":1}}],[\"自注意力机制能够捕捉图像中任意两个\",{\"1\":{\"292\":1}}],[\"自注意力机制\",{\"1\":{\"76\":1,\"304\":1}}],[\"自注意力\",{\"1\":{\"59\":1,\"264\":1,\"285\":2}}],[\"自注意力完成内部信息建模\",{\"1\":{\"59\":1}}],[\"消融研究\",{\"1\":{\"449\":1}}],[\"消融实验\",{\"0\":{\"229\":1}}],[\"消融项分析\",{\"1\":{\"24\":1}}],[\"消除量化误差\",{\"1\":{\"397\":1}}],[\"消除尺度差异\",{\"1\":{\"68\":1}}],[\"消除模态差异\",{\"1\":{\"59\":1}}],[\"保存指数\",{\"1\":{\"660\":1}}],[\"保存最优模型\",{\"1\":{\"514\":1}}],[\"保存字典到文件\",{\"1\":{\"511\":1}}],[\"保存注意力头的数量\",{\"1\":{\"295\":1}}],[\"保存嵌入维度\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"保存分类任务的类别数\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"保证运算按预期执行\",{\"1\":{\"660\":1}}],[\"保证整个\",{\"1\":{\"590\":1}}],[\"保证数量一致\",{\"1\":{\"512\":1}}],[\"保证训练的开始此旁路矩阵依然是\",{\"1\":{\"425\":1}}],[\"保证训练集和验证集的数据处理方式一致\",{\"1\":{\"290\":1}}],[\"保证顺序一致\",{\"1\":{\"289\":1}}],[\"保证随机结果可复现\",{\"1\":{\"289\":1}}],[\"保证变换是刚性的\",{\"1\":{\"107\":1}}],[\"保障了模型的通用性与灵活性\",{\"1\":{\"197\":1}}],[\"保留中间结果\",{\"1\":{\"658\":1}}],[\"保留反向传播所需的计算图连接\",{\"1\":{\"658\":1}}],[\"保留英文\",{\"1\":{\"481\":1}}],[\"保留接口以备后续扩展\",{\"1\":{\"403\":1}}],[\"保留接口以备扩展\",{\"1\":{\"402\":1,\"404\":1}}],[\"保留浮点型的\",{\"1\":{\"397\":1}}],[\"保留\",{\"1\":{\"390\":1}}],[\"保留维度\",{\"1\":{\"387\":1}}],[\"保留原函数元信息\",{\"0\":{\"372\":1}}],[\"保留原始几何信息\",{\"1\":{\"114\":1}}],[\"保留和text\",{\"1\":{\"283\":2}}],[\"保留为原始\",{\"1\":{\"163\":1}}],[\"保留下来\",{\"1\":{\"76\":2}}],[\"保留跨模态共享的显著特征\",{\"1\":{\"59\":1}}],[\"保持内容的时效性\",{\"1\":{\"679\":1}}],[\"保持原意清晰\",{\"1\":{\"590\":1}}],[\"保持原样不动的形式进行处理\",{\"1\":{\"512\":1}}],[\"保持原样\",{\"1\":{\"163\":1}}],[\"保持偏置和层归一化可训练\",{\"1\":{\"428\":1}}],[\"保持预训练模型的原始参数\",{\"1\":{\"423\":1}}],[\"保持高损失权重\",{\"1\":{\"404\":1}}],[\"保持冻结\",{\"1\":{\"227\":1}}],[\"保持视觉编码器和语言模型参数冻结\",{\"1\":{\"226\":1}}],[\"保持\",{\"1\":{\"143\":1}}],[\"保持不变\",{\"1\":{\"76\":1,\"493\":1,\"495\":1}}],[\"保持评估完整性\",{\"1\":{\"66\":1}}],[\"保持一致性是因为我们需要字典中的特征尽可能使用同一个或者相近的编码器进行表征\",{\"1\":{\"238\":1}}],[\"保持一致\",{\"1\":{\"20\":1,\"161\":1}}],[\"筛选测试\",{\"1\":{\"470\":1}}],[\"筛选高质量图像\",{\"1\":{\"190\":1}}],[\"筛选出的图文对与人工标注数据结合\",{\"1\":{\"128\":1}}],[\"筛选关键信息\",{\"1\":{\"59\":1}}],[\"筛选相关的几何特征\",{\"1\":{\"32\":1}}],[\"映射关系的反向求解\",{\"1\":{\"597\":1}}],[\"映射为实数\",{\"1\":{\"565\":1}}],[\"映射为\",{\"1\":{\"565\":1}}],[\"映射为4\",{\"1\":{\"59\":2}}],[\"映射\",{\"1\":{\"396\":1}}],[\"映射到实数\",{\"1\":{\"565\":1}}],[\"映射到特征图上之后是一个大小为\",{\"1\":{\"396\":1}}],[\"映射到特征图上\",{\"1\":{\"396\":1}}],[\"映射到分类空间中去\",{\"1\":{\"296\":1}}],[\"映射到离散标记\",{\"1\":{\"170\":1}}],[\"映射到\",{\"1\":{\"42\":1,\"401\":1}}],[\"根据导数公式\",{\"1\":{\"660\":1,\"662\":1}}],[\"根据观测数据对某一感兴趣的未知量的分布进行更新的过程\",{\"1\":{\"596\":1}}],[\"根据观察数据更新置信度的方法\",{\"1\":{\"596\":1}}],[\"根据条件概率的定义\",{\"1\":{\"569\":1,\"570\":1}}],[\"根据这个定义\",{\"1\":{\"568\":1}}],[\"根据decoder的隐状态输出一个词\",{\"1\":{\"550\":1}}],[\"根据编码器的输出生成目标序列\",{\"1\":{\"548\":1}}],[\"根据你的理解\",{\"1\":{\"542\":1}}],[\"根据索引矩阵\",{\"1\":{\"514\":1}}],[\"根据索引获取数据集中的图像和对应的标签\",{\"1\":{\"289\":1}}],[\"根据上下文\",{\"1\":{\"505\":1}}],[\"根据上述计算得到的和其相似度最高的分类文本索引\",{\"1\":{\"275\":1}}],[\"根据所在gptblock层级\",{\"1\":{\"477\":1}}],[\"根据频次表构建最终的词汇表\",{\"1\":{\"410\":1}}],[\"根据n轮迭代合并后的vocab来构建最终的频次表\",{\"1\":{\"410\":1}}],[\"根据特定的分割任务的需求和特点\",{\"1\":{\"408\":1}}],[\"根据预测的\",{\"1\":{\"542\":1}}],[\"根据预测误差对预测结果进行排序\",{\"1\":{\"406\":1}}],[\"根据预测值从灰色\",{\"1\":{\"83\":1}}],[\"根据正负样本比例调整\",{\"1\":{\"404\":1}}],[\"根据通用近似定理\",{\"1\":{\"395\":1}}],[\"根据公式\",{\"1\":{\"327\":1}}],[\"根据相关性\",{\"1\":{\"312\":1}}],[\"根据图像特征\",{\"1\":{\"285\":1}}],[\"根据imagenet数据集上的zero\",{\"1\":{\"278\":1}}],[\"根据文字搜索图片\",{\"1\":{\"276\":1,\"277\":1}}],[\"根据任务的分类需求\",{\"1\":{\"273\":1}}],[\"根据输入图像的长宽比和分辨率\",{\"1\":{\"207\":1}}],[\"根据视觉码本将图像像素\",{\"1\":{\"170\":1}}],[\"根据\",{\"1\":{\"163\":1,\"338\":1,\"407\":1}}],[\"根据注意力机制加权求和\",{\"1\":{\"115\":1}}],[\"根据距离反比加权\",{\"1\":{\"100\":1}}],[\"根据距离分配权重\",{\"1\":{\"100\":1}}],[\"根据样本索引取出样本数据\",{\"1\":{\"68\":1}}],[\"根据自然语言问题找出与之相关的功能区域\",{\"1\":{\"62\":1}}],[\"根据交互主体框\",{\"1\":{\"59\":1}}],[\"根据目标物体框\",{\"1\":{\"59\":1}}],[\"一文详尽之scaling\",{\"1\":{\"688\":1}}],[\"一文的建议\",{\"1\":{\"461\":1}}],[\"一样的人工智能\",{\"1\":{\"678\":1}}],[\"一系列推理加速技术\",{\"1\":{\"674\":1}}],[\"一共有\",{\"1\":{\"599\":1}}],[\"一共合并merges个高频字符对后\",{\"1\":{\"410\":2}}],[\"一定距离的\",{\"1\":{\"594\":1}}],[\"一定会发生\",{\"1\":{\"566\":1}}],[\"一方面是因为其数学处理非常方便\",{\"1\":{\"588\":1}}],[\"一词既可以指\",{\"1\":{\"566\":1}}],[\"一次性\",{\"1\":{\"495\":1}}],[\"一次训练的成本就在上千亿美元\",{\"1\":{\"424\":1}}],[\"一维卷积进行线性变换和升维\",{\"1\":{\"477\":1}}],[\"一维的存储空间\",{\"1\":{\"321\":1}}],[\"一些\",{\"1\":{\"675\":1}}],[\"一些常见的概率分布\",{\"0\":{\"573\":1}}],[\"一些任务\",{\"1\":{\"462\":1}}],[\"一些损失函数具有额外的超参数\",{\"1\":{\"408\":1}}],[\"一步步引导llm得出复杂问题的结果\",{\"1\":{\"436\":1}}],[\"一步步重建回原始点数量\",{\"1\":{\"70\":1}}],[\"一是完整的ltm的例子\",{\"1\":{\"436\":1}}],[\"一条是第一个\",{\"1\":{\"654\":1}}],[\"一条是只对部分的参数进行训练\",{\"1\":{\"416\":1}}],[\"一条是对全量的参数\",{\"1\":{\"416\":1}}],[\"一起变大或一起变小\",{\"1\":{\"355\":1}}],[\"一起送入\",{\"1\":{\"286\":1}}],[\"一起送入mini\",{\"1\":{\"91\":1}}],[\"一种更具鲁棒性的分布是\",{\"1\":{\"586\":1}}],[\"一种构造这类分布的方法是设定\",{\"1\":{\"585\":1}}],[\"一种假设是\",{\"1\":{\"449\":1}}],[\"一种平均值\",{\"1\":{\"348\":1}}],[\"一种思路是在转换之前\",{\"1\":{\"293\":1}}],[\"一种朴素的想法就是把一个个像素点拉平\",{\"1\":{\"291\":1}}],[\"一种比较好理解的方式\",{\"1\":{\"282\":1}}],[\"一种是平移不变形\",{\"1\":{\"287\":1}}],[\"一种是局部性\",{\"1\":{\"287\":1}}],[\"一种是常用的cnn架构resnet\",{\"1\":{\"272\":1}}],[\"一种是single\",{\"1\":{\"256\":1}}],[\"一种是单独使用\",{\"1\":{\"191\":1}}],[\"一般指通过不断发现\",{\"1\":{\"687\":1}}],[\"一般使用诸如\",{\"1\":{\"687\":1}}],[\"一般应先设定最小化目标\",{\"1\":{\"687\":1}}],[\"一般从2开始调优\",{\"1\":{\"404\":1}}],[\"一般\",{\"1\":{\"280\":2}}],[\"一般来说\",{\"1\":{\"240\":1,\"687\":1}}],[\"一会详细讲\",{\"1\":{\"236\":1}}],[\"一句话说\",{\"1\":{\"234\":1}}],[\"一句话总结\",{\"1\":{\"112\":1}}],[\"一类使用多模态\",{\"1\":{\"150\":1}}],[\"一张图对应一条生成的\",{\"1\":{\"128\":1}}],[\"一张图片对应多张同物体但形状不同的点云图片\",{\"1\":{\"58\":1}}],[\"一\",{\"0\":{\"305\":1,\"331\":1,\"603\":1},\"1\":{\"112\":1,\"469\":1}}],[\"一致性\",{\"1\":{\"241\":1}}],[\"一致的表征对齐\",{\"1\":{\"181\":1}}],[\"一致\",{\"1\":{\"98\":1,\"219\":1}}],[\"一组新的关键点位置\",{\"1\":{\"96\":1}}],[\"一组点云被处理和抽象\",{\"1\":{\"88\":1}}],[\"一个人检测阳性的总体概率是多少\",{\"1\":{\"569\":1}}],[\"一个人检测为阳性的总体概率是多少\",{\"1\":{\"569\":1}}],[\"一个事件总体概率\",{\"1\":{\"569\":1}}],[\"一个把\",{\"1\":{\"565\":1}}],[\"一个\",{\"1\":{\"508\":1}}],[\"一个具有1750亿参数的自回归语言模型\",{\"1\":{\"459\":1}}],[\"一个问题\",{\"1\":{\"445\":1}}],[\"一个问题可以作用于多个物体类别\",{\"1\":{\"65\":1}}],[\"一个简单的技巧是在你的prompt后面\",{\"1\":{\"433\":1}}],[\"一个简单的线性层\",{\"1\":{\"226\":1}}],[\"一个可迭代对象\",{\"1\":{\"381\":1}}],[\"一个装饰器\",{\"1\":{\"375\":1}}],[\"一个装饰器的实现\",{\"1\":{\"367\":1}}],[\"一个例子帮助理解\",{\"1\":{\"355\":1}}],[\"一个block之后维度依然和输入相同\",{\"1\":{\"294\":1}}],[\"一个改进的想法就是把一张图片分成nxn个patch\",{\"1\":{\"291\":1}}],[\"一个批次的数据\",{\"1\":{\"289\":1}}],[\"一个文件夹对应一个类别\",{\"1\":{\"289\":1}}],[\"一个的轻量q\",{\"1\":{\"280\":1}}],[\"一个视觉模型和一个文本模型\",{\"1\":{\"273\":1}}],[\"一个one\",{\"1\":{\"240\":1}}],[\"一个很大的优势在于\",{\"1\":{\"237\":1}}],[\"一个是该\",{\"1\":{\"540\":1}}],[\"一个是叫灾难性遗忘\",{\"1\":{\"416\":1}}],[\"一个是训练的成本会比较高\",{\"1\":{\"416\":1}}],[\"一个是image\",{\"1\":{\"258\":1}}],[\"一个是保证字典内特征一致性\",{\"1\":{\"241\":1}}],[\"一个是构建很大的字典\",{\"1\":{\"241\":1}}],[\"一个是正常样本\",{\"1\":{\"240\":1}}],[\"一个是e12\",{\"1\":{\"238\":1}}],[\"一个是e11\",{\"1\":{\"238\":1}}],[\"一个是\",{\"1\":{\"235\":2}}],[\"一个最经典的代理任务就是\",{\"1\":{\"235\":1}}],[\"一个列表\",{\"1\":{\"100\":1}}],[\"一个非法索引\",{\"1\":{\"92\":1}}],[\"一个元组\",{\"1\":{\"92\":1,\"107\":1,\"109\":1}}],[\"一旦点云被划分成小的子集\",{\"1\":{\"86\":1}}],[\"一名普通但十分热爱探索技术的coder\",{\"1\":{\"2\":1}}],[\"联合关系\",{\"1\":{\"355\":1}}],[\"联合使用\",{\"1\":{\"228\":1}}],[\"联合\",{\"0\":{\"228\":1}}],[\"联合推理时\",{\"1\":{\"227\":1}}],[\"联合qllama二次编码视觉特征\",{\"1\":{\"188\":1}}],[\"联合训练三种任务\",{\"1\":{\"120\":1}}],[\"联合建模文本\",{\"1\":{\"178\":1}}],[\"联合建模\",{\"1\":{\"59\":1}}],[\"联合注意力生成对齐特征\",{\"1\":{\"54\":1}}],[\"联合区域对齐\",{\"1\":{\"54\":1}}],[\"意味着gpt\",{\"1\":{\"463\":1}}],[\"意味着从第\",{\"1\":{\"323\":2}}],[\"意味着相邻列的元素在内存中也是相邻的\",{\"1\":{\"323\":1}}],[\"意味着如果你从第\",{\"1\":{\"323\":1}}],[\"意味着在处理图像时\",{\"1\":{\"300\":1}}],[\"意味着动量分布和\",{\"1\":{\"159\":1}}],[\"意义\",{\"1\":{\"49\":1}}],[\"意图协同推理\",{\"1\":{\"7\":1}}],[\"场景文本\",{\"1\":{\"220\":1}}],[\"场景\",{\"1\":{\"56\":1,\"78\":1,\"355\":1,\"363\":1}}],[\"场景特征\",{\"1\":{\"54\":1}}],[\"场景的评估\",{\"1\":{\"49\":1}}],[\"场景交互以揭示功能\",{\"1\":{\"49\":1}}],[\"主因是书籍数据量不足\",{\"1\":{\"482\":1}}],[\"主是为了解决cot这种从易到难的迁移能力不足而诞生的\",{\"1\":{\"436\":1}}],[\"主对角线\",{\"1\":{\"359\":1}}],[\"主成分分析\",{\"1\":{\"355\":1}}],[\"主干部分全部冻结\",{\"1\":{\"300\":1}}],[\"主要由以下\",{\"1\":{\"683\":1}}],[\"主要特点包括\",{\"1\":{\"674\":1}}],[\"主要原因如下\",{\"1\":{\"657\":1}}],[\"主要输出项解释\",{\"1\":{\"540\":1}}],[\"主要来源包括\",{\"1\":{\"481\":1}}],[\"主要来自美国和东南亚\",{\"1\":{\"472\":1}}],[\"主要用于测试小型数据集的语言模型训练效果\",{\"1\":{\"510\":1}}],[\"主要用于冷启动训练\",{\"1\":{\"470\":1}}],[\"主要用于解决目标检测任务中前景\",{\"1\":{\"404\":1}}],[\"主要针对电影评论来做情感分类\",{\"1\":{\"448\":1}}],[\"主要有以下两个\",{\"1\":{\"416\":1}}],[\"主要变化方向\",{\"1\":{\"355\":1}}],[\"主要包括行优先和列优先两类\",{\"1\":{\"322\":1}}],[\"主要包含encoder和decoder结构\",{\"1\":{\"287\":1}}],[\"主要包含以下组件\",{\"1\":{\"214\":1}}],[\"主要的区别就是去掉了paddding\",{\"1\":{\"295\":1}}],[\"主要区别在于norm层的顺序\",{\"1\":{\"294\":1}}],[\"主要是因为这些方法难以实现较高的性能\",{\"1\":{\"278\":1}}],[\"主要步骤如下\",{\"1\":{\"216\":1}}],[\"主要采用固定分辨率\",{\"1\":{\"211\":1}}],[\"主要代表包括\",{\"1\":{\"210\":1}}],[\"主要体现在三个方面\",{\"1\":{\"208\":1}}],[\"主要作用是\",{\"1\":{\"100\":1}}],[\"主视觉特征\",{\"1\":{\"163\":1}}],[\"主网络进行\",{\"1\":{\"163\":1}}],[\"主模型参数\",{\"1\":{\"160\":1}}],[\"主模型的预测为\",{\"1\":{\"157\":1}}],[\"主模型被训练去匹配动量模型的预测\",{\"1\":{\"157\":1}}],[\"主流知识型模型对比\",{\"1\":{\"674\":1}}],[\"主流程\",{\"1\":{\"142\":1}}],[\"主流\",{\"1\":{\"122\":1}}],[\"主体\",{\"1\":{\"49\":1,\"54\":1,\"56\":1}}],[\"主页\",{\"0\":{\"0\":1}}],[\"盛放\",{\"1\":{\"49\":1}}],[\"握持\",{\"1\":{\"49\":1}}],[\"握柄倒水\",{\"1\":{\"6\":1}}],[\"既可\",{\"1\":{\"49\":1}}],[\"杯子\",{\"1\":{\"49\":1}}],[\"需在多样化的应用场景中保持高效和准确\",{\"1\":{\"679\":1}}],[\"需使用领域特定语言\",{\"1\":{\"662\":1}}],[\"需编写繁琐的函数调用\",{\"1\":{\"662\":1}}],[\"需包含\",{\"1\":{\"661\":1}}],[\"需处理左右操作数的除法运算\",{\"1\":{\"660\":1}}],[\"需分别实现\",{\"1\":{\"660\":1}}],[\"需通过弱引用解决\",{\"1\":{\"657\":1}}],[\"需通过交互上下文建模解决\",{\"1\":{\"49\":1}}],[\"需依赖垃圾回收机制处理\",{\"1\":{\"657\":1}}],[\"需从\",{\"1\":{\"494\":1}}],[\"需后处理\",{\"1\":{\"483\":1}}],[\"需后续治理\",{\"1\":{\"482\":1}}],[\"需约\",{\"1\":{\"472\":1}}],[\"需调参\",{\"1\":{\"404\":1}}],[\"需二值化\",{\"1\":{\"403\":1}}],[\"需正则化\",{\"1\":{\"395\":1}}],[\"需直接建模像素到类别的复杂映射\",{\"1\":{\"395\":1}}],[\"需数百个神经元拟合嵌套正弦波\",{\"1\":{\"395\":1}}],[\"需高阶泰勒展开\",{\"1\":{\"395\":1}}],[\"需训练整个模型\",{\"1\":{\"231\":1}}],[\"需搭配大规模视觉编码器\",{\"1\":{\"221\":1}}],[\"需先\",{\"1\":{\"82\":2}}],[\"需结合\",{\"1\":{\"82\":1,\"484\":1}}],[\"需严格的空间对应\",{\"1\":{\"52\":1}}],[\"需跨源对齐区域\",{\"1\":{\"49\":1}}],[\"需要进行切片\",{\"1\":{\"687\":1}}],[\"需要设计本应用所要提供的功能\",{\"1\":{\"687\":1}}],[\"需要首先构造训练集\",{\"1\":{\"686\":1}}],[\"需要额外的资源来支持检索机制和数据库的维护\",{\"1\":{\"681\":1}}],[\"需要谨慎\",{\"1\":{\"675\":1}}],[\"需要分别处理左右操作数\",{\"1\":{\"660\":1}}],[\"需要知道正向传播时的\",{\"1\":{\"630\":1}}],[\"需要给出\",{\"1\":{\"597\":1}}],[\"需要指定概率度量\",{\"1\":{\"565\":1}}],[\"需要对掩码数量不足max\",{\"1\":{\"514\":1}}],[\"需要对哪个物体做归一化呢\",{\"1\":{\"86\":1}}],[\"需要被掩码的词之上\",{\"1\":{\"512\":1}}],[\"需要准备好每一层\",{\"1\":{\"477\":1}}],[\"需要至少50个token的上下文理解\",{\"1\":{\"455\":1}}],[\"需要做些修改以便用在这些任务上\",{\"1\":{\"445\":1}}],[\"需要更长的反应时间\",{\"1\":{\"433\":1}}],[\"需要更多的训练数据\",{\"1\":{\"166\":1}}],[\"需要特别说明\",{\"1\":{\"414\":1}}],[\"需要管理员权限\",{\"1\":{\"379\":1}}],[\"需要跨越多少个内存元素\",{\"1\":{\"325\":1}}],[\"需要移动\",{\"1\":{\"323\":1}}],[\"需要再将第一个添加的class\",{\"1\":{\"296\":1}}],[\"需要单独将这个token再提取出来\",{\"1\":{\"292\":1}}],[\"需要相对少的数据就可以学习一个比较好的模型\",{\"1\":{\"287\":1}}],[\"需要复制图像特征以匹配beam数量\",{\"1\":{\"286\":1}}],[\"需要数百个gpu训练数十天\",{\"1\":{\"280\":1}}],[\"需要注意的是\",{\"1\":{\"596\":1}}],[\"需要注意的一点是\",{\"1\":{\"519\":1}}],[\"需要注意的一点\",{\"1\":{\"241\":1}}],[\"需要注意在对交互文本进行编码时\",{\"1\":{\"31\":1}}],[\"需要大量的计算资源进行训练和推理\",{\"1\":{\"675\":1}}],[\"需要大量的数据标注\",{\"1\":{\"278\":1}}],[\"需要大量神经元构造多个\",{\"1\":{\"395\":1}}],[\"需要大量高质量指令数据\",{\"1\":{\"231\":1}}],[\"需要大量人工或机器生成的\",{\"1\":{\"231\":1}}],[\"需要捕捉局部结构\",{\"1\":{\"104\":1}}],[\"需要确保这些划分具有一定的一致性或共同结构\",{\"1\":{\"86\":1}}],[\"需要先将\",{\"1\":{\"82\":1}}],[\"需要将输入图像调整为这个固定的尺寸\",{\"1\":{\"300\":1}}],[\"需要将\",{\"1\":{\"29\":1,\"546\":1}}],[\"首个版本于\",{\"1\":{\"674\":1}}],[\"首轮推理\",{\"1\":{\"477\":1}}],[\"首轮统计展示\",{\"1\":{\"410\":1}}],[\"首先要确定应用的核心功能\",{\"1\":{\"687\":1}}],[\"首先需要将非常复杂的业务逻辑依次拆解\",{\"1\":{\"686\":1}}],[\"首先需要对输入图片进行尺寸变化\",{\"1\":{\"290\":1}}],[\"首先在大规模文本数据上进行预训练\",{\"1\":{\"675\":1}}],[\"首先在输入句子的开头加一个代表分类的符号\",{\"1\":{\"508\":1}}],[\"首先求rosenbrock函数在处的导数和\",{\"1\":{\"667\":1}}],[\"首先调用a\",{\"1\":{\"660\":1}}],[\"首先模型会根据传入的tokens列表生成一个pad\",{\"1\":{\"517\":1}}],[\"首先加入特殊标记\",{\"1\":{\"511\":1}}],[\"首先是其初始化方法中需要完成\",{\"1\":{\"511\":1}}],[\"首先我们需要准备一个小型语料库\",{\"1\":{\"510\":1}}],[\"首先我们用data目录充当我们的图片库来源\",{\"1\":{\"276\":1}}],[\"首先将\",{\"1\":{\"512\":1}}],[\"首先将橙色和所有的黄色向量进行\",{\"1\":{\"508\":1}}],[\"首先将问题和文章通过\",{\"1\":{\"508\":1}}],[\"首先利用人工演示数据对gpt\",{\"1\":{\"467\":1}}],[\"首先说明\",{\"1\":{\"300\":1}}],[\"首先说对比学习想要做到什么呢\",{\"1\":{\"234\":1}}],[\"首先输入图片\",{\"1\":{\"286\":1}}],[\"首先从\",{\"1\":{\"240\":1}}],[\"首先通过\",{\"1\":{\"140\":1}}],[\"首先\",{\"1\":{\"97\":1,\"215\":1,\"238\":1,\"242\":1,\"273\":1,\"275\":1,\"420\":1,\"440\":1,\"449\":1,\"457\":1}}],[\"首先使用卷积层对输入图像进行处理\",{\"1\":{\"291\":1}}],[\"首先使用\",{\"1\":{\"8\":1}}],[\"首次引入的\",{\"1\":{\"676\":1}}],[\"首次发布\",{\"1\":{\"674\":1}}],[\"首次将深度学习的思想融入到语言模型中\",{\"1\":{\"673\":1}}],[\"首次实现了视觉与语言模型在参数和特征空间的深度协同\",{\"1\":{\"186\":1}}],[\"首次sample\",{\"1\":{\"93\":1}}],[\"首次通过非配对的2d\",{\"1\":{\"51\":1}}],[\"首次提出通过2d交互图像预测3d物体功能区域\",{\"1\":{\"49\":1}}],[\"坐标值\",{\"1\":{\"397\":1}}],[\"坐标时进行了两次量化\",{\"1\":{\"397\":1}}],[\"坐标信息\",{\"1\":{\"114\":1}}],[\"坐标\",{\"1\":{\"101\":1,\"114\":1,\"396\":1}}],[\"坐标与热力图形式的可供性值\",{\"1\":{\"18\":1}}],[\"坐\",{\"1\":{\"49\":1}}],[\"概念\",{\"1\":{\"49\":1}}],[\"概率论的核心是\",{\"1\":{\"597\":1}}],[\"概率论基础模型\",{\"0\":{\"595\":1},\"1\":{\"595\":1}}],[\"概率论基础概念\",{\"0\":{\"563\":1},\"1\":{\"563\":1}}],[\"概率论基础知识\",{\"0\":{\"562\":1}}],[\"概率公理\",{\"0\":{\"567\":1}}],[\"概率由积分给出\",{\"1\":{\"566\":1}}],[\"概率空间\",{\"0\":{\"564\":1}}],[\"概率保持不变\",{\"1\":{\"511\":2}}],[\"概率替换为随机词\",{\"1\":{\"511\":2}}],[\"概率替换为\",{\"1\":{\"511\":2}}],[\"概率\",{\"1\":{\"163\":1}}],[\"概率值\",{\"1\":{\"46\":1}}],[\"概率分布\",{\"1\":{\"40\":1,\"111\":1,\"566\":1}}],[\"验证迭代在大模型开发中是极其重要的一步\",{\"1\":{\"687\":1}}],[\"验证迭代\",{\"1\":{\"687\":1}}],[\"验证迭代优化\",{\"1\":{\"686\":1}}],[\"验证dezero对复杂表达式的计算图构建能力\",{\"1\":{\"666\":1}}],[\"验证安装\",{\"1\":{\"666\":1}}],[\"验证案例\",{\"1\":{\"658\":1}}],[\"验证函数的正向传播和反向传播\",{\"1\":{\"645\":1}}],[\"验证反向传播的正确性\",{\"1\":{\"623\":1,\"646\":1}}],[\"验证小模型+长训练的有效性\",{\"1\":{\"482\":1}}],[\"验证了更多数据能显著提升模型性能\",{\"1\":{\"494\":1}}],[\"验证了该技术在生产环境下的可行性和价值\",{\"1\":{\"472\":1}}],[\"验证了数据与模型规模对泛化能力的提升作用\",{\"1\":{\"178\":1}}],[\"验证了\",{\"1\":{\"132\":1,\"136\":1}}],[\"验证了任务设定的可行性和方法的有效性\",{\"1\":{\"48\":1}}],[\"验证集上准确率达到了98\",{\"1\":{\"300\":1}}],[\"验证集上进行评估的核心代码实现如下\",{\"1\":{\"82\":1}}],[\"验证集不需要进行数据增强\",{\"1\":{\"290\":1}}],[\"验证集的预处理转换操作\",{\"1\":{\"290\":1}}],[\"验证集\",{\"1\":{\"80\":1,\"686\":1}}],[\"验证\",{\"1\":{\"66\":1,\"68\":1,\"142\":1}}],[\"验证环境\",{\"1\":{\"29\":1}}],[\"建议伴随伦理审查\",{\"1\":{\"472\":1}}],[\"建议设为较高值\",{\"1\":{\"404\":1}}],[\"建议在\",{\"1\":{\"385\":1}}],[\"建议总是先激活\",{\"1\":{\"338\":1}}],[\"建议用linux或者windows系统进行测试\",{\"1\":{\"38\":1}}],[\"建立了测试机制\",{\"1\":{\"648\":1}}],[\"建立变量与函数的连接\",{\"0\":{\"634\":1}}],[\"建立更强的拒绝机制以识别恶意请求\",{\"1\":{\"472\":1}}],[\"建立\",{\"1\":{\"267\":1}}],[\"建立物体类型和功能类型的索引映射关系\",{\"1\":{\"68\":1}}],[\"建模交互上下文以明确功能区域\",{\"1\":{\"48\":1}}],[\"旨在帮助开发者提高应用程序的质量\",{\"1\":{\"685\":1}}],[\"旨在帮助开发者们快速构建基于大型语言模型的端到端应用程序或工作流程\",{\"1\":{\"682\":1}}],[\"旨在满足不同用户和应用场景的需求\",{\"1\":{\"674\":1}}],[\"旨在提升任务泛化能力\",{\"1\":{\"483\":1}}],[\"旨在将模型输出行为与人类意图对齐\",{\"1\":{\"469\":1}}],[\"旨在将图像中的每个像素分类为不同的语义类别\",{\"1\":{\"399\":1}}],[\"旨在缩小开源模型与商业多模态模型\",{\"1\":{\"207\":1}}],[\"旨在与大型语言模型\",{\"1\":{\"189\":1}}],[\"旨在解决当前视觉与视觉\",{\"1\":{\"180\":1}}],[\"旨在实现语言引导下的\",{\"1\":{\"69\":1}}],[\"旨在推动\",{\"1\":{\"60\":1}}],[\"旨在为具身智能体建立感知与操作之间的联系\",{\"1\":{\"48\":1}}],[\"旨在通过系统性的实验评估不同超参数和数据规模对模型性能的影响\",{\"1\":{\"492\":1}}],[\"旨在通过任意指令定位物体上支持特定交互的\",{\"1\":{\"6\":1}}],[\"旨在通过挖掘物体的不变几何属性和潜在交互意图\",{\"1\":{\"5\":1}}],[\"yellow\",{\"1\":{\"667\":1}}],[\"ys\",{\"1\":{\"651\":5,\"656\":5,\"658\":5}}],[\"y0\",{\"1\":{\"622\":2}}],[\"yn\",{\"1\":{\"414\":1}}],[\"y2\",{\"1\":{\"414\":1}}],[\"y1\",{\"1\":{\"414\":1,\"622\":2}}],[\"ylabel\",{\"1\":{\"289\":1,\"667\":1}}],[\"yl3800\",{\"1\":{\"60\":1}}],[\"y=v\",{\"1\":{\"289\":1}}],[\"yield\",{\"1\":{\"658\":1}}],[\"yi\",{\"1\":{\"215\":2}}],[\"y\",{\"1\":{\"93\":2,\"114\":2,\"352\":1,\"355\":1,\"382\":1,\"383\":1,\"384\":2,\"387\":1,\"404\":1,\"414\":1,\"470\":2,\"542\":1,\"596\":1,\"613\":2,\"617\":2,\"630\":2,\"632\":2,\"638\":2,\"645\":4,\"646\":2,\"651\":2,\"654\":3,\"655\":2,\"656\":2,\"658\":10,\"659\":6,\"660\":5,\"662\":26,\"666\":24,\"667\":15}}],[\"yanx27\",{\"1\":{\"85\":1}}],[\"yaml\",{\"1\":{\"83\":3}}],[\"yawen\",{\"1\":{\"4\":1}}],[\"you\",{\"1\":{\"70\":1}}],[\"yyvhang\",{\"1\":{\"47\":1}}],[\"做的就是这个定位任务\",{\"1\":{\"542\":1}}],[\"做分类\",{\"1\":{\"508\":1,\"542\":1}}],[\"做为最终的答案\",{\"1\":{\"435\":1}}],[\"做简单的矩阵加法即可\",{\"1\":{\"420\":1}}],[\"做了大量的简化\",{\"1\":{\"414\":1}}],[\"做了个近似\",{\"1\":{\"240\":1}}],[\"做attention\",{\"1\":{\"284\":1}}],[\"做\",{\"1\":{\"283\":1}}],[\"做到一个近似的全部数据集的多分类\",{\"1\":{\"242\":1}}],[\"做判断\",{\"1\":{\"228\":1}}],[\"做掩码操作\",{\"1\":{\"76\":1}}],[\"做全局池化\",{\"1\":{\"46\":1}}],[\"做回答\",{\"1\":{\"28\":1}}],[\"执行项目初始化\",{\"1\":{\"685\":1}}],[\"执行具体行动计划至关重要\",{\"1\":{\"684\":1}}],[\"执行掩码的词的原token列表\",{\"1\":{\"512\":2}}],[\"执行掩码的词的位置列表\",{\"1\":{\"512\":2}}],[\"执行bert的掩码策略\",{\"1\":{\"511\":1}}],[\"执行以下任务\",{\"1\":{\"470\":1}}],[\"执行微调训练\",{\"1\":{\"423\":1}}],[\"执行顺序\",{\"1\":{\"371\":1}}],[\"执行复杂的视觉推理任务\",{\"1\":{\"227\":1}}],[\"执行语言建模训练\",{\"1\":{\"142\":1}}],[\"执行一次训练\",{\"1\":{\"142\":1}}],[\"执行一个\",{\"1\":{\"142\":1}}],[\"执行多轮训练\",{\"1\":{\"142\":1}}],[\"执行预测和可视化\",{\"1\":{\"83\":1}}],[\"执行\",{\"1\":{\"46\":1,\"76\":1,\"323\":1,\"383\":1,\"471\":1,\"666\":1}}],[\"执行交叉注意力机制\",{\"1\":{\"45\":1}}],[\"全新的范式\",{\"1\":{\"677\":1}}],[\"全能\",{\"1\":{\"674\":1}}],[\"全面扩展\",{\"1\":{\"650\":1}}],[\"全灰\",{\"1\":{\"594\":1}}],[\"全概率公式\",{\"0\":{\"569\":1},\"1\":{\"569\":1}}],[\"全连接再加上一个softmax\",{\"1\":{\"550\":1}}],[\"全连接层预测变换矩阵\",{\"1\":{\"107\":1}}],[\"全部从维基百科的\",{\"1\":{\"510\":1}}],[\"全部为1\",{\"1\":{\"161\":1}}],[\"全\",{\"1\":{\"506\":2}}],[\"全称为\",{\"1\":{\"504\":1}}],[\"全1\",{\"1\":{\"286\":1}}],[\"全模型微调\",{\"1\":{\"219\":1}}],[\"全参数调优\",{\"1\":{\"205\":1}}],[\"全量微调\",{\"1\":{\"193\":1}}],[\"全局性\",{\"1\":{\"395\":1}}],[\"全局逼近\",{\"1\":{\"395\":1}}],[\"全局信息\",{\"1\":{\"111\":1}}],[\"全局信息融合机制\",{\"1\":{\"105\":1}}],[\"全局特征表示\",{\"1\":{\"292\":1}}],[\"全局特征不能很好地反映每个点的上下文\",{\"1\":{\"112\":1}}],[\"全局特征不能直接用于分割\",{\"1\":{\"111\":1}}],[\"全局特征只有一份\",{\"1\":{\"111\":1}}],[\"全局特征\",{\"1\":{\"109\":1}}],[\"全局特征开关\",{\"1\":{\"109\":1}}],[\"全局特征都不一样了\",{\"1\":{\"86\":1}}],[\"全局最大池化\",{\"1\":{\"107\":1}}],[\"全局质心点\",{\"1\":{\"92\":1}}],[\"全局平均池化层\",{\"1\":{\"46\":1}}],[\"全为\",{\"1\":{\"43\":1,\"143\":1,\"285\":1}}],[\"数值微分的问题\",{\"0\":{\"623\":1}}],[\"数值微分的实现\",{\"0\":{\"621\":1}}],[\"数值微分\",{\"0\":{\"619\":1}}],[\"数值\",{\"1\":{\"565\":1}}],[\"数值可以是\",{\"1\":{\"64\":1}}],[\"数字拆分为独立字符\",{\"1\":{\"481\":1}}],[\"数理一致性等方面\",{\"1\":{\"463\":1}}],[\"数学解释\",{\"0\":{\"593\":1}}],[\"数学期望与方差\",{\"0\":{\"581\":1}}],[\"数学\",{\"1\":{\"483\":1}}],[\"数学等专业任务中\",{\"1\":{\"482\":1}}],[\"数学能力\",{\"1\":{\"482\":1}}],[\"数学本质对比\",{\"1\":{\"395\":1}}],[\"数学知识点\",{\"0\":{\"354\":1},\"1\":{\"354\":1}}],[\"数学推理\",{\"1\":{\"220\":1}}],[\"数学定义\",{\"1\":{\"115\":1,\"355\":1,\"407\":1}}],[\"数下\",{\"1\":{\"136\":1}}],[\"数组混合运算\",{\"1\":{\"663\":1}}],[\"数组转换为\",{\"1\":{\"290\":2}}],[\"数组后\",{\"1\":{\"275\":1}}],[\"数组格式返回\",{\"1\":{\"275\":1}}],[\"数组\",{\"1\":{\"83\":1,\"323\":1,\"327\":2}}],[\"数量一致\",{\"1\":{\"309\":1}}],[\"数量有限\",{\"1\":{\"128\":1}}],[\"数量\",{\"1\":{\"46\":2,\"143\":1,\"291\":1}}],[\"数据工程\",{\"1\":{\"686\":2}}],[\"数据连接\",{\"1\":{\"683\":1,\"684\":1}}],[\"数据存储等等\",{\"1\":{\"682\":1}}],[\"数据存储在data属性中\",{\"1\":{\"609\":1}}],[\"数据处理\",{\"1\":{\"681\":1}}],[\"数据处理阶段\",{\"1\":{\"680\":1}}],[\"数据分析与可视化\",{\"1\":{\"674\":1}}],[\"数据分析和第三方服务调用\",{\"1\":{\"674\":1}}],[\"数据加载器与数据集支持\",{\"1\":{\"670\":1}}],[\"数据类型检查\",{\"0\":{\"643\":1}}],[\"数据预加载\",{\"1\":{\"511\":1}}],[\"数据预处理一般包括从多种格式向纯文本的转化\",{\"1\":{\"687\":1}}],[\"数据预处理\",{\"0\":{\"520\":1},\"1\":{\"83\":1,\"410\":1}}],[\"数据清洗\",{\"0\":{\"510\":1}}],[\"数据清洗与分词\",{\"1\":{\"447\":1}}],[\"数据量增加后\",{\"1\":{\"498\":1}}],[\"数据量和计算量按幂律缩放\",{\"1\":{\"464\":1}}],[\"数据规模和模型设置\",{\"1\":{\"496\":1}}],[\"数据复制增加了存储和计算开销\",{\"1\":{\"495\":1}}],[\"数据复制\",{\"1\":{\"495\":1}}],[\"数据配置\",{\"1\":{\"494\":1}}],[\"数据根源\",{\"1\":{\"484\":1}}],[\"数据不透明\",{\"1\":{\"483\":1}}],[\"数据策略与开源兼容性\",{\"1\":{\"480\":1}}],[\"数据在训练中并非按体量采样\",{\"1\":{\"461\":1}}],[\"数据在底层存储为\",{\"1\":{\"326\":1}}],[\"数据质量\",{\"1\":{\"455\":1}}],[\"数据质量差\",{\"1\":{\"120\":1}}],[\"数据重叠检测\",{\"1\":{\"455\":1}}],[\"数据与架构创新\",{\"1\":{\"454\":1}}],[\"数据与规模的重要性\",{\"1\":{\"178\":1}}],[\"数据以避免测试集污染\",{\"1\":{\"454\":1}}],[\"数据来源\",{\"1\":{\"454\":1}}],[\"数据来源包括\",{\"1\":{\"224\":1}}],[\"数据安全的问题\",{\"1\":{\"415\":1}}],[\"数据可能沿某个方向拉长\",{\"1\":{\"359\":1}}],[\"数据内容\",{\"1\":{\"327\":1}}],[\"数据指针偏移\",{\"1\":{\"325\":1}}],[\"数据还是指向内存中已有的数据\",{\"1\":{\"325\":1}}],[\"数据元素在内存空间的排列顺序\",{\"1\":{\"322\":1}}],[\"数据的分布更加稀疏\",{\"1\":{\"294\":1}}],[\"数据下载\",{\"0\":{\"289\":1}}],[\"数据会出去\",{\"1\":{\"241\":1}}],[\"数据依赖性强\",{\"1\":{\"231\":1}}],[\"数据需求高\",{\"1\":{\"460\":1}}],[\"数据需求\",{\"1\":{\"231\":1}}],[\"数据均匀采样\",{\"1\":{\"227\":1}}],[\"数据构建方式\",{\"1\":{\"226\":1}}],[\"数据从头训练一个新模型\",{\"1\":{\"137\":1}}],[\"数据训练\",{\"1\":{\"137\":1,\"202\":1}}],[\"数据\",{\"1\":{\"131\":1,\"190\":3,\"493\":1,\"494\":1}}],[\"数据增强和更强的视觉编码器\",{\"1\":{\"212\":1}}],[\"数据增强包括随机裁剪和翻转\",{\"1\":{\"204\":1}}],[\"数据增强\",{\"0\":{\"124\":1}}],[\"数据增强与配对策略\",{\"0\":{\"66\":1}}],[\"数据自举方法\",{\"1\":{\"120\":1}}],[\"数据路径设置\",{\"1\":{\"83\":1}}],[\"数据组织形式\",{\"1\":{\"68\":1}}],[\"数据总量\",{\"1\":{\"65\":1}}],[\"数据划分方式\",{\"1\":{\"65\":1}}],[\"数据划分\",{\"0\":{\"20\":1}}],[\"数据收集问题\",{\"1\":{\"472\":1}}],[\"数据收集\",{\"0\":{\"17\":1},\"1\":{\"472\":1}}],[\"数据集预处理完后\",{\"1\":{\"520\":1}}],[\"数据集预训练模型\",{\"1\":{\"447\":1}}],[\"数据集评估\",{\"1\":{\"470\":1}}],[\"数据集构建\",{\"1\":{\"470\":1}}],[\"数据集构成与过滤策略\",{\"1\":{\"461\":1}}],[\"数据集级别\",{\"1\":{\"404\":1}}],[\"数据集加载代码\",{\"1\":{\"289\":1}}],[\"数据集下载\",{\"1\":{\"289\":1}}],[\"数据集训练的检测模型\",{\"1\":{\"253\":1}}],[\"数据集进行训练\",{\"1\":{\"227\":1}}],[\"数据集进行微调\",{\"1\":{\"142\":1,\"145\":1}}],[\"数据集处理方面\",{\"1\":{\"145\":1}}],[\"数据集上的性能下降\",{\"1\":{\"472\":1}}],[\"数据集上的实验结果\",{\"1\":{\"471\":1}}],[\"数据集上取得了很好的结果\",{\"1\":{\"239\":1}}],[\"数据集上\",{\"1\":{\"237\":1,\"471\":1}}],[\"数据集上进行预训练的\",{\"1\":{\"300\":1}}],[\"数据集上进行迁移学习\",{\"1\":{\"227\":1}}],[\"数据集上进行训练后\",{\"1\":{\"143\":1}}],[\"数据集上开展了系统性的实验评估\",{\"1\":{\"21\":1}}],[\"数据集的\",{\"1\":{\"228\":1}}],[\"数据集的训练\",{\"1\":{\"142\":1}}],[\"数据集的初始化\",{\"1\":{\"29\":1}}],[\"数据集应当用于重新训练一个新模型\",{\"1\":{\"137\":1}}],[\"数据集重新训练模型\",{\"0\":{\"137\":1}}],[\"数据集一致\",{\"1\":{\"136\":1}}],[\"数据集中每一个样本最终都会解析得到一个inputfeatures\",{\"1\":{\"520\":1}}],[\"数据集中图像的数量\",{\"1\":{\"289\":1}}],[\"数据集中\",{\"1\":{\"78\":1}}],[\"数据集类型\",{\"1\":{\"68\":1}}],[\"数据集存放目录\",{\"1\":{\"68\":1}}],[\"数据集统计信息\",{\"0\":{\"67\":1}}],[\"数据集设置\",{\"1\":{\"65\":1}}],[\"数据集组织方式\",{\"0\":{\"65\":1}}],[\"数据集基于\",{\"1\":{\"62\":1,\"69\":1}}],[\"数据集初始化的核心代码实现如下\",{\"1\":{\"68\":1}}],[\"数据集初始化\",{\"1\":{\"58\":1}}],[\"数据集目录下的组织方式\",{\"1\":{\"58\":1}}],[\"数据集贡献\",{\"1\":{\"6\":1}}],[\"数据集\",{\"0\":{\"16\":1,\"29\":1,\"58\":1,\"61\":1},\"1\":{\"4\":1,\"47\":1,\"138\":1,\"202\":1,\"227\":1,\"280\":1,\"469\":1,\"470\":3,\"471\":1}}],[\"热图回归损失\",{\"1\":{\"55\":1}}],[\"热图\",{\"1\":{\"46\":1}}],[\"它让计算机更好地理解和使用语言\",{\"1\":{\"678\":1}}],[\"它能够让语言模型与其他数据来源连接\",{\"1\":{\"682\":1}}],[\"它能够成功的两个关键点\",{\"1\":{\"674\":1}}],[\"它能够从每个子集中提取有用的信息或特征\",{\"1\":{\"86\":1}}],[\"它也总是会沿着某个分支进行dfs直到\",{\"1\":{\"655\":1}}],[\"它也倾向于过度规避风险\",{\"1\":{\"471\":1}}],[\"它描述了在观测到\",{\"1\":{\"597\":1}}],[\"它提供了一种根据观测数据\",{\"1\":{\"572\":1}}],[\"它从一些基本的\",{\"1\":{\"566\":1}}],[\"它必须满足以下三条规则\",{\"1\":{\"566\":1}}],[\"它必须要在超大数据集上进行预训练\",{\"1\":{\"297\":1}}],[\"它本身不\",{\"1\":{\"565\":1}}],[\"它本身不随机\",{\"1\":{\"565\":1}}],[\"它允许模型在处理每个词时关注输入序列中的所有词\",{\"1\":{\"548\":1}}],[\"它允许不同形状的张量进行逐元素\",{\"1\":{\"327\":1}}],[\"它较难学习到长距离的依赖关系\",{\"1\":{\"547\":1}}],[\"它在小型模型中不明显\",{\"1\":{\"676\":1}}],[\"它在编码每一词的时候都能够注意\",{\"1\":{\"547\":1}}],[\"它在技术上并非从零出发\",{\"1\":{\"464\":1}}],[\"它在某种程度上也具备一定的多语言能力\",{\"1\":{\"464\":1}}],[\"它在这个维度上的偏差也会被直接算入距离\",{\"1\":{\"357\":1}}],[\"它面临的问题包括任务适应不均\",{\"1\":{\"463\":1}}],[\"它验证了单一模型架构通过规模扩展即可实现多任务处理的可能性\",{\"1\":{\"457\":1}}],[\"它被证明在许多任务上有很强的表现\",{\"1\":{\"440\":1}}],[\"它得到的结果是错的\",{\"1\":{\"435\":1}}],[\"它并不是一个通用智能系统\",{\"1\":{\"463\":1}}],[\"它并不保证结果的合理性和正确性\",{\"1\":{\"430\":1}}],[\"它并不像单词那样有很强的语义信息\",{\"1\":{\"238\":1}}],[\"它只是在给定的信息的前提下\",{\"1\":{\"430\":1}}],[\"它基于jaccard指数\",{\"1\":{\"403\":1}}],[\"它基于前面的特征提取模块\",{\"1\":{\"110\":1,\"111\":1}}],[\"它来源于\",{\"1\":{\"401\":1}}],[\"它离\",{\"1\":{\"397\":4}}],[\"它会把一个张量\",{\"1\":{\"389\":1}}],[\"它会复制数据\",{\"1\":{\"386\":1}}],[\"它会将多个样本收集起来形成一个批次\",{\"1\":{\"289\":1}}],[\"它会将图像切分为\",{\"1\":{\"126\":1}}],[\"它接收一个函数或类作为参数\",{\"1\":{\"368\":1}}],[\"它接收来自编码器和解码器的特征\",{\"1\":{\"46\":1}}],[\"它\",{\"1\":{\"366\":1}}],[\"它包含了每个特征的方差\",{\"1\":{\"359\":1}}],[\"它包含了这个区域内所有点的信息\",{\"1\":{\"92\":1}}],[\"它直接对每个维度的偏差做平方加总\",{\"1\":{\"359\":1}}],[\"它对每个维度的偏差一视同仁\",{\"1\":{\"357\":1}}],[\"它就是我们平时量两个点之间\",{\"1\":{\"357\":1}}],[\"它表示我们在看到数据\",{\"1\":{\"596\":1}}],[\"它表示\",{\"1\":{\"355\":1}}],[\"它可以改进搜索引擎\",{\"1\":{\"678\":1}}],[\"它可以帮助计算机更好地理解和生成文本\",{\"1\":{\"678\":1}}],[\"它可以当成函数调用\",{\"1\":{\"552\":1}}],[\"它可以在指定维度上\",{\"1\":{\"514\":1}}],[\"它可以以任意的精度来近似任何一个定义在实数空间中的有界闭集函数\",{\"1\":{\"395\":1}}],[\"它可以拥有任意数量的维度\",{\"1\":{\"321\":1}}],[\"它可按下式计算\",{\"1\":{\"348\":1}}],[\"它位于最终分类头之前\",{\"1\":{\"296\":1}}],[\"它与类的实例和类本身都没有直接关联\",{\"1\":{\"289\":1}}],[\"它首次将nlp领域火热的transformer模型架构移植到了cv领域\",{\"1\":{\"287\":1}}],[\"它将权重矩阵量化为\",{\"1\":{\"428\":1}}],[\"它将\",{\"1\":{\"407\":1,\"656\":1}}],[\"它将每个\",{\"1\":{\"285\":1}}],[\"它将输入的向量转换为\",{\"1\":{\"275\":1}}],[\"它由一些基本区间\",{\"1\":{\"566\":1}}],[\"它由18层组成\",{\"1\":{\"392\":1}}],[\"它由两个部分组成\",{\"1\":{\"273\":1}}],[\"它由大量带有位置信息的点组成\",{\"1\":{\"114\":1}}],[\"它比谷歌的jft\",{\"1\":{\"272\":1}}],[\"它代表着一种基于对比文本\",{\"1\":{\"271\":1}}],[\"它通过位置编码将序列中词的位置信息注入到输入中\",{\"1\":{\"548\":1}}],[\"它通过一组非线性基函数\",{\"1\":{\"395\":1}}],[\"它通过广播\",{\"1\":{\"387\":1}}],[\"它通过处理器对输入文本进行处理\",{\"1\":{\"275\":1}}],[\"它通过学习一个相似度函数\",{\"1\":{\"154\":1}}],[\"它通常用于像\",{\"1\":{\"268\":1}}],[\"它非常灵活\",{\"1\":{\"238\":1}}],[\"它们仍可能无法提供准确的答案\",{\"1\":{\"679\":1}}],[\"它们为自然语言理解和生成任务提供了强大的工具\",{\"1\":{\"675\":1}}],[\"它们的多语言能力使得跨文化和跨语言的应用变得更加容易\",{\"1\":{\"675\":1}}],[\"它们的能力依次递增\",{\"1\":{\"674\":1}}],[\"它们的平方距离越来越集中在\",{\"1\":{\"593\":1}}],[\"它们的出现会导致比独立情形更大的方差\",{\"1\":{\"582\":1}}],[\"它们都是在数万亿个字符上训练的\",{\"1\":{\"674\":1}}],[\"它们都是按照其序列长度的20\",{\"1\":{\"514\":1}}],[\"它们都是被过度参数化的\",{\"1\":{\"420\":1}}],[\"它们在海量的文本数据上进行训练\",{\"1\":{\"673\":1}}],[\"它们在\",{\"1\":{\"510\":1}}],[\"它们有更小的内在维度\",{\"1\":{\"424\":1}}],[\"它们分布在\",{\"1\":{\"397\":1}}],[\"它们是离散的信号\",{\"1\":{\"238\":1}}],[\"它们共同帮助我们判断模型是否真正理解语言引导下的功能区域语义\",{\"1\":{\"82\":1}}],[\"它引入了局部区域搜索\",{\"1\":{\"112\":1}}],[\"它使用双线性插值来避免量化误差\",{\"1\":{\"396\":1}}],[\"它使用\",{\"1\":{\"110\":2}}],[\"它负责从输入点云中提取出可用于分类或分割的特征\",{\"1\":{\"109\":1}}],[\"它具有以下特点\",{\"1\":{\"107\":1}}],[\"它是用于执行bert\",{\"1\":{\"511\":1}}],[\"它是在lora的基础上\",{\"1\":{\"421\":1}}],[\"它是一种用于度量集合之间相似性的指标\",{\"1\":{\"405\":1}}],[\"它是一个用于计算在给定观测数据\",{\"1\":{\"596\":1}}],[\"它是一个函数\",{\"1\":{\"565\":1}}],[\"它是一个指向内存中某个元素的指针偏移量\",{\"1\":{\"325\":1}}],[\"它是一个非常强大且灵活的张量操作函数\",{\"1\":{\"76\":1}}],[\"它是\",{\"1\":{\"297\":1,\"673\":1}}],[\"它是无序点云数据特征提取的高效算法\",{\"1\":{\"86\":1}}],[\"它常用于图像检索\",{\"1\":{\"82\":1}}],[\"它结合了两种\",{\"1\":{\"402\":1}}],[\"它结合了\",{\"1\":{\"78\":1}}],[\"它的尾部非常厚重\",{\"1\":{\"587\":1}}],[\"它的概率密度函数全部集中在正实数轴上\",{\"1\":{\"587\":1}}],[\"它的概率密度函数\",{\"1\":{\"586\":1,\"587\":1}}],[\"它的定义是\",{\"1\":{\"566\":1}}],[\"它的样本空间是连续的\",{\"1\":{\"566\":1}}],[\"它的语法是\",{\"1\":{\"541\":1}}],[\"它的应用自不必提\",{\"1\":{\"428\":1}}],[\"它的更新可表示为\",{\"1\":{\"425\":1}}],[\"它的结构包括一个输入层\",{\"1\":{\"392\":1}}],[\"它的名字和文档字符串也被覆盖了\",{\"1\":{\"372\":1}}],[\"它的作用是给定一个完整的句子\",{\"1\":{\"542\":1}}],[\"它的作用是对一个样本进行两次增强\",{\"1\":{\"244\":1}}],[\"它的作用是负责从输入的点云数据中采样关键点\",{\"1\":{\"92\":1}}],[\"它的基本思想是把一个\",{\"1\":{\"240\":1}}],[\"它的目标是为各种大型语言模型应用提供通用接口\",{\"1\":{\"682\":1}}],[\"它的目标是在一个由一个正样本和多个负样本构成的集合中\",{\"1\":{\"240\":1}}],[\"它的目标是\",{\"1\":{\"107\":1}}],[\"它的核心思想是通过不断合并最常见的字符对来构建一个高效的词汇表\",{\"1\":{\"409\":1}}],[\"它的核心思想是\",{\"1\":{\"92\":1}}],[\"它的本质是\",{\"1\":{\"76\":1}}],[\"它的主要目标是\",{\"1\":{\"76\":1}}],[\"它不仅是\",{\"1\":{\"665\":1}}],[\"它不仅考虑两个点之间的差异\",{\"1\":{\"358\":1}}],[\"它不具有生成能力\",{\"1\":{\"542\":1}}],[\"它不会\",{\"1\":{\"542\":1}}],[\"它不是单纯地\",{\"1\":{\"224\":1}}],[\"它不是直接包含原始图像\",{\"1\":{\"41\":1}}],[\"它不使用任何注意力机制\",{\"1\":{\"73\":1}}],[\"→68\",{\"1\":{\"483\":1}}],[\"→\",{\"0\":{\"310\":1},\"1\":{\"45\":3,\"46\":4,\"82\":1,\"98\":1,\"99\":9,\"101\":4,\"105\":4,\"107\":4,\"147\":8,\"159\":3,\"160\":5,\"216\":1,\"319\":4,\"355\":3,\"387\":4,\"396\":1,\"404\":4,\"405\":4,\"511\":1,\"566\":4,\"596\":1}}],[\"指导大型语言模型生成更为精准的答案\",{\"1\":{\"679\":1}}],[\"指导抓取和运动规划\",{\"1\":{\"51\":1}}],[\"指代准确率81\",{\"1\":{\"484\":1}}],[\"指的就是上述公式中的矩阵w\",{\"1\":{\"414\":1}}],[\"指的是输入图像的尺寸为\",{\"1\":{\"300\":1}}],[\"指数中的表达式\",{\"1\":{\"590\":1}}],[\"指数级表达能力\",{\"1\":{\"395\":1}}],[\"指数增长\",{\"1\":{\"395\":2}}],[\"指南\",{\"1\":{\"347\":1}}],[\"指向\",{\"1\":{\"325\":1}}],[\"指明在每个维度上移动一个索引单位时\",{\"1\":{\"325\":1}}],[\"指明哪些位置是有效的\",{\"1\":{\"285\":1}}],[\"指从一个大的数据结构中提取出一部分连续的数据子集\",{\"1\":{\"325\":1}}],[\"指令遵循与文本生成提供了复杂业务逻辑的简单平替方案\",{\"1\":{\"686\":1}}],[\"指令遵循\",{\"1\":{\"676\":1}}],[\"指令跟随能力\",{\"1\":{\"472\":1}}],[\"指令的顺从\",{\"1\":{\"471\":1}}],[\"指令中的显式约束\",{\"1\":{\"471\":1}}],[\"指令\",{\"1\":{\"224\":2,\"226\":2,\"231\":2}}],[\"指令调优\",{\"1\":{\"224\":1,\"231\":2}}],[\"指令微调\",{\"0\":{\"483\":1},\"1\":{\"197\":1,\"482\":1,\"676\":1}}],[\"指令理解特征\",{\"1\":{\"45\":1}}],[\"指标的选择和权衡\",{\"0\":{\"347\":1}}],[\"指标\",{\"1\":{\"82\":2,\"347\":1,\"403\":1}}],[\"指定的列\",{\"1\":{\"68\":1}}],[\"指出图像中物体与人交互的部分\",{\"1\":{\"11\":1}}],[\"作用是\",{\"1\":{\"372\":1}}],[\"作用\",{\"1\":{\"267\":1,\"404\":1}}],[\"作用于点云特征图\",{\"1\":{\"76\":1}}],[\"作为大模型开发的初学者\",{\"1\":{\"686\":1}}],[\"作为一个不断进化的创新平台\",{\"1\":{\"684\":1}}],[\"作为一个大语言模型开发框架\",{\"1\":{\"683\":1}}],[\"作为一个指标就没有那么有意义和实用\",{\"1\":{\"345\":1}}],[\"作为节点唯一id\",{\"1\":{\"666\":1}}],[\"作为划分事件\",{\"1\":{\"569\":1}}],[\"作为答案开始的可能性\",{\"1\":{\"540\":1}}],[\"作为整个输入序列的全局信息聚合表示\",{\"1\":{\"513\":1}}],[\"作为整个点云的\",{\"1\":{\"107\":1,\"109\":1}}],[\"作为最终的\",{\"1\":{\"508\":1}}],[\"作为奖励函数\",{\"1\":{\"470\":1}}],[\"作为激活函数\",{\"1\":{\"447\":1}}],[\"作为分隔符\",{\"1\":{\"432\":1}}],[\"作为分词时的合并规则和优先选择权\",{\"1\":{\"410\":1}}],[\"作为模型的缓冲区注册\",{\"1\":{\"389\":1}}],[\"作为平衡数据集的模型训练进度\",{\"1\":{\"347\":1}}],[\"作为特征提取器\",{\"1\":{\"299\":1}}],[\"作为特征提取器的方式差不多\",{\"1\":{\"237\":1}}],[\"作为文本解码器的初始状态\",{\"1\":{\"285\":1}}],[\"作为图像的分类预测结果\",{\"1\":{\"273\":1}}],[\"作为图像编码器\",{\"1\":{\"126\":1}}],[\"作为视觉与llms之间的\",{\"1\":{\"188\":1}}],[\"作为编码器\",{\"1\":{\"175\":1,\"247\":1,\"299\":1}}],[\"作为骨干网络\",{\"1\":{\"171\":1}}],[\"作为结束标记\",{\"1\":{\"143\":1}}],[\"作为对称函数\",{\"1\":{\"105\":1}}],[\"作为初始输入\",{\"1\":{\"76\":1}}],[\"作为第二个\",{\"1\":{\"45\":1}}],[\"作为第一个\",{\"1\":{\"45\":1}}],[\"作为\",{\"0\":{\"605\":1},\"1\":{\"45\":1,\"143\":2,\"162\":1,\"163\":2,\"196\":2,\"242\":1,\"282\":1}}],[\"作者强调\",{\"1\":{\"472\":1,\"501\":1}}],[\"作者列出多个值得进一步研究的问题\",{\"1\":{\"472\":1}}],[\"作者建议未来采用更多元标注\",{\"1\":{\"472\":1}}],[\"作者清晰指出当前模型对齐行为的\",{\"1\":{\"472\":1}}],[\"作者采用了\",{\"1\":{\"468\":1}}],[\"作者称这些过程使得模型输出更符合人类偏好\",{\"1\":{\"468\":1}}],[\"作者借鉴了\",{\"1\":{\"464\":1}}],[\"作者首先展示了8个不同规模的模型在训练过程中的表现\",{\"1\":{\"462\":1}}],[\"作者首先定义了语言模型执行任务的四种方式\",{\"1\":{\"461\":1}}],[\"作者对common\",{\"1\":{\"461\":1}}],[\"作者训练了从125m到175b参数的8个模型\",{\"1\":{\"461\":1}}],[\"作者训练流程有两个阶段\",{\"1\":{\"442\":1}}],[\"作者开放了模型代码和小型预训练模型\",{\"1\":{\"457\":1}}],[\"作者同时指出gpt\",{\"1\":{\"456\":1}}],[\"作者归因于该数据集的句子级打乱破坏了长程依赖\",{\"1\":{\"455\":1}}],[\"作者特别指出\",{\"1\":{\"455\":1}}],[\"作者指出\",{\"1\":{\"453\":1,\"454\":1,\"455\":1,\"463\":1,\"465\":1,\"468\":1}}],[\"作者的工作表明\",{\"1\":{\"450\":1}}],[\"作者模型获得了重要的世界知识和处理长距离依赖的能力\",{\"1\":{\"450\":1}}],[\"作者介绍了一种框架\",{\"1\":{\"450\":1}}],[\"作者观察标准结果\",{\"1\":{\"449\":1}}],[\"作者修改输入序列来包含\",{\"1\":{\"445\":1}}],[\"作者额外要微调的参数只有\",{\"1\":{\"444\":1}}],[\"作者用以下优化\",{\"1\":{\"444\":1}}],[\"作者用无监督的预训练和监督的微调组合来探索关于语言理解任务半监督方法\",{\"1\":{\"440\":1}}],[\"作者通过实验验证这一假设\",{\"1\":{\"453\":1}}],[\"作者通过fps来抽样点集中较为重要的点\",{\"1\":{\"89\":1}}],[\"作者通用的任务未知task\",{\"1\":{\"440\":1}}],[\"作者利用源于遍历式\",{\"1\":{\"440\":1}}],[\"作者假设采用一个大型无标记文本语料库和几个人工标记训练样本的数据集\",{\"1\":{\"440\":1}}],[\"作者证明通过在丰富的无标签文本语料库生成预训练generative\",{\"1\":{\"439\":1}}],[\"作者又探索了一种混合模型\",{\"1\":{\"299\":1}}],[\"作者先是在imagenet21k上进行预训练\",{\"1\":{\"296\":1}}],[\"作者随后也对一维位置编码的结果进行了可视化\",{\"1\":{\"293\":1}}],[\"作者最终选择了对比学习方法来进行训练\",{\"1\":{\"278\":1}}],[\"作者提出一种通过人类反馈对模型进行微调的方法\",{\"1\":{\"467\":1}}],[\"作者提出\",{\"1\":{\"453\":1}}],[\"作者提出的vilt可以认为是目前最简单的多模态transformer方法\",{\"1\":{\"257\":1}}],[\"作者提出的vilt属于\",{\"1\":{\"255\":1}}],[\"作者提出这4种类型的主要依据有两点\",{\"1\":{\"255\":1}}],[\"作者提出了一个新的数据处理流程\",{\"1\":{\"128\":1}}],[\"作者提出了一个多任务模型架构\",{\"1\":{\"126\":1}}],[\"作者提出了一个统一的视觉语言预训练框架\",{\"1\":{\"125\":1}}],[\"作者说到\",{\"1\":{\"242\":1}}],[\"作者认为\",{\"1\":{\"453\":1,\"463\":1}}],[\"作者认为大规模数据没有被充分利用\",{\"1\":{\"239\":1}}],[\"作者认为这是因为cv领域和nlp领域的原始信号空间不同\",{\"1\":{\"238\":1}}],[\"作者认为编码和解码之间的主要差异体现在\",{\"1\":{\"126\":1}}],[\"作者还发现加入语言模型作为辅助目标来微调有助于学习\",{\"1\":{\"444\":1}}],[\"作者还提出了一种创新方法\",{\"1\":{\"228\":1}}],[\"作者还构建了一个包含图像\",{\"1\":{\"48\":1}}],[\"作者使用遍历式方法\",{\"1\":{\"445\":1}}],[\"作者使用了谷歌制作的jft\",{\"1\":{\"297\":1}}],[\"作者使用了四个核心评估指标来衡量模型对语言引导下功能区域的识别能力\",{\"1\":{\"82\":1}}],[\"作者使用的是大规模图文对数据集\",{\"1\":{\"226\":1}}],[\"作者将vit和之前图像分类领域比较强的resnet模型进行了对比测试\",{\"1\":{\"297\":1}}],[\"作者将模型训练分为两个阶段\",{\"1\":{\"225\":1}}],[\"作者将通过\",{\"1\":{\"128\":1}}],[\"作者确定了在性能和效率之间取得平衡的最佳配置\",{\"1\":{\"189\":1}}],[\"作者推测英语语言模型的强大概率补偿了翻译知识的不足\",{\"1\":{\"455\":1}}],[\"作者推测\",{\"1\":{\"133\":1}}],[\"作者在微调时用辅助的lm目标来检查作者模型的性能\",{\"1\":{\"449\":1}}],[\"作者在微调阶段使用任务感知的输入转换来实现有效的迁移\",{\"1\":{\"439\":1}}],[\"作者在下面部分和可视化插图\",{\"1\":{\"445\":1}}],[\"作者在监督学习目标任务上调整参数\",{\"1\":{\"444\":1}}],[\"作者在四种类型的语言理解任务评估作者的方法\",{\"1\":{\"440\":1}}],[\"作者在常识推理\",{\"1\":{\"439\":1}}],[\"作者在不同模型深度\",{\"1\":{\"196\":1}}],[\"作者在模型中采用了\",{\"1\":{\"126\":1}}],[\"作者在提出的\",{\"1\":{\"21\":1}}],[\"作者设计了\",{\"1\":{\"71\":1}}],[\"作者设计了一个名为iag\",{\"1\":{\"48\":1}}],[\"作者设计了多个分析实验\",{\"1\":{\"25\":1}}],[\"定价为\",{\"1\":{\"674\":1}}],[\"定价降低约\",{\"1\":{\"674\":1}}],[\"定性分析与模型行为观察\",{\"1\":{\"471\":1}}],[\"定理\",{\"0\":{\"394\":1}}],[\"定理表明\",{\"1\":{\"105\":1}}],[\"定义节点id为1\",{\"1\":{\"666\":1}}],[\"定义包含节点x和y的有向图\",{\"1\":{\"666\":1}}],[\"定义如下\",{\"1\":{\"590\":1}}],[\"定义在实数上的连续分布\",{\"0\":{\"583\":1}}],[\"定义失败为红球\",{\"0\":{\"579\":1}}],[\"定义随机变量\",{\"1\":{\"565\":1}}],[\"定义事件空间后\",{\"1\":{\"565\":1}}],[\"定义字典保存路径\",{\"1\":{\"511\":1}}],[\"定义为\",{\"1\":{\"407\":1,\"575\":1,\"576\":1,\"584\":2,\"586\":2,\"587\":1,\"590\":1}}],[\"定义时\",{\"1\":{\"363\":1}}],[\"定义注意力矩阵的丢弃层\",{\"1\":{\"295\":1}}],[\"定义一个线性层\",{\"1\":{\"295\":1}}],[\"定义一个二维卷积层\",{\"1\":{\"291\":1}}],[\"定义一个字典\",{\"1\":{\"290\":1}}],[\"定义当前目录\",{\"1\":{\"275\":1,\"277\":1}}],[\"定义\",{\"0\":{\"590\":1},\"1\":{\"231\":1,\"404\":1,\"565\":1}}],[\"定义与核心思想\",{\"1\":{\"231\":1}}],[\"定义的一个结构体\",{\"1\":{\"163\":1}}],[\"定义相似度函数为\",{\"1\":{\"154\":1}}],[\"定义局部区域的形心\",{\"1\":{\"88\":1}}],[\"定义投影层的丢弃层\",{\"1\":{\"295\":1}}],[\"定义投影层\",{\"1\":{\"45\":1,\"295\":1}}],[\"定位答案\",{\"1\":{\"542\":1}}],[\"定位交互部位\",{\"1\":{\"28\":1}}],[\"定位交互部件并分析几何结构\",{\"1\":{\"6\":1}}],[\"定位交互发生的对象区域\",{\"1\":{\"28\":1}}],[\"定位精度显著更高\",{\"1\":{\"23\":1}}],[\"定位\",{\"1\":{\"7\":1}}],[\"调试和测试\",{\"1\":{\"685\":1}}],[\"调试便捷\",{\"1\":{\"662\":1}}],[\"调节因子\",{\"1\":{\"404\":1}}],[\"调整的过程\",{\"1\":{\"430\":1}}],[\"调整为适合下游任务的\",{\"1\":{\"423\":1}}],[\"调整某一系数会影响整个函数\",{\"1\":{\"395\":1}}],[\"调整某一系数会影响全局拟合\",{\"1\":{\"395\":1}}],[\"调整后的\",{\"1\":{\"327\":1}}],[\"调整以下三个要素来创建一个新的视图\",{\"1\":{\"325\":1}}],[\"调整维度\",{\"1\":{\"101\":1}}],[\"调整输出格式为\",{\"1\":{\"45\":1}}],[\"调整\",{\"1\":{\"45\":1,\"494\":1}}],[\"调用函数或工具\",{\"1\":{\"684\":1}}],[\"调用x\",{\"1\":{\"660\":1}}],[\"调用方法\",{\"1\":{\"373\":1}}],[\"调用后\",{\"1\":{\"365\":1,\"369\":2,\"377\":1}}],[\"调用前\",{\"1\":{\"365\":1,\"369\":2,\"377\":1}}],[\"调用时\",{\"1\":{\"363\":1}}],[\"调用q\",{\"1\":{\"286\":1}}],[\"调用bertmodel\",{\"1\":{\"268\":1}}],[\"调用跨模态解码器\",{\"1\":{\"142\":1}}],[\"调用\",{\"1\":{\"40\":1,\"111\":1,\"163\":1,\"285\":2,\"379\":1}}],[\"来提升系统效果\",{\"1\":{\"687\":1}}],[\"来提升应用性能\",{\"1\":{\"687\":1}}],[\"来提升模型在特定任务上的表现\",{\"1\":{\"681\":1}}],[\"来满足验证集效果\",{\"1\":{\"686\":1}}],[\"来解决任务\",{\"1\":{\"686\":1}}],[\"来替代子模型的训练调优\",{\"1\":{\"686\":1}}],[\"来开发基于大型语言模型的应用程序\",{\"1\":{\"682\":1}}],[\"来理解语言\",{\"1\":{\"673\":1}}],[\"来更新我们对未知变量\",{\"1\":{\"572\":1}}],[\"来更新分组特征\",{\"1\":{\"73\":1}}],[\"来定义事件空间\",{\"1\":{\"566\":1}}],[\"来定位图像中的物体区域\",{\"1\":{\"253\":1}}],[\"来完成\",{\"1\":{\"477\":1}}],[\"来完成点云物体分割任务\",{\"1\":{\"111\":1}}],[\"来完成点云分类任务\",{\"1\":{\"110\":1}}],[\"来挑选具有敏感内容识别能力的标注者\",{\"1\":{\"470\":1}}],[\"来隐式学习任务\",{\"1\":{\"454\":1}}],[\"来指定\",{\"1\":{\"454\":1}}],[\"来限制语言模型的输出分布只有\",{\"1\":{\"449\":1}}],[\"来正则化\",{\"1\":{\"447\":1}}],[\"来最终得到复杂问题的结果\",{\"1\":{\"436\":1}}],[\"来引导llm展现出更好的推理能力\",{\"1\":{\"434\":1}}],[\"来间接训练神经网络中的一些密集层\",{\"1\":{\"424\":1}}],[\"来减少模型对于计算资源的需求的方法\",{\"1\":{\"421\":1}}],[\"来增加表达能力\",{\"1\":{\"395\":1}}],[\"来扩大逼近空间的容量\",{\"1\":{\"395\":1}}],[\"来接收和返回函数\",{\"1\":{\"367\":1}}],[\"来命名环境\",{\"1\":{\"331\":1}}],[\"来读取数据\",{\"1\":{\"326\":1}}],[\"来帮助大家梳理清楚vision\",{\"1\":{\"288\":1}}],[\"来预测mask的\",{\"1\":{\"258\":1}}],[\"来计算textual\",{\"1\":{\"258\":1}}],[\"来说\",{\"1\":{\"242\":1,\"425\":1}}],[\"来训练模型\",{\"1\":{\"240\":1}}],[\"来适配不同类别\",{\"1\":{\"231\":1}}],[\"来生成图文结合的指令响应对\",{\"1\":{\"224\":1}}],[\"来实现大语言模型的控制\",{\"1\":{\"686\":1}}],[\"来实现数据的压缩\",{\"1\":{\"428\":1}}],[\"来实现\",{\"1\":{\"224\":1}}],[\"来实现全局信息建模\",{\"1\":{\"73\":1}}],[\"来对齐视觉和语言模型的特征\",{\"1\":{\"181\":1}}],[\"来表示整句话的语义\",{\"1\":{\"126\":1}}],[\"来表达各种线性代数运算\",{\"1\":{\"76\":1}}],[\"来模拟不同的采样密度\",{\"1\":{\"96\":1}}],[\"来学习优化的策略\",{\"1\":{\"95\":1}}],[\"来构建局部区域点集\",{\"1\":{\"88\":1}}],[\"来自图像\",{\"1\":{\"267\":1}}],[\"来自文本\",{\"1\":{\"267\":1}}],[\"来自视觉模型\",{\"1\":{\"263\":1}}],[\"来自动量编码器\",{\"1\":{\"163\":1}}],[\"来自下一级的特征\",{\"1\":{\"97\":1}}],[\"来自论文的理论分析\",{\"1\":{\"112\":1}}],[\"来自论文\",{\"1\":{\"82\":1}}],[\"来自论文图3\",{\"0\":{\"67\":1}}],[\"来自\",{\"1\":{\"46\":2,\"67\":1,\"98\":1,\"163\":1,\"226\":1,\"266\":1}}],[\"来源于真实\",{\"1\":{\"472\":1}}],[\"来源之二\",{\"1\":{\"45\":1}}],[\"来源之一\",{\"1\":{\"45\":1}}],[\"来源\",{\"1\":{\"45\":1}}],[\"来标记哪些多模态\",{\"1\":{\"43\":1}}],[\"来推断交互可能性\",{\"1\":{\"32\":1}}],[\"都可以提供对应的解决方案\",{\"1\":{\"682\":1}}],[\"都可以归纳为一种字典查询的工作\",{\"1\":{\"238\":1}}],[\"都使用了残差连接和层归一化\",{\"1\":{\"548\":1}}],[\"都单独构造一个\",{\"1\":{\"544\":1}}],[\"都等于max\",{\"1\":{\"520\":1}}],[\"都无所谓\",{\"1\":{\"505\":1}}],[\"都要\",{\"1\":{\"477\":1}}],[\"都随着模型参数的增长而呈现平滑的幂律提升趋势\",{\"1\":{\"462\":1}}],[\"都预测为负类别\",{\"1\":{\"343\":1}}],[\"都需要进行有监督微调\",{\"1\":{\"278\":1}}],[\"都存到了一起\",{\"1\":{\"242\":1}}],[\"都是有足够资源的来开发大模型\",{\"1\":{\"424\":1}}],[\"都是有效的\",{\"1\":{\"43\":1}}],[\"都是一个非线性变换\",{\"1\":{\"395\":1}}],[\"都是高阶函数\",{\"1\":{\"365\":1}}],[\"都是在寻找最相关的\",{\"1\":{\"312\":1}}],[\"都是197\",{\"1\":{\"294\":1}}],[\"都是从同一个\",{\"1\":{\"242\":1}}],[\"都展现出更强的像素级感知能力\",{\"1\":{\"193\":1}}],[\"都能接收相同的图像信息\",{\"1\":{\"143\":1}}],[\"都有一个对应的\",{\"1\":{\"540\":1}}],[\"都有一个\",{\"1\":{\"477\":1}}],[\"都有一个低维的本质模型\",{\"1\":{\"420\":1}}],[\"都有一个唯一的地址\",{\"1\":{\"321\":1}}],[\"都有机会执行\",{\"1\":{\"263\":1}}],[\"都有\",{\"1\":{\"115\":1}}],[\"都有效\",{\"1\":{\"43\":1}}],[\"都尝试引入更复杂的结构来提升建模能力\",{\"1\":{\"112\":1}}],[\"都会乘以相同的输入\",{\"1\":{\"425\":1}}],[\"都会对输入序列的长度有限制\",{\"1\":{\"415\":1}}],[\"都会处理一部分\",{\"1\":{\"249\":1}}],[\"都会被压缩为一个固定长度的特征向量\",{\"1\":{\"92\":1}}],[\"都会基于其语言语义\",{\"1\":{\"76\":1}}],[\"都与所有点\",{\"1\":{\"76\":1}}],[\"都带有原始语言上下文\",{\"1\":{\"76\":1}}],[\"则安装成功\",{\"1\":{\"666\":1}}],[\"则调用b\",{\"1\":{\"660\":1}}],[\"则尝试调用右操作数b的\",{\"1\":{\"660\":1}}],[\"则反向传播后清除中间变量的导数\",{\"1\":{\"658\":1}}],[\"则用来指代那些使用概率理论来表示\",{\"1\":{\"596\":1}}],[\"则二项分布退化为伯努利分布\",{\"1\":{\"575\":1}}],[\"则称\",{\"1\":{\"568\":2}}],[\"则定义事件\",{\"1\":{\"568\":1}}],[\"则复合事件的概率可通过求和得到\",{\"1\":{\"565\":1}}],[\"则将其设置为\",{\"1\":{\"541\":2}}],[\"则将其转换为\",{\"1\":{\"291\":1}}],[\"则生成的pad\",{\"1\":{\"517\":1}}],[\"则写出听起来\",{\"1\":{\"471\":1}}],[\"则容易自信地给出错误答案\",{\"1\":{\"471\":1}}],[\"则\",{\"1\":{\"425\":1,\"426\":1,\"565\":1,\"566\":1,\"625\":1,\"660\":1}}],[\"则它的参数量\",{\"1\":{\"425\":1}}],[\"则在这里激活\",{\"1\":{\"403\":1,\"404\":1}}],[\"则应注释掉这一行\",{\"1\":{\"402\":1,\"403\":1}}],[\"则点\",{\"1\":{\"353\":1}}],[\"则可能有必要选择\",{\"1\":{\"353\":1}}],[\"则可以用\",{\"1\":{\"350\":1}}],[\"则需要更新预训练模型参数\",{\"1\":{\"425\":1}}],[\"则需要在这里激活\",{\"1\":{\"401\":1}}],[\"则需要再多一层函数嵌套\",{\"1\":{\"371\":1}}],[\"则需要使用不同的工具\",{\"1\":{\"349\":1}}],[\"则需要将图像特征复制\",{\"1\":{\"143\":1}}],[\"则其概率质量函数\",{\"1\":{\"577\":1}}],[\"则其补集\",{\"1\":{\"566\":1}}],[\"则其\",{\"1\":{\"351\":1}}],[\"则其假正例数为零\",{\"1\":{\"346\":1}}],[\"则其准确性得分为\",{\"1\":{\"343\":1}}],[\"则最好改为针对其他指标进行优化\",{\"1\":{\"343\":1}}],[\"则表示数据集不平衡\",{\"1\":{\"342\":1}}],[\"则表示图像块是正方形\",{\"1\":{\"291\":1}}],[\"则表示图像是正方形\",{\"1\":{\"291\":1}}],[\"则会调用右操作数b的\",{\"1\":{\"660\":1}}],[\"则会得到以下表格\",{\"1\":{\"342\":1}}],[\"则会因\",{\"1\":{\"134\":1}}],[\"则创建线性分类头\",{\"1\":{\"296\":1}}],[\"则默认为\",{\"1\":{\"294\":2}}],[\"则默认多模态\",{\"1\":{\"43\":1}}],[\"则使用unk替换\",{\"1\":{\"511\":1}}],[\"则使用beam\",{\"1\":{\"461\":1}}],[\"则使用\",{\"1\":{\"296\":1}}],[\"则使用默认的\",{\"1\":{\"296\":1}}],[\"则使用默认的缩放因子\",{\"1\":{\"295\":1}}],[\"则使用该层进行归一化\",{\"1\":{\"291\":2}}],[\"则使用来自文献\",{\"1\":{\"191\":1}}],[\"则对图像进行处理\",{\"1\":{\"289\":1}}],[\"则对整个点云做全局特征提取\",{\"1\":{\"92\":1}}],[\"则计算当前层bertlayer时\",{\"1\":{\"285\":1}}],[\"则q来自query\",{\"1\":{\"285\":1}}],[\"则自动生成\",{\"1\":{\"284\":1}}],[\"则采用了两种不同的架构\",{\"1\":{\"272\":1}}],[\"则是以文本作为监督信号\",{\"1\":{\"270\":1}}],[\"则加载多语言\",{\"1\":{\"200\":1}}],[\"则按给定的概率矩阵进行伯努利采样\",{\"1\":{\"163\":1}}],[\"则做\",{\"1\":{\"162\":1}}],[\"则为\",{\"1\":{\"161\":1}}],[\"则视为\",{\"1\":{\"156\":1}}],[\"则基于高质量人工标注的数据集如\",{\"1\":{\"140\":1}}],[\"则模型体积大\",{\"1\":{\"134\":1}}],[\"则认为是噪声文本\",{\"1\":{\"128\":1}}],[\"则返回所有样本损失的平均值\",{\"1\":{\"405\":1,\"407\":1}}],[\"则返回\",{\"1\":{\"109\":2}}],[\"则返回该问题文本\",{\"1\":{\"68\":1}}],[\"则拼接起来\",{\"1\":{\"100\":1}}],[\"则保存\",{\"1\":{\"82\":1}}],[\"则从问题1～15中随机选择一个问题\",{\"1\":{\"68\":1}}],[\"则进行额外处理\",{\"1\":{\"40\":1}}],[\"插件系统\",{\"1\":{\"674\":1}}],[\"插值并融合后的特征\",{\"1\":{\"100\":1}}],[\"插值得到的密集特征\",{\"1\":{\"98\":1}}],[\"插值阶段\",{\"1\":{\"30\":1}}],[\"插入对应层的list缓存中去\",{\"1\":{\"477\":1}}],[\"插入缓存中\",{\"1\":{\"477\":1}}],[\"插入新维度的位置\",{\"1\":{\"381\":1}}],[\"插入到了\",{\"1\":{\"191\":1}}],[\"插入异常点\",{\"1\":{\"112\":1}}],[\"插入批次索引\",{\"1\":{\"59\":1}}],[\"插入的多模态嵌入\",{\"1\":{\"43\":1}}],[\"非负\",{\"1\":{\"574\":1}}],[\"非负性\",{\"1\":{\"567\":1}}],[\"非多任务学习\",{\"1\":{\"494\":1}}],[\"非英语泛化能力未系统评估\",{\"1\":{\"472\":1}}],[\"非英语指令等非监督数据上表现较好\",{\"1\":{\"471\":1}}],[\"非英语指令\",{\"1\":{\"471\":1}}],[\"非功能区域\",{\"1\":{\"404\":1}}],[\"非目标点\",{\"1\":{\"404\":1}}],[\"非目标区域匹配度\",{\"1\":{\"78\":1}}],[\"非连续\",{\"1\":{\"384\":1}}],[\"非对角元素\",{\"1\":{\"359\":1}}],[\"非对角线元素为\",{\"1\":{\"590\":1}}],[\"非对角线\",{\"1\":{\"355\":1}}],[\"非对角线上的元素\",{\"1\":{\"355\":1}}],[\"非常适合比较模型\",{\"1\":{\"352\":1}}],[\"非常高\",{\"1\":{\"280\":2}}],[\"非垃圾邮件被正确分类为非垃圾邮件\",{\"1\":{\"342\":1}}],[\"非垃圾邮件被误分类为垃圾邮件\",{\"1\":{\"342\":1}}],[\"非动量\",{\"1\":{\"163\":1}}],[\"非标答案\",{\"1\":{\"157\":1}}],[\"非均匀\",{\"1\":{\"116\":1}}],[\"非均匀密度下稳定的特征学习\",{\"0\":{\"94\":1}}],[\"非刚性运动\",{\"1\":{\"116\":1}}],[\"非刚性变形\",{\"1\":{\"112\":1}}],[\"非结构化\",{\"1\":{\"114\":1}}],[\"非线性替换为\",{\"1\":{\"674\":1}}],[\"非线性激活后的基\",{\"1\":{\"395\":1}}],[\"非线性变换\",{\"1\":{\"59\":1,\"262\":1,\"268\":1}}],[\"非线性增强和空间对齐\",{\"1\":{\"41\":2}}],[\"非\",{\"1\":{\"43\":1,\"163\":1,\"285\":1,\"338\":1}}],[\"供\",{\"1\":{\"43\":1}}],[\"默认优先级为100\",{\"1\":{\"660\":1}}],[\"默认模式\",{\"1\":{\"658\":1}}],[\"默认接受并执行\",{\"1\":{\"471\":1}}],[\"默认情况下\",{\"1\":{\"433\":1}}],[\"默认要求\",{\"1\":{\"402\":1}}],[\"默认是\",{\"1\":{\"381\":1}}],[\"默认仓库\",{\"1\":{\"338\":1}}],[\"默认使用行优先方式\",{\"1\":{\"322\":1}}],[\"默认使用\",{\"1\":{\"294\":1}}],[\"默认忽略的标签值\",{\"1\":{\"285\":1}}],[\"默认采用gelu激活函数\",{\"1\":{\"268\":1}}],[\"默认会忽略标签为\",{\"1\":{\"163\":1}}],[\"默认\",{\"1\":{\"147\":1,\"160\":2,\"389\":1,\"658\":1}}],[\"默认为0\",{\"1\":{\"295\":2}}],[\"默认为false\",{\"1\":{\"295\":1}}],[\"默认为8\",{\"1\":{\"295\":1}}],[\"默认为\",{\"1\":{\"142\":1,\"289\":1,\"291\":5,\"401\":1,\"402\":1,\"403\":1,\"404\":1}}],[\"默认为全\",{\"1\":{\"43\":1}}],[\"默认有\",{\"1\":{\"93\":1}}],[\"默认图像注意力掩码为空\",{\"1\":{\"40\":1}}],[\"形\",{\"1\":{\"520\":1}}],[\"形式和内容高度敏感\",{\"1\":{\"463\":1}}],[\"形式存储和计算\",{\"1\":{\"428\":1}}],[\"形式\",{\"1\":{\"410\":1,\"544\":1}}],[\"形式上\",{\"1\":{\"169\":1}}],[\"形式的监督信号\",{\"1\":{\"159\":1}}],[\"形式为\",{\"1\":{\"78\":1,\"512\":1}}],[\"形心点的坐标来实现\",{\"1\":{\"91\":1}}],[\"形成了一个具备如下特征的微型深度学习框架\",{\"1\":{\"670\":1}}],[\"形成复合函数\",{\"1\":{\"616\":1}}],[\"形成离散化的语义表示\",{\"1\":{\"166\":1}}],[\"形成多尺度特征表示\",{\"1\":{\"98\":1}}],[\"形成最终的局部特征表示\",{\"1\":{\"96\":1}}],[\"形成最终的局部区域表示\",{\"1\":{\"92\":1}}],[\"形成\",{\"1\":{\"92\":1,\"145\":1}}],[\"形成一个连续的一维空间\",{\"1\":{\"321\":1}}],[\"形成一个特征向量\",{\"1\":{\"97\":1}}],[\"形成一个综合的多尺度特征表示\",{\"1\":{\"95\":1}}],[\"形成一个新的子集\",{\"1\":{\"92\":1}}],[\"形成一个类似于\",{\"1\":{\"73\":1}}],[\"形成更稳定的联合表示\",{\"1\":{\"41\":1}}],[\"形状和数据类型\",{\"1\":{\"666\":1}}],[\"形状如\",{\"1\":{\"405\":1,\"407\":1}}],[\"形状是\",{\"1\":{\"283\":1}}],[\"形状与\",{\"1\":{\"98\":1,\"405\":1,\"407\":1}}],[\"形状也为\",{\"1\":{\"78\":1}}],[\"形状\",{\"1\":{\"76\":4,\"100\":5,\"101\":2,\"160\":1,\"286\":1,\"323\":2,\"325\":2,\"328\":1,\"355\":1,\"540\":2}}],[\"形状多样\",{\"1\":{\"71\":1}}],[\"形状的一维数组\",{\"1\":{\"64\":1}}],[\"形状为\",{\"1\":{\"43\":1,\"78\":1,\"83\":1,\"291\":2,\"326\":1,\"327\":1,\"401\":2,\"402\":2,\"403\":2,\"404\":2}}],[\"层的作用\",{\"1\":{\"540\":1}}],[\"层共享权重\",{\"1\":{\"513\":1}}],[\"层与\",{\"1\":{\"513\":1}}],[\"层有自注意力头\",{\"1\":{\"447\":1}}],[\"层数的迁移学习影响\",{\"1\":{\"449\":1}}],[\"层数\",{\"1\":{\"395\":1}}],[\"层relu网络可生成\",{\"1\":{\"395\":1}}],[\"层网络的输出是多次复合的结果\",{\"1\":{\"395\":1}}],[\"层交错堆叠\",{\"1\":{\"299\":1}}],[\"层和\",{\"1\":{\"299\":1}}],[\"层和后\",{\"1\":{\"152\":1}}],[\"层处理后的输出\",{\"1\":{\"294\":1}}],[\"层或\",{\"1\":{\"196\":1}}],[\"层明显优于传统\",{\"1\":{\"196\":1}}],[\"层也能取得良好效果\",{\"1\":{\"190\":1}}],[\"层连接\",{\"1\":{\"190\":1}}],[\"层视觉\",{\"1\":{\"152\":1}}],[\"层不共享参数\",{\"1\":{\"134\":1}}],[\"层则可以共享\",{\"1\":{\"126\":1}}],[\"层以外的所有参数\",{\"1\":{\"126\":1}}],[\"层维度\",{\"1\":{\"112\":1}}],[\"层进一步融合局部\",{\"1\":{\"111\":1}}],[\"层后面都加了\",{\"1\":{\"107\":1}}],[\"层后的结果\",{\"1\":{\"43\":1}}],[\"层层插值并融合特征\",{\"1\":{\"101\":1}}],[\"层层下采样并提取特征\",{\"1\":{\"101\":1}}],[\"层\",{\"0\":{\"264\":1},\"1\":{\"99\":2,\"101\":4,\"109\":1,\"126\":1,\"134\":1,\"152\":2,\"171\":1,\"174\":1,\"191\":1,\"196\":1,\"202\":2,\"294\":3,\"395\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"513\":1}}],[\"层堆叠\",{\"1\":{\"98\":2}}],[\"层构成了一个\",{\"1\":{\"93\":1}}],[\"层提取特征\",{\"1\":{\"92\":1}}],[\"层次化结构由多个set\",{\"1\":{\"88\":1}}],[\"层次化点集特征学习\",{\"0\":{\"88\":1}}],[\"层归一化调整\",{\"1\":{\"454\":1}}],[\"层归一化来生成所有可能答案的分布\",{\"1\":{\"445\":1}}],[\"层归一化\",{\"1\":{\"45\":1,\"447\":1,\"532\":1,\"557\":1}}],[\"层将计算结果映射至词表空间\",{\"1\":{\"474\":1}}],[\"层将\",{\"1\":{\"40\":1,\"286\":1}}],[\"应对边界情况\",{\"1\":{\"687\":1}}],[\"应被及时回收\",{\"1\":{\"657\":1}}],[\"应为\",{\"1\":{\"326\":1}}],[\"应使用\",{\"0\":{\"137\":1}}],[\"应用就可以上线体验了\",{\"1\":{\"687\":1}}],[\"应用程序的开发\",{\"1\":{\"685\":1}}],[\"应用程序的开发提供了坚实的基础\",{\"1\":{\"685\":1}}],[\"应用程序部署到云端\",{\"1\":{\"685\":1}}],[\"应用的复杂度\",{\"1\":{\"684\":1}}],[\"应用的开发和部署提供了坚实的基础\",{\"1\":{\"684\":1}}],[\"应用中的每一步操作及其输入输出有一个清晰的认识\",{\"1\":{\"684\":1}}],[\"应用开发的大一统基座模型\",{\"1\":{\"677\":1}}],[\"应用于真实的\",{\"1\":{\"472\":1}}],[\"应用层\",{\"0\":{\"437\":1}}],[\"应用量化低秩适应\",{\"1\":{\"428\":1}}],[\"应用自定义的权重初始化函数\",{\"1\":{\"296\":1}}],[\"应用注意力掩码\",{\"1\":{\"285\":1}}],[\"应用注意力权重\",{\"1\":{\"266\":1}}],[\"应用\",{\"1\":{\"267\":1,\"294\":1,\"355\":1,\"674\":2,\"682\":1,\"683\":1}}],[\"应用场景适应性受限\",{\"1\":{\"679\":1}}],[\"应用场景举例\",{\"1\":{\"231\":1}}],[\"应用场景\",{\"1\":{\"115\":1,\"404\":1}}],[\"应用到原始点云上\",{\"1\":{\"107\":1}}],[\"应用改良的交叉注意力机制\",{\"1\":{\"36\":1}}],[\"应该把\",{\"1\":{\"505\":1}}],[\"应该采用什么样的方法来进行训练\",{\"1\":{\"278\":1}}],[\"应该更高地加权第二个向量\",{\"1\":{\"97\":1}}],[\"应该关注哪些新能力\",{\"1\":{\"688\":1}}],[\"应该关注哪些点云点\",{\"1\":{\"41\":1}}],[\"应该关注图像中的哪些位置\",{\"1\":{\"41\":1}}],[\"每份采用不同的随机掩码模式\",{\"1\":{\"495\":1}}],[\"每轮推理时\",{\"1\":{\"477\":1}}],[\"每轮只使用\",{\"1\":{\"131\":1}}],[\"每生成一个新\",{\"1\":{\"477\":1}}],[\"每生成一个\",{\"1\":{\"475\":1}}],[\"每\",{\"1\":{\"474\":1}}],[\"每种异排列对应的全排列数\",{\"1\":{\"601\":2}}],[\"每种异排列对应的重复排列数\",{\"1\":{\"600\":2}}],[\"每种方法在不同任务上都优于其它方法\",{\"1\":{\"440\":1}}],[\"每种物体可以对应多种形状实例和功能类别\",{\"1\":{\"68\":2}}],[\"每家公司都去从头训练一个自己的大模型\",{\"1\":{\"415\":1}}],[\"每层对应一个\",{\"1\":{\"395\":1}}],[\"每层聚焦不同抽象层次\",{\"1\":{\"395\":1}}],[\"每层\",{\"1\":{\"395\":1}}],[\"每层学习一个转折点\",{\"1\":{\"395\":1}}],[\"每层逐步下采样\",{\"1\":{\"101\":1}}],[\"每封电子邮件的实际分类取决于您选择的阈值\",{\"1\":{\"351\":1}}],[\"每行复制\",{\"1\":{\"387\":1}}],[\"每行的总和表示所有预测正例\",{\"1\":{\"342\":1}}],[\"每行是一个\",{\"1\":{\"41\":1}}],[\"每隔一列\",{\"1\":{\"325\":1}}],[\"每隔一行\",{\"1\":{\"325\":1}}],[\"每一小块大小不同\",{\"1\":{\"396\":1}}],[\"每一层transformer层能带来9\",{\"1\":{\"449\":1}}],[\"每一层的非线性变换\",{\"1\":{\"395\":1}}],[\"每一层只需学习局部特征\",{\"1\":{\"395\":1}}],[\"每一层\",{\"1\":{\"395\":1}}],[\"每一层bertlayer产生的key\",{\"1\":{\"285\":1}}],[\"每一个存储单元\",{\"1\":{\"321\":1}}],[\"每一个向量作为一个单独的输入\",{\"1\":{\"291\":1}}],[\"每一个patch作为一个token\",{\"1\":{\"291\":1}}],[\"每一行表示一张图像和所有文本之间的相似度\",{\"1\":{\"283\":1}}],[\"每次取出辈分最大的函数\",{\"1\":{\"656\":1}}],[\"每次输入模型时动态生成掩码\",{\"1\":{\"495\":1}}],[\"每次输入序列时生成新的掩码模式\",{\"1\":{\"495\":1}}],[\"每次输入时重新生成掩码\",{\"1\":{\"493\":1}}],[\"每次计算后返回本次可能需要缓存的key\",{\"1\":{\"285\":1}}],[\"每次一个\",{\"1\":{\"241\":1}}],[\"每次都是使用新的编码器更新\",{\"1\":{\"238\":1}}],[\"每次迭代中\",{\"1\":{\"66\":1}}],[\"每张图像生成一个序列\",{\"1\":{\"143\":1}}],[\"每对图文样本只需通过一次视觉\",{\"1\":{\"127\":1}}],[\"每列是一个\",{\"1\":{\"41\":1,\"249\":1}}],[\"每个矩形代表了一个数据状态\",{\"1\":{\"682\":1}}],[\"每个椭圆形代表了\",{\"1\":{\"682\":1}}],[\"每个函数节点需要计算其导数\",{\"1\":{\"627\":1}}],[\"每个像素亮度相同\",{\"1\":{\"594\":1}}],[\"每个像素都被视为图像的基本单位\",{\"1\":{\"399\":1}}],[\"每个头64维\",{\"1\":{\"558\":1}}],[\"每个位置的输出会通过一个独立的前馈神经网络进行进一步处理\",{\"1\":{\"548\":1}}],[\"每个词汇还同时保留产生该词汇的原始文章\",{\"1\":{\"510\":1}}],[\"每个词的每个字符后都加上空格\",{\"1\":{\"410\":1}}],[\"每个句子在训练过程中会被看到\",{\"1\":{\"495\":1}}],[\"每个任务需要数千至数十万标注样本\",{\"1\":{\"460\":1}}],[\"每个这些序列用作者的模型独立处理后通过一个\",{\"1\":{\"445\":1}}],[\"每个实例有输入字符的序列构成\",{\"1\":{\"444\":1}}],[\"每个子层\",{\"1\":{\"548\":1}}],[\"每个子词出现次数加1\",{\"1\":{\"410\":1}}],[\"每个子词的出现次数\",{\"1\":{\"410\":1}}],[\"每个子步骤都由一个\",{\"1\":{\"28\":1}}],[\"每个撒下去的小数点都用周围的4个整数点去\",{\"1\":{\"397\":2}}],[\"每个网格是\",{\"1\":{\"396\":1}}],[\"每个小块做\",{\"1\":{\"396\":1}}],[\"每个残差块还包含一条跨层的连接线\",{\"1\":{\"392\":1}}],[\"每个残差块包含两个3x3的卷积层\",{\"1\":{\"392\":1}}],[\"每个卷积层后面都跟着一个batch\",{\"1\":{\"392\":1}}],[\"每个卷积层后跟一个\",{\"1\":{\"109\":1}}],[\"每个变量自己的方差\",{\"1\":{\"355\":1}}],[\"每个列中的总和会显示所有真实正例\",{\"1\":{\"342\":1}}],[\"每个维度上的\",{\"1\":{\"323\":1}}],[\"每个通道有\",{\"1\":{\"323\":1}}],[\"每个图像块的尺寸\",{\"1\":{\"291\":1}}],[\"每个图像位置对应的所有点云点\",{\"1\":{\"41\":1}}],[\"每个patch是三通道的小图片\",{\"1\":{\"291\":1}}],[\"每个output\",{\"1\":{\"284\":1}}],[\"每个query\",{\"1\":{\"283\":1}}],[\"每个query通道维度做特征融合\",{\"1\":{\"76\":1}}],[\"每个模态单独使用重的transformer\",{\"1\":{\"255\":1}}],[\"每个特征\",{\"1\":{\"242\":1}}],[\"每个样本都需要做一个\",{\"1\":{\"241\":1}}],[\"每个样本为一个\",{\"1\":{\"18\":1}}],[\"每个问题包含文本或图像上下文\",{\"1\":{\"227\":1}}],[\"每个块大小为\",{\"1\":{\"169\":1}}],[\"每个格子表示是否有物体\",{\"1\":{\"114\":1}}],[\"每个原始点都有了一个新的特征向量\",{\"1\":{\"100\":1}}],[\"每个原始点都有了它最近的\",{\"1\":{\"100\":1}}],[\"每个原始点对应的\",{\"1\":{\"100\":1}}],[\"每个层是一个conv1d\",{\"1\":{\"100\":1}}],[\"每个结构中心点不变\",{\"1\":{\"96\":1}}],[\"每个关键点的多尺度特征表示\",{\"1\":{\"96\":1}}],[\"每个关键点对应的局部区域点和特征\",{\"1\":{\"92\":1}}],[\"每个尺度可以有不同的网络深度和宽度\",{\"1\":{\"96\":1}}],[\"每个尺度\",{\"1\":{\"95\":1}}],[\"每个半径定义了一个局部邻域的大小\",{\"1\":{\"95\":1}}],[\"每个查询点点局部特征\",{\"1\":{\"92\":1}}],[\"每个查询点对所有原始点的距离\",{\"1\":{\"92\":1}}],[\"每个邻域内采样的关键点数量\",{\"1\":{\"92\":2}}],[\"每个区域中点的数量𝐾和query的半径𝑟\",{\"1\":{\"90\":1}}],[\"每个\",{\"1\":{\"76\":5,\"142\":1,\"249\":1,\"263\":1,\"305\":2,\"310\":1,\"312\":1,\"386\":1,\"390\":2,\"397\":4,\"540\":3}}],[\"每个文本token询问所有点key后\",{\"1\":{\"72\":1}}],[\"每个形状实例随机匹配一个与其功能类型一致的问题\",{\"1\":{\"66\":1}}],[\"每个物体类别可有多个形状实例\",{\"1\":{\"65\":1}}],[\"每个物体都以点云形式表示\",{\"1\":{\"62\":1}}],[\"每个点都代表一个阈值\",{\"1\":{\"353\":1}}],[\"每个点都需要全局上下文\",{\"1\":{\"111\":1}}],[\"每个点通常包含\",{\"1\":{\"114\":1}}],[\"每个点有\",{\"1\":{\"100\":1,\"107\":1,\"114\":1}}],[\"每个点到当前所有已选中心点的最小距离\",{\"1\":{\"92\":1}}],[\"每个点是否属于当前问题描述的功能区域\",{\"1\":{\"64\":1}}],[\"每个点的分类结果\",{\"1\":{\"101\":1}}],[\"每个点的三个邻近点的权重\",{\"1\":{\"100\":1}}],[\"每个点的值\",{\"1\":{\"76\":1}}],[\"每个点的\",{\"1\":{\"46\":1}}],[\"每个点云约\",{\"1\":{\"67\":1}}],[\"每个点云点对应的所有图像位置\",{\"1\":{\"41\":1}}],[\"每个点云点与图像中每个位置之间的相似度得分\",{\"1\":{\"41\":1}}],[\"每个点云实例按可供性类别标注\",{\"1\":{\"18\":1}}],[\"每个点包括\",{\"1\":{\"18\":1}}],[\"防止同一个函数被多次添加到func列表中\",{\"1\":{\"656\":1}}],[\"防止跨字符类别的合并\",{\"1\":{\"454\":1}}],[\"防止出现\",{\"1\":{\"407\":1}}],[\"防止被背景淹没\",{\"1\":{\"404\":1}}],[\"防止模型只关注简单样本\",{\"1\":{\"404\":1}}],[\"防止模型偏向特定句式或长度\",{\"1\":{\"63\":1}}],[\"防止前景点\",{\"1\":{\"404\":1}}],[\"防止除零错误\",{\"1\":{\"401\":1,\"402\":1,\"403\":1,\"404\":1}}],[\"防止除以零\",{\"1\":{\"401\":2,\"402\":1,\"405\":1,\"407\":1}}],[\"防止内积过大导致\",{\"1\":{\"318\":1}}],[\"防止过拟合\",{\"1\":{\"226\":1,\"295\":4,\"470\":1}}],[\"防止重复生成\",{\"1\":{\"143\":1}}],[\"防止忽略小区域\",{\"1\":{\"78\":1}}],[\"防止其影响后续计算\",{\"1\":{\"76\":1}}],[\"防止\",{\"1\":{\"41\":1,\"407\":2}}],[\"仅当config\",{\"1\":{\"658\":1}}],[\"仅终端变量的导数需要被保留\",{\"1\":{\"658\":1}}],[\"仅供参考和学习\",{\"1\":{\"515\":1}}],[\"仅177gb\",{\"1\":{\"482\":1}}],[\"仅比人类低几分\",{\"1\":{\"462\":1}}],[\"仅为49\",{\"1\":{\"462\":1}}],[\"仅提供任务描述\",{\"1\":{\"461\":1}}],[\"仅需自然语言提示\",{\"1\":{\"455\":1}}],[\"仅需4个tanh神经元即可高精度拟合\",{\"1\":{\"395\":1}}],[\"仅仅需要小小修改模型架构\",{\"1\":{\"439\":1}}],[\"仅借鉴了encoder结构\",{\"1\":{\"287\":1}}],[\"仅能和自己的\",{\"1\":{\"283\":1}}],[\"仅从其中提取特征\",{\"1\":{\"237\":1}}],[\"仅\",{\"1\":{\"228\":1,\"338\":1}}],[\"仅训练分类头\",{\"1\":{\"300\":1}}],[\"仅训练\",{\"1\":{\"196\":1}}],[\"仅训练新增参数\",{\"1\":{\"201\":1}}],[\"仅训练新增的\",{\"1\":{\"190\":1}}],[\"仅训练新引入的参数\",{\"1\":{\"191\":1}}],[\"仅通过增加语言模型的规模\",{\"1\":{\"467\":1}}],[\"仅通过自然语言提示和示例\",{\"1\":{\"465\":1}}],[\"仅通过few\",{\"1\":{\"462\":1}}],[\"仅通过语言建模目标就能在零样本设置下完成多种nlp任务\",{\"1\":{\"457\":1}}],[\"仅通过\",{\"1\":{\"194\":1}}],[\"仅通过扩充原始数据进行更长时间训练\",{\"1\":{\"136\":1}}],[\"仅微调\",{\"1\":{\"190\":1}}],[\"仅使用cpu就能完成整个训练过程\",{\"1\":{\"510\":1}}],[\"仅使用交叉熵损失\",{\"1\":{\"407\":1}}],[\"仅使用最后一层视觉特征\",{\"1\":{\"229\":1}}],[\"仅使用\",{\"1\":{\"190\":1,\"407\":1}}],[\"仅用\",{\"1\":{\"674\":1}}],[\"仅用squad数据\",{\"1\":{\"499\":1}}],[\"仅用internvit\",{\"1\":{\"188\":1}}],[\"仅用于生成\",{\"1\":{\"163\":1}}],[\"仅支持单模态任务\",{\"1\":{\"188\":1}}],[\"仅对\",{\"1\":{\"163\":1}}],[\"仅参数使用\",{\"1\":{\"160\":1}}],[\"仅在\",{\"1\":{\"134\":1,\"425\":1}}],[\"仅靠\",{\"1\":{\"112\":1}}],[\"仅靠局部特征很难判断某个点属于哪个部件\",{\"1\":{\"111\":1}}],[\"仅返回预测结果\",{\"1\":{\"40\":1}}],[\"仅更新插入的\",{\"1\":{\"231\":1}}],[\"仅更新投影矩阵\",{\"1\":{\"226\":1}}],[\"仅更新\",{\"1\":{\"10\":1}}],[\"语音\",{\"1\":{\"674\":1}}],[\"语法简洁\",{\"1\":{\"658\":1}}],[\"语法纠错等任务中\",{\"1\":{\"462\":1}}],[\"语句连贯性\",{\"1\":{\"440\":1}}],[\"语言中\",{\"1\":{\"597\":1}}],[\"语言中间件\",{\"1\":{\"189\":1}}],[\"语言识别\",{\"1\":{\"481\":1}}],[\"语言多样性不足\",{\"1\":{\"472\":1}}],[\"语言建模的研究可以追溯到\",{\"1\":{\"673\":1}}],[\"语言建模的核心框架\",{\"1\":{\"454\":1}}],[\"语言建模任务评估\",{\"1\":{\"455\":1}}],[\"语言建模头\",{\"1\":{\"268\":1}}],[\"语言可接受性\",{\"1\":{\"449\":1}}],[\"语言对齐与生成能力\",{\"1\":{\"197\":1}}],[\"语言对比训练\",{\"1\":{\"190\":1}}],[\"语言生成训练\",{\"1\":{\"190\":1}}],[\"语言大模型\",{\"1\":{\"181\":1}}],[\"语言大规模预训练能提升多种视觉语言任务\",{\"1\":{\"149\":1}}],[\"语言任务中取得了竞争力甚至更优的性能\",{\"1\":{\"253\":1}}],[\"语言任务中取得显著进展\",{\"1\":{\"211\":1}}],[\"语言任务中表现出色\",{\"1\":{\"180\":1}}],[\"语言任务上以零样本方式迁移也表现优异\",{\"1\":{\"120\":1}}],[\"语言基础模型的发展却相对滞后\",{\"1\":{\"181\":1}}],[\"语言基础模型发展滞后于大型语言模型\",{\"1\":{\"180\":1}}],[\"语言基础模型\",{\"1\":{\"180\":1,\"198\":1}}],[\"语言表示学习主要分为两类\",{\"1\":{\"150\":1}}],[\"语言研究的发展\",{\"1\":{\"138\":1}}],[\"语言预训练所使用的图文对大多来自网页\",{\"1\":{\"157\":1}}],[\"语言预训练框架\",{\"1\":{\"138\":1}}],[\"语言预训练中使用合成图像描述的独特优势\",{\"1\":{\"124\":1}}],[\"语言预训练\",{\"0\":{\"122\":1},\"1\":{\"120\":1,\"253\":2}}],[\"语言查询特征\",{\"1\":{\"76\":1}}],[\"语言含义\",{\"1\":{\"76\":1}}],[\"语言引导的点特征筛选\",{\"1\":{\"72\":1}}],[\"语言引导的\",{\"1\":{\"70\":1,\"82\":1}}],[\"语言引导下的\",{\"1\":{\"60\":1}}],[\"语言辅助方法\",{\"1\":{\"51\":1}}],[\"语言后半段\",{\"1\":{\"43\":1}}],[\"语言前半段\",{\"1\":{\"43\":1}}],[\"语言部分的\",{\"1\":{\"43\":1}}],[\"语言\",{\"1\":{\"43\":1}}],[\"语言模型发展脉络\",{\"1\":{\"485\":1}}],[\"语言模型会生成有害或偏见内容\",{\"1\":{\"469\":1}}],[\"语言模型可能直接学会执行任务\",{\"1\":{\"456\":1}}],[\"语言模型通过预测多样化文本中的任务演示\",{\"1\":{\"454\":1}}],[\"语言模型通过链式法则计算联合概率\",{\"1\":{\"454\":1}}],[\"语言模型本身可以通过观察任务的自然语言演示\",{\"1\":{\"454\":1}}],[\"语言模型本身可能通过无监督学习捕捉任务相关的知识\",{\"1\":{\"453\":1}}],[\"语言模型使用多层的\",{\"1\":{\"443\":1}}],[\"语言模型从nous\",{\"1\":{\"215\":1}}],[\"语言模型\",{\"1\":{\"42\":1,\"214\":1,\"226\":1,\"227\":1,\"674\":1}}],[\"语言指令理解特征\",{\"1\":{\"40\":1}}],[\"语义角色标注\",{\"1\":{\"543\":1}}],[\"语义解析\",{\"1\":{\"469\":1}}],[\"语义比较等\",{\"1\":{\"462\":1}}],[\"语义一致性\",{\"1\":{\"462\":1}}],[\"语义文本相似度数据集\",{\"1\":{\"448\":1}}],[\"语义组块chuking\",{\"1\":{\"441\":1}}],[\"语义相似度\",{\"1\":{\"448\":2}}],[\"语义相似度评估\",{\"1\":{\"439\":1,\"450\":1}}],[\"语义相识度\",{\"1\":{\"440\":1}}],[\"语义信息不应该发生变化\",{\"1\":{\"235\":1}}],[\"语义分割任务需要对图像中的每个像素进行分类\",{\"1\":{\"399\":1}}],[\"语义分割任务中\",{\"1\":{\"399\":1}}],[\"语义分割可以被看作是像素级别的图像分割\",{\"1\":{\"399\":1}}],[\"语义分割不仅需要识别图像中的物体\",{\"1\":{\"399\":1}}],[\"语义分割是计算机视觉领域中的一项任务\",{\"1\":{\"399\":1}}],[\"语义分割中常用的损失函数\",{\"0\":{\"398\":1},\"1\":{\"398\":1}}],[\"语义分割\",{\"0\":{\"399\":1},\"1\":{\"175\":1,\"180\":1,\"193\":1,\"399\":1}}],[\"语义对齐\",{\"1\":{\"33\":1}}],[\"语义空间受限\",{\"1\":{\"6\":1}}],[\"降为用4bit来表示\",{\"1\":{\"421\":1}}],[\"降训练成本\",{\"1\":{\"421\":1}}],[\"降低了产生幻觉的概率\",{\"1\":{\"681\":1}}],[\"降低幻觉\",{\"1\":{\"681\":1}}],[\"降低部署风险\",{\"1\":{\"472\":1}}],[\"降低易分类样本的权重\",{\"1\":{\"404\":1}}],[\"降低动量负样本带来的扰动\",{\"1\":{\"159\":1}}],[\"降维适配器\",{\"0\":{\"44\":1},\"1\":{\"40\":1,\"44\":1}}],[\"降到\",{\"1\":{\"24\":1}}],[\"返回第1维度的元素数\",{\"1\":{\"659\":1}}],[\"返回值形如\",{\"1\":{\"540\":1}}],[\"返回两个任务的结果\",{\"1\":{\"513\":1}}],[\"返回构建得到的单个样本列表\",{\"1\":{\"512\":1}}],[\"返回处理后的句子列表\",{\"1\":{\"510\":1}}],[\"返回合并最高频字符对后的vocab\",{\"1\":{\"410\":1}}],[\"返回视图\",{\"1\":{\"387\":1}}],[\"返回具有新形状的张量\",{\"1\":{\"385\":1}}],[\"返回具有新形状\",{\"1\":{\"384\":1}}],[\"返回一个函数\",{\"1\":{\"365\":1}}],[\"返回分类标记对应的特征\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"返回数据集中图像的数量\",{\"1\":{\"289\":1}}],[\"返回字典格式结果\",{\"1\":{\"285\":1}}],[\"返回预测出来的\",{\"1\":{\"268\":1}}],[\"返回预测得分\",{\"1\":{\"268\":1}}],[\"返回掩码后的\",{\"1\":{\"163\":1}}],[\"返回结构化输出\",{\"1\":{\"162\":1}}],[\"返回三个\",{\"1\":{\"147\":1}}],[\"返回的张量虽然是视图\",{\"1\":{\"383\":1}}],[\"返回的是\",{\"1\":{\"372\":1}}],[\"返回的\",{\"1\":{\"369\":1}}],[\"返回的每个\",{\"1\":{\"244\":1}}],[\"返回的caption前不添加prompt\",{\"1\":{\"147\":1}}],[\"返回的高维隐向量维度为\",{\"1\":{\"30\":1}}],[\"返回itc损失和itm损失\",{\"1\":{\"145\":1}}],[\"返回每个点的分类结果和抽象特征\",{\"1\":{\"101\":1}}],[\"返回位置索引\",{\"1\":{\"92\":1}}],[\"返回目标物体区域特征图\",{\"1\":{\"59\":1}}],[\"返回拼接好的输入和\",{\"1\":{\"43\":1}}],[\"返回\",{\"1\":{\"40\":1,\"68\":1,\"78\":1,\"92\":1,\"100\":1,\"101\":2,\"107\":1,\"109\":1,\"163\":1,\"333\":1,\"371\":1,\"401\":2,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1}}],[\"函数负责绑定运算符方法\",{\"1\":{\"661\":1}}],[\"函数和\",{\"1\":{\"652\":1}}],[\"函数在处的导数定义为\",{\"1\":{\"620\":1}}],[\"函数的python化\",{\"0\":{\"641\":1}}],[\"函数的表达式为\",{\"1\":{\"630\":1}}],[\"函数的连续调用\",{\"0\":{\"615\":1}}],[\"函数的作用是将整个点云视为一个\",{\"1\":{\"92\":1}}],[\"函数类的设计\",{\"0\":{\"612\":1}}],[\"函数定义\",{\"1\":{\"661\":1,\"662\":3}}],[\"函数定义了变量之间的对应关系\",{\"1\":{\"611\":1}}],[\"函数定义见\",{\"1\":{\"82\":1}}],[\"函数与计算图\",{\"0\":{\"611\":1}}],[\"函数生成目标序列的概率分布\",{\"1\":{\"548\":1}}],[\"函数复合\",{\"1\":{\"395\":3}}],[\"函数空间\",{\"1\":{\"395\":1}}],[\"函数替换器\",{\"1\":{\"375\":1}}],[\"函数替代了原来的\",{\"1\":{\"369\":1}}],[\"函数上\",{\"1\":{\"372\":1}}],[\"函数返回一个函数\",{\"1\":{\"365\":1}}],[\"函数接收另一个函数作为参数\",{\"1\":{\"365\":1}}],[\"函数中\",{\"1\":{\"240\":1}}],[\"函数\",{\"1\":{\"40\":1,\"76\":1,\"105\":1,\"115\":1,\"275\":1,\"277\":1,\"289\":1,\"368\":1,\"369\":1,\"372\":1,\"375\":1,\"405\":1,\"586\":2}}],[\"拼接等操作\",{\"1\":{\"651\":1}}],[\"拼接前提文本\",{\"1\":{\"445\":1}}],[\"拼接平均池化的\",{\"1\":{\"204\":1}}],[\"拼接当前动量文本特征和文本队列\",{\"1\":{\"161\":1}}],[\"拼接当前动量图像特征和图像队列\",{\"1\":{\"161\":1}}],[\"拼接机制不够精细\",{\"1\":{\"112\":1}}],[\"拼接方式缺乏动态调整机制\",{\"1\":{\"112\":1}}],[\"拼接\",{\"1\":{\"59\":1,\"512\":1}}],[\"拼接嵌入向量\",{\"1\":{\"43\":1}}],[\"拼接在一起\",{\"1\":{\"43\":1,\"285\":1,\"474\":1}}],[\"拼接多模态嵌入与语言嵌入\",{\"0\":{\"43\":1},\"1\":{\"40\":1}}],[\"拼接后的输入嵌入\",{\"1\":{\"43\":1}}],[\"拼接后\",{\"1\":{\"34\":1,\"35\":1,\"36\":1}}],[\"拼接后送入解码器输出可供性预测\",{\"1\":{\"15\":1}}],[\"设定传入的input和返回的output均为variable类型\",{\"1\":{\"613\":1}}],[\"设定下\",{\"1\":{\"462\":1}}],[\"设想我们要从一张二维图像\",{\"1\":{\"597\":1}}],[\"设随机变量\",{\"1\":{\"565\":1,\"577\":1}}],[\"设两维特征的协方差矩阵是\",{\"1\":{\"359\":1}}],[\"设动量模型在图像\",{\"1\":{\"157\":1}}],[\"设动量编码器生成的相似度为\",{\"1\":{\"157\":1}}],[\"设\",{\"1\":{\"155\":1,\"426\":1,\"565\":1,\"566\":1,\"575\":1,\"578\":1,\"579\":1}}],[\"设为\",{\"1\":{\"143\":1}}],[\"设是一个函数\",{\"1\":{\"115\":1}}],[\"设计产品页面\",{\"1\":{\"687\":1}}],[\"设计后\",{\"1\":{\"687\":1}}],[\"设计的一般原则及技巧\",{\"1\":{\"687\":1}}],[\"设计的一种\",{\"1\":{\"566\":1}}],[\"设计功能\",{\"1\":{\"687\":1}}],[\"设计功能热图\",{\"1\":{\"51\":1}}],[\"设计合理\",{\"1\":{\"686\":1}}],[\"设计调优\",{\"1\":{\"686\":1}}],[\"设计function类作为基类\",{\"1\":{\"612\":1}}],[\"设计或改进对比损失函数\",{\"1\":{\"240\":1}}],[\"设计图文对比\",{\"1\":{\"149\":1}}],[\"设计局部\",{\"1\":{\"105\":1}}],[\"设计了\",{\"1\":{\"470\":1}}],[\"设计了一个统一架构\",{\"1\":{\"103\":1}}],[\"设计了多头部功能链式思维\",{\"1\":{\"5\":1}}],[\"设置其优先级高于ndarray\",{\"1\":{\"660\":1}}],[\"设置变量的creator\",{\"1\":{\"634\":1}}],[\"设置的回调方法cllote\",{\"1\":{\"522\":1}}],[\"设置下均达到sota\",{\"1\":{\"482\":1}}],[\"设置下的表现\",{\"1\":{\"460\":1}}],[\"设置下完成多种自然语言处理任务\",{\"1\":{\"452\":1}}],[\"设置为\",{\"1\":{\"447\":1}}],[\"设置为评估模式\",{\"1\":{\"83\":1}}],[\"设置默认参数\",{\"1\":{\"405\":1}}],[\"设置全局参数\",{\"1\":{\"404\":1}}],[\"设置步幅为\",{\"1\":{\"327\":1}}],[\"设置柱状图的标题\",{\"1\":{\"289\":1}}],[\"设置y坐标\",{\"1\":{\"289\":1}}],[\"设置x坐标\",{\"1\":{\"289\":1}}],[\"设置渲染参数\",{\"1\":{\"83\":1}}],[\"设置颜色\",{\"1\":{\"83\":1}}],[\"设置训练轮数\",{\"1\":{\"83\":1}}],[\"设置学习率\",{\"1\":{\"83\":1}}],[\"设置batch\",{\"1\":{\"83\":1}}],[\"设置后台运行\",{\"1\":{\"83\":1}}],[\"设置\",{\"0\":{\"447\":1},\"1\":{\"40\":1,\"78\":1}}],[\"∈\",{\"1\":{\"40\":2,\"72\":2,\"73\":1,\"76\":1,\"82\":1,\"160\":1,\"403\":1}}],[\"输入越长\",{\"1\":{\"679\":1}}],[\"输入与输出的关系\",{\"1\":{\"542\":1}}],[\"输入会变成如下结构\",{\"1\":{\"540\":1}}],[\"输入数据格式\",{\"1\":{\"520\":1}}],[\"输入格式\",{\"1\":{\"495\":1,\"540\":1}}],[\"输入格式与下一句预测\",{\"1\":{\"495\":1}}],[\"输入由两个文本片段\",{\"1\":{\"493\":1}}],[\"输入词的val\",{\"1\":{\"477\":1}}],[\"输入词的key\",{\"1\":{\"477\":1}}],[\"输入一个\",{\"1\":{\"474\":1}}],[\"输入微小变动影响大\",{\"1\":{\"463\":1}}],[\"输入文档+对话历史+\",{\"1\":{\"454\":1}}],[\"输入文本的嵌入表示\",{\"1\":{\"162\":1}}],[\"输入表示\",{\"1\":{\"454\":1}}],[\"输入通过作者的预训练模型会得到最好的\",{\"1\":{\"444\":1}}],[\"输入序列\",{\"1\":{\"414\":1,\"511\":1,\"544\":1}}],[\"输入调整等\",{\"1\":{\"382\":1}}],[\"输入维度\",{\"0\":{\"310\":1},\"1\":{\"477\":1}}],[\"输入加上经过归一化和\",{\"1\":{\"294\":1}}],[\"输入加上经过归一化和注意力层处理后的输出\",{\"1\":{\"294\":1}}],[\"输入encoder的最左侧部分添加了一个0\",{\"1\":{\"292\":1}}],[\"输入的图像张量\",{\"1\":{\"291\":1}}],[\"输入的图片尺寸必须为224x224\",{\"1\":{\"290\":1}}],[\"输入的图片尺寸并不是自定义的\",{\"1\":{\"290\":1}}],[\"输入样本\",{\"1\":{\"286\":1}}],[\"输入包含三部分\",{\"1\":{\"282\":1}}],[\"输入image\",{\"1\":{\"273\":1}}],[\"输入分辨率保持\",{\"1\":{\"201\":1}}],[\"输入向量表示为\",{\"1\":{\"171\":1}}],[\"输入是结构化的\",{\"1\":{\"445\":1}}],[\"输入是一个\",{\"1\":{\"315\":1}}],[\"输入是一个指令\",{\"1\":{\"224\":1}}],[\"输入是图像块序列\",{\"1\":{\"171\":1}}],[\"输入是原始点云\",{\"1\":{\"107\":1}}],[\"输入做前向传播\",{\"1\":{\"163\":1}}],[\"输入到mlp\",{\"1\":{\"292\":1}}],[\"输入到\",{\"1\":{\"162\":1,\"299\":1}}],[\"输入网页图像\",{\"1\":{\"128\":1}}],[\"输入形式\",{\"1\":{\"112\":1}}],[\"输入标准化\",{\"0\":{\"107\":1}}],[\"输入特征的通道数\",{\"1\":{\"100\":1}}],[\"输入特征维度\",{\"1\":{\"45\":1}}],[\"输入输出示例\",{\"1\":{\"98\":1}}],[\"输入点云变换矩阵\",{\"1\":{\"109\":1}}],[\"输入点云变换网络\",{\"1\":{\"109\":1}}],[\"输入点云可能来自不同角度\",{\"1\":{\"107\":1}}],[\"输入点云可能缺失或含有异常点\",{\"1\":{\"105\":1}}],[\"输入点云可能缺失或包含噪声\",{\"1\":{\"104\":1}}],[\"输入点云数据\",{\"1\":{\"83\":2}}],[\"输入点的特征维度\",{\"1\":{\"92\":1}}],[\"输入\",{\"1\":{\"72\":1,\"73\":1,\"74\":1,\"78\":1,\"82\":2,\"98\":1,\"100\":1,\"101\":1,\"166\":1,\"285\":1,\"308\":1,\"310\":1,\"401\":1,\"402\":1,\"454\":1,\"513\":1,\"557\":1}}],[\"输入为\",{\"1\":{\"46\":1,\"160\":1,\"291\":1,\"470\":1}}],[\"输入图像被分割成\",{\"1\":{\"292\":1}}],[\"输入图像的通道数\",{\"1\":{\"291\":1,\"292\":1}}],[\"输入图像的尺寸\",{\"1\":{\"291\":1,\"292\":1}}],[\"输入图像大小\",{\"1\":{\"147\":1}}],[\"输入图像尺寸\",{\"1\":{\"142\":1}}],[\"输入图像及文本\",{\"1\":{\"128\":1}}],[\"输入图像\",{\"1\":{\"40\":1,\"226\":1,\"227\":1}}],[\"输出解析等都来自这个库\",{\"1\":{\"685\":1}}],[\"输出解析\",{\"1\":{\"684\":1}}],[\"输出解释\",{\"1\":{\"540\":1}}],[\"输出效果\",{\"1\":{\"659\":1}}],[\"输出处理\",{\"1\":{\"650\":1}}],[\"输出层\",{\"1\":{\"548\":1}}],[\"输出后\",{\"1\":{\"544\":1}}],[\"输出做问答预测\",{\"1\":{\"540\":1}}],[\"输出token会与输入tokens\",{\"1\":{\"474\":1}}],[\"输出有害内容\",{\"1\":{\"472\":1}}],[\"输出更符合人类偏好\",{\"1\":{\"471\":1}}],[\"输出更少毒性内容\",{\"1\":{\"471\":1}}],[\"输出相较于标准\",{\"1\":{\"471\":1}}],[\"输出时将\",{\"1\":{\"425\":1}}],[\"输出序列y\",{\"1\":{\"414\":1}}],[\"输出形状\",{\"1\":{\"390\":1}}],[\"输出形状为\",{\"1\":{\"92\":2}}],[\"输出示例\",{\"1\":{\"334\":1}}],[\"输出通道\",{\"1\":{\"291\":1}}],[\"输出一个\",{\"1\":{\"396\":1}}],[\"输出一个固定维度的向量\",{\"1\":{\"247\":1}}],[\"输出一个字符\",{\"1\":{\"83\":1}}],[\"输出不一致时\",{\"1\":{\"228\":1}}],[\"输出是对这个回答的评分\",{\"1\":{\"224\":1}}],[\"输出是模型应该生成的响应\",{\"1\":{\"224\":1}}],[\"输出是一个变换矩阵\",{\"1\":{\"107\":1}}],[\"输出图像特征\",{\"1\":{\"161\":1}}],[\"输出维度为\",{\"1\":{\"513\":1}}],[\"输出维度\",{\"0\":{\"310\":1},\"1\":{\"160\":2}}],[\"输出合成描述\",{\"1\":{\"128\":1}}],[\"输出结果之后\",{\"1\":{\"296\":1}}],[\"输出结果都保持不变\",{\"1\":{\"115\":1}}],[\"输出结果\",{\"1\":{\"100\":1}}],[\"输出最终的插值后特征\",{\"1\":{\"100\":1}}],[\"输出的dot语言示例包含变量节点\",{\"1\":{\"666\":1}}],[\"输出的特征图\",{\"1\":{\"396\":1}}],[\"输出的特征送入一个小型\",{\"1\":{\"98\":1}}],[\"输出的\",{\"1\":{\"163\":1}}],[\"输出的数据集是经过图文对齐质量优化的图文对集合\",{\"1\":{\"140\":1}}],[\"输出的该\",{\"1\":{\"126\":1}}],[\"输出的隐藏状态映射回原始嵌入维度\",{\"1\":{\"44\":1}}],[\"输出类别概率\",{\"1\":{\"98\":1}}],[\"输出类别数\",{\"1\":{\"93\":1}}],[\"输出张量维度为\",{\"1\":{\"474\":1}}],[\"输出张量\",{\"1\":{\"76\":1}}],[\"输出增强后的点特征\",{\"1\":{\"75\":1}}],[\"输出分组标记\",{\"1\":{\"72\":1}}],[\"输出范围\",{\"1\":{\"46\":1,\"82\":1,\"403\":1}}],[\"输出为标量奖励\",{\"1\":{\"470\":1}}],[\"输出为二分类\",{\"1\":{\"160\":1}}],[\"输出为\",{\"1\":{\"46\":1,\"101\":1,\"323\":1}}],[\"输出每个点的类别概率\",{\"1\":{\"101\":1}}],[\"输出每个点的\",{\"1\":{\"46\":2}}],[\"输出每个点云点的\",{\"1\":{\"46\":1}}],[\"输出映射回合适维度\",{\"1\":{\"40\":1}}],[\"输出\",{\"1\":{\"8\":1,\"14\":1,\"40\":1,\"46\":1,\"74\":1,\"91\":1,\"98\":1,\"100\":1,\"109\":1,\"160\":2,\"308\":1,\"310\":1,\"323\":2,\"325\":4,\"326\":3,\"366\":1,\"369\":1,\"372\":2,\"471\":1,\"474\":1,\"513\":1,\"540\":1,\"608\":2,\"617\":1,\"658\":2,\"659\":8,\"660\":4,\"662\":3,\"683\":1}}],[\"不公开\",{\"1\":{\"674\":1}}],[\"不具备\",{\"1\":{\"660\":1}}],[\"不具备生成能力\",{\"1\":{\"542\":1}}],[\"不构建计算图\",{\"1\":{\"658\":1}}],[\"不保留中间结果\",{\"1\":{\"658\":1}}],[\"不当的内存管理可能导致内存泄漏或程序崩溃\",{\"1\":{\"657\":1}}],[\"不幸的是\",{\"1\":{\"597\":1}}],[\"不碰那些太反直觉或病态的集合\",{\"1\":{\"566\":1}}],[\"不使用传统的循环或卷积结构\",{\"1\":{\"548\":1}}],[\"不计入答案\",{\"1\":{\"542\":1}}],[\"不足长度用padding填充\",{\"1\":{\"520\":1}}],[\"不少人很自然地有了这样子的想法\",{\"1\":{\"504\":1}}],[\"不跨文档\",{\"1\":{\"497\":1}}],[\"不再预先固定掩码模式\",{\"1\":{\"495\":1}}],[\"不再这里一一介绍\",{\"1\":{\"421\":1}}],[\"不展开细聊\",{\"1\":{\"477\":1}}],[\"不展开计算细节\",{\"1\":{\"355\":1}}],[\"不为空\",{\"1\":{\"477\":1}}],[\"不启用kv\",{\"1\":{\"477\":1}}],[\"不代表所有潜在用户\",{\"1\":{\"472\":1}}],[\"不如在现有模型上投资对齐方法\",{\"1\":{\"472\":1}}],[\"不如抽样一部分数据来计算损失\",{\"1\":{\"240\":1}}],[\"不遵循指令等问题\",{\"1\":{\"468\":1}}],[\"不利于迁移与泛化\",{\"1\":{\"464\":1}}],[\"不利于高维空间建模\",{\"1\":{\"112\":1}}],[\"不给任何示例\",{\"1\":{\"461\":1}}],[\"不清楚在学习文本表示时\",{\"1\":{\"440\":1}}],[\"不要求互斥\",{\"1\":{\"567\":1}}],[\"不要求原始张量是连续的\",{\"1\":{\"385\":1}}],[\"不要预训练\",{\"1\":{\"449\":1}}],[\"不要让llm只生成最合适的唯一一个结果\",{\"1\":{\"435\":1}}],[\"不变\",{\"1\":{\"423\":1}}],[\"不对应实际的\",{\"1\":{\"397\":1}}],[\"不进行任何量化\",{\"1\":{\"397\":1}}],[\"不进行采样\",{\"1\":{\"92\":1}}],[\"不复制数据\",{\"1\":{\"385\":1}}],[\"不会过滤掉任何词\",{\"1\":{\"410\":1}}],[\"不会只看一个村\",{\"1\":{\"397\":1}}],[\"不会出现在model\",{\"1\":{\"389\":1}}],[\"不会被当作模型的可训练参数\",{\"1\":{\"389\":1}}],[\"不会跟踪计算图\",{\"1\":{\"388\":1}}],[\"不会复制数据\",{\"1\":{\"384\":1}}],[\"不会影响其他环境或系统\",{\"1\":{\"338\":1}}],[\"不是标准差\",{\"1\":{\"586\":1}}],[\"不是像离散情况那样简单地枚举出来的\",{\"1\":{\"566\":1}}],[\"不是自回归生成器\",{\"1\":{\"542\":1}}],[\"不是三角函数\",{\"1\":{\"506\":1}}],[\"不是视图\",{\"1\":{\"386\":1}}],[\"不是函数原文档\",{\"1\":{\"372\":1}}],[\"不是\",{\"1\":{\"372\":1}}],[\"不是一千个类别\",{\"1\":{\"235\":1}}],[\"不相关\",{\"1\":{\"355\":1}}],[\"不过迭代次数较多\",{\"1\":{\"667\":1}}],[\"不过一个更恰当的术语可能是\",{\"1\":{\"586\":1}}],[\"不过\",{\"1\":{\"343\":1}}],[\"不过这种标签信息\",{\"1\":{\"234\":1}}],[\"不平衡数据集的一个示例可能是一组数以千计的云彩照片\",{\"1\":{\"342\":1}}],[\"不管理\",{\"1\":{\"338\":1}}],[\"不一定是字节\",{\"1\":{\"323\":1}}],[\"不一致\",{\"1\":{\"241\":1}}],[\"不一致则重新随机选\",{\"1\":{\"29\":1}}],[\"不就是分类操作吗\",{\"1\":{\"238\":1}}],[\"不做预训练\",{\"1\":{\"229\":1}}],[\"不做任何变化\",{\"1\":{\"107\":1}}],[\"不含\",{\"1\":{\"202\":1}}],[\"不修改原始函数代码的前提下\",{\"1\":{\"368\":1}}],[\"不修改\",{\"1\":{\"163\":1}}],[\"不匹配\",{\"1\":{\"162\":1}}],[\"不考虑元素之间的顺序\",{\"0\":{\"601\":1}}],[\"不考虑各维度数据的分布特征\",{\"1\":{\"357\":1}}],[\"不考虑从动量队列拿到的负样本\",{\"1\":{\"162\":1}}],[\"不考虑空间邻域关系\",{\"1\":{\"107\":1}}],[\"不参与反向传播\",{\"1\":{\"161\":1}}],[\"不可靠\",{\"1\":{\"159\":1}}],[\"不包括\",{\"1\":{\"117\":1}}],[\"不改变形状和大小\",{\"1\":{\"117\":1}}],[\"不改变物体形状和内部结构\",{\"1\":{\"116\":1}}],[\"不改变\",{\"1\":{\"116\":1}}],[\"不常用\",{\"1\":{\"115\":1,\"163\":1,\"540\":1}}],[\"不够大\",{\"1\":{\"112\":1}}],[\"不够精细\",{\"1\":{\"112\":1}}],[\"不稳定\",{\"1\":{\"107\":1}}],[\"不同的\",{\"1\":{\"504\":1}}],[\"不同的措辞\",{\"1\":{\"463\":1}}],[\"不同的消融研究如下表5\",{\"1\":{\"449\":1}}],[\"不同的分类角度\",{\"1\":{\"416\":1}}],[\"不同类别的\",{\"1\":{\"355\":1}}],[\"不同类别的物体处于不相邻的区域\",{\"1\":{\"234\":1}}],[\"不同特征之间能够进行更多样的组合\",{\"1\":{\"294\":1}}],[\"不同模态数据的提取与融合\",{\"1\":{\"281\":1}}],[\"不同配置对性能影响较小\",{\"1\":{\"189\":1}}],[\"不同\",{\"1\":{\"166\":1,\"480\":1}}],[\"不同位置\",{\"1\":{\"107\":1}}],[\"不同尺度的查询半径列表\",{\"1\":{\"96\":1}}],[\"不同尺度的特征被串联形成多尺度特征向量\",{\"1\":{\"95\":1}}],[\"不灵活等问题\",{\"1\":{\"103\":1}}],[\"不带法向量或其他属性\",{\"1\":{\"101\":1}}],[\"不能\",{\"1\":{\"542\":1}}],[\"不能超出上下文范围\",{\"1\":{\"542\":1}}],[\"不能像\",{\"1\":{\"542\":1}}],[\"不能用\",{\"1\":{\"426\":1}}],[\"不能变成\",{\"1\":{\"387\":1}}],[\"不能直接使用这些操作\",{\"1\":{\"100\":1}}],[\"不能作为\",{\"1\":{\"82\":1}}],[\"不断对点云进行下采样\",{\"1\":{\"99\":1}}],[\"不通过缓存直接将内容打印到屏幕上\",{\"1\":{\"83\":1}}],[\"不支持\",{\"1\":{\"82\":1,\"387\":1}}],[\"不直接优化最终目标\",{\"1\":{\"82\":1}}],[\"不像\",{\"1\":{\"82\":1,\"401\":1,\"403\":1}}],[\"不仅限于英语\",{\"1\":{\"675\":1}}],[\"不仅具备了现代框架应有的可视化与控制能力\",{\"1\":{\"665\":1}}],[\"不仅因为其前向计算功能\",{\"1\":{\"650\":1}}],[\"不仅在分类任务上逼近了有监督的基线模型\",{\"1\":{\"233\":1}}],[\"不仅考虑从当前分辨率下抽象得到的特征\",{\"1\":{\"97\":1}}],[\"不仅理解语言指令\",{\"1\":{\"82\":1}}],[\"不仅理解自己的语义\",{\"1\":{\"76\":1}}],[\"不仅看交集\",{\"1\":{\"82\":1}}],[\"不需要先\",{\"1\":{\"401\":1}}],[\"不需要创建类的实例\",{\"1\":{\"289\":1}}],[\"不需要额外增加一个视觉encoder\",{\"1\":{\"254\":1,\"257\":1}}],[\"不需要额外标注数据\",{\"1\":{\"231\":1}}],[\"不需要知道第三张图片是狗这个类别\",{\"1\":{\"234\":1}}],[\"不需要\",{\"1\":{\"82\":1}}],[\"不依赖绝对数量\",{\"1\":{\"402\":1}}],[\"不依赖目标检测器\",{\"1\":{\"149\":1}}],[\"不依赖特定阈值\",{\"1\":{\"82\":1}}],[\"不依赖\",{\"1\":{\"82\":1}}],[\"不加\",{\"1\":{\"75\":1}}],[\"不在训练中透露\",{\"1\":{\"66\":1}}],[\"不方便进行处理\",{\"1\":{\"38\":1}}],[\"待微调的参数量下降到原来的9\",{\"1\":{\"423\":1}}],[\"待完善\",{\"0\":{\"38\":1}}],[\"待预测功能区域类型列表\",{\"1\":{\"29\":1}}],[\"待预测功能区域类型\",{\"1\":{\"29\":1}}],[\"待预测功能区域类型全部隐含在了样本对应的文件路径中\",{\"1\":{\"29\":1}}],[\"由多层\",{\"1\":{\"477\":1}}],[\"由标注者排序\",{\"1\":{\"470\":1}}],[\"由人类标注者根据偏好进行排序\",{\"1\":{\"470\":1}}],[\"由openai团队提出\",{\"1\":{\"452\":1}}],[\"由何恺明团队在2017年论文\",{\"1\":{\"404\":1}}],[\"由权重\",{\"1\":{\"395\":1}}],[\"由协方差控制\",{\"1\":{\"355\":1}}],[\"由此诱导出的\",{\"1\":{\"585\":1}}],[\"由此提出低秩自适应\",{\"1\":{\"424\":1}}],[\"由此可见vit工作的局限性\",{\"1\":{\"297\":1}}],[\"由此把图片转换为序列的embedding形式\",{\"1\":{\"291\":1}}],[\"由两个transformer模块组成\",{\"1\":{\"282\":1}}],[\"由三角形面片组成的\",{\"1\":{\"114\":1}}],[\"由\",{\"1\":{\"72\":1,\"73\":1,\"88\":1,\"510\":1}}],[\"由于大模型应用需要进行向量语义检索\",{\"1\":{\"687\":1}}],[\"由于与人类交流的出色能力\",{\"1\":{\"674\":1}}],[\"由于for语句反复使用variable实例x0和x1求导\",{\"1\":{\"667\":1}}],[\"由于减法不满足交换律\",{\"1\":{\"660\":1}}],[\"由于乘法满足交换律\",{\"1\":{\"660\":2}}],[\"由于目前variable\",{\"1\":{\"655\":1}}],[\"由于目前只支持竖线形状的计算图\",{\"1\":{\"638\":1}}],[\"由于目标功能区域的尺度\",{\"1\":{\"71\":1}}],[\"由于概率密度函数的峰值\",{\"1\":{\"591\":1}}],[\"由于该集合是不可数的\",{\"1\":{\"566\":1}}],[\"由于模型输出的logits\",{\"1\":{\"514\":1}}],[\"由于掩码模式是固定的\",{\"1\":{\"495\":1}}],[\"由于bert训练时会多次遍历数据\",{\"1\":{\"495\":1}}],[\"由于不涉及kv\",{\"1\":{\"477\":1}}],[\"由于gpt\",{\"1\":{\"455\":1}}],[\"由于矩阵\",{\"1\":{\"426\":1}}],[\"由于需要对每个像素进行分类\",{\"1\":{\"399\":1}}],[\"由于坐标是浮点数\",{\"1\":{\"397\":1}}],[\"由于特征图的尺寸比原图小\",{\"1\":{\"396\":1}}],[\"由于准确率包含混淆矩阵中的所有四种结果\",{\"1\":{\"343\":1}}],[\"由于使用了\",{\"1\":{\"326\":1}}],[\"由于作者是首次将transformer应用到图像领域\",{\"1\":{\"298\":1}}],[\"由于训练数据量和模型计算量较大\",{\"1\":{\"278\":1}}],[\"由于它们在预训练数据集上采用固定类别数的分类器\",{\"1\":{\"278\":1}}],[\"由于这些文本往往只是一个单词\",{\"1\":{\"274\":1}}],[\"由于数据量巨大\",{\"1\":{\"272\":1}}],[\"由于正样本的定义规则是经过编码器之后所在的语义空间才为正样本\",{\"1\":{\"241\":1}}],[\"由于第一阶段中已获得了强大的表示能力\",{\"1\":{\"191\":1}}],[\"由于\",{\"1\":{\"190\":1,\"286\":1,\"477\":1,\"548\":1,\"590\":1}}],[\"由于潜在视觉标记是离散的\",{\"1\":{\"170\":1}}],[\"由于图文对来自自然语言描述\",{\"1\":{\"145\":1}}],[\"由于经过\",{\"1\":{\"136\":1}}],[\"由于高质量人工标注图文对\",{\"1\":{\"128\":1}}],[\"由于表情不同\",{\"1\":{\"112\":1}}],[\"由于子区域在计算第一个向量时包含的点更稀疏\",{\"1\":{\"97\":1}}],[\"由于点集在不同区域可能会有不同的采样密度\",{\"1\":{\"94\":1}}],[\"由于论文数据集还未开源\",{\"1\":{\"37\":1}}],[\"由成对的\",{\"1\":{\"16\":1}}],[\"加法运算的add类已在上文中实现\",{\"1\":{\"660\":1}}],[\"加法运算的运算符重载\",{\"1\":{\"660\":1}}],[\"加法规则\",{\"1\":{\"567\":1}}],[\"加起来\",{\"1\":{\"569\":1}}],[\"加速收敛\",{\"1\":{\"444\":1}}],[\"加\",{\"1\":{\"266\":1}}],[\"加单位矩阵\",{\"1\":{\"107\":1}}],[\"加入到验证集中\",{\"1\":{\"686\":1}}],[\"加入数值稳定性处理\",{\"1\":{\"407\":1}}],[\"加入数据增强后缓解\",{\"1\":{\"112\":1}}],[\"加入平滑项防止除以零\",{\"1\":{\"403\":1}}],[\"加入动量队列中的样本作为负样本\",{\"1\":{\"161\":1}}],[\"加入正则项约束变换矩阵接近正交\",{\"1\":{\"105\":1}}],[\"加入\",{\"1\":{\"75\":2,\"107\":1,\"401\":1,\"402\":2}}],[\"加上参数规模的扩展\",{\"1\":{\"461\":1}}],[\"加上这样一句话\",{\"1\":{\"433\":1}}],[\"加上位置嵌入并进行随机丢弃\",{\"1\":{\"293\":1,\"296\":1}}],[\"加上全局特征后\",{\"1\":{\"111\":1}}],[\"加上单位矩阵作为初始偏置\",{\"1\":{\"107\":1}}],[\"加上原始\",{\"1\":{\"72\":1}}],[\"加上经典的\",{\"1\":{\"36\":1}}],[\"加权惩罚项\",{\"1\":{\"405\":1}}],[\"加权\",{\"1\":{\"403\":1,\"444\":1}}],[\"加权标准化器\",{\"1\":{\"359\":1}}],[\"加权和\",{\"1\":{\"115\":1}}],[\"加权平均插值\",{\"1\":{\"100\":1}}],[\"加权平均系数\",{\"1\":{\"100\":1}}],[\"加权交叉熵损失\",{\"1\":{\"407\":1}}],[\"加权交叉熵\",{\"1\":{\"78\":1}}],[\"加权后的输出\",{\"1\":{\"45\":2}}],[\"加权总损失\",{\"1\":{\"40\":1}}],[\"加之原本在github上开源的代码后续被下架\",{\"1\":{\"37\":1}}],[\"加载检查点\",{\"1\":{\"514\":1}}],[\"加载该模型后\",{\"1\":{\"300\":1}}],[\"加载模型和\",{\"1\":{\"474\":1}}],[\"加载模型和处理器\",{\"1\":{\"275\":1,\"277\":1}}],[\"加载模型配置\",{\"1\":{\"83\":1}}],[\"加载文本编码器\",{\"1\":{\"160\":2}}],[\"加载\",{\"1\":{\"142\":1,\"147\":1}}],[\"加载数据\",{\"1\":{\"142\":1}}],[\"加载预训练好的vit\",{\"1\":{\"300\":1}}],[\"加载预训练模型\",{\"0\":{\"300\":1}}],[\"加载预训练视觉编码器与文本解码器\",{\"1\":{\"142\":1}}],[\"加载预训练权重\",{\"1\":{\"83\":1}}],[\"加载预训练多模态大模型\",{\"1\":{\"28\":1}}],[\"加载训练集\",{\"1\":{\"80\":1}}],[\"加载的标注数据中每个样本的组织形式如下\",{\"1\":{\"68\":1}}],[\"加载58种物体\",{\"1\":{\"68\":1}}],[\"加载标注数据\",{\"1\":{\"68\":1,\"83\":1}}],[\"加载点云数据\",{\"1\":{\"68\":1,\"83\":1}}],[\"加载点云数据和功能区域掩码\",{\"1\":{\"29\":1}}],[\"加载点云\",{\"1\":{\"58\":1}}],[\"加载点云样本\",{\"1\":{\"29\":1}}],[\"加载列表中所有点云样本\",{\"1\":{\"29\":1}}],[\"加载图像数据\",{\"1\":{\"28\":1}}],[\"先定义计算图再执行\",{\"1\":{\"662\":1}}],[\"先进先出\",{\"1\":{\"655\":1}}],[\"先进行\",{\"1\":{\"477\":1}}],[\"先验分布\",{\"1\":{\"597\":1}}],[\"先有预期\",{\"1\":{\"596\":1}}],[\"先通过\",{\"1\":{\"404\":1}}],[\"先把目标区域平均切成小格子\",{\"1\":{\"397\":1}}],[\"先应用最内层的\",{\"1\":{\"376\":1}}],[\"先返回\",{\"1\":{\"371\":1}}],[\"先计算每一维的均值\",{\"1\":{\"355\":1}}],[\"先激活你的conda环境\",{\"1\":{\"338\":1}}],[\"先经过一层或多层\",{\"1\":{\"299\":1}}],[\"先交给预输出层进行处理\",{\"1\":{\"296\":1}}],[\"先预训练好一个骨干模型\",{\"1\":{\"237\":1}}],[\"先生成答案再推理\",{\"1\":{\"229\":1}}],[\"先用\",{\"1\":{\"299\":1}}],[\"先用自定义\",{\"1\":{\"202\":1}}],[\"先用对比学习对齐图文表示\",{\"1\":{\"150\":1}}],[\"先在大规模噪声数据上对比学习\",{\"1\":{\"181\":1}}],[\"先在\",{\"1\":{\"175\":1}}],[\"先以\",{\"1\":{\"159\":1,\"202\":1}}],[\"先对齐后融合\",{\"1\":{\"149\":1}}],[\"先升维后降维\",{\"1\":{\"59\":1}}],[\"先降维\",{\"1\":{\"36\":1}}],[\"先了解一下great项目对应的数据集目录结构\",{\"1\":{\"29\":1}}],[\"逐位置前馈神经网络\",{\"1\":{\"447\":1}}],[\"逐元素相加然后送入线性输出层\",{\"1\":{\"445\":1}}],[\"逐列\",{\"1\":{\"325\":1}}],[\"逐渐丰富了其功能\",{\"1\":{\"674\":1}}],[\"逐渐引入\",{\"1\":{\"159\":1}}],[\"逐渐恢复点数\",{\"1\":{\"98\":1}}],[\"逐点\",{\"1\":{\"107\":1,\"402\":1}}],[\"逐点误差\",{\"1\":{\"82\":1}}],[\"逐层提取边缘→纹理→部件→物体\",{\"1\":{\"395\":1}}],[\"逐层融合上下文信息\",{\"1\":{\"101\":1}}],[\"逐层恢复到原始点数\",{\"1\":{\"99\":1}}],[\"逐层将特征插值回原始点数量\",{\"1\":{\"98\":1}}],[\"逐层抽象后融合成全局特征\",{\"1\":{\"96\":1}}],[\"逐层点云特征列表\",{\"1\":{\"35\":1}}],[\"逐步进行完善和优化\",{\"1\":{\"687\":1}}],[\"逐步推理\",{\"1\":{\"676\":1}}],[\"逐步合并高频的字符对\",{\"1\":{\"409\":1}}],[\"逐步压缩信息\",{\"1\":{\"395\":1}}],[\"逐步聚合全局信息\",{\"1\":{\"292\":1}}],[\"逐步缩小差距\",{\"1\":{\"212\":1}}],[\"逐步提升模型性能\",{\"1\":{\"181\":1}}],[\"逐步向正交矩阵靠拢\",{\"1\":{\"107\":1}}],[\"逐步减少点的数量\",{\"1\":{\"98\":1}}],[\"逐步以自上而下的方式细化点特征图\",{\"1\":{\"71\":1}}],[\"逐步分析交互图像中的几何属性和交互意图\",{\"1\":{\"5\":1}}],[\"逐级做点集抽象得到的每层的点集坐标和点集特征集合\",{\"1\":{\"70\":1}}],[\"逐个拼接\",{\"1\":{\"43\":1}}],[\"通义\",{\"1\":{\"674\":1}}],[\"通义千问是由阿里巴巴基于\",{\"1\":{\"674\":1}}],[\"通义千问\",{\"1\":{\"673\":1,\"674\":2}}],[\"通俗易懂讲解lora微调\",{\"0\":{\"422\":1},\"1\":{\"422\":1}}],[\"通俗易懂解读bpe分词算法实现\",{\"0\":{\"409\":1},\"1\":{\"409\":1}}],[\"通俗讲人话\",{\"1\":{\"420\":1}}],[\"通用大模型的架构\",{\"1\":{\"687\":1}}],[\"通用人工智能\",{\"1\":{\"678\":1}}],[\"通用网络层封装与模型训练流程构建\",{\"1\":{\"669\":1}}],[\"通用的任务未知task\",{\"1\":{\"439\":1}}],[\"通用近似定理中\",{\"1\":{\"395\":1}}],[\"通用近似定理\",{\"0\":{\"395\":1},\"1\":{\"395\":1}}],[\"通用视觉助手\",{\"1\":{\"227\":1}}],[\"通用任务\",{\"1\":{\"221\":1}}],[\"通用多模态任务\",{\"1\":{\"220\":1}}],[\"通常需要重新训练来保持知识和数据的更新\",{\"1\":{\"681\":1}}],[\"通常需要使用高性能的\",{\"1\":{\"675\":1}}],[\"通常需要数千到数十万个标注样本\",{\"1\":{\"461\":1}}],[\"通常具有巨大的参数规模\",{\"1\":{\"675\":1}}],[\"通常大模型由三个阶段构成\",{\"1\":{\"673\":1}}],[\"通常指包含数百亿\",{\"1\":{\"673\":1}}],[\"通常用于实现完整功能\",{\"1\":{\"661\":1}}],[\"通常用于分割模型中\",{\"1\":{\"402\":1}}],[\"通常在内存不足或满足特定条件时才会触发\",{\"1\":{\"657\":1}}],[\"通常在大规模数据集\",{\"1\":{\"290\":1}}],[\"通常这些开源的大模型都是需要用自有数据进行微调\",{\"1\":{\"415\":1}}],[\"通常问题不大\",{\"1\":{\"415\":1}}],[\"通常最后会将预训练生成的频次表和词汇表写入文件保存\",{\"1\":{\"410\":1}}],[\"通常最好优先考虑召回率\",{\"1\":{\"347\":1}}],[\"通常\",{\"1\":{\"404\":2,\"415\":1,\"434\":1}}],[\"通常加入平滑项\",{\"1\":{\"403\":1}}],[\"通常意味着对各个轴进行更一般的排列组合\",{\"1\":{\"326\":1}}],[\"通常使用预训练的目标检测器\",{\"1\":{\"253\":1}}],[\"通常使用轻量级的\",{\"1\":{\"181\":1}}],[\"通常会用\",{\"1\":{\"381\":1}}],[\"通常会加入一个\",{\"1\":{\"240\":1}}],[\"通常会设置一个上限k\",{\"1\":{\"90\":1}}],[\"通常保留原始结构\",{\"1\":{\"231\":1}}],[\"通常是模糊的\",{\"1\":{\"597\":1}}],[\"通常是在1000亿参数\",{\"1\":{\"434\":1}}],[\"通常是原图的\",{\"1\":{\"396\":1}}],[\"通常是字节\",{\"1\":{\"321\":1}}],[\"通常是\",{\"1\":{\"297\":1,\"541\":1}}],[\"通常是随机初始化或者初始化为零\",{\"1\":{\"293\":1}}],[\"通常是图像内容的全面视觉描述\",{\"1\":{\"226\":1}}],[\"通常是旋转或反射\",{\"1\":{\"107\":1}}],[\"通常采用对比学习预训练的\",{\"1\":{\"215\":1}}],[\"通常仅3亿参数\",{\"1\":{\"208\":1}}],[\"通常比卷积神经网络\",{\"1\":{\"166\":1}}],[\"通常为4个采样点\",{\"1\":{\"397\":1}}],[\"通常为全\",{\"1\":{\"162\":1}}],[\"通常为\",{\"1\":{\"160\":1}}],[\"通常依赖预训练教师\",{\"1\":{\"150\":1}}],[\"通常我们会使用多个\",{\"1\":{\"82\":1}}],[\"通道维度特征提取阶段\",{\"1\":{\"107\":1}}],[\"通道维度完成拼接后\",{\"1\":{\"59\":1}}],[\"通道维度作信息融合\",{\"1\":{\"36\":1}}],[\"通道维度上进行特征融合\",{\"1\":{\"34\":1,\"35\":1}}],[\"通过与\",{\"1\":{\"684\":1}}],[\"通过与其他\",{\"1\":{\"292\":1}}],[\"通过在训练集上训练模型\",{\"1\":{\"686\":1}}],[\"通过在特定数据集上进一步训练大语言模型\",{\"1\":{\"681\":1}}],[\"通过在长篇连续文本的多样化语料库上预训练\",{\"1\":{\"450\":1}}],[\"通过提供额外的背景知识和数据支持\",{\"1\":{\"679\":1}}],[\"通过提示词让它自己做任务\",{\"1\":{\"231\":1}}],[\"通过检索到的真实信息生成回答\",{\"1\":{\"681\":1}}],[\"通过检索和整合长文本信息\",{\"1\":{\"679\":1}}],[\"通过检索特定领域的相关文档\",{\"1\":{\"679\":1}}],[\"通过检索数据源\",{\"1\":{\"679\":1}}],[\"通过实时检索最新数据\",{\"1\":{\"679\":1}}],[\"通过采用\",{\"1\":{\"676\":1}}],[\"通过使用自然语言描述的多任务数据进行微调\",{\"1\":{\"676\":1}}],[\"通过使用自身参数的滑动平均作为教师\",{\"1\":{\"150\":1}}],[\"通过理解上下文并生成相应输出的方式来执行任务\",{\"1\":{\"676\":1}}],[\"通过稀疏计算以经济的成本训练强大的模型\",{\"1\":{\"674\":1}}],[\"通过改进的对齐技术\",{\"1\":{\"674\":1}}],[\"通过backward\",{\"1\":{\"667\":1}}],[\"通过seen\",{\"1\":{\"666\":1}}],[\"通过softmax函数转换后\",{\"1\":{\"273\":1}}],[\"通过homebrew执行\",{\"1\":{\"666\":1}}],[\"通过config类和no\",{\"1\":{\"663\":1}}],[\"通过cmafm模块将几何属性\",{\"1\":{\"6\":1}}],[\"通过列表处理可变长参数\",{\"1\":{\"663\":1}}],[\"通过梯度检验可知结果正确\",{\"1\":{\"662\":1}}],[\"通过运算符重载\",{\"1\":{\"662\":1}}],[\"通过步骤20\",{\"1\":{\"660\":1}}],[\"通过组合加法和乘法运算符\",{\"1\":{\"660\":1}}],[\"通过组合不同组件\",{\"1\":{\"188\":1}}],[\"通过retain\",{\"1\":{\"658\":1}}],[\"通过roi\",{\"1\":{\"54\":1}}],[\"通过output\",{\"1\":{\"657\":1}}],[\"通过弱引用主动打破function与variable之间的循环引用\",{\"1\":{\"657\":1}}],[\"通过以上修改\",{\"1\":{\"656\":1}}],[\"通过以下改进缩小差距\",{\"1\":{\"208\":1}}],[\"通过以下两个核心组件实现跨模态深度协同\",{\"1\":{\"188\":1}}],[\"通过以下关键设计解决上述问题\",{\"1\":{\"181\":1}}],[\"通过循环替代递归\",{\"1\":{\"638\":1}}],[\"通过递归或循环遍历计算图\",{\"1\":{\"635\":1}}],[\"通过计算图可以直观地表示变量与函数的关系\",{\"1\":{\"611\":1}}],[\"通过计算损失\",{\"1\":{\"292\":1}}],[\"通过观察结果\",{\"1\":{\"597\":1}}],[\"通过对复杂测试函数求导\",{\"1\":{\"662\":1}}],[\"通过对每个\",{\"1\":{\"596\":1}}],[\"通过对比学习在大规模图文对中对齐表示\",{\"1\":{\"150\":1}}],[\"通过对比学习先对齐图像和文本的表示\",{\"1\":{\"149\":1}}],[\"通过反复地做并集\",{\"1\":{\"566\":1}}],[\"通过反向传播算法更新低秩矩阵\",{\"1\":{\"423\":1}}],[\"通过填充掩码\",{\"1\":{\"548\":1}}],[\"通过调用\",{\"1\":{\"540\":1}}],[\"通过调整训练策略\",{\"1\":{\"496\":1}}],[\"通过调整这些关键因素\",{\"1\":{\"493\":1}}],[\"通过调整\",{\"1\":{\"407\":1}}],[\"通过调整步长\",{\"1\":{\"325\":1}}],[\"通过全连接层\",{\"1\":{\"513\":1}}],[\"通过全连接层进行分类\",{\"1\":{\"93\":1}}],[\"通过系统优化bert的预训练策略\",{\"1\":{\"502\":1}}],[\"通过系统性的实验发现bert存在训练不足的问题\",{\"1\":{\"491\":1}}],[\"通过复制数据来增加多样性\",{\"1\":{\"495\":1}}],[\"通过复合实现指数级增长的分段线性区域\",{\"1\":{\"395\":1}}],[\"通过控制数据规模\",{\"1\":{\"494\":1}}],[\"通过优化训练策略\",{\"1\":{\"492\":1}}],[\"通过数据与训练优化达到大模型\",{\"1\":{\"482\":1}}],[\"通过xformers库实现因果多头注意力的高效计算\",{\"1\":{\"481\":1}}],[\"通过触发短语过滤预训练语料\",{\"1\":{\"469\":1}}],[\"通过图1的结果可见\",{\"1\":{\"468\":1}}],[\"通过图像分词器生成\",{\"1\":{\"165\":1}}],[\"通过人类反馈进行强化学习\",{\"1\":{\"468\":1}}],[\"通过人为构造\",{\"1\":{\"240\":1}}],[\"通过微调在多个任务上实现了sota\",{\"1\":{\"464\":1}}],[\"通过大语言模型的强大理解能力和生成能力\",{\"1\":{\"686\":1}}],[\"通过大规模预训练和精心设计的上下文输入\",{\"1\":{\"461\":1}}],[\"通过大规模训练显著提升了少样本学习能力\",{\"1\":{\"459\":1}}],[\"通过大量文本数据训练这些模型\",{\"1\":{\"673\":1}}],[\"通过大量\",{\"1\":{\"231\":1}}],[\"通过添加停用词过滤器\",{\"1\":{\"455\":1}}],[\"通过可逆去token化\",{\"1\":{\"455\":1}}],[\"通过引导llm解决子问题\",{\"1\":{\"436\":1}}],[\"通过引入两个可调节参数来增强模型对假阳性\",{\"1\":{\"405\":1}}],[\"通过引入动量蒸馏\",{\"1\":{\"157\":1}}],[\"通过把一个个的简单问题解决掉\",{\"1\":{\"436\":1}}],[\"通过解决这一系列的简单问题\",{\"1\":{\"436\":1}}],[\"通过增加旁路\",{\"1\":{\"428\":1}}],[\"通过简单有效的方案来达成轻量微调的目的\",{\"1\":{\"428\":1}}],[\"通过降低参数的精度\",{\"1\":{\"421\":1}}],[\"通过强化学习的方式\",{\"1\":{\"416\":1}}],[\"通过插值获得值后聚合\",{\"1\":{\"397\":1}}],[\"通过分析前面的词汇来预测下一个词汇\",{\"1\":{\"673\":1}}],[\"通过分层降维\",{\"1\":{\"395\":1}}],[\"通过分层组合简单函数\",{\"1\":{\"395\":1}}],[\"通过分类头\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"通过逐层抽象\",{\"1\":{\"395\":1}}],[\"通过协方差矩阵找出\",{\"1\":{\"355\":1}}],[\"通过encoder\",{\"1\":{\"296\":1}}],[\"通过投影层对合并后的张量进行线性变换\",{\"1\":{\"295\":1}}],[\"通过qkv线性层将输入x映射到dim\",{\"1\":{\"295\":1}}],[\"通过qllama实现动态特征交互\",{\"1\":{\"188\":1}}],[\"通过第一阶段的10个步骤\",{\"1\":{\"648\":1}}],[\"通过第一个全连接层\",{\"1\":{\"294\":1}}],[\"通过第二个全连接层\",{\"1\":{\"294\":1}}],[\"通过激活函数层\",{\"1\":{\"294\":1}}],[\"通过随机裁剪可以增加训练数据的多样性\",{\"1\":{\"290\":1}}],[\"通过随机丢弃来模拟不同密度的采样\",{\"1\":{\"96\":1}}],[\"通过视觉编码器\",{\"1\":{\"286\":1}}],[\"通过视觉编码器提取图像的视觉特征\",{\"1\":{\"143\":1}}],[\"通过学习image\",{\"1\":{\"284\":1}}],[\"通过一些自动化的手段将web\",{\"1\":{\"278\":1}}],[\"通过模型获取图片的特征嵌入\",{\"1\":{\"275\":1}}],[\"通过linear\",{\"1\":{\"257\":1}}],[\"通过word\",{\"1\":{\"257\":1}}],[\"通过卷积提取每个网格的视觉特征\",{\"1\":{\"253\":1}}],[\"通过设计合理的预训练任务\",{\"1\":{\"240\":1}}],[\"通过这\",{\"1\":{\"650\":1,\"665\":1}}],[\"通过这些区间的并\",{\"1\":{\"566\":1}}],[\"通过这种方式\",{\"1\":{\"238\":1,\"273\":1}}],[\"通过这个阶段训练后\",{\"1\":{\"226\":1}}],[\"通过结合人工标注数据和强化学习\",{\"1\":{\"224\":1}}],[\"通过结合描述交互的图像或语言与3d几何结构引入外部先验\",{\"1\":{\"6\":1}}],[\"通过机器生成的指令数据进行调优\",{\"1\":{\"224\":1}}],[\"通过持续优化的视觉编码器internvit\",{\"1\":{\"222\":1}}],[\"通过持续学习优化\",{\"1\":{\"214\":1}}],[\"通过持续学习策略增强视觉理解能力\",{\"1\":{\"212\":1}}],[\"通过持续学习策略优化大规模视觉基础模型\",{\"1\":{\"207\":1}}],[\"通过翻译管道\",{\"1\":{\"217\":1}}],[\"通过中英文问答对标注\",{\"1\":{\"208\":1}}],[\"通过将核心组件\",{\"1\":{\"684\":1}}],[\"通过将键值\",{\"1\":{\"674\":1}}],[\"通过将查询\",{\"1\":{\"674\":2}}],[\"通过将图像切成小片\",{\"1\":{\"301\":1}}],[\"通过将特征映射到特定的维度并进行非线性变换\",{\"1\":{\"296\":1}}],[\"通过将视觉基础模型扩展到\",{\"1\":{\"198\":1}}],[\"通过将局部区域中的每个点\",{\"1\":{\"91\":1}}],[\"通过最小化配置\",{\"1\":{\"196\":1}}],[\"通过注意力池化生成全局特征\",{\"1\":{\"189\":1}}],[\"通过预训练权重实现视觉特征到llm表示的对齐\",{\"1\":{\"189\":1}}],[\"通过超参数搜索\",{\"1\":{\"189\":1}}],[\"通过超参数搜索优化了深度\",{\"1\":{\"188\":1}}],[\"通过扩大模型规模和参数量\",{\"1\":{\"183\":1}}],[\"通过12b参数的稀疏transformer学习联合分布\",{\"1\":{\"178\":1}}],[\"通过两阶段训练流程实现\",{\"1\":{\"178\":1}}],[\"通过自注意力机制与其他\",{\"1\":{\"292\":1}}],[\"通过自注意力建模模态内结构关系\",{\"1\":{\"54\":1}}],[\"通过自监督学习中的掩码图像建模\",{\"1\":{\"167\":1}}],[\"通过高层语义的离散标记学习更有效的视觉表示\",{\"1\":{\"165\":1}}],[\"通过pointnet获取每个形心多尺度信息\",{\"1\":{\"96\":1}}],[\"通过较大的邻域尺度避免过度稀疏的问题\",{\"1\":{\"95\":1}}],[\"通过较小的邻域尺度捕获细节\",{\"1\":{\"95\":1}}],[\"通过多任务微调训练语言模型\",{\"1\":{\"469\":1}}],[\"通过多个局部区域球查询提取不同尺度的局部特征\",{\"1\":{\"96\":1}}],[\"通过多层\",{\"1\":{\"93\":1,\"292\":1}}],[\"通过多头可供性链式思维\",{\"1\":{\"26\":1}}],[\"通过查找形心点周围的\",{\"1\":{\"88\":1}}],[\"通过局部特征学习器\",{\"1\":{\"86\":1}}],[\"通过\",{\"1\":{\"76\":2,\"105\":1,\"107\":1,\"122\":1,\"129\":1,\"190\":1,\"217\":1,\"285\":2,\"404\":1,\"455\":1,\"470\":1,\"471\":1,\"686\":3}}],[\"通过共享mlp迫使图像和点云特征在相同空间分布\",{\"1\":{\"59\":1}}],[\"通过瓶颈结构\",{\"1\":{\"59\":1}}],[\"通过mlp\",{\"1\":{\"59\":1}}],[\"通过双路交叉注意力分别建模物体\",{\"1\":{\"56\":1}}],[\"通过跨模态相似性\",{\"1\":{\"56\":1}}],[\"通过特征传播层上采样\",{\"1\":{\"54\":1}}],[\"通过智能体主动交互学习\",{\"1\":{\"51\":1}}],[\"通过智能体在虚拟环境中主动交互学习功能\",{\"1\":{\"49\":1}}],[\"通过密集跨模态相似性\",{\"1\":{\"49\":1}}],[\"通过标注物体交互区域建立几何结构与功能的固定关联\",{\"1\":{\"49\":1}}],[\"通过联合区域对齐模块\",{\"1\":{\"48\":1}}],[\"通过2d图像中的交互信息来预测3d物体的功能区域\",{\"1\":{\"48\":1}}],[\"通过适配器层将\",{\"1\":{\"40\":1}}],[\"通过建立意图与几何的显式关联\",{\"1\":{\"32\":1}}],[\"通过为每张图像配对多个点云\",{\"1\":{\"29\":1}}],[\"通过交叉注意力建模物体\",{\"1\":{\"49\":1}}],[\"通过交叉注意力层\",{\"1\":{\"13\":1}}],[\"通过交叉注意力机制计算对象几何特征\",{\"1\":{\"8\":1}}],[\"通过文本条件查询分割功能区域\",{\"1\":{\"7\":1}}],[\"泛化强\",{\"1\":{\"472\":1}}],[\"泛化机制的模糊性\",{\"1\":{\"463\":1}}],[\"泛化差\",{\"1\":{\"149\":1}}],[\"泛化能力有限\",{\"1\":{\"460\":1}}],[\"泛化能力\",{\"1\":{\"65\":1}}],[\"泛化\",{\"1\":{\"33\":1}}],[\"泛化性\",{\"1\":{\"471\":1}}],[\"泛化性就不足\",{\"1\":{\"238\":1}}],[\"泛化性不如\",{\"1\":{\"231\":1}}],[\"泛化性不足\",{\"1\":{\"7\":1}}],[\"泛化性差\",{\"1\":{\"51\":1}}],[\"泛化性下降明显\",{\"1\":{\"24\":1}}],[\"第四阶段共\",{\"1\":{\"670\":1}}],[\"第四阶段\",{\"1\":{\"669\":1}}],[\"第四范式\",{\"1\":{\"421\":1}}],[\"第\",{\"1\":{\"387\":2}}],[\"第3维\",{\"1\":{\"323\":1}}],[\"第3行\",{\"1\":{\"145\":2}}],[\"第2维\",{\"1\":{\"323\":1}}],[\"第2行\",{\"1\":{\"145\":2}}],[\"第1列到第3列\",{\"1\":{\"325\":1}}],[\"第1维\",{\"1\":{\"323\":1}}],[\"第1行\",{\"1\":{\"145\":2}}],[\"第0维\",{\"1\":{\"323\":1}}],[\"第三个实验有\",{\"1\":{\"599\":1}}],[\"第三张是狗\",{\"1\":{\"234\":1}}],[\"第三张图片不是同一个类别的信息\",{\"1\":{\"234\":1}}],[\"第三张图片不是一个类别\",{\"1\":{\"234\":1}}],[\"第三张图片和它们的距离比较远\",{\"1\":{\"234\":1}}],[\"第三张图片是狗\",{\"1\":{\"234\":1}}],[\"第三阶段\",{\"1\":{\"191\":1,\"664\":1}}],[\"第三步\",{\"1\":{\"98\":1}}],[\"第500个样本\",{\"1\":{\"83\":1}}],[\"第二部分是一个移动平均的编码器\",{\"1\":{\"237\":1}}],[\"第二张图片是人悲伤\",{\"1\":{\"234\":1}}],[\"第二步\",{\"1\":{\"98\":1}}],[\"第二个实验有\",{\"1\":{\"599\":1}}],[\"第二个维度的偏差乘上\",{\"1\":{\"359\":1}}],[\"第二个维度方差是\",{\"1\":{\"359\":1}}],[\"第二个维度\",{\"1\":{\"323\":2}}],[\"第二个全连接层\",{\"1\":{\"294\":1}}],[\"第二个归一化层\",{\"1\":{\"294\":1}}],[\"第二个线性层再把高维特征映射回原来的维度\",{\"1\":{\"294\":1}}],[\"第二个参数\",{\"1\":{\"290\":1}}],[\"第二个分支的编码器就更新得太快了\",{\"1\":{\"241\":1}}],[\"第二个分支不能随着走这一支的样本使用梯度回传进行更新\",{\"1\":{\"241\":1}}],[\"第二个是在训练的时候尽量保持一致性\",{\"1\":{\"238\":1}}],[\"第二个\",{\"1\":{\"45\":1,\"159\":1}}],[\"第二分支\",{\"1\":{\"45\":1}}],[\"第二阶段总结\",{\"0\":{\"663\":1}}],[\"第二阶段将从第11步延续\",{\"1\":{\"650\":1}}],[\"第二阶段\",{\"1\":{\"33\":1,\"191\":1,\"649\":1}}],[\"第一次输入完整句子\",{\"1\":{\"477\":1}}],[\"第一部分是一个队列\",{\"1\":{\"237\":1}}],[\"第一张图是人高兴\",{\"1\":{\"234\":1}}],[\"第一层是一个叫做\",{\"1\":{\"107\":1}}],[\"第一步\",{\"1\":{\"98\":1}}],[\"第一个实验有\",{\"1\":{\"599\":1}}],[\"第一个关键点\",{\"1\":{\"432\":1}}],[\"第一个维度方差是\",{\"1\":{\"359\":1}}],[\"第一个维度\",{\"1\":{\"323\":2}}],[\"第一个全连接层\",{\"1\":{\"294\":1}}],[\"第一个归一化层\",{\"1\":{\"294\":1}}],[\"第一个线性层将输入特征映射到更高维度的空间\",{\"1\":{\"294\":1}}],[\"第一个参数\",{\"1\":{\"290\":1}}],[\"第一个基于patch\",{\"1\":{\"253\":1}}],[\"第一个是字典足够大\",{\"1\":{\"238\":1}}],[\"第一个向量提供了更细致的信息\",{\"1\":{\"97\":1}}],[\"第一个点\",{\"1\":{\"92\":1}}],[\"第一个\",{\"1\":{\"45\":1,\"159\":1}}],[\"第一分支\",{\"1\":{\"45\":1}}],[\"第一阶段中\",{\"1\":{\"650\":1}}],[\"第一阶段总结\",{\"0\":{\"648\":1}}],[\"第一阶段共包含10个步骤\",{\"1\":{\"604\":1}}],[\"第一阶段设计了三个训练目标\",{\"1\":{\"282\":1}}],[\"第一阶段\",{\"1\":{\"33\":1,\"191\":1,\"603\":1}}],[\"信息更新成本低\",{\"1\":{\"681\":1}}],[\"信息偏差\",{\"1\":{\"679\":1}}],[\"信息丢失\",{\"1\":{\"396\":1}}],[\"信息\",{\"1\":{\"33\":1}}],[\"把世界事件映射成数字\",{\"1\":{\"565\":1}}],[\"把layernorm放到了前面\",{\"1\":{\"552\":1}}],[\"把llm的慢思考调动起来\",{\"1\":{\"433\":1}}],[\"把新\",{\"1\":{\"477\":1}}],[\"把复杂问题分解成一系列的简单子问题\",{\"1\":{\"436\":1}}],[\"把要求尽可能明确\",{\"1\":{\"432\":1}}],[\"把人类的反馈\",{\"1\":{\"416\":1}}],[\"把原函数的\",{\"1\":{\"372\":1}}],[\"把原始点云\",{\"1\":{\"92\":1}}],[\"把这些数字映射成它们发生的概率\",{\"1\":{\"565\":1}}],[\"把这些\",{\"1\":{\"542\":1}}],[\"把这些方向\",{\"1\":{\"359\":1}}],[\"把这些点的坐标归一化到以质心为中心的局部坐标系下\",{\"1\":{\"96\":1}}],[\"把数据的不同尺度和相关性都考虑进来\",{\"1\":{\"358\":1}}],[\"把数据转换成llm能识别的格式\",{\"1\":{\"281\":1}}],[\"把相似度矩阵对角线元素置为负无穷大\",{\"1\":{\"284\":1}}],[\"把query\",{\"1\":{\"284\":1}}],[\"把q\",{\"1\":{\"282\":1}}],[\"把当前\",{\"1\":{\"238\":1}}],[\"把每次训练的\",{\"1\":{\"238\":1}}],[\"把f12\",{\"1\":{\"238\":1}}],[\"把对比学习看成了是一个字典查询的过程\",{\"1\":{\"237\":1}}],[\"把全局特征复制\",{\"1\":{\"109\":1}}],[\"把它们\",{\"1\":{\"107\":1}}],[\"把它们相对于关键点的位置进行归一化\",{\"1\":{\"92\":1}}],[\"把不同尺度学到的特征拼接在一起\",{\"1\":{\"96\":1}}],[\"把邻域点的数据整理成适合卷积的格式\",{\"1\":{\"92\":1}}],[\"把邻近点的坐标和特征拼接在一起\",{\"1\":{\"92\":1}}],[\"把距离超过\",{\"1\":{\"92\":1}}],[\"把刚才找到的邻近点的坐标提取出来\",{\"1\":{\"92\":1}}],[\"把他的大小归一化到一个球中\",{\"1\":{\"86\":1}}],[\"把\",{\"1\":{\"33\":1,\"240\":1,\"375\":1}}],[\"把手\",{\"1\":{\"32\":1}}],[\"➜\",{\"1\":{\"33\":2}}],[\"qwq\",{\"1\":{\"674\":2,\"688\":2}}],[\"qwen3\",{\"1\":{\"674\":2}}],[\"qwen2\",{\"1\":{\"674\":3}}],[\"qwen\",{\"1\":{\"185\":1,\"194\":1,\"210\":1,\"211\":1,\"674\":4}}],[\"qnli\",{\"1\":{\"499\":1}}],[\"qqp\",{\"1\":{\"448\":1}}],[\"q进行\",{\"1\":{\"428\":1}}],[\"qlora就是量化版的lora\",{\"1\":{\"421\":1}}],[\"qlora\",{\"0\":{\"421\":1},\"1\":{\"421\":1,\"428\":2}}],[\"qllama利用其大规模参数重组视觉表示并生成文本\",{\"1\":{\"189\":1}}],[\"qllama的优势包括\",{\"1\":{\"189\":1}}],[\"qllama是一个80亿参数的语言中间件\",{\"1\":{\"189\":1}}],[\"qllama\",{\"1\":{\"181\":1,\"188\":1,\"189\":1,\"190\":4,\"191\":5,\"194\":1,\"195\":1,\"196\":4,\"197\":1,\"198\":1,\"201\":1,\"202\":2}}],[\"quac\",{\"1\":{\"462\":2}}],[\"quantized\",{\"1\":{\"421\":1,\"428\":1}}],[\"quantization\",{\"1\":{\"397\":1,\"421\":1}}],[\"quora\",{\"1\":{\"448\":2}}],[\"queies\",{\"1\":{\"285\":1}}],[\"queue\",{\"1\":{\"145\":20,\"147\":16,\"160\":13,\"161\":3,\"246\":6,\"247\":1,\"249\":5,\"389\":1,\"390\":1}}],[\"queries被用来从image\",{\"1\":{\"282\":1}}],[\"queries是一组可学习的embeddings\",{\"1\":{\"282\":1}}],[\"queries\",{\"1\":{\"70\":1,\"76\":3,\"191\":1,\"247\":1,\"282\":3,\"283\":1,\"284\":1,\"286\":3}}],[\"query的数量\",{\"1\":{\"305\":1}}],[\"query的响应图进行平均池化\",{\"1\":{\"76\":1}}],[\"query和query\",{\"1\":{\"283\":1}}],[\"query是基于欧氏距离的均匀性假设\",{\"1\":{\"90\":1}}],[\"query通过确保每个局部区域都有一个固定的尺度\",{\"1\":{\"90\":1}}],[\"query通过固定区域尺度而不是固定邻居数量来定义邻域\",{\"1\":{\"90\":1}}],[\"query找到该查询点在半径为𝑟范围内点\",{\"1\":{\"90\":1}}],[\"query来查询形心的邻居点\",{\"1\":{\"90\":1}}],[\"query=self\",{\"1\":{\"76\":1}}],[\"query=x\",{\"1\":{\"75\":1}}],[\"query=q\",{\"1\":{\"75\":1}}],[\"query\",{\"1\":{\"14\":1,\"45\":13,\"70\":1,\"72\":8,\"74\":8,\"76\":10,\"92\":7,\"96\":2,\"98\":1,\"145\":1,\"162\":6,\"191\":1,\"238\":1,\"240\":1,\"242\":1,\"244\":2,\"247\":3,\"264\":1,\"266\":6,\"267\":1,\"276\":4,\"277\":4,\"282\":8,\"283\":2,\"284\":17,\"285\":30,\"286\":5,\"304\":1,\"305\":1,\"307\":1,\"309\":1,\"310\":2,\"312\":2,\"314\":1,\"390\":2,\"477\":7,\"531\":7,\"558\":9,\"674\":4}}],[\"questions这三个问答任务中\",{\"1\":{\"462\":1}}],[\"questions\",{\"1\":{\"455\":1,\"472\":1}}],[\"question数据可视化图\",{\"1\":{\"68\":1}}],[\"question数据可视化\",{\"1\":{\"63\":1}}],[\"question\",{\"0\":{\"63\":1},\"1\":{\"68\":6,\"76\":1,\"81\":2,\"82\":2,\"448\":1,\"454\":1,\"540\":1,\"596\":1}}],[\"question4\",{\"1\":{\"28\":2}}],[\"question3\",{\"1\":{\"28\":2}}],[\"question2\",{\"1\":{\"28\":2}}],[\"question1\",{\"1\":{\"28\":2}}],[\"qk^t\",{\"1\":{\"318\":1}}],[\"qk\",{\"1\":{\"293\":1,\"294\":2,\"295\":2,\"296\":2}}],[\"qkv\",{\"1\":{\"160\":2,\"293\":1,\"294\":2,\"295\":8,\"296\":2,\"425\":1}}],[\"q来自query\",{\"1\":{\"285\":1}}],[\"q2t\",{\"1\":{\"283\":2}}],[\"q=images\",{\"1\":{\"244\":1}}],[\"q=6\",{\"1\":{\"145\":1}}],[\"qa\",{\"1\":{\"227\":1,\"231\":1,\"470\":1,\"482\":1,\"540\":2,\"541\":2,\"542\":1}}],[\"qid\",{\"1\":{\"68\":5}}],[\"qformer\",{\"1\":{\"40\":1,\"196\":1,\"282\":2,\"284\":1,\"285\":3,\"286\":1}}],[\"q\",{\"0\":{\"306\":1},\"1\":{\"32\":8,\"40\":1,\"45\":9,\"70\":1,\"72\":6,\"74\":3,\"75\":5,\"76\":2,\"145\":11,\"147\":2,\"244\":2,\"246\":4,\"247\":9,\"248\":3,\"282\":3,\"285\":2,\"286\":3,\"295\":5,\"304\":1,\"310\":1,\"319\":2,\"390\":2,\"475\":1,\"477\":4,\"517\":3,\"558\":1}}],[\"交换操作数实现a\",{\"1\":{\"660\":2}}],[\"交和补运算\",{\"1\":{\"566\":1}}],[\"交并比\",{\"1\":{\"403\":2}}],[\"交错堆叠\",{\"1\":{\"299\":1}}],[\"交集\",{\"1\":{\"82\":1,\"401\":1,\"566\":1}}],[\"交替作用\",{\"1\":{\"73\":1}}],[\"交叉熵在总损失中的占比越高\",{\"1\":{\"407\":1}}],[\"交叉熵是一个多分类问题的损失函数\",{\"1\":{\"240\":1}}],[\"交叉熵损失为\",{\"1\":{\"404\":1}}],[\"交叉熵损失函数\",{\"1\":{\"240\":1}}],[\"交叉熵损失\",{\"1\":{\"163\":1,\"407\":2}}],[\"交叉熵损失监督\",{\"1\":{\"55\":1}}],[\"交叉注意力运算\",{\"1\":{\"285\":1}}],[\"交叉注意力\",{\"1\":{\"285\":1}}],[\"交叉注意力则key和value都来自图像\",{\"1\":{\"285\":1}}],[\"交叉注意力建模交互上下文\",{\"1\":{\"54\":1}}],[\"交叉注意力模拟了这种双向推理过程\",{\"1\":{\"32\":1}}],[\"交互的应用程序\",{\"1\":{\"685\":1}}],[\"交互层\",{\"1\":{\"683\":1}}],[\"交互式可视化网站\",{\"1\":{\"573\":1}}],[\"交互难度大\",{\"1\":{\"149\":1}}],[\"交互区域特征\",{\"1\":{\"59\":1}}],[\"交互主体区域特征图\",{\"1\":{\"59\":1}}],[\"交互主体区域特征\",{\"1\":{\"59\":2}}],[\"交互主体框\",{\"1\":{\"58\":2}}],[\"交互\",{\"1\":{\"56\":1,\"76\":1}}],[\"交互信息与图像特征的融合\",{\"0\":{\"34\":1}}],[\"交互信息文本\",{\"1\":{\"29\":1}}],[\"交互文本信息与图像信息进行融合\",{\"1\":{\"30\":1}}],[\"交互文本和几何结构文本的信息通过改良的交叉注意力机制进行交互融合\",{\"1\":{\"30\":1}}],[\"交互行为名\",{\"1\":{\"29\":1}}],[\"交互图像\",{\"1\":{\"29\":1}}],[\"交互图像与\",{\"1\":{\"16\":1}}],[\"交互知识\",{\"1\":{\"28\":1}}],[\"交互部位\",{\"1\":{\"28\":1}}],[\"交互类比推理\",{\"1\":{\"12\":1}}],[\"交互细节描述\",{\"1\":{\"12\":1}}],[\"功能增强\",{\"0\":{\"659\":1}}],[\"功能属性匹配\",{\"1\":{\"68\":1}}],[\"功能类别\",{\"1\":{\"68\":2}}],[\"功能类别预测\",{\"1\":{\"54\":1}}],[\"功能类型索引\",{\"1\":{\"68\":1}}],[\"功能类型\",{\"1\":{\"67\":1}}],[\"功能类型数\",{\"1\":{\"65\":1}}],[\"功能组合的标注数据\",{\"1\":{\"68\":1}}],[\"功能组合\",{\"1\":{\"63\":2,\"67\":1}}],[\"功能分类损失\",{\"1\":{\"55\":1}}],[\"功能揭示\",{\"1\":{\"54\":1}}],[\"功能模糊性\",{\"1\":{\"49\":1}}],[\"功能\",{\"1\":{\"49\":1}}],[\"功能可供性\",{\"1\":{\"49\":1}}],[\"功能意图\",{\"1\":{\"32\":1}}],[\"功能区域\",{\"1\":{\"83\":1,\"404\":2}}],[\"功能区域预测结果\",{\"1\":{\"83\":1}}],[\"功能区域识别任务中\",{\"1\":{\"82\":1}}],[\"功能区域分割\",{\"1\":{\"69\":1}}],[\"功能区域掩码\",{\"1\":{\"68\":3}}],[\"功能区域类型\",{\"1\":{\"29\":1}}],[\"功能区域热力图列表\",{\"1\":{\"29\":1}}],[\"功能区域热力图\",{\"1\":{\"29\":2}}],[\"700\",{\"1\":{\"684\":1}}],[\"704\",{\"1\":{\"520\":2}}],[\"70b的55\",{\"1\":{\"482\":1}}],[\"70b\",{\"1\":{\"480\":1,\"482\":2,\"674\":4}}],[\"70b和palm\",{\"1\":{\"479\":1}}],[\"78\",{\"1\":{\"497\":2}}],[\"72b\",{\"1\":{\"674\":1}}],[\"72\",{\"1\":{\"482\":1,\"499\":1}}],[\"79\",{\"1\":{\"482\":1,\"484\":1}}],[\"71\",{\"1\":{\"471\":1}}],[\"73±1\",{\"1\":{\"470\":1}}],[\"7个百分点\",{\"1\":{\"455\":1}}],[\"76gb\",{\"1\":{\"492\":1,\"494\":1}}],[\"762m和1\",{\"1\":{\"455\":1}}],[\"768\",{\"1\":{\"35\":2,\"46\":2,\"59\":2,\"70\":1,\"160\":4,\"174\":1,\"291\":4,\"292\":6,\"293\":4,\"294\":1,\"296\":4,\"447\":1,\"519\":3,\"525\":2}}],[\"7×7\",{\"1\":{\"396\":6,\"397\":3}}],[\"75\",{\"1\":{\"355\":1,\"404\":3}}],[\"751\",{\"1\":{\"65\":1,\"67\":1,\"69\":1}}],[\"7py1jdq1wp0nnyt3a\",{\"1\":{\"289\":1}}],[\"77\",{\"1\":{\"227\":1,\"483\":1,\"497\":1}}],[\"774\",{\"1\":{\"226\":1}}],[\"7b的0\",{\"1\":{\"482\":1,\"484\":1}}],[\"7b在1t\",{\"1\":{\"480\":1}}],[\"7b\",{\"1\":{\"190\":1,\"191\":3,\"200\":2,\"229\":1,\"674\":4}}],[\"7b初始化\",{\"1\":{\"189\":1}}],[\"7b初始化的80亿参数语言中间件\",{\"1\":{\"188\":1}}],[\"7章\",{\"1\":{\"134\":1}}],[\"7\",{\"0\":{\"68\":1,\"296\":1},\"1\":{\"30\":2,\"40\":1,\"58\":1,\"59\":5,\"75\":1,\"82\":1,\"105\":1,\"145\":4,\"161\":1,\"163\":1,\"191\":1,\"237\":1,\"247\":1,\"286\":1,\"321\":2,\"322\":2,\"323\":1,\"325\":2,\"396\":2,\"405\":2,\"410\":1,\"439\":1,\"454\":1,\"455\":4,\"462\":3,\"463\":1,\"469\":1,\"470\":1,\"471\":1,\"474\":1,\"477\":1,\"482\":4,\"484\":1,\"497\":2,\"498\":1,\"499\":1,\"566\":1,\"660\":1,\"674\":6}}],[\"用其来表示函数\",{\"1\":{\"656\":1}}],[\"用方框表示函数\",{\"1\":{\"611\":1}}],[\"用来处理实数范围内的\",{\"1\":{\"566\":1}}],[\"用来判断图像文本是否匹配\",{\"1\":{\"258\":1}}],[\"用到多少\",{\"1\":{\"563\":1,\"595\":1}}],[\"用随机词替换\",{\"1\":{\"512\":1}}],[\"用随机高斯分布初始化\",{\"1\":{\"425\":1,\"426\":1}}],[\"用mask掩码替换\",{\"1\":{\"512\":1}}],[\"用两句话总结\",{\"1\":{\"472\":1}}],[\"用两个可能的替换说法来代替定义的代词\",{\"1\":{\"449\":1}}],[\"用奖励模型得分选择最佳模型\",{\"1\":{\"470\":1}}],[\"用正则化或投影技术缓解嵌入空间中的偏见\",{\"1\":{\"469\":1}}],[\"用正则匹配并替换匹配上的\",{\"1\":{\"410\":1}}],[\"用人类偏好训练强化学习代理\",{\"1\":{\"469\":1}}],[\"用人工标注的数据\",{\"1\":{\"416\":1}}],[\"用任务未知的生成式预训练模型和判别式微调在自然语言理解上取得了很强的效果\",{\"1\":{\"450\":1}}],[\"用以区分不同的句子\",{\"1\":{\"520\":1}}],[\"用以实现语言模型对人类意图的对齐\",{\"1\":{\"470\":1}}],[\"用以更好地对齐模型行为与用户意图\",{\"1\":{\"467\":1}}],[\"用以训练步数的\",{\"1\":{\"447\":1}}],[\"用以完成特定任务的指令\",{\"1\":{\"430\":1}}],[\"用分隔符分隔\",{\"1\":{\"445\":1}}],[\"用标记的数据对特定任务微调模型\",{\"1\":{\"442\":1}}],[\"用特定的训练数据\",{\"1\":{\"420\":1}}],[\"用特定训练数据去微调可能会把这个领域的表现变好\",{\"1\":{\"416\":1}}],[\"用传统机器学习中监督学习的方法\",{\"1\":{\"416\":1}}],[\"用浅层网络逼近需要指数级神经元\",{\"1\":{\"395\":1}}],[\"用深度\",{\"1\":{\"395\":1}}],[\"用法\",{\"1\":{\"355\":1}}],[\"用一个通用大模型\",{\"1\":{\"686\":1}}],[\"用一个简化版的例子说明上述过程\",{\"1\":{\"291\":1}}],[\"用一组由语言引导的动态卷积核\",{\"1\":{\"76\":1}}],[\"用文本描述去匹配最合适的图片内容\",{\"1\":{\"276\":1}}],[\"用当前图片外层目录的名字作为其分类名词\",{\"1\":{\"275\":1}}],[\"用卷积神经网络\",{\"1\":{\"253\":1}}],[\"用户愈发期待像钢铁侠中\",{\"1\":{\"678\":1}}],[\"用户可与\",{\"1\":{\"674\":1}}],[\"用户可根据任务需求选择分辨率\",{\"1\":{\"208\":1}}],[\"用户本地运行时\",{\"1\":{\"546\":1}}],[\"用户为申请加入测试队列的群体\",{\"1\":{\"472\":1}}],[\"用户\",{\"1\":{\"472\":2}}],[\"用户行为\",{\"1\":{\"472\":1}}],[\"用户的\",{\"1\":{\"470\":1}}],[\"用户提交\",{\"1\":{\"471\":1,\"472\":1}}],[\"用户提交的指令\",{\"1\":{\"469\":1}}],[\"用户提交给\",{\"1\":{\"224\":1}}],[\"用户提问或指令\",{\"1\":{\"227\":1}}],[\"用作\",{\"1\":{\"169\":1}}],[\"用动量模型生成伪标签\",{\"1\":{\"149\":1}}],[\"用全连接层逐步压缩到\",{\"1\":{\"107\":1}}],[\"用训练好的模型权重\",{\"1\":{\"83\":1}}],[\"用不同\",{\"1\":{\"82\":1}}],[\"用增强后的语言查询去\",{\"1\":{\"76\":1}}],[\"用语言引导点特征分组\",{\"1\":{\"75\":1}}],[\"用\",{\"1\":{\"73\":1,\"137\":1,\"147\":1,\"204\":1,\"363\":1,\"401\":1,\"425\":2,\"426\":1,\"445\":1,\"520\":1,\"600\":1,\"686\":1}}],[\"用于构建\",{\"1\":{\"685\":1}}],[\"用于构造\",{\"1\":{\"147\":1}}],[\"用于复杂的应用的调用序列\",{\"1\":{\"683\":2}}],[\"用于链的多次运行之间持久化应用程序状态\",{\"1\":{\"683\":1}}],[\"用于与人类对话式应用的大胆尝试\",{\"1\":{\"673\":1}}],[\"用于验证基础微分逻辑\",{\"1\":{\"662\":1}}],[\"用于封装功能\",{\"1\":{\"661\":1}}],[\"用于函数的输入接口\",{\"1\":{\"651\":1}}],[\"用于存储导数\",{\"1\":{\"629\":1}}],[\"用于存储训练集和验证集的图像预处理转换操作\",{\"1\":{\"290\":1}}],[\"用于排除或降低某些不太可能的世界状态\",{\"1\":{\"597\":1}}],[\"用于确保整个密度函数的积分为\",{\"1\":{\"584\":1}}],[\"用于指定在计算损失时忽略的标签索引\",{\"1\":{\"541\":1}}],[\"用于判断给出的两个句子是否连续\",{\"1\":{\"520\":1}}],[\"用于判断图文对是否匹配\",{\"1\":{\"156\":1}}],[\"用于提取\",{\"1\":{\"513\":1}}],[\"用于提升图文语料的质量\",{\"1\":{\"128\":1}}],[\"用于我们的预训练任务\",{\"1\":{\"512\":1}}],[\"用于监督微调\",{\"1\":{\"470\":1}}],[\"用于交叉熵部分\",{\"1\":{\"407\":1}}],[\"用于处理类别不平衡\",{\"1\":{\"405\":1,\"407\":1}}],[\"用于抑制易分类样本\",{\"1\":{\"404\":1}}],[\"用于平衡正负样本数量差异\",{\"1\":{\"404\":1}}],[\"用于平衡训练效率与显存使用\",{\"1\":{\"142\":1}}],[\"用于度量两个集合之间的重叠程度\",{\"1\":{\"403\":1}}],[\"用于语义分割任务中评估模型的分割结果与真实分割标签之间的相似性\",{\"1\":{\"403\":1}}],[\"用于语言引导下的功能区域分割\",{\"1\":{\"78\":1}}],[\"用于类别加权\",{\"1\":{\"402\":1,\"403\":1,\"404\":1}}],[\"用于写入结果\",{\"1\":{\"381\":1}}],[\"用于选择模型和阈值的\",{\"0\":{\"353\":1}}],[\"用于选择难负样本\",{\"1\":{\"162\":1}}],[\"用于匹配查询\",{\"1\":{\"310\":1}}],[\"用于同时生成查询\",{\"1\":{\"295\":1}}],[\"用于调整注意力分数\",{\"1\":{\"295\":1}}],[\"用于调节\",{\"1\":{\"240\":1}}],[\"用于防止过拟合\",{\"1\":{\"294\":1}}],[\"用于随机深度\",{\"1\":{\"294\":1}}],[\"用于位置嵌入后的随机丢弃\",{\"1\":{\"293\":1,\"296\":1}}],[\"用于预测图像的类别\",{\"1\":{\"292\":1}}],[\"用于预训练新模型\",{\"1\":{\"140\":1}}],[\"用于计算交叉熵损失\",{\"1\":{\"544\":1}}],[\"用于计算\",{\"1\":{\"285\":1}}],[\"用于分类任务\",{\"1\":{\"262\":1,\"292\":1}}],[\"用于训练奖励模型\",{\"1\":{\"470\":1}}],[\"用于训练或评估模型\",{\"1\":{\"224\":1}}],[\"用于训练下一个更强的模型\",{\"1\":{\"128\":1}}],[\"用于从视觉特征中提取信息\",{\"1\":{\"191\":1}}],[\"用于零样本分类\",{\"1\":{\"188\":1}}],[\"用于文本编码\",{\"1\":{\"160\":1}}],[\"用于将张量中的值限制在指定的范围内\",{\"1\":{\"541\":1}}],[\"用于将张量\",{\"1\":{\"407\":1}}],[\"用于将任意大小的候选框\",{\"1\":{\"397\":1}}],[\"用于将输入图像分割成多个图像块并进行嵌入\",{\"1\":{\"291\":1}}],[\"用于将一个方法定义为静态方法\",{\"1\":{\"289\":1}}],[\"用于将一个批次的数据组合成一个张量\",{\"1\":{\"289\":1}}],[\"用于将\",{\"1\":{\"154\":1,\"685\":1}}],[\"用于二分类匹配\",{\"1\":{\"147\":1}}],[\"用于多模态\",{\"1\":{\"147\":1}}],[\"用于生成\",{\"1\":{\"147\":1}}],[\"用于拷贝和更新\",{\"1\":{\"147\":1}}],[\"用于对齐语言模型在广泛任务分布下的行为\",{\"1\":{\"469\":1}}],[\"用于对齐视觉与语言特征\",{\"1\":{\"189\":1}}],[\"用于对齐图像和文本的表示空间\",{\"1\":{\"127\":1}}],[\"用于对比学习中的\",{\"1\":{\"160\":1}}],[\"用于对比学习\",{\"1\":{\"145\":1}}],[\"用于论文3\",{\"1\":{\"142\":1}}],[\"用于图像字幕生成\",{\"1\":{\"142\":1}}],[\"用于学习图文之间的细粒度对齐关系\",{\"1\":{\"127\":1}}],[\"用于其他任务\",{\"1\":{\"101\":1}}],[\"用于保存卷积层和批归一化层\",{\"1\":{\"100\":1}}],[\"用于后续解码时区分提示与生成文本\",{\"1\":{\"142\":1}}],[\"用于后续比较\",{\"1\":{\"108\":1}}],[\"用于后续计算其他点到该点的距离\",{\"1\":{\"92\":1}}],[\"用于后续分割掩码预测\",{\"1\":{\"74\":1}}],[\"用于快速访问每个\",{\"1\":{\"92\":1}}],[\"用于衡量被错误分类为垃圾邮件的合法电子邮件的比例\",{\"1\":{\"345\":1}}],[\"用于衡量两个概率分布之间的匹配程度\",{\"1\":{\"82\":1}}],[\"用于衡量模型输出的\",{\"1\":{\"82\":1}}],[\"用于衡量预测掩码与真实标签之间的空间重合度\",{\"1\":{\"78\":1}}],[\"用于缓解类别不平衡问题\",{\"1\":{\"78\":1}}],[\"用于解决一个新颖的任务\",{\"1\":{\"70\":1}}],[\"用于支持该任务\",{\"1\":{\"48\":1}}],[\"用于稳定训练过程\",{\"1\":{\"45\":1}}],[\"用于\",{\"1\":{\"45\":1,\"92\":1,\"147\":1,\"160\":1,\"161\":1,\"162\":1,\"163\":3,\"470\":2,\"513\":1}}],[\"用于降低计算复杂度\",{\"1\":{\"45\":1}}],[\"用于融合来自语言模型的不同语义信息\",{\"1\":{\"45\":1}}],[\"用几何结构\",{\"1\":{\"32\":1}}],[\"用意图文本\",{\"1\":{\"32\":1}}],[\"用resnet18对图像进行编码\",{\"1\":{\"30\":1}}],[\"用手握住壶把倒水\",{\"1\":{\"28\":1}}],[\"训练的模型\",{\"1\":{\"674\":1}}],[\"训练的时候固定\",{\"1\":{\"425\":1}}],[\"训练能够准确预测下一个单词的\",{\"1\":{\"674\":1}}],[\"训练步骤分析\",{\"0\":{\"495\":1}}],[\"训练硬件与效率\",{\"1\":{\"494\":1}}],[\"训练动态\",{\"1\":{\"482\":1}}],[\"训练优化策略\",{\"1\":{\"481\":1}}],[\"训练任务覆盖广泛\",{\"1\":{\"470\":1}}],[\"训练语言模型以遵循自然语言指令\",{\"1\":{\"469\":1}}],[\"训练语料所在的文件列表\",{\"1\":{\"410\":1}}],[\"训练语料为\",{\"1\":{\"410\":1}}],[\"训练依赖微软提供的高带宽gpu集群\",{\"1\":{\"461\":1}}],[\"训练数据中的敏感信息需要妥善处理\",{\"1\":{\"681\":1}}],[\"训练数据中的例子\",{\"1\":{\"224\":1}}],[\"训练数据主要为英文\",{\"1\":{\"472\":1}}],[\"训练数据和奖励信号均来自一组英语标注者\",{\"1\":{\"472\":1}}],[\"训练数据集\",{\"1\":{\"454\":1}}],[\"训练一个少量参数的小模型\",{\"1\":{\"418\":1}}],[\"训练一个轻量级的微调模型\",{\"1\":{\"415\":1}}],[\"训练一个奖励模型\",{\"1\":{\"224\":1}}],[\"训练成本非常高\",{\"1\":{\"415\":1}}],[\"训练与评估流程的代码为模版代码\",{\"1\":{\"300\":1}}],[\"训练与测试灵活性\",{\"1\":{\"216\":1}}],[\"训练了10个epoch\",{\"1\":{\"300\":1}}],[\"训练过程就比较常规了\",{\"1\":{\"514\":1}}],[\"训练过程与资源分配\",{\"1\":{\"461\":1}}],[\"训练过程中的一些\",{\"1\":{\"470\":1}}],[\"训练过程中\",{\"1\":{\"292\":1}}],[\"训练过程代码实现基本遵循moco论文中所提出的动量慢更新对比学习代码实现\",{\"1\":{\"145\":1}}],[\"训练代价\",{\"1\":{\"280\":1}}],[\"训练代码如下所示\",{\"1\":{\"244\":1}}],[\"训练代码\",{\"1\":{\"159\":1}}],[\"训练效率成为一个至关重要的因素\",{\"1\":{\"278\":1}}],[\"训练效率可以提高4倍\",{\"1\":{\"278\":1}}],[\"训练使用到的数据集和alexnet保持一致\",{\"1\":{\"275\":1}}],[\"训练出a和b即可得到∆w\",{\"1\":{\"420\":1}}],[\"训练出具有可迁移能力的视觉模型\",{\"1\":{\"270\":1}}],[\"训练出一个能预测人类偏好的奖励模型\",{\"1\":{\"468\":1}}],[\"训练出一个能自然理解图像内容\",{\"1\":{\"227\":1}}],[\"训练出一个能看懂图的视觉分词器\",{\"1\":{\"226\":1}}],[\"训练更稳定但区分度下降\",{\"1\":{\"240\":1}}],[\"训练方式\",{\"1\":{\"231\":1}}],[\"训练策略如下\",{\"1\":{\"470\":1}}],[\"训练策略遵循了\",{\"1\":{\"461\":1}}],[\"训练策略\",{\"1\":{\"229\":1}}],[\"训练目标的引导\",{\"1\":{\"292\":1}}],[\"训练目标\",{\"1\":{\"226\":1,\"227\":1,\"231\":1,\"285\":1}}],[\"训练流程\",{\"1\":{\"226\":1,\"227\":1}}],[\"训练奖励模型\",{\"1\":{\"224\":1,\"467\":1}}],[\"训练监督模型\",{\"1\":{\"224\":1}}],[\"训练图像的分辨率从固定的448×448扩展为动态的448×448\",{\"1\":{\"215\":1}}],[\"训练离散变分自编码器\",{\"1\":{\"178\":1}}],[\"训练期间对\",{\"1\":{\"170\":1}}],[\"训练稳定\",{\"1\":{\"159\":1}}],[\"训练初期模型尚不稳定\",{\"1\":{\"159\":1}}],[\"训练函数\",{\"1\":{\"142\":1}}],[\"训练多个模型\",{\"1\":{\"138\":1}}],[\"训练时\",{\"1\":{\"477\":1}}],[\"训练时将所有配对作为一个\",{\"1\":{\"470\":1}}],[\"训练时最多12区块\",{\"1\":{\"216\":1}}],[\"训练时根据输入图像的宽高比和分辨率\",{\"1\":{\"214\":1}}],[\"训练时间会更长\",{\"1\":{\"136\":1}}],[\"训练时会对每个图像随机采样\",{\"1\":{\"29\":1}}],[\"训练轮数为\",{\"1\":{\"131\":1}}],[\"训练配置如下\",{\"1\":{\"131\":1}}],[\"训练模型以自回归方式生成文本\",{\"1\":{\"127\":1}}],[\"训练模型根据图像生成文本描述\",{\"1\":{\"127\":1}}],[\"训练\",{\"0\":{\"79\":1,\"81\":1,\"272\":1,\"514\":1},\"1\":{\"68\":1,\"79\":1,\"174\":1,\"183\":1,\"190\":1,\"202\":1,\"203\":1,\"204\":1,\"447\":1,\"470\":2,\"472\":3,\"493\":1}}],[\"训练集与测试集性能对比\",{\"1\":{\"455\":1}}],[\"训练集与测试集中的物体与可供性类别相同\",{\"1\":{\"20\":1}}],[\"训练集的预处理转换操作\",{\"1\":{\"290\":1}}],[\"训练集\",{\"1\":{\"68\":1}}],[\"训练阶段唯一需要注意的一点就是数据集的构造过程中\",{\"1\":{\"142\":1}}],[\"训练阶段则是模型的核心迭代过程\",{\"1\":{\"81\":1}}],[\"训练阶段\",{\"1\":{\"66\":1,\"219\":1}}],[\"训练和测试阶段共享相似的物体类别和功能类型的分布\",{\"1\":{\"65\":1}}],[\"中蒸馏出的六个\",{\"1\":{\"674\":1}}],[\"中速\",{\"1\":{\"674\":1}}],[\"中间变量y和t的导数被立即释放\",{\"1\":{\"658\":1}}],[\"中间变量导数被清除\",{\"1\":{\"658\":1}}],[\"中间变量的导数往往无用\",{\"1\":{\"658\":1}}],[\"中间微调\",{\"1\":{\"175\":1}}],[\"中支持多输出节点\",{\"1\":{\"652\":1}}],[\"中推断出一个三维形状\",{\"1\":{\"597\":1}}],[\"中采样\",{\"1\":{\"591\":1}}],[\"中采用的是每个\",{\"1\":{\"145\":1}}],[\"中有\",{\"1\":{\"578\":1}}],[\"中取出对应的\",{\"1\":{\"542\":1}}],[\"中取出被掩码位置的\",{\"1\":{\"513\":1}}],[\"中取值\",{\"1\":{\"514\":2}}],[\"中非常经典\",{\"1\":{\"510\":1}}],[\"中自己的值占大头也无所谓\",{\"1\":{\"508\":1}}],[\"中实际上会更加看重\",{\"1\":{\"508\":1}}],[\"中类型\",{\"1\":{\"508\":1}}],[\"中全面超越chinchilla\",{\"1\":{\"482\":1}}],[\"中快速适应新任务\",{\"1\":{\"464\":1}}],[\"中获得广泛能力\",{\"1\":{\"464\":1}}],[\"中则表现平庸甚至接近随机\",{\"1\":{\"463\":1}}],[\"中零样本设定下创下新sota\",{\"1\":{\"462\":1}}],[\"中等模型\",{\"1\":{\"454\":1}}],[\"中随机采样得到\",{\"1\":{\"447\":1}}],[\"中随机抽样字典大小就可以了\",{\"1\":{\"242\":1}}],[\"中选定若干个浮点坐标点\",{\"1\":{\"397\":1}}],[\"中选取了\",{\"1\":{\"63\":1}}],[\"中为了解决\",{\"1\":{\"397\":1}}],[\"中为每张图像选择一个最相似的非匹配文本作为负样本\",{\"1\":{\"156\":1}}],[\"中引入卷积操作\",{\"1\":{\"299\":1}}],[\"中进行进一步的处理\",{\"1\":{\"299\":1}}],[\"中加载数据时\",{\"1\":{\"289\":1}}],[\"中每个\",{\"1\":{\"283\":1}}],[\"中每个word映射到词空间\",{\"1\":{\"268\":1}}],[\"中去\",{\"1\":{\"242\":1}}],[\"中来的\",{\"1\":{\"242\":1}}],[\"中文能力相对来说是非常不错的开源模型\",{\"1\":{\"674\":1}}],[\"中文能力突出\",{\"1\":{\"220\":1}}],[\"中文任务和高分辨率场景下的性能\",{\"1\":{\"217\":1}}],[\"中超越商业模型\",{\"1\":{\"208\":1}}],[\"中英文问答对\",{\"1\":{\"207\":1}}],[\"中新加入的可学习查询\",{\"1\":{\"191\":1}}],[\"中期\",{\"1\":{\"159\":1}}],[\"中更稳定地训练\",{\"1\":{\"157\":1}}],[\"中第一个线性层把输入特征投影到一个更高维度的空间后\",{\"1\":{\"294\":1}}],[\"中第\",{\"1\":{\"134\":1}}],[\"中优先选择对比相似度高的负样本来增强训练信号\",{\"1\":{\"127\":1}}],[\"中添加一层\",{\"1\":{\"126\":1}}],[\"中心差分近似可以问问gpt\",{\"1\":{\"621\":1}}],[\"中心或等距分布\",{\"1\":{\"397\":1}}],[\"中心\",{\"1\":{\"112\":1}}],[\"中心化\",{\"1\":{\"83\":1}}],[\"中用于扩展张量尺寸但不复制数据的一种高效方法\",{\"1\":{\"387\":1}}],[\"中用于沿指定维度重复张量内容的操作\",{\"1\":{\"386\":1}}],[\"中用于将多个形状相同的张量沿一个新维度拼接的函数\",{\"1\":{\"381\":1}}],[\"中用于约束变换矩阵接近正交性的正则化损失函数\",{\"1\":{\"108\":1}}],[\"中用于点云\",{\"1\":{\"100\":1}}],[\"中用于从点云中选择具有代表性的采样点的一种策略\",{\"1\":{\"92\":1}}],[\"中均能有效提取特征\",{\"1\":{\"95\":1}}],[\"中只有一组\",{\"1\":{\"92\":1}}],[\"中找到最大的那个距离对应的点\",{\"1\":{\"92\":2}}],[\"中提出\",{\"1\":{\"404\":1}}],[\"中提到\",{\"1\":{\"78\":1}}],[\"中提取信息\",{\"1\":{\"312\":1}}],[\"中提取与\",{\"1\":{\"282\":2}}],[\"中提取对应的点\",{\"1\":{\"92\":1}}],[\"中提取\",{\"1\":{\"46\":1,\"542\":1}}],[\"中提取共性的几何属性\",{\"1\":{\"29\":1}}],[\"中的几个语法技巧\",{\"1\":{\"651\":1}}],[\"中的全连接层权重\",{\"1\":{\"513\":1}}],[\"中的全局特征学习\",{\"1\":{\"92\":1}}],[\"中的参数微调就可以了\",{\"1\":{\"508\":1}}],[\"中的不一样\",{\"1\":{\"506\":1}}],[\"中的目标函数模型后\",{\"1\":{\"444\":1}}],[\"中的值限制在区间\",{\"1\":{\"407\":1}}],[\"中的每个元素限制在\",{\"1\":{\"407\":1}}],[\"中的样本取平均\",{\"1\":{\"401\":1}}],[\"中的标准组件\",{\"1\":{\"397\":1}}],[\"中的多个样本打包成一个大\",{\"1\":{\"381\":1}}],[\"中的一个方法\",{\"1\":{\"541\":1}}],[\"中的一个函数\",{\"1\":{\"407\":1}}],[\"中的一个装饰器\",{\"1\":{\"289\":1}}],[\"中的一种语法结构\",{\"1\":{\"368\":1}}],[\"中的变量\",{\"1\":{\"366\":1}}],[\"中的图像一一对应\",{\"1\":{\"289\":1}}],[\"中的特征加入队列\",{\"1\":{\"238\":1}}],[\"中的名词短语\",{\"1\":{\"226\":1}}],[\"中的指令调优思想引入多模态领域的研究\",{\"1\":{\"224\":1}}],[\"中的词嵌入\",{\"1\":{\"169\":1}}],[\"中的核心操作\",{\"1\":{\"115\":1}}],[\"中的核心模块\",{\"1\":{\"92\":1}}],[\"中的原始特征\",{\"1\":{\"98\":1}}],[\"中的\",{\"1\":{\"96\":1,\"240\":1,\"304\":1,\"365\":1,\"504\":1,\"506\":1}}],[\"中的语言引导\",{\"1\":{\"78\":1}}],[\"中的数据\",{\"1\":{\"76\":1}}],[\"中的空间卷积\",{\"1\":{\"73\":1}}],[\"中的自注意力机制\",{\"1\":{\"73\":1}}],[\"中筛选出同时满足\",{\"1\":{\"68\":1}}],[\"中椅子的\",{\"1\":{\"49\":1}}],[\"中使用\",{\"1\":{\"45\":1,\"115\":1}}],[\"中\",{\"1\":{\"29\":1,\"82\":1,\"108\":1,\"145\":1,\"157\":2,\"191\":1,\"194\":1,\"240\":1,\"273\":2,\"280\":2,\"284\":1,\"285\":1,\"290\":1,\"292\":5,\"304\":1,\"305\":1,\"315\":1,\"322\":2,\"326\":1,\"338\":2,\"385\":1,\"389\":2,\"395\":2,\"462\":1,\"495\":1,\"505\":1,\"513\":1,\"520\":1,\"540\":1,\"596\":1,\"673\":1}}],[\">表示箭头连接\",{\"1\":{\"666\":1}}],[\">>\",{\"1\":{\"395\":1}}],[\">nk\",{\"1\":{\"247\":1,\"390\":1}}],[\">n\",{\"1\":{\"247\":1,\"390\":1}}],[\">=\",{\"1\":{\"82\":2,\"510\":1}}],[\">bln\",{\"1\":{\"70\":1,\"76\":3}}],[\">2表示从节点1到节点3的有向边\",{\"1\":{\"666\":1}}],[\">2048\",{\"1\":{\"70\":1}}],[\">256\",{\"1\":{\"70\":1}}],[\">1024\",{\"1\":{\"70\":1}}],[\">512\",{\"1\":{\"70\":1}}],[\">object\",{\"1\":{\"35\":1}}],[\">\",{\"1\":{\"29\":1,\"34\":3,\"35\":4,\"36\":3,\"40\":2,\"45\":3,\"46\":2,\"59\":5,\"82\":1,\"83\":2,\"92\":1,\"100\":1,\"111\":1,\"145\":1,\"244\":2,\"246\":1,\"248\":1,\"249\":1,\"267\":2,\"278\":2,\"285\":1,\"286\":1,\"292\":2,\"293\":2,\"294\":1,\"295\":8,\"296\":3,\"355\":1,\"407\":1,\"410\":10,\"411\":7,\"412\":23,\"477\":2,\"511\":3,\"512\":2,\"520\":3,\"522\":1,\"541\":1,\"558\":1,\"566\":1,\"600\":1,\"601\":1,\"651\":1,\"656\":1,\"658\":1,\"666\":5}}],[\"所提供的代码展开进行讲解\",{\"1\":{\"546\":1}}],[\"所参考源仓库未提供requirements\",{\"1\":{\"519\":1}}],[\"所采用的\",{\"1\":{\"469\":1}}],[\"所占的内存资源和计算资源呢\",{\"1\":{\"423\":1}}],[\"所需神经元\",{\"1\":{\"395\":1}}],[\"所需的交集\",{\"1\":{\"407\":1}}],[\"所需的关键词参数\",{\"1\":{\"143\":1}}],[\"所需的输入格式\",{\"1\":{\"43\":1}}],[\"所谓\",{\"1\":{\"395\":1}}],[\"所具备的\",{\"1\":{\"287\":1}}],[\"所给代码删除了大量非核心逻辑\",{\"1\":{\"261\":1}}],[\"所使用的训练数据总共包含约\",{\"1\":{\"131\":1}}],[\"所使用的隐藏状态空间维度\",{\"1\":{\"42\":1}}],[\"所以在反向传播前需调用各变量的cleargrad方法重置导数\",{\"1\":{\"667\":1}}],[\"所以除以\",{\"1\":{\"590\":1}}],[\"所以模型的任务是\",{\"1\":{\"542\":1}}],[\"所以模型必须具有对点顺序的不变性\",{\"1\":{\"115\":1}}],[\"所以最终的答案只能来自原始输入文本中的某一段子串\",{\"1\":{\"542\":1}}],[\"所以我只是取其中的一部分数据\",{\"1\":{\"519\":1}}],[\"所以我们需要一个规则体系来规定\",{\"1\":{\"566\":1}}],[\"所以我们需要加一个正则化项来鼓励变换矩阵接近正交矩阵\",{\"1\":{\"108\":1}}],[\"所以我们可以很容易地去建立一个字典\",{\"1\":{\"238\":1}}],[\"所以准确度不是在我的考虑范围之内\",{\"1\":{\"519\":1}}],[\"所以就算\",{\"1\":{\"508\":1}}],[\"所以不过多展开\",{\"1\":{\"477\":1}}],[\"所以还是很有挑战性的\",{\"1\":{\"448\":1}}],[\"所以矩阵\",{\"1\":{\"426\":2}}],[\"所以使用装饰器会导致原函数的\",{\"1\":{\"372\":1}}],[\"所以正确的strides是\",{\"1\":{\"326\":1}}],[\"所以正样本和负样本要走同一个编码器\",{\"1\":{\"238\":1}}],[\"所以参数量为\",{\"1\":{\"297\":1}}],[\"所以把\",{\"1\":{\"285\":1}}],[\"所以抽样的部分还是要大一点\",{\"1\":{\"240\":1}}],[\"所以作者提出\",{\"1\":{\"239\":1}}],[\"所以moco要做的就是\",{\"1\":{\"238\":1}}],[\"所以如果key这个字典足够大\",{\"1\":{\"238\":1}}],[\"所以cv领域并不适合去建立一个字典来学习模型\",{\"1\":{\"238\":1}}],[\"所以个体判别这个代理任务定义了什么是正样本\",{\"1\":{\"235\":1}}],[\"所以个体判别这个代理任务经过模型训练\",{\"1\":{\"235\":1}}],[\"所以这两张图片就可以称之为正样本\",{\"1\":{\"235\":1}}],[\"所以这个表达式的含义是\",{\"1\":{\"76\":1}}],[\"所以\",{\"1\":{\"117\":1,\"240\":2,\"307\":1,\"372\":1,\"404\":1,\"508\":1,\"565\":2,\"567\":1,\"569\":1,\"592\":1}}],[\"所以要把它复制\",{\"1\":{\"111\":1}}],[\"所以小的才是有效点\",{\"1\":{\"92\":1}}],[\"所支持的23种物体类型和17种功能类型\",{\"1\":{\"68\":1}}],[\"所有可能的\",{\"1\":{\"566\":1}}],[\"所有输入序列等长\",{\"1\":{\"520\":1}}],[\"所有序列都填充到max\",{\"1\":{\"520\":1}}],[\"所有样本列表构成batch数据返回\",{\"1\":{\"512\":1}}],[\"所有模型都基于\",{\"1\":{\"470\":1}}],[\"所有模型都使用3000亿tokens进行训练\",{\"1\":{\"461\":1}}],[\"所有模型共享最大上下文窗口为2048\",{\"1\":{\"461\":1}}],[\"所有模型均采用相同的架构\",{\"1\":{\"455\":1}}],[\"所有模型均训练了32个周期\",{\"1\":{\"272\":1}}],[\"所有模型在\",{\"1\":{\"454\":1}}],[\"所有无偏差或增益权重设置为\",{\"1\":{\"447\":1}}],[\"所有的转换包括添加随机初初始化的开始和结束标记\",{\"1\":{\"445\":1}}],[\"所有的数据都将发生变化\",{\"1\":{\"86\":1}}],[\"所有张量必须具有完全相同的\",{\"1\":{\"381\":1}}],[\"所有变量两两之间协方差的矩阵表示\",{\"1\":{\"355\":1}}],[\"所有这些指标都是基于单个分类阈值值计算得出的\",{\"1\":{\"349\":1}}],[\"所有行均指向原始数据的第\",{\"1\":{\"327\":1}}],[\"所有logits的平均作为最终的matching\",{\"1\":{\"284\":1}}],[\"所有参数可训练\",{\"1\":{\"203\":1}}],[\"所有参数均参与训练\",{\"1\":{\"200\":1}}],[\"所有参数都是可训练的\",{\"1\":{\"191\":1}}],[\"所有图文对的前向传播\",{\"1\":{\"162\":1}}],[\"所有\",{\"1\":{\"145\":1,\"285\":2,\"566\":1}}],[\"所有卷积和\",{\"1\":{\"107\":1}}],[\"所有点相乘\",{\"1\":{\"115\":1}}],[\"所有点相加\",{\"1\":{\"115\":1}}],[\"所有点经过共享参数的\",{\"1\":{\"105\":1}}],[\"所有点组成的局部区域\",{\"1\":{\"92\":1}}],[\"所有尺度的特征保存到\",{\"1\":{\"96\":1}}],[\"所有尺度的网络并行运行\",{\"1\":{\"96\":1}}],[\"所有问题专属于评估阶段\",{\"1\":{\"66\":1}}],[\"所有物体几何结构文本数据\",{\"1\":{\"29\":1}}],[\"所有人类交互文本数据\",{\"1\":{\"29\":1}}],[\"所示\",{\"1\":{\"6\":1,\"17\":1,\"19\":1,\"136\":1,\"157\":1,\"166\":1,\"191\":2,\"461\":2,\"597\":1}}],[\"+5\",{\"1\":{\"483\":1}}],[\"+9\",{\"1\":{\"483\":1}}],[\"+1\",{\"1\":{\"411\":2,\"412\":2}}],[\"+i\",{\"1\":{\"147\":1}}],[\"+qid\",{\"1\":{\"68\":1}}],[\"+0\",{\"1\":{\"59\":9}}],[\"+=\",{\"1\":{\"29\":1,\"58\":1,\"82\":3,\"275\":1,\"277\":1,\"285\":1,\"296\":2,\"410\":6,\"411\":2,\"412\":5,\"474\":1,\"512\":1,\"514\":1,\"666\":7}}],[\"+\",{\"0\":{\"665\":2},\"1\":{\"28\":4,\"29\":1,\"32\":2,\"36\":3,\"40\":4,\"43\":2,\"45\":9,\"46\":7,\"55\":1,\"58\":1,\"59\":8,\"72\":1,\"73\":2,\"74\":3,\"75\":1,\"76\":4,\"78\":15,\"81\":1,\"82\":3,\"83\":1,\"92\":2,\"93\":2,\"96\":6,\"98\":1,\"99\":1,\"100\":8,\"105\":1,\"107\":2,\"111\":1,\"112\":3,\"115\":1,\"116\":1,\"117\":1,\"122\":1,\"145\":20,\"147\":10,\"159\":2,\"161\":8,\"162\":5,\"163\":3,\"190\":1,\"195\":1,\"197\":2,\"217\":1,\"224\":1,\"227\":1,\"248\":1,\"249\":2,\"262\":1,\"265\":1,\"266\":2,\"267\":7,\"268\":2,\"272\":1,\"273\":1,\"275\":2,\"277\":2,\"280\":1,\"283\":1,\"284\":4,\"285\":6,\"289\":1,\"293\":2,\"294\":2,\"295\":11,\"296\":2,\"325\":1,\"342\":4,\"367\":1,\"368\":1,\"370\":1,\"397\":4,\"401\":3,\"402\":8,\"403\":4,\"405\":6,\"407\":6,\"410\":4,\"411\":5,\"412\":5,\"420\":2,\"448\":2,\"464\":1,\"470\":1,\"471\":2,\"472\":1,\"475\":2,\"477\":11,\"494\":1,\"498\":1,\"511\":2,\"512\":21,\"513\":5,\"514\":6,\"520\":21,\"523\":4,\"525\":1,\"529\":2,\"531\":3,\"532\":2,\"536\":1,\"538\":2,\"541\":2,\"542\":1,\"543\":1,\"544\":6,\"552\":4,\"553\":1,\"556\":4,\"557\":1,\"596\":1,\"622\":1,\"626\":2,\"654\":2,\"656\":2,\"658\":1,\"659\":1,\"660\":14,\"662\":14,\"666\":16,\"667\":3,\"670\":1,\"686\":1,\"687\":2}}],[\"推荐系统等多种应用场景\",{\"1\":{\"679\":1}}],[\"推出了其推理模型\",{\"1\":{\"674\":1}}],[\"推测约\",{\"1\":{\"674\":1}}],[\"推测其他交互\",{\"1\":{\"28\":1}}],[\"推动开源大模型发展\",{\"1\":{\"485\":1}}],[\"推动双向预训练\",{\"1\":{\"485\":1}}],[\"推动更通用\",{\"1\":{\"460\":1}}],[\"推动学界重新思考语言模型在多任务学习中的角色\",{\"1\":{\"456\":1}}],[\"推动技术民主化进程\",{\"1\":{\"222\":1}}],[\"推动了多模态大模型的发展\",{\"1\":{\"198\":1}}],[\"推动多模态应用发展\",{\"1\":{\"181\":1}}],[\"推动\",{\"1\":{\"69\":1}}],[\"推理延迟\",{\"1\":{\"681\":1}}],[\"推理策略\",{\"1\":{\"676\":1}}],[\"推理能力限制\",{\"1\":{\"679\":1}}],[\"推理能力和代码能力提升非常显著\",{\"1\":{\"674\":1}}],[\"推理能力受限\",{\"1\":{\"24\":1}}],[\"推理深度和创意表达方面有显著提升\",{\"1\":{\"674\":1}}],[\"推理型大模型\",{\"1\":{\"674\":1}}],[\"推理型模型\",{\"1\":{\"674\":1}}],[\"推理型\",{\"1\":{\"674\":3}}],[\"推理模式优化\",{\"1\":{\"658\":1}}],[\"推理模式\",{\"1\":{\"658\":1}}],[\"推理任务\",{\"1\":{\"508\":1}}],[\"推理任务仍存在幻觉\",{\"1\":{\"483\":1}}],[\"推理效率的核心目标\",{\"1\":{\"480\":1}}],[\"推理过程修改的关键\",{\"1\":{\"477\":1}}],[\"推理即编程\",{\"1\":{\"464\":1}}],[\"推理有限\",{\"1\":{\"463\":1}}],[\"推理与数学能力仍然有限\",{\"1\":{\"463\":1}}],[\"推理成本是不得不要考虑的一个因素\",{\"1\":{\"415\":1}}],[\"推理时的常规流程\",{\"1\":{\"477\":1}}],[\"推理时\",{\"1\":{\"282\":1}}],[\"推理等\",{\"1\":{\"231\":1}}],[\"推理优先\",{\"1\":{\"229\":1}}],[\"推理阶段\",{\"1\":{\"66\":1}}],[\"推理或训练分支\",{\"1\":{\"40\":1}}],[\"推理几何结构\",{\"1\":{\"28\":1}}],[\"推理几何形态支持该交互\",{\"1\":{\"28\":1}}],[\"推理除了当前交互以外\",{\"1\":{\"28\":1}}],[\"推理\",{\"0\":{\"273\":1},\"1\":{\"26\":1,\"433\":1,\"596\":1}}],[\"推理的交互描述被\",{\"1\":{\"13\":1}}],[\"推理潜在意图\",{\"1\":{\"6\":1}}],[\"ndim\",{\"1\":{\"659\":4}}],[\"ndarray或variable\",{\"1\":{\"660\":1}}],[\"ndarray混合运算\",{\"1\":{\"660\":1}}],[\"ndarray\",{\"1\":{\"609\":1,\"643\":1,\"656\":1,\"659\":1,\"660\":1}}],[\"nr\",{\"1\":{\"599\":1,\"600\":1}}],[\"n3\",{\"1\":{\"599\":1}}],[\"n2\",{\"1\":{\"599\":2,\"600\":1}}],[\"n1\",{\"1\":{\"599\":2,\"600\":1}}],[\"nbatches\",{\"1\":{\"558\":3}}],[\"n个解码器层\",{\"1\":{\"557\":1}}],[\"nvidia\",{\"1\":{\"494\":1}}],[\"nfinal\",{\"1\":{\"477\":1}}],[\"ninput\",{\"1\":{\"474\":1}}],[\"ngo\",{\"1\":{\"469\":1,\"472\":1}}],[\"n维\",{\"1\":{\"420\":2}}],[\"ntoken\",{\"1\":{\"410\":2,\"412\":2}}],[\"ntransposed\",{\"1\":{\"326\":1}}],[\"n阶张量的排列规律如下图所示\",{\"1\":{\"328\":1}}],[\"n为序列长度\",{\"1\":{\"295\":1}}],[\"nc\",{\"1\":{\"247\":3,\"390\":3}}],[\"nce\",{\"1\":{\"240\":5}}],[\"n+1\",{\"1\":{\"160\":1,\"161\":3,\"268\":2}}],[\"null\",{\"1\":{\"520\":1}}],[\"nucleus\",{\"1\":{\"133\":2,\"143\":2,\"286\":3}}],[\"numerical\",{\"1\":{\"622\":1,\"646\":2}}],[\"numpy\",{\"1\":{\"83\":4,\"107\":1,\"273\":2,\"275\":4,\"277\":3,\"290\":2,\"323\":3,\"331\":1,\"608\":1,\"651\":1,\"666\":2,\"667\":2}}],[\"nums\",{\"1\":{\"82\":2}}],[\"num个点云样本\",{\"1\":{\"29\":1}}],[\"num参数\",{\"1\":{\"29\":1}}],[\"number\",{\"1\":{\"29\":6,\"58\":6,\"92\":2,\"246\":1,\"289\":1,\"558\":1}}],[\"num\",{\"1\":{\"29\":4,\"40\":1,\"41\":6,\"58\":2,\"59\":11,\"73\":3,\"80\":7,\"81\":1,\"82\":11,\"83\":4,\"93\":3,\"96\":2,\"98\":1,\"101\":8,\"142\":1,\"143\":5,\"145\":1,\"147\":1,\"160\":2,\"244\":1,\"246\":3,\"263\":2,\"275\":2,\"277\":2,\"285\":1,\"286\":6,\"289\":5,\"291\":5,\"292\":9,\"293\":11,\"294\":2,\"295\":24,\"296\":18,\"300\":4,\"519\":1,\"526\":1,\"529\":5,\"531\":5,\"541\":3,\"543\":6,\"544\":9,\"646\":2}}],[\"num=12\",{\"1\":{\"28\":1}}],[\"nz\",{\"1\":{\"93\":1}}],[\"ny\",{\"1\":{\"93\":1}}],[\"nxk\",{\"1\":{\"247\":1}}],[\"nx1\",{\"1\":{\"247\":1}}],[\"nxc\",{\"1\":{\"247\":2}}],[\"nx\",{\"1\":{\"93\":1,\"247\":1}}],[\"neox\",{\"1\":{\"485\":1}}],[\"neox方案\",{\"1\":{\"481\":1}}],[\"need\",{\"0\":{\"474\":1}}],[\"needed\",{\"1\":{\"275\":2,\"277\":2}}],[\"ner\",{\"1\":{\"441\":1,\"543\":1}}],[\"neighborhood\",{\"1\":{\"287\":1}}],[\"neural\",{\"1\":{\"251\":2,\"461\":1,\"547\":1,\"548\":1,\"673\":1}}],[\"negativity\",{\"1\":{\"567\":1}}],[\"negatives\",{\"1\":{\"283\":2,\"405\":1}}],[\"negative\",{\"0\":{\"578\":1},\"1\":{\"78\":6,\"145\":8,\"147\":2,\"156\":1,\"162\":2,\"240\":1,\"246\":1,\"247\":1,\"405\":2,\"579\":1}}],[\"neg\",{\"1\":{\"145\":22,\"147\":22,\"162\":22,\"247\":2,\"284\":20,\"660\":5}}],[\"next取出索引0或者1下标对应的值即可知道我们是否预测正确\",{\"1\":{\"514\":1}}],[\"next=false\",{\"1\":{\"512\":1}}],[\"next=true\",{\"1\":{\"512\":1}}],[\"next\",{\"0\":{\"506\":1},\"1\":{\"142\":1,\"268\":1,\"285\":1,\"477\":6,\"497\":1,\"504\":1,\"512\":2,\"513\":2,\"514\":2,\"520\":1,\"538\":5}}],[\"news数据集已开源以促进后续研究\",{\"1\":{\"502\":1}}],[\"news\",{\"1\":{\"492\":1,\"493\":1,\"494\":1,\"498\":1,\"500\":1}}],[\"new\",{\"1\":{\"92\":46,\"96\":16,\"100\":9,\"266\":2,\"285\":2,\"384\":1,\"385\":1,\"474\":18,\"477\":2,\"531\":4}}],[\"net\",{\"1\":{\"98\":1,\"103\":1,\"105\":2,\"107\":3,\"112\":2}}],[\"networks等方法异曲同工\",{\"1\":{\"464\":1}}],[\"networks\",{\"1\":{\"294\":1,\"447\":1,\"464\":1}}],[\"network\",{\"1\":{\"48\":1,\"107\":1,\"454\":1,\"548\":1}}],[\"netpoll\",{\"1\":{\"2\":1}}],[\"nli和adversarial推理任务仍具挑战性\",{\"1\":{\"462\":1}}],[\"nli\",{\"1\":{\"448\":1,\"460\":1,\"508\":1}}],[\"nltk\",{\"1\":{\"412\":1}}],[\"nlp半监督学习\",{\"1\":{\"441\":1}}],[\"nlp\",{\"1\":{\"166\":1,\"224\":1,\"239\":1,\"272\":1,\"453\":1,\"460\":1,\"464\":1,\"469\":2,\"470\":1,\"471\":2,\"472\":1,\"504\":3,\"510\":3,\"674\":1}}],[\"nl=token数\",{\"1\":{\"40\":1}}],[\"nl\",{\"1\":{\"40\":4}}],[\"noise\",{\"1\":{\"240\":2}}],[\"noisy\",{\"1\":{\"104\":1}}],[\"noun\",{\"1\":{\"226\":1}}],[\"non\",{\"1\":{\"159\":1,\"512\":7,\"567\":1}}],[\"none\",{\"1\":{\"40\":1,\"43\":1,\"75\":1,\"76\":6,\"83\":2,\"92\":3,\"93\":1,\"96\":6,\"100\":2,\"101\":1,\"108\":2,\"109\":1,\"142\":3,\"145\":3,\"147\":1,\"160\":3,\"162\":1,\"163\":8,\"244\":3,\"246\":1,\"248\":1,\"249\":1,\"262\":2,\"266\":2,\"268\":3,\"275\":3,\"276\":2,\"277\":4,\"284\":4,\"285\":10,\"289\":2,\"291\":1,\"300\":1,\"372\":1,\"389\":2,\"410\":2,\"411\":2,\"412\":4,\"474\":1,\"477\":14,\"511\":3,\"520\":4,\"523\":2,\"529\":2,\"531\":1,\"538\":2,\"541\":2,\"543\":2,\"544\":7,\"558\":4,\"629\":1,\"635\":1,\"638\":1,\"642\":1,\"643\":1,\"652\":2,\"653\":1,\"654\":3,\"656\":6,\"657\":1,\"658\":6,\"659\":5,\"666\":4}}],[\"no\",{\"0\":{\"388\":1},\"1\":{\"145\":2,\"147\":2,\"161\":1,\"162\":1,\"163\":1,\"247\":2,\"248\":1,\"249\":1,\"273\":2,\"275\":2,\"276\":1,\"277\":3,\"284\":1,\"471\":1,\"474\":1,\"477\":1,\"658\":3,\"661\":2}}],[\"nohup\",{\"1\":{\"83\":2}}],[\"notebook中直接显示图像\",{\"1\":{\"666\":1}}],[\"notes\",{\"1\":{\"46\":1}}],[\"notimplementederror\",{\"1\":{\"68\":1,\"613\":1,\"630\":1,\"651\":2}}],[\"not\",{\"1\":{\"68\":2,\"75\":1,\"80\":1,\"92\":3,\"96\":2,\"100\":1,\"143\":1,\"162\":1,\"163\":6,\"246\":1,\"262\":1,\"266\":2,\"268\":1,\"275\":3,\"276\":2,\"277\":5,\"284\":3,\"285\":6,\"286\":1,\"289\":2,\"300\":2,\"379\":1,\"411\":1,\"412\":2,\"477\":4,\"510\":2,\"511\":1,\"512\":1,\"520\":1,\"529\":1,\"531\":1,\"538\":2,\"541\":2,\"543\":2,\"544\":4,\"558\":3,\"635\":1,\"638\":1,\"643\":3,\"651\":1,\"652\":2,\"654\":2,\"656\":7,\"658\":5,\"659\":3,\"666\":5,\"691\":1}}],[\"norm=1\",{\"1\":{\"514\":1}}],[\"norm层之后同样是多头注意力层\",{\"1\":{\"294\":1}}],[\"normalization和relu激活函数\",{\"1\":{\"392\":1}}],[\"normalization\",{\"1\":{\"294\":1,\"481\":1,\"548\":1,\"567\":1,\"674\":1}}],[\"normalize\",{\"1\":{\"29\":4,\"58\":2,\"68\":1,\"83\":2,\"145\":6,\"146\":2,\"147\":6,\"160\":2,\"161\":4,\"244\":3,\"246\":1,\"247\":2,\"272\":2,\"282\":2,\"290\":2,\"531\":1}}],[\"normal\",{\"0\":{\"585\":1,\"589\":1},\"1\":{\"93\":6,\"96\":5,\"114\":1,\"292\":1,\"293\":2,\"296\":2,\"584\":1,\"585\":1,\"588\":1}}],[\"norm3\",{\"1\":{\"76\":1}}],[\"norm\",{\"1\":{\"72\":3,\"74\":3,\"92\":3,\"93\":3,\"96\":3,\"100\":2,\"108\":1,\"160\":2,\"265\":1,\"268\":1,\"273\":2,\"275\":2,\"277\":2,\"291\":6,\"293\":1,\"294\":3,\"296\":7,\"514\":1,\"523\":1,\"525\":1,\"532\":1,\"535\":1,\"552\":2,\"554\":2,\"557\":2}}],[\"norm2\",{\"1\":{\"36\":1,\"73\":2,\"74\":1,\"76\":1,\"294\":2}}],[\"norm1\",{\"1\":{\"36\":1,\"73\":2,\"76\":1,\"294\":2}}],[\"nsp任务\",{\"1\":{\"520\":1}}],[\"nsp任务非必要\",{\"1\":{\"500\":1}}],[\"nsp\",{\"1\":{\"491\":1,\"492\":2,\"493\":4,\"495\":1,\"513\":6,\"514\":2,\"515\":1,\"520\":1}}],[\"nsample=none\",{\"1\":{\"93\":1}}],[\"nsample=64\",{\"1\":{\"93\":1}}],[\"nsample=32\",{\"1\":{\"93\":1,\"101\":4}}],[\"nsample\",{\"1\":{\"92\":23,\"96\":4}}],[\"ns\",{\"1\":{\"40\":5,\"105\":1}}],[\"nphysically\",{\"1\":{\"326\":1}}],[\"npoint=64\",{\"1\":{\"101\":1}}],[\"npoint=256\",{\"1\":{\"101\":1}}],[\"npoint=16\",{\"1\":{\"101\":1}}],[\"npoint=1024\",{\"1\":{\"101\":1}}],[\"npoint=128\",{\"1\":{\"93\":1}}],[\"npoint=none\",{\"1\":{\"93\":1}}],[\"npoint=512\",{\"1\":{\"93\":1}}],[\"npoint\",{\"1\":{\"46\":2,\"59\":2,\"70\":2,\"92\":35,\"96\":4}}],[\"np\",{\"1\":{\"40\":1,\"68\":5,\"82\":15,\"83\":15,\"107\":2,\"272\":5,\"273\":1,\"275\":6,\"276\":1,\"277\":8,\"323\":2,\"608\":3,\"617\":2,\"632\":1,\"642\":1,\"643\":1,\"645\":4,\"646\":2,\"652\":1,\"654\":2,\"655\":1,\"656\":2,\"658\":6,\"659\":8,\"660\":6,\"662\":6,\"666\":6,\"667\":11}}],[\"n\",{\"1\":{\"30\":1,\"32\":2,\"35\":1,\"36\":2,\"40\":1,\"41\":10,\"43\":6,\"45\":29,\"46\":21,\"59\":40,\"64\":2,\"70\":4,\"72\":1,\"74\":1,\"75\":1,\"76\":12,\"78\":2,\"80\":8,\"82\":3,\"83\":14,\"92\":30,\"96\":4,\"98\":4,\"100\":29,\"101\":7,\"107\":1,\"109\":5,\"111\":9,\"112\":3,\"145\":5,\"147\":5,\"161\":1,\"162\":4,\"272\":7,\"278\":1,\"295\":3,\"310\":1,\"319\":4,\"326\":4,\"331\":1,\"335\":1,\"386\":1,\"387\":1,\"390\":2,\"395\":1,\"401\":4,\"402\":4,\"403\":4,\"404\":4,\"405\":2,\"407\":1,\"424\":1,\"454\":1,\"485\":1,\"512\":3,\"513\":3,\"514\":1,\"517\":2,\"519\":1,\"546\":1,\"554\":3,\"557\":3,\"575\":1,\"600\":6,\"601\":1,\"659\":2,\"666\":4}}],[\"nn\",{\"1\":{\"30\":1,\"32\":1,\"33\":1,\"34\":11,\"35\":17,\"36\":15,\"41\":8,\"42\":4,\"44\":4,\"45\":12,\"46\":8,\"59\":31,\"70\":1,\"72\":1,\"73\":15,\"74\":1,\"75\":1,\"76\":2,\"78\":4,\"80\":1,\"92\":5,\"93\":8,\"96\":15,\"98\":6,\"100\":5,\"101\":5,\"107\":13,\"109\":7,\"110\":8,\"111\":8,\"142\":1,\"145\":9,\"146\":4,\"147\":9,\"160\":10,\"162\":1,\"244\":1,\"246\":2,\"247\":2,\"262\":3,\"263\":2,\"264\":1,\"265\":5,\"266\":2,\"268\":5,\"284\":1,\"285\":4,\"291\":5,\"292\":4,\"293\":6,\"294\":6,\"295\":5,\"296\":14,\"389\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1,\"477\":1,\"506\":1,\"513\":5,\"514\":3,\"517\":2,\"523\":5,\"525\":6,\"526\":2,\"527\":3,\"529\":2,\"531\":6,\"532\":3,\"533\":1,\"535\":2,\"536\":3,\"537\":2,\"540\":1,\"541\":1,\"543\":2,\"544\":2,\"549\":1,\"550\":2,\"552\":2,\"553\":1,\"554\":1,\"556\":1,\"557\":1,\"558\":3}}],[\"nagative\",{\"1\":{\"449\":1}}],[\"naturalquestions\",{\"1\":{\"482\":1}}],[\"natural\",{\"1\":{\"274\":1,\"421\":1,\"455\":1,\"462\":1,\"469\":1}}],[\"nangia\",{\"1\":{\"469\":1}}],[\"nanmean\",{\"1\":{\"82\":2}}],[\"nan\",{\"1\":{\"82\":3,\"407\":1}}],[\"na\",{\"1\":{\"40\":1}}],[\"name=none\",{\"1\":{\"659\":1}}],[\"name=\",{\"1\":{\"83\":2,\"519\":1,\"659\":2}}],[\"named\",{\"1\":{\"80\":2,\"300\":1}}],[\"name\",{\"1\":{\"29\":4,\"58\":2,\"68\":2,\"83\":1,\"275\":8,\"277\":8,\"300\":4,\"331\":5,\"367\":2,\"371\":2,\"372\":8,\"373\":1,\"377\":2,\"379\":2,\"389\":3,\"474\":1,\"477\":1,\"510\":1,\"511\":2,\"519\":1,\"658\":4,\"659\":2,\"666\":14}}],[\"narrow\",{\"1\":{\"28\":2}}],[\"jet\",{\"1\":{\"667\":1}}],[\"jx\",{\"1\":{\"300\":1}}],[\"javascript\",{\"1\":{\"684\":1}}],[\"javaer\",{\"1\":{\"2\":1}}],[\"jarvis\",{\"1\":{\"678\":1}}],[\"jaques\",{\"1\":{\"469\":1}}],[\"jaccard\",{\"0\":{\"403\":1},\"1\":{\"403\":7}}],[\"jax\",{\"1\":{\"300\":1}}],[\"jupyter\",{\"1\":{\"666\":1}}],[\"judge\",{\"1\":{\"228\":1}}],[\"just\",{\"1\":{\"28\":1,\"59\":1}}],[\"jpeg\",{\"1\":{\"275\":1,\"277\":1}}],[\"jp\",{\"1\":{\"194\":1}}],[\"jpg\",{\"1\":{\"28\":1,\"275\":1,\"277\":1,\"289\":2}}],[\"jft\",{\"1\":{\"183\":1,\"278\":2}}],[\"json\",{\"1\":{\"142\":2,\"145\":1,\"146\":1,\"147\":1,\"160\":1,\"289\":6,\"410\":6,\"412\":13,\"510\":4,\"511\":4,\"514\":1,\"519\":1}}],[\"join\",{\"1\":{\"68\":3,\"83\":2,\"142\":1,\"275\":3,\"276\":1,\"277\":4,\"289\":3,\"410\":5,\"411\":2,\"412\":6,\"511\":2,\"666\":2}}],[\"joint\",{\"0\":{\"588\":1},\"1\":{\"36\":2,\"41\":4,\"45\":2,\"59\":7}}],[\"jra模块\",{\"1\":{\"54\":1,\"56\":1}}],[\"jra\",{\"1\":{\"48\":1,\"59\":1}}],[\"j\",{\"1\":{\"35\":5,\"59\":13,\"96\":3,\"275\":2,\"277\":2,\"323\":2,\"327\":1,\"514\":5}}],[\"ubuntu安装\",{\"1\":{\"666\":1}}],[\"ulmfit\",{\"1\":{\"464\":1,\"510\":1}}],[\"utf\",{\"1\":{\"410\":4,\"411\":1,\"412\":7,\"481\":1,\"510\":3,\"511\":1}}],[\"utm\",{\"1\":{\"140\":1}}],[\"utility\",{\"1\":{\"83\":4}}],[\"util\",{\"1\":{\"83\":1}}],[\"utils\",{\"1\":{\"82\":1,\"83\":1,\"244\":1,\"289\":3,\"290\":2,\"514\":1,\"661\":1}}],[\"u\",{\"1\":{\"83\":4}}],[\"upon\",{\"1\":{\"477\":1}}],[\"upwork\",{\"1\":{\"470\":1}}],[\"updating\",{\"1\":{\"246\":1,\"249\":1}}],[\"update\",{\"0\":{\"248\":1},\"1\":{\"145\":1,\"147\":1,\"161\":1,\"246\":1,\"247\":2,\"248\":2,\"410\":2,\"412\":1,\"477\":2}}],[\"up\",{\"1\":{\"35\":6,\"40\":1,\"42\":1,\"46\":6,\"59\":6,\"70\":11,\"76\":7,\"147\":1,\"179\":1,\"421\":1}}],[\"upsample\",{\"1\":{\"35\":1,\"59\":1,\"100\":1}}],[\"umbrella\",{\"1\":{\"29\":1}}],[\"using\",{\"1\":{\"28\":6,\"31\":6,\"558\":1,\"658\":2,\"661\":2}}],[\"users\",{\"1\":{\"519\":1}}],[\"user\",{\"1\":{\"332\":1,\"334\":3,\"379\":1,\"470\":1}}],[\"used\",{\"1\":{\"294\":1,\"520\":1}}],[\"use\",{\"1\":{\"28\":1,\"247\":1,\"275\":1,\"277\":1,\"282\":1,\"285\":5,\"286\":2,\"295\":1,\"477\":7}}],[\"usage=true\",{\"1\":{\"28\":1}}],[\"unusual\",{\"1\":{\"531\":1}}],[\"unk\",{\"1\":{\"411\":1,\"412\":7,\"511\":5,\"512\":1,\"520\":2}}],[\"unter和pixel\",{\"1\":{\"255\":1}}],[\"undo\",{\"1\":{\"247\":1}}],[\"underfitting\",{\"1\":{\"455\":1}}],[\"underlying\",{\"1\":{\"449\":1}}],[\"understanding\",{\"1\":{\"119\":2,\"134\":1,\"438\":1}}],[\"under\",{\"1\":{\"22\":1,\"82\":3,\"104\":1}}],[\"uncased\",{\"1\":{\"147\":2}}],[\"unittest\",{\"1\":{\"645\":2,\"646\":1}}],[\"uniter\",{\"1\":{\"268\":1}}],[\"unicode\",{\"1\":{\"454\":2,\"525\":1,\"535\":1}}],[\"uni\",{\"1\":{\"283\":2}}],[\"unimodal\",{\"1\":{\"126\":1,\"127\":1}}],[\"unified\",{\"1\":{\"119\":2,\"134\":1}}],[\"union\",{\"0\":{\"403\":1},\"1\":{\"22\":1,\"82\":6,\"403\":5,\"477\":2}}],[\"unordered\",{\"1\":{\"104\":1,\"115\":1}}],[\"unbuffered\",{\"1\":{\"83\":1}}],[\"un\",{\"1\":{\"75\":1}}],[\"ungroup\",{\"1\":{\"75\":2}}],[\"ungrouping阶段\",{\"1\":{\"75\":1}}],[\"ungrouping\",{\"0\":{\"74\":1},\"1\":{\"71\":1,\"75\":3}}],[\"unshuffle\",{\"1\":{\"247\":3}}],[\"unsupervised\",{\"1\":{\"232\":2,\"451\":1,\"452\":1}}],[\"unsqueeze\",{\"1\":{\"70\":2,\"75\":1,\"76\":3,\"82\":2,\"83\":1,\"247\":1,\"283\":4,\"474\":2,\"477\":1,\"513\":1,\"517\":1,\"523\":1,\"528\":2,\"558\":1}}],[\"unseen\",{\"1\":{\"20\":2,\"23\":5,\"24\":1,\"65\":1,\"82\":1}}],[\"=50000\",{\"1\":{\"667\":1}}],[\"=10000\",{\"1\":{\"667\":1}}],[\"=>\",{\"1\":{\"558\":1}}],[\"=self\",{\"1\":{\"30\":1}}],[\"=====\",{\"1\":{\"163\":4}}],[\"==============\",{\"1\":{\"283\":1,\"284\":1}}],[\"===================\",{\"1\":{\"283\":1,\"284\":1}}],[\"=====================\",{\"1\":{\"147\":12}}],[\"========================\",{\"1\":{\"163\":1,\"285\":1}}],[\"=================================\",{\"1\":{\"162\":3}}],[\"=======================\",{\"1\":{\"147\":6}}],[\"=================\",{\"1\":{\"163\":1,\"285\":1}}],[\"===============\",{\"1\":{\"160\":4}}],[\"==========\",{\"1\":{\"161\":4}}],[\"==\",{\"1\":{\"29\":5,\"40\":1,\"58\":4,\"68\":3,\"82\":1,\"83\":2,\"92\":1,\"100\":1,\"142\":1,\"147\":1,\"163\":2,\"249\":1,\"264\":1,\"275\":1,\"277\":1,\"284\":1,\"285\":2,\"404\":1,\"411\":1,\"412\":1,\"474\":1,\"477\":2,\"510\":1,\"511\":2,\"514\":1,\"522\":1,\"525\":1,\"529\":1,\"535\":1,\"542\":1,\"543\":1,\"546\":1,\"558\":2}}],[\"=\",{\"1\":{\"28\":15,\"29\":47,\"30\":7,\"32\":18,\"33\":3,\"34\":9,\"35\":21,\"36\":19,\"40\":20,\"41\":24,\"42\":1,\"43\":7,\"44\":1,\"45\":36,\"46\":18,\"58\":37,\"59\":94,\"63\":1,\"68\":25,\"70\":15,\"72\":4,\"73\":8,\"74\":5,\"75\":4,\"76\":27,\"78\":16,\"80\":15,\"81\":10,\"82\":35,\"83\":45,\"92\":63,\"93\":24,\"96\":53,\"98\":2,\"100\":24,\"101\":26,\"105\":2,\"107\":25,\"108\":5,\"109\":30,\"110\":13,\"111\":20,\"116\":1,\"142\":32,\"143\":12,\"145\":81,\"146\":28,\"147\":77,\"159\":7,\"160\":28,\"161\":23,\"162\":41,\"163\":44,\"244\":12,\"246\":12,\"247\":11,\"248\":1,\"249\":6,\"255\":3,\"262\":8,\"263\":3,\"264\":2,\"265\":7,\"266\":16,\"268\":22,\"272\":9,\"273\":10,\"275\":40,\"276\":9,\"277\":46,\"282\":10,\"283\":8,\"284\":40,\"285\":56,\"286\":10,\"289\":25,\"290\":6,\"291\":11,\"292\":12,\"293\":15,\"294\":19,\"295\":16,\"296\":28,\"297\":2,\"300\":5,\"321\":1,\"323\":4,\"325\":2,\"326\":5,\"327\":5,\"355\":1,\"366\":2,\"367\":2,\"368\":1,\"369\":1,\"370\":1,\"371\":1,\"374\":1,\"375\":2,\"376\":1,\"377\":2,\"378\":4,\"379\":2,\"382\":1,\"383\":1,\"384\":4,\"385\":1,\"386\":3,\"387\":6,\"389\":1,\"397\":5,\"401\":7,\"402\":12,\"403\":10,\"404\":17,\"405\":16,\"407\":15,\"410\":42,\"411\":19,\"412\":55,\"414\":3,\"454\":2,\"470\":2,\"474\":14,\"477\":60,\"495\":1,\"504\":1,\"510\":11,\"511\":30,\"512\":22,\"513\":21,\"514\":23,\"515\":4,\"517\":6,\"520\":14,\"522\":7,\"523\":16,\"525\":17,\"526\":2,\"527\":5,\"528\":10,\"529\":14,\"531\":25,\"532\":6,\"533\":4,\"535\":7,\"536\":5,\"537\":4,\"538\":11,\"540\":9,\"541\":17,\"542\":2,\"543\":16,\"544\":17,\"549\":5,\"550\":1,\"552\":2,\"553\":6,\"554\":3,\"556\":9,\"557\":4,\"558\":15,\"563\":2,\"565\":1,\"569\":1,\"592\":1,\"595\":2,\"600\":3,\"601\":2,\"607\":1,\"608\":3,\"613\":3,\"617\":7,\"622\":4,\"626\":1,\"629\":2,\"630\":5,\"631\":2,\"632\":4,\"634\":1,\"635\":3,\"638\":4,\"642\":1,\"643\":1,\"645\":4,\"646\":3,\"651\":6,\"652\":7,\"653\":1,\"654\":11,\"655\":5,\"656\":22,\"657\":12,\"658\":30,\"659\":13,\"660\":44,\"661\":4,\"662\":13,\"666\":28,\"667\":44}}],[\"开区间\",{\"1\":{\"566\":1}}],[\"开发也有质的差异\",{\"1\":{\"686\":1}}],[\"开发在整体思路上有着较大的不同\",{\"1\":{\"686\":1}}],[\"开发大模型相关应用\",{\"1\":{\"686\":1}}],[\"开发者平台\",{\"1\":{\"685\":1}}],[\"开发者可以轻松地构建和定制\",{\"1\":{\"684\":1}}],[\"开发\",{\"1\":{\"674\":1,\"686\":2}}],[\"开发数据1k\",{\"1\":{\"519\":1}}],[\"开发具备\",{\"1\":{\"472\":1}}],[\"开放接口控制\",{\"1\":{\"472\":1}}],[\"开放型和封闭型\",{\"1\":{\"470\":1}}],[\"开放任务\",{\"1\":{\"470\":1}}],[\"开销大\",{\"1\":{\"387\":1}}],[\"开头提到过\",{\"1\":{\"239\":1}}],[\"开始的预训练\",{\"0\":{\"509\":1}}],[\"开始之前\",{\"1\":{\"414\":1}}],[\"开始调用\",{\"1\":{\"371\":1}}],[\"开始\",{\"1\":{\"370\":1,\"447\":1,\"509\":1,\"687\":1}}],[\"开始数\",{\"1\":{\"325\":1}}],[\"开始顺序排列\",{\"1\":{\"321\":1}}],[\"开始生成句子\",{\"1\":{\"285\":1}}],[\"开始输出回答\",{\"1\":{\"227\":1}}],[\"开始充分利用动量编码器提供的软标签来训练\",{\"1\":{\"159\":1}}],[\"开壶盖\",{\"1\":{\"28\":1}}],[\"开源了\",{\"1\":{\"674\":1}}],[\"开源的一组参数规模从\",{\"1\":{\"674\":1}}],[\"开源贡献\",{\"1\":{\"500\":1}}],[\"开源项目\",{\"0\":{\"303\":1}}],[\"开源模型进展\",{\"1\":{\"485\":1}}],[\"开源模型在视觉\",{\"1\":{\"211\":1}}],[\"开源模型与商业专有模型\",{\"1\":{\"208\":1}}],[\"开源模型\",{\"1\":{\"184\":1}}],[\"开源网络库\",{\"1\":{\"2\":1}}],[\"开源框架\",{\"1\":{\"2\":1}}],[\"细致地识别交互动作及其参与部位\",{\"1\":{\"28\":1}}],[\"wget\",{\"1\":{\"519\":1}}],[\"wpe\",{\"1\":{\"477\":1}}],[\"wpa\",{\"1\":{\"258\":1}}],[\"wte\",{\"1\":{\"477\":1}}],[\"wmt\",{\"1\":{\"455\":1,\"470\":1}}],[\"w+∆w\",{\"1\":{\"420\":1}}],[\"wx\",{\"1\":{\"414\":1}}],[\"w>\",{\"1\":{\"410\":3,\"411\":2,\"412\":3}}],[\"w`代表输出特征图的宽和高\",{\"1\":{\"291\":1}}],[\"write\",{\"1\":{\"289\":1,\"666\":2}}],[\"wrod\",{\"1\":{\"257\":1}}],[\"wraps\",{\"1\":{\"372\":3,\"373\":1,\"379\":3}}],[\"wrapper\",{\"1\":{\"367\":5,\"369\":4,\"370\":2,\"371\":3,\"372\":8,\"373\":2,\"379\":6}}],[\"wrap\",{\"1\":{\"68\":1}}],[\"wrapgrasp\",{\"1\":{\"29\":1,\"58\":1}}],[\"wukong\",{\"1\":{\"217\":1}}],[\"way\",{\"1\":{\"474\":17}}],[\"warnings\",{\"1\":{\"277\":2}}],[\"warmup\",{\"1\":{\"159\":1}}],[\"walk\",{\"1\":{\"275\":1,\"277\":1}}],[\"water\",{\"1\":{\"157\":1}}],[\"want\",{\"1\":{\"83\":3}}],[\"wow\",{\"1\":{\"410\":1}}],[\"world\",{\"1\":{\"284\":6,\"410\":1,\"411\":1}}],[\"wordpiecetokenizer\",{\"1\":{\"511\":1}}],[\"wordpunct\",{\"1\":{\"410\":1,\"411\":1,\"412\":2}}],[\"word2idx\",{\"1\":{\"511\":14,\"512\":9,\"517\":1}}],[\"word2vec\",{\"1\":{\"240\":1,\"464\":1}}],[\"word\",{\"1\":{\"258\":3,\"284\":1,\"410\":10,\"412\":12,\"447\":1,\"455\":1,\"511\":17,\"512\":2,\"513\":4,\"523\":2}}],[\"words\",{\"1\":{\"142\":1,\"163\":2,\"278\":3,\"412\":5,\"523\":2}}],[\"workers\",{\"1\":{\"244\":1}}],[\"workers=args\",{\"1\":{\"244\":1}}],[\"workers=\",{\"1\":{\"142\":1,\"145\":1,\"147\":1}}],[\"workers=8\",{\"1\":{\"80\":3}}],[\"worker\",{\"1\":{\"244\":1}}],[\"work\",{\"0\":{\"121\":1,\"150\":1,\"240\":1,\"242\":1}}],[\"would\",{\"1\":{\"70\":1}}],[\"wei\",{\"1\":{\"469\":1,\"485\":1}}],[\"weighted\",{\"1\":{\"407\":5}}],[\"weightedloss\",{\"1\":{\"163\":1}}],[\"weight=none\",{\"1\":{\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1}}],[\"weights\",{\"1\":{\"145\":6,\"147\":6,\"162\":6,\"275\":2,\"277\":2,\"284\":4,\"296\":1,\"300\":5,\"477\":3,\"513\":1,\"514\":1,\"528\":1,\"529\":1,\"536\":1}}],[\"weight\",{\"1\":{\"80\":1,\"100\":4,\"142\":2,\"145\":2,\"147\":2,\"163\":1,\"244\":2,\"328\":1,\"401\":3,\"402\":2,\"403\":2,\"404\":7,\"405\":4,\"407\":4,\"513\":9,\"514\":1}}],[\"were\",{\"1\":{\"289\":1}}],[\"webquestions\",{\"1\":{\"462\":1}}],[\"webtext2\",{\"1\":{\"461\":1}}],[\"webtext的多样性和规模是关键\",{\"1\":{\"455\":1}}],[\"webtext训练集和测试集的困惑度同步下降\",{\"1\":{\"455\":1}}],[\"webtext\",{\"1\":{\"454\":4,\"457\":1}}],[\"web\",{\"1\":{\"278\":1}}],[\"weakref\",{\"1\":{\"657\":2,\"658\":1,\"666\":1}}],[\"weakly\",{\"1\":{\"278\":1}}],[\"wear\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"we\",{\"0\":{\"474\":1},\"1\":{\"78\":1,\"206\":2,\"472\":1,\"527\":1,\"529\":1,\"541\":1,\"558\":1}}],[\"w₂\",{\"1\":{\"74\":1}}],[\"winogender\",{\"1\":{\"469\":1,\"470\":1,\"471\":1,\"482\":1,\"484\":1}}],[\"winograd\",{\"1\":{\"455\":1}}],[\"windows\",{\"1\":{\"338\":1}}],[\"window\",{\"1\":{\"83\":4}}],[\"wikitext2是wikitext\",{\"1\":{\"510\":1}}],[\"wikitext\",{\"1\":{\"455\":2,\"510\":15,\"514\":1}}],[\"wikipedia\",{\"1\":{\"454\":1,\"461\":1,\"493\":1,\"494\":1,\"498\":1,\"674\":1}}],[\"will\",{\"1\":{\"412\":1}}],[\"wild\",{\"1\":{\"157\":2}}],[\"width=4\",{\"1\":{\"323\":1}}],[\"width=600\",{\"1\":{\"83\":1}}],[\"width\",{\"1\":{\"40\":1,\"142\":3,\"145\":7,\"146\":4,\"147\":6,\"160\":10,\"289\":1,\"323\":1}}],[\"wise\",{\"1\":{\"30\":2,\"46\":1,\"59\":3,\"70\":2,\"327\":1,\"447\":1}}],[\"without\",{\"0\":{\"475\":1},\"1\":{\"252\":2}}],[\"with\",{\"0\":{\"228\":1,\"476\":1},\"1\":{\"28\":4,\"37\":1,\"40\":1,\"68\":2,\"76\":3,\"83\":4,\"145\":2,\"147\":2,\"148\":2,\"161\":1,\"162\":1,\"163\":1,\"206\":2,\"226\":1,\"247\":1,\"273\":3,\"275\":2,\"277\":2,\"284\":1,\"289\":1,\"410\":3,\"412\":11,\"416\":2,\"466\":1,\"474\":1,\"477\":1,\"510\":3,\"511\":3,\"514\":1,\"520\":3,\"557\":1,\"658\":2,\"666\":2}}],[\"w\",{\"1\":{\"30\":3,\"34\":1,\"40\":4,\"41\":2,\"59\":4,\"145\":1,\"244\":1,\"272\":5,\"282\":1,\"289\":1,\"291\":4,\"292\":1,\"293\":1,\"296\":1,\"405\":2,\"407\":1,\"410\":5,\"412\":5,\"420\":1,\"470\":1,\"510\":2,\"511\":1,\"558\":4,\"666\":2}}],[\"who\",{\"1\":{\"455\":1,\"472\":1}}],[\"whole\",{\"1\":{\"258\":2}}],[\"whether\",{\"1\":{\"412\":1}}],[\"where\",{\"1\":{\"70\":1,\"71\":1,\"338\":1,\"404\":1}}],[\"what\",{\"0\":{\"234\":1},\"1\":{\"596\":1}}],[\"white\",{\"1\":{\"273\":1}}],[\"while\",{\"1\":{\"29\":1,\"411\":1,\"412\":1,\"474\":1,\"638\":1,\"652\":1,\"654\":1,\"656\":1,\"658\":1,\"662\":1,\"666\":1}}],[\"which\",{\"1\":{\"28\":2,\"83\":3,\"338\":1,\"531\":1}}],[\"why\",{\"0\":{\"474\":1},\"1\":{\"28\":2}}],[\"解题过程可靠\",{\"1\":{\"674\":1}}],[\"解释\",{\"1\":{\"323\":2}}],[\"解包语法\",{\"1\":{\"651\":1}}],[\"解包\",{\"1\":{\"363\":2}}],[\"解包数据\",{\"1\":{\"296\":1}}],[\"解包编码器输出的不同层级特征\",{\"1\":{\"46\":1}}],[\"解组\",{\"1\":{\"71\":1}}],[\"解码成自然语言\",{\"1\":{\"542\":1}}],[\"解码成自然语言文本\",{\"1\":{\"542\":1}}],[\"解码生成的token\",{\"1\":{\"286\":1}}],[\"解码生成的\",{\"1\":{\"143\":1}}],[\"解码过程\",{\"1\":{\"99\":1}}],[\"解码过程中点特征的语言引导能力\",{\"1\":{\"71\":1}}],[\"解码\",{\"1\":{\"98\":1,\"268\":2}}],[\"解码器层\",{\"1\":{\"556\":1}}],[\"解码器的最终输出通过一个线性层和\",{\"1\":{\"548\":1}}],[\"解码器进行文本生成\",{\"1\":{\"285\":1}}],[\"解码器生成文本描述\",{\"1\":{\"285\":1}}],[\"解码器架构\",{\"1\":{\"142\":1}}],[\"解码器模型配置文件路径\",{\"1\":{\"142\":1}}],[\"解码器模型\",{\"1\":{\"138\":1,\"142\":1}}],[\"解码器模型难以用于图文检索\",{\"1\":{\"120\":1}}],[\"解码器参数共享与解耦\",{\"0\":{\"134\":1}}],[\"解码器适合生成任务但不适用于检索\",{\"1\":{\"122\":1}}],[\"解码器部分\",{\"1\":{\"101\":1}}],[\"解码器\",{\"1\":{\"98\":2,\"99\":1,\"101\":1,\"126\":1,\"170\":1,\"190\":2,\"548\":1,\"557\":1}}],[\"解码器与点云特征交互\",{\"1\":{\"76\":2}}],[\"解码器将这些查询与点云特征进行交互\",{\"1\":{\"70\":1}}],[\"解码器融合所有特征以预测可操作性特征\",{\"0\":{\"45\":1},\"1\":{\"40\":1}}],[\"解码结构提取多尺度点特征\",{\"1\":{\"70\":1}}],[\"解码输出\",{\"1\":{\"54\":1}}],[\"解码阶段\",{\"0\":{\"35\":1}}],[\"解决变量重复使用时的梯度累加问题\",{\"1\":{\"663\":1}}],[\"解决子问题阶段\",{\"1\":{\"436\":1}}],[\"解决前景\",{\"1\":{\"404\":1}}],[\"解决模型训练时\",{\"1\":{\"404\":1}}],[\"解决数据集中\",{\"1\":{\"404\":1}}],[\"解决点云姿态不一致问题\",{\"1\":{\"107\":1}}],[\"解决了共享变量梯度重置的问题\",{\"1\":{\"655\":1}}],[\"解决了点云处理中的四大技术难点\",{\"1\":{\"105\":1}}],[\"解决了两个问题\",{\"1\":{\"86\":1}}],[\"解决不同来源物体区域的对齐问题\",{\"1\":{\"48\":1}}],[\"解决方案与创新\",{\"1\":{\"208\":1}}],[\"解决方案与核心设计\",{\"1\":{\"181\":1}}],[\"解决方案\",{\"0\":{\"105\":1},\"1\":{\"29\":1,\"49\":1,\"657\":1}}],[\"解决此问题\",{\"1\":{\"7\":1}}],[\"解析几何属性\",{\"1\":{\"28\":1}}],[\"使开发者能够通过终端与\",{\"1\":{\"685\":1}}],[\"使开发者能以自然的数学表达式编写代码\",{\"1\":{\"660\":1}}],[\"使它们成为了解决复杂问题和应用于多领域的强大工具\",{\"1\":{\"676\":1}}],[\"使它们能够通过阅读大量文本来深入理解语言规则和模式\",{\"1\":{\"673\":1}}],[\"使深度学习框架具备自动求导能力\",{\"1\":{\"662\":1}}],[\"使variable实例具备ndarray的行为特征\",{\"1\":{\"659\":1}}],[\"使转置后的数据在新的布局中是连续存储的\",{\"1\":{\"326\":1}}],[\"使\",{\"1\":{\"190\":1}}],[\"使模型能够更准确地识别何时以及如何调用外部工具\",{\"1\":{\"674\":1}}],[\"使模型能学到更强的图文对齐表示\",{\"1\":{\"161\":1}}],[\"使模型行为更可预测和可控\",{\"1\":{\"674\":1}}],[\"使模型聚焦难分类样本\",{\"1\":{\"404\":1}}],[\"使模型更关注难分类的样本\",{\"1\":{\"404\":1}}],[\"使模型在零样本分类\",{\"1\":{\"190\":1}}],[\"使标准\",{\"1\":{\"169\":1}}],[\"使特征分布更稳定\",{\"1\":{\"108\":1}}],[\"使网络能够应对实际中各种密度变换的情况\",{\"1\":{\"96\":1}}],[\"使网络能学习不同采样密度下局部点云特征的提取\",{\"1\":{\"96\":1}}],[\"使局部特征的表示不够精确\",{\"1\":{\"90\":1}}],[\"使得开发者可以更容易地构建复杂和强大的应用程序\",{\"1\":{\"685\":1}}],[\"使得开发者能够轻松地将私有数据与\",{\"1\":{\"684\":1}}],[\"使得它们可以理解和生成不同媒体类型的内容\",{\"1\":{\"675\":1}}],[\"使得它们能够在统一的语义空间中进行有效的跨模态交互\",{\"1\":{\"41\":2}}],[\"使得gpt\",{\"1\":{\"461\":1}}],[\"使得要充分做区分地训练模型非常有挑战性\",{\"1\":{\"439\":1}}],[\"使得模型训练具备了基础的\",{\"1\":{\"665\":1}}],[\"使得模型可以在资源有限的设备上进行训练和部署\",{\"1\":{\"428\":1}}],[\"使得模型在下游任务上的表现逐渐优化\",{\"1\":{\"423\":1}}],[\"使得模型在训练过程中可以更灵活地平衡这两部分损失\",{\"1\":{\"407\":1}}],[\"使得模型能够区分正负样本对\",{\"1\":{\"240\":1}}],[\"使得模型能够更有效地学习到有判别性的表示\",{\"1\":{\"240\":1}}],[\"使得检测\",{\"1\":{\"396\":1}}],[\"使得\",{\"1\":{\"292\":1,\"679\":1}}],[\"使得预训练模型能够直接应用于下游任务\",{\"1\":{\"274\":1}}],[\"使得后续指令调优时\",{\"1\":{\"226\":1}}],[\"使得语言模型具备视觉融合能力\",{\"1\":{\"191\":1}}],[\"使得配对的图文具有更高的相似度得分\",{\"1\":{\"154\":1}}],[\"使得每个\",{\"1\":{\"136\":1}}],[\"使得每个点的权重之和为1\",{\"1\":{\"100\":1}}],[\"使得采样点在整个点云空间中分布尽可能均匀\",{\"1\":{\"92\":1}}],[\"使得在它们之间可以共享学习到的特征表示的权重\",{\"1\":{\"86\":1}}],[\"使其在长文本理解和复杂任务处理方面具有更强的优势\",{\"1\":{\"674\":1}}],[\"使其在ocr和中文任务上表现优异\",{\"1\":{\"212\":1}}],[\"使其行为更接近numpy数组\",{\"1\":{\"663\":1}}],[\"使其支持左右操作数为variable的情况\",{\"1\":{\"660\":1}}],[\"使其支持更复杂的计算和神经网络的构建\",{\"1\":{\"648\":1}}],[\"使其完全可开源\",{\"1\":{\"480\":1}}],[\"使其学会预测哪一输出更受偏好\",{\"1\":{\"470\":1}}],[\"使其能够建模长距离依赖关系\",{\"1\":{\"454\":1}}],[\"使其能与\",{\"1\":{\"327\":1}}],[\"使其变得\",{\"1\":{\"384\":1}}],[\"使其符合模型的输入要求\",{\"1\":{\"275\":1}}],[\"使其更好地理解和执行用户给出的自然语言指令\",{\"1\":{\"231\":1}}],[\"使其标准化\",{\"1\":{\"108\":1}}],[\"使其姿态统一\",{\"1\":{\"107\":1}}],[\"使其简短但仍有意义\",{\"1\":{\"63\":1}}],[\"使其专注于特定领域\",{\"1\":{\"26\":1}}],[\"使问题更具体地连接目标对象的功能\",{\"1\":{\"63\":1}}],[\"使功能表征与对齐特征相互增强\",{\"1\":{\"56\":1}}],[\"使用统一的大模型可以极大地提高研发效率\",{\"1\":{\"677\":1}}],[\"使用文心\",{\"1\":{\"674\":2}}],[\"使用相似的架构和预训练任务\",{\"1\":{\"673\":1}}],[\"使用梯度下降法寻找rosenbrock函数最小值的代码如下\",{\"1\":{\"667\":1}}],[\"使用梯度下降法能高效找到目标值\",{\"1\":{\"667\":1}}],[\"使用tinypytorch实现的代码如下\",{\"1\":{\"667\":1}}],[\"使用transformer架构为未来的多模态统一提供了可能性\",{\"1\":{\"301\":1}}],[\"使用funcs\",{\"1\":{\"666\":1}}],[\"使用farthest\",{\"1\":{\"89\":1}}],[\"使用函数类名作为标签\",{\"1\":{\"666\":1}}],[\"使用id\",{\"1\":{\"666\":1}}],[\"使用dot\",{\"1\":{\"666\":1}}],[\"使用弱引用解决循环引用\",{\"1\":{\"663\":1}}],[\"使用户无需关心data属性\",{\"1\":{\"659\":1}}],[\"使用with语句便捷切换模式\",{\"1\":{\"658\":1}}],[\"使用output\",{\"1\":{\"657\":1}}],[\"使用赋值运算符\",{\"1\":{\"657\":1}}],[\"使用列表保存待处理的函数\",{\"1\":{\"638\":1}}],[\"使用中心差分近似计算数值微分\",{\"1\":{\"621\":1}}],[\"使用示例\",{\"0\":{\"608\":1},\"1\":{\"658\":1,\"659\":1}}],[\"使用贝叶斯公式\",{\"1\":{\"596\":1}}],[\"使用最广泛的联合概率分布是多元高斯分布\",{\"1\":{\"588\":1}}],[\"使用最大池化聚合局部信息\",{\"1\":{\"96\":1}}],[\"使用掩码机制确保模型在生成目标序列时只能看到当前及之前的词\",{\"1\":{\"548\":1}}],[\"使用nltk库\",{\"1\":{\"510\":1}}],[\"使用nltk库提供的sent\",{\"1\":{\"411\":1}}],[\"使用单句对\",{\"1\":{\"495\":1}}],[\"使用静态掩码\",{\"1\":{\"493\":1}}],[\"使用更多数据等\",{\"1\":{\"492\":1}}],[\"使用更长的序列以及动态调整掩码模式\",{\"1\":{\"491\":1}}],[\"使用旋转位置嵌入\",{\"1\":{\"481\":1}}],[\"使用sentencepiece的bpe算法\",{\"1\":{\"481\":1}}],[\"使用如下\",{\"1\":{\"470\":1}}],[\"使用余弦学习率衰减\",{\"1\":{\"470\":1}}],[\"使用人类标注者示范的优质输出\",{\"1\":{\"470\":1}}],[\"使用外部语言模型引导生成方向\",{\"1\":{\"469\":1}}],[\"使用数十个\",{\"1\":{\"469\":1}}],[\"使用奖励模型的反馈\",{\"1\":{\"468\":1}}],[\"使用bloom过滤器统计测试集与webtext的8\",{\"1\":{\"455\":1}}],[\"使用无监督预训练来提升在判别式任务上的表现是机器学习研究的长期目标\",{\"1\":{\"450\":1}}],[\"使用无监督预训练超参数设置\",{\"1\":{\"447\":1}}],[\"使用学习的位置嵌入\",{\"1\":{\"447\":1}}],[\"使用标注者示范数据\",{\"1\":{\"470\":1}}],[\"使用标注好的指令\",{\"1\":{\"224\":1}}],[\"使用标准的语言模型目标并最大化其似然\",{\"1\":{\"443\":1}}],[\"使用对应特定任务的监督目标来调整这些参数\",{\"1\":{\"440\":1}}],[\"使用对称函数\",{\"1\":{\"105\":1}}],[\"使用复杂的学习方案以及添加辅助学习目标的组合\",{\"1\":{\"440\":1}}],[\"使用不同的损失函数进行训练\",{\"1\":{\"408\":1}}],[\"使用不同句式结构\",{\"1\":{\"63\":1}}],[\"使用双线性插值\",{\"1\":{\"397\":1}}],[\"使用截断正态分布初始化位置嵌入\",{\"1\":{\"293\":1,\"296\":1}}],[\"使用截断正态分布初始化分类标记\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"使用全连接\",{\"1\":{\"286\":1}}],[\"使用adamw优化器\",{\"1\":{\"272\":1}}],[\"使用池化后的图像特征点积计算特征相似性\",{\"1\":{\"255\":1}}],[\"使用简单的点积或者浅层attention层来表示两种模态特征的相似性\",{\"1\":{\"255\":1}}],[\"使用动量更新\",{\"1\":{\"241\":1}}],[\"使用动量编码器对\",{\"1\":{\"163\":1}}],[\"使用动量编码器获取特征\",{\"1\":{\"145\":1}}],[\"使用倒数第二层视觉特征更有利于细节理解\",{\"1\":{\"229\":1}}],[\"使用较小的\",{\"1\":{\"229\":1}}],[\"使用可训练的投影矩阵\",{\"1\":{\"227\":1}}],[\"使用可学习的\",{\"1\":{\"10\":1}}],[\"使用强化学习优化策略\",{\"1\":{\"224\":1}}],[\"使用上述人类偏好数据\",{\"1\":{\"224\":1}}],[\"使用python的weakref模块创建弱引用\",{\"1\":{\"657\":1}}],[\"使用python的unittest模块编写测试用例\",{\"1\":{\"645\":1}}],[\"使用pycharm导入项目\",{\"1\":{\"519\":1}}],[\"使用perspectiveapi对100k提示生成内容进行毒性评分\",{\"1\":{\"484\":1}}],[\"使用paddleocr生成中英文文本标注\",{\"1\":{\"217\":1}}],[\"使用pointnet++编码点云\",{\"1\":{\"70\":1}}],[\"使用了如下逻辑\",{\"1\":{\"540\":1}}],[\"使用了\",{\"1\":{\"494\":1}}],[\"使用了综合性评估框架\",{\"1\":{\"470\":1}}],[\"使用了混合的图像描述和ocr专用数据集\",{\"1\":{\"215\":1}}],[\"使用了一个偏置项\",{\"1\":{\"78\":1}}],[\"使用完整\",{\"1\":{\"190\":1}}],[\"使用大规模但噪声较多的公开网络图像\",{\"1\":{\"190\":1}}],[\"使用预训练\",{\"1\":{\"175\":1}}],[\"使用平均池化聚合表示\",{\"1\":{\"175\":1}}],[\"使用主分支特征与队列拼接结果计算图像\",{\"1\":{\"161\":1}}],[\"使用的是tnews数据集\",{\"1\":{\"519\":1}}],[\"使用的是transformer\",{\"1\":{\"440\":1}}],[\"使用的数据集\",{\"1\":{\"448\":1}}],[\"使用的\",{\"1\":{\"147\":1}}],[\"使用交叉熵损失衡量生成与真实之间的差异\",{\"1\":{\"285\":1}}],[\"使用交叉熵损失函数\",{\"1\":{\"226\":1}}],[\"使用交叉熵损失\",{\"1\":{\"127\":1}}],[\"使用一个\",{\"1\":{\"238\":1}}],[\"使用一个线性分类头进行预测\",{\"1\":{\"127\":1}}],[\"使用一个轻量级的交叉注意力模块\",{\"1\":{\"72\":1}}],[\"使用训练好的\",{\"1\":{\"120\":1}}],[\"使用神经网络直接学习对称函数\",{\"1\":{\"115\":1}}],[\"使用分块策略\",{\"1\":{\"112\":1}}],[\"使用分割头预测最终的\",{\"0\":{\"46\":1},\"1\":{\"40\":1}}],[\"使用多语言llama初始化中间件\",{\"1\":{\"181\":1}}],[\"使用多模态混合的编码器\",{\"1\":{\"138\":1}}],[\"使用多层\",{\"1\":{\"111\":1}}],[\"使用多个不同的\",{\"1\":{\"138\":1}}],[\"使用多个不同大小的邻域球\",{\"1\":{\"96\":1}}],[\"使用多个\",{\"1\":{\"92\":1}}],[\"使用多个阈值计算\",{\"1\":{\"82\":1}}],[\"使用反距离加权\",{\"1\":{\"100\":1}}],[\"使用mini\",{\"1\":{\"88\":1}}],[\"使用第4列作为默认掩码\",{\"1\":{\"83\":1}}],[\"使用另一个注意力模块\",{\"1\":{\"74\":2}}],[\"使用robert编码文本\",{\"1\":{\"70\":1}}],[\"使用残差连接\",{\"1\":{\"45\":1}}],[\"使用自注意力机制提炼两个模态之间的语义一致性\",{\"1\":{\"41\":1}}],[\"使用\",{\"0\":{\"372\":1},\"1\":{\"8\":1,\"31\":1,\"40\":4,\"43\":1,\"45\":4,\"46\":1,\"63\":2,\"70\":1,\"73\":1,\"76\":2,\"82\":2,\"92\":2,\"96\":1,\"98\":2,\"101\":2,\"105\":1,\"107\":1,\"109\":2,\"111\":1,\"112\":2,\"126\":2,\"128\":2,\"140\":1,\"143\":4,\"161\":1,\"166\":1,\"196\":1,\"203\":1,\"212\":1,\"214\":1,\"224\":1,\"226\":2,\"227\":2,\"228\":1,\"231\":1,\"239\":1,\"282\":1,\"285\":1,\"370\":1,\"372\":1,\"387\":1,\"428\":1,\"447\":1,\"470\":3,\"471\":2,\"472\":1,\"493\":2,\"513\":2,\"542\":1,\"666\":1}}],[\"未患病\",{\"1\":{\"569\":1}}],[\"未归一化\",{\"1\":{\"540\":1}}],[\"未微调模型即接近sota\",{\"1\":{\"482\":1}}],[\"未能回答\",{\"1\":{\"471\":1}}],[\"未登录词\",{\"1\":{\"409\":1}}],[\"未使用\",{\"1\":{\"403\":1,\"404\":1}}],[\"未经扰动训练时\",{\"1\":{\"112\":1}}],[\"未经roi\",{\"1\":{\"59\":1}}],[\"未见\",{\"1\":{\"49\":1,\"65\":1}}],[\"未来将带来更多创新特性和性能提升\",{\"1\":{\"684\":1}}],[\"未来将持续优化对话和推理能力\",{\"1\":{\"222\":1}}],[\"未来方向\",{\"1\":{\"472\":1}}],[\"未来若需面向多元人群\",{\"1\":{\"472\":1}}],[\"未来研究方向\",{\"1\":{\"456\":1}}],[\"未来可进一步探索以下方向以提升\",{\"1\":{\"138\":1}}],[\"未来\",{\"1\":{\"26\":1}}],[\"未充分挖掘物体间共享的几何不变性\",{\"1\":{\"6\":1}}],[\"有较好的可解释性和可追踪性\",{\"1\":{\"681\":1}}],[\"有限的数据集可能无法显著提高性能\",{\"1\":{\"681\":1}}],[\"有限上下文容量成为当前few\",{\"1\":{\"463\":1}}],[\"有标签数据\",{\"1\":{\"675\":1}}],[\"有5万次\",{\"1\":{\"667\":1}}],[\"有病\",{\"1\":{\"569\":1}}],[\"有病的人不能诊断为健康\",{\"1\":{\"347\":1}}],[\"有一点不同就是bert预训练阶段的学习目标是\",{\"1\":{\"514\":1}}],[\"有一个问题\",{\"1\":{\"396\":1}}],[\"有的问题就是没有答案的\",{\"1\":{\"508\":1}}],[\"有的是不同义的\",{\"1\":{\"448\":1}}],[\"有的是同义的\",{\"1\":{\"448\":1}}],[\"有了分词器后\",{\"1\":{\"512\":1}}],[\"有了\",{\"1\":{\"477\":1}}],[\"有了以上\",{\"1\":{\"75\":1}}],[\"有帮助\",{\"1\":{\"468\":1,\"493\":1}}],[\"有时会产生与客观事实不符的信息\",{\"1\":{\"679\":1}}],[\"有时我们也会讨论高斯分布的精度\",{\"1\":{\"584\":1}}],[\"有时更低\",{\"1\":{\"471\":1}}],[\"有时更贴近人类学习习惯\",{\"1\":{\"461\":1}}],[\"有时也会加入一个平滑项\",{\"1\":{\"401\":1}}],[\"有监督微调\",{\"0\":{\"444\":1}}],[\"有监督预训练还是占据主导地位\",{\"1\":{\"238\":1}}],[\"有两个主要原因\",{\"1\":{\"440\":1}}],[\"有2个是正确的\",{\"1\":{\"435\":1}}],[\"有1个是错的\",{\"1\":{\"435\":1}}],[\"有几个注意点\",{\"1\":{\"434\":1}}],[\"有以下一些原因\",{\"1\":{\"415\":1}}],[\"有以下几个优点\",{\"1\":{\"403\":1}}],[\"有多少个\",{\"1\":{\"565\":1}}],[\"有多少点被正确分类\",{\"1\":{\"401\":1}}],[\"有多少组点云\",{\"1\":{\"107\":1}}],[\"有没有低成本的方法微调大模型\",{\"1\":{\"424\":1}}],[\"有没有办法能够减少\",{\"1\":{\"423\":1}}],[\"有没有覆盖正确区域\",{\"1\":{\"401\":1}}],[\"有没有可能将\",{\"1\":{\"239\":1}}],[\"有\",{\"1\":{\"323\":1,\"444\":1,\"471\":1,\"505\":3,\"567\":3,\"569\":1,\"570\":1,\"590\":1,\"596\":1,\"673\":1}}],[\"有助于构建更通用\",{\"1\":{\"472\":1}}],[\"有助于迁移\",{\"1\":{\"449\":1}}],[\"有助于模型学习到不同方向的特征\",{\"1\":{\"290\":1}}],[\"有助于加快收敛\",{\"1\":{\"229\":1}}],[\"有可能因为对于现有的任务来说\",{\"1\":{\"259\":1}}],[\"有个细节\",{\"1\":{\"241\":1}}],[\"有些\",{\"1\":{\"157\":1}}],[\"有些位置被标记为\",{\"1\":{\"92\":1}}],[\"有效突破了输入长度的限制\",{\"1\":{\"679\":1}}],[\"有效\",{\"1\":{\"471\":1}}],[\"有效地从无标记的原始文本中学习的能力能减轻对监督学习的依赖\",{\"1\":{\"440\":1}}],[\"有效适应输入图像的不同分辨率和宽高比\",{\"1\":{\"216\":1}}],[\"有效提升了下游任务中的表现\",{\"1\":{\"140\":1}}],[\"有效语言部分\",{\"1\":{\"43\":1}}],[\"有效的部分\",{\"1\":{\"43\":1}}],[\"有望提升机器人在未知环境中的自主交互能力\",{\"1\":{\"26\":1}}],[\"有问题需要咨询的小伙伴\",{\"1\":{\"2\":1}}],[\"大脑\",{\"1\":{\"673\":1}}],[\"大批量训练提升mlm困惑度\",{\"1\":{\"497\":1}}],[\"大批量训练\",{\"1\":{\"495\":2,\"497\":1}}],[\"大规模语言模型在任务通用性与灵活性方面具有巨大潜力\",{\"1\":{\"465\":1}}],[\"大规模地在zero\",{\"1\":{\"464\":1}}],[\"大规模视觉编码器\",{\"1\":{\"189\":1}}],[\"大部分任务通过自然语言指令表达意图\",{\"1\":{\"470\":1}}],[\"大部分任务\",{\"1\":{\"447\":1}}],[\"大部分深度学习方法需要大量人工标注的数据\",{\"1\":{\"440\":1}}],[\"大公司或者研究机构\",{\"1\":{\"424\":1}}],[\"大\",{\"1\":{\"414\":1}}],[\"大模型api使用\",{\"0\":{\"689\":1}}],[\"大模型开发与传统\",{\"1\":{\"686\":1}}],[\"大模型开发却更多是一个工程问题\",{\"1\":{\"686\":1}}],[\"大模型开发\",{\"0\":{\"686\":1},\"1\":{\"686\":2}}],[\"大模型可以成为\",{\"1\":{\"677\":1}}],[\"大模型研发\",{\"1\":{\"674\":1}}],[\"大模型领域仅存在预训练阶段的\",{\"1\":{\"673\":1}}],[\"大模型加速技术之kv\",{\"1\":{\"473\":1}}],[\"大模型使用较大的batch\",{\"1\":{\"461\":1}}],[\"大模型中有其中一部分参数\",{\"1\":{\"420\":1}}],[\"大模型参数很多\",{\"1\":{\"420\":1}}],[\"大模型的性能不断增长\",{\"1\":{\"673\":1}}],[\"大模型的微调有以下几条技术路线\",{\"1\":{\"416\":1}}],[\"大模型的微调分成两条技术路线\",{\"1\":{\"416\":1}}],[\"大模型的推理成本越高\",{\"1\":{\"415\":1}}],[\"大模型\",{\"1\":{\"414\":1,\"674\":2}}],[\"大模型微调大致发展历史\",{\"0\":{\"424\":1}}],[\"大模型微调\",{\"0\":{\"413\":1},\"1\":{\"413\":1}}],[\"大多数样本会集中在这个区域上\",{\"1\":{\"592\":1}}],[\"大多数样本应该靠近原点\",{\"1\":{\"591\":1}}],[\"大多数sota模型依赖于\",{\"1\":{\"464\":1}}],[\"大多数方法求解出来结果都一样的答案\",{\"1\":{\"435\":1}}],[\"大多数实际应用中都是如此\",{\"1\":{\"343\":1}}],[\"大多使用从网络收集的嘈杂图文对作为训练数据\",{\"1\":{\"120\":1}}],[\"大家参考仓库源码即可\",{\"1\":{\"513\":1}}],[\"大家可以自行拉取项目完整代码进行学习\",{\"1\":{\"300\":1}}],[\"大家注意区分\",{\"1\":{\"282\":1}}],[\"大于\",{\"1\":{\"294\":1}}],[\"大致上两者结构是相同的\",{\"1\":{\"294\":1}}],[\"大大提高学习效率\",{\"1\":{\"287\":1}}],[\"大幅降低训练成本\",{\"1\":{\"280\":1}}],[\"大幅超越所有基线\",{\"1\":{\"23\":1}}],[\"大放异彩的一年\",{\"1\":{\"270\":1}}],[\"大字典是怎么做到的\",{\"1\":{\"238\":1}}],[\"大型模型不仅可以缩短每个具体应用的开发周期\",{\"1\":{\"677\":1}}],[\"大型视觉编码器\",{\"1\":{\"197\":1}}],[\"大型语言模型\",{\"1\":{\"181\":1,\"208\":1,\"224\":1,\"468\":1,\"679\":1}}],[\"大语言模型的两个核心能力\",{\"1\":{\"686\":1}}],[\"大语言模型的发展历程虽然只有短短不到五年的时间\",{\"1\":{\"674\":1}}],[\"大语言模型是这个新模式的典型例子\",{\"1\":{\"677\":1}}],[\"大语言模型是一种具有强大语言处理能力的技术\",{\"1\":{\"675\":1}}],[\"大语言模型具有多种显著特点\",{\"1\":{\"675\":1}}],[\"大语言模型应用开发基础知识速览\",{\"1\":{\"672\":1}}],[\"大语言模型应用开发课程\",{\"0\":{\"561\":1}}],[\"大语言模型\",{\"0\":{\"118\":1,\"673\":1},\"1\":{\"221\":1,\"673\":2,\"686\":1}}],[\"大小的图像\",{\"1\":{\"290\":1}}],[\"大小的队列特征\",{\"1\":{\"238\":1}}],[\"大小\",{\"1\":{\"242\":1,\"290\":1,\"396\":1,\"565\":1}}],[\"大小和字典大小是等价的\",{\"1\":{\"242\":1}}],[\"大小和字典大小剥离开\",{\"1\":{\"241\":1}}],[\"大小要足够的大\",{\"1\":{\"242\":1}}],[\"大小不超过\",{\"1\":{\"105\":1}}],[\"大小为\",{\"1\":{\"59\":1}}],[\"大区域\",{\"1\":{\"92\":1}}],[\"大局部区域\",{\"1\":{\"92\":1}}],[\"大量实验验证了我们提出的\",{\"1\":{\"26\":1}}],[\"万\",{\"1\":{\"674\":1}}],[\"万亿个\",{\"1\":{\"674\":1}}],[\"万亿参数\",{\"1\":{\"674\":1}}],[\"万亿\",{\"1\":{\"414\":1,\"674\":1}}],[\"万个特征\",{\"1\":{\"242\":1}}],[\"万个标注完整的\",{\"1\":{\"26\":1}}],[\"万增长到\",{\"1\":{\"239\":1}}],[\"万条图文对\",{\"1\":{\"226\":1}}],[\"万高质量指令数据\",{\"1\":{\"190\":1}}],[\"万象多模态大模型\",{\"0\":{\"179\":1,\"206\":1}}],[\"万张图像\",{\"1\":{\"131\":1}}],[\"万张交互图像与超过\",{\"1\":{\"26\":1}}],[\"涵盖广泛的主题和写作风格\",{\"1\":{\"454\":1}}],[\"涵盖多种任务类型\",{\"1\":{\"227\":1}}],[\"涵盖常见场景和文档图像\",{\"1\":{\"208\":1}}],[\"涵盖图像描述\",{\"1\":{\"190\":1}}],[\"涵盖\",{\"1\":{\"26\":1,\"217\":1,\"494\":1}}],[\"能跑起来\",{\"1\":{\"665\":1}}],[\"能更精准地控制内存释放时机\",{\"1\":{\"657\":1}}],[\"能更有效地重组视觉特征为llm兼容的序列\",{\"1\":{\"188\":1}}],[\"能更有效应对含噪声的网络数据\",{\"1\":{\"149\":1}}],[\"能不能自己生成答案\",{\"1\":{\"542\":1}}],[\"能提升掩码语言模型的困惑度和下游任务性能\",{\"1\":{\"495\":1}}],[\"能处理非训练分布指令\",{\"1\":{\"471\":1}}],[\"能带来质的改善\",{\"1\":{\"468\":1}}],[\"能力提升伴随风险增加\",{\"1\":{\"484\":1}}],[\"能力\",{\"1\":{\"465\":1,\"665\":1,\"674\":2}}],[\"能力分布不均\",{\"1\":{\"463\":1}}],[\"能生成连贯但虚构的内容\",{\"1\":{\"455\":1}}],[\"能缓解类别不平衡问题\",{\"1\":{\"402\":1}}],[\"能缓解弱标注图文数据中的噪声问题\",{\"1\":{\"150\":1}}],[\"能自适应频率和相位\",{\"1\":{\"395\":1}}],[\"能起作用的原因在于\",{\"1\":{\"292\":1}}],[\"能否基于互联网上的大量文本来预训练视觉模型\",{\"1\":{\"278\":1}}],[\"能让无监督的对比学习取得很好的效果\",{\"1\":{\"237\":1}}],[\"能根据用户指令回答问题\",{\"1\":{\"231\":1}}],[\"能反映边缘响应质量\",{\"1\":{\"82\":1}}],[\"能够以结构化的格式返回信息\",{\"1\":{\"684\":1}}],[\"能够以指数级减少所需的神经元数量\",{\"1\":{\"395\":1}}],[\"能够根据任务指令执行任务\",{\"1\":{\"676\":1}}],[\"能够理解和生成依赖于前文的文本内容\",{\"1\":{\"675\":1}}],[\"能够分析和理解用户提供的图片\",{\"1\":{\"674\":1}}],[\"能够匹配甚至超越后续提出的多种模型\",{\"1\":{\"492\":1}}],[\"能够泛化\",{\"1\":{\"472\":1}}],[\"能够在零样本\",{\"1\":{\"452\":1}}],[\"能够在开放场景下支持可供性理解\",{\"1\":{\"26\":1}}],[\"能够通过检索对应应用场景数据的方式\",{\"1\":{\"679\":1}}],[\"能够通过预测任务的自然语言演示\",{\"1\":{\"456\":1}}],[\"能够通过预测任务的自然语言描述\",{\"1\":{\"453\":1}}],[\"能够通过自有数据\",{\"1\":{\"415\":1}}],[\"能够通过多层\",{\"1\":{\"292\":1}}],[\"能够动态地聚合图像信息\",{\"1\":{\"292\":1}}],[\"能够更好地聚合图像信息\",{\"1\":{\"292\":1}}],[\"能够很近似在整个数据集上做的多分类损失\",{\"1\":{\"241\":1}}],[\"能够很好地支持\",{\"1\":{\"62\":1}}],[\"能够有效重组视觉特征\",{\"1\":{\"190\":1}}],[\"能够\",{\"1\":{\"157\":1}}],[\"能够自适应地选择最适合的特征尺度进行组合\",{\"1\":{\"96\":1}}],[\"能够突破预定义样本空间的限制\",{\"1\":{\"26\":1}}],[\"能正确捕捉如\",{\"1\":{\"23\":1}}],[\"结束调用\",{\"1\":{\"371\":1}}],[\"结束\",{\"1\":{\"370\":1,\"542\":1}}],[\"结束符\",{\"1\":{\"286\":1}}],[\"结果分析\",{\"1\":{\"662\":1}}],[\"结果验证\",{\"1\":{\"662\":2}}],[\"结果包含误差\",{\"1\":{\"623\":1}}],[\"结果是一个形状为\",{\"1\":{\"325\":1}}],[\"结果\",{\"0\":{\"462\":1,\"471\":1,\"482\":1},\"1\":{\"309\":1,\"492\":1,\"497\":2,\"498\":2,\"597\":1}}],[\"结果如下表3\",{\"1\":{\"448\":1}}],[\"结果如下\",{\"1\":{\"297\":1}}],[\"结果如下图所示\",{\"1\":{\"293\":1,\"667\":1}}],[\"结果发现在imagenet数据集上能够带来3\",{\"1\":{\"274\":1}}],[\"结果显示\",{\"1\":{\"112\":1,\"193\":1,\"455\":1,\"460\":1,\"495\":1}}],[\"结果就不会变\",{\"1\":{\"105\":1}}],[\"结果表明\",{\"1\":{\"75\":1}}],[\"结合上述分析\",{\"1\":{\"687\":1}}],[\"结合特殊的数据或业务逻辑来提供独特功能的应用称为大模型开发\",{\"1\":{\"686\":1}}],[\"结合检索到的信息和模型的生成能力\",{\"1\":{\"679\":1}}],[\"结合全概率公式\",{\"1\":{\"571\":1}}],[\"结合全局池化增强语义表达\",{\"1\":{\"46\":1}}],[\"结合缓存的attention\",{\"1\":{\"285\":1}}],[\"结合低分辨率和高分辨率特征\",{\"1\":{\"211\":1}}],[\"结合文本通过多模态编码器处理\",{\"1\":{\"149\":1}}],[\"结合文本描述提升语义理解\",{\"1\":{\"51\":1}}],[\"结合\",{\"1\":{\"105\":1,\"190\":1}}],[\"结合了层级特征提取和多尺度融合机制\",{\"1\":{\"98\":1}}],[\"结合论文理解这些指标的意义\",{\"1\":{\"82\":1}}],[\"结合人工+gpt\",{\"1\":{\"69\":1}}],[\"结合其对应的功能类型和原始点云标注信息\",{\"1\":{\"64\":1}}],[\"结合cot在目标检测\",{\"1\":{\"7\":1}}],[\"结构一致\",{\"1\":{\"590\":1}}],[\"结构正常的区间组合\",{\"1\":{\"566\":1}}],[\"结构化问答\",{\"1\":{\"463\":1}}],[\"结构与主模型相同\",{\"1\":{\"160\":1}}],[\"结构规整\",{\"1\":{\"114\":1}}],[\"结构单一\",{\"1\":{\"112\":1}}],[\"结构简单\",{\"1\":{\"112\":1}}],[\"结构多样性\",{\"1\":{\"63\":1}}],[\"结构的隐式关联\",{\"1\":{\"52\":1}}],[\"结构融合\",{\"1\":{\"33\":1}}],[\"结构\",{\"0\":{\"549\":1,\"551\":1,\"555\":1},\"1\":{\"32\":1,\"98\":1,\"447\":1,\"493\":1,\"548\":1}}],[\"结论如下\",{\"1\":{\"134\":1}}],[\"结论\",{\"0\":{\"26\":1,\"198\":1,\"222\":1,\"450\":1,\"465\":1},\"1\":{\"359\":1,\"472\":1}}],[\"同理\",{\"1\":{\"238\":1,\"568\":1}}],[\"同一卷积核在不同位置重复使用\",{\"1\":{\"395\":1}}],[\"同一个观测结果\",{\"1\":{\"597\":1}}],[\"同一个句子在训练的不同批次\",{\"1\":{\"495\":1}}],[\"同一个视频里的任意两帧是正样本\",{\"1\":{\"235\":1}}],[\"同一个类别的物体处于相邻的区域\",{\"1\":{\"234\":1}}],[\"同一物体区域可能支持多功能\",{\"1\":{\"49\":1}}],[\"同一物体在不同交互图像中被推理出不同的\",{\"1\":{\"25\":1}}],[\"同形状的矩阵\",{\"1\":{\"163\":1}}],[\"同样可以比较好地记录二维信息\",{\"1\":{\"293\":1}}],[\"同样需要位置编码来记录各图像块之间的位置信息\",{\"1\":{\"293\":1}}],[\"同样\",{\"1\":{\"291\":1,\"426\":1,\"660\":1,\"674\":1,\"687\":1}}],[\"同样会将像素值从\",{\"1\":{\"290\":1}}],[\"同样会为coco数据集中每个样本的caption前添加固定长度的prompt\",{\"1\":{\"145\":1}}],[\"同样是数据增强的手段\",{\"1\":{\"290\":1}}],[\"同样给计算机视觉领域带来了巨大影响\",{\"1\":{\"270\":1}}],[\"同样地\",{\"1\":{\"156\":1,\"508\":1,\"597\":1}}],[\"同时降低了调用成本\",{\"1\":{\"679\":1}}],[\"同时降维\",{\"1\":{\"34\":1,\"35\":1}}],[\"同时还开源了代码模型和数学模型\",{\"1\":{\"674\":1}}],[\"同时还能适应更多的参数\",{\"1\":{\"428\":1}}],[\"同时开源了用\",{\"1\":{\"674\":1}}],[\"同时发布了\",{\"1\":{\"674\":2}}],[\"同时减少参数量和计算量\",{\"1\":{\"674\":1}}],[\"同时又优先处理列表末尾的函数\",{\"1\":{\"655\":1}}],[\"同时计算self\",{\"1\":{\"547\":1}}],[\"同时通过masked\",{\"1\":{\"514\":1}}],[\"同时通过引入动量蒸馏\",{\"1\":{\"149\":1}}],[\"同时进行\",{\"1\":{\"507\":1}}],[\"同时表明bert的掩码语言模型目标在优化后仍具有竞争力\",{\"1\":{\"491\":1}}],[\"同时分析模型偏见\",{\"1\":{\"480\":1}}],[\"同时返回对应的key和val\",{\"1\":{\"477\":1}}],[\"同时构建适用于未来更强\",{\"1\":{\"472\":1}}],[\"同时允许空格合并以提高压缩效率\",{\"1\":{\"454\":1}}],[\"同时揭示了模型容量与任务性能之间的紧密关联\",{\"1\":{\"453\":1}}],[\"同时使用这个旁路的更新来模拟\",{\"1\":{\"425\":1}}],[\"同时列表末尾追加<\",{\"1\":{\"411\":1}}],[\"同时末尾加上\",{\"1\":{\"410\":1}}],[\"同时完成断句分词任务\",{\"1\":{\"410\":1}}],[\"同时避免\",{\"1\":{\"409\":1}}],[\"同时也促进了社区的合作和共享\",{\"1\":{\"685\":1}}],[\"同时也引发了对未来人工智能的无限探索\",{\"1\":{\"678\":1}}],[\"同时也引发了对其伦理和风险问题的关注\",{\"1\":{\"675\":1}}],[\"同时也是方差\",{\"1\":{\"577\":1}}],[\"同时也是\",{\"1\":{\"405\":1}}],[\"同时也指运用该方法构建的模型\",{\"1\":{\"271\":1}}],[\"同时保持了向后兼容性\",{\"1\":{\"684\":1}}],[\"同时保持了低资源部署的高效性\",{\"1\":{\"674\":1}}],[\"同时保持了模型性能\",{\"1\":{\"674\":2}}],[\"同时保持开源可复现性\",{\"1\":{\"486\":1}}],[\"同时保持预先训练的权重不变\",{\"1\":{\"424\":1}}],[\"同时保持空间连续性\",{\"1\":{\"399\":1}}],[\"同时保留最有代表性的空间信息\",{\"1\":{\"396\":1}}],[\"同时保留其捕捉长距离依赖的优势\",{\"1\":{\"299\":1}}],[\"同时保留全局缩略图以捕捉上下文信息\",{\"1\":{\"207\":1}}],[\"同时作为特征数量\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"同时会将像素值从\",{\"1\":{\"290\":1}}],[\"同时删除不相关的视觉信息\",{\"1\":{\"286\":1}}],[\"同时因llm而具有了视觉推理能力\",{\"1\":{\"280\":1}}],[\"同时具有很好的性能\",{\"1\":{\"280\":1}}],[\"同时最小化个负样本的相似度\",{\"1\":{\"272\":1}}],[\"同时key\",{\"1\":{\"246\":1}}],[\"同时认为\",{\"1\":{\"235\":1}}],[\"同时支持对比学习\",{\"1\":{\"188\":1}}],[\"同时支持编码器\",{\"1\":{\"120\":1}}],[\"同时将flower\",{\"1\":{\"275\":1}}],[\"同时将对应的\",{\"1\":{\"163\":1}}],[\"同时将运行时输出写入日志\",{\"1\":{\"83\":1}}],[\"同时获取对应的\",{\"1\":{\"162\":1}}],[\"同时获取裁剪后的物体框\",{\"1\":{\"58\":1}}],[\"同时摒弃目标检测器\",{\"1\":{\"150\":1}}],[\"同时结合了一些前人工作的改进\",{\"1\":{\"674\":1}}],[\"同时结合图文对比\",{\"1\":{\"129\":1}}],[\"同时结合跨模态特征对齐\",{\"1\":{\"26\":1}}],[\"同时区分负样本\",{\"1\":{\"127\":1}}],[\"同时优化三个预训练目标\",{\"1\":{\"127\":1}}],[\"同时兼顾理解与生成任务\",{\"1\":{\"122\":1}}],[\"同时\",{\"1\":{\"120\":1,\"427\":1,\"460\":1,\"464\":1,\"502\":1,\"660\":1,\"684\":1,\"686\":2}}],[\"同时确保这些分区的处理方式允许在它们之间共享模型权重\",{\"1\":{\"86\":1}}],[\"同时缺少\",{\"1\":{\"86\":1}}],[\"同时等比例对物体框做同样的缩放\",{\"1\":{\"58\":1}}],[\"同时记录每类物体共对应多少不同的点云\",{\"1\":{\"58\":1}}],[\"同时记录每类物体对应的样本总量\",{\"1\":{\"29\":1}}],[\"同时判断是否与当前图片交互行为一致\",{\"1\":{\"29\":1}}],[\"同时包含部分新物体类别\",{\"1\":{\"20\":1}}],[\"时的rosenbrock函数\",{\"1\":{\"667\":1}}],[\"时展开变量列表\",{\"1\":{\"651\":1}}],[\"时非常有用\",{\"1\":{\"582\":1}}],[\"时取\",{\"1\":{\"576\":1}}],[\"时让它同时进行两个任务\",{\"1\":{\"504\":1}}],[\"时间小于\",{\"1\":{\"566\":1}}],[\"时间\",{\"1\":{\"504\":1}}],[\"时间戳等\",{\"1\":{\"114\":1}}],[\"时代的开启\",{\"1\":{\"673\":1}}],[\"时代推向了\",{\"1\":{\"464\":1}}],[\"时代\",{\"1\":{\"464\":1}}],[\"时才涌现出来的能力\",{\"1\":{\"434\":1}}],[\"时使用\",{\"1\":{\"294\":1}}],[\"时性能会下降\",{\"1\":{\"215\":1}}],[\"时不是用\",{\"1\":{\"145\":1}}],[\"时\",{\"1\":{\"24\":1,\"78\":1,\"162\":1,\"238\":2,\"325\":1,\"405\":4,\"425\":1,\"471\":1,\"477\":1,\"505\":1,\"566\":1,\"567\":1,\"582\":1,\"587\":1,\"656\":1}}],[\"下载地址\",{\"1\":{\"510\":1}}],[\"下句的\",{\"1\":{\"506\":1}}],[\"下一个步骤会介绍并实现另一种优化方法\",{\"1\":{\"667\":1}}],[\"下一个句子预测损失\",{\"1\":{\"538\":1}}],[\"下一句话\",{\"1\":{\"506\":1}}],[\"下一句预测\",{\"1\":{\"493\":1,\"513\":2}}],[\"下一轮推理时直接读取缓存结果\",{\"1\":{\"477\":1}}],[\"下一轮计算时直接读取缓存结果\",{\"1\":{\"474\":1}}],[\"下一步只送入新\",{\"1\":{\"477\":1}}],[\"下图按照时间线给出了\",{\"1\":{\"674\":1}}],[\"下图展示的是步骤7中以元组形式返回的kv\",{\"1\":{\"477\":1}}],[\"下图展示了\",{\"1\":{\"284\":1}}],[\"下图给出了有\",{\"1\":{\"476\":1}}],[\"下图给出了无\",{\"1\":{\"475\":1}}],[\"下进行训练\",{\"1\":{\"470\":1}}],[\"下表2是作者模型和之前sota模型nli的结果比较\",{\"1\":{\"448\":1}}],[\"下表中对比了vit\",{\"1\":{\"299\":1}}],[\"下\",{\"1\":{\"328\":2}}],[\"下训练并\",{\"1\":{\"200\":1}}],[\"下游任务微调\",{\"0\":{\"175\":1}}],[\"下采样后的点坐标数据\",{\"1\":{\"100\":1}}],[\"下采样点的特征\",{\"1\":{\"100\":1}}],[\"下采样点数量\",{\"1\":{\"100\":1}}],[\"下采样点对应的特征数据\",{\"1\":{\"100\":1}}],[\"下采样点特征\",{\"1\":{\"98\":1}}],[\"下采样点坐标\",{\"1\":{\"98\":1}}],[\"下的\",{\"1\":{\"82\":1}}],[\"下的表现\",{\"1\":{\"82\":1}}],[\"下面为\",{\"1\":{\"653\":1}}],[\"下面将展示gpt2block模块的实现逻辑\",{\"1\":{\"477\":1}}],[\"下面将给出使用了\",{\"1\":{\"477\":1}}],[\"下面将给出captioner模块基于coco数据集\",{\"1\":{\"142\":1}}],[\"下面我们来看看论文中ltm的例子\",{\"1\":{\"436\":1}}],[\"下面我们来看论文中给的self\",{\"1\":{\"435\":1}}],[\"下面我们来看论文中给的cot的例子\",{\"1\":{\"434\":1}}],[\"下面我们将进入训练流程\",{\"1\":{\"300\":1}}],[\"下面我们将用于图片变换的transforms流水线和上面自定义的mydataset类都封装到dataloader去\",{\"1\":{\"290\":1}}],[\"下面的3是因为我们用一次矩阵运算得到了拼接在一起的q\",{\"1\":{\"295\":1}}],[\"下面所给出的代码实现\",{\"1\":{\"295\":1}}],[\"下面来实际展示一下效果\",{\"1\":{\"276\":1}}],[\"下面首先给出的是\",{\"1\":{\"147\":1}}],[\"下面先给出\",{\"1\":{\"145\":1}}],[\"下面给出的是一个基于pointnet++的点云语义分割模型定义\",{\"1\":{\"101\":1}}],[\"下面详细介绍一下动态卷机核卷积的过程\",{\"1\":{\"76\":1}}],[\"下面只需要把以上的三个步骤按流程组织起来即可得到afm模块的完整实现了\",{\"1\":{\"75\":1}}],[\"下降了\",{\"1\":{\"24\":1}}],[\"下均显著优于现有方法\",{\"1\":{\"23\":1}}],[\"无所不知\",{\"1\":{\"678\":1}}],[\"无所不能\",{\"1\":{\"678\":1}}],[\"无标签数据\",{\"1\":{\"675\":1}}],[\"无缝理解和生成多种形式内容\",{\"1\":{\"674\":1}}],[\"无病\",{\"1\":{\"569\":1}}],[\"无限多种可能的区间组合\",{\"1\":{\"566\":1}}],[\"无限接近于\",{\"1\":{\"238\":1}}],[\"无外部数据\",{\"1\":{\"499\":1}}],[\"无提示时\",{\"1\":{\"471\":1}}],[\"无害\",{\"1\":{\"468\":1}}],[\"无梯度更新的few\",{\"1\":{\"464\":1}}],[\"无任务特定架构\",{\"1\":{\"464\":1}}],[\"无关的常数\",{\"1\":{\"596\":1}}],[\"无关\",{\"1\":{\"355\":1}}],[\"无论计算图结构多复杂\",{\"1\":{\"662\":1}}],[\"无论预处理方式\",{\"1\":{\"454\":1}}],[\"无论到什么程度\",{\"1\":{\"440\":1}}],[\"无论是正类还是负类\",{\"1\":{\"343\":1}}],[\"无论是有监督还是自监督方法\",{\"1\":{\"278\":1}}],[\"无论其有效性如何\",{\"1\":{\"342\":1}}],[\"无论你如何打乱输入元素的顺序\",{\"1\":{\"115\":1}}],[\"无监督任务学习作为预训练技术成功的关键因素\",{\"1\":{\"456\":1}}],[\"无监督任务学习是预训练技术成功的关键因素之一\",{\"1\":{\"453\":1}}],[\"无监督多任务学习的可行性证明\",{\"1\":{\"454\":1}}],[\"无监督预训练+监督微调方式\",{\"1\":{\"441\":1}}],[\"无监督预训练\",{\"0\":{\"443\":1},\"1\":{\"441\":1,\"447\":1}}],[\"无监督学习得到的好的表示也能提供显著的提升\",{\"1\":{\"440\":1}}],[\"无监督学习最大的一个卖点\",{\"1\":{\"238\":1}}],[\"无监督学习的表现往往不如有监督学习\",{\"1\":{\"238\":1}}],[\"无监督学习真的可行\",{\"1\":{\"233\":1}}],[\"无监督就很难去建模\",{\"1\":{\"238\":1}}],[\"无权重衰减\",{\"1\":{\"204\":1}}],[\"无意义内容\",{\"1\":{\"190\":1}}],[\"无意图推理\",{\"1\":{\"24\":1}}],[\"无序性\",{\"1\":{\"114\":1}}],[\"无需重新训练\",{\"1\":{\"681\":1}}],[\"无需\",{\"1\":{\"674\":1}}],[\"无需手动推导导数公式\",{\"1\":{\"662\":1}}],[\"无需计算导数\",{\"1\":{\"658\":1}}],[\"无需等待gc\",{\"1\":{\"657\":1}}],[\"无需额外接口\",{\"1\":{\"662\":1}}],[\"无需额外预处理\",{\"1\":{\"495\":1}}],[\"无需额外架构调整\",{\"1\":{\"460\":1}}],[\"无需复杂结构调整\",{\"1\":{\"500\":1}}],[\"无需复杂架构调整\",{\"1\":{\"483\":1}}],[\"无需复制数据\",{\"1\":{\"495\":1}}],[\"无需参数更新\",{\"1\":{\"461\":1}}],[\"无需任务特定的监督训练\",{\"1\":{\"452\":1}}],[\"无需针对特定数据集进行微调即可生成高保真图像\",{\"1\":{\"177\":1}}],[\"无需人工标注即可通过自注意力机制学习语义区域和物体边界\",{\"1\":{\"166\":1}}],[\"无需预处理\",{\"1\":{\"112\":1}}],[\"无需切分\",{\"1\":{\"31\":1}}],[\"无局部聚合机制\",{\"1\":{\"112\":1}}],[\"无法及时反映最新的信息动态\",{\"1\":{\"679\":1}}],[\"无法回收\",{\"1\":{\"657\":1}}],[\"无法回答时\",{\"1\":{\"228\":1}}],[\"无法\",{\"1\":{\"542\":1}}],[\"无法利用结构化监督信号\",{\"1\":{\"463\":1}}],[\"无法更新参数\",{\"1\":{\"426\":1}}],[\"无法实现zero\",{\"1\":{\"278\":1}}],[\"无法模拟整个数据集\",{\"1\":{\"240\":1}}],[\"无法直接使用\",{\"1\":{\"166\":1}}],[\"无法直接用于每个点\",{\"1\":{\"111\":1}}],[\"无法充分利用\",{\"1\":{\"112\":1}}],[\"无法区分顺序信息\",{\"1\":{\"112\":1}}],[\"无法有效利用局部结构\",{\"1\":{\"112\":1}}],[\"无法捕捉边缘\",{\"1\":{\"112\":1}}],[\"无法应对弯曲\",{\"1\":{\"112\":1}}],[\"无法像离散情形那样枚举所有子集\",{\"1\":{\"566\":1}}],[\"无法像\",{\"1\":{\"112\":1}}],[\"无法处理不在原文中的答案\",{\"1\":{\"542\":1}}],[\"无法处理非刚性变形\",{\"1\":{\"112\":1}}],[\"无法处理非刚性形变\",{\"1\":{\"112\":2}}],[\"无法处理开放词汇场景\",{\"1\":{\"7\":1}}],[\"无效\",{\"1\":{\"92\":1}}],[\"无相机参数方法\",{\"1\":{\"52\":1}}],[\"无\",{\"1\":{\"24\":1,\"196\":1,\"477\":1}}],[\"无跨模态融合\",{\"1\":{\"24\":1}}],[\"无几何推理\",{\"1\":{\"24\":1}}],[\"✗\",{\"1\":{\"24\":4}}],[\"表9\",{\"1\":{\"482\":1}}],[\"表5\",{\"1\":{\"482\":1}}],[\"表7\",{\"1\":{\"480\":1,\"482\":1}}],[\"表8\",{\"1\":{\"480\":1,\"482\":1}}],[\"表征学习\",{\"0\":{\"282\":1},\"1\":{\"282\":1}}],[\"表征会很细\",{\"1\":{\"235\":1}}],[\"表征不一致\",{\"1\":{\"181\":1}}],[\"表征\",{\"1\":{\"163\":1,\"240\":1}}],[\"表征作为跨模态图文对表示\",{\"1\":{\"162\":1}}],[\"表征作为图文对的联合表示\",{\"1\":{\"156\":1}}],[\"表征映射到归一化的低维\",{\"1\":{\"154\":1}}],[\"表\",{\"1\":{\"137\":1,\"190\":4,\"470\":1,\"492\":1}}],[\"表4\",{\"1\":{\"134\":1,\"480\":1,\"482\":1}}],[\"表23列出了三种配置的超参数\",{\"1\":{\"205\":1}}],[\"表2\",{\"1\":{\"133\":1}}],[\"表14\",{\"1\":{\"484\":1}}],[\"表16\",{\"1\":{\"483\":1}}],[\"表13\",{\"1\":{\"482\":1,\"484\":1}}],[\"表13展示gpt\",{\"1\":{\"455\":1}}],[\"表10\",{\"1\":{\"482\":1,\"483\":1}}],[\"表11\",{\"1\":{\"480\":1,\"482\":1,\"484\":1}}],[\"表12\",{\"1\":{\"480\":1,\"482\":1,\"484\":1}}],[\"表1\",{\"1\":{\"132\":1,\"189\":1}}],[\"表达能力增长方式\",{\"1\":{\"395\":1}}],[\"表达能力受限于\",{\"1\":{\"112\":1,\"231\":1}}],[\"表达不同维度之间的\",{\"1\":{\"355\":1}}],[\"表达式语言\",{\"1\":{\"684\":1,\"685\":1}}],[\"表达式\",{\"1\":{\"76\":1,\"390\":1}}],[\"表现优于gpt\",{\"1\":{\"484\":1}}],[\"表现接近palm\",{\"1\":{\"482\":1}}],[\"表现出诸如捏造事实\",{\"1\":{\"468\":1}}],[\"表现不稳定的问题\",{\"1\":{\"464\":1}}],[\"表现不错\",{\"1\":{\"112\":1}}],[\"表现仍然较弱\",{\"1\":{\"463\":1}}],[\"表现远不如fine\",{\"1\":{\"462\":1}}],[\"表现则不及微调模型\",{\"1\":{\"462\":1}}],[\"表现为\",{\"1\":{\"453\":1}}],[\"表现类似\",{\"1\":{\"401\":1}}],[\"表现\",{\"1\":{\"138\":1,\"194\":1,\"440\":1}}],[\"表现良好\",{\"1\":{\"112\":1}}],[\"表明训练目标的改变\",{\"1\":{\"468\":1}}],[\"表明缩放定律\",{\"1\":{\"460\":1}}],[\"表明模型容量和训练数据规模仍需进一步扩大\",{\"1\":{\"456\":1}}],[\"表明模型容量是限制因素\",{\"1\":{\"455\":1}}],[\"表明进一步扩大模型和数据规模可能带来额外提升\",{\"1\":{\"455\":1}}],[\"表明进一步扩大数据或模型可能提升性能\",{\"1\":{\"454\":1}}],[\"表明在迁移中\",{\"1\":{\"449\":1}}],[\"表明\",{\"1\":{\"112\":1,\"455\":1}}],[\"表明交互意图推理对泛化至新\",{\"1\":{\"24\":1}}],[\"表面纹理等\",{\"1\":{\"112\":1}}],[\"表示我们省略了分母\",{\"1\":{\"596\":1}}],[\"表示变量之间不相关\",{\"1\":{\"590\":1}}],[\"表示变量之间的相关性\",{\"1\":{\"355\":1}}],[\"表示分布的方差\",{\"1\":{\"584\":1}}],[\"表示分布的均值\",{\"1\":{\"584\":1}}],[\"表示成功的次数\",{\"1\":{\"579\":1}}],[\"表示成一个\",{\"1\":{\"576\":1}}],[\"表示在假设\",{\"1\":{\"596\":1}}],[\"表示在我们看到任何数据之前\",{\"1\":{\"596\":1}}],[\"表示在这过程中抽到的\",{\"1\":{\"579\":1}}],[\"表示在行方向移动时\",{\"1\":{\"327\":1}}],[\"表示这\",{\"1\":{\"578\":1}}],[\"表示类别\",{\"1\":{\"576\":1}}],[\"表示当前样本属于第\",{\"1\":{\"576\":1}}],[\"表示当\",{\"1\":{\"576\":1}}],[\"表示选择类别\",{\"1\":{\"576\":1}}],[\"表示事件\",{\"1\":{\"567\":1}}],[\"表示正面\",{\"1\":{\"565\":1}}],[\"表示正确对随机正例和负例进行排名的概率为\",{\"1\":{\"351\":1}}],[\"表示骰子掷出面为\",{\"1\":{\"565\":2}}],[\"表示所有方向的方差相同\",{\"1\":{\"590\":2}}],[\"表示所有可能的实验结果\",{\"1\":{\"565\":1}}],[\"表示所有图像token有效\",{\"1\":{\"286\":1}}],[\"表示实验中所有可能的结果组成的集合\",{\"1\":{\"564\":1}}],[\"表示将张量中的值限制在\",{\"1\":{\"541\":1}}],[\"表示是否是连续句子\",{\"1\":{\"513\":1}}],[\"表示单词结束\",{\"1\":{\"410\":2}}],[\"表示对\",{\"1\":{\"404\":1}}],[\"表示先占位\",{\"1\":{\"389\":1}}],[\"表示保持原来的大小\",{\"1\":{\"387\":1}}],[\"表示不同变量之间的线性相关性\",{\"1\":{\"355\":1}}],[\"表示不进行归一化\",{\"1\":{\"291\":2}}],[\"表示阈值\",{\"1\":{\"353\":1}}],[\"表示两行两列\",{\"1\":{\"325\":1}}],[\"表示新视图从原始内存块中的哪个位置开始\",{\"1\":{\"325\":1}}],[\"表示模型更\",{\"1\":{\"471\":1}}],[\"表示模型的规模\",{\"1\":{\"300\":1}}],[\"表示模型预测的被\",{\"1\":{\"155\":1}}],[\"表示输入结束\",{\"1\":{\"227\":1}}],[\"表示无\",{\"1\":{\"161\":1}}],[\"表示被\",{\"1\":{\"155\":1}}],[\"表示图文对的真实匹配状态\",{\"1\":{\"156\":1}}],[\"表示图文对的多模态表示\",{\"1\":{\"126\":1}}],[\"表示图像特征没有被\",{\"1\":{\"143\":1}}],[\"表示结束\",{\"1\":{\"126\":1}}],[\"表示形式\",{\"1\":{\"114\":1}}],[\"表示只在通道维度操作\",{\"1\":{\"107\":1}}],[\"表示最原始的点云\",{\"1\":{\"101\":1}}],[\"表示每一层mlp的输出通道数\",{\"1\":{\"100\":1}}],[\"表示每个图像块的大小是\",{\"1\":{\"290\":1}}],[\"表示每个\",{\"1\":{\"76\":1}}],[\"表示每个点是否属于目标功能区域\",{\"1\":{\"404\":1}}],[\"表示每个点是否具有可操作性\",{\"1\":{\"46\":1}}],[\"表示每个点是否具有特定可操作性的概率\",{\"1\":{\"40\":1}}],[\"表示每个点属于功能区域的概率\",{\"1\":{\"404\":1}}],[\"表示每个点属于目标功能区域的概率\",{\"1\":{\"76\":1}}],[\"表示其属于目标功能区域的概率\",{\"1\":{\"76\":1}}],[\"表示该维度上复制的次数\",{\"1\":{\"386\":1}}],[\"表示该模型优于左侧曲线对应的模型\",{\"1\":{\"353\":1}}],[\"表示该层点数\",{\"1\":{\"72\":1}}],[\"表示该点是否具有可操作性\",{\"1\":{\"46\":1}}],[\"表示哪些点属于目标功能区域\",{\"1\":{\"70\":1}}],[\"表示点属于该功能区域的概率\",{\"1\":{\"64\":1}}],[\"表示概率\",{\"1\":{\"46\":1}}],[\"表示填充\",{\"1\":{\"43\":1}}],[\"表示有效\",{\"1\":{\"43\":1}}],[\"表示\",{\"1\":{\"41\":2,\"76\":1,\"92\":1,\"107\":1,\"109\":1,\"157\":1,\"163\":1,\"285\":1,\"323\":1,\"351\":1,\"513\":1,\"520\":1,\"565\":1}}],[\"表示池化后扩展为\",{\"1\":{\"14\":1}}],[\"表3\",{\"1\":{\"24\":1,\"134\":1,\"480\":1,\"482\":1}}],[\"04\",{\"1\":{\"662\":2}}],[\"04744\",{\"1\":{\"37\":1}}],[\"0时\",{\"1\":{\"660\":1}}],[\"0+cu113\",{\"1\":{\"546\":1}}],[\"0和sst\",{\"1\":{\"497\":1}}],[\"0分\",{\"1\":{\"484\":1}}],[\"0或1\",{\"1\":{\"404\":1}}],[\"087\",{\"1\":{\"484\":1}}],[\"081\",{\"1\":{\"482\":1,\"484\":1}}],[\"08\",{\"1\":{\"229\":1,\"397\":1}}],[\"08485\",{\"1\":{\"223\":1}}],[\"0到1\",{\"1\":{\"210\":1}}],[\"01\",{\"1\":{\"193\":1,\"514\":1}}],[\"012个点云和5\",{\"1\":{\"49\":1}}],[\"02\",{\"1\":{\"292\":1,\"293\":2,\"296\":2}}],[\"02等\",{\"1\":{\"183\":1}}],[\"02413\",{\"1\":{\"85\":1}}],[\"000+基词\",{\"1\":{\"454\":1}}],[\"000\",{\"1\":{\"226\":1,\"227\":4,\"447\":1,\"493\":1}}],[\"001\",{\"1\":{\"160\":1,\"667\":3}}],[\"00593\",{\"1\":{\"102\":1}}],[\"07\",{\"1\":{\"145\":1,\"147\":1,\"160\":1,\"246\":2}}],[\"093\",{\"1\":{\"75\":1,\"82\":1}}],[\"0~100\",{\"1\":{\"359\":1}}],[\"0~1\",{\"1\":{\"46\":1}}],[\"03b\",{\"1\":{\"190\":1}}],[\"03\",{\"1\":{\"24\":1,\"519\":1}}],[\"0\",{\"0\":{\"179\":1,\"223\":1,\"289\":1},\"1\":{\"23\":2,\"29\":46,\"30\":1,\"32\":2,\"34\":1,\"35\":14,\"36\":2,\"41\":3,\"43\":4,\"45\":2,\"46\":12,\"58\":25,\"59\":22,\"64\":1,\"68\":4,\"70\":11,\"75\":4,\"76\":2,\"78\":4,\"80\":2,\"81\":2,\"82\":17,\"83\":9,\"92\":12,\"93\":2,\"96\":14,\"98\":1,\"100\":6,\"101\":2,\"107\":8,\"108\":1,\"109\":1,\"111\":1,\"127\":1,\"131\":1,\"142\":3,\"143\":3,\"145\":61,\"146\":4,\"147\":14,\"154\":1,\"157\":1,\"159\":4,\"160\":4,\"161\":5,\"162\":6,\"163\":6,\"179\":1,\"194\":1,\"200\":2,\"201\":1,\"203\":2,\"204\":1,\"223\":1,\"226\":8,\"228\":1,\"229\":1,\"240\":3,\"241\":1,\"244\":14,\"246\":4,\"247\":1,\"248\":1,\"249\":3,\"262\":1,\"266\":1,\"268\":2,\"273\":1,\"275\":1,\"277\":1,\"282\":2,\"283\":2,\"284\":5,\"285\":9,\"286\":3,\"289\":2,\"290\":23,\"291\":3,\"292\":3,\"293\":3,\"294\":2,\"295\":3,\"296\":6,\"321\":1,\"322\":2,\"323\":5,\"326\":1,\"327\":5,\"343\":1,\"344\":1,\"345\":3,\"346\":1,\"348\":2,\"350\":4,\"351\":6,\"353\":1,\"355\":4,\"374\":2,\"381\":1,\"383\":1,\"384\":1,\"387\":1,\"397\":14,\"401\":2,\"402\":1,\"403\":6,\"404\":6,\"405\":4,\"407\":10,\"410\":4,\"411\":3,\"412\":5,\"425\":3,\"426\":10,\"447\":6,\"448\":1,\"455\":1,\"462\":1,\"470\":1,\"474\":7,\"477\":6,\"481\":1,\"482\":3,\"484\":2,\"492\":1,\"494\":1,\"498\":2,\"499\":2,\"506\":9,\"510\":2,\"511\":1,\"512\":2,\"513\":1,\"514\":3,\"515\":2,\"519\":1,\"520\":29,\"523\":1,\"525\":1,\"527\":1,\"528\":2,\"531\":2,\"535\":1,\"541\":3,\"542\":1,\"543\":1,\"546\":1,\"553\":1,\"556\":1,\"558\":3,\"566\":2,\"569\":2,\"576\":2,\"585\":1,\"587\":1,\"590\":2,\"608\":4,\"617\":1,\"632\":1,\"645\":4,\"651\":1,\"654\":1,\"655\":1,\"656\":3,\"658\":6,\"659\":3,\"660\":15,\"662\":18,\"666\":4,\"667\":43,\"674\":15}}],[\"05\",{\"1\":{\"23\":1,\"131\":1,\"201\":1,\"228\":1,\"569\":1}}],[\"约40人\",{\"1\":{\"472\":1}}],[\"约4000多个样本\",{\"1\":{\"289\":1}}],[\"约束\",{\"1\":{\"469\":1}}],[\"约33\",{\"1\":{\"462\":1}}],[\"约95\",{\"1\":{\"455\":1}}],[\"约\",{\"1\":{\"23\":2,\"470\":1}}],[\"8个头\",{\"1\":{\"558\":1}}],[\"88\",{\"1\":{\"498\":1}}],[\"889\",{\"1\":{\"17\":1}}],[\"84\",{\"1\":{\"497\":2}}],[\"8×32gb\",{\"1\":{\"494\":1}}],[\"8回退到字节级处理\",{\"1\":{\"481\":1}}],[\"8的语言建模基准上达到sota水平\",{\"1\":{\"457\":1}}],[\"8的数据集上刷新了零样本sota\",{\"1\":{\"455\":1}}],[\"8降至8\",{\"1\":{\"455\":1}}],[\"8以后支持\",{\"1\":{\"389\":1}}],[\"8b\",{\"1\":{\"191\":1,\"674\":5}}],[\"8bit=true\",{\"1\":{\"28\":1}}],[\"8600000\",{\"1\":{\"297\":1}}],[\"86×1000000\",{\"1\":{\"297\":1}}],[\"86m\",{\"1\":{\"297\":1}}],[\"86\",{\"1\":{\"140\":1}}],[\"896×1344\",{\"1\":{\"216\":1}}],[\"89\",{\"1\":{\"112\":1,\"455\":1,\"499\":2}}],[\"85\",{\"1\":{\"112\":1,\"194\":1,\"455\":1,\"462\":1,\"471\":1,\"569\":1}}],[\"87\",{\"1\":{\"75\":1,\"82\":1}}],[\"870\",{\"1\":{\"63\":2,\"65\":1,\"67\":1}}],[\"82\",{\"1\":{\"75\":1,\"193\":1,\"455\":1}}],[\"8064\",{\"1\":{\"662\":1}}],[\"80gb\",{\"1\":{\"481\":1}}],[\"80层\",{\"1\":{\"481\":1}}],[\"80k\",{\"1\":{\"201\":1}}],[\"800\",{\"1\":{\"174\":1}}],[\"80\",{\"1\":{\"59\":3,\"163\":1,\"493\":1,\"505\":1,\"511\":2}}],[\"8\",{\"0\":{\"44\":1,\"69\":1},\"1\":{\"26\":1,\"34\":3,\"35\":6,\"40\":1,\"46\":2,\"59\":4,\"75\":1,\"80\":1,\"82\":2,\"96\":1,\"100\":1,\"101\":1,\"134\":1,\"161\":1,\"163\":1,\"188\":1,\"207\":1,\"321\":2,\"322\":2,\"323\":1,\"325\":2,\"331\":1,\"397\":2,\"404\":1,\"410\":5,\"411\":1,\"412\":7,\"427\":1,\"428\":2,\"448\":1,\"449\":1,\"462\":4,\"471\":1,\"472\":1,\"474\":1,\"482\":5,\"498\":1,\"499\":2,\"510\":3,\"511\":1,\"558\":1,\"590\":1,\"597\":1,\"667\":1,\"674\":1}}],[\"8192\",{\"1\":{\"242\":1}}],[\"81\",{\"1\":{\"23\":1,\"482\":1,\"499\":1}}],[\"依次通过每个编码器层\",{\"1\":{\"513\":1}}],[\"依此类推\",{\"1\":{\"322\":2}}],[\"依赖高质量的训练数\",{\"1\":{\"681\":1}}],[\"依赖于构建高质量的数据集\",{\"1\":{\"681\":1}}],[\"依赖于输入x的具体值\",{\"1\":{\"630\":1}}],[\"依赖任务特定的架构\",{\"1\":{\"460\":1}}],[\"依赖闭包机制来记住原函数的引用\",{\"1\":{\"367\":1}}],[\"依赖\",{\"1\":{\"338\":2}}],[\"依赖浅层相似度计算\",{\"1\":{\"188\":1}}],[\"依赖视角选择\",{\"1\":{\"114\":1}}],[\"依赖初始点和距离度量方式的选择\",{\"1\":{\"89\":1}}],[\"依赖预定义类别\",{\"1\":{\"6\":1}}],[\"依旧表现出色\",{\"1\":{\"23\":1}}],[\"在完成前后端搭建之后\",{\"1\":{\"687\":1}}],[\"在完成上一步的初始化\",{\"1\":{\"687\":1}}],[\"在该步中\",{\"1\":{\"687\":1}}],[\"在该步骤中\",{\"1\":{\"687\":1}}],[\"在该阶段\",{\"1\":{\"191\":2}}],[\"在确定开发目标后\",{\"1\":{\"687\":1}}],[\"在验证集上最终验证模型效果来实现性能的评估\",{\"1\":{\"686\":1}}],[\"在测试集上调优模型\",{\"1\":{\"686\":1}}],[\"在评估思路上\",{\"1\":{\"686\":1}}],[\"在评估模型和选择阈值时\",{\"1\":{\"347\":1}}],[\"在开发过程中\",{\"1\":{\"683\":1}}],[\"在开源模型中领先\",{\"1\":{\"220\":1}}],[\"在下图中\",{\"1\":{\"682\":1}}],[\"在提升大语言模型效果中\",{\"1\":{\"681\":1}}],[\"在理解和生成长篇内容时受限于有限的上下文窗口\",{\"1\":{\"679\":1}}],[\"在信息检索领域\",{\"1\":{\"678\":1}}],[\"在处理特定领域的专业知识时\",{\"1\":{\"679\":1}}],[\"在处理各种任务时表现出色\",{\"1\":{\"676\":1}}],[\"在处理文本时具有强大的上下文感知能力\",{\"1\":{\"675\":1}}],[\"在处理浮点型的\",{\"1\":{\"397\":1}}],[\"在各种基准测试中均优于\",{\"1\":{\"674\":1}}],[\"在各种任务中的表现均显著提升\",{\"1\":{\"673\":1}}],[\"在遵循复杂指令\",{\"1\":{\"674\":1}}],[\"在国内率先开启邀测\",{\"1\":{\"674\":1}}],[\"在性能和效率上有显著提升\",{\"1\":{\"674\":1}}],[\"在回答前会先生成一段思维链\",{\"1\":{\"674\":1}}],[\"在回答简单问题时冗长解释或\",{\"1\":{\"471\":1}}],[\"在知识广度\",{\"1\":{\"674\":1}}],[\"在语音互动中传递更丰富的情感变化\",{\"1\":{\"674\":1}}],[\"在语义分割任务中\",{\"1\":{\"399\":1}}],[\"在解决复杂任务和评估任务上展现出较大的性能提升\",{\"1\":{\"674\":1}}],[\"在解码器中\",{\"1\":{\"548\":1}}],[\"在他的经典论文\",{\"1\":{\"673\":1}}],[\"在点上\",{\"1\":{\"667\":1}}],[\"在点云中逐步选择离已选点尽可能远的点\",{\"1\":{\"92\":1}}],[\"在第四阶段\",{\"1\":{\"670\":1}}],[\"在第三阶段\",{\"1\":{\"665\":1}}],[\"在第一阶段\",{\"1\":{\"650\":1}}],[\"在接收输入时自动将参数转换为variable\",{\"1\":{\"660\":1}}],[\"在表达式a\",{\"1\":{\"660\":3}}],[\"在tinypytorch中\",{\"1\":{\"657\":2}}],[\"在transformer\",{\"1\":{\"296\":1}}],[\"在transformer中\",{\"1\":{\"293\":1}}],[\"在transformer模型中\",{\"1\":{\"287\":1}}],[\"在步骤14中\",{\"1\":{\"655\":1}}],[\"在反向传播过程中\",{\"1\":{\"654\":1}}],[\"在调用如\",{\"1\":{\"651\":1}}],[\"在variable的backward方法中自动初始化梯度\",{\"1\":{\"642\":1}}],[\"在variable类中添加name属性\",{\"1\":{\"659\":1}}],[\"在variable类中添加backward方法\",{\"1\":{\"635\":1}}],[\"在variable类中添加creator属性\",{\"1\":{\"634\":1}}],[\"在function类的\",{\"1\":{\"634\":1}}],[\"在few\",{\"1\":{\"461\":1}}],[\"在编程中\",{\"1\":{\"606\":1}}],[\"在已知世界状态\",{\"1\":{\"597\":1}}],[\"在公式\",{\"1\":{\"596\":1}}],[\"在公开\",{\"1\":{\"471\":1}}],[\"在高维图像空间中\",{\"1\":{\"594\":1}}],[\"在高维空间\",{\"1\":{\"395\":1}}],[\"在高维空间中\",{\"1\":{\"359\":1,\"591\":2}}],[\"在二维空间中\",{\"1\":{\"590\":1}}],[\"在本节中\",{\"1\":{\"583\":1,\"589\":1}}],[\"在本文中\",{\"1\":{\"125\":1,\"440\":1}}],[\"在总共\",{\"1\":{\"576\":1}}],[\"在构建各种类型的模型时\",{\"1\":{\"573\":1}}],[\"在给定均值和协方差矩的约束下\",{\"1\":{\"588\":1}}],[\"在给定\",{\"1\":{\"568\":1}}],[\"在事件\",{\"1\":{\"568\":1}}],[\"在事实型问答测试中\",{\"1\":{\"455\":1}}],[\"在有限的情况下\",{\"1\":{\"567\":1}}],[\"在持续时间的例子中\",{\"1\":{\"566\":1}}],[\"在输入序列长度不一致时\",{\"1\":{\"548\":1}}],[\"在输入中添加可学习的前缀\",{\"1\":{\"231\":1}}],[\"在上下文明确的情况下\",{\"1\":{\"565\":1}}],[\"在上下文中找到最可能的答案起始位置和结束位置\",{\"1\":{\"542\":1}}],[\"在上面的推理过程中\",{\"1\":{\"474\":1}}],[\"在上面的例子中\",{\"1\":{\"435\":1}}],[\"在上面的结构图中可以看到\",{\"1\":{\"292\":1}}],[\"在问答任务中一般不会使用这个输出\",{\"1\":{\"540\":1}}],[\"在问答任务中\",{\"1\":{\"540\":1}}],[\"在问答上提升5\",{\"1\":{\"439\":1}}],[\"在返回前进行预处理\",{\"1\":{\"522\":1}}],[\"在注意力得分矩阵计算完毕后\",{\"1\":{\"517\":1}}],[\"在注意力机制\",{\"1\":{\"304\":1}}],[\"在glue\",{\"1\":{\"495\":1}}],[\"在gpt\",{\"1\":{\"464\":1}}],[\"在bert和roberta的预训练中\",{\"1\":{\"495\":1}}],[\"在boolq\",{\"1\":{\"462\":1}}],[\"在配备\",{\"1\":{\"494\":1}}],[\"在固定计算预算下\",{\"1\":{\"480\":1}}],[\"在首轮推理的过程中\",{\"1\":{\"477\":1}}],[\"在面对带错误前提的指令时\",{\"1\":{\"471\":1}}],[\"在偏见测试中未表现出优势\",{\"1\":{\"471\":1}}],[\"在加入指导性提示\",{\"1\":{\"471\":1}}],[\"在所有规模下均优于\",{\"1\":{\"471\":1}}],[\"在所有划分\",{\"1\":{\"23\":1}}],[\"在两句话之间和句末加\",{\"1\":{\"506\":1}}],[\"在两类提示分布\",{\"1\":{\"471\":1}}],[\"在两个线性层之间通常会插入一个非线性激活函数\",{\"1\":{\"294\":1}}],[\"在两个视频\",{\"1\":{\"120\":1}}],[\"在用户任务分布中\",{\"1\":{\"471\":1}}],[\"在用户偏好评估中仍表现更优\",{\"1\":{\"467\":1}}],[\"在三个参数规模\",{\"1\":{\"470\":1}}],[\"在人类偏好标注中\",{\"1\":{\"470\":1}}],[\"在模拟环境中用人类反馈改进行为策略\",{\"1\":{\"469\":1}}],[\"在模型训练和推理过程中\",{\"1\":{\"428\":1}}],[\"在模型训练时可能会出现问题\",{\"1\":{\"241\":1}}],[\"在模型架构中\",{\"1\":{\"299\":1}}],[\"在模型初始化时\",{\"1\":{\"293\":1}}],[\"在模型规模\",{\"1\":{\"188\":1}}],[\"在模型设计上提出了一个灵活\",{\"1\":{\"129\":1}}],[\"在使用\",{\"1\":{\"540\":1}}],[\"在使用gpt\",{\"1\":{\"463\":1}}],[\"在使用cot这种prompt\",{\"1\":{\"434\":1}}],[\"在使用clip模型进行zero\",{\"1\":{\"274\":1}}],[\"在特定任务上\",{\"1\":{\"463\":1}}],[\"在特定任务相关的数据集上执行有监督全量参数微调\",{\"1\":{\"423\":1}}],[\"在某些问题中\",{\"1\":{\"585\":1}}],[\"在某些任务中可与sota模型媲美\",{\"1\":{\"463\":1}}],[\"在某些标准数据集\",{\"1\":{\"112\":1}}],[\"在zero\",{\"1\":{\"462\":1}}],[\"在合成任务和灵活性测试中展现强大泛化能力\",{\"1\":{\"462\":1}}],[\"在阅读理解任务中\",{\"1\":{\"462\":1}}],[\"在没有使用外部检索信息\",{\"1\":{\"462\":1}}],[\"在封闭式问答任务中接近甚至超越sota\",{\"1\":{\"462\":1}}],[\"在lambada数据集上\",{\"1\":{\"462\":1}}],[\"在任务特定数据集上更新模型权重\",{\"1\":{\"461\":1}}],[\"在少样本设置下\",{\"1\":{\"460\":1}}],[\"在one\",{\"1\":{\"455\":1}}],[\"在webtext验证集上仍未完全收敛\",{\"1\":{\"455\":1}}],[\"在nli和qqp任务上辅助lm目标有帮助\",{\"1\":{\"449\":1}}],[\"在nlp中\",{\"1\":{\"440\":1}}],[\"在nlp领域\",{\"1\":{\"278\":1}}],[\"在文章末尾添加\",{\"1\":{\"454\":1}}],[\"在文档和问题给定条件下\",{\"1\":{\"449\":1}}],[\"在文本前添加\",{\"1\":{\"126\":1}}],[\"在阈值下进行预测的\",{\"1\":{\"449\":1}}],[\"在零样本上\",{\"1\":{\"449\":1}}],[\"在零样本条件下实现了与领域专用模型相媲美的性能\",{\"1\":{\"177\":1}}],[\"在应用到很多任务时可以提高语言建模的能力并且transformer更具结构化的注意力记忆\",{\"1\":{\"449\":1}}],[\"在superglue基准测试中\",{\"1\":{\"462\":1}}],[\"在sts\",{\"1\":{\"448\":1}}],[\"在storycloze和hellaswag等故事完形任务中\",{\"1\":{\"462\":1}}],[\"在story\",{\"1\":{\"448\":1}}],[\"在seq\",{\"1\":{\"284\":1}}],[\"在句子级别打乱顺序以破坏长距离结构信息\",{\"1\":{\"447\":1}}],[\"在作者的实验中\",{\"1\":{\"443\":1}}],[\"在迁移阶段\",{\"1\":{\"440\":1}}],[\"在迁移到其他数据集时也需要加上新的分类器进行有监督训练\",{\"1\":{\"278\":1}}],[\"在迁移到下游任务时\",{\"1\":{\"278\":1}}],[\"在12个的9个数据集取得了sota结果\",{\"1\":{\"450\":1}}],[\"在12个数据集上的9个取得sota结果\",{\"1\":{\"448\":1}}],[\"在12个研究任务中9个提升到sota\",{\"1\":{\"439\":1,\"440\":1}}],[\"在18个多模态基准测试中展现出媲美商业模型的性能\",{\"1\":{\"222\":1}}],[\"在前一阶段\",{\"1\":{\"665\":1}}],[\"在前向传播中\",{\"1\":{\"544\":1}}],[\"在前向过程中\",{\"1\":{\"425\":1}}],[\"在前面我们介绍了\",{\"1\":{\"426\":1}}],[\"在前景像素远少于背景像素时表现良好\",{\"1\":{\"407\":1}}],[\"在原始预训练语言模型\",{\"1\":{\"425\":1}}],[\"在原始点数量下的每个点都拥有一个合理的特征向量\",{\"1\":{\"100\":1}}],[\"在微调完成后\",{\"1\":{\"423\":1}}],[\"在微调过程中\",{\"1\":{\"423\":1}}],[\"在准备好的数据集上\",{\"1\":{\"423\":1}}],[\"在推理大模型方面\",{\"1\":{\"674\":1}}],[\"在推理方面就十分出色\",{\"1\":{\"674\":1}}],[\"在推理能力\",{\"1\":{\"674\":1}}],[\"在推理型模型中可选择性展示思考过程\",{\"1\":{\"674\":1}}],[\"在推理阶段\",{\"1\":{\"658\":1}}],[\"在推理阶段用文本输入指定任务\",{\"1\":{\"464\":1}}],[\"在推理时为模型提供10\",{\"1\":{\"461\":1}}],[\"在推理过程中\",{\"1\":{\"425\":1}}],[\"在推理的过程中直接将∆w加到w上去\",{\"1\":{\"420\":1}}],[\"在推动通用人工智能\",{\"1\":{\"208\":1}}],[\"在python开发中\",{\"1\":{\"661\":1}}],[\"在python的运算符重载中\",{\"1\":{\"660\":1}}],[\"在python中\",{\"1\":{\"660\":1}}],[\"在prompt中加入的示例不是1条\",{\"1\":{\"434\":1}}],[\"在prompt中加入一些示例\",{\"1\":{\"434\":1}}],[\"在prompt中加入一些例子\",{\"1\":{\"433\":1}}],[\"在prompt上下文中添加适当的条件\",{\"1\":{\"419\":1}}],[\"在pointnet中\",{\"1\":{\"86\":1}}],[\"在x前面加上了一些特定的内容\",{\"1\":{\"418\":1}}],[\"在具体执行特定任务的时候按需调用\",{\"1\":{\"418\":1}}],[\"在类别平衡时效果好\",{\"1\":{\"402\":1}}],[\"在目标检测任务中\",{\"1\":{\"396\":1}}],[\"在目标物体区域掩码之上\",{\"1\":{\"59\":1}}],[\"在思想上都体现了用更多自由度提升精度\",{\"1\":{\"395\":1}}],[\"在考虑数据分布形状后\",{\"1\":{\"359\":1}}],[\"在深度学习框架中\",{\"1\":{\"657\":1,\"660\":1}}],[\"在深度学习与生成模型中\",{\"1\":{\"355\":1}}],[\"在深度学习领域\",{\"1\":{\"297\":1}}],[\"在垃圾邮件分类器示例中\",{\"1\":{\"351\":1}}],[\"在垃圾邮件分类示例中\",{\"1\":{\"343\":1,\"344\":1,\"345\":1,\"346\":1,\"347\":1}}],[\"在激活的\",{\"1\":{\"338\":1}}],[\"在当前环境下安装包\",{\"0\":{\"338\":1}}],[\"在逻辑上表现为每行都是\",{\"1\":{\"327\":1}}],[\"在逻辑转置中\",{\"1\":{\"326\":1}}],[\"在数学\",{\"1\":{\"674\":2}}],[\"在数学上\",{\"1\":{\"344\":1,\"346\":1,\"348\":1}}],[\"在数据预处理阶段\",{\"1\":{\"495\":1}}],[\"在数据方面\",{\"1\":{\"129\":1}}],[\"在数组运算中的底层实现原理\",{\"1\":{\"327\":1}}],[\"在一些不够大的llm上\",{\"1\":{\"434\":1}}],[\"在一些数据集上的表现比不过c类方法\",{\"1\":{\"259\":1}}],[\"在一个简单的二维情况\",{\"1\":{\"326\":1}}],[\"在内存中是行优先存储\",{\"1\":{\"325\":1}}],[\"在列优先顺序\",{\"1\":{\"322\":1}}],[\"在行优先顺序\",{\"1\":{\"322\":1}}],[\"在映射机制里\",{\"1\":{\"321\":1}}],[\"在机器学习\",{\"1\":{\"321\":1}}],[\"在标准的\",{\"1\":{\"305\":1}}],[\"在如此大规模的数据集上进行预训练\",{\"1\":{\"300\":1}}],[\"在每一步中\",{\"1\":{\"682\":1}}],[\"在每一层\",{\"1\":{\"292\":1}}],[\"在每个\",{\"1\":{\"470\":1}}],[\"在每个小格子里撒几个点\",{\"1\":{\"397\":1}}],[\"在每个层上\",{\"1\":{\"88\":1}}],[\"在代码\",{\"1\":{\"471\":1,\"482\":1}}],[\"在代码中\",{\"1\":{\"291\":1}}],[\"在代理任务上做文章\",{\"1\":{\"240\":1}}],[\"在柱状图上添加数值标签\",{\"1\":{\"289\":1}}],[\"在自注意力机制之后\",{\"1\":{\"548\":1}}],[\"在自然语言处理领域\",{\"1\":{\"678\":1}}],[\"在自然语言处理领域取得了突破性进展\",{\"1\":{\"184\":1}}],[\"在自然语言推理任务\",{\"1\":{\"462\":1}}],[\"在自己的数据上继续训练\",{\"1\":{\"424\":1}}],[\"在自回归生成时\",{\"1\":{\"285\":1}}],[\"在图文检索中\",{\"1\":{\"275\":1}}],[\"在图像识别中加入\",{\"1\":{\"231\":1}}],[\"在图像分类任务中\",{\"1\":{\"287\":1,\"299\":1}}],[\"在图像分类\",{\"1\":{\"197\":1}}],[\"在图像分类和语义分割任务上优于从零训练的模型和其他自监督方法\",{\"1\":{\"166\":1}}],[\"在imagenet数据集上可以提升1\",{\"1\":{\"274\":1}}],[\"在imagenet分类\",{\"1\":{\"181\":1}}],[\"在之前的问题上表现明显变差\",{\"1\":{\"424\":1}}],[\"在之前的例子中\",{\"1\":{\"274\":1}}],[\"在之前有很多优秀的对比学习工作\",{\"1\":{\"238\":1}}],[\"在实现中应当如下处理\",{\"1\":{\"654\":1}}],[\"在实现时可采用自然语言处理\",{\"1\":{\"272\":1}}],[\"在实现时\",{\"1\":{\"90\":1}}],[\"在实验部分也采用了这些基准\",{\"1\":{\"469\":1}}],[\"在实验中\",{\"1\":{\"408\":1}}],[\"在实践中通过梯度下降自动学习逼近策略\",{\"1\":{\"395\":1}}],[\"在实践中\",{\"1\":{\"350\":1}}],[\"在实际部署中可能导致意外错误\",{\"1\":{\"463\":1}}],[\"在实际的模型中\",{\"1\":{\"414\":1}}],[\"在实际训练中更稳定\",{\"1\":{\"403\":1}}],[\"在实际正例数量非常少\",{\"1\":{\"346\":1}}],[\"在实际正例数量非常少的不平衡数据集中\",{\"1\":{\"344\":1}}],[\"在实际负例数量非常少\",{\"1\":{\"345\":1}}],[\"在实际应用中可以选用常见的卷积神经网络\",{\"1\":{\"272\":1}}],[\"在大模型开发中\",{\"1\":{\"686\":1}}],[\"在大部分任务中基本上\",{\"1\":{\"447\":1}}],[\"在大多数情况下\",{\"1\":{\"256\":1}}],[\"在大规模文本语料上学习高容量的语言模型\",{\"1\":{\"442\":1}}],[\"在大规模场景理解任务中表现一般\",{\"1\":{\"112\":1}}],[\"在大规模或实时应用中可能成为瓶颈\",{\"1\":{\"26\":1}}],[\"在网络深层中\",{\"1\":{\"255\":1}}],[\"在参数或者计算上\",{\"1\":{\"255\":1}}],[\"在全局平均池化之后\",{\"1\":{\"247\":1}}],[\"在全参数微调下\",{\"1\":{\"193\":1}}],[\"在执行的时候\",{\"1\":{\"242\":1}}],[\"在蒸馏相关内容里其实提到过\",{\"1\":{\"240\":1}}],[\"在看\",{\"1\":{\"240\":1}}],[\"在损失函数上做文章\",{\"1\":{\"240\":1}}],[\"在其他任务\",{\"1\":{\"233\":1}}],[\"在copa任务中仅落后1\",{\"1\":{\"462\":1}}],[\"在cv领域\",{\"1\":{\"238\":1}}],[\"在chartqa和ocrbench上超越所有商业模型\",{\"1\":{\"220\":1}}],[\"在caption前添加prompt\",{\"1\":{\"142\":1}}],[\"在此类任务中依然适用\",{\"1\":{\"460\":1}}],[\"在此实现中未使用\",{\"1\":{\"403\":1}}],[\"在此版本中\",{\"1\":{\"215\":1}}],[\"在此阶段\",{\"1\":{\"200\":1}}],[\"在无标记数据上使用语言模型目标来学习神经网络初始化的参数\",{\"1\":{\"440\":1}}],[\"在无需ocr工具的情况下展现强大的多模态能力\",{\"1\":{\"210\":1}}],[\"在无空间先验下实现跨源特征对齐\",{\"1\":{\"52\":1}}],[\"在8个任务中达到sota\",{\"1\":{\"208\":1}}],[\"在中文任务中表现优于gpt\",{\"1\":{\"208\":1}}],[\"在对抗性问题上易产生幻觉\",{\"1\":{\"484\":1}}],[\"在对毒性任务加入\",{\"1\":{\"469\":1}}],[\"在对话式问答数据集上\",{\"1\":{\"455\":1}}],[\"在对话任务\",{\"1\":{\"196\":1}}],[\"在对比学习框架中\",{\"1\":{\"238\":1}}],[\"在对比学习\",{\"1\":{\"157\":1}}],[\"在计算iou得分之前\",{\"1\":{\"406\":1}}],[\"在计算机视觉领域\",{\"1\":{\"278\":1,\"678\":1}}],[\"在计算机视觉领域展现出强大潜力\",{\"1\":{\"166\":1}}],[\"在计算成本和准确率之间取得了良好平衡\",{\"1\":{\"196\":1}}],[\"在不进行任何梯度更新的前提下\",{\"1\":{\"465\":1}}],[\"在不进行梯度更新的前提下实现任务适应\",{\"1\":{\"461\":1}}],[\"在不损失太多性能的情况下减少了模型的大小\",{\"1\":{\"428\":1}}],[\"在不改变大模型的前提下\",{\"1\":{\"419\":1}}],[\"在不复制数据的前提下\",{\"1\":{\"384\":1}}],[\"在不使用指令微调的前提下\",{\"1\":{\"194\":1}}],[\"在不同情形下它发生的概率\",{\"1\":{\"569\":1}}],[\"在不同微调策略下\",{\"1\":{\"193\":1}}],[\"在不同解码阶段注入语言线索\",{\"1\":{\"71\":1}}],[\"在不同解码层注入语言信息\",{\"1\":{\"70\":1}}],[\"在英中双语的\",{\"1\":{\"194\":1}}],[\"在我们的实验中\",{\"1\":{\"169\":1}}],[\"在预训练\",{\"1\":{\"504\":1}}],[\"在预训练层面进行数据过滤\",{\"1\":{\"472\":1}}],[\"在预训练阶段学习泛化模式\",{\"1\":{\"464\":1}}],[\"在预训练阶段随机掩码部分图像块并让模型预测原始视觉标记\",{\"1\":{\"165\":1}}],[\"在预训练和使用该模型时\",{\"1\":{\"300\":1}}],[\"在预训练的基础上\",{\"1\":{\"225\":1}}],[\"在预训练时\",{\"1\":{\"166\":1,\"290\":1}}],[\"在预训练模型基础上继续使用\",{\"1\":{\"137\":1}}],[\"在进行开发前\",{\"1\":{\"687\":1}}],[\"在进行普通计算\",{\"1\":{\"656\":1}}],[\"在进行\",{\"1\":{\"162\":1}}],[\"在多项基准测试中超越了\",{\"1\":{\"674\":1}}],[\"在多数情况下会返回原张量的视图\",{\"1\":{\"385\":1}}],[\"在多模态对话基准\",{\"1\":{\"195\":1}}],[\"在多语言理解和代码生成等方面表现出色\",{\"1\":{\"674\":1}}],[\"在多语言图像→文本检索任务\",{\"1\":{\"194\":1}}],[\"在多语言版本的\",{\"1\":{\"194\":1}}],[\"在多个基准测试中达到与更大规模专有模型相当的性能\",{\"1\":{\"486\":1}}],[\"在多个方面仍存在不足\",{\"1\":{\"472\":1}}],[\"在多个nlp任务上的零样本\",{\"1\":{\"455\":1}}],[\"在多个下游任务\",{\"1\":{\"166\":1}}],[\"在多个任务\",{\"1\":{\"120\":1}}],[\"在多任务\",{\"1\":{\"157\":1}}],[\"在噪声标签数据上提高学习效果\",{\"1\":{\"157\":1}}],[\"在单模态编码器上进行图文对比学习\",{\"1\":{\"153\":1}}],[\"在众多下游任务中都实现了最先进\",{\"1\":{\"138\":1}}],[\"在保持相同计算成本下\",{\"1\":{\"495\":1}}],[\"在保持预训练高效的前提下\",{\"1\":{\"122\":1}}],[\"在保留细节信息的同时兼容多样化的图像分辨率\",{\"1\":{\"216\":1}}],[\"在保留信息的同时提升训练数据质量\",{\"1\":{\"120\":1}}],[\"在保证高性能的同时显著降低了计算成本\",{\"1\":{\"150\":1}}],[\"在速度和效率上占优\",{\"1\":{\"112\":1}}],[\"在部件分割任务中\",{\"1\":{\"112\":1}}],[\"在通道维度进行拼接\",{\"1\":{\"109\":1}}],[\"在分类任务中更有用\",{\"1\":{\"540\":1}}],[\"在分类任务中\",{\"1\":{\"404\":1}}],[\"在分割任务中\",{\"1\":{\"105\":1}}],[\"在分组内进行信息交换\",{\"1\":{\"75\":1}}],[\"在这方面表现较差\",{\"1\":{\"673\":1}}],[\"在这种连续的空间里\",{\"1\":{\"566\":1}}],[\"在这种情况下\",{\"1\":{\"97\":1,\"242\":1,\"278\":1,\"440\":1}}],[\"在这一理论指导下\",{\"1\":{\"464\":1}}],[\"在这之后用生成模型分配的剩下序列的平均token对数概率高的作为结果\",{\"1\":{\"449\":1}}],[\"在这些任务中\",{\"1\":{\"471\":1}}],[\"在这些示例中\",{\"1\":{\"434\":1}}],[\"在这些点中选出若干个中心点\",{\"1\":{\"87\":1}}],[\"在这里\",{\"1\":{\"414\":1}}],[\"在这个装饰器修饰的函数内\",{\"1\":{\"388\":1}}],[\"在这个高维空间里\",{\"1\":{\"294\":1}}],[\"在这个流派只有一个编码器\",{\"1\":{\"242\":1}}],[\"在这个向量上做交叉熵\",{\"1\":{\"240\":1}}],[\"在这个过程中\",{\"1\":{\"234\":1,\"235\":1,\"240\":1,\"273\":1}}],[\"在这个任务上\",{\"1\":{\"227\":1}}],[\"在multinli上转移embedding能提升结果\",{\"1\":{\"449\":1}}],[\"在multinli和race上的性能随着层数的变化而变化\",{\"1\":{\"449\":1}}],[\"在mmbench\",{\"1\":{\"220\":1}}],[\"在ms\",{\"1\":{\"178\":1}}],[\"在msg中\",{\"1\":{\"95\":1}}],[\"在mrg中\",{\"1\":{\"97\":1}}],[\"在训练公式\",{\"1\":{\"444\":1}}],[\"在训练过程中发现\",{\"1\":{\"237\":1}}],[\"在训练过程中\",{\"1\":{\"224\":1,\"271\":1}}],[\"在训练中\",{\"1\":{\"157\":1}}],[\"在训练时引入随机丢弃形心来模拟不同密度情况\",{\"1\":{\"96\":1}}],[\"在训练时引入不同密度的点集情况\",{\"1\":{\"96\":1}}],[\"在训练和测试中不作为显式监督信号\",{\"1\":{\"64\":1}}],[\"在密集采样的区域中学到的特征可能无法很好地泛化到稀疏采样的区域\",{\"1\":{\"94\":1}}],[\"在屋里空间或某些特定的抽象空间中\",{\"1\":{\"90\":1}}],[\"在平移不变性上也有局限性\",{\"1\":{\"86\":1}}],[\"在论文的最后\",{\"1\":{\"299\":1}}],[\"在论文中\",{\"1\":{\"272\":1,\"296\":1,\"297\":1}}],[\"在论文中提到\",{\"1\":{\"112\":1}}],[\"在论文\",{\"1\":{\"78\":1}}],[\"在扩展过程中\",{\"1\":{\"63\":1}}],[\"在跨模态融合后\",{\"1\":{\"41\":1}}],[\"在混合精度下运行\",{\"1\":{\"40\":1}}],[\"在几何形状变化显著的同类物体中\",{\"1\":{\"25\":1}}],[\"在同一张交互图像中存在多个物体时\",{\"1\":{\"25\":1}}],[\"在\",{\"1\":{\"23\":1,\"29\":1,\"33\":1,\"45\":1,\"71\":1,\"73\":1,\"76\":1,\"78\":1,\"82\":2,\"83\":1,\"92\":3,\"107\":1,\"108\":1,\"112\":1,\"126\":2,\"134\":1,\"140\":1,\"156\":1,\"170\":1,\"190\":1,\"193\":2,\"194\":1,\"195\":1,\"200\":1,\"201\":1,\"207\":1,\"227\":1,\"237\":2,\"239\":1,\"240\":1,\"241\":1,\"270\":1,\"275\":1,\"290\":1,\"299\":2,\"352\":2,\"353\":1,\"385\":1,\"425\":2,\"428\":1,\"464\":1,\"471\":4,\"492\":1,\"505\":1,\"540\":2,\"542\":1,\"544\":1,\"568\":1,\"585\":1,\"652\":1,\"660\":2,\"662\":1,\"667\":1,\"673\":1,\"674\":2,\"677\":1,\"684\":1}}],[\"总排列数\",{\"1\":{\"600\":2,\"601\":2}}],[\"总的排列方式共有\",{\"1\":{\"600\":1}}],[\"总句对数量\",{\"1\":{\"515\":1}}],[\"总掩码的词数量\",{\"1\":{\"515\":1}}],[\"总词汇量\",{\"1\":{\"511\":1}}],[\"总数据量达160gb\",{\"1\":{\"498\":1}}],[\"总计超过\",{\"1\":{\"494\":1}}],[\"总计\",{\"1\":{\"493\":1}}],[\"总计1\",{\"1\":{\"480\":1}}],[\"总规模1\",{\"1\":{\"481\":1}}],[\"总之\",{\"1\":{\"444\":1,\"449\":1,\"468\":1,\"678\":1}}],[\"总批量大小\",{\"1\":{\"200\":1,\"201\":1}}],[\"总体而言\",{\"1\":{\"448\":1}}],[\"总体来看\",{\"1\":{\"278\":1}}],[\"总体\",{\"1\":{\"191\":1}}],[\"总体数据统计\",{\"1\":{\"17\":1}}],[\"总损失\",{\"1\":{\"78\":1,\"514\":1}}],[\"总损失包含三项\",{\"1\":{\"55\":1}}],[\"总样本数\",{\"1\":{\"65\":1}}],[\"总共得到\",{\"1\":{\"63\":1}}],[\"总结系统\",{\"1\":{\"472\":1}}],[\"总结并解释\",{\"1\":{\"471\":1}}],[\"总结一句话\",{\"1\":{\"231\":1}}],[\"总结一下这篇文章\",{\"1\":{\"231\":1}}],[\"总结文章\",{\"1\":{\"224\":1}}],[\"总结表格\",{\"1\":{\"112\":1}}],[\"总结\",{\"0\":{\"69\":1,\"197\":1,\"301\":1,\"310\":1,\"360\":1,\"378\":1,\"457\":1,\"486\":1,\"502\":1},\"1\":{\"33\":1,\"90\":1,\"105\":1,\"267\":1,\"355\":1,\"395\":1,\"454\":1,\"461\":1,\"462\":1,\"463\":1,\"464\":1,\"471\":1,\"472\":1,\"481\":1,\"483\":1,\"484\":1,\"542\":1,\"566\":1,\"657\":1}}],[\"总训练轮次\",{\"1\":{\"22\":1}}],[\"x^\",{\"1\":{\"660\":1}}],[\"x的tf\",{\"1\":{\"662\":1}}],[\"x的正向传播简单地对输入取反\",{\"1\":{\"660\":1}}],[\"x的\",{\"1\":{\"660\":1}}],[\"x需要调用x的\",{\"1\":{\"660\":1}}],[\"x时\",{\"1\":{\"660\":2}}],[\"xs\",{\"1\":{\"651\":5,\"656\":2,\"658\":2,\"660\":1}}],[\"xsystem\",{\"1\":{\"227\":2}}],[\"x0\",{\"1\":{\"622\":2,\"658\":4,\"660\":31,\"666\":5,\"667\":36}}],[\"xu\",{\"1\":{\"469\":1}}],[\"xu等人2018\",{\"1\":{\"178\":1}}],[\"x和y之间的关系是\",{\"1\":{\"414\":1}}],[\"xm\",{\"1\":{\"414\":1}}],[\"xmf\",{\"1\":{\"22\":1}}],[\"xlnet\",{\"1\":{\"464\":1,\"492\":1}}],[\"xlabel\",{\"1\":{\"289\":1,\"667\":1}}],[\"xlm\",{\"1\":{\"194\":1}}],[\"x=i\",{\"1\":{\"289\":1}}],[\"xticks\",{\"1\":{\"289\":1}}],[\"xtd\",{\"1\":{\"194\":1}}],[\"x4等等也经过e12得到了真正的负样本表征f2\",{\"1\":{\"238\":1}}],[\"x3\",{\"1\":{\"238\":1,\"268\":3}}],[\"xa\",{\"1\":{\"227\":1}}],[\"x2\",{\"1\":{\"268\":3,\"414\":1}}],[\"x2a\",{\"1\":{\"227\":1}}],[\"x2instruct\",{\"1\":{\"227\":1}}],[\"xinstruct\",{\"1\":{\"227\":1}}],[\"xi\",{\"1\":{\"105\":1}}],[\"xn\",{\"1\":{\"105\":3,\"268\":3}}],[\"x1的计算图\",{\"1\":{\"666\":1}}],[\"x1的导数公式为\",{\"1\":{\"660\":1}}],[\"x1的反向传播中\",{\"1\":{\"660\":1}}],[\"x12叫做x11的正样本\",{\"1\":{\"238\":1}}],[\"x1a\",{\"1\":{\"227\":1}}],[\"x1instruct\",{\"1\":{\"227\":1}}],[\"x1\",{\"1\":{\"105\":3,\"414\":1,\"622\":2,\"658\":3,\"660\":34,\"666\":5,\"667\":34,\"674\":1}}],[\"xx\",{\"1\":{\"83\":1}}],[\"xyz2\",{\"1\":{\"98\":2,\"100\":6}}],[\"xyz1\",{\"1\":{\"98\":3,\"100\":6}}],[\"xyz\",{\"1\":{\"30\":4,\"59\":4,\"70\":5,\"76\":1,\"92\":60,\"93\":11,\"96\":31,\"98\":2,\"101\":22}}],[\"x\",{\"1\":{\"29\":2,\"34\":2,\"35\":2,\"36\":3,\"58\":2,\"59\":6,\"68\":2,\"72\":4,\"73\":10,\"74\":5,\"75\":3,\"76\":2,\"93\":12,\"96\":10,\"101\":9,\"107\":29,\"109\":32,\"110\":10,\"111\":20,\"114\":2,\"244\":3,\"268\":3,\"291\":8,\"292\":14,\"293\":15,\"294\":21,\"295\":9,\"296\":19,\"319\":7,\"323\":6,\"352\":1,\"355\":1,\"366\":4,\"378\":2,\"382\":1,\"383\":1,\"384\":3,\"385\":1,\"386\":6,\"387\":9,\"397\":3,\"407\":2,\"410\":6,\"412\":4,\"414\":1,\"420\":1,\"470\":2,\"531\":7,\"542\":1,\"550\":2,\"552\":3,\"553\":8,\"554\":4,\"556\":12,\"557\":4,\"558\":7,\"575\":1,\"608\":4,\"613\":5,\"614\":2,\"617\":6,\"622\":3,\"626\":3,\"630\":2,\"631\":2,\"632\":1,\"635\":3,\"638\":4,\"641\":4,\"645\":5,\"646\":5,\"651\":2,\"652\":4,\"654\":17,\"655\":2,\"656\":13,\"658\":19,\"659\":11,\"660\":24,\"662\":26,\"666\":25,\"667\":6}}],[\"x9x\",{\"1\":{\"4\":1}}],[\"引用计数无法降至0\",{\"1\":{\"657\":1}}],[\"引用计数\",{\"1\":{\"657\":1}}],[\"引用计数和分代垃圾回收\",{\"1\":{\"657\":1}}],[\"引起的对齐误差问题而提出的关键组件\",{\"1\":{\"397\":1}}],[\"引号中是\",{\"1\":{\"390\":1}}],[\"引言\",{\"0\":{\"270\":1,\"321\":1,\"492\":1,\"604\":1,\"650\":1,\"665\":1,\"670\":1}}],[\"引入上下文\",{\"1\":{\"464\":1}}],[\"引入了先进的检索技术\",{\"1\":{\"684\":1}}],[\"引入了多模态能力\",{\"1\":{\"674\":1}}],[\"引入了新数据集\",{\"1\":{\"492\":1}}],[\"引入了额外的推理延迟\",{\"1\":{\"424\":1}}],[\"引入了两个变换网络\",{\"1\":{\"108\":1}}],[\"引入到对大模型的微调中去\",{\"1\":{\"416\":1}}],[\"引入生成对抗网络\",{\"1\":{\"178\":1}}],[\"引入更丰富的监督\",{\"1\":{\"159\":1}}],[\"引入多尺度采样等\",{\"1\":{\"112\":1}}],[\"引入两个空间变换网络\",{\"1\":{\"105\":1}}],[\"引入\",{\"1\":{\"103\":1,\"105\":1,\"112\":1,\"470\":1,\"472\":1,\"663\":1,\"665\":2}}],[\"引入一组可学习的\",{\"1\":{\"70\":1}}],[\"引入交互先验\",{\"1\":{\"7\":1}}],[\"引导答案生成\",{\"1\":{\"454\":1}}],[\"引导模型学习有用的特征\",{\"1\":{\"240\":1}}],[\"引导模型生成特定任务的结果\",{\"1\":{\"231\":1}}],[\"引导语言模型更好地遵循用户指令\",{\"1\":{\"224\":1}}],[\"引导下提取的点云信息增强后的图像特征\",{\"1\":{\"41\":1}}],[\"引导\",{\"1\":{\"28\":1}}],[\"引导的3d可供性方法\",{\"1\":{\"22\":1}}],[\"包初始化文件\",{\"1\":{\"661\":1}}],[\"包和库是组织代码的重要方式\",{\"1\":{\"661\":1}}],[\"包裹起来\",{\"1\":{\"376\":1}}],[\"包\",{\"1\":{\"338\":1,\"661\":1}}],[\"包类型\",{\"1\":{\"338\":1}}],[\"包可能会安装到基础环境或系统\",{\"1\":{\"338\":1}}],[\"包会安装到该环境的\",{\"1\":{\"338\":1}}],[\"包名\",{\"1\":{\"331\":1}}],[\"包括让\",{\"1\":{\"684\":1}}],[\"包括中间步骤的数据流传输\",{\"1\":{\"684\":1}}],[\"包括批量处理\",{\"1\":{\"684\":1}}],[\"包括写文章\",{\"1\":{\"678\":1}}],[\"包括生成有害内容\",{\"1\":{\"675\":1}}],[\"包括文本分割\",{\"1\":{\"684\":1}}],[\"包括文本\",{\"1\":{\"675\":1}}],[\"包括了\",{\"1\":{\"674\":1}}],[\"包括开源和闭源\",{\"1\":{\"674\":1}}],[\"包括问题和上下文\",{\"1\":{\"540\":1}}],[\"包括wikitext\",{\"1\":{\"510\":1}}],[\"包括动态掩码\",{\"1\":{\"502\":1}}],[\"包括但不限于\",{\"1\":{\"470\":1}}],[\"包括预归一化\",{\"1\":{\"461\":1}}],[\"包括翻译\",{\"1\":{\"459\":1}}],[\"包括探索gpt\",{\"1\":{\"456\":1}}],[\"包括penn\",{\"1\":{\"455\":1}}],[\"包括以下几步\",{\"1\":{\"420\":1}}],[\"包括以下核心模块\",{\"1\":{\"70\":1}}],[\"包括基于对比学习的方法\",{\"1\":{\"278\":1}}],[\"包括图像分类\",{\"1\":{\"180\":1}}],[\"包括\",{\"1\":{\"140\":1,\"217\":1,\"292\":1,\"469\":1,\"494\":1,\"661\":1,\"674\":3,\"682\":1}}],[\"包括理解类任务和生成类任务\",{\"1\":{\"138\":1}}],[\"包括前向传播\",{\"1\":{\"81\":1}}],[\"包括与多个先进方法的对比以及消融实验和可视化分析\",{\"1\":{\"21\":1}}],[\"包含绘图的完整代码\",{\"1\":{\"667\":1}}],[\"包含多层pow\",{\"1\":{\"666\":1}}],[\"包含add函数节点和变量连接\",{\"1\":{\"666\":1}}],[\"包含高次项和交叉项\",{\"1\":{\"662\":1}}],[\"包含整个样本空间和空集\",{\"1\":{\"566\":1}}],[\"包含争议性内容\",{\"1\":{\"470\":1}}],[\"包含从\",{\"1\":{\"323\":1}}],[\"包含大约\",{\"1\":{\"300\":1}}],[\"包含一个线性层和一个\",{\"1\":{\"296\":1}}],[\"包含分类token\",{\"1\":{\"295\":1}}],[\"包含所有图像对应类别的列表\",{\"1\":{\"289\":1}}],[\"包含所有图像文件路径的列表\",{\"1\":{\"289\":1}}],[\"包含图像和对应的标签\",{\"1\":{\"289\":1}}],[\"包含图像和可选文本\",{\"1\":{\"286\":1}}],[\"包含图像信息\",{\"1\":{\"285\":1}}],[\"包含\",{\"1\":{\"285\":1}}],[\"包含约\",{\"1\":{\"226\":1,\"227\":1}}],[\"包含60亿参数的视觉编码器\",{\"1\":{\"181\":1}}],[\"包含离散标记索引\",{\"1\":{\"170\":1}}],[\"包含三个目标\",{\"1\":{\"153\":1}}],[\"包含三个关键步骤\",{\"1\":{\"71\":1}}],[\"包含两个文档片段和nsp损失\",{\"1\":{\"495\":1}}],[\"包含两个模块\",{\"1\":{\"128\":1}}],[\"包含两种适应性特征学习层\",{\"1\":{\"94\":1}}],[\"包含表面细节\",{\"1\":{\"114\":1}}],[\"包含7\",{\"1\":{\"49\":1}}],[\"包含jra模块\",{\"1\":{\"49\":1}}],[\"包含物体的坐标\",{\"1\":{\"8\":1}}],[\"包含15k交互图像和38k标注的3d物体实例\",{\"1\":{\"5\":1}}],[\"类定义\",{\"1\":{\"661\":1}}],[\"类的\",{\"1\":{\"656\":1}}],[\"类将\",{\"1\":{\"656\":1}}],[\"类提供一个新的方法\",{\"1\":{\"653\":1}}],[\"类以支持\",{\"1\":{\"651\":1}}],[\"类变量\",{\"1\":{\"576\":1}}],[\"类gpt式生成式模型进行推理的过程\",{\"1\":{\"475\":1,\"476\":1}}],[\"类装饰器通常通过实现\",{\"1\":{\"377\":1}}],[\"类装饰器\",{\"0\":{\"377\":1}}],[\"类方法\",{\"0\":{\"373\":1}}],[\"类型应用前景的思考\",{\"1\":{\"678\":1}}],[\"类型签名会失效\",{\"1\":{\"372\":1}}],[\"类型检查\",{\"1\":{\"372\":1}}],[\"类型\",{\"1\":{\"255\":4,\"378\":1,\"542\":1}}],[\"类似物理学中的相变现象\",{\"1\":{\"676\":1}}],[\"类似地\",{\"1\":{\"660\":2}}],[\"类似预激活残差网络\",{\"1\":{\"454\":1}}],[\"类似数据库查询\",{\"1\":{\"314\":1}}],[\"类似gpt\",{\"1\":{\"208\":1}}],[\"类似\",{\"1\":{\"190\":1,\"202\":1,\"270\":1}}],[\"类似于多项式逼近中的基函数组合\",{\"1\":{\"395\":1}}],[\"类似于在cnn中权重共享的概念\",{\"1\":{\"86\":1}}],[\"类似于传统的全连接层或\",{\"1\":{\"73\":1}}],[\"类似于\",{\"1\":{\"73\":1,\"169\":1,\"240\":1,\"674\":1}}],[\"类中提供的方法\",{\"1\":{\"389\":1}}],[\"类中\",{\"1\":{\"107\":1}}],[\"类\",{\"1\":{\"65\":2,\"67\":2,\"291\":1,\"576\":1,\"650\":1}}],[\"类比\",{\"1\":{\"28\":1}}],[\"类比额外交互\",{\"1\":{\"28\":1}}],[\"类比推理\",{\"0\":{\"12\":1}}],[\"类别权重\",{\"1\":{\"405\":1,\"407\":1}}],[\"类别数量不平衡\",{\"1\":{\"404\":1}}],[\"类别平衡权重\",{\"1\":{\"404\":1}}],[\"类别不平衡越严重\",{\"1\":{\"404\":1}}],[\"类别不平衡的问题\",{\"1\":{\"404\":1}}],[\"类别不平衡\",{\"1\":{\"401\":1}}],[\"类别\",{\"1\":{\"231\":3}}],[\"类别下图像与点云的数量比例\",{\"1\":{\"19\":1}}],[\"类别间存在明显的多对多关系\",{\"1\":{\"19\":1}}],[\"类平衡分析\",{\"1\":{\"19\":1}}],[\"多头潜在注意力\",{\"1\":{\"674\":1}}],[\"多头自注意力机制通过并行计算多个注意力头\",{\"1\":{\"548\":1}}],[\"多头自注意力机制\",{\"1\":{\"533\":1,\"548\":1}}],[\"多头自注意力计算流程图\",{\"1\":{\"531\":1,\"558\":1}}],[\"多头自注意力\",{\"0\":{\"295\":1,\"558\":1}}],[\"多头自注意力层\",{\"1\":{\"294\":1}}],[\"多输入与多输出\",{\"0\":{\"651\":1}}],[\"多输入\",{\"1\":{\"650\":1}}],[\"多元场景下也称为偏导数\",{\"1\":{\"626\":1}}],[\"多元正态分布\",{\"0\":{\"589\":1}}],[\"多元高斯\",{\"1\":{\"355\":1}}],[\"多元高斯分布的行为可能会显得非常反直觉\",{\"1\":{\"591\":1}}],[\"多元高斯分布的概率密度函数定义如下\",{\"1\":{\"590\":1}}],[\"多元高斯分布被称为二维高斯分布\",{\"1\":{\"590\":1}}],[\"多元高斯分布\",{\"1\":{\"355\":1}}],[\"多项系数\",{\"1\":{\"576\":1}}],[\"多项选择题\",{\"1\":{\"544\":1}}],[\"多项选择任务是指给定一个问题和多个候选答案\",{\"1\":{\"544\":1}}],[\"多项选择任务\",{\"0\":{\"544\":1}}],[\"多项式项数\",{\"1\":{\"395\":1}}],[\"多项式\",{\"1\":{\"395\":2}}],[\"多项式阶数\",{\"1\":{\"395\":1}}],[\"多项式函数空间\",{\"1\":{\"395\":1}}],[\"多项式逼近通过增加阶数\",{\"1\":{\"395\":1}}],[\"多项式逼近需要\",{\"1\":{\"395\":1}}],[\"多项式逼近的基函数是固定的\",{\"1\":{\"395\":1}}],[\"多项式逼近\",{\"1\":{\"395\":2}}],[\"多领域竞争力\",{\"1\":{\"482\":1}}],[\"多数投票69\",{\"1\":{\"482\":1}}],[\"多数比较数据仅有\",{\"1\":{\"472\":1}}],[\"多群体条件对齐\",{\"1\":{\"472\":1}}],[\"多条件限制\",{\"1\":{\"472\":1}}],[\"多任务理解\",{\"1\":{\"482\":1}}],[\"多任务与多语言学习的基础\",{\"1\":{\"464\":1}}],[\"多任务学习的概率视角\",{\"1\":{\"454\":1}}],[\"多任务学习的挑战与机遇\",{\"1\":{\"453\":1}}],[\"多任务学习\",{\"1\":{\"453\":1}}],[\"多步推理\",{\"1\":{\"463\":1}}],[\"多语言支持\",{\"1\":{\"675\":1}}],[\"多语言理解和创意生成方面有显著提升\",{\"1\":{\"674\":1}}],[\"多语言翻译能力显著提升\",{\"1\":{\"462\":1}}],[\"多语言能力\",{\"1\":{\"208\":1}}],[\"多达千亿\",{\"1\":{\"414\":1}}],[\"多次折叠\",{\"1\":{\"395\":1}}],[\"多轮交互\",{\"1\":{\"483\":1}}],[\"多轮对话能力\",{\"1\":{\"225\":1}}],[\"多轮对话\",{\"1\":{\"220\":1}}],[\"多轮数据集\",{\"1\":{\"138\":1}}],[\"多阶段训练策略\",{\"1\":{\"197\":1}}],[\"多阶段编码\",{\"1\":{\"70\":1}}],[\"多功能性\",{\"1\":{\"181\":1}}],[\"多视角图像\",{\"1\":{\"114\":1}}],[\"多视角\",{\"1\":{\"112\":2}}],[\"多\",{\"1\":{\"98\":1}}],[\"多个应用可以只依赖于一个或少数几个大模型进行统一建设\",{\"1\":{\"677\":1}}],[\"多个包的集合\",{\"1\":{\"661\":1}}],[\"多个模块的集合\",{\"1\":{\"661\":1}}],[\"多个函数可以连续调用\",{\"1\":{\"616\":1}}],[\"多个装饰器叠加时的执行顺序\",{\"0\":{\"376\":1}}],[\"多个装饰器嵌套时更容易出错\",{\"1\":{\"372\":1}}],[\"多个变量\",{\"1\":{\"355\":1}}],[\"多个合成描述\",{\"1\":{\"138\":1}}],[\"多个\",{\"1\":{\"98\":2,\"513\":1}}],[\"多个物体场景\",{\"1\":{\"25\":1}}],[\"多分辨率分组\",{\"0\":{\"97\":1},\"1\":{\"97\":1}}],[\"多尺度建模能力\",{\"1\":{\"112\":1}}],[\"多尺度聚合机制\",{\"1\":{\"112\":1}}],[\"多尺度特征提取机制\",{\"1\":{\"96\":1}}],[\"多尺度分组分类模型\",{\"0\":{\"96\":1}}],[\"多尺度分组\",{\"0\":{\"95\":1},\"1\":{\"94\":1,\"95\":1,\"96\":1}}],[\"多尺度上采样模块\",{\"1\":{\"46\":1}}],[\"多阈值下的\",{\"1\":{\"82\":1}}],[\"多阈值评估更稳定\",{\"1\":{\"82\":1}}],[\"多形状的功能区域的感知能力\",{\"1\":{\"71\":1}}],[\"多层感知机\",{\"1\":{\"59\":1,\"73\":1}}],[\"多模态支持\",{\"1\":{\"675\":1}}],[\"多模态融合\",{\"1\":{\"674\":1}}],[\"多模态能力\",{\"1\":{\"674\":1}}],[\"多模态因果自监督\",{\"1\":{\"285\":1}}],[\"多模态网络设计\",{\"1\":{\"280\":1}}],[\"多模态模型\",{\"1\":{\"674\":1}}],[\"多模态模型vit原理与图片分类实战演练\",{\"1\":{\"287\":1}}],[\"多模态模型在过往发展的过程中\",{\"1\":{\"280\":1}}],[\"多模态模型clip原理与图片分类\",{\"1\":{\"269\":1}}],[\"多模态生成任务中充当解码器\",{\"1\":{\"268\":1}}],[\"多模态交互核心\",{\"1\":{\"267\":1}}],[\"多模态交叉注意力\",{\"1\":{\"264\":1}}],[\"多模态交叉注意力模块\",{\"1\":{\"45\":1}}],[\"多模态时\",{\"1\":{\"263\":1}}],[\"多模态关键点\",{\"1\":{\"263\":1,\"264\":1}}],[\"多模态论文中常用的改编版本的bert代码实现记录\",{\"1\":{\"260\":1}}],[\"多模态常用改编bert代码实现\",{\"0\":{\"260\":1}}],[\"多模态\",{\"0\":{\"250\":1,\"261\":1},\"1\":{\"674\":1}}],[\"多模态聊天机器人\",{\"1\":{\"227\":1}}],[\"多模态对话等任务上的领先性能\",{\"1\":{\"198\":1}}],[\"多模态对话等任务上全面领先于现有开源模型\",{\"1\":{\"197\":1}}],[\"多模态对话\",{\"1\":{\"189\":1}}],[\"多模态混合的编码器\",{\"1\":{\"126\":1}}],[\"多模态数据的\",{\"1\":{\"43\":1}}],[\"多模态嵌入\",{\"1\":{\"43\":2}}],[\"多模态特征投影到语言语义空间\",{\"0\":{\"42\":1},\"1\":{\"40\":1}}],[\"多样化描述更有利于学习\",{\"1\":{\"120\":1}}],[\"多样化的几何变体\",{\"1\":{\"29\":1}}],[\"多样性未公开\",{\"1\":{\"483\":1}}],[\"多样性不足\",{\"1\":{\"472\":1}}],[\"多样性目标\",{\"1\":{\"454\":1}}],[\"多样性\",{\"1\":{\"6\":1}}],[\"多实例鲁棒性\",{\"1\":{\"25\":1}}],[\"多种可供性\",{\"1\":{\"25\":1}}],[\"多对多关系分析\",{\"1\":{\"19\":1}}],[\"含special\",{\"1\":{\"520\":1}}],[\"含非\",{\"1\":{\"338\":1}}],[\"含义\",{\"1\":{\"76\":2,\"310\":2,\"355\":1,\"378\":1,\"390\":1}}],[\"含\",{\"1\":{\"18\":1,\"201\":1,\"202\":1}}],[\"覆盖了从\",{\"1\":{\"684\":1}}],[\"覆盖\",{\"1\":{\"674\":1}}],[\"覆盖以下数据集\",{\"1\":{\"131\":1}}],[\"覆盖23类物体和17种功能\",{\"1\":{\"49\":1}}],[\"覆盖类别\",{\"1\":{\"17\":1}}],[\"覆盖多对多关联\",{\"1\":{\"6\":1}}],[\"而更需要掌握使用大模型的实践技巧\",{\"1\":{\"686\":1}}],[\"而每次反向传播时导数会累加\",{\"1\":{\"667\":1}}],[\"而每一层\",{\"1\":{\"477\":1}}],[\"而弱引用不会增加对象的引用计数\",{\"1\":{\"657\":1}}],[\"而弱引用可通过避免循环引用直接解决问题\",{\"1\":{\"657\":1}}],[\"而言可能产生显著的性能损耗\",{\"1\":{\"657\":1}}],[\"而链式法则的计算需要知晓每个函数在正向传播时的输入值和输出值\",{\"1\":{\"630\":1}}],[\"而反向传播算法可以高效地求出每个变量的导数\",{\"1\":{\"618\":1}}],[\"而tinypytorch的变量实现为variable类\",{\"1\":{\"606\":1}}],[\"而本系列旨在揭开这些技术和机制的神秘面纱\",{\"1\":{\"604\":1}}],[\"而逆概率问题关注的则是\",{\"1\":{\"597\":1}}],[\"而没病的人也有误报\",{\"1\":{\"569\":1}}],[\"而没有建模点与点之间的局部几何关系\",{\"1\":{\"112\":1}}],[\"而可能是\",{\"1\":{\"566\":1}}],[\"而针对这20\",{\"1\":{\"512\":1}}],[\"而要考虑它的上下文\",{\"1\":{\"505\":1}}],[\"而设计这些模型并测试其\",{\"1\":{\"504\":1}}],[\"而后续工作通过多任务微调\",{\"1\":{\"501\":1}}],[\"而roberta的目标是通过系统性地复现\",{\"1\":{\"501\":1}}],[\"而roberta改为每次输入时动态生成掩码\",{\"1\":{\"497\":1}}],[\"而roberta通过系统性的调整\",{\"1\":{\"495\":1}}],[\"而原始\",{\"1\":{\"494\":1}}],[\"而hoffmann推荐的10b模型仅训练200b\",{\"1\":{\"480\":1}}],[\"而llama\",{\"1\":{\"479\":1}}],[\"而common\",{\"1\":{\"461\":1}}],[\"而现有模型难以实现类似能力\",{\"1\":{\"460\":1}}],[\"而现代模型\",{\"1\":{\"460\":1}}],[\"而无需事先见过具体示例\",{\"1\":{\"676\":1}}],[\"而无需额外的训练或参数更新\",{\"1\":{\"676\":1}}],[\"而无需调用特定函数\",{\"1\":{\"660\":1}}],[\"而无需考虑pad词的全局上下文信息是否需要进行计算\",{\"1\":{\"517\":1}}],[\"而无需依赖专有数据\",{\"1\":{\"479\":1}}],[\"而无需明确的监督信号\",{\"1\":{\"457\":1}}],[\"而无需显式监督\",{\"1\":{\"454\":1,\"456\":1}}],[\"而无需参数调整或架构修改\",{\"1\":{\"453\":1}}],[\"而通用系统需要能够根据任务描述动态调整行为\",{\"1\":{\"454\":1}}],[\"而通过引入特定的归纳偏置\",{\"1\":{\"287\":1}}],[\"而机器学习系统通常需要数百至数千个任务示例才能实现良好的泛化\",{\"1\":{\"453\":1}}],[\"而特定任务学习的标注数据有非常少\",{\"1\":{\"439\":1}}],[\"而给的问题却是难度大很多的问题\",{\"1\":{\"436\":1}}],[\"而一些领域差距比较大的任务可能需要更大的\",{\"1\":{\"427\":1}}],[\"而对于较短的句子\",{\"1\":{\"514\":1}}],[\"而对于\",{\"1\":{\"425\":1}}],[\"而模型的输入输出维度不变\",{\"1\":{\"425\":1}}],[\"而用了qlora之后\",{\"1\":{\"421\":1}}],[\"而过度参数化的大模型背后\",{\"1\":{\"420\":1}}],[\"而prefix\",{\"1\":{\"419\":1}}],[\"而这个恒等式又来自于概率的乘法法则\",{\"1\":{\"596\":1}}],[\"而这些\",{\"1\":{\"565\":1}}],[\"而这些内容可以影响x生成期望中y的概率\",{\"1\":{\"418\":1}}],[\"而这里我们将会反转这个逻辑\",{\"1\":{\"276\":1}}],[\"而当\",{\"1\":{\"405\":1}}],[\"而我们检测到一个\",{\"1\":{\"396\":1}}],[\"而深层网络只需多项式数量\",{\"1\":{\"395\":1}}],[\"而单层网络需要\",{\"1\":{\"395\":1}}],[\"而神经网络的全连接层只能接受固定大小的输入\",{\"1\":{\"396\":1}}],[\"而神经网络的函数复合\",{\"1\":{\"395\":1}}],[\"而神经网络的基函数\",{\"1\":{\"395\":1}}],[\"而神经网络\",{\"1\":{\"395\":1}}],[\"而神经网络通过非线性激活和分层结构\",{\"1\":{\"395\":1}}],[\"而多项式需极高阶数才能近似突变\",{\"1\":{\"395\":1}}],[\"而多模态大型语言模型\",{\"1\":{\"208\":1}}],[\"而假负例\",{\"1\":{\"353\":1}}],[\"而降低阈值则会产生相反的效果\",{\"1\":{\"346\":1}}],[\"而物理转置中\",{\"1\":{\"326\":1}}],[\"而预训练过程中使用的输入图像尺寸通常固定为\",{\"1\":{\"290\":1}}],[\"而vit将其放到前面\",{\"1\":{\"294\":1}}],[\"而vit\",{\"1\":{\"287\":1}}],[\"而vit则选择了三种不同尺寸的模型\",{\"1\":{\"272\":1}}],[\"而最大的vit模型vit\",{\"1\":{\"272\":1}}],[\"而图像编码器\",{\"1\":{\"272\":1,\"273\":1}}],[\"而图像编码器则用于提取图像的特征\",{\"1\":{\"272\":1}}],[\"而剩余的个文本\",{\"1\":{\"272\":1}}],[\"而且任何一个词都有可能是被替换掉的\",{\"1\":{\"505\":1}}],[\"而且能适应不同大小数据集\",{\"1\":{\"448\":1}}],[\"而且下游任务表现出相似甚至更好的性能\",{\"1\":{\"259\":1}}],[\"而且动量队列可能重复包含同一个样本\",{\"1\":{\"145\":1}}],[\"而dual\",{\"1\":{\"256\":1}}],[\"而开源模型主要依赖英语数据\",{\"1\":{\"208\":1}}],[\"而开源模型多采用固定分辨率\",{\"1\":{\"208\":1}}],[\"而开源模型的视觉基础模型\",{\"1\":{\"208\":1}}],[\"而文本编码器\",{\"1\":{\"191\":1}}],[\"而参数数量相同时\",{\"1\":{\"189\":1}}],[\"而视觉编码器通常仅约10亿参数\",{\"1\":{\"181\":1}}],[\"而不必担心底层的基础设施和运维工作\",{\"1\":{\"685\":1}}],[\"而不能看到未来的词\",{\"1\":{\"548\":1}}],[\"而不能为\",{\"1\":{\"410\":1}}],[\"而不会将精力聚焦在优化模型本身上\",{\"1\":{\"686\":1}}],[\"而不会释放\",{\"1\":{\"366\":1}}],[\"而不会考虑模型分类\",{\"1\":{\"342\":1}}],[\"而不是密度最大的中心点\",{\"1\":{\"594\":1}}],[\"而不是重新生成一个新的答案\",{\"1\":{\"542\":1}}],[\"而不是进行真正意义上的概念抽象和泛化\",{\"1\":{\"463\":1}}],[\"而不是原始\",{\"1\":{\"447\":1}}],[\"而不是原始像素\",{\"1\":{\"170\":1}}],[\"而不是逐点分类\",{\"1\":{\"401\":1}}],[\"而不是\",{\"1\":{\"397\":1,\"401\":1}}],[\"而不是真正地重新排列数据\",{\"1\":{\"326\":1}}],[\"而不需要依赖专有或不可访问的数据集\",{\"1\":{\"674\":1}}],[\"而不需要重新计算所有之前\",{\"1\":{\"477\":1}}],[\"而不需要显式复制数据\",{\"1\":{\"327\":1}}],[\"而不需要人为地去标注这种标签信息\",{\"1\":{\"234\":1}}],[\"而不需要改变整个模型参数\",{\"1\":{\"231\":1}}],[\"而不仅仅依赖单一实例\",{\"1\":{\"29\":1}}],[\"而非实时回收循环引用对象\",{\"1\":{\"657\":1}}],[\"而非生成答案\",{\"1\":{\"542\":1}}],[\"而非本质的\",{\"1\":{\"472\":1}}],[\"而非验证\",{\"1\":{\"470\":1}}],[\"而非具备深层理解与推理能力的系统\",{\"1\":{\"463\":1}}],[\"而非真正理解语言和任务的系统\",{\"1\":{\"463\":1}}],[\"而非通用的多任务处理者\",{\"1\":{\"453\":1}}],[\"而非物理复制数据\",{\"1\":{\"327\":1}}],[\"而非\",{\"1\":{\"285\":1,\"468\":1,\"472\":1}}],[\"而非像素值\",{\"1\":{\"166\":1}}],[\"而非高层语义\",{\"1\":{\"166\":1}}],[\"而非直接回归像素值\",{\"1\":{\"165\":1}}],[\"而非训练时间变长\",{\"1\":{\"136\":1}}],[\"而在本阶段\",{\"1\":{\"650\":1}}],[\"而在返回的内部函数中\",{\"1\":{\"367\":1}}],[\"而在物理转置中\",{\"1\":{\"326\":1}}],[\"而在掩码语言建模\",{\"1\":{\"157\":1}}],[\"而在\",{\"1\":{\"145\":1}}],[\"而自动爬取的网页图文对\",{\"1\":{\"128\":1}}],[\"而嵌入层\",{\"1\":{\"126\":1}}],[\"而点云是无序集合\",{\"1\":{\"115\":1}}],[\"而是将大模型作为一个调用工具\",{\"1\":{\"686\":1}}],[\"而是真正朝着\",{\"1\":{\"665\":1}}],[\"而是真实泛化能力\",{\"1\":{\"455\":1}}],[\"而是直接覆盖梯度值\",{\"1\":{\"654\":1}}],[\"而是对输入文本中每个\",{\"1\":{\"542\":1}}],[\"而是学习出来的\",{\"1\":{\"506\":1}}],[\"而是学会根据用户指令理解任务意图并生成合适的结果\",{\"1\":{\"224\":1}}],[\"而是训练策略和数据规模的优化\",{\"1\":{\"502\":1}}],[\"而是在网络的每一层添加了位置编码\",{\"1\":{\"674\":1}}],[\"而是在\",{\"1\":{\"495\":1}}],[\"而是特定群体与目标下的实用性对齐\",{\"1\":{\"472\":1}}],[\"而是列举可能性并犹豫\",{\"1\":{\"471\":1}}],[\"而是有机融合并推升了这些已有成果\",{\"1\":{\"464\":1}}],[\"而是按质量设权重采样\",{\"1\":{\"461\":1}}],[\"而是组合学到的知识\",{\"1\":{\"455\":1}}],[\"而是利用llm结果的多样性\",{\"1\":{\"435\":1}}],[\"而是多条\",{\"1\":{\"434\":1}}],[\"而是关注预测和\",{\"1\":{\"402\":1}}],[\"而是综合周围村子的情况加权得出\",{\"1\":{\"397\":1}}],[\"而是采用缓慢更新呢\",{\"1\":{\"241\":1}}],[\"而是100多万个类别\",{\"1\":{\"235\":1}}],[\"而是\",{\"1\":{\"147\":1,\"471\":1,\"472\":1}}],[\"而是能够捕获多个尺度上的局部特征\",{\"1\":{\"95\":1}}],[\"而是通过单一语言建模目标\",{\"1\":{\"464\":1}}],[\"而是通过以下方式创建了一个新的\",{\"1\":{\"325\":1}}],[\"而是通过\",{\"1\":{\"41\":1,\"73\":1,\"145\":1}}],[\"而是通过监督点级热图\",{\"1\":{\"15\":1}}],[\"而\",{\"1\":{\"78\":1,\"83\":1,\"133\":1,\"162\":1,\"191\":1,\"270\":1,\"366\":1,\"404\":1,\"414\":1,\"426\":1,\"471\":1,\"472\":1,\"493\":2,\"504\":1,\"508\":3,\"673\":1}}],[\"而下面这行代码实现的是一个\",{\"1\":{\"76\":1}}],[\"而几何结构文本则是单一连贯的几何描述\",{\"1\":{\"31\":1}}],[\"而great通过几何\",{\"1\":{\"7\":1}}],[\"这对于\",{\"1\":{\"684\":1}}],[\"这影响了其对问题的理解和回答\",{\"1\":{\"679\":1}}],[\"这可能会影响到其在相关领域的回答质量\",{\"1\":{\"679\":1}}],[\"这可能导致模型的知识更新滞后\",{\"1\":{\"679\":1}}],[\"这可能是因为参数量增加需要更多的语料\",{\"1\":{\"427\":1}}],[\"这引发了对未来人工智能发展的许多思考和计划\",{\"1\":{\"678\":1}}],[\"这引发我们对于\",{\"1\":{\"678\":1}}],[\"这使得开发者能够对\",{\"1\":{\"684\":1}}],[\"这使得它们在对话\",{\"1\":{\"675\":1}}],[\"这使得它们能够捕捉更多的语言知识和复杂的语法结构\",{\"1\":{\"675\":1}}],[\"这使得它在细粒度识别\",{\"1\":{\"112\":1}}],[\"这使得其语义编码能力得到了极大的增强\",{\"1\":{\"674\":1}}],[\"这使得当表达式为np\",{\"1\":{\"660\":1}}],[\"这\",{\"1\":{\"599\":1}}],[\"这被称为\",{\"1\":{\"673\":1}}],[\"这被称为似然函数\",{\"1\":{\"596\":1}}],[\"这被称为观测分布\",{\"1\":{\"596\":1}}],[\"这被称为先验分布\",{\"1\":{\"596\":1}}],[\"这非常反直觉\",{\"1\":{\"594\":1}}],[\"这等价于找出\",{\"1\":{\"565\":1}}],[\"这和原始论文稍有不同\",{\"1\":{\"552\":1}}],[\"这和使用\",{\"1\":{\"237\":1}}],[\"这比较容易并行\",{\"1\":{\"547\":1}}],[\"这段代码的意思是\",{\"1\":{\"542\":1}}],[\"这尤其适合当需要长时依赖\",{\"1\":{\"510\":1}}],[\"这本质上是一个三分类的问题\",{\"1\":{\"508\":1}}],[\"这本书里介绍了我们人类大脑的\",{\"1\":{\"433\":1}}],[\"这其实是\",{\"1\":{\"596\":1}}],[\"这其实是一个很容易理解的任务\",{\"1\":{\"505\":1}}],[\"这其实就是在说\",{\"1\":{\"542\":1}}],[\"这其实也是一种标签信息\",{\"1\":{\"234\":1}}],[\"这为自动微分奠定了基础\",{\"1\":{\"618\":1}}],[\"这为\",{\"1\":{\"470\":1}}],[\"这为未来探索更灵活的文本生成方式提供了启示\",{\"1\":{\"456\":1}}],[\"这造成了目标的不一致\",{\"1\":{\"468\":1}}],[\"这说明gpt\",{\"1\":{\"462\":1}}],[\"这得益于更高效的数据利用率\",{\"1\":{\"461\":1}}],[\"这限制了其在金融\",{\"1\":{\"463\":1}}],[\"这限制了模型的广泛应用\",{\"1\":{\"460\":1}}],[\"这限制了它们在许多缺乏标记数据领域的适用性\",{\"1\":{\"440\":1}}],[\"这项工作为后续gpt系列模型的发展奠定了理论基础和方法框架\",{\"1\":{\"457\":1}}],[\"这跟之前的工作一样\",{\"1\":{\"444\":1}}],[\"这句话会引导llm\",{\"1\":{\"433\":1}}],[\"这句话没看懂没关系\",{\"1\":{\"236\":1}}],[\"这会占用大量的内存资源并消耗较多的计算资源\",{\"1\":{\"428\":1}}],[\"这会导致如下图所示的共享变量a的梯度被重复累加\",{\"1\":{\"655\":1}}],[\"这会导致在跨任务上性能降低14\",{\"1\":{\"449\":1}}],[\"这会导致特征图上的空间对齐误差\",{\"1\":{\"397\":1}}],[\"这会导致尺度大的特征\",{\"1\":{\"359\":1}}],[\"这主要是因为如果矩阵\",{\"1\":{\"426\":1}}],[\"这条路径叫peft\",{\"1\":{\"416\":1}}],[\"这条路径叫全量微调fft\",{\"1\":{\"416\":1}}],[\"这三个关键要素是\",{\"1\":{\"325\":1}}],[\"这三个步骤构成了一个完整的跨模态融合流程\",{\"1\":{\"71\":1}}],[\"这表示从\",{\"1\":{\"579\":1}}],[\"这表示在内存中访问该张量时\",{\"1\":{\"323\":1}}],[\"这表明它在需要长期优化和知识整合的任务中仍有较大局限\",{\"1\":{\"463\":1}}],[\"这表明其具有一定程度的推理和快速适应能力\",{\"1\":{\"462\":1}}],[\"这表明大模型能够更好地吸收语言知识和上下文信息\",{\"1\":{\"462\":1}}],[\"这表明预训练模型中的每一层都包含了解决目标问题有用的功能\",{\"1\":{\"449\":1}}],[\"这表明训练clip模型需要消耗大量的资源\",{\"1\":{\"272\":1}}],[\"这表明\",{\"1\":{\"215\":1}}],[\"这有助于模型发现输入数据中更复杂的模式和关系\",{\"1\":{\"294\":1}}],[\"这也算是一类事件\",{\"1\":{\"566\":1}}],[\"这也是一个非常严重的问题\",{\"1\":{\"415\":1}}],[\"这也是为什么结构图中mlp\",{\"1\":{\"292\":1}}],[\"这也间接说明\",{\"1\":{\"137\":1}}],[\"这时分类分布可以写为\",{\"1\":{\"576\":1}}],[\"这时候cot的效果就不尽如人意\",{\"1\":{\"436\":1}}],[\"这时候也需要对大模型进行微调\",{\"1\":{\"415\":1}}],[\"这时候针对每个用户的数据\",{\"1\":{\"415\":1}}],[\"这时候微调就非常适用\",{\"1\":{\"415\":1}}],[\"这时就可以自定义\",{\"1\":{\"289\":1}}],[\"这时使用动量更新即可\",{\"1\":{\"238\":1}}],[\"这几乎是不可能完成的任务\",{\"1\":{\"287\":1}}],[\"这减少了llm学习视觉语言对齐的负担\",{\"1\":{\"286\":1}}],[\"这大大限制了它们的迁移能力和扩展性\",{\"1\":{\"278\":1}}],[\"这远远低于imagenet上的sota\",{\"1\":{\"278\":1}}],[\"这方面的工作并不多\",{\"1\":{\"278\":1}}],[\"这展示了其在图像分类任务中的灵活性和强大能力\",{\"1\":{\"273\":1}}],[\"这不仅展示了clip的强大功能\",{\"1\":{\"273\":1}}],[\"这不就是\",{\"1\":{\"239\":1}}],[\"这边\",{\"1\":{\"242\":1}}],[\"这要求\",{\"1\":{\"242\":1}}],[\"这部分关键参数就是上面提到的低维的本质模型\",{\"1\":{\"420\":1}}],[\"这部分数据是过时的\",{\"1\":{\"241\":1}}],[\"这部分代码实现如下\",{\"1\":{\"72\":1,\"73\":1}}],[\"这篇论文介绍了gpt\",{\"1\":{\"459\":1}}],[\"这篇论文\",{\"1\":{\"452\":1}}],[\"这篇论文中介绍的方法\",{\"1\":{\"436\":1}}],[\"这篇论文里讲的另一个prompt\",{\"1\":{\"435\":1}}],[\"这篇论文里讲的一个prompt\",{\"1\":{\"434\":1}}],[\"这篇论文首次尝试使用仅支持文本输入的\",{\"1\":{\"224\":1}}],[\"这篇论文提出了一项新的任务和一个配套的数据集\",{\"1\":{\"60\":1}}],[\"这篇论文提出了一种新颖的任务设定\",{\"1\":{\"48\":1}}],[\"这与原始bert的结论相反\",{\"1\":{\"495\":1}}],[\"这与few\",{\"1\":{\"464\":1}}],[\"这与多项式逼近等传统方法有本质区别\",{\"1\":{\"395\":1}}],[\"这与传统的预训练加微调的方法有所不同\",{\"1\":{\"274\":1}}],[\"这与知识蒸馏领域的常见做法一致\",{\"1\":{\"137\":1}}],[\"这与在传统cnn中学习图像局部区域特征的过程相似\",{\"1\":{\"86\":1}}],[\"这就构成了函数与变量的\",{\"1\":{\"656\":1}}],[\"这就定义了随机变量\",{\"1\":{\"565\":1}}],[\"这就产生了一个问题\",{\"1\":{\"396\":1}}],[\"这就实现了\",{\"1\":{\"359\":1}}],[\"这就要求网络中的某些关键操作必须是对称函数\",{\"1\":{\"115\":1}}],[\"这就是梯度下降法\",{\"1\":{\"667\":1}}],[\"这就是为什么我们说样本会集中在距离原点约为\",{\"1\":{\"593\":1}}],[\"这就是贝叶斯法则在离散和连续两种情形下的表达方式\",{\"1\":{\"572\":1}}],[\"这就是答案\",{\"1\":{\"542\":1}}],[\"这就是我们要去学习prompt\",{\"1\":{\"430\":1}}],[\"这就是我们熟悉的\",{\"1\":{\"359\":1}}],[\"这就是blip\",{\"1\":{\"280\":1}}],[\"这就是\",{\"1\":{\"108\":1,\"433\":1,\"565\":1}}],[\"这就是下一个\",{\"1\":{\"92\":2}}],[\"这两种类型分别作为预训练时的输入和输出表示\",{\"1\":{\"168\":1}}],[\"这两种特征被concat为一个复合特征向量\",{\"1\":{\"97\":1}}],[\"这两个数表示\",{\"1\":{\"508\":1}}],[\"这两个模型都属于融合图像与文本的多模态模型\",{\"1\":{\"270\":1}}],[\"这两个网络输出的是变换矩阵\",{\"1\":{\"108\":1}}],[\"这两个操作交替进行\",{\"1\":{\"73\":1}}],[\"这里将数值数据封装在variable中\",{\"1\":{\"667\":1}}],[\"这里试验顺序重要\",{\"1\":{\"579\":1}}],[\"这里我准备做一个文本分类任务\",{\"1\":{\"519\":1}}],[\"这里pad部分指的是对于不同的句子\",{\"1\":{\"514\":1}}],[\"这里也非常像我们人类学习解决复杂问题的过程\",{\"1\":{\"436\":1}}],[\"这里有像我们人类解决问题的过程\",{\"1\":{\"435\":1}}],[\"这里简单推理一下\",{\"1\":{\"426\":1}}],[\"这里简单介绍一下cls\",{\"1\":{\"292\":1}}],[\"这里先简单介绍一下\",{\"1\":{\"421\":1}}],[\"这里面∆w主是我们要微调得到的结果\",{\"1\":{\"420\":1}}],[\"这里是想解决反馈系统的效率问题\",{\"1\":{\"416\":1}}],[\"这里为了方便理解\",{\"1\":{\"414\":1}}],[\"这里把所有\",{\"1\":{\"407\":1}}],[\"这里采用的是\",{\"1\":{\"510\":1}}],[\"这里采用的方法是一种\",{\"1\":{\"404\":1}}],[\"这里采用了余弦相似度的计算方法\",{\"1\":{\"275\":1}}],[\"这里使用符号\",{\"1\":{\"596\":1}}],[\"这里使用的是负的\",{\"1\":{\"407\":1}}],[\"这里使用的是之前讲过的个体判别任务\",{\"1\":{\"238\":1}}],[\"这里使用\",{\"1\":{\"404\":1}}],[\"这里暂时未使用\",{\"1\":{\"402\":1}}],[\"这里解释一下\",{\"1\":{\"326\":1}}],[\"这里不直接使用\",{\"1\":{\"407\":1}}],[\"这里不再贴出\",{\"1\":{\"300\":1}}],[\"这里不过多进行展开\",{\"1\":{\"147\":1}}],[\"这里需要注意一点\",{\"1\":{\"514\":1}}],[\"这里需要激活\",{\"1\":{\"402\":1}}],[\"这里需要将其分离开来\",{\"1\":{\"295\":1}}],[\"这里需要和\",{\"1\":{\"145\":1}}],[\"这里主要有两种位置编码思路\",{\"1\":{\"293\":1}}],[\"这里设置为图像块的大小\",{\"1\":{\"291\":2}}],[\"这里对训练集的处理方式是随机切成224x224像素的图片\",{\"1\":{\"290\":1}}],[\"这里对提取的文本特征和图像特征进行对比学习\",{\"1\":{\"272\":1}}],[\"这里以搜索向日葵花为例\",{\"1\":{\"276\":1}}],[\"这里共有个正样本\",{\"1\":{\"272\":1}}],[\"这里就是\",{\"1\":{\"242\":1}}],[\"这里只取前b个样本\",{\"1\":{\"162\":1}}],[\"这里的例子比较简单\",{\"1\":{\"508\":1}}],[\"这里的关键是在prompt中加入的示例\",{\"1\":{\"434\":1}}],[\"这里的原因\",{\"1\":{\"294\":1}}],[\"这里的相似度直接计算文本特征和图像特征的余弦相似性\",{\"1\":{\"272\":1}}],[\"这里的重点是\",{\"1\":{\"244\":1}}],[\"这里的\",{\"1\":{\"157\":1,\"291\":1}}],[\"这里的三个\",{\"1\":{\"93\":1}}],[\"这里\",{\"1\":{\"154\":1,\"366\":1,\"443\":2,\"565\":1,\"586\":1,\"652\":1}}],[\"这里tgt就是roberta编码得到的文本特征嵌入向量\",{\"1\":{\"76\":1}}],[\"这在模拟某些\",{\"1\":{\"582\":1}}],[\"这在lora这篇论文中也被称为低秩分解自适应技术\",{\"1\":{\"423\":1}}],[\"这在某些场景下会导致位置偏差\",{\"1\":{\"396\":1}}],[\"这在调试\",{\"1\":{\"372\":1}}],[\"这在日志记录\",{\"1\":{\"368\":1}}],[\"这在处理非均匀采样的数据时可能不是最优的选择\",{\"1\":{\"90\":1}}],[\"这在使用\",{\"1\":{\"83\":1}}],[\"这需要一个\",{\"1\":{\"86\":1}}],[\"这样它就可以恢复\",{\"1\":{\"674\":1}}],[\"这样才能确保所有路径的贡献都被纳入最终的梯度值\",{\"1\":{\"654\":1}}],[\"这样在后续的softmax计算中\",{\"1\":{\"517\":1}}],[\"这样在训练中最小化损失就等于最大化重叠度\",{\"1\":{\"405\":1}}],[\"这样难免会影响到最终的结果\",{\"1\":{\"508\":1}}],[\"这样强迫模型在编码当前时刻词的时候不能太依赖当前的词\",{\"1\":{\"505\":1}}],[\"这样不断反复直到遇到终止符\",{\"1\":{\"474\":1}}],[\"这样不就不一致了吗\",{\"1\":{\"238\":1}}],[\"这样llm输出的内容也会更加贴合我们的需求\",{\"1\":{\"432\":1}}],[\"这样一来\",{\"1\":{\"423\":1}}],[\"这样越接近\",{\"1\":{\"403\":1}}],[\"这样计算量就大大减小了\",{\"1\":{\"291\":1}}],[\"这样就将原本不定尺寸的\",{\"1\":{\"396\":1}}],[\"这样就能让\",{\"1\":{\"294\":1,\"544\":1}}],[\"这样就可以完成vit的训练过程\",{\"1\":{\"296\":1}}],[\"这样就可以提取出对最终任务有帮助的特征组合\",{\"1\":{\"294\":1}}],[\"这样就可以直接利用交互层来处理视觉特征\",{\"1\":{\"254\":1,\"257\":1}}],[\"这样就成了一个一维序列\",{\"1\":{\"291\":1}}],[\"这样可以减少内存使用和计算开销\",{\"1\":{\"388\":1}}],[\"这样可以为\",{\"1\":{\"299\":1}}],[\"这样可以在模型的不同阶段交替利用\",{\"1\":{\"299\":1}}],[\"这样可以在每个部分上独立地学习特征\",{\"1\":{\"86\":1}}],[\"这样可以提升计算效率\",{\"1\":{\"295\":1}}],[\"这样可以保证模型的特征提取能力和性能\",{\"1\":{\"290\":1}}],[\"这样字典是高度一致的\",{\"1\":{\"242\":1}}],[\"这样我们的队列就可以设置得很大\",{\"1\":{\"238\":1}}],[\"这样我们会得到两个不太一样的照片\",{\"1\":{\"235\":1}}],[\"这样做其实类似于把骨干网络当成一个特征提取器\",{\"1\":{\"237\":1}}],[\"这样做的好处是\",{\"1\":{\"505\":1}}],[\"这样做的目的是提高模型的效率和泛化能力\",{\"1\":{\"86\":1}}],[\"这样做的原因是因为交互文本由当前图片反映的交互行为和模型额外补充的当前物体存在的其他交互行为构成\",{\"1\":{\"31\":1}}],[\"这样的集合都定义概率\",{\"1\":{\"566\":1}}],[\"这样的\",{\"1\":{\"542\":1}}],[\"这样的大模型\",{\"1\":{\"424\":1}}],[\"这样的大模型中\",{\"1\":{\"409\":1}}],[\"这样的格式来生成文本描述\",{\"1\":{\"274\":1}}],[\"这样的设计使得\",{\"1\":{\"157\":1}}],[\"这样的共享设计能够提升训练效率\",{\"1\":{\"126\":1}}],[\"这样模型就能根据上下文更准确地做出判断\",{\"1\":{\"111\":1}}],[\"这样每个点在预测标签时都能看到整个物体的上下文\",{\"1\":{\"105\":1}}],[\"这样\",{\"1\":{\"95\":2,\"107\":1,\"253\":1,\"291\":1,\"403\":1,\"495\":1,\"660\":1,\"677\":1}}],[\"这种能力可能是通过对代码的训练获得的\",{\"1\":{\"676\":1}}],[\"这种能力允许语言模型在提供自然语言指令或多个任务示例的情况下\",{\"1\":{\"676\":1}}],[\"这种归一化方法可以避免梯度爆炸和消失的问题\",{\"1\":{\"674\":1}}],[\"这种分布之所以受欢迎\",{\"1\":{\"588\":1}}],[\"这种分离的设计让模型更灵活\",{\"1\":{\"313\":1}}],[\"这种含有两个参数的分布比泊松分布更具有建模灵活性\",{\"1\":{\"582\":1}}],[\"这种组合你也得能谈\",{\"1\":{\"566\":1}}],[\"这种情况下\",{\"1\":{\"566\":1,\"657\":1}}],[\"这种不可解释性限制了其在高风险领域的应用\",{\"1\":{\"463\":1}}],[\"这种\",{\"1\":{\"461\":1,\"463\":2,\"660\":1}}],[\"这种性能差异揭示了当前方法的边界\",{\"1\":{\"456\":1}}],[\"这种局限性部分源于单任务\",{\"1\":{\"453\":1}}],[\"这种转置后的布局在缓存访问上可能效率较低\",{\"1\":{\"326\":1}}],[\"这种差异一方面是由于文本和图像属于两个完全不同的模态\",{\"1\":{\"278\":1}}],[\"这种预训练通常是基于有监督学习的\",{\"1\":{\"278\":1}}],[\"这种格式\",{\"1\":{\"274\":1}}],[\"这种模型\",{\"1\":{\"240\":1}}],[\"这种方式在输出为标量的情况下计算效率更高\",{\"1\":{\"626\":1}}],[\"这种方式在训练初期让模型更多依赖于\",{\"1\":{\"159\":1}}],[\"这种方式的问题是\",{\"1\":{\"475\":1}}],[\"这种方式避免了目标检测的步骤\",{\"1\":{\"253\":1}}],[\"这种方法虽然效果强大\",{\"1\":{\"464\":1}}],[\"这种方法无需微调\",{\"1\":{\"454\":1}}],[\"这种方法重新引入了大量特定任务的定制化输入\",{\"1\":{\"445\":1}}],[\"这种方法不需要复制数据\",{\"1\":{\"326\":1}}],[\"这种方法实际上与nlp领域的一个研究方向\",{\"1\":{\"274\":1}}],[\"这种方法能够捕获较为精细的对象信息\",{\"1\":{\"253\":1}}],[\"这种方法大大增强了网络处理非均匀采样数据的能力\",{\"1\":{\"96\":1}}],[\"这种方法使得网络能够在细节丰富的区域\",{\"1\":{\"95\":1}}],[\"这种方法使网络能够通过在训练期间随机丢弃输入点\",{\"1\":{\"95\":1}}],[\"这种伪标签不仅能补充视觉信息中的遗漏\",{\"1\":{\"157\":1}}],[\"这种非均匀性为点集特征学习带来了显著挑战\",{\"1\":{\"94\":1}}],[\"这种类别不平衡\",{\"1\":{\"82\":1}}],[\"这种设计使得网络能够更好地处理深层特征\",{\"1\":{\"392\":1}}],[\"这种设计使得网络只关注\",{\"1\":{\"112\":1}}],[\"这种设计使得语言信息能有效地指导点特征的学习过程\",{\"1\":{\"75\":1}}],[\"这种设计无需依赖具体的可供性分类标签\",{\"1\":{\"15\":1}}],[\"这意味着重复计算\",{\"1\":{\"477\":1}}],[\"这意味着对齐方法不仅优化模型行为\",{\"1\":{\"472\":1}}],[\"这意味着few\",{\"1\":{\"463\":1}}],[\"这意味着它能够在没有任何特定任务训练数据的情况下\",{\"1\":{\"273\":1}}],[\"这意味着即使是不同的局部子集\",{\"1\":{\"86\":1}}],[\"这意味着\",{\"1\":{\"82\":1,\"112\":1,\"351\":1,\"355\":1,\"593\":1,\"676\":1}}],[\"这是由python的运算符调度机制决定的\",{\"1\":{\"660\":1}}],[\"这是视觉场景理解中的一个经典问题\",{\"1\":{\"597\":1}}],[\"这是抽取式问答模型的局限性\",{\"1\":{\"542\":1}}],[\"这是毋庸置疑的\",{\"1\":{\"508\":1}}],[\"这是因为同一个变量对输出的影响路径有多条\",{\"1\":{\"654\":1}}],[\"这是因为样本几乎全部落在离\",{\"1\":{\"594\":1}}],[\"这是因为它们的训练目标是最大化互联网文本的下一个词预测概率\",{\"1\":{\"468\":1}}],[\"这是因为在实际采集过程中\",{\"1\":{\"107\":1}}],[\"这是我们使用llm的人的职责\",{\"1\":{\"430\":1}}],[\"这是交叉熵损失\",{\"1\":{\"404\":1}}],[\"这是它最大的优势\",{\"1\":{\"385\":1}}],[\"这是通过\",{\"1\":{\"325\":1}}],[\"这是注意力权重矩阵的来源\",{\"1\":{\"307\":1}}],[\"这是一项本质上的进步\",{\"1\":{\"677\":1}}],[\"这是一种全新的\",{\"1\":{\"677\":1}}],[\"这是一种数据增强的方式\",{\"1\":{\"290\":1}}],[\"这是一款在推理和通用任务上有显著提升的模型\",{\"1\":{\"674\":1}}],[\"这是一个一举多得\",{\"1\":{\"677\":1}}],[\"这是一个根本上的病态问题\",{\"1\":{\"597\":1}}],[\"这是一个椅子\",{\"1\":{\"111\":1}}],[\"这是为了让字典里的特征尽可能保持一致\",{\"1\":{\"237\":1}}],[\"这是为了每个\",{\"1\":{\"143\":1}}],[\"这是首次尝试将大语言模型用于模型集成\",{\"1\":{\"228\":1}}],[\"这是实现\",{\"1\":{\"227\":1}}],[\"这是后续指令调优的基础\",{\"1\":{\"225\":1}}],[\"这是第一个系统性地将\",{\"1\":{\"224\":1}}],[\"这是主流做法\",{\"1\":{\"191\":1}}],[\"这是\",{\"1\":{\"76\":1,\"92\":1,\"163\":1,\"224\":1,\"285\":1,\"434\":1,\"435\":1,\"436\":1}}],[\"这是训练中未曾遇到过的功能\",{\"1\":{\"65\":1}}],[\"这个除数被称为边际似然\",{\"1\":{\"596\":1}}],[\"这个分布的一个显著特点是\",{\"1\":{\"587\":1}}],[\"这个分布可以被看作是标准正态分布\",{\"1\":{\"585\":1}}],[\"这个结论来自于\",{\"1\":{\"567\":1}}],[\"这个公式可以由以下恒等式直接推出\",{\"1\":{\"596\":1}}],[\"这个公式适用于任意两个事件\",{\"1\":{\"567\":1}}],[\"这个公式对应的是\",{\"1\":{\"567\":1}}],[\"这个公式其实可以直接用在对比学习中\",{\"1\":{\"240\":1}}],[\"这个事件也应该存在\",{\"1\":{\"566\":1}}],[\"这个事件存在\",{\"1\":{\"566\":1}}],[\"这个事情的性价比非常低\",{\"1\":{\"415\":1}}],[\"这个规则体系就是σ\",{\"1\":{\"566\":1}}],[\"这个数据集来源是这里\",{\"1\":{\"519\":1}}],[\"这个数据集中剩余的所有图片都是负样本\",{\"1\":{\"235\":1}}],[\"这个问题的答案\",{\"1\":{\"508\":1}}],[\"这个阶段的prompt中包含三部分内容\",{\"1\":{\"436\":1}}],[\"这个阶段的prompt中要包含分解问题的示例\",{\"1\":{\"436\":1}}],[\"这个阶段相当于在语言模型的词空间中\",{\"1\":{\"226\":1}}],[\"这个词\",{\"1\":{\"414\":1}}],[\"这个表达式其实是通过\",{\"1\":{\"404\":1}}],[\"这个衰减因子能够使得易分类的样本\",{\"1\":{\"404\":1}}],[\"这个张量会成为模型的一个成员\",{\"1\":{\"389\":1}}],[\"这个新维度就是拼接的那一维\",{\"1\":{\"381\":1}}],[\"这个新内存布局使得每一行是连续的\",{\"1\":{\"326\":1}}],[\"这个矩阵是\",{\"1\":{\"355\":1}}],[\"这个矩阵就叫\",{\"1\":{\"355\":1}}],[\"这个矩阵表示的是每个\",{\"1\":{\"307\":1}}],[\"这个矩阵表示对点云所做的变换\",{\"1\":{\"107\":1}}],[\"这个维度上添加一维\",{\"1\":{\"293\":1}}],[\"这个就是额外添加的一个\",{\"1\":{\"292\":1}}],[\"这个token\",{\"1\":{\"292\":1}}],[\"这个输出向量会通过其\",{\"1\":{\"247\":1}}],[\"这个增强方法\",{\"1\":{\"244\":1}}],[\"这个值\",{\"1\":{\"241\":1}}],[\"这个时候\",{\"1\":{\"240\":1}}],[\"这个是softmax的公式\",{\"1\":{\"240\":1}}],[\"这个动态的字典分为两个部分\",{\"1\":{\"237\":1}}],[\"这个名字就是来源于前两个单词的前两个字母\",{\"1\":{\"236\":1}}],[\"这个粒度其实是很细\",{\"1\":{\"235\":1}}],[\"这个代理任务\",{\"1\":{\"235\":1}}],[\"这个代理任务是指\",{\"1\":{\"235\":1}}],[\"这个变换矩阵是近似正交的\",{\"1\":{\"107\":1}}],[\"这个模型使用了\",{\"1\":{\"96\":1}}],[\"这个模块实现了\",{\"1\":{\"96\":1}}],[\"这个特征向量代表了这个局部区域的高维特征\",{\"1\":{\"92\":1}}],[\"这个操作被称为\",{\"1\":{\"92\":1}}],[\"这个函数的有一个输入参数\",{\"1\":{\"552\":1}}],[\"这个函数的作用是将输入的文本转化为对应的嵌入表示\",{\"1\":{\"275\":1}}],[\"这个函数的作用是从点云中找出每个查询点周围一定半径范围内的邻近点索引\",{\"1\":{\"92\":1}}],[\"这个函数的作用是从输入点云中\",{\"1\":{\"92\":1}}],[\"这个函数实现的是根据给定的索引\",{\"1\":{\"92\":1}}],[\"这个函数实现的是最远点采样\",{\"1\":{\"92\":1}}],[\"这个过程有点像\",{\"1\":{\"434\":1}}],[\"这个过程其实也是自监督训练的一个过程\",{\"1\":{\"234\":1}}],[\"这个过程类似于在传统的卷积神经网络中如何处理图像的小区域\",{\"1\":{\"86\":1}}],[\"这个过程发生在数据集准备阶段\",{\"1\":{\"28\":1}}],[\"这个\",{\"1\":{\"74\":1,\"275\":1}}],[\"这些涌现能力让\",{\"1\":{\"676\":1}}],[\"这些特点使\",{\"1\":{\"675\":1}}],[\"这些特点使它们在自然语言处理和其他领域中引起了广泛的兴趣和研究\",{\"1\":{\"675\":1}}],[\"这些特征\",{\"1\":{\"241\":1}}],[\"这些函数被称为测试函数\",{\"1\":{\"662\":1}}],[\"这些都属于\",{\"1\":{\"566\":1}}],[\"这些位置的概率就会接近0\",{\"1\":{\"517\":1}}],[\"这些是\",{\"1\":{\"511\":1}}],[\"这些词汇是从wikipedia的优质文章和标杆文章中提取得到\",{\"1\":{\"510\":1}}],[\"这些改进通常依赖于更大模型和更多数据\",{\"1\":{\"501\":1}}],[\"这些改进共同构成了roberta的核心优化策略\",{\"1\":{\"495\":1}}],[\"这些改进包括更长的训练时间\",{\"1\":{\"491\":1}}],[\"这些改写使得在预训练模型架构上用最小的修改就会有效\",{\"1\":{\"440\":1}}],[\"这些研究强调\",{\"1\":{\"469\":1}}],[\"这些限制提示我们\",{\"1\":{\"463\":1}}],[\"这些任务测试了gpt\",{\"1\":{\"462\":1}}],[\"这些发现为后续gpt\",{\"1\":{\"456\":1}}],[\"这些实验为后续研究\",{\"1\":{\"455\":1}}],[\"这些实验表明\",{\"1\":{\"229\":1}}],[\"这些设计使\",{\"1\":{\"454\":1}}],[\"这些系统往往对数据分布或任务定义的微小变化非常敏感\",{\"1\":{\"453\":1}}],[\"这些输入转换使作者避免跨任务架构的大改\",{\"1\":{\"445\":1}}],[\"这些参数会以随机梯度下降训练\",{\"1\":{\"443\":1}}],[\"这些不确定性使得开发有效的语言处理半监督学习方法变得困难\",{\"1\":{\"440\":1}}],[\"这些区域的大小各不相同\",{\"1\":{\"396\":1}}],[\"这些变量依然存在\",{\"1\":{\"366\":1}}],[\"这些地址从\",{\"1\":{\"321\":1}}],[\"这些值会根据模型的损失函数不断调整\",{\"1\":{\"293\":1}}],[\"这些相似度值可以被视为logits\",{\"1\":{\"273\":1}}],[\"这些图像特征会与之前得到的个文本特征进行余弦相似度计算\",{\"1\":{\"273\":1}}],[\"这些文本随后被输入到文本编码器\",{\"1\":{\"273\":1}}],[\"这些数据集包括\",{\"1\":{\"674\":1}}],[\"这些数据在论文中被称为webimagetext\",{\"1\":{\"272\":1}}],[\"这些数据由\",{\"1\":{\"227\":1}}],[\"这些规则可以去定义哪些图片是相似的\",{\"1\":{\"234\":1}}],[\"这些代理任务\",{\"1\":{\"234\":1}}],[\"这些指令可以是开放式的\",{\"1\":{\"224\":1}}],[\"这些指标共同构成了\",{\"1\":{\"82\":1}}],[\"这些\",{\"1\":{\"215\":1,\"505\":1}}],[\"这些模型都是以cnn为基础\",{\"1\":{\"547\":1}}],[\"这些模型的发布旨在促进研究社区的开放访问和研究\",{\"1\":{\"479\":1}}],[\"这些模型的优势在于大规模参数\",{\"1\":{\"210\":1}}],[\"这些模型多基于纯视觉数据\",{\"1\":{\"183\":1}}],[\"这些工作主要依赖轻量级适配层\",{\"1\":{\"185\":1}}],[\"这些点大致构成物体的骨架\",{\"1\":{\"112\":1}}],[\"这些点彼此之间的最小距离尽可能大\",{\"1\":{\"89\":1}}],[\"这些方法为\",{\"1\":{\"469\":1}}],[\"这些方法在数据需求和任务适应能力之间形成一个光谱\",{\"1\":{\"461\":1}}],[\"这些方法仍依赖监督数据\",{\"1\":{\"453\":1}}],[\"这些方法仍受限于训练语义空间\",{\"1\":{\"7\":1}}],[\"这些方法都有各自的特点\",{\"1\":{\"424\":1}}],[\"这些方法使用深层transformer进行交互作用\",{\"1\":{\"255\":1}}],[\"这些方法会导致信息损失\",{\"1\":{\"103\":1}}],[\"这些人离我太远了\",{\"1\":{\"92\":1}}],[\"这些功能标签仅用于构造问题和定位正确功能区域\",{\"1\":{\"64\":1}}],[\"这些功能标注是人工标注的\",{\"1\":{\"62\":1}}],[\"这一里程碑式的更新\",{\"1\":{\"684\":1}}],[\"这一阶段\",{\"1\":{\"663\":1}}],[\"这一扩展使我们的函数定义更接近\",{\"1\":{\"651\":1}}],[\"这一策略虽限制数据量\",{\"1\":{\"480\":1}}],[\"这一设计理念直接反映在模型架构选择上\",{\"1\":{\"480\":1}}],[\"这一方法起初应用于强化学习场景\",{\"1\":{\"469\":1}}],[\"这一研究为探索语言模型的元学习机制和实际应用奠定了基础\",{\"1\":{\"460\":1}}],[\"这一研究为功能学习领域提供了新的视角\",{\"1\":{\"48\":1}}],[\"这一发现标志着大型语言模型\",{\"1\":{\"673\":1}}],[\"这一发现挑战了单纯追求参数规模的范式\",{\"1\":{\"480\":1}}],[\"这一发现表明\",{\"1\":{\"457\":1}}],[\"这一发现为理解当前预训练模型的有效性提供了新视角\",{\"1\":{\"456\":1}}],[\"这一发现为构建通用语言系统提供了新方向\",{\"1\":{\"453\":1}}],[\"这一框架允许模型不仅生成文本\",{\"1\":{\"454\":1}}],[\"这一块的技巧性很强\",{\"1\":{\"433\":1}}],[\"这一条件\",{\"1\":{\"395\":1}}],[\"这一部分的目标是\",{\"1\":{\"285\":1}}],[\"这一过程对大规模计算框架\",{\"1\":{\"657\":1}}],[\"这一过程与训练时相同\",{\"1\":{\"273\":1}}],[\"这一过程通过对每个子区域应用集合抽象层\",{\"1\":{\"97\":1}}],[\"这一成果为开源多模态模型的发展提供了重要支持\",{\"1\":{\"207\":1}}],[\"这一渐进式策略确保模型\",{\"1\":{\"190\":1}}],[\"这一步很关键\",{\"1\":{\"372\":1}}],[\"这一步的操作在论文中是直接采用切割的处理办法\",{\"1\":{\"291\":1}}],[\"这一步是为了保证图像的整体比例不变\",{\"1\":{\"290\":1}}],[\"这一步不计算梯度\",{\"1\":{\"163\":1}}],[\"这一步不在\",{\"1\":{\"107\":1}}],[\"这一步相当于图像任务中的\",{\"1\":{\"100\":1}}],[\"这一最具挑战性的设置下\",{\"1\":{\"23\":1}}],[\"这类抽取式问答任务中\",{\"1\":{\"542\":1}}],[\"这类指令\",{\"1\":{\"471\":1}}],[\"这类\",{\"1\":{\"23\":1,\"390\":1}}],[\"激活参数\",{\"1\":{\"674\":1}}],[\"激活检查点\",{\"1\":{\"481\":1}}],[\"激活模型的相关能力\",{\"1\":{\"454\":1}}],[\"激活后\",{\"1\":{\"332\":1}}],[\"激活环境的命令\",{\"1\":{\"332\":1}}],[\"激活已有知识\",{\"1\":{\"231\":1}}],[\"激活函数层\",{\"1\":{\"294\":1}}],[\"激活函数\",{\"1\":{\"98\":1,\"294\":1,\"296\":2,\"405\":1,\"447\":1,\"481\":1,\"513\":2,\"525\":2,\"674\":2}}],[\"激活交互主体所在区域\",{\"1\":{\"59\":1}}],[\"激活\",{\"0\":{\"332\":1},\"1\":{\"15\":1,\"268\":1,\"674\":1}}],[\"融合了\",{\"1\":{\"674\":1}}],[\"融合模块\",{\"1\":{\"280\":2}}],[\"融合过程在多模态编码器的每一层中通过跨模态注意力\",{\"1\":{\"152\":1}}],[\"融合两者优势\",{\"1\":{\"150\":1}}],[\"融合特征\",{\"1\":{\"147\":1}}],[\"融合+增强\",{\"1\":{\"100\":1}}],[\"融合多个\",{\"1\":{\"76\":1}}],[\"融合多模态空间特征\",{\"0\":{\"41\":1},\"1\":{\"40\":1}}],[\"融合所有\",{\"1\":{\"76\":1}}],[\"融合后的文本特征\",{\"1\":{\"74\":1}}],[\"融合生成功能表征\",{\"1\":{\"54\":1}}],[\"融合不同来源的信息\",{\"1\":{\"45\":1}}],[\"融合通道信息\",{\"1\":{\"45\":1}}],[\"融合\",{\"1\":{\"40\":1}}],[\"融合语言与视觉特征\",{\"1\":{\"40\":1}}],[\"融合表示为\",{\"1\":{\"14\":1}}],[\"融合至\",{\"1\":{\"14\":1}}],[\"卷积后剩余的长和宽相乘作为时间维度\",{\"1\":{\"291\":1}}],[\"卷积核个数为768的卷积层来进行实现\",{\"1\":{\"291\":1}}],[\"卷积\",{\"1\":{\"14\":1,\"73\":1,\"107\":1}}],[\"跨度预测和自回归预训练\",{\"1\":{\"501\":1}}],[\"跨领域泛化能力\",{\"1\":{\"455\":1}}],[\"跨gpu部分实现\",{\"1\":{\"145\":1}}],[\"跨模态大模型\",{\"1\":{\"674\":1}}],[\"跨模态注意力\",{\"1\":{\"264\":1}}],[\"跨模态注意力机制\",{\"1\":{\"76\":1}}],[\"跨模态注意力矩阵\",{\"1\":{\"41\":1}}],[\"跨模态融合\",{\"1\":{\"6\":1}}],[\"跨注意力融合公式为\",{\"1\":{\"14\":1}}],[\"kru13\",{\"1\":{\"586\":1}}],[\"kolmogorov\",{\"1\":{\"567\":1}}],[\"korthikanti\",{\"1\":{\"481\":1}}],[\"kaplan\",{\"1\":{\"485\":1}}],[\"karma\",{\"1\":{\"454\":1}}],[\"karpathy\",{\"1\":{\"142\":1}}],[\"kv\",{\"0\":{\"473\":1,\"474\":1},\"1\":{\"474\":1,\"477\":11,\"674\":1}}],[\"k通常为10\",{\"1\":{\"461\":1}}],[\"k采样\",{\"1\":{\"455\":1}}],[\"kg\",{\"1\":{\"355\":1}}],[\"k矩阵乘积\",{\"1\":{\"298\":1}}],[\"kullback\",{\"1\":{\"157\":1}}],[\"kimi\",{\"1\":{\"673\":1}}],[\"kim\",{\"1\":{\"126\":1}}],[\"k=int\",{\"1\":{\"289\":1}}],[\"k=images\",{\"1\":{\"244\":1}}],[\"k=2\",{\"1\":{\"110\":1,\"455\":1}}],[\"k=64\",{\"1\":{\"109\":1}}],[\"k×k\",{\"1\":{\"108\":1}}],[\"k\",{\"0\":{\"306\":1},\"1\":{\"72\":3,\"74\":3,\"76\":2,\"96\":3,\"105\":1,\"110\":1,\"111\":11,\"112\":5,\"161\":4,\"163\":1,\"244\":3,\"246\":10,\"247\":13,\"248\":4,\"249\":3,\"282\":1,\"289\":2,\"295\":7,\"304\":1,\"310\":1,\"319\":2,\"323\":1,\"390\":2,\"470\":2,\"475\":1,\"477\":2,\"511\":2,\"517\":4,\"531\":2,\"558\":9,\"576\":1}}],[\"kwargs=cache\",{\"1\":{\"477\":1}}],[\"kwargs说明\",{\"1\":{\"477\":1}}],[\"kwargs\",{\"1\":{\"68\":1,\"73\":1,\"143\":3,\"286\":2,\"367\":2,\"370\":3,\"371\":2,\"372\":4,\"373\":2,\"377\":2,\"379\":7,\"477\":2}}],[\"kl\",{\"1\":{\"59\":2,\"157\":1,\"161\":1,\"163\":2,\"469\":1,\"470\":2}}],[\"kl散度约束\",{\"1\":{\"55\":1}}],[\"knn查询寻找最近的k个邻居\",{\"1\":{\"90\":1}}],[\"knn\",{\"1\":{\"90\":1}}],[\"knife\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"knowledge\",{\"0\":{\"13\":1,\"123\":1},\"1\":{\"28\":2,\"32\":2,\"35\":1,\"150\":1}}],[\"keep\",{\"1\":{\"543\":1}}],[\"keepdims=true\",{\"1\":{\"275\":1,\"277\":1}}],[\"keepdim=true\",{\"1\":{\"100\":1,\"107\":1,\"109\":1,\"145\":1,\"273\":2,\"474\":1,\"477\":1}}],[\"kernel\",{\"1\":{\"76\":1,\"107\":1,\"291\":2}}],[\"kernels\",{\"1\":{\"70\":1,\"76\":2}}],[\"kettle\",{\"1\":{\"24\":1,\"28\":12,\"29\":1,\"31\":6}}],[\"keyword\",{\"1\":{\"363\":1}}],[\"key来自query\",{\"1\":{\"285\":1}}],[\"key=pairs\",{\"1\":{\"410\":1,\"411\":1,\"412\":1}}],[\"key=value\",{\"1\":{\"363\":1}}],[\"key=self\",{\"1\":{\"76\":1}}],[\"key=gt\",{\"1\":{\"75\":1}}],[\"key=x\",{\"1\":{\"75\":1}}],[\"key=lambda\",{\"1\":{\"68\":1,\"410\":3,\"412\":2,\"656\":1,\"658\":1,\"666\":1}}],[\"key和value为融合后的文本特征\",{\"1\":{\"74\":1}}],[\"key和value都是点云特征\",{\"1\":{\"72\":1}}],[\"keys\",{\"1\":{\"29\":1,\"58\":1,\"246\":1,\"247\":2,\"249\":7,\"410\":1,\"412\":1,\"477\":2}}],[\"keyboard\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"key\",{\"1\":{\"14\":1,\"32\":8,\"45\":19,\"63\":1,\"68\":4,\"70\":1,\"72\":8,\"74\":11,\"75\":1,\"76\":7,\"145\":1,\"162\":6,\"238\":1,\"242\":1,\"244\":2,\"246\":2,\"247\":6,\"248\":3,\"249\":2,\"264\":1,\"266\":5,\"267\":2,\"284\":3,\"285\":44,\"289\":2,\"304\":1,\"305\":2,\"307\":1,\"310\":1,\"312\":1,\"314\":1,\"477\":43,\"531\":7,\"558\":7,\"674\":2}}],[\"将不断从业务逻辑中收集当下\",{\"1\":{\"686\":1}}],[\"将组件组合实现端到端应用\",{\"1\":{\"683\":1}}],[\"将增强后的信息输入到生成模型中\",{\"1\":{\"680\":1}}],[\"将用户的问题输入到检索系统中\",{\"1\":{\"680\":1}}],[\"将处理后的数据存储在对应的数据库中\",{\"1\":{\"680\":1}}],[\"将处理后的数据转化为检索模型可以使用的格式\",{\"1\":{\"680\":1}}],[\"将生成内容与检索到的原始资料建立链接\",{\"1\":{\"679\":1}}],[\"将生成模型分配的平均对数概率高的token作为答案\",{\"1\":{\"449\":1}}],[\"将上下文长度大幅提升至\",{\"1\":{\"674\":1}}],[\"将复杂问题分解为可管理的子问题\",{\"1\":{\"674\":1}}],[\"将计算结果绘制在图上\",{\"1\":{\"667\":1}}],[\"将普通数值计算转换为可微分计算\",{\"1\":{\"662\":1}}],[\"将other参数统一转换为ndarray\",{\"1\":{\"660\":1}}],[\"将ndarray或数值转换为variable\",{\"1\":{\"660\":1}}],[\"将非variable对象转换为variable实例\",{\"1\":{\"660\":1}}],[\"将梯度分别乘以x1和x0\",{\"1\":{\"660\":1}}],[\"将function对variable的引用改为弱引用\",{\"1\":{\"657\":1}}],[\"将强引用改为弱引用\",{\"1\":{\"657\":1}}],[\"将蜕变为一个更通用\",{\"1\":{\"650\":1}}],[\"将数值微分的结果与反向传播的结果进行比较\",{\"1\":{\"646\":1}}],[\"将数据集压缩包下载到dataset目录下\",{\"1\":{\"510\":1}}],[\"将数据下载到当前项目目录下\",{\"1\":{\"275\":1}}],[\"将函数类封装为python函数\",{\"1\":{\"614\":1,\"641\":1}}],[\"将先验概率\",{\"1\":{\"596\":1}}],[\"将选项展平\",{\"1\":{\"544\":1}}],[\"将所有输入序列填充到等长max\",{\"1\":{\"512\":1}}],[\"将所有文本合并成一个字符串\",{\"1\":{\"511\":1}}],[\"将一整段文本按\",{\"1\":{\"510\":1}}],[\"将一篇文章\",{\"1\":{\"508\":1}}],[\"将一个形状为\",{\"1\":{\"326\":1}}],[\"将一个批次的数据拆分为图像和标签两个元组\",{\"1\":{\"289\":1}}],[\"将句子中各个字对应位置的\",{\"1\":{\"508\":1}}],[\"将训练数据复制\",{\"1\":{\"495\":1}}],[\"将训练语料中的每个单词按字符拆分\",{\"1\":{\"410\":1}}],[\"将kv\",{\"1\":{\"477\":1}}],[\"将抽象对齐技术成功应用于现实世界模型部署\",{\"1\":{\"472\":1}}],[\"将预训练语言模型从\",{\"1\":{\"464\":1}}],[\"将预训练模型的参数\",{\"1\":{\"423\":1}}],[\"将文档和问题跟每个可能答案拼接起来\",{\"1\":{\"445\":1}}],[\"将文本和query\",{\"1\":{\"284\":1}}],[\"将文本看成一个词序列\",{\"1\":{\"257\":1}}],[\"将文本特征作为查询\",{\"1\":{\"72\":1}}],[\"将文本语义信息与点云特征进行跨模态融合\",{\"1\":{\"71\":1}}],[\"将文本切分为多个句子\",{\"1\":{\"31\":1}}],[\"将原始的\",{\"1\":{\"423\":1}}],[\"将原始点云\",{\"1\":{\"109\":1}}],[\"将原本用16bit表示的参数\",{\"1\":{\"421\":1}}],[\"将∆w进行低维分解∆w=ab\",{\"1\":{\"420\":1}}],[\"将y=wx变成y=\",{\"1\":{\"420\":1}}],[\"将w变成\",{\"1\":{\"416\":1}}],[\"将传入的最高频字符对中的两个字符用空格拼接起来\",{\"1\":{\"410\":1}}],[\"将样本的权重进行动态调整\",{\"1\":{\"404\":1}}],[\"将该梯度向量与初始误差向量相乘\",{\"1\":{\"406\":1}}],[\"将该\",{\"1\":{\"396\":1}}],[\"将与较差的指标相似\",{\"1\":{\"348\":1}}],[\"将模型放入到仓库对应位置\",{\"1\":{\"519\":1}}],[\"将模型以元组形式返回的缓存重新封装为legacy\",{\"1\":{\"477\":1}}],[\"将模型的预测作为行\",{\"1\":{\"342\":1}}],[\"将模型层数从48层缩减至45层\",{\"1\":{\"215\":1}}],[\"将维度大小为\",{\"1\":{\"327\":1}}],[\"将多个注意力头的输出合并为一个张量\",{\"1\":{\"295\":1}}],[\"将多头注意力的输出进行线性变换\",{\"1\":{\"295\":1}}],[\"将注意力权重矩阵与v相乘\",{\"1\":{\"295\":1}}],[\"将q和k的转置相乘\",{\"1\":{\"295\":1}}],[\"将隐藏特征映射到输出特征空间\",{\"1\":{\"294\":1}}],[\"将分类标记和图像块嵌入拼接\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"将卷积后的通道维数作为embedding的维度\",{\"1\":{\"291\":1}}],[\"将裁剪后的图像调整为\",{\"1\":{\"290\":1}}],[\"将标签元组转换为一个一维张量\",{\"1\":{\"289\":1}}],[\"将横坐标0\",{\"1\":{\"289\":1}}],[\"将缓存的\",{\"1\":{\"285\":1}}],[\"将第一个\",{\"1\":{\"285\":1}}],[\"将会在下文进行详细讲解\",{\"1\":{\"284\":1}}],[\"将inputfeatures\",{\"1\":{\"520\":1}}],[\"将input\",{\"1\":{\"282\":1}}],[\"将图片编码成\",{\"1\":{\"282\":1}}],[\"将图像数据移动到指定设备上\",{\"1\":{\"296\":1}}],[\"将图像的短边缩放为\",{\"1\":{\"290\":1}}],[\"将图像元组堆叠成一个四维张量\",{\"1\":{\"289\":1}}],[\"将图像切块看成一个图像块序列\",{\"1\":{\"257\":1}}],[\"将图像划分为固定大小的网格\",{\"1\":{\"253\":1}}],[\"将图像特征注入到文本流中\",{\"1\":{\"263\":1}}],[\"将图像特征\",{\"1\":{\"227\":1}}],[\"将图像分割为\",{\"1\":{\"214\":1}}],[\"将图像分割为1至40个448×448像素的区块\",{\"1\":{\"208\":1}}],[\"将图像压缩为的8192维视觉词表\",{\"1\":{\"178\":1}}],[\"将图像表示为离散标记序列\",{\"1\":{\"170\":1}}],[\"将图像patch和点云点拼接成一个统一的token序列\",{\"1\":{\"41\":1}}],[\"将图像+点云特征插入语言嵌入中\",{\"1\":{\"40\":1}}],[\"将这个问题转化为一个多标签分类任务\",{\"1\":{\"278\":1}}],[\"将这些掩码token对应的嵌入向量映射到词向量空间中去\",{\"1\":{\"513\":1}}],[\"将这些排序用作训练奖励模型\",{\"1\":{\"470\":1}}],[\"将这些图文对转换为如下格式\",{\"1\":{\"226\":1}}],[\"将这些特征映射到类别空间\",{\"1\":{\"110\":2}}],[\"将这些知识注入点云特征并与图像特征融合\",{\"1\":{\"8\":1}}],[\"将这些知识与点云和图像特征结合\",{\"1\":{\"5\":1}}],[\"将待分类的图像输入到图像编码器\",{\"1\":{\"273\":1}}],[\"将个文本特征和个图像特征两两组合\",{\"1\":{\"272\":1}}],[\"将问题转化为一个二分类问题\",{\"1\":{\"240\":1}}],[\"将最有用的信息提供给\",{\"1\":{\"286\":1}}],[\"将最老的\",{\"1\":{\"238\":1}}],[\"将最后一层\",{\"1\":{\"98\":1}}],[\"将英文数据转为中文\",{\"1\":{\"217\":1}}],[\"将高分辨率图像分割为多个低分辨率区块处理\",{\"1\":{\"211\":1}}],[\"将其变为归一化分布\",{\"1\":{\"596\":1}}],[\"将其喂进一个参数为\",{\"1\":{\"444\":1}}],[\"将其重塑为一个\",{\"1\":{\"323\":1}}],[\"将其展平就变成了一个长度为768的向量\",{\"1\":{\"291\":1}}],[\"将其动态分割为\",{\"1\":{\"207\":1}}],[\"将其编码为上下文相关的向量表示\",{\"1\":{\"167\":1}}],[\"将其代入标准的对比学习\",{\"1\":{\"157\":1}}],[\"将主编码器和动量编码器配对\",{\"1\":{\"160\":1}}],[\"将主模型和动量模型参数组织成配对\",{\"1\":{\"147\":1}}],[\"将空间划分成立方体格子\",{\"1\":{\"114\":1}}],[\"将每个结果\",{\"1\":{\"565\":1}}],[\"将每个词从str转换为list列表形式\",{\"1\":{\"411\":1}}],[\"将每个\",{\"1\":{\"169\":1,\"540\":1}}],[\"将每个点的局部特征与整个点云的全局特征拼接起来\",{\"1\":{\"109\":1}}],[\"将每个样本属于的物体类型\",{\"1\":{\"29\":1}}],[\"将全局特征与每个点的局部特征拼接起来\",{\"1\":{\"105\":1}}],[\"将全局语义向量扩展回原始点云数量\",{\"1\":{\"46\":1}}],[\"将点云转换为体素网格\",{\"1\":{\"103\":1}}],[\"将点云投影到图像平面\",{\"1\":{\"52\":1}}],[\"将之前计算好的权重扩展维度\",{\"1\":{\"100\":1}}],[\"将距离转换为\",{\"1\":{\"100\":1}}],[\"将坐标和特征从\",{\"1\":{\"100\":1}}],[\"将稀疏点集points2插值到密集点集xyz1的位置上\",{\"1\":{\"100\":1}}],[\"将稀疏点集的特征插值回原始点集的位置上\",{\"1\":{\"100\":1}}],[\"将邻域点组合成局部点云组\",{\"1\":{\"98\":1}}],[\"将来自下一级\",{\"1\":{\"97\":1}}],[\"将view\",{\"1\":{\"92\":1}}],[\"将当前词的key和val进行缓存\",{\"1\":{\"477\":1}}],[\"将当前词列表中每个子词映射为字典中对于的词id\",{\"1\":{\"411\":2}}],[\"将当前动量特征送入队列\",{\"1\":{\"161\":1}}],[\"将当前选中的\",{\"1\":{\"92\":1}}],[\"将当前样本的物体信息值追加到对应列表中\",{\"1\":{\"68\":1}}],[\"将转换后的坐标以及点的附加特征\",{\"1\":{\"91\":1}}],[\"将两个方向的\",{\"1\":{\"78\":1}}],[\"将两个注意力输出拼接在一起\",{\"1\":{\"45\":1}}],[\"将响应值映射到\",{\"1\":{\"76\":1}}],[\"将融合特征重新分配给每个点\",{\"1\":{\"74\":2}}],[\"将融合特征映射回点空间\",{\"0\":{\"74\":1}}],[\"将融合后的\",{\"1\":{\"42\":1}}],[\"将融合后的空间特征通过适配器上采样到与语言模型匹配的维度\",{\"1\":{\"40\":1}}],[\"将异构特征\",{\"1\":{\"59\":1}}],[\"将交互主体区域框在特征图中框出的区域\",{\"1\":{\"59\":1}}],[\"将交互主体框和目标物体框等比例缩小\",{\"1\":{\"59\":1}}],[\"将目标物体区域框在特征图中框出的区域\",{\"1\":{\"59\":1}}],[\"将掩码图像中目标物体所在区域激活\",{\"1\":{\"59\":1}}],[\"将特征映射到\",{\"1\":{\"46\":1}}],[\"将输入序列\",{\"1\":{\"548\":1}}],[\"将输入和目标张量展平为一维\",{\"1\":{\"405\":1,\"407\":1}}],[\"将输入和目标展平成一维张量便于计算\",{\"1\":{\"403\":1}}],[\"将输入和目标展平成一维张量\",{\"1\":{\"402\":1}}],[\"将输入展平便于后续计算\",{\"1\":{\"404\":1}}],[\"将输入展平成一维张量\",{\"1\":{\"401\":1}}],[\"将输入映射到\",{\"1\":{\"403\":1}}],[\"将输入映射到概率空间\",{\"1\":{\"402\":1}}],[\"将输入映射到低维空间以进行\",{\"1\":{\"45\":1}}],[\"将输入直接连接到输出\",{\"1\":{\"392\":1}}],[\"将输入特征映射到隐藏特征空间\",{\"1\":{\"294\":1}}],[\"将输入图像进行图像块嵌入\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"将输入图像编码为一系列嵌入向量\",{\"1\":{\"152\":1}}],[\"将输入图片\",{\"1\":{\"291\":1}}],[\"将输入文本转换为嵌入列表后和query\",{\"1\":{\"284\":1}}],[\"将输入的维度dim映射到dim\",{\"1\":{\"295\":1}}],[\"将输入的\",{\"1\":{\"282\":1}}],[\"将输入分别投影到低维空间\",{\"1\":{\"45\":1}}],[\"将语言嵌入\",{\"1\":{\"43\":1}}],[\"将\",{\"1\":{\"14\":1,\"15\":1,\"43\":1,\"44\":1,\"63\":1,\"82\":1,\"169\":1,\"226\":1,\"228\":1,\"268\":1,\"285\":2,\"290\":2,\"291\":1,\"299\":1,\"327\":1,\"396\":1,\"397\":1,\"401\":1,\"405\":1,\"428\":1,\"469\":2,\"472\":1,\"674\":1}}],[\"从deepseek\",{\"1\":{\"688\":1}}],[\"从构建一个\",{\"1\":{\"687\":1}}],[\"从实际业务需求出发构造小批量验证集\",{\"1\":{\"686\":1}}],[\"从实验结果来看\",{\"1\":{\"237\":1}}],[\"从一个模块流向另一个模块\",{\"1\":{\"682\":1}}],[\"从简单函数组合进化到模块化神经网络\",{\"1\":{\"670\":1}}],[\"从输出信息可看到\",{\"1\":{\"667\":1}}],[\"从输出变量出发遍历所有节点\",{\"1\":{\"666\":1}}],[\"从输入点云\",{\"1\":{\"92\":1}}],[\"从输入点中选取一组点\",{\"1\":{\"88\":1}}],[\"从自动微分迈向可训练的神经网络模型\",{\"0\":{\"670\":1}}],[\"从自动微分迈向通用框架\",{\"0\":{\"650\":1}}],[\"从自动微分走向\",{\"0\":{\"665\":1}}],[\"从弱引用中获取variable实例\",{\"1\":{\"657\":1}}],[\"从递归改为循环\",{\"1\":{\"648\":1}}],[\"从递归到循环\",{\"0\":{\"636\":1}}],[\"从零构建深度学习框架\",{\"0\":{\"603\":1,\"649\":1,\"664\":1,\"669\":1}}],[\"从样本数据出发\",{\"1\":{\"596\":1}}],[\"从样本空间\",{\"1\":{\"565\":1}}],[\"从中可以计算区间概率\",{\"1\":{\"566\":1}}],[\"从源张量中提取特定位置的元素\",{\"1\":{\"514\":1}}],[\"从bertencoders编码输出结果中提取出被掩码的位置对应的嵌入向量\",{\"1\":{\"513\":1}}],[\"从4开始\",{\"1\":{\"511\":1}}],[\"从本章开始为每个chapter设计如下目录结构\",{\"1\":{\"661\":1}}],[\"从本节开始\",{\"1\":{\"509\":1}}],[\"从本质上来说\",{\"1\":{\"86\":1}}],[\"从统计语言模型\",{\"1\":{\"485\":1}}],[\"从词向量到上下文表示的发展历程\",{\"1\":{\"464\":1}}],[\"从早期的词向量\",{\"1\":{\"453\":1}}],[\"从早期的alexnet和cnn架构\",{\"1\":{\"183\":1}}],[\"从预训练到微调迁移学习过程中\",{\"1\":{\"449\":1}}],[\"从预测结果中找出每个样本预测概率最大的类别索引\",{\"1\":{\"296\":1}}],[\"从无标注文本中充分利用词级别以外的信息是有挑战性的\",{\"1\":{\"440\":1}}],[\"从上图中\",{\"1\":{\"436\":1}}],[\"从易至难技术\",{\"0\":{\"436\":1}}],[\"从结果集合中投票选择\",{\"1\":{\"435\":1}}],[\"从这些公理可以推导出一些常用结论\",{\"1\":{\"567\":1}}],[\"从这可以看出要全参数微调大语言模型\",{\"1\":{\"425\":1}}],[\"从这个数据集中\",{\"1\":{\"235\":1}}],[\"从个人使用情况来说\",{\"1\":{\"424\":1}}],[\"从成本和效果的角度综合考虑\",{\"1\":{\"417\":1}}],[\"从训练数据的来源\",{\"1\":{\"416\":1}}],[\"从参数规模的角度\",{\"1\":{\"416\":1}}],[\"从字符级别开始\",{\"1\":{\"409\":1}}],[\"从不同大小的\",{\"1\":{\"396\":1}}],[\"从内到外\",{\"0\":{\"376\":1}}],[\"从shape列表的最右边往左遍历\",{\"1\":{\"328\":1}}],[\"从对应的\",{\"1\":{\"312\":1}}],[\"从qkv张量中分离出查询\",{\"1\":{\"295\":1}}],[\"从数据库中检索相关信息\",{\"1\":{\"680\":1}}],[\"从数据集\",{\"1\":{\"289\":1}}],[\"从数据对的数量来看\",{\"1\":{\"272\":1}}],[\"从第44步到第57步\",{\"1\":{\"670\":1}}],[\"从第二个开始\",{\"1\":{\"285\":1}}],[\"从第几层开始启用\",{\"1\":{\"147\":1}}],[\"从冻结的llm引到vision\",{\"1\":{\"281\":1}}],[\"从冻结的image\",{\"1\":{\"281\":1}}],[\"从任务难度来看\",{\"1\":{\"278\":1}}],[\"从候选分类文本集合中取出其分类名词\",{\"1\":{\"275\":1}}],[\"从公式来说\",{\"1\":{\"238\":1}}],[\"从多个角度描述图像内容\",{\"1\":{\"226\":1}}],[\"从多个角度渲染点云或\",{\"1\":{\"114\":1}}],[\"从35种预设宽高比中选择最接近输入图像的配置\",{\"1\":{\"216\":1}}],[\"从1\",{\"1\":{\"210\":1}}],[\"从粗粒度对齐过渡到细粒度优化\",{\"1\":{\"190\":1}}],[\"从所有点中选出每个通道的最大响应值\",{\"1\":{\"107\":1,\"109\":1}}],[\"从稀疏点恢复到原始点密度\",{\"1\":{\"101\":1}}],[\"从下采样点中取出每个原始点对应的最近邻点的特征\",{\"1\":{\"100\":1}}],[\"从最稀疏的点开始\",{\"1\":{\"98\":1,\"99\":1}}],[\"从点云中选出有代表性的点作为中心点\",{\"1\":{\"98\":1}}],[\"从点云中根据索引提取特定点\",{\"1\":{\"92\":1}}],[\"从点云中找出最相关的功能区域\",{\"1\":{\"76\":1}}],[\"从原始点云中选出\",{\"1\":{\"92\":1}}],[\"从交互区域特征和背景特征中提取相关信息分别单独加到自己身上\",{\"1\":{\"59\":1}}],[\"从文件路径中提取物体名\",{\"1\":{\"58\":1}}],[\"从图中可看出逐渐接近星号所指的目的地位置\",{\"1\":{\"667\":1}}],[\"从图像的中心位置裁剪出\",{\"1\":{\"290\":1}}],[\"从图像\",{\"1\":{\"51\":1}}],[\"从图片路径中截取得到物体名\",{\"1\":{\"29\":1}}],[\"从索引区间中随机采样pair\",{\"1\":{\"29\":1}}],[\"从而不断迭代优化\",{\"1\":{\"687\":1}}],[\"从而将传统的模型训练调优转变成了更简单\",{\"1\":{\"686\":1}}],[\"从而为用户提供更加流畅的体验\",{\"1\":{\"684\":1}}],[\"从而为后续的动态卷积和掩码预测提供基础\",{\"1\":{\"76\":1}}],[\"从而简化应用程序的开发流程\",{\"1\":{\"682\":1}}],[\"从而显著提升了回答的准确性与深度\",{\"1\":{\"679\":1}}],[\"从而显著提升了模型的性能\",{\"1\":{\"674\":1}}],[\"从而得出最终答案\",{\"1\":{\"676\":1}}],[\"从而得到最终的\",{\"1\":{\"470\":1}}],[\"从而得到另外两张图\",{\"1\":{\"235\":1}}],[\"从而无法被自动回收\",{\"1\":{\"657\":1}}],[\"从而避免一个函数的backward方法被错误地多次调用\",{\"1\":{\"656\":1}}],[\"从而\",{\"1\":{\"593\":1}}],[\"从而增强模型的表达能力\",{\"1\":{\"548\":1}}],[\"从而增强了模型的表达能力\",{\"1\":{\"294\":1}}],[\"从而可以解决长距离依赖的问题\",{\"1\":{\"547\":1}}],[\"从而可以提供一些监督信号给到模型去训练\",{\"1\":{\"234\":1}}],[\"从而很难并行\",{\"1\":{\"547\":1}}],[\"从而在各种nlp\",{\"1\":{\"675\":1}}],[\"从而在注意力机制中就不会考虑到这些pad部分的token了\",{\"1\":{\"517\":1}}],[\"从而在实际应用中实现更快\",{\"1\":{\"26\":1}}],[\"从而计算交叉熵损失就很简单了\",{\"1\":{\"514\":1}}],[\"从而模型训练学习到每个词的含义需要更大量的数据集且最终效果也不会很好\",{\"1\":{\"511\":1}}],[\"从而帮助社区更好地理解不同改进的相对贡献\",{\"1\":{\"501\":1}}],[\"从而发现数据内部的逻辑与联系\",{\"1\":{\"428\":1}}],[\"从而应用于自己的业务场景\",{\"1\":{\"424\":1}}],[\"从而引导模型学习更多语义信息\",{\"1\":{\"404\":1}}],[\"从而减少对显式监督的依赖\",{\"1\":{\"453\":1}}],[\"从而减少对分类正确样本的贡献\",{\"1\":{\"404\":1}}],[\"从而减少计算复杂度并保持性能\",{\"1\":{\"73\":1}}],[\"从而扩展张量的形状\",{\"1\":{\"386\":1}}],[\"从而实现对图像的细粒度理解和分析\",{\"1\":{\"399\":1}}],[\"从而实现对原函数行为的增强或修改\",{\"1\":{\"367\":1}}],[\"从而实现点云的分层特征学习\",{\"1\":{\"92\":1}}],[\"从而导致偏差\",{\"1\":{\"359\":1}}],[\"从而导致更大的训练代价\",{\"1\":{\"280\":1}}],[\"从而提高了其理解和生成文本的能力\",{\"1\":{\"674\":1}}],[\"从而提高模型的分类性能\",{\"1\":{\"296\":1}}],[\"从而提升了在专业领域内的问题回答质量和深度\",{\"1\":{\"679\":1}}],[\"从而提升了用户对生成内容的信任度\",{\"1\":{\"679\":1}}],[\"从而提升对目标函数的拟合精度\",{\"1\":{\"395\":1}}],[\"从而提升其在各种任务上的泛化能力\",{\"1\":{\"224\":1}}],[\"从而提升泛化能力和鲁棒性\",{\"1\":{\"162\":1}}],[\"从而提升鲁棒性和泛化能力\",{\"1\":{\"157\":1}}],[\"从而提升模型对多尺度\",{\"1\":{\"71\":1}}],[\"从而完成图像分类任务\",{\"1\":{\"292\":1}}],[\"从而缓解了灾难性的遗忘问题\",{\"1\":{\"286\":1}}],[\"从而能够训练更大的模型\",{\"1\":{\"428\":1}}],[\"从而能够保持队列中的特征尽可能的\",{\"1\":{\"241\":1}}],[\"从而能够更好地捕捉点云的局部结构和层次信息\",{\"1\":{\"112\":1}}],[\"从而让特征的获取过程保持一致性\",{\"1\":{\"238\":1}}],[\"从而让字典的规模变得很大\",{\"1\":{\"237\":1}}],[\"从而让中间学习到的字典特征尽可能保持一致\",{\"1\":{\"236\":1}}],[\"从而更贴近人类期望\",{\"1\":{\"224\":1}}],[\"从而使对比学习更加稳健\",{\"1\":{\"145\":1}}],[\"从而考虑负样本中的潜在正样本\",{\"1\":{\"127\":1}}],[\"从而保证变换是刚性的\",{\"1\":{\"107\":1}}],[\"从而插值得到该点的特征\",{\"1\":{\"100\":1}}],[\"从而训练网络在面对实际应用中可能遇到的各种采样密度时\",{\"1\":{\"96\":1}}],[\"从而学会根据输入数据的变化自适应地加权不同尺度上检测到的模式\",{\"1\":{\"95\":1}}],[\"从而形成更完整的语言上下文理解\",{\"1\":{\"76\":1}}],[\"从而消除模态间的分布差异\",{\"1\":{\"59\":1}}],[\"从而获得对语言深层次的理解\",{\"1\":{\"673\":1}}],[\"从而获得\",{\"1\":{\"28\":1}}],[\"从\",{\"0\":{\"503\":1,\"509\":1},\"1\":{\"13\":1,\"24\":1,\"46\":1,\"63\":1,\"68\":1,\"92\":1,\"107\":1,\"147\":1,\"242\":1,\"282\":2,\"325\":1,\"327\":1,\"447\":1,\"454\":1,\"468\":1,\"513\":1,\"542\":1,\"674\":1,\"682\":1}}],[\"从几何结构解释该部位可以交互的原因\",{\"1\":{\"11\":1}}],[\"挖掘物体的不变几何属性\",{\"1\":{\"26\":1}}],[\"挖掘其他可能交互意图\",{\"1\":{\"12\":1}}],[\"挖掘不变几何属性与潜在交互意图\",{\"1\":{\"8\":1}}],[\"模拟用户需求的任务\",{\"1\":{\"470\":1}}],[\"模拟人类通过观察学习物体功能的能力\",{\"1\":{\"49\":1}}],[\"模拟人类对交互方式的联想\",{\"1\":{\"12\":1}}],[\"模拟人观察交互图像时的思维链条\",{\"1\":{\"28\":1}}],[\"模态融合比较轻量\",{\"1\":{\"280\":1}}],[\"模态融合比较弱\",{\"1\":{\"280\":1}}],[\"模态融合模式\",{\"1\":{\"162\":1}}],[\"模态交互部分可以分成两种方式\",{\"1\":{\"256\":1}}],[\"模式自动恢复\",{\"1\":{\"658\":1}}],[\"模式切换示例\",{\"1\":{\"658\":1}}],[\"模式匹配引擎\",{\"1\":{\"463\":1}}],[\"模式\",{\"1\":{\"163\":2,\"208\":1,\"289\":1,\"662\":1}}],[\"模式控制\",{\"1\":{\"163\":1}}],[\"模式中提供\",{\"1\":{\"163\":1}}],[\"模型输入\",{\"1\":{\"683\":1}}],[\"模型输出应不受刚性变换影响\",{\"1\":{\"104\":1}}],[\"模型输出的预测结果\",{\"1\":{\"405\":1}}],[\"模型输出的预测值\",{\"1\":{\"401\":1}}],[\"模型输出的\",{\"1\":{\"402\":1}}],[\"模型输出的概率值或二值化结果\",{\"1\":{\"403\":1}}],[\"模型输出的概率值大于某个值时\",{\"1\":{\"346\":1}}],[\"模型输出的概率值\",{\"1\":{\"82\":1,\"402\":1,\"407\":1}}],[\"模型输出的原始\",{\"1\":{\"78\":1,\"403\":1,\"404\":1}}],[\"模型等\",{\"1\":{\"683\":1}}],[\"模型学习特定领域的数据有助于减少幻觉\",{\"1\":{\"681\":1}}],[\"模型学会将图像中与分类任务相关的信息汇聚到\",{\"1\":{\"292\":1}}],[\"模型学会如何通过注意力机制将图像的有效信息汇聚到\",{\"1\":{\"292\":2}}],[\"模型定制\",{\"1\":{\"681\":1}}],[\"模型也采用了\",{\"1\":{\"674\":1}}],[\"模型还使用了高效的数据并行和流水线并行技术\",{\"1\":{\"674\":1}}],[\"模型是典型的\",{\"1\":{\"674\":1}}],[\"模型是过参数化的\",{\"1\":{\"424\":1}}],[\"模型展现出了一些惊人的能力\",{\"1\":{\"673\":1}}],[\"模型展现出组合概念\",{\"1\":{\"178\":1}}],[\"模型类\",{\"1\":{\"670\":1}}],[\"模型类型\",{\"1\":{\"147\":1}}],[\"模型代表\",{\"1\":{\"542\":1}}],[\"模型代码解读与复现\",{\"0\":{\"37\":1,\"60\":1}}],[\"模型那样逐词生成新内容\",{\"1\":{\"542\":1}}],[\"模型有点大\",{\"1\":{\"519\":1}}],[\"模型返回的logits\",{\"1\":{\"514\":1}}],[\"模型整体实现也比较简单\",{\"1\":{\"513\":1}}],[\"模型很难学到它们的语义表示\",{\"1\":{\"511\":1}}],[\"模型要判断出这个假设是\",{\"1\":{\"508\":1}}],[\"模型要理解并遵循人类语言中的任务描述\",{\"1\":{\"231\":1}}],[\"模型和cc\",{\"1\":{\"502\":1}}],[\"模型层\",{\"0\":{\"489\":1}}],[\"模型越大毒性倾向越高\",{\"1\":{\"482\":1}}],[\"模型与序列并行\",{\"1\":{\"481\":1}}],[\"模型会对每个选项分别编码\",{\"1\":{\"544\":1}}],[\"模型会输出两个数\",{\"1\":{\"508\":1}}],[\"模型会输出一个回答\",{\"1\":{\"474\":1}}],[\"模型会通过mask的上下文信息\",{\"1\":{\"258\":1}}],[\"模型顺从性过高\",{\"1\":{\"472\":1}}],[\"模型仍可能在对抗性攻击下暴露隐私\",{\"1\":{\"472\":1}}],[\"模型仍存在简单错误与对\",{\"1\":{\"471\":1}}],[\"模型应能自动识别并拒绝执行\",{\"1\":{\"472\":1}}],[\"模型机制\",{\"1\":{\"472\":1}}],[\"模型行为问题\",{\"1\":{\"472\":1}}],[\"模型行为干预与有害输出缓解策略\",{\"1\":{\"469\":1}}],[\"模型中常用的特殊\",{\"1\":{\"511\":1}}],[\"模型中\",{\"1\":{\"472\":1,\"674\":1}}],[\"模型泛化能力强\",{\"1\":{\"471\":1}}],[\"模型显著提升回答的真实性与信息性\",{\"1\":{\"471\":1}}],[\"模型均优于\",{\"1\":{\"471\":1}}],[\"模型比\",{\"1\":{\"471\":1}}],[\"模型提供输入\",{\"1\":{\"470\":1}}],[\"模型提取图像的特征图\",{\"1\":{\"299\":1}}],[\"模型进一步使用\",{\"1\":{\"470\":1}}],[\"模型依然在用户指令任务中获得更高的偏好评分\",{\"1\":{\"469\":1}}],[\"模型依然能稳定预测合理的交互区域\",{\"1\":{\"25\":1}}],[\"模型不可解释性问题严重\",{\"1\":{\"463\":1}}],[\"模型不再被\",{\"1\":{\"157\":1}}],[\"模型性能随规模扩展而持续提升\",{\"1\":{\"462\":1}}],[\"模型容易过拟合训练数据的虚假相关性\",{\"1\":{\"460\":1}}],[\"模型容量与零样本性能强相关\",{\"1\":{\"455\":1}}],[\"模型容量与任务性能呈对数线性关系\",{\"1\":{\"455\":1}}],[\"模型容量增加直接缩小了与人类表现的差距\",{\"1\":{\"455\":1}}],[\"模型容量对任务性能至关重要\",{\"1\":{\"452\":1}}],[\"模型规模与毒性正相关\",{\"1\":{\"484\":1}}],[\"模型规模与毒性\",{\"1\":{\"482\":1}}],[\"模型规模与性能的重新思考\",{\"1\":{\"480\":1}}],[\"模型规模与少样本学习能力呈正相关\",{\"1\":{\"460\":1}}],[\"模型规模扩展趋势与\",{\"1\":{\"464\":1}}],[\"模型规模的扩大可能显著提升上下文学习能力\",{\"1\":{\"460\":1}}],[\"模型规模的扩大带来了性能的持续提升\",{\"1\":{\"459\":1}}],[\"模型规模对性能有显著影响\",{\"1\":{\"229\":1}}],[\"模型局限性及其社会影响\",{\"1\":{\"459\":1}}],[\"模型倾向于使用简单的检索启发式\",{\"1\":{\"455\":1}}],[\"模型在编码\",{\"1\":{\"505\":1}}],[\"模型在训练过程中看到更多的掩码变体\",{\"1\":{\"495\":1}}],[\"模型在训练期间学会了抓取包和杯子\",{\"1\":{\"65\":1}}],[\"模型在一些公开\",{\"1\":{\"472\":1}}],[\"模型在未明确训练的任务上也表现良好\",{\"1\":{\"472\":1}}],[\"模型在\",{\"1\":{\"454\":1,\"471\":1,\"674\":1}}],[\"模型主要依赖于这个低的内在维度\",{\"1\":{\"424\":1}}],[\"模型更不喜欢\",{\"1\":{\"405\":2}}],[\"模型更新参数\",{\"1\":{\"292\":1}}],[\"模型分类错误\",{\"1\":{\"404\":1}}],[\"模型已自信分类\",{\"1\":{\"404\":1}}],[\"模型已经具备基本的视觉理解能力\",{\"1\":{\"226\":1}}],[\"模型被大量简单负样本主导\",{\"1\":{\"404\":1}}],[\"模型被强制学会将图像的有效信息汇聚到\",{\"1\":{\"292\":1}}],[\"模型将随机选择的正例正确排在随机选择的负例之上的概率为\",{\"1\":{\"351\":1}}],[\"模型将正例排在负例之上的概率\",{\"1\":{\"351\":1}}],[\"模型将完全依赖随机初始化的参数去\",{\"1\":{\"76\":1}}],[\"模型才会将该样本分类为正类\",{\"1\":{\"346\":1}}],[\"模型自动学习到了如果注意画面中的分类主体\",{\"1\":{\"298\":1}}],[\"模型计算每个\",{\"1\":{\"292\":1}}],[\"模型难以计算\",{\"1\":{\"291\":1}}],[\"模型名称\",{\"1\":{\"277\":1,\"674\":4}}],[\"模型预测出的功能区域\",{\"1\":{\"401\":1}}],[\"模型预测出来的\",{\"1\":{\"268\":1}}],[\"模型预测该点属于功能区域的概率\",{\"1\":{\"82\":1}}],[\"模型对非二元代词\",{\"1\":{\"482\":1}}],[\"模型对真实类别的预测概率\",{\"1\":{\"404\":1}}],[\"模型对正负样本差异更加敏感\",{\"1\":{\"240\":1}}],[\"模型对物体关键交互区域的识别能力下降\",{\"1\":{\"24\":1}}],[\"模型吗\",{\"1\":{\"239\":1}}],[\"模型通过在输入前后插入一些可训练的提示词来\",{\"1\":{\"231\":1}}],[\"模型通常在固定低分辨率\",{\"1\":{\"215\":1}}],[\"模型可能过拟合这些特定的掩码模式\",{\"1\":{\"495\":1}}],[\"模型可能就只会学习到和query使用同样编码器的那个key\",{\"1\":{\"238\":1}}],[\"模型可以更好地理解和生成图文结合的内容\",{\"1\":{\"226\":1}}],[\"模型可自适应调整分辨率\",{\"1\":{\"221\":1}}],[\"模型尝试生成尽可能高奖励的回答\",{\"1\":{\"224\":1}}],[\"模型作为视觉基础模型\",{\"1\":{\"215\":1}}],[\"模型深度对速度的影响在gpu计算饱和后可以忽略\",{\"1\":{\"189\":1}}],[\"模型优势\",{\"1\":{\"181\":1,\"208\":1}}],[\"模型优化目标为证据下界\",{\"1\":{\"178\":1}}],[\"模型训练\",{\"1\":{\"470\":2}}],[\"模型训练的第二阶段\",{\"1\":{\"227\":1}}],[\"模型训练的第一阶段\",{\"1\":{\"226\":1}}],[\"模型训练不可微分\",{\"1\":{\"170\":1}}],[\"模型训练过程中采用混合并行策略以适应大规模参数训练\",{\"1\":{\"461\":1}}],[\"模型训练过程中\",{\"1\":{\"159\":1}}],[\"模型前向传播代码如下所示\",{\"1\":{\"247\":1}}],[\"模型前向传播中的\",{\"1\":{\"161\":1,\"162\":1,\"163\":1}}],[\"模型前向\",{\"1\":{\"163\":1}}],[\"模型由三个主要部分组成\",{\"1\":{\"152\":1}}],[\"模型基于\",{\"1\":{\"147\":1}}],[\"模型使用了大规模的数据过滤和清洗技术\",{\"1\":{\"674\":1}}],[\"模型使用\",{\"1\":{\"131\":1}}],[\"模型设计上\",{\"1\":{\"126\":1}}],[\"模型架构图\",{\"1\":{\"548\":1}}],[\"模型架构改进\",{\"1\":{\"481\":1}}],[\"模型架构与规模设计\",{\"1\":{\"461\":1}}],[\"模型架构\",{\"0\":{\"126\":1,\"521\":1,\"548\":1},\"1\":{\"125\":1,\"454\":1}}],[\"模型构建两个模块\",{\"1\":{\"120\":1}}],[\"模型限制\",{\"1\":{\"120\":1}}],[\"模型为\",{\"1\":{\"114\":1}}],[\"模型表现良好\",{\"1\":{\"112\":1}}],[\"模型必须对输入点的排列顺序不敏感\",{\"1\":{\"104\":1}}],[\"模型需要从中选择最合适的答案\",{\"1\":{\"544\":1}}],[\"模型需要为每个点预测一个类别标签\",{\"1\":{\"98\":1}}],[\"模型需要根据自然语言问题识别点云中最相关的功能区域\",{\"1\":{\"78\":1}}],[\"模型需要根据自然语言问题\",{\"1\":{\"71\":1}}],[\"模型推理\",{\"1\":{\"83\":1}}],[\"模型初始化代码如下所示\",{\"1\":{\"246\":1}}],[\"模型初始化\",{\"1\":{\"80\":1,\"160\":1}}],[\"模型的输入不再使用位置编码\",{\"1\":{\"674\":1}}],[\"模型的基本原则是通过语言建模将世界知识压缩到仅解码器\",{\"1\":{\"674\":1}}],[\"模型的每个测试样本前会插入k条示例\",{\"1\":{\"461\":1}}],[\"模型的迁移能力逐渐增强\",{\"1\":{\"453\":1}}],[\"模型的具体配置\",{\"1\":{\"447\":1}}],[\"模型的认知\",{\"1\":{\"428\":1}}],[\"模型的参数是根据这个特定尺寸的输入数据进行优化和学习的\",{\"1\":{\"290\":1}}],[\"模型的泛化性就很强\",{\"1\":{\"238\":1}}],[\"模型的前向传播流程和albef实现基本一致\",{\"1\":{\"147\":1}}],[\"模型的\",{\"1\":{\"147\":1}}],[\"模型的训练代码\",{\"1\":{\"147\":1}}],[\"模型的训练过程大体分为了\",{\"1\":{\"79\":1}}],[\"模型的监督信号\",{\"1\":{\"78\":1}}],[\"模型变体包括\",{\"1\":{\"131\":1}}],[\"模型变体\",{\"1\":{\"75\":1}}],[\"模型目标\",{\"1\":{\"70\":1}}],[\"模型实现与优化\",{\"1\":{\"494\":1}}],[\"模型实现\",{\"0\":{\"70\":1}}],[\"模型结构与训练细节\",{\"1\":{\"470\":1}}],[\"模型结构与原始\",{\"1\":{\"224\":1}}],[\"模型结构图\",{\"1\":{\"39\":1}}],[\"模型结构\",{\"0\":{\"39\":1,\"87\":1,\"281\":1},\"1\":{\"120\":1,\"122\":1,\"226\":1,\"227\":1}}],[\"模型\",{\"0\":{\"30\":1,\"59\":1,\"391\":1,\"513\":1},\"1\":{\"114\":1,\"145\":1,\"163\":1,\"189\":1,\"191\":1,\"195\":1,\"229\":1,\"239\":1,\"246\":1,\"268\":1,\"272\":1,\"402\":1,\"414\":1,\"470\":4,\"513\":1,\"542\":1,\"670\":1,\"674\":9,\"683\":1}}],[\"模型能够泛化到新标注者的偏好\",{\"1\":{\"470\":1}}],[\"模型能够学习到丰富的图像特征和模式\",{\"1\":{\"300\":1}}],[\"模型能够学习从\",{\"1\":{\"29\":1}}],[\"模型能从无标记数据中充分利用语义信息\",{\"1\":{\"440\":1}}],[\"模型能准确对每个对象生成独立的\",{\"1\":{\"25\":1}}],[\"模型无法精确聚焦于关键交互部位\",{\"1\":{\"24\":1}}],[\"模型无法进行类比推理\",{\"1\":{\"24\":1}}],[\"模块的\",{\"1\":{\"477\":1}}],[\"模块的前向传播\",{\"1\":{\"46\":1}}],[\"模块查看参数\",{\"1\":{\"372\":1}}],[\"模块可以看作是一个特征预处理模块\",{\"1\":{\"296\":1}}],[\"模块增强后的数据集上\",{\"1\":{\"147\":1}}],[\"模块同样也是基于\",{\"1\":{\"145\":1}}],[\"模块在\",{\"1\":{\"143\":1}}],[\"模块微调阶段\",{\"1\":{\"140\":1}}],[\"模块实现\",{\"0\":{\"140\":1}}],[\"模块通过\",{\"1\":{\"132\":1}}],[\"模块通过剔除噪声文本完成隐式知识过滤\",{\"1\":{\"123\":1}}],[\"模块用生成的语义丰富描述进行蒸馏\",{\"1\":{\"123\":1}}],[\"模块用于最终的\",{\"1\":{\"46\":1}}],[\"模块\",{\"0\":{\"141\":1,\"144\":1,\"265\":1},\"1\":{\"10\":1,\"71\":1,\"280\":1,\"299\":1,\"477\":1,\"661\":2}}],[\"描述在给定\",{\"1\":{\"597\":1}}],[\"描述这个新视图的维度大小\",{\"1\":{\"325\":1}}],[\"描述器与过滤器的组合能显著提升性能\",{\"1\":{\"120\":1}}],[\"描述器\",{\"1\":{\"120\":1}}],[\"描述功能的文本提示\",{\"1\":{\"83\":1}}],[\"描述\",{\"1\":{\"63\":1,\"82\":4,\"112\":1,\"115\":1,\"401\":1,\"402\":1,\"403\":1,\"407\":1}}],[\"描述图像中人与物体的交互方式\",{\"1\":{\"12\":1}}],[\"描述图像中人与物体之间的完整交互过程\",{\"1\":{\"12\":1}}],[\"描述实际交互\",{\"1\":{\"6\":1}}],[\"进而可以自由构建\",{\"1\":{\"683\":1}}],[\"进而导致大模型的输出质量打折口\",{\"1\":{\"415\":1}}],[\"进入堆叠gpt2block模块前向传播流程\",{\"1\":{\"477\":1}}],[\"进入pointnet++经典的特征传播阶段\",{\"1\":{\"35\":1}}],[\"进来\",{\"1\":{\"241\":1}}],[\"进一步增强了模型性能\",{\"1\":{\"674\":1}}],[\"进一步增强特征\",{\"1\":{\"101\":1}}],[\"进一步强化变量的\",{\"1\":{\"659\":1}}],[\"进一步提升了性能\",{\"1\":{\"501\":1}}],[\"进一步提取和融合特征\",{\"1\":{\"100\":1}}],[\"进一步提取各自模态内部的语义一致性与结构关系\",{\"1\":{\"41\":1}}],[\"进一步降低了2\",{\"1\":{\"455\":1}}],[\"进一步来说\",{\"1\":{\"440\":1}}],[\"进一步帮llm明确要求\",{\"1\":{\"432\":1}}],[\"进一步减少冗余\",{\"1\":{\"395\":1}}],[\"进一步地\",{\"1\":{\"273\":1}}],[\"进一步推动了模型的下游表现\",{\"1\":{\"253\":1}}],[\"进一步训练模型理解和执行更复杂的视觉指令任务\",{\"1\":{\"225\":1}}],[\"进一步微调模型\",{\"1\":{\"224\":1}}],[\"进一步扩展了\",{\"1\":{\"674\":1}}],[\"进一步扩展了这个方法来预测n\",{\"1\":{\"278\":1}}],[\"进一步扩展了文本与视觉信息的交互能力\",{\"1\":{\"208\":1}}],[\"进一步扩大语料规模\",{\"1\":{\"138\":1}}],[\"进一步对齐视觉与语言特征\",{\"1\":{\"190\":1}}],[\"进一步引入了视觉定位能力\",{\"1\":{\"185\":1}}],[\"进一步从几何结构角度推理为什么该部位适合交互\",{\"1\":{\"11\":1}}],[\"进行个性化定制\",{\"1\":{\"687\":1}}],[\"进行自然的语音和视频交流\",{\"1\":{\"674\":1}}],[\"进行边际化\",{\"1\":{\"596\":1}}],[\"进行解压\",{\"1\":{\"519\":1}}],[\"进行二分类任务\",{\"1\":{\"513\":1}}],[\"进行预测即可\",{\"1\":{\"508\":1}}],[\"进行预训练\",{\"1\":{\"153\":1,\"282\":1}}],[\"进行经典的多头自注意力运算\",{\"1\":{\"477\":1}}],[\"进行划分\",{\"1\":{\"470\":1}}],[\"进行的进一步优化\",{\"1\":{\"435\":1}}],[\"进行思考\",{\"1\":{\"433\":1}}],[\"进行一次降维再升维的操作\",{\"1\":{\"425\":1}}],[\"进行训练\",{\"1\":{\"423\":1}}],[\"进行训练和评估\",{\"1\":{\"140\":1}}],[\"进行全量的训练\",{\"1\":{\"416\":1}}],[\"进行逐元素运算\",{\"1\":{\"327\":1}}],[\"进行逐级上采样\",{\"1\":{\"46\":1}}],[\"进行相加操作\",{\"1\":{\"327\":1}}],[\"进行重新排列\",{\"1\":{\"326\":1}}],[\"进行\",{\"1\":{\"325\":1,\"508\":1}}],[\"进行局部特征提取\",{\"1\":{\"299\":1}}],[\"进行反向传播\",{\"1\":{\"296\":1}}],[\"进行并行输入\",{\"1\":{\"291\":1}}],[\"进行文本编码\",{\"1\":{\"285\":1}}],[\"进行编码\",{\"1\":{\"282\":2}}],[\"进行缓慢更新的代码实现如下所示\",{\"1\":{\"248\":1}}],[\"进行了进一步的量化\",{\"1\":{\"421\":1}}],[\"进行了一次随机抽样\",{\"1\":{\"240\":1}}],[\"进行了以下筛选\",{\"1\":{\"226\":1}}],[\"进行测试\",{\"1\":{\"237\":1}}],[\"进行渐进式对齐\",{\"1\":{\"198\":1}}],[\"进行对比\",{\"1\":{\"196\":1}}],[\"进行替换\",{\"1\":{\"163\":1}}],[\"进行线性变换\",{\"1\":{\"161\":1}}],[\"进行线性映射形成\",{\"1\":{\"14\":1}}],[\"进行蒸馏监督\",{\"1\":{\"157\":1}}],[\"进行归一化\",{\"1\":{\"145\":1,\"296\":1}}],[\"进行分词并转为\",{\"1\":{\"143\":1}}],[\"进行分类\",{\"1\":{\"112\":1,\"543\":1}}],[\"进行增强\",{\"1\":{\"140\":1}}],[\"进行特征插值和上采样\",{\"1\":{\"101\":1}}],[\"进行特征学习\",{\"1\":{\"86\":1}}],[\"进行多尺度特征提取和下采样\",{\"1\":{\"101\":1}}],[\"进行推理的代码\",{\"1\":{\"477\":1}}],[\"进行推理\",{\"1\":{\"83\":1}}],[\"进行内积操作\",{\"1\":{\"76\":1}}],[\"进行组内和通道间的信息混合\",{\"0\":{\"73\":1}}],[\"进行联合推理\",{\"1\":{\"40\":1}}],[\"进行信息压缩\",{\"1\":{\"36\":1}}],[\"进行微调\",{\"1\":{\"10\":1}}],[\"识别错误并主动纠正\",{\"1\":{\"674\":1}}],[\"识别点云中的功能区域\",{\"1\":{\"71\":1}}],[\"识别交互部位\",{\"1\":{\"28\":1}}],[\"识别交互部件\",{\"1\":{\"6\":1}}],[\"识别图像中物体与人发生交互的部分\",{\"1\":{\"11\":1}}],[\"pmf\",{\"1\":{\"565\":4,\"577\":1}}],[\"ppl\",{\"1\":{\"462\":2}}],[\"ppo\",{\"1\":{\"224\":1,\"468\":1,\"469\":2,\"470\":11,\"471\":8,\"472\":3}}],[\"pwd=vket\",{\"1\":{\"300\":1}}],[\"pwd=qvmq\",{\"1\":{\"289\":1}}],[\"p采样\",{\"1\":{\"286\":1}}],[\"png将dot格式文件转换为png图像\",{\"1\":{\"666\":1}}],[\"png\",{\"1\":{\"275\":1,\"277\":1,\"289\":2,\"666\":4}}],[\"platform\",{\"1\":{\"678\":1}}],[\"place\",{\"1\":{\"511\":2}}],[\"plain\",{\"1\":{\"470\":1}}],[\"playground\",{\"1\":{\"470\":1}}],[\"play\",{\"1\":{\"29\":1,\"469\":1}}],[\"plug\",{\"1\":{\"469\":1}}],[\"plus\",{\"1\":{\"210\":1}}],[\"plm\",{\"1\":{\"425\":3,\"676\":1}}],[\"plot\",{\"1\":{\"289\":2,\"666\":4,\"667\":2}}],[\"plt\",{\"1\":{\"276\":4,\"277\":5,\"289\":7,\"667\":11}}],[\"pseudo\",{\"1\":{\"157\":2}}],[\"p=dropout\",{\"1\":{\"558\":1}}],[\"p=drop\",{\"1\":{\"293\":1,\"296\":1}}],[\"p=top\",{\"1\":{\"143\":1,\"286\":1}}],[\"p=0\",{\"1\":{\"110\":1,\"143\":1,\"244\":1,\"286\":1,\"404\":2}}],[\"pkill\",{\"1\":{\"83\":1}}],[\"pkl\",{\"1\":{\"68\":2,\"83\":2}}],[\"py中导入核心类并初始化运算符重载\",{\"1\":{\"661\":1}}],[\"py文件\",{\"1\":{\"519\":1,\"661\":1}}],[\"pypi\",{\"1\":{\"338\":1}}],[\"pyplot\",{\"1\":{\"277\":1,\"667\":1}}],[\"py\",{\"1\":{\"83\":3,\"93\":1,\"96\":1,\"260\":1,\"289\":1,\"477\":1,\"519\":2,\"661\":6}}],[\"python会调用x的\",{\"1\":{\"660\":1}}],[\"python会根据操作数的类型选择不同的方法调用路径\",{\"1\":{\"660\":1}}],[\"python数值类型\",{\"1\":{\"660\":1}}],[\"python首先尝试调用左操作数a的\",{\"1\":{\"660\":1}}],[\"python中\",{\"1\":{\"660\":1}}],[\"python通过跟踪对象的引用次数来管理内存\",{\"1\":{\"657\":1}}],[\"python的内存管理主要依靠两种机制\",{\"1\":{\"657\":1}}],[\"python=3\",{\"1\":{\"331\":2,\"519\":1,\"546\":1}}],[\"python\",{\"0\":{\"362\":1},\"1\":{\"83\":7,\"289\":1,\"331\":2,\"338\":7,\"365\":1,\"366\":1,\"368\":1,\"372\":1,\"519\":1,\"651\":1,\"660\":1,\"684\":1}}],[\"pytorch中使用比较多的tensor的阶为4\",{\"1\":{\"328\":1}}],[\"pytorch张量存储与访问原理\",{\"0\":{\"320\":1},\"1\":{\"320\":1}}],[\"pytorch版本\",{\"0\":{\"106\":1}}],[\"pytorch\",{\"0\":{\"380\":1},\"1\":{\"76\":1,\"85\":2,\"102\":2,\"131\":1,\"163\":1,\"289\":2,\"290\":2,\"300\":1,\"322\":1,\"381\":1,\"385\":1,\"386\":1,\"387\":1,\"388\":1,\"389\":1,\"407\":1,\"506\":1,\"519\":3,\"541\":1}}],[\"pd\",{\"1\":{\"68\":1}}],[\"pdf等格式\",{\"1\":{\"666\":1}}],[\"pdfs\",{\"1\":{\"217\":1}}],[\"pdf\",{\"1\":{\"60\":1,\"566\":2,\"584\":1,\"586\":1,\"587\":1,\"590\":1,\"666\":1,\"687\":1}}],[\"phenomenon\",{\"1\":{\"592\":1}}],[\"photos下的子目录名作为我们的候选待匹配分类文本列表\",{\"1\":{\"275\":1}}],[\"photos\",{\"1\":{\"275\":4,\"277\":2}}],[\"photos目录下读取出所有图片的路径\",{\"1\":{\"275\":1}}],[\"photo\",{\"1\":{\"273\":2,\"274\":2,\"275\":2,\"276\":1,\"277\":2}}],[\"phrase\",{\"1\":{\"226\":1}}],[\"phrasing\",{\"1\":{\"63\":1}}],[\"philosophy\",{\"1\":{\"483\":1}}],[\"phi\",{\"1\":{\"41\":7,\"59\":7}}],[\"p3\",{\"1\":{\"46\":1}}],[\"p2\",{\"1\":{\"46\":1}}],[\"p1\",{\"1\":{\"46\":1}}],[\"p0\",{\"1\":{\"46\":1}}],[\"p+n\",{\"1\":{\"41\":1,\"59\":1}}],[\"ptx\",{\"1\":{\"469\":1,\"470\":4,\"471\":5,\"472\":1}}],[\"ptb\",{\"1\":{\"455\":1,\"510\":1}}],[\"pt较高\",{\"1\":{\"404\":1}}],[\"pth\",{\"1\":{\"300\":1,\"514\":2}}],[\"ptr\",{\"1\":{\"145\":1,\"147\":1,\"160\":1,\"246\":1,\"249\":9}}],[\"pts\",{\"1\":{\"109\":2,\"111\":2}}],[\"pt\",{\"1\":{\"40\":1,\"82\":1,\"83\":1,\"142\":1,\"143\":1,\"145\":1,\"146\":1,\"147\":1,\"159\":1,\"275\":2,\"277\":2,\"282\":1,\"404\":7,\"477\":1}}],[\"pca\",{\"1\":{\"355\":2}}],[\"pcd\",{\"1\":{\"83\":4}}],[\"pc\",{\"1\":{\"29\":2,\"58\":1,\"68\":1,\"83\":11}}],[\"punctuation\",{\"1\":{\"510\":1}}],[\"pull\",{\"1\":{\"29\":1,\"68\":1}}],[\"push\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"p\",{\"1\":{\"29\":3,\"30\":2,\"35\":22,\"36\":7,\"41\":15,\"45\":6,\"46\":27,\"58\":3,\"59\":56,\"70\":19,\"72\":1,\"74\":4,\"75\":1,\"80\":6,\"82\":6,\"143\":1,\"286\":1,\"404\":3,\"410\":2,\"412\":2,\"424\":2,\"477\":1,\"511\":3,\"558\":5}}],[\"piqa\",{\"1\":{\"482\":1}}],[\"pii\",{\"1\":{\"470\":1}}],[\"pip\",{\"1\":{\"338\":9,\"546\":1}}],[\"pil\",{\"1\":{\"277\":1,\"289\":1,\"290\":2}}],[\"pin\",{\"1\":{\"244\":1}}],[\"picture\",{\"1\":{\"142\":2,\"143\":2,\"145\":1}}],[\"pickle\",{\"1\":{\"68\":2,\"83\":3}}],[\"pixel\",{\"1\":{\"28\":5,\"214\":1}}],[\"piad数据集\",{\"1\":{\"49\":1}}],[\"piad\",{\"1\":{\"17\":1,\"20\":1,\"29\":1,\"58\":1}}],[\"piadv2\",{\"1\":{\"16\":1,\"20\":1,\"21\":1,\"26\":1}}],[\"palm\",{\"1\":{\"482\":1,\"483\":1,\"673\":1,\"674\":1}}],[\"palm方案\",{\"1\":{\"481\":1}}],[\"palm等依赖未公开数据\",{\"1\":{\"480\":1}}],[\"passed\",{\"1\":{\"520\":1}}],[\"pass\",{\"1\":{\"374\":1,\"375\":1,\"376\":1,\"482\":1,\"511\":1,\"554\":1,\"666\":1}}],[\"past\",{\"1\":{\"162\":1,\"284\":3,\"285\":27,\"477\":27}}],[\"package\",{\"1\":{\"338\":1,\"661\":1}}],[\"packages\",{\"1\":{\"338\":1}}],[\"pandas\",{\"1\":{\"331\":1}}],[\"pan\",{\"1\":{\"289\":1,\"300\":1}}],[\"page\",{\"1\":{\"273\":1}}],[\"pad\",{\"1\":{\"142\":1,\"143\":5,\"147\":1,\"163\":2,\"285\":1,\"286\":2,\"412\":3,\"511\":2,\"512\":9,\"513\":3,\"517\":11,\"520\":3}}],[\"padding=true\",{\"1\":{\"275\":1,\"277\":1}}],[\"padding=\",{\"1\":{\"40\":1,\"142\":1,\"145\":1,\"146\":1,\"147\":1,\"159\":1,\"282\":1}}],[\"padding\",{\"0\":{\"517\":1},\"1\":{\"40\":2,\"43\":3,\"70\":1,\"74\":4,\"75\":1,\"76\":8,\"142\":1,\"143\":1,\"161\":1,\"262\":2,\"282\":1,\"285\":6,\"477\":1,\"512\":1,\"513\":1,\"520\":6,\"523\":1,\"528\":1,\"529\":1,\"548\":1}}],[\"paris\",{\"1\":{\"542\":3}}],[\"paragraph\",{\"1\":{\"510\":7}}],[\"paraphrase\",{\"1\":{\"448\":1}}],[\"para\",{\"1\":{\"300\":2}}],[\"parameter\",{\"1\":{\"145\":1,\"147\":1,\"160\":1,\"205\":1,\"272\":1,\"292\":1,\"293\":2,\"296\":2,\"416\":1,\"418\":1,\"421\":1,\"536\":1}}],[\"parameters\",{\"1\":{\"80\":2,\"142\":1,\"145\":1,\"147\":1,\"244\":1,\"246\":2,\"248\":2,\"300\":1,\"389\":1,\"514\":2,\"528\":1}}],[\"params=model\",{\"1\":{\"142\":1,\"145\":1,\"147\":1}}],[\"params\",{\"1\":{\"80\":3,\"145\":1,\"147\":1,\"160\":1}}],[\"param\",{\"1\":{\"80\":2,\"246\":5,\"248\":5,\"289\":5,\"291\":6}}],[\"parts\",{\"1\":{\"543\":1}}],[\"partial\",{\"1\":{\"296\":1,\"365\":1}}],[\"partition\",{\"1\":{\"569\":1}}],[\"partitioning\",{\"1\":{\"86\":2}}],[\"partitions\",{\"0\":{\"20\":1}}],[\"part\",{\"1\":{\"28\":11,\"83\":3}}],[\"paper\",{\"1\":{\"60\":1,\"300\":1,\"531\":1}}],[\"papers\",{\"1\":{\"60\":1}}],[\"pa\",{\"1\":{\"46\":5,\"59\":5}}],[\"patch16\",{\"1\":{\"300\":6}}],[\"patch14\",{\"1\":{\"275\":2,\"277\":1}}],[\"patchembed\",{\"1\":{\"291\":2}}],[\"patches\",{\"1\":{\"73\":3,\"86\":1,\"291\":5,\"292\":6,\"293\":4,\"295\":11,\"296\":4,\"300\":1,\"301\":1}}],[\"patch\",{\"1\":{\"41\":5,\"59\":2,\"73\":9,\"126\":1,\"160\":2,\"161\":1,\"168\":1,\"204\":1,\"253\":1,\"256\":1,\"258\":1,\"267\":1,\"291\":12,\"292\":11,\"293\":5,\"296\":5,\"300\":1}}],[\"path$bert\",{\"1\":{\"519\":2}}],[\"path=prev\",{\"1\":{\"519\":1}}],[\"path=val\",{\"1\":{\"290\":1}}],[\"path=train\",{\"1\":{\"290\":1}}],[\"paths\",{\"1\":{\"275\":17,\"276\":6,\"277\":21}}],[\"path\",{\"1\":{\"28\":6,\"29\":30,\"58\":24,\"68\":3,\"82\":3,\"83\":5,\"142\":3,\"275\":12,\"276\":2,\"277\":14,\"289\":30,\"290\":5,\"293\":1,\"294\":7,\"296\":2,\"410\":6,\"412\":10,\"510\":12,\"511\":5,\"514\":5,\"666\":8,\"667\":1}}],[\"pair+nsp\",{\"1\":{\"495\":2}}],[\"pairwise\",{\"1\":{\"470\":1}}],[\"pair是否match\",{\"1\":{\"284\":1}}],[\"pairs\",{\"1\":{\"67\":1,\"145\":1,\"147\":1,\"160\":1,\"410\":9,\"411\":4,\"412\":10,\"448\":1,\"469\":1,\"470\":1,\"471\":1,\"482\":1,\"484\":1,\"512\":14}}],[\"pair\",{\"1\":{\"29\":5,\"58\":3,\"409\":1,\"410\":17,\"411\":14,\"412\":42,\"454\":1,\"520\":2}}],[\"pair=2\",{\"1\":{\"29\":1,\"58\":1}}],[\"price函数为例\",{\"1\":{\"666\":1}}],[\"price函数求导\",{\"1\":{\"662\":1}}],[\"priority\",{\"1\":{\"660\":2}}],[\"prior\",{\"1\":{\"571\":1,\"596\":1}}],[\"print\",{\"1\":{\"28\":4,\"82\":1,\"83\":2,\"159\":1,\"275\":6,\"276\":4,\"277\":10,\"289\":3,\"300\":1,\"321\":1,\"323\":7,\"325\":4,\"326\":10,\"365\":2,\"366\":1,\"367\":3,\"369\":3,\"370\":3,\"371\":2,\"372\":8,\"373\":2,\"374\":1,\"377\":3,\"379\":2,\"386\":1,\"474\":3,\"477\":3,\"510\":2,\"514\":3,\"608\":2,\"617\":1,\"658\":2,\"659\":8,\"660\":4,\"662\":3,\"666\":1,\"667\":2}}],[\"prc\",{\"1\":{\"352\":1}}],[\"property\",{\"1\":{\"659\":4}}],[\"propagation\",{\"1\":{\"98\":4,\"99\":1,\"100\":1,\"101\":2}}],[\"procedure\",{\"1\":{\"511\":1,\"512\":1}}],[\"process\",{\"1\":{\"510\":3,\"549\":1}}],[\"processor\",{\"1\":{\"275\":3,\"277\":3}}],[\"processing\",{\"1\":{\"274\":1,\"421\":1}}],[\"promt\",{\"1\":{\"477\":2}}],[\"prompt敏感性高\",{\"1\":{\"463\":1}}],[\"prompt依赖性强\",{\"1\":{\"463\":1}}],[\"prompt写得好不好\",{\"1\":{\"430\":1}}],[\"prompt太长会因超过限制而被截断\",{\"1\":{\"415\":1}}],[\"prompts\",{\"1\":{\"286\":1,\"419\":1,\"470\":3,\"471\":1}}],[\"prompting有一个直观的认知\",{\"1\":{\"436\":1}}],[\"prompting技巧\",{\"1\":{\"435\":1}}],[\"prompting\",{\"1\":{\"274\":1,\"421\":1,\"434\":1,\"436\":1,\"454\":1}}],[\"prompt+pre\",{\"1\":{\"142\":1}}],[\"prompt=config\",{\"1\":{\"142\":1}}],[\"prompt\",{\"0\":{\"231\":1,\"418\":1,\"429\":1},\"1\":{\"11\":2,\"12\":2,\"28\":9,\"142\":10,\"143\":8,\"231\":11,\"274\":3,\"415\":3,\"418\":5,\"419\":1,\"421\":1,\"424\":2,\"429\":1,\"430\":2,\"432\":1,\"434\":1,\"463\":1,\"464\":1,\"470\":8,\"471\":1,\"472\":1,\"477\":4,\"683\":1,\"685\":1,\"686\":10,\"687\":12}}],[\"prototypical\",{\"1\":{\"464\":1}}],[\"provided\",{\"1\":{\"412\":2}}],[\"proximal\",{\"1\":{\"224\":1,\"470\":1}}],[\"pro\",{\"1\":{\"210\":1,\"674\":3}}],[\"probabilistic\",{\"1\":{\"596\":1,\"673\":1}}],[\"probabilities\",{\"1\":{\"531\":1}}],[\"probability\",{\"0\":{\"569\":1},\"1\":{\"160\":2,\"163\":6,\"565\":1,\"566\":1,\"571\":2}}],[\"problems\",{\"0\":{\"597\":1}}],[\"problem\",{\"1\":{\"468\":1,\"597\":1}}],[\"probs\",{\"1\":{\"266\":4,\"273\":3,\"285\":7,\"531\":5}}],[\"prob\",{\"1\":{\"265\":1,\"523\":1,\"525\":1,\"529\":1,\"531\":1,\"532\":1,\"543\":1,\"544\":1}}],[\"probing\",{\"1\":{\"205\":1,\"237\":1}}],[\"product\",{\"1\":{\"115\":1,\"305\":1,\"508\":2,\"531\":1,\"558\":1,\"596\":1}}],[\"projected\",{\"1\":{\"558\":1}}],[\"projects\",{\"1\":{\"279\":1}}],[\"projections\",{\"1\":{\"558\":1}}],[\"projection转化成visual\",{\"1\":{\"257\":1}}],[\"projection方法将输入图片切片投影提取特征\",{\"1\":{\"256\":1}}],[\"projection来做visual\",{\"1\":{\"253\":1,\"256\":1}}],[\"projection的多模态预训练模型\",{\"1\":{\"253\":1}}],[\"projection\",{\"1\":{\"160\":2,\"558\":1}}],[\"proj\",{\"1\":{\"32\":6,\"34\":3,\"35\":4,\"36\":5,\"45\":32,\"59\":5,\"80\":2,\"83\":2,\"145\":18,\"146\":4,\"147\":12,\"160\":8,\"161\":4,\"272\":2,\"282\":2,\"291\":2,\"294\":1,\"295\":6,\"477\":2,\"550\":2}}],[\"precomputed\",{\"1\":{\"531\":1}}],[\"precision\",{\"1\":{\"405\":1,\"584\":1}}],[\"prev\",{\"1\":{\"519\":1}}],[\"preview\",{\"0\":{\"242\":1},\"1\":{\"674\":3}}],[\"prepare\",{\"1\":{\"410\":2,\"412\":4,\"512\":3}}],[\"present\",{\"1\":{\"285\":2}}],[\"press\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"pretraining\",{\"0\":{\"258\":1},\"1\":{\"470\":1,\"490\":1}}],[\"pretrain\",{\"1\":{\"147\":3,\"470\":1}}],[\"pretrainedtokenizer\",{\"1\":{\"520\":1}}],[\"pretrained=config\",{\"1\":{\"142\":1,\"145\":1}}],[\"pretrained\",{\"1\":{\"28\":2,\"142\":1,\"145\":1,\"147\":2,\"160\":2,\"275\":4,\"277\":4,\"474\":2,\"477\":1}}],[\"prefix\",{\"0\":{\"419\":1},\"1\":{\"143\":1,\"231\":2,\"371\":3,\"419\":5,\"424\":2}}],[\"pre\",{\"0\":{\"153\":1},\"1\":{\"119\":2,\"120\":1,\"134\":1,\"164\":2,\"225\":1,\"271\":1,\"274\":1,\"296\":3,\"300\":2,\"421\":1,\"425\":1,\"438\":1,\"439\":1,\"481\":1,\"674\":2}}],[\"pred的进行pad填充\",{\"1\":{\"514\":1}}],[\"predicted\",{\"1\":{\"83\":1,\"275\":6,\"277\":6}}],[\"predict\",{\"1\":{\"83\":2,\"142\":1,\"274\":1,\"421\":1,\"508\":1}}],[\"predictions\",{\"1\":{\"537\":2}}],[\"prediction\",{\"0\":{\"506\":1},\"1\":{\"78\":1,\"83\":1,\"163\":5,\"268\":7,\"278\":1,\"285\":6,\"497\":1,\"504\":1,\"520\":1,\"537\":2,\"538\":4}}],[\"pred\",{\"1\":{\"78\":10,\"82\":4,\"83\":24,\"296\":5,\"512\":7,\"513\":7}}],[\"pepper\",{\"1\":{\"600\":2}}],[\"penn\",{\"1\":{\"510\":1}}],[\"penalty\",{\"1\":{\"143\":1}}],[\"penalty=repetition\",{\"1\":{\"143\":1}}],[\"penalty=1\",{\"1\":{\"143\":2,\"286\":1}}],[\"petaflop\",{\"1\":{\"472\":3}}],[\"peft\",{\"1\":{\"421\":1,\"428\":1}}],[\"peft是目前业界比较流行的微调方案\",{\"1\":{\"417\":1}}],[\"peft也是目前比较主流的微调方案\",{\"1\":{\"416\":1}}],[\"peft主要想解决的问题\",{\"1\":{\"416\":1}}],[\"people\",{\"1\":{\"28\":6,\"31\":4,\"226\":1}}],[\"perplexity\",{\"1\":{\"454\":1,\"455\":1}}],[\"perspective\",{\"1\":{\"471\":1}}],[\"persistent\",{\"1\":{\"389\":2}}],[\"person\",{\"1\":{\"28\":8,\"31\":2,\"226\":1,\"273\":1}}],[\"permissionerror\",{\"1\":{\"379\":1}}],[\"permutation\",{\"1\":{\"104\":1,\"115\":2}}],[\"permute维度\",{\"1\":{\"36\":2}}],[\"permute后\",{\"1\":{\"35\":1}}],[\"permute\",{\"0\":{\"383\":1},\"1\":{\"30\":2,\"32\":2,\"34\":1,\"35\":2,\"36\":2,\"41\":2,\"45\":1,\"59\":3,\"92\":4,\"96\":4,\"100\":5,\"101\":1,\"266\":1,\"283\":1,\"285\":1,\"295\":2,\"383\":2,\"384\":2,\"385\":1,\"531\":2}}],[\"per\",{\"1\":{\"98\":1,\"99\":2,\"295\":6,\"519\":2}}],[\"performs\",{\"1\":{\"28\":2}}],[\"performance\",{\"0\":{\"25\":1},\"1\":{\"504\":1}}],[\"perception\",{\"1\":{\"11\":1,\"28\":2}}],[\"pow\",{\"1\":{\"660\":5,\"662\":2}}],[\"power\",{\"1\":{\"418\":1}}],[\"pop\",{\"1\":{\"638\":1,\"652\":1,\"654\":1,\"656\":1,\"658\":1,\"666\":2}}],[\"pope\",{\"1\":{\"190\":1,\"195\":1}}],[\"poisson\",{\"0\":{\"577\":1}}],[\"pointer\",{\"1\":{\"246\":1,\"249\":1,\"325\":1}}],[\"pointcnn\",{\"1\":{\"112\":1}}],[\"pointcloud\",{\"1\":{\"83\":2,\"92\":2}}],[\"pointfeat\",{\"1\":{\"109\":2}}],[\"point点云数据\",{\"1\":{\"70\":1}}],[\"pointrefer模型结构图\",{\"1\":{\"70\":1}}],[\"pointrefer\",{\"1\":{\"70\":5,\"76\":1,\"78\":1,\"80\":1,\"82\":2,\"83\":4}}],[\"pointnetdensecls\",{\"1\":{\"111\":3}}],[\"pointnetcls\",{\"1\":{\"110\":3}}],[\"pointnetfeat\",{\"1\":{\"109\":3,\"110\":4,\"111\":3}}],[\"pointnetfeaturepropagation\",{\"1\":{\"35\":3,\"46\":5,\"59\":3,\"100\":3,\"101\":4}}],[\"pointnet网络模型结构图\",{\"1\":{\"106\":1}}],[\"pointnet后\",{\"1\":{\"93\":3}}],[\"pointnetsetabstractionmsg\",{\"1\":{\"96\":5}}],[\"pointnetsetabstraction\",{\"1\":{\"92\":3,\"93\":3,\"96\":1,\"101\":4}}],[\"pointnet来提取局部区域中的特征\",{\"1\":{\"91\":1}}],[\"pointnet将局部区域编码为特征向量\",{\"1\":{\"88\":1}}],[\"pointnet\",{\"0\":{\"91\":1},\"1\":{\"85\":1,\"87\":1,\"88\":2,\"92\":2,\"96\":1,\"98\":1,\"102\":2,\"103\":1,\"105\":4,\"107\":4,\"108\":2,\"109\":1,\"110\":1,\"111\":1,\"112\":26,\"115\":2}}],[\"pointnet2\",{\"1\":{\"85\":2,\"93\":1,\"96\":2}}],[\"pointnet编码点云\",{\"1\":{\"59\":1}}],[\"pointnet++提出了密度自适应pointnet层\",{\"1\":{\"94\":1}}],[\"pointnet++提取特征\",{\"1\":{\"54\":1}}],[\"pointnet++应用pointnet递归地对输入集进行嵌套分区\",{\"1\":{\"86\":1}}],[\"pointnet++选择pointnet作为局部特征学习器\",{\"1\":{\"86\":1}}],[\"pointnet++在进行点集划分时\",{\"1\":{\"86\":1}}],[\"pointnet++的下一个任务是学习这些子集\",{\"1\":{\"86\":1}}],[\"pointnet++需要一种方法来有效地将点云分割成多个部分\",{\"1\":{\"86\":1}}],[\"pointnet++\",{\"1\":{\"8\":1,\"14\":1,\"22\":1,\"30\":1,\"40\":1,\"70\":3,\"71\":1,\"86\":1,\"92\":3,\"93\":2,\"96\":2,\"98\":2,\"99\":1,\"100\":1,\"101\":1,\"112\":4}}],[\"points2\",{\"1\":{\"98\":2,\"100\":7}}],[\"points1\",{\"1\":{\"98\":2,\"100\":7}}],[\"points\",{\"1\":{\"29\":9,\"35\":1,\"40\":1,\"58\":9,\"63\":1,\"83\":18,\"92\":68,\"93\":7,\"96\":39,\"98\":2,\"99\":7,\"100\":15,\"101\":27,\"104\":2,\"107\":1}}],[\"point\",{\"1\":{\"16\":1,\"28\":3,\"29\":28,\"30\":1,\"40\":8,\"41\":6,\"46\":3,\"58\":14,\"59\":1,\"68\":6,\"70\":3,\"76\":3,\"81\":3,\"82\":6,\"83\":15,\"89\":1,\"92\":7,\"96\":2,\"98\":2,\"99\":2,\"105\":1,\"107\":2,\"112\":2,\"114\":1}}],[\"portion\",{\"1\":{\"520\":1}}],[\"ported\",{\"1\":{\"300\":1}}],[\"portrait\",{\"1\":{\"273\":1}}],[\"policy\",{\"1\":{\"224\":3,\"468\":1,\"470\":1}}],[\"polar\",{\"1\":{\"157\":1}}],[\"pond\",{\"1\":{\"157\":1}}],[\"posed\",{\"1\":{\"597\":1}}],[\"posterior\",{\"1\":{\"571\":1,\"596\":2}}],[\"possible\",{\"1\":{\"412\":1}}],[\"positive和\",{\"1\":{\"449\":1}}],[\"positives\",{\"1\":{\"405\":1}}],[\"positive\",{\"1\":{\"78\":6,\"145\":1,\"244\":1,\"247\":2,\"403\":1,\"405\":4}}],[\"positions\",{\"1\":{\"541\":7,\"542\":1}}],[\"positions=none\",{\"1\":{\"541\":2}}],[\"position=cache\",{\"1\":{\"477\":1}}],[\"position=none\",{\"1\":{\"477\":1}}],[\"positional\",{\"1\":{\"293\":1,\"363\":1,\"548\":1}}],[\"position\",{\"1\":{\"28\":2,\"76\":1,\"92\":5,\"96\":2,\"163\":2,\"262\":2,\"268\":2,\"284\":9,\"285\":2,\"447\":1,\"477\":16,\"506\":1,\"523\":12,\"528\":2,\"529\":2,\"538\":2,\"541\":2,\"543\":2,\"544\":6,\"674\":1}}],[\"pos\",{\"1\":{\"76\":8,\"145\":7,\"147\":2,\"162\":2,\"247\":2,\"293\":5,\"296\":5,\"511\":10,\"512\":6,\"513\":9,\"514\":2,\"543\":1}}],[\"pos1d\",{\"1\":{\"70\":1,\"76\":1}}],[\"pos=self\",{\"1\":{\"70\":1,\"76\":1}}],[\"po\",{\"1\":{\"36\":6}}],[\"pooler\",{\"1\":{\"262\":3,\"393\":1,\"513\":7,\"528\":2}}],[\"pooled\",{\"1\":{\"262\":5,\"527\":4,\"528\":2,\"529\":4,\"537\":2,\"538\":2,\"540\":2,\"544\":4}}],[\"pooling\",{\"0\":{\"396\":1},\"1\":{\"92\":1,\"98\":1,\"105\":5,\"107\":1,\"112\":7,\"115\":3,\"145\":2,\"146\":1,\"147\":2,\"268\":1,\"396\":6,\"397\":1}}],[\"pool后\",{\"1\":{\"36\":1}}],[\"pool\",{\"1\":{\"36\":2,\"46\":4,\"59\":8,\"157\":1,\"513\":2,\"527\":1}}],[\"pour\",{\"1\":{\"6\":1,\"7\":1,\"23\":1,\"28\":4,\"29\":1,\"31\":2,\"32\":1,\"58\":1,\"68\":1}}],[\"该架构巧妙地整合了从庞大知识库中检索到的相关信息\",{\"1\":{\"679\":1}}],[\"该架构支持以下三种功能模式\",{\"1\":{\"126\":1}}],[\"该函数常作为优化问题的基准函数使用\",{\"1\":{\"667\":1}}],[\"该函数的形状如下图所示\",{\"1\":{\"667\":1}}],[\"该函数自动调用系统命令转换文件\",{\"1\":{\"666\":1}}],[\"该函数形式复杂\",{\"1\":{\"662\":1}}],[\"该函数作用是针对给定的图片路径\",{\"1\":{\"275\":1}}],[\"该分布会表现得像高斯分布\",{\"1\":{\"586\":1}}],[\"该\",{\"1\":{\"517\":1}}],[\"该过程由make\",{\"1\":{\"512\":1}}],[\"该流程可参见论文图\",{\"1\":{\"470\":1}}],[\"该方向在对话系统\",{\"1\":{\"469\":1}}],[\"该方法包括三个关键步骤\",{\"1\":{\"468\":1}}],[\"该方法正是clip在vlp领域发扬光大的\",{\"1\":{\"283\":1}}],[\"该方法的主要计算量都集中在模态交互上\",{\"1\":{\"255\":1}}],[\"该方法会将所有\",{\"1\":{\"249\":1}}],[\"该方法通过灵活分割图像图块\",{\"1\":{\"216\":1}}],[\"该方法通过将文本和图像标记建模为单一数据流\",{\"1\":{\"177\":1}}],[\"该方法在生成质量和泛化能力上表现优异\",{\"1\":{\"177\":1}}],[\"该方法在piad数据集上表现优异\",{\"1\":{\"48\":1}}],[\"该方法避免了传统像素级重建的局限性\",{\"1\":{\"165\":1}}],[\"该方法无需额外模型\",{\"1\":{\"150\":1}}],[\"该方法能够有效从噪声图文对中学习\",{\"1\":{\"125\":1}}],[\"该方法能更好的覆盖整个点集\",{\"1\":{\"89\":1}}],[\"该方法摆脱了对几何标注或固定场景的依赖\",{\"1\":{\"49\":1}}],[\"该方法从交互图像中进行推理\",{\"1\":{\"26\":1}}],[\"该任务要求预测句子的最后一个词\",{\"1\":{\"455\":1}}],[\"该任务主要是对一个给定句子\",{\"1\":{\"448\":1}}],[\"该设置不需要这些目标任务和无标记语料库是一个领域的\",{\"1\":{\"440\":1}}],[\"该系数也等于f1得分\",{\"1\":{\"405\":1}}],[\"该参数未在当前代码中使用\",{\"1\":{\"401\":1}}],[\"该缓冲区会包含在\",{\"1\":{\"389\":1}}],[\"该图展示了\",{\"1\":{\"327\":1}}],[\"该模块可以为后续的分类任务提供更具区分性和稳定性的特征表示\",{\"1\":{\"296\":1}}],[\"该模型在上下文\",{\"1\":{\"443\":1}}],[\"该模型在文本上处理长期依赖提供了更结构化的内存\",{\"1\":{\"440\":1}}],[\"该模型是在\",{\"1\":{\"300\":1}}],[\"该模型联合训练一个cnn和文本transformer来预测图像的文本描述\",{\"1\":{\"278\":1}}],[\"该模型的输入是一对\",{\"1\":{\"224\":1}}],[\"该模型通过将视觉基础模型扩展到60亿参数\",{\"1\":{\"180\":1}}],[\"该类的作用是将二维图像分割成多个图像块\",{\"1\":{\"291\":1}}],[\"该示例中的任务涉及8个类别\",{\"1\":{\"273\":1}}],[\"该向量即表示一个\",{\"1\":{\"247\":1}}],[\"该编码器支持密集预测任务\",{\"1\":{\"189\":1}}],[\"该范围确保局部区域的尺度是固定的\",{\"1\":{\"90\":1}}],[\"该部分首先回顾了自然语言处理\",{\"1\":{\"464\":1}}],[\"该部分包含\",{\"1\":{\"11\":1,\"12\":1}}],[\"该部位的几何属性推理\",{\"1\":{\"28\":1}}],[\"该物体常见的其他交互\",{\"1\":{\"28\":1}}],[\"该数据集大大超越了前作\",{\"1\":{\"17\":1}}],[\"该框架结合了多模态大语言模型\",{\"1\":{\"5\":1}}],[\"以调用\",{\"1\":{\"686\":1}}],[\"以防泄露\",{\"1\":{\"681\":1}}],[\"以防对齐过程中性能退化\",{\"1\":{\"470\":1}}],[\"以改善多媒体交互\",{\"1\":{\"678\":1}}],[\"以goldstein\",{\"1\":{\"666\":1}}],[\"以目录形式存在\",{\"1\":{\"661\":1}}],[\"以乘法为例\",{\"1\":{\"660\":1}}],[\"以def\",{\"1\":{\"660\":2}}],[\"以加速模型的训练和扩展其中\",{\"1\":{\"674\":1}}],[\"以加速训练并提高模型的稳定性\",{\"1\":{\"548\":1}}],[\"以加法a\",{\"1\":{\"660\":1}}],[\"以释放内存供后续计算使用\",{\"1\":{\"657\":1}}],[\"以平方函数为例\",{\"1\":{\"631\":1}}],[\"以平衡计算效率\",{\"1\":{\"481\":1}}],[\"以至于定义均值的积分并不收敛\",{\"1\":{\"587\":1}}],[\"以往为了解决不同的\",{\"1\":{\"504\":1}}],[\"以无监督的方式利用大量无标注文本\",{\"1\":{\"504\":1}}],[\"以88\",{\"1\":{\"499\":1}}],[\"以降低毒性输出倾向\",{\"1\":{\"469\":1}}],[\"以上模型的上下文长度为\",{\"1\":{\"674\":1}}],[\"以上就是variable类的新\",{\"1\":{\"652\":1}}],[\"以上下文为接口\",{\"1\":{\"461\":1}}],[\"以上代码注释中统一用b代替image\",{\"1\":{\"282\":1}}],[\"以研究性能与规模之间的关系\",{\"1\":{\"461\":1}}],[\"以研究模型容量对性能的影响\",{\"1\":{\"454\":1}}],[\"以节省\",{\"1\":{\"428\":1}}],[\"以此来模拟所谓的内在秩\",{\"1\":{\"425\":1}}],[\"以增加模型接触不同任务\",{\"1\":{\"454\":1}}],[\"以增大生成期望序列的概率\",{\"1\":{\"418\":1}}],[\"以增强理解\",{\"1\":{\"469\":1}}],[\"以增强性能\",{\"1\":{\"272\":1}}],[\"以增强\",{\"1\":{\"71\":1}}],[\"以增强泛化能力\",{\"1\":{\"19\":1}}],[\"以输入\",{\"1\":{\"411\":1}}],[\"以三个变量为例\",{\"1\":{\"355\":1}}],[\"以两个变量为例\",{\"1\":{\"355\":1}}],[\"以避免安装到错误的位置\",{\"1\":{\"338\":1}}],[\"以避免模型将匹配图文对挑选为负样本\",{\"1\":{\"284\":1}}],[\"以使得模型能够学习到最适合当前任务的位置表示\",{\"1\":{\"293\":1}}],[\"以最大程度地惩罚降低iou得分的预测结果\",{\"1\":{\"406\":1}}],[\"以最大化互信息\",{\"1\":{\"283\":1}}],[\"以最小化损失\",{\"1\":{\"292\":1}}],[\"以保留图像的空间信息\",{\"1\":{\"292\":1}}],[\"以保持与其他模型的一致性\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"以保持计算的可管理性\",{\"1\":{\"90\":1}}],[\"以保持原始模型识别能力的同时增强其推理能力\",{\"1\":{\"10\":1}}],[\"以vit\",{\"1\":{\"291\":1}}],[\"以bos\",{\"1\":{\"286\":1}}],[\"以利用\",{\"1\":{\"286\":1}}],[\"以细粒度对齐\",{\"1\":{\"284\":1}}],[\"以弥合模态差距\",{\"1\":{\"281\":1}}],[\"以每个文本描述为一行\",{\"1\":{\"276\":1}}],[\"以获取图像特征\",{\"1\":{\"273\":1}}],[\"以获得更强的局部几何感知能力\",{\"1\":{\"96\":1}}],[\"以生成相应的文本特征\",{\"1\":{\"273\":1}}],[\"以分析不同训练策略的影响\",{\"1\":{\"229\":1}}],[\"以奖励模型为环境反馈信号\",{\"1\":{\"470\":1}}],[\"以奖励模型为\",{\"1\":{\"224\":1}}],[\"以促进mllm社区发展\",{\"1\":{\"208\":1}}],[\"以促进视觉\",{\"1\":{\"138\":1}}],[\"以提高数据质量和多样性\",{\"1\":{\"674\":1}}],[\"以提高效率\",{\"1\":{\"190\":1}}],[\"以提升大批次训练的稳定性\",{\"1\":{\"494\":1}}],[\"以提升效率\",{\"1\":{\"461\":1}}],[\"以提升词表利用率\",{\"1\":{\"178\":1}}],[\"以\",{\"0\":{\"315\":1},\"1\":{\"159\":1,\"290\":1,\"447\":1,\"673\":1}}],[\"以下模型的上下文长度为\",{\"1\":{\"674\":1}}],[\"以下情况会增加引用计数\",{\"1\":{\"657\":1}}],[\"以下小节总结了其中一些常见分布\",{\"1\":{\"573\":1}}],[\"以下内容来自\",{\"1\":{\"395\":1}}],[\"以下引用clip论文图做说明\",{\"1\":{\"283\":1}}],[\"以下为动量编码器分支\",{\"1\":{\"161\":1}}],[\"以下首先给出的是\",{\"1\":{\"147\":1}}],[\"以下是大语言模型的一些主要特点\",{\"1\":{\"675\":1}}],[\"以下是你提供内容的逐段翻译与解释\",{\"1\":{\"590\":1}}],[\"以下是具体分析\",{\"1\":{\"395\":1}}],[\"以下是一个官方给出的clip模型的示例\",{\"1\":{\"273\":1}}],[\"以下是两者的主要区别\",{\"1\":{\"231\":1}}],[\"以下是论文\",{\"1\":{\"134\":1}}],[\"以下是对\",{\"1\":{\"129\":1,\"469\":1}}],[\"以下代码是我自己写的一个测试代码\",{\"1\":{\"83\":1}}],[\"以计算以下三种损失\",{\"1\":{\"127\":1}}],[\"以实现生成任务\",{\"1\":{\"126\":1}}],[\"以实现最佳的性能\",{\"1\":{\"96\":1}}],[\"以及每一个功能的大体实现逻辑\",{\"1\":{\"687\":1}}],[\"以及即将推出的\",{\"1\":{\"674\":1}}],[\"以及对错误数据\",{\"1\":{\"687\":1}}],[\"以及对\",{\"1\":{\"674\":1}}],[\"以及add\",{\"1\":{\"661\":1}}],[\"以及函数等更复杂的样本空间\",{\"1\":{\"566\":1}}],[\"以及特定群体优先原则\",{\"1\":{\"472\":1}}],[\"以及一系列元数据\",{\"1\":{\"470\":1}}],[\"以及缺乏透明性等\",{\"1\":{\"463\":1}}],[\"以及如何高效地进行多次调用和推理\",{\"1\":{\"684\":1}}],[\"以及如何更好地利用其隐含学习到的多任务能力\",{\"1\":{\"457\":1}}],[\"以及如何通过局部特征学习器\",{\"1\":{\"86\":1}}],[\"以及研究双向表示\",{\"1\":{\"456\":1}}],[\"以及分割字符嵌入矩阵\",{\"1\":{\"444\":1}}],[\"以及语言模型来提升标记的语义角色\",{\"1\":{\"441\":1}}],[\"以及prompt的长度\",{\"1\":{\"434\":1}}],[\"以及训练的方法的角度\",{\"1\":{\"416\":1}}],[\"以及seq\",{\"1\":{\"282\":1}}],[\"以及\",{\"1\":{\"217\":1,\"454\":1,\"674\":1}}],[\"以及在多模态编码器上进行掩码语言模型\",{\"1\":{\"153\":1}}],[\"以及数据集自举策略\",{\"1\":{\"125\":1}}],[\"以适配后面的卷积操作\",{\"1\":{\"100\":1}}],[\"以便开发各种下游应用\",{\"1\":{\"682\":1}}],[\"以便生成模型可以更好地理解和使用\",{\"1\":{\"680\":1}}],[\"以便一起处理\",{\"1\":{\"544\":1}}],[\"以便送入全连接层进行分类和回归\",{\"1\":{\"397\":1}}],[\"以便\",{\"1\":{\"282\":1}}],[\"以便和特征相乘\",{\"1\":{\"100\":1}}],[\"以便可以在这些分区上独立地学习特征\",{\"1\":{\"86\":1}}],[\"以便可以在这些区域上应用局部操作\",{\"1\":{\"86\":1}}],[\"以结合来自不同尺度的特征\",{\"1\":{\"95\":1}}],[\"以反映其相对位置\",{\"1\":{\"91\":1}}],[\"以限制每个局部区域中考虑的点的数量\",{\"1\":{\"90\":1}}],[\"以产生一个更少元素的新集合\",{\"1\":{\"88\":1}}],[\"以二维欧几里得空间为例\",{\"1\":{\"87\":1}}],[\"以确保问题多样性和语义丰富性\",{\"1\":{\"63\":1}}],[\"以支持训练阶段的匹配与推理\",{\"1\":{\"18\":1}}],[\"以开放词汇的方式定位3d物体的功能区域\",{\"1\":{\"5\":1}}],[\"其技术核心点虽然在大语言模型上\",{\"1\":{\"686\":1}}],[\"其涵盖了模型的输入与输出处理\",{\"1\":{\"684\":1}}],[\"其工作流程可以简单地分为数据处理\",{\"1\":{\"680\":1}}],[\"其性能可与具备\",{\"1\":{\"674\":1}}],[\"其性能与商业模型\",{\"1\":{\"207\":1}}],[\"其最大值可能不在梯度指示方向\",{\"1\":{\"667\":1}}],[\"其最后的全连接层\",{\"1\":{\"247\":1}}],[\"其式子为\",{\"1\":{\"667\":1}}],[\"其支持节点和箭头构成的数据结构可视化\",{\"1\":{\"666\":1}}],[\"其类型可能是原生数值\",{\"1\":{\"660\":1}}],[\"其反向传播逻辑为将上游梯度原封不动地传递给两个输入变量\",{\"1\":{\"660\":1}}],[\"其导数公式为\",{\"1\":{\"660\":1}}],[\"其导数依赖于输出值\",{\"1\":{\"630\":1}}],[\"其导数\",{\"1\":{\"630\":2}}],[\"其\",{\"1\":{\"591\":1}}],[\"其形式为\",{\"1\":{\"587\":1}}],[\"其形状是\",{\"1\":{\"323\":1}}],[\"其形状和大小保持不变的运动方式\",{\"1\":{\"116\":1}}],[\"其概率密度函数可表示为\",{\"1\":{\"590\":1}}],[\"其概率密度函数为\",{\"1\":{\"585\":1}}],[\"其概率质量函数定义为\",{\"1\":{\"579\":1}}],[\"其它元素为\",{\"1\":{\"576\":1}}],[\"其它方法都有各自的一些问题\",{\"1\":{\"424\":1}}],[\"其定义如下\",{\"1\":{\"566\":1}}],[\"其定义为\",{\"1\":{\"346\":1}}],[\"其三个面分别标记为\",{\"1\":{\"565\":1}}],[\"其由七大主要部分构成\",{\"1\":{\"548\":1}}],[\"其掩码数量可能会偏少\",{\"1\":{\"514\":1}}],[\"其架构为\",{\"1\":{\"504\":1}}],[\"其预训练任务包括\",{\"1\":{\"493\":1}}],[\"其设计始终围绕推理效率目标\",{\"1\":{\"481\":1}}],[\"其特点在于仅使用公开可用的数据集进行训练\",{\"1\":{\"479\":1}}],[\"其任务形式和风格可能代表一类高频商业用途\",{\"1\":{\"472\":1}}],[\"其回答\",{\"1\":{\"471\":1}}],[\"其输出也比175b的原始gpt\",{\"1\":{\"468\":1}}],[\"其输出支持密集特征图\",{\"1\":{\"188\":1}}],[\"其表现呈现出高度任务依赖性\",{\"1\":{\"463\":1}}],[\"其表现多次逼近甚至超越传统fine\",{\"1\":{\"462\":1}}],[\"其表达能力是有限的\",{\"1\":{\"294\":1}}],[\"其局限性\",{\"1\":{\"460\":1}}],[\"其优势包括\",{\"1\":{\"454\":1}}],[\"其关键特点是\",{\"1\":{\"454\":1}}],[\"其将结构化文本输入处理为单一的连续字符序列\",{\"1\":{\"440\":1}}],[\"其维度分别为\",{\"1\":{\"423\":1}}],[\"其目标是为图像中的每个像素分配一个特定的语义类别标签\",{\"1\":{\"399\":1}}],[\"其目标是让预训练语言模型\",{\"1\":{\"224\":1}}],[\"其曲线下面积\",{\"1\":{\"351\":1}}],[\"其数学定义为\",{\"1\":{\"343\":1,\"345\":1}}],[\"其下标索引公式可表示为\",{\"1\":{\"325\":1}}],[\"其在内存中列优先布局\",{\"1\":{\"322\":1}}],[\"其在内存中行优先布局\",{\"1\":{\"322\":1}}],[\"其在冻结骨干网络的前提下\",{\"1\":{\"193\":1}}],[\"其均能匹配到正确的文本标签\",{\"1\":{\"273\":1}}],[\"其规模与gpt\",{\"1\":{\"272\":1}}],[\"其主要改进包括\",{\"1\":{\"496\":1}}],[\"其主要用于\",{\"1\":{\"268\":1}}],[\"其主要功能是\",{\"1\":{\"101\":1}}],[\"其是首个使用patch\",{\"1\":{\"253\":1}}],[\"其实也就是按照拓扑排序的方式去遍历计算图\",{\"1\":{\"655\":1}}],[\"其实等价于问\",{\"1\":{\"565\":1}}],[\"其实在某些训练集里\",{\"1\":{\"508\":1}}],[\"其实这本质上还是个分类问题\",{\"1\":{\"508\":1}}],[\"其实该过程中执行了n次推理过程\",{\"1\":{\"474\":1}}],[\"其实那些负样本很有可能是潜在的正样本\",{\"1\":{\"240\":1}}],[\"其实就可以用在对比学习上\",{\"1\":{\"240\":1}}],[\"其实有一个很有意思的点\",{\"1\":{\"235\":1}}],[\"其次是准备训练数据\",{\"1\":{\"519\":1}}],[\"其次\",{\"1\":{\"238\":1,\"275\":1,\"420\":1,\"449\":1,\"457\":1}}],[\"其已有的知识\",{\"1\":{\"231\":1}}],[\"其开源的模型权重和研究方法为多模态ai发展提供了重要基准\",{\"1\":{\"222\":1}}],[\"其模块化设计\",{\"1\":{\"217\":1}}],[\"其参数量提升42倍\",{\"1\":{\"188\":1}}],[\"其证据下界\",{\"1\":{\"173\":1}}],[\"其核心功能是保存和管理数据\",{\"1\":{\"606\":1}}],[\"其核心思想是通过调整难易样本的权重\",{\"1\":{\"404\":1}}],[\"其核心思想借鉴了bert的掩码语言建模任务\",{\"1\":{\"165\":1}}],[\"其核心改进包括以下三点\",{\"1\":{\"207\":1}}],[\"其核心挑战在于\",{\"1\":{\"166\":1}}],[\"其核心目标是将来自不同模态\",{\"1\":{\"59\":1}}],[\"其余部分无论输出什么东西\",{\"1\":{\"505\":1}}],[\"其余部分不做损失\",{\"1\":{\"505\":1,\"514\":1}}],[\"其余部分共享\",{\"1\":{\"134\":1}}],[\"其余100万\",{\"1\":{\"240\":1}}],[\"其余的key离query远\",{\"1\":{\"238\":1}}],[\"其余的照片又分为了很多类别\",{\"1\":{\"235\":1}}],[\"其余参数冻结\",{\"1\":{\"231\":1}}],[\"其余参数保持冻结\",{\"1\":{\"10\":1}}],[\"其余位置设为\",{\"1\":{\"163\":1}}],[\"其两大创新点\",{\"1\":{\"120\":1}}],[\"其他运算符绑定\",{\"1\":{\"661\":1}}],[\"其他路径上的梯度信息将丢失\",{\"1\":{\"654\":1}}],[\"其他代码\",{\"1\":{\"634\":1,\"642\":1,\"643\":1}}],[\"其他可推出的结论\",{\"1\":{\"567\":1}}],[\"其他下游任务\",{\"0\":{\"539\":1}}],[\"其他模型2tb\",{\"1\":{\"482\":1}}],[\"其他数据如wikipedia\",{\"1\":{\"481\":1}}],[\"其他全部相同\",{\"1\":{\"474\":1}}],[\"其他位置必须\",{\"1\":{\"387\":1}}],[\"其他权重全部冻结\",{\"1\":{\"300\":1}}],[\"其他视频里的帧是负样本\",{\"1\":{\"235\":1}}],[\"其他两类为单轮对话\",{\"1\":{\"227\":1}}],[\"其他语言任务表现较差\",{\"1\":{\"208\":1}}],[\"其他为0\",{\"1\":{\"161\":1}}],[\"其他方法倾向于错误地预测为训练集中频繁出现的\",{\"1\":{\"23\":1}}],[\"其他开源许可网站\",{\"1\":{\"17\":1}}],[\"其图像数量是前者的三倍\",{\"1\":{\"17\":1}}],[\"其中一个备受关注的项目就是\",{\"1\":{\"682\":1}}],[\"其中一个类别出现的频率非常低\",{\"1\":{\"343\":1}}],[\"其中和是常数\",{\"1\":{\"667\":1}}],[\"其中的概率和为\",{\"1\":{\"596\":1}}],[\"其中的layers就是transformer\",{\"1\":{\"297\":1}}],[\"其中每一个事件就是事件空间中的一个元素\",{\"1\":{\"565\":1}}],[\"其中每个矩形的高表示相对计算量大小\",{\"1\":{\"255\":1}}],[\"其中训练数据使用1k\",{\"1\":{\"519\":1}}],[\"其中关于bertencoders编码并输出结果的整个过程如下图所示\",{\"1\":{\"513\":1}}],[\"其中4项任务\",{\"1\":{\"499\":1}}],[\"其中80\",{\"1\":{\"495\":1}}],[\"其中sst\",{\"1\":{\"448\":1}}],[\"其中single\",{\"1\":{\"256\":1}}],[\"其中秩\",{\"1\":{\"425\":1}}],[\"其中第三步通过反向传播全量更新模型参数的过程如下\",{\"1\":{\"423\":1}}],[\"其中包含多个形状相同的\",{\"1\":{\"381\":1}}],[\"其中包括注意力可视化\",{\"1\":{\"298\":1}}],[\"其中包括两个理解任务和一个生成任务\",{\"1\":{\"127\":1}}],[\"其中您感兴趣的罕见云彩类型\",{\"1\":{\"342\":1}}],[\"其中0标志位表示word\",{\"1\":{\"257\":1}}],[\"其中word\",{\"1\":{\"257\":1}}],[\"其中region\",{\"1\":{\"256\":1}}],[\"其中基础图块大小为448×448\",{\"1\":{\"215\":1}}],[\"其中词汇表\",{\"1\":{\"170\":1}}],[\"其中默认使用\",{\"1\":{\"131\":1}}],[\"其中额外添加的\",{\"1\":{\"126\":1}}],[\"其中\",{\"1\":{\"8\":2,\"14\":1,\"15\":1,\"28\":1,\"54\":1,\"64\":1,\"72\":2,\"73\":1,\"74\":1,\"82\":2,\"98\":1,\"105\":1,\"154\":1,\"155\":1,\"156\":1,\"157\":1,\"169\":1,\"171\":2,\"178\":1,\"207\":1,\"226\":1,\"227\":2,\"244\":1,\"270\":1,\"272\":1,\"291\":1,\"305\":1,\"327\":1,\"358\":1,\"359\":1,\"396\":1,\"401\":1,\"402\":2,\"403\":1,\"404\":2,\"405\":1,\"407\":2,\"423\":1,\"425\":1,\"470\":1,\"493\":1,\"564\":1,\"565\":1,\"566\":1,\"567\":1,\"571\":1,\"575\":2,\"576\":3,\"577\":1,\"578\":1,\"579\":1,\"584\":1,\"586\":1,\"587\":1,\"590\":2,\"591\":1,\"594\":1,\"661\":1,\"666\":1,\"674\":1}}],[\"5平均分刷新sota\",{\"1\":{\"499\":1}}],[\"5点\",{\"1\":{\"455\":1}}],[\"5b模型仍欠拟合\",{\"1\":{\"455\":1}}],[\"5b模型\",{\"1\":{\"455\":1}}],[\"5b参数模型在多数任务上逼近或超越监督基线\",{\"1\":{\"455\":1}}],[\"5b参数\",{\"1\":{\"455\":1}}],[\"5b\",{\"1\":{\"454\":2,\"674\":2}}],[\"5是五分类\",{\"1\":{\"448\":1}}],[\"5的情感极性区分的更细致\",{\"1\":{\"448\":1}}],[\"5的概率将文本对应的图片替换成不同的图片\",{\"1\":{\"258\":1}}],[\"5的开发基于internvit\",{\"1\":{\"215\":1}}],[\"5作为开源多模态大语言模型\",{\"1\":{\"222\":1}}],[\"5v\",{\"1\":{\"210\":1}}],[\"5版本模型具备了强大的鲁棒性\",{\"1\":{\"215\":1}}],[\"5版本\",{\"1\":{\"210\":1}}],[\"5在18个多模态基准测试中表现优异\",{\"1\":{\"208\":1}}],[\"595\",{\"1\":{\"226\":1}}],[\"59\",{\"1\":{\"195\":1,\"674\":1}}],[\"5500\",{\"1\":{\"520\":2}}],[\"55\",{\"1\":{\"193\":1,\"355\":1}}],[\"558k\",{\"1\":{\"202\":1}}],[\"558\",{\"1\":{\"75\":1}}],[\"5亿图文对\",{\"1\":{\"177\":1}}],[\"5e\",{\"1\":{\"174\":1,\"201\":1,\"447\":1}}],[\"5376\",{\"1\":{\"662\":1}}],[\"53\",{\"1\":{\"83\":1,\"217\":1,\"227\":1,\"228\":1}}],[\"504\",{\"1\":{\"515\":1}}],[\"50k词汇表\",{\"1\":{\"495\":1}}],[\"50000\",{\"1\":{\"667\":1}}],[\"500k步时\",{\"1\":{\"498\":1}}],[\"500\",{\"1\":{\"83\":3}}],[\"50\",{\"1\":{\"83\":1,\"105\":1,\"190\":1,\"200\":1,\"351\":2,\"455\":1,\"511\":2,\"512\":3,\"674\":1}}],[\"51\",{\"1\":{\"596\":1}}],[\"516\",{\"1\":{\"65\":1}}],[\"512k\",{\"1\":{\"674\":1}}],[\"512\",{\"1\":{\"30\":6,\"34\":11,\"35\":19,\"36\":11,\"41\":1,\"46\":6,\"59\":32,\"93\":6,\"96\":5,\"99\":4,\"101\":1,\"107\":3,\"110\":3,\"111\":3,\"202\":1,\"447\":1,\"493\":1,\"494\":1}}],[\"58\",{\"1\":{\"63\":2,\"65\":1,\"67\":1,\"193\":1,\"227\":1}}],[\"5400\",{\"1\":{\"673\":1}}],[\"540b的53\",{\"1\":{\"482\":1}}],[\"540b\",{\"1\":{\"482\":4,\"673\":1}}],[\"540b等顶尖模型表现相当\",{\"1\":{\"479\":1}}],[\"54\",{\"1\":{\"22\":1}}],[\"5250\",{\"1\":{\"515\":1}}],[\"52\",{\"1\":{\"22\":1,\"482\":1}}],[\"56\",{\"1\":{\"17\":1,\"20\":1,\"22\":1}}],[\"57\",{\"1\":{\"17\":1,\"482\":1}}],[\"5\",{\"0\":{\"22\":1,\"23\":1,\"24\":1,\"25\":1,\"66\":1,\"206\":1,\"266\":1,\"294\":1},\"1\":{\"10\":1,\"17\":1,\"26\":1,\"28\":1,\"30\":1,\"35\":1,\"36\":1,\"40\":1,\"41\":1,\"45\":1,\"58\":1,\"59\":13,\"63\":1,\"76\":1,\"78\":3,\"80\":1,\"81\":1,\"82\":2,\"83\":2,\"96\":1,\"98\":1,\"101\":1,\"131\":1,\"145\":6,\"147\":1,\"159\":2,\"160\":1,\"161\":1,\"163\":3,\"188\":1,\"193\":1,\"195\":2,\"196\":1,\"200\":1,\"201\":1,\"202\":1,\"207\":2,\"208\":1,\"212\":1,\"214\":1,\"215\":1,\"217\":1,\"219\":2,\"229\":1,\"247\":1,\"273\":1,\"274\":1,\"278\":1,\"285\":1,\"286\":1,\"289\":1,\"290\":19,\"295\":1,\"300\":1,\"321\":2,\"322\":2,\"323\":3,\"325\":5,\"326\":7,\"328\":1,\"351\":2,\"353\":1,\"370\":1,\"387\":3,\"395\":1,\"397\":4,\"404\":3,\"405\":4,\"407\":4,\"410\":1,\"439\":1,\"440\":2,\"447\":2,\"448\":1,\"454\":1,\"455\":5,\"460\":1,\"461\":1,\"462\":3,\"463\":1,\"464\":1,\"470\":1,\"474\":1,\"477\":2,\"480\":2,\"481\":4,\"482\":4,\"483\":2,\"484\":1,\"492\":1,\"494\":2,\"513\":2,\"519\":1,\"523\":1,\"566\":1,\"569\":1,\"590\":1,\"617\":1,\"659\":1,\"674\":18}}],[\"echo\",{\"1\":{\"678\":1}}],[\"edge\",{\"1\":{\"666\":4}}],[\"e=3\",{\"1\":{\"508\":2}}],[\"efficient\",{\"1\":{\"416\":1,\"418\":1,\"421\":2,\"474\":17,\"478\":1}}],[\"eow\",{\"1\":{\"412\":3}}],[\"eol\",{\"1\":{\"411\":2,\"412\":3}}],[\"eos\",{\"1\":{\"143\":2,\"286\":1,\"474\":2,\"477\":2}}],[\"ee\",{\"1\":{\"410\":1}}],[\"escape\",{\"1\":{\"410\":1,\"412\":1}}],[\"estimation\",{\"1\":{\"240\":2}}],[\"e^\",{\"1\":{\"404\":1}}],[\"euclidean\",{\"0\":{\"357\":1}}],[\"e5005f0a\",{\"1\":{\"300\":1}}],[\"equals\",{\"1\":{\"558\":1}}],[\"equivariance\",{\"1\":{\"287\":1}}],[\"eq\",{\"1\":{\"145\":3,\"296\":1,\"517\":1}}],[\"et\",{\"1\":{\"126\":5,\"127\":2,\"131\":4,\"454\":3,\"469\":17,\"470\":2,\"472\":2,\"481\":1,\"483\":1,\"485\":10}}],[\"eta\",{\"1\":{\"80\":1}}],[\"eye\",{\"1\":{\"108\":1}}],[\"epsilon\",{\"1\":{\"296\":1}}],[\"eps\",{\"1\":{\"265\":1,\"268\":1,\"407\":5,\"523\":1,\"525\":1,\"532\":1,\"535\":1,\"622\":3}}],[\"eps=config\",{\"1\":{\"265\":1,\"268\":1,\"523\":1,\"525\":1,\"532\":1,\"535\":1}}],[\"eps=1e\",{\"1\":{\"80\":1,\"160\":2,\"296\":1,\"407\":1,\"622\":1,\"646\":1}}],[\"epochs=4\",{\"1\":{\"519\":1}}],[\"epochs\",{\"1\":{\"147\":1,\"244\":1,\"495\":1,\"514\":2}}],[\"epoch>0\",{\"1\":{\"145\":1,\"159\":1}}],[\"epoch+1\",{\"1\":{\"81\":1}}],[\"epoch\",{\"1\":{\"10\":1,\"80\":1,\"81\":2,\"82\":2,\"83\":1,\"136\":2,\"142\":5,\"145\":4,\"147\":6,\"159\":3,\"244\":5,\"296\":2,\"470\":1,\"514\":4}}],[\"each\",{\"1\":{\"145\":2,\"147\":2,\"412\":1,\"536\":1,\"554\":1}}],[\"easy\",{\"1\":{\"78\":1,\"404\":1}}],[\"earphone\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"einstein\",{\"1\":{\"76\":1,\"247\":1,\"390\":1}}],[\"einsum\",{\"0\":{\"390\":1},\"1\":{\"70\":1,\"76\":5,\"247\":2,\"390\":4}}],[\"ema\",{\"1\":{\"147\":2,\"160\":2,\"161\":1}}],[\"employ\",{\"1\":{\"78\":1}}],[\"empty\",{\"1\":{\"68\":1}}],[\"embed\",{\"1\":{\"73\":7,\"76\":3,\"145\":7,\"146\":3,\"147\":7,\"160\":14,\"161\":2,\"272\":2,\"291\":9,\"292\":13,\"293\":15,\"295\":9,\"296\":17,\"300\":1,\"477\":6,\"549\":8}}],[\"embeds则拼接\",{\"1\":{\"284\":1}}],[\"embeds=query\",{\"1\":{\"282\":1,\"284\":1,\"285\":2,\"286\":1}}],[\"embeds=encoder\",{\"1\":{\"163\":1}}],[\"embeds=inputs\",{\"1\":{\"163\":1}}],[\"embeds=none\",{\"1\":{\"163\":2,\"284\":1,\"285\":1}}],[\"embeds=llm\",{\"1\":{\"40\":1}}],[\"embeds\",{\"1\":{\"40\":4,\"43\":10,\"142\":3,\"143\":5,\"145\":16,\"146\":4,\"147\":16,\"161\":10,\"162\":22,\"163\":4,\"282\":4,\"284\":16,\"285\":3,\"286\":6,\"477\":7}}],[\"embedding相同维度\",{\"1\":{\"286\":1}}],[\"embedding计算出key和value\",{\"1\":{\"285\":1}}],[\"embedding送到二分类器中\",{\"1\":{\"284\":1}}],[\"embedding分别都嵌入了一个额外的可学习\",{\"1\":{\"257\":1}}],[\"embedding部分\",{\"1\":{\"257\":2}}],[\"embedding标志位来区分\",{\"1\":{\"257\":1}}],[\"embedding通过可学习的modal\",{\"1\":{\"257\":1}}],[\"embedding和visual\",{\"1\":{\"257\":2}}],[\"embedding和modality\",{\"1\":{\"255\":1}}],[\"embedding进行concate\",{\"1\":{\"257\":2}}],[\"embedding进行相加\",{\"1\":{\"257\":2}}],[\"embedding是现有vlp模型的瓶颈\",{\"1\":{\"256\":1}}],[\"embedding存在着差异\",{\"1\":{\"256\":1}}],[\"embedding基本上都使用类bert结构\",{\"1\":{\"256\":1}}],[\"embedding的方法总共有三大类\",{\"1\":{\"256\":1}}],[\"embedding的方法\",{\"1\":{\"253\":1,\"256\":1}}],[\"embeddings前面\",{\"1\":{\"286\":1}}],[\"embeddings和query\",{\"1\":{\"284\":1}}],[\"embeddings\",{\"1\":{\"40\":1,\"43\":1,\"76\":2,\"143\":1,\"145\":2,\"147\":2,\"162\":2,\"163\":1,\"262\":1,\"267\":1,\"275\":12,\"276\":4,\"277\":15,\"282\":3,\"284\":17,\"513\":1,\"523\":23,\"527\":1,\"528\":2,\"536\":1,\"538\":1}}],[\"embedding\",{\"1\":{\"35\":1,\"40\":1,\"43\":1,\"45\":1,\"46\":1,\"76\":1,\"255\":3,\"257\":4,\"262\":2,\"275\":4,\"276\":1,\"277\":3,\"284\":1,\"285\":1,\"291\":1,\"293\":1,\"506\":5,\"513\":3,\"523\":3,\"528\":2,\"674\":1,\"683\":1}}],[\"emb\",{\"1\":{\"30\":1,\"34\":6,\"35\":12,\"36\":16,\"41\":14,\"42\":4,\"44\":2,\"45\":20,\"46\":14,\"59\":41,\"70\":3,\"80\":2,\"83\":2,\"276\":2,\"277\":2,\"513\":1}}],[\"events\",{\"1\":{\"568\":1}}],[\"every\",{\"1\":{\"159\":1,\"289\":5,\"412\":1}}],[\"eva\",{\"1\":{\"183\":1,\"193\":1}}],[\"evaluation\",{\"1\":{\"470\":1}}],[\"evaluating\",{\"1\":{\"82\":1}}],[\"evaluate\",{\"1\":{\"82\":1,\"514\":1}}],[\"eval\",{\"1\":{\"28\":1,\"82\":1,\"83\":1,\"474\":1,\"477\":1,\"519\":2}}],[\"ev\",{\"1\":{\"45\":2}}],[\"ek\",{\"1\":{\"45\":2}}],[\"e\",{\"1\":{\"40\":1,\"45\":7,\"59\":10,\"147\":10,\"270\":3,\"272\":6,\"275\":4,\"276\":2,\"277\":6,\"410\":5,\"470\":2,\"508\":2,\"600\":2,\"666\":1}}],[\"exchange\",{\"1\":{\"481\":1}}],[\"exception\",{\"1\":{\"275\":2,\"276\":1,\"277\":3}}],[\"except\",{\"1\":{\"275\":2,\"276\":1,\"277\":3,\"666\":1}}],[\"exist\",{\"1\":{\"276\":1,\"277\":1,\"289\":1,\"510\":2}}],[\"exists\",{\"1\":{\"275\":1,\"276\":1,\"277\":2,\"289\":1,\"511\":1,\"514\":1,\"666\":1}}],[\"extend\",{\"1\":{\"510\":1}}],[\"extended\",{\"1\":{\"262\":1,\"285\":1,\"528\":6}}],[\"extension\",{\"1\":{\"275\":2,\"277\":2,\"666\":3}}],[\"extract\",{\"1\":{\"29\":2,\"58\":1}}],[\"examples\",{\"1\":{\"78\":2,\"404\":2}}],[\"export\",{\"1\":{\"519\":1}}],[\"exponential\",{\"1\":{\"157\":1}}],[\"experimental\",{\"1\":{\"469\":1}}],[\"experiments\",{\"0\":{\"130\":1},\"1\":{\"129\":1}}],[\"exp\",{\"1\":{\"272\":1,\"404\":6,\"617\":5,\"641\":2}}],[\"expansion\",{\"1\":{\"73\":4}}],[\"expanduser\",{\"1\":{\"666\":1}}],[\"expanded\",{\"1\":{\"387\":1}}],[\"expand\",{\"0\":{\"387\":1},\"1\":{\"46\":1,\"59\":1,\"282\":1,\"284\":1,\"286\":1,\"292\":1,\"293\":1,\"296\":1,\"385\":1,\"387\":8,\"513\":1,\"517\":1,\"523\":1}}],[\"explain\",{\"1\":{\"28\":2}}],[\"enable\",{\"1\":{\"658\":5}}],[\"enables\",{\"1\":{\"436\":1}}],[\"entire\",{\"1\":{\"531\":1}}],[\"entropy\",{\"1\":{\"78\":1,\"145\":1,\"147\":1,\"162\":1,\"163\":1,\"272\":3,\"283\":2,\"284\":1,\"402\":5,\"404\":4,\"407\":2,\"471\":1}}],[\"enginnering\",{\"1\":{\"686\":1}}],[\"engineering技巧的时候\",{\"1\":{\"434\":1}}],[\"engineering技术\",{\"1\":{\"142\":1}}],[\"engineering呢\",{\"1\":{\"430\":1}}],[\"engineering的技巧\",{\"1\":{\"434\":1,\"435\":1}}],[\"engineering的原因\",{\"1\":{\"430\":1}}],[\"engineering的实践表明\",{\"1\":{\"419\":1}}],[\"engineering的效果达不到要求\",{\"1\":{\"415\":1}}],[\"engineering的方式\",{\"1\":{\"415\":1}}],[\"engineering的方式是一种相对来说容易上手的使用大模型的方式\",{\"1\":{\"415\":1}}],[\"engineering\",{\"0\":{\"429\":1,\"430\":1},\"1\":{\"274\":1,\"415\":1,\"429\":1,\"430\":1,\"433\":1,\"686\":3,\"687\":4}}],[\"english\",{\"1\":{\"454\":3}}],[\"environments\",{\"1\":{\"334\":1}}],[\"envs\",{\"1\":{\"334\":3}}],[\"env\",{\"1\":{\"334\":1,\"335\":1}}],[\"ensemble\",{\"0\":{\"228\":1},\"1\":{\"228\":1}}],[\"ensure\",{\"1\":{\"63\":1,\"410\":3,\"412\":3,\"510\":2}}],[\"end\",{\"1\":{\"225\":2,\"275\":4,\"277\":4,\"412\":1,\"540\":12,\"541\":14,\"542\":5}}],[\"en\",{\"1\":{\"190\":1,\"194\":1,\"217\":1}}],[\"enc\",{\"1\":{\"145\":1,\"147\":1,\"513\":2}}],[\"encoded\",{\"1\":{\"239\":1,\"520\":7}}],[\"encode\",{\"1\":{\"126\":1,\"273\":2,\"474\":1,\"477\":1,\"511\":3,\"549\":2}}],[\"encoder模型结构图\",{\"1\":{\"554\":1}}],[\"encoderdecoder\",{\"1\":{\"549\":2}}],[\"encoderdecoder模型结构图\",{\"1\":{\"549\":1}}],[\"encoder中mlp\",{\"1\":{\"297\":1}}],[\"encoder中重复堆叠encoder\",{\"1\":{\"297\":1}}],[\"encoder输出结果之后\",{\"1\":{\"296\":1}}],[\"encoder输出的embeddings里提取与input\",{\"1\":{\"282\":1}}],[\"encoder的结构\",{\"1\":{\"294\":1}}],[\"encoderlayer模型结构图\",{\"1\":{\"553\":1}}],[\"encoderlayer\",{\"0\":{\"553\":1},\"1\":{\"285\":1,\"513\":1,\"553\":2}}],[\"encoder提取的图像embeddings\",{\"1\":{\"282\":1}}],[\"encoder提取图像特征\",{\"1\":{\"273\":1}}],[\"encoder引到vision\",{\"1\":{\"281\":1}}],[\"encoder不参与梯度运算\",{\"1\":{\"246\":1}}],[\"encoder进行初始化\",{\"1\":{\"246\":1}}],[\"encoder参数使用query\",{\"1\":{\"246\":1}}],[\"encoders\",{\"1\":{\"246\":1,\"513\":2}}],[\"encoder2\",{\"1\":{\"30\":1}}],[\"encoder\",{\"0\":{\"126\":1,\"294\":1,\"549\":1,\"551\":1,\"554\":1},\"1\":{\"30\":3,\"35\":3,\"40\":2,\"46\":3,\"59\":5,\"70\":1,\"80\":2,\"99\":1,\"120\":1,\"126\":4,\"127\":2,\"138\":1,\"142\":5,\"143\":4,\"145\":24,\"146\":7,\"147\":26,\"160\":13,\"161\":5,\"162\":15,\"163\":17,\"165\":1,\"246\":8,\"247\":4,\"248\":5,\"255\":1,\"262\":5,\"263\":5,\"264\":4,\"265\":4,\"266\":6,\"267\":2,\"268\":4,\"272\":8,\"273\":4,\"280\":2,\"282\":4,\"284\":2,\"285\":20,\"286\":5,\"292\":1,\"504\":3,\"528\":2,\"542\":3,\"548\":2,\"549\":4,\"554\":3}}],[\"encoding=\",{\"1\":{\"410\":4,\"411\":1,\"412\":7,\"510\":3,\"511\":1}}],[\"encoding\",{\"0\":{\"13\":1},\"1\":{\"70\":3,\"409\":1,\"454\":1,\"495\":1,\"548\":1}}],[\"enqueue\",{\"0\":{\"249\":1},\"1\":{\"145\":1,\"147\":1,\"161\":1,\"247\":2,\"249\":2}}],[\"enrichment\",{\"1\":{\"63\":1}}],[\"enhance\",{\"1\":{\"41\":6,\"59\":4}}],[\"enumerate\",{\"1\":{\"29\":1,\"68\":2,\"81\":1,\"82\":1,\"83\":1,\"92\":1,\"96\":1,\"100\":1,\"159\":1,\"244\":1,\"275\":1,\"277\":1,\"289\":2,\"296\":1,\"477\":1,\"511\":2,\"512\":1,\"514\":1,\"526\":1}}],[\"ele\",{\"1\":{\"514\":4}}],[\"element\",{\"1\":{\"327\":1}}],[\"elmo\",{\"1\":{\"447\":1,\"448\":1,\"464\":1}}],[\"elicits\",{\"1\":{\"434\":1}}],[\"elif\",{\"1\":{\"146\":1,\"285\":1,\"477\":1,\"511\":1}}],[\"eldan\",{\"1\":{\"395\":1}}],[\"elb\",{\"1\":{\"178\":1}}],[\"elbo\",{\"1\":{\"173\":1}}],[\"else\",{\"1\":{\"29\":5,\"40\":2,\"43\":1,\"58\":4,\"68\":3,\"72\":2,\"74\":2,\"82\":1,\"83\":1,\"92\":4,\"93\":2,\"96\":3,\"100\":2,\"109\":2,\"143\":1,\"145\":2,\"159\":1,\"162\":1,\"163\":2,\"262\":1,\"266\":1,\"268\":1,\"275\":1,\"276\":1,\"277\":2,\"284\":2,\"285\":4,\"286\":1,\"289\":1,\"291\":1,\"294\":1,\"296\":1,\"300\":2,\"411\":3,\"412\":4,\"477\":4,\"511\":4,\"520\":2,\"522\":1,\"525\":1,\"529\":1,\"535\":1,\"543\":1,\"544\":3,\"651\":1,\"654\":1,\"656\":2,\"658\":2,\"666\":1}}],[\"elowen\",{\"0\":{\"3\":1}}],[\"error\",{\"1\":{\"22\":1,\"82\":4,\"275\":2,\"276\":1,\"277\":3}}],[\"个性化大模型应用需要有个性化数据库进行支撑\",{\"1\":{\"687\":1}}],[\"个集成\",{\"1\":{\"684\":1}}],[\"个核心组件组成\",{\"1\":{\"683\":1}}],[\"个月的最快记录\",{\"1\":{\"674\":1}}],[\"个步骤\",{\"1\":{\"650\":1,\"665\":1,\"670\":1}}],[\"个字母进行排列\",{\"1\":{\"600\":1}}],[\"个也彼此相同\",{\"1\":{\"600\":1}}],[\"个彼此相同\",{\"1\":{\"600\":1}}],[\"个不同的元素中取\",{\"1\":{\"601\":1}}],[\"个不同的元素\",{\"1\":{\"600\":1}}],[\"个实验一共有\",{\"1\":{\"599\":1}}],[\"个实验\",{\"1\":{\"599\":1}}],[\"个自由参数\",{\"1\":{\"590\":2}}],[\"个是蓝球\",{\"1\":{\"578\":1}}],[\"个是红球\",{\"1\":{\"578\":1}}],[\"个球中蓝球的数量\",{\"1\":{\"578\":1}}],[\"个球\",{\"1\":{\"578\":2}}],[\"个的组合数\",{\"1\":{\"575\":1}}],[\"个可能的取值\",{\"1\":{\"596\":1}}],[\"个可能取值的离散随机变量\",{\"1\":{\"571\":1}}],[\"个可训练参数\",{\"1\":{\"297\":2}}],[\"个选项\",{\"1\":{\"544\":1}}],[\"个选项中选择正确答案\",{\"1\":{\"494\":1}}],[\"个词之间\",{\"1\":{\"542\":1}}],[\"个词\",{\"1\":{\"508\":1}}],[\"个词到第\",{\"1\":{\"508\":1}}],[\"个人理解是因为\",{\"1\":{\"508\":1}}],[\"个人对上述内容的理解\",{\"1\":{\"395\":1}}],[\"个句子相接\",{\"1\":{\"504\":1}}],[\"个句子在原始本文中是否跟第\",{\"1\":{\"504\":1}}],[\"个英语语料库\",{\"1\":{\"494\":1}}],[\"个响应\",{\"1\":{\"470\":1}}],[\"个序列表示\",{\"1\":{\"445\":1}}],[\"个角的中间位置\",{\"1\":{\"397\":1}}],[\"个神经元才能达到相同效果\",{\"1\":{\"395\":1}}],[\"个线性区域的函数\",{\"1\":{\"395\":1}}],[\"个分段\",{\"1\":{\"395\":1}}],[\"个分布尽可能均匀的采样点索引\",{\"1\":{\"92\":1}}],[\"个示例\",{\"1\":{\"345\":1,\"346\":1}}],[\"个通道\",{\"1\":{\"323\":1}}],[\"个数字\",{\"1\":{\"323\":1}}],[\"个元素彼此相同\",{\"1\":{\"600\":1}}],[\"个元素的向量\",{\"1\":{\"596\":1}}],[\"个元素中选出\",{\"1\":{\"575\":1}}],[\"个元素\",{\"1\":{\"323\":6,\"327\":1,\"600\":1}}],[\"个类别和\",{\"1\":{\"300\":1}}],[\"个用新的编码器做一个编码\",{\"1\":{\"242\":1}}],[\"个样本\",{\"1\":{\"241\":1}}],[\"个下游任务\",{\"1\":{\"237\":1}}],[\"个多模态基准测试中表现优异\",{\"1\":{\"207\":1}}],[\"个注意力头\",{\"1\":{\"174\":1,\"447\":1}}],[\"个视觉标记\",{\"1\":{\"172\":1}}],[\"个图像块\",{\"1\":{\"172\":1}}],[\"个图文对\",{\"1\":{\"162\":1}}],[\"个图文表示\",{\"1\":{\"154\":1}}],[\"个块\",{\"1\":{\"169\":1}}],[\"个为负样本\",{\"1\":{\"162\":1}}],[\"个为正样本\",{\"1\":{\"162\":1}}],[\"个负样本和一个正样本\",{\"1\":{\"240\":1}}],[\"个负样本\",{\"1\":{\"162\":2}}],[\"个正样本\",{\"1\":{\"162\":2}}],[\"个历史负样本\",{\"1\":{\"161\":1}}],[\"个输出\",{\"1\":{\"107\":1}}],[\"个坐标值\",{\"1\":{\"107\":1}}],[\"个最近邻点索引\",{\"1\":{\"100\":1}}],[\"个邻近点的特征\",{\"1\":{\"100\":1}}],[\"个邻近点\",{\"1\":{\"100\":1}}],[\"个关键点作为局部区域中心\",{\"1\":{\"96\":1}}],[\"个关键点对应的全局区域特征向量\",{\"1\":{\"93\":1}}],[\"个关键点对应的局部区域特征向量\",{\"1\":{\"93\":2}}],[\"个关键点的坐标\",{\"1\":{\"93\":3}}],[\"个维度\",{\"1\":{\"92\":1}}],[\"个具有代表性的点\",{\"1\":{\"92\":1}}],[\"个定制化问题\",{\"1\":{\"67\":1}}],[\"个专家设计的问题\",{\"1\":{\"63\":1,\"65\":1}}],[\"个问题\",{\"1\":{\"63\":1}}],[\"个代表性问题\",{\"1\":{\"63\":1}}],[\"个同类别点云\",{\"1\":{\"29\":1}}],[\"个点的值按这个比例加起来\",{\"1\":{\"397\":1}}],[\"个点的子集决定\",{\"1\":{\"112\":1}}],[\"个点云\",{\"1\":{\"65\":1,\"69\":1}}],[\"个点\",{\"1\":{\"18\":1,\"67\":1,\"92\":2,\"100\":3}}],[\"个\",{\"1\":{\"10\":1,\"161\":2,\"191\":1,\"207\":1,\"470\":1,\"601\":1}}],[\"tmp\",{\"1\":{\"666\":5}}],[\"t指定输出格式\",{\"1\":{\"666\":1}}],[\"tf\",{\"1\":{\"519\":2}}],[\"tying\",{\"1\":{\"513\":1}}],[\"typical\",{\"1\":{\"591\":1}}],[\"typing\",{\"1\":{\"412\":1}}],[\"typeerror\",{\"1\":{\"643\":1,\"656\":1,\"659\":1}}],[\"type=bert\",{\"1\":{\"519\":1}}],[\"type==\",{\"1\":{\"29\":1,\"58\":1}}],[\"type\",{\"1\":{\"28\":2,\"29\":8,\"58\":9,\"68\":1,\"163\":3,\"257\":3,\"284\":1,\"520\":9,\"522\":4,\"523\":9,\"528\":3,\"529\":3,\"538\":3,\"540\":2,\"541\":3,\"542\":1,\"543\":3,\"544\":7,\"643\":1,\"656\":1,\"659\":1}}],[\"tgz\",{\"1\":{\"510\":2}}],[\"tgt2\",{\"1\":{\"76\":6}}],[\"tgt\",{\"1\":{\"70\":1,\"76\":21,\"549\":12,\"556\":2,\"557\":2}}],[\"t0++\",{\"1\":{\"469\":1}}],[\"t0\",{\"1\":{\"469\":1}}],[\"tl\",{\"1\":{\"454\":1,\"455\":1}}],[\"tlr\",{\"1\":{\"80\":1}}],[\"tqdm\",{\"1\":{\"410\":1,\"412\":3}}],[\"tverskyloss\",{\"1\":{\"405\":2}}],[\"tversky指数简化为dice系数\",{\"1\":{\"405\":1}}],[\"tversky\",{\"0\":{\"405\":1},\"1\":{\"405\":9,\"407\":1}}],[\"tnews\",{\"1\":{\"519\":4}}],[\"tn\",{\"1\":{\"342\":3,\"343\":1}}],[\"tpu\",{\"1\":{\"675\":1}}],[\"tp+α⋅fp+β⋅fn\",{\"1\":{\"405\":1}}],[\"tp\",{\"1\":{\"342\":3,\"343\":1,\"403\":1,\"405\":6}}],[\"tpr\",{\"1\":{\"82\":1,\"344\":2,\"350\":3,\"353\":3}}],[\"t2q\",{\"1\":{\"283\":2}}],[\"t2i\",{\"1\":{\"145\":13,\"147\":12,\"161\":8,\"162\":4,\"283\":4,\"284\":5}}],[\"tinypytorch计算图转换为dot语言\",{\"1\":{\"666\":1}}],[\"tinypytorch采用define\",{\"1\":{\"662\":1}}],[\"tinypytorch的动态计算图模式使其在易用性和灵活性上表现突出\",{\"1\":{\"662\":1}}],[\"tinypytorch的核心能力总结\",{\"1\":{\"662\":1}}],[\"tinypytorch的计算图需要动态构建和销毁\",{\"1\":{\"657\":1}}],[\"tinypytorch框架能自动处理复杂表达式的微分\",{\"1\":{\"662\":1}}],[\"tinypytorch框架实现了完整的运算符重载体系\",{\"1\":{\"660\":1}}],[\"tinypytorch当前的反向传播会保留所有变量的导数\",{\"1\":{\"658\":1}}],[\"tinypytorch中的循环引用\",{\"1\":{\"657\":1}}],[\"tinypytorch尽量用最少的代码实现了现代深度学习框架的功能\",{\"1\":{\"604\":1}}],[\"tinypytorch\",{\"0\":{\"671\":1},\"1\":{\"603\":2,\"604\":1,\"649\":2,\"650\":2,\"661\":1,\"664\":2,\"665\":2,\"669\":2,\"670\":1}}],[\"tinybert\",{\"1\":{\"503\":2}}],[\"tie\",{\"1\":{\"513\":1}}],[\"timing\",{\"1\":{\"379\":1}}],[\"time\",{\"1\":{\"275\":10,\"277\":10,\"379\":5,\"477\":1,\"673\":1}}],[\"title\",{\"1\":{\"276\":1,\"277\":1,\"289\":1,\"667\":1}}],[\"t5\",{\"1\":{\"224\":1,\"231\":1,\"464\":2,\"485\":1,\"542\":1}}],[\"taking\",{\"1\":{\"527\":1}}],[\"takes\",{\"1\":{\"412\":1}}],[\"taken\",{\"1\":{\"275\":2,\"277\":1,\"531\":1}}],[\"take\",{\"1\":{\"244\":1,\"531\":1,\"549\":1,\"558\":1}}],[\"tax\",{\"1\":{\"469\":1,\"470\":1,\"471\":2,\"472\":1}}],[\"tag\",{\"1\":{\"441\":1}}],[\"tage\",{\"1\":{\"282\":1}}],[\"tabby\",{\"1\":{\"273\":1}}],[\"table\",{\"1\":{\"29\":1,\"58\":1,\"68\":1,\"82\":2,\"494\":1}}],[\"tanh\",{\"1\":{\"262\":1,\"296\":2,\"527\":1,\"665\":1}}],[\"tao\",{\"1\":{\"240\":1}}],[\"task\",{\"0\":{\"235\":1,\"507\":1},\"1\":{\"448\":1,\"454\":2,\"460\":1,\"515\":2,\"519\":1,\"520\":1}}],[\"tasks\",{\"1\":{\"179\":1,\"470\":1}}],[\"tails\",{\"1\":{\"587\":1}}],[\"tail\",{\"1\":{\"83\":1}}],[\"targets=none\",{\"1\":{\"163\":1}}],[\"targets=labels\",{\"1\":{\"163\":1}}],[\"targets\",{\"1\":{\"81\":1,\"82\":11,\"142\":3,\"145\":8,\"147\":10,\"157\":1,\"161\":9,\"163\":5,\"247\":1,\"283\":3,\"401\":7,\"402\":11,\"403\":7,\"404\":7,\"405\":7,\"407\":8}}],[\"target\",{\"1\":{\"78\":8,\"145\":2,\"147\":4,\"157\":1,\"163\":1,\"244\":2,\"403\":1,\"404\":1,\"512\":2,\"549\":1}}],[\"t×d\",{\"1\":{\"72\":1}}],[\"turbo\",{\"1\":{\"674\":1}}],[\"turn\",{\"1\":{\"554\":1}}],[\"tuned\",{\"1\":{\"487\":1}}],[\"tuned模型\",{\"1\":{\"462\":3}}],[\"tuned大型模型表现\",{\"1\":{\"462\":1}}],[\"tuned方法的能力\",{\"1\":{\"461\":1}}],[\"tune\",{\"1\":{\"224\":1}}],[\"tuning完全不相同的另一条技术路线\",{\"1\":{\"420\":1}}],[\"tuning和prefix\",{\"1\":{\"420\":1}}],[\"tuning也保证了基座模型本身是没有变的\",{\"1\":{\"419\":1}}],[\"tuning是在transformer的encoder和decoder的网络中都加了一些特定的前缀\",{\"1\":{\"419\":1}}],[\"tuning是在embedding环节\",{\"1\":{\"419\":1}}],[\"tuning是发生在embedding这个环节的\",{\"1\":{\"418\":1}}],[\"tuning就是在保证函数本身不变的前提下\",{\"1\":{\"418\":1}}],[\"tuning的是类似的\",{\"1\":{\"419\":1}}],[\"tuning的灵感来源是\",{\"1\":{\"419\":1}}],[\"tuning的具体细节\",{\"1\":{\"418\":1,\"419\":1}}],[\"tuning的基本原理是在输入序列x之前\",{\"1\":{\"418\":1}}],[\"tuning的出发点\",{\"1\":{\"418\":1,\"419\":1}}],[\"tuning\",{\"0\":{\"10\":1,\"231\":2,\"413\":1,\"418\":1,\"419\":1,\"508\":1},\"1\":{\"190\":1,\"193\":1,\"205\":2,\"224\":3,\"225\":1,\"231\":13,\"413\":1,\"416\":3,\"418\":1,\"419\":1,\"421\":1,\"424\":7,\"425\":2,\"439\":1,\"461\":1,\"470\":1,\"508\":1}}],[\"tuple\",{\"1\":{\"40\":1,\"163\":1,\"289\":1,\"292\":2,\"295\":1,\"363\":2,\"410\":2,\"411\":1,\"412\":6,\"477\":7,\"540\":1,\"651\":2,\"652\":1,\"654\":1,\"656\":2,\"658\":2}}],[\"txt文件\",{\"1\":{\"519\":1}}],[\"txt\",{\"1\":{\"40\":1,\"282\":1,\"338\":2,\"546\":2,\"666\":8}}],[\"t\",{\"0\":{\"586\":2},\"1\":{\"30\":9,\"34\":5,\"35\":3,\"36\":6,\"70\":11,\"72\":1,\"76\":11,\"80\":1,\"82\":5,\"98\":1,\"103\":1,\"105\":2,\"107\":3,\"112\":2,\"145\":6,\"146\":1,\"147\":4,\"161\":3,\"244\":1,\"246\":4,\"247\":1,\"249\":1,\"272\":15,\"273\":2,\"275\":1,\"277\":1,\"289\":1,\"319\":1,\"321\":3,\"323\":3,\"326\":16,\"404\":2,\"411\":2,\"514\":1,\"517\":1,\"586\":2,\"658\":3,\"666\":2}}],[\"term\",{\"1\":{\"510\":1}}],[\"terms\",{\"1\":{\"412\":3,\"541\":1}}],[\"telgarsky\",{\"1\":{\"395\":1}}],[\"te和mi分别是visual\",{\"1\":{\"255\":1}}],[\"te\",{\"1\":{\"255\":1}}],[\"teacher\",{\"1\":{\"163\":2}}],[\"testcase\",{\"1\":{\"645\":1,\"646\":1}}],[\"testenv\",{\"1\":{\"334\":2}}],[\"test\",{\"1\":{\"58\":1,\"65\":1,\"80\":4,\"82\":1,\"142\":3,\"145\":4,\"275\":2,\"277\":1,\"439\":1,\"440\":1,\"455\":1,\"510\":8,\"645\":2,\"646\":1,\"673\":1}}],[\"tensordataset\",{\"1\":{\"520\":1}}],[\"tensor2\",{\"1\":{\"390\":1}}],[\"tensor1\",{\"1\":{\"390\":1}}],[\"tensors\",{\"1\":{\"381\":2}}],[\"tensors=\",{\"1\":{\"40\":1,\"142\":1,\"143\":1,\"145\":1,\"146\":1,\"147\":1,\"159\":1,\"275\":2,\"277\":2,\"282\":1,\"477\":1}}],[\"tensorrt\",{\"1\":{\"326\":1}}],[\"tensorflow\",{\"1\":{\"85\":1,\"102\":1,\"328\":1}}],[\"tensor\",{\"1\":{\"43\":1,\"58\":2,\"59\":2,\"76\":6,\"143\":1,\"163\":1,\"262\":2,\"265\":2,\"273\":1,\"289\":1,\"290\":2,\"295\":1,\"321\":3,\"323\":3,\"325\":13,\"326\":4,\"381\":2,\"384\":1,\"385\":1,\"386\":4,\"387\":5,\"389\":5,\"390\":1,\"401\":4,\"402\":4,\"403\":4,\"404\":4,\"474\":2,\"477\":4,\"514\":2,\"525\":2,\"527\":2,\"532\":2,\"533\":3,\"541\":1}}],[\"tennisracket\",{\"1\":{\"29\":1}}],[\"temperature\",{\"1\":{\"240\":1,\"246\":1,\"247\":1,\"272\":1}}],[\"temp3\",{\"1\":{\"78\":2}}],[\"temp2\",{\"1\":{\"78\":3}}],[\"temp1\",{\"1\":{\"78\":3}}],[\"temp\",{\"1\":{\"29\":2,\"58\":2,\"78\":2,\"81\":2,\"82\":5,\"145\":7,\"147\":5,\"160\":4,\"161\":4,\"283\":2,\"511\":2}}],[\"textual\",{\"1\":{\"448\":1}}],[\"text和text\",{\"1\":{\"283\":1}}],[\"text最相关的视觉信息\",{\"1\":{\"282\":1}}],[\"text转化为18291个类别\",{\"1\":{\"278\":1}}],[\"text=texts\",{\"1\":{\"275\":1,\"277\":1}}],[\"texts\",{\"1\":{\"272\":1,\"273\":2,\"275\":1,\"277\":1}}],[\"textvqa\",{\"1\":{\"220\":1,\"222\":1}}],[\"textcaps\",{\"1\":{\"217\":1}}],[\"text\",{\"0\":{\"154\":1,\"156\":1,\"176\":1,\"283\":1,\"284\":1,\"285\":1},\"1\":{\"29\":18,\"30\":7,\"34\":1,\"40\":3,\"43\":1,\"70\":4,\"71\":1,\"76\":2,\"80\":2,\"83\":7,\"126\":2,\"127\":4,\"142\":7,\"143\":2,\"145\":60,\"146\":17,\"147\":60,\"156\":1,\"159\":4,\"160\":21,\"161\":30,\"162\":24,\"163\":7,\"176\":2,\"178\":1,\"194\":1,\"255\":2,\"258\":1,\"262\":1,\"267\":4,\"272\":6,\"273\":13,\"275\":9,\"276\":9,\"277\":16,\"280\":2,\"282\":17,\"283\":11,\"284\":32,\"285\":5,\"286\":1,\"289\":1,\"410\":2,\"412\":2,\"454\":2,\"474\":7,\"477\":2,\"495\":1,\"511\":8,\"520\":2}}],[\"treebank\",{\"1\":{\"448\":1,\"455\":1,\"510\":1}}],[\"tree\",{\"1\":{\"279\":1}}],[\"triviaqa零样本68\",{\"1\":{\"482\":1}}],[\"trinh\",{\"1\":{\"455\":1}}],[\"tripod\",{\"1\":{\"273\":1}}],[\"trick\",{\"1\":{\"59\":1}}],[\"try\",{\"1\":{\"73\":1,\"83\":1,\"275\":2,\"276\":1,\"277\":3,\"658\":1,\"666\":1}}],[\"truncate\",{\"1\":{\"410\":2,\"412\":4}}],[\"truncation=true\",{\"1\":{\"40\":1,\"142\":1,\"145\":1,\"146\":1,\"147\":1,\"159\":1,\"282\":1}}],[\"truncation\",{\"1\":{\"40\":2}}],[\"trunc\",{\"1\":{\"292\":1,\"293\":2,\"296\":2}}],[\"truthfulqa\",{\"1\":{\"469\":1,\"470\":1,\"471\":2,\"484\":1}}],[\"truth\",{\"0\":{\"64\":1},\"1\":{\"78\":2,\"82\":2,\"162\":1,\"240\":1,\"401\":2,\"402\":1,\"403\":2,\"404\":1,\"405\":1}}],[\"truediv\",{\"1\":{\"660\":1}}],[\"true\",{\"1\":{\"29\":1,\"40\":2,\"76\":1,\"82\":5,\"96\":1,\"109\":1,\"142\":1,\"145\":1,\"146\":2,\"147\":1,\"160\":1,\"162\":2,\"163\":3,\"289\":1,\"300\":1,\"326\":1,\"389\":3,\"403\":1,\"405\":3,\"407\":1,\"411\":1,\"412\":1,\"477\":2,\"658\":1,\"667\":1}}],[\"trust\",{\"1\":{\"28\":2}}],[\"traditional\",{\"1\":{\"474\":5}}],[\"traversal\",{\"1\":{\"440\":1}}],[\"trange\",{\"1\":{\"410\":3,\"412\":2}}],[\"translate\",{\"1\":{\"454\":1}}],[\"translation\",{\"1\":{\"287\":1}}],[\"trans\",{\"1\":{\"108\":6,\"109\":11,\"110\":4,\"111\":4,\"326\":7}}],[\"transform=data\",{\"1\":{\"290\":2}}],[\"transform=none\",{\"1\":{\"289\":1}}],[\"transform=feature\",{\"1\":{\"110\":1,\"111\":2}}],[\"transform=false\",{\"1\":{\"110\":1,\"111\":1}}],[\"transforms\",{\"1\":{\"244\":7,\"290\":10}}],[\"transformation\",{\"1\":{\"108\":1}}],[\"transformations\",{\"1\":{\"104\":1}}],[\"transform\",{\"1\":{\"108\":3,\"109\":5,\"110\":3,\"111\":2,\"142\":1,\"244\":5,\"268\":5,\"289\":5,\"290\":4,\"535\":3,\"536\":2}}],[\"transformer架构\",{\"1\":{\"464\":1}}],[\"transformer架构导入偏差是有帮助的\",{\"1\":{\"449\":1}}],[\"transformer比lstm能获取长距离信息\",{\"1\":{\"441\":1}}],[\"transformer跨各种各样任务的迁移性能更强\",{\"1\":{\"440\":1}}],[\"transformer证明了使用transformer结构可以有效处理图像数据\",{\"1\":{\"301\":1}}],[\"transformer需要输入的是一维的token\",{\"1\":{\"291\":1}}],[\"transformer的核心流程实现\",{\"1\":{\"288\":1}}],[\"transformer的模型结构相比于transformer来说更简单\",{\"1\":{\"287\":1}}],[\"transformer是2021年谷歌在iclr上提出的算法\",{\"1\":{\"287\":1}}],[\"transformer结合起来用于多模态transformer\",{\"1\":{\"253\":1}}],[\"transformers\",{\"1\":{\"164\":2,\"165\":1,\"277\":1,\"474\":1,\"477\":3}}],[\"transformerdecoderlayer\",{\"1\":{\"76\":1}}],[\"transformer\",{\"0\":{\"171\":1,\"264\":1,\"315\":1},\"1\":{\"70\":1,\"73\":2,\"76\":2,\"107\":1,\"112\":3,\"115\":1,\"126\":1,\"127\":2,\"150\":1,\"152\":2,\"163\":1,\"166\":3,\"169\":1,\"171\":2,\"174\":1,\"183\":1,\"189\":1,\"252\":2,\"266\":1,\"267\":1,\"270\":3,\"272\":3,\"275\":2,\"278\":1,\"281\":1,\"287\":1,\"292\":3,\"294\":1,\"299\":10,\"300\":2,\"304\":1,\"315\":1,\"425\":1,\"443\":2,\"444\":1,\"447\":3,\"454\":2,\"461\":1,\"464\":1,\"493\":1,\"504\":3,\"506\":1,\"508\":2,\"513\":3,\"531\":1,\"542\":1,\"546\":5,\"548\":4,\"673\":1,\"674\":3}}],[\"transposed\",{\"1\":{\"326\":1}}],[\"transpose\",{\"0\":{\"326\":1,\"382\":1},\"1\":{\"29\":2,\"34\":1,\"35\":2,\"36\":1,\"58\":1,\"59\":3,\"68\":1,\"70\":7,\"73\":2,\"76\":1,\"83\":1,\"100\":1,\"108\":1,\"109\":4,\"111\":1,\"162\":5,\"249\":1,\"266\":6,\"285\":8,\"291\":1,\"295\":4,\"326\":2,\"382\":1,\"383\":3,\"384\":1,\"385\":1,\"477\":3,\"517\":1,\"531\":5,\"558\":3}}],[\"trained\",{\"1\":{\"425\":1,\"519\":2}}],[\"traindir\",{\"1\":{\"244\":1}}],[\"trains=\",{\"1\":{\"142\":1,\"145\":1,\"147\":1}}],[\"training语言模型\",{\"1\":{\"439\":1}}],[\"training\",{\"0\":{\"153\":1},\"1\":{\"81\":1,\"119\":2,\"120\":1,\"134\":1,\"164\":2,\"190\":2,\"225\":1,\"271\":1,\"289\":1,\"300\":1,\"438\":1,\"466\":1,\"477\":2,\"495\":1,\"497\":1,\"514\":1,\"674\":1}}],[\"train\",{\"0\":{\"159\":1,\"244\":1},\"1\":{\"29\":8,\"58\":8,\"65\":1,\"68\":2,\"80\":4,\"81\":3,\"83\":5,\"142\":9,\"145\":7,\"147\":2,\"159\":1,\"244\":11,\"274\":1,\"289\":7,\"290\":7,\"296\":1,\"410\":1,\"412\":1,\"421\":1,\"510\":8,\"514\":1,\"519\":3,\"520\":1,\"522\":6}}],[\"trashcan\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"tool\",{\"1\":{\"687\":1}}],[\"toothbrush\",{\"1\":{\"29\":1}}],[\"toxicity\",{\"0\":{\"484\":1}}],[\"totensor\",{\"1\":{\"244\":1,\"290\":2}}],[\"total\",{\"0\":{\"569\":1},\"1\":{\"43\":2,\"78\":1,\"81\":1,\"82\":4,\"145\":1,\"275\":4,\"277\":4,\"295\":3,\"403\":2,\"514\":3,\"515\":2,\"538\":2,\"541\":2}}],[\"touvron\",{\"1\":{\"131\":1}}],[\"token用于分类任务即可\",{\"1\":{\"529\":1}}],[\"token分类任务\",{\"0\":{\"543\":1}}],[\"token分类\",{\"1\":{\"508\":2}}],[\"token序列\",{\"1\":{\"474\":1}}],[\"token窗口中\",{\"1\":{\"461\":1}}],[\"token提取出来\",{\"1\":{\"296\":1}}],[\"token的作用\",{\"1\":{\"292\":1}}],[\"token的相似度\",{\"1\":{\"283\":1}}],[\"token对齐\",{\"1\":{\"292\":1}}],[\"token开头\",{\"1\":{\"286\":1}}],[\"token作为最后的相似度得分\",{\"1\":{\"283\":2}}],[\"token作为input\",{\"1\":{\"282\":1}}],[\"token是有效的\",{\"1\":{\"282\":1}}],[\"token压缩\",{\"1\":{\"214\":1}}],[\"token做投影\",{\"1\":{\"145\":2}}],[\"tokens可知这些被掩码token对应的真实词作为label\",{\"1\":{\"514\":1}}],[\"tokens后hellaswag分数提升至82\",{\"1\":{\"482\":1}}],[\"tokens训练耗时约21天\",{\"1\":{\"481\":1}}],[\"tokens训练后性能持续提升\",{\"1\":{\"480\":1}}],[\"tokens即停止\",{\"1\":{\"480\":1}}],[\"tokens=false\",{\"1\":{\"520\":1}}],[\"tokens=50\",{\"1\":{\"477\":1}}],[\"tokens=true\",{\"1\":{\"143\":1,\"286\":1,\"542\":2}}],[\"tokens形状\",{\"1\":{\"286\":1}}],[\"tokens做attention\",{\"1\":{\"285\":1}}],[\"tokens的embeddings在seq\",{\"1\":{\"284\":1}}],[\"tokens同时输入bertmodel时\",{\"1\":{\"284\":1}}],[\"tokens部分的mask列表\",{\"1\":{\"520\":1}}],[\"tokens部分的每个位置都映射到2维匹配空间\",{\"1\":{\"284\":1}}],[\"tokens部分的结果\",{\"1\":{\"284\":1}}],[\"tokens拼接得到的结果和图像嵌入进行cross\",{\"1\":{\"284\":1}}],[\"tokens列表中所有句子掩码数量一致\",{\"1\":{\"514\":1}}],[\"tokens列表\",{\"1\":{\"284\":1}}],[\"tokens\",{\"1\":{\"40\":3,\"75\":2,\"273\":2,\"282\":9,\"283\":3,\"284\":16,\"285\":16,\"286\":6,\"292\":1,\"293\":2,\"296\":2,\"410\":4,\"411\":6,\"412\":7,\"447\":1,\"454\":1,\"461\":1,\"463\":1,\"474\":6,\"477\":4,\"480\":1,\"481\":2,\"494\":1,\"511\":17,\"512\":13,\"513\":4,\"514\":2,\"517\":4,\"520\":6,\"531\":1,\"540\":2,\"542\":2,\"674\":1}}],[\"token\",{\"0\":{\"292\":1},\"1\":{\"40\":2,\"43\":5,\"46\":3,\"72\":2,\"73\":4,\"76\":10,\"126\":5,\"142\":4,\"143\":12,\"145\":1,\"147\":3,\"155\":2,\"156\":1,\"160\":1,\"161\":3,\"162\":1,\"163\":14,\"168\":1,\"190\":1,\"200\":1,\"204\":2,\"216\":2,\"226\":1,\"262\":3,\"267\":1,\"268\":1,\"282\":1,\"283\":1,\"285\":16,\"286\":5,\"292\":23,\"293\":5,\"296\":5,\"410\":10,\"411\":4,\"412\":10,\"443\":1,\"445\":1,\"449\":1,\"474\":7,\"475\":2,\"477\":20,\"493\":2,\"505\":3,\"506\":4,\"511\":4,\"512\":2,\"513\":4,\"520\":35,\"522\":4,\"523\":9,\"527\":4,\"528\":2,\"529\":2,\"536\":1,\"538\":3,\"540\":9,\"541\":2,\"542\":8,\"543\":4,\"544\":6,\"674\":3}}],[\"tokenize方法进行分词\",{\"1\":{\"411\":1}}],[\"tokenize方法完成分词功能\",{\"1\":{\"410\":1}}],[\"tokenize方法完成断句功能\",{\"1\":{\"410\":1}}],[\"tokenize\",{\"1\":{\"147\":1,\"273\":1,\"410\":2,\"411\":2,\"412\":4,\"447\":1,\"510\":1,\"511\":4}}],[\"tokenizer\",{\"1\":{\"28\":5,\"40\":4,\"142\":6,\"143\":7,\"145\":4,\"146\":3,\"147\":7,\"159\":2,\"160\":4,\"163\":3,\"282\":1,\"285\":2,\"286\":4,\"411\":7,\"412\":1,\"474\":5,\"477\":6,\"481\":1,\"511\":1,\"512\":17,\"514\":6,\"542\":4,\"674\":1}}],[\"tokenization\",{\"1\":{\"40\":1,\"540\":1}}],[\"topk\",{\"1\":{\"273\":1}}],[\"top\",{\"1\":{\"28\":2,\"63\":1,\"143\":2,\"273\":2,\"286\":3,\"455\":1}}],[\"to\",{\"0\":{\"176\":1,\"436\":1},\"1\":{\"28\":1,\"36\":6,\"40\":1,\"59\":9,\"71\":1,\"78\":1,\"83\":3,\"92\":7,\"142\":2,\"143\":2,\"145\":3,\"146\":2,\"147\":4,\"159\":2,\"161\":2,\"162\":1,\"163\":1,\"176\":2,\"178\":1,\"206\":4,\"225\":1,\"247\":1,\"272\":2,\"275\":6,\"277\":5,\"280\":1,\"282\":2,\"284\":3,\"285\":1,\"286\":2,\"291\":1,\"296\":3,\"300\":1,\"372\":3,\"410\":14,\"411\":3,\"412\":53,\"421\":2,\"436\":2,\"454\":1,\"466\":1,\"472\":1,\"474\":16,\"477\":2,\"514\":2,\"519\":3,\"520\":1,\"527\":1,\"528\":1,\"531\":4,\"547\":1,\"558\":1,\"666\":5}}],[\"torch==1\",{\"1\":{\"546\":1}}],[\"torchscript\",{\"1\":{\"295\":1}}],[\"torch\",{\"0\":{\"388\":1},\"1\":{\"28\":2,\"32\":8,\"34\":1,\"35\":2,\"36\":1,\"40\":1,\"41\":4,\"43\":6,\"45\":5,\"46\":3,\"58\":2,\"59\":13,\"70\":3,\"76\":5,\"78\":21,\"80\":3,\"81\":2,\"82\":1,\"83\":4,\"92\":12,\"96\":3,\"100\":3,\"107\":6,\"108\":4,\"109\":7,\"111\":4,\"142\":2,\"143\":1,\"145\":30,\"146\":1,\"147\":26,\"160\":4,\"161\":7,\"162\":14,\"163\":9,\"244\":2,\"246\":2,\"247\":5,\"248\":1,\"249\":1,\"266\":2,\"273\":3,\"275\":4,\"277\":5,\"282\":1,\"283\":3,\"284\":17,\"285\":6,\"286\":2,\"289\":5,\"290\":2,\"292\":2,\"293\":3,\"296\":5,\"300\":1,\"321\":3,\"323\":7,\"325\":2,\"326\":4,\"381\":3,\"384\":1,\"386\":2,\"387\":3,\"389\":2,\"390\":3,\"403\":1,\"404\":6,\"407\":4,\"474\":6,\"477\":16,\"513\":2,\"514\":4,\"517\":2,\"522\":1,\"523\":2,\"531\":2,\"536\":1,\"540\":4,\"546\":1,\"558\":2}}],[\"twocropstransform\",{\"1\":{\"244\":3}}],[\"two\",{\"1\":{\"28\":2,\"244\":1,\"281\":2,\"287\":1,\"520\":1}}],[\"through\",{\"1\":{\"554\":1}}],[\"thre\",{\"1\":{\"82\":2}}],[\"thres\",{\"1\":{\"82\":2}}],[\"threshold\",{\"1\":{\"82\":9,\"401\":1}}],[\"thresholding\",{\"1\":{\"82\":2}}],[\"than\",{\"1\":{\"474\":6}}],[\"that\",{\"1\":{\"28\":6,\"63\":1,\"474\":13}}],[\"thinking\",{\"1\":{\"674\":2}}],[\"think\",{\"1\":{\"433\":1}}],[\"this\",{\"1\":{\"28\":3,\"43\":7,\"70\":1,\"83\":3,\"273\":1,\"531\":1}}],[\"these\",{\"1\":{\"541\":1}}],[\"there\",{\"1\":{\"536\":1}}],[\"they\",{\"1\":{\"529\":1}}],[\"them\",{\"1\":{\"482\":1,\"484\":1}}],[\"their\",{\"1\":{\"412\":4,\"482\":1,\"484\":1}}],[\"theorem\",{\"1\":{\"112\":1,\"395\":1,\"570\":1}}],[\"thecvf\",{\"1\":{\"60\":1}}],[\"theta\",{\"1\":{\"45\":4,\"59\":4}}],[\"the\",{\"0\":{\"589\":1},\"1\":{\"28\":56,\"31\":12,\"40\":1,\"59\":2,\"63\":2,\"78\":1,\"82\":2,\"83\":3,\"147\":1,\"157\":1,\"206\":2,\"244\":1,\"246\":3,\"247\":1,\"248\":1,\"249\":1,\"273\":1,\"276\":1,\"277\":1,\"289\":1,\"412\":2,\"418\":1,\"448\":1,\"495\":1,\"510\":1,\"520\":4,\"527\":3,\"531\":5,\"536\":3,\"541\":1,\"542\":2,\"543\":1,\"546\":1,\"554\":1,\"558\":2,\"596\":1,\"666\":1}}],[\"thought\",{\"0\":{\"9\":1,\"28\":1,\"434\":1},\"1\":{\"7\":1,\"26\":1,\"434\":2,\"435\":1,\"676\":1}}],[\"f放缩因子\",{\"1\":{\"626\":1}}],[\"f1放缩因子\",{\"1\":{\"626\":2}}],[\"f1达89\",{\"1\":{\"498\":1,\"499\":1}}],[\"f1从87\",{\"1\":{\"498\":1}}],[\"f1\",{\"0\":{\"348\":1},\"1\":{\"348\":4,\"455\":2,\"462\":1,\"497\":1,\"626\":2}}],[\"fft的原理\",{\"1\":{\"416\":1}}],[\"ffmpeg\",{\"1\":{\"338\":1}}],[\"ffn\",{\"1\":{\"74\":1,\"126\":2,\"475\":1}}],[\"f2放缩因子\",{\"1\":{\"626\":2}}],[\"f2\",{\"1\":{\"238\":1,\"626\":1}}],[\"f2d\",{\"1\":{\"40\":1}}],[\"f3放缩因子\",{\"1\":{\"626\":1}}],[\"f3\",{\"1\":{\"238\":2,\"626\":1}}],[\"f3d\",{\"1\":{\"40\":1}}],[\"few\",{\"1\":{\"458\":1,\"460\":1,\"461\":1,\"462\":3,\"464\":2,\"470\":2,\"471\":2}}],[\"feed\",{\"1\":{\"447\":1,\"477\":2,\"548\":1,\"553\":4,\"556\":5}}],[\"feedforward\",{\"1\":{\"267\":1}}],[\"feedback\",{\"1\":{\"224\":1,\"416\":2,\"466\":1,\"468\":1,\"469\":1}}],[\"fe\",{\"1\":{\"258\":3}}],[\"feat相似度最大的那个query\",{\"1\":{\"283\":2}}],[\"feats\",{\"1\":{\"282\":1,\"283\":5}}],[\"feat=false\",{\"1\":{\"109\":2,\"111\":1}}],[\"feat=true\",{\"1\":{\"109\":2,\"110\":1}}],[\"feat\",{\"1\":{\"70\":8,\"76\":8,\"109\":10,\"110\":4,\"111\":4,\"145\":22,\"146\":4,\"147\":18,\"161\":25,\"282\":1,\"283\":8}}],[\"featured\",{\"1\":{\"510\":1}}],[\"feature的方法快了4倍\",{\"1\":{\"259\":1}}],[\"feature的方法速度快了60倍\",{\"1\":{\"259\":1}}],[\"feature方法直接使用cnn提取grid的特征\",{\"1\":{\"256\":1}}],[\"feature方法通常采用faster\",{\"1\":{\"256\":1}}],[\"features=none\",{\"1\":{\"294\":2}}],[\"features=mlp\",{\"1\":{\"294\":1}}],[\"features=dim\",{\"1\":{\"294\":1}}],[\"features\",{\"1\":{\"35\":1,\"40\":1,\"98\":2,\"162\":4,\"247\":2,\"273\":10,\"275\":4,\"277\":4,\"278\":1,\"292\":3,\"293\":3,\"294\":15,\"296\":5,\"328\":1,\"520\":1}}],[\"feature\",{\"1\":{\"35\":2,\"36\":1,\"40\":18,\"41\":19,\"45\":9,\"46\":10,\"59\":18,\"73\":2,\"76\":1,\"86\":2,\"92\":1,\"96\":1,\"98\":5,\"99\":1,\"100\":1,\"101\":2,\"108\":3,\"109\":5,\"110\":4,\"111\":3,\"225\":1,\"246\":1,\"253\":2,\"397\":3}}],[\"fn放缩因子\",{\"1\":{\"626\":1}}],[\"fn负责对返回的一个batch\",{\"1\":{\"522\":1}}],[\"fn=collate\",{\"1\":{\"522\":1}}],[\"fn=val\",{\"1\":{\"290\":1}}],[\"fn=train\",{\"1\":{\"290\":1}}],[\"fn叫做\",{\"1\":{\"238\":1}}],[\"fn\",{\"1\":{\"238\":1,\"268\":3,\"289\":2,\"290\":2,\"342\":3,\"343\":1,\"405\":8,\"407\":2,\"522\":2,\"525\":3,\"535\":3}}],[\"fns=\",{\"1\":{\"142\":1,\"145\":1,\"147\":1}}],[\"fnn\",{\"1\":{\"36\":1}}],[\"fstn\",{\"1\":{\"109\":2}}],[\"f^2\",{\"1\":{\"105\":1}}],[\"fxia22\",{\"1\":{\"102\":1}}],[\"flush=true\",{\"1\":{\"474\":1,\"477\":1}}],[\"floor\",{\"1\":{\"396\":1}}],[\"flowerclassify\",{\"1\":{\"275\":1,\"277\":1}}],[\"flower\",{\"1\":{\"275\":4,\"277\":2,\"289\":8}}],[\"floattensor\",{\"1\":{\"477\":5}}],[\"float32\",{\"1\":{\"107\":1}}],[\"float64\",{\"1\":{\"83\":4,\"666\":1}}],[\"float\",{\"1\":{\"58\":2,\"59\":2,\"70\":2,\"76\":3,\"83\":1,\"145\":1,\"163\":1,\"246\":2,\"273\":2,\"289\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":3,\"514\":2,\"660\":1}}],[\"flashattention\",{\"1\":{\"674\":1}}],[\"flash\",{\"1\":{\"674\":6}}],[\"flat迭代器等\",{\"1\":{\"659\":1}}],[\"flatten\",{\"1\":{\"82\":2,\"291\":1}}],[\"flag\",{\"1\":{\"273\":1,\"412\":1}}],[\"flan\",{\"1\":{\"224\":1,\"231\":1,\"469\":1}}],[\"flickr30k\",{\"1\":{\"140\":1,\"194\":2,\"203\":2}}],[\"fct\",{\"1\":{\"163\":2,\"268\":2,\"285\":2,\"529\":4,\"538\":3,\"541\":3,\"543\":3,\"544\":2}}],[\"fc3\",{\"1\":{\"93\":2,\"96\":2,\"107\":2,\"110\":2}}],[\"fc2\",{\"1\":{\"93\":2,\"96\":2,\"107\":2,\"110\":2,\"294\":2}}],[\"fc1\",{\"1\":{\"93\":2,\"96\":2,\"107\":2,\"110\":2,\"294\":2}}],[\"fc\",{\"1\":{\"36\":3,\"107\":1,\"156\":1,\"246\":1,\"286\":1,\"296\":1,\"513\":5}}],[\"fp4\",{\"1\":{\"101\":2}}],[\"fp\",{\"1\":{\"99\":3,\"101\":1,\"342\":3,\"343\":1,\"405\":8,\"407\":2}}],[\"fps是一种在点云\",{\"1\":{\"89\":1}}],[\"fps\",{\"1\":{\"89\":1,\"92\":7,\"96\":1,\"98\":1}}],[\"fpr\",{\"1\":{\"82\":1,\"345\":3,\"350\":3,\"353\":2}}],[\"fp16\",{\"1\":{\"494\":1,\"528\":1}}],[\"fp1\",{\"1\":{\"35\":2,\"46\":3,\"59\":2,\"70\":1,\"101\":2}}],[\"fp2\",{\"1\":{\"35\":2,\"46\":3,\"59\":2,\"70\":1,\"101\":2}}],[\"fp3\",{\"1\":{\"35\":2,\"46\":4,\"59\":2,\"70\":1,\"101\":2}}],[\"figsize=\",{\"1\":{\"667\":1}}],[\"figure\",{\"1\":{\"397\":1,\"461\":2,\"469\":1,\"470\":1,\"471\":7,\"472\":4,\"553\":1,\"556\":1,\"558\":1,\"667\":1}}],[\"field\",{\"1\":{\"566\":3}}],[\"fill\",{\"1\":{\"142\":1,\"145\":2,\"147\":4,\"161\":1,\"162\":2,\"285\":1,\"286\":1,\"517\":1,\"558\":1}}],[\"filterwarnings\",{\"1\":{\"277\":1}}],[\"filter\",{\"0\":{\"144\":1},\"1\":{\"120\":1,\"123\":1,\"128\":2,\"132\":4,\"134\":2,\"138\":1,\"140\":1,\"145\":1}}],[\"filtering\",{\"1\":{\"120\":1,\"128\":1,\"226\":1}}],[\"filename=to\",{\"1\":{\"666\":1}}],[\"file=\",{\"1\":{\"666\":2}}],[\"file$bert\",{\"1\":{\"519\":1}}],[\"filepath\",{\"1\":{\"411\":1}}],[\"filepaths\",{\"1\":{\"410\":5,\"412\":6}}],[\"file\",{\"1\":{\"29\":7,\"58\":5,\"142\":1,\"160\":1,\"275\":5,\"277\":5,\"289\":2,\"510\":2,\"511\":3,\"666\":3}}],[\"files\",{\"1\":{\"29\":11,\"58\":8,\"275\":2,\"277\":2,\"412\":1}}],[\"first\",{\"1\":{\"92\":2,\"147\":1,\"229\":1,\"262\":2,\"520\":2,\"527\":3}}],[\"find\",{\"1\":{\"68\":2,\"276\":2,\"277\":2,\"412\":1}}],[\"fingers\",{\"1\":{\"28\":2,\"31\":2}}],[\"finally\",{\"1\":{\"658\":1}}],[\"final\",{\"1\":{\"28\":1,\"98\":1,\"558\":1}}],[\"finetuning\",{\"1\":{\"421\":1}}],[\"finetune\",{\"0\":{\"681\":1},\"1\":{\"272\":1,\"681\":1}}],[\"fine\",{\"0\":{\"10\":1,\"413\":1,\"508\":1},\"1\":{\"190\":1,\"224\":1,\"225\":1,\"413\":1,\"416\":3,\"421\":1,\"425\":2,\"439\":1,\"461\":1,\"470\":1,\"487\":1,\"508\":1}}],[\"fairseq\",{\"1\":{\"494\":1}}],[\"fast\",{\"1\":{\"510\":2}}],[\"fastai\",{\"1\":{\"510\":1}}],[\"faster\",{\"1\":{\"396\":1,\"397\":1}}],[\"fast=false\",{\"1\":{\"28\":1}}],[\"facial\",{\"1\":{\"273\":1}}],[\"facebook\",{\"1\":{\"239\":1}}],[\"facebookresearch\",{\"1\":{\"232\":1}}],[\"factor\",{\"1\":{\"59\":3}}],[\"far\",{\"1\":{\"206\":2}}],[\"farthest\",{\"1\":{\"92\":8,\"96\":1,\"98\":1}}],[\"fallback\",{\"1\":{\"660\":1}}],[\"falcon等\",{\"1\":{\"184\":1}}],[\"false\",{\"1\":{\"40\":1,\"109\":1,\"142\":3,\"145\":2,\"146\":1,\"163\":2,\"246\":2,\"300\":1,\"389\":1,\"405\":6,\"411\":1,\"412\":1,\"477\":2,\"658\":2}}],[\"faucet\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"f\",{\"1\":{\"28\":8,\"30\":6,\"33\":4,\"34\":3,\"35\":7,\"36\":21,\"41\":2,\"46\":5,\"59\":58,\"68\":6,\"70\":2,\"73\":2,\"74\":1,\"78\":1,\"82\":1,\"83\":8,\"92\":1,\"93\":3,\"96\":4,\"100\":1,\"101\":2,\"105\":2,\"107\":5,\"109\":2,\"110\":3,\"111\":4,\"145\":11,\"146\":2,\"147\":11,\"161\":8,\"162\":3,\"163\":2,\"272\":4,\"275\":7,\"276\":4,\"277\":11,\"282\":2,\"283\":2,\"284\":3,\"323\":2,\"366\":2,\"367\":1,\"371\":2,\"372\":2,\"373\":1,\"377\":1,\"378\":8,\"379\":2,\"401\":1,\"402\":3,\"404\":2,\"405\":1,\"410\":3,\"412\":6,\"474\":3,\"477\":2,\"510\":8,\"511\":7,\"514\":3,\"550\":1,\"622\":3,\"635\":4,\"638\":4,\"643\":1,\"646\":1,\"652\":5,\"654\":4,\"656\":8,\"657\":1,\"658\":9,\"659\":2,\"666\":16,\"667\":2}}],[\"france\",{\"1\":{\"542\":2}}],[\"french\",{\"1\":{\"454\":3}}],[\"frequencies\",{\"1\":{\"412\":5}}],[\"frequency\",{\"1\":{\"412\":10}}],[\"freqs\",{\"1\":{\"410\":25,\"411\":2,\"412\":30}}],[\"freq\",{\"1\":{\"159\":1,\"410\":7,\"412\":5}}],[\"frozen\",{\"1\":{\"282\":1}}],[\"frobenius\",{\"1\":{\"108\":2}}],[\"from\",{\"1\":{\"28\":7,\"31\":2,\"47\":1,\"83\":3,\"107\":1,\"142\":1,\"145\":2,\"147\":2,\"160\":3,\"165\":1,\"224\":1,\"275\":2,\"276\":2,\"277\":8,\"278\":2,\"282\":1,\"289\":2,\"300\":2,\"323\":1,\"373\":1,\"412\":6,\"468\":1,\"474\":3,\"477\":5,\"504\":1,\"514\":1,\"520\":2,\"531\":1,\"558\":1,\"661\":7,\"666\":4,\"667\":3}}],[\"frcnn\",{\"1\":{\"22\":1}}],[\"ftfy\",{\"1\":{\"447\":1}}],[\"ft\",{\"1\":{\"24\":1}}],[\"func函数\",{\"1\":{\"666\":1}}],[\"funcs\",{\"1\":{\"638\":4,\"652\":4,\"654\":4,\"656\":5,\"658\":5,\"666\":5}}],[\"func\",{\"1\":{\"365\":2,\"367\":5,\"369\":3,\"370\":2,\"371\":4,\"372\":6,\"373\":4,\"375\":5,\"376\":3,\"377\":4,\"378\":2,\"379\":11,\"634\":2,\"656\":6,\"658\":3,\"666\":10}}],[\"functools\",{\"1\":{\"365\":1,\"372\":3}}],[\"function及运算符相关类\",{\"1\":{\"661\":1}}],[\"function引用variable\",{\"1\":{\"657\":1}}],[\"function和variable之间原本存在强引用循环\",{\"1\":{\"657\":1}}],[\"function和variable实例存在循环引用\",{\"1\":{\"657\":1}}],[\"function实例引用输入和输出的variable实例\",{\"1\":{\"657\":1}}],[\"functional\",{\"1\":{\"78\":1,\"145\":2,\"147\":2,\"160\":2,\"246\":1,\"247\":2}}],[\"function\",{\"1\":{\"28\":2,\"105\":1,\"115\":1,\"244\":1,\"296\":1,\"365\":1,\"395\":1,\"531\":1,\"565\":1,\"566\":1,\"613\":2,\"617\":1,\"630\":1,\"631\":1,\"634\":1,\"650\":1,\"651\":2,\"656\":2,\"657\":1,\"658\":1,\"660\":6,\"661\":1,\"665\":1,\"666\":1,\"667\":1,\"674\":2}}],[\"functions\",{\"0\":{\"15\":1},\"1\":{\"661\":1}}],[\"fuse\",{\"1\":{\"148\":2}}],[\"fusion\",{\"0\":{\"14\":1},\"1\":{\"30\":2,\"33\":1,\"34\":3,\"35\":6,\"36\":3,\"40\":1,\"41\":1,\"45\":2,\"59\":2,\"70\":1,\"162\":2,\"163\":2}}],[\"full\",{\"1\":{\"145\":1,\"163\":3,\"205\":1,\"416\":1,\"425\":2,\"495\":1,\"590\":1}}],[\"fullattncatblock\",{\"1\":{\"74\":1}}],[\"found\",{\"1\":{\"276\":1,\"277\":1,\"289\":1,\"691\":1}}],[\"foundation\",{\"1\":{\"179\":1,\"418\":1,\"478\":1,\"487\":1,\"677\":1}}],[\"folded\",{\"1\":{\"585\":1}}],[\"folders\",{\"1\":{\"4\":1,\"47\":1}}],[\"follow\",{\"1\":{\"466\":1,\"553\":1,\"556\":1}}],[\"following\",{\"1\":{\"224\":2,\"469\":1,\"520\":1}}],[\"focalloss\",{\"1\":{\"404\":2}}],[\"focal\",{\"0\":{\"404\":1},\"1\":{\"15\":1,\"40\":1,\"55\":1,\"78\":6,\"403\":1,\"404\":23}}],[\"forgetting\",{\"1\":{\"416\":1}}],[\"forge\",{\"1\":{\"338\":1}}],[\"fortran\",{\"1\":{\"323\":1}}],[\"format\",{\"1\":{\"82\":1,\"289\":5,\"300\":1,\"520\":1,\"656\":1,\"666\":5}}],[\"former的生成方法\",{\"1\":{\"286\":1}}],[\"former学习\",{\"1\":{\"285\":1}}],[\"former类比为一个self\",{\"1\":{\"282\":1}}],[\"former模块做模态融合\",{\"1\":{\"280\":1}}],[\"former\",{\"1\":{\"40\":1,\"282\":4,\"285\":2,\"286\":3}}],[\"forward\",{\"0\":{\"247\":1},\"1\":{\"30\":1,\"32\":1,\"33\":1,\"34\":2,\"35\":2,\"36\":2,\"40\":1,\"41\":1,\"45\":2,\"46\":1,\"59\":7,\"70\":2,\"72\":3,\"73\":1,\"74\":3,\"75\":1,\"76\":2,\"78\":1,\"92\":1,\"93\":1,\"96\":2,\"98\":1,\"100\":1,\"101\":1,\"107\":1,\"109\":1,\"110\":1,\"111\":1,\"142\":1,\"145\":1,\"146\":1,\"147\":1,\"161\":1,\"162\":1,\"163\":1,\"242\":1,\"247\":1,\"262\":2,\"263\":1,\"264\":1,\"265\":2,\"266\":1,\"268\":3,\"282\":1,\"284\":1,\"285\":5,\"291\":1,\"292\":3,\"293\":3,\"294\":2,\"295\":1,\"296\":3,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1,\"447\":1,\"477\":5,\"513\":1,\"517\":1,\"523\":1,\"525\":3,\"526\":1,\"527\":1,\"528\":1,\"529\":1,\"531\":2,\"532\":1,\"533\":1,\"535\":1,\"536\":1,\"537\":1,\"538\":1,\"541\":1,\"543\":1,\"544\":1,\"548\":1,\"549\":1,\"550\":1,\"552\":1,\"553\":5,\"554\":1,\"556\":6,\"557\":1,\"558\":1,\"613\":3,\"617\":1,\"630\":1,\"645\":1,\"651\":3,\"656\":1,\"658\":1,\"660\":5}}],[\"fork\",{\"1\":{\"29\":1}}],[\"for\",{\"1\":{\"4\":1,\"29\":3,\"43\":1,\"58\":2,\"59\":2,\"68\":3,\"73\":1,\"80\":2,\"81\":2,\"82\":4,\"83\":1,\"92\":3,\"96\":4,\"100\":2,\"107\":1,\"119\":2,\"134\":1,\"142\":2,\"143\":1,\"145\":6,\"147\":6,\"159\":1,\"162\":7,\"179\":1,\"232\":2,\"244\":3,\"246\":1,\"247\":1,\"248\":1,\"249\":1,\"263\":2,\"266\":5,\"273\":1,\"275\":6,\"276\":2,\"277\":8,\"284\":2,\"285\":8,\"289\":9,\"296\":2,\"300\":1,\"404\":1,\"410\":15,\"411\":5,\"412\":19,\"418\":1,\"419\":1,\"461\":1,\"477\":2,\"510\":4,\"511\":8,\"512\":7,\"513\":2,\"514\":4,\"526\":2,\"531\":5,\"536\":1,\"553\":1,\"554\":1,\"556\":1,\"557\":1,\"558\":1,\"651\":3,\"652\":3,\"654\":2,\"656\":6,\"657\":2,\"658\":8,\"660\":2,\"666\":5,\"667\":2,\"688\":1}}],[\"4b\",{\"1\":{\"674\":1}}],[\"4o\",{\"1\":{\"674\":5}}],[\"4423742632\",{\"1\":{\"666\":1}}],[\"4423761088\",{\"1\":{\"666\":1}}],[\"448px\",{\"1\":{\"215\":3}}],[\"448×448\",{\"1\":{\"207\":1}}],[\"4个特殊词\",{\"1\":{\"511\":1}}],[\"4次\",{\"1\":{\"495\":2}}],[\"4t\",{\"1\":{\"480\":1,\"481\":2,\"482\":1}}],[\"465953\",{\"1\":{\"667\":1}}],[\"465651\",{\"1\":{\"667\":1}}],[\"465348\",{\"1\":{\"667\":1}}],[\"465046\",{\"1\":{\"667\":1}}],[\"464743\",{\"1\":{\"667\":1}}],[\"464440\",{\"1\":{\"667\":1}}],[\"464137\",{\"1\":{\"667\":1}}],[\"463833\",{\"1\":{\"667\":1}}],[\"4638\",{\"1\":{\"520\":2}}],[\"46\",{\"1\":{\"470\":1}}],[\"4点\",{\"1\":{\"455\":1}}],[\"4监督基线的性能\",{\"1\":{\"455\":1}}],[\"4种不同规模的模型\",{\"1\":{\"454\":1}}],[\"4f\",{\"1\":{\"379\":1}}],[\"4×3\",{\"1\":{\"327\":2}}],[\"4替换为相应的类别名称\",{\"1\":{\"289\":1}}],[\"4的视觉能力\",{\"1\":{\"210\":1}}],[\"4k分辨率\",{\"1\":{\"214\":1,\"219\":1}}],[\"4k\",{\"1\":{\"207\":1}}],[\"4v仍有差距\",{\"1\":{\"220\":1}}],[\"4v的\",{\"1\":{\"208\":1}}],[\"4v\",{\"1\":{\"206\":2,\"207\":2,\"208\":2,\"210\":1,\"220\":2,\"222\":1,\"674\":1}}],[\"4c\",{\"1\":{\"190\":1}}],[\"4等\",{\"1\":{\"185\":1}}],[\"404\",{\"1\":{\"691\":1}}],[\"405b\",{\"1\":{\"674\":3}}],[\"40gb文本\",{\"1\":{\"454\":1}}],[\"406\",{\"1\":{\"244\":1}}],[\"40图块\",{\"1\":{\"219\":1}}],[\"40区块\",{\"1\":{\"214\":1}}],[\"400m数据\",{\"1\":{\"280\":1}}],[\"400\",{\"1\":{\"190\":1,\"667\":5}}],[\"40\",{\"1\":{\"166\":1,\"172\":1,\"207\":1,\"447\":1,\"470\":1,\"495\":1,\"666\":1}}],[\"4096\",{\"1\":{\"45\":1,\"101\":2}}],[\"4️⃣\",{\"0\":{\"75\":1},\"1\":{\"100\":1}}],[\"416\",{\"1\":{\"65\":1}}],[\"4大小的特征图\",{\"1\":{\"59\":3}}],[\"4960\",{\"1\":{\"520\":2}}],[\"496\",{\"1\":{\"216\":1}}],[\"49\",{\"1\":{\"30\":2,\"34\":6,\"35\":5,\"674\":1}}],[\"4847711968\",{\"1\":{\"666\":2}}],[\"4847712064\",{\"1\":{\"666\":4}}],[\"4847712112\",{\"1\":{\"666\":2}}],[\"4873\",{\"1\":{\"520\":2}}],[\"485\",{\"1\":{\"226\":1,\"244\":1}}],[\"48层\",{\"1\":{\"188\":1}}],[\"48\",{\"1\":{\"24\":1,\"275\":1,\"397\":1,\"662\":3,\"666\":1}}],[\"4775983056\",{\"1\":{\"666\":2}}],[\"4788\",{\"1\":{\"520\":2}}],[\"47\",{\"1\":{\"22\":1}}],[\"456\",{\"1\":{\"244\":1}}],[\"45层\",{\"1\":{\"214\":1}}],[\"45\",{\"1\":{\"22\":1}}],[\"4e\",{\"1\":{\"10\":1}}],[\"4\",{\"0\":{\"15\":1,\"25\":1,\"65\":1,\"228\":2,\"265\":1,\"293\":1},\"1\":{\"8\":1,\"10\":1,\"22\":1,\"28\":5,\"29\":2,\"30\":1,\"35\":1,\"36\":1,\"40\":1,\"41\":1,\"58\":1,\"59\":28,\"63\":3,\"69\":1,\"76\":2,\"78\":3,\"81\":1,\"82\":2,\"83\":3,\"93\":3,\"96\":3,\"101\":1,\"105\":1,\"142\":3,\"145\":3,\"147\":5,\"157\":1,\"161\":1,\"163\":2,\"190\":1,\"191\":2,\"195\":1,\"200\":1,\"211\":1,\"214\":1,\"220\":1,\"224\":1,\"227\":2,\"228\":6,\"244\":5,\"247\":1,\"268\":4,\"285\":1,\"286\":1,\"295\":1,\"300\":1,\"321\":2,\"322\":2,\"323\":10,\"325\":6,\"326\":7,\"327\":3,\"353\":1,\"384\":1,\"386\":7,\"387\":4,\"395\":2,\"397\":6,\"404\":1,\"410\":5,\"427\":1,\"428\":2,\"447\":2,\"448\":1,\"454\":1,\"455\":5,\"460\":1,\"461\":1,\"462\":5,\"463\":1,\"464\":1,\"469\":1,\"470\":2,\"471\":2,\"472\":1,\"474\":1,\"477\":8,\"480\":1,\"481\":3,\"482\":5,\"483\":2,\"492\":1,\"494\":4,\"498\":1,\"508\":1,\"511\":1,\"513\":2,\"514\":2,\"523\":1,\"544\":1,\"558\":1,\"622\":1,\"645\":1,\"646\":1,\"659\":6,\"669\":1,\"674\":26}}],[\"43类\",{\"1\":{\"17\":1}}],[\"43类物体\",{\"1\":{\"6\":1}}],[\"43\",{\"1\":{\"8\":1,\"22\":1,\"29\":1}}],[\"最重要的是\",{\"1\":{\"678\":1}}],[\"最显著的特征之一是它们的\",{\"1\":{\"676\":1}}],[\"最显著的点\",{\"1\":{\"112\":1}}],[\"最强性能\",{\"1\":{\"674\":1}}],[\"最强知识型\",{\"1\":{\"674\":1}}],[\"最早的\",{\"1\":{\"674\":1}}],[\"最快\",{\"1\":{\"674\":1}}],[\"最广泛使用的一元分布是高斯分布\",{\"1\":{\"584\":1}}],[\"最简单的情况是实验的结果是可数的\",{\"1\":{\"565\":1}}],[\"最简单的转置就是将行和列交换\",{\"1\":{\"326\":1}}],[\"最好要弄清楚为什么预训练模型会有效\",{\"1\":{\"449\":1}}],[\"最有说服力的证据就是到目前为止大量使用预训练的词嵌入来提升一系列nlp任务表现\",{\"1\":{\"440\":1}}],[\"最基本装饰器\",{\"1\":{\"378\":1}}],[\"最基本的函数装饰器\",{\"0\":{\"369\":1}}],[\"最开始三个阶按照\",{\"1\":{\"328\":1}}],[\"最相关的视觉信息\",{\"1\":{\"282\":2}}],[\"最近的一些工作采用新的模型架构和预训练方法来从文本学习视觉特征\",{\"1\":{\"278\":1}}],[\"最老的那个部分\",{\"1\":{\"241\":1}}],[\"最初用于语言模型\",{\"1\":{\"240\":1}}],[\"最具创新性的部分\",{\"1\":{\"224\":1}}],[\"最高支持\",{\"1\":{\"207\":1}}],[\"最多256词\",{\"1\":{\"178\":1}}],[\"最多保留\",{\"1\":{\"92\":1}}],[\"最佳方案是\",{\"1\":{\"134\":1}}],[\"最大序列长度\",{\"1\":{\"513\":1}}],[\"最大模型\",{\"1\":{\"454\":1}}],[\"最大\",{\"1\":{\"447\":1}}],[\"最大的优点就是上述特定数据领域的表现会好很多\",{\"1\":{\"416\":1}}],[\"最大的resnet模型rn50x64需要在592个v100\",{\"1\":{\"272\":1}}],[\"最大生成长度\",{\"1\":{\"286\":1}}],[\"最大化正样本的得分\",{\"1\":{\"240\":1}}],[\"最大池化\",{\"1\":{\"115\":1,\"396\":1}}],[\"最大文本长度\",{\"1\":{\"40\":1}}],[\"最远点\",{\"1\":{\"92\":4}}],[\"最远点采样\",{\"1\":{\"92\":2,\"96\":1}}],[\"最贴近实际应用需求\",{\"1\":{\"82\":1}}],[\"最后实现较好的泛化效果\",{\"1\":{\"686\":1}}],[\"最后形成完整的模型链路来解决整个业务逻辑\",{\"1\":{\"686\":1}}],[\"最后一次失败\",{\"1\":{\"579\":1}}],[\"最后一层的\",{\"1\":{\"160\":1}}],[\"最后一层输出\",{\"1\":{\"109\":1}}],[\"最后一层抽象特征\",{\"1\":{\"101\":1}}],[\"最后一层使用\",{\"1\":{\"98\":1}}],[\"最后重新\",{\"1\":{\"544\":1}}],[\"最后返回两个列表\",{\"1\":{\"512\":1}}],[\"最后经\",{\"1\":{\"474\":1}}],[\"最后相加\",{\"1\":{\"425\":1}}],[\"最后得到当前句子对应的token列表\",{\"1\":{\"411\":1}}],[\"最后把所有点的值求平均\",{\"1\":{\"397\":1}}],[\"最后组合成全局解\",{\"1\":{\"395\":1}}],[\"最后再经过一个线性层映射回原始维度\",{\"1\":{\"315\":1}}],[\"最后再与i\",{\"1\":{\"30\":1}}],[\"最后交换第\",{\"1\":{\"291\":1}}],[\"最后的效果才能好\",{\"1\":{\"259\":1}}],[\"最后和modal\",{\"1\":{\"257\":2}}],[\"最后\",{\"1\":{\"128\":1,\"200\":1,\"275\":1,\"449\":1,\"457\":1}}],[\"最后将结果转换为\",{\"1\":{\"275\":1}}],[\"最后将每个点的局部特征与整个点云的全局特征拼接起来\",{\"1\":{\"111\":1}}],[\"最后将这两个特征送入解码器以获得可供性输出\",{\"1\":{\"8\":1}}],[\"最后通过两个卷积层输出每个点的分类结果\",{\"1\":{\"101\":1}}],[\"最后通过全连接层完成分类任务\",{\"1\":{\"96\":1}}],[\"最后拼接结果\",{\"1\":{\"96\":1}}],[\"最后加上残差连接形成最终输出\",{\"1\":{\"74\":1}}],[\"最小可行性产品\",{\"1\":{\"687\":1}}],[\"最小值也可能不在梯度反方向\",{\"1\":{\"667\":1}}],[\"最小模型仅1\",{\"1\":{\"455\":1}}],[\"最小模型\",{\"1\":{\"454\":1}}],[\"最小生成长度\",{\"1\":{\"286\":1}}],[\"最小化语言模型输出与真实答案之间的交叉熵损失\",{\"1\":{\"227\":1}}],[\"最小化语言模型输出与真实答案之间的差异\",{\"1\":{\"226\":1}}],[\"最小化图像\",{\"1\":{\"190\":1}}],[\"最小池化\",{\"1\":{\"115\":1}}],[\"最小距离\",{\"1\":{\"92\":1}}],[\"最小\",{\"1\":{\"23\":1}}],[\"最终完成一个能训练分类任务的通用框架\",{\"1\":{\"670\":1}}],[\"最终影响训练结果\",{\"1\":{\"654\":1}}],[\"最终答案就是上下文中这两个位置之间的字符串\",{\"1\":{\"542\":1}}],[\"最终答案是大多数投票的结果\",{\"1\":{\"435\":1}}],[\"最终预测得\",{\"1\":{\"508\":1}}],[\"最终选择full\",{\"1\":{\"495\":1,\"497\":1}}],[\"最终选择了参数约为\",{\"1\":{\"196\":1}}],[\"最终选择了深度48\",{\"1\":{\"189\":1}}],[\"最终演进至transformer架构\",{\"1\":{\"485\":1}}],[\"最终在多个基准测试中超越更大规模的闭源模型\",{\"1\":{\"481\":1}}],[\"最终学习率为峰值10\",{\"1\":{\"481\":1}}],[\"最终所得的instructgpt模型\",{\"1\":{\"467\":1}}],[\"最终结果在公开测试集或开发集上报告\",{\"1\":{\"461\":1}}],[\"最终把复杂问题也解决了\",{\"1\":{\"436\":1}}],[\"最终解出了新的问题\",{\"1\":{\"434\":1}}],[\"最终解码为3d功能热图\",{\"1\":{\"6\":1}}],[\"最终子词频率\",{\"1\":{\"410\":1}}],[\"最终生成一个既能表示常见单词\",{\"1\":{\"409\":1}}],[\"最终损失是\",{\"1\":{\"405\":1}}],[\"最终损失为加权和\",{\"1\":{\"55\":1}}],[\"最终都会被映射为一维\",{\"1\":{\"321\":1}}],[\"最终每个\",{\"1\":{\"312\":1,\"540\":1}}],[\"最终每个采样得到的关键点所在的局部领域\",{\"1\":{\"92\":1}}],[\"最终准确率达到\",{\"1\":{\"228\":1}}],[\"最终使1\",{\"1\":{\"215\":1}}],[\"最终使用\",{\"1\":{\"46\":1}}],[\"最终实现强大的多模态理解和生成能力\",{\"1\":{\"190\":1}}],[\"最终的训练效果也是一般\",{\"1\":{\"515\":1}}],[\"最终的\",{\"1\":{\"163\":1}}],[\"最终计算\",{\"1\":{\"156\":1}}],[\"最终构建出高质量的自举数据集\",{\"1\":{\"140\":1}}],[\"最终分类头\",{\"1\":{\"101\":1}}],[\"最终回到原始点数量\",{\"1\":{\"98\":1}}],[\"最终得到约\",{\"1\":{\"226\":1}}],[\"最终得到\",{\"1\":{\"92\":1}}],[\"最终取所有样本的\",{\"1\":{\"82\":1}}],[\"最终\",{\"1\":{\"78\":1,\"107\":1,\"273\":1,\"292\":2,\"404\":1,\"492\":1,\"508\":1}}],[\"最终通过这些\",{\"1\":{\"76\":1}}],[\"最终通过卷积操作生成分割掩码\",{\"1\":{\"70\":1}}],[\"最终输出的维度是\",{\"1\":{\"309\":1}}],[\"最终输出\",{\"1\":{\"111\":1}}],[\"最终输出每个类别的概率分布\",{\"1\":{\"110\":1}}],[\"最终输出高维特征\",{\"1\":{\"109\":1}}],[\"最终输出与点的顺序无关\",{\"1\":{\"105\":1}}],[\"最终输出特征\",{\"1\":{\"100\":1}}],[\"最终输出就是\",{\"1\":{\"96\":1}}],[\"最终输出形状为\",{\"1\":{\"76\":1}}],[\"最终输出融合特征\",{\"1\":{\"73\":1}}],[\"最终输出头\",{\"1\":{\"46\":1}}],[\"最终将融合后的图像特征\",{\"1\":{\"15\":1}}],[\"最终点云融合特征表示为\",{\"1\":{\"14\":1}}],[\"最深层特征\",{\"1\":{\"14\":1}}],[\"见附录图\",{\"1\":{\"471\":1}}],[\"见论文第\",{\"1\":{\"469\":1}}],[\"见论文\",{\"1\":{\"397\":1}}],[\"见表7\",{\"1\":{\"499\":1}}],[\"见表6\",{\"1\":{\"499\":1}}],[\"见表5\",{\"1\":{\"499\":2}}],[\"见表4\",{\"1\":{\"498\":2}}],[\"见表3\",{\"1\":{\"497\":1}}],[\"见表\",{\"1\":{\"461\":2,\"462\":2}}],[\"见表2的参数字段与学习率配置\",{\"1\":{\"480\":1}}],[\"见表2\",{\"1\":{\"208\":1,\"455\":1,\"497\":1}}],[\"见表1的采样比例与磁盘大小\",{\"1\":{\"481\":1}}],[\"见表1\",{\"1\":{\"208\":1,\"497\":1}}],[\"见过\",{\"1\":{\"65\":1}}],[\"见图\",{\"1\":{\"462\":5,\"470\":1}}],[\"见图3\",{\"1\":{\"214\":1}}],[\"见图2\",{\"1\":{\"208\":1}}],[\"见图1训练损失曲线\",{\"1\":{\"480\":1}}],[\"见图1\",{\"1\":{\"208\":1}}],[\"见图4\",{\"1\":{\"208\":1}}],[\"见图4b\",{\"1\":{\"188\":1}}],[\"见图8\",{\"1\":{\"25\":1}}],[\"见图7\",{\"1\":{\"25\":1}}],[\"见图6\",{\"1\":{\"25\":1}}],[\"见图5\",{\"1\":{\"24\":1}}],[\"见\",{\"1\":{\"8\":3,\"29\":1,\"469\":1,\"472\":2}}],[\"r1看scaling\",{\"1\":{\"688\":1}}],[\"r1\",{\"1\":{\"674\":5}}],[\"rtruediv\",{\"1\":{\"660\":1}}],[\"rte\",{\"1\":{\"499\":1}}],[\"rte数据集比较小\",{\"1\":{\"448\":1}}],[\"rdiv\",{\"1\":{\"660\":2}}],[\"rsub\",{\"1\":{\"660\":8}}],[\"r单独进行重排\",{\"1\":{\"600\":1}}],[\"r的相对顺序不变\",{\"1\":{\"600\":1}}],[\"rv\",{\"1\":{\"565\":1}}],[\"rl\",{\"1\":{\"471\":1,\"673\":1,\"674\":1}}],[\"rlhf\",{\"1\":{\"224\":2,\"468\":1,\"469\":4,\"471\":1,\"472\":5}}],[\"rmsnorm\",{\"1\":{\"674\":1}}],[\"rmul\",{\"1\":{\"660\":6,\"661\":1}}],[\"rm\",{\"1\":{\"468\":1,\"470\":7}}],[\"r=1\",{\"1\":{\"427\":1}}],[\"r就是上述假设中的低维\",{\"1\":{\"420\":1}}],[\"r维\",{\"1\":{\"420\":1}}],[\"rudinger\",{\"1\":{\"469\":1}}],[\"rule\",{\"0\":{\"596\":1},\"1\":{\"321\":1,\"567\":2,\"568\":1,\"570\":1,\"596\":2}}],[\"runtime等加速库中可以更好地利用缓存从而提升性能\",{\"1\":{\"326\":1}}],[\"runs\",{\"1\":{\"83\":1}}],[\"run\",{\"1\":{\"29\":7,\"58\":8,\"83\":1,\"339\":1,\"519\":1,\"662\":4,\"666\":1}}],[\"rwightman\",{\"1\":{\"300\":1}}],[\"rn50x16和rnx64\",{\"1\":{\"272\":1}}],[\"rn50x4\",{\"1\":{\"272\":1}}],[\"rnn等模型的缺点是需要顺序计算\",{\"1\":{\"547\":1}}],[\"rnn\",{\"1\":{\"105\":1,\"382\":1,\"460\":1,\"485\":1}}],[\"rcnn\",{\"1\":{\"254\":1}}],[\"rpo\",{\"0\":{\"76\":1}}],[\"rpd\",{\"1\":{\"70\":1,\"76\":2}}],[\"r^\",{\"1\":{\"72\":2,\"73\":1}}],[\"rb\",{\"1\":{\"68\":2,\"83\":2}}],[\"rb×cp×np\",{\"1\":{\"40\":1}}],[\"rb×ci×h×w\",{\"1\":{\"40\":1}}],[\"rotary\",{\"1\":{\"674\":1}}],[\"rosenbrock\",{\"1\":{\"667\":6}}],[\"rosenbrock函数的严格定义是\",{\"1\":{\"667\":1}}],[\"rop\",{\"1\":{\"660\":2}}],[\"rope\",{\"1\":{\"481\":2,\"674\":4}}],[\"robustly\",{\"1\":{\"490\":1,\"496\":1}}],[\"roberta在glue\",{\"1\":{\"502\":1}}],[\"roberta在9项任务中全面超越bert和xlnet\",{\"1\":{\"499\":1}}],[\"roberta证明更长训练\",{\"1\":{\"500\":1}}],[\"roberta延长至300k~500k步\",{\"1\":{\"498\":1}}],[\"roberta新增cc\",{\"1\":{\"498\":1}}],[\"roberta增大至8k\",{\"1\":{\"497\":1}}],[\"roberta核心改进总结\",{\"0\":{\"496\":1}}],[\"roberta改用full\",{\"1\":{\"497\":1}}],[\"roberta改用基于字节的bpe\",{\"1\":{\"495\":1}}],[\"roberta改为动态掩码\",{\"1\":{\"495\":1}}],[\"roberta采用8k批次进行训练\",{\"1\":{\"495\":1}}],[\"roberta尝试增大批次至2k和8k\",{\"1\":{\"495\":1}}],[\"roberta对比了多种输入格式\",{\"1\":{\"495\":1}}],[\"roberta的改进表明\",{\"1\":{\"500\":1}}],[\"roberta的改进\",{\"1\":{\"495\":1}}],[\"roberta是一项针对bert预训练方法的优化研究\",{\"1\":{\"491\":1}}],[\"roberta\",{\"0\":{\"490\":1},\"1\":{\"8\":1,\"13\":1,\"30\":1,\"31\":1,\"72\":1,\"490\":2,\"492\":4,\"493\":5,\"494\":5,\"496\":1}}],[\"rouge分数仅略高于随机选句基线\",{\"1\":{\"455\":1}}],[\"row\",{\"1\":{\"322\":1,\"323\":1,\"510\":2}}],[\"rocket\",{\"1\":{\"273\":1}}],[\"roc\",{\"0\":{\"349\":1,\"350\":1,\"353\":1},\"1\":{\"82\":4,\"350\":3,\"351\":3,\"352\":1,\"353\":2}}],[\"root\",{\"1\":{\"68\":3,\"83\":3,\"142\":1,\"275\":2,\"277\":2,\"289\":8}}],[\"root=\",{\"1\":{\"68\":1}}],[\"roipool\",{\"1\":{\"397\":2}}],[\"roipooling\",{\"1\":{\"397\":1}}],[\"roialign目标\",{\"1\":{\"397\":1}}],[\"roialign\",{\"1\":{\"397\":2}}],[\"roi\",{\"0\":{\"396\":1,\"397\":1},\"1\":{\"59\":24,\"396\":15,\"397\":6}}],[\"rag\",{\"0\":{\"679\":1,\"681\":1},\"1\":{\"679\":8,\"680\":1,\"681\":4,\"682\":1,\"688\":1}}],[\"radd\",{\"1\":{\"660\":10,\"661\":1}}],[\"radford\",{\"1\":{\"454\":1,\"485\":1}}],[\"radius=none\",{\"1\":{\"93\":1}}],[\"radius=0\",{\"1\":{\"93\":2,\"101\":4}}],[\"radius^2\",{\"1\":{\"92\":1}}],[\"radius\",{\"1\":{\"92\":12,\"96\":7}}],[\"race阅读理解\",{\"1\":{\"499\":1}}],[\"race和squad\",{\"1\":{\"491\":1}}],[\"race数据集由初高中考试题构成\",{\"1\":{\"448\":1}}],[\"race\",{\"1\":{\"439\":1,\"440\":1,\"462\":1,\"492\":1,\"494\":1}}],[\"ram\",{\"1\":{\"321\":1}}],[\"ramp\",{\"1\":{\"147\":1}}],[\"raffel\",{\"1\":{\"485\":1}}],[\"raf\",{\"1\":{\"258\":2}}],[\"ratio\",{\"1\":{\"293\":1,\"294\":7,\"295\":2,\"296\":4,\"407\":4}}],[\"ratio=dpr\",{\"1\":{\"296\":1}}],[\"ratio=drop\",{\"1\":{\"294\":1,\"296\":1}}],[\"ratio=mlp\",{\"1\":{\"296\":1}}],[\"ratio=attn\",{\"1\":{\"294\":1,\"296\":1}}],[\"ratio=0\",{\"1\":{\"293\":3,\"294\":3,\"295\":2,\"296\":3}}],[\"ratio=4\",{\"1\":{\"59\":2,\"160\":2,\"293\":1,\"294\":1,\"296\":1}}],[\"rate的缩写\",{\"1\":{\"667\":1}}],[\"rate=2e\",{\"1\":{\"519\":1}}],[\"rate\",{\"1\":{\"80\":1,\"289\":2}}],[\"raise\",{\"1\":{\"68\":1,\"289\":1,\"379\":1,\"511\":1,\"613\":1,\"630\":1,\"643\":1,\"651\":2,\"656\":1,\"659\":1}}],[\"raw=dict\",{\"1\":{\"80\":1,\"83\":1}}],[\"raw\",{\"1\":{\"46\":10,\"59\":6,\"80\":1,\"83\":1,\"401\":1,\"402\":1,\"531\":1}}],[\"rand\",{\"1\":{\"511\":2,\"646\":1}}],[\"randn\",{\"1\":{\"145\":2,\"147\":2,\"160\":2,\"246\":1,\"384\":1,\"387\":1}}],[\"randint\",{\"1\":{\"29\":1,\"68\":1,\"92\":1,\"163\":1,\"511\":1}}],[\"randomsampler\",{\"1\":{\"522\":1}}],[\"randomhorizontalflip\",{\"1\":{\"244\":1,\"290\":1}}],[\"randomgrayscale\",{\"1\":{\"244\":1}}],[\"randomresizedcrop\",{\"1\":{\"244\":1,\"290\":2}}],[\"random\",{\"1\":{\"29\":2,\"58\":1,\"68\":1,\"95\":1,\"163\":5,\"244\":1,\"289\":2,\"511\":5,\"512\":4,\"565\":1,\"646\":1}}],[\"range\",{\"1\":{\"29\":6,\"43\":1,\"58\":4,\"59\":2,\"81\":1,\"82\":2,\"92\":1,\"96\":2,\"142\":1,\"145\":3,\"147\":3,\"162\":2,\"244\":1,\"263\":2,\"275\":1,\"277\":1,\"284\":2,\"285\":1,\"289\":2,\"296\":1,\"410\":2,\"411\":1,\"412\":3,\"477\":1,\"511\":1,\"512\":2,\"513\":1,\"514\":1,\"526\":1,\"667\":2}}],[\"ranking\",{\"1\":{\"470\":1}}],[\"rank=false\",{\"1\":{\"145\":1}}],[\"rank=config\",{\"1\":{\"145\":1}}],[\"rank\",{\"1\":{\"10\":1,\"145\":6,\"420\":1,\"422\":1,\"424\":1,\"425\":1,\"522\":1}}],[\"riezler\",{\"1\":{\"469\":1}}],[\"rigid\",{\"1\":{\"108\":1,\"116\":1}}],[\"right\",{\"1\":{\"40\":1,\"556\":1}}],[\"ride\",{\"1\":{\"29\":1}}],[\"r\",{\"1\":{\"36\":2,\"193\":1,\"194\":1,\"256\":1,\"338\":1,\"396\":2,\"397\":2,\"410\":3,\"412\":6,\"427\":2,\"470\":3,\"510\":1,\"511\":2,\"546\":1,\"599\":2,\"601\":1}}],[\"rgb为彩色图片\",{\"1\":{\"289\":1}}],[\"rgb\",{\"1\":{\"29\":1,\"58\":1,\"114\":1,\"142\":1,\"275\":1,\"277\":1,\"289\":3,\"291\":1}}],[\"reject\",{\"1\":{\"472\":1}}],[\"reverse=true\",{\"1\":{\"410\":1,\"412\":1}}],[\"revealed\",{\"1\":{\"59\":1}}],[\"require\",{\"1\":{\"379\":1}}],[\"requirements\",{\"1\":{\"338\":2,\"546\":2}}],[\"requires\",{\"1\":{\"80\":2,\"246\":1,\"300\":1}}],[\"removed\",{\"1\":{\"412\":1}}],[\"remove\",{\"1\":{\"335\":1}}],[\"remote\",{\"1\":{\"28\":2}}],[\"reddit\",{\"1\":{\"454\":1,\"494\":1}}],[\"red\",{\"1\":{\"273\":1,\"667\":1}}],[\"reduction==\",{\"1\":{\"268\":1}}],[\"reduction=reduction\",{\"1\":{\"268\":1,\"285\":1}}],[\"reduction=\",{\"1\":{\"268\":1,\"285\":1,\"402\":1,\"404\":1}}],[\"reduction\",{\"1\":{\"163\":1,\"285\":1,\"404\":1}}],[\"reduce=none\",{\"1\":{\"163\":1}}],[\"reward\",{\"1\":{\"224\":2,\"470\":1}}],[\"reinforcement\",{\"1\":{\"224\":1,\"416\":2,\"468\":1}}],[\"ret\",{\"1\":{\"666\":4}}],[\"retain\",{\"1\":{\"658\":3}}],[\"retrieval\",{\"0\":{\"679\":1},\"1\":{\"145\":4,\"194\":1,\"679\":1,\"688\":1}}],[\"returnfps\",{\"1\":{\"92\":1}}],[\"returnfps=false\",{\"1\":{\"92\":1}}],[\"returns\",{\"1\":{\"43\":1,\"45\":1,\"46\":1,\"83\":2,\"412\":9,\"520\":1}}],[\"return\",{\"1\":{\"28\":4,\"29\":2,\"30\":1,\"32\":1,\"33\":1,\"34\":2,\"35\":2,\"36\":2,\"40\":4,\"41\":1,\"43\":1,\"45\":2,\"46\":1,\"58\":2,\"59\":8,\"68\":3,\"70\":1,\"72\":2,\"73\":1,\"74\":2,\"75\":1,\"76\":2,\"78\":1,\"83\":2,\"92\":13,\"93\":1,\"96\":3,\"98\":1,\"100\":1,\"101\":1,\"107\":1,\"108\":1,\"109\":2,\"110\":1,\"111\":1,\"142\":4,\"143\":3,\"145\":6,\"146\":5,\"147\":7,\"159\":1,\"161\":2,\"162\":2,\"163\":14,\"244\":1,\"247\":1,\"262\":2,\"263\":1,\"264\":1,\"265\":2,\"266\":1,\"268\":5,\"275\":9,\"276\":4,\"277\":13,\"282\":3,\"284\":2,\"285\":10,\"286\":1,\"289\":7,\"291\":2,\"292\":2,\"293\":2,\"294\":2,\"295\":1,\"296\":2,\"300\":1,\"365\":1,\"366\":1,\"367\":2,\"369\":1,\"370\":3,\"371\":4,\"372\":4,\"373\":2,\"374\":1,\"377\":1,\"379\":6,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1,\"410\":7,\"411\":5,\"412\":22,\"477\":9,\"510\":1,\"511\":5,\"512\":2,\"513\":1,\"517\":2,\"520\":6,\"522\":1,\"523\":1,\"525\":3,\"526\":1,\"527\":1,\"528\":1,\"529\":1,\"531\":2,\"532\":1,\"533\":1,\"535\":1,\"536\":1,\"537\":1,\"538\":1,\"541\":1,\"543\":1,\"544\":1,\"549\":3,\"550\":1,\"552\":1,\"553\":1,\"554\":1,\"556\":1,\"557\":1,\"558\":2,\"613\":2,\"614\":1,\"617\":2,\"622\":1,\"630\":1,\"631\":1,\"641\":2,\"651\":1,\"656\":1,\"658\":2,\"659\":7,\"660\":24,\"662\":3,\"666\":6,\"667\":2}}],[\"regression\",{\"1\":{\"529\":1}}],[\"register\",{\"0\":{\"389\":1},\"1\":{\"145\":4,\"147\":3,\"160\":3,\"246\":2,\"285\":1,\"389\":2}}],[\"region\",{\"1\":{\"59\":1,\"82\":1,\"92\":2,\"252\":2,\"253\":1,\"397\":1}}],[\"regularizer\",{\"1\":{\"108\":3}}],[\"reg\",{\"1\":{\"105\":1}}],[\"record等任务上表现接近sota\",{\"1\":{\"462\":1}}],[\"recovered\",{\"1\":{\"98\":1}}],[\"recall\",{\"0\":{\"344\":1},\"1\":{\"194\":2,\"405\":1}}],[\"recip\",{\"1\":{\"100\":3}}],[\"receiver\",{\"0\":{\"350\":1},\"1\":{\"82\":1}}],[\"rendering\",{\"1\":{\"103\":1}}],[\"render\",{\"1\":{\"83\":1}}],[\"ref\",{\"1\":{\"657\":1,\"658\":1}}],[\"reference\",{\"1\":{\"83\":2}}],[\"referred\",{\"1\":{\"70\":2,\"76\":3}}],[\"refrigerator\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"repr\",{\"1\":{\"659\":1}}],[\"representation\",{\"0\":{\"282\":1},\"1\":{\"148\":2,\"150\":1,\"165\":1,\"232\":2,\"251\":2,\"282\":3,\"283\":2,\"284\":2,\"293\":1,\"296\":3,\"300\":1,\"504\":1,\"542\":1}}],[\"repo\",{\"1\":{\"275\":1,\"277\":1}}],[\"replace\",{\"1\":{\"249\":1,\"275\":1,\"277\":1,\"511\":2,\"659\":1}}],[\"replaced\",{\"1\":{\"163\":3}}],[\"repetition\",{\"1\":{\"143\":3,\"286\":1}}],[\"repeated\",{\"1\":{\"386\":1}}],[\"repeat\",{\"0\":{\"386\":1},\"1\":{\"36\":1,\"92\":6,\"100\":1,\"107\":1,\"109\":1,\"143\":1,\"286\":1,\"386\":7,\"387\":1}}],[\"rephrase\",{\"1\":{\"68\":3}}],[\"releases\",{\"1\":{\"300\":1}}],[\"related\",{\"0\":{\"121\":1,\"150\":1,\"240\":1},\"1\":{\"294\":1}}],[\"relationship\",{\"1\":{\"537\":4,\"538\":4}}],[\"relation\",{\"1\":{\"59\":3}}],[\"relu神经元可自动学习\",{\"1\":{\"395\":1}}],[\"relu\",{\"1\":{\"34\":2,\"35\":3,\"36\":2,\"41\":2,\"42\":1,\"44\":1,\"45\":1,\"46\":2,\"59\":5,\"92\":2,\"93\":2,\"96\":5,\"98\":1,\"100\":4,\"101\":1,\"107\":8,\"109\":2,\"110\":4,\"111\":3,\"670\":1,\"674\":1}}],[\"re\",{\"1\":{\"29\":1,\"410\":2,\"412\":3}}],[\"realnews\",{\"1\":{\"674\":1}}],[\"realtoxicityprompts\",{\"1\":{\"470\":1,\"471\":1,\"482\":1,\"484\":1}}],[\"reader\",{\"1\":{\"510\":3}}],[\"ready\",{\"1\":{\"412\":1}}],[\"read\",{\"1\":{\"29\":5,\"58\":4,\"68\":1,\"83\":2,\"289\":1,\"290\":1,\"410\":1,\"411\":1,\"412\":1,\"519\":9}}],[\"reasoning\",{\"0\":{\"11\":1,\"12\":1},\"1\":{\"11\":1,\"12\":1,\"28\":4,\"227\":1,\"229\":1,\"434\":1,\"435\":1,\"436\":1,\"482\":1}}],[\"respectful\",{\"1\":{\"484\":1}}],[\"respectful类毒性分0\",{\"1\":{\"482\":1}}],[\"response\",{\"1\":{\"224\":1,\"470\":1}}],[\"response4\",{\"1\":{\"28\":2}}],[\"response3\",{\"1\":{\"28\":2}}],[\"response2\",{\"1\":{\"28\":2}}],[\"response1\",{\"1\":{\"28\":2}}],[\"res\",{\"1\":{\"160\":2}}],[\"resolution\",{\"0\":{\"97\":1},\"1\":{\"94\":1,\"98\":1}}],[\"research\",{\"1\":{\"73\":1,\"300\":1,\"448\":1}}],[\"resid\",{\"1\":{\"477\":1}}],[\"residual\",{\"1\":{\"72\":1,\"74\":1,\"267\":1,\"477\":6,\"548\":1}}],[\"resize\",{\"1\":{\"29\":2,\"58\":2,\"290\":2}}],[\"resulting\",{\"1\":{\"412\":1}}],[\"result\",{\"1\":{\"28\":1,\"68\":5,\"367\":2,\"370\":2,\"371\":2,\"377\":2,\"379\":2}}],[\"results\",{\"0\":{\"23\":1},\"1\":{\"81\":1,\"82\":3}}],[\"reshaped\",{\"1\":{\"544\":4}}],[\"reshape后\",{\"1\":{\"34\":1,\"35\":1}}],[\"reshape\",{\"0\":{\"385\":1},\"1\":{\"8\":1,\"34\":2,\"35\":2,\"92\":1,\"107\":1,\"266\":1,\"295\":4,\"323\":2,\"385\":6,\"477\":1,\"544\":1}}],[\"resnet和混合模型的效果均不如vit模型\",{\"1\":{\"299\":1}}],[\"resnet和混合模型在不同图像分类数据集上的测试结果\",{\"1\":{\"299\":1}}],[\"resnet101\",{\"1\":{\"272\":1}}],[\"resnet18在图像分类任务中表现出色\",{\"1\":{\"392\":1}}],[\"resnet18是一种深度残差网络\",{\"1\":{\"392\":1}}],[\"resnet18\",{\"0\":{\"392\":1},\"1\":{\"22\":1,\"40\":1,\"59\":1}}],[\"resnet50\",{\"1\":{\"272\":1}}],[\"resnet包含五种不同尺寸的模型\",{\"1\":{\"272\":1}}],[\"resnet提取特征\",{\"1\":{\"54\":1}}],[\"resnet\",{\"1\":{\"8\":1,\"237\":1,\"246\":1,\"247\":1,\"253\":1,\"272\":1,\"299\":1,\"396\":1}}],[\"随着不断迭代\",{\"1\":{\"674\":1}}],[\"随着各阶段计算量的增加\",{\"1\":{\"673\":1}}],[\"随着语言模型规模的扩大\",{\"1\":{\"673\":1}}],[\"随着\",{\"1\":{\"665\":1,\"673\":1}}],[\"随着时代演进\",{\"1\":{\"504\":1}}],[\"随着模型规模扩展\",{\"1\":{\"465\":1}}],[\"随着假负例的减少\",{\"1\":{\"346\":1}}],[\"随着假正例的减少\",{\"1\":{\"346\":1}}],[\"随着训练的进行\",{\"1\":{\"292\":1}}],[\"随着3d数据集\",{\"1\":{\"7\":1}}],[\"随后\",{\"1\":{\"215\":1,\"673\":1}}],[\"随后reed等人\",{\"1\":{\"178\":1}}],[\"随后线性衰减\",{\"1\":{\"131\":1}}],[\"随后将\",{\"1\":{\"8\":1}}],[\"随机\",{\"1\":{\"565\":1,\"579\":1}}],[\"随机的是它作用的输入\",{\"1\":{\"565\":1}}],[\"随机变量是函数\",{\"1\":{\"565\":1}}],[\"随机变量等于这个值的概率是多少\",{\"1\":{\"565\":1}}],[\"随机变量\",{\"1\":{\"565\":2}}],[\"随机生成等量的非相邻句对\",{\"1\":{\"512\":1}}],[\"随机把一句话中\",{\"1\":{\"505\":1}}],[\"随机高斯分布初始化\",{\"1\":{\"425\":1}}],[\"随机裁剪输入图像\",{\"1\":{\"290\":1}}],[\"随机裁剪图片\",{\"1\":{\"58\":1}}],[\"随机以0\",{\"1\":{\"258\":2}}],[\"随机初始化\",{\"1\":{\"201\":1,\"214\":1}}],[\"随机掩码约\",{\"1\":{\"166\":1,\"172\":1}}],[\"随机采样\",{\"1\":{\"133\":1}}],[\"随机选择\",{\"1\":{\"493\":1}}],[\"随机选\",{\"1\":{\"68\":1}}],[\"随机配对使模型暴露于各种语义上下文中\",{\"1\":{\"66\":1}}],[\"分为训练\",{\"1\":{\"519\":1}}],[\"分为三类\",{\"1\":{\"470\":1}}],[\"分为三种响应类型\",{\"1\":{\"227\":1}}],[\"分隔\",{\"1\":{\"508\":1}}],[\"分隔符\",{\"1\":{\"227\":1}}],[\"分层cache\",{\"1\":{\"477\":1}}],[\"分层学习率衰减\",{\"1\":{\"203\":1}}],[\"分解阶段\",{\"1\":{\"436\":1}}],[\"分析规模带来的质变\",{\"1\":{\"485\":1}}],[\"分析表明\",{\"1\":{\"455\":1}}],[\"分析比较2048单元的单层lstm和transformer\",{\"1\":{\"449\":1}}],[\"分析\",{\"0\":{\"449\":1}}],[\"分析能力\",{\"1\":{\"434\":1}}],[\"分析了不同的参数共享策略对模型性能的影响\",{\"1\":{\"134\":1}}],[\"分母中\",{\"1\":{\"405\":1}}],[\"分母中的\",{\"1\":{\"405\":1}}],[\"分母是两者的并集\",{\"1\":{\"403\":1}}],[\"分母累加那里的\",{\"1\":{\"240\":1}}],[\"分子是预测和\",{\"1\":{\"403\":1}}],[\"分成固定数量的\",{\"1\":{\"397\":1}}],[\"分成\",{\"1\":{\"396\":1}}],[\"分治策略\",{\"1\":{\"395\":1}}],[\"分批次预测\",{\"1\":{\"275\":1}}],[\"分批次从图像列表中取出一批图像\",{\"1\":{\"275\":1}}],[\"分数为1\",{\"1\":{\"448\":1}}],[\"分数\",{\"1\":{\"266\":1,\"402\":1,\"540\":1}}],[\"分数归一化\",{\"1\":{\"45\":1}}],[\"分块与缩略图\",{\"1\":{\"216\":1}}],[\"分块策略\",{\"1\":{\"211\":1}}],[\"分两步\",{\"1\":{\"202\":1}}],[\"分\",{\"1\":{\"195\":1}}],[\"分辨率过高可能略微降低效果\",{\"1\":{\"221\":1}}],[\"分辨率输入\",{\"1\":{\"207\":1}}],[\"分辨率\",{\"1\":{\"190\":1,\"200\":2,\"203\":1}}],[\"分词过程\",{\"0\":{\"411\":1}}],[\"分词器的实现较为简单\",{\"1\":{\"511\":1}}],[\"分词器实现\",{\"0\":{\"511\":1}}],[\"分词器完整代码实现\",{\"1\":{\"412\":1}}],[\"分词器\",{\"1\":{\"170\":1}}],[\"分词为\",{\"1\":{\"170\":1,\"172\":1}}],[\"分配权重\",{\"1\":{\"100\":1}}],[\"分割时的量化\",{\"1\":{\"397\":1}}],[\"分割等任务\",{\"1\":{\"188\":1}}],[\"分割为\",{\"1\":{\"172\":1}}],[\"分割\",{\"1\":{\"166\":1,\"188\":1,\"233\":1}}],[\"分割性能更好\",{\"1\":{\"396\":1}}],[\"分割性能\",{\"1\":{\"112\":1}}],[\"分割精度不高\",{\"1\":{\"112\":1}}],[\"分割任务依赖拼接机制\",{\"1\":{\"112\":1}}],[\"分割任务\",{\"0\":{\"111\":1}}],[\"分割网络\",{\"1\":{\"101\":1}}],[\"分割的整体结构\",{\"1\":{\"98\":1}}],[\"分割出\",{\"1\":{\"40\":1}}],[\"分别提供基础版\",{\"1\":{\"674\":1}}],[\"分别提取图像特征和文本特征\",{\"1\":{\"272\":1}}],[\"分别提取特征\",{\"1\":{\"96\":1}}],[\"分别适用于不同的场景\",{\"1\":{\"674\":1}}],[\"分别计算答案起始下标和结束下标预测得到的交叉熵损失\",{\"1\":{\"541\":1}}],[\"分别送入不同的\",{\"1\":{\"508\":1}}],[\"分别为更受欢迎和较差的响应\",{\"1\":{\"470\":1}}],[\"分别如下\",{\"1\":{\"282\":1}}],[\"分别基于lenet\",{\"1\":{\"275\":1}}],[\"分别在\",{\"1\":{\"203\":1}}],[\"分别表示真实的\",{\"1\":{\"154\":1}}],[\"分别初始化自\",{\"1\":{\"152\":1}}],[\"分别进行微调\",{\"1\":{\"134\":1}}],[\"分别用于生成和过滤文本\",{\"1\":{\"128\":1}}],[\"分别是文本编码器\",{\"1\":{\"272\":1}}],[\"分别是\",{\"1\":{\"108\":1,\"323\":1}}],[\"分别是最大值和它们的位置索引\",{\"1\":{\"92\":1,\"107\":1,\"109\":1}}],[\"分别从以下角度衡量模型表现\",{\"1\":{\"82\":1}}],[\"分别得到\",{\"1\":{\"8\":1}}],[\"分类分布与多项分布\",{\"0\":{\"576\":1}}],[\"分类输出\",{\"1\":{\"513\":1}}],[\"分类结果\",{\"1\":{\"513\":1}}],[\"分类层使用\",{\"1\":{\"447\":1}}],[\"分类阈值\",{\"1\":{\"346\":1}}],[\"分类标记\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"分类累加了\",{\"1\":{\"240\":1}}],[\"分类器共享词嵌入矩阵\",{\"1\":{\"513\":1}}],[\"分类器\",{\"1\":{\"175\":1,\"513\":2}}],[\"分类器预测所有可能的候选块\",{\"1\":{\"166\":1}}],[\"分类头\",{\"1\":{\"160\":1,\"296\":1}}],[\"分类损失\",{\"1\":{\"145\":1}}],[\"分类性能\",{\"1\":{\"112\":1}}],[\"分类性能略逊于多视角方法\",{\"1\":{\"112\":1}}],[\"分类精度略低\",{\"1\":{\"112\":1}}],[\"分类准确率\",{\"1\":{\"112\":1}}],[\"分类准确率仅下降约\",{\"1\":{\"105\":1}}],[\"分类任务是指对输入文本中的每个\",{\"1\":{\"543\":1}}],[\"分类任务的类别数\",{\"1\":{\"292\":1}}],[\"分类任务中对缺失点具有一定鲁棒性\",{\"1\":{\"112\":1}}],[\"分类任务\",{\"0\":{\"110\":1},\"1\":{\"231\":1,\"670\":1}}],[\"分类类别数\",{\"1\":{\"101\":1}}],[\"分类每个点\",{\"1\":{\"98\":1}}],[\"分类判别能力\",{\"1\":{\"82\":1}}],[\"分类\",{\"1\":{\"82\":1,\"101\":1,\"162\":1,\"399\":1,\"448\":1,\"470\":1}}],[\"分类误差\",{\"1\":{\"78\":1,\"407\":1}}],[\"分组并在组内共享键\",{\"1\":{\"674\":2}}],[\"分组查询注意力\",{\"1\":{\"674\":2}}],[\"分组后的文本引导特征\",{\"1\":{\"73\":2}}],[\"分组操作实现了\",{\"1\":{\"72\":1}}],[\"分组\",{\"1\":{\"71\":1}}],[\"分区\",{\"1\":{\"65\":1}}],[\"分布被称为\",{\"1\":{\"587\":1}}],[\"分布上的实验结果\",{\"1\":{\"471\":1}}],[\"分布评估\",{\"1\":{\"470\":1}}],[\"分布是一个\",{\"1\":{\"355\":2}}],[\"分布会更尖锐\",{\"1\":{\"240\":1}}],[\"分布会很平滑\",{\"1\":{\"240\":1}}],[\"分布之间的\",{\"1\":{\"163\":1}}],[\"分布中采样\",{\"1\":{\"147\":1}}],[\"分布一致性\",{\"1\":{\"82\":1}}],[\"分布\",{\"0\":{\"586\":1},\"1\":{\"41\":2,\"55\":1,\"145\":1,\"586\":2}}],[\"分多个步骤\",{\"1\":{\"28\":1}}],[\"分步推理\",{\"1\":{\"6\":1}}],[\"92\",{\"1\":{\"227\":2,\"228\":1}}],[\"914\",{\"1\":{\"226\":1}}],[\"98900063\",{\"1\":{\"667\":1}}],[\"98\",{\"1\":{\"201\":1}}],[\"98b\",{\"1\":{\"190\":1}}],[\"95\",{\"1\":{\"200\":1,\"275\":1}}],[\"9b\",{\"1\":{\"196\":1,\"594\":1,\"674\":5}}],[\"9b参数\",{\"1\":{\"188\":1}}],[\"998\",{\"1\":{\"667\":1}}],[\"997\",{\"1\":{\"667\":1}}],[\"996\",{\"1\":{\"667\":1}}],[\"99449622\",{\"1\":{\"667\":1}}],[\"994\",{\"1\":{\"667\":1}}],[\"993\",{\"1\":{\"667\":1}}],[\"992\",{\"1\":{\"667\":1}}],[\"99\",{\"1\":{\"241\":1,\"343\":1,\"497\":1,\"569\":1}}],[\"995\",{\"1\":{\"145\":1,\"147\":1,\"160\":1,\"667\":1}}],[\"999\",{\"1\":{\"80\":1,\"246\":2,\"667\":1}}],[\"960\",{\"1\":{\"515\":1}}],[\"96\",{\"1\":{\"96\":1,\"191\":1,\"194\":1,\"229\":1}}],[\"9027\",{\"1\":{\"515\":1}}],[\"90\",{\"1\":{\"52\":1,\"112\":1,\"227\":1,\"499\":1,\"673\":1}}],[\"9\",{\"0\":{\"45\":1},\"1\":{\"8\":1,\"22\":1,\"23\":1,\"40\":1,\"59\":1,\"80\":1,\"82\":1,\"107\":4,\"112\":1,\"143\":1,\"145\":3,\"161\":1,\"190\":1,\"193\":1,\"200\":1,\"201\":1,\"203\":1,\"217\":1,\"286\":1,\"321\":2,\"322\":2,\"323\":1,\"325\":2,\"331\":1,\"404\":1,\"407\":1,\"439\":1,\"440\":1,\"455\":1,\"462\":2,\"470\":1,\"471\":1,\"472\":2,\"474\":1,\"482\":3,\"483\":1,\"494\":1,\"519\":1,\"546\":1,\"569\":1,\"659\":1,\"673\":1,\"674\":5,\"684\":1}}],[\"即要开发的应用的应用场景\",{\"1\":{\"687\":1}}],[\"即后训练阶段的强化学习\",{\"1\":{\"673\":1}}],[\"即学习率\",{\"1\":{\"667\":1}}],[\"即调用\",{\"1\":{\"660\":3}}],[\"即y\",{\"1\":{\"660\":1}}],[\"即y=fn\",{\"1\":{\"626\":1}}],[\"即y=x\",{\"1\":{\"626\":1}}],[\"即正向传播\",{\"1\":{\"656\":1}}],[\"即需要知道正向传播时的输出值\",{\"1\":{\"630\":1}}],[\"即求和\",{\"1\":{\"596\":1}}],[\"即确定程度\",{\"1\":{\"596\":1}}],[\"即灰色图像\",{\"1\":{\"594\":1}}],[\"即离原点越远\",{\"1\":{\"592\":1}}],[\"即没有期望值\",{\"1\":{\"587\":1}}],[\"即归一化常数\",{\"1\":{\"587\":1}}],[\"即所谓的标准正态分布\",{\"1\":{\"584\":1}}],[\"即所有实际正例被正确分类为正例的比例\",{\"1\":{\"344\":1}}],[\"即所有\",{\"1\":{\"43\":1}}],[\"即方差小\",{\"1\":{\"584\":1}}],[\"即方差的倒数\",{\"1\":{\"584\":1}}],[\"即蓝球个数\",{\"1\":{\"579\":1}}],[\"即互斥\",{\"1\":{\"567\":1}}],[\"即为事件空间中的每个集合赋予一个\",{\"1\":{\"565\":1}}],[\"即为与当前文本描述相似度最高的那副图片\",{\"1\":{\"276\":1}}],[\"即忽略掉pad部分的损失计算\",{\"1\":{\"514\":2}}],[\"即给定一个前提\",{\"1\":{\"508\":1}}],[\"即给定input\",{\"1\":{\"285\":1}}],[\"即完型填空\",{\"1\":{\"504\":1}}],[\"即随机遮盖输入文本的部分单词\",{\"1\":{\"495\":1}}],[\"即随机移除一部分输入点\",{\"1\":{\"96\":1}}],[\"即预处理时固定掩码模式\",{\"1\":{\"493\":1}}],[\"即gpt类模型一次推理只输出一个token\",{\"1\":{\"474\":1}}],[\"即对不同用户群体可调节输出风格\",{\"1\":{\"472\":1}}],[\"即对点顺序不敏感\",{\"1\":{\"105\":1}}],[\"即便是只有1\",{\"1\":{\"468\":1}}],[\"即便是那些有大量标注数据的场景\",{\"1\":{\"440\":1}}],[\"即标注者和研究者\",{\"1\":{\"468\":1}}],[\"即验证集损失随着模型规模\",{\"1\":{\"464\":1}}],[\"即模型在\",{\"1\":{\"464\":1}}],[\"即模型对真实类别的预测概率\",{\"1\":{\"404\":1}}],[\"即先在大语料上预训练\",{\"1\":{\"464\":1}}],[\"即通过自然语言描述或示例引导模型生成目标输出\",{\"1\":{\"454\":1}}],[\"即通过无监督学习估计文本序列的概率分布\",{\"1\":{\"454\":1}}],[\"即建模\",{\"1\":{\"454\":1}}],[\"即将原文本按照空格\",{\"1\":{\"410\":1}}],[\"即会被保存和加载\",{\"1\":{\"389\":1}}],[\"即内存地址偏移\",{\"1\":{\"325\":1}}],[\"即形状为\",{\"1\":{\"325\":1}}],[\"即注意力得分\",{\"1\":{\"307\":1}}],[\"即网格大小的乘积\",{\"1\":{\"291\":1}}],[\"即图像在水平和垂直方向上分别可以划分的图像块数量\",{\"1\":{\"291\":1}}],[\"即图像特征作为\",{\"1\":{\"143\":1}}],[\"即图片上相邻的区域具有相似的特征\",{\"1\":{\"287\":1}}],[\"即一种先验知识\",{\"1\":{\"287\":1}}],[\"即文本和图像可能不完全匹配\",{\"1\":{\"278\":1}}],[\"即基于对比学习的方法\",{\"1\":{\"278\":1}}],[\"即基于文本弱监督来提升性能\",{\"1\":{\"278\":1}}],[\"即基于动量的对比学习\",{\"1\":{\"236\":1}}],[\"即真正属于一对的文本和图像\",{\"1\":{\"272\":1}}],[\"即上图中橙色和黄色的向量\",{\"1\":{\"508\":1}}],[\"即上图所示的矩阵\",{\"1\":{\"272\":1}}],[\"即上一层级\",{\"1\":{\"97\":1}}],[\"即判断某个样本是否是正样本\",{\"1\":{\"240\":1}}],[\"即从大量候选中选出正确样本\",{\"1\":{\"240\":1}}],[\"即可在多种语言任务中实现从零样本到少样本的泛化\",{\"1\":{\"465\":1}}],[\"即可\",{\"1\":{\"425\":1,\"508\":1}}],[\"即可生成高质量图像描述\",{\"1\":{\"194\":1}}],[\"即可用于生成图像描述\",{\"1\":{\"143\":1}}],[\"即同一个图像可以有多个\",{\"1\":{\"145\":1}}],[\"即在数据预处理阶段生成掩码模式并固定\",{\"1\":{\"495\":1}}],[\"即在预训练阶段让模型隐式学习多种技能\",{\"1\":{\"460\":1}}],[\"即在\",{\"1\":{\"127\":1}}],[\"即在欧氏空间中\",{\"1\":{\"90\":1}}],[\"即只改变物体的方向而不改变形状和大小\",{\"1\":{\"108\":1}}],[\"即只依赖一小部分关键点就能判断整体形状\",{\"1\":{\"105\":1}}],[\"即使计算图不再被用户访问\",{\"1\":{\"657\":1}}],[\"即使它们可能有重叠\",{\"1\":{\"567\":1}}],[\"即使参数量远小于\",{\"1\":{\"469\":1}}],[\"即使参数量远小于原始gpt\",{\"1\":{\"467\":1}}],[\"即使1\",{\"1\":{\"455\":1}}],[\"即使是少样本提示增强的\",{\"1\":{\"471\":1}}],[\"即使是gpt\",{\"1\":{\"462\":1}}],[\"即使是最大的1\",{\"1\":{\"455\":1}}],[\"即使是一维位置编码\",{\"1\":{\"293\":1}}],[\"即使外部函数已经执行完毕\",{\"1\":{\"366\":1}}],[\"即使冻结\",{\"1\":{\"190\":1}}],[\"即使冻结llm解码器也能在多模态对话任务中表现优异\",{\"1\":{\"189\":1}}],[\"即使其他点都在\",{\"1\":{\"112\":1}}],[\"即使\",{\"1\":{\"105\":1,\"353\":1}}],[\"即使丢失一些点或加入异常点\",{\"1\":{\"105\":1}}],[\"即根据最近的几个邻近点的距离进行加权平均\",{\"1\":{\"98\":1}}],[\"即每个图像块经过卷积操作后得到的特征向量的维度\",{\"1\":{\"291\":1}}],[\"即每个点的各个类别得分\",{\"1\":{\"111\":1}}],[\"即每个点都有一个类别预测\",{\"1\":{\"101\":1}}],[\"即每个点对应的\",{\"1\":{\"40\":1}}],[\"即每个查询点有一个特征向量\",{\"1\":{\"92\":1}}],[\"即每个局部邻域内的点数量维度\",{\"1\":{\"92\":1}}],[\"即空间中的任何距离值具有相似的含义\",{\"1\":{\"90\":1}}],[\"即任何方向上的度量都是等价的\",{\"1\":{\"90\":1}}],[\"即任务是找到点云集中的局部区域的中心点\",{\"1\":{\"89\":1}}],[\"即无功能区域\",{\"1\":{\"82\":1}}],[\"即生成一个二值掩码\",{\"1\":{\"70\":1}}],[\"即物体支持的交互可能性\",{\"1\":{\"49\":1}}],[\"即\",{\"1\":{\"8\":1,\"76\":1,\"82\":1,\"109\":1,\"112\":1,\"115\":1,\"226\":1,\"343\":1,\"375\":1,\"376\":1,\"404\":1,\"468\":1,\"506\":1,\"508\":1,\"541\":1,\"542\":1,\"564\":2,\"565\":1,\"569\":2,\"575\":1,\"577\":1,\"578\":1,\"583\":1}}],[\"为开发者带来了全面而强大的功能支持\",{\"1\":{\"684\":1}}],[\"为开源社区提供了可复现的基线\",{\"1\":{\"483\":1}}],[\"为模型提供丰富的上下文信息\",{\"1\":{\"679\":1}}],[\"为variable添加shape等属性\",{\"1\":{\"663\":1}}],[\"为variable类添加grad属性\",{\"1\":{\"629\":1}}],[\"为优化算法和神经网络训练奠定基础\",{\"1\":{\"662\":1}}],[\"为将\",{\"1\":{\"661\":1}}],[\"为避免手动修改config属性\",{\"1\":{\"658\":1}}],[\"为function类添加反向传播方法backward\",{\"1\":{\"630\":1}}],[\"为止\",{\"1\":{\"579\":1}}],[\"为归一化常数\",{\"1\":{\"571\":1}}],[\"为定义概率度量\",{\"1\":{\"566\":1}}],[\"为随机变量\",{\"1\":{\"565\":1}}],[\"为简化记号\",{\"1\":{\"565\":1}}],[\"为当前批次中的每个序列样本生成一个位置序列\",{\"1\":{\"523\":1}}],[\"为这些方法提供一个更清晰的性能基准\",{\"1\":{\"501\":1}}],[\"为ai研究的民主化提供了重要范例\",{\"1\":{\"486\":1}}],[\"为ai安全和实用性的发展提供了关键路径\",{\"1\":{\"468\":1}}],[\"为缓解对奖励函数的过度优化\",{\"1\":{\"470\":1}}],[\"为生成类任务\",{\"1\":{\"470\":1}}],[\"为防止泄露隐私\",{\"1\":{\"470\":1}}],[\"为确保训练集与评估集分离\",{\"1\":{\"470\":1}}],[\"为语言模型行为与用户意图之间架起了桥梁\",{\"1\":{\"468\":1}}],[\"为未来的系统化扩展和安全性提升奠定了坚实基础\",{\"1\":{\"684\":1}}],[\"为未来通用语言智能系统的发展提供了重要方向\",{\"1\":{\"465\":1}}],[\"为未来通用语言智能系统奠定了基础\",{\"1\":{\"461\":1}}],[\"为未来探索更通用的ai系统奠定了基础\",{\"1\":{\"453\":1}}],[\"为保证数据质量\",{\"1\":{\"461\":1}}],[\"为残差层数\",{\"1\":{\"454\":1}}],[\"为构建通用语言处理系统提供了新思路\",{\"1\":{\"452\":1}}],[\"为收集更多的标注数据提供了更多一个有价值的替代方案\",{\"1\":{\"440\":1}}],[\"为例\",{\"0\":{\"315\":1},\"1\":{\"411\":1,\"660\":2,\"673\":1}}],[\"为特定任务动态构建了一个分类器\",{\"1\":{\"273\":1}}],[\"为下一阶段的端到端微调提供了良好的初始化\",{\"1\":{\"226\":1}}],[\"为赋予模型高分辨率处理和ocr能力\",{\"1\":{\"215\":1}}],[\"为提升效率\",{\"1\":{\"200\":1}}],[\"为多模态大模型的发展提供了重要贡献\",{\"1\":{\"180\":1}}],[\"为视觉\",{\"1\":{\"166\":1}}],[\"为视觉transformer的自监督预训练提供了新思路\",{\"1\":{\"165\":1}}],[\"为hidden\",{\"1\":{\"161\":1}}],[\"为文本长度\",{\"1\":{\"161\":1}}],[\"为主\",{\"1\":{\"159\":1}}],[\"为何前期要让\",{\"1\":{\"159\":1}}],[\"为解决这一问题\",{\"1\":{\"157\":1,\"166\":1,\"215\":1,\"468\":1}}],[\"为每张图像生成\",{\"1\":{\"138\":1}}],[\"为每个特定任务\",{\"1\":{\"418\":1}}],[\"为每个图像选择一个负样本文本\",{\"1\":{\"284\":1}}],[\"为每个文本选择一个负样本图像\",{\"1\":{\"284\":1}}],[\"为每个文本选择一个最相似的非匹配图像\",{\"1\":{\"156\":1}}],[\"为每个类别创建一个描述性的文本\",{\"1\":{\"273\":1}}],[\"为每个原始点\",{\"1\":{\"100\":1}}],[\"为每个中心点找到其邻域内的点\",{\"1\":{\"98\":1}}],[\"为每个尺度构建一个独立的小型\",{\"1\":{\"96\":1}}],[\"为每个关键点构建局部邻域\",{\"1\":{\"92\":1}}],[\"为每个组合额外生成\",{\"1\":{\"63\":1}}],[\"为后续实现神经网络层和优化算法奠定了基础\",{\"1\":{\"660\":1}}],[\"为后续研究\",{\"1\":{\"454\":1}}],[\"为后续研究提供了灵活的基础\",{\"1\":{\"217\":1}}],[\"为后续三维深度学习奠定了基础\",{\"1\":{\"105\":1}}],[\"为后续的神经网络模块与训练机制打下坚实基础\",{\"1\":{\"665\":1}}],[\"为后续的处理步骤提供信息\",{\"1\":{\"97\":1}}],[\"为后续的跨模态交互提供基础\",{\"1\":{\"59\":1}}],[\"为点云\",{\"1\":{\"74\":1}}],[\"为点云坐标\",{\"1\":{\"54\":1}}],[\"为此引入第三方工具graphviz\",{\"1\":{\"666\":1}}],[\"为此\",{\"1\":{\"71\":1,\"97\":1,\"604\":1,\"658\":1}}],[\"为功能类别标签\",{\"1\":{\"54\":1}}],[\"为rgb图像\",{\"1\":{\"54\":1}}],[\"为机器人操作\",{\"1\":{\"49\":1}}],[\"为坐标\",{\"1\":{\"35\":1}}],[\"为什么高斯样本集中在壳层上\",{\"0\":{\"593\":1}}],[\"为什么答案来自\",{\"1\":{\"542\":1}}],[\"为什么还需要prompt\",{\"1\":{\"430\":1}}],[\"为什么还要使用一个移动平均的编码器呢\",{\"1\":{\"237\":1}}],[\"为什么不也用\",{\"1\":{\"426\":1}}],[\"为什么要用第一个位置\",{\"1\":{\"508\":1}}],[\"为什么要对大模型进行微调\",{\"0\":{\"415\":1}}],[\"为什么要把它们结合起来\",{\"1\":{\"402\":1}}],[\"为什么要除以\",{\"0\":{\"318\":1}}],[\"为什么函数复合更高效\",{\"1\":{\"395\":1}}],[\"为什么\",{\"0\":{\"317\":1}}],[\"为什么第二个分支不直接不更新\",{\"1\":{\"241\":1}}],[\"为什么呢\",{\"1\":{\"241\":1}}],[\"为什么叫做个体判别呢\",{\"1\":{\"235\":1}}],[\"为什么需要这个正则化项\",{\"1\":{\"108\":1}}],[\"为什么使用队列这种数据结构存储字典呢\",{\"1\":{\"241\":1}}],[\"为什么使用\",{\"1\":{\"62\":1,\"407\":1}}],[\"为什么我们需要pair\",{\"1\":{\"29\":1}}],[\"为什么壶嘴适合倒水\",{\"1\":{\"6\":1}}],[\"为实现这一目标\",{\"1\":{\"26\":1}}],[\"为进一步评估模型的理解与泛化能力\",{\"1\":{\"25\":1}}],[\"为验证所提方法\",{\"1\":{\"21\":1}}],[\"为支撑开放词汇\",{\"1\":{\"16\":1}}],[\"为输出头\",{\"1\":{\"15\":1}}],[\"为全连接层\",{\"1\":{\"14\":1}}],[\"为了提高训练稳定性\",{\"1\":{\"674\":1}}],[\"为了提升框架的易用性\",{\"1\":{\"660\":1}}],[\"为了提升数据质量\",{\"1\":{\"226\":1}}],[\"为了提升模型对点云姿态变化的鲁棒性\",{\"1\":{\"108\":1}}],[\"为了探索性能的极限\",{\"1\":{\"673\":1}}],[\"为了确保variable实例在混合运算中优先被处理\",{\"1\":{\"660\":1}}],[\"为了让variable实例支持自然的乘法表达式\",{\"1\":{\"660\":1}}],[\"为了让llm给出的答案更加靠谱\",{\"1\":{\"433\":1}}],[\"为了便于区分和调试\",{\"1\":{\"659\":1}}],[\"为了解决大型语言模型在生成文本时面临的一系列挑战\",{\"1\":{\"679\":1}}],[\"为了解决上述的问题\",{\"1\":{\"656\":1}}],[\"为了解决这个问题\",{\"1\":{\"682\":1}}],[\"为了解决这类反向问题\",{\"1\":{\"597\":1}}],[\"为了解决这一问题\",{\"1\":{\"94\":1,\"467\":1}}],[\"为了更好地支持多输入函数\",{\"1\":{\"651\":1}}],[\"为了更精确\",{\"1\":{\"396\":1}}],[\"为了方便使用\",{\"1\":{\"614\":1,\"660\":1}}],[\"为了方便大家理解\",{\"1\":{\"414\":1}}],[\"为了简洁\",{\"1\":{\"565\":1}}],[\"为了简单\",{\"1\":{\"552\":1}}],[\"为了避免每次训练时看到相同的掩码模式\",{\"1\":{\"495\":1}}],[\"为了避免除以零\",{\"1\":{\"403\":1}}],[\"为了衡量模型的\",{\"1\":{\"470\":1}}],[\"为了保证标注质量\",{\"1\":{\"470\":1}}],[\"为了反映这点\",{\"1\":{\"445\":1}}],[\"为了将\",{\"1\":{\"403\":1}}],[\"为了将其作为损失函数使用\",{\"1\":{\"401\":1}}],[\"为了将几何属性与点云特征更好地对齐融合\",{\"1\":{\"14\":1}}],[\"为了充分利用预训练的权重\",{\"1\":{\"290\":1}}],[\"为了弥补数据规模上的差距\",{\"1\":{\"278\":1}}],[\"为了实现这一点\",{\"1\":{\"653\":1}}],[\"为了实现文字搜索图像的功能\",{\"1\":{\"276\":1}}],[\"为了实现多任务高效预训练\",{\"1\":{\"126\":1}}],[\"为了训练一个能够泛化到多种任务的语言模型\",{\"1\":{\"454\":1}}],[\"为了训练一个既具理解能力又具生成能力的统一模型\",{\"1\":{\"126\":1}}],[\"为了训练好q\",{\"1\":{\"282\":1}}],[\"为了训练clip模型\",{\"1\":{\"272\":1}}],[\"为了模拟用户提问和模型回答的形式\",{\"1\":{\"226\":1}}],[\"为了模仿传统卷积网络中的权重共享机制以提高学习效率和模型的泛化能力\",{\"1\":{\"86\":1}}],[\"为了验证\",{\"1\":{\"136\":1}}],[\"为了能够在不同的局部子集上共享权重\",{\"1\":{\"86\":1}}],[\"为了获得对物体可供性更深入的理解\",{\"1\":{\"10\":1}}],[\"为\",{\"1\":{\"8\":1,\"10\":1,\"14\":1,\"15\":1,\"142\":1,\"147\":1,\"173\":1,\"296\":1,\"315\":1,\"323\":1,\"344\":1,\"350\":2,\"351\":4,\"410\":1,\"445\":1,\"470\":1,\"471\":1,\"472\":1,\"565\":1,\"577\":1,\"581\":1,\"593\":1,\"655\":1,\"684\":1,\"685\":1}}],[\"为图像生成合成文本\",{\"1\":{\"140\":1}}],[\"为图像中主体和物体的边界框\",{\"1\":{\"54\":1}}],[\"为图像\",{\"1\":{\"8\":1}}],[\"可组合性\",{\"1\":{\"684\":1}}],[\"可观察性\",{\"1\":{\"684\":1}}],[\"可解释性相对较低\",{\"1\":{\"681\":1}}],[\"可解释性\",{\"1\":{\"681\":1}}],[\"可规避捷径行为\",{\"1\":{\"674\":1}}],[\"可在\",{\"1\":{\"674\":1}}],[\"可逐渐接近目标位置\",{\"1\":{\"667\":1}}],[\"可直接将数学表达式转译为代码\",{\"1\":{\"662\":1}}],[\"可直接微调\",{\"1\":{\"460\":1}}],[\"可验证tinypytorch框架处理高阶微分的能力\",{\"1\":{\"662\":1}}],[\"可微分编程\",{\"1\":{\"660\":1,\"662\":1}}],[\"可正常转换3为variable\",{\"1\":{\"660\":1}}],[\"可共用同一实现\",{\"1\":{\"660\":1}}],[\"可继续添加ndarray的其他属性\",{\"1\":{\"659\":1}}],[\"可变参数输入与输出列表\",{\"1\":{\"651\":1}}],[\"可将其比作存放数据的\",{\"1\":{\"606\":1}}],[\"可由补集规则推出\",{\"1\":{\"567\":1}}],[\"可加性\",{\"1\":{\"567\":1}}],[\"可进一步限制事件空间只包含区间\",{\"1\":{\"566\":1}}],[\"可进一步增强性能\",{\"1\":{\"132\":1}}],[\"可数个区间并集交集\",{\"1\":{\"566\":1}}],[\"可用于展示tinypytorch计算图\",{\"1\":{\"666\":1}}],[\"可用于梯度检验\",{\"1\":{\"623\":1}}],[\"可用于多头自注意力的图像\",{\"1\":{\"262\":1}}],[\"可用于多头自注意力的文本\",{\"1\":{\"262\":1}}],[\"可用于多模态任务\",{\"1\":{\"231\":1}}],[\"可用计数法则进行理解\",{\"1\":{\"600\":1}}],[\"可用柱状图表示\",{\"1\":{\"565\":1}}],[\"可考虑换成\",{\"1\":{\"511\":1}}],[\"可挪到其他地方实现\",{\"1\":{\"511\":1}}],[\"可输出规范代码\",{\"1\":{\"483\":1}}],[\"可复用\",{\"1\":{\"660\":1}}],[\"可复用已有大模型权重\",{\"1\":{\"231\":1}}],[\"可复现性\",{\"1\":{\"482\":1}}],[\"可显著降低对齐带来的性能损失\",{\"1\":{\"472\":1}}],[\"可\",{\"1\":{\"471\":1}}],[\"可基本恢复甚至超越\",{\"1\":{\"471\":1}}],[\"可有效缓解\",{\"1\":{\"470\":1}}],[\"可逆tokenizer等设计\",{\"1\":{\"461\":1}}],[\"可表示任意\",{\"1\":{\"454\":1}}],[\"可调性强\",{\"1\":{\"407\":1}}],[\"可调\",{\"1\":{\"404\":1}}],[\"可处理连续值掩码\",{\"1\":{\"402\":1}}],[\"可处理连续概率值\",{\"1\":{\"401\":1}}],[\"可通过contextlib模块实现with语句上下文管理\",{\"1\":{\"658\":1}}],[\"可通过禁用反向传播模式进一步节省内存\",{\"1\":{\"658\":1}}],[\"可通过反证法证明\",{\"1\":{\"567\":1}}],[\"可通过局部神经元\",{\"1\":{\"395\":1}}],[\"可通过训练动态调整\",{\"1\":{\"395\":1}}],[\"可逼近连续但不可微的函数\",{\"1\":{\"395\":1}}],[\"可最大限度地提高\",{\"1\":{\"353\":1}}],[\"可使用\",{\"1\":{\"331\":1,\"335\":1}}],[\"可使用成熟的\",{\"1\":{\"114\":1}}],[\"可认为是模型参数一部分\",{\"1\":{\"282\":1}}],[\"可作为独立视觉编码器或与语言中间件结合\",{\"1\":{\"181\":1}}],[\"可学习位置嵌入\",{\"1\":{\"293\":1}}],[\"可学习query\",{\"1\":{\"286\":1}}],[\"可学习查询和交叉注意力层\",{\"1\":{\"190\":1}}],[\"可学习\",{\"1\":{\"147\":1}}],[\"可学习的位置编码\",{\"1\":{\"76\":1}}],[\"可视为一种结构化的知识蒸馏方式\",{\"1\":{\"123\":1}}],[\"可视化结果显示复杂计算图\",{\"1\":{\"666\":1}}],[\"可视化工具封装\",{\"1\":{\"666\":1}}],[\"可视化计算图\",{\"0\":{\"666\":1}}],[\"可视化\",{\"0\":{\"665\":1}}],[\"可视化示意\",{\"0\":{\"319\":1}}],[\"可视化当前物体点云\",{\"1\":{\"83\":1}}],[\"可视化功能区域预测结果\",{\"1\":{\"83\":1}}],[\"可视化支持\",{\"1\":{\"24\":1}}],[\"可视化分析\",{\"1\":{\"23\":1}}],[\"可为\",{\"1\":{\"100\":1}}],[\"可为空\",{\"1\":{\"98\":1}}],[\"可改进的地方\",{\"1\":{\"90\":1}}],[\"可能缺乏必要的推理能力\",{\"1\":{\"679\":1}}],[\"可能对应多个潜在的隐藏状态\",{\"1\":{\"597\":1}}],[\"可能取值的最新信念状态\",{\"1\":{\"596\":1}}],[\"可能取值的概率分布的公式\",{\"1\":{\"596\":1}}],[\"可能是被替换的词\",{\"1\":{\"505\":1}}],[\"可能是经验设定\",{\"1\":{\"78\":1}}],[\"可能因无法学习长距离依赖\",{\"1\":{\"495\":1}}],[\"可能会被掩码不同的单词\",{\"1\":{\"495\":1}}],[\"可能会有人这么问\",{\"1\":{\"430\":1}}],[\"可能提升性能\",{\"1\":{\"493\":1}}],[\"可能限制泛化\",{\"1\":{\"483\":1}}],[\"可能限制模型性能\",{\"1\":{\"178\":1}}],[\"可能遗漏歧义与分歧点\",{\"1\":{\"472\":1}}],[\"可能遗漏重要细节\",{\"1\":{\"112\":1}}],[\"可能影响输出的一致性与代表性\",{\"1\":{\"472\":1}}],[\"可能需要模型具备多偏好条件控制能力\",{\"1\":{\"472\":1}}],[\"可能需要大量神经元\",{\"1\":{\"395\":1}}],[\"可能不会质疑\",{\"1\":{\"471\":1}}],[\"可能不足以代表复杂的局部结构\",{\"1\":{\"112\":1}}],[\"可能以\",{\"1\":{\"395\":1}}],[\"可能更为合适\",{\"1\":{\"353\":1}}],[\"可能采用一个更好的代理任务会取得更好的效果\",{\"1\":{\"239\":1}}],[\"可能误导主模型\",{\"1\":{\"159\":1}}],[\"可能存在多个正样本\",{\"1\":{\"145\":1}}],[\"可能存在的问题\",{\"1\":{\"89\":1}}],[\"可能无法捕捉重要的几何细节\",{\"1\":{\"89\":1}}],[\"可能导致样本在高密度区域内过度集中\",{\"1\":{\"89\":1}}],[\"可选部分\",{\"1\":{\"660\":1}}],[\"可选参数\",{\"1\":{\"402\":1,\"403\":1,\"404\":1}}],[\"可选输出张量\",{\"1\":{\"381\":1}}],[\"可选的\",{\"1\":{\"163\":1}}],[\"可选颜色\",{\"1\":{\"114\":1}}],[\"可选属性\",{\"1\":{\"114\":1}}],[\"可选地拼接\",{\"1\":{\"98\":1}}],[\"可选\",{\"0\":{\"113\":1},\"1\":{\"83\":1,\"92\":1,\"109\":1,\"401\":1,\"405\":1,\"407\":1,\"423\":1}}],[\"可以充分发挥大语言模型的强大能力\",{\"1\":{\"687\":1}}],[\"可以基本实现目标的\",{\"1\":{\"687\":1}}],[\"可以追溯到具体的数据来源\",{\"1\":{\"681\":1}}],[\"可以根据特定风格或术语调整\",{\"1\":{\"681\":1}}],[\"可以根据图像描述生成合理的文字解释\",{\"1\":{\"226\":1}}],[\"可以达到数十亿甚至数千亿个参数\",{\"1\":{\"675\":1}}],[\"可以保持效果的情况下\",{\"1\":{\"674\":1}}],[\"可以显示部分思维链\",{\"1\":{\"674\":1}}],[\"可以处理简单的计算图\",{\"1\":{\"648\":1}}],[\"可以处理任意顺序的点集\",{\"1\":{\"105\":1}}],[\"可以通过学习上下文来解决少样本任务\",{\"1\":{\"673\":1}}],[\"可以通过连续使用平方函数和指数函数实现\",{\"1\":{\"616\":1}}],[\"可以通过调整这些超参数来进一步优化损失函数的性能\",{\"1\":{\"408\":1}}],[\"可以证明\",{\"1\":{\"578\":1,\"579\":1,\"582\":1}}],[\"可以推导出贝叶斯法则\",{\"1\":{\"570\":1}}],[\"可以得到未归一化的联合分布\",{\"1\":{\"596\":1}}],[\"可以得到乘法法则\",{\"1\":{\"568\":1}}],[\"可以得到每个类别的预测概率\",{\"1\":{\"273\":1}}],[\"可以自发地学习执行任务\",{\"1\":{\"457\":1}}],[\"可以避免传统tokenization的损失\",{\"1\":{\"455\":1}}],[\"可以避免因为输出缓存导致日志卡在某一行不输出的问题\",{\"1\":{\"83\":1}}],[\"可以实现巨大的收益\",{\"1\":{\"439\":1}}],[\"可以实现按行\",{\"1\":{\"325\":1}}],[\"可以用于多种语言\",{\"1\":{\"675\":1}}],[\"可以用于训练大型数据集\",{\"1\":{\"392\":1}}],[\"可以用\",{\"1\":{\"426\":1}}],[\"可以被看作是\",{\"1\":{\"425\":1}}],[\"可以阅读这篇论文\",{\"1\":{\"421\":1}}],[\"可以在更快的时间内响应\",{\"1\":{\"674\":1}}],[\"可以在保持对齐的同时维持甚至提升性能\",{\"1\":{\"472\":1}}],[\"可以在保证模型效果的同时\",{\"1\":{\"421\":1}}],[\"可以在下游任务中获得较好的迁移效果\",{\"1\":{\"287\":1}}],[\"可以媲美全量微调的效果了\",{\"1\":{\"421\":1}}],[\"可以引导大模型有更加出色的表现\",{\"1\":{\"419\":1}}],[\"可以参见lora\",{\"1\":{\"420\":1}}],[\"可以参见\",{\"1\":{\"418\":1,\"419\":1,\"421\":1}}],[\"可以参考论文\",{\"1\":{\"274\":1}}],[\"可以多个方案一起\",{\"1\":{\"416\":1}}],[\"可以有效控制词汇表大小\",{\"1\":{\"409\":1}}],[\"可以考虑使用tversky\",{\"1\":{\"408\":1}}],[\"可以采用\",{\"1\":{\"397\":1}}],[\"可以扩展成\",{\"1\":{\"387\":1}}],[\"可以任意重新排列所有维度\",{\"1\":{\"383\":1}}],[\"可以使用\",{\"1\":{\"335\":1,\"338\":1}}],[\"可以使用预训练的\",{\"1\":{\"299\":1}}],[\"可以不同\",{\"0\":{\"317\":1}}],[\"可以独立于设计\",{\"1\":{\"313\":1}}],[\"可以利用其自注意力机制捕捉特征之间的长距离依赖关系\",{\"1\":{\"299\":1}}],[\"可以看出第\",{\"1\":{\"474\":1}}],[\"可以看这篇文章\",{\"1\":{\"295\":1}}],[\"可以看到当epochs增大时\",{\"1\":{\"299\":1}}],[\"可以看到8个图像\",{\"1\":{\"273\":1}}],[\"可以看到对于要预测的8个图像\",{\"1\":{\"273\":1}}],[\"可以看到\",{\"1\":{\"238\":1,\"293\":2,\"294\":1,\"297\":1,\"298\":1,\"506\":1}}],[\"可以变成一个\",{\"1\":{\"291\":1}}],[\"可以直接通过类名调用\",{\"1\":{\"289\":1}}],[\"可以直接接受图像数据\",{\"1\":{\"169\":1}}],[\"可以将\",{\"1\":{\"683\":1}}],[\"可以将现在的各种大模型通过轻量微调变成各个不同领域的专业模型\",{\"1\":{\"428\":1}}],[\"可以将低秩矩阵\",{\"1\":{\"423\":1}}],[\"可以将规律总结为\",{\"1\":{\"328\":1}}],[\"可以将搜索范围限制在满足这些性质的模型子空间内\",{\"1\":{\"287\":1}}],[\"可以将其平移到坐标系的中心\",{\"1\":{\"86\":1}}],[\"可以和所有的query\",{\"1\":{\"285\":1}}],[\"可以和所有自己的tokens做attention\",{\"1\":{\"285\":1}}],[\"可以和很多代理任务结合\",{\"1\":{\"238\":1}}],[\"可以学习到如何更好地结合文本提取图片信息\",{\"1\":{\"282\":1}}],[\"可以学习更丰富的词汇表达\",{\"1\":{\"157\":1}}],[\"可以概括为以下两个主要步骤\",{\"1\":{\"273\":1}}],[\"可以mask成\",{\"1\":{\"258\":1}}],[\"可以进行梯度回传进行更新\",{\"1\":{\"242\":1}}],[\"可以设置为\",{\"1\":{\"241\":1}}],[\"可以取得和之前最优的无监督方法相近甚至更好的结果\",{\"1\":{\"237\":1}}],[\"可以先去了解一下python中的高级索引机制\",{\"1\":{\"92\":1}}],[\"可以理解为对多维张量的各个维度\",{\"1\":{\"326\":1}}],[\"可以理解为\",{\"1\":{\"86\":1}}],[\"可以是\",{\"1\":{\"82\":1}}],[\"可以确保语言语义不会丢失\",{\"1\":{\"72\":1}}],[\"可以加微信备注来意\",{\"1\":{\"2\":1}}],[\"可见这两句话就不是连续的\",{\"1\":{\"506\":1}}],[\"可见\",{\"1\":{\"49\":1}}],[\"可操作性\",{\"1\":{\"46\":1}}],[\"可操作性热图\",{\"0\":{\"46\":1},\"1\":{\"40\":1}}],[\"可抓握\",{\"1\":{\"29\":1}}],[\"可供性数据集\",{\"1\":{\"26\":1}}],[\"可供性区域的精准定位\",{\"1\":{\"26\":1}}],[\"可供性类别\",{\"1\":{\"17\":1}}],[\"可供性与交互图像直接联系起来\",{\"1\":{\"15\":1}}],[\"可供性预测质量\",{\"1\":{\"22\":1}}],[\"可供性预测\",{\"1\":{\"15\":1}}],[\"可供性意图知识特征\",{\"1\":{\"13\":1}}],[\"可供性标注\",{\"1\":{\"8\":1}}],[\"是前两代\",{\"1\":{\"674\":1}}],[\"是专为复杂推理设计的模型\",{\"1\":{\"674\":1}}],[\"是验证框架能力的理想案例\",{\"1\":{\"662\":1}}],[\"是包含交叉项的二维函数\",{\"1\":{\"662\":1}}],[\"是简单的平方和函数\",{\"1\":{\"662\":1}}],[\"是如何产生的\",{\"1\":{\"597\":1}}],[\"是真的\",{\"1\":{\"596\":1}}],[\"是真实标签\",{\"1\":{\"404\":1}}],[\"是已知的固定值\",{\"1\":{\"596\":1}}],[\"是维度数\",{\"1\":{\"591\":1}}],[\"是相关系数\",{\"1\":{\"590\":1}}],[\"是beta\",{\"1\":{\"586\":1}}],[\"是gamma\",{\"1\":{\"586\":1}}],[\"是尺度参数\",{\"1\":{\"586\":1}}],[\"是均值向量\",{\"1\":{\"590\":1}}],[\"是均值\",{\"1\":{\"586\":1}}],[\"是归一化常数\",{\"1\":{\"584\":1}}],[\"是实数\",{\"1\":{\"580\":1}}],[\"是该分布的均值\",{\"1\":{\"577\":1}}],[\"是分布的均值\",{\"1\":{\"575\":1}}],[\"是从\",{\"1\":{\"575\":1}}],[\"是整个贝叶斯推断的核心\",{\"1\":{\"572\":1}}],[\"是独立事件\",{\"1\":{\"568\":1}}],[\"是你允许讨论的事件的全集合\",{\"1\":{\"566\":1}}],[\"是你输入的问题\",{\"1\":{\"314\":1}}],[\"是所有让\",{\"1\":{\"565\":1}}],[\"是概率度量\",{\"1\":{\"564\":1}}],[\"是事件空间\",{\"1\":{\"564\":1}}],[\"是样本空间\",{\"1\":{\"564\":1}}],[\"是样本协方差矩阵\",{\"1\":{\"359\":1}}],[\"是编码器模型\",{\"1\":{\"542\":1}}],[\"是答案终点的得分\",{\"1\":{\"540\":1}}],[\"是答案起点的得分\",{\"1\":{\"540\":1}}],[\"是模型预测出的答案的起始和结束位置\",{\"1\":{\"542\":1}}],[\"是模型预测的概率\",{\"1\":{\"404\":1}}],[\"是模型最后一层所有\",{\"1\":{\"540\":1}}],[\"是序列实际长度\",{\"1\":{\"520\":1}}],[\"是序列长度\",{\"1\":{\"310\":1}}],[\"是没有任何实际意义的\",{\"1\":{\"508\":1}}],[\"是没有一个单独的编码器\",{\"1\":{\"242\":1}}],[\"是非常耗成本的\",{\"1\":{\"504\":1}}],[\"是非常重要的\",{\"1\":{\"420\":1}}],[\"是什么\",{\"0\":{\"504\":1}}],[\"是对bert预训练过程的系统性优化\",{\"1\":{\"496\":1}}],[\"是对称的\",{\"1\":{\"105\":1,\"590\":1}}],[\"是多头自注意力中每个头切分到的维度\",{\"1\":{\"477\":1}}],[\"是历史缓存\",{\"1\":{\"477\":1}}],[\"是系统性地探索\",{\"1\":{\"464\":1}}],[\"是目前元学习方法的一个重要限制\",{\"1\":{\"463\":1}}],[\"是蕴含\",{\"1\":{\"448\":1}}],[\"是字符嵌入矩阵\",{\"1\":{\"443\":1}}],[\"是层数\",{\"1\":{\"443\":1}}],[\"是参数\",{\"1\":{\"443\":1}}],[\"是上下文字符向量\",{\"1\":{\"443\":1}}],[\"是上下文窗口大小\",{\"1\":{\"443\":1}}],[\"是上一个时刻的输出\",{\"1\":{\"236\":1}}],[\"是正确的\",{\"1\":{\"435\":1}}],[\"是人类发给各种人工智能模型\",{\"1\":{\"430\":1}}],[\"是训练参数\",{\"1\":{\"425\":1}}],[\"是固定不变的\",{\"1\":{\"425\":1}}],[\"是预训练模型初始化的参数\",{\"1\":{\"425\":1}}],[\"是预测的概率值\",{\"1\":{\"404\":1}}],[\"是预测值与真实值之间的平均绝对误差\",{\"1\":{\"82\":1}}],[\"是低秩的秩\",{\"1\":{\"423\":1}}],[\"是影响大模型生成结果的关键参数\",{\"1\":{\"420\":1}}],[\"是基座模型\",{\"1\":{\"418\":1}}],[\"是基于对话聊天的\",{\"1\":{\"678\":1}}],[\"是基于基座模型开发出来的\",{\"1\":{\"674\":1}}],[\"是基于文本输入来生成图像的模型\",{\"1\":{\"270\":1}}],[\"是基于\",{\"1\":{\"268\":1}}],[\"是能够在可控成本的前提下\",{\"1\":{\"416\":1}}],[\"是交叉熵和\",{\"1\":{\"407\":1}}],[\"是两种主流的方法\",{\"1\":{\"681\":1}}],[\"是两种用于提升预训练语言模型\",{\"1\":{\"231\":1}}],[\"是两个广泛用于语言模型训练和评估的英文维基百科语料数据集\",{\"1\":{\"510\":1}}],[\"是两个可调节的超参数\",{\"1\":{\"405\":1}}],[\"是衡量两个样本集合之间重叠程度的一种指标\",{\"1\":{\"401\":1}}],[\"是特征图相对于原图的缩放比例\",{\"1\":{\"396\":1}}],[\"是原张量的一个视图\",{\"1\":{\"384\":1}}],[\"是闭包\",{\"1\":{\"367\":1,\"369\":1}}],[\"是它的逆矩阵\",{\"1\":{\"359\":1}}],[\"是协方差矩阵的逆\",{\"1\":{\"358\":1}}],[\"是比较两个不同模型性能的有效衡量指标\",{\"1\":{\"353\":1}}],[\"是按选定的间隔\",{\"1\":{\"350\":1}}],[\"是在张量\",{\"1\":{\"327\":1}}],[\"是数据向量\",{\"1\":{\"590\":1}}],[\"是数据的协方差矩阵\",{\"1\":{\"358\":1}}],[\"是数据库中的内容\",{\"1\":{\"314\":1}}],[\"是数据库中的索引\",{\"1\":{\"314\":1}}],[\"是数学和物理学中用于表示多维数组的一个概念\",{\"1\":{\"321\":1}}],[\"是用于信息表达的维度\",{\"1\":{\"317\":1}}],[\"是用于计算相似度的维度\",{\"1\":{\"317\":1}}],[\"是hidden\",{\"1\":{\"297\":1}}],[\"是批量大小\",{\"1\":{\"291\":1}}],[\"是卷积核的步长\",{\"1\":{\"291\":1}}],[\"是卷积核的大小\",{\"1\":{\"291\":1}}],[\"是输出通道数\",{\"1\":{\"291\":1}}],[\"是输入通道数\",{\"1\":{\"291\":1}}],[\"是输入图像分辨率\",{\"1\":{\"169\":1}}],[\"是第一个transformer模块的input\",{\"1\":{\"282\":1}}],[\"是许多早期vlp模型的标准做法\",{\"1\":{\"253\":1}}],[\"是动量超参数\",{\"1\":{\"236\":1}}],[\"是很灵活的\",{\"1\":{\"235\":1}}],[\"是引导模型\",{\"1\":{\"231\":1}}],[\"是教会模型\",{\"1\":{\"231\":1}}],[\"是由深度求索\",{\"1\":{\"674\":1}}],[\"是由单词词缀组成的\",{\"1\":{\"238\":1}}],[\"是由\",{\"1\":{\"224\":1}}],[\"是主流选择\",{\"1\":{\"212\":1}}],[\"是随机初始化的\",{\"1\":{\"191\":2}}],[\"是位置嵌入矩阵\",{\"1\":{\"443\":1}}],[\"是位置嵌入\",{\"1\":{\"171\":1}}],[\"是通道数\",{\"1\":{\"169\":1,\"291\":1}}],[\"是通过\",{\"1\":{\"145\":1}}],[\"是为了支持任意参数签名\",{\"1\":{\"370\":1}}],[\"是为了把\",{\"1\":{\"286\":1}}],[\"是为了在突出\",{\"1\":{\"162\":1}}],[\"是为了扩大负样本池\",{\"1\":{\"161\":1}}],[\"是为了扩展成\",{\"1\":{\"108\":1}}],[\"是二分类\",{\"1\":{\"156\":1}}],[\"是当前文本对应的类别标签\",{\"1\":{\"520\":1}}],[\"是当前时刻的输入\",{\"1\":{\"236\":1}}],[\"是当前最具代表性的通用多模态基础模型之一\",{\"1\":{\"197\":1}}],[\"是当前\",{\"1\":{\"145\":1}}],[\"是当前规模最大的同类数据集\",{\"1\":{\"16\":1}}],[\"是严格的一对一\",{\"1\":{\"145\":1}}],[\"是每个块的分辨率\",{\"1\":{\"169\":1}}],[\"是每个图像对应的索引编号\",{\"1\":{\"145\":1}}],[\"是每个点的高维特征\",{\"1\":{\"105\":1}}],[\"是每个点云局部区域关键点引导下提取的图像信息增强后的点云关键点局部区域特征\",{\"1\":{\"41\":1}}],[\"是指示函数\",{\"1\":{\"576\":1}}],[\"是指我们为了让llm能够更好地完成我们给它的任务\",{\"1\":{\"430\":1}}],[\"是指用于训练模型的参数非常多\",{\"1\":{\"414\":1}}],[\"是指被错误地归类为正例的所有实际负例所占的比例\",{\"1\":{\"345\":1}}],[\"是指位置嵌入的参数是可以在模型训练过程中通过反向传播算法进行更新的\",{\"1\":{\"293\":1}}],[\"是指\",{\"1\":{\"116\":1,\"596\":1}}],[\"是最具挑战也最通用的形式\",{\"1\":{\"461\":1}}],[\"是最大可容忍的点云范围\",{\"1\":{\"105\":1}}],[\"是最终的\",{\"1\":{\"15\":1}}],[\"是关键点集合\",{\"1\":{\"105\":1}}],[\"是后续的全连接网络\",{\"1\":{\"105\":1}}],[\"是类别数\",{\"1\":{\"98\":1}}],[\"是无缓存的\",{\"1\":{\"83\":1}}],[\"是有局限的\",{\"1\":{\"472\":1}}],[\"是有缓存的\",{\"1\":{\"83\":1}}],[\"是有效的\",{\"1\":{\"43\":1}}],[\"是否能生成新文本\",{\"1\":{\"542\":1}}],[\"是否为连贯的上下句\",{\"1\":{\"512\":1}}],[\"是否为推理模式\",{\"1\":{\"40\":1}}],[\"是否尊重约束\",{\"1\":{\"470\":1}}],[\"是否适合类别不平衡\",{\"1\":{\"407\":1}}],[\"是否直接优化\",{\"1\":{\"403\":1}}],[\"是否对类别不平衡敏感\",{\"1\":{\"403\":1}}],[\"是否对\",{\"1\":{\"401\":1,\"402\":1,\"403\":1,\"404\":1}}],[\"是否可调\",{\"1\":{\"407\":1}}],[\"是否可用于改变维度\",{\"1\":{\"387\":1}}],[\"是否可微\",{\"1\":{\"115\":1}}],[\"是否复制数据\",{\"1\":{\"387\":1}}],[\"是否一起变化\",{\"1\":{\"355\":1}}],[\"是否在生成q\",{\"1\":{\"295\":1}}],[\"是否采样\",{\"1\":{\"286\":1}}],[\"是否使用核采样\",{\"1\":{\"286\":1}}],[\"是否使用梯度检查点\",{\"1\":{\"147\":1}}],[\"是否使用梯度检查点优化vit显存占用\",{\"1\":{\"142\":1}}],[\"是否更新全部参数\",{\"1\":{\"231\":1}}],[\"是否修改模型结构\",{\"1\":{\"231\":1}}],[\"是否仅返回\",{\"1\":{\"163\":1}}],[\"是否被\",{\"1\":{\"163\":1}}],[\"是否被后续模型改进\",{\"1\":{\"112\":1}}],[\"是否跨\",{\"1\":{\"145\":1}}],[\"是否包含法线信息\",{\"1\":{\"93\":1}}],[\"是否关注区域匹配\",{\"1\":{\"407\":1}}],[\"是否关注像素分类\",{\"1\":{\"407\":1}}],[\"是否关注空间重合度\",{\"1\":{\"82\":1}}],[\"是否关注分布相似性\",{\"1\":{\"82\":1}}],[\"是否依赖\",{\"1\":{\"82\":1}}],[\"是否支持广播\",{\"1\":{\"387\":1}}],[\"是否支持\",{\"1\":{\"82\":1,\"403\":1}}],[\"是语言增强后的点特征\",{\"1\":{\"74\":1}}],[\"是线性变换\",{\"1\":{\"74\":1,\"154\":1}}],[\"是roberta编码后的文本特征\",{\"1\":{\"72\":1}}],[\"是点数\",{\"1\":{\"64\":1}}],[\"是点云\",{\"1\":{\"8\":1}}],[\"是\",{\"1\":{\"64\":1,\"76\":1,\"78\":1,\"82\":9,\"83\":1,\"92\":2,\"100\":1,\"105\":1,\"108\":1,\"115\":7,\"143\":1,\"155\":1,\"163\":1,\"212\":1,\"227\":1,\"231\":1,\"240\":1,\"285\":1,\"289\":1,\"367\":2,\"381\":1,\"383\":1,\"386\":1,\"387\":3,\"390\":1,\"397\":2,\"403\":8,\"407\":1,\"470\":1,\"472\":1,\"495\":1,\"504\":1,\"540\":1,\"541\":1,\"590\":1,\"674\":2}}],[\"是一款面向消费级应用的轻量级模型\",{\"1\":{\"674\":1}}],[\"是一张\",{\"1\":{\"594\":1}}],[\"是一项针对\",{\"1\":{\"492\":1}}],[\"是一些句子对\",{\"1\":{\"448\":1}}],[\"是一点点地更新的\",{\"1\":{\"238\":1}}],[\"是一种令人兴奋的技术\",{\"1\":{\"678\":1}}],[\"是一种像人类一样思考和学习的人工智能\",{\"1\":{\"678\":1}}],[\"是一种旨在理解和生成人类语言的人工智能模型\",{\"1\":{\"673\":1}}],[\"是一种低成本高回报的对齐方法\",{\"1\":{\"472\":1}}],[\"是一种在保证模型效果基本不降低的前提下\",{\"1\":{\"421\":1}}],[\"是一种结合了多个损失函数优点的混合损失函数\",{\"1\":{\"407\":1}}],[\"是一种针对类别不平衡\",{\"1\":{\"404\":1}}],[\"是一种常用的损失函数\",{\"1\":{\"403\":1}}],[\"是一种常用于语义分割任务的损失函数\",{\"1\":{\"401\":1}}],[\"是一种基于均值为\",{\"1\":{\"587\":1}}],[\"是一种基于自注意力机制\",{\"1\":{\"548\":1}}],[\"是一种基于自监督学习的视觉transformer预训练模型\",{\"1\":{\"165\":1}}],[\"是一种基于频率统计的子词分词算法\",{\"1\":{\"409\":1}}],[\"是一种基于直方图交集的相似性指标\",{\"1\":{\"82\":1}}],[\"是一种对输入顺序不敏感的函数\",{\"1\":{\"115\":1}}],[\"是一种表示三维空间中物体或场景的方式\",{\"1\":{\"114\":1}}],[\"是一种单尺度网络\",{\"1\":{\"112\":1}}],[\"是一种\",{\"1\":{\"73\":1}}],[\"是一种残差连接\",{\"1\":{\"72\":1}}],[\"是一种新颖的框架\",{\"1\":{\"5\":1}}],[\"是一个完整的系统\",{\"1\":{\"680\":1}}],[\"是一个从事件\",{\"1\":{\"564\":1}}],[\"是一个线性层\",{\"1\":{\"540\":1}}],[\"是一个线性变换\",{\"1\":{\"72\":1}}],[\"是一个包含1亿个词汇的英文词库数据\",{\"1\":{\"510\":1}}],[\"是一个非常强大且直观的张量操作工具\",{\"1\":{\"390\":1}}],[\"是一个闭包\",{\"1\":{\"366\":1}}],[\"是一个\",{\"1\":{\"359\":1,\"542\":2,\"566\":1}}],[\"是一个大规模的图像数据集\",{\"1\":{\"300\":1}}],[\"是一个随机初始化的向量\",{\"1\":{\"292\":1}}],[\"是一个分类头\",{\"1\":{\"285\":1}}],[\"是一个batch一个batch地去做\",{\"1\":{\"238\":1}}],[\"是一个开源的多模态大语言模型\",{\"1\":{\"207\":1}}],[\"是一个可学习的温度参数\",{\"1\":{\"154\":1}}],[\"是一个新的视觉\",{\"1\":{\"138\":1}}],[\"是一个新颖的\",{\"1\":{\"120\":1}}],[\"是一个兼顾理解与生成\",{\"1\":{\"129\":1}}],[\"是一个图像引导的文本编码器\",{\"1\":{\"128\":1}}],[\"是一个图像引导的文本解码器\",{\"1\":{\"128\":1}}],[\"是一个小型神经网络\",{\"1\":{\"107\":1}}],[\"是一个跨模态特征投影模块\",{\"1\":{\"59\":1}}],[\"是连接具身智能体感知与操作的关键\",{\"1\":{\"49\":1}}],[\"是图像块的总数\",{\"1\":{\"291\":1}}],[\"是图像宽度\",{\"1\":{\"291\":1}}],[\"是图像高度\",{\"1\":{\"291\":1}}],[\"是图像每个通道的标准差\",{\"1\":{\"290\":1}}],[\"是图像每个通道的均值\",{\"1\":{\"290\":1}}],[\"是图像\",{\"1\":{\"41\":1,\"82\":1}}],[\"关系\",{\"1\":{\"656\":1}}],[\"关注学习固定词向量\",{\"1\":{\"464\":1}}],[\"关注每个像素的分类准确性\",{\"1\":{\"407\":1}}],[\"关注每个点的分类误差\",{\"1\":{\"402\":1}}],[\"关注整体区域匹配程度\",{\"1\":{\"407\":1}}],[\"关注整体区域匹配\",{\"1\":{\"403\":1}}],[\"关注整体区域匹配度\",{\"1\":{\"402\":1}}],[\"关注整体掩码匹配度\",{\"1\":{\"78\":1}}],[\"关注图像\",{\"1\":{\"264\":1}}],[\"关注排序能力\",{\"1\":{\"82\":1}}],[\"关键在于实现类型转换工具函数as\",{\"1\":{\"660\":1}}],[\"关键要点\",{\"0\":{\"609\":1}}],[\"关键里程碑\",{\"1\":{\"485\":1}}],[\"关键性能提升\",{\"1\":{\"483\":1}}],[\"关键发现\",{\"1\":{\"482\":1,\"484\":1}}],[\"关键结论\",{\"1\":{\"472\":1}}],[\"关键区别\",{\"1\":{\"395\":1}}],[\"关键共同点\",{\"1\":{\"395\":1}}],[\"关键字参数\",{\"1\":{\"363\":2}}],[\"关键优势\",{\"1\":{\"220\":1}}],[\"关键参数的作用\",{\"1\":{\"404\":1}}],[\"关键参数\",{\"1\":{\"174\":1}}],[\"关键点可能丢失\",{\"1\":{\"112\":1}}],[\"关键点集\",{\"1\":{\"105\":1}}],[\"关键功能\",{\"1\":{\"59\":1}}],[\"关键问题\",{\"1\":{\"7\":1,\"484\":1}}],[\"关于qlora的具体细节\",{\"1\":{\"421\":1}}],[\"关于lora的具体细节\",{\"1\":{\"420\":1}}],[\"关于计算\",{\"1\":{\"404\":1}}],[\"关于vit模型的不同版本\",{\"1\":{\"297\":1}}],[\"关于多头注意力机制流程不太清楚的\",{\"1\":{\"295\":1}}],[\"关于norm层\",{\"1\":{\"294\":1}}],[\"关于bertmodel的代码解析部分\",{\"1\":{\"284\":1}}],[\"关于这一领域的详细综述\",{\"1\":{\"274\":1}}],[\"关于利用roi\",{\"1\":{\"59\":1}}],[\"关于我们\",{\"0\":{\"1\":1}}],[\"视图\",{\"1\":{\"325\":1}}],[\"视觉编码阶段\",{\"1\":{\"285\":1}}],[\"视觉编码器提取图像特征\",{\"1\":{\"226\":1}}],[\"视觉编码器规模的影响\",{\"1\":{\"221\":1}}],[\"视觉编码器\",{\"1\":{\"214\":1,\"219\":1,\"226\":1,\"227\":1}}],[\"视觉编码器+注意力池化\",{\"1\":{\"188\":1}}],[\"视觉分支\",{\"1\":{\"280\":1}}],[\"视觉+语言\",{\"1\":{\"231\":1}}],[\"视觉联合推理能力\",{\"1\":{\"220\":1}}],[\"视觉主干设计选择\",{\"1\":{\"196\":1}}],[\"视觉推理任务中均表现稳定\",{\"1\":{\"195\":1}}],[\"视觉定位等任务\",{\"1\":{\"190\":1}}],[\"视觉感知任务\",{\"1\":{\"189\":1}}],[\"视觉模型的规模不足和对齐低效是阻碍多模态能力提升的主要障碍\",{\"1\":{\"186\":1}}],[\"视觉模型通常基于纯视觉数据或bert系列模型训练\",{\"1\":{\"181\":1}}],[\"视觉基础模型的性能瓶颈仍是制约vllms发展的关键因素\",{\"1\":{\"185\":1}}],[\"视觉基础模型在过去十年中经历了显著发展\",{\"1\":{\"183\":1}}],[\"视觉标记学习包含两个模块\",{\"1\":{\"170\":1}}],[\"视觉标记\",{\"0\":{\"170\":1}}],[\"视觉标记通过离散变分自编码器\",{\"1\":{\"166\":1}}],[\"视觉transformer模型大小\",{\"1\":{\"142\":1}}],[\"视觉\",{\"0\":{\"122\":1},\"1\":{\"126\":1,\"147\":1,\"149\":1,\"157\":1,\"190\":2,\"253\":1}}],[\"视觉的跨模态交互能力\",{\"1\":{\"75\":1}}],[\"视觉与大语言模型\",{\"1\":{\"69\":1}}],[\"视觉语义特征\",{\"1\":{\"40\":1,\"45\":1}}],[\"视觉任务中\",{\"1\":{\"7\":1}}],[\"视频检索等任务中达到sota\",{\"1\":{\"181\":1}}],[\"视频分类\",{\"1\":{\"180\":1}}],[\"视频中分割功能区域\",{\"1\":{\"51\":1}}],[\"视频\",{\"1\":{\"7\":1,\"180\":1}}],[\"与用户负反馈\",{\"1\":{\"687\":1}}],[\"与合作伙伴包进行有效分离\",{\"1\":{\"684\":1}}],[\"与合成文本\",{\"1\":{\"132\":1}}],[\"与特定应用程序的数据进行交互的接口\",{\"1\":{\"683\":1}}],[\"与语言模型交互的接口\",{\"1\":{\"683\":1}}],[\"与以前的预训练语言模型\",{\"1\":{\"676\":1}}],[\"与以往在合成任务或小型模型上的研究不同\",{\"1\":{\"472\":1}}],[\"与基座模型有本质的区别\",{\"1\":{\"674\":1}}],[\"与训练流程规范\",{\"1\":{\"670\":1}}],[\"与梯度相乘的值设为lr=0\",{\"1\":{\"667\":1}}],[\"与反向传播类似\",{\"1\":{\"666\":1}}],[\"与define\",{\"1\":{\"662\":1}}],[\"与运行结果一致\",{\"1\":{\"662\":1}}],[\"与函数\",{\"1\":{\"650\":1}}],[\"与正向传播方向相反\",{\"1\":{\"626\":1}}],[\"与似然函数\",{\"1\":{\"596\":1}}],[\"与原点的距离为\",{\"1\":{\"591\":1}}],[\"与原始\",{\"1\":{\"454\":1}}],[\"与原始步幅一致\",{\"1\":{\"327\":1}}],[\"与原始点\",{\"1\":{\"92\":1}}],[\"与事件空间相关联的概率规律\",{\"1\":{\"567\":1}}],[\"与bert结论相反\",{\"1\":{\"500\":1}}],[\"与chinchiila\",{\"1\":{\"484\":1}}],[\"与chinchilla\",{\"1\":{\"480\":1}}],[\"与opt等模型趋势一致\",{\"1\":{\"482\":1}}],[\"与指令微调\",{\"1\":{\"482\":1}}],[\"与triviaqa\",{\"1\":{\"482\":1}}],[\"与毒性\",{\"1\":{\"480\":1}}],[\"与预训练相比\",{\"1\":{\"472\":1}}],[\"与上述方法的不同之处在于其训练数据源真实\",{\"1\":{\"469\":1}}],[\"与人类学习方式不匹配\",{\"1\":{\"460\":1}}],[\"与单向语言模型的互补性\",{\"1\":{\"456\":1}}],[\"与传统指针网络方法形成鲜明对比\",{\"1\":{\"456\":1}}],[\"与传统的依赖复杂架构或辅助损失的方法不同\",{\"1\":{\"177\":1}}],[\"与常规训练\",{\"1\":{\"455\":1}}],[\"与lstms相比\",{\"1\":{\"449\":1}}],[\"与llama\",{\"1\":{\"181\":1}}],[\"与llms的特征空间存在差异\",{\"1\":{\"181\":1}}],[\"与升维矩阵\",{\"1\":{\"425\":1}}],[\"与负样本\",{\"1\":{\"404\":1}}],[\"与背景的极端不平衡问题\",{\"1\":{\"404\":1}}],[\"与交叉熵损失函数相比\",{\"1\":{\"404\":1}}],[\"与交互意图\",{\"1\":{\"6\":1}}],[\"与对象检测任务不同\",{\"1\":{\"399\":1}}],[\"与多项式逼近的对比\",{\"1\":{\"395\":1}}],[\"与多项式逼近\",{\"1\":{\"395\":1}}],[\"与多模态嵌入\",{\"1\":{\"43\":1}}],[\"与多模态大模型\",{\"1\":{\"7\":1}}],[\"与所有负样本的相似度\",{\"1\":{\"390\":1}}],[\"与之类似的还有\",{\"1\":{\"297\":1}}],[\"与其训练更大的模型\",{\"1\":{\"472\":1}}],[\"与其正样本的点积\",{\"1\":{\"390\":1}}],[\"与其他\",{\"1\":{\"292\":1,\"403\":1}}],[\"与其在整个数据集上计算损失\",{\"1\":{\"240\":1}}],[\"与计算机视觉\",{\"1\":{\"271\":1}}],[\"与计算效率间取得平衡\",{\"1\":{\"188\":1}}],[\"与此同时\",{\"1\":{\"270\":1,\"342\":1,\"462\":1,\"673\":1}}],[\"与视觉\",{\"1\":{\"267\":1}}],[\"与图像标记\",{\"1\":{\"178\":1}}],[\"与自然语言类似\",{\"1\":{\"170\":1}}],[\"与自注意力层\",{\"1\":{\"13\":1}}],[\"与主模型参数不同步\",{\"1\":{\"147\":1}}],[\"与动量队列的相似度\",{\"1\":{\"145\":1}}],[\"与解码\",{\"1\":{\"134\":1}}],[\"与k最近邻\",{\"1\":{\"90\":1}}],[\"与\",{\"1\":{\"15\":1,\"19\":1,\"29\":1,\"33\":1,\"49\":2,\"55\":1,\"82\":1,\"134\":1,\"161\":1,\"166\":1,\"190\":1,\"228\":1,\"283\":2,\"284\":1,\"289\":1,\"350\":1,\"395\":1,\"402\":1,\"425\":3,\"454\":1,\"470\":1,\"471\":2,\"510\":1,\"566\":1,\"568\":2,\"660\":1,\"665\":1,\"674\":3}}],[\"与意图特征\",{\"1\":{\"14\":1}}],[\"利用包含中间推理步骤的提示机制来解决这些任务\",{\"1\":{\"676\":1}}],[\"利用了恒等式\",{\"1\":{\"580\":1}}],[\"利用pytorch从\",{\"1\":{\"503\":1}}],[\"利用cot\",{\"1\":{\"435\":1}}],[\"利用clip编码器实现文本\",{\"1\":{\"7\":1}}],[\"利用nltk库提供的wordpunct\",{\"1\":{\"410\":1}}],[\"利用nltk库提供的sent\",{\"1\":{\"410\":1}}],[\"利用已有的vit\",{\"1\":{\"280\":1}}],[\"利用已有知识进行推理\",{\"1\":{\"231\":1}}],[\"利用其对齐的特征空间提升性能\",{\"1\":{\"190\":1}}],[\"利用大规模数据和模型参数\",{\"1\":{\"177\":1}}],[\"利用图文对比损失中的相似度作为度量\",{\"1\":{\"156\":1}}],[\"利用图像和上下文文本共同预测被\",{\"1\":{\"155\":1}}],[\"利用对称函数\",{\"1\":{\"103\":1}}],[\"利用相对坐标与点特征相结合的方式可以捕获局部区域中点与点之间的关系\",{\"1\":{\"91\":1}}],[\"利用上一步得到的中心点将点集划分成若干个区域\",{\"1\":{\"87\":1}}],[\"利用一组问题条件化的\",{\"1\":{\"76\":2}}],[\"利用\",{\"1\":{\"70\":1,\"282\":1,\"299\":1,\"494\":1,\"682\":1}}],[\"利用1x1卷积完成通道维度上的信息融合\",{\"1\":{\"59\":1}}],[\"利用联合建模特征作为query\",{\"1\":{\"59\":1}}],[\"利用roi\",{\"1\":{\"59\":3}}],[\"利用功能\",{\"1\":{\"52\":1}}],[\"利用物体结构辅助推断更多人类交互意图\",{\"1\":{\"32\":1}}],[\"利用跨模态自适应融合模块\",{\"1\":{\"8\":1}}],[\"利用2d交互语义指导3d功能定位\",{\"1\":{\"7\":1}}],[\"ipython\",{\"1\":{\"666\":1}}],[\"ill\",{\"1\":{\"597\":1}}],[\"iloc\",{\"1\":{\"68\":2}}],[\"io\",{\"1\":{\"573\":1}}],[\"iouloss\",{\"1\":{\"403\":2}}],[\"iou\",{\"0\":{\"403\":1},\"1\":{\"82\":27,\"401\":2,\"403\":21}}],[\"i证明小规模微调即可显著提升任务适应性\",{\"1\":{\"483\":1}}],[\"ibarz\",{\"1\":{\"469\":1}}],[\"i+1\",{\"1\":{\"410\":1,\"411\":1,\"412\":2,\"474\":2,\"511\":1}}],[\"icmlm和convirt仅在10万级别的数据上训练了几天\",{\"1\":{\"278\":1}}],[\"icmlm基于语言掩码的方法\",{\"1\":{\"278\":1}}],[\"ignored\",{\"1\":{\"541\":4}}],[\"ignore\",{\"1\":{\"163\":1,\"277\":1,\"514\":1,\"538\":1,\"541\":3}}],[\"iter\",{\"1\":{\"667\":9}}],[\"iters是iterations的缩写\",{\"1\":{\"667\":1}}],[\"iters\",{\"1\":{\"667\":6}}],[\"items\",{\"1\":{\"289\":1,\"410\":4,\"412\":3,\"511\":2}}],[\"item\",{\"1\":{\"68\":5,\"82\":1,\"145\":2,\"147\":2,\"162\":2,\"284\":2,\"289\":5,\"477\":1,\"514\":1,\"522\":1}}],[\"it\",{\"1\":{\"194\":1}}],[\"itg\",{\"0\":{\"285\":1},\"1\":{\"190\":1}}],[\"ita\",{\"1\":{\"145\":4,\"147\":4,\"159\":2,\"161\":1}}],[\"itm分类头\",{\"1\":{\"145\":1}}],[\"itm\",{\"0\":{\"127\":1,\"156\":1,\"162\":1,\"284\":1},\"1\":{\"127\":1,\"128\":2,\"145\":11,\"146\":7,\"147\":12,\"153\":2,\"156\":2,\"159\":2,\"160\":2,\"162\":8,\"190\":1,\"258\":1,\"284\":12}}],[\"itc\",{\"0\":{\"127\":1,\"161\":1,\"283\":1},\"1\":{\"127\":1,\"128\":1,\"145\":1,\"146\":1,\"147\":1,\"149\":1,\"153\":1,\"157\":3,\"161\":1,\"162\":1,\"190\":1,\"283\":1}}],[\"ia\",{\"1\":{\"59\":5}}],[\"iag网络输入为四元组\",{\"1\":{\"54\":1}}],[\"iag框架\",{\"1\":{\"49\":1}}],[\"iag\",{\"1\":{\"22\":1,\"23\":1,\"59\":1}}],[\"iagnet\",{\"0\":{\"47\":1},\"1\":{\"7\":1,\"47\":1}}],[\"i2t维度为\",{\"1\":{\"162\":1}}],[\"i2t\",{\"1\":{\"145\":13,\"147\":13,\"161\":9,\"162\":4,\"283\":5,\"284\":4}}],[\"i2\",{\"1\":{\"32\":4,\"45\":4}}],[\"i1\",{\"1\":{\"32\":4,\"45\":4}}],[\"id为文本\",{\"1\":{\"286\":1}}],[\"id=198\",{\"1\":{\"477\":1}}],[\"id=model\",{\"1\":{\"275\":1,\"277\":1}}],[\"id=self\",{\"1\":{\"143\":4,\"286\":2}}],[\"identity\",{\"1\":{\"291\":3,\"294\":1,\"296\":1}}],[\"identify\",{\"1\":{\"63\":1}}],[\"iden\",{\"1\":{\"107\":4}}],[\"idw\",{\"1\":{\"98\":1,\"100\":2}}],[\"ids作用图解\",{\"1\":{\"520\":1}}],[\"ids=position\",{\"1\":{\"163\":1,\"262\":1,\"268\":1,\"285\":1,\"528\":1,\"529\":1,\"538\":1,\"541\":1,\"543\":1,\"544\":1}}],[\"ids=token\",{\"1\":{\"163\":1,\"520\":1,\"528\":1,\"529\":1,\"538\":1,\"540\":1,\"541\":1,\"543\":1,\"544\":1}}],[\"ids=none\",{\"1\":{\"163\":3,\"262\":2,\"268\":2,\"284\":2,\"285\":2,\"477\":2,\"523\":2,\"528\":2,\"529\":2,\"538\":2,\"541\":2,\"543\":2,\"544\":2}}],[\"ids=input\",{\"1\":{\"143\":2,\"262\":1,\"286\":1,\"477\":1,\"520\":1}}],[\"ids\",{\"1\":{\"40\":1,\"142\":7,\"143\":8,\"145\":16,\"146\":2,\"147\":21,\"161\":2,\"163\":24,\"262\":2,\"268\":2,\"282\":1,\"284\":21,\"285\":9,\"286\":2,\"411\":11,\"412\":11,\"477\":13,\"511\":7,\"512\":11,\"514\":4,\"520\":36,\"522\":8,\"523\":14,\"528\":4,\"529\":4,\"538\":4,\"540\":2,\"541\":4,\"542\":4,\"543\":4,\"544\":17}}],[\"id\",{\"1\":{\"29\":2,\"40\":2,\"58\":2,\"68\":6,\"83\":1,\"142\":5,\"143\":5,\"145\":1,\"147\":3,\"163\":4,\"285\":3,\"286\":3,\"411\":6,\"412\":7,\"470\":1,\"477\":1,\"511\":1,\"513\":1,\"520\":9,\"542\":3,\"666\":6}}],[\"idx2word\",{\"1\":{\"511\":7}}],[\"idx说明\",{\"1\":{\"477\":1}}],[\"idx=0\",{\"1\":{\"517\":1,\"523\":1}}],[\"idx=none\",{\"1\":{\"477\":1}}],[\"idx=idx\",{\"1\":{\"145\":1}}],[\"idxs\",{\"1\":{\"145\":3,\"411\":2,\"412\":2}}],[\"idx\",{\"1\":{\"29\":10,\"58\":2,\"92\":21,\"96\":3,\"100\":5,\"145\":32,\"147\":5,\"162\":5,\"247\":2,\"275\":5,\"277\":5,\"284\":5,\"410\":14,\"411\":3,\"412\":34,\"477\":1,\"511\":8,\"512\":4,\"514\":2,\"517\":1}}],[\"i\",{\"1\":{\"29\":4,\"30\":7,\"32\":8,\"34\":7,\"35\":10,\"41\":17,\"43\":10,\"45\":18,\"46\":2,\"58\":2,\"59\":62,\"68\":4,\"81\":1,\"82\":12,\"83\":10,\"92\":4,\"96\":7,\"100\":2,\"108\":5,\"142\":1,\"145\":2,\"147\":1,\"159\":2,\"244\":1,\"263\":4,\"272\":12,\"275\":2,\"277\":2,\"285\":3,\"289\":4,\"296\":2,\"323\":2,\"327\":2,\"386\":1,\"410\":7,\"411\":4,\"412\":10,\"471\":1,\"474\":5,\"477\":1,\"482\":1,\"483\":1,\"511\":2,\"512\":8,\"514\":5,\"526\":2,\"652\":2,\"667\":3,\"683\":1}}],[\"imbalance\",{\"1\":{\"401\":1,\"404\":1}}],[\"imshow\",{\"1\":{\"276\":1,\"277\":1}}],[\"im\",{\"1\":{\"244\":2,\"247\":8}}],[\"impacts\",{\"1\":{\"472\":1}}],[\"improving\",{\"1\":{\"438\":1}}],[\"improves\",{\"1\":{\"435\":1}}],[\"implements\",{\"1\":{\"558\":1}}],[\"implementation\",{\"0\":{\"139\":1,\"158\":1,\"243\":1,\"245\":1}}],[\"impl\",{\"1\":{\"300\":1}}],[\"import\",{\"1\":{\"78\":3,\"83\":7,\"277\":9,\"289\":3,\"321\":1,\"323\":4,\"325\":1,\"326\":2,\"372\":1,\"379\":1,\"386\":1,\"412\":5,\"474\":2,\"477\":3,\"608\":1,\"645\":1,\"657\":1,\"658\":1,\"661\":7,\"666\":6,\"667\":5}}],[\"img\",{\"1\":{\"29\":28,\"30\":5,\"34\":1,\"40\":8,\"41\":7,\"58\":25,\"59\":14,\"142\":2,\"160\":2,\"276\":2,\"277\":2,\"289\":9,\"291\":10,\"292\":3,\"293\":2,\"296\":2,\"300\":1}}],[\"imagetext\",{\"1\":{\"258\":1}}],[\"imagefolder\",{\"1\":{\"244\":1}}],[\"imagenet\",{\"1\":{\"131\":1,\"175\":1,\"193\":1,\"194\":1,\"237\":1,\"239\":1,\"240\":1,\"242\":1,\"290\":1,\"300\":3}}],[\"images=images\",{\"1\":{\"275\":1,\"277\":1}}],[\"images\",{\"1\":{\"47\":1,\"114\":1,\"244\":1,\"247\":2,\"272\":1,\"273\":1,\"275\":3,\"276\":2,\"277\":5,\"285\":2,\"289\":40,\"290\":12,\"296\":3}}],[\"image\",{\"0\":{\"154\":1,\"156\":1,\"176\":1,\"283\":1,\"284\":1,\"285\":1},\"1\":{\"16\":1,\"28\":9,\"29\":2,\"40\":2,\"43\":6,\"58\":1,\"119\":2,\"120\":1,\"126\":2,\"127\":4,\"134\":1,\"142\":25,\"143\":13,\"145\":50,\"146\":14,\"147\":47,\"156\":1,\"159\":4,\"160\":5,\"161\":30,\"162\":19,\"163\":5,\"164\":2,\"165\":1,\"167\":1,\"168\":1,\"176\":2,\"178\":1,\"193\":1,\"194\":3,\"244\":3,\"267\":1,\"271\":1,\"272\":5,\"273\":10,\"275\":31,\"276\":21,\"277\":51,\"278\":1,\"280\":2,\"282\":14,\"283\":11,\"284\":20,\"285\":7,\"286\":15,\"289\":9,\"291\":1,\"300\":1,\"666\":3}}],[\"isscalar\",{\"1\":{\"660\":1}}],[\"issues\",{\"1\":{\"140\":1}}],[\"isotropic\",{\"1\":{\"590\":1}}],[\"isn\",{\"1\":{\"289\":1}}],[\"isdir\",{\"1\":{\"275\":1,\"277\":1,\"289\":1}}],[\"isinstance\",{\"1\":{\"268\":1,\"477\":1,\"511\":2,\"525\":2,\"535\":2,\"643\":1,\"651\":1,\"652\":1,\"654\":1,\"656\":3,\"658\":2,\"659\":1,\"660\":1}}],[\"is\",{\"0\":{\"234\":1},\"1\":{\"28\":1,\"43\":1,\"72\":2,\"74\":2,\"75\":1,\"83\":2,\"92\":3,\"96\":2,\"100\":1,\"107\":1,\"108\":1,\"142\":1,\"145\":1,\"147\":1,\"162\":3,\"163\":8,\"244\":1,\"246\":1,\"247\":1,\"262\":2,\"266\":4,\"268\":3,\"273\":1,\"275\":2,\"276\":2,\"277\":4,\"284\":4,\"285\":12,\"289\":1,\"326\":2,\"379\":1,\"385\":1,\"411\":1,\"412\":1,\"474\":32,\"477\":6,\"505\":7,\"511\":1,\"512\":4,\"514\":2,\"517\":1,\"520\":4,\"523\":2,\"529\":1,\"531\":4,\"536\":1,\"538\":2,\"541\":2,\"542\":2,\"543\":2,\"544\":4,\"554\":1,\"556\":1,\"558\":3,\"635\":1,\"638\":1,\"642\":1,\"643\":2,\"652\":2,\"654\":3,\"656\":5,\"658\":3,\"659\":3,\"666\":5}}],[\"if\",{\"1\":{\"28\":1,\"29\":6,\"40\":2,\"43\":1,\"58\":5,\"68\":3,\"72\":2,\"74\":2,\"75\":1,\"80\":2,\"82\":2,\"83\":7,\"92\":5,\"93\":2,\"96\":4,\"100\":2,\"107\":1,\"108\":1,\"109\":3,\"143\":2,\"145\":2,\"146\":1,\"159\":1,\"162\":1,\"163\":8,\"262\":1,\"264\":1,\"266\":2,\"268\":4,\"275\":9,\"276\":3,\"277\":12,\"284\":5,\"285\":11,\"286\":1,\"289\":6,\"291\":1,\"294\":1,\"296\":1,\"300\":2,\"379\":1,\"410\":1,\"411\":6,\"412\":9,\"474\":1,\"477\":12,\"510\":4,\"511\":8,\"512\":6,\"514\":3,\"520\":7,\"522\":1,\"523\":2,\"525\":1,\"529\":3,\"531\":1,\"535\":1,\"538\":1,\"541\":1,\"543\":2,\"544\":4,\"558\":3,\"635\":1,\"638\":1,\"642\":1,\"643\":2,\"651\":2,\"652\":3,\"654\":4,\"656\":9,\"658\":9,\"659\":3,\"660\":2,\"662\":1,\"666\":6}}],[\"inverse\",{\"0\":{\"597\":1}}],[\"invertible\",{\"1\":{\"455\":1}}],[\"invariance\",{\"1\":{\"104\":1,\"115\":1}}],[\"invariant\",{\"1\":{\"104\":1}}],[\"in21k模型权重文件\",{\"1\":{\"300\":1}}],[\"in21k\",{\"1\":{\"300\":4}}],[\"in21k这个模型\",{\"1\":{\"300\":1}}],[\"inf\",{\"1\":{\"514\":1}}],[\"infiniband\",{\"1\":{\"494\":1}}],[\"infile\",{\"1\":{\"412\":6}}],[\"info\",{\"1\":{\"334\":1,\"336\":1,\"525\":1,\"535\":1}}],[\"infonce\",{\"1\":{\"145\":1,\"147\":4,\"240\":7,\"241\":1,\"242\":1}}],[\"inference\",{\"1\":{\"4\":1,\"5\":1,\"40\":3,\"425\":1,\"596\":5,\"673\":1}}],[\"inplace=true\",{\"1\":{\"101\":1}}],[\"inputfeatures\",{\"1\":{\"520\":1}}],[\"inputfeatures组成图解\",{\"1\":{\"520\":1}}],[\"inputs和self\",{\"1\":{\"657\":1}}],[\"inputs\",{\"1\":{\"40\":6,\"43\":6,\"163\":2,\"275\":4,\"277\":4,\"401\":9,\"402\":14,\"403\":9,\"404\":8,\"405\":10,\"407\":12,\"477\":4,\"520\":8,\"541\":1,\"651\":4,\"652\":2,\"654\":1,\"656\":6,\"657\":1,\"658\":6,\"660\":9,\"666\":2}}],[\"input\",{\"1\":{\"40\":7,\"43\":19,\"92\":11,\"95\":1,\"96\":3,\"98\":1,\"99\":1,\"142\":5,\"143\":8,\"145\":9,\"146\":2,\"147\":14,\"159\":2,\"161\":2,\"163\":23,\"247\":1,\"262\":2,\"265\":2,\"268\":2,\"273\":2,\"282\":6,\"284\":10,\"285\":8,\"286\":3,\"403\":1,\"474\":19,\"477\":6,\"511\":4,\"512\":6,\"514\":4,\"520\":13,\"522\":4,\"523\":6,\"525\":2,\"528\":2,\"529\":2,\"532\":2,\"533\":3,\"536\":1,\"538\":2,\"540\":1,\"541\":2,\"542\":4,\"543\":2,\"544\":7,\"554\":1,\"613\":2,\"630\":4,\"631\":1,\"634\":1,\"635\":1,\"638\":1,\"652\":1,\"659\":2}}],[\"independent\",{\"1\":{\"568\":2}}],[\"indent=4\",{\"1\":{\"289\":1,\"410\":3,\"412\":3,\"510\":2,\"511\":1}}],[\"index=5\",{\"1\":{\"542\":2}}],[\"index=ignored\",{\"1\":{\"541\":1}}],[\"index=\",{\"1\":{\"538\":1}}],[\"index=0\",{\"1\":{\"514\":3}}],[\"index=masked\",{\"1\":{\"513\":2}}],[\"indexed\",{\"1\":{\"92\":1}}],[\"index\",{\"1\":{\"29\":14,\"58\":13,\"68\":3,\"92\":8,\"96\":3,\"100\":2,\"142\":2,\"163\":1,\"275\":2,\"276\":2,\"277\":4,\"338\":1,\"403\":2,\"405\":1,\"412\":3,\"514\":2,\"517\":1,\"540\":4,\"541\":6,\"542\":5}}],[\"inductive\",{\"1\":{\"287\":1}}],[\"indicating\",{\"1\":{\"412\":1}}],[\"indicators\",{\"1\":{\"247\":1}}],[\"indices=none\",{\"1\":{\"163\":1}}],[\"indices\",{\"1\":{\"92\":5,\"107\":1,\"109\":1,\"163\":12,\"275\":2,\"277\":2,\"284\":6,\"289\":4,\"412\":6,\"512\":3}}],[\"inner\",{\"1\":{\"72\":2,\"74\":2,\"365\":2,\"366\":4}}],[\"inherent\",{\"1\":{\"59\":3}}],[\"inspect\",{\"1\":{\"372\":1}}],[\"insert\",{\"1\":{\"59\":2}}],[\"install命令安装即可\",{\"1\":{\"338\":1}}],[\"install\",{\"1\":{\"338\":4,\"546\":1,\"666\":2}}],[\"instance\",{\"0\":{\"235\":1},\"1\":{\"235\":1}}],[\"instances\",{\"1\":{\"25\":1,\"412\":2}}],[\"instruct\",{\"1\":{\"227\":2}}],[\"instructgpt没能直接回答\",{\"1\":{\"471\":1}}],[\"instructgpt在输出真实性\",{\"1\":{\"467\":1}}],[\"instructgpt\",{\"0\":{\"466\":1},\"1\":{\"224\":5,\"231\":1,\"466\":1,\"469\":10,\"470\":7,\"471\":18,\"472\":5}}],[\"instructblip\",{\"1\":{\"195\":1}}],[\"instruction+qa\",{\"1\":{\"471\":1}}],[\"instruction\",{\"0\":{\"231\":1},\"1\":{\"40\":1,\"224\":6,\"231\":7,\"469\":1,\"674\":1}}],[\"instructional\",{\"1\":{\"40\":3,\"45\":3}}],[\"instructions\",{\"1\":{\"37\":1,\"466\":1,\"469\":1}}],[\"into\",{\"1\":{\"540\":1}}],[\"intrinsic\",{\"1\":{\"424\":2,\"425\":1}}],[\"introduction\",{\"0\":{\"120\":1,\"149\":1,\"233\":1,\"238\":1,\"253\":1}}],[\"intuitive\",{\"1\":{\"247\":1}}],[\"int64\",{\"1\":{\"68\":2,\"659\":1}}],[\"int\",{\"1\":{\"59\":8,\"73\":2,\"82\":2,\"142\":2,\"163\":1,\"246\":2,\"249\":1,\"292\":5,\"294\":1,\"300\":1,\"410\":21,\"411\":4,\"412\":37,\"511\":1,\"512\":1,\"531\":1}}],[\"intensity\",{\"1\":{\"114\":1}}],[\"intention\",{\"1\":{\"4\":1,\"5\":1}}],[\"integration\",{\"0\":{\"13\":1}}],[\"intermediate\",{\"1\":{\"525\":9}}],[\"interface\",{\"1\":{\"477\":1}}],[\"inter\",{\"1\":{\"470\":1}}],[\"interpolation\",{\"1\":{\"397\":1}}],[\"interpolated\",{\"1\":{\"98\":1,\"100\":4}}],[\"interest\",{\"1\":{\"397\":1}}],[\"interleave\",{\"1\":{\"143\":1,\"286\":1}}],[\"intersect\",{\"1\":{\"82\":3}}],[\"intersection\",{\"0\":{\"403\":1},\"1\":{\"22\":1,\"78\":4,\"82\":3,\"401\":2,\"402\":3,\"403\":6,\"407\":2}}],[\"internlm2\",{\"1\":{\"214\":1,\"219\":1}}],[\"internlm\",{\"1\":{\"190\":1}}],[\"internvit\",{\"1\":{\"181\":1,\"188\":1,\"189\":2,\"190\":4,\"191\":4,\"193\":2,\"196\":2,\"197\":1,\"198\":1,\"200\":1,\"201\":1,\"207\":1,\"208\":1,\"212\":1,\"214\":1,\"215\":3,\"219\":1}}],[\"internvl通过组合视觉编码器和语言中间件\",{\"1\":{\"189\":1}}],[\"internvl通过规模化视觉编码器和渐进式跨模态对齐\",{\"1\":{\"186\":1}}],[\"internvl可灵活切换为四种模式\",{\"1\":{\"188\":1}}],[\"internvl的架构设计显著区别于\",{\"1\":{\"188\":1}}],[\"internvl的整体架构\",{\"1\":{\"188\":1}}],[\"internvl在32个通用视觉\",{\"1\":{\"180\":1}}],[\"internvl是一个大规模视觉\",{\"1\":{\"180\":1}}],[\"internvl\",{\"0\":{\"179\":1,\"206\":1},\"1\":{\"10\":1,\"28\":1,\"179\":2,\"181\":1,\"188\":3,\"189\":2,\"190\":1,\"191\":1,\"194\":5,\"195\":3,\"197\":1,\"198\":2,\"202\":2,\"207\":2,\"208\":1,\"212\":1,\"214\":1,\"215\":1,\"217\":1,\"219\":1,\"222\":1}}],[\"internet\",{\"1\":{\"28\":1}}],[\"interact\",{\"1\":{\"28\":4}}],[\"interacts\",{\"1\":{\"28\":2}}],[\"interactive\",{\"1\":{\"12\":1,\"28\":2,\"59\":2}}],[\"interaction的简写\",{\"1\":{\"255\":1}}],[\"interactions\",{\"1\":{\"28\":4,\"37\":1,\"47\":1}}],[\"interaction\",{\"0\":{\"256\":1},\"1\":{\"11\":1,\"12\":1,\"28\":12,\"48\":1,\"104\":1,\"255\":1}}],[\"initialize\",{\"1\":{\"246\":1,\"412\":1}}],[\"init\",{\"0\":{\"160\":1,\"246\":1},\"1\":{\"29\":2,\"34\":4,\"35\":4,\"36\":4,\"41\":2,\"45\":4,\"46\":2,\"58\":2,\"59\":12,\"68\":1,\"73\":2,\"78\":2,\"92\":2,\"93\":2,\"96\":4,\"100\":2,\"101\":2,\"107\":2,\"109\":2,\"110\":2,\"111\":2,\"142\":4,\"145\":4,\"146\":2,\"147\":5,\"160\":3,\"163\":1,\"244\":2,\"246\":2,\"262\":2,\"263\":1,\"265\":2,\"268\":4,\"289\":1,\"291\":2,\"292\":3,\"293\":4,\"294\":4,\"295\":2,\"296\":5,\"339\":2,\"377\":1,\"401\":2,\"402\":2,\"403\":2,\"404\":2,\"405\":2,\"407\":2,\"412\":1,\"477\":1,\"511\":1,\"513\":2,\"523\":2,\"525\":6,\"526\":2,\"527\":2,\"528\":3,\"529\":3,\"531\":2,\"532\":2,\"533\":2,\"535\":2,\"536\":2,\"537\":2,\"538\":2,\"541\":2,\"543\":2,\"544\":2,\"549\":2,\"550\":2,\"552\":2,\"553\":2,\"554\":2,\"556\":2,\"557\":2,\"558\":2,\"607\":1,\"629\":1,\"643\":1,\"656\":1,\"659\":1,\"660\":1,\"661\":3}}],[\"including\",{\"1\":{\"28\":2}}],[\"in\",{\"1\":{\"28\":7,\"29\":3,\"35\":3,\"43\":1,\"46\":3,\"47\":1,\"58\":2,\"59\":5,\"68\":4,\"80\":4,\"81\":2,\"82\":5,\"83\":1,\"92\":6,\"93\":4,\"96\":8,\"100\":5,\"101\":8,\"142\":2,\"143\":1,\"145\":4,\"147\":5,\"157\":1,\"159\":1,\"162\":2,\"193\":3,\"244\":2,\"246\":1,\"248\":1,\"263\":2,\"273\":2,\"274\":1,\"275\":7,\"277\":7,\"283\":2,\"284\":2,\"285\":1,\"289\":10,\"291\":4,\"292\":3,\"293\":2,\"294\":8,\"296\":4,\"300\":3,\"410\":18,\"411\":7,\"412\":33,\"421\":1,\"434\":1,\"435\":1,\"436\":1,\"460\":1,\"464\":1,\"474\":9,\"477\":2,\"510\":5,\"511\":9,\"512\":8,\"513\":2,\"514\":4,\"517\":1,\"520\":1,\"526\":2,\"531\":1,\"549\":1,\"554\":2,\"557\":1,\"558\":4,\"651\":3,\"652\":2,\"654\":2,\"656\":7,\"657\":2,\"658\":9,\"660\":2,\"666\":5,\"667\":2}}],[\"提交的\",{\"1\":{\"470\":1}}],[\"提前做好的假设\",{\"1\":{\"287\":1}}],[\"提示仍可能触发毒性响应\",{\"1\":{\"484\":1}}],[\"提示下\",{\"1\":{\"471\":1}}],[\"提示下生成翻译\",{\"1\":{\"454\":1}}],[\"提示时\",{\"1\":{\"469\":1,\"471\":1}}],[\"提示未来需在结构理解与逻辑泛化方面进一步改进\",{\"1\":{\"462\":1}}],[\"提示+top\",{\"1\":{\"455\":1}}],[\"提示生成答案\",{\"1\":{\"455\":1}}],[\"提示词列表的\",{\"1\":{\"477\":1}}],[\"提示词列表\",{\"1\":{\"477\":1}}],[\"提示词工程\",{\"1\":{\"430\":1}}],[\"提示词\",{\"1\":{\"430\":1}}],[\"提示\",{\"1\":{\"274\":1}}],[\"提示调优\",{\"1\":{\"231\":2}}],[\"提出并开源\",{\"1\":{\"510\":1}}],[\"提出数据\",{\"1\":{\"485\":1}}],[\"提出了一种通过人类偏好比较训练代理的强化学习方法\",{\"1\":{\"469\":1}}],[\"提出了一种基于\",{\"1\":{\"166\":1}}],[\"提出了更先进的方法\",{\"1\":{\"396\":1}}],[\"提出掩码图像建模\",{\"1\":{\"165\":1}}],[\"提出使用\",{\"1\":{\"157\":1}}],[\"提出动量蒸馏\",{\"1\":{\"149\":1}}],[\"提出\",{\"1\":{\"122\":2,\"166\":1}}],[\"提出的一种通过\",{\"1\":{\"224\":1}}],[\"提出的\",{\"1\":{\"49\":1,\"127\":1,\"150\":1,\"470\":1,\"674\":1}}],[\"提高开发效率\",{\"1\":{\"685\":1}}],[\"提高了与外部系统集成的能力\",{\"1\":{\"674\":1}}],[\"提高了大型模型的推理效率\",{\"1\":{\"674\":2}}],[\"提高了灵活性\",{\"1\":{\"651\":1}}],[\"提高了框架的易用性和健壮性\",{\"1\":{\"648\":1}}],[\"提高了模型的性能和效率\",{\"1\":{\"674\":1}}],[\"提高了模型的泛化性和稳健性\",{\"1\":{\"96\":1}}],[\"提高了模型在空间上的泛化能力\",{\"1\":{\"90\":1}}],[\"提高代码健壮性\",{\"1\":{\"643\":1}}],[\"提高执行效率\",{\"1\":{\"639\":1}}],[\"提高逐点判别能力\",{\"1\":{\"402\":1}}],[\"提高多项式阶数\",{\"1\":{\"395\":1}}],[\"提高精度\",{\"1\":{\"395\":2}}],[\"提高其中一个会降低另一个\",{\"1\":{\"346\":1}}],[\"提高分类阈值往往会减少假正例的数量并增加假负例的数量\",{\"1\":{\"346\":1}}],[\"提高训练的稳定性\",{\"1\":{\"290\":1}}],[\"提高至\",{\"1\":{\"196\":1}}],[\"提高泛化\",{\"1\":{\"159\":1}}],[\"提高泛化能力\",{\"1\":{\"149\":1}}],[\"提高预训练模型在下游任务中的表现\",{\"1\":{\"157\":1}}],[\"提高算法鲁棒性\",{\"1\":{\"96\":1}}],[\"提高模型的性能和输出质量\",{\"1\":{\"679\":1}}],[\"提高模型的收敛速度和性能\",{\"1\":{\"674\":1}}],[\"提高模型的泛化能力\",{\"1\":{\"290\":1}}],[\"提高模型的泛化性能\",{\"1\":{\"96\":1}}],[\"提高模型对语言指令下功能区域的理解能力\",{\"1\":{\"78\":1}}],[\"提升代码可读性\",{\"1\":{\"663\":1}}],[\"提升代码可读性和鲁棒性\",{\"1\":{\"658\":1}}],[\"提升调试效率\",{\"1\":{\"659\":1}}],[\"提升效率的同时改善泛化能力\",{\"1\":{\"500\":1}}],[\"提升显著\",{\"1\":{\"483\":1}}],[\"提升训练稳定性\",{\"1\":{\"481\":1}}],[\"提升训练难度\",{\"1\":{\"161\":1}}],[\"提升\",{\"1\":{\"471\":1}}],[\"提升至52\",{\"1\":{\"455\":1}}],[\"提升监督模型的泛化能力\",{\"1\":{\"444\":1}}],[\"提升边缘识别精度\",{\"1\":{\"402\":1}}],[\"提升边界识别能力\",{\"1\":{\"78\":1}}],[\"提升精度\",{\"1\":{\"395\":1}}],[\"提升依然不明显\",{\"1\":{\"239\":1}}],[\"提升多语言支持\",{\"1\":{\"217\":1}}],[\"提升计算效率\",{\"1\":{\"214\":1}}],[\"提升模型的少样本学习能力\",{\"1\":{\"460\":1}}],[\"提升模型的泛化性\",{\"1\":{\"212\":1}}],[\"提升模型在各种任务上的泛化能力\",{\"1\":{\"231\":1}}],[\"提升模型鲁棒性\",{\"1\":{\"107\":2}}],[\"提升视觉表征能力\",{\"1\":{\"212\":1}}],[\"提升其视觉理解能力\",{\"1\":{\"207\":1}}],[\"提升检索精度\",{\"1\":{\"188\":1}}],[\"提升表示稳定性和泛化能力\",{\"1\":{\"150\":1}}],[\"提升表示的准确性\",{\"1\":{\"149\":1}}],[\"提升预训练及下游表现\",{\"1\":{\"149\":1}}],[\"提升了多语言能力\",{\"1\":{\"674\":1}}],[\"提升了多模态学习效果\",{\"1\":{\"124\":1}}],[\"提升了模型的泛化能力和稳定性\",{\"1\":{\"105\":1}}],[\"提升泛化能力\",{\"1\":{\"7\":1,\"66\":1,\"122\":1,\"495\":1}}],[\"提炼问题本质\",{\"1\":{\"63\":1}}],[\"提供可视化界面和性能分析工具\",{\"1\":{\"685\":1}}],[\"提供可扩展\",{\"1\":{\"685\":1}}],[\"提供基础架构和工具\",{\"1\":{\"685\":1}}],[\"提供的\",{\"1\":{\"682\":1}}],[\"提供的点云和功能区域标注构建\",{\"1\":{\"62\":1}}],[\"提供个性化体验\",{\"1\":{\"674\":1}}],[\"提供20b参数开源基线\",{\"1\":{\"485\":1}}],[\"提供现实路径\",{\"1\":{\"472\":1}}],[\"提供高质量示范\",{\"1\":{\"470\":1}}],[\"提供一条示例和任务描述\",{\"1\":{\"461\":1}}],[\"提供一个又大又一致的字典\",{\"1\":{\"238\":1}}],[\"提供了基础抽象和\",{\"1\":{\"685\":1}}],[\"提供了丰富的智能体和工具集合\",{\"1\":{\"684\":1}}],[\"提供了业界领先的调试和观测功能\",{\"1\":{\"684\":1}}],[\"提供了一系列强大的输出解析工具\",{\"1\":{\"684\":1}}],[\"提供了一个用于测试模型生成信息真实性的基准数据集\",{\"1\":{\"469\":1}}],[\"提供了一种更可行的替代方法\",{\"1\":{\"240\":1}}],[\"提供了重要基准\",{\"1\":{\"455\":1}}],[\"提供了这些输入的简洁描述\",{\"1\":{\"445\":1}}],[\"提供了\",{\"1\":{\"372\":1,\"687\":1}}],[\"提供强大的语言理解能力\",{\"1\":{\"214\":1}}],[\"提供两种配置\",{\"1\":{\"202\":1}}],[\"提供理论解释\",{\"1\":{\"166\":1}}],[\"提供更多额外信息\",{\"1\":{\"133\":1}}],[\"提供新描述\",{\"1\":{\"132\":1}}],[\"提供三种标准划分方式\",{\"1\":{\"20\":1}}],[\"提取并去重后用于训练\",{\"1\":{\"470\":1}}],[\"提取输入图像的特征\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"提取语言模型损失\",{\"1\":{\"285\":1}}],[\"提取语言建模损失\",{\"1\":{\"142\":1}}],[\"提取文本特征\",{\"1\":{\"273\":1}}],[\"提取的特征图转换为序列形式\",{\"1\":{\"299\":1}}],[\"提取的图像特征则是分类器的输入数据\",{\"1\":{\"273\":1}}],[\"提取的关键点集合\",{\"1\":{\"112\":1}}],[\"提取速度相对更快\",{\"1\":{\"253\":1}}],[\"提取每条\",{\"1\":{\"226\":1}}],[\"提取每一点的特征向量\",{\"1\":{\"107\":1}}],[\"提取视觉特征\",{\"1\":{\"147\":1}}],[\"提取图像特征\",{\"1\":{\"227\":1,\"273\":1,\"286\":1}}],[\"提取图像特征表示\",{\"1\":{\"142\":1}}],[\"提取图像和文本特征\",{\"1\":{\"145\":1}}],[\"提取图像和点云的特征\",{\"1\":{\"40\":1}}],[\"提取全局特征\",{\"1\":{\"110\":2}}],[\"提取更高维的特征\",{\"1\":{\"109\":1}}],[\"提取更高级的特征表示\",{\"1\":{\"73\":1}}],[\"提取邻近点的特征\",{\"1\":{\"100\":1}}],[\"提取局部特征\",{\"1\":{\"99\":1}}],[\"提取点云的层次化特征\",{\"1\":{\"93\":1}}],[\"提取这些区域的高维特征\",{\"1\":{\"92\":1}}],[\"提取这些局部区域中的点及其特征\",{\"1\":{\"92\":1}}],[\"提取\",{\"1\":{\"40\":2,\"147\":1,\"162\":1,\"513\":1}}],[\"提取特征值\",{\"1\":{\"397\":1}}],[\"提取特征\",{\"1\":{\"8\":1,\"98\":1,\"105\":1,\"111\":1,\"112\":2}}],[\"提取几何属性\",{\"1\":{\"6\":1}}],[\"的向量数据库\",{\"1\":{\"687\":1}}],[\"的向量映射到词汇表空间\",{\"1\":{\"285\":1}}],[\"的生态系统\",{\"1\":{\"685\":1}}],[\"的生成逻辑\",{\"1\":{\"29\":1}}],[\"的持续优化和功能迭代\",{\"1\":{\"684\":1}}],[\"的巨大成功激发了越来越多的开发者兴趣\",{\"1\":{\"682\":1}}],[\"的战略\",{\"1\":{\"678\":1}}],[\"的概念\",{\"1\":{\"677\":1}}],[\"的概率是\",{\"1\":{\"569\":1}}],[\"的概率是多少\",{\"1\":{\"565\":1}}],[\"的概率\",{\"1\":{\"567\":1,\"576\":1}}],[\"的概率都是\",{\"1\":{\"565\":1}}],[\"的概率最大\",{\"1\":{\"508\":1}}],[\"的概率随机水平翻转图像\",{\"1\":{\"290\":1}}],[\"的概率随机将输入文本中的\",{\"1\":{\"155\":1}}],[\"的概率分布\",{\"1\":{\"155\":1,\"565\":1,\"597\":1}}],[\"的概率值\",{\"1\":{\"46\":1,\"78\":1,\"403\":1,\"404\":1}}],[\"的支持\",{\"1\":{\"674\":1,\"684\":1}}],[\"的卓越能力\",{\"1\":{\"674\":1}}],[\"的基础语言模型\",{\"1\":{\"674\":1}}],[\"的基础上将双向\",{\"1\":{\"126\":1}}],[\"的会话应用\",{\"1\":{\"674\":1}}],[\"的推出\",{\"1\":{\"673\":1}}],[\"的推理水平\",{\"1\":{\"674\":1}}],[\"的推理机制\",{\"0\":{\"228\":1}}],[\"的推理能力\",{\"1\":{\"5\":1}}],[\"的深度集成\",{\"1\":{\"684\":1}}],[\"的深度学习训练系统\",{\"1\":{\"670\":1}}],[\"的深度融合\",{\"1\":{\"69\":1}}],[\"的等高线\",{\"1\":{\"667\":1}}],[\"的内存管理与执行流程\",{\"1\":{\"665\":1}}],[\"的代码组织为可复用的包\",{\"1\":{\"661\":1}}],[\"的代价很高\",{\"1\":{\"353\":1}}],[\"的代价高于另一种错误\",{\"1\":{\"343\":1}}],[\"的完整实现\",{\"1\":{\"660\":1}}],[\"的完整计算流程\",{\"1\":{\"284\":1}}],[\"的运算符重载中\",{\"1\":{\"660\":1}}],[\"的梯度来自两个路径\",{\"1\":{\"654\":1}}],[\"的梯度就始终为\",{\"1\":{\"426\":1}}],[\"的导数值对应于gxs\",{\"1\":{\"652\":1}}],[\"的导数\",{\"1\":{\"630\":1}}],[\"的先验信念\",{\"1\":{\"596\":1}}],[\"的可能性\",{\"1\":{\"678\":1}}],[\"的可能取值的了解\",{\"1\":{\"596\":1}}],[\"的可操作性特征\",{\"1\":{\"46\":1}}],[\"的图片\",{\"1\":{\"594\":1}}],[\"的图像块矩阵添加二维\",{\"1\":{\"293\":1}}],[\"的图像块\",{\"1\":{\"166\":1}}],[\"的薄壳层上\",{\"1\":{\"593\":1}}],[\"的速率快速增长\",{\"1\":{\"592\":1}}],[\"的马氏距离\",{\"1\":{\"590\":1}}],[\"的积分为\",{\"1\":{\"590\":1}}],[\"的版本\",{\"1\":{\"587\":1}}],[\"的柯西分布进行\",{\"1\":{\"587\":1}}],[\"的泊松分布\",{\"1\":{\"577\":1}}],[\"的条件下是条件独立的\",{\"1\":{\"568\":1}}],[\"的条件输入\",{\"1\":{\"143\":1}}],[\"的补集\",{\"1\":{\"567\":1}}],[\"的事件\",{\"1\":{\"567\":1,\"570\":1}}],[\"的区间生成的最小\",{\"1\":{\"566\":1}}],[\"的区域\",{\"1\":{\"396\":1,\"402\":1,\"542\":1}}],[\"的要求\",{\"1\":{\"566\":1}}],[\"的次数\",{\"1\":{\"565\":1}}],[\"的总概率\",{\"1\":{\"565\":1}}],[\"的总概率是多少\",{\"1\":{\"565\":1}}],[\"的原像\",{\"1\":{\"565\":1}}],[\"的神经网络架构\",{\"1\":{\"548\":1}}],[\"的神经网络模型\",{\"1\":{\"443\":1}}],[\"的角色进行分类\",{\"1\":{\"542\":1}}],[\"的序列\",{\"1\":{\"540\":1}}],[\"的序列挤占了下游任务的输入序列空间\",{\"1\":{\"424\":1}}],[\"的隐藏状态\",{\"1\":{\"540\":1}}],[\"的隐藏维度\",{\"1\":{\"294\":1}}],[\"的问答任务中\",{\"1\":{\"540\":1}}],[\"的问题\",{\"1\":{\"180\":1,\"404\":2,\"471\":1}}],[\"的词执行掩码策略\",{\"1\":{\"512\":1}}],[\"的子集\",{\"1\":{\"510\":1}}],[\"的几率原封不动\",{\"1\":{\"505\":1}}],[\"的几率被替换成任意一个其它的\",{\"1\":{\"505\":1}}],[\"的几率被替换成\",{\"1\":{\"505\":1}}],[\"的时候也只计算被遮盖部分的\",{\"1\":{\"505\":1}}],[\"的时间复杂度虽然是\",{\"1\":{\"112\":1}}],[\"的语言模型\",{\"1\":{\"504\":1}}],[\"的语义而不是\",{\"1\":{\"505\":1}}],[\"的语义\",{\"1\":{\"41\":2,\"505\":1}}],[\"的单词进行掩码\",{\"1\":{\"495\":1}}],[\"的固定学习率\",{\"1\":{\"494\":1}}],[\"的固定大小特征图\",{\"1\":{\"396\":1}}],[\"的掩码语言模型目标在优化后仍具竞争力\",{\"1\":{\"492\":1}}],[\"的竞争力\",{\"1\":{\"480\":1}}],[\"的缓存\",{\"1\":{\"477\":1}}],[\"的出发点就在这里\",{\"1\":{\"474\":1}}],[\"的出现让人们重新思考了\",{\"1\":{\"678\":1}}],[\"的出现也掀起了新一轮的研究热潮\",{\"1\":{\"270\":1}}],[\"的出现\",{\"1\":{\"7\":1}}],[\"的模型战略形成了\",{\"1\":{\"674\":1}}],[\"的模型\",{\"1\":{\"472\":1,\"542\":1}}],[\"的模块\",{\"1\":{\"107\":1}}],[\"的通用方法\",{\"1\":{\"472\":1}}],[\"的通用性\",{\"1\":{\"454\":1}}],[\"的答案\",{\"1\":{\"471\":1}}],[\"的残差\",{\"1\":{\"470\":1}}],[\"的任务分布\",{\"1\":{\"470\":1}}],[\"的监督信号\",{\"1\":{\"470\":1}}],[\"的三步训练流程\",{\"1\":{\"470\":1}}],[\"的详细总结\",{\"1\":{\"469\":1}}],[\"的控制\",{\"1\":{\"469\":1}}],[\"的工作属于对上述方法的泛化\",{\"1\":{\"469\":1}}],[\"的偏好比为\",{\"1\":{\"471\":1}}],[\"的偏好\",{\"1\":{\"468\":1}}],[\"的偏离平均值的乘积\",{\"1\":{\"355\":1}}],[\"的新判断\",{\"1\":{\"596\":1}}],[\"的新范式\",{\"1\":{\"464\":1}}],[\"的新视图\",{\"1\":{\"325\":1}}],[\"的新视角\",{\"1\":{\"123\":1}}],[\"的理念\",{\"1\":{\"464\":1}}],[\"的假设\",{\"1\":{\"464\":1}}],[\"的提取并非依赖微调\",{\"1\":{\"462\":1}}],[\"的准确率\",{\"1\":{\"462\":1}}],[\"的准确率超越前sota\",{\"1\":{\"455\":1}}],[\"的元学习方法\",{\"1\":{\"461\":1}}],[\"的设定\",{\"1\":{\"461\":1}}],[\"的设计\",{\"1\":{\"231\":1,\"454\":1}}],[\"的成果表明\",{\"1\":{\"460\":1}}],[\"的差距已显著缩小\",{\"1\":{\"455\":1}}],[\"的少样本学习\",{\"1\":{\"454\":1}}],[\"的规模化改进\",{\"1\":{\"454\":1}}],[\"的规模相匹配\",{\"1\":{\"189\":1}}],[\"的高质量数据\",{\"1\":{\"454\":1}}],[\"的零样本能力依赖于任务提示\",{\"1\":{\"454\":1}}],[\"的零样本图像描述结果\",{\"1\":{\"190\":1}}],[\"的外链网页\",{\"1\":{\"454\":1}}],[\"的mqan\",{\"1\":{\"454\":1}}],[\"的mlp层\",{\"1\":{\"100\":1}}],[\"的引入是为了加速自回归模型的推理速度\",{\"1\":{\"477\":1}}],[\"的引入显著提升了语言模型的表达能力\",{\"1\":{\"454\":1}}],[\"的引用\",{\"1\":{\"369\":1}}],[\"的绝对提升\",{\"1\":{\"448\":1}}],[\"的正余弦曲线\",{\"1\":{\"447\":1}}],[\"的正方形\",{\"1\":{\"351\":1}}],[\"的添加的线性输出层来预测\",{\"1\":{\"444\":1}}],[\"的激活状态\",{\"1\":{\"444\":1}}],[\"的变量\",{\"0\":{\"605\":1}}],[\"的变种\",{\"1\":{\"443\":1}}],[\"的变换矩阵\",{\"1\":{\"107\":2}}],[\"的本质是对训练数据的有效压缩\",{\"1\":{\"428\":1}}],[\"的影响\",{\"1\":{\"426\":1}}],[\"的影响就越大\",{\"1\":{\"405\":2}}],[\"的计算中增加一个旁路\",{\"1\":{\"425\":1}}],[\"的计算流程如下\",{\"1\":{\"82\":1}}],[\"的特点与能力\",{\"0\":{\"675\":1}}],[\"的特例\",{\"1\":{\"425\":1}}],[\"的特征图\",{\"1\":{\"291\":1,\"396\":1,\"397\":1}}],[\"的特征表示\",{\"1\":{\"247\":1}}],[\"的特征从队列中抽离\",{\"1\":{\"238\":1}}],[\"的特征进行汇总\",{\"1\":{\"97\":1}}],[\"的特征进行对齐\",{\"1\":{\"29\":1}}],[\"的特征\",{\"1\":{\"86\":1}}],[\"的特征映射到一个统一的公共特征空间\",{\"1\":{\"59\":1}}],[\"的过程\",{\"1\":{\"425\":1}}],[\"的这种思想有点类似于残差连接\",{\"1\":{\"425\":1}}],[\"的训练存在显著不足\",{\"1\":{\"492\":1}}],[\"的训练数据主要来自以下两个来源\",{\"1\":{\"470\":1}}],[\"的训练过程中\",{\"1\":{\"425\":1}}],[\"的训练流程\",{\"1\":{\"142\":1}}],[\"的思想与之有相通之处\",{\"1\":{\"428\":1}}],[\"的思想很简单\",{\"1\":{\"425\":1}}],[\"的思想有深刻的相似性\",{\"1\":{\"395\":1}}],[\"的发现\",{\"1\":{\"424\":1}}],[\"的更新量与原始参数\",{\"1\":{\"423\":1}}],[\"的微调方法\",{\"1\":{\"421\":1}}],[\"的顺序排序\",{\"1\":{\"410\":1}}],[\"的顺序排列\",{\"1\":{\"328\":2}}],[\"的极端值\",{\"1\":{\"407\":1}}],[\"的惩罚比例\",{\"1\":{\"407\":1}}],[\"的惩罚权重\",{\"1\":{\"405\":2}}],[\"的敏感度控制\",{\"1\":{\"405\":1}}],[\"的优点\",{\"1\":{\"402\":1}}],[\"的优势\",{\"1\":{\"112\":1,\"299\":1,\"401\":1}}],[\"的替代指标\",{\"1\":{\"401\":1}}],[\"的情况下\",{\"1\":{\"597\":1}}],[\"的情况下表现优异\",{\"1\":{\"401\":1}}],[\"的情况下会将随机垃圾邮件的垃圾邮件概率设为高于随机合法邮件的垃圾邮件概率\",{\"1\":{\"351\":1}}],[\"的最后一个非思维链模型\",{\"1\":{\"674\":1}}],[\"的最终输出为这些采样点值的聚合结果\",{\"1\":{\"397\":1}}],[\"的最大缺陷在于它\",{\"1\":{\"112\":1}}],[\"的值为\",{\"1\":{\"667\":1}}],[\"的值设置为父函数的\",{\"1\":{\"656\":1}}],[\"的值\",{\"1\":{\"397\":1,\"656\":1}}],[\"的距离是\",{\"1\":{\"397\":4}}],[\"的距离度量\",{\"1\":{\"359\":1}}],[\"的边界坐标再取整\",{\"1\":{\"397\":1}}],[\"的划分方式中涉及到了取整\",{\"1\":{\"396\":1}}],[\"的小格子做最大池化\",{\"1\":{\"396\":1}}],[\"的网格\",{\"1\":{\"396\":1}}],[\"的网络结构可视化理解\",{\"1\":{\"75\":1}}],[\"的函数\",{\"1\":{\"395\":1,\"565\":1,\"596\":1}}],[\"的relu网络可以构造具有\",{\"1\":{\"395\":1}}],[\"的简写\",{\"1\":{\"390\":1}}],[\"的泛化\",{\"1\":{\"383\":1}}],[\"的意思是\",{\"1\":{\"367\":1}}],[\"的协方差矩阵\",{\"1\":{\"359\":1,\"590\":1}}],[\"的期望\",{\"1\":{\"355\":1}}],[\"的度量\",{\"1\":{\"355\":1}}],[\"的阈值\",{\"1\":{\"353\":1}}],[\"的垃圾邮件分类器仅在\",{\"1\":{\"351\":1}}],[\"的垃圾邮件分类器始终会为随机垃圾邮件分配比随机合规电子邮件更高的垃圾邮件概率\",{\"1\":{\"351\":1}}],[\"的满分\",{\"1\":{\"348\":1}}],[\"的满分时\",{\"1\":{\"348\":1}}],[\"的不平衡数据集中\",{\"1\":{\"345\":1,\"346\":1}}],[\"的多输入实现\",{\"0\":{\"652\":1}}],[\"的多种呈现方式\",{\"1\":{\"328\":1}}],[\"的多模态对话能力还体现在\",{\"1\":{\"195\":1}}],[\"的轴扩展至目标大小\",{\"1\":{\"327\":1}}],[\"的数据上进行预训练\",{\"1\":{\"674\":1}}],[\"的数组\",{\"1\":{\"326\":1}}],[\"的数值会很大\",{\"1\":{\"318\":1}}],[\"的列优先\",{\"1\":{\"323\":1}}],[\"的列表\",{\"1\":{\"43\":1}}],[\"的步长是\",{\"1\":{\"323\":4}}],[\"的初始维度对结果的影响\",{\"0\":{\"306\":1}}],[\"的初始化方法随机初始化\",{\"1\":{\"200\":1}}],[\"的维度命名\",{\"1\":{\"390\":1}}],[\"的维度数相同\",{\"1\":{\"386\":1}}],[\"的维度决定\",{\"1\":{\"309\":1}}],[\"的维度\",{\"1\":{\"305\":2}}],[\"的维度对最终注意力输出的结果维度有直接影响\",{\"1\":{\"304\":1}}],[\"的局部特征提取能力快速捕捉图像的底层特征\",{\"1\":{\"299\":1}}],[\"的局限\",{\"1\":{\"188\":1}}],[\"的形式衰减\",{\"1\":{\"592\":1}}],[\"的形式明确指定的参数\",{\"1\":{\"363\":1}}],[\"的形式\",{\"1\":{\"291\":1}}],[\"的形状和位置\",{\"1\":{\"395\":1}}],[\"的形状是\",{\"1\":{\"249\":1}}],[\"的形状为\",{\"1\":{\"45\":1}}],[\"的张量视图\",{\"1\":{\"384\":1}}],[\"的张量转置为\",{\"1\":{\"326\":1}}],[\"的张量\",{\"1\":{\"290\":2,\"291\":2,\"321\":1,\"325\":1}}],[\"的generate方法负责完成图像描述生成\",{\"1\":{\"286\":1}}],[\"的text\",{\"1\":{\"286\":1}}],[\"的真实导数就会被低估一半\",{\"1\":{\"654\":1}}],[\"的真实值\",{\"1\":{\"285\":1}}],[\"的真正例率\",{\"1\":{\"350\":1}}],[\"的真正价值\",{\"1\":{\"136\":1}}],[\"的能力相结合\",{\"1\":{\"684\":1}}],[\"的能力\",{\"1\":{\"285\":1,\"674\":1}}],[\"的相似度\",{\"1\":{\"283\":1,\"307\":1}}],[\"的相同通道进行混合\",{\"1\":{\"73\":1}}],[\"的zero\",{\"1\":{\"278\":1,\"464\":1}}],[\"的架构设计变得更加条理清晰和稳固\",{\"1\":{\"684\":1}}],[\"的架构和训练方法\",{\"1\":{\"493\":1}}],[\"的架构中融入卷积操作\",{\"1\":{\"299\":1}}],[\"的架构\",{\"1\":{\"275\":1}}],[\"的vit\",{\"1\":{\"272\":1}}],[\"的对角线\",{\"1\":{\"351\":1}}],[\"的对齐\",{\"1\":{\"267\":1}}],[\"的对比如下\",{\"1\":{\"662\":1}}],[\"的对比\",{\"1\":{\"132\":1,\"403\":1}}],[\"的空间一致性\",{\"1\":{\"403\":1}}],[\"的空间\",{\"1\":{\"242\":1}}],[\"的编码器\",{\"1\":{\"242\":1}}],[\"的信念的机制\",{\"1\":{\"572\":1}}],[\"的信号空间不一致\",{\"1\":{\"239\":1}}],[\"的信息让模型分开上下句\",{\"1\":{\"506\":1}}],[\"的信息\",{\"1\":{\"76\":1}}],[\"的操作流程可以分为三个步骤\",{\"1\":{\"396\":1}}],[\"的操作\",{\"1\":{\"239\":1}}],[\"的改变会非常缓慢\",{\"1\":{\"236\":1}}],[\"的改进\",{\"1\":{\"122\":1,\"404\":1,\"454\":1}}],[\"的改进版\",{\"1\":{\"78\":1,\"404\":1}}],[\"的一个模块\",{\"1\":{\"682\":1}}],[\"的一个杰出应用就是\",{\"1\":{\"673\":1}}],[\"的一个划分\",{\"1\":{\"569\":1}}],[\"的一个参数\",{\"1\":{\"236\":1}}],[\"的一种早期形式\",{\"1\":{\"678\":1}}],[\"的一种泛化形式\",{\"1\":{\"405\":1}}],[\"的一种改进或变体\",{\"1\":{\"240\":1}}],[\"的一致性\",{\"1\":{\"402\":1}}],[\"的一系列二分类问题又转为了多分类问题\",{\"1\":{\"240\":1}}],[\"的一部分\",{\"1\":{\"191\":1}}],[\"的研究表明\",{\"1\":{\"480\":1}}],[\"的研究如潮水般涌来\",{\"1\":{\"270\":1}}],[\"的研究\",{\"1\":{\"228\":1}}],[\"的参数是需要从头开始学习的\",{\"1\":{\"508\":1}}],[\"的参数会发生更新\",{\"1\":{\"426\":1}}],[\"的参数叠加\",{\"1\":{\"425\":1}}],[\"的参数不变\",{\"1\":{\"418\":1}}],[\"的参数\",{\"1\":{\"227\":1,\"423\":1,\"425\":1}}],[\"的关键一步\",{\"1\":{\"227\":1}}],[\"的关注信息\",{\"1\":{\"76\":1}}],[\"的指令模型\",{\"1\":{\"674\":1}}],[\"的指令调优版本\",{\"1\":{\"226\":1,\"227\":1}}],[\"的指代准确率\",{\"1\":{\"482\":1}}],[\"的指导\",{\"1\":{\"159\":1}}],[\"的短语\",{\"1\":{\"226\":2}}],[\"的方向进化\",{\"1\":{\"665\":1}}],[\"的方法主要基于\",{\"1\":{\"470\":1}}],[\"的方法论\",{\"1\":{\"464\":1}}],[\"的方法可能行不通\",{\"1\":{\"239\":1}}],[\"的方法\",{\"1\":{\"224\":1,\"357\":1,\"495\":1}}],[\"的方式不仅降低了学习成本\",{\"1\":{\"660\":1}}],[\"的方式会把prompt搞得很长\",{\"1\":{\"415\":1}}],[\"的方式去读这个数组\",{\"1\":{\"326\":1}}],[\"的方式优化数据质量\",{\"1\":{\"122\":1}}],[\"的方式\",{\"1\":{\"107\":1,\"325\":1,\"433\":1}}],[\"的方式重构图像\",{\"1\":{\"41\":1}}],[\"的互联网爬取图像\",{\"1\":{\"215\":1}}],[\"的重合部分\",{\"1\":{\"401\":1}}],[\"的重合度\",{\"1\":{\"78\":1}}],[\"的重要性验证\",{\"1\":{\"196\":1}}],[\"的每一层\",{\"1\":{\"191\":1}}],[\"的每个元素都是一一对应的\",{\"1\":{\"652\":1}}],[\"的每个\",{\"1\":{\"126\":1}}],[\"的预训练效率和下游任务表现\",{\"1\":{\"493\":1}}],[\"的预训练权重\",{\"1\":{\"190\":1,\"200\":1}}],[\"的预测能力\",{\"1\":{\"455\":1}}],[\"的预测\",{\"1\":{\"285\":1}}],[\"的预测分布\",{\"1\":{\"163\":1}}],[\"的预测结果\",{\"1\":{\"123\":1,\"228\":1}}],[\"的视觉编码器\",{\"1\":{\"189\":1}}],[\"的视觉模型架构\",{\"1\":{\"73\":1}}],[\"的涌现进一步加速了多模态研究的进程\",{\"1\":{\"184\":1}}],[\"的快速发展推动了通用人工智能\",{\"1\":{\"181\":1}}],[\"的开创性工作以来\",{\"1\":{\"178\":1}}],[\"的解决方案是\",{\"1\":{\"166\":1}}],[\"的自监督预训练方法\",{\"1\":{\"166\":1}}],[\"的位置有效\",{\"1\":{\"163\":1}}],[\"的位置计算\",{\"1\":{\"163\":1}}],[\"的位置\",{\"1\":{\"163\":3,\"325\":1,\"387\":1,\"540\":1}}],[\"的权重降低\",{\"1\":{\"404\":1}}],[\"的权重\",{\"1\":{\"163\":1,\"405\":2}}],[\"的同时保持训练的稳定性和多样性\",{\"1\":{\"162\":1}}],[\"的动量蒸馏损失为\",{\"1\":{\"157\":1}}],[\"的交并比\",{\"1\":{\"402\":1}}],[\"的交互\",{\"1\":{\"292\":1}}],[\"的交叉熵损失\",{\"1\":{\"156\":1,\"162\":1}}],[\"的交集\",{\"1\":{\"78\":1,\"403\":1}}],[\"的文本对\",{\"1\":{\"540\":1}}],[\"的文本生成能力\",{\"1\":{\"286\":1}}],[\"的文本化表示\",{\"1\":{\"226\":1}}],[\"的文本\",{\"1\":{\"155\":1,\"157\":1}}],[\"的前提下\",{\"1\":{\"596\":1,\"597\":2}}],[\"的前\",{\"1\":{\"152\":1}}],[\"的前缀\",{\"1\":{\"143\":1}}],[\"的样本索引\",{\"1\":{\"145\":1}}],[\"的样本数量与\",{\"1\":{\"136\":1}}],[\"的样本数\",{\"1\":{\"82\":1}}],[\"的效果会好于其它几种方法\",{\"1\":{\"424\":1}}],[\"的效果\",{\"1\":{\"138\":1,\"274\":1}}],[\"的性能提升\",{\"1\":{\"274\":1}}],[\"的性能提升并非源于更长的训练时间\",{\"0\":{\"136\":1}}],[\"的性能会显著下降\",{\"1\":{\"112\":1}}],[\"的大规模网页数据集\",{\"1\":{\"131\":1}}],[\"的大小\",{\"1\":{\"107\":1}}],[\"的标签平滑\",{\"1\":{\"127\":1}}],[\"的使用更具计算效率\",{\"1\":{\"126\":1}}],[\"的贡献包括\",{\"1\":{\"166\":1}}],[\"的贡献\",{\"1\":{\"124\":1}}],[\"的主要缺陷\",{\"1\":{\"112\":1}}],[\"的主要局限在于其多步推理机制带来了较高的计算复杂度\",{\"1\":{\"26\":1}}],[\"的实验表明移除\",{\"1\":{\"493\":1}}],[\"的实验结果充分证明了其设计策略的有效性\",{\"1\":{\"197\":1}}],[\"的实验\",{\"1\":{\"112\":1}}],[\"的实现逻辑\",{\"1\":{\"655\":1}}],[\"的实现逻辑总是将函数追加到待处理列表的末尾\",{\"1\":{\"655\":1}}],[\"的实现\",{\"1\":{\"72\":1,\"73\":1,\"660\":4}}],[\"的输出比\",{\"1\":{\"472\":1}}],[\"的输出用于捕获整个句子的全局语义信息\",{\"1\":{\"393\":1}}],[\"的输出向量能够很好地表示图像的全局特征\",{\"1\":{\"292\":2}}],[\"的输出向量被输入到分类头中\",{\"1\":{\"292\":1}}],[\"的输出蕴含了视觉信息\",{\"1\":{\"286\":1}}],[\"的输出\",{\"1\":{\"263\":1,\"286\":1,\"292\":1,\"540\":1}}],[\"的输出仅由一个不超过\",{\"1\":{\"112\":1}}],[\"的输入序列\",{\"1\":{\"544\":1}}],[\"的输入方式是\",{\"1\":{\"544\":1}}],[\"的输入组织形式与普通分类或问答任务略有不同\",{\"1\":{\"544\":1}}],[\"的输入\",{\"1\":{\"285\":1,\"493\":1}}],[\"的输入特征\",{\"1\":{\"169\":1}}],[\"的输入和标签副本\",{\"1\":{\"163\":1}}],[\"的输入起点\",{\"1\":{\"143\":1}}],[\"的输入为\",{\"1\":{\"8\":1}}],[\"的限制\",{\"1\":{\"112\":1}}],[\"的表达式为\",{\"1\":{\"586\":1}}],[\"的表达能力\",{\"1\":{\"313\":1}}],[\"的表达能力受\",{\"1\":{\"112\":1}}],[\"的表示\",{\"1\":{\"513\":1,\"540\":1}}],[\"的表示当前激活的环境\",{\"1\":{\"334\":1}}],[\"的表现优于\",{\"1\":{\"194\":1}}],[\"的表现不如基于图结构的模型\",{\"1\":{\"112\":1}}],[\"的全局特征来自于\",{\"1\":{\"112\":1}}],[\"的分布\",{\"1\":{\"596\":1,\"597\":1}}],[\"的分布被称为半正态分布\",{\"1\":{\"585\":1}}],[\"的分类准确率略低于\",{\"1\":{\"112\":1}}],[\"的分类模块\",{\"1\":{\"110\":1}}],[\"的分割网络将全局特征复制\",{\"1\":{\"112\":1}}],[\"的分割模块通过拼接全局特征\",{\"1\":{\"112\":1}}],[\"的分割模块\",{\"1\":{\"111\":1}}],[\"的作用是用\",{\"1\":{\"506\":1}}],[\"的作用是通过训练过程中损失值的降低\",{\"1\":{\"292\":1}}],[\"的作用\",{\"1\":{\"108\":1,\"157\":1,\"358\":1,\"471\":1}}],[\"的创新点在于动态分辨率\",{\"1\":{\"212\":1}}],[\"的创新点\",{\"1\":{\"103\":1}}],[\"的整体结构是一个典型的\",{\"1\":{\"99\":1}}],[\"的核心部分\",{\"1\":{\"670\":1}}],[\"的核心能力\",{\"1\":{\"650\":1}}],[\"的核心\",{\"1\":{\"548\":1}}],[\"的核心实验基于真实用户提交的指令性\",{\"1\":{\"471\":1}}],[\"的核心技术基础是\",{\"1\":{\"469\":1}}],[\"的核心思想\",{\"1\":{\"409\":1}}],[\"的核心思想是\",{\"1\":{\"73\":1,\"224\":1,\"434\":1}}],[\"的核心是\",{\"1\":{\"405\":1}}],[\"的核心组件\",{\"1\":{\"212\":1}}],[\"的核心特征提取模块\",{\"1\":{\"109\":1}}],[\"的核心模块\",{\"1\":{\"100\":1}}],[\"的核心就是逐层提取局部特征\",{\"1\":{\"93\":1}}],[\"的第\",{\"1\":{\"92\":1}}],[\"的结合也很简单\",{\"1\":{\"425\":1}}],[\"的结构过于简单\",{\"1\":{\"112\":1}}],[\"的结构\",{\"1\":{\"92\":1}}],[\"的结果证明了\",{\"1\":{\"471\":1}}],[\"的结果\",{\"1\":{\"82\":1}}],[\"的点表示给定模型效果最佳的阈值范围\",{\"1\":{\"353\":1}}],[\"的点缺失\",{\"1\":{\"105\":1}}],[\"的点集群都将独立地送入对应的pointnet网络进行特征提取\",{\"1\":{\"95\":1}}],[\"的点及其特征\",{\"1\":{\"92\":1}}],[\"的点全部替换为\",{\"1\":{\"92\":1}}],[\"的点\",{\"1\":{\"92\":1}}],[\"的点云和功能标注\",{\"1\":{\"69\":1}}],[\"的索引数组\",{\"1\":{\"92\":1}}],[\"的索引\",{\"1\":{\"92\":1,\"513\":1}}],[\"的所有子集的集合\",{\"1\":{\"564\":1}}],[\"的所有单词\",{\"1\":{\"410\":1}}],[\"的所有邻近点\",{\"1\":{\"92\":1}}],[\"的所有通道进行处理\",{\"1\":{\"73\":1}}],[\"的缩写\",{\"1\":{\"83\":1,\"297\":2}}],[\"的组合形式\",{\"1\":{\"78\":1}}],[\"的加权组合\",{\"1\":{\"407\":1}}],[\"的加权信息\",{\"1\":{\"310\":1}}],[\"的加权比固定\",{\"1\":{\"159\":1}}],[\"的加权和逼近目标函数\",{\"1\":{\"395\":1}}],[\"的加权和\",{\"1\":{\"78\":1,\"402\":1}}],[\"的加入是为了让\",{\"1\":{\"78\":1}}],[\"的得分结果\",{\"1\":{\"76\":1}}],[\"的有效性是否真正来自其机制本身\",{\"1\":{\"136\":1}}],[\"的有效性\",{\"1\":{\"75\":1,\"471\":1}}],[\"的有效性与泛化能力\",{\"1\":{\"21\":1}}],[\"的堆叠结构\",{\"1\":{\"73\":1}}],[\"的行\",{\"1\":{\"68\":1}}],[\"的目标与贡献\",{\"1\":{\"460\":1}}],[\"的目标就是\",{\"1\":{\"396\":1}}],[\"的目标是基于编码向量恢复被掩码的图像块\",{\"1\":{\"167\":1}}],[\"的目标是\",{\"1\":{\"71\":1}}],[\"的目标是预测出与该问题相关的点云区域\",{\"1\":{\"70\":1}}],[\"的目标\",{\"1\":{\"62\":1}}],[\"的框架\",{\"1\":{\"48\":1}}],[\"的拼接特征\",{\"1\":{\"46\":1}}],[\"的损失函数\",{\"1\":{\"403\":1}}],[\"的损失函数的时候\",{\"1\":{\"240\":1}}],[\"的损失\",{\"1\":{\"40\":1}}],[\"的共享结构特征\",{\"1\":{\"29\":1}}],[\"的回答\",{\"1\":{\"28\":2}}],[\"的\",{\"1\":{\"24\":2,\"40\":2,\"45\":5,\"76\":1,\"83\":1,\"93\":1,\"96\":1,\"143\":1,\"147\":1,\"161\":1,\"162\":1,\"163\":5,\"194\":1,\"195\":1,\"196\":1,\"224\":2,\"240\":2,\"285\":9,\"323\":1,\"424\":1,\"447\":2,\"471\":1,\"472\":1,\"475\":1,\"477\":5,\"494\":2,\"505\":1,\"508\":5,\"511\":1,\"520\":1,\"540\":1,\"542\":1,\"600\":1,\"674\":8,\"686\":1}}],[\"的矩阵\",{\"1\":{\"18\":1,\"108\":1,\"291\":1,\"326\":1}}],[\"6f\",{\"1\":{\"514\":2}}],[\"671b\",{\"1\":{\"674\":1}}],[\"67b\",{\"1\":{\"674\":1}}],[\"67b7e751e6b5931a9f45274653f4f653a4e6cdf6\",{\"1\":{\"289\":1}}],[\"67\",{\"1\":{\"480\":1,\"481\":1,\"482\":1}}],[\"6→8\",{\"1\":{\"455\":1}}],[\"66\",{\"1\":{\"455\":1,\"482\":1,\"483\":1}}],[\"665k\",{\"1\":{\"202\":1}}],[\"6平均分数\",{\"1\":{\"449\":1}}],[\"62b\",{\"1\":{\"482\":4,\"483\":1}}],[\"62\",{\"1\":{\"275\":1}}],[\"629\",{\"1\":{\"75\":1,\"82\":1}}],[\"683712\",{\"1\":{\"667\":1}}],[\"683492\",{\"1\":{\"667\":1}}],[\"683271\",{\"1\":{\"667\":1}}],[\"683051\",{\"1\":{\"667\":1}}],[\"682830\",{\"1\":{\"667\":1}}],[\"682609\",{\"1\":{\"667\":1}}],[\"682388\",{\"1\":{\"667\":1}}],[\"682166\",{\"1\":{\"667\":1}}],[\"681\",{\"1\":{\"226\":1}}],[\"6883\",{\"1\":{\"65\":1}}],[\"600m\",{\"1\":{\"242\":1}}],[\"60\",{\"1\":{\"198\":1,\"472\":1}}],[\"63\",{\"1\":{\"195\":1}}],[\"638\",{\"1\":{\"65\":1}}],[\"6b在mllm预训练阶段学习到的视觉特征具有广泛适用性\",{\"1\":{\"215\":1}}],[\"6b模型进行了持续预训练\",{\"1\":{\"215\":1}}],[\"6b的兼容性\",{\"1\":{\"217\":1}}],[\"6b的大参数规模使其视觉表征能力媲美200亿参数的llms\",{\"1\":{\"208\":1}}],[\"6b的持续学习策略\",{\"1\":{\"208\":1}}],[\"6b提取特征\",{\"1\":{\"189\":1}}],[\"6b是一个基于vision\",{\"1\":{\"189\":1}}],[\"6b处理图像分类\",{\"1\":{\"188\":1}}],[\"6b\",{\"1\":{\"181\":1,\"188\":1,\"189\":1,\"190\":3,\"191\":4,\"193\":2,\"196\":2,\"197\":1,\"198\":1,\"200\":1,\"201\":1,\"207\":1,\"212\":2,\"214\":1,\"215\":3,\"219\":1,\"222\":1,\"470\":1,\"674\":1}}],[\"6章\",{\"1\":{\"134\":1}}],[\"648721270700128\",{\"1\":{\"617\":1}}],[\"64头注意力\",{\"1\":{\"481\":1}}],[\"64×64\",{\"1\":{\"105\":1}}],[\"640\",{\"1\":{\"96\":1,\"200\":1}}],[\"64\",{\"1\":{\"35\":3,\"36\":12,\"46\":1,\"59\":10,\"93\":2,\"96\":7,\"100\":1,\"101\":6,\"107\":4,\"109\":3,\"447\":1,\"482\":3}}],[\"696\",{\"1\":{\"226\":1}}],[\"694\",{\"1\":{\"226\":1}}],[\"69\",{\"1\":{\"23\":1,\"51\":1,\"482\":2}}],[\"65b毒性分0\",{\"1\":{\"484\":1}}],[\"65b平均偏见得分66\",{\"1\":{\"482\":1}}],[\"65b平均得分63\",{\"1\":{\"482\":1}}],[\"65b\",{\"1\":{\"482\":1,\"483\":1}}],[\"65b在humaneval\",{\"1\":{\"482\":1}}],[\"65b在8个常识推理基准\",{\"1\":{\"482\":1}}],[\"65b模型真实答案率仅57\",{\"1\":{\"484\":1}}],[\"65b模型\",{\"1\":{\"482\":1}}],[\"65b模型未经数学微调即达50\",{\"1\":{\"482\":1}}],[\"65b模型以60\",{\"1\":{\"482\":1}}],[\"65b模型在mmlu上提升至68\",{\"1\":{\"482\":1}}],[\"65b模型在零样本和少样本\",{\"1\":{\"482\":1}}],[\"65b模型在2048块a100\",{\"1\":{\"481\":1}}],[\"65b模型在常识推理\",{\"1\":{\"480\":1}}],[\"65b得分57\",{\"1\":{\"482\":1}}],[\"65b则与chinchilla\",{\"1\":{\"479\":1}}],[\"65b的llama的微调要780gb的gpu内存\",{\"1\":{\"421\":1}}],[\"65536\",{\"1\":{\"160\":1,\"241\":1,\"246\":2}}],[\"65\",{\"1\":{\"22\":1,\"355\":1}}],[\"6\",{\"0\":{\"43\":1,\"67\":1,\"267\":1,\"295\":1},\"1\":{\"7\":1,\"17\":1,\"28\":1,\"30\":1,\"36\":1,\"40\":1,\"52\":1,\"58\":1,\"59\":1,\"78\":7,\"80\":1,\"81\":1,\"82\":1,\"83\":1,\"93\":1,\"147\":1,\"152\":3,\"160\":2,\"161\":1,\"163\":1,\"178\":1,\"194\":1,\"247\":1,\"286\":1,\"296\":2,\"321\":2,\"322\":2,\"323\":3,\"325\":2,\"326\":7,\"387\":3,\"395\":1,\"397\":7,\"410\":1,\"440\":1,\"454\":1,\"455\":3,\"462\":1,\"463\":1,\"469\":1,\"471\":1,\"474\":1,\"477\":1,\"482\":3,\"484\":1,\"492\":1,\"497\":1,\"513\":2,\"600\":1,\"645\":1,\"659\":2,\"662\":1,\"666\":1,\"667\":1,\"674\":5,\"683\":1}}],[\"但一般通过调用\",{\"1\":{\"686\":1}}],[\"但一般人还是玩不起\",{\"1\":{\"259\":1}}],[\"但要创建完整的应用程序\",{\"1\":{\"682\":1}}],[\"但要注意的是\",{\"1\":{\"326\":1}}],[\"但面对未见过的输入时仍可能出现幻觉\",{\"1\":{\"681\":1}}],[\"但单一模型可能难以全面适应所有场景\",{\"1\":{\"679\":1}}],[\"但大型语言模型的出现再次让聊天机器人这种交互模式可以重新涌现\",{\"1\":{\"678\":1}}],[\"但我们更关注的是其通用能力\",{\"1\":{\"676\":1}}],[\"但尚未到达\",{\"1\":{\"667\":1}}],[\"但从局部看\",{\"1\":{\"667\":1}}],[\"但需直观呈现计算图全貌以辅助调试与理解\",{\"1\":{\"666\":1}}],[\"但无法替代弱引用在框架设计中的针对性优化\",{\"1\":{\"657\":1}}],[\"但无法定位具体交互部位\",{\"1\":{\"51\":1}}],[\"但却是高维高斯的真实现象\",{\"1\":{\"594\":1}}],[\"但事实恰恰相反\",{\"1\":{\"594\":1}}],[\"但当维度\",{\"1\":{\"593\":1}}],[\"但当张量的内存布局不连续\",{\"1\":{\"385\":1}}],[\"但体积增加更快\",{\"1\":{\"592\":1}}],[\"但与此同时\",{\"1\":{\"592\":1}}],[\"但实验表明去除nsp后性能反而提升\",{\"1\":{\"497\":1}}],[\"但实际训练所需的计算资源与较小模型相当\",{\"1\":{\"461\":1}}],[\"但实际上总体阳性概率只有\",{\"1\":{\"569\":1}}],[\"但实际上这行代码的意图是计算\",{\"1\":{\"404\":1}}],[\"但实际上\",{\"1\":{\"274\":1}}],[\"但因循环引用\",{\"1\":{\"657\":1}}],[\"但因批次大小可变\",{\"1\":{\"495\":1}}],[\"但因为\",{\"1\":{\"323\":1}}],[\"但每次掩码不同\",{\"1\":{\"495\":1}}],[\"但每个输出向量的维度由\",{\"1\":{\"309\":1}}],[\"但透明性与可靠性仍需优化\",{\"1\":{\"483\":1}}],[\"但低于gpt\",{\"1\":{\"483\":1}}],[\"但书籍\",{\"1\":{\"482\":1}}],[\"但宗教类别偏差显著\",{\"1\":{\"482\":1}}],[\"但远低于minerva\",{\"1\":{\"482\":1}}],[\"但远低于人类\",{\"1\":{\"455\":1}}],[\"但小模型在长期训练后推理效率更高\",{\"1\":{\"480\":1}}],[\"但hoffmann等人\",{\"1\":{\"480\":1}}],[\"但仍在胡说\",{\"1\":{\"471\":1}}],[\"但仍依赖卷积架构\",{\"1\":{\"253\":1}}],[\"但通用性更强\",{\"1\":{\"497\":1}}],[\"但通过优化关键设计选择提升性能\",{\"1\":{\"493\":1}}],[\"但通过高效训练仍实现sota\",{\"1\":{\"480\":1}}],[\"但通过引入预训练梯度混合\",{\"1\":{\"472\":1}}],[\"但通过在\",{\"1\":{\"471\":1}}],[\"但通常不开源\",{\"1\":{\"210\":1}}],[\"但未具内置限制\",{\"1\":{\"471\":1}}],[\"但强调这种对齐是相对于特定人群\",{\"1\":{\"468\":1}}],[\"但gpt\",{\"1\":{\"464\":1}}],[\"但依赖大量任务标注数据\",{\"1\":{\"464\":1}}],[\"但依赖于大量标注数据和监督学习\",{\"1\":{\"453\":1}}],[\"但作者明确指出\",{\"1\":{\"463\":1}}],[\"但有短板\",{\"1\":{\"462\":1}}],[\"但采用稀疏注意力机制\",{\"1\":{\"461\":1}}],[\"但采用图文检索和图文匹配作为训练目标\",{\"1\":{\"145\":1}}],[\"但泛化能力弱\",{\"1\":{\"461\":1}}],[\"但泛化性受限\",{\"1\":{\"49\":1}}],[\"但性能远低于微调方法\",{\"1\":{\"460\":1}}],[\"但性能全面超越\",{\"1\":{\"133\":1}}],[\"但某些任务仍存在挑战\",{\"1\":{\"459\":1}}],[\"但数据重叠对结果影响有限\",{\"1\":{\"455\":1}}],[\"但数目固定\",{\"1\":{\"396\":1}}],[\"但对于如wic\",{\"1\":{\"462\":1}}],[\"但对于对外提供服务的企业来说\",{\"1\":{\"415\":1}}],[\"但对高置信度预测\",{\"1\":{\"455\":1}}],[\"但定性分析显示生成内容类似摘要\",{\"1\":{\"455\":1}}],[\"但层数\",{\"1\":{\"455\":1}}],[\"但进行了以下优化\",{\"1\":{\"454\":1}}],[\"但结果表明\",{\"1\":{\"465\":1}}],[\"但结果表明语言模型在无监督条件下已具备初步的多任务处理能力\",{\"1\":{\"453\":1}}],[\"但结果与人类表现\",{\"1\":{\"455\":1}}],[\"但结果会更加靠谱\",{\"1\":{\"433\":1}}],[\"但其通用性优势使其成为最终选择\",{\"1\":{\"495\":1}}],[\"但其本质仍是一个\",{\"1\":{\"463\":1}}],[\"但其在自然语言处理\",{\"1\":{\"453\":1}}],[\"但其主要目的是训练可迁移的视觉模型\",{\"1\":{\"272\":1}}],[\"但并不能保证结果的正确性\",{\"1\":{\"433\":1}}],[\"但并不是所有的参数都是发挥同样作用的\",{\"1\":{\"420\":1}}],[\"但起核心作用的参数是低秩的\",{\"1\":{\"428\":1}}],[\"但也可能会把原来表现好的别的领域的能力变差\",{\"1\":{\"416\":1}}],[\"但也不能写成\",{\"1\":{\"387\":1}}],[\"但fft也会带来一些问题\",{\"1\":{\"416\":1}}],[\"但保留作为接口兼容\",{\"1\":{\"407\":1}}],[\"但保留它们作为接口兼容\",{\"1\":{\"405\":1}}],[\"但保留接口以备后续扩展\",{\"1\":{\"401\":1}}],[\"但真实是正类的样本数\",{\"1\":{\"405\":1}}],[\"但真实是负类的样本数\",{\"1\":{\"405\":1}}],[\"但更易梯度下降\",{\"1\":{\"401\":1}}],[\"但带来了\",{\"1\":{\"395\":1}}],[\"但神经网络的自适应基函数和分层结构使其\",{\"1\":{\"395\":1}}],[\"但神经网络的非线性基函数组合比传统多项式逼近更灵活\",{\"1\":{\"395\":1}}],[\"但神经网络的通用近似性质也被证明对于其他类型的激活函数\",{\"1\":{\"395\":1}}],[\"但不会跑出系统之外\",{\"1\":{\"566\":1}}],[\"但不会被视为可训练参数\",{\"1\":{\"389\":1}}],[\"但不一定更公正\",{\"1\":{\"471\":1}}],[\"但不同于它们使用结构明确的元任务\",{\"1\":{\"464\":1}}],[\"但不占用额外内存\",{\"1\":{\"387\":1}}],[\"但如果我们在内部函数中引用了外部函数的变量\",{\"1\":{\"366\":1}}],[\"但如果训练时没有加入扰动\",{\"1\":{\"112\":1}}],[\"但考虑变量相关性\",{\"1\":{\"355\":1}}],[\"但物理内存中数据不复制\",{\"1\":{\"327\":1}}],[\"但此时并没有复制任何数据\",{\"1\":{\"325\":1}}],[\"但默认的组合方式可能不满足所有需求\",{\"1\":{\"289\":1}}],[\"但发现这种方法的训练效率\",{\"1\":{\"278\":1}}],[\"但存在词汇表限制或效率问题\",{\"1\":{\"454\":1}}],[\"但存在两大局限\",{\"1\":{\"395\":1}}],[\"但存在一定的噪声\",{\"1\":{\"278\":1}}],[\"但存在以下局限性\",{\"1\":{\"6\":1}}],[\"但计算复杂且处理速度较慢\",{\"1\":{\"253\":1}}],[\"但提升幅度不大\",{\"1\":{\"239\":1}}],[\"但已经可以处理基本的图文问答任务\",{\"1\":{\"226\":1}}],[\"但较gpt\",{\"1\":{\"220\":1}}],[\"但internvit仍展现出与新语言模型的优秀兼容性和可移植性\",{\"1\":{\"215\":1}}],[\"但开源模型通过高分辨率优化\",{\"1\":{\"212\":1}}],[\"但开源模型在文档\",{\"1\":{\"211\":1}}],[\"但受限于视觉模型的规模和对齐效率\",{\"1\":{\"185\":1}}],[\"但受限于预定义类别\",{\"1\":{\"7\":1}}],[\"但\",{\"1\":{\"166\":1,\"403\":1,\"470\":1,\"493\":1}}],[\"但视觉和视觉\",{\"1\":{\"181\":1}}],[\"但视觉和文本的token未对齐\",{\"1\":{\"149\":1}}],[\"但视觉\",{\"1\":{\"166\":1}}],[\"但标准\",{\"1\":{\"157\":1}}],[\"但难以处理复杂语义交互\",{\"1\":{\"150\":1}}],[\"但文本\",{\"1\":{\"127\":1}}],[\"但多用于低资源语言场景\",{\"1\":{\"124\":1}}],[\"但语言任务的数据增强较困难\",{\"1\":{\"124\":1}}],[\"但包含大量噪声文本\",{\"1\":{\"122\":1}}],[\"但普遍存在两个问题\",{\"1\":{\"120\":1}}],[\"但可能无法充分定制模型行为或写作风格\",{\"1\":{\"681\":1}}],[\"但可能会出错\",{\"1\":{\"433\":1}}],[\"但可用于特定任务\",{\"1\":{\"115\":1}}],[\"但可供性类别相同\",{\"1\":{\"20\":1}}],[\"但这仍然限制了few\",{\"1\":{\"463\":1}}],[\"但这一方向为减少对人工标注数据的依赖提供了新思路\",{\"1\":{\"457\":1}}],[\"但这些信息不需要训练\",{\"1\":{\"389\":1}}],[\"但这些模型仍然采用固定类别的softmax分类器进行预训练\",{\"1\":{\"278\":1}}],[\"但这种方法存在三个主要限制\",{\"1\":{\"181\":1}}],[\"但这种方式表达能力有限\",{\"1\":{\"112\":1}}],[\"但这可能导致所选邻域的实际尺寸随点的密度变化而变化\",{\"1\":{\"90\":1}}],[\"但遇到遮挡严重或点分布不均匀时性能下降明显\",{\"1\":{\"112\":1}}],[\"但由于它们是神经网络直接预测出来的\",{\"1\":{\"108\":1}}],[\"但由于其无序性和非规则性\",{\"1\":{\"103\":1}}],[\"但在大型模型中特别突出\",{\"1\":{\"676\":1}}],[\"但在理解复杂语言规则方面存在一定局限性\",{\"1\":{\"673\":1}}],[\"但在实际应用中\",{\"1\":{\"658\":1}}],[\"但在对内存敏感的场景下依然存在问题\",{\"1\":{\"657\":1}}],[\"但在强化学习微调中加入\",{\"1\":{\"469\":1}}],[\"但在\",{\"1\":{\"463\":1}}],[\"但在其他任务\",{\"1\":{\"463\":1}}],[\"但在如wic\",{\"1\":{\"462\":1}}],[\"但在结构化或需要多步推理的任务中\",{\"1\":{\"462\":1}}],[\"但在摘要\",{\"1\":{\"456\":1}}],[\"但在前景远少于背景时容易偏向负样本\",{\"1\":{\"402\":1}}],[\"但在处理超大规模点云时\",{\"1\":{\"112\":1}}],[\"但在精度上仍略逊一筹\",{\"1\":{\"112\":1}}],[\"但在一些复杂区域\",{\"1\":{\"112\":1}}],[\"但在点云这种非结构化数据中\",{\"1\":{\"100\":1}}],[\"但在计算上可能非常昂贵\",{\"1\":{\"97\":1}}],[\"但在测试集中保留\",{\"1\":{\"65\":1}}],[\"但是越清晰\",{\"1\":{\"687\":1}}],[\"但是发展速度相当惊人\",{\"1\":{\"674\":1}}],[\"但是这里为了方便理解\",{\"1\":{\"656\":1}}],[\"但是这一改动也引发了另一个问题\",{\"1\":{\"655\":1}}],[\"但是这样造成的一个后果是计算量太庞大\",{\"1\":{\"291\":1}}],[\"但是这样做的逻辑不太清晰\",{\"1\":{\"240\":1}}],[\"但是这样做\",{\"1\":{\"240\":1}}],[\"但是问题是\",{\"1\":{\"566\":1}}],[\"但是和rnn相比\",{\"1\":{\"547\":1}}],[\"但是和transformer原始的encoder还是有所区别\",{\"1\":{\"294\":1}}],[\"但是反馈的来源是ai\",{\"1\":{\"416\":1}}],[\"但是它的缺点也非常明显\",{\"1\":{\"415\":1}}],[\"但是\",{\"1\":{\"349\":1,\"389\":1,\"430\":1,\"508\":2,\"565\":1}}],[\"但是训练速度还是挺快的\",{\"1\":{\"300\":1}}],[\"但是训练速度很慢\",{\"1\":{\"259\":1}}],[\"但是当数据量逐渐增大时\",{\"1\":{\"297\":1}}],[\"但是当训练数据集不够大的时候\",{\"1\":{\"287\":1}}],[\"但是迁移到其它数据集训练时\",{\"1\":{\"296\":1}}],[\"但是实际的代码实现中\",{\"1\":{\"291\":1}}],[\"但是对于一般的小公司或者个人来说\",{\"1\":{\"424\":1}}],[\"但是对于vit这个结构而言\",{\"1\":{\"290\":1}}],[\"但是对于cv领域来讲\",{\"1\":{\"238\":1}}],[\"但是visual\",{\"1\":{\"256\":1}}],[\"但是由于ve仍然使用重的卷积网络进行特征抽取\",{\"1\":{\"255\":1}}],[\"但是由于这两张图片是从同一个图片经过某种变化得到的\",{\"1\":{\"235\":1}}],[\"但是表现都会比有监督要差\",{\"1\":{\"238\":1}}],[\"但是在cv领域\",{\"1\":{\"238\":1}}],[\"但是在一个场景中有多个物体时则不好办\",{\"1\":{\"86\":1}}],[\"但是我们在计算损失时指定了ignore\",{\"1\":{\"514\":1}}],[\"但是我们每次更新这个队列\",{\"1\":{\"238\":1}}],[\"但是我们用到了另外一种信息\",{\"1\":{\"234\":1}}],[\"但是我们需要知道前两张图片是一个类别\",{\"1\":{\"234\":1}}],[\"但是官方仓库的issue给出了明确答复\",{\"1\":{\"140\":1}}],[\"但是尺度不同\",{\"1\":{\"96\":1}}],[\"但它能为后续许多针对转置数据的操作带来更好的内存局部性\",{\"1\":{\"326\":1}}],[\"但它们也引发了伦理和风险问题\",{\"1\":{\"675\":1}}],[\"但它们展现出截然不同的能力\",{\"1\":{\"673\":1}}],[\"但它们常常偏离用户意图\",{\"1\":{\"468\":1}}],[\"但它们的\",{\"1\":{\"383\":1}}],[\"但它们的目标\",{\"1\":{\"231\":1}}],[\"但它们在非互联网图像\",{\"1\":{\"212\":1}}],[\"但它也为后续模型奠定了基础\",{\"1\":{\"112\":1}}],[\"但它是为了统一接口设计的一个占位符\",{\"1\":{\"92\":1}}],[\"但它本质上是\",{\"1\":{\"78\":1}}],[\"但完全不使用注意力机制\",{\"1\":{\"73\":1}}],[\"但测试时要求\",{\"1\":{\"65\":1}}],[\"但效率低\",{\"1\":{\"51\":1}}],[\"但搜索空间大\",{\"1\":{\"49\":1}}],[\"但同一物体的不同实例可能有几何差异\",{\"1\":{\"29\":1}}],[\"但动态功能特性使得mllms难以直接从交互图像推理3d功能\",{\"1\":{\"7\":1}}],[\"但机器人操作需要3d信息\",{\"1\":{\"7\":1}}],[\"2m\",{\"1\":{\"674\":1}}],[\"27\",{\"1\":{\"662\":1,\"666\":1}}],[\"2+1\",{\"1\":{\"660\":1}}],[\"2和wikitext\",{\"1\":{\"510\":1}}],[\"2任务上表现略优\",{\"1\":{\"497\":1}}],[\"2论文\",{\"0\":{\"487\":1}}],[\"2tb\",{\"1\":{\"480\":1}}],[\"2分\",{\"1\":{\"462\":1}}],[\"2通过示例提示\",{\"1\":{\"455\":1}}],[\"2通过训练一个包含45百万网页链接的webtext数据集\",{\"1\":{\"452\":1}}],[\"2仅通过文档+历史对话+\",{\"1\":{\"455\":1}}],[\"2以70\",{\"1\":{\"455\":1}}],[\"2将\",{\"1\":{\"455\":1}}],[\"2使用字节级bpe\",{\"1\":{\"455\":1}}],[\"2在非分布数据\",{\"1\":{\"455\":1}}],[\"2在验证集\",{\"1\":{\"455\":1}}],[\"2在7\",{\"1\":{\"455\":1}}],[\"2在8个标准语言建模数据集上进行了测试\",{\"1\":{\"455\":1}}],[\"2在零样本设置下能完成多种任务\",{\"1\":{\"453\":1}}],[\"2在生成连贯文本方面的能力\",{\"1\":{\"452\":1}}],[\"2模型证明了大规模语言模型在无监督多任务学习中的强大潜力\",{\"1\":{\"457\":1}}],[\"2模型\",{\"1\":{\"452\":1,\"455\":1}}],[\"2取得91\",{\"1\":{\"448\":1}}],[\"2是二分类\",{\"1\":{\"448\":1}}],[\"2相比prompt\",{\"1\":{\"432\":1}}],[\"2×2\",{\"1\":{\"396\":2,\"397\":2}}],[\"2×1000000000\",{\"1\":{\"297\":1}}],[\"2x3\",{\"1\":{\"325\":1}}],[\"2的微调潜力\",{\"1\":{\"456\":1}}],[\"2的完全抽象式输出\",{\"1\":{\"456\":1}}],[\"2的局限性\",{\"1\":{\"456\":1}}],[\"2的困惑度\",{\"1\":{\"455\":1}}],[\"2的起因\",{\"1\":{\"280\":1}}],[\"2的强健基础进一步预训练\",{\"1\":{\"215\":1}}],[\"2f\",{\"1\":{\"275\":2,\"277\":2}}],[\"2训练时使用的webtext数据集相似\",{\"1\":{\"272\":1}}],[\"2版本中对internvit\",{\"1\":{\"215\":1}}],[\"2k\",{\"1\":{\"201\":1}}],[\"2n\",{\"1\":{\"162\":3}}],[\"2b\",{\"1\":{\"145\":4,\"147\":4,\"297\":1}}],[\"2节中使用的prompt\",{\"1\":{\"142\":1}}],[\"2>\",{\"1\":{\"83\":1}}],[\"2️⃣\",{\"0\":{\"73\":1},\"1\":{\"100\":1}}],[\"235b\",{\"1\":{\"674\":1}}],[\"2301\",{\"1\":{\"279\":1}}],[\"2304\",{\"1\":{\"223\":1}}],[\"2303\",{\"1\":{\"47\":1}}],[\"2312\",{\"1\":{\"179\":1}}],[\"23\",{\"1\":{\"65\":1,\"67\":1,\"83\":1,\"227\":1,\"323\":3,\"470\":1,\"482\":1}}],[\"22的实现\",{\"1\":{\"660\":1}}],[\"225\",{\"1\":{\"244\":1}}],[\"229\",{\"1\":{\"244\":1}}],[\"22b\",{\"1\":{\"193\":1}}],[\"22b除外\",{\"1\":{\"183\":1}}],[\"22\",{\"0\":{\"660\":1},\"1\":{\"49\":1,\"323\":1,\"519\":1,\"546\":1}}],[\"224x224\",{\"1\":{\"290\":3,\"291\":1,\"300\":1}}],[\"224×224\",{\"1\":{\"190\":1,\"200\":1,\"201\":1,\"300\":1}}],[\"224\",{\"1\":{\"29\":2,\"58\":2,\"244\":2,\"290\":2,\"291\":2,\"300\":6}}],[\"2c\",{\"1\":{\"45\":1,\"59\":1}}],[\"200k\",{\"1\":{\"674\":5}}],[\"2003\",{\"1\":{\"673\":1}}],[\"200\",{\"1\":{\"660\":1}}],[\"2000\",{\"1\":{\"447\":1}}],[\"20b\",{\"1\":{\"214\":1,\"215\":1,\"219\":1}}],[\"20k\",{\"1\":{\"201\":1}}],[\"2010\",{\"1\":{\"300\":1}}],[\"2016\",{\"1\":{\"395\":2,\"469\":1,\"678\":1}}],[\"2016年的工作\",{\"1\":{\"278\":1}}],[\"2016b\",{\"1\":{\"178\":1}}],[\"2018将人类偏好学习应用于模仿学习\",{\"1\":{\"469\":1}}],[\"2018\",{\"1\":{\"178\":1,\"454\":2,\"455\":1,\"469\":2,\"485\":2,\"519\":1,\"673\":1,\"674\":1}}],[\"2015\",{\"1\":{\"178\":1,\"470\":1}}],[\"2017年的工作\",{\"1\":{\"278\":1}}],[\"2017\",{\"1\":{\"131\":1,\"454\":1,\"469\":1,\"485\":1}}],[\"2019b\",{\"1\":{\"472\":1}}],[\"2019\",{\"1\":{\"126\":1,\"131\":1,\"469\":3,\"470\":1,\"485\":1,\"674\":2}}],[\"2014\",{\"1\":{\"49\":1}}],[\"20\",{\"1\":{\"75\":1,\"82\":3,\"131\":1,\"145\":2,\"323\":1,\"404\":1,\"462\":1,\"512\":4,\"673\":1}}],[\"2025\",{\"1\":{\"674\":9}}],[\"2022的协议\",{\"1\":{\"483\":1}}],[\"2022\",{\"1\":{\"480\":1,\"481\":1,\"485\":5,\"674\":2,\"682\":1}}],[\"2022年清华提出的\",{\"1\":{\"424\":1}}],[\"2020\",{\"1\":{\"126\":1,\"131\":1,\"469\":4,\"470\":1,\"485\":3}}],[\"2021年微软提出的\",{\"1\":{\"424\":1}}],[\"2021a\",{\"1\":{\"126\":1,\"127\":2}}],[\"2021\",{\"1\":{\"73\":1,\"126\":2,\"131\":2,\"270\":2,\"469\":8,\"472\":1,\"481\":1,\"674\":1,\"677\":1}}],[\"2024年\",{\"1\":{\"674\":1}}],[\"2024\",{\"1\":{\"22\":1,\"60\":1,\"673\":1,\"674\":19,\"684\":1}}],[\"2023\",{\"1\":{\"22\":1,\"674\":17}}],[\"2048\",{\"1\":{\"18\":1,\"29\":2,\"30\":1,\"35\":7,\"40\":3,\"46\":2,\"59\":4,\"67\":1,\"68\":2,\"70\":1,\"81\":2,\"83\":1,\"174\":1}}],[\"290\",{\"1\":{\"23\":1}}],[\"29\",{\"1\":{\"22\":1,\"24\":1,\"83\":1,\"470\":1,\"471\":1,\"472\":1,\"674\":1}}],[\"2501\",{\"1\":{\"520\":2}}],[\"2504\",{\"1\":{\"37\":1}}],[\"25e\",{\"1\":{\"447\":1}}],[\"25~0\",{\"1\":{\"404\":1}}],[\"2578\",{\"1\":{\"520\":2}}],[\"257\",{\"1\":{\"286\":3,\"454\":1}}],[\"255\",{\"1\":{\"83\":2,\"290\":2}}],[\"256维\",{\"1\":{\"154\":1}}],[\"256\",{\"1\":{\"36\":3,\"93\":5,\"96\":6,\"100\":1,\"101\":11,\"107\":3,\"110\":3,\"111\":3,\"146\":1,\"241\":2,\"290\":2,\"493\":1}}],[\"25\",{\"1\":{\"22\":1,\"78\":1,\"133\":1,\"134\":1,\"188\":1,\"404\":1}}],[\"2d功能检测\",{\"1\":{\"51\":1}}],[\"2d演示与3d物体来自不同实例\",{\"1\":{\"49\":1}}],[\"2d\",{\"1\":{\"16\":1,\"22\":2,\"29\":1,\"40\":1,\"42\":1,\"47\":1,\"114\":2,\"169\":1,\"291\":1,\"382\":1}}],[\"2d方法难以直接迁移\",{\"1\":{\"7\":1}}],[\"260亿参数\",{\"1\":{\"219\":1}}],[\"26\",{\"1\":{\"15\":1,\"482\":1,\"662\":2}}],[\"287\",{\"1\":{\"200\":1}}],[\"2880\",{\"1\":{\"131\":1}}],[\"28\",{\"1\":{\"8\":1}}],[\"2\",{\"0\":{\"9\":1,\"23\":1,\"41\":1,\"63\":1,\"263\":1,\"284\":1,\"286\":1,\"291\":1,\"308\":1,\"313\":1,\"451\":1,\"689\":1},\"1\":{\"7\":1,\"8\":1,\"28\":4,\"29\":5,\"30\":2,\"32\":6,\"34\":3,\"35\":10,\"36\":8,\"40\":1,\"41\":5,\"45\":9,\"46\":4,\"58\":1,\"59\":30,\"65\":1,\"70\":17,\"73\":2,\"76\":3,\"78\":5,\"80\":1,\"82\":4,\"83\":4,\"92\":8,\"93\":1,\"96\":7,\"99\":2,\"100\":6,\"101\":2,\"105\":1,\"107\":2,\"108\":2,\"109\":6,\"111\":4,\"112\":2,\"145\":6,\"146\":1,\"147\":9,\"157\":1,\"160\":2,\"161\":2,\"162\":3,\"163\":3,\"166\":1,\"185\":1,\"190\":3,\"191\":2,\"194\":2,\"196\":1,\"200\":1,\"204\":1,\"215\":3,\"219\":1,\"220\":1,\"221\":1,\"244\":5,\"246\":1,\"247\":1,\"249\":1,\"259\":1,\"262\":2,\"264\":1,\"266\":3,\"268\":3,\"272\":1,\"273\":1,\"280\":1,\"281\":2,\"283\":2,\"284\":5,\"285\":5,\"286\":2,\"289\":2,\"291\":3,\"293\":1,\"295\":4,\"321\":2,\"322\":2,\"323\":13,\"325\":11,\"326\":10,\"327\":3,\"328\":1,\"345\":1,\"346\":1,\"351\":1,\"383\":1,\"384\":3,\"386\":9,\"387\":10,\"395\":2,\"397\":5,\"401\":1,\"402\":2,\"404\":4,\"407\":1,\"410\":8,\"411\":9,\"427\":1,\"445\":2,\"447\":1,\"448\":2,\"449\":1,\"451\":1,\"454\":8,\"455\":5,\"460\":2,\"461\":12,\"462\":3,\"463\":1,\"464\":3,\"469\":1,\"470\":5,\"471\":1,\"474\":1,\"477\":10,\"481\":3,\"482\":5,\"483\":1,\"484\":1,\"487\":2,\"492\":3,\"493\":1,\"494\":2,\"495\":1,\"497\":1,\"498\":2,\"499\":3,\"504\":1,\"510\":10,\"511\":1,\"512\":2,\"513\":6,\"514\":1,\"517\":1,\"523\":1,\"525\":1,\"528\":1,\"529\":1,\"531\":4,\"535\":1,\"537\":1,\"538\":2,\"540\":2,\"541\":3,\"553\":1,\"556\":1,\"558\":5,\"566\":4,\"590\":2,\"594\":1,\"596\":1,\"597\":1,\"600\":2,\"608\":2,\"613\":1,\"622\":1,\"631\":1,\"645\":1,\"655\":1,\"658\":2,\"659\":11,\"660\":7,\"662\":18,\"666\":12,\"667\":18,\"673\":3,\"674\":19}}],[\"21843\",{\"1\":{\"300\":1}}],[\"21000\",{\"1\":{\"300\":1}}],[\"21k\",{\"1\":{\"300\":3}}],[\"215\",{\"1\":{\"65\":1}}],[\"213\",{\"1\":{\"17\":1}}],[\"21\",{\"1\":{\"7\":1,\"323\":1,\"482\":1}}],[\"244\",{\"1\":{\"291\":1}}],[\"242\",{\"1\":{\"226\":1}}],[\"2400\",{\"1\":{\"131\":1}}],[\"24\",{\"1\":{\"29\":1,\"34\":3,\"323\":6,\"455\":1}}],[\"24类\",{\"1\":{\"17\":1}}],[\"24类功能\",{\"1\":{\"6\":1}}],[\"2411\",{\"1\":{\"4\":1}}],[\"31\",{\"1\":{\"674\":1}}],[\"3167\",{\"1\":{\"515\":1}}],[\"31gb\",{\"1\":{\"494\":1}}],[\"3提升至87\",{\"1\":{\"498\":1}}],[\"3但可靠性仍不足\",{\"1\":{\"484\":1}}],[\"33\",{\"1\":{\"482\":1}}],[\"336\",{\"1\":{\"272\":1}}],[\"36\",{\"1\":{\"482\":1,\"662\":1,\"666\":1}}],[\"3640\",{\"1\":{\"472\":1}}],[\"364×364\",{\"1\":{\"203\":1}}],[\"3小10倍却性能更优\",{\"1\":{\"480\":1}}],[\"3更受人类偏好\",{\"1\":{\"468\":1}}],[\"3更像是一个巨大的\",{\"1\":{\"463\":1}}],[\"3模型\",{\"1\":{\"468\":1}}],[\"3模型架构基本沿用gpt\",{\"1\":{\"461\":1}}],[\"3进行监督学习微调\",{\"1\":{\"467\":1}}],[\"3站在了词向量\",{\"1\":{\"464\":1}}],[\"3是首次系统性\",{\"1\":{\"464\":1}}],[\"3并未对每个任务建立单独的模型\",{\"1\":{\"464\":1}}],[\"3验证了一个关键假设\",{\"1\":{\"464\":1}}],[\"3以175b参数扩展至前代模型的10倍以上\",{\"1\":{\"464\":1}}],[\"3完全通过文本学习并表达任务结构\",{\"1\":{\"464\":1}}],[\"3完全不依赖梯度更新\",{\"1\":{\"463\":1}}],[\"3通过扩展模型容量\",{\"1\":{\"464\":1}}],[\"3之前\",{\"1\":{\"464\":1}}],[\"3继承了这一发展路线\",{\"1\":{\"464\":1}}],[\"3及其衍生模型时\",{\"1\":{\"463\":1}}],[\"3虽然能完成基础算术和简单逻辑题\",{\"1\":{\"463\":1}}],[\"3虽然模型更大\",{\"1\":{\"461\":1}}],[\"3对提示\",{\"1\":{\"463\":1}}],[\"3对复杂语义结构的掌握仍有提升空间\",{\"1\":{\"462\":1}}],[\"3展示了推理能力的不足\",{\"1\":{\"462\":1}}],[\"3展现出更强的语言建模优势\",{\"1\":{\"462\":1}}],[\"3表现较差\",{\"1\":{\"462\":1}}],[\"3也表现出明显的few\",{\"1\":{\"462\":1}}],[\"3508\",{\"1\":{\"515\":1}}],[\"35\",{\"1\":{\"462\":1}}],[\"3比较不同答案的语言模型概率\",{\"1\":{\"461\":1}}],[\"3主要研究后三种方法\",{\"1\":{\"461\":1}}],[\"3在设计的算术\",{\"1\":{\"462\":1}}],[\"3在识别细粒度语义差异上仍有明显不足\",{\"1\":{\"462\":1}}],[\"3在少样本\",{\"1\":{\"462\":1}}],[\"3在few\",{\"1\":{\"462\":1,\"463\":1}}],[\"3在winograd\",{\"1\":{\"462\":1}}],[\"3在英法\",{\"1\":{\"462\":1}}],[\"3在triviaqa\",{\"1\":{\"462\":1}}],[\"3在传统语言建模任务\",{\"1\":{\"462\":1}}],[\"3在语言建模和完形填空任务中的表现\",{\"1\":{\"462\":1}}],[\"3在多个任务上取得了令人印象深刻的成绩\",{\"1\":{\"463\":1}}],[\"3在多个任务上展现出超越以往fine\",{\"1\":{\"461\":1}}],[\"3在多数nlp任务中\",{\"1\":{\"462\":1}}],[\"3在多种自然语言处理任务中表现出色\",{\"1\":{\"459\":1}}],[\"3在通用语言系统发展中的潜力及其可能带来的广泛社会影响\",{\"1\":{\"459\":1}}],[\"3等更大规模模型的开发奠定了基础\",{\"1\":{\"456\":1}}],[\"3的175b参数\",{\"1\":{\"480\":1}}],[\"3的核心创新之一\",{\"1\":{\"464\":1}}],[\"3的推理过程完全由大量参数和非线性变换组成\",{\"1\":{\"463\":1}}],[\"3的表现明显弱于专门微调过的模型\",{\"1\":{\"463\":1}}],[\"3的上下文窗口扩大到2048\",{\"1\":{\"463\":1}}],[\"3的few\",{\"1\":{\"462\":1,\"463\":1}}],[\"3的研究方法基于\",{\"1\":{\"461\":1}}],[\"3的训练数据主要来自以下五个来源\",{\"1\":{\"461\":1}}],[\"3的少样本学习\",{\"1\":{\"455\":1}}],[\"3的准确率\",{\"1\":{\"448\":1}}],[\"3的维度\",{\"1\":{\"295\":1}}],[\"3层即可\",{\"1\":{\"395\":1}}],[\"3×4\",{\"1\":{\"325\":1}}],[\"3×3\",{\"1\":{\"105\":1,\"107\":1,\"108\":1}}],[\"3x4\",{\"1\":{\"325\":1}}],[\"3x3\",{\"1\":{\"107\":2}}],[\"396\",{\"1\":{\"275\":1}}],[\"3m\",{\"1\":{\"226\":1}}],[\"3v\",{\"1\":{\"210\":1}}],[\"3n\",{\"1\":{\"162\":1}}],[\"3b参数的instructgpt模型\",{\"1\":{\"468\":1}}],[\"3b对比175b\",{\"1\":{\"467\":1}}],[\"3b\",{\"1\":{\"145\":3,\"147\":2,\"469\":1,\"470\":1,\"471\":1,\"472\":1}}],[\"30b\",{\"1\":{\"674\":1}}],[\"3072\",{\"1\":{\"447\":1,\"525\":2}}],[\"300m数据集\",{\"1\":{\"297\":1}}],[\"300m数据集的规模达到了上亿级别\",{\"1\":{\"278\":1}}],[\"300m数据集取得了较好的结果\",{\"1\":{\"278\":1}}],[\"300m数据集是谷歌从互联网上收集的\",{\"1\":{\"278\":1}}],[\"300m数据集来预训练模型在imagenet上取得sota\",{\"1\":{\"278\":1}}],[\"300m数据集还要多出1亿对\",{\"1\":{\"272\":1}}],[\"300\",{\"1\":{\"226\":1}}],[\"30\",{\"1\":{\"145\":2,\"566\":2,\"662\":1,\"666\":1,\"667\":1}}],[\"3节中的条件生成任务\",{\"1\":{\"142\":1}}],[\"3+d\",{\"1\":{\"92\":3}}],[\"3️⃣\",{\"0\":{\"74\":1},\"1\":{\"100\":1}}],[\"345m\",{\"1\":{\"454\":1,\"455\":1}}],[\"34b更换为internlm2\",{\"1\":{\"215\":1}}],[\"34b结合\",{\"1\":{\"215\":1}}],[\"34\",{\"1\":{\"23\":1,\"470\":1,\"471\":1,\"472\":1}}],[\"38gb\",{\"1\":{\"494\":1}}],[\"384\",{\"1\":{\"142\":1,\"146\":1,\"226\":1}}],[\"38\",{\"1\":{\"17\":1,\"24\":1}}],[\"32b\",{\"1\":{\"674\":6,\"688\":1}}],[\"32k\",{\"1\":{\"674\":2}}],[\"32个示例\",{\"1\":{\"462\":1}}],[\"32768\",{\"1\":{\"272\":1}}],[\"328\",{\"1\":{\"216\":1}}],[\"32的预训练权重进行初始化\",{\"1\":{\"191\":1}}],[\"320\",{\"1\":{\"35\":1,\"59\":1,\"96\":1}}],[\"32\",{\"1\":{\"17\":1,\"96\":4,\"99\":2,\"101\":2,\"217\":1,\"272\":1,\"397\":1,\"428\":1,\"447\":1,\"600\":1,\"662\":1,\"666\":1}}],[\"37b\",{\"1\":{\"674\":1}}],[\"37\",{\"1\":{\"15\":1,\"482\":1}}],[\"3\",{\"0\":{\"9\":1,\"14\":2,\"15\":1,\"24\":1,\"42\":1,\"64\":1,\"264\":1,\"285\":1,\"292\":1,\"309\":1,\"314\":1,\"458\":1},\"1\":{\"7\":2,\"8\":4,\"26\":1,\"28\":4,\"29\":3,\"30\":4,\"34\":6,\"35\":7,\"36\":1,\"40\":3,\"46\":2,\"58\":2,\"59\":12,\"68\":2,\"70\":9,\"75\":1,\"76\":8,\"78\":1,\"80\":1,\"82\":5,\"83\":12,\"92\":14,\"93\":5,\"96\":6,\"98\":2,\"99\":2,\"100\":16,\"101\":1,\"105\":2,\"107\":9,\"109\":1,\"110\":1,\"142\":1,\"145\":2,\"147\":2,\"161\":1,\"163\":2,\"174\":1,\"184\":1,\"190\":2,\"191\":1,\"193\":1,\"195\":1,\"196\":2,\"200\":1,\"203\":1,\"216\":1,\"220\":1,\"224\":4,\"226\":1,\"242\":2,\"244\":1,\"246\":1,\"247\":1,\"262\":1,\"266\":1,\"268\":1,\"273\":1,\"274\":1,\"277\":1,\"278\":1,\"284\":10,\"285\":2,\"286\":1,\"289\":1,\"291\":3,\"295\":7,\"321\":5,\"322\":2,\"323\":12,\"325\":4,\"326\":11,\"327\":8,\"328\":2,\"352\":1,\"370\":1,\"384\":1,\"386\":8,\"387\":16,\"395\":3,\"397\":5,\"404\":1,\"405\":2,\"410\":7,\"411\":1,\"425\":2,\"447\":1,\"454\":2,\"455\":12,\"458\":1,\"459\":1,\"460\":5,\"461\":1,\"462\":14,\"463\":2,\"464\":1,\"465\":1,\"467\":1,\"468\":1,\"469\":5,\"470\":2,\"471\":17,\"472\":2,\"474\":1,\"477\":6,\"479\":1,\"481\":2,\"482\":6,\"483\":1,\"484\":1,\"492\":1,\"494\":4,\"497\":4,\"513\":2,\"523\":1,\"531\":2,\"556\":1,\"558\":1,\"566\":1,\"600\":1,\"645\":1,\"654\":1,\"659\":8,\"660\":10,\"662\":3,\"666\":4,\"667\":2,\"673\":5,\"674\":24,\"676\":1,\"684\":1}}],[\"3d`\",{\"1\":{\"82\":1}}],[\"3d对象功能区域分割\",{\"1\":{\"60\":1}}],[\"3d对象功能定位\",{\"1\":{\"6\":1}}],[\"3d功能热图\",{\"1\":{\"54\":1}}],[\"3d功能定位\",{\"1\":{\"51\":1}}],[\"3d数据学习功能\",{\"1\":{\"51\":1}}],[\"3daffordance\",{\"1\":{\"30\":2,\"35\":4,\"59\":8,\"70\":6,\"76\":12}}],[\"3d点云样本\",{\"1\":{\"29\":1}}],[\"3dir\",{\"1\":{\"17\":1}}],[\"3d实例\",{\"1\":{\"6\":1}}],[\"3d\",{\"0\":{\"46\":1,\"84\":1},\"1\":{\"2\":1,\"4\":1,\"7\":1,\"8\":2,\"15\":2,\"16\":2,\"17\":1,\"22\":2,\"25\":1,\"26\":4,\"29\":1,\"37\":1,\"40\":2,\"42\":1,\"46\":2,\"47\":1,\"48\":1,\"60\":3,\"62\":2,\"63\":1,\"67\":1,\"69\":3,\"70\":4,\"81\":2,\"82\":6,\"107\":1,\"109\":1,\"112\":1,\"114\":4,\"382\":1}}],[\"图解transformer\",{\"0\":{\"545\":1},\"1\":{\"545\":1}}],[\"图解bert\",{\"1\":{\"518\":1}}],[\"图解\",{\"0\":{\"518\":1}}],[\"图7\",{\"1\":{\"427\":1}}],[\"图6\",{\"1\":{\"425\":1}}],[\"图示\",{\"1\":{\"397\":1}}],[\"图生文\",{\"1\":{\"285\":1}}],[\"图中有什么\",{\"1\":{\"226\":1}}],[\"图中展示的是\",{\"1\":{\"159\":1}}],[\"图表解析\",{\"1\":{\"220\":1}}],[\"图表和场景文本理解方面仍显著落后于商业模型\",{\"1\":{\"211\":1}}],[\"图5\",{\"1\":{\"217\":1,\"425\":1}}],[\"图块数量为1至12个\",{\"1\":{\"215\":1}}],[\"图\",{\"1\":{\"147\":1,\"166\":1,\"190\":1,\"350\":1,\"351\":1,\"352\":1,\"353\":2,\"469\":1,\"470\":2,\"590\":1,\"594\":1}}],[\"图文到文本\",{\"1\":{\"286\":1}}],[\"图文生成等多模态任务\",{\"1\":{\"267\":1}}],[\"图文融合\",{\"1\":{\"163\":1}}],[\"图文共享表示的嵌入维度\",{\"1\":{\"147\":1}}],[\"图文对比\",{\"1\":{\"283\":1}}],[\"图文对比损失\",{\"1\":{\"161\":1}}],[\"图文对比损失定义为交叉熵\",{\"1\":{\"154\":1}}],[\"图文对比目标\",{\"1\":{\"145\":1}}],[\"图文对比学习后的共同嵌入维度\",{\"1\":{\"160\":1}}],[\"图文对比学习旨在融合之前学习更好的单模态表示\",{\"1\":{\"154\":1}}],[\"图文对比学习\",{\"1\":{\"120\":1}}],[\"图文数据的自举式清洗机制\",{\"0\":{\"128\":1}}],[\"图文检索\",{\"1\":{\"120\":1,\"198\":1,\"267\":1}}],[\"图文匹配任务\",{\"1\":{\"145\":1}}],[\"图文匹配目标\",{\"1\":{\"145\":1}}],[\"图文匹配\",{\"1\":{\"120\":1,\"147\":2}}],[\"图卷积\",{\"1\":{\"112\":1}}],[\"图4显示\",{\"1\":{\"455\":1}}],[\"图4c\",{\"1\":{\"188\":1,\"189\":1}}],[\"图4b\",{\"1\":{\"188\":2}}],[\"图4a\",{\"1\":{\"188\":1,\"189\":1}}],[\"图4\",{\"1\":{\"49\":1,\"132\":1,\"188\":1,\"189\":1,\"423\":1}}],[\"图2显示模型性能与训练token量强相关\",{\"1\":{\"482\":1}}],[\"图2\",{\"1\":{\"49\":2,\"423\":1,\"455\":1}}],[\"图片切割\",{\"0\":{\"291\":1}}],[\"图片预处理\",{\"0\":{\"290\":1}}],[\"图片库中的图片\",{\"1\":{\"276\":1}}],[\"图片分类\",{\"1\":{\"275\":1,\"277\":1}}],[\"图片分类实战\",{\"1\":{\"275\":1}}],[\"图片x1经过数据增强t2得到图片x12\",{\"1\":{\"238\":1}}],[\"图片\",{\"1\":{\"29\":1,\"58\":1}}],[\"图片索引文件路径\",{\"1\":{\"29\":1}}],[\"图像三种模态的深度理解能力\",{\"1\":{\"674\":1}}],[\"图像空间的含义\",{\"0\":{\"594\":1}}],[\"图像中各个类别的像素数量通常不均衡\",{\"1\":{\"399\":1}}],[\"图像中也可能存在未被文本描述的实体\",{\"1\":{\"157\":1}}],[\"图像直观理解\",{\"1\":{\"355\":1}}],[\"图像到文本的桥梁\",{\"1\":{\"301\":1}}],[\"图像到文本的相似度权重\",{\"1\":{\"162\":1}}],[\"图像或\",{\"1\":{\"290\":2}}],[\"图像预处理的转换操作\",{\"1\":{\"289\":1}}],[\"图像预训练分辨率为\",{\"1\":{\"131\":1}}],[\"图像注意力掩码\",{\"1\":{\"286\":1}}],[\"图像通过视觉编码器\",{\"1\":{\"285\":1}}],[\"图像对是从互联网收集的\",{\"1\":{\"278\":1}}],[\"图像对为负样本\",{\"1\":{\"272\":1}}],[\"图像对的相似度\",{\"1\":{\"272\":1}}],[\"图像对的训练batch\",{\"1\":{\"272\":1}}],[\"图像对的预训练方法\",{\"1\":{\"271\":1}}],[\"图像对\",{\"1\":{\"271\":1}}],[\"图像掩码\",{\"1\":{\"262\":1}}],[\"图像模态特征\",{\"1\":{\"262\":1}}],[\"图像的索引\",{\"1\":{\"289\":1}}],[\"图像的更重\",{\"1\":{\"255\":1}}],[\"图像的文字描述\",{\"1\":{\"226\":1}}],[\"图像调整至目标分辨率\",{\"1\":{\"216\":1}}],[\"图像字幕\",{\"1\":{\"195\":1}}],[\"图像→文本检索任务中达到\",{\"1\":{\"194\":1}}],[\"图像有两种表示视图\",{\"1\":{\"168\":1}}],[\"图像表示\",{\"0\":{\"168\":1}}],[\"图像块嵌入层\",{\"1\":{\"292\":1}}],[\"图像块的尺寸\",{\"1\":{\"292\":1}}],[\"图像块保留了原始像素\",{\"1\":{\"169\":1}}],[\"图像块没有现成的词汇表\",{\"1\":{\"166\":1}}],[\"图像块\",{\"0\":{\"169\":1},\"1\":{\"165\":1,\"166\":1,\"168\":1,\"169\":1,\"172\":1}}],[\"图像部分的\",{\"1\":{\"163\":1}}],[\"图像部分主要来源于\",{\"1\":{\"17\":1}}],[\"图像全局语义\",{\"1\":{\"161\":1}}],[\"图像动量编码器\",{\"1\":{\"147\":1}}],[\"图像编码阶段\",{\"1\":{\"286\":1}}],[\"图像编码\",{\"1\":{\"147\":1}}],[\"图像编码器采用了\",{\"1\":{\"275\":1}}],[\"图像编码器采用了预训练的\",{\"1\":{\"152\":1}}],[\"图像编码器初始\",{\"1\":{\"200\":1}}],[\"图像编码器输出\",{\"1\":{\"163\":1}}],[\"图像编码器基于在\",{\"1\":{\"131\":1}}],[\"图像编码器\",{\"1\":{\"40\":1,\"152\":1,\"191\":1,\"200\":1}}],[\"图像与文本特征提取\",{\"1\":{\"147\":1}}],[\"图像与点云之间不需要一一对应\",{\"1\":{\"19\":1}}],[\"图像队列\",{\"1\":{\"147\":1}}],[\"图像引导的文本解码器\",{\"1\":{\"126\":1}}],[\"图像引导的文本编码器\",{\"1\":{\"126\":1}}],[\"图像和声音\",{\"1\":{\"675\":1}}],[\"图像和音频\",{\"1\":{\"210\":1}}],[\"图像和文本特征空间不一致\",{\"1\":{\"149\":1}}],[\"图像和文本分别编码\",{\"1\":{\"126\":1}}],[\"图像和点云\",{\"1\":{\"59\":1}}],[\"图像任务中数据增强广泛应用\",{\"1\":{\"124\":1}}],[\"图像描述生成的引导提示词\",{\"1\":{\"142\":1}}],[\"图像描述\",{\"1\":{\"120\":1,\"198\":1,\"217\":1,\"226\":2}}],[\"图像条件语言建模\",{\"1\":{\"120\":1}}],[\"图像条件解码器三种模式\",{\"1\":{\"120\":1}}],[\"图像条件编码器\",{\"1\":{\"120\":1}}],[\"图像处理或其他数据集中用于抽样的算法\",{\"1\":{\"89\":1}}],[\"图像分辨率\",{\"1\":{\"208\":1}}],[\"图像分类\",{\"1\":{\"175\":1,\"193\":1,\"395\":1}}],[\"图像分词器\",{\"1\":{\"170\":1}}],[\"图像分割为\",{\"1\":{\"169\":1,\"219\":1}}],[\"图像分割为一系列块序列\",{\"1\":{\"169\":1}}],[\"图像分割等任务\",{\"1\":{\"82\":1}}],[\"图像分支依赖目标检测器\",{\"1\":{\"280\":1}}],[\"图像分支\",{\"1\":{\"54\":1,\"280\":1}}],[\"图像网格特征\",{\"1\":{\"59\":1}}],[\"图像交互信息与点云特征做融合\",{\"1\":{\"35\":1}}],[\"图像样本\",{\"1\":{\"29\":1}}],[\"图像所属的物体名\",{\"1\":{\"28\":1}}],[\"图像\",{\"0\":{\"171\":1},\"1\":{\"22\":1,\"114\":1,\"145\":1,\"162\":1,\"163\":1,\"180\":1,\"190\":4,\"200\":1,\"227\":1,\"282\":1,\"291\":1}}],[\"图像融合两阶段3d检测框架\",{\"1\":{\"22\":1}}],[\"图像按可供性类别进行分类\",{\"1\":{\"18\":1}}],[\"图像数\",{\"1\":{\"17\":1}}],[\"图像特征和掩码\",{\"1\":{\"286\":1}}],[\"图像特征提取和模态融合都很重\",{\"1\":{\"280\":1}}],[\"图像特征提取和投影\",{\"1\":{\"145\":1}}],[\"图像特征提取与分类\",{\"1\":{\"273\":1}}],[\"图像特征输入部分\",{\"1\":{\"257\":1}}],[\"图像特征作为cross\",{\"1\":{\"286\":1}}],[\"图像特征作为\",{\"1\":{\"162\":1,\"267\":1}}],[\"图像特征与所有文本特征做内积\",{\"1\":{\"161\":1}}],[\"图像特征队列\",{\"1\":{\"160\":1}}],[\"图像特征\",{\"1\":{\"14\":1,\"160\":1}}],[\"图3\",{\"1\":{\"6\":1,\"17\":1,\"19\":1,\"189\":1,\"423\":1}}],[\"图1中显示的不同模型在人类偏好评估中的胜率清晰反映了该方法的有效性\",{\"1\":{\"468\":1}}],[\"图1\",{\"1\":{\"6\":2,\"256\":1,\"423\":1}}],[\"×\",{\"1\":{\"63\":1,\"76\":3,\"396\":1,\"404\":2,\"407\":4,\"569\":1,\"592\":1}}],[\"×5\",{\"1\":{\"6\":1}}],[\"×3\",{\"1\":{\"6\":1}}],[\"对大模型能力具有极大影响\",{\"1\":{\"687\":1}}],[\"对大模型进行微调\",{\"1\":{\"416\":1}}],[\"对大模型进行训练\",{\"1\":{\"416\":1}}],[\"对流式处理进行了深度优化\",{\"1\":{\"684\":1}}],[\"对检索到的信息进行处理和增强\",{\"1\":{\"680\":1}}],[\"对原始数据进行清洗和处理\",{\"1\":{\"680\":1}}],[\"对原始点云做刚性变换\",{\"1\":{\"108\":1}}],[\"对语言有了更深刻的理解\",{\"1\":{\"673\":1}}],[\"对语言指令进行分词\",{\"1\":{\"40\":1}}],[\"对p\",{\"1\":{\"600\":1}}],[\"对各种可能世界状态\",{\"1\":{\"597\":1}}],[\"对任意事件\",{\"1\":{\"567\":1}}],[\"对任意行\",{\"1\":{\"327\":1}}],[\"对三面骰子设\",{\"1\":{\"565\":1}}],[\"对外提供的编码和解码两个方法实现如下\",{\"1\":{\"511\":1}}],[\"对列表数据进行解析\",{\"1\":{\"511\":1}}],[\"对下游任务\",{\"1\":{\"493\":1}}],[\"对非二元代词\",{\"1\":{\"484\":1}}],[\"对非平滑函数的适应性\",{\"1\":{\"395\":1}}],[\"对抗性脆弱\",{\"1\":{\"472\":1}}],[\"对荒谬命令未进行识别\",{\"1\":{\"471\":1}}],[\"对指令遵循度高\",{\"1\":{\"471\":1}}],[\"对最终模型进行评估\",{\"1\":{\"470\":1}}],[\"对模型输出进行偏好排序\",{\"1\":{\"470\":1}}],[\"对模型已经分类正确的样本\",{\"1\":{\"404\":1}}],[\"对训练数据进行了\",{\"1\":{\"470\":1}}],[\"对性能影响有限\",{\"1\":{\"492\":1}}],[\"对性能影响\",{\"1\":{\"455\":1}}],[\"对同一个大模型的微调\",{\"1\":{\"416\":1}}],[\"对当前词的子词进行合并\",{\"1\":{\"411\":1}}],[\"对当前句子中每个词进行子词合并加词id映射\",{\"1\":{\"411\":1}}],[\"对经过预处理的vocab中的每个词按空格进行切分\",{\"1\":{\"410\":1}}],[\"对经过注意力层的输出进行归一化处理\",{\"1\":{\"294\":1}}],[\"对损失求均值\",{\"1\":{\"407\":1}}],[\"对假阴性\",{\"1\":{\"407\":1}}],[\"对假阳性\",{\"1\":{\"407\":1}}],[\"对类别不平衡问题鲁棒\",{\"1\":{\"407\":1}}],[\"对类别不平衡不敏感\",{\"1\":{\"401\":1,\"403\":1}}],[\"对噪声标签敏感\",{\"1\":{\"404\":1}}],[\"对噪声点敏感\",{\"1\":{\"112\":2}}],[\"对分类错误的样本\",{\"1\":{\"404\":1}}],[\"对细节更敏感\",{\"1\":{\"402\":1}}],[\"对单个点的分类精度不够敏感\",{\"1\":{\"402\":1}}],[\"对前景响应弱\",{\"1\":{\"402\":1}}],[\"对采样点的值进行聚合\",{\"1\":{\"397\":1}}],[\"对高维和非平滑函数更鲁棒\",{\"1\":{\"395\":1}}],[\"对其进行加工\",{\"1\":{\"368\":1}}],[\"对不同方向的偏差做缩放和正交旋转\",{\"1\":{\"359\":1}}],[\"对不同阈值计算\",{\"1\":{\"82\":1}}],[\"对两个张量\",{\"1\":{\"327\":1}}],[\"对两个特征进行线性投射\",{\"1\":{\"272\":1}}],[\"对一个张量\",{\"1\":{\"325\":1}}],[\"对特征进行更深入的建模\",{\"1\":{\"299\":1}}],[\"对特征空间进行变换\",{\"1\":{\"109\":1}}],[\"对特征空间做变换\",{\"1\":{\"105\":1,\"108\":1}}],[\"对投影后的结果应用丢弃层\",{\"1\":{\"295\":1}}],[\"对输出有了更加明确具体的要求\",{\"1\":{\"432\":1}}],[\"对输出应用\",{\"1\":{\"405\":1}}],[\"对输出进行维度交换和形状调整\",{\"1\":{\"295\":1}}],[\"对输入的文本进行断句加分词\",{\"1\":{\"411\":1}}],[\"对输入的点云做刚性变换\",{\"1\":{\"107\":1}}],[\"对输入数据进行初步的特征提取\",{\"1\":{\"299\":1}}],[\"对输入进行归一化处理\",{\"1\":{\"294\":1}}],[\"对输入进行非线性变换\",{\"1\":{\"268\":1}}],[\"对输入点云做刚性变换\",{\"1\":{\"105\":1}}],[\"对输入点云中的每个点进行分类\",{\"1\":{\"101\":1}}],[\"对输入点进行采样\",{\"1\":{\"87\":1}}],[\"对输入特征进行非线性映射\",{\"1\":{\"59\":1}}],[\"对注意力权重矩阵应用丢弃层\",{\"1\":{\"295\":1}}],[\"对注意力分数矩阵应用softmax函数\",{\"1\":{\"295\":1}}],[\"对处理后的张量进行归一化操作\",{\"1\":{\"291\":1}}],[\"对验证集的处理方式是先resize成256x256的图片\",{\"1\":{\"290\":1}}],[\"对数据的处理和操作要求极低\",{\"1\":{\"681\":1}}],[\"对数据集和验证集划分之后\",{\"1\":{\"290\":1}}],[\"对数值变化敏感\",{\"1\":{\"115\":1}}],[\"对称的\",{\"1\":{\"355\":1}}],[\"对称的对比学习损失\",{\"1\":{\"272\":1}}],[\"对称函数\",{\"0\":{\"115\":1},\"1\":{\"115\":2}}],[\"对整张图像进行处理\",{\"1\":{\"253\":1}}],[\"对这个图片做随机裁剪\",{\"1\":{\"235\":1}}],[\"对微调模型\",{\"1\":{\"231\":1}}],[\"对话即平台\",{\"1\":{\"678\":1}}],[\"对话模型\",{\"1\":{\"674\":1,\"683\":1}}],[\"对话模式\",{\"1\":{\"188\":1}}],[\"对话系统中的候选回复选择\",{\"1\":{\"544\":1}}],[\"对话\",{\"1\":{\"470\":1}}],[\"对话型问答为多轮对话\",{\"1\":{\"227\":1}}],[\"对话型\",{\"1\":{\"227\":1}}],[\"对被\",{\"1\":{\"163\":1}}],[\"对角协方差矩阵\",{\"1\":{\"590\":1}}],[\"对角线\",{\"1\":{\"355\":1}}],[\"对角线上的元素\",{\"1\":{\"355\":1}}],[\"对角线元素的labels\",{\"1\":{\"272\":1}}],[\"对角线为正例\",{\"1\":{\"147\":1}}],[\"对角为1\",{\"1\":{\"161\":1}}],[\"对角为正样本\",{\"1\":{\"161\":1}}],[\"对主编码器做\",{\"1\":{\"161\":1}}],[\"对文本进行编码\",{\"1\":{\"161\":1}}],[\"对多个大规模噪声网页图文数据集\",{\"1\":{\"140\":1}}],[\"对异常点鲁棒性差\",{\"1\":{\"112\":1}}],[\"对异常点也有一定容忍能力\",{\"1\":{\"105\":1}}],[\"对少量异常点有一定鲁棒性\",{\"1\":{\"112\":1}}],[\"对稀疏点云敏感\",{\"1\":{\"112\":1}}],[\"对局部形状变化敏感\",{\"1\":{\"112\":1}}],[\"对局部点云组做最大池化或平均池化\",{\"1\":{\"98\":1}}],[\"对局部点云进行变换\",{\"1\":{\"98\":1}}],[\"对几何变换的不变性\",{\"1\":{\"104\":1,\"105\":1}}],[\"对参数选择依赖性高\",{\"1\":{\"90\":1}}],[\"对上述得到的每个区域进行编码\",{\"1\":{\"87\":1}}],[\"对所有历史\",{\"1\":{\"477\":1}}],[\"对所有\",{\"1\":{\"82\":1}}],[\"对所有点\",{\"1\":{\"73\":1}}],[\"对边界敏感\",{\"1\":{\"82\":1}}],[\"对边界模糊区域友好\",{\"1\":{\"82\":1}}],[\"对边界模糊区域不敏感\",{\"1\":{\"82\":1}}],[\"对affordance\",{\"1\":{\"76\":1}}],[\"对无效\",{\"1\":{\"76\":1}}],[\"对象可被立即回收\",{\"1\":{\"657\":1}}],[\"对象方法工厂\",{\"1\":{\"378\":1}}],[\"对象方法装饰器\",{\"1\":{\"378\":1}}],[\"对象功能区域分割\",{\"1\":{\"70\":1}}],[\"对象的交互区域\",{\"1\":{\"28\":1}}],[\"对每一个选项\",{\"1\":{\"544\":1}}],[\"对每一行做\",{\"1\":{\"308\":1}}],[\"对每种组合手工编写\",{\"1\":{\"63\":1}}],[\"对每个选项分别进行编码\",{\"1\":{\"544\":1}}],[\"对每个句对构建用于mlm任务的样本\",{\"1\":{\"512\":1}}],[\"对每个句对构建用于nsp任务的样本\",{\"1\":{\"512\":1}}],[\"对每个句子随机选择15\",{\"1\":{\"495\":1}}],[\"对每个句子进行分词\",{\"1\":{\"411\":1}}],[\"对每个句子独立进行编码\",{\"1\":{\"31\":1}}],[\"对每个词进行子词合并\",{\"1\":{\"411\":2}}],[\"对每个小块区域做\",{\"1\":{\"396\":1}}],[\"对每个维度根据尺度差异进行惩罚调整\",{\"1\":{\"359\":1}}],[\"对每个原始点\",{\"1\":{\"100\":1}}],[\"对每个尺度的局部点集应用对应的\",{\"1\":{\"96\":1}}],[\"对每个半径\",{\"1\":{\"96\":1}}],[\"对每个局部区域内所有点的最大响应值进行池化\",{\"1\":{\"92\":1}}],[\"对每个查询点的邻近点按索引排序\",{\"1\":{\"92\":1}}],[\"对每个点单独计算分类误差\",{\"1\":{\"402\":1}}],[\"对每个点独立处理\",{\"1\":{\"112\":1}}],[\"对每个点进行特征提取\",{\"1\":{\"109\":1}}],[\"对每个点取预测值和真实值中的较小者\",{\"1\":{\"82\":1}}],[\"对每个点的3个权重求和\",{\"1\":{\"100\":1}}],[\"对每个点的特征做一个简单的分类器\",{\"1\":{\"98\":1}}],[\"对每个点的响应值\",{\"1\":{\"76\":1}}],[\"对每个点的关注程度\",{\"1\":{\"76\":1}}],[\"对每个点的关注响应\",{\"1\":{\"76\":1}}],[\"对每个\",{\"1\":{\"73\":1,\"390\":1,\"396\":1,\"397\":1,\"542\":1,\"674\":1}}],[\"对每个样本单独处理\",{\"1\":{\"43\":1}}],[\"对交互主体框位置进行精细调整\",{\"1\":{\"59\":1}}],[\"对交互文本和几何结构文本进行编码这块\",{\"1\":{\"31\":1}}],[\"对交互文本和几何结构文本进行编码\",{\"1\":{\"30\":1}}],[\"对图像进行归一化处理\",{\"1\":{\"290\":2}}],[\"对图像进行编码\",{\"1\":{\"161\":1}}],[\"对图像和文本独立使用encoder\",{\"1\":{\"255\":1}}],[\"对图像和点云特征进行\",{\"1\":{\"41\":2}}],[\"对图片进行缩放\",{\"1\":{\"58\":1}}],[\"对齐本身可被滥用\",{\"1\":{\"472\":1}}],[\"对齐方法以处理价值多样性\",{\"1\":{\"472\":1}}],[\"对齐语言模型所需的计算成本极低\",{\"1\":{\"472\":1}}],[\"对齐程度\",{\"1\":{\"470\":1}}],[\"对齐损失\",{\"1\":{\"469\":1}}],[\"对齐问题\",{\"1\":{\"468\":1}}],[\"对齐图文特征空间\",{\"1\":{\"149\":1}}],[\"对齐\",{\"1\":{\"107\":1,\"472\":1}}],[\"对齐区域\",{\"1\":{\"49\":1}}],[\"对齐模糊性\",{\"1\":{\"49\":1}}],[\"对齐二者\",{\"1\":{\"13\":1}}],[\"对\",{\"1\":{\"46\":1,\"54\":1,\"67\":1,\"76\":1,\"82\":1,\"143\":1,\"163\":1,\"231\":1,\"428\":1,\"454\":1,\"470\":1,\"471\":2,\"508\":1,\"596\":2}}],[\"对点数维度做\",{\"1\":{\"46\":1}}],[\"对点积结果进行缩放\",{\"1\":{\"41\":1}}],[\"对点云的旋转\",{\"1\":{\"105\":1}}],[\"对点云进行下采样\",{\"1\":{\"98\":1}}],[\"对点云进行编码\",{\"1\":{\"30\":1}}],[\"对点云密度变换较为敏感\",{\"1\":{\"90\":1}}],[\"对点云数据做平移操作后\",{\"1\":{\"86\":1}}],[\"对点云数据进行转置操作\",{\"1\":{\"68\":1}}],[\"对点云数据进行归一化处理\",{\"1\":{\"68\":1}}],[\"对点云特征\",{\"1\":{\"14\":1}}],[\"对自然语言指令进行\",{\"1\":{\"40\":1}}],[\"对应于头两个实验的每一种实验结果\",{\"1\":{\"599\":1}}],[\"对应于第一个实验的每一种实验结果\",{\"1\":{\"599\":1}}],[\"对应代码如下\",{\"1\":{\"519\":1}}],[\"对应着标签\",{\"1\":{\"444\":1}}],[\"对应所有\",{\"1\":{\"307\":1}}],[\"对应维度为\",{\"1\":{\"292\":1}}],[\"对应\",{\"1\":{\"291\":1}}],[\"对应论文图1中的视觉transformer\",{\"1\":{\"142\":1}}],[\"对应论文3\",{\"1\":{\"142\":2}}],[\"对应一个形状为\",{\"1\":{\"326\":1}}],[\"对应一个\",{\"1\":{\"107\":1}}],[\"对应一组相关的点特征\",{\"1\":{\"72\":1}}],[\"对应半径下最多取多少邻近点\",{\"1\":{\"96\":1}}],[\"对应指标\",{\"1\":{\"82\":1}}],[\"对应的id为\",{\"1\":{\"520\":1}}],[\"对应的参数为\",{\"1\":{\"519\":1}}],[\"对应的输出概率最大\",{\"1\":{\"508\":1}}],[\"对应的内存布局为一维数组\",{\"1\":{\"325\":1}}],[\"对应的伪代码实现如下所示\",{\"1\":{\"272\":1}}],[\"对应的蒸馏损失定义为\",{\"1\":{\"157\":1}}],[\"对应的\",{\"1\":{\"76\":1,\"142\":1}}],[\"对应的注意力掩码\",{\"1\":{\"43\":1}}],[\"对应的几何属性\",{\"1\":{\"28\":1}}],[\"对应多少个\",{\"1\":{\"29\":1}}],[\"对比不同输入格式\",{\"1\":{\"492\":1,\"493\":1}}],[\"对比静态掩码\",{\"1\":{\"492\":1}}],[\"对比原始架构\",{\"1\":{\"481\":1}}],[\"对比之前的方法\",{\"1\":{\"439\":1}}],[\"对比其他损失函数\",{\"1\":{\"407\":1}}],[\"对比维度\",{\"1\":{\"231\":1}}],[\"对比\",{\"1\":{\"197\":1,\"198\":1,\"542\":1}}],[\"对比任务\",{\"1\":{\"189\":1}}],[\"对比模式\",{\"1\":{\"188\":2}}],[\"对比学习的过程就是想要在特征空间里\",{\"1\":{\"238\":1}}],[\"对比学习从2019年开始到现在一直都比较火\",{\"1\":{\"233\":1}}],[\"对比学习和自蒸馏等方法已被探索\",{\"1\":{\"166\":1}}],[\"对比学习损失\",{\"1\":{\"147\":1}}],[\"对比学习中的队列长度\",{\"1\":{\"160\":1}}],[\"对比学习中的动量蒸馏\",{\"1\":{\"157\":1}}],[\"对比学习中的负样本缓存\",{\"1\":{\"147\":1}}],[\"对比学习中图文特征队列长度\",{\"1\":{\"147\":1}}],[\"对比学习温度参数\",{\"1\":{\"145\":1}}],[\"对比损失\",{\"1\":{\"145\":1}}],[\"对比方法\",{\"1\":{\"22\":1}}],[\"对比见表1\",{\"1\":{\"6\":1}}],[\"对于个体开发者或小型开发团队来说\",{\"1\":{\"687\":1}}],[\"对于个体开发者或小型开发团队而言\",{\"1\":{\"687\":1}}],[\"对于个人使用者而言\",{\"1\":{\"415\":1}}],[\"对于形状复杂的函数\",{\"1\":{\"667\":1}}],[\"对于不满足交换律的运算符\",{\"1\":{\"660\":1}}],[\"对于共享变量\",{\"1\":{\"655\":1}}],[\"对于函数\",{\"1\":{\"630\":2}}],[\"对于y=f2\",{\"1\":{\"626\":1}}],[\"对于复合函数\",{\"1\":{\"625\":1}}],[\"对于连续型随机变量\",{\"1\":{\"588\":1}}],[\"对于任意两个满足\",{\"1\":{\"570\":1}}],[\"对于任意两个事件\",{\"1\":{\"567\":1}}],[\"对于任意一列两两互不相交\",{\"1\":{\"567\":1}}],[\"对于事件\",{\"1\":{\"566\":1}}],[\"对于这样的多选问题\",{\"1\":{\"544\":1}}],[\"对于这些任务\",{\"1\":{\"445\":1}}],[\"对于分类任务来说\",{\"1\":{\"529\":1}}],[\"对于字典中不存在的词\",{\"1\":{\"520\":1}}],[\"对于mlm任务损失计算来说\",{\"1\":{\"514\":2}}],[\"对于所有掩码候选位置执行掩码策略\",{\"1\":{\"511\":1}}],[\"对于生成类任务\",{\"1\":{\"461\":1}}],[\"对于没有训练集的数据集\",{\"1\":{\"461\":1}}],[\"对于dprd\",{\"1\":{\"449\":1}}],[\"对于race\",{\"1\":{\"449\":1}}],[\"对于sst\",{\"1\":{\"449\":1}}],[\"对于cola\",{\"1\":{\"449\":1}}],[\"对于相似任务\",{\"1\":{\"445\":1}}],[\"对于作者的模型架构\",{\"1\":{\"440\":1}}],[\"对于将这些学习到的表征迁移到目标任务的最有效方法\",{\"1\":{\"440\":1}}],[\"对于llm越友好\",{\"1\":{\"432\":1}}],[\"对于一些复杂的问题\",{\"1\":{\"430\":1}}],[\"对于一般的任务\",{\"1\":{\"427\":1}}],[\"对于一个连续型随机变量\",{\"1\":{\"572\":1}}],[\"对于一个具有\",{\"1\":{\"571\":1}}],[\"对于一个样本\",{\"1\":{\"404\":1}}],[\"对于一个二维矩阵\",{\"1\":{\"326\":1}}],[\"对于一个包含个文本\",{\"1\":{\"272\":1}}],[\"对于需要微调的密集层\",{\"1\":{\"423\":1}}],[\"对于具有线性输出层和至少一个使用\",{\"1\":{\"395\":1}}],[\"对于二元分类器\",{\"1\":{\"351\":1}}],[\"对于二维的图像\",{\"1\":{\"291\":1}}],[\"对于类别不平衡的数据集\",{\"1\":{\"348\":1,\"408\":1}}],[\"对于模型效果\",{\"1\":{\"347\":1}}],[\"对于疾病预测等应用\",{\"1\":{\"344\":1}}],[\"对于严重不均衡的数据集\",{\"1\":{\"343\":1}}],[\"对于更高维的张量\",{\"1\":{\"326\":1}}],[\"对于上述代码中的矩阵\",{\"1\":{\"322\":2}}],[\"对于q\",{\"1\":{\"282\":1}}],[\"对于自监督模型\",{\"1\":{\"278\":1}}],[\"对于有监督模型\",{\"1\":{\"278\":1}}],[\"对于vit\",{\"1\":{\"272\":1}}],[\"对于\",{\"1\":{\"242\":2,\"426\":1}}],[\"对于nlp领域来说\",{\"1\":{\"238\":1}}],[\"对于imagenet这个数据集来说\",{\"1\":{\"235\":1}}],[\"对于频率大于\",{\"1\":{\"226\":1}}],[\"对于同一个指令\",{\"1\":{\"224\":1}}],[\"对于下游任务\",{\"1\":{\"167\":1}}],[\"对于非刚性变形\",{\"1\":{\"112\":1}}],[\"对于多选题\",{\"1\":{\"461\":1}}],[\"对于多标签\",{\"1\":{\"98\":1}}],[\"对于多分类\",{\"1\":{\"98\":1}}],[\"对于某个形心\",{\"1\":{\"90\":1}}],[\"对于单个的物体还好\",{\"1\":{\"86\":1}}],[\"对于每个图像和文本\",{\"1\":{\"154\":1}}],[\"对于每个局部区域\",{\"1\":{\"96\":1}}],[\"对于每个质心点\",{\"1\":{\"95\":1}}],[\"对于每个选中的关键点\",{\"1\":{\"92\":1}}],[\"对于每个问题\",{\"1\":{\"64\":1}}],[\"对于每一个子业务训练优化模型\",{\"1\":{\"686\":1}}],[\"对于每一个子业务构造训练数据与验证数据\",{\"1\":{\"686\":1}}],[\"对于每一个输出位置\",{\"1\":{\"514\":2}}],[\"对于每一个\",{\"1\":{\"76\":1}}],[\"对于点云中的每一个点\",{\"1\":{\"41\":1}}],[\"对于点云实例\",{\"1\":{\"18\":1}}],[\"对于图像中的每一个位置\",{\"1\":{\"41\":1}}],[\"对于图像\",{\"1\":{\"18\":1}}],[\"对机器人感知与操作至关重要\",{\"1\":{\"6\":1}}],[\"和长文本推理\",{\"1\":{\"674\":1}}],[\"和长程依赖任务\",{\"1\":{\"455\":1}}],[\"和指令微调版\",{\"1\":{\"674\":1}}],[\"和在线推理阶段的\",{\"1\":{\"673\":1}}],[\"和funcs\",{\"1\":{\"666\":1}}],[\"和函数节点\",{\"1\":{\"666\":1}}],[\"和variable\",{\"1\":{\"660\":1}}],[\"和一个问题\",{\"1\":{\"508\":1}}],[\"和一个可能答案集\",{\"1\":{\"445\":1}}],[\"和xlnet\",{\"1\":{\"499\":1}}],[\"和下游任务准确率\",{\"1\":{\"497\":1}}],[\"和衰减策略\",{\"1\":{\"493\":1}}],[\"和bloom\",{\"1\":{\"485\":1}}],[\"和bert\",{\"1\":{\"455\":1}}],[\"和人文任务\",{\"1\":{\"483\":1}}],[\"和palm\",{\"1\":{\"482\":1}}],[\"和mbpp\",{\"1\":{\"482\":1}}],[\"和mlp比率\",{\"1\":{\"188\":1}}],[\"和工程创新\",{\"1\":{\"481\":1}}],[\"和书籍\",{\"1\":{\"481\":1}}],[\"和质量过滤\",{\"1\":{\"481\":1}}],[\"和数学推理\",{\"1\":{\"480\":1}}],[\"和位于词序列的索引\",{\"1\":{\"477\":1}}],[\"和聊天合计约\",{\"1\":{\"470\":1}}],[\"和少样本\",{\"1\":{\"460\":1}}],[\"和假设\",{\"1\":{\"445\":1}}],[\"和假阴性\",{\"1\":{\"405\":1}}],[\"和假正例率\",{\"1\":{\"350\":1}}],[\"和负类\",{\"1\":{\"404\":1}}],[\"和模块化设计\",{\"1\":{\"395\":1}}],[\"和保存\",{\"1\":{\"389\":1}}],[\"和特征间的协方差\",{\"1\":{\"359\":1}}],[\"和这些曲线下的面积可以更好地直观比较模型性能\",{\"1\":{\"352\":1}}],[\"和所有真实负例\",{\"1\":{\"342\":1}}],[\"和所有预测负例\",{\"1\":{\"342\":1}}],[\"和步长\",{\"1\":{\"321\":1}}],[\"和transformer中的一样\",{\"1\":{\"294\":1}}],[\"和text都能和所有的tokens\",{\"1\":{\"284\":1}}],[\"和冻结参数的\",{\"1\":{\"286\":1}}],[\"和文本分类\",{\"1\":{\"440\":1}}],[\"和文本\",{\"1\":{\"285\":1}}],[\"和文本编码器\",{\"1\":{\"190\":1}}],[\"和基于图像掩码的方法\",{\"1\":{\"278\":1}}],[\"和我真实输出做一个损失\",{\"1\":{\"240\":1}}],[\"和当前时刻的\",{\"1\":{\"238\":1}}],[\"和当前输出的\",{\"1\":{\"163\":1}}],[\"和队列大小分离开\",{\"1\":{\"238\":1}}],[\"和语言模型\",{\"1\":{\"227\":1}}],[\"和对应的\",{\"1\":{\"224\":1}}],[\"和中文理解方面表现突出\",{\"1\":{\"222\":1}}],[\"和中文相关任务中的表现\",{\"1\":{\"207\":1}}],[\"和高质量双语数据集\",{\"1\":{\"222\":1}}],[\"和类别\",{\"1\":{\"204\":1}}],[\"和其他多语言模型\",{\"1\":{\"194\":1}}],[\"和其对应的\",{\"1\":{\"8\":1}}],[\"和跨注意力层\",{\"1\":{\"191\":1}}],[\"和生成任务\",{\"1\":{\"188\":1}}],[\"和双塔模型\",{\"1\":{\"188\":1}}],[\"和80亿参数的语言中间件\",{\"1\":{\"181\":1}}],[\"和辅助损失函数进一步优化了生成质量\",{\"1\":{\"178\":1}}],[\"和视觉标记\",{\"1\":{\"166\":1,\"168\":1}}],[\"和离散视觉标记\",{\"1\":{\"165\":1}}],[\"和被\",{\"1\":{\"157\":1}}],[\"和图\",{\"1\":{\"462\":3}}],[\"和图像编码器\",{\"1\":{\"272\":1}}],[\"和图像分类任务\",{\"1\":{\"189\":1}}],[\"和图像特征\",{\"1\":{\"6\":1}}],[\"和图文匹配\",{\"1\":{\"153\":1}}],[\"和第\",{\"1\":{\"134\":1}}],[\"和稀疏采样的区域\",{\"1\":{\"95\":1}}],[\"和多分辨率分组\",{\"1\":{\"94\":1}}],[\"和每个查询点上\",{\"1\":{\"92\":1}}],[\"和点维度的平均\",{\"1\":{\"78\":1}}],[\"和点云特征\",{\"1\":{\"15\":1}}],[\"和通道混合\",{\"1\":{\"73\":1}}],[\"和通道维度\",{\"1\":{\"45\":1}}],[\"和值\",{\"1\":{\"72\":1,\"295\":2,\"674\":2}}],[\"和物体\",{\"1\":{\"56\":1}}],[\"和联合注意力\",{\"1\":{\"56\":1}}],[\"和arm模块\",{\"1\":{\"49\":1}}],[\"和自注意力机制融合图像与点云特征\",{\"1\":{\"40\":1}}],[\"和自然语言理解出发\",{\"1\":{\"7\":1}}],[\"和知识特征\",{\"1\":{\"14\":1}}],[\"和可供性意图特征\",{\"1\":{\"8\":1}}],[\"和\",{\"0\":{\"231\":1,\"317\":1,\"353\":1},\"1\":{\"7\":1,\"8\":2,\"32\":1,\"40\":3,\"42\":1,\"45\":5,\"46\":1,\"54\":1,\"68\":1,\"70\":1,\"82\":3,\"88\":1,\"107\":1,\"108\":1,\"126\":3,\"128\":1,\"131\":4,\"138\":1,\"140\":2,\"142\":1,\"146\":1,\"147\":1,\"154\":2,\"157\":1,\"162\":1,\"163\":5,\"190\":1,\"191\":2,\"194\":1,\"195\":1,\"196\":1,\"201\":1,\"203\":2,\"212\":2,\"217\":1,\"219\":1,\"228\":1,\"231\":1,\"239\":2,\"242\":1,\"264\":1,\"268\":1,\"270\":3,\"280\":3,\"283\":3,\"285\":2,\"291\":1,\"299\":2,\"305\":1,\"327\":1,\"350\":1,\"351\":1,\"352\":1,\"353\":3,\"355\":2,\"370\":1,\"383\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":2,\"407\":2,\"409\":1,\"410\":1,\"423\":5,\"425\":1,\"433\":1,\"462\":1,\"469\":1,\"470\":4,\"471\":4,\"477\":3,\"492\":2,\"493\":1,\"494\":1,\"506\":1,\"508\":1,\"510\":2,\"513\":2,\"517\":1,\"538\":1,\"540\":2,\"541\":2,\"542\":1,\"553\":1,\"567\":1,\"568\":2,\"570\":1,\"652\":1,\"660\":3,\"673\":4,\"674\":11,\"681\":2,\"684\":1}}],[\"和38k\",{\"1\":{\"6\":1}}],[\"和同一物体的多交互意图关联\",{\"1\":{\"6\":1}}],[\"注重解题中间步骤的正确性\",{\"1\":{\"674\":1}}],[\"注\",{\"1\":{\"334\":1}}],[\"注解\",{\"1\":{\"240\":1,\"372\":1}}],[\"注释和计算开销大\",{\"1\":{\"149\":1}}],[\"注入多样的合成描述\",{\"1\":{\"138\":1}}],[\"注入点云\",{\"1\":{\"6\":1}}],[\"注意事项\",{\"1\":{\"542\":1}}],[\"注意是通过一个线性层来同时计算qkv三个矩阵\",{\"1\":{\"295\":1}}],[\"注意下面的embed\",{\"1\":{\"291\":1}}],[\"注意一张图像的视觉标记数量与图像块数量相同\",{\"1\":{\"170\":1}}],[\"注意力权重\",{\"1\":{\"309\":1,\"312\":1}}],[\"注意力可视化\",{\"0\":{\"298\":1}}],[\"注意力矩阵的丢弃率\",{\"1\":{\"295\":1}}],[\"注意力头的数量\",{\"1\":{\"295\":1}}],[\"注意力汇聚\",{\"1\":{\"292\":1}}],[\"注意力机制的基本流程\",{\"0\":{\"305\":1}}],[\"注意力机制的特性\",{\"1\":{\"292\":1}}],[\"注意力机制\",{\"1\":{\"178\":1}}],[\"注意力机制让每个点从融合特征中提取相关信息\",{\"1\":{\"74\":1}}],[\"注意力机制使得每个文本\",{\"1\":{\"72\":1}}],[\"注意力后\",{\"1\":{\"36\":1}}],[\"注意\",{\"0\":{\"428\":1},\"1\":{\"29\":1,\"64\":1,\"163\":2,\"295\":1,\"355\":1,\"381\":1,\"401\":1,\"402\":1,\"407\":1,\"586\":1}}],[\"注水\",{\"1\":{\"6\":2}}],[\"并针对性分析\",{\"1\":{\"687\":1}}],[\"并针对性改进\",{\"1\":{\"687\":1}}],[\"并带有监控和日志功能\",{\"1\":{\"685\":1}}],[\"并以此为基础\",{\"1\":{\"679\":1}}],[\"并于\",{\"1\":{\"674\":1}}],[\"并进一步提升了代码质量和多轮对话一致性\",{\"1\":{\"674\":1}}],[\"并进行l2归一化\",{\"1\":{\"272\":1}}],[\"并进行视觉对话的\",{\"1\":{\"227\":1}}],[\"并改进了工具调用和多模态能力\",{\"1\":{\"674\":1}}],[\"并改造为a\",{\"1\":{\"275\":1}}],[\"并充当通用任务求解器\",{\"1\":{\"674\":1}}],[\"并支持在jupyter\",{\"1\":{\"666\":1}}],[\"并保存为文件\",{\"1\":{\"666\":1}}],[\"并标注\",{\"1\":{\"659\":1}}],[\"并为后续的功能扩展奠定了坚实基础\",{\"1\":{\"648\":1}}],[\"并为后续阶段提供稳健的视觉表示\",{\"1\":{\"190\":1}}],[\"并定义随机变量为恒等函数\",{\"1\":{\"566\":1}}],[\"并重新命名为\",{\"1\":{\"519\":1}}],[\"并解压到当前目录下\",{\"1\":{\"510\":1}}],[\"并调整学习率\",{\"1\":{\"495\":1,\"497\":1}}],[\"并让模型预测这些被遮盖的单词\",{\"1\":{\"495\":1}}],[\"并让模型基于上下文预测原始视觉标记\",{\"1\":{\"166\":1}}],[\"并优化了训练细节\",{\"1\":{\"494\":1}}],[\"并添加特殊标记\",{\"1\":{\"493\":1}}],[\"并添加缩略图以保留全局上下文\",{\"1\":{\"208\":1}}],[\"并验证了数据规模对预训练的关键作用\",{\"1\":{\"492\":1}}],[\"并提供了定义工具的简便方法\",{\"1\":{\"684\":1}}],[\"并提升了整体的处理效率\",{\"1\":{\"679\":1}}],[\"并提出了一系列改进措施\",{\"1\":{\"491\":1}}],[\"并提取每个区域的特征\",{\"1\":{\"253\":1}}],[\"并提取更高级别的局部特征\",{\"1\":{\"101\":1}}],[\"并作为下一轮的输入tokens\",{\"1\":{\"474\":1}}],[\"并未直接采用这些方法\",{\"1\":{\"469\":1}}],[\"并未带来性能提升\",{\"1\":{\"136\":1}}],[\"并指出\",{\"1\":{\"469\":1}}],[\"并指出gpt\",{\"1\":{\"452\":1}}],[\"并非\",{\"1\":{\"472\":1}}],[\"并非普遍\",{\"1\":{\"472\":1}}],[\"并非广义上的\",{\"1\":{\"468\":1}}],[\"并非通用智能\",{\"1\":{\"463\":1}}],[\"并结合\",{\"1\":{\"674\":1}}],[\"并结合论文图表\",{\"1\":{\"469\":1}}],[\"并结合强化学习进一步优化模型\",{\"1\":{\"467\":1}}],[\"并结合正则化损失\",{\"1\":{\"107\":1}}],[\"并探索更强的系统性泛化能力和稳健性\",{\"1\":{\"463\":1}}],[\"并探讨了数据污染和社会影响等问题\",{\"1\":{\"460\":1}}],[\"并按f1\",{\"1\":{\"461\":1}}],[\"并按行优先排序来实现\",{\"1\":{\"301\":1}}],[\"并系统评估其在零样本\",{\"1\":{\"460\":1}}],[\"并采用更灵活的任务无关\",{\"1\":{\"460\":1}}],[\"并采用两段式训练流程\",{\"1\":{\"440\":1}}],[\"并表明在极限情况下\",{\"1\":{\"456\":1}}],[\"并给出了提示transformer类模型和长距离依赖的文本数据集最好用这种方法来训练\",{\"1\":{\"450\":1}}],[\"并独立地处理\",{\"1\":{\"445\":1}}],[\"并最终引导出示例问题的正确结果\",{\"1\":{\"434\":1}}],[\"并最终输出分类结果\",{\"1\":{\"93\":1}}],[\"并评估它们在验证集或测试集上的性能\",{\"1\":{\"408\":1}}],[\"并行化操作和备选方案等高级功能\",{\"1\":{\"684\":1}}],[\"并行化要求更高\",{\"1\":{\"395\":1}}],[\"并行策略\",{\"1\":{\"481\":1}}],[\"并行优化\",{\"1\":{\"112\":1,\"481\":1}}],[\"并返回一个新的函数或类对象\",{\"1\":{\"368\":1}}],[\"并返回\",{\"1\":{\"367\":1}}],[\"并累加到累计正确样本数中\",{\"1\":{\"296\":1}}],[\"并获得该批次图像列表对应的图像嵌入向量列表\",{\"1\":{\"275\":1}}],[\"并计算与文本特征的余弦相似度\",{\"1\":{\"273\":1}}],[\"并迁移到标注数据较少的任务上\",{\"1\":{\"237\":1}}],[\"并从多个选项中选择正确答案\",{\"1\":{\"227\":1}}],[\"并用\",{\"1\":{\"542\":1}}],[\"并用它的返回值替换\",{\"1\":{\"375\":1}}],[\"并用这些数据训练一个端到端的视觉语言模型\",{\"1\":{\"224\":1}}],[\"并用其产生的软标签作为训练目标\",{\"1\":{\"127\":1}}],[\"并不知道\",{\"1\":{\"505\":1}}],[\"并不能显著提升其对用户意图的理解与遵循能力\",{\"1\":{\"467\":1}}],[\"并不能保证这些矩阵是正交矩阵\",{\"1\":{\"108\":1}}],[\"并不依赖于特定的语言模型\",{\"1\":{\"215\":1}}],[\"并将\",{\"1\":{\"686\":1}}],[\"并将结果传递给前一层变量\",{\"1\":{\"627\":1}}],[\"并将它们组合成一个批次进行处理\",{\"1\":{\"544\":1}}],[\"并将它们组织称为局部区域集\",{\"1\":{\"90\":1}}],[\"并将参数规模推至前所未有的高度\",{\"1\":{\"464\":1}}],[\"并将元素按照转置后的位置写入新内存\",{\"1\":{\"326\":1}}],[\"并将这些图像块嵌入到一个低维向量空间中\",{\"1\":{\"291\":1}}],[\"并将图片展示出来\",{\"1\":{\"276\":1}}],[\"并将其绑定到\",{\"1\":{\"660\":1}}],[\"并将其标记为vit\",{\"1\":{\"272\":1}}],[\"并将其与nous\",{\"1\":{\"215\":1}}],[\"并适配不同llms\",{\"1\":{\"212\":1}}],[\"并扩展至100万tokens的上下文窗口\",{\"1\":{\"210\":1}}],[\"并使其能够适配不同的语言模型\",{\"1\":{\"207\":1}}],[\"并与文本token一同输入transformer处理\",{\"1\":{\"253\":1}}],[\"并与一个由\",{\"1\":{\"198\":1}}],[\"并与图像特征联合用于预测\",{\"1\":{\"14\":1}}],[\"并应用\",{\"1\":{\"190\":1}}],[\"并能生成全局或局部视觉特征\",{\"1\":{\"189\":1}}],[\"并利用\",{\"1\":{\"596\":1}}],[\"并利用多源网络图像\",{\"1\":{\"180\":1}}],[\"并利用这些数据集对多模态模型进行知识蒸馏\",{\"1\":{\"26\":1}}],[\"并展示了零样本泛化能力\",{\"1\":{\"178\":1}}],[\"并引入高质量参考语料\",{\"1\":{\"461\":1}}],[\"并引入一个可调节的权重参数\",{\"1\":{\"407\":1}}],[\"并引入缩略图保留全局信息\",{\"1\":{\"214\":1}}],[\"并引入\",{\"1\":{\"163\":1}}],[\"并且允许语言模型与其所处的环境进行互动\",{\"1\":{\"682\":1}}],[\"并且是首个开源的推理型大模型\",{\"1\":{\"674\":1}}],[\"并且具有更好的泛化能力\",{\"1\":{\"674\":1}}],[\"并且它不是一个概率分布\",{\"1\":{\"596\":1}}],[\"并且使用预测结果计算nsp任务损失值\",{\"1\":{\"514\":1}}],[\"并且该优势在训练标注者和\",{\"1\":{\"471\":1}}],[\"并且不会对额外的架构组件使用迁移学习\",{\"1\":{\"445\":1}}],[\"并且不需要低秩适应\",{\"1\":{\"428\":1}}],[\"并且\",{\"1\":{\"410\":1,\"425\":1,\"477\":1}}],[\"并且可以避免梯度消失问题\",{\"1\":{\"392\":1}}],[\"并且取得了与卷积神经网络\",{\"1\":{\"301\":1}}],[\"并且与clip模型的训练数据不完全一致\",{\"1\":{\"274\":1}}],[\"并且albef\",{\"1\":{\"149\":1}}],[\"并且在所有重载运算符函数实现中\",{\"1\":{\"660\":1}}],[\"并且在训练过程中采用了一个相对较大的批次大小\",{\"1\":{\"272\":1}}],[\"并且在\",{\"1\":{\"82\":1}}],[\"并替换开头\",{\"1\":{\"142\":1}}],[\"并剔除低质量描述\",{\"1\":{\"138\":1}}],[\"并有利于多任务学习\",{\"1\":{\"126\":1}}],[\"并有望应用于机器人操作\",{\"1\":{\"48\":1}}],[\"并编码为一系列嵌入表示\",{\"1\":{\"126\":1}}],[\"并在\",{\"1\":{\"630\":1}}],[\"并在多数任务上击败palm\",{\"1\":{\"482\":1}}],[\"并在多种任务上表现良好\",{\"1\":{\"224\":1}}],[\"并在推理时通过上下文\",{\"1\":{\"460\":1}}],[\"并在7\",{\"1\":{\"457\":1}}],[\"并在最终自注意力块后增加额外层归一化\",{\"1\":{\"454\":1}}],[\"并在结尾加上特殊标记\",{\"1\":{\"410\":1}}],[\"并在被调用时依然保留这个引用\",{\"1\":{\"367\":1}}],[\"并在特定数据集上微调参数\",{\"1\":{\"167\":1}}],[\"并在每个局部区域提取特征\",{\"1\":{\"98\":1}}],[\"并在此基础上强化自身的语义表达\",{\"1\":{\"72\":1}}],[\"并集\",{\"1\":{\"78\":1,\"82\":1}}],[\"并取得不错的成效\",{\"1\":{\"287\":1}}],[\"并取消掩码\",{\"1\":{\"200\":1}}],[\"并取\",{\"1\":{\"78\":1}}],[\"并仅保留\",{\"1\":{\"68\":1}}],[\"并生成连贯的文本描述\",{\"1\":{\"190\":1}}],[\"并生成\",{\"1\":{\"40\":1}}],[\"并对潜在交互方式进行类比推理\",{\"1\":{\"26\":1}}],[\"并推广至未见场景\",{\"1\":{\"26\":1}}],[\"并通过variable\",{\"1\":{\"660\":1}}],[\"并通过链式法则自动推导了导数\",{\"1\":{\"650\":1}}],[\"并通过模型进行前向传播\",{\"1\":{\"296\":1}}],[\"并通过线性变换映射到嵌入空间\",{\"1\":{\"292\":1}}],[\"并通过对大规模嘈杂图文数据进行\",{\"1\":{\"138\":1}}],[\"并通过局部+全局特征融合机制实现强大的点云建模能力\",{\"1\":{\"105\":1}}],[\"并通过一个小型\",{\"1\":{\"92\":1}}],[\"并通过功能揭示模块\",{\"1\":{\"48\":1}}],[\"并通过多尺度上采样与融合\",{\"1\":{\"46\":1}}],[\"并通过复合损失优化整个流程\",{\"1\":{\"8\":1}}],[\"并通过跨模态自适应融合模块\",{\"1\":{\"5\":1}}],[\"并联想潜在意图\",{\"1\":{\"6\":1}}],[\"dtype属性\",{\"1\":{\"659\":1}}],[\"dtype\",{\"1\":{\"528\":1,\"659\":3,\"666\":1}}],[\"dtype=next\",{\"1\":{\"528\":1}}],[\"dtype=bool\",{\"1\":{\"83\":1}}],[\"dtype=torch\",{\"1\":{\"28\":1,\"43\":1,\"92\":5,\"142\":1,\"143\":1,\"145\":4,\"146\":1,\"147\":4,\"160\":1,\"161\":1,\"162\":2,\"163\":1,\"246\":1,\"247\":1,\"282\":1,\"284\":4,\"285\":1,\"286\":1,\"523\":1}}],[\"dgx\",{\"1\":{\"494\":1}}],[\"dgcnn\",{\"1\":{\"112\":2,\"115\":1}}],[\"dump\",{\"1\":{\"410\":3,\"412\":3,\"510\":2,\"511\":1,\"519\":1}}],[\"dumps\",{\"1\":{\"289\":1}}],[\"d2\",{\"1\":{\"326\":2,\"376\":3}}],[\"d1\",{\"1\":{\"326\":2,\"376\":3}}],[\"d1+d2\",{\"1\":{\"100\":1}}],[\"d0\",{\"1\":{\"326\":2}}],[\"dv\",{\"1\":{\"319\":3}}],[\"dvae\",{\"1\":{\"166\":1,\"170\":2,\"178\":1}}],[\"dk\",{\"1\":{\"319\":2}}],[\"da\",{\"1\":{\"519\":1}}],[\"days\",{\"1\":{\"472\":3}}],[\"dathathri\",{\"1\":{\"469\":1}}],[\"data方法实现\",{\"1\":{\"512\":1}}],[\"data为列表形式的情况\",{\"1\":{\"511\":1}}],[\"data文件所提供代码对原始数据格式进行解析\",{\"1\":{\"510\":1}}],[\"dataloader\",{\"0\":{\"522\":1},\"1\":{\"80\":3,\"244\":1,\"289\":1,\"290\":2,\"514\":4,\"522\":3}}],[\"dataframe\",{\"1\":{\"68\":1}}],[\"data\",{\"0\":{\"20\":1,\"124\":1},\"1\":{\"28\":1,\"68\":9,\"83\":7,\"92\":14,\"96\":4,\"142\":2,\"145\":3,\"147\":6,\"159\":3,\"224\":2,\"244\":2,\"246\":2,\"248\":3,\"276\":9,\"277\":9,\"278\":2,\"289\":3,\"290\":6,\"296\":4,\"325\":1,\"327\":1,\"470\":1,\"511\":9,\"512\":5,\"514\":3,\"517\":1,\"519\":1,\"607\":3,\"608\":5,\"613\":1,\"617\":1,\"622\":4,\"629\":3,\"630\":1,\"631\":1,\"642\":1,\"643\":6,\"645\":1,\"651\":1,\"652\":1,\"654\":1,\"656\":8,\"658\":2,\"659\":15,\"660\":7,\"666\":1,\"667\":8,\"683\":1}}],[\"datasets\",{\"1\":{\"147\":2,\"244\":1}}],[\"dataset\",{\"1\":{\"16\":1,\"29\":1,\"58\":1,\"68\":2,\"80\":6,\"81\":2,\"83\":1,\"140\":1,\"142\":9,\"145\":9,\"147\":2,\"244\":2,\"289\":5,\"290\":6,\"470\":1,\"510\":1,\"511\":1,\"514\":3,\"520\":1,\"522\":3}}],[\"daily\",{\"1\":{\"455\":1}}],[\"dall\",{\"1\":{\"270\":3}}],[\"dandelin\",{\"1\":{\"252\":1}}],[\"ddp\",{\"1\":{\"247\":2}}],[\"d为投影后的embedding维度\",{\"1\":{\"161\":1}}],[\"dr\",{\"1\":{\"454\":1,\"455\":1}}],[\"draw\",{\"1\":{\"83\":1}}],[\"drop=0\",{\"1\":{\"294\":1}}],[\"drop=drop\",{\"1\":{\"294\":1}}],[\"dropping\",{\"1\":{\"531\":1}}],[\"dropped\",{\"1\":{\"266\":2,\"285\":2}}],[\"droppath\",{\"1\":{\"200\":1,\"203\":1,\"294\":2}}],[\"drop2\",{\"1\":{\"93\":2,\"96\":2}}],[\"drop1\",{\"1\":{\"93\":2,\"96\":2,\"101\":2}}],[\"dropout=none\",{\"1\":{\"558\":1}}],[\"dropout=0\",{\"1\":{\"558\":1}}],[\"dropout=self\",{\"1\":{\"477\":1,\"558\":1}}],[\"dropout防止过拟合\",{\"1\":{\"285\":1}}],[\"dropout3\",{\"1\":{\"76\":1}}],[\"dropout2\",{\"1\":{\"76\":1}}],[\"dropout1\",{\"1\":{\"76\":1}}],[\"dropout\",{\"1\":{\"73\":4,\"76\":1,\"93\":2,\"95\":1,\"96\":2,\"98\":1,\"101\":1,\"110\":3,\"265\":4,\"266\":2,\"284\":1,\"285\":1,\"293\":1,\"294\":4,\"295\":2,\"296\":1,\"447\":2,\"470\":1,\"477\":2,\"523\":4,\"525\":4,\"529\":4,\"531\":4,\"532\":4,\"543\":4,\"544\":4,\"552\":6,\"553\":2,\"556\":2,\"558\":5}}],[\"drop\",{\"1\":{\"72\":1,\"73\":5,\"74\":1,\"75\":1,\"80\":1,\"244\":1,\"293\":5,\"294\":16,\"295\":8,\"296\":9,\"462\":1,\"470\":1,\"471\":1,\"472\":1}}],[\"driven\",{\"1\":{\"48\":1}}],[\"drive\",{\"1\":{\"4\":2,\"47\":2}}],[\"dynamiccache\",{\"1\":{\"477\":2}}],[\"dynamic\",{\"1\":{\"70\":1,\"76\":4,\"492\":1,\"495\":2,\"497\":1}}],[\"df\",{\"1\":{\"68\":7}}],[\"doing\",{\"1\":{\"529\":1}}],[\"doc\",{\"1\":{\"372\":4,\"495\":1}}],[\"docvqa\",{\"1\":{\"217\":1,\"220\":1,\"222\":1}}],[\"dog\",{\"1\":{\"285\":1,\"454\":2,\"505\":7}}],[\"does\",{\"1\":{\"276\":1,\"277\":1,\"289\":1}}],[\"dot语言基础语法\",{\"1\":{\"666\":1}}],[\"dot\",{\"1\":{\"272\":3,\"275\":1,\"277\":1,\"305\":1,\"508\":2,\"531\":1,\"558\":1,\"666\":29}}],[\"do\",{\"1\":{\"143\":1,\"244\":1,\"286\":1,\"519\":3,\"558\":1}}],[\"dosovitskiy\",{\"1\":{\"126\":1,\"131\":1}}],[\"downloaded\",{\"1\":{\"275\":1,\"277\":1}}],[\"downloading\",{\"1\":{\"275\":2,\"277\":2}}],[\"download\",{\"1\":{\"275\":3,\"277\":4,\"300\":1}}],[\"down\",{\"1\":{\"40\":1,\"44\":1,\"421\":1}}],[\"door\",{\"1\":{\"29\":1,\"58\":1,\"63\":1,\"67\":1,\"68\":1}}],[\"digraph\",{\"1\":{\"666\":5}}],[\"diff\",{\"1\":{\"622\":1,\"646\":2}}],[\"different\",{\"1\":{\"28\":1}}],[\"dinan\",{\"1\":{\"472\":1}}],[\"diag\",{\"1\":{\"284\":6}}],[\"diagonal\",{\"1\":{\"147\":3,\"161\":1,\"162\":2,\"590\":1}}],[\"dir=\",{\"1\":{\"519\":3}}],[\"dir=save\",{\"1\":{\"275\":1,\"277\":1}}],[\"directional\",{\"1\":{\"284\":2}}],[\"directory\",{\"1\":{\"275\":5,\"276\":1,\"277\":6}}],[\"dirname\",{\"1\":{\"275\":1,\"277\":1,\"510\":2}}],[\"dir\",{\"1\":{\"275\":16,\"276\":7,\"277\":23,\"519\":4,\"666\":4}}],[\"divide\",{\"1\":{\"395\":1}}],[\"diversity\",{\"1\":{\"63\":1}}],[\"div\",{\"1\":{\"41\":2,\"59\":2,\"660\":5}}],[\"dim1\",{\"1\":{\"382\":1}}],[\"dim0\",{\"1\":{\"382\":1}}],[\"dim代表的是卷积核的数量\",{\"1\":{\"291\":1}}],[\"dims\",{\"1\":{\"73\":13,\"383\":1}}],[\"dimensional\",{\"1\":{\"287\":1}}],[\"dimension\",{\"1\":{\"45\":1,\"46\":1,\"112\":1,\"246\":2,\"424\":2}}],[\"dimensions\",{\"1\":{\"40\":1}}],[\"dim=3\",{\"1\":{\"323\":1}}],[\"dim=embed\",{\"1\":{\"292\":1,\"293\":1,\"296\":2}}],[\"dim=768\",{\"1\":{\"160\":2,\"291\":1,\"292\":1,\"293\":1,\"296\":1,\"300\":1}}],[\"dim=256\",{\"1\":{\"145\":1,\"147\":1}}],[\"dim=2\",{\"1\":{\"100\":2,\"285\":2,\"323\":1,\"477\":1}}],[\"dim=dict\",{\"1\":{\"80\":2,\"83\":2}}],[\"dim=0\",{\"1\":{\"78\":1,\"143\":1,\"145\":11,\"147\":11,\"160\":2,\"162\":9,\"246\":1,\"284\":7,\"286\":1,\"289\":1,\"323\":1,\"381\":1,\"514\":1}}],[\"dim=\",{\"1\":{\"32\":2,\"41\":1,\"45\":2,\"59\":1,\"78\":1,\"92\":3,\"96\":1,\"100\":2,\"108\":1,\"111\":1,\"145\":4,\"146\":2,\"147\":4,\"161\":4,\"163\":3,\"266\":1,\"273\":4,\"282\":2,\"285\":1,\"295\":1,\"474\":1,\"477\":2,\"517\":1,\"531\":1,\"540\":3,\"541\":1,\"550\":1,\"558\":1}}],[\"dim=1\",{\"1\":{\"32\":4,\"34\":1,\"35\":1,\"40\":1,\"41\":2,\"45\":1,\"46\":2,\"59\":6,\"78\":4,\"96\":1,\"101\":1,\"110\":1,\"145\":11,\"147\":10,\"161\":8,\"162\":2,\"247\":3,\"284\":5,\"285\":1,\"292\":1,\"293\":1,\"296\":2,\"323\":1,\"390\":1,\"474\":1,\"513\":2,\"514\":1}}],[\"dim\",{\"1\":{\"30\":1,\"34\":9,\"35\":16,\"36\":22,\"41\":14,\"42\":4,\"43\":2,\"44\":2,\"45\":42,\"46\":14,\"59\":46,\"70\":3,\"80\":2,\"83\":2,\"145\":6,\"146\":3,\"147\":6,\"160\":12,\"161\":2,\"244\":1,\"246\":3,\"291\":8,\"292\":6,\"293\":6,\"294\":7,\"295\":17,\"296\":9,\"381\":1,\"387\":1,\"477\":11}}],[\"discrete\",{\"1\":{\"251\":2}}],[\"discrimination\",{\"0\":{\"235\":1},\"1\":{\"235\":1}}],[\"discussions\",{\"0\":{\"130\":1},\"1\":{\"129\":1}}],[\"distributedsampler\",{\"1\":{\"522\":1}}],[\"distributions\",{\"0\":{\"575\":1,\"576\":1,\"588\":1}}],[\"distribution\",{\"0\":{\"577\":1,\"578\":1,\"586\":1,\"587\":1},\"1\":{\"289\":1,\"573\":1,\"575\":2,\"576\":2,\"579\":1,\"582\":1,\"584\":2,\"585\":1,\"586\":1,\"587\":3,\"590\":1,\"596\":3}}],[\"distill\",{\"1\":{\"163\":4,\"674\":1}}],[\"distillation\",{\"0\":{\"123\":1,\"157\":1},\"1\":{\"148\":2,\"149\":1,\"150\":2}}],[\"dists\",{\"1\":{\"100\":6}}],[\"dist\",{\"1\":{\"92\":3,\"100\":3}}],[\"distance\",{\"0\":{\"357\":1,\"358\":1},\"1\":{\"92\":7,\"100\":1,\"590\":1}}],[\"dishwasher\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"displayed\",{\"1\":{\"666\":1}}],[\"display\",{\"1\":{\"29\":2,\"58\":2,\"68\":2,\"666\":2}}],[\"dict=return\",{\"1\":{\"163\":1,\"285\":1}}],[\"dict=none\",{\"1\":{\"163\":1,\"285\":1}}],[\"dict=true\",{\"1\":{\"142\":1,\"145\":4,\"147\":5,\"161\":2,\"282\":2,\"284\":1,\"285\":3}}],[\"dict=false\",{\"1\":{\"40\":1}}],[\"dicts\",{\"1\":{\"80\":2}}],[\"dictionary\",{\"1\":{\"30\":1,\"33\":1,\"246\":1,\"412\":16}}],[\"dict\",{\"1\":{\"29\":7,\"58\":6,\"80\":2,\"81\":1,\"82\":2,\"83\":5,\"146\":2,\"162\":2,\"163\":6,\"244\":1,\"285\":1,\"289\":2,\"300\":3,\"363\":2,\"389\":3,\"410\":13,\"412\":23,\"511\":8,\"514\":2}}],[\"dicebceloss\",{\"1\":{\"402\":2}}],[\"diceloss\",{\"1\":{\"78\":4,\"401\":2}}],[\"dice\",{\"0\":{\"401\":1,\"402\":1},\"1\":{\"15\":1,\"40\":1,\"55\":1,\"78\":14,\"401\":17,\"402\":21,\"403\":2,\"405\":2,\"407\":14}}],[\"dezero\",{\"1\":{\"666\":1}}],[\"dependency\",{\"1\":{\"510\":2}}],[\"depth\",{\"1\":{\"296\":1,\"395\":1}}],[\"depth=12\",{\"1\":{\"160\":2,\"293\":1,\"296\":1,\"300\":1}}],[\"density\",{\"1\":{\"566\":1}}],[\"dense\",{\"1\":{\"262\":2,\"265\":2,\"268\":2,\"404\":1,\"525\":4,\"527\":2,\"532\":2,\"535\":2,\"552\":1,\"674\":2}}],[\"dennison\",{\"1\":{\"469\":1}}],[\"del\",{\"1\":{\"410\":1,\"412\":1,\"558\":3}}],[\"detokenize\",{\"1\":{\"511\":2}}],[\"detokenizers\",{\"1\":{\"455\":1}}],[\"detection\",{\"1\":{\"404\":1}}],[\"details\",{\"0\":{\"516\":1},\"1\":{\"469\":1}}],[\"detail\",{\"0\":{\"241\":1}}],[\"detailed\",{\"1\":{\"12\":1,\"28\":2,\"227\":1}}],[\"detach\",{\"1\":{\"83\":1,\"145\":3,\"147\":2,\"161\":2,\"247\":1}}],[\"deactivate\",{\"1\":{\"333\":1}}],[\"deit\",{\"1\":{\"160\":1}}],[\"dequeue\",{\"0\":{\"249\":1},\"1\":{\"145\":1,\"147\":1,\"161\":1,\"247\":2,\"249\":2}}],[\"devlin\",{\"1\":{\"126\":1,\"131\":1,\"485\":1}}],[\"device=input\",{\"1\":{\"523\":1}}],[\"device=inputs\",{\"1\":{\"477\":1}}],[\"device=image\",{\"1\":{\"283\":1}}],[\"device=sim\",{\"1\":{\"284\":1}}],[\"device=multi\",{\"1\":{\"43\":1}}],[\"device\",{\"1\":{\"28\":1,\"40\":3,\"43\":1,\"59\":5,\"70\":1,\"82\":1,\"92\":15,\"142\":4,\"143\":2,\"145\":5,\"146\":2,\"147\":6,\"159\":3,\"161\":2,\"162\":1,\"163\":3,\"275\":5,\"277\":5,\"282\":2,\"283\":1,\"284\":4,\"285\":1,\"286\":2,\"296\":4,\"300\":1,\"477\":2,\"514\":2,\"523\":1}}],[\"deeplearning\",{\"1\":{\"244\":2}}],[\"deepseekr1\",{\"1\":{\"674\":1}}],[\"deepseekmoe\",{\"1\":{\"674\":1}}],[\"deepseek\",{\"1\":{\"211\":1,\"673\":1,\"674\":17}}],[\"deep\",{\"1\":{\"115\":1}}],[\"descent\",{\"1\":{\"667\":2}}],[\"desc=\",{\"1\":{\"410\":4,\"412\":2}}],[\"desc\",{\"1\":{\"273\":2}}],[\"describe\",{\"1\":{\"28\":4}}],[\"description\",{\"1\":{\"12\":1,\"28\":2,\"40\":3,\"227\":1}}],[\"destroy\",{\"1\":{\"83\":1}}],[\"debug\",{\"1\":{\"82\":1,\"371\":2}}],[\"decorate\",{\"1\":{\"374\":2}}],[\"decorator\",{\"1\":{\"367\":3,\"369\":4,\"370\":2,\"371\":4,\"372\":5,\"375\":3,\"378\":2}}],[\"decode\",{\"1\":{\"126\":1,\"143\":1,\"286\":1,\"474\":2,\"477\":2,\"511\":1,\"542\":1,\"549\":2}}],[\"decoder模型结构图\",{\"1\":{\"556\":1,\"557\":1}}],[\"decoderlayer\",{\"0\":{\"556\":1},\"1\":{\"556\":2}}],[\"decoder=true时\",{\"1\":{\"285\":1}}],[\"decoder=true\",{\"1\":{\"268\":1,\"285\":1}}],[\"decoder=is\",{\"1\":{\"163\":1,\"268\":1,\"285\":1}}],[\"decoder=false\",{\"1\":{\"163\":1,\"262\":1}}],[\"decoder\",{\"0\":{\"15\":1,\"126\":1,\"549\":1,\"555\":1,\"557\":1},\"1\":{\"30\":1,\"35\":1,\"40\":1,\"45\":1,\"46\":1,\"59\":2,\"70\":2,\"76\":3,\"99\":1,\"120\":1,\"126\":2,\"127\":1,\"138\":1,\"142\":8,\"143\":5,\"147\":10,\"163\":2,\"191\":1,\"268\":3,\"280\":1,\"285\":9,\"286\":2,\"443\":1,\"447\":1,\"536\":2,\"542\":1,\"548\":2,\"549\":4,\"556\":1,\"557\":3,\"674\":5}}],[\"decoding过程\",{\"1\":{\"70\":2,\"76\":1}}],[\"decoding\",{\"1\":{\"70\":1}}],[\"decay=0\",{\"1\":{\"514\":1}}],[\"decay=args\",{\"1\":{\"244\":1}}],[\"decay=config\",{\"1\":{\"142\":1,\"145\":1,\"147\":1}}],[\"decay=opt\",{\"1\":{\"80\":1}}],[\"decay\",{\"1\":{\"80\":1,\"142\":1,\"145\":1,\"147\":1,\"244\":1}}],[\"definition\",{\"0\":{\"590\":1}}],[\"defined\",{\"1\":{\"556\":1}}],[\"define\",{\"1\":{\"244\":1,\"662\":2}}],[\"defaultdict\",{\"1\":{\"410\":5,\"412\":5}}],[\"default\",{\"1\":{\"83\":1,\"246\":4}}],[\"def\",{\"1\":{\"29\":2,\"30\":1,\"32\":1,\"33\":1,\"34\":4,\"35\":4,\"36\":4,\"40\":1,\"41\":2,\"43\":1,\"45\":4,\"46\":2,\"58\":2,\"59\":14,\"68\":3,\"70\":1,\"72\":2,\"73\":2,\"74\":2,\"75\":1,\"76\":2,\"78\":2,\"80\":1,\"83\":4,\"92\":7,\"93\":2,\"96\":4,\"98\":1,\"100\":2,\"101\":2,\"107\":2,\"108\":1,\"109\":2,\"110\":2,\"111\":2,\"142\":5,\"143\":1,\"145\":4,\"146\":2,\"147\":4,\"159\":1,\"160\":1,\"161\":1,\"162\":1,\"163\":3,\"244\":4,\"246\":1,\"247\":1,\"248\":1,\"249\":1,\"262\":3,\"263\":2,\"264\":1,\"265\":3,\"266\":1,\"268\":6,\"275\":8,\"276\":3,\"277\":11,\"282\":1,\"284\":1,\"285\":4,\"286\":1,\"289\":5,\"291\":2,\"292\":3,\"293\":3,\"294\":4,\"295\":2,\"296\":4,\"300\":1,\"365\":2,\"366\":2,\"367\":3,\"369\":3,\"370\":3,\"371\":4,\"372\":6,\"373\":3,\"374\":1,\"375\":1,\"376\":1,\"377\":3,\"379\":6,\"401\":2,\"402\":2,\"403\":2,\"404\":2,\"405\":2,\"407\":2,\"410\":9,\"411\":6,\"412\":23,\"474\":1,\"477\":6,\"510\":2,\"511\":6,\"512\":3,\"513\":2,\"517\":2,\"520\":3,\"522\":1,\"523\":2,\"525\":6,\"526\":2,\"527\":2,\"528\":2,\"529\":2,\"531\":3,\"532\":2,\"533\":2,\"535\":2,\"536\":2,\"537\":2,\"538\":2,\"541\":2,\"543\":2,\"544\":2,\"549\":4,\"550\":2,\"552\":2,\"553\":2,\"554\":2,\"556\":2,\"557\":2,\"558\":3,\"607\":1,\"613\":3,\"614\":1,\"617\":2,\"622\":1,\"629\":1,\"630\":2,\"631\":1,\"634\":2,\"635\":1,\"638\":1,\"641\":2,\"642\":1,\"643\":1,\"645\":2,\"646\":2,\"651\":3,\"652\":1,\"653\":1,\"654\":1,\"656\":5,\"657\":2,\"658\":5,\"659\":7,\"660\":24,\"661\":1,\"662\":3,\"666\":6,\"667\":2}}],[\"d\",{\"1\":{\"6\":1,\"19\":1,\"40\":1,\"70\":1,\"92\":4,\"96\":4,\"100\":17,\"108\":4,\"145\":11,\"147\":16,\"161\":8,\"162\":1,\"163\":1,\"188\":2,\"189\":1,\"190\":1,\"191\":1,\"255\":1,\"272\":6,\"280\":2,\"286\":4,\"293\":2,\"323\":1,\"513\":10,\"517\":2,\"531\":2,\"544\":1,\"550\":3,\"558\":15,\"590\":1,\"667\":1}}],[\"方式提供了对照方案\",{\"1\":{\"469\":1}}],[\"方案退火到\",{\"1\":{\"447\":1}}],[\"方差为\",{\"1\":{\"593\":1}}],[\"方差\",{\"1\":{\"389\":1}}],[\"方差差异大且可能相关的情况\",{\"1\":{\"360\":1}}],[\"方差是\",{\"1\":{\"359\":1}}],[\"方差大\",{\"1\":{\"357\":1}}],[\"方便后续的计算和比较\",{\"1\":{\"275\":1}}],[\"方便和下游任务对接\",{\"1\":{\"257\":1}}],[\"方面发挥了重要作用\",{\"1\":{\"208\":1}}],[\"方向\",{\"1\":{\"40\":1,\"355\":1}}],[\"方向研究\",{\"1\":{\"2\":1}}],[\"方法求导\",{\"1\":{\"667\":1}}],[\"方法会被优先调用\",{\"1\":{\"660\":1}}],[\"方法以支持数组索引\",{\"1\":{\"659\":1}}],[\"方法被调用时\",{\"1\":{\"656\":1}}],[\"方法与目标\",{\"1\":{\"483\":1}}],[\"方法的实例\",{\"1\":{\"660\":3}}],[\"方法的核心概括图\",{\"1\":{\"470\":1}}],[\"方法的特定任务的输入改写\",{\"1\":{\"440\":1}}],[\"方法进行下游迁移学习\",{\"1\":{\"460\":1}}],[\"方法进行训练的代码实现\",{\"1\":{\"142\":1}}],[\"方法处理variable\",{\"1\":{\"660\":1}}],[\"方法处理输入输出的变量封装\",{\"1\":{\"612\":1}}],[\"方法处理\",{\"1\":{\"447\":1}}],[\"方法来模拟函数行为\",{\"1\":{\"377\":1}}],[\"方法中保存输入输出变量\",{\"1\":{\"630\":1}}],[\"方法中\",{\"1\":{\"285\":1,\"634\":1,\"660\":1}}],[\"方法中已开始尝试\",{\"1\":{\"123\":1}}],[\"方法和应用场景有显著区别\",{\"1\":{\"231\":1}}],[\"方法二\",{\"1\":{\"228\":1}}],[\"方法一\",{\"1\":{\"228\":1}}],[\"方法代码实现\",{\"1\":{\"163\":1}}],[\"方法实现\",{\"1\":{\"145\":1}}],[\"方法参考\",{\"1\":{\"127\":1}}],[\"方法部分主要分为三个内容\",{\"1\":{\"125\":1}}],[\"方法依赖从网络抓取的图文对数据\",{\"1\":{\"122\":1}}],[\"方法虽然在多模态任务上取得进展\",{\"1\":{\"120\":1}}],[\"方法\",{\"0\":{\"8\":1,\"53\":1,\"167\":1,\"187\":1,\"213\":1,\"225\":1,\"454\":1,\"461\":1,\"470\":1,\"481\":1},\"1\":{\"82\":1,\"112\":1,\"114\":1,\"165\":1,\"190\":3,\"424\":1,\"652\":1,\"658\":1,\"659\":1,\"660\":8}}],[\"方法创新\",{\"1\":{\"6\":1}}],[\"倒水\",{\"1\":{\"6\":1,\"32\":1}}],[\"壶嘴的形状\",{\"1\":{\"32\":1}}],[\"壶嘴上开口狭窄\",{\"1\":{\"28\":1}}],[\"壶嘴\",{\"1\":{\"6\":1,\"32\":1}}],[\"html\",{\"1\":{\"687\":1}}],[\"https\",{\"1\":{\"4\":3,\"37\":2,\"47\":3,\"60\":2,\"85\":3,\"102\":3,\"119\":1,\"140\":1,\"148\":1,\"179\":2,\"223\":2,\"232\":1,\"252\":1,\"279\":2,\"289\":2,\"300\":4,\"406\":1,\"477\":1,\"503\":1,\"510\":2,\"519\":2,\"546\":1,\"573\":1,\"603\":1,\"649\":1,\"664\":1,\"669\":1}}],[\"hh\",{\"1\":{\"410\":1}}],[\"h`和\",{\"1\":{\"291\":1}}],[\"hyperparameter\",{\"1\":{\"240\":1}}],[\"hybrid\",{\"0\":{\"78\":1},\"1\":{\"78\":1,\"299\":1}}],[\"hr\",{\"1\":{\"211\":1}}],[\"hpt\",{\"1\":{\"210\":1}}],[\"hoffmann\",{\"1\":{\"485\":1}}],[\"honest\",{\"1\":{\"468\":1,\"470\":1}}],[\"houlsby\",{\"1\":{\"424\":1}}],[\"home\",{\"1\":{\"334\":3}}],[\"hook\",{\"1\":{\"285\":1}}],[\"horse\",{\"1\":{\"273\":1}}],[\"how\",{\"1\":{\"206\":2}}],[\"hot向量\",{\"1\":{\"240\":1}}],[\"hot\",{\"1\":{\"145\":3,\"154\":1,\"155\":1,\"156\":1,\"157\":3,\"159\":3,\"576\":1}}],[\"hold\",{\"1\":{\"83\":2,\"511\":2}}],[\"holyoak\",{\"1\":{\"6\":1}}],[\"hub\",{\"1\":{\"277\":1}}],[\"huggingface\",{\"0\":{\"477\":1},\"1\":{\"163\":2,\"277\":1,\"477\":3,\"511\":1}}],[\"hutter\",{\"1\":{\"131\":1}}],[\"human\",{\"1\":{\"28\":1,\"29\":2,\"30\":2,\"32\":1,\"33\":2,\"224\":1,\"226\":1,\"227\":2,\"416\":1,\"466\":1,\"468\":1,\"469\":1,\"470\":1}}],[\"hw\",{\"1\":{\"41\":1,\"45\":3,\"59\":1}}],[\"hm\",{\"0\":{\"78\":1},\"1\":{\"40\":6,\"78\":3,\"80\":2,\"81\":3}}],[\"he\",{\"1\":{\"410\":1}}],[\"hey\",{\"1\":{\"410\":1}}],[\"hellaswag\",{\"1\":{\"470\":1}}],[\"hello\",{\"1\":{\"367\":1,\"369\":7,\"372\":5,\"373\":2,\"410\":1,\"411\":1}}],[\"held\",{\"1\":{\"454\":1,\"470\":2,\"471\":1}}],[\"helpful\",{\"1\":{\"468\":1,\"470\":1}}],[\"help\",{\"1\":{\"335\":2,\"372\":1}}],[\"hermes\",{\"1\":{\"215\":2}}],[\"here\",{\"1\":{\"73\":1,\"529\":1}}],[\"heavy\",{\"1\":{\"587\":1}}],[\"heatmap\",{\"1\":{\"40\":1,\"46\":2}}],[\"head结构由linear+tanh激活函数+linear组成\",{\"1\":{\"296\":1}}],[\"head进行分类\",{\"1\":{\"296\":1}}],[\"head的位置是和这个\",{\"1\":{\"292\":1}}],[\"head之中再输出分类结果\",{\"1\":{\"292\":1}}],[\"head将输出feature映射成一个二值logits\",{\"1\":{\"258\":1}}],[\"header\",{\"1\":{\"159\":1}}],[\"head==\",{\"1\":{\"146\":2}}],[\"head=\",{\"1\":{\"146\":1}}],[\"heads代表transformer中multi\",{\"1\":{\"297\":1}}],[\"heads=8\",{\"1\":{\"295\":1}}],[\"heads=num\",{\"1\":{\"294\":1,\"296\":1}}],[\"heads=12\",{\"1\":{\"160\":2,\"293\":1,\"296\":1,\"300\":1}}],[\"heads=dict\",{\"1\":{\"80\":1,\"83\":1}}],[\"heads\",{\"1\":{\"41\":6,\"59\":6,\"80\":1,\"83\":1,\"285\":1,\"294\":2,\"295\":12,\"296\":1,\"477\":1,\"517\":2,\"531\":7,\"558\":3}}],[\"head\",{\"0\":{\"9\":1,\"11\":1,\"12\":1,\"28\":1,\"296\":1},\"1\":{\"6\":2,\"13\":2,\"26\":1,\"35\":2,\"40\":1,\"46\":6,\"59\":4,\"98\":1,\"99\":1,\"145\":2,\"146\":2,\"147\":2,\"160\":1,\"162\":2,\"163\":2,\"193\":1,\"205\":1,\"266\":2,\"268\":1,\"284\":1,\"285\":10,\"292\":1,\"293\":1,\"294\":1,\"295\":8,\"296\":3,\"297\":1,\"300\":1,\"313\":1,\"315\":1,\"477\":6,\"526\":2,\"528\":1,\"529\":2,\"531\":10,\"538\":2,\"541\":1,\"543\":2,\"544\":2,\"548\":1,\"674\":1}}],[\"height=3\",{\"1\":{\"323\":1}}],[\"height=600\",{\"1\":{\"83\":1}}],[\"height\",{\"1\":{\"40\":1,\"289\":1,\"323\":1,\"328\":1}}],[\"hezhu\",{\"1\":{\"37\":1}}],[\"him\",{\"1\":{\"482\":1,\"484\":1}}],[\"his\",{\"1\":{\"482\":1,\"484\":1}}],[\"history=\",{\"1\":{\"28\":2}}],[\"history=history\",{\"1\":{\"28\":3}}],[\"history=true\",{\"1\":{\"28\":4}}],[\"history=none\",{\"1\":{\"28\":1}}],[\"history\",{\"1\":{\"28\":2}}],[\"hinge\",{\"0\":{\"406\":1},\"1\":{\"406\":1}}],[\"hi\",{\"1\":{\"377\":1}}],[\"higher\",{\"1\":{\"365\":1}}],[\"hidden\",{\"1\":{\"40\":4,\"42\":2,\"43\":3,\"44\":3,\"45\":1,\"142\":1,\"143\":1,\"145\":7,\"146\":3,\"147\":7,\"160\":3,\"161\":4,\"162\":13,\"163\":10,\"262\":8,\"263\":9,\"264\":4,\"265\":17,\"266\":8,\"267\":1,\"268\":25,\"282\":8,\"283\":7,\"284\":7,\"285\":34,\"286\":1,\"294\":9,\"297\":1,\"477\":26,\"513\":2,\"523\":5,\"525\":24,\"526\":5,\"527\":5,\"529\":4,\"531\":8,\"532\":12,\"535\":15,\"536\":7,\"537\":1,\"538\":1,\"540\":6,\"541\":2,\"543\":3,\"544\":4}}],[\"hierarchy\",{\"1\":{\"35\":1,\"59\":1}}],[\"hv\",{\"1\":{\"32\":1}}],[\"hq\",{\"1\":{\"32\":1}}],[\"h进行交互融合\",{\"1\":{\"30\":1}}],[\"h=\",{\"1\":{\"30\":1}}],[\"h\",{\"1\":{\"30\":9,\"33\":5,\"34\":6,\"35\":5,\"40\":3,\"41\":2,\"43\":1,\"59\":4,\"105\":5,\"145\":1,\"161\":3,\"244\":1,\"272\":1,\"282\":1,\"291\":4,\"292\":1,\"293\":1,\"296\":1,\"405\":2,\"407\":1,\"410\":5,\"474\":1,\"477\":1,\"513\":5,\"519\":3,\"558\":9,\"565\":1,\"596\":3}}],[\"hdf5\",{\"1\":{\"338\":1}}],[\"hd\",{\"1\":{\"29\":3}}],[\"haiku\",{\"1\":{\"674\":2}}],[\"hairy\",{\"1\":{\"505\":2}}],[\"hairy→my\",{\"1\":{\"505\":3}}],[\"half\",{\"0\":{\"585\":1},\"1\":{\"585\":1,\"587\":1}}],[\"hallucination\",{\"1\":{\"471\":1}}],[\"hallucinate\",{\"1\":{\"470\":1}}],[\"hallusionbench分数最高\",{\"1\":{\"220\":1}}],[\"have\",{\"1\":{\"471\":1}}],[\"harvardnlp\",{\"1\":{\"546\":1}}],[\"harmless\",{\"1\":{\"468\":1,\"470\":1}}],[\"hard\",{\"1\":{\"78\":1,\"82\":1,\"147\":1,\"156\":1,\"159\":1,\"161\":1,\"162\":2,\"163\":2,\"404\":2}}],[\"happy\",{\"1\":{\"295\":1}}],[\"ha=\",{\"1\":{\"289\":1}}],[\"haotian\",{\"1\":{\"223\":1}}],[\"has\",{\"1\":{\"40\":1,\"285\":1,\"300\":3,\"520\":2}}],[\"hammer\",{\"1\":{\"29\":1}}],[\"hat\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"hands\",{\"1\":{\"83\":2}}],[\"handle\",{\"1\":{\"28\":1,\"31\":2,\"32\":1}}],[\"hand\",{\"1\":{\"28\":4,\"31\":4}}],[\"hk\",{\"1\":{\"29\":4,\"32\":20,\"33\":2}}],[\"研究和应用\",{\"1\":{\"675\":1}}],[\"研究人员提出了一种新的模型架构\",{\"1\":{\"679\":1}}],[\"研究人员还在努力让计算机理解图像和文字\",{\"1\":{\"678\":1}}],[\"研究人员发现\",{\"1\":{\"673\":1}}],[\"研究人员不断尝试改进\",{\"1\":{\"673\":1}}],[\"研究揭示了模型性能提升的关键因素并非复杂结构改动\",{\"1\":{\"502\":1}}],[\"研究强调了预训练中设计选择和数据规模的重要性\",{\"1\":{\"491\":1}}],[\"研究团队定义了标注规则\",{\"1\":{\"472\":1}}],[\"研究团队开源模型权重\",{\"1\":{\"208\":1}}],[\"研究者的设计意图\",{\"1\":{\"472\":1}}],[\"研究还探讨了数据污染问题\",{\"1\":{\"459\":1}}],[\"研究还提出了目前最大的3d功能数据集piadv2\",{\"1\":{\"5\":1}}],[\"研究结果对构建通用语言系统具有重要意义\",{\"1\":{\"457\":1}}],[\"研究意义\",{\"1\":{\"453\":1,\"460\":1}}],[\"研究发现\",{\"1\":{\"452\":1,\"492\":1}}],[\"研究背景与问题\",{\"1\":{\"181\":1,\"208\":1}}],[\"研究指出\",{\"1\":{\"178\":1}}],[\"研究表明\",{\"1\":{\"6\":1,\"456\":1,\"467\":1}}],[\"b中\",{\"1\":{\"660\":3}}],[\"b时\",{\"1\":{\"660\":1}}],[\"b上有1个点的绝对提升\",{\"1\":{\"448\":1}}],[\"b为例\",{\"1\":{\"660\":1}}],[\"b为r\",{\"1\":{\"420\":1}}],[\"b为批量大小\",{\"1\":{\"295\":1}}],[\"bpe\",{\"1\":{\"409\":2,\"410\":8,\"412\":7,\"447\":1,\"454\":4,\"492\":1,\"497\":1}}],[\"bpe编码\",{\"1\":{\"178\":1}}],[\"b+k\",{\"1\":{\"161\":3,\"162\":1}}],[\"b+q\",{\"1\":{\"147\":6}}],[\"b=3\",{\"1\":{\"145\":1}}],[\"bubble\",{\"1\":{\"592\":1}}],[\"but\",{\"1\":{\"531\":1,\"536\":1}}],[\"build\",{\"1\":{\"474\":15,\"520\":1}}],[\"builder\",{\"1\":{\"244\":1}}],[\"buffer的作用和意义\",{\"1\":{\"389\":1}}],[\"buffer\",{\"0\":{\"389\":1},\"1\":{\"145\":4,\"147\":3,\"160\":3,\"246\":2,\"389\":1}}],[\"bucket\",{\"1\":{\"29\":1}}],[\"bce\",{\"0\":{\"402\":1},\"1\":{\"78\":3,\"401\":1,\"402\":17,\"403\":2,\"404\":2}}],[\"bcn\",{\"1\":{\"70\":1,\"76\":3}}],[\"bleu或精确匹配评估\",{\"1\":{\"461\":1}}],[\"bleu\",{\"1\":{\"455\":2}}],[\"blob\",{\"1\":{\"289\":1,\"477\":1}}],[\"block第一个全连接的节点个数\",{\"1\":{\"297\":1}}],[\"block的次数\",{\"1\":{\"297\":1}}],[\"block块序列\",{\"1\":{\"296\":2}}],[\"blocking=true\",{\"1\":{\"159\":1}}],[\"block\",{\"1\":{\"126\":1,\"191\":1,\"294\":3,\"296\":1,\"444\":1,\"477\":2}}],[\"blocks\",{\"1\":{\"96\":7,\"296\":2}}],[\"black\",{\"1\":{\"226\":1,\"273\":1,\"485\":1}}],[\"blip2qformer\",{\"1\":{\"282\":1,\"286\":2}}],[\"blip2qformer核心代码实现如下\",{\"1\":{\"282\":1}}],[\"blip2\",{\"1\":{\"279\":1}}],[\"blip2base\",{\"1\":{\"40\":1,\"282\":1,\"286\":1}}],[\"blip\",{\"0\":{\"119\":1,\"147\":1},\"1\":{\"119\":3,\"120\":2,\"122\":1,\"123\":1,\"124\":1,\"125\":1,\"127\":1,\"129\":2,\"131\":1,\"134\":1,\"138\":3,\"140\":3,\"142\":4,\"145\":4,\"146\":1,\"147\":7,\"191\":1,\"194\":1,\"260\":1,\"268\":1,\"280\":2,\"281\":1}}],[\"blc\",{\"1\":{\"70\":1,\"76\":3}}],[\"bytenet和convs2s等网络模型\",{\"1\":{\"547\":1}}],[\"bytepairs\",{\"1\":{\"412\":1}}],[\"bytepairtokenizer\",{\"1\":{\"410\":2,\"411\":2,\"412\":5}}],[\"bytes\",{\"1\":{\"410\":2,\"411\":20,\"412\":35}}],[\"byte\",{\"1\":{\"321\":1,\"409\":1,\"410\":4,\"411\":7,\"412\":35,\"454\":1,\"497\":1}}],[\"by\",{\"1\":{\"68\":1,\"246\":1,\"412\":1,\"433\":1,\"438\":1,\"527\":1,\"662\":2}}],[\"bn5\",{\"1\":{\"107\":2}}],[\"bn4\",{\"1\":{\"107\":2}}],[\"bn3\",{\"1\":{\"107\":2,\"109\":2,\"111\":2}}],[\"bn2\",{\"1\":{\"93\":2,\"96\":2,\"107\":2,\"109\":2,\"110\":2,\"111\":2}}],[\"bn1\",{\"1\":{\"93\":2,\"96\":2,\"101\":2,\"107\":2,\"109\":2,\"110\":2,\"111\":2}}],[\"bns\",{\"1\":{\"92\":3,\"96\":3,\"100\":3}}],[\"bn\",{\"1\":{\"46\":1,\"92\":2,\"96\":7,\"100\":4,\"107\":1,\"247\":1}}],[\"bs\",{\"1\":{\"43\":2,\"83\":1,\"145\":5,\"147\":7,\"162\":7,\"284\":19}}],[\"bmm\",{\"1\":{\"32\":4,\"41\":3,\"45\":4,\"59\":3,\"107\":1,\"108\":1,\"109\":2,\"390\":1}}],[\"brew\",{\"1\":{\"666\":1}}],[\"break\",{\"1\":{\"29\":1,\"412\":1,\"474\":1,\"477\":1}}],[\"broader\",{\"1\":{\"472\":1}}],[\"broadcasting\",{\"1\":{\"387\":1}}],[\"broadcast\",{\"0\":{\"327\":1},\"1\":{\"327\":1}}],[\"broom\",{\"1\":{\"29\":1}}],[\"borel\",{\"1\":{\"566\":6}}],[\"bos\",{\"1\":{\"142\":2,\"143\":2,\"147\":2,\"285\":3,\"286\":3}}],[\"bookcorpus\",{\"1\":{\"493\":1,\"494\":1,\"498\":1}}],[\"book\",{\"1\":{\"455\":1,\"462\":1,\"482\":1}}],[\"books3\",{\"1\":{\"481\":1}}],[\"books2\",{\"1\":{\"461\":1}}],[\"books1\",{\"1\":{\"461\":1}}],[\"bookscorpus\",{\"1\":{\"447\":1}}],[\"books\",{\"1\":{\"447\":1,\"480\":1,\"674\":1}}],[\"bool\",{\"1\":{\"142\":1,\"163\":3,\"246\":1,\"300\":1,\"389\":2,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"411\":1,\"412\":2,\"477\":1}}],[\"bootstrapped\",{\"0\":{\"137\":1},\"1\":{\"136\":1,\"137\":1,\"138\":1,\"140\":1}}],[\"bootstrapping\",{\"1\":{\"119\":2,\"120\":1,\"134\":1,\"138\":2}}],[\"bounding\",{\"1\":{\"59\":2,\"226\":2}}],[\"box框文件路径\",{\"1\":{\"58\":1}}],[\"box\",{\"1\":{\"58\":17,\"59\":44,\"226\":2}}],[\"bottleneck\",{\"1\":{\"71\":1,\"105\":1,\"112\":1}}],[\"bottle\",{\"1\":{\"29\":1,\"40\":1,\"58\":1,\"68\":1}}],[\"bowl\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"body\",{\"1\":{\"28\":1}}],[\"bivariate\",{\"1\":{\"590\":1}}],[\"bit\",{\"1\":{\"531\":1}}],[\"bid\",{\"1\":{\"412\":2}}],[\"bidirectional\",{\"1\":{\"165\":1,\"504\":1}}],[\"bigram\",{\"1\":{\"410\":2,\"412\":2}}],[\"bilstm\",{\"1\":{\"448\":1}}],[\"bilinear\",{\"1\":{\"397\":1}}],[\"billion\",{\"1\":{\"297\":1,\"455\":1}}],[\"binomial\",{\"0\":{\"575\":1,\"578\":1},\"1\":{\"575\":1,\"579\":1}}],[\"bin\",{\"1\":{\"397\":7,\"519\":1}}],[\"binary\",{\"0\":{\"2\":1},\"1\":{\"0\":1,\"64\":1,\"78\":1,\"82\":11,\"402\":6,\"404\":2,\"407\":1}}],[\"binaryoracle\",{\"1\":{\"0\":1,\"503\":1,\"603\":1,\"649\":1,\"664\":1,\"669\":1}}],[\"bias\",{\"0\":{\"484\":1},\"1\":{\"287\":1,\"294\":1,\"295\":1,\"296\":1,\"536\":3}}],[\"bias=qkv\",{\"1\":{\"294\":1,\"295\":1,\"296\":1}}],[\"bias=true\",{\"1\":{\"160\":2,\"293\":1,\"296\":1}}],[\"bias=false\",{\"1\":{\"73\":2,\"268\":1,\"294\":1,\"295\":1,\"513\":1,\"536\":1}}],[\"bi\",{\"1\":{\"284\":2}}],[\"bicycle\",{\"1\":{\"29\":1}}],[\"bfloat16\",{\"1\":{\"28\":2}}],[\"below\",{\"1\":{\"556\":1}}],[\"bengio\",{\"1\":{\"673\":1}}],[\"ben18785\",{\"1\":{\"573\":1}}],[\"bender\",{\"1\":{\"469\":1}}],[\"benchmarks\",{\"1\":{\"447\":1}}],[\"benchmark\",{\"0\":{\"22\":1},\"1\":{\"448\":1,\"455\":1}}],[\"be\",{\"1\":{\"412\":1,\"511\":1,\"520\":1,\"666\":1}}],[\"begin\",{\"1\":{\"285\":1}}],[\"beginning\",{\"1\":{\"143\":1}}],[\"beit在图像分类和语义分割等下游任务中表现优异\",{\"1\":{\"165\":1}}],[\"beit将图像表示为两种视图\",{\"1\":{\"165\":1}}],[\"beit\",{\"0\":{\"164\":1,\"172\":1},\"1\":{\"164\":2,\"165\":1,\"166\":5,\"167\":3,\"169\":1,\"173\":1,\"175\":1,\"200\":1}}],[\"bermanmaxim\",{\"1\":{\"406\":1}}],[\"bernoulli\",{\"0\":{\"575\":1},\"1\":{\"163\":3,\"575\":1}}],[\"bert支持的下游任务图\",{\"1\":{\"539\":1}}],[\"bertformultiplechoice\",{\"1\":{\"544\":2}}],[\"bertformaskedlm\",{\"1\":{\"160\":2}}],[\"bertfortokenclassification\",{\"1\":{\"543\":2}}],[\"bertforquestionanswering\",{\"1\":{\"540\":1,\"541\":2}}],[\"bertforpretraining结构图\",{\"1\":{\"538\":1}}],[\"bertforpretraining\",{\"0\":{\"538\":1},\"1\":{\"538\":2}}],[\"bertforsequenceclassification模型结构图\",{\"1\":{\"529\":1}}],[\"bertforsequenceclassification\",{\"0\":{\"529\":1},\"1\":{\"529\":2}}],[\"bertoutput\",{\"1\":{\"525\":3}}],[\"bertonlymlmhead\",{\"1\":{\"285\":1}}],[\"bertintermediate\",{\"1\":{\"525\":3}}],[\"bert文本分类实战\",{\"1\":{\"518\":1}}],[\"bertdataset\",{\"1\":{\"514\":1}}],[\"berttokenizer中的特殊token\",{\"1\":{\"520\":1}}],[\"berttokenizer\",{\"1\":{\"511\":1,\"520\":3,\"540\":1}}],[\"bert的mlm目标本身足够强大\",{\"1\":{\"500\":1}}],[\"bert原始设计未充分优化\",{\"1\":{\"500\":1}}],[\"bert原始设计存在优化空间\",{\"1\":{\"495\":1}}],[\"bert训练1m步\",{\"1\":{\"498\":1}}],[\"bert训练数据\",{\"1\":{\"498\":1}}],[\"bert使用256的批次大小\",{\"1\":{\"497\":1}}],[\"bert使用nsp任务\",{\"1\":{\"497\":1}}],[\"bert采用\",{\"1\":{\"495\":1}}],[\"bertembeddings\",{\"0\":{\"523\":1},\"1\":{\"284\":1,\"523\":2,\"528\":1}}],[\"bertembeddings会将text\",{\"1\":{\"284\":1}}],[\"bertencoder模型结构图\",{\"1\":{\"526\":1}}],[\"bertencoder\",{\"0\":{\"263\":1,\"524\":1,\"526\":1},\"1\":{\"263\":1,\"285\":2,\"526\":2,\"528\":1}}],[\"bertselfoutput计算流程图\",{\"1\":{\"532\":1}}],[\"bertselfoutput\",{\"0\":{\"532\":1},\"1\":{\"265\":1,\"532\":2,\"533\":1}}],[\"bertselfattention\",{\"0\":{\"266\":1,\"531\":1},\"1\":{\"162\":1,\"266\":1,\"285\":2,\"531\":2,\"533\":1}}],[\"bertattention计算流程图\",{\"1\":{\"533\":1}}],[\"bertattention\",{\"0\":{\"265\":1,\"530\":1,\"533\":1},\"1\":{\"265\":1,\"525\":1,\"533\":2}}],[\"bertlmpredictionhead结构图\",{\"1\":{\"536\":1}}],[\"bertlmpredictionhead\",{\"0\":{\"536\":1},\"1\":{\"268\":2,\"536\":2,\"537\":1}}],[\"bertlmheadmodel自回归语言建模实现\",{\"1\":{\"142\":1}}],[\"bertlmheadmodel\",{\"1\":{\"142\":1,\"147\":1,\"268\":2,\"285\":2}}],[\"bertlayer模型结构图\",{\"1\":{\"525\":1}}],[\"bertlayernorm\",{\"1\":{\"523\":1,\"525\":1,\"532\":1,\"535\":1}}],[\"bertlayer\",{\"0\":{\"264\":1,\"525\":1},\"1\":{\"263\":1,\"264\":1,\"285\":3,\"525\":2,\"526\":1}}],[\"bertpretrainingheads结构图\",{\"1\":{\"537\":1}}],[\"bertpretrainingheads\",{\"0\":{\"537\":1},\"1\":{\"537\":2,\"538\":1}}],[\"bertpretrainedmodel\",{\"1\":{\"262\":1,\"268\":1,\"285\":1,\"528\":1,\"529\":1,\"538\":1,\"541\":1,\"543\":1,\"544\":1}}],[\"bertpredictionheadtransform结构图\",{\"1\":{\"535\":1}}],[\"bertpredictionheadtransform\",{\"0\":{\"535\":1},\"1\":{\"268\":2,\"535\":2,\"536\":1}}],[\"bertpooler模型结构图\",{\"1\":{\"527\":1}}],[\"bertpooler\",{\"0\":{\"527\":1},\"1\":{\"262\":1,\"527\":2,\"528\":1}}],[\"bert属于\",{\"1\":{\"255\":1}}],[\"bertmodel模型结构图\",{\"1\":{\"528\":1}}],[\"bertmodel\",{\"0\":{\"262\":1,\"528\":1},\"1\":{\"145\":2,\"146\":1,\"147\":2,\"262\":1,\"268\":1,\"285\":2,\"528\":2,\"529\":1,\"531\":1,\"538\":1,\"541\":1,\"543\":1,\"544\":1}}],[\"bertconfig\",{\"1\":{\"142\":1,\"160\":2}}],[\"bertbase\",{\"1\":{\"131\":1,\"152\":1}}],[\"bert\",{\"0\":{\"261\":1,\"393\":1,\"503\":1,\"504\":1,\"518\":1},\"1\":{\"126\":2,\"147\":5,\"160\":3,\"161\":3,\"162\":2,\"163\":3,\"164\":2,\"166\":1,\"169\":1,\"231\":1,\"239\":2,\"240\":1,\"268\":3,\"282\":2,\"284\":1,\"285\":2,\"409\":1,\"454\":1,\"464\":1,\"485\":1,\"490\":1,\"492\":3,\"493\":7,\"494\":4,\"496\":1,\"504\":4,\"505\":2,\"507\":1,\"508\":4,\"511\":1,\"513\":4,\"514\":1,\"519\":16,\"520\":1,\"529\":2,\"538\":2,\"540\":5,\"541\":2,\"542\":9,\"543\":2,\"544\":7,\"673\":1}}],[\"before\",{\"1\":{\"148\":2,\"249\":1,\"339\":1,\"367\":1,\"372\":2}}],[\"bear\",{\"1\":{\"157\":1}}],[\"beam数量\",{\"1\":{\"286\":1}}],[\"beams=num\",{\"1\":{\"143\":1,\"286\":1}}],[\"beams=3\",{\"1\":{\"143\":1,\"286\":1}}],[\"beams\",{\"1\":{\"143\":3,\"286\":4}}],[\"beam\",{\"1\":{\"133\":3,\"143\":4,\"286\":1}}],[\"beat\",{\"1\":{\"29\":1}}],[\"best\",{\"1\":{\"82\":6,\"83\":1,\"410\":18,\"412\":8,\"514\":7}}],[\"beta=beta\",{\"1\":{\"405\":1,\"407\":1}}],[\"beta\",{\"1\":{\"405\":4}}],[\"beta得分\",{\"1\":{\"405\":1}}],[\"betas=\",{\"1\":{\"80\":1}}],[\"between\",{\"1\":{\"28\":2,\"531\":1}}],[\"bed\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"bad\",{\"1\":{\"686\":2,\"687\":3}}],[\"bayesian\",{\"1\":{\"596\":3}}],[\"bayes\",{\"0\":{\"596\":1},\"1\":{\"570\":2,\"596\":2}}],[\"bahdanau\",{\"1\":{\"469\":1}}],[\"ba\",{\"1\":{\"425\":1}}],[\"bashrc\",{\"1\":{\"339\":1}}],[\"bash\",{\"1\":{\"339\":2}}],[\"basemodeloutputwithpastandcrossattentions\",{\"1\":{\"285\":1,\"477\":1}}],[\"basemodeloutputwithpoolingandcrossattentions\",{\"1\":{\"262\":1}}],[\"basename\",{\"1\":{\"275\":1,\"277\":1}}],[\"base作为默认视觉编码器\",{\"1\":{\"142\":1}}],[\"base\",{\"1\":{\"142\":1,\"145\":1,\"146\":1,\"147\":4,\"244\":5,\"246\":3,\"290\":1,\"300\":7,\"325\":1,\"333\":1,\"334\":1,\"519\":6,\"674\":2}}],[\"based\",{\"1\":{\"112\":2,\"115\":1,\"286\":1,\"412\":1,\"470\":1}}],[\"baseballbat\",{\"1\":{\"29\":1}}],[\"baseline\",{\"1\":{\"22\":1}}],[\"bart\",{\"1\":{\"542\":1}}],[\"bar\",{\"1\":{\"289\":1}}],[\"baidu\",{\"1\":{\"289\":1,\"300\":1}}],[\"bank\",{\"1\":{\"242\":4}}],[\"ball\",{\"1\":{\"90\":4,\"92\":6,\"96\":2,\"98\":1}}],[\"batch数据准备\",{\"0\":{\"512\":1}}],[\"batch=1\",{\"1\":{\"323\":2}}],[\"batch和text\",{\"1\":{\"282\":1}}],[\"batchsize\",{\"1\":{\"107\":2,\"108\":1,\"111\":2,\"238\":1}}],[\"batches\",{\"1\":{\"81\":1,\"275\":2,\"277\":2,\"514\":3,\"558\":1}}],[\"batched\",{\"1\":{\"43\":1}}],[\"batchnorm2d\",{\"1\":{\"92\":1,\"96\":1}}],[\"batchnorm\",{\"1\":{\"46\":1,\"92\":1,\"109\":1,\"204\":1}}],[\"batchnorm1d\",{\"1\":{\"34\":2,\"35\":3,\"36\":3,\"41\":2,\"45\":1,\"46\":1,\"59\":6,\"93\":2,\"96\":2,\"98\":1,\"100\":2,\"101\":1,\"107\":5,\"109\":3,\"110\":2,\"111\":3}}],[\"batch\",{\"1\":{\"30\":8,\"34\":11,\"35\":3,\"40\":2,\"43\":7,\"45\":1,\"59\":4,\"70\":1,\"76\":2,\"78\":1,\"80\":3,\"82\":1,\"92\":13,\"107\":6,\"108\":2,\"127\":1,\"142\":2,\"145\":6,\"147\":2,\"156\":1,\"162\":1,\"238\":4,\"241\":3,\"242\":4,\"244\":6,\"247\":4,\"249\":6,\"275\":9,\"277\":9,\"282\":1,\"283\":2,\"284\":1,\"285\":1,\"286\":3,\"289\":4,\"290\":2,\"295\":10,\"315\":1,\"323\":3,\"328\":1,\"381\":1,\"401\":2,\"402\":1,\"403\":1,\"404\":3,\"470\":1,\"477\":4,\"495\":2,\"497\":1,\"512\":4,\"513\":14,\"514\":10,\"517\":7,\"519\":2,\"522\":4,\"531\":2,\"540\":6,\"541\":3,\"543\":2,\"544\":7,\"558\":2}}],[\"backprop为true时\",{\"1\":{\"658\":1}}],[\"backprop\",{\"1\":{\"658\":4}}],[\"backpack\",{\"1\":{\"29\":1,\"226\":1}}],[\"background\",{\"1\":{\"83\":1}}],[\"back\",{\"1\":{\"83\":3}}],[\"backward方法\",{\"1\":{\"658\":1}}],[\"backward\",{\"0\":{\"652\":1},\"1\":{\"81\":1,\"142\":1,\"145\":1,\"147\":1,\"159\":1,\"244\":1,\"296\":1,\"514\":1,\"630\":1,\"631\":1,\"632\":3,\"635\":3,\"638\":2,\"642\":1,\"645\":2,\"646\":1,\"651\":1,\"652\":4,\"654\":2,\"655\":2,\"656\":2,\"657\":1,\"658\":3,\"660\":6,\"662\":3,\"666\":1,\"667\":3}}],[\"backbone\",{\"1\":{\"22\":2,\"70\":3,\"160\":1}}],[\"bag\",{\"1\":{\"29\":1,\"58\":1,\"68\":1,\"278\":3}}],[\"b\",{\"1\":{\"6\":1,\"17\":1,\"24\":1,\"30\":4,\"32\":2,\"35\":2,\"40\":15,\"41\":11,\"43\":1,\"45\":28,\"46\":10,\"49\":2,\"58\":2,\"59\":72,\"70\":9,\"72\":2,\"73\":2,\"74\":2,\"75\":2,\"76\":17,\"78\":2,\"82\":3,\"92\":56,\"93\":2,\"96\":11,\"98\":4,\"100\":33,\"101\":7,\"107\":3,\"109\":4,\"111\":7,\"131\":4,\"145\":39,\"147\":15,\"152\":1,\"160\":7,\"161\":15,\"162\":6,\"163\":2,\"188\":1,\"189\":1,\"212\":1,\"255\":1,\"272\":2,\"280\":2,\"282\":8,\"283\":17,\"284\":4,\"286\":3,\"290\":3,\"291\":8,\"292\":7,\"293\":5,\"295\":3,\"296\":5,\"297\":1,\"300\":2,\"327\":6,\"353\":2,\"355\":1,\"370\":2,\"371\":2,\"401\":4,\"402\":4,\"403\":4,\"404\":4,\"407\":1,\"425\":3,\"444\":1,\"448\":1,\"470\":1,\"474\":2,\"499\":1,\"511\":5,\"512\":20,\"513\":1,\"517\":1,\"520\":1,\"544\":1,\"565\":2,\"617\":4,\"632\":3,\"655\":2,\"657\":6,\"660\":14}}],[\"如多层嵌套\",{\"1\":{\"662\":1}}],[\"如z\",{\"1\":{\"662\":1}}],[\"如sub\",{\"1\":{\"662\":1}}],[\"如y\",{\"1\":{\"660\":1}}],[\"如减法\",{\"1\":{\"660\":1}}],[\"如模型预测\",{\"1\":{\"658\":2}}],[\"如模型深度\",{\"1\":{\"189\":1}}],[\"如平方函数\",{\"1\":{\"630\":1}}],[\"如自注意力层和前馈层\",{\"1\":{\"548\":1}}],[\"如自然语言推理\",{\"1\":{\"463\":1,\"493\":1}}],[\"如句子\",{\"1\":{\"548\":1}}],[\"如句子长度\",{\"1\":{\"305\":1}}],[\"如判断哪个是答案的开始\",{\"1\":{\"542\":1}}],[\"如上图所示\",{\"1\":{\"656\":1}}],[\"如上图\",{\"1\":{\"508\":1}}],[\"如上描述是我们通常认知的gpt推理过程\",{\"1\":{\"474\":1}}],[\"如xlnet使用10倍于bert的数据\",{\"1\":{\"501\":1}}],[\"如xlnet\",{\"1\":{\"501\":1}}],[\"如8k\",{\"1\":{\"495\":1}}],[\"如800×1300\",{\"1\":{\"216\":1}}],[\"如对比\",{\"1\":{\"494\":1}}],[\"如对抗性推理\",{\"1\":{\"462\":1}}],[\"如动态掩码\",{\"1\":{\"493\":1}}],[\"如延长训练时间\",{\"1\":{\"492\":1}}],[\"如glue\",{\"1\":{\"491\":1}}],[\"如gpt\",{\"1\":{\"184\":1,\"208\":1,\"220\":1,\"455\":1,\"460\":1,\"468\":1,\"480\":1}}],[\"如wikipedia引用过滤\",{\"1\":{\"484\":1}}],[\"如word2vec\",{\"1\":{\"240\":1,\"453\":1}}],[\"如后续alpaca\",{\"1\":{\"483\":1}}],[\"如象棋开局策略分析\",{\"1\":{\"483\":1}}],[\"如html标签清理的正则表达式\",{\"1\":{\"483\":1}}],[\"如a\",{\"1\":{\"657\":1,\"660\":1}}],[\"如astronomy\",{\"1\":{\"483\":1}}],[\"如and\",{\"1\":{\"395\":1}}],[\"如boolq\",{\"1\":{\"482\":1}}],[\"如bert\",{\"1\":{\"453\":1,\"456\":1,\"464\":1}}],[\"如bert和uniter\",{\"1\":{\"256\":1}}],[\"如bed\",{\"1\":{\"68\":1}}],[\"如小模型长期训练\",{\"1\":{\"481\":1}}],[\"如标点缺失\",{\"1\":{\"481\":1}}],[\"如针对少数群体敏感任务\",{\"1\":{\"472\":1}}],[\"如非英语任务\",{\"1\":{\"472\":1}}],[\"如法语\",{\"1\":{\"471\":1}}],[\"如法线\",{\"1\":{\"92\":1}}],[\"如选择\",{\"1\":{\"471\":1}}],[\"如是否\",{\"1\":{\"470\":1}}],[\"如创作\",{\"1\":{\"470\":1}}],[\"如摘要\",{\"1\":{\"469\":1}}],[\"如医疗\",{\"1\":{\"463\":1}}],[\"如医学图像\",{\"1\":{\"407\":1}}],[\"如ner\",{\"1\":{\"463\":1}}],[\"如roberta训练500k步\",{\"1\":{\"495\":1}}],[\"如rte和anli\",{\"1\":{\"462\":1}}],[\"如rag\",{\"1\":{\"462\":1}}],[\"如retinanet\",{\"1\":{\"404\":1}}],[\"如relu\",{\"1\":{\"395\":2}}],[\"如resnet\",{\"1\":{\"183\":1,\"188\":1}}],[\"如结合双向架构或多模态训练\",{\"1\":{\"460\":1}}],[\"如结合clip和dinov2\",{\"1\":{\"212\":1}}],[\"如计算成本\",{\"1\":{\"460\":1}}],[\"如词向量\",{\"1\":{\"460\":1}}],[\"如词汇蕴含\",{\"1\":{\"448\":1}}],[\"如偏见和能源消耗\",{\"1\":{\"459\":1}}],[\"如在glue等基准上的表现\",{\"1\":{\"456\":1}}],[\"如用文档中的人名回答\",{\"1\":{\"455\":1}}],[\"如perspectiveapi过滤\",{\"1\":{\"484\":1}}],[\"如ptb\",{\"1\":{\"455\":1,\"462\":1}}],[\"如pos\",{\"1\":{\"441\":1}}],[\"如翻译后的句子\",{\"1\":{\"548\":1}}],[\"如翻译对\",{\"1\":{\"454\":1,\"456\":1}}],[\"如翻译\",{\"1\":{\"454\":1}}],[\"如elmo\",{\"1\":{\"453\":1}}],[\"如问答时生成而非抽取答案\",{\"1\":{\"456\":1}}],[\"如问答对\",{\"1\":{\"454\":1}}],[\"如问答\",{\"1\":{\"450\":1,\"452\":1,\"453\":1}}],[\"如有序句子对\",{\"1\":{\"445\":1}}],[\"如从词级别信息到更高的\",{\"1\":{\"441\":1}}],[\"如作者在实验中证明的\",{\"1\":{\"440\":1}}],[\"如机翻\",{\"1\":{\"440\":1}}],[\"如语言建模\",{\"1\":{\"501\":1}}],[\"如语言模型\",{\"1\":{\"440\":1}}],[\"如语义分割\",{\"1\":{\"189\":1}}],[\"如tensorflow\",{\"1\":{\"662\":1}}],[\"如textvqa\",{\"1\":{\"208\":1}}],[\"如t转置\",{\"1\":{\"659\":1}}],[\"如tinypytorch中每一步运算都会动态创建计算图链接\",{\"1\":{\"662\":1}}],[\"如tinypytorch框架\",{\"1\":{\"661\":1}}],[\"如tinypytorch\",{\"1\":{\"657\":1}}],[\"如triviaqa\",{\"1\":{\"482\":1}}],[\"如transformer\",{\"1\":{\"460\":1}}],[\"如tversky\",{\"1\":{\"408\":1}}],[\"如正样本少则增大\",{\"1\":{\"404\":1}}],[\"如欺诈检测\",{\"1\":{\"404\":1}}],[\"如功能区域边缘不确定性较高\",{\"1\":{\"403\":1}}],[\"如经过\",{\"1\":{\"401\":1}}],[\"如经过下采样后的点\",{\"1\":{\"100\":1}}],[\"如池化\",{\"1\":{\"395\":1}}],[\"如边缘→纹理→物体\",{\"1\":{\"395\":1}}],[\"如残差块\",{\"1\":{\"395\":1}}],[\"如卷积核\",{\"1\":{\"395\":1}}],[\"如大数相减损失精度\",{\"1\":{\"395\":1}}],[\"如使用relu\",{\"1\":{\"395\":1}}],[\"如泰勒展开\",{\"1\":{\"395\":2}}],[\"如构造重复输入\",{\"1\":{\"387\":1}}],[\"如列表\",{\"1\":{\"381\":1}}],[\"如身高标准化后波动在\",{\"1\":{\"359\":1}}],[\"如年龄\",{\"1\":{\"359\":1}}],[\"如需进一步帮助\",{\"1\":{\"335\":1}}],[\"如需了解各类优化手段\",{\"1\":{\"261\":1}}],[\"如此往复循环\",{\"1\":{\"328\":1}}],[\"如此反复\",{\"1\":{\"299\":1}}],[\"如加法\",{\"1\":{\"327\":1}}],[\"如下\",{\"1\":{\"586\":1}}],[\"如下表2\",{\"1\":{\"449\":1}}],[\"如下例所示\",{\"1\":{\"323\":2}}],[\"如下所示\",{\"1\":{\"321\":1,\"325\":1,\"600\":1}}],[\"如下图所示\",{\"1\":{\"294\":1,\"327\":1,\"423\":1,\"656\":1,\"667\":1}}],[\"如交叉熵损失\",{\"1\":{\"292\":1}}],[\"如局部性和平移不变性\",{\"1\":{\"287\":1}}],[\"如variable与数值\",{\"1\":{\"660\":1}}],[\"如vse++\",{\"1\":{\"280\":1}}],[\"如vilt\",{\"1\":{\"280\":1}}],[\"如vilbert和lxmert\",{\"1\":{\"256\":1}}],[\"如vicuna\",{\"1\":{\"188\":1}}],[\"如vit\",{\"1\":{\"183\":2,\"188\":1,\"286\":1}}],[\"如function和临时variable\",{\"1\":{\"657\":1}}],[\"如f\",{\"1\":{\"657\":1}}],[\"如faster\",{\"1\":{\"254\":1}}],[\"如flamingo\",{\"1\":{\"185\":1}}],[\"如基于\",{\"1\":{\"253\":1}}],[\"如分割\",{\"1\":{\"237\":1}}],[\"如分类\",{\"1\":{\"166\":1}}],[\"如检测\",{\"1\":{\"233\":1}}],[\"如检索\",{\"1\":{\"188\":1,\"189\":1}}],[\"如mae和beit\",{\"1\":{\"278\":1}}],[\"如moco和simclr\",{\"1\":{\"278\":1}}],[\"如moco\",{\"1\":{\"240\":1}}],[\"如mmmu\",{\"1\":{\"221\":1}}],[\"如ms\",{\"1\":{\"178\":1}}],[\"如65b毒性最高\",{\"1\":{\"484\":1}}],[\"如6b参数\",{\"1\":{\"221\":1}}],[\"如68\",{\"1\":{\"52\":1}}],[\"如224×224\",{\"1\":{\"215\":1}}],[\"如256\",{\"1\":{\"161\":1}}],[\"如40个epoch\",{\"1\":{\"495\":1}}],[\"如448×448图像对应256个token\",{\"1\":{\"214\":1}}],[\"如48\",{\"1\":{\"51\":1}}],[\"如dot\",{\"1\":{\"666\":1}}],[\"如docvqa\",{\"1\":{\"221\":1}}],[\"如drop\",{\"1\":{\"462\":1}}],[\"如dropout\",{\"1\":{\"395\":1}}],[\"如draw模型\",{\"1\":{\"178\":1}}],[\"如deepseek\",{\"1\":{\"212\":1}}],[\"如文本生成\",{\"1\":{\"285\":1}}],[\"如文本或图像\",{\"1\":{\"7\":1}}],[\"如文档图像\",{\"1\":{\"215\":1}}],[\"如文档\",{\"1\":{\"212\":1}}],[\"如ureader\",{\"1\":{\"211\":1}}],[\"如lambada\",{\"1\":{\"455\":1}}],[\"如lay\",{\"1\":{\"68\":1}}],[\"如llava\",{\"1\":{\"211\":1}}],[\"如低分辨率用于场景描述\",{\"1\":{\"208\":1}}],[\"如ocr和中文场景理解\",{\"1\":{\"208\":1}}],[\"如33b模型在1\",{\"1\":{\"482\":1}}],[\"如336×336或448×448\",{\"1\":{\"208\":1,\"211\":1}}],[\"如34b参数\",{\"1\":{\"221\":1}}],[\"如36\",{\"1\":{\"51\":1}}],[\"如重复文本\",{\"1\":{\"190\":1}}],[\"如零样本图像分类和检索\",{\"1\":{\"189\":1}}],[\"如描述\",{\"1\":{\"188\":1}}],[\"如core\",{\"1\":{\"661\":1}}],[\"如coqa\",{\"1\":{\"462\":1}}],[\"如clip\",{\"1\":{\"188\":2,\"280\":1}}],[\"如chatglm\",{\"1\":{\"184\":1}}],[\"如kosmos\",{\"1\":{\"185\":1}}],[\"如qformer\",{\"1\":{\"185\":1}}],[\"如qformer或线性投影\",{\"1\":{\"181\":1,\"189\":1}}],[\"如知识蒸馏\",{\"1\":{\"163\":1}}],[\"如论文中的图\",{\"1\":{\"157\":1}}],[\"如方差\",{\"1\":{\"115\":1}}],[\"如时间序列点云\",{\"1\":{\"112\":1}}],[\"如只有几十个点\",{\"1\":{\"112\":1}}],[\"如弯曲\",{\"1\":{\"112\":1}}],[\"如人体姿态变化\",{\"1\":{\"112\":1}}],[\"如椅子腿和桌面连接处\",{\"1\":{\"112\":1}}],[\"如旋转\",{\"1\":{\"104\":1,\"107\":1,\"108\":1}}],[\"如最大池化\",{\"1\":{\"103\":1}}],[\"如桌子边缘\",{\"1\":{\"112\":1}}],[\"如桌子\",{\"1\":{\"98\":1,\"101\":1}}],[\"如何获取答案\",{\"1\":{\"542\":1}}],[\"如何利用\",{\"1\":{\"540\":1}}],[\"如何生成并起作用的\",{\"0\":{\"517\":1}}],[\"如何生成点集的划分\",{\"1\":{\"86\":1}}],[\"如何建立更强的\",{\"1\":{\"472\":1}}],[\"如何应对多价值体系的冲突\",{\"1\":{\"472\":1}}],[\"如何更有效缓解毒性与偏见\",{\"1\":{\"472\":1}}],[\"如何写好prompt\",{\"0\":{\"431\":1}}],[\"如何对大模型进行微调\",{\"0\":{\"416\":1}}],[\"如何选择\",{\"0\":{\"408\":1}}],[\"如何通过调整步幅\",{\"1\":{\"327\":1}}],[\"如何理解这个过程\",{\"0\":{\"311\":1}}],[\"如何计算loss的\",{\"1\":{\"283\":1}}],[\"如何降低模型训练成本\",{\"1\":{\"280\":1}}],[\"如何将不同尺寸的roi特征\",{\"1\":{\"396\":1}}],[\"如何将二维图像转换为一维时间序列\",{\"1\":{\"301\":1}}],[\"如何将这个预训练的视觉模型应用到新的任务中呢\",{\"1\":{\"273\":1}}],[\"如何将其与视觉模态结合成为关键挑战\",{\"1\":{\"184\":1}}],[\"如何设计一个能够从这些局部分区中学习有用特征的机制\",{\"1\":{\"86\":1}}],[\"如何有效地对点云进行分区\",{\"1\":{\"86\":1}}],[\"如1e\",{\"1\":{\"497\":1}}],[\"如13b\",{\"1\":{\"482\":1}}],[\"如13b模型比gpt\",{\"1\":{\"480\":1}}],[\"如130\",{\"1\":{\"454\":1}}],[\"如10\",{\"1\":{\"453\":1}}],[\"如16×16像素的局部区域\",{\"1\":{\"165\":1}}],[\"如1\",{\"1\":{\"52\":1,\"404\":1,\"666\":1}}],[\"如12\",{\"1\":{\"51\":1}}],[\"如11\",{\"1\":{\"49\":1,\"51\":1}}],[\"如表4所示\",{\"1\":{\"495\":1}}],[\"如表3所示\",{\"1\":{\"495\":1}}],[\"如表20所示\",{\"1\":{\"200\":1}}],[\"如表2所示\",{\"1\":{\"7\":1,\"23\":1,\"495\":1}}],[\"如表\",{\"1\":{\"136\":1,\"190\":1,\"492\":3,\"493\":2}}],[\"如表1所示\",{\"1\":{\"51\":1,\"495\":1}}],[\"如果左操作数未实现\",{\"1\":{\"660\":1}}],[\"如果存在多个function先后作用在x上\",{\"1\":{\"626\":1}}],[\"如果存在function作用在x上\",{\"1\":{\"626\":1}}],[\"如果存在记录出现次数\",{\"1\":{\"411\":1}}],[\"如果其中\",{\"1\":{\"600\":1}}],[\"如果变量是多值离散型的\",{\"1\":{\"576\":1}}],[\"如果没病\",{\"1\":{\"569\":1}}],[\"如果没有function作用于x上\",{\"1\":{\"626\":1}}],[\"如果没有任何归纳偏置\",{\"1\":{\"287\":1}}],[\"如果没有则下载\",{\"1\":{\"275\":1,\"277\":1}}],[\"如果没有这个字典\",{\"1\":{\"238\":1}}],[\"如果没有\",{\"1\":{\"107\":1}}],[\"如果没有提供激活函数层\",{\"1\":{\"296\":1}}],[\"如果没有提供归一化层\",{\"1\":{\"296\":1}}],[\"如果没有提供\",{\"1\":{\"43\":1}}],[\"如果两个事件是无关的\",{\"1\":{\"568\":1}}],[\"如果两个变量\",{\"1\":{\"355\":2}}],[\"如果每个\",{\"1\":{\"565\":1}}],[\"如果正确答案没有出现在上下文中\",{\"1\":{\"542\":1}}],[\"如果值大于\",{\"1\":{\"541\":1}}],[\"如果值小于\",{\"1\":{\"541\":1}}],[\"如果本地不存在\",{\"1\":{\"519\":1}}],[\"如果遇到不存在于字典中的word\",{\"1\":{\"511\":1}}],[\"如果任一句子长度超过50\",{\"1\":{\"511\":1}}],[\"如果现在的任务是\",{\"1\":{\"508\":4}}],[\"如果现在有这么一句话\",{\"1\":{\"506\":1}}],[\"如果给的例子是比较简单的问题\",{\"1\":{\"436\":1}}],[\"如果给定随机选择的正例和负例\",{\"1\":{\"351\":1}}],[\"如果矩阵\",{\"1\":{\"426\":1}}],[\"如果需要\",{\"1\":{\"423\":1}}],[\"如果要用lora适配不同的场景\",{\"1\":{\"420\":1}}],[\"如果将大模型比做一个函数\",{\"1\":{\"418\":1}}],[\"如果数据是不能传递给第三方大模型服务的\",{\"1\":{\"415\":1}}],[\"如果数据集在类别之间大致平衡\",{\"1\":{\"352\":1}}],[\"如果数据集不平衡\",{\"1\":{\"343\":1}}],[\"如果已经包含\",{\"1\":{\"402\":1,\"403\":1}}],[\"如果为\",{\"1\":{\"389\":2,\"405\":1,\"407\":1}}],[\"如果张量来源于\",{\"1\":{\"385\":1}}],[\"如果一个变量变大时另一个变小\",{\"1\":{\"355\":1}}],[\"如果费用大致相当\",{\"1\":{\"353\":1}}],[\"如果假正例成本较低\",{\"1\":{\"353\":1}}],[\"如果假正例\",{\"1\":{\"353\":1}}],[\"如果忽略所有其他阈值\",{\"1\":{\"350\":1}}],[\"如果您想评估模型在所有可能阈值下的质量\",{\"1\":{\"349\":1}}],[\"如果模型预测\",{\"1\":{\"542\":1}}],[\"如果模型最后一层没有\",{\"1\":{\"405\":1}}],[\"如果模型最后没有\",{\"1\":{\"402\":1,\"403\":1,\"404\":1}}],[\"如果模型的效果与随机猜测或抛硬币的效果完全一样\",{\"1\":{\"351\":1}}],[\"如果模型\",{\"1\":{\"343\":1}}],[\"如果模型过度关注这个困难的负样本\",{\"1\":{\"240\":1}}],[\"如果实际正例的总数与实际负例的总数不接近\",{\"1\":{\"342\":1}}],[\"如果原始数据是按行优先方式存储的\",{\"1\":{\"326\":1}}],[\"如果原始点有自己的特征\",{\"1\":{\"100\":1}}],[\"如果原始点有特征\",{\"1\":{\"100\":1}}],[\"如果类别数大于\",{\"1\":{\"296\":1}}],[\"如果未激活任何环境时使用\",{\"1\":{\"338\":1}}],[\"如果未指定\",{\"1\":{\"294\":2}}],[\"如果未提供位置id\",{\"1\":{\"284\":1}}],[\"如果传入了归一化层类\",{\"1\":{\"291\":1}}],[\"如果传入的是整数\",{\"1\":{\"291\":1}}],[\"如果传入一个归一化层类\",{\"1\":{\"291\":1}}],[\"如果传入一个整数\",{\"1\":{\"291\":2}}],[\"如果定义了图像预处理转换操作\",{\"1\":{\"289\":1}}],[\"如果该路径在采样的验证集样本中则存入验证集\",{\"1\":{\"289\":1}}],[\"如果进一步采用convirt\",{\"1\":{\"278\":1}}],[\"如果在读取图片过程中出现错误\",{\"1\":{\"275\":1}}],[\"如果有第i个元素\",{\"1\":{\"652\":1}}],[\"如果有病\",{\"1\":{\"569\":1}}],[\"如果有一个能直接处理各式\",{\"1\":{\"504\":1}}],[\"如果有缓存\",{\"1\":{\"285\":1}}],[\"如果有缓存的key\",{\"1\":{\"285\":1}}],[\"如果有个类别\",{\"1\":{\"273\":1}}],[\"如果有额外特征\",{\"1\":{\"92\":1,\"96\":1}}],[\"如果有额外的点特征\",{\"1\":{\"92\":1}}],[\"如果你能谈一堆事件\",{\"1\":{\"566\":1}}],[\"如果你能谈某个事件\",{\"1\":{\"566\":1}}],[\"如果你希望模型能\",{\"1\":{\"542\":1}}],[\"如果你希望装饰器\",{\"1\":{\"371\":1}}],[\"如果你设置\",{\"1\":{\"405\":2}}],[\"如果你的模型最后没有\",{\"1\":{\"401\":1}}],[\"如果你接下来要对它们执行\",{\"1\":{\"383\":1}}],[\"如果你想把一个\",{\"1\":{\"381\":1}}],[\"如果你想删除某个环境\",{\"1\":{\"335\":1}}],[\"如果你有\",{\"1\":{\"355\":1}}],[\"如果你有一张人脸的点云\",{\"1\":{\"112\":1}}],[\"如果你在多\",{\"1\":{\"249\":1}}],[\"如果第二个分支一直不变\",{\"1\":{\"241\":1}}],[\"如果选取的抽样部分过少\",{\"1\":{\"240\":1}}],[\"如果把这个损失函数\",{\"1\":{\"240\":1}}],[\"如果把对比学习的过程看成一个动态字典的过程\",{\"1\":{\"238\":1}}],[\"如果字典很小\",{\"1\":{\"238\":1}}],[\"如果采用\",{\"1\":{\"237\":1}}],[\"如果是全参数微调\",{\"1\":{\"425\":1}}],[\"如果是解决自己日常生活\",{\"1\":{\"415\":1}}],[\"如果是\",{\"1\":{\"339\":2,\"425\":1}}],[\"如果是beam\",{\"1\":{\"286\":1}}],[\"如果是个体判别任务\",{\"1\":{\"235\":1}}],[\"如果是分割任务\",{\"1\":{\"109\":2}}],[\"如果是分类任务\",{\"1\":{\"109\":2}}],[\"如果我们不对这两个梯度求和\",{\"1\":{\"654\":1}}],[\"如果我们不进行累加\",{\"1\":{\"654\":1}}],[\"如果我们从\",{\"1\":{\"591\":1}}],[\"如果我们用多种不同的方法去求解\",{\"1\":{\"435\":1}}],[\"如果我们画出一个二维正态分布\",{\"1\":{\"355\":1}}],[\"如果我们将标准答案作为列\",{\"1\":{\"342\":1}}],[\"如果我们做如下切片操作\",{\"1\":{\"325\":1}}],[\"如果我们直接使用类别标签作为文本描述\",{\"1\":{\"274\":1}}],[\"如果我们有一个没有标注的数据集\",{\"1\":{\"235\":1}}],[\"如果我本来就有特征\",{\"1\":{\"100\":1}}],[\"如果二者参数共享\",{\"1\":{\"134\":1}}],[\"如果共享\",{\"1\":{\"134\":1}}],[\"如果完全不共享参数\",{\"1\":{\"134\":1}}],[\"如果对于任意排列\",{\"1\":{\"115\":1}}],[\"如果训练过程中加入了噪声\",{\"1\":{\"112\":1}}],[\"如果点太少\",{\"1\":{\"112\":1}}],[\"如果这些关键点缺失或被遮挡\",{\"1\":{\"112\":1}}],[\"如果直接作为变换矩阵\",{\"1\":{\"107\":1}}],[\"如果不是则抛出异常\",{\"1\":{\"289\":1}}],[\"如果不加处理\",{\"1\":{\"107\":1}}],[\"如果不够就重复最近的点来填充\",{\"1\":{\"92\":1}}],[\"如果只有1个下采样点\",{\"1\":{\"100\":1}}],[\"如果只用\",{\"1\":{\"76\":1}}],[\"如果\",{\"1\":{\"92\":1,\"112\":2,\"236\":1,\"240\":2,\"407\":2,\"514\":2,\"540\":1,\"568\":1,\"575\":1,\"576\":1,\"580\":1,\"584\":1,\"596\":2}}],[\"如果某个维度的数值范围很大\",{\"1\":{\"359\":1}}],[\"如果某个查询点附近的点太少\",{\"1\":{\"92\":1}}],[\"如果某个点到新中心点的距离比之前记录的\",{\"1\":{\"92\":1}}],[\"如果包含强度值\",{\"1\":{\"83\":1}}],[\"如果当前\",{\"1\":{\"82\":1}}],[\"如果当前样本没有正类\",{\"1\":{\"82\":1}}],[\"如果当前是训练模式\",{\"1\":{\"68\":1}}],[\"如果当前物体类别在排序后的字典中\",{\"1\":{\"68\":1}}],[\"如果当前物体类别不在排序后的字典中\",{\"1\":{\"68\":1}}],[\"如果使用多头注意力\",{\"1\":{\"315\":1}}],[\"如果使用\",{\"1\":{\"40\":1,\"143\":1}}],[\"如54\",{\"1\":{\"49\":1,\"51\":1}}],[\"如5\",{\"1\":{\"7\":1}}],[\"如图7\",{\"1\":{\"178\":1}}],[\"如图\",{\"1\":{\"166\":1,\"191\":2,\"461\":2,\"469\":1,\"597\":1}}],[\"如图2\",{\"1\":{\"49\":1,\"52\":1,\"178\":1}}],[\"如图2所示\",{\"1\":{\"8\":1,\"181\":1}}],[\"如图4所示\",{\"1\":{\"23\":1}}],[\"如图3所示\",{\"1\":{\"54\":1,\"188\":1}}],[\"如图3\",{\"1\":{\"17\":1,\"19\":1}}],[\"如图像转置\",{\"1\":{\"382\":1}}],[\"如图像重建\",{\"1\":{\"240\":1}}],[\"如图像描述\",{\"1\":{\"189\":1}}],[\"如图像分类和语义分割\",{\"1\":{\"167\":1}}],[\"如图像或点云特征\",{\"1\":{\"43\":1}}],[\"如图像\",{\"1\":{\"7\":1,\"43\":1}}],[\"如图1c所示\",{\"1\":{\"181\":1}}],[\"如图15所示\",{\"1\":{\"56\":1}}],[\"如图1所示\",{\"1\":{\"7\":1,\"167\":1,\"178\":1,\"188\":1,\"481\":1}}],[\"如图1\",{\"1\":{\"6\":1,\"49\":1}}],[\"如\",{\"1\":{\"6\":3,\"23\":1,\"24\":1,\"28\":5,\"29\":1,\"32\":4,\"45\":1,\"46\":3,\"49\":1,\"63\":1,\"67\":2,\"71\":1,\"78\":1,\"82\":1,\"100\":2,\"112\":3,\"115\":1,\"126\":1,\"128\":1,\"132\":1,\"147\":1,\"157\":1,\"159\":1,\"160\":2,\"163\":1,\"178\":1,\"190\":2,\"191\":1,\"193\":1,\"195\":1,\"196\":1,\"207\":2,\"224\":5,\"226\":3,\"227\":2,\"231\":1,\"253\":1,\"263\":1,\"285\":2,\"290\":1,\"294\":1,\"327\":1,\"338\":2,\"355\":1,\"384\":1,\"387\":1,\"395\":5,\"397\":1,\"404\":5,\"410\":1,\"425\":1,\"428\":2,\"439\":1,\"454\":3,\"455\":1,\"464\":4,\"469\":1,\"471\":1,\"472\":5,\"480\":1,\"492\":1,\"493\":2,\"494\":1,\"660\":1,\"666\":1,\"670\":1}}],[\"如int\",{\"1\":{\"660\":1}}],[\"如internvit\",{\"1\":{\"212\":1,\"217\":1}}],[\"如internvl\",{\"1\":{\"6\":1,\"7\":1}}],[\"如imagenet\",{\"1\":{\"183\":1,\"278\":1,\"392\":1}}],[\"如iagnet\",{\"1\":{\"6\":1}}],[\"如手柄的抓握属性\",{\"1\":{\"6\":1}}],[\"如将\",{\"1\":{\"6\":1,\"7\":1}}],[\"现已更新到\",{\"1\":{\"674\":1}}],[\"现实中的神经网络操作往往不仅仅接受一个输入\",{\"1\":{\"651\":1}}],[\"现代深度学习的延伸\",{\"1\":{\"395\":1}}],[\"现在我们重新定义抽红球为\",{\"1\":{\"579\":1}}],[\"现在我们问\",{\"1\":{\"565\":1}}],[\"现在我们按\",{\"1\":{\"326\":1}}],[\"现在假设使用\",{\"1\":{\"508\":1}}],[\"现在的大模型要解决的问题\",{\"1\":{\"414\":1}}],[\"现在看一个典型的装饰器例子\",{\"1\":{\"367\":1}}],[\"现状问题\",{\"1\":{\"122\":1}}],[\"现有的vlp模型的text\",{\"1\":{\"256\":1}}],[\"现有的视觉语言模型的三种结构类别\",{\"1\":{\"255\":1}}],[\"现有的视觉\",{\"1\":{\"181\":1}}],[\"现有\",{\"1\":{\"215\":1}}],[\"现有工作表明\",{\"1\":{\"186\":1}}],[\"现有做法\",{\"1\":{\"123\":1,\"124\":1}}],[\"现有研究通常仅联合训练少量任务\",{\"1\":{\"453\":1}}],[\"现有研究可分为三类\",{\"1\":{\"51\":1}}],[\"现有研究主要分为两类\",{\"1\":{\"49\":1}}],[\"现有研究主要从2d数据\",{\"1\":{\"7\":1}}],[\"现有方法的局限性\",{\"1\":{\"460\":1}}],[\"现有方法仍需要针对每个任务进行大规模监督数据微调\",{\"1\":{\"460\":1}}],[\"现有方法依赖两类对齐策略\",{\"1\":{\"52\":1}}],[\"现有方法依赖数据对齐\",{\"1\":{\"7\":1}}],[\"现有方法\",{\"1\":{\"6\":1}}],[\"现就读于电子科技大学\",{\"1\":{\"3\":1}}],[\"现就读于四川大学\",{\"1\":{\"2\":1}}],[\"实时对话\",{\"1\":{\"674\":1}}],[\"实时语音和视频对话\",{\"1\":{\"674\":1}}],[\"实体嵌入\",{\"1\":{\"501\":1}}],[\"实质是一种\",{\"1\":{\"464\":1}}],[\"实线为\",{\"1\":{\"397\":1}}],[\"实例说明\",{\"1\":{\"395\":1}}],[\"实例化验证数据集\",{\"1\":{\"290\":1}}],[\"实例化训练数据集\",{\"1\":{\"290\":1}}],[\"实际任务训练\",{\"1\":{\"670\":1}}],[\"实际此处的函数放缩因子也称为函数的导数\",{\"1\":{\"626\":1}}],[\"实际实现过程中\",{\"1\":{\"511\":1}}],[\"实际操作如下\",{\"1\":{\"505\":1}}],[\"实际对齐的是训练流程中的多重人为偏好叠加\",{\"1\":{\"472\":1}}],[\"实际复制\",{\"1\":{\"387\":1}}],[\"实际负例\",{\"1\":{\"342\":1}}],[\"实际正例\",{\"1\":{\"342\":1}}],[\"实际创建一个新的数据缓冲区\",{\"1\":{\"326\":1}}],[\"实际携带信息\",{\"1\":{\"310\":1}}],[\"实际取6\",{\"1\":{\"178\":1}}],[\"实际采集的点云常有遮挡\",{\"1\":{\"104\":1}}],[\"实际上梯度下降法并不擅长处理rosenbrock这种类型的函数\",{\"1\":{\"667\":1}}],[\"实际上也体现了对\",{\"1\":{\"469\":1}}],[\"实际上变成了\",{\"1\":{\"372\":1}}],[\"实际上并没有复制内存中的任何数据值\",{\"1\":{\"325\":1}}],[\"实际上没用\",{\"1\":{\"323\":1}}],[\"实际上没有局部的概念\",{\"1\":{\"86\":1}}],[\"实际上\",{\"1\":{\"278\":1}}],[\"实际上是一个很薄的壳层或环带\",{\"1\":{\"591\":1}}],[\"实际上是一个动态生成的卷积核\",{\"1\":{\"76\":1}}],[\"实际上是用一个固定大小的全局特征去\",{\"1\":{\"112\":1}}],[\"实验在三大基准任务上进行\",{\"1\":{\"494\":1}}],[\"实验步骤\",{\"0\":{\"494\":1}}],[\"实验覆盖了翻译\",{\"1\":{\"460\":1}}],[\"实验结论\",{\"1\":{\"455\":1}}],[\"实验结果如下表所示\",{\"1\":{\"293\":1}}],[\"实验结果与表现\",{\"1\":{\"120\":1}}],[\"实验结果表明\",{\"1\":{\"48\":1,\"207\":1,\"479\":1,\"491\":1,\"495\":1}}],[\"实验的关键发现是\",{\"1\":{\"455\":1}}],[\"实验设计与模型配置\",{\"1\":{\"455\":1}}],[\"实验设置与模型规模\",{\"1\":{\"454\":1}}],[\"实验评估\",{\"1\":{\"408\":1}}],[\"实验采用的是花蕊数据集\",{\"1\":{\"289\":1}}],[\"实验采用以下评估指标评估\",{\"1\":{\"22\":1}}],[\"实验发现\",{\"1\":{\"189\":1}}],[\"实验效果\",{\"1\":{\"178\":1}}],[\"实验表明\",{\"1\":{\"165\":1,\"166\":1,\"177\":1,\"459\":1,\"492\":1,\"495\":2}}],[\"实验中统一设为\",{\"1\":{\"157\":1}}],[\"实验观察\",{\"1\":{\"112\":1}}],[\"实验验证\",{\"1\":{\"105\":1,\"112\":1}}],[\"实验证明动态掩码效果更优\",{\"1\":{\"493\":1}}],[\"实验证明\",{\"1\":{\"105\":1,\"120\":1,\"221\":1}}],[\"实验证明了great在开放词汇场景下的有效性和优越性\",{\"1\":{\"5\":1}}],[\"实验\",{\"0\":{\"21\":1,\"192\":1,\"218\":1,\"446\":1,\"455\":1},\"1\":{\"229\":1,\"565\":1}}],[\"实现从用户输入到数据库再到大模型最后输出的整体架构连接\",{\"1\":{\"687\":1}}],[\"实现从用户输入到应用输出的全流程贯通\",{\"1\":{\"687\":1}}],[\"实现更多样化的应用\",{\"1\":{\"675\":1}}],[\"实现全面的多模态交互\",{\"1\":{\"674\":1}}],[\"实现网页浏览\",{\"1\":{\"674\":1}}],[\"实现可复用的层\",{\"1\":{\"670\":1}}],[\"实现后序遍历\",{\"1\":{\"666\":1}}],[\"实现计算图的可视化渲染\",{\"1\":{\"665\":1}}],[\"实现a\",{\"1\":{\"660\":1}}],[\"实现using\",{\"1\":{\"658\":1}}],[\"实现变量导数的重置\",{\"1\":{\"653\":1}}],[\"实现变换不变性\",{\"1\":{\"105\":1}}],[\"实现真正意义上的自动反向传播\",{\"1\":{\"650\":1}}],[\"实现如前所述\",{\"1\":{\"646\":1}}],[\"实现bert\",{\"1\":{\"503\":1}}],[\"实现高效训练\",{\"1\":{\"481\":1}}],[\"实现讲解\",{\"1\":{\"477\":1}}],[\"实现任务统一与跨任务迁移\",{\"1\":{\"464\":1}}],[\"实现显著的性能提升确实是可能的\",{\"1\":{\"450\":1}}],[\"实现步骤如下\",{\"1\":{\"397\":1}}],[\"实现精确的像素级对齐\",{\"1\":{\"397\":1}}],[\"实现精准的3d功能定位\",{\"1\":{\"5\":1}}],[\"实现复杂功能\",{\"1\":{\"395\":1}}],[\"实现分段逼近\",{\"1\":{\"395\":1}}],[\"实现虚拟复制\",{\"1\":{\"327\":1}}],[\"实现虚拟扩展\",{\"1\":{\"327\":1}}],[\"实现数据虚拟扩展\",{\"1\":{\"327\":1}}],[\"实现zero\",{\"1\":{\"273\":1,\"281\":1}}],[\"实现信息融合\",{\"1\":{\"264\":1}}],[\"实现了基础的计算图结构与反向传播流程\",{\"1\":{\"650\":1}}],[\"实现了数值微分作为梯度检验工具\",{\"1\":{\"648\":1}}],[\"实现了变量和函数的基本结构\",{\"1\":{\"648\":1}}],[\"实现了比多项式逼近更高效的函数近似\",{\"1\":{\"395\":1}}],[\"实现了比基于区域特征的模型快数十倍\",{\"1\":{\"253\":1}}],[\"实现了图像与语言之间的初步语义对齐\",{\"1\":{\"226\":1}}],[\"实现了在图像分类\",{\"1\":{\"198\":1}}],[\"实现主要包括以下三个关键阶段\",{\"1\":{\"224\":1}}],[\"实现多元函数反向传播\",{\"1\":{\"663\":1}}],[\"实现多模态能力的协同提升\",{\"1\":{\"208\":1}}],[\"实现多任务预训练与灵活迁移\",{\"1\":{\"120\":1}}],[\"实现图文信息的深层交互\",{\"1\":{\"152\":1}}],[\"实现论文3\",{\"1\":{\"142\":1}}],[\"实现论文中提出的图像\",{\"1\":{\"142\":1}}],[\"实现点集顺序不变性\",{\"1\":{\"103\":1}}],[\"实现方式\",{\"1\":{\"98\":1}}],[\"实现方法\",{\"1\":{\"91\":1}}],[\"实现跨模态信息的充分交互\",{\"1\":{\"73\":1}}],[\"实现语言引导下的跨模态融合\",{\"1\":{\"70\":1}}],[\"实现无先验对齐\",{\"1\":{\"56\":1}}],[\"实现\",{\"0\":{\"503\":1},\"1\":{\"46\":1,\"78\":1,\"131\":1,\"152\":1,\"265\":1,\"511\":1,\"659\":1,\"665\":1}}],[\"实现类似人类的类比推理能力\",{\"1\":{\"32\":1}}],[\"实现对称性\",{\"1\":{\"105\":1}}],[\"实现对\",{\"1\":{\"26\":1}}],[\"实现细节\",{\"0\":{\"191\":1},\"1\":{\"22\":1}}],[\"实现开放词汇功能定位\",{\"1\":{\"7\":1}}],[\"策略预训练轻量级查询\",{\"1\":{\"281\":1}}],[\"策略对交互图像进行推理\",{\"1\":{\"8\":1}}],[\"策略\",{\"1\":{\"5\":1,\"96\":1,\"149\":1,\"163\":1}}],[\"摘要等多样化任务\",{\"1\":{\"455\":1}}],[\"摘要生成\",{\"1\":{\"455\":1}}],[\"摘要任务\",{\"1\":{\"454\":1}}],[\"摘要和阅读理解等\",{\"1\":{\"452\":1}}],[\"摘要\",{\"0\":{\"5\":1,\"48\":1,\"165\":1,\"177\":1,\"180\":1,\"207\":1,\"439\":1,\"452\":1,\"459\":1,\"467\":1,\"479\":1,\"488\":1,\"491\":1},\"1\":{\"454\":1,\"455\":1,\"470\":1}}],[\"mcp\",{\"1\":{\"674\":1}}],[\"mccann\",{\"1\":{\"454\":1}}],[\"mnist\",{\"1\":{\"670\":1}}],[\"mnli\",{\"1\":{\"494\":1,\"497\":1,\"499\":2}}],[\"mkdir\",{\"1\":{\"666\":1}}],[\"mkl\",{\"1\":{\"338\":1}}],[\"mvp\",{\"1\":{\"687\":1}}],[\"mvn\",{\"1\":{\"588\":1,\"589\":1}}],[\"mvcnn\",{\"1\":{\"112\":3}}],[\"mrpc\",{\"1\":{\"448\":1}}],[\"mrg策略在处理每个局部区域时\",{\"1\":{\"97\":1}}],[\"mrg通过结合来自不同分辨率的特征来实现效率和适应性的平衡\",{\"1\":{\"97\":1}}],[\"mrg为一种低成本的替代方案\",{\"1\":{\"97\":1}}],[\"mrg\",{\"1\":{\"94\":1}}],[\"mmlu\",{\"1\":{\"482\":2,\"483\":1}}],[\"mm1\",{\"1\":{\"210\":1}}],[\"mme\",{\"1\":{\"190\":1,\"195\":2,\"196\":2}}],[\"mseloss\",{\"1\":{\"529\":1}}],[\"msqrt\",{\"1\":{\"517\":1}}],[\"msg方法虽然有效\",{\"1\":{\"97\":1}}],[\"msg相当于并联了多个hierarchical\",{\"1\":{\"96\":1}}],[\"msg的关键优点在于它通过在训练期间的随机输入丢弃\",{\"1\":{\"96\":1}}],[\"msg通过应用不同尺度的分组层\",{\"1\":{\"95\":1}}],[\"msg\",{\"1\":{\"94\":1,\"96\":3}}],[\"msmvpam\",{\"1\":{\"4\":1}}],[\"mydecorator\",{\"1\":{\"377\":2}}],[\"mydataset\",{\"1\":{\"289\":1,\"290\":2}}],[\"myservice\",{\"1\":{\"374\":2}}],[\"myclass\",{\"1\":{\"373\":1}}],[\"myenv\",{\"1\":{\"331\":3,\"332\":1,\"334\":2,\"335\":1}}],[\"my\",{\"1\":{\"83\":2,\"367\":3,\"369\":4,\"370\":2,\"372\":5,\"505\":4}}],[\"m\",{\"1\":{\"74\":1,\"83\":2,\"145\":34,\"147\":29,\"160\":8,\"161\":26,\"163\":4,\"238\":1,\"244\":1,\"246\":4,\"248\":2,\"297\":1,\"319\":5,\"326\":4,\"497\":1,\"499\":1,\"556\":3}}],[\"mt\",{\"1\":{\"41\":2,\"45\":6,\"46\":3,\"59\":12}}],[\"mla\",{\"1\":{\"674\":1}}],[\"mlm任务掩码策略的方法\",{\"1\":{\"511\":1}}],[\"mlm的目标是通过文本的上下文信息去预测masked的文本tokens\",{\"1\":{\"258\":1}}],[\"mlm\",{\"0\":{\"155\":1,\"163\":1},\"1\":{\"127\":1,\"153\":1,\"155\":1,\"157\":3,\"159\":2,\"160\":3,\"163\":12,\"239\":1,\"258\":1,\"493\":1,\"495\":1,\"512\":1,\"513\":6,\"514\":2,\"515\":1}}],[\"mlp投影层\",{\"1\":{\"214\":1}}],[\"mlp比率12800的稳定配置\",{\"1\":{\"189\":1}}],[\"mlp映射\",{\"1\":{\"74\":1}}],[\"mlpmixerlayer\",{\"1\":{\"73\":1}}],[\"mlp₂\",{\"1\":{\"73\":1}}],[\"mlp₁\",{\"1\":{\"73\":1}}],[\"mlp\",{\"0\":{\"73\":1,\"296\":1},\"1\":{\"40\":1,\"41\":3,\"46\":2,\"73\":10,\"75\":1,\"76\":1,\"92\":8,\"96\":3,\"98\":5,\"100\":9,\"101\":1,\"105\":1,\"110\":2,\"160\":2,\"190\":2,\"196\":3,\"202\":2,\"214\":1,\"244\":1,\"246\":1,\"265\":2,\"293\":1,\"294\":14,\"296\":4,\"297\":1,\"477\":2}}],[\"mlp=\",{\"1\":{\"35\":3,\"46\":3,\"59\":3,\"93\":3,\"101\":8}}],[\"mllm\",{\"0\":{\"10\":1},\"1\":{\"24\":1,\"28\":1,\"207\":1,\"215\":1}}],[\"mllms\",{\"1\":{\"5\":1,\"7\":2,\"208\":1,\"212\":1}}],[\"must\",{\"1\":{\"511\":1}}],[\"mul等运算符函数\",{\"1\":{\"661\":1}}],[\"mul等\",{\"1\":{\"661\":1}}],[\"mul\",{\"1\":{\"78\":4,\"660\":12,\"661\":3,\"662\":3,\"666\":1}}],[\"multivariate\",{\"0\":{\"589\":1},\"1\":{\"588\":2}}],[\"multirc等任务上显著低于fine\",{\"1\":{\"462\":1}}],[\"multitask\",{\"1\":{\"451\":1,\"452\":1,\"453\":1}}],[\"multinli\",{\"1\":{\"439\":1,\"440\":1}}],[\"multinomial\",{\"0\":{\"576\":1},\"1\":{\"145\":2,\"147\":2,\"162\":2,\"284\":2,\"576\":2}}],[\"multiplication\",{\"1\":{\"568\":1}}],[\"multiply\",{\"1\":{\"295\":2,\"371\":2}}],[\"multiple\",{\"1\":{\"25\":3,\"98\":1,\"544\":1}}],[\"multimodal\",{\"0\":{\"126\":1},\"1\":{\"120\":1,\"138\":1,\"206\":2,\"227\":2,\"262\":1,\"263\":1,\"264\":1,\"268\":1,\"285\":2}}],[\"multiheadedattention\",{\"1\":{\"558\":2}}],[\"multihead\",{\"1\":{\"76\":1}}],[\"multi\",{\"0\":{\"9\":1,\"28\":1,\"95\":1,\"97\":1,\"507\":1},\"1\":{\"26\":1,\"40\":2,\"41\":2,\"43\":5,\"46\":5,\"62\":1,\"94\":2,\"96\":1,\"103\":1,\"114\":1,\"145\":1,\"163\":1,\"190\":1,\"294\":1,\"315\":1,\"454\":1,\"548\":1,\"674\":1}}],[\"mug\",{\"1\":{\"29\":1,\"58\":1,\"63\":1,\"67\":1,\"68\":1,\"70\":1}}],[\"might\",{\"1\":{\"531\":1}}],[\"mit\",{\"1\":{\"481\":1}}],[\"misinformation\",{\"0\":{\"484\":1}}],[\"mishra\",{\"1\":{\"469\":1}}],[\"misalignment\",{\"1\":{\"397\":1}}],[\"missing\",{\"1\":{\"104\":1}}],[\"million\",{\"1\":{\"297\":1}}],[\"mi\",{\"1\":{\"255\":1}}],[\"mim\",{\"1\":{\"165\":1,\"166\":2,\"167\":2,\"172\":1}}],[\"mincount\",{\"1\":{\"410\":4,\"412\":5}}],[\"min=a\",{\"1\":{\"407\":1}}],[\"min=1e\",{\"1\":{\"80\":1}}],[\"min\",{\"1\":{\"115\":1,\"143\":3,\"145\":1,\"147\":2,\"159\":1,\"275\":1,\"277\":1,\"286\":2,\"512\":5,\"541\":4}}],[\"minimum\",{\"1\":{\"412\":2,\"667\":1}}],[\"minibatch\",{\"1\":{\"241\":1,\"272\":2}}],[\"minigpt\",{\"1\":{\"185\":1,\"211\":1,\"268\":1}}],[\"mini\",{\"1\":{\"92\":1,\"93\":3,\"156\":1,\"674\":8}}],[\"miou\",{\"1\":{\"75\":1,\"82\":11,\"193\":1}}],[\"mixed\",{\"1\":{\"162\":2,\"266\":2,\"285\":2,\"531\":6}}],[\"mixer\",{\"0\":{\"73\":1},\"1\":{\"73\":11,\"75\":2,\"294\":1}}],[\"mixture\",{\"0\":{\"126\":1},\"1\":{\"120\":1,\"138\":1}}],[\"mix\",{\"1\":{\"73\":6,\"202\":1}}],[\"mixing阶段\",{\"1\":{\"75\":1}}],[\"mixing\",{\"0\":{\"73\":1},\"1\":{\"71\":1,\"73\":4,\"75\":3}}],[\"microsoft\",{\"1\":{\"448\":1}}],[\"microwave\",{\"1\":{\"29\":1,\"58\":1,\"67\":1,\"68\":1}}],[\"microphone\",{\"1\":{\"29\":1}}],[\"middle\",{\"1\":{\"28\":1}}],[\"moe\",{\"1\":{\"674\":2}}],[\"most\",{\"0\":{\"436\":1},\"1\":{\"276\":10,\"277\":10,\"436\":2}}],[\"more\",{\"1\":{\"247\":1,\"474\":19}}],[\"moving\",{\"1\":{\"157\":1}}],[\"move\",{\"1\":{\"29\":1,\"58\":1,\"62\":1,\"67\":1,\"68\":1,\"83\":2,\"249\":1}}],[\"moco证明了一点\",{\"1\":{\"233\":1}}],[\"moco作为一个无监督的表征学习工作\",{\"1\":{\"233\":1}}],[\"moco是视觉领域使用对比学习一个里程碑的工作\",{\"1\":{\"233\":1}}],[\"moco\",{\"0\":{\"232\":1},\"1\":{\"145\":2,\"154\":1,\"232\":1,\"236\":2,\"237\":4,\"238\":1,\"239\":2,\"240\":1,\"241\":1,\"244\":9,\"246\":3}}],[\"momentum=args\",{\"1\":{\"244\":1}}],[\"momentum=0\",{\"1\":{\"145\":1,\"147\":1}}],[\"momentum\",{\"0\":{\"157\":1,\"236\":1,\"248\":1},\"1\":{\"145\":4,\"147\":4,\"148\":2,\"149\":1,\"150\":1,\"157\":1,\"160\":2,\"161\":2,\"163\":3,\"232\":2,\"244\":1,\"246\":1,\"247\":1,\"248\":2,\"670\":1}}],[\"motivation\",{\"0\":{\"254\":1}}],[\"motions\",{\"1\":{\"116\":1}}],[\"motorcycle\",{\"1\":{\"29\":1,\"273\":1}}],[\"mop\",{\"1\":{\"29\":1}}],[\"mod\",{\"1\":{\"149\":1}}],[\"mode=none\",{\"1\":{\"264\":1}}],[\"mode=mode\",{\"1\":{\"163\":1,\"262\":1,\"263\":1,\"268\":1}}],[\"mode=\",{\"1\":{\"145\":2,\"147\":2,\"161\":2,\"163\":1,\"262\":1,\"263\":1,\"268\":1}}],[\"mode=false\",{\"1\":{\"40\":1}}],[\"mode\",{\"1\":{\"40\":2,\"146\":1,\"162\":2,\"264\":1,\"289\":2,\"584\":1}}],[\"model是decoder输出的大小\",{\"1\":{\"550\":1}}],[\"models\",{\"1\":{\"179\":1,\"206\":2,\"244\":1,\"260\":1,\"300\":1,\"420\":1,\"422\":1,\"434\":1,\"435\":1,\"436\":1,\"451\":1,\"452\":1,\"458\":1,\"461\":1,\"466\":1,\"469\":1,\"470\":1,\"477\":1,\"478\":1,\"487\":1,\"519\":1,\"688\":2}}],[\"modeling\",{\"0\":{\"155\":1},\"1\":{\"127\":1,\"155\":1,\"163\":1,\"167\":1,\"239\":1,\"258\":2,\"268\":2,\"454\":1,\"477\":2,\"495\":1,\"510\":1}}],[\"modelnet40\",{\"1\":{\"112\":2}}],[\"model\",{\"0\":{\"152\":1,\"160\":1,\"245\":1,\"246\":1,\"247\":1,\"257\":1,\"505\":1},\"1\":{\"28\":5,\"40\":3,\"42\":2,\"43\":1,\"44\":3,\"46\":1,\"70\":1,\"80\":3,\"81\":3,\"82\":8,\"83\":10,\"93\":2,\"96\":2,\"101\":2,\"142\":5,\"143\":3,\"145\":6,\"147\":6,\"157\":1,\"159\":2,\"160\":1,\"224\":2,\"228\":1,\"244\":6,\"273\":2,\"275\":16,\"277\":16,\"278\":1,\"286\":2,\"296\":2,\"300\":6,\"389\":2,\"418\":1,\"425\":1,\"449\":1,\"470\":1,\"474\":2,\"477\":5,\"504\":1,\"505\":1,\"513\":10,\"514\":11,\"519\":6,\"527\":1,\"541\":1,\"550\":2,\"558\":7,\"670\":2,\"673\":2,\"677\":1,\"683\":1}}],[\"modulelist\",{\"1\":{\"92\":2,\"96\":4,\"100\":2,\"263\":1,\"513\":1,\"526\":1}}],[\"module\",{\"0\":{\"14\":1},\"1\":{\"30\":1,\"32\":1,\"33\":1,\"34\":2,\"35\":2,\"36\":2,\"41\":1,\"45\":3,\"46\":1,\"59\":8,\"70\":2,\"72\":1,\"73\":1,\"74\":1,\"75\":1,\"76\":2,\"78\":1,\"92\":1,\"93\":1,\"96\":2,\"100\":1,\"101\":1,\"107\":1,\"109\":1,\"110\":1,\"111\":1,\"142\":1,\"145\":1,\"146\":1,\"147\":1,\"162\":1,\"246\":1,\"262\":1,\"263\":3,\"264\":1,\"265\":2,\"266\":1,\"268\":2,\"284\":1,\"285\":5,\"291\":1,\"292\":2,\"293\":1,\"294\":2,\"295\":1,\"296\":1,\"372\":1,\"389\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1,\"477\":1,\"513\":1,\"517\":1,\"523\":1,\"525\":3,\"526\":3,\"527\":1,\"531\":1,\"532\":1,\"533\":1,\"535\":1,\"536\":1,\"537\":1,\"549\":1,\"550\":1,\"552\":1,\"553\":1,\"554\":1,\"556\":1,\"557\":1,\"558\":1,\"661\":1}}],[\"modality\",{\"0\":{\"256\":1},\"1\":{\"255\":1}}],[\"modal\",{\"0\":{\"14\":1},\"1\":{\"35\":1,\"36\":1,\"163\":1,\"262\":1,\"283\":2}}],[\"markdown\",{\"1\":{\"687\":1}}],[\"markersize=12\",{\"1\":{\"667\":1}}],[\"markersize=2\",{\"1\":{\"667\":1}}],[\"marker=\",{\"1\":{\"667\":1}}],[\"marginal\",{\"1\":{\"571\":1,\"596\":1}}],[\"mass\",{\"1\":{\"565\":1}}],[\"mask作用图解\",{\"1\":{\"520\":2}}],[\"mask模样为\",{\"1\":{\"517\":1}}],[\"mask矩阵将注意力得分矩阵中对应位置的得分设置为一个非常小的值\",{\"1\":{\"517\":1}}],[\"mask矩阵\",{\"1\":{\"517\":1}}],[\"mask部分相关的掩码逻辑\",{\"1\":{\"295\":1}}],[\"mask和casual\",{\"1\":{\"295\":1}}],[\"mask方法中\",{\"1\":{\"285\":1}}],[\"mask标注哪些image\",{\"1\":{\"282\":1}}],[\"masking是将连续的子词tokens进行mask的技巧\",{\"1\":{\"258\":1}}],[\"masking技巧\",{\"1\":{\"258\":1}}],[\"masking\",{\"1\":{\"258\":1,\"387\":1,\"492\":1,\"495\":4,\"497\":1,\"511\":1,\"512\":1,\"548\":1,\"557\":1}}],[\"maskedlmoutput\",{\"1\":{\"163\":2}}],[\"masked\",{\"0\":{\"155\":1,\"505\":1},\"1\":{\"142\":1,\"145\":2,\"147\":1,\"155\":1,\"163\":15,\"167\":1,\"239\":1,\"258\":1,\"285\":1,\"495\":1,\"504\":1,\"505\":2,\"511\":8,\"512\":6,\"513\":12,\"514\":4,\"517\":1,\"538\":5,\"549\":1,\"558\":1}}],[\"mask=mask\",{\"1\":{\"558\":1}}],[\"mask=memory\",{\"1\":{\"76\":2}}],[\"mask=extended\",{\"1\":{\"262\":1}}],[\"mask=encoder\",{\"1\":{\"163\":1,\"262\":1,\"268\":1,\"285\":1}}],[\"mask=head\",{\"1\":{\"163\":1,\"285\":1,\"529\":1,\"538\":1,\"543\":1,\"544\":1}}],[\"mask=attention\",{\"1\":{\"163\":1,\"268\":1,\"284\":1,\"285\":2,\"477\":1,\"520\":1,\"529\":1,\"538\":1,\"540\":1,\"541\":1,\"543\":1,\"544\":1}}],[\"mask=image\",{\"1\":{\"142\":1,\"145\":2,\"147\":3,\"282\":1,\"284\":1,\"285\":1}}],[\"mask=q\",{\"1\":{\"75\":1}}],[\"mask=none\",{\"1\":{\"72\":1,\"74\":1,\"75\":1,\"83\":1,\"162\":3,\"163\":3,\"262\":2,\"263\":2,\"264\":2,\"265\":2,\"266\":2,\"268\":2,\"285\":12,\"477\":2,\"525\":1,\"526\":2,\"528\":2,\"529\":2,\"531\":2,\"533\":1,\"538\":2,\"541\":2,\"543\":2,\"544\":2,\"558\":2}}],[\"mask=text\",{\"1\":{\"142\":1,\"145\":4,\"147\":5,\"161\":2,\"282\":1}}],[\"mask=tgt\",{\"1\":{\"76\":2}}],[\"mask=t\",{\"1\":{\"70\":1,\"76\":1}}],[\"mask=llm\",{\"1\":{\"40\":1}}],[\"mask\",{\"0\":{\"64\":2,\"78\":1,\"517\":1},\"1\":{\"40\":5,\"43\":13,\"59\":12,\"64\":3,\"68\":6,\"70\":4,\"72\":1,\"74\":3,\"75\":3,\"76\":12,\"78\":4,\"81\":2,\"82\":19,\"83\":9,\"92\":6,\"142\":1,\"143\":3,\"145\":8,\"146\":5,\"147\":6,\"155\":4,\"157\":3,\"160\":1,\"161\":3,\"162\":11,\"163\":26,\"239\":1,\"258\":1,\"262\":4,\"263\":2,\"264\":3,\"265\":2,\"266\":5,\"268\":2,\"282\":2,\"283\":2,\"284\":10,\"285\":29,\"286\":1,\"295\":2,\"396\":1,\"397\":1,\"401\":2,\"402\":3,\"403\":1,\"477\":6,\"493\":1,\"495\":1,\"505\":3,\"511\":6,\"512\":1,\"513\":5,\"517\":7,\"520\":14,\"522\":4,\"525\":1,\"526\":2,\"528\":8,\"529\":3,\"531\":3,\"533\":1,\"538\":2,\"540\":1,\"541\":1,\"543\":4,\"544\":6,\"548\":1,\"549\":11,\"553\":2,\"554\":3,\"556\":4,\"557\":4,\"558\":6}}],[\"made\",{\"1\":{\"556\":1}}],[\"mail\",{\"1\":{\"455\":1}}],[\"main\",{\"1\":{\"28\":2,\"80\":1,\"83\":1,\"142\":1,\"145\":1,\"147\":1,\"244\":1,\"279\":1,\"474\":3,\"477\":4,\"510\":3}}],[\"mahalanobis\",{\"0\":{\"358\":1},\"1\":{\"590\":1}}],[\"macos安装\",{\"1\":{\"666\":1}}],[\"macos\",{\"1\":{\"546\":1}}],[\"macos系统某些包的加载和依赖关系上存在问题\",{\"1\":{\"38\":1}}],[\"machine\",{\"1\":{\"332\":1}}],[\"major\",{\"1\":{\"322\":2,\"323\":3}}],[\"makedirs\",{\"1\":{\"510\":2}}],[\"make\",{\"1\":{\"295\":1,\"512\":1,\"514\":1}}],[\"making\",{\"1\":{\"247\":1}}],[\"matyas\",{\"1\":{\"662\":2}}],[\"matyas函数求导\",{\"1\":{\"662\":1}}],[\"matplotlib\",{\"1\":{\"277\":1,\"667\":1}}],[\"math\",{\"1\":{\"266\":1,\"285\":1,\"482\":1,\"531\":1,\"558\":1,\"674\":1}}],[\"mathvista基准\",{\"1\":{\"220\":1}}],[\"matmul\",{\"1\":{\"266\":2,\"283\":2,\"285\":2,\"390\":3,\"517\":2,\"531\":2,\"558\":2}}],[\"match\",{\"1\":{\"146\":3}}],[\"matching\",{\"0\":{\"156\":1,\"284\":1},\"1\":{\"127\":1,\"156\":1,\"258\":2,\"276\":11,\"277\":11,\"284\":4,\"464\":1}}],[\"matrix转化成word\",{\"1\":{\"257\":1}}],[\"matrix=none\",{\"1\":{\"163\":1}}],[\"matrix\",{\"0\":{\"342\":1},\"1\":{\"82\":3,\"108\":1,\"163\":4,\"590\":4}}],[\"maybe\",{\"1\":{\"40\":1}}],[\"maximum\",{\"1\":{\"412\":3}}],[\"max=b\",{\"1\":{\"407\":1}}],[\"max=dict\",{\"1\":{\"80\":1}}],[\"max\",{\"1\":{\"28\":1,\"40\":2,\"83\":1,\"92\":4,\"96\":1,\"105\":6,\"107\":2,\"109\":1,\"112\":7,\"115\":1,\"142\":3,\"143\":3,\"145\":3,\"146\":2,\"147\":3,\"159\":1,\"208\":1,\"210\":1,\"282\":3,\"283\":2,\"286\":2,\"296\":1,\"396\":2,\"397\":1,\"410\":3,\"411\":12,\"412\":19,\"477\":2,\"511\":1,\"512\":7,\"513\":10,\"514\":2,\"519\":1,\"520\":6,\"522\":7,\"523\":1,\"541\":4,\"656\":1,\"658\":1}}],[\"mapping\",{\"1\":{\"412\":6}}],[\"maps\",{\"1\":{\"410\":2,\"412\":2}}],[\"map\",{\"1\":{\"28\":1,\"83\":1,\"285\":1,\"300\":1,\"365\":1,\"397\":2,\"412\":3,\"514\":1,\"522\":1}}],[\"map=device\",{\"1\":{\"28\":1}}],[\"mae\",{\"1\":{\"22\":1,\"23\":1,\"75\":1,\"82\":16,\"239\":2}}],[\"merged\",{\"1\":{\"410\":6,\"411\":6,\"412\":13}}],[\"merge\",{\"1\":{\"410\":4,\"411\":5,\"412\":12}}],[\"merges\",{\"1\":{\"410\":4,\"412\":3}}],[\"merging\",{\"1\":{\"410\":3,\"412\":1}}],[\"message\",{\"1\":{\"227\":2}}],[\"meshgrid\",{\"1\":{\"667\":1}}],[\"mesh\",{\"1\":{\"114\":1}}],[\"med\",{\"0\":{\"126\":1},\"1\":{\"120\":2,\"122\":1,\"125\":1,\"126\":1,\"129\":1,\"138\":1,\"142\":6,\"145\":2,\"146\":2,\"147\":1,\"260\":1}}],[\"meta\",{\"1\":{\"460\":1,\"464\":1,\"674\":1}}],[\"metamind\",{\"1\":{\"0\":1}}],[\"methods\",{\"1\":{\"274\":1,\"421\":1,\"469\":1}}],[\"method\",{\"0\":{\"125\":1,\"255\":1},\"1\":{\"373\":3,\"378\":4}}],[\"metrics\",{\"1\":{\"82\":7}}],[\"metric\",{\"1\":{\"82\":1,\"159\":1}}],[\"members\",{\"1\":{\"412\":1}}],[\"memory=true\",{\"1\":{\"244\":1}}],[\"memory\",{\"1\":{\"76\":5,\"242\":4,\"326\":1,\"449\":1,\"549\":2,\"556\":2,\"557\":2,\"683\":1}}],[\"mem\",{\"1\":{\"28\":1}}],[\"mean=\",{\"1\":{\"244\":1}}],[\"mean\",{\"1\":{\"22\":1,\"78\":2,\"82\":11,\"83\":1,\"108\":1,\"145\":2,\"147\":2,\"161\":2,\"163\":2,\"268\":1,\"284\":1,\"285\":1,\"402\":1,\"404\":2,\"407\":1,\"514\":1,\"584\":1,\"596\":1}}],[\"mhacot是一种类人推理方式\",{\"1\":{\"28\":1}}],[\"mhacot推理链\",{\"1\":{\"6\":1}}],[\"mhacot\",{\"1\":{\"5\":1,\"8\":1,\"28\":1}}],[\"lcel\",{\"1\":{\"684\":2,\"685\":1}}],[\"lstm\",{\"1\":{\"485\":1}}],[\"lstm会掉5\",{\"1\":{\"449\":1}}],[\"lstm表现高方差\",{\"1\":{\"449\":1}}],[\"ltm的过程包含两个阶段\",{\"1\":{\"436\":1}}],[\"ltm的核心思想是\",{\"1\":{\"436\":1}}],[\"ltm\",{\"1\":{\"436\":1}}],[\"l为灰度图片\",{\"1\":{\"289\":1}}],[\"ln\",{\"1\":{\"282\":1,\"286\":1,\"477\":3}}],[\"luggage\",{\"1\":{\"226\":1}}],[\"lgs\",{\"1\":{\"202\":1}}],[\"llama3\",{\"1\":{\"674\":4}}],[\"llama2\",{\"1\":{\"674\":3}}],[\"llama呈现出与同类模型相似的偏见\",{\"1\":{\"484\":1}}],[\"llama通过轻量级指令微调\",{\"1\":{\"483\":1}}],[\"llama的核心成果\",{\"1\":{\"482\":1}}],[\"llama的方法论核心是通过数据质量优化\",{\"1\":{\"481\":1}}],[\"llama采用纯公开数据混合\",{\"1\":{\"481\":1}}],[\"llama仅使用公开数据\",{\"1\":{\"480\":1}}],[\"llama强调推理成本优化而非单纯训练速度\",{\"1\":{\"480\":1}}],[\"llama是一系列高效的基础语言模型\",{\"1\":{\"479\":1}}],[\"llama\",{\"0\":{\"478\":1,\"487\":1},\"1\":{\"190\":1,\"191\":4,\"198\":1,\"200\":2,\"224\":1,\"226\":1,\"227\":1,\"409\":1,\"478\":2,\"479\":1,\"480\":1,\"482\":7,\"483\":1,\"487\":2,\"673\":1,\"674\":16}}],[\"llama系列模型通过高效架构设计和纯公开数据训练\",{\"1\":{\"486\":1}}],[\"llama系列\",{\"1\":{\"184\":1}}],[\"llava模型结构\",{\"1\":{\"226\":1}}],[\"llava\",{\"0\":{\"223\":1},\"1\":{\"185\":1,\"190\":1,\"195\":2,\"202\":2,\"211\":1,\"219\":1,\"223\":2,\"224\":1,\"226\":1,\"227\":4,\"228\":4,\"231\":1}}],[\"llm就像是一个快思考的系统\",{\"1\":{\"433\":1}}],[\"llm是一个概率模型\",{\"1\":{\"430\":1}}],[\"llm已经这么强了\",{\"1\":{\"430\":1}}],[\"llm友好性\",{\"1\":{\"181\":1}}],[\"llms本身缺乏视觉理解能力\",{\"1\":{\"184\":1}}],[\"llms的参数规模已达千亿级\",{\"1\":{\"181\":1}}],[\"llms\",{\"1\":{\"180\":1,\"181\":1,\"184\":1,\"207\":1,\"208\":1,\"421\":1}}],[\"llm\",{\"0\":{\"675\":1},\"1\":{\"3\":1,\"40\":12,\"42\":3,\"43\":14,\"44\":4,\"45\":1,\"69\":1,\"189\":1,\"190\":4,\"202\":2,\"214\":1,\"231\":1,\"280\":1,\"286\":4,\"673\":7,\"674\":2,\"675\":9,\"676\":6,\"678\":4,\"679\":10,\"681\":2,\"683\":2,\"684\":9,\"685\":1,\"688\":1,\"689\":1}}],[\"lm中同样含有pad部分\",{\"1\":{\"514\":1}}],[\"lm也只包含被掩码的token对应的模型预测真实词\",{\"1\":{\"514\":1}}],[\"lm三个目标进行训练\",{\"1\":{\"147\":1}}],[\"lm\",{\"0\":{\"127\":1},\"1\":{\"127\":2,\"128\":1,\"142\":2,\"147\":6,\"163\":7,\"268\":4,\"285\":7,\"454\":1,\"505\":1,\"513\":3,\"514\":6,\"538\":5}}],[\"lmaffordance3d\",{\"0\":{\"37\":1,\"40\":1},\"1\":{\"37\":1,\"40\":1,\"332\":1}}],[\"l0\",{\"1\":{\"101\":8}}],[\"l4\",{\"1\":{\"101\":6}}],[\"l3\",{\"1\":{\"93\":4,\"96\":4,\"101\":9}}],[\"l2\",{\"1\":{\"93\":4,\"96\":4,\"101\":9,\"247\":1,\"272\":2,\"447\":1}}],[\"l1\",{\"1\":{\"93\":4,\"96\":4,\"101\":9}}],[\"lr是learning\",{\"1\":{\"667\":1}}],[\"lr=1e\",{\"1\":{\"514\":1}}],[\"lr=6\",{\"1\":{\"447\":1}}],[\"lr=2\",{\"1\":{\"447\":1}}],[\"lr=config\",{\"1\":{\"142\":1,\"145\":1,\"147\":1}}],[\"lr=dict\",{\"1\":{\"80\":1}}],[\"lr\",{\"1\":{\"80\":4,\"83\":1,\"142\":1,\"145\":1,\"147\":1,\"244\":1,\"447\":1,\"514\":1,\"667\":6}}],[\"l×d\",{\"1\":{\"72\":1,\"73\":1}}],[\"legend\",{\"1\":{\"667\":1}}],[\"legacy\",{\"1\":{\"477\":8}}],[\"le\",{\"1\":{\"455\":1}}],[\"least\",{\"0\":{\"436\":1},\"1\":{\"436\":2}}],[\"learned\",{\"1\":{\"272\":3,\"282\":2}}],[\"learners\",{\"1\":{\"451\":1,\"452\":1,\"458\":1}}],[\"learner\",{\"1\":{\"86\":2}}],[\"learning领域中如maml\",{\"1\":{\"464\":1}}],[\"learning的核心思想是通过设计合适的prompt\",{\"1\":{\"274\":1}}],[\"learning或prompt\",{\"1\":{\"274\":1}}],[\"learning\",{\"0\":{\"154\":1,\"234\":1,\"282\":1,\"283\":1,\"286\":1,\"507\":1},\"1\":{\"148\":2,\"150\":1,\"224\":1,\"232\":2,\"251\":2,\"278\":2,\"282\":1,\"286\":1,\"416\":2,\"453\":1,\"460\":2,\"461\":3,\"464\":2,\"465\":1,\"468\":1,\"519\":1}}],[\"learnable\",{\"1\":{\"76\":2,\"191\":2,\"231\":1,\"293\":1}}],[\"let\",{\"1\":{\"433\":1}}],[\"leibler\",{\"1\":{\"157\":1}}],[\"levels=np\",{\"1\":{\"667\":1}}],[\"level\",{\"1\":{\"97\":1,\"497\":1}}],[\"lens\",{\"1\":{\"520\":1,\"522\":2}}],[\"len=input\",{\"1\":{\"520\":1}}],[\"len的维度拼接起来\",{\"1\":{\"285\":1}}],[\"len维度上拼接起来\",{\"1\":{\"284\":2}}],[\"len和hidden\",{\"1\":{\"282\":1}}],[\"length长度\",{\"1\":{\"520\":1}}],[\"length=128\",{\"1\":{\"519\":1}}],[\"length=10\",{\"1\":{\"143\":1,\"286\":1}}],[\"length=0\",{\"1\":{\"284\":1,\"285\":2}}],[\"length=25\",{\"1\":{\"159\":1}}],[\"length=35\",{\"1\":{\"145\":1,\"146\":1}}],[\"length=30\",{\"1\":{\"143\":1,\"147\":1,\"286\":1}}],[\"length=min\",{\"1\":{\"143\":2,\"286\":1}}],[\"length=max\",{\"1\":{\"143\":2,\"286\":1}}],[\"length=40\",{\"1\":{\"142\":1}}],[\"length=self\",{\"1\":{\"40\":1,\"282\":1}}],[\"length\",{\"1\":{\"43\":4,\"45\":1,\"142\":2,\"143\":4,\"145\":1,\"146\":1,\"147\":1,\"282\":1,\"284\":5,\"285\":3,\"286\":2,\"477\":1,\"520\":11,\"522\":1,\"523\":2,\"540\":4,\"544\":4}}],[\"len\",{\"1\":{\"40\":1,\"70\":1,\"81\":3,\"83\":2,\"92\":1,\"96\":2,\"142\":1,\"143\":1,\"145\":1,\"147\":2,\"159\":1,\"268\":1,\"275\":1,\"277\":1,\"282\":7,\"283\":5,\"284\":10,\"285\":1,\"289\":8,\"410\":2,\"411\":3,\"412\":4,\"474\":1,\"477\":4,\"510\":2,\"511\":9,\"512\":18,\"513\":9,\"514\":2,\"517\":10,\"520\":13,\"522\":5,\"531\":2,\"541\":4,\"543\":2,\"558\":1,\"651\":1,\"656\":1,\"658\":1,\"659\":5}}],[\"left\",{\"1\":{\"40\":1,\"553\":1}}],[\"langserve\",{\"1\":{\"685\":1}}],[\"langsmith\",{\"1\":{\"684\":1,\"685\":1}}],[\"langchian\",{\"1\":{\"683\":1}}],[\"langchain\",{\"0\":{\"682\":1},\"1\":{\"682\":6,\"683\":1,\"684\":13,\"685\":10,\"687\":3}}],[\"language生成学习\",{\"1\":{\"281\":1}}],[\"language表征学习\",{\"1\":{\"281\":1}}],[\"language\",{\"0\":{\"84\":1,\"155\":1,\"223\":1,\"505\":1},\"1\":{\"37\":1,\"40\":1,\"43\":1,\"46\":1,\"60\":3,\"70\":1,\"82\":1,\"119\":4,\"120\":1,\"127\":1,\"134\":2,\"148\":2,\"150\":1,\"155\":1,\"163\":1,\"190\":2,\"223\":1,\"239\":1,\"252\":2,\"258\":2,\"268\":2,\"271\":1,\"274\":1,\"278\":1,\"420\":1,\"421\":1,\"422\":1,\"425\":1,\"434\":1,\"435\":1,\"436\":1,\"438\":1,\"451\":1,\"452\":1,\"454\":1,\"458\":1,\"461\":1,\"466\":1,\"469\":1,\"478\":1,\"495\":1,\"504\":1,\"505\":1,\"510\":1,\"673\":2,\"688\":2}}],[\"latent\",{\"1\":{\"674\":1}}],[\"latency\",{\"1\":{\"425\":1}}],[\"lambda\",{\"1\":{\"553\":1,\"556\":2}}],[\"lambada去除重叠样本后\",{\"1\":{\"455\":1}}],[\"lambada\",{\"1\":{\"455\":1}}],[\"lambada等\",{\"1\":{\"455\":1}}],[\"law的未来\",{\"1\":{\"688\":1}}],[\"law\",{\"0\":{\"569\":1},\"1\":{\"673\":7,\"688\":1}}],[\"lawrence\",{\"1\":{\"469\":1}}],[\"laws\",{\"1\":{\"460\":1,\"461\":1,\"464\":2}}],[\"lavis\",{\"1\":{\"279\":1,\"339\":1}}],[\"launchpad\",{\"1\":{\"273\":1}}],[\"large\",{\"0\":{\"223\":1},\"1\":{\"147\":1,\"223\":1,\"275\":2,\"277\":1,\"278\":1,\"420\":1,\"422\":1,\"434\":1,\"436\":1,\"454\":1,\"495\":1,\"497\":1,\"673\":1,\"688\":2}}],[\"laion\",{\"1\":{\"131\":1,\"190\":2,\"217\":1}}],[\"last\",{\"1\":{\"92\":3,\"96\":3,\"100\":3,\"145\":5,\"146\":2,\"147\":4,\"161\":4,\"162\":2,\"262\":1,\"282\":2,\"284\":1,\"285\":1,\"477\":1}}],[\"last=true\",{\"1\":{\"80\":1,\"244\":1}}],[\"laso\",{\"0\":{\"60\":1},\"1\":{\"6\":1,\"7\":1,\"22\":1,\"23\":1,\"60\":4,\"62\":1,\"68\":1,\"69\":1,\"70\":1,\"71\":1,\"76\":1,\"78\":2,\"82\":3,\"83\":2}}],[\"laptop\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"layer=act\",{\"1\":{\"294\":1,\"296\":1}}],[\"layer=norm\",{\"1\":{\"296\":1}}],[\"layer=none\",{\"1\":{\"291\":1,\"292\":1,\"293\":3,\"296\":3}}],[\"layer=nn\",{\"1\":{\"294\":3}}],[\"layer=partial\",{\"1\":{\"160\":2}}],[\"layer=false\",{\"1\":{\"145\":2,\"146\":1,\"147\":2,\"268\":1}}],[\"layer=0\",{\"1\":{\"145\":1,\"147\":1}}],[\"layer=config\",{\"1\":{\"142\":1,\"145\":1,\"147\":1}}],[\"layer的任务是通过中心点找到邻居点\",{\"1\":{\"90\":1}}],[\"layers\",{\"1\":{\"88\":1,\"98\":2,\"191\":1,\"263\":2,\"285\":1,\"513\":3,\"514\":1,\"526\":1,\"531\":1,\"554\":3,\"557\":2}}],[\"layers组成\",{\"1\":{\"88\":1}}],[\"layers主要包括3个部分\",{\"1\":{\"87\":1}}],[\"layer\",{\"0\":{\"89\":1,\"90\":1,\"91\":1},\"1\":{\"72\":1,\"75\":2,\"87\":3,\"88\":6,\"99\":6,\"142\":4,\"145\":2,\"146\":2,\"147\":2,\"162\":7,\"263\":5,\"265\":1,\"266\":17,\"268\":1,\"285\":36,\"291\":3,\"292\":2,\"293\":1,\"294\":4,\"296\":8,\"477\":3,\"513\":3,\"523\":1,\"525\":3,\"526\":4,\"531\":21,\"532\":1,\"535\":1,\"548\":1,\"554\":6,\"557\":6,\"670\":1}}],[\"layer指的是pointnet++中提供的pointnetfeaturepropagation特征传播类\",{\"1\":{\"70\":1}}],[\"layer指的是pointnet++中提供的pointnetsetabstractionmsg多尺度分组点集特征抽取类\",{\"1\":{\"70\":1}}],[\"layernorm\",{\"1\":{\"32\":2,\"36\":2,\"45\":6,\"73\":2,\"160\":2,\"265\":3,\"267\":1,\"268\":3,\"284\":1,\"294\":1,\"296\":2,\"447\":1,\"523\":2,\"525\":2,\"532\":2,\"535\":2,\"552\":2,\"554\":1,\"557\":1}}],[\"lay\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"label=\",{\"1\":{\"666\":12,\"667\":2}}],[\"label=none\",{\"1\":{\"538\":1}}],[\"label=label\",{\"1\":{\"520\":1}}],[\"labels=labels\",{\"1\":{\"285\":1}}],[\"labels=none\",{\"1\":{\"163\":2,\"268\":1,\"285\":1,\"529\":1,\"538\":1,\"543\":1,\"544\":1}}],[\"labels=decoder\",{\"1\":{\"142\":1,\"147\":1}}],[\"labels\",{\"1\":{\"62\":1,\"98\":2,\"99\":1,\"145\":2,\"147\":2,\"162\":2,\"163\":15,\"247\":3,\"268\":4,\"272\":3,\"273\":1,\"284\":2,\"285\":8,\"289\":4,\"296\":3,\"520\":1,\"522\":2,\"529\":8,\"538\":2,\"541\":3,\"543\":11,\"544\":2}}],[\"label\",{\"1\":{\"29\":9,\"40\":3,\"58\":10,\"64\":1,\"81\":1,\"82\":4,\"159\":6,\"161\":3,\"162\":1,\"163\":8,\"268\":1,\"273\":1,\"274\":2,\"283\":2,\"285\":1,\"289\":8,\"290\":4,\"404\":1,\"514\":2,\"520\":2,\"538\":2}}],[\"lite\",{\"1\":{\"674\":2}}],[\"library\",{\"1\":{\"661\":1}}],[\"limitations\",{\"1\":{\"472\":1}}],[\"liang\",{\"1\":{\"469\":1}}],[\"liu\",{\"1\":{\"223\":1}}],[\"lin\",{\"1\":{\"469\":1,\"558\":2}}],[\"lineids\",{\"1\":{\"411\":5}}],[\"line\",{\"1\":{\"411\":9,\"474\":1,\"510\":8,\"666\":1}}],[\"lines\",{\"1\":{\"411\":2}}],[\"line>\",{\"1\":{\"410\":1,\"411\":1,\"412\":2}}],[\"linears\",{\"1\":{\"558\":3}}],[\"linear1\",{\"1\":{\"76\":1}}],[\"linear2\",{\"1\":{\"76\":1}}],[\"linear\",{\"1\":{\"34\":2,\"35\":4,\"36\":2,\"42\":2,\"44\":2,\"45\":5,\"46\":2,\"59\":4,\"73\":4,\"93\":3,\"96\":3,\"107\":3,\"110\":3,\"145\":5,\"146\":3,\"147\":5,\"160\":7,\"205\":1,\"237\":1,\"262\":1,\"265\":1,\"268\":2,\"294\":2,\"295\":2,\"296\":2,\"508\":3,\"513\":3,\"525\":2,\"527\":1,\"529\":1,\"531\":3,\"532\":1,\"535\":1,\"536\":1,\"537\":1,\"540\":1,\"541\":1,\"543\":1,\"544\":1,\"550\":1,\"558\":3,\"670\":1}}],[\"linalg\",{\"1\":{\"275\":2,\"277\":2}}],[\"linguistic\",{\"1\":{\"179\":1,\"448\":1}}],[\"linspace\",{\"1\":{\"82\":2,\"667\":2}}],[\"lightgroupattnblock\",{\"1\":{\"72\":1}}],[\"li\",{\"1\":{\"60\":1,\"126\":1,\"127\":2}}],[\"likert\",{\"1\":{\"470\":1}}],[\"likelihood\",{\"1\":{\"470\":1,\"571\":2,\"596\":2}}],[\"like\",{\"0\":{\"283\":1,\"285\":1},\"1\":{\"59\":1,\"523\":1,\"642\":1,\"652\":1,\"654\":1,\"656\":1,\"658\":1}}],[\"lift\",{\"1\":{\"29\":1,\"58\":1,\"62\":1,\"67\":1,\"68\":1}}],[\"lid\",{\"1\":{\"28\":2,\"31\":2}}],[\"lidar\",{\"1\":{\"22\":1}}],[\"liquid\",{\"1\":{\"28\":2,\"31\":2}}],[\"list列表组装起来得到需要的dataset\",{\"1\":{\"520\":1}}],[\"listdir\",{\"1\":{\"275\":1,\"277\":1,\"289\":2}}],[\"listen\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"list\",{\"1\":{\"28\":2,\"29\":13,\"43\":1,\"58\":13,\"70\":1,\"83\":2,\"92\":2,\"96\":15,\"289\":2,\"334\":1,\"337\":1,\"363\":1,\"410\":4,\"411\":7,\"412\":27,\"477\":3,\"511\":6,\"667\":6}}],[\"lorentz\",{\"1\":{\"587\":1}}],[\"lora微调\",{\"1\":{\"423\":1}}],[\"lora的基本思路\",{\"1\":{\"420\":1}}],[\"lora背后有一个假设\",{\"1\":{\"420\":1}}],[\"lora是跟prompt\",{\"1\":{\"420\":1}}],[\"lora\",{\"0\":{\"420\":1,\"425\":1},\"1\":{\"10\":1,\"421\":1,\"422\":1,\"424\":5,\"425\":9,\"428\":7}}],[\"lovaszsoftmax\",{\"1\":{\"406\":1}}],[\"lovasz\",{\"0\":{\"406\":1},\"1\":{\"406\":1}}],[\"looking\",{\"1\":{\"273\":1}}],[\"loshchilov\",{\"1\":{\"131\":1}}],[\"loss中的alpha和beta\",{\"1\":{\"408\":1}}],[\"loss等能够处理不平衡情况的损失函数\",{\"1\":{\"408\":1}}],[\"loss或combo\",{\"1\":{\"408\":1}}],[\"loss的设计思想是\",{\"1\":{\"406\":1}}],[\"loss的设计灵感来自tversky指数\",{\"1\":{\"405\":1}}],[\"loss引入了一个衰减因子\",{\"1\":{\"404\":1}}],[\"loss和标准的二元交叉熵\",{\"1\":{\"402\":1}}],[\"loss是将dice\",{\"1\":{\"402\":1}}],[\"loss=lm\",{\"1\":{\"268\":1,\"285\":1}}],[\"loss=masked\",{\"1\":{\"163\":1}}],[\"loss监督\",{\"1\":{\"55\":1}}],[\"loss\",{\"0\":{\"15\":1,\"78\":2,\"283\":1,\"284\":1,\"285\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"406\":1,\"407\":1},\"1\":{\"15\":2,\"40\":8,\"55\":1,\"78\":25,\"80\":1,\"81\":8,\"82\":1,\"108\":2,\"127\":3,\"142\":5,\"145\":15,\"147\":20,\"159\":8,\"161\":7,\"162\":1,\"163\":20,\"244\":3,\"268\":7,\"272\":8,\"283\":2,\"284\":1,\"285\":10,\"296\":3,\"401\":10,\"402\":21,\"403\":21,\"404\":38,\"405\":5,\"407\":22,\"470\":3,\"505\":2,\"507\":1,\"514\":23,\"529\":8,\"538\":10,\"541\":10,\"543\":11,\"544\":5}}],[\"loal\",{\"1\":{\"86\":1}}],[\"loaded\",{\"1\":{\"514\":1}}],[\"loader\",{\"1\":{\"80\":3,\"81\":2,\"82\":2,\"142\":7,\"145\":8,\"147\":7,\"159\":3,\"244\":6,\"290\":2,\"296\":2}}],[\"loading\",{\"1\":{\"244\":1,\"275\":1,\"277\":1}}],[\"load\",{\"1\":{\"28\":2,\"68\":2,\"83\":4,\"300\":2,\"389\":1,\"412\":4,\"511\":2,\"514\":2}}],[\"logspace\",{\"1\":{\"667\":1}}],[\"logging\",{\"1\":{\"519\":1}}],[\"logger\",{\"1\":{\"82\":2,\"159\":1}}],[\"logp\",{\"1\":{\"470\":1}}],[\"log\",{\"1\":{\"78\":3,\"83\":2,\"93\":1,\"96\":1,\"101\":1,\"110\":2,\"111\":3,\"145\":2,\"147\":2,\"159\":1,\"161\":2,\"163\":1,\"371\":3,\"373\":2,\"379\":1,\"407\":4,\"470\":2,\"550\":1}}],[\"logit\",{\"1\":{\"78\":1,\"403\":1,\"404\":1}}],[\"logits外\",{\"1\":{\"300\":1}}],[\"logits=prediction\",{\"1\":{\"163\":1,\"268\":1,\"285\":1}}],[\"logits=false\",{\"1\":{\"163\":1,\"285\":1,\"300\":1}}],[\"logits\",{\"1\":{\"59\":6,\"81\":1,\"98\":1,\"163\":7,\"240\":1,\"247\":9,\"268\":1,\"272\":3,\"284\":2,\"285\":2,\"296\":3,\"300\":3,\"401\":2,\"402\":1,\"405\":2,\"474\":4,\"477\":5,\"513\":6,\"514\":4,\"529\":5,\"540\":12,\"541\":15,\"543\":6,\"544\":6}}],[\"locality\",{\"1\":{\"287\":1,\"326\":1}}],[\"local\",{\"1\":{\"86\":3,\"92\":2,\"275\":2,\"277\":2,\"522\":1}}],[\"location=device\",{\"1\":{\"300\":1,\"514\":1}}],[\"location=\",{\"1\":{\"83\":1}}],[\"loc\",{\"1\":{\"68\":1}}],[\"longterm\",{\"1\":{\"510\":1}}],[\"longtensor\",{\"1\":{\"286\":1,\"477\":2,\"514\":1}}],[\"long\",{\"1\":{\"43\":1,\"92\":5,\"142\":1,\"143\":1,\"145\":4,\"146\":1,\"147\":4,\"160\":1,\"161\":1,\"162\":2,\"163\":1,\"246\":1,\"247\":1,\"282\":1,\"284\":4,\"285\":1,\"286\":1,\"510\":1,\"523\":1}}],[\"longest\",{\"1\":{\"40\":1,\"142\":1,\"159\":1}}],[\"lower\",{\"1\":{\"68\":1,\"275\":1,\"277\":1,\"412\":1,\"519\":1}}],[\"low\",{\"1\":{\"28\":1,\"420\":1,\"422\":1,\"424\":2}}],[\"l\",{\"1\":{\"4\":1,\"45\":5,\"70\":2,\"72\":1,\"73\":2,\"74\":1,\"75\":1,\"76\":12,\"105\":1,\"131\":3,\"132\":1,\"145\":7,\"147\":2,\"160\":1,\"161\":3,\"163\":2,\"226\":1,\"227\":1,\"247\":4,\"272\":5,\"275\":1,\"470\":1,\"519\":3}}],[\"1=none\",{\"1\":{\"520\":3}}],[\"1m\",{\"1\":{\"493\":1,\"674\":5}}],[\"1分\",{\"1\":{\"484\":1}}],[\"1论文\",{\"0\":{\"478\":1}}],[\"1bw\",{\"1\":{\"455\":1}}],[\"1b\",{\"1\":{\"447\":1}}],[\"1b参数\",{\"1\":{\"188\":1}}],[\"1×3虚拟扩展为\",{\"1\":{\"327\":1}}],[\"1×3\",{\"1\":{\"327\":1}}],[\"1x4\",{\"1\":{\"325\":1}}],[\"1x1\",{\"1\":{\"73\":1}}],[\"1rkdjdlr37o7gsr9j1mhjbg\",{\"1\":{\"300\":1}}],[\"1标志位表示visual\",{\"1\":{\"257\":1}}],[\"1+k\",{\"1\":{\"247\":1}}],[\"1个0\",{\"1\":{\"240\":1}}],[\"1个1\",{\"1\":{\"240\":1}}],[\"1个图片经过编码器2得到的表征都是负样本\",{\"1\":{\"240\":1}}],[\"1~12个448×448图块\",{\"1\":{\"219\":1}}],[\"1~12个448×448区块\",{\"1\":{\"214\":1}}],[\"1v\",{\"1\":{\"210\":1}}],[\"1k\",{\"1\":{\"193\":1}}],[\"1亿参数\",{\"1\":{\"189\":1}}],[\"14b\",{\"1\":{\"674\":1}}],[\"141\",{\"1\":{\"482\":1,\"484\":1}}],[\"14×14\",{\"1\":{\"396\":2}}],[\"14模型\",{\"1\":{\"272\":1}}],[\"14则需要在256个v100\",{\"1\":{\"272\":1}}],[\"14238\",{\"1\":{\"179\":1}}],[\"14\",{\"1\":{\"161\":1,\"226\":1,\"227\":1,\"272\":2,\"275\":1,\"291\":2,\"293\":2,\"323\":1,\"455\":1,\"474\":1,\"650\":1,\"662\":2,\"666\":2,\"670\":1}}],[\"1400\",{\"1\":{\"131\":1}}],[\"1节中的跨模态解码器\",{\"1\":{\"142\":1}}],[\"1节中提到的多模态融合模块配置\",{\"1\":{\"142\":1}}],[\"1节实验设置中使用384x384\",{\"1\":{\"142\":1}}],[\"137mo\",{\"1\":{\"289\":1}}],[\"1317\",{\"1\":{\"196\":1}}],[\"13b模型在单v100\",{\"1\":{\"482\":1}}],[\"13b性能优于gpt\",{\"1\":{\"482\":1}}],[\"13b在多数基准测试中优于gpt\",{\"1\":{\"479\":1}}],[\"13b\",{\"1\":{\"195\":2,\"196\":1}}],[\"13\",{\"1\":{\"137\":1,\"145\":4,\"161\":1,\"323\":1,\"474\":1,\"480\":1}}],[\"1d\",{\"1\":{\"107\":1}}],[\"1️⃣\",{\"0\":{\"72\":1},\"1\":{\"100\":1}}],[\"1750\",{\"1\":{\"673\":1,\"674\":1}}],[\"1750亿参数\",{\"1\":{\"460\":1}}],[\"175b的69\",{\"1\":{\"484\":1}}],[\"175b\",{\"1\":{\"469\":1,\"470\":1,\"471\":2,\"472\":2,\"479\":1,\"482\":2,\"673\":1}}],[\"175b也只能在few\",{\"1\":{\"462\":1}}],[\"175k\",{\"1\":{\"200\":1}}],[\"17个任务\",{\"1\":{\"453\":1}}],[\"170\",{\"1\":{\"355\":1}}],[\"1706\",{\"1\":{\"85\":1}}],[\"174\",{\"1\":{\"65\":1}}],[\"17\",{\"1\":{\"65\":1,\"67\":1,\"75\":1,\"161\":1,\"323\":1}}],[\"1f242tsdxjrzkkqotibsin2u6rjagrz2w\",{\"1\":{\"47\":1}}],[\"119\",{\"1\":{\"674\":1}}],[\"11929\",{\"1\":{\"300\":1}}],[\"117m\",{\"1\":{\"454\":1,\"455\":1}}],[\"110\",{\"1\":{\"75\":1}}],[\"11\",{\"1\":{\"40\":1,\"82\":1,\"161\":1,\"229\":1,\"323\":1,\"455\":1,\"474\":1,\"519\":1,\"546\":2,\"674\":4}}],[\"12597\",{\"1\":{\"279\":1}}],[\"12m\",{\"1\":{\"131\":1}}],[\"1200000000\",{\"1\":{\"297\":1}}],[\"120亿参数的transformer训练于2\",{\"1\":{\"177\":1}}],[\"120\",{\"1\":{\"65\":1}}],[\"128k\",{\"1\":{\"674\":9}}],[\"128\",{\"1\":{\"35\":2,\"59\":1,\"70\":1,\"73\":1,\"93\":5,\"96\":10,\"98\":4,\"99\":4,\"100\":1,\"101\":12,\"107\":4,\"109\":3,\"111\":3,\"194\":1,\"242\":2,\"246\":2,\"247\":1,\"540\":1}}],[\"127\",{\"1\":{\"23\":1}}],[\"12\",{\"1\":{\"23\":1,\"24\":1,\"82\":1,\"136\":1,\"152\":1,\"161\":1,\"174\":2,\"323\":4,\"397\":1,\"447\":2,\"474\":1,\"519\":6,\"662\":1,\"666\":1,\"674\":3}}],[\"1e9\",{\"1\":{\"517\":1,\"558\":1}}],[\"1e10\",{\"1\":{\"92\":2}}],[\"1e\",{\"1\":{\"22\":1,\"78\":7,\"83\":1,\"100\":1,\"147\":2,\"200\":2,\"296\":1,\"494\":1,\"514\":1}}],[\"15的概率mask掉tokens\",{\"1\":{\"258\":1}}],[\"158\",{\"1\":{\"227\":1}}],[\"158k\",{\"1\":{\"227\":2}}],[\"1586\",{\"1\":{\"195\":1}}],[\"15\",{\"1\":{\"17\":1,\"63\":1,\"68\":1,\"83\":1,\"131\":1,\"155\":1,\"161\":1,\"163\":1,\"229\":1,\"323\":1,\"474\":1,\"480\":1,\"481\":1,\"482\":1,\"493\":1,\"505\":1,\"673\":1,\"674\":3}}],[\"15k交互图像\",{\"1\":{\"6\":1}}],[\"180\",{\"1\":{\"355\":1}}],[\"18\",{\"1\":{\"17\":1,\"207\":1,\"323\":1,\"447\":1,\"662\":1,\"666\":1}}],[\"16k\",{\"1\":{\"674\":1}}],[\"16gb\",{\"1\":{\"493\":1,\"494\":2,\"498\":1}}],[\"16×16\",{\"1\":{\"300\":1}}],[\"16这个模型进行微调\",{\"1\":{\"300\":1}}],[\"16x16\",{\"1\":{\"290\":1}}],[\"16为例\",{\"1\":{\"290\":1,\"291\":1}}],[\"16和vit\",{\"1\":{\"272\":1}}],[\"16倍和64倍得到的\",{\"1\":{\"272\":1}}],[\"160gb\",{\"1\":{\"493\":1,\"494\":2}}],[\"160\",{\"1\":{\"201\":1,\"355\":1}}],[\"164k\",{\"1\":{\"200\":1}}],[\"1612\",{\"1\":{\"102\":1}}],[\"162张图像\",{\"1\":{\"49\":1}}],[\"16\",{\"1\":{\"10\":1,\"22\":1,\"59\":8,\"65\":1,\"73\":1,\"83\":1,\"96\":1,\"101\":1,\"131\":2,\"152\":1,\"161\":1,\"290\":2,\"291\":3,\"300\":1,\"323\":1,\"396\":1,\"397\":3,\"470\":1,\"474\":1,\"482\":1,\"684\":1}}],[\"101\",{\"1\":{\"520\":2}}],[\"104\",{\"1\":{\"520\":1}}],[\"10437\",{\"1\":{\"47\":1}}],[\"10份\",{\"1\":{\"495\":1}}],[\"103是超过\",{\"1\":{\"510\":1}}],[\"103两个版本\",{\"1\":{\"510\":1}}],[\"103\",{\"1\":{\"510\":5,\"511\":1,\"520\":1}}],[\"103测试集1\",{\"1\":{\"455\":1}}],[\"1035\",{\"1\":{\"65\":1}}],[\"102\",{\"1\":{\"520\":2}}],[\"1022\",{\"1\":{\"196\":1}}],[\"1024词\",{\"1\":{\"178\":1}}],[\"1024+64\",{\"1\":{\"109\":1}}],[\"1024维\",{\"1\":{\"107\":1,\"109\":1,\"110\":2}}],[\"1024\",{\"1\":{\"34\":1,\"36\":1,\"93\":3,\"96\":3,\"99\":2,\"100\":1,\"101\":3,\"107\":9,\"109\":7,\"110\":1,\"111\":2,\"203\":1,\"204\":1}}],[\"100个任务示例作为上下文\",{\"1\":{\"461\":1}}],[\"1000\",{\"1\":{\"404\":1,\"667\":1}}],[\"10000\",{\"1\":{\"284\":3,\"528\":1}}],[\"100万\",{\"1\":{\"240\":1}}],[\"100万个图片\",{\"1\":{\"240\":1}}],[\"100\",{\"1\":{\"142\":2,\"145\":1,\"147\":1,\"159\":1,\"163\":8,\"226\":2,\"239\":1,\"273\":1,\"275\":1,\"277\":1,\"285\":2,\"343\":2,\"344\":1,\"351\":1,\"355\":2,\"359\":1,\"404\":1,\"447\":1,\"461\":1,\"514\":1,\"520\":2,\"658\":6,\"667\":3,\"674\":2}}],[\"1088\",{\"1\":{\"109\":2,\"111\":1}}],[\"10\",{\"0\":{\"46\":1},\"1\":{\"10\":2,\"40\":1,\"63\":1,\"82\":2,\"161\":1,\"163\":2,\"190\":1,\"191\":1,\"194\":1,\"203\":1,\"204\":1,\"216\":1,\"239\":1,\"323\":1,\"366\":2,\"474\":1,\"482\":1,\"493\":3,\"495\":3,\"505\":2,\"511\":4,\"512\":2,\"558\":3,\"665\":1,\"674\":9}}],[\"1\",{\"0\":{\"22\":1,\"62\":1,\"179\":1,\"206\":1,\"223\":1,\"262\":1,\"282\":1,\"283\":1,\"290\":1,\"307\":1,\"312\":1,\"438\":1,\"672\":1},\"1\":{\"7\":1,\"22\":1,\"24\":1,\"26\":1,\"28\":5,\"29\":6,\"30\":5,\"32\":8,\"34\":4,\"35\":37,\"36\":28,\"40\":4,\"41\":10,\"43\":3,\"45\":14,\"46\":16,\"58\":2,\"59\":47,\"64\":1,\"65\":1,\"68\":1,\"70\":29,\"73\":2,\"75\":2,\"76\":14,\"78\":13,\"80\":1,\"81\":2,\"82\":12,\"83\":10,\"92\":34,\"93\":3,\"96\":9,\"98\":2,\"99\":2,\"100\":17,\"101\":4,\"105\":1,\"107\":12,\"108\":4,\"109\":13,\"111\":8,\"112\":1,\"127\":1,\"131\":2,\"142\":2,\"143\":4,\"145\":30,\"146\":3,\"147\":14,\"154\":1,\"155\":1,\"159\":1,\"160\":2,\"161\":10,\"162\":5,\"163\":8,\"166\":1,\"174\":1,\"179\":1,\"190\":2,\"191\":1,\"194\":1,\"195\":1,\"200\":1,\"202\":2,\"204\":1,\"207\":3,\"208\":2,\"210\":1,\"212\":1,\"214\":1,\"215\":2,\"217\":1,\"219\":3,\"220\":1,\"221\":1,\"222\":1,\"223\":1,\"229\":2,\"240\":1,\"244\":5,\"246\":2,\"247\":2,\"248\":1,\"249\":1,\"259\":1,\"262\":2,\"264\":1,\"266\":3,\"268\":12,\"270\":1,\"273\":5,\"275\":4,\"277\":4,\"281\":1,\"282\":7,\"283\":13,\"284\":15,\"285\":20,\"286\":7,\"289\":2,\"290\":2,\"291\":5,\"292\":10,\"293\":10,\"295\":16,\"296\":10,\"297\":2,\"300\":2,\"321\":2,\"322\":4,\"323\":14,\"325\":9,\"326\":17,\"327\":12,\"343\":2,\"344\":1,\"345\":1,\"346\":2,\"348\":2,\"350\":3,\"351\":5,\"353\":1,\"359\":2,\"374\":2,\"384\":2,\"386\":8,\"387\":19,\"389\":1,\"395\":3,\"396\":1,\"397\":2,\"401\":7,\"402\":7,\"403\":10,\"404\":11,\"405\":7,\"407\":12,\"410\":18,\"411\":11,\"412\":12,\"432\":1,\"438\":1,\"444\":1,\"445\":1,\"447\":2,\"454\":5,\"455\":8,\"460\":1,\"461\":5,\"462\":2,\"463\":1,\"464\":1,\"469\":3,\"470\":4,\"471\":3,\"472\":3,\"474\":7,\"477\":19,\"478\":1,\"481\":3,\"482\":4,\"483\":2,\"484\":2,\"492\":1,\"493\":1,\"494\":4,\"495\":1,\"499\":1,\"504\":1,\"505\":1,\"506\":9,\"508\":1,\"510\":2,\"511\":1,\"512\":9,\"513\":5,\"514\":8,\"517\":4,\"520\":26,\"522\":1,\"523\":2,\"528\":2,\"529\":6,\"531\":5,\"538\":5,\"540\":6,\"541\":5,\"542\":2,\"543\":6,\"544\":15,\"546\":1,\"550\":1,\"553\":2,\"556\":2,\"558\":15,\"566\":4,\"567\":1,\"569\":1,\"576\":1,\"584\":1,\"590\":1,\"596\":2,\"600\":3,\"603\":1,\"608\":2,\"617\":1,\"626\":4,\"632\":1,\"646\":1,\"649\":1,\"651\":1,\"656\":2,\"658\":4,\"659\":8,\"660\":9,\"662\":10,\"664\":1,\"666\":12,\"667\":13,\"674\":18}}],[\"198\",{\"1\":{\"474\":1}}],[\"1980\",{\"1\":{\"6\":1}}],[\"1930\",{\"1\":{\"472\":1}}],[\"197\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"196+1\",{\"1\":{\"293\":1}}],[\"196\",{\"1\":{\"291\":2,\"292\":1,\"293\":1,\"296\":1}}],[\"196×196\",{\"1\":{\"190\":1,\"200\":1}}],[\"19626\",{\"1\":{\"4\":1}}],[\"190\",{\"1\":{\"83\":3}}],[\"19\",{\"1\":{\"65\":1,\"67\":1,\"69\":1,\"133\":1,\"323\":1,\"662\":1,\"666\":1}}],[\"1asow2t2mltykaia\",{\"1\":{\"4\":1}}],[\"1n\",{\"1\":{\"4\":1}}],[\"s型智能增长曲线\",{\"1\":{\"688\":1}}],[\"srl\",{\"1\":{\"543\":1}}],[\"src\",{\"1\":{\"477\":1,\"549\":16,\"556\":7,\"557\":2}}],[\"s3\",{\"1\":{\"510\":2}}],[\"s>e\",{\"1\":{\"508\":1}}],[\"s=2\",{\"1\":{\"508\":2}}],[\"s=str\",{\"1\":{\"289\":1}}],[\"swiglu\",{\"1\":{\"481\":1,\"674\":4}}],[\"swapaxes\",{\"1\":{\"34\":3,\"35\":5,\"36\":5,\"59\":5}}],[\"sørensen\",{\"1\":{\"401\":1}}],[\"slice\",{\"0\":{\"325\":1}}],[\"slight\",{\"1\":{\"28\":2}}],[\"snapshot\",{\"1\":{\"275\":1,\"277\":2}}],[\"sst\",{\"1\":{\"448\":4,\"492\":1,\"494\":1}}],[\"ssl\",{\"1\":{\"244\":2}}],[\"ssg\",{\"1\":{\"93\":3}}],[\"sgd\",{\"1\":{\"204\":1,\"244\":2,\"670\":1}}],[\"sft\",{\"1\":{\"190\":1,\"202\":1,\"468\":1,\"470\":6,\"471\":1,\"472\":1,\"674\":1}}],[\"smooth\",{\"1\":{\"401\":4,\"402\":3,\"403\":3,\"404\":1,\"405\":3,\"407\":3}}],[\"smooth=1\",{\"1\":{\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1}}],[\"smoothing=0\",{\"1\":{\"268\":1,\"283\":2,\"285\":1,\"514\":2}}],[\"smoothing\",{\"1\":{\"163\":1}}],[\"small\",{\"1\":{\"82\":1}}],[\"sbu\",{\"1\":{\"131\":1,\"140\":1}}],[\"symb\",{\"1\":{\"511\":2}}],[\"symbol\",{\"1\":{\"474\":1}}],[\"symbols\",{\"1\":{\"410\":4,\"412\":4}}],[\"symlinks=false\",{\"1\":{\"275\":1,\"277\":1}}],[\"symmetric\",{\"1\":{\"105\":1,\"115\":1}}],[\"systematic\",{\"1\":{\"274\":1,\"421\":1}}],[\"sys\",{\"1\":{\"83\":3,\"525\":1,\"535\":1}}],[\"squad和race任务上实现了sota性能\",{\"1\":{\"502\":1}}],[\"squad和race等任务上达到了新的\",{\"1\":{\"495\":1}}],[\"squad\",{\"1\":{\"470\":1,\"471\":1,\"472\":1,\"492\":2,\"494\":1,\"498\":2,\"499\":1,\"542\":1}}],[\"squadv2\",{\"1\":{\"462\":1}}],[\"squaretest\",{\"1\":{\"645\":1,\"646\":1}}],[\"square\",{\"1\":{\"92\":1,\"100\":1,\"613\":1,\"614\":2,\"617\":2,\"631\":1,\"641\":2,\"645\":2,\"646\":2,\"655\":3,\"658\":7}}],[\"squeeze\",{\"1\":{\"70\":1,\"83\":1,\"283\":2,\"540\":2,\"541\":2}}],[\"sqrdists\",{\"1\":{\"92\":2}}],[\"sqrt\",{\"1\":{\"83\":1,\"266\":1,\"285\":1,\"531\":1,\"558\":1}}],[\"sonnet\",{\"1\":{\"674\":4}}],[\"soap\",{\"1\":{\"592\":1}}],[\"sometimes\",{\"1\":{\"541\":1}}],[\"someone\",{\"1\":{\"372\":3}}],[\"southampton\",{\"1\":{\"519\":1}}],[\"source\",{\"1\":{\"206\":2,\"300\":1,\"339\":1}}],[\"source=chatgpt\",{\"1\":{\"140\":1}}],[\"solaiman\",{\"1\":{\"469\":1}}],[\"sol\",{\"1\":{\"411\":2,\"412\":3}}],[\"solely\",{\"1\":{\"78\":1}}],[\"sota\",{\"1\":{\"112\":1,\"138\":1,\"190\":1,\"194\":1,\"195\":1,\"227\":1,\"228\":1,\"455\":1,\"492\":1}}],[\"sorted\",{\"1\":{\"68\":1,\"365\":1,\"410\":1,\"412\":1}}],[\"sort\",{\"1\":{\"68\":5,\"92\":1,\"100\":1,\"289\":1,\"656\":1,\"658\":1,\"666\":1}}],[\"soft\",{\"1\":{\"64\":1,\"78\":2,\"82\":10,\"145\":1,\"147\":3,\"157\":1,\"159\":4,\"161\":3,\"162\":1,\"163\":13,\"401\":1,\"402\":2,\"403\":1,\"404\":1}}],[\"softmax归一化得到注意力概率\",{\"1\":{\"285\":1}}],[\"softmax\",{\"1\":{\"32\":2,\"41\":5,\"45\":3,\"59\":2,\"93\":1,\"96\":1,\"98\":2,\"101\":2,\"110\":2,\"111\":2,\"115\":2,\"145\":6,\"147\":6,\"154\":1,\"156\":1,\"157\":1,\"160\":1,\"161\":4,\"162\":2,\"163\":3,\"166\":1,\"170\":1,\"175\":1,\"240\":7,\"246\":1,\"266\":2,\"273\":1,\"284\":2,\"285\":1,\"295\":1,\"308\":1,\"318\":1,\"319\":1,\"404\":2,\"445\":1,\"508\":1,\"517\":1,\"531\":1,\"540\":3,\"548\":1,\"550\":1,\"558\":1}}],[\"sanh\",{\"1\":{\"469\":1}}],[\"say\",{\"1\":{\"369\":5,\"372\":3}}],[\"saucer\",{\"1\":{\"273\":1}}],[\"sam\",{\"1\":{\"212\":1}}],[\"same\",{\"1\":{\"145\":2,\"536\":1,\"558\":1}}],[\"sampling=false\",{\"1\":{\"286\":1}}],[\"sampling\",{\"0\":{\"89\":1},\"1\":{\"59\":2,\"88\":2,\"89\":1,\"92\":1,\"98\":1,\"133\":2,\"143\":1,\"240\":1,\"286\":2}}],[\"sampler=train\",{\"1\":{\"244\":1,\"522\":1}}],[\"sampler\",{\"1\":{\"244\":2,\"522\":2}}],[\"samplers\",{\"1\":{\"142\":1,\"145\":1,\"147\":1}}],[\"sample=use\",{\"1\":{\"286\":1}}],[\"sample=true\",{\"1\":{\"143\":1}}],[\"sample=false\",{\"1\":{\"143\":1}}],[\"sampled\",{\"1\":{\"92\":6,\"96\":1}}],[\"samples\",{\"1\":{\"82\":1,\"92\":1,\"282\":3,\"286\":2}}],[\"sample\",{\"1\":{\"28\":1,\"29\":6,\"35\":6,\"46\":6,\"58\":3,\"59\":6,\"70\":11,\"76\":7,\"87\":1,\"92\":14,\"96\":2,\"143\":2,\"289\":1,\"296\":1,\"512\":7,\"666\":3}}],[\"salesforce\",{\"1\":{\"119\":1,\"140\":1,\"148\":1,\"279\":1,\"510\":1}}],[\"sa4\",{\"1\":{\"101\":2}}],[\"sa3\",{\"1\":{\"93\":2,\"96\":2,\"101\":2}}],[\"sa\",{\"1\":{\"93\":1,\"99\":3,\"101\":1,\"126\":4,\"134\":2}}],[\"saved\",{\"1\":{\"511\":3,\"514\":1}}],[\"save\",{\"1\":{\"82\":2,\"275\":3,\"277\":3,\"285\":3,\"389\":1,\"410\":1,\"412\":1,\"514\":1,\"519\":1}}],[\"sa1\",{\"1\":{\"46\":1,\"59\":1,\"70\":1,\"93\":2,\"96\":2,\"101\":2}}],[\"sa2\",{\"1\":{\"46\":1,\"59\":1,\"70\":1,\"93\":2,\"96\":2,\"101\":2}}],[\"sv\",{\"1\":{\"45\":2}}],[\"skip\",{\"1\":{\"98\":1,\"100\":1,\"143\":1,\"286\":1,\"542\":2}}],[\"sk\",{\"1\":{\"45\":2}}],[\"skateboard\",{\"1\":{\"29\":1}}],[\"scao\",{\"1\":{\"485\":1}}],[\"scalar\",{\"1\":{\"404\":1}}],[\"scaling\",{\"1\":{\"179\":1,\"421\":1,\"460\":1,\"461\":1,\"464\":2,\"472\":1,\"673\":7}}],[\"scaleai\",{\"1\":{\"470\":1}}],[\"scaleddotproductattention\",{\"1\":{\"517\":1}}],[\"scaled\",{\"1\":{\"305\":1,\"558\":1}}],[\"scale未指定\",{\"1\":{\"295\":1}}],[\"scale=qk\",{\"1\":{\"294\":1,\"296\":1}}],[\"scale=none\",{\"1\":{\"293\":1,\"294\":1,\"295\":1,\"296\":1}}],[\"scale=\",{\"1\":{\"244\":1}}],[\"scale=0\",{\"1\":{\"147\":1}}],[\"scales\",{\"1\":{\"98\":1}}],[\"scale\",{\"0\":{\"95\":1},\"1\":{\"32\":2,\"41\":2,\"45\":3,\"59\":5,\"83\":2,\"93\":1,\"94\":1,\"96\":1,\"294\":1,\"295\":3,\"296\":1,\"418\":1,\"421\":1}}],[\"science\",{\"1\":{\"227\":1}}],[\"scienceqa\",{\"1\":{\"227\":2,\"228\":1}}],[\"scissors\",{\"1\":{\"29\":1,\"58\":1,\"63\":2,\"68\":1}}],[\"schema\",{\"0\":{\"256\":1},\"1\":{\"455\":1,\"462\":1}}],[\"scheduler\",{\"1\":{\"80\":4,\"159\":1}}],[\"schuhmann\",{\"1\":{\"131\":1}}],[\"scores\",{\"1\":{\"162\":5,\"163\":6,\"266\":11,\"268\":7,\"285\":20,\"517\":4,\"531\":12,\"537\":2,\"538\":4,\"540\":4,\"543\":1,\"558\":4}}],[\"score\",{\"1\":{\"46\":3,\"78\":1,\"82\":5,\"224\":1,\"283\":2,\"284\":2,\"359\":1,\"401\":2,\"402\":2,\"403\":4,\"407\":4,\"537\":2,\"538\":4}}],[\"scene\",{\"1\":{\"45\":8,\"59\":7}}],[\"sudo\",{\"1\":{\"666\":1}}],[\"sunflowers\",{\"1\":{\"276\":1,\"277\":1}}],[\"survey\",{\"1\":{\"274\":1,\"421\":1,\"688\":2}}],[\"surfboard\",{\"1\":{\"29\":1}}],[\"suites\",{\"1\":{\"206\":2}}],[\"suitcase\",{\"1\":{\"29\":1}}],[\"successfully\",{\"1\":{\"275\":1,\"277\":1}}],[\"successful\",{\"1\":{\"63\":1}}],[\"subprocess\",{\"1\":{\"666\":1}}],[\"sublayer是传入的参数\",{\"1\":{\"552\":1}}],[\"sublayer\",{\"1\":{\"552\":3,\"553\":3,\"556\":4}}],[\"sublayerconnection模型结构图\",{\"1\":{\"552\":1}}],[\"sublayerconnection\",{\"0\":{\"552\":1},\"1\":{\"552\":2,\"553\":1,\"556\":1}}],[\"subword\",{\"1\":{\"410\":4,\"412\":2}}],[\"subset的对齐分数\",{\"1\":{\"258\":1}}],[\"subset和visual\",{\"1\":{\"258\":1}}],[\"subject\",{\"1\":{\"58\":2,\"59\":1}}],[\"sub\",{\"1\":{\"45\":8,\"58\":5,\"59\":24,\"275\":5,\"277\":5,\"325\":1,\"410\":1,\"412\":1,\"660\":9}}],[\"summation\",{\"1\":{\"76\":1,\"115\":1,\"390\":1}}],[\"sum\",{\"1\":{\"43\":1,\"70\":2,\"76\":4,\"78\":6,\"81\":1,\"82\":3,\"83\":1,\"92\":1,\"100\":2,\"115\":1,\"145\":3,\"147\":2,\"161\":2,\"163\":1,\"247\":1,\"268\":1,\"285\":1,\"289\":1,\"296\":1,\"390\":1,\"401\":3,\"402\":9,\"403\":2,\"405\":3,\"407\":3}}],[\"suppose\",{\"1\":{\"517\":1}}],[\"supported\",{\"1\":{\"289\":2,\"643\":1,\"656\":1,\"659\":1}}],[\"support\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"superglue整体表现良好\",{\"1\":{\"462\":1}}],[\"supervision\",{\"1\":{\"252\":2}}],[\"supervised\",{\"1\":{\"190\":1,\"224\":1,\"278\":1,\"416\":1,\"470\":1}}],[\"super\",{\"1\":{\"29\":1,\"34\":2,\"35\":2,\"36\":2,\"41\":1,\"45\":2,\"46\":1,\"58\":1,\"59\":6,\"73\":1,\"78\":1,\"92\":1,\"93\":1,\"96\":2,\"100\":1,\"101\":1,\"107\":1,\"109\":1,\"110\":1,\"111\":1,\"142\":1,\"147\":1,\"160\":1,\"246\":1,\"262\":1,\"265\":1,\"268\":1,\"291\":1,\"292\":1,\"293\":1,\"294\":2,\"295\":1,\"296\":1,\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1,\"513\":1,\"523\":1,\"525\":3,\"526\":1,\"527\":1,\"528\":1,\"529\":1,\"531\":1,\"532\":1,\"533\":1,\"535\":1,\"536\":1,\"537\":1,\"538\":1,\"541\":1,\"543\":1,\"544\":1,\"549\":1,\"550\":1,\"552\":1,\"553\":1,\"554\":1,\"556\":1,\"557\":1,\"558\":1}}],[\"s\",{\"1\":{\"28\":8,\"31\":6,\"45\":4,\"59\":8,\"92\":16,\"96\":8,\"100\":11,\"105\":1,\"145\":1,\"289\":1,\"300\":1,\"379\":1,\"410\":2,\"412\":2,\"433\":1,\"454\":3,\"455\":1,\"472\":3,\"474\":2,\"508\":2,\"520\":1,\"596\":1}}],[\"shell=true\",{\"1\":{\"666\":1}}],[\"shells\",{\"0\":{\"591\":1}}],[\"shinyapps\",{\"1\":{\"573\":1}}],[\"shifted\",{\"1\":{\"268\":2,\"285\":3}}],[\"shuffle\",{\"1\":{\"214\":1,\"247\":3,\"512\":3}}],[\"shuffle=\",{\"1\":{\"244\":1}}],[\"shuffle=false\",{\"1\":{\"80\":2,\"290\":1}}],[\"shuffle=true\",{\"1\":{\"80\":1,\"290\":1,\"514\":1}}],[\"show\",{\"1\":{\"276\":1,\"277\":1,\"289\":1,\"667\":1}}],[\"shown\",{\"1\":{\"28\":2}}],[\"shot条件下进行全面评估的工作\",{\"1\":{\"464\":1}}],[\"shot模型\",{\"1\":{\"464\":1}}],[\"shot能力会随着模型规模的增加而显著增强\",{\"1\":{\"464\":1}}],[\"shot能力主要依赖于识别任务格式和输出模式\",{\"1\":{\"463\":1}}],[\"shot效果难以稳定复现\",{\"1\":{\"463\":1}}],[\"shot情境下\",{\"1\":{\"462\":1}}],[\"shot元学习能力\",{\"1\":{\"462\":1}}],[\"shot得分\",{\"1\":{\"462\":1}}],[\"shot得分达到77\",{\"1\":{\"462\":1}}],[\"shot得分达到71\",{\"1\":{\"462\":1}}],[\"shot翻译任务中\",{\"1\":{\"462\":1}}],[\"shot优势\",{\"1\":{\"462\":1}}],[\"shot设定就达到了与微调sota模型相当甚至更优的水平\",{\"1\":{\"462\":1}}],[\"shot设定下均展示了强大的任务适应能力\",{\"1\":{\"462\":1}}],[\"shot设定下稍高于随机水平\",{\"1\":{\"462\":1}}],[\"shot设定下表现优异\",{\"1\":{\"462\":1}}],[\"shot设定下\",{\"1\":{\"461\":1}}],[\"shot设定下的表现令人惊喜\",{\"1\":{\"461\":1}}],[\"shot设置下准确率达到86\",{\"1\":{\"462\":1}}],[\"shot图文生成\",{\"1\":{\"281\":1}}],[\"shot图像分类\",{\"1\":{\"273\":1}}],[\"shot性能评估\",{\"1\":{\"278\":1}}],[\"shot性能\",{\"1\":{\"278\":1}}],[\"shot迁移到下游任务\",{\"1\":{\"278\":1}}],[\"shot学习方面展现出极强的能力\",{\"1\":{\"463\":1}}],[\"shot学习的启发\",{\"1\":{\"464\":1}}],[\"shot学习的\",{\"1\":{\"463\":1}}],[\"shot学习中可用的示例数量\",{\"1\":{\"463\":1}}],[\"shot学习\",{\"1\":{\"278\":1,\"464\":1}}],[\"shot推理\",{\"1\":{\"275\":1}}],[\"shot分类时\",{\"1\":{\"274\":1}}],[\"shot分类的过程相当直接\",{\"1\":{\"273\":1}}],[\"shot\",{\"0\":{\"176\":1},\"1\":{\"176\":2,\"194\":2,\"224\":1,\"280\":1,\"440\":1,\"452\":1,\"455\":1,\"458\":1,\"460\":3,\"461\":3,\"464\":2,\"470\":2,\"471\":2,\"482\":3,\"483\":1}}],[\"should\",{\"1\":{\"83\":3}}],[\"shared\",{\"1\":{\"513\":4}}],[\"sharegpt4v\",{\"1\":{\"217\":1}}],[\"shamir\",{\"1\":{\"395\":1}}],[\"shape=box\",{\"1\":{\"666\":4}}],[\"shape=box可将节点设为矩形\",{\"1\":{\"666\":1}}],[\"shape为\",{\"1\":{\"291\":1,\"328\":1}}],[\"shape的形状从\",{\"1\":{\"92\":1}}],[\"shape\",{\"1\":{\"35\":1,\"40\":8,\"68\":6,\"82\":11,\"83\":2,\"92\":15,\"93\":1,\"96\":2,\"98\":1,\"100\":2,\"107\":3,\"109\":3,\"111\":4,\"145\":3,\"162\":1,\"163\":4,\"247\":1,\"249\":1,\"266\":2,\"282\":1,\"284\":1,\"285\":2,\"286\":1,\"291\":1,\"292\":1,\"293\":1,\"295\":1,\"296\":2,\"315\":1,\"321\":1,\"323\":2,\"325\":2,\"327\":3,\"381\":1,\"384\":2,\"385\":1,\"387\":4,\"404\":1,\"477\":10,\"513\":7,\"531\":4,\"544\":1,\"659\":3,\"666\":1}}],[\"shao\",{\"1\":{\"4\":1}}],[\"sphere\",{\"1\":{\"662\":2}}],[\"sphere函数求导\",{\"1\":{\"662\":1}}],[\"spherical\",{\"1\":{\"590\":1}}],[\"spurious\",{\"1\":{\"460\":1}}],[\"span\",{\"1\":{\"540\":1,\"542\":2}}],[\"sparse\",{\"1\":{\"461\":1}}],[\"space\",{\"1\":{\"412\":1}}],[\"spacy\",{\"1\":{\"226\":1,\"447\":1}}],[\"spatial\",{\"1\":{\"40\":5,\"45\":3,\"107\":1}}],[\"special\",{\"1\":{\"143\":1,\"286\":1,\"520\":6,\"542\":2}}],[\"speak\",{\"1\":{\"29\":1}}],[\"spidercnn\",{\"1\":{\"112\":2}}],[\"spoon\",{\"1\":{\"29\":1}}],[\"spout\",{\"1\":{\"24\":1,\"28\":6,\"31\":2}}],[\"splitext\",{\"1\":{\"275\":1,\"277\":1,\"289\":1,\"666\":1}}],[\"split=\",{\"1\":{\"68\":1}}],[\"split\",{\"1\":{\"28\":1,\"29\":8,\"40\":2,\"46\":4,\"58\":6,\"59\":4,\"68\":5,\"275\":1,\"277\":1,\"289\":1,\"290\":1,\"410\":2,\"412\":2,\"477\":2,\"510\":1,\"540\":2,\"541\":1}}],[\"spring\",{\"1\":{\"2\":1}}],[\"siri\",{\"1\":{\"678\":1}}],[\"sin\",{\"1\":{\"665\":1}}],[\"single\",{\"1\":{\"93\":1}}],[\"silhouette\",{\"1\":{\"273\":1}}],[\"sig\",{\"1\":{\"410\":1,\"411\":1,\"412\":1}}],[\"sigma\",{\"1\":{\"395\":1,\"566\":3}}],[\"sigmoid\",{\"1\":{\"15\":1,\"35\":3,\"46\":3,\"59\":3,\"70\":1,\"76\":3,\"78\":1,\"98\":1,\"401\":3,\"402\":4,\"403\":4,\"404\":3,\"405\":3,\"407\":1}}],[\"siglip\",{\"1\":{\"212\":2}}],[\"sign\",{\"1\":{\"82\":1}}],[\"side\",{\"1\":{\"40\":2}}],[\"size和较小的学习率\",{\"1\":{\"461\":1}}],[\"size的四倍\",{\"1\":{\"297\":1}}],[\"size是transformer\",{\"1\":{\"297\":1}}],[\"size就是对应通过embedding层后每个token的dim\",{\"1\":{\"297\":1}}],[\"size也是同样处理手段\",{\"1\":{\"282\":1}}],[\"size\",{\"1\":{\"29\":4,\"30\":1,\"36\":1,\"40\":6,\"41\":1,\"42\":2,\"43\":12,\"44\":3,\"45\":4,\"46\":3,\"58\":4,\"59\":13,\"70\":1,\"80\":3,\"83\":2,\"107\":3,\"108\":2,\"109\":1,\"111\":2,\"142\":6,\"143\":2,\"145\":14,\"146\":3,\"147\":12,\"160\":9,\"161\":3,\"162\":2,\"163\":6,\"238\":2,\"244\":2,\"246\":1,\"249\":4,\"262\":2,\"265\":3,\"266\":3,\"268\":8,\"275\":4,\"277\":4,\"282\":6,\"283\":8,\"284\":10,\"285\":7,\"286\":4,\"289\":1,\"290\":2,\"291\":24,\"292\":4,\"293\":2,\"295\":10,\"296\":4,\"321\":1,\"323\":1,\"387\":3,\"401\":4,\"402\":3,\"403\":3,\"404\":4,\"405\":5,\"407\":5,\"474\":1,\"477\":1,\"511\":3,\"513\":19,\"514\":2,\"517\":1,\"522\":1,\"523\":7,\"525\":5,\"527\":2,\"529\":1,\"531\":15,\"532\":3,\"535\":3,\"536\":3,\"537\":1,\"538\":1,\"540\":12,\"541\":3,\"543\":2,\"544\":13,\"552\":2,\"553\":4,\"554\":1,\"556\":4,\"557\":1,\"558\":3,\"659\":4}}],[\"size=32\",{\"1\":{\"514\":1}}],[\"size=384\",{\"1\":{\"145\":1}}],[\"size=768\",{\"1\":{\"300\":1}}],[\"size=none\",{\"1\":{\"293\":1,\"296\":1}}],[\"size=img\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"size=patch\",{\"1\":{\"291\":1,\"292\":1,\"293\":1,\"296\":1}}],[\"size=64\",{\"1\":{\"275\":2,\"277\":2}}],[\"size=args\",{\"1\":{\"244\":1,\"522\":1}}],[\"size=224\",{\"1\":{\"147\":1,\"291\":1,\"292\":1,\"293\":1,\"296\":1,\"300\":1}}],[\"size=57600\",{\"1\":{\"145\":1,\"147\":1}}],[\"size=config\",{\"1\":{\"142\":1,\"145\":2,\"147\":2,\"160\":2}}],[\"size=16\",{\"1\":{\"160\":2,\"291\":1,\"292\":1,\"293\":1,\"296\":1,\"300\":1,\"519\":2}}],[\"size=1\",{\"1\":{\"107\":1}}],[\"size=10\",{\"1\":{\"80\":1}}],[\"size=batch\",{\"1\":{\"80\":3,\"290\":2}}],[\"size=\",{\"1\":{\"29\":1,\"58\":1,\"59\":3,\"142\":1,\"145\":1,\"147\":1}}],[\"site\",{\"1\":{\"338\":1}}],[\"sit\",{\"1\":{\"29\":1,\"58\":1,\"68\":1,\"83\":2}}],[\"simply\",{\"1\":{\"527\":1}}],[\"simplicity\",{\"1\":{\"249\":1}}],[\"similarities\",{\"1\":{\"275\":2,\"276\":2,\"277\":4}}],[\"similarity\",{\"1\":{\"22\":1,\"82\":3,\"272\":1,\"273\":1,\"275\":2,\"276\":1,\"277\":3,\"283\":4,\"448\":1}}],[\"simclr\",{\"1\":{\"240\":1,\"242\":2}}],[\"sim\",{\"1\":{\"22\":1,\"23\":1,\"75\":1,\"82\":16,\"145\":20,\"146\":2,\"147\":20,\"161\":18,\"162\":3,\"283\":13,\"284\":4}}],[\"segments\",{\"1\":{\"513\":3}}],[\"segment\",{\"1\":{\"493\":1,\"495\":1,\"506\":1,\"512\":3,\"513\":1,\"514\":2,\"520\":1}}],[\"segmentation\",{\"1\":{\"60\":3,\"78\":1,\"82\":1,\"193\":1,\"273\":1,\"523\":1}}],[\"sense\",{\"1\":{\"482\":1}}],[\"sentiment\",{\"1\":{\"448\":1}}],[\"sent\",{\"1\":{\"410\":1,\"411\":1,\"412\":2,\"510\":1,\"511\":4}}],[\"sentences格式\",{\"1\":{\"495\":1}}],[\"sentences\",{\"1\":{\"410\":2,\"412\":2,\"495\":3,\"497\":3,\"510\":7,\"511\":4,\"512\":11}}],[\"sentence\",{\"0\":{\"506\":1},\"1\":{\"28\":1,\"143\":1,\"285\":1,\"410\":2,\"412\":2,\"454\":3,\"495\":1,\"497\":1,\"504\":1,\"520\":1,\"538\":5}}],[\"seem\",{\"1\":{\"531\":1}}],[\"seed\",{\"1\":{\"289\":1}}],[\"seen\",{\"1\":{\"20\":1,\"23\":2,\"29\":2,\"58\":2,\"65\":1,\"477\":3,\"656\":3,\"658\":3,\"666\":3}}],[\"separated\",{\"1\":{\"412\":1}}],[\"separation\",{\"1\":{\"395\":1}}],[\"sep\",{\"1\":{\"143\":3,\"286\":1,\"493\":1,\"506\":7,\"508\":1,\"511\":2,\"512\":9,\"513\":1,\"520\":11,\"540\":2,\"542\":2,\"544\":2}}],[\"search输出\",{\"1\":{\"461\":1}}],[\"search扩展\",{\"1\":{\"286\":1}}],[\"search的beam数量\",{\"1\":{\"286\":1}}],[\"searchpicbytext\",{\"1\":{\"276\":1,\"277\":1}}],[\"search\",{\"1\":{\"133\":3,\"143\":2,\"286\":1}}],[\"set避免重复处理节点\",{\"1\":{\"666\":1}}],[\"setup\",{\"1\":{\"661\":4}}],[\"setitem\",{\"1\":{\"659\":1}}],[\"setattr\",{\"1\":{\"658\":2}}],[\"sets\",{\"1\":{\"115\":1}}],[\"set\",{\"1\":{\"68\":6,\"88\":1,\"93\":1,\"97\":1,\"98\":4,\"99\":1,\"101\":2,\"105\":1,\"112\":2,\"115\":1,\"511\":1,\"591\":1,\"634\":2,\"651\":1,\"656\":7,\"658\":5,\"666\":4}}],[\"setting\",{\"0\":{\"22\":1},\"1\":{\"23\":2,\"29\":2,\"58\":2}}],[\"seq\",{\"1\":{\"45\":1,\"268\":1,\"282\":6,\"283\":5,\"284\":13,\"285\":1,\"474\":1,\"477\":5,\"512\":3,\"513\":6,\"517\":10,\"519\":1,\"523\":2,\"531\":2,\"537\":4,\"538\":4,\"540\":4,\"541\":4,\"543\":2,\"544\":4,\"558\":1}}],[\"sequences\",{\"1\":{\"315\":1,\"520\":2,\"549\":1}}],[\"sequences=1\",{\"1\":{\"143\":1}}],[\"sequence\",{\"1\":{\"43\":2,\"163\":2,\"262\":2,\"268\":2,\"285\":2,\"520\":4,\"528\":3,\"537\":2,\"538\":2,\"540\":3,\"541\":2,\"543\":4}}],[\"sequential\",{\"1\":{\"34\":2,\"35\":3,\"36\":2,\"41\":1,\"42\":1,\"44\":1,\"45\":1,\"46\":1,\"59\":4,\"73\":2,\"98\":1,\"296\":2}}],[\"semantic\",{\"1\":{\"40\":4,\"45\":3,\"98\":1,\"193\":1,\"448\":1}}],[\"select\",{\"1\":{\"29\":1,\"145\":2,\"147\":2}}],[\"sele\",{\"1\":{\"29\":2}}],[\"self指代右操作数b\",{\"1\":{\"660\":1}}],[\"self指代左操作数a\",{\"1\":{\"660\":1}}],[\"self和other是两个关键入参\",{\"1\":{\"660\":2}}],[\"self\",{\"0\":{\"435\":1,\"475\":1,\"476\":1},\"1\":{\"29\":49,\"30\":8,\"32\":11,\"33\":5,\"34\":13,\"35\":29,\"36\":30,\"40\":18,\"41\":32,\"42\":7,\"43\":1,\"44\":6,\"45\":40,\"46\":20,\"58\":45,\"59\":90,\"68\":21,\"70\":10,\"72\":8,\"73\":10,\"74\":10,\"75\":5,\"76\":17,\"78\":9,\"92\":17,\"93\":25,\"96\":41,\"100\":9,\"101\":27,\"107\":26,\"109\":24,\"110\":19,\"111\":24,\"126\":2,\"142\":23,\"143\":9,\"145\":59,\"146\":15,\"147\":57,\"160\":35,\"161\":17,\"162\":14,\"163\":14,\"244\":5,\"246\":13,\"247\":9,\"248\":5,\"249\":6,\"262\":11,\"263\":5,\"264\":4,\"265\":14,\"266\":14,\"267\":1,\"268\":22,\"282\":9,\"283\":4,\"284\":12,\"285\":49,\"286\":7,\"289\":12,\"291\":12,\"292\":16,\"293\":22,\"294\":25,\"295\":16,\"296\":33,\"373\":1,\"377\":4,\"401\":5,\"402\":3,\"403\":3,\"404\":3,\"405\":5,\"407\":5,\"410\":4,\"411\":13,\"412\":45,\"435\":3,\"477\":31,\"508\":2,\"511\":34,\"513\":24,\"517\":1,\"520\":9,\"523\":13,\"525\":26,\"526\":5,\"527\":7,\"528\":11,\"529\":14,\"531\":28,\"532\":9,\"533\":11,\"535\":10,\"536\":9,\"537\":7,\"538\":8,\"540\":3,\"541\":8,\"543\":12,\"544\":9,\"548\":2,\"549\":16,\"550\":5,\"552\":8,\"553\":15,\"554\":7,\"556\":19,\"557\":7,\"558\":15,\"607\":2,\"613\":4,\"617\":1,\"629\":3,\"630\":5,\"631\":2,\"634\":4,\"635\":3,\"638\":2,\"642\":4,\"643\":2,\"645\":4,\"646\":2,\"651\":7,\"652\":5,\"653\":2,\"654\":5,\"656\":19,\"657\":4,\"658\":11,\"659\":19,\"660\":25}}],[\"second\",{\"1\":{\"520\":1}}],[\"seconds\",{\"1\":{\"275\":2,\"277\":1}}],[\"section\",{\"1\":{\"78\":2,\"494\":3}}],[\"sections\",{\"1\":{\"59\":1}}],[\"sections=self\",{\"1\":{\"46\":2,\"59\":1}}],[\"sections=spatial\",{\"1\":{\"40\":1}}],[\"sec\",{\"1\":{\"8\":3,\"481\":1}}],[\"student\",{\"0\":{\"586\":1},\"1\":{\"586\":1,\"587\":1}}],[\"study\",{\"0\":{\"24\":1,\"135\":1,\"229\":1},\"1\":{\"134\":1}}],[\"stem\",{\"1\":{\"483\":1}}],[\"steps=100\",{\"1\":{\"519\":2}}],[\"steps\",{\"1\":{\"159\":1}}],[\"steplr\",{\"1\":{\"80\":1}}],[\"step\",{\"0\":{\"41\":1,\"42\":1,\"43\":1,\"44\":1,\"45\":1,\"46\":1},\"1\":{\"40\":11,\"78\":5,\"80\":1,\"81\":1,\"142\":1,\"145\":1,\"147\":1,\"159\":1,\"163\":12,\"210\":1,\"244\":2,\"285\":5,\"296\":1,\"325\":1,\"404\":5,\"433\":2,\"474\":19,\"477\":3,\"512\":2,\"514\":1,\"519\":4}}],[\"stiennon\",{\"1\":{\"469\":1,\"470\":1}}],[\"sts\",{\"1\":{\"448\":1,\"499\":1}}],[\"storage\",{\"1\":{\"519\":1}}],[\"storagefurniture\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"stories\",{\"1\":{\"439\":1,\"440\":1,\"494\":1}}],[\"stop\",{\"1\":{\"325\":1}}],[\"style=filled\",{\"1\":{\"666\":12}}],[\"style\",{\"1\":{\"163\":1,\"440\":1}}],[\"stn\",{\"1\":{\"109\":3}}],[\"stnkd\",{\"1\":{\"105\":1,\"108\":1,\"109\":1}}],[\"stn3d\",{\"1\":{\"105\":1,\"107\":5,\"108\":1,\"109\":2}}],[\"std=0\",{\"1\":{\"292\":1,\"293\":2,\"296\":2}}],[\"std=\",{\"1\":{\"244\":1}}],[\"stderr\",{\"1\":{\"83\":2}}],[\"stdout\",{\"1\":{\"83\":1}}],[\"string\",{\"1\":{\"510\":1}}],[\"strip\",{\"1\":{\"510\":4}}],[\"strict=false\",{\"1\":{\"300\":1}}],[\"strides=\",{\"1\":{\"327\":3}}],[\"strides\",{\"1\":{\"321\":1,\"323\":4,\"325\":2,\"326\":4,\"327\":2,\"383\":1}}],[\"stride=patch\",{\"1\":{\"291\":1}}],[\"stride\",{\"1\":{\"291\":1,\"323\":4,\"326\":3,\"384\":1,\"396\":1}}],[\"stream会引入额外的计算量\",{\"1\":{\"256\":1}}],[\"stream的交互方式\",{\"1\":{\"256\":1}}],[\"stream是不对图像和文本concate然后进行交互操作\",{\"1\":{\"256\":1}}],[\"stream是对图像和文本concate然后进行交互操作\",{\"1\":{\"256\":1}}],[\"stream\",{\"1\":{\"256\":2}}],[\"str\",{\"1\":{\"68\":2,\"142\":3,\"163\":1,\"268\":1,\"289\":3,\"389\":2,\"410\":19,\"411\":6,\"412\":44,\"525\":1,\"535\":1,\"659\":3,\"666\":2}}],[\"structural\",{\"1\":{\"63\":1}}],[\"structure\",{\"0\":{\"152\":1,\"257\":1},\"1\":{\"11\":1,\"28\":4,\"96\":1,\"287\":1}}],[\"staats\",{\"1\":{\"481\":1}}],[\"stanford\",{\"1\":{\"448\":1}}],[\"standing\",{\"1\":{\"226\":1,\"273\":2}}],[\"standard\",{\"1\":{\"163\":1}}],[\"stage流程\",{\"1\":{\"281\":1}}],[\"stage\",{\"0\":{\"282\":1,\"286\":1},\"1\":{\"191\":3,\"281\":3,\"282\":1,\"286\":1}}],[\"stack\",{\"0\":{\"381\":1},\"1\":{\"43\":2,\"145\":3,\"147\":3,\"162\":3,\"273\":1,\"284\":3,\"289\":1,\"381\":3,\"481\":1,\"522\":1,\"554\":1}}],[\"static\",{\"1\":{\"495\":2}}],[\"staticmethod\",{\"1\":{\"289\":2,\"412\":2}}],[\"statistical\",{\"0\":{\"19\":1}}],[\"stats\",{\"1\":{\"142\":1,\"410\":4,\"412\":2}}],[\"state=hidden\",{\"1\":{\"285\":1,\"477\":1}}],[\"state=sequence\",{\"1\":{\"262\":1}}],[\"state\",{\"1\":{\"82\":2,\"83\":1,\"145\":5,\"146\":2,\"147\":4,\"161\":4,\"162\":3,\"282\":2,\"284\":1,\"300\":1,\"389\":3,\"495\":1,\"514\":2,\"527\":1}}],[\"states=all\",{\"1\":{\"285\":1,\"477\":1}}],[\"states=false\",{\"1\":{\"285\":1}}],[\"states=outputs\",{\"1\":{\"163\":1,\"285\":1}}],[\"states=output\",{\"1\":{\"163\":1,\"285\":1}}],[\"states=encoder\",{\"1\":{\"163\":1,\"262\":1,\"268\":1,\"285\":1}}],[\"states=none\",{\"1\":{\"162\":1,\"163\":2,\"262\":1,\"263\":1,\"264\":1,\"265\":1,\"266\":1,\"268\":1,\"285\":5}}],[\"states=image\",{\"1\":{\"142\":1,\"145\":2,\"147\":3,\"282\":1,\"284\":1,\"285\":1}}],[\"states\",{\"1\":{\"40\":4,\"143\":1,\"146\":1,\"162\":9,\"163\":5,\"262\":3,\"263\":6,\"264\":3,\"265\":12,\"266\":7,\"267\":1,\"268\":15,\"285\":21,\"286\":1,\"477\":43,\"525\":16,\"526\":4,\"527\":2,\"529\":2,\"531\":4,\"532\":8,\"535\":8,\"536\":6,\"538\":1,\"544\":1}}],[\"startswith\",{\"1\":{\"510\":1}}],[\"start\",{\"1\":{\"29\":4,\"58\":4,\"81\":1,\"147\":1,\"244\":1,\"275\":5,\"277\":5,\"325\":1,\"379\":2,\"540\":12,\"541\":15,\"542\":5}}],[\"stab\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"a3b\",{\"1\":{\"674\":1}}],[\"a为m\",{\"1\":{\"420\":1}}],[\"a23\",{\"1\":{\"325\":2}}],[\"a22b\",{\"1\":{\"674\":1}}],[\"a22\",{\"1\":{\"325\":3}}],[\"a21\",{\"1\":{\"325\":3}}],[\"a20\",{\"1\":{\"325\":2}}],[\"a13\",{\"1\":{\"325\":2}}],[\"a12\",{\"1\":{\"325\":3}}],[\"a11\",{\"1\":{\"325\":4}}],[\"a10\",{\"1\":{\"325\":2}}],[\"a100\",{\"1\":{\"200\":1,\"201\":1}}],[\"a03\",{\"1\":{\"325\":2}}],[\"a02\",{\"1\":{\"325\":2}}],[\"a01\",{\"1\":{\"325\":2}}],[\"a00\",{\"1\":{\"325\":3}}],[\"axis\",{\"1\":{\"276\":1,\"277\":1}}],[\"axis=1\",{\"1\":{\"83\":1,\"272\":3,\"275\":3,\"277\":3}}],[\"axis=0\",{\"1\":{\"83\":1,\"272\":1}}],[\"across\",{\"1\":{\"283\":2}}],[\"acceptability语言可接受性语料库\",{\"1\":{\"448\":1}}],[\"accu\",{\"1\":{\"296\":1}}],[\"accuracy\",{\"0\":{\"343\":1},\"1\":{\"275\":8,\"277\":6,\"515\":2}}],[\"acc\",{\"1\":{\"275\":2,\"277\":2}}],[\"active\",{\"1\":{\"543\":8}}],[\"activate\",{\"1\":{\"332\":2,\"335\":1,\"338\":1,\"339\":2,\"519\":1,\"546\":1}}],[\"activation\",{\"1\":{\"76\":1,\"262\":2,\"527\":2}}],[\"actually\",{\"1\":{\"531\":1}}],[\"actual\",{\"1\":{\"275\":2,\"277\":2}}],[\"act2fn\",{\"1\":{\"268\":1,\"525\":1,\"535\":1}}],[\"act\",{\"1\":{\"268\":6,\"293\":1,\"294\":6,\"296\":5,\"525\":7,\"535\":7}}],[\"avg\",{\"1\":{\"514\":5}}],[\"available\",{\"1\":{\"275\":1,\"277\":1}}],[\"average=true\",{\"1\":{\"401\":1,\"402\":1,\"403\":1,\"404\":1,\"405\":1,\"407\":1}}],[\"average=none\",{\"1\":{\"163\":1}}],[\"average\",{\"1\":{\"22\":1,\"82\":1,\"105\":1,\"115\":1,\"157\":1,\"397\":2,\"401\":3,\"402\":2,\"403\":2,\"404\":2,\"405\":4,\"407\":4}}],[\"amazonaws\",{\"1\":{\"510\":2}}],[\"american\",{\"1\":{\"273\":1}}],[\"among\",{\"1\":{\"104\":1}}],[\"apt\",{\"1\":{\"666\":1}}],[\"apache许可项目\",{\"1\":{\"481\":1}}],[\"applied\",{\"1\":{\"558\":1}}],[\"apple\",{\"1\":{\"505\":5}}],[\"apply\",{\"1\":{\"247\":1,\"296\":1,\"531\":1,\"558\":2}}],[\"approach\",{\"1\":{\"490\":1,\"496\":1}}],[\"append\",{\"1\":{\"29\":3,\"43\":3,\"58\":3,\"59\":2,\"68\":1,\"82\":1,\"92\":2,\"96\":5,\"100\":2,\"143\":1,\"145\":3,\"147\":3,\"162\":3,\"275\":3,\"277\":3,\"284\":3,\"289\":5,\"411\":2,\"412\":3,\"511\":6,\"512\":4,\"520\":1,\"638\":1,\"652\":1,\"654\":1,\"656\":1,\"658\":1,\"666\":2,\"667\":2}}],[\"api的调用\",{\"1\":{\"689\":1}}],[\"api\",{\"1\":{\"224\":1,\"390\":1,\"469\":1,\"470\":3,\"471\":2,\"472\":4,\"674\":1,\"682\":2,\"686\":1}}],[\"ai\",{\"1\":{\"227\":1,\"416\":1,\"472\":3,\"474\":38,\"510\":2,\"674\":4,\"677\":2,\"686\":4}}],[\"ai2d科学图表任务表现接近商业模型\",{\"1\":{\"220\":1}}],[\"aiou\",{\"1\":{\"22\":1,\"23\":1,\"24\":2,\"82\":3}}],[\"agents\",{\"1\":{\"683\":1}}],[\"agent\",{\"1\":{\"674\":1,\"678\":1,\"684\":1}}],[\"agreement\",{\"1\":{\"470\":1}}],[\"agnostic\",{\"1\":{\"460\":1}}],[\"agnostic模型优于那些为每个任务精心设计的模型\",{\"1\":{\"439\":1,\"440\":1}}],[\"aggregate\",{\"1\":{\"283\":2}}],[\"agi\",{\"1\":{\"181\":1,\"208\":1,\"678\":3}}],[\"agd20k\",{\"1\":{\"17\":1}}],[\"a^t||\",{\"1\":{\"105\":1}}],[\"always\",{\"1\":{\"558\":1}}],[\"already\",{\"1\":{\"520\":1}}],[\"alice\",{\"1\":{\"377\":1}}],[\"align=\",{\"1\":{\"289\":1}}],[\"aligned\",{\"1\":{\"272\":2}}],[\"aligning\",{\"1\":{\"179\":1,\"472\":1}}],[\"alignment\",{\"1\":{\"59\":1,\"225\":1,\"258\":1,\"468\":1,\"469\":2,\"470\":1,\"471\":2,\"472\":1}}],[\"align进行映射\",{\"1\":{\"59\":1}}],[\"align对齐方法需要\",{\"1\":{\"59\":1}}],[\"align映射为4\",{\"1\":{\"59\":1}}],[\"align技术\",{\"1\":{\"59\":4}}],[\"align获取物体\",{\"1\":{\"54\":1}}],[\"align\",{\"0\":{\"397\":1},\"1\":{\"46\":2,\"59\":7,\"148\":2,\"396\":1}}],[\"alexnet\",{\"1\":{\"275\":1}}],[\"albef\",{\"0\":{\"148\":1,\"151\":1},\"1\":{\"148\":1,\"149\":2,\"150\":2,\"152\":1,\"153\":1,\"156\":1,\"157\":2,\"159\":1,\"160\":1,\"161\":1,\"162\":3,\"163\":1}}],[\"al\",{\"1\":{\"126\":5,\"127\":2,\"131\":4,\"454\":3,\"469\":17,\"470\":2,\"472\":2,\"481\":1,\"483\":1,\"485\":10}}],[\"alpha+beta=1\",{\"1\":{\"405\":1}}],[\"alpha=beta=1\",{\"1\":{\"405\":1}}],[\"alpha=beta=0\",{\"1\":{\"405\":1}}],[\"alpha=0\",{\"1\":{\"161\":1,\"163\":1}}],[\"alpha=alpha\",{\"1\":{\"145\":1,\"404\":1,\"405\":1,\"407\":1}}],[\"alpha\",{\"1\":{\"78\":3,\"145\":9,\"147\":10,\"159\":13,\"161\":6,\"163\":4,\"404\":8,\"405\":4,\"407\":7}}],[\"allclose\",{\"1\":{\"646\":1}}],[\"all=false\",{\"1\":{\"93\":2,\"101\":4}}],[\"all=true\",{\"1\":{\"92\":1,\"93\":1}}],[\"all流程图\",{\"1\":{\"92\":1}}],[\"all\",{\"1\":{\"73\":1,\"92\":8,\"145\":26,\"147\":14,\"161\":8,\"162\":8,\"249\":1,\"266\":1,\"275\":3,\"276\":1,\"277\":3,\"283\":2,\"284\":12,\"285\":1,\"412\":4,\"510\":4,\"511\":2,\"520\":5,\"522\":16,\"531\":6,\"558\":3}}],[\"after\",{\"1\":{\"326\":2,\"367\":1}}],[\"afm\",{\"0\":{\"71\":1,\"75\":1},\"1\":{\"70\":1,\"71\":3,\"74\":1,\"75\":5,\"76\":1}}],[\"aff\",{\"1\":{\"81\":1,\"82\":1,\"83\":2}}],[\"aff2idx\",{\"1\":{\"68\":2}}],[\"afford\",{\"1\":{\"68\":2}}],[\"affordq\",{\"1\":{\"68\":1,\"80\":3}}],[\"affordance=dict\",{\"1\":{\"83\":1}}],[\"affordances\",{\"1\":{\"25\":1}}],[\"affordancenet\",{\"1\":{\"17\":1,\"62\":2,\"63\":1,\"67\":1,\"69\":1}}],[\"affordance\",{\"0\":{\"9\":1,\"12\":1,\"28\":1},\"1\":{\"4\":1,\"5\":1,\"6\":1,\"7\":2,\"13\":1,\"16\":1,\"19\":1,\"20\":1,\"23\":4,\"24\":3,\"25\":2,\"26\":1,\"28\":1,\"29\":18,\"30\":1,\"33\":1,\"37\":1,\"40\":4,\"45\":5,\"46\":12,\"47\":1,\"48\":2,\"49\":1,\"58\":15,\"59\":15,\"60\":3,\"62\":1,\"63\":1,\"67\":1,\"68\":15,\"70\":1,\"76\":6,\"80\":2,\"82\":3,\"83\":14}}],[\"affcot\",{\"1\":{\"24\":2}}],[\"ascii=false\",{\"1\":{\"410\":3,\"412\":3,\"510\":2}}],[\"astronaut\",{\"1\":{\"273\":1}}],[\"astype\",{\"1\":{\"68\":2,\"82\":2,\"83\":4,\"107\":1}}],[\"assume\",{\"1\":{\"558\":1}}],[\"asserttrue\",{\"1\":{\"646\":1}}],[\"assertequal\",{\"1\":{\"645\":2}}],[\"assert\",{\"1\":{\"249\":1,\"289\":1,\"511\":1,\"558\":1}}],[\"assistant\",{\"0\":{\"223\":1},\"1\":{\"223\":1,\"226\":1,\"227\":2}}],[\"as\",{\"1\":{\"68\":2,\"78\":2,\"83\":4,\"244\":1,\"275\":2,\"276\":1,\"277\":5,\"289\":2,\"294\":1,\"295\":1,\"323\":1,\"410\":3,\"412\":6,\"510\":3,\"511\":3,\"523\":1,\"536\":1,\"608\":1,\"651\":1,\"656\":1,\"658\":1,\"660\":6,\"661\":4,\"666\":5,\"667\":3,\"678\":1}}],[\"attn有4\",{\"1\":{\"448\":1}}],[\"attn\",{\"1\":{\"72\":1,\"74\":1,\"76\":4,\"163\":1,\"285\":3,\"293\":1,\"294\":4,\"295\":10,\"296\":2,\"477\":18,\"517\":4,\"553\":4,\"556\":10,\"558\":7}}],[\"atts=none\",{\"1\":{\"43\":1}}],[\"atts\",{\"1\":{\"40\":2,\"43\":12,\"142\":2,\"143\":2,\"145\":13,\"146\":2,\"147\":14,\"161\":2,\"162\":12,\"163\":2,\"282\":2,\"284\":11,\"285\":4,\"286\":2}}],[\"attenion\",{\"1\":{\"552\":1}}],[\"attend\",{\"1\":{\"531\":1,\"547\":1}}],[\"atten1\",{\"1\":{\"36\":2}}],[\"atten\",{\"1\":{\"32\":8,\"33\":3,\"45\":10,\"59\":8}}],[\"attention可以用矩阵乘法一次计算所有的时刻\",{\"1\":{\"547\":1}}],[\"attention机制\",{\"1\":{\"547\":1}}],[\"attention=false\",{\"1\":{\"477\":1}}],[\"attentional\",{\"1\":{\"449\":1}}],[\"attention运算过程中维度变换的理解\",{\"0\":{\"304\":1},\"1\":{\"304\":1}}],[\"attention的heads数\",{\"1\":{\"297\":1}}],[\"attention的输入\",{\"1\":{\"286\":1}}],[\"attention计算\",{\"1\":{\"284\":1}}],[\"attention模块\",{\"1\":{\"282\":1}}],[\"attentions=all\",{\"1\":{\"285\":2,\"477\":2}}],[\"attentions=outputs\",{\"1\":{\"163\":1,\"285\":2}}],[\"attentions=output\",{\"1\":{\"163\":1,\"285\":3}}],[\"attentions=none\",{\"1\":{\"163\":1,\"285\":1}}],[\"attentions=false\",{\"1\":{\"162\":1,\"285\":3}}],[\"attentions\",{\"1\":{\"76\":1,\"163\":2,\"285\":9,\"477\":2,\"529\":1,\"531\":2,\"538\":1,\"544\":1}}],[\"attention\",{\"0\":{\"265\":1,\"475\":1,\"476\":1},\"1\":{\"32\":1,\"33\":2,\"36\":1,\"40\":7,\"41\":9,\"43\":12,\"45\":14,\"46\":1,\"59\":1,\"75\":2,\"112\":3,\"115\":1,\"126\":3,\"142\":3,\"143\":3,\"145\":11,\"146\":5,\"147\":15,\"152\":1,\"161\":5,\"162\":17,\"163\":14,\"191\":2,\"262\":7,\"263\":5,\"264\":12,\"265\":5,\"266\":21,\"267\":4,\"268\":5,\"282\":3,\"283\":3,\"284\":10,\"285\":68,\"286\":1,\"294\":2,\"295\":2,\"305\":1,\"310\":1,\"314\":1,\"315\":1,\"387\":1,\"425\":1,\"475\":1,\"477\":7,\"508\":2,\"520\":6,\"522\":4,\"525\":7,\"526\":2,\"528\":8,\"529\":3,\"531\":26,\"533\":4,\"538\":2,\"540\":1,\"541\":2,\"543\":4,\"544\":6,\"548\":2,\"558\":4,\"674\":3}}],[\"at\",{\"1\":{\"28\":2,\"83\":1,\"98\":2,\"249\":1,\"273\":1}}],[\"augmented\",{\"0\":{\"679\":1},\"1\":{\"679\":1,\"688\":1}}],[\"augmentation\",{\"0\":{\"124\":1},\"1\":{\"244\":2}}],[\"auto\",{\"1\":{\"678\":1}}],[\"autocast\",{\"1\":{\"40\":1}}],[\"autotokenizer\",{\"1\":{\"28\":1}}],[\"automodel\",{\"1\":{\"28\":1}}],[\"auc\",{\"0\":{\"349\":1,\"351\":1,\"353\":1},\"1\":{\"22\":1,\"23\":1,\"75\":1,\"82\":23,\"350\":1,\"351\":6,\"352\":1,\"353\":3}}],[\"adjacent\",{\"1\":{\"512\":14}}],[\"adversarial\",{\"1\":{\"472\":2}}],[\"admin\",{\"1\":{\"379\":2}}],[\"ade20k\",{\"1\":{\"193\":1}}],[\"ade20k分割\",{\"1\":{\"181\":1}}],[\"add等操作节点\",{\"1\":{\"666\":1}}],[\"addition\",{\"1\":{\"567\":1}}],[\"additional\",{\"1\":{\"28\":2,\"35\":1,\"46\":2,\"59\":1,\"134\":1}}],[\"additivity\",{\"1\":{\"567\":1}}],[\"added\",{\"1\":{\"412\":1}}],[\"add\",{\"1\":{\"83\":1,\"145\":2,\"146\":1,\"147\":2,\"268\":1,\"370\":2,\"529\":1,\"654\":1,\"655\":1,\"656\":4,\"658\":6,\"660\":16,\"661\":4,\"662\":1,\"666\":6}}],[\"adamw\",{\"1\":{\"131\":1,\"142\":2,\"145\":1,\"147\":1,\"200\":1,\"201\":1,\"203\":1,\"481\":1,\"514\":1}}],[\"adam\",{\"1\":{\"22\":1,\"80\":1,\"447\":1,\"493\":1,\"494\":2}}],[\"adaptation\",{\"1\":{\"420\":1,\"422\":1}}],[\"adaptiveavgpool1d\",{\"1\":{\"36\":1,\"46\":1,\"59\":1}}],[\"adaptive\",{\"0\":{\"14\":1},\"1\":{\"70\":1}}],[\"adapter\",{\"1\":{\"10\":2,\"40\":2,\"42\":1,\"44\":1,\"424\":2}}],[\"about\",{\"1\":{\"273\":1}}],[\"ablation\",{\"0\":{\"24\":1,\"135\":1,\"229\":1},\"1\":{\"134\":1,\"229\":1}}],[\"abstract\",{\"0\":{\"237\":1}}],[\"abstraction\",{\"1\":{\"87\":1,\"88\":2,\"93\":1,\"97\":1,\"98\":4,\"99\":1,\"101\":2}}],[\"absolute\",{\"1\":{\"22\":1,\"82\":4,\"284\":1}}],[\"abs\",{\"1\":{\"4\":1,\"37\":1,\"47\":1,\"59\":1,\"78\":4,\"85\":1,\"102\":1,\"179\":1,\"223\":1,\"279\":1,\"300\":1,\"512\":1}}],[\"art\",{\"1\":{\"495\":1}}],[\"arc挑战集\",{\"1\":{\"482\":1}}],[\"arc等\",{\"1\":{\"482\":1}}],[\"arch\",{\"1\":{\"244\":1}}],[\"architecture\",{\"1\":{\"71\":1,\"73\":1}}],[\"are\",{\"1\":{\"206\":2,\"412\":1,\"451\":1,\"452\":1,\"458\":1,\"472\":1,\"529\":2,\"536\":1,\"541\":1}}],[\"area\",{\"1\":{\"22\":1,\"78\":2,\"82\":3,\"404\":1}}],[\"argument\",{\"1\":{\"363\":2}}],[\"argmax\",{\"1\":{\"162\":1,\"275\":1,\"276\":1,\"277\":2,\"474\":1,\"477\":1,\"540\":2}}],[\"args\",{\"1\":{\"43\":1,\"45\":2,\"46\":2,\"83\":4,\"142\":2,\"145\":1,\"147\":1,\"244\":13,\"292\":1,\"300\":1,\"367\":2,\"370\":3,\"371\":2,\"372\":4,\"373\":2,\"377\":2,\"378\":2,\"379\":7,\"412\":11,\"522\":1,\"651\":1}}],[\"arange\",{\"1\":{\"92\":3,\"272\":1,\"283\":1,\"284\":1,\"323\":2,\"477\":1,\"523\":1}}],[\"array\",{\"1\":{\"68\":2,\"82\":1,\"83\":3,\"107\":1,\"275\":2,\"277\":2,\"323\":1,\"608\":2,\"617\":1,\"632\":1,\"645\":4,\"651\":1,\"654\":1,\"655\":1,\"656\":1,\"658\":4,\"659\":7,\"660\":12,\"661\":2,\"662\":6,\"666\":4,\"667\":6}}],[\"ar\",{\"1\":{\"49\":1,\"194\":1}}],[\"arm模块\",{\"1\":{\"54\":1,\"56\":1}}],[\"arm\",{\"1\":{\"48\":1,\"59\":1}}],[\"around\",{\"1\":{\"28\":2,\"31\":2}}],[\"arxiv\",{\"1\":{\"4\":1,\"37\":1,\"47\":1,\"85\":1,\"102\":1,\"179\":1,\"223\":1,\"279\":1,\"300\":1,\"481\":1}}],[\"a\",{\"1\":{\"17\":1,\"24\":1,\"28\":4,\"49\":1,\"52\":1,\"59\":1,\"105\":1,\"142\":2,\"143\":2,\"145\":3,\"147\":2,\"188\":1,\"193\":1,\"226\":2,\"247\":2,\"255\":1,\"273\":16,\"274\":3,\"275\":1,\"276\":2,\"277\":3,\"280\":2,\"323\":2,\"327\":4,\"353\":2,\"355\":1,\"370\":2,\"371\":2,\"407\":1,\"412\":4,\"421\":2,\"425\":3,\"444\":1,\"454\":1,\"455\":1,\"470\":1,\"474\":20,\"477\":1,\"490\":1,\"511\":6,\"512\":18,\"513\":1,\"517\":1,\"519\":3,\"520\":4,\"531\":1,\"544\":1,\"554\":1,\"558\":2,\"565\":2,\"617\":4,\"632\":3,\"655\":3,\"657\":6,\"660\":10,\"666\":1,\"673\":1,\"678\":1,\"688\":2}}],[\"anthropic\",{\"1\":{\"674\":3}}],[\"answer\",{\"1\":{\"542\":3}}],[\"answering\",{\"1\":{\"454\":1}}],[\"anaconda\",{\"1\":{\"338\":1}}],[\"anaconda3\",{\"1\":{\"334\":3}}],[\"analysis\",{\"0\":{\"19\":1,\"25\":1}}],[\"analogical\",{\"1\":{\"12\":1,\"28\":2}}],[\"anchor\",{\"1\":{\"238\":1}}],[\"ann\",{\"1\":{\"142\":5}}],[\"annotated\",{\"1\":{\"546\":5}}],[\"annotator\",{\"1\":{\"470\":1}}],[\"annotation\",{\"0\":{\"18\":1},\"1\":{\"142\":1}}],[\"anno\",{\"1\":{\"68\":9,\"83\":5}}],[\"an\",{\"1\":{\"73\":1,\"273\":1,\"536\":1}}],[\"and\",{\"0\":{\"13\":1,\"15\":1,\"130\":1,\"223\":1,\"249\":1,\"484\":1,\"575\":1,\"576\":1},\"1\":{\"28\":6,\"37\":1,\"68\":1,\"78\":1,\"80\":2,\"92\":8,\"119\":2,\"120\":1,\"128\":1,\"129\":1,\"134\":1,\"145\":1,\"147\":1,\"148\":2,\"161\":1,\"179\":1,\"223\":1,\"244\":3,\"246\":1,\"247\":2,\"249\":2,\"252\":2,\"273\":1,\"274\":1,\"285\":1,\"294\":1,\"300\":1,\"326\":1,\"395\":1,\"412\":2,\"421\":1,\"469\":5,\"474\":9,\"477\":1,\"478\":1,\"487\":1,\"510\":2,\"512\":1,\"520\":1,\"525\":1,\"529\":1,\"531\":1,\"535\":1,\"538\":1,\"540\":1,\"541\":1,\"549\":2,\"554\":1,\"556\":1,\"558\":2,\"662\":2,\"666\":1}}],[\"论文的主要改进包括\",{\"1\":{\"492\":1}}],[\"论文的核心方法是基于语言建模\",{\"1\":{\"454\":1}}],[\"论文的核心假设与目标\",{\"1\":{\"453\":1}}],[\"论文第5章重点讨论\",{\"1\":{\"482\":1}}],[\"论文最终强调\",{\"1\":{\"460\":1}}],[\"论文也分析了模型在自然语言推理\",{\"1\":{\"460\":1}}],[\"论文假设\",{\"1\":{\"460\":1}}],[\"论文系统评估了不同规模的gpt\",{\"1\":{\"455\":1}}],[\"论文训练了gpt\",{\"1\":{\"460\":1}}],[\"论文训练了\",{\"1\":{\"454\":1}}],[\"论文构建了一个新的数据集\",{\"1\":{\"454\":1}}],[\"论文强调了gpt\",{\"1\":{\"459\":1}}],[\"论文强调\",{\"1\":{\"453\":1}}],[\"论文还进行了一组\",{\"1\":{\"470\":1}}],[\"论文还探讨了模型泛化与记忆的关系\",{\"1\":{\"452\":1}}],[\"论文还实验了使用80个不同的prompt进行集成\",{\"1\":{\"274\":1}}],[\"论文实验结果显示\",{\"1\":{\"427\":1}}],[\"论文实现进行区分\",{\"1\":{\"145\":1}}],[\"论文推荐\",{\"1\":{\"397\":1}}],[\"论文里也做了说明\",{\"1\":{\"297\":1}}],[\"论文里没有做解释\",{\"1\":{\"294\":1}}],[\"论文作者也对其做了实验\",{\"1\":{\"293\":1}}],[\"论文指出传统观点认为模型参数越多性能越优\",{\"1\":{\"480\":1}}],[\"论文指出这仅是通向更通用ai系统的初步探索\",{\"1\":{\"457\":1}}],[\"论文指出\",{\"1\":{\"274\":1,\"480\":1}}],[\"论文发现这个模型的效果最佳\",{\"1\":{\"272\":1}}],[\"论文采用\",{\"1\":{\"247\":1}}],[\"论文在\",{\"1\":{\"239\":1}}],[\"论文核心创新点\",{\"1\":{\"224\":1}}],[\"论文提出通过元学习\",{\"1\":{\"460\":1}}],[\"论文提出internvl\",{\"1\":{\"208\":1}}],[\"论文提出\",{\"1\":{\"181\":1}}],[\"论文提出了一个全新的模型\",{\"1\":{\"70\":1}}],[\"论文简析\",{\"1\":{\"148\":1,\"179\":1,\"206\":1,\"223\":1,\"232\":1,\"251\":1,\"252\":1}}],[\"论文附录a中提到的训练优化策略\",{\"1\":{\"142\":1}}],[\"论文4\",{\"1\":{\"142\":1}}],[\"论文链接\",{\"1\":{\"119\":1,\"148\":1,\"164\":1,\"176\":1,\"179\":1,\"206\":1,\"223\":1,\"232\":1,\"251\":1,\"252\":1,\"422\":1,\"451\":1,\"458\":1,\"466\":1,\"478\":1,\"487\":1,\"490\":1}}],[\"论文中第\",{\"1\":{\"469\":1}}],[\"论文中\",{\"1\":{\"425\":1}}],[\"论文中举的例子\",{\"1\":{\"421\":1}}],[\"论文中进行对比实验的clip模型也采用了这一配置\",{\"1\":{\"272\":1}}],[\"论文中还进行了多项\",{\"1\":{\"229\":1}}],[\"论文中重点测试了以下两个应用场景\",{\"1\":{\"227\":1}}],[\"论文中采用vit\",{\"1\":{\"142\":1}}],[\"论文中做了\",{\"1\":{\"112\":1}}],[\"论文中的验证\",{\"1\":{\"112\":1}}],[\"论文中也进行了大量消融实验来验证\",{\"1\":{\"75\":1}}],[\"论文中所给的模型架构图中的decoder\",{\"1\":{\"70\":1}}],[\"论文中所给的模型架构图中的encoder\",{\"1\":{\"70\":1}}],[\"论文代码解读与复现\",{\"1\":{\"37\":1,\"60\":1}}],[\"论文\",{\"0\":{\"119\":1,\"148\":1,\"164\":1,\"176\":1,\"232\":1,\"251\":1,\"252\":1,\"438\":1,\"451\":1,\"458\":1,\"466\":1,\"490\":1},\"1\":{\"4\":1,\"37\":1,\"47\":1,\"60\":1,\"85\":1,\"102\":1,\"176\":1,\"279\":1,\"438\":2,\"451\":1,\"458\":1,\"466\":1,\"478\":1,\"487\":1,\"490\":1}}],[\"论文解读\",{\"0\":{\"4\":1,\"47\":1},\"1\":{\"4\":1,\"47\":1,\"119\":1}}],[\"v3\",{\"1\":{\"674\":1}}],[\"v0\",{\"1\":{\"300\":1,\"684\":1}}],[\"v计算来源相同\",{\"1\":{\"295\":1}}],[\"v矩阵\",{\"1\":{\"295\":1}}],[\"v时使用偏置\",{\"1\":{\"295\":1}}],[\"vgg进行实现\",{\"1\":{\"275\":1}}],[\"vq\",{\"0\":{\"251\":1}}],[\"vqa\",{\"1\":{\"120\":1,\"190\":1,\"195\":1,\"197\":1,\"198\":1,\"267\":1}}],[\"verbose=true\",{\"1\":{\"666\":2}}],[\"verbose=false\",{\"1\":{\"666\":2}}],[\"verbose\",{\"1\":{\"666\":4}}],[\"verbose参数控制是否显示详细信息\",{\"1\":{\"666\":1}}],[\"very\",{\"1\":{\"449\":1}}],[\"versa\",{\"1\":{\"412\":1}}],[\"version\",{\"1\":{\"374\":2,\"525\":1,\"535\":1,\"666\":1}}],[\"vec2\",{\"1\":{\"275\":5,\"277\":5}}],[\"vec1\",{\"1\":{\"275\":5,\"277\":5}}],[\"vector\",{\"1\":{\"508\":1}}],[\"vectors\",{\"1\":{\"98\":1,\"558\":1}}],[\"vector3dvector\",{\"1\":{\"83\":4}}],[\"ve\",{\"1\":{\"255\":2}}],[\"vehicle\",{\"1\":{\"226\":1}}],[\"v100\",{\"1\":{\"494\":1}}],[\"v1\",{\"1\":{\"215\":3,\"494\":1}}],[\"vfms\",{\"1\":{\"212\":1}}],[\"vfm\",{\"1\":{\"208\":1}}],[\"vocab是词典大小\",{\"1\":{\"550\":1}}],[\"vocab\",{\"1\":{\"163\":5,\"268\":2,\"285\":1,\"410\":58,\"411\":3,\"412\":76,\"474\":1,\"511\":16,\"513\":6,\"514\":2,\"523\":2,\"536\":2,\"538\":1,\"550\":2}}],[\"vocabulary\",{\"1\":{\"4\":1,\"6\":1,\"7\":1,\"409\":1,\"410\":1,\"412\":18}}],[\"voxnet\",{\"1\":{\"112\":1}}],[\"voxel\",{\"1\":{\"103\":1,\"114\":1}}],[\"v\",{\"0\":{\"306\":1},\"1\":{\"72\":2,\"74\":2,\"147\":4,\"163\":1,\"282\":1,\"289\":4,\"295\":4,\"304\":1,\"310\":1,\"319\":2,\"410\":6,\"412\":7,\"475\":1,\"477\":2,\"511\":2,\"517\":3,\"558\":2,\"666\":10}}],[\"vse++和scan属于\",{\"1\":{\"255\":1}}],[\"vse\",{\"1\":{\"255\":1}}],[\"vs\",{\"0\":{\"681\":1},\"1\":{\"59\":1,\"111\":1,\"112\":2,\"395\":2,\"399\":1,\"423\":1,\"469\":1,\"482\":3,\"484\":1,\"494\":1,\"495\":1,\"497\":3,\"499\":1,\"542\":1}}],[\"vr等应用提供了更通用的功能理解范式\",{\"1\":{\"49\":1}}],[\"via\",{\"1\":{\"469\":1}}],[\"vice\",{\"1\":{\"412\":1}}],[\"vicuna工作\",{\"1\":{\"483\":1}}],[\"vicuna\",{\"1\":{\"190\":1,\"196\":1,\"226\":1,\"227\":2}}],[\"vicuna等\",{\"1\":{\"184\":1}}],[\"vicuna等llms无缝集成\",{\"1\":{\"181\":1}}],[\"virtex\",{\"1\":{\"278\":1}}],[\"vilbert\",{\"1\":{\"255\":1}}],[\"vilt预训练的优化目标有两个\",{\"1\":{\"258\":1}}],[\"vilt是首个使用patch\",{\"1\":{\"256\":1}}],[\"vilt是首个将ve设计的如te一样轻量的方法\",{\"1\":{\"255\":1}}],[\"vilt延用single\",{\"1\":{\"256\":1}}],[\"vilt使用预训练的vit来初始化交互的transformer\",{\"1\":{\"254\":1,\"257\":1}}],[\"vilt首次引入了全词掩码和图像增强技术于视觉\",{\"1\":{\"253\":1}}],[\"vilt不仅极大降低了模型参数和计算负担\",{\"1\":{\"253\":1}}],[\"vilt模型提出了一种极简化的视觉嵌入方案\",{\"1\":{\"253\":1}}],[\"vilt\",{\"0\":{\"252\":1},\"1\":{\"252\":3}}],[\"vit核心\",{\"1\":{\"301\":1}}],[\"vitjx\",{\"1\":{\"300\":1}}],[\"vit这篇论文长达二十多页\",{\"1\":{\"298\":1}}],[\"vit才会慢慢超越resnet\",{\"1\":{\"297\":1}}],[\"vit的效果表现不如resnet\",{\"1\":{\"297\":1}}],[\"vit的表现通常比同等大小的resnets要差一些\",{\"1\":{\"287\":1}}],[\"vit的表现就会超过cnn\",{\"1\":{\"287\":1}}],[\"vit仍是采用transformer中用到layer\",{\"1\":{\"294\":1}}],[\"vit虽然采用的是transformer\",{\"1\":{\"294\":1}}],[\"vit中的多头自注意力模块实现逻辑和transformer基本一致\",{\"1\":{\"295\":1}}],[\"vit中\",{\"1\":{\"293\":1}}],[\"vit原论文中最核心的结论是\",{\"1\":{\"287\":1}}],[\"vit及其衍生模型\",{\"1\":{\"183\":1}}],[\"vit=\",{\"1\":{\"145\":1,\"147\":1}}],[\"vit=config\",{\"1\":{\"142\":1,\"145\":1,\"147\":1}}],[\"vit梯度检查点层数\",{\"1\":{\"142\":1}}],[\"vit\",{\"1\":{\"126\":2,\"131\":8,\"132\":1,\"142\":15,\"145\":13,\"146\":7,\"147\":15,\"152\":1,\"160\":2,\"161\":1,\"183\":1,\"189\":1,\"193\":1,\"212\":1,\"214\":1,\"215\":2,\"226\":1,\"227\":1,\"263\":1,\"270\":2,\"272\":2,\"275\":4,\"277\":1,\"282\":1,\"285\":1,\"290\":3,\"296\":1,\"300\":6}}],[\"vis\",{\"1\":{\"83\":6}}],[\"visiontransformer\",{\"1\":{\"160\":2,\"292\":2,\"293\":2,\"296\":2,\"300\":1}}],[\"vision\",{\"0\":{\"84\":1,\"223\":1},\"1\":{\"40\":1,\"43\":1,\"46\":1,\"73\":1,\"119\":2,\"134\":1,\"142\":2,\"145\":10,\"146\":4,\"147\":9,\"148\":2,\"150\":1,\"160\":9,\"161\":2,\"179\":1,\"190\":2,\"223\":1,\"252\":2,\"255\":1,\"270\":1,\"272\":1,\"275\":1,\"282\":2,\"286\":1,\"287\":3,\"294\":1,\"300\":2,\"301\":1}}],[\"visualization\",{\"1\":{\"83\":3}}],[\"visualizer\",{\"1\":{\"83\":1}}],[\"visualize\",{\"1\":{\"83\":4}}],[\"visual\",{\"1\":{\"37\":1,\"131\":1,\"140\":1,\"142\":2,\"143\":1,\"145\":6,\"146\":2,\"147\":6,\"160\":4,\"161\":2,\"168\":1,\"179\":1,\"224\":1,\"232\":2,\"253\":1,\"256\":2,\"278\":2,\"282\":1,\"286\":2}}],[\"view\",{\"0\":{\"384\":1},\"1\":{\"30\":1,\"41\":1,\"45\":2,\"59\":4,\"92\":11,\"93\":1,\"96\":2,\"100\":2,\"103\":1,\"107\":3,\"109\":2,\"111\":2,\"114\":1,\"145\":1,\"163\":2,\"266\":1,\"268\":3,\"285\":4,\"325\":1,\"326\":1,\"383\":2,\"384\":7,\"385\":1,\"401\":2,\"402\":2,\"403\":2,\"404\":2,\"405\":2,\"407\":2,\"477\":4,\"514\":2,\"529\":4,\"531\":3,\"538\":4,\"543\":5,\"544\":6,\"558\":3}}],[\"var\",{\"1\":{\"666\":5}}],[\"var函数\",{\"1\":{\"666\":1}}],[\"various\",{\"1\":{\"226\":1}}],[\"variance\",{\"1\":{\"584\":1}}],[\"variant\",{\"1\":{\"196\":1}}],[\"variable等工具函数\",{\"1\":{\"661\":1}}],[\"variable通过creator引用function\",{\"1\":{\"657\":1}}],[\"variable实例通过creator属性引用创建它的function实例\",{\"1\":{\"657\":1}}],[\"variable类封装了numpy的多维数组\",{\"1\":{\"609\":1}}],[\"variable\",{\"0\":{\"659\":1},\"1\":{\"107\":1,\"565\":1,\"607\":1,\"608\":1,\"613\":1,\"617\":1,\"622\":2,\"629\":1,\"630\":1,\"634\":1,\"635\":1,\"638\":1,\"642\":1,\"643\":1,\"645\":2,\"646\":1,\"650\":1,\"651\":1,\"652\":2,\"653\":2,\"654\":2,\"655\":1,\"656\":4,\"657\":1,\"658\":7,\"659\":16,\"660\":21,\"661\":11,\"662\":6,\"665\":1,\"666\":6,\"667\":8}}],[\"vaswani\",{\"1\":{\"454\":1,\"485\":1}}],[\"vase\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"vae\",{\"0\":{\"251\":1}}],[\"valid\",{\"1\":{\"512\":3}}],[\"validation\",{\"1\":{\"289\":1}}],[\"val\",{\"1\":{\"29\":1,\"58\":2,\"65\":1,\"80\":4,\"81\":2,\"82\":1,\"83\":2,\"142\":3,\"145\":3,\"289\":13,\"290\":7,\"477\":1}}],[\"value的数量\",{\"1\":{\"305\":1}}],[\"valueerror\",{\"1\":{\"289\":1,\"511\":1}}],[\"value都会进行缓存\",{\"1\":{\"285\":1}}],[\"value在seq\",{\"1\":{\"285\":1}}],[\"value传入\",{\"1\":{\"285\":1}}],[\"value=tokenizer\",{\"1\":{\"512\":1}}],[\"value=tgt\",{\"1\":{\"76\":1}}],[\"value=past\",{\"1\":{\"477\":1}}],[\"value=self\",{\"1\":{\"285\":1}}],[\"value=none\",{\"1\":{\"162\":1,\"285\":2}}],[\"value=memory\",{\"1\":{\"76\":1}}],[\"value=gt\",{\"1\":{\"75\":1}}],[\"value=x\",{\"1\":{\"75\":1}}],[\"values=outputs\",{\"1\":{\"285\":1}}],[\"values=past\",{\"1\":{\"285\":1,\"477\":2}}],[\"values=query\",{\"1\":{\"285\":1}}],[\"values=next\",{\"1\":{\"285\":1}}],[\"values=none\",{\"1\":{\"285\":2,\"477\":1}}],[\"values\",{\"1\":{\"28\":5,\"92\":1,\"107\":1,\"109\":1,\"284\":3,\"285\":8,\"477\":19}}],[\"value\",{\"1\":{\"14\":1,\"32\":8,\"45\":19,\"68\":3,\"72\":7,\"74\":6,\"162\":5,\"264\":1,\"266\":5,\"267\":2,\"285\":30,\"304\":1,\"305\":1,\"309\":2,\"310\":2,\"312\":1,\"314\":1,\"477\":20,\"512\":1,\"531\":6,\"558\":7,\"658\":4,\"674\":2}}],[\"v2\",{\"1\":{\"16\":1,\"193\":1,\"424\":2,\"494\":1,\"674\":2}}],[\"vlbert\",{\"1\":{\"268\":1}}],[\"vllms\",{\"1\":{\"181\":1}}],[\"vlp\",{\"0\":{\"122\":1},\"1\":{\"120\":2,\"122\":1,\"123\":1,\"253\":1}}],[\"vlmevalkit\",{\"1\":{\"219\":1}}],[\"vlm\",{\"1\":{\"43\":1}}],[\"vl\",{\"1\":{\"2\":1,\"145\":4,\"147\":4,\"162\":4,\"185\":1,\"194\":1,\"208\":1,\"210\":1,\"211\":2,\"212\":1,\"284\":4}}],[\"omni\",{\"1\":{\"674\":1}}],[\"o3\",{\"1\":{\"674\":3}}],[\"o3d\",{\"1\":{\"83\":9}}],[\"o1\",{\"1\":{\"673\":1,\"674\":12}}],[\"o指定输出文件名\",{\"1\":{\"666\":1}}],[\"other指代左操作数a\",{\"1\":{\"660\":1}}],[\"other指代右操作数b\",{\"1\":{\"660\":1}}],[\"other\",{\"1\":{\"660\":7}}],[\"old\",{\"1\":{\"658\":2}}],[\"our\",{\"1\":{\"541\":1}}],[\"outfile\",{\"1\":{\"410\":6,\"412\":6}}],[\"out=none\",{\"1\":{\"381\":1}}],[\"outer\",{\"1\":{\"365\":1,\"366\":4}}],[\"outside\",{\"1\":{\"226\":1,\"541\":1}}],[\"output=pooled\",{\"1\":{\"262\":1}}],[\"outputs\",{\"1\":{\"143\":3,\"163\":3,\"265\":2,\"268\":2,\"285\":26,\"286\":2,\"474\":2,\"477\":3,\"519\":1,\"528\":2,\"529\":7,\"533\":2,\"538\":6,\"540\":4,\"541\":8,\"543\":6,\"544\":6,\"651\":7,\"652\":1,\"654\":1,\"656\":8,\"657\":4,\"658\":9,\"666\":1}}],[\"output\",{\"1\":{\"28\":1,\"36\":2,\"59\":3,\"76\":1,\"99\":1,\"142\":2,\"143\":2,\"145\":10,\"146\":6,\"147\":12,\"161\":6,\"162\":7,\"163\":11,\"244\":3,\"246\":1,\"247\":1,\"262\":11,\"264\":4,\"265\":1,\"267\":1,\"268\":2,\"282\":5,\"284\":4,\"285\":24,\"310\":1,\"319\":1,\"321\":1,\"386\":1,\"393\":1,\"401\":1,\"404\":1,\"474\":2,\"477\":20,\"508\":10,\"510\":8,\"513\":9,\"519\":3,\"525\":9,\"527\":4,\"528\":7,\"529\":4,\"531\":2,\"533\":4,\"536\":2,\"537\":4,\"538\":4,\"540\":5,\"541\":2,\"543\":4,\"544\":4,\"613\":2,\"630\":4,\"634\":1,\"638\":1,\"651\":2,\"652\":2,\"654\":2,\"656\":4,\"657\":4,\"658\":6,\"666\":5}}],[\"out\",{\"1\":{\"28\":3,\"35\":2,\"40\":5,\"46\":5,\"59\":2,\"73\":5,\"92\":7,\"96\":4,\"100\":4,\"294\":5,\"381\":1,\"409\":1,\"410\":5,\"412\":5,\"454\":1,\"470\":2,\"471\":1,\"474\":6,\"531\":1}}],[\"occurred\",{\"1\":{\"412\":1}}],[\"occurrences\",{\"1\":{\"412\":1}}],[\"ocr任务\",{\"1\":{\"221\":1}}],[\"ocr相关任务\",{\"1\":{\"220\":1}}],[\"ocr数据\",{\"1\":{\"217\":1}}],[\"ocr能力和高分辨率处理能力\",{\"1\":{\"215\":1}}],[\"ocr\",{\"1\":{\"190\":1,\"195\":1,\"207\":2,\"217\":1}}],[\"oov\",{\"1\":{\"409\":1,\"497\":1}}],[\"os\",{\"1\":{\"68\":3,\"83\":3,\"142\":1,\"275\":11,\"276\":2,\"277\":14,\"289\":8,\"510\":4,\"511\":1,\"514\":1,\"666\":6}}],[\"opus\",{\"1\":{\"674\":3}}],[\"op\",{\"1\":{\"660\":3}}],[\"operating\",{\"0\":{\"350\":1},\"1\":{\"82\":1}}],[\"openwebtext2\",{\"1\":{\"674\":1}}],[\"openwebtext等\",{\"1\":{\"498\":1}}],[\"openwebtext\",{\"1\":{\"493\":1,\"494\":1}}],[\"openbookqa\",{\"1\":{\"482\":1}}],[\"openclip\",{\"1\":{\"193\":1,\"194\":1}}],[\"opengvlab\",{\"1\":{\"179\":1}}],[\"open3d\",{\"1\":{\"83\":1}}],[\"openai首先尝试了virtex模型\",{\"1\":{\"278\":1}}],[\"openai从网络上收集了4亿条数据进行实验\",{\"1\":{\"278\":1}}],[\"openai从网络上收集了总计4亿对文本和图像\",{\"1\":{\"272\":1}}],[\"openai\",{\"1\":{\"210\":1,\"224\":2,\"270\":1,\"275\":2,\"277\":1,\"428\":1,\"470\":3,\"472\":4,\"673\":1,\"674\":10,\"682\":1}}],[\"openaccess\",{\"1\":{\"60\":1}}],[\"openad\",{\"1\":{\"7\":1}}],[\"opening\",{\"1\":{\"28\":2,\"78\":1,\"276\":1,\"277\":1}}],[\"openimage\",{\"1\":{\"17\":1}}],[\"openkd\",{\"1\":{\"7\":1}}],[\"open\",{\"1\":{\"4\":1,\"6\":1,\"7\":1,\"28\":2,\"29\":2,\"31\":2,\"58\":2,\"62\":1,\"63\":1,\"67\":1,\"68\":3,\"83\":2,\"142\":1,\"206\":2,\"275\":1,\"276\":1,\"277\":2,\"289\":2,\"410\":4,\"411\":1,\"412\":7,\"472\":1,\"474\":20,\"478\":1,\"487\":1,\"510\":3,\"511\":3,\"666\":2}}],[\"option\",{\"1\":{\"83\":1}}],[\"optional\",{\"1\":{\"43\":1,\"76\":6,\"163\":1,\"389\":1,\"477\":10}}],[\"optimized\",{\"1\":{\"490\":1,\"496\":1}}],[\"optimizer\",{\"1\":{\"80\":3,\"81\":2,\"82\":2,\"142\":5,\"145\":5,\"147\":5,\"159\":3,\"244\":6,\"296\":1,\"514\":2,\"670\":1}}],[\"optimizing\",{\"1\":{\"419\":1}}],[\"optimization算法进一步优化模型行为\",{\"1\":{\"468\":1}}],[\"optimization\",{\"1\":{\"224\":2,\"470\":1}}],[\"optim\",{\"1\":{\"80\":3,\"142\":1,\"145\":1,\"147\":1,\"244\":1}}],[\"opt\",{\"1\":{\"80\":2,\"81\":1,\"83\":3,\"485\":1}}],[\"observation\",{\"1\":{\"596\":1}}],[\"observations\",{\"1\":{\"37\":1}}],[\"obj\",{\"1\":{\"29\":4,\"45\":7,\"58\":9,\"59\":28,\"378\":4,\"657\":4,\"660\":4}}],[\"objcot\",{\"1\":{\"24\":2}}],[\"objaverse\",{\"1\":{\"17\":1}}],[\"objective\",{\"1\":{\"470\":1}}],[\"objectives\",{\"0\":{\"153\":1,\"258\":1}}],[\"objects\",{\"1\":{\"25\":1,\"68\":3,\"83\":3}}],[\"object\",{\"0\":{\"11\":1},\"1\":{\"4\":1,\"6\":1,\"11\":1,\"13\":1,\"19\":2,\"20\":1,\"23\":1,\"28\":16,\"29\":9,\"30\":2,\"32\":1,\"33\":2,\"37\":1,\"47\":1,\"58\":9,\"59\":1,\"60\":3,\"67\":1,\"68\":4,\"82\":2,\"404\":1,\"666\":1}}],[\"oq\",{\"1\":{\"32\":1}}],[\"o\",{\"1\":{\"30\":5,\"33\":5,\"35\":3,\"74\":2,\"75\":1,\"112\":2,\"666\":2,\"667\":1,\"674\":1,\"683\":1}}],[\"od\",{\"1\":{\"29\":3}}],[\"ok=true\",{\"1\":{\"510\":2}}],[\"okvqa\",{\"1\":{\"196\":1}}],[\"ok\",{\"1\":{\"29\":4,\"32\":20,\"33\":2}}],[\"once\",{\"1\":{\"477\":1}}],[\"onnx\",{\"1\":{\"326\":1}}],[\"only=true\",{\"1\":{\"514\":1}}],[\"only\",{\"1\":{\"262\":1,\"286\":1,\"520\":1,\"536\":1,\"542\":2,\"543\":1,\"674\":5}}],[\"on\",{\"1\":{\"28\":2,\"31\":2,\"60\":3,\"63\":1,\"82\":1,\"83\":2,\"273\":3,\"412\":1,\"558\":1,\"667\":1}}],[\"ones\",{\"1\":{\"43\":8,\"92\":1,\"142\":1,\"143\":1,\"145\":3,\"146\":1,\"147\":3,\"160\":1,\"161\":1,\"162\":1,\"282\":1,\"284\":3,\"285\":1,\"286\":1,\"642\":1,\"652\":1,\"654\":1,\"656\":1,\"658\":3}}],[\"one\",{\"1\":{\"28\":1,\"145\":2,\"154\":1,\"155\":1,\"156\":1,\"157\":3,\"159\":3,\"244\":2,\"296\":1,\"460\":1,\"461\":1,\"462\":1,\"464\":1,\"514\":2,\"576\":1,\"596\":1}}],[\"ov\",{\"1\":{\"32\":1}}],[\"overwrite\",{\"1\":{\"519\":1}}],[\"overall\",{\"1\":{\"82\":2}}],[\"over\",{\"0\":{\"403\":1},\"1\":{\"22\":1,\"82\":3,\"403\":2,\"585\":1}}],[\"ovag旨在通过额外指令\",{\"1\":{\"7\":1}}],[\"ovag\",{\"1\":{\"6\":1,\"7\":1}}],[\"offset\",{\"1\":{\"325\":2}}],[\"official\",{\"1\":{\"300\":1}}],[\"off\",{\"1\":{\"276\":1,\"277\":1}}],[\"of\",{\"0\":{\"9\":1,\"28\":1,\"126\":1,\"434\":1,\"569\":1},\"1\":{\"7\":1,\"26\":1,\"28\":16,\"31\":2,\"59\":2,\"83\":3,\"92\":1,\"120\":1,\"138\":1,\"142\":2,\"143\":3,\"145\":1,\"164\":2,\"226\":2,\"244\":1,\"246\":2,\"247\":3,\"248\":1,\"272\":4,\"273\":6,\"274\":3,\"275\":2,\"276\":1,\"277\":2,\"278\":3,\"285\":1,\"289\":1,\"315\":1,\"397\":1,\"409\":1,\"412\":23,\"418\":1,\"420\":1,\"421\":2,\"422\":1,\"434\":2,\"435\":1,\"448\":1,\"495\":1,\"504\":1,\"517\":1,\"520\":1,\"542\":2,\"543\":1,\"554\":1,\"556\":1,\"558\":1,\"596\":1,\"676\":1,\"688\":1}}],[\"or门组合\",{\"1\":{\"395\":1}}],[\"ordered\",{\"1\":{\"410\":3,\"412\":3}}],[\"ordereddict\",{\"1\":{\"296\":1}}],[\"order=\",{\"1\":{\"323\":1}}],[\"order\",{\"1\":{\"322\":2,\"323\":1,\"365\":1,\"412\":1}}],[\"orthogonal\",{\"1\":{\"108\":1}}],[\"original\",{\"1\":{\"98\":1,\"300\":1,\"326\":1,\"531\":1}}],[\"or\",{\"1\":{\"40\":1,\"46\":2,\"59\":2,\"104\":1,\"162\":1,\"252\":2,\"272\":2,\"294\":2,\"295\":1,\"296\":2,\"511\":1,\"519\":1,\"525\":1,\"535\":1}}],[\"org\",{\"1\":{\"4\":1,\"37\":1,\"47\":1,\"85\":1,\"102\":1,\"179\":1,\"223\":1,\"279\":1,\"300\":1}}],[\"oracle\",{\"0\":{\"2\":1},\"1\":{\"0\":1}}],[\"gc可处理开发者未显式解决的循环引用\",{\"1\":{\"657\":1}}],[\"gc可能无法及时释放内存\",{\"1\":{\"657\":1}}],[\"gc作为兜底机制\",{\"1\":{\"657\":1}}],[\"gc与弱引用的互补关系\",{\"1\":{\"657\":1}}],[\"gc需要扫描整个对象图来检测循环引用\",{\"1\":{\"657\":1}}],[\"gc是一种后台机制\",{\"1\":{\"657\":1}}],[\"gc\",{\"1\":{\"657\":2}}],[\"gx1\",{\"1\":{\"660\":2}}],[\"gx0\",{\"1\":{\"660\":2}}],[\"gxs\",{\"1\":{\"652\":6,\"654\":5,\"656\":5,\"658\":5}}],[\"gx\",{\"1\":{\"631\":2,\"652\":2,\"654\":3,\"656\":3,\"658\":3,\"660\":2}}],[\"gys\",{\"1\":{\"651\":1,\"652\":2,\"654\":2,\"656\":2,\"657\":1,\"658\":2}}],[\"gy\",{\"1\":{\"630\":1,\"631\":2,\"660\":14}}],[\"gsm8k\",{\"1\":{\"482\":1}}],[\"gmm\",{\"1\":{\"355\":2}}],[\"gqa\",{\"1\":{\"195\":1,\"196\":1,\"674\":3}}],[\"gutenberg\",{\"1\":{\"481\":1}}],[\"gumbel\",{\"1\":{\"170\":1}}],[\"guid\",{\"1\":{\"520\":1}}],[\"guide\",{\"1\":{\"78\":1,\"421\":1}}],[\"guided\",{\"1\":{\"60\":3,\"82\":1}}],[\"guitar\",{\"1\":{\"29\":1}}],[\"gaussian\",{\"0\":{\"588\":1,\"591\":1},\"1\":{\"584\":1,\"588\":1,\"590\":1,\"592\":1}}],[\"garage\",{\"1\":{\"273\":1}}],[\"gap\",{\"1\":{\"206\":2}}],[\"gans\",{\"1\":{\"178\":1}}],[\"gather函数比较灵活\",{\"1\":{\"514\":1}}],[\"gather\",{\"1\":{\"145\":2,\"249\":2,\"513\":4}}],[\"gamma=gamma\",{\"1\":{\"404\":1}}],[\"gamma=0\",{\"1\":{\"80\":1}}],[\"gamma\",{\"1\":{\"78\":3,\"404\":3}}],[\"glm4\",{\"1\":{\"674\":1}}],[\"glm系列模型是\",{\"1\":{\"674\":1}}],[\"glm\",{\"1\":{\"673\":1,\"674\":5}}],[\"gleu\",{\"1\":{\"525\":2}}],[\"glove\",{\"1\":{\"464\":1}}],[\"global\",{\"1\":{\"107\":1,\"109\":8,\"110\":1,\"111\":1,\"112\":1}}],[\"glue基准\",{\"1\":{\"499\":1}}],[\"glue多任务提升5\",{\"1\":{\"440\":1}}],[\"glue\",{\"1\":{\"196\":1,\"492\":1,\"494\":1}}],[\"glasses\",{\"1\":{\"29\":1}}],[\"gt\",{\"0\":{\"64\":1},\"1\":{\"64\":2,\"68\":2,\"75\":5,\"78\":1,\"81\":2,\"82\":2,\"401\":1,\"402\":2,\"403\":2}}],[\"gp\",{\"1\":{\"355\":1}}],[\"gpblock\",{\"1\":{\"75\":1}}],[\"gpb\",{\"1\":{\"70\":3}}],[\"gpt系列\",{\"1\":{\"485\":1}}],[\"gpt2pretrainedmodel\",{\"1\":{\"477\":1}}],[\"gpt2model\",{\"1\":{\"477\":1}}],[\"gpt2attention\",{\"1\":{\"477\":3}}],[\"gpt2block\",{\"1\":{\"477\":3}}],[\"gpt2config\",{\"1\":{\"477\":2}}],[\"gpt2\",{\"1\":{\"474\":2,\"477\":5}}],[\"gpt2tokenizer\",{\"1\":{\"474\":2,\"477\":2}}],[\"gpt2lmheadmodel\",{\"1\":{\"474\":2,\"477\":2}}],[\"gpt和bert通过不同训练目标\",{\"1\":{\"501\":1}}],[\"gpt和bert\",{\"1\":{\"238\":1}}],[\"gpt\",{\"0\":{\"228\":2,\"285\":1,\"438\":1,\"451\":1,\"458\":1},\"1\":{\"63\":3,\"206\":2,\"207\":2,\"210\":1,\"224\":5,\"227\":2,\"228\":6,\"409\":1,\"425\":2,\"428\":2,\"438\":1,\"451\":1,\"452\":1,\"453\":1,\"454\":10,\"455\":8,\"456\":1,\"458\":1,\"459\":1,\"460\":3,\"461\":6,\"462\":13,\"463\":7,\"464\":11,\"465\":1,\"469\":2,\"470\":2,\"471\":14,\"472\":2,\"481\":1,\"482\":2,\"485\":1,\"542\":2,\"673\":8,\"674\":21,\"676\":1,\"678\":1}}],[\"gpu上的矩阵运算都是充分优化和高度并行的\",{\"1\":{\"547\":1}}],[\"gpu上推理时\",{\"1\":{\"482\":1}}],[\"gpu上训练12天\",{\"1\":{\"272\":1}}],[\"gpu上训练18天\",{\"1\":{\"272\":1}}],[\"gpu节点\",{\"1\":{\"131\":1}}],[\"gpu\",{\"1\":{\"40\":1,\"112\":1,\"145\":3,\"200\":1,\"201\":1,\"238\":1,\"244\":1,\"249\":3,\"428\":1,\"481\":1,\"494\":1,\"519\":2,\"547\":1,\"675\":1}}],[\"g\",{\"1\":{\"40\":1,\"72\":1,\"73\":1,\"105\":1,\"183\":1,\"188\":1,\"189\":1,\"193\":2,\"194\":3,\"666\":6}}],[\"gehman\",{\"1\":{\"469\":1}}],[\"gemini系列\",{\"1\":{\"220\":1}}],[\"gemini系列和qwen\",{\"1\":{\"208\":1}}],[\"gemini\",{\"1\":{\"207\":1,\"210\":1,\"673\":1,\"674\":14}}],[\"generic\",{\"1\":{\"179\":1,\"557\":1}}],[\"generator模型结构图\",{\"1\":{\"550\":1}}],[\"generator\",{\"0\":{\"550\":1},\"1\":{\"549\":3,\"550\":2}}],[\"generative\",{\"0\":{\"286\":1},\"1\":{\"190\":1,\"286\":1,\"438\":1,\"449\":1,\"674\":1}}],[\"generation能力\",{\"1\":{\"280\":1}}],[\"generation\",{\"0\":{\"176\":1,\"285\":1,\"679\":1},\"1\":{\"28\":4,\"119\":2,\"134\":1,\"176\":2,\"178\":1,\"285\":1,\"412\":1,\"419\":1,\"656\":11,\"658\":3,\"659\":1,\"666\":1,\"679\":1,\"688\":1}}],[\"generate\",{\"1\":{\"143\":3,\"286\":2,\"477\":2}}],[\"genome\",{\"1\":{\"131\":1,\"140\":1,\"253\":1}}],[\"gelu\",{\"1\":{\"73\":2,\"294\":4,\"296\":2,\"447\":1,\"513\":5}}],[\"getattr\",{\"1\":{\"658\":1}}],[\"getcwd\",{\"1\":{\"275\":1,\"277\":1}}],[\"get\",{\"1\":{\"29\":1,\"40\":1,\"58\":3,\"59\":3,\"80\":1,\"83\":3,\"93\":2,\"96\":2,\"101\":2,\"275\":10,\"276\":4,\"277\":12,\"410\":5,\"411\":10,\"412\":16,\"477\":1,\"513\":1,\"517\":1,\"520\":1,\"531\":1,\"666\":4}}],[\"getitem\",{\"1\":{\"29\":2,\"58\":1,\"68\":1,\"142\":1,\"289\":1,\"659\":1}}],[\"geometries\",{\"1\":{\"83\":1}}],[\"geometric\",{\"1\":{\"11\":1,\"28\":4,\"582\":1}}],[\"geometry\",{\"1\":{\"4\":1,\"5\":1,\"83\":3}}],[\"git\",{\"1\":{\"519\":2,\"546\":1}}],[\"github\",{\"1\":{\"4\":1,\"37\":1,\"47\":1,\"60\":1,\"85\":2,\"102\":2,\"119\":1,\"140\":1,\"148\":1,\"179\":1,\"223\":1,\"232\":1,\"252\":1,\"279\":1,\"289\":1,\"300\":2,\"406\":1,\"477\":1,\"480\":1,\"481\":1,\"503\":1,\"519\":1,\"546\":1,\"603\":1,\"649\":1,\"664\":1,\"669\":1}}],[\"gi\",{\"1\":{\"258\":3}}],[\"giraffe\",{\"1\":{\"258\":1}}],[\"gibson\",{\"1\":{\"49\":1}}],[\"given\",{\"1\":{\"412\":4}}],[\"give\",{\"1\":{\"28\":1}}],[\"gick\",{\"1\":{\"6\":1}}],[\"goldstein\",{\"1\":{\"662\":3,\"666\":3}}],[\"golang\",{\"1\":{\"2\":1}}],[\"good\",{\"1\":{\"510\":1}}],[\"googleapis\",{\"1\":{\"519\":1}}],[\"google\",{\"1\":{\"4\":1,\"47\":1,\"73\":1,\"210\":1,\"300\":2,\"504\":2,\"674\":3}}],[\"gotcha\",{\"1\":{\"484\":1}}],[\"greet\",{\"1\":{\"367\":1,\"372\":10,\"377\":2}}],[\"great通过模拟人类多步推理\",{\"1\":{\"7\":1}}],[\"great通过微调mllms并设计mhacot策略解决这一问题\",{\"1\":{\"7\":1}}],[\"great框架通过以下设计模拟这一过程\",{\"1\":{\"6\":1}}],[\"great\",{\"0\":{\"4\":1},\"1\":{\"4\":2,\"5\":1,\"8\":2,\"21\":1,\"23\":3,\"26\":2,\"29\":2,\"30\":1}}],[\"grit\",{\"1\":{\"217\":1}}],[\"grids\",{\"1\":{\"114\":1}}],[\"grid\",{\"1\":{\"103\":1,\"253\":1,\"256\":1,\"291\":3,\"667\":1}}],[\"grok\",{\"1\":{\"210\":1,\"673\":1}}],[\"group流程图\",{\"1\":{\"92\":1}}],[\"grouped\",{\"1\":{\"92\":12,\"96\":13,\"674\":2}}],[\"groups=40\",{\"1\":{\"83\":1}}],[\"groups=opt\",{\"1\":{\"80\":1}}],[\"groups\",{\"1\":{\"80\":1}}],[\"group\",{\"1\":{\"72\":1,\"75\":2,\"92\":23,\"93\":3,\"96\":3,\"101\":4,\"226\":1}}],[\"grouping阶段\",{\"1\":{\"75\":1}}],[\"grouping\",{\"0\":{\"72\":1,\"90\":1,\"95\":1,\"97\":1},\"1\":{\"71\":1,\"75\":3,\"87\":1,\"88\":2,\"90\":1,\"93\":4,\"94\":2,\"98\":1}}],[\"grounded\",{\"0\":{\"285\":1},\"1\":{\"126\":2,\"127\":2,\"285\":1}}],[\"ground\",{\"0\":{\"64\":1},\"1\":{\"78\":2,\"82\":2,\"162\":1,\"240\":1,\"401\":2,\"402\":1,\"403\":2,\"404\":1,\"405\":1}}],[\"grounding\",{\"1\":{\"4\":1,\"7\":2,\"37\":1,\"47\":1,\"48\":1}}],[\"graph函数\",{\"1\":{\"666\":1}}],[\"graph\",{\"1\":{\"666\":14}}],[\"graph=true\",{\"1\":{\"665\":1}}],[\"graphviz\",{\"1\":{\"665\":1,\"666\":3}}],[\"gram\",{\"1\":{\"485\":1}}],[\"gram重叠率\",{\"1\":{\"455\":1}}],[\"grams\",{\"1\":{\"278\":2}}],[\"grad模式优化内存管理\",{\"1\":{\"663\":1}}],[\"grad函数\",{\"1\":{\"658\":1}}],[\"grad默认false\",{\"1\":{\"658\":1}}],[\"grad=false\",{\"1\":{\"658\":1}}],[\"grad参数及时清除中间变量导数\",{\"1\":{\"658\":1}}],[\"grad参数\",{\"1\":{\"658\":1}}],[\"gradientcheckpointinglayer\",{\"1\":{\"477\":1}}],[\"gradients\",{\"1\":{\"285\":1}}],[\"gradient\",{\"1\":{\"244\":1,\"246\":1,\"247\":1,\"477\":1,\"646\":1,\"667\":2}}],[\"grad\",{\"0\":{\"388\":1},\"1\":{\"80\":2,\"81\":1,\"142\":6,\"145\":7,\"146\":2,\"147\":7,\"159\":1,\"161\":1,\"162\":1,\"163\":1,\"244\":1,\"246\":1,\"247\":1,\"248\":1,\"249\":1,\"273\":2,\"275\":2,\"277\":2,\"284\":1,\"300\":1,\"474\":1,\"477\":1,\"514\":1,\"629\":1,\"632\":7,\"635\":2,\"638\":2,\"642\":2,\"645\":1,\"646\":3,\"652\":4,\"653\":1,\"654\":7,\"656\":8,\"657\":1,\"658\":16,\"659\":1,\"660\":3,\"661\":2,\"662\":6,\"667\":6}}],[\"grasping\",{\"1\":{\"63\":2,\"78\":1,\"404\":1}}],[\"grasp\",{\"1\":{\"6\":1,\"7\":1,\"23\":1,\"28\":2,\"29\":1,\"31\":2,\"40\":1,\"58\":1,\"62\":1,\"63\":1,\"67\":1,\"68\":2,\"70\":1,\"71\":1}}],[\"转化成功之后\",{\"1\":{\"519\":1}}],[\"转化为一系列\",{\"1\":{\"240\":1}}],[\"转向\",{\"1\":{\"468\":1}}],[\"转换后的图像展示x0\",{\"1\":{\"666\":1}}],[\"转换视角\",{\"0\":{\"579\":1}}],[\"转换成y序列的权重参数组成的矩阵\",{\"1\":{\"414\":1}}],[\"转换成一个固定大小的特征图\",{\"1\":{\"396\":1}}],[\"转换成\",{\"1\":{\"396\":1}}],[\"转换为一系列高维向量表示\",{\"1\":{\"548\":1}}],[\"转换为概率\",{\"1\":{\"405\":1}}],[\"转换为可优化的损失函数\",{\"1\":{\"403\":1}}],[\"转换为固定大小\",{\"1\":{\"397\":1}}],[\"转换为元组形式\",{\"1\":{\"291\":1}}],[\"转换为语言嵌入\",{\"1\":{\"227\":1}}],[\"转换为语言模型可用的\",{\"1\":{\"226\":1}}],[\"转换为同构表示\",{\"1\":{\"59\":1}}],[\"转换为嵌入向量\",{\"1\":{\"40\":1}}],[\"转换为\",{\"1\":{\"40\":1,\"43\":1,\"82\":1,\"100\":1}}],[\"转折点\",{\"1\":{\"395\":2}}],[\"转为连续张量\",{\"1\":{\"385\":1}}],[\"转为numpy数组\",{\"1\":{\"83\":1}}],[\"转置前\",{\"1\":{\"326\":1}}],[\"转置包括逻辑转置和物理转置\",{\"1\":{\"326\":1}}],[\"转置\",{\"0\":{\"326\":1},\"1\":{\"326\":2}}],[\"转置为\",{\"1\":{\"83\":1}}],[\"转置后的步长是\",{\"1\":{\"326\":1}}],[\"转置后的矩阵步长是\",{\"1\":{\"326\":1}}],[\"转置后\",{\"1\":{\"34\":1,\"73\":1,\"326\":1}}],[\"转\",{\"1\":{\"3\":1}}],[\"转型\",{\"1\":{\"2\":1}}],[\"c为常数指数\",{\"1\":{\"660\":1}}],[\"c为输入token的总维度\",{\"1\":{\"295\":1}}],[\"c中\",{\"1\":{\"660\":1}}],[\"c并求导\",{\"1\":{\"660\":1}}],[\"c的引用计数仍为1\",{\"1\":{\"657\":1}}],[\"cdf\",{\"1\":{\"566\":2,\"584\":1}}],[\"cd\",{\"1\":{\"519\":1,\"546\":1}}],[\"c4\",{\"1\":{\"480\":1,\"481\":1}}],[\"ctrl\",{\"1\":{\"464\":1}}],[\"cb\",{\"1\":{\"462\":1}}],[\"cbt测试模型对不同词类\",{\"1\":{\"455\":1}}],[\"cbt\",{\"1\":{\"455\":1}}],[\"cbow\",{\"1\":{\"272\":1}}],[\"c=in\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"c=3\",{\"1\":{\"291\":1,\"292\":1,\"293\":1,\"296\":1}}],[\"ck\",{\"1\":{\"247\":1,\"390\":1}}],[\"ckpt=false\",{\"1\":{\"145\":1,\"147\":1}}],[\"ckpt=config\",{\"1\":{\"142\":1,\"145\":1,\"147\":1}}],[\"ckpt\",{\"1\":{\"142\":9,\"145\":6,\"146\":4,\"147\":6,\"519\":1}}],[\"c或internvl\",{\"1\":{\"189\":1}}],[\"cc\",{\"1\":{\"492\":1,\"493\":1,\"494\":1}}],[\"ccbench等中文基准上大幅领先\",{\"1\":{\"220\":1}}],[\"cc3m\",{\"1\":{\"140\":1,\"226\":1}}],[\"cc12m\",{\"1\":{\"140\":1}}],[\"c+d\",{\"1\":{\"92\":3}}],[\"ceil\",{\"1\":{\"396\":1}}],[\"centercrop\",{\"1\":{\"290\":1}}],[\"center\",{\"1\":{\"289\":2}}],[\"centroids\",{\"1\":{\"92\":4}}],[\"centroid\",{\"1\":{\"83\":2,\"92\":2}}],[\"ce\",{\"1\":{\"80\":1,\"81\":3,\"404\":15,\"407\":10}}],[\"celoss\",{\"1\":{\"78\":4}}],[\"chroma\",{\"1\":{\"687\":1}}],[\"christiano\",{\"1\":{\"469\":1}}],[\"chinesegluedatasets\",{\"1\":{\"519\":2}}],[\"chinese\",{\"1\":{\"519\":5}}],[\"chinchilla\",{\"1\":{\"482\":1,\"485\":1}}],[\"children\",{\"1\":{\"455\":1}}],[\"check\",{\"1\":{\"646\":1}}],[\"checkpointing\",{\"1\":{\"477\":1,\"481\":1}}],[\"checkpoint\",{\"1\":{\"82\":2,\"83\":2,\"147\":1,\"514\":5,\"519\":2}}],[\"chen\",{\"1\":{\"126\":1}}],[\"chunking\",{\"1\":{\"112\":1}}],[\"choices\",{\"1\":{\"544\":9}}],[\"choice\",{\"1\":{\"63\":1,\"512\":1,\"544\":1}}],[\"chapter3\",{\"1\":{\"666\":3,\"667\":2}}],[\"chapter2\",{\"1\":{\"661\":7}}],[\"challenge中零样本即可取得88\",{\"1\":{\"462\":1}}],[\"challenge\",{\"1\":{\"455\":1}}],[\"character\",{\"1\":{\"412\":1}}],[\"characteristic\",{\"0\":{\"350\":1},\"1\":{\"82\":1}}],[\"chartqa\",{\"1\":{\"208\":1,\"217\":1,\"220\":1,\"222\":1}}],[\"charlesq34\",{\"1\":{\"85\":1,\"102\":1}}],[\"channels\",{\"1\":{\"40\":1,\"289\":1}}],[\"channel=2\",{\"1\":{\"323\":1}}],[\"channel=256+3\",{\"1\":{\"101\":1}}],[\"channel=256\",{\"1\":{\"93\":1}}],[\"channel=320\",{\"1\":{\"101\":1}}],[\"channel=384\",{\"1\":{\"101\":1}}],[\"channel=768\",{\"1\":{\"101\":1}}],[\"channel=64+3\",{\"1\":{\"101\":1}}],[\"channel=9+3\",{\"1\":{\"101\":1}}],[\"channel=128+3\",{\"1\":{\"101\":1}}],[\"channel=128\",{\"1\":{\"93\":1,\"101\":1}}],[\"channel=in\",{\"1\":{\"93\":1}}],[\"channel=true\",{\"1\":{\"93\":1,\"96\":1}}],[\"channel=518+additional\",{\"1\":{\"35\":1,\"46\":1,\"59\":1}}],[\"channel=512+self\",{\"1\":{\"35\":1,\"46\":1,\"59\":1}}],[\"channel=832\",{\"1\":{\"35\":1,\"46\":1,\"59\":1}}],[\"channel\",{\"1\":{\"30\":2,\"34\":1,\"35\":2,\"46\":3,\"59\":2,\"73\":11,\"92\":12,\"93\":7,\"96\":15,\"100\":11,\"323\":1}}],[\"chair\",{\"1\":{\"29\":1,\"58\":1,\"67\":1,\"68\":1,\"83\":6}}],[\"chains\",{\"1\":{\"683\":1}}],[\"chain\",{\"0\":{\"9\":1,\"28\":1,\"434\":1},\"1\":{\"7\":1,\"26\":1,\"434\":2,\"435\":1,\"676\":1,\"684\":2,\"687\":1}}],[\"chatglm\",{\"1\":{\"674\":2}}],[\"chatgpt\",{\"1\":{\"224\":1,\"227\":1,\"231\":1,\"424\":1,\"673\":1,\"674\":7,\"678\":1,\"682\":1}}],[\"chatbot\",{\"1\":{\"227\":1}}],[\"chat\",{\"1\":{\"28\":4,\"189\":1,\"195\":2,\"202\":2,\"219\":1,\"227\":1,\"487\":1,\"674\":4}}],[\"creator\",{\"1\":{\"634\":3,\"635\":1,\"638\":3,\"651\":1,\"652\":3,\"654\":3,\"656\":8,\"658\":4,\"659\":1,\"666\":3}}],[\"creating\",{\"1\":{\"410\":1,\"412\":1}}],[\"creates\",{\"1\":{\"520\":1}}],[\"create\",{\"1\":{\"83\":1,\"142\":3,\"145\":4,\"146\":1,\"147\":4,\"246\":2,\"331\":4,\"335\":1,\"410\":4,\"412\":6,\"519\":1,\"520\":1,\"546\":1,\"665\":1}}],[\"crawl这类数据在整个训练中只被读取一次左右\",{\"1\":{\"461\":1}}],[\"crawl执行了质量过滤和模糊去重\",{\"1\":{\"461\":1}}],[\"crawl\",{\"1\":{\"217\":1,\"454\":1,\"461\":1,\"674\":1}}],[\"crafting\",{\"0\":{\"63\":1}}],[\"critical\",{\"1\":{\"105\":1,\"112\":2}}],[\"criterion2\",{\"1\":{\"514\":2}}],[\"criterion1\",{\"1\":{\"514\":2}}],[\"criterion\",{\"1\":{\"80\":2,\"81\":2,\"244\":5}}],[\"crows\",{\"1\":{\"469\":1,\"470\":1,\"471\":1,\"482\":1,\"484\":1}}],[\"crops\",{\"1\":{\"244\":1}}],[\"crop\",{\"1\":{\"58\":1}}],[\"crossentropy\",{\"1\":{\"407\":3}}],[\"crossentropyloss\",{\"1\":{\"80\":1,\"163\":3,\"244\":1,\"268\":1,\"285\":2,\"514\":2,\"529\":1,\"538\":1,\"541\":1,\"543\":1,\"544\":1}}],[\"crossattention\",{\"1\":{\"162\":1,\"264\":2,\"285\":1}}],[\"cross\",{\"0\":{\"14\":1},\"1\":{\"32\":1,\"33\":2,\"35\":1,\"36\":4,\"40\":1,\"45\":8,\"59\":3,\"75\":1,\"78\":1,\"126\":1,\"143\":1,\"145\":1,\"147\":2,\"152\":1,\"162\":5,\"163\":2,\"191\":2,\"244\":2,\"262\":2,\"263\":1,\"264\":1,\"266\":3,\"267\":2,\"272\":2,\"283\":2,\"284\":1,\"285\":13,\"402\":5,\"404\":4,\"407\":2,\"477\":3}}],[\"cs231n\",{\"0\":{\"560\":1}}],[\"cs224n\",{\"0\":{\"559\":1}}],[\"csv\",{\"1\":{\"68\":2,\"510\":6}}],[\"cs\",{\"1\":{\"40\":2,\"105\":2}}],[\"csdn\",{\"1\":{\"0\":1}}],[\"cp\",{\"1\":{\"40\":1,\"667\":1}}],[\"cpu\",{\"1\":{\"28\":1,\"40\":1,\"83\":2,\"273\":3,\"275\":3,\"277\":3,\"547\":1}}],[\"cider\",{\"1\":{\"194\":1}}],[\"ci\",{\"1\":{\"40\":1}}],[\"cnn中\",{\"1\":{\"395\":1}}],[\"cnn具有两种归纳偏置\",{\"1\":{\"287\":1}}],[\"cnn二阶段检测器提取region的特征\",{\"1\":{\"256\":1}}],[\"cnn\",{\"1\":{\"73\":1,\"107\":1,\"112\":4,\"114\":3,\"166\":1,\"272\":1,\"287\":1,\"299\":9,\"301\":1,\"396\":2,\"397\":2,\"455\":1}}],[\"cn\",{\"1\":{\"37\":1,\"194\":2,\"203\":1,\"220\":1}}],[\"cmap=\",{\"1\":{\"667\":1}}],[\"cmafm\",{\"0\":{\"14\":1},\"1\":{\"5\":1,\"8\":1,\"14\":1,\"24\":1}}],[\"cmd\",{\"1\":{\"666\":2}}],[\"cm\",{\"1\":{\"355\":1}}],[\"cmff\",{\"1\":{\"35\":2}}],[\"cylindrical\",{\"1\":{\"32\":1}}],[\"cli\",{\"1\":{\"685\":1}}],[\"clipprocessor\",{\"1\":{\"275\":1,\"277\":2}}],[\"clipmodel\",{\"1\":{\"275\":1,\"277\":2}}],[\"clip模型均能够以较高的置信度给出正确的分类结果\",{\"1\":{\"273\":1}}],[\"clip模型能够在没有特定任务训练数据的情况下\",{\"1\":{\"273\":1}}],[\"clip模型的一个显著优势是它能够进行zero\",{\"1\":{\"273\":1}}],[\"clip模型会预测出个可能的文本\",{\"1\":{\"272\":1}}],[\"clip包含两个核心模型\",{\"1\":{\"272\":1}}],[\"clip的训练数据采用的是文本\",{\"1\":{\"271\":1}}],[\"clip的英文全称为contrastive\",{\"1\":{\"271\":1}}],[\"clip原始论文链接\",{\"1\":{\"269\":1}}],[\"clip属于基于对比学习的多模态模型\",{\"1\":{\"271\":1}}],[\"clip属于\",{\"1\":{\"255\":1}}],[\"clip\",{\"0\":{\"283\":1},\"1\":{\"190\":1,\"193\":1,\"212\":1,\"226\":2,\"227\":2,\"270\":3,\"273\":1,\"275\":3,\"277\":1,\"278\":1,\"514\":1}}],[\"cleargrad\",{\"1\":{\"653\":1,\"667\":4}}],[\"clean\",{\"1\":{\"29\":1}}],[\"clm\",{\"1\":{\"268\":1}}],[\"clamp\",{\"1\":{\"407\":2,\"541\":4}}],[\"cla\",{\"1\":{\"289\":9}}],[\"claude\",{\"1\":{\"210\":1,\"673\":1,\"674\":14}}],[\"classifier\",{\"1\":{\"508\":2,\"513\":3,\"519\":2,\"529\":2,\"543\":2,\"544\":2}}],[\"classification\",{\"1\":{\"99\":1,\"193\":1,\"194\":1,\"520\":1}}],[\"class=val\",{\"1\":{\"290\":1}}],[\"class=train\",{\"1\":{\"290\":1}}],[\"classes=5\",{\"1\":{\"300\":1}}],[\"classes=num\",{\"1\":{\"300\":1}}],[\"classes=1000\",{\"1\":{\"292\":1,\"293\":1,\"296\":1}}],[\"classes=dim\",{\"1\":{\"246\":2}}],[\"classes\",{\"1\":{\"68\":2,\"98\":1,\"101\":8,\"246\":1,\"292\":3,\"293\":2,\"296\":6,\"300\":2}}],[\"class\",{\"0\":{\"292\":1},\"1\":{\"29\":1,\"30\":1,\"32\":1,\"33\":1,\"34\":2,\"35\":2,\"36\":2,\"40\":1,\"41\":1,\"45\":2,\"46\":1,\"58\":1,\"59\":7,\"62\":1,\"68\":6,\"70\":1,\"72\":1,\"73\":1,\"74\":1,\"75\":1,\"76\":2,\"78\":1,\"83\":1,\"92\":1,\"93\":4,\"96\":4,\"100\":1,\"101\":1,\"107\":1,\"109\":1,\"110\":1,\"111\":1,\"142\":2,\"145\":1,\"146\":1,\"147\":1,\"162\":1,\"163\":1,\"244\":1,\"246\":1,\"257\":1,\"262\":2,\"263\":1,\"264\":1,\"265\":2,\"266\":1,\"268\":3,\"282\":1,\"284\":1,\"285\":4,\"286\":1,\"289\":27,\"291\":1,\"292\":3,\"293\":1,\"294\":2,\"295\":1,\"296\":1,\"373\":1,\"374\":3,\"377\":1,\"401\":2,\"402\":1,\"403\":1,\"404\":3,\"405\":1,\"407\":1,\"412\":1,\"477\":3,\"508\":1,\"511\":1,\"513\":1,\"517\":1,\"523\":1,\"525\":3,\"526\":1,\"527\":1,\"528\":1,\"529\":1,\"531\":1,\"532\":1,\"533\":1,\"535\":1,\"536\":1,\"537\":1,\"538\":1,\"541\":1,\"543\":1,\"544\":1,\"549\":1,\"550\":1,\"552\":1,\"553\":1,\"554\":1,\"556\":1,\"557\":1,\"558\":1,\"607\":1,\"613\":2,\"617\":1,\"629\":1,\"630\":1,\"631\":1,\"634\":2,\"635\":1,\"638\":1,\"642\":1,\"643\":1,\"645\":1,\"646\":1,\"651\":1,\"652\":1,\"653\":1,\"654\":1,\"656\":3,\"657\":2,\"658\":3,\"659\":4,\"660\":7,\"666\":1}}],[\"clues\",{\"1\":{\"71\":1}}],[\"cls是一个二分类值\",{\"1\":{\"514\":1}}],[\"cls特征\",{\"1\":{\"147\":2}}],[\"cls2idx\",{\"1\":{\"68\":2}}],[\"cls\",{\"1\":{\"59\":2,\"68\":7,\"81\":3,\"93\":1,\"96\":2,\"126\":2,\"147\":3,\"154\":1,\"156\":1,\"160\":1,\"161\":3,\"162\":3,\"163\":3,\"262\":1,\"268\":2,\"285\":2,\"292\":22,\"293\":5,\"296\":5,\"374\":3,\"493\":1,\"506\":4,\"508\":6,\"511\":2,\"512\":5,\"513\":10,\"514\":4,\"520\":9,\"527\":1,\"538\":3,\"540\":2,\"542\":2,\"544\":1}}],[\"cl=语言嵌入维度\",{\"1\":{\"40\":1}}],[\"cl\",{\"1\":{\"40\":3,\"68\":2}}],[\"cloze和race提升明显\",{\"1\":{\"448\":1}}],[\"cloze\",{\"1\":{\"440\":1}}],[\"closed\",{\"1\":{\"462\":1,\"482\":1}}],[\"close\",{\"1\":{\"439\":1}}],[\"closing\",{\"1\":{\"206\":2}}],[\"clones\",{\"1\":{\"553\":1,\"554\":1,\"556\":1,\"557\":1,\"558\":1}}],[\"clone\",{\"1\":{\"59\":1,\"145\":4,\"147\":4,\"161\":2,\"163\":2,\"247\":1,\"284\":1,\"285\":1,\"477\":1,\"519\":1,\"546\":1}}],[\"cloud\",{\"1\":{\"29\":1,\"46\":3,\"83\":6,\"114\":1}}],[\"clock\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"cauchy\",{\"0\":{\"587\":1},\"1\":{\"587\":2}}],[\"causallmoutputwithcrossattentions\",{\"1\":{\"268\":1,\"285\":1}}],[\"causal\",{\"1\":{\"268\":1,\"285\":2,\"477\":1}}],[\"case\",{\"1\":{\"508\":1,\"519\":1,\"686\":2,\"687\":3}}],[\"casual\",{\"1\":{\"295\":1}}],[\"cache的实现细节\",{\"1\":{\"477\":1}}],[\"cache结构\",{\"1\":{\"477\":1}}],[\"cache用元组的形式进行返回\",{\"1\":{\"477\":1}}],[\"cache形式\",{\"1\":{\"477\":1}}],[\"cache后\",{\"1\":{\"477\":1}}],[\"cache进行推理\",{\"1\":{\"474\":1}}],[\"cache详解\",{\"1\":{\"473\":1}}],[\"cache\",{\"0\":{\"473\":1,\"474\":1,\"475\":1,\"476\":1},\"1\":{\"285\":4,\"474\":1,\"475\":1,\"476\":1,\"477\":41}}],[\"cache=use\",{\"1\":{\"285\":1,\"477\":2}}],[\"cache=none\",{\"1\":{\"285\":1,\"477\":1}}],[\"cache=true\",{\"1\":{\"282\":1,\"285\":2,\"477\":1}}],[\"camera\",{\"1\":{\"273\":1}}],[\"callbacks\",{\"1\":{\"683\":1}}],[\"calling\",{\"1\":{\"674\":1}}],[\"call\",{\"1\":{\"244\":1,\"367\":2,\"372\":2,\"377\":2,\"612\":1,\"613\":1,\"630\":2,\"634\":2,\"651\":1,\"656\":1,\"657\":1,\"658\":2,\"660\":2,\"674\":1}}],[\"capital\",{\"1\":{\"542\":2}}],[\"captions\",{\"1\":{\"131\":2,\"140\":2,\"143\":3,\"226\":1,\"286\":2}}],[\"caption\",{\"1\":{\"128\":1,\"142\":10,\"143\":2,\"145\":5,\"146\":2,\"147\":4,\"226\":3,\"278\":1,\"285\":1}}],[\"captioner模块初始化\",{\"1\":{\"142\":1}}],[\"captioner\",{\"0\":{\"141\":1},\"1\":{\"120\":1,\"123\":1,\"128\":1,\"132\":3,\"134\":1,\"138\":1,\"140\":1,\"142\":1,\"143\":1}}],[\"captioning\",{\"1\":{\"120\":1,\"128\":1,\"194\":1,\"285\":1}}],[\"capfilt\",{\"0\":{\"128\":1,\"132\":1,\"136\":1,\"140\":1},\"1\":{\"120\":1,\"122\":1,\"123\":1,\"125\":1,\"128\":2,\"129\":1,\"132\":1,\"134\":1,\"136\":3,\"137\":3,\"138\":1,\"140\":3,\"147\":1}}],[\"cardinality\",{\"1\":{\"78\":4}}],[\"carry\",{\"1\":{\"29\":1}}],[\"ca\",{\"1\":{\"40\":1,\"126\":2}}],[\"categorical\",{\"0\":{\"576\":1},\"1\":{\"576\":1}}],[\"category\",{\"1\":{\"82\":2,\"275\":4,\"277\":4}}],[\"catastrophic\",{\"1\":{\"416\":1}}],[\"cat\",{\"1\":{\"32\":4,\"34\":1,\"35\":2,\"36\":1,\"41\":1,\"43\":3,\"45\":1,\"46\":1,\"59\":4,\"70\":1,\"92\":2,\"96\":2,\"100\":1,\"109\":1,\"145\":9,\"147\":8,\"161\":2,\"162\":6,\"247\":1,\"273\":1,\"284\":6,\"285\":3,\"292\":1,\"293\":1,\"296\":1,\"474\":1,\"477\":1}}],[\"cand\",{\"1\":{\"511\":2,\"512\":3}}],[\"candidates\",{\"1\":{\"275\":13,\"277\":10,\"512\":3}}],[\"cannot\",{\"1\":{\"295\":1}}],[\"can\",{\"1\":{\"28\":4}}],[\"cup\",{\"1\":{\"273\":1}}],[\"current\",{\"1\":{\"59\":2,\"82\":3,\"275\":2,\"276\":1,\"277\":3}}],[\"curve\",{\"1\":{\"22\":1,\"28\":2,\"82\":3}}],[\"cut\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"cuda\",{\"1\":{\"28\":1,\"107\":2,\"108\":2,\"244\":1,\"247\":1,\"273\":2,\"275\":2,\"277\":2,\"338\":1}}],[\"c\",{\"1\":{\"6\":2,\"19\":1,\"30\":1,\"32\":2,\"35\":1,\"40\":1,\"41\":9,\"45\":16,\"46\":3,\"59\":25,\"70\":4,\"72\":2,\"73\":2,\"74\":2,\"75\":2,\"76\":12,\"92\":14,\"96\":5,\"98\":3,\"100\":7,\"101\":1,\"188\":2,\"191\":1,\"194\":2,\"244\":1,\"249\":1,\"255\":1,\"272\":1,\"280\":2,\"282\":1,\"291\":6,\"292\":3,\"293\":2,\"295\":3,\"296\":2,\"353\":2,\"355\":1,\"405\":1,\"470\":1,\"471\":1,\"477\":4,\"510\":2,\"517\":1,\"544\":1,\"565\":2,\"617\":2,\"632\":1,\"655\":2,\"657\":5,\"660\":17}}],[\"cos\",{\"1\":{\"665\":1}}],[\"cosine\",{\"1\":{\"272\":1,\"275\":2,\"276\":1,\"277\":3,\"447\":1}}],[\"cosineannealinglr\",{\"1\":{\"80\":1}}],[\"copies\",{\"1\":{\"495\":1}}],[\"copy\",{\"1\":{\"145\":1,\"147\":1,\"160\":1,\"246\":1,\"477\":1,\"667\":2}}],[\"coqa\",{\"1\":{\"455\":1}}],[\"core\",{\"1\":{\"554\":1,\"661\":9,\"684\":1,\"685\":1}}],[\"corpus\",{\"1\":{\"447\":1,\"448\":2}}],[\"correlations\",{\"1\":{\"460\":1}}],[\"corresponding\",{\"1\":{\"412\":1,\"527\":1}}],[\"correct\",{\"1\":{\"275\":3,\"277\":3,\"515\":2}}],[\"coefficient\",{\"1\":{\"401\":3,\"402\":2,\"576\":1}}],[\"covariance\",{\"1\":{\"590\":4}}],[\"cov\",{\"1\":{\"355\":1}}],[\"count\",{\"1\":{\"275\":7,\"277\":7,\"410\":2,\"412\":5,\"512\":3}}],[\"coffee\",{\"1\":{\"273\":1}}],[\"cogvlm\",{\"1\":{\"211\":1}}],[\"coyo\",{\"1\":{\"190\":1,\"217\":1}}],[\"coco数据集上零样本评估\",{\"1\":{\"178\":1}}],[\"coco\",{\"1\":{\"128\":2,\"131\":1,\"140\":1,\"142\":4,\"143\":1,\"145\":1,\"178\":1,\"194\":4}}],[\"cola上取得45\",{\"1\":{\"448\":1}}],[\"cola\",{\"1\":{\"448\":1}}],[\"column\",{\"1\":{\"322\":1,\"323\":2}}],[\"color=\",{\"1\":{\"667\":2}}],[\"color=lightblue\",{\"1\":{\"666\":3}}],[\"color=orange\",{\"1\":{\"666\":9}}],[\"colorjitter\",{\"1\":{\"244\":1}}],[\"colors\",{\"1\":{\"83\":6}}],[\"color\",{\"1\":{\"83\":9}}],[\"collect\",{\"1\":{\"412\":1}}],[\"collections\",{\"1\":{\"412\":1}}],[\"collection\",{\"0\":{\"17\":1},\"1\":{\"470\":1}}],[\"collate可以参考\",{\"1\":{\"289\":1}}],[\"collate\",{\"1\":{\"142\":1,\"145\":1,\"147\":1,\"289\":3,\"290\":4,\"522\":2}}],[\"collaborative\",{\"1\":{\"4\":1,\"5\":1}}],[\"coordinates\",{\"1\":{\"83\":6}}],[\"cond\",{\"1\":{\"662\":1}}],[\"conditionally\",{\"1\":{\"568\":1}}],[\"condaerror\",{\"1\":{\"339\":1}}],[\"conda\",{\"1\":{\"331\":4,\"332\":2,\"333\":1,\"334\":4,\"335\":3,\"336\":1,\"337\":1,\"338\":6,\"339\":4,\"519\":2,\"546\":2}}],[\"conda虚拟环境管理\",{\"0\":{\"330\":1},\"1\":{\"330\":1}}],[\"consistency的过程\",{\"1\":{\"435\":1}}],[\"consistency的例子\",{\"1\":{\"435\":1}}],[\"consistency的大致原理是这样\",{\"1\":{\"435\":1}}],[\"consistency技术是在cot技术的基础之上\",{\"1\":{\"435\":1}}],[\"consistency\",{\"0\":{\"435\":1},\"1\":{\"435\":1}}],[\"conquer\",{\"1\":{\"395\":1}}],[\"confidence\",{\"1\":{\"404\":1}}],[\"config函数\",{\"1\":{\"658\":1}}],[\"config=bert\",{\"1\":{\"160\":2}}],[\"config=decoder\",{\"1\":{\"147\":1}}],[\"config=encoder\",{\"1\":{\"147\":2}}],[\"config=\",{\"1\":{\"145\":1,\"147\":1}}],[\"config=med\",{\"1\":{\"142\":1,\"145\":2,\"146\":1}}],[\"configs\",{\"1\":{\"142\":1,\"145\":1,\"146\":1,\"147\":1}}],[\"config\",{\"1\":{\"28\":4,\"42\":2,\"44\":3,\"83\":1,\"142\":11,\"145\":13,\"146\":3,\"147\":11,\"159\":4,\"160\":13,\"163\":2,\"262\":3,\"263\":4,\"265\":5,\"268\":15,\"285\":2,\"477\":3,\"519\":2,\"523\":9,\"525\":16,\"526\":3,\"527\":3,\"528\":5,\"529\":7,\"531\":9,\"532\":5,\"533\":3,\"535\":8,\"536\":5,\"537\":3,\"538\":5,\"540\":1,\"541\":6,\"543\":7,\"544\":5,\"658\":8,\"661\":3}}],[\"confusion\",{\"0\":{\"342\":1}}],[\"connections\",{\"1\":{\"553\":1,\"556\":1}}],[\"connection\",{\"1\":{\"72\":1,\"74\":1,\"98\":1,\"100\":1,\"477\":2,\"548\":1,\"683\":1}}],[\"conclusion\",{\"0\":{\"138\":1,\"239\":1,\"259\":1},\"1\":{\"134\":1}}],[\"conceptual\",{\"1\":{\"131\":2,\"226\":1}}],[\"concise\",{\"1\":{\"63\":1}}],[\"concat\",{\"1\":{\"40\":2,\"43\":1,\"92\":1,\"96\":3,\"145\":2,\"249\":1,\"508\":1,\"558\":1}}],[\"contour\",{\"1\":{\"667\":1}}],[\"continue\",{\"1\":{\"511\":2}}],[\"continuous\",{\"1\":{\"419\":1}}],[\"contig\",{\"1\":{\"326\":4}}],[\"contiguity\",{\"1\":{\"321\":1}}],[\"contiguous\",{\"1\":{\"59\":2,\"111\":1,\"266\":1,\"268\":3,\"285\":3,\"326\":7,\"383\":1,\"384\":2,\"385\":2,\"477\":1,\"531\":1,\"558\":1}}],[\"contribution\",{\"1\":{\"253\":1}}],[\"contrast\",{\"0\":{\"234\":1,\"236\":1},\"1\":{\"232\":2}}],[\"contrastive\",{\"0\":{\"154\":1,\"283\":1},\"1\":{\"127\":1,\"145\":1,\"190\":1,\"240\":2,\"278\":1,\"283\":1}}],[\"content\",{\"1\":{\"60\":1}}],[\"contextmanager\",{\"1\":{\"658\":1}}],[\"contextlib\",{\"1\":{\"658\":2}}],[\"context设定的早期尝试\",{\"1\":{\"464\":1}}],[\"contextualized\",{\"1\":{\"542\":1}}],[\"contextual\",{\"1\":{\"63\":1,\"542\":1}}],[\"context\",{\"1\":{\"45\":2,\"59\":2,\"86\":2,\"266\":7,\"285\":10,\"460\":1,\"464\":1,\"465\":1,\"517\":3,\"527\":1,\"531\":9,\"540\":1}}],[\"containing\",{\"1\":{\"412\":2}}],[\"contain\",{\"1\":{\"29\":1,\"58\":1,\"68\":1}}],[\"convirt基于对比学习的方法\",{\"1\":{\"278\":1}}],[\"convbench评估显示\",{\"1\":{\"220\":1}}],[\"conv4\",{\"1\":{\"111\":2}}],[\"conv3\",{\"1\":{\"107\":2,\"109\":2,\"111\":2}}],[\"conv2\",{\"1\":{\"101\":2,\"107\":2,\"109\":2,\"111\":2}}],[\"conv2d\",{\"1\":{\"92\":2,\"96\":3,\"291\":1}}],[\"conv1\",{\"1\":{\"101\":2,\"107\":2,\"109\":2,\"111\":2}}],[\"conv1d\",{\"1\":{\"34\":1,\"35\":1,\"36\":1,\"41\":2,\"45\":2,\"59\":3,\"98\":2,\"100\":3,\"101\":2,\"107\":3,\"109\":4,\"111\":5,\"112\":1,\"477\":2}}],[\"conv\",{\"1\":{\"92\":2,\"96\":6,\"100\":2}}],[\"convs\",{\"1\":{\"92\":3,\"96\":3,\"100\":3}}],[\"conversation\",{\"1\":{\"227\":1,\"678\":1}}],[\"convert\",{\"1\":{\"29\":1,\"58\":1,\"142\":1,\"275\":1,\"277\":1,\"519\":1}}],[\"convention\",{\"1\":{\"76\":1}}],[\"convolution\",{\"1\":{\"76\":2,\"100\":1,\"252\":2}}],[\"cot的特点是同类型问题的迁移思考\",{\"1\":{\"436\":1}}],[\"cot的效果并不明显\",{\"1\":{\"434\":1}}],[\"cot是llm足够大\",{\"1\":{\"434\":1}}],[\"cot及其变体通过多步推理增强mllms能力\",{\"1\":{\"7\":1}}],[\"cot\",{\"1\":{\"7\":2,\"434\":1,\"676\":1}}],[\"coder\",{\"1\":{\"674\":2}}],[\"code=true\",{\"1\":{\"28\":2}}],[\"code\",{\"0\":{\"139\":1,\"158\":1,\"243\":1,\"244\":1},\"1\":{\"4\":1,\"244\":1,\"519\":7}}],[\"comboloss\",{\"1\":{\"407\":2}}],[\"combo\",{\"0\":{\"407\":1},\"1\":{\"407\":12}}],[\"complement\",{\"1\":{\"567\":1}}],[\"complex\",{\"1\":{\"227\":1,\"436\":1}}],[\"compatibility\",{\"1\":{\"528\":1}}],[\"comparison\",{\"0\":{\"23\":1}}],[\"compile\",{\"1\":{\"410\":1,\"412\":1}}],[\"composition\",{\"1\":{\"395\":1}}],[\"compose\",{\"1\":{\"244\":1,\"290\":2}}],[\"compute\",{\"1\":{\"244\":2,\"247\":3,\"558\":1}}],[\"community\",{\"1\":{\"685\":1}}],[\"comment\",{\"1\":{\"471\":1}}],[\"commercial\",{\"1\":{\"206\":2}}],[\"commoncrawl等网络数据隐含的社会偏见难以完全过滤\",{\"1\":{\"484\":1}}],[\"commoncrawl\",{\"1\":{\"480\":1,\"481\":1}}],[\"common\",{\"1\":{\"28\":2,\"59\":4,\"217\":1,\"454\":1,\"461\":1,\"482\":1,\"674\":1}}],[\"committer\",{\"1\":{\"2\":2}}],[\"com\",{\"1\":{\"4\":2,\"37\":1,\"47\":2,\"60\":2,\"85\":2,\"102\":2,\"119\":1,\"140\":2,\"148\":1,\"179\":1,\"223\":1,\"232\":1,\"252\":1,\"279\":1,\"289\":2,\"300\":3,\"406\":1,\"477\":1,\"503\":1,\"510\":2,\"519\":2,\"546\":1,\"603\":1,\"649\":1,\"664\":1,\"669\":1}}],[\"cv领域的信号是在一个连续而且高维的空间\",{\"1\":{\"238\":1}}],[\"cvpr\",{\"1\":{\"60\":1}}],[\"cvpr2024\",{\"1\":{\"60\":1}}],[\"cv\",{\"1\":{\"3\":1,\"239\":1,\"271\":1,\"674\":1}}]],\"version\":2}}")).map(([e,t])=>[e,jn(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:n,options:s,id:r}})=>{const o=An[n];e==="suggest"?self.postMessage([e,r,Et(t,o,s)]):e==="search"?self.postMessage([e,r,vt(t,o,s,"max")]):self.postMessage({suggestions:[e,r,Et(t,o,s)],results:[e,r,vt(t,o,s,__SLIMSEARCH_SORT_STRATEGY__)]})};
//# sourceMappingURL=index.js.map
