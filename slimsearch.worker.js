/**
* @vue/shared v3.5.13
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/const Se={},ze=()=>{},Ce=Object.assign,Oe=Array.isArray,D=e=>typeof e=="function",Me=e=>typeof e=="string",Ne=e=>typeof e=="symbol";let X;const L=()=>X||(X=typeof globalThis<"u"?globalThis:typeof self<"u"?self:typeof window<"u"?window:typeof global<"u"?global:{});/**
* @vue/reactivity v3.5.13
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/new Set(Object.getOwnPropertyNames(Symbol).filter(e=>e!=="arguments"&&e!=="caller").map(e=>Symbol[e]).filter(Ne));function P(e){const t=e&&e.__v_raw;return t?P(t):e}function Te(e){return e?e.__v_isRef===!0:!1}/**
* @vue/runtime-core v3.5.13
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/const v=[];function kt(e){v.push(e)}function It(){v.pop()}let W=!1;function Et(e,...t){if(W)return;W=!0;const n=v.length?v[v.length-1].component:null,o=n&&n.appContext.config.warnHandler,s=Fe();if(o)A(o,n,11,[e+t.map(r=>{var i,c;return(c=(i=r.toString)==null?void 0:i.call(r))!=null?c:JSON.stringify(r)}).join(""),n&&n.proxy,s.map(({vnode:r})=>`at <${re(n,r.type)}>`).join(`
`),s]);else{const r=[`[Vue warn]: ${e}`,...t];s.length&&r.push(`
`,...$e(s)),console.warn(...r)}W=!1}function Fe(){let e=v[v.length-1];if(!e)return[];const t=[];for(;e;){const n=t[0];n&&n.vnode===e?n.recurseCount++:t.push({vnode:e,recurseCount:0});const o=e.component&&e.component.parent;e=o&&o.vnode}return t}function $e(e){const t=[];return e.forEach((n,o)=>{t.push(...o===0?[]:[`
`],...Ve(n))}),t}function Ve({vnode:e,recurseCount:t}){const n=t>0?`... (${t} recursive calls)`:"",o=e.component?e.component.parent==null:!1,s=` at <${re(e.component,e.type,o)}`,r=">"+n;return e.props?[s,...Re(e.props),r]:[s+r]}function Re(e){const t=[],n=Object.keys(e);return n.slice(0,3).forEach(o=>{t.push(...Z(o,e[o]))}),n.length>3&&t.push(" ..."),t}function Z(e,t,n){return Me(t)?(t=JSON.stringify(t),n?t:[`${e}=${t}`]):typeof t=="number"||typeof t=="boolean"||t==null?n?t:[`${e}=${t}`]:Te(t)?(t=Z(e,P(t.value),!0),n?t:[`${e}=Ref<`,t,">"]):D(t)?[`${e}=fn${t.name?`<${t.name}>`:""}`]:(t=P(t),n?t:[`${e}=`,t])}const vt={sp:"serverPrefetch hook",bc:"beforeCreate hook",c:"created hook",bm:"beforeMount hook",m:"mounted hook",bu:"beforeUpdate hook",u:"updated",bum:"beforeUnmount hook",um:"unmounted hook",a:"activated hook",da:"deactivated hook",ec:"errorCaptured hook",rtc:"renderTracked hook",rtg:"renderTriggered hook",0:"setup function",1:"render function",2:"watcher getter",3:"watcher callback",4:"watcher cleanup function",5:"native event handler",6:"component event handler",7:"vnode hook",8:"directive hook",9:"transition hook",10:"app errorHandler",11:"app warnHandler",12:"ref function",13:"async component loader",14:"scheduler flush",15:"component update",16:"app unmount cleanup function"};function A(e,t,n,o){try{return o?e(...o):e()}catch(s){ee(s,t,n)}}function ee(e,t,n,o=!0){const s=t?t.vnode:null,{errorHandler:r,throwUnhandledErrorInProduction:i}=t&&t.appContext.config||Se;if(t){let c=t.parent;const l=t.proxy,u=`https://vuejs.org/error-reference/#runtime-${n}`;for(;c;){const a=c.ec;if(a){for(let h=0;h<a.length;h++)if(a[h](e,l,u)===!1)return}c=c.parent}if(r){A(r,null,10,[e,l,u]);return}}je(e,n,s,o,i)}function je(e,t,n,o=!0,s=!1){if(s)throw e;console.error(e)}const b=[];let x=-1;const S=[];let k=null,z=0;const De=Promise.resolve();let q=null;const Le=100;function Pe(e){let t=x+1,n=b.length;for(;t<n;){const o=t+n>>>1,s=b[o],r=M(s);r<e||r===e&&s.flags&2?t=o+1:n=o}return t}function We(e){if(!(e.flags&1)){const t=M(e),n=b[b.length-1];!n||!(e.flags&2)&&t>=M(n)?b.push(e):b.splice(Pe(t),0,e),e.flags|=1,te()}}function te(){q||(q=De.then(ne))}function Ae(e){Oe(e)?S.push(...e):k&&e.id===-1?k.splice(z+1,0,e):e.flags&1||(S.push(e),e.flags|=1),te()}function qe(e){if(S.length){const t=[...new Set(S)].sort((n,o)=>M(n)-M(o));if(S.length=0,k){k.push(...t);return}for(k=t,z=0;z<k.length;z++){const n=k[z];n.flags&4&&(n.flags&=-2),n.flags&8||n(),n.flags&=-2}k=null,z=0}}const M=e=>e.id==null?e.flags&2?-1:1/0:e.id;function ne(e){const t=ze;try{for(x=0;x<b.length;x++){const n=b[x];n&&!(n.flags&8)&&(n.flags&4&&(n.flags&=-2),A(n,n.i,n.i?15:14),n.flags&4||(n.flags&=-2))}}finally{for(;x<b.length;x++){const n=b[x];n&&(n.flags&=-2)}x=-1,b.length=0,qe(e),q=null,(b.length||S.length)&&ne(e)}}function St(e,t){const n=e.get(t)||0;if(n>Le){const o=t.i,s=o&&se(o.type);return ee(`Maximum recursive updates exceeded${s?` in component <${s}>`:""}. This means you have a reactive effect that is mutating its own dependencies and thus recursively triggering itself. Possible sources include component template, render function, updated hook or watcher source function.`,null,10),!0}return e.set(t,n+1),!1}const H=new Map,F=new Map;function zt(e,t){return F.has(e)?!1:(F.set(e,{initialDef:$(t),instances:new Set}),!0)}function $(e){return Je(e)?e.__vccOpts:e}function Ct(e,t){const n=F.get(e);n&&(n.initialDef.render=t,[...n.instances].forEach(o=>{t&&(o.render=t,$(o.type).render=t),o.renderCache=[],o.update()}))}function Ot(e,t){const n=F.get(e);if(!n)return;t=$(t),oe(n.initialDef,t);const o=[...n.instances];for(let s=0;s<o.length;s++){const r=o[s],i=$(r.type);let c=H.get(i);c||(i!==n.initialDef&&oe(i,t),H.set(i,c=new Set)),c.add(r),r.appContext.propsCache.delete(r.type),r.appContext.emitsCache.delete(r.type),r.appContext.optionsCache.delete(r.type),r.ceReload?(c.add(r),r.ceReload(t.styles),c.delete(r)):r.parent?We(()=>{r.parent.update(),c.delete(r)}):r.appContext.reload?r.appContext.reload():typeof window<"u"?window.location.reload():console.warn("[HMR] Root or manually mounted instance modified. Full reload required."),r.root.ce&&r!==r.root&&r.root.ce._removeChildStyle(i)}Ae(()=>{H.clear()})}function oe(e,t){Ce(e,t);for(const n in e)n!=="__file"&&!(n in t)&&delete e[n]}function Mt(e){return(t,n)=>{try{return e(t,n)}catch(o){console.error(o),console.warn("[HMR] Something went wrong during Vue component hot-reload. Full reload required.")}}}L().requestIdleCallback,L().cancelIdleCallback;const Nt={};{const e=L(),t=(n,o)=>{let s;return(s=e[n])||(s=e[n]=[]),s.push(o),r=>{s.length>1?s.forEach(i=>i(r)):s[0](r)}};t("__VUE_INSTANCE_SETTERS__",n=>n),t("__VUE_SSR_SETTERS__",n=>n)}const He=/(?:^|[-_])(\w)/g,Ue=e=>e.replace(He,t=>t.toUpperCase()).replace(/[-_]/g,"");function se(e,t=!0){return D(e)?e.displayName||e.name:e.name||t&&e.__name}function re(e,t,n=!1){let o=se(t);if(!o&&t.__file){const s=t.__file.match(/([^/\\]+)\.\w+$/);s&&(o=s[1])}if(!o&&e&&e.parent){const s=r=>{for(const i in r)if(r[i]===t)return i};o=s(e.components||e.parent.type.components)||s(e.appContext.components)}return o?Ue(o):n?"App":"Anonymous"}function Je(e){return D(e)&&"__vccOpts"in e}[...new Array(6)].map((e,t)=>`[vp-content] h${t+1}`).join(",");const{entries:Ge}=Object,{fromEntries:Be}=Object,Ye="ENTRIES",ie="KEYS",ce="VALUES",y="";class U{set;_type;_path;constructor(t,n){const o=t._tree,s=Array.from(o.keys());this.set=t,this._type=n,this._path=s.length>0?[{node:o,keys:s}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:n}=C(this._path);if(C(n)===y)return{done:!1,value:this.result()};const o=t.get(C(n));return this._path.push({node:o,keys:Array.from(o.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=C(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>C(t)).filter(t=>t!==y).join("")}value(){return C(this._path).node.get(y)}result(){switch(this._type){case ce:return this.value();case ie:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const C=e=>e[e.length-1],Ke=(e,t,n)=>{const o=new Map;if(typeof t!="string")return o;const s=t.length+1,r=s+n,i=new Uint8Array(r*s).fill(n+1);for(let c=0;c<s;++c)i[c]=c;for(let c=1;c<r;++c)i[c*s]=c;return le(e,t,n,o,i,1,s,""),o},le=(e,t,n,o,s,r,i,c)=>{const l=r*i;e:for(const u of e.keys())if(u===y){const a=s[l-1];a<=n&&o.set(c,[e.get(u),a])}else{let a=r;for(let h=0;h<u.length;++h,++a){const g=u[h],m=i*a,w=m-i;let d=s[m];const f=Math.max(0,a-n-1),p=Math.min(i-1,a+n);for(let _=f;_<p;++_){const I=g!==t[_],j=s[w+_]+ +I,T=s[w+_+1]+1,E=s[m+_]+1,O=s[m+_+1]=Math.min(j,T,E);O<d&&(d=O)}if(d>n)continue e}le(e.get(u),t,n,o,s,a,i,c+u)}};let ue=class N{_tree;_prefix;_size=void 0;constructor(t=new Map,n=""){this._tree=t,this._prefix=n}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[n,o]=V(this._tree,t.slice(this._prefix.length));if(n===void 0){const[s,r]=B(o);for(const i of s.keys())if(i!==y&&i.startsWith(r)){const c=new Map;return c.set(i.slice(r.length),s.get(i)),new N(c,t)}}return new N(n,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,Qe(this._tree,t)}entries(){return new U(this,Ye)}forEach(t){for(const[n,o]of this)t(n,o,this)}fuzzyGet(t,n){return Ke(this._tree,t,n)}get(t){const n=J(this._tree,t);return n!==void 0?n.get(y):void 0}has(t){return J(this._tree,t)?.has(y)??!1}keys(){return new U(this,ie)}set(t,n){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,G(this._tree,t).set(y,n),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,n){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const o=G(this._tree,t);return o.set(y,n(o.get(y))),this}fetch(t,n){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const o=G(this._tree,t);let s=o.get(y);return s===void 0&&o.set(y,s=n()),s}values(){return new U(this,ce)}[Symbol.iterator](){return this.entries()}static from(t){const n=new N;for(const[o,s]of t)n.set(o,s);return n}static fromObject(t){return N.from(Object.entries(t))}};const V=(e,t,n=[])=>{if(t.length===0||e==null)return[e,n];for(const o of e.keys())if(o!==y&&t.startsWith(o))return n.push([e,o]),V(e.get(o),t.slice(o.length),n);return n.push([e,t]),V(void 0,"",n)},J=(e,t)=>{if(t.length===0||!e)return e;for(const n of e.keys())if(n!==y&&t.startsWith(n))return J(e.get(n),t.slice(n.length))},G=(e,t)=>{const n=t.length;e:for(let o=0;e&&o<n;){for(const r of e.keys())if(r!==y&&t[o]===r[0]){const i=Math.min(n-o,r.length);let c=1;for(;c<i&&t[o+c]===r[c];)++c;const l=e.get(r);if(c===r.length)e=l;else{const u=new Map;u.set(r.slice(c),l),e.set(t.slice(o,o+c),u),e.delete(r),e=u}o+=c;continue e}const s=new Map;return e.set(t.slice(o),s),s}return e},Qe=(e,t)=>{const[n,o]=V(e,t);if(n!==void 0){if(n.delete(y),n.size===0)ae(o);else if(n.size===1){const[s,r]=n.entries().next().value;fe(o,s,r)}}},ae=e=>{if(e.length===0)return;const[t,n]=B(e);if(t.delete(n),t.size===0)ae(e.slice(0,-1));else if(t.size===1){const[o,s]=t.entries().next().value;o!==y&&fe(e.slice(0,-1),o,s)}},fe=(e,t,n)=>{if(e.length===0)return;const[o,s]=B(e);o.set(s+t,n),o.delete(s)},B=e=>e[e.length-1],Xe=(e,t)=>{const n=e._idToShortId.get(t);if(n!=null)return e._storedFields.get(n)},Ze=/[\n\r\p{Z}\p{P}]+/u,Y="or",de="and",et="and_not",tt=(e,t)=>{e.includes(t)||e.push(t)},he=(e,t)=>{for(const n of t)e.includes(n)||e.push(n)},pe=({score:e},{score:t})=>t-e,nt=()=>new Map,R=e=>{const t=new Map;for(const n of Object.keys(e))t.set(parseInt(n,10),e[n]);return t},ge=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,me={[Y]:(e,t)=>{for(const n of t.keys()){const o=e.get(n);if(o==null)e.set(n,t.get(n));else{const{score:s,terms:r,match:i}=t.get(n);o.score=o.score+s,o.match=Object.assign(o.match,i),he(o.terms,r)}}return e},[de]:(e,t)=>{const n=new Map;for(const o of t.keys()){const s=e.get(o);if(s==null)continue;const{score:r,terms:i,match:c}=t.get(o);he(s.terms,i),n.set(o,{score:s.score+r,terms:s.terms,match:Object.assign(s.match,c)})}return n},[et]:(e,t)=>{for(const n of t.keys())e.delete(n);return e}},ot=(e,t,n,o,s,r)=>{const{k:i,b:c,d:l}=r;return Math.log(1+(n-t+.5)/(t+.5))*(l+e*(i+1)/(e+i*(1-c+c*o/s)))},st=e=>(t,n,o)=>({term:t,fuzzy:typeof e.fuzzy=="function"?e.fuzzy(t,n,o):e.fuzzy??!1,prefix:typeof e.prefix=="function"?e.prefix(t,n,o):e.prefix===!0,termBoost:typeof e.boostTerm=="function"?e.boostTerm(t,n,o):1}),_e=(e,t,n,o)=>{for(const s of Object.keys(e._fieldIds))if(e._fieldIds[s]===n){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${o}" was not present in field "${s}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},rt=(e,t,n,o)=>{if(!e._index.has(o)){_e(e,n,t,o);return}const s=e._index.fetch(o,nt),r=s.get(t),i=r?.get(n);!r||typeof i>"u"?_e(e,n,t,o):i<=1?r.size<=1?s.delete(t):r.delete(n):r.set(n,i-1),e._index.get(o).size===0&&e._index.delete(o)},it={k:1.2,b:.7,d:.5},ct={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(Ze),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{console?.[e]?.(t)},autoVacuum:!0},ye={combineWith:Y,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:it},lt={combineWith:de,prefix:(e,t,n)=>t===n.length-1},ut={batchSize:1e3,batchWait:10},we={minDirtFactor:.1,minDirtCount:20},at={...ut,...we},be=Symbol("*"),ft=(e,t)=>{const n=new Map,o={...e._options.searchOptions,...t};for(const[s,r]of e._documentIds){const i=o.boostDocument?o.boostDocument(r,"",e._storedFields.get(s)):1;n.set(s,{score:i,terms:[],match:{}})}return n},xe=(e,t=Y)=>{if(e.length===0)return new Map;const n=t.toLowerCase();if(!(n in me))throw new Error(`Invalid combination operator: ${t}`);return e.reduce(me[n])},K=(e,t,n,o,s,r,i,c,l,u=new Map)=>{if(r==null)return u;for(const a of Object.keys(i)){const h=i[a],g=e._fieldIds[a],m=r.get(g);if(m==null)continue;let w=m.size;const d=e._avgFieldLength[g];for(const f of m.keys()){if(!e._documentIds.has(f)){rt(e,g,f,n),w-=1;continue}const p=c?c(e._documentIds.get(f),n,e._storedFields.get(f)):1;if(!p)continue;const _=m.get(f),I=e._fieldLength.get(f)[g],j=ot(_,w,e._documentCount,I,d,l),T=o*s*h*p*j,E=u.get(f);if(E){E.score+=T,tt(E.terms,t);const O=ge(E.match,n);O?O.push(a):E.match[n]=[a]}else u.set(f,{score:T,terms:[t],match:{[n]:[a]}})}}return u},dt=(e,t,n)=>{const o={...e._options.searchOptions,...n},s=(o.fields??e._options.fields).reduce((d,f)=>({...d,[f]:ge(o.boost,f)||1}),{}),{boostDocument:r,weights:i,maxFuzzy:c,bm25:l}=o,{fuzzy:u,prefix:a}={...ye.weights,...i},h=e._index.get(t.term),g=K(e,t.term,t.term,1,t.termBoost,h,s,r,l);let m,w;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const d=t.fuzzy===!0?.2:t.fuzzy,f=d<1?Math.min(c,Math.round(t.term.length*d)):d;f&&(w=e._index.fuzzyGet(t.term,f))}if(m)for(const[d,f]of m){const p=d.length-t.term.length;if(!p)continue;w?.delete(d);const _=a*d.length/(d.length+.3*p);K(e,t.term,d,_,t.termBoost,f,s,r,l,g)}if(w)for(const d of w.keys()){const[f,p]=w.get(d);if(!p)continue;const _=u*d.length/(d.length+p);K(e,t.term,d,_,t.termBoost,f,s,r,l,g)}return g},ke=(e,t,n={})=>{if(t===be)return ft(e,n);if(typeof t!="string"){const a={...n,...t,queries:void 0},h=t.queries.map(g=>ke(e,g,a));return xe(h,a.combineWith)}const{tokenize:o,processTerm:s,searchOptions:r}=e._options,i={tokenize:o,processTerm:s,...r,...n},{tokenize:c,processTerm:l}=i,u=c(t).flatMap(a=>l(a)).filter(a=>!!a).map(st(i)).map(a=>dt(e,a,i));return xe(u,i.combineWith)},Ie=(e,t,n={})=>{const{searchOptions:o}=e._options,s={...o,...n},r=ke(e,t,n),i=[];for(const[c,{score:l,terms:u,match:a}]of r){const h=u.length||1,g={id:e._documentIds.get(c),score:l*h,terms:Object.keys(a),queryTerms:u,match:a};Object.assign(g,e._storedFields.get(c)),(s.filter==null||s.filter(g))&&i.push(g)}return t===be&&s.boostDocument==null||i.sort(pe),i},ht=(e,t,n={})=>{n={...e._options.autoSuggestOptions,...n};const o=new Map;for(const{score:r,terms:i}of Ie(e,t,n)){const c=i.join(" "),l=o.get(c);l!=null?(l.score+=r,l.count+=1):o.set(c,{score:r,terms:i,count:1})}const s=[];for(const[r,{score:i,terms:c,count:l}]of o)s.push({suggestion:r,terms:c,score:i/l});return s.sort(pe),s};class pt{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(!t?.fields)throw new Error('SlimSearch: option "fields" must be provided');const n=t.autoVacuum==null||t.autoVacuum===!0?at:t.autoVacuum;this._options={...ct,...t,autoVacuum:n,searchOptions:{...ye,...t.searchOptions},autoSuggestOptions:{...lt,...t.autoSuggestOptions}},this._index=new ue,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=we,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[n,o]of this._index){const s={};for(const[r,i]of o)s[r]=Object.fromEntries(i);t.push([n,s])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,version:2}}addFields(t){for(let n=0;n<t.length;n++)this._fieldIds[t[n]]=n}}const gt=e=>new pt(e),mt=({documentCount:e,nextId:t,fieldIds:n,averageFieldLength:o,dirtCount:s,version:r},i)=>{if(r!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const c=gt(i);return c._documentCount=e,c._nextId=t,c._idToShortId=new Map,c._fieldIds=n,c._avgFieldLength=o,c._dirtCount=s??0,c._index=new ue,c},_t=(e,t)=>{const{index:n,documentIds:o,fieldLength:s,storedFields:r}=e,i=mt(e,t);i._documentIds=R(o),i._fieldLength=R(s),i._storedFields=R(r);for(const[c,l]of i._documentIds)i._idToShortId.set(l,c);for(const[c,l]of n){const u=new Map;for(const a of Object.keys(l))u.set(parseInt(a,10),R(l[a]));i._index.set(c,u)}return i},Q=(e,t)=>{const n=e.toLowerCase(),o=t.toLowerCase(),s=[];let r=0,i=0;const c=(u,a=!1)=>{let h;i===0?h=u.length>20?`… ${u.slice(-20)}`:u:a?h=u.length+i>100?`${u.slice(0,100-i)}… `:u:h=u.length>20?`${u.slice(0,20)} … ${u.slice(-20)}`:u,h&&s.push(h),i+=h.length,a||(s.push(["mark",t]),i+=t.length,i>=100&&s.push(" …"))};let l=n.indexOf(o,r);if(l===-1)return null;for(;l>=0;){const u=l+o.length;if(c(e.slice(r,l)),r=u,i>100)break;l=n.indexOf(o,r)}return i<100&&c(e.slice(r),!0),s},{entries:yt}=Object,wt=(e,t)=>t.contents.reduce((n,[,o])=>n+o,0)-e.contents.reduce((n,[,o])=>n+o,0),bt=(e,t)=>Math.max(...t.contents.map(([,n])=>n))-Math.max(...e.contents.map(([,n])=>n)),Ee=(e,t,n={},o="max")=>{const s={};return Ie(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...n}).forEach(r=>{const{id:i,terms:c,score:l}=r,u=i.includes("@"),a=i.includes("#"),[h,g]=i.split(/[#@]/),m=Number(h),w=c.sort((f,p)=>f.length-p.length).filter((f,p)=>c.slice(p+1).every(_=>!_.includes(f))),{contents:d}=s[m]??={title:"",contents:[]};if(u)d.push([{type:"customField",id:m,index:g,display:w.map(f=>r.c.map(p=>Q(p,f))).flat().filter(f=>f!==null)},l]);else{const f=w.map(p=>Q(r.h,p)).filter(p=>p!==null);if(f.length&&d.push([{type:a?"heading":"title",id:m,...a&&{anchor:g},display:f},l]),"t"in r&&r.t)for(const p of r.t){const _=w.map(I=>Q(p,I)).filter(I=>I!==null);_.length&&d.push([{type:"text",id:m,...a&&{anchor:g},display:_},l])}}}),yt(s).sort(([,r],[,i])=>(o?wt:bt)(r,i)).map(([r,{title:i,contents:c}])=>{if(!i){const l=Xe(t,r);l&&(i=l.h)}return{title:i,contents:c.map(([l])=>l)}})},ve=(e,t,n={})=>{const o=ht(t,e,{fuzzy:.2,maxFuzzy:3,...n}).map(({suggestion:s})=>s);return e.includes(" ")?o:o.filter(s=>!s.includes(" "))},xt=Be(Ge(JSON.parse("{\"/\":{\"documentCount\":77,\"nextId\":77,\"documentIds\":{\"0\":\"0\",\"1\":\"1\",\"2\":\"1#binary-oracle\",\"3\":\"1#elowen\",\"4\":\"2\",\"5\":\"3\",\"6\":\"4\",\"7\":\"4#环境搭建\",\"8\":\"4#数据预处理\",\"9\":\"4#模型架构\",\"10\":\"4#dataloader\",\"11\":\"4#bertembeddings\",\"12\":\"4#bertencoder\",\"13\":\"4#bertlayer\",\"14\":\"4#bertencoder-1\",\"15\":\"4#bertpooler\",\"16\":\"4#bertmodel\",\"17\":\"4#bertforsequenceclassification\",\"18\":\"4#bertattention\",\"19\":\"4#bertselfattention\",\"20\":\"4#bertselfoutput\",\"21\":\"4#bertattention-1\",\"22\":\"4#预训练\",\"23\":\"4#bertpredictionheadtransform\",\"24\":\"4#bertlmpredictionhead\",\"25\":\"4#bertpretrainingheads\",\"26\":\"4#bertforpretraining\",\"27\":\"4#其他下游任务\",\"28\":\"4#问答任务\",\"29\":\"4#代码实现\",\"30\":\"4#易混淆\",\"31\":\"4#token分类任务\",\"32\":\"4#多项选择任务\",\"33\":\"5\",\"34\":\"5#环境\",\"35\":\"5#背景\",\"36\":\"5#模型架构\",\"37\":\"5#encoder-decoder-结构\",\"38\":\"5#generator\",\"39\":\"5#encoder-结构\",\"40\":\"5#sublayerconnection\",\"41\":\"5#encoderlayer\",\"42\":\"5#encoder\",\"43\":\"5#decoder-结构\",\"44\":\"5#decoderlayer\",\"45\":\"5#decoder\",\"46\":\"5#多头自注意力\",\"47\":\"6\",\"48\":\"7\",\"49\":\"7#引言\",\"50\":\"7#介绍\",\"51\":\"7#训练\",\"52\":\"7#推理\",\"53\":\"7#文本描述生成\",\"54\":\"7#花卉图片分类\",\"55\":\"7#文字搜索图像\",\"56\":\"7#完整代码\",\"57\":\"7#小结\",\"58\":\"8\",\"59\":\"9\",\"60\":\"9#原理\",\"61\":\"9#_0-数据下载\",\"62\":\"9#_1-图片预处理\",\"63\":\"9#_2-图片切割\",\"64\":\"9#_3-添加-class-token\",\"65\":\"9#_4-添加位置编码\",\"66\":\"9#_5-encoder\",\"67\":\"9#_6-多头自注意力\",\"68\":\"9#_7-mlp-head\",\"69\":\"9#效果对比\",\"70\":\"9#注意力可视化\",\"71\":\"9#混合模型探索\",\"72\":\"9#加载预训练模型\",\"73\":\"9#总结\",\"74\":\"10\",\"75\":\"11\",\"76\":\"12\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1,7],\"1\":[1],\"2\":[2,13],\"3\":[1,5],\"4\":[3],\"5\":[1],\"6\":[2,2],\"7\":[1,139],\"8\":[1,165],\"9\":[1],\"10\":[1,48],\"11\":[1,66],\"12\":[1],\"13\":[1,58],\"14\":[1,30],\"15\":[1,40],\"16\":[1,49],\"17\":[1,73],\"18\":[1],\"19\":[1,104],\"20\":[1,32],\"21\":[1,23],\"22\":[1,1],\"23\":[1,41],\"24\":[1,44],\"25\":[1,28],\"26\":[1,75],\"27\":[1,1],\"28\":[1,120],\"29\":[1,95],\"30\":[1,173],\"31\":[1,75],\"32\":[1,119],\"33\":[1,2],\"34\":[1,41],\"35\":[1,23],\"36\":[1,64],\"37\":[3,30],\"38\":[1,28],\"39\":[2],\"40\":[1,33],\"41\":[1,37],\"42\":[1,37],\"43\":[2],\"44\":[1,49],\"45\":[1,39],\"46\":[1,116],\"47\":[1],\"48\":[1,3],\"49\":[1,31],\"50\":[1,20],\"51\":[1,146],\"52\":[1,156],\"53\":[1,57],\"54\":[1,245],\"55\":[1,92],\"56\":[1,210],\"57\":[1,132],\"58\":[1,1],\"59\":[1,53],\"60\":[1,4],\"61\":[2,234],\"62\":[2,130],\"63\":[2,181],\"64\":[4,160],\"65\":[2,141],\"66\":[2,153],\"67\":[2,138],\"68\":[3,220],\"69\":[1,61],\"70\":[1,13],\"71\":[1,51],\"72\":[1,158],\"73\":[1,15],\"74\":[1],\"75\":[1],\"76\":[1,3]},\"averageFieldLength\":[1.2467532467532474,69.7858716550783],\"storedFields\":{\"0\":{\"h\":\"主页\",\"t\":[\"知识星球: MetaMind , 小红书: BinaryOracle , CSDN: Binary Oracle\"]},\"1\":{\"h\":\"关于我们\"},\"2\":{\"h\":\"Binary Oracle\",\"t\":[\"一名普通但十分热爱探索技术的Coder\",\"开源框架 Spring committer\",\"Golang 开源网络库 netpoll committer\",\"Javaer 转型 3D - VL 方向研究\",\"现就读于四川大学\"]},\"3\":{\"h\":\"Elowen\",\"t\":[\"CV 转 LLM 领域\",\"现就读于电子科技大学\"]},\"4\":{\"h\":\"3D-Vision Language\"},\"5\":{\"h\":\"大语言模型\"},\"6\":{\"h\":\"图解 Bert\",\"t\":[\"图解Bert & Bert文本分类实战\"]},\"7\":{\"h\":\"环境搭建\",\"t\":[\"按序执行以下命令完成环境搭建:\",\"git clone https://github.com/DA-southampton/Read_Bert_Code.git cd Read_Bert_Code conda create -n Read_Bert_Code python=3.9.22 conda activate Read_Bert_Code\",\"本文使用的是谷歌的中文预训练模型：chinese_L-12_H-768_A-12.zip，模型有点大，我就不上传了，如果本地不存在，就点击这里直接下载,或者直接命令行运行\",\"wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\",\"预训练模型下载下来之后，进行解压，然后将tf模型转为对应的pytorch版本即可。对应代码如下:\",\"export BERT_BASE_DIR=/Users/zhandaohong/Read_Bert_Code/chinese_L-12_H-768_A-12 python convert_tf_checkpoint_to_pytorch.py \\\\ --tf_checkpoint_path $BERT_BASE_DIR/bert_model.ckpt \\\\ --bert_config_file $BERT_BASE_DIR/bert_config.json \\\\ --pytorch_dump_path $BERT_BASE_DIR/pytorch_model.bin\",\"转化成功之后，将模型放入到仓库对应位置：\",\"Read_Bert_Code/bert_read_step_to_step/prev_trained_model/\",\"并重新命名为：\",\" bert-base-chinese\",\"其次是准备训练数据，这里我准备做一个文本分类任务，使用的是Tnews数据集，这个数据集来源是这里，分为训练，测试和开发集，我已经上传到了仓库中，具体位置在\",\"Read_Bert_Code/bert_read_step_to_step/chineseGLUEdatasets/tnews\",\"需要注意的一点是，因为我只是为了了解内部代码情况，所以准确度不是在我的考虑范围之内，所以我只是取其中的一部分数据，其中训练数据使用1k，测试数据使用1k，开发数据1k。\",\"准备就绪，使用pycharm导入项目，准备调试，我的调试文件是run_classifier.py文件，对应的参数为\",\"--model_type=bert --model_name_or_path=prev_trained_model/bert-base-chinese --task_name=\\\"tnews\\\" --do_train --do_eval --do_lower_case --data_dir=./chineseGLUEdatasets/tnews --max_seq_length=128 --per_gpu_train_batch_size=16 --per_gpu_eval_batch_size=16 --learning_rate=2e-5 --num_train_epochs=4.0 --logging_steps=100 --save_steps=100 --output_dir=./outputs/tnews_output/ --overwrite_output_dir\",\"然后启动 run_classifier.py 文件进行调试即可 , 所参考源仓库未提供requirements.txt文件，因此需要大家自行完成运行时缺失依赖包的安装。\"]},\"8\":{\"h\":\"数据预处理\",\"t\":[\"输入数据格式\",\"{ \\\"guid\\\": \\\"train-0\\\", \\\"label\\\": \\\"104\\\", // 文本分类任务: 文本对应的标签 \\\"text_a\\\": \\\"股票中的突破形态\\\", \\\"text_b\\\": null // NSP任务: 用于判断给出的两个句子是否连续 }\",\"NSP (Next Sentence Prediction)\",\"文本分词 & 借助字典映射为word id\",\"\\\"股票中的突破形态\\\" --> ['股', '票', '中', '的', '突', '破', '形', '态'] --> [5500, 4873, 704, 4638, 4960, 4788, 2501, 2578]\",\"对于字典中不存在的词 , 用 [UNK] 表示, 对应的id为 100\",\"过长截断策略\",\"添加特殊Token标记\",\"原序列添加特殊Token标记图\",\"[101, 5500, 4873, 704, 4638, 4960, 4788, 2501, 2578, 102]\",\"BertTokenizer中的特殊token id:\",\"[CLS]: 101\",\"[SEP]: 102\",\"[MASK]: 103\",\"[UNK]: 100\",\"[PAD]: 0\",\" # BertTokenizer def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None): if token_ids_1 is None: return [self.cls_token_id] + token_ids_0 + [self.sep_token_id] cls = [self.cls_token_id] sep = [self.sep_token_id] return cls + token_ids_0 + sep + token_ids_1 + sep\",\"创建句子辨识列表，用以区分不同的句子\",\"token_type_ids作用图解\",\" # BertTokenizer def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None): \\\"\\\"\\\" Creates a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence pair mask has the following format: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 | first sequence | second sequence if token_ids_1 is None, only returns the first portion of the mask (0's). \\\"\\\"\\\" sep = [self.sep_token_id] cls = [self.cls_token_id] if token_ids_1 is None: return len(cls + token_ids_0 + sep) * [0] return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\",\"创建用以区分special tokens部分的mask列表\",\"special_tokens_mask作用图解\",\" # BertTokenizer def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False): if token_ids_1 is not None: return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1] return [1] + ([0] * len(token_ids_0)) + [1]\",\"超长截断\",\" # PreTrainedTokenizer if max_length and len(encoded_inputs[\\\"input_ids\\\"]) > max_length: encoded_inputs[\\\"input_ids\\\"] = encoded_inputs[\\\"input_ids\\\"][:max_length] encoded_inputs[\\\"token_type_ids\\\"] = encoded_inputs[\\\"token_type_ids\\\"][:max_length] encoded_inputs[\\\"special_tokens_mask\\\"] = encoded_inputs[\\\"special_tokens_mask\\\"][:max_length]\",\"生成padding部分的mask列表\",\"attention_mask作用图解\",\" # 生成注意力掩码，真实token对应1，填充token对应0 attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\",\"所有序列都填充到max_length长度,不足长度用padding填充\",\"填充过程图\",\" # 记录输入长度 input_len = len(input_ids) # 计算需要填充的长度 --- 所有输入序列等长，都等于max_length padding_length = max_length - len(input_ids) # 右填充 input_ids = input_ids + ([pad_token] * padding_length) attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length) token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\",\"数据集中每一个样本最终都会解析得到一个InputFeatures\",\"InputFeatures组成图解\",\"features.append( InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=label, input_len=input_len))\",\"label 是当前文本对应的类别标签 input_len 是序列实际长度(含special tokens)\",\"数据集预处理完后，将InputFeatures List列表组装起来得到需要的DataSet\",\"dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_lens,all_labels)\"]},\"9\":{\"h\":\"模型架构\"},\"10\":{\"h\":\"DataLoader\",\"t\":[\" train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset) train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size,collate_fn=collate_fn)\",\"DataLoader 设置的回调方法cllote_fn负责对返回的一个batch，在返回前进行预处理:\",\"def collate_fn(batch): all_input_ids, all_attention_mask, all_token_type_ids, all_lens, all_labels = map(torch.stack, zip(*batch)) max_len = max(all_lens).item() # 计算当前批次中所有序列的实际最大长度 all_input_ids = all_input_ids[:, :max_len] # 按照本批次序列中最大长度进行截断: max_length --> max_len all_attention_mask = all_attention_mask[:, :max_len] all_token_type_ids = all_token_type_ids[:, :max_len] return all_input_ids, all_attention_mask, all_token_type_ids, all_labels\"]},\"11\":{\"h\":\"BertEmbeddings\",\"t\":[\"input embeddings = token embeddings + segmentation embeddings + position embeddings\",\"class BertEmbeddings(nn.Module): def __init__(self, config): super(BertEmbeddings, self).__init__() self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0) self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size) self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, input_ids, token_type_ids=None, position_ids=None): seq_length = input_ids.size(1) if position_ids is None: # 为当前批次中的每个序列样本生成一个位置序列: (1,2,3,4,5,...) , 构成一个位置序列矩阵 position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) position_ids = position_ids.unsqueeze(0).expand_as(input_ids) if token_type_ids is None: token_type_ids = torch.zeros_like(input_ids) words_embeddings = self.word_embeddings(input_ids) position_embeddings = self.position_embeddings(position_ids) # 位置编码为可学习的矩阵 token_type_embeddings = self.token_type_embeddings(token_type_ids) # 让模型自己学会区分不同的句子 embeddings = words_embeddings + position_embeddings + token_type_embeddings embeddings = self.LayerNorm(embeddings) embeddings = self.dropout(embeddings) return embeddings\",\"嵌入向量生成过程图\"]},\"12\":{\"h\":\"BertEncoder\"},\"13\":{\"h\":\"BertLayer\",\"t\":[\"BertLayer模型结构图\",\"class BertIntermediate(nn.Module): def __init__(self, config): super(BertIntermediate, self).__init__() self.dense = nn.Linear(config.hidden_size, config.intermediate_size) # (768,3072) # 激活函数 - GLEU if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)): self.intermediate_act_fn = ACT2FN[config.hidden_act] else: self.intermediate_act_fn = config.hidden_act def forward(self, hidden_states): hidden_states = self.dense(hidden_states) hidden_states = self.intermediate_act_fn(hidden_states) # 激活函数 - GLEU return hidden_states class BertOutput(nn.Module): def __init__(self, config): super(BertOutput, self).__init__() self.dense = nn.Linear(config.intermediate_size, config.hidden_size) # (3072,768) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, hidden_states, input_tensor): hidden_states = self.dense(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.LayerNorm(hidden_states + input_tensor) return hidden_states class BertLayer(nn.Module): def __init__(self, config): super(BertLayer, self).__init__() self.attention = BertAttention(config) self.intermediate = BertIntermediate(config) self.output = BertOutput(config) def forward(self, hidden_states, attention_mask=None): attention_output = self.attention(hidden_states, attention_mask) intermediate_output = self.intermediate(attention_output) layer_output = self.output(intermediate_output, attention_output) return layer_output\"]},\"14\":{\"h\":\"BertEncoder\",\"t\":[\"BertEncoder模型结构图\",\"class BertEncoder(nn.Module): def __init__(self, config): super(BertEncoder, self).__init__() self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)]) def forward(self, hidden_states, attention_mask=None, head_mask=None): for i, layer_module in enumerate(self.layer): hidden_states = layer_module(hidden_states, attention_mask, head_mask[i]) return hidden_states\"]},\"15\":{\"h\":\"BertPooler\",\"t\":[\"BertPooler模型结构图\",\"class BertPooler(nn.Module): def __init__(self, config): super(BertPooler, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) self.activation = nn.Tanh() def forward(self, hidden_states): # We \\\"pool\\\" the model by simply taking the hidden state corresponding # to the first token. first_token_tensor = hidden_states[:, 0] # CLS Token Context Embeddings pooled_output = self.dense(first_token_tensor) pooled_output = self.activation(pooled_output) return pooled_output\"]},\"16\":{\"h\":\"BertModel\",\"t\":[\"BertModel模型结构图\",\"class BertModel(BertPreTrainedModel): def __init__(self, config): super(BertModel, self).__init__(config) self.embeddings = BertEmbeddings(config) self.encoder = BertEncoder(config) self.pooler = BertPooler(config) self.init_weights() def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None): extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2) extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0 embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids) sequence_output = self.encoder(embedding_output, extended_attention_mask, # padding mask ) pooled_output = self.pooler(sequence_output) outputs = (sequence_output, pooled_output,) return outputs\"]},\"17\":{\"h\":\"BertForSequenceClassification\",\"t\":[\"BertForSequenceClassification模型结构图\",\"class BertForSequenceClassification(BertPreTrainedModel): def __init__(self, config): super(BertForSequenceClassification, self).__init__(config) self.num_labels = config.num_labels self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, self.config.num_labels) self.init_weights() def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None): outputs = self.bert(input_ids, attention_mask=attention_mask, # padding mask token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) # None ? pooled_output = outputs[1] # 对于分类任务来说，只需要去除CLS Token用于分类任务即可 pooled_output = self.dropout(pooled_output) logits = self.classifier(pooled_output) outputs = (logits,) + outputs[2:] # add hidden states and attention if they are here if labels is not None: if self.num_labels == 1: # We are doing regression loss_fct = MSELoss() loss = loss_fct(logits.view(-1), labels.view(-1)) else: loss_fct = CrossEntropyLoss() loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) outputs = (loss,) + outputs return outputs # (loss), logits, (hidden_states), (attentions)\"]},\"18\":{\"h\":\"BertAttention\"},\"19\":{\"h\":\"BertSelfAttention\",\"t\":[\"多头自注意力计算流程图\",\"class BertSelfAttention(nn.Module): def __init__(self, config): super(BertSelfAttention, self).__init__() self.output_attentions = config.output_attentions self.num_attention_heads = config.num_attention_heads self.attention_head_size = int(config.hidden_size / config.num_attention_heads) self.all_head_size = self.num_attention_heads * self.attention_head_size self.query = nn.Linear(config.hidden_size, self.all_head_size) self.key = nn.Linear(config.hidden_size, self.all_head_size) self.value = nn.Linear(config.hidden_size, self.all_head_size) self.dropout = nn.Dropout(config.attention_probs_dropout_prob) def transpose_for_scores(self, x): new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size) x = x.view(*new_x_shape) return x.permute(0, 2, 1, 3) def forward(self, hidden_states, attention_mask=None, head_mask=None): mixed_query_layer = self.query(hidden_states) mixed_key_layer = self.key(hidden_states) mixed_value_layer = self.value(hidden_states) # view 成多头格式: (batch,heads,seq_len,d_k) query_layer = self.transpose_for_scores(mixed_query_layer) key_layer = self.transpose_for_scores(mixed_key_layer) value_layer = self.transpose_for_scores(mixed_value_layer) # Take the dot product between \\\"query\\\" and \\\"key\\\" to get the raw attention scores. attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) # (batch,heads,d_k,seq_len) attention_scores = attention_scores / math.sqrt(self.attention_head_size) if attention_mask is not None: # Apply the attention mask is (precomputed for all layers in BertModel forward() function) attention_scores = attention_scores + attention_mask # Normalize the attention scores to probabilities. attention_probs = nn.Softmax(dim=-1)(attention_scores) # This is actually dropping out entire tokens to attend to, which might # seem a bit unusual, but is taken from the original Transformer paper. attention_probs = self.dropout(attention_probs) context_layer = torch.matmul(attention_probs, value_layer) context_layer = context_layer.permute(0, 2, 1, 3).contiguous() new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) context_layer = context_layer.view(*new_context_layer_shape) # 合并头结果 return context_layer\"]},\"20\":{\"h\":\"BertSelfOutput\",\"t\":[\"BertSelfOutput计算流程图\",\"class BertSelfOutput(nn.Module): def __init__(self, config): super(BertSelfOutput, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.dropout = nn.Dropout(config.hidden_dropout_prob) # 残差链接 + 层归一化 def forward(self, hidden_states, input_tensor): hidden_states = self.dense(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.LayerNorm(hidden_states + input_tensor) return hidden_states\"]},\"21\":{\"h\":\"BertAttention\",\"t\":[\"BertAttention计算流程图\",\"class BertAttention(nn.Module): def __init__(self, config): super(BertAttention, self).__init__() self.self = BertSelfAttention(config) self.output = BertSelfOutput(config) def forward(self, input_tensor, attention_mask=None): self_outputs = self.self(input_tensor, attention_mask) # 多头自注意力机制 attention_output = self.output(self_outputs, input_tensor) return attention_output\"]},\"22\":{\"h\":\"预训练\",\"t\":[\"预训练与微调\"]},\"23\":{\"h\":\"BertPredictionHeadTransform\",\"t\":[\"BertPredictionHeadTransform结构图\",\"class BertPredictionHeadTransform(nn.Module): def __init__(self, config): super(BertPredictionHeadTransform, self).__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)): self.transform_act_fn = ACT2FN[config.hidden_act] else: self.transform_act_fn = config.hidden_act self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps) def forward(self, hidden_states): hidden_states = self.dense(hidden_states) hidden_states = self.transform_act_fn(hidden_states) hidden_states = self.LayerNorm(hidden_states) return hidden_states\"]},\"24\":{\"h\":\"BertLMPredictionHead\",\"t\":[\"BertLMPredictionHead结构图\",\"class BertLMPredictionHead(nn.Module): def __init__(self, config): super(BertLMPredictionHead, self).__init__() self.transform = BertPredictionHeadTransform(config) # The output weights are the same as the input embeddings, but there is # an output-only bias for each token. self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False) self.bias = nn.Parameter(torch.zeros(config.vocab_size)) def forward(self, hidden_states): hidden_states = self.transform(hidden_states) hidden_states = self.decoder(hidden_states) + self.bias return hidden_states\"]},\"25\":{\"h\":\"BertPreTrainingHeads\",\"t\":[\"BertPreTrainingHeads结构图\",\"class BertPreTrainingHeads(nn.Module): def __init__(self, config): super(BertPreTrainingHeads, self).__init__() self.predictions = BertLMPredictionHead(config) self.seq_relationship = nn.Linear(config.hidden_size, 2) def forward(self, sequence_output, pooled_output): prediction_scores = self.predictions(sequence_output) # seq_relationship_score = self.seq_relationship(pooled_output) # 两个句子是否为上下句关系 return prediction_scores, seq_relationship_score\"]},\"26\":{\"h\":\"BertForPreTraining\",\"t\":[\"BertForPreTraining结构图\",\"class BertForPreTraining(BertPreTrainedModel): def __init__(self, config): super(BertForPreTraining, self).__init__(config) self.bert = BertModel(config) self.cls = BertPreTrainingHeads(config) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, masked_lm_labels=None, next_sentence_label=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) sequence_output, pooled_output = outputs[:2] # 隐藏层输出,CLS Token Embeddings prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output) outputs = (prediction_scores, seq_relationship_score,) # 计算掩码语言损失 和 下一个句子预测损失 if masked_lm_labels is not None and next_sentence_label is not None: loss_fct = CrossEntropyLoss(ignore_index=-1) masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1)) next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1)) total_loss = masked_lm_loss + next_sentence_loss outputs = (total_loss,) + outputs return outputs # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)\"]},\"27\":{\"h\":\"其他下游任务\",\"t\":[\"Bert支持的下游任务图\"]},\"28\":{\"h\":\"问答任务\",\"t\":[\"在 BERT 的问答任务中，典型的输入是一个包含 问题（Question） 和 上下文（Context） 的文本对。例如：\",\"问题: “谁写了《哈姆雷特》？”上下文: “莎士比亚是英国文学史上最伟大的作家之一，他写了包括《哈姆雷特》、《麦克白》等著名悲剧。”\",\"输入格式（Tokenization 后的形式），在使用 BertTokenizer 编码后，输入会变成如下结构：\",\"[CLS] 问题 tokens [SEP] 上下文 tokens [SEP]\",\"BERT 的输出（Outputs），通过调用 self.bert(...)，你将得到一个包含多个元素的 tuple 输出：\",\"outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\",\"返回值形如：\",\"( sequence_output, # (batch_size, seq_length, hidden_size) pooled_output, # (batch_size, hidden_size) )\",\"主要输出项解释:\",\"✅ sequence_output: 最终每个 token 的表示\",\"形状：(batch_size, seq_length, hidden_size)\",\"是模型最后一层所有 token（包括问题和上下文）的隐藏状态。\",\"在问答任务中，我们主要使用它来预测答案的起始和结束位置。\",\"✅ pooled_output: 句子级别表示（不常用）\",\"形状：(batch_size, hidden_size)\",\"是 [CLS] token 经过一层全连接后的输出。\",\"在分类任务中更有用，在问答任务中一般不会使用这个输出。\",\"如何利用 BERT 输出做问答预测？\",\"在 BertForQuestionAnswering 中，使用了如下逻辑：\",\"logits = self.qa_outputs(sequence_output) # (batch_size, seq_length, 2) start_logits, end_logits = logits.split(1, dim=-1) # split into start and end start_logits = start_logits.squeeze(-1) # (batch_size, seq_length) end_logits = end_logits.squeeze(-1)\",\"qa_outputs 层的作用：\",\"是一个线性层：nn.Linear(config.hidden_size, 2)\",\"将每个 token 的 hidden_size 向量映射成两个分数：一个是该 token 作为答案开始的可能性，另一个是作为答案结束的可能性。\",\"输出解释：\",\"start_logits: 每个 token 是答案起点的得分（未归一化）。\",\"end_logits: 每个 token 是答案终点的得分。\",\"比如对于一个长度为 128 的序列，每个 token 都有一个对应的 start/end 分数：\",\"start_scores = torch.softmax(start_logits, dim=-1) # softmax 得到概率 end_scores = torch.softmax(end_logits, dim=-1) # 找出最可能是 start 和 end 的位置 start_index = torch.argmax(start_scores) end_index = torch.argmax(end_scores)\",\"如果 start_index <= end_index，那么可以组合这两个索引得到答案 span。\"]},\"29\":{\"h\":\"代码实现\",\"t\":[\"class BertForQuestionAnswering(BertPreTrainedModel): def __init__(self, config): super(BertForQuestionAnswering, self).__init__(config) self.num_labels = config.num_labels # 通常是 2，即 start 和 end self.bert = BertModel(config) self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, start_positions=None, end_positions=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids) sequence_output = outputs[0] # (batch,seq_len,hidden_size) ---> (batch,seq_len,2) logits = self.qa_outputs(sequence_output) start_logits, end_logits = logits.split(1, dim=-1) start_logits = start_logits.squeeze(-1) # (batch,seq_len) end_logits = end_logits.squeeze(-1) outputs = (start_logits, end_logits,) # 计算交叉熵损失 if start_positions is not None and end_positions is not None: # sometimes the start/end positions are outside our model inputs, we ignore these terms # ignored_index = seq_len ignored_index = start_logits.size(1) # clamp_ 是 PyTorch 中的一个方法，用于将张量中的值限制在指定的范围内。 # 它的语法是 tensor.clamp_(min, max) ，表示将张量中的值限制在 min 和 max 之间。 # 如果值小于 min ，则将其设置为 min ；如果值大于 max ，则将其设置为 max 。 start_positions.clamp_(0, ignored_index) end_positions.clamp_(0, ignored_index) # ignore_index: 用于指定在计算损失时忽略的标签索引。 loss_fct = CrossEntropyLoss(ignore_index=ignored_index) # 分别计算答案起始下标和结束下标预测得到的交叉熵损失 start_loss = loss_fct(start_logits, start_positions) end_loss = loss_fct(end_logits, end_positions) total_loss = (start_loss + end_loss) / 2 outputs = (total_loss,) + outputs return outputs # (loss), start_logits, end_logits\"]},\"30\":{\"h\":\"易混淆\",\"t\":[\"BERT 是一个 基于上下文编码（Contextual Encoder） 的模型，不是自回归生成器。它不会“生成”新的文本，而是对输入文本中每个 token 的角色进行分类（如判断哪个是答案的开始、结束）。所以最终的答案只能来自原始输入文本中的某一段子串。\",\"📚 详细解释\",\"✅ BERT 是一个 Encoder-only 模型\",\"BERT 只包含 Transformer 的 encoder 部分。\",\"它的作用是给定一个完整的句子（或两个句子），对每个 token 生成一个上下文相关的表示（contextualized representation）。\",\"它不具有生成能力，不能像 GPT 这样的 decoder-only 模型那样逐词生成新内容。\",\"🔍 QA 任务的本质：定位答案 span 而非生成答案\",\"在 SQuAD 这类抽取式问答任务中：\",\"答案必须是原文中的连续片段（span）。\",\"所以模型的任务是：\",\"给出问题和上下文；\",\"在上下文中找到最可能的答案起始位置和结束位置；\",\"最终答案就是上下文中这两个位置之间的字符串。\",\"BERT 做的就是这个定位任务，而不是重新生成一个新的答案。\",\"🧩 输入与输出的关系\",\"answer_tokens = input_ids[0][start_index : end_index + 1] answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\",\"这段代码的意思是：\",\"start_index 和 end_index 是模型预测出的答案的起始和结束位置。\",\"我们从原始输入的 input_ids 中取出对应的 token ID 子序列。\",\"使用 tokenizer 把这些 token ID 解码成自然语言文本。\",\"得到的就是答案。\",\"这其实就是在说：\",\"“根据你的理解，答案应该在这段文字中的第 X 到第 Y 个词之间，请把这部分原文告诉我。”\",\"🧪 举个例子\",\"假设原始上下文是：\",\"The capital of France is Paris.\",\"经过 Tokenizer 编码后可能是：\",\"[CLS] the capital of france is paris [SEP]\",\"如果模型预测 start_index=5，end_index=5，那么对应的就是单词 \\\"paris\\\"，这就是答案。\",\"⚠️ 注意事项\",\"不能超出上下文范围\",\"start/end positions 必须落在上下文部分（即 token_type_id == 1 的区域）。\",\"否则答案可能不合理（比如取到了问题部分的内容）。\",\"特殊 token 不计入答案\",\"[CLS], [SEP] 等会被 skip_special_tokens=True 自动跳过。\",\"无法处理不在原文中的答案\",\"如果正确答案没有出现在上下文中，BERT 无法“编造”出来。\",\"这是抽取式问答模型的局限性。\",\"💡 对比：生成式 vs 抽取式问答\",\"类型\",\"模型代表\",\"是否能生成新文本\",\"答案是否必须在原文中\",\"示例\",\"抽取式\",\"BERT\",\"❌\",\"✅\",\"答案是原文中的一段\",\"生成式\",\"T5 / BART / GPT\",\"✅\",\"❌\",\"答案可以是任意文本\",\"如果你希望模型能“自己写答案”，那就需要使用生成式模型。\",\"✅ 总结\",\"问题\",\"回答\",\"为什么答案来自 input_ids？\",\"因为 BERT 是编码器模型，只做抽取式问答，答案必须是原文中的一段文本。\",\"BERT 能不能自己生成答案？\",\"不能，BERT 不具备生成能力，只能对输入文本中的 token 做分类。\",\"如何获取答案？\",\"根据预测的 start/end index，从 input_ids 中提取 token，并用 tokenizer 解码成自然语言。\"]},\"31\":{\"h\":\"Token分类任务\",\"t\":[\"Token 分类任务是指对输入文本中的每个 token 进行分类，常见的应用场景包括：\",\"命名实体识别 (NER)\",\"词性标注 (POS)\",\"语义角色标注 (SRL)\",\"class BertForTokenClassification(BertPreTrainedModel): def __init__(self, config): super(BertForTokenClassification, self).__init__(config) self.num_labels = config.num_labels self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, config.num_labels) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) sequence_output = outputs[0] # (batch,seq_len,hidden_size) sequence_output = self.dropout(sequence_output) logits = self.classifier(sequence_output) # （batch,seq_len,num_labels） outputs = (logits,) if labels is not None: loss_fct = CrossEntropyLoss() # Only keep active parts of the loss if attention_mask is not None: active_loss = attention_mask.view(-1) == 1 active_logits = logits.view(-1, self.num_labels)[active_loss] active_labels = labels.view(-1)[active_loss] loss = loss_fct(active_logits, active_labels) else: loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) outputs = (loss,) + outputs return outputs # (loss), scores\"]},\"32\":{\"h\":\"多项选择任务\",\"t\":[\"多项选择任务是指给定一个问题和多个候选答案，模型需要从中选择最合适的答案。常见的应用场景包括：\",\"阅读理解任务\",\"问答系统中的候选答案选择\",\"对话系统中的候选回复选择\",\"在 多项选择题（Multiple Choice） 任务中，BERT 的输入组织形式与普通分类或问答任务略有不同。你需要为每个选项分别构造一个完整的 BERT 输入序列，并将它们组合成一个批次进行处理。\",\"✅ 假设你有一个问题 + 4 个选项：\",\"问题：谁写了《哈姆雷特》？ A. 雨果 B. 歌德 C. 莎士比亚 D. 托尔斯泰\",\"对于这样的多选问题，BERT 的输入方式是：\",\"对每一个选项，都单独构造一个 [CLS] + 问题 + [SEP] + 选项内容 + [SEP] 的输入序列。\",\"也就是说，模型会对每个选项分别编码 ，然后从中选出最合适的那个。\",\"class BertForMultipleChoice(BertPreTrainedModel): def __init__(self, config): super(BertForMultipleChoice, self).__init__(config) self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, 1) def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None): # 获取选项个数 num_choices = input_ids.shape[1] # (batch_size, num_choices, seq_length) # 将选项展平，以便一起处理: (batch_size * num_choices, seq_length) input_ids = input_ids.view(-1, input_ids.size(-1)) attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask) pooled_output = outputs[1] # (batch_size * num_choices, hidden_size) pooled_output = self.dropout(pooled_output) logits = self.classifier(pooled_output) # (batch_size * num_choices, 1) reshaped_logits = logits.view(-1, num_choices) # (batch_size , num_choices, 1) outputs = (reshaped_logits,) if labels is not None: loss_fct = CrossEntropyLoss() loss = loss_fct(reshaped_logits, labels) outputs = (loss,) + outputs return outputs # (loss), reshaped_logits, (hidden_states), (attentions)\",\"在前向传播中，会将这些输入展平，变成：\",\"input_ids.view(-1, seq_length) # (batch_size * num_choices, seq_length)\",\"这样就能让 BERT 对每个选项分别进行编码。\",\"BERT 输出后，再对每个选项做分类打分，最后重新 reshape 成 (batch_size, num_choices) 形式，用于计算交叉熵损失。\"]},\"33\":{\"h\":\"图解Transformer\",\"t\":[\"图解Transformer & 机器翻译实战\"]},\"34\":{\"h\":\"环境\",\"t\":[\"本文基于 The Annotated Transformer 所提供的代码展开进行讲解。\",\"环境搭建遵从如下步骤即可:\",\"git clone https://github.com/harvardnlp/annotated-transformer cd annotated-transformer conda create -n annotated-transformer python=3.9.22 conda activate annotated-transformer pip install -r requirements.txt\",\"MacOS 用户本地运行时，需要将 requirements.txt 文件中的 torch == 1.11.0+cu113 改为 torch==1.11.0，因为CUDA不支持MacOS。\"]},\"35\":{\"h\":\"背景\",\"t\":[\"RNN等模型的缺点是需要顺序计算，从而很难并行。因此出现了Extended Neural GPU、ByteNet和ConvS2S等网络模型。这些模型都是以CNN为基础，这比较容易并行。但是和RNN相比，它较难学习到长距离的依赖关系。\",\"本文的Transformer使用了Self-Attention机制，它在编码每一词的时候都能够注意(attend to)整个句子，从而可以解决长距离依赖的问题，同时计算Self-Attention可以用矩阵乘法一次计算所有的时刻，因此可以充分利用计算资源(CPU/GPU上的矩阵运算都是充分优化和高度并行的)。\"]},\"36\":{\"h\":\"模型架构\",\"t\":[\"Transformer 模型架构图\",\"Transformer 是一种基于自注意力机制(Self-Attention) 的神经网络架构,其由七大主要部分构成:\",\"Encoder-Decoder 结构\",\"编码器(Encoder)：将输入序列（如句子）转换为一系列高维向量表示。\",\"解码器(Decoder)：根据编码器的输出生成目标序列（如翻译后的句子）。\",\"多头自注意力机制（Multi-Head Self-Attention）\",\"自注意力机制是 Transformer 的核心，它允许模型在处理每个词时关注输入序列中的所有词。\",\"多头自注意力机制通过并行计算多个注意力头，捕捉不同子空间的信息，从而增强模型的表达能力。\",\"位置编码（Positional Encoding）\",\"由于 Transformer 不使用传统的循环或卷积结构，它通过位置编码将序列中词的位置信息注入到输入中。位置编码通常使用正弦和余弦函数生成。\",\"前馈神经网络（Feed-Forward Neural Network）\",\"在自注意力机制之后，每个位置的输出会通过一个独立的前馈神经网络进行进一步处理。\",\"残差连接与层归一化（Residual Connection & Layer Normalization）\",\"每个子层（如自注意力层和前馈层）都使用了残差连接和层归一化，以加速训练并提高模型的稳定性。\",\"掩码机制（Masking）\",\"在解码器中，使用掩码机制确保模型在生成目标序列时只能看到当前及之前的词，而不能看到未来的词。\",\"在输入序列长度不一致时，通过填充掩码（Padding Mask）屏蔽填充部分的信息。\",\"输出层\",\"解码器的最终输出通过一个线性层和 Softmax 函数生成目标序列的概率分布。\"]},\"37\":{\"h\":\"Encoder-Decoder 结构\",\"t\":[\"EncoderDecoder模型结构图\",\"class EncoderDecoder(nn.Module): def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): \\\"Take in and process masked src and target sequences.\\\" return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) def encode(self, src, src_mask): return self.encoder(self.src_embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\"]},\"38\":{\"h\":\"Generator\",\"t\":[\"Generator模型结构图\",\"class Generator(nn.Module): # 根据Decoder的隐状态输出一个词 # d_model是Decoder输出的大小，vocab是词典大小 def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) # 全连接再加上一个softmax def forward(self, x): return F.log_softmax(self.proj(x), dim=-1)\"]},\"39\":{\"h\":\"Encoder 结构\"},\"40\":{\"h\":\"SublayerConnection\",\"t\":[\"SublayerConnection模型结构图\",\"class SublayerConnection(nn.Module): \\\"\\\"\\\" LayerNorm + sublayer(Self-Attenion/Dense) + dropout + 残差连接 为了简单，把LayerNorm放到了前面，这和原始论文稍有不同，原始论文LayerNorm在最后。 \\\"\\\"\\\" def __init__(self, size, dropout): super(SublayerConnection, self).__init__() self.norm = LayerNorm(size) self.dropout = nn.Dropout(dropout) def forward(self, x, sublayer): \\\"sublayer是传入的参数，参考DecoderLayer，它可以当成函数调用，这个函数的有一个输入参数\\\" return x + self.dropout(sublayer(self.norm(x)))\"]},\"41\":{\"h\":\"EncoderLayer\",\"t\":[\"EncoderLayer模型结构图\",\"# 编码器层 = 自注意力子层 + 前馈层 class EncoderLayer(nn.Module): def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward # 自注意力子层 和 前馈层 各需要一个 self.sublayer = clones(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): \\\"Follow Figure 1 (left) for connections.\\\" x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) return self.sublayer[1](x, self.feed_forward)\"]},\"42\":{\"h\":\"Encoder\",\"t\":[\"Encoder模型结构图\",\"class Encoder(nn.Module): \\\"Core encoder is a stack of N layers\\\" def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): \\\"Pass the input (and mask) through each layer in turn.\\\" for layer in self.layers: x = layer(x, mask) return self.norm(x)\"]},\"43\":{\"h\":\"Decoder 结构\"},\"44\":{\"h\":\"DecoderLayer\",\"t\":[\"Decoder模型结构图\",\"# 解码器层 = 自注意力子层 + 源注意力子层 + 前馈层 class DecoderLayer(nn.Module): \\\"Decoder is made of self-attn, src-attn, and feed forward (defined below)\\\" def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward # 自注意力子层 + 源注意力子层 + 前馈层 各需要一个 self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): \\\"Follow Figure 1 (right) for connections.\\\" m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward)\"]},\"45\":{\"h\":\"Decoder\",\"t\":[\"Decoder模型结构图\",\"# 解码器 = N个解码器层 + 层归一化 class Decoder(nn.Module): \\\"Generic N layer decoder with masking.\\\" def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): # 输入,编码器隐藏层输出,源掩码,目标掩码 for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x)\"]},\"46\":{\"h\":\"多头自注意力\",\"t\":[\"多头自注意力计算流程图\",\"class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): \\\"Take in model size and number of heads.\\\" super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # We assume d_v always equals d_k self.d_k = d_model // h # 每个头64维 self.h = h # 8个头 self.linears = clones(nn.Linear(d_model, d_model), 4) # W_q,W_k,W_v,W_projection self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): \\\"Implements Figure 2\\\" if mask is not None: # Same mask applied to all h heads. mask = mask.unsqueeze(1) nbatches = query.size(0) # 1) Do all the linear projections in batch from d_model => h x d_k query, key, value = [ lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) # (batches,heads,seq_len,d_k) for lin, x in zip(self.linears, (query, key, value)) ] # 2) Apply attention on all the projected vectors in batch. x, self.attn = attention( query, key, value, mask=mask, dropout=self.dropout ) # 3) \\\"Concat\\\" using a view and apply a final linear. x = ( x.transpose(1, 2) .contiguous() .view(nbatches, -1, self.h * self.d_k) ) del query del key del value return self.linears[-1](x)\",\"def attention(query, key, value, mask=None, dropout=None): \\\"Compute 'Scaled Dot Product Attention'\\\" d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) # 广播: (1,1,1,10) ---> (1,8,10,10) p_attn = scores.softmax(dim=-1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn\"]},\"47\":{\"h\":\"多模态\"},\"48\":{\"h\":\"庖丁解牛CLIP\",\"t\":[\"多模态模型CLIP原理与图片分类，文字搜索图像实战演练\",\"CLIP原始论文链接\"]},\"49\":{\"h\":\"引言\",\"t\":[\"2021 年可谓是视觉 Transformer（Vision Transformer）大放异彩的一年。自谷歌提出 ViT 之后，众多基于视觉 Transformer 的研究如潮水般涌来，广泛应用于各类计算机视觉任务。与此同时，OpenAI 在 2021 年 1 月发布的 DALL-E 和 CLIP，同样给计算机视觉领域带来了巨大影响。这两个模型都属于融合图像与文本的多模态模型，其中 DALL-E 是基于文本输入来生成图像的模型，而 CLIP 则是以文本作为监督信号，训练出具有可迁移能力的视觉模型。和 ViT 类似，DALL-E 和 CLIP 的出现也掀起了新一轮的研究热潮。\"]},\"50\":{\"h\":\"介绍\",\"t\":[\"CLIP的英文全称为Contrastive Language-Image Pre-training，它代表着一种基于对比文本-图像对的预训练方法，同时也指运用该方法构建的模型。CLIP属于基于对比学习的多模态模型。与计算机视觉（CV）领域中的一些对比学习方法，像MoCo和SimCLR有所不同，CLIP的训练数据采用的是文本-图像对，也就是一张图像搭配与之对应的文本描述。在训练过程中，借助对比学习机制，期望模型能够学习到文本和图像之间的匹配关系。\"]},\"51\":{\"h\":\"训练\",\"t\":[\"CLIP包含两个核心模型，分别是文本编码器（Text Encoder）和图像编码器（Image Encoder）。其中，文本编码器的作用是提取文本的特征，在实现时可采用自然语言处理（NLP）领域常用的文本Transformer模型；而图像编码器则用于提取图像的特征，在实际应用中可以选用常见的卷积神经网络（CNN）模型，也可以采用视觉Transformer模型。\",\"这里对提取的文本特征和图像特征进行对比学习。对于一个包含个文本-图像对的训练batch，将个文本特征和个图像特征两两组合，CLIP模型会预测出个可能的文本-图像对的相似度，这里的相似度直接计算文本特征和图像特征的余弦相似性（cosine similarity），即上图所示的矩阵。这里共有个正样本，即真正属于一对的文本和图像（矩阵中的对角线元素），而剩余的个文本-图像对为负样本，那么CLIP的训练目标就是最大个正样本的相似度，同时最小化个负样本的相似度，对应的伪代码实现如下所示：\",\"# image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter # 分别提取图像特征和文本特征 I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # 对两个特征进行线性投射，得到相同维度的特征，并进行l2归一化 I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # 计算缩放的余弦相似度：[n, n] logits = np.dot(I_e, T_e.T) * np.exp(t) # 对称的对比学习损失：等价于N个类别的cross_entropy_loss labels = np.arange(n) # 对角线元素的labels loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2\",\"为了训练CLIP模型，OpenAI从网络上收集了总计4亿对文本和图像，这些数据在论文中被称为WebImageText。若以文本单词数量来衡量，其规模与GPT-2训练时使用的WebText数据集相似。然而，从数据对的数量来看，它比谷歌的JFT-300M数据集还要多出1亿对，因此这是一个非常庞大的数据集。\",\"尽管CLIP是一个多模态模型，但其主要目的是训练可迁移的视觉模型。在论文中，文本编码器（Text Encoder）选择了一个包含6300万参数的Transformer模型，而图像编码器（Image Encoder）则采用了两种不同的架构：\",\"一种是常用的CNN架构ResNet。\",\"另一种是基于 Transformer 的ViT。\",\"ResNet包含五种不同尺寸的模型：ResNet50、ResNet101、RN50x4、RN50x16和RNx64（后三种模型是按照EfficientNet的缩放规则对ResNet分别放大4倍、16倍和64倍得到的），而ViT则选择了三种不同尺寸的模型：ViT-B/32、ViT-B/16和ViT-L/14。\",\"所有模型均训练了32个周期，使用AdamW优化器，并且在训练过程中采用了一个相对较大的批次大小：32768。由于数据量巨大，最大的ResNet模型RN50x64需要在592个V100 GPU上训练18天，而最大的ViT模型ViT-L/14则需要在256个V100 GPU上训练12天，这表明训练CLIP模型需要消耗大量的资源。对于ViT-L/14模型，还在336的分辨率下额外进行了一个周期的微调（finetune）以增强性能，论文发现这个模型的效果最佳，并将其标记为ViT-L/14@336，论文中进行对比实验的CLIP模型也采用了这一配置。\"]},\"52\":{\"h\":\"推理\",\"t\":[\"我们已经探讨了CLIP模型的运作机制，它由两个部分组成：一个视觉模型和一个文本模型。那么，如何将这个预训练的视觉模型应用到新的任务中呢？CLIP模型的一个显著优势是它能够进行zero-shot图像分类，这意味着它能够在没有任何特定任务训练数据的情况下，直接对图像进行分类。这不仅展示了CLIP的强大功能，也是其一大亮点。实现zero-shot分类的过程相当直接，可以概括为以下两个主要步骤：\",\"构建描述文本并提取特征：首先，根据任务的分类需求，为每个类别创建一个描述性的文本，例如“A photo of {label}”。这些文本随后被输入到文本编码器（Text Encoder）中，以生成相应的文本特征。如果有个类别，那么就会得到个文本特征。\",\"图像特征提取与分类：接下来，将待分类的图像输入到图像编码器（Image Encoder）中，以获取图像特征。然后，这些图像特征会与之前得到的个文本特征进行余弦相似度计算（这一过程与训练时相同）。最终，选择与图像特征相似度最高的文本所对应的类别，作为图像的分类预测结果。此外，这些相似度值可以被视为logits，通过softmax函数转换后，可以得到每个类别的预测概率。\",\"通过这种方式，CLIP模型能够在没有特定任务训练数据的情况下，直接对图像进行分类，这展示了其在图像分类任务中的灵活性和强大能力。\",\" 显然，我们通过利用CLIP模型的多模态能力，为特定任务动态构建了一个分类器。在这个过程中，文本编码器（Text Encoder）生成的文本特征相当于分类器的权重，而图像编码器（Image Encoder）提取的图像特征则是分类器的输入数据。以下是一个官方给出的CLIP模型的示例 ，该示例中的任务涉及8个类别:\",\"我们首先创建了各类别的文本描述，然后提取了相应的文本特征；\",\"然后我们读取要预测的图像，输入Image Encoder提取图像特征，并计算与文本特征的余弦相似度。\",\"# 1. 提取文本特征 texts = [ \\\"a page of text about segmentation\\\", \\\"a facial photo of a tabby cat\\\", \\\"a portrait of an astronaut with the American flag\\\", \\\"a rocket standing on a launchpad\\\", \\\"a red motorcycle standing in a garage\\\", \\\"a person looking at a camera on a tripod\\\", \\\"a black-and-white silhouette of a horse\\\", \\\"a cup of coffee on a saucer\\\" ] text_tokens = clip.tokenize([\\\"This is \\\" + desc for desc in texts]).cuda() with torch.no_grad(): text_features = model.encode_text(text_tokens).float() # 2. 提取图像特征 image_input = torch.tensor(np.stack(images)).cuda() with torch.no_grad(): image_features = model.encode_image(image_input).float() # 3. 计算余弦相似度 image_features /= image_features.norm(dim=-1, keepdim=True) text_features /= text_features.norm(dim=-1, keepdim=True) similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\",\"相似度如下所示，可以看到对于要预测的8个图像，按照最大相似度，其均能匹配到正确的文本标签：\",\"进一步地，我们也可以对得到的余弦相似度计算softmax，得到每个预测类别的概率值：\",\"text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1) top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)\",\"得到的预测概率如下所示，可以看到8个图像，CLIP模型均能够以较高的置信度给出正确的分类结果：\"]},\"53\":{\"h\":\"文本描述生成\",\"t\":[\"在使用CLIP模型进行zero-shot分类时，除了模型本身的应用，文本描述的生成也是一个关键环节。在之前的例子中，我们使用了“A photo of {label}”这样的格式来生成文本描述，但实际上，我们还有其他的选择。例如，我们可以直接使用类别标签作为文本描述。这种方法实际上与NLP领域的一个研究方向——prompt learning或prompt engineering——紧密相关。关于这一领域的详细综述，可以参考论文《Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing》。\",\"简单来说，prompt learning的核心思想是通过设计合适的prompt（提示），使得预训练模型能够直接应用于下游任务。这与传统的预训练加微调的方法有所不同。论文指出，如果我们直接使用类别标签作为文本描述，由于这些文本往往只是一个单词，缺乏具体的上下文，并且与CLIP模型的训练数据不完全一致，因此在效果上可能不如使用“A photo of {label}”这种格式（在ImageNet数据集上可以提升1.3%的效果）。\",\"此外，论文还实验了使用80个不同的prompt进行集成，结果发现在ImageNet数据集上能够带来3.5%的性能提升。具体的实验结果可以参考CLIP公开的notebook。\"]},\"54\":{\"h\":\"花卉图片分类\",\"t\":[\"本节我们将基于CLIP预训练模型实现Zero-Shot推理，训练使用到的数据集和AlexNet保持一致，因此这里就不再给出数据集下载链接了。\",\"图片分类实战 – 分别基于LeNet，AlexNet，VGG进行实现\",\"# 预训练模型名称 model_name = \\\"openai/clip-vit-large-patch14\\\" # 定义当前目录 current_dir = os.getcwd() model_dir = os.path.join(current_dir, model_name.replace(\\\"/\\\", \\\"_\\\")) # 检查当前目录是否有预训练权重文件，如果没有则下载 def download_pretrained_weights_if_needed(model_name, save_dir): if not os.path.exists(save_dir): try: print(f\\\"Downloading {model_name} to {save_dir}...\\\") snapshot_download(repo_id=model_name, local_dir=save_dir, local_dir_use_symlinks=False) print(f\\\"{model_name} downloaded successfully.\\\") except Exception as e: print(f\\\"Error downloading {model_name}: {e}\\\") download_pretrained_weights_if_needed(model_name, model_dir) # 加载模型和处理器 model = CLIPModel.from_pretrained(model_dir) processor = CLIPProcessor.from_pretrained(model_dir) device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\") model = model.to(device)\",\"在 openai/clip-vit-large-patch14 这个 CLIP 预训练模型中，图像编码器采用了 Vision Transformer（ViT）架构，具体使用的是 ViT-L/14 版本，文本编码器使用的是基于 Transformer 的架构。\",\"# 函数：生成文本嵌入 def text_embedding(texts): inputs = processor(text=texts, return_tensors=\\\"pt\\\", padding=True).to(device) with torch.no_grad(): embeddings = model.get_text_features(**inputs) return embeddings.cpu().numpy()\",\"这个函数的作用是将输入的文本转化为对应的嵌入表示（embedding）。它通过处理器对输入文本进行处理，使其符合模型的输入要求，然后利用模型获取文本特征，最后将结果转换为 numpy 数组格式返回，方便后续的计算和比较。\",\"def get_image_embeddings(image_paths): images = [] for image_path in image_paths: try: image = Image.open(image_path).convert(\\\"RGB\\\") images.append(image) except Exception as e: print(f\\\"Error loading image {e}\\\") if not images: return None inputs = processor(images=images, return_tensors=\\\"pt\\\").to(device) with torch.no_grad(): image_features = model.get_image_features(**inputs) return image_features.cpu().numpy()\",\"该函数作用是针对给定的图片路径，读取图片并将其转换为合适的格式后，通过模型获取图片的特征嵌入。如果在读取图片过程中出现错误，会进行相应的错误提示并返回 None。\",\"def cosine_similarity(vec1, vec2): vec1 = np.array(vec1) vec2 = np.array(vec2) return np.dot(vec1, vec2.T) / (np.linalg.norm(vec1, axis=1, keepdims=True) * np.linalg.norm(vec2, axis=1))\",\"在图文检索中，我们常常需要衡量文本嵌入和图片嵌入之间的相似度，这里采用了余弦相似度的计算方法。它将输入的向量转换为 numpy 数组后，按照余弦相似度的数学公式来计算两者的相似度数值。\",\"首先，我们需要根据上面给出的花卉数据集下载链接，将数据下载到当前项目目录下:\",\"其次，我们从flower_photos目录下读取出所有图片的路径:\",\"# 递归遍历目录获取所有图片路径 def get_all_image_paths(directory): image_paths = [] for root, _, files in os.walk(directory): for file in files: file_extension = os.path.splitext(file)[1].lower() if file_extension in ['.png', '.jpg', '.jpeg']: image_paths.append(os.path.join(root, file)) return image_paths image_paths = get_all_image_paths(\\\"./flower_photos\\\")\",\"同时将flower_photos下的子目录名作为我们的候选待匹配分类文本列表，并改造为a photo of 子目录名的格式，然后计算每个分类文本对应的文本嵌入向量:\",\"# 获取候选分类名列表 def get_candidates(directory): candidates = [] for sub_dir in os.listdir(directory): sub_dir_path = os.path.join(directory, sub_dir) if os.path.isdir(sub_dir_path): candidates.append(f\\\"a photo of {sub_dir}\\\") return candidates candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates)\",\"最后:\",\"分批次从图像列表中取出一批图像，获取其对应的图像嵌入向量列表\",\"计算这批图像嵌入向量中每一个图像嵌入向量和我们所有分类文本嵌入向量的余弦相似度\",\"针对每个图像嵌入向量取出和其相似度最高的那个文本嵌入向量的索引\",\"判断预测是否正确，统计正确率\",\"# 测试图片分类正确率 def accuracy(image_paths, candidates, text_embeddings, batch_size=64): correct_count = 0 total_count = len(image_paths) num_batches = (total_count + batch_size - 1) // batch_size # 分批次预测 for i in range(num_batches): start_idx = i * batch_size end_idx = min(start_idx + batch_size, total_count) # 取出当前批次的图像列表，并获得该批次图像列表对应的图像嵌入向量列表 batch_image_paths = image_paths[start_idx:end_idx] image_embeddings = get_image_embeddings(batch_image_paths) if image_embeddings is not None: # 计算图像嵌入向量列表中每个图像对应的嵌入向量和传入的分类文本嵌入向量的余弦相似度 similarities = cosine_similarity(image_embeddings, text_embeddings) # 针对每个图像嵌入向量获取和其相似度最高的分类文本嵌入向量索引下标 predicted_indices = np.argmax(similarities, axis=1) for j, predicted_index in enumerate(predicted_indices): # 针对每张图像，根据上述计算得到的和其相似度最高的分类文本索引，从候选分类文本集合中取出其分类名词 predicted_category = candidates[predicted_index].split(\\\" \\\")[-1] # 用当前图片外层目录的名字作为其分类名词 actual_category = os.path.basename(os.path.dirname(batch_image_paths[j])) # 比较两个分类名词是否相等 if predicted_category == actual_category: correct_count += 1 accuracy = correct_count / total_count return accuracy # 图片分类 def flowerClassify(): image_paths = get_all_image_paths(\\\"./flower_photos\\\") candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates) start_time = time.time() acc = accuracy(image_paths, candidates, text_embeddings, batch_size=64) end_time = time.time() print(f\\\"Time taken to test accuracy: {end_time - start_time:.2f} seconds\\\") print(f\\\"Accuracy: {acc * 100:.2f}%\\\")\",\"Time taken to test accuracy: 396.62 seconds Accuracy: 95.48%\"]},\"55\":{\"h\":\"文字搜索图像\",\"t\":[\"上面的花卉分类案例相当于使用图像去匹配最合适的文本描述，而这里我们将会反转这个逻辑，用文本描述去匹配最合适的图片内容。\",\"为了实现文字搜索图像的功能，我们只需要在计算出相似度得分矩阵后，以每个文本描述为一行，取出该行中得分最大的那一列，即为与当前文本描述相似度最高的那副图片，具体代码实现如下：\",\"# 找到与文本最匹配的图片 def find_most_matching_image(text, image_paths): text_emb = text_embedding([text]) image_embeddings = get_image_embeddings(image_paths) if image_embeddings is None: return None similarities = cosine_similarity(text_emb, image_embeddings) most_matching_index = np.argmax(similarities) return image_paths[most_matching_index]\",\"下面来实际展示一下效果，首先我们用data目录充当我们的图片库来源:\",\" 遍历data目录，拿到所有图片路径:\",\"# 遍历 data 目录获取所有图片路径 def get_images_from_data_dir(): data_dir = os.path.join(current_dir, 'data') if not os.path.exists(data_dir): print(f\\\"Data directory {data_dir} does not exist.\\\") return [] return get_all_image_paths(data_dir)\",\"这里以搜索向日葵花为例，我们首先获取图片库中所有图片，然后计算出和当前文本描述相似度最高的那副图片，并将图片展示出来:\",\"# 根据文字搜索图片 def searchPicByText(): image_paths = get_images_from_data_dir() query_text = \\\"a photo of a sunflowers\\\" most_matching_image = find_most_matching_image(query_text, image_paths) if most_matching_image: print(f\\\"The most matching image for '{query_text}' is: {most_matching_image}\\\") try: img = Image.open(most_matching_image) plt.imshow(img) plt.axis('off') plt.title(f\\\"Most matching image for '{query_text}'\\\") plt.show() except Exception as e: print(f\\\"Error opening image: {e}\\\") else: print(\\\"No matching image found.\\\")\",\"图片库中的图片： 运行上述代码，搜索出来的图片:\"]},\"56\":{\"h\":\"完整代码\",\"t\":[\"import time from matplotlib import pyplot as plt from transformers import CLIPProcessor, CLIPModel import torch from PIL import Image import numpy as np import warnings import os from huggingface_hub import snapshot_download warnings.filterwarnings(\\\"ignore\\\") # 模型名称 model_name = \\\"openai/clip-vit-large-patch14\\\" # 定义当前目录 current_dir = os.getcwd() model_dir = os.path.join(current_dir, model_name.replace(\\\"/\\\", \\\"_\\\")) # 检查当前目录是否有预训练权重文件，如果没有则下载 def download_pretrained_weights_if_needed(model_name, save_dir): if not os.path.exists(save_dir): try: print(f\\\"Downloading {model_name} to {save_dir}...\\\") snapshot_download(repo_id=model_name, local_dir=save_dir, local_dir_use_symlinks=False) print(f\\\"{model_name} downloaded successfully.\\\") except Exception as e: print(f\\\"Error downloading {model_name}: {e}\\\") download_pretrained_weights_if_needed(model_name, model_dir) # 加载模型和处理器 model = CLIPModel.from_pretrained(model_dir) processor = CLIPProcessor.from_pretrained(model_dir) device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\") model = model.to(device) # 函数：生成文本嵌入 def text_embedding(texts): inputs = processor(text=texts, return_tensors=\\\"pt\\\", padding=True).to(device) with torch.no_grad(): embeddings = model.get_text_features(**inputs) return embeddings.cpu().numpy() def get_image_embeddings(image_paths): images = [] for image_path in image_paths: try: image = Image.open(image_path).convert(\\\"RGB\\\") images.append(image) except Exception as e: print(f\\\"Error loading image {e}\\\") if not images: return None inputs = processor(images=images, return_tensors=\\\"pt\\\").to(device) with torch.no_grad(): image_features = model.get_image_features(**inputs) return image_features.cpu().numpy() def cosine_similarity(vec1, vec2): vec1 = np.array(vec1) vec2 = np.array(vec2) return np.dot(vec1, vec2.T) / (np.linalg.norm(vec1, axis=1, keepdims=True) * np.linalg.norm(vec2, axis=1)) # 递归遍历目录获取所有图片路径 def get_all_image_paths(directory): image_paths = [] for root, _, files in os.walk(directory): for file in files: file_extension = os.path.splitext(file)[1].lower() if file_extension in ['.png', '.jpg', '.jpeg']: image_paths.append(os.path.join(root, file)) return image_paths # 获取候选分类名列表 def get_candidates(directory): candidates = [] for sub_dir in os.listdir(directory): sub_dir_path = os.path.join(directory, sub_dir) if os.path.isdir(sub_dir_path): candidates.append(f\\\"a photo of {sub_dir}\\\") return candidates # 测试图片分类正确率 def accuracy(image_paths, candidates, text_embeddings, batch_size=64): correct_count = 0 total_count = len(image_paths) num_batches = (total_count + batch_size - 1) // batch_size for i in range(num_batches): start_idx = i * batch_size end_idx = min(start_idx + batch_size, total_count) batch_image_paths = image_paths[start_idx:end_idx] image_embeddings = get_image_embeddings(batch_image_paths) if image_embeddings is not None: similarities = cosine_similarity(image_embeddings, text_embeddings) predicted_indices = np.argmax(similarities, axis=1) for j, predicted_index in enumerate(predicted_indices): predicted_category = candidates[predicted_index].split(\\\" \\\")[-1] actual_category = os.path.basename(os.path.dirname(batch_image_paths[j])) if predicted_category == actual_category: correct_count += 1 accuracy = correct_count / total_count return accuracy # 图片分类 def flowerClassify(): image_paths = get_all_image_paths(\\\"./flower_photos\\\") candidates = get_candidates(\\\"./flower_photos\\\") text_embeddings = text_embedding(candidates) start_time = time.time() acc = accuracy(image_paths, candidates, text_embeddings, batch_size=64) end_time = time.time() print(f\\\"Time taken to test accuracy: {end_time - start_time:.2f} seconds\\\") print(f\\\"Accuracy: {acc * 100:.2f}%\\\") ##################################################################################################3 # 遍历 data 目录获取所有图片路径 def get_images_from_data_dir(): data_dir = os.path.join(current_dir, 'data') if not os.path.exists(data_dir): print(f\\\"Data directory {data_dir} does not exist.\\\") return [] return get_all_image_paths(data_dir) # 找到与文本最匹配的图片 def find_most_matching_image(text, image_paths): text_emb = text_embedding([text]) image_embeddings = get_image_embeddings(image_paths) if image_embeddings is None: return None similarities = cosine_similarity(text_emb, image_embeddings) most_matching_index = np.argmax(similarities) return image_paths[most_matching_index] # 根据文字搜索图片 def searchPicByText(): image_paths = get_images_from_data_dir() query_text = \\\"a photo of a sunflowers\\\" most_matching_image = find_most_matching_image(query_text, image_paths) if most_matching_image: print(f\\\"The most matching image for '{query_text}' is: {most_matching_image}\\\") try: img = Image.open(most_matching_image) plt.imshow(img) plt.axis('off') plt.title(f\\\"Most matching image for '{query_text}'\\\") plt.show() except Exception as e: print(f\\\"Error opening image: {e}\\\") else: print(\\\"No matching image found.\\\")\"]},\"57\":{\"h\":\"小结\",\"t\":[\"在计算机视觉领域，常见的迁移学习方法是首先在大规模数据集（如ImageNet）上进行预训练，然后在具体的下游任务上进行微调。这种预训练通常是基于有监督学习的，需要大量的数据标注，因此成本较高。近年来，出现了一些基于自监督的方法，包括基于对比学习的方法（如MoCo和SimCLR）和基于图像掩码的方法（如MAE和BeiT）。自监督方法的优势在于不再需要标注数据。然而，无论是有监督还是自监督方法，在迁移到下游任务时，都需要进行有监督微调，无法实现zero-shot学习。对于有监督模型，由于它们在预训练数据集上采用固定类别数的分类器，因此在新的数据集上需要定义新的分类器来重新训练。对于自监督模型，代理任务通常是辅助进行表征学习，在迁移到其他数据集时也需要加上新的分类器进行有监督训练。\",\"然而，在NLP领域，基于自回归或语言掩码的预训练方法已经相对成熟，预训练模型很容易直接zero-shot迁移到下游任务，例如OpenAI的GPT-3。这种差异一方面是由于文本和图像属于两个完全不同的模态，另一个原因是NLP模型可以利用从互联网上收集的大量文本。因此，问题来了：能否基于互联网上的大量文本来预训练视觉模型？\",\"实际上，之前已经有一些研究工作探讨了使用文本作为监督信号来训练视觉模型。例如，2016年的工作《Learning Visual Features from Large Weakly Supervised Data》将这个问题转化为一个多标签分类任务，预测图像对应的文本的词袋模型；2017年的工作《Learning Visual N-Grams from Web Data》进一步扩展了这个方法来预测n-grams。最近的一些工作采用新的模型架构和预训练方法来从文本学习视觉特征，例如VirTex基于transformer的语言模型，ICMLM基于语言掩码的方法，ConVIRT基于对比学习的方法。总体来看，这方面的工作并不多，主要是因为这些方法难以实现较高的性能，例如2017年的那篇工作只在ImageNet上实现了11.5%的zero-shot性能，这远远低于ImageNet上的SOTA。此外，还有另一个方向，即基于文本弱监督来提升性能，例如谷歌的BiT和ViT基于JFT-300M数据集来预训练模型在ImageNet上取得SOTA。JFT-300M数据集是谷歌从互联网上收集的，通过一些自动化的手段将web text转化为18291个类别，但存在一定的噪声。尽管谷歌基于JFT-300M数据集取得了较好的结果，但这些模型仍然采用固定类别的softmax分类器进行预训练，这大大限制了它们的迁移能力和扩展性。\",\"谷歌采用的弱监督方法与以往方法的一个主要区别在于规模，或者说在于计算能力和数据集的规模。JFT-300M数据集的规模达到了上亿级别，谷歌利用强大的计算能力进行了预训练。相比之下，VirTex、ICMLM和ConVIRT仅在10万级别的数据上训练了几天。为了弥补数据规模上的差距，OpenAI从网络上收集了4亿条数据进行实验。然而，新的问题出现了：应该采用什么样的方法来进行训练。\",\"OpenAI首先尝试了VirTex模型，该模型联合训练一个CNN和文本transformer来预测图像的文本描述（image caption），但发现这种方法的训练效率（根据ImageNet数据集上的zero-shot性能评估）还不如直接预测词袋模型（bag of words），两者的训练效率相差3倍。如果进一步采用ConVIRT，即基于对比学习的方法，训练效率可以提高4倍。出现这种差异的原因不难理解，因为训练数据集中的文本-图像对是从互联网收集的，存在一定的噪声，即文本和图像可能不完全匹配。在这种情况下，适当降低训练目标反而可能取得更好的效果。\",\"从任务难度来看，排序为：Transformer Language Model > Bag of Words Prediction > Bag of Words Contrastive (CLIP)。由于训练数据量和模型计算量较大，训练效率成为一个至关重要的因素。因此，作者最终选择了对比学习方法来进行训练。\"]},\"58\":{\"h\":\"庖丁解牛BLIP2\",\"t\":[\"庖丁解牛BLIP2\"]},\"59\":{\"h\":\"庖丁解牛VIT\",\"t\":[\"多模态模型VIT原理与图片分类实战演练\",\"Vision Transformer是2021年谷歌在ICLR上提出的算法，它首次将NLP领域火热的Transformer模型架构移植到了CV领域，打破了这两个领域壁垒，并取得不错的成效。\",\"Vision Transformer的模型结构相比于Transformer来说更简单，在Transformer模型中，主要包含Encoder和Decoder结构，而ViT(Vision Transformer)仅借鉴了Encoder结构。\",\"ViT原论文中最核心的结论是: 当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果。\",\"归纳偏置:\",\"归纳偏置能够帮助学习算法缩小搜索范围，快速找到合适的模型。\",\"例如，在图像分类任务中，如果没有任何归纳偏置，学习算法需要在所有可能的函数空间中搜索最优模型，这几乎是不可能完成的任务。而通过引入特定的归纳偏置，如局部性和平移不变性（CNN 所具备的），可以将搜索范围限制在满足这些性质的模型子空间内，大大提高学习效率。\",\"但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些，因为Transformer和CNN相比缺少归纳偏置（inductive bias），即一种先验知识，提前做好的假设。CNN具有两种归纳偏置，一种是局部性（locality/two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征；一种是平移不变形（translation equivariance）。当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型。\"]},\"60\":{\"h\":\"原理\",\"t\":[\"本文将通过一个花卉分类的实战案例结合ViT原论文，来帮助大家梳理清楚Vision Transformer的核心流程实现。\"]},\"61\":{\"h\":\"0. 数据下载\",\"t\":[\"实验采用的是花蕊数据集，共5个类别，约4000多个样本。\",\"数据集下载：https://pan.baidu.com/s/137mO-7PY1jDq1Wp0NNyT3A?pwd=qvmq\",\"数据集加载代码:\",\"def read_split_data(root: str, val_rate: float = 0.2): random.seed(0) # 保证随机结果可复现 assert os.path.exists(root), \\\"dataset root: {} does not exist.\\\".format(root) # 遍历文件夹，一个文件夹对应一个类别 flower_class = [cla for cla in os.listdir(root) if os.path.isdir(os.path.join(root, cla))] # 排序，保证顺序一致 flower_class.sort() # 生成类别名称以及对应的数字索引 class_indices = dict((k, v) for v, k in enumerate(flower_class)) json_str = json.dumps(dict((val, key) for key, val in class_indices.items()), indent=4) with open('class_indices.json', 'w') as json_file: json_file.write(json_str) train_images_path = [] # 存储训练集的所有图片路径 train_images_label = [] # 存储训练集图片对应索引信息 val_images_path = [] # 存储验证集的所有图片路径 val_images_label = [] # 存储验证集图片对应索引信息 every_class_num = [] # 存储每个类别的样本总数 supported = [\\\".jpg\\\", \\\".JPG\\\", \\\".png\\\", \\\".PNG\\\"] # 支持的文件后缀类型 # 遍历每个文件夹下的文件 for cla in flower_class: cla_path = os.path.join(root, cla) # 遍历获取supported支持的所有文件路径 images = [os.path.join(root, cla, i) for i in os.listdir(cla_path) if os.path.splitext(i)[-1] in supported] # 获取该类别对应的索引 image_class = class_indices[cla] # 记录该类别的样本数量 every_class_num.append(len(images)) # 按比例随机采样验证样本 val_path = random.sample(images, k=int(len(images) * val_rate)) for img_path in images: if img_path in val_path: # 如果该路径在采样的验证集样本中则存入验证集 val_images_path.append(img_path) val_images_label.append(image_class) else: # 否则存入训练集 train_images_path.append(img_path) train_images_label.append(image_class) print(\\\"{} images were found in the dataset.\\\".format(sum(every_class_num))) print(\\\"{} images for training.\\\".format(len(train_images_path))) print(\\\"{} images for validation.\\\".format(len(val_images_path))) plot_image = True if plot_image: # 绘制每种类别个数柱状图 plt.bar(range(len(flower_class)), every_class_num, align='center') # 将横坐标0,1,2,3,4替换为相应的类别名称 plt.xticks(range(len(flower_class)), flower_class) # 在柱状图上添加数值标签 for i, v in enumerate(every_class_num): plt.text(x=i, y=v + 5, s=str(v), ha='center') # 设置x坐标 plt.xlabel('image class') # 设置y坐标 plt.ylabel('number of images') # 设置柱状图的标题 plt.title('flower class distribution') plt.show() return train_images_path, train_images_label, val_images_path, val_images_label\",\"自定义一个MyDataSet类来封装我们加载得到的数据集:\",\"from torch.utils.data import Dataset from PIL import Image import torch class MyDataSet(Dataset): \\\"\\\"\\\"自定义数据集\\\"\\\"\\\" def __init__(self, images_path: list, images_class: list, transform=None): \\\"\\\"\\\" 初始化自定义数据集类 :param images_path: 包含所有图像文件路径的列表 :param images_class: 包含所有图像对应类别的列表，与 images_path 中的图像一一对应 :param transform: 图像预处理的转换操作，默认为 None \\\"\\\"\\\" self.images_path = images_path self.images_class = images_class self.transform = transform def __len__(self): \\\"\\\"\\\" 返回数据集中图像的数量 :return: 数据集中图像的数量 \\\"\\\"\\\" return len(self.images_path) def __getitem__(self, item): \\\"\\\"\\\" 根据索引获取数据集中的图像和对应的标签 :param item: 图像的索引 :return: 经过预处理的图像和对应的标签 \\\"\\\"\\\" # 打开指定索引的图像文件 img = Image.open(self.images_path[item]) # RGB为彩色图片，L为灰度图片 # 检查图像是否为 RGB 模式，如果不是则抛出异常 if img.mode != 'RGB': raise ValueError(\\\"image: {} isn't RGB mode.\\\".format(self.images_path[item])) # 获取对应图像的标签 label = self.images_class[item] # 如果定义了图像预处理转换操作，则对图像进行处理 if self.transform is not None: img = self.transform(img) return img, label @staticmethod def collate_fn(batch): \\\"\\\"\\\" 自定义的批量数据处理函数，用于将一个批次的数据组合成一个张量 :param batch: 一个批次的数据，包含图像和对应的标签 :return: 组合后的图像张量和标签张量 \\\"\\\"\\\" # 官方实现的default_collate可以参考 # https://github.com/pytorch/pytorch/blob/67b7e751e6b5931a9f45274653f4f653a4e6cdf6/torch/utils/data/_utils/collate.py # 将一个批次的数据拆分为图像和标签两个元组 images, labels = tuple(zip(*batch)) # 将图像元组堆叠成一个四维张量，维度为 (batch_size, channels, height, width) images = torch.stack(images, dim=0) # 将标签元组转换为一个一维张量 labels = torch.as_tensor(labels) return images, labels\",\"两点注意:\",\"当使用 DataLoader 从数据集（Dataset）中加载数据时，它会将多个样本收集起来形成一个批次，但默认的组合方式可能不满足所有需求，这时就可以自定义 collate_fn 函数。\",\"@staticmethod 是 Python 中的一个装饰器，用于将一个方法定义为静态方法。静态方法是类中的一种特殊方法，它与类的实例和类本身都没有直接关联，可以直接通过类名调用，不需要创建类的实例。\"]},\"62\":{\"h\":\"1. 图片预处理\",\"t\":[\"预处理这个步骤在论文里并没有详细说明，但是对于ViT这个结构而言，输入的图片尺寸并不是自定义的，ViT-B/16为例，输入的图片尺寸必须为224x224。\",\"在 ViT - B/16 中，“B” 代表的是模型的基础（Base）版本 ，“16” 表示每个图像块的大小是 16x16 像素；ViT 通常在大规模数据集（如 ImageNet）上进行预训练，而预训练过程中使用的输入图像尺寸通常固定为 224x224。在预训练时，模型的参数是根据这个特定尺寸的输入数据进行优化和学习的。当我们在其他任务中使用预训练好的模型时，为了充分利用预训练的权重，也需要保持输入图像尺寸与预训练时一致，这样可以保证模型的特征提取能力和性能。\",\"因此，首先需要对输入图片进行尺寸变化，具体方式可以是直接缩放(Resize)，也可以进行随机裁剪(RandomResizedCrop)。\",\"对数据集和验证集划分之后，这里对训练集的处理方式是随机切成224x224像素的图片，然后进行水平翻转，再进行归一化和标准化处理；对验证集的处理方式是先Resize成256x256的图片，再从中心位置裁剪成224x224，再进行归一化和标准化处理。\",\"# 定义一个字典 data_transform，用于存储训练集和验证集的图像预处理转换操作 data_transform = { # 训练集的预处理转换操作 \\\"train\\\": transforms.Compose([ # 随机裁剪输入图像，将裁剪后的图像调整为 224x224 大小 # 这是一种数据增强的方式，通过随机裁剪可以增加训练数据的多样性，提高模型的泛化能力 transforms.RandomResizedCrop(224), # 以 0.5 的概率随机水平翻转图像 # 同样是数据增强的手段，增加了图像的多样性，有助于模型学习到不同方向的特征 transforms.RandomHorizontalFlip(), # 将 PIL 图像或 NumPy 数组转换为 PyTorch 的张量（Tensor） # 同时会将像素值从 [0, 255] 范围缩放到 [0, 1] 范围 transforms.ToTensor(), # 对图像进行归一化处理 # 第一个参数 [0.5, 0.5, 0.5] 是图像每个通道的均值，第二个参数 [0.5, 0.5, 0.5] 是图像每个通道的标准差 # 归一化有助于模型更快地收敛，提高训练的稳定性 transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) ]), # 验证集的预处理转换操作 \\\"val\\\": transforms.Compose([ # 将图像的短边缩放为 256 像素，长边按比例缩放 # 这一步是为了保证图像的整体比例不变，后续再进行裁剪操作 transforms.Resize(256), # 从图像的中心位置裁剪出 224x224 大小的图像 # 验证集不需要进行数据增强，只需要将图像调整到合适的大小 transforms.CenterCrop(224), # 将 PIL 图像或 NumPy 数组转换为 PyTorch 的张量（Tensor） # 同样会将像素值从 [0, 255] 范围缩放到 [0, 1] 范围 transforms.ToTensor(), # 对图像进行归一化处理，参数与训练集的归一化参数相同 # 保证训练集和验证集的数据处理方式一致 transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) ]) }\",\"下面我们将用于图片变换的transforms流水线和上面自定义的MyDataSet类都封装到DataLoader去。\",\"train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(data_path) # 实例化训练数据集 train_dataset = MyDataSet(images_path=train_images_path, images_class=train_images_label, transform=data_transform[\\\"train\\\"]) # 实例化验证数据集 val_dataset = MyDataSet(images_path=val_images_path, images_class=val_images_label, transform=data_transform[\\\"val\\\"]) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn) val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=val_dataset.collate_fn)\"]},\"63\":{\"h\":\"2. 图片切割\",\"t\":[\"Transformer需要输入的是一维的Token，对于二维的图像，一种朴素的想法就是把一个个像素点拉平，这样就成了一个一维序列。但是这样造成的一个后果是计算量太庞大，比如一张224x224的图片，变成1维度之后就成了50176，相当于直接输入一篇五万字的文章，模型难以计算。\",\"那么，一个改进的想法就是把一张图片分成nxn个Patch，每一个Patch作为一个Token，这样计算量就大大减小了。\",\"以ViT-B/16为例，将输入图片(224x224)按照16x16大小的Patch进行划分，划分后可以得到共个Patch。每个Patch是三通道的小图片，shape为(16, 16, 3)，将其展平就变成了一个长度为768的向量。\",\"每一个向量作为一个单独的输入，那样我们总共有196个向量，在代码中，可以变成一个[196,768]的矩阵，进行并行输入。\",\"这一步的操作在论文中是直接采用切割的处理办法，但是实际的代码实现中，采用了一种更巧妙的解决思路，就是利用一个卷积核大小为16x16，步距为16，卷积核个数为768的卷积层来进行实现。\",\"再来回顾我们的卷积层计算公式：\",\"输入为[224,244,3]，经过卷积层变成[14,14,768]，再映射为[196,768]。\",\"这样，就完成了从图片到Token之间的转换，我们通过自定义一个PatchEmbed类完成上述工作。\",\"class PatchEmbed(nn.Module): \\\"\\\"\\\" 2D Image to Patch Embedding 该类的作用是将二维图像分割成多个图像块（patch），并将这些图像块嵌入到一个低维向量空间中 \\\"\\\"\\\" def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None): \\\"\\\"\\\" 初始化 PatchEmbed 类 :param img_size: 输入图像的尺寸，默认为 224。如果传入一个整数，则表示图像是正方形，边长为该整数； :param patch_size: 每个图像块的尺寸，默认为 16。同样，如果传入一个整数，则表示图像块是正方形，边长为该整数； :param in_c: 输入图像的通道数，默认为 3（对应 RGB 图像） :param embed_dim: 嵌入维度，即每个图像块经过卷积操作后得到的特征向量的维度，默认为 768 :param norm_layer: 归一化层，默认为 None。如果传入一个归一化层类，则使用该层进行归一化；否则使用 nn.Identity() 表示不进行归一化 \\\"\\\"\\\" super().__init__() # 将 img_size 和 patch_size 转换为元组形式，如果传入的是整数，则将其转换为 (整数, 整数) 的形式 img_size = (img_size, img_size) patch_size = (patch_size, patch_size) self.img_size = img_size self.patch_size = patch_size # 计算网格大小，即图像在水平和垂直方向上分别可以划分的图像块数量 self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) # 计算图像块的总数，即网格大小的乘积 self.num_patches = self.grid_size[0] * self.grid_size[1] # 定义一个二维卷积层，用于将输入图像分割成多个图像块并进行嵌入 # in_c 是输入通道数，embed_dim 是输出通道数（也就是卷积核的数量） # kernel_size 是卷积核的大小，这里设置为图像块的大小 # stride 是卷积核的步长，这里设置为图像块的大小，确保卷积操作不会重叠 self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size) # 如果传入了归一化层类，则使用该层进行归一化；否则使用 nn.Identity() 表示不进行归一化 self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() def forward(self, x): \\\"\\\"\\\" 前向传播函数 :param x: 输入的图像张量，形状为 [B, C, H, W]，其中 B 是批量大小，C 是通道数，H 是图像高度，W 是图像宽度 :return: 经过处理后的图像块嵌入张量，形状为 [B, num_patches, embed_dim] \\\"\\\"\\\" # 获取输入图像张量的形状 B, C, H, W = x.shape # 注意下面的embed_dim代表的是卷积核的数量，也就是经过卷积后拼接得到的特征图(输出通道)数量 # H`和 W`代表输出特征图的宽和高 # 首先使用卷积层对输入图像进行处理，得到形状为 [B, embed_dim, H', W'] 的特征图 # 然后将特征图的最后两维展平为一维，得到形状为 [B, embed_dim, num_patches] 的张量 # 最后交换第 1 维和第 2 维，得到形状为 [B, num_patches, embed_dim] 的张量 # 这里的 num_patches 是图像块的总数 x = self.proj(x).flatten(2).transpose(1, 2) # 对处理后的张量进行归一化操作 x = self.norm(x) return x\",\"用一个简化版的例子说明上述过程:\",\"核心要点: 将卷积后的通道维数作为embedding的维度，卷积后剩余的长和宽相乘作为时间维度，由此把图片转换为序列的embedding形式。\"]},\"64\":{\"h\":\"3. 添加[class]token\",\"t\":[\"在上面的结构图中可以看到，输入Encoder的最左侧部分添加了一个0*这个Token，这个就是额外添加的一个[class]token，单独用来处理类别信息，经过Encoder之后，需要单独将这个Token再提取出来，输入到MLP Head之中再输出分类结果。\",\"这也是为什么结构图中MLP Head的位置是和这个[class]token对齐。\",\"这里简单介绍一下CLS TOKEN的作用:\",\"[CLS] Token 的作用是通过训练过程中损失值的降低，模型学会如何通过注意力机制将图像的有效信息汇聚到 [CLS] Token 中，从而完成图像分类任务。\",\"初始化： \",\"[CLS] Token 是一个随机初始化的向量，初始时没有任何语义信息。\",\"位置编码被添加到 patch 嵌入中，以保留图像的空间信息。\",\"前向传播： \",\"输入图像被分割成 patches，并通过线性变换映射到嵌入空间。\",\"[CLS] Token 被添加到 patch 嵌入序列的开头。\",\"通过多层 Transformer Encoder，模型计算每个 patch 嵌入（包括 [CLS] Token）与其他 patch 嵌入的关系。\",\"注意力汇聚： \",\"在每一层 Transformer 中，[CLS] Token 通过自注意力机制与其他 patch 嵌入交互。\",\"模型学会将图像中与分类任务相关的信息汇聚到 [CLS] Token 中。\",\"损失计算与反向传播： \",\"[CLS] Token 的输出向量被输入到分类头中，用于预测图像的类别。\",\"通过计算损失（如交叉熵损失），模型更新参数，使得 [CLS] Token 能够更好地聚合图像信息。\",\"收敛： \",\"随着训练的进行，损失值逐渐降低，模型学会如何通过注意力机制将图像的有效信息汇聚到 [CLS] Token 中。\",\"最终，[CLS] Token 的输出向量能够很好地表示图像的全局特征，用于分类任务。\",\"[CLS] Token 能起作用的原因在于：\",\"注意力机制的特性： \",\"自注意力机制能够捕捉图像中任意两个 patches 之间的关系。\",\"[CLS] Token 通过与其他 patches 的交互，能够动态地聚合图像信息。\",\"训练目标的引导： \",\"训练过程中，损失函数直接作用于 [CLS] Token 的输出。\",\"模型被强制学会将图像的有效信息汇聚到 [CLS] Token 中，以最小化损失。\",\"全局特征表示： \",\"[CLS] Token 位于序列的开头，能够通过多层 Transformer 逐步聚合全局信息。\",\"最终，[CLS] Token 的输出向量能够很好地表示图像的全局特征。\",\"class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, embed_layer=None): \\\"\\\"\\\" Args: img_size (int, tuple): 输入图像的尺寸 patch_size (int, tuple): 图像块的尺寸 in_c (int): 输入图像的通道数 num_classes (int): 分类任务的类别数 embed_dim (int): 嵌入维度 embed_layer (nn.Module): 图像块嵌入层 \\\"\\\"\\\" super(VisionTransformer, self).__init__() # 保存分类任务的类别数 self.num_classes = num_classes # 保存嵌入维度，同时作为特征数量，以保持与其他模型的一致性 self.num_features = self.embed_dim = embed_dim # 只有一个特殊标记（分类标记） self.num_tokens = 1 # 创建图像块嵌入层 self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # 计算图像块的数量 num_patches = self.patch_embed.num_patches # 创建可学习的分类标记 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 使用截断正态分布初始化分类标记 nn.init.trunc_normal_(self.cls_token, std=0.02) ... def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] # 将输入图像进行图像块嵌入 x = self.patch_embed(x) # [B, 196, 768] # [1, 1, 768] -> [B, 1, 768] # 扩展分类标记以匹配输入批次大小 cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将分类标记和图像块嵌入拼接 x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] ... # 返回分类标记对应的特征,x[:,0]对应维度为[B,1,768] return x[:,0]; def forward(self, x): # 提取输入图像的特征 x = self.forward_features(x) # 通过分类头 -- [B,1,768] x = self.head(x) return x\"]},\"65\":{\"h\":\"4. 添加位置编码\",\"t\":[\"在Transformer中，位置编码的作用是为了记忆输入的语序信息。ViT中，同样需要位置编码来记录各图像块之间的位置信息。\",\"这里主要有两种位置编码思路，一种思路是在转换之前(14,14)的图像块矩阵添加二维(2-D)位置编码，另一种思路是在转换后(196+1)这个维度上添加一维(1-D)位置编码。\",\"论文作者也对其做了实验，实验结果如下表所示：\",\" 可以看到，添加一维位置编码和二维位置编码并没有太大的差异。作者随后也对一维位置编码的结果进行了可视化，结果如下图所示：\",\" 上图中是每一个Patch中各位置的位置编码相似性度量，越接近黄色的位置代表越靠近位置编码的中心位置，可以看到，即使是一维位置编码，同样可以比较好地记录二维信息。\",\"class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, representation_size=None, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., embed_layer=None, norm_layer=None, act_layer=None): super(VisionTransformer, self).__init__() # 保存分类任务的类别数 self.num_classes = num_classes # 保存嵌入维度，同时作为特征数量，以保持与其他模型的一致性 self.num_features = self.embed_dim = embed_dim # 只有一个特殊标记（分类标记） self.num_tokens = 1 # 创建图像块嵌入层 self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # 计算图像块的数量 num_patches = self.patch_embed.num_patches # 创建可学习的分类标记 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 创建可学习的位置嵌入 self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim)) # 创建丢弃层，用于位置嵌入后的随机丢弃 self.pos_drop = nn.Dropout(p=drop_ratio) ... # 权重初始化 # 使用截断正态分布初始化位置嵌入 nn.init.trunc_normal_(self.pos_embed, std=0.02) # 使用截断正态分布初始化分类标记 nn.init.trunc_normal_(self.cls_token, std=0.02) ... def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] # 将输入图像进行图像块嵌入 x = self.patch_embed(x) # [B, 196, 768] # [1, 1, 768] -> [B, 1, 768] # 扩展分类标记以匹配输入批次大小 cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将分类标记和图像块嵌入拼接 x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] # 加上位置嵌入并进行随机丢弃 x = self.pos_drop(x + self.pos_embed) ... # 返回分类标记对应的特征 return x[:, 0] def forward(self, x): # 提取输入图像的特征 x = self.forward_features(x) # 通过分类头 x = self.head(x) return x\",\"上面代码实现中使用的是可学习位置嵌入，具体解释如下:\",\"可学习位置嵌入（learnable positional embedding）是指位置嵌入的参数是可以在模型训练过程中通过反向传播算法进行更新的。具体来说，在模型初始化时，位置嵌入会被初始化为一组特定的值（通常是随机初始化或者初始化为零），然后在训练过程中，这些值会根据模型的损失函数不断调整，以使得模型能够学习到最适合当前任务的位置表示。\"]},\"66\":{\"h\":\"5. Encoder\",\"t\":[\"ViT虽然采用的是Transformer Encoder的结构，但是和Transformer原始的Encoder还是有所区别，我将两者的结构进行对比，如下图所示，左侧为Transformer原始的Encoder结构。\",\" 可以看到，大致上两者结构是相同的，主要区别在于Norm层的顺序，原始Transformer的Norm层在多头注意力和前馈网络之后，而ViT将其放到前面，这里的原因，论文里没有做解释。\",\"关于Norm层，ViT仍是采用Transformer中用到Layer Normalization，计算公式如下：\",\"Norm层之后同样是多头注意力层(Multi-Head Attention)，和Transformer中的一样。\",\"后面的MLP是个单独的结构，就是两个线性层+GELU激活函数+Dropout的结构 ：\",\" MLP Block 中第一个线性层把输入特征投影到一个更高维度的空间后，不同特征之间能够进行更多样的组合。这有助于模型发现输入数据中更复杂的模式和关系。第二个线性层再把高维特征映射回原来的维度，这样就可以提取出对最终任务有帮助的特征组合。\",\"单一的线性层只能进行线性变换，其表达能力是有限的。在两个线性层之间通常会插入一个非线性激活函数（如 GELU），这样就能让 MLP 学习到输入数据的非线性特征。第一个线性层将输入特征映射到更高维度的空间，在这个高维空间里，数据的分布更加稀疏，也就为非线性激活函数提供了更多可以学习的特征组合，从而增强了模型的表达能力。\",\"class Block(nn.Module): def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm): super(Block, self).__init__() # 第一个归一化层，对输入进行归一化处理 self.norm1 = norm_layer(dim) # 多头自注意力层 self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio) # DropPath 层，用于随机深度，当 drop_path_ratio 大于 0 时使用，否则使用恒等映射 self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity() # 第二个归一化层，对经过注意力层的输出进行归一化处理 self.norm2 = norm_layer(dim) # 计算 MLP 的隐藏维度 mlp_hidden_dim = int(dim * mlp_ratio) # 创建 MLP 层 self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio) def forward(self, x): # 残差连接：输入加上经过归一化和注意力层处理后的输出 x = x + self.drop_path(self.attn(self.norm1(x))) # 残差连接：输入加上经过归一化和 MLP 层处理后的输出 x = x + self.drop_path(self.mlp(self.norm2(x))) return x\",\"class Mlp(nn.Module): \\\"\\\"\\\" MLP as used in Vision Transformer, MLP-Mixer and related networks \\\"\\\"\\\" def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.): super().__init__() # 如果未指定 out_features，则默认为 in_features out_features = out_features or in_features # 如果未指定 hidden_features，则默认为 in_features hidden_features = hidden_features or in_features # 第一个全连接层，将输入特征映射到隐藏特征空间 self.fc1 = nn.Linear(in_features, hidden_features) # 激活函数层，默认使用 GELU 激活函数 self.act = act_layer() # 第二个全连接层，将隐藏特征映射到输出特征空间 self.fc2 = nn.Linear(hidden_features, out_features) # Dropout 层，用于防止过拟合 self.drop = nn.Dropout(drop) def forward(self, x): # 通过第一个全连接层 x = self.fc1(x) # 通过激活函数层 x = self.act(x) # 应用 Dropout x = self.drop(x) # 通过第二个全连接层 x = self.fc2(x) # 再次应用 Dropout x = self.drop(x) return x\",\"一个block之后维度依然和输入相同，都是197 x 768 ，因此可以堆叠多个block。\"]},\"67\":{\"h\":\"6. 多头自注意力\",\"t\":[\"ViT中的多头自注意力模块实现逻辑和Transformer基本一致，主要的区别就是去掉了Paddding_Mask和Casual_Mask部分相关的掩码逻辑。\",\"下面所给出的代码实现，注意是通过一个线性层来同时计算qkv三个矩阵，这样可以提升计算效率。\",\"class Attention(nn.Module): def __init__(self, dim, # 嵌入层维度 num_heads=8, # 注意力头的数量，默认为8 qkv_bias=False, # 是否在生成Q、K、V时使用偏置，默认为False qk_scale=None, # 缩放因子，用于调整注意力分数，若为None则使用默认值 attn_drop_ratio=0., # 注意力矩阵的丢弃率，默认为0 proj_drop_ratio=0.): # 投影层的丢弃率，默认为0 super(Attention, self).__init__() self.num_heads = num_heads # 保存注意力头的数量 head_dim = dim // num_heads # 计算每个注意力头的维度 self.scale = qk_scale or head_dim ** -0.5 # 确定缩放因子，若qk_scale未指定，则使用默认的缩放因子 # 定义一个线性层，将输入的维度dim映射到dim * 3，用于同时生成查询（Q）、键（K）和值（V） self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) # 定义注意力矩阵的丢弃层，防止过拟合 self.attn_drop = nn.Dropout(attn_drop_ratio) # 定义投影层，将多头注意力的输出进行线性变换 self.proj = nn.Linear(dim, dim) # 定义投影层的丢弃层，防止过拟合 self.proj_drop = nn.Dropout(proj_drop_ratio) # 没有padding_mask, casual_mask def forward(self, x): # [batch_size, num_patches + 1, total_embed_dim] # 获取输入张量x的形状，B为批量大小，N为序列长度（包含分类token），C为输入token的总维度 B, N, C = x.shape # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim] # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head] # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head] # 通过qkv线性层将输入x映射到dim * 3的维度，然后调整形状并重新排列维度 # 下面的3是因为我们用一次矩阵运算得到了拼接在一起的Q,K,V矩阵，这里需要将其分离开来 qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) # [batch_size, num_heads, num_patches + 1, embed_dim_per_head] # 从qkv张量中分离出查询（Q）、键（K）和值（V） # 注意: Q,K,V计算来源相同,因此是自注意力 q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use tensor as tuple) # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1] # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1] # 将Q和K的转置相乘，得到注意力分数矩阵，再乘以缩放因子scale attn = (q @ k.transpose(-2, -1)) * self.scale # 对注意力分数矩阵应用softmax函数，得到注意力权重矩阵 attn = attn.softmax(dim=-1) # 对注意力权重矩阵应用丢弃层，防止过拟合 attn = self.attn_drop(attn) # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head] # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head] # reshape: -> [batch_size, num_patches + 1, total_embed_dim] # 将注意力权重矩阵与V相乘，得到每个注意力头的输出 # 对输出进行维度交换和形状调整，将多个注意力头的输出合并为一个张量 x = (attn @ v).transpose(1, 2).reshape(B, N, C) # 通过投影层对合并后的张量进行线性变换 x = self.proj(x) # 对投影后的结果应用丢弃层，防止过拟合 x = self.proj_drop(x) return x\",\"关于多头注意力机制流程不太清楚的，可以看这篇文章。\"]},\"68\":{\"h\":\"7. MLP Head\",\"t\":[\"在Transformer Encoder输出结果之后，需要再将第一个添加的Class Token提取出来，然后输入到MLP Head进行分类。在论文中，作者先是在ImageNet21K上进行预训练，MLP Head结构由Linear+tanh激活函数+Linear组成，但是迁移到其它数据集训练时，只需要用一个一个Linear即可。\",\"class VisionTransformer(nn.Module): def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, representation_size=None, drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., embed_layer=None, norm_layer=None, act_layer=None): super(VisionTransformer, self).__init__() # 保存分类任务的类别数 self.num_classes = num_classes # 保存嵌入维度，同时作为特征数量，以保持与其他模型的一致性 self.num_features = self.embed_dim = embed_dim # 只有一个特殊标记（分类标记） self.num_tokens = 1 # 如果没有提供归一化层，则使用默认的 LayerNorm，epsilon 为 1e-6 norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6) # 如果没有提供激活函数层，则使用 GELU 激活函数 act_layer = act_layer or nn.GELU # 创建图像块嵌入层 self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim) # 计算图像块的数量 num_patches = self.patch_embed.num_patches # 创建可学习的分类标记 self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 创建可学习的位置嵌入 self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim)) # 创建丢弃层，用于位置嵌入后的随机丢弃 self.pos_drop = nn.Dropout(p=drop_ratio) # 创建Encoder Block块序列 self.blocks = nn.Sequential(*[ Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i], norm_layer=norm_layer, act_layer=act_layer) for i in range(depth) ]) # 创建归一化层 self.norm = norm_layer(embed_dim) ############################# MLP Head ############################################ # 更新特征数量为表示层的维度 self.num_features = representation_size # 创建预输出层，包含一个线性层和一个 Tanh 激活函数 self.pre_logits = nn.Sequential(OrderedDict([ (\\\"fc\\\", nn.Linear(embed_dim, representation_size)), (\\\"act\\\", nn.Tanh()) ])) # 分类头 # 如果类别数大于 0，则创建线性分类头，否则为恒等映射 self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity() ########################################################################### # 权重初始化 # 使用截断正态分布初始化位置嵌入 nn.init.trunc_normal_(self.pos_embed, std=0.02) # 使用截断正态分布初始化分类标记 nn.init.trunc_normal_(self.cls_token, std=0.02) # 应用自定义的权重初始化函数 self.apply(_init_vit_weights) def forward_features(self, x): # [B, C, H, W] -> [B, num_patches, embed_dim] # 将输入图像进行图像块嵌入 x = self.patch_embed(x) # [B, 196, 768] # [1, 1, 768] -> [B, 1, 768] # 扩展分类标记以匹配输入批次大小 cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将分类标记和图像块嵌入拼接 x = torch.cat((cls_token, x), dim=1) # [B, 197, 768] # 加上位置嵌入并进行随机丢弃 x = self.pos_drop(x + self.pos_embed) # 通过Encoder Block块序列 x = self.blocks(x) # 进行归一化 x = self.norm(x) # 返回分类标记对应的特征 -- 先交给预输出层进行处理 return self.pre_logits(x[:, 0]) def forward(self, x): # 提取输入图像的特征 x = self.forward_features(x) # 通过分类头 -- 映射到分类空间中去 x = self.head(x) return x\",\"self.pre_logits 模块可以看作是一个特征预处理模块，它位于最终分类头之前。通过将特征映射到特定的维度并进行非线性变换，该模块可以为后续的分类任务提供更具区分性和稳定性的特征表示，从而提高模型的分类性能。\",\"输出结果之后，再和真实标签做交叉熵损失，这样就可以完成ViT的训练过程。\",\"def train_one_epoch(model, optimizer, data_loader, device, epoch): ... # 遍历数据加载器中的每个批次数据 for step, data in enumerate(data_loader): # 解包数据，得到图像和对应的标签 images, labels = data # 累加当前批次的样本数到总样本数中 sample_num += images.shape[0] # 将图像数据移动到指定设备上，并通过模型进行前向传播，得到预测结果 pred = model(images.to(device)) # 从预测结果中找出每个样本预测概率最大的类别索引 pred_classes = torch.max(pred, dim=1)[1] # 计算预测正确的样本数，并累加到累计正确样本数中 accu_num += torch.eq(pred_classes, labels.to(device)).sum() # 计算预测结果与真实标签之间的交叉熵损失 loss = loss_function(pred, labels.to(device)) # 进行反向传播，计算梯度 loss.backward() ...\"]},\"69\":{\"h\":\"效果对比\",\"t\":[\"在论文中，作者将ViT和之前图像分类领域比较强的ResNet模型进行了对比测试，结果如下：\",\" 可以看到，右图中，作者使用了谷歌制作的JFT-300M数据集，当数据量小于30M时，ViT的效果表现不如ResNet，但是当数据量逐渐增大时，ViT才会慢慢超越ResNet。由此可见ViT工作的局限性，它必须要在超大数据集上进行预训练，然后再拿到其它数据集上做迁移学习，才会有好的效果。\",\"关于ViT模型的不同版本，论文里也做了说明： 其中的Layers就是Transformer Encoder中重复堆叠Encoder Block的次数，Hidden Size就是对应通过Embedding层后每个token的dim（向量的长度），MLP size是Transformer Encoder中MLP Block第一个全连接的节点个数（是Hidden Size的四倍），Heads代表Transformer中Multi-Head Attention的heads数。\",\"在深度学习领域，当提到模型参数量时，“M” 通常是 “million” 的缩写，代表 “百万”。所以参数量为 86M 就意味着模型大约有 86×1000000 = 8600000（八百六十万）个可训练参数。\",\"与之类似的还有 “B”，它是 “billion” 的缩写，代表 “十亿”。例如参数量为 1.2B 就表示模型大约有 1.2×1000000000 = 1200000000（十二亿）个可训练参数。\"]},\"70\":{\"h\":\"注意力可视化\",\"t\":[\"ViT这篇论文长达二十多页，里面包含了非常丰富的成果，其中包括注意力可视化。由于作者是首次将Transformer应用到图像领域，里面包含了注意力机制，那么作者就想把注意力得到的结果(也就是Q-K矩阵乘积)换源到图像上，得到结果如下图所示：\",\"可以看到，模型自动学习到了如果注意画面中的分类主体。\"]},\"71\":{\"h\":\"混合模型探索\",\"t\":[\"在论文的最后，作者又探索了一种混合模型(Hybrid)，就是将传统CNN和Transformer进行结合。\",\"下表中对比了ViT、ResNet和混合模型在不同图像分类数据集上的测试结果，可以看到当Epochs增大时，ResNet和混合模型的效果均不如ViT模型。\",\"混合模型的常见结合方式:\",\"CNN 作为特征提取器，Transformer 作为编码器 \",\"先用 CNN 对输入数据进行初步的特征提取，利用 CNN 的局部特征提取能力快速捕捉图像的底层特征。例如，在图像分类任务中，可以使用预训练的 ResNet 等 CNN 模型提取图像的特征图。\",\"然后将 CNN 提取的特征图转换为序列形式，输入到 Transformer 中进行进一步的处理。Transformer 可以利用其自注意力机制捕捉特征之间的长距离依赖关系，对特征进行更深入的建模。\",\"交错堆叠 CNN 和 Transformer 模块 \",\"在模型架构中，将 CNN 层和 Transformer 层交错堆叠。例如，先经过一层或多层 CNN 进行局部特征提取，然后再经过一层 Transformer 捕捉全局信息，如此反复。这样可以在模型的不同阶段交替利用 CNN 和 Transformer 的优势。\",\"在 Transformer 中引入卷积操作 \",\"在 Transformer 的架构中融入卷积操作，例如在多头自注意力机制或前馈网络中引入卷积层。这样可以为 Transformer 赋予局部特征提取的能力，同时保留其捕捉长距离依赖的优势。\"]},\"72\":{\"h\":\"加载预训练模型\",\"t\":[\"上面已经给出了数据集加载以及ViT模型核心代码实现了，下面我们将进入训练流程；首先说明，本次训练是基于预训练好的ViT-B/16这个模型进行微调，整体结构图如下：\",\"具体为vit_base_patch16_224_in21k这个模型:\",\"vit：代表 Vision Transformer。\",\"base：表示模型的规模。\",\"patch16：意味着在处理图像时，会将输入图像分割成大小为 16×16 像素的图像块（patches）。\",\"224：指的是输入图像的尺寸为 224×224 像素。在预训练和使用该模型时，需要将输入图像调整为这个固定的尺寸。\",\"in21k：该模型是在 ImageNet - 21k 数据集上进行预训练的。ImageNet - 21k 是一个大规模的图像数据集，包含大约 21000 个类别和 1.4 亿张图像。在如此大规模的数据集上进行预训练，模型能够学习到丰富的图像特征和模式，具有较强的泛化能力。\",\"def vit_base_patch16_224_in21k(num_classes: int = 21843, has_logits: bool = True): \\\"\\\"\\\" ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929). ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer. weights ported from official Google JAX impl: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch16_224_in21k-e5005f0a.pth \\\"\\\"\\\" model = VisionTransformer(img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, representation_size=768 if has_logits else None, num_classes=num_classes) return model # 加载预训练好的vit_base_patch16_224_in21k模型权重文件 model = vit_base_patch16_224_in21k(num_classes=5, has_logits=False).to(device) weights_dict = torch.load(args.weights, map_location=device) model.load_state_dict(weights_dict, strict=False)\",\"加载该模型后，训练了10个epoch，验证集上准确率达到了98.5%。整体模型还是比较大的，预训练权重大小为393MB，但是训练速度还是挺快的，因为在代码中有个冻结权重的操作，主干部分全部冻结，仅训练分类头。\",\"for name, para in model.named_parameters(): # 除head, pre_logits外，其他权重全部冻结 if \\\"head\\\" not in name and \\\"pre_logits\\\" not in name: para.requires_grad_(False) else: print(\\\"training {}\\\".format(name))\",\"训练与评估流程的代码为模版代码，考虑篇幅原因，这里不再贴出，大家可以自行拉取项目完整代码进行学习:\",\"https://pan.baidu.com/s/1rkdjdlR37O7gSr9j1mhjBg?pwd=vket\"]},\"73\":{\"h\":\"总结\",\"t\":[\"Vision Transformer证明了使用Transformer结构可以有效处理图像数据，并且取得了与卷积神经网络（CNN）相媲美的效果。\",\"统一多模态的可能性：使用Transformer架构为未来的多模态统一提供了可能性。\",\"图像到文本的桥梁：架起了图像空间到文本空间的桥梁。\",\"ViT核心：如何将二维图像转换为一维时间序列？通过将图像切成小片（Patches），并按行优先排序来实现。\"]},\"74\":{\"h\":\"开源项目\"},\"75\":{\"h\":\"杂谈\"},\"76\":{\"h\":\"\",\"t\":[\"404 Not Found\"]}},\"dirtCount\":0,\"index\":[[\"杂谈\",{\"0\":{\"75\":1}}],[\"架起了图像空间到文本空间的桥梁\",{\"1\":{\"73\":1}}],[\"架构\",{\"1\":{\"54\":1}}],[\"统一多模态的可能性\",{\"1\":{\"73\":1}}],[\"统计正确率\",{\"1\":{\"54\":1}}],[\"考虑篇幅原因\",{\"1\":{\"72\":1}}],[\"除head\",{\"1\":{\"72\":1}}],[\"除了模型本身的应用\",{\"1\":{\"53\":1}}],[\"仅训练分类头\",{\"1\":{\"72\":1}}],[\"仅借鉴了encoder结构\",{\"1\":{\"59\":1}}],[\"具有较强的泛化能力\",{\"1\":{\"72\":1}}],[\"具体为vit\",{\"1\":{\"72\":1}}],[\"具体来说\",{\"1\":{\"65\":1}}],[\"具体解释如下\",{\"1\":{\"65\":1}}],[\"具体方式可以是直接缩放\",{\"1\":{\"62\":1}}],[\"具体代码实现如下\",{\"1\":{\"55\":1}}],[\"具体使用的是\",{\"1\":{\"54\":1}}],[\"具体的实验结果可以参考clip公开的notebook\",{\"1\":{\"53\":1}}],[\"具体位置在\",{\"1\":{\"7\":1}}],[\"亿张图像\",{\"1\":{\"72\":1}}],[\"指的是输入图像的尺寸为\",{\"1\":{\"72\":1}}],[\"意味着在处理图像时\",{\"1\":{\"72\":1}}],[\"赋予局部特征提取的能力\",{\"1\":{\"71\":1}}],[\"捕捉全局信息\",{\"1\":{\"71\":1}}],[\"捕捉不同子空间的信息\",{\"1\":{\"36\":1}}],[\"交错堆叠\",{\"1\":{\"71\":1}}],[\"利用\",{\"1\":{\"71\":1}}],[\"先经过一层或多层\",{\"1\":{\"71\":1}}],[\"先用\",{\"1\":{\"71\":1}}],[\"先交给预输出层进行处理\",{\"1\":{\"68\":1}}],[\"混合模型的常见结合方式\",{\"1\":{\"71\":1}}],[\"混合模型探索\",{\"0\":{\"71\":1}}],[\"换源到图像上\",{\"1\":{\"70\":1}}],[\"里面包含了注意力机制\",{\"1\":{\"70\":1}}],[\"里面包含了非常丰富的成果\",{\"1\":{\"70\":1}}],[\"十二亿\",{\"1\":{\"69\":1}}],[\"十亿\",{\"1\":{\"69\":1}}],[\"八百六十万\",{\"1\":{\"69\":1}}],[\"百万\",{\"1\":{\"69\":1}}],[\"向量的长度\",{\"1\":{\"69\":1}}],[\"向量映射成两个分数\",{\"1\":{\"28\":1}}],[\"才会有好的效果\",{\"1\":{\"69\":1}}],[\"右图中\",{\"1\":{\"69\":1}}],[\"右填充\",{\"1\":{\"8\":1}}],[\"效果对比\",{\"0\":{\"69\":1}}],[\"累加当前批次的样本数到总样本数中\",{\"1\":{\"68\":1}}],[\"解包数据\",{\"1\":{\"68\":1}}],[\"解码器层\",{\"1\":{\"44\":1}}],[\"解码器的最终输出通过一个线性层和\",{\"1\":{\"36\":1}}],[\"解码器\",{\"1\":{\"36\":1,\"45\":1}}],[\"解码成自然语言\",{\"1\":{\"30\":1}}],[\"解码成自然语言文本\",{\"1\":{\"30\":1}}],[\"映射到分类空间中去\",{\"1\":{\"68\":1}}],[\"更新特征数量为表示层的维度\",{\"1\":{\"68\":1}}],[\"没有padding\",{\"1\":{\"67\":1}}],[\"防止过拟合\",{\"1\":{\"67\":4}}],[\"键\",{\"1\":{\"67\":2}}],[\"确定缩放因子\",{\"1\":{\"67\":1}}],[\"确保卷积操作不会重叠\",{\"1\":{\"63\":1}}],[\"投影层的丢弃率\",{\"1\":{\"67\":1}}],[\"若qk\",{\"1\":{\"67\":1}}],[\"若为none则使用默认值\",{\"1\":{\"67\":1}}],[\"若以文本单词数量来衡量\",{\"1\":{\"51\":1}}],[\"缩放因子\",{\"1\":{\"67\":1}}],[\"应用自定义的权重初始化函数\",{\"1\":{\"68\":1}}],[\"应用\",{\"1\":{\"66\":1}}],[\"应该采用什么样的方法来进行训练\",{\"1\":{\"57\":1}}],[\"默认使用\",{\"1\":{\"66\":1}}],[\"默认为0\",{\"1\":{\"67\":2}}],[\"默认为false\",{\"1\":{\"67\":1}}],[\"默认为8\",{\"1\":{\"67\":1}}],[\"默认为\",{\"1\":{\"61\":1,\"63\":5}}],[\"时使用\",{\"1\":{\"66\":1}}],[\"学习到输入数据的非线性特征\",{\"1\":{\"66\":1}}],[\"学习算法需要在所有可能的函数空间中搜索最优模型\",{\"1\":{\"59\":1}}],[\"单一的线性层只能进行线性变换\",{\"1\":{\"66\":1}}],[\"单独用来处理类别信息\",{\"1\":{\"64\":1}}],[\"左侧为transformer原始的encoder结构\",{\"1\":{\"66\":1}}],[\"可学习位置嵌入\",{\"1\":{\"65\":1}}],[\"可以利用其自注意力机制捕捉特征之间的长距离依赖关系\",{\"1\":{\"71\":1}}],[\"可以使用预训练的\",{\"1\":{\"71\":1}}],[\"可以看这篇文章\",{\"1\":{\"67\":1}}],[\"可以看到当epochs增大时\",{\"1\":{\"71\":1}}],[\"可以看到\",{\"1\":{\"65\":2,\"66\":1,\"69\":1,\"70\":1}}],[\"可以看到8个图像\",{\"1\":{\"52\":1}}],[\"可以看到对于要预测的8个图像\",{\"1\":{\"52\":1}}],[\"可以变成一个\",{\"1\":{\"63\":1}}],[\"可以直接通过类名调用\",{\"1\":{\"61\":1}}],[\"可以将搜索范围限制在满足这些性质的模型子空间内\",{\"1\":{\"59\":1}}],[\"可以在下游任务中获得较好的迁移效果\",{\"1\":{\"59\":1}}],[\"可以参考论文\",{\"1\":{\"53\":1}}],[\"可以得到每个类别的预测概率\",{\"1\":{\"52\":1}}],[\"可以概括为以下两个主要步骤\",{\"1\":{\"52\":1}}],[\"加载该模型后\",{\"1\":{\"72\":1}}],[\"加载预训练好的vit\",{\"1\":{\"72\":1}}],[\"加载预训练模型\",{\"0\":{\"72\":1}}],[\"加载模型和处理器\",{\"1\":{\"54\":1,\"56\":1}}],[\"加上位置嵌入并进行随机丢弃\",{\"1\":{\"65\":1,\"68\":1}}],[\"权重初始化\",{\"1\":{\"65\":1,\"68\":1}}],[\"越接近黄色的位置代表越靠近位置编码的中心位置\",{\"1\":{\"65\":1}}],[\"扩展分类标记以匹配输入批次大小\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"保存注意力头的数量\",{\"1\":{\"67\":1}}],[\"保存嵌入维度\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"保存分类任务的类别数\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"保证训练集和验证集的数据处理方式一致\",{\"1\":{\"62\":1}}],[\"保证顺序一致\",{\"1\":{\"61\":1}}],[\"保证随机结果可复现\",{\"1\":{\"61\":1}}],[\"逐步聚合全局信息\",{\"1\":{\"64\":1}}],[\"位置嵌入会被初始化为一组特定的值\",{\"1\":{\"65\":1}}],[\"位置编码的作用是为了记忆输入的语序信息\",{\"1\":{\"65\":1}}],[\"位置编码被添加到\",{\"1\":{\"64\":1}}],[\"位置编码通常使用正弦和余弦函数生成\",{\"1\":{\"36\":1}}],[\"位置编码\",{\"1\":{\"36\":1,\"65\":2}}],[\"位置编码为可学习的矩阵\",{\"1\":{\"11\":1}}],[\"位于序列的开头\",{\"1\":{\"64\":1}}],[\"全局特征表示\",{\"1\":{\"64\":1}}],[\"全连接再加上一个softmax\",{\"1\":{\"38\":1}}],[\"损失函数直接作用于\",{\"1\":{\"64\":1}}],[\"损失值逐渐降低\",{\"1\":{\"64\":1}}],[\"损失计算与反向传播\",{\"1\":{\"64\":1}}],[\"随着训练的进行\",{\"1\":{\"64\":1}}],[\"随机裁剪输入图像\",{\"1\":{\"62\":1}}],[\"收敛\",{\"1\":{\"64\":1}}],[\"被添加到\",{\"1\":{\"64\":1}}],[\"初始时没有任何语义信息\",{\"1\":{\"64\":1}}],[\"初始化\",{\"1\":{\"63\":1,\"64\":1}}],[\"初始化自定义数据集类\",{\"1\":{\"61\":1}}],[\"添加一维位置编码和二维位置编码并没有太大的差异\",{\"1\":{\"65\":1}}],[\"添加位置编码\",{\"0\":{\"65\":1}}],[\"添加\",{\"0\":{\"64\":1}}],[\"添加特殊token标记\",{\"1\":{\"8\":1}}],[\"由此可见vit工作的局限性\",{\"1\":{\"69\":1}}],[\"由此把图片转换为序列的embedding形式\",{\"1\":{\"63\":1}}],[\"由于作者是首次将transformer应用到图像领域\",{\"1\":{\"70\":1}}],[\"由于训练数据量和模型计算量较大\",{\"1\":{\"57\":1}}],[\"由于它们在预训练数据集上采用固定类别数的分类器\",{\"1\":{\"57\":1}}],[\"由于这些文本往往只是一个单词\",{\"1\":{\"53\":1}}],[\"由于数据量巨大\",{\"1\":{\"51\":1}}],[\"由于\",{\"1\":{\"36\":1}}],[\"卷积后剩余的长和宽相乘作为时间维度\",{\"1\":{\"63\":1}}],[\"卷积核个数为768的卷积层来进行实现\",{\"1\":{\"63\":1}}],[\"核心要点\",{\"1\":{\"63\":1}}],[\"维\",{\"1\":{\"63\":1}}],[\"维和第\",{\"1\":{\"63\":1}}],[\"维度为\",{\"1\":{\"61\":1}}],[\"注意\",{\"1\":{\"67\":1}}],[\"注意是通过一个线性层来同时计算qkv三个矩阵\",{\"1\":{\"67\":1}}],[\"注意力可视化\",{\"0\":{\"70\":1}}],[\"注意力矩阵的丢弃率\",{\"1\":{\"67\":1}}],[\"注意力头的数量\",{\"1\":{\"67\":1}}],[\"注意力机制的特性\",{\"1\":{\"64\":1}}],[\"注意力汇聚\",{\"1\":{\"64\":1}}],[\"注意下面的embed\",{\"1\":{\"63\":1}}],[\"注意事项\",{\"1\":{\"30\":1}}],[\"前向传播\",{\"1\":{\"64\":1}}],[\"前向传播函数\",{\"1\":{\"63\":1}}],[\"前馈层\",{\"1\":{\"41\":2,\"44\":2}}],[\"前馈神经网络\",{\"1\":{\"36\":1}}],[\"整体模型还是比较大的\",{\"1\":{\"72\":1}}],[\"整体结构图如下\",{\"1\":{\"72\":1}}],[\"整数\",{\"1\":{\"63\":2}}],[\"整个句子\",{\"1\":{\"35\":1}}],[\"嵌入层维度\",{\"1\":{\"67\":1}}],[\"嵌入交互\",{\"1\":{\"64\":1}}],[\"嵌入的关系\",{\"1\":{\"64\":1}}],[\"嵌入\",{\"1\":{\"64\":1}}],[\"嵌入序列的开头\",{\"1\":{\"64\":1}}],[\"嵌入中\",{\"1\":{\"64\":1}}],[\"嵌入维度\",{\"1\":{\"63\":1,\"64\":1}}],[\"嵌入向量生成过程图\",{\"1\":{\"11\":1}}],[\"边长为该整数\",{\"1\":{\"63\":2}}],[\"步距为16\",{\"1\":{\"63\":1}}],[\"采用了一种更巧妙的解决思路\",{\"1\":{\"63\":1}}],[\"划分后可以得到共个patch\",{\"1\":{\"63\":1}}],[\"每一个向量作为一个单独的输入\",{\"1\":{\"63\":1}}],[\"每一个patch作为一个token\",{\"1\":{\"63\":1}}],[\"每个图像块的尺寸\",{\"1\":{\"63\":1}}],[\"每个patch是三通道的小图片\",{\"1\":{\"63\":1}}],[\"每个头64维\",{\"1\":{\"46\":1}}],[\"每个子层\",{\"1\":{\"36\":1}}],[\"每个位置的输出会通过一个独立的前馈神经网络进行进一步处理\",{\"1\":{\"36\":1}}],[\"每个\",{\"1\":{\"28\":3}}],[\"参数与训练集的归一化参数相同\",{\"1\":{\"62\":1}}],[\"参考decoderlayer\",{\"1\":{\"40\":1}}],[\"验证集上准确率达到了98\",{\"1\":{\"72\":1}}],[\"验证集不需要进行数据增强\",{\"1\":{\"62\":1}}],[\"验证集的预处理转换操作\",{\"1\":{\"62\":1}}],[\"长边按比例缩放\",{\"1\":{\"62\":1}}],[\"归一化层\",{\"1\":{\"63\":1}}],[\"归一化有助于模型更快地收敛\",{\"1\":{\"62\":1}}],[\"归纳偏置能够帮助学习算法缩小搜索范围\",{\"1\":{\"59\":1}}],[\"归纳偏置\",{\"1\":{\"59\":1}}],[\"第一个全连接层\",{\"1\":{\"66\":1}}],[\"第一个归一化层\",{\"1\":{\"66\":1}}],[\"第一个线性层将输入特征映射到更高维度的空间\",{\"1\":{\"66\":1}}],[\"第一个参数\",{\"1\":{\"62\":1}}],[\"第二个全连接层\",{\"1\":{\"66\":1}}],[\"第二个归一化层\",{\"1\":{\"66\":1}}],[\"第二个线性层再把高维特征映射回原来的维度\",{\"1\":{\"66\":1}}],[\"第二个参数\",{\"1\":{\"62\":1}}],[\"范围\",{\"1\":{\"62\":2}}],[\"范围缩放到\",{\"1\":{\"62\":2}}],[\"有助于模型学习到不同方向的特征\",{\"1\":{\"62\":1}}],[\"增加了图像的多样性\",{\"1\":{\"62\":1}}],[\"再和真实标签做交叉熵损失\",{\"1\":{\"68\":1}}],[\"再乘以缩放因子scale\",{\"1\":{\"67\":1}}],[\"再次应用\",{\"1\":{\"66\":1}}],[\"再映射为\",{\"1\":{\"63\":1}}],[\"再来回顾我们的卷积层计算公式\",{\"1\":{\"63\":1}}],[\"再从中心位置裁剪成224x224\",{\"1\":{\"62\":1}}],[\"再进行归一化和标准化处理\",{\"1\":{\"62\":2}}],[\"再对每个选项做分类打分\",{\"1\":{\"32\":1}}],[\"像素的图像块\",{\"1\":{\"72\":1}}],[\"像素\",{\"1\":{\"62\":2,\"72\":1}}],[\"像moco和simclr有所不同\",{\"1\":{\"50\":1}}],[\"静态方法是类中的一种特殊方法\",{\"1\":{\"61\":1}}],[\"6\",{\"0\":{\"67\":1},\"1\":{\"68\":2}}],[\"67b7e751e6b5931a9f45274653f4f653a4e6cdf6\",{\"1\":{\"61\":1}}],[\"62\",{\"1\":{\"54\":1}}],[\"官方实现的default\",{\"1\":{\"61\":1}}],[\"组合后的图像张量和标签张量\",{\"1\":{\"61\":1}}],[\"模块\",{\"1\":{\"71\":1}}],[\"模块可以看作是一个特征预处理模块\",{\"1\":{\"68\":1}}],[\"模式\",{\"1\":{\"61\":1}}],[\"模型能够学习到丰富的图像特征和模式\",{\"1\":{\"72\":1}}],[\"模型提取图像的特征图\",{\"1\":{\"71\":1}}],[\"模型自动学习到了如果注意画面中的分类主体\",{\"1\":{\"70\":1}}],[\"模型被强制学会将图像的有效信息汇聚到\",{\"1\":{\"64\":1}}],[\"模型更新参数\",{\"1\":{\"64\":1}}],[\"模型学会将图像中与分类任务相关的信息汇聚到\",{\"1\":{\"64\":1}}],[\"模型学会如何通过注意力机制将图像的有效信息汇聚到\",{\"1\":{\"64\":2}}],[\"模型计算每个\",{\"1\":{\"64\":1}}],[\"模型难以计算\",{\"1\":{\"63\":1}}],[\"模型的参数是根据这个特定尺寸的输入数据进行优化和学习的\",{\"1\":{\"62\":1}}],[\"模型名称\",{\"1\":{\"56\":1}}],[\"模型会对每个选项分别编码\",{\"1\":{\"32\":1}}],[\"模型需要从中选择最合适的答案\",{\"1\":{\"32\":1}}],[\"模型代表\",{\"1\":{\"30\":1}}],[\"模型那样逐词生成新内容\",{\"1\":{\"30\":1}}],[\"模型\",{\"1\":{\"30\":1,\"51\":1}}],[\"模型架构图\",{\"1\":{\"36\":1}}],[\"模型架构\",{\"0\":{\"9\":1,\"36\":1}}],[\"模型有点大\",{\"1\":{\"7\":1}}],[\"检查图像是否为\",{\"1\":{\"61\":1}}],[\"检查当前目录是否有预训练权重文件\",{\"1\":{\"54\":1,\"56\":1}}],[\"打开指定索引的图像文件\",{\"1\":{\"61\":1}}],[\"打破了这两个领域壁垒\",{\"1\":{\"59\":1}}],[\"返回分类标记对应的特征\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"返回数据集中图像的数量\",{\"1\":{\"61\":1}}],[\"返回值形如\",{\"1\":{\"28\":1}}],[\"包含大约\",{\"1\":{\"72\":1}}],[\"包含一个线性层和一个\",{\"1\":{\"68\":1}}],[\"包含分类token\",{\"1\":{\"67\":1}}],[\"包含图像和对应的标签\",{\"1\":{\"61\":1}}],[\"包含所有图像对应类别的列表\",{\"1\":{\"61\":1}}],[\"包含所有图像文件路径的列表\",{\"1\":{\"61\":1}}],[\"包括\",{\"1\":{\"64\":1}}],[\"包括基于对比学习的方法\",{\"1\":{\"57\":1}}],[\"包括问题和上下文\",{\"1\":{\"28\":1}}],[\"设置柱状图的标题\",{\"1\":{\"61\":1}}],[\"设置y坐标\",{\"1\":{\"61\":1}}],[\"设置x坐标\",{\"1\":{\"61\":1}}],[\"设置的回调方法cllote\",{\"1\":{\"10\":1}}],[\"绘制每种类别个数柱状图\",{\"1\":{\"61\":1}}],[\"否则为恒等映射\",{\"1\":{\"68\":1}}],[\"否则使用恒等映射\",{\"1\":{\"66\":1}}],[\"否则使用\",{\"1\":{\"63\":2}}],[\"否则存入训练集\",{\"1\":{\"61\":1}}],[\"否则答案可能不合理\",{\"1\":{\"30\":1}}],[\"记录该类别的样本数量\",{\"1\":{\"61\":1}}],[\"记录输入长度\",{\"1\":{\"8\":1}}],[\"支持的文件后缀类型\",{\"1\":{\"61\":1}}],[\"存储每个类别的样本总数\",{\"1\":{\"61\":1}}],[\"存储验证集图片对应索引信息\",{\"1\":{\"61\":1}}],[\"存储验证集的所有图片路径\",{\"1\":{\"61\":1}}],[\"存储训练集图片对应索引信息\",{\"1\":{\"61\":1}}],[\"存储训练集的所有图片路径\",{\"1\":{\"61\":1}}],[\"存在一定的噪声\",{\"1\":{\"57\":1}}],[\"排序\",{\"1\":{\"61\":1}}],[\"排序为\",{\"1\":{\"57\":1}}],[\"约4000多个样本\",{\"1\":{\"61\":1}}],[\"共5个类别\",{\"1\":{\"61\":1}}],[\"来帮助大家梳理清楚vision\",{\"1\":{\"60\":1}}],[\"就表示模型大约有\",{\"1\":{\"69\":1}}],[\"就意味着模型大约有\",{\"1\":{\"69\":1}}],[\"就是将传统cnn和transformer进行结合\",{\"1\":{\"71\":1}}],[\"就是两个线性层+gelu激活函数+dropout的结构\",{\"1\":{\"66\":1}}],[\"就是利用一个卷积核大小为16x16\",{\"1\":{\"63\":1}}],[\"就完成了从图片到token之间的转换\",{\"1\":{\"63\":1}}],[\"就有了很多先验信息\",{\"1\":{\"59\":1}}],[\"就点击这里直接下载\",{\"1\":{\"7\":1}}],[\"当提到模型参数量时\",{\"1\":{\"69\":1}}],[\"当数据量小于30m时\",{\"1\":{\"69\":1}}],[\"当\",{\"1\":{\"66\":1}}],[\"当我们在其他任务中使用预训练好的模型时\",{\"1\":{\"62\":1}}],[\"当使用\",{\"1\":{\"61\":1}}],[\"当cnn具有以上两种归纳偏置\",{\"1\":{\"59\":1}}],[\"当拥有足够多的数据进行预训练的时候\",{\"1\":{\"59\":1}}],[\"快速找到合适的模型\",{\"1\":{\"59\":1}}],[\"庖丁解牛vit\",{\"0\":{\"59\":1}}],[\"庖丁解牛blip2\",{\"0\":{\"58\":1},\"1\":{\"58\":1}}],[\"庖丁解牛clip\",{\"0\":{\"48\":1}}],[\"作者又探索了一种混合模型\",{\"1\":{\"71\":1}}],[\"作者使用了谷歌制作的jft\",{\"1\":{\"69\":1}}],[\"作者将vit和之前图像分类领域比较强的resnet模型进行了对比测试\",{\"1\":{\"69\":1}}],[\"作者先是在imagenet21k上进行预训练\",{\"1\":{\"68\":1}}],[\"作者随后也对一维位置编码的结果进行了可视化\",{\"1\":{\"65\":1}}],[\"作者最终选择了对比学习方法来进行训练\",{\"1\":{\"57\":1}}],[\"作为编码器\",{\"1\":{\"71\":1}}],[\"作为特征提取器\",{\"1\":{\"71\":1}}],[\"作为图像的分类预测结果\",{\"1\":{\"52\":1}}],[\"作为答案开始的可能性\",{\"1\":{\"28\":1}}],[\"适当降低训练目标反而可能取得更好的效果\",{\"1\":{\"57\":1}}],[\"两点注意\",{\"1\":{\"61\":1}}],[\"两者的训练效率相差3倍\",{\"1\":{\"57\":1}}],[\"两个句子是否为上下句关系\",{\"1\":{\"25\":1}}],[\"新的问题出现了\",{\"1\":{\"57\":1}}],[\"新的文本\",{\"1\":{\"30\":1}}],[\"相媲美的效果\",{\"1\":{\"73\":1}}],[\"相当于直接输入一篇五万字的文章\",{\"1\":{\"63\":1}}],[\"相比之下\",{\"1\":{\"57\":1}}],[\"相似度如下所示\",{\"1\":{\"52\":1}}],[\"谷歌利用强大的计算能力进行了预训练\",{\"1\":{\"57\":1}}],[\"谷歌采用的弱监督方法与以往方法的一个主要区别在于规模\",{\"1\":{\"57\":1}}],[\"尽管谷歌基于jft\",{\"1\":{\"57\":1}}],[\"尽管clip是一个多模态模型\",{\"1\":{\"51\":1}}],[\"还不如直接预测词袋模型\",{\"1\":{\"57\":1}}],[\"还有另一个方向\",{\"1\":{\"57\":1}}],[\"还在336的分辨率下额外进行了一个周期的微调\",{\"1\":{\"51\":1}}],[\"总体来看\",{\"1\":{\"57\":1}}],[\"总结\",{\"0\":{\"73\":1},\"1\":{\"30\":1}}],[\"预处理这个步骤在论文里并没有详细说明\",{\"1\":{\"62\":1}}],[\"预测图像对应的文本的词袋模型\",{\"1\":{\"57\":1}}],[\"预训练权重大小为393mb\",{\"1\":{\"72\":1}}],[\"预训练模型很容易直接zero\",{\"1\":{\"57\":1}}],[\"预训练模型中\",{\"1\":{\"54\":1}}],[\"预训练模型名称\",{\"1\":{\"54\":1}}],[\"预训练模型下载下来之后\",{\"1\":{\"7\":1}}],[\"预训练与微调\",{\"1\":{\"22\":1}}],[\"预训练\",{\"0\":{\"22\":1}}],[\"实验结果如下表所示\",{\"1\":{\"65\":1}}],[\"实验采用的是花蕊数据集\",{\"1\":{\"61\":1}}],[\"实例化验证数据集\",{\"1\":{\"62\":1}}],[\"实例化训练数据集\",{\"1\":{\"62\":1}}],[\"实际上\",{\"1\":{\"57\":1}}],[\"实现zero\",{\"1\":{\"52\":1}}],[\"能够通过多层\",{\"1\":{\"64\":1}}],[\"能够动态地聚合图像信息\",{\"1\":{\"64\":1}}],[\"能够更好地聚合图像信息\",{\"1\":{\"64\":1}}],[\"能起作用的原因在于\",{\"1\":{\"64\":1}}],[\"能否基于互联网上的大量文本来预训练视觉模型\",{\"1\":{\"57\":1}}],[\"能不能自己生成答案\",{\"1\":{\"30\":1}}],[\"基于自回归或语言掩码的预训练方法已经相对成熟\",{\"1\":{\"57\":1}}],[\"基于上下文编码\",{\"1\":{\"30\":1}}],[\"代表\",{\"1\":{\"69\":2,\"72\":1}}],[\"代表的是模型的基础\",{\"1\":{\"62\":1}}],[\"代理任务通常是辅助进行表征学习\",{\"1\":{\"57\":1}}],[\"代码实现\",{\"0\":{\"29\":1}}],[\"无论是有监督还是自监督方法\",{\"1\":{\"57\":1}}],[\"无法实现zero\",{\"1\":{\"57\":1}}],[\"无法\",{\"1\":{\"30\":1}}],[\"无法处理不在原文中的答案\",{\"1\":{\"30\":1}}],[\"出现这种差异的原因不难理解\",{\"1\":{\"57\":1}}],[\"出现了一些基于自监督的方法\",{\"1\":{\"57\":1}}],[\"出来\",{\"1\":{\"30\":1}}],[\"近年来\",{\"1\":{\"57\":1}}],[\"常见的迁移学习方法是首先在大规模数据集\",{\"1\":{\"57\":1}}],[\"常见的应用场景包括\",{\"1\":{\"31\":1,\"32\":1}}],[\"小结\",{\"0\":{\"57\":1}}],[\"小红书\",{\"1\":{\"0\":1}}],[\"完整代码\",{\"0\":{\"56\":1}}],[\"搜索出来的图片\",{\"1\":{\"55\":1}}],[\"运行上述代码\",{\"1\":{\"55\":1}}],[\"目录获取所有图片路径\",{\"1\":{\"55\":1,\"56\":1}}],[\"目标掩码\",{\"1\":{\"45\":1}}],[\"遍历数据加载器中的每个批次数据\",{\"1\":{\"68\":1}}],[\"遍历获取supported支持的所有文件路径\",{\"1\":{\"61\":1}}],[\"遍历每个文件夹下的文件\",{\"1\":{\"61\":1}}],[\"遍历文件夹\",{\"1\":{\"61\":1}}],[\"遍历\",{\"1\":{\"55\":1,\"56\":1}}],[\"遍历data目录\",{\"1\":{\"55\":1}}],[\"拿到所有图片路径\",{\"1\":{\"55\":1}}],[\"下表中对比了vit\",{\"1\":{\"71\":1}}],[\"下面我们将进入训练流程\",{\"1\":{\"72\":1}}],[\"下面我们将用于图片变换的transforms流水线和上面自定义的mydataset类都封装到dataloader去\",{\"1\":{\"62\":1}}],[\"下面的3是因为我们用一次矩阵运算得到了拼接在一起的q\",{\"1\":{\"67\":1}}],[\"下面所给出的代码实现\",{\"1\":{\"67\":1}}],[\"下面来实际展示一下效果\",{\"1\":{\"55\":1}}],[\"下一个句子预测损失\",{\"1\":{\"26\":1}}],[\"找到与文本最匹配的图片\",{\"1\":{\"55\":1,\"56\":1}}],[\"找出最可能是\",{\"1\":{\"28\":1}}],[\"取出该行中得分最大的那一列\",{\"1\":{\"55\":1}}],[\"取出当前批次的图像列表\",{\"1\":{\"54\":1}}],[\"上面已经给出了数据集加载以及vit模型核心代码实现了\",{\"1\":{\"72\":1}}],[\"上面代码实现中使用的是可学习位置嵌入\",{\"1\":{\"65\":1}}],[\"上面的花卉分类案例相当于使用图像去匹配最合适的文本描述\",{\"1\":{\"55\":1}}],[\"上图中是每一个patch中各位置的位置编码相似性度量\",{\"1\":{\"65\":1}}],[\"上进行预训练\",{\"1\":{\"57\":1,\"62\":1}}],[\"上下文\",{\"1\":{\"28\":3}}],[\"比较两个分类名词是否相等\",{\"1\":{\"54\":1}}],[\"比如一张224x224的图片\",{\"1\":{\"63\":1}}],[\"比如取到了问题部分的内容\",{\"1\":{\"30\":1}}],[\"比如对于一个长度为\",{\"1\":{\"28\":1}}],[\"针对每张图像\",{\"1\":{\"54\":1}}],[\"针对每个图像嵌入向量获取和其相似度最高的分类文本嵌入向量索引下标\",{\"1\":{\"54\":1}}],[\"针对每个图像嵌入向量取出和其相似度最高的那个文本嵌入向量的索引\",{\"1\":{\"54\":1}}],[\"判断预测是否正确\",{\"1\":{\"54\":1}}],[\"获取输入张量x的形状\",{\"1\":{\"67\":1}}],[\"获取输入图像张量的形状\",{\"1\":{\"63\":1}}],[\"获取对应图像的标签\",{\"1\":{\"61\":1}}],[\"获取该类别对应的索引\",{\"1\":{\"61\":1}}],[\"获取其对应的图像嵌入向量列表\",{\"1\":{\"54\":1}}],[\"获取候选分类名列表\",{\"1\":{\"54\":1,\"56\":1}}],[\"获取选项个数\",{\"1\":{\"32\":1}}],[\"子目录名的格式\",{\"1\":{\"54\":1}}],[\"子序列\",{\"1\":{\"30\":1}}],[\"递归遍历目录获取所有图片路径\",{\"1\":{\"54\":1,\"56\":1}}],[\"会将输入图像分割成大小为\",{\"1\":{\"72\":1}}],[\"会将这些输入展平\",{\"1\":{\"32\":1}}],[\"会进行相应的错误提示并返回\",{\"1\":{\"54\":1}}],[\"读取图片并将其转换为合适的格式后\",{\"1\":{\"54\":1}}],[\"该模型是在\",{\"1\":{\"72\":1}}],[\"该模型联合训练一个cnn和文本transformer来预测图像的文本描述\",{\"1\":{\"57\":1}}],[\"该模块可以为后续的分类任务提供更具区分性和稳定性的特征表示\",{\"1\":{\"68\":1}}],[\"该类的作用是将二维图像分割成多个图像块\",{\"1\":{\"63\":1}}],[\"该函数作用是针对给定的图片路径\",{\"1\":{\"54\":1}}],[\"该示例中的任务涉及8个类别\",{\"1\":{\"52\":1}}],[\"方便后续的计算和比较\",{\"1\":{\"54\":1}}],[\"方向研究\",{\"1\":{\"2\":1}}],[\"数量\",{\"1\":{\"63\":1}}],[\"数组转换为\",{\"1\":{\"62\":2}}],[\"数组后\",{\"1\":{\"54\":1}}],[\"数组格式返回\",{\"1\":{\"54\":1}}],[\"数据的分布更加稀疏\",{\"1\":{\"66\":1}}],[\"数据下载\",{\"0\":{\"61\":1}}],[\"数据集上进行预训练的\",{\"1\":{\"72\":1}}],[\"数据集中图像的数量\",{\"1\":{\"61\":1}}],[\"数据集中每一个样本最终都会解析得到一个inputfeatures\",{\"1\":{\"8\":1}}],[\"数据集加载代码\",{\"1\":{\"61\":1}}],[\"数据集下载\",{\"1\":{\"61\":1}}],[\"数据集预处理完后\",{\"1\":{\"8\":1}}],[\"数据预处理\",{\"0\":{\"8\":1}}],[\"函数\",{\"1\":{\"54\":1,\"56\":1,\"61\":1}}],[\"函数生成目标序列的概率分布\",{\"1\":{\"36\":1}}],[\"版本\",{\"1\":{\"54\":1,\"62\":1}}],[\"定义投影层的丢弃层\",{\"1\":{\"67\":1}}],[\"定义投影层\",{\"1\":{\"67\":1}}],[\"定义注意力矩阵的丢弃层\",{\"1\":{\"67\":1}}],[\"定义一个线性层\",{\"1\":{\"67\":1}}],[\"定义一个二维卷积层\",{\"1\":{\"63\":1}}],[\"定义一个字典\",{\"1\":{\"62\":1}}],[\"定义当前目录\",{\"1\":{\"54\":1,\"56\":1}}],[\"定位答案\",{\"1\":{\"30\":1}}],[\"本次训练是基于预训练好的vit\",{\"1\":{\"72\":1}}],[\"本节我们将基于clip预训练模型实现zero\",{\"1\":{\"54\":1}}],[\"本文将通过一个花卉分类的实战案例结合vit原论文\",{\"1\":{\"60\":1}}],[\"本文的transformer使用了self\",{\"1\":{\"35\":1}}],[\"本文基于\",{\"1\":{\"34\":1}}],[\"本文使用的是谷歌的中文预训练模型\",{\"1\":{\"7\":1}}],[\"花卉图片分类\",{\"0\":{\"54\":1}}],[\"缺乏具体的上下文\",{\"1\":{\"53\":1}}],[\"使得\",{\"1\":{\"64\":1}}],[\"使得预训练模型能够直接应用于下游任务\",{\"1\":{\"53\":1}}],[\"使其符合模型的输入要求\",{\"1\":{\"54\":1}}],[\"使用transformer架构为未来的多模态统一提供了可能性\",{\"1\":{\"73\":1}}],[\"使用截断正态分布初始化位置嵌入\",{\"1\":{\"65\":1,\"68\":1}}],[\"使用截断正态分布初始化分类标记\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"使用adamw优化器\",{\"1\":{\"51\":1}}],[\"使用掩码机制确保模型在生成目标序列时只能看到当前及之前的词\",{\"1\":{\"36\":1}}],[\"使用\",{\"1\":{\"30\":1}}],[\"使用了如下逻辑\",{\"1\":{\"28\":1}}],[\"使用pycharm导入项目\",{\"1\":{\"7\":1}}],[\"使用的是tnews数据集\",{\"1\":{\"7\":1}}],[\"提高训练的稳定性\",{\"1\":{\"62\":1}}],[\"提高模型的泛化能力\",{\"1\":{\"62\":1}}],[\"提前做好的假设\",{\"1\":{\"59\":1}}],[\"提示\",{\"1\":{\"53\":1}}],[\"提取的特征图转换为序列形式\",{\"1\":{\"71\":1}}],[\"提取的图像特征则是分类器的输入数据\",{\"1\":{\"52\":1}}],[\"提取输入图像的特征\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"提取图像特征\",{\"1\":{\"52\":1}}],[\"提取文本特征\",{\"1\":{\"52\":1}}],[\"简单来说\",{\"1\":{\"53\":1}}],[\"关于vit模型的不同版本\",{\"1\":{\"69\":1}}],[\"关于多头注意力机制流程不太清楚的\",{\"1\":{\"67\":1}}],[\"关于norm层\",{\"1\":{\"66\":1}}],[\"关于这一领域的详细综述\",{\"1\":{\"53\":1}}],[\"关于我们\",{\"0\":{\"1\":1}}],[\"紧密相关\",{\"1\":{\"53\":1}}],[\"进一步扩展了这个方法来预测n\",{\"1\":{\"57\":1}}],[\"进一步地\",{\"1\":{\"52\":1}}],[\"进行局部特征提取\",{\"1\":{\"71\":1}}],[\"进行反向传播\",{\"1\":{\"68\":1}}],[\"进行归一化\",{\"1\":{\"68\":1}}],[\"进行并行输入\",{\"1\":{\"63\":1}}],[\"进行分类\",{\"1\":{\"31\":1}}],[\"进行解压\",{\"1\":{\"7\":1}}],[\"显然\",{\"1\":{\"52\":1}}],[\"此外\",{\"1\":{\"52\":1,\"53\":1,\"57\":1}}],[\"接下来\",{\"1\":{\"52\":1}}],[\"首先说明\",{\"1\":{\"72\":1}}],[\"首先使用卷积层对输入图像进行处理\",{\"1\":{\"63\":1}}],[\"首先需要对输入图片进行尺寸变化\",{\"1\":{\"62\":1}}],[\"首先我们用data目录充当我们的图片库来源\",{\"1\":{\"55\":1}}],[\"首先\",{\"1\":{\"52\":1,\"54\":1}}],[\"构建描述文本并提取特征\",{\"1\":{\"52\":1}}],[\"构成一个位置序列矩阵\",{\"1\":{\"11\":1}}],[\"直接对图像进行分类\",{\"1\":{\"52\":2}}],[\"推理\",{\"0\":{\"52\":1}}],[\"论文里也做了说明\",{\"1\":{\"69\":1}}],[\"论文里没有做解释\",{\"1\":{\"66\":1}}],[\"论文作者也对其做了实验\",{\"1\":{\"65\":1}}],[\"论文还实验了使用80个不同的prompt进行集成\",{\"1\":{\"53\":1}}],[\"论文指出\",{\"1\":{\"53\":1}}],[\"论文中进行对比实验的clip模型也采用了这一配置\",{\"1\":{\"51\":1}}],[\"论文发现这个模型的效果最佳\",{\"1\":{\"51\":1}}],[\"后面的mlp是个单独的结构\",{\"1\":{\"66\":1}}],[\"后续再进行裁剪操作\",{\"1\":{\"62\":1}}],[\"后三种模型是按照efficientnet的缩放规则对resnet分别放大4倍\",{\"1\":{\"51\":1}}],[\"后的形式\",{\"1\":{\"28\":1}}],[\"另一种思路是在转换后\",{\"1\":{\"65\":1}}],[\"另一种是基于\",{\"1\":{\"51\":1}}],[\"另一个原因是nlp模型可以利用从互联网上收集的大量文本\",{\"1\":{\"57\":1}}],[\"另一个是作为答案结束的可能性\",{\"1\":{\"28\":1}}],[\"选择与图像特征相似度最高的文本所对应的类别\",{\"1\":{\"52\":1}}],[\"选择了一个包含6300万参数的transformer模型\",{\"1\":{\"51\":1}}],[\"选项内容\",{\"1\":{\"32\":1}}],[\"但默认的组合方式可能不满足所有需求\",{\"1\":{\"61\":1}}],[\"但是训练速度还是挺快的\",{\"1\":{\"72\":1}}],[\"但是当数据量逐渐增大时\",{\"1\":{\"69\":1}}],[\"但是当训练数据集不够大的时候\",{\"1\":{\"59\":1}}],[\"但是迁移到其它数据集训练时\",{\"1\":{\"68\":1}}],[\"但是和transformer原始的encoder还是有所区别\",{\"1\":{\"66\":1}}],[\"但是和rnn相比\",{\"1\":{\"35\":1}}],[\"但是实际的代码实现中\",{\"1\":{\"63\":1}}],[\"但是这样造成的一个后果是计算量太庞大\",{\"1\":{\"63\":1}}],[\"但是对于vit这个结构而言\",{\"1\":{\"62\":1}}],[\"但发现这种方法的训练效率\",{\"1\":{\"57\":1}}],[\"但这些模型仍然采用固定类别的softmax分类器进行预训练\",{\"1\":{\"57\":1}}],[\"但存在一定的噪声\",{\"1\":{\"57\":1}}],[\"但实际上\",{\"1\":{\"53\":1}}],[\"但其主要目的是训练可迁移的视觉模型\",{\"1\":{\"51\":1}}],[\"然而\",{\"1\":{\"51\":1,\"57\":3}}],[\"然后再经过一层\",{\"1\":{\"71\":1}}],[\"然后再拿到其它数据集上做迁移学习\",{\"1\":{\"69\":1}}],[\"然后输入到mlp\",{\"1\":{\"68\":1}}],[\"然后调整形状并重新排列维度\",{\"1\":{\"67\":1}}],[\"然后在训练过程中\",{\"1\":{\"65\":1}}],[\"然后在具体的下游任务上进行微调\",{\"1\":{\"57\":1}}],[\"然后将\",{\"1\":{\"71\":1}}],[\"然后将特征图的最后两维展平为一维\",{\"1\":{\"63\":1}}],[\"然后将tf模型转为对应的pytorch版本即可\",{\"1\":{\"7\":1}}],[\"然后进行水平翻转\",{\"1\":{\"62\":1}}],[\"然后计算出和当前文本描述相似度最高的那副图片\",{\"1\":{\"55\":1}}],[\"然后计算每个分类文本对应的文本嵌入向量\",{\"1\":{\"54\":1}}],[\"然后利用模型获取文本特征\",{\"1\":{\"54\":1}}],[\"然后我们读取要预测的图像\",{\"1\":{\"52\":1}}],[\"然后提取了相应的文本特征\",{\"1\":{\"52\":1}}],[\"然后\",{\"1\":{\"52\":1}}],[\"然后从中选出最合适的那个\",{\"1\":{\"32\":1}}],[\"然后启动\",{\"1\":{\"7\":1}}],[\"矩阵中的对角线元素\",{\"1\":{\"51\":1}}],[\"也就为非线性激活函数提供了更多可以学习的特征组合\",{\"1\":{\"66\":1}}],[\"也就是q\",{\"1\":{\"70\":1}}],[\"也就是经过卷积后拼接得到的特征图\",{\"1\":{\"63\":1}}],[\"也就是卷积核的数量\",{\"1\":{\"63\":1}}],[\"也就是一张图像搭配与之对应的文本描述\",{\"1\":{\"50\":1}}],[\"也就是说\",{\"1\":{\"32\":1}}],[\"也可以进行随机裁剪\",{\"1\":{\"62\":1}}],[\"也可以采用视觉transformer模型\",{\"1\":{\"51\":1}}],[\"也需要保持输入图像尺寸与预训练时一致\",{\"1\":{\"62\":1}}],[\"也是其一大亮点\",{\"1\":{\"52\":1}}],[\"训练与评估流程的代码为模版代码\",{\"1\":{\"72\":1}}],[\"训练了10个epoch\",{\"1\":{\"72\":1}}],[\"训练过程中\",{\"1\":{\"64\":1}}],[\"训练目标的引导\",{\"1\":{\"64\":1}}],[\"训练集的预处理转换操作\",{\"1\":{\"62\":1}}],[\"训练效率成为一个至关重要的因素\",{\"1\":{\"57\":1}}],[\"训练效率可以提高4倍\",{\"1\":{\"57\":1}}],[\"训练使用到的数据集和alexnet保持一致\",{\"1\":{\"54\":1}}],[\"训练\",{\"0\":{\"51\":1}}],[\"训练出具有可迁移能力的视觉模型\",{\"1\":{\"49\":1}}],[\"期望模型能够学习到文本和图像之间的匹配关系\",{\"1\":{\"50\":1}}],[\"借助对比学习机制\",{\"1\":{\"50\":1}}],[\"借助字典映射为word\",{\"1\":{\"8\":1}}],[\"与之类似的还有\",{\"1\":{\"69\":1}}],[\"与其他\",{\"1\":{\"64\":1}}],[\"与\",{\"1\":{\"61\":1}}],[\"与计算机视觉\",{\"1\":{\"50\":1}}],[\"与此同时\",{\"1\":{\"49\":1}}],[\"图片切割\",{\"0\":{\"63\":1}}],[\"图片预处理\",{\"0\":{\"62\":1}}],[\"图片库中的图片\",{\"1\":{\"55\":1}}],[\"图片分类\",{\"1\":{\"54\":1,\"56\":1}}],[\"图片分类实战\",{\"1\":{\"54\":1}}],[\"图像到文本的桥梁\",{\"1\":{\"73\":1}}],[\"图像块嵌入层\",{\"1\":{\"64\":1}}],[\"图像块的尺寸\",{\"1\":{\"64\":1}}],[\"图像\",{\"1\":{\"63\":1}}],[\"图像或\",{\"1\":{\"62\":2}}],[\"图像的索引\",{\"1\":{\"61\":1}}],[\"图像预处理的转换操作\",{\"1\":{\"61\":1}}],[\"图像编码器采用了\",{\"1\":{\"54\":1}}],[\"图像特征提取与分类\",{\"1\":{\"52\":1}}],[\"图像对是从互联网收集的\",{\"1\":{\"57\":1}}],[\"图像对为负样本\",{\"1\":{\"51\":1}}],[\"图像对的相似度\",{\"1\":{\"51\":1}}],[\"图像对的训练batch\",{\"1\":{\"51\":1}}],[\"图像对的预训练方法\",{\"1\":{\"50\":1}}],[\"图像对\",{\"1\":{\"50\":1}}],[\"图解transformer\",{\"0\":{\"33\":1},\"1\":{\"33\":1}}],[\"图解bert\",{\"1\":{\"6\":1}}],[\"图解\",{\"0\":{\"6\":1}}],[\"介绍\",{\"0\":{\"50\":1}}],[\"类\",{\"1\":{\"63\":1}}],[\"类似\",{\"1\":{\"49\":1}}],[\"类型\",{\"1\":{\"30\":1}}],[\"则创建线性分类头\",{\"1\":{\"68\":1}}],[\"则使用\",{\"1\":{\"68\":1}}],[\"则使用默认的\",{\"1\":{\"68\":1}}],[\"则使用默认的缩放因子\",{\"1\":{\"67\":1}}],[\"则使用该层进行归一化\",{\"1\":{\"63\":2}}],[\"则默认为\",{\"1\":{\"66\":2}}],[\"则将其转换为\",{\"1\":{\"63\":1}}],[\"则将其设置为\",{\"1\":{\"29\":2}}],[\"则表示图像块是正方形\",{\"1\":{\"63\":1}}],[\"则表示图像是正方形\",{\"1\":{\"63\":1}}],[\"则对图像进行处理\",{\"1\":{\"61\":1}}],[\"则采用了两种不同的架构\",{\"1\":{\"51\":1}}],[\"则是以文本作为监督信号\",{\"1\":{\"49\":1}}],[\"同样可以比较好地记录二维信息\",{\"1\":{\"65\":1}}],[\"同样需要位置编码来记录各图像块之间的位置信息\",{\"1\":{\"65\":1}}],[\"同样\",{\"1\":{\"63\":1}}],[\"同样会将像素值从\",{\"1\":{\"62\":1}}],[\"同样是数据增强的手段\",{\"1\":{\"62\":1}}],[\"同样给计算机视觉领域带来了巨大影响\",{\"1\":{\"49\":1}}],[\"同时保留其捕捉长距离依赖的优势\",{\"1\":{\"71\":1}}],[\"同时作为特征数量\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"同时会将像素值从\",{\"1\":{\"62\":1}}],[\"同时将flower\",{\"1\":{\"54\":1}}],[\"同时最小化个负样本的相似度\",{\"1\":{\"51\":1}}],[\"同时也指运用该方法构建的模型\",{\"1\":{\"50\":1}}],[\"同时计算self\",{\"1\":{\"35\":1}}],[\"月发布的\",{\"1\":{\"49\":1}}],[\"年\",{\"1\":{\"49\":1}}],[\"年可谓是视觉\",{\"1\":{\"49\":1}}],[\"广泛应用于各类计算机视觉任务\",{\"1\":{\"49\":1}}],[\"广播\",{\"1\":{\"46\":1}}],[\"众多基于视觉\",{\"1\":{\"49\":1}}],[\"之前已经有一些研究工作探讨了使用文本作为监督信号来训练视觉模型\",{\"1\":{\"57\":1}}],[\"之后\",{\"1\":{\"49\":1}}],[\"之间的关系\",{\"1\":{\"64\":1}}],[\"之间\",{\"1\":{\"29\":1}}],[\"大家可以自行拉取项目完整代码进行学习\",{\"1\":{\"72\":1}}],[\"大于\",{\"1\":{\"66\":1}}],[\"大致上两者结构是相同的\",{\"1\":{\"66\":1}}],[\"大小的图像\",{\"1\":{\"62\":1}}],[\"大小\",{\"1\":{\"62\":1}}],[\"大大提高学习效率\",{\"1\":{\"59\":1}}],[\"大放异彩的一年\",{\"1\":{\"49\":1}}],[\"大语言模型\",{\"0\":{\"5\":1}}],[\"引言\",{\"0\":{\"49\":1}}],[\"8600000\",{\"1\":{\"69\":1}}],[\"86×1000000\",{\"1\":{\"69\":1}}],[\"86m\",{\"1\":{\"69\":1}}],[\"8\",{\"1\":{\"46\":1}}],[\"8个头\",{\"1\":{\"46\":1}}],[\"源掩码\",{\"1\":{\"45\":1}}],[\"源注意力子层\",{\"1\":{\"44\":2}}],[\"各需要一个\",{\"1\":{\"41\":1,\"44\":1}}],[\"原始transformer的norm层在多头注意力和前馈网络之后\",{\"1\":{\"66\":1}}],[\"原始论文layernorm在最后\",{\"1\":{\"40\":1}}],[\"原理\",{\"0\":{\"60\":1}}],[\"原序列添加特殊token标记图\",{\"1\":{\"8\":1}}],[\"把layernorm放到了前面\",{\"1\":{\"40\":1}}],[\"把这些\",{\"1\":{\"30\":1}}],[\"屏蔽填充部分的信息\",{\"1\":{\"36\":1}}],[\"掩码机制\",{\"1\":{\"36\":1}}],[\"以使得模型能够学习到最适合当前任务的位置表示\",{\"1\":{\"65\":1}}],[\"以保持与其他模型的一致性\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"以保留图像的空间信息\",{\"1\":{\"64\":1}}],[\"以最小化损失\",{\"1\":{\"64\":1}}],[\"以vit\",{\"1\":{\"63\":1}}],[\"以\",{\"1\":{\"62\":1}}],[\"以每个文本描述为一行\",{\"1\":{\"55\":1}}],[\"以下是一个官方给出的clip模型的示例\",{\"1\":{\"52\":1}}],[\"以获取图像特征\",{\"1\":{\"52\":1}}],[\"以生成相应的文本特征\",{\"1\":{\"52\":1}}],[\"以增强性能\",{\"1\":{\"51\":1}}],[\"以加速训练并提高模型的稳定性\",{\"1\":{\"36\":1}}],[\"以便一起处理\",{\"1\":{\"32\":1}}],[\"残差连接\",{\"1\":{\"40\":1,\"66\":2}}],[\"残差连接与层归一化\",{\"1\":{\"36\":1}}],[\"残差链接\",{\"1\":{\"20\":1}}],[\"结果如下\",{\"1\":{\"69\":1}}],[\"结果如下图所示\",{\"1\":{\"65\":1}}],[\"结果发现在imagenet数据集上能够带来3\",{\"1\":{\"53\":1}}],[\"结构\",{\"0\":{\"37\":1,\"39\":1,\"43\":1},\"1\":{\"36\":1}}],[\"结束\",{\"1\":{\"30\":1}}],[\"背景\",{\"0\":{\"35\":1}}],[\"改为\",{\"1\":{\"34\":1}}],[\"需要再将第一个添加的class\",{\"1\":{\"68\":1}}],[\"需要单独将这个token再提取出来\",{\"1\":{\"64\":1}}],[\"需要相对少的数据就可以学习一个比较好的模型\",{\"1\":{\"59\":1}}],[\"需要大量的数据标注\",{\"1\":{\"57\":1}}],[\"需要将输入图像调整为这个固定的尺寸\",{\"1\":{\"72\":1}}],[\"需要将\",{\"1\":{\"34\":1}}],[\"需要注意的一点是\",{\"1\":{\"7\":1}}],[\"环境\",{\"0\":{\"34\":1}}],[\"环境搭建遵从如下步骤即可\",{\"1\":{\"34\":1}}],[\"环境搭建\",{\"0\":{\"7\":1}}],[\"机器翻译实战\",{\"1\":{\"33\":1}}],[\"成\",{\"1\":{\"32\":1}}],[\"成多头格式\",{\"1\":{\"19\":1}}],[\"最近的一些工作采用新的模型架构和预训练方法来从文本学习视觉特征\",{\"1\":{\"57\":1}}],[\"最后交换第\",{\"1\":{\"63\":1}}],[\"最后\",{\"1\":{\"54\":1}}],[\"最后将结果转换为\",{\"1\":{\"54\":1}}],[\"最后重新\",{\"1\":{\"32\":1}}],[\"最大的resnet模型rn50x64需要在592个v100\",{\"1\":{\"51\":1}}],[\"最终\",{\"1\":{\"52\":1,\"64\":2}}],[\"最终答案就是上下文中这两个位置之间的字符串\",{\"1\":{\"30\":1}}],[\"最终每个\",{\"1\":{\"28\":1}}],[\"变成1维度之后就成了50176\",{\"1\":{\"63\":1}}],[\"变成\",{\"1\":{\"32\":1}}],[\"托尔斯泰\",{\"1\":{\"32\":1}}],[\"莎士比亚\",{\"1\":{\"32\":1}}],[\"莎士比亚是英国文学史上最伟大的作家之一\",{\"1\":{\"28\":1}}],[\"歌德\",{\"1\":{\"32\":1}}],[\"雨果\",{\"1\":{\"32\":1}}],[\"个类别和\",{\"1\":{\"72\":1}}],[\"个可训练参数\",{\"1\":{\"69\":2}}],[\"个选项\",{\"1\":{\"32\":1}}],[\"个词之间\",{\"1\":{\"30\":1}}],[\"假设你有一个问题\",{\"1\":{\"32\":1}}],[\"假设原始上下文是\",{\"1\":{\"30\":1}}],[\"你需要为每个选项分别构造一个完整的\",{\"1\":{\"32\":1}}],[\"你将得到一个包含多个元素的\",{\"1\":{\"28\":1}}],[\"任务中\",{\"1\":{\"32\":1}}],[\"任务的本质\",{\"1\":{\"30\":1}}],[\"阅读理解任务\",{\"1\":{\"32\":1}}],[\"多模态模型vit原理与图片分类实战演练\",{\"1\":{\"59\":1}}],[\"多模态模型clip原理与图片分类\",{\"1\":{\"48\":1}}],[\"多模态\",{\"0\":{\"47\":1}}],[\"多项选择题\",{\"1\":{\"32\":1}}],[\"多项选择任务是指给定一个问题和多个候选答案\",{\"1\":{\"32\":1}}],[\"多项选择任务\",{\"0\":{\"32\":1}}],[\"多头自注意力层\",{\"1\":{\"66\":1}}],[\"多头自注意力\",{\"0\":{\"46\":1,\"67\":1}}],[\"多头自注意力机制通过并行计算多个注意力头\",{\"1\":{\"36\":1}}],[\"多头自注意力机制\",{\"1\":{\"21\":1,\"36\":1}}],[\"多头自注意力计算流程图\",{\"1\":{\"19\":1,\"46\":1}}],[\"语义角色标注\",{\"1\":{\"31\":1}}],[\"词性标注\",{\"1\":{\"31\":1}}],[\"命名实体识别\",{\"1\":{\"31\":1}}],[\"并按行优先排序来实现\",{\"1\":{\"73\":1}}],[\"并累加到累计正确样本数中\",{\"1\":{\"68\":1}}],[\"并通过模型进行前向传播\",{\"1\":{\"68\":1}}],[\"并通过线性变换映射到嵌入空间\",{\"1\":{\"64\":1}}],[\"并取得不错的成效\",{\"1\":{\"59\":1}}],[\"并获得该批次图像列表对应的图像嵌入向量列表\",{\"1\":{\"54\":1}}],[\"并改造为a\",{\"1\":{\"54\":1}}],[\"并且取得了与卷积神经网络\",{\"1\":{\"73\":1}}],[\"并且与clip模型的训练数据不完全一致\",{\"1\":{\"53\":1}}],[\"并且在训练过程中采用了一个相对较大的批次大小\",{\"1\":{\"51\":1}}],[\"并计算与文本特征的余弦相似度\",{\"1\":{\"52\":1}}],[\"并将这些图像块嵌入到一个低维向量空间中\",{\"1\":{\"63\":1}}],[\"并将图片展示出来\",{\"1\":{\"55\":1}}],[\"并将其标记为vit\",{\"1\":{\"51\":1}}],[\"并将它们组合成一个批次进行处理\",{\"1\":{\"32\":1}}],[\"并进行l2归一化\",{\"1\":{\"51\":1}}],[\"并用\",{\"1\":{\"30\":1}}],[\"并重新命名为\",{\"1\":{\"7\":1}}],[\"从预测结果中找出每个样本预测概率最大的类别索引\",{\"1\":{\"68\":1}}],[\"从qkv张量中分离出查询\",{\"1\":{\"67\":1}}],[\"从图像的中心位置裁剪出\",{\"1\":{\"62\":1}}],[\"从数据集\",{\"1\":{\"61\":1}}],[\"从数据对的数量来看\",{\"1\":{\"51\":1}}],[\"从任务难度来看\",{\"1\":{\"57\":1}}],[\"从候选分类文本集合中取出其分类名词\",{\"1\":{\"54\":1}}],[\"从而提高模型的分类性能\",{\"1\":{\"68\":1}}],[\"从而增强了模型的表达能力\",{\"1\":{\"66\":1}}],[\"从而增强模型的表达能力\",{\"1\":{\"36\":1}}],[\"从而完成图像分类任务\",{\"1\":{\"64\":1}}],[\"从而可以解决长距离依赖的问题\",{\"1\":{\"35\":1}}],[\"从而很难并行\",{\"1\":{\"35\":1}}],[\"从\",{\"1\":{\"30\":1}}],[\"根据索引获取数据集中的图像和对应的标签\",{\"1\":{\"61\":1}}],[\"根据imagenet数据集上的zero\",{\"1\":{\"57\":1}}],[\"根据文字搜索图片\",{\"1\":{\"55\":1,\"56\":1}}],[\"根据上述计算得到的和其相似度最高的分类文本索引\",{\"1\":{\"54\":1}}],[\"根据任务的分类需求\",{\"1\":{\"52\":1}}],[\"根据decoder的隐状态输出一个词\",{\"1\":{\"38\":1}}],[\"根据编码器的输出生成目标序列\",{\"1\":{\"36\":1}}],[\"根据预测的\",{\"1\":{\"30\":1}}],[\"根据你的理解\",{\"1\":{\"30\":1}}],[\"做分类\",{\"1\":{\"30\":1}}],[\"做的就是这个定位任务\",{\"1\":{\"30\":1}}],[\"为\",{\"1\":{\"68\":1}}],[\"为特定任务动态构建了一个分类器\",{\"1\":{\"52\":1}}],[\"为每个类别创建一个描述性的文本\",{\"1\":{\"52\":1}}],[\"为了充分利用预训练的权重\",{\"1\":{\"62\":1}}],[\"为了弥补数据规模上的差距\",{\"1\":{\"57\":1}}],[\"为了实现文字搜索图像的功能\",{\"1\":{\"55\":1}}],[\"为了训练clip模型\",{\"1\":{\"51\":1}}],[\"为了简单\",{\"1\":{\"40\":1}}],[\"为什么答案来自\",{\"1\":{\"30\":1}}],[\"为当前批次中的每个序列样本生成一个位置序列\",{\"1\":{\"11\":1}}],[\"回答\",{\"1\":{\"30\":1}}],[\"那样我们总共有196个向量\",{\"1\":{\"63\":1}}],[\"那就需要使用生成式模型\",{\"1\":{\"30\":1}}],[\"那么作者就想把注意力得到的结果\",{\"1\":{\"70\":1}}],[\"那么就会得到个文本特征\",{\"1\":{\"52\":1}}],[\"那么\",{\"1\":{\"52\":1,\"63\":1}}],[\"那么clip的训练目标就是最大个正样本的相似度\",{\"1\":{\"51\":1}}],[\"那么对应的就是单词\",{\"1\":{\"30\":1}}],[\"那么可以组合这两个索引得到答案\",{\"1\":{\"28\":1}}],[\"自定义的批量数据处理函数\",{\"1\":{\"61\":1}}],[\"自定义数据集\",{\"1\":{\"61\":1}}],[\"自定义一个mydataset类来封装我们加载得到的数据集\",{\"1\":{\"61\":1}}],[\"自监督方法的优势在于不再需要标注数据\",{\"1\":{\"57\":1}}],[\"自谷歌提出\",{\"1\":{\"49\":1}}],[\"自注意力机制能够捕捉图像中任意两个\",{\"1\":{\"64\":1}}],[\"自注意力机制是\",{\"1\":{\"36\":1}}],[\"自注意力子层\",{\"1\":{\"41\":2,\"44\":2}}],[\"自己写答案\",{\"1\":{\"30\":1}}],[\"自动跳过\",{\"1\":{\"30\":1}}],[\"❌\",{\"1\":{\"30\":2}}],[\"抽取式\",{\"1\":{\"30\":1}}],[\"抽取式问答\",{\"1\":{\"30\":1}}],[\"示例\",{\"1\":{\"30\":1}}],[\"编码器隐藏层输出\",{\"1\":{\"45\":1}}],[\"编码器层\",{\"1\":{\"41\":1}}],[\"编码器\",{\"1\":{\"36\":1}}],[\"编码后可能是\",{\"1\":{\"30\":1}}],[\"编码后\",{\"1\":{\"28\":1}}],[\"编造\",{\"1\":{\"30\":1}}],[\"等\",{\"1\":{\"71\":1}}],[\"等价于n个类别的cross\",{\"1\":{\"51\":1}}],[\"等会被\",{\"1\":{\"30\":1}}],[\"等著名悲剧\",{\"1\":{\"28\":1}}],[\"特殊\",{\"1\":{\"30\":1}}],[\"必须落在上下文部分\",{\"1\":{\"30\":1}}],[\"⚠️\",{\"1\":{\"30\":1}}],[\"经过encoder之后\",{\"1\":{\"64\":1}}],[\"经过处理后的图像块嵌入张量\",{\"1\":{\"63\":1}}],[\"经过卷积层变成\",{\"1\":{\"63\":1}}],[\"经过预处理的图像和对应的标签\",{\"1\":{\"61\":1}}],[\"经过\",{\"1\":{\"30\":1}}],[\"经过一层全连接后的输出\",{\"1\":{\"28\":1}}],[\"举个例子\",{\"1\":{\"30\":1}}],[\"🧪\",{\"1\":{\"30\":1}}],[\"🧩\",{\"1\":{\"30\":1}}],[\"请把这部分原文告诉我\",{\"1\":{\"30\":1}}],[\"ylabel\",{\"1\":{\"61\":1}}],[\"y=v\",{\"1\":{\"61\":1}}],[\"y\",{\"1\":{\"30\":1}}],[\"到第\",{\"1\":{\"30\":1}}],[\"答案必须是原文中的一段文本\",{\"1\":{\"30\":1}}],[\"答案必须是原文中的连续片段\",{\"1\":{\"30\":1}}],[\"答案可以是任意文本\",{\"1\":{\"30\":1}}],[\"答案是原文中的一段\",{\"1\":{\"30\":1}}],[\"答案是否必须在原文中\",{\"1\":{\"30\":1}}],[\"答案应该在这段文字中的第\",{\"1\":{\"30\":1}}],[\"得到结果如下图所示\",{\"1\":{\"70\":1}}],[\"得到预测结果\",{\"1\":{\"68\":1}}],[\"得到图像和对应的标签\",{\"1\":{\"68\":1}}],[\"得到每个注意力头的输出\",{\"1\":{\"67\":1}}],[\"得到每个预测类别的概率值\",{\"1\":{\"52\":1}}],[\"得到注意力权重矩阵\",{\"1\":{\"67\":1}}],[\"得到注意力分数矩阵\",{\"1\":{\"67\":1}}],[\"得到形状为\",{\"1\":{\"63\":3}}],[\"得到的预测概率如下所示\",{\"1\":{\"52\":1}}],[\"得到的就是答案\",{\"1\":{\"30\":1}}],[\"得到相同维度的特征\",{\"1\":{\"51\":1}}],[\"得到概率\",{\"1\":{\"28\":1}}],[\"给出问题和上下文\",{\"1\":{\"30\":1}}],[\"而预训练过程中使用的输入图像尺寸通常固定为\",{\"1\":{\"62\":1}}],[\"而通过引入特定的归纳偏置\",{\"1\":{\"59\":1}}],[\"而vit将其放到前面\",{\"1\":{\"66\":1}}],[\"而vit\",{\"1\":{\"59\":1}}],[\"而vit则选择了三种不同尺寸的模型\",{\"1\":{\"51\":1}}],[\"而这里我们将会反转这个逻辑\",{\"1\":{\"55\":1}}],[\"而最大的vit模型vit\",{\"1\":{\"51\":1}}],[\"而图像编码器\",{\"1\":{\"51\":1,\"52\":1}}],[\"而图像编码器则用于提取图像的特征\",{\"1\":{\"51\":1}}],[\"而剩余的个文本\",{\"1\":{\"51\":1}}],[\"而\",{\"1\":{\"49\":1}}],[\"而不能看到未来的词\",{\"1\":{\"36\":1}}],[\"而不是重新生成一个新的答案\",{\"1\":{\"30\":1}}],[\"而非生成答案\",{\"1\":{\"30\":1}}],[\"而是对输入文本中每个\",{\"1\":{\"30\":1}}],[\"💡\",{\"1\":{\"30\":1}}],[\"🔍\",{\"1\":{\"30\":1}}],[\"📚\",{\"1\":{\"30\":1}}],[\"或者说在于计算能力和数据集的规模\",{\"1\":{\"57\":1}}],[\"或者直接命令行运行\",{\"1\":{\"7\":1}}],[\"或两个句子\",{\"1\":{\"30\":1}}],[\"部分\",{\"1\":{\"30\":1}}],[\"只有一个特殊标记\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"只需要用一个一个linear即可\",{\"1\":{\"68\":1}}],[\"只需要将图像调整到合适的大小\",{\"1\":{\"62\":1}}],[\"只需要去除cls\",{\"1\":{\"17\":1}}],[\"只能对输入文本中的\",{\"1\":{\"30\":1}}],[\"只做抽取式问答\",{\"1\":{\"30\":1}}],[\"只包含\",{\"1\":{\"30\":1}}],[\"详细解释\",{\"1\":{\"30\":1}}],[\"它是\",{\"1\":{\"69\":1}}],[\"它必须要在超大数据集上进行预训练\",{\"1\":{\"69\":1}}],[\"它位于最终分类头之前\",{\"1\":{\"68\":1}}],[\"它与类的实例和类本身都没有直接关联\",{\"1\":{\"61\":1}}],[\"它会将多个样本收集起来形成一个批次\",{\"1\":{\"61\":1}}],[\"它首次将nlp领域火热的transformer模型架构移植到了cv领域\",{\"1\":{\"59\":1}}],[\"它将输入的向量转换为\",{\"1\":{\"54\":1}}],[\"它通过处理器对输入文本进行处理\",{\"1\":{\"54\":1}}],[\"它通过位置编码将序列中词的位置信息注入到输入中\",{\"1\":{\"36\":1}}],[\"它由两个部分组成\",{\"1\":{\"52\":1}}],[\"它比谷歌的jft\",{\"1\":{\"51\":1}}],[\"它代表着一种基于对比文本\",{\"1\":{\"50\":1}}],[\"它可以当成函数调用\",{\"1\":{\"40\":1}}],[\"它允许模型在处理每个词时关注输入序列中的所有词\",{\"1\":{\"36\":1}}],[\"它在编码每一词的时候都能够注意\",{\"1\":{\"35\":1}}],[\"它较难学习到长距离的依赖关系\",{\"1\":{\"35\":1}}],[\"它不具有生成能力\",{\"1\":{\"30\":1}}],[\"它不会\",{\"1\":{\"30\":1}}],[\"它的作用是给定一个完整的句子\",{\"1\":{\"30\":1}}],[\"它的语法是\",{\"1\":{\"29\":1}}],[\"易混淆\",{\"0\":{\"30\":1}}],[\"即使是一维位置编码\",{\"1\":{\"65\":1}}],[\"即网格大小的乘积\",{\"1\":{\"63\":1}}],[\"即图像在水平和垂直方向上分别可以划分的图像块数量\",{\"1\":{\"63\":1}}],[\"即图片上相邻的区域具有相似的特征\",{\"1\":{\"59\":1}}],[\"即每个图像块经过卷积操作后得到的特征向量的维度\",{\"1\":{\"63\":1}}],[\"即一种先验知识\",{\"1\":{\"59\":1}}],[\"即文本和图像可能不完全匹配\",{\"1\":{\"57\":1}}],[\"即基于对比学习的方法\",{\"1\":{\"57\":1}}],[\"即基于文本弱监督来提升性能\",{\"1\":{\"57\":1}}],[\"即为与当前文本描述相似度最高的那副图片\",{\"1\":{\"55\":1}}],[\"即真正属于一对的文本和图像\",{\"1\":{\"51\":1}}],[\"即上图所示的矩阵\",{\"1\":{\"51\":1}}],[\"即\",{\"1\":{\"29\":1,\"30\":1}}],[\"通常在大规模数据集\",{\"1\":{\"62\":1}}],[\"通常是随机初始化或者初始化为零\",{\"1\":{\"65\":1}}],[\"通常是\",{\"1\":{\"29\":1,\"69\":1}}],[\"通过将图像切成小片\",{\"1\":{\"73\":1}}],[\"通过将特征映射到特定的维度并进行非线性变换\",{\"1\":{\"68\":1}}],[\"通过encoder\",{\"1\":{\"68\":1}}],[\"通过投影层对合并后的张量进行线性变换\",{\"1\":{\"67\":1}}],[\"通过qkv线性层将输入x映射到dim\",{\"1\":{\"67\":1}}],[\"通过第二个全连接层\",{\"1\":{\"66\":1}}],[\"通过第一个全连接层\",{\"1\":{\"66\":1}}],[\"通过激活函数层\",{\"1\":{\"66\":1}}],[\"通过分类头\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"通过与其他\",{\"1\":{\"64\":1}}],[\"通过计算损失\",{\"1\":{\"64\":1}}],[\"通过自注意力机制与其他\",{\"1\":{\"64\":1}}],[\"通过多层\",{\"1\":{\"64\":1}}],[\"通过随机裁剪可以增加训练数据的多样性\",{\"1\":{\"62\":1}}],[\"通过一些自动化的手段将web\",{\"1\":{\"57\":1}}],[\"通过模型获取图片的特征嵌入\",{\"1\":{\"54\":1}}],[\"通过这种方式\",{\"1\":{\"52\":1}}],[\"通过softmax函数转换后\",{\"1\":{\"52\":1}}],[\"通过填充掩码\",{\"1\":{\"36\":1}}],[\"通过调用\",{\"1\":{\"28\":1}}],[\"<=\",{\"1\":{\"28\":1}}],[\"分类头\",{\"1\":{\"68\":1}}],[\"分类标记\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"分类任务的类别数\",{\"1\":{\"64\":1}}],[\"分类任务是指对输入文本中的每个\",{\"1\":{\"31\":1}}],[\"分批次预测\",{\"1\":{\"54\":1}}],[\"分批次从图像列表中取出一批图像\",{\"1\":{\"54\":1}}],[\"分别基于lenet\",{\"1\":{\"54\":1}}],[\"分别提取图像特征和文本特征\",{\"1\":{\"51\":1}}],[\"分别是文本编码器\",{\"1\":{\"51\":1}}],[\"分别计算答案起始下标和结束下标预测得到的交叉熵损失\",{\"1\":{\"29\":1}}],[\"分数\",{\"1\":{\"28\":1}}],[\"分为训练\",{\"1\":{\"7\":1}}],[\"都是197\",{\"1\":{\"66\":1}}],[\"都需要进行有监督微调\",{\"1\":{\"57\":1}}],[\"都使用了残差连接和层归一化\",{\"1\":{\"36\":1}}],[\"都单独构造一个\",{\"1\":{\"32\":1}}],[\"都有一个对应的\",{\"1\":{\"28\":1}}],[\"都等于max\",{\"1\":{\"8\":1}}],[\"未归一化\",{\"1\":{\"28\":1}}],[\"一种思路是在转换之前\",{\"1\":{\"65\":1}}],[\"一种朴素的想法就是把一个个像素点拉平\",{\"1\":{\"63\":1}}],[\"一种是平移不变形\",{\"1\":{\"59\":1}}],[\"一种是局部性\",{\"1\":{\"59\":1}}],[\"一种是常用的cnn架构resnet\",{\"1\":{\"51\":1}}],[\"一个block之后维度依然和输入相同\",{\"1\":{\"66\":1}}],[\"一个改进的想法就是把一张图片分成nxn个patch\",{\"1\":{\"63\":1}}],[\"一个批次的数据\",{\"1\":{\"61\":1}}],[\"一个文件夹对应一个类别\",{\"1\":{\"61\":1}}],[\"一个视觉模型和一个文本模型\",{\"1\":{\"52\":1}}],[\"一个是该\",{\"1\":{\"28\":1}}],[\"一名普通但十分热爱探索技术的coder\",{\"1\":{\"2\":1}}],[\"层交错堆叠\",{\"1\":{\"71\":1}}],[\"层和\",{\"1\":{\"71\":1}}],[\"层处理后的输出\",{\"1\":{\"66\":1}}],[\"层\",{\"1\":{\"66\":3}}],[\"层的作用\",{\"1\":{\"28\":1}}],[\"层归一化\",{\"1\":{\"20\":1,\"45\":1}}],[\"qk\",{\"1\":{\"65\":1,\"66\":2,\"67\":2,\"68\":2}}],[\"qkv\",{\"1\":{\"65\":1,\"66\":2,\"67\":8,\"68\":2}}],[\"q\",{\"1\":{\"46\":1,\"67\":5}}],[\"qa\",{\"1\":{\"28\":2,\"29\":2,\"30\":1}}],[\"question\",{\"1\":{\"28\":1}}],[\"query\",{\"1\":{\"19\":7,\"46\":9,\"55\":4,\"56\":4}}],[\"如此反复\",{\"1\":{\"71\":1}}],[\"如下图所示\",{\"1\":{\"66\":1}}],[\"如交叉熵损失\",{\"1\":{\"64\":1}}],[\"如\",{\"1\":{\"62\":1,\"66\":1}}],[\"如局部性和平移不变性\",{\"1\":{\"59\":1}}],[\"如mae和beit\",{\"1\":{\"57\":1}}],[\"如moco和simclr\",{\"1\":{\"57\":1}}],[\"如imagenet\",{\"1\":{\"57\":1}}],[\"如自注意力层和前馈层\",{\"1\":{\"36\":1}}],[\"如翻译后的句子\",{\"1\":{\"36\":1}}],[\"如句子\",{\"1\":{\"36\":1}}],[\"如何将二维图像转换为一维时间序列\",{\"1\":{\"73\":1}}],[\"如何将这个预训练的视觉模型应用到新的任务中呢\",{\"1\":{\"52\":1}}],[\"如何获取答案\",{\"1\":{\"30\":1}}],[\"如何利用\",{\"1\":{\"28\":1}}],[\"如判断哪个是答案的开始\",{\"1\":{\"30\":1}}],[\"如果类别数大于\",{\"1\":{\"68\":1}}],[\"如果未指定\",{\"1\":{\"66\":2}}],[\"如果传入了归一化层类\",{\"1\":{\"63\":1}}],[\"如果传入的是整数\",{\"1\":{\"63\":1}}],[\"如果传入一个归一化层类\",{\"1\":{\"63\":1}}],[\"如果传入一个整数\",{\"1\":{\"63\":2}}],[\"如果定义了图像预处理转换操作\",{\"1\":{\"61\":1}}],[\"如果不是则抛出异常\",{\"1\":{\"61\":1}}],[\"如果该路径在采样的验证集样本中则存入验证集\",{\"1\":{\"61\":1}}],[\"如果没有提供激活函数层\",{\"1\":{\"68\":1}}],[\"如果没有提供归一化层\",{\"1\":{\"68\":1}}],[\"如果没有任何归纳偏置\",{\"1\":{\"59\":1}}],[\"如果没有则下载\",{\"1\":{\"54\":1,\"56\":1}}],[\"如果进一步采用convirt\",{\"1\":{\"57\":1}}],[\"如果在读取图片过程中出现错误\",{\"1\":{\"54\":1}}],[\"如果我们直接使用类别标签作为文本描述\",{\"1\":{\"53\":1}}],[\"如果有个类别\",{\"1\":{\"52\":1}}],[\"如果你希望模型能\",{\"1\":{\"30\":1}}],[\"如果正确答案没有出现在上下文中\",{\"1\":{\"30\":1}}],[\"如果模型预测\",{\"1\":{\"30\":1}}],[\"如果值大于\",{\"1\":{\"29\":1}}],[\"如果值小于\",{\"1\":{\"29\":1}}],[\"如果\",{\"1\":{\"28\":1}}],[\"如果本地不存在\",{\"1\":{\"7\":1}}],[\"不同特征之间能够进行更多样的组合\",{\"1\":{\"66\":1}}],[\"不需要创建类的实例\",{\"1\":{\"61\":1}}],[\"不使用传统的循环或卷积结构\",{\"1\":{\"36\":1}}],[\"不具备生成能力\",{\"1\":{\"30\":1}}],[\"不计入答案\",{\"1\":{\"30\":1}}],[\"不能\",{\"1\":{\"30\":1}}],[\"不能超出上下文范围\",{\"1\":{\"30\":1}}],[\"不能像\",{\"1\":{\"30\":1}}],[\"不是自回归生成器\",{\"1\":{\"30\":1}}],[\"不常用\",{\"1\":{\"28\":1}}],[\"不足长度用padding填充\",{\"1\":{\"8\":1}}],[\"句子级别表示\",{\"1\":{\"28\":1}}],[\"✅\",{\"1\":{\"28\":2,\"30\":4,\"32\":1}}],[\"主干部分全部冻结\",{\"1\":{\"72\":1}}],[\"主要的区别就是去掉了paddding\",{\"1\":{\"67\":1}}],[\"主要区别在于norm层的顺序\",{\"1\":{\"66\":1}}],[\"主要包含encoder和decoder结构\",{\"1\":{\"59\":1}}],[\"主要是因为这些方法难以实现较高的性能\",{\"1\":{\"57\":1}}],[\"主要输出项解释\",{\"1\":{\"28\":1}}],[\"主页\",{\"0\":{\"0\":1}}],[\"输出结果之后\",{\"1\":{\"68\":1}}],[\"输出通道\",{\"1\":{\"63\":1}}],[\"输出层\",{\"1\":{\"36\":1}}],[\"输出后\",{\"1\":{\"32\":1}}],[\"输出解释\",{\"1\":{\"28\":1}}],[\"输出做问答预测\",{\"1\":{\"28\":1}}],[\"输出\",{\"1\":{\"28\":1}}],[\"输入到\",{\"1\":{\"71\":1}}],[\"输入到mlp\",{\"1\":{\"64\":1}}],[\"输入加上经过归一化和\",{\"1\":{\"66\":1}}],[\"输入加上经过归一化和注意力层处理后的输出\",{\"1\":{\"66\":1}}],[\"输入图像被分割成\",{\"1\":{\"64\":1}}],[\"输入图像的通道数\",{\"1\":{\"63\":1,\"64\":1}}],[\"输入图像的尺寸\",{\"1\":{\"63\":1,\"64\":1}}],[\"输入encoder的最左侧部分添加了一个0\",{\"1\":{\"64\":1}}],[\"输入的图像张量\",{\"1\":{\"63\":1}}],[\"输入的图片尺寸必须为224x224\",{\"1\":{\"62\":1}}],[\"输入的图片尺寸并不是自定义的\",{\"1\":{\"62\":1}}],[\"输入为\",{\"1\":{\"63\":1}}],[\"输入image\",{\"1\":{\"52\":1}}],[\"输入\",{\"1\":{\"45\":1}}],[\"输入序列\",{\"1\":{\"32\":1}}],[\"输入与输出的关系\",{\"1\":{\"30\":1}}],[\"输入会变成如下结构\",{\"1\":{\"28\":1}}],[\"输入格式\",{\"1\":{\"28\":1}}],[\"输入数据格式\",{\"1\":{\"8\":1}}],[\"麦克白\",{\"1\":{\"28\":1}}],[\"他写了包括\",{\"1\":{\"28\":1}}],[\"哈姆雷特\",{\"1\":{\"28\":2,\"32\":1}}],[\"谁写了\",{\"1\":{\"28\":1,\"32\":1}}],[\"例如在多头自注意力机制或前馈网络中引入卷积层\",{\"1\":{\"71\":1}}],[\"例如参数量为\",{\"1\":{\"69\":1}}],[\"例如谷歌的bit和vit基于jft\",{\"1\":{\"57\":1}}],[\"例如2017年的那篇工作只在imagenet上实现了11\",{\"1\":{\"57\":1}}],[\"例如virtex基于transformer的语言模型\",{\"1\":{\"57\":1}}],[\"例如openai的gpt\",{\"1\":{\"57\":1}}],[\"例如\",{\"1\":{\"28\":1,\"52\":1,\"53\":1,\"57\":1,\"59\":1,\"71\":2}}],[\"问答系统中的候选答案选择\",{\"1\":{\"32\":1}}],[\"问答任务\",{\"0\":{\"28\":1}}],[\"问题来了\",{\"1\":{\"57\":1}}],[\"问题\",{\"1\":{\"28\":3,\"30\":1,\"32\":2}}],[\"典型的输入是一个包含\",{\"1\":{\"28\":1}}],[\"在如此大规模的数据集上进行预训练\",{\"1\":{\"72\":1}}],[\"在预训练和使用该模型时\",{\"1\":{\"72\":1}}],[\"在预训练时\",{\"1\":{\"62\":1}}],[\"在模型架构中\",{\"1\":{\"71\":1}}],[\"在模型初始化时\",{\"1\":{\"65\":1}}],[\"在论文的最后\",{\"1\":{\"71\":1}}],[\"在论文中\",{\"1\":{\"51\":1,\"68\":1,\"69\":1}}],[\"在深度学习领域\",{\"1\":{\"69\":1}}],[\"在两个线性层之间通常会插入一个非线性激活函数\",{\"1\":{\"66\":1}}],[\"在transformer\",{\"1\":{\"68\":1}}],[\"在transformer中\",{\"1\":{\"65\":1}}],[\"在transformer模型中\",{\"1\":{\"59\":1}}],[\"在每一层\",{\"1\":{\"64\":1}}],[\"在上面的结构图中可以看到\",{\"1\":{\"64\":1}}],[\"在上下文中找到最可能的答案起始位置和结束位置\",{\"1\":{\"30\":1}}],[\"在代码中\",{\"1\":{\"63\":1}}],[\"在柱状图上添加数值标签\",{\"1\":{\"61\":1}}],[\"在图像分类任务中\",{\"1\":{\"59\":1,\"71\":1}}],[\"在图文检索中\",{\"1\":{\"54\":1}}],[\"在这个高维空间里\",{\"1\":{\"66\":1}}],[\"在这个过程中\",{\"1\":{\"52\":1}}],[\"在这种情况下\",{\"1\":{\"57\":1}}],[\"在nlp领域\",{\"1\":{\"57\":1}}],[\"在迁移到其他数据集时也需要加上新的分类器进行有监督训练\",{\"1\":{\"57\":1}}],[\"在迁移到下游任务时\",{\"1\":{\"57\":1}}],[\"在计算机视觉领域\",{\"1\":{\"57\":1}}],[\"在imagenet数据集上可以提升1\",{\"1\":{\"53\":1}}],[\"在之前的例子中\",{\"1\":{\"53\":1}}],[\"在实际应用中可以选用常见的卷积神经网络\",{\"1\":{\"51\":1}}],[\"在实现时可采用自然语言处理\",{\"1\":{\"51\":1}}],[\"在训练过程中\",{\"1\":{\"50\":1}}],[\"在输入序列长度不一致时\",{\"1\":{\"36\":1}}],[\"在解码器中\",{\"1\":{\"36\":1}}],[\"在自注意力机制之后\",{\"1\":{\"36\":1}}],[\"在前向传播中\",{\"1\":{\"32\":1}}],[\"在分类任务中更有用\",{\"1\":{\"28\":1}}],[\"在问答任务中一般不会使用这个输出\",{\"1\":{\"28\":1}}],[\"在问答任务中\",{\"1\":{\"28\":1}}],[\"在使用clip模型进行zero\",{\"1\":{\"53\":1}}],[\"在使用\",{\"1\":{\"28\":1}}],[\"在\",{\"1\":{\"28\":2,\"30\":1,\"32\":1,\"49\":1,\"54\":1,\"62\":1,\"71\":2}}],[\"在返回前进行预处理\",{\"1\":{\"10\":1}}],[\"和值\",{\"1\":{\"67\":2}}],[\"和transformer中的一样\",{\"1\":{\"66\":1}}],[\"和基于图像掩码的方法\",{\"1\":{\"57\":1}}],[\"和图像编码器\",{\"1\":{\"51\":1}}],[\"和\",{\"1\":{\"26\":1,\"28\":2,\"29\":2,\"30\":1,\"41\":1,\"49\":3,\"63\":1,\"71\":2}}],[\"隐藏层输出\",{\"1\":{\"26\":1}}],[\"合并头结果\",{\"1\":{\"19\":1}}],[\"k矩阵乘积\",{\"1\":{\"70\":1}}],[\"k=int\",{\"1\":{\"61\":1}}],[\"kernel\",{\"1\":{\"63\":2}}],[\"keepdims=true\",{\"1\":{\"54\":1,\"56\":1}}],[\"keepdim=true\",{\"1\":{\"52\":2}}],[\"keep\",{\"1\":{\"31\":1}}],[\"key\",{\"1\":{\"19\":7,\"46\":7,\"61\":2}}],[\"k\",{\"1\":{\"19\":2,\"46\":9,\"61\":2,\"67\":7}}],[\"xlabel\",{\"1\":{\"61\":1}}],[\"x=i\",{\"1\":{\"61\":1}}],[\"xticks\",{\"1\":{\"61\":1}}],[\"x\",{\"1\":{\"19\":7,\"30\":1,\"38\":2,\"40\":3,\"41\":8,\"42\":4,\"44\":12,\"45\":4,\"46\":7,\"63\":8,\"64\":14,\"65\":15,\"66\":21,\"67\":9,\"68\":19}}],[\"激活函数层\",{\"1\":{\"66\":1}}],[\"激活函数\",{\"1\":{\"13\":2,\"66\":1,\"68\":2}}],[\"让模型自己学会区分不同的句子\",{\"1\":{\"11\":1}}],[\"3的维度\",{\"1\":{\"67\":1}}],[\"396\",{\"1\":{\"54\":1}}],[\"336\",{\"1\":{\"51\":1}}],[\"32768\",{\"1\":{\"51\":1}}],[\"32\",{\"1\":{\"51\":1}}],[\"300m数据集\",{\"1\":{\"69\":1}}],[\"300m数据集的规模达到了上亿级别\",{\"1\":{\"57\":1}}],[\"300m数据集取得了较好的结果\",{\"1\":{\"57\":1}}],[\"300m数据集是谷歌从互联网上收集的\",{\"1\":{\"57\":1}}],[\"300m数据集来预训练模型在imagenet上取得sota\",{\"1\":{\"57\":1}}],[\"300m数据集还要多出1亿对\",{\"1\":{\"51\":1}}],[\"3072\",{\"1\":{\"13\":2}}],[\"3\",{\"0\":{\"64\":1},\"1\":{\"11\":1,\"19\":2,\"44\":1,\"46\":1,\"52\":1,\"53\":1,\"56\":1,\"57\":1,\"61\":1,\"63\":3,\"67\":7}}],[\"3d\",{\"0\":{\"4\":1},\"1\":{\"2\":1}}],[\"按比例随机采样验证样本\",{\"1\":{\"61\":1}}],[\"按照16x16大小的patch进行划分\",{\"1\":{\"63\":1}}],[\"按照余弦相似度的数学公式来计算两者的相似度数值\",{\"1\":{\"54\":1}}],[\"按照最大相似度\",{\"1\":{\"52\":1}}],[\"按照本批次序列中最大长度进行截断\",{\"1\":{\"10\":1}}],[\"按序执行以下命令完成环境搭建\",{\"1\":{\"7\":1}}],[\"计算梯度\",{\"1\":{\"68\":1}}],[\"计算预测结果与真实标签之间的交叉熵损失\",{\"1\":{\"68\":1}}],[\"计算预测正确的样本数\",{\"1\":{\"68\":1}}],[\"计算每个注意力头的维度\",{\"1\":{\"67\":1}}],[\"计算\",{\"1\":{\"66\":1}}],[\"计算公式如下\",{\"1\":{\"66\":1}}],[\"计算图像块的数量\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"计算图像块的总数\",{\"1\":{\"63\":1}}],[\"计算图像嵌入向量列表中每个图像对应的嵌入向量和传入的分类文本嵌入向量的余弦相似度\",{\"1\":{\"54\":1}}],[\"计算网格大小\",{\"1\":{\"63\":1}}],[\"计算这批图像嵌入向量中每一个图像嵌入向量和我们所有分类文本嵌入向量的余弦相似度\",{\"1\":{\"54\":1}}],[\"计算余弦相似度\",{\"1\":{\"52\":1}}],[\"计算缩放的余弦相似度\",{\"1\":{\"51\":1}}],[\"计算交叉熵损失\",{\"1\":{\"29\":1}}],[\"计算掩码语言损失\",{\"1\":{\"26\":1}}],[\"计算当前批次中所有序列的实际最大长度\",{\"1\":{\"10\":1}}],[\"计算需要填充的长度\",{\"1\":{\"8\":1}}],[\"将多个注意力头的输出合并为一个张量\",{\"1\":{\"67\":1}}],[\"将多头注意力的输出进行线性变换\",{\"1\":{\"67\":1}}],[\"将注意力权重矩阵与v相乘\",{\"1\":{\"67\":1}}],[\"将q和k的转置相乘\",{\"1\":{\"67\":1}}],[\"将隐藏特征映射到输出特征空间\",{\"1\":{\"66\":1}}],[\"将分类标记和图像块嵌入拼接\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"将卷积后的通道维数作为embedding的维度\",{\"1\":{\"63\":1}}],[\"将其展平就变成了一个长度为768的向量\",{\"1\":{\"63\":1}}],[\"将输入的维度dim映射到dim\",{\"1\":{\"67\":1}}],[\"将输入特征映射到隐藏特征空间\",{\"1\":{\"66\":1}}],[\"将输入图像进行图像块嵌入\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"将输入图片\",{\"1\":{\"63\":1}}],[\"将输入序列\",{\"1\":{\"36\":1}}],[\"将图像数据移动到指定设备上\",{\"1\":{\"68\":1}}],[\"将图像的短边缩放为\",{\"1\":{\"62\":1}}],[\"将图像元组堆叠成一个四维张量\",{\"1\":{\"61\":1}}],[\"将\",{\"1\":{\"62\":2,\"63\":1,\"71\":1}}],[\"将裁剪后的图像调整为\",{\"1\":{\"62\":1}}],[\"将标签元组转换为一个一维张量\",{\"1\":{\"61\":1}}],[\"将一个批次的数据拆分为图像和标签两个元组\",{\"1\":{\"61\":1}}],[\"将横坐标0\",{\"1\":{\"61\":1}}],[\"将这个问题转化为一个多标签分类任务\",{\"1\":{\"57\":1}}],[\"将数据下载到当前项目目录下\",{\"1\":{\"54\":1}}],[\"将待分类的图像输入到图像编码器\",{\"1\":{\"52\":1}}],[\"将个文本特征和个图像特征两两组合\",{\"1\":{\"51\":1}}],[\"将选项展平\",{\"1\":{\"32\":1}}],[\"将每个\",{\"1\":{\"28\":1}}],[\"将inputfeatures\",{\"1\":{\"8\":1}}],[\"将模型放入到仓库对应位置\",{\"1\":{\"7\":1}}],[\"含special\",{\"1\":{\"8\":1}}],[\"是hidden\",{\"1\":{\"69\":1}}],[\"是否在生成q\",{\"1\":{\"67\":1}}],[\"是否能生成新文本\",{\"1\":{\"30\":1}}],[\"是指位置嵌入的参数是可以在模型训练过程中通过反向传播算法进行更新的\",{\"1\":{\"65\":1}}],[\"是图像块的总数\",{\"1\":{\"63\":1}}],[\"是图像宽度\",{\"1\":{\"63\":1}}],[\"是图像高度\",{\"1\":{\"63\":1}}],[\"是图像每个通道的标准差\",{\"1\":{\"62\":1}}],[\"是图像每个通道的均值\",{\"1\":{\"62\":1}}],[\"是通道数\",{\"1\":{\"63\":1}}],[\"是批量大小\",{\"1\":{\"63\":1}}],[\"是卷积核的步长\",{\"1\":{\"63\":1}}],[\"是卷积核的大小\",{\"1\":{\"63\":1}}],[\"是输出通道数\",{\"1\":{\"63\":1}}],[\"是输入通道数\",{\"1\":{\"63\":1}}],[\"是基于文本输入来生成图像的模型\",{\"1\":{\"49\":1}}],[\"是一种基于自注意力机制\",{\"1\":{\"36\":1}}],[\"是一个大规模的图像数据集\",{\"1\":{\"72\":1}}],[\"是一个随机初始化的向量\",{\"1\":{\"64\":1}}],[\"是一个\",{\"1\":{\"30\":2}}],[\"是一个线性层\",{\"1\":{\"28\":1}}],[\"是编码器模型\",{\"1\":{\"30\":1}}],[\"是模型预测出的答案的起始和结束位置\",{\"1\":{\"30\":1}}],[\"是模型最后一层所有\",{\"1\":{\"28\":1}}],[\"是答案终点的得分\",{\"1\":{\"28\":1}}],[\"是答案起点的得分\",{\"1\":{\"28\":1}}],[\"是\",{\"1\":{\"28\":1,\"29\":1,\"61\":1}}],[\"是序列实际长度\",{\"1\":{\"8\":1}}],[\"是当前文本对应的类别标签\",{\"1\":{\"8\":1}}],[\"填充过程图\",{\"1\":{\"8\":1}}],[\"填充token对应0\",{\"1\":{\"8\":1}}],[\"真实token对应1\",{\"1\":{\"8\":1}}],[\"生成类别名称以及对应的数字索引\",{\"1\":{\"61\":1}}],[\"生成文本嵌入\",{\"1\":{\"54\":1,\"56\":1}}],[\"生成的文本特征相当于分类器的权重\",{\"1\":{\"52\":1}}],[\"生成式\",{\"1\":{\"30\":2}}],[\"生成一个上下文相关的表示\",{\"1\":{\"30\":1}}],[\"生成\",{\"1\":{\"30\":1}}],[\"生成注意力掩码\",{\"1\":{\"8\":1}}],[\"生成padding部分的mask列表\",{\"1\":{\"8\":1}}],[\"超长截断\",{\"1\":{\"8\":1}}],[\"创建预输出层\",{\"1\":{\"68\":1}}],[\"创建归一化层\",{\"1\":{\"68\":1}}],[\"创建encoder\",{\"1\":{\"68\":1}}],[\"创建\",{\"1\":{\"66\":1}}],[\"创建丢弃层\",{\"1\":{\"65\":1,\"68\":1}}],[\"创建可学习的位置嵌入\",{\"1\":{\"65\":1,\"68\":1}}],[\"创建可学习的分类标记\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"创建图像块嵌入层\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"创建用以区分special\",{\"1\":{\"8\":1}}],[\"创建句子辨识列表\",{\"1\":{\"8\":1}}],[\"|\",{\"1\":{\"8\":2}}],[\"false\",{\"1\":{\"72\":1}}],[\"facial\",{\"1\":{\"52\":1}}],[\"fc\",{\"1\":{\"68\":1}}],[\"fc2\",{\"1\":{\"66\":2}}],[\"fc1\",{\"1\":{\"66\":2}}],[\"fct\",{\"1\":{\"17\":4,\"26\":3,\"29\":3,\"31\":3,\"32\":2}}],[\"flatten\",{\"1\":{\"63\":1}}],[\"flag\",{\"1\":{\"52\":1}}],[\"flowerclassify\",{\"1\":{\"54\":1,\"56\":1}}],[\"flower\",{\"1\":{\"54\":4,\"56\":2,\"61\":8}}],[\"float\",{\"1\":{\"52\":2,\"61\":1}}],[\"f\",{\"1\":{\"38\":1,\"51\":4,\"54\":7,\"55\":4,\"56\":11}}],[\"feed\",{\"1\":{\"36\":1,\"41\":4,\"44\":5}}],[\"features=none\",{\"1\":{\"66\":2}}],[\"features=mlp\",{\"1\":{\"66\":1}}],[\"features=dim\",{\"1\":{\"66\":1}}],[\"features\",{\"1\":{\"8\":1,\"52\":10,\"54\":4,\"56\":4,\"57\":1,\"64\":3,\"65\":3,\"66\":15,\"68\":5}}],[\"france\",{\"1\":{\"30\":2}}],[\"from\",{\"1\":{\"8\":2,\"19\":1,\"46\":1,\"54\":2,\"55\":2,\"56\":8,\"57\":2,\"61\":2,\"72\":2}}],[\"function\",{\"1\":{\"19\":1,\"68\":1}}],[\"fp16\",{\"1\":{\"16\":1}}],[\"fn=val\",{\"1\":{\"62\":1}}],[\"fn=train\",{\"1\":{\"62\":1}}],[\"fn=collate\",{\"1\":{\"10\":1}}],[\"fn负责对返回的一个batch\",{\"1\":{\"10\":1}}],[\"fn\",{\"1\":{\"10\":2,\"13\":3,\"23\":3,\"61\":2,\"62\":2}}],[\"find\",{\"1\":{\"55\":2,\"56\":2}}],[\"finetune\",{\"1\":{\"51\":1}}],[\"final\",{\"1\":{\"46\":1}}],[\"filterwarnings\",{\"1\":{\"56\":1}}],[\"fill\",{\"1\":{\"46\":1}}],[\"files\",{\"1\":{\"54\":2,\"56\":2}}],[\"file\",{\"1\":{\"7\":1,\"54\":5,\"56\":5,\"61\":2}}],[\"figure\",{\"1\":{\"41\":1,\"44\":1,\"46\":1}}],[\"first\",{\"1\":{\"8\":2,\"15\":3}}],[\"found\",{\"1\":{\"55\":1,\"56\":1,\"61\":1,\"76\":1}}],[\"follow\",{\"1\":{\"41\":1,\"44\":1}}],[\"following\",{\"1\":{\"8\":1}}],[\"for\",{\"1\":{\"14\":2,\"19\":5,\"24\":1,\"41\":1,\"42\":1,\"44\":1,\"45\":1,\"46\":1,\"52\":1,\"54\":6,\"55\":2,\"56\":8,\"61\":9,\"68\":2,\"72\":1}}],[\"forward\",{\"1\":{\"11\":1,\"13\":3,\"14\":1,\"15\":1,\"16\":1,\"17\":1,\"19\":2,\"20\":1,\"21\":1,\"23\":1,\"24\":1,\"25\":1,\"26\":1,\"29\":1,\"31\":1,\"32\":1,\"36\":1,\"37\":1,\"38\":1,\"40\":1,\"41\":5,\"42\":1,\"44\":6,\"45\":1,\"46\":1,\"63\":1,\"64\":3,\"65\":3,\"66\":2,\"67\":1,\"68\":3}}],[\"format\",{\"1\":{\"8\":1,\"61\":5,\"72\":1}}],[\"=>\",{\"1\":{\"46\":1}}],[\"==\",{\"1\":{\"10\":1,\"13\":1,\"17\":1,\"23\":1,\"30\":1,\"31\":1,\"34\":1,\"46\":2,\"54\":1,\"56\":1}}],[\"=\",{\"1\":{\"8\":14,\"10\":7,\"11\":16,\"13\":17,\"14\":2,\"15\":5,\"16\":10,\"17\":14,\"19\":25,\"20\":6,\"21\":4,\"23\":7,\"24\":5,\"25\":4,\"26\":11,\"28\":9,\"29\":17,\"30\":2,\"31\":16,\"32\":17,\"37\":5,\"38\":1,\"40\":2,\"41\":6,\"42\":3,\"44\":9,\"45\":4,\"46\":15,\"51\":9,\"52\":10,\"54\":40,\"55\":9,\"56\":46,\"61\":25,\"62\":6,\"63\":11,\"64\":12,\"65\":15,\"66\":19,\"67\":16,\"68\":28,\"69\":2,\"72\":5}}],[\"+=\",{\"1\":{\"54\":1,\"56\":1,\"68\":2}}],[\"+\",{\"1\":{\"8\":21,\"11\":4,\"13\":1,\"17\":2,\"19\":3,\"20\":2,\"24\":1,\"26\":2,\"29\":2,\"30\":1,\"31\":1,\"32\":6,\"40\":4,\"41\":1,\"44\":4,\"45\":1,\"51\":1,\"52\":1,\"54\":2,\"56\":2,\"61\":1,\"65\":2,\"66\":2,\"67\":11,\"68\":2}}],[\"w`代表输出特征图的宽和高\",{\"1\":{\"63\":1}}],[\"width\",{\"1\":{\"61\":1}}],[\"with\",{\"1\":{\"8\":3,\"45\":1,\"52\":3,\"54\":2,\"56\":2,\"61\":1}}],[\"write\",{\"1\":{\"61\":1}}],[\"warnings\",{\"1\":{\"56\":2}}],[\"walk\",{\"1\":{\"54\":1,\"56\":1}}],[\"white\",{\"1\":{\"52\":1}}],[\"which\",{\"1\":{\"19\":1}}],[\"w\",{\"1\":{\"46\":4,\"51\":5,\"61\":1,\"63\":4,\"64\":1,\"65\":1,\"68\":1}}],[\"were\",{\"1\":{\"61\":1}}],[\"web\",{\"1\":{\"57\":1}}],[\"weakly\",{\"1\":{\"57\":1}}],[\"weights\",{\"1\":{\"16\":1,\"17\":1,\"24\":1,\"54\":2,\"56\":2,\"68\":1,\"72\":5}}],[\"we\",{\"1\":{\"15\":1,\"17\":1,\"29\":1,\"46\":1}}],[\"words\",{\"1\":{\"11\":2,\"57\":3}}],[\"word\",{\"1\":{\"11\":2}}],[\"wget\",{\"1\":{\"7\":1}}],[\"icmlm和convirt仅在10万级别的数据上训练了几天\",{\"1\":{\"57\":1}}],[\"icmlm基于语言掩码的方法\",{\"1\":{\"57\":1}}],[\"impl\",{\"1\":{\"72\":1}}],[\"implements\",{\"1\":{\"46\":1}}],[\"import\",{\"1\":{\"56\":9,\"61\":3}}],[\"imshow\",{\"1\":{\"55\":1,\"56\":1}}],[\"img\",{\"1\":{\"55\":2,\"56\":2,\"61\":9,\"63\":10,\"64\":3,\"65\":2,\"68\":2,\"72\":1}}],[\"imagenet\",{\"1\":{\"62\":1,\"72\":3}}],[\"images=images\",{\"1\":{\"54\":1,\"56\":1}}],[\"images\",{\"1\":{\"51\":1,\"52\":1,\"54\":3,\"55\":2,\"56\":5,\"61\":40,\"62\":12,\"68\":3}}],[\"image\",{\"1\":{\"50\":1,\"51\":5,\"52\":10,\"54\":31,\"55\":21,\"56\":51,\"57\":1,\"61\":9,\"63\":1,\"72\":1}}],[\"ignored\",{\"1\":{\"29\":4}}],[\"ignore\",{\"1\":{\"26\":1,\"29\":3,\"56\":1}}],[\"i\",{\"1\":{\"14\":2,\"51\":12,\"54\":2,\"56\":2,\"61\":4,\"68\":2}}],[\"items\",{\"1\":{\"61\":1}}],[\"item\",{\"1\":{\"10\":1,\"61\":5}}],[\"in21k模型权重文件\",{\"1\":{\"72\":1}}],[\"in21k\",{\"1\":{\"72\":4}}],[\"in21k这个模型\",{\"1\":{\"72\":1}}],[\"indent=4\",{\"1\":{\"61\":1}}],[\"index\",{\"1\":{\"28\":4,\"29\":6,\"30\":5,\"54\":2,\"55\":2,\"56\":4}}],[\"index=5\",{\"1\":{\"30\":2}}],[\"index=ignored\",{\"1\":{\"29\":1}}],[\"index=\",{\"1\":{\"26\":1}}],[\"inductive\",{\"1\":{\"59\":1}}],[\"indices\",{\"1\":{\"54\":2,\"56\":2,\"61\":4}}],[\"install\",{\"1\":{\"34\":1}}],[\"into\",{\"1\":{\"28\":1}}],[\"int\",{\"1\":{\"19\":1,\"64\":5,\"66\":1,\"72\":1}}],[\"intermediate\",{\"1\":{\"13\":9}}],[\"info\",{\"1\":{\"13\":1,\"23\":1}}],[\"init\",{\"1\":{\"11\":2,\"13\":6,\"14\":2,\"15\":2,\"16\":3,\"17\":3,\"19\":2,\"20\":2,\"21\":2,\"23\":2,\"24\":2,\"25\":2,\"26\":2,\"29\":2,\"31\":2,\"32\":2,\"37\":2,\"38\":2,\"40\":2,\"41\":2,\"42\":2,\"44\":2,\"45\":2,\"46\":2,\"61\":1,\"63\":2,\"64\":3,\"65\":4,\"66\":4,\"67\":2,\"68\":5}}],[\"inputfeatures\",{\"1\":{\"8\":1}}],[\"inputfeatures组成图解\",{\"1\":{\"8\":1}}],[\"input\",{\"1\":{\"8\":13,\"10\":4,\"11\":6,\"13\":2,\"16\":2,\"17\":2,\"20\":2,\"21\":3,\"24\":1,\"26\":2,\"28\":1,\"29\":2,\"30\":4,\"31\":2,\"32\":7,\"42\":1,\"52\":2}}],[\"inputs\",{\"1\":{\"8\":8,\"29\":1,\"54\":4,\"56\":4}}],[\"in\",{\"1\":{\"8\":1,\"14\":2,\"19\":1,\"37\":1,\"42\":2,\"45\":1,\"46\":4,\"52\":2,\"53\":1,\"54\":7,\"56\":7,\"61\":10,\"63\":4,\"64\":3,\"65\":2,\"66\":8,\"68\":4,\"72\":3}}],[\"isn\",{\"1\":{\"61\":1}}],[\"isdir\",{\"1\":{\"54\":1,\"56\":1,\"61\":1}}],[\"isinstance\",{\"1\":{\"13\":2,\"23\":2}}],[\"is\",{\"1\":{\"8\":4,\"11\":2,\"17\":1,\"19\":4,\"24\":1,\"26\":2,\"29\":2,\"30\":2,\"31\":2,\"32\":4,\"42\":1,\"44\":1,\"46\":3,\"52\":1,\"54\":2,\"55\":2,\"56\":4,\"61\":1}}],[\"if\",{\"1\":{\"8\":7,\"10\":1,\"11\":2,\"13\":1,\"17\":3,\"19\":1,\"23\":1,\"26\":1,\"29\":1,\"31\":2,\"32\":4,\"46\":3,\"54\":9,\"55\":3,\"56\":12,\"61\":6,\"63\":1,\"66\":1,\"68\":1,\"72\":2}}],[\"identity\",{\"1\":{\"63\":3,\"66\":1,\"68\":1}}],[\"idx\",{\"1\":{\"54\":5,\"56\":5}}],[\"idx=0\",{\"1\":{\"11\":1}}],[\"id=model\",{\"1\":{\"54\":1,\"56\":1}}],[\"ids=position\",{\"1\":{\"16\":1,\"17\":1,\"26\":1,\"29\":1,\"31\":1,\"32\":1}}],[\"ids=none\",{\"1\":{\"11\":2,\"16\":2,\"17\":2,\"26\":2,\"29\":2,\"31\":2,\"32\":2}}],[\"ids=token\",{\"1\":{\"8\":1,\"16\":1,\"17\":1,\"26\":1,\"28\":1,\"29\":1,\"31\":1,\"32\":1}}],[\"ids=input\",{\"1\":{\"8\":1}}],[\"ids作用图解\",{\"1\":{\"8\":1}}],[\"ids\",{\"1\":{\"8\":36,\"10\":8,\"11\":14,\"16\":4,\"17\":4,\"26\":4,\"28\":2,\"29\":4,\"30\":4,\"31\":4,\"32\":17}}],[\"id\",{\"1\":{\"8\":9,\"30\":3}}],[\"过长截断策略\",{\"1\":{\"8\":1}}],[\"表示模型的规模\",{\"1\":{\"72\":1}}],[\"表示不进行归一化\",{\"1\":{\"63\":2}}],[\"表示每个图像块的大小是\",{\"1\":{\"62\":1}}],[\"表示将张量中的值限制在\",{\"1\":{\"29\":1}}],[\"表示\",{\"1\":{\"8\":1}}],[\"utils\",{\"1\":{\"61\":3,\"62\":2}}],[\"using\",{\"1\":{\"46\":1}}],[\"use\",{\"1\":{\"54\":1,\"56\":1,\"67\":1}}],[\"used\",{\"1\":{\"8\":1,\"66\":1}}],[\"users\",{\"1\":{\"7\":1}}],[\"unusual\",{\"1\":{\"19\":1}}],[\"unicode\",{\"1\":{\"13\":1,\"23\":1}}],[\"unsqueeze\",{\"1\":{\"11\":1,\"16\":2,\"46\":1}}],[\"unk\",{\"1\":{\"8\":2}}],[\"用一个简化版的例子说明上述过程\",{\"1\":{\"63\":1}}],[\"用文本描述去匹配最合适的图片内容\",{\"1\":{\"55\":1}}],[\"用当前图片外层目录的名字作为其分类名词\",{\"1\":{\"54\":1}}],[\"用户本地运行时\",{\"1\":{\"34\":1}}],[\"用于同时生成查询\",{\"1\":{\"67\":1}}],[\"用于调整注意力分数\",{\"1\":{\"67\":1}}],[\"用于防止过拟合\",{\"1\":{\"66\":1}}],[\"用于随机深度\",{\"1\":{\"66\":1}}],[\"用于位置嵌入后的随机丢弃\",{\"1\":{\"65\":1,\"68\":1}}],[\"用于分类任务\",{\"1\":{\"64\":1}}],[\"用于预测图像的类别\",{\"1\":{\"64\":1}}],[\"用于存储训练集和验证集的图像预处理转换操作\",{\"1\":{\"62\":1}}],[\"用于将输入图像分割成多个图像块并进行嵌入\",{\"1\":{\"63\":1}}],[\"用于将一个方法定义为静态方法\",{\"1\":{\"61\":1}}],[\"用于将一个批次的数据组合成一个张量\",{\"1\":{\"61\":1}}],[\"用于将张量中的值限制在指定的范围内\",{\"1\":{\"29\":1}}],[\"用于计算交叉熵损失\",{\"1\":{\"32\":1}}],[\"用于指定在计算损失时忽略的标签索引\",{\"1\":{\"29\":1}}],[\"用于判断给出的两个句子是否连续\",{\"1\":{\"8\":1}}],[\"用以区分不同的句子\",{\"1\":{\"8\":1}}],[\"用\",{\"1\":{\"8\":1}}],[\"对特征进行更深入的建模\",{\"1\":{\"71\":1}}],[\"对投影后的结果应用丢弃层\",{\"1\":{\"67\":1}}],[\"对输入数据进行初步的特征提取\",{\"1\":{\"71\":1}}],[\"对输入进行归一化处理\",{\"1\":{\"66\":1}}],[\"对输出进行维度交换和形状调整\",{\"1\":{\"67\":1}}],[\"对注意力权重矩阵应用丢弃层\",{\"1\":{\"67\":1}}],[\"对注意力分数矩阵应用softmax函数\",{\"1\":{\"67\":1}}],[\"对经过注意力层的输出进行归一化处理\",{\"1\":{\"66\":1}}],[\"对处理后的张量进行归一化操作\",{\"1\":{\"63\":1}}],[\"对图像进行归一化处理\",{\"1\":{\"62\":2}}],[\"对验证集的处理方式是先resize成256x256的图片\",{\"1\":{\"62\":1}}],[\"对数据集和验证集划分之后\",{\"1\":{\"62\":1}}],[\"对角线元素的labels\",{\"1\":{\"51\":1}}],[\"对称的对比学习损失\",{\"1\":{\"51\":1}}],[\"对两个特征进行线性投射\",{\"1\":{\"51\":1}}],[\"对每一个选项\",{\"1\":{\"32\":1}}],[\"对每个选项分别进行编码\",{\"1\":{\"32\":1}}],[\"对每个\",{\"1\":{\"30\":1}}],[\"对话系统中的候选回复选择\",{\"1\":{\"32\":1}}],[\"对比\",{\"1\":{\"30\":1}}],[\"对于二维的图像\",{\"1\":{\"63\":1}}],[\"对于自监督模型\",{\"1\":{\"57\":1}}],[\"对于有监督模型\",{\"1\":{\"57\":1}}],[\"对于vit\",{\"1\":{\"51\":1}}],[\"对于一个包含个文本\",{\"1\":{\"51\":1}}],[\"对于这样的多选问题\",{\"1\":{\"32\":1}}],[\"对于分类任务来说\",{\"1\":{\"17\":1}}],[\"对于字典中不存在的词\",{\"1\":{\"8\":1}}],[\"对应维度为\",{\"1\":{\"64\":1}}],[\"对应\",{\"1\":{\"63\":1}}],[\"对应的伪代码实现如下所示\",{\"1\":{\"51\":1}}],[\"对应的id为\",{\"1\":{\"8\":1}}],[\"对应的参数为\",{\"1\":{\"7\":1}}],[\"对应代码如下\",{\"1\":{\"7\":1}}],[\"404\",{\"1\":{\"76\":1}}],[\"4替换为相应的类别名称\",{\"1\":{\"61\":1}}],[\"48\",{\"1\":{\"54\":1}}],[\"4873\",{\"1\":{\"8\":2}}],[\"4\",{\"0\":{\"65\":1},\"1\":{\"11\":1,\"32\":1,\"46\":1,\"67\":1,\"72\":1}}],[\"4788\",{\"1\":{\"8\":2}}],[\"4960\",{\"1\":{\"8\":2}}],[\"4638\",{\"1\":{\"8\":2}}],[\"7\",{\"0\":{\"68\":1}}],[\"7py1jdq1wp0nnyt3a\",{\"1\":{\"61\":1}}],[\"704\",{\"1\":{\"8\":2}}],[\"768\",{\"1\":{\"7\":3,\"13\":2,\"63\":4,\"64\":6,\"65\":4,\"66\":1,\"68\":4}}],[\"态\",{\"1\":{\"8\":1}}],[\"形式\",{\"1\":{\"32\":1}}],[\"形状为\",{\"1\":{\"63\":2}}],[\"形状\",{\"1\":{\"28\":2}}],[\"形\",{\"1\":{\"8\":1}}],[\"破\",{\"1\":{\"8\":1}}],[\"突破transformer缺少归纳偏置的限制\",{\"1\":{\"59\":1}}],[\"突\",{\"1\":{\"8\":1}}],[\"的优势\",{\"1\":{\"71\":1}}],[\"的局部特征提取能力快速捕捉图像的底层特征\",{\"1\":{\"71\":1}}],[\"的缩写\",{\"1\":{\"69\":2}}],[\"的隐藏维度\",{\"1\":{\"66\":1}}],[\"的隐藏状态\",{\"1\":{\"28\":1}}],[\"的图像块矩阵添加二维\",{\"1\":{\"65\":1}}],[\"的交互\",{\"1\":{\"64\":1}}],[\"的作用是通过训练过程中损失值的降低\",{\"1\":{\"64\":1}}],[\"的特征图\",{\"1\":{\"63\":1}}],[\"的形式\",{\"1\":{\"63\":1}}],[\"的矩阵\",{\"1\":{\"63\":1}}],[\"的张量\",{\"1\":{\"62\":2,\"63\":2}}],[\"的概率随机水平翻转图像\",{\"1\":{\"62\":1}}],[\"的zero\",{\"1\":{\"57\":1}}],[\"的架构中融入卷积操作\",{\"1\":{\"71\":1}}],[\"的架构\",{\"1\":{\"54\":1}}],[\"的性能提升\",{\"1\":{\"53\":1}}],[\"的效果\",{\"1\":{\"53\":1}}],[\"的vit\",{\"1\":{\"51\":1}}],[\"的出现也掀起了新一轮的研究热潮\",{\"1\":{\"49\":1}}],[\"的研究如潮水般涌来\",{\"1\":{\"49\":1}}],[\"的核心\",{\"1\":{\"36\":1}}],[\"的神经网络架构\",{\"1\":{\"36\":1}}],[\"的输入序列\",{\"1\":{\"32\":1}}],[\"的输入方式是\",{\"1\":{\"32\":1}}],[\"的输入组织形式与普通分类或问答任务略有不同\",{\"1\":{\"32\":1}}],[\"的输出向量能够很好地表示图像的全局特征\",{\"1\":{\"64\":2}}],[\"的输出向量被输入到分类头中\",{\"1\":{\"64\":1}}],[\"的输出\",{\"1\":{\"28\":1,\"64\":1}}],[\"的区域\",{\"1\":{\"30\":1}}],[\"的角色进行分类\",{\"1\":{\"30\":1}}],[\"的模型\",{\"1\":{\"30\":1}}],[\"的位置\",{\"1\":{\"28\":1}}],[\"的序列\",{\"1\":{\"28\":1}}],[\"的表示\",{\"1\":{\"28\":1}}],[\"的文本对\",{\"1\":{\"28\":1}}],[\"的问答任务中\",{\"1\":{\"28\":1}}],[\"的\",{\"1\":{\"8\":1,\"28\":1,\"30\":1}}],[\"中引入卷积操作\",{\"1\":{\"71\":1}}],[\"中进行进一步的处理\",{\"1\":{\"71\":1}}],[\"中第一个线性层把输入特征投影到一个更高维度的空间后\",{\"1\":{\"66\":1}}],[\"中加载数据时\",{\"1\":{\"61\":1}}],[\"中的一个装饰器\",{\"1\":{\"61\":1}}],[\"中的一个方法\",{\"1\":{\"29\":1}}],[\"中的图像一一对应\",{\"1\":{\"61\":1}}],[\"中提取\",{\"1\":{\"30\":1}}],[\"中取出对应的\",{\"1\":{\"30\":1}}],[\"中\",{\"1\":{\"8\":1,\"28\":1,\"52\":2,\"62\":1,\"64\":5}}],[\"票\",{\"1\":{\"8\":1}}],[\"股\",{\"1\":{\"8\":1}}],[\"股票中的突破形态\",{\"1\":{\"8\":2}}],[\">\",{\"1\":{\"8\":3,\"10\":1,\"29\":1,\"46\":1,\"57\":2,\"64\":2,\"65\":2,\"66\":1,\"67\":8,\"68\":3}}],[\"文字搜索图像\",{\"0\":{\"55\":1}}],[\"文字搜索图像实战演练\",{\"1\":{\"48\":1}}],[\"文件中的\",{\"1\":{\"34\":1}}],[\"文件进行调试即可\",{\"1\":{\"7\":1}}],[\"文本描述的生成也是一个关键环节\",{\"1\":{\"53\":1}}],[\"文本描述生成\",{\"0\":{\"53\":1}}],[\"文本编码器使用的是基于\",{\"1\":{\"54\":1}}],[\"文本编码器\",{\"1\":{\"51\":1,\"52\":1}}],[\"文本编码器的作用是提取文本的特征\",{\"1\":{\"51\":1}}],[\"文本分词\",{\"1\":{\"8\":1}}],[\"文本分类任务\",{\"1\":{\"8\":1}}],[\"文本对应的标签\",{\"1\":{\"8\":1}}],[\"因此是自注意力\",{\"1\":{\"67\":1}}],[\"因此可以堆叠多个block\",{\"1\":{\"66\":1}}],[\"因此可以充分利用计算资源\",{\"1\":{\"35\":1}}],[\"因此\",{\"1\":{\"57\":2,\"62\":1}}],[\"因此在新的数据集上需要定义新的分类器来重新训练\",{\"1\":{\"57\":1}}],[\"因此在效果上可能不如使用\",{\"1\":{\"53\":1}}],[\"因此成本较高\",{\"1\":{\"57\":1}}],[\"因此这里就不再给出数据集下载链接了\",{\"1\":{\"54\":1}}],[\"因此这是一个非常庞大的数据集\",{\"1\":{\"51\":1}}],[\"因此出现了extended\",{\"1\":{\"35\":1}}],[\"因此需要大家自行完成运行时缺失依赖包的安装\",{\"1\":{\"7\":1}}],[\"因为在代码中有个冻结权重的操作\",{\"1\":{\"72\":1}}],[\"因为transformer和cnn相比缺少归纳偏置\",{\"1\":{\"59\":1}}],[\"因为训练数据集中的文本\",{\"1\":{\"57\":1}}],[\"因为cuda不支持macos\",{\"1\":{\"34\":1}}],[\"因为\",{\"1\":{\"30\":1}}],[\"因为我只是为了了解内部代码情况\",{\"1\":{\"7\":1}}],[\"所具备的\",{\"1\":{\"59\":1}}],[\"所提供的代码展开进行讲解\",{\"1\":{\"34\":1}}],[\"所有模型均训练了32个周期\",{\"1\":{\"51\":1}}],[\"所有输入序列等长\",{\"1\":{\"8\":1}}],[\"所有序列都填充到max\",{\"1\":{\"8\":1}}],[\"所参考源仓库未提供requirements\",{\"1\":{\"7\":1}}],[\"所以参数量为\",{\"1\":{\"69\":1}}],[\"所以模型的任务是\",{\"1\":{\"30\":1}}],[\"所以最终的答案只能来自原始输入文本中的某一段子串\",{\"1\":{\"30\":1}}],[\"所以我只是取其中的一部分数据\",{\"1\":{\"7\":1}}],[\"所以准确度不是在我的考虑范围之内\",{\"1\":{\"7\":1}}],[\"optimizer\",{\"1\":{\"68\":1}}],[\"opening\",{\"1\":{\"55\":1,\"56\":1}}],[\"open\",{\"1\":{\"54\":1,\"55\":1,\"56\":2,\"61\":2}}],[\"openai首先尝试了virtex模型\",{\"1\":{\"57\":1}}],[\"openai从网络上收集了4亿条数据进行实验\",{\"1\":{\"57\":1}}],[\"openai从网络上收集了总计4亿对文本和图像\",{\"1\":{\"51\":1}}],[\"openai\",{\"1\":{\"49\":1,\"54\":2,\"56\":1}}],[\"os\",{\"1\":{\"54\":11,\"55\":2,\"56\":14,\"61\":8}}],[\"one\",{\"1\":{\"68\":1}}],[\"on\",{\"1\":{\"46\":1,\"52\":3}}],[\"only\",{\"1\":{\"8\":1,\"24\":1,\"30\":2,\"31\":1}}],[\"our\",{\"1\":{\"29\":1}}],[\"outside\",{\"1\":{\"29\":1}}],[\"out\",{\"1\":{\"19\":1,\"66\":5}}],[\"outputs\",{\"1\":{\"7\":1,\"16\":2,\"17\":7,\"21\":2,\"26\":6,\"28\":4,\"29\":8,\"31\":6,\"32\":6}}],[\"output\",{\"1\":{\"7\":3,\"13\":9,\"15\":4,\"16\":7,\"17\":4,\"19\":2,\"21\":4,\"24\":2,\"25\":4,\"26\":4,\"28\":5,\"29\":2,\"31\":4,\"32\":4}}],[\"official\",{\"1\":{\"72\":1}}],[\"off\",{\"1\":{\"55\":1,\"56\":1}}],[\"of\",{\"1\":{\"8\":1,\"30\":2,\"31\":1,\"42\":1,\"44\":1,\"46\":1,\"51\":4,\"52\":6,\"53\":3,\"54\":2,\"55\":1,\"56\":2,\"57\":3,\"61\":1}}],[\"overwrite\",{\"1\":{\"7\":1}}],[\"org\",{\"1\":{\"72\":1}}],[\"ordereddict\",{\"1\":{\"68\":1}}],[\"original\",{\"1\":{\"19\":1,\"72\":1}}],[\"or\",{\"1\":{\"7\":1,\"13\":1,\"23\":1,\"51\":2,\"66\":2,\"67\":1,\"68\":2}}],[\"oracle\",{\"0\":{\"2\":1},\"1\":{\"0\":1}}],[\"02\",{\"1\":{\"64\":1,\"65\":2,\"68\":2}}],[\"0+cu113\",{\"1\":{\"34\":1}}],[\"0\",{\"0\":{\"61\":1},\"1\":{\"7\":1,\"8\":29,\"11\":1,\"13\":1,\"15\":1,\"16\":2,\"19\":2,\"23\":1,\"29\":3,\"30\":1,\"31\":1,\"34\":1,\"41\":1,\"44\":1,\"46\":3,\"52\":1,\"54\":1,\"56\":1,\"61\":2,\"62\":23,\"63\":3,\"64\":3,\"65\":3,\"66\":2,\"67\":3,\"68\":6}}],[\"03\",{\"1\":{\"7\":1}}],[\"5500\",{\"1\":{\"8\":2}}],[\"5\",{\"0\":{\"66\":1},\"1\":{\"7\":1,\"11\":1,\"52\":1,\"53\":1,\"57\":1,\"61\":1,\"62\":19,\"67\":1,\"72\":1}}],[\"rwightman\",{\"1\":{\"72\":1}}],[\"root\",{\"1\":{\"54\":2,\"56\":2,\"61\":8}}],[\"rocket\",{\"1\":{\"52\":1}}],[\"rgb为彩色图片\",{\"1\":{\"61\":1}}],[\"rgb\",{\"1\":{\"54\":1,\"56\":1,\"61\":3,\"63\":1}}],[\"rn50x16和rnx64\",{\"1\":{\"51\":1}}],[\"rn50x4\",{\"1\":{\"51\":1}}],[\"rnn等模型的缺点是需要顺序计算\",{\"1\":{\"35\":1}}],[\"right\",{\"1\":{\"44\":1}}],[\"r\",{\"1\":{\"34\":1}}],[\"ratio\",{\"1\":{\"65\":1,\"66\":7,\"67\":2,\"68\":4}}],[\"ratio=dpr\",{\"1\":{\"68\":1}}],[\"ratio=drop\",{\"1\":{\"66\":1,\"68\":1}}],[\"ratio=mlp\",{\"1\":{\"68\":1}}],[\"ratio=attn\",{\"1\":{\"66\":1,\"68\":1}}],[\"ratio=0\",{\"1\":{\"65\":3,\"66\":3,\"67\":2,\"68\":3}}],[\"ratio=4\",{\"1\":{\"65\":1,\"66\":1,\"68\":1}}],[\"rate\",{\"1\":{\"61\":2}}],[\"rate=2e\",{\"1\":{\"7\":1}}],[\"raise\",{\"1\":{\"61\":1}}],[\"raw\",{\"1\":{\"19\":1}}],[\"randomhorizontalflip\",{\"1\":{\"62\":1}}],[\"randomresizedcrop\",{\"1\":{\"62\":2}}],[\"random\",{\"1\":{\"61\":2}}],[\"randomsampler\",{\"1\":{\"10\":1}}],[\"range\",{\"1\":{\"14\":1,\"54\":1,\"56\":1,\"61\":2,\"68\":1}}],[\"rank\",{\"1\":{\"10\":1}}],[\"requires\",{\"1\":{\"72\":1}}],[\"requirements\",{\"1\":{\"34\":2}}],[\"releases\",{\"1\":{\"72\":1}}],[\"related\",{\"1\":{\"66\":1}}],[\"relationship\",{\"1\":{\"25\":4,\"26\":4}}],[\"repo\",{\"1\":{\"54\":1,\"56\":1}}],[\"replace\",{\"1\":{\"54\":1,\"56\":1}}],[\"representation\",{\"1\":{\"30\":1,\"65\":1,\"68\":3,\"72\":1}}],[\"red\",{\"1\":{\"52\":1}}],[\"research\",{\"1\":{\"72\":1}}],[\"resize\",{\"1\":{\"62\":2}}],[\"residual\",{\"1\":{\"36\":1}}],[\"resnet和混合模型的效果均不如vit模型\",{\"1\":{\"71\":1}}],[\"resnet和混合模型在不同图像分类数据集上的测试结果\",{\"1\":{\"71\":1}}],[\"resnet101\",{\"1\":{\"51\":1}}],[\"resnet50\",{\"1\":{\"51\":1}}],[\"resnet包含五种不同尺寸的模型\",{\"1\":{\"51\":1}}],[\"resnet\",{\"1\":{\"51\":1,\"71\":1}}],[\"reshape\",{\"1\":{\"32\":1,\"67\":4}}],[\"reshaped\",{\"1\":{\"32\":4}}],[\"regression\",{\"1\":{\"17\":1}}],[\"returns\",{\"1\":{\"8\":1}}],[\"return\",{\"1\":{\"8\":6,\"10\":1,\"11\":1,\"13\":3,\"14\":1,\"15\":1,\"16\":1,\"17\":1,\"19\":2,\"20\":1,\"21\":1,\"23\":1,\"24\":1,\"25\":1,\"26\":1,\"29\":1,\"31\":1,\"32\":1,\"37\":3,\"38\":1,\"40\":1,\"41\":1,\"42\":1,\"44\":1,\"45\":1,\"46\":2,\"54\":9,\"55\":4,\"56\":13,\"61\":7,\"63\":2,\"64\":2,\"65\":2,\"66\":2,\"67\":1,\"68\":2,\"72\":1}}],[\"read\",{\"1\":{\"7\":9,\"61\":1,\"62\":1}}],[\"run\",{\"1\":{\"7\":1}}],[\"准备调试\",{\"1\":{\"7\":1}}],[\"准备就绪\",{\"1\":{\"7\":1}}],[\"开发数据1k\",{\"1\":{\"7\":1}}],[\"开源项目\",{\"0\":{\"74\":1}}],[\"开源网络库\",{\"1\":{\"2\":1}}],[\"开源框架\",{\"1\":{\"2\":1}}],[\"测试图片分类正确率\",{\"1\":{\"54\":1,\"56\":1}}],[\"测试数据使用1k\",{\"1\":{\"7\":1}}],[\"测试和开发集\",{\"1\":{\"7\":1}}],[\"其他权重全部冻结\",{\"1\":{\"72\":1}}],[\"其他下游任务\",{\"0\":{\"27\":1}}],[\"其表达能力是有限的\",{\"1\":{\"66\":1}}],[\"其次\",{\"1\":{\"54\":1}}],[\"其次是准备训练数据\",{\"1\":{\"7\":1}}],[\"其均能匹配到正确的文本标签\",{\"1\":{\"52\":1}}],[\"其规模与gpt\",{\"1\":{\"51\":1}}],[\"其中包括注意力可视化\",{\"1\":{\"70\":1}}],[\"其中的layers就是transformer\",{\"1\":{\"69\":1}}],[\"其中\",{\"1\":{\"49\":1,\"51\":1,\"63\":1}}],[\"其中训练数据使用1k\",{\"1\":{\"7\":1}}],[\"其由七大主要部分构成\",{\"1\":{\"36\":1}}],[\"我将两者的结构进行对比\",{\"1\":{\"66\":1}}],[\"我们通过自定义一个patchembed类完成上述工作\",{\"1\":{\"63\":1}}],[\"我们通过利用clip模型的多模态能力\",{\"1\":{\"52\":1}}],[\"我们首先获取图片库中所有图片\",{\"1\":{\"55\":1}}],[\"我们首先创建了各类别的文本描述\",{\"1\":{\"52\":1}}],[\"我们只需要在计算出相似度得分矩阵后\",{\"1\":{\"55\":1}}],[\"我们从flower\",{\"1\":{\"54\":1}}],[\"我们从原始输入的\",{\"1\":{\"30\":1}}],[\"我们需要根据上面给出的花卉数据集下载链接\",{\"1\":{\"54\":1}}],[\"我们常常需要衡量文本嵌入和图片嵌入之间的相似度\",{\"1\":{\"54\":1}}],[\"我们可以直接使用类别标签作为文本描述\",{\"1\":{\"53\":1}}],[\"我们还有其他的选择\",{\"1\":{\"53\":1}}],[\"我们使用了\",{\"1\":{\"53\":1}}],[\"我们也可以对得到的余弦相似度计算softmax\",{\"1\":{\"52\":1}}],[\"我们已经探讨了clip模型的运作机制\",{\"1\":{\"52\":1}}],[\"我们主要使用它来预测答案的起始和结束位置\",{\"1\":{\"28\":1}}],[\"我的调试文件是run\",{\"1\":{\"7\":1}}],[\"我已经上传到了仓库中\",{\"1\":{\"7\":1}}],[\"我就不上传了\",{\"1\":{\"7\":1}}],[\"这有助于模型发现输入数据中更复杂的模式和关系\",{\"1\":{\"66\":1}}],[\"这也是为什么结构图中mlp\",{\"1\":{\"64\":1}}],[\"这一步的操作在论文中是直接采用切割的处理办法\",{\"1\":{\"63\":1}}],[\"这一步是为了保证图像的整体比例不变\",{\"1\":{\"62\":1}}],[\"这一过程与训练时相同\",{\"1\":{\"52\":1}}],[\"这是一种数据增强的方式\",{\"1\":{\"62\":1}}],[\"这是抽取式问答模型的局限性\",{\"1\":{\"30\":1}}],[\"这时就可以自定义\",{\"1\":{\"61\":1}}],[\"这几乎是不可能完成的任务\",{\"1\":{\"59\":1}}],[\"这大大限制了它们的迁移能力和扩展性\",{\"1\":{\"57\":1}}],[\"这远远低于imagenet上的sota\",{\"1\":{\"57\":1}}],[\"这方面的工作并不多\",{\"1\":{\"57\":1}}],[\"这种差异一方面是由于文本和图像属于两个完全不同的模态\",{\"1\":{\"57\":1}}],[\"这种预训练通常是基于有监督学习的\",{\"1\":{\"57\":1}}],[\"这种格式\",{\"1\":{\"53\":1}}],[\"这种方法实际上与nlp领域的一个研究方向\",{\"1\":{\"53\":1}}],[\"这与传统的预训练加微调的方法有所不同\",{\"1\":{\"53\":1}}],[\"这展示了其在图像分类任务中的灵活性和强大能力\",{\"1\":{\"52\":1}}],[\"这不仅展示了clip的强大功能\",{\"1\":{\"52\":1}}],[\"这意味着它能够在没有任何特定任务训练数据的情况下\",{\"1\":{\"52\":1}}],[\"这表明训练clip模型需要消耗大量的资源\",{\"1\":{\"51\":1}}],[\"这些值会根据模型的损失函数不断调整\",{\"1\":{\"65\":1}}],[\"这些相似度值可以被视为logits\",{\"1\":{\"52\":1}}],[\"这些图像特征会与之前得到的个文本特征进行余弦相似度计算\",{\"1\":{\"52\":1}}],[\"这些文本随后被输入到文本编码器\",{\"1\":{\"52\":1}}],[\"这些数据在论文中被称为webimagetext\",{\"1\":{\"51\":1}}],[\"这些模型都是以cnn为基础\",{\"1\":{\"35\":1}}],[\"这里不再贴出\",{\"1\":{\"72\":1}}],[\"这里需要将其分离开来\",{\"1\":{\"67\":1}}],[\"这里主要有两种位置编码思路\",{\"1\":{\"65\":1}}],[\"这里简单介绍一下cls\",{\"1\":{\"64\":1}}],[\"这里的原因\",{\"1\":{\"66\":1}}],[\"这里的\",{\"1\":{\"63\":1}}],[\"这里的相似度直接计算文本特征和图像特征的余弦相似性\",{\"1\":{\"51\":1}}],[\"这里设置为图像块的大小\",{\"1\":{\"63\":2}}],[\"这里对训练集的处理方式是随机切成224x224像素的图片\",{\"1\":{\"62\":1}}],[\"这里对提取的文本特征和图像特征进行对比学习\",{\"1\":{\"51\":1}}],[\"这里以搜索向日葵花为例\",{\"1\":{\"55\":1}}],[\"这里采用了余弦相似度的计算方法\",{\"1\":{\"54\":1}}],[\"这里共有个正样本\",{\"1\":{\"51\":1}}],[\"这里我准备做一个文本分类任务\",{\"1\":{\"7\":1}}],[\"这两个模型都属于融合图像与文本的多模态模型\",{\"1\":{\"49\":1}}],[\"这个维度上添加一维\",{\"1\":{\"65\":1}}],[\"这个就是额外添加的一个\",{\"1\":{\"64\":1}}],[\"这个token\",{\"1\":{\"64\":1}}],[\"这个函数的作用是将输入的文本转化为对应的嵌入表示\",{\"1\":{\"54\":1}}],[\"这个函数的有一个输入参数\",{\"1\":{\"40\":1}}],[\"这个\",{\"1\":{\"54\":1}}],[\"这个数据集来源是这里\",{\"1\":{\"7\":1}}],[\"这和原始论文稍有不同\",{\"1\":{\"40\":1}}],[\"这比较容易并行\",{\"1\":{\"35\":1}}],[\"这样可以为\",{\"1\":{\"71\":1}}],[\"这样可以在模型的不同阶段交替利用\",{\"1\":{\"71\":1}}],[\"这样可以提升计算效率\",{\"1\":{\"67\":1}}],[\"这样可以保证模型的特征提取能力和性能\",{\"1\":{\"62\":1}}],[\"这样\",{\"1\":{\"63\":1}}],[\"这样计算量就大大减小了\",{\"1\":{\"63\":1}}],[\"这样就可以完成vit的训练过程\",{\"1\":{\"68\":1}}],[\"这样就可以提取出对最终任务有帮助的特征组合\",{\"1\":{\"66\":1}}],[\"这样就成了一个一维序列\",{\"1\":{\"63\":1}}],[\"这样就能让\",{\"1\":{\"32\":1,\"66\":1}}],[\"这样的格式来生成文本描述\",{\"1\":{\"53\":1}}],[\"这样的\",{\"1\":{\"30\":1}}],[\"这就是答案\",{\"1\":{\"30\":1}}],[\"这其实就是在说\",{\"1\":{\"30\":1}}],[\"这段代码的意思是\",{\"1\":{\"30\":1}}],[\"这类抽取式问答任务中\",{\"1\":{\"30\":1}}],[\"jx\",{\"1\":{\"72\":1}}],[\"jax\",{\"1\":{\"72\":1}}],[\"javaer\",{\"1\":{\"2\":1}}],[\"jft\",{\"1\":{\"57\":2}}],[\"j\",{\"1\":{\"54\":2,\"56\":2}}],[\"jpeg\",{\"1\":{\"54\":1,\"56\":1}}],[\"jpg\",{\"1\":{\"54\":1,\"56\":1,\"61\":2}}],[\"join\",{\"1\":{\"54\":3,\"55\":1,\"56\":4,\"61\":3}}],[\"json\",{\"1\":{\"7\":1,\"61\":6}}],[\"$bert\",{\"1\":{\"7\":3}}],[\"pwd=vket\",{\"1\":{\"72\":1}}],[\"pwd=qvmq\",{\"1\":{\"61\":1}}],[\"p=drop\",{\"1\":{\"65\":1,\"68\":1}}],[\"p=dropout\",{\"1\":{\"46\":1}}],[\"plot\",{\"1\":{\"61\":2}}],[\"plt\",{\"1\":{\"55\":4,\"56\":5,\"61\":7}}],[\"pil\",{\"1\":{\"56\":1,\"61\":1,\"62\":2}}],[\"pip\",{\"1\":{\"34\":1}}],[\"png\",{\"1\":{\"54\":1,\"56\":1,\"61\":2}}],[\"pth\",{\"1\":{\"72\":1}}],[\"pt\",{\"1\":{\"54\":2,\"56\":2}}],[\"photos下的子目录名作为我们的候选待匹配分类文本列表\",{\"1\":{\"54\":1}}],[\"photos\",{\"1\":{\"54\":4,\"56\":2}}],[\"photos目录下读取出所有图片的路径\",{\"1\":{\"54\":1}}],[\"photo\",{\"1\":{\"52\":2,\"53\":2,\"54\":2,\"55\":1,\"56\":2}}],[\"p\",{\"1\":{\"46\":5}}],[\"print\",{\"1\":{\"54\":6,\"55\":4,\"56\":10,\"61\":3,\"72\":1}}],[\"prompting\",{\"1\":{\"53\":1}}],[\"prompt\",{\"1\":{\"53\":3}}],[\"projected\",{\"1\":{\"46\":1}}],[\"projections\",{\"1\":{\"46\":1}}],[\"projection\",{\"1\":{\"46\":1}}],[\"proj\",{\"1\":{\"38\":2,\"51\":2,\"63\":2,\"66\":1,\"67\":6}}],[\"processor\",{\"1\":{\"54\":3,\"56\":3}}],[\"processing\",{\"1\":{\"53\":1}}],[\"process\",{\"1\":{\"37\":1}}],[\"product\",{\"1\":{\"19\":1,\"46\":1}}],[\"probabilities\",{\"1\":{\"19\":1}}],[\"probs\",{\"1\":{\"19\":5,\"52\":3}}],[\"prob\",{\"1\":{\"11\":1,\"13\":1,\"17\":1,\"19\":1,\"20\":1,\"31\":1,\"32\":1}}],[\"pred\",{\"1\":{\"68\":5}}],[\"predicted\",{\"1\":{\"54\":6,\"56\":6}}],[\"predict\",{\"1\":{\"53\":1}}],[\"predictions\",{\"1\":{\"25\":2}}],[\"prediction\",{\"1\":{\"8\":1,\"25\":2,\"26\":4,\"57\":1}}],[\"pretrained\",{\"1\":{\"54\":4,\"56\":4}}],[\"pretrainedtokenizer\",{\"1\":{\"8\":1}}],[\"pre\",{\"1\":{\"50\":1,\"53\":1,\"68\":3,\"72\":2}}],[\"precomputed\",{\"1\":{\"19\":1}}],[\"prev\",{\"1\":{\"7\":1}}],[\"ported\",{\"1\":{\"72\":1}}],[\"portrait\",{\"1\":{\"52\":1}}],[\"portion\",{\"1\":{\"8\":1}}],[\"pos\",{\"1\":{\"31\":1,\"65\":5,\"68\":5}}],[\"positional\",{\"1\":{\"36\":1,\"65\":1}}],[\"positions\",{\"1\":{\"29\":7,\"30\":1}}],[\"positions=none\",{\"1\":{\"29\":2}}],[\"position\",{\"1\":{\"11\":12,\"16\":2,\"17\":2,\"26\":2,\"29\":2,\"31\":2,\"32\":6}}],[\"pooler\",{\"1\":{\"16\":2}}],[\"pooled\",{\"1\":{\"15\":4,\"16\":2,\"17\":4,\"25\":2,\"26\":2,\"28\":2,\"32\":4}}],[\"pool\",{\"1\":{\"15\":1}}],[\"pan\",{\"1\":{\"61\":1,\"72\":1}}],[\"patch16\",{\"1\":{\"72\":6}}],[\"patch14\",{\"1\":{\"54\":2,\"56\":1}}],[\"patches\",{\"1\":{\"63\":5,\"64\":6,\"65\":4,\"67\":11,\"68\":4,\"72\":1,\"73\":1}}],[\"patchembed\",{\"1\":{\"63\":2}}],[\"patch\",{\"1\":{\"63\":12,\"64\":11,\"65\":5,\"68\":5,\"72\":1}}],[\"path=val\",{\"1\":{\"62\":1}}],[\"path=train\",{\"1\":{\"62\":1}}],[\"path=prev\",{\"1\":{\"7\":1}}],[\"paths\",{\"1\":{\"54\":17,\"55\":6,\"56\":21}}],[\"path\",{\"1\":{\"7\":2,\"54\":12,\"55\":2,\"56\":14,\"61\":30,\"62\":5,\"65\":1,\"66\":7,\"68\":2}}],[\"page\",{\"1\":{\"52\":1}}],[\"pass\",{\"1\":{\"42\":1}}],[\"passed\",{\"1\":{\"8\":1}}],[\"para\",{\"1\":{\"72\":2}}],[\"param\",{\"1\":{\"61\":5,\"63\":6}}],[\"parameter\",{\"1\":{\"24\":1,\"51\":1,\"64\":1,\"65\":2,\"68\":2}}],[\"parameters\",{\"1\":{\"16\":1,\"72\":1}}],[\"partial\",{\"1\":{\"68\":1}}],[\"parts\",{\"1\":{\"31\":1}}],[\"paris\",{\"1\":{\"30\":3}}],[\"paper\",{\"1\":{\"19\":1,\"72\":1}}],[\"pair\",{\"1\":{\"8\":2}}],[\"padding=true\",{\"1\":{\"54\":1,\"56\":1}}],[\"padding\",{\"1\":{\"8\":6,\"11\":1,\"16\":1,\"17\":1,\"36\":1}}],[\"pad\",{\"1\":{\"8\":3}}],[\"person\",{\"1\":{\"52\":1}}],[\"permute\",{\"1\":{\"19\":2,\"67\":2}}],[\"per\",{\"1\":{\"7\":2,\"67\":6}}],[\"pyplot\",{\"1\":{\"56\":1}}],[\"py文件\",{\"1\":{\"7\":1}}],[\"py\",{\"1\":{\"7\":2,\"61\":1}}],[\"pytorch\",{\"1\":{\"7\":3,\"29\":1,\"61\":2,\"62\":2,\"72\":1}}],[\"python\",{\"1\":{\"7\":1,\"61\":1}}],[\"python=3\",{\"1\":{\"7\":1,\"34\":1}}],[\"title\",{\"1\":{\"55\":1,\"56\":1,\"61\":1}}],[\"time\",{\"1\":{\"54\":10,\"56\":10}}],[\"trunc\",{\"1\":{\"64\":1,\"65\":2,\"68\":2}}],[\"true\",{\"1\":{\"61\":1,\"72\":1}}],[\"try\",{\"1\":{\"54\":2,\"55\":1,\"56\":3}}],[\"tripod\",{\"1\":{\"52\":1}}],[\"translation\",{\"1\":{\"59\":1}}],[\"transform=data\",{\"1\":{\"62\":2}}],[\"transform=none\",{\"1\":{\"61\":1}}],[\"transforms\",{\"1\":{\"62\":10}}],[\"transform\",{\"1\":{\"23\":3,\"24\":2,\"61\":5,\"62\":4}}],[\"transformer证明了使用transformer结构可以有效处理图像数据\",{\"1\":{\"73\":1}}],[\"transformer需要输入的是一维的token\",{\"1\":{\"63\":1}}],[\"transformer的核心流程实现\",{\"1\":{\"60\":1}}],[\"transformer的模型结构相比于transformer来说更简单\",{\"1\":{\"59\":1}}],[\"transformer是2021年谷歌在iclr上提出的算法\",{\"1\":{\"59\":1}}],[\"transformers\",{\"1\":{\"56\":1}}],[\"transformer\",{\"1\":{\"19\":1,\"30\":1,\"34\":5,\"36\":4,\"49\":3,\"51\":3,\"54\":2,\"57\":1,\"59\":1,\"64\":3,\"66\":1,\"71\":10,\"72\":2}}],[\"transpose\",{\"1\":{\"19\":5,\"46\":3,\"63\":1,\"67\":4}}],[\"training\",{\"1\":{\"50\":1,\"61\":1,\"72\":1}}],[\"train\",{\"1\":{\"7\":3,\"8\":1,\"10\":6,\"53\":1,\"61\":7,\"62\":7,\"68\":1}}],[\"trained\",{\"1\":{\"7\":2}}],[\"t\",{\"1\":{\"51\":15,\"52\":2,\"54\":1,\"56\":1,\"61\":1}}],[\"turn\",{\"1\":{\"42\":1}}],[\"tuple\",{\"1\":{\"28\":1,\"61\":1,\"64\":2,\"67\":1}}],[\"tgt\",{\"1\":{\"37\":12,\"44\":2,\"45\":2}}],[\"txt\",{\"1\":{\"34\":2}}],[\"txt文件\",{\"1\":{\"7\":1}}],[\"t5\",{\"1\":{\"30\":1}}],[\"through\",{\"1\":{\"42\":1}}],[\"this\",{\"1\":{\"19\":1,\"52\":1}}],[\"these\",{\"1\":{\"29\":1}}],[\"there\",{\"1\":{\"24\":1}}],[\"they\",{\"1\":{\"17\":1}}],[\"the\",{\"1\":{\"8\":4,\"15\":3,\"19\":5,\"24\":3,\"29\":1,\"30\":2,\"31\":1,\"34\":1,\"42\":1,\"46\":2,\"52\":1,\"55\":1,\"56\":1,\"61\":1}}],[\"tabby\",{\"1\":{\"52\":1}}],[\"target\",{\"1\":{\"37\":1}}],[\"taken\",{\"1\":{\"19\":1,\"54\":2,\"56\":1}}],[\"take\",{\"1\":{\"19\":1,\"37\":1,\"46\":1}}],[\"taking\",{\"1\":{\"15\":1}}],[\"tanh\",{\"1\":{\"15\":1,\"68\":2}}],[\"task\",{\"1\":{\"7\":1,\"8\":1}}],[\"test\",{\"1\":{\"54\":2,\"56\":1}}],[\"temperature\",{\"1\":{\"51\":1}}],[\"terms\",{\"1\":{\"29\":1}}],[\"tensors=\",{\"1\":{\"54\":2,\"56\":2}}],[\"tensor\",{\"1\":{\"13\":2,\"15\":2,\"20\":2,\"21\":3,\"29\":1,\"52\":1,\"61\":1,\"62\":2,\"67\":1}}],[\"tensordataset\",{\"1\":{\"8\":1}}],[\"text转化为18291个类别\",{\"1\":{\"57\":1}}],[\"text=texts\",{\"1\":{\"54\":1,\"56\":1}}],[\"texts\",{\"1\":{\"51\":1,\"52\":2,\"54\":1,\"56\":1}}],[\"text\",{\"1\":{\"8\":2,\"51\":6,\"52\":13,\"54\":9,\"55\":9,\"56\":16,\"61\":1}}],[\"two\",{\"1\":{\"8\":1,\"59\":1}}],[\"type\",{\"1\":{\"8\":9,\"10\":4,\"11\":9,\"16\":3,\"17\":3,\"26\":3,\"28\":2,\"29\":3,\"30\":1,\"31\":3,\"32\":7}}],[\"type=bert\",{\"1\":{\"7\":1}}],[\"tnews\",{\"1\":{\"7\":4}}],[\"totensor\",{\"1\":{\"62\":2}}],[\"total\",{\"1\":{\"26\":2,\"29\":2,\"54\":4,\"56\":4,\"67\":3}}],[\"topk\",{\"1\":{\"52\":1}}],[\"top\",{\"1\":{\"52\":2}}],[\"torchscript\",{\"1\":{\"67\":1}}],[\"torch==1\",{\"1\":{\"34\":1}}],[\"torch\",{\"1\":{\"10\":1,\"11\":2,\"19\":2,\"24\":1,\"28\":4,\"34\":1,\"46\":2,\"52\":3,\"54\":4,\"56\":5,\"61\":5,\"62\":2,\"64\":2,\"65\":3,\"68\":5,\"72\":1}}],[\"token提取出来\",{\"1\":{\"68\":1}}],[\"token的作用\",{\"1\":{\"64\":1}}],[\"token对齐\",{\"1\":{\"64\":1}}],[\"token分类任务\",{\"0\":{\"31\":1}}],[\"tokenize\",{\"1\":{\"52\":1}}],[\"tokenizer\",{\"1\":{\"30\":4}}],[\"tokenization\",{\"1\":{\"28\":1}}],[\"token用于分类任务即可\",{\"1\":{\"17\":1}}],[\"token\",{\"0\":{\"64\":1},\"1\":{\"8\":35,\"10\":4,\"11\":9,\"15\":4,\"16\":2,\"17\":2,\"24\":1,\"26\":3,\"28\":9,\"29\":2,\"30\":8,\"31\":4,\"32\":6,\"64\":23,\"65\":5,\"68\":5}}],[\"tokens=true\",{\"1\":{\"30\":2}}],[\"tokens=false\",{\"1\":{\"8\":1}}],[\"tokens部分的mask列表\",{\"1\":{\"8\":1}}],[\"tokens\",{\"1\":{\"8\":6,\"19\":1,\"28\":2,\"30\":2,\"52\":2,\"64\":1,\"65\":2,\"68\":2}}],[\"to\",{\"1\":{\"7\":3,\"8\":1,\"15\":1,\"16\":1,\"19\":4,\"35\":1,\"46\":1,\"51\":2,\"54\":6,\"56\":5,\"63\":1,\"68\":3,\"72\":1}}],[\"tf\",{\"1\":{\"7\":2}}],[\"zeros\",{\"1\":{\"11\":1,\"24\":1,\"64\":1,\"65\":2,\"68\":2}}],[\"zero\",{\"1\":{\"8\":2}}],[\"zhandaohong\",{\"1\":{\"7\":1}}],[\"zip\",{\"1\":{\"7\":2,\"10\":1,\"46\":1,\"61\":1}}],[\"drop=0\",{\"1\":{\"66\":1}}],[\"drop=drop\",{\"1\":{\"66\":1}}],[\"droppath\",{\"1\":{\"66\":2}}],[\"dropping\",{\"1\":{\"19\":1}}],[\"drop\",{\"1\":{\"65\":5,\"66\":16,\"67\":8,\"68\":9}}],[\"dropout=none\",{\"1\":{\"46\":1}}],[\"dropout=self\",{\"1\":{\"46\":1}}],[\"dropout=0\",{\"1\":{\"46\":1}}],[\"dropout\",{\"1\":{\"11\":4,\"13\":4,\"17\":4,\"19\":4,\"20\":4,\"31\":4,\"32\":4,\"40\":6,\"41\":2,\"44\":2,\"46\":5,\"65\":1,\"66\":4,\"67\":2,\"68\":1}}],[\"d\",{\"1\":{\"19\":2,\"32\":1,\"38\":3,\"46\":15,\"51\":6,\"65\":2}}],[\"dtype\",{\"1\":{\"16\":1}}],[\"dtype=next\",{\"1\":{\"16\":1}}],[\"dtype=torch\",{\"1\":{\"11\":1}}],[\"depth\",{\"1\":{\"68\":1}}],[\"depth=12\",{\"1\":{\"65\":1,\"68\":1,\"72\":1}}],[\"desc\",{\"1\":{\"52\":2}}],[\"del\",{\"1\":{\"46\":3}}],[\"decode\",{\"1\":{\"30\":1,\"37\":2}}],[\"decoder模型结构图\",{\"1\":{\"44\":1,\"45\":1}}],[\"decoderlayer\",{\"0\":{\"44\":1},\"1\":{\"44\":2}}],[\"decoder\",{\"0\":{\"37\":1,\"43\":1,\"45\":1},\"1\":{\"24\":2,\"30\":1,\"36\":2,\"37\":4,\"44\":1,\"45\":3}}],[\"dense\",{\"1\":{\"13\":4,\"15\":2,\"20\":2,\"23\":2,\"40\":1}}],[\"device\",{\"1\":{\"11\":1,\"54\":5,\"56\":5,\"68\":4,\"72\":1}}],[\"device=input\",{\"1\":{\"11\":1}}],[\"defined\",{\"1\":{\"44\":1}}],[\"def\",{\"1\":{\"8\":3,\"10\":1,\"11\":2,\"13\":6,\"14\":2,\"15\":2,\"16\":2,\"17\":2,\"19\":3,\"20\":2,\"21\":2,\"23\":2,\"24\":2,\"25\":2,\"26\":2,\"29\":2,\"31\":2,\"32\":2,\"37\":4,\"38\":2,\"40\":2,\"41\":2,\"42\":2,\"44\":2,\"45\":2,\"46\":3,\"54\":8,\"55\":3,\"56\":11,\"61\":5,\"63\":2,\"64\":3,\"65\":3,\"66\":4,\"67\":2,\"68\":4,\"72\":1}}],[\"distribution\",{\"1\":{\"61\":1}}],[\"distributedsampler\",{\"1\":{\"10\":1}}],[\"dict\",{\"1\":{\"61\":2,\"72\":3}}],[\"dim代表的是卷积核的数量\",{\"1\":{\"63\":1}}],[\"dim\",{\"1\":{\"63\":8,\"64\":6,\"65\":6,\"66\":7,\"67\":17,\"68\":9}}],[\"dimensional\",{\"1\":{\"59\":1}}],[\"dim=1\",{\"1\":{\"64\":1,\"65\":1,\"68\":2}}],[\"dim=embed\",{\"1\":{\"64\":1,\"65\":1,\"68\":2}}],[\"dim=768\",{\"1\":{\"63\":1,\"64\":1,\"65\":1,\"68\":1,\"72\":1}}],[\"dim=0\",{\"1\":{\"61\":1}}],[\"dim=\",{\"1\":{\"19\":1,\"28\":3,\"29\":1,\"38\":1,\"46\":1,\"52\":4,\"67\":1}}],[\"dirname\",{\"1\":{\"54\":1,\"56\":1}}],[\"directory\",{\"1\":{\"54\":5,\"55\":1,\"56\":6}}],[\"dir\",{\"1\":{\"7\":4,\"54\":16,\"55\":7,\"56\":23}}],[\"dir=save\",{\"1\":{\"54\":1,\"56\":1}}],[\"dir=\",{\"1\":{\"7\":3}}],[\"does\",{\"1\":{\"55\":1,\"56\":1,\"61\":1}}],[\"downloaded\",{\"1\":{\"54\":1,\"56\":1}}],[\"downloading\",{\"1\":{\"54\":2,\"56\":2}}],[\"download\",{\"1\":{\"54\":3,\"56\":4,\"72\":1}}],[\"dot\",{\"1\":{\"19\":1,\"46\":1,\"51\":3,\"54\":1,\"56\":1}}],[\"doing\",{\"1\":{\"17\":1}}],[\"do\",{\"1\":{\"7\":3,\"46\":1}}],[\"dumps\",{\"1\":{\"61\":1}}],[\"dump\",{\"1\":{\"7\":1}}],[\"dall\",{\"1\":{\"49\":3}}],[\"dataloader\",{\"0\":{\"10\":1},\"1\":{\"10\":3,\"61\":1,\"62\":2}}],[\"dataset\",{\"1\":{\"8\":1,\"10\":3,\"61\":5,\"62\":6}}],[\"data\",{\"1\":{\"7\":1,\"55\":9,\"56\":9,\"57\":2,\"61\":3,\"62\":6,\"68\":4}}],[\"da\",{\"1\":{\"7\":1}}],[\"e5005f0a\",{\"1\":{\"72\":1}}],[\"eq\",{\"1\":{\"68\":1}}],[\"equivariance\",{\"1\":{\"59\":1}}],[\"equals\",{\"1\":{\"46\":1}}],[\"every\",{\"1\":{\"61\":5}}],[\"eval\",{\"1\":{\"7\":2}}],[\"emb\",{\"1\":{\"55\":2,\"56\":2}}],[\"embed\",{\"1\":{\"37\":8,\"51\":2,\"63\":9,\"64\":13,\"65\":15,\"67\":9,\"68\":17,\"72\":1}}],[\"embedding\",{\"1\":{\"11\":3,\"16\":2,\"54\":4,\"55\":1,\"56\":3,\"63\":1,\"65\":1}}],[\"embeddings\",{\"1\":{\"11\":23,\"15\":1,\"16\":2,\"24\":1,\"26\":1,\"54\":12,\"55\":4,\"56\":15}}],[\"error\",{\"1\":{\"54\":2,\"55\":1,\"56\":3}}],[\"e\",{\"1\":{\"49\":3,\"51\":6,\"54\":4,\"55\":2,\"56\":6}}],[\"each\",{\"1\":{\"24\":1,\"42\":1}}],[\"exist\",{\"1\":{\"55\":1,\"56\":1,\"61\":1}}],[\"exists\",{\"1\":{\"54\":1,\"55\":1,\"56\":2,\"61\":1}}],[\"extension\",{\"1\":{\"54\":2,\"56\":2}}],[\"extended\",{\"1\":{\"16\":6}}],[\"exception\",{\"1\":{\"54\":2,\"55\":1,\"56\":3}}],[\"except\",{\"1\":{\"54\":2,\"55\":1,\"56\":3}}],[\"exp\",{\"1\":{\"51\":1}}],[\"expand\",{\"1\":{\"11\":1,\"64\":1,\"65\":1,\"68\":1}}],[\"export\",{\"1\":{\"7\":1}}],[\"engineering\",{\"1\":{\"53\":1}}],[\"entropy\",{\"1\":{\"51\":3}}],[\"entire\",{\"1\":{\"19\":1}}],[\"encoding\",{\"1\":{\"36\":1}}],[\"encode\",{\"1\":{\"37\":2,\"52\":2}}],[\"encoder中mlp\",{\"1\":{\"69\":1}}],[\"encoder中重复堆叠encoder\",{\"1\":{\"69\":1}}],[\"encoder输出结果之后\",{\"1\":{\"68\":1}}],[\"encoder的结构\",{\"1\":{\"66\":1}}],[\"encoder提取图像特征\",{\"1\":{\"52\":1}}],[\"encoder模型结构图\",{\"1\":{\"42\":1}}],[\"encoderlayer模型结构图\",{\"1\":{\"41\":1}}],[\"encoderlayer\",{\"0\":{\"41\":1},\"1\":{\"41\":2}}],[\"encoderdecoder\",{\"1\":{\"37\":2}}],[\"encoderdecoder模型结构图\",{\"1\":{\"37\":1}}],[\"encoder\",{\"0\":{\"37\":1,\"39\":1,\"42\":1,\"66\":1},\"1\":{\"16\":2,\"30\":3,\"36\":2,\"37\":4,\"42\":3,\"51\":8,\"52\":4,\"64\":1}}],[\"encoded\",{\"1\":{\"8\":7}}],[\"end\",{\"1\":{\"28\":12,\"29\":14,\"30\":5,\"54\":4,\"56\":4}}],[\"enumerate\",{\"1\":{\"14\":1,\"54\":1,\"56\":1,\"61\":2,\"68\":1}}],[\"epoch\",{\"1\":{\"68\":2}}],[\"epochs=4\",{\"1\":{\"7\":1}}],[\"eps=1e\",{\"1\":{\"68\":1}}],[\"eps=config\",{\"1\":{\"11\":1,\"13\":1,\"20\":1,\"23\":1}}],[\"epsilon\",{\"1\":{\"68\":1}}],[\"eps\",{\"1\":{\"11\":1,\"13\":1,\"20\":1,\"23\":1}}],[\"else\",{\"1\":{\"8\":2,\"10\":1,\"13\":1,\"17\":1,\"23\":1,\"31\":1,\"32\":3,\"54\":1,\"55\":1,\"56\":2,\"61\":1,\"63\":1,\"66\":1,\"68\":1,\"72\":2}}],[\"elowen\",{\"0\":{\"3\":1}}],[\"1rkdjdlr37o7gsr9j1mhjbg\",{\"1\":{\"72\":1}}],[\"1e\",{\"1\":{\"68\":1}}],[\"1e9\",{\"1\":{\"46\":1}}],[\"197\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"196+1\",{\"1\":{\"65\":1}}],[\"196\",{\"1\":{\"63\":2,\"64\":1,\"65\":1,\"68\":1}}],[\"137mo\",{\"1\":{\"61\":1}}],[\"14模型\",{\"1\":{\"51\":1}}],[\"14则需要在256个v100\",{\"1\":{\"51\":1}}],[\"14\",{\"1\":{\"51\":2,\"54\":1,\"63\":2,\"65\":2}}],[\"16×16\",{\"1\":{\"72\":1}}],[\"16这个模型进行微调\",{\"1\":{\"72\":1}}],[\"16x16\",{\"1\":{\"62\":1}}],[\"16\",{\"1\":{\"62\":2,\"63\":3,\"72\":1}}],[\"16为例\",{\"1\":{\"62\":1,\"63\":1}}],[\"16和vit\",{\"1\":{\"51\":1}}],[\"16倍和64倍得到的\",{\"1\":{\"51\":1}}],[\"1\",{\"0\":{\"62\":1},\"1\":{\"8\":26,\"10\":1,\"11\":2,\"16\":2,\"17\":6,\"19\":5,\"26\":5,\"28\":6,\"29\":5,\"30\":2,\"31\":6,\"32\":15,\"34\":1,\"38\":1,\"41\":2,\"44\":2,\"46\":15,\"49\":1,\"52\":5,\"54\":4,\"56\":4,\"61\":2,\"62\":2,\"63\":5,\"64\":10,\"65\":10,\"67\":16,\"68\":10,\"69\":2,\"72\":2}}],[\"1=none\",{\"1\":{\"8\":3}}],[\"10\",{\"1\":{\"46\":3}}],[\"103\",{\"1\":{\"8\":1}}],[\"102\",{\"1\":{\"8\":2}}],[\"101\",{\"1\":{\"8\":2}}],[\"10000\",{\"1\":{\"16\":1}}],[\"100\",{\"1\":{\"8\":2,\"52\":1,\"54\":1,\"56\":1}}],[\"104\",{\"1\":{\"8\":1}}],[\"11929\",{\"1\":{\"72\":1}}],[\"11\",{\"1\":{\"7\":1,\"34\":2}}],[\"1200000000\",{\"1\":{\"69\":1}}],[\"128\",{\"1\":{\"28\":1}}],[\"12\",{\"1\":{\"7\":6}}],[\"21843\",{\"1\":{\"72\":1}}],[\"21000\",{\"1\":{\"72\":1}}],[\"21k\",{\"1\":{\"72\":3}}],[\"2×1000000000\",{\"1\":{\"69\":1}}],[\"2b\",{\"1\":{\"69\":1}}],[\"2d\",{\"1\":{\"63\":1}}],[\"244\",{\"1\":{\"63\":1}}],[\"2f\",{\"1\":{\"54\":2,\"56\":2}}],[\"2训练时使用的webtext数据集相似\",{\"1\":{\"51\":1}}],[\"2010\",{\"1\":{\"72\":1}}],[\"2017年的工作\",{\"1\":{\"57\":1}}],[\"2016年的工作\",{\"1\":{\"57\":1}}],[\"2018\",{\"1\":{\"7\":1}}],[\"2021\",{\"1\":{\"49\":2}}],[\"2\",{\"0\":{\"63\":1},\"1\":{\"11\":1,\"13\":1,\"16\":1,\"17\":1,\"19\":4,\"23\":1,\"25\":1,\"26\":2,\"28\":2,\"29\":3,\"41\":1,\"44\":1,\"46\":5,\"51\":1,\"52\":1,\"61\":2,\"63\":3,\"65\":1,\"67\":4}}],[\"256\",{\"1\":{\"62\":2}}],[\"255\",{\"1\":{\"62\":2}}],[\"2578\",{\"1\":{\"8\":2}}],[\"2501\",{\"1\":{\"8\":2}}],[\"224×224\",{\"1\":{\"72\":1}}],[\"224\",{\"1\":{\"62\":2,\"63\":2,\"72\":6}}],[\"224x224\",{\"1\":{\"62\":3,\"63\":1,\"72\":1}}],[\"22\",{\"1\":{\"7\":1,\"34\":1}}],[\"mlp\",{\"0\":{\"68\":1},\"1\":{\"65\":1,\"66\":14,\"68\":4,\"69\":1}}],[\"mydataset\",{\"1\":{\"61\":1,\"62\":2}}],[\"most\",{\"1\":{\"55\":10,\"56\":10}}],[\"motorcycle\",{\"1\":{\"52\":1}}],[\"mode\",{\"1\":{\"61\":2}}],[\"model是decoder输出的大小\",{\"1\":{\"38\":1}}],[\"model\",{\"1\":{\"7\":6,\"15\":1,\"29\":1,\"38\":2,\"46\":7,\"52\":2,\"54\":16,\"56\":16,\"57\":1,\"68\":2,\"72\":6}}],[\"models\",{\"1\":{\"7\":1,\"72\":1}}],[\"modulelist\",{\"1\":{\"14\":1}}],[\"module\",{\"1\":{\"11\":1,\"13\":3,\"14\":3,\"15\":1,\"19\":1,\"20\":1,\"21\":1,\"23\":1,\"24\":1,\"25\":1,\"37\":1,\"38\":1,\"40\":1,\"41\":1,\"42\":1,\"44\":1,\"45\":1,\"46\":1,\"63\":1,\"64\":2,\"65\":1,\"66\":2,\"67\":1,\"68\":1}}],[\"m\",{\"1\":{\"44\":3,\"69\":1}}],[\"methods\",{\"1\":{\"53\":1}}],[\"metamind\",{\"1\":{\"0\":1}}],[\"memory\",{\"1\":{\"37\":2,\"44\":2,\"45\":2}}],[\"multiply\",{\"1\":{\"67\":2}}],[\"multiple\",{\"1\":{\"32\":1}}],[\"multiheadedattention\",{\"1\":{\"46\":2}}],[\"multi\",{\"1\":{\"36\":1,\"66\":1}}],[\"million\",{\"1\":{\"69\":1}}],[\"mixer\",{\"1\":{\"66\":1}}],[\"mixed\",{\"1\":{\"19\":6}}],[\"minibatch\",{\"1\":{\"51\":2}}],[\"min\",{\"1\":{\"29\":4,\"54\":1,\"56\":1}}],[\"might\",{\"1\":{\"19\":1}}],[\"mseloss\",{\"1\":{\"17\":1}}],[\"make\",{\"1\":{\"67\":1}}],[\"made\",{\"1\":{\"44\":1}}],[\"macos\",{\"1\":{\"34\":1}}],[\"matplotlib\",{\"1\":{\"56\":1}}],[\"matching\",{\"1\":{\"55\":11,\"56\":11}}],[\"math\",{\"1\":{\"19\":1,\"46\":1}}],[\"matmul\",{\"1\":{\"19\":2,\"46\":2}}],[\"map\",{\"1\":{\"10\":1,\"72\":1}}],[\"mask部分相关的掩码逻辑\",{\"1\":{\"67\":1}}],[\"mask和casual\",{\"1\":{\"67\":1}}],[\"masking\",{\"1\":{\"36\":1,\"45\":1}}],[\"masked\",{\"1\":{\"26\":5,\"37\":1,\"46\":1}}],[\"mask=mask\",{\"1\":{\"46\":1}}],[\"mask=head\",{\"1\":{\"17\":1,\"26\":1,\"31\":1,\"32\":1}}],[\"mask=none\",{\"1\":{\"13\":1,\"14\":2,\"16\":2,\"17\":2,\"19\":2,\"21\":1,\"26\":2,\"29\":2,\"31\":2,\"32\":2,\"46\":2}}],[\"mask=attention\",{\"1\":{\"8\":1,\"17\":1,\"26\":1,\"28\":1,\"29\":1,\"31\":1,\"32\":1}}],[\"mask作用图解\",{\"1\":{\"8\":2}}],[\"mask\",{\"1\":{\"8\":14,\"10\":4,\"13\":1,\"14\":2,\"16\":8,\"17\":3,\"19\":3,\"21\":1,\"26\":2,\"28\":1,\"29\":1,\"31\":4,\"32\":6,\"36\":1,\"37\":11,\"41\":2,\"42\":3,\"44\":4,\"45\":4,\"46\":6,\"67\":2}}],[\"max\",{\"1\":{\"7\":1,\"8\":6,\"10\":7,\"11\":1,\"29\":4,\"68\":1}}],[\"abs\",{\"1\":{\"72\":1}}],[\"about\",{\"1\":{\"52\":1}}],[\"axis\",{\"1\":{\"55\":1,\"56\":1}}],[\"axis=0\",{\"1\":{\"51\":1}}],[\"axis=1\",{\"1\":{\"51\":3,\"54\":3,\"56\":3}}],[\"accu\",{\"1\":{\"68\":1}}],[\"accuracy\",{\"1\":{\"54\":8,\"56\":6}}],[\"acc\",{\"1\":{\"54\":2,\"56\":2}}],[\"actual\",{\"1\":{\"54\":2,\"56\":2}}],[\"actually\",{\"1\":{\"19\":1}}],[\"active\",{\"1\":{\"31\":8}}],[\"activation\",{\"1\":{\"15\":2}}],[\"activate\",{\"1\":{\"7\":1,\"34\":1}}],[\"act2fn\",{\"1\":{\"13\":1,\"23\":1}}],[\"act\",{\"1\":{\"13\":7,\"23\":7,\"65\":1,\"66\":6,\"68\":5}}],[\"available\",{\"1\":{\"54\":1,\"56\":1}}],[\"at\",{\"1\":{\"52\":1}}],[\"attn\",{\"1\":{\"41\":4,\"44\":10,\"46\":7,\"65\":1,\"66\":4,\"67\":10,\"68\":2}}],[\"attenion\",{\"1\":{\"40\":1}}],[\"attend\",{\"1\":{\"19\":1,\"35\":1}}],[\"attention的heads数\",{\"1\":{\"69\":1}}],[\"attention可以用矩阵乘法一次计算所有的时刻\",{\"1\":{\"35\":1}}],[\"attention机制\",{\"1\":{\"35\":1}}],[\"attentions\",{\"1\":{\"17\":1,\"19\":2,\"26\":1,\"32\":1}}],[\"attention\",{\"1\":{\"8\":6,\"10\":4,\"13\":7,\"14\":2,\"16\":8,\"17\":3,\"19\":26,\"21\":4,\"26\":2,\"28\":1,\"29\":2,\"31\":4,\"32\":6,\"36\":2,\"46\":4,\"66\":2,\"67\":2}}],[\"american\",{\"1\":{\"52\":1}}],[\"annotated\",{\"1\":{\"34\":5}}],[\"answer\",{\"1\":{\"30\":3}}],[\"an\",{\"1\":{\"24\":1,\"52\":1}}],[\"and\",{\"1\":{\"8\":1,\"13\":1,\"17\":1,\"19\":1,\"23\":1,\"26\":1,\"28\":1,\"29\":1,\"37\":2,\"42\":1,\"44\":1,\"46\":2,\"52\":1,\"53\":1,\"66\":1,\"72\":1}}],[\"applied\",{\"1\":{\"46\":1}}],[\"apply\",{\"1\":{\"19\":1,\"46\":2,\"68\":1}}],[\"append\",{\"1\":{\"8\":1,\"54\":3,\"56\":3,\"61\":5}}],[\"add\",{\"1\":{\"17\":1}}],[\"astronaut\",{\"1\":{\"52\":1}}],[\"assume\",{\"1\":{\"46\":1}}],[\"assert\",{\"1\":{\"46\":1,\"61\":1}}],[\"as\",{\"1\":{\"11\":1,\"24\":1,\"54\":2,\"55\":1,\"56\":5,\"61\":2,\"66\":1,\"67\":1}}],[\"arxiv\",{\"1\":{\"72\":1}}],[\"array\",{\"1\":{\"54\":2,\"56\":2}}],[\"argmax\",{\"1\":{\"28\":2,\"54\":1,\"55\":1,\"56\":2}}],[\"args\",{\"1\":{\"10\":1,\"64\":1,\"72\":1}}],[\"are\",{\"1\":{\"17\":2,\"24\":1,\"29\":1}}],[\"arange\",{\"1\":{\"11\":1,\"51\":1}}],[\"align=\",{\"1\":{\"61\":1}}],[\"aligned\",{\"1\":{\"51\":2}}],[\"alexnet\",{\"1\":{\"54\":1}}],[\"always\",{\"1\":{\"46\":1}}],[\"all\",{\"1\":{\"8\":5,\"10\":16,\"19\":6,\"46\":3,\"54\":3,\"55\":1,\"56\":3}}],[\"already\",{\"1\":{\"8\":1}}],[\"a\",{\"1\":{\"7\":3,\"8\":4,\"19\":1,\"32\":1,\"42\":1,\"46\":2,\"52\":16,\"53\":3,\"54\":1,\"55\":2,\"56\":3}}],[\"hybrid\",{\"1\":{\"71\":1}}],[\"h`和\",{\"1\":{\"63\":1}}],[\"hub\",{\"1\":{\"56\":1}}],[\"huggingface\",{\"1\":{\"56\":1}}],[\"horse\",{\"1\":{\"52\":1}}],[\"happy\",{\"1\":{\"67\":1}}],[\"ha=\",{\"1\":{\"61\":1}}],[\"harvardnlp\",{\"1\":{\"34\":1}}],[\"has\",{\"1\":{\"8\":2,\"72\":3}}],[\"height\",{\"1\":{\"61\":1}}],[\"here\",{\"1\":{\"17\":1}}],[\"head结构由linear+tanh激活函数+linear组成\",{\"1\":{\"68\":1}}],[\"head进行分类\",{\"1\":{\"68\":1}}],[\"head的位置是和这个\",{\"1\":{\"64\":1}}],[\"head之中再输出分类结果\",{\"1\":{\"64\":1}}],[\"heads代表transformer中multi\",{\"1\":{\"69\":1}}],[\"heads=8\",{\"1\":{\"67\":1}}],[\"heads=num\",{\"1\":{\"66\":1,\"68\":1}}],[\"heads=12\",{\"1\":{\"65\":1,\"68\":1,\"72\":1}}],[\"heads\",{\"1\":{\"19\":7,\"46\":3,\"66\":2,\"67\":12,\"68\":1}}],[\"head\",{\"0\":{\"68\":1},\"1\":{\"14\":2,\"16\":1,\"17\":2,\"19\":10,\"26\":2,\"29\":1,\"31\":2,\"32\":2,\"36\":1,\"64\":1,\"65\":1,\"66\":1,\"67\":8,\"68\":3,\"69\":1,\"72\":1}}],[\"hidden\",{\"1\":{\"11\":5,\"13\":24,\"14\":5,\"15\":5,\"17\":4,\"19\":8,\"20\":12,\"23\":15,\"24\":7,\"25\":1,\"26\":1,\"28\":6,\"29\":2,\"31\":3,\"32\":4,\"66\":9,\"69\":1}}],[\"h\",{\"1\":{\"7\":3,\"46\":9,\"51\":1,\"63\":4,\"64\":1,\"65\":1,\"68\":1}}],[\"https\",{\"1\":{\"7\":2,\"34\":1,\"61\":2,\"72\":4}}],[\"95\",{\"1\":{\"54\":1}}],[\"9\",{\"1\":{\"7\":1,\"34\":1}}],[\"n为序列长度\",{\"1\":{\"67\":1}}],[\"natural\",{\"1\":{\"53\":1}}],[\"named\",{\"1\":{\"72\":1}}],[\"name=\",{\"1\":{\"7\":1}}],[\"name\",{\"1\":{\"7\":1,\"54\":8,\"56\":8,\"72\":4}}],[\"np\",{\"1\":{\"51\":5,\"52\":1,\"54\":6,\"55\":1,\"56\":8}}],[\"nlp\",{\"1\":{\"51\":1}}],[\"nbatches\",{\"1\":{\"46\":3}}],[\"n个解码器层\",{\"1\":{\"45\":1}}],[\"nn\",{\"1\":{\"11\":5,\"13\":6,\"14\":2,\"15\":3,\"17\":2,\"19\":6,\"20\":3,\"21\":1,\"23\":2,\"24\":3,\"25\":2,\"28\":1,\"29\":1,\"31\":2,\"32\":2,\"37\":1,\"38\":2,\"40\":2,\"41\":1,\"42\":1,\"44\":1,\"45\":1,\"46\":3,\"63\":5,\"64\":4,\"65\":6,\"66\":6,\"67\":5,\"68\":14}}],[\"no\",{\"1\":{\"52\":2,\"54\":2,\"55\":1,\"56\":3}}],[\"norm2\",{\"1\":{\"66\":2}}],[\"norm1\",{\"1\":{\"66\":2}}],[\"norm层之后同样是多头注意力层\",{\"1\":{\"66\":1}}],[\"normal\",{\"1\":{\"64\":1,\"65\":2,\"68\":2}}],[\"normalization\",{\"1\":{\"36\":1,\"66\":1}}],[\"normalize\",{\"1\":{\"19\":1,\"51\":2,\"62\":2}}],[\"norm\",{\"1\":{\"11\":1,\"13\":1,\"20\":1,\"23\":1,\"40\":2,\"42\":2,\"45\":2,\"52\":2,\"54\":2,\"56\":2,\"63\":6,\"65\":1,\"66\":3,\"68\":7}}],[\"not\",{\"1\":{\"8\":1,\"17\":1,\"19\":1,\"26\":2,\"29\":2,\"31\":2,\"32\":4,\"46\":3,\"54\":3,\"55\":2,\"56\":5,\"61\":2,\"72\":2,\"76\":1}}],[\"none\",{\"1\":{\"8\":4,\"11\":2,\"17\":2,\"19\":1,\"26\":2,\"29\":2,\"31\":2,\"32\":7,\"46\":4,\"54\":3,\"55\":2,\"56\":4,\"61\":2,\"63\":1,\"72\":1}}],[\"neighborhood\",{\"1\":{\"59\":1}}],[\"needed\",{\"1\":{\"54\":2,\"56\":2}}],[\"networks\",{\"1\":{\"66\":1}}],[\"network\",{\"1\":{\"36\":1}}],[\"netpoll\",{\"1\":{\"2\":1}}],[\"neural\",{\"1\":{\"35\":1,\"36\":1}}],[\"ner\",{\"1\":{\"31\":1}}],[\"new\",{\"1\":{\"19\":4}}],[\"next\",{\"1\":{\"8\":1,\"26\":5}}],[\"nsp\",{\"1\":{\"8\":1}}],[\"nsp任务\",{\"1\":{\"8\":1}}],[\"null\",{\"1\":{\"8\":1}}],[\"numpy\",{\"1\":{\"52\":2,\"54\":4,\"56\":3,\"62\":2}}],[\"number\",{\"1\":{\"46\":1,\"61\":1}}],[\"num\",{\"1\":{\"7\":1,\"14\":1,\"17\":5,\"19\":5,\"29\":3,\"31\":6,\"32\":9,\"54\":2,\"56\":2,\"61\":5,\"63\":5,\"64\":9,\"65\":11,\"66\":2,\"67\":24,\"68\":18,\"72\":4}}],[\"n\",{\"1\":{\"7\":1,\"34\":1,\"42\":3,\"45\":3,\"51\":7,\"57\":1,\"67\":3}}],[\"s=str\",{\"1\":{\"61\":1}}],[\"symlinks=false\",{\"1\":{\"54\":1,\"56\":1}}],[\"systematic\",{\"1\":{\"53\":1}}],[\"sys\",{\"1\":{\"13\":1,\"23\":1}}],[\"snapshot\",{\"1\":{\"54\":1,\"56\":2}}],[\"shuffle=false\",{\"1\":{\"62\":1}}],[\"shuffle=true\",{\"1\":{\"62\":1}}],[\"show\",{\"1\":{\"55\":1,\"56\":1,\"61\":1}}],[\"shot性能评估\",{\"1\":{\"57\":1}}],[\"shot性能\",{\"1\":{\"57\":1}}],[\"shot迁移到下游任务\",{\"1\":{\"57\":1}}],[\"shot学习\",{\"1\":{\"57\":1}}],[\"shot推理\",{\"1\":{\"54\":1}}],[\"shot分类时\",{\"1\":{\"53\":1}}],[\"shot分类的过程相当直接\",{\"1\":{\"52\":1}}],[\"shot图像分类\",{\"1\":{\"52\":1}}],[\"shape为\",{\"1\":{\"63\":1}}],[\"shape\",{\"1\":{\"19\":4,\"32\":1,\"63\":1,\"64\":1,\"65\":1,\"67\":1,\"68\":2}}],[\"scale未指定\",{\"1\":{\"67\":1}}],[\"scale\",{\"1\":{\"66\":1,\"67\":3,\"68\":1}}],[\"scale=qk\",{\"1\":{\"66\":1,\"68\":1}}],[\"scale=none\",{\"1\":{\"65\":1,\"66\":1,\"67\":1,\"68\":1}}],[\"scaled\",{\"1\":{\"46\":1}}],[\"score\",{\"1\":{\"25\":2,\"26\":4}}],[\"scores\",{\"1\":{\"19\":12,\"25\":2,\"26\":4,\"28\":4,\"31\":1,\"46\":4}}],[\"sum\",{\"1\":{\"61\":1,\"68\":1}}],[\"supported\",{\"1\":{\"61\":2}}],[\"supervised\",{\"1\":{\"57\":1}}],[\"super\",{\"1\":{\"11\":1,\"13\":3,\"14\":1,\"15\":1,\"16\":1,\"17\":1,\"19\":1,\"20\":1,\"21\":1,\"23\":1,\"24\":1,\"25\":1,\"26\":1,\"29\":1,\"31\":1,\"32\":1,\"37\":1,\"38\":1,\"40\":1,\"41\":1,\"42\":1,\"44\":1,\"45\":1,\"46\":1,\"63\":1,\"64\":1,\"65\":1,\"66\":2,\"67\":1,\"68\":1}}],[\"sunflowers\",{\"1\":{\"55\":1,\"56\":1}}],[\"sub\",{\"1\":{\"54\":5,\"56\":5}}],[\"sublayer是传入的参数\",{\"1\":{\"40\":1}}],[\"sublayer\",{\"1\":{\"40\":3,\"41\":3,\"44\":4}}],[\"sublayerconnection模型结构图\",{\"1\":{\"40\":1}}],[\"sublayerconnection\",{\"0\":{\"40\":1},\"1\":{\"40\":2,\"41\":1,\"44\":1}}],[\"successfully\",{\"1\":{\"54\":1,\"56\":1}}],[\"survey\",{\"1\":{\"53\":1}}],[\"src\",{\"1\":{\"37\":16,\"44\":7,\"45\":2}}],[\"srl\",{\"1\":{\"31\":1}}],[\"skip\",{\"1\":{\"30\":2}}],[\"squad\",{\"1\":{\"30\":1}}],[\"squeeze\",{\"1\":{\"28\":2,\"29\":2}}],[\"sqrt\",{\"1\":{\"19\":1,\"46\":1}}],[\"source\",{\"1\":{\"72\":1}}],[\"southampton\",{\"1\":{\"7\":1}}],[\"sort\",{\"1\":{\"61\":1}}],[\"sometimes\",{\"1\":{\"29\":1}}],[\"softmax\",{\"1\":{\"19\":1,\"28\":3,\"36\":1,\"38\":1,\"46\":1,\"52\":1,\"67\":1}}],[\"silhouette\",{\"1\":{\"52\":1}}],[\"similarities\",{\"1\":{\"54\":2,\"55\":2,\"56\":4}}],[\"similarity\",{\"1\":{\"51\":1,\"52\":1,\"54\":2,\"55\":1,\"56\":3}}],[\"simply\",{\"1\":{\"15\":1}}],[\"size的四倍\",{\"1\":{\"69\":1}}],[\"size是transformer\",{\"1\":{\"69\":1}}],[\"size就是对应通过embedding层后每个token的dim\",{\"1\":{\"69\":1}}],[\"size\",{\"1\":{\"10\":1,\"11\":7,\"13\":5,\"15\":2,\"17\":1,\"19\":15,\"20\":3,\"23\":3,\"24\":3,\"25\":1,\"26\":1,\"28\":12,\"29\":3,\"31\":2,\"32\":13,\"40\":2,\"41\":4,\"42\":1,\"44\":4,\"45\":1,\"46\":3,\"54\":4,\"56\":4,\"61\":1,\"62\":2,\"63\":24,\"64\":4,\"65\":2,\"67\":10,\"68\":4}}],[\"size=768\",{\"1\":{\"72\":1}}],[\"size=none\",{\"1\":{\"65\":1,\"68\":1}}],[\"size=img\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"size=patch\",{\"1\":{\"63\":1,\"64\":1,\"65\":1,\"68\":1}}],[\"size=224\",{\"1\":{\"63\":1,\"64\":1,\"65\":1,\"68\":1,\"72\":1}}],[\"size=batch\",{\"1\":{\"62\":2}}],[\"size=64\",{\"1\":{\"54\":2,\"56\":2}}],[\"size=args\",{\"1\":{\"10\":1}}],[\"size=16\",{\"1\":{\"7\":2,\"63\":1,\"64\":1,\"65\":1,\"68\":1,\"72\":1}}],[\"saucer\",{\"1\":{\"52\":1}}],[\"sample\",{\"1\":{\"61\":1,\"68\":1}}],[\"sampler=train\",{\"1\":{\"10\":1}}],[\"sampler\",{\"1\":{\"10\":2}}],[\"same\",{\"1\":{\"24\":1,\"46\":1}}],[\"save\",{\"1\":{\"7\":1,\"54\":3,\"56\":3}}],[\"s\",{\"1\":{\"8\":1,\"61\":1,\"72\":1}}],[\"span\",{\"1\":{\"28\":1,\"30\":2}}],[\"splitext\",{\"1\":{\"54\":1,\"56\":1,\"61\":1}}],[\"split\",{\"1\":{\"28\":2,\"29\":1,\"54\":1,\"56\":1,\"61\":1,\"62\":1}}],[\"special\",{\"1\":{\"8\":6,\"30\":2}}],[\"spring\",{\"1\":{\"2\":1}}],[\"seed\",{\"1\":{\"61\":1}}],[\"seem\",{\"1\":{\"19\":1}}],[\"searchpicbytext\",{\"1\":{\"55\":1,\"56\":1}}],[\"segmentation\",{\"1\":{\"11\":1,\"52\":1}}],[\"segment\",{\"1\":{\"8\":1}}],[\"seconds\",{\"1\":{\"54\":2,\"56\":1}}],[\"second\",{\"1\":{\"8\":1}}],[\"self\",{\"1\":{\"8\":9,\"11\":13,\"13\":26,\"14\":5,\"15\":7,\"16\":11,\"17\":14,\"19\":28,\"20\":9,\"21\":11,\"23\":10,\"24\":9,\"25\":7,\"26\":8,\"28\":3,\"29\":8,\"31\":12,\"32\":9,\"36\":2,\"37\":16,\"38\":5,\"40\":8,\"41\":15,\"42\":7,\"44\":19,\"45\":7,\"46\":15,\"61\":12,\"63\":12,\"64\":16,\"65\":22,\"66\":25,\"67\":16,\"68\":33}}],[\"sep\",{\"1\":{\"8\":11,\"28\":2,\"30\":2,\"32\":2}}],[\"sentence\",{\"1\":{\"8\":1,\"26\":5}}],[\"sequential\",{\"1\":{\"68\":2}}],[\"sequence\",{\"1\":{\"8\":4,\"16\":3,\"25\":2,\"26\":2,\"28\":3,\"29\":2,\"31\":4}}],[\"sequences\",{\"1\":{\"8\":2,\"37\":1}}],[\"seq\",{\"1\":{\"7\":1,\"11\":2,\"19\":2,\"25\":4,\"26\":4,\"28\":4,\"29\":4,\"31\":2,\"32\":4,\"46\":1}}],[\"std=0\",{\"1\":{\"64\":1,\"65\":2,\"68\":2}}],[\"staticmethod\",{\"1\":{\"61\":2}}],[\"state\",{\"1\":{\"15\":1,\"72\":1}}],[\"states\",{\"1\":{\"13\":16,\"14\":4,\"15\":2,\"17\":2,\"19\":4,\"20\":8,\"23\":8,\"24\":6,\"26\":1,\"32\":1}}],[\"standing\",{\"1\":{\"52\":2}}],[\"start\",{\"1\":{\"28\":12,\"29\":15,\"30\":5,\"54\":5,\"56\":5}}],[\"stack\",{\"1\":{\"10\":1,\"42\":1,\"52\":1,\"61\":1}}],[\"strict=false\",{\"1\":{\"72\":1}}],[\"stride=patch\",{\"1\":{\"63\":1}}],[\"stride\",{\"1\":{\"63\":1}}],[\"structure\",{\"1\":{\"59\":1}}],[\"str\",{\"1\":{\"13\":1,\"23\":1,\"61\":3}}],[\"steps=100\",{\"1\":{\"7\":2}}],[\"step\",{\"1\":{\"7\":4,\"68\":1}}],[\"storage\",{\"1\":{\"7\":1}}],[\"grid\",{\"1\":{\"63\":3}}],[\"grams\",{\"1\":{\"57\":2}}],[\"grad\",{\"1\":{\"52\":2,\"54\":2,\"56\":2,\"72\":1}}],[\"garage\",{\"1\":{\"52\":1}}],[\"gelu\",{\"1\":{\"66\":4,\"68\":2}}],[\"generic\",{\"1\":{\"45\":1}}],[\"generator模型结构图\",{\"1\":{\"38\":1}}],[\"generator\",{\"0\":{\"38\":1},\"1\":{\"37\":3,\"38\":2}}],[\"getitem\",{\"1\":{\"61\":1}}],[\"getcwd\",{\"1\":{\"54\":1,\"56\":1}}],[\"get\",{\"1\":{\"8\":1,\"19\":1,\"54\":10,\"55\":4,\"56\":12}}],[\"gpt\",{\"1\":{\"30\":2}}],[\"gpu上训练12天\",{\"1\":{\"51\":1}}],[\"gpu上训练18天\",{\"1\":{\"51\":1}}],[\"gpu上的矩阵运算都是充分优化和高度并行的\",{\"1\":{\"35\":1}}],[\"gpu\",{\"1\":{\"7\":2,\"35\":1}}],[\"gleu\",{\"1\":{\"13\":2}}],[\"guid\",{\"1\":{\"8\":1}}],[\"google\",{\"1\":{\"72\":2}}],[\"googleapis\",{\"1\":{\"7\":1}}],[\"golang\",{\"1\":{\"2\":1}}],[\"github\",{\"1\":{\"7\":1,\"34\":1,\"61\":1,\"72\":2}}],[\"git\",{\"1\":{\"7\":2,\"34\":1}}],[\"bool\",{\"1\":{\"72\":1}}],[\"b为批量大小\",{\"1\":{\"67\":1}}],[\"block第一个全连接的节点个数\",{\"1\":{\"69\":1}}],[\"block的次数\",{\"1\":{\"69\":1}}],[\"blocks\",{\"1\":{\"68\":2}}],[\"block块序列\",{\"1\":{\"68\":2}}],[\"block\",{\"1\":{\"66\":3,\"68\":1}}],[\"blob\",{\"1\":{\"61\":1}}],[\"black\",{\"1\":{\"52\":1}}],[\"but\",{\"1\":{\"19\":1,\"24\":1}}],[\"build\",{\"1\":{\"8\":1}}],[\"billion\",{\"1\":{\"69\":1}}],[\"bias=qkv\",{\"1\":{\"66\":1,\"67\":1,\"68\":1}}],[\"bias=true\",{\"1\":{\"65\":1,\"68\":1}}],[\"bias=false\",{\"1\":{\"24\":1,\"66\":1,\"67\":1}}],[\"bias\",{\"1\":{\"24\":3,\"59\":1,\"66\":1,\"67\":1,\"68\":1}}],[\"bit\",{\"1\":{\"19\":1}}],[\"bin\",{\"1\":{\"7\":1}}],[\"binary\",{\"0\":{\"2\":1},\"1\":{\"0\":1}}],[\"binaryoracle\",{\"1\":{\"0\":1}}],[\"bytenet和convs2s等网络模型\",{\"1\":{\"35\":1}}],[\"by\",{\"1\":{\"15\":1}}],[\"below\",{\"1\":{\"44\":1}}],[\"between\",{\"1\":{\"19\":1}}],[\"be\",{\"1\":{\"8\":1}}],[\"bert支持的下游任务图\",{\"1\":{\"27\":1}}],[\"bertformultiplechoice\",{\"1\":{\"32\":2}}],[\"bertfortokenclassification\",{\"1\":{\"31\":2}}],[\"bertforquestionanswering\",{\"1\":{\"28\":1,\"29\":2}}],[\"bertforpretraining结构图\",{\"1\":{\"26\":1}}],[\"bertforpretraining\",{\"0\":{\"26\":1},\"1\":{\"26\":2}}],[\"bertforsequenceclassification模型结构图\",{\"1\":{\"17\":1}}],[\"bertforsequenceclassification\",{\"0\":{\"17\":1},\"1\":{\"17\":2}}],[\"bertlmpredictionhead结构图\",{\"1\":{\"24\":1}}],[\"bertlmpredictionhead\",{\"0\":{\"24\":1},\"1\":{\"24\":2,\"25\":1}}],[\"bertlayer模型结构图\",{\"1\":{\"13\":1}}],[\"bertlayer\",{\"0\":{\"13\":1},\"1\":{\"13\":2,\"14\":1}}],[\"bertlayernorm\",{\"1\":{\"11\":1,\"13\":1,\"20\":1,\"23\":1}}],[\"bertselfoutput计算流程图\",{\"1\":{\"20\":1}}],[\"bertselfoutput\",{\"0\":{\"20\":1},\"1\":{\"20\":2,\"21\":1}}],[\"bertselfattention\",{\"0\":{\"19\":1},\"1\":{\"19\":2,\"21\":1}}],[\"bertpretrainingheads结构图\",{\"1\":{\"25\":1}}],[\"bertpretrainingheads\",{\"0\":{\"25\":1},\"1\":{\"25\":2,\"26\":1}}],[\"bertpretrainedmodel\",{\"1\":{\"16\":1,\"17\":1,\"26\":1,\"29\":1,\"31\":1,\"32\":1}}],[\"bertpredictionheadtransform结构图\",{\"1\":{\"23\":1}}],[\"bertpredictionheadtransform\",{\"0\":{\"23\":1},\"1\":{\"23\":2,\"24\":1}}],[\"bertpooler模型结构图\",{\"1\":{\"15\":1}}],[\"bertpooler\",{\"0\":{\"15\":1},\"1\":{\"15\":2,\"16\":1}}],[\"bertmodel模型结构图\",{\"1\":{\"16\":1}}],[\"bertmodel\",{\"0\":{\"16\":1},\"1\":{\"16\":2,\"17\":1,\"19\":1,\"26\":1,\"29\":1,\"31\":1,\"32\":1}}],[\"bertattention计算流程图\",{\"1\":{\"21\":1}}],[\"bertattention\",{\"0\":{\"18\":1,\"21\":1},\"1\":{\"13\":1,\"21\":2}}],[\"bertoutput\",{\"1\":{\"13\":3}}],[\"bertintermediate\",{\"1\":{\"13\":3}}],[\"bertencoder模型结构图\",{\"1\":{\"14\":1}}],[\"bertencoder\",{\"0\":{\"12\":1,\"14\":1},\"1\":{\"14\":2,\"16\":1}}],[\"bertembeddings\",{\"0\":{\"11\":1},\"1\":{\"11\":2,\"16\":1}}],[\"berttokenizer\",{\"1\":{\"8\":3,\"28\":1}}],[\"berttokenizer中的特殊token\",{\"1\":{\"8\":1}}],[\"bert文本分类实战\",{\"1\":{\"6\":1}}],[\"bert\",{\"0\":{\"6\":1},\"1\":{\"7\":16,\"8\":1,\"17\":2,\"26\":2,\"28\":5,\"29\":2,\"30\":9,\"31\":2,\"32\":7}}],[\"b\",{\"1\":{\"8\":1,\"32\":1,\"51\":2,\"62\":3,\"63\":8,\"64\":7,\"65\":5,\"67\":3,\"68\":5,\"69\":1,\"72\":2}}],[\"backward\",{\"1\":{\"68\":1}}],[\"bar\",{\"1\":{\"61\":1}}],[\"bart\",{\"1\":{\"30\":1}}],[\"baidu\",{\"1\":{\"61\":1,\"72\":1}}],[\"bag\",{\"1\":{\"57\":3}}],[\"batches\",{\"1\":{\"46\":1,\"54\":2,\"56\":2}}],[\"batch\",{\"1\":{\"7\":2,\"10\":4,\"19\":2,\"28\":6,\"29\":3,\"31\":2,\"32\":7,\"46\":2,\"54\":9,\"56\":9,\"61\":4,\"62\":2,\"67\":10}}],[\"basename\",{\"1\":{\"54\":1,\"56\":1}}],[\"base\",{\"1\":{\"7\":6,\"62\":1,\"72\":7}}],[\"l为灰度图片\",{\"1\":{\"61\":1}}],[\"l2\",{\"1\":{\"51\":2}}],[\"lm\",{\"1\":{\"26\":5}}],[\"list\",{\"1\":{\"61\":2}}],[\"listdir\",{\"1\":{\"54\":1,\"56\":1,\"61\":2}}],[\"list列表组装起来得到需要的dataset\",{\"1\":{\"8\":1}}],[\"linalg\",{\"1\":{\"54\":2,\"56\":2}}],[\"lin\",{\"1\":{\"46\":2}}],[\"linears\",{\"1\":{\"46\":3}}],[\"linear\",{\"1\":{\"13\":2,\"15\":1,\"17\":1,\"19\":3,\"20\":1,\"23\":1,\"24\":1,\"25\":1,\"28\":1,\"29\":1,\"31\":1,\"32\":1,\"38\":1,\"46\":3,\"66\":2,\"67\":2,\"68\":2}}],[\"like\",{\"1\":{\"11\":1}}],[\"large\",{\"1\":{\"54\":2,\"56\":1,\"57\":1}}],[\"launchpad\",{\"1\":{\"52\":1}}],[\"lambda\",{\"1\":{\"41\":1,\"44\":2}}],[\"layer=act\",{\"1\":{\"66\":1,\"68\":1}}],[\"layer=norm\",{\"1\":{\"68\":1}}],[\"layer=none\",{\"1\":{\"63\":1,\"64\":1,\"65\":3,\"68\":3}}],[\"layer=nn\",{\"1\":{\"66\":3}}],[\"layers\",{\"1\":{\"14\":1,\"19\":1,\"42\":3,\"45\":2}}],[\"layer\",{\"1\":{\"11\":1,\"13\":3,\"14\":4,\"19\":21,\"20\":1,\"23\":1,\"36\":1,\"42\":6,\"45\":6,\"63\":3,\"64\":2,\"65\":1,\"66\":4,\"68\":8}}],[\"layernorm\",{\"1\":{\"11\":2,\"13\":2,\"20\":2,\"23\":2,\"40\":2,\"42\":1,\"45\":1,\"66\":1,\"68\":2}}],[\"label=none\",{\"1\":{\"26\":1}}],[\"label=label\",{\"1\":{\"8\":1}}],[\"labels=none\",{\"1\":{\"17\":1,\"26\":1,\"31\":1,\"32\":1}}],[\"labels\",{\"1\":{\"8\":1,\"10\":2,\"17\":8,\"26\":2,\"29\":3,\"31\":11,\"32\":2,\"51\":3,\"52\":1,\"61\":4,\"68\":3}}],[\"label\",{\"1\":{\"8\":2,\"26\":2,\"52\":1,\"53\":2,\"61\":8,\"62\":4}}],[\"language\",{\"0\":{\"4\":1},\"1\":{\"50\":1,\"53\":1,\"57\":1}}],[\"location=device\",{\"1\":{\"72\":1}}],[\"locality\",{\"1\":{\"59\":1}}],[\"local\",{\"1\":{\"10\":1,\"54\":2,\"56\":2}}],[\"load\",{\"1\":{\"72\":2}}],[\"loader\",{\"1\":{\"62\":2,\"68\":2}}],[\"loading\",{\"1\":{\"54\":1,\"56\":1}}],[\"looking\",{\"1\":{\"52\":1}}],[\"loss\",{\"1\":{\"17\":8,\"26\":10,\"29\":10,\"31\":11,\"32\":5,\"51\":8,\"68\":3}}],[\"log\",{\"1\":{\"38\":1}}],[\"logits外\",{\"1\":{\"72\":1}}],[\"logits=false\",{\"1\":{\"72\":1}}],[\"logits\",{\"1\":{\"17\":5,\"28\":12,\"29\":15,\"31\":6,\"32\":6,\"51\":3,\"68\":3,\"72\":3}}],[\"logging\",{\"1\":{\"7\":1}}],[\"long\",{\"1\":{\"11\":1}}],[\"lower\",{\"1\":{\"7\":1,\"54\":1,\"56\":1}}],[\"learnable\",{\"1\":{\"65\":1}}],[\"learned\",{\"1\":{\"51\":3}}],[\"learning的核心思想是通过设计合适的prompt\",{\"1\":{\"53\":1}}],[\"learning或prompt\",{\"1\":{\"53\":1}}],[\"learning\",{\"1\":{\"7\":1,\"57\":2}}],[\"left\",{\"1\":{\"41\":1}}],[\"lens\",{\"1\":{\"8\":1,\"10\":2}}],[\"len=input\",{\"1\":{\"8\":1}}],[\"length长度\",{\"1\":{\"8\":1}}],[\"length\",{\"1\":{\"8\":11,\"10\":1,\"11\":2,\"28\":4,\"32\":4}}],[\"length=128\",{\"1\":{\"7\":1}}],[\"len\",{\"1\":{\"8\":13,\"10\":5,\"19\":2,\"29\":4,\"31\":2,\"46\":1,\"54\":1,\"56\":1,\"61\":8}}],[\"l\",{\"1\":{\"7\":3,\"51\":5,\"54\":1}}],[\"llm\",{\"1\":{\"3\":1}}],[\"v0\",{\"1\":{\"72\":1}}],[\"v计算来源相同\",{\"1\":{\"67\":1}}],[\"v矩阵\",{\"1\":{\"67\":1}}],[\"v时使用偏置\",{\"1\":{\"67\":1}}],[\"validation\",{\"1\":{\"61\":1}}],[\"val\",{\"1\":{\"61\":13,\"62\":7}}],[\"valueerror\",{\"1\":{\"61\":1}}],[\"value\",{\"1\":{\"19\":6,\"46\":7}}],[\"vgg进行实现\",{\"1\":{\"54\":1}}],[\"vec2\",{\"1\":{\"54\":5,\"56\":5}}],[\"vec1\",{\"1\":{\"54\":5,\"56\":5}}],[\"vectors\",{\"1\":{\"46\":1}}],[\"version\",{\"1\":{\"13\":1,\"23\":1}}],[\"v\",{\"1\":{\"46\":2,\"61\":4,\"67\":4}}],[\"vs\",{\"1\":{\"30\":1}}],[\"virtex\",{\"1\":{\"57\":1}}],[\"visual\",{\"1\":{\"57\":2}}],[\"visiontransformer\",{\"1\":{\"64\":2,\"65\":2,\"68\":2,\"72\":1}}],[\"vision\",{\"0\":{\"4\":1},\"1\":{\"49\":1,\"51\":1,\"54\":1,\"59\":3,\"66\":1,\"72\":2,\"73\":1}}],[\"vit核心\",{\"1\":{\"73\":1}}],[\"vitjx\",{\"1\":{\"72\":1}}],[\"vit这篇论文长达二十多页\",{\"1\":{\"70\":1}}],[\"vit才会慢慢超越resnet\",{\"1\":{\"69\":1}}],[\"vit的效果表现不如resnet\",{\"1\":{\"69\":1}}],[\"vit的表现通常比同等大小的resnets要差一些\",{\"1\":{\"59\":1}}],[\"vit的表现就会超过cnn\",{\"1\":{\"59\":1}}],[\"vit仍是采用transformer中用到layer\",{\"1\":{\"66\":1}}],[\"vit虽然采用的是transformer\",{\"1\":{\"66\":1}}],[\"vit中的多头自注意力模块实现逻辑和transformer基本一致\",{\"1\":{\"67\":1}}],[\"vit中\",{\"1\":{\"65\":1}}],[\"vit原论文中最核心的结论是\",{\"1\":{\"59\":1}}],[\"vit\",{\"1\":{\"49\":2,\"51\":2,\"54\":4,\"56\":1,\"62\":3,\"68\":1,\"72\":6}}],[\"view\",{\"1\":{\"17\":4,\"19\":3,\"26\":4,\"31\":5,\"32\":6,\"46\":3}}],[\"vocab是词典大小\",{\"1\":{\"38\":1}}],[\"vocab\",{\"1\":{\"11\":2,\"24\":2,\"26\":1,\"38\":2}}],[\"vl\",{\"1\":{\"2\":1}}],[\"现就读于电子科技大学\",{\"1\":{\"3\":1}}],[\"现就读于四川大学\",{\"1\":{\"2\":1}}],[\"领域常用的文本transformer模型\",{\"1\":{\"51\":1}}],[\"领域中的一些对比学习方法\",{\"1\":{\"50\":1}}],[\"领域\",{\"1\":{\"3\":1}}],[\"转换为元组形式\",{\"1\":{\"63\":1}}],[\"转换为一系列高维向量表示\",{\"1\":{\"36\":1}}],[\"转化成功之后\",{\"1\":{\"7\":1}}],[\"转\",{\"1\":{\"3\":1}}],[\"转型\",{\"1\":{\"2\":1}}],[\"c为输入token的总维度\",{\"1\":{\"67\":1}}],[\"c=in\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"c=3\",{\"1\":{\"63\":1,\"64\":1,\"65\":1,\"68\":1}}],[\"centercrop\",{\"1\":{\"62\":1}}],[\"center\",{\"1\":{\"61\":2}}],[\"current\",{\"1\":{\"54\":2,\"55\":1,\"56\":3}}],[\"cuda\",{\"1\":{\"52\":2,\"54\":2,\"56\":2}}],[\"cup\",{\"1\":{\"52\":1}}],[\"cbow\",{\"1\":{\"51\":1}}],[\"cnn具有两种归纳偏置\",{\"1\":{\"59\":1}}],[\"cnn\",{\"1\":{\"51\":1,\"59\":1,\"71\":9,\"73\":1}}],[\"cpu\",{\"1\":{\"35\":1,\"52\":3,\"54\":3,\"56\":3}}],[\"c\",{\"1\":{\"32\":1,\"51\":1,\"63\":6,\"64\":3,\"65\":2,\"67\":3,\"68\":2}}],[\"cannot\",{\"1\":{\"67\":1}}],[\"candidates\",{\"1\":{\"54\":13,\"56\":10}}],[\"casual\",{\"1\":{\"67\":1}}],[\"case\",{\"1\":{\"7\":1}}],[\"caption\",{\"1\":{\"57\":1}}],[\"capital\",{\"1\":{\"30\":2}}],[\"camera\",{\"1\":{\"52\":1}}],[\"category\",{\"1\":{\"54\":4,\"56\":4}}],[\"cat\",{\"1\":{\"52\":1,\"64\":1,\"65\":1,\"68\":1}}],[\"cross\",{\"1\":{\"51\":2}}],[\"crossentropyloss\",{\"1\":{\"17\":1,\"26\":1,\"29\":1,\"31\":1,\"32\":1}}],[\"creates\",{\"1\":{\"8\":1}}],[\"create\",{\"1\":{\"7\":1,\"8\":1,\"34\":1}}],[\"clipprocessor\",{\"1\":{\"54\":1,\"56\":2}}],[\"clipmodel\",{\"1\":{\"54\":1,\"56\":2}}],[\"clip模型均能够以较高的置信度给出正确的分类结果\",{\"1\":{\"52\":1}}],[\"clip模型能够在没有特定任务训练数据的情况下\",{\"1\":{\"52\":1}}],[\"clip模型的一个显著优势是它能够进行zero\",{\"1\":{\"52\":1}}],[\"clip模型会预测出个可能的文本\",{\"1\":{\"51\":1}}],[\"clip包含两个核心模型\",{\"1\":{\"51\":1}}],[\"clip的训练数据采用的是文本\",{\"1\":{\"50\":1}}],[\"clip的英文全称为contrastive\",{\"1\":{\"50\":1}}],[\"clip属于基于对比学习的多模态模型\",{\"1\":{\"50\":1}}],[\"clip\",{\"1\":{\"49\":3,\"52\":1,\"54\":3,\"56\":1,\"57\":1}}],[\"clip原始论文链接\",{\"1\":{\"48\":1}}],[\"cla\",{\"1\":{\"61\":9}}],[\"clamp\",{\"1\":{\"29\":4}}],[\"classes=5\",{\"1\":{\"72\":1}}],[\"classes=num\",{\"1\":{\"72\":1}}],[\"classes=1000\",{\"1\":{\"64\":1,\"65\":1,\"68\":1}}],[\"classes\",{\"1\":{\"64\":3,\"65\":2,\"68\":6,\"72\":2}}],[\"class=val\",{\"1\":{\"62\":1}}],[\"class=train\",{\"1\":{\"62\":1}}],[\"class\",{\"0\":{\"64\":1},\"1\":{\"11\":1,\"13\":3,\"14\":1,\"15\":1,\"16\":1,\"17\":1,\"19\":1,\"20\":1,\"21\":1,\"23\":1,\"24\":1,\"25\":1,\"26\":1,\"29\":1,\"31\":1,\"32\":1,\"37\":1,\"38\":1,\"40\":1,\"41\":1,\"42\":1,\"44\":1,\"45\":1,\"46\":1,\"61\":27,\"63\":1,\"64\":3,\"65\":1,\"66\":2,\"67\":1,\"68\":1}}],[\"classification\",{\"1\":{\"8\":1}}],[\"classifier\",{\"1\":{\"7\":2,\"17\":2,\"31\":2,\"32\":2}}],[\"cls\",{\"1\":{\"8\":9,\"15\":1,\"26\":3,\"28\":2,\"30\":2,\"32\":1,\"64\":22,\"65\":5,\"68\":5}}],[\"clones\",{\"1\":{\"41\":1,\"42\":1,\"44\":1,\"45\":1,\"46\":1}}],[\"clone\",{\"1\":{\"7\":1,\"34\":1}}],[\"ckpt\",{\"1\":{\"7\":1}}],[\"channels\",{\"1\":{\"61\":1}}],[\"choices\",{\"1\":{\"32\":9}}],[\"choice\",{\"1\":{\"32\":1}}],[\"checkpoint\",{\"1\":{\"7\":2}}],[\"chinesegluedatasets\",{\"1\":{\"7\":2}}],[\"chinese\",{\"1\":{\"7\":5}}],[\"cd\",{\"1\":{\"7\":1,\"34\":1}}],[\"count\",{\"1\":{\"54\":7,\"56\":7}}],[\"coffee\",{\"1\":{\"52\":1}}],[\"cosine\",{\"1\":{\"51\":1,\"54\":2,\"55\":1,\"56\":3}}],[\"correct\",{\"1\":{\"54\":3,\"56\":3}}],[\"corresponding\",{\"1\":{\"15\":1}}],[\"core\",{\"1\":{\"42\":1}}],[\"collate可以参考\",{\"1\":{\"61\":1}}],[\"collate\",{\"1\":{\"10\":2,\"61\":3,\"62\":4}}],[\"conv2d\",{\"1\":{\"63\":1}}],[\"convirt基于对比学习的方法\",{\"1\":{\"57\":1}}],[\"convert\",{\"1\":{\"7\":1,\"54\":1,\"56\":1}}],[\"concat\",{\"1\":{\"46\":1}}],[\"connections\",{\"1\":{\"41\":1,\"44\":1}}],[\"connection\",{\"1\":{\"36\":1}}],[\"contrastive\",{\"1\":{\"57\":1}}],[\"contiguous\",{\"1\":{\"19\":1,\"46\":1}}],[\"contextualized\",{\"1\":{\"30\":1}}],[\"contextual\",{\"1\":{\"30\":1}}],[\"context\",{\"1\":{\"15\":1,\"19\":9,\"28\":1}}],[\"config\",{\"1\":{\"7\":2,\"11\":9,\"13\":16,\"14\":3,\"15\":3,\"16\":5,\"17\":7,\"19\":9,\"20\":5,\"21\":3,\"23\":8,\"24\":5,\"25\":3,\"26\":5,\"28\":1,\"29\":6,\"31\":7,\"32\":5}}],[\"conda\",{\"1\":{\"7\":2,\"34\":2}}],[\"code\",{\"1\":{\"7\":7}}],[\"compose\",{\"1\":{\"62\":2}}],[\"compute\",{\"1\":{\"46\":1}}],[\"compatibility\",{\"1\":{\"16\":1}}],[\"com\",{\"1\":{\"7\":2,\"34\":1,\"61\":2,\"72\":3}}],[\"committer\",{\"1\":{\"2\":2}}],[\"cv\",{\"1\":{\"3\":1,\"50\":1}}],[\"csdn\",{\"1\":{\"0\":1}}],[\"知识星球\",{\"1\":{\"0\":1}}]],\"version\":2}}")).map(([e,t])=>[e,_t(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:n,options:o,id:s}})=>{const r=xt[n];e==="suggest"?self.postMessage([e,s,ve(t,r,o)]):e==="search"?self.postMessage([e,s,Ee(t,r,o,"max")]):self.postMessage({suggestions:[e,s,ve(t,r,o)],results:[e,s,Ee(t,r,o,__SLIMSEARCH_SORT_STRATEGY__)]})};
//# sourceMappingURL=index.js.map
