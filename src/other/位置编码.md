---
title: 位置编码
icon: file
category:
  - 位置编码
tag:
  - 已发布
footer: 技术共建，知识共享
date: 2025-08-13
author:
  - BinaryOracle
---

`位置编码` 

<!-- more -->

## 绝对位置编码（Absolute Positional Encoding, APE）

### 正弦/余弦位置编码（Sinusoidal Positional Encoding）

正弦余弦位置编码（Sinusoidal Positional Encoding）是一种无需训练的位置编码方法，它通过固定的周期性函数（正弦和余弦）来为序列的不同位置提供唯一的编码。对于每个位置 $𝑖$ 和每个维度 $𝑑$，位置编码通过以下公式计算：

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

* $𝑝𝑜𝑠$ 表示位置索引，表示计算哪个位置的编码

* $𝑖$ 表示编码维度，$𝑑𝑚𝑜𝑑𝑒𝑙$ 是编码空间的总维度

* $PE_{(pos, 2i)}$ 和 $PE_{(pos, 2i+1)}$ 通过正弦和余弦函数分裂映射到偶数和奇数的维度

假设：我们要计算输入序列第 2 个位置 Token 对应的位置编码，编码的维度设定为 4 ，则：

![](位置编码/1.png)

最终，位置 2 的编码向量为：$(𝑠𝑖𝑛(2),𝑐𝑜𝑠(2),𝑠𝑖𝑛(0.02),𝑐𝑜𝑠(0.02))$，我们把它加到第二个 Token 的词嵌入向量上，就相当于给其注入了顺序信息。

计算起来是比较容易的，如何去理解这个位置编码？请看下面这张图：

![](位置编码/2.png)

这三张图分别打印了 128 个位置向量第 2、6、12 维度的编码值的变化，我们发现这些值呈现周期性的变化。另外，我们也可以发现，向量维度越高，其周期就越长。

![](位置编码/3.png)

上图，我们打印了位置 1、5 的编码向量中的 sin 和 cos 计算得到的编码值。我们可以发现，基于正弦和余弦函数得到的位置编码可以保证唯一性。另外，也可以看到，向量的维度越高，编码值的波动就越小，向量就越接近。

简单总结下：

1. 因为正弦和余弦函数都是周期函数，编码在不同维度上具有不同的周期性

2. 位置编码向量是唯一的，因为不同位置的编码由不同的正弦和余弦值组成

3. 低维度的编码值波动性很大（周期短），高纬度的编码值波动性较小（周期长）

所以，可以得到一个简单的结论：

1. 低维分量（小 i）的变化较快，主要捕捉局部位置关系

2. 高维分量（大 i）的变化较慢，可以用于编码全局信息

这个怎么去理解？我们把某个位置的向量大概划分为两部分：低维向量部分 + 高维向量部分，低纬向量部分数值波动幅度很大，在一个周期内只能包含少量相邻的位置，并且一定程度上也表达了位置的局部的相对信息，这就是捕捉局部位置关系。那么，对于高纬向量部分而言，它的波动幅度很小，一个周期能够包含更多的位置信息，这也是我们理解的编码全局位置信息的含义。

所以，对于一个基于正弦余弦编码的位置向量，可以理解为该向量中隐含了一些局部和全局的位置信息。使得 Transformer 既能感知局部相对位置，也能感知全局位置信息，从而弥补其原生结构中缺少位置感知能力的缺陷。

当然，这种位置编码方法也存在以下一些不足之处：

随着序列长度的增加，位置编码的周期性可能导致不同位置之间的区分度逐渐降低，难以准确表示极长序列中各个位置的独特信息。

虽然正弦余弦位置编码能够隐含地表达一定的局部位置信息，但由于它是固定的、不可学习的，并没有专门针对局部依赖关系进行优化，因此在建模局部依赖关系时能力相对不足。

正弦余弦位置编码是一种基于三角函数的固定编码方式，它是一种静态的位置信息表示。而注意力机制更关注的是文本中不同位置之间的动态语义关联。这两种信息在表示形式和语义侧重点上存在差异，导致在融合时可能无法很好地相互补充。

正弦余弦位置编码通常是高维向量，其计算量会随着维度的增加不仅需要更多的计算时间，还可能占用大量的内存空间，影响模型的运行效率。

> 正余弦位置编码通过在不同维度上引入不同波长的正余弦信号，使得低维对局部位置变化敏感，高维对全局位置变化敏感。虽然位置和语义在所有维度上混合，但在训练中，模型可能学到一种“低维更多位置，高维更多语义”的分工模式。

### 基于可学习的嵌入

可学习的位置编码（Learnable Positional Encoding, LPE）是一种通过梯度下降自动学习位置编码的方法，不同于固定编码（如正弦/余弦函数编码），它不依赖任何手工设计的公式，而是直接让模型在训练过程中优化位置信息。对于一个长度为 𝐿 的输入序列，每个位置 𝑖 都对应一个可学习的向量。当训练或测试时，将输入 Token 的编码和对应位置的可学习位置编码向量相加，从而赋予 Token 相应的位置信息。

这种位置编码方式能够根据具体的任务和数据特点，模型可以学习到更适合该类文本的位置表示方式，捕捉文本中位置相关的语义和结构信息，这是固定的位置编码（例如：正弦余弦位置编码）难以做到的。但是也存在一些不足之处，例如：

1. 如果训练时 max_len=512，测试时输入 1024 长度的序列，模型就无法处理了

2. 需要存储 max_len×d_model 维度的参数，可能导致大模型训练更难收敛

## 相对位置编码（Relative Position Encoding, RPE）

相对位置编码直接对 两个 token 之间的距离 建模。例如：

1. token i 关注 token j 时，注意力分数不仅取决于它们的内容，还取决于 i - j 的相对位置。

2. 如果两个位置的相对距离相同，那么它们的相对位置信息也是相同的（模型可以更好泛化到更长的序列）。

### Relative Position Representations (Shaw et al., 2018)

普通自注意力打分公式：

$$
\text{score}(i,j) = \frac{Q_i \cdot K_j}{\sqrt{d_k}}
$$

其中：

* $Q_i = x_i W_Q$

* $K_j = x_j W_K$

Shaw 相对位置编码改成：

$$
\text{score}(i,j) = \frac{Q_i \cdot K_j + Q_i \cdot R_{i-j}}{\sqrt{d_k}}
$$

这里：

* $R_{i-j}$ 是和**相对距离** $i-j$ 对应的向量（可训练）。

* 只要知道 $i-j$，就能从一个 embedding table 里查到对应的 $R$ 向量。

下面先给出完整代码实现，然后再进行详细解析:

```python
import torch
import torch.nn as nn
import math

class RelPosAttention(nn.Module):
    def __init__(self, d_model, n_heads, max_len):
        super().__init__()
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.rel_emb = nn.Embedding(2 * max_len - 1, self.d_k)  # 相对位置向量表
        self.max_len = max_len
        
    def forward(self, x):
        B, L, _ = x.size()
        assert L <= self.max_len, "输入序列长度超过max_len"
        
        Q = self.W_q(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2)  # (B, h, L, d_k)
        K = self.W_k(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2)
        
        # ===== 1. 普通注意力部分 =====
        content_score = torch.matmul(Q, K.transpose(-2, -1))  # (B, h, L, L)
        
        # ===== 2. 相对位置部分 =====
        # 相对位置索引矩阵
        rel_pos = torch.arange(L, device=x.device).unsqueeze(1) - torch.arange(L, device=x.device).unsqueeze(0)
        rel_pos += self.max_len - 1  # shift到[0, 2L-2]
        R = self.rel_emb(rel_pos)    # (L, L, d_k)
        
        # 使用爱因斯坦求和公式计算 Q_i ⋅ R_{i-j}
        pos_score = torch.einsum('bhld,lmd->bhlm', Q, R)
        
        # ===== 3. 合并并归一化 =====
        scores = (content_score + pos_score) / math.sqrt(self.d_k)
        attn = torch.softmax(scores, dim=-1)
        
        out = torch.matmul(attn, V)  # (B, h, L, d_k)
        out = out.transpose(1, 2).contiguous().view(B, L, -1)
        return self.W_o(out)
```

首先来看一下 Shaw 相对位置编码中的相对位置矩阵的详解:

1. 生成相对位置差矩阵

```python
rel_pos = torch.arange(L, device=x.device).unsqueeze(1) - torch.arange(L, device=x.device).unsqueeze(0)
```

假设序列长度 `L = 4`，`torch.arange(L)` 是：

```
[0, 1, 2, 3]
```

* `unsqueeze(1)` 变成列向量 shape `(4,1)`

* `unsqueeze(0)` 变成行向量 shape `(1,4)`

做减法（广播规则）：

```
[[ 0-0, 0-1, 0-2, 0-3],
 [ 1-0, 1-1, 1-2, 1-3],
 [ 2-0, 2-1, 2-2, 2-3],
 [ 3-0, 3-1, 3-2, 3-3]]
```

结果是：

```
[[ 0, -1, -2, -3],
 [ 1,  0, -1, -2],
 [ 2,  1,  0, -1],
 [ 3,  2,  1,  0]]
```

**含义**：第 i 行第 j 列的值就是 `i - j`，即 **token i 与 token j 的相对距离**。

---

2. 平移到正索引区间

```python
rel_pos += self.max_len - 1
```

Embedding 的索引必须是 **非负整数**，所以要把负值平移到正数区间。如果 `max_len=4`，`self.max_len - 1 = 3`，加 3 后：

```
[[3, 2, 1, 0],
 [4, 3, 2, 1],
 [5, 4, 3, 2],
 [6, 5, 4, 3]]
```

值域范围是 `[0, 2*max_len-2]`。这正好对应 `self.rel_emb` 的 embedding 表大小 `(2*max_len - 1, d_k)`。

---

3. 查表得到相对位置向量

```python
R = self.rel_emb(rel_pos)  # shape: (L, L, d_k)
```

`self.rel_emb` 是一个可训练的 embedding 表，每个相对距离对应一个向量。`rel_pos` 的形状是 `(L, L)`，查表后：

* 第一维 = 查询位置 i

* 第二维 = 被关注位置 j

* 第三维 = 对应的相对位置编码向量 $R_{i-j}$，长度 `d_k`

所以 `R` 是一个 `(L, L, d_k)` 张量。

---

4. 用爱因斯坦求和公式计算位置分数

```python
pos_score = torch.einsum('bhld,lmd->bhlm', Q, R)
```
先看两个输入的形状:

* **Q**: `(B, h, L, d_k)`

  * `B` = batch 大小

  * `h` = 注意力头数
  
  * `L` = 序列长度（查询 token 的位置）
  
  * `d_k` = 每个头的向量维度（Q 向量长度）

* **R**: `(L, L, d_k)`

  * 第 1 维：查询位置 index $i$
  
  * 第 2 维：被关注位置 index $j$
  
  * 第 3 维：与相对位置 $i-j$ 对应的向量（长度 `d_k`）

爱因斯坦求和规则: `'bhld,lmd->bhlm'`

* **左边 Q 的维度**：`b h l d`

* **右边 R 的维度**：`l m d`

* 两者中**相同字母**代表要“配对”的轴：

  * `l`：查询位置 i → **保持不变**（参与配对但保留在输出里）

  * `d`：向量维度 → **相同字母且不出现在输出，表示要相乘后求和**（点积）

* **不同字母**：

  * `m`：来自 R 的“被关注位置”维度，出现在输出
  
  * `b`、`h`：来自 Q 的 batch 和头维度，直接保留

我们想算：

$$
\text{pos\_score}[b,h,l,m] = \sum_{d=1}^{d_k} Q[b,h,l,d] \times R[l,m,d]
$$

* 固定 batch `b`、head `h`、查询位置 `l`、被关注位置 `m`

* 从 Q 里取对应的查询向量 $Q_{b,h,l,:}$

* 从 R 里取对应的相对位置向量 $R_{l,m,:}$

* 对它们做**向量点积**（沿 `d` 维求和）

--- 

5. 为什么相对位置矩阵 $R$ 不直接加到注意力分数里，而是还要和 $Q$ 做点积？

在标准自注意力中，打分是：

$$
\text{score}(i,j) = Q_i \cdot K_j
$$

其中：

* $Q_i = x_i W_Q$：**查询 token i 的内容向量**

* $K_j = x_j W_K$：**被关注 token j 的内容向量**

这两个向量都在**同一个向量空间**中（维度 `d_k`），点积才能得到一个有意义的相似度分数。Shaw 的做法是：

$$
\text{score}(i,j) = Q_i \cdot K_j + Q_i \cdot R_{i-j}
$$

第二项的解释：

* $R_{i-j}$ 也是在 `d_k` 维的向量空间中。

* 用 $Q_i$ 和 $R_{i-j}$ 做点积，把**位置信息向量**投影到和 `Q_i` 一致的表示空间。

* 这样得到的结果是一个标量，可以直接和 `Q·K` 的结果相加。

换句话说：

* **K 部分**带来**内容相关性**

* **R 部分**带来**位置相关性**

* 二者都要在**同一个“Q 向量的视角”下衡量**，所以都用 $Q_i$ 做点积

> 可以把注意力打分看作“你(i)对别人(j)的关注程度”，它可能由两部分组成：
> 
> 1. **内容相似度**：你关心和你内容类似的人（Q·K）
> 
> 2. **位置偏好**：你关心离你近的人（Q·R）