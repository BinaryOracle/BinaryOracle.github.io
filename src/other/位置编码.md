---
title: ä½ç½®ç¼–ç 
icon: file
category:
  - ä½ç½®ç¼–ç 
tag:
  - å·²å‘å¸ƒ
footer: æŠ€æœ¯å…±å»ºï¼ŒçŸ¥è¯†å…±äº«
date: 2025-08-13
author:
  - BinaryOracle
---

`ä½ç½®ç¼–ç ` 

<!-- more -->

## ç»å¯¹ä½ç½®ç¼–ç ï¼ˆAbsolute Positional Encoding, APEï¼‰

### æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç ï¼ˆSinusoidal Positional Encodingï¼‰

æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç ï¼ˆSinusoidal Positional Encodingï¼‰æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„ä½ç½®ç¼–ç æ–¹æ³•ï¼Œå®ƒé€šè¿‡å›ºå®šçš„å‘¨æœŸæ€§å‡½æ•°ï¼ˆæ­£å¼¦å’Œä½™å¼¦ï¼‰æ¥ä¸ºåºåˆ—çš„ä¸åŒä½ç½®æä¾›å”¯ä¸€çš„ç¼–ç ã€‚å¯¹äºæ¯ä¸ªä½ç½® $ğ‘–$ å’Œæ¯ä¸ªç»´åº¦ $ğ‘‘$ï¼Œä½ç½®ç¼–ç é€šè¿‡ä»¥ä¸‹å…¬å¼è®¡ç®—ï¼š

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

* $ğ‘ğ‘œğ‘ $ è¡¨ç¤ºä½ç½®ç´¢å¼•ï¼Œè¡¨ç¤ºè®¡ç®—å“ªä¸ªä½ç½®çš„ç¼–ç 

* $ğ‘–$ è¡¨ç¤ºç¼–ç ç»´åº¦ï¼Œ$ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™$ æ˜¯ç¼–ç ç©ºé—´çš„æ€»ç»´åº¦

* $PE_{(pos, 2i)}$ å’Œ $PE_{(pos, 2i+1)}$ é€šè¿‡æ­£å¼¦å’Œä½™å¼¦å‡½æ•°åˆ†è£‚æ˜ å°„åˆ°å¶æ•°å’Œå¥‡æ•°çš„ç»´åº¦

å‡è®¾ï¼šæˆ‘ä»¬è¦è®¡ç®—è¾“å…¥åºåˆ—ç¬¬ 2 ä¸ªä½ç½® Token å¯¹åº”çš„ä½ç½®ç¼–ç ï¼Œç¼–ç çš„ç»´åº¦è®¾å®šä¸º 4 ï¼Œåˆ™ï¼š

![](ä½ç½®ç¼–ç /1.png)

æœ€ç»ˆï¼Œä½ç½® 2 çš„ç¼–ç å‘é‡ä¸ºï¼š$(ğ‘ ğ‘–ğ‘›(2),ğ‘ğ‘œğ‘ (2),ğ‘ ğ‘–ğ‘›(0.02),ğ‘ğ‘œğ‘ (0.02))$ï¼Œæˆ‘ä»¬æŠŠå®ƒåŠ åˆ°ç¬¬äºŒä¸ª Token çš„è¯åµŒå…¥å‘é‡ä¸Šï¼Œå°±ç›¸å½“äºç»™å…¶æ³¨å…¥äº†é¡ºåºä¿¡æ¯ã€‚

è®¡ç®—èµ·æ¥æ˜¯æ¯”è¾ƒå®¹æ˜“çš„ï¼Œå¦‚ä½•å»ç†è§£è¿™ä¸ªä½ç½®ç¼–ç ï¼Ÿè¯·çœ‹ä¸‹é¢è¿™å¼ å›¾ï¼š

![](ä½ç½®ç¼–ç /2.png)

è¿™ä¸‰å¼ å›¾åˆ†åˆ«æ‰“å°äº† 128 ä¸ªä½ç½®å‘é‡ç¬¬ 2ã€6ã€12 ç»´åº¦çš„ç¼–ç å€¼çš„å˜åŒ–ï¼Œæˆ‘ä»¬å‘ç°è¿™äº›å€¼å‘ˆç°å‘¨æœŸæ€§çš„å˜åŒ–ã€‚å¦å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å‘ç°ï¼Œå‘é‡ç»´åº¦è¶Šé«˜ï¼Œå…¶å‘¨æœŸå°±è¶Šé•¿ã€‚

![](ä½ç½®ç¼–ç /3.png)

ä¸Šå›¾ï¼Œæˆ‘ä»¬æ‰“å°äº†ä½ç½® 1ã€5 çš„ç¼–ç å‘é‡ä¸­çš„ sin å’Œ cos è®¡ç®—å¾—åˆ°çš„ç¼–ç å€¼ã€‚æˆ‘ä»¬å¯ä»¥å‘ç°ï¼ŒåŸºäºæ­£å¼¦å’Œä½™å¼¦å‡½æ•°å¾—åˆ°çš„ä½ç½®ç¼–ç å¯ä»¥ä¿è¯å”¯ä¸€æ€§ã€‚å¦å¤–ï¼Œä¹Ÿå¯ä»¥çœ‹åˆ°ï¼Œå‘é‡çš„ç»´åº¦è¶Šé«˜ï¼Œç¼–ç å€¼çš„æ³¢åŠ¨å°±è¶Šå°ï¼Œå‘é‡å°±è¶Šæ¥è¿‘ã€‚

ç®€å•æ€»ç»“ä¸‹ï¼š

1. å› ä¸ºæ­£å¼¦å’Œä½™å¼¦å‡½æ•°éƒ½æ˜¯å‘¨æœŸå‡½æ•°ï¼Œç¼–ç åœ¨ä¸åŒç»´åº¦ä¸Šå…·æœ‰ä¸åŒçš„å‘¨æœŸæ€§

2. ä½ç½®ç¼–ç å‘é‡æ˜¯å”¯ä¸€çš„ï¼Œå› ä¸ºä¸åŒä½ç½®çš„ç¼–ç ç”±ä¸åŒçš„æ­£å¼¦å’Œä½™å¼¦å€¼ç»„æˆ

3. ä½ç»´åº¦çš„ç¼–ç å€¼æ³¢åŠ¨æ€§å¾ˆå¤§ï¼ˆå‘¨æœŸçŸ­ï¼‰ï¼Œé«˜çº¬åº¦çš„ç¼–ç å€¼æ³¢åŠ¨æ€§è¾ƒå°ï¼ˆå‘¨æœŸé•¿ï¼‰

æ‰€ä»¥ï¼Œå¯ä»¥å¾—åˆ°ä¸€ä¸ªç®€å•çš„ç»“è®ºï¼š

1. ä½ç»´åˆ†é‡ï¼ˆå° iï¼‰çš„å˜åŒ–è¾ƒå¿«ï¼Œä¸»è¦æ•æ‰å±€éƒ¨ä½ç½®å…³ç³»

2. é«˜ç»´åˆ†é‡ï¼ˆå¤§ iï¼‰çš„å˜åŒ–è¾ƒæ…¢ï¼Œå¯ä»¥ç”¨äºç¼–ç å…¨å±€ä¿¡æ¯

è¿™ä¸ªæ€ä¹ˆå»ç†è§£ï¼Ÿæˆ‘ä»¬æŠŠæŸä¸ªä½ç½®çš„å‘é‡å¤§æ¦‚åˆ’åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šä½ç»´å‘é‡éƒ¨åˆ† + é«˜ç»´å‘é‡éƒ¨åˆ†ï¼Œä½çº¬å‘é‡éƒ¨åˆ†æ•°å€¼æ³¢åŠ¨å¹…åº¦å¾ˆå¤§ï¼Œåœ¨ä¸€ä¸ªå‘¨æœŸå†…åªèƒ½åŒ…å«å°‘é‡ç›¸é‚»çš„ä½ç½®ï¼Œå¹¶ä¸”ä¸€å®šç¨‹åº¦ä¸Šä¹Ÿè¡¨è¾¾äº†ä½ç½®çš„å±€éƒ¨çš„ç›¸å¯¹ä¿¡æ¯ï¼Œè¿™å°±æ˜¯æ•æ‰å±€éƒ¨ä½ç½®å…³ç³»ã€‚é‚£ä¹ˆï¼Œå¯¹äºé«˜çº¬å‘é‡éƒ¨åˆ†è€Œè¨€ï¼Œå®ƒçš„æ³¢åŠ¨å¹…åº¦å¾ˆå°ï¼Œä¸€ä¸ªå‘¨æœŸèƒ½å¤ŸåŒ…å«æ›´å¤šçš„ä½ç½®ä¿¡æ¯ï¼Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬ç†è§£çš„ç¼–ç å…¨å±€ä½ç½®ä¿¡æ¯çš„å«ä¹‰ã€‚

æ‰€ä»¥ï¼Œå¯¹äºä¸€ä¸ªåŸºäºæ­£å¼¦ä½™å¼¦ç¼–ç çš„ä½ç½®å‘é‡ï¼Œå¯ä»¥ç†è§£ä¸ºè¯¥å‘é‡ä¸­éšå«äº†ä¸€äº›å±€éƒ¨å’Œå…¨å±€çš„ä½ç½®ä¿¡æ¯ã€‚ä½¿å¾— Transformer æ—¢èƒ½æ„ŸçŸ¥å±€éƒ¨ç›¸å¯¹ä½ç½®ï¼Œä¹Ÿèƒ½æ„ŸçŸ¥å…¨å±€ä½ç½®ä¿¡æ¯ï¼Œä»è€Œå¼¥è¡¥å…¶åŸç”Ÿç»“æ„ä¸­ç¼ºå°‘ä½ç½®æ„ŸçŸ¥èƒ½åŠ›çš„ç¼ºé™·ã€‚

å½“ç„¶ï¼Œè¿™ç§ä½ç½®ç¼–ç æ–¹æ³•ä¹Ÿå­˜åœ¨ä»¥ä¸‹ä¸€äº›ä¸è¶³ä¹‹å¤„ï¼š

éšç€åºåˆ—é•¿åº¦çš„å¢åŠ ï¼Œä½ç½®ç¼–ç çš„å‘¨æœŸæ€§å¯èƒ½å¯¼è‡´ä¸åŒä½ç½®ä¹‹é—´çš„åŒºåˆ†åº¦é€æ¸é™ä½ï¼Œéš¾ä»¥å‡†ç¡®è¡¨ç¤ºæé•¿åºåˆ—ä¸­å„ä¸ªä½ç½®çš„ç‹¬ç‰¹ä¿¡æ¯ã€‚

è™½ç„¶æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç èƒ½å¤Ÿéšå«åœ°è¡¨è¾¾ä¸€å®šçš„å±€éƒ¨ä½ç½®ä¿¡æ¯ï¼Œä½†ç”±äºå®ƒæ˜¯å›ºå®šçš„ã€ä¸å¯å­¦ä¹ çš„ï¼Œå¹¶æ²¡æœ‰ä¸“é—¨é’ˆå¯¹å±€éƒ¨ä¾èµ–å…³ç³»è¿›è¡Œä¼˜åŒ–ï¼Œå› æ­¤åœ¨å»ºæ¨¡å±€éƒ¨ä¾èµ–å…³ç³»æ—¶èƒ½åŠ›ç›¸å¯¹ä¸è¶³ã€‚

æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç æ˜¯ä¸€ç§åŸºäºä¸‰è§’å‡½æ•°çš„å›ºå®šç¼–ç æ–¹å¼ï¼Œå®ƒæ˜¯ä¸€ç§é™æ€çš„ä½ç½®ä¿¡æ¯è¡¨ç¤ºã€‚è€Œæ³¨æ„åŠ›æœºåˆ¶æ›´å…³æ³¨çš„æ˜¯æ–‡æœ¬ä¸­ä¸åŒä½ç½®ä¹‹é—´çš„åŠ¨æ€è¯­ä¹‰å…³è”ã€‚è¿™ä¸¤ç§ä¿¡æ¯åœ¨è¡¨ç¤ºå½¢å¼å’Œè¯­ä¹‰ä¾§é‡ç‚¹ä¸Šå­˜åœ¨å·®å¼‚ï¼Œå¯¼è‡´åœ¨èåˆæ—¶å¯èƒ½æ— æ³•å¾ˆå¥½åœ°ç›¸äº’è¡¥å……ã€‚

æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç é€šå¸¸æ˜¯é«˜ç»´å‘é‡ï¼Œå…¶è®¡ç®—é‡ä¼šéšç€ç»´åº¦çš„å¢åŠ ä¸ä»…éœ€è¦æ›´å¤šçš„è®¡ç®—æ—¶é—´ï¼Œè¿˜å¯èƒ½å ç”¨å¤§é‡çš„å†…å­˜ç©ºé—´ï¼Œå½±å“æ¨¡å‹çš„è¿è¡Œæ•ˆç‡ã€‚

> æ­£ä½™å¼¦ä½ç½®ç¼–ç é€šè¿‡åœ¨ä¸åŒç»´åº¦ä¸Šå¼•å…¥ä¸åŒæ³¢é•¿çš„æ­£ä½™å¼¦ä¿¡å·ï¼Œä½¿å¾—ä½ç»´å¯¹å±€éƒ¨ä½ç½®å˜åŒ–æ•æ„Ÿï¼Œé«˜ç»´å¯¹å…¨å±€ä½ç½®å˜åŒ–æ•æ„Ÿã€‚è™½ç„¶ä½ç½®å’Œè¯­ä¹‰åœ¨æ‰€æœ‰ç»´åº¦ä¸Šæ··åˆï¼Œä½†åœ¨è®­ç»ƒä¸­ï¼Œæ¨¡å‹å¯èƒ½å­¦åˆ°ä¸€ç§â€œä½ç»´æ›´å¤šä½ç½®ï¼Œé«˜ç»´æ›´å¤šè¯­ä¹‰â€çš„åˆ†å·¥æ¨¡å¼ã€‚

### åŸºäºå¯å­¦ä¹ çš„åµŒå…¥

å¯å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼ˆLearnable Positional Encoding, LPEï¼‰æ˜¯ä¸€ç§é€šè¿‡æ¢¯åº¦ä¸‹é™è‡ªåŠ¨å­¦ä¹ ä½ç½®ç¼–ç çš„æ–¹æ³•ï¼Œä¸åŒäºå›ºå®šç¼–ç ï¼ˆå¦‚æ­£å¼¦/ä½™å¼¦å‡½æ•°ç¼–ç ï¼‰ï¼Œå®ƒä¸ä¾èµ–ä»»ä½•æ‰‹å·¥è®¾è®¡çš„å…¬å¼ï¼Œè€Œæ˜¯ç›´æ¥è®©æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜åŒ–ä½ç½®ä¿¡æ¯ã€‚å¯¹äºä¸€ä¸ªé•¿åº¦ä¸º ğ¿ çš„è¾“å…¥åºåˆ—ï¼Œæ¯ä¸ªä½ç½® ğ‘– éƒ½å¯¹åº”ä¸€ä¸ªå¯å­¦ä¹ çš„å‘é‡ã€‚å½“è®­ç»ƒæˆ–æµ‹è¯•æ—¶ï¼Œå°†è¾“å…¥ Token çš„ç¼–ç å’Œå¯¹åº”ä½ç½®çš„å¯å­¦ä¹ ä½ç½®ç¼–ç å‘é‡ç›¸åŠ ï¼Œä»è€Œèµ‹äºˆ Token ç›¸åº”çš„ä½ç½®ä¿¡æ¯ã€‚

è¿™ç§ä½ç½®ç¼–ç æ–¹å¼èƒ½å¤Ÿæ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®ç‰¹ç‚¹ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ›´é€‚åˆè¯¥ç±»æ–‡æœ¬çš„ä½ç½®è¡¨ç¤ºæ–¹å¼ï¼Œæ•æ‰æ–‡æœ¬ä¸­ä½ç½®ç›¸å…³çš„è¯­ä¹‰å’Œç»“æ„ä¿¡æ¯ï¼Œè¿™æ˜¯å›ºå®šçš„ä½ç½®ç¼–ç ï¼ˆä¾‹å¦‚ï¼šæ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç ï¼‰éš¾ä»¥åšåˆ°çš„ã€‚ä½†æ˜¯ä¹Ÿå­˜åœ¨ä¸€äº›ä¸è¶³ä¹‹å¤„ï¼Œä¾‹å¦‚ï¼š

1. å¦‚æœè®­ç»ƒæ—¶ max_len=512ï¼Œæµ‹è¯•æ—¶è¾“å…¥ 1024 é•¿åº¦çš„åºåˆ—ï¼Œæ¨¡å‹å°±æ— æ³•å¤„ç†äº†

2. éœ€è¦å­˜å‚¨ max_lenÃ—d_model ç»´åº¦çš„å‚æ•°ï¼Œå¯èƒ½å¯¼è‡´å¤§æ¨¡å‹è®­ç»ƒæ›´éš¾æ”¶æ•›

## ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆRelative Position Encoding, RPEï¼‰

ç›¸å¯¹ä½ç½®ç¼–ç ç›´æ¥å¯¹ ä¸¤ä¸ª token ä¹‹é—´çš„è·ç¦» å»ºæ¨¡ã€‚ä¾‹å¦‚ï¼š

1. token i å…³æ³¨ token j æ—¶ï¼Œæ³¨æ„åŠ›åˆ†æ•°ä¸ä»…å–å†³äºå®ƒä»¬çš„å†…å®¹ï¼Œè¿˜å–å†³äº i - j çš„ç›¸å¯¹ä½ç½®ã€‚

2. å¦‚æœä¸¤ä¸ªä½ç½®çš„ç›¸å¯¹è·ç¦»ç›¸åŒï¼Œé‚£ä¹ˆå®ƒä»¬çš„ç›¸å¯¹ä½ç½®ä¿¡æ¯ä¹Ÿæ˜¯ç›¸åŒçš„ï¼ˆæ¨¡å‹å¯ä»¥æ›´å¥½æ³›åŒ–åˆ°æ›´é•¿çš„åºåˆ—ï¼‰ã€‚

### Relative Position Representations (Shaw et al., 2018)

æ™®é€šè‡ªæ³¨æ„åŠ›æ‰“åˆ†å…¬å¼ï¼š

$$
\text{score}(i,j) = \frac{Q_i \cdot K_j}{\sqrt{d_k}}
$$

å…¶ä¸­ï¼š

* $Q_i = x_i W_Q$

* $K_j = x_j W_K$

Shaw ç›¸å¯¹ä½ç½®ç¼–ç æ”¹æˆï¼š

$$
\text{score}(i,j) = \frac{Q_i \cdot K_j + Q_i \cdot R_{i-j}}{\sqrt{d_k}}
$$

è¿™é‡Œï¼š

* $R_{i-j}$ æ˜¯å’Œ**ç›¸å¯¹è·ç¦»** $i-j$ å¯¹åº”çš„å‘é‡ï¼ˆå¯è®­ç»ƒï¼‰ã€‚

* åªè¦çŸ¥é“ $i-j$ï¼Œå°±èƒ½ä»ä¸€ä¸ª embedding table é‡ŒæŸ¥åˆ°å¯¹åº”çš„ $R$ å‘é‡ã€‚

ä¸‹é¢å…ˆç»™å‡ºå®Œæ•´ä»£ç å®ç°ï¼Œç„¶åå†è¿›è¡Œè¯¦ç»†è§£æ:

```python
import torch
import torch.nn as nn
import math

class RelPosAttention(nn.Module):
    def __init__(self, d_model, n_heads, max_len):
        super().__init__()
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.rel_emb = nn.Embedding(2 * max_len - 1, self.d_k)  # ç›¸å¯¹ä½ç½®å‘é‡è¡¨
        self.max_len = max_len
        
    def forward(self, x):
        B, L, _ = x.size()
        assert L <= self.max_len, "è¾“å…¥åºåˆ—é•¿åº¦è¶…è¿‡max_len"
        
        Q = self.W_q(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2)  # (B, h, L, d_k)
        K = self.W_k(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2)
        
        # ===== 1. æ™®é€šæ³¨æ„åŠ›éƒ¨åˆ† =====
        content_score = torch.matmul(Q, K.transpose(-2, -1))  # (B, h, L, L)
        
        # ===== 2. ç›¸å¯¹ä½ç½®éƒ¨åˆ† =====
        # ç›¸å¯¹ä½ç½®ç´¢å¼•çŸ©é˜µ
        rel_pos = torch.arange(L, device=x.device).unsqueeze(1) - torch.arange(L, device=x.device).unsqueeze(0)
        rel_pos += self.max_len - 1  # shiftåˆ°[0, 2L-2]
        R = self.rel_emb(rel_pos)    # (L, L, d_k)
        
        # ä½¿ç”¨çˆ±å› æ–¯å¦æ±‚å’Œå…¬å¼è®¡ç®— Q_i â‹… R_{i-j}
        pos_score = torch.einsum('bhld,lmd->bhlm', Q, R)
        
        # ===== 3. åˆå¹¶å¹¶å½’ä¸€åŒ– =====
        scores = (content_score + pos_score) / math.sqrt(self.d_k)
        attn = torch.softmax(scores, dim=-1)
        
        out = torch.matmul(attn, V)  # (B, h, L, d_k)
        out = out.transpose(1, 2).contiguous().view(B, L, -1)
        return self.W_o(out)
```

é¦–å…ˆæ¥çœ‹ä¸€ä¸‹ Shaw ç›¸å¯¹ä½ç½®ç¼–ç ä¸­çš„ç›¸å¯¹ä½ç½®çŸ©é˜µçš„è¯¦è§£:

1. ç”Ÿæˆç›¸å¯¹ä½ç½®å·®çŸ©é˜µ

```python
rel_pos = torch.arange(L, device=x.device).unsqueeze(1) - torch.arange(L, device=x.device).unsqueeze(0)
```

å‡è®¾åºåˆ—é•¿åº¦ `L = 4`ï¼Œ`torch.arange(L)` æ˜¯ï¼š

```
[0, 1, 2, 3]
```

* `unsqueeze(1)` å˜æˆåˆ—å‘é‡ shape `(4,1)`

* `unsqueeze(0)` å˜æˆè¡Œå‘é‡ shape `(1,4)`

åšå‡æ³•ï¼ˆå¹¿æ’­è§„åˆ™ï¼‰ï¼š

```
[[ 0-0, 0-1, 0-2, 0-3],
 [ 1-0, 1-1, 1-2, 1-3],
 [ 2-0, 2-1, 2-2, 2-3],
 [ 3-0, 3-1, 3-2, 3-3]]
```

ç»“æœæ˜¯ï¼š

```
[[ 0, -1, -2, -3],
 [ 1,  0, -1, -2],
 [ 2,  1,  0, -1],
 [ 3,  2,  1,  0]]
```

**å«ä¹‰**ï¼šç¬¬ i è¡Œç¬¬ j åˆ—çš„å€¼å°±æ˜¯ `i - j`ï¼Œå³ **token i ä¸ token j çš„ç›¸å¯¹è·ç¦»**ã€‚

---

2. å¹³ç§»åˆ°æ­£ç´¢å¼•åŒºé—´

```python
rel_pos += self.max_len - 1
```

Embedding çš„ç´¢å¼•å¿…é¡»æ˜¯ **éè´Ÿæ•´æ•°**ï¼Œæ‰€ä»¥è¦æŠŠè´Ÿå€¼å¹³ç§»åˆ°æ­£æ•°åŒºé—´ã€‚å¦‚æœ `max_len=4`ï¼Œ`self.max_len - 1 = 3`ï¼ŒåŠ  3 åï¼š

```
[[3, 2, 1, 0],
 [4, 3, 2, 1],
 [5, 4, 3, 2],
 [6, 5, 4, 3]]
```

å€¼åŸŸèŒƒå›´æ˜¯ `[0, 2*max_len-2]`ã€‚è¿™æ­£å¥½å¯¹åº” `self.rel_emb` çš„ embedding è¡¨å¤§å° `(2*max_len - 1, d_k)`ã€‚

---

3. æŸ¥è¡¨å¾—åˆ°ç›¸å¯¹ä½ç½®å‘é‡

```python
R = self.rel_emb(rel_pos)  # shape: (L, L, d_k)
```

`self.rel_emb` æ˜¯ä¸€ä¸ªå¯è®­ç»ƒçš„ embedding è¡¨ï¼Œæ¯ä¸ªç›¸å¯¹è·ç¦»å¯¹åº”ä¸€ä¸ªå‘é‡ã€‚`rel_pos` çš„å½¢çŠ¶æ˜¯ `(L, L)`ï¼ŒæŸ¥è¡¨åï¼š

* ç¬¬ä¸€ç»´ = æŸ¥è¯¢ä½ç½® i

* ç¬¬äºŒç»´ = è¢«å…³æ³¨ä½ç½® j

* ç¬¬ä¸‰ç»´ = å¯¹åº”çš„ç›¸å¯¹ä½ç½®ç¼–ç å‘é‡ $R_{i-j}$ï¼Œé•¿åº¦ `d_k`

æ‰€ä»¥ `R` æ˜¯ä¸€ä¸ª `(L, L, d_k)` å¼ é‡ã€‚

---

4. ç”¨çˆ±å› æ–¯å¦æ±‚å’Œå…¬å¼è®¡ç®—ä½ç½®åˆ†æ•°

```python
pos_score = torch.einsum('bhld,lmd->bhlm', Q, R)
```
å…ˆçœ‹ä¸¤ä¸ªè¾“å…¥çš„å½¢çŠ¶:

* **Q**: `(B, h, L, d_k)`

  * `B` = batch å¤§å°

  * `h` = æ³¨æ„åŠ›å¤´æ•°
  
  * `L` = åºåˆ—é•¿åº¦ï¼ˆæŸ¥è¯¢ token çš„ä½ç½®ï¼‰
  
  * `d_k` = æ¯ä¸ªå¤´çš„å‘é‡ç»´åº¦ï¼ˆQ å‘é‡é•¿åº¦ï¼‰

* **R**: `(L, L, d_k)`

  * ç¬¬ 1 ç»´ï¼šæŸ¥è¯¢ä½ç½® index $i$
  
  * ç¬¬ 2 ç»´ï¼šè¢«å…³æ³¨ä½ç½® index $j$
  
  * ç¬¬ 3 ç»´ï¼šä¸ç›¸å¯¹ä½ç½® $i-j$ å¯¹åº”çš„å‘é‡ï¼ˆé•¿åº¦ `d_k`ï¼‰

çˆ±å› æ–¯å¦æ±‚å’Œè§„åˆ™: `'bhld,lmd->bhlm'`

* **å·¦è¾¹ Q çš„ç»´åº¦**ï¼š`b h l d`

* **å³è¾¹ R çš„ç»´åº¦**ï¼š`l m d`

* ä¸¤è€…ä¸­**ç›¸åŒå­—æ¯**ä»£è¡¨è¦â€œé…å¯¹â€çš„è½´ï¼š

  * `l`ï¼šæŸ¥è¯¢ä½ç½® i â†’ **ä¿æŒä¸å˜**ï¼ˆå‚ä¸é…å¯¹ä½†ä¿ç•™åœ¨è¾“å‡ºé‡Œï¼‰

  * `d`ï¼šå‘é‡ç»´åº¦ â†’ **ç›¸åŒå­—æ¯ä¸”ä¸å‡ºç°åœ¨è¾“å‡ºï¼Œè¡¨ç¤ºè¦ç›¸ä¹˜åæ±‚å’Œ**ï¼ˆç‚¹ç§¯ï¼‰

* **ä¸åŒå­—æ¯**ï¼š

  * `m`ï¼šæ¥è‡ª R çš„â€œè¢«å…³æ³¨ä½ç½®â€ç»´åº¦ï¼Œå‡ºç°åœ¨è¾“å‡º
  
  * `b`ã€`h`ï¼šæ¥è‡ª Q çš„ batch å’Œå¤´ç»´åº¦ï¼Œç›´æ¥ä¿ç•™

æˆ‘ä»¬æƒ³ç®—ï¼š

$$
\text{pos\_score}[b,h,l,m] = \sum_{d=1}^{d_k} Q[b,h,l,d] \times R[l,m,d]
$$

* å›ºå®š batch `b`ã€head `h`ã€æŸ¥è¯¢ä½ç½® `l`ã€è¢«å…³æ³¨ä½ç½® `m`

* ä» Q é‡Œå–å¯¹åº”çš„æŸ¥è¯¢å‘é‡ $Q_{b,h,l,:}$

* ä» R é‡Œå–å¯¹åº”çš„ç›¸å¯¹ä½ç½®å‘é‡ $R_{l,m,:}$

* å¯¹å®ƒä»¬åš**å‘é‡ç‚¹ç§¯**ï¼ˆæ²¿ `d` ç»´æ±‚å’Œï¼‰

--- 

5. ä¸ºä»€ä¹ˆç›¸å¯¹ä½ç½®çŸ©é˜µ $R$ ä¸ç›´æ¥åŠ åˆ°æ³¨æ„åŠ›åˆ†æ•°é‡Œï¼Œè€Œæ˜¯è¿˜è¦å’Œ $Q$ åšç‚¹ç§¯ï¼Ÿ

åœ¨æ ‡å‡†è‡ªæ³¨æ„åŠ›ä¸­ï¼Œæ‰“åˆ†æ˜¯ï¼š

$$
\text{score}(i,j) = Q_i \cdot K_j
$$

å…¶ä¸­ï¼š

* $Q_i = x_i W_Q$ï¼š**æŸ¥è¯¢ token i çš„å†…å®¹å‘é‡**

* $K_j = x_j W_K$ï¼š**è¢«å…³æ³¨ token j çš„å†…å®¹å‘é‡**

è¿™ä¸¤ä¸ªå‘é‡éƒ½åœ¨**åŒä¸€ä¸ªå‘é‡ç©ºé—´**ä¸­ï¼ˆç»´åº¦ `d_k`ï¼‰ï¼Œç‚¹ç§¯æ‰èƒ½å¾—åˆ°ä¸€ä¸ªæœ‰æ„ä¹‰çš„ç›¸ä¼¼åº¦åˆ†æ•°ã€‚Shaw çš„åšæ³•æ˜¯ï¼š

$$
\text{score}(i,j) = Q_i \cdot K_j + Q_i \cdot R_{i-j}
$$

ç¬¬äºŒé¡¹çš„è§£é‡Šï¼š

* $R_{i-j}$ ä¹Ÿæ˜¯åœ¨ `d_k` ç»´çš„å‘é‡ç©ºé—´ä¸­ã€‚

* ç”¨ $Q_i$ å’Œ $R_{i-j}$ åšç‚¹ç§¯ï¼ŒæŠŠ**ä½ç½®ä¿¡æ¯å‘é‡**æŠ•å½±åˆ°å’Œ `Q_i` ä¸€è‡´çš„è¡¨ç¤ºç©ºé—´ã€‚

* è¿™æ ·å¾—åˆ°çš„ç»“æœæ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œå¯ä»¥ç›´æ¥å’Œ `QÂ·K` çš„ç»“æœç›¸åŠ ã€‚

æ¢å¥è¯è¯´ï¼š

* **K éƒ¨åˆ†**å¸¦æ¥**å†…å®¹ç›¸å…³æ€§**

* **R éƒ¨åˆ†**å¸¦æ¥**ä½ç½®ç›¸å…³æ€§**

* äºŒè€…éƒ½è¦åœ¨**åŒä¸€ä¸ªâ€œQ å‘é‡çš„è§†è§’â€ä¸‹è¡¡é‡**ï¼Œæ‰€ä»¥éƒ½ç”¨ $Q_i$ åšç‚¹ç§¯

> å¯ä»¥æŠŠæ³¨æ„åŠ›æ‰“åˆ†çœ‹ä½œâ€œä½ (i)å¯¹åˆ«äºº(j)çš„å…³æ³¨ç¨‹åº¦â€ï¼Œå®ƒå¯èƒ½ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š
> 
> 1. **å†…å®¹ç›¸ä¼¼åº¦**ï¼šä½ å…³å¿ƒå’Œä½ å†…å®¹ç±»ä¼¼çš„äººï¼ˆQÂ·Kï¼‰
> 
> 2. **ä½ç½®åå¥½**ï¼šä½ å…³å¿ƒç¦»ä½ è¿‘çš„äººï¼ˆQÂ·Rï¼‰

### T5 ç›¸å¯¹ä½ç½®åç½®ï¼ˆRelative Position Bias, RPBï¼‰

åœ¨è‡ªæ³¨æ„åŠ›é‡Œï¼Œæ³¨æ„åŠ›æ‰“åˆ†æ˜¯

$$
\text{score}(i,j)=\frac{Q_i\cdot K_j}{\sqrt{d_k}}
$$

T5 åœ¨è¿™ä¸ªåˆ†æ•°ä¸Š**å†åŠ ä¸€ä¸ªä¸â€œç›¸å¯¹è·ç¦»â€ç›¸å…³çš„æ ‡é‡åç½®**ï¼š

$$
\text{score}(i,j)=\frac{Q_i\cdot K_j}{\sqrt{d_k}}+\underbrace{b_{\text{bucket}(i-j)}}_{\text{ç›¸å¯¹ä½ç½®åç½®ï¼ˆæ ‡é‡ï¼‰}}
$$

* $i-j$ æ˜¯æŸ¥è¯¢ä½ç½®åˆ°é”®ä½ç½®çš„ç›¸å¯¹è·ç¦»ã€‚

* ä¸ºäº†**å‚æ•°æ›´çœ**ã€**æ³›åŒ–æ›´å¥½**ï¼ŒT5 ä¸ä¸ºæ¯ä¸ªè·ç¦»å•ç‹¬å­¦ä¸€ä¸ªå‚æ•°ï¼Œè€Œæ˜¯æŠŠè·ç¦»æ˜ å°„åˆ°**å°‘é‡â€œæ¡¶â€ï¼ˆbucketï¼‰**ï¼šè¿‘è·ç¦»ç”¨ç»†æ¡¶ã€è¿œè·ç¦»ç”¨ç²—æ¡¶ï¼ˆå¯¹æ•°åˆ†æ¡¶ï¼‰ã€‚

* æ¯ä¸ªæ¡¶å­¦ä¸€ä¸ª**æ ‡é‡åç½®**ï¼Œé€šå¸¸ **æŒ‰å¤´**ï¼ˆper-headï¼‰ç‹¬ç«‹å­¦ä¹ ï¼ˆå½¢çŠ¶ï¼š`[num_buckets, n_heads]`ï¼‰ï¼Œå†å¹¿æ’­åˆ° `(B, n_heads, L_q, L_k)`ã€‚

è¿™æ ·æ—¢è®©æ¨¡å‹â€œåå¥½ä¸´è¿‘â€æˆ–â€œæƒ©ç½šå¤ªè¿œâ€ï¼Œåˆå‡ ä¹ä¸å¢åŠ è®¡ç®—é‡ã€‚

> ![](ä½ç½®ç¼–ç /4.png)
> 
> - Shaw RPRï¼šç›¸å¯¹ä½ç½®åƒâ€œé¢å¤–çš„ Key ç‰¹å¾â€ï¼Œå‚ä¸ç‚¹ç§¯è®¡ç®—
>
> - T5 RPBï¼šç›¸å¯¹ä½ç½®åƒâ€œåˆ†æ•°ä¿®æ­£è¡¨â€ï¼Œåªåœ¨ attention åˆ†æ•°ä¸ŠåŠ åç½®

---

1. åç½®è¡¨ï¼ˆbias tableï¼‰æ€ä¹ˆç†è§£ ?

```python
bias_table = nn.Parameter(torch.zeros(num_buckets, H))
```

* **num\_buckets**ï¼šè¡¨ç¤ºâ€œç›¸å¯¹ä½ç½®çš„åˆ†ç»„æ•°é‡â€ã€‚

  * T5 å¹¶ä¸å¯¹æ¯ä¸ªç›¸å¯¹ä½ç½®éƒ½å•ç‹¬å­˜ä¸€ä¸ªåç½®ï¼Œè€Œæ˜¯æŠŠç›¸å¯¹è·ç¦»å‹ç¼©åˆ°è‹¥å¹²ä¸ªæ¡¶ï¼ˆbucketï¼‰é‡Œã€‚

  * æ¯”å¦‚å°è·ç¦» 1ã€2ã€3 å¯ä»¥æ˜ å°„åˆ°åŒä¸€ä¸ªæ¡¶ï¼Œè¿œè·ç¦»å¯èƒ½æ˜ å°„åˆ°ä¸åŒæ¡¶ã€‚

* **H**ï¼šè¡¨ç¤ºæ³¨æ„åŠ›å¤´æ•°ï¼Œæ¯ä¸ªå¤´çš„åç½®å¯ä»¥ä¸åŒã€‚

  * ä¸åŒå¤´å¯ä»¥å­¦ä¹ åˆ°ä¸åŒçš„ç›¸å¯¹ä½ç½®åç½®æ¨¡å¼ï¼Œæ¯”å¦‚ä¸€ä¸ªå¤´å…³æ³¨çŸ­è·ç¦»ï¼Œä¸€ä¸ªå¤´å…³æ³¨é•¿è·ç¦»ã€‚

åç½®è¡¨ä½œç”¨:

* åç½®è¡¨é‡Œçš„æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª**æ ‡é‡**ï¼Œå®ƒå¯¹åº”æŸä¸ª**ç›¸å¯¹ä½ç½®æ¡¶ + æ³¨æ„åŠ›å¤´**çš„åç½®ã€‚

* å®ƒä¸ä¼šå‚ä¸ç‚¹ç§¯è¿ç®—ï¼Œåªæ˜¯ç›´æ¥**åŠ åˆ°æ³¨æ„åŠ›åˆ†æ•°**ä¸Šï¼š

$$
\text{attn\_score}[b,h,i,j] = Q_i \cdot K_j + \text{bias}[bucket_{i-j}, h]
$$

* é€šè¿‡åŠ åç½®ï¼Œæ¨¡å‹å¯ä»¥**ç¼–ç â€œç›¸å¯¹ä½ç½®ä¿¡æ¯â€**ï¼Œæ¯”å¦‚è®©æ¨¡å‹åå‘å…³æ³¨é™„è¿‘çš„ tokenã€‚

å¯ä»¥æƒ³è±¡æˆä¸€ä¸ªå°è¡¨æ ¼ï¼š

| æ¡¶ (bucket) | head0 | head1 | head2 |
| ---------- | ----- | ----- | ----- |
| 0          | 0.1   | -0.2  | 0.05  |
| 1          | 0.3   | 0.0   | 0.1   |
| 2          | -0.1  | 0.2   | 0.0   |
| â€¦          | â€¦     | â€¦     | â€¦     |

* æ¯ä¸€åˆ— = ä¸€ä¸ªå¤´çš„æ‰€æœ‰åç½®

* æ¯ä¸€è¡Œ = ä¸€ä¸ªç›¸å¯¹è·ç¦»æ¡¶çš„åç½®

å½“æ¨¡å‹è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°æ—¶ï¼Œå®ƒä¼šæ ¹æ® query-key çš„ç›¸å¯¹è·ç¦»æ‰¾åˆ°å¯¹åº”çš„æ¡¶ï¼Œç„¶åæŸ¥è¡¨å–å‡ºåç½®ï¼ŒåŠ åˆ°è¯¥å¤´çš„æ³¨æ„åŠ›åˆ†æ•°ä¸Šã€‚

---

2. æ¡¶çš„ä½œç”¨æ˜¯ä»€ä¹ˆ ï¼Ÿ

æˆ‘ä»¬å·²ç»æœ‰äº† **ç›¸å¯¹ä½ç½®çŸ©é˜µ** `relative_position[i,j] = j - i` ï¼Œä¾‹å¦‚é•¿åº¦ 8 çš„åºåˆ—ï¼š

```
i\j   0   1   2   3   4   5   6   7
0     0   1   2   3   4   5   6   7
1    -1   0   1   2   3   4   5   6
2    -2  -1   0   1   2   3   4   5
3    -3  -2  -1   0   1   2   3   4
...
```

* æ¯ä¸ªå…ƒç´ è¡¨ç¤º query i å¯¹ key j çš„ç›¸å¯¹è·ç¦»

**é—®é¢˜ï¼šåºåˆ—å¾ˆé•¿æ—¶ï¼Œå¦‚æœç›´æ¥ä¸ºæ¯ä¸ªè·ç¦»å­¦ä¹ åç½®ï¼Œå‚æ•°é‡ä¼šéå¸¸å¤§**ã€‚

æ¡¶æ˜ å°„çš„ç›®çš„:

* **æŠŠç›¸å¯¹è·ç¦»å‹ç¼©åˆ°å›ºå®šæ•°é‡çš„æ¡¶é‡Œ**

* T5 çš„åšæ³•ï¼š**å°è·ç¦»ç”¨ç‹¬ç«‹æ¡¶ï¼Œå¤§è·ç¦»ç”¨å¯¹æ•°å‹ç¼©çš„æ¡¶**

* è¿™æ ·ï¼š

  1. **çŸ­è·ç¦»**ï¼šæ¯ä¸ªè·ç¦»æœ‰è‡ªå·±çš„åç½®ï¼ˆç»†ç²’åº¦ï¼‰

  2. **é•¿è·ç¦»**ï¼šè·ç¦»è¾ƒå¤§çš„ token å…±äº«åŒä¸€æ¡¶ï¼ˆç²—ç²’åº¦ï¼‰

* ä¼˜ç‚¹ï¼š

  * èŠ‚çœå‚æ•°

  * ä¿æŒæ¨¡å‹å…³æ³¨çŸ­è·ç¦»ç²¾ç»†ä¿¡æ¯ï¼ŒåŒæ—¶å¯¹é•¿è·ç¦»ä¸å¿…è¿‡äºç²¾ç»†

å‡è®¾ `num_buckets = 4`ï¼ˆç®€åŒ–ï¼‰ï¼š

* relative\_position â‰¤ 1 â†’ bucket 0

* relative\_position = 2 â†’ bucket 1

* relative\_position = 3\~4 â†’ bucket 2

* relative\_position > 4 â†’ bucket 3

å¯¹åºåˆ— `[A,B,C,D]`ï¼š

```
relative_position:

i\j   0   1   2   3
0     0   1   2   3
1    -1   0   1   2
2    -2  -1   0   1
3    -3  -2  -1   0

æ˜ å°„åˆ°æ¡¶ (bucket):

i\j   0   1   2   3
0     0   0   1   2
1     0   0   0   1
2     1   0   0   0
3     2   1   0   0
```

* æ¯ä¸ªå…ƒç´ éƒ½å˜æˆäº† **0\~num\_buckets-1** çš„æ•´æ•°

* è¿™ä¸ªæ•´æ•°å°±æ˜¯åç½®æŸ¥è¡¨çš„ç´¢å¼•

**æ ¸å¿ƒç†è§£**ï¼š

* **ç›¸å¯¹ä½ç½®æ¡¶** = â€œè·ç¦»åˆ†ç»„â€

* é€šè¿‡æ¡¶æ˜ å°„ï¼Œå¯ä»¥è®©æ¨¡å‹ **å¯¹çŸ­è·ç¦»æ•æ„Ÿï¼Œå¯¹é•¿è·ç¦»ç²—ç•¥å¤„ç†**

* æ¡¶å·æœ€ç»ˆç”¨æ¥æŸ¥ `bias_table`ï¼Œå¾—åˆ°æ¯ä¸ª query-key å¯¹çš„åç½®

---

3. æ•´ä½“æµç¨‹ ?

å‡è®¾ï¼š

* batch size: `B`

* åºåˆ—é•¿åº¦: `L`

* æ³¨æ„åŠ›å¤´æ•°: `H`

* ç›¸å¯¹ä½ç½®æ¡¶æ•°: `num_buckets`

**(1) åç½®è¡¨**

```python
bias_table = nn.Parameter(torch.zeros(num_buckets, H))
```

* shape: `(num_buckets, H)`

* æ¯ä¸ªæ¡¶æ¯ä¸ªå¤´éƒ½æœ‰ä¸€ä¸ªæ ‡é‡åç½®

**(2) ç›¸å¯¹ä½ç½®çŸ©é˜µ**

```python
relative_position = memory_position - context_position  # (L,L)
```

* shape: `(L, L)`

* å€¼æ˜¯ `j-i`ï¼Œè¡¨ç¤º query i å¯¹ key j çš„ç›¸å¯¹ä½ç½®

**(3) æ¡¶æ˜ å°„**

```python
relative_buckets = relative_position_bucket(relative_position)  # (L,L)
```

* shape: `(L,L)`

* æ¯ä¸ªå…ƒç´ æ˜¯ 0\~`num_buckets-1`

**(4) æŸ¥è¡¨å¾—åˆ°åç½®**

```python
bias = bias_table[relative_buckets]  # (L,L,H)
bias = bias.permute(2,0,1)           # (H,L,L)
```

* `bias_table[relative_buckets]` ç”¨æ¯ä¸ª `(i,j)` æ¡¶å·æŸ¥è¡¨

* å¾—åˆ° `(L,L,H)`ï¼Œç„¶åæ¢ç»´åº¦åˆ° `(H,L,L)`ï¼Œæ–¹ä¾¿åŠ åˆ°æ³¨æ„åŠ›åˆ†æ•°

**(5) å¹¿æ’­ batch**

æ³¨æ„åŠ›åˆ†æ•°ï¼š

```python
attn_scores = Q @ K.transpose(-1,-2)  # (B,H,L,L)
attn_scores = attn_scores + bias.unsqueeze(0)  # (B,H,L,L)
```

* `bias.unsqueeze(0)` shape â†’ `(1,H,L,L)`

* è‡ªåŠ¨å¹¿æ’­åˆ° batch ç»´åº¦ `(B,H,L,L)`

* **æ¯ä¸ªå¤´æœ‰ç‹¬ç«‹åç½®**ï¼Œä¸åŒ batch å…±ç”¨

**(6) ç›¸åŠ è¿‡ç¨‹æ€»ç»“**

1. å…ˆç®— **åŸå§‹æ³¨æ„åŠ›åˆ†æ•°**ï¼š`Q @ K^T` â†’ `(B,H,L,L)`

2. æŸ¥è¡¨å¾—åˆ° **ç›¸å¯¹ä½ç½®åç½®**ï¼š`bias` â†’ `(H,L,L)`

3. åŠ åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼š

   $$
   \text{attn\_scores}_{b,h,i,j} = Q_i \cdot K_j + B_{i-j,h}
   $$

4. æœ€ç»ˆ shape ä»ç„¶ `(B,H,L,L)`ï¼Œç”¨äº softmax

ä¸‹é¢ç»™å‡ºä¸€ä¸ªå¯ç›´æ¥è¿è¡Œã€å¯å¤ç”¨çš„ PyTorch å®ç°ï¼Œå®Œæ•´è¦†ç›– T5 çš„ç›¸å¯¹ä½ç½®åç½®ï¼ˆRelative Position Bias, RPBï¼‰ã€‚åŒ…å«

1. åˆ†æ¡¶ï¼ˆbucketingï¼‰

2. RPB æ¨¡å—

3. é›†æˆåˆ°å¤šå¤´æ³¨æ„åŠ›ï¼ˆæ”¯æŒåŒå‘/å•å‘ä¸ maskï¼‰

4. ç®€çŸ­ä½¿ç”¨ç¤ºä¾‹

```python
import math
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F


# -------- 1) T5 åˆ†æ¡¶å‡½æ•°ï¼ˆåŒå‘/å•å‘å‡å¯ï¼‰ --------
def relative_position_bucket(
    relative_position: torch.Tensor,
    num_buckets: int = 32,
    max_distance: int = 128,
    bidirectional: bool = True,
) -> torch.Tensor:
    """
    å°†ç›¸å¯¹è·ç¦»æ˜ å°„åˆ° [0, num_buckets-1] çš„æ¡¶ç´¢å¼•ï¼ˆint64ï¼‰ã€‚
    T5 æ€æƒ³ï¼šå°è·ç¦»ç”¨çº¿æ€§æ¡¶ï¼Œå¤§è·ç¦»ç”¨å¯¹æ•°æ¡¶ï¼›åŒå‘æ—¶ä¸€åŠæ¡¶ç»™è´Ÿå‘ï¼Œä¸€åŠç»™æ­£å‘ã€‚
    relative_position: (Lq, Lk), å€¼ä¸º i - jï¼ˆquery i, key jï¼‰
    """
    if bidirectional:
        # ä¸€åŠç»™è´Ÿå‘, ä¸€åŠç»™éè´Ÿå‘
        num_buckets_half = num_buckets // 2
        # ç»å¯¹è·ç¦»
        n = relative_position.abs()
    else:
        # å•å‘ï¼ˆé€šå¸¸è§£ç å™¨è‡ªæ³¨æ„åŠ›ï¼‰ï¼šåªè€ƒè™‘è¿‡å»ï¼ˆi>=jï¼‰ï¼Œè´Ÿå€¼æŒ‰0èµ·ç®—
        num_buckets_half = num_buckets
        n = -torch.minimum(relative_position, torch.zeros_like(relative_position))
        # å¦‚ i<j æ—¶ relative_position ä¸ºè´Ÿï¼Œè¢«æ˜ å°„ä¸ºæ­£è·ç¦»

    # çº¿æ€§-å¯¹æ•°åˆ†ç•Œï¼ˆç»éªŒï¼šç”¨åŠæ•°æ¡¶çš„å‰åŠæ®µåšâ€œç²¾ç¡®çº¿æ€§â€ï¼‰
    max_exact = num_buckets_half // 2  # çº¿æ€§éƒ¨åˆ†ä¸Šç•Œï¼ˆå«ï¼‰
    # çº¿æ€§åŒºåŸŸ
    is_small = n < max_exact
    val_small = n

    # å¯¹æ•°åŒºåŸŸï¼ˆå‹ç¼©åˆ° [max_exact, num_buckets_half-1]ï¼‰
    # é¿å…é™¤é›¶/å–å¯¹æ•°é—®é¢˜
    n_float = n.to(torch.float32)
    max_exact_f = float(max_exact)
    max_distance_f = float(max_distance)

    # clamp ç¡®ä¿ >= max_exactï¼Œé¿å… log(0)
    n_clamped = torch.maximum(n_float, torch.tensor(max_exact_f, device=n.device))
    # å°†è·ç¦»æ˜ å°„åˆ° [0,1] çš„å¯¹æ•°æ¯”ä¾‹ï¼Œå†çº¿æ€§æ”¾ç¼©åˆ°æ¡¶åŒºé—´
    # log_ratio = log(n/max_exact) / log(max_distance/max_exact)
    denom = math.log(max_distance_f / max_exact_f + 1e-10)
    log_ratio = torch.log(n_clamped / max_exact_f) / denom if denom > 0 else torch.zeros_like(n_float)
    val_large = max_exact + (log_ratio * (num_buckets_half - max_exact)).to(torch.int64)
    val_large = torch.minimum(val_large, torch.full_like(val_large, num_buckets_half - 1))

    buckets = torch.where(is_small, val_small.to(torch.int64), val_large)

    if bidirectional:
        # è´Ÿæ–¹å‘ï¼ˆrelative_position < 0ï¼‰è½åˆ°å‰åŠåŒºï¼Œéè´Ÿæ–¹å‘è½åˆ°ååŠåŒº
        is_negative = (relative_position < 0).to(torch.int64)
        buckets = buckets + is_negative * 0 + (1 - is_negative) * num_buckets_half
        # æ³¨ï¼šå¦‚æœä½ å¸Œæœ›å®Œå…¨å¯¹ç§°ï¼Œä¹Ÿå¯å°†è´Ÿå‘åŠ 0ã€æ­£å‘åŠ  num_buckets_half

    return buckets


# -------- 2) ç›¸å¯¹ä½ç½®åç½®æ¨¡å—ï¼ˆå¯ç¼“å­˜é•¿åº¦é…ç½®ï¼‰ --------
class T5RelativePositionBias(nn.Module):
    """
    T5 ç›¸å¯¹ä½ç½®åç½®ï¼š
    - ä¸ºæ¯ä¸ªæ¡¶ã€æ¯ä¸ªå¤´å­¦ä¹ ä¸€ä¸ªæ ‡é‡åç½®ï¼ˆEmbedding: [num_buckets, n_heads]ï¼‰
    - forward ä»…ä¾èµ– (Lq, Lk)ï¼Œè¿”å› (1, H, Lq, Lk) ä»¥ä¾¿ä¸ (B, H, Lq, Lk) å¹¿æ’­ç›¸åŠ 
    - å¯é…ç½®åŒå‘/å•å‘ï¼Œæ¡¶æ•°ä¸æœ€å¤§è·ç¦»
    """
    def __init__(
        self,
        n_heads: int,
        num_buckets: int = 32,
        max_distance: int = 128,
        bidirectional: bool = True,
        cache: bool = True,
    ):
        super().__init__()
        self.n_heads = n_heads
        self.num_buckets = num_buckets
        self.max_distance = max_distance
        self.bidirectional = bidirectional

        # æ¯ä¸ªæ¡¶ã€æ¯ä¸ªå¤´ä¸€ä¸ªæ ‡é‡åç½®
        self.relative_attention_bias = nn.Embedding(num_buckets, n_heads)
        nn.init.zeros_(self.relative_attention_bias.weight)

        # ç®€æ˜“ç¼“å­˜ï¼šæŒ‰ (Lq, Lk) ç¼“å­˜å·²æ„é€ çš„ (1, H, Lq, Lk) åç½®
        self._cache_enabled = cache
        self._cache = {}  # key: (Lq, Lk, device) -> Tensor

    @torch.no_grad()
    def _maybe_from_cache(self, Lq: int, Lk: int, device: torch.device) -> Optional[torch.Tensor]:
        if not self._cache_enabled:
            return None
        key = (Lq, Lk, device.index if device.type == "cuda" else -1)
        return self._cache.get(key, None)

    @torch.no_grad()
    def _save_cache(self, Lq: int, Lk: int, device: torch.device, tensor: torch.Tensor):
        if not self._cache_enabled:
            return
        key = (Lq, Lk, device.index if device.type == "cuda" else -1)
        self._cache[key] = tensor

    def forward(self, q_len: int, k_len: int, device: Optional[torch.device] = None) -> torch.Tensor:
        device = device or self.relative_attention_bias.weight.device

        cached = self._maybe_from_cache(q_len, k_len, device)
        if cached is not None:
            return cached

        q_pos = torch.arange(q_len, device=device)[:, None]         # (Lq, 1)
        k_pos = torch.arange(k_len, device=device)[None, :]         # (1, Lk)
        rel_pos = q_pos - k_pos                                     # (Lq, Lk), å€¼ä¸º i - j

        buckets = relative_position_bucket(
            rel_pos, num_buckets=self.num_buckets,
            max_distance=self.max_distance, bidirectional=self.bidirectional
        )                                                            # (Lq, Lk), int64

        # æŸ¥è¡¨ï¼šå¾—åˆ° (Lq, Lk, H)
        bias_per_head = self.relative_attention_bias(buckets)        # (Lq, Lk, H)

        # å˜æ¢åˆ° (1, H, Lq, Lk)ï¼Œæ–¹ä¾¿ä¸ (B, H, Lq, Lk) å¹¿æ’­
        bias = bias_per_head.permute(2, 0, 1).unsqueeze(0).contiguous()  # (1, H, Lq, Lk)

        # ç¼“å­˜
        self._save_cache(q_len, k_len, device, bias)
        return bias


# -------- 3) é›†æˆåˆ°å¤šå¤´æ³¨æ„åŠ›ï¼ˆæ”¯æŒåŒå‘/å•å‘ã€å¤–éƒ¨maskï¼‰ --------
class MHAWithT5RPB(nn.Module):
    """
    ä¸€ä¸ªæç®€çš„å¤šå¤´æ³¨æ„åŠ› + T5 RPB å®ç°ã€‚
    - causal=True æ—¶ï¼šå•å‘æ³¨æ„åŠ›ï¼ˆè§£ç å™¨è‡ªæ³¨æ„åŠ›ï¼‰ï¼ŒåŒæ—¶åŠ å› æœ maskã€‚
    - attn_mask: å¯é€‰å¤–éƒ¨ maskï¼Œä¸ scores åŒå½¢çŠ¶å¯å¹¿æ’­ï¼›å¡« -inf çš„ä½ç½®å°†è¢«å±è”½ã€‚
    """
    def __init__(
        self,
        d_model: int,
        n_heads: int,
        num_buckets: int = 32,
        max_distance: int = 128,
        causal: bool = False,
        dropout: float = 0.0,
    ):
        super().__init__()
        assert d_model % n_heads == 0
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.causal = causal

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        self.attn_drop = nn.Dropout(dropout)

        self.rpb = T5RelativePositionBias(
            n_heads=n_heads,
            num_buckets=num_buckets,
            max_distance=max_distance,
            bidirectional=not causal,
            cache=True,
        )

    def forward(
        self,
        x_q: torch.Tensor,                 # (B, Lq, d_model)
        x_kv: Optional[torch.Tensor] = None,  # (B, Lk, d_model) æˆ– Noneï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰
        attn_mask: Optional[torch.Tensor] = None,  # (B, 1 or H, Lq, Lk) æˆ– (1,H,Lq,Lk)
        need_weights: bool = False,
    ):
        if x_kv is None:
            x_kv = x_q

        B, Lq, _ = x_q.shape
        _, Lk, _ = x_kv.shape
        H, D = self.n_heads, self.d_k
        device = x_q.device

        Q = self.W_q(x_q).view(B, Lq, H, D).transpose(1, 2)  # (B, H, Lq, D)
        K = self.W_k(x_kv).view(B, Lk, H, D).transpose(1, 2) # (B, H, Lk, D)
        V = self.W_v(x_kv).view(B, Lk, H, D).transpose(1, 2) # (B, H, Lk, D)

        # åŸºç¡€ logits
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(D)  # (B, H, Lq, Lk)

        # åŠ  RPBï¼ˆä¸ batch å¹¿æ’­ï¼‰
        scores = scores + self.rpb(Lq, Lk, device=device)             # (B, H, Lq, Lk)

        # å¤–éƒ¨ maskï¼ˆå¦‚ padding / è·¨å¥ç­‰ï¼‰ï¼Œå¡« -inf çš„ä½ç½®å°†è¢«å±è”½
        if attn_mask is not None:
            scores = scores + attn_mask

        # å› æœ maskï¼ˆå•å‘ï¼‰
        if self.causal:
            causal_mask = torch.ones(Lq, Lk, device=device, dtype=torch.bool).triu(1)
            scores = scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float("-inf"))

        attn = F.softmax(scores, dim=-1)
        attn = self.attn_drop(attn)

        out = torch.matmul(attn, V)                  # (B, H, Lq, D)
        out = out.transpose(1, 2).contiguous().view(B, Lq, self.d_model)
        out = self.W_o(out)                          # (B, Lq, d_model)

        if need_weights:
            return out, attn                         # è¿”å›æ³¨æ„åŠ›æƒé‡ä»¥ä¾¿å¯è§†åŒ–/è°ƒè¯•
        return out


# -------- 4) ç®€çŸ­ä½¿ç”¨ç¤ºä¾‹ --------
if __name__ == "__main__":
    torch.manual_seed(0)

    B, L, d_model, H = 2, 16, 256, 8
    x = torch.randn(B, L, d_model)

    # ç¼–ç å™¨è‡ªæ³¨æ„åŠ›ï¼ˆåŒå‘ï¼‰
    enc_attn = MHAWithT5RPB(d_model=d_model, n_heads=H, num_buckets=32, max_distance=128, causal=False, dropout=0.1)
    y_enc, w_enc = enc_attn(x, need_weights=True)
    print("Encoder:", y_enc.shape, w_enc.shape)  # (B, L, d_model), (B, H, L, L)

    # è§£ç å™¨è‡ªæ³¨æ„åŠ›ï¼ˆå•å‘ + å› æœ maskï¼‰
    dec_attn = MHAWithT5RPB(d_model=d_model, n_heads=H, num_buckets=32, max_distance=128, causal=True, dropout=0.1)
    y_dec, w_dec = dec_attn(x, need_weights=True)
    print("Decoder:", y_dec.shape, w_dec.shape)  # (B, L, d_model), (B, H, L, L)
```