---
title: 位置编码
icon: file
category:
  - 位置编码
tag:
  - 已发布
footer: 技术共建，知识共享
date: 2025-08-13
author:
  - BinaryOracle
---

`位置编码` 

<!-- more -->

## 绝对位置编码（Absolute Positional Encoding, APE）

### 正弦/余弦位置编码（Sinusoidal Positional Encoding）

正弦余弦位置编码（Sinusoidal Positional Encoding）是一种无需训练的位置编码方法，它通过固定的周期性函数（正弦和余弦）来为序列的不同位置提供唯一的编码。对于每个位置 $𝑖$ 和每个维度 $𝑑$，位置编码通过以下公式计算：

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

* $𝑝𝑜𝑠$ 表示位置索引，表示计算哪个位置的编码

* $𝑖$ 表示编码维度，$𝑑𝑚𝑜𝑑𝑒𝑙$ 是编码空间的总维度

* $PE_{(pos, 2i)}$ 和 $PE_{(pos, 2i+1)}$ 通过正弦和余弦函数分裂映射到偶数和奇数的维度

假设：我们要计算输入序列第 2 个位置 Token 对应的位置编码，编码的维度设定为 4 ，则：

![](位置编码/1.png)

最终，位置 2 的编码向量为：$(𝑠𝑖𝑛(2),𝑐𝑜𝑠(2),𝑠𝑖𝑛(0.02),𝑐𝑜𝑠(0.02))$，我们把它加到第二个 Token 的词嵌入向量上，就相当于给其注入了顺序信息。

计算起来是比较容易的，如何去理解这个位置编码？请看下面这张图：

![](位置编码/2.png)

这三张图分别打印了 128 个位置向量第 2、6、12 维度的编码值的变化，我们发现这些值呈现周期性的变化。另外，我们也可以发现，向量维度越高，其周期就越长。

![](位置编码/3.png)

上图，我们打印了位置 1、5 的编码向量中的 sin 和 cos 计算得到的编码值。我们可以发现，基于正弦和余弦函数得到的位置编码可以保证唯一性。另外，也可以看到，向量的维度越高，编码值的波动就越小，向量就越接近。

简单总结下：

1. 因为正弦和余弦函数都是周期函数，编码在不同维度上具有不同的周期性

2. 位置编码向量是唯一的，因为不同位置的编码由不同的正弦和余弦值组成

3. 低维度的编码值波动性很大（周期短），高纬度的编码值波动性较小（周期长）

所以，可以得到一个简单的结论：

1. 低维分量（小 i）的变化较快，主要捕捉局部位置关系

2. 高维分量（大 i）的变化较慢，可以用于编码全局信息

这个怎么去理解？我们把某个位置的向量大概划分为两部分：低维向量部分 + 高维向量部分，低纬向量部分数值波动幅度很大，在一个周期内只能包含少量相邻的位置，并且一定程度上也表达了位置的局部的相对信息，这就是捕捉局部位置关系。那么，对于高纬向量部分而言，它的波动幅度很小，一个周期能够包含更多的位置信息，这也是我们理解的编码全局位置信息的含义。

所以，对于一个基于正弦余弦编码的位置向量，可以理解为该向量中隐含了一些局部和全局的位置信息。使得 Transformer 既能感知局部相对位置，也能感知全局位置信息，从而弥补其原生结构中缺少位置感知能力的缺陷。

当然，这种位置编码方法也存在以下一些不足之处：

随着序列长度的增加，位置编码的周期性可能导致不同位置之间的区分度逐渐降低，难以准确表示极长序列中各个位置的独特信息。

虽然正弦余弦位置编码能够隐含地表达一定的局部位置信息，但由于它是固定的、不可学习的，并没有专门针对局部依赖关系进行优化，因此在建模局部依赖关系时能力相对不足。

正弦余弦位置编码是一种基于三角函数的固定编码方式，它是一种静态的位置信息表示。而注意力机制更关注的是文本中不同位置之间的动态语义关联。这两种信息在表示形式和语义侧重点上存在差异，导致在融合时可能无法很好地相互补充。

正弦余弦位置编码通常是高维向量，其计算量会随着维度的增加不仅需要更多的计算时间，还可能占用大量的内存空间，影响模型的运行效率。

> 正余弦位置编码通过在不同维度上引入不同波长的正余弦信号，使得低维对局部位置变化敏感，高维对全局位置变化敏感。虽然位置和语义在所有维度上混合，但在训练中，模型可能学到一种“低维更多位置，高维更多语义”的分工模式。

### 基于可学习的嵌入

可学习的位置编码（Learnable Positional Encoding, LPE）是一种通过梯度下降自动学习位置编码的方法，不同于固定编码（如正弦/余弦函数编码），它不依赖任何手工设计的公式，而是直接让模型在训练过程中优化位置信息。对于一个长度为 𝐿 的输入序列，每个位置 𝑖 都对应一个可学习的向量。当训练或测试时，将输入 Token 的编码和对应位置的可学习位置编码向量相加，从而赋予 Token 相应的位置信息。

这种位置编码方式能够根据具体的任务和数据特点，模型可以学习到更适合该类文本的位置表示方式，捕捉文本中位置相关的语义和结构信息，这是固定的位置编码（例如：正弦余弦位置编码）难以做到的。但是也存在一些不足之处，例如：

1. 如果训练时 max_len=512，测试时输入 1024 长度的序列，模型就无法处理了

2. 需要存储 max_len×d_model 维度的参数，可能导致大模型训练更难收敛

