---
title: 常用评估指标
icon: file
category:
  - metric
tag:
  - 已发布
footer: 技术共建，知识共享
date: 2025-06-11
author:
  - BinaryOracle
---

`常用评估指标` 

<!-- more -->

## 混淆矩阵 (confusion_matrix)

二元分类器的每个输出有四种可能的结果，如果我们将标准答案作为列，将模型的预测作为行，则会得到以下表格（称为混淆矩阵）：

|               | 实际正例              | 实际负例              |
|---------------|-----------------------|-----------------------|
| **预测为正例**  | 真正例 (TP)：垃圾邮件被正确分类为垃圾邮件。 | <span style="background-color:#ffd6cc">假正例 (FP)：非垃圾邮件被误分类为垃圾邮件。</span> |
| **预测为负例**  | <span style="background-color:#ffd6cc">假负例 (FN)：垃圾邮件被误分类为非垃圾邮件。</span> | 真负例 (TN)：非垃圾邮件被正确分类为非垃圾邮件。 |

请注意，每行的总和表示所有预测正例 (TP + FP) 和所有预测负例 (FN + TN)，无论其有效性如何。与此同时，每个列中的总和会显示所有真实正例 (TP + FN) 和所有真实负例 (FP + TN)，而不会考虑模型分类。

如果实际正例的总数与实际负例的总数不接近，则表示数据集不平衡。不平衡数据集的一个示例可能是一组数以千计的云彩照片，其中您感兴趣的罕见云彩类型（例如卷云）只出现了几次。

## 分类场景

### 准确率 (accuracy)

准确性是指所有分类（无论是正类还是负类）正确分类的比例。其数学定义为：

$$
accuracy = \frac{correct \ classifications}{total \ classifications} = \frac{TP + TN}{TP + TN + FP + FN}
$$

在垃圾邮件分类示例中，准确率衡量的是所有电子邮件正确分类所占的比例。

完美的模型没有假正例和假负例，因此准确率为 1.0，即 100%。

由于准确率包含混淆矩阵中的所有四种结果（TP、FP、TN、FN），因此在类别数量相近且平衡的数据集的情况下，准确率可以作为衡量模型质量的粗略指标。

不过，如果数据集不平衡，或者一种错误（假负例或假正例）的代价高于另一种错误（大多数实际应用中都是如此），则最好改为针对其他指标进行优化。

对于严重不均衡的数据集（其中一个类别出现的频率非常低，例如 1%），如果模型 100% 都预测为负类别，则其准确性得分为 99%，尽管该模型毫无用处。

### 召回率 (recall) / 真正例率

真正例率 (TPR)，即所有实际正例被正确分类为正例的比例，也称为召回率。

在数学上，召回率的定义为：

$$
recall = \frac{correctly \ classified \ actual \ positvies}{all \ actual \ positives} = \frac{TP}{TP + FN}
$$

假负例是指被误分类为负例的实际正例，因此会出现在分母中。在垃圾邮件分类示例中，召回率衡量的是被正确分类为垃圾邮件的垃圾邮件电子邮件的比例。

假设一个完美的模型不会出现假负例，因此其召回率 (TPR) 为 1.0，也就是说，检测率为 100%。

在实际正例数量非常少的不平衡数据集中，召回率比准确率更有意义，因为它衡量的是模型正确识别所有正例实例的能力。对于疾病预测等应用，正确识别阳性病例至关重要。假负例通常比假正例的后果更严重。

### 误报概率 / 假正例率

假正例率 (FPR) 是指被错误地归类为正例的所有实际负例所占的比例，也称为误报概率。其数学定义为：

$$
FPR = \frac{incorrectly \ classified \ actual \ negatives}{all \ actual \ negatives} = \frac{FP}{FP + TN}
$$

假正例是被错误分类的实际负例，因此会出现在分母中。在垃圾邮件分类示例中，FPR 用于衡量被错误分类为垃圾邮件的合法电子邮件的比例，或模型的误报率。

完美的模型不会产生假正例，因此其假正例率为 0.0，也就是说，假正例率为 0%。

在实际负例数量非常少（例如总共 1-2 个示例）的不平衡数据集中，FPR 作为一个指标就没有那么有意义和实用。

### 精确率

精确率是指模型所有正类别分类中实际为正类别的分类所占的比例。在数学上，其定义为：

$$
precision = \frac{correctly \ classified \ actual \ positvies}{all \ classified \ positives} = \frac{TP}{TP + FP}
$$

在垃圾邮件分类示例中，精确率衡量的是被归类为垃圾邮件且实际上是垃圾邮件的电子邮件所占的比例。

假设有一个完美的模型，则其假正例数为零，因此精确率为 1.0。

在实际正例数量非常少（例如总共 1-2 个示例）的不平衡数据集中，精确率作为指标的意义和实用性较低。

随着假正例的减少，精确率会提高；随着假负例的减少，召回率会提高。但正如前面所述，提高分类阈值往往会减少假正例的数量并增加假负例的数量，而降低阈值则会产生相反的效果。因此，精确率和召回率通常呈现反向关系，提高其中一个会降低另一个。

### 指标的选择和权衡

在评估模型和选择阈值时，您选择优先考虑的指标取决于特定问题的成本、收益和风险。在垃圾邮件分类示例中，通常最好优先考虑召回率（抓取所有垃圾邮件）或准确率（尝试确保被标记为垃圾邮件的电子邮件实际上是垃圾邮件），或者在达到某个最低准确性水平的情况下，兼顾这两者。

| 指标 | 指南 |
| --- | --- |
| 准确率 | 作为平衡数据集的模型训练进度/收敛情况的粗略指标。<br>对于模型效果，请仅与其他指标搭配使用。<br>避免使用不平衡的数据集。考虑使用其他指标。 |
| 召回率（真正例率） | 当假负例的代价高于假正例时使用，有病的人不能诊断为健康。 |
| 假正例率 | 当假正例的代价高于假负例时使用，误报很可怕。 |
| 精确率 | 当正例预测的准确性非常重要时，请使用此方法。 |

### F1 得分

F1 得分是精确率和召回率的调和平均数（一种平均值）。

在数学上，它可按下式计算：

$$
F1 = 2 \times \frac{precision \times recall}{precision + recall} = \frac{2 \times TP}{2 \times TP + FP + FN}
$$


此指标可平衡精确率和召回率的重要性，对于类别不平衡的数据集，优先于准确率。当精确率和召回率均为 1.0 的满分时，F1 得分也会为 1.0 的满分。更广泛地说，当精确率和召回率的值接近时，F1 得分也会接近它们的值。当精确率和召回率相差很大时，F1 将与较差的指标相似。

### ROC 曲线和 AUC



