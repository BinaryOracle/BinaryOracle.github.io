---
title: 分割任务中常用的损失函数
icon: file
category:
  - tools
tag:
  - 已发布
footer: 技术共建，知识共享
date: 2025-06-11
author:
  - BinaryOracle
---

`分割任务中常用的损失函数` 

<!-- more -->

# 语义分割

语义分割是计算机视觉领域中的一项任务，旨在将图像中的每个像素分类为不同的语义类别。与对象检测任务不同，语义分割不仅需要识别图像中的物体，还需要对每个像素进行分类，从而实现对图像的细粒度理解和分析。

语义分割可以被看作是像素级别的图像分割，其目标是为图像中的每个像素分配一个特定的语义类别标签。每个像素都被视为图像的基本单位，因此语义分割可以提供更详细和准确的图像分析结果。

***语义分割 vs 分类 :***

1. 在语义分割任务中，由于需要对每个像素进行分类，因此需要使用像素级别的损失函数。

2. 语义分割任务中，图像中各个类别的像素数量通常不均衡，例如背景像素可能占据了大部分。

3. 语义分割任务需要对图像中的每个像素进行分类，同时保持空间连续性。

# 损失函数

## Dice Loss

Dice Loss 是一种常用于语义分割任务的损失函数，尤其在目标区域较小、类别不平衡（class imbalance）的情况下表现优异。它来源于 Dice 系数（Dice Coefficient） ，又称为 Sørensen-Dice 系数 ，是衡量两个样本集合之间重叠程度的一种指标。

Dice 系数衡量的是预测掩码与真实标签之间的相似性，公式如下：

$$
Dice = \frac{2|X \cap Y|}{|X| + |Y|}
$$
 
其中：

- $X$ ：模型预测出的功能区域（如经过 sigmoid 后的概率值）；

- $Y$ ：Ground Truth 掩码（二值化或软标签）；

- $∣X∩Y∣$ ：预测为正类且实际也为正类的部分（交集）；

- $∣X∣+∣Y∣$ ：预测和真实中所有正类区域之和；

> ⚠️ 注意：Dice 系数范围是 [0, 1]，越大越好。 


Dice Loss 为了将其作为损失函数使用，我们通常取其补集：

$$
Dice = 1−Dice
$$

有时也会加入一个平滑项 ϵ 防止除以零：

$$
L_{Dice} = 1 - \frac{2\sum(X \cdot Y) + \epsilon}{\sum X + \sum Y + \epsilon}
$$

Dice Loss 的优势:

| 优势 | 描述 |
| --- | --- |
| 对类别不平衡不敏感,更关注“有没有覆盖正确区域”，而不是“有多少点被正确分类” | 不像 BCE Loss 那样对负样本过多敏感 |
| 直接优化 IoU 的替代指标 | Dice 和 IoU 表现类似，但更易梯度下降 |
| 支持 soft mask 输入 | 可处理连续概率值，不需要先 threshold |
| 更关注整体区域匹配 | 而不是逐点分类 |

代码实现:

```python
class DiceLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        """
        初始化函数，支持加权和平均损失。
        
        参数:
            weight (Tensor): 各类别的权重（可选）
            size_average (bool): 是否对 batch 中的样本取平均 loss
        """
        super(DiceLoss, self).__init__()
        # 该参数未在当前代码中使用，但保留接口以备后续扩展
        self.weight = weight
        # 控制是否对 batch 内 loss 取均值或求和
        self.size_average = size_average

    def forward(self, inputs, targets, smooth=1):
        """
        前向传播函数，计算 Dice Loss。
        
        参数:
            inputs (Tensor): 模型输出的预测值（logits 或 raw output），形状为 [B, N]
            targets (Tensor): 真实标签（ground truth mask），形状为 [B, N]
            smooth (float): 平滑项，防止除零错误，默认为 1
        
        返回:
            dice_loss (Tensor): 计算得到的 Dice Loss
        """

        # 如果你的模型最后没有 sigmoid 层，则需要在这里激活，否则应注释掉这行
        inputs = F.sigmoid(inputs)  # 将 logits 映射到 [0,1] 区间
        
        # 将输入展平成一维张量，便于后续计算
        # inputs: [B*N]
        # targets: [B*N]
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        
        # 计算交集：预测与 GT 的重合部分
        intersection = (inputs * targets).sum()  
        
        # 计算 Dice Coefficient，加入 smooth 防止除以零
        dice_score = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)
        
        # 返回 Dice Loss，用 1 - Dice Coefficient
        # 值越小表示匹配越好
        return 1 - dice_score
```
## BCE-Dice Loss

BCE-Dice Loss是将Dice Loss和标准的二元交叉熵（Binary Cross-Entropy, BCE）损失结合在一起的一种损失函数，通常用于分割模型中。它结合了两种 loss 的优点：

- BCE Loss ：关注每个点的分类误差；
- Dice Loss ：关注整体区域匹配度；

**Binary Cross Entropy Loss（BCE Loss）**

公式（逐点）：

$$
\mathcal{L}_{\text{BCE}}(y, \hat{y}) = - y \log(\hat{y}) - (1 - y)\log(1 - \hat{y})
$$

其中：
- $y \in \{0, 1\}$：真实标签（binary 或 soft mask）；
- $\hat{y} \in [0, 1]$：模型输出的概率值；


特点：

- 对每个点单独计算分类误差；
- 强调预测与 GT 的一致性；
- **在类别平衡时效果好，但在前景远少于背景时容易偏向负样本**；

---

**Dice Loss**

公式（简化版）：

$$
\mathcal{L}_{\text{Dice}} = 1 - \frac{2 \sum \hat{y}_i y_i + \epsilon}{\sum \hat{y}_i + \sum y_i + \epsilon}
$$

其中：

- $\hat{y}_i$：预测概率；
- $y_i$：真实标签；
- $\epsilon$：平滑项，防止除以零；

特点：

- 不依赖绝对数量，而是关注预测和 GT 的交并比；
- **更适合前景极少的小区域识别；**
- 能缓解类别不平衡问题；

---

为什么要把它们结合起来？

| 模型 | 缺陷 | 补充方式 |
|------|------|-----------|
| **BCE Loss** | 对前景响应弱，易受类别不平衡影响 | 加入 Dice Loss 增强区域匹配 |
| **Dice Loss** | 对单个点的分类精度不够敏感 | 加入 BCE Loss 提高逐点判别能力 |

组合后的优势：

| 优势 | 描述 |
|------|------|
| ✔️ 抗类别不平衡能力强 | Dice Loss 起主导作用 |
| ✔️ 对细节更敏感 | BCE Loss 提升边缘识别精度 |
| ✔️ 支持 soft mask 输入 | 可处理连续值掩码 |
| ✔️ 更稳定地收敛 | 两者互补，避免训练震荡 |

---

代码实现:

```python
class DiceBCELoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        """
        初始化函数，构建一个组合损失函数 Dice + BCE。
        
        参数:
            weight (Tensor): 可选参数，用于类别加权；
            size_average (bool): 是否对 batch 内样本取平均 loss（已弃用）；
        """
        super(DiceBCELoss, self).__init__()
        # 这里暂时未使用 weight 和 size_average，保留接口以备扩展

    def forward(self, inputs, targets, smooth=1):
        """
        前向传播函数，计算预测输出与真实标签之间的 Dice Loss 与 BCE Loss 的加权和。
        
        参数:
            inputs (Tensor): 模型输出的 logits 或 raw 分数，形状为 [B, N]
            targets (Tensor): 真实掩码（ground truth mask），形状为 [B, N]
            smooth (float): 平滑项，防止除零错误，默认为 1
        
        返回:
            Dice_BCE (Tensor): Dice + BCE 组合损失值
        """

        # 如果模型最后没有 sigmoid 层，这里需要激活
        # 如果已经包含 sigmoid，则应注释掉这一行
        inputs = F.sigmoid(inputs)  # 将输入映射到概率空间 [0, 1]

        # 将输入和目标展平成一维张量，便于后续计算
        # inputs: [B*N]
        # targets: [B*N]
        inputs = inputs.view(-1)
        targets = targets.view(-1)

        # 计算交集：预测值和真实值都为 1 的区域
        intersection = (inputs * targets).sum()

        # 计算 Dice Loss：
        # Dice Coefficient = (2 * intersection) / (inputs_sum + targets_sum)
        # Dice Loss = 1 - Dice Coefficient
        inputs_sum = inputs.sum()
        targets_sum = targets.sum()
        dice_score = (2. * intersection + smooth) / (inputs_sum + targets_sum + smooth)
        dice_loss = 1 - dice_score

        # 计算 Binary Cross Entropy Loss（BCE）
        # 注意：F.binary_cross_entropy 默认要求 inputs 已经经过 sigmoid
        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')

        # 组合损失：BCE + Dice Loss
        Dice_BCE = BCE + dice_loss

        return Dice_BCE
```

## Jaccard/Intersection over Union (IoU) Loss



## Focal Loss


## Tversky Loss



## Lovasz Hinge Loss



## Combo Loss



