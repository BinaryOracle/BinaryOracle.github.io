---
title: 分割任务中常用的损失函数
icon: file
category:
  - tools
tag:
  - 已发布
footer: 技术共建，知识共享
date: 2025-06-11
author:
  - BinaryOracle
---

`分割任务中常用的损失函数` 

<!-- more -->

# 语义分割

语义分割是计算机视觉领域中的一项任务，旨在将图像中的每个像素分类为不同的语义类别。与对象检测任务不同，语义分割不仅需要识别图像中的物体，还需要对每个像素进行分类，从而实现对图像的细粒度理解和分析。

语义分割可以被看作是像素级别的图像分割，其目标是为图像中的每个像素分配一个特定的语义类别标签。每个像素都被视为图像的基本单位，因此语义分割可以提供更详细和准确的图像分析结果。

***语义分割 vs 分类 :***

1. 在语义分割任务中，由于需要对每个像素进行分类，因此需要使用像素级别的损失函数。

2. 语义分割任务中，图像中各个类别的像素数量通常不均衡，例如背景像素可能占据了大部分。

3. 语义分割任务需要对图像中的每个像素进行分类，同时保持空间连续性。

# 损失函数

## Dice Loss

Dice Loss 是一种常用于语义分割任务的损失函数，尤其在目标区域较小、类别不平衡（class imbalance）的情况下表现优异。它来源于 Dice 系数（Dice Coefficient） ，又称为 Sørensen-Dice 系数 ，是衡量两个样本集合之间重叠程度的一种指标。

Dice 系数衡量的是预测掩码与真实标签之间的相似性，公式如下：

$$
Dice = \frac{2|X \cap Y|}{|X| + |Y|}
$$
 
其中：

- $X$ ：模型预测出的功能区域（如经过 sigmoid 后的概率值）；

- $Y$ ：Ground Truth 掩码（二值化或软标签）；

- $∣X∩Y∣$ ：预测为正类且实际也为正类的部分（交集）；

- $∣X∣+∣Y∣$ ：预测和真实中所有正类区域之和；

> ⚠️ 注意：Dice 系数范围是 [0, 1]，越大越好。 


Dice Loss 为了将其作为损失函数使用，我们通常取其补集：

$$
Dice = 1−Dice
$$

有时也会加入一个平滑项 ϵ 防止除以零：

$$
L_{Dice} = 1 - \frac{2\sum(X \cdot Y) + \epsilon}{\sum X + \sum Y + \epsilon}
$$

Dice Loss 的优势:

| 优势 | 描述 |
| --- | --- |
| 对类别不平衡不敏感,更关注“有没有覆盖正确区域”，而不是“有多少点被正确分类” | 不像 BCE Loss 那样对负样本过多敏感 |
| 直接优化 IoU 的替代指标 | Dice 和 IoU 表现类似，但更易梯度下降 |
| 支持 soft mask 输入 | 可处理连续概率值，不需要先 threshold |
| 更关注整体区域匹配 | 而不是逐点分类 |

代码实现:

```python
class DiceLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        """
        初始化函数，支持加权和平均损失。
        
        参数:
            weight (Tensor): 各类别的权重（可选）
            size_average (bool): 是否对 batch 中的样本取平均 loss
        """
        super(DiceLoss, self).__init__()
        # 该参数未在当前代码中使用，但保留接口以备后续扩展
        self.weight = weight
        # 控制是否对 batch 内 loss 取均值或求和
        self.size_average = size_average

    def forward(self, inputs, targets, smooth=1):
        """
        前向传播函数，计算 Dice Loss。
        
        参数:
            inputs (Tensor): 模型输出的预测值（logits 或 raw output），形状为 [B, N]
            targets (Tensor): 真实标签（ground truth mask），形状为 [B, N]
            smooth (float): 平滑项，防止除零错误，默认为 1
        
        返回:
            dice_loss (Tensor): 计算得到的 Dice Loss
        """

        # 如果你的模型最后没有 sigmoid 层，则需要在这里激活，否则应注释掉这行
        inputs = F.sigmoid(inputs)  # 将 logits 映射到 [0,1] 区间
        
        # 将输入展平成一维张量，便于后续计算
        # inputs: [B*N]
        # targets: [B*N]
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        
        # 计算交集：预测与 GT 的重合部分
        intersection = (inputs * targets).sum()  
        
        # 计算 Dice Coefficient，加入 smooth 防止除以零
        dice_score = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)
        
        # 返回 Dice Loss，用 1 - Dice Coefficient
        # 值越小表示匹配越好
        return 1 - dice_score
```
## BCE-Dice Loss

