---
title: APIè®°å½•ä¹‹Pytorchç¯‡
icon: file
category:
  - tools
tag:
  - å·²å‘å¸ƒ
footer: æŠ€æœ¯å…±å»ºï¼ŒçŸ¥è¯†å…±äº«
date: 2025-06-11
author:
  - BinaryOracle
---

`APIè®°å½•ä¹‹Pytorchç¯‡` 

<!-- more -->

## Pytorch

### stack

`torch.stack()` æ˜¯ PyTorch ä¸­ç”¨äºå°†å¤šä¸ªå½¢çŠ¶ç›¸åŒçš„å¼ é‡æ²¿ä¸€ä¸ªæ–°ç»´åº¦æ‹¼æ¥çš„å‡½æ•°ã€‚

```python
torch.stack(tensors, dim=0, *, out=None)
```
- tensorsï¼šä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ï¼ˆå¦‚åˆ—è¡¨ã€å…ƒç»„ï¼‰ï¼Œå…¶ä¸­åŒ…å«å¤šä¸ªå½¢çŠ¶ç›¸åŒçš„ Tensorã€‚

- dimï¼šæ’å…¥æ–°ç»´åº¦çš„ä½ç½®ï¼ˆé»˜è®¤æ˜¯ 0ï¼‰ã€‚è¿™ä¸ªæ–°ç»´åº¦å°±æ˜¯æ‹¼æ¥çš„é‚£ä¸€ç»´ã€‚

- outï¼šå¯é€‰è¾“å‡ºå¼ é‡ï¼Œç”¨äºå†™å…¥ç»“æœã€‚

![](API/1.png)

ä¾‹å­å¦‚ä¸‹:

![](API/2.png)

æ³¨æ„:

- æ‰€æœ‰å¼ é‡å¿…é¡»å…·æœ‰å®Œå…¨ç›¸åŒçš„ shapeã€‚

- å¦‚æœä½ æƒ³æŠŠä¸€ä¸ª batch ä¸­çš„å¤šä¸ªæ ·æœ¬æ‰“åŒ…æˆä¸€ä¸ªå¤§ tensorï¼Œé€šå¸¸ä¼šç”¨ torch.stack()ã€‚

### transpose

```python
y = x.transpose(dim0, dim1)
```

åªäº¤æ¢`ä¸¤ä¸ªæŒ‡å®šç»´åº¦`ï¼Œå¸¸ç”¨äº 2D æˆ– 3D å¼ é‡ï¼Œå¦‚å›¾åƒè½¬ç½®ã€RNN è¾“å…¥è°ƒæ•´ç­‰ã€‚

### permute

```python
y = x.permute(dims)
```
å¯ä»¥ä»»æ„é‡æ–°æ’åˆ—æ‰€æœ‰ç»´åº¦ï¼Œæ˜¯ transpose çš„æ³›åŒ–ï¼Œæ”¯æŒå¤šç»´åº¦åŒæ—¶äº¤æ¢ã€‚

> transpose() å’Œ permute() è¿”å›çš„å¼ é‡è™½ç„¶æ˜¯è§†å›¾ï¼ˆviewï¼‰ï¼Œä½†å®ƒä»¬çš„ å†…å­˜å¸ƒå±€ï¼ˆstridesï¼‰è¢«æ”¹å˜ã€‚å¦‚æœä½ æ¥ä¸‹æ¥è¦å¯¹å®ƒä»¬æ‰§è¡Œ .view() æˆ–æŸäº›è¦æ±‚å†…å­˜è¿ç»­çš„æ“ä½œï¼Œå°±å¿…é¡»å…ˆè°ƒç”¨ .contiguous()ã€‚

![](API/4.png)
 
æ‰§è¡Œ transpose(0, 2) å:

![](API/5.png)

### view

view: åœ¨ä¸å¤åˆ¶æ•°æ®çš„å‰æä¸‹ï¼Œè¿”å›å…·æœ‰æ–°å½¢çŠ¶ï¼ˆshapeï¼‰çš„å¼ é‡è§†å›¾ï¼ˆviewï¼‰ã€‚

```python
new_tensor = x.view(shape)
```
.view() åªé€‚ç”¨äºè¿ç»­å†…å­˜çš„å¼ é‡ï¼ŒæŸäº›æ“ä½œï¼ˆå¦‚ permute, transposeï¼‰ä¼šæ”¹å˜å¼ é‡çš„ strideï¼ˆå†…å­˜æ­¥é•¿ï¼‰ï¼Œä½¿å…¶å˜å¾— éè¿ç»­ã€‚æ­¤æ—¶å¿…é¡»å…ˆ .contiguous() å† .view()ï¼š

```python
x = torch.randn(2, 3, 4)
y = x.permute(0, 2, 1)          # æ”¹å˜ç»´åº¦é¡ºåº
z = y.contiguous().view(2, -1)  # å¦åˆ™å¯èƒ½æŠ¥é”™
```
> .view() ä¸ä¼šå¤åˆ¶æ•°æ®ï¼Œæ˜¯åŸå¼ é‡çš„ä¸€ä¸ªè§†å›¾ï¼ˆå…±äº«å†…å­˜ï¼‰

### reshape

reshape: è¿”å›å…·æœ‰æ–°å½¢çŠ¶çš„å¼ é‡ã€‚å¿…è¦æ—¶ä¼šå¤åˆ¶æ•°æ®ï¼Œå¦åˆ™è¿”å›è§†å›¾ã€‚ ç›¸æ¯” .view()ï¼Œreshape() ä¸è¦æ±‚åŸå§‹å¼ é‡æ˜¯è¿ç»­çš„ï¼Œè¿™æ˜¯å®ƒæœ€å¤§çš„ä¼˜åŠ¿ã€‚

```python
new_tensor = x.reshape(shape)
```
åœ¨ PyTorch ä¸­ï¼Œ`reshape()` åœ¨å¤šæ•°æƒ…å†µä¸‹ä¼šè¿”å›åŸå¼ é‡çš„è§†å›¾ï¼ˆä¸å¤åˆ¶æ•°æ®ï¼‰ï¼Œä½†**å½“å¼ é‡çš„å†…å­˜å¸ƒå±€ä¸è¿ç»­**ï¼ˆä¾‹å¦‚ç»è¿‡äº† `permute()`ã€`transpose()` ç­‰æ“ä½œï¼‰ï¼Œæˆ–æ–°å½¢çŠ¶æ— æ³•ä¸åŸå†…å­˜å¸ƒå±€å…¼å®¹æ—¶ï¼Œ`reshape()` å°±ä¼šè¿›è¡Œæ•°æ®å¤åˆ¶ä»¥åˆ›å»ºæ–°çš„å¼ é‡ã€‚æ­¤å¤–ï¼Œå¦‚æœå¼ é‡æ¥æºäº `expand()`ï¼ˆå¹¿æ’­è§†å›¾ï¼‰ï¼Œæˆ–è€…è·¨è®¾å¤‡/ç‰¹æ®Šæ“ä½œåçš„ä¸­é—´ç»“æœï¼Œä¹Ÿå¯èƒ½è§¦å‘å¤åˆ¶ã€‚å› æ­¤ï¼Œè‹¥å¸Œæœ›ç¡®ä¿å†…å­˜æ•ˆç‡ï¼Œå»ºè®®åœ¨ reshape å‰ä½¿ç”¨ `.is_contiguous()` æ£€æŸ¥ï¼Œå¿…è¦æ—¶ç”¨ `.contiguous()` è½¬ä¸ºè¿ç»­å¼ é‡ã€‚

### repeat

tensor.repeat() æ˜¯ PyTorch ä¸­ç”¨äºæ²¿æŒ‡å®šç»´åº¦é‡å¤å¼ é‡å†…å®¹çš„æ“ä½œï¼Œå®ƒä¼šå¤åˆ¶æ•°æ®ï¼Œä»è€Œæ‰©å±•å¼ é‡çš„å½¢çŠ¶ï¼ˆä¸æ˜¯è§†å›¾ï¼‰ã€‚

```python
repeated_tensor = x.repeat(repeat_1, repeat_2, ..., repeat_n)
```

- å‚æ•°ä¸ªæ•°å¿…é¡»å’Œ x çš„ç»´åº¦æ•°ç›¸åŒã€‚

- æ¯ä¸ª repeat_i è¡¨ç¤ºè¯¥ç»´åº¦ä¸Šå¤åˆ¶çš„æ¬¡æ•°ã€‚

```python
import torch

x = torch.tensor([[1, 2], [3, 4]])
x = x.repeat(2,3)
print(x)

output:

tensor([[1, 2, 1, 2, 1, 2],
        [3, 4, 3, 4, 3, 4],
        [1, 2, 1, 2, 1, 2],
        [3, 4, 3, 4, 3, 4]])
```
### expand

tensor.expand() æ˜¯ PyTorch ä¸­ç”¨äºæ‰©å±•å¼ é‡å°ºå¯¸ä½†ä¸å¤åˆ¶æ•°æ®çš„ä¸€ç§é«˜æ•ˆæ–¹æ³•ï¼Œå®ƒé€šè¿‡å¹¿æ’­ï¼ˆbroadcastingï¼‰æœºåˆ¶ç”Ÿæˆæ–°çš„è§†å›¾ï¼ŒèŠ‚çœå†…å­˜ã€‚

```python
expanded_tensor = x.expand(size_1, size_2, ..., size_n)
```

- å‚æ•°ä¸ªæ•°å¿…é¡»å’Œ x.dim() ç›¸åŒï¼Œæˆ–å¯ä»¥é€šè¿‡åœ¨å‰é¢æ·»åŠ ç»´åº¦æ¥è‡ªåŠ¨å¹¿æ’­ã€‚

- æŸä¸€ç»´å¦‚æœæ˜¯ -1ï¼Œè¡¨ç¤ºä¿æŒåŸæ¥çš„å¤§å°ã€‚

```python
x = torch.tensor([[1], [2], [3]])  # shape: [3, 1]

x.expand(3, 4)
# â†’ æ¯è¡Œå¤åˆ¶ 4 æ¬¡ï¼Œä½†ä¸å ç”¨é¢å¤–å†…å­˜
# tensor([[1, 1, 1, 1],
#         [2, 2, 2, 2],
#         [3, 3, 3, 3]])
```
ä½¿ç”¨ -1 ä¿ç•™ç»´åº¦ï¼š

```python
x = torch.randn(3, 1, 5)  # shape: [3, 1, 5]
x.expand(-1, 4, -1)       # shape â†’ [3, 4, 5]
```
æ ¸å¿ƒåŸåˆ™ï¼šåªæœ‰åŸå§‹ç»´åº¦ = 1 çš„ä½ç½®ï¼Œæ‰èƒ½é€šè¿‡ expand å˜å¤§ï¼›å…¶ä»–ä½ç½®å¿…é¡» ç›¸ç­‰ã€‚

```python
x = torch.tensor([[1, 2, 3]])  # shape: [1, 3]
y = x.expand(2, 3)  # âœ… ç¬¬ 0 ç»´æ˜¯ 1 â†’ å¯ä»¥æ‰©å±•æˆ 2
                    # âŒ ç¬¬ 1 ç»´æ˜¯ 3 â†’ ç›®æ ‡ä»æ˜¯ 3ï¼Œè™½ç„¶æ²¡å˜ï¼Œä½†ä¹Ÿä¸èƒ½å†™æˆ 6ï¼
x.expand(2, 6)  # âŒ æŠ¥é”™ï¼å› ä¸ºç¬¬ 1 ç»´æ˜¯ 3ï¼Œä¸èƒ½å˜æˆ 6
```

| ç‰¹æ€§        | `.expand()`                | `.repeat()`        |
| --------- | -------------------------- | ------------------ |
| æ˜¯å¦å¤åˆ¶æ•°æ®    | âŒ å¦ï¼ˆè¿”å›è§†å›¾ï¼ŒèŠ‚çœå†…å­˜ï¼‰             | âœ… æ˜¯ï¼ˆåˆ›å»ºæ–°å¼ é‡ï¼Œå¼€é”€å¤§ï¼‰     |
| æ˜¯å¦æ”¯æŒå¹¿æ’­    | âœ… æ”¯æŒï¼ˆè‡ªåŠ¨æŒ‰ç»´åº¦æ‰©å±•ï¼‰              | âŒ ä¸æ”¯æŒï¼Œå¿…é¡»ç²¾ç¡®æŒ‡å®šæ¯ç»´é‡å¤æ¬¡æ•° |
| æ˜¯å¦å¯ç”¨äºæ”¹å˜ç»´åº¦ | âŒ å¦ï¼ˆç»´åº¦å¿…é¡»å…¼å®¹ï¼‰                | âœ… æ˜¯                |
| å¸¸ç”¨äº       | é«˜æ•ˆå¹¿æ’­ï¼Œå¦‚ attentionã€masking ç­‰ | å®é™…å¤åˆ¶ï¼Œå¦‚æ„é€ é‡å¤è¾“å…¥       |

### @torch.no_grad()

1. åœ¨è¿™ä¸ªè£…é¥°å™¨ä¿®é¥°çš„å‡½æ•°å†…ï¼ŒPyTorch ä¸ä¼šè·Ÿè¸ªè®¡ç®—å›¾ï¼Œä¹Ÿä¸ä¼šè®¡ç®—æ¢¯åº¦ã€‚

2. è¿™æ ·å¯ä»¥å‡å°‘å†…å­˜ä½¿ç”¨å’Œè®¡ç®—å¼€é”€ï¼Œå› ä¸ºä¸éœ€è¦ä¿å­˜ä¸­é—´å˜é‡ç”¨äºåå‘ä¼ æ’­ã€‚

3. é€‚ç”¨äºåªéœ€è¦å‰å‘æ¨ç†ä¸”ä¸éœ€è¦æ›´æ–°æ¨¡å‹å‚æ•°çš„åœºæ™¯ã€‚

### register_buffer

```python
# nn.Module ç±»ä¸­æä¾›çš„æ–¹æ³•
register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True)
```      
1. name (str)

    * ç¼“å†²åŒºçš„åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰ã€‚

    * ä¹‹åå¯ä»¥ç”¨ model.name è®¿é—®ï¼Œæ¯”å¦‚ model.queueã€‚

2. tensor (torch.Tensor æˆ– None)
    
    * è¦æ³¨å†Œçš„å¼ é‡ã€‚
    
    * è¿™ä¸ªå¼ é‡ä¼šæˆä¸ºæ¨¡å‹çš„ä¸€ä¸ªæˆå‘˜ï¼Œä½†ä¸ä¼šè¢«è§†ä¸ºå¯è®­ç»ƒå‚æ•°ã€‚
    
    * ä¹Ÿå¯ä»¥ä¼  Noneï¼Œè¡¨ç¤ºå…ˆå ä½ï¼Œåé¢å†èµ‹å€¼ã€‚

3. persistent (boolï¼Œé»˜è®¤ Trueï¼ŒPyTorch 1.8ä»¥åæ”¯æŒ)

    * å¦‚æœä¸º Trueï¼Œè¯¥ç¼“å†²åŒºä¼šåŒ…å«åœ¨ state_dict() ä¸­ï¼Œå³ä¼šè¢«ä¿å­˜å’ŒåŠ è½½ã€‚

    * å¦‚æœä¸º Falseï¼Œç¼“å†²åŒºä¸ä¼šä¿å­˜åˆ° state_dict()ï¼Œå¸¸ç”¨äºä¸´æ—¶ç¼“å­˜æ•°æ®ã€‚  

**register_bufferçš„ä½œç”¨å’Œæ„ä¹‰**ï¼š

* å®ƒä¼šæŠŠä¸€ä¸ªå¼ é‡ï¼ˆtensorï¼‰ä½œä¸ºæ¨¡å‹çš„ç¼“å†²åŒºæ³¨å†Œï¼Œä¸ä¼šè¢«å½“ä½œæ¨¡å‹çš„å¯è®­ç»ƒå‚æ•°ï¼ˆä¸ä¼šå‡ºç°åœ¨model.parameters()é‡Œï¼Œä¹Ÿä¸ä¼šå‚ä¸æ¢¯åº¦è®¡ç®—æˆ–ä¼˜åŒ–ï¼‰ã€‚

* ä½†æ˜¯ï¼Œç¼“å†²åŒºä¼šè¢«è‡ªåŠ¨ä¿å­˜åˆ°æ¨¡å‹çš„çŠ¶æ€å­—å…¸ï¼ˆstate_dictï¼‰ä¸­ï¼Œä¹Ÿä¼šè¢«åŠ è½½ï¼ˆloadï¼‰å’Œä¿å­˜ï¼ˆsaveï¼‰ã€‚

* å¸¸ç”¨äºä¿å­˜ä¸€äº›æ¨¡å‹çš„çŠ¶æ€ä¿¡æ¯ï¼Œä½†è¿™äº›ä¿¡æ¯ä¸éœ€è¦è®­ç»ƒï¼Œæ¯”å¦‚ï¼šå‡å€¼ã€æ–¹å·®ã€é˜Ÿåˆ—ã€æ©ç ç­‰ã€‚

### einsum

`einsum` æ˜¯ **çˆ±å› æ–¯å¦æ±‚å’Œçº¦å®šï¼ˆEinstein Summationï¼‰** çš„ç®€å†™ï¼Œæ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§ä¸”ç›´è§‚çš„å¼ é‡æ“ä½œå·¥å…·ã€‚

ç›¸æ¯” `matmul`ã€`bmm`ã€`torch.matmul` è¿™ç±» APIï¼Œ`einsum` è®©ä½ **æ˜¾å¼æŒ‡å®šç»´åº¦ä¹‹é—´æ€ä¹ˆç›¸ä¹˜/æ±‚å’Œ/ä¿ç•™**ã€‚

```python
torch.einsum("ç»´åº¦è§„åˆ™", [tensor1, tensor2, ...])
```

* å¼•å·ä¸­æ˜¯ **å¯¹æ¯ä¸ª tensor çš„ç»´åº¦å‘½å**

* ç›¸åŒçš„ç»´åº¦å­—æ¯è¡¨ç¤ºè¦åš **ç‚¹ç§¯/æ±‚å’Œ**

* æ²¡æœ‰é‡å¤çš„ç»´åº¦å­—æ¯è¡¨ç¤ºä¿ç•™è¯¥ç»´åº¦


| einsum è¡¨è¾¾å¼    | ç­‰ä»·æ“ä½œ                     | è¾“å‡ºå½¢çŠ¶   | å«ä¹‰                  |
| ------------- | ------------------------ | ------ | ------------------- |
| `"nc,nc->n"`  | `(q * k).sum(dim=1)`     | (N,)   | æ¯ä¸ª query ä¸å…¶æ­£æ ·æœ¬çš„ç‚¹ç§¯   |
| `"nc,ck->nk"` | `torch.matmul(q, queue)` | (N, K) | æ¯ä¸ª query ä¸æ‰€æœ‰è´Ÿæ ·æœ¬çš„ç›¸ä¼¼åº¦ |

### where

```python
torch.where(condition, x, y)
```

* `condition`ï¼šä¸€ä¸ªå¸ƒå°”å‹å¼ é‡ï¼Œç”¨æ¥åˆ¤æ–­æ¡ä»¶æ˜¯å¦æˆç«‹ã€‚

* è¿”å›ä¸€ä¸ªæ–°å¼ é‡ï¼š

  * **å½“ `condition` å¯¹åº”ä½ç½®ä¸º True æ—¶ï¼Œå– `x` ä¸­å¯¹åº”ä½ç½®çš„å…ƒç´ **ï¼›

  * **å½“ `condition` å¯¹åº”ä½ç½®ä¸º False æ—¶ï¼Œå– `y` ä¸­å¯¹åº”ä½ç½®çš„å…ƒç´ **ã€‚

### torch.nn.functional.pad

```python
text = F.pad(text, (1, 0), value=0)
```
- textï¼šå¾…å¡«å……çš„å¼ é‡ï¼Œæ¯”å¦‚å½¢çŠ¶æ˜¯ (batch_size, seq_len)ã€‚

- (1, 0)ï¼šæŒ‡å®šå¡«å……çš„æ–¹å¼ï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ªé•¿åº¦ä¸º2çš„å…ƒç»„ (padding_left, padding_right)ï¼Œè¡¨ç¤ºåœ¨æœ€åä¸€ä¸ªç»´åº¦çš„å·¦ä¾§å¡«å……1ä¸ªå…ƒç´ ï¼Œå³ä¾§å¡«å……0ä¸ªå…ƒç´ ã€‚

- value=0ï¼šç”¨æ¥å¡«å……çš„æ•°å€¼ï¼Œè¿™é‡Œæ˜¯ç”¨0å¡«å……ã€‚

```python
    x = torch.tensor([1, 2, 3, 4, 5])
    print("Original tensor:", x)

    # åœ¨æœ€åä¸€ä¸ªç»´åº¦å·¦è¾¹å¡«å……1ä¸ª0ï¼Œå³è¾¹ä¸å¡«å……
    padded_1 = F.pad(x, (1, 0), value=0)
    print("Pad (1, 0):", padded_1)

    # åœ¨æœ€åä¸€ä¸ªç»´åº¦å·¦è¾¹ä¸å¡«å……ï¼Œå³è¾¹å¡«å……2ä¸ª9
    padded_2 = F.pad(x, (0, 2), value=9)
    print("Pad (0, 2) with 9:", padded_2)

    # åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸¤è¾¹å„å¡«å……2ä¸ª-1
    padded_3 = F.pad(x, (2, 2), value=-1)
    print("Pad (2, 2) with -1:", padded_3)
```

output:

```python
Original tensor: tensor([1, 2, 3, 4, 5])
Pad (1, 0): tensor([0, 1, 2, 3, 4, 5])
Pad (0, 2) with 9: tensor([1, 2, 3, 4, 5, 9, 9])
Pad (2, 2) with -1: tensor([-1, -1,  1,  2,  3,  4,  5, -1, -1])
```

### rearrange

rearrange æ˜¯ä¸€ä¸ªæ¥è‡ª einopsï¼ˆEinstein Operationsï¼‰åº“çš„å‡½æ•°ï¼Œç”¨äºå¯¹å¼ é‡ï¼ˆTensorï¼‰è¿›è¡Œçµæ´»ã€ç›´è§‚çš„é‡æ’ã€ç»´åº¦å˜æ¢ã€è½¬ç½®ã€æ‰©å±•ç­‰æ“ä½œã€‚

```python
from einops import rearrange

output = rearrange(tensor, pattern)
```

- tensor æ˜¯è¾“å…¥å¼ é‡ã€‚

- pattern æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæè¿°è¾“å…¥å’Œè¾“å‡ºç»´åº¦çš„å¯¹åº”å…³ç³»ï¼Œç±»ä¼¼æ¨¡å¼åŒ¹é…ã€‚

```python
rearrange(x, 'b c h w -> b h w c')  # äº¤æ¢ç»´åº¦é¡ºåº

x = torch.randn(4)  # shape (4,)
y = rearrange(x, 'b -> b 1')  # å˜æˆ (4,1)ï¼Œå¢åŠ ä¸€ä¸ªç»´åº¦

x = torch.randn(2, 3, 4)
y = rearrange(x, 'b c d -> b (c d)')  # æŠŠcå’Œdåˆå¹¶æˆä¸€ä¸ªç»´åº¦

x = torch.randn(2, 12)
y = rearrange(x, 'b (c d) -> b c d', c=3)  # æŠŠ12æ‹†åˆ†æˆ3å’Œ4
```

### Tensor.uniform_

```python
Tensor.uniform_(from=0, to=1)
```
1. æŠŠä¸€ä¸ª å·²æœ‰çš„å¼ é‡ï¼Œç”¨ å‡åŒ€åˆ†å¸ƒéšæœºæ•°å¡«å……ã€‚

2. ç”Ÿæˆçš„å€¼åœ¨ [from, to) èŒƒå›´å†…ï¼Œé»˜è®¤æ˜¯ [0, 1)ã€‚

3. åŠ ä¸Š _ è¯´æ˜æ˜¯åŸåœ°ä¿®æ”¹ï¼šç›´æ¥åœ¨åŸå¼ é‡ä¸Šè¿›è¡Œæ“ä½œï¼Œä¸åˆ›å»ºæ–°å¼ é‡ã€‚

### torch.unique_consecutive

**ä½œç”¨**ï¼šè¿”å›è¾“å…¥å¼ é‡ä¸­ **è¿ç»­ä¸é‡å¤çš„å…ƒç´ **ï¼Œç±»ä¼¼äº NumPy çš„ `np.unique`ï¼Œä½†å®ƒåªå»æ‰ **ç›¸é‚»é‡å¤å€¼**ï¼Œè€Œä¸æ˜¯å…¨å±€å»é‡ã€‚

```python
torch.unique_consecutive(
    input,
    return_inverse=False,
    return_counts=False,
    dim=None
) -> (Tensor, Optional[Tensor], Optional[Tensor])
```

* **`input`**ï¼šè¾“å…¥å¼ é‡ã€‚

* **`return_inverse`**ï¼šå¦‚æœä¸º `True`ï¼Œä¼šé¢å¤–è¿”å›ä¸€ä¸ªå¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªå…ƒç´ åœ¨å”¯ä¸€å€¼å¼ é‡ä¸­çš„ç´¢å¼•ã€‚

* **`return_counts`**ï¼šå¦‚æœä¸º `True`ï¼Œä¼šé¢å¤–è¿”å›æ¯ä¸ªå”¯ä¸€å€¼çš„ **è¿ç»­å‡ºç°æ¬¡æ•°**ã€‚

* **`dim`**ï¼šæŒ‡å®šæ“ä½œçš„ç»´åº¦ã€‚å¦‚æœä¸º `None`ï¼Œé»˜è®¤ä¼šå±•å¹³ä¸º 1D å¤„ç†ã€‚


**ç¤ºä¾‹1**:

```python
import torch

x = torch.tensor([1, 1, 2, 2, 3, 1, 1])
out = torch.unique_consecutive(x)
print(out)  
# tensor([1, 2, 3, 1])
```

> è¿™é‡Œæ²¡æœ‰å»æ‰æœ€åé‚£ä¸ª `1`ï¼Œå› ä¸ºå®ƒå’Œå‰é¢çš„ `3` ä¸ç›¸é‚»ã€‚

**ç¤ºä¾‹2ï¼šè¿”å›è®¡æ•°**: 

```python
out, counts = torch.unique_consecutive(x, return_counts=True)
print(out)     # tensor([1, 2, 3, 1])
print(counts)  # tensor([2, 2, 1, 2])
```

**ç¤ºä¾‹3: è¿”å›åå‘ç´¢å¼•**:

```python
out, inverse = torch.unique_consecutive(x, return_inverse=True)
print(out)      # tensor([1, 2, 3, 1])
print(inverse)  # tensor([0, 0, 1, 1, 2, 3, 3])
```

**ç¤ºä¾‹4: æŒ‡å®šç»´åº¦**:

```python
x = torch.tensor([[1, 1, 2],
                  [1, 2, 2],
                  [3, 3, 3]])
out = torch.unique_consecutive(x, dim=0)
print(out)
# tensor([[1, 1, 2],
#         [1, 2, 2],
#         [3, 3, 3]])
```

> è¿™é‡ŒæŒ‰ **è¡Œ** å»é‡ï¼Œåªè¦ç›¸é‚»ä¸¤è¡Œå®Œå…¨ç›¸åŒå°±ä¼šåˆå¹¶ã€‚

### torch.cumsum

**ä½œç”¨**ï¼šå¯¹å¼ é‡æ²¿æŒ‡å®šç»´åº¦åš **ç´¯åŠ æ±‚å’Œ**ï¼ˆcumulative sumï¼‰ï¼Œè¿”å›ä¸€ä¸ªæ–°çš„å¼ é‡ã€‚

```python
torch.cumsum(input, dim, *, dtype=None, out=None) -> Tensor
```

* **`input`**: è¾“å…¥å¼ é‡

* **`dim`**: æ²¿ç€å“ªä¸ªç»´åº¦è®¡ç®—ç´¯ç§¯å’Œ

* **`dtype`**: æŒ‡å®šè¾“å‡ºæ•°æ®ç±»å‹ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸æŒ‡å®šå°±ä¿æŒè¾“å…¥ dtype

* **`out`**: è¾“å‡ºå¼ é‡ï¼ˆå¯é€‰ï¼‰

**è¿”å›å€¼**: è¿”å›ä¸€ä¸ªå’Œ `input` å½¢çŠ¶ç›¸åŒçš„å¼ é‡ï¼Œå…ƒç´ æ˜¯æŒ‰ `dim` ç´¯åŠ åçš„å€¼ã€‚

**ç¤ºä¾‹1: ä¸€ç»´å¼ é‡**

```python
import torch
x = torch.tensor([1, 2, 3, 4])
y = torch.cumsum(x, dim=0)
print(y)  
# tensor([ 1,  3,  6, 10])
```

**ç¤ºä¾‹2: äºŒç»´å¼ é‡**

```python
x = torch.tensor([[1, 2, 3],
                  [4, 5, 6]])
y = torch.cumsum(x, dim=0)  # æ²¿ç€è¡Œæ–¹å‘
print(y)
# tensor([[ 1,  2,  3],
#         [ 5,  7,  9]])
```

### torch.Tensor çš„ chunk æ–¹æ³•

ä½œç”¨:  ç”¨äºå°†å¼ é‡æ²¿æŒ‡å®šç»´åº¦ **åˆ†å—**ï¼ŒåŸºæœ¬ç”¨æ³•å¦‚ä¸‹ï¼š

```python
import torch

x = torch.arange(8)  # [0,1,2,3,4,5,6,7]

# å°†å¼ é‡æ²¿ dim=0 å¹³å‡åˆ†æˆ 4 å—
chunks = x.chunk(4, dim=0)
for c in chunks:
    print(c)
```

è¾“å‡ºï¼š

```
tensor([0, 1])
tensor([2, 3])
tensor([4, 5])
tensor([6, 7])
```

**å‚æ•°è¯´æ˜**:

* `chunks`ï¼šè¦åˆ†æˆçš„å—æ•°

* `dim`ï¼šæ²¿å“ªä¸ªç»´åº¦åˆ†å—ï¼Œé»˜è®¤ `dim=0`

* è¿”å›å€¼ï¼šä¸€ä¸ª **tuple**ï¼ŒåŒ…å«åˆ‡åˆ†åçš„å¼ é‡å—

> * å¦‚æœå¼ é‡ä¸èƒ½æ•´é™¤å—æ•°ï¼Œå‰å‡ ä¸ªå—ä¼šæ¯”åé¢çš„å¤šä¸€ä¸ªå…ƒç´ ã€‚
> 
> * è¿”å›çš„æ˜¯ tuple è€Œä¸æ˜¯ listã€‚

ä¸¾ä¸ªäºŒç»´ä¾‹å­ï¼š

```python
x = torch.arange(16).view(4, 4)
chunks = x.chunk(2, dim=0)  # æŒ‰è¡Œåˆ†æˆ2å—
for c in chunks:
    print(c)
```

è¾“å‡ºï¼š

```
tensor([[0, 1, 2, 3],
        [4, 5, 6, 7]])
tensor([[ 8,  9, 10, 11],
        [12, 13, 14, 15]])
```

### torch.randperm

`torch.randperm(n)` è¿”å›ä¸€ä¸ªé•¿åº¦ä¸º `n` çš„ä¸€ç»´å¼ é‡ï¼ŒåŒ…å« `0 ~ n-1` çš„æ•´æ•°ï¼Œé¡ºåºè¢«éšæœºæ‰“ä¹±ã€‚å¸¸ç”¨äºéšæœºæ‰“ä¹±ç´¢å¼•ï¼Œä¾‹å¦‚ï¼š

```python
idx = torch.randperm(5)
# å¯èƒ½è¾“å‡º: tensor([3, 0, 4, 1, 2])
```

### torch.randint

`torch.randint(low, high, size)` è¿”å›åœ¨ `[low, high)` åŒºé—´å†…éšæœºç”Ÿæˆæ•´æ•°çš„å¼ é‡ï¼Œå½¢çŠ¶ç”± `size` æŒ‡å®šã€‚ç¤ºä¾‹ï¼š

```python
x = torch.randint(0, 10, (3, 2))
# å¯èƒ½è¾“å‡º: tensor([[7, 1],
#                   [3, 9],
#                   [0, 4]])
```

### torch.bincount

`torch.bincount(input, weights=None, minlength=0)` ç”¨äºç»Ÿè®¡ **éè´Ÿæ•´æ•°å¼ é‡** `input` ä¸­æ¯ä¸ªæ•´æ•°å‡ºç°çš„æ¬¡æ•°ï¼Œè¿”å›ä¸€ä¸ªä¸€ç»´å¼ é‡ã€‚

**å‚æ•°**:

* `input`ï¼šéè´Ÿæ•´æ•°å¼ é‡ï¼Œä¸€ç»´ã€‚

* `weights`ï¼ˆå¯é€‰ï¼‰ï¼šä¸ `input` åŒé•¿åº¦çš„æµ®ç‚¹å¼ é‡ï¼Œç”¨äºåŠ æƒè®¡æ•°ã€‚

* `minlength`ï¼ˆå¯é€‰ï¼‰ï¼šè¾“å‡ºå¼ é‡çš„æœ€å°é•¿åº¦ï¼Œå¦‚æœç»Ÿè®¡ç»“æœé•¿åº¦å°äº `minlength`ï¼Œåœ¨æœ«å°¾è¡¥ 0ã€‚

**è¿”å›å€¼**:

* ä¸€ç»´å¼ é‡ `counts`ï¼Œ`counts[i]` è¡¨ç¤ºæ•´æ•° `i` åœ¨ `input` ä¸­çš„å‡ºç°æ¬¡æ•°ï¼ˆæˆ–åŠ æƒå’Œï¼Œå¦‚æœæŒ‡å®š `weights`ï¼‰ã€‚

**ä¾‹å¦‚:**

**æ™®é€šè®¡æ•°ï¼š**

```python
x = torch.tensor([0, 1, 1, 3])
torch.bincount(x)
# è¾“å‡º: tensor([1, 2, 0, 1])
```

**åŠ æƒè®¡æ•°ï¼š**

```python
x = torch.tensor([0, 1, 1, 3])
w = torch.tensor([0.5, 1.0, 2.0, 1.5])
torch.bincount(x, weights=w)
# è¾“å‡º: tensor([0.5, 3.0, 0.0, 1.5])
```

**æŒ‡å®šæœ€å°é•¿åº¦ï¼š**

```python
x = torch.tensor([0, 1, 1])
torch.bincount(x, minlength=5)
# è¾“å‡º: tensor([1, 2, 0, 0, 0])
```

### Tensor.new_zeros

`Tensor.new_zeros(*size, dtype=None, device=None)` æ˜¯ PyTorch çš„ä¸€ä¸ª **å¼ é‡åˆ›å»ºæ–¹æ³•**ï¼Œå®ƒæ ¹æ®å·²æœ‰å¼ é‡çš„å±æ€§åˆ›å»ºä¸€ä¸ªå…¨é›¶å¼ é‡ã€‚

**ä½œç”¨**:

* ç”Ÿæˆå½¢çŠ¶ä¸º `size` çš„å…¨é›¶å¼ é‡ã€‚

* å¼ é‡ä¼šå’Œè°ƒç”¨å®ƒçš„åŸå¼ é‡ **åœ¨åŒä¸€è®¾å¤‡ä¸Š**ï¼ˆCPU/GPUï¼‰ï¼Œå¹¶ä¸”é»˜è®¤ç»§æ‰¿åŸå¼ é‡çš„æ•°æ®ç±»å‹ï¼Œé™¤éé€šè¿‡ `dtype` æŒ‡å®šã€‚

**ä¾‹å­**:

```python
x = torch.randn(3, 4, device='cuda')   # åŸå¼ é‡åœ¨ GPU
y = x.new_zeros(2, 5)                  # åœ¨ GPU ä¸Šåˆ›å»º 2x5 çš„å…¨é›¶å¼ é‡
print(y.device)  # è¾“å‡º: cuda:0
```

### tensor.scatter_add_

```python
tensor.scatter_add_(dim, index, src)
```

* **dim**ï¼šæŒ‡å®šæ²¿å“ªä¸€ç»´ç´¯åŠ ã€‚

  * 0 è¡¨ç¤ºæŒ‰è¡Œç´¯åŠ ï¼ˆä¸åŒæ ·æœ¬ç´¯åŠ åˆ°ä¸åŒçš„ç°‡è¡Œï¼‰ã€‚
 
  * 1 è¡¨ç¤ºæŒ‰åˆ—ç´¯åŠ ï¼ˆæŒ‰åˆ—ç´¢å¼•ç´¯åŠ å…ƒç´ ï¼‰ã€‚

* **index**ï¼šä¸ `src` åŒå½¢çŠ¶çš„æ•´æ•°å¼ é‡ï¼Œè¡¨ç¤º `src` ä¸­çš„æ¯ä¸ªå…ƒç´ è¦åŠ åˆ°ç›®æ ‡å¼ é‡çš„å“ªä¸ªä½ç½®ã€‚

  * å¦‚æœ `dim=0`ï¼Œ`index[i,j]` è¡¨ç¤º `src[i,j]` è¦åŠ åˆ° `tensor[index[i,j], j]`ã€‚

  * å¦‚æœ `dim=1`ï¼Œ`index[i,j]` è¡¨ç¤º `src[i,j]` è¦åŠ åˆ° `tensor[i, index[i,j]]`ã€‚

### torch.topk

`torch.topk()` æ˜¯ PyTorch ä¸­ä¸€ä¸ªéå¸¸å®ç”¨çš„å‡½æ•°ï¼Œç”¨äºè·å–å¼ é‡ä¸­æœ€å¤§æˆ–æœ€å°çš„ k ä¸ªå€¼åŠå…¶ç´¢å¼•ã€‚

```python
torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None)
```

1. `input` (å¿…éœ€)

- è¾“å…¥å¼ é‡

- ç¤ºä¾‹ï¼š`distances` å½¢çŠ¶ä¸º `(m_batch, n_batch)` çš„è·ç¦»çŸ©é˜µ

2. `k` (å¿…éœ€)

- è¦è¿”å›çš„æœ€å¤§/æœ€å°å€¼çš„æ•°é‡

- ç¤ºä¾‹ï¼š`actual_nsample` å®é™…éœ€è¦çš„æœ€è¿‘é‚»æ•°é‡

3. `dim` (å¯é€‰)

- æ²¿ç€å“ªä¸ªç»´åº¦è¿›è¡Œæ“ä½œ

- ç¤ºä¾‹ï¼š`dim=1` è¡¨ç¤ºåœ¨æ¯è¡Œä¸­æ‰¾ topk

- é»˜è®¤å€¼ï¼šæœ€åä¸€ä¸ªç»´åº¦ (`dim=-1`)

4. `largest` (å¯é€‰)

- `True`: è¿”å›æœ€å¤§çš„ k ä¸ªå€¼

- `False`: è¿”å›æœ€å°çš„ k ä¸ªå€¼

- ç¤ºä¾‹ï¼š`largest=False` ç”¨äºæ‰¾æœ€å°è·ç¦»ï¼ˆæœ€è¿‘é‚»ï¼‰

5. `sorted` (å¯é€‰)

- `True`: è¿”å›çš„å€¼æŒ‰é¡ºåºæ’åˆ—

- `False`: è¿”å›çš„å€¼ä¸ä¿è¯é¡ºåº

- é»˜è®¤å€¼ï¼š`True`

### è¿ç»­æ€§

> â€œè¿ç»­å†…å­˜â€: ä¸€ä¸ªå¤šç»´å¼ é‡åœ¨å†…å­˜ä¸­å®é™…ä¸Šæ˜¯ä»¥ä¸€ç»´æ•°ç»„çš„å½¢å¼å­˜å‚¨çš„ã€‚
> 
> *   **å†…å­˜è¿ç»­**ï¼šæ„å‘³ç€æŒ‰ç…§å¼ é‡çš„**æœ€å³ç»´åº¦ï¼ˆæœ€å†…å±‚ç»´åº¦ï¼‰** å˜åŒ–æœ€å¿«çš„æ–¹å¼ï¼ˆå³è¡Œä¼˜å…ˆï¼ŒRow Majorï¼‰é¡ºåºï¼Œå°†å…¶æ‰€æœ‰å…ƒç´ æ— é—´éš”åœ°ã€é¡ºåºåœ°å­˜å‚¨åœ¨ä¸€å—å†…å­˜ä¸­ã€‚
> 
> *   **å†…å­˜ä¸è¿ç»­**ï¼šæ„å‘³ç€å¼ é‡çš„å…ƒç´ åœ¨å†…å­˜ä¸­çš„å­˜å‚¨é¡ºåºä¸å…¶é€»è¾‘ä¸Šçš„ç»´åº¦é¡ºåºä¸åŒ¹é…ï¼Œæˆ–è€…å†…å­˜ä¸­å­˜åœ¨é—´éš”ï¼ˆStrideï¼‰ã€‚

`stride` æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œè¡¨ç¤ºåœ¨æ¯ä¸ªç»´åº¦ä¸Šç§»åŠ¨ä¸€ä¸ªå…ƒç´ æ—¶ï¼Œ**éœ€è¦åœ¨å†…å­˜ä¸­è·³è¿‡å¤šå°‘ä¸ªå…ƒç´ **ã€‚å®ƒæ˜¯ç†è§£è¿ç»­æ€§çš„å…³é”®ã€‚ å¯¹äºä¸€ä¸ªå½¢çŠ¶ä¸º `(C, H, W)` çš„è¿ç»­å¼ é‡ï¼Œå…¶ `stride` é€šå¸¸æ˜¯ `(H*W, W, 1)`ã€‚

*   åœ¨ `C` ç»´åº¦ä¸Šç§»åŠ¨ 1 ä½ï¼Œéœ€è¦åœ¨å†…å­˜ä¸­è·³è¿‡ `H*W` ä¸ªå…ƒç´ ã€‚

*   åœ¨ `H` ç»´åº¦ä¸Šç§»åŠ¨ 1 ä½ï¼Œéœ€è¦åœ¨å†…å­˜ä¸­è·³è¿‡ `W` ä¸ªå…ƒç´ ã€‚

*   åœ¨ `W` ç»´åº¦ä¸Šç§»åŠ¨ 1 ä½ï¼Œåªéœ€è¦ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªå…ƒç´ ï¼ˆ`1` ä¸ªï¼‰ã€‚

**åˆ¤æ–­è¿ç»­æ€§çš„æ¡ä»¶**ï¼šå½“å¼ é‡çš„ `stride` ä¸å…¶ `size` æ»¡è¶³ç‰¹å®šå…³ç³»æ—¶ï¼ˆå³ `stride[i] == stride[i+1] * size[i+1]`ï¼‰ï¼Œè¯¥å¼ é‡æ‰æ˜¯è¿ç»­çš„ã€‚

---

#### tensor.is_contiguous()

*   **ä½œç”¨**ï¼šåˆ¤æ–­å½“å‰å¼ é‡çš„å†…å­˜å¸ƒå±€æ˜¯å¦æ˜¯è¿ç»­çš„ã€‚

*   **è¿”å›å€¼**ï¼šä¸€ä¸ªå¸ƒå°”å€¼ï¼ˆ`True` æˆ– `False`ï¼‰ã€‚

*   **ç‰¹ç‚¹**ï¼šè¿™æ˜¯ä¸€ä¸ª**è½»é‡çº§**çš„æ£€æŸ¥æ“ä½œï¼Œåªæ£€æŸ¥å…ƒæ•°æ®ï¼ˆ`stride`, `size`ï¼‰ï¼Œä¸å¤åˆ¶ä»»ä½•æ•°æ®ã€‚

**ç¤ºä¾‹**ï¼š

```python
import torch

# åˆ›å»ºä¸€ä¸ªè¿ç»­å¼ é‡
x = torch.randn(2, 3, 4)
print(x.is_contiguous())  # è¾“å‡º: True
print(x.stride())         # è¾“å‡º: (12, 4, 1) -> (3*4, 4, 1)

# åˆ›å»ºä¸€ä¸ªä¸è¿ç»­å¼ é‡çš„å¸¸è§æ“ä½œï¼šè½¬ç½®ï¼ˆTransposeï¼‰
y = x.transpose(0, 2) # å°†ç»´åº¦0å’Œç»´åº¦2äº¤æ¢
print(y.shape)        # è¾“å‡º: torch.Size([4, 3, 2])
print(y.stride())     # è¾“å‡º: (1, 4, 12) -> ä¸è¿ç»­æ—¶çš„æ­¥é•¿è§„åˆ™ä¸ç¬¦
print(y.is_contiguous()) # è¾“å‡º: False
```

åƒ `transpose()`, `permute()`, `narrow()`, `expand()`, `t()` ç­‰æ“ä½œé€šå¸¸ä¼šäº§ç”Ÿ**ä¸è¿ç»­**çš„å¼ é‡ï¼Œå› ä¸ºå®ƒä»¬åªæ”¹å˜äº†è§†å›¾ï¼ˆViewï¼‰ï¼Œè€Œæ²¡æœ‰å®é™…é‡æ–°æ’åˆ—å†…å­˜ä¸­çš„æ•°æ®ã€‚

---

#### tensor.contiguous()

*   **ä½œç”¨**ï¼šè¿”å›ä¸€ä¸ª**å†…å­˜è¿ç»­**çš„ã€æ•°æ®å†…å®¹ç›¸åŒçš„å¼ é‡ã€‚

*   **è¿”å›å€¼**ï¼šä¸€ä¸ªæ–°çš„å¼ é‡ã€‚

*   **ç‰¹ç‚¹**ï¼š

    *   å¦‚æœåŸå¼ é‡**å·²ç»æ˜¯è¿ç»­çš„**ï¼Œåˆ™ `contiguous()` **ä¸ä¼šè¿›è¡Œä»»ä½•å¤åˆ¶æ“ä½œ**ï¼Œç›´æ¥è¿”å›åŸå¼ é‡æœ¬èº«ï¼ˆ`self`ï¼‰ã€‚

    *   å¦‚æœåŸå¼ é‡**ä¸æ˜¯è¿ç»­çš„**ï¼Œåˆ™ `contiguous()` **ä¼šåˆ†é…ä¸€å—æ–°çš„è¿ç»­å†…å­˜**ï¼Œå¹¶å°†åŸå¼ é‡çš„æ•°æ®**æŒ‰ç…§å…¶é€»è¾‘é¡ºåºå¤åˆ¶**åˆ°è¿™å—æ–°å†…å­˜ä¸­ã€‚

**ç¤ºä¾‹**ï¼š

```python
import torch

x = torch.randn(2, 3)
print(f"x is contiguous: {x.is_contiguous()}") # True

# åˆ›å»ºä¸€ä¸ªä¸è¿ç»­çš„è§†å›¾
y = x.t() # è½¬ç½®æ“ä½œ
print(f"y is contiguous: {y.is_contiguous()}") # False

# å¯¹ä¸è¿ç»­çš„ y è°ƒç”¨ contiguous()
z = y.contiguous()
print(f"z is contiguous: {z.is_contiguous()}") # True

# éªŒè¯å†…å­˜åœ°å€å’Œæ•°æ®
print(f"y data ptr: {y.storage().data_ptr()}") # ä¸ x ç›¸åŒ
print(f"z data ptr: {z.storage().data_ptr()}") # ä¸ x/y ä¸åŒï¼Œæ˜¯æ–°åˆ†é…çš„

# éªŒè¯æ•°æ®å†…å®¹æ˜¯å¦ä¸€è‡´
print(torch.all(y == z)) # è¾“å‡º: Trueï¼Œæ•°æ®å€¼ç›¸åŒ
```

---

#### ä¸ºä»€ä¹ˆéœ€è¦ `contiguous()` ï¼Ÿ

è®¸å¤š PyTorch æ“ä½œï¼ˆå°¤å…¶æ˜¯åº•å±‚ç”± CUDA/C++ å®ç°çš„æ“ä½œï¼‰**è¦æ±‚è¾“å…¥å¼ é‡å¿…é¡»æ˜¯å†…å­˜è¿ç»­çš„**ï¼Œå¦åˆ™ä¼šæŠ¥é”™æˆ–å¾—åˆ°é”™è¯¯çš„ç»“æœã€‚æœ€å¸¸è§çš„åœºæ™¯åŒ…æ‹¬ï¼š

1.  **è§†å›¾æ“ä½œï¼ˆViewï¼‰**ï¼š`tensor.view()` **è¦æ±‚**å¼ é‡æ˜¯è¿ç»­çš„ã€‚
   
    ```python
    y = x.t()
    # z = y.view(-1) # è¿™é‡Œä¼šæŠ¥é”™ï¼šRuntimeError: view size is not compatible with input tensor's size and stride...
    z = y.contiguous().view(-1) # æ­£ç¡®åšæ³•ï¼šå…ˆè¿ç»­åŒ–ï¼Œå†æ”¹å˜è§†å›¾
   
    ```
2.  **`.data_ptr()` è®¿é—®**ï¼šå¦‚æœä½ æƒ³è·å¾—åº•å±‚æ•°æ®å­˜å‚¨åŒºçš„æŒ‡é’ˆï¼Œéœ€è¦ç¡®ä¿å®ƒæ˜¯è¿ç»­çš„ã€‚

3.  **ä¸å¤–éƒ¨åº“äº¤äº’**ï¼šä¾‹å¦‚å°† PyTorch å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„ï¼ˆ`tensor.numpy()`ï¼‰æˆ–ä¼ é€’ç»™å…¶ä»– C++ æ‰©å±•æ—¶ï¼Œé€šå¸¸éœ€è¦è¿ç»­çš„å†…å­˜å¸ƒå±€ã€‚

4.  **æŸäº›æ€§èƒ½å…³é”®çš„æ“ä½œ**ï¼šè¿ç»­çš„å†…å­˜è®¿é—®æ¨¡å¼å¯¹ CPU/GPU ç¼“å­˜æ›´å‹å¥½ï¼Œæœ‰æ—¶èƒ½æå‡è®¡ç®—æ•ˆç‡ã€‚

#### å½’ä¸€åŒ–å±‚å¯¹è¿ç»­æ€§çš„è¦æ±‚

> ğŸ”´ **å¿…é¡»ä¼ å…¥è¿ç»­å¼ é‡**

**BatchNormç³»åˆ—** (`nn.BatchNorm1d/2d/3d`)

- **åŸå› **ï¼šåº•å±‚CUDAå®ç°ä¸¥æ ¼ä¾èµ–è¿ç»­å†…å­˜å¸ƒå±€è¿›è¡Œè·¨æ‰¹æ¬¡ç»Ÿè®¡è®¡ç®—

- **é£é™©**ï¼šç›´æ¥ä¼ å…¥ä¸è¿ç»­å¼ é‡æé«˜æ¦‚ç‡å¯¼è‡´è¿è¡Œæ—¶é”™è¯¯æˆ–è®¡ç®—ç»“æœé”™è¯¯

> ğŸŸ¡ **å¼ºçƒˆå»ºè®®ä¼ å…¥è¿ç»­å¼ é‡**  

**LayerNorm** (`nn.LayerNorm`)

- **åŸå› **ï¼šè™½ç„¶æŸäº›å®ç°èƒ½å¤„ç†ä¸è¿ç»­è¾“å…¥ï¼Œä½†ä¸ºä¿éšœè·¨å¹³å°ä¸€è‡´æ€§å’Œæœ€ä½³æ€§èƒ½

- **å»ºè®®**ï¼šæ€»æ˜¯ä½¿ç”¨ `.contiguous()` ç¡®ä¿ç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡

> ğŸŸ¡ **å¼ºçƒˆå»ºè®®ä¼ å…¥è¿ç»­å¼ é‡**

**InstanceNormç³»åˆ—** (`nn.InstanceNorm1d/2d/3d`)  

- **åŸå› **ï¼šé€šé“çº§åˆ«çš„ç»Ÿè®¡è®¡ç®—åŒæ ·å—ç›Šäºè¿ç»­å†…å­˜è®¿é—®æ¨¡å¼

- **å»ºè®®**ï¼šé¢„å¤„ç†ä¸­ç¡®ä¿å¼ é‡è¿ç»­æ€§ä»¥é¿å…æ½œåœ¨é—®é¢˜

> ğŸŸ¡ **å¼ºçƒˆå»ºè®®ä¼ å…¥è¿ç»­å¼ é‡**

**GroupNorm** (`nn.GroupNorm`)

- **åŸå› **ï¼šåˆ†ç»„ç»Ÿè®¡è®¡ç®—éœ€è¦é«˜æ•ˆçš„å†…å­˜è®¿é—®æ¨¡å¼

- **å»ºè®®**ï¼šä¿æŒè¿ç»­æ€§ä»¥è·å¾—æœ€ä½³æ€§èƒ½å’Œæ­£ç¡®æ€§

**åªæœ‰BatchNormæ˜¯"å¿…é¡»"çš„ï¼Œå…¶ä»–éƒ½æ˜¯"å¼ºçƒˆå»ºè®®"ã€‚ä½†ç»Ÿä¸€çš„æœ€ä½³å®è·µæ˜¯ï¼šåœ¨æ‰€æœ‰å½’ä¸€åŒ–æ“ä½œå‰éƒ½è°ƒç”¨ `.contiguous()`ï¼Œç”¨å¾®å°çš„å¼€é”€æ¢å–ä»£ç çš„å¥å£®æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚**

#### æ€»ç»“

| æ–¹æ³• | ä½œç”¨ | æ•°æ®å¤åˆ¶è¡Œä¸º |
| :--- | :--- | :--- |
| `tensor.is_contiguous()` | **æ£€æŸ¥**å¼ é‡å†…å­˜æ˜¯å¦è¿ç»­ | ç»ä¸å¤åˆ¶æ•°æ® |
| `tensor.contiguous()` | **ç¡®ä¿**è¿”å›ä¸€ä¸ªè¿ç»­çš„å¼ é‡ | **æ¡ä»¶æ€§å¤åˆ¶**ï¼ˆä»…åœ¨åŸå¼ é‡ä¸è¿ç»­æ—¶å¤åˆ¶ï¼‰ |

**æœ€ä½³å®è·µ**ï¼šå½“ä½ å¯¹ä¸€ä¸ªå¼ é‡è¿›è¡Œäº† `transpose`, `permute` ç­‰å¯èƒ½æ”¹å˜å†…å­˜å¸ƒå±€çš„æ“ä½œåï¼Œå¦‚æœåç»­éœ€è¦ç”¨åˆ° `view` æˆ–è€…è¦å°†å…¶ä¼ å…¥æŸäº›ç‰¹å®šå‡½æ•°ï¼Œ**å®‰å…¨èµ·è§ï¼Œå…ˆè°ƒç”¨ `.contiguous()`**ã€‚è™½ç„¶æœ‰æ—¶ä¸è°ƒç”¨ä¹Ÿèƒ½å·¥ä½œï¼Œä½†æ˜¾å¼åœ°è°ƒç”¨å¯ä»¥é¿å…éš¾ä»¥è°ƒè¯•çš„è¿è¡Œæ—¶é”™è¯¯ã€‚

### `F.nll_loss` æˆ– `F.cross_entropy` çš„ `reduction` å‚æ•°

> cross_entropy = log_softmax + nll_loss

* å¯¹äº `F.nll_loss` æˆ– `F.cross_entropy`ï¼Œé»˜è®¤ `reduction='mean'`

  * ä¼šå¯¹ batch å†…æ‰€æœ‰æ ·æœ¬çš„æŸå¤± **å–å¹³å‡**
  
  * è¾“å‡ºæ˜¯ä¸€ä¸ªæ ‡é‡

* å¦‚æœè®¾ç½® `reduction='sum'`

  * ä¼šå¯¹ batch å†…æ‰€æœ‰æ ·æœ¬çš„æŸå¤± **æ±‚å’Œ**

  * è¾“å‡ºä¹Ÿæ˜¯ä¸€ä¸ªæ ‡é‡

```python
import torch
import torch.nn.functional as F

# å‡è®¾ batch æœ‰ 4 ä¸ªæ ·æœ¬ï¼Œå±äº 3 ä¸ªç±»åˆ«
logits = torch.tensor([[2.0, 0.5, 1.0],
                       [0.1, 1.2, 0.3],
                       [1.0, 0.5, 2.0],
                       [0.2, 0.1, 0.7]])  # shape [4,3]

# å¯¹åº”çš„çœŸå®æ ‡ç­¾
targets = torch.tensor([0, 1, 2, 2])  # shape [4]

# å…ˆå¯¹ logits åš log_softmax
log_probs = F.log_softmax(logits, dim=1)

# default
loss_none = F.nll_loss(log_probs, targets)
print("default:", loss_none)

# reduction='none'
loss_none = F.nll_loss(log_probs, targets, reduction='none')
print("reduction='none':", loss_none)

# reduction='mean'
loss_mean = F.nll_loss(log_probs, targets, reduction='mean')
print("reduction='mean':", loss_mean)

# reduction='sum'
loss_sum = F.nll_loss(log_probs, targets, reduction='sum')
print("reduction='sum':", loss_sum)
```

è¾“å‡º:

```python
default: tensor(0.5626)
reduction='none': tensor([0.4644, 0.5536, 0.4644, 0.7679])
reduction='mean': tensor(0.5626)
reduction='sum': tensor(2.2503)
```