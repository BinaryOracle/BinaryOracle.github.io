---
title: SeqAfford 论文
icon: file
category:
  - 3D-VL
tag:
  - 3D-VL
  - 编辑中
footer: 技术共建，知识共享
date: 2025-09-15
author:
  - BinaryOracle
---

`SeqAfford: Sequential 3D Affordance Reasoning via  Multimodal Large Language Model 论文` 

<!-- more -->

> 论文: [SeqAfford: Sequential 3D Affordance Reasoning via  Multimodal Large Language Model](https://arxiv.org/abs/2412.01550)
> 代码: [https://github.com/hq-King/SeqAfford](https://github.com/hq-King/SeqAfford)

# 引言

三维可供性分割的目标，是把人类指令和三维物体上可以触摸或操作的区域对应起来，用于机器人等具身智能体的操作。相比二维可供性（只提供视觉提示），三维可供性能在真实环境中提供更直观的指导，是下游机器人任务的基础。

现有研究大多遵循“单物体、单可供性”的模式：

* 每条指令或描述只能对应到某个固定的可供性区域

* 可能依赖于 **类别标签**、**二维示范图像** 或 **自然语言问题** 来完成映射

* 语言模型（如 BERT、RoBERTa）可以解析简单问题，例如 *“How can you go through the door?”* → “门把手（openable）”

但是这些方法存在严重不足：

* 不能主动推理复杂人类意图

* 不能将复杂任务分解成可执行的动作原语

* 不能形成多物体、多步骤的“可供性链条”

现实操作中却常常需要 **多物体参与的顺序推理**。例如：*“用微波炉加热碗里的食物”* → 抓碗 → 开门 → 放进去。这种能力对下一代可供性系统至关重要。

---

近年来，**大语言模型（LLMs）** 在顺序推理上展现出强大能力，因为它们内化了来自大规模文本的常识知识。进一步，**三维多模态大语言模型（3D MLLMs）** 也能理解三维物体的形状，为复杂推理提供了基础。

但即使是最新的 3D MLLMs：

* 仍然主要服务于“物体为中心”的文本生成任务

* 还不足以处理基于 3D 物体的视觉可供性推理

这引出了一个关键问题： **能否设计一种 3D 多模态大语言模型，使其能基于长时序的人类指令进行多物体的顺序推理与可供性分割？**

---

为解决上述问题，作者提出了 **顺序三维可供性推理（Sequential 3D Affordance Reasoning）** 任务，并作出以下贡献：

**核心工作：**

* 构建了一个 **大规模顺序可供性推理基准**，包含 **18万条指令–点云对**。

  * 指令通过四种方式生成，其中包括利用二维 HOI 图像和彩色网格渲染来提示 GPT-4o 生成多样化、符合常识的指令（如图2所示）。

* 提出模型 **SeqAfford**，为现有 3D MLLMs 赋予顺序可供性分割能力。

  * 支持多目标、细粒度的顺序推理与解释

  * 在统一框架下结合了“世界知识 + 可供性分割”

* 设计 **多粒度语言–点云融合模块**：

  * 将 LLM 的分割 token 条件化稠密点特征，与稀疏点特征结合

  * 有效地将 LLM 的推理结果注入稠密特征

  * 提升三维密集预测和可供性分割任务的表现

**实验结果：**

* SeqAfford 在多个评估中显著超越现有方法

* 展现出 **顺序推理能力** 和 **开放世界泛化能力**