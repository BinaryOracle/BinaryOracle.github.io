---
title: IAGNet è®ºæ–‡è§£è¯»
icon: file
category:
  - 3D-VL
  - 3D Affordance
tag:
  - 3D-VL
  - 3D Affordance
  - å·²å‘å¸ƒ
footer: æŠ€æœ¯å…±å»ºï¼ŒçŸ¥è¯†å…±äº«
date: 2025-07-11
author:
  - BinaryOracle
---

`Grounding 3D Object Affordance from 2D Interactions in Images è®ºæ–‡è§£è¯»` 

<!-- more -->

> è®ºæ–‡: [https://arxiv.org/abs/2303.10437](https://arxiv.org/abs/2303.10437)
> ä»£ç : [https://github.com/yyvhang/IAGNet](https://github.com/yyvhang/IAGNet)
> æ•°æ®é›†: [https://drive.google.com/drive/folders/1F242TsdXjRZkKQotiBsiN2u6rJAGRZ2W](https://drive.google.com/drive/folders/1F242TsdXjRZkKQotiBsiN2u6rJAGRZ2W)

## æ‘˜è¦

è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä»»åŠ¡è®¾å®šï¼šé€šè¿‡2Då›¾åƒä¸­çš„äº¤äº’ä¿¡æ¯æ¥é¢„æµ‹3Dç‰©ä½“çš„åŠŸèƒ½åŒºåŸŸï¼ˆaffordanceï¼‰ï¼Œæ—¨åœ¨ä¸ºå…·èº«æ™ºèƒ½ä½“å»ºç«‹æ„ŸçŸ¥ä¸æ“ä½œä¹‹é—´çš„è”ç³»ã€‚ä½œè€…è®¾è®¡äº†ä¸€ä¸ªåä¸ºIAGï¼ˆInteraction-driven 3D Affordance Grounding Networkï¼‰çš„æ¡†æ¶ï¼Œé€šè¿‡è”åˆåŒºåŸŸå¯¹é½æ¨¡å—ï¼ˆJRAï¼‰è§£å†³ä¸åŒæ¥æºç‰©ä½“åŒºåŸŸçš„å¯¹é½é—®é¢˜ï¼Œå¹¶é€šè¿‡åŠŸèƒ½æ­ç¤ºæ¨¡å—ï¼ˆARMï¼‰å»ºæ¨¡äº¤äº’ä¸Šä¸‹æ–‡ä»¥æ˜ç¡®åŠŸèƒ½åŒºåŸŸã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«å›¾åƒ-ç‚¹äº‘é…å¯¹æ•°æ®çš„æ•°æ®é›†PIADï¼Œç”¨äºæ”¯æŒè¯¥ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨PIADæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†ä»»åŠ¡è®¾å®šçš„å¯è¡Œæ€§å’Œæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€ç ”ç©¶ä¸ºåŠŸèƒ½å­¦ä¹ é¢†åŸŸæä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶æœ‰æœ›åº”ç”¨äºæœºå™¨äººæ“ä½œã€å¢å¼ºç°å®ç­‰é¢†åŸŸã€‚

## ç®€ä»‹

Gibsonï¼ˆ[2014](https://www.taylorfrancis.com/books/mono/10.4324/9781315740218/ecological-approach-visual-perception-james-gibson)ï¼‰æå‡ºçš„â€œåŠŸèƒ½å¯ä¾›æ€§â€ï¼ˆaffordanceï¼‰æ¦‚å¿µï¼Œå³ç‰©ä½“æ”¯æŒçš„äº¤äº’å¯èƒ½æ€§ï¼Œæ˜¯è¿æ¥å…·èº«æ™ºèƒ½ä½“æ„ŸçŸ¥ä¸æ“ä½œçš„å…³é”®ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼š  

1. **å‡ ä½•ç»“æ„æ˜ å°„æ–¹æ³•**ï¼ˆå¦‚[11](https://openaccess.thecvf.com/content/CVPR2021/html/Deng_3D-AffordanceNet_A_Benchmark_for_Visual_Object_Affordance_Understanding_CVPR_2021_paper.html)ã€[22](https://ieeexplore.ieee.org/document/7139362)ï¼‰é€šè¿‡æ ‡æ³¨ç‰©ä½“äº¤äº’åŒºåŸŸå»ºç«‹å‡ ä½•ç»“æ„ä¸åŠŸèƒ½çš„å›ºå®šå…³è”ï¼Œä½†æ³›åŒ–æ€§å—é™ï¼Œä¸”å¯¹å¤šåŠŸèƒ½çš„ç›¸ä¼¼ç»“æ„æ˜“äº§ç”Ÿæ··æ·†ï¼ˆå¦‚å›¾2(b)ä¸­æ¤…å­çš„â€œåâ€ä¸â€œç§»åŠ¨â€åŠŸèƒ½ï¼‰ã€‚  

![](IAGNet/2.png)

2. **å¼ºåŒ–å­¦ä¹ æ–¹æ³•**ï¼ˆå¦‚[54](https://proceedings.neurips.cc/paper/2020/hash/6dd4e10e3296fa63738371ec0d5df818-Abstract.html)ï¼‰é€šè¿‡æ™ºèƒ½ä½“åœ¨è™šæ‹Ÿç¯å¢ƒä¸­ä¸»åŠ¨äº¤äº’å­¦ä¹ åŠŸèƒ½ï¼Œä½†æœç´¢ç©ºé—´å¤§ã€è€—æ—¶ä¸¥é‡ã€‚  

**æœ¬æ–‡åˆ›æ–°ç‚¹**ï¼š  
![](IAGNet/1.png)
- **ä»»åŠ¡è®¾å®š**ï¼šé¦–æ¬¡æå‡ºé€šè¿‡2Däº¤äº’å›¾åƒé¢„æµ‹3Dç‰©ä½“åŠŸèƒ½åŒºåŸŸï¼ˆå¦‚å›¾1ï¼‰ï¼Œæ¨¡æ‹Ÿäººç±»é€šè¿‡è§‚å¯Ÿå­¦ä¹ ç‰©ä½“åŠŸèƒ½çš„èƒ½åŠ›ã€‚  
- **æ ¸å¿ƒæŒ‘æˆ˜**ï¼š  
  - **å¯¹é½æ¨¡ç³Šæ€§**ï¼š2Dæ¼”ç¤ºä¸3Dç‰©ä½“æ¥è‡ªä¸åŒå®ä¾‹ï¼Œéœ€è·¨æºå¯¹é½åŒºåŸŸï¼ˆå›¾2(a)å±•ç¤ºåŒç±»ç‰©ä½“çš„ç»“æ„ç›¸ä¼¼æ€§å¯è¾…åŠ©å¯¹é½ï¼‰ã€‚  
  - **åŠŸèƒ½æ¨¡ç³Šæ€§**ï¼šåŒä¸€ç‰©ä½“åŒºåŸŸå¯èƒ½æ”¯æŒå¤šåŠŸèƒ½ï¼ˆå¦‚â€œæ¯å­â€æ—¢å¯â€œæ¡æŒâ€ä¹Ÿå¯â€œç››æ”¾â€ï¼‰ï¼Œéœ€é€šè¿‡äº¤äº’ä¸Šä¸‹æ–‡å»ºæ¨¡è§£å†³ï¼ˆå›¾2(b)ï¼‰ã€‚  

**è§£å†³æ–¹æ¡ˆ**ï¼š  
- **IAGæ¡†æ¶**ï¼šåŒ…å«**JRAæ¨¡å—**ï¼ˆé€šè¿‡å¯†é›†è·¨æ¨¡æ€ç›¸ä¼¼æ€§ $Ï†_{i,j}=\frac{e^{(P_i,I_j)}}{\sum e^{(P_i,I_j)}}$ å¯¹é½åŒºåŸŸï¼‰å’Œ**ARMæ¨¡å—**ï¼ˆé€šè¿‡äº¤å‰æ³¨æ„åŠ›å»ºæ¨¡ç‰©ä½“-ä¸»ä½“/åœºæ™¯äº¤äº’ä»¥æ­ç¤ºåŠŸèƒ½ï¼‰ã€‚  
- **PIADæ•°æ®é›†**ï¼šåŒ…å«7,012ä¸ªç‚¹äº‘å’Œ5,162å¼ å›¾åƒï¼Œè¦†ç›–23ç±»ç‰©ä½“å’Œ17ç§åŠŸèƒ½ï¼Œæ”¯æŒâ€œå¯è§â€ä¸â€œæœªè§â€åœºæ™¯çš„è¯„ä¼°ï¼ˆå›¾4ï¼‰ã€‚  

![](IAGNet/3.png)

**æ„ä¹‰**ï¼šè¯¥æ–¹æ³•æ‘†è„±äº†å¯¹å‡ ä½•æ ‡æ³¨æˆ–å›ºå®šåœºæ™¯çš„ä¾èµ–ï¼Œä¸ºæœºå™¨äººæ“ä½œã€AR/VRç­‰åº”ç”¨æä¾›äº†æ›´é€šç”¨çš„åŠŸèƒ½ç†è§£èŒƒå¼ã€‚

## ç›¸å…³å·¥ä½œ

#### **1. åŠŸèƒ½å­¦ä¹ ï¼ˆAffordance Learningï¼‰**
ç°æœ‰ç ”ç©¶å¯åˆ†ä¸ºä¸‰ç±»ï¼ˆå¦‚è¡¨1æ‰€ç¤ºï¼‰ï¼š

- **2DåŠŸèƒ½æ£€æµ‹**ï¼š  
  - æ—©æœŸå·¥ä½œï¼ˆå¦‚[12](https://ieeexplore.ieee.org/document/8460928)ã€[69](https://ieeexplore.ieee.org/document/9053830)ï¼‰ä»å›¾åƒ/è§†é¢‘ä¸­åˆ†å‰²åŠŸèƒ½åŒºåŸŸï¼Œä½†æ— æ³•å®šä½å…·ä½“äº¤äº’éƒ¨ä½ã€‚  
  - è¯­è¨€è¾…åŠ©æ–¹æ³•ï¼ˆå¦‚[36](https://ieeexplore.ieee.org/document/9200506)ï¼‰ç»“åˆæ–‡æœ¬æè¿°æå‡è¯­ä¹‰ç†è§£ã€‚  

- **3DåŠŸèƒ½å®šä½**ï¼š  
  - åŸºäºå‡ ä½•æ˜ å°„çš„æ–¹æ³•ï¼ˆå¦‚[11](https://openaccess.thecvf.com/content/CVPR2021/html/Deng_3D-AffordanceNet_A_Benchmark_for_Visual_Object_Affordance_Understanding_CVPR_2021_paper.html)ï¼‰ç›´æ¥å…³è”ç»“æ„ä¸åŠŸèƒ½ï¼Œæ³›åŒ–æ€§å·®ã€‚  
  - å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚[54](https://proceedings.neurips.cc/paper/2020/hash/6dd4e10e3296fa63738371ec0d5df818-Abstract.html)ï¼‰é€šè¿‡æ™ºèƒ½ä½“ä¸»åŠ¨äº¤äº’å­¦ä¹ ï¼Œä½†æ•ˆç‡ä½ã€‚  

- **æœºå™¨äººæ“ä½œåº”ç”¨**ï¼š  
  - é’ˆå¯¹é“°æ¥ç‰©ä½“ï¼ˆå¦‚[48](https://openaccess.thecvf.com/content/ICCV2021/html/Mo_Where2Act_From_Pixels_to_Actions_for_Articulated_3D_Objects_ICCV_2021_paper.html)ï¼‰è®¾è®¡åŠŸèƒ½çƒ­å›¾ï¼ŒæŒ‡å¯¼æŠ“å–å’Œè¿åŠ¨è§„åˆ’ã€‚  

**æœ¬æ–‡åŒºåˆ«**ï¼šé¦–æ¬¡é€šè¿‡**éé…å¯¹**çš„2D-3Dæ•°æ®å­¦ä¹ åŠŸèƒ½ï¼Œæ‘†è„±å‡ ä½•æ ‡æ³¨å’Œå›ºå®šåœºæ™¯é™åˆ¶ã€‚

---

#### **2. å›¾åƒ-ç‚¹äº‘è·¨æ¨¡æ€å­¦ä¹ **
ç°æœ‰æ–¹æ³•ä¾èµ–ä¸¤ç±»å¯¹é½ç­–ç•¥ï¼š

- **ç©ºé—´å…ˆéªŒå¯¹é½**ï¼š  
  - åŸºäºç›¸æœºå‚æ•°ï¼ˆå¦‚[68](https://dl.acm.org/doi/10.1145/3474085.3479228)ã€[90](https://openaccess.thecvf.com/content/ICCV2021/html/Zhuang_Perception-Aware_Multi-Sensor_Fusion_for_3D_LiDAR_Semantic_Segmentation_ICCV_2021_paper.html)ï¼‰å°†ç‚¹äº‘æŠ•å½±åˆ°å›¾åƒå¹³é¢ï¼Œéœ€ä¸¥æ ¼çš„ç©ºé—´å¯¹åº”ã€‚  

- **ç‰¹å¾ç©ºé—´å¯¹é½**ï¼š  
  - æ— ç›¸æœºå‚æ•°æ–¹æ³•ï¼ˆå¦‚[1](https://arxiv.org/abs/2209.09552)ã€[6](https://dl.acm.org/doi/10.1145/3528233.3530731)ï¼‰ç›´æ¥å»ºæ¨¡è·¨æ¨¡æ€ç‰¹å¾ç›¸ä¼¼æ€§ã€‚  

**æœ¬æ–‡åˆ›æ–°**ï¼šåˆ©ç”¨åŠŸèƒ½-ç»“æ„çš„éšå¼å…³è”ï¼ˆå¦‚å›¾2(a)ï¼‰ï¼Œåœ¨æ— ç©ºé—´å…ˆéªŒä¸‹å®ç°è·¨æºç‰¹å¾å¯¹é½ã€‚

## æ–¹æ³•

#### **1. æ•´ä½“æ¡†æ¶ï¼ˆIAGç½‘ç»œï¼‰**

![](IAGNet/4.png)

å¦‚å›¾3æ‰€ç¤ºï¼ŒIAGç½‘ç»œè¾“å…¥ä¸ºå››å…ƒç»„ $\{P,I,\mathcal{B},y\}$ï¼Œå…¶ä¸­ï¼š

- $P\in\mathbb{R}^{N\times3}$ ä¸ºç‚¹äº‘åæ ‡

- $I\in\mathbb{R}^{3\times H\times W}$ ä¸ºRGBå›¾åƒ

- $\mathcal{B}=\{B_{sub},B_{obj}\}$ ä¸ºå›¾åƒä¸­ä¸»ä½“å’Œç‰©ä½“çš„è¾¹ç•Œæ¡†

- $y$ ä¸ºåŠŸèƒ½ç±»åˆ«æ ‡ç­¾

**å¤„ç†æµç¨‹**ï¼š
1. **ç‰¹å¾æå–**ï¼š

   - å›¾åƒåˆ†æ”¯ï¼šResNetæå–ç‰¹å¾ $\mathbf{F}_1\in\mathbb{R}^{C\times H'\times W'}$

   - ç‚¹äº‘åˆ†æ”¯ï¼šPointNet++æå–ç‰¹å¾ $\mathbf{F}_p\in\mathbb{R}^{C\times N_p}$

2. **åŒºåŸŸå®šä½**ï¼š

   - é€šè¿‡ROI-Alignè·å–ç‰©ä½“/ä¸»ä½“/åœºæ™¯ç‰¹å¾ $\mathbf{F}_i,\mathbf{F}_s,\mathbf{F}_e\in\mathbb{R}^{C\times N_i}$ï¼ˆ$N_i=H_1\times W_1$ï¼‰

3. **è”åˆåŒºåŸŸå¯¹é½ï¼ˆJRAæ¨¡å—ï¼‰**ï¼š

   - è®¡ç®—å¯†é›†è·¨æ¨¡æ€ç›¸ä¼¼æ€§çŸ©é˜µï¼š
     $$
     \varphi_{i,j}=\frac{e^{(\mathbf{P}_i,\mathbf{I}_j)}}{\sum_{i=1}^{N_p}\sum_{j=1}^{N_i}e^{(\mathbf{P}_i,\mathbf{I}_j)}}
     $$

   - é€šè¿‡è‡ªæ³¨æ„åŠ›å»ºæ¨¡æ¨¡æ€å†…ç»“æ„å…³ç³»ï¼š
     $$
     \tilde{\mathbf{P}}=f_p(\mathbf{I}\cdot\varphi^T), \quad \tilde{\mathbf{I}}=f_i(\mathbf{P}\cdot\varphi)
     $$

   - è”åˆæ³¨æ„åŠ›ç”Ÿæˆå¯¹é½ç‰¹å¾ $\mathbf{F}_j=f_\delta[\tilde{\mathbf{P}},\tilde{\mathbf{I}}]$

4. **åŠŸèƒ½æ­ç¤ºï¼ˆARMæ¨¡å—ï¼‰**ï¼š

   - äº¤å‰æ³¨æ„åŠ›å»ºæ¨¡äº¤äº’ä¸Šä¸‹æ–‡ï¼š
     $$
     \boldsymbol{\Theta}_{1/2}=\text{softmax}(\mathbf{Q}^T\cdot\mathbf{K}_{1/2}/\sqrt{d})\cdot\mathbf{V}_{1/2}^T
     $$

   - èåˆç”ŸæˆåŠŸèƒ½è¡¨å¾ $\mathbf{F}_\alpha=f_\xi(\boldsymbol{\Theta}_1,\boldsymbol{\Theta}_2)$

5. **è§£ç è¾“å‡º**ï¼š

   - åŠŸèƒ½ç±»åˆ«é¢„æµ‹ $\hat{y}$ï¼šå¯¹ $\mathbf{F}_{po}$ å’Œ $\mathbf{F}_{io}$ æ± åŒ–åæ‹¼æ¥

   - 3DåŠŸèƒ½çƒ­å›¾ $\hat{\phi}$ï¼šé€šè¿‡ç‰¹å¾ä¼ æ’­å±‚ä¸Šé‡‡æ ·ï¼š
     $$
     \hat{\phi}=f_\phi(\text{FP}(\tilde{\mathbf{F}}_p)\odot\Gamma(\mathbf{F}_{po}))
     $$

#### **2. æŸå¤±å‡½æ•°**

æ€»æŸå¤±åŒ…å«ä¸‰é¡¹ï¼š

1. **åŠŸèƒ½åˆ†ç±»æŸå¤±** $\mathcal{L}_{CE}$ï¼šäº¤å‰ç†µæŸå¤±ç›‘ç£ $\hat{y}$

2. **ç‰¹å¾åˆ†å¸ƒå¯¹é½æŸå¤±** $\mathcal{L}_{KL}$ï¼šKLæ•£åº¦çº¦æŸ $\mathbf{F}_{io}$ ä¸ $\tilde{\mathbf{F}}_i$ åˆ†å¸ƒï¼š
   $$
   \mathcal{L}_{KL}=\sum_n \mathbf{F}_{io_n}\log(\epsilon+\frac{\mathbf{F}_{io_n}}{\epsilon+\tilde{\mathbf{F}}_{i_n}})
   $$

3. **çƒ­å›¾å›å½’æŸå¤±** $\mathcal{L}_{HM}$ï¼šFocal Loss + Dice Lossç›‘ç£ $\hat{\phi}$

æœ€ç»ˆæŸå¤±ä¸ºåŠ æƒå’Œï¼š
$$
\mathcal{L}_{total}=\lambda_1\mathcal{L}_{CE}+\lambda_2\mathcal{L}_{KL}+\lambda_3\mathcal{L}_{HM}
$$

#### **3. å…³é”®åˆ›æ–°**

- **JRAæ¨¡å—**ï¼šé€šè¿‡è·¨æ¨¡æ€ç›¸ä¼¼æ€§ï¼ˆ$\varphi$ï¼‰å’Œè”åˆæ³¨æ„åŠ›ï¼ˆ$f_\delta$ï¼‰å®ç°æ— å…ˆéªŒå¯¹é½

- **ARMæ¨¡å—**ï¼šé€šè¿‡åŒè·¯äº¤å‰æ³¨æ„åŠ›åˆ†åˆ«å»ºæ¨¡ç‰©ä½“-ä¸»ä½“ï¼ˆ$\boldsymbol{\Theta}_1$ï¼‰å’Œç‰©ä½“-åœºæ™¯ï¼ˆ$\boldsymbol{\Theta}_2$ï¼‰äº¤äº’

- **äº’ä¼˜åŒ–æœºåˆ¶**ï¼š$\mathcal{L}_{KL}$ ä½¿åŠŸèƒ½è¡¨å¾ä¸å¯¹é½ç‰¹å¾ç›¸äº’å¢å¼ºï¼ˆå¦‚å›¾15æ‰€ç¤ºï¼‰

## ä»£ç 

### æ•°æ®é›†

> æ•°æ®é›†ç›®å½•ä¸‹çš„ç»„ç»‡æ–¹å¼:
> ![](IAGNet/7.png)

1. æ•°æ®é›†åˆå§‹åŒ–

```python
class PIAD(Dataset):
    def __init__(self, run_type, setting_type, point_path, img_path, box_path, pair=2, img_size=(224, 224)):
        super().__init__()

        self.run_type = run_type # train/val/test
        self.p_path = point_path
        self.i_path = img_path
        self.b_path = box_path # è®°å½•ç‰©ä½“è¾¹ç•Œæ¡†
        self.pair_num = pair
        self.affordance_label_list = ['grasp', 'contain', 'lift', 'open', 
                        'lay', 'sit', 'support', 'wrapgrasp', 'pour', 'move', 'display',
                        'push', 'listen', 'wear', 'press', 'cut', 'stab']
        
        ...                

        '''
        Seen
        '''
        if setting_type == 'Seen':
            number_dict = {'Earphone': 0, 'Bag': 0, 'Chair': 0, 'Refrigerator': 0, 'Knife': 0, 'Dishwasher': 0, 'Keyboard': 0, 'Scissors': 0, 'Table': 0, 
            'StorageFurniture': 0, 'Bottle': 0, 'Bowl': 0, 'Microwave': 0, 'Display': 0, 'TrashCan': 0, 'Hat': 0, 'Clock': 0, 
            'Door': 0, 'Mug': 0, 'Faucet': 0, 'Vase': 0, 'Laptop': 0, 'Bed': 0}

        # è¯»å–å‡ºæ‰€æœ‰å›¾ç‰‡è·¯å¾„ï¼Œå­˜å‚¨äº†ç‰©ä½“è¾¹ç•Œæ¡†æ–‡ä»¶è·¯å¾„ 
        self.img_files = self.read_file(self.i_path)
        self.box_files = self.read_file(self.b_path)
        self.img_size = img_size

        if self.run_type == 'train':
           # è¯»å–å‡ºæ‰€æœ‰ç‚¹äº‘æ–‡ä»¶è·¯å¾„,åŒæ—¶è®°å½•æ¯ç±»ç‰©ä½“å…±å¯¹åº”å¤šå°‘ä¸åŒçš„ç‚¹äº‘
            self.point_files, self.number_dict = self.read_file(self.p_path, number_dict)
            self.object_list = list(number_dict.keys())
            self.object_train_split = {}
            start_index = 0
            # è®°å½•æ¯ç±»ç‰©ä½“å¯¹åº”çš„ç‚¹äº‘æ–‡ä»¶ä¸‹æ ‡ç´¢å¼•åŒºé—´
            for obj_ in self.object_list:
                temp_split = [start_index, start_index + self.number_dict[obj_]]
                self.object_train_split[obj_] = temp_split
                start_index += self.number_dict[obj_]
        else:
            self.point_files = self.read_file(self.p_path)
```
2. è·å–æ•°æ®

```python
    def __getitem__(self, index):
        # 1. è·å–å›¾ç‰‡ï¼ŒBoxæ¡†æ–‡ä»¶è·¯å¾„
        img_path = self.img_files[index]
        box_path = self.box_files[index]

        if (self.run_type=='val'):
            point_path = self.point_files[index]
        else:
            # 2. ä»æ–‡ä»¶è·¯å¾„ä¸­æå–ç‰©ä½“å
            object_name = img_path.split('_')[-3]
            # 3. ä¸€å¼ å›¾ç‰‡å¯¹åº”å¤šå¼ åŒç‰©ä½“ä½†å½¢çŠ¶ä¸åŒçš„ç‚¹äº‘å›¾ç‰‡
            range_ = self.object_train_split[object_name]
            point_sample_idx = random.sample(range(range_[0],range_[1]), self.pair_num)

        Img = Image.open(img_path).convert('RGB')

        if(self.run_type == 'train'):
            # 4. éšæœºè£å‰ªå›¾ç‰‡ï¼ŒåŒæ—¶è·å–è£å‰ªåçš„ç‰©ä½“æ¡†(äº¤äº’ä¸»ä½“æ¡†ï¼Œç›®æ ‡ç‰©ä½“æ¡†)
            Img, subject, object = self.get_crop(box_path, Img, self.run_type)
            # 5. å¯¹å›¾ç‰‡è¿›è¡Œç¼©æ”¾ï¼ŒåŒæ—¶ç­‰æ¯”ä¾‹å¯¹ç‰©ä½“æ¡†åšåŒæ ·çš„ç¼©æ”¾
            sub_box, obj_box = self.get_resize_box(Img, self.img_size, subject, object)
            sub_box, obj_box = torch.tensor(sub_box).float(), torch.tensor(obj_box).float()
            Img = Img.resize(self.img_size)
            Img = img_normalize_train(Img)
            
            Points_List = []
            affordance_label_List = []
            affordance_index_List = []
            # 6. åŠ è½½ç‚¹äº‘
            for id_x in point_sample_idx:
                point_path = self.point_files[id_x]
                Points, affordance_label = self.extract_point_file(point_path)
                Points,_,_ = pc_normalize(Points)
                Points = Points.transpose()
                affordance_label, affordance_index = self.get_affordance_label(img_path, affordance_label)
                Points_List.append(Points)
                affordance_label_List.append(affordance_label)
                affordance_index_List.append(affordance_index)

        else:
            ...

        if(self.run_type == 'train'):
            # 7. å›¾ç‰‡ï¼Œç‚¹äº‘åˆ—è¡¨ï¼Œç‚¹äº‘åŠŸèƒ½åŒºåŸŸæ©ç åˆ—è¡¨ï¼Œç‚¹äº‘åŠŸèƒ½åŒºåŸŸç´¢å¼•åˆ—è¡¨ï¼Œäº¤äº’ä¸»ä½“æ¡†ï¼Œç›®æ ‡ç‰©ä½“æ¡†
            return Img, Points_List, affordance_label_List, affordance_index_List, sub_box, obj_box
        else:
            return Img, Point, affordance_label, img_path, point_path, sub_box, obj_box
```
### æ¨¡å‹

```python
class IAG(nn.Module):
    ...        
    def forward(self, img, xyz, sub_box, obj_box):

        '''
        img: [B, 3, H, W]
        xyz: [B, 3, 2048]
        sub_box: bounding box of the interactive subject
        obj_box: bounding box of the interactive object
        '''
        
        B, C, N = xyz.size()
        ...
        # 1. ResNet18 ç¼–ç å›¾åƒ (batch,512,7,7)
        F_I = self.img_encoder(img)
        # 2. åˆ©ç”¨ROI AlignæŠ€æœ¯ï¼Œå¾—åˆ°ç›®æ ‡ç‰©ä½“åŒºåŸŸç‰¹å¾ï¼Œäº¤äº’ä¸»ä½“åŒºåŸŸç‰¹å¾ï¼ŒèƒŒæ™¯åŒºåŸŸç‰¹å¾
        ROI_box = self.get_roi_box(B).to(device)
        F_i, F_s, F_e = self.get_mask_feature(img, F_I, sub_box, obj_box, device)
        # èƒŒæ™¯åŒºåŸŸç‰¹å¾å›¾ç»è¿‡ROI Alignæ˜ å°„ä¸º4*4å¤§å°çš„ç‰¹å¾å›¾
        # ROI_box å¤§å°ä¸º 7*7 , æ­£å¥½ä¸ºresnet18æœ€åç”Ÿæˆçš„ç‰¹å¾å›¾çš„åˆ†è¾¨ç‡, å› ä¸ºèƒŒæ™¯åŒºåŸŸå¤§å°ç­‰äºç‰¹å¾å›¾å¤§å°
        F_e = roi_align(F_e, ROI_box, output_size=(4,4))
        # F_i (batch,512,4,4) , F_s (batch,512,4,4) , F_e (batch,512,4,4)

        # 3. PointNetç¼–ç ç‚¹äº‘
        # (B,3,2048) , (B,320,512) , (B,512,128) ï¼Œ (B,512,64)
        F_p_wise = self.point_encoder(xyz)
        # 4. å»ºç«‹ç›®æ ‡ç‰©ä½“å±€éƒ¨äº¤äº’åŒºåŸŸåˆ°ç‚¹äº‘å±€éƒ¨åŒºåŸŸçš„ç‰¹å¾å¯¹åº”æ˜ å°„å…³ç³» 
        # (B,80,512)
        F_j = self.JRA(F_i, F_p_wise[-1][1])
        # 5. ç»“åˆäº¤äº’ä¸»ä½“å’ŒèƒŒæ™¯ä¿¡æ¯ï¼Œæ¨å‡ºäº¤äº’åŒºåŸŸä¿¡æ¯
        # (B,80,512)
        affordance = self.ARM(F_j, F_s, F_e)
        # 6. è§£ç 
        _3daffordance, logits, to_KL = self.decoder(F_j, affordance, F_p_wise)
         
        return _3daffordance, logits, to_KL
```

å…³äºåˆ©ç”¨ROI AlignæŠ€æœ¯ï¼Œå¾—åˆ°ç›®æ ‡ç‰©ä½“åŒºåŸŸç‰¹å¾ï¼Œäº¤äº’ä¸»ä½“åŒºåŸŸç‰¹å¾ï¼ŒèƒŒæ™¯åŒºåŸŸç‰¹å¾è¿‡ç¨‹çš„å®ç°ç»†èŠ‚å¦‚ä¸‹:

```python
    def get_mask_feature(self, raw_img, img_feature, sub_box, obj_box, device):
        raw_size = raw_img.size(2)
        current_size = img_feature.size(2)
        B = img_feature.size(0)
        # 1. è®¡ç®—ç»è¿‡ä¸‹é‡‡æ ·å¾—åˆ°çš„ç‰¹å¾å›¾ç›¸æ¯”äºåŸå§‹å›¾ç‰‡çš„ç¼©å°æ¯”ä¾‹
        scale_factor = current_size / raw_size
        # 2. å°†äº¤äº’ä¸»ä½“æ¡†å’Œç›®æ ‡ç‰©ä½“æ¡†ç­‰æ¯”ä¾‹ç¼©å°
        sub_box[:, :] = sub_box[:, :] * scale_factor
        obj_box[:, :] = obj_box[:, :] * scale_factor
        
        # 3. æ ¹æ®ç›®æ ‡ç‰©ä½“æ¡†ï¼Œå°†æ©ç å›¾åƒä¸­ç›®æ ‡ç‰©ä½“æ‰€åœ¨åŒºåŸŸæ¿€æ´»ï¼Œå¾—åˆ°ç›®æ ‡ç‰©ä½“åŒºåŸŸæ©ç 
        obj_mask = torch.zeros_like(img_feature)
        obj_roi_box = []
        for i in range(B):
            obj_mask[i,:, int(obj_box[i][1]+0.5):int(obj_box[i][3]+0.5), int(obj_box[i][0]+0.5):int(obj_box[i][2]+0.5)] = 1
            roi_obj = [obj_box[i][0], obj_box[i][1], obj_box[i][2]+0.5, obj_box[i][3]]  # å¯¹äº¤äº’ä¸»ä½“æ¡†ä½ç½®è¿›è¡Œç²¾ç»†è°ƒæ•´(just a trick)
            roi_obj.insert(0, i) # æ’å…¥æ‰¹æ¬¡ç´¢å¼• -- ROI Alignå¯¹é½æ–¹æ³•éœ€è¦
            obj_roi_box.append(roi_obj)
        obj_roi_box = torch.tensor(obj_roi_box).float().to(device)

        sub_roi_box = []
        # 4. æ ¹æ®äº¤äº’ä¸»ä½“æ¡†ï¼Œåœ¨ç›®æ ‡ç‰©ä½“åŒºåŸŸæ©ç ä¹‹ä¸Šï¼Œæ¿€æ´»äº¤äº’ä¸»ä½“æ‰€åœ¨åŒºåŸŸ
        Scene_mask = obj_mask.clone()
        for i in range(B):
            Scene_mask[i,:, int(sub_box[i][1]+0.5):int(sub_box[i][3]+0.5), int(sub_box[i][0]+0.5):int(sub_box[i][2]+0.5)] = 1
            roi_sub = [sub_box[i][0], sub_box[i][1], sub_box[i][2], sub_box[i][3]]
            roi_sub.insert(0,i)
            sub_roi_box.append(roi_sub)
        # 5. å€ŸåŠ©å–åæ¿€æ´»å›¾ç‰‡èƒŒæ™¯åŒºåŸŸ    
        Scene_mask = torch.abs(Scene_mask - 1)
        # 6. æ‹¿åˆ°å›¾ç‰‡èƒŒæ™¯åŒºåŸŸç‰¹å¾å›¾
        Scene_mask_feature = img_feature * Scene_mask
        sub_roi_box = torch.tensor(sub_roi_box).float().to(device)
        # 7. åˆ©ç”¨ROI AlignæŠ€æœ¯ï¼Œå°†ç›®æ ‡ç‰©ä½“åŒºåŸŸæ¡†åœ¨ç‰¹å¾å›¾ä¸­æ¡†å‡ºçš„åŒºåŸŸï¼Œæ˜ å°„ä¸º4*4å¤§å°çš„ç‰¹å¾å›¾
        obj_feature = roi_align(img_feature, obj_roi_box, output_size=(4,4), sampling_ratio=4)
        # 8. åˆ©ç”¨ROI AlignæŠ€æœ¯ï¼Œå°†äº¤äº’ä¸»ä½“åŒºåŸŸæ¡†åœ¨ç‰¹å¾å›¾ä¸­æ¡†å‡ºçš„åŒºåŸŸï¼Œæ˜ å°„ä¸º4*4å¤§å°çš„ç‰¹å¾å›¾
        sub_feature = roi_align(img_feature, sub_roi_box, output_size=(4,4), sampling_ratio=4) 
        # 9. è¿”å›ç›®æ ‡ç‰©ä½“åŒºåŸŸç‰¹å¾å›¾ï¼Œäº¤äº’ä¸»ä½“åŒºåŸŸç‰¹å¾å›¾ï¼ŒèƒŒæ™¯åŒºåŸŸç‰¹å¾å›¾(æœªç»ROI Alignè¿›è¡Œæ˜ å°„)
        return obj_feature, sub_feature, Scene_mask_feature
```

JRA æ¨¡å—å°±æ˜¯åœ¨å›¾åƒå’Œç‚¹äº‘ä¹‹é—´å»ºç«‹â€œå±€éƒ¨åŒºåŸŸçº§åˆ«â€çš„å¯¹åº”å…³ç³»ï¼Œè®© 2D äº¤äº’ä¿¡å·èƒ½å‡†ç¡®è½åˆ° 3D å¯¹è±¡ä¸Šï¼›å…·ä½“æ¥è¯´:

1. å›¾åƒåˆ†æ”¯æå–åˆ°çš„æ˜¯ 2D ç›®æ ‡åŒºåŸŸçš„ç‰¹å¾ï¼ŒåŒ…å«äº†äº¤äº’æç¤ºï¼ˆæ¯”å¦‚â€œäººæ‰‹æ¥è§¦æ¯å­çš„è¾¹ç¼˜â€ï¼‰ï¼›

2. ç‚¹äº‘åˆ†æ”¯æå–åˆ°çš„æ˜¯ 3D ç‰©ä½“ç‚¹äº‘çš„åŒºåŸŸç‰¹å¾ï¼ŒåŒ…å«äº†å‡ ä½•ç»“æ„ï¼ˆæ¯”å¦‚â€œæ¯å­çš„è¾¹ç¼˜æ›²é¢â€ï¼‰ï¼›

3. JRA æ¨¡å—é€šè¿‡ " æŠ•å½±ç»Ÿä¸€ â†’ è·¨æ¨¡æ€ç›¸ä¼¼æ€§åŒ¹é… â†’ å±€éƒ¨è‡ªæ³¨æ„åŠ› â†’ å…¨å±€è‡ªæ³¨æ„åŠ› " ï¼Œåœ¨å…±äº«ç©ºé—´é‡ŒæŠŠå›¾åƒçš„å±€éƒ¨åŒºåŸŸå’Œç‚¹äº‘çš„å±€éƒ¨åŒºåŸŸå¯¹åº”èµ·æ¥ï¼›

4. è¿™æ ·ï¼Œæ¨¡å‹å°±èƒ½ç†è§£â€œå›¾åƒé‡Œäº¤äº’çš„è¿™éƒ¨åˆ† â†’ ç‚¹äº‘é‡Œå¯¹åº”çš„è¿™éƒ¨åˆ†ç»“æ„â€ï¼Œä¸ºåç»­ 3D affordance grounding æä¾›æ”¯æ’‘ã€‚

```python
class Joint_Region_Alignment(nn.Module):
    def __init__(self, emb_dim = 512, num_heads = 4):
        super().__init__()
        class SwapAxes(nn.Module):
            def __init__(self):
                super().__init__()
            
            def forward(self, x):
                return x.transpose(1, 2)
        self.emb_dim = emb_dim
        self.div_scale = self.emb_dim ** (-0.5)
        self.num_heads = num_heads

        self.to_common = nn.Sequential(
            nn.Conv1d(self.emb_dim, 2*self.emb_dim, 1, 1),
            nn.BatchNorm1d(2*self.emb_dim),
            nn.ReLU(),
            nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1),
            nn.BatchNorm1d(self.emb_dim),
            nn.ReLU()         
        )

        self.i_atten = Inherent_relation(self.emb_dim, self.num_heads)
        self.p_atten = Inherent_relation(self.emb_dim, self.num_heads)
        self.joint_atten = Inherent_relation(self.emb_dim, self.num_heads)

    def forward(self, F_i, F_p):
        '''
        i_feature: [B, C, H, W]
        p_feature: [B, C, N_p]
        HW = N_i
        '''
        
        B,_,N_p = F_p.size() # (B,512,64)
        # 1. ç‰©ä½“åŒºåŸŸç‰¹å¾å›¾å±•å¹³: (B,512,4,4) --> (B,512,4*4)
        F_i = F_i.view(B, self.emb_dim, -1)                                             #[B, C, N_i]
        
        # 2. é€šè¿‡å…±äº«MLPè¿«ä½¿å›¾åƒå’Œç‚¹äº‘ç‰¹å¾åœ¨ç›¸åŒç©ºé—´åˆ†å¸ƒï¼Œæ¶ˆé™¤æ¨¡æ€å·®å¼‚
        I = self.to_common(F_i) # (B,512,16)
        P = self.to_common(F_p) # (B,512,64)
        
        # 3. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ: (B,64,512) * (B,512,16) = (B,64,16)
        phi = torch.bmm(P.permute(0, 2, 1), I)*self.div_scale                           #[B, N_p, N_i]
        phi_p = F.softmax(phi,dim=1) # è®¡ç®—ç‰¹å¾å›¾ä¸­æ¯ä¸ªç‚¹å’Œç‚¹äº‘æ¯ä¸ªç‚¹ç‰¹å¾çš„ç›¸ä¼¼åº¦
        phi_i = F.softmax(phi,dim=-1) # è®¡ç®—ç‚¹äº‘ä¸­æ¯ä¸ªç‚¹å’Œç‰¹å¾å›¾ä¸­æ¯ä¸ªç‚¹ç‰¹å¾çš„ç›¸ä¼¼åº¦

        # 4. ç‰¹å¾å¢å¼º(æŒ‰ç…§ç›¸ä¼¼åº¦å®Œæˆä¿¡æ¯èåˆ + è‡ªæ³¨æ„åŠ›å®Œæˆå†…éƒ¨ä¿¡æ¯å»ºæ¨¡)
        I_enhance = torch.bmm(P, phi_p)  # (B,512,64) * (B,64,16) = ï¼ˆB,512,16ï¼‰          [B, C, N_i]
        P_enhance = torch.bmm(I, phi_i.permute(0,2,1)) # (B,512,16) * (B,16,64) = ï¼ˆB,512,64ï¼‰  [B, C, N_p]
        I_ = self.i_atten(I_enhance.mT)                                                 #[B, N_i, C]
        P_ = self.p_atten(P_enhance.mT)                                                 #[B, N_p, C]
        # I_ (B,16,512) , P_ (B,64,512)
        
        # 5. è”åˆå»ºæ¨¡: æ‹¼æ¥ (B,80,512)  +  è‡ªæ³¨æ„åŠ›
        joint_patch = torch.cat((P_, I_), dim=1)                                       
        F_j = self.joint_atten(joint_patch)                                             #[B, N_p+N_i, C]

        return F_j
```

![](IAGNet/6.png)

ARM çš„ä»»åŠ¡æ˜¯ï¼š**åœ¨å¯¹é½åçš„ joint feature åŸºç¡€ä¸Šï¼Œèåˆäº¤äº’ä¸»ä½“å’Œç¯å¢ƒçš„è¯­ä¹‰çº¿ç´¢ï¼Œæ˜¾å¼åœ°â€œæ­ç¤ºâ€å‡ºç‰©ä½“ä¸Šå¯èƒ½çš„ affordance åŒºåŸŸã€‚**

æ¢å¥è¯è¯´ï¼Œå®ƒè¦å›ç­”ï¼š

> â€œåœ¨è¿™ä¸ªäº¤äº’åœºæ™¯é‡Œï¼Œç‰©ä½“çš„å“ªä¸€éƒ¨åˆ†å› ä¸ºä¸»ä½“å’Œç¯å¢ƒçš„ä½œç”¨è€Œå…·å¤‡å¯äº¤äº’æ½œèƒ½ï¼Ÿâ€

1. **ä¸Šä¸‹æ–‡æ³¨å…¥ (Context injection)**

   * `F_s` æä¾›â€œè°åœ¨å’Œç‰©ä½“äº¤äº’â€ï¼ˆæ¯”å¦‚äººæ‰‹/æ‰‹è‡‚ï¼‰
  
   * `F_e` æä¾›â€œäº¤äº’å‘ç”Ÿçš„åœºæ™¯èƒŒæ™¯â€
  
   * å°†è¿™äº›ä¿¡æ¯å’Œ `F_j` èåˆï¼Œå¯ä»¥é¿å…ä»…å‡­ç‰©ä½“å‡ ä½•å»çŒœ affordanceã€‚

2. **æ˜¾å¼åŒºåŸŸæŒ–æ˜ (Explicit affordance mining)**

   * å¯¹é½ç‰¹å¾ `F_j` å·²ç»æŠŠâ€œå›¾åƒäº¤äº’æç¤ºåŒºåŸŸ â†” ç‚¹äº‘å‡ ä½•å±€éƒ¨â€å¯¹åº”èµ·æ¥ï¼Œä½†è¿˜æ²¡æœ‰æ˜ç¡®è¯´â€œè¿™é‡Œå°±æ˜¯ affordance åŒºåŸŸâ€ã€‚
  
   * ARM è¿›ä¸€æ­¥å¤„ç†åï¼Œè¾“å‡ºä¸€ä¸ªæ›´æŠ½è±¡çš„ **affordance è¯­ä¹‰è¡¨ç¤º**ï¼Œå‘Šè¯‰ decoder å“ªäº›åŒºåŸŸåº”è¯¥è¢«æ¿€æ´»ã€‚

3. **è¾“å‡º â†’ é€å…¥ Decoder**

   * `affordance` è¢«é€è¿› `self.decoder(F_j, affordance, F_p_wise)`
  
   * Decoder å†ç»“åˆåŸå§‹ç‚¹äº‘é€ç‚¹ç‰¹å¾ï¼ŒæŠŠè¿™äº›æŠ½è±¡è¯­ä¹‰è½¬åŒ–ä¸º **ç‚¹çº§åˆ«çš„ affordance mask**ã€‚

å¯ä»¥æŠŠ **JRA + ARM** çš„å…³ç³»æƒ³æˆï¼š

* **JRA**ï¼šå¸®ä½ åœ¨â€œäº¤äº’å›¾ç‰‡é‡Œçš„åŒºåŸŸâ€ å’Œ â€œç‚¹äº‘é‡Œçš„å‡ ä½•éƒ¨åˆ†â€ä¹‹é—´æ‹‰äº†ä¸€æ ¹çº¿ï¼ˆå¯¹é½ï¼‰ã€‚

* **ARM**ï¼šåœ¨è¿™æ ¹çº¿çš„ä¸¤ç«¯åŠ ä¸Šâ€œè¯­ä¹‰ç”µæµâ€ï¼ˆä¸»ä½“ & èƒŒæ™¯ï¼‰ï¼Œè®©ç½‘ç»œæ˜ç¡®çŸ¥é“å“ªäº›åŒºåŸŸçœŸæ­£å…·å¤‡ **affordance**ã€‚

**ARM æ¨¡å—çš„ä½œç”¨æ˜¯å°† JRA å¯¹é½å¾—åˆ°çš„å›¾åƒâ€“ç‚¹äº‘è”åˆç‰¹å¾ï¼Œä¸äº¤äº’ä¸»ä½“å’Œç¯å¢ƒè¯­ä¹‰ç»“åˆï¼ŒæŒ–æ˜å¹¶ç”Ÿæˆæ˜¾å¼çš„ affordance è¡¨ç¤ºï¼Œä¸º Decoder è¾“å‡ºé€ç‚¹ affordance mask æä¾›è¯­ä¹‰æŒ‡å¯¼ã€‚**

```python
class Affordance_Revealed_Module(nn.Module):
    def __init__(self, emb_dim, proj_dim):
        class SwapAxes(nn.Module):
            def __init__(self):
                super().__init__()
            
            def forward(self, x):
                return x.transpose(1, 2)
        super().__init__()
        self.emb_dim = emb_dim
        self.proj_dim = proj_dim
        self.cross_atten = Cross_Attention(emb_dim = self.emb_dim, proj_dim = self.proj_dim)
        self.fusion = nn.Sequential(
            nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1),
            nn.BatchNorm1d(self.emb_dim),
            nn.ReLU()
        )

    def forward(self, F_j, F_s, F_e):

        '''
        F_j: [B, N_p + N_i, C]  (B,80,512) ç‰©ä½“åŒºåŸŸç‰¹å¾å’Œç‚¹äº‘ç‰¹å¾çš„è”åˆå»ºæ¨¡
        F_s: [B, H, W, C]  (B,512,4,4)  äº¤äº’åŒºåŸŸç‰¹å¾
        F_e: [B, H, W, C]  (B,512,4,4)  èƒŒæ™¯ç‰¹å¾
        '''

        B,_,C = F_j.size()

        # æ‹‰å¹³: (B,512,4,4) --> (B,512,4*4)
        F_s = F_s.view(B, C, -1)                                        #[B, N_i, C]
        F_e = F_e.view(B, C, -1)                                        #[B, N_i, C]
        # åˆ©ç”¨è”åˆå»ºæ¨¡ç‰¹å¾ä½œä¸ºqueryï¼Œä»ç›®æ ‡ä¸»ä½“åŒºåŸŸç‰¹å¾å’ŒèƒŒæ™¯ç‰¹å¾ä¸­æå–ç›¸å…³ä¿¡æ¯åˆ†åˆ«å•ç‹¬åŠ åˆ°è‡ªå·±èº«ä¸Š
        Theta_1, Theta_2 = self.cross_atten(F_j, F_s.mT, F_e.mT)        #[B, C, N_p + N_i]

        # é€šé“ç»´åº¦å®Œæˆæ‹¼æ¥åï¼Œåˆ©ç”¨1x1å·ç§¯å®Œæˆé€šé“ç»´åº¦ä¸Šçš„ä¿¡æ¯èåˆ 
        joint_context = torch.cat((Theta_1.mT, Theta_2.mT), dim=1)      #[B, 2C, N_p + N_i]
        affordance = self.fusion(joint_context)                         #[B, C, N_p + N_i]
        affordance = affordance.permute(0, 2, 1)                        #[B, N_p + N_i, C]

        return affordance # ï¼ˆB,80,512)
```
Decoder å°±æ˜¯æŠŠå‰é¢ **JRA + ARM å¾—åˆ°çš„æŠ½è±¡ç‰¹å¾**ï¼Œä¸€æ­¥æ­¥è¿˜åŸåˆ° **åŸå§‹ç‚¹äº‘çš„æ¯ä¸ªç‚¹**ï¼Œæœ€åç»™å‡ºä¸¤ä¸ªç»“æœï¼š

1. **å…¨å±€å±‚é¢**ï¼šé¢„æµ‹è¿™ä¸ªç‰©ä½“æ•´ä½“å…·æœ‰ä»€ä¹ˆäº¤äº’åŠŸèƒ½ï¼ˆæ¯”å¦‚â€œèƒ½æ‹¿â€â€œèƒ½åâ€â€œèƒ½æ”¾ä¸œè¥¿â€ï¼‰ã€‚

2. **å±€éƒ¨å±‚é¢**ï¼šå‘Šè¯‰ä½ ç‰©ä½“è¡¨é¢ **å“ªäº›ç‚¹**æ˜¯å’Œäº¤äº’æœ‰å…³çš„åŒºåŸŸï¼ˆä¹Ÿå°±æ˜¯ 3D affordance maskï¼‰ã€‚

æ‰€ä»¥å®ƒåŒæ—¶è´Ÿè´£ **â€œå¤§æ–¹å‘â€ + â€œå…·ä½“éƒ¨ä½â€** çš„é¢„æµ‹ã€‚

å‰å‘ä¼ æ’­æµç¨‹:

1. **ç‰¹å¾å¯¹é½åçš„è¾“å…¥**

   * JRA å·²ç»è®©å›¾åƒåŒºåŸŸå’Œç‚¹äº‘å±€éƒ¨å¯¹é½ï¼ŒARM åˆèå…¥äº†ä¸»ä½“å’Œç¯å¢ƒçš„è¯­ä¹‰ï¼Œå¾—åˆ°â€œå¸¦äº¤äº’è¯­ä¹‰çš„è”åˆç‰¹å¾â€ã€‚
   
   * Decoder æ¥æ”¶è¿™äº›ç‰¹å¾ + ç¼–ç å™¨é€å±‚ä¿å­˜çš„ç‚¹äº‘ç‰¹å¾ã€‚

2. **é€å±‚ä¸Šé‡‡æ ·ï¼ˆæ”¾å¤§è¿˜åŸï¼‰**

   * å‰é¢çš„ç‰¹å¾æ¯”è¾ƒæŠ½è±¡ã€åˆ†è¾¨ç‡ä½ï¼ˆåªå¯¹åº”å°‘é‡ç‚¹ï¼‰ã€‚
  
   * Decoder é€šè¿‡ PointNet çš„ä¸Šé‡‡æ ·æœºåˆ¶ï¼ŒæŠŠè¿™äº›ç‰¹å¾ä¸€æ­¥æ­¥â€œè¿˜åŸâ€åˆ°åŸå§‹ç‚¹äº‘çš„ 2048 ä¸ªç‚¹ä¸Šã€‚
  
   * è¿™æ ·ï¼Œæ¯ä¸ªç‚¹éƒ½æœ‰äº†å’Œäº¤äº’è¯­ä¹‰ç›¸å…³çš„æè¿°ã€‚

3. **å…¨å±€æ±‡æ€»ï¼ˆæ•´ä½“åˆ†ç±»ï¼‰**

   * å¯¹æ‰€æœ‰ç‚¹çš„ç‰¹å¾åšæ± åŒ–ï¼Œç›¸å½“äºâ€œå‹ç¼©æˆä¸€å¥æ€»ç»“â€ã€‚
  
   * è¿™ä¸€æ­¥ç”¨æ¥é¢„æµ‹ç‰©ä½“æ•´ä½“çš„ affordance ç±»å‹ã€‚

4. **é€ç‚¹é¢„æµ‹ï¼ˆå±€éƒ¨æ©ç ï¼‰**

   * åŒæ—¶ï¼ŒDecoder æŠŠå…¨å±€çš„äº¤äº’è¯­ä¹‰å†ä¼ æ’­å›æ¯ä¸€ä¸ªç‚¹ã€‚

   * æ¯ä¸ªç‚¹å¾—åˆ°ä¸€ä¸ªæ¦‚ç‡å€¼ï¼šæ˜¯å¦å±äºå¯äº¤äº’åŒºåŸŸã€‚
   
   * è¾“å‡ºå°±æ˜¯ä¸€ä¸ªç‚¹äº‘ä¸Šçš„ **çƒ­åŠ›å›¾**ï¼Œé«˜å€¼åŒºåŸŸå°±æ˜¯ affordance éƒ¨ä½ã€‚

```python
class Decoder(nn.Module):
    def __init__(self, additional_channel, emb_dim, N_p, N_raw, num_affordance):
        """
        Decoder æ¨¡å—
        å‚æ•°:
            additional_channel: é™„åŠ è¾“å…¥é€šé“æ•°
            emb_dim: ç‰¹å¾åµŒå…¥ç»´åº¦
            N_p: ç‚¹äº‘å­é›†æ•°é‡ (point number for part/point-level alignment)
            N_raw: åŸå§‹ç‚¹äº‘ç‚¹æ•°
            num_affordance: affordance åˆ†ç±»æ•°é‡
        """

        class SwapAxes(nn.Module):
            """äº¤æ¢å¼ é‡çš„ç¬¬1ç»´å’Œç¬¬2ç»´, ç”¨äºLinear/BNçš„ç»´åº¦åŒ¹é…"""
            def __init__(self):
                super().__init__()
            
            def forward(self, x):
                # x: [B, N, C] -> [B, C, N]
                return x.transpose(1, 2)

        super().__init__()
        
        self.emb_dim = emb_dim
        self.N_p = N_p
        self.N = N_raw
        self.num_affordance = num_affordance

        # ---------- ç‰¹å¾ä¼ æ’­å±‚ (PointNet++ Feature Propagation) ----------
        # é€å±‚ä¸Šé‡‡æ ·ï¼Œå°† encoder è¾“å‡ºçš„å±‚æ¬¡åŒ–ç‚¹ç‰¹å¾æ¢å¤åˆ°åŸå§‹ç‚¹æ•° N
        self.fp3 = PointNetFeaturePropagation(in_channel=512+self.emb_dim, mlp=[768, 512])  
        self.fp2 = PointNetFeaturePropagation(in_channel=832, mlp=[768, 512]) 
        self.fp1 = PointNetFeaturePropagation(in_channel=518+additional_channel, mlp=[512, 512]) 

        # å…¨å±€æ± åŒ– (ç”¨äº part-level å’Œ image-level ç‰¹å¾å‹ç¼©)
        self.pool = nn.AdaptiveAvgPool1d(1)   # è¾“å…¥ [B, C, N] -> è¾“å‡º [B, C, 1]

        # ---------- è¾“å‡ºå¤´ (3D affordance é¢„æµ‹) ----------
        self.out_head = nn.Sequential(
            nn.Linear(self.emb_dim, self.emb_dim // 8),  # [B, N, C] -> [B, N, C/8]
            SwapAxes(),                                  # [B, N, C/8] -> [B, C/8, N] æ–¹ä¾¿ BatchNorm1d
            nn.BatchNorm1d(self.emb_dim // 8),
            nn.ReLU(),
            SwapAxes(),                                  # [B, C/8, N] -> [B, N, C/8]
            nn.Linear(self.emb_dim // 8, 1),             # [B, N, C/8] -> [B, N, 1]
        )

        # ---------- åˆ†ç±»å¤´ (affordance åˆ†ç±») ----------
        self.cls_head = nn.Sequential(
            nn.Linear(2*self.emb_dim, self.emb_dim // 2),      # [B, 2C] -> [B, C/2]
            nn.BatchNorm1d(self.emb_dim // 2),
            nn.ReLU(),
            nn.Linear(self.emb_dim // 2, self.num_affordance), # [B, C/2] -> [B, num_affordance]
            nn.BatchNorm1d(self.num_affordance)
        )

        self.sigmoid = nn.Sigmoid()


    def forward(self, F_j, affordance, encoder_p):
        """
        å‰å‘ä¼ æ’­
        è¾“å…¥:
            F_j: [B, N_p + N_i, C]  (joint features, part/image å¯¹é½åçš„ç‰¹å¾)
            affordance: [B, N_p + N_i, C] (affordance ç‰¹å¾)
            encoder_p: [p0, p1, p2, p3] (encoder åˆ†å±‚ç‰¹å¾, PointNet++ è¾“å‡º)
        è¾“å‡º:
            _3daffordance: [B, N, 1]  (ç‚¹äº‘æ¯ä¸ªç‚¹çš„ affordance æ¿€æ´»æ¦‚ç‡)
            logits: [B, num_affordance] (å…¨å±€ affordance åˆ†ç±»ç»“æœ)
            [F_ia^T, I_align^T]: [B, C, N_i], [B, C, N_i] (image-aligned features)
        """

        B, _, _ = F_j.size()
        p_0, p_1, p_2, p_3 = encoder_p

        # --- å°† joint feature æ‹†æˆ part-aligned (P_align) å’Œ image-aligned (I_align) ---
        P_align, I_align = torch.split(F_j, split_size_or_sections=self.N_p, dim=1)     
        # P_align: [B, N_p, C]
        # I_align: [B, N_i, C]

        # --- å°† affordance ç‰¹å¾æ‹†åˆ†ä¸º part-level å’Œ image-level ---
        F_pa, F_ia = torch.split(affordance, split_size_or_sections=self.N_p, dim=1)  
        # F_pa: [B, N_p, C]
        # F_ia: [B, N_i, C]

        # --- ä¸Šé‡‡æ ·ç‰¹å¾ (é€çº§æ¢å¤ç‚¹äº‘åˆ†è¾¨ç‡) ---
        # p_k: [ç‚¹åæ ‡, ç‚¹ç‰¹å¾]
        up_sample = self.fp3(p_2[0], p_3[0], p_2[1], P_align.mT)                        
        # P_align.mT: [B, C, N_p]  -> èåˆåˆ° SA2 å±‚ç‚¹æ•° (npoint_sa2)
        # è¾“å‡º: [B, C, npoint_sa2]

        up_sample = self.fp2(p_1[0], p_2[0], p_1[1], up_sample)                         
        # è¾“å‡º: [B, C, npoint_sa1]

        up_sample = self.fp1(p_0[0], p_1[0], torch.cat([p_0[0], p_0[1]],1), up_sample)  
        # è¾“å‡º: [B, C, N]  (æ¢å¤åˆ°åŸå§‹ç‚¹æ•° N)

        # --- å…¨å±€æ± åŒ– (part/image ç‰¹å¾æ± åŒ–) ---
        F_pa_pool = self.pool(F_pa.mT)   # [B, C, N_p] -> [B, C, 1]
        F_ia_pool = self.pool(F_ia.mT)   # [B, C, N_i] -> [B, C, 1]

        # --- åˆ†ç±»é¢„æµ‹ (å…¨å±€ affordance ç±»åˆ«é¢„æµ‹) ---
        logits = torch.cat((F_pa_pool, F_ia_pool), dim=1)   # [B, 2C, 1]
        logits = self.cls_head(logits.view(B,-1))           # [B, num_affordance]

        # --- 3D affordance åŒºåŸŸé¢„æµ‹ (ç‚¹çº§åˆ«) ---
        # ä¸ªäººç†è§£: å°†å…¨å±€è¯­ä¹‰ä¿¡æ¯æ³¨å…¥åˆ°æ¯ä¸ªç‚¹çš„ç‰¹å¾ä¸­ï¼Œä»è€Œä½¿å¾—æ¯ä¸ªç‚¹çš„ç‰¹å¾ä¸ä»…åŒ…å«å±€éƒ¨å‡ ä½•ä¿¡æ¯ï¼Œè¿˜èåˆäº†å…¨å±€è¯­ä¹‰ä¿¡æ¯ã€‚è¿™ä¸€æ­¥éª¤æœ‰åŠ©äºæé«˜æ¨¡å‹å¯¹ç‰©ä½“äº¤äº’åŠŸèƒ½çš„ç†è§£å’Œé¢„æµ‹èƒ½åŠ›ã€‚ 
        _3daffordance = up_sample * F_pa_pool.expand(-1, -1, self.N)  
        # up_sample: [B, C, N]
        # F_pa_pool: [B, C, 1] -> expand: [B, C, N]
        # ç›¸ä¹˜: [B, C, N]

        _3daffordance = self.out_head(_3daffordance.mT)  
        # _3daffordance.mT: [B, N, C]
        # out_head: [B, N, 1]

        _3daffordance = self.sigmoid(_3daffordance)         
        # [B, N, 1], æ¯ä¸ªç‚¹çš„ affordance æ¦‚ç‡ (0~1)

        # è¿”å›:
        #   - ç‚¹çº§åˆ« 3D affordance mask
        #   - å…¨å±€ affordance åˆ†ç±»ç»“æœ
        #   - image-aligned çš„ç‰¹å¾
        return _3daffordance, logits, [F_ia.mT.contiguous(), I_align.mT.contiguous()]
```

æ ¸å¿ƒè®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹:

```python
def main(opt, dict):
    ...

    # ---------- åˆå§‹åŒ–æ¨¡å‹ ----------
    model = get_IAGNet(
        img_model_path=dict['res18_pre'],  # ResNet18é¢„è®­ç»ƒæƒé‡è·¯å¾„
        N_p=dict['N_p'],                    # point cloud subset size
        emb_dim=dict['emb_dim'],            # embedding dimension
        proj_dim=dict['proj_dim'],          # æŠ•å½±ç»´åº¦
        num_heads=dict['num_heads'],        # attentionå¤´æ•°
        N_raw=dict['N_raw'],                # åŸå§‹ç‚¹äº‘ç‚¹æ•°
        num_affordance=dict['num_affordance']  # affordanceç±»åˆ«æ•°
    )

    # ---------- æŸå¤±å‡½æ•° ----------
    criterion_hm = HM_Loss()                # heatmap loss: ç”¨äºç‚¹çº§åˆ« 3D affordance mask
    criterion_ce = nn.CrossEntropyLoss()    # å…¨å±€åˆ†ç±»æŸå¤±ï¼Œç”¨äº affordance åˆ†ç±»

    ...

    '''
    Training Loop
    '''
    for epoch in range(start_epoch+1, dict['Epoch']):
        ...
        for i, (img, points, labels, logits_labels, sub_box, obj_box) in enumerate(train_loader):
            # img: [B, 3, H, W]           å›¾åƒ
            # points: [B, 3, N_raw]       åŸå§‹ç‚¹äº‘
            # labels: [B, N, 1]           ç‚¹çº§åˆ«3D affordance mask
            # logits_labels: [B, num_affordance] å…¨å±€åˆ†ç±»æ ‡ç­¾
            # sub_box / obj_box: äº¤äº’ä¸»ä½“å’Œç‰©ä½“çš„bounding box

            temp_loss = 0.0

            # æœ‰äº›è®­ç»ƒå®ç°ä¸­ä¼šæŒ‰ batch å†…æ¯ä¸ªæ ·æœ¬å¾ªç¯å¤„ç†
            for point, label, logits_label in zip(points, labels, logits_labels):
                # point: [3, N_raw] å•ä¸ªæ ·æœ¬ç‚¹äº‘
                # label: [N, 1] å•ä¸ªæ ·æœ¬ç‚¹çº§mask
                # logits_label: [num_affordance] å•ä¸ªæ ·æœ¬å…¨å±€æ ‡ç­¾

                # ---------- æ¨¡å‹å‰å‘ä¼ æ’­ ----------
                _3d, logits, to_KL = model(img, point, sub_box, obj_box)
                # _3d: [N, 1] ç‚¹çº§åˆ«3D affordanceé¢„æµ‹
                # logits: [num_affordance] å…¨å±€ affordance åˆ†ç±»é¢„æµ‹
                # to_KL: [F_ia, I_align] ä¸­é—´ç‰¹å¾ï¼Œç”¨äº KL æ•£åº¦æ­£åˆ™åŒ–

                # ---------- æŸå¤±è®¡ç®— ----------
                loss_hm = criterion_hm(_3d, label)           # ç‚¹çº§mask loss
                loss_ce = criterion_ce(logits, logits_label) # å…¨å±€åˆ†ç±» loss
                loss_kl = kl_div(to_KL[0], to_KL[1])         # KLæ•£åº¦ lossï¼Œæ­£åˆ™åŒ–ç‰¹å¾å¯¹é½

                # ---------- æ€»æŸå¤± ----------
                # temp_loss = heatmap loss + åˆ†ç±» loss + KL loss (å¯åŠ æƒ)
                temp_loss += loss_hm + opt.loss_cls * loss_ce + opt.loss_kl * loss_kl

            # ---------- åå‘ä¼ æ’­ ----------
            temp_loss.backward()  # è®¡ç®—æ¢¯åº¦
            optimizer.step()      # æ›´æ–°æ¨¡å‹å‚æ•°
            optimizer.zero_grad() # æ¸…ç©ºæ¢¯åº¦ï¼Œå‡†å¤‡ä¸‹ä¸€æ­¥

            # ç´¯è®¡æŸå¤±ï¼Œç”¨äºè®°å½•
            loss_sum += temp_loss.item()

    ...
```

> ä»¥ä¸‹å†…å®¹å¾…ç»§ç»­æ€è€ƒ...

IAGNet æ¨¡å‹å®ç°ä¸»è¦ç”¨åˆ°çš„ä»¥ä¸‹ä¸‰ä¸ªæŸå¤±å‡½æ•°:

1ï¸âƒ£ **Heatmap Loss (HM_Loss)**

**ä½œç”¨**ï¼šç”¨äºç‚¹çº§åˆ« 3D affordance mask çš„ç›‘ç£ï¼ŒæŒ‡å¯¼æ¨¡å‹é¢„æµ‹ç‚¹äº‘ä¸Šå“ªäº›ç‚¹å±äºå¯äº¤äº’åŒºåŸŸã€‚

**è¾“å…¥è¾“å‡º**ï¼š

* é¢„æµ‹ï¼š`_3daffordance`ï¼Œæ¯ä¸ªç‚¹çš„æ¦‚ç‡ `[B, N, 1]`

* æ ‡ç­¾ï¼š`label`ï¼Œæ¯ä¸ªç‚¹çš„çœŸå® affordance mask `[B, N, 1]`

**åŸç†**ï¼šé€šå¸¸æ˜¯ **MSE æˆ– L2 æŸå¤±**ï¼Œè®¡ç®—é¢„æµ‹æ¦‚ç‡å’ŒçœŸå® mask ä¹‹é—´çš„å·®å¼‚ï¼š

$$
\text{loss}_{hm} = \frac{1}{N} \sum_{i=1}^N (\hat{y}_i - y_i)^2
$$

**ç›´è§‚ç†è§£**ï¼šè®©æ¨¡å‹é€ç‚¹å­¦ä¹ â€œè¿™ä¸ªç‚¹æ˜¯ä¸æ˜¯ affordance åŒºåŸŸâ€ã€‚

---

2ï¸âƒ£ **Cross-Entropy Loss (CE Loss)**

**ä½œç”¨**ï¼šç”¨äºå…¨å±€ affordance åˆ†ç±»ï¼ˆåˆ¤æ–­ç‰©ä½“æ•´ä½“èƒ½åšä»€ä¹ˆäº¤äº’åŠ¨ä½œï¼‰ã€‚

**è¾“å…¥è¾“å‡º**ï¼š

* é¢„æµ‹ï¼š`logits` `[B, num_affordance]`

* æ ‡ç­¾ï¼š`logits_labels` `[B, num_affordance]`ï¼ˆone-hot æˆ– class indexï¼‰

**åŸç†**ï¼šæ ‡å‡†äº¤å‰ç†µæŸå¤±ï¼Œè¡¡é‡é¢„æµ‹ç±»åˆ«åˆ†å¸ƒå’ŒçœŸå®åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼š

$$
\text{loss}_{ce} = - \sum_{c=1}^{num\_aff} y_c \log(\hat{p}_c)
$$

**ç›´è§‚ç†è§£**ï¼šè®©æ¨¡å‹å­¦ä¼šé¢„æµ‹ç‰©ä½“æ•´ä½“çš„ affordance ç±»åˆ«ã€‚

---

3ï¸âƒ£ **KL Divergence Loss (KL Loss)**

**ä½œç”¨**ï¼šç”¨äºæ­£åˆ™åŒ– **JRA è¾“å‡ºçš„è”åˆç‰¹å¾**ï¼Œè®©å›¾åƒåŒºåŸŸç‰¹å¾å’Œç‚¹äº‘åŒºåŸŸç‰¹å¾åœ¨ç‰¹å¾ç©ºé—´å¯¹é½å¾—æ›´ä¸€è‡´ã€‚

**è¾“å…¥è¾“å‡º**ï¼š

* `to_KL = [F_ia, I_align]`ï¼Œåˆ†åˆ«æ˜¯å›¾åƒå¯¹é½ç‰¹å¾å’Œç‚¹äº‘å¯¹é½ç‰¹å¾

**åŸç†**ï¼šè®¡ç®—ä¸¤ç»„ç‰¹å¾çš„æ¦‚ç‡åˆ†å¸ƒå·®å¼‚ï¼š

$$
\text{loss}_{kl} = D_{KL}(P \| Q) = \sum_i P_i \log \frac{P_i}{Q_i}
$$

**ç›´è§‚ç†è§£**ï¼šçº¦æŸè·¨æ¨¡æ€ç‰¹å¾ä¸€è‡´ï¼Œè®© 2D-3D å¯¹é½æ›´å‡†ç¡®ï¼Œä»è€Œæé«˜ç‚¹çº§ affordance mask çš„è´¨é‡ã€‚

---

ğŸ”‘ **æ€»æŸå¤±**

æœ€ç»ˆè®­ç»ƒç›®æ ‡æ˜¯ä¸‰è€…åŠ æƒæ±‚å’Œï¼š

$$
\text{Loss} = \text{HM\_Loss} + \lambda_{cls} \cdot \text{CE\_Loss} + \lambda_{kl} \cdot \text{KL\_Loss}
$$

* HM\_Loss â†’ ç‚¹çº§æ©ç 

* CE\_Loss â†’ å…¨å±€ç±»åˆ«

* KL\_Loss â†’ è·¨æ¨¡æ€ç‰¹å¾å¯¹é½

è¿™æ ·æ¨¡å‹æ—¢å­¦åˆ° **é€ç‚¹äº¤äº’åŒºåŸŸ**ï¼Œä¹Ÿå­¦åˆ° **ç‰©ä½“æ•´ä½“ affordance**ï¼Œè¿˜ä¿è¯ **å›¾åƒ-ç‚¹äº‘ç‰¹å¾ä¸€è‡´æ€§**ã€‚


```python
def kl_div(p_out, q_out, get_softmax=True):
    KLD = nn.KLDivLoss(reduction='batchmean')
    B = p_out.size(0)

    if get_softmax:
        p_out = F.softmax(p_out.view(B,-1),dim=-1)
        q_out = F.log_softmax(q_out.view(B,-1),dim=-1)

    kl_loss = KLD(q_out, p_out)

    return kl_loss

class HM_Loss(nn.Module):
    def __init__(self):
        super(HM_Loss, self).__init__()
        self.gamma = 2
        self.alpha = 0.25

    def forward(self, pred, target):
        #[B, N, 18]
        temp1 = -(1-self.alpha)*torch.mul(pred**self.gamma,
                           torch.mul(1-target, torch.log(1-pred+1e-6)))
        temp2 = -self.alpha*torch.mul((1-pred)**self.gamma,
                           torch.mul(target, torch.log(pred+1e-6)))
        temp = temp1+temp2
        CELoss = torch.sum(torch.mean(temp, (0, 1)))

        intersection_positive = torch.sum(pred*target, 1)
        cardinality_positive = torch.sum(torch.abs(pred)+torch.abs(target), 1)
        dice_positive = (intersection_positive+1e-6) / \
            (cardinality_positive+1e-6)

        intersection_negative = torch.sum((1.-pred)*(1.-target), 1)
        cardinality_negative = torch.sum(
            2-torch.abs(pred)-torch.abs(target), 1)
        dice_negative = (intersection_negative+1e-6) / \
            (cardinality_negative+1e-6)
        temp3 = torch.mean(1.5-dice_positive-dice_negative, 0)

        DICELoss = torch.sum(temp3)
        return CELoss+1.0*DICELoss

class CrossModalCenterLoss(nn.Module):
    """Center loss.    
    Args:
        num_classes (int): number of classes.
        feat_dim (int): feature dimension.
    """
    def __init__(self, num_classes, feat_dim=512, local_rank=None):
        super(CrossModalCenterLoss, self).__init__()
        self.num_classes = num_classes
        self.feat_dim = feat_dim
        self.local_rank = local_rank

        if self.local_rank != None:
            self.device = torch.device('cuda', self.local_rank)
        else:
            self.device = torch.device('cuda:0')
        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).to(self.device))

    def forward(self, x, labels):
        """
        Args:
            x: feature matrix with shape (batch_size, feat_dim).
            labels: ground truth labels with shape (batch_size).
        """
        batch_size = x.size(0)
        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \
                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()
        temp = torch.mm(x, self.centers.t())
        distmat = distmat - 2*temp

        classes = torch.arange(self.num_classes).long()
        classes = classes.to(self.device)
        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)
        mask = labels.eq(classes.expand(batch_size, self.num_classes))
        dist = distmat * mask.float()
        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size

        return loss
```