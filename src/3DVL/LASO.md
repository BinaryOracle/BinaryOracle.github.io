---
icon: file
category:
  - 3D-VL
  - 3D Affordance
tag:
  - 3D-VL
  - 3D Affordance
  - ç¼–è¾‘ä¸­
footer: æŠ€æœ¯å…±å»ºï¼ŒçŸ¥è¯†å…±äº«
date: 2025-05-30
cover: assets/cover/LASO.png
author:
  - BinaryOracle
---

`LASO: Language-guided Affordance Segmentation on 3D Object è®ºæ–‡ä»£ç è§£è¯»ä¸å¤ç°` 

<!-- more -->

# LASO æ¨¡å‹ä»£ç è§£è¯»ä¸å¤ç°

> è®ºæ–‡: [https://openaccess.thecvf.com/content/CVPR2024/papers/Li_LASO_Language-guided_Affordance_Segmentation_on_3D_Object_CVPR_2024_paper.pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_LASO_Language-guided_Affordance_Segmentation_on_3D_Object_CVPR_2024_paper.pdf)
> ä»£ç : [https://github.com/yl3800/LASO](https://github.com/yl3800/LASO)


è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€é¡¹æ–°çš„ä»»åŠ¡å’Œä¸€ä¸ªé…å¥—çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æ¨åŠ¨ **è¯­è¨€å¼•å¯¼ä¸‹çš„** 3Då¯¹è±¡åŠŸèƒ½åŒºåŸŸåˆ†å‰²ï¼ˆLanguage-guided Affordance Segmentation on 3D Object, ç®€ç§° LASOï¼‰ã€‚

## æ•°æ®é›†

### 1. åŸºç¡€æ•°æ®æ¥æº

æ•°æ®é›†åŸºäº **3D-AffordanceNet** æä¾›çš„ç‚¹äº‘å’ŒåŠŸèƒ½åŒºåŸŸæ ‡æ³¨æ„å»ºï¼š

- æ¯ä¸ªç‰©ä½“éƒ½ä»¥ç‚¹äº‘å½¢å¼è¡¨ç¤ºï¼›
- ç‚¹äº‘ä¸­çš„æ¯ä¸ªç‚¹è¢«æ ‡æ³¨ä¸ºæ”¯æŒä¸€ä¸ªæˆ–å¤šä¸ªåŠŸèƒ½ç±»å‹ï¼ˆmulti-class affordance labelsï¼‰ï¼Œä¾‹å¦‚ graspã€openã€liftã€move ç­‰ï¼›
- è¿™äº›åŠŸèƒ½æ ‡æ³¨æ˜¯äººå·¥æ ‡æ³¨çš„ï¼Œå…·æœ‰è¯­ä¹‰æ„ä¹‰ï¼›

> **ä¸ºä»€ä¹ˆä½¿ç”¨ 3D-AffordanceNetï¼Ÿ** 
> - å› ä¸ºå®ƒæä¾›äº†é«˜è´¨é‡çš„ç‚¹äº‘å’ŒåŠŸèƒ½æ ‡æ³¨ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°æ”¯æŒ LASO çš„ç›®æ ‡ï¼šæ ¹æ®è‡ªç„¶è¯­è¨€é—®é¢˜æ‰¾å‡ºä¸ä¹‹ç›¸å…³çš„åŠŸèƒ½åŒºåŸŸã€‚


### 2. æ„å»ºé—®é¢˜ï¼ˆQuestion Craftingï¼‰

1. **é€‰å–ç‰©ä½“-åŠŸèƒ½ç»„åˆ**ï¼š
   - ä» 3D-AffordanceNet ä¸­é€‰å–äº† **58 ç§ç‰©ä½“-åŠŸèƒ½ç»„åˆ**ï¼ˆå¦‚ mug-graspã€door-open ç­‰ï¼‰ï¼›
2. **æ‰‹å·¥è®¾è®¡é—®é¢˜**ï¼š
   - å¯¹æ¯ç§ç»„åˆæ‰‹å·¥ç¼–å†™ **5 ä¸ªä»£è¡¨æ€§é—®é¢˜**ï¼›
3. **ä½¿ç”¨ GPT-4 æ‰©å±•ç”Ÿæˆæ›´å¤šé—®é¢˜**ï¼š
   - ä½¿ç”¨ GPT-4 ä¸ºæ¯ä¸ªç»„åˆé¢å¤–ç”Ÿæˆ **10 ä¸ªé—®é¢˜**ï¼›
   - æ€»å…±å¾—åˆ° **870 ä¸ªä¸“å®¶è®¾è®¡çš„é—®é¢˜**ï¼ˆ58 Ã— 15 = 870ï¼‰ï¼›


![Affordance-Questionæ•°æ®å¯è§†åŒ–](LASO/1.png)   



åœ¨æ‰©å±•è¿‡ç¨‹ä¸­ï¼ŒGPT-4 ç”Ÿæˆçš„é—®é¢˜éµå¾ªä»¥ä¸‹ä¸‰ä¸ªå…³é”®åŸåˆ™ï¼Œä»¥ç¡®ä¿é—®é¢˜å¤šæ ·æ€§å’Œè¯­ä¹‰ä¸°å¯Œæ€§ï¼š

| åŸåˆ™ | æè¿° |
|------|------|
| **Contextual Enrichmentï¼ˆä¸Šä¸‹æ–‡ä¸°å¯ŒåŒ–ï¼‰** | æ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡ç»†èŠ‚ï¼Œä½¿é—®é¢˜æ›´å…·ä½“åœ°è¿æ¥ç›®æ ‡å¯¹è±¡çš„åŠŸèƒ½ï¼›<br>ä¾‹ï¼šå°† â€œGrasping scissors: top choice?â€ æ”¹ä¸º â€œIdentify the key points on the scissors that ensure successful grasping.â€ |
| **Concise Phrasingï¼ˆç®€æ´è¡¨è¾¾ï¼‰** | æç‚¼é—®é¢˜æœ¬è´¨ï¼Œä½¿å…¶ç®€çŸ­ä½†ä»æœ‰æ„ä¹‰ï¼› |
| **Structural Diversityï¼ˆç»“æ„å¤šæ ·æ€§ï¼‰** | ä½¿ç”¨ä¸åŒå¥å¼ç»“æ„ï¼ˆç–‘é—®å¥ã€é™ˆè¿°å¥ç­‰ï¼‰ï¼Œé˜²æ­¢æ¨¡å‹åå‘ç‰¹å®šå¥å¼æˆ–é•¿åº¦ï¼› |

### 3. æ ‡æ³¨ GT Maskï¼ˆGround Truth Maskï¼‰

å¯¹äºæ¯ä¸ªé—®é¢˜ï¼Œç»“åˆå…¶å¯¹åº”çš„åŠŸèƒ½ç±»å‹å’ŒåŸå§‹ç‚¹äº‘æ ‡æ³¨ä¿¡æ¯ï¼Œæ„é€ å‡ºå¯¹åº”çš„äºŒå€¼æ©ç  `gt_mask`ï¼š

- æ¯ä¸ªç‚¹æ˜¯å¦å±äºå½“å‰é—®é¢˜æè¿°çš„åŠŸèƒ½åŒºåŸŸï¼›
- `gt_mask` æ˜¯ `(N,)` å½¢çŠ¶çš„ä¸€ç»´æ•°ç»„ï¼Œå…¶ä¸­ N æ˜¯ç‚¹æ•°ï¼›
- æ•°å€¼å¯ä»¥æ˜¯ 0/1ï¼ˆbinary maskï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯**è½¯æ ‡ç­¾ï¼ˆsoft labelï¼‰**ï¼Œè¡¨ç¤ºç‚¹å±äºè¯¥åŠŸèƒ½åŒºåŸŸçš„æ¦‚ç‡ï¼›
- è½¯æ ‡ç­¾é€šå¸¸ç”¨äºè¾¹ç•Œæ¨¡ç³ŠåŒºåŸŸï¼Œåæ˜ ç‚¹ä¸åŠŸèƒ½æ ¸å¿ƒåŒºåŸŸçš„è·ç¦»è¿œè¿‘ï¼›

> ğŸ’¡ æ³¨æ„ï¼šè¿™äº›åŠŸèƒ½æ ‡ç­¾ä»…ç”¨äºæ„é€ é—®é¢˜å’Œå®šä½æ­£ç¡®åŠŸèƒ½åŒºåŸŸï¼Œåœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­ä¸ä½œä¸ºæ˜¾å¼ç›‘ç£ä¿¡å·ã€‚

### 4. æ•°æ®é›†ç»„ç»‡æ–¹å¼

æ•°æ®æ€»é‡ï¼š

- **æ€»æ ·æœ¬æ•°**ï¼š19,751 ä¸ªç‚¹äº‘-é—®é¢˜é…å¯¹ï¼›
- **ç‰©ä½“ç±»åˆ«æ•°**ï¼š23 ç±»ï¼›
- **åŠŸèƒ½ç±»å‹æ•°**ï¼š17 ç±»ï¼›
- **é—®é¢˜æ€»æ•°**ï¼š870 ä¸ªä¸“å®¶è®¾è®¡çš„é—®é¢˜ï¼›
- **æ¯ä¸ªç‰©ä½“ç±»åˆ«å¯æœ‰å¤šä¸ªå½¢çŠ¶å®ä¾‹**ï¼›
- **ä¸€ä¸ªé—®é¢˜å¯ä»¥ä½œç”¨äºå¤šä¸ªç‰©ä½“ç±»åˆ«**ï¼ˆæ³›åŒ–èƒ½åŠ›ï¼‰ï¼›

æ•°æ®é›†è®¾ç½®ï¼ˆä¸¤ç§æ¨¡å¼ï¼‰ï¼š

ğŸ”¹ Seenï¼ˆè§è¿‡ï¼‰

- è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µå…±äº«ç›¸ä¼¼çš„ç‰©ä½“ç±»åˆ«å’ŒåŠŸèƒ½ç±»å‹çš„åˆ†å¸ƒï¼›
- ç›®çš„æ˜¯è¯„ä¼°æ¨¡å‹åœ¨ç†Ÿæ‚‰åœºæ™¯ä¸‹çš„è¡¨ç°ï¼›

ğŸ”¹ Unseenï¼ˆæœªè§ï¼‰

- æŸäº›åŠŸèƒ½ç±»å‹åœ¨ç‰¹å®šç‰©ä½“ç±»åˆ«ä¸‹ä¼šä»è®­ç»ƒé›†ä¸­çœç•¥ï¼Œä½†åœ¨æµ‹è¯•é›†ä¸­ä¿ç•™ï¼›
- **ç›®çš„æ˜¯æµ‹è¯•æ¨¡å‹å¯¹æ–°ç»„åˆçš„æ³›åŒ–èƒ½åŠ›ï¼›**
- ä¾‹å¦‚ï¼šæ¨¡å‹åœ¨è®­ç»ƒæœŸé—´å­¦ä¼šäº†æŠ“å–åŒ…å’Œæ¯å­ï¼Œä½†æµ‹è¯•æ—¶è¦æ±‚â€œæŠ“å–è€³æœºâ€â€”â€”è¿™æ˜¯è®­ç»ƒä¸­æœªæ›¾é‡åˆ°è¿‡çš„åŠŸèƒ½-ç‰©ä½“ç»„åˆï¼›

æ•°æ®åˆ’åˆ†æ–¹å¼ï¼š

| åˆ†åŒº | ç‰©ä½“ç±»åˆ«æ•° | é—®é¢˜æ•° | æ ·æœ¬æ•° |
|------|-------------|--------|---------|
| Train | 6883 | 638 | 16,120 |
| Val | 516 | 58 | 1,215 |
| Test | 1035 | 174 | 2,416 |

### 5. æ•°æ®å¢å¼ºä¸é…å¯¹ç­–ç•¥

è®­ç»ƒé˜¶æ®µï¼š

- æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæ¯ä¸ªå½¢çŠ¶å®ä¾‹éšæœºåŒ¹é…ä¸€ä¸ªä¸å…¶åŠŸèƒ½ç±»å‹ä¸€è‡´çš„é—®é¢˜ï¼›
- éšæœºé…å¯¹ä½¿æ¨¡å‹æš´éœ²äºå„ç§è¯­ä¹‰ä¸Šä¸‹æ–‡ä¸­ï¼Œæå‡æ³›åŒ–èƒ½åŠ›ï¼›

æ¨ç†é˜¶æ®µï¼ˆéªŒè¯ & æµ‹è¯•ï¼‰ï¼š

- é—®é¢˜é…å¯¹æ˜¯å›ºå®šçš„ï¼›
- æ‰€æœ‰é—®é¢˜ä¸“å±äºè¯„ä¼°é˜¶æ®µï¼Œä¸åœ¨è®­ç»ƒä¸­é€éœ²ï¼›
- ç¡®ä¿æ¨ç†ä¸€è‡´æ€§ï¼Œä¿æŒè¯„ä¼°å®Œæ•´æ€§ï¼›


### 6. æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¥è‡ªè®ºæ–‡å›¾3ï¼‰

| ç»´åº¦ | å†…å®¹ |
|------|------|
| åŠŸèƒ½ç±»å‹ | 17 ç±»ï¼Œå¦‚ graspã€openã€liftã€move ç­‰ |
| ç‰©ä½“ç±»åˆ« | 23 ç±»ï¼Œå¦‚ mugã€microwaveã€chairã€door ç­‰ |
| ç‰©ä½“-åŠŸèƒ½ç»„åˆ | 58 ç§å”¯ä¸€ç»„åˆï¼ˆobject-affordance pairsï¼‰ |
| é—®é¢˜æ€»æ•° | 870 ä¸ªå®šåˆ¶åŒ–é—®é¢˜ |
| ç‚¹äº‘-é—®é¢˜é…å¯¹ | 19,751 å¯¹ |
| ç‚¹äº‘æ¥æº | æ¥è‡ª 3D-AffordanceNetï¼Œæ¯ä¸ªç‚¹äº‘çº¦ 2048 ä¸ªç‚¹ |

### 7. ä»£ç å®ç°

**æ•°æ®é›†åˆå§‹åŒ–çš„æ ¸å¿ƒä»£ç å®ç°å¦‚ä¸‹:**

```python
class AffordQ(Dataset):

    def __init__(self,
                 split='train',
                 **kwargs
                 ):
        # æ•°æ®é›†å­˜æ”¾ç›®å½•         
        data_root='LASO_dataset'
        # æ•°æ®é›†ç±»å‹: è®­ç»ƒé›†ï¼Œè¯„ä¼°é›†ï¼Œæµ‹è¯•é›†
        self.split = split
        # æ‰€æ”¯æŒçš„23ç§ç‰©ä½“ç±»å‹å’Œ17ç§åŠŸèƒ½ç±»å‹ 
        classes = ["Bag", "Bed", "Bowl","Clock", "Dishwasher", "Display", "Door", "Earphone", "Faucet",
            "Hat", "StorageFurniture", "Keyboard", "Knife", "Laptop", "Microwave", "Mug",
            "Refrigerator", "Chair", "Scissors", "Table", "TrashCan", "Vase", "Bottle"]
        
        afford_cl = ['lay','sit','support','grasp','lift','contain','open','wrap_grasp','pour', 
                     'move','display','push','pull','listen','wear','press','cut','stab']
        # å»ºç«‹ç‰©ä½“ç±»å‹å’ŒåŠŸèƒ½ç±»å‹çš„ç´¢å¼•æ˜ å°„å…³ç³»ï¼Œç¥ç»ç½‘ç»œæ¨¡å‹åªè®¤è¯†æ•°å­— 
        self.cls2idx = {cls.lower():np.array(i).astype(np.int64) for i, cls in enumerate(classes)}
        self.aff2idx = {cls:np.array(i).astype(np.int64) for i, cls in enumerate(afford_cl)}
        # åŠ è½½æ ‡æ³¨æ•°æ®
        with open(os.path.join(data_root, f'anno_{split}.pkl'), 'rb') as f:
            self.anno = pickle.load(f)
        # åŠ è½½ç‚¹äº‘æ•°æ®
        with open(os.path.join(data_root, f'objects_{split}.pkl'), 'rb') as f:
            self.objects = pickle.load(f)

        # åŠ è½½58ç§ç‰©ä½“-åŠŸèƒ½ç»„åˆçš„æ ‡æ³¨æ•°æ® (æ•°æ®ç»„ç»‡å½¢å¼ï¼Œå‚è€ƒä¸Šæ–‡çš„ Affordance-Questionæ•°æ®å¯è§†åŒ–å›¾)
        self.question_df = pd.read_csv(os.path.join(data_root, 'Affordance-Question.csv'))

        # sort anno by object class and affordance type -- éå†æ ‡æ³¨æ•°æ®åˆ—è¡¨
        self.sort_anno ={}
        for item in sorted(self.anno, key=lambda x: x['class']):
            # è·å–å½“å‰æ ·æœ¬çš„ç‰©ä½“ç±»åˆ«å’Œç‰©ä½“ä¿¡æ¯å€¼: ç‚¹äº‘ID, åŠŸèƒ½åŒºåŸŸæ©ç , åŠŸèƒ½ç±»åˆ«
            key = item['class']
            value = {'shape_id': item['shape_id'], 'mask': item['mask'], 'affordance': item['affordance']}
            
            # æ¯ç§ç‰©ä½“å¯ä»¥å¯¹åº”å¤šç§å½¢çŠ¶å®ä¾‹å’ŒåŠŸèƒ½ç±»åˆ«
            if key not in self.sort_anno:
                # å¦‚æœå½“å‰ç‰©ä½“ç±»åˆ«ä¸åœ¨æ’åºåçš„å­—å…¸ä¸­ï¼Œç›´æ¥æ·»åŠ 
                self.sort_anno[key] = [value]
            else:
                # å¦‚æœå½“å‰ç‰©ä½“ç±»åˆ«åœ¨æ’åºåçš„å­—å…¸ä¸­ï¼Œå°†å½“å‰æ ·æœ¬çš„ç‰©ä½“ä¿¡æ¯å€¼è¿½åŠ åˆ°å¯¹åº”åˆ—è¡¨ä¸­
                self.sort_anno[key].append(value)
```
åŠ è½½çš„æ ‡æ³¨æ•°æ®ä¸­æ¯ä¸ªæ ·æœ¬çš„ç»„ç»‡å½¢å¼å¦‚ä¸‹:
- shape_id ï¼šç‚¹äº‘ID
- class ï¼šç‰©ä½“ç±»åˆ«ï¼ˆå¦‚bedï¼‰
- affordance ï¼šåŠŸèƒ½ç±»åˆ«ï¼ˆå¦‚layï¼‰
- mask ï¼šåŠŸèƒ½åŒºåŸŸæ©ç ï¼ˆç‚¹çº§åˆ«æ ‡æ³¨ï¼‰

![æ ‡æ³¨æ•°æ®ç»„ç»‡å½¢å¼](LASO/2.png)   

![ç‚¹äº‘æ•°æ®ç»„ç»‡å½¢å¼](LASO/3.png)   

![æ¯ç§ç‰©ä½“å¯ä»¥å¯¹åº”å¤šç§å½¢çŠ¶å®ä¾‹å’ŒåŠŸèƒ½ç±»åˆ«](LASO/4.png)   

**è·å–æ ·æœ¬çš„ä»£ç å®ç°:**

```python
    def __getitem__(self, index):
        # æ ¹æ®æ ·æœ¬ç´¢å¼•å–å‡ºæ ·æœ¬æ•°æ®
        data = self.anno[index]    
        # è·å–å½“å‰æ ·æœ¬å¯¹åº”çš„ç‚¹äº‘ID        
        shape_id = data['shape_id']
        # è·å–å½“å‰æ ·æœ¬å¯¹åº”çš„ç‰©ä½“ç±»åˆ«
        cls = data['class']
        #  è·å–å½“å‰æ ·æœ¬å¯¹åº”çš„åŠŸèƒ½ç±»å‹
        affordance = data['affordance']
        # è·å–å½“å‰æ ·æœ¬å¯¹åº”çš„åŠŸèƒ½åŒºåŸŸæ©ç 
        gt_mask = data['mask']
        # å–å‡ºå½“å‰æ ·æœ¬å¯¹åº”çš„ç‚¹äº‘æ•°æ® ï¼Œï¼ˆ2048,3)
        point_set = self.objects[str(shape_id)]
        # å¯¹ç‚¹äº‘æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œæ¶ˆé™¤å°ºåº¦å·®å¼‚
        point_set,_,_ = pc_normalize(point_set)
        # å¯¹ç‚¹äº‘æ•°æ®è¿›è¡Œè½¬ç½®æ“ä½œ ï¼Œï¼ˆ3,2048)
        point_set = point_set.transpose()

        # è·å–å½“å‰æ ·æœ¬å¯¹åº”çš„é—®é¢˜æ–‡æœ¬(è®­ç»ƒ: éšæœºé€‰ï¼› éªŒè¯&æµ‹è¯•: å›ºå®šè¿”å›é—®é¢˜0)
        question = self.find_rephrase(self.question_df, cls, affordance)
        # è·å–å½“å‰åŠŸèƒ½ç±»å‹å¯¹åº”çš„ç´¢å¼•å€¼
        affordance = self.aff2idx[affordance]

        # è¿”å›: ç‚¹äº‘æ•°æ®ï¼Œ ç‰©ä½“ç±»åˆ«ç´¢å¼•ï¼Œ åŠŸèƒ½åŒºåŸŸæ©ç ï¼Œ é—®é¢˜æ–‡æœ¬ï¼Œ åŠŸèƒ½ç±»å‹ç´¢å¼•
        return point_set, self.cls2idx[cls], gt_mask, question, affordance

    def find_rephrase(self, df, object_name, affordance):
        # å¦‚æœå½“å‰æ˜¯è®­ç»ƒæ¨¡å¼ï¼Œåˆ™ä»é—®é¢˜1ï½15ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªé—®é¢˜ï¼Œå¦åˆ™å›ºå®šè¿”å›é—®é¢˜0
        qid = str(np.random.randint(1, 15)) if self.split == 'train' else '0'
        qid = 'Question'+qid
        # ä» DataFrame df ä¸­ç­›é€‰å‡ºåŒæ—¶æ»¡è¶³ ç‰©ä½“åç§°åŒ¹é… å’Œ åŠŸèƒ½å±æ€§åŒ¹é… çš„è¡Œï¼Œå¹¶ä»…ä¿ç•™ qid æŒ‡å®šçš„åˆ—ï¼Œä¹Ÿå°±æ˜¯å–å‡ºä¸Šé¢éšæœºé€‰æ‹©çš„é—®é¢˜æ–‡æœ¬
        result = df.loc[(df['Object'] == object_name) & (df['Affordance'] == affordance), [qid]]
        # é—®é¢˜æ–‡æœ¬ä¸ä¸ºç©ºï¼Œåˆ™è¿”å›è¯¥é—®é¢˜æ–‡æœ¬
        if not result.empty:
            # return result.index[0], result.iloc[0]['Rephrase']
            return result.iloc[0][qid]
        else:
            raise NotImplementedError
```

### 8. æ€»ç»“

LASO æ•°æ®é›†åŸºäº 3D-AffordanceNet çš„ç‚¹äº‘å’ŒåŠŸèƒ½æ ‡æ³¨ï¼Œç»“åˆäººå·¥+GPT-4 ç”Ÿæˆçš„å¤šæ ·åŒ–é—®é¢˜ï¼Œæ„é€ å‡º 19,751 ä¸ªç‚¹äº‘-é—®é¢˜é…å¯¹ï¼Œæ—¨åœ¨å®ç°è¯­è¨€å¼•å¯¼ä¸‹çš„ 3D åŠŸèƒ½åŒºåŸŸåˆ†å‰²ï¼Œæ¨åŠ¨ 3D è§†è§‰ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ·±åº¦èåˆã€‚

## æ¨¡å‹å®ç°

è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„æ¨¡å‹ï¼š**PointRefer**ï¼Œç”¨äºè§£å†³ä¸€ä¸ªæ–°é¢–çš„ä»»åŠ¡ â€”â€” **è¯­è¨€å¼•å¯¼çš„ 3D å¯¹è±¡åŠŸèƒ½åŒºåŸŸåˆ†å‰²ï¼ˆLASOï¼‰**ã€‚

æ¨¡å‹ç›®æ ‡ï¼š ç»™å®šä¸€ä¸ª 3D ç‚¹äº‘å¯¹è±¡å’Œä¸€ä¸ªè‡ªç„¶è¯­è¨€é—®é¢˜ï¼ˆä¾‹å¦‚ï¼šâ€œWhere would you grasp this mug?â€ï¼‰ï¼ŒPointRefer çš„ç›®æ ‡æ˜¯é¢„æµ‹å‡ºä¸è¯¥é—®é¢˜ç›¸å…³çš„ç‚¹äº‘åŒºåŸŸï¼Œå³ç”Ÿæˆä¸€ä¸ªäºŒå€¼æ©ç ï¼Œè¡¨ç¤ºå“ªäº›ç‚¹å±äºç›®æ ‡åŠŸèƒ½åŒºåŸŸã€‚

PointRefer åŒ…æ‹¬ä»¥ä¸‹æ ¸å¿ƒæ¨¡å—ï¼š

1. **3D éª¨å¹²ç½‘ç»œï¼ˆ3D Backboneï¼‰**
   - ä½¿ç”¨ PointNet++ ç¼–ç ç‚¹äº‘ç‰¹å¾ï¼›
   - å¤šé˜¶æ®µç¼–ç -è§£ç ç»“æ„æå–å¤šå°ºåº¦ç‚¹ç‰¹å¾ï¼›

2. **è‡ªé€‚åº”èåˆæ¨¡å—ï¼ˆAdaptive Fusion Module, AFMï¼‰**
   - åœ¨ä¸åŒè§£ç å±‚æ³¨å…¥è¯­è¨€ä¿¡æ¯ï¼›
   - å®ç°è¯­è¨€å¼•å¯¼ä¸‹çš„è·¨æ¨¡æ€èåˆï¼›
   - å¢å¼ºç‚¹ç‰¹å¾çš„è¯­ä¹‰åˆ¤åˆ«èƒ½åŠ›ï¼›

3. **å‚è€ƒç‚¹è§£ç å™¨ï¼ˆReferred Point Decoder, RPDï¼‰**
   - å¼•å…¥ä¸€ç»„å¯å­¦ä¹ çš„â€œé—®é¢˜æ¡ä»¶åŒ–æŸ¥è¯¢â€ï¼ˆaffordance queriesï¼‰ï¼›
   - åˆ©ç”¨ Transformer è§£ç å™¨å°†è¿™äº›æŸ¥è¯¢ä¸ç‚¹äº‘ç‰¹å¾è¿›è¡Œäº¤äº’ï¼›
   - ç”ŸæˆåŠ¨æ€å·ç§¯æ ¸ï¼ˆdynamic kernelsï¼‰ï¼›
   - æœ€ç»ˆé€šè¿‡å·ç§¯æ“ä½œç”Ÿæˆåˆ†å‰²æ©ç ï¼›

![PointReferæ¨¡å‹ç»“æ„å›¾](LASO/5.png)   

PointRefer å‰å‘ä¼ æ’­è¿‡ç¨‹å¦‚ä¸‹:

```python
class PointRefer(nn.Module):

    # ä¼ å…¥questionæ–‡æœ¬ å’Œ pointç‚¹äº‘æ•°æ® 
    def forward(self, text, xyz):

        '''
        text: [B, L, 768]
        xyz: [B, 3, 2048] -- (b,c,n)
        '''
         
        B, C, N = xyz.size()
        # 1. Encoding è¿‡ç¨‹
        # 1.1 Language Encoding ä½¿ç”¨RoBertç¼–ç æ–‡æœ¬
        t_feat, t_mask = self.forward_text(list(text), xyz.device)  # [batch, q_len, d_model]
        # 1.2 BackBone Encoding ä½¿ç”¨PointNet++ç¼–ç ç‚¹äº‘
        F_p_wise = self.point_encoder(xyz)     

        """ 
        Decoding
        """
        # 1.3 PointNet++ é€çº§åšç‚¹é›†æŠ½è±¡å¾—åˆ°çš„æ¯å±‚çš„ç‚¹é›†åæ ‡å’Œç‚¹é›†ç‰¹å¾é›†åˆ
        p_0, p_1, p_2, p_3 = F_p_wise

        # 2.Backbone Decodingè¿‡ç¨‹
        # 2.1 ç‚¹é›†é›†åˆä¸­æ¯ä¸ªç‚¹çš„ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ä¿¡æ¯è¿›è¡Œèåˆ,ä¼ å…¥çš„ç‚¹é›†ç‰¹å¾é›†åˆç»è¿‡è½¬ç½®å¤„ç†åçš„ç»´åº¦ä¸º: (b, n, c)
        p_3[1] = self.gpb(t_feat, p_3[1].transpose(-2, -1)).transpose(-2, -1)
        # 2.2 PointNet++ ç‰¹å¾ä¼ æ’­é˜¶æ®µ: ä¸Šé‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œä¸Šä¸€å±‚ç‚¹é›†ä¸­çš„ç‚¹ç‰¹å¾é‡å»ºè¿‡ç¨‹ä¸­ï¼Œå……åˆ†å¸æ”¶äº†é«˜çº§åŒºåŸŸæŠ½è±¡ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾
        up_sample = self.fp3(p_2[0], p_3[0], p_2[1], p_3[1])   #[B, emb_dim, npoint_sa2]

        up_sample = self.gpb(t_feat, up_sample.transpose(-2, -1)).transpose(-2, -1)
        up_sample = self.fp2(p_1[0], p_2[0], p_1[1], up_sample)    #[B, emb_dim, npoint_sa1]   
        
        up_sample = self.gpb(t_feat, up_sample.transpose(-2, -1)).transpose(-2, -1) 
        # 2.3 ç‰¹å¾ä¼ æ’­é˜¶æ®µç»“æŸ: ä¸€æ­¥æ­¥é‡å»ºå›åŸå§‹ç‚¹æ•°é‡ 128->256->512->1024->2048        
        up_sample = self.fp1(p_0[0], p_1[0], torch.cat([p_0[0], p_0[1]],1), up_sample)  #[B, emb_dim, N]
        
        #  3. Referred Point Decodingè¿‡ç¨‹
        t_feat = self.decoder(t_feat, up_sample.transpose(-2, -1), tgt_key_padding_mask=t_mask, query_pos=self.pos1d) # b,l,c
        t_feat *= t_mask.unsqueeze(-1).float()
        _3daffordance = torch.einsum('blc,bcn->bln', t_feat, up_sample)
        _3daffordance = _3daffordance.sum(1)/(t_mask.float().sum(1).unsqueeze(-1))
        _3daffordance = torch.sigmoid(_3daffordance)
        return _3daffordance.squeeze(-1)
```
> è®ºæ–‡ä¸­æ‰€ç»™çš„æ¨¡å‹æ¶æ„å›¾ä¸­çš„Encoder layeræŒ‡çš„æ˜¯PointNet++ä¸­æä¾›çš„PointNetSetAbstractionMsgå¤šå°ºåº¦åˆ†ç»„ç‚¹é›†ç‰¹å¾æŠ½å–ç±»

> è®ºæ–‡ä¸­æ‰€ç»™çš„æ¨¡å‹æ¶æ„å›¾ä¸­çš„Decoder layeræŒ‡çš„æ˜¯PointNet++ä¸­æä¾›çš„PointNetFeaturePropagationç‰¹å¾ä¼ æ’­ç±»

### AFM è‡ªé€‚åº”èåˆæ¨¡å—

åœ¨ LASO ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹éœ€è¦æ ¹æ®è‡ªç„¶è¯­è¨€é—®é¢˜ï¼ˆå¦‚ â€œWhere to grasp?â€ï¼‰è¯†åˆ«ç‚¹äº‘ä¸­çš„åŠŸèƒ½åŒºåŸŸã€‚ç”±äºç›®æ ‡åŠŸèƒ½åŒºåŸŸçš„å°ºåº¦ã€å½¢çŠ¶å¤šæ ·ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥é€‚åº”ä¸åŒæƒ…å†µã€‚ä¸ºæ­¤ï¼Œä½œè€…è®¾è®¡äº† AFM æ¨¡å—ï¼Œä»¥å¢å¼º PointNet++ è§£ç è¿‡ç¨‹ä¸­ç‚¹ç‰¹å¾çš„è¯­è¨€å¼•å¯¼èƒ½åŠ›ã€‚

AFM çš„ç›®æ ‡æ˜¯ï¼šåœ¨ä¸åŒè§£ç é˜¶æ®µæ³¨å…¥è¯­è¨€çº¿ç´¢ï¼ˆtext cluesï¼‰ï¼Œå°†æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯ä¸ç‚¹äº‘ç‰¹å¾è¿›è¡Œè·¨æ¨¡æ€èåˆï¼Œé€æ­¥ä»¥è‡ªä¸Šè€Œä¸‹çš„æ–¹å¼ç»†åŒ–ç‚¹ç‰¹å¾å›¾ï¼Œä»è€Œæå‡æ¨¡å‹å¯¹å¤šå°ºåº¦ã€å¤šå½¢çŠ¶çš„åŠŸèƒ½åŒºåŸŸçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚

AFM éµå¾ªä¸€ä¸ª **ç“¶é¢ˆå¼æ¶æ„ï¼ˆbottleneck architectureï¼‰**ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼š

1. **Groupingï¼ˆåˆ†ç»„ï¼‰**
2. **Mixingï¼ˆæ··åˆï¼‰**
3. **Ungroupingï¼ˆè§£ç»„ï¼‰**

è¿™ä¸‰ä¸ªæ­¥éª¤æ„æˆäº†ä¸€ä¸ªå®Œæ•´çš„è·¨æ¨¡æ€èåˆæµç¨‹ã€‚

#### 1ï¸âƒ£ Groupingï¼šæ–‡æœ¬å¼•å¯¼çš„ç‚¹ç‰¹å¾åˆ†ç»„

è¾“å…¥ï¼š
- `X âˆˆ R^{LÃ—d}`ï¼šé—®é¢˜ç¼–ç åçš„æ–‡æœ¬ç‰¹å¾ï¼ˆç”± RoBERTa ç¼–ç å¾—åˆ°ï¼‰
- `P âˆˆ R^{TÃ—d}`ï¼šæŸä¸€å±‚è§£ç å™¨è¾“å‡ºçš„ç‚¹ç‰¹å¾ï¼Œå…¶ä¸­ T è¡¨ç¤ºè¯¥å±‚ç‚¹æ•°

å¤„ç†è¿‡ç¨‹ï¼š
- ä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œå°†æ–‡æœ¬ç‰¹å¾ä½œä¸ºæŸ¥è¯¢ï¼ˆqueryï¼‰ï¼Œç‚¹ç‰¹å¾ä½œä¸ºé”®ï¼ˆkeyï¼‰å’Œå€¼ï¼ˆvalueï¼‰ï¼Œè¾“å‡ºåˆ†ç»„æ ‡è®° Gï¼š

$$
G = \text{Attention}(X, W_1P, P) + X
$$
$$
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
$$

å…¶ä¸­ï¼š
- $W_1$ æ˜¯ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼›
- æ³¨æ„åŠ›æœºåˆ¶ä½¿å¾—æ¯ä¸ªæ–‡æœ¬ token å¯¹åº”ä¸€ç»„ç›¸å…³çš„ç‚¹ç‰¹å¾ï¼›
- åˆ†ç»„æ“ä½œå®ç°äº†â€œè¯­è¨€å¼•å¯¼çš„ç‚¹ç‰¹å¾ç­›é€‰â€ã€‚

> é‡ç‚¹æ˜¯å¦‚ä½•ç†è§£è¿™é‡Œçš„åˆ†ç»„: æ¯ä¸ªæ–‡æœ¬Tokenè¯¢é—®æ‰€æœ‰ç‚¹Keyåï¼ŒçŸ¥é“äº†å“ªäº›ç‚¹è·Ÿè‡ªèº«çš„ç›¸å…³åº¦æ›´å¤§ï¼Œå› æ­¤åŠ æƒèåˆçš„æ—¶å€™ï¼Œä¾§é‡äºç»™è¿™äº›ç‚¹çš„ç‰¹å¾åˆ†é…æ›´å¤§çš„èåˆæƒé‡ã€‚

è¿™éƒ¨åˆ†ä»£ç å®ç°å¦‚ä¸‹:

```python
# group_layer çš„å®ç°
class LightGroupAttnBlock(nn.Module):
    
    # query æ˜¯RoBertaç¼–ç åçš„æ–‡æœ¬ç‰¹å¾ , (b,l,c)
    # keyå’Œvalueéƒ½æ˜¯ç‚¹äº‘ç‰¹å¾ , (b,n,c)
    def forward(self, query, key, value, q_mask=None):
        def _inner_forward(query, key, value):
            q = self.norm_query(query)
            k = q if self.key_is_query else self.norm_key(key)
            v = k if self.value_is_key else self.norm_value(value)
            # è®©æ¯ä¸ªè¯­è¨€ token å»å…³æ³¨ç‚¹äº‘ä¸­æœ€ç›¸å…³çš„åŒºåŸŸï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¼ºåŒ–è‡ªèº«çš„è¯­ä¹‰è¡¨è¾¾ã€‚
            # åŠ ä¸ŠåŸå§‹ X æ˜¯ä¸€ç§æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰ï¼Œå¯ä»¥ç¡®ä¿è¯­è¨€è¯­ä¹‰ä¸ä¼šä¸¢å¤±ã€‚
            x = self.attn(q, k, v, q_mask) + self.drop(q) 
            return x
        
        return _inner_forward(query, key, value)
```

---

#### 2ï¸âƒ£ Mixingï¼šMLP-Mixer è¿›è¡Œç»„å†…å’Œé€šé“é—´çš„ä¿¡æ¯æ··åˆ

MLP-Mixer æ˜¯ä¸€ç§ åŸºäº MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰çš„è§†è§‰æ¨¡å‹æ¶æ„ ï¼Œç”± Google Research åœ¨ 2021 å¹´æå‡ºã€‚å®ƒä¸ä½¿ç”¨ä»»ä½•æ³¨æ„åŠ›æœºåˆ¶ï¼Œè€Œæ˜¯é€šè¿‡ ç©ºé—´æ··åˆï¼ˆmixingï¼‰å’Œé€šé“æ··åˆï¼ˆmixingï¼‰æ“ä½œ æ¥å®ç°å…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚

> [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601)

MLP-Mixer çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šç”¨ MLP æ›¿ä»£ Transformer ä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ ï¼Œä»è€Œå‡å°‘è®¡ç®—å¤æ‚åº¦å¹¶ä¿æŒæ€§èƒ½ã€‚

1. Token-mixing MLP

- å¯¹æ‰€æœ‰ç‚¹/patch çš„ç›¸åŒé€šé“è¿›è¡Œæ··åˆï¼›
- ç›¸å½“äºè·¨ç©ºé—´ä½ç½®çš„ä¿¡æ¯äº¤æ¢ï¼›
- ç±»ä¼¼äº CNN ä¸­çš„ç©ºé—´å·ç§¯ï¼›

2. Channel-mixing MLP

- å¯¹æ¯ä¸ª token çš„æ‰€æœ‰é€šé“è¿›è¡Œå¤„ç†ï¼›
- æå–æ›´é«˜çº§çš„ç‰¹å¾è¡¨ç¤ºï¼›
- ç±»ä¼¼äºä¼ ç»Ÿçš„å…¨è¿æ¥å±‚æˆ– 1x1 å·ç§¯ï¼›

è¿™ä¸¤ä¸ªæ“ä½œäº¤æ›¿è¿›è¡Œï¼Œå½¢æˆä¸€ä¸ªç±»ä¼¼äº Transformer çš„å †å ç»“æ„ï¼Œä½†å®Œå…¨ä¸ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ã€‚

è¾“å…¥ï¼š
- `G âˆˆ R^{LÃ—d}`ï¼šåˆ†ç»„åçš„æ–‡æœ¬å¼•å¯¼ç‰¹å¾

å¤„ç†è¿‡ç¨‹ï¼š

- ä½¿ç”¨ MLP-Mixer æ¥æ›´æ–°åˆ†ç»„ç‰¹å¾ï¼Œç”Ÿæˆèåˆç‰¹å¾ Fï¼š

$$
G' = G + \text{MLP}_1(G^T)^T
$$
$$
F = G' + \text{MLP}_2(G')
$$

å…¶ä¸­ï¼š
- `MLPâ‚` è´Ÿè´£ç»„å†…ä¿¡æ¯æ··åˆï¼ˆtoken å†…éƒ¨ï¼‰ï¼›
- `MLPâ‚‚` è´Ÿè´£é€šé“é—´ä¿¡æ¯æ··åˆï¼ˆfeature channelï¼‰ï¼›
- ä¸¤ä¸ª MLP äº¤æ›¿ä½œç”¨ï¼Œå®ç°è·¨æ¨¡æ€ä¿¡æ¯çš„å……åˆ†äº¤äº’ï¼›
- æœ€ç»ˆè¾“å‡ºèåˆç‰¹å¾ `F`ï¼›

è¿™éƒ¨åˆ†ä»£ç å®ç°å¦‚ä¸‹:

```python
# mixer çš„å®ç°
class MLPMixerLayer(nn.Module):
    def __init__(self,
                 num_patches,
                 embed_dims,
                 patch_expansion,
                 channel_expansion,
                 drop_out,
                 **kwargs):

        super().__init__()

        patch_mix_dims = int(patch_expansion * embed_dims) # 16
        channel_mix_dims = int(channel_expansion * embed_dims) # 128

        self.patch_mixer = nn.Sequential(
            nn.Linear(num_patches, patch_mix_dims, bias=False), # try here
            nn.GELU(),
            nn.Dropout(drop_out),
            nn.Linear(patch_mix_dims, num_patches, bias=False),
            nn.Dropout(drop_out)
        )

        self.channel_mixer = nn.Sequential(
            nn.Linear(embed_dims, channel_mix_dims),
            nn.GELU(),
            nn.Dropout(drop_out),
            nn.Linear(channel_mix_dims, embed_dims),
            nn.Dropout(drop_out)
        )

        self.norm1 = nn.LayerNorm(embed_dims)
        self.norm2 = nn.LayerNorm(embed_dims)
    
    # x åˆ†ç»„åçš„æ–‡æœ¬å¼•å¯¼ç‰¹å¾ : (b,l,c)
    def forward(self, x):
        # x è½¬ç½®å: (b,c,l) , patch_mixer è´Ÿè´£ç»„å†…ä¿¡æ¯æ··åˆï¼ˆtoken å†…éƒ¨ï¼‰
        x = x + self.patch_mixer(self.norm1(x).transpose(1,2)).transpose(1,2)
        # channel_mixer è´Ÿè´£é€šé“é—´ä¿¡æ¯æ··åˆï¼ˆfeature channel) 
        x = x + self.channel_mixer(self.norm2(x))
        return x
```
---

#### 3ï¸âƒ£ Ungroupingï¼šå°†èåˆç‰¹å¾æ˜ å°„å›ç‚¹ç©ºé—´

è¾“å…¥ï¼š

- åŸå§‹ç‚¹ç‰¹å¾ `P`ï¼›
- èåˆåçš„æ–‡æœ¬ç‰¹å¾ `F`ï¼›

å¤„ç†è¿‡ç¨‹ï¼š

- ä½¿ç”¨å¦ä¸€ä¸ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå°†èåˆç‰¹å¾é‡æ–°åˆ†é…ç»™æ¯ä¸ªç‚¹ï¼š

$$
P_m = \text{Attention}(P, W_2F, F) + P
$$

å…¶ä¸­ï¼š
- `Wâ‚‚` æ˜¯çº¿æ€§å˜æ¢ï¼›
- æ³¨æ„åŠ›æœºåˆ¶è®©æ¯ä¸ªç‚¹ä»èåˆç‰¹å¾ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼›
- è¾“å‡º `P_m` æ˜¯è¯­è¨€å¢å¼ºåçš„ç‚¹ç‰¹å¾ï¼›
- æœ€ååŠ ä¸Šæ®‹å·®è¿æ¥å½¢æˆæœ€ç»ˆè¾“å‡º `P_o`ï¼š

$$
P_o = P_m + \text{residual}
$$

è¿™ä¸ª `P_o` å°±æ˜¯ç»è¿‡ AFM å¢å¼ºçš„ç‚¹ç‰¹å¾å›¾ï¼Œç”¨äºåç»­åˆ†å‰²æ©ç é¢„æµ‹ã€‚

```python
class FullAttnCatBlock(nn.Module):
    # query ä¸ºç‚¹äº‘: (b,n,c) , keyå’Œvalueä¸ºèåˆåçš„æ–‡æœ¬ç‰¹å¾: (b,l,c)
    def forward(self, query, key, value, key_padding_mask=None):
        def _inner_forward(query, key, value, key_padding_mask):
            q = self.norm_query(query)
            k = q if self.key_is_query else self.norm_key(key)
            v = k if self.value_is_key else self.norm_value(value)
            
            # ä½¿ç”¨å¦ä¸€ä¸ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå°†èåˆç‰¹å¾é‡æ–°åˆ†é…ç»™æ¯ä¸ªç‚¹
            x = self.attn(q, k, v, key_padding_mask) + self.drop(query)
            # MLPæ˜ å°„ + Residual Connection
            x = self.ffn(self.norm2(x)) + x
            return x
        
        return _inner_forward(query, key, value, key_padding_mask)
```
---
#### 4ï¸âƒ£ AFM è‡ªé€‚åº”èåˆæ¨¡å—

æœ‰äº†ä»¥ä¸Š Grouping - Mixing - Ungrouping ä¸‰ä¸ªå…³é”®æ­¥éª¤çš„å®ç°ï¼Œä¸‹é¢åªéœ€è¦æŠŠä»¥ä¸Šçš„ä¸‰ä¸ªæ­¥éª¤æŒ‰æµç¨‹ç»„ç»‡èµ·æ¥å³å¯å¾—åˆ°AFMæ¨¡å—çš„å®Œæ•´å®ç°äº†:

```python
class GPBlock(nn.Module):
    # q: æ–‡æœ¬ç‰¹å¾ (b, l, c) ï¼Œ x: ç‚¹é›†ç‰¹å¾é›†åˆ (b, n, c)
    def forward(self, q, x, q_mask=None):
        # Groupingé˜¶æ®µ
        gt = self.group_layer(query=q, key=x, value=x)
        if q_mask is not None:
            gt *= q_mask.unsqueeze(-1)
        # Mixingé˜¶æ®µ
        gt = self.mixer(gt) + self.drop(gt)
        # Ungroupingé˜¶æ®µ
        ungroup_tokens = self.un_group_layer(query=x, key=gt, value=gt, key_padding_mask=q_mask)
        return ungroup_tokens
```

AFM çš„ç½‘ç»œç»“æ„å¯è§†åŒ–ç†è§£

```
æ–‡æœ¬ç‰¹å¾ X â”€â”€â”
             â†“
           Grouping (Cross-Attention)
             â†“
           Mixing (MLP-Mixer)
             â†“
          Ungrouping (Attention)
             â†“
        è¾“å‡ºå¢å¼ºåçš„ç‚¹ç‰¹å¾ P_o
```

- **Grouping**ï¼šç”¨è¯­è¨€å¼•å¯¼ç‚¹ç‰¹å¾åˆ†ç»„ï¼›
- **Mixing**ï¼šåœ¨åˆ†ç»„å†…è¿›è¡Œä¿¡æ¯äº¤æ¢ï¼›
- **Ungrouping**ï¼šå†å°†èåˆä¿¡æ¯è¿”å›ç‚¹ç©ºé—´ï¼›

è¿™ç§è®¾è®¡ä½¿å¾—è¯­è¨€ä¿¡æ¯èƒ½æœ‰æ•ˆåœ°æŒ‡å¯¼ç‚¹ç‰¹å¾çš„å­¦ä¹ è¿‡ç¨‹ï¼Œè®ºæ–‡ä¸­ä¹Ÿè¿›è¡Œäº†å¤§é‡æ¶ˆèå®éªŒæ¥éªŒè¯ AFM çš„æœ‰æ•ˆæ€§ï¼š

| æ¨¡å‹å˜ä½“ | mIoU | AUC | SIM | MAE |
|----------|-------|-----|-----|-----|
| åŸºçº¿ï¼ˆä¸åŠ  AFMï¼‰ | 17.7 | 82.1 | 0.558 | 0.110 |
| åŠ å…¥ AFM å | **20.8** | **87.3** | **0.629** | **0.093** |

ç»“æœè¡¨æ˜ï¼šåŠ å…¥ AFM æ˜¾è‘—æå‡äº†æ‰€æœ‰æŒ‡æ ‡ï¼Œè¯´æ˜å…¶ç¡®å®æœ‰æ•ˆå¢å¼ºäº†è¯­è¨€-è§†è§‰çš„è·¨æ¨¡æ€äº¤äº’èƒ½åŠ›ã€‚

### RPO å‚è€ƒç‚¹è§£ç å™¨

Referred Point Decoderï¼ˆRPDï¼‰æ˜¯ LASO ä»»åŠ¡ä¸­ç”¨äºç”ŸæˆåŠŸèƒ½åŒºåŸŸæ©ç çš„æ ¸å¿ƒæ¨¡å—ã€‚ 

å®ƒçš„ä¸»è¦ç›®æ ‡æ˜¯ï¼š

- åˆ©ç”¨ä¸€ç»„é—®é¢˜æ¡ä»¶åŒ–çš„ affordance queries é€šè¿‡ Transformer è§£ç å™¨ä¸ç‚¹äº‘ç‰¹å¾äº¤äº’ ï¼Œç”Ÿæˆä¸€ç»„åŠ¨æ€å·ç§¯æ ¸ï¼ˆdynamic kernelsï¼‰ï¼Œæœ€ç»ˆé€šè¿‡è¿™äº› kernel å¯¹ AFM å¢å¼ºåçš„ç‚¹ç‰¹å¾è¿›è¡Œå·ç§¯ï¼Œå¾—åˆ°åˆ†å‰²æ©ç ã€‚

```python
class TransformerDecoderLayer(nn.Module):
    
    # tgt: text feature (b,l,c),  memory: up_sample (b,n,c)
    def forward(self, tgt, memory,
                tgt_mask: Optional[Tensor] = None,
                memory_mask: Optional[Tensor] = None,
                tgt_key_padding_mask: Optional[Tensor] = None,
                memory_key_padding_mask: Optional[Tensor] = None,
                pos: Optional[Tensor] = None,
                query_pos: Optional[Tensor] = None):
        # 1. Affordance Query = é—®é¢˜åµŒå…¥ï¼ˆQuestion Embeddingï¼‰X + å¯å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼ˆLearnable Position Embeddingsï¼‰
        # è¿™é‡Œtgtå°±æ˜¯Robertaç¼–ç å¾—åˆ°çš„æ–‡æœ¬ç‰¹å¾åµŒå…¥å‘é‡
        # ä½¿ç”¨ X ä½œä¸ºåˆå§‹è¾“å…¥ï¼Œç¡®ä¿æ¯ä¸ª query éƒ½å¸¦æœ‰åŸå§‹è¯­è¨€ä¸Šä¸‹æ–‡ï¼›
        # å¦‚æœåªç”¨ learnable embeddingsï¼Œæ¨¡å‹å°†å®Œå…¨ä¾èµ–éšæœºåˆå§‹åŒ–çš„å‚æ•°å»â€œçŒœâ€è¯­è¨€å«ä¹‰ï¼Œæ•ˆç‡æä½ï¼›
        q = k = self.with_pos_embed(tgt, query_pos)
        # 2. è‡ªæ³¨æ„åŠ›æœºåˆ¶: è®©æ¯ä¸ª query ä¸ä»…ç†è§£è‡ªå·±çš„è¯­ä¹‰ï¼Œè¿˜èƒ½æ„ŸçŸ¥å…¶ä»– query çš„ä¿¡æ¯ï¼Œä»è€Œå½¢æˆæ›´å®Œæ•´çš„è¯­è¨€ä¸Šä¸‹æ–‡ç†è§£ã€‚
        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,
                              key_padding_mask=tgt_key_padding_mask)
        tgt = tgt + self.dropout1(tgt2)
        tgt = self.norm1(tgt) # (b,l,c)
         
         # 3. è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶: æ¯ä¸ª affordance query éƒ½ä¼šåŸºäºå…¶è¯­è¨€è¯­ä¹‰ï¼Œä»ç‚¹äº‘ä¸­æ‰¾å‡ºæœ€ç›¸å…³çš„åŠŸèƒ½åŒºåŸŸï¼Œä»è€Œä¸ºåç»­çš„åŠ¨æ€å·ç§¯å’Œæ©ç é¢„æµ‹æä¾›åŸºç¡€ã€‚
        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),
                                   key=self.with_pos_embed(memory, pos),
                                   value=memory, attn_mask=memory_mask,
                                   key_padding_mask=memory_key_padding_mask,
                                   output_attentions = True)
        tgt = tgt + self.dropout2(tgt2)
        tgt = self.norm2(tgt) # (b,l,c)
        
        # 4. MLP: æ¯ä¸ªqueryé€šé“ç»´åº¦åšç‰¹å¾èåˆ
        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt)))) 
        tgt = tgt + self.dropout3(tgt2)
        tgt = self.norm3(tgt) # (b,l,c)
        return tgt

class PointRefer(nn.Module):

    def forward(self, text, xyz):
        ...
        #  3. Referred Point Decodingè¿‡ç¨‹
        #  3.1 åˆ©ç”¨ä¸€ç»„é—®é¢˜æ¡ä»¶åŒ–çš„ affordance queries é€šè¿‡ Transformer è§£ç å™¨ä¸ç‚¹äº‘ç‰¹å¾äº¤äº’ ï¼Œç”Ÿæˆä¸€ç»„åŠ¨æ€å·ç§¯æ ¸ï¼ˆdynamic kernelsï¼‰(b,l,c)
        t_feat = self.decoder(t_feat, up_sample.transpose(-2, -1), tgt_key_padding_mask=t_mask, query_pos=self.pos1d)
        #  3.2 å¯¹æ— æ•ˆ tokenï¼ˆpaddingï¼‰åšæ©ç æ“ä½œï¼Œé˜²æ­¢å…¶å½±å“åç»­è®¡ç®—ã€‚ (b,l,c)
        t_feat *= t_mask.unsqueeze(-1).float()
        #  3.3 æ‰§è¡Œ åŠ¨æ€å·ç§¯ï¼ˆDynamic Convolutionï¼‰ æ“ä½œï¼Œç”¨å¢å¼ºåçš„è¯­è¨€æŸ¥è¯¢å»â€œæ‰«æâ€ç‚¹äº‘ç‰¹å¾å›¾ ï¼ˆb,l,n)
        _3daffordance = torch.einsum('blc,bcn->bln', t_feat, up_sample)
        #  3.4 å¯¹affordance queryçš„å“åº”å›¾è¿›è¡Œå¹³å‡æ± åŒ–ï¼Œèåˆæ‰€æœ‰ affordance query çš„å¾—åˆ†ç»“æœã€‚ (b,n)
        _3daffordance = _3daffordance.sum(1)/(t_mask.float().sum(1).unsqueeze(-1))
        #  3.5 å°†å“åº”å€¼æ˜ å°„åˆ° [0, 1] åŒºé—´ï¼Œè¡¨ç¤ºæ¯ä¸ªç‚¹å±äºç›®æ ‡åŠŸèƒ½åŒºåŸŸçš„æ¦‚ç‡ã€‚ (b,n)
        _3daffordance = torch.sigmoid(_3daffordance)
        return _3daffordance # (b,n)
```

> PyTorch çš„ `einsum` å‡½æ•°ï¼Œå®ƒæ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§ä¸”çµæ´»çš„å¼ é‡æ“ä½œå‡½æ•°ï¼Œæ”¯æŒ**é€šè¿‡çˆ±å› æ–¯å¦æ±‚å’Œçº¦å®šï¼ˆEinstein Summation Conventionï¼‰** æ¥è¡¨è¾¾å„ç§çº¿æ€§ä»£æ•°è¿ç®—ã€‚

ä¸‹é¢è¯¦ç»†ä»‹ç»ä¸€ä¸‹åŠ¨æ€å·æœºæ ¸å·ç§¯çš„è¿‡ç¨‹:

- t_feat: è¯­è¨€æŸ¥è¯¢ç‰¹å¾ , å½¢çŠ¶ï¼š`(B, L, C)` , è¿™æ˜¯ **ç»è¿‡ Referred Point Decoder (RPD)** å¤„ç†åçš„ affordance queriesï¼Œè¡¨ç¤ºæ¯ä¸ª token å¯¹åº”çš„â€œåŠ¨æ€å·ç§¯æ ¸â€ã€‚

- up_sample: ä¸Šé‡‡æ ·åçš„ç‚¹äº‘ç‰¹å¾ , å½¢çŠ¶ï¼š`(B, C, N)`ã€‚

è€Œä¸‹é¢è¿™è¡Œä»£ç å®ç°çš„æ˜¯ä¸€ä¸ª **åŠ¨æ€å·ç§¯ï¼ˆDynamic Convolutionï¼‰** æ“ä½œï¼š

```python
_3daffordance = torch.einsum('blc,bcn->bln', t_feat, up_sample)
```

å®ƒçš„æœ¬è´¨æ˜¯ï¼š **ç”¨ä¸€ç»„ç”±è¯­è¨€å¼•å¯¼çš„åŠ¨æ€å·ç§¯æ ¸ `t_feat` å»å·ç§¯ç‚¹äº‘ç‰¹å¾ `up_sample`ï¼Œå¾—åˆ°æ¯ä¸ª token å¯¹æ¯ä¸ªç‚¹çš„å…³æ³¨å“åº”ã€‚**

è¯¦ç»†è§£é‡Š einsum è¡¨è¾¾å¼:

```python
torch.einsum('blc,bcn->bln', t_feat, up_sample)
```

| ç»´åº¦ | å«ä¹‰ |
|------|------|
| `b` | batch ç»´åº¦ï¼Œä¿æŒä¸å˜ |
| `l` | token ç»´åº¦ï¼Œä¿ç•™ä¸‹æ¥ |
| `c` | ç‰¹å¾é€šé“ç»´åº¦ï¼Œè¿›è¡Œå†…ç§¯æ“ä½œï¼ˆæ±‚å’Œï¼‰ |
| `n` | ç‚¹äº‘ç»´åº¦ï¼Œä¿ç•™ä¸‹æ¥ |

æ‰€ä»¥è¿™ä¸ªè¡¨è¾¾å¼çš„å«ä¹‰æ˜¯ï¼š

$$
\text{output}[b, l, n] = \sum_c \text{t\_feat}[b, l, c] \cdot \text{up\_sample}[b, c, n]
$$

ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºæ¯ä¸€ä¸ª batch ä¸­çš„æ•°æ®ï¼š
- æ¯ä¸ª tokenï¼ˆ`l`ï¼‰éƒ½ä¸æ‰€æœ‰ç‚¹ï¼ˆ`n`ï¼‰äº¤äº’ï¼›
- æ¯ä¸ª token å®é™…ä¸Šæ˜¯ä¸€ä¸ªåŠ¨æ€ç”Ÿæˆçš„å·ç§¯æ ¸ï¼ˆ`C Ã— 1 Ã— 1`ï¼‰ï¼Œä½œç”¨äºç‚¹äº‘ç‰¹å¾å›¾ï¼ˆ`C Ã— N`ï¼‰ï¼›
- æœ€ç»ˆè¾“å‡ºå½¢çŠ¶ä¸º `(B, L, N)`ï¼Œè¡¨ç¤ºï¼š
  - æ¯ä¸ª token å¯¹æ¯ä¸ªç‚¹çš„å…³æ³¨ç¨‹åº¦ï¼›

| è¾“å‡ºå¼ é‡ | å½¢çŠ¶ | å«ä¹‰ |
|----------|------|------|
| `_3daffordance` | `(B, L, N)` | æ¯ä¸ª token å¯¹æ¯ä¸ªç‚¹çš„å“åº”å€¼ï¼ˆå¾—åˆ†ï¼‰ |

ç„¶ååœ¨åç»­ä¼šè¿›è¡Œå¦‚ä¸‹å¤„ç†ï¼š

```python
_3daffordance = _3daffordance.sum(1) / (t_mask.float().sum(1).unsqueeze(-1))
_3daffordance = torch.sigmoid(_3daffordance)
```
å³ï¼š
- åœ¨ token ç»´åº¦æ±‚å’Œï¼ˆæˆ–å¹³å‡æ± åŒ–ï¼‰ï¼Œèåˆå¤šä¸ª token çš„å…³æ³¨ä¿¡æ¯ï¼›
- ä½¿ç”¨ sigmoid å¾—åˆ°æœ€ç»ˆçš„æ©ç ï¼Œå½¢çŠ¶ `(B, N)`ï¼›
- æ¯ä¸ªç‚¹çš„å€¼ âˆˆ [0, 1]ï¼Œè¡¨ç¤ºå…¶å±äºç›®æ ‡åŠŸèƒ½åŒºåŸŸçš„æ¦‚ç‡ï¼›

## æŸå¤±å‡½æ•°

### HM_Lossï¼ˆHybrid Mask Lossï¼‰

åœ¨ LASO æ•°æ®é›†ä¸­ï¼Œæ¨¡å‹éœ€è¦æ ¹æ®è‡ªç„¶è¯­è¨€é—®é¢˜è¯†åˆ«ç‚¹äº‘ä¸­æœ€ç›¸å…³çš„åŠŸèƒ½åŒºåŸŸï¼ˆå¦‚ grasping area, opening area ç­‰ï¼‰ï¼Œè€Œ HM_Loss æ˜¯ PointRefer æ¨¡å‹çš„ç›‘ç£ä¿¡å·ï¼Œå®ƒç»“åˆäº†ï¼š 

- Focal Loss ï¼šç”¨äºç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼›

- Dice Loss ï¼šç”¨äºè¡¡é‡é¢„æµ‹æ©ç ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„ç©ºé—´é‡åˆåº¦ï¼›

æœ€ç»ˆ loss = CELoss + DiceLossï¼Œè®©æ¨¡å‹åŒæ—¶å…³æ³¨é€ç‚¹åˆ†ç±»ç²¾åº¦å’Œæ•´ä½“åŒºåŸŸåŒ¹é…ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class HM_Loss(nn.Module):
    def __init__(self):
        """
        Hybrid Mask Loss å®ç°ï¼š
        - BCE-Focal Lossï¼ˆåŠ æƒäº¤å‰ç†µï¼‰
        - Dice Lossï¼ˆè¡¡é‡é¢„æµ‹æ©ç ä¸ GT çš„é‡åˆåº¦ï¼‰

        å…¬å¼æ¥è‡ªè®ºæ–‡ Section 4.2ï¼Œç”¨äºè¯­è¨€å¼•å¯¼ä¸‹çš„åŠŸèƒ½åŒºåŸŸåˆ†å‰²ã€‚
        """
        super(HM_Loss, self).__init__()
        # è®¾ç½® Focal Loss å‚æ•°
        self.gamma = 2      # èšç„¦å‚æ•°ï¼Œæ”¾å¤§éš¾åˆ†ç±»æ ·æœ¬å½±å“
        self.alpha = 0.25   # å¹³è¡¡å› å­ï¼Œå¼ºè°ƒæ­£ç±»ï¼ˆå‰æ™¯ç‚¹ï¼‰loss

    def forward(self, pred, target):
        """
        è¾“å…¥ï¼š
            pred: æ¨¡å‹è¾“å‡ºçš„åŸå§‹ logit æˆ–ç»è¿‡ sigmoid çš„æ¦‚ç‡å€¼ï¼›
                  å½¢çŠ¶ä¸º [B, N]
            target: ground truth æ©ç ï¼ˆsoft maskï¼‰ï¼Œå½¢çŠ¶ä¹Ÿä¸º [B, N]

        è¿”å›ï¼š
            total_loss: CELoss + DiceLoss çš„åŠ æƒå’Œ
        """

        # Step 1: æ„å»º Focal Loss æƒé‡é¡¹
        # temp1ï¼šè´Ÿç±» lossï¼ˆèƒŒæ™¯ç‚¹ï¼‰
        # temp2ï¼šæ­£ç±» lossï¼ˆç›®æ ‡åŠŸèƒ½åŒºåŸŸï¼‰
        # 1e-6 çš„åŠ å…¥æ˜¯ä¸ºäº†è®© log è®¡ç®—ä¿æŒç¨³å®šï¼Œå°¤å…¶æ˜¯åœ¨é¢„æµ‹å€¼æ¥è¿‘æç«¯å€¼ï¼ˆ0 æˆ– 1ï¼‰æ—¶
        temp1 = -(1 - self.alpha) * torch.mul(
            pred ** self.gamma,
            torch.mul(1 - target, torch.log(1 - pred + 1e-6))
        )
        temp2 = -self.alpha * torch.mul(
            (1 - pred) ** self.gamma,
            torch.mul(target, torch.log(pred + 1e-6))
        )

        # å°†ä¸¤ä¸ªæ–¹å‘çš„ loss åˆå¹¶ï¼Œå¹¶å– batch å’Œç‚¹ç»´åº¦çš„å¹³å‡
        temp = temp1 + temp2
        CELoss = torch.sum(torch.mean(temp, dim=(0, 1)))

        # Step 2: è®¡ç®—æ­£ç±» Dice Lossï¼ˆé¢„æµ‹ä¸ Ground Truth çš„äº¤é›† / å¹¶é›†ï¼‰
        intersection_positive = torch.sum(pred * target, dim=1)
        cardinality_positive = torch.sum(torch.abs(pred) + torch.abs(target), dim=1)
        dice_positive = (intersection_positive + 1e-6) / (cardinality_positive + 1e-6)

        # Step 3: è®¡ç®—è´Ÿç±» Dice Lossï¼ˆéç›®æ ‡åŒºåŸŸåŒ¹é…åº¦ï¼‰
        intersection_negative = torch.sum((1 - pred) * (1 - target), dim=1)
        cardinality_negative = torch.sum(2 - torch.abs(pred) - torch.abs(target), dim=1)
        dice_negative = (intersection_negative + 1e-6) / (cardinality_negative + 1e-6)

        # Step 4: æ„å»º Dice Lossï¼Œå½¢å¼ä¸º 1 - Dice Score
        # ä½¿ç”¨äº†ä¸€ä¸ªåç½®é¡¹ 1.5ï¼ˆå¯èƒ½æ˜¯ç»éªŒè®¾å®šï¼‰
        temp3 = torch.mean(1.5 - dice_positive - dice_negative, dim=0)
        DICELoss = torch.sum(temp3)

        # Step 5: æ€»æŸå¤± = åˆ†ç±»è¯¯å·® + åŒºåŸŸåŒ¹é…è¯¯å·®
        return CELoss + 1.0 * DICELoss
```

åœ¨è®ºæ–‡ Section 4.2 ä¸­æåˆ°ï¼š

> â€œWe solely employ Dice loss and Binary Cross-Entropy (BCE) loss to guide the segmentation mask prediction.â€ 

è™½ç„¶è¿™é‡Œç”¨çš„æ˜¯ Focal Loss + Dice Loss çš„ç»„åˆå½¢å¼ï¼Œä½†å®ƒæœ¬è´¨ä¸Šæ˜¯ BCE + Dice çš„æ”¹è¿›ç‰ˆï¼Œå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

- Focal Loss: æŠ‘åˆ¶ easy examplesï¼Œæ”¾å¤§ hard examplesï¼Œé˜²æ­¢å¿½ç•¥å°åŒºåŸŸ

- Dice Loss: å…³æ³¨æ•´ä½“æ©ç åŒ¹é…åº¦ï¼Œæå‡è¾¹ç•Œè¯†åˆ«èƒ½åŠ›

ä¸¤è€…ç»“åˆå¯ä»¥ï¼š

- ç¼“è§£ç±»åˆ«æåº¦ä¸å¹³è¡¡é—®é¢˜ï¼›

- æé«˜æ¨¡å‹å¯¹è¯­è¨€æŒ‡ä»¤ä¸‹åŠŸèƒ½åŒºåŸŸçš„ç†è§£èƒ½åŠ›ï¼›

- æ›´å¥½åœ°åº”å¯¹ LASO ä¸­çš„è¯­è¨€å¼•å¯¼ + soft mask åœºæ™¯ï¼›

## è®­ç»ƒ

æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹å¤§ä½“åˆ†ä¸ºäº† å‡†å¤‡ï¼Œè®­ç»ƒï¼Œè¯„ä¼° ä¸‰ä¸ªæµç¨‹ï¼›å‡†å¤‡é˜¶æ®µä¸»è¦å®Œæˆæ•°æ®é›†åŠ è½½ï¼Œæ¨¡å‹åˆå§‹åŒ–ï¼ŒæŸå¤±å‡½æ•°å®šä¹‰ï¼Œä¼˜åŒ–å™¨è®¾ç½®ï¼Œå­¦ä¹ ç‡è°ƒåº¦å™¨åˆå§‹åŒ–ç­‰ï¼›

```python
def main(opt, dict):
    # 1. åŠ è½½è®­ç»ƒé›†ï¼ŒéªŒè¯é›†ï¼Œæµ‹è¯•é›†
    train_dataset = AffordQ('train')
    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8 ,shuffle=True, drop_last=True)
    val_dataset = AffordQ('val')
    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=8, shuffle=False)
    test_dataset = AffordQ('test')
    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=8, shuffle=False)
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = get_PointRefer(emb_dim=dict['emb_dim'],
                       proj_dim=dict['proj_dim'], num_heads=dict['num_heads'], N_raw=dict['N_raw'],
                       num_affordance = dict['num_affordance'], n_groups=opt.n_groups)
     
    # 3. åˆå§‹åŒ–æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    criterion_hm = HM_Loss()
    criterion_ce = nn.CrossEntropyLoss()
    param_dicts = [
    {"params": [p for n, p in model.named_parameters() if "text_encoder" not in n and p.requires_grad]},
    {"params": [p for n, p in model.named_parameters() if "text_encoder" in n and p.requires_grad], "lr": opt.tlr}]
    optimizer = torch.optim.Adam(params = param_dicts, lr=dict['lr'], betas=(0.9, 0.999), eps=1e-8, weight_decay=opt.decay_rate)
    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=dict['Epoch'], eta_min=1e-6)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
```
è®­ç»ƒé˜¶æ®µåˆ™æ˜¯æ¨¡å‹çš„æ ¸å¿ƒè¿­ä»£è¿‡ç¨‹ï¼ŒåŒ…æ‹¬å‰å‘ä¼ æ’­ï¼ŒæŸå¤±è®¡ç®—ï¼Œåå‘ä¼ æ’­ï¼Œå‚æ•°æ›´æ–°ç­‰:

```python
    '''
    Training
    '''
    for epoch in range(start_epoch+1, dict['Epoch']):
        num_batches = len(train_loader)
        loss_sum = 0
        total_point = 0
        model = model.train()

        for i,(point, cls, gt_mask, question, aff_label) in enumerate(train_loader):
            optimizer.zero_grad()      
            # 4. å‰å‘ä¼ æ’­è¿‡ç¨‹
            _3d = model(question, point)
            # 5. è®¡ç®—æŸå¤±
            loss_hm = criterion_hm(_3d, gt_mask)
            # loss_ce = criterion_ce(logits, cls)
            # 6. åå‘ä¼ æ’­
            temp_loss = loss_hm # + opt.loss_cls*loss_ce
            temp_loss.backward()
            optimizer.step()
        
        results = torch.zeros((len(val_dataset), 2048, 1))
        targets = torch.zeros((len(val_dataset), 2048, 1))
```

è¯„ä¼°é˜¶æ®µåˆ™æ˜¯åœ¨éªŒè¯é›†æˆ–æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œè®¡ç®—æŒ‡æ ‡åŒ…æ‹¬ MAEï¼ŒSIMï¼ŒAUCï¼ŒmIoUã€‚

åœ¨ **LASOï¼ˆLanguage-guided Affordance Segmentation on 3D Objectï¼‰ä»»åŠ¡** ä¸­ï¼Œä½œè€…ä½¿ç”¨äº†å››ä¸ªæ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹å¯¹è¯­è¨€å¼•å¯¼ä¸‹åŠŸèƒ½åŒºåŸŸçš„è¯†åˆ«èƒ½åŠ›ï¼š

| æŒ‡æ ‡ | åç§° | è‹±æ–‡å…¨ç§° |
|------|------|------------|
| MAE | å¹³å‡ç»å¯¹è¯¯å·® | Mean Absolute Error |
| SIM | ç›¸ä¼¼æ€§å¾—åˆ† | Similarity Score |
| AUC | æ›²çº¿ä¸‹é¢ç§¯ | Area Under the Curve |
| mIoU | å¹³å‡äº¤å¹¶æ¯” | mean Intersection over Union |

---

1. MAEï¼ˆMean Absolute Errorï¼‰æ˜¯é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼Œç”¨äºè¡¡é‡æ¨¡å‹è¾“å‡ºçš„ soft mask ä¸ ground truth æ©ç ä¹‹é—´çš„é€ç‚¹åå·®ã€‚

$$
\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |\hat{y}_i - y_i|
$$

å…¶ä¸­ï¼š

- $N$ï¼šç‚¹äº‘ä¸­ç‚¹çš„æ•°é‡ï¼›

- $\hat{y}_i$ï¼šæ¨¡å‹é¢„æµ‹è¯¥ç‚¹å±äºåŠŸèƒ½åŒºåŸŸçš„æ¦‚ç‡ï¼›

- $y_i$ï¼šground truth æ ‡ç­¾ï¼ˆå¯ä»¥æ˜¯ soft mask æˆ– binary maskï¼‰ï¼›

ç‰¹ç‚¹ä¸ä½œç”¨ï¼š

| ç‰¹æ€§ | æè¿° |
|--------|--------|
| âœ”ï¸ æ”¯æŒ soft mask è¾“å…¥ | ä¸ä¾èµ– thresholdingï¼Œé€‚ç”¨äºè¿ç»­å“åº”å€¼ |
| âœ”ï¸ è¡¡é‡æ•´ä½“åˆ†å¸ƒä¸€è‡´æ€§ | åæ˜ æ¨¡å‹æ˜¯å¦å‡†ç¡®å­¦ä¹ è¯­è¨€å¼•å¯¼ä¸‹çš„å“åº”å¼ºåº¦ |
| âš ï¸ å¯¹è¾¹ç•Œæ¨¡ç³ŠåŒºåŸŸä¸æ•æ„Ÿ | IoU ç­‰æŒ‡æ ‡æ›´å…³æ³¨é‡åˆåº¦ |

---

2. SIMï¼ˆSimilarityï¼‰æ˜¯ä¸€ç§åŸºäºç›´æ–¹å›¾äº¤é›†çš„ç›¸ä¼¼æ€§æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„åŒ¹é…ç¨‹åº¦ã€‚å®ƒå¸¸ç”¨äºå›¾åƒæ£€ç´¢ã€å›¾åƒåˆ†å‰²ç­‰ä»»åŠ¡ã€‚

$$
\text{SIM} = \sum_i \min(\hat{y}_i, y_i)
$$

å³ï¼šå¯¹æ¯ä¸ªç‚¹å–é¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¸­çš„è¾ƒå°è€…ï¼Œç„¶åæ±‚å’Œã€‚ä¹Ÿå¯ä»¥å½’ä¸€åŒ–ä¸ºï¼š

$$
\text{SIM} = \frac{\sum_i \min(\hat{y}_i, y_i)}{\sum_i y_i}
$$

ç‰¹ç‚¹ä¸ä½œç”¨ï¼š

| ç‰¹æ€§ | æè¿° |
|--------|--------|
| âœ”ï¸ ä¸éœ€è¦ thresholding | æ”¯æŒ soft mask è¾“å…¥ |
| âœ”ï¸ å¼ºè°ƒåˆ†å¸ƒåŒ¹é… | ä¸ä»…çœ‹äº¤é›†ï¼Œè¿˜çœ‹å“åº”å¼ºåº¦åˆ†å¸ƒ |
| âœ”ï¸ å¯¹è¾¹ç•Œæ¨¡ç³ŠåŒºåŸŸå‹å¥½ | ä¸åƒ IoU é‚£æ ·ä¾èµ– hard threshold |
| âš ï¸ ä¸ç›´æ¥ä¼˜åŒ–æœ€ç»ˆç›®æ ‡ | ä¸èƒ½ä½œä¸º loss ä½¿ç”¨ï¼Œæ›´é€‚åˆè¯„ä¼° |

---

3. AUCï¼ˆArea Under ROC Curveï¼‰æ˜¯ Receiver Operating Characteristic (ROC) æ›²çº¿ä¸‹çš„é¢ç§¯ï¼Œè¡¡é‡æ¨¡å‹å¯¹äºŒåˆ†ç±»é—®é¢˜çš„åˆ¤åˆ«èƒ½åŠ›ã€‚

AUC çš„è®¡ç®—æµç¨‹å¦‚ä¸‹ï¼š

1. å°†é¢„æµ‹å€¼æ’åºï¼›

2. å¯¹ä¸åŒé˜ˆå€¼è®¡ç®— TPR å’Œ FPRï¼›

3. ç»˜åˆ¶ ROC æ›²çº¿ï¼›

4. è®¡ç®—æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰ï¼›

ç‰¹ç‚¹ä¸ä½œç”¨ï¼š

| ç‰¹æ€§ | æè¿° |
|--------|--------|
| âœ”ï¸ ä¸ä¾èµ–ç‰¹å®šé˜ˆå€¼ | è€ƒå¯Ÿæ‰€æœ‰å¯èƒ½çš„ threshold ä¸‹çš„è¡¨ç° |
| âœ”ï¸ å…³æ³¨æ’åºèƒ½åŠ› | åˆ¤æ–­æ¨¡å‹æ˜¯å¦èƒ½æ­£ç¡®åŒºåˆ†å‰æ™¯å’ŒèƒŒæ™¯ |
| âœ”ï¸ é€‚ç”¨äº binary åˆ†ç±» | éœ€è¦å…ˆå°† soft mask è½¬æ¢ä¸º binary |
| âš ï¸ å¯¹ small region æ•æ„Ÿåº¦æœ‰é™ | éœ€ç»“åˆ mIoU ä½¿ç”¨ |

---

4. mIoUï¼ˆmean Intersection over Unionï¼‰æ˜¯å›¾åƒ/ç‚¹äº‘åˆ†å‰²ä¸­æœ€å¸¸ç”¨çš„æŒ‡æ ‡ä¹‹ä¸€ï¼Œè¡¡é‡é¢„æµ‹åŒºåŸŸä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„ç©ºé—´é‡åˆåº¦ã€‚

### ğŸ”¢ å…¬å¼å¦‚ä¸‹ï¼š

$$
\text{IoU} = \frac{|X \cap Y|}{|X \cup Y|}
= \frac{\sum (\hat{y}_i \cdot y_i)}{\sum (\hat{y}_i + y_i) - \sum (\hat{y}_i \cdot y_i)}
$$

å…¶ä¸­ï¼š
- $ X $ï¼šé¢„æµ‹çš„ binary maskï¼›
- $ Y $ï¼šçœŸå®çš„ binary maskï¼›

é€šå¸¸æˆ‘ä»¬ä¼šä½¿ç”¨å¤šä¸ª thresholdï¼ˆå¦‚ `np.linspace(0, 1, 20)`ï¼‰ï¼Œç„¶åå–å¹³å‡å¾—åˆ° aiouï¼ˆaverage IoUï¼‰ã€‚

### ğŸ§  ç‰¹ç‚¹ä¸ä½œç”¨ï¼š

| ç‰¹æ€§ | æè¿° |
|--------|--------|
| âœ”ï¸ ç›´æ¥è¯„ä»·åˆ†å‰²ç²¾åº¦ | æœ€è´´è¿‘å®é™…åº”ç”¨éœ€æ±‚ |
| âœ”ï¸ å¯¹è¾¹ç•Œæ•æ„Ÿ | èƒ½åæ˜ è¾¹ç¼˜å“åº”è´¨é‡ |
| âœ”ï¸ æ˜“å— threshold å½±å“ | å¤šé˜ˆå€¼è¯„ä¼°æ›´ç¨³å®š |
| âš ï¸ ä¸æ”¯æŒ soft mask ç›´æ¥è¾“å…¥ | éœ€å…ˆ threshold æˆ binary mask |

### ğŸ¯ åœ¨ LASO ä¸­çš„åº”ç”¨ï¼š

- æ¨¡å‹è¾“å‡ºçš„æ˜¯ soft maskï¼Œéœ€å…ˆ threshold æˆ binaryï¼›
- ä½¿ç”¨å¤š threshold æé«˜é²æ£’æ€§ï¼›
- è®ºæ–‡ä¸­è¾¾åˆ° 20.8%ï¼Œè¡¨æ˜ PointRefer åœ¨è¯­è¨€å¼•å¯¼ä¸‹å…·å¤‡è¾ƒå¥½çš„åŒºåŸŸå®šä½èƒ½åŠ›ï¼›

---

## ğŸ“Š å››ä¸ªæŒ‡æ ‡å¯¹æ¯”æ€»ç»“

| æŒ‡æ ‡ | æ˜¯å¦æ”¯æŒ soft mask | æ˜¯å¦ä¾èµ– threshold | æ˜¯å¦å…³æ³¨åˆ†å¸ƒç›¸ä¼¼æ€§ | æ˜¯å¦å…³æ³¨ç©ºé—´é‡åˆåº¦ | è¾“å‡ºèŒƒå›´ |
|--------|---------------------|-------------------|---------------------|-----------------------|------------|
| **MAE** | âœ… æ˜¯ | âŒ å¦ | âœ… æ˜¯ | âŒ å¦ | [0, âˆ) |
| **SIM** | âœ… æ˜¯ | âŒ å¦ | âœ… æ˜¯ | âŒ å¦ | [0, 1] |
| **AUC** | âœ… æ˜¯ï¼ˆæ’åºï¼‰ | âœ… æ˜¯ï¼ˆbinaryï¼‰ | âŒ å¦ | âŒ å¦ | [0, 1] |
| **mIoU** | âŒ å¦ï¼ˆéœ€å…ˆ thresholdï¼‰ | âœ… æ˜¯ | âŒ å¦ | âœ… æ˜¯ | [0, 1] |

---

## ğŸ¯ ç»“åˆè®ºæ–‡ç†è§£è¿™äº›æŒ‡æ ‡çš„æ„ä¹‰

æ¥è‡ªè®ºæ–‡ Table 3 çš„ç»“æœï¼š

| æ–¹æ³• | mIoU | AUC | SIM | MAE |
|------|-------|------|-------|------|
| **PointReferï¼ˆå®Œæ•´æ–¹æ³•ï¼‰** | **20.8%** | **87.3%** | **0.629** | **0.093** |

è¿™äº›æŒ‡æ ‡å…±åŒæ„æˆäº† LASO ä»»åŠ¡çš„è¯„ä¼°ä½“ç³»ï¼Œåˆ†åˆ«ä»ä»¥ä¸‹è§’åº¦è¡¡é‡æ¨¡å‹è¡¨ç°ï¼š

| è§’åº¦ | å¯¹åº”æŒ‡æ ‡ |
|--------|-------------|
| 1. åˆ†å¸ƒä¸€è‡´æ€§ | SIM |
| 2. åˆ†ç±»åˆ¤åˆ«èƒ½åŠ› | AUC |
| 3. é€ç‚¹è¯¯å·® | MAE |
| 4. åŒºåŸŸé‡åˆåº¦ | mIoU |

è¿™æ„å‘³ç€ï¼š
- PointRefer ä¸ä»…ç†è§£è¯­è¨€æŒ‡ä»¤ï¼›
- è¿˜èƒ½ç”Ÿæˆä¸ GT æ©ç é«˜åº¦åŒ¹é…çš„åŠŸèƒ½åŒºåŸŸï¼›
- å¹¶ä¸”åœ¨ unseen object ä¸Šä¹Ÿå…·æœ‰æ³›åŒ–èƒ½åŠ›ï¼›

---

## ğŸ§ª å®éªŒå»ºè®®ï¼šå¦‚ä½•åˆ©ç”¨è¿™äº›æŒ‡æ ‡è¿›è¡Œè®­ç»ƒç›‘æ§ï¼Ÿ

ä½ å¯ä»¥å°†è¿™äº›æŒ‡æ ‡å°è£…è¿›ä½ çš„è®­ç»ƒè„šæœ¬ä¸­ï¼Œå¹¶é€šè¿‡ TensorBoard è®°å½•å®ƒä»¬çš„å˜åŒ–è¶‹åŠ¿ï¼š

```python
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter(log_dir=os.path.join(save_path, 'logs'))

# åœ¨éªŒè¯é˜¶æ®µ
writer.add_scalar('val/AUC', AUC, epoch)
writer.add_scalar('val/mIoU', IOU, epoch)
writer.add_scalar('val/SIM', sim, epoch)
writer.add_scalar('val/MAE', mean_mae, epoch)
```

è¿™æ ·å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®æ—¶è§‚å¯Ÿå“ªäº›éƒ¨åˆ†æå‡æˆ–ä¸‹é™ï¼Œä¾¿äºè°ƒè¯•å’Œæ”¹è¿›ã€‚

---

## ğŸ“ˆ ä¸€å¥è¯æ€»ç»“

> åœ¨ LASO è¿™ç§ç±»åˆ«ä¸å¹³è¡¡ã€soft maskã€è¯­è¨€å¼•å¯¼çš„ 3D åŠŸèƒ½åŒºåŸŸè¯†åˆ«ä»»åŠ¡ä¸­ï¼Œå››ä¸ªæŒ‡æ ‡ååŒå·¥ä½œï¼š
>
> - **MAE** è¡¡é‡é€ç‚¹è¯¯å·®ï¼›
> - **SIM** è¡¡é‡åˆ†å¸ƒç›¸ä¼¼æ€§ï¼›
> - **AUC** è¡¡é‡åˆ†ç±»å™¨æ’åºèƒ½åŠ›ï¼›
> - **mIoU** è¡¡é‡ç©ºé—´é‡åˆåº¦ï¼›
>
> å®ƒä»¬å…±åŒå¸®åŠ©æˆ‘ä»¬åˆ¤æ–­æ¨¡å‹æ˜¯å¦çœŸæ­£ç†è§£è¯­è¨€å¼•å¯¼ä¸‹çš„åŠŸèƒ½åŒºåŸŸè¯­ä¹‰ã€‚

---

å¦‚æœä½ è¿˜æƒ³äº†è§£ï¼š
- å¦‚ä½•å¯è§†åŒ–è¿™äº›æŒ‡æ ‡åœ¨ç‚¹äº‘ä¸Šçš„åˆ†å¸ƒï¼›
- å¦‚ä½•æ”¹è¿›è¿™äº›æŒ‡æ ‡ä»¥é€‚åº”åŠ¨æ€åœºæ™¯ï¼›
- æˆ–è€…å¦‚ä½•å°è£…è¿› TensorBoard æ—¥å¿—è®°å½•ï¼›

æ¬¢è¿ç»§ç»­æé—®ï¼æˆ‘å¯ä»¥ä¸ºä½ è¡¥å……å®Œæ•´å®ç°æˆ–å®éªŒåˆ†æ âœ…


```python
'''
Evalization
'''       
if ((epoch + 1) % 1 == 0):  # æ¯ä¸ª epoch ç»“æŸåéƒ½è¿›è¡Œè¯„ä¼°ï¼ˆå¯æ”¹ä¸ºæ¯å‡ ä¸ª epochï¼‰
    num = 0
    with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜å’Œè®¡ç®—èµ„æº
        logger.debug(f'EVALUATION start-------')
        num_batches = len(val_loader)
        total_MAE = 0.0
        total_point = 0
        model = model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œå…³é—­ dropout / batchnorm çš„éšæœºæ€§

        for i, (point, _, label, question, aff_label) in enumerate(val_loader):
            print(f'iteration: {i}/{len(val_loader)} start----')
            
            # å°†è¾“å…¥æ•°æ®è½¬ä¸º float å¹¶ç§»åŠ¨åˆ° GPU ä¸Šï¼ˆå¦‚æœä½¿ç”¨ GPUï¼‰
            point, label = point.float(), label.float()
            if opt.use_gpu:
                point = point.to(device)
                label = label.to(device)

            # å‰å‘ä¼ æ’­ï¼Œå¾—åˆ°é¢„æµ‹çš„ soft mask `_3d` âˆˆ [B, N]
            _3d = model(question, point)

            # è®¡ç®— MAEï¼ˆMean Absolute Errorï¼‰ï¼Œè¡¡é‡é€ç‚¹è¯¯å·®
            mae, point_nums = evaluating(_3d, label)
            total_point += point_nums
            total_MAE += mae.item()
            pred_num = _3d.shape[0]  # å½“å‰ batch çš„æ ·æœ¬æ•°

            # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹ç»“æœï¼Œä¾¿äºåç»­ç»Ÿä¸€è¯„ä¼°
            results[num : num + pred_num, :, :] = _3d.unsqueeze(-1)  # shape: [B, N, 1]
            targets[num : num + pred_num, :, :] = label.unsqueeze(-1)  # shape: [B, N, 1]
            num += pred_num  # æ›´æ–°ç´¢å¼•

        # è®¡ç®—å¹³å‡ MAEï¼ˆMean Absolute Errorï¼‰
        mean_mae = total_MAE / total_point
        results = results.detach().numpy()
        targets = targets.detach().numpy()

        # è®¡ç®— SIMï¼ˆSimilarity Metricï¼‰â€”â€”ç›´æ–¹å›¾äº¤é›†ï¼Œè¡¡é‡åˆ†å¸ƒç›¸ä¼¼æ€§
        SIM_matrix = np.zeros(targets.shape[0])
        for i in range(targets.shape[0]):
            SIM_matrix[i] = SIM(results[i], targets[i])  # SIM å‡½æ•°å®šä¹‰è§ utils.eval
        sim = np.mean(SIM_matrix)

        # åˆå§‹åŒ– AUC å’Œ IOU å­˜å‚¨æ•°ç»„
        AUC = np.zeros((targets.shape[0], targets.shape[2]))  # shape: [num_samples, 1]
        IOU = np.zeros((targets.shape[0], targets.shape[2]))
        IOU_thres = np.linspace(0, 1, 20)  # å¤šé˜ˆå€¼ä¸‹çš„ IoU è®¡ç®—

        # å°† GT æ ‡ç­¾äºŒå€¼åŒ–ï¼ˆsoft mask â†’ binary maskï¼‰
        targets_binary = (targets >= 0.5).astype(int)

        for i in range(AUC.shape[0]):
            t_true = targets_binary[i].flatten()  # çœŸå®æ ‡ç­¾
            p_score = results[i].flatten()        # æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡å€¼

            if np.sum(t_true) == 0:
                # å¦‚æœå½“å‰æ ·æœ¬æ²¡æœ‰æ­£ç±»ï¼ˆå³æ— åŠŸèƒ½åŒºåŸŸï¼‰ï¼Œæ ‡è®°ä¸º nan
                AUC[i] = np.nan
                IOU[i] = np.nan
            else:
                # è®¡ç®— AUCï¼ˆArea Under the Curveï¼‰ï¼Œè¡¡é‡åˆ†ç±»å™¨æ•´ä½“åˆ¤åˆ«èƒ½åŠ›
                auc = roc_auc_score(t_true, p_score)
                AUC[i] = auc

                # ä½¿ç”¨å¤šä¸ªé˜ˆå€¼è®¡ç®— mIoUï¼ˆmean Intersection over Unionï¼‰
                temp_iou = []
                for thre in IOU_thres:
                    p_mask = (p_score >= thre).astype(int)  # ç”¨ä¸åŒ threshold ç”Ÿæˆ binary mask
                    intersect = np.sum(p_mask & t_true)      # äº¤é›†
                    union = np.sum(p_mask | t_true)         # å¹¶é›†
                    temp_iou.append(intersect / union)      # IoU = intersect / union
                temp_iou = np.array(temp_iou)
                aiou = np.mean(temp_iou)  # å¯¹æ‰€æœ‰ threshold ä¸‹çš„ IoU å–å‡å€¼
                IOU[i] = aiou

        # æœ€ç»ˆå–æ‰€æœ‰æ ·æœ¬çš„ AUC å’Œ mIoU å‡å€¼ä½œä¸ºæœ€ç»ˆè¯„ä¼°æŒ‡æ ‡
        AUC = np.nanmean(AUC)
        IOU = np.nanmean(IOU)

        # æ‰“å°å½“å‰æ€§èƒ½æŒ‡æ ‡
        logger.debug(f'AUC:{AUC} | IOU:{IOU} | SIM:{sim} | MAE:{mean_mae}')

        current_IOU = IOU
        # å¦‚æœå½“å‰ mIoU è¶…è¿‡å†å²æœ€ä½³ï¼Œåˆ™ä¿å­˜ best model
        if current_IOU > best_IOU:
            best_IOU = current_IOU
            best_model_path = save_path + '/best_model-{}.pt'.format(sign)
            checkpoint = {
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'Epoch': epoch
            }
            torch.save(checkpoint, best_model_path)
            logger.debug(f'best model saved at {best_model_path}')
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ step
    scheduler.step()
# è®°å½•æœ€ä½³éªŒè¯é›† mIoU
logger.debug(f'Best Val IOU:{best_IOU}')

# æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°
category_metrics, affordance_metrics, overall_metrics = evaluate(model, test_loader, device, 3)
print_metrics_in_table(category_metrics, affordance_metrics, overall_metrics, logger)
```

## éƒ¨ç½²

