---
title: GREAT è®ºæ–‡è§£è¯» 
icon: file
category:
  - 3D-VL
  - 3D Affordance
tag:
  - 3D-VL
  - 3D Affordance
  - ç¼–è¾‘ä¸­
footer: æŠ€æœ¯å…±å»ºï¼ŒçŸ¥è¯†å…±äº«
date: 2025-06-15
cover: assets/cover/GREAT.png
author:
  - BinaryOracle
---

`GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding è®ºæ–‡è§£è¯»` 

<!-- more -->

> è®ºæ–‡: [https://arxiv.org/abs/2411.19626](https://arxiv.org/abs/2411.19626)
> ä»£ç : [https://github.com/yawen-shao/GREAT_code](https://github.com/yawen-shao/GREAT_code)
> æ•°æ®é›†: [https://drive.google.com/drive/folders/1n_L_mSmVpAM-1ASoW2T2MltYkaiA_X9X](https://drive.google.com/drive/folders/1n_L_mSmVpAM-1ASoW2T2MltYkaiA_X9X)


## æ‘˜è¦

GREATï¼ˆGeometry-Intention Collaborative Inferenceï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æŒ–æ˜ç‰©ä½“çš„ä¸å˜å‡ ä½•å±æ€§å’Œæ½œåœ¨äº¤äº’æ„å›¾ï¼Œä»¥å¼€æ”¾è¯æ±‡çš„æ–¹å¼å®šä½3Dç‰©ä½“çš„åŠŸèƒ½åŒºåŸŸï¼ˆaffordanceï¼‰ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè®¾è®¡äº†å¤šå¤´éƒ¨åŠŸèƒ½é“¾å¼æ€ç»´ï¼ˆMHACoTï¼‰ç­–ç•¥ï¼Œé€æ­¥åˆ†æäº¤äº’å›¾åƒä¸­çš„å‡ ä½•å±æ€§å’Œäº¤äº’æ„å›¾ï¼Œå¹¶é€šè¿‡è·¨æ¨¡æ€è‡ªé€‚åº”èåˆæ¨¡å—ï¼ˆCMAFMï¼‰å°†è¿™äº›çŸ¥è¯†ä¸ç‚¹äº‘å’Œå›¾åƒç‰¹å¾ç»“åˆï¼Œå®ç°ç²¾å‡†çš„3DåŠŸèƒ½å®šä½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ç›®å‰æœ€å¤§çš„3DåŠŸèƒ½æ•°æ®é›†PIADv2ï¼ŒåŒ…å«15Käº¤äº’å›¾åƒå’Œ38Kæ ‡æ³¨çš„3Dç‰©ä½“å®ä¾‹ã€‚å®éªŒè¯æ˜äº†GREATåœ¨å¼€æ”¾è¯æ±‡åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ç®€ä»‹

Open-Vocabulary 3Då¯¹è±¡åŠŸèƒ½å®šä½ï¼ˆOVAGï¼‰æ—¨åœ¨é€šè¿‡ä»»æ„æŒ‡ä»¤å®šä½ç‰©ä½“ä¸Šæ”¯æŒç‰¹å®šäº¤äº’çš„â€œåŠ¨ä½œå¯èƒ½æ€§â€åŒºåŸŸï¼Œå¯¹æœºå™¨äººæ„ŸçŸ¥ä¸æ“ä½œè‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ï¼ˆå¦‚[IAGNet](https://arxiv.org/abs/2303.10437)ã€[LASO](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_LASO_Language-guided_Affordance_Segmentation_on_3D_Object_CVPR_2024_paper.pdf)ï¼‰é€šè¿‡ç»“åˆæè¿°äº¤äº’çš„å›¾åƒæˆ–è¯­è¨€ä¸3Då‡ ä½•ç»“æ„å¼•å…¥å¤–éƒ¨å…ˆéªŒï¼Œä½†å­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼ˆå¦‚å›¾1(b)æ‰€ç¤ºï¼‰ï¼š  

- **è¯­ä¹‰ç©ºé—´å—é™**ï¼šä¾èµ–é¢„å®šä¹‰ç±»åˆ«ï¼Œéš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„åŠŸèƒ½ï¼ˆå¦‚å°†â€œpourâ€é”™è¯¯åˆ†ç±»ä¸ºâ€œgraspâ€ï¼‰ã€‚  
               
- **å‡ ä½•ä¸æ„å›¾åˆ©ç”¨ä¸è¶³**ï¼šæœªå……åˆ†æŒ–æ˜ç‰©ä½“é—´å…±äº«çš„å‡ ä½•ä¸å˜æ€§ï¼ˆå¦‚æ‰‹æŸ„çš„æŠ“æ¡å±æ€§ï¼‰å’ŒåŒä¸€ç‰©ä½“çš„å¤šäº¤äº’æ„å›¾å…³è”ã€‚  

![](GREAT/1.png)

**äººç±»è®¤çŸ¥å¯å‘**:

ç ”ç©¶è¡¨æ˜ï¼ˆ[Gick & Holyoak, 1980](https://www.sciencedirect.com/science/article/abs/pii/0010028580900134)ï¼‰ï¼Œäººç±»é€šè¿‡å¤šæ­¥æ¨ç†å’Œç±»æ¯”æ€ç»´è§£å†³å¤æ‚ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œè§‚å¯Ÿå€’æ°´åœºæ™¯æ—¶ï¼ˆå›¾1(c)ï¼‰ï¼Œäººç±»ä¼šï¼š  

1. è¯†åˆ«äº¤äº’éƒ¨ä»¶ï¼ˆå£¶å˜´ï¼‰  

2. æå–å‡ ä½•å±æ€§ï¼ˆå€¾æ–œæ›²é¢ï¼‰  

3. æ¨ç†æ½œåœ¨æ„å›¾ï¼ˆå€’æ°´/æ³¨æ°´ï¼‰  

**æ–¹æ³•åˆ›æ–°**:  

GREATæ¡†æ¶é€šè¿‡ä»¥ä¸‹è®¾è®¡æ¨¡æ‹Ÿè¿™ä¸€è¿‡ç¨‹ï¼ˆå›¾1(d)ï¼‰ï¼š  

1. **MHACoTæ¨ç†é“¾**ï¼šåŸºäºå¾®è°ƒçš„MLLMï¼ˆå¦‚[InternVL](https://arxiv.org/abs/2404.16821)ï¼‰åˆ†æ­¥æ¨ç†ï¼š  

   - **Object-Head**ï¼šå®šä½äº¤äº’éƒ¨ä»¶å¹¶åˆ†æå‡ ä½•ç»“æ„ï¼ˆå¦‚â€œä¸ºä»€ä¹ˆå£¶å˜´é€‚åˆå€’æ°´â€ï¼‰  

   - **Affordance-Head**ï¼šæè¿°å®é™…äº¤äº’ï¼ˆå¦‚â€œæ¡æŸ„å€’æ°´â€ï¼‰å¹¶è”æƒ³æ½œåœ¨æ„å›¾ï¼ˆå¦‚â€œæ³¨æ°´/æ¸…æ´—â€ï¼‰  

2. **è·¨æ¨¡æ€èåˆ**ï¼šé€šè¿‡CMAFMæ¨¡å—å°†å‡ ä½•å±æ€§ï¼ˆ$\mathbf{\hat{T}}_o$ï¼‰ä¸äº¤äº’æ„å›¾ï¼ˆ$\mathbf{\hat{T}}_a$ï¼‰æ³¨å…¥ç‚¹äº‘ï¼ˆ$\mathbf{F}_{tp}$ï¼‰å’Œå›¾åƒç‰¹å¾ï¼ˆ$\mathbf{F}_{ti}$ï¼‰ï¼Œæœ€ç»ˆè§£ç ä¸º3DåŠŸèƒ½çƒ­å›¾ $\phi = \sigma(f_\phi(\mathbf{F}_\alpha))$ã€‚  

**æ•°æ®é›†è´¡çŒ®**:

æ‰©å±•æ„å»ºäº†**PIADv2**ï¼ˆå¯¹æ¯”è§è¡¨1ï¼‰ï¼š  

- **è§„æ¨¡**ï¼š15Käº¤äº’å›¾åƒï¼ˆÃ—3ï¼‰å’Œ38K 3Då®ä¾‹ï¼ˆÃ—5ï¼‰  

- **å¤šæ ·æ€§**ï¼š43ç±»ç‰©ä½“ã€24ç±»åŠŸèƒ½ï¼Œè¦†ç›–å¤šå¯¹å¤šå…³è”ï¼ˆå›¾3(c)ï¼‰  

![](GREAT/2.png)

![](GREAT/3.png)

## ç›¸å…³å·¥ä½œ

**1. Affordance Grounding**  

ç°æœ‰ç ”ç©¶ä¸»è¦ä»2Dæ•°æ®ï¼ˆå¦‚å›¾åƒã€è§†é¢‘ï¼‰å’Œè‡ªç„¶è¯­è¨€ç†è§£å‡ºå‘ï¼Œå®šä½â€œåŠ¨ä½œå¯èƒ½æ€§â€åŒºåŸŸã€‚ä¾‹å¦‚ï¼Œéƒ¨åˆ†å·¥ä½œé€šè¿‡è¯­è¨€ç†è§£åœ¨2Dæ•°æ®ä¸­å®šä½åŠŸèƒ½åŒºåŸŸï¼ˆ[3](https://arxiv.org/abs/2405.12461), [21](https://arxiv.org/abs/2311.17776)ï¼‰ï¼Œä½†æœºå™¨äººæ“ä½œéœ€è¦3Dä¿¡æ¯ï¼Œ2Dæ–¹æ³•éš¾ä»¥ç›´æ¥è¿ç§»ã€‚éšç€3Dæ•°æ®é›†ï¼ˆå¦‚[5](https://arxiv.org/abs/2212.08051), [6](https://arxiv.org/abs/2103.16397)ï¼‰çš„å‡ºç°ï¼Œéƒ¨åˆ†ç ”ç©¶å¼€å§‹æ˜ å°„è¯­ä¹‰åŠŸèƒ½åˆ°3Dç»“æ„ï¼Œä½†å—é™äºé¢„å®šä¹‰ç±»åˆ«ï¼Œæ— æ³•å¤„ç†å¼€æ”¾è¯æ±‡åœºæ™¯ã€‚  

**2. Open-Vocabulary 3D Affordance Grounding (OVAG)**  

OVAGæ—¨åœ¨é€šè¿‡é¢å¤–æŒ‡ä»¤ï¼ˆå¦‚æ–‡æœ¬æˆ–å›¾åƒï¼‰å¼•å…¥äº¤äº’å…ˆéªŒï¼Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼š  

- IAGNet  åˆ©ç”¨2Däº¤äº’è¯­ä¹‰æŒ‡å¯¼3DåŠŸèƒ½å®šä½ï¼›  

- LASO é€šè¿‡æ–‡æœ¬æ¡ä»¶æŸ¥è¯¢åˆ†å‰²åŠŸèƒ½åŒºåŸŸï¼›  

- OpenAD å’Œ OpenKD åˆ©ç”¨CLIPç¼–ç å™¨å®ç°æ–‡æœ¬-ç‚¹äº‘å…³è”ã€‚  

è¿™äº›æ–¹æ³•ä»å—é™äºè®­ç»ƒè¯­ä¹‰ç©ºé—´ï¼Œè€ŒGREATé€šè¿‡å‡ ä½•-æ„å›¾ååŒæ¨ç†ï¼ˆCoTï¼‰è§£å†³æ­¤é—®é¢˜ï¼ˆå¦‚è¡¨2æ‰€ç¤ºï¼‰ã€‚ 

![](GREAT/4.png)

**3. Chain-of-Thought (CoT) ä¸å¤šæ¨¡æ€å¤§æ¨¡å‹ (MLLMs)**  

CoTåŠå…¶å˜ä½“é€šè¿‡å¤šæ­¥æ¨ç†å¢å¼ºMLLMsèƒ½åŠ›ã€‚ä¾‹å¦‚ï¼š  

- è§†è§‰ä»»åŠ¡ä¸­ï¼ŒMLLMsï¼ˆå¦‚InternVLï¼‰ç»“åˆCoTåœ¨ç›®æ ‡æ£€æµ‹ã€æœºå™¨äººæ“ä½œç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼›  

- ä½†åŠ¨æ€åŠŸèƒ½ç‰¹æ€§ä½¿å¾—MLLMséš¾ä»¥ç›´æ¥ä»äº¤äº’å›¾åƒæ¨ç†3DåŠŸèƒ½ï¼ŒGREATé€šè¿‡å¾®è°ƒMLLMså¹¶è®¾è®¡MHACoTç­–ç•¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚  

**å…³é”®é—®é¢˜**ï¼ˆå¦‚å›¾1æ‰€ç¤ºï¼‰ï¼š  

- ç°æœ‰æ–¹æ³•ä¾èµ–æ•°æ®å¯¹é½ï¼Œæ³›åŒ–æ€§ä¸è¶³ï¼ˆå¦‚å°†â€œpourâ€è¯¯åˆ†ç±»ä¸ºâ€œgraspâ€ï¼‰ï¼›  

- GREATé€šè¿‡æ¨¡æ‹Ÿäººç±»å¤šæ­¥æ¨ç†ï¼ˆå‡ ä½•å±æ€§æå–+æ„å›¾ç±»æ¯”ï¼‰å®ç°å¼€æ”¾è¯æ±‡åŠŸèƒ½å®šä½ã€‚

## æ–¹æ³•

**1. æ¡†æ¶æ¦‚è¿°**  

![](GREAT/5.png)

GREAT çš„è¾“å…¥ä¸ºç‚¹äº‘ $P \in \mathbb{R}^{N \times 4}$ï¼ˆå«åæ ‡ $P_c$ å’ŒåŠŸèƒ½æ ‡æ³¨ $P_{label}$ï¼‰å’Œå›¾åƒ $I \in \mathbb{R}^{3 \times H \times W}$ï¼Œè¾“å‡ºä¸º3DåŠŸèƒ½åŒºåŸŸ $\phi = f_\theta(P_c, I)$ã€‚æ•´ä½“æµç¨‹ï¼ˆå¦‚å›¾2æ‰€ç¤ºï¼‰åŒ…æ‹¬ï¼š  

- é€šè¿‡ ResNet [9](https://arxiv.org/abs/1512.03385) å’Œ PointNet++ [43](https://arxiv.org/abs/1706.02413) æå–ç‰¹å¾ $\mathbf{F}_i$ å’Œ $\mathbf{F}_p$ï¼›  

- åˆ©ç”¨å¾®è°ƒçš„ MLLMï¼ˆInternVL [4](https://arxiv.org/abs/2404.16821)ï¼‰è¿›è¡Œå¤šæ­¥æ¨ç†ï¼ˆMHACoTï¼‰ï¼›  

- é€šè¿‡ Cross-Modal Adaptive Fusion Module (CMAFM) èåˆå‡ ä½•ä¸æ„å›¾çŸ¥è¯†ï¼›  

- è§£ç å™¨è”åˆé¢„æµ‹åŠŸèƒ½åŒºåŸŸ $\phi$ï¼ŒæŸå¤±å‡½æ•°ä¸º $\mathcal{L}_{total} = \mathcal{L}_{focal} + \mathcal{L}_{dice}$ã€‚  

**2. å¤šæ­¥æ¨ç†ï¼ˆMHACoTï¼‰**  

åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š  

- **Object-Head Reasoning**ï¼š  

  - äº¤äº’éƒ¨ä»¶å®šä½ï¼ˆæç¤ºï¼šâ€œ*Point out which part interacts...*â€ï¼‰ï¼›  

  - å‡ ä½•å±æ€§æ¨ç†ï¼ˆæç¤ºï¼šâ€œ*Explain why this part can interact...*â€ï¼‰ï¼Œç”Ÿæˆç‰¹å¾ $\mathbf{T}_o$ã€‚  

- **Affordance-Head Reasoning**ï¼š  

  - äº¤äº’è¿‡ç¨‹æè¿°ï¼ˆæç¤ºï¼šâ€œ*Describe the interaction...*â€ï¼‰ï¼›  

  - æ½œåœ¨æ„å›¾ç±»æ¯”ï¼ˆæç¤ºï¼šâ€œ*List two additional interactions...*â€ï¼‰ï¼Œç”Ÿæˆç‰¹å¾ $\mathbf{T}_a$ã€‚  

é€šè¿‡ Roberta [28](https://arxiv.org/abs/1907.11692) ç¼–ç åï¼Œäº¤å‰æ³¨æ„åŠ›å¯¹é½ç‰¹å¾ï¼š  

$$\bar{\mathbf{T}}_o = f_\delta(f_m(\mathbf{T}_o, \mathbf{T}_a)), \quad \bar{\mathbf{T}}_a = f_\delta(f_m(\mathbf{T}_a, \mathbf{T}_o))$$  

**3. è·¨æ¨¡æ€èåˆï¼ˆCMAFMï¼‰**  

- å°†å‡ ä½•çŸ¥è¯† $\bar{\mathbf{T}}_o$ æ³¨å…¥ç‚¹äº‘ç‰¹å¾ $\mathbf{F}_p$ï¼š  

  - æŠ•å½±ä¸º $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ï¼Œè®¡ç®—äº¤å‰æ³¨æ„åŠ› $\mathbf{F}'_p$ï¼ˆå…¬å¼2ï¼‰ï¼›  

  - é€šè¿‡å…¨è¿æ¥å±‚å’Œå·ç§¯èåˆï¼Œå¾—åˆ° $\mathbf{F}_{tp}$ã€‚  

- å°†æ„å›¾çŸ¥è¯† $\bar{\mathbf{T}}_a$ ç›´æ¥ä¸å›¾åƒç‰¹å¾ $\mathbf{F}_i$ æ‹¼æ¥ï¼Œå¾—åˆ° $\mathbf{F}_{ti}$ã€‚  

**4. è§£ç ä¸è¾“å‡º**  

èåˆç‰¹å¾ $\mathbf{F}_\alpha = f[\Gamma(\mathbf{F}_{ti}), \mathbf{F}_{tp}]$ é€šè¿‡è§£ç å™¨ç”ŸæˆåŠŸèƒ½çƒ­å›¾ $\phi = \sigma(f_\phi(\mathbf{F}_\alpha))$ï¼Œå…¶ä¸­ $\sigma$ ä¸º Sigmoid å‡½æ•°ã€‚  
 
**å…³é”®è®¾è®¡**ï¼ˆå¦‚å›¾5æ‰€ç¤ºï¼‰ï¼š  

- **å‡ ä½•-æ„å›¾ååŒ**ï¼šMHACoT åŒæ—¶å»ºæ¨¡ç‰©ä½“å±æ€§å’Œäº¤äº’æ„å›¾ï¼Œæå‡å¼€æ”¾è¯æ±‡æ³›åŒ–æ€§ï¼›  

- **åŠ¨æ€èåˆ**ï¼šCMAFM è‡ªé€‚åº”å¯¹é½ç‚¹äº‘ä¸å›¾åƒæ¨¡æ€ï¼Œé¿å…ç‰¹å¾åå·®(å¦‚è¡¨3æ¶ˆèå®éªŒæ‰€ç¤º)

## ä»£ç 

### Multi-Head Affordance Chain-of-Thought

MHACoTæ˜¯ä¸€ç§**ç±»äººæ¨ç†æ–¹å¼**ï¼Œåˆ†å¤šä¸ªæ­¥éª¤ï¼Œæ¨¡æ‹Ÿäººè§‚å¯Ÿäº¤äº’å›¾åƒæ—¶çš„æ€ç»´é“¾æ¡ï¼š

1. **è¯†åˆ«äº¤äº’éƒ¨ä½**ï¼ˆObject Interaction Perceptionï¼‰

2. **è§£æå‡ ä½•å±æ€§**ï¼ˆGeometric Structure Reasoningï¼‰

3. **è¯¦ç»†æè¿°äº¤äº’**ï¼ˆInteraction Detailed Descriptionï¼‰

4. **ç±»æ¯”é¢å¤–äº¤äº’**ï¼ˆInteractive Analogical Reasoningï¼‰

æ¯ä¸ªå­æ­¥éª¤éƒ½ç”±ä¸€ä¸ª prompt å¼•å¯¼ MLLMï¼ˆå¦‚ InternVLï¼‰åšå›ç­”ï¼Œä»è€Œè·å¾—ï¼š

* å¯¹è±¡çš„äº¤äº’åŒºåŸŸ


> **Object Interaction Perception**
> Prompt 1: Point out which part of the object in the image interacts with the person.

ğŸ”¹ç›®æ ‡ï¼šå®šä½äº¤äº’å‘ç”Ÿçš„å¯¹è±¡åŒºåŸŸï¼ˆå¦‚â€œæ°´å£¶çš„å£¶å˜´â€ï¼‰

---
* å¯¹åº”çš„å‡ ä½•å±æ€§

> **Geometric Structure Reasoning**
> Prompt 2: Explain why this part can interact from the geometric structure of the object.

ğŸ”¹ç›®æ ‡ï¼šæ¨ç†å‡ ä½•å½¢æ€æ”¯æŒè¯¥äº¤äº’ï¼ˆå¦‚â€œå£¶å˜´ä¸Šå¼€å£ç‹­çª„ã€å¸¦æ›²çº¿â€ï¼‰

---
* å½“å‰äº¤äº’è¡Œä¸º

> **Interaction Detailed Description**
> Prompt 3: Describe the interaction between object and the person.

ğŸ”¹ç›®æ ‡ï¼šç»†è‡´åœ°è¯†åˆ«äº¤äº’åŠ¨ä½œåŠå…¶å‚ä¸éƒ¨ä½ï¼ˆå¦‚â€œç”¨æ‰‹æ¡ä½å£¶æŠŠå€’æ°´â€ï¼‰

---
* æ½œåœ¨äº¤äº’æ„å›¾

> **Interactive Analogical Reasoning**
> Prompt 4: List two interactions that describe additional common interactions that the object can interact with people.

ğŸ”¹ç›®æ ‡ï¼šæ¨ç†é™¤äº†å½“å‰äº¤äº’ä»¥å¤–ï¼Œè¯¥ç‰©ä½“å¸¸è§çš„å…¶ä»–äº¤äº’ï¼ˆå¦‚â€œå¼€å£¶ç›–ã€æŠ“æ¡ä¸­éƒ¨â€ï¼‰


æ ¸å¿ƒä»£ç å®ç°å¦‚ä¸‹:

```python
# 1. åŠ è½½é¢„è®­ç»ƒå¤šæ¨¡æ€å¤§æ¨¡å‹
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    #load_in_8bit=True,
    low_cpu_mem_usage=True,
    trust_remote_code=True,
    device_map=device_map).eval()
tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)

# 2. åŠ è½½å›¾åƒæ•°æ®
image_path = 'PATH/Data/Kettle/Internet/pour/kettle_pour_1.jpg'
pixel_values = load_image(image_path, max_num=12).to(torch.bfloat16).cuda()
object = image_path.split('/')[-4] # å›¾åƒæ‰€å±çš„ç‰©ä½“å

# 3. å®šä½äº¤äº’éƒ¨ä½
question1 = f'Point out which part of the {object} in the image interacts with the person. If this part is different from the part of the {object} shown in the image that performs the main function, point out the part of the {object} that performs the main function shown in the image.'
response1, history = model.chat(tokenizer, pixel_values, question1, generation_config, history=None, return_history=True)
print(f'{response1}')

# 4. æ¨ç†å‡ ä½•ç»“æ„
question2 = f'Explain why this part can interact from the geometric structure of the {object}. Just give the final result in one sentence.'
response2, history = model.chat(tokenizer, pixel_values, question2, generation_config, history=history, return_history=True)
print(f'{response2}')

# 5. è¯¦ç»†äº¤äº’è¡Œä¸º
question3 = f'Describe the interaction between {object} and the person in the image, including the interaction type, the interaction part of the {object}, and the interaction part of the person.'
response3, history= model.chat(tokenizer, pixel_values, question3, generation_config, history=history, return_history=True)
print(f'{response3}')

# 6. æ¨æµ‹å…¶ä»–äº¤äº’
question4 = f'List two interactions that describe additional common interactions that the {object} can interact with people, including the interaction type, the interaction part of the {object}, and the interaction part of the person.'
response4, history= model.chat(tokenizer, pixel_values, question4, generation_config, history=history, return_history=True)
print(f'{response4}')

'''
Sample output
1. the spout of kettle.
2. a narrow opening, a slight curve and the spout's position at the top of the kettle.
3. pour the liquid from the spout of the kettle using peopleâ€™s hand
4. grasp the kettle using person's hand around middle body, open the kettle using people's fingers on the lid

object knowledge: the spout of kettle: a narrow opening, a slight curve and the spout's position at the top of the kettle.
affordance/human knowledge: pour the liquid from the spout of the kettle using peopleâ€™s hand, grasp the kettle using person's hand around handle, open the kettle using people's fingers on the lid
''
```
å…¶ä¸­:

- å‡ ä½•ç»“æ„çŸ¥è¯† = Prompt 1 + Prompt 2 çš„å›ç­” = äº¤äº’éƒ¨ä½ + è¯¥éƒ¨ä½çš„å‡ ä½•å±æ€§æ¨ç†

- äº¤äº’çŸ¥è¯† = Prompt 3 + Prompt 4 çš„å›ç­” = å½“å‰äº¤äº’ + ç±»æ¯”/è¡¥å……çš„äº¤äº’æ–¹å¼ 

> MHACoT è¿™ä¸ªè¿‡ç¨‹å‘ç”Ÿåœ¨æ•°æ®é›†å‡†å¤‡é˜¶æ®µã€‚

### æ•°æ®é›†

> å…ˆäº†è§£ä¸€ä¸‹GREATé¡¹ç›®å¯¹åº”çš„æ•°æ®é›†ç›®å½•ç»“æ„:
>
> ![](GREAT/7.png)

æ•°æ®é›†çš„åˆå§‹åŒ–:

```python
class PIAD(Dataset):
    def __init__(self, run_type, setting_type, point_path, img_path, text_hk_path, text_ok_path, pair=2, img_size=(224, 224)):
        super().__init__()

        self.run_type = run_type # å½“å‰æ˜¯è®­ç»ƒ/æµ‹è¯•/éªŒè¯ç¯å¢ƒ
        self.p_path = point_path # ç‚¹äº‘ç´¢å¼•æ–‡ä»¶è·¯å¾„
        self.i_path = img_path  # å›¾ç‰‡ç´¢å¼•æ–‡ä»¶è·¯å¾„
        self.text_hk_path = text_hk_path # ç‰©ä½“å‡ ä½•ç»“æ„æ–‡æœ¬æ•°æ®æ–‡ä»¶è·¯å¾„
        self.text_ok_path = text_ok_path # äººç±»äº¤äº’æ–‡æœ¬æ•°æ®æ–‡ä»¶è·¯å¾„
        self.pair_num = pair  # æ§åˆ¶æ¯ä¸ª å›¾åƒæ ·æœ¬ å¯¹åº”å¤šå°‘ä¸ª 3Dç‚¹äº‘æ ·æœ¬
        self.affordance_label_list = ['grasp', 'contain', 'lift', 'open', 
                        'lay', 'sit', 'support', 'wrapgrasp', 'pour', 'move', 'display',
                        'push', 'listen', 'wear', 'press', 'cut', 'stab', 'carry', 'ride',
                        'clean', 'play', 'beat', 'speak', 'pull']  # 24
        
        ...

        '''
        Seen
        '''  # 43

        if setting_type == 'Seen':
            number_dict = {'Bag': 0, 'Microphone': 0, 'Toothbrush': 0, 'TrashCan': 0, 'Bicycle': 0,
                           'Guitar': 0, 'Glasses': 0, 'Hat': 0, 'Microwave':0, 'Backpack': 0, 'Door':0, 'Scissors': 0, 'Bowl': 0,
                           'Baseballbat': 0, 'Mop': 0, 'Dishwasher': 0, 'Bed': 0, 'Keyboard': 0, 'Clock': 0, 'Vase': 0, 'Knife': 0,
                           'Suitcase': 0, 'Hammer': 0, 'Refrigerator': 0, 'Chair': 0, 'Umbrella': 0, 'Bucket': 0,
                           'Display': 0, 'Earphone': 0, 'Motorcycle': 0, 'StorageFurniture': 0, 'Fork': 0, 'Broom': 0, 'Skateboard': 0,
                           'Tennisracket': 0, 'Laptop': 0, 'Table':0, 'Bottle': 0, 'Faucet': 0, 'Kettle': 0, 'Surfboard': 0, 'Mug': 0,
                            'Spoon': 0 
                           }  
        
        # è¯»å–æ‰€æœ‰å›¾ç‰‡è·¯å¾„ï¼Œæ‰€æœ‰äººç±»äº¤äº’æ–‡æœ¬æ•°æ®ï¼Œæ‰€æœ‰ç‰©ä½“å‡ ä½•ç»“æ„æ–‡æœ¬æ•°æ®
        self.img_files = self.read_file(self.i_path)
        self.text_human_files = self.read_file(self.text_hk_path)
        self.text_object_files = self.read_file(self.text_ok_path)
        self.img_size = img_size

        if self.run_type == 'train':
            # è¯»å–æ‰€æœ‰ç‚¹äº‘è·¯å¾„ï¼ŒåŒæ—¶è®°å½•æ¯ç±»ç‰©ä½“å¯¹åº”çš„æ ·æœ¬æ€»é‡ï¼Œæ¯”å¦‚: æ¤…å­å¯¹åº”çš„ç‚¹äº‘ä¸€å…±1000ä¸ª
            self.point_files, self.number_dict = self.read_file(self.p_path, number_dict)
            self.object_list = list(number_dict.keys()) # æ³¨æ„: Dict æŒ‰ç…§keyçš„æ’å…¥é¡ºåºè¿”å›çš„
            self.object_train_split = {}
            start_index = 0
            # è®°å½•æ¯ä¸ªç‰©ä½“å¯¹åº”çš„ç‚¹äº‘ç´¢å¼•ä¸‹æ ‡åŒºé—´
            for obj_ in self.object_list:
                temp_split = [start_index, start_index + self.number_dict[obj_]]
                self.object_train_split[obj_] = temp_split
                start_index += self.number_dict[obj_]
        else:
            self.point_files = self.read_file(self.p_path)
```
**ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦pair_numå‚æ•°?**

- é—®é¢˜èƒŒæ™¯ï¼šGREAT éœ€è¦å°† 2D äº¤äº’å›¾åƒï¼ˆImageï¼‰ä¸ 3D ç‚¹äº‘ï¼ˆPoint Cloudï¼‰çš„ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œä½†åŒä¸€ç‰©ä½“çš„ä¸åŒå®ä¾‹å¯èƒ½æœ‰å‡ ä½•å·®å¼‚ï¼ˆä¾‹å¦‚ä¸åŒå½¢çŠ¶çš„æ¤…å­ï¼‰ã€‚

- è§£å†³æ–¹æ¡ˆï¼šé€šè¿‡ä¸ºæ¯å¼ å›¾åƒé…å¯¹å¤šä¸ªç‚¹äº‘ï¼ˆpair_num > 1ï¼‰ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä» å¤šæ ·åŒ–çš„å‡ ä½•å˜ä½“ ä¸­æå–å…±æ€§çš„å‡ ä½•å±æ€§ï¼ˆå¦‚â€œå¯æŠ“æ¡â€çš„å…±äº«ç»“æ„ç‰¹å¾ï¼‰ï¼Œè€Œä¸ä»…ä»…ä¾èµ–å•ä¸€å®ä¾‹ã€‚

- ä»£ç ä½“ç°ï¼šåœ¨ __getitem__ ä¸­ï¼Œè®­ç»ƒæ—¶ä¼šå¯¹æ¯ä¸ªå›¾åƒéšæœºé‡‡æ · pair_num ä¸ªåŒç±»åˆ«ç‚¹äº‘ï¼ˆè§ point_sample_idx çš„ç”Ÿæˆé€»è¾‘ï¼‰

> GREAT é¡¹ç›®çš„æ•°æ®ç»„ç»‡ä¸­ï¼Œå°†æ¯ä¸ªæ ·æœ¬å±äºçš„ç‰©ä½“ç±»å‹ï¼Œå¾…é¢„æµ‹åŠŸèƒ½åŒºåŸŸç±»å‹å…¨éƒ¨éšå«åœ¨äº†æ ·æœ¬å¯¹åº”çš„æ–‡ä»¶è·¯å¾„ä¸­:
>
> ![](GREAT/6.png)

è·å–æ•°æ®:

```python
    def __getitem__(self, index):
        # 1. è·å–å›¾ç‰‡ï¼Œäººç±»äº¤äº’æ–‡æœ¬ï¼Œç‰©ä½“å‡ ä½•ç»“æ„æ–‡æœ¬
        img_path = self.img_files[index]
        text_hd = self.text_human_files[index]
        text_od = self.text_object_files[index]
       
        # 2.1 è¯„ä¼°æ—¶éœ€è¦æ ‡å‡†çš„å•ä¸€æ ·æœ¬å¯¹æ¯”
        if (self.run_type=='val'):
            point_path = self.point_files[index]
        else:
        # 2.2 ä»å›¾ç‰‡è·¯å¾„ä¸­æˆªå–å¾—åˆ°ç‰©ä½“åï¼Œäº¤äº’è¡Œä¸ºåï¼Œç‚¹äº‘ç´¢å¼•ä¸‹æ ‡åŒºé—´  
            object_name = img_path.split('/')[-4]
            affordance_name = img_path.split('/')[-2]
            range_ = self.object_train_split[object_name]
            # ä»ç´¢å¼•åŒºé—´ä¸­éšæœºé‡‡æ ·pair_numä¸ªç‚¹äº‘æ ·æœ¬
            point_sample_idx = random.sample(range(range_[0],range_[1]), self.pair_num)
      
            # 3. åŠ è½½ç‚¹äº‘æ ·æœ¬ï¼ŒåŒæ—¶åˆ¤æ–­æ˜¯å¦ä¸å½“å‰å›¾ç‰‡äº¤äº’è¡Œä¸ºä¸€è‡´ï¼Œä¸ä¸€è‡´åˆ™é‡æ–°éšæœºé€‰
            for i ,idx in enumerate(point_sample_idx):
                while True:
                    point_path = self.point_files[idx]
                    sele_affordance = point_path.split('/')[-2]
                    if sele_affordance == affordance_name:
                        point_sample_idx[i] = idx 
                        break
                    else:
                        idx = random.randint(range_[0],range_[1]-1)  # re-select idx
         
        Img = Image.open(img_path).convert('RGB')
        
        if(self.run_type == 'train'):
            Img = Img.resize(self.img_size)
            Img = img_normalize_train(Img)
            
            # 4. åŠ è½½åˆ—è¡¨ä¸­æ‰€æœ‰ç‚¹äº‘æ ·æœ¬
            Points_List = []
            affordance_label_List = []
            affordance_index_List = []
            for id_x in point_sample_idx:
                point_path = self.point_files[id_x]
                # åŠ è½½ç‚¹äº‘æ•°æ®å’ŒåŠŸèƒ½åŒºåŸŸæ©ç (åŠŸèƒ½åŒºåŸŸçƒ­åŠ›å›¾)
                Points, affordance_label = self.extract_point_file(point_path) # ï¼ˆ2048ï¼Œ3ï¼‰
                Points,_,_ = pc_normalize(Points)
                Points = Points.transpose() # (3,2048)
                affordance_index = self.get_affordance_label(img_path) # å½“å‰ç‚¹äº‘å¾…é¢„æµ‹çš„äº¤äº’è¡Œä¸º/åŠŸèƒ½åŒºåŸŸç±»å‹
                Points_List.append(Points)  # ç‚¹äº‘
                affordance_label_List.append(affordance_label) # åŠŸèƒ½åŒºåŸŸçƒ­åŠ›å›¾
                affordance_index_List.append(affordance_index) # å¾…é¢„æµ‹åŠŸèƒ½åŒºåŸŸç±»å‹

        else:
            Img = Img.resize(self.img_size)
            Img = img_normalize_train(Img)

            Point, affordance_label = self.extract_point_file(point_path)
            Point,_,_ = pc_normalize(Point)
            Point = Point.transpose()
 
        if(self.run_type == 'train'):
            # å›¾ç‰‡ ï¼Œ äº¤äº’ä¿¡æ¯æ–‡æœ¬ï¼Œç‰©ä½“å‡ ä½•ç»“æ„æ–‡æœ¬ï¼Œç‚¹äº‘æ ·æœ¬åˆ—è¡¨ï¼ŒåŠŸèƒ½åŒºåŸŸçƒ­åŠ›å›¾åˆ—è¡¨ï¼Œå¾…é¢„æµ‹åŠŸèƒ½åŒºåŸŸç±»å‹åˆ—è¡¨
            return Img, text_hd, text_od, Points_List, affordance_label_List, affordance_index_List
        else:
            return Img, text_hd, text_od, Point, affordance_label, img_path, point_path
```
### æ¨¡å‹

```python
class GREAT(nn.Module):
    ... 
    def forward(self, img, xyz, text_human, text_object):

        '''
        img: [B, 3, H, W]
        xyz: [B, 3, 2048]
        '''
       
        B, C, N = xyz.size()
        # 1. ç”¨Resnet18å¯¹å›¾åƒè¿›è¡Œç¼–ç ï¼Œè¿”å›çš„é«˜ç»´éšå‘é‡ç»´åº¦ä¸º (batch,512,7,7) -- ï¼ˆbatch,channel,h,w)
        F_I = self.img_encoder(img)     
        #   ç»´åº¦å±•å¹³(batch,channel,h*w)
        F_i = F_I.view(B, self.emb_dim, -1)         
        
        # 2ï¼Œ PointNet++ å¯¹ç‚¹äº‘è¿›è¡Œç¼–ç 
        F_p_wise = self.point_encoder(xyz)
        # 3. Roberta å¯¹äº¤äº’æ–‡æœ¬å’Œå‡ ä½•ç»“æ„æ–‡æœ¬è¿›è¡Œç¼–ç 
        T_h= self.text_encoder(text_human)
        T_o = self.text_encoder2(text_object)
        
        # 4. äº¤äº’æ–‡æœ¬å’Œå‡ ä½•ç»“æ„æ–‡æœ¬çš„ä¿¡æ¯é€šè¿‡æ”¹è‰¯çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œäº¤äº’èåˆ
        T_h_, T_o_ =self.affordance_dictionary_fusion(T_h, T_o)     

        # 5. äº¤äº’æ–‡æœ¬ä¿¡æ¯ä¸å›¾åƒä¿¡æ¯è¿›è¡Œèåˆ
        I_h = self.img_text_fusion(F_i,T_h_)         
        
        # 6. å‡ ä½•ç»“æ„æ–‡æœ¬ä¿¡æ¯ä¸ç‚¹äº‘ä¿¡æ¯è¿›è¡Œèåˆï¼Œç„¶åè¿›å…¥pointnet++çš„ç‰¹å¾ä¼ æ’­é˜¶æ®µ(æ’å€¼é˜¶æ®µ)ï¼Œæœ€åå†ä¸I_hè¿›è¡Œäº¤äº’èåˆ
        _3daffordance = self.decoder(T_o_, I_h.permute(0,2,1), F_p_wise)
        
        return _3daffordance
```
#### æ–‡æœ¬ç¼–ç 

ä½¿ç”¨ RoBerta å¯¹äº¤äº’æ–‡æœ¬å’Œå‡ ä½•ç»“æ„æ–‡æœ¬è¿›è¡Œç¼–ç è¿™å—ï¼Œéœ€è¦æ³¨æ„åœ¨å¯¹äº¤äº’æ–‡æœ¬è¿›è¡Œç¼–ç æ—¶ï¼Œä¼šæŒ‰ç…§ "," å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºå¤šä¸ªå¥å­ï¼Œå¯¹æ¯ä¸ªå¥å­ç‹¬ç«‹è¿›è¡Œç¼–ç :

```bash
åŸå§‹äº¤äº’æ–‡æœ¬:

pour the liquid from the spout of the kettle using peopleâ€™s hand, grasp the kettle using person's hand around handle, open the kettle using people's fingers on the lid

åˆ‡åˆ†å:

pour the liquid from the spout of the kettle using peopleâ€™s hand
grasp the kettle using person's hand around handle
open the kettle using people's fingers on the lid
```

è¿™æ ·åšçš„åŸå› æ˜¯å› ä¸ºäº¤äº’æ–‡æœ¬ç”±å½“å‰å›¾ç‰‡åæ˜ çš„äº¤äº’è¡Œä¸ºå’Œæ¨¡å‹é¢å¤–è¡¥å……çš„å½“å‰ç‰©ä½“å­˜åœ¨çš„å…¶ä»–äº¤äº’è¡Œä¸ºæ„æˆï¼Œä»–ä»¬ä¹‹é—´çš„å…³ç³»æ˜¯ç‹¬ç«‹çš„ã€‚è€Œå‡ ä½•ç»“æ„æ–‡æœ¬åˆ™æ˜¯å•ä¸€è¿è´¯çš„å‡ ä½•æè¿°ï¼Œæ— éœ€åˆ‡åˆ†ï¼Œç›´æ¥å¯¹æ•´å¥è¿›è¡Œç¼–ç ã€‚



