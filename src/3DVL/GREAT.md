---
title: GREAT è®ºæ–‡è§£è¯» 
icon: file
category:
  - 3D-VL
  - 3D Affordance
tag:
  - 3D-VL
  - 3D Affordance
  - å·²å‘å¸ƒ
footer: æŠ€æœ¯å…±å»ºï¼ŒçŸ¥è¯†å…±äº«
date: 2025-06-15
cover: assets/cover/GREAT.png
author:
  - BinaryOracle
---

`GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding è®ºæ–‡è§£è¯»` 

<!-- more -->

> è®ºæ–‡: [https://arxiv.org/abs/2411.19626](https://arxiv.org/abs/2411.19626)
> ä»£ç : [https://github.com/yawen-shao/GREAT_code](https://github.com/yawen-shao/GREAT_code)
> æ•°æ®é›†: [https://drive.google.com/drive/folders/1n_L_mSmVpAM-1ASoW2T2MltYkaiA_X9X](https://drive.google.com/drive/folders/1n_L_mSmVpAM-1ASoW2T2MltYkaiA_X9X)


## æ‘˜è¦

GREATï¼ˆGeometry-Intention Collaborative Inferenceï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æŒ–æ˜ç‰©ä½“çš„ä¸å˜å‡ ä½•å±æ€§å’Œæ½œåœ¨äº¤äº’æ„å›¾ï¼Œä»¥å¼€æ”¾è¯æ±‡çš„æ–¹å¼å®šä½3Dç‰©ä½“çš„åŠŸèƒ½åŒºåŸŸï¼ˆaffordanceï¼‰ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè®¾è®¡äº†å¤šå¤´éƒ¨åŠŸèƒ½é“¾å¼æ€ç»´ï¼ˆMHACoTï¼‰ç­–ç•¥ï¼Œé€æ­¥åˆ†æäº¤äº’å›¾åƒä¸­çš„å‡ ä½•å±æ€§å’Œäº¤äº’æ„å›¾ï¼Œå¹¶é€šè¿‡è·¨æ¨¡æ€è‡ªé€‚åº”èåˆæ¨¡å—ï¼ˆCMAFMï¼‰å°†è¿™äº›çŸ¥è¯†ä¸ç‚¹äº‘å’Œå›¾åƒç‰¹å¾ç»“åˆï¼Œå®ç°ç²¾å‡†çš„3DåŠŸèƒ½å®šä½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ç›®å‰æœ€å¤§çš„3DåŠŸèƒ½æ•°æ®é›†PIADv2ï¼ŒåŒ…å«15Käº¤äº’å›¾åƒå’Œ38Kæ ‡æ³¨çš„3Dç‰©ä½“å®ä¾‹ã€‚å®éªŒè¯æ˜äº†GREATåœ¨å¼€æ”¾è¯æ±‡åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚

## ç®€ä»‹

Open-Vocabulary 3Då¯¹è±¡åŠŸèƒ½å®šä½ï¼ˆOVAGï¼‰æ—¨åœ¨é€šè¿‡ä»»æ„æŒ‡ä»¤å®šä½ç‰©ä½“ä¸Šæ”¯æŒç‰¹å®šäº¤äº’çš„â€œåŠ¨ä½œå¯èƒ½æ€§â€åŒºåŸŸï¼Œå¯¹æœºå™¨äººæ„ŸçŸ¥ä¸æ“ä½œè‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ï¼ˆå¦‚[IAGNet](https://arxiv.org/abs/2303.10437)ã€[LASO](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_LASO_Language-guided_Affordance_Segmentation_on_3D_Object_CVPR_2024_paper.pdf)ï¼‰é€šè¿‡ç»“åˆæè¿°äº¤äº’çš„å›¾åƒæˆ–è¯­è¨€ä¸3Då‡ ä½•ç»“æ„å¼•å…¥å¤–éƒ¨å…ˆéªŒï¼Œä½†å­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼ˆå¦‚å›¾1(b)æ‰€ç¤ºï¼‰ï¼š  

- **è¯­ä¹‰ç©ºé—´å—é™**ï¼šä¾èµ–é¢„å®šä¹‰ç±»åˆ«ï¼Œéš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„åŠŸèƒ½ï¼ˆå¦‚å°†â€œpourâ€é”™è¯¯åˆ†ç±»ä¸ºâ€œgraspâ€ï¼‰ã€‚  
               
- **å‡ ä½•ä¸æ„å›¾åˆ©ç”¨ä¸è¶³**ï¼šæœªå……åˆ†æŒ–æ˜ç‰©ä½“é—´å…±äº«çš„å‡ ä½•ä¸å˜æ€§ï¼ˆå¦‚æ‰‹æŸ„çš„æŠ“æ¡å±æ€§ï¼‰å’ŒåŒä¸€ç‰©ä½“çš„å¤šäº¤äº’æ„å›¾å…³è”ã€‚  

![](GREAT/1.png)

**äººç±»è®¤çŸ¥å¯å‘**:

ç ”ç©¶è¡¨æ˜ï¼ˆ[Gick & Holyoak, 1980](https://www.sciencedirect.com/science/article/abs/pii/0010028580900134)ï¼‰ï¼Œäººç±»é€šè¿‡å¤šæ­¥æ¨ç†å’Œç±»æ¯”æ€ç»´è§£å†³å¤æ‚ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œè§‚å¯Ÿå€’æ°´åœºæ™¯æ—¶ï¼ˆå›¾1(c)ï¼‰ï¼Œäººç±»ä¼šï¼š  

1. è¯†åˆ«äº¤äº’éƒ¨ä»¶ï¼ˆå£¶å˜´ï¼‰  

2. æå–å‡ ä½•å±æ€§ï¼ˆå€¾æ–œæ›²é¢ï¼‰  

3. æ¨ç†æ½œåœ¨æ„å›¾ï¼ˆå€’æ°´/æ³¨æ°´ï¼‰  

**æ–¹æ³•åˆ›æ–°**:  

GREATæ¡†æ¶é€šè¿‡ä»¥ä¸‹è®¾è®¡æ¨¡æ‹Ÿè¿™ä¸€è¿‡ç¨‹ï¼ˆå›¾1(d)ï¼‰ï¼š  

1. **MHACoTæ¨ç†é“¾**ï¼šåŸºäºå¾®è°ƒçš„MLLMï¼ˆå¦‚[InternVL](https://arxiv.org/abs/2404.16821)ï¼‰åˆ†æ­¥æ¨ç†ï¼š  

   - **Object-Head**ï¼šå®šä½äº¤äº’éƒ¨ä»¶å¹¶åˆ†æå‡ ä½•ç»“æ„ï¼ˆå¦‚â€œä¸ºä»€ä¹ˆå£¶å˜´é€‚åˆå€’æ°´â€ï¼‰  

   - **Affordance-Head**ï¼šæè¿°å®é™…äº¤äº’ï¼ˆå¦‚â€œæ¡æŸ„å€’æ°´â€ï¼‰å¹¶è”æƒ³æ½œåœ¨æ„å›¾ï¼ˆå¦‚â€œæ³¨æ°´/æ¸…æ´—â€ï¼‰  

2. **è·¨æ¨¡æ€èåˆ**ï¼šé€šè¿‡CMAFMæ¨¡å—å°†å‡ ä½•å±æ€§ï¼ˆ$\mathbf{\hat{T}}_o$ï¼‰ä¸äº¤äº’æ„å›¾ï¼ˆ$\mathbf{\hat{T}}_a$ï¼‰æ³¨å…¥ç‚¹äº‘ï¼ˆ$\mathbf{F}_{tp}$ï¼‰å’Œå›¾åƒç‰¹å¾ï¼ˆ$\mathbf{F}_{ti}$ï¼‰ï¼Œæœ€ç»ˆè§£ç ä¸º3DåŠŸèƒ½çƒ­å›¾ $\phi = \sigma(f_\phi(\mathbf{F}_\alpha))$ã€‚  

**æ•°æ®é›†è´¡çŒ®**:

æ‰©å±•æ„å»ºäº†**PIADv2**ï¼ˆå¯¹æ¯”è§è¡¨1ï¼‰ï¼š  

- **è§„æ¨¡**ï¼š15Käº¤äº’å›¾åƒï¼ˆÃ—3ï¼‰å’Œ38K 3Då®ä¾‹ï¼ˆÃ—5ï¼‰  

- **å¤šæ ·æ€§**ï¼š43ç±»ç‰©ä½“ã€24ç±»åŠŸèƒ½ï¼Œè¦†ç›–å¤šå¯¹å¤šå…³è”ï¼ˆå›¾3(c)ï¼‰  

![](GREAT/2.png)

![](GREAT/3.png)

## ç›¸å…³å·¥ä½œ

**1. Affordance Grounding**  

ç°æœ‰ç ”ç©¶ä¸»è¦ä»2Dæ•°æ®ï¼ˆå¦‚å›¾åƒã€è§†é¢‘ï¼‰å’Œè‡ªç„¶è¯­è¨€ç†è§£å‡ºå‘ï¼Œå®šä½â€œåŠ¨ä½œå¯èƒ½æ€§â€åŒºåŸŸã€‚ä¾‹å¦‚ï¼Œéƒ¨åˆ†å·¥ä½œé€šè¿‡è¯­è¨€ç†è§£åœ¨2Dæ•°æ®ä¸­å®šä½åŠŸèƒ½åŒºåŸŸï¼ˆ[3](https://arxiv.org/abs/2405.12461), [21](https://arxiv.org/abs/2311.17776)ï¼‰ï¼Œä½†æœºå™¨äººæ“ä½œéœ€è¦3Dä¿¡æ¯ï¼Œ2Dæ–¹æ³•éš¾ä»¥ç›´æ¥è¿ç§»ã€‚éšç€3Dæ•°æ®é›†ï¼ˆå¦‚[5](https://arxiv.org/abs/2212.08051), [6](https://arxiv.org/abs/2103.16397)ï¼‰çš„å‡ºç°ï¼Œéƒ¨åˆ†ç ”ç©¶å¼€å§‹æ˜ å°„è¯­ä¹‰åŠŸèƒ½åˆ°3Dç»“æ„ï¼Œä½†å—é™äºé¢„å®šä¹‰ç±»åˆ«ï¼Œæ— æ³•å¤„ç†å¼€æ”¾è¯æ±‡åœºæ™¯ã€‚  

**2. Open-Vocabulary 3D Affordance Grounding (OVAG)**  

OVAGæ—¨åœ¨é€šè¿‡é¢å¤–æŒ‡ä»¤ï¼ˆå¦‚æ–‡æœ¬æˆ–å›¾åƒï¼‰å¼•å…¥äº¤äº’å…ˆéªŒï¼Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼š  

- IAGNet  åˆ©ç”¨2Däº¤äº’è¯­ä¹‰æŒ‡å¯¼3DåŠŸèƒ½å®šä½ï¼›  

- LASO é€šè¿‡æ–‡æœ¬æ¡ä»¶æŸ¥è¯¢åˆ†å‰²åŠŸèƒ½åŒºåŸŸï¼›  

- OpenAD å’Œ OpenKD åˆ©ç”¨CLIPç¼–ç å™¨å®ç°æ–‡æœ¬-ç‚¹äº‘å…³è”ã€‚  

è¿™äº›æ–¹æ³•ä»å—é™äºè®­ç»ƒè¯­ä¹‰ç©ºé—´ï¼Œè€ŒGREATé€šè¿‡å‡ ä½•-æ„å›¾ååŒæ¨ç†ï¼ˆCoTï¼‰è§£å†³æ­¤é—®é¢˜ï¼ˆå¦‚è¡¨2æ‰€ç¤ºï¼‰ã€‚ 

![](GREAT/4.png)

**3. Chain-of-Thought (CoT) ä¸å¤šæ¨¡æ€å¤§æ¨¡å‹ (MLLMs)**  

CoTåŠå…¶å˜ä½“é€šè¿‡å¤šæ­¥æ¨ç†å¢å¼ºMLLMsèƒ½åŠ›ã€‚ä¾‹å¦‚ï¼š  

- è§†è§‰ä»»åŠ¡ä¸­ï¼ŒMLLMsï¼ˆå¦‚InternVLï¼‰ç»“åˆCoTåœ¨ç›®æ ‡æ£€æµ‹ã€æœºå™¨äººæ“ä½œç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼›  

- ä½†åŠ¨æ€åŠŸèƒ½ç‰¹æ€§ä½¿å¾—MLLMséš¾ä»¥ç›´æ¥ä»äº¤äº’å›¾åƒæ¨ç†3DåŠŸèƒ½ï¼ŒGREATé€šè¿‡å¾®è°ƒMLLMså¹¶è®¾è®¡MHACoTç­–ç•¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚  

**å…³é”®é—®é¢˜**ï¼ˆå¦‚å›¾1æ‰€ç¤ºï¼‰ï¼š  

- ç°æœ‰æ–¹æ³•ä¾èµ–æ•°æ®å¯¹é½ï¼Œæ³›åŒ–æ€§ä¸è¶³ï¼ˆå¦‚å°†â€œpourâ€è¯¯åˆ†ç±»ä¸ºâ€œgraspâ€ï¼‰ï¼›  

- GREATé€šè¿‡æ¨¡æ‹Ÿäººç±»å¤šæ­¥æ¨ç†ï¼ˆå‡ ä½•å±æ€§æå–+æ„å›¾ç±»æ¯”ï¼‰å®ç°å¼€æ”¾è¯æ±‡åŠŸèƒ½å®šä½ã€‚

## æ–¹æ³•

GREAT çš„è¾“å…¥ä¸º ${P, I}$ï¼Œå…¶ä¸­ $P \in \mathbb{R}^{N \times 4}$ æ˜¯ç‚¹äº‘ï¼ŒåŒ…å«ç‰©ä½“çš„åæ ‡ $P\_c \in \mathbb{R}^{N \times 3}$ å’Œå…¶å¯¹åº”çš„ 3D å¯ä¾›æ€§æ ‡æ³¨ $P\_{label} \in \mathbb{R}^{N \times 1}$ï¼Œ$I \in \mathbb{R}^{3 \times H \times W}$ ä¸ºå›¾åƒã€‚ç›®æ ‡æ˜¯ä¼˜åŒ–æ¨¡å‹ $f\_\theta$ï¼Œè¾“å‡º 3D ç‰©ä½“å¯ä¾›æ€§ $\varphi$ï¼Œå³ï¼š

$$
\varphi = f_\theta(P_c, I)
$$

å¦‚å›¾2æ‰€ç¤ºï¼Œé¦–å…ˆä½¿ç”¨ ResNet \[9] å’Œ PointNet++ \[43] æå–ç‰¹å¾ï¼Œåˆ†åˆ«å¾—åˆ° $F\_i \in \mathbb{R}^{C \times H\_1 \times W\_1}$ å’Œ $F\_p \in \mathbb{R}^{C \times N\_p}$ï¼Œéšåå°† $F\_i$ reshape ä¸º $F\_i \in \mathbb{R}^{C \times N\_i}$ï¼ˆå…¶ä¸­ $N\_i = H\_1 \times W\_1$ï¼‰ã€‚æ¥ç€é€šè¿‡å¤šå¤´å¯ä¾›æ€§é“¾å¼æ€ç»´ï¼ˆMHACoTï¼‰ç­–ç•¥å¯¹äº¤äº’å›¾åƒè¿›è¡Œæ¨ç†ï¼ŒæŒ–æ˜ä¸å˜å‡ ä½•å±æ€§ä¸æ½œåœ¨äº¤äº’æ„å›¾ã€‚

![](GREAT/5.png)

ç„¶åï¼Œä½¿ç”¨ Roberta \[28] ç¼–ç æ¨ç†ç»“æœï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—å¯¹è±¡å‡ ä½•ç‰¹å¾ $\bar{T}*o$ å’Œå¯ä¾›æ€§æ„å›¾ç‰¹å¾ $\bar{T}*a$ï¼ˆè§ Sec. 3.2ï¼‰ã€‚GREAT åˆ©ç”¨è·¨æ¨¡æ€è‡ªé€‚åº”èåˆæ¨¡å—ï¼ˆCMAFMï¼‰å°†è¿™äº›çŸ¥è¯†æ³¨å…¥ç‚¹äº‘ç‰¹å¾å¹¶ä¸å›¾åƒç‰¹å¾èåˆï¼Œå¾—åˆ°èåˆç‰¹å¾ $F*{tp}, F*{ti}$ï¼ˆè§ Sec. 3.3ï¼‰ã€‚æœ€åå°†è¿™ä¸¤ä¸ªç‰¹å¾é€å…¥è§£ç å™¨ä»¥è·å¾—å¯ä¾›æ€§è¾“å‡º $\varphi$ï¼Œå¹¶é€šè¿‡å¤åˆæŸå¤±ä¼˜åŒ–æ•´ä¸ªæµç¨‹ï¼ˆè§ Sec. 3.4ï¼‰ã€‚

---

### 3.2 Multi-Head Affordance Chain-of-Thought

#### Fine-Tuning MLLM

ä¸ºäº†è·å¾—å¯¹ç‰©ä½“å¯ä¾›æ€§æ›´æ·±å…¥çš„ç†è§£ï¼Œæˆ‘ä»¬å¯¹ InternVL \[4] ä½¿ç”¨å¯å­¦ä¹ çš„ Adapter \[10] è¿›è¡Œå¾®è°ƒï¼Œä»…æ›´æ–° Adapter æ¨¡å—ï¼ˆ10 ä¸ª epochï¼Œå­¦ä¹ ç‡ 4e-5ï¼ŒLoRA rank ä¸º 16ï¼‰ï¼Œå…¶ä½™å‚æ•°ä¿æŒå†»ç»“ï¼Œä»¥ä¿æŒåŸå§‹æ¨¡å‹è¯†åˆ«èƒ½åŠ›çš„åŒæ—¶å¢å¼ºå…¶æ¨ç†èƒ½åŠ›ã€‚

#### Object-Head Reasoningï¼ˆå‡ ä½•æ¨ç†ï¼‰

è¯¥éƒ¨åˆ†åŒ…å«ï¼š

* **ç‰©ä½“äº¤äº’æ„ŸçŸ¥ï¼ˆObject Interaction Perceptionï¼‰**ï¼šè¯†åˆ«å›¾åƒä¸­ç‰©ä½“ä¸äººå‘ç”Ÿäº¤äº’çš„éƒ¨åˆ†ã€‚Prompt ç¤ºä¾‹ä¸ºï¼šâ€œæŒ‡å‡ºå›¾åƒä¸­ç‰©ä½“ä¸äººäº¤äº’çš„éƒ¨åˆ†ã€‚â€

* **å‡ ä½•ç»“æ„æ¨ç†ï¼ˆGeometric Structure Reasoningï¼‰**ï¼šè¿›ä¸€æ­¥ä»å‡ ä½•ç»“æ„è§’åº¦æ¨ç†ä¸ºä»€ä¹ˆè¯¥éƒ¨ä½é€‚åˆäº¤äº’ã€‚Prompt ç¤ºä¾‹ä¸ºï¼šâ€œä»å‡ ä½•ç»“æ„è§£é‡Šè¯¥éƒ¨ä½å¯ä»¥äº¤äº’çš„åŸå› ã€‚â€

#### Affordance-Head Reasoningï¼ˆç±»æ¯”æ¨ç†ï¼‰

è¯¥éƒ¨åˆ†åŒ…å«ï¼š

* **äº¤äº’ç»†èŠ‚æè¿°ï¼ˆInteraction Detailed Descriptionï¼‰**ï¼šæè¿°å›¾åƒä¸­äººä¸ç‰©ä½“ä¹‹é—´çš„å®Œæ•´äº¤äº’è¿‡ç¨‹ï¼Œç”Ÿæˆç»†ç²’åº¦è¡¨ç¤ºã€‚Prompt ç¤ºä¾‹ä¸ºï¼šâ€œæè¿°å›¾åƒä¸­äººä¸ç‰©ä½“çš„äº¤äº’æ–¹å¼ã€‚â€

* **äº¤äº’ç±»æ¯”æ¨ç†ï¼ˆInteractive Analogical Reasoningï¼‰**ï¼šæ¨¡æ‹Ÿäººç±»å¯¹äº¤äº’æ–¹å¼çš„è”æƒ³ï¼ŒæŒ–æ˜å…¶ä»–å¯èƒ½äº¤äº’æ„å›¾ï¼Œå¢å¼ºç±»æ¯”èƒ½åŠ›ã€‚Prompt ç¤ºä¾‹ä¸ºï¼šâ€œåˆ—ä¸¾ä¸¤ä¸ªè¯¥ç‰©ä½“å¸¸è§çš„å…¶ä»–äº¤äº’æ–¹å¼ã€‚â€

#### Knowledge Encoding and Integration

ä» Object-Head å¾—åˆ°çš„å‡ ä½•å±æ€§æè¿°ä¸ Affordance-Head æ¨ç†çš„äº¤äº’æè¿°è¢« Roberta ç¼–ç ä¸ºä¸¤ä¸ªç‰¹å¾ï¼š

* $T\_o \in \mathbb{R}^{N\_o \times C}$ï¼šç‰©ä½“å‡ ä½•çŸ¥è¯†ç‰¹å¾
* $T\_a \in \mathbb{R}^{N\_a \times C}$ï¼šå¯ä¾›æ€§æ„å›¾çŸ¥è¯†ç‰¹å¾

é€šè¿‡äº¤å‰æ³¨æ„åŠ›å±‚ $f\_m$ ä¸è‡ªæ³¨æ„åŠ›å±‚ $f\_\delta$ å¯¹é½äºŒè€…ï¼Œå…¬å¼å¦‚ä¸‹ï¼š

$$
\bar{T}_o = f_\delta(f_m(T_o, T_a)), \quad \bar{T}_a = f_\delta(f_m(T_a, T_o))
$$

---

### 3.3 Cross-Modal Adaptive Fusion Module (CMAFM)

ä¸ºäº†å°†å‡ ä½•å±æ€§ä¸ç‚¹äº‘ç‰¹å¾æ›´å¥½åœ°å¯¹é½èåˆï¼ŒCMAFM å°† $\bar{T}\_o$ èåˆè‡³ PointNet++ æœ€æ·±å±‚ç‰¹å¾ï¼Œå¹¶ä¸å›¾åƒç‰¹å¾è”åˆç”¨äºé¢„æµ‹ã€‚

å…·ä½“åœ°ï¼Œå¯¹ç‚¹äº‘ç‰¹å¾ $F\_p$ å’ŒçŸ¥è¯†ç‰¹å¾ $\bar{T}\_o$ è¿›è¡Œçº¿æ€§æ˜ å°„å½¢æˆ Queryã€Keyã€Valueï¼š

* $Q = F\_p W\_1$
* $K = \bar{T}\_o W\_2$
* $V = \bar{T}\_o W\_3$

è·¨æ³¨æ„åŠ›èåˆå…¬å¼ä¸ºï¼š

$$
F_p' = \left( \text{softmax}\left( \frac{Q^\top \cdot K}{\sqrt{d}} \right) \cdot V^\top \right)^\top
$$

æœ€ç»ˆç‚¹äº‘èåˆç‰¹å¾è¡¨ç¤ºä¸ºï¼š

$$
P_o = f\left[ F_p' + f_\phi(F_p'), \Theta(\bar{T}_o' + f_\phi(\bar{T}_o')) \right]
$$

å…¶ä¸­ $f\_\phi$ ä¸ºå…¨è¿æ¥å±‚ï¼Œ$\Theta$ è¡¨ç¤ºæ± åŒ–åæ‰©å±•ä¸º $RC \times N\_p$ï¼Œ$f$ ä¸º $1 \times 1$ å·ç§¯ï¼Œè¾“å‡º $P\_o$ ä¸Šé‡‡æ ·è‡³åŸå§‹ç‚¹æ•°åè®°ä¸ºï¼š

$$
F_{tp} = \text{FP}(P_o)
$$

å›¾åƒç‰¹å¾ $F\_i$ ä¸æ„å›¾ç‰¹å¾ $\bar{T}\_a$ èåˆè¡¨ç¤ºä¸ºï¼š

$$
F_{ti} = f[\Gamma(\bar{T}_a), F_i], \quad F_{ti} \in \mathbb{R}^{C \times N_i}
$$

---

### 3.4 Decoder and Loss Functions

æœ€ç»ˆå°†èåˆåçš„å›¾åƒç‰¹å¾ $F\_{ti}$ å’Œç‚¹äº‘ç‰¹å¾ $F\_{tp}$ æ‹¼æ¥åé€å…¥è§£ç å™¨è¾“å‡ºå¯ä¾›æ€§é¢„æµ‹ï¼š

$$
F_\alpha = f[\Gamma(F_{ti}), F_{tp}], \quad \varphi = \sigma(f_\varphi(F_\alpha))
$$

å…¶ä¸­ $\sigma$ ä¸º sigmoid æ¿€æ´»ï¼Œ$f\_\varphi$ ä¸ºè¾“å‡ºå¤´ï¼Œ$F\_\alpha \in \mathbb{R}^{C \times N}$ï¼Œ$\varphi \in \mathbb{R}^{N \times 1}$ æ˜¯æœ€ç»ˆçš„ 3D å¯ä¾›æ€§é¢„æµ‹ã€‚

æŸå¤±å‡½æ•°ç”± focal loss \[26] ä¸ dice loss \[37] ç»„æˆï¼š

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{focal}} + \mathcal{L}_{\text{dice}}
$$

è¿™ç§è®¾è®¡æ— éœ€ä¾èµ–å…·ä½“çš„å¯ä¾›æ€§åˆ†ç±»æ ‡ç­¾ï¼Œè€Œæ˜¯é€šè¿‡ç›‘ç£ç‚¹çº§çƒ­å›¾ï¼Œå°† 3D å¯ä¾›æ€§ä¸äº¤äº’å›¾åƒç›´æ¥è”ç³»èµ·æ¥ã€‚

## æ•°æ®é›†

ä¸ºæ”¯æ’‘å¼€æ”¾è¯æ±‡ 3D ç‰©ä½“å¯ä¾›æ€§å®šä½ä»»åŠ¡ï¼Œæœ¬æ–‡æ„å»ºäº† **PIADv2ï¼ˆPoint Image Affordance Dataset v2ï¼‰**ï¼Œç”±æˆå¯¹çš„ 2D äº¤äº’å›¾åƒä¸ 3D ç‚¹äº‘å¯¹è±¡ç»„æˆï¼Œæ˜¯å½“å‰è§„æ¨¡æœ€å¤§çš„åŒç±»æ•°æ®é›†ã€‚

### æ•°æ®æ”¶é›†ï¼ˆCollectionï¼‰

* ç‚¹äº‘éƒ¨åˆ†ä¸»è¦æ¥è‡ªä»¥ä¸‹å¼€æºæ•°æ®æºï¼š

  * 3DIR \[57]
  * 3D-AffordanceNet \[6]
  * Objaverse \[5]

* å›¾åƒéƒ¨åˆ†ä¸»è¦æ¥æºäºï¼š

  * AGD20k \[32]
  * OpenImage \[18]
  * å…¶ä»–å¼€æºè®¸å¯ç½‘ç«™

**æ€»ä½“æ•°æ®ç»Ÿè®¡**ï¼š

* å›¾åƒæ•°ï¼š15,213
* ç‚¹äº‘æ•°ï¼š38,889
* è¦†ç›–ç±»åˆ«ï¼š

  * ç‰©ä½“ç±»åˆ«ï¼š43ç±»
  * å¯ä¾›æ€§ç±»åˆ«ï¼š24ç±»

è¯¥æ•°æ®é›†å¤§å¤§è¶…è¶Šäº†å‰ä½œ PIAD \[56]ï¼Œå…¶å›¾åƒæ•°é‡æ˜¯å‰è€…çš„ä¸‰å€ï¼Œç‚¹äº‘æ•°é‡æ˜¯å‰è€…çš„äº”å€ã€‚

> å¦‚å›¾3(a) æ‰€ç¤ºï¼Œçº¢è‰²åŒºåŸŸä¸ºç‚¹äº‘çš„å¯ä¾›æ€§æ ‡æ³¨ã€‚å›¾3(b) å±•ç¤ºäº†å„ç±»åˆ«çš„åˆ†å¸ƒæƒ…å†µï¼Œæ˜¾ç¤ºå‡ºæ•°æ®é›†å¯¹äº¤äº’å¤šæ ·æ€§å’Œç±»åˆ«å¤šæ ·æ€§çš„å…¨é¢è¦†ç›–ã€‚

---

### æ ‡æ³¨ç­–ç•¥ï¼ˆAnnotationï¼‰

å¯¹äºç‚¹äº‘å®ä¾‹ï¼š

* æ¯ä¸ªç‚¹äº‘å®ä¾‹æŒ‰å¯ä¾›æ€§ç±»åˆ«æ ‡æ³¨
* æ¯ä¸ªæ ·æœ¬ä¸ºä¸€ä¸ª $2048 \times4$ çš„çŸ©é˜µï¼Œå«ï¼š

  * 2048 ä¸ªç‚¹
  * æ¯ä¸ªç‚¹åŒ…æ‹¬ $(x, y, z)$ åæ ‡ä¸çƒ­åŠ›å›¾å½¢å¼çš„å¯ä¾›æ€§å€¼

å¯¹äºå›¾åƒï¼š

* å›¾åƒæŒ‰å¯ä¾›æ€§ç±»åˆ«è¿›è¡Œåˆ†ç±»ï¼Œä»¥æ”¯æŒè®­ç»ƒé˜¶æ®µçš„åŒ¹é…ä¸æ¨ç†

---

### ç»Ÿè®¡åˆ†æï¼ˆStatistical Analysisï¼‰

* **å›¾åƒä¸ç‚¹äº‘ä¹‹é—´ä¸éœ€è¦ä¸€ä¸€å¯¹åº”**ï¼ŒäºŒè€…åˆ†åˆ«ä»ä¸åŒå®ä¾‹ä¸­é‡‡æ ·ï¼Œä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›
* **å¤šå¯¹å¤šå…³ç³»åˆ†æ**ï¼šå¦‚å›¾3(c) æ‰€ç¤ºï¼Œaffordance ä¸ object ç±»åˆ«é—´å­˜åœ¨æ˜æ˜¾çš„å¤šå¯¹å¤šå…³ç³»ï¼ŒæŒ‘æˆ˜æ¨¡å‹å¯¹å¯ä¾›æ€§çš„æ³›åŒ–èƒ½åŠ›
* **ç±»å¹³è¡¡åˆ†æ**ï¼šå›¾3(d) å±•ç¤ºäº†å„ object ç±»åˆ«ä¸‹å›¾åƒä¸ç‚¹äº‘çš„æ•°é‡æ¯”ä¾‹ï¼Œä½“ç°å‡ºæ•°æ®é›†åœ¨æ ·æœ¬åˆ†å¸ƒä¸Šçš„å…¨é¢æ€§å’Œå‡è¡¡æ€§

---

### æ•°æ®åˆ’åˆ†ï¼ˆData Partitionsï¼‰

PIADv2 æä¾›ä¸‰ç§æ ‡å‡†åˆ’åˆ†æ–¹å¼ï¼ˆå‰ä¸¤ç§ä¸ PIAD \[56] ä¿æŒä¸€è‡´ï¼‰ï¼š

**Seen**ï¼š

* è®­ç»ƒé›†ä¸æµ‹è¯•é›†ä¸­çš„ç‰©ä½“ä¸å¯ä¾›æ€§ç±»åˆ«ç›¸åŒ

**Unseen Object**ï¼š

* æµ‹è¯•é›†ä¸­åŒ…å«è®­ç»ƒé›†ä¸­æœªå‡ºç°çš„ç‰©ä½“ç±»åˆ«ï¼Œä½†å¯ä¾›æ€§ç±»åˆ«ç›¸åŒ

**Unseen Affordance**ï¼š

* æµ‹è¯•é›†ä¸­çš„å¯ä¾›æ€§ç±»åˆ«æœªåœ¨è®­ç»ƒé›†ä¸­å‡ºç°ï¼ŒåŒæ—¶åŒ…å«éƒ¨åˆ†æ–°ç‰©ä½“ç±»åˆ«

## å®éªŒ

ä¸ºéªŒè¯æ‰€ææ–¹æ³• GREAT çš„æœ‰æ•ˆæ€§ä¸æ³›åŒ–èƒ½åŠ›ï¼Œä½œè€…åœ¨æå‡ºçš„ **PIADv2** æ•°æ®é›†ä¸Šå¼€å±•äº†ç³»ç»Ÿæ€§çš„å®éªŒè¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸å¤šä¸ªå…ˆè¿›æ–¹æ³•çš„å¯¹æ¯”ä»¥åŠæ¶ˆèå®éªŒå’Œå¯è§†åŒ–åˆ†æã€‚

### 5.1 Benchmark Setting

**è¯„ä¼°æŒ‡æ ‡**ï¼š

å®éªŒé‡‡ç”¨ä»¥ä¸‹è¯„ä¼°æŒ‡æ ‡è¯„ä¼° 3D å¯ä¾›æ€§é¢„æµ‹è´¨é‡ï¼ˆå‚è€ƒ \[25, 56]ï¼‰ï¼š

* AUCï¼ˆArea Under Curveï¼‰\[29]
* aIOUï¼ˆaverage Intersection over Unionï¼‰\[45]
* SIMï¼ˆSimilarityï¼‰\[47]
* MAEï¼ˆMean Absolute Errorï¼‰\[52]

**å¯¹æ¯”æ–¹æ³•**ï¼š

* **IAG** ([2023](https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Grounding_3D_Object_Affordance_From_2D_Interactions_in_Images_ICCV_2023_paper.html))ï¼š2D-å¼•å¯¼çš„3Då¯ä¾›æ€§æ–¹æ³•
* **LASO** ([2024](https://openaccess.thecvf.com/content/CVPR2024/html/Li_LASO_Language-Guided_Affordance_Segmentation_on_3D_Object_CVPR_2024_paper.html))ï¼šåŸºäºè¯­è¨€å¼•å¯¼çš„3Då¯ä¾›æ€§åˆ†å‰²
* **FRCNN** \[54]ï¼šLiDAR-å›¾åƒèåˆä¸¤é˜¶æ®µ3Dæ£€æµ‹æ¡†æ¶
* **XMF** \[1]ï¼šå›¾åƒ-ç‚¹äº‘çš„è·¨æ¨¡æ€ç‚¹äº‘å½¢çŠ¶è¡¥å…¨æ–¹æ³•
* **Baseline**ï¼šç›´æ¥æ‹¼æ¥å›¾åƒä¸ç‚¹äº‘ç‰¹å¾ä½œä¸ºè¾“å…¥

**å®ç°ç»†èŠ‚**ï¼š

* 3D backboneï¼šPointNet++ \[43]
* 2D backboneï¼šResNet18 \[9]
* ä¼˜åŒ–å™¨ï¼šAdam
* å­¦ä¹ ç‡ï¼š1e-4
* æ‰¹å¤§å°ï¼š16
* æ€»è®­ç»ƒè½®æ¬¡ï¼š65

---

### 5.2 Comparison Results

å¦‚è¡¨2æ‰€ç¤ºï¼ŒGREAT åœ¨æ‰€æœ‰åˆ’åˆ†ï¼ˆSeenã€Unseen Objectã€Unseen Affordanceï¼‰ä¸‹å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾æˆæœ€æ–°æœ€ä¼˜æ€§èƒ½ã€‚

![](GREAT/8.png)

**é‡åŒ–åˆ†æ**ï¼š

åœ¨ **Unseen Affordance** è¿™ä¸€æœ€å…·æŒ‘æˆ˜æ€§çš„è®¾ç½®ä¸‹ï¼ŒGREAT ä¾æ—§è¡¨ç°å‡ºè‰²ï¼š

* AUCï¼š69.81ï¼ˆé«˜å‡º LASO çº¦ 9%ï¼‰
* aIOUï¼š12.05ï¼ˆé«˜å‡º IAG çº¦ 34%ï¼‰
* SIMï¼š0.290ï¼ˆå¤§å¹…è¶…è¶Šæ‰€æœ‰åŸºçº¿ï¼‰
* MAEï¼š0.127ï¼ˆæœ€å°ï¼‰

**å¯è§†åŒ–åˆ†æ**ï¼ˆå¦‚å›¾4æ‰€ç¤ºï¼‰ï¼š

* **Seen setting**ï¼šå„æ–¹æ³•å·®åˆ«ä¸å¤§
* **Unseen setting**ï¼š

  * å…¶ä»–æ–¹æ³•å€¾å‘äºé”™è¯¯åœ°é¢„æµ‹ä¸ºè®­ç»ƒé›†ä¸­é¢‘ç¹å‡ºç°çš„ affordanceï¼ˆå¦‚ graspï¼‰
  * GREAT èƒ½æ­£ç¡®æ•æ‰å¦‚ "pour" è¿™ç±» unseen affordanceï¼Œå®šä½ç²¾åº¦æ˜¾è‘—æ›´é«˜

![](GREAT/9.png)

---

### 5.3 Ablation Study

è¡¨3 å±•ç¤ºäº†å¯¹å…³é”®æ¨¡å—çš„æ¶ˆèå®éªŒç»“æœï¼š

![](GREAT/10.png)

**æ¶ˆèé¡¹åˆ†æ**ï¼š

* **âœ— AffCoTï¼ˆæ— æ„å›¾æ¨ç†ï¼‰**ï¼š

  * unseen affordance çš„ aIOU ä¸‹é™äº† 1.12ï¼Œè¡¨æ˜äº¤äº’æ„å›¾æ¨ç†å¯¹æ³›åŒ–è‡³æ–° affordance æä¸ºé‡è¦
* **âœ— ObjCoTï¼ˆæ— å‡ ä½•æ¨ç†ï¼‰**ï¼š

  * æ¨¡å‹å¯¹ç‰©ä½“å…³é”®äº¤äº’åŒºåŸŸçš„è¯†åˆ«èƒ½åŠ›ä¸‹é™
* **âœ— CMAFMï¼ˆæ— è·¨æ¨¡æ€èåˆï¼‰**ï¼š

  * å‡ ä½•ä¿¡æ¯æ— æ³•æœ‰æ•ˆæ³¨å…¥ç‚¹äº‘ï¼Œå¯¼è‡´å„é¡¹æŒ‡æ ‡å¤§å¹…ä¸‹é™ï¼ˆaIOU ä» 38.03 é™åˆ° 29.48ï¼‰
* **âœ— FTï¼ˆæ—  MLLM å¾®è°ƒï¼‰**ï¼š

  * æ¨ç†èƒ½åŠ›å—é™ï¼Œæ³›åŒ–æ€§ä¸‹é™æ˜æ˜¾

**å¯è§†åŒ–æ”¯æŒ**ï¼ˆè§å›¾5ï¼‰ï¼š

* **(a)**ï¼šè‹¥ç¼ºå¤± AffCoTï¼Œæ¨¡å‹æ— æ³•è¿›è¡Œç±»æ¯”æ¨ç†ï¼Œé¢„æµ‹å€¾å‘è®­ç»ƒé›†ä¸­å·²æœ‰çš„ affordance
* **(b)**ï¼šç¼ºå¤± ObjCoT æ—¶ï¼Œæ¨¡å‹æ— æ³•ç²¾ç¡®èšç„¦äºå…³é”®äº¤äº’éƒ¨ä½ï¼ˆå¦‚ kettle çš„ spoutï¼‰

![](GREAT/11.png)

---

### 5.4 Performance Analysis

ä¸ºè¿›ä¸€æ­¥è¯„ä¼°æ¨¡å‹çš„ç†è§£ä¸æ³›åŒ–èƒ½åŠ›ï¼Œä½œè€…è®¾è®¡äº†å¤šä¸ªåˆ†æå®éªŒã€‚

**å¤šä¸ªç‰©ä½“åœºæ™¯ï¼ˆMultiple Objectsï¼‰**ï¼š

* åœ¨åŒä¸€å¼ äº¤äº’å›¾åƒä¸­å­˜åœ¨å¤šä¸ªç‰©ä½“æ—¶ï¼Œæ¨¡å‹èƒ½å‡†ç¡®å¯¹æ¯ä¸ªå¯¹è±¡ç”Ÿæˆç‹¬ç«‹çš„ affordance åŒºåŸŸï¼ˆè§å›¾6ï¼‰

![](GREAT/12.png)

**å¤šç§å¯ä¾›æ€§ï¼ˆMultiple Affordancesï¼‰**ï¼š

* åŒä¸€ç‰©ä½“åœ¨ä¸åŒäº¤äº’å›¾åƒä¸­è¢«æ¨ç†å‡ºä¸åŒçš„ 3D affordance åŒºåŸŸï¼Œä½“ç°å‡ºæ¨¡å‹å¯¹è¯­ä¹‰çš„çµæ´»è§£æèƒ½åŠ›ï¼ˆè§å›¾7ï¼‰

![](GREAT/13.png)

**å¤šå®ä¾‹é²æ£’æ€§ï¼ˆMultiple Instancesï¼‰**ï¼š

* åœ¨å‡ ä½•å½¢çŠ¶å˜åŒ–æ˜¾è‘—çš„åŒç±»ç‰©ä½“ä¸­ï¼Œæ¨¡å‹ä¾ç„¶èƒ½ç¨³å®šé¢„æµ‹åˆç†çš„äº¤äº’åŒºåŸŸï¼ˆè§å›¾8ï¼‰ï¼Œè¯´æ˜å…¶å…·å¤‡è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ä¸é²æ£’æ€§

![](GREAT/14.png)

## ç»“è®º

æˆ‘ä»¬æå‡ºäº†ä¸€ç§**å¼€æ”¾è¯æ±‡å½¢å¼çš„ 3D ç‰©ä½“å¯ä¾›æ€§å®šä½æ–¹æ³•**ï¼Œè¯¥æ–¹æ³•ä»äº¤äº’å›¾åƒä¸­è¿›è¡Œæ¨ç†ï¼Œèƒ½å¤Ÿçªç ´é¢„å®šä¹‰æ ·æœ¬ç©ºé—´çš„é™åˆ¶ï¼Œå¹¶æ¨å¹¿è‡³æœªè§åœºæ™¯ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ â€”â€” é€šè¿‡å¤šå¤´å¯ä¾›æ€§é“¾å¼æ€ç»´ï¼ˆMulti-Head Affordance Chain-of-Thoughtï¼‰æ¨ç†ï¼ŒæŒ–æ˜ç‰©ä½“çš„**ä¸å˜å‡ ä½•å±æ€§**ï¼Œå¹¶å¯¹æ½œåœ¨äº¤äº’æ–¹å¼è¿›è¡Œç±»æ¯”æ¨ç†ï¼ŒåŒæ—¶ç»“åˆè·¨æ¨¡æ€ç‰¹å¾å¯¹é½ï¼Œå®ç°å¯¹ 3D å¯ä¾›æ€§åŒºåŸŸçš„ç²¾å‡†å®šä½ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç›®å‰æœ€å¤§è§„æ¨¡çš„ 3D å¯ä¾›æ€§æ•°æ®é›† **PIADv2**ï¼Œæ¶µç›– 1.5 ä¸‡å¼ äº¤äº’å›¾åƒä¸è¶…è¿‡ 3.8 ä¸‡ä¸ªæ ‡æ³¨å®Œæ•´çš„ 3D ç‰©ä½“ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æå‡ºçš„ GREAT æ¡†æ¶åœ¨å¤šé¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿåœ¨å¼€æ”¾åœºæ™¯ä¸‹æ”¯æŒå¯ä¾›æ€§ç†è§£ï¼Œæœ‰æœ›æå‡æœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„è‡ªä¸»äº¤äº’èƒ½åŠ›ã€‚æˆ‘ä»¬ç›¸ä¿¡è¯¥ç ”ç©¶å°†ä¸ºè§†è§‰å¯ä¾›æ€§ç†è§£é¢†åŸŸå¸¦æ¥æ–°çš„å¯å‘å¹¶æ¨åŠ¨å…¶å‘å±•ã€‚

**å±€é™æ€§ä¸æœªæ¥å·¥ä½œ**ï¼š
GREAT çš„ä¸»è¦å±€é™åœ¨äºå…¶å¤šæ­¥æ¨ç†æœºåˆ¶å¸¦æ¥äº†è¾ƒé«˜çš„è®¡ç®—å¤æ‚åº¦ï¼Œåœ¨å¤§è§„æ¨¡æˆ–å®æ—¶åº”ç”¨ä¸­å¯èƒ½æˆä¸ºç“¶é¢ˆã€‚æœªæ¥ï¼Œæˆ‘ä»¬è®¡åˆ’æ„å»º**ä¸“ç”¨äºæ¨ç†çš„æ•°æ®é›†**ï¼Œå¹¶åˆ©ç”¨è¿™äº›æ•°æ®é›†å¯¹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œä½¿å…¶ä¸“æ³¨äºç‰¹å®šé¢†åŸŸï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´å¿«ã€æ›´é«˜æ•ˆçš„æ€§èƒ½ã€‚

## ä»£ç 

### Multi-Head Affordance Chain-of-Thought

MHACoTæ˜¯ä¸€ç§**ç±»äººæ¨ç†æ–¹å¼**ï¼Œåˆ†å¤šä¸ªæ­¥éª¤ï¼Œæ¨¡æ‹Ÿäººè§‚å¯Ÿäº¤äº’å›¾åƒæ—¶çš„æ€ç»´é“¾æ¡ï¼š

1. **è¯†åˆ«äº¤äº’éƒ¨ä½**ï¼ˆObject Interaction Perceptionï¼‰

2. **è§£æå‡ ä½•å±æ€§**ï¼ˆGeometric Structure Reasoningï¼‰

3. **è¯¦ç»†æè¿°äº¤äº’**ï¼ˆInteraction Detailed Descriptionï¼‰

4. **ç±»æ¯”é¢å¤–äº¤äº’**ï¼ˆInteractive Analogical Reasoningï¼‰

æ¯ä¸ªå­æ­¥éª¤éƒ½ç”±ä¸€ä¸ª prompt å¼•å¯¼ MLLMï¼ˆå¦‚ InternVLï¼‰åšå›ç­”ï¼Œä»è€Œè·å¾—ï¼š

* å¯¹è±¡çš„äº¤äº’åŒºåŸŸ


> **Object Interaction Perception**
> Prompt 1: Point out which part of the object in the image interacts with the person.

ğŸ”¹ç›®æ ‡ï¼šå®šä½äº¤äº’å‘ç”Ÿçš„å¯¹è±¡åŒºåŸŸï¼ˆå¦‚â€œæ°´å£¶çš„å£¶å˜´â€ï¼‰

---
* å¯¹åº”çš„å‡ ä½•å±æ€§

> **Geometric Structure Reasoning**
> Prompt 2: Explain why this part can interact from the geometric structure of the object.

ğŸ”¹ç›®æ ‡ï¼šæ¨ç†å‡ ä½•å½¢æ€æ”¯æŒè¯¥äº¤äº’ï¼ˆå¦‚â€œå£¶å˜´ä¸Šå¼€å£ç‹­çª„ã€å¸¦æ›²çº¿â€ï¼‰

---
* å½“å‰äº¤äº’è¡Œä¸º

> **Interaction Detailed Description**
> Prompt 3: Describe the interaction between object and the person.

ğŸ”¹ç›®æ ‡ï¼šç»†è‡´åœ°è¯†åˆ«äº¤äº’åŠ¨ä½œåŠå…¶å‚ä¸éƒ¨ä½ï¼ˆå¦‚â€œç”¨æ‰‹æ¡ä½å£¶æŠŠå€’æ°´â€ï¼‰

---
* æ½œåœ¨äº¤äº’æ„å›¾

> **Interactive Analogical Reasoning**
> Prompt 4: List two interactions that describe additional common interactions that the object can interact with people.

ğŸ”¹ç›®æ ‡ï¼šæ¨ç†é™¤äº†å½“å‰äº¤äº’ä»¥å¤–ï¼Œè¯¥ç‰©ä½“å¸¸è§çš„å…¶ä»–äº¤äº’ï¼ˆå¦‚â€œå¼€å£¶ç›–ã€æŠ“æ¡ä¸­éƒ¨â€ï¼‰


æ ¸å¿ƒä»£ç å®ç°å¦‚ä¸‹:

```python
# 1. åŠ è½½é¢„è®­ç»ƒå¤šæ¨¡æ€å¤§æ¨¡å‹
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    #load_in_8bit=True,
    low_cpu_mem_usage=True,
    trust_remote_code=True,
    device_map=device_map).eval()
tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)

# 2. åŠ è½½å›¾åƒæ•°æ®
image_path = 'PATH/Data/Kettle/Internet/pour/kettle_pour_1.jpg'
pixel_values = load_image(image_path, max_num=12).to(torch.bfloat16).cuda()
object = image_path.split('/')[-4] # å›¾åƒæ‰€å±çš„ç‰©ä½“å

# 3. å®šä½äº¤äº’éƒ¨ä½
question1 = f'Point out which part of the {object} in the image interacts with the person. If this part is different from the part of the {object} shown in the image that performs the main function, point out the part of the {object} that performs the main function shown in the image.'
response1, history = model.chat(tokenizer, pixel_values, question1, generation_config, history=None, return_history=True)
print(f'{response1}')

# 4. æ¨ç†å‡ ä½•ç»“æ„
question2 = f'Explain why this part can interact from the geometric structure of the {object}. Just give the final result in one sentence.'
response2, history = model.chat(tokenizer, pixel_values, question2, generation_config, history=history, return_history=True)
print(f'{response2}')

# 5. è¯¦ç»†äº¤äº’è¡Œä¸º
question3 = f'Describe the interaction between {object} and the person in the image, including the interaction type, the interaction part of the {object}, and the interaction part of the person.'
response3, history= model.chat(tokenizer, pixel_values, question3, generation_config, history=history, return_history=True)
print(f'{response3}')

# 6. æ¨æµ‹å…¶ä»–äº¤äº’
question4 = f'List two interactions that describe additional common interactions that the {object} can interact with people, including the interaction type, the interaction part of the {object}, and the interaction part of the person.'
response4, history= model.chat(tokenizer, pixel_values, question4, generation_config, history=history, return_history=True)
print(f'{response4}')

'''
Sample output
1. the spout of kettle.
2. a narrow opening, a slight curve and the spout's position at the top of the kettle.
3. pour the liquid from the spout of the kettle using peopleâ€™s hand
4. grasp the kettle using person's hand around middle body, open the kettle using people's fingers on the lid

object knowledge: the spout of kettle: a narrow opening, a slight curve and the spout's position at the top of the kettle.
affordance/human knowledge: pour the liquid from the spout of the kettle using peopleâ€™s hand, grasp the kettle using person's hand around handle, open the kettle using people's fingers on the lid
''
```
å…¶ä¸­:

- å‡ ä½•ç»“æ„çŸ¥è¯† = Prompt 1 + Prompt 2 çš„å›ç­” = äº¤äº’éƒ¨ä½ + è¯¥éƒ¨ä½çš„å‡ ä½•å±æ€§æ¨ç†

- äº¤äº’çŸ¥è¯† = Prompt 3 + Prompt 4 çš„å›ç­” = å½“å‰äº¤äº’ + ç±»æ¯”/è¡¥å……çš„äº¤äº’æ–¹å¼ 

> MHACoT è¿™ä¸ªè¿‡ç¨‹å‘ç”Ÿåœ¨æ•°æ®é›†å‡†å¤‡é˜¶æ®µã€‚

### æ•°æ®é›†

> å…ˆäº†è§£ä¸€ä¸‹GREATé¡¹ç›®å¯¹åº”çš„æ•°æ®é›†ç›®å½•ç»“æ„:
>
> ![](GREAT/7.png)

æ•°æ®é›†çš„åˆå§‹åŒ–:

```python
class PIAD(Dataset):
    def __init__(self, run_type, setting_type, point_path, img_path, text_hk_path, text_ok_path, pair=2, img_size=(224, 224)):
        super().__init__()

        self.run_type = run_type # å½“å‰æ˜¯è®­ç»ƒ/æµ‹è¯•/éªŒè¯ç¯å¢ƒ
        self.p_path = point_path # ç‚¹äº‘ç´¢å¼•æ–‡ä»¶è·¯å¾„
        self.i_path = img_path  # å›¾ç‰‡ç´¢å¼•æ–‡ä»¶è·¯å¾„
        self.text_hk_path = text_hk_path # ç‰©ä½“å‡ ä½•ç»“æ„æ–‡æœ¬æ•°æ®æ–‡ä»¶è·¯å¾„
        self.text_ok_path = text_ok_path # äººç±»äº¤äº’æ–‡æœ¬æ•°æ®æ–‡ä»¶è·¯å¾„
        self.pair_num = pair  # æ§åˆ¶æ¯ä¸ª å›¾åƒæ ·æœ¬ å¯¹åº”å¤šå°‘ä¸ª 3Dç‚¹äº‘æ ·æœ¬
        self.affordance_label_list = ['grasp', 'contain', 'lift', 'open', 
                        'lay', 'sit', 'support', 'wrapgrasp', 'pour', 'move', 'display',
                        'push', 'listen', 'wear', 'press', 'cut', 'stab', 'carry', 'ride',
                        'clean', 'play', 'beat', 'speak', 'pull']  # 24
        
        ...

        '''
        Seen
        '''  # 43

        if setting_type == 'Seen':
            number_dict = {'Bag': 0, 'Microphone': 0, 'Toothbrush': 0, 'TrashCan': 0, 'Bicycle': 0,
                           'Guitar': 0, 'Glasses': 0, 'Hat': 0, 'Microwave':0, 'Backpack': 0, 'Door':0, 'Scissors': 0, 'Bowl': 0,
                           'Baseballbat': 0, 'Mop': 0, 'Dishwasher': 0, 'Bed': 0, 'Keyboard': 0, 'Clock': 0, 'Vase': 0, 'Knife': 0,
                           'Suitcase': 0, 'Hammer': 0, 'Refrigerator': 0, 'Chair': 0, 'Umbrella': 0, 'Bucket': 0,
                           'Display': 0, 'Earphone': 0, 'Motorcycle': 0, 'StorageFurniture': 0, 'Fork': 0, 'Broom': 0, 'Skateboard': 0,
                           'Tennisracket': 0, 'Laptop': 0, 'Table':0, 'Bottle': 0, 'Faucet': 0, 'Kettle': 0, 'Surfboard': 0, 'Mug': 0,
                            'Spoon': 0 
                           }  
        
        # è¯»å–æ‰€æœ‰å›¾ç‰‡è·¯å¾„ï¼Œæ‰€æœ‰äººç±»äº¤äº’æ–‡æœ¬æ•°æ®ï¼Œæ‰€æœ‰ç‰©ä½“å‡ ä½•ç»“æ„æ–‡æœ¬æ•°æ®
        self.img_files = self.read_file(self.i_path)
        self.text_human_files = self.read_file(self.text_hk_path)
        self.text_object_files = self.read_file(self.text_ok_path)
        self.img_size = img_size

        if self.run_type == 'train':
            # è¯»å–æ‰€æœ‰ç‚¹äº‘è·¯å¾„ï¼ŒåŒæ—¶è®°å½•æ¯ç±»ç‰©ä½“å¯¹åº”çš„æ ·æœ¬æ€»é‡ï¼Œæ¯”å¦‚: æ¤…å­å¯¹åº”çš„ç‚¹äº‘ä¸€å…±1000ä¸ª
            self.point_files, self.number_dict = self.read_file(self.p_path, number_dict)
            self.object_list = list(number_dict.keys()) # æ³¨æ„: Dict æŒ‰ç…§keyçš„æ’å…¥é¡ºåºè¿”å›çš„
            self.object_train_split = {}
            start_index = 0
            # è®°å½•æ¯ä¸ªç‰©ä½“å¯¹åº”çš„ç‚¹äº‘ç´¢å¼•ä¸‹æ ‡åŒºé—´
            for obj_ in self.object_list:
                temp_split = [start_index, start_index + self.number_dict[obj_]]
                self.object_train_split[obj_] = temp_split
                start_index += self.number_dict[obj_]
        else:
            self.point_files = self.read_file(self.p_path)
```
**ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦pair_numå‚æ•°?**

- é—®é¢˜èƒŒæ™¯ï¼šGREAT éœ€è¦å°† 2D äº¤äº’å›¾åƒï¼ˆImageï¼‰ä¸ 3D ç‚¹äº‘ï¼ˆPoint Cloudï¼‰çš„ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œä½†åŒä¸€ç‰©ä½“çš„ä¸åŒå®ä¾‹å¯èƒ½æœ‰å‡ ä½•å·®å¼‚ï¼ˆä¾‹å¦‚ä¸åŒå½¢çŠ¶çš„æ¤…å­ï¼‰ã€‚

- è§£å†³æ–¹æ¡ˆï¼šé€šè¿‡ä¸ºæ¯å¼ å›¾åƒé…å¯¹å¤šä¸ªç‚¹äº‘ï¼ˆpair_num > 1ï¼‰ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä» å¤šæ ·åŒ–çš„å‡ ä½•å˜ä½“ ä¸­æå–å…±æ€§çš„å‡ ä½•å±æ€§ï¼ˆå¦‚â€œå¯æŠ“æ¡â€çš„å…±äº«ç»“æ„ç‰¹å¾ï¼‰ï¼Œè€Œä¸ä»…ä»…ä¾èµ–å•ä¸€å®ä¾‹ã€‚

- ä»£ç ä½“ç°ï¼šåœ¨ __getitem__ ä¸­ï¼Œè®­ç»ƒæ—¶ä¼šå¯¹æ¯ä¸ªå›¾åƒéšæœºé‡‡æ · pair_num ä¸ªåŒç±»åˆ«ç‚¹äº‘ï¼ˆè§ point_sample_idx çš„ç”Ÿæˆé€»è¾‘ï¼‰

> GREAT é¡¹ç›®çš„æ•°æ®ç»„ç»‡ä¸­ï¼Œå°†æ¯ä¸ªæ ·æœ¬å±äºçš„ç‰©ä½“ç±»å‹ï¼Œå¾…é¢„æµ‹åŠŸèƒ½åŒºåŸŸç±»å‹å…¨éƒ¨éšå«åœ¨äº†æ ·æœ¬å¯¹åº”çš„æ–‡ä»¶è·¯å¾„ä¸­:
>
> ![](GREAT/6.png)

è·å–æ•°æ®:

```python
    def __getitem__(self, index):
        # 1. è·å–å›¾ç‰‡ï¼Œäººç±»äº¤äº’æ–‡æœ¬ï¼Œç‰©ä½“å‡ ä½•ç»“æ„æ–‡æœ¬
        img_path = self.img_files[index]
        text_hd = self.text_human_files[index]
        text_od = self.text_object_files[index]
       
        # 2.1 è¯„ä¼°æ—¶éœ€è¦æ ‡å‡†çš„å•ä¸€æ ·æœ¬å¯¹æ¯”
        if (self.run_type=='val'):
            point_path = self.point_files[index]
        else:
        # 2.2 ä»å›¾ç‰‡è·¯å¾„ä¸­æˆªå–å¾—åˆ°ç‰©ä½“åï¼Œäº¤äº’è¡Œä¸ºåï¼Œç‚¹äº‘ç´¢å¼•ä¸‹æ ‡åŒºé—´  
            object_name = img_path.split('/')[-4]
            affordance_name = img_path.split('/')[-2]
            range_ = self.object_train_split[object_name]
            # ä»ç´¢å¼•åŒºé—´ä¸­éšæœºé‡‡æ ·pair_numä¸ªç‚¹äº‘æ ·æœ¬
            point_sample_idx = random.sample(range(range_[0],range_[1]), self.pair_num)
      
            # 3. åŠ è½½ç‚¹äº‘æ ·æœ¬ï¼ŒåŒæ—¶åˆ¤æ–­æ˜¯å¦ä¸å½“å‰å›¾ç‰‡äº¤äº’è¡Œä¸ºä¸€è‡´ï¼Œä¸ä¸€è‡´åˆ™é‡æ–°éšæœºé€‰
            for i ,idx in enumerate(point_sample_idx):
                while True:
                    point_path = self.point_files[idx]
                    sele_affordance = point_path.split('/')[-2]
                    if sele_affordance == affordance_name:
                        point_sample_idx[i] = idx 
                        break
                    else:
                        idx = random.randint(range_[0],range_[1]-1)  # re-select idx
         
        Img = Image.open(img_path).convert('RGB')
        
        if(self.run_type == 'train'):
            Img = Img.resize(self.img_size)
            Img = img_normalize_train(Img)
            
            # 4. åŠ è½½åˆ—è¡¨ä¸­æ‰€æœ‰ç‚¹äº‘æ ·æœ¬
            Points_List = []
            affordance_label_List = []
            affordance_index_List = []
            for id_x in point_sample_idx:
                point_path = self.point_files[id_x]
                # åŠ è½½ç‚¹äº‘æ•°æ®å’ŒåŠŸèƒ½åŒºåŸŸæ©ç (åŠŸèƒ½åŒºåŸŸçƒ­åŠ›å›¾)
                Points, affordance_label = self.extract_point_file(point_path) # ï¼ˆ2048ï¼Œ3ï¼‰
                Points,_,_ = pc_normalize(Points)
                Points = Points.transpose() # (3,2048)
                affordance_index = self.get_affordance_label(img_path) # å½“å‰ç‚¹äº‘å¾…é¢„æµ‹çš„äº¤äº’è¡Œä¸º/åŠŸèƒ½åŒºåŸŸç±»å‹
                Points_List.append(Points)  # ç‚¹äº‘
                affordance_label_List.append(affordance_label) # åŠŸèƒ½åŒºåŸŸçƒ­åŠ›å›¾
                affordance_index_List.append(affordance_index) # å¾…é¢„æµ‹åŠŸèƒ½åŒºåŸŸç±»å‹

        else:
            Img = Img.resize(self.img_size)
            Img = img_normalize_train(Img)

            Point, affordance_label = self.extract_point_file(point_path)
            Point,_,_ = pc_normalize(Point)
            Point = Point.transpose()
 
        if(self.run_type == 'train'):
            # å›¾ç‰‡ ï¼Œ äº¤äº’ä¿¡æ¯æ–‡æœ¬ï¼Œç‰©ä½“å‡ ä½•ç»“æ„æ–‡æœ¬ï¼Œç‚¹äº‘æ ·æœ¬åˆ—è¡¨ï¼ŒåŠŸèƒ½åŒºåŸŸçƒ­åŠ›å›¾åˆ—è¡¨ï¼Œå¾…é¢„æµ‹åŠŸèƒ½åŒºåŸŸç±»å‹åˆ—è¡¨
            return Img, text_hd, text_od, Points_List, affordance_label_List, affordance_index_List
        else:
            return Img, text_hd, text_od, Point, affordance_label, img_path, point_path
```
### æ¨¡å‹

```python
class GREAT(nn.Module):
    ... 
    def forward(self, img, xyz, text_human, text_object):

        '''
        img: [B, 3, H, W]
        xyz: [B, 3, 2048]
        '''
       
        B, C, N = xyz.size()
        # 1. ç”¨Resnet18å¯¹å›¾åƒè¿›è¡Œç¼–ç ï¼Œè¿”å›çš„é«˜ç»´éšå‘é‡ç»´åº¦ä¸º (batch,512,7,7) -- ï¼ˆbatch,channel,h,w)
        F_I = self.img_encoder(img)     
        #   ç»´åº¦å±•å¹³(batch,channel,h*w)
        F_i = F_I.view(B, self.emb_dim, -1)         
        
        # 2ï¼Œ PointNet++ å¯¹ç‚¹äº‘è¿›è¡Œç¼–ç 
        F_p_wise = self.point_encoder(xyz)
        # 3. Roberta å¯¹äº¤äº’æ–‡æœ¬å’Œå‡ ä½•ç»“æ„æ–‡æœ¬è¿›è¡Œç¼–ç 
        T_h= self.text_encoder(text_human) # (batch,3,512)
        T_o = self.text_encoder2(text_object) # (batch,1,512)
        
        # 4. äº¤äº’æ–‡æœ¬å’Œå‡ ä½•ç»“æ„æ–‡æœ¬çš„ä¿¡æ¯é€šè¿‡æ”¹è‰¯çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œäº¤äº’èåˆ
        T_h_, T_o_ =self.affordance_dictionary_fusion(T_h, T_o)  # ç»´åº¦åŒä¸Šï¼Œå‡ä¿æŒä¸å˜

        # 5. äº¤äº’æ–‡æœ¬ä¿¡æ¯ä¸å›¾åƒä¿¡æ¯è¿›è¡Œèåˆ
        I_h = self.img_text_fusion(F_i,T_h_)   # (batch,512,49)
        
        # 6. å‡ ä½•ç»“æ„æ–‡æœ¬ä¿¡æ¯ä¸ç‚¹äº‘ä¿¡æ¯è¿›è¡Œèåˆï¼Œç„¶åè¿›å…¥pointnet++çš„ç‰¹å¾ä¼ æ’­é˜¶æ®µ(æ’å€¼é˜¶æ®µ)ï¼Œæœ€åå†ä¸I_hè¿›è¡Œäº¤äº’èåˆ
        _3daffordance = self.decoder(T_o_, I_h.permute(0,2,1), F_p_wise) # T_o_(batch,1,512)ï¼ŒI_h.permute(batch,49,512)ï¼Œç‚¹äº‘ç‰¹å¾ 
        
        return _3daffordance
```
#### æ–‡æœ¬ç¼–ç 

ä½¿ç”¨ RoBerta å¯¹äº¤äº’æ–‡æœ¬å’Œå‡ ä½•ç»“æ„æ–‡æœ¬è¿›è¡Œç¼–ç è¿™å—ï¼Œéœ€è¦æ³¨æ„åœ¨å¯¹äº¤äº’æ–‡æœ¬è¿›è¡Œç¼–ç æ—¶ï¼Œä¼šæŒ‰ç…§ "," å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºå¤šä¸ªå¥å­ï¼Œå¯¹æ¯ä¸ªå¥å­ç‹¬ç«‹è¿›è¡Œç¼–ç :

```bash
åŸå§‹äº¤äº’æ–‡æœ¬:

pour the liquid from the spout of the kettle using peopleâ€™s hand, grasp the kettle using person's hand around handle, open the kettle using people's fingers on the lid

åˆ‡åˆ†å:

pour the liquid from the spout of the kettle using peopleâ€™s hand
grasp the kettle using person's hand around handle
open the kettle using people's fingers on the lid
```

è¿™æ ·åšçš„åŸå› æ˜¯å› ä¸ºäº¤äº’æ–‡æœ¬ç”±å½“å‰å›¾ç‰‡åæ˜ çš„äº¤äº’è¡Œä¸ºå’Œæ¨¡å‹é¢å¤–è¡¥å……çš„å½“å‰ç‰©ä½“å­˜åœ¨çš„å…¶ä»–äº¤äº’è¡Œä¸ºæ„æˆï¼Œä»–ä»¬ä¹‹é—´çš„å…³ç³»æ˜¯ç‹¬ç«‹çš„ã€‚è€Œå‡ ä½•ç»“æ„æ–‡æœ¬åˆ™æ˜¯å•ä¸€è¿è´¯çš„å‡ ä½•æè¿°ï¼Œæ— éœ€åˆ‡åˆ†ï¼Œç›´æ¥å¯¹æ•´å¥è¿›è¡Œç¼–ç ã€‚

#### æ”¹è‰¯çš„äº¤å‰æ³¨æ„åŠ›

äººç±»é€šè¿‡åŒæ—¶åˆ†æç‰©ä½“çš„ åŠŸèƒ½æ„å›¾ï¼ˆå¦‚"å€’æ°´"ï¼‰å’Œ å‡ ä½•å±æ€§ï¼ˆå¦‚"å£¶å˜´çš„å½¢çŠ¶"ï¼‰æ¥æ¨æ–­äº¤äº’å¯èƒ½æ€§ã€‚äº¤å‰æ³¨æ„åŠ›æ¨¡æ‹Ÿäº†è¿™ç§åŒå‘æ¨ç†è¿‡ç¨‹ï¼Œé€šè¿‡å»ºç«‹æ„å›¾ä¸å‡ ä½•çš„æ˜¾å¼å…³è”ï¼Œå®ç°ç±»ä¼¼äººç±»çš„ç±»æ¯”æ¨ç†èƒ½åŠ›ã€‚

```python
class Cross_Attention(nn.Module):    
    ...
    def forward(self, hk, ok):

        '''
        hk : human knowledge [B,N_hk,C]
        ok : object knowledge [B,N_ok,C]
        '''

        # ç”¨æ„å›¾æ–‡æœ¬ï¼ˆå¦‚"pour"ï¼‰ç­›é€‰ç›¸å…³çš„å‡ ä½•ç‰¹å¾ï¼ˆå¼ºåŒ–"å£¶å˜´"ç»“æ„ï¼Œå¼±åŒ–"æŠŠæ‰‹"ï¼‰
        hk_q = self.proj_hq(hk)                                        
        ok_key = self.proj_ok(ok)                                       
        ok_value = self.proj_ov(ok)

        ok_key_ = torch.cat((hk_q,ok_key),dim=1)  # å¼ºåŒ–äººç±»æ„å›¾åœ¨ç‰©ä½“è¯­ä¹‰æ¨ç†ä¸­çš„å¼•å¯¼ä½œç”¨
        ok_value_ = torch.cat((hk_q,ok_value),dim=1) 

        atten_I1 = torch.bmm(hk_q, ok_key_.permute(0, 2, 1))*self.scale                 
        atten_I1 = atten_I1.softmax(dim=-1)                        
        I_1 = torch.bmm(atten_I1, ok_value_)  

        I_1 = self.layernorm(hk + I_1) 

        # ç”¨å‡ ä½•ç»“æ„ï¼ˆå¦‚"cylindrical handle"ï¼‰ä¿®æ­£æ„å›¾ç†è§£ï¼ˆæ’é™¤ä¸å‡ ä½•çŸ›ç›¾çš„æ„å›¾ï¼‰
        ok_q = self.proj_oq(ok)
        hk_key = self.proj_hk(hk)
        hk_value = self.proj_hv(hk)

        hk_key_ = torch.cat((ok_q,hk_key),dim=1)  # åˆ©ç”¨ç‰©ä½“ç»“æ„è¾…åŠ©æ¨æ–­æ›´å¤šäººç±»äº¤äº’æ„å›¾
        hk_value_ = torch.cat((ok_q,hk_value),dim=1)
                              
        atten_I2 = torch.bmm(ok_q, hk_key_.permute(0, 2, 1))*self.scale                 
        atten_I2 = atten_I2.softmax(dim=-1)
        I_2 = torch.bmm(atten_I2, hk_value_)                              
                                
        I_2 = self.layernorm(ok + I_2)    
        return I_1, I_2
```
#### å‡ ä½•ç»“æ„ä¿¡æ¯ä¸äº¤äº’ä¿¡æ¯çš„èåˆ


```python
class affordance_dictionary_fusion(nn.Module):
    ...
    def forward(self,f_hk,f_ok):
        # ç¬¬ä¸€é˜¶æ®µï¼šè¯­ä¹‰å¯¹é½ï¼ˆcross attentionï¼‰âœ æŠŠ Human ä¸ Object ä¿¡æ¯â€œè¿æ¥â€èµ·æ¥
        H, O = self.cross_atten(f_hk, f_ok)
        # ç¬¬äºŒé˜¶æ®µï¼šç»“æ„èåˆï¼ˆself attentionï¼‰âœ åœ¨ Human å†…éƒ¨æˆ– Object å†…éƒ¨ â€œæ•´ç†ã€æ€»ç»“ã€æ³›åŒ–â€          
        H_= self.h_atten(H)
        O_= self.o_atten(O)
        return H_, O_
```
#### äº¤äº’ä¿¡æ¯ä¸å›¾åƒç‰¹å¾çš„èåˆ

```python
class img_text_fusion(nn.Module):
    def __init__(self, emb_dim = 512, proj_dim = 512):
        class SwapAxes(nn.Module):
            def __init__(self):
                super().__init__()
            
            def forward(self, x):
                return x.transpose(1, 2)
        super().__init__()

        self.emb_dim = emb_dim
        self.proj_dim = proj_dim
        self.fusion = nn.Sequential(
            nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1),
            nn.BatchNorm1d(self.emb_dim),
            nn.ReLU()
        )         
        self.reshape = nn.Sequential(
            nn.Linear(3, 3 * 8), # (batch,512,24)
            SwapAxes(), # (batch,24,512)
            nn.BatchNorm1d(3 * 8),
            nn.ReLU(),
            SwapAxes(), # (batch,512,24)
            nn.Linear(3 * 8, 49), # ï¼ˆbatch,512,49)
        )

    # F_i (batch,512,49) --> (batch,channel,H*W) 
    def forward(self,F_i,T_h_):    
        # T_h_(batch,3,512) ---> è½¬ç½®å (batch,512,3) --> reshapeå (batch,512,49)
        T_h_ = self.reshape(T_h_.permute(0,2,1))
        # æ‹¼æ¥å: (batch,1024,49)  
        I_ = torch.cat((F_i, T_h_),dim=1)
        # é€šé“ç»´åº¦ä¸Šè¿›è¡Œç‰¹å¾èåˆï¼ŒåŒæ—¶é™ç»´: (batch.512,49)
        I_ = self.fusion(I_)  
        return I_
```
#### è§£ç é˜¶æ®µ

```python
class Decoder(nn.Module):
    def __init__(self, additional_channel, emb_dim, proj_dim):
        class SwapAxes(nn.Module):
            def __init__(self):
                super().__init__()
            
            def forward(self, x):
                return x.transpose(1, 2)
        super().__init__()
        
        self.emb_dim = emb_dim
        self.proj_dim = proj_dim
        #upsample
        self.fp3 = PointNetFeaturePropagation(in_channel=512+self.emb_dim, mlp=[768, 512])   
        self.fp2 = PointNetFeaturePropagation(in_channel=832, mlp=[768, 512])  
        self.fp1 = PointNetFeaturePropagation(in_channel=518+additional_channel, mlp=[512, 512]) 

        self.cmff = Cross_Modal_Feature_Fusion(emb_dim, proj_dim)
        self.out_head = nn.Sequential(
            nn.Linear(self.emb_dim, self.emb_dim // 8),
            SwapAxes(),
            nn.BatchNorm1d(self.emb_dim // 8),
            nn.ReLU(),
            SwapAxes(),
            nn.Linear(self.emb_dim // 8, 1),
        )
        self.reshape = nn.Sequential(
            nn.Linear(49, 49 * 8),
            SwapAxes(),
            nn.BatchNorm1d(49 * 8),
            nn.ReLU(),
            SwapAxes(),
            nn.Linear(49 * 8, 2048),
        )          
        self.sigmoid = nn.Sigmoid()
        self.fusion = nn.Sequential(
            nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1),
            nn.BatchNorm1d(self.emb_dim),
            nn.ReLU()
        )  
    
    def forward(self, T_o, I_h, encoder_p):

        '''
        T_o --->object knowledge embedding   (batch,1,512)
        I_h ---> [B, N_i, C] (batch,49,512)
        encoder_p  ---> [Hierarchy feature] 
        '''
        B, _, _ = I_h.shape

        # p_i[1]: (1,3,2048) , ï¼ˆ1ï¼Œ320ï¼Œ512) , (1,512,128) , (1,512,64) --> (batch,features,points)
        # p_i[0] ä¸ºåæ ‡
        p_0, p_1, p_2, p_3 = encoder_p  # é€å±‚ç‚¹äº‘ç‰¹å¾åˆ—è¡¨
        
        # 1. ä¼ å…¥æ•°æ®ç»´åº¦: (1,1,512) , (1,64,512) , ç‚¹äº‘ç‰¹å¾å’Œå‡ ä½•ç»“æ„ç‰¹å¾åšç‰¹å¾èåˆ
        p_3[1] = self.cmff(T_o, p_3[1].transpose(-2, -1)) # (1,512,64)

        # 2. è¿›å…¥PointNet++ç»å…¸çš„ç‰¹å¾ä¼ æ’­é˜¶æ®µ
        up_sample = self.fp3(p_2[0], p_3[0], p_2[1], p_3[1]) # (1,512,128)
        
        up_sample = self.fp2(p_1[0], p_2[0], p_1[1], up_sample) # (1,512,512)  
        
        up_sample = self.fp1(p_0[0], p_1[0], torch.cat([p_0[0], p_0[1]],1), up_sample) # (1,512,2048) 
        
        # 3. I_h reshapeå (1,512,2048)
        F_I = self.reshape(I_h.permute(0,2,1))
        
        # 4. å›¾åƒäº¤äº’ä¿¡æ¯ä¸ç‚¹äº‘ç‰¹å¾åšèåˆ: æ‹¼æ¥åï¼Œé€šé“ç»´åº¦ä¸Šè¿›è¡Œç‰¹å¾èåˆï¼ŒåŒæ—¶é™ç»´: (1,512,2048)
        F_j = torch.cat((F_I, up_sample),dim=1)
        F_j_fusion = self.fusion(F_j) 
        
        # 5. F_j_fusion.permuteå(1,2048,512) --> (1,2048,1)
        _3daffordance = self.out_head(F_j_fusion.permute(0, 2, 1))                   
        _3daffordance = self.sigmoid(_3daffordance) # ç”ŸæˆåŠŸèƒ½åŒºåŸŸæ©ç 

        return _3daffordance
```   

##### ç‚¹äº‘ç‰¹å¾ä¸å‡ ä½•ç»“æ„ç‰¹å¾çš„èåˆ

```python
class Cross_Modal_Feature_Fusion(nn.Module):
    def __init__(self, emb_dim, proj_dim):
        class SwapAxes(nn.Module):
            def __init__(self):
                super().__init__()
            
            def forward(self, x):
                return x.transpose(1, 2)
        super().__init__()
        self.emb_dim = emb_dim
        self.proj_dim = proj_dim
        self.cross_atten1 = Cross_Attention(emb_dim = self.emb_dim, proj_dim = self.proj_dim)

        # å‡è®¾è¾“å…¥æ•°æ®ç»´åº¦ä¸º (1,64,512) : å…ˆé™ç»´ï¼Œè¿›è¡Œä¿¡æ¯å‹ç¼©
        self.fc = nn.Sequential(
            nn.Linear(self.emb_dim, self.emb_dim//2), # (1,64,256) 
            SwapAxes(), # (1,256,64) 
            nn.BatchNorm1d(self.emb_dim // 2),
            nn.ReLU(),
            SwapAxes(), # (1,64,256) 
            nn.Linear(self.emb_dim//2, self.emb_dim), # (1,64,512)
            SwapAxes(), # (1,512,64)
            nn.BatchNorm1d(self.emb_dim),
            SwapAxes(), # (1,64,512)
        )

        self.norm1 = nn.LayerNorm(self.emb_dim)
        self.norm2 = nn.LayerNorm(self.emb_dim)
        self.pool = nn.AdaptiveAvgPool1d(1)

        self.fusion = nn.Sequential(                                        
            nn.Conv1d(2*self.emb_dim, self.emb_dim, 1, 1),
            nn.BatchNorm1d(self.emb_dim),   
            nn.ReLU()        
        )

    # (1,1,512) , (1,64,512)    
    def forward(self,f_t,f_p):
        _, N_P, _ = f_p.size()
        # 1. åº”ç”¨æ”¹è‰¯çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
        f_to, f_po = self.cross_atten1(f_t, f_p)    

        # 2. æ³¨æ„åŠ›åï¼ŒåŠ ä¸Šç»å…¸çš„: x + FNN 
        f_to = f_to + self.fc(f_to)                     
        f_po = f_po + self.fc(f_po)

        # 3. f_to.permuteç»´åº¦(1,512,1) --> poolå(1,512,1)
        f_t_p = self.pool(f_to.permute(0,2,1))
        # 4. ç»´åº¦æ‰©å±•åˆ°64 --> (1,512,64)                 
        f_t_r = f_t_p.repeat(1, 1, N_P)               

        # 5. f_po.permuteç»´åº¦(1,512,64) --> æ‹¼æ¥å(1,1024,64) 
        joint = torch.cat((f_po.permute(0,2,1), f_t_r), dim = 1)
        # 6. é€šé“ç»´åº¦ä½œä¿¡æ¯èåˆ(1,512,64)
        output = self.fusion(joint)   
        return output
```
