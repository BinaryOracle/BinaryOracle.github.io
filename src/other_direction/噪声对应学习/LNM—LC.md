---
title: Unsupervised Label Noise Modeling and Loss Correction 论文
icon: file
category:
  - 噪声对比学习
tag:
  - 噪声对比学习
  - 编辑中
footer: 技术共建，知识共享
date: 2025-09-18
author:
  - BinaryOracle
---

`Unsupervised Label Noise Modeling and Loss Correction 论文` 

<!-- more -->

> 论文链接: [Unsupervised Label Noise Modeling and Loss Correction](https://arxiv.org/abs/1904.11238)
> 代码链接: [https://github.com/PaulAlbert31/LabelNoiseCorrection](https://github.com/PaulAlbert31/LabelNoiseCorrection)

## 引言

卷积神经网络（CNN）是计算机视觉任务中的主流方法，能够在有大量标注数据时建模复杂模式。但实际中获取大规模干净数据很困难，人工或自动标注往往会产生错误，形成噪声样本。

已有研究发现：

* CNN 虽然对少量噪声有一定鲁棒性，但在随机梯度下降训练下，容易 **拟合随机标签**。

* 当同时存在干净和错误标签时，网络会先拟合干净样本，再逐步拟合噪声样本。

* 过拟合噪声会严重损害模型的泛化能力，也会加剧训练中的偏差问题（如类别不平衡）。

因此，**如何在不丢弃噪声样本的前提下，避免网络过拟合错误标签，同时利用这些样本的潜在信息**，成为一个关键挑战。

---

**现有方法与不足**：

* **损失校正类方法**：

  * *Bootstrapping loss*（Reed 等, 2015）在目标函数中引入一致性约束，通过结合网络预测结果和原始标签，减少噪声的负面影响。
  
  * 类别概率修正（Patrini 等, 2017; Hendrycks 等, 2018）通过估计每类的噪声分布，对损失进行调整，引导网络更靠近真实类别。

* **课程学习类方法**：

  * 通过“由易到难”的顺序训练（Bengio 等, 2009），在噪声场景中，易样本≈干净样本，难样本≈噪声样本，因此可以降低噪声样本对损失的贡献。
  
  * 但简单丢弃噪声样本可能损失了关于数据分布的重要信息。

* **其他改进方法**：

  * 相似性学习（Wang 等, 2018b）：将噪声样本的表示与干净样本拉开。
  
  * *Mixup* 数据增强（Zhang 等, 2018）：无需显式建模噪声，却展现出对标签噪声的强鲁棒性。

然而，这些方法往往假设存在一部分干净数据，或者在利用噪声样本方面不够充分。

---

**本文的核心思想与贡献**：

本文提出一种新的训练流程，即使在高比例噪声下，也能避免过拟合错误标签，同时有效利用噪声样本：

* 通过对 **样本损失分布** 进行建模，提出 **双成分 Beta 混合模型 (BMM)**，分别对应“干净样本”和“噪声样本”。

* 使用 BMM 的后验概率来动态调整 **bootstrapping loss**，从而实现对每个样本的损失校正。

* 在不丢弃噪声样本的情况下，保持训练鲁棒性，并能学习到有利的表征。

* 将方法与 **mixup 数据增强** 结合，进一步提升性能，即便在极端噪声条件下依然能收敛。

**主要贡献**：

* 提出基于样本损失的无监督标签噪声建模方法。

* 提出结合该模型的损失校正策略，避免过拟合噪声标签。

* 将方法与 mixup 数据增强结合，推动了当前最优水平的提升。

* 在极端噪声水平下，依然能通过改进的 mixup 策略实现收敛。

## 相关工作

**标签噪声的两类场景**：

* **封闭集 (closed-set)**：所有样本（包括噪声样本）的真实标签都在已知集合 $S$ 内。

* **开放集 (open-set)**：某些噪声样本 $x_i$ 的真实标签可能在 $S$ 之外，即 $x_i$ 可能是分布外样本。

本文主要关注 **封闭集标签噪声**，并在此场景下开展研究。

---

**封闭集下的噪声类型**：

* **均匀随机噪声（对称噪声）**：真实标签以相同概率被随机翻转到其他类别。

* **非均匀随机噪声（类别条件噪声）**：不同类别有不同的翻转概率。

已有研究（Patrini 等, 2017）表明：**均匀噪声比非均匀噪声更难处理**。

---

**现有应对方法**：

* **丢弃噪声样本**

  简单但有风险：难样本可能被误判为噪声样本。
  
  尽管如此，研究（Ding 等, 2018）表明，丢弃高概率错误的样本并将它们用于半监督学习仍然有效。

* **重新标注样本**

  通过不同模型尝试恢复真实标签：

  * 有向图模型 (Xiao 等, 2015)

  * 条件随机场 (Vahdat, 2017)

  * CNN (Veit 等, 2017)

  但这些方法普遍假设存在一小部分干净数据。Tanaka 等（2018）则证明，可以利用网络预测在无监督下进行重新标注，生成软标签或硬标签。

* **损失校正方法**

  通过修改损失函数或计算损失所需的概率来抵消噪声：

  * Reed 等（2015）：在损失中加入感知一致性项，引入对模型预测的依赖，但噪声标签始终影响目标函数。

  * Patrini 等（2017）：提出两种方法：

    * **Backward 方法**：利用噪声转移矩阵 $T$ 的逆来加权损失。

    * **Forward 方法**：对预测概率进行修正，即用 $T$ 对预测分布做变换。

  * Hendrycks 等（2018）：基于在干净数据上训练的模型，计算腐蚀矩阵 (corruption matrix)，并用它来修正预测概率。

* **基于重加权的课程学习**

  * Jiang 等（2018b）：提出“导师-学生”框架，导师网络学习课程（即样本权重），指导学生网络在噪声下训练。

  * Guo 等（2018）：利用特征空间分布复杂度进行无监督估计，结合干净和噪声样本训练。

  * Ren 等（2018）：根据训练梯度与验证集梯度的方向一致性来为样本加权。

这些方法与重新标注方法一样，通常依赖于一部分干净数据，限制了实际应用。

---

**不依赖干净数据的方法**：

* Wang 等（2018b）：提出无监督的噪声检测方法，结合相似性学习，将噪声样本的表示与各类别的干净样本表示拉开，同时不丢弃噪声样本。

---

**本文的区别与方法**：

与现有方法不同，本文仅依赖 **每个样本的训练损失**，而无需任何干净数据。核心思路是：

* 对样本的损失分布拟合一个 **双成分 Beta 混合模型 (BMM)**，分别建模干净样本与噪声样本。

* 利用 BMM 的后验概率，实现 **损失校正**。

* 方法结合 **bootstrapping loss**（Reed 等, 2015）与 **mixup 数据增强**（Zhang 等, 2018），从而更好地应对封闭集标签噪声场景。

## 方法

### 1. 标签噪声建模

图像分类被定义为从训练集

$D = \{(x_i, y_i)\}_{i=1}^N$

学习一个模型 $h_\theta(x)$，其中 $y_i$ 是 one-hot 标签。本文中 $h_\theta$ 是 CNN，$\theta$ 是权重和偏置。训练通过最小化交叉熵损失：

$$
\mathcal{L}(\theta) = - \sum_{i=1}^N y_i^T \log(h_\theta(x_i)) .
$$

当标签含有噪声时，样本 $x_i$ 可能带有错误标签，因此需要扩展损失函数来抵抗噪声。

---

![](LNM-LC/2.png)

**核心观察**（见图1与图2）：

* 随机错误标签比正确标签更难学，因此在训练早期，噪声样本的损失值更高。

* CNN 在拟合干净样本后才逐渐拟合噪声样本。

* 因此，仅通过损失分布就能区分干净与噪声样本。

![](LNM-LC/1.png)

这启发了本文提出的：通过 **混合模型** 对损失分布建模，进而判别样本是否为噪声。

---

**混合模型的选择**：

* 一般混合模型定义为：

  $$
  p(\mathcal{L}) = \sum_{k=1}^K \lambda_k p(\mathcal{L} \mid k) ,
  $$

  其中 $\lambda_k$ 是混合系数。

* 直观做法是使用 **高斯混合模型(GMM)** 来区分干净与噪声（$K=2$）。

* 但高斯分布不能很好刻画干净样本的分布，因为其损失高度偏向零。

* **Beta 分布** 更灵活，能拟合 $[0,1]$ 区间的对称或偏态分布，因此 **Beta 混合模型(BMM)** 更适合。

* 实验表明：在 CIFAR-10 上 80% 标签噪声条件下，BMM 相比 GMM 在 **ROC-AUC 提升约 5 个点**（见附录A）。

---

**Beta 分布定义**：

$$
p(\mathcal{L} \mid \alpha, \beta) = 
\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} 
\mathcal{L}^{\alpha - 1} (1 - \mathcal{L})^{\beta - 1}, \quad \alpha, \beta > 0
$$

将其代入公式(2)，即可得到 BMM 的概率密度函数。

---

**参数估计：EM 算法流程**

* **E 步**（公式(4)）：固定参数，计算后验概率 $\gamma_k(\mathcal{L})$，即样本损失来自分量 $k$ 的概率。

* **M 步**（公式(5)(6)(7)）：根据加权矩方法更新 $\alpha_k, \beta_k$，并计算加权均值 $\bar{\mathcal{L}}_k$ 与方差 $s_k^2$。

* **更新混合系数**（公式(8)）：

  $$
  \lambda_k = \frac{1}{N} \sum_{i=1}^N \gamma_k(\mathcal{L}_i)
  $$

* 交替进行 E/M 步，直至收敛或迭代 10 次（本文实验设置）。

* **数值稳定性处理**：为避免 $\mathcal{L} \approx 0$ 或 $\mathcal{L} \approx 1$ 导致的不稳定，作者将观测值限制在区间 $[\epsilon, 1-\epsilon]$，其中 $\epsilon = 10^{-4}$。

---

**最终判别结果**（公式(9)）：

* 得到样本为干净或噪声的概率：

  $$
  p(k \mid \mathcal{L}_i) = \frac{p(k) p(\mathcal{L}_i \mid k)}{p(\mathcal{L}_i)}, \quad k=0,1
  $$

* $k=0$ 表示干净样本，$k=1$ 表示噪声样本。

* 用于估计分布的损失始终是 **标准交叉熵损失**，在每个 epoch 后对所有样本重新计算。这与训练损失可能不同，因为训练时会引入噪声修正项。
