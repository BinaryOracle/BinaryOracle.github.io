---
title: ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) å­¦ä¹ ç¬”è®°
icon: file
category:
  - ç”Ÿæˆæ¨¡å‹
tag:
  - ç”Ÿæˆæ¨¡å‹
  - å·²å‘å¸ƒ
footer: æŠ€æœ¯å…±å»ºï¼ŒçŸ¥è¯†å…±äº«
date: 2025-08-09
author:
  - BinaryOracle
---

`ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) å­¦ä¹ ç¬”è®°` 

<!-- more -->

## å‰ç½®çŸ¥è¯†

### æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMaximum Likelihood Estimation, MLEï¼‰

æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ˜¯ä¸€ç§**åˆ©ç”¨è§‚æµ‹æ•°æ®åå‘æ¨æ–­æ¨¡å‹å‚æ•°çš„æ–¹æ³•**ã€‚ç›´è§‚åœ°è¯´ï¼Œå®ƒå‡è®¾æ•°æ®ç”Ÿæˆè¿‡ç¨‹å·²çŸ¥ï¼ˆæ¦‚ç‡æ¨¡å‹å·²å®šï¼‰ï¼Œä½†å‚æ•°æœªçŸ¥ï¼Œæˆ‘ä»¬é€šè¿‡å·²è§‚æµ‹åˆ°çš„æ ·æœ¬æƒ…å†µå»â€œ**çŒœæµ‹**â€å“ªä¸ªå‚æ•°å€¼æœ€åˆç†ã€‚å…·ä½“åœ°ï¼ŒMLE é€‰å–ä½¿å¾—è§‚æµ‹åˆ°çš„æ•°æ®å‡ºç°çš„æ¦‚ç‡ï¼ˆå³**ä¼¼ç„¶**ï¼‰æœ€å¤§çš„å‚æ•°å€¼ä¸ºä¼°è®¡ç»“æœã€‚

ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æŠ›ç¡¬å¸100æ¬¡å‡å‡ºç°æ­£é¢ï¼Œæˆ‘ä»¬å¾ˆè‡ªç„¶åœ°è®¤ä¸ºè¿™æšç¡¬å¸ä¸æ˜¯å…¬å¹³çš„ï¼Œè€Œæœ€å¯èƒ½çš„æ˜¯ä¸¤é¢éƒ½ä¸ºæ­£é¢ã€‚è¿™ç§æ ¹æ®å®éªŒç»“æœæ¨æ–­æœ€æœ‰å¯èƒ½çš„ç¡¬å¸å±æ€§ï¼Œå°±æ˜¯æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„ç›´è§‚æ€æƒ³ã€‚åŒæ ·åœ°ï¼Œè‹¥10æ¬¡æŠ›ç¡¬å¸å‡ºç°6æ¬¡æ­£é¢ï¼Œæˆ‘ä»¬å€¾å‘äºä¼°è®¡ç¡¬å¸å‡ºç°æ­£é¢çš„æ¦‚ç‡ä¸º0.6ï¼Œå› ä¸ºè¿™ä¸ªå‡è®¾ä½¿å¾—äº§ç”Ÿâ€œ6æ¬¡æ­£é¢â€çš„å¯èƒ½æ€§æœ€å¤§ã€‚æ›´å½¢å¼åŒ–åœ°è¯´ï¼Œå¯¹äºå‚æ•° $\theta$ çš„ä»»æ„å‡è®¾ï¼Œå®šä¹‰**ä¼¼ç„¶å‡½æ•°** $L(\theta)=P(\text{æ•°æ®}\mid\theta)$ï¼ŒMLE å°±æ˜¯å–ä½¿ $L(\theta)$ æœ€å¤§çš„ $\theta$ã€‚**åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œè¿™ä¸ªæœ€å¤§ç‚¹ç§°ä¸ºå‚æ•°çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡**ã€‚

#### æ•°å­¦æ¨å¯¼

è®¾æ ·æœ¬æ•°æ®ä¸º $X_1,X_2,\dots,X_n$ï¼Œå‡å®šå…¶ç‹¬ç«‹åŒåˆ†å¸ƒäºå‚æ•°ä¸º $\theta$ çš„æŸä¸ªæ¦‚ç‡åˆ†å¸ƒã€‚**ä¼¼ç„¶å‡½æ•°**å®šä¹‰ä¸ºåœ¨å‚æ•° $\theta$ ç»™å®šä¸‹è§‚æµ‹åˆ°è¯¥æ ·æœ¬çš„è”åˆæ¦‚ç‡æˆ–æ¦‚ç‡å¯†åº¦ï¼š

$$
L(\theta\mid X_1,\dots,X_n)=P(X_1,\dots,X_n\mid \theta)\,.
$$

è‹¥å„è§‚æµ‹ç‹¬ç«‹ï¼Œåˆ™

$$
L(\theta\mid \mathbf{X})=\prod_{i=1}^n f(X_i;\theta)\,,
$$

å…¶ä¸­ $f(x;\theta)$ æ˜¯å•ä¸ªæ ·æœ¬çš„æ¦‚ç‡è´¨é‡ï¼ˆæˆ–å¯†åº¦ï¼‰å‡½æ•°ã€‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡å°±æ˜¯æ±‚è§£ä¼˜åŒ–é—®é¢˜

$$
\hat\theta_{\text{MLE}}=\underset{\theta}{\arg\max}\;L(\theta\mid \mathbf{X})\,,
$$

å³å¯»æ‰¾ä½¿ä¼¼ç„¶å‡½æ•°å–å¾—æœ€å¤§å€¼çš„å‚æ•°ã€‚ä¸ºäº†ç®€åŒ–è®¡ç®—ï¼Œé€šå¸¸å–å¯¹æ•°å¾—åˆ°**å¯¹æ•°ä¼¼ç„¶** $\ell(\theta)=\ln L(\theta)$ï¼Œç”±äºå¯¹æ•°æ˜¯å•è°ƒé€’å¢çš„ï¼Œ$\ell(\theta)$ çš„æœ€å¤§å€¼å¤„å³å¯¹åº” $L(\theta)$ çš„æœ€å¤§å€¼ã€‚å¯¹æ•°ä¼¼ç„¶å±•å¼€ä¸º

$$
\ell(\theta)=\sum_{i=1}^n\ln f(X_i;\theta)\,.
$$

ç„¶åå¯¹ $\ell(\theta)$ å…³äºå„å‚æ•°æ±‚åå¯¼å¹¶ä»¤å…¶ä¸ºé›¶ï¼Œå³å¯å¾—åˆ°æå¤§ä¼¼ç„¶ä¼°è®¡çš„æ–¹ç¨‹ï¼ˆä¼¼ç„¶æ–¹ç¨‹ï¼‰ï¼Œè¿›è€Œæ±‚å‡º $\hat\theta$ã€‚

ä¸‹é¢ç»™å‡ºå‡ ä¸ªç¤ºä¾‹ï¼š

* **ç¦»æ•£åˆ†å¸ƒï¼ˆæ·ç¡¬å¸ï¼‰**ï¼šè®¾æ¯æ¬¡æŠ›ç¡¬å¸ç»“æœ $X_i\in{0,1}$ï¼ˆ1 è¡¨ç¤ºæ­£é¢ï¼‰ï¼Œä¸”ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œ$P(X_i=1)=p$ã€‚è‹¥ $n$ æ¬¡æŠ›æ·ä¸­å‡ºç° $k$ æ¬¡æ­£é¢ï¼Œåˆ™ä¼¼ç„¶å‡½æ•°ä¸º $L(p)=p^k(1-p)^{n-k}$ã€‚å–å¯¹æ•°å¾— $\ell(p)=k\ln p+(n-k)\ln(1-p)$ï¼Œå¯¹ $p$ æ±‚å¯¼åä»¤å¯¼æ•°ä¸ºé›¶ï¼š$\frac{k}{p}-\frac{n-k}{1-p}=0$ï¼Œè§£å¾—æå¤§ä¼¼ç„¶ä¼°è®¡ $\hat p=k/n$ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç¡¬å¸æ­£é¢æ¦‚ç‡çš„ MLE ç­‰äºæ­£é¢æ¬¡æ•°å æ€»æ¬¡æ•°çš„æ¯”ä¾‹ã€‚
* **è¿ç»­åˆ†å¸ƒï¼ˆæ­£æ€åˆ†å¸ƒï¼‰**ï¼šå‡è®¾ $X_1,\dots,X_n$ ç‹¬ç«‹åŒåˆ†å¸ƒäº $N(\mu,\sigma^2)$ï¼Œä¼¼ç„¶å‡½æ•°ä¸º

  $$
  L(\mu,\sigma^2)\propto(\sigma^2)^{-n/2}\exp\Big(-\frac{1}{2\sigma^2}\sum_{i=1}^n(X_i-\mu)^2\Big).
  $$

  å¯¹ $\mu,\sigma^2$ æ±‚å¯¼å¹¶ä»¤åå¯¼æ•°ä¸ºé›¶å¯çŸ¥ï¼ŒMLE è§£ä¸ºæ ·æœ¬å‡å€¼å’Œï¼ˆæ— åè°ƒæ•´å‰çš„ï¼‰æ ·æœ¬æ–¹å·®ï¼š$\hat\mu=\frac{1}{n}\sum_i X_i,\ \hat\sigma^2=\frac{1}{n}\sum_i (X_i-\hat\mu)^2$ã€‚æ¢è¨€ä¹‹ï¼Œåœ¨æ­£æ€åˆ†å¸ƒå‡è®¾ä¸‹ï¼ŒMLE ç»™å‡ºäº†ç›´è§‚çš„æ ·æœ¬ç»Ÿè®¡é‡ä½œä¸ºå‚æ•°ä¼°è®¡ã€‚

### ä¿¡æ¯è®º: ä¿¡æ¯é‡ï¼Œç†µï¼Œäº¤å‰ç†µï¼ŒKLæ•£åº¦

#### 1. **ä¿¡æ¯é‡ï¼ˆSelf-Informationï¼‰**

ä¿¡æ¯è®ºæœ€æ ¸å¿ƒçš„é—®é¢˜æ˜¯ï¼šä¸€ä¸ªäº‹ä»¶çš„å‘ç”Ÿç»™æˆ‘ä»¬å¸¦æ¥å¤šå°‘â€œæƒŠè®¶â€æˆ–â€œæ–°ä¿¡æ¯â€ï¼Ÿ

* å…¬å¼ï¼š

$$
I(x) = -\log p(x)
$$

è¿™é‡Œ $p(x)$ æ˜¯äº‹ä»¶ $x$ å‘ç”Ÿçš„æ¦‚ç‡ã€‚

* æ¦‚ç‡è¶Šå° â†’ äº‹ä»¶è¶Šç½•è§ â†’ ä¿¡æ¯é‡è¶Šå¤§ã€‚

* å¯¹æ•°åº•çš„é€‰æ‹©ï¼š

  * $\log_2$ï¼šä¿¡æ¯å•ä½æ˜¯ **bit**ï¼ˆäºŒè¿›åˆ¶æ¯”ç‰¹ï¼‰

  * $\ln$ï¼šä¿¡æ¯å•ä½æ˜¯ **nat**ï¼ˆè‡ªç„¶å¯¹æ•°ï¼‰

**ä¾‹å­**ï¼š

* æŠ›ç¡¬å¸å¾—åˆ°æ­£é¢ï¼ˆ$p=0.5$ï¼‰ï¼šä¿¡æ¯é‡ = $-\log_2 0.5 = 1$ bit

* æŠ›ç¡¬å¸è¿ç»­ä¸¤æ¬¡éƒ½æ­£é¢ï¼ˆ$p=0.25$ï¼‰ï¼šä¿¡æ¯é‡ = $-\log_2 0.25 = 2$ bit

---

#### 2. **ç†µï¼ˆEntropyï¼‰**

ç†µæ˜¯**å¹³å‡ä¿¡æ¯é‡**ï¼Œç”¨æ¥è¡¡é‡ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„ä¸ç¡®å®šæ€§ã€‚

å…¬å¼ï¼ˆç¦»æ•£åˆ†å¸ƒï¼‰ï¼š

$$
H(P) = -\sum_x P(x) \log P(x)
$$

* ç†µè¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šâ€œæ··ä¹±â€æˆ–ä¸ç¡®å®šã€‚

* å¦‚æœäº‹ä»¶æ¦‚ç‡å…¨ä¸€æ ·ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰ï¼Œç†µæœ€å¤§ã€‚

* å¦‚æœä¸€ä¸ªäº‹ä»¶æ¦‚ç‡æ˜¯ 1ï¼ˆç¡®å®šäº‹ä»¶ï¼‰ï¼Œç†µä¸º 0ï¼ˆæ²¡æœ‰ä¸ç¡®å®šæ€§ï¼‰ã€‚

**ä¾‹å­**ï¼š

* å…¬å¹³ç¡¬å¸ï¼š$H=1$ bit

* åç½®ç¡¬å¸ï¼ˆæ­£é¢ 0.9ï¼‰ï¼š$H \approx 0.47$ bit

---

#### 3. **äº¤å‰ç†µï¼ˆCross-Entropyï¼‰**

äº¤å‰ç†µè¡¡é‡**ç”¨åˆ†å¸ƒ Q å»ç¼–ç çœŸå®æ¥è‡ª P çš„äº‹ä»¶æ—¶çš„å¹³å‡ä¿¡æ¯é‡**ã€‚

å…¬å¼ï¼š

$$
H(P, Q) = -\sum_x P(x) \log Q(x)
$$

è§£é‡Šï¼š

* å¦‚æœ $P=Q$ï¼Œäº¤å‰ç†µ = ç†µï¼Œç¼–ç æ˜¯æœ€ä¼˜çš„ã€‚

* å¦‚æœ $Q$ åç¦» $P$ï¼Œäº¤å‰ç†µä¼šå¤§äºç†µï¼ˆç¼–ç å˜é•¿ï¼‰ã€‚

---

#### 4. **KL æ•£åº¦ï¼ˆä¿¡æ¯å¢ç›Šï¼‰**

KL æ•£åº¦å°±æ˜¯**äº¤å‰ç†µ - ç†µ**ï¼š

$$
D_{\mathrm{KL}}(P \parallel Q) = H(P, Q) - H(P)
$$

> äº¤å‰ç†µå¯ä»¥çœ‹ä½œæ˜¯å¹³å‡ä¿¡æ¯é‡ åŠ ä¸Šå› ä¸ºè¿‘ä¼¼ä¸å‡†è€Œå¤šèŠ±çš„é‚£éƒ¨åˆ†æˆæœ¬ï¼Œè€Œé‚£éƒ¨åˆ†æˆæœ¬å°±æ˜¯ KL æ•£åº¦ã€‚

ä¹Ÿå¯ä»¥å†™æˆï¼š

$$
D_{\mathrm{KL}}(P \parallel Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
$$

å«ä¹‰ï¼š

* å®ƒè¡¨ç¤ºï¼šå¦‚æœä½ ç”¨ Q æ¥è¿‘ä¼¼ Pï¼Œå¹³å‡æ¯ä¸ªæ ·æœ¬è¦å¤šèŠ±å¤šå°‘ä¿¡æ¯é‡ï¼ˆæ¯”ç‰¹ / natï¼‰ã€‚

* å®ƒæ€»æ˜¯ $\geq 0$ï¼ˆ**Gibbs ä¸ç­‰å¼**ï¼‰ï¼Œä¸”å½“ $P=Q$ æ—¶ä¸º 0ã€‚

---

âœ… **å°ç»“å…³ç³»å›¾**ï¼š

```
ä¿¡æ¯é‡ I(x)  â†’  ç†µ H(P) = å¹³å‡ I(x)  
              â†˜  äº¤å‰ç†µ H(P, Q) = å¹³å‡ -log Q(x)
               â†˜ KLæ•£åº¦ = H(P, Q) - H(P)

æ³¨: å¹³å‡æ˜¯æŒ‡æ±‚æœŸæœ›               
```

### äº¤å‰ç†µæŸå¤±ï¼ˆCross-Entropy Lossï¼‰

äº¤å‰ç†µæŸå¤±æ˜¯ä¸€ç§è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒå·®å¼‚çš„æŒ‡æ ‡ï¼Œå¸¸ç”¨åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯äºŒåˆ†ç±»å’Œå¤šåˆ†ç±»é—®é¢˜ã€‚å®ƒç”¨æ¥è¡¡é‡æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒ $\hat{y}$ å’ŒçœŸå®æ ‡ç­¾åˆ†å¸ƒ $y$ ä¹‹é—´çš„è·ç¦»ã€‚

> **äºŒåˆ†ç±»äº¤å‰ç†µ:**

å¯¹äºæ ‡ç­¾ $y \in \{0,1\}$ï¼Œé¢„æµ‹æ¦‚ç‡ $p = \hat{y} = P(y=1)$ï¼Œäº¤å‰ç†µå®šä¹‰ä¸ºï¼š

$$
L = -[y \log p + (1 - y) \log (1 - p)]
$$

è§£é‡Šï¼š

* å¦‚æœçœŸå®æ˜¯æ­£ç±»ï¼ˆ$y=1$ï¼‰ï¼ŒæŸå¤±å°±æ˜¯ $-\log p$ï¼Œé¢„æµ‹è¶Šæ¥è¿‘1æŸå¤±è¶Šå°ã€‚

* å¦‚æœçœŸå®æ˜¯è´Ÿç±»ï¼ˆ$y=0$ï¼‰ï¼ŒæŸå¤±å°±æ˜¯ $-\log (1-p)$ï¼Œé¢„æµ‹è¶Šæ¥è¿‘0æŸå¤±è¶Šå°ã€‚

---

> **å¤šåˆ†ç±»äº¤å‰ç†µ(å¯¹å•ä¸ªæ ·æœ¬):**

è®¾çœŸå®æ ‡ç­¾æ˜¯ one-hot å‘é‡ $y = (y_1, y_2, ..., y_C)$ï¼Œé¢„æµ‹æ¦‚ç‡åˆ†å¸ƒæ˜¯ $\hat{y} = (\hat{y}_1, \hat{y}_2, ..., \hat{y}_C)$ï¼Œå…¶ä¸­ $C$ æ˜¯ç±»åˆ«æ•°ï¼š

$$
L = - \sum_{i=1}^C y_i \log \hat{y}_i
$$

å³åªå¯¹çœŸå®ç±»åˆ«å¯¹åº”çš„æ¦‚ç‡å–è´Ÿå¯¹æ•°ã€‚

###  JSæ•£åº¦ï¼ˆJensen-Shannon divergenceï¼‰

#### 1. JSæ•£åº¦æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆæµ…å±‚ç›´è§‚ï¼‰

* JSæ•£åº¦æ˜¯è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒå·®å¼‚çš„ä¸€ä¸ªæ–¹æ³•ã€‚

* å®ƒæ˜¯ **KLæ•£åº¦çš„â€œå¯¹ç§°æ”¹è¿›ç‰ˆâ€**ï¼Œæ‰€ä»¥å®ƒæ€»æ˜¯éè´Ÿä¸”æœ‰é™ï¼Œä¸”æ»¡è¶³å¯¹ç§°æ€§ï¼š

  $$
  \mathrm{JS}(P \parallel Q) = \mathrm{JS}(Q \parallel P)
  $$

* ç®€å•è¯´ï¼Œå°±æ˜¯å‘Šè¯‰ä½  $P$ å’Œ $Q$ ä¸¤ä¸ªåˆ†å¸ƒç›¸å·®å¤šè¿œã€‚

---

#### 2. ä¸ºä»€ä¹ˆè¦ç”¨JSæ•£åº¦è€Œä¸æ˜¯KLæ•£åº¦ï¼Ÿ

* **KLæ•£åº¦**ä¸å¯¹ç§°ï¼š

  $$
  \mathrm{KL}(P \parallel Q) \neq \mathrm{KL}(Q \parallel P)
  $$

* KLæ•£åº¦æœ‰æ—¶ä¼šæ— ç©·å¤§ï¼ˆå¦‚æœ $Q$ åœ¨ $P$ æ”¯æŒçš„åŒºåŸŸä¸º0ï¼Œä¼šå¯¼è‡´ $\log 0$ ä¸å­˜åœ¨ï¼‰ã€‚

* **JSæ•£åº¦**è§£å†³äº†è¿™äº›é—®é¢˜ï¼Œå˜å¾—å¯¹ç§°ä¸”æœ‰ç•Œï¼ˆæœ€å¤§å€¼æ˜¯ $\log 2$ï¼‰ã€‚

---

#### 3. JSæ•£åº¦çš„æ•°å­¦å®šä¹‰

ç»™ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒ $P$ å’Œ $Q$ï¼Œå®šä¹‰å®ƒä»¬çš„å¹³å‡åˆ†å¸ƒï¼š

$$
M = \frac{1}{2}(P + Q)
$$

JSæ•£åº¦å®šä¹‰ä¸ºï¼š

$$
\mathrm{JS}(P \parallel Q) = \frac{1}{2} \mathrm{KL}(P \parallel M) + \frac{1}{2} \mathrm{KL}(Q \parallel M)
$$

å…¶ä¸­ï¼Œ$\mathrm{KL}(\cdot \parallel \cdot)$ æ˜¯KLæ•£åº¦ã€‚

---

#### 4. ç›´è§‚ç†è§£JSæ•£åº¦

* $M$ æ˜¯ $P$ å’Œ $Q$ çš„â€œä¸­é—´â€åˆ†å¸ƒã€‚

* ä½ å…ˆæµ‹é‡ $P$ ä¸ $M$ çš„å·®å¼‚ï¼ˆKLæ•£åº¦ï¼‰ï¼Œå†æµ‹é‡ $Q$ ä¸ $M$ çš„å·®å¼‚ï¼Œå–å¹³å‡ã€‚

* å¦‚æœ $P$ å’Œ $Q$ å¾ˆæ¥è¿‘ï¼Œé‚£ä¹ˆå®ƒä»¬éƒ½å’Œ $M$ å¾ˆæ¥è¿‘ï¼ŒJSæ•£åº¦å°ã€‚

* å¦‚æœå¾ˆä¸ä¸€æ ·ï¼ŒJSæ•£åº¦å°±å¤§ã€‚


> æƒ³è±¡æœ‰ä¸¤ä¸ªäººåˆ†åˆ«ç«™åœ¨ä¸€æ¡ç›´çº¿ä¸Šçš„ä¸åŒç‚¹ï¼Œğ‘ƒ å’Œ Q å°±æ˜¯ä¸¤ä¸ªäººçš„ä½ç½®ï¼ŒM æ˜¯ä»–ä»¬çš„ä¸­é—´ç‚¹ã€‚ä½ æµ‹é‡ä¸¤ä¸ªäººåˆ°ä¸­é—´ç‚¹çš„è·ç¦»ï¼Œå–å¹³å‡ã€‚è¿™ä¸ªå¹³å‡è·ç¦»è¶Šå¤§ï¼Œè¯´æ˜ä¸¤ä¸ªäººç›¸è·è¶Šè¿œã€‚è¿™æ ·å°±å…¬å¹³ã€å¯¹ç§°åœ°åæ˜ äº†ä¸¤äººè·ç¦»ï¼Œè€Œä¸æ˜¯å•æ–¹é¢å»çœ‹æŸä¸€ä¸ªäººçš„ä½ç½®ã€‚

---

#### 5. ä¸¾ä¸ªç®€å•ä¾‹å­

å‡è®¾ï¼š

* $P = (1, 0)$ï¼Œ

* $Q = (0, 1)$ï¼ˆä¸¤ä¸ªå®Œå…¨ä¸åŒçš„åˆ†å¸ƒï¼Œäº’æ–¥äº‹ä»¶ï¼‰ã€‚

é‚£ä¹ˆï¼š

* $M = (0.5, 0.5)$

* $\mathrm{KL}(P \parallel M) = 1 \times \log \frac{1}{0.5} + 0 \times \log \frac{0}{0.5} = \log 2$

* $\mathrm{KL}(Q \parallel M) = \log 2$

* æ‰€ä»¥ $\mathrm{JS}(P \parallel Q) = \frac{1}{2} \log 2 + \frac{1}{2} \log 2 = \log 2$

è¿™æ˜¯æœ€å¤§å€¼ï¼Œè¡¨ç¤ºä¸¤åˆ†å¸ƒå·®å¼‚æœ€å¤§ã€‚

### 1-Lipschitz å‡½æ•°

ä¸€ä¸ªå‡½æ•° $f: \mathbb{R}^n \to \mathbb{R}$ å¦‚æœå­˜åœ¨ä¸€ä¸ªå¸¸æ•° $K$ï¼ˆå« **Lipschitz å¸¸æ•°**ï¼‰ï¼Œä½¿å¾—ï¼š

$$
|f(x_1) - f(x_2)| \le K \cdot \|x_1 - x_2\|
$$

å¯¹ä»»æ„ $x_1, x_2$ éƒ½æˆç«‹ï¼Œé‚£ä¹ˆ $f$ å«åš **K-Lipschitz å‡½æ•°**ã€‚

* **K è¶Šå°**ï¼šå‡½æ•°è¶Šå¹³æ»‘ï¼Œå˜åŒ–è¶Šæ…¢ã€‚

* **K è¶Šå¤§**ï¼šå‡½æ•°å˜åŒ–å¯èƒ½å¾ˆé™¡ï¼Œä½†ä»æœ‰é™åˆ¶ã€‚

---

å½“ $K = 1$ æ—¶ï¼š

$$
|f(x_1) - f(x_2)| \le \|x_1 - x_2\|
$$

è¿™æ„å‘³ç€ï¼š

* è¾“å…¥å˜åŒ–å¤šå°‘ï¼Œè¾“å‡ºçš„å˜åŒ–é‡æœ€å¤šç­‰äºè¾“å…¥å˜åŒ–é‡ã€‚

* **ç›¸å½“äºé™åˆ¶äº†å‡½æ•°çš„æœ€å¤§â€œæ–œç‡â€æ˜¯ 1**ã€‚

æƒ³è±¡ä½ èµ°å±±è·¯ï¼š

* å¦‚æœæ˜¯ 1-Lipschitzï¼Œèµ° 1 ç±³æ°´å¹³è·¯ï¼Œæµ·æ‹”æœ€å¤šå˜ 1 ç±³ã€‚

* å¦‚æœæ˜¯ 2-Lipschitzï¼Œèµ° 1 ç±³æ°´å¹³è·¯ï¼Œæµ·æ‹”å¯èƒ½å˜ 2 ç±³ï¼Œæ›´é™¡ã€‚

è¿™ä¸ªçº¦æŸèƒ½é˜²æ­¢å‡½æ•°çš„å˜åŒ–å¤ªå¿«ï¼Œè®©å®ƒæ¯”è¾ƒâ€œæ¸©å’Œâ€ã€‚

## åŸå§‹ GAN

GAN æ˜¯ç”± Ian Goodfellow ç­‰äººåœ¨2014å¹´æå‡ºçš„ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ä¸¤ä¸ªç¥ç»ç½‘ç»œä¹‹é—´çš„â€œå¯¹æŠ—â€è®­ç»ƒï¼Œç”Ÿæˆé€¼çœŸçš„æ•°æ®æ ·æœ¬ã€‚

GAN é‡Œæœ‰ä¸¤ä¸ªè§’è‰²ï¼š

* **ç”Ÿæˆå™¨ï¼ˆGeneratorï¼ŒGï¼‰**ï¼šè´Ÿè´£ä»éšæœºå™ªå£°ç”Ÿæˆâ€œå‡æ•°æ®â€ï¼Œç›®çš„æ˜¯â€œéª—è¿‡â€åˆ¤åˆ«å™¨ã€‚

* **åˆ¤åˆ«å™¨ï¼ˆDiscriminatorï¼ŒDï¼‰**ï¼šè´Ÿè´£åˆ¤æ–­è¾“å…¥æ˜¯çœŸå®æ•°æ®è¿˜æ˜¯ç”Ÿæˆå™¨é€ å‡ºæ¥çš„å‡æ•°æ®ã€‚

è¿™ä¸¤ä¸ªç½‘ç»œäº’ç›¸å¯¹æŠ—ï¼Œåˆ¤åˆ«å™¨åŠ›æ±‚è¯†åˆ«çœŸå‡æ ·æœ¬ï¼Œç”Ÿæˆå™¨åŠ›æ±‚ç”Ÿæˆæ›´é€¼çœŸçš„æ ·æœ¬â€œéª—è¿‡â€åˆ¤åˆ«å™¨ã€‚ GAN æ˜¯ä¸€ä¸ªæå¤§æå°æ¸¸æˆï¼Œç›®æ ‡å‡½æ•°æ˜¯ï¼š

$$
\min_G \max_D V(D,G) = \mathbb{E}_{x \sim p_{r}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log (1 - D(G(z)))]
$$

è§£é‡Šï¼š

* $p_r$ï¼šçœŸå®æ•°æ®åˆ†å¸ƒ

* $p_z$ï¼šéšæœºå™ªå£°åˆ†å¸ƒï¼ˆé€šå¸¸æ˜¯å‡åŒ€æˆ–é«˜æ–¯ï¼‰

* $D(x)$ï¼šåˆ¤åˆ«å™¨ç»™è¾“å…¥ $x$ æ˜¯çœŸå®æ•°æ®çš„æ¦‚ç‡

* $G(z)$ï¼šç”Ÿæˆå™¨å°†å™ªå£° $z$ è½¬æ¢æˆæ ·æœ¬

åˆ¤åˆ«å™¨æƒ³æœ€å¤§åŒ–è¯†åˆ«çœŸå‡çš„æ¦‚ç‡ï¼Œç”Ÿæˆå™¨æƒ³æœ€å°åŒ–åˆ¤åˆ«å™¨è¯†åˆ«ç”Ÿæˆæ ·æœ¬ä¸ºå‡çš„æ¦‚ç‡ã€‚

---

**ä¼ ç»ŸGANè®­ç»ƒçš„å®Œæ•´æµç¨‹**:

1. **åˆå§‹åŒ–**ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ç½‘ç»œå‚æ•°ã€‚

2. **è®­ç»ƒåˆ¤åˆ«å™¨**

   * é‡‡æ ·ä¸€æ‰¹çœŸå®æ ·æœ¬ $x \sim p_r$ã€‚
   
   * é‡‡æ ·ä¸€æ‰¹å™ªå£° $z \sim p_z$ï¼Œç”Ÿæˆå‡æ ·æœ¬ $G(z)$ã€‚

   * è®¡ç®—åˆ¤åˆ«å™¨æŸå¤±ï¼š

     $$
     L_D = -\left(\mathbb{E}_{x \sim p_r}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log (1 - D(G(z)))]\right)
     $$

   * ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°åˆ¤åˆ«å™¨å‚æ•°ï¼Œå¢å¼ºå®ƒåŒºåˆ†çœŸå‡æ ·æœ¬çš„èƒ½åŠ›ã€‚

3. **è®­ç»ƒç”Ÿæˆå™¨**

   * å†é‡‡æ ·ä¸€æ‰¹å™ªå£° $z \sim p_z$ï¼Œç”Ÿæˆå‡æ ·æœ¬ $G(z)$ã€‚

   * è®¡ç®—ç”Ÿæˆå™¨æŸå¤±ï¼š

     $$
     L_G = -\mathbb{E}_{z \sim p_z}[\log D(G(z))]
     $$

     è¿™é‡Œç”Ÿæˆå™¨çš„ç›®æ ‡æ˜¯è®©åˆ¤åˆ«å™¨è®¤ä¸ºç”Ÿæˆæ ·æœ¬æ˜¯çœŸçš„ï¼ˆè¾“å‡ºæ¦‚ç‡é«˜ï¼‰ã€‚

   * ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°ç”Ÿæˆå™¨å‚æ•°ï¼Œä½¿ç”Ÿæˆæ ·æœ¬æ›´é€¼çœŸã€‚

4. **é‡å¤æ­¥éª¤2å’Œ3**ï¼Œäº¤æ›¿è®­ç»ƒåˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨ï¼Œç›´åˆ°ç”Ÿæˆå™¨èƒ½å¤Ÿç”Ÿæˆçœ‹èµ·æ¥å¾ˆçœŸå®çš„æ•°æ®ã€‚

---

**ä»£ç å®ç°**:

1. å¯¼åŒ… + å‚æ•°å‡†å¤‡

```python
import argparse
import os
import numpy as np
import torchvision.transforms as transforms
from torchvision.utils import save_image
from torchvision import datasets
from torch.autograd import Variable
import torch.nn as nn
import torch

os.makedirs("images", exist_ok=True)

parser = argparse.ArgumentParser()
parser.add_argument("--n_epochs", type=int, default=200, help="number of epochs of training")
parser.add_argument("--batch_size", type=int, default=64, help="size of the batches")
parser.add_argument("--lr", type=float, default=0.0002, help="adam: learning rate")
parser.add_argument("--b1", type=float, default=0.5, help="adam: decay of first order momentum of gradient")
parser.add_argument("--b2", type=float, default=0.999, help="adam: decay of first order momentum of gradient")
parser.add_argument("--n_cpu", type=int, default=8, help="number of cpu threads to use during batch generation")
parser.add_argument("--latent_dim", type=int, default=100, help="dimensionality of the latent space")
parser.add_argument("--img_size", type=int, default=28, help="size of each image dimension")
parser.add_argument("--channels", type=int, default=1, help="number of image channels")
parser.add_argument("--sample_interval", type=int, default=400, help="interval betwen image samples")
opt = parser.parse_args()
print(opt)

img_shape = (opt.channels, opt.img_size, opt.img_size)
cuda = True if torch.cuda.is_available() else False
```

2. ç”Ÿæˆå™¨ä»£ç å®ç°

```python
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()

        def block(in_feat, out_feat, normalize=True):
            layers = [nn.Linear(in_feat, out_feat)]
            if normalize:
                layers.append(nn.BatchNorm1d(out_feat, 0.8))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers

        self.model = nn.Sequential(
            *block(opt.latent_dim, 128, normalize=False),
            *block(128, 256),
            *block(256, 512),
            *block(512, 1024),
            # np.prod æ˜¯ NumPy é‡Œçš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨æ¥è®¡ç®—ä¸€ä¸ªæ•°ç»„æˆ–å…ƒç»„ æ‰€æœ‰å…ƒç´ çš„ä¹˜ç§¯
            nn.Linear(1024, int(np.prod(img_shape))),
            nn.Tanh()
        )

    def forward(self, z):
        img = self.model(z)
        img = img.view(img.size(0), *img_shape)
        return img
```

3. åˆ¤åˆ«å™¨ä»£ç å®ç°

```python
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()

        self.model = nn.Sequential(
            # np.prod æ˜¯ NumPy é‡Œçš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨æ¥è®¡ç®—ä¸€ä¸ªæ•°ç»„æˆ–å…ƒç»„ æ‰€æœ‰å…ƒç´ çš„ä¹˜ç§¯
            nn.Linear(int(np.prod(img_shape)), 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1),
            nn.Sigmoid(),
        )

    def forward(self, img):
        img_flat = img.view(img.size(0), -1)
        validity = self.model(img_flat)

        return validity
```

4. æ•°æ®ï¼Œæ¨¡å‹ï¼Œä¼˜åŒ–å™¨å‡†å¤‡

```python
# Loss function
adversarial_loss = torch.nn.BCELoss()

# Initialize generator and discriminator
generator = Generator()
discriminator = Discriminator()

if cuda:
    generator.cuda()
    discriminator.cuda()
    adversarial_loss.cuda()

# Configure data loader
os.makedirs("./data/mnist", exist_ok=True)
dataloader = torch.utils.data.DataLoader(
    datasets.MNIST(
        "./data/mnist",
        train=True,
        download=True,
        transform=transforms.Compose(
            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]
        ),
    ),
    batch_size=opt.batch_size,
    shuffle=True,
)

# Optimizers
optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))
optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))

Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor
```

5. è®­ç»ƒ

```python
# ----------
#  Training
# ----------

for epoch in range(opt.n_epochs):
    for i, (imgs, _) in enumerate(dataloader):

        # Adversarial ground truths
        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)
        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)

        # Configure input
        real_imgs = Variable(imgs.type(Tensor))

        # -----------------
        #  Train Generator
        # -----------------

        optimizer_G.zero_grad()

        # Sample noise as generator input
        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))

        # Generate a batch of images
        gen_imgs = generator(z)

        # Loss measures generator's ability to fool the discriminator
        g_loss = adversarial_loss(discriminator(gen_imgs), valid)

        g_loss.backward()
        optimizer_G.step()

        # ---------------------
        #  Train Discriminator
        # ---------------------

        optimizer_D.zero_grad()

        # Measure discriminator's ability to classify real from generated samples
        real_loss = adversarial_loss(discriminator(real_imgs), valid)
        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)
        d_loss = (real_loss + fake_loss) / 2

        d_loss.backward()
        optimizer_D.step()

        print(
            "[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]"
            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())
        )

        batches_done = epoch * len(dataloader) + i
        if batches_done % opt.sample_interval == 0:
            save_image(gen_imgs.data[:25], "images/%d.png" % batches_done, nrow=5, normalize=True)
```

6. æ•ˆæœ

![](GAN/1.png)

## æ¨èèµ„æ–™

![äº’æ€¼çš„è‰ºæœ¯ï¼šä»é›¶ç›´è¾¾WGAN-GP](https://kexue.fm/archives/4439)

![èƒ½é‡è§†è§’ä¸‹çš„GANæ¨¡å‹ï¼ˆä¸€ï¼‰ï¼šGANï¼â€œæŒ–å‘â€ï¼‹â€œè·³å‘â€](https://kexue.fm/archives/6316#%E6%8A%8AGAN%E5%86%99%E4%B8%8B%E6%9D%A5)

![WGANçš„æˆåŠŸï¼Œå¯èƒ½è·ŸWassersteinè·ç¦»æ²¡å•¥å…³ç³»](https://kexue.fm/archives/8244)

