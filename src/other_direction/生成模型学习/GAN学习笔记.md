---
title: 生成对抗网络 (GAN) 学习笔记
icon: file
category:
  - 生成模型
tag:
  - 生成模型
  - 编辑中
footer: 技术共建，知识共享
date: 2025-08-09
author:
  - BinaryOracle
---

`生成对抗网络 (GAN) 学习笔记` 

<!-- more -->

## 前置知识

### 最大似然估计（Maximum Likelihood Estimation, MLE）

最大似然估计是一种**利用观测数据反向推断模型参数的方法**。直观地说，它假设数据生成过程已知（概率模型已定），但参数未知，我们通过已观测到的样本情况去“**猜测**”哪个参数值最合理。具体地，MLE 选取使得观测到的数据出现的概率（即**似然**）最大的参数值为估计结果。

例如，如果我们抛硬币100次均出现正面，我们很自然地认为这枚硬币不是公平的，而最可能的是两面都为正面。这种根据实验结果推断最有可能的硬币属性，就是最大似然估计的直观思想。同样地，若10次抛硬币出现6次正面，我们倾向于估计硬币出现正面的概率为0.6，因为这个假设使得产生“6次正面”的可能性最大。更形式化地说，对于参数 $\theta$ 的任意假设，定义**似然函数** $L(\theta)=P(\text{数据}\mid\theta)$，MLE 就是取使 $L(\theta)$ 最大的 $\theta$。**在统计学中，这个最大点称为参数的最大似然估计**。

#### 数学推导

设样本数据为 $X_1,X_2,\dots,X_n$，假定其独立同分布于参数为 $\theta$ 的某个概率分布。**似然函数**定义为在参数 $\theta$ 给定下观测到该样本的联合概率或概率密度：

$$
L(\theta\mid X_1,\dots,X_n)=P(X_1,\dots,X_n\mid \theta)\,.
$$

若各观测独立，则

$$
L(\theta\mid \mathbf{X})=\prod_{i=1}^n f(X_i;\theta)\,,
$$

其中 $f(x;\theta)$ 是单个样本的概率质量（或密度）函数。最大似然估计就是求解优化问题

$$
\hat\theta_{\text{MLE}}=\underset{\theta}{\arg\max}\;L(\theta\mid \mathbf{X})\,,
$$

即寻找使似然函数取得最大值的参数。为了简化计算，通常取对数得到**对数似然** $\ell(\theta)=\ln L(\theta)$，由于对数是单调递增的，$\ell(\theta)$ 的最大值处即对应 $L(\theta)$ 的最大值。对数似然展开为

$$
\ell(\theta)=\sum_{i=1}^n\ln f(X_i;\theta)\,.
$$

然后对 $\ell(\theta)$ 关于各参数求偏导并令其为零，即可得到极大似然估计的方程（似然方程），进而求出 $\hat\theta$。

下面给出几个示例：

* **离散分布（掷硬币）**：设每次抛硬币结果 $X_i\in{0,1}$（1 表示正面），且独立同分布，$P(X_i=1)=p$。若 $n$ 次抛掷中出现 $k$ 次正面，则似然函数为 $L(p)=p^k(1-p)^{n-k}$。取对数得 $\ell(p)=k\ln p+(n-k)\ln(1-p)$，对 $p$ 求导后令导数为零：$\frac{k}{p}-\frac{n-k}{1-p}=0$，解得极大似然估计 $\hat p=k/n$。也就是说，硬币正面概率的 MLE 等于正面次数占总次数的比例。
* **连续分布（正态分布）**：假设 $X_1,\dots,X_n$ 独立同分布于 $N(\mu,\sigma^2)$，似然函数为

  $$
  L(\mu,\sigma^2)\propto(\sigma^2)^{-n/2}\exp\Big(-\frac{1}{2\sigma^2}\sum_{i=1}^n(X_i-\mu)^2\Big).
  $$

  对 $\mu,\sigma^2$ 求导并令偏导数为零可知，MLE 解为样本均值和（无偏调整前的）样本方差：$\hat\mu=\frac{1}{n}\sum_i X_i,\ \hat\sigma^2=\frac{1}{n}\sum_i (X_i-\hat\mu)^2$。换言之，在正态分布假设下，MLE 给出了直观的样本统计量作为参数估计。

### 信息论: 信息量，熵，交叉熵，KL散度

#### 1. **信息量（Self-Information）**

信息论最核心的问题是：一个事件的发生给我们带来多少“惊讶”或“新信息”？

* 公式：

$$
I(x) = -\log p(x)
$$

这里 $p(x)$ 是事件 $x$ 发生的概率。

* 概率越小 → 事件越罕见 → 信息量越大。

* 对数底的选择：

  * $\log_2$：信息单位是 **bit**（二进制比特）

  * $\ln$：信息单位是 **nat**（自然对数）

**例子**：

* 抛硬币得到正面（$p=0.5$）：信息量 = $-\log_2 0.5 = 1$ bit

* 抛硬币连续两次都正面（$p=0.25$）：信息量 = $-\log_2 0.25 = 2$ bit

---

#### 2. **熵（Entropy）**

熵是**平均信息量**，用来衡量一个概率分布的不确定性。

公式（离散分布）：

$$
H(P) = -\sum_x P(x) \log P(x)
$$

* 熵越大，分布越“混乱”或不确定。

* 如果事件概率全一样（均匀分布），熵最大。

* 如果一个事件概率是 1（确定事件），熵为 0（没有不确定性）。

**例子**：

* 公平硬币：$H=1$ bit

* 偏置硬币（正面 0.9）：$H \approx 0.47$ bit

---

#### 3. **交叉熵（Cross-Entropy）**

交叉熵衡量**用分布 Q 去编码真实来自 P 的事件时的平均信息量**。

公式：

$$
H(P, Q) = -\sum_x P(x) \log Q(x)
$$

解释：

* 如果 $P=Q$，交叉熵 = 熵，编码是最优的。

* 如果 $Q$ 偏离 $P$，交叉熵会大于熵（编码变长）。

---

#### 4. **KL 散度（信息增益）**

KL 散度就是**交叉熵 - 熵**：

$$
D_{\mathrm{KL}}(P \parallel Q) = H(P, Q) - H(P)
$$

> 交叉熵可以看作是平均信息量 加上因为近似不准而多花的那部分成本，而那部分成本就是 KL 散度。

也可以写成：

$$
D_{\mathrm{KL}}(P \parallel Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
$$

含义：

* 它表示：如果你用 Q 来近似 P，平均每个样本要多花多少信息量（比特 / nat）。

* 它总是 $\geq 0$（**Gibbs 不等式**），且当 $P=Q$ 时为 0。

---

✅ **小结关系图**：

```
信息量 I(x)  →  熵 H(P) = 平均 I(x)  
              ↘  交叉熵 H(P, Q) = 平均 -log Q(x)
               ↘ KL散度 = H(P, Q) - H(P)

注: 平均是指求期望               
```

### 交叉熵损失（Cross-Entropy Loss）

交叉熵损失是一种衡量两个概率分布差异的指标，常用在分类任务中，尤其是二分类和多分类问题。它用来衡量模型预测的概率分布 $\hat{y}$ 和真实标签分布 $y$ 之间的距离。

> **二分类交叉熵:**

对于标签 $y \in \{0,1\}$，预测概率 $p = \hat{y} = P(y=1)$，交叉熵定义为：

$$
L = -[y \log p + (1 - y) \log (1 - p)]
$$

解释：

* 如果真实是正类（$y=1$），损失就是 $-\log p$，预测越接近1损失越小。

* 如果真实是负类（$y=0$），损失就是 $-\log (1-p)$，预测越接近0损失越小。

---

> **多分类交叉熵(对单个样本):**

设真实标签是 one-hot 向量 $y = (y_1, y_2, ..., y_C)$，预测概率分布是 $\hat{y} = (\hat{y}_1, \hat{y}_2, ..., \hat{y}_C)$，其中 $C$ 是类别数：

$$
L = - \sum_{i=1}^C y_i \log \hat{y}_i
$$

即只对真实类别对应的概率取负对数。

###  JS散度（Jensen-Shannon divergence）

#### 1. JS散度是什么？（浅层直观）

* JS散度是衡量两个概率分布差异的一个方法。

* 它是 **KL散度的“对称改进版”**，所以它总是非负且有限，且满足对称性：

  $$
  \mathrm{JS}(P \parallel Q) = \mathrm{JS}(Q \parallel P)
  $$

* 简单说，就是告诉你 $P$ 和 $Q$ 两个分布相差多远。

---

#### 2. 为什么要用JS散度而不是KL散度？

* **KL散度**不对称：

  $$
  \mathrm{KL}(P \parallel Q) \neq \mathrm{KL}(Q \parallel P)
  $$

* KL散度有时会无穷大（如果 $Q$ 在 $P$ 支持的区域为0，会导致 $\log 0$ 不存在）。

* **JS散度**解决了这些问题，变得对称且有界（最大值是 $\log 2$）。

---

#### 3. JS散度的数学定义

给两个概率分布 $P$ 和 $Q$，定义它们的平均分布：

$$
M = \frac{1}{2}(P + Q)
$$

JS散度定义为：

$$
\mathrm{JS}(P \parallel Q) = \frac{1}{2} \mathrm{KL}(P \parallel M) + \frac{1}{2} \mathrm{KL}(Q \parallel M)
$$

其中，$\mathrm{KL}(\cdot \parallel \cdot)$ 是KL散度。

---

#### 4. 直观理解JS散度

* $M$ 是 $P$ 和 $Q$ 的“中间”分布。

* 你先测量 $P$ 与 $M$ 的差异（KL散度），再测量 $Q$ 与 $M$ 的差异，取平均。

* 如果 $P$ 和 $Q$ 很接近，那么它们都和 $M$ 很接近，JS散度小。

* 如果很不一样，JS散度就大。


> 想象有两个人分别站在一条直线上的不同点，𝑃 和 Q 就是两个人的位置，M 是他们的中间点。你测量两个人到中间点的距离，取平均。这个平均距离越大，说明两个人相距越远。这样就公平、对称地反映了两人距离，而不是单方面去看某一个人的位置。

---

#### 5. 举个简单例子

假设：

* $P = (1, 0)$，

* $Q = (0, 1)$（两个完全不同的分布，互斥事件）。

那么：

* $M = (0.5, 0.5)$

* $\mathrm{KL}(P \parallel M) = 1 \times \log \frac{1}{0.5} + 0 \times \log \frac{0}{0.5} = \log 2$

* $\mathrm{KL}(Q \parallel M) = \log 2$

* 所以 $\mathrm{JS}(P \parallel Q) = \frac{1}{2} \log 2 + \frac{1}{2} \log 2 = \log 2$

这是最大值，表示两分布差异最大。

### 1-Lipschitz 函数

一个函数 $f: \mathbb{R}^n \to \mathbb{R}$ 如果存在一个常数 $K$（叫 **Lipschitz 常数**），使得：

$$
|f(x_1) - f(x_2)| \le K \cdot \|x_1 - x_2\|
$$

对任意 $x_1, x_2$ 都成立，那么 $f$ 叫做 **K-Lipschitz 函数**。

* **K 越小**：函数越平滑，变化越慢。

* **K 越大**：函数变化可能很陡，但仍有限制。

---

当 $K = 1$ 时：

$$
|f(x_1) - f(x_2)| \le \|x_1 - x_2\|
$$

这意味着：

* 输入变化多少，输出的变化量最多等于输入变化量。

* **相当于限制了函数的最大“斜率”是 1**。

想象你走山路：

* 如果是 1-Lipschitz，走 1 米水平路，海拔最多变 1 米。

* 如果是 2-Lipschitz，走 1 米水平路，海拔可能变 2 米，更陡。

这个约束能防止函数的变化太快，让它比较“温和”。

## 原始 GAN


## WGAN
