---
title: 证据深度学习概念和完整推导过程
icon: file
category:
  - 概率论
tag:
  - 已发布
footer: 技术共建，知识共享
date: 2025-09-29
order: 3
author:
  - BinaryOracle
---

`证据深度学习概念和完整推导过程` 

<!-- more -->

## 前置知识

### 主观逻辑 (Subjective Logic)

**主观逻辑 (Subjective Logic)** 是一种**概率逻辑**，它明确地将**认知不确定性**和**信息源信任度**纳入考量。简单来说，它是一种用于在**不确定、信息不完整或信息源不可靠**的情况下进行推理和决策的数学框架。

> **主观意见**（Subjective Opinion）

主观逻辑的核心是**主观意见**（Subjective Opinion），它取代了传统概率论中的单一概率值。一个主观意见 $\omega$ 不仅表达了对某个命题（例如：“命题 $x$ 为真”）的**信念程度**，还表达了关于这个信念本身的**不确定程度**。

对于一个二元命题（如：是/否），一个主观意见 $\omega_x$ 通常由四个分量组成：

1.  **信念值 (Belief Mass, $b_x$)**：相信命题 $x$ 为真的程度。

2.  **不信值 (Disbelief Mass, $d_x$)**：相信命题 $x$ 为假的程度。

3.  **不确定性 (Uncertainty Mass, $u_x$)**：由于缺乏证据或证据不完整而导致的不确定程度。

4.  **先验概率/基准率 (Base Rate, $a_x$)**：在没有任何证据的情况下，命题 $x$ 预期的概率。

这三个信念分量之和必须满足：

$$b_x + d_x + u_x = 1$$

> **主观意见与概率的关系**

主观意见可以看作是对传统概率的推广。通过将不确定性 $u_x$ 明确地表示出来，主观逻辑能够区分“**确定的概率**”（$u_x=0$）和“**不确定的概率**”（$u_x>0$）。

可以根据主观意见计算出对应的**期望概率 (Expectation Probability)** $P(x)$：

$$P(x) = b_x + a_x u_x$$

这个期望概率是在考虑了信念、不信和不确定性后，对命题 $x$ 为真的最佳点估计。

> 主要特点与优势

1. 明确建模不确定性 (Explicitly Models Uncertainty)

主观逻辑的核心优势在于它能够**量化和保留不确定性**。在传统的概率论中，如果不确定性很高，我们可能仍然被迫给出一个 $P(x)=0.5$ 的概率值，这看起来和掷硬币的确定概率一样。而在主观逻辑中，一个完全不确定的情况会表示为 $b_x=0, d_x=0, u_x=1$，这与 $b_x=0.5, d_x=0.5, u_x=0$（掷硬币）明显不同，从而避免了“**无知与均等信念**”之间的混淆。

2. 与二元逻辑和概率论兼容 (Compatibility)

主观逻辑的运算符（如合取、析取、条件演绎等）是对传统**布尔逻辑**（当 $u_x=0$ 且 $b_x, d_x$ 只有 0 或 1 时）和**概率演算**（当 $u_x=0$ 时）的**泛化**。

3. 处理信任和信息来源 (Source Trust and Evidence)

主观逻辑特别适用于建模**信任网络**。它允许在聚合来自不同来源的信息时，考虑每个信息来源的**信任度**，通过“**折算 (Discounting)**”等操作来调整意见。

### Dempster-Shafer 证据理论 (DST)

**Dempster-Shafer 证据理论 (DST, Dempster-Shafer Theory of Evidence)**，又称 **信度理论 (Theory of Belief Functions)**，是一种用于 **不确定性推理** 的数学框架。

它最早由 **Arthur P. Dempster** 在1967年提出，后由 **Glenn Shafer** 在1976年的著作《A Mathematical Theory of Evidence》中系统化。

DST 的核心思想是：

* 在处理 **不完整、模糊或冲突性证据** 时，直接用 **概率论** 往往过于严格或困难。

* DST 提供了一种更宽松的表示方式，它不要求为所有事件精确给出概率，而是允许以 **信度 (belief)** 和 **似然度 (plausibility)** 的区间来描述。

DST 的关键组成部分包括：

* **识别框架 (Frame of Discernment, Θ)**：所有可能结果的集合。

* **基本概率分配 (Basic Probability Assignment, BPA 或 Mass function, m)**：把一定的置信度分配给 Θ 的子集，而不是单个事件。

* **信度函数 (Belief, Bel)**：表示对某个命题的最小可信度。

* **似然度函数 (Plausibility, Pl)**：表示对某个命题的最大可能可信度。

关系：

$$
Bel(A) \leq P(A) \leq Pl(A), \quad \forall A \subseteq \Theta
$$

这意味着，概率 $P(A)$ 必然落在信度和似然度之间。

> DST 的机制可以分为三部分：

1. 基本概率分配 (BPA)

- 给定识别框架 Θ（比如 Θ = {猫, 狗, 兔子}），我们把置信度分配到子集上：
   
    - m({猫}) = 0.3
   
    - m({狗}) = 0.2
   
    - m({猫,狗}) = 0.4 （表示“可能是猫也可能是狗”）
   
    - m(Θ) = 0.1 （表示“完全不确定”）

- 要求：
    
    $$
    m(\emptyset) = 0, \quad \sum_{A \subseteq \Theta} m(A) = 1
    $$
    
2. 信度与似然度

- 信度 Bel(A)：所有 **完全支持 A 的子集** 的 BPA 之和。

- 似然度 Pl(A)：所有 **不与 A 矛盾的子集** 的 BPA 之和。

    $$ 
    Bel(A) = \sum_{B \subseteq A} m(B), \quad Pl(A) = \sum_{B \cap A \neq \emptyset} m(B)
    $$

3. Dempster 合成规则

- 用于将来自 **不同信息源** 的 BPA 进行合并：
    
    $$
    m_{12}(C) = \frac{1}{1-K} \sum_{A \cap B = C} m_1(A) \cdot m_2(B)
    $$
    
    其中 $K = \sum_{A \cap B = \emptyset} m_1(A) \cdot m_2(B)$ 表示冲突度。
    
- 意义：如果两个证据源冲突较大（K接近1），合成后的结果会受到显著影响。

#### Dempster 合成规则

我们想判断一个目标是不是 **猫(Cat)** 还是 **狗(Dog)**。识别框架：$\Theta = {\text{Cat}, \text{Dog}}$。

现在有两个证据源：

* **证据源1（传感器1）**：比较确信是猫

  * $m_1({Cat}) = 0.6$

  * $m_1({Dog}) = 0.3$
  
  * $m_1(\Theta) = 0.1$（表示不确定，是猫还是狗）

* **证据源2（传感器2）**：更偏向狗

  * $m_2({Cat}) = 0.2$
  
  * $m_2({Dog}) = 0.7$
  
  * $m_2(\Theta) = 0.1$

> 合成步骤

1. 两两相乘，分类到“交集”

我们把每个集合 (A)（来自证据1）和 (B)（来自证据2）组合，看它们交集是什么：

| 来自证据1 | 来自证据2 | 交集    | 乘积                   |
| ----- | ----- | ----- | -------------------- |
| `{Cat}` | `{Cat}` | `{Cat}` | 0.6 × 0.2 = **0.12** |
| `{Cat}` | `{Dog}` | ∅（冲突） | 0.6 × 0.7 = **0.42** |
| `{Cat}` | Θ     | `{Cat}` | 0.6 × 0.1 = **0.06** |
| `{Dog}` | `{Cat}` | ∅（冲突） | 0.3 × 0.2 = **0.06** |
| `{Dog}` | `{Dog}` | `{Dog}` | 0.3 × 0.7 = **0.21** |
| `{Dog}` | Θ     | `{Dog}` | 0.3 × 0.1 = **0.03** |
| Θ     | `{Cat}` | `{Cat}` | 0.1 × 0.2 = **0.02** |
| Θ     | `{Dog}` | `{Dog}` | 0.1 × 0.7 = **0.07** |
| Θ     | Θ     | Θ     | 0.1 × 0.1 = **0.01** |

--- 

2. 整理结果

* 分给 **{Cat}** 的：0.12 + 0.06 + 0.02 = **0.20**

* 分给 **{Dog}** 的：0.21 + 0.03 + 0.07 = **0.31**

* 分给 **Θ** 的：0.01

* **冲突 K** = 0.42 + 0.06 = **0.48**

--- 

3. 归一化（去掉冲突部分）

冲突比例 $K = 0.48$，所以归一化因子 $(1 - K) = 0.52$。

把前面非冲突部分都除以 0.52：

* $m_{12}({Cat}) = 0.20 / 0.52 ≈ 0.385$

* $m_{12}({Dog}) = 0.31 / 0.52 ≈ 0.596$

* $m_{12}(\Theta) = 0.01 / 0.52 ≈ 0.019$

---

4. 最终结果

* 猫的支持度 ≈ 38.5%

* 狗的支持度 ≈ 59.6%

* 仍然有一点点不确定 ≈ 1.9%

> 理解

* 虽然第一个传感器更偏向猫（0.6），但第二个传感器更强烈支持狗（0.7），合成后系统整体更倾向于“狗”。

* 中间的大冲突（0.48）被“抹掉”，剩余证据按比例分给了相容的结果。

##### 为什么抹除冲突部分 ？

在合成两个证据时，如果一个证据支持集合 (A)，另一个证据支持集合 (B)，而 **(A \cap B = \varnothing)**，就出现了冲突。

例子：

* 证据1：90% 认为是 **猫**

* 证据2：90% 认为是 **狗**

因为“猫”和“狗”不可能同时为真，所以这部分乘积就是**冲突度 K**；Dempster–Shafer 理论想要的是 **“一致证据的合力”**。它假设：

* 证据源是 **相互独立** 的，冲突是由于“偶然噪声、有限性信息”等原因产生的，因此，冲突不代表“第三种情况”，而应该**被忽略**。

所以：

* 他把所有相容证据累加，把冲突度 (K) “丢掉”，然后把剩下的相容质量归一化到 1。

数学上，就是：

$$
m_{12}(C) = \frac{\sum_{A\cap B=C} m_1(A)m_2(B)}{1-K}
$$

可以这样理解：

* 只保留“有可能共存的证据”。

* 把冲突部分看成“垃圾数据”，不允许它稀释相容证据。

* 最后归一化，是为了保持 **总和 = 1** 的概率质量框架。

所以，**“抹除冲突部分”其实是 DST 的设计选择**：认为冲突只是无效信息，不必在结果里占比。这种做法在冲突较小时是合理的，但如果冲突很大（(K) → 1）：

* 归一化会把很少的一点相容证据“放大”，导致结果可能严重偏向某一方，出现“反直觉”现象。

* 因此学界提出了替代方法：

  * **Yager 规则**：不抹掉冲突，而是把冲突质量分配到全集 $\Theta$，表示“完全不确定”。

  * **Dubois–Prade 规则**：把冲突分配到交集为空的“并集”。

  * **PCR（Proportional Conflict Redistribution）**：把冲突按比例分配回原始冲突的集合。