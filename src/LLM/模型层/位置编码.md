---
title: ä½ç½®ç¼–ç 
icon: file
category:
  - ä½ç½®ç¼–ç 
tag:
  - å·²å‘å¸ƒ
footer: æŠ€æœ¯å…±å»ºï¼ŒçŸ¥è¯†å…±äº«
date: 2025-08-13
author:
  - BinaryOracle
---

`ä½ç½®ç¼–ç ` 

<!-- more -->

## ç»å¯¹ä½ç½®ç¼–ç ï¼ˆAbsolute Positional Encoding, APEï¼‰

### æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç ï¼ˆSinusoidal Positional Encodingï¼‰

æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç ï¼ˆSinusoidal Positional Encodingï¼‰æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„ä½ç½®ç¼–ç æ–¹æ³•ï¼Œå®ƒé€šè¿‡å›ºå®šçš„å‘¨æœŸæ€§å‡½æ•°ï¼ˆæ­£å¼¦å’Œä½™å¼¦ï¼‰æ¥ä¸ºåºåˆ—çš„ä¸åŒä½ç½®æä¾›å”¯ä¸€çš„ç¼–ç ã€‚å¯¹äºæ¯ä¸ªä½ç½® $ğ‘–$ å’Œæ¯ä¸ªç»´åº¦ $ğ‘‘$ï¼Œä½ç½®ç¼–ç é€šè¿‡ä»¥ä¸‹å…¬å¼è®¡ç®—ï¼š

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

* $ğ‘ğ‘œğ‘ $ è¡¨ç¤ºä½ç½®ç´¢å¼•ï¼Œè¡¨ç¤ºè®¡ç®—å“ªä¸ªä½ç½®çš„ç¼–ç 

* $ğ‘–$ è¡¨ç¤ºç¼–ç ç»´åº¦ï¼Œ$ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™$ æ˜¯ç¼–ç ç©ºé—´çš„æ€»ç»´åº¦

* $PE_{(pos, 2i)}$ å’Œ $PE_{(pos, 2i+1)}$ é€šè¿‡æ­£å¼¦å’Œä½™å¼¦å‡½æ•°åˆ†è£‚æ˜ å°„åˆ°å¶æ•°å’Œå¥‡æ•°çš„ç»´åº¦

å‡è®¾ï¼šæˆ‘ä»¬è¦è®¡ç®—è¾“å…¥åºåˆ—ç¬¬ 2 ä¸ªä½ç½® Token å¯¹åº”çš„ä½ç½®ç¼–ç ï¼Œç¼–ç çš„ç»´åº¦è®¾å®šä¸º 4 ï¼Œåˆ™ï¼š

![](ä½ç½®ç¼–ç /1.png)

æœ€ç»ˆï¼Œä½ç½® 2 çš„ç¼–ç å‘é‡ä¸ºï¼š$(ğ‘ ğ‘–ğ‘›(2),ğ‘ğ‘œğ‘ (2),ğ‘ ğ‘–ğ‘›(0.02),ğ‘ğ‘œğ‘ (0.02))$ï¼Œæˆ‘ä»¬æŠŠå®ƒåŠ åˆ°ç¬¬äºŒä¸ª Token çš„è¯åµŒå…¥å‘é‡ä¸Šï¼Œå°±ç›¸å½“äºç»™å…¶æ³¨å…¥äº†é¡ºåºä¿¡æ¯ã€‚

è®¡ç®—èµ·æ¥æ˜¯æ¯”è¾ƒå®¹æ˜“çš„ï¼Œå¦‚ä½•å»ç†è§£è¿™ä¸ªä½ç½®ç¼–ç ï¼Ÿè¯·çœ‹ä¸‹é¢è¿™å¼ å›¾ï¼š

![](ä½ç½®ç¼–ç /2.png)

è¿™ä¸‰å¼ å›¾åˆ†åˆ«æ‰“å°äº† 128 ä¸ªä½ç½®å‘é‡ç¬¬ 2ã€6ã€12 ç»´åº¦çš„ç¼–ç å€¼çš„å˜åŒ–ï¼Œæˆ‘ä»¬å‘ç°è¿™äº›å€¼å‘ˆç°å‘¨æœŸæ€§çš„å˜åŒ–ã€‚å¦å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å‘ç°ï¼Œå‘é‡ç»´åº¦è¶Šé«˜ï¼Œå…¶å‘¨æœŸå°±è¶Šé•¿ã€‚

![](ä½ç½®ç¼–ç /3.png)

ä¸Šå›¾ï¼Œæˆ‘ä»¬æ‰“å°äº†ä½ç½® 1ã€5 çš„ç¼–ç å‘é‡ä¸­çš„ sin å’Œ cos è®¡ç®—å¾—åˆ°çš„ç¼–ç å€¼ã€‚æˆ‘ä»¬å¯ä»¥å‘ç°ï¼ŒåŸºäºæ­£å¼¦å’Œä½™å¼¦å‡½æ•°å¾—åˆ°çš„ä½ç½®ç¼–ç å¯ä»¥ä¿è¯å”¯ä¸€æ€§ã€‚å¦å¤–ï¼Œä¹Ÿå¯ä»¥çœ‹åˆ°ï¼Œå‘é‡çš„ç»´åº¦è¶Šé«˜ï¼Œç¼–ç å€¼çš„æ³¢åŠ¨å°±è¶Šå°ï¼Œå‘é‡å°±è¶Šæ¥è¿‘ã€‚

ç®€å•æ€»ç»“ä¸‹ï¼š

1. å› ä¸ºæ­£å¼¦å’Œä½™å¼¦å‡½æ•°éƒ½æ˜¯å‘¨æœŸå‡½æ•°ï¼Œç¼–ç åœ¨ä¸åŒç»´åº¦ä¸Šå…·æœ‰ä¸åŒçš„å‘¨æœŸæ€§

2. ä½ç½®ç¼–ç å‘é‡æ˜¯å”¯ä¸€çš„ï¼Œå› ä¸ºä¸åŒä½ç½®çš„ç¼–ç ç”±ä¸åŒçš„æ­£å¼¦å’Œä½™å¼¦å€¼ç»„æˆ

3. ä½ç»´åº¦çš„ç¼–ç å€¼æ³¢åŠ¨æ€§å¾ˆå¤§ï¼ˆå‘¨æœŸçŸ­ï¼‰ï¼Œé«˜çº¬åº¦çš„ç¼–ç å€¼æ³¢åŠ¨æ€§è¾ƒå°ï¼ˆå‘¨æœŸé•¿ï¼‰

æ‰€ä»¥ï¼Œå¯ä»¥å¾—åˆ°ä¸€ä¸ªç®€å•çš„ç»“è®ºï¼š

1. ä½ç»´åˆ†é‡ï¼ˆå° iï¼‰çš„å˜åŒ–è¾ƒå¿«ï¼Œä¸»è¦æ•æ‰å±€éƒ¨ä½ç½®å…³ç³»

2. é«˜ç»´åˆ†é‡ï¼ˆå¤§ iï¼‰çš„å˜åŒ–è¾ƒæ…¢ï¼Œå¯ä»¥ç”¨äºç¼–ç å…¨å±€ä¿¡æ¯

è¿™ä¸ªæ€ä¹ˆå»ç†è§£ï¼Ÿæˆ‘ä»¬æŠŠæŸä¸ªä½ç½®çš„å‘é‡å¤§æ¦‚åˆ’åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šä½ç»´å‘é‡éƒ¨åˆ† + é«˜ç»´å‘é‡éƒ¨åˆ†ï¼Œä½çº¬å‘é‡éƒ¨åˆ†æ•°å€¼æ³¢åŠ¨å¹…åº¦å¾ˆå¤§ï¼Œåœ¨ä¸€ä¸ªå‘¨æœŸå†…åªèƒ½åŒ…å«å°‘é‡ç›¸é‚»çš„ä½ç½®ï¼Œå¹¶ä¸”ä¸€å®šç¨‹åº¦ä¸Šä¹Ÿè¡¨è¾¾äº†ä½ç½®çš„å±€éƒ¨çš„ç›¸å¯¹ä¿¡æ¯ï¼Œè¿™å°±æ˜¯æ•æ‰å±€éƒ¨ä½ç½®å…³ç³»ã€‚é‚£ä¹ˆï¼Œå¯¹äºé«˜çº¬å‘é‡éƒ¨åˆ†è€Œè¨€ï¼Œå®ƒçš„æ³¢åŠ¨å¹…åº¦å¾ˆå°ï¼Œä¸€ä¸ªå‘¨æœŸèƒ½å¤ŸåŒ…å«æ›´å¤šçš„ä½ç½®ä¿¡æ¯ï¼Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬ç†è§£çš„ç¼–ç å…¨å±€ä½ç½®ä¿¡æ¯çš„å«ä¹‰ã€‚

æ‰€ä»¥ï¼Œå¯¹äºä¸€ä¸ªåŸºäºæ­£å¼¦ä½™å¼¦ç¼–ç çš„ä½ç½®å‘é‡ï¼Œå¯ä»¥ç†è§£ä¸ºè¯¥å‘é‡ä¸­éšå«äº†ä¸€äº›å±€éƒ¨å’Œå…¨å±€çš„ä½ç½®ä¿¡æ¯ã€‚ä½¿å¾— Transformer æ—¢èƒ½æ„ŸçŸ¥å±€éƒ¨ç›¸å¯¹ä½ç½®ï¼Œä¹Ÿèƒ½æ„ŸçŸ¥å…¨å±€ä½ç½®ä¿¡æ¯ï¼Œä»è€Œå¼¥è¡¥å…¶åŸç”Ÿç»“æ„ä¸­ç¼ºå°‘ä½ç½®æ„ŸçŸ¥èƒ½åŠ›çš„ç¼ºé™·ã€‚

å½“ç„¶ï¼Œè¿™ç§ä½ç½®ç¼–ç æ–¹æ³•ä¹Ÿå­˜åœ¨ä»¥ä¸‹ä¸€äº›ä¸è¶³ä¹‹å¤„ï¼š

éšç€åºåˆ—é•¿åº¦çš„å¢åŠ ï¼Œä½ç½®ç¼–ç çš„å‘¨æœŸæ€§å¯èƒ½å¯¼è‡´ä¸åŒä½ç½®ä¹‹é—´çš„åŒºåˆ†åº¦é€æ¸é™ä½ï¼Œéš¾ä»¥å‡†ç¡®è¡¨ç¤ºæé•¿åºåˆ—ä¸­å„ä¸ªä½ç½®çš„ç‹¬ç‰¹ä¿¡æ¯ã€‚

è™½ç„¶æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç èƒ½å¤Ÿéšå«åœ°è¡¨è¾¾ä¸€å®šçš„å±€éƒ¨ä½ç½®ä¿¡æ¯ï¼Œä½†ç”±äºå®ƒæ˜¯å›ºå®šçš„ã€ä¸å¯å­¦ä¹ çš„ï¼Œå¹¶æ²¡æœ‰ä¸“é—¨é’ˆå¯¹å±€éƒ¨ä¾èµ–å…³ç³»è¿›è¡Œä¼˜åŒ–ï¼Œå› æ­¤åœ¨å»ºæ¨¡å±€éƒ¨ä¾èµ–å…³ç³»æ—¶èƒ½åŠ›ç›¸å¯¹ä¸è¶³ã€‚

æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç æ˜¯ä¸€ç§åŸºäºä¸‰è§’å‡½æ•°çš„å›ºå®šç¼–ç æ–¹å¼ï¼Œå®ƒæ˜¯ä¸€ç§é™æ€çš„ä½ç½®ä¿¡æ¯è¡¨ç¤ºã€‚è€Œæ³¨æ„åŠ›æœºåˆ¶æ›´å…³æ³¨çš„æ˜¯æ–‡æœ¬ä¸­ä¸åŒä½ç½®ä¹‹é—´çš„åŠ¨æ€è¯­ä¹‰å…³è”ã€‚è¿™ä¸¤ç§ä¿¡æ¯åœ¨è¡¨ç¤ºå½¢å¼å’Œè¯­ä¹‰ä¾§é‡ç‚¹ä¸Šå­˜åœ¨å·®å¼‚ï¼Œå¯¼è‡´åœ¨èåˆæ—¶å¯èƒ½æ— æ³•å¾ˆå¥½åœ°ç›¸äº’è¡¥å……ã€‚

æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç é€šå¸¸æ˜¯é«˜ç»´å‘é‡ï¼Œå…¶è®¡ç®—é‡ä¼šéšç€ç»´åº¦çš„å¢åŠ ä¸ä»…éœ€è¦æ›´å¤šçš„è®¡ç®—æ—¶é—´ï¼Œè¿˜å¯èƒ½å ç”¨å¤§é‡çš„å†…å­˜ç©ºé—´ï¼Œå½±å“æ¨¡å‹çš„è¿è¡Œæ•ˆç‡ã€‚

> æ­£ä½™å¼¦ä½ç½®ç¼–ç é€šè¿‡åœ¨ä¸åŒç»´åº¦ä¸Šå¼•å…¥ä¸åŒæ³¢é•¿çš„æ­£ä½™å¼¦ä¿¡å·ï¼Œä½¿å¾—ä½ç»´å¯¹å±€éƒ¨ä½ç½®å˜åŒ–æ•æ„Ÿï¼Œé«˜ç»´å¯¹å…¨å±€ä½ç½®å˜åŒ–æ•æ„Ÿã€‚è™½ç„¶ä½ç½®å’Œè¯­ä¹‰åœ¨æ‰€æœ‰ç»´åº¦ä¸Šæ··åˆï¼Œä½†åœ¨è®­ç»ƒä¸­ï¼Œæ¨¡å‹å¯èƒ½å­¦åˆ°ä¸€ç§â€œä½ç»´æ›´å¤šä½ç½®ï¼Œé«˜ç»´æ›´å¤šè¯­ä¹‰â€çš„åˆ†å·¥æ¨¡å¼ã€‚

### åŸºäºå¯å­¦ä¹ çš„åµŒå…¥

å¯å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼ˆLearnable Positional Encoding, LPEï¼‰æ˜¯ä¸€ç§é€šè¿‡æ¢¯åº¦ä¸‹é™è‡ªåŠ¨å­¦ä¹ ä½ç½®ç¼–ç çš„æ–¹æ³•ï¼Œä¸åŒäºå›ºå®šç¼–ç ï¼ˆå¦‚æ­£å¼¦/ä½™å¼¦å‡½æ•°ç¼–ç ï¼‰ï¼Œå®ƒä¸ä¾èµ–ä»»ä½•æ‰‹å·¥è®¾è®¡çš„å…¬å¼ï¼Œè€Œæ˜¯ç›´æ¥è®©æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜åŒ–ä½ç½®ä¿¡æ¯ã€‚å¯¹äºä¸€ä¸ªé•¿åº¦ä¸º ğ¿ çš„è¾“å…¥åºåˆ—ï¼Œæ¯ä¸ªä½ç½® ğ‘– éƒ½å¯¹åº”ä¸€ä¸ªå¯å­¦ä¹ çš„å‘é‡ã€‚å½“è®­ç»ƒæˆ–æµ‹è¯•æ—¶ï¼Œå°†è¾“å…¥ Token çš„ç¼–ç å’Œå¯¹åº”ä½ç½®çš„å¯å­¦ä¹ ä½ç½®ç¼–ç å‘é‡ç›¸åŠ ï¼Œä»è€Œèµ‹äºˆ Token ç›¸åº”çš„ä½ç½®ä¿¡æ¯ã€‚

è¿™ç§ä½ç½®ç¼–ç æ–¹å¼èƒ½å¤Ÿæ ¹æ®å…·ä½“çš„ä»»åŠ¡å’Œæ•°æ®ç‰¹ç‚¹ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ›´é€‚åˆè¯¥ç±»æ–‡æœ¬çš„ä½ç½®è¡¨ç¤ºæ–¹å¼ï¼Œæ•æ‰æ–‡æœ¬ä¸­ä½ç½®ç›¸å…³çš„è¯­ä¹‰å’Œç»“æ„ä¿¡æ¯ï¼Œè¿™æ˜¯å›ºå®šçš„ä½ç½®ç¼–ç ï¼ˆä¾‹å¦‚ï¼šæ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç ï¼‰éš¾ä»¥åšåˆ°çš„ã€‚ä½†æ˜¯ä¹Ÿå­˜åœ¨ä¸€äº›ä¸è¶³ä¹‹å¤„ï¼Œä¾‹å¦‚ï¼š

1. å¦‚æœè®­ç»ƒæ—¶ max_len=512ï¼Œæµ‹è¯•æ—¶è¾“å…¥ 1024 é•¿åº¦çš„åºåˆ—ï¼Œæ¨¡å‹å°±æ— æ³•å¤„ç†äº†

2. éœ€è¦å­˜å‚¨ max_lenÃ—d_model ç»´åº¦çš„å‚æ•°ï¼Œå¯èƒ½å¯¼è‡´å¤§æ¨¡å‹è®­ç»ƒæ›´éš¾æ”¶æ•›

## ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆRelative Position Encoding, RPEï¼‰

ç›¸å¯¹ä½ç½®ç¼–ç ç›´æ¥å¯¹ ä¸¤ä¸ª token ä¹‹é—´çš„è·ç¦» å»ºæ¨¡ã€‚ä¾‹å¦‚ï¼š

1. token i å…³æ³¨ token j æ—¶ï¼Œæ³¨æ„åŠ›åˆ†æ•°ä¸ä»…å–å†³äºå®ƒä»¬çš„å†…å®¹ï¼Œè¿˜å–å†³äº i - j çš„ç›¸å¯¹ä½ç½®ã€‚

2. å¦‚æœä¸¤ä¸ªä½ç½®çš„ç›¸å¯¹è·ç¦»ç›¸åŒï¼Œé‚£ä¹ˆå®ƒä»¬çš„ç›¸å¯¹ä½ç½®ä¿¡æ¯ä¹Ÿæ˜¯ç›¸åŒçš„ï¼ˆæ¨¡å‹å¯ä»¥æ›´å¥½æ³›åŒ–åˆ°æ›´é•¿çš„åºåˆ—ï¼‰ã€‚

### Relative Position Representations (Shaw et al., 2018)

æ™®é€šè‡ªæ³¨æ„åŠ›æ‰“åˆ†å…¬å¼ï¼š

$$
\text{score}(i,j) = \frac{Q_i \cdot K_j}{\sqrt{d_k}}
$$

å…¶ä¸­ï¼š

* $Q_i = x_i W_Q$

* $K_j = x_j W_K$

Shaw ç›¸å¯¹ä½ç½®ç¼–ç æ”¹æˆï¼š

$$
\text{score}(i,j) = \frac{Q_i \cdot K_j + Q_i \cdot R_{i-j}}{\sqrt{d_k}}
$$

è¿™é‡Œï¼š

* $R_{i-j}$ æ˜¯å’Œ**ç›¸å¯¹è·ç¦»** $i-j$ å¯¹åº”çš„å‘é‡ï¼ˆå¯è®­ç»ƒï¼‰ã€‚

* åªè¦çŸ¥é“ $i-j$ï¼Œå°±èƒ½ä»ä¸€ä¸ª embedding table é‡ŒæŸ¥åˆ°å¯¹åº”çš„ $R$ å‘é‡ã€‚

ä¸‹é¢å…ˆç»™å‡ºå®Œæ•´ä»£ç å®ç°ï¼Œç„¶åå†è¿›è¡Œè¯¦ç»†è§£æ:

```python
import torch
import torch.nn as nn
import math

class RelPosAttention(nn.Module):
    def __init__(self, d_model, n_heads, max_len):
        super().__init__()
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.rel_emb = nn.Embedding(2 * max_len - 1, self.d_k)  # ç›¸å¯¹ä½ç½®å‘é‡è¡¨
        self.max_len = max_len
        
    def forward(self, x):
        B, L, _ = x.size()
        assert L <= self.max_len, "è¾“å…¥åºåˆ—é•¿åº¦è¶…è¿‡max_len"
        
        Q = self.W_q(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2)  # (B, h, L, d_k)
        K = self.W_k(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(B, L, self.n_heads, self.d_k).transpose(1, 2)
        
        # ===== 1. æ™®é€šæ³¨æ„åŠ›éƒ¨åˆ† =====
        content_score = torch.matmul(Q, K.transpose(-2, -1))  # (B, h, L, L)
        
        # ===== 2. ç›¸å¯¹ä½ç½®éƒ¨åˆ† =====
        # ç›¸å¯¹ä½ç½®ç´¢å¼•çŸ©é˜µ
        rel_pos = torch.arange(L, device=x.device).unsqueeze(1) - torch.arange(L, device=x.device).unsqueeze(0)
        rel_pos += self.max_len - 1  # shiftåˆ°[0, 2L-2]
        R = self.rel_emb(rel_pos)    # (L, L, d_k)
        
        # ä½¿ç”¨çˆ±å› æ–¯å¦æ±‚å’Œå…¬å¼è®¡ç®— Q_i â‹… R_{i-j}
        pos_score = torch.einsum('bhld,lmd->bhlm', Q, R)
        
        # ===== 3. åˆå¹¶å¹¶å½’ä¸€åŒ– =====
        scores = (content_score + pos_score) / math.sqrt(self.d_k)
        attn = torch.softmax(scores, dim=-1)
        
        out = torch.matmul(attn, V)  # (B, h, L, d_k)
        out = out.transpose(1, 2).contiguous().view(B, L, -1)
        return self.W_o(out)
```

é¦–å…ˆæ¥çœ‹ä¸€ä¸‹ Shaw ç›¸å¯¹ä½ç½®ç¼–ç ä¸­çš„ç›¸å¯¹ä½ç½®çŸ©é˜µçš„è¯¦è§£:

1. ç”Ÿæˆç›¸å¯¹ä½ç½®å·®çŸ©é˜µ

```python
rel_pos = torch.arange(L, device=x.device).unsqueeze(1) - torch.arange(L, device=x.device).unsqueeze(0)
```

å‡è®¾åºåˆ—é•¿åº¦ `L = 4`ï¼Œ`torch.arange(L)` æ˜¯ï¼š

```
[0, 1, 2, 3]
```

* `unsqueeze(1)` å˜æˆåˆ—å‘é‡ shape `(4,1)`

* `unsqueeze(0)` å˜æˆè¡Œå‘é‡ shape `(1,4)`

åšå‡æ³•ï¼ˆå¹¿æ’­è§„åˆ™ï¼‰ï¼š

```
[[ 0-0, 0-1, 0-2, 0-3],
 [ 1-0, 1-1, 1-2, 1-3],
 [ 2-0, 2-1, 2-2, 2-3],
 [ 3-0, 3-1, 3-2, 3-3]]
```

ç»“æœæ˜¯ï¼š

```
[[ 0, -1, -2, -3],
 [ 1,  0, -1, -2],
 [ 2,  1,  0, -1],
 [ 3,  2,  1,  0]]
```

**å«ä¹‰**ï¼šç¬¬ i è¡Œç¬¬ j åˆ—çš„å€¼å°±æ˜¯ `i - j`ï¼Œå³ **token i ä¸ token j çš„ç›¸å¯¹è·ç¦»**ã€‚

---

2. å¹³ç§»åˆ°æ­£ç´¢å¼•åŒºé—´

```python
rel_pos += self.max_len - 1
```

Embedding çš„ç´¢å¼•å¿…é¡»æ˜¯ **éè´Ÿæ•´æ•°**ï¼Œæ‰€ä»¥è¦æŠŠè´Ÿå€¼å¹³ç§»åˆ°æ­£æ•°åŒºé—´ã€‚å¦‚æœ `max_len=4`ï¼Œ`self.max_len - 1 = 3`ï¼ŒåŠ  3 åï¼š

```
[[3, 2, 1, 0],
 [4, 3, 2, 1],
 [5, 4, 3, 2],
 [6, 5, 4, 3]]
```

å€¼åŸŸèŒƒå›´æ˜¯ `[0, 2*max_len-2]`ã€‚è¿™æ­£å¥½å¯¹åº” `self.rel_emb` çš„ embedding è¡¨å¤§å° `(2*max_len - 1, d_k)`ã€‚

---

3. æŸ¥è¡¨å¾—åˆ°ç›¸å¯¹ä½ç½®å‘é‡

```python
R = self.rel_emb(rel_pos)  # shape: (L, L, d_k)
```

`self.rel_emb` æ˜¯ä¸€ä¸ªå¯è®­ç»ƒçš„ embedding è¡¨ï¼Œæ¯ä¸ªç›¸å¯¹è·ç¦»å¯¹åº”ä¸€ä¸ªå‘é‡ã€‚`rel_pos` çš„å½¢çŠ¶æ˜¯ `(L, L)`ï¼ŒæŸ¥è¡¨åï¼š

* ç¬¬ä¸€ç»´ = æŸ¥è¯¢ä½ç½® i

* ç¬¬äºŒç»´ = è¢«å…³æ³¨ä½ç½® j

* ç¬¬ä¸‰ç»´ = å¯¹åº”çš„ç›¸å¯¹ä½ç½®ç¼–ç å‘é‡ $R_{i-j}$ï¼Œé•¿åº¦ `d_k`

æ‰€ä»¥ `R` æ˜¯ä¸€ä¸ª `(L, L, d_k)` å¼ é‡ã€‚

---

4. ç”¨çˆ±å› æ–¯å¦æ±‚å’Œå…¬å¼è®¡ç®—ä½ç½®åˆ†æ•°

```python
pos_score = torch.einsum('bhld,lmd->bhlm', Q, R)
```
å…ˆçœ‹ä¸¤ä¸ªè¾“å…¥çš„å½¢çŠ¶:

* **Q**: `(B, h, L, d_k)`

  * `B` = batch å¤§å°

  * `h` = æ³¨æ„åŠ›å¤´æ•°
  
  * `L` = åºåˆ—é•¿åº¦ï¼ˆæŸ¥è¯¢ token çš„ä½ç½®ï¼‰
  
  * `d_k` = æ¯ä¸ªå¤´çš„å‘é‡ç»´åº¦ï¼ˆQ å‘é‡é•¿åº¦ï¼‰

* **R**: `(L, L, d_k)`

  * ç¬¬ 1 ç»´ï¼šæŸ¥è¯¢ä½ç½® index $i$
  
  * ç¬¬ 2 ç»´ï¼šè¢«å…³æ³¨ä½ç½® index $j$
  
  * ç¬¬ 3 ç»´ï¼šä¸ç›¸å¯¹ä½ç½® $i-j$ å¯¹åº”çš„å‘é‡ï¼ˆé•¿åº¦ `d_k`ï¼‰

çˆ±å› æ–¯å¦æ±‚å’Œè§„åˆ™: `'bhld,lmd->bhlm'`

* **å·¦è¾¹ Q çš„ç»´åº¦**ï¼š`b h l d`

* **å³è¾¹ R çš„ç»´åº¦**ï¼š`l m d`

* ä¸¤è€…ä¸­**ç›¸åŒå­—æ¯**ä»£è¡¨è¦â€œé…å¯¹â€çš„è½´ï¼š

  * `l`ï¼šæŸ¥è¯¢ä½ç½® i â†’ **ä¿æŒä¸å˜**ï¼ˆå‚ä¸é…å¯¹ä½†ä¿ç•™åœ¨è¾“å‡ºé‡Œï¼‰

  * `d`ï¼šå‘é‡ç»´åº¦ â†’ **ç›¸åŒå­—æ¯ä¸”ä¸å‡ºç°åœ¨è¾“å‡ºï¼Œè¡¨ç¤ºè¦ç›¸ä¹˜åæ±‚å’Œ**ï¼ˆç‚¹ç§¯ï¼‰

* **ä¸åŒå­—æ¯**ï¼š

  * `m`ï¼šæ¥è‡ª R çš„â€œè¢«å…³æ³¨ä½ç½®â€ç»´åº¦ï¼Œå‡ºç°åœ¨è¾“å‡º
  
  * `b`ã€`h`ï¼šæ¥è‡ª Q çš„ batch å’Œå¤´ç»´åº¦ï¼Œç›´æ¥ä¿ç•™

æˆ‘ä»¬æƒ³ç®—ï¼š

$$
\text{pos\_score}[b,h,l,m] = \sum_{d=1}^{d_k} Q[b,h,l,d] \times R[l,m,d]
$$

* å›ºå®š batch `b`ã€head `h`ã€æŸ¥è¯¢ä½ç½® `l`ã€è¢«å…³æ³¨ä½ç½® `m`

* ä» Q é‡Œå–å¯¹åº”çš„æŸ¥è¯¢å‘é‡ $Q_{b,h,l,:}$

* ä» R é‡Œå–å¯¹åº”çš„ç›¸å¯¹ä½ç½®å‘é‡ $R_{l,m,:}$

* å¯¹å®ƒä»¬åš**å‘é‡ç‚¹ç§¯**ï¼ˆæ²¿ `d` ç»´æ±‚å’Œï¼‰

--- 

5. ä¸ºä»€ä¹ˆç›¸å¯¹ä½ç½®çŸ©é˜µ $R$ ä¸ç›´æ¥åŠ åˆ°æ³¨æ„åŠ›åˆ†æ•°é‡Œï¼Œè€Œæ˜¯è¿˜è¦å’Œ $Q$ åšç‚¹ç§¯ï¼Ÿ

åœ¨æ ‡å‡†è‡ªæ³¨æ„åŠ›ä¸­ï¼Œæ‰“åˆ†æ˜¯ï¼š

$$
\text{score}(i,j) = Q_i \cdot K_j
$$

å…¶ä¸­ï¼š

* $Q_i = x_i W_Q$ï¼š**æŸ¥è¯¢ token i çš„å†…å®¹å‘é‡**

* $K_j = x_j W_K$ï¼š**è¢«å…³æ³¨ token j çš„å†…å®¹å‘é‡**

è¿™ä¸¤ä¸ªå‘é‡éƒ½åœ¨**åŒä¸€ä¸ªå‘é‡ç©ºé—´**ä¸­ï¼ˆç»´åº¦ `d_k`ï¼‰ï¼Œç‚¹ç§¯æ‰èƒ½å¾—åˆ°ä¸€ä¸ªæœ‰æ„ä¹‰çš„ç›¸ä¼¼åº¦åˆ†æ•°ã€‚Shaw çš„åšæ³•æ˜¯ï¼š

$$
\text{score}(i,j) = Q_i \cdot K_j + Q_i \cdot R_{i-j}
$$

ç¬¬äºŒé¡¹çš„è§£é‡Šï¼š

* $R_{i-j}$ ä¹Ÿæ˜¯åœ¨ `d_k` ç»´çš„å‘é‡ç©ºé—´ä¸­ã€‚

* ç”¨ $Q_i$ å’Œ $R_{i-j}$ åšç‚¹ç§¯ï¼ŒæŠŠ**ä½ç½®ä¿¡æ¯å‘é‡**æŠ•å½±åˆ°å’Œ `Q_i` ä¸€è‡´çš„è¡¨ç¤ºç©ºé—´ã€‚

* è¿™æ ·å¾—åˆ°çš„ç»“æœæ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œå¯ä»¥ç›´æ¥å’Œ `QÂ·K` çš„ç»“æœç›¸åŠ ã€‚

æ¢å¥è¯è¯´ï¼š

* **K éƒ¨åˆ†**å¸¦æ¥**å†…å®¹ç›¸å…³æ€§**

* **R éƒ¨åˆ†**å¸¦æ¥**ä½ç½®ç›¸å…³æ€§**

* äºŒè€…éƒ½è¦åœ¨**åŒä¸€ä¸ªâ€œQ å‘é‡çš„è§†è§’â€ä¸‹è¡¡é‡**ï¼Œæ‰€ä»¥éƒ½ç”¨ $Q_i$ åšç‚¹ç§¯

> å¯ä»¥æŠŠæ³¨æ„åŠ›æ‰“åˆ†çœ‹ä½œâ€œä½ (i)å¯¹åˆ«äºº(j)çš„å…³æ³¨ç¨‹åº¦â€ï¼Œå®ƒå¯èƒ½ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š
> 
> 1. **å†…å®¹ç›¸ä¼¼åº¦**ï¼šä½ å…³å¿ƒå’Œä½ å†…å®¹ç±»ä¼¼çš„äººï¼ˆQÂ·Kï¼‰
> 
> 2. **ä½ç½®åå¥½**ï¼šä½ å…³å¿ƒç¦»ä½ è¿‘çš„äººï¼ˆQÂ·Rï¼‰

### T5 ç›¸å¯¹ä½ç½®åç½®ï¼ˆRelative Position Bias, RPBï¼‰

åœ¨è‡ªæ³¨æ„åŠ›é‡Œï¼Œæ³¨æ„åŠ›æ‰“åˆ†æ˜¯

$$
\text{score}(i,j)=\frac{Q_i\cdot K_j}{\sqrt{d_k}}
$$

T5 åœ¨è¿™ä¸ªåˆ†æ•°ä¸Š**å†åŠ ä¸€ä¸ªä¸â€œç›¸å¯¹è·ç¦»â€ç›¸å…³çš„æ ‡é‡åç½®**ï¼š

$$
\text{score}(i,j)=\frac{Q_i\cdot K_j}{\sqrt{d_k}}+\underbrace{b_{\text{bucket}(i-j)}}_{\text{ç›¸å¯¹ä½ç½®åç½®ï¼ˆæ ‡é‡ï¼‰}}
$$

* $i-j$ æ˜¯æŸ¥è¯¢ä½ç½®åˆ°é”®ä½ç½®çš„ç›¸å¯¹è·ç¦»ã€‚

* ä¸ºäº†**å‚æ•°æ›´çœ**ã€**æ³›åŒ–æ›´å¥½**ï¼ŒT5 ä¸ä¸ºæ¯ä¸ªè·ç¦»å•ç‹¬å­¦ä¸€ä¸ªå‚æ•°ï¼Œè€Œæ˜¯æŠŠè·ç¦»æ˜ å°„åˆ°**å°‘é‡â€œæ¡¶â€ï¼ˆbucketï¼‰**ï¼šè¿‘è·ç¦»ç”¨ç»†æ¡¶ã€è¿œè·ç¦»ç”¨ç²—æ¡¶ï¼ˆå¯¹æ•°åˆ†æ¡¶ï¼‰ã€‚

* æ¯ä¸ªæ¡¶å­¦ä¸€ä¸ª**æ ‡é‡åç½®**ï¼Œé€šå¸¸ **æŒ‰å¤´**ï¼ˆper-headï¼‰ç‹¬ç«‹å­¦ä¹ ï¼ˆå½¢çŠ¶ï¼š`[num_buckets, n_heads]`ï¼‰ï¼Œå†å¹¿æ’­åˆ° `(B, n_heads, L_q, L_k)`ã€‚

è¿™æ ·æ—¢è®©æ¨¡å‹â€œåå¥½ä¸´è¿‘â€æˆ–â€œæƒ©ç½šå¤ªè¿œâ€ï¼Œåˆå‡ ä¹ä¸å¢åŠ è®¡ç®—é‡ã€‚

> ![](ä½ç½®ç¼–ç /4.png)
> 
> - Shaw RPRï¼šç›¸å¯¹ä½ç½®åƒâ€œé¢å¤–çš„ Key ç‰¹å¾â€ï¼Œå‚ä¸ç‚¹ç§¯è®¡ç®—
>
> - T5 RPBï¼šç›¸å¯¹ä½ç½®åƒâ€œåˆ†æ•°ä¿®æ­£è¡¨â€ï¼Œåªåœ¨ attention åˆ†æ•°ä¸ŠåŠ åç½®

---

1. åç½®è¡¨ï¼ˆbias tableï¼‰æ€ä¹ˆç†è§£ ?

```python
bias_table = nn.Parameter(torch.zeros(num_buckets, H))
```

* **num\_buckets**ï¼šè¡¨ç¤ºâ€œç›¸å¯¹ä½ç½®çš„åˆ†ç»„æ•°é‡â€ã€‚

  * T5 å¹¶ä¸å¯¹æ¯ä¸ªç›¸å¯¹ä½ç½®éƒ½å•ç‹¬å­˜ä¸€ä¸ªåç½®ï¼Œè€Œæ˜¯æŠŠç›¸å¯¹è·ç¦»å‹ç¼©åˆ°è‹¥å¹²ä¸ªæ¡¶ï¼ˆbucketï¼‰é‡Œã€‚

  * æ¯”å¦‚å°è·ç¦» 1ã€2ã€3 å¯ä»¥æ˜ å°„åˆ°åŒä¸€ä¸ªæ¡¶ï¼Œè¿œè·ç¦»å¯èƒ½æ˜ å°„åˆ°ä¸åŒæ¡¶ã€‚

* **H**ï¼šè¡¨ç¤ºæ³¨æ„åŠ›å¤´æ•°ï¼Œæ¯ä¸ªå¤´çš„åç½®å¯ä»¥ä¸åŒã€‚

  * ä¸åŒå¤´å¯ä»¥å­¦ä¹ åˆ°ä¸åŒçš„ç›¸å¯¹ä½ç½®åç½®æ¨¡å¼ï¼Œæ¯”å¦‚ä¸€ä¸ªå¤´å…³æ³¨çŸ­è·ç¦»ï¼Œä¸€ä¸ªå¤´å…³æ³¨é•¿è·ç¦»ã€‚

åç½®è¡¨ä½œç”¨:

* åç½®è¡¨é‡Œçš„æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª**æ ‡é‡**ï¼Œå®ƒå¯¹åº”æŸä¸ª**ç›¸å¯¹ä½ç½®æ¡¶ + æ³¨æ„åŠ›å¤´**çš„åç½®ã€‚

* å®ƒä¸ä¼šå‚ä¸ç‚¹ç§¯è¿ç®—ï¼Œåªæ˜¯ç›´æ¥**åŠ åˆ°æ³¨æ„åŠ›åˆ†æ•°**ä¸Šï¼š

$$
\text{attn\_score}[b,h,i,j] = Q_i \cdot K_j + \text{bias}[bucket_{i-j}, h]
$$

* é€šè¿‡åŠ åç½®ï¼Œæ¨¡å‹å¯ä»¥**ç¼–ç â€œç›¸å¯¹ä½ç½®ä¿¡æ¯â€**ï¼Œæ¯”å¦‚è®©æ¨¡å‹åå‘å…³æ³¨é™„è¿‘çš„ tokenã€‚

å¯ä»¥æƒ³è±¡æˆä¸€ä¸ªå°è¡¨æ ¼ï¼š

| æ¡¶ (bucket) | head0 | head1 | head2 |
| ---------- | ----- | ----- | ----- |
| 0          | 0.1   | -0.2  | 0.05  |
| 1          | 0.3   | 0.0   | 0.1   |
| 2          | -0.1  | 0.2   | 0.0   |
| â€¦          | â€¦     | â€¦     | â€¦     |

* æ¯ä¸€åˆ— = ä¸€ä¸ªå¤´çš„æ‰€æœ‰åç½®

* æ¯ä¸€è¡Œ = ä¸€ä¸ªç›¸å¯¹è·ç¦»æ¡¶çš„åç½®

å½“æ¨¡å‹è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°æ—¶ï¼Œå®ƒä¼šæ ¹æ® query-key çš„ç›¸å¯¹è·ç¦»æ‰¾åˆ°å¯¹åº”çš„æ¡¶ï¼Œç„¶åæŸ¥è¡¨å–å‡ºåç½®ï¼ŒåŠ åˆ°è¯¥å¤´çš„æ³¨æ„åŠ›åˆ†æ•°ä¸Šã€‚

---

2. æ¡¶çš„ä½œç”¨æ˜¯ä»€ä¹ˆ ï¼Ÿ

æˆ‘ä»¬å·²ç»æœ‰äº† **ç›¸å¯¹ä½ç½®çŸ©é˜µ** `relative_position[i,j] = j - i` ï¼Œä¾‹å¦‚é•¿åº¦ 8 çš„åºåˆ—ï¼š

```
i\j   0   1   2   3   4   5   6   7
0     0   1   2   3   4   5   6   7
1    -1   0   1   2   3   4   5   6
2    -2  -1   0   1   2   3   4   5
3    -3  -2  -1   0   1   2   3   4
...
```

* æ¯ä¸ªå…ƒç´ è¡¨ç¤º query i å¯¹ key j çš„ç›¸å¯¹è·ç¦»

**é—®é¢˜ï¼šåºåˆ—å¾ˆé•¿æ—¶ï¼Œå¦‚æœç›´æ¥ä¸ºæ¯ä¸ªè·ç¦»å­¦ä¹ åç½®ï¼Œå‚æ•°é‡ä¼šéå¸¸å¤§**ã€‚

æ¡¶æ˜ å°„çš„ç›®çš„:

* **æŠŠç›¸å¯¹è·ç¦»å‹ç¼©åˆ°å›ºå®šæ•°é‡çš„æ¡¶é‡Œ**

* T5 çš„åšæ³•ï¼š**å°è·ç¦»ç”¨ç‹¬ç«‹æ¡¶ï¼Œå¤§è·ç¦»ç”¨å¯¹æ•°å‹ç¼©çš„æ¡¶**

* è¿™æ ·ï¼š

  1. **çŸ­è·ç¦»**ï¼šæ¯ä¸ªè·ç¦»æœ‰è‡ªå·±çš„åç½®ï¼ˆç»†ç²’åº¦ï¼‰

  2. **é•¿è·ç¦»**ï¼šè·ç¦»è¾ƒå¤§çš„ token å…±äº«åŒä¸€æ¡¶ï¼ˆç²—ç²’åº¦ï¼‰

* ä¼˜ç‚¹ï¼š

  * èŠ‚çœå‚æ•°

  * ä¿æŒæ¨¡å‹å…³æ³¨çŸ­è·ç¦»ç²¾ç»†ä¿¡æ¯ï¼ŒåŒæ—¶å¯¹é•¿è·ç¦»ä¸å¿…è¿‡äºç²¾ç»†

å‡è®¾ `num_buckets = 4`ï¼ˆç®€åŒ–ï¼‰ï¼š

* relative\_position â‰¤ 1 â†’ bucket 0

* relative\_position = 2 â†’ bucket 1

* relative\_position = 3\~4 â†’ bucket 2

* relative\_position > 4 â†’ bucket 3

å¯¹åºåˆ— `[A,B,C,D]`ï¼š

```
relative_position:

i\j   0   1   2   3
0     0   1   2   3
1    -1   0   1   2
2    -2  -1   0   1
3    -3  -2  -1   0

æ˜ å°„åˆ°æ¡¶ (bucket):

i\j   0   1   2   3
0     0   0   1   2
1     0   0   0   1
2     1   0   0   0
3     2   1   0   0
```

* æ¯ä¸ªå…ƒç´ éƒ½å˜æˆäº† **0\~num\_buckets-1** çš„æ•´æ•°

* è¿™ä¸ªæ•´æ•°å°±æ˜¯åç½®æŸ¥è¡¨çš„ç´¢å¼•

**æ ¸å¿ƒç†è§£**ï¼š

* **ç›¸å¯¹ä½ç½®æ¡¶** = â€œè·ç¦»åˆ†ç»„â€

* é€šè¿‡æ¡¶æ˜ å°„ï¼Œå¯ä»¥è®©æ¨¡å‹ **å¯¹çŸ­è·ç¦»æ•æ„Ÿï¼Œå¯¹é•¿è·ç¦»ç²—ç•¥å¤„ç†**

* æ¡¶å·æœ€ç»ˆç”¨æ¥æŸ¥ `bias_table`ï¼Œå¾—åˆ°æ¯ä¸ª query-key å¯¹çš„åç½®

---

3. æ•´ä½“æµç¨‹ ?

å‡è®¾ï¼š

* batch size: `B`

* åºåˆ—é•¿åº¦: `L`

* æ³¨æ„åŠ›å¤´æ•°: `H`

* ç›¸å¯¹ä½ç½®æ¡¶æ•°: `num_buckets`

**(1) åç½®è¡¨**

```python
bias_table = nn.Parameter(torch.zeros(num_buckets, H))
```

* shape: `(num_buckets, H)`

* æ¯ä¸ªæ¡¶æ¯ä¸ªå¤´éƒ½æœ‰ä¸€ä¸ªæ ‡é‡åç½®

**(2) ç›¸å¯¹ä½ç½®çŸ©é˜µ**

```python
relative_position = memory_position - context_position  # (L,L)
```

* shape: `(L, L)`

* å€¼æ˜¯ `j-i`ï¼Œè¡¨ç¤º query i å¯¹ key j çš„ç›¸å¯¹ä½ç½®

**(3) æ¡¶æ˜ å°„**

```python
relative_buckets = relative_position_bucket(relative_position)  # (L,L)
```

* shape: `(L,L)`

* æ¯ä¸ªå…ƒç´ æ˜¯ 0\~`num_buckets-1`

**(4) æŸ¥è¡¨å¾—åˆ°åç½®**

```python
bias = bias_table[relative_buckets]  # (L,L,H)
bias = bias.permute(2,0,1)           # (H,L,L)
```

* `bias_table[relative_buckets]` ç”¨æ¯ä¸ª `(i,j)` æ¡¶å·æŸ¥è¡¨

* å¾—åˆ° `(L,L,H)`ï¼Œç„¶åæ¢ç»´åº¦åˆ° `(H,L,L)`ï¼Œæ–¹ä¾¿åŠ åˆ°æ³¨æ„åŠ›åˆ†æ•°

**(5) å¹¿æ’­ batch**

æ³¨æ„åŠ›åˆ†æ•°ï¼š

```python
attn_scores = Q @ K.transpose(-1,-2)  # (B,H,L,L)
attn_scores = attn_scores + bias.unsqueeze(0)  # (B,H,L,L)
```

* `bias.unsqueeze(0)` shape â†’ `(1,H,L,L)`

* è‡ªåŠ¨å¹¿æ’­åˆ° batch ç»´åº¦ `(B,H,L,L)`

* **æ¯ä¸ªå¤´æœ‰ç‹¬ç«‹åç½®**ï¼Œä¸åŒ batch å…±ç”¨

**(6) ç›¸åŠ è¿‡ç¨‹æ€»ç»“**

1. å…ˆç®— **åŸå§‹æ³¨æ„åŠ›åˆ†æ•°**ï¼š`Q @ K^T` â†’ `(B,H,L,L)`

2. æŸ¥è¡¨å¾—åˆ° **ç›¸å¯¹ä½ç½®åç½®**ï¼š`bias` â†’ `(H,L,L)`

3. åŠ åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼š

   $$
   \text{attn\_scores}_{b,h,i,j} = Q_i \cdot K_j + B_{i-j,h}
   $$

4. æœ€ç»ˆ shape ä»ç„¶ `(B,H,L,L)`ï¼Œç”¨äº softmax

ä»£ç å®ç°å¦‚ä¸‹:

```python
import torch
import torch.nn as nn

class T5RelativePositionBias(nn.Module):
    def __init__(self, num_buckets=32, max_distance=128, n_heads=12):
        super().__init__()
        self.num_buckets = num_buckets
        self.max_distance = max_distance
        self.n_heads = n_heads
        # åç½®è¡¨ï¼šæ¯ä¸ª bucket å¯¹æ¯ä¸ª attention head å­˜ä¸€ä¸ªæ ‡é‡
        self.relative_attention_bias = nn.Embedding(num_buckets, n_heads)

    def _relative_position_bucket(self, relative_position):
        """
        å°†ç›¸å¯¹ä½ç½®æ˜ å°„åˆ° [0, num_buckets-1] çš„æ¡¶ç´¢å¼•ã€‚
        
        è®¾è®¡ç›®çš„ï¼š
            - ç›¸å¯¹ä½ç½®å¯èƒ½èŒƒå›´å¾ˆå¤§ï¼ˆå¦‚ -512 ~ +512ï¼‰
            - å¦‚æœç›´æ¥ç»™æ¯ä¸ªä½ç½®éƒ½åˆ†é…ä¸€ä¸ªç‹¬ç«‹å‚æ•°ï¼Œå‚æ•°é‡ä¼šå¾ˆå¤§
            - è§£å†³æ–¹æ³•ï¼š
                1. å¯¹çŸ­è·ç¦»ç”¨çº¿æ€§åˆ†æ¡¶ï¼ˆç²¾ç¡®è¡¨ç¤ºï¼‰
                2. å¯¹é•¿è·ç¦»ç”¨å¯¹æ•°åˆ†æ¡¶ï¼ˆç²—ç•¥è¡¨ç¤ºï¼‰
                3. å¯é€‰åœ°åˆ†å¼€å¤„ç†æ–¹å‘ï¼ˆå·¦/å³ï¼‰

        å‚æ•°ï¼š
            relative_position: torch.Tensor
                - å½¢çŠ¶ (Lq, Lk) æˆ– (..., Lq, Lk)
                - å…ƒç´ ä¸º key_position - query_position
                  è´Ÿæ•°ï¼škey åœ¨ query å·¦è¾¹
                  æ­£æ•°ï¼škey åœ¨ query å³è¾¹

        è¿”å›ï¼š
            ret: torch.LongTensor
                - å½¢çŠ¶ä¸ relative_position ç›¸åŒ
                - æ¯ä¸ªå…ƒç´ æ˜¯è¯¥ç›¸å¯¹ä½ç½®å¯¹åº”çš„æ¡¶ç¼–å·
        """

        num_buckets = self.num_buckets    # æ¡¶çš„æ€»æ•°é‡ï¼Œä¾‹å¦‚ 32
        max_distance = self.max_distance  # èƒ½æ˜ å°„çš„æœ€å¤§è·ç¦»ï¼ˆè¶…è¿‡æŒ‰æœ€å¤§å¤„ç†ï¼‰
        ret = 0                            # åˆå§‹åŒ–æ¡¶ç¼–å·

        # 1. å–åï¼ˆT5 å®šä¹‰ç›¸å¯¹ä½ç½®ä¸º memory_pos - context_posï¼Œè¿™é‡Œä¿æŒä¸€è‡´æ€§ï¼‰
        n = -relative_position  # n < 0 è¡¨ç¤º key åœ¨ query å³è¾¹ï¼Œn > 0 è¡¨ç¤º key åœ¨ query å·¦è¾¹

        # 2. æ–¹å‘åˆ†æ¡¶ï¼ˆå¯é€‰ï¼‰
        #    å¦‚æœ key åœ¨ query å·¦è¾¹ (n < 0)ï¼Œæ¡¶å·åŠ  num_buckets//2
        #    è¿™æ ·å‰ä¸€åŠæ¡¶è¡¨ç¤ºå·¦æ–¹å‘ï¼Œåä¸€åŠæ¡¶è¡¨ç¤ºå³æ–¹å‘
        ret += (n < 0).to(torch.long) * num_buckets // 2

        # 3. åªå–ç»å¯¹å€¼ï¼ˆæ–¹å‘ä¿¡æ¯å·²åœ¨ä¸Šä¸€æ­¥ç¼–ç ï¼‰
        n = torch.abs(n)

        # 4. å®šä¹‰çŸ­è·ç¦»çº¿æ€§æ˜ å°„çš„é˜ˆå€¼
        #    å‰ä¸€åŠæ¡¶ï¼ˆnum_buckets//2ï¼‰ç”¨äºç²¾ç¡®è¡¨ç¤ºçŸ­è·ç¦»
        max_exact = num_buckets // 2  # ä¾‹å¦‚ num_buckets=32 æ—¶ï¼Œmax_exact=16

        # 5. åˆ¤æ–­å“ªäº›æ˜¯çŸ­è·ç¦»
        is_small = n < max_exact  # å¸ƒå°”å¼ é‡

        # 6. å¯¹é•¿è·ç¦»åšå¯¹æ•°æ˜ å°„
        #    - å°†è·ç¦»èŒƒå›´ [max_exact, max_distance] æ˜ å°„åˆ°æ¡¶ [max_exact, num_buckets-1]
        #    - å¯¹æ•°æ˜ å°„å¯ä»¥æŠŠå¤§èŒƒå›´çš„è·ç¦»å‹ç¼©åˆ°å°‘é‡æ¡¶
        val_if_large = max_exact + (
            (torch.log(n.float() / max_exact) /                 # è·ç¦»å½’ä¸€åŒ–å¹¶å–å¯¹æ•°
             torch.log(max_distance / max_exact))               # å¯¹æ•°åˆ†æ¯ï¼šå½’ä¸€åŒ–æœ€å¤§è·ç¦»
            * (num_buckets - max_exact)                         # æ˜ å°„åˆ°é•¿è·ç¦»æ¡¶åŒºé—´
        ).to(torch.long)

        # 7. é˜²æ­¢æº¢å‡ºï¼ˆå¤§äºæœ€å¤§æ¡¶å·çš„å…¨éƒ¨å‹åˆ°æœ€åä¸€ä¸ªæ¡¶ï¼‰
        val_if_large = torch.min(
            val_if_large,
            torch.full_like(val_if_large, num_buckets - 1)
        )

        # 8. æ ¹æ®è·ç¦»ç±»åˆ«ï¼ˆçŸ­/é•¿ï¼‰é€‰æ‹©æ¡¶ç¼–å·
        ret += torch.where(is_small, n, val_if_large)

        return ret

    def forward(self, query_length, key_length):
        """
        è¿”å› shape (1, n_heads, query_length, key_length) çš„åç½®çŸ©é˜µ
        """
        # è®¡ç®—ç›¸å¯¹ä½ç½®çŸ©é˜µ (i - j)
        context_pos = torch.arange(query_length)[:, None]  # (Lq, 1)
        memory_pos = torch.arange(key_length)[None, :]     # (1, Lk)
        relative_position = memory_pos - context_pos       # (Lq, Lk)

        # æ˜ å°„åˆ°æ¡¶ç´¢å¼•
        rp_bucket = self._relative_position_bucket(relative_position)

        # æŸ¥è¡¨è·å–åç½®å€¼ (Lq, Lk, n_heads)
        values = self.relative_attention_bias(rp_bucket)

        # è°ƒæ•´ç»´åº¦ â†’ (1, n_heads, Lq, Lk)ï¼Œæ–¹ä¾¿åŠ åˆ° attention scores ä¸Š
        values = values.permute(2, 0, 1).unsqueeze(0)
        return values
```

**Step 1 â€” åç½®è¡¨çš„å®šä¹‰**

```python
self.relative_attention_bias = nn.Embedding(num_buckets, n_heads)
```

* å½¢çŠ¶ = `(num_buckets, n_heads)`

* è¡¨ç¤ºæ¯ä¸ª **æ¡¶** å¯¹åº”æ¯ä¸ª **æ³¨æ„åŠ›å¤´** çš„ä¸€ä¸ªæ ‡é‡åç½®

* **ä¸ºä»€ä¹ˆæ˜¯ Embedding**ï¼š

  * æ¡¶å·æ˜¯æ•´æ•°ç´¢å¼•ï¼ˆ0\~num\_buckets-1ï¼‰

  * Embedding å¯ä»¥é«˜æ•ˆæŸ¥è¡¨

---

**Step 2 â€” æ„é€ ç›¸å¯¹ä½ç½®çŸ©é˜µ**

```python
context_pos = torch.arange(query_length)[:, None]  # (Lq, 1)
memory_pos = torch.arange(key_length)[None, :]     # (1, Lk)
relative_position = memory_pos - context_pos       # (Lq, Lk)
```

* `(i,j)` å…ƒç´  = `j - i`

* ä¾‹å¦‚ `L=4`ï¼š

```
[[0, 1, 2, 3],
 [-1, 0, 1, 2],
 [-2, -1, 0, 1],
 [-3, -2, -1, 0]]
```

---

**Step 3 â€” ç›¸å¯¹ä½ç½®æ¡¶æ˜ å°„**

```python
rp_bucket = self._relative_position_bucket(relative_position)
```

* **ç›®çš„**ï¼šæŠŠ `[-max_len, +max_len]` æ˜ å°„åˆ° `0 ~ num_buckets-1`

* **T5ç­–ç•¥**ï¼š

  1. çŸ­è·ç¦»ï¼ˆ`|n| < max_exact`ï¼‰â†’ çº¿æ€§æ˜ å°„ï¼Œæ¯ä¸ªè·ç¦»å•ç‹¬ä¸€ä¸ªæ¡¶

  2. é•¿è·ç¦» â†’ å¯¹æ•°æ˜ å°„ï¼Œå¤šä¸ªè·ç¦»å…±äº«ä¸€ä¸ªæ¡¶

  3. æ­£è´Ÿæ–¹å‘å¯èƒ½åˆ†æ¡¶ï¼ˆæ–¹å‘ä¿¡æ¯ä¿ç•™ï¼‰

* **ç»“æœ**ï¼šæ¡¶çŸ©é˜µ shape `(Lq, Lk)`ï¼Œå€¼æ˜¯æ•´æ•°

---

**Step 4 â€” æŸ¥è¡¨è·å–åç½®**

```python
values = self.relative_attention_bias(rp_bucket)  # (Lq, Lk, n_heads)
```

* å¯¹ `(i,j)` çš„æ¡¶å· `rp_bucket[i,j]`ï¼ŒæŸ¥è¡¨å¾—åˆ° shape `(n_heads,)` çš„åç½®

* æœ€ç»ˆå¾—åˆ° `(Lq, Lk, n_heads)`ï¼Œå³æ¯ä¸ªä½ç½®å¯¹æ¯ä¸ª head çš„åç½®å€¼

---

**Step 5 â€” è°ƒæ•´ç»´åº¦ä»¥ä¾¿åŠ åˆ°æ³¨æ„åŠ›åˆ†æ•°**

```python
values = values.permute(2, 0, 1).unsqueeze(0)  # (1, n_heads, Lq, Lk)
```

* attention åˆ†æ•° shape = `(batch, n_heads, Lq, Lk)`

* åç½®åŠ æ³•å…¬å¼ï¼š

$$
\text{scores}[b,h,i,j] = Q_i \cdot K_j + \text{bias}[0,h,i,j]
$$

* `unsqueeze(0)` æ˜¯ä¸ºäº† batch ç»´åº¦å¯å¹¿æ’­

---

**Step 6 â€” åŠ åˆ°æ³¨æ„åŠ›åˆ†æ•°çš„æ—¶æœº**

åœ¨ T5 çš„å¤šå¤´æ³¨æ„åŠ›é‡Œï¼Œè¿™ä¸€æ­¥é€šå¸¸æ˜¯è¿™æ ·åšçš„ï¼š

```python
scores = torch.matmul(Q, K.transpose(-1, -2))  # (B, H, Lq, Lk)
scores += position_bias  # (1, H, Lq, Lk) å¹¿æ’­ç›¸åŠ 
```

åŒºåˆ«äº **Shaw 2018** çš„åšæ³•ï¼ˆå®ƒåœ¨ QÂ·K ä¹‹å‰ï¼ŒæŠŠä½ç½®å‘é‡åŠ åˆ° K é‡Œï¼‰ï¼ŒT5 æ˜¯åœ¨ **attention scores å·²ç»è®¡ç®—å®Œä¹‹å** å†åŠ åç½®ã€‚

---

**ä¸€å¥è¯æ€»ç»“**ï¼š

T5 RPB ç”¨ **æ¡¶æ˜ å°„ + æŸ¥è¡¨** çš„æ–¹å¼ï¼Œä¸ºæ¯ä¸ª query-key å¯¹åŠ ä¸€ä¸ªä¸ç›¸å¯¹ä½ç½®æœ‰å…³çš„æ ‡é‡åç½®ï¼ˆæ¯ä¸ª head å•ç‹¬å­¦ä¹ ï¼‰ï¼Œå®ƒæ˜¯åœ¨æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—å®Œä¹‹ååŠ ä¸Šå»çš„ï¼Œå› æ­¤åªéœ€è¦äºŒç»´æ ‡é‡è¡¨ï¼Œä¸éœ€è¦åƒ Shaw é‚£æ ·å­˜æ•´ä¸ªå‘é‡çŸ©é˜µã€‚

