---
icon: file
category:
  - NLP
tag:
  - é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
  - ç¼–è¾‘ä¸­
footer: æ¢ç´¢AIè¾¹ç•Œï¼Œæ‹¥æŠ±æ™ºèƒ½æœªæ¥
date: 2025-05-19
cover: assets/cover/BERT.jpg
order: 2
author:
  - BinaryOracle
---

`å›¾è§£Bert & Bertæ–‡æœ¬åˆ†ç±»å®æˆ˜` 
<!-- more -->

# å›¾è§£ Bert

## ç¯å¢ƒæ­å»º

æŒ‰åºæ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®Œæˆç¯å¢ƒæ­å»º:

```bash
git clone https://github.com/DA-southampton/Read_Bert_Code.git
cd Read_Bert_Code
conda create -n Read_Bert_Code python=3.9.22
conda activate Read_Bert_Code
```
æœ¬æ–‡ä½¿ç”¨çš„æ˜¯è°·æ­Œçš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ï¼šchinese_L-12_H-768_A-12.zipï¼Œæ¨¡å‹æœ‰ç‚¹å¤§ï¼Œæˆ‘å°±ä¸ä¸Šä¼ äº†ï¼Œå¦‚æœæœ¬åœ°ä¸å­˜åœ¨ï¼Œå°±ç‚¹å‡»[è¿™é‡Œ](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip)ç›´æ¥ä¸‹è½½,æˆ–è€…ç›´æ¥å‘½ä»¤è¡Œè¿è¡Œ

```shell
wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip
```

é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½ä¸‹æ¥ä¹‹åï¼Œè¿›è¡Œè§£å‹ï¼Œç„¶åå°†tfæ¨¡å‹è½¬ä¸ºå¯¹åº”çš„pytorchç‰ˆæœ¬å³å¯ã€‚å¯¹åº”ä»£ç å¦‚ä¸‹:

```shell
export BERT_BASE_DIR=/Users/zhandaohong/Read_Bert_Code/chinese_L-12_H-768_A-12

python convert_tf_checkpoint_to_pytorch.py \
  --tf_checkpoint_path $BERT_BASE_DIR/bert_model.ckpt \
  --bert_config_file $BERT_BASE_DIR/bert_config.json \
  --pytorch_dump_path $BERT_BASE_DIR/pytorch_model.bin
```

è½¬åŒ–æˆåŠŸä¹‹åï¼Œå°†æ¨¡å‹æ”¾å…¥åˆ°ä»“åº“å¯¹åº”ä½ç½®ï¼š

```shell
Read_Bert_Code/bert_read_step_to_step/prev_trained_model/
```

å¹¶é‡æ–°å‘½åä¸ºï¼š

```shell
 bert-base-chinese
```
å…¶æ¬¡æ˜¯å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œè¿™é‡Œæˆ‘å‡†å¤‡åšä¸€ä¸ªæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨çš„æ˜¯Tnewsæ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†æ¥æºæ˜¯[è¿™é‡Œ](https://github.com/ChineseGLUE/ChineseGLUE/tree/master/baselines/models_pytorch/classifier_pytorch/chineseGLUEdatasets)ï¼Œåˆ†ä¸ºè®­ç»ƒï¼Œæµ‹è¯•å’Œå¼€å‘é›†ï¼Œæˆ‘å·²ç»ä¸Šä¼ åˆ°äº†ä»“åº“ä¸­ï¼Œå…·ä½“ä½ç½®åœ¨

```shell
Read_Bert_Code/bert_read_step_to_step/chineseGLUEdatasets/tnews
```
éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œå› ä¸ºæˆ‘åªæ˜¯ä¸ºäº†äº†è§£å†…éƒ¨ä»£ç æƒ…å†µï¼Œæ‰€ä»¥å‡†ç¡®åº¦ä¸æ˜¯åœ¨æˆ‘çš„è€ƒè™‘èŒƒå›´ä¹‹å†…ï¼Œæ‰€ä»¥æˆ‘åªæ˜¯å–å…¶ä¸­çš„ä¸€éƒ¨åˆ†æ•°æ®ï¼Œå…¶ä¸­è®­ç»ƒæ•°æ®ä½¿ç”¨1kï¼Œæµ‹è¯•æ•°æ®ä½¿ç”¨1kï¼Œå¼€å‘æ•°æ®1kã€‚

å‡†å¤‡å°±ç»ªï¼Œä½¿ç”¨pycharmå¯¼å…¥é¡¹ç›®ï¼Œå‡†å¤‡è°ƒè¯•ï¼Œæˆ‘çš„è°ƒè¯•æ–‡ä»¶æ˜¯`run_classifier.py`æ–‡ä»¶ï¼Œå¯¹åº”çš„å‚æ•°ä¸º

```shell
--model_type=bert --model_name_or_path=prev_trained_model/bert-base-chinese --task_name="tnews" --do_train --do_eval --do_lower_case --data_dir=./chineseGLUEdatasets/tnews --max_seq_length=128 --per_gpu_train_batch_size=16 --per_gpu_eval_batch_size=16 --learning_rate=2e-5 --num_train_epochs=4.0 --logging_steps=100 --save_steps=100 --output_dir=./outputs/tnews_output/ --overwrite_output_dir
```
ç„¶åå¯åŠ¨ run_classifier.py æ–‡ä»¶è¿›è¡Œè°ƒè¯•å³å¯ , æ‰€å‚è€ƒæºä»“åº“æœªæä¾›requirements.txtæ–‡ä»¶ï¼Œå› æ­¤éœ€è¦å¤§å®¶è‡ªè¡Œå®Œæˆè¿è¡Œæ—¶ç¼ºå¤±ä¾èµ–åŒ…çš„å®‰è£…ã€‚


## æ•°æ®é¢„å¤„ç†

1. è¾“å…¥æ•°æ®æ ¼å¼

```json
{
  "guid": "train-0",
  "label": "104",              // æ–‡æœ¬åˆ†ç±»ä»»åŠ¡: æ–‡æœ¬å¯¹åº”çš„æ ‡ç­¾
  "text_a": "è‚¡ç¥¨ä¸­çš„çªç ´å½¢æ€", 
  "text_b": null               // NSPä»»åŠ¡: ç”¨äºåˆ¤æ–­ç»™å‡ºçš„ä¸¤ä¸ªå¥å­æ˜¯å¦è¿ç»­
}
```
> NSP (Next Sentence Prediction)

2. æ–‡æœ¬åˆ†è¯ & å€ŸåŠ©å­—å…¸æ˜ å°„ä¸ºword id

```json
"è‚¡ç¥¨ä¸­çš„çªç ´å½¢æ€" --> ['è‚¡', 'ç¥¨', 'ä¸­', 'çš„', 'çª', 'ç ´', 'å½¢', 'æ€'] --> [5500, 4873, 704, 4638, 4960, 4788, 2501, 2578]
```
> å¯¹äºå­—å…¸ä¸­ä¸å­˜åœ¨çš„è¯ , ç”¨ `[UNK]` è¡¨ç¤º, å¯¹åº”çš„idä¸º 100

3. è¿‡é•¿æˆªæ–­ç­–ç•¥

4. æ·»åŠ ç‰¹æ®ŠTokenæ ‡è®°

![åŸåºåˆ—æ·»åŠ ç‰¹æ®ŠTokenæ ‡è®°å›¾](å›¾è§£BERT/1.png)

```json
[101, 5500, 4873, 704, 4638, 4960, 4788, 2501, 2578, 102]
```

> BertTokenizerä¸­çš„ç‰¹æ®Štoken id:
> - `[CLS]`: 101
> - `[SEP]`: 102
> - `[MASK]`: 103
> - `[UNK]`: 100
> - `[PAD]`: 0

```python
    # BertTokenizer
    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):
        if token_ids_1 is None:
            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]
        cls = [self.cls_token_id]
        sep = [self.sep_token_id]
        return cls + token_ids_0 + sep + token_ids_1 + sep
```
5. åˆ›å»ºå¥å­è¾¨è¯†åˆ—è¡¨ï¼Œç”¨ä»¥åŒºåˆ†ä¸åŒçš„å¥å­

![token_type_idsä½œç”¨å›¾è§£](å›¾è§£BERT/2.png)

```python
     # BertTokenizer
     def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):
        """
        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.
        A BERT sequence pair mask has the following format:
        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1
        | first sequence    | second sequence

        if token_ids_1 is None, only returns the first portion of the mask (0's).
        """
        sep = [self.sep_token_id]
        cls = [self.cls_token_id]
        if token_ids_1 is None:
            return len(cls + token_ids_0 + sep) * [0]
        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]
```
6. åˆ›å»ºç”¨ä»¥åŒºåˆ†special tokenséƒ¨åˆ†çš„maskåˆ—è¡¨

![special_tokens_maskä½œç”¨å›¾è§£](å›¾è§£BERT/3.png)

```python
    # BertTokenizer
    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):
        if token_ids_1 is not None:
            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]
        return [1] + ([0] * len(token_ids_0)) + [1]
```
7. è¶…é•¿æˆªæ–­

```python
       # PreTrainedTokenizer
       if max_length and len(encoded_inputs["input_ids"]) > max_length:
            encoded_inputs["input_ids"] = encoded_inputs["input_ids"][:max_length]
            encoded_inputs["token_type_ids"] = encoded_inputs["token_type_ids"][:max_length]
            encoded_inputs["special_tokens_mask"] = encoded_inputs["special_tokens_mask"][:max_length]
```

8. ç”Ÿæˆpaddingéƒ¨åˆ†çš„maskåˆ—è¡¨

![attention_maskä½œç”¨å›¾è§£](å›¾è§£BERT/4.png)
```python
        # ç”Ÿæˆæ³¨æ„åŠ›æ©ç ï¼ŒçœŸå®tokenå¯¹åº”1ï¼Œå¡«å……tokenå¯¹åº”0
        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)
 ```

 9. æ‰€æœ‰åºåˆ—éƒ½å¡«å……åˆ°max_lengthé•¿åº¦,ä¸è¶³é•¿åº¦ç”¨paddingå¡«å……

![å¡«å……è¿‡ç¨‹å›¾](å›¾è§£BERT/5.png)

```python
        # è®°å½•è¾“å…¥é•¿åº¦
        input_len = len(input_ids)
        # è®¡ç®—éœ€è¦å¡«å……çš„é•¿åº¦ --- æ‰€æœ‰è¾“å…¥åºåˆ—ç­‰é•¿ï¼Œéƒ½ç­‰äºmax_length
        padding_length = max_length - len(input_ids)
        # å³å¡«å……
        input_ids = input_ids + ([pad_token] * padding_length)
        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)
        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)
```

10. æ•°æ®é›†ä¸­æ¯ä¸€ä¸ªæ ·æœ¬æœ€ç»ˆéƒ½ä¼šè§£æå¾—åˆ°ä¸€ä¸ªInputFeatures

![InputFeaturesç»„æˆå›¾è§£](å›¾è§£BERT/6.png)

```python
features.append(
            InputFeatures(input_ids=input_ids,
                          attention_mask=attention_mask,
                          token_type_ids=token_type_ids,
                          label=label,
                          input_len=input_len))
```
> label æ˜¯å½“å‰æ–‡æœ¬å¯¹åº”çš„ç±»åˆ«æ ‡ç­¾
> input_len æ˜¯åºåˆ—å®é™…é•¿åº¦(å«special tokens)

11. æ•°æ®é›†é¢„å¤„ç†å®Œåï¼Œå°†InputFeatures Liståˆ—è¡¨ç»„è£…èµ·æ¥å¾—åˆ°éœ€è¦çš„DataSet

```python
dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_lens,all_labels)
```

## æ¨¡å‹æ¶æ„

### DataLoader

```python
    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)
    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size,collate_fn=collate_fn)
```
DataLoader è®¾ç½®çš„å›è°ƒæ–¹æ³•cllote_fnè´Ÿè´£å¯¹è¿”å›çš„ä¸€ä¸ªbatchï¼Œåœ¨è¿”å›å‰è¿›è¡Œé¢„å¤„ç†:

```python
def collate_fn(batch):
    all_input_ids, all_attention_mask, all_token_type_ids, all_lens, all_labels = map(torch.stack, zip(*batch))
    max_len = max(all_lens).item() # è®¡ç®—å½“å‰æ‰¹æ¬¡ä¸­æ‰€æœ‰åºåˆ—çš„å®é™…æœ€å¤§é•¿åº¦
    all_input_ids = all_input_ids[:, :max_len] # æŒ‰ç…§æœ¬æ‰¹æ¬¡åºåˆ—ä¸­æœ€å¤§é•¿åº¦è¿›è¡Œæˆªæ–­: max_length --> max_len
    all_attention_mask = all_attention_mask[:, :max_len]
    all_token_type_ids = all_token_type_ids[:, :max_len]
    return all_input_ids, all_attention_mask, all_token_type_ids, all_labels
```

### BertEmbeddings 

![input embeddings =  token embeddings + segmentation embeddings + position embeddings](å›¾è§£BERT/7.png)

```python
class BertEmbeddings(nn.Module):
    def __init__(self, config):
        super(BertEmbeddings, self).__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)
        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, input_ids, token_type_ids=None, position_ids=None):
        seq_length = input_ids.size(1)
        if position_ids is None: 
            # ä¸ºå½“å‰æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªåºåˆ—æ ·æœ¬ç”Ÿæˆä¸€ä¸ªä½ç½®åºåˆ—: (1,2,3,4,5,...) , æ„æˆä¸€ä¸ªä½ç½®åºåˆ—çŸ©é˜µ
            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)
            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)

        words_embeddings = self.word_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids) # ä½ç½®ç¼–ç ä¸ºå¯å­¦ä¹ çš„çŸ©é˜µ
        token_type_embeddings = self.token_type_embeddings(token_type_ids) # è®©æ¨¡å‹è‡ªå·±å­¦ä¼šåŒºåˆ†ä¸åŒçš„å¥å­

        embeddings = words_embeddings + position_embeddings + token_type_embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings
```

![åµŒå…¥å‘é‡ç”Ÿæˆè¿‡ç¨‹å›¾](å›¾è§£BERT/8.png)

### BertEncoder

#### BertLayer

![BertLayeræ¨¡å‹ç»“æ„å›¾](å›¾è§£BERT/9.png)

```python
class BertIntermediate(nn.Module):
    def __init__(self, config):
        super(BertIntermediate, self).__init__()
        self.dense = nn.Linear(config.hidden_size, config.intermediate_size) # (768,3072)
        # æ¿€æ´»å‡½æ•° - GLEU
        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):
            self.intermediate_act_fn = ACT2FN[config.hidden_act]
        else:
            self.intermediate_act_fn = config.hidden_act

    def forward(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)  # æ¿€æ´»å‡½æ•° - GLEU
        return hidden_states

class BertOutput(nn.Module):
    def __init__(self, config):
        super(BertOutput, self).__init__()
        self.dense = nn.Linear(config.intermediate_size, config.hidden_size) # (3072,768)
        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states

class BertLayer(nn.Module):
    def __init__(self, config):
        super(BertLayer, self).__init__()
        self.attention = BertAttention(config)
        self.intermediate = BertIntermediate(config)
        self.output = BertOutput(config)

    def forward(self, hidden_states, attention_mask=None):
        attention_output = self.attention(hidden_states, attention_mask)
        intermediate_output = self.intermediate(attention_output)
        layer_output = self.output(intermediate_output, attention_output)
        return layer_output
```

#### BertEncoder

![BertEncoderæ¨¡å‹ç»“æ„å›¾](å›¾è§£BERT/11.png)

```python
class BertEncoder(nn.Module):
    def __init__(self, config):
        super(BertEncoder, self).__init__()
        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])

    def forward(self, hidden_states, attention_mask=None, head_mask=None):
        for i, layer_module in enumerate(self.layer):
            hidden_states = layer_module(hidden_states, attention_mask, head_mask[i])
        return hidden_states
```

### BertPooler

![BertPooleræ¨¡å‹ç»“æ„å›¾](å›¾è§£BERT/10.png)

```python
class BertPooler(nn.Module):
    def __init__(self, config):
        super(BertPooler, self).__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states):
        # We "pool" the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0] # CLS Token Context Embeddings
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output
```
### BertModel


![BertModelæ¨¡å‹ç»“æ„å›¾](å›¾è§£BERT/12.png)

```python
class BertModel(BertPreTrainedModel):
    def __init__(self, config):
        super(BertModel, self).__init__(config)
        self.embeddings = BertEmbeddings(config)
        self.encoder = BertEncoder(config)
        self.pooler = BertPooler(config)
        self.init_weights()

    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):
        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility
        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0

        embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids)
        sequence_output = self.encoder(embedding_output,
                                       extended_attention_mask, # padding mask
                                      )
        pooled_output = self.pooler(sequence_output)

        outputs = (sequence_output, pooled_output,)
        return outputs
```

### BertForSequenceClassification

![BertForSequenceClassificationæ¨¡å‹ç»“æ„å›¾](å›¾è§£BERT/13.png)

```python
class BertForSequenceClassification(BertPreTrainedModel):
    def __init__(self, config):
        super(BertForSequenceClassification, self).__init__(config)
        self.num_labels = config.num_labels
        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)

        self.init_weights()

    def forward(self, input_ids, attention_mask=None, token_type_ids=None,
                position_ids=None, head_mask=None, labels=None):

        outputs = self.bert(input_ids,
                            attention_mask=attention_mask, # padding mask
                            token_type_ids=token_type_ids,
                            position_ids=position_ids, 
                            head_mask=head_mask) # None ?

        pooled_output = outputs[1] # å¯¹äºåˆ†ç±»ä»»åŠ¡æ¥è¯´ï¼Œåªéœ€è¦å»é™¤CLS Tokenç”¨äºåˆ†ç±»ä»»åŠ¡å³å¯

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here

        if labels is not None:
            if self.num_labels == 1:
                #  We are doing regression
                loss_fct = MSELoss()
                loss = loss_fct(logits.view(-1), labels.view(-1))
            else:
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            outputs = (loss,) + outputs

        return outputs  # (loss), logits, (hidden_states), (attentions)
```

### BertAttention

#### BertSelfAttention

![å¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—æµç¨‹å›¾](å›¾è§£BERT/14.png)

```python
class BertSelfAttention(nn.Module):
    def __init__(self, config):
        super(BertSelfAttention, self).__init__()
        self.output_attentions = config.output_attentions

        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)

        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, hidden_states, attention_mask=None, head_mask=None):
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(hidden_states)
        mixed_value_layer = self.value(hidden_states)
        # view æˆå¤šå¤´æ ¼å¼: (batch,heads,seq_len,d_k)
        query_layer = self.transpose_for_scores(mixed_query_layer)
        key_layer = self.transpose_for_scores(mixed_key_layer)
        value_layer = self.transpose_for_scores(mixed_value_layer)

        # Take the dot product between "query" and "key" to get the raw attention scores.
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) # (batch,heads,d_k,seq_len)
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
        if attention_mask is not None:
            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)
            attention_scores = attention_scores + attention_mask

        # Normalize the attention scores to probabilities.
        attention_probs = nn.Softmax(dim=-1)(attention_scores)

        # This is actually dropping out entire tokens to attend to, which might
        # seem a bit unusual, but is taken from the original Transformer paper.
        attention_probs = self.dropout(attention_probs)

        context_layer = torch.matmul(attention_probs, value_layer)

        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape) # åˆå¹¶å¤´ç»“æœ
        return context_layer
```

#### BertSelfOutput

![BertSelfOutputè®¡ç®—æµç¨‹å›¾](å›¾è§£BERT/15.png)

```python
class BertSelfOutput(nn.Module):
    def __init__(self, config):
        super(BertSelfOutput, self).__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
    
    # æ®‹å·®é“¾æ¥ + å±‚å½’ä¸€åŒ–
    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states
```

#### BertAttention

![BertAttentionè®¡ç®—æµç¨‹å›¾](å›¾è§£BERT/16.png)

```python
class BertAttention(nn.Module):
    def __init__(self, config):
        super(BertAttention, self).__init__()
        self.self = BertSelfAttention(config)
        self.output = BertSelfOutput(config)

    def forward(self, input_tensor, attention_mask=None):
        self_outputs = self.self(input_tensor, attention_mask) # å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
        attention_output = self.output(self_outputs, input_tensor)
        return attention_output
```

## é¢„è®­ç»ƒ

![é¢„è®­ç»ƒä¸å¾®è°ƒ](å›¾è§£BERT/22.png)

### BertPredictionHeadTransform

![BertPredictionHeadTransformç»“æ„å›¾](å›¾è§£BERT/17.png)

```python
class BertPredictionHeadTransform(nn.Module):
    def __init__(self, config):
        super(BertPredictionHeadTransform, self).__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):
            self.transform_act_fn = ACT2FN[config.hidden_act]
        else:
            self.transform_act_fn = config.hidden_act
        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)

    def forward(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.transform_act_fn(hidden_states)
        hidden_states = self.LayerNorm(hidden_states)
        return hidden_states
```       

### BertLMPredictionHead

![BertLMPredictionHeadç»“æ„å›¾](å›¾è§£BERT/18.png)

```python
class BertLMPredictionHead(nn.Module):
    def __init__(self, config):
        super(BertLMPredictionHead, self).__init__()
        self.transform = BertPredictionHeadTransform(config)

        # The output weights are the same as the input embeddings, but there is
        # an output-only bias for each token.
        self.decoder = nn.Linear(config.hidden_size,
                                 config.vocab_size,
                                 bias=False)

        self.bias = nn.Parameter(torch.zeros(config.vocab_size))

    def forward(self, hidden_states):
        hidden_states = self.transform(hidden_states)
        hidden_states = self.decoder(hidden_states) + self.bias
        return hidden_states
```        

### BertPreTrainingHeads

![BertPreTrainingHeadsç»“æ„å›¾](å›¾è§£BERT/19.png)

```python
class BertPreTrainingHeads(nn.Module):
    def __init__(self, config):
        super(BertPreTrainingHeads, self).__init__()
        self.predictions = BertLMPredictionHead(config)
        self.seq_relationship = nn.Linear(config.hidden_size, 2)

    def forward(self, sequence_output, pooled_output):
        prediction_scores = self.predictions(sequence_output) #
        seq_relationship_score = self.seq_relationship(pooled_output) # ä¸¤ä¸ªå¥å­æ˜¯å¦ä¸ºä¸Šä¸‹å¥å…³ç³»
        return prediction_scores, seq_relationship_score
```

### BertForPreTraining

![BertForPreTrainingç»“æ„å›¾](å›¾è§£BERT/20.png)

```python
class BertForPreTraining(BertPreTrainedModel):
    def __init__(self, config):
        super(BertForPreTraining, self).__init__(config)
        self.bert = BertModel(config)
        self.cls = BertPreTrainingHeads(config)

    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,
                masked_lm_labels=None, next_sentence_label=None):

        outputs = self.bert(input_ids,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids,
                            position_ids=position_ids, 
                            head_mask=head_mask)

        sequence_output, pooled_output = outputs[:2] # éšè—å±‚è¾“å‡º,CLS Token Embeddings
        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)

        outputs = (prediction_scores, seq_relationship_score,)
        # è®¡ç®—æ©ç è¯­è¨€æŸå¤± å’Œ ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹æŸå¤±
        if masked_lm_labels is not None and next_sentence_label is not None:
            loss_fct = CrossEntropyLoss(ignore_index=-1)
            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))
            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))
            total_loss = masked_lm_loss + next_sentence_loss
            outputs = (total_loss,) + outputs

        return outputs  # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)
```

## å…¶ä»–ä¸‹æ¸¸ä»»åŠ¡

![Bertæ”¯æŒçš„ä¸‹æ¸¸ä»»åŠ¡å›¾](å›¾è§£BERT/21.png)

### é—®ç­”ä»»åŠ¡

åœ¨ BERT çš„é—®ç­”ä»»åŠ¡ä¸­ï¼Œå…¸å‹çš„è¾“å…¥æ˜¯ä¸€ä¸ªåŒ…å« **é—®é¢˜ï¼ˆQuestionï¼‰** å’Œ **ä¸Šä¸‹æ–‡ï¼ˆContextï¼‰** çš„æ–‡æœ¬å¯¹ã€‚ä¾‹å¦‚ï¼š

> **é—®é¢˜**: â€œè°å†™äº†ã€Šå“ˆå§†é›·ç‰¹ã€‹ï¼Ÿâ€  
> **ä¸Šä¸‹æ–‡**: â€œèå£«æ¯”äºšæ˜¯è‹±å›½æ–‡å­¦å²ä¸Šæœ€ä¼Ÿå¤§çš„ä½œå®¶ä¹‹ä¸€ï¼Œä»–å†™äº†åŒ…æ‹¬ã€Šå“ˆå§†é›·ç‰¹ã€‹ã€ã€Šéº¦å…‹ç™½ã€‹ç­‰è‘—åæ‚²å‰§ã€‚â€

1. è¾“å…¥æ ¼å¼ï¼ˆTokenization åçš„å½¢å¼ï¼‰ï¼Œåœ¨ä½¿ç”¨ `BertTokenizer` ç¼–ç åï¼Œè¾“å…¥ä¼šå˜æˆå¦‚ä¸‹ç»“æ„ï¼š

```json
[CLS] é—®é¢˜ tokens [SEP] ä¸Šä¸‹æ–‡ tokens [SEP]
```
2. BERT çš„è¾“å‡ºï¼ˆOutputsï¼‰ï¼Œé€šè¿‡è°ƒç”¨ `self.bert(...)`ï¼Œä½ å°†å¾—åˆ°ä¸€ä¸ªåŒ…å«å¤šä¸ªå…ƒç´ çš„ tuple è¾“å‡ºï¼š

```python
outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
```

è¿”å›å€¼å½¢å¦‚ï¼š

```python
(
    sequence_output,          # (batch_size, seq_length, hidden_size)
    pooled_output,            # (batch_size, hidden_size)
)
```
ä¸»è¦è¾“å‡ºé¡¹è§£é‡Š:

âœ… `sequence_output`: æœ€ç»ˆæ¯ä¸ª token çš„è¡¨ç¤º

- å½¢çŠ¶ï¼š`(batch_size, seq_length, hidden_size)`
- æ˜¯æ¨¡å‹æœ€åä¸€å±‚æ‰€æœ‰ tokenï¼ˆåŒ…æ‹¬é—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼‰çš„éšè—çŠ¶æ€ã€‚
- åœ¨é—®ç­”ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦ä½¿ç”¨å®ƒæ¥é¢„æµ‹ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®ã€‚

âœ… `pooled_output`: å¥å­çº§åˆ«è¡¨ç¤ºï¼ˆä¸å¸¸ç”¨ï¼‰

- å½¢çŠ¶ï¼š`(batch_size, hidden_size)`
- æ˜¯ `[CLS]` token ç»è¿‡ä¸€å±‚å…¨è¿æ¥åçš„è¾“å‡ºã€‚
- åœ¨åˆ†ç±»ä»»åŠ¡ä¸­æ›´æœ‰ç”¨ï¼Œåœ¨é—®ç­”ä»»åŠ¡ä¸­ä¸€èˆ¬ä¸ä¼šä½¿ç”¨è¿™ä¸ªè¾“å‡ºã€‚

3. å¦‚ä½•åˆ©ç”¨ BERT è¾“å‡ºåšé—®ç­”é¢„æµ‹ï¼Ÿ

åœ¨ `BertForQuestionAnswering` ä¸­ï¼Œä½¿ç”¨äº†å¦‚ä¸‹é€»è¾‘ï¼š

```python
logits = self.qa_outputs(sequence_output)  # (batch_size, seq_length, 2)
start_logits, end_logits = logits.split(1, dim=-1)  # split into start and end
start_logits = start_logits.squeeze(-1)  # (batch_size, seq_length)
end_logits = end_logits.squeeze(-1)
```
`qa_outputs` å±‚çš„ä½œç”¨ï¼š
- æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼š`nn.Linear(config.hidden_size, 2)`
- å°†æ¯ä¸ª token çš„ `hidden_size` å‘é‡æ˜ å°„æˆä¸¤ä¸ªåˆ†æ•°ï¼šä¸€ä¸ªæ˜¯è¯¥ token ä½œä¸ºç­”æ¡ˆå¼€å§‹çš„å¯èƒ½æ€§ï¼Œå¦ä¸€ä¸ªæ˜¯ä½œä¸ºç­”æ¡ˆç»“æŸçš„å¯èƒ½æ€§ã€‚

è¾“å‡ºè§£é‡Šï¼š
- `start_logits`: æ¯ä¸ª token æ˜¯ç­”æ¡ˆèµ·ç‚¹çš„å¾—åˆ†ï¼ˆæœªå½’ä¸€åŒ–ï¼‰ã€‚
- `end_logits`: æ¯ä¸ª token æ˜¯ç­”æ¡ˆç»ˆç‚¹çš„å¾—åˆ†ã€‚

æ¯”å¦‚å¯¹äºä¸€ä¸ªé•¿åº¦ä¸º 128 çš„åºåˆ—ï¼Œæ¯ä¸ª token éƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„ start/end åˆ†æ•°ï¼š

```python
start_scores = torch.softmax(start_logits, dim=-1)  # softmax å¾—åˆ°æ¦‚ç‡
end_scores = torch.softmax(end_logits, dim=-1)

# æ‰¾å‡ºæœ€å¯èƒ½æ˜¯ start å’Œ end çš„ä½ç½®
start_index = torch.argmax(start_scores)
end_index = torch.argmax(end_scores)
```

å¦‚æœ `start_index <= end_index`ï¼Œé‚£ä¹ˆå¯ä»¥ç»„åˆè¿™ä¸¤ä¸ªç´¢å¼•å¾—åˆ°ç­”æ¡ˆ spanã€‚


#### ä»£ç å®ç°

```python
class BertForQuestionAnswering(BertPreTrainedModel):
    def __init__(self, config):
        super(BertForQuestionAnswering, self).__init__(config)
        self.num_labels = config.num_labels # é€šå¸¸æ˜¯ 2ï¼Œå³ start å’Œ end
        self.bert = BertModel(config)
        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)

    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,
                start_positions=None, end_positions=None):

        outputs = self.bert(input_ids,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids,
                            position_ids=position_ids)

        sequence_output = outputs[0]
        # (batch,seq_len,hidden_size) ---> (batch,seq_len,2)
        logits = self.qa_outputs(sequence_output)

        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits = start_logits.squeeze(-1) # (batch,seq_len)
        end_logits = end_logits.squeeze(-1)
        
        outputs = (start_logits, end_logits,)
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        if start_positions is not None and end_positions is not None:
            # sometimes the start/end positions are outside our model inputs, we ignore these terms
            # ignored_index = seq_len
            ignored_index = start_logits.size(1)
            # clamp_ æ˜¯ PyTorch ä¸­çš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºå°†å¼ é‡ä¸­çš„å€¼é™åˆ¶åœ¨æŒ‡å®šçš„èŒƒå›´å†…ã€‚
            # å®ƒçš„è¯­æ³•æ˜¯ tensor.clamp_(min, max) ï¼Œè¡¨ç¤ºå°†å¼ é‡ä¸­çš„å€¼é™åˆ¶åœ¨ min å’Œ max ä¹‹é—´ã€‚
            # å¦‚æœå€¼å°äº min ï¼Œåˆ™å°†å…¶è®¾ç½®ä¸º min ï¼›å¦‚æœå€¼å¤§äº max ï¼Œåˆ™å°†å…¶è®¾ç½®ä¸º max ã€‚
            start_positions.clamp_(0, ignored_index)
            end_positions.clamp_(0, ignored_index)

            # ignore_index: ç”¨äºæŒ‡å®šåœ¨è®¡ç®—æŸå¤±æ—¶å¿½ç•¥çš„æ ‡ç­¾ç´¢å¼•ã€‚ 
            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)
            # åˆ†åˆ«è®¡ç®—ç­”æ¡ˆèµ·å§‹ä¸‹æ ‡å’Œç»“æŸä¸‹æ ‡é¢„æµ‹å¾—åˆ°çš„äº¤å‰ç†µæŸå¤±
            start_loss = loss_fct(start_logits, start_positions)
            end_loss = loss_fct(end_logits, end_positions)
            total_loss = (start_loss + end_loss) / 2
            outputs = (total_loss,) + outputs

        return outputs  # (loss), start_logits, end_logits

```

#### æ˜“æ··æ·†

BERT æ˜¯ä¸€ä¸ª **åŸºäºä¸Šä¸‹æ–‡ç¼–ç ï¼ˆContextual Encoderï¼‰** çš„æ¨¡å‹ï¼Œä¸æ˜¯è‡ªå›å½’ç”Ÿæˆå™¨ã€‚å®ƒä¸ä¼šâ€œç”Ÿæˆâ€æ–°çš„æ–‡æœ¬ï¼Œè€Œæ˜¯å¯¹è¾“å…¥æ–‡æœ¬ä¸­æ¯ä¸ª token çš„è§’è‰²è¿›è¡Œåˆ†ç±»ï¼ˆå¦‚åˆ¤æ–­å“ªä¸ªæ˜¯ç­”æ¡ˆçš„å¼€å§‹ã€ç»“æŸï¼‰ã€‚æ‰€ä»¥æœ€ç»ˆçš„ç­”æ¡ˆåªèƒ½æ¥è‡ªåŸå§‹è¾“å…¥æ–‡æœ¬ä¸­çš„æŸä¸€æ®µå­ä¸²ã€‚

ğŸ“š è¯¦ç»†è§£é‡Š

1. âœ… BERT æ˜¯ä¸€ä¸ª Encoder-only æ¨¡å‹

- BERT åªåŒ…å« Transformer çš„ encoder éƒ¨åˆ†ã€‚

- å®ƒçš„ä½œç”¨æ˜¯ç»™å®šä¸€ä¸ªå®Œæ•´çš„å¥å­ï¼ˆæˆ–ä¸¤ä¸ªå¥å­ï¼‰ï¼Œå¯¹æ¯ä¸ª token ç”Ÿæˆä¸€ä¸ªä¸Šä¸‹æ–‡ç›¸å…³çš„è¡¨ç¤ºï¼ˆcontextualized representationï¼‰ã€‚

- å®ƒ**ä¸å…·æœ‰ç”Ÿæˆèƒ½åŠ›**ï¼Œä¸èƒ½åƒ GPT è¿™æ ·çš„ decoder-only æ¨¡å‹é‚£æ ·é€è¯ç”Ÿæˆæ–°å†…å®¹ã€‚

--- 

2. ğŸ” QA ä»»åŠ¡çš„æœ¬è´¨ï¼šå®šä½ç­”æ¡ˆ span è€Œéç”Ÿæˆç­”æ¡ˆ

åœ¨ SQuAD è¿™ç±»æŠ½å–å¼é—®ç­”ä»»åŠ¡ä¸­ï¼š

- ç­”æ¡ˆå¿…é¡»æ˜¯åŸæ–‡ä¸­çš„è¿ç»­ç‰‡æ®µï¼ˆspanï¼‰ã€‚

- æ‰€ä»¥æ¨¡å‹çš„ä»»åŠ¡æ˜¯ï¼š

  - ç»™å‡ºé—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼›

  - åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°æœ€å¯èƒ½çš„ç­”æ¡ˆèµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®ï¼›

  - æœ€ç»ˆç­”æ¡ˆå°±æ˜¯ä¸Šä¸‹æ–‡ä¸­è¿™ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„å­—ç¬¦ä¸²ã€‚

BERT åšçš„å°±æ˜¯è¿™ä¸ªå®šä½ä»»åŠ¡ï¼Œè€Œä¸æ˜¯é‡æ–°ç”Ÿæˆä¸€ä¸ªæ–°çš„ç­”æ¡ˆã€‚

--- 

3. ğŸ§© è¾“å…¥ä¸è¾“å‡ºçš„å…³ç³»

```python
answer_tokens = input_ids[0][start_index : end_index + 1]
answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)
```

è¿™æ®µä»£ç çš„æ„æ€æ˜¯ï¼š

- `start_index` å’Œ `end_index` æ˜¯æ¨¡å‹é¢„æµ‹å‡ºçš„ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®ã€‚

- æˆ‘ä»¬ä»åŸå§‹è¾“å…¥çš„ `input_ids` ä¸­å–å‡ºå¯¹åº”çš„ token ID å­åºåˆ—ã€‚

- ä½¿ç”¨ tokenizer æŠŠè¿™äº› token ID è§£ç æˆè‡ªç„¶è¯­è¨€æ–‡æœ¬ã€‚

- å¾—åˆ°çš„å°±æ˜¯ç­”æ¡ˆã€‚

è¿™å…¶å®å°±æ˜¯åœ¨è¯´ï¼š

> â€œæ ¹æ®ä½ çš„ç†è§£ï¼Œç­”æ¡ˆåº”è¯¥åœ¨è¿™æ®µæ–‡å­—ä¸­çš„ç¬¬ X åˆ°ç¬¬ Y ä¸ªè¯ä¹‹é—´ï¼Œè¯·æŠŠè¿™éƒ¨åˆ†åŸæ–‡å‘Šè¯‰æˆ‘ã€‚â€

---

4. ğŸ§ª ä¸¾ä¸ªä¾‹å­

å‡è®¾åŸå§‹ä¸Šä¸‹æ–‡æ˜¯ï¼š

```
The capital of France is Paris.
```

ç»è¿‡ Tokenizer ç¼–ç åå¯èƒ½æ˜¯ï¼š

```
[CLS] the capital of france is paris [SEP]
```
å¦‚æœæ¨¡å‹é¢„æµ‹ start_index=5ï¼Œend_index=5ï¼Œé‚£ä¹ˆå¯¹åº”çš„å°±æ˜¯å•è¯ `"paris"`ï¼Œè¿™å°±æ˜¯ç­”æ¡ˆã€‚

--- 

âš ï¸ æ³¨æ„äº‹é¡¹

1. **ä¸èƒ½è¶…å‡ºä¸Šä¸‹æ–‡èŒƒå›´**
   - start/end positions å¿…é¡»è½åœ¨ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼ˆå³ token_type_id == 1 çš„åŒºåŸŸï¼‰ã€‚
   - å¦åˆ™ç­”æ¡ˆå¯èƒ½ä¸åˆç†ï¼ˆæ¯”å¦‚å–åˆ°äº†é—®é¢˜éƒ¨åˆ†çš„å†…å®¹ï¼‰ã€‚

2. **ç‰¹æ®Š token ä¸è®¡å…¥ç­”æ¡ˆ**
   - `[CLS]`, `[SEP]` ç­‰ä¼šè¢« `skip_special_tokens=True` è‡ªåŠ¨è·³è¿‡ã€‚

3. **æ— æ³•å¤„ç†ä¸åœ¨åŸæ–‡ä¸­çš„ç­”æ¡ˆ**
   - å¦‚æœæ­£ç¡®ç­”æ¡ˆæ²¡æœ‰å‡ºç°åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼ŒBERT æ— æ³•â€œç¼–é€ â€å‡ºæ¥ã€‚
   - è¿™æ˜¯æŠ½å–å¼é—®ç­”æ¨¡å‹çš„å±€é™æ€§ã€‚

---

ğŸ’¡ å¯¹æ¯”ï¼šç”Ÿæˆå¼ vs æŠ½å–å¼é—®ç­”

| ç±»å‹ | æ¨¡å‹ä»£è¡¨ | æ˜¯å¦èƒ½ç”Ÿæˆæ–°æ–‡æœ¬ | ç­”æ¡ˆæ˜¯å¦å¿…é¡»åœ¨åŸæ–‡ä¸­ | ç¤ºä¾‹ |
|------|----------|------------------|-----------------------|------|
| æŠ½å–å¼ | BERT | âŒ | âœ… | ç­”æ¡ˆæ˜¯åŸæ–‡ä¸­çš„ä¸€æ®µ |
| ç”Ÿæˆå¼ | T5 / BART / GPT | âœ… | âŒ | ç­”æ¡ˆå¯ä»¥æ˜¯ä»»æ„æ–‡æœ¬ |

å¦‚æœä½ å¸Œæœ›æ¨¡å‹èƒ½â€œè‡ªå·±å†™ç­”æ¡ˆâ€ï¼Œé‚£å°±éœ€è¦ä½¿ç”¨ç”Ÿæˆå¼æ¨¡å‹ã€‚

---

âœ… æ€»ç»“

| é—®é¢˜ | å›ç­” |
|------|------|
| ä¸ºä»€ä¹ˆç­”æ¡ˆæ¥è‡ª `input_ids`ï¼Ÿ | å› ä¸º BERT æ˜¯ç¼–ç å™¨æ¨¡å‹ï¼ŒåªåšæŠ½å–å¼é—®ç­”ï¼Œç­”æ¡ˆå¿…é¡»æ˜¯åŸæ–‡ä¸­çš„ä¸€æ®µæ–‡æœ¬ã€‚ |
| BERT èƒ½ä¸èƒ½è‡ªå·±ç”Ÿæˆç­”æ¡ˆï¼Ÿ | ä¸èƒ½ï¼ŒBERT ä¸å…·å¤‡ç”Ÿæˆèƒ½åŠ›ï¼Œåªèƒ½å¯¹è¾“å…¥æ–‡æœ¬ä¸­çš„ token åšåˆ†ç±»ã€‚ |
| å¦‚ä½•è·å–ç­”æ¡ˆï¼Ÿ | æ ¹æ®é¢„æµ‹çš„ start/end indexï¼Œä» `input_ids` ä¸­æå– tokenï¼Œå¹¶ç”¨ tokenizer è§£ç æˆè‡ªç„¶è¯­è¨€ã€‚ |


### Tokenåˆ†ç±»ä»»åŠ¡

Token åˆ†ç±»ä»»åŠ¡æ˜¯æŒ‡å¯¹è¾“å…¥æ–‡æœ¬ä¸­çš„æ¯ä¸ª token è¿›è¡Œåˆ†ç±»ï¼Œå¸¸è§çš„åº”ç”¨åœºæ™¯åŒ…æ‹¬ï¼š

- å‘½åå®ä½“è¯†åˆ« (NER)
- è¯æ€§æ ‡æ³¨ (POS)
- è¯­ä¹‰è§’è‰²æ ‡æ³¨ (SRL)

```python
class BertForTokenClassification(BertPreTrainedModel):
    def __init__(self, config):
        super(BertForTokenClassification, self).__init__(config)
        self.num_labels = config.num_labels
        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

    def forward(self, input_ids, attention_mask=None, token_type_ids=None,
                position_ids=None, head_mask=None, labels=None):

        outputs = self.bert(input_ids,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids,
                            position_ids=position_ids, 
                            head_mask=head_mask)

        sequence_output = outputs[0] # (batch,seq_len,hidden_size)

        sequence_output = self.dropout(sequence_output)
        logits = self.classifier(sequence_output) # ï¼ˆbatch,seq_len,num_labelsï¼‰

        outputs = (logits,)
        if labels is not None:
            loss_fct = CrossEntropyLoss()
            # Only keep active parts of the loss
            if attention_mask is not None:
                active_loss = attention_mask.view(-1) == 1
                active_logits = logits.view(-1, self.num_labels)[active_loss]
                active_labels = labels.view(-1)[active_loss]
                loss = loss_fct(active_logits, active_labels)
            else:
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            outputs = (loss,) + outputs

        return outputs  # (loss), scores
```
### å¤šé¡¹é€‰æ‹©ä»»åŠ¡

å¤šé¡¹é€‰æ‹©ä»»åŠ¡æ˜¯æŒ‡ç»™å®šä¸€ä¸ªé—®é¢˜å’Œå¤šä¸ªå€™é€‰ç­”æ¡ˆï¼Œæ¨¡å‹éœ€è¦ä»ä¸­é€‰æ‹©æœ€åˆé€‚çš„ç­”æ¡ˆã€‚å¸¸è§çš„åº”ç”¨åœºæ™¯åŒ…æ‹¬ï¼š

- é˜…è¯»ç†è§£ä»»åŠ¡

- é—®ç­”ç³»ç»Ÿä¸­çš„å€™é€‰ç­”æ¡ˆé€‰æ‹©

- å¯¹è¯ç³»ç»Ÿä¸­çš„å€™é€‰å›å¤é€‰æ‹©


åœ¨ å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMultiple Choiceï¼‰ ä»»åŠ¡ä¸­ï¼ŒBERT çš„è¾“å…¥ç»„ç»‡å½¢å¼ä¸æ™®é€šåˆ†ç±»æˆ–é—®ç­”ä»»åŠ¡ç•¥æœ‰ä¸åŒã€‚ä½ éœ€è¦ä¸ºæ¯ä¸ªé€‰é¡¹åˆ†åˆ«æ„é€ ä¸€ä¸ªå®Œæ•´çš„ BERT è¾“å…¥åºåˆ—ï¼Œå¹¶å°†å®ƒä»¬ç»„åˆæˆä¸€ä¸ªæ‰¹æ¬¡è¿›è¡Œå¤„ç†ã€‚

âœ… å‡è®¾ä½ æœ‰ä¸€ä¸ªé—®é¢˜ + 4 ä¸ªé€‰é¡¹ï¼š

```json
é—®é¢˜ï¼šè°å†™äº†ã€Šå“ˆå§†é›·ç‰¹ã€‹ï¼Ÿ
A. é›¨æœ
B. æ­Œå¾·
C. èå£«æ¯”äºš
D. æ‰˜å°”æ–¯æ³°
```

å¯¹äºè¿™æ ·çš„å¤šé€‰é—®é¢˜ï¼ŒBERT çš„è¾“å…¥æ–¹å¼æ˜¯ï¼š

å¯¹æ¯ä¸€ä¸ªé€‰é¡¹ï¼Œéƒ½å•ç‹¬æ„é€ ä¸€ä¸ª `[CLS] + é—®é¢˜ + [SEP] + é€‰é¡¹å†…å®¹ + [SEP]` çš„è¾“å…¥åºåˆ—ã€‚ 

ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¨¡å‹ä¼šå¯¹æ¯ä¸ªé€‰é¡¹åˆ†åˆ«ç¼–ç  ï¼Œç„¶åä»ä¸­é€‰å‡ºæœ€åˆé€‚çš„é‚£ä¸ªã€‚

```python
class BertForMultipleChoice(BertPreTrainedModel):
    def __init__(self, config):
        super(BertForMultipleChoice, self).__init__(config)
        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, 1)

    def forward(self, input_ids, attention_mask=None, token_type_ids=None,
                position_ids=None, head_mask=None, labels=None):
        # è·å–é€‰é¡¹ä¸ªæ•°        
        num_choices = input_ids.shape[1] # (batch_size, num_choices, seq_length)
        # å°†é€‰é¡¹å±•å¹³ï¼Œä»¥ä¾¿ä¸€èµ·å¤„ç†: (batch_size * num_choices, seq_length)
        input_ids = input_ids.view(-1, input_ids.size(-1))
        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None
        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None
        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None
        
        outputs = self.bert(input_ids,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids,
                            position_ids=position_ids,
                            head_mask=head_mask)

        pooled_output = outputs[1] # (batch_size * num_choices, hidden_size)

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output) # (batch_size * num_choices, 1)
        reshaped_logits = logits.view(-1, num_choices) # (batch_size , num_choices, 1)

        outputs = (reshaped_logits,)
       
        if labels is not None:
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(reshaped_logits, labels)
            outputs = (loss,) + outputs

        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)
```
åœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œä¼šå°†è¿™äº›è¾“å…¥å±•å¹³ï¼Œå˜æˆï¼š

```python
input_ids.view(-1, seq_length)  # (batch_size * num_choices, seq_length)
```

è¿™æ ·å°±èƒ½è®© BERT å¯¹æ¯ä¸ªé€‰é¡¹åˆ†åˆ«è¿›è¡Œç¼–ç ã€‚

BERT è¾“å‡ºåï¼Œå†å¯¹æ¯ä¸ªé€‰é¡¹åšåˆ†ç±»æ‰“åˆ†ï¼Œæœ€åé‡æ–° reshape æˆ (batch_size, num_choices) å½¢å¼ï¼Œç”¨äºè®¡ç®—äº¤å‰ç†µæŸå¤±ã€‚

