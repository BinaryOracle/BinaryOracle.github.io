---
title: 书生·万象多模态大模型（InternVL 1.0）
icon: file
category:
  - 多模态
tag:
  - 多模态
  - 编辑中
footer: 技术共建，知识共享
date: 2025-06-15
cover: assets/cover/InternVL.png
author:
  - BinaryOracle
---

`InternVL 1.0: Scaling up Vision Foundation Models and Aligning  for Generic Visual-Linguistic Tasks 论文简析` 

<!-- more -->

> 论文链接: [https://arxiv.org/abs/2312.14238](https://arxiv.org/abs/2312.14238)
> 代码链接: [https://github.com/OpenGVLab/InternVL](https://github.com/OpenGVLab/InternVL)

## 摘要

InternVL是一个大规模视觉-语言基础模型，旨在解决当前视觉与视觉-语言基础模型发展滞后于大型语言模型（LLMs）的问题。该模型通过将视觉基础模型扩展到60亿参数，并利用多源网络图像-文本数据进行渐进式对齐训练，成功实现了视觉与语言模型在参数规模和特征表示上的协调。InternVL在32个通用视觉-语言任务中表现出色，包括图像分类、语义分割、视频分类、图像/视频-文本检索以及多模态对话系统等，展现了强大的视觉能力和与LLMs的无缝集成潜力，为多模态大模型的发展提供了重要贡献。

## 简介

1. **研究背景与问题**: 大型语言模型（LLMs）的快速发展推动了通用人工智能（AGI）系统的进步，但视觉和视觉-语言基础模型的发展却相对滞后。现有的视觉-语言大模型（VLLMs）通常使用轻量级的“胶水层”（如QFormer或线性投影）来对齐视觉和语言模型的特征，但这种方法存在三个主要限制：  
   
   - **参数规模不匹配**：LLMs的参数规模已达千亿级，而视觉编码器通常仅约10亿参数，限制了LLM的能力利用。  
   
   - **表征不一致**：视觉模型通常基于纯视觉数据或BERT系列模型训练，与LLMs的特征空间存在差异。  
   
   - **低效连接**：轻量级胶水层难以捕捉跨模态的复杂交互。  

2. **解决方案与核心设计**: 论文提出 **InternVL**，通过以下关键设计解决上述问题：  
   
   - **参数平衡的视觉与语言组件**：包含60亿参数的视觉编码器（InternViT-6B）和80亿参数的语言中间件（QLLaMA），后者作为强大的“胶水层”重组视觉特征。  
   
   - **一致的表征对齐**：使用多语言LLaMA初始化中间件，确保视觉编码器与LLMs的特征空间一致。  
   
   - **渐进式图像-文本对齐策略**：先在大规模噪声数据上对比学习，再在高质量数据上生成学习，逐步提升模型性能（如图1c所示）。  

![](InternVL-1.0/1.png)

3. **模型优势**  
   
   - **多功能性**：可作为独立视觉编码器或与语言中间件结合，支持感知、检索、生成和对话任务。  
   
   - **强大性能**：在ImageNet分类、ADE20K分割、视频检索等任务中达到SOTA（如图2所示）。  
   
   - **LLM友好性**：与LLaMA、Vicuna等LLMs无缝集成，推动多模态应用发展。  

![](InternVL-1.0/2.png)

## 相关工作

#### **1. 视觉基础模型（Vision Foundation Models）**
视觉基础模型在过去十年中经历了显著发展，从早期的AlexNet和CNN架构（如ResNet）到近年来的Vision Transformer（ViT）及其变体。ViT及其衍生模型（如ViT-G、EVA-02等）通过扩大模型规模和参数量，显著提升了视觉任务的性能。然而，当前广泛使用的视觉模型参数量仍停留在约10亿级别（如ViT-22B除外），远落后于LLMs的规模。此外，这些模型多基于纯视觉数据（如ImageNet、JFT）训练，或与BERT系列模型对齐，缺乏与LLMs的直接特征兼容性，限制了其在多模态任务中的表现。

#### **2. 大语言模型（Large Language Models, LLMs）**
LLMs（如GPT-3、LLaMA系列、Vicuna等）在自然语言处理领域取得了突破性进展，展示了强大的少样本和零样本学习能力。开源模型（如ChatGLM、Falcon等）的涌现进一步加速了多模态研究的进程。然而，LLMs本身缺乏视觉理解能力，如何将其与视觉模态结合成为关键挑战。

#### **3. 视觉大语言模型（Vision Large Language Models, VLLMs）**
近期研究通过将视觉模型与LLMs结合，构建了多模态对话系统（如Flamingo、LLaVA、MiniGPT-4等）。这些工作主要依赖轻量级适配层（如QFormer、线性投影）连接视觉编码器和LLM，但受限于视觉模型的规模和对齐效率。部分模型（如KOSMOS-2、Qwen-VL）进一步引入了视觉定位能力，支持区域描述和问答。尽管如此，视觉基础模型的性能瓶颈仍是制约VLLMs发展的关键因素。

#### **核心问题与本文定位**
现有工作表明，视觉模型的规模不足和对齐低效是阻碍多模态能力提升的主要障碍。InternVL通过**规模化视觉编码器**和**渐进式跨模态对齐**，首次实现了视觉与语言模型在参数和特征空间的深度协同，填补了这一领域的空白。

## 方法

