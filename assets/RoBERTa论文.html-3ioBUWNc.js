import{_ as s}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as t,e as i,f as o,b as l,o as r}from"./app-3KwXTIaH.js";const T="/assets/1-DFPTlpxI.png",e="/assets/2-BqH3BfT8.png",Q="/assets/3-BVr4hn_l.png",p="/assets/4-Dw80kE2X.png",g="/assets/5-Co5V6o3S.png",d="/assets/6-CqFK0Gq6.png",m="/assets/7-HTL5b7xW.png",u="/assets/8-e8ujHyJS.png",R={},h={class:"MathJax",jax:"SVG",style:{position:"relative"}},B={style:{"vertical-align":"-0.439ex"},xmlns:"http://www.w3.org/2000/svg",width:"8.177ex",height:"2.034ex",role:"img",focusable:"false",viewBox:"0 -705 3614.1 899","aria-hidden":"true"},E={class:"MathJax",jax:"SVG",style:{position:"relative"}},c={style:{"vertical-align":"-0.439ex"},xmlns:"http://www.w3.org/2000/svg",width:"10.439ex",height:"2.034ex",role:"img",focusable:"false",viewBox:"0 -705 4614.1 899","aria-hidden":"true"},f={class:"MathJax",jax:"SVG",style:{position:"relative"}},M={style:{"vertical-align":"-0.186ex"},xmlns:"http://www.w3.org/2000/svg",width:"10.018ex",height:"1.692ex",role:"img",focusable:"false",viewBox:"0 -666 4428 748","aria-hidden":"true"},x={class:"MathJax",jax:"SVG",style:{position:"relative"}},L={style:{"vertical-align":"-0.025ex"},xmlns:"http://www.w3.org/2000/svg",width:"0.919ex",height:"1ex",role:"img",focusable:"false",viewBox:"0 -431 406 442","aria-hidden":"true"},H={class:"MathJax",jax:"SVG",style:{position:"relative"}},S={style:{"vertical-align":"-0.439ex"},xmlns:"http://www.w3.org/2000/svg",width:"9.308ex",height:"2.034ex",role:"img",focusable:"false",viewBox:"0 -705 4114.1 899","aria-hidden":"true"};function w(b,a){return r(),n("div",null,[a[26]||(a[26]=t("p",null,[t("code",null,"RoBERTa 论文")],-1)),i(" more "),a[27]||(a[27]=o('<blockquote><p>论文链接: <a href="https://arxiv.org/abs/1907.11692" target="_blank" rel="noopener noreferrer">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></p></blockquote><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>RoBERTa是一项针对BERT预训练方法的优化研究，通过系统性的实验发现BERT存在训练不足的问题，并提出了一系列改进措施。这些改进包括更长的训练时间、更大的批次规模、更多的数据、移除下一句预测（NSP）目标、使用更长的序列以及动态调整掩码模式。实验结果表明，优化后的RoBERTa在多个基准测试（如GLUE、RACE和SQuAD）上达到了最先进的性能，甚至超越了后续提出的模型。研究强调了预训练中设计选择和数据规模的重要性，同时表明BERT的掩码语言模型目标在优化后仍具有竞争力。相关模型和代码已公开供进一步研究。</p><h2 id="引言" tabindex="-1"><a class="header-anchor" href="#引言"><span>引言</span></a></h2><p>RoBERTa 是一项针对 BERT 预训练方法的复制研究，旨在通过系统性的实验评估不同超参数和数据规模对模型性能的影响。研究发现，BERT 的训练存在显著不足，通过优化训练策略（如延长训练时间、增大批次规模、使用更多数据等），RoBERTa 能够匹配甚至超越后续提出的多种模型（如 XLNet）。</p><p>论文的主要改进包括：</p><ol><li><strong>动态掩码（Dynamic Masking）</strong>（对比静态掩码，如表 1 显示动态掩码在 SQuAD 2.0 和 SST-2 任务上表现更优）；</li></ol><figure><img src="'+T+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol start="2"><li><strong>移除下一句预测（NSP）目标</strong>（实验表明 NSP 对性能影响有限，甚至可能损害模型表现，如表 2 对比不同输入格式）；</li></ol><figure><img src="'+e+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol start="3"><li><strong>更大批次训练</strong>（表 3 显示增大批次规模可提升模型困惑度和下游任务准确率）；</li></ol><figure><img src="'+Q+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol start="4"><li><strong>更高效的字节级 BPE 编码</strong>（减少未知词影响）。</li></ol><p>此外，RoBERTa 引入了新数据集 <strong>CC-News</strong>（76GB），并验证了数据规模对预训练的关键作用。最终，RoBERTa 在 GLUE、SQuAD 和 RACE 上取得 SOTA 结果（如表 4、5、6），证明 BERT 的掩码语言模型目标在优化后仍具竞争力。</p><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="背景" tabindex="-1"><a class="header-anchor" href="#背景"><span>背景</span></a></h2><p>RoBERTa 基于 BERT 的架构和训练方法，但通过优化关键设计选择提升性能。BERT 采用 Transformer 结构，输入由两个文本片段（Segment）组成，并添加特殊标记（如 <code>[CLS]</code>、<code>[SEP]</code>）。其预训练任务包括：</p>',19)),t("ol",null,[a[13]||(a[13]=t("li",null,[t("p",null,[t("strong",null,"掩码语言模型（MLM）"),l("：随机选择 15% 的输入 token，其中 80% 替换为 "),t("code",null,"[MASK]"),l("，10% 保持不变，10% 替换为随机 token。原始 BERT 使用静态掩码（即预处理时固定掩码模式），而 RoBERTa 改用动态掩码（每次输入时重新生成掩码），实验证明动态掩码效果更优（如表 1）。")])],-1)),a[14]||(a[14]=t("li",null,[t("p",null,[t("strong",null,"下一句预测（NSP）"),l("：判断两个片段是否连续。尽管 BERT 认为 NSP 对下游任务（如自然语言推理）有帮助，但 RoBERTa 的实验表明移除 NSP 可能提升性能（如表 2 对比不同输入格式）。")])],-1)),t("li",null,[a[12]||(a[12]=t("p",null,[t("strong",null,"优化策略"),l("：")],-1)),t("ul",null,[t("li",null,[t("p",null,[a[6]||(a[6]=l("使用 Adam 优化器（")),t("mjx-container",h,[(r(),n("svg",B,a[0]||(a[0]=[o('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g><g data-mml-node="mn" transform="translate(599,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1280.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(2336.1,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" transform="translate(778,0)"></path></g></g></g>',1)]))),a[1]||(a[1]=t("mjx-assistive-mml",{unselectable:"on",display:"inline"},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("msub",null,[t("mi",null,"β"),t("mn",null,"1")]),t("mo",null,"="),t("mn",null,"0.9")])],-1))]),a[7]||(a[7]=l(", ")),t("mjx-container",E,[(r(),n("svg",c,a[2]||(a[2]=[o('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g><g data-mml-node="mn" transform="translate(599,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(1280.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(2336.1,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" transform="translate(778,0)"></path><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" transform="translate(1278,0)"></path><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" transform="translate(1778,0)"></path></g></g></g>',1)]))),a[3]||(a[3]=t("mjx-assistive-mml",{unselectable:"on",display:"inline"},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("msub",null,[t("mi",null,"β"),t("mn",null,"2")]),t("mo",null,"="),t("mn",null,"0.999")])],-1))]),a[8]||(a[8]=l(", ")),t("mjx-container",f,[(r(),n("svg",M,a[4]||(a[4]=[o('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g><g data-mml-node="mo" transform="translate(683.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1739.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(2239.6,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mo" transform="translate(2927.8,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(3928,0)"><path data-c="36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path></g></g></g>',1)]))),a[5]||(a[5]=t("mjx-assistive-mml",{unselectable:"on",display:"inline"},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mi",null,"ϵ"),t("mo",null,"="),t("mn",null,"1"),t("mi",null,"e"),t("mo",null,"−"),t("mn",null,"6")])],-1))]),a[9]||(a[9]=l("）。"))])]),a[10]||(a[10]=t("li",null,[t("p",null,"学习率采用线性预热（10,000 步）和衰减策略。")],-1)),a[11]||(a[11]=t("li",null,[t("p",null,"原始 BERT 训练 1M 步，批次大小 256，序列长度 512。")],-1))])]),a[15]||(a[15]=t("li",null,[t("p",null,[t("strong",null,"数据"),l("：BERT 使用 BookCorpus 和 Wikipedia（共 16GB），而 RoBERTa 扩展至更大规模数据（如 CC-News、OpenWebText 等，总计 160GB）。")])],-1))]),a[28]||(a[28]=t("p",null,"RoBERTa 通过调整这些关键因素（如动态掩码、移除 NSP、增大批次和数据规模），显著提升了 BERT 的预训练效率和下游任务表现。",-1)),a[29]||(a[29]=t("h2",{id:"实验步骤",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#实验步骤"},[t("span",null,"实验步骤")])],-1)),a[30]||(a[30]=t("p",null,[t("strong",null,"1. 模型实现与优化")],-1)),a[31]||(a[31]=t("p",null,"RoBERTa 基于 fairseq 工具包重新实现了 BERT，并优化了训练细节：",-1)),t("ul",null,[a[24]||(a[24]=t("li",null,[t("p",null,[t("strong",null,"学习率调整"),l("：相比原始 BERT 的固定学习率（1e-4），RoBERTa 针对不同设置调整峰值学习率和预热步数。")])],-1)),t("li",null,[t("p",null,[a[20]||(a[20]=t("strong",null,"Adam 优化器改进",-1)),a[21]||(a[21]=l("：发现 Adam 的 ")),t("mjx-container",x,[(r(),n("svg",L,a[16]||(a[16]=[t("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[t("g",{"data-mml-node":"math"},[t("g",{"data-mml-node":"mi"},[t("path",{"data-c":"1D716",d:"M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"})])])],-1)]))),a[17]||(a[17]=t("mjx-assistive-mml",{unselectable:"on",display:"inline"},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mi",null,"ϵ")])],-1))]),a[22]||(a[22]=l(" 项对训练稳定性影响较大，调整 ")),t("mjx-container",H,[(r(),n("svg",S,a[18]||(a[18]=[o('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g><g data-mml-node="mn" transform="translate(599,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(1280.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(2336.1,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" transform="translate(778,0)"></path><path data-c="38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z" transform="translate(1278,0)"></path></g></g></g>',1)]))),a[19]||(a[19]=t("mjx-assistive-mml",{unselectable:"on",display:"inline"},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("msub",null,[t("mi",null,"β"),t("mn",null,"2")]),t("mo",null,"="),t("mn",null,"0.98")])],-1))]),a[23]||(a[23]=l(" 以提升大批次训练的稳定性（参考 Section 3.1）。"))])]),a[25]||(a[25]=t("li",null,[t("p",null,[t("strong",null,"序列长度"),l("：始终使用完整长度序列（512 tokens），而原始 BERT 会在训练初期使用较短序列。")])],-1))]),a[32]||(a[32]=o('<hr><p><strong>2. 训练硬件与效率</strong></p><ul><li>采用 <strong>混合精度训练</strong>（FP16），在配备 8×32GB NVIDIA V100 GPU 的 DGX-1 机器上进行分布式训练，利用 Infiniband 互联提升效率。</li></ul><hr><p><strong>3. 数据配置</strong></p><p>RoBERTa 使用了 <strong>5 个英语语料库</strong>，总计超过 160GB 文本，包括：</p><ol><li><p><strong>BookCorpus + Wikipedia</strong>（16GB，原始 BERT 数据）</p></li><li><p><strong>CC-News</strong>（76GB，新闻数据）</p></li><li><p><strong>OpenWebText</strong>（38GB，Reddit 高赞网页内容）</p></li><li><p><strong>Stories</strong>（31GB，故事类文本）</p></li></ol><p>通过控制数据规模（如对比 16GB vs. 160GB），RoBERTa 验证了更多数据能显著提升模型性能（参考 Section 5 和 Table 4）。</p><figure><img src="'+m+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><hr><p><strong>4. 评估基准</strong></p><p>实验在三大基准任务上进行：</p><ol><li><p><strong>GLUE</strong>：涵盖 9 项自然语言理解任务（如 MNLI、SST-2 等），采用单任务微调（非多任务学习）。</p></li><li><p><strong>SQuAD</strong>：</p><ul><li><p>V1.1：答案必存在于上下文中。</p></li><li><p>V2.0：支持无答案问题，RoBERTa 增加了二分类器判断可答性（参考 Section 3.3）。</p></li></ul></li><li><p><strong>RACE</strong>：长文本阅读理解任务，需从 4 个选项中选择正确答案，测试模型的长距离依赖能力。</p></li></ol><h2 id="训练步骤分析" tabindex="-1"><a class="header-anchor" href="#训练步骤分析"><span>训练步骤分析</span></a></h2><ol><li><p><strong>静态与动态掩码（Static vs. Dynamic Masking）</strong></p><ul><li><p>原始BERT使用静态掩码，即在数据预处理阶段生成掩码模式并固定，通过复制数据来增加多样性。</p></li><li><p>RoBERTa改为动态掩码，每次输入序列时生成新的掩码模式。实验表明，动态掩码性能略优于静态掩码（如表1所示），且更高效。因此，后续实验均采用动态掩码。</p></li></ul></li></ol><p>在BERT和RoBERTa的预训练中，<strong>掩码（Masking）</strong> 是 <strong>Masked Language Modeling (MLM)</strong> 任务的核心步骤，即随机遮盖输入文本的部分单词，并让模型预测这些被遮盖的单词。</p><blockquote><p><strong>1. 静态掩码（Static Masking）</strong></p><ul><li><p><strong>原始BERT的做法</strong>：</p></li><li><p>在数据预处理阶段，<strong>一次性</strong> 对每个句子随机选择15%的单词进行掩码（其中80%替换为 <code>[MASK]</code>，10%保持不变，10%替换为随机单词）。</p></li><li><p>由于BERT训练时会多次遍历数据（如40个epoch），为了避免每次训练时看到相同的掩码模式，BERT采用 <strong>数据复制</strong> 的方法：</p><ul><li><p>将训练数据复制 <strong>10份</strong>，每份采用不同的随机掩码模式。</p></li><li><p>这样，每个句子在训练过程中会被看到 <strong>4次</strong>（40 epochs / 10 copies = 4次），但每次掩码不同。</p></li></ul></li><li><p><strong>问题</strong>：</p><ul><li><p>数据复制增加了存储和计算开销。</p></li><li><p>由于掩码模式是固定的（尽管有10种变体），模型可能过拟合这些特定的掩码模式，影响泛化能力。</p></li></ul></li></ul></blockquote><blockquote><p><strong>2. 动态掩码（Dynamic Masking）</strong></p><ul><li><p><strong>RoBERTa的改进</strong>：</p><ul><li><p><strong>不再预先固定掩码模式</strong>，而是在 <strong>每次输入模型时动态生成掩码</strong>。</p></li><li><p>例如，同一个句子在训练的不同批次（batch）中，可能会被掩码不同的单词。</p></li></ul></li><li><p><strong>优势</strong>：</p><ol><li><p><strong>减少存储开销</strong>：无需复制数据，节省内存。</p></li><li><p><strong>增加多样性</strong>：模型在训练过程中看到更多的掩码变体，提升泛化能力。</p></li><li><p><strong>更适合长训练周期</strong>：当训练步数远超过BERT的1M步时（如RoBERTa训练500K步），动态掩码能持续提供新的掩码模式，避免过拟合。</p></li></ol></li></ul></blockquote><ol start="2"><li><p><strong>输入格式与下一句预测（NSP）</strong></p><ul><li><p>原始BERT使用“Segment-pair+NSP”输入格式，包含两个文档片段和NSP损失。</p></li><li><p>RoBERTa对比了多种输入格式（如表2所示）：</p><ul><li><p><strong>Sentence-pair+NSP</strong>：使用单句对，性能下降，可能因无法学习长距离依赖。</p></li><li><p><strong>Full-sentences</strong>：连续句子打包，去除NSP损失，性能优于原始BERT。</p></li><li><p><strong>Doc-sentences</strong>：限制输入来自同一文档，性能略优于Full-sentences，但因批次大小可变，最终选择Full-sentences格式。</p></li></ul></li><li><p>实验表明，<strong>去除NSP损失</strong>不仅未降低性能，反而有所提升，这与原始BERT的结论相反。</p></li></ul></li><li><p><strong>大批量训练（Large Batch Training）</strong></p><ul><li><p>原始BERT使用256的批次大小训练1M步。RoBERTa尝试增大批次至2K和8K，并调整学习率（如表3所示）。</p></li><li><p>结果显示，大批量训练（如8K）在保持相同计算成本下，能提升掩码语言模型的困惑度和下游任务性能。因此，RoBERTa采用8K批次进行训练。</p></li></ul></li><li><p><strong>文本编码（Text Encoding）</strong></p><ul><li><p>原始BERT使用30K的字符级BPE词汇表。</p></li><li><p>RoBERTa改用基于字节的BPE（50K词汇表），无需额外预处理。虽然早期实验显示性能略有下降，但其通用性优势使其成为最终选择。</p></li></ul></li></ol><p>这些改进共同构成了RoBERTa的核心优化策略，显著提升了模型性能（如表4所示）。实验结果表明，BERT原始设计存在优化空间，而RoBERTa通过系统性的调整，在GLUE、SQuAD和RACE等任务上达到了新的 state-of-the-art 水平。</p><h2 id="roberta核心改进总结" tabindex="-1"><a class="header-anchor" href="#roberta核心改进总结"><span>RoBERTa核心改进总结</span></a></h2><p>RoBERTa（<strong>Robustly Optimized BERT Approach</strong>）是对BERT预训练过程的系统性优化，通过调整训练策略、数据规模和模型设置，显著提升了性能。其主要改进包括：</p><h4 id="_1-训练策略优化" tabindex="-1"><a class="header-anchor" href="#_1-训练策略优化"><span><strong>1. 训练策略优化</strong></span></a></h4><ul><li><p><strong>动态掩码（Dynamic Masking）</strong>：</p><ul><li><p>原始BERT使用静态掩码（预处理阶段固定掩码模式），而RoBERTa改为<strong>每次输入时动态生成掩码</strong>，减少存储开销并提升泛化能力（见表1）。</p></li><li><p><strong>结果</strong>：动态掩码在SQuAD 2.0和SST-2任务上表现略优（F1 78.7 vs. 78.3）。</p></li></ul></li><li><p><strong>移除NSP任务（Next Sentence Prediction）</strong>：</p><ul><li><p>BERT使用NSP任务（判断两个句子是否连续），但实验表明<strong>去除NSP后性能反而提升</strong>（见表2）。</p></li><li><p>RoBERTa改用<strong>Full-sentences</strong>（连续句子打包，不跨文档）或<strong>Doc-sentences</strong>（单文档内句子打包），后者效果略优但计算复杂，最终选择Full-sentences。</p></li></ul></li><li><p><strong>大批量训练（Large Batch Training）</strong>：</p><ul><li><p>BERT使用256的批次大小，RoBERTa增大至<strong>8K</strong>，并调整学习率（如1e-3）。</p></li><li><p><strong>结果</strong>：大批量训练提升MLM困惑度（3.77 vs. 3.99）和下游任务准确率（MNLI-m 84.6 vs. 84.7）（见表3）。</p></li></ul></li><li><p><strong>字节级BPE（Byte-level BPE）</strong>：</p><ul><li>改用50K词汇表的字节级BPE编码，减少未登录词（OOV）问题，虽对部分任务性能略有影响，但通用性更强。</li></ul></li></ul><hr><h4 id="_2-数据规模与训练时长" tabindex="-1"><a class="header-anchor" href="#_2-数据规模与训练时长"><span><strong>2. 数据规模与训练时长</strong></span></a></h4><ul><li><p><strong>更大规模数据</strong>：</p><ul><li><p>BERT训练数据：16GB（BookCorpus + Wikipedia）。</p></li><li><p>RoBERTa新增CC-News、OpenWebText等，总数据量达<strong>160GB</strong>。</p></li><li><p><strong>结果</strong>：数据量增加后，SQuAD 2.0 F1从87.3提升至87.7（见表4）。</p></li></ul></li><li><p><strong>更长训练步数</strong>：</p><ul><li><p>BERT训练1M步，RoBERTa延长至<strong>300K~500K步</strong>（计算成本相当，因批次更大）。</p></li><li><p><strong>结果</strong>：500K步时，SQuAD 2.0 F1达89.4，超越XLNet（88.8）（见表4）。</p></li></ul></li></ul><hr><h4 id="_3-性能表现-关键结果" tabindex="-1"><a class="header-anchor" href="#_3-性能表现-关键结果"><span><strong>3. 性能表现（关键结果）</strong></span></a></h4><ul><li><p><strong>GLUE基准</strong>：</p><ul><li><p><strong>单任务微调</strong>：RoBERTa在9项任务中全面超越BERT和XLNet（MNLI-m 90.2 vs. 89.8）（见表5）。</p></li><li><p><strong>排行榜提交</strong>：以<strong>88.5平均分</strong>刷新SOTA，其中4项任务（MNLI、QNLI、RTE、STS-B）领先（见表5）。</p></li></ul></li><li><p><strong>SQuAD 2.0</strong>：</p><ul><li>仅用SQuAD数据（无外部数据），F1达89.8，超越XLNet（89.1）（见表6）。</li></ul></li><li><p><strong>RACE阅读理解</strong>：</p><ul><li>准确率83.2%，显著高于BERT（72.0）和XLNet（81.7）（见表7）。</li></ul></li></ul><figure><img src="'+u+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><hr><h4 id="_4-结论与启示" tabindex="-1"><a class="header-anchor" href="#_4-结论与启示"><span><strong>4. 结论与启示</strong></span></a></h4><ul><li><p><strong>BERT原始设计未充分优化</strong>：RoBERTa证明<strong>更长训练、更大批次、更多数据</strong>是关键。</p></li><li><p><strong>NSP任务非必要</strong>：去除后性能反而提升，与BERT结论相反。</p></li><li><p><strong>动态掩码与大批量训练</strong>：提升效率的同时改善泛化能力。</p></li><li><p><strong>开源贡献</strong>：发布模型、代码及新数据集CC-News。</p></li></ul><p>RoBERTa的改进表明，<strong>BERT的MLM目标本身足够强大</strong>，只需优化训练策略即可达到SOTA，无需复杂结构调整。</p><h2 id="相关工作" tabindex="-1"><a class="header-anchor" href="#相关工作"><span>相关工作</span></a></h2><p>早期方法如ELMo、GPT和BERT通过不同训练目标（如语言建模、机器翻译、掩码语言建模）取得了显著进展，而后续工作通过多任务微调、实体嵌入、跨度预测和自回归预训练（如XLNet）进一步提升了性能。作者强调，这些改进通常依赖于更大模型和更多数据（如XLNet使用10倍于BERT的数据），而RoBERTa的目标是通过系统性地复现、简化和优化BERT的训练过程，为这些方法提供一个更清晰的性能基准，从而帮助社区更好地理解不同改进的相对贡献。</p><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><p>通过系统优化BERT的预训练策略（包括动态掩码、移除NSP任务、增大批次和训练数据、延长训练时间），RoBERTa在GLUE、SQuAD和RACE任务上实现了SOTA性能，证明了BERT原始设计的潜力尚未被充分挖掘；</p><p>同时，研究揭示了模型性能提升的关键因素并非复杂结构改动，而是训练策略和数据规模的优化，相关代码、模型和CC-News数据集已开源以促进后续研究。</p>',40))])}const V=s(R,[["render",w]]),N=JSON.parse('{"path":"/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/RoBERTa%E8%AE%BA%E6%96%87.html","title":"RoBERTa 论文","lang":"zh-CN","frontmatter":{"title":"RoBERTa 论文","icon":"file","category":["NLP"],"tag":["预训练语言模型","已发布"],"footer":"技术共建，知识共享","date":"2025-06-27T00:00:00.000Z","order":2,"author":["BinaryOracle"],"description":"RoBERTa 论文","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"RoBERTa 论文\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-06-27T00:00:00.000Z\\",\\"dateModified\\":\\"2025-07-03T08:28:44.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BinaryOracle\\"}]}"],["meta",{"property":"og:url","content":"https://mister-hope.github.io/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/RoBERTa%E8%AE%BA%E6%96%87.html"}],["meta",{"property":"og:site_name","content":"MetaMind"}],["meta",{"property":"og:title","content":"RoBERTa 论文"}],["meta",{"property":"og:description","content":"RoBERTa 论文"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-03T08:28:44.000Z"}],["meta",{"property":"article:author","content":"BinaryOracle"}],["meta",{"property":"article:tag","content":"已发布"}],["meta",{"property":"article:tag","content":"预训练语言模型"}],["meta",{"property":"article:published_time","content":"2025-06-27T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-03T08:28:44.000Z"}]]},"git":{"createdTime":1751007597000,"updatedTime":1751531324000,"contributors":[{"name":"BinaryOracle","username":"BinaryOracle","email":"3076679680@qq.com","commits":7,"url":"https://github.com/BinaryOracle"}]},"readingTime":{"minutes":11.01,"words":3302},"filePathRelative":"LLM/模型层/RoBERTa论文.md","excerpt":"<p><code>RoBERTa 论文</code></p>\\n","autoDesc":true}');export{V as comp,N as data};
