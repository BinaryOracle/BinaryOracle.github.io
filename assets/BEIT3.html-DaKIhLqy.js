import{_ as r}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as o,a as e,e as n,f as a,o as i}from"./app-Dws4Qgcl.js";const s={};function p(l,t){return i(),o("div",null,[t[0]||(t[0]=e("p",null,[e("code",null,"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks 论文解读")],-1)),n(" more "),t[1]||(t[1]=a('<blockquote><p>论文链接: <a href="https://arxiv.org/abs/2208.10442" target="_blank" rel="noopener noreferrer">Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks</a><br> 代码链接: <a href="https://github.com/microsoft/unilm/tree/master/beit3" target="_blank" rel="noopener noreferrer">https://github.com/microsoft/unilm/tree/master/beit3</a></p></blockquote><h2 id="引言" tabindex="-1"><a class="header-anchor" href="#引言"><span>引言</span></a></h2><p>近年来，<strong>语言、视觉与多模态预训练</strong>正在出现“大融合”的趋势。研究者发现，只要在<strong>海量数据</strong>上进行大规模预训练，就可以把模型轻松迁移到各种下游任务中。一个理想的方向是：<strong>预训练一个通用基础模型，能够同时处理多种模态</strong>。</p><p>BEiT-3 正是顺应这一趋势提出的，它在 <strong>视觉任务</strong>和 <strong>视觉-语言任务</strong>上都取得了最新的迁移性能；BEiT-3 核心贡献如下:</p><p><strong>1. 统一的骨干架构</strong></p><p>Transformer 的成功已经从 <strong>语言</strong> 扩展到了 <strong>视觉</strong> 和 <strong>多模态</strong>任务，这使得用统一网络结构来处理不同模态成为可能。不过，不同下游任务常常需要不同架构：</p><ul><li><p><strong>双编码器 (dual-encoder)</strong>：用于高效检索（如跨模态检索）。</p></li><li><p><strong>编码器-解码器 (encoder-decoder)</strong>：用于生成任务（如图像描述）。</p></li><li><p><strong>融合编码器 (fusion-encoder)</strong>：用于图文联合表示学习。</p></li></ul><p>问题在于：大多数基础模型需要针对不同任务手动调整网络格式，且不同模态之间的参数往往难以有效共享。</p><p>BEiT-3 引入了 <strong>Multiway Transformer</strong>（多路 Transformer），作为通用建模框架。它既能做模态特定的编码，也能实现跨模态深度融合，<strong>做到“一套架构适配所有下游任务”</strong>。</p><hr><p><strong>2. 统一的预训练任务</strong></p><p>掩码建模（Masked Data Modeling）已在多种模态上取得成功：</p><ul><li><p>文本（Masked Language Modeling, MLM）</p></li><li><p>图像（Masked Image Modeling, MIM）</p></li><li><p>图文对（Masked Multimodal Modeling）</p></li></ul><p>现有视觉-语言基础模型通常需要 <strong>多任务训练</strong>（如图文匹配、对比学习），但这会导致扩展到大规模数据时效率低。</p><p>BEiT-3 的做法是：只保留 <strong>单一任务</strong>——<strong>mask-then-predict（掩码预测）</strong>。</p><ul><li><p>把图像当作外语（Imglish），和文本用相同的方式建模。</p></li><li><p>图文对被看作“平行句子”，用来学习模态间的对齐关系。</p></li></ul><p>这种方法虽然简单，却能学习到很强的可迁移表征，并在视觉与视觉-语言任务中取得了最新结果。</p><hr><p><strong>3. 模型与数据的规模化</strong></p><p>扩大模型规模与数据规模，可以显著提高基础模型的泛化能力。</p><ul><li><p>BEiT-3 将模型扩展到了 <strong>数十亿参数</strong>级别。</p></li><li><p>预训练数据规模也被扩大，但仅使用 <strong>公开数据集</strong>，保证学术可复现性。</p></li></ul><p>即使没有依赖私有数据，BEiT-3 依然超过了许多依赖私有大数据的基础模型。</p><p>此外，将图像当作外语的方式还能直接复用大规模语言模型的训练管线，从而在规模化上进一步受益。</p><hr><p>BEiT-3 使用 Multiway Transformer，在 <strong>图像、文本和图文对</strong>上进行统一的掩码建模。</p><ul><li><p>在训练中，会随机掩码部分文本 token 或图像 patch。</p></li><li><p>学习目标是恢复原始 token（文本 token 或视觉 token）。</p></li></ul><p>这是一个标准的自监督学习任务，使模型在预训练阶段就能获得通用性。 BEiT-3 在多种任务上都取得了最新性能，包括：</p><ul><li><p><strong>视觉任务</strong>：目标检测（COCO）、实例分割（COCO）、语义分割（ADE20K）、图像分类（ImageNet）</p></li><li><p><strong>视觉-语言任务</strong>：视觉推理（NLVR2）、视觉问答（VQAv2）、图像描述（COCO）、跨模态检索（Flickr30K、COCO）</p></li></ul><p>结果显示：</p><ul><li><p>即使只使用公开数据，BEiT-3 依然超越了许多依赖私有数据的强大模型。</p></li><li><p>它不仅在多模态任务上表现优异，在<strong>纯视觉任务</strong>中也能达到甚至超过专用模型的效果。</p></li></ul>',30))])}const m=r(s,[["render",p]]),d=JSON.parse('{"path":"/MMLLM/BEIT3.html","title":"BEIT3 论文","lang":"zh-CN","frontmatter":{"title":"BEIT3 论文","icon":"file","category":["多模态"],"tag":["多模态","编辑中"],"footer":"技术共建，知识共享","date":"2025-08-22T00:00:00.000Z","author":["BinaryOracle"],"description":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks 论文解读","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"BEIT3 论文\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-08-22T00:00:00.000Z\\",\\"dateModified\\":\\"2025-08-28T01:59:04.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BinaryOracle\\"}]}"],["meta",{"property":"og:url","content":"https://mister-hope.github.io/MMLLM/BEIT3.html"}],["meta",{"property":"og:site_name","content":"MetaMind"}],["meta",{"property":"og:title","content":"BEIT3 论文"}],["meta",{"property":"og:description","content":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks 论文解读"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-08-28T01:59:04.000Z"}],["meta",{"property":"article:author","content":"BinaryOracle"}],["meta",{"property":"article:tag","content":"编辑中"}],["meta",{"property":"article:tag","content":"多模态"}],["meta",{"property":"article:published_time","content":"2025-08-22T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-08-28T01:59:04.000Z"}]]},"git":{"createdTime":1755826642000,"updatedTime":1756346344000,"contributors":[{"name":"BinaryOracle","username":"BinaryOracle","email":"3076679680@qq.com","commits":2,"url":"https://github.com/BinaryOracle"}]},"readingTime":{"minutes":3.37,"words":1011},"filePathRelative":"MMLLM/BEIT3.md","excerpt":"<p><code>Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks 论文解读</code></p>\\n","autoDesc":true}');export{m as comp,d as data};
