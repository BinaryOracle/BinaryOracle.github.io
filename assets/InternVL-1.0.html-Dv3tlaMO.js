import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,a as n,e as r,d as o,o as i}from"./app-gnjlIeY7.js";const s="/assets/1-CnFJ92wa.png",l="/assets/2-CBJSW4X9.png",p={};function g(c,e){return i(),a("div",null,[e[0]||(e[0]=n("p",null,[n("code",null,"InternVL 1.0: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks 论文简析")],-1)),r(" more "),e[1]||(e[1]=o('<blockquote><p>论文链接: <a href="https://arxiv.org/abs/2312.14238" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2312.14238</a><br> 代码链接: <a href="https://github.com/OpenGVLab/InternVL" target="_blank" rel="noopener noreferrer">https://github.com/OpenGVLab/InternVL</a></p></blockquote><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>InternVL是一个大规模视觉-语言基础模型，旨在解决当前视觉与视觉-语言基础模型发展滞后于大型语言模型（LLMs）的问题。该模型通过将视觉基础模型扩展到60亿参数，并利用多源网络图像-文本数据进行渐进式对齐训练，成功实现了视觉与语言模型在参数规模和特征表示上的协调。InternVL在32个通用视觉-语言任务中表现出色，包括图像分类、语义分割、视频分类、图像/视频-文本检索以及多模态对话系统等，展现了强大的视觉能力和与LLMs的无缝集成潜力，为多模态大模型的发展提供了重要贡献。</p><h2 id="简介" tabindex="-1"><a class="header-anchor" href="#简介"><span>简介</span></a></h2><ol><li><p><strong>研究背景与问题</strong>: 大型语言模型（LLMs）的快速发展推动了通用人工智能（AGI）系统的进步，但视觉和视觉-语言基础模型的发展却相对滞后。现有的视觉-语言大模型（VLLMs）通常使用轻量级的“胶水层”（如QFormer或线性投影）来对齐视觉和语言模型的特征，但这种方法存在三个主要限制：</p><ul><li><p><strong>参数规模不匹配</strong>：LLMs的参数规模已达千亿级，而视觉编码器通常仅约10亿参数，限制了LLM的能力利用。</p></li><li><p><strong>表征不一致</strong>：视觉模型通常基于纯视觉数据或BERT系列模型训练，与LLMs的特征空间存在差异。</p></li><li><p><strong>低效连接</strong>：轻量级胶水层难以捕捉跨模态的复杂交互。</p></li></ul></li><li><p><strong>解决方案与核心设计</strong>: 论文提出 <strong>InternVL</strong>，通过以下关键设计解决上述问题：</p><ul><li><p><strong>参数平衡的视觉与语言组件</strong>：包含60亿参数的视觉编码器（InternViT-6B）和80亿参数的语言中间件（QLLaMA），后者作为强大的“胶水层”重组视觉特征。</p></li><li><p><strong>一致的表征对齐</strong>：使用多语言LLaMA初始化中间件，确保视觉编码器与LLMs的特征空间一致。</p></li><li><p><strong>渐进式图像-文本对齐策略</strong>：先在大规模噪声数据上对比学习，再在高质量数据上生成学习，逐步提升模型性能（如图1c所示）。</p></li></ul></li></ol><figure><img src="'+s+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol start="3"><li><p><strong>模型优势</strong></p><ul><li><p><strong>多功能性</strong>：可作为独立视觉编码器或与语言中间件结合，支持感知、检索、生成和对话任务。</p></li><li><p><strong>强大性能</strong>：在ImageNet分类、ADE20K分割、视频检索等任务中达到SOTA（如图2所示）。</p></li><li><p><strong>LLM友好性</strong>：与LLaMA、Vicuna等LLMs无缝集成，推动多模态应用发展。</p></li></ul></li></ol><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="相关工作" tabindex="-1"><a class="header-anchor" href="#相关工作"><span>相关工作</span></a></h2><h4 id="_1-视觉基础模型-vision-foundation-models" tabindex="-1"><a class="header-anchor" href="#_1-视觉基础模型-vision-foundation-models"><span><strong>1. 视觉基础模型（Vision Foundation Models）</strong></span></a></h4><p>视觉基础模型在过去十年中经历了显著发展，从早期的AlexNet和CNN架构（如ResNet）到近年来的Vision Transformer（ViT）及其变体。ViT及其衍生模型（如ViT-G、EVA-02等）通过扩大模型规模和参数量，显著提升了视觉任务的性能。然而，当前广泛使用的视觉模型参数量仍停留在约10亿级别（如ViT-22B除外），远落后于LLMs的规模。此外，这些模型多基于纯视觉数据（如ImageNet、JFT）训练，或与BERT系列模型对齐，缺乏与LLMs的直接特征兼容性，限制了其在多模态任务中的表现。</p><h4 id="_2-大语言模型-large-language-models-llms" tabindex="-1"><a class="header-anchor" href="#_2-大语言模型-large-language-models-llms"><span><strong>2. 大语言模型（Large Language Models, LLMs）</strong></span></a></h4><p>LLMs（如GPT-3、LLaMA系列、Vicuna等）在自然语言处理领域取得了突破性进展，展示了强大的少样本和零样本学习能力。开源模型（如ChatGLM、Falcon等）的涌现进一步加速了多模态研究的进程。然而，LLMs本身缺乏视觉理解能力，如何将其与视觉模态结合成为关键挑战。</p><h4 id="_3-视觉大语言模型-vision-large-language-models-vllms" tabindex="-1"><a class="header-anchor" href="#_3-视觉大语言模型-vision-large-language-models-vllms"><span><strong>3. 视觉大语言模型（Vision Large Language Models, VLLMs）</strong></span></a></h4><p>近期研究通过将视觉模型与LLMs结合，构建了多模态对话系统（如Flamingo、LLaVA、MiniGPT-4等）。这些工作主要依赖轻量级适配层（如QFormer、线性投影）连接视觉编码器和LLM，但受限于视觉模型的规模和对齐效率。部分模型（如KOSMOS-2、Qwen-VL）进一步引入了视觉定位能力，支持区域描述和问答。尽管如此，视觉基础模型的性能瓶颈仍是制约VLLMs发展的关键因素。</p><h4 id="核心问题与本文定位" tabindex="-1"><a class="header-anchor" href="#核心问题与本文定位"><span><strong>核心问题与本文定位</strong></span></a></h4><p>现有工作表明，视觉模型的规模不足和对齐低效是阻碍多模态能力提升的主要障碍。InternVL通过<strong>规模化视觉编码器</strong>和<strong>渐进式跨模态对齐</strong>，首次实现了视觉与语言模型在参数和特征空间的深度协同，填补了这一领域的空白。</p><h2 id="方法" tabindex="-1"><a class="header-anchor" href="#方法"><span>方法</span></a></h2>',18))])}const m=t(p,[["render",g]]),h=JSON.parse('{"path":"/MMLLM/InternVL-1.0.html","title":"书生·万象多模态大模型（InternVL 1.0）","lang":"zh-CN","frontmatter":{"title":"书生·万象多模态大模型（InternVL 1.0）","icon":"file","category":["多模态"],"tag":["多模态","编辑中"],"footer":"技术共建，知识共享","date":"2025-06-15T00:00:00.000Z","cover":"assets/cover/InternVL.png","author":["BinaryOracle"],"description":"InternVL 1.0: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks 论文简析","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"书生·万象多模态大模型（InternVL 1.0）\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-06-15T00:00:00.000Z\\",\\"dateModified\\":\\"2025-07-03T09:12:08.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BinaryOracle\\"}]}"],["meta",{"property":"og:url","content":"https://mister-hope.github.io/MMLLM/InternVL-1.0.html"}],["meta",{"property":"og:site_name","content":"MetaMind"}],["meta",{"property":"og:title","content":"书生·万象多模态大模型（InternVL 1.0）"}],["meta",{"property":"og:description","content":"InternVL 1.0: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks 论文简析"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-03T09:12:08.000Z"}],["meta",{"property":"article:author","content":"BinaryOracle"}],["meta",{"property":"article:tag","content":"编辑中"}],["meta",{"property":"article:tag","content":"多模态"}],["meta",{"property":"article:published_time","content":"2025-06-15T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-03T09:12:08.000Z"}]]},"git":{"createdTime":1750144311000,"updatedTime":1751533928000,"contributors":[{"name":"BinaryOracle","username":"BinaryOracle","email":"3076679680@qq.com","commits":4,"url":"https://github.com/BinaryOracle"}]},"readingTime":{"minutes":4.32,"words":1295},"filePathRelative":"MMLLM/InternVL-1.0.md","excerpt":"<p><code>InternVL 1.0: Scaling up Vision Foundation Models and Aligning  for Generic Visual-Linguistic Tasks 论文简析</code></p>\\n","autoDesc":true}');export{m as comp,h as data};
