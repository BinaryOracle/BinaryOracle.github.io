import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,a as e,e as a,f as r,o}from"./app-9WXoDpPu.js";const l={};function p(s,t){return o(),i("div",null,[t[0]||(t[0]=e("p",null,[e("code",null,"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 论文解读")],-1)),a(" more "),t[1]||(t[1]=r('<blockquote><p>论文链接: <a href="https://arxiv.org/abs/2201.12086" target="_blank" rel="noopener noreferrer">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a><br> 代码链接: <a href="https://github.com/salesforce/BLIP" target="_blank" rel="noopener noreferrer">https://github.com/salesforce/BLIP</a></p></blockquote><h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction"><span>Introduction</span></a></h2><p>当前视觉-语言预训练（VLP）方法虽然在多模态任务上取得进展，但普遍存在两个问题：</p><ol><li><p><strong>模型限制</strong>：编码器模型不适合文本生成任务；编码器-解码器模型难以用于图文检索。</p></li><li><p><strong>数据质量差</strong>：大多使用从网络收集的嘈杂图文对作为训练数据，监督信号不理想。</p></li></ol><p>BLIP（Bootstrapping Language-Image Pre-training）是一个新颖的 VLP 框架，兼顾理解与生成能力。其两大创新点：</p><ol><li><p><strong>MED 模型结构（Multimodal Mixture of Encoder-Decoder）</strong>：</p><ul><li><p>同时支持编码器、图像条件编码器、图像条件解码器三种模式。</p></li><li><p>联合训练三种任务：图文对比学习、图文匹配、图像条件语言建模。</p></li><li><p>实现多任务预训练与灵活迁移。</p></li></ul></li><li><p><strong>CapFilt 数据自举方法（Captioning and Filtering）</strong>：</p><ul><li><p>使用训练好的 MED 模型构建两个模块：</p><ul><li><p>描述器（captioner）生成图像的合成描述；</p></li><li><p>过滤器（filter）剔除原始和生成的低质量描述。</p></li></ul></li><li><p>在保留信息的同时提升训练数据质量。</p></li></ul></li></ol><p>实验结果与表现:</p><ul><li><p>BLIP 在多个任务（图文检索、图像描述、VQA 等）上取得<strong>最先进性能</strong>。</p></li><li><p>同时，在两个视频-语言任务上以<strong>零样本方式</strong>迁移也表现优异。</p></li><li><p>实验证明：描述器与过滤器的组合能显著提升性能，多样化描述更有利于学习。</p></li></ul><h2 id="related-work" tabindex="-1"><a class="header-anchor" href="#related-work"><span>Related Work</span></a></h2><h3 id="视觉-语言预训练-vlp" tabindex="-1"><a class="header-anchor" href="#视觉-语言预训练-vlp"><span>视觉-语言预训练（VLP）</span></a></h3><ul><li><p><strong>现状问题：</strong></p><ul><li><p>主流 VLP 方法依赖从网络抓取的图文对数据，虽然规模大，但包含大量噪声文本。</p></li><li><p>尽管使用简单的过滤规则，噪声仍广泛存在。</p></li><li><p>编码器模型适合理解类任务但难以生成文本；编码器-解码器适合生成任务但不适用于检索。</p></li></ul></li><li><p><strong>BLIP 的改进：</strong></p><ul><li><p>提出 <strong>CapFilt</strong>：通过“生成 + 过滤”的方式优化数据质量。</p></li><li><p>提出 <strong>MED 模型结构</strong>：在保持预训练高效的前提下，同时兼顾理解与生成任务，提升泛化能力。</p></li></ul></li></ul><h3 id="知识蒸馏-knowledge-distillation" tabindex="-1"><a class="header-anchor" href="#知识蒸馏-knowledge-distillation"><span>知识蒸馏（Knowledge Distillation）</span></a></h3><ul><li><p><strong>现有做法：</strong></p><ul><li><p>知识蒸馏让小模型（学生）学习大模型（教师）的预测结果。</p></li><li><p>自蒸馏也取得了不错效果，尤其在图像分类与部分 VLP 方法中已开始尝试。</p></li></ul></li><li><p><strong>BLIP 的新视角：</strong></p><ul><li><p>CapFilt 可视为一种结构化的知识蒸馏方式：</p><ul><li><p><strong>Captioner 模块</strong>用生成的语义丰富描述进行蒸馏；</p></li><li><p><strong>Filter 模块</strong>通过剔除噪声文本完成隐式知识过滤。</p></li></ul></li></ul></li></ul><h3 id="数据增强-data-augmentation" tabindex="-1"><a class="header-anchor" href="#数据增强-data-augmentation"><span>数据增强（Data Augmentation）</span></a></h3><ul><li><p><strong>现有做法：</strong></p><ul><li><p>图像任务中数据增强广泛应用，但语言任务的数据增强较困难。</p></li><li><p>近年来生成模型被用于文本任务的样本合成，但多用于低资源语言场景。</p></li></ul></li><li><p><strong>BLIP 的贡献：</strong></p><ul><li>展示了在<strong>大规模视觉-语言预训练中</strong>使用合成图像描述的独特优势，提升了多模态学习效果。</li></ul></li></ul>',15))])}const c=n(l,[["render",p]]),u=JSON.parse('{"path":"/MMLLM/%20BLIP.html","title":"BLIP 论文","lang":"zh-CN","frontmatter":{"title":"BLIP 论文","icon":"file","category":["多模态"],"tag":["多模态","编辑中"],"footer":"技术共建，知识共享","date":"2025-07-20T00:00:00.000Z","author":["BinaryOracle"],"description":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 论文解读","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"BLIP 论文\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-07-20T00:00:00.000Z\\",\\"dateModified\\":\\"2025-07-21T14:55:27.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BinaryOracle\\"}]}"],["meta",{"property":"og:url","content":"https://mister-hope.github.io/MMLLM/%20BLIP.html"}],["meta",{"property":"og:site_name","content":"MetaMind"}],["meta",{"property":"og:title","content":"BLIP 论文"}],["meta",{"property":"og:description","content":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 论文解读"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-21T14:55:27.000Z"}],["meta",{"property":"article:author","content":"BinaryOracle"}],["meta",{"property":"article:tag","content":"编辑中"}],["meta",{"property":"article:tag","content":"多模态"}],["meta",{"property":"article:published_time","content":"2025-07-20T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-21T14:55:27.000Z"}]]},"git":{"createdTime":1753080726000,"updatedTime":1753109727000,"contributors":[{"name":"BinaryOracle","username":"BinaryOracle","email":"3076679680@qq.com","commits":2,"url":"https://github.com/BinaryOracle"}]},"readingTime":{"minutes":2.78,"words":833},"filePathRelative":"MMLLM/ BLIP.md","excerpt":"<p><code>BLIP: Bootstrapping Language-Image Pre-training for  Unified Vision-Language Understanding and Generation 论文解读</code></p>\\n","autoDesc":true}');export{c as comp,u as data};
