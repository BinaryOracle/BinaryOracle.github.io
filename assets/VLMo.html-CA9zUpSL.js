import{_ as r}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,a as t,e as o,f as n,o as a}from"./app-D_WNQ9eT.js";const p={};function l(s,e){return a(),i("div",null,[e[0]||(e[0]=t("p",null,[t("code",null,"VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts 论文简析")],-1)),o(" more "),e[1]||(e[1]=n('<blockquote><p>论文链接: <a href="https://arxiv.org/abs/2111.02358" target="_blank" rel="noopener noreferrer">VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</a><br> 代码链接: <a href="https://github.com/microsoft/unilm/tree/master/vlmo" target="_blank" rel="noopener noreferrer">https://github.com/microsoft/unilm/tree/master/vlmo</a></p></blockquote><h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction"><span>Introduction</span></a></h2><p>视觉-语言（VL）预训练旨在从大规模图文对中学习通用的跨模态表示。现有模型通常通过图文匹配、图文对比学习、掩码区域分类/特征回归、词-区域/块对齐以及掩码语言建模等方法来聚合和对齐视觉与语言信息，然后在下游任务如图文检索、视觉问答（VQA）、视觉推理等进行微调。</p><p>现有两类主流架构各有优缺点：</p><p><strong>双编码器架构</strong>（Dual-Encoder，如 CLIP、ALIGN）：</p><ul><li><p>图像和文本分别编码，模态间交互通过特征向量的余弦相似度进行。</p></li><li><p>优点：检索任务高效，特征向量可提前计算存储，线性复杂度。</p></li><li><p>缺点：交互浅，对复杂视觉-语言分类任务表现有限，如 CLIP 在视觉推理任务上准确率偏低。</p></li></ul><p><strong>融合编码器架构</strong>（Fusion-Encoder）：</p><ul><li><p>使用多层 Transformer 通过跨模态注意力融合图像和文本表示。</p></li><li><p>优点：在视觉-语言分类任务上性能优异。</p></li><li><p>缺点：检索任务需对所有图文对联合编码，时间复杂度为二次方，推理速度慢。</p></li></ul><hr><p><strong>VLMO 的提出</strong></p><p>为兼顾双编码器和融合编码器的优势，论文提出了 <strong>统一视觉-语言预训练模型 VLMO</strong>，其特点如下：</p><ul><li><p>可作为双编码器用于图文检索，也可作为融合编码器处理图文对分类任务。</p></li><li><p>核心组件为 <strong>Mixture-of-Modality-Experts (MOME) Transformer</strong>，一个 Transformer 块内可编码图像、文本及图文对。</p></li><li><p>MOME 替换标准 Transformer 的前馈网络为模态专家池，捕获模态特定信息，同时共享自注意力层进行跨模态对齐。</p></li><li><p>三类模态专家：视觉专家（图像编码）、语言专家（文本编码）、视觉-语言专家（图文融合）。</p></li><li><p>模型灵活性高，可复用共享参数实现文本编码器、图像编码器和图文融合编码器。</p></li></ul><hr><p><strong>预训练任务与策略</strong></p><p>VLMO 采用三种联合预训练任务：</p><ul><li><p>图文对比学习（image-text contrastive learning）</p></li><li><p>图文匹配（image-text matching）</p></li><li><p>掩码语言建模（masked language modeling）</p></li></ul><p>同时提出 <strong>分阶段预训练策略</strong>，充分利用大规模图像单模态和文本单模态数据：</p><ol><li><p>在图像单模态数据上预训练视觉专家和自注意力模块，采用 BEIT 的掩码图像建模方法。</p></li><li><p>在文本单模态数据上预训练语言专家，采用掩码语言建模方法。</p></li><li><p>最终初始化视觉-语言预训练模型，解决图文对数量有限、描述短小的问题，从而学习更泛化的表示。</p></li></ol><hr><p><strong>实验结果与贡献</strong></p><ul><li><p>在图文检索任务中，VLMO 作为双编码器比融合编码器更快，并且性能优于其他融合编码器模型。</p></li><li><p>在视觉问答（VQA）和自然语言视觉推理（NLVR2）任务中，作为融合编码器的 VLMO 达到最先进性能。</p></li></ul><p><strong>主要贡献</strong>：</p><ul><li><p>提出统一视觉-语言预训练模型 VLMO，可灵活用作融合编码器或双编码器。</p></li><li><p>引入通用多模态 Transformer（MOME Transformer），通过模态专家捕获模态特定信息，并通过共享自注意力实现跨模态对齐。</p></li><li><p>分阶段预训练策略利用大规模图像单模态和文本单模态数据，显著提升模型性能。</p></li></ul><h2 id="related-work" tabindex="-1"><a class="header-anchor" href="#related-work"><span>Related Work</span></a></h2>',24))])}const g=r(p,[["render",l]]),d=JSON.parse('{"path":"/MMLLM/VLMo.html","title":"VLMo 论文","lang":"zh-CN","frontmatter":{"title":"VLMo 论文","icon":"file","category":["多模态"],"tag":["多模态","编辑中"],"footer":"技术共建，知识共享","date":"2025-08-16T00:00:00.000Z","author":["BinaryOracle"],"description":"VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts 论文简析","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"VLMo 论文\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-08-16T00:00:00.000Z\\",\\"dateModified\\":\\"2025-08-16T03:14:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BinaryOracle\\"}]}"],["meta",{"property":"og:url","content":"https://mister-hope.github.io/MMLLM/VLMo.html"}],["meta",{"property":"og:site_name","content":"MetaMind"}],["meta",{"property":"og:title","content":"VLMo 论文"}],["meta",{"property":"og:description","content":"VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts 论文简析"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-08-16T03:14:15.000Z"}],["meta",{"property":"article:author","content":"BinaryOracle"}],["meta",{"property":"article:tag","content":"编辑中"}],["meta",{"property":"article:tag","content":"多模态"}],["meta",{"property":"article:published_time","content":"2025-08-16T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-08-16T03:14:15.000Z"}]]},"git":{"createdTime":1753599408000,"updatedTime":1755314055000,"contributors":[{"name":"BinaryOracle","username":"BinaryOracle","email":"3076679680@qq.com","commits":4,"url":"https://github.com/BinaryOracle"}]},"readingTime":{"minutes":3.18,"words":954},"filePathRelative":"MMLLM/VLMo.md","excerpt":"<p><code>VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts 论文简析</code></p>\\n","autoDesc":true}');export{g as comp,d as data};
