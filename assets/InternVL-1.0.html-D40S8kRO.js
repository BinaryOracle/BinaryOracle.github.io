import{_ as o}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as r,a as n,d as e,e as a,b as i,o as s}from"./app-wMBLzBuZ.js";const l="/assets/1-CnFJ92wa.png",g="/assets/2-CBJSW4X9.png",p="/assets/4-hwg4kkTG.png",L="/assets/3-D5hIiDVn.png",d="/assets/5-BqRyp2N1.png",c="/assets/6-BnEq3iaW.png",Q="/assets/7-BLxW_ML7.png",u="/assets/8-1eAvB5Ek.png",h="/assets/9-D7MRqKrr.png",m="/assets/10-BN2hRT82.png",T="/assets/11-CVXPP5bm.png",f="/assets/12-BMMlrjI2.png",V="/assets/13-7j5Anx3V.png",M="/assets/14-BrDHgYNM.png",I="/assets/15-BhMR9P7h.png",b="/assets/16-D2qtI_1Y.png",A="/assets/17-rArJ-hqO.png",x="/assets/18-CHwdk95z.png",B="/assets/19-57kzfclo.png",_="/assets/20-9RQf4_Uk.png",H={},y={class:"MathJax",jax:"SVG",style:{position:"relative"}},C={style:{"vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"18.572ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 8208.9 1000","aria-hidden":"true"};function k(P,t){return s(),r("div",null,[t[5]||(t[5]=n("p",null,[n("code",null,"InternVL 1.0: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks 论文简析")],-1)),e(" more "),t[6]||(t[6]=a('<blockquote><p>论文链接: <a href="https://arxiv.org/abs/2312.14238" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2312.14238</a><br> 代码链接: <a href="https://github.com/OpenGVLab/InternVL" target="_blank" rel="noopener noreferrer">https://github.com/OpenGVLab/InternVL</a></p></blockquote><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>InternVL是一个大规模视觉-语言基础模型，旨在解决当前视觉与视觉-语言基础模型发展滞后于大型语言模型（LLMs）的问题。该模型通过将视觉基础模型扩展到60亿参数，并利用多源网络图像-文本数据进行渐进式对齐训练，成功实现了视觉与语言模型在参数规模和特征表示上的协调。InternVL在32个通用视觉-语言任务中表现出色，包括图像分类、语义分割、视频分类、图像/视频-文本检索以及多模态对话系统等，展现了强大的视觉能力和与LLMs的无缝集成潜力，为多模态大模型的发展提供了重要贡献。</p><h2 id="简介" tabindex="-1"><a class="header-anchor" href="#简介"><span>简介</span></a></h2><ol><li><p><strong>研究背景与问题</strong>: 大型语言模型（LLMs）的快速发展推动了通用人工智能（AGI）系统的进步，但视觉和视觉-语言基础模型的发展却相对滞后。现有的视觉-语言大模型（VLLMs）通常使用轻量级的“胶水层”（如QFormer或线性投影）来对齐视觉和语言模型的特征，但这种方法存在三个主要限制：</p><ul><li><p><strong>参数规模不匹配</strong>：LLMs的参数规模已达千亿级，而视觉编码器通常仅约10亿参数，限制了LLM的能力利用。</p></li><li><p><strong>表征不一致</strong>：视觉模型通常基于纯视觉数据或BERT系列模型训练，与LLMs的特征空间存在差异。</p></li><li><p><strong>低效连接</strong>：轻量级胶水层难以捕捉跨模态的复杂交互。</p></li></ul></li><li><p><strong>解决方案与核心设计</strong>: 论文提出 <strong>InternVL</strong>，通过以下关键设计解决上述问题：</p><ul><li><p><strong>参数平衡的视觉与语言组件</strong>：包含60亿参数的视觉编码器（InternViT-6B）和80亿参数的语言中间件（QLLaMA），后者作为强大的“胶水层”重组视觉特征。</p></li><li><p><strong>一致的表征对齐</strong>：使用多语言LLaMA初始化中间件，确保视觉编码器与LLMs的特征空间一致。</p></li><li><p><strong>渐进式图像-文本对齐策略</strong>：先在大规模噪声数据上对比学习，再在高质量数据上生成学习，逐步提升模型性能（如图1c所示）。</p></li></ul></li></ol><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol start="3"><li><p><strong>模型优势</strong></p><ul><li><p><strong>多功能性</strong>：可作为独立视觉编码器或与语言中间件结合，支持感知、检索、生成和对话任务。</p></li><li><p><strong>强大性能</strong>：在ImageNet分类、ADE20K分割、视频检索等任务中达到SOTA（如图2所示）。</p></li><li><p><strong>LLM友好性</strong>：与LLaMA、Vicuna等LLMs无缝集成，推动多模态应用发展。</p></li></ul></li></ol><figure><img src="'+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="相关工作" tabindex="-1"><a class="header-anchor" href="#相关工作"><span>相关工作</span></a></h2><h4 id="_1-视觉基础模型-vision-foundation-models" tabindex="-1"><a class="header-anchor" href="#_1-视觉基础模型-vision-foundation-models"><span><strong>1. 视觉基础模型（Vision Foundation Models）</strong></span></a></h4><p>视觉基础模型在过去十年中经历了显著发展，从早期的AlexNet和CNN架构（如ResNet）到近年来的Vision Transformer（ViT）及其变体。ViT及其衍生模型（如ViT-G、EVA-02等）通过扩大模型规模和参数量，显著提升了视觉任务的性能。然而，当前广泛使用的视觉模型参数量仍停留在约10亿级别（如ViT-22B除外），远落后于LLMs的规模。此外，这些模型多基于纯视觉数据（如ImageNet、JFT）训练，或与BERT系列模型对齐，缺乏与LLMs的直接特征兼容性，限制了其在多模态任务中的表现。</p><h4 id="_2-大语言模型-large-language-models-llms" tabindex="-1"><a class="header-anchor" href="#_2-大语言模型-large-language-models-llms"><span><strong>2. 大语言模型（Large Language Models, LLMs）</strong></span></a></h4><p>LLMs（如GPT-3、LLaMA系列、Vicuna等）在自然语言处理领域取得了突破性进展，展示了强大的少样本和零样本学习能力。开源模型（如ChatGLM、Falcon等）的涌现进一步加速了多模态研究的进程。然而，LLMs本身缺乏视觉理解能力，如何将其与视觉模态结合成为关键挑战。</p><h4 id="_3-视觉大语言模型-vision-large-language-models-vllms" tabindex="-1"><a class="header-anchor" href="#_3-视觉大语言模型-vision-large-language-models-vllms"><span><strong>3. 视觉大语言模型（Vision Large Language Models, VLLMs）</strong></span></a></h4><p>近期研究通过将视觉模型与LLMs结合，构建了多模态对话系统（如Flamingo、LLaVA、MiniGPT-4等）。这些工作主要依赖轻量级适配层（如QFormer、线性投影）连接视觉编码器和LLM，但受限于视觉模型的规模和对齐效率。部分模型（如KOSMOS-2、Qwen-VL）进一步引入了视觉定位能力，支持区域描述和问答。尽管如此，视觉基础模型的性能瓶颈仍是制约VLLMs发展的关键因素。</p><h4 id="核心问题与本文定位" tabindex="-1"><a class="header-anchor" href="#核心问题与本文定位"><span><strong>核心问题与本文定位</strong></span></a></h4><p>现有工作表明，视觉模型的规模不足和对齐低效是阻碍多模态能力提升的主要障碍。InternVL通过<strong>规模化视觉编码器</strong>和<strong>渐进式跨模态对齐</strong>，首次实现了视觉与语言模型在参数和特征空间的深度协同，填补了这一领域的空白。</p><h2 id="方法" tabindex="-1"><a class="header-anchor" href="#方法"><span>方法</span></a></h2><h3 id="_1-整体架构设计" tabindex="-1"><a class="header-anchor" href="#_1-整体架构设计"><span><strong>1. 整体架构设计</strong></span></a></h3><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>InternVL的整体架构（如图3所示）突破了传统视觉模型（如ViT）和双塔模型（如CLIP）的局限，通过以下两个核心组件实现跨模态深度协同：</p>',21)),n("ul",null,[n("li",null,[t[2]||(t[2]=n("strong",null,"InternViT-6B",-1)),t[3]||(t[3]=i(": 基于标准ViT架构的60亿参数视觉编码器，通过超参数搜索优化了深度（48层）、头数（25）和MLP比率（8），在模型规模（5.9B参数）与计算效率间取得平衡（详见表1）。其输出支持密集特征图（")),n("mjx-container",y,[(s(),r("svg",C,t[0]||(t[0]=[a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(888,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mn" transform="translate(1388,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(500,0)"></path></g><g data-mml-node="mo" transform="translate(2610.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(3610.4,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(4658.4,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mn" transform="translate(5158.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(500,0)"></path></g><g data-mml-node="mo" transform="translate(6380.7,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(7380.9,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g></g></g>',1)]))),t[1]||(t[1]=n("mjx-assistive-mml",{unselectable:"on",display:"inline"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("mi",null,"H"),n("mrow",{"data-mjx-texclass":"ORD"},[n("mo",null,"/")]),n("mn",null,"14"),n("mo",null,"×"),n("mi",null,"W"),n("mrow",{"data-mjx-texclass":"ORD"},[n("mo",null,"/")]),n("mn",null,"14"),n("mo",null,"×"),n("mi",null,"D")])],-1))]),t[4]||(t[4]=i("）或全局池化特征，适配分类、分割等任务。"))])]),t[7]||(t[7]=a('<figure><img src="'+L+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li><strong>QLLaMA</strong>: 基于多语言LLaMA-7B初始化的80亿参数语言中间件，新增<strong>96个可学习查询</strong>和交叉注意力层（1B参数），作为视觉与LLMs之间的&quot;重型胶水层&quot;。相比QFormer等轻量适配器，其参数量提升42倍，能更有效地重组视觉特征为LLM兼容的序列（见图4b/d）。</li></ul><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>如图1所示，InternVL的架构设计显著区别于：</p><ul><li><p><strong>(a) 纯视觉模型</strong>（如ResNet）：仅支持单模态任务，缺乏语言对齐。</p></li><li><p><strong>(b) 双塔模型</strong>（如CLIP）：独立编码图像/文本，依赖浅层相似度计算。</p></li><li><p><strong>(c) InternVL</strong>：通过QLLaMA实现动态特征交互，同时支持对比学习（如检索）和生成任务（如描述）。</p></li></ul><p>通过组合不同组件，InternVL可灵活切换为四种模式（图4）：</p><ol><li><p><strong>纯视觉模式</strong>（图4a）：仅用InternViT-6B处理图像分类/分割。</p></li><li><p><strong>对比模式-InternVL-C</strong>（图4b）：视觉编码器+注意力池化，用于零样本分类/检索。</p></li><li><p><strong>对比模式-InternVL-G</strong>（图4b）：联合QLLaMA二次编码视觉特征，提升检索精度。</p></li><li><p><strong>对话模式</strong>（图4c/d）：连接LLM（如Vicuna），支持多模态问答。</p></li></ol><h3 id="_2-模型设计" tabindex="-1"><a class="header-anchor" href="#_2-模型设计"><span><strong>2. 模型设计</strong></span></a></h3><ol><li><p><strong>大规模视觉编码器（InternViT-6B）</strong></p><ul><li><p>InternViT-6B是一个基于Vision Transformer（ViT）的视觉编码器，参数量达到60亿，旨在与大型语言模型（LLM）的规模相匹配。</p></li><li><p>通过超参数搜索（如模型深度、头维度和MLP比例），作者确定了在性能和效率之间取得平衡的最佳配置（表1）。实验发现，模型深度对速度的影响在GPU计算饱和后可以忽略，而参数数量相同时，不同配置对性能影响较小。最终选择了深度48、宽度3200、MLP比率12800的稳定配置。</p></li><li><p>该编码器支持密集预测任务（如语义分割）和图像分类任务，并能生成全局或局部视觉特征（图4a/b）。</p></li></ul></li><li><p><strong>语言中间件（QLLaMA）</strong></p><ul><li><p>QLLaMA是一个80亿参数的语言中间件，基于多语言LLaMA-7B初始化，新增了96个可学习查询和交叉注意力层（1亿参数），用于对齐视觉与语言特征（图3）。</p></li><li><p>相比传统轻量级“胶水层”（如QFormer或线性投影），QLLaMA的优势包括：</p><ul><li><p>通过预训练权重实现视觉特征到LLM表示的对齐；</p></li><li><p>参数量是QFormer的42倍，即使冻结LLM解码器也能在多模态对话任务中表现优异；</p></li><li><p>支持对比学习任务（如零样本图像分类和检索）。</p></li></ul></li></ul></li><li><p><strong>灵活的组合方式（“瑞士军刀”模型）</strong></p><ul><li><p>InternVL通过组合视觉编码器和语言中间件，支持多种任务模式（图4）：</p><ul><li><p><strong>视觉感知任务</strong>：直接使用InternViT-6B提取特征。</p></li><li><p><strong>对比任务</strong>（如检索）：通过注意力池化生成全局特征（InternVL-C或InternVL-G）。</p></li><li><p><strong>生成任务</strong>（如图像描述）：QLLaMA利用其大规模参数重组视觉表示并生成文本。</p></li><li><p><strong>多模态对话</strong>：连接LLM解码器（InternVL-Chat），支持两种配置（图4c/d）。</p></li></ul></li></ul></li></ol><h3 id="_3-对齐策略" tabindex="-1"><a class="header-anchor" href="#_3-对齐策略"><span><strong>3. 对齐策略</strong></span></a></h3><p><strong>1. 视觉-语言对比训练（Vision-Language Contrastive Training）</strong></p><ul><li><p><strong>目标</strong>：初步对齐视觉编码器（InternViT-6B）和文本编码器（LLaMA-7B）。</p></li><li><p><strong>数据</strong>：使用大规模但噪声较多的公开网络图像-文本对（如 LAION-en、LAION-multi、COYO 等，共 4.98B 样本，表 2）。</p></li></ul><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li><p><strong>方法</strong>：</p><ul><li><p>采用 <strong>CLIP 风格的对比学习</strong>，最小化图像-文本对的对称交叉熵损失。</p></li><li><p>初始阶段在较低分辨率（196×196）训练，并应用 <strong>50% 图像 token 掩码</strong> 以提高效率，后期切换至 224×224 分辨率。</p></li></ul></li><li><p><strong>效果</strong>：使模型在零样本分类、图像-文本检索等对比任务上表现优异，并为后续阶段提供稳健的视觉表示。</p></li></ul><hr><p><strong>2. 视觉-语言生成训练（Vision-Language Generative Training）</strong></p><ul><li><p><strong>目标</strong>：增强模型生成能力，进一步对齐视觉与语言特征。</p></li><li><p><strong>数据</strong>：筛选高质量图像-文本数据（1.03B，表 2），去除低质量描述（如重复文本、无意义内容）。</p></li><li><p><strong>方法</strong>：</p><ul><li><p>冻结 InternViT-6B 和 QLLaMA 的预训练权重，仅训练新增的 <strong>可学习查询和交叉注意力层</strong>。</p></li><li><p>结合 <strong>三种损失函数</strong>：</p><ul><li><p><strong>ITC（图像-文本对比损失）</strong></p></li><li><p><strong>ITM（图像-文本匹配损失）</strong></p></li><li><p><strong>ITG（基于图像的文本生成损失）</strong></p></li></ul></li></ul></li><li><p><strong>效果</strong>：使 QLLaMA 能够有效重组视觉特征，并生成连贯的文本描述（如表 10 的零样本图像描述结果）。</p></li></ul><figure><img src="'+Q+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><hr><p><strong>3. 监督微调（Supervised Fine-tuning, SFT）</strong></p><ul><li><p><strong>目标</strong>：优化多模态对话能力，连接 LLM 解码器（如 Vicuna、InternLM）。</p></li><li><p><strong>数据</strong>：收集约 400 万高质量指令数据（表 3），涵盖图像描述、VQA、OCR、视觉定位等任务。</p></li></ul><figure><img src="'+u+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li><p><strong>方法</strong>：</p><ul><li><p><strong>两种配置</strong>（图 4c/d）：</p><ol><li><p><strong>仅使用 InternViT-6B</strong>，通过 MLP 层连接 LLM（类似 LLaVA）。</p></li><li><p><strong>使用完整 InternVL（InternViT + QLLaMA）</strong>，利用其对齐的特征空间提升性能。</p></li></ol></li><li><p>由于 QLLaMA 与 LLM 特征空间一致，即使冻结 LLM 解码器，仅微调 MLP 层也能取得良好效果。</p></li></ul></li><li><p><strong>效果</strong>：在 MME、POPE 等多模态对话基准上达到 SOTA（表 9）。</p></li></ul><figure><img src="'+h+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>这一渐进式策略确保模型 <strong>从粗粒度对齐过渡到细粒度优化</strong>，充分利用不同质量的数据，最终实现强大的多模态理解和生成能力。</p><h3 id="实现细节" tabindex="-1"><a class="header-anchor" href="#实现细节"><span>实现细节</span></a></h3><p><strong>第一阶段（Stage 1）</strong></p><ul><li>在该阶段，图像编码器 InternViT-6B 是随机初始化的 <a href="https://arxiv.org/abs/2106.08254" target="_blank" rel="noopener noreferrer">7</a>，而文本编码器 LLaMA-7B 则使用来自文献 <a href="https://arxiv.org/abs/2304.08177" target="_blank" rel="noopener noreferrer">32</a>的预训练权重进行初始化。此阶段中，<strong>所有参数都是可训练的</strong>。</li></ul><p><strong>第二阶段（Stage 2）</strong></p><ul><li>在该阶段，InternViT-6B 和 QLLaMA 继承了第一阶段中学习到的权重，而 <strong>QLLaMA 中新加入的可学习查询（learnable queries）和跨注意力层（cross-attention layers）是随机初始化的</strong>。由于第一阶段中已获得了强大的表示能力，我们在该阶段<strong>冻结 InternViT-6B 和 QLLaMA，仅训练新引入的参数</strong>。</li></ul><blockquote><p>基座是 LLaMA-7B：QLLaMA 继承了经过第一阶段对比训练后得到的 LLaMA-7B 权重；</p><p>新增模块：</p><ul><li><p>96 个 learnable query 向量：用于从视觉特征中提取信息；</p></li><li><p>Cross-Attention 层：插入到了 LLaMA 的每一层 decoder block 中（这是主流做法，如 BLIP-2 也是如此），使得语言模型具备视觉融合能力；</p></li><li><p>参数量：新加入模块约为 10 亿参数，占 QLLaMA 总体 8B 的一部分；</p></li></ul></blockquote><p><strong>第三阶段（Stage 3）</strong></p><ul><li><p>此阶段有两种不同的配置方式：</p><ul><li><p>一种是<strong>单独使用 InternViT-6B</strong>，如图 4(c) 所示；</p></li><li><p>另一种是<strong>同时使用完整的 InternVL 模型</strong>，如图 4(d) 所示。</p></li></ul></li></ul><h2 id="实验" tabindex="-1"><a class="header-anchor" href="#实验"><span>实验</span></a></h2><h3 id="视觉感知能力验证-visual-perception-benchmarks" tabindex="-1"><a class="header-anchor" href="#视觉感知能力验证-visual-perception-benchmarks"><span><strong>视觉感知能力验证（Visual Perception Benchmarks）</strong></span></a></h3><p>图像分类（Image Classification）：</p><ul><li>InternViT-6B 在 ImageNet-1K 及其多个变种（如 IN-A、IN-R、IN-V2 等）上进行线性探测评估。结果显示，其在冻结骨干网络的前提下，取得了领先的零样本分类准确率，<strong>平均精度达到 82.5%</strong>，超过了如 OpenCLIP-G、EVA-01-CLIP-g 等主流模型。</li></ul><figure><img src="'+m+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>语义分割（Semantic Segmentation）：</p><ul><li>在 ADE20K 上进行语义分割测试，在不同微调策略下（线性探测、Head Tuning、全量微调），InternViT-6B 都展现出更强的像素级感知能力。例如，在全参数微调下，<strong>mIoU 达到 58.9%</strong>，显著优于 ViT-22B（55.3%）。</li></ul><figure><img src="'+T+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="视觉-语言任务能力-vision-language-benchmarks" tabindex="-1"><a class="header-anchor" href="#视觉-语言任务能力-vision-language-benchmarks"><span><strong>视觉-语言任务能力（Vision-Language Benchmarks）</strong></span></a></h3><p>零样本图像分类（Zero-Shot Image Classification）：</p><ul><li>在多语言版本的 ImageNet 上（EN, ZH, JP, AR, IT），InternVL-C 的表现优于 OpenCLIP-XLM-R 和其他多语言模型，展示了良好的语言泛化能力。</li></ul><figure><img src="'+f+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>零样本图像-文本检索（Image-Text Retrieval）：</p><ul><li>InternVL-C 和 InternVL-G 在英中双语的 Flickr30K / COCO / Flickr30K-CN / COCO-CN 上均取得 SoTA 表现，<strong>InternVL-G 的 Recall@1 在 COCO 图像→文本检索任务中达到 85.0%</strong>，在多语言图像→文本检索任务 XTD 中，Recall@10 平均可达 <strong>96.6%</strong>，显著超越现有方法。</li></ul><figure><img src="'+V+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+M+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>零样本图像字幕生成（Image Captioning）：</p><ul><li>InternVL-G 在不使用指令微调的前提下，仅通过 QLLaMA 即可生成高质量图像描述。例如在 COCO 测试集上，<strong>zero-shot CIDEr 得分达到 128.2</strong>，超越如 BLIP-2、Qwen-VL 等多模态生成模型。</li></ul><figure><img src="'+I+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="多模态对话任务-multi-modal-dialogue-benchmarks" tabindex="-1"><a class="header-anchor" href="#多模态对话任务-multi-modal-dialogue-benchmarks"><span><strong>多模态对话任务（Multi-Modal Dialogue Benchmarks）</strong></span></a></h3><p>InternVL-Chat 在多模态对话基准（如 MME、POPE）上超越了多个 SoTA 模型。比如，在 MME 综合指标上，<strong>InternVL-Chat（13B + QLLaMA）达到 1586.4 分</strong>，优于 LLaVA-1.5 和 InstructBLIP 等方法。</p><figure><img src="'+b+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>此外，InternVL 的多模态对话能力还体现在：</p><ul><li><p>VQA 子任务上：GQA 得分达 59.5（优于 LLaVA-13B 的 63.3）；</p></li><li><p>图像字幕、OCR、视觉推理任务中均表现稳定，兼具理解和生成能力。</p></li></ul><h3 id="消融实验-ablation-study" tabindex="-1"><a class="header-anchor" href="#消融实验-ablation-study"><span><strong>消融实验（Ablation Study）</strong></span></a></h3><p>视觉主干设计选择（InternViT-6B）：</p><ul><li>作者在不同模型深度、宽度、MLP 比例等超参数组合上进行对比试验，最终选择了参数约为 5.9B 的 <strong>variant 3</strong> 作为 InternViT-6B 版本，在计算成本和准确率之间取得了良好平衡。</li></ul><figure><img src="'+A+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>QLLaMA 的重要性验证：</p><ul><li>通过最小化配置（仅训练 MLP 层）进行对比，发现<strong>使用 QLLaMA 作为 glue 层明显优于传统 MLP 层或 QFormer</strong>，在对话任务（如 MME、OKVQA、GQA）上均有显著提升。例如 MME 得分从 1022.3（无 QLLaMA）提高至 1317.2（使用 QLLaMA 和 Vicuna-13B）。</li></ul><figure><img src="'+x+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h3><p>InternVL 的实验结果充分证明了其设计策略的有效性：</p><ol><li><p><strong>大型视觉编码器（InternViT-6B）具备极强的感知能力</strong>；</p></li><li><p><strong>QLLaMA 显著提升了视觉-语言对齐与生成能力</strong>；</p></li><li><p><strong>多阶段训练策略（对比 + 生成 + 指令微调）保障了模型的通用性与灵活性</strong>；</p></li><li><p><strong>在图像分类、文本检索、VQA、多模态对话等任务上全面领先于现有开源模型</strong>，是当前最具代表性的通用多模态基础模型之一。</p></li></ol><h2 id="结论" tabindex="-1"><a class="header-anchor" href="#结论"><span>结论</span></a></h2><p>通过将视觉基础模型扩展到 60 亿参数规模（InternViT-6B），并与一个由 LLaMA 初始化的语言中间件（QLLaMA）进行渐进式对齐，InternVL 构建了一个强大且通用的视觉-语言基础模型。借助海量图文数据和多阶段训练策略（对比、生成、微调），InternVL 实现了在图像分类、图文检索、图像描述、VQA、多模态对话等任务上的领先性能，成功弥合了视觉模型与大型语言模型之间的能力与表示鸿沟，推动了多模态大模型的发展。</p><h2 id="详细训练设置-附录内容" tabindex="-1"><a class="header-anchor" href="#详细训练设置-附录内容"><span>详细训练设置(附录内容)</span></a></h2><h4 id="第一阶段设置-stage-1" tabindex="-1"><a class="header-anchor" href="#第一阶段设置-stage-1"><span><strong>第一阶段设置（Stage 1）</strong></span></a></h4><p>如表20所示，在此阶段：</p><ul><li><p><strong>图像编码器（InternViT-6B）</strong> 采用 BEiT 的初始化方法随机初始化，<strong>文本编码器（LLaMA-7B）</strong> 则加载多语言 LLaMA-7B 的预训练权重。所有参数均参与训练。</p></li><li><p>优化器使用 <strong>AdamW</strong>，超参数为 β1=0.9、β2=0.95，权重衰减 0.1，学习率采用余弦退火策略（图像编码器初始 1e-3，文本编码器 1e-4）。</p></li><li><p>采用 <strong>DropPath 率 0.2</strong>，总批量大小 164K，在 640 张 A100 GPU 上训练 175K 步，处理约 287 亿样本。</p></li><li><p>为提升效率，初期在 <strong>196×196 分辨率</strong> 下训练并 <strong>掩码 50% 图像 token</strong>，后期切换至 <strong>224×224 分辨率</strong> 并取消掩码（最后 5 亿样本）。</p></li></ul><figure><img src="'+B+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="第二阶段设置-stage-2" tabindex="-1"><a class="header-anchor" href="#第二阶段设置-stage-2"><span><strong>第二阶段设置（Stage 2）</strong></span></a></h4><ul><li><p><strong>InternViT-6B 和 QLLaMA</strong> 继承第一阶段权重，<strong>新增的可学习查询和交叉注意力层</strong> 随机初始化。</p></li><li><p>冻结主干网络，仅训练新增参数，输入分辨率保持 224×224。</p></li><li><p>优化器使用 <strong>AdamW</strong>（β1=0.9、β2=0.98，权重衰减 0.05），总批量大小 20K，在 160 张 A100 GPU 上训练 80K 步（含 2K 步热身），峰值学习率 5e-5。</p></li></ul><h4 id="第三阶段设置-stage-3" tabindex="-1"><a class="header-anchor" href="#第三阶段设置-stage-3"><span><strong>第三阶段设置（Stage 3）</strong></span></a></h4><p>提供两种配置：</p><ol><li><p><strong>InternVL-Chat（不含 QLLaMA）</strong></p><ul><li>类似 LLaVA-1.5，先以 <strong>LGS-558K 数据集</strong> 训练 MLP 层，再用 <strong>LLaVA-Mix-665K</strong> 微调 LLM，各训练 1 轮。</li></ul></li><li><p><strong>InternVL-Chat（含 QLLaMA）</strong></p><ul><li>分两步：先用自定义 SFT 数据训练 MLP 层，再微调 LLM。因数据量扩大，批量大小增至 512。</li></ul></li></ol><h4 id="检索任务微调设置" tabindex="-1"><a class="header-anchor" href="#检索任务微调设置"><span><strong>检索任务微调设置</strong></span></a></h4><ul><li><p>所有参数可训练，分别在 <strong>Flickr30K</strong> 和 <strong>Flickr30K-CN</strong> 上微调。</p></li><li><p>分辨率 364×364，采用 <strong>分层学习率衰减（0.9）</strong> 和 <strong>DropPath 率 0.3</strong>。</p></li><li><p>使用 <strong>AdamW</strong>，批量大小 1024，训练 10 轮。</p></li></ul><h4 id="imagenet-线性探测设置" tabindex="-1"><a class="header-anchor" href="#imagenet-线性探测设置"><span><strong>ImageNet 线性探测设置</strong></span></a></h4><ul><li><p>遵循常规做法：</p><ul><li><p>用 <strong>BatchNorm</strong> 归一化特征，拼接平均池化的 patch token 和类别 token。</p></li><li><p>线性分类头使用 <strong>SGD</strong> 训练 10 轮（批量 1024，峰值学习率 0.2，1 轮热身，无权重衰减），数据增强包括随机裁剪和翻转。</p></li></ul></li></ul><h4 id="ade20k-语义分割设置" tabindex="-1"><a class="header-anchor" href="#ade20k-语义分割设置"><span><strong>ADE20K 语义分割设置</strong></span></a></h4><p>表23列出了三种配置的超参数：</p><ul><li><p><strong>线性探测（Linear Probing）</strong></p></li><li><p><strong>头部调优（Head Tuning）</strong></p></li><li><p><strong>全参数调优（Full-parameter Tuning）</strong></p></li></ul><figure><img src="'+_+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>',87))])}const S=o(H,[["render",k]]),v=JSON.parse('{"path":"/MMLLM/InternVL-1.0.html","title":"书生·万象多模态大模型（InternVL 1.0）","lang":"zh-CN","frontmatter":{"title":"书生·万象多模态大模型（InternVL 1.0）","icon":"file","category":["多模态"],"tag":["多模态","已发布"],"footer":"技术共建，知识共享","date":"2025-06-15T00:00:00.000Z","author":["BinaryOracle"],"description":"InternVL 1.0: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks 论文简析","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"书生·万象多模态大模型（InternVL 1.0）\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-06-15T00:00:00.000Z\\",\\"dateModified\\":\\"2025-07-04T08:27:18.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BinaryOracle\\"}]}"],["meta",{"property":"og:url","content":"https://mister-hope.github.io/MMLLM/InternVL-1.0.html"}],["meta",{"property":"og:site_name","content":"MetaMind"}],["meta",{"property":"og:title","content":"书生·万象多模态大模型（InternVL 1.0）"}],["meta",{"property":"og:description","content":"InternVL 1.0: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks 论文简析"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-04T08:27:18.000Z"}],["meta",{"property":"article:author","content":"BinaryOracle"}],["meta",{"property":"article:tag","content":"已发布"}],["meta",{"property":"article:tag","content":"多模态"}],["meta",{"property":"article:published_time","content":"2025-06-15T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-04T08:27:18.000Z"}]]},"git":{"createdTime":1750144311000,"updatedTime":1751617638000,"contributors":[{"name":"BinaryOracle","username":"BinaryOracle","email":"3076679680@qq.com","commits":8,"url":"https://github.com/BinaryOracle"}]},"readingTime":{"minutes":15.65,"words":4695},"filePathRelative":"MMLLM/InternVL-1.0.md","excerpt":"<p><code>InternVL 1.0: Scaling up Vision Foundation Models and Aligning  for Generic Visual-Linguistic Tasks 论文简析</code></p>\\n","autoDesc":true}');export{S as comp,v as data};
