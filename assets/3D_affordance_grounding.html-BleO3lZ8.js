import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as o,a,d as i,e as r,o as n}from"./app-_INieMuD.js";const l="/assets/1-D1pbm_VH.png",p="/assets/4-b4D0c1aJ.png",s="/assets/2-Ch_jbl-N.png",c="/assets/5-DSB_t-NN.png",d="/assets/2-DhTi-rFx.png",f={};function g(h,e){return n(),o("div",null,[e[0]||(e[0]=a("p",null,[a("code",null,"3D Affordance Grounding 方向复盘")],-1)),i(" more "),e[1]||(e[1]=r('<h2 id="点云-文本" tabindex="-1"><a class="header-anchor" href="#点云-文本"><span>点云 + 文本</span></a></h2><h3 id="affogato-arxiv-2025-06" tabindex="-1"><a class="header-anchor" href="#affogato-arxiv-2025-06"><span><a href="https://arxiv.org/abs/2506.12009" target="_blank" rel="noopener noreferrer">Affogato (Arxiv 2025.06)</a></span></a></h3><p>特点:</p><ol><li><p>AFFOrdance Grounding All aT Once</p></li><li><p>a large-scale dataset for 3D and 2D affordance grounding</p></li><li><p>minimalistic architecture</p></li></ol><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>损失函数:</p><ol><li><p>Focal Loss to handle class imbalance</p></li><li><p>Dice Loss to improve region-level alignment.</p></li></ol><p>现状:</p><ol><li><p>wait for code release</p></li><li><p>dataset available</p></li></ol><h3 id="seqafford-cvpr-2025" tabindex="-1"><a class="header-anchor" href="#seqafford-cvpr-2025"><span><a href="https://arxiv.org/abs/2412.01550" target="_blank" rel="noopener noreferrer">SeqAfford (CVPR 2025)</a></span></a></h3><p>特点:</p><ol><li><p>Propose a 3D multimodal large language model (referring to the LLaVA model architecture)</p></li><li><p>Feed the <code>&lt;SEG&gt;</code> segmentation tokens output by the 3D MMLLM into the multi-granularity language-point cloud combination module to complete 3D dense prediction</p></li><li><p>Support sequential instruction execution</p></li><li><p>Large-scale instruction-point cloud pair dataset: A dataset with 180,000 instruction-point cloud pairs, covering single and sequential operability reasoning tasks</p></li></ol><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>损失函数:</p><ol><li><p>Autoregressive Cross-Entropy Loss</p></li><li><p>Dice Loss</p></li><li><p>Binary Cross-Entropy Loss</p></li></ol><p>现状:</p><ol><li><p>code available</p></li><li><p>dataset available</p></li></ol><h2 id="点云-图像" tabindex="-1"><a class="header-anchor" href="#点云-图像"><span>点云 + 图像</span></a></h2><h3 id="iagnet-iccv-2023" tabindex="-1"><a class="header-anchor" href="#iagnet-iccv-2023"><span><a href="https://arxiv.org/abs/2303.10437" target="_blank" rel="noopener noreferrer">IAGNet (ICCV 2023)</a></span></a></h3><p>特点:</p><p>损失函数:</p><p>现状:</p><h2 id="点云-文本-图像" tabindex="-1"><a class="header-anchor" href="#点云-文本-图像"><span>点云 + 文本 + 图像</span></a></h2><h3 id="great-cvpr-2025" tabindex="-1"><a class="header-anchor" href="#great-cvpr-2025"><span><a href="https://arxiv.org/abs/2411.19626" target="_blank" rel="noopener noreferrer">GREAT (CVPR 2025)</a></span></a></h3><p>特点:</p><ol><li><p>grounding 3D object affordance in an Open-Vocabulary fashion</p></li><li><p>Multi-Head Affordance Chain-of-Thought</p></li></ol><blockquote><p>Data preparation stage:</p><ol><li><p>Use prompts to generate descriptions of the object interaction area, the morphology(形态学) of the interaction area, the interaction behavior, and other common interaction behaviors of the object.</p></li><li><p>Geometric structure knowledge = Answers to Prompt 1 + Prompt 2 = Interaction parts + Inference of geometric properties of these parts</p></li><li><p>Interaction knowledge = Answers to Prompt 3 + Prompt 4 = Current interaction + Analogous(类似的)/supplementary(补充) interaction methods</p></li></ol></blockquote><ol start="3"><li>PIADv2 dataset</li></ol><blockquote><p>24 affordance , 43 object categories, 15K interaction images , 38K 3D objects with annotations.</p></blockquote><figure><img src="'+s+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>损失函数:</p><ol><li><p>Focal Loss to handle class imbalance</p></li><li><p>Dice Loss to improve region-level alignment.</p></li></ol><p>现状:</p><ol><li><p>code available</p></li><li><p>dataset available</p></li></ol><h3 id="lmaffordance3d-cvpr-2025" tabindex="-1"><a class="header-anchor" href="#lmaffordance3d-cvpr-2025"><span><a href="https://arxiv.org/abs/2504.04744" target="_blank" rel="noopener noreferrer">LMAffordance3D (CVPR 2025)</a></span></a></h3><p>特点:</p><ol><li><p>Combine language instructions, visual observations, and interaction information to locate the affordance of manipulable objects in 3D space.</p></li><li><p>AGPIL（Affordance Grounding dataset with Points, Images and Language instructions）</p></li></ol><blockquote><p>This dataset includes estimations of object affordances observed from full-view, partial-view, and rotated perspectives, taking into account factors such as real-world observation angles, object rotation, and spatial occlusion (遮挡).</p></blockquote><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>损失函数:</p><ol><li><p>focal loss</p></li><li><p>dice loss</p></li></ol><p>现状:</p><ol><li><p>wait for code release</p></li><li><p>dataset available</p></li></ol><h2 id="_3d-gaussian-splatting-3dgs" tabindex="-1"><a class="header-anchor" href="#_3d-gaussian-splatting-3dgs"><span>3D Gaussian Splatting (3DGS)</span></a></h2><h3 id="geal-cvpr-2025" tabindex="-1"><a class="header-anchor" href="#geal-cvpr-2025"><span><a href="https://arxiv.org/abs/2412.09511" target="_blank" rel="noopener noreferrer">GEAL (CVPR 2025)</a></span></a></h3><p>特点:</p><ol><li><p>&quot;Knowledge Distillation&quot; from 2D to 3D: Transfer the semantic capabilities of pre-trained 2D models to the 3D affordance prediction model through Gaussian splat mapping, cross-modal consistency alignment, and multi-scale fusion.</p></li><li><p>Noisy Dataset: Construct a new benchmark with multiple types of noise/damage to evaluate the generalization and robustness of the model under real/harsh conditions.</p></li></ol><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>损失函数:</p><ol><li><p>BCE</p></li><li><p>Dice Loss</p></li><li><p>Consistency Loss（MSE 损失）</p></li></ol><p>现状:</p><ol><li><p>wait for code release</p></li><li><p>wait for dataset release</p></li></ol><h3 id="_3daffordsplat-arxiv-2025-04" tabindex="-1"><a class="header-anchor" href="#_3daffordsplat-arxiv-2025-04"><span><a href="https://arxiv.org/abs/2504.11218" target="_blank" rel="noopener noreferrer">3DAffordSplat (Arxiv 2025.04)</a></span></a></h3><h3 id="iaao-cvpr-2025" tabindex="-1"><a class="header-anchor" href="#iaao-cvpr-2025"><span><a href="https://arxiv.org/abs/2504.06827" target="_blank" rel="noopener noreferrer">IAAO (CVPR 2025)</a></span></a></h3><h2 id="idea" tabindex="-1"><a class="header-anchor" href="#idea"><span>idea</span></a></h2><p>Momentum Encoder 生成伪标签应对噪声问题，实现更加稳健的学习 ？(参考: MoCo , ALBEF , DINO)</p>',56))])}const b=t(f,[["render",g]]),v=JSON.parse('{"path":"/3DVL/3D_affordance_grounding.html","title":"3D Affordance Grounding 方向复盘","lang":"zh-CN","frontmatter":{"title":"3D Affordance Grounding 方向复盘","icon":"file","category":["3D-VL"],"tag":["3D-VL","编辑中"],"footer":"技术共建，知识共享","date":"2025-09-15T00:00:00.000Z","author":["BinaryOracle"],"description":"3D Affordance Grounding 方向复盘","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"3D Affordance Grounding 方向复盘\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-09-15T00:00:00.000Z\\",\\"dateModified\\":\\"2025-09-15T12:28:27.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BinaryOracle\\"}]}"],["meta",{"property":"og:url","content":"https://mister-hope.github.io/3DVL/3D_affordance_grounding.html"}],["meta",{"property":"og:site_name","content":"MetaMind"}],["meta",{"property":"og:title","content":"3D Affordance Grounding 方向复盘"}],["meta",{"property":"og:description","content":"3D Affordance Grounding 方向复盘"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-09-15T12:28:27.000Z"}],["meta",{"property":"article:author","content":"BinaryOracle"}],["meta",{"property":"article:tag","content":"编辑中"}],["meta",{"property":"article:tag","content":"3D-VL"}],["meta",{"property":"article:published_time","content":"2025-09-15T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-09-15T12:28:27.000Z"}]]},"git":{"createdTime":1757901173000,"updatedTime":1757939307000,"contributors":[{"name":"BinaryOracle","username":"BinaryOracle","email":"3076679680@qq.com","commits":7,"url":"https://github.com/BinaryOracle"}]},"readingTime":{"minutes":2.03,"words":610},"filePathRelative":"3DVL/3D_affordance_grounding.md","excerpt":"<p><code>3D Affordance Grounding 方向复盘</code></p>\\n","autoDesc":true}');export{b as comp,v as data};
