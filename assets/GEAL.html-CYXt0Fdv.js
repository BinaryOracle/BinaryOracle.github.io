import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as r,d as o,e as a,o as p}from"./app-bWYNuPyZ.js";const s="/assets/1-B58Ku3Av.png",i={};function l(g,t){return p(),n("div",null,[t[0]||(t[0]=r("p",null,[r("code",null,"GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency 论文")],-1)),o(" more "),t[1]||(t[1]=a('<blockquote><p>论文链接: <a href="https://arxiv.org/abs/2412.09511" target="_blank" rel="noopener noreferrer">GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency</a></p></blockquote><h2 id="引言" tabindex="-1"><a class="header-anchor" href="#引言"><span>引言</span></a></h2><p><strong>研究背景与动机</strong>：</p><p>3D 可供性学习的目标是根据语义线索（例如图像、文本指令），在三维物体上找出可以交互的区域。比如机器人需要知道 <strong>哪里能抓住把手、哪里能按下按钮</strong>。这种能力对机器人学和人机交互非常重要，能够支持 <strong>动作预测、物体操作以及自主决策</strong> 等任务。</p><p>然而，现有 3D 可供性学习方法存在几个主要问题：</p><ul><li><p><strong>数据稀缺</strong>：与 2D 任务相比，3D 数据的标注非常有限，因此泛化性不足。</p></li><li><p><strong>主干网络受限</strong>：当前 3D 模型大多依赖几何与位置编码，无法很好地捕捉全局语义，导致在 <strong>复杂场景、噪声干扰、传感器误差或数据损坏</strong> 下表现不佳。</p></li></ul><p>这些问题导致现有方法在鲁棒性和适应性上均受限制。</p><hr><p><strong>本文提出的解决方案 GEAL</strong>：</p><p>GEAL（<em>Generalizable 3D Affordance Learning</em>）旨在同时提升 <strong>泛化性</strong> 和 <strong>鲁棒性</strong>。它的设计核心包括：</p><ul><li><p><strong>双分支架构</strong>：一个分支处理 3D 点云，另一个分支通过 <strong>3D Gaussian Splatting (3DGS)</strong> 将稀疏点云渲染成逼真的 2D 图像，从而建立一致的 2D-3D 映射。这样可以利用 <strong>大规模预训练 2D 模型</strong> 的语义知识与泛化能力来增强 3D 分支。</p></li><li><p><strong>粒度自适应融合模块</strong>：动态融合多层次的视觉与文本特征，使模型能在不同尺度、不同粒度下准确回答可供性相关问题。</p></li><li><p><strong>2D-3D 一致性对齐模块</strong>：在特征层面建立 2D 与 3D 模态的可靠对应关系（通过嵌入到 3DGS 的高斯基元中实现），确保知识有效迁移，并提升 3D 分支的泛化与鲁棒性。</p></li></ul><hr><figure><img src="'+s+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>新的鲁棒性基准</strong>：</p><p>为了弥补现有研究缺乏鲁棒性评测的不足，作者构建了两个新的数据集：</p><ul><li><p><strong>PIAD-Corrupt</strong></p></li><li><p><strong>LASO-Corrupt</strong></p></li></ul><p>这两个基准数据集基于常用的 3D 可供性数据集构建，并通过引入 <strong>缩放、裁剪等真实场景中的损坏方式</strong> 来模拟噪声和破坏，从而提供一个标准化的评测平台。</p><hr><p><strong>实验结果与贡献总结</strong>：</p><p>大量实验表明，GEAL 在 <strong>已见类别、未见类别以及带有噪声/损坏的数据</strong> 上，均优于现有方法，显示出强大的适应性和鲁棒性。</p><p>本文的主要贡献可以总结如下：</p><ul><li><p>提出 GEAL，一种用于通用化 3D 可供性学习的新方法；通过 3DGS 构建 2D 分支，并利用预训练 2D 模型的语义知识提升 3D 预测能力。</p></li><li><p>设计 <strong>粒度自适应融合模块</strong> 与 <strong>2D-3D 一致性对齐模块</strong>，在双分支架构下实现跨模态知识整合与传播。</p></li><li><p>构建两个基于损坏的评测基准：<strong>PIAD-C</strong> 和 <strong>LASO-C</strong>，为社区提供了一个衡量 3D 可供性方法鲁棒性的标准。</p></li><li><p>在主流与损坏基准上进行大量实验，验证了 GEAL 在多种条件下均能保持优秀性能，具备较强的泛化能力和鲁棒性。</p></li></ul><h2 id="相关工作" tabindex="-1"><a class="header-anchor" href="#相关工作"><span>相关工作</span></a></h2><p><strong>2D 可供性学习</strong>：</p><p>可供性（affordances）是指物体或环境的属性决定了观察者可以执行的潜在动作。早期方法主要在图像或视频中识别交互区域，但缺乏对物体可供性相关部分的精确定位。</p><p>后来研究通过示例性的 2D 数据，改进了可供性定位精度。同时，大规模预训练模型可以将视觉特征与可供性相关的文本描述对齐，从而减少对人工标注的依赖，并在新场景下提升可供性预测能力。</p><p>近期一些研究进一步利用 <strong>基础模型（foundation models）</strong>，将可供性检测推广到新颖物体和不同视角，实现更好的泛化。</p><hr><p><strong>3D 可供性学习</strong>：</p><p>将可供性检测扩展到 3D 空间更具挑战，因为需要精确的空间和深度信息。</p><ul><li><p>一些方法尝试使用 2D 数据来预测 3D 可供性区域，但难以精确定位交互点位。</p></li><li><p>随着大规模 3D 物体数据集的出现，研究者开始直接将可供性映射到 3D 结构上，以捕捉复杂空间关系。</p></li><li><p>最近的方法利用 2D 视觉和语言模型进行开放词汇（open-vocabulary）可供性检测，在无需固定标签集的情况下增强泛化能力。</p></li></ul><p>尽管如此，3D 模型仍然缺乏 2D 基础模型的泛化能力，因此仍然很难实现稳健泛化。本文的方法正是通过引入 <strong>大规模 2D 基础模型</strong> 来提升 3D 可供性学习的泛化性。</p><hr><p><strong>3D 可供性学习的鲁棒性</strong>：</p><p>在真实世界中，3D 可供性学习容易受到点云损坏影响，这些损坏可能源自：</p><ul><li><p>场景复杂性</p></li><li><p>传感器误差</p></li><li><p>数据处理错误</p></li></ul><p>现有研究尝试提升 3D 感知在噪声和损坏条件下的鲁棒性，但可供性学习要求在 <strong>数据退化的情况下依然精确识别交互区域</strong>。</p><p>据作者所述，本文是 <strong>首个专门针对 3D 可供性学习鲁棒性</strong> 的研究，提出了一种针对性的解决方案，旨在提升模型在各种复杂环境下的可靠性。</p><h2 id="方法" tabindex="-1"><a class="header-anchor" href="#方法"><span>方法</span></a></h2>',39))])}const d=e(i,[["render",l]]),m=JSON.parse('{"path":"/3DVL/GEAL.html","title":"GEAL 论文","lang":"zh-CN","frontmatter":{"title":"GEAL 论文","icon":"file","category":["3D-VL"],"tag":["3D-VL","编辑中"],"footer":"技术共建，知识共享","date":"2025-09-15T00:00:00.000Z","author":["BinaryOracle"],"description":"GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency 论文","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"GEAL 论文\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-09-15T00:00:00.000Z\\",\\"dateModified\\":\\"2025-09-15T02:48:36.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BinaryOracle\\"}]}"],["meta",{"property":"og:url","content":"https://mister-hope.github.io/3DVL/GEAL.html"}],["meta",{"property":"og:site_name","content":"MetaMind"}],["meta",{"property":"og:title","content":"GEAL 论文"}],["meta",{"property":"og:description","content":"GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency 论文"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-09-15T02:48:36.000Z"}],["meta",{"property":"article:author","content":"BinaryOracle"}],["meta",{"property":"article:tag","content":"编辑中"}],["meta",{"property":"article:tag","content":"3D-VL"}],["meta",{"property":"article:published_time","content":"2025-09-15T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-09-15T02:48:36.000Z"}]]},"git":{"createdTime":1757904516000,"updatedTime":1757904516000,"contributors":[{"name":"BinaryOracle","username":"BinaryOracle","email":"3076679680@qq.com","commits":1,"url":"https://github.com/BinaryOracle"}]},"readingTime":{"minutes":4.87,"words":1460},"filePathRelative":"3DVL/GEAL.md","excerpt":"<p><code>GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency 论文</code></p>\\n","autoDesc":true}');export{d as comp,m as data};
