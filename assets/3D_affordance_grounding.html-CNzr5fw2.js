import{_ as o}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,a,d as i,e as n,o as r}from"./app-C6qHDjp7.js";const l="/assets/1-D1pbm_VH.png",s="/assets/4-b4D0c1aJ.png",p="/assets/7-B-7hJU15.png",c="/assets/6-B_Lsw_cS.png",d="/assets/2-Ch_jbl-N.png",f="/assets/5-DSB_t-NN.png",g="/assets/2-DhTi-rFx.png",h={};function u(m,e){return r(),t("div",null,[e[0]||(e[0]=a("p",null,[a("code",null,"3D Affordance Grounding 方向复盘")],-1)),i(" more "),e[1]||(e[1]=n('<h2 id="点云-文本" tabindex="-1"><a class="header-anchor" href="#点云-文本"><span>点云 + 文本</span></a></h2><h3 id="affogato-arxiv-2025-06" tabindex="-1"><a class="header-anchor" href="#affogato-arxiv-2025-06"><span><a href="https://arxiv.org/abs/2506.12009" target="_blank" rel="noopener noreferrer">Affogato (Arxiv 2025.06)</a></span></a></h3><p>特点:</p><ol><li><p>AFFOrdance Grounding All aT Once</p></li><li><p>a large-scale dataset for 3D and 2D affordance grounding</p></li><li><p>minimalistic architecture</p></li></ol><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>损失函数:</p><ol><li><p>Focal Loss to handle class imbalance</p></li><li><p>Dice Loss to improve region-level alignment.</p></li></ol><p>现状:</p><ol><li><p>wait for code release</p></li><li><p>dataset available</p></li></ol><h3 id="seqafford-cvpr-2025" tabindex="-1"><a class="header-anchor" href="#seqafford-cvpr-2025"><span><a href="https://arxiv.org/abs/2412.01550" target="_blank" rel="noopener noreferrer">SeqAfford (CVPR 2025)</a></span></a></h3><p>特点:</p><ol><li><p>Propose a 3D multimodal large language model (referring to the LLaVA model architecture)</p></li><li><p>Feed the <code>&lt;SEG&gt;</code> segmentation tokens output by the 3D MMLLM into the multi-granularity language-point cloud combination module to complete 3D dense prediction</p></li><li><p>Support sequential instruction execution</p></li><li><p>Large-scale instruction-point cloud pair dataset: A dataset with 180,000 instruction-point cloud pairs, covering single and sequential operability reasoning tasks</p></li></ol><figure><img src="'+s+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>损失函数:</p><ol><li><p>Autoregressive Cross-Entropy Loss</p></li><li><p>Dice Loss</p></li><li><p>Binary Cross-Entropy Loss</p></li></ol><p>现状:</p><ol><li><p>code available</p></li><li><p>dataset available</p></li></ol><h3 id="laso-cvpr-2024" tabindex="-1"><a class="header-anchor" href="#laso-cvpr-2024"><span><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_LASO_Language-guided_Affordance_Segmentation_on_3D_Object_CVPR_2024_paper.pdf" target="_blank" rel="noopener noreferrer">LASO (CVPR 2024)</a></span></a></h3><blockquote><p>需要二次回顾思考</p></blockquote><p>特点:</p><ol><li><p>PointRefer : The Adaptive Fusion Module is responsible for injecting semantic information at multiple scales. The Referred Point Decoder will introduce a set of affordance queries to interact with the point cloud features and complete the generation of dynamic convolution kernels.</p></li><li><p>LASO Dataset : 19,751 question-point affordance pairs</p></li></ol><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>损失函数:</p><ol><li>Focal Loss + Dice Loss</li></ol><p>现状:</p><ol><li>code and dataset available</li></ol><h2 id="点云-图像" tabindex="-1"><a class="header-anchor" href="#点云-图像"><span>点云 + 图像</span></a></h2><h3 id="iagnet-iccv-2023" tabindex="-1"><a class="header-anchor" href="#iagnet-iccv-2023"><span><a href="https://arxiv.org/abs/2303.10437" target="_blank" rel="noopener noreferrer">IAGNet (ICCV 2023)</a></span></a></h3><p>特点:</p><ol><li>Learn from 2D interactive images and generalize to 3D point clouds to infer affordance regions</li></ol><blockquote><p>Joint_Region_Alignment(JRA), Affordance_Revealed_Module(ARM), Alignment of feature distributions between image and point cloud regions (KL Loss), Local + Global Prediction</p></blockquote><ol start="2"><li>Propose the PIAD dataset: It comprises 7012 point clouds and 5162 images, spanning 23 object classes and 17 affordance categories.</li></ol><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>损失函数:</p><ol><li><p>Heatmap Loss (HM_Loss): Point-wise 3D affordance mask prediction = Focal Loss + Dice Loss</p></li><li><p>Cross-Entropy Loss (CE Loss): Global affordance classification</p></li><li><p>KL-Divergence Loss (KL Loss): Make the feature distributions of the interaction regions on the image side close to those on the point cloud side</p></li></ol><p>现状:</p><ol><li>code and dataset available</li></ol><h2 id="点云-文本-图像" tabindex="-1"><a class="header-anchor" href="#点云-文本-图像"><span>点云 + 文本 + 图像</span></a></h2><h3 id="great-cvpr-2025" tabindex="-1"><a class="header-anchor" href="#great-cvpr-2025"><span><a href="https://arxiv.org/abs/2411.19626" target="_blank" rel="noopener noreferrer">GREAT (CVPR 2025)</a></span></a></h3><p>特点:</p><ol><li><p>grounding 3D object affordance in an Open-Vocabulary fashion</p></li><li><p>Multi-Head Affordance Chain-of-Thought</p></li></ol><blockquote><p>Data preparation stage:</p><ol><li><p>Use prompts to generate descriptions of the object interaction area, the morphology(形态学) of the interaction area, the interaction behavior, and other common interaction behaviors of the object.</p></li><li><p>Geometric structure knowledge = Answers to Prompt 1 + Prompt 2 = Interaction parts + Inference of geometric properties of these parts</p></li><li><p>Interaction knowledge = Answers to Prompt 3 + Prompt 4 = Current interaction + Analogous(类似的)/supplementary(补充) interaction methods</p></li></ol></blockquote><ol start="3"><li>PIADv2 dataset</li></ol><blockquote><p>24 affordance , 43 object categories, 15K interaction images , 38K 3D objects with annotations.</p></blockquote><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>损失函数:</p><ol><li><p>Focal Loss to handle class imbalance</p></li><li><p>Dice Loss to improve region-level alignment.</p></li></ol><p>现状:</p><ol><li><p>code available</p></li><li><p>dataset available</p></li></ol><h3 id="lmaffordance3d-cvpr-2025" tabindex="-1"><a class="header-anchor" href="#lmaffordance3d-cvpr-2025"><span><a href="https://arxiv.org/abs/2504.04744" target="_blank" rel="noopener noreferrer">LMAffordance3D (CVPR 2025)</a></span></a></h3><p>特点:</p><ol><li><p>Combine language instructions, visual observations, and interaction information to locate the affordance of manipulable objects in 3D space.</p></li><li><p>AGPIL（Affordance Grounding dataset with Points, Images and Language instructions）</p></li></ol><blockquote><p>This dataset includes estimations of object affordances observed from full-view, partial-view, and rotated perspectives, taking into account factors such as real-world observation angles, object rotation, and spatial occlusion (遮挡).</p></blockquote><figure><img src="'+f+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>损失函数:</p><ol><li><p>focal loss</p></li><li><p>dice loss</p></li></ol><p>现状:</p><ol><li><p>wait for code release</p></li><li><p>dataset available</p></li></ol><h2 id="_3d-gaussian-splatting-3dgs" tabindex="-1"><a class="header-anchor" href="#_3d-gaussian-splatting-3dgs"><span>3D Gaussian Splatting (3DGS)</span></a></h2><h3 id="geal-cvpr-2025" tabindex="-1"><a class="header-anchor" href="#geal-cvpr-2025"><span><a href="https://arxiv.org/abs/2412.09511" target="_blank" rel="noopener noreferrer">GEAL (CVPR 2025)</a></span></a></h3><p>特点:</p><ol><li><p>&quot;Knowledge Distillation&quot; from 2D to 3D: Transfer the semantic capabilities of pre-trained 2D models to the 3D affordance prediction model through Gaussian splat mapping, cross-modal consistency alignment, and multi-scale fusion.</p></li><li><p>Noisy Dataset: Construct a new benchmark with multiple types of noise/damage to evaluate the generalization and robustness of the model under real/harsh conditions.</p></li></ol><figure><img src="'+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>损失函数:</p><ol><li><p>BCE</p></li><li><p>Dice Loss</p></li><li><p>Consistency Loss（MSE 损失）</p></li></ol><p>现状:</p><ol><li><p>wait for code release</p></li><li><p>wait for dataset release</p></li></ol><h3 id="_3daffordsplat-arxiv-2025-04" tabindex="-1"><a class="header-anchor" href="#_3daffordsplat-arxiv-2025-04"><span><a href="https://arxiv.org/abs/2504.11218" target="_blank" rel="noopener noreferrer">3DAffordSplat (Arxiv 2025.04)</a></span></a></h3><h3 id="iaao-cvpr-2025" tabindex="-1"><a class="header-anchor" href="#iaao-cvpr-2025"><span><a href="https://arxiv.org/abs/2504.06827" target="_blank" rel="noopener noreferrer">IAAO (CVPR 2025)</a></span></a></h3><h2 id="idea" tabindex="-1"><a class="header-anchor" href="#idea"><span>idea</span></a></h2><p>Momentum Encoder 生成伪标签应对噪声问题，实现更加稳健的学习 ？(参考: MoCo , ALBEF , DINO) --&gt; 更大的数据集，包含更多噪声</p><p>语言，图像，点云 统一化输入到 多模态大模型中 ？ --》 参考 VLMo or BEiT ？</p>',72))])}const v=o(h,[["render",u]]),D=JSON.parse('{"path":"/3DVL/3D_affordance_grounding.html","title":"3D Affordance Grounding 方向复盘","lang":"zh-CN","frontmatter":{"title":"3D Affordance Grounding 方向复盘","icon":"file","category":["3D-VL"],"tag":["3D-VL","编辑中"],"footer":"技术共建，知识共享","date":"2025-09-15T00:00:00.000Z","author":["BinaryOracle"],"description":"3D Affordance Grounding 方向复盘","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"3D Affordance Grounding 方向复盘\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-09-15T00:00:00.000Z\\",\\"dateModified\\":\\"2025-09-16T06:03:08.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"BinaryOracle\\"}]}"],["meta",{"property":"og:url","content":"https://mister-hope.github.io/3DVL/3D_affordance_grounding.html"}],["meta",{"property":"og:site_name","content":"MetaMind"}],["meta",{"property":"og:title","content":"3D Affordance Grounding 方向复盘"}],["meta",{"property":"og:description","content":"3D Affordance Grounding 方向复盘"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-09-16T06:03:08.000Z"}],["meta",{"property":"article:author","content":"BinaryOracle"}],["meta",{"property":"article:tag","content":"编辑中"}],["meta",{"property":"article:tag","content":"3D-VL"}],["meta",{"property":"article:published_time","content":"2025-09-15T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-09-16T06:03:08.000Z"}]]},"git":{"createdTime":1757901173000,"updatedTime":1758002588000,"contributors":[{"name":"BinaryOracle","username":"BinaryOracle","email":"3076679680@qq.com","commits":9,"url":"https://github.com/BinaryOracle"}]},"readingTime":{"minutes":2.82,"words":846},"filePathRelative":"3DVL/3D_affordance_grounding.md","excerpt":"<p><code>3D Affordance Grounding 方向复盘</code></p>\\n","autoDesc":true}');export{v as comp,D as data};
