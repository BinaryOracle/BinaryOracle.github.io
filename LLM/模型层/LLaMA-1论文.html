<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.24" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.94" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"LLaMA-1论文","image":[""],"datePublished":"2025-06-27T00:00:00.000Z","dateModified":"2025-06-27T06:41:50.000Z","author":[{"@type":"Person","name":"BinaryOracle"}]}</script><meta property="og:url" content="https://mister-hope.github.io/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/LLaMA-1%E8%AE%BA%E6%96%87.html"><meta property="og:site_name" content="MetaMind"><meta property="og:title" content="LLaMA-1论文"><meta property="og:description" content="LLaMA-1 论文"><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-06-27T06:41:50.000Z"><meta property="article:author" content="BinaryOracle"><meta property="article:tag" content="已发布"><meta property="article:tag" content="预训练语言模型"><meta property="article:published_time" content="2025-06-27T00:00:00.000Z"><meta property="article:modified_time" content="2025-06-27T06:41:50.000Z"><title>LLaMA-1论文 | MetaMind</title><meta name="description" content="LLaMA-1 论文">
    <link rel="preload" href="/assets/style-B9ka0LB5.css" as="style"><link rel="stylesheet" href="/assets/style-B9ka0LB5.css">
    <link rel="modulepreload" href="/assets/app-B1cJYw99.js"><link rel="modulepreload" href="/assets/LLaMA-1论文.html-BD2dwtB5.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/assets/index.html-DL9PmnzV.js" as="script"><link rel="prefetch" href="/assets/intro.html-jI0UiWWL.js" as="script"><link rel="prefetch" href="/assets/GREAT.html-Cwm8Kk80.js" as="script"><link rel="prefetch" href="/assets/Grounding_3D_Object_Affordance.html-DhOrlO4y.js" as="script"><link rel="prefetch" href="/assets/IAGNet.html-DhUZij5x.js" as="script"><link rel="prefetch" href="/assets/LASO.html-CjHavASx.js" as="script"><link rel="prefetch" href="/assets/index.html-BIKOkmJo.js" as="script"><link rel="prefetch" href="/assets/简析PointNet__.html-BU9qeKGW.js" as="script"><link rel="prefetch" href="/assets/简析PointNet.html-Bg07hd9-.js" as="script"><link rel="prefetch" href="/assets/index.html-PODBhNtD.js" as="script"><link rel="prefetch" href="/assets/ BLIP.html-B7HE3c9c.js" as="script"><link rel="prefetch" href="/assets/ALBEF.html-DybVnsDb.js" as="script"><link rel="prefetch" href="/assets/BEIT2.html-DG5idGEe.js" as="script"><link rel="prefetch" href="/assets/BEIT3.html-ZJqC_R1w.js" as="script"><link rel="prefetch" href="/assets/BEiT.html-DDwZOOU7.js" as="script"><link rel="prefetch" href="/assets/BEiT模型代码解读.html-Ufysiy_x.js" as="script"><link rel="prefetch" href="/assets/DINO.html-C3EDXrP-.js" as="script"><link rel="prefetch" href="/assets/InternVL-1.0.html-CWMhUfsw.js" as="script"><link rel="prefetch" href="/assets/InternVl-1.5.html-k833Nx8e.js" as="script"><link rel="prefetch" href="/assets/LLaVA_1.0.html-B9FZUFXZ.js" as="script"><link rel="prefetch" href="/assets/MoCo.html-tUJi1cNR.js" as="script"><link rel="prefetch" href="/assets/index.html-E6dxBCQP.js" as="script"><link rel="prefetch" href="/assets/VLMo.html-CxwrFwUI.js" as="script"><link rel="prefetch" href="/assets/VLMo代码解读.html-Dq8Hu3aa.js" as="script"><link rel="prefetch" href="/assets/ViLT.html-CqKK8EZN.js" as="script"><link rel="prefetch" href="/assets/多模态常用改编Bert实现.html-BcU-SMfQ.js" as="script"><link rel="prefetch" href="/assets/多模态模型CLIP原理与图片分类，文字搜索图像实战演练.html-D6TwCbLh.js" as="script"><link rel="prefetch" href="/assets/庖丁解牛BLIP2.html-CsVFN0sc.js" as="script"><link rel="prefetch" href="/assets/庖丁解牛VIT.html-BVV1GUel.js" as="script"><link rel="prefetch" href="/assets/index.html-Cjh7XVoF.js" as="script"><link rel="prefetch" href="/assets/API记录之Numpy篇.html-B0FlLm1V.js" as="script"><link rel="prefetch" href="/assets/API记录之Python篇.html-KWf92X1n.js" as="script"><link rel="prefetch" href="/assets/API记录之Pytorch篇.html-CQNshe_B.js" as="script"><link rel="prefetch" href="/assets/API记录之杂类篇.html-DFZAJFyp.js" as="script"><link rel="prefetch" href="/assets/API记录之框架篇.html-D9UT5Qan.js" as="script"><link rel="prefetch" href="/assets/Attention运算过程中维度变换的理解.html-BYEyg2Hg.js" as="script"><link rel="prefetch" href="/assets/Pytorch张量存储与访问原理.html-4kHSIVmy.js" as="script"><link rel="prefetch" href="/assets/index.html-CZ0lz8fM.js" as="script"><link rel="prefetch" href="/assets/conda虚拟环境管理.html-gMsAjE-g.js" as="script"><link rel="prefetch" href="/assets/常用评估指标.html-CKFrbsc0.js" as="script"><link rel="prefetch" href="/assets/数学知识点.html-C82BZTkM.js" as="script"><link rel="prefetch" href="/assets/注意力图可视化.html-vsMLN87q.js" as="script"><link rel="prefetch" href="/assets/语义分割中常用的损失函数.html-BGDaBR34.js" as="script"><link rel="prefetch" href="/assets/通俗易懂解读BPE分词算法实现.html-Bx6Cyy69.js" as="script"><link rel="prefetch" href="/assets/index.html-rO6B9hfB.js" as="script"><link rel="prefetch" href="/assets/Fine_Tuning知识扫盲.html-D13Bq7kq.js" as="script"><link rel="prefetch" href="/assets/LoRA微调系列.html-B0Yx4oKO.js" as="script"><link rel="prefetch" href="/assets/Prompt_Engineering知识扫盲.html-GwXJpGCk.js" as="script"><link rel="prefetch" href="/assets/index.html-CcR-ZXcH.js" as="script"><link rel="prefetch" href="/assets/GPT-1论文.html-fAp6T9c7.js" as="script"><link rel="prefetch" href="/assets/GPT-2论文.html-Bx-1CqOk.js" as="script"><link rel="prefetch" href="/assets/GPT-3论文.html-d8vjbsav.js" as="script"><link rel="prefetch" href="/assets/InstructGPT论文.html-Dm6zzRVN.js" as="script"><link rel="prefetch" href="/assets/KV-Cache.html-B3sVza1M.js" as="script"><link rel="prefetch" href="/assets/LLaMA-2论文.html-GPZhfaDn.js" as="script"><link rel="prefetch" href="/assets/index.html-Dgxz-7Po.js" as="script"><link rel="prefetch" href="/assets/RoBERTa论文.html-DlIu4_mM.js" as="script"><link rel="prefetch" href="/assets/从零实现Bert.html-B_Xcmzo3.js" as="script"><link rel="prefetch" href="/assets/位置编码.html-BCzn5DMk.js" as="script"><link rel="prefetch" href="/assets/图解BERT.html-CCDElCnA.js" as="script"><link rel="prefetch" href="/assets/图解Transformer.html-BBEjEx5C.js" as="script"><link rel="prefetch" href="/assets/1.自动微分.html-CcaCEPD8.js" as="script"><link rel="prefetch" href="/assets/2.用自然的代码表达.html-C5DqgBnB.js" as="script"><link rel="prefetch" href="/assets/3.高阶导数.html-D0WJBrm-.js" as="script"><link rel="prefetch" href="/assets/4.神经网络.html-DuSSyOCr.js" as="script"><link rel="prefetch" href="/assets/index.html-BC_DE9R9.js" as="script"><link rel="prefetch" href="/assets/1.前置知识.html-CeHvhQQr.js" as="script"><link rel="prefetch" href="/assets/2.大模型API.html-DpLqezP1.js" as="script"><link rel="prefetch" href="/assets/index.html-rWolRo6n.js" as="script"><link rel="prefetch" href="/assets/index.html-hUyNVABD.js" as="script"><link rel="prefetch" href="/assets/index.html-6qncRfRt.js" as="script"><link rel="prefetch" href="/assets/index.html-DBSf3Kng.js" as="script"><link rel="prefetch" href="/assets/index.html-BORdW0wI.js" as="script"><link rel="prefetch" href="/assets/基础概念.html-DKc0K3Jv.js" as="script"><link rel="prefetch" href="/assets/基础模型.html-BY7jR_iv.js" as="script"><link rel="prefetch" href="/assets/组合分析.html-i8NFhutn.js" as="script"><link rel="prefetch" href="/assets/DALL-E论文.html-B6uiUA0y.js" as="script"><link rel="prefetch" href="/assets/DALL·E模型代码解读.html-Cf6IDZzj.js" as="script"><link rel="prefetch" href="/assets/GAN学习笔记.html-CTaPQfiN.js" as="script"><link rel="prefetch" href="/assets/PixelCNN.html-ymIxd-NQ.js" as="script"><link rel="prefetch" href="/assets/Pytorch实现VAE和CVAE.html-UEHFydLF.js" as="script"><link rel="prefetch" href="/assets/index.html-BZAJq6DW.js" as="script"><link rel="prefetch" href="/assets/Tutorial_VAE.html-9Gy3l946.js" as="script"><link rel="prefetch" href="/assets/VQ-VAE.html-BqVdcN02.js" as="script"><link rel="prefetch" href="/assets/WGAN.html-C_2cyEds.js" as="script"><link rel="prefetch" href="/assets/404.html-bi3vZbpF.js" as="script"><link rel="prefetch" href="/assets/index.html-CdgRxuj1.js" as="script"><link rel="prefetch" href="/assets/index.html-CkhJnbmL.js" as="script"><link rel="prefetch" href="/assets/index.html-kjDdy_hd.js" as="script"><link rel="prefetch" href="/assets/index.html-BiA2o8UZ.js" as="script"><link rel="prefetch" href="/assets/index.html-Z3ERwf2m.js" as="script"><link rel="prefetch" href="/assets/index.html-CBU4vHSf.js" as="script"><link rel="prefetch" href="/assets/index.html-BrM7699q.js" as="script"><link rel="prefetch" href="/assets/index.html-HJ2_RpNs.js" as="script"><link rel="prefetch" href="/assets/index.html-C2AdUToo.js" as="script"><link rel="prefetch" href="/assets/index.html-DaLAlvSC.js" as="script"><link rel="prefetch" href="/assets/index.html-C4iuW3e_.js" as="script"><link rel="prefetch" href="/assets/index.html-X4FeSzmg.js" as="script"><link rel="prefetch" href="/assets/index.html-DyPva3Dn.js" as="script"><link rel="prefetch" href="/assets/index.html-BpRVvavz.js" as="script"><link rel="prefetch" href="/assets/index.html-t6VJl9AA.js" as="script"><link rel="prefetch" href="/assets/index.html-qx2I63ej.js" as="script"><link rel="prefetch" href="/assets/index.html-D1U095Ua.js" as="script"><link rel="prefetch" href="/assets/index.html-DF_G4rg_.js" as="script"><link rel="prefetch" href="/assets/index.html-BgieBYvj.js" as="script"><link rel="prefetch" href="/assets/index.html-DXip8zJ8.js" as="script"><link rel="prefetch" href="/assets/index.html-DmATBDNL.js" as="script"><link rel="prefetch" href="/assets/index.html-B8YqqdvN.js" as="script"><link rel="prefetch" href="/assets/index.html-D1Rxdrun.js" as="script"><link rel="prefetch" href="/assets/index.html-B-o5Yi4Y.js" as="script"><link rel="prefetch" href="/assets/index.html-Bv3CSDqx.js" as="script"><link rel="prefetch" href="/assets/index.html-DwxJkVJO.js" as="script"><link rel="prefetch" href="/assets/index.html-myBh4gkI.js" as="script"><link rel="prefetch" href="/assets/index.html-BgHAk5Gp.js" as="script"><link rel="prefetch" href="/assets/index.html-u2cp-7KS.js" as="script"><link rel="prefetch" href="/assets/index.html-Db12i2NY.js" as="script"><link rel="prefetch" href="/assets/index.html-BHRZC1AI.js" as="script"><link rel="prefetch" href="/assets/index.html-CJlZktBc.js" as="script"><link rel="prefetch" href="/assets/index.html-Dauq5EGj.js" as="script"><link rel="prefetch" href="/assets/index.html-C8DZVGlT.js" as="script"><link rel="prefetch" href="/assets/index.html-DoxQqK45.js" as="script"><link rel="prefetch" href="/assets/index.html-T7S7gSzG.js" as="script"><link rel="prefetch" href="/assets/index.html-C07_-Jhy.js" as="script"><link rel="prefetch" href="/assets/index.html-B07AgUPJ.js" as="script"><link rel="prefetch" href="/assets/index.html-BsqJQrh2.js" as="script"><link rel="prefetch" href="/assets/index-BzBQJFYZ.js" as="script"><link rel="prefetch" href="/assets/flowchart-BKGBxuOE.js" as="script"><link rel="prefetch" href="/assets/mermaid.esm.min-D0OH-Mft.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-CKV1Bsxh.js" as="script"><link rel="prefetch" href="/assets/SearchResult-CllyFnF-.js" as="script"><link rel="prefetch" href="/assets/waline-meta-l0sNRNKZ.js" as="script"><link rel="prefetch" href="/assets/component-C_TZJxkU.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><a class="route-link vp-brand" href="/" aria-label="带我回家"><img class="vp-nav-logo" src="/assets/images/head.png" alt><!----><span class="vp-site-name hide-in-pad">MetaMind</span></a><!--]--></div><div class="vp-navbar-center"><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/" aria-label="主页"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:house" sizing="height" height="1em"></iconify-icon><!--]-->主页<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link route-link-active auto-link" href="/LLM/" aria-label="大语言模型"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:book" sizing="height" height="1em"></iconify-icon><!--]-->大语言模型<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/MMLLM/" aria-label="多模态"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:laptop-code" sizing="height" height="1em"></iconify-icon><!--]-->多模态<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/3DVL/" aria-label="3D-Vision Language"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:eye" sizing="height" height="1em"></iconify-icon><!--]-->3D-Vision Language<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/other_direction/" aria-label="其他方向"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:computer" sizing="height" height="1em"></iconify-icon><!--]-->其他方向<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/open_projects/" aria-label="开源项目"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:star" sizing="height" height="1em"></iconify-icon><!--]-->开源项目<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/other/" aria-label="杂谈"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:sun" sizing="height" height="1em"></iconify-icon><!--]-->杂谈<!----></a></div></nav><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!----><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!--[--><button type="button" class="slimsearch-button" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="slimsearch-placeholder">搜索</div><div class="slimsearch-key-hints"><kbd class="slimsearch-key">Ctrl</kbd><kbd class="slimsearch-key">K</kbd></div></button><!--]--><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/" aria-label="主页"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:house" sizing="both" width="1em" height="1em"></iconify-icon><!--]-->主页<!----></a></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:book" sizing="both" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">大语言模型</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:robot" sizing="both" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">应用层</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:book" sizing="both" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">模型层</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/%E5%9B%BE%E8%A7%A3Transformer.html" aria-label="图解Transformer"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" sizing="both" width="1em" height="1em"></iconify-icon><!--]-->图解Transformer<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/GPT-1%E8%AE%BA%E6%96%87.html" aria-label="GPT-1 论文"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" sizing="both" width="1em" height="1em"></iconify-icon><!--]-->GPT-1 论文<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/GPT-2%E8%AE%BA%E6%96%87.html" aria-label="GPT-2 论文"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" sizing="both" width="1em" height="1em"></iconify-icon><!--]-->GPT-2 论文<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/GPT-3%E8%AE%BA%E6%96%87.html" aria-label="GPT-3 论文"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" sizing="both" width="1em" height="1em"></iconify-icon><!--]-->GPT-3 论文<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/InstructGPT%E8%AE%BA%E6%96%87.html" aria-label="InstructGPT 论文"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" sizing="both" width="1em" height="1em"></iconify-icon><!--]-->InstructGPT 论文<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/KV-Cache.html" aria-label="KV-Cache 详解"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" sizing="both" width="1em" height="1em"></iconify-icon><!--]-->KV-Cache 详解<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/LLaMA-1%E8%AE%BA%E6%96%87.html" aria-label="LLaMA-1论文"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" sizing="both" width="1em" height="1em"></iconify-icon><!--]-->LLaMA-1论文<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/LLaMA-2%E8%AE%BA%E6%96%87.html" aria-label="LLaMA-2论文"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" sizing="both" width="1em" height="1em"></iconify-icon><!--]-->LLaMA-2论文<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/RoBERTa%E8%AE%BA%E6%96%87.html" aria-label="RoBERTa 论文"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" sizing="both" width="1em" height="1em"></iconify-icon><!--]-->RoBERTa 论文<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0Bert.html" aria-label="从&quot;零&quot;实现 Bert"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" sizing="both" width="1em" height="1em"></iconify-icon><!--]-->从&quot;零&quot;实现 Bert<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/%E5%9B%BE%E8%A7%A3BERT.html" aria-label="图解 Bert"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" sizing="both" width="1em" height="1em"></iconify-icon><!--]-->图解 Bert<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81.html" aria-label="位置编码"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" sizing="both" width="1em" height="1em"></iconify-icon><!--]-->位置编码<!----></a></li></ul></section></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:laptop-code" sizing="both" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">多模态</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:eye" sizing="both" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">3D-Vision Language</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:computer" sizing="both" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">其他方向</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:star" sizing="both" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">开源项目</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:sun" sizing="both" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">杂谈</span><span class="vp-arrow end"></span></button><!----></section></li><li><a class="route-link auto-link vp-sidebar-link" href="/intro.html" aria-label="关于我们"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:circle-info" sizing="both" width="1em" height="1em"></iconify-icon><!--]-->关于我们<!----></a></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><iconify-icon class="vp-icon" icon="fa6-solid:file" sizing="height" height="1em"></iconify-icon>LLaMA-1论文</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">BinaryOracle</span></span><span property="author" content="BinaryOracle"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">2025/6/27</span><meta property="datePublished" content="2025-06-27T00:00:00.000Z"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color2 clickable" role="navigation">NLP</span><!--]--><meta property="articleSection" content="NLP"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color6 clickable" role="navigation">预训练语言模型</span><span class="page-tag-item color4 clickable" role="navigation">已发布</span><!--]--><meta property="keywords" content="预训练语言模型,已发布"></span><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 9 分钟</span><meta property="timeRequired" content="PT9M"></span><span class="page-word-info" aria-label="字数🔠" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon word-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="word icon" name="word"><path d="M518.217 432.64V73.143A73.143 73.143 0 01603.43 1.097a512 512 0 01419.474 419.474 73.143 73.143 0 01-72.046 85.212H591.36a73.143 73.143 0 01-73.143-73.143z"></path><path d="M493.714 566.857h340.297a73.143 73.143 0 0173.143 85.577A457.143 457.143 0 11371.566 117.76a73.143 73.143 0 0185.577 73.143v339.383a36.571 36.571 0 0036.571 36.571z"></path></svg><span>约 2731 字</span><meta property="wordCount" content="2731"></span></div><hr></div><!----><div class="" vp-content><!----><div id="markdown-content"><p><code>LLaMA-1 论文</code></p><!-- more --><blockquote><p>论文链接: <a href="https://arxiv.org/abs/2302.13971" target="_blank" rel="noopener noreferrer">LLaMA: Open and Efficient Foundation Language Models</a></p></blockquote><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>LLaMA是一系列高效的基础语言模型，参数规模从7B到65B不等，其特点在于仅使用公开可用的数据集进行训练，而无需依赖专有数据。实验结果表明，LLaMA-13B在多数基准测试中优于GPT-3（175B），而LLaMA-65B则与Chinchilla-70B和PaLM-540B等顶尖模型表现相当。这些模型的发布旨在促进研究社区的开放访问和研究，部分模型甚至可以在单个GPU上运行。</p><h2 id="简介" tabindex="-1"><a class="header-anchor" href="#简介"><span>简介</span></a></h2><ol><li><strong>模型规模与性能的重新思考</strong></li></ol><p>论文指出传统观点认为模型参数越多性能越优（如GPT-3的175B参数），但Hoffmann等人（<a href="https://arxiv.org/abs/2203.15556" target="_blank" rel="noopener noreferrer">2022</a>）的研究表明，在固定计算预算下，<strong>小模型+更多数据训练</strong>可能更优。例如，LLaMA-7B在1T tokens训练后性能持续提升（见图1训练损失曲线），而Hoffmann推荐的10B模型仅训练200B tokens即停止。这一发现挑战了单纯追求参数规模的范式。</p><figure><img src="/assets/1-C6oXP9eH.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol start="2"><li><strong>推理效率的核心目标</strong></li></ol><p>LLaMA强调<strong>推理成本优化</strong>而非单纯训练速度。论文指出，虽然大模型训练更快达到目标性能，但小模型在长期训练后推理效率更高（如13B模型比GPT-3小10倍却性能更优）。这一设计理念直接反映在模型架构选择上（见表2的参数字段与学习率配置）。</p><figure><img src="/assets/2-D96oYONU.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol start="3"><li><strong>数据策略与开源兼容性</strong></li></ol><p>与Chinchilla、PaLM等依赖未公开数据（如&quot;Books-2TB&quot;）不同，LLaMA<strong>仅使用公开数据</strong>（CommonCrawl 67%、C4 15%、GitHub 4.5%等，详见表1），使其完全可开源。这一策略虽限制数据量（总计1.4T tokens），但通过高效训练仍实现SOTA。</p><figure><img src="/assets/3-DG16E1t9.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol start="4"><li><strong>性能验证与社会责任</strong></li></ol><ul><li><p>65B模型在常识推理（表3）、闭卷问答（表4-5）等任务上超越Chinchilla-70B</p></li><li><p>代码生成（表8）和数学推理（表7）的竞争力</p></li><li><p>同时分析模型偏见（表12-13）与毒性（表11），呼应AI伦理需求</p></li></ul><h2 id="方法" tabindex="-1"><a class="header-anchor" href="#方法"><span>方法</span></a></h2><p><strong>1. 预训练数据与处理</strong></p><p>LLaMA采用<strong>纯公开数据混合</strong>，总规模1.4T tokens，主要来源包括：</p><ul><li><p><strong>CommonCrawl（67%）</strong>：经CCNet流水线去重、语言识别（保留英文）和质量过滤（基于Wikipedia引用分类）。</p></li><li><p><strong>C4（15%）</strong>：补充多样性，启发式过滤低质量网页（如标点缺失）。</p></li><li><p><strong>代码与学术数据</strong>：GitHub（4.5%，MIT/Apache许可项目）、ArXiv（2.5%，移除宏定义和参考文献）、Stack Exchange（2%，按评分排序答案）。</p></li></ul><p>其他数据如Wikipedia（4.5%）和书籍（Gutenberg/Books3，4.5%）均经过严格去重（见表1的采样比例与磁盘大小）。</p><p><strong>Tokenizer</strong>：使用SentencePiece的BPE算法，数字拆分为独立字符，UTF-8回退到字节级处理。</p><hr><p><strong>2. 模型架构改进</strong></p><p>基于Transformer的优化设计（对比原始架构）：</p><ul><li><p><strong>预归一化（Pre-normalization）</strong>：采用RMSNorm对子层输入归一化（灵感来自GPT-3），提升训练稳定性。</p></li><li><p><strong>激活函数</strong>：替换ReLU为<strong>SwiGLU</strong>（PaLM方案），隐藏层维度设为 <mjx-container class="MathJax" jax="SVG" style="position:relative;"><svg style="vertical-align:-0.816ex;" xmlns="http://www.w3.org/2000/svg" width="4.103ex" height="2.773ex" role="img" focusable="false" viewBox="0 -864.9 1813.6 1225.5" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(220,394) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mn" transform="translate(220,-345) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><rect width="553.6" height="60" x="120" y="220"></rect></g><g data-mml-node="mn" transform="translate(793.6,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mi" transform="translate(1293.6,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mn>2</mn><mn>3</mn></mfrac><mn>4</mn><mi>d</mi></math></mjx-assistive-mml></mjx-container> 以平衡计算效率。</p></li><li><p><strong>位置编码</strong>：使用<strong>旋转位置嵌入（RoPE）</strong>（GPT-NeoX方案），替代绝对位置编码。</p></li></ul><p>详细参数配置见表2，例如65B模型维度为8192、64头注意力、80层。</p><hr><p><strong>3. 训练优化策略</strong></p><ul><li><p><strong>优化器</strong>：AdamW（<mjx-container class="MathJax" jax="SVG" style="position:relative;"><svg style="vertical-align:-0.439ex;" xmlns="http://www.w3.org/2000/svg" width="18.491ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 8172.9 899" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g><g data-mml-node="mn" transform="translate(599,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1280.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(2336.1,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(3614.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(4058.8,0)"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g><g data-mml-node="mn" transform="translate(599,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(5339.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(6394.9,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z" transform="translate(778,0)"></path><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" transform="translate(1278,0)"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>β</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn><mo>,</mo><msub><mi>β</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></math></mjx-assistive-mml></mjx-container>），余弦学习率调度（最终学习率为峰值10%），权重衰减0.1，梯度裁剪1.0。</p></li><li><p><strong>效率优化</strong>：</p><ul><li><p><strong>内存管理</strong>：通过<code>xformers</code>库实现因果多头注意力的高效计算，避免存储注意力权重（参考<a href="https://arxiv.org/abs/2112.05682" target="_blank" rel="noopener noreferrer">Rabe &amp; Staats 2021</a>）。</p></li><li><p><strong>激活检查点（Checkpointing）</strong>：手动实现线性层反向传播，减少重计算（节省GPU内存）。</p></li><li><p><strong>并行策略</strong>：模型与序列并行（<a href="https://arxiv.org/abs/2205.05198" target="_blank" rel="noopener noreferrer">Korthikanti et al. 2022</a>），重叠计算与GPU通信。</p></li></ul></li></ul><p>如图1所示，65B模型在2048块A100（80GB）上训练速度达<strong>380 tokens/sec/GPU</strong>，1.4T tokens训练耗时约21天。</p><hr><p><strong>总结</strong></p><p>LLaMA的方法论核心是通过<strong>数据质量优化</strong>（公开数据+严格过滤）、<strong>架构微调</strong>（SwiGLU/RoPE）和<strong>工程创新</strong>（内存/并行优化）实现高效训练。其设计始终围绕推理效率目标（如小模型长期训练），最终在多个基准测试中超越更大规模的闭源模型。</p><h2 id="结果" tabindex="-1"><a class="header-anchor" href="#结果"><span>结果</span></a></h2><p><strong>1. 常识推理（Common Sense Reasoning）</strong></p><ul><li><p><strong>零样本性能</strong>（表3）： LLaMA-65B在8个常识推理基准（如BoolQ、PIQA、ARC等）中全面超越Chinchilla-70B，并在多数任务上击败PaLM-540B（除BoolQ和WinoGrande）。例如：</p><ul><li><p><strong>ARC挑战集</strong>：LLaMA-65B得分57.8，显著高于PaLM-540B的53.0。</p></li><li><p><strong>OpenBookQA</strong>：65B模型以60.2%准确率刷新SOTA。</p></li><li><p><strong>关键发现</strong>：LLaMA-13B性能优于GPT-3（175B），验证小模型+长训练的有效性。</p></li></ul></li></ul><h2 id="" tabindex="-1"><a class="header-anchor" href="#"><span><img src="/assets/4-BdV2nLkz.png" alt="" loading="lazy"></span></a></h2><p><strong>2. 闭卷问答（Closed-Book QA）</strong></p><ul><li><p><strong>NaturalQuestions</strong>（表4）与<strong>TriviaQA</strong>（表5）：</p><ul><li><p>65B模型在零样本和少样本（64-shot）设置下均达到SOTA（TriviaQA零样本68.2%，超越Chinchilla-70B的55.4%）。</p></li><li><p><strong>13B模型</strong>在单V100 GPU上推理时，性能仍优于GPT-3（如TriviaQA 64-shot 64.0% vs. GPT-3 57.2%）。</p></li><li><p><strong>训练动态</strong>：图2显示模型性能与训练token量强相关（如33B模型在1.4T tokens后HellaSwag分数提升至82.8）。</p></li></ul></li></ul><figure><img src="/assets/6-BtD6SdSY.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/assets/7-hqLPPuQj.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/assets/5-XSGaJYZP.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><hr><p><strong>3. 代码生成与数学推理</strong></p><ul><li><strong>代码生成</strong>（表8）: LLaMA-65B在HumanEval（pass@1 23.7%）和MBPP（37.7%）上超越未微调的PaLM-62B（15.9%/21.4%），接近PaLM-540B（26.2%/36.8%）。</li></ul><figure><img src="/assets/8-DtmO_6ZX.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li><p><strong>数学能力</strong>（表7）：</p><ul><li><p><strong>GSM8k</strong>：65B模型未经数学微调即达50.9%（多数投票69.7%），优于Minerva-62B（52.4%）。</p></li><li><p><strong>MATH</strong>：65B模型（10.6%）表现接近PaLM-62B（8.8%），但远低于Minerva-540B（33.6%），凸显领域微调的重要性。</p></li></ul></li></ul><figure><img src="/assets/9-BdT6Bxp3.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><hr><p><strong>4. 多任务理解（MMLU）与指令微调</strong></p><ul><li><strong>MMLU 5-shot</strong>（表9/16）: LLaMA-65B平均得分63.4%，落后于Chinchilla-70B（67.5%）和PaLM-540B（69.3%），主因是书籍数据量不足（仅177GB vs. 其他模型2TB）。</li></ul><figure><img src="/assets/10-BSEzXLyW.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/assets/13-5rriMbaF.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li><strong>指令微调（LLaMA-I）</strong>（表10）: 简单微调后，65B模型在MMLU上提升至68.9%，超越Flan-PaLM-62B（66.1%），证明指令适应的高效性。</li></ul><figure><img src="/assets/12-QCQiuJhd.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><hr><p><strong>5. 偏见与毒性分析</strong></p><ul><li><strong>RealToxicityPrompts</strong>（表11）: 模型越大毒性倾向越高（65B Respectful类毒性分0.141 vs. 7B的0.081），与OPT等模型趋势一致。</li></ul><figure><img src="/assets/14-BxGp8qYG.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li><strong>CrowS-Pairs</strong>（表12）: LLaMA-65B平均偏见得分66.6，优于OPT-175B（69.5），但宗教类别偏差显著（79.0）。</li></ul><figure><img src="/assets/15-Dvh6ny2F.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li><strong>WinoGender</strong>（表13）: 模型对非二元代词（their/them）的指代准确率（81.7%）高于性别化代词（his/him 72.1%），反映社会偏见。</li></ul><figure><img src="/assets/16-C_-bm_kM.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><hr><p><strong>LLaMA的核心成果</strong>：</p><ol><li><p><strong>效率突破</strong>：小模型（如13B）通过数据与训练优化达到大模型（GPT-3/Chinchilla）性能。</p></li><li><p><strong>多领域竞争力</strong>：在代码、数学等专业任务中，未微调模型即接近SOTA。</p></li><li><p><strong>可复现性</strong>：纯公开数据训练结果挑战了专有数据的必要性，但书籍/学术数据不足限制MMLU表现。</p></li><li><p><strong>责任缺陷</strong>：模型规模与毒性/偏见正相关，需后续治理（论文第5章重点讨论）。</p></li></ol><h2 id="指令微调" tabindex="-1"><a class="header-anchor" href="#指令微调"><span>指令微调</span></a></h2><ol><li><p><strong>方法与目标</strong>: LLaMA通过<strong>轻量级指令微调</strong>（遵循<a href="https://arxiv.org/abs/2210.11416" target="_blank" rel="noopener noreferrer">Chung et al., 2022的协议</a>）优化LLaMA-65B，得到<strong>LLaMA-I</strong>，旨在提升任务泛化能力，无需复杂架构调整。</p></li><li><p><strong>关键性能提升（表10）</strong></p><ul><li><p><strong>MMLU 5-shot</strong>：微调后准确率从63.4%→68.9%，超越Flan-PaLM-62B（66.1%），但低于GPT-3.5（77.4%）。</p></li><li><p><strong>领域差异（表16 - 参考上文）</strong>：STEM（如Astronomy +9.2%）和人文任务（Philosophy +5.1%）提升显著。</p></li></ul></li></ol><figure><img src="/assets/17-x5_14Nof.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol start="3"><li><p><strong>生成能力（附录D）</strong></p><ul><li><p><strong>代码生成</strong>：可输出规范代码（如HTML标签清理的正则表达式）。</p></li><li><p><strong>多轮交互</strong>：支持复杂对话（如象棋开局策略分析）。</p></li><li><p><strong>伦理响应</strong>：自动生成AI使用指南，强调责任约束。</p></li></ul></li><li><p><strong>局限性与挑战</strong></p><ul><li><p><strong>数据不透明</strong>：微调数据规模/多样性未公开，可能限制泛化。</p></li><li><p><strong>逻辑缺陷</strong>：数学/推理任务仍存在幻觉（需后处理）。</p></li></ul></li></ol><p><strong>总结</strong></p><p>LLaMA-I证明<strong>小规模微调即可显著提升任务适应性</strong>，但透明性与可靠性仍需优化，为开源社区提供了可复现的基线（如后续Alpaca/Vicuna工作）。</p><h2 id="bias-toxicity-and-misinformation" tabindex="-1"><a class="header-anchor" href="#bias-toxicity-and-misinformation"><span>Bias, Toxicity and Misinformation</span></a></h2><ol><li><p><strong>毒性生成评估（RealToxicityPrompts）</strong></p><ul><li><p>使用PerspectiveAPI对100k提示生成内容进行毒性评分（0-1分）</p></li><li><p><strong>关键发现（表11）</strong>：</p><ul><li><p>模型规模与毒性正相关（65B毒性分0.141 vs 7B的0.081）</p></li><li><p>&quot;Respectful&quot;提示仍可能触发毒性响应</p></li></ul></li><li><p>与Chinchiila（0.087）等模型趋势一致</p></li></ul></li></ol><figure><img src="/assets/18-iSbBD6qP.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol start="2"><li><p><strong>社会偏见分析</strong></p><ul><li><p><strong>CrowS-Pairs（表12）</strong>：</p><ul><li><p>平均偏见得分66.6（优于OPT-175B的69.5）</p></li><li><p>宗教类别偏见最显著（79.0分）</p></li></ul></li></ul></li></ol><figure><img src="/assets/19-DsLzHQQE.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li><p><strong>WinoGender（表13）</strong>：</p><ul><li><p>对非二元代词（their/them）指代准确率81.7%</p></li><li><p>性别化代词（his/him）准确率低至72.1%</p></li><li><p>&quot;gotcha&quot;测试显示职业性别刻板印象明显</p></li></ul></li></ul><figure><img src="/assets/20-TPqYJpCy.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol start="3"><li><p><strong>真实性缺陷（TruthfulQA）</strong></p><ul><li><p>65B模型真实答案率仅57%（表14）</p></li><li><p>在对抗性问题上易产生幻觉</p></li><li><p>表现优于GPT-3但可靠性仍不足</p></li></ul></li></ol><figure><img src="/assets/21-CLLMjUIn.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>关键问题</strong></p><ul><li><p>数据根源：CommonCrawl等网络数据隐含的社会偏见难以完全过滤</p></li><li><p>规模悖论：能力提升伴随风险增加（如65B毒性最高）</p></li></ul><p><strong>总结</strong></p><p>LLaMA呈现出与同类模型相似的偏见/毒性模式，凸显公开数据训练的固有挑战。需结合：</p><p>1）更严格的数据清洗（如Wikipedia引用过滤）</p><p>2）后处理技术（如perspectiveAPI过滤）</p><p>3）社区治理框架</p><h2 id="相关工作" tabindex="-1"><a class="header-anchor" href="#相关工作"><span>相关工作</span></a></h2><ol><li><strong>语言模型发展脉络</strong></li></ol><ul><li><p>从统计语言模型（n-gram）到神经网络（RNN/LSTM），最终演进至Transformer架构（<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">Vaswani et al., 2017</a>）</p></li><li><p>关键里程碑：</p><ul><li><p>GPT系列（<a href="https://arxiv.org/abs/1801.06146" target="_blank" rel="noopener noreferrer">Radford et al., 2018</a>, <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener noreferrer">2019</a>, <a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener noreferrer">2020</a>）确立自回归范式</p></li><li><p>BERT（<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener noreferrer">Devlin et al., 2018</a>）推动双向预训练</p></li><li><p>T5（<a href="https://arxiv.org/abs/1910.10683" target="_blank" rel="noopener noreferrer">Raffel et al., 2020</a>）统一文本到文本框架</p></li></ul></li></ul><ol start="2"><li><strong>规模化研究</strong></li></ol><ul><li><p>计算律发现（<a href="https://arxiv.org/abs/2001.08361" target="_blank" rel="noopener noreferrer">Kaplan et al., 2020</a>）揭示模型性能与规模的关系</p></li><li><p>Chinchilla（<a href="https://arxiv.org/abs/2203.15556" target="_blank" rel="noopener noreferrer">Hoffmann et al., 2022</a>）提出数据-计算最优平衡理论</p></li><li><p>涌现能力研究（<a href="https://arxiv.org/abs/2206.07682" target="_blank" rel="noopener noreferrer">Wei et al., 2022</a>）分析规模带来的质变</p></li></ul><ol start="3"><li><strong>开源模型进展</strong></li></ol><ul><li><p>OPT（<a href="https://arxiv.org/abs/2205.01068" target="_blank" rel="noopener noreferrer">Zhang et al., 2022</a>）和BLOOM（<a href="https://arxiv.org/abs/2211.05100" target="_blank" rel="noopener noreferrer">Scao et al., 2022</a>）推动开源大模型发展</p></li><li><p>GPT-NeoX（<a href="https://arxiv.org/abs/2204.06745" target="_blank" rel="noopener noreferrer">Black et al., 2022</a>）提供20B参数开源基线</p></li></ul><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><p>LLaMA系列模型通过高效架构设计和纯公开数据训练，在多个基准测试中达到与更大规模专有模型相当的性能，同时保持开源可复现性，为AI研究的民主化提供了重要范例。</p></div><!----><!----><!----></div><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">最近更新</span><time class="vp-meta-info" datetime="2025-06-27T06:41:50.000Z" data-allow-mismatch>2025/6/27 06:41</time></div><div class="contributors"><span class="vp-meta-label">贡献者: </span><!--[--><!--[--><span class="vp-meta-info" title="email: 3076679680@qq.com">BinaryOracle</span><!--]--><!--]--></div></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/KV-Cache.html" aria-label="KV-Cache 详解"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><iconify-icon class="vp-icon" icon="fa6-solid:file" sizing="height" height="1em"></iconify-icon>KV-Cache 详解</div></a><a class="route-link auto-link next" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/LLaMA-2%E8%AE%BA%E6%96%87.html" aria-label="LLaMA-2论文"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">LLaMA-2论文<iconify-icon class="vp-icon" icon="fa6-solid:file" sizing="height" height="1em"></iconify-icon></div></a></nav><div id="comment" class="waline-wrapper vp-comment" vp-comment darkmode="false" style="display:block;"><!----></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><div class="vp-footer">技术共建，知识共享</div><div class="vp-copyright">Copyright © 2025 BinaryOracle </div></footer></div><!--]--><!--[--><!----><!--[--><!--]--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-B1cJYw99.js" defer></script>
  </body>
</html>
