<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.23" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.88" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"InstructGPT 论文","image":[""],"datePublished":"2025-06-26T00:00:00.000Z","dateModified":"2025-06-27T02:32:54.000Z","author":[{"@type":"Person","name":"BinaryOracle"}]}</script><meta property="og:url" content="https://mister-hope.github.io/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/InstructGPT%E8%AE%BA%E6%96%87.html"><meta property="og:site_name" content="MetaMind"><meta property="og:title" content="InstructGPT 论文"><meta property="og:description" content="InstructGPT 论文"><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-06-27T02:32:54.000Z"><meta property="article:author" content="BinaryOracle"><meta property="article:tag" content="已发布"><meta property="article:tag" content="预训练语言模型"><meta property="article:published_time" content="2025-06-26T00:00:00.000Z"><meta property="article:modified_time" content="2025-06-27T02:32:54.000Z"><title>InstructGPT 论文 | MetaMind</title><meta name="description" content="InstructGPT 论文">
    <link rel="preload" href="/assets/style-BM9BeOMB.css" as="style"><link rel="stylesheet" href="/assets/style-BM9BeOMB.css">
    <link rel="modulepreload" href="/assets/app-Zfpr4h-A.js"><link rel="modulepreload" href="/assets/InstructGPT论文.html-XVvVFLVL.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/assets/index.html-2E79kMqk.js" as="script"><link rel="prefetch" href="/assets/intro.html-Dc08EqCf.js" as="script"><link rel="prefetch" href="/assets/GREAT.html-DR4tAS6f.js" as="script"><link rel="prefetch" href="/assets/Grounding_3D_Object_Affordance.html-D1nkGX22.js" as="script"><link rel="prefetch" href="/assets/LASO.html-Zn3rOtNu.js" as="script"><link rel="prefetch" href="/assets/index.html-Dt5l90hY.js" as="script"><link rel="prefetch" href="/assets/简析PointNet__.html-Dc7H4o47.js" as="script"><link rel="prefetch" href="/assets/简析PointNet.html-CsXO2ZAk.js" as="script"><link rel="prefetch" href="/assets/index.html-DlsVSAfX.js" as="script"><link rel="prefetch" href="/assets/InternVL-1.0.html-DG5hxg6l.js" as="script"><link rel="prefetch" href="/assets/LLaVA_1.0.html-DsZrX9hf.js" as="script"><link rel="prefetch" href="/assets/index.html-DQ0dmDin.js" as="script"><link rel="prefetch" href="/assets/多模态模型CLIP原理与图片分类，文字搜索图像实战演练.html-BNWMJ-9L.js" as="script"><link rel="prefetch" href="/assets/庖丁解牛BLIP2.html-Ndj_DW9l.js" as="script"><link rel="prefetch" href="/assets/庖丁解牛VIT.html-DwZqqRLY.js" as="script"><link rel="prefetch" href="/assets/index.html-Dx2ZMMZo.js" as="script"><link rel="prefetch" href="/assets/index.html-Dh1G8UAg.js" as="script"><link rel="prefetch" href="/assets/Attention运算过程中维度变换的理解.html-CmvESQCl.js" as="script"><link rel="prefetch" href="/assets/index.html-BciSbzY9.js" as="script"><link rel="prefetch" href="/assets/conda虚拟环境管理.html-BRqghqMl.js" as="script"><link rel="prefetch" href="/assets/常用评估指标.html-CI7N85DF.js" as="script"><link rel="prefetch" href="/assets/语义分割中常用的损失函数.html-BTIiPnFm.js" as="script"><link rel="prefetch" href="/assets/通俗易懂解读BPE分词算法实现.html-BcgpYUyi.js" as="script"><link rel="prefetch" href="/assets/Fine_Tuning知识扫盲.html-PW5G3qBb.js" as="script"><link rel="prefetch" href="/assets/LoRA微调系列.html-Cg-xJIm0.js" as="script"><link rel="prefetch" href="/assets/Prompt_Engineering知识扫盲.html-2Jj-RbyR.js" as="script"><link rel="prefetch" href="/assets/index.html-8Ckf8sfr.js" as="script"><link rel="prefetch" href="/assets/GPT-1论文.html-BsJmGEM8.js" as="script"><link rel="prefetch" href="/assets/GPT-2论文.html-BLwJNFS-.js" as="script"><link rel="prefetch" href="/assets/GPT-3论文.html-KAU4lnxH.js" as="script"><link rel="prefetch" href="/assets/LLaMA-1论文.html-BUmaiN9Y.js" as="script"><link rel="prefetch" href="/assets/LLaMA-2论文.html-DUsd58lQ.js" as="script"><link rel="prefetch" href="/assets/index.html-_azukgIY.js" as="script"><link rel="prefetch" href="/assets/RoBERTa论文.html-NE_OR4nc.js" as="script"><link rel="prefetch" href="/assets/从零实现Bert.html-DCvV60Ba.js" as="script"><link rel="prefetch" href="/assets/图解BERT.html-D6tQndU7.js" as="script"><link rel="prefetch" href="/assets/图解Transformer.html-C-HvGRIR.js" as="script"><link rel="prefetch" href="/assets/index.html-CYytADWS.js" as="script"><link rel="prefetch" href="/assets/index.html-Cdj66RUx.js" as="script"><link rel="prefetch" href="/assets/index.html-sqz9AbaH.js" as="script"><link rel="prefetch" href="/assets/1.自动微分.html-DuXThBGt.js" as="script"><link rel="prefetch" href="/assets/index.html-CThijV6G.js" as="script"><link rel="prefetch" href="/assets/1.前置知识.html-CJSDYI8_.js" as="script"><link rel="prefetch" href="/assets/2.大模型API.html-BNWJ6dNh.js" as="script"><link rel="prefetch" href="/assets/index.html-BdRmdovz.js" as="script"><link rel="prefetch" href="/assets/404.html-DykHuJp6.js" as="script"><link rel="prefetch" href="/assets/index.html-Cc5r6XDo.js" as="script"><link rel="prefetch" href="/assets/index.html-BwIvH0lC.js" as="script"><link rel="prefetch" href="/assets/index.html-leAB2SBy.js" as="script"><link rel="prefetch" href="/assets/index.html-BD0X7taH.js" as="script"><link rel="prefetch" href="/assets/index.html-SdY37Daq.js" as="script"><link rel="prefetch" href="/assets/index.html-kJLRbhwC.js" as="script"><link rel="prefetch" href="/assets/index.html-BXwv5JKv.js" as="script"><link rel="prefetch" href="/assets/index.html-BQ1G_q_s.js" as="script"><link rel="prefetch" href="/assets/index.html-BnhaWwIg.js" as="script"><link rel="prefetch" href="/assets/index.html-DiYMRoWd.js" as="script"><link rel="prefetch" href="/assets/index.html-E9ufiSWE.js" as="script"><link rel="prefetch" href="/assets/index.html-BijSxAZ7.js" as="script"><link rel="prefetch" href="/assets/index.html-CO8vmxNE.js" as="script"><link rel="prefetch" href="/assets/index.html-qyriFSC-.js" as="script"><link rel="prefetch" href="/assets/index.html-D3fZHIbX.js" as="script"><link rel="prefetch" href="/assets/index.html-AOCo6wwW.js" as="script"><link rel="prefetch" href="/assets/index.html-BiTWSRsj.js" as="script"><link rel="prefetch" href="/assets/index.html-BVaxwrh6.js" as="script"><link rel="prefetch" href="/assets/index.html-B3qcCBMd.js" as="script"><link rel="prefetch" href="/assets/index.html-Dj49twZL.js" as="script"><link rel="prefetch" href="/assets/index.html-Di9rE6pl.js" as="script"><link rel="prefetch" href="/assets/index.html-Bi24WjpD.js" as="script"><link rel="prefetch" href="/assets/index.html-SvlZlkBp.js" as="script"><link rel="prefetch" href="/assets/index.html-CxQ36Ds-.js" as="script"><link rel="prefetch" href="/assets/index.html-DfW4E_nh.js" as="script"><link rel="prefetch" href="/assets/index.html-BET3ekgH.js" as="script"><link rel="prefetch" href="/assets/index.html-D9gy8DqU.js" as="script"><link rel="prefetch" href="/assets/index.html-f5_dRA6x.js" as="script"><link rel="prefetch" href="/assets/index.html-CLRoQS5p.js" as="script"><link rel="prefetch" href="/assets/index.html-CUcn_3A_.js" as="script"><link rel="prefetch" href="/assets/index.html-B4CYZwsr.js" as="script"><link rel="prefetch" href="/assets/index-YsyrxmwF.js" as="script"><link rel="prefetch" href="/assets/flowchart-CTwbLKUk.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-DXWKOczD.js" as="script"><link rel="prefetch" href="/assets/SearchResult-DFMIq6L1.js" as="script"><link rel="prefetch" href="/assets/waline-meta-l0sNRNKZ.js" as="script"><link rel="prefetch" href="/assets/component-CFh-3Rs9.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><a class="route-link vp-brand" href="/" aria-label="带我回家"><img class="vp-nav-logo" src="/assets/images/head.png" alt><!----><span class="vp-site-name hide-in-pad">MetaMind</span></a><!--]--></div><div class="vp-navbar-center"><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/" aria-label="主页"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:house" height="1em" sizing="height"></iconify-icon><!--]-->主页<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link route-link-active auto-link" href="/LLM/" aria-label="大语言模型"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:book" height="1em" sizing="height"></iconify-icon><!--]-->大语言模型<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/MMLLM/" aria-label="多模态"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:laptop-code" height="1em" sizing="height"></iconify-icon><!--]-->多模态<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/3DVL/" aria-label="3D-Vision Language"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:eye" height="1em" sizing="height"></iconify-icon><!--]-->3D-Vision Language<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/open_course/" aria-label="开源课程笔记"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:computer" height="1em" sizing="height"></iconify-icon><!--]-->开源课程笔记<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/open_projects/" aria-label="开源项目"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:star" height="1em" sizing="height"></iconify-icon><!--]-->开源项目<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/other/" aria-label="杂谈"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:sun" height="1em" sizing="height"></iconify-icon><!--]-->杂谈<!----></a></div></nav><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!----><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!--[--><button type="button" class="slimsearch-button" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="slimsearch-placeholder">搜索</div><div class="slimsearch-key-hints"><kbd class="slimsearch-key">Ctrl</kbd><kbd class="slimsearch-key">K</kbd></div></button><!--]--><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/" aria-label="主页"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:house" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->主页<!----></a></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:book" width="1em" height="1em" sizing="both"></iconify-icon><span class="vp-sidebar-title">大语言模型</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:robot" width="1em" height="1em" sizing="both"></iconify-icon><span class="vp-sidebar-title">应用层</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:book" width="1em" height="1em" sizing="both"></iconify-icon><span class="vp-sidebar-title">模型层</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/%E5%9B%BE%E8%A7%A3Transformer.html" aria-label="图解Transformer"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->图解Transformer<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/GPT-1%E8%AE%BA%E6%96%87.html" aria-label="GPT-1 论文"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->GPT-1 论文<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/GPT-2%E8%AE%BA%E6%96%87.html" aria-label="GPT-2 论文"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->GPT-2 论文<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/GPT-3%E8%AE%BA%E6%96%87.html" aria-label="GPT-3 论文"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->GPT-3 论文<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/InstructGPT%E8%AE%BA%E6%96%87.html" aria-label="InstructGPT 论文"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->InstructGPT 论文<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/LLaMA-1%E8%AE%BA%E6%96%87.html" aria-label="LLaMA-1论文"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->LLaMA-1论文<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/LLaMA-2%E8%AE%BA%E6%96%87.html" aria-label="LLaMA-2论文"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->LLaMA-2论文<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/RoBERTa%E8%AE%BA%E6%96%87.html" aria-label="RoBERTa 论文"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->RoBERTa 论文<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0Bert.html" aria-label="从&quot;零&quot;实现 Bert"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->从&quot;零&quot;实现 Bert<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/%E5%9B%BE%E8%A7%A3BERT.html" aria-label="图解 Bert"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:file" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->图解 Bert<!----></a></li></ul></section></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:laptop-code" width="1em" height="1em" sizing="both"></iconify-icon><span class="vp-sidebar-title">多模态</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:eye" width="1em" height="1em" sizing="both"></iconify-icon><span class="vp-sidebar-title">3D-Vision Language</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:computer" width="1em" height="1em" sizing="both"></iconify-icon><span class="vp-sidebar-title">开源课程笔记</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:star" width="1em" height="1em" sizing="both"></iconify-icon><span class="vp-sidebar-title">开源项目</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><iconify-icon class="vp-icon" icon="fa6-solid:sun" width="1em" height="1em" sizing="both"></iconify-icon><span class="vp-sidebar-title">杂谈</span><span class="vp-arrow end"></span></button><!----></section></li><li><a class="route-link auto-link vp-sidebar-link" href="/intro.html" aria-label="关于我们"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:circle-info" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->关于我们<!----></a></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><iconify-icon class="vp-icon" icon="fa6-solid:file" height="1em" sizing="height"></iconify-icon>InstructGPT 论文</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">BinaryOracle</span></span><span property="author" content="BinaryOracle"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">2025/6/26</span><meta property="datePublished" content="2025-06-26T00:00:00.000Z"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color2 clickable" role="navigation">NLP</span><!--]--><meta property="articleSection" content="NLP"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color6 clickable" role="navigation">预训练语言模型</span><span class="page-tag-item color4 clickable" role="navigation">已发布</span><!--]--><meta property="keywords" content="预训练语言模型,已发布"></span><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 18 分钟</span><meta property="timeRequired" content="PT18M"></span><span class="page-word-info" aria-label="字数🔠" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon word-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="word icon" name="word"><path d="M518.217 432.64V73.143A73.143 73.143 0 01603.43 1.097a512 512 0 01419.474 419.474 73.143 73.143 0 01-72.046 85.212H591.36a73.143 73.143 0 01-73.143-73.143z"></path><path d="M493.714 566.857h340.297a73.143 73.143 0 0173.143 85.577A457.143 457.143 0 11371.566 117.76a73.143 73.143 0 0185.577 73.143v339.383a36.571 36.571 0 0036.571 36.571z"></path></svg><span>约 5508 字</span><meta property="wordCount" content="5508"></span></div><hr></div><!----><div class="" vp-content><!----><div id="markdown-content"><p><code>InstructGPT 论文</code></p><!-- more --><blockquote><p>论文链接: <a href="https://arxiv.org/abs/2203.02155" target="_blank" rel="noopener noreferrer">Training language models to follow instructions with human feedback</a></p></blockquote><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本研究指出，<strong>仅通过增加语言模型的规模，并不能显著提升其对用户意图的理解与遵循能力</strong>。为了解决这一问题，作者提出一种<strong>通过人类反馈对模型进行微调的方法，用以更好地对齐模型行为与用户意图</strong>。</p><p>具体方法包括：<strong>首先利用人工演示数据对GPT-3进行监督学习微调；然后通过人类对多个模型输出的偏好进行排序，训练奖励模型，并结合强化学习进一步优化模型。最终所得的InstructGPT模型，即使参数量远小于原始GPT-3（例如1.3B对比175B），在用户偏好评估中仍表现更优</strong>。此外，InstructGPT在输出真实性、减少有害内容生成等方面也有所改进，且在公开NLP任务上的性能损失极小。研究表明，人类反馈微调是一种有效的模型对齐手段，尽管仍有提升空间。</p><h2 id="简介" tabindex="-1"><a class="header-anchor" href="#简介"><span>简介</span></a></h2><p>作者指出，大型语言模型（如GPT-3）虽然具备强大的自然语言处理能力，但它们常常偏离用户意图，表现出诸如捏造事实、生成有害或无关文本、不遵循指令等问题。这是因为<strong>它们的训练目标是最大化互联网文本的下一个词预测概率，而非“安全且有用地遵循用户指令”，这造成了目标的不一致，即“对齐问题”（alignment problem）</strong>。</p><p>为解决这一问题，本文提出了一种对齐语言模型与用户意图的策略：<strong>通过人类反馈进行强化学习（Reinforcement Learning from Human Feedback, RLHF）</strong>。该方法包括三个关键步骤：</p><ol><li><p><strong>监督学习微调（SFT）</strong>：收集人类示范数据，微调预训练的GPT-3模型；</p></li><li><p><strong>奖励模型训练（RM）</strong>：收集人类对模型多个输出的排序偏好，训练出一个能预测人类偏好的奖励模型；</p></li><li><p><strong>强化学习微调（PPO）</strong>：使用奖励模型的反馈，采用Proximal Policy Optimization算法进一步优化模型行为。</p></li></ol><figure><img src="/assets/2-BBz8zigK.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>作者称这些过程使得模型输出更符合人类偏好，但强调这种对齐是<strong>相对于特定人群（即标注者和研究者）的偏好</strong>，并非广义上的“人类价值”。</p><p>通过图1的结果可见，<strong>即便是只有1.3B参数的InstructGPT模型，其输出也比175B的原始GPT-3更受人类偏好</strong>，显示出这种人类反馈驱动的微调策略极具潜力。图1中显示的不同模型在人类偏好评估中的胜率清晰反映了该方法的有效性，表明训练目标的改变（从“预测下一个词”转向“优化人类偏好”）能带来质的改善。</p><figure><img src="/assets/1-DsopUWcg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>此外，作者采用了“有帮助（helpful）、诚实（honest）、无害（harmless）”三大原则来评估模型对齐效果，强调未来开发和部署语言模型时需格外关注其社会影响及安全性。</p><p>总之，本文引入了一种有效的对齐方法，为语言模型行为与用户意图之间架起了桥梁，为AI安全和实用性的发展提供了关键路径。</p><h2 id="相关工作" tabindex="-1"><a class="header-anchor" href="#相关工作"><span>相关工作</span></a></h2><p><strong>一、基于人类反馈的模型对齐（Alignment via Human Feedback）</strong></p><p>InstructGPT 的核心技术基础是 “<strong>强化学习来自人类反馈（RLHF）</strong>”，旨在将模型输出行为与人类意图对齐。这一方法起初应用于强化学习场景：</p><ul><li><p><strong>用人类偏好训练强化学习代理</strong> : <a href="https://arxiv.org/abs/1706.03741" target="_blank" rel="noopener noreferrer">Christiano et al., 2017</a> 提出了一种通过人类偏好比较训练代理的强化学习方法。</p></li><li><p><strong>在模拟环境中用人类反馈改进行为策略</strong> : <a href="https://arxiv.org/abs/1811.06521" target="_blank" rel="noopener noreferrer">Ibarz et al., 2018</a>将人类偏好学习应用于模仿学习。</p></li></ul><p>RLHF 后来被应用于语言任务，如摘要：</p><ul><li><p><strong>风格延续任务中的偏好学习</strong> : <a href="https://arxiv.org/abs/1909.08593" target="_blank" rel="noopener noreferrer">Ziegler et al., 2019</a></p></li><li><p><strong>文本摘要中的奖励建模与 PPO 微调</strong> : <a href="https://arxiv.org/abs/2009.01325" target="_blank" rel="noopener noreferrer">Stiennon et al., 2020</a></p></li></ul><p>此外，该方向在对话系统（<a href="https://arxiv.org/abs/1907.00456" target="_blank" rel="noopener noreferrer">Jaques et al., 2019</a>）、机器翻译（<a href="https://arxiv.org/abs/1607.07086" target="_blank" rel="noopener noreferrer">Bahdanau et al., 2016</a>）、语义解析（<a href="https://aclanthology.org/P18-1071.pdf" target="_blank" rel="noopener noreferrer">Lawrence and Riezler, 2018</a>）、故事生成（<a href="https://arxiv.org/abs/2009.04887" target="_blank" rel="noopener noreferrer">Zhou and Xu, 2020</a>）等任务中也得到了广泛实践。</p><p>InstructGPT 的工作属于对上述方法的泛化：将 RLHF 用于对齐语言模型在<strong>广泛任务分布</strong>下的行为。</p><hr><p><strong>二、训练语言模型以遵循自然语言指令（Instruction Following）</strong></p><p>另一相关研究方向是使用自然语言指令训练模型以实现跨任务泛化：</p><ul><li><p><strong>FLAN</strong>：使用数十个 NLP 数据集、配以自然语言任务说明进行微调。 （<a href="https://arxiv.org/abs/2109.01652" target="_blank" rel="noopener noreferrer">Wei et al., 2021</a>）</p></li><li><p><strong>T0 / T0++</strong>：将 NLP 基准任务转换为指令格式，通过多任务微调训练语言模型。 （<a href="https://arxiv.org/abs/2110.08207" target="_blank" rel="noopener noreferrer">Sanh et al., 2021</a>）</p></li><li><p><strong>Natural Instructions</strong>：探索指令格式变化对模型泛化能力的影响。 （<a href="https://arxiv.org/abs/2112.04139" target="_blank" rel="noopener noreferrer">Mishra et al., 2021</a>）</p></li></ul><p>InstructGPT 与上述方法的不同之处在于其训练数据源真实 API 用户提交的指令，更具 <strong>任务多样性与实用性</strong>。</p><p>图 1 支持这一点：即使参数量远小于 GPT-3（1.3B vs. 175B），InstructGPT 模型依然在用户指令任务中获得更高的偏好评分。</p><hr><p><strong>三、评估语言模型的风险与危害</strong></p><p>InstructGPT 还借鉴了对语言模型潜在风险的研究，这些研究强调：</p><ul><li><p><strong>语言模型会生成有害或偏见内容</strong>（<a href="https://dl.acm.org/doi/10.1145/3442188.3445922" target="_blank" rel="noopener noreferrer">Bender et al., 2021</a>；<a href="https://arxiv.org/abs/2009.11462" target="_blank" rel="noopener noreferrer">Gehman et al., 2020</a>）</p></li><li><p><strong>TruthfulQA</strong> 提供了一个用于测试模型生成信息真实性的基准数据集 （<a href="https://arxiv.org/abs/2109.07958" target="_blank" rel="noopener noreferrer">Lin et al., 2021</a>）</p></li><li><p><strong>偏见评估数据集</strong>：包括 Winogender（性别偏见）和 CrowS-Pairs（社会偏见）（<a href="https://arxiv.org/abs/1804.09301" target="_blank" rel="noopener noreferrer">Rudinger et al., 2018</a>,<a href="https://arxiv.org/abs/2010.00133" target="_blank" rel="noopener noreferrer">Nangia et al., 2020</a>）</p></li></ul><p>InstructGPT 在实验部分也采用了这些基准（见论文第 4 节），并指出：在对毒性任务加入“请尊重”提示时，InstructGPT 比 GPT-3 更少生成有害内容（见 Figure 7）。</p><figure><img src="/assets/4-D44bmiut.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><hr><p><strong>四、模型行为干预与有害输出缓解策略</strong></p><p>文献中也探索了多种控制模型输出的策略，这些方法为 InstructGPT 所采用的 RLHF 方式提供了对照方案：</p><ul><li><p><strong>微调小型数据集以嵌入价值观</strong> （<a href="https://arxiv.org/abs/2106.10328" target="_blank" rel="noopener noreferrer">Solaiman and Dennison, 2021</a>）</p></li><li><p><strong>通过触发短语过滤预训练语料</strong>，以降低毒性输出倾向 （<a href="https://arxiv.org/abs/2108.07258" target="_blank" rel="noopener noreferrer">Ngo et al., 2021</a>）</p></li><li><p><strong>使用外部语言模型引导生成方向</strong>（如 Plug-and-Play Language Models） （<a href="https://arxiv.org/abs/1912.02164" target="_blank" rel="noopener noreferrer">Dathathri et al., 2019</a>）</p></li><li><p><strong>用正则化或投影技术缓解嵌入空间中的偏见</strong> （<a href="https://arxiv.org/abs/2010.07096" target="_blank" rel="noopener noreferrer">Liang et al., 2021</a>）</p></li></ul><p>尽管 InstructGPT 并未直接采用这些方法，但在强化学习微调中加入 KL 约束、预训练梯度（PPO-ptx）等机制，实际上也体现了对 <strong>对齐损失(alignment tax)</strong> 的控制。</p><p>以下是对 InstructGPT 论文中第 3 节 <strong>“Methods and Experimental Details”</strong> 的详细总结，内容结构遵循原文小节安排（3.1–3.6），并结合论文图表（如图 2）以增强理解。部分内容需分多段呈现以保留关键信息。</p><h2 id="方法" tabindex="-1"><a class="header-anchor" href="#方法"><span>方法</span></a></h2><p>InstructGPT 的方法主要基于 <a href="https://arxiv.org/abs/2009.01325" target="_blank" rel="noopener noreferrer">Stiennon et al. (2020)</a> 和 <a href="https://arxiv.org/abs/1909.08593" target="_blank" rel="noopener noreferrer">Ziegler et al. (2019)</a> 提出的 <strong>三步训练框架</strong>，用以实现语言模型对人类意图的对齐。该流程可参见论文图 2 的三步训练流程：</p><figure><img src="/assets/2-BBz8zigK.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol><li><p><strong>监督微调（Supervised Fine-Tuning, SFT）</strong>: 使用人类标注者示范的优质输出，微调预训练 GPT-3 模型，得到初始策略模型。</p></li><li><p><strong>奖励模型训练（Reward Model, RM）</strong>: 收集一组模型输出对（针对同一输入），由人类标注者根据偏好进行排序。将这些排序用作训练奖励模型（RM）的监督信号，使其学会预测哪一输出更受偏好。</p></li><li><p><strong>使用 PPO 强化学习（Proximal Policy Optimization）微调模型</strong>: 以奖励模型为环境反馈信号，对 SFT 模型进一步使用 PPO 算法进行强化学习优化，从而得到最终的 InstructGPT 模型。</p></li></ol><blockquote><p><strong>图 2（Figure 2）</strong> 明确展示了这三步流程之间的数据流和优化路径，是 InstructGPT 方法的核心概括图。</p></blockquote><hr><p><strong>数据集构建（Dataset）</strong></p><p>InstructGPT 的训练数据主要来自以下两个来源：</p><ol><li><p><strong>真实用户在 OpenAI API Playground 提交的 prompt</strong></p><ul><li><p>提取并去重后用于训练 SFT、RM 和 PPO 模型。为确保训练集与评估集分离，按用户 ID 进行划分。</p></li><li><p>为防止泄露隐私，对训练数据进行了 PII 过滤。</p></li></ul></li><li><p><strong>标注者创作的 prompt</strong>（主要用于冷启动训练）</p><p>分为三类：</p><ul><li><p>Plain（开放任务）</p></li><li><p>Few-shot（带示例的任务）</p></li><li><p>User-based（模拟用户需求的任务）</p></li></ul></li></ol><p>三类子数据集：</p><ul><li><p><strong>SFT 数据集</strong>（~13k prompts）用于监督微调</p></li><li><p><strong>RM 数据集</strong>（~33k prompts）用于训练奖励模型</p></li><li><p><strong>PPO 数据集</strong>（~31k prompts）为 PPO 模型提供输入</p></li></ul><blockquote><p><strong>表 1</strong> 显示了 API prompt 的任务分布：约 46% 为生成类任务，QA 和聊天合计约 23%，突出了真实用户需求的多样性。</p></blockquote><figure><img src="/assets/5-Wy7GpdFK.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><hr><p><strong>任务类型（Tasks）</strong></p><p>训练任务覆盖广泛，包括但不限于：</p><ul><li><p>文本生成（如创作、补全）</p></li><li><p>问答（开放型和封闭型）</p></li><li><p>对话、重写、摘要、分类、抽取等</p></li></ul><p>大部分任务通过自然语言指令表达意图。少量则通过 few-shot 示例或文本上下文隐式表达。标注者在判断指令时需考虑信息准确性、避免偏见与毒性，这为 InstructGPT 模型“helpful, honest, harmless”标准提供训练信号。</p><hr><p><strong>人类数据采集（Human Data Collection）</strong></p><p>OpenAI 雇佣了约 40 名标注者（通过 Upwork 与 ScaleAI）参与数据标注，执行以下任务：</p><ul><li><p>提供高质量示范（用于 SFT）</p></li><li><p>对模型输出进行偏好排序（用于 RM）</p></li><li><p>对最终模型进行评估</p></li></ul><p>为了保证标注质量，OpenAI 设计了 <strong>筛选测试</strong> 来挑选具有敏感内容识别能力的标注者。训练过程中的一些 prompt 包含争议性内容，故特别强调标注者的社会敏感性。</p><p>在人类偏好标注中，<strong>inter-annotator agreement</strong> 达到 73±1.5%，说明标注者之间达成了较高的一致性。论文还进行了一组 held-out 标注者实验，显示 InstructGPT 模型能够泛化到新标注者的偏好。</p><hr><p><strong>模型结构与训练细节（Models）</strong></p><p>所有模型都基于 GPT-3 架构 ，在三个参数规模（1.3B、6B、175B）下进行训练，训练策略如下：</p><br><ol><li><strong>SFT 模型训练</strong></li></ol><ul><li><p>使用标注者示范数据，训练 16 个 epoch</p></li><li><p>使用余弦学习率衰减，0.2 的残差 dropout</p></li><li><p>用奖励模型得分选择最佳模型（而非验证 loss）</p></li></ul><br><ol start="2"><li><strong>奖励模型（RM）训练</strong></li></ol><ul><li><p>输入为 prompt 和 response，输出为标量奖励</p></li><li><p>在每个 prompt 上收集 K（4–9）个响应，由标注者排序，训练时将所有配对作为一个 batch，防止过拟合</p></li><li><p>使用如下 pairwise ranking loss：</p></li></ul><div class="language-math line-numbers-mode" data-highlighter="shiki" data-ext="math" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>loss(θ) = − (1 / C(K,2)) * E[log(σ(r_θ(x, y_w) − r_θ(x, y_l)))]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>其中 <mjx-container class="MathJax" jax="SVG" style="position:relative;"><svg style="vertical-align:-0.464ex;" xmlns="http://www.w3.org/2000/svg" width="2.442ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 1079.3 647" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>y</mi><mi>w</mi></msub></math></mjx-assistive-mml></mjx-container>、<mjx-container class="MathJax" jax="SVG" style="position:relative;"><svg style="vertical-align:-0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.773ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 783.7 647" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>y</mi><mi>l</mi></msub></math></mjx-assistive-mml></mjx-container> 分别为更受欢迎和较差的响应。</p><br><ol start="3"><li><strong>PPO 和 PPO-ptx 模型训练</strong></li></ol><ul><li><p>PPO 使用 RM 作为奖励函数</p></li><li><p>为缓解对奖励函数的过度优化，引入 KL 惩罚项</p></li><li><p>PPO-ptx 版本进一步加入 pretraining 任务的 log-likelihood 更新项，以防对齐过程中性能退化（alignment tax）</p></li></ul><blockquote><p>PPO-ptx 目标函数如下：</p></blockquote><div class="language-math line-numbers-mode" data-highlighter="shiki" data-ext="math" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>Objective = E[r − β * KL + γ * logP_pretrain]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><hr><p><strong>评估方式（Evaluation）</strong></p><p>为了衡量模型的“对齐程度”，InstructGPT 使用了综合性评估框架：</p><br><p>A. <strong>API prompt 分布评估</strong></p><ul><li><p>使用 held-out 用户的 prompt</p></li><li><p>人类评估输出的偏好、质量（Likert 1–7）、以及一系列元数据（如是否 hallucinate、是否尊重约束）</p></li><li><p><strong>图 4</strong> 展示了模型在是否遵循指令、幻觉率等多个维度的性能</p></li></ul><br><figure><img src="/assets/6-61v2KtcI.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><br><p>B. <strong>公共 NLP 数据集评估</strong></p><ul><li><p>涉及 TruthfulQA（真实性）、RealToxicityPrompts（毒性）、Winogender/CrowS-Pairs（偏见）</p></li><li><p>还评估模型在 SQuAD、DROP、HellaSwag、WMT 2015 等任务上的零样本表现</p></li><li><p>显示模型在强化学习过程中存在轻微性能损失，但 PPO-ptx 可有效缓解（见图 29–34，原文）</p></li></ul><figure><img src="/assets/7-Bh3fQ0g0.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/assets/8-DsqHY2rC.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/assets/9-CvroljWP.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/assets/10-Ba7YTC5R.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/assets/11-C4kzy9sM.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/assets/12-BQ1c7utG.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="结果" tabindex="-1"><a class="header-anchor" href="#结果"><span>结果</span></a></h2><p><strong>1. 在 API prompt 分布上的实验结果:</strong></p><p>InstructGPT 的核心实验基于真实用户提交的指令性 prompts，在这些任务中：</p><p><strong>人类评估者显著偏好 InstructGPT 输出</strong></p><ul><li><p><strong>Figure 1</strong> 显示：在用户任务分布中，1.3B 的 InstructGPT 模型比 175B GPT-3 更受偏好。</p></li><li><p>即使是少样本提示增强的 GPT-3（few-shot GPT-3），也不及 InstructGPT。</p></li><li><p>例如，175B InstructGPT 输出相较于标准 GPT-3 的偏好比为 <strong>85% ± 3%</strong>，相比 few-shot GPT-3 为 <strong>71% ± 4%</strong>。</p></li></ul><figure><img src="/assets/1-DsopUWcg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>PPO-ptx 与 PPO 模型均优于 SFT 和 GPT-3</strong></p><ul><li><p><strong>Figure 3</strong> 显示，在两类提示分布（GPT-3 与 InstructGPT 用户提交）上，InstructGPT 在所有规模下均优于 GPT-3。</p></li><li><p>并且该优势在训练标注者和 held-out 标注者之间都保持一致，说明偏好并非训练数据过拟合造成。</p></li></ul><figure><img src="/assets/13-Bv63AX-q.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>更好地遵循指令，减少幻觉，更适合作为用户助手</strong></p><ul><li><p><strong>Figure 4</strong> 展示了模型输出的多维质量元数据对比：</p><ul><li><p>InstructGPT 更少“幻觉”（hallucination）</p></li><li><p>更能遵守“指令中的显式约束”</p></li><li><p>更常“尝试正确完成任务”</p></li><li><p>更适合用于“客户助手场景”</p></li></ul></li></ul><figure><img src="/assets/14-BW9F2NAr.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><hr><p><strong>2. 在公开 NLP 数据集上的实验结果</strong></p><p><strong>在 TruthfulQA 上更真实、更少编造</strong></p><ul><li><p><strong>Figure 6</strong> 显示，在 TruthfulQA 基准上，PPO 和 PPO-ptx 模型显著提升回答的真实性与信息性。</p></li><li><p>例如，在加入指导性提示（instruction+QA）时，InstructGPT 倾向于不作伪答（如选择“I have no comment”），而 GPT-3 则容易自信地给出错误答案。</p></li></ul><figure><img src="/assets/15-dbwcXYjm.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>InstructGPT 输出更少毒性内容，尤其在有“尊重”提示下</strong></p><ul><li><p>使用 RealToxicityPrompts 数据集 + Perspective API 自动打分 + 人类评估。</p></li><li><p><strong>Figure 7</strong> 显示：</p><ul><li><p>有“请保持尊重”提示时，InstructGPT 显著比 GPT-3 更少生成有毒文本。</p></li><li><p>无提示时，两者毒性差异减小。</p></li><li><p>若刻意要求生成毒性内容，InstructGPT 反而更“有效”执行（更高毒性），说明其任务执行能力更强，但未具内置限制。</p></li></ul></li></ul><figure><img src="/assets/16-C8sMC9yg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>在偏见测试中未表现出优势</strong></p><ul><li>在 CrowS-Pairs 和 Winogender 数据集上，InstructGPT 和 GPT-3 偏见水平相当，有时更低 entropy 表示模型更“确信”其回答，但不一定更公正。</li></ul><br><p><strong>使用 PPO-ptx 缓解了对齐损失（alignment tax）</strong></p><ul><li><p>原始 PPO 模型在 SQuAD、DROP 等任务上表现退化。</p></li><li><p>但通过在 RL 过程中混入预训练目标（PPO-ptx），可基本恢复甚至超越 GPT-3 性能（见附录图 29–34）。</p></li></ul><hr><p><strong>3. 定性分析与模型行为观察</strong></p><p><strong>模型泛化能力强：能处理非训练分布指令</strong></p><ul><li><p>InstructGPT 可：</p><ul><li><p>处理 <strong>非英语指令</strong>，如法语（尽管有时仍用英文回应）</p></li><li><p>总结并解释 <strong>代码片段</strong></p></li></ul></li><li><p><strong>Figure 8</strong> 示例显示：</p><ul><li>GPT-3 未能回答“列表 C 的作用”问题，InstructGPT 给出较为合理的解释（虽然也不完全正确）</li></ul></li></ul><figure><img src="/assets/17-C6qemh7-.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>模型仍存在简单错误与对“荒谬”指令的顺从</strong></p><ul><li><p>InstructGPT 在面对带<strong>错误前提</strong>的指令时，可能不会质疑，而是“默认接受并执行”。</p></li><li><p>它也倾向于<strong>过度规避风险</strong>，在回答简单问题时冗长解释或“过于中性”。</p></li><li><p><strong>Figure 9</strong> 展示：</p><ul><li><p>对“冥想后吃袜子有何用”这类指令，GPT-3 胡编乱造；InstructGPT 则写出听起来“认真合理”的答案，但仍在胡说。</p></li><li><p>对“炮弹打南瓜”的问题，InstructGPT没能直接回答（如“炸碎”），而是列举可能性并犹豫。</p></li></ul></li></ul><figure><img src="/assets/18-BPVdRTOY.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><hr><p><strong>总结：InstructGPT 的结果证明了 RLHF 的有效性</strong></p><ul><li><p><strong>提升</strong>：输出更符合人类偏好，减少幻觉与毒性，对指令遵循度高。</p></li><li><p><strong>挑战</strong>：仍可生成有害内容，对荒谬命令未进行识别，任务复杂性上限未显现。</p></li><li><p><strong>泛化性</strong>：在代码、非英语指令等非监督数据上表现较好。</p></li><li><p><strong>控制手段</strong>：通过 PPO-ptx 控制 alignment tax，维持 NLP 性能。</p></li></ul><h2 id="讨论" tabindex="-1"><a class="header-anchor" href="#讨论"><span>讨论</span></a></h2><p>InstructGPT 是 OpenAI 迭代式对齐研究计划的一部分，目标是使现有模型更符合人类意图，同时构建适用于未来更强 AI 的通用方法。</p><p><strong>RLHF 是一种低成本高回报的对齐方法</strong></p><ul><li><p>与预训练相比，使用 RLHF 对齐语言模型所需的计算成本极低：</p><ul><li><p>训练 GPT-3（175B）需约 <strong>3640 petaflop/s-days</strong>；</p></li><li><p>而 InstructGPT 的 SFT 阶段只需 <strong>4.9 petaflop/s-days</strong>；</p></li><li><p>PPO 微调也仅为 <strong>60 petaflop/s-days</strong>。</p></li></ul></li><li><p>与其训练更大的模型，不如在现有模型上投资对齐方法：例如，1.3B InstructGPT 的输出比 175B GPT-3 更受欢迎（见 Figure 1）。</p></li></ul><p><strong>RLHF 能够泛化“指令跟随能力”</strong></p><ul><li><p>模型在未明确训练的任务上也表现良好，如非英语任务、代码任务（见 Figure 8）。</p></li><li><p>这意味着对齐方法不仅优化模型行为，还能提高其泛化能力，有助于构建更通用、适应性强的智能系统。</p></li></ul><p><strong>可显著降低对齐带来的性能损失(alignment tax)</strong></p><ul><li><p>原始 PPO 模型在一些公开 NLP 数据集上的性能下降（如 DROP、SQuAD）。</p></li><li><p>但通过引入预训练梯度混合（PPO-ptx），可以在保持对齐的同时维持甚至提升性能（详见附录 Figure 29–34）。</p></li></ul><p><strong>将抽象对齐技术成功应用于现实世界模型部署</strong></p><ul><li>与以往在合成任务或小型模型上的研究不同，InstructGPT 将 RLHF 应用于真实的 API 模型中，验证了该技术在生产环境下的可行性和价值。</li></ul><hr><p><strong>我们到底在“对齐”谁？(Who Are We Aligning To ?)</strong></p><p>作者清晰指出当前模型对齐行为的“参考群体”是有局限的，实际对齐的是训练流程中的多重人为偏好叠加：</p><ol><li><p><strong>标注者的偏好</strong>：训练数据和奖励信号均来自一组英语标注者（主要来自美国和东南亚），并非普遍“人类代表”。</p></li><li><p><strong>研究者的设计意图</strong>：OpenAI 研究团队定义了标注规则、标准与示例，标注者受其引导。</p></li><li><p><strong>API 用户行为</strong>：训练 prompt 来源于真实 API 用户提交，其任务形式和风格可能代表一类高频商业用途。</p></li><li><p><strong>用户 ≠ 社会</strong>：OpenAI API 用户为申请加入测试队列的群体，不代表所有潜在用户，更不代表所有受语言模型影响的人群。</p></li></ol><p><strong>结论</strong>：当前对齐并非通用意义上的“人类价值对齐”，而是<strong>特定群体与目标下的实用性对齐</strong>。未来若需面向多元人群，可能需要模型具备多偏好条件控制能力。</p><hr><p><strong>限制与盲点（Limitations）</strong></p><p>InstructGPT 在多个方面仍存在不足：</p><p><strong>模型行为问题：</strong></p><ul><li><p><strong>仍会生成有害、偏见或捏造内容</strong>。</p></li><li><p><strong>过度顺从错误指令</strong>：如“吃袜子”、“南瓜吸引炮弹”等（Figure 9）。</p></li><li><p><strong>复杂指令处理能力不足</strong>：多条件限制（如“用两句话总结 1930 年代法国电影”）仍表现不佳。</p></li></ul><p><strong>数据收集问题：</strong></p><ul><li><p><strong>标注者人数有限（约40人）</strong>，且偏好有偏，可能影响输出的一致性与代表性。</p></li><li><p><strong>多数比较数据仅有 1 位标注者进行判断</strong>，可能遗漏歧义与分歧点。</p></li><li><p><strong>语言多样性不足</strong>：训练数据主要为英文，非英语泛化能力未系统评估。</p></li></ul><p><strong>作者建议未来采用更多元标注、歧义加权、以及特定群体优先原则（如针对少数群体敏感任务）</strong>。</p><hr><p><strong>尚待探索的问题（Open Questions）</strong></p><p>作者列出多个值得进一步研究的问题：</p><ul><li><p><strong>如何更有效缓解毒性与偏见？</strong></p><ul><li><p>引入 adversarial 数据收集（如 <a href="https://arxiv.org/abs/1908.06083" target="_blank" rel="noopener noreferrer">Dinan et al., 2019b</a>）；</p></li><li><p>在预训练层面进行数据过滤（如 <a href="https://arxiv.org/abs/2108.07258" target="_blank" rel="noopener noreferrer">Ngo et al., 2021</a>）；</p></li><li><p>建立更强的拒绝机制以识别恶意请求。</p></li></ul></li><li><p><strong>如何应对多价值体系的冲突？</strong></p><ul><li><p>开发具备“偏好条件化能力”的模型（即对不同用户群体可调节输出风格/规范）；</p></li><li><p>探索“社会契约式”对齐方法以处理价值多样性。</p></li></ul></li><li><p><strong>如何建立更强的 reject 模型机制？</strong></p><ul><li>当任务违反道德或逻辑前提时，模型应能自动识别并拒绝执行，而非“高质量完成”。</li></ul></li></ul><hr><p><strong>社会影响与部署考量（Broader Impacts）</strong></p><p><strong>正面影响：</strong></p><ul><li><p>更符合用户指令、更具可控性、更少毒性，适合用于构建对话助手、总结系统、教育工具等。</p></li><li><p>为“人类偏好引导型 AI”提供现实路径，降低部署风险。</p></li></ul><p><strong>潜在风险：</strong></p><ul><li><p>当前偏好群体有限，若未经适当调节可能导致某些群体观点被系统性排除；</p></li><li><p>对齐本身可被滥用，尤其在军事、虚假宣传等敏感场景下；</p></li><li><p>若拒绝机制不足，模型仍可能在对抗性攻击下暴露隐私、输出有害内容。</p></li></ul><p>作者强调，<strong>技术细节必须伴随规范治理与透明流程</strong>，否则对齐仅为形式上的“驯化”，而非本质的 AI 安全。</p><hr><p><strong>总结:</strong></p><table><thead><tr><th>项目</th><th>关键结论</th></tr></thead><tbody><tr><td>RLHF 价值</td><td>成本低、泛化强、性能好，优于简单 scaling</td></tr><tr><td>当前对齐对象</td><td>并非“人类普遍价值”，而是 OpenAI 标注者 + 用户</td></tr><tr><td>局限性</td><td>模型顺从性过高、多样性不足、对抗性脆弱</td></tr><tr><td>未来方向</td><td>多群体条件对齐、拒绝模型、反毒性 adversarial 训练</td></tr><tr><td>部署建议</td><td>建议伴随伦理审查、偏好反馈机制、开放接口控制</td></tr></tbody></table></div><!----><!----><!----></div><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">最近更新：</span><time class="vp-meta-info" datetime="2025-06-27T02:32:54.000Z" data-allow-mismatch>2025/6/27 02:32</time></div><div class="contributors"><span class="vp-meta-label">贡献者: </span><!--[--><!--[--><span class="vp-meta-info" title="email: 3076679680@qq.com">BinaryOracle</span><!--]--><!--]--></div></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/GPT-3%E8%AE%BA%E6%96%87.html" aria-label="GPT-3 论文"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><iconify-icon class="vp-icon" icon="fa6-solid:file" height="1em" sizing="height"></iconify-icon>GPT-3 论文</div></a><a class="route-link auto-link next" href="/LLM/%E6%A8%A1%E5%9E%8B%E5%B1%82/LLaMA-1%E8%AE%BA%E6%96%87.html" aria-label="LLaMA-1论文"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">LLaMA-1论文<iconify-icon class="vp-icon" icon="fa6-solid:file" height="1em" sizing="height"></iconify-icon></div></a></nav><div id="comment" class="waline-wrapper vp-comment" vp-comment darkmode="false" style="display:block;"><!----></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><div class="vp-footer">技术共建，知识共享</div><div class="vp-copyright">Copyright © 2025 BinaryOracle </div></footer></div><!--]--><!--[--><!----><!--[--><!--]--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-Zfpr4h-A.js" defer></script>
  </body>
</html>
